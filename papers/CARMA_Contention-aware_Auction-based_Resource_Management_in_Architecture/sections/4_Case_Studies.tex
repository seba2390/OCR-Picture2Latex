\section{Case Studies} \label{Case_Studies}
%\subsection{A case study of the main processor (Xeon) and coprocessor (Xeon-Phi) congestion game}
\subsection{CPU Scale-up Scale-out Game}
The emerging high-performance computing applications lead to the advent of \textit{Intel Xeon Phi} co-processor, that when their highly parallel architecture is fully utilized, can run in order of magnitude more performance than the existing processor architectures. The \textit{Xeon Phi} co-processors are the first commercial product of Intel \textit{MIC} processors where the hardware architecture is exposed to the programmer to choose running the code on either \textit{Xeon} processor or \textit{Xeon Phi} co-processors. It is possible that, during the course of execution, either the processor or the co-processor get congested and the performance of the application degrades a lot. Therefore, making a decision to offload the most time-consuming part of the program on \textit{Xeon} or \textit{Xeon Phi} should be made online, based on the contention level.  In this section, we look at the case study of our auction-based model on decision making of running the application on the main or co-processor in a highly congested environment. \\
\indent The experimental results of this section are run on \textit{Stampede} cluster of \textit{Texas Advanced Computing Center}. Table~\ref{Table:Xeon} shows the comparison of \textit{Intel Xeon} and \textit{Xeon Phi} architectures which is used in this section. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table}[!tb] 
\centering
\caption{The comparison of \textit{Intel Xeon} processor and \textit{Intel Xeon Phi} processor.}\label{Table:Xeon}
\begin{tabular}{|c|p{0.8in}|p{1in}|} 
\hline Processors & Xeon E5-2680 & Xeon Phi SE10P \\
\hline Cores/Sockets & 8/2 & 61/1 \\
\hline Clock Frequency & 2.7 GHz & 1.1 GHz  \\
\hline Memory & 32GB 8x4G 4-channels DDR3-1600MHz & 8GB GDDR5 \\
\hline L1 cache & 32 KB & 32 KB \\
\hline L2 cache & 256 KB & 512 KB \\
\hline L3 cache & 20 MB & - \\
\hline
\end{tabular}
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
It is observed that congestion has a significant impact on the performance of running the application on \textit{Xeon} and \textit{Xeon Phi} machines. Since most cloud computing machines are shared between thousands of users, the programmer not only should get the benefit of parallelism by offloading the most time-consuming part of the code to the larger number of low-performance cores (\textit{Xeon Phi}) but also should consider the congestion level (number of co-runners) in the system. To this end, we performed experiments on \textit{Stampede} clusters. We executed \textit{MiniGhost} application which is a part of \textit{Mantevo} project \cite{mantevo} which uses difference stencils to solve partial differential equations using numerical methods. The applications use the profiling utility functions at $t=0$ and during the course of execution update the utility function based on the observed performance on each core using Equation~\ref{eq:belief}. Then, they can revisit their previous action on running the code on either the processor or co-processor during run-time. \\
\indent Figure~\ref{Fig:congestion} shows the total execution time with respect to congestion we made in \textit{Xeon} and \textit{Xeon Phi}. In this experiment we ran the same problem size on a \textit{Xeon} and \textit{Xeon Phi} machine multiple times so that we could see the effect of load on the total execution time of our application. It was observed that with the same number of threads \textit{Xeon}'s performance degrades more than \textit{Xeon phi}. 
Next, we tried to change the application behavior using a congestion-aware game theoretic algorithm to offload the most time-consuming part of the application based on the performance behavior of applications. Figure~\ref{Fig:performance_over_time} shows the result of our game-theoretic model during the execution time. It is observed that during the course of execution, the applications change their strategy on either choosing the main processor or the co-processor and all applications' performance converge to an equilibrium point where applications don't want to change their strategy. \\
\indent Furthermore, it is shown that CARMA can bring in up to 106.6\% improvement in total execution time of applications compared to static approach when the number of co-runners is six. The performance improvement would be significant when the number of co-runners increase. Figure~\ref{Fig:Perfomance_Comparison} shows the performance comparison of CARMA and static approach which does not consider the congestion dynamism in the system and the decision is only made based on the parallelism level in the code. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[!tb]
\centering
%\includegraphics[height=3in, width=2.5in]{NodeArchs2.pdf}
\includegraphics[height=1.5in, width=3.5in]{Images/Xeon.pdf} % diman.pdf
%\epsfig{file=Dataset.eps, height=2.5in, width=3in}
\caption{Congestion effect on \textit{Xeon} and \textit{Xeon Phi} machines.}
\label{Fig:congestion}
\vspace{-1\baselineskip}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[!tb]
\centering
%\includegraphics[height=3in, width=2.5in]{NodeArchs2.pdf}
\includegraphics[height=1.5in, width=3.5in]{Images/Game.pdf}%Game_during_time.pdf
%\epsfig{file=Dataset.eps, height=2.5in, width=3in}
\caption{Performance of 6 instances of applications during the execution time for our proposed game model.}\label{Fig:performance_over_time}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[!tb]
\centering
%\includegraphics[height=3in, width=1.5in]{NodeArchs2.pdf}
\includegraphics[height=1.5in, width=3.5in]{Images/Congestion.pdf}  %Congestion_aware.pdf
%\epsfig{file=Dataset.eps, height=2.5in, width=3in}
\caption{Performance comparison of congestion-aware schedule versus static schedule.}
\label{Fig:Perfomance_Comparison}
\vspace{-1\baselineskip}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{A Case Study of Private and shared cache game}
One of the challenging problems in \textit{CMP} resource management systems is whether applications benefit from a shared large last level cache or an isolated private cache. We evaluated CARMA's performance, on a 10MB LLC shown in Figure~\ref{Fig:cache_hierarchy}, where 2MB, 1MB, 512kB, 256kB and 128kB levels of LLC can potentially be shared between 16, 8, 4, 2 and 1 applications respectively, the cache levels have 16, 8, 4, 2, and 1 ways. Table~\ref{Table:Workloads} summarizes the studies workloads and their characteristics, including miss per kilo instructions (\textit{MPKI}), memory bandwidth usage, and IPC. We use applications from \textit{Spec 2006} benchmark suite \cite{Spec:website}. We use \textit{Gem5} full system simulator in our experiment ~\cite{binkert2011gem5, Gem5:website}. Table~\ref{Table:Experimental_Set_Up} shows the experimental setup in our experiments.\\
\indent To evaluate the performance of our proposed approach we use utility functions for different number of cache ways shown in Figure~\ref{fig:Cache_IPC}. These utility functions at the start of the execution can be found using either profiling techniques or stack distance profile \cite{kim2004fair, suh2002new, suh2004dynamic} of applications assuming there are no co-runners in the system. Next, during run-time, the applications can update their utility functions based on Equation~\ref{eq:belief}. Therefore, there is a learning phase where applications learn about the state of the system and update the utilities accordingly. The stack distance profile indicates how many more cache misses will be added if the application has less number of ways in the cache. Based on the stack distance profile, the applications can update their utility function and bid for the next iteration of the auction if they like to change their allocation. Next, we bring an example of the auction for one time step of the game. This time step can be repeated once an application arrives or leaves the system or when an application's phase changes during run-time. However, in case of one application's phase change or arriving or leaving the system, the algorithm reaches the optimal assignment in much fewer iterations since all other assignments are fixed and a few applications would be affected.\\
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[!tb]
\centering
%\includegraphics[height=3in, width=2.5in]{NodeArchs2.pdf}
\includegraphics[height=1.5in, width=3.3in]{Images/Cache_Hierarchy_2.pdf}
%\epsfig{file=Dataset.eps, height=2.5in, width=3in}
\caption{The proposed last level cache hierarchy model.}\label{Fig:cache_hierarchy}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table}[!tb] %\scriptsize
\centering
\caption{Experimental Setup.}
\label{Table:Experimental_Set_Up}
\begin{tabular}{|c|p{1.5in}|} 
\hline Processors & Single threaded with private L1 instruction and data caches \\
\hline Frequency & 1GHz \\
\hline L1 Private ICache & 32 kB, 64-byte lines, 4-way associative\\
\hline L1 Private DCache & 32 kB, 64-byte lines, 4-way associative \\
\hline L2 Shared Cache & 128 kb-2 MB, 64-byte lines, 16-way associative \\
\hline RAM & 12 GB \\
\hline
\end{tabular}
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table}[!tb] 
\centering
\caption{Evaluated workloads.}
\label{Table:Workloads}
\begin{tabular}{p{0.7cm} p{1.5cm} p{1cm} p{1.7cm} p{1cm} }
\hline
%\begin{tabular}{|l|c|c|c|} \hline
{\bf \#} & \bf Benchmark & MPKI & Memory BW & IPC  \\
\hline 
{\bf 1} & astar & 1.319 & 373 MB/s & 2.057 \\
{\bf 2} & bwaves & 10.47 & 1715 MB/s & 0.661 \\
{\bf 3} & bzip2 & 3.557 & 1194 MB/s & 1.367 \\ 
{\bf 4} & dealII &  0.935 & 307 MB/s & 2.107 \\
{\bf 5} & GemsFDTD & 0.004 & 2.19 MB/s & 2.023 \\
{\bf 6} & hmmer & 2.113 & 1547 MB/s & 2.861 \\
{\bf 7} & lbm & 19.287 & 3954 MB/s & 0.533 \\
{\bf 8} & leslie3d & 8.469 & 1942 MB/s & 1.297 \\
{\bf 9} & libquantum & 10.388 & 1589 MB/s & 0.531 \\
{\bf 10} & mcf & 16.93 & 820 MB/s & 0.073 \\
{\bf 11} & namd & 0.051 & 20.32 MB/s & 2.362\\
{\bf 12} & omnetpp & 10.34 & 1147 MB/s & 0.504 \\
{\bf 13} & sjeng & 0.375 & 139.2 MB/s & 1.403 \\
{\bf 14} & soplex & 4.672 & 390.8 MB/s & 0.513 \\
{\bf 15} & sphinx3 & 0.349 & 202.8 MB/s & 2.223 \\
{\bf 16} & streamL & 31.682 & 3619 MB/s & 0.581 \\
{\bf 17} & tonto & 0.260 & 107 MB/s & 2.036 \\
{\bf 18} & xalancbmk & 12.703 & 1200 MB/s & 0.558 \\
%\hline  
\hline
\end{tabular}
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{comment}
Assume $n$ different applications denoted by ${U1, U2, .. Un}$ with different cache benefits which may affect each other with different cost functions. Figure 1 shows the different applications which are the vertices of the graph with their impact on each other which are the weights of edges in the graph. If two vertices are not connected in the graph, it means that they would not affect each others' performance. For example, one application is CPU bound and does not benefit from larger memory bandwith and the other is memory bound and does not benefit from having more CPU capacity. So different applications affect each other's performance with different coefficients. 
\end{comment}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[!tb]
\centering
\includegraphics[height=3.5in, width=6.5in]{Images/Cache_IPC_v2.pdf}
\caption{IPC for different size of LLC.}\label{fig:Cache_IPC}  
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\indent \textbf{Example:} As an example, suppose we have 5 different applications and 5 different cache levels with different capacities of 128KB, 256KB, 512KB, 1MB and 2MB. In addition, suppose the 128kB cache level can not accomodate more than one application and 256kB cache can accomodate 2 applications, 512kB level can have 4 applications, 1MB cache can have 8 applications and 2MB cache can have at most 16 applications. Let's assume the following matrix be the utility function of each application on each cache level. \\
\indent Some applications may get better utility from smaller cache space since they are less congested and since these applications have low data locality, moving to larger cache spaces not only does not increase their performance but also degrades the performance by evicting other applications from the cache and making contention on the memory bandwidth which is a more vital resource for them \footnote{\textit{libquantum}, \textit{streamL}, \textit{sphinx3}, \textit{lbm} and \textit{mcf} are examples of such applications.}.  \\
\begin{equation}
M = \bordermatrix{~ & 1way & 2way & 4way & 8way & 16way \cr
  App1 & 1.9 & 1.7 & 1.5 & 1 & 0.9 \cr
  App2 & 1.6 & 1.3 & 1.1 & 0.8 & 0.7 \cr
  App3 & 1.4 & 1.0 & 0.6 & 0.5 & 0.4 \cr
  App4 & 0.3 & 0.6 & 0.9 & 1.2 & 1.4 \cr
  App5 & 0.7 & 0.8 & 1.1 & 1.4 & 1.7 \cr}
\end{equation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
In the first iteration of the bidding, the first 3 applications bid for the most profitable resource which is 128kB cache and they submit a bid equal to the difference of profit between the first and the second most profitable resource. Therefore, the first application, submits 0.2 bid to 128kb and the second application submits 0.3 and the third application submits 0.4. Since only one of the players can acquire the 128kB cache space, the first application will get it. The 4th and 5th application compete for 2MB cache space and they both get it with the sum bid of both which is 0.5. In the next round, the prices will be updated and since applications 2 and 3 don't have any cache assignment compete for the 256kB cache space and each bid 0.2 which is the difference between 1.7 and 1.5 and 1.3 and 1.1 in the performance matrix accordingly. Since the second level cache can accommodate both applications the price will be updated and the minimum bidding price for someone to get this cache level is updated to the minimum bid of both which is 0.2. Therefore, if some application bid more than 0.2 it can acquire the resource and the application with the smallest bid has to resubmit the bid to acquire the resource. Figure~\ref{fig:first_round}, ~\ref{fig:second_round}, and ~\ref{fig:third_round} show the bidding steps and the prices and minimum price of bidding accordingly. As seen from the figures, the auction terminates in three iterations when there exist five applications. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
\begin{figure*}[!htb]
        \centering
        \begin{subfigure}[b]{0.28\textwidth} %//0.28 bood
                \includegraphics[width=\textwidth]{Images/bid0.pdf}
                \caption{first round.}
                \label{fig:first_round}
        \end{subfigure}%
        ~ %add desired spacing between images, e. g. ~, \quad, \qquad etc.
          %(or a blank line to force the subfigure onto a new line)
        \begin{subfigure}[b]{0.28\textwidth}
                \includegraphics[width=\textwidth]{Images/bid2.pdf}
                \caption{second round.}
                \label{fig:second_round}
        \end{subfigure}
        ~ %add desired spacing between images, e. g. ~, \quad, \qquad etc.
          %(or a blank line to force the subfigure onto a new line)
        \begin{subfigure}[b]{0.28\textwidth}
                \includegraphics[width=\textwidth]{Images/bid3.pdf}
                \caption{third round.}
                \label{fig:third_round}
        \end{subfigure}  

                \caption{Cache allocation, a) first round, b) second round and c) third round of bidding.}\label{fig:Auction_rounds}    
       % \vspace{-2\baselineskip}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{-1\baselineskip}
\subsection{A Case Study for Hybrid Cache Game}
\indent In hybrid cache game, each cache partition can have a cluster of applications. We use different mixes of 4 to 16 applications from \textit{Spec 2006} to evaluate the performance of our proposed approach compared to others. To evaluate our approach, we selected the state-of-the-art centralized cache partitioner~\cite{8327002} (KPart) as a competitor which aims at maximizing the global IPC speedup. CARMA uses multi-resource valuations, so each application can have any criteria to maximize its payoff. In order to provide a fair comparison with our approach, we use IPC speedup as the optimization goal for all applications.\\% having the profile of the applications and similar bundled resources for the participants.\\ 
\indent Figure~\ref{fig:IPC_mix} shows the normalized throughput of 10 different mix of applications \cite{srikantaiah2011morphcache}, using CARMA, KPart~\cite{8327002}, equal separate cache partitioning and completely shared cache space after convergence. Furthermore, Figure~\ref{fig:scalability} shows the scalability of our proposed algorithm. When the number of co-runners increases from 2 to 16, the performance improves without any need to track each applications' performance in a central module. Having full information about applications' profiles, CARMA outperforms the other centralized competitors, when the number of the applications increases.\\
%The results of throughput and performance compared to DABMFT are pretty the same except little changes because of partitioning is a little sensitive to initial state, but the optimal valuation and allocation after KPart partitioning are the same. It is why that we haven't shown it in the figures not to confuse the other results. However, DABMFT calculation is more complex for computer architecture purpose, since it is designed for multiple applications with multiple sellers.
\indent Since KPart is a centralized (not an auction-based) approach, we assume that it has an unlimited budget. The budget matters in CARMA. We setup another experiment to track the variations of the normalized throughput versus the normalized budget for a mix of 16 applications. Figure~\ref{fig:Relative_Thr} shows that the throughput of CARMA, is very sensitive to the budget. The throughput changes dramatically at some inflection point, and at the end it is saturated but higher than KPart.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[!tb]
\centering
%\includegraphics[height=3in, width=1.5in]{NodeArchs2.pdf}
\includegraphics[height=1.5in, width=3.5in]{Images/IPC.pdf}
%\epsfig{file=Dataset.eps, height=2.5in, width=3in}
\caption{Throughput of a shared, solo, CARMA and KPart cache allocation schemes.}
\label{fig:IPC_mix}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[!tb]
\centering
%\includegraphics[height=3in, width=1.5in]{NodeArchs2.pdf}
\includegraphics[height=1.5in, width=3.5in]{Images/Scalability.pdf}
%\epsfig{file=Dataset.eps, height=2.5in, width=3in}
\caption{Performance improvement of CARMA and KPart for different number of applications with respect to shared LLC for the case study of cache congestion game.}
\label{fig:scalability}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[!tb]
\centering
%\includegraphics[height=3in, width=1.5in]{NodeArchs2.pdf}
\includegraphics[height=1.5in, width=2.5in]{Images/Thr_relative-eps-converted-to.pdf}
%\epsfig{file=Dataset.eps, height=2.5in, width=3in}
\caption{Relative throughput w.r.t applications' normalized budget.}
\label{fig:Relative_Thr}
\vspace{-1\baselineskip}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{comment}
\subsection{Experimental setup}


\newtheorem{defin}{Definition}
\begin{defin}
Let's assume each user $Ui$ in the system is defined as one vertice of a graph $G$ in the system and let each edge in the graph shows which subset of users can impact each others' performance and the associated weight of each edge show the cost function of how two users affect each others' performance in the system. Each edge has a weight function denoted by ${P1(n), P2(n), ... Pe(n)}$, where $e$ is the number of edges in Graph $G$. Let $A=A_1 \times A_2 \times ... \times A_n $ be the set of actions that each user can play. 
\end{defin}

This representation of the cache congestion game has a space complexity of which is exponential in terms of a larges degree in the graph. 
If the number of users and the action set is polynomially bounded then the game has a polynomial representation and  




For a simple illustrative example assume two applications which would affect each other's performance with $f_i$. The payoff table of a subset of users who can affect each other can be shown in Table 1. We can easily extend the game for $n$ users using $f_1$ and $f_2$ to be a function of the number of users which can impact each other.
We used Gem5, full system simulator running Spec OMP benchmarks to find how different applications sharing a specific amount of shared cache impact each other's performance. 
The applications which we did experiments on are listed in Table 2.
\end{comment}  