\IEEEraisesectionheading{\section{Introduction} \label{introduction}}
% no \IEEEPARstart
\IEEEPARstart{T}{he} number of cores on chip multiprocessors (\textit{CMP}) is increasing each year and it is believed that only many-core architectures can handle the massive parallel applications. Server-side \textit{CMP}s usually have more than 16 cores and potentially more than hundreds of applications can run on each server. These systems are going to be the future generation of the multi-core processor servers. Applications running on these systems share the same resources like last level cache (\textit{LLC}), interconnection network, memory controllers, off-chip memories or co-processors where the higher number of applications makes the interaction and impacts of various resource levels more complex. Along with the rapid growth of core integration, the performance of applications highly depend on the allocation of the resources and especially the \textit{contention} for shared resources \cite{tang2011impact, zhuravlev2010addressing, hsu2006communist, kim2004fair, cho2006managing, tootaghajICCD, farhat2016stochastic, tootaghaj2016optimal, tootaghaj2015evaluating, farhat2016towardsStoc, tootaghaj2015thesis}. In particular, as the number of co-runners running on a shared resource increases, the magnitude of performance degradation increases. Also, the selection of the objective function to define what \enquote{best} means for all applications is challenging or even theoretically impossible to improve IPC of one application and memory latency of another application simultaneously in a system. As a result, this new architectural paradigm introduces several new challenges in terms of scalability of resource management and assignment on these large-scale servers. Therefore, a scalable competition method between applications to reach the optimal assignment can significantly improve the performance of co-runners on a shared resource. Figure~\ref{fig:Slow_down} shows an example of performance degradation for 10 \textit{spec 2006} applications when running on a shared 10MB \textit{LLC} (Shared), or when running on a private 1MB \textit{LLC} (Separate).  \\
%\indent Mohammad:\\
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[!tb]
\centering
%\includegraphics[height=3in, width=2.5in]{NodeArchs2.pdf}
\includegraphics[height=1.5in, width=3.5in]{Images/Perf.pdf} %Slow_down.pdf
%\epsfig{file=Dataset.eps, height=2.5in, width=3in}
\vspace{-1.5\baselineskip}
\caption{Performance degradation of 10 different \textit{spec 2006} applications sharing \textit{LLC}.}
\label{fig:Slow_down}
\vspace{-1.2\baselineskip}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\indent Among these shared resources, sharing \textit{CPU}s and \textit{LLC}s plays an important role in overall CMP utilization and performance. Modern \textit{CMP}s are moving towards heterogeneous architecture designs where one can get advantage of both small number of high performance \textit{CPU}s or higher number of low performance cores. The advent \textit{Intel Xeon Phi} co-processors is an example of such heterogeneous architectures that during run-time the programmer can decide to run any part of the code on small number of \textit{Xeon} processors or higher number of \textit{Xeon Phi} co-processors. Therefore, the burden of making decisions on getting the shared resources is moving towards the applications. In addition to the shared \textit{CPU}s, shared \textit{LLC} keeps data on chip and reduces off-chip communication costs \cite{liu2004organizing}. Sometimes an application may flood on a cache and occupy a large portion of available memory and hurt performance of another application which rarely loads on memory, but its accesses are usually latency-sensitive. Recently, many proposals target partitioning the cache space between applications such that (1) each application gets the minimum required space, so that per-application performance is guaranteed to be at an acceptable level, (2) system performance is improved by deciding how the remaining space should be allocated to each one. \\
\indent Prior schemes \cite{zhuravlev2010addressing, qureshi2006utility, lin2008gaining, iyer2004cqos, liu2004organizing, rafique2006architectural, jiang2008analysis} are marching towards these two goals, usually by trading off the system complexity and maximum system utilization. It is shown that neither a pure private \textit{LLC}, nor a pure shared \textit{LLC}, can provide optimal performance for different workloads \cite{cho2006managing}. In general, cache partitioning techniques can be divided into way partitioning and co-scheduling techniques. In a set-associative cache, partitioning is done by per-way allocation. For example, in a 4-way 512KB shared cache allocating 128KB to application \textit{A} means to allow it storing data blocks in only one way per-set, without accessing remaining. Co-scheduling techniques try to co-schedule a set of applications with lowest interference together at the same time such that the magnitude of slow-down for each application is the same or a performance metric is optimized for all applications. However, it is shown that, depending on the objective function for the performance metric, cache allocation can result in totally different allocations \cite{hsu2006communist}. In general Prior schemes have the following three limitations:\\
%\begin{enumerate}
\indent \textbf{1. Scalability}: All of the prior schemes suffer from scalability; especially when the approach is tracking the application's dynamism \cite{qureshi2006utility, zhuravlev2010addressing, jiang2008analysis}. The reason is that algorithm complexity becomes higher in dynamic approaches. The root cause of this complexity is that all previous techniques make decisions (cache partitioning, co-scheduling) centralized using a central hardware or software. For example, main algorithm of \cite{qureshi2006utility} has exponential complexity $O( \binom{N+K-1}{K-1} )$ where $N$ is the number of applications sharing \textit{LLC} and $K$ is the number of ways. Table~\ref{table:complexity} shows the state of the art cache partitioning algorithms and their complexity of checking performance of different permutations.\\
\indent \textbf{2. Static-based}: Most of the prior works, use static co-scheduling to degrade slow-down of co-running applications on the same shared cache. However, static-based approaches cannot catch dynamic behavior of applications. Figure~\ref{fig:Dynamic} shows an example of two applications' IPC (\textit{hmmer} and \textit{mcf}) from \textit{Spec 2006} under different \textit{LLC} sizes. Let us consider a case where we have two cache sizes, a large cache of 1MB which can be shared between applications, and two private caches of 512KB which are not shared. The two applications are competing for the cache space. Suppose that both applications have two phases $(0,T)$ and $(T,2T)$. If the first application gets the larger cache space its \textit{IPC} increases by 35 percent in the first phase and by 20.6 percent in the second phase. The second application's \textit{IPC} increases by 15 percent in the first phase and by 36.84 percent in the second phase if it gets the larger cache space. In a static-based scheduling approach, the larger \textit{LLC} is always allocated to the first application with higher IPC in the time interval $(0, 2T)$, but in \textit{CARMA}, the applications compete for the shared resources, and in the first phase, the larger \textit{LLC} is allocated to the first application and in the second phase \textit{CARMA} allocates the larger \textit{LLC} to the second application. Therefore, static-based approaches cannot capture the dynamism in application's behavior and ultimately degrade the performance significantly. \\
\indent \textbf{3. Fairness}: Defining a single parameter for fairness is challenging for multiple applications, since applications have different performance benefits from each resource during each phase. In prior works fairness has been defined as a unique metric (eg. IPC, Power, Weighted Speed-up) for all applications. Therefore, in current approaches, the optimization goal of algorithms is the same for all applications. Consequently, we cannot sum up applications that desire different metrics in the same platform to decide on. However, if one application needs better IPC and another requires lower energy, the previous algorithms are not able to model it. The only way to address diversity of metrics (to be optimized) is to have an appropriate translation between different metrics (eg. IPC to Power) that is not trivial, while not addressed in prior study.\\
\begin{table}[!tb] %\scriptsize
\centering
\caption{\label{table:complexity} Complexity comparison of state-of-the-art \textit{LLC} partitioning/co-scheduling algorithms.}
\begin{tabular}{|c|c|} 
\hline Algorithm & Search Space\\
\hline Utility-based main algorithm \cite{qureshi2006utility} & $ \binom{N+K-1}{N-1}$  \\
\hline \pbox{20cm}{Greedy Co-scheduling \cite{jiang2008analysis}\\  $N$ applications and $N/K$ caches} & $ \binom{N}{K}  $ \\
\hline \pbox{20cm}{Hierarchical perfect matching \cite{jiang2008analysis} \\ $N$ applications} & $N^4 $ \\
\hline \pbox{20cm}{Local optimization \cite{jiang2008analysis} \\ $N$ applications and $N/K$ caches} & ${(N/K)}^2 \binom{2K}{K} $\\
\hline \pbox{20cm}{CARMA \\ $N$ applications and $K$ resources} & ${O(NK)}$\\
\hline
\end{tabular}
\vspace{-1\baselineskip}
\end{table}
\begin{comment}
\begin{figure}%[!htb]
\centering
%\includegraphics[height=3in, width=2.5in]{NodeArchs2.pdf}
\includegraphics[height=2in, width=3.5in]{Images/Complexity.pdf}
%\epsfig{file=Dataset.eps, height=2.5in, width=3in}
\caption{\label{fig:complexity} Complexity comparison of state-of-the-art \textit{LLC} partitioning algorithms.}
\end{figure}
\end{comment}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[!b]
\vspace{-1.5\baselineskip}
\centering
%\includegraphics[height=3in, width=2.5in]{NodeArchs2.pdf}
\includegraphics[height=1.5in, width=3.5in]{Images/Dynamic.pdf} %Slow_down.pdf
%\epsfig{file=Dataset.eps, height=2.5in, width=3in}
\vspace{-2\baselineskip}
\caption{\label{fig:Dynamic} Performance comparison of static and dynamic scheduling of two applications (\textit{hmmer} and \textit{mcf} from \textit{Spec 2006}) under two different \textit{LLC} sizes.}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\textcolor{red}{( Histogram charts on 128Kb cache for each and show how another 256Kb shared should be allocated to them on [Moin] and how it can be improved by dynamic decision (Figure 1 in STT-RAM paper).}\\
%\end{enumerate}
\indent In this paper, we present a game-theoretic resource assignment method to address all the above shortcomings including scalability, dynamism and fairness, while applications can get their desired performance based on their utility functions.\\
\indent \textbf{1. Semi-Decentralized}: Dual of each centralized problem is decentralized, if the optimization goal is broken into a smaller meaningful sub-problems. In the context of heterogeneous resource assignment this is straightforward. The profiling, analyzing and evaluating the demands are on application side, but the final decision on assigning the resources to applications based on the applications' bids is easily performed by the OS while they compete with each other for the best assignment. Like a capitalist system, the complexity of the governing transfers to the independent entities, and the government just make the policies and the final decisions. To achieve this, we introduce a novel market-based approach. Roughly speaking, the complexity of our approach in worst case scenario (for each application) is $O(NK)$ where $N$ is the number of the applications and $K$ is the number of available resources. However, on average the auction terminates in less than $N/2$ iterations.\\
\indent \textbf{2. Dynamic}: In order to confront the scalability problem of previous approaches, we use a market-based approach to move the decision making to the individual applications. Iterative auctions have been designed to solve non-trivial resource allocation problems with low complexity cost in government sale of resources, \textit{eBay}, real estate sales and stock market. Similarly, decentralized computation complexity is lower than centralized (for each application) which provides the opportunity to make the decision revisiting the allocation in small time quantum, or when a new application leaves or comes into the system.\\
\indent \textbf{3. Fair}: The proposed method solves the heterogeneous resource assignment problem in the context of marketing. Applications' demand regardless of the global optimization objective (IPC, Power, etc.) translates to the true valuation of their own performance. Resource assignment to the applications with the highest bids is performed by the auctioneer (the OS); making it local optimization objectives. Hence, resource assignment can be performed for different applications with different objectives known as utility functions.\\
\indent Overall, the proposed approach for cache contention game on average brings in 33.6\% improvement in system performance (when running 16 applications) compared to shared \textit{LLC}; while reaching less than 11.1\% of the maximum achievable performance in the best dynamic scheme. In the case study of heterogeneous CPU assignment, it brings in 106.6\% improvement (when running 16 applications at the same time). Also, the performance improvement increases even more as the number of co-running applications increases in the system. \\
\indent \textbf{Other potentials}: We introduce an auction-based resource management approach for different applications in large-scale competition games. In short, we as a system owner pay for a high-end CMP system for servers and guarantee that each application/user takes its best from the system by paying us back, or we as an application owner bid/pay the system to get the resources for my best performance. The auctioneer is application-agnostic, and does not interfere with applications' profile to globally optimize the system, but the applications compete for their own improvement. The two case studies of cache partitioning and CPU sharing are examples for resource sharing and the proposed approach can be employed in other resource partitioning algorithms. \\
\indent The reminder of the paper is organized as follows. Section~\ref{Motivation} discusses the background and motivation behind this work. In section~\ref{Problem_definition}, we discuss our auction-based game model. Section~\ref{Case_Studies} discusses the case study of cache contention game and the case study of main processor and co-processor contention and simulation results. Section~\ref{Related_works} studies related works and Section~\ref{Conclusion} concludes the paper with a summary.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%The conventional resource management is usually done by the operating system; however, this approach is not scalable for today's' large-scale servers and the operating system may not have enough information about application's demands. For example, one application may benefit more from more memory and others may benefit from having more CPU or network bandwidth and assigning the same amount of resources to different applications/users is not a good way for resource management. Furthermore, the traditional resource management systems rely on applications' utilization rather than applications' performance, but the higher utility does not always lead to higher performance. For example, take the traditional LRU-based cache assignment strategy. The resource manager gives more cache space to the applications with higher cache utility. But there may be streaming applications which have high cache utility but their cache reuse is very low and giving more cache capacity to them wouldn't affect the performance. \\ 
%%\indent Mohammad:\\
\begin{comment}
%\indent The conventional resource assignment approaches does not work properly for the new large-scale CMPs. 
In short, we pay for a high-end CMP system for servers and guarantee that each application/user takes its best from the system. Figure~\ref{fig:Slow_down} shows an example of performance degradation for 10 \textit{spec 2006} applications running on a shared 10MB \textit{LLC} and separate run on 1MB \textit{LLC}.   \\
\indent In this paper we present a distributed game-theoretic resource management approach for different applications and users in large-scale resource competition games.   
The conventional resource management is usually done by the operating system; however, this approach is not scalable for today's' large-scale servers and the operating system may not have enough information about application's demands. For example, one application may benefit more from more memory and others may benefit from having more CPU or network bandwidth and assigning the same amount of resources to different applications/users is not a good way for resource management. Furthermore, the traditional resource management systems rely on applications' utilization rather than applications' performance, but the higher utility does not always lead to higher performance. For example, take the traditional LRU-based cache assignment strategy. The resource manager gives more cache space to the applications with higher cache utility. But there may be streaming applications which have high cache utility but their cache reuse is very low and giving more cache capacity to them wouldn't affect the performance. \\ 
\indent Game theory has been used extensively in economics, political and mathematical decision-making situations \cite{tootaghaj2011game, tootaghaj2011risk, kotobi2017spectrum, kotobi2015introduction}. A game is a situation, where the output of each player not only depends on her own action in the game but also on the action of other players \cite{osborne1994course}. Auction games are a class of games which has been used to formulate real-world problems of assigning different resources between $n$ users. Auction game framework can model resource competition, where the payoff (cost) of each application in the system is a function of the contention level (number of applications) in the game.\\
\indent Inspired by nature predator-prey interactions in real life games, there exists a repeated interaction between competitors in a resource sharing game. We show that, assuming a large number of applications the service rate of each application on each resource converges to each other. Furthermore, we show that the auction model is strategy-proof, such that no application can get more utilization by bidding more or less than the true value of the resource.   \\  
\indent The main questions we address in this paper are:
\begin{itemize}
  \item Does pure Nash equilibrium exist in a heterogeneous resource competition game? 
  \item How do different applications affect each other if they are given a private/shared cache.
  \item Having the dynamicity of different applications coming and leaving in the system and using shared resources, what is the convergence time of reaching the Nash equilibrium in the system with respect to the dynamic changes in the system.
  \item Defining a strategy for the best response for each application and a learning phase to converge to the Nash equilibrium in the system. 
\end{itemize}  
\end{comment}
