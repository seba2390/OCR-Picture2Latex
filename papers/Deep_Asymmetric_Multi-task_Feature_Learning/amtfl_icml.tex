%%%%%%%% ICML 2018 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2018} with \usepackage[nohyperref]{icml2018} above.
\usepackage{hyperref}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
%\usepackage{icml2018}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2018}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Deep Asymmetric Muti-task Feature Learning}

%%% load AMS-Latex Package
\usepackage{amsmath,amsfonts}
% \usepackage{amsthm}
\usepackage{amssymb,amsopn}
%s\usepackage{bm} % bold symbol
%\usepackage{bbm}

% define fonts
\newcommand{\vct}[1]{\boldsymbol{#1}} % vector
\newcommand{\mat}[1]{\boldsymbol{#1}} % matrix
\newcommand{\vu}{\vct{u}}

%%%% Special math symbols
\newcommand{\field}[1]{\mathbb{#1}}
\newcommand{\R}{\field{R}} % real domain
%\newcommand{\C}{\field{C}} % complex domain
\newcommand{\F}{\field{F}} % functional domain
%\newcommand{\T}{^{\top}\!\!} % transpose
\newcommand{\T}{^{\textrm T}} % transpose
\newcommand{\TN}{^{-\textrm T}} % transpose


%%% define constant
\newcommand{\cst}[1]{\mathsf{#1}}

%% operator in linear algebra, functional analysis
\newcommand{\inner}[2]{#1\cdot #2}
\newcommand{\norm}[1]{\left\|#1\right\|}
\newcommand{\twonorm}[1]{\|#1\|_2^2}
% operator in functios, maps such as M: domain1 --> domain 2
\newcommand{\Map}[1]{\mathcal{#1}}

% operator in probability: expectation, covariance,
\newcommand{\ProbOpr}[1]{\mathbb{#1}}
% independence
\newcommand\independent{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}
% conditional independence
\newcommand{\cind}[3]{{#1} \independent{#2}\,|\,#3}
% conditional expectation
\newcommand{\cndexp}[2]{\ProbOpr{E}\,[ #1\,|\,#2\,]}

\def\D{\mathcal{D}}
\def\loss{\mathcal{L}}
\def\R{\mathcal{R}}
\def\reals{\mathbb{R}}
\def\S{\mathcal{S}}

\def\U{\mathcal{U}}
\def\T{\mathcal{T}}

\newcommand\innerP[2]{\langle #1, \, #2 \rangle}
%\newcommand\biginner[2]{\big\langle #1, \, #2 \big\rangle}


% operator in optimization
%\DeclareMathOperator{\argmax}{arg\,max}
%\DeclareMathOperator{\argmin}{arg\,min}
\newcommand{\todo}[1]{{\color{red}#1}}

\DeclareMathOperator*{\minimize}{minimize}
\DeclareMathOperator*{\st}{s.t.}
\newcommand{\argmin}{\operatornamewithlimits{argmin}}

\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{remarks}{Remark}
\newtheorem{example}{Example}

\newlength{\widebarargwidth}
\newlength{\widebarargheight}
\newlength{\widebarargdepth}
\DeclareRobustCommand{\widebar}[1]{%
	\settowidth{\widebarargwidth}{\ensuremath{#1}}%
	\settoheight{\widebarargheight}{\ensuremath{#1}}%
	\settodepth{\widebarargdepth}{\ensuremath{#1}}%
	\addtolength{\widebarargwidth}{-0.3\widebarargheight}%
	\addtolength{\widebarargwidth}{-0.3\widebarargdepth}%
	\makebox[0pt][l]{\hspace{0.15\widebarargheight}%
		\hspace{0.3\widebarargdepth}%
		\addtolength{\widebarargheight}{0.3ex}%
		\rule[\widebarargheight]{0.95\widebarargwidth}{0.1ex}}%
	{#1}}

\newcommand{\eat}[1]{}

\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}

\newcommand{\sj}[1]{{\color{red}{\small\bf\sf [Sungju: #1]}}}
\newcommand{\eh}[1]{{\color{magenta}{\small\bf\sf [Eunho: #1]}}}
\newcommand{\hb}[1]{{\color{blue}{\small\bf\sf [Haebeom: #1]}}}

\newcommand{\by}{\mathbf{y}}
\newcommand{\bh}{\mathbf{h}}
\newcommand{\bo}{\mathbf{o}}
\newcommand{\bW}{\mathbf{W}}
\newcommand{\bb}{\mathbf{b}}
\newcommand{\ent}{\mathcal{H}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\real}{\mathbb{R}} 
% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{wrapfig}

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2018} with \usepackage[nohyperref]{icml2018} above.
\usepackage{hyperref}

% Attempt to make hyperref and algorithmic work together better:

% Use the following line for the initial blind version submitted for review:
%\usepackage{icml2018}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2018}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Deep Asymmetric Multi-task Feature Learning}

\begin{document}
	
	\twocolumn[
	\icmltitle{Deep Asymmetric Multi-task Feature Learning}
	
	% It is OKAY to include author information, even for blind
	% submissions: the style file will automatically remove it for you
	% unless you've provided the [accepted] option to the icml2018
	% package.
	
	% List of affiliations: The first argument should be a (short)
	% identifier you will use later to specify author affiliations
	% Academic affiliations should list Department, University, City, Region, Country
	% Industry affiliations should list Company, City, Region, Country
	
	% You can specify symbols, otherwise they are numbered in order.
	% Ideally, you should not use this facility. Affiliations will be numbered
	% in order of appearance and this is the preferred way.
%	\icmlsetsymbol{equal}{*}
	
	\begin{icmlauthorlist}
		\icmlauthor{Hae Beom Lee}{unist,aitrics}
		\icmlauthor{Eunho Yang}{kaist,aitrics}
		\icmlauthor{Sung Ju Hwang}{kaist,aitrics}
	\end{icmlauthorlist}
	
	\icmlaffiliation{unist}{UNIST, Ulsan, South Korea}
	\icmlaffiliation{kaist}{KAIST, Daejeon, South Korea}
	\icmlaffiliation{aitrics}{AItrics, Seoul, South Korea}
	
	\icmlcorrespondingauthor{Hae Beom Lee}{hblee@unist.ac.kr}
	\icmlcorrespondingauthor{Eunho Yang}{eunhoy@kaist.ac.kr}
	\icmlcorrespondingauthor{Sung Ju Hwang}{sjhwang82@kaist.ac.kr}
	
	% You may provide any keywords that you
	% find helpful for describing your paper; these are used to populate
	% the "keywords" metadata in the PDF but will not be shown in the document
	\icmlkeywords{Machine Learning, ICML}
	
	\vskip 0.3in
	]
	
	% this must go after the closing bracket ] following \twocolumn[ ...
	
	% This command actually creates the footnote in the first column
	% listing the affiliations and the copyright notice.
	% The command takes one argument, which is text to display at the start of the footnote.
	% The \icmlEqualContribution command is standard text for equal contribution.
	% Remove it (just {}) if you do not need this facility.
	
	%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
	\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

	\begin{abstract}
		We propose Deep Asymmetric Multitask Feature Learning (Deep-AMTFL) which can learn deep representations shared across multiple tasks while effectively preventing negative transfer that may happen in the feature sharing process. Specifically, we introduce an asymmetric autoencoder term that allows reliable predictors for the easy tasks to have high contribution to the feature learning while suppressing the influences of unreliable predictors for more difficult tasks. This allows the learning of less noisy representations, and enables unreliable predictors to exploit knowledge from the reliable predictors via the shared latent features. Such asymmetric knowledge transfer through shared features is also more scalable and efficient than inter-task asymmetric transfer. We validate our Deep-AMTFL model on multiple benchmark datasets for multitask learning and image classification, on which it significantly outperforms existing symmetric and asymmetric multitask learning models, by effectively preventing negative transfer in deep feature learning.
	\end{abstract}
	
	\section{Introduction}
	
	Multi-task learning~\cite{caruana97} aims to improve the generalization performance of the multiple task predictors by jointly training them, while allowing some kinds of knowledge transfer between them. One of the crucial challenges in multi-task learning is tackling the problem of \emph{negative transfer}, which describes the situation where accurate predictors for easier tasks are negatively affected by inaccurate predictors for more difficult tasks. A recently introduced method, Asymmetric Multi-task Learning (AMTL)~\cite{amtl}, proposes to solve this negative transfer problem by allowing asymmetric knowledge transfer between tasks through inter-task parameter regularization. Specifically, AMTL enforces the task parameters for each task to be also represented as a sparse combination of the parameters for other tasks, which results in learning a directed graph that decides the amount of knowledge transfer between tasks.
	
	However, such inter-task transfer model based on parameter regularization is limited in several aspects. First of all, in most cases, the tasks exhibit relatedness to certain degree, but the model parameter for a task might not be reconstructed as a combination of the parameter for other tasks, because the tasks are only partly related. Consider the example in Fig.~\ref{concept}(b), where the task is to predict whether the given image has any of the three animal classes. Here, the three animal classes are obviously related as they share a common visual attribute, \emph{stripe}. Yet, we will not be able to reconstruct the model parameter for class hyena by combining the model parameters for class tiger and zebra, as there are other important attributes that define the hyena class, and the \emph{stripe} is merely a single attribute among them that is also shared by other classes. Thus, it is more natural to presume that the related tasks leverage a common set of latent representations, rather than considering that a task parameter is generated from the parameters for a set of relevant tasks, as assume in inter-task transfer models.
	
	\begin{figure*}
		\begin{center}
			\hfill
			\subfigure[Symmetric Feature Sharing MTL]{\raisebox{.03\height}{\includegraphics[height=3.8cm]{concept_gomtl_new.png}}\label{GOMTL}}
			\hfill
			\subfigure[Asymmetric MTL]{\includegraphics[height=3.9cm]{concept_amtl_new.png}\label{AMTL}}
			\hfill
			\subfigure[Asymmetric Multi-task Feature Learning]{\raisebox{-.00\height}{\includegraphics[height=4.3cm]{concept_amtfl_new.png}}\label{AMTFL}}
			\hfill
			%\vspace{-0.05in}
			\caption{\small \textbf{Concept.} (a) Feature-sharing multi-task learning models such as Go-MTL suffers from negative transfer from unreliable predictors, which can result in learning noisy representations. (b) AMTL, an inter-task transfer asymmetric multi-task learning model that enforces the parameter of each task predictor to be generated as a linear combination of the parameters of relevant task predictors, may not make sense when the tasks are only partially related. (c) Our asymmetric multi-task feature learning enforces the learning of shared representations to be affected only by reliable predictors, and thus the learned features can transfer knowledge to unreliable predictors.}\label{concept}
		\end{center}
		\vspace{-0.12in}
	\end{figure*}
	
	Moreover, AMTL does not scale well with the increasing number of tasks since the inter-task knowledge transfer graph grows quadratically, and thus will become both inefficient and prone to overfitting when there are large number of tasks, such as in large-scale classification. While sparsity can help reduce the number of parameters, it does not reduce the intrinsic complexity of the problem.
	
	Finally, the inter-task transfer models only store the knowledge in the means of learned model parameters and their relationship graph. However, at times, it might be beneficial to store what has been learned, in the form of explicit representations which can be used later for other tasks, such as transfer learning. 
	
	Thus, we resort to the multi-task feature learning approach that aims to learn latent features, which is one of the most popular ways of sharing knowledge between tasks in the multi-task learning framework~\cite{argyriou08,go-mtl}, and aim to prevent negative transfer under this scenario by enforcing asymmetric knowledge transfer. Specifically, we allow reliable task predictors to affect the learning of the shared features more, while downweighting the influences of unreliable task predictors, such that they have less or no contributions to the feature learning. Figure~\ref{concept} illustrates the concept of our model, which we refer to as asymmetric multi-task feature learning (AMTFL). 
	
	Another important advantage of our AMTFL, is that it naturally extends to the feature learning in deep neural networks, in which case the top layer of the network contains additional weight matrix for feed-back connection, along with the original feed-forward connections, that allows asymmetric transfer from each task predictor to the bottom layer. This allows our model to leverage state-of-the-art deep neural network models to benefit from recent advancement in deep learning.
	
	We extensively validate our method on a synthetic dataset as well as eight benchmark datasets for multi-task learning and image classification using both the shallow and the deep neural network models, on which our models obtain superior performances over existing symmetric feature-sharing multi-task learning model as well as the inter-task parameter regularization based asymmetric multi-task learning model. 
	
	Our contributions are threefold:
	\begin{itemize}
		\item We propose a novel multi-task learning model that prevents negative transfer by allowing asymmetric transfer between tasks, through latent shared features, which is more natural when tasks are correlated but not cause and effect relationships, and is more scalable than existing inter-task knowledge transfer model. 
		\item We extend our asymmetric multi-task learning model to deep learning setting, where our model obtains even larger performance improvements over the base network and linear multi-task models.  
		\item We leverage learned deep features for knowledge transfer in a transfer learning scenarios, which demonstrates that our model learns more useful features than the base deep networks.
	\end{itemize}
	
	\section{Related Work}
	\paragraph{Multitask learning}
	Multi-task Learning ~\cite{caruana97} is a learning framework that jointly trains a set of task predictors while sharing knowledge among them, by exploiting relatedness between participating tasks. Such task relatedness is the main idea on which multi-task learning is based, and there are several assumptions on how the tasks are related. Probably the most common assumption is that the task parameters lie in a low-dimensional subspace. One example of a model based on this assumption is multi-task feature learning (MTFL)~\cite{argyriou08}, where a set of related tasks learns common features (or representations) shared across multiple tasks. Specifically, they propose to discard features that are not used by most tasks by imposing the $(2,1)$-norm regularization on the coefficient matrix, and solved the regularized objective with an equivalent convex optimization problem. The assumption in MTFL is rather strict, in that the $(2,1)$-norm requires the features to be shared across all tasks, regardless of whether the tasks are related or not. To overcome this shortcoming, \cite{kang11} suggest a method that also learns to group tasks based on their relatedness, and enforces sharing only within each group. However, since such strict grouping might not exist between real-world tasks, \cite{go-mtl} and~\cite{maurer2012sparse} suggest to learn overlapping groups by learning latent parameter bases that are shared across multiple tasks. Multi-task learning on neural networks is fairly straightforward simply by sharing a single network for all tasks. Recently, there has been some effort on finding more meaningful sharing structures between tasks instead of sharing between all tasks~\cite{tnrdmtl,dmtrl,sluice}. Yet, while these models consider task relatedness, they do not consider asymmetry in knowledge transfer direction between the related tasks.
	
	\paragraph{Asymmetric Multitask learning}
	The main limitation in the multi-task learning models based on common bases assumption is that they cannot prevent negative transfer as shared bases are trained without consideration of the quality of the predictors. To tackle the problem, asymmetric multi-task learning (AMTL)~\cite{amtl} suggests to break the symmetry in the knowledge transfer direction between tasks. It assumes that each task parameter can be represented as a sparse linear combination of other task parameters, in which the knowledge flows from task predictors with low loss to predictors with high loss. Then, hard tasks could exploit more reliable information from easy tasks, whereas easy tasks do not have to rely on hard tasks when it has enough amount of information to be accurately predicted. This helps prevent performance degradation on easier tasks from \emph{negative transfer}. However, in AMTL, knowledge is transferred from one task to another, rather than from tasks to some common feature spaces. Thus the model is not scalable and also it is not straightforward to apply this model to deep learning. On the contrary, our model is scalable and straightforward to implement into a deep network as it learns to transfer from tasks to shared features.
	
	\paragraph{Autoencoders}
	Our asymmetric multi-task learning formulation has a sparse nonlinear autoencoder term for feature learning. The specific role of the term is to reconstruct latent features from the model parameters using sparse nonlinear feedback connections, which results in the denoising of the latent features. Autoencoders were first introduced in~\cite{autoencoder} for unsupervised learning, where the model is given the input features as the output and learns to transform the input features into a latent space and then decode them back to the original features. While there exist various autoencoder models, our reconstruction term closely resembles the sparse autoencoder~\cite{ranzato07}, where the transformation onto and from the latent feature space is sparse. \cite{hinton2006science} introduce a deep autoencoder architecture in the form of restricted Boltzmann machine, along with an efficient learning algorithm that trains each layer in a bottom-up fashion. \cite{dae} propose a denoising autoencoder, which is trained with the corrupted data as the input and the clean data as output, to make the internal representations to be robust from corruption. Our regularizer in some sense can be also considered as a denoising autoencoder, since its goal is to refine the features through the autoencoder form such that the reconstructed latent features reflect the loss of more reliable predictors, thus obtaining \emph{cleaner} representations.
	
	\section{Asymmetric Multi-task Feature Learning}
	\begin{figure*}
		\begin{center}
			\hfill
			\subfigure[]{\raisebox{.02\height}{\includegraphics[height=3.5cm]{negative_transfer_rere.png}}\label{negative_transfer}}
			\hfill
			\subfigure[]{\raisebox{.02\height}{\includegraphics[height=3.5cm]{negative_transfer_2_rere.png}}\label{negative_transfer_2}}
			\hfill
			\subfigure[]{\raisebox{.02\height}{\includegraphics[height=3.5cm]{prevent_negative_transfer_rere.png}}\label{prevent_negative_transfer}}
			\hfill
			\subfigure[]{\raisebox{.02\height}{\includegraphics[height=3cm]{relu_projection_re.png}}\label{relu_projection}}
			\hfill
			\vspace{-0.1in}
			\caption{\small (a) An illustration of negative transfer in common latent bases model. (b) The effects of inter-task $\ell_2$ regularization on top of common latent bases model. (c) Asymmetric task-to-basis transfer. (d) An illustration of ReLU transformation with a bias term.}
		\end{center}
		\vspace{-0.15in}
	\end{figure*} 
	
	In our multi-task learning setting, we have $T$ different tasks with varying degree of difficulties. For each task $t\in\{1,\dots,T\}$, we have an associated training dataset $\mathcal{D}_t=\{(\mat{X}_t,\vct{y}_t) | \mat{X}_t \in \mathbb{R}^{N_t \times d}, \vct{y}_t \in \mathbb{R}^{N_t \times 1}\}$ where $\mat{X}_t$ and $\vct{y}_t$ respectively represent the $d$-dimensional feature vector and the corresponding labels for $N_t$ data instances. The goal of multi-task learning is then to jointly train models for all $T$ tasks simultaneously via the following generic learning objective:
	\begin{equation}
	\begin{aligned}
	\operatorname*{min.}_{\mat{W}} \sum_{t=1}^T \ \mathcal{L}(\vct{w}_t; \mat{X}_t, \vct{y}_t) + \Omega(\mat{W}).\label{eq:base_mtl}
	\end{aligned}
	\end{equation}
	where $\mathcal{L}$ is the loss function applied across the tasks, $\vct{w}_t \in \mathbb{R}^{d}$ is the model parameter for task $t$ and $\mat{W} \in \mathbb{R}^{d\times T}$ is the column-wise concatenated matrix of $\vct{w}$ defined as $\mat{W} = [\vct{w}_1\ \vct{w}_2 \cdots \vct{w}_T]$. Here, the penalty $\Omega$ enforces certain prior assumption on sharing properties across tasks in terms of $\mat{W}$. 
	
	One of the popular assumptions is that there exists a common set of latent bases across tasks ~\cite{argyriou08,go-mtl}, in which case the matrix $\mat{W}$ can be decomposed as $\mat{W}=\mat{L}\mat{S}$. Here, $\mat{L} \in \mathbb{R}^{d \times k}$ is the collection of $k$ latent bases while $\mat{S} \in \mathbb{R}^{k \times T}$ is the coefficient matrix for linearly combining those bases. Then, with a regularization term depending on $\mat{L}$ and $\mat{S}$, we build the following multi-task learning formulation:
	\begin{equation}
	\begin{aligned}
	\operatorname*{min.}_{\mat{L},\mat{S}} \sum_{t=1}^T \ \mathcal{L}(\mat{L}\vct{s}_t; \mat{X}_t, \vct{y}_t) + \Omega(\mat{L},\mat{S}).\label{eq:mtl}
	\end{aligned}
	\end{equation}
	where $\vct{s}_t$ is $t^{th}$ column of $\mat{S}$ to represent $\vct{w}_t$ as the linear combination of shared latent bases $\mat{L}$, that is, $\vct{w}_t = \mat{L}\vct{s}_t$. As a special case of \eqref{eq:mtl}, Go-MTL\cite{go-mtl}, for example, encourages $\mat{L}$ to be element-wisely $\ell_2$ regularized and each $\vct{s}_t$ sparse:
	\begin{equation}
	\begin{aligned}
	\operatorname*{min.}_{\mat{L},\mat{S}} \sum_{t=1}^T \Big\{ \mathcal{L}(\mat{L}\vct{s}_t; \mat{X}_t, \vct{y}_t) + \mu \norm{\vct{s}_t}_1 \Big\} + \lambda \norm{\mat{L}}_F^2. \label{eq:go-mtl}
	\end{aligned}
	\end{equation}
	
	On the other hand, it is possible to exploit a task relatedness without the explicit assumption on shared latent bases. AMTL~\cite{amtl} is such an instance of multi-task learning, based on the assumption that each task parameter $\vct{w}_t \in \mathbb{R}^{d}$ is reconstructed as a sparse combination of other task parameters $\{\vct{w}_{s}\}_{s\neq t}$. In other words, it encourages that $\vct{w}_t \approx \sum_{s \neq t} B_{st}\vct{w}_{s}$ where the weight $B_{st}$ of $\vct{w}_s$ in reconstructing $\vct{w}_t$, can be interpreted as the amount of knowledge transfer from task $s$ to $t$. Since there is no symmetry constraint on the matrix $\mat{B}$, AMTL learns asymmetric knowledge transfer from easier tasks to harder ones. Towards this goal, AMTL solves the multi-task learning problem via the following optimization problem:
	\begin{equation}
	\begin{aligned}
	\operatorname*{min.}_{\mat{W},\mat{B}} \sum_{t=1}^T \ (1+\alpha\norm{\vct{b}_t^o}_1) \mathcal{L}(\vct{w}_t; \mat{X}_t, \vct{y}_t) + \gamma \norm{\mat{W}-\mat{W}\mat{B}}_F^2. \label{eq:amtl}
	\end{aligned}
	\end{equation}
	where $B_{tt} = 0$ for $t = 1,...,T$ and $\mat{B}$'s row vector $\vct{b}_t^o \in \mathbb{R}^{1 \times T}$ controls the amount of outgoing transfer from task $t$ to other tasks $s \neq t$. The sparsity parameter $\alpha$ is multiplied by the amount of training loss $\mathcal{L}(\vct{w}_t;\mat{X}_t,\vct{y}_t)$, making the outgoing transfer from hard tasks more sparse than those of easy tasks. The second Frobenius norm based penalty is on the inter-task regularization term for reconstructing each task parameter $\vct{w}_t$.
	
	\subsection{Asymmetric Transfer from Task to Bases}
	One critical drawback of \eqref{eq:go-mtl} is on the severe negative transfer from unreliable models to reliable ones since all task models {equally} contribute to the construction of latent bases. On the other hand, \eqref{eq:amtl} is not scalable to large number of tasks, and does not learn explicit features. In this section, we provide a novel framework for \emph{asymmetric} multi-task \emph{feature} learning that overcomes the limitations of these two previous approaches, and find an effective way to achieve asymmetric knowledge transfer in deep neural networks while preventing negative transfers.
	
	We start our discussion with the observation of how negative transfer occurs in a common latent bases multi-task learning models as in \eqref{eq:go-mtl}. Suppose that we train a multi-task learning model for three tasks, where the model parameters of each task is generated from the bases $\{\vct{l}_1, \vct{l}_2, \vct{l}_3\}$. Specifically, $\vct{w}_1$ is generated from $\{\vct{l}_1,\vct{l}_3\}$, $\vct{w}_2$ from $\{\vct{l}_1,\vct{l}_2\}$, and $\vct{w}_3$ from $\{\vct{l}_2,\vct{l}_3\}$. Further, we assume that the predictor for task $3$ is unreliable and noisy, while the predictors for task $1$ and $2$ are reliable, as illustrated in Figure \ref{negative_transfer}. In such a case, when we train the task predictors in a multi-task learning framework, $\vct{w}_3$ will transfer noise to the shared bases $\{\vct{l}_2,\vct{l}_3\}$, which will in turn negatively affect the models parameterized by $\vct{w}_1$ and $\vct{w}_2$. 
	
	One might consider the naive combination of the shared basis model \eqref{eq:go-mtl} and AMTL \eqref{eq:amtl} to prevent negative transfer among latent features where the task parameter matrix is decomposed into $\mat{L}\mat{S}$ in \eqref{eq:amtl}: 
	\begin{align}\label{eq:amtl_plus_gomtl}
	\operatorname*{min.}_{\mat{L}, \mat{S}, \mat{B}} &\sum_{t=1}^T \ \Big\{ (1+\alpha\norm{\vct{b}_t^o}_1) \mathcal{L}(\mat{L}\vct{s}_t; \mat{X}_t,\vct{y}_t) + \mu \norm{\vct{s}_t}_1 \Big\} \nonumber\\
	&+ \gamma \norm{\mat{LS}-\mat{LS}\mat{B}}_F^2 + \lambda\norm{\mat{L}}_F^2 
	\end{align}
	where $B_{tt}=0$ for $t=1,..,T$. However, this simple combination cannot resolve the issue mainly due to two limitations. First, the inter-task transfer matrix $\mat{B}$ still grows quadratically with respect to $T$ as in AMTL, which is not scalable for large $T$. Second and more importantly, this approach would induce additional negative transfer. In the previous example in Figure \ref{negative_transfer_2}, the unreliable model $\vct{w}_3$ is enforced to be a linear combination of other reliable models via the matrix $\mat{B}$ (the purple dashed lines in the figure). In other words, $\vct{w}_3$ can now affect the clean basis $\vct{l}_1$ that is only trained by the reliable models in Figure \ref{negative_transfer}. As a result, the noise will be transferred to $\vct{l}_1$, and consequently, to the reliable models based on it. As shown in this simple example, the introduction of inter-task asymmetric transfer in the shared basis MTL \eqref{eq:go-mtl} leads to more severe negative transfer, which is in contrast to the original intention.  
	
	To resolve this issue, we propose a completely new type of regularization in order to prevent the negative transfer from the task predictors to the shared latent features, which we refer to as \emph{asymmetric task-to-basis} transfer. Specifically, we encourage the latent features to be reconstructed by the task predictors' parameters in an asymmetric manner, where we enforce the reconstruction to be done by the parameters of reliable predictors only, as shown in Figure \ref{prevent_negative_transfer}. Since the parameters for the task predictors are reconstructed from the bases, this regularization can be considered as an autoencoder framework. The difference here is that the consideration of predictor loss result in learning denoising of the representations. We describe the details of our asymmetric framework of task-to-basis transferring in the following subsection.    
	
	\subsection{Feature Reconstruction with Nonlinearity}
	There are two main desiderata in our construction of asymmetric feature learning framework.  
	%There are a couple of things to further consider regarding our asymmetric basis learning framework. 
	First, the reconstruction should be achieved in a non-linear manner. Suppose that we perform linear reconstruction of the bases as shown in Figure \ref{prevent_negative_transfer}. In this case, the linear span of $\{\vct{w}_1,\vct{w}_2\}$ does not cover any of $\{\vct{l}_1, \vct{l}_2,\vct{l}_3\}$. Thus we need a nonlinearity to solve the problem. Second, the reconstruction needs to be done in the feature space, not on the bases of the parameters, in order to directly apply it to a deep learning framework. 
	
	We first define notations before introducing our framework. Let $\mat{X}$ be the row-wise concatenation of $\{\mat{X}_1,..,\mat{X}_T\}$. We assume a neural network with a single hidden layer, where $\mat{L}$ and $\mat{S}$ are the parameters for the first and the second layer respectively. As for the nonlinearity in the hidden layer, we use rectified linear unit (ReLU), denoted as $\sigma(\cdot)$. The nonnegative feature matrix is denoted as $\mat{Z} = \sigma(\mat{XL})$, or $\vct{z}_i = \sigma(\mat{X}\vct{l}_i)$ for each column $i=1,..,k$. The task-to-feature transfer matrix is $\mat{A} \in \mathbb{R}^{T \times k}$. Using the above notations, our asymmetric multi-task feature learning framework is defined as follows:
	\begin{align}
	\operatorname*{min.}_{\mat{L},\mat{S}, \mat{A}} & \sum_{t=1}^T \Big\{(1+\alpha ||\vct{a}_t^o||_1)\ \mathcal{L}(\mat{L},\vct{s}_t; \mat{X}_t, \vct{y}_t) + \mu \norm{\vct{s}_t}_1 \Big\} \nonumber\\
	&+ \gamma \|\mat{Z}-\sigma(\mat{Z}\mat{S}\mat{A})\|_{F}^2 + \lambda \norm{\mat{L}}_F^2. \label{eq:amtfl_shallow}
	\end{align}
	The goal of the autoencoder term is to reconstruct features $\mat{Z}$ with model outputs $\mat{ZS}$ with the nonlinear transformation $\sigma(\cdot ; \mat{A})$. 
	%$f$ in the last layer is the nonlinearity for the prediction model (e.g. softmax or logistic function).
	\begin{figure}
		\begin{center}
			\includegraphics[width=5cm]{deep.png}
		\end{center}
		\vspace{-0.1in}
		\caption{\small \textbf{Deep-AMTFL.} The green lines denote feedback connections with $\ell_{2}$ constraints on the features. Different color scales denote different amount of reliabilities (blue) and knowledge transfers from task predictions to features (green).}\label{deep}
		\vspace{-0.1in}
	\end{figure}
	
	\begin{figure*}[t]
		\hfill
		\subfigure[true $\mat{L}$]{\includegraphics[height=3.8cm]{trueL.png}\label{trueL}} 
		\hfill
		\subfigure[true $\mat{LS}$]{\includegraphics[height=3.8cm]{trueW.png}\label{trueW}} 
		\hfill
		\subfigure[$\mat{W}''$]{\raisebox{-.00\height}{\includegraphics[height=3.8cm]{W_amtl.png}}\label{W_amtl}}
		\hfill
		\subfigure[$\mat{L}'$]{\raisebox{-.00\height}{\includegraphics[height=3.8cm]{L_gomtl.png}}\label{L_gomtl}}
		\hfill
		\subfigure[$(\mat{LS})'$]{\raisebox{-.00\height}{\includegraphics[height=3.8cm]{W_gomtl.png}}\label{W_gomtl}} 
		\hfill
		\subfigure[$\mat{L}$]{\raisebox{-.00\height}{\includegraphics[height=3.8cm]{L_amtfl_nolhat.png}}\label{L}} 
		\hfill
		\subfigure[$\mat{LS}$]{\raisebox{-.00\height}{\includegraphics[height=3.8cm]{W_amtfl_nolhat.png}}\label{W}} 
		\hfill
		\subfigure[$\mat{AS}$]{\raisebox{-.00\height}{\includegraphics[width=2.3cm]{AS_amtfl_nolhat.png}}\label{AS}} 
		\hfill
		\vspace{-0.1in}
		\caption{\small \textbf{Visualization of the learned features and paramters on the synthetic dataset.} (a-b) True parameters that generated the data. (c) Reconstructed parameters from AMTL (d-e) Reconstructed parameters from Go-MTL. (f-h) Reconstructed parameters from AMTFL.}
		\vspace{-0.1in}
	\end{figure*}	
	
	We also use ReLU nonlinearity for the reconstruction term as in the original network, since this will allow the reconstruction $\widehat{\mat{Z}}$ to explore the same manifold of $\mat{Z}$, thus making it easier to find an accurate reconstruction. In Fig.\ref{relu_projection}, for example, the linear span of task output vectors $\{\vct{y}_1,\vct{y}_2\}$ forms the blue hyperplane. Transforming this hyperplane with ReLU and a bias term will result in the manifold colored as gray and yellow, which includes the original feature vectors $\{\vct{z}_1,\vct{z}_2,\vct{z}_3\}$.
	
	Since our framework considers the asymmetric transfer in the feature space, we can seamlessly generalize \eqref{eq:amtfl_shallow} to deep networks with multiple layers. Specifically, the auto-encoding regularization term is formulated at the penultimate layer to achieve the asymmetric transfer. We name this approach Deep-AMTFL:
	\begin{equation}
	\begin{aligned}
	&\operatorname*{min.}_{\mat{A},\{\mat{W}^{(l)}\}_{l=1}^L} \sum_{t=1}^T \Big\{(1+\alpha ||\vct{a}_t^o||_1)\ \mathcal{L}_t + \mu\norm{\vct{w}_t^{(L)}}_1\Big\}\\ 
	&+ \gamma \norm{\sigma \big(\mat{Z}\mat{W}^{(L)}\mat{A}\big)-\mat{Z}}_{F}^2 + \lambda \sum_{l=1}^{L-1} \norm{\mat{W}^{(l)}}_F^2,\label{eq:amtfl_deep}
	\end{aligned}
	\end{equation}
	where $\mat{W}^{(l)}$ is the weight matrix for the $l^{th}$ layer, with $\vct{w}_t^{(L)}$ denoting the $t^{th}$ column vector of $\mat{W}^{(L)}$, and
	\begin{equation*}
	\begin{aligned}
	\mat{Z} &\ = \sigma(\mat{W}^{(L-1)} \sigma(\mat{W}^{(L-2)} \dots \sigma(\vct{X}\mat{W}^{(1)}))) \\
	\mathcal{L}_t &:= \mathcal{L}(\vct{w}_t^{(L)}, \mat{W}^{(L-1)},..,\mat{W}^{(1)}; \mat{X}_t,\vct{y}_t),
	\end{aligned}
	\end{equation*}
	are the hidden representations at layer $L-1$ and the loss for each task $t$. See Figure (\ref{deep}) for the description. 
	%We use $\widehat{\mathcal{L}}_t = \epsilon/N_t$ such that $\sum_t \mathcal{L}_t \approx \sum_t \widehat{\mathcal{L}}_t$, to avoid retraining the model twice.
	
	\paragraph{Loss functions}
	The loss function $\mathcal{L}(\vct{w}; \mat{X}, \vct{y})$ could be any generic loss function. We mainly consider the two most popular instances. For regression tasks, we use the squared loss: $\mathcal{L}(\vct{w}_t;\mat{X}_t,\vct{y}_t) = \frac{1}{N_t} \norm{\vct{y}_t-\mat{X}_t\vct{w}_t}_2^2 + \delta/\sqrt{N_t}$.
	For classification tasks, we use the logistic loss: $\mathcal{L}(\vct{w}_t;\mat{X}_t,\vct{y}_t) = \frac{1}{N_t} \sum_{i=1}^{N_t} \{y_{ti} \log{\sigma(\vct{x}_{ti}\vct{w}_t)} + (1-y_{ti})\log{(1-\sigma(\vct{x}_{ti}\vct{w}_t))} \} + \delta/\sqrt{N_t}$, where $\sigma$ is the sigmoid function. Note that we augment the loss terms with $\delta/\sqrt{N_t}$ to express the imbalance for the training instances for each task. %As for $\delta$, we empirically found that $\delta=10^{-3}$ works well in practice.
	As for $\delta$, we roughly tune $\delta/\sqrt{N_t}$ to have similar scale to the first term of $\mathcal{L}(\vct{w}_t;\mat{X}_t, \vct{y}_t)$.
	
	\section{Experiments}
	We validate AMTFL on both synthetic and real datasets against relevant baselines, using both shallow and deep neural networks as base models. 
	
	\subsection{Shallow models - Feedforward Networks}
	We first validate our shallow AMTFL model \eqref{eq:amtfl_shallow} on synthetic and real datasets with shallow neural networks. Note that we use one-vs-all logistic loss for multi-class classification.
	\paragraph{Baselines and our models}
	\ \newline \noindent \textbf{1) STL.} A linear single-task learning model.
	
	\noindent \textbf{2) GO-MTL.} A feature-sharing MTL model from~\cite{go-mtl}, where different task predictors share a common set of latent bases \eqref{eq:go-mtl}.
	
	\noindent \textbf{3) AMTL.} Asymmetric multi-task learning model~\cite{amtl}, with inter-task knowledge transfer through a parameter-based regularization \eqref{eq:amtl}.
	
	\noindent \textbf{4) NN.} A simple feedforward neural network with a single hidden layer.
	
	\noindent \textbf{5) MT-NN.} Same as NN, but with each task loss divided by $N_t$, for balancing the task loss. Note that this model applies $\ell_1$-regularization at the last fully connected layer.
	
	\noindent \textbf{6) AMTFL.} Our asymmetric multi-task feature learning model with feedback connections \eqref{eq:amtfl_shallow}.
	
	\paragraph{Synthetic dataset experiment}
	\begin{figure*}
		\begin{center}
			\hfill
			\subfigure[RMSE]{\raisebox{.04\height}{\includegraphics[height=3.3cm]{rmse_nolhat-eps-converted-to.pdf}}\label{RMSE}}
			\hfill
			\subfigure[RMSE reduction over STL]{\raisebox{-.01\height}{\includegraphics[height=3.3cm]{rmse_reduction_nolhat-eps-converted-to.pdf}}\label{RMSE_reduction}}
			\hfill
			%\leftskip-0.05in
			\subfigure[Scalability]{\raisebox{-.01\height}{\includegraphics[height=3.4cm]{scalability_nolhat-eps-converted-to.pdf}}\label{scalability}}
			%\rightskip-0.1in
			\hfill
			\vspace{-0.15in}
			\caption{\small \textbf{Results of synthetic dataset experiment.} (a) Average RMSE for clean/noisy/all tasks. 
				(a) Per-task RMSE reduction over STL. (b) RMSE and training time for increasing number of tasks.}
			\hfill
			\vspace{-0.25in}
		\end{center}
	\end{figure*}
	
	We first check the validity of AMTFL on a synthetic dataset. We first generate six $30$-dimensional true bases in Figure~\ref{trueL}. Then, we generate parameters for $12$ tasks from them with noise $\epsilon \sim \mathcal{N}(0, \sigma)$. We vary $\sigma$ to create two groups based on the noise level: easy and hard. Easy tasks have noise level of $\sigma=1$ and hard tasks have $\sigma=2$. Each predictor for easy task $\vct{w}_t$ combinatorially picks two out of four bases - $\{\vct{l}_1,..,\vct{l}_4\}$ to linearly combine $\vct{w}_t \in \mathbb{R}^{30}$, while each predictor for hard task selects among $\{\vct{l}_3,..,\vct{l}_6\}$. Thus the bases $\{\vct{l}_3,\vct{l}_4\}$ overlap both easy and hard tasks, while other bases are used exclusive by each group in Figure~\ref{trueW}. We generate five random train/val/test splits for each group - $50/50/100$ for easy tasks and $25/25/100$ for hard tasks.
	
	For this dataset, we implement all base models as neural networks to better compare with AMTFL. We add in $\ell_1$ to $\mat{L}$ for all models for better reconstruction of $\mat{L}$. We remove ReLU at the hidden layer in AMTFL since the features are linear\footnote{We avoid adding nonlinearity to features to make qualitative analysis much easier.}. All the hyper-parameters are found with separate validation set. For AMTL, we remove the nonnegative constraint on $\mat{B}$ due to the characteristic of this dataset.
	
	We first check whether AMTFL can accurately reconstruct the true bases in Figure~\ref{trueL}. We observe that $\mat{L}$ learned by AMTFL in Figure~\ref{L} more closely resembles the true bases than $\mat{L}'$ reconstructed using Go-MTL in Figure~\ref{L_gomtl}), which is more noisy. The reconstructed $\mat{W}=\mat{L}\mat{S}$ from AMTFL in Figure~\ref{W}, in turn, is closer to the true parameters than $\mat{W}'=(\mat{L}\mat{S})'$ generated with Go-MTL in Figure~\ref{W_gomtl} and $\mat{W}''$ from AMTL in Figure~\ref{W_amtl} for both easy and hard tasks. Further analysis of the inter-task transfer matrix $\mat{A}\mat{S}$ in Figure~\ref{AS} reveals that this accurate reconstruction is due to the asymmetric inter-task transfer, as it shows no transfer from hard to easy tasks, while we see significant amount of transfers from easy to hard tasks.
	
	Quantitative evaluation result in Figure~\ref{RMSE} further shows that AMTFL significantly outperforms existing MTL methods. 
	AMTFL obtains lower errors on both easy and hard tasks to STL, while Go-MTL results in even higher errors than those obtained by STL on hard tasks. We attribute this result to the negative transfer from other hard tasks. AMTFL also outperforms AMTL by significant margin, maybe because it is hard for AMTL to find meaningful relation between tasks in case of this particular synthetic dataset, where data for each task is assumed to be generated from the same set of latent bases. Also, a closer look at the per-task error reduction over STL in Figure~\ref{RMSE_reduction} shows that AMTFL effectively prevents negative transfer while GO-MTL suffers from negative transfer, and make larger improvements than AMTL. Further, Figure~\ref{scalability} shows that AMTFL is more scalable than AMTL, both in terms of error reduction and training time, especially when we have large number of tasks. One thing to note is that, for AMTFL, the error goes down as the number of tasks increases. This is a reasonable result, since the feature reconstruction using the task-specific model parameters will become increasingly accurate with larger number of tasks. %The detailed experimental setup is in the supplementary file.
	
	\paragraph{Real dataset experiment}	
	We further test our model on one binary classification, one regression, and two multi-class classification datasets, which are the ones used for experiments in~\cite{go-mtl,amtl}. We report averaged performance of each model on five random splits for all datasets. 
	
	\noindent \textbf{1) AWA-A:} This is a classification dataset \cite{lampert-attributes} that consists of $30,475$ images, where the task is to predict \textit{85 binary attributes} for each image describing a single animal. The feature dimension is reduced from $4096$ (Decaf) to $500$ by using PCA. The number of instances for train, validation, and test set for each task is $1080$, $60$, and $60$, respectively. We set the number of hidden neurons to $1000$ which is tuned on the base NN.
	
	\noindent \textbf{2) MNIST:} This is a standard dataset for classification~\cite{mnist} that consists of $60,000$ training and $10,000$ test images of $28\times28$ that describe $10$ handwritten digits (0-9). Following the procedure of~\cite{go-mtl}, feature dimension is reduced to $64$ by using PCA, and 5 random 100/50/50 splits are used for each train/val/test. We set the number of hidden neurons to $500$.
	
	\noindent \textbf{3) School:} This is a regression dataset where the task is to predict the exam scores of $15,362$ student's from $139$ schools. Prediction of the exam score for each school is considered as a single task. The splits used are from \cite{argyriou08} and we use the first 5 splits among 10. We set the number of hidden neurons to $10$ or $15$.
	
	\noindent \textbf{4) Room:} This is a subset of the ImageNet dataset~\cite{imagenet} from~\cite{amtl}, where the task is to classify $14,140$ images of $20$ different indoor scene classes. The number of train/val instances varies from $30$ to over $1000$, while test set has $20$ per each class. The feature dimension is reduced from $4096$ (Decaf) to $500$ by using PCA. We set the number of hidden neurons to $1000$.
	
	Table 1 shows the results on the real datasets. As expected, the AMTFL (\ref{eq:amtfl_shallow}) outperforms the baselines on most datasets. The only exception is the School dataset, on which GO-MTL obtains the best performance, but as mentioned in~\cite{amtl} this is due to the strong homogeneity among the tasks in this particular dataset.
	
	\subsection{Deep models - Convolutional Networks}
	Next, we validate our Deep-AMTFL (\ref{eq:amtfl_deep}) using CNN as base networks for end-to-end image classification. Note that we use one-vs-all classifier instead of softmax, since we want to consider the classification of each class as a separate task. 

	\paragraph{Baselines and our models}
	\vspace{-0.05in}
	\ \newline\newline \noindent \textbf{1) CNN:} The base convolutional neural network.
	
	\noindent \textbf{2) MT-CNN:} The base CNN with $\ell_1$-regularization on the parameters for the last fully connected layer $\mat{W}^{(L)}$ instead of $\ell_2$-regularization, similarly to \eqref{eq:amtfl_deep}. %Also, for the imbalanced dataset, loss for each class $t$ is divided by $N_t$, the number of positive instance for each class.
	
	\noindent \textbf{3) Deep-AMTL:} Base CNN with the asymmetric multi-task learning objective in~\cite{amtl} replacing the original loss. Note that the model is deep version of \eqref{eq:amtl_plus_gomtl}, where $\mat{LS}$ corresponds to the last fully connected layer $\mat{W}^{(L)}$.
	
	%\noindent \textbf{4) Deep-SMTFL:} The same as Deep-AMTFL, but we fix $\alpha=0$ in \eqref{eq:amtfl_deep}, so that the transfers are symmetric.
	
	\noindent \textbf{4) Deep-AMTFL:} Our deep asymmetric multi-task feature learning model with the asymmetric autoencoder based on task loss. \eqref{eq:amtfl_deep}.
	
		\begin{table}[t]
		\vspace{-0.07in}
		\caption{\small Performance of the linear and shallow baselines and our asymmetric multi-task feature learning model. We report the RMSE for regression and mean classification error(\%) for classification, along with the standard error for 95\% confidence interval. 
		%Performance of AMTL on AWA-A is not available, since multi-label classification is not implemented in provided codes.
		}\label{maintable}
		\small
		\vspace{-0.07in}
		\begin{center}
			\resizebox{0.49\textwidth}{!}{
				\begin{tabular}{cccccccc}
					& AWA-A & MNIST & School & Room \\
					\hline
					\hline
					STL    & 37.6$\pm$0.5 & 14.8$\pm$0.6 & 10.16$\pm$0.08 & 45.9$\pm$1.4 \\
					GO-MTL & 35.6$\pm$0.2 & 14.4$\pm$1.3 & \bf9.87$\pm$0.06 & 47.1$\pm$1.4 \\
					AMTL   & 33.4$\pm$0.3 & 12.9$\pm$1.4 & 10.13$\pm$0.08& 40.8$\pm$1.5 \\
					\hline
					NN  & 26.3$\pm$0.3 & 8.96$\pm$0.9 & 9.89$\pm$0.03 & 44.5$\pm$2.0 \\
					MT-NN  & 26.2$\pm$0.3& 8.76$\pm$1.0 & 9.91$\pm$0.04 & 41.7$\pm$1.7 \\
					AMTFL & \bf 25.2$\pm$0.3 & \bf 8.68$\pm$0.9& 9.89$\pm$0.09 & \bf  40.4$\pm$2.4 \\
					\hline
				\end{tabular}
			}
		\end{center}
		\vspace{-0.1in}
	\end{table}
	
	\begin{table}[t]
		\small
		\begin{center}
			\caption{\small Classification performance of the deep learning baselines and Deep-AMTFL. The reported numbers for MNIST-Imbalanced and CUB datasets are averages over $5$ runs.}\label{table:deep}
			\resizebox{0.49\textwidth}{!}{
			\begin{tabular}{c c c c c c}
				\small
				& MNIST-Imbal. & CUB & AWA-C & Small \\
				\hline
				\hline
				CNN & $8.13$ &$46.18$& $33.36$ & $66.54$ \\
				\hline
				MT-CNN & $8.72$ &$43.92$& $32.80$ & $65.69$ \\
				Deep-AMTL & $8.52$ &$45.26$& $32.32$ & $65.61$ \\
				%Deep-SMTFL &$6.78\pm 0.28$ & $6.49\pm 0.49$   &$43.85\pm 0.26$& $32.12$ & N/A \\
				Deep-AMTFL & $\bf 5.82$  &$\bf43.75$& $\bf 31.88$ & $\bf64.49$ \\
				\hline
			\end{tabular}
		}
		\end{center}
		\vspace{-0.15in}
	\end{table}
	
	\paragraph{Datasets and base networks}
	%\ \newline \noindent \textbf{1) MNIST-1K:} This is the same MNIST dataset used in the shallow model experiments, but we used raw-pixel images for end-to-end training. Among $60,000$ training instances, we randomly sample $1000$ as a training set and set the rest as a validation set. The number of test instances is $10,000$. We used Lenet-Conv as the base network.
	
	\ \newline\newline \noindent \textbf{1) MNIST-Imbalanced:} This is an imbalanced version of MNIST dataset. Among $6,000$ training samples for each class $0, 1, \dots, 9$, we used $200, 180, \dots, 20$ samples for training respectively, and the rest for validation. For test, we use $1,000$ instances per class as with the standard MNIST dataset. As the base network, we used Lenet-Conv.
	%\noindent \textbf{3) CIFAR-100:} This dataset~\cite{cifar} consists of $60,000$ images from $100$ object categories. The predefined train/test split is $500$/$100$ per class. We used ResNet-34~\cite{resnet} for the base network.
	%, with $28$ layers and the widening factor of $10$`
	%\footnote{We extensively tuned the WRN to reproduce the results in~\cite{wrn}, but could not reproduce it; this incident have been reported by various researchers.}.
	
	\noindent \textbf{2) CUB-200:} This dataset consists of images describing 200 bird classes including \emph{Cardinal}, \emph{Tree Sparrow}, and \emph{Gray Catbird}, of which $5,994$ images are used for training and $5,794$ are used for test. As for the base network, we used ResNet-18~\cite{resnet}.
	
	\noindent \textbf{3) AWA-C:} This is the same AWA dataset used in the shallow model experiments, but we used the class labels for $50$ animals instead of binary attribute labels. Among $30,475$ images, we randomly sampled $50$ instances per each class to use as the test set, and used the rest of them for training, which results in an imbalanced training set ($42$-$1118$ samples per class). As with the CUB dataset, we used ResNet-18 as the base network. %We fined-tune ResNet~\cite{resnet} with $50$ layers trained on ImageNet-1K~\cite{imagenet} dataset for the base network.
	
	\noindent \textbf{4) ImageNet-Small:} This is a subset of the ImageNet 22K dataset~\cite{imagenet}, which contains $352$ classes. We deliberately created the dataset to be largely imbalanced, with the number of images per class ranging from $2$ to $1,044$. We used ResNet-50 for the base network.
	
	\paragraph{Experimental Setup}
	For the implementation of all the baselines and our deep models, we use Tensorflow~\cite{tensorflow} and Caffe~\cite{caffe} framework. For AWA-C and Small datasets, we first train Base model from scratch, and finetune the the other models based on it for expedited training. We trained from scratch for other datasets. Note that we simply use the weight decay $\lambda$ provided by the code of the base networks, and set $\mu=\lambda$ to reduce the effective number of hyperparameters to tune. We searched for $\alpha$ and $\gamma$ in the range of $\{1, 0.1, 10^{-2}, 10^{-3},10^{-4}\}$. For CUB dataset, we gradually increase $\alpha$ and $\gamma$ from $0$, which helps with stability of learning at the initial stage of the training.
	
	\paragraph{Quantitative evaluation}
	We report the average per-class classification performances of baselines and our models in Table \ref{table:deep}. Our Deep-AMTFL outperforms all baselines, including MT-CNN and Deep-AMTL, which shows the effectiveness of our asymmetric knowledge transfer from tasks to features, and back to tasks in deep learning frameworks.
	
	To see where the performance improvement comes from, we further examine the per-task (class) performance improvement of baselines and our Deep-AMTFL over the base CNN on MNIST-Imbalanced dataset along with average per-task loss (Figure \ref{pertask}). We see that MT-CNN improves the performance over CNN on half of the tasks (5 out of 10) while degenerating performance on the remainders. Deep-AMTL obtains larger performance gains on later tasks with large loss (task 8 and 9) due to its asymmetric inter-task knowledge transfer, but still suffers from performance degradation (task 6 and 7). Our Deep-AMTFL, on the other hand, does not suffer from accuracy loss on any tasks and shows significantly improved performances on all tasks, especially on the tasks with large loss (task 9). This result suggests that the performance gain mostly comes from the suppression of negative transfer. %The classes with large performance degeneration, \emph{cow} and \emph{hamster}, are the ones with large training loss; this is a somewhat expected result since the proxy term $\delta/\sqrt{N_t}$, which may be unreliable at times as the measure of true risk.
%	\begin{figure}
%		\includegraphics[width=8cm]{awa_pertask.pdf}
%		\vspace{-0.1in}
%		\caption{\small Per-class performance improvements on AWA dataset. }\label{fig:perclass}
%		\vskip -0.1in
%	\end{figure}
	

	\begin{figure}
		\vspace{-0.1in}
		\begin{center}
			\subfigure[Per-task Improvements]{\includegraphics[height=3.4cm]{new1.pdf}\label{pertask}}
			\hfill
			\subfigure[Inter-task Transfer]{\raisebox{.12\height}{\includegraphics[height=3.2cm]{AS_new.png}}\label{AS_MNIST}}
		\vspace{-0.15in}
			\caption{\small \textbf{Results of experiments on the MNIST-Imbalanced dataset.} (a) Accuracy improvements over the CNN and the per-task losses. (b) The inter-task transfer matrkx $\mat{AS}$.  We remove the sign of values for better visualization.}
			\vspace{-0.23in}
		\end{center}
	\end{figure}
	
	\paragraph{Quanlitative analysis}
	As further qualitative analysis, we examine how inter-task knowledge transfer is done in Deep-AMTFL in Figure~\ref{AS_MNIST}. Although Deep-AMTFL does not explicitly model inter-task knowledge transfer graph, we can obtain one by computing $\mat{AS}$, as in Figure~\ref{AS_MNIST}. We see that each task transfers to later tasks (upper triangular submatrix) that comes with fewer training instances but does not receive knowledge transfer from them, which demonstrates that Deep-AMTFL is performing asymmetric knowledge transfer in correct directions implicitly via the latent feature space. The only exception is the tansfer from task $5$ to task $2$, which is reasonable since they have similar losses (Figure~\ref{pertask}). 

	\section{Conclusion}
	We propose a novel deep asymmetric multi-task feature learning framework that can effectively prevent negative transfer resulting from symmetric influences of each task in feature learning. By introducing an asymmetric feedback connections in the form of autoencoder, our AMTFL enforces the participating task predictors to asymmetrically affect the learning of shared representations based on task loss. We perform extensive experimental evaluation of our model on various types of tasks on multiple public datasets. The experimental results show that our model significantly outperforms both the symmetric multi-task feature learning and asymmetric multi-task learning based on inter-task knowledge transfer, for both shallow and deep frameworks.
	
	\section*{Acknowledgements} This research was supported by Samsung Research Funding Center of
Samsung Electronics (SRFC-IT150203), Basic Science Research Program through the National Research Foundation of Korea (NRF) funded by the Ministry of Education (2015R1D1A1A01061019), a Machine Learning and Statistical Inference Framework for Explainable Artificial Intelligence (No.2017-0-01779) and Development of Autonomous IoT Collaboration Framework for Space Intelligence (2017-0-00537) supervised by the IITP(Institute for Information \& communications Technology Promotion).
	
		\bibliographystyle{icml2018}
		\bibliography{refs,strings}
		\appendix
		
		\section{Application to Transfer Learning}	
		\vspace{-0.15in}
		\begin{table}[h]
			\small
			\caption{\small Classification error(\%) of the baselines and our model on the transfer learning task. Source networks denote types of networks that is trained on the source dataset with $40$ classes, and Target accuracy is the accuracy of the softmax classifier on $10$ target classes trained on the representations obtained at the layer just below the softmax layer of the source network.}
			\label{awa_transfer}
			\begin{center}
				\begin{tabular}{c c}
					Source Network & Target Accuracy\\
					\hline
					\hline
					CNN & 5.00 \\
					Deep-AMTL & 5.00 \\
					\hline
					Deep-AMTFL & \bf 4.33 \\
				\end{tabular}
			\end{center}
			\vspace{-0.15in}
		\end{table}
		
		For this experiment, we use the AWA dataset, which is a standard dataset for transfer learning that provides source/target task class split. The source dataset contains $40$ animal classes including \emph{grizzly bear}, \emph{hamster}, \emph{blue whale}, and \emph{tiger}, and the target dataset contains $10$ animal classes, including \emph{giant panda}, \emph{rat}, \emph{humpback whale}, and \emph{leopard}. Thus the tasks in two datasets exhibit large degree of relatedness. We train baseline networks and our Deep-AMTFL model on the source dataset, and trained the last fully connected layer of the original network while maintaining all other layers to be fixed, for the classification of the target dataset. 
		
		\section{Other Baselines}
		In the below table, we show the performances of the two recently proposed multi-task learning models~\cite{dmtrl,tnrdmtl} on two datasets used in the shallow model experiments. The results show that our AMTFL significantly ouperforms those models.
		\begin{table}[h]
			\small
			\label{tabel:other_baselines}
			\begin{center}
				\begin{tabular}{cccccc}
					& MNIST & Room \\
					\hline
					\hline
					DMTRL   & 11.9 $\pm$ 0.8 & 47.2 $\pm$ 2.9 \\
					TNRDMTL & 11.0 $\pm$ 1.3 & 49.4 $\pm$ 1.4  \\			
					\hline
					AMTFL & \bf8.68$\pm$0.9& \bf 40.4 $\pm$ 2.4 \\
					\hline
				\end{tabular}
			\end{center}
			\vskip -0.1in
		\end{table}
		
		\section{Experimental Setup}
		
		\paragraph {Synthetic dataset experiment}
		All the hyperparameters are found with separate validation sets. For the latent bases models (Go-MTL, AMTFL), we use one hidden layer with six neurons, while other models (STL and AMTL) do not have any hidden layer. The base learning rate is $0.1$, and is multiplied by $0.2$ every $500$ iterations. The batch size is set as $100$. The total number of iterations is $2500$. RMSProp is used for the latent bases models (Go-MTL, AMTFL), and SGD is used for the other models (STL, AMTL), which has been empirically found to be optimal for each model. Weight decay is set as $0.02$. The weights are initialized with gaussian distribution with 0.01 standard deviation. For Go-MTL, the sparsity for $\mat{L}$ is $0.3$ and $\mu$ is $0.2$. For AMTL, $\lambda$ and $\mu$ are set as $0.3$ and $0.0001$ each. For AMTFL, the sparsity for $\mat{L}$ is $0.5$, $\mu$ is $0.3$, $\alpha$ is $0.2$, and $\gamma$ is $0.003$.
		
		\vspace{-0.1in}
		\paragraph {Real dataset experiment (shallow models)}
		Here we mention a few important settings for the experiments. The base learning rate varying from $10^{-1}$ to $10^{-4}$, and stepwisely decreases when training loss saturates. Batch size also varies from $10^2$ to $10^3$, which is jointly controlled with learning rate. The number of hidden neurons is set via cross validation, along with other hyperparameters. The weights are initialized with zero-mean gaussian with $0.01$ stddev.
		
		\vspace{-0.1in}
		\paragraph {Real dataset experiment (deep models)}
		For \textbf{MNIST-Imbalanced} dataset, we ran total $200$ epochs with batchsize $100$. We used the Adam~\cite{adam} optimizer, with the learning rate starts from $10^{-4}$ and is multiplied by $0.1$ after $100$ epochs. We set $\lambda = \mu = 10^{-4}$, $\alpha=0.1$, and $\gamma=0.01$.
		%For Cifar-100, the base network is WRN28-10.  The base learning rate is set 0.02, and multiplied by 0.2 at every 10000 iteration over total 30000 iterations. $\gamma$ and $\alpha$ are set 0.1. 
		For \textbf{CUB} dataset, we ran total $400$ epochs with batchsize $125$. We used SGD optimizer with $0.9$ momentum. Learning rate starts from $10^{-2}$ and is multiplied by $0.1$ after $200$ and $300$ epochs. We set $\lambda = \mu = 10^{-3}$, $\alpha=1$ and $\gamma=10^{-3}$.
		For \textbf{AWA-C} dataset, we ran total $300$ epochs with batchsize $125$. We used SGD
		optimizer with $0.9$ momentum. Learning rate starts from $10^{-2}$, and is multiplied by $0.1$ at $150$ and $250$ epochs. We set $\lambda = \mu = 10^{-4}$, $\alpha=0.1$ and $\gamma=10^{-4}$.
		For \textbf{ImageNet-Small} dataset, we ran total $40,000$ iterations with batchsize $30$. The base learning rate is $10^{-4}$ and multiplied by $0.1$ at every $4,500$ iteration. 
		%For AwA dataset, the base network is ResNet-50. The base learning rate is $10^{-4}$ and multiplied by $0.1$ at every 20000 iteration. The batch size is $30$, and the total number of iteration is 40000. $\gamma$ and $\alpha$ are set $0.0003$.

	
\end{document}