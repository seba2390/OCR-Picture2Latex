%%%%%%%% ICML 2018 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2018} with \usepackage[nohyperref]{icml2018} above.
\usepackage{hyperref}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
%\usepackage{icml2018}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2018}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Deep Asymmetric Muti-task Feature Learning}

%%% load AMS-Latex Package
\usepackage{amsmath,amsfonts}
% \usepackage{amsthm}
\usepackage{amssymb,amsopn}
\usepackage{bm} % bold symbol
\usepackage{bbm}

% define fonts
\newcommand{\vct}[1]{\boldsymbol{#1}} % vector
\newcommand{\mat}[1]{\boldsymbol{#1}} % matrix
\newcommand{\vu}{\vct{u}}

%%%% Special math symbols
\newcommand{\field}[1]{\mathbb{#1}}
\newcommand{\R}{\field{R}} % real domain
%\newcommand{\C}{\field{C}} % complex domain
\newcommand{\F}{\field{F}} % functional domain
%\newcommand{\T}{^{\top}\!\!} % transpose
\newcommand{\T}{^{\textrm T}} % transpose
\newcommand{\TN}{^{-\textrm T}} % transpose


%%% define constant
\newcommand{\cst}[1]{\mathsf{#1}}

%% operator in linear algebra, functional analysis
\newcommand{\inner}[2]{#1\cdot #2}
\newcommand{\norm}[1]{\left\|#1\right\|}
\newcommand{\twonorm}[1]{\|#1\|_2^2}
% operator in functios, maps such as M: domain1 --> domain 2
\newcommand{\Map}[1]{\mathcal{#1}}

% operator in probability: expectation, covariance,
\newcommand{\ProbOpr}[1]{\mathbb{#1}}
% independence
\newcommand\independent{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}
% conditional independence
\newcommand{\cind}[3]{{#1} \independent{#2}\,|\,#3}
% conditional expectation
\newcommand{\cndexp}[2]{\ProbOpr{E}\,[ #1\,|\,#2\,]}

\def\D{\mathcal{D}}
\def\loss{\mathcal{L}}
\def\R{\mathcal{R}}
\def\reals{\mathbb{R}}
\def\S{\mathcal{S}}

\def\U{\mathcal{U}}
\def\T{\mathcal{T}}

\newcommand\innerP[2]{\langle #1, \, #2 \rangle}
%\newcommand\biginner[2]{\big\langle #1, \, #2 \big\rangle}


% operator in optimization
%\DeclareMathOperator{\argmax}{arg\,max}
%\DeclareMathOperator{\argmin}{arg\,min}
\newcommand{\todo}[1]{{\color{red}#1}}

\DeclareMathOperator*{\minimize}{minimize}
\DeclareMathOperator*{\st}{s.t.}
\newcommand{\argmin}{\operatornamewithlimits{argmin}}

\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{remarks}{Remark}
\newtheorem{example}{Example}

\newlength{\widebarargwidth}
\newlength{\widebarargheight}
\newlength{\widebarargdepth}
\DeclareRobustCommand{\widebar}[1]{%
	\settowidth{\widebarargwidth}{\ensuremath{#1}}%
	\settoheight{\widebarargheight}{\ensuremath{#1}}%
	\settodepth{\widebarargdepth}{\ensuremath{#1}}%
	\addtolength{\widebarargwidth}{-0.3\widebarargheight}%
	\addtolength{\widebarargwidth}{-0.3\widebarargdepth}%
	\makebox[0pt][l]{\hspace{0.15\widebarargheight}%
		\hspace{0.3\widebarargdepth}%
		\addtolength{\widebarargheight}{0.3ex}%
		\rule[\widebarargheight]{0.95\widebarargwidth}{0.1ex}}%
	{#1}}

\newcommand{\eat}[1]{}

\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}

\newcommand{\sj}[1]{{\color{red}{\small\bf\sf [Sungju: #1]}}}
\newcommand{\eh}[1]{{\color{magenta}{\small\bf\sf [Eunho: #1]}}}
\newcommand{\hb}[1]{{\color{blue}{\small\bf\sf [Haebeom: #1]}}}

\newcommand{\by}{\mathbf{y}}
\newcommand{\bh}{\mathbf{h}}
\newcommand{\bo}{\mathbf{o}}
\newcommand{\bW}{\mathbf{W}}
\newcommand{\bb}{\mathbf{b}}
\newcommand{\ent}{\mathcal{H}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\real}{\mathbb{R}} 
% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{wrapfig}

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2018} with \usepackage[nohyperref]{icml2018} above.
\usepackage{hyperref}

% Attempt to make hyperref and algorithmic work together better:

% Use the following line for the initial blind version submitted for review:
%\usepackage{icml2018}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2018}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Deep Asymmetric Multi-task Feature Learning}

\begin{document}
	
	\twocolumn[
	\icmltitle{Supplementary File for \\
		 Deep Asymmetric Multi-task Feature Learning}
	
	% It is OKAY to include author information, even for blind
	% submissions: the style file will automatically remove it for you
	% unless you've provided the [accepted] option to the icml2018
	% package.
	
	% List of affiliations: The first argument should be a (short)
	% identifier you will use later to specify author affiliations
	% Academic affiliations should list Department, University, City, Region, Country
	% Industry affiliations should list Company, City, Region, Country
	
	% You can specify symbols, otherwise they are numbered in order.
	% Ideally, you should not use this facility. Affiliations will be numbered
	% in order of appearance and this is the preferred way.
	\icmlsetsymbol{equal}{*}
	
	\begin{icmlauthorlist}
	\icmlauthor{Hae Beom Lee}{to}
	\icmlauthor{Eunho Yang}{goo}
	\icmlauthor{Sung Ju Hwang}{goo}
\end{icmlauthorlist}

\icmlaffiliation{to}{UNIST, Ulsan, South Korea}
\icmlaffiliation{goo}{KAIST, Daejeon, South Korea}

\icmlcorrespondingauthor{Hae Beom Lee}{hblee@unist.ac.kr}
\icmlcorrespondingauthor{Eunho Yang}{eunhoy@kaist.ac.kr}
\icmlcorrespondingauthor{Sung Ju Hwang}{sjhwang82@kaist.ac.kr}
	
	% You may provide any keywords that you
	% find helpful for describing your paper; these are used to populate
	% the "keywords" metadata in the PDF but will not be shown in the document
	\icmlkeywords{Machine Learning, ICML}
	
	\vskip 0.3in
	]
	
	% this must go after the closing bracket ] following \twocolumn[ ...
	
	% This command actually creates the footnote in the first column
	% listing the affiliations and the copyright notice.
	% The command takes one argument, which is text to display at the start of the footnote.
	% The \icmlEqualContribution command is standard text for equal contribution.
	% Remove it (just {}) if you do not need this facility.
	
	%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
	\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.
	
\section{Application to Transfer Learning}	
For this experiment, we use the AWA dataset, which is a standard dataset for transfer learning that provides source/target task class split. The source dataset contains $40$ animal classes including \emph{grizzly bear}, \emph{hamster}, \emph{blue whale}, and \emph{tiger}, and the target dataset contains $10$ animal classes, including \emph{giant panda}, \emph{rat}, \emph{humpback whale}, and \emph{leopard}. Thus the tasks in two datasets exhibit large degree of relatedness. We train baseline networks and our Deep-AMTFL model on the source dataset, and trained the last fully connected layer of the original network while maintaining all other layers to be fixed, for the classification of the target dataset. 

\begin{table}[h]
	\small
	\vspace{-0.15in}
	\caption{\small Classification error(\%) of the baselines and our model on the transfer learning task. Source networks denote types of networks that is trained on the source dataset with $40$ classes, and Target accuracy is the accuracy of the softmax classifier on $10$ target classes trained on the representations obtained at the layer just below the softmax layer of the source network.}
	\label{awa_transfer}
	\begin{center}
		\begin{tabular}{c c}
			Source Network & Target Accuracy\\
			\hline
			\hline
			CNN & 5.00 \\
			Deep-AMTL & 5.00 \\
			\hline
			Deep-AMTFL & \bf 4.33 \\
		\end{tabular}
	\end{center}
	\vskip -0.1in
	\vspace{-0.15in}
\end{table}

\section{Experimental Setup}

\paragraph {Synthetic dataset experiment}
All the hyperparameters are found with separate validation sets. For the latent bases models (Go-MTL, AMTFL), we use one hidden layer with six neurons, while other models (STL and AMTL) do not have any hidden layer. The base learning rate is $0.1$, and is multiplied by $0.2$ every $500$ iterations. The batch size is set as $100$. The total number of iterations is $2500$. RMSProp is used for the latent bases models (Go-MTL, AMTFL), and SGD is used for the other models (STL, AMTL), which has been empirically found to be optimal for each model. Weight decay is set as $0.02$. The weights are initialized with gaussian distribution with 0.01 standard deviation. For Go-MTL, the sparsity for $\mat{L}$ is $0.3$ and $\mu$ is $0.2$. For AMTL, $\lambda$ and $\mu$ are set as $0.3$ and $0.0001$ each. For AMTFL, the sparsity for $\mat{L}$ is $0.5$, $\mu$ is $0.3$, $\alpha$ is $0.2$, and $\gamma$ is $0.003$.

\vspace{-0.1in}
\paragraph {Real dataset experiment (shallow models)}
Here we mention a few important settings for the experiments. The base learning rate varying from $10^{-1}$ to $10^{-4}$, and stepwisely decreases when training loss saturates. Batch size also varies from $10^2$ to $10^3$, which is jointly controlled with learning rate. The number of hidden neurons is set via cross validation, along with other hyperparameters. The weights are initialized with zero-mean gaussian with $0.01$ stddev.

\vspace{-0.1in}
\paragraph {Real dataset experiment (deep models)}
For \textbf{MNIST-Imbalanced} dataset, we ran total $200$ epochs with batchsize $100$. We used the Adam~\cite{adam} optimizer, with the learning rate starts from $10^{-4}$ and is multiplied by $0.1$ after $100$ epochs. We set $\lambda = \mu = 10^{-4}$, $\alpha=0.1$, and $\gamma=0.01$.
%For Cifar-100, the base network is WRN28-10.  The base learning rate is set 0.02, and multiplied by 0.2 at every 10000 iteration over total 30000 iterations. $\gamma$ and $\alpha$ are set 0.1. 
For \textbf{CUB} dataset, we ran total $400$ epochs with batchsize $125$. We used SGD optimizer with $0.9$ momentum. Learning rate starts from $10^{-2}$ and is multiplied by $0.1$ after $200$ and $300$ epochs. We set $\lambda = \mu = 10^{-3}$, $\alpha=1$ and $\gamma=10^{-3}$.
For \textbf{AWA-C} dataset, we ran total $300$ epochs with batchsize $125$. We used SGD
optimizer with $0.9$ momentum. Learning rate starts from $10^{-2}$, and is multiplied by $0.1$ at $150$ and $250$ epochs. We set $\lambda = \mu = 10^{-4}$, $\alpha=0.1$ and $\gamma=10^{-4}$.
For \textbf{ImageNet-Small} dataset, we ran total $40,000$ iterations with batchsize $30$. The base learning rate is $10^{-4}$ and multiplied by $0.1$ at every $4,500$ iteration. 
%For AwA dataset, the base network is ResNet-50. The base learning rate is $10^{-4}$ and multiplied by $0.1$ at every 20000 iteration. The batch size is $30$, and the total number of iteration is 40000. $\gamma$ and $\alpha$ are set $0.0003$.

\section{Other Baselines}
In the below table, we show the performances of the two recently proposed multi-task learning models~\cite{dmtrl,tnrdmtl} on two datasets used in the shallow model experiments. The results show that our AMTFL significantly ouperforms those models.
\begin{table}[h]
	\small
	\vspace{-0.15in}
	\label{tabel:other_baselines}
	\begin{center}
		\begin{tabular}{cccccc}
			& MNIST & Room \\
			\hline
			\hline
			DMTRL   & 11.9 $\pm$ 0.8 & 47.2 $\pm$ 2.9 \\
			TNRDMTL & 11.0 $\pm$ 1.3 & 49.4 $\pm$ 1.4  \\			
			\hline
			AMTFL & \bf8.68$\pm$0.9& \bf 40.4 $\pm$ 2.4 \\
			\hline
		\end{tabular}
	\end{center}
	\vskip -0.1in
	\vspace{-0.15in}
\end{table}

%\begin{table}[t]\label{tabel:other_baselines}
%	\small
%	\begin{center}
%		\begin{tabular}{cccccc}
%			& MNIST & Room \\
%			\hline
%			\hline
%			DMTRL   & 11.9 $\pm$ 0.8 & 47.2 $\pm$ 2.9 \\
%			TNRDMTL & 11.0 $\pm$ 1.3 & 49.4 $\pm$ 1.4  \\			
%			\hline
%			AMTFL & \bf8.52$\pm$1.1& \bf 40.1 $\pm$ 1.9 \\
%			\hline
%		\end{tabular}
%	\end{center}
%\vspace{-0.2in}
%\end{table}
\bibliographystyle{icml2018}
\bibliography{refs,strings}



\end{document}
