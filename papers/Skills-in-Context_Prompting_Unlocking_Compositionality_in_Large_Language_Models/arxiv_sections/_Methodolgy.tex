





\section{Methodology}
\label{sec: methodology}














Recent progress in LLMs has demonstrated strong capabilities in solving various NLP tasks \citep{brown2020language, Radford2019LanguageMA, smith2022using, chowdhery2022palm, lewkowycz2022solving, sanh2021multitask, wei2021finetuned, mishra2022cross, chung2022scaling, ouyang2022training}. However, they usually suffer from generalizing to more complex problems that require LLMs to compose different capabilities \citep{zhou2022least} from the examples they have seen, i.e., compositional generalization or ``easy-to-hard'' generalization. This discrepancy is mainly due to the lack of the ability to compose \textit{basic skills} in innovative ways to solve more difficult problems \citep{dziri2023faith}, which is natural for humans in problem solving. Empowering language models with the ability to compose the skills that they have seen to solve more complex tasks is important to mirror human intelligence and to reach superintelligence. To this end, this work introduces a novel prompting strategy, Skills-in-Context (SKiC) prompting, to teach language models composing elementary skills to solve problems for better compositional generalization.









\subsection{Skills-in-Context Prompting}
Skills-in-context prompting facilitates compositional generalization by explicitly instructing language models to utilize basic skills to solve complex problems.\footnote{The term ``basic skills'' within SKiC prompting are not necessarily atomic skills. Rather, they could be any skills (e.g., a composite skill by itself) that serve as the foundational blocks for tackling more complex problems.} A SKiC prompt consists of three major parts: \textbf{(i)} Characterization of the basic skills that are needed to solve complex problems, including the description of the skills and the instruction on how to use them (with few-shot examplars). \textbf{(ii)} Examples of how to compose basic skills into solutions to complex problems. \textbf{(iii)} The problem to be solved. An example is shown in Figure~\ref{fig:example}. The language model is first provided with several basic skills such as getting the last letter of one word followed by several examples introduced to illustrate the process of utilizing these basic skills to answer the complex problem. For example, to take the last letter of a series of words, language models need to use the ``words\_to\_list'' skill to first add the asked words to a list and then use the ``last\_letter'' skill to iteratively obtain the last letter of each word. 




\begin{figure*}[t]
\begin{center}
\centerline{\includegraphics[width=1.0\columnwidth]{figures/example_2.png}}
\caption{Skills-in-Context Prompting. The prompt consists of three building blocks: (i) the (basic) skills for solving a complex task, (ii) (few-shot) examples of how to compose the skills to solve the complex problems, and (iii) the problem to be solved. The few-shot demonstration examples for the skills are omitted above for brevity. The above prompt will be fed into an LLM to generate the output --- see Figure \ref{Tab:example_last_letter_skill} for an example of the output. Note that the compositional examplars demonstrate how to ground the reasoning steps onto the basic skills (highlighted in colors).} \label{fig:example}
\end{center}
\vskip -0.2in
\end{figure*}





\begin{figure*}[t]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.9\columnwidth]{figures/method.png}}
\caption{The building blocks of different prompting strategies. Blue cells stand for different intermediate steps, green cells denote the answers to the asked question, and red cells refer to the provided skills in our Skills-in-Context prompting. A block of several cells represents one distinct stage in a two-stage prompting strategy (e.g., problem decomposition stage in the Least-to-Most prompting). Standard prompting provides only labeled examplars in the context. Chain-of-Thoughts prompting further provides a step-by-step rationale preceding the answer. Decomposed prompting is a two-stage prompting method, which first breaks the questions into sub-problems, and then utilizes standard or Chain-of-Thoughts prompting to solve each sub-problem sequentially to derive the final answer. Least-to-Most prompting adopts a two-stage strategy: it first generates multiple questions in an easy-to-hard manner, and then sequentially answers each of them until solving the original question. In contrast, our Skills-in-Context prompting is a simple one-stage prompting, which places both the (basic) skills and the demonstrations of how to compose them into solutions within the same prompt context. This teaches the LLM how to explicitly and adeptly ground each reasoning step onto the skills (illustrated in dashed lines), which unleashes strong synergies between skills and composition capabilities in LLMs, leading to strong compositionality over unseen harder problems. 
} 
\label{fig:methods}
\end{center}
\vskip -0.2in
\end{figure*}

















\paragraph{Comparison to previous prompting strategies}
Figure~\ref{fig:methods} visualizes the differences between our proposed SKiC prompting and the previous related prompting methods. Different from Chain-of-Thoughts prompting, our SKiC prompting provides explicit grounding on the basic skills for reasoning steps towards final answers. Compared to recent prompting methods for handling compositional problems such as Least-to-Most prompting (LtM) \citep{zhou2022least} and Decomp \citep{khot2022decomposed}, our SKiC is superior in several dimensions: (i) Our SKiC prompting is more general to solve extended sets of problems. Previous decomposing-based approaches like LtM and Decomp usually solve complex problems in a two-stage fashion by first decomposing the problem into a linear sequence of subproblems and then solving them sequentially. However, many of the tasks that have complex computation graphs such as multiplication and dynamic programming problems~\citep{dziri2023faith} cannot be easily and fully decomposed in one stage, which makes it hard to apply these decomposition-based approaches. (ii) The decomposition operation can also be viewed as one basic skill in our SKiC prompt (for example, we view the decomposition operation as one of the skills in the question-answer task in Figure~\ref{Tab:qa_skill}). (iii) SKiC solves the complex problems in a single stage, which could alleviate the error propagation compared to decomposition-based approaches that require multiple distinct stages.


Due to the one-stage nature, our SKiC prompting can replace other one-stage strategies such as the CoT promptings in a plug-and-play manner. And it can also be easily combined with other ensemble techniques such as self-consistency \citep{cot_wei_sc} and Progressive-Hint Prompting \citep{zheng2023progressive} to further boost the performance.




\subsection{Construction of the SKiC Prompts} \label{Sec:skic_construction}





One key step in constructing our SKiC prompts is to distill the (basic) skills that might be needed for solving problems associated with a task. We now introduce two approaches (shown in Figure~\ref{fig:construction}) to achieve this objective.






\paragraph{Distill Skills via Human} 
This is a fully manual approach, where the basic skills are manually summarized from a few (less than 10) problems we want to solve. For example, given several samples from the last-letter-concatenation task, we manually identify that ``words\_to\_list'' and ``last\_letter'' are common basic skills to be used. Based on the discovered skills, we add a few ($1 \sim 2$) simple examples to illustrate these basic skills alone. Once the in-context skills are constructed, we add the compositional examplars to demonstrate the composition of these skills to solve a problem (Figure~\ref{fig:example}). This approach puts all the essential skills in the context and is generally applicable to narrow domain problems that require the composition of limited basic skills for solving harder problems (e.g., with larger-size). It is also beneficial for semi-parametric LLMs, which can dynamically access the most relevant skills from external memories based on each input instance and integrate them into the problem context \citep{pan2023knowledgeincontext}.




\paragraph{Distill Skills via Prompting LLMs} 
This is a semi-automatic approach, where we first prompt the LLMs to automatically generate the necessary basic skills (i.e., the descriptions and examples) followed by human review and adjustment. For instance, when identifying the skills required to address the mathematical reasoning problems in the MATH task \citep{hendrycks2021measuring}, which encompasses a range of problems from Algebra, Counting and Probability, Geometry, Intermediate Algebra, Number Theory, PreAlgebra, to PreCalculus, we prompt the LLM with phrases like ``basic skills in Algebra''. This leads the model to generate foundational skills, such as ``Factoring'' (see Figure~\ref{Tab:math_skill} for the full list of the skills). Next, we manually construct the compositional examplars by grounding the reasoning steps on the skills. It is worth noting that an exemplar might require skills not explicitly presented in the prompt context. In these instances, we anchor the reasoning to inherent skills within the LLMs, confirming their presence through zero-shot prompting. For example, in the compositional exemplar showcased in Figure~\ref{Tab:compose_math_skill}, aside from leveraging in-context skills such as ``Combination'' and ``Sub'', it also employs skills like ``Pascal's Triangle'' --- a capability not present in the context but inherently known to the LLM. Such a construction of the examplars will encourage the model to generalize beyond the in-context skills and compose solutions from the internal capabilities as well --- see Figure~\ref{Tab:example_compose_math_skill_5} for an example of the generated solution that activates the internal skills $<$Angle Bisector Theorem$>$ and $<$Heron's Formula$>$. To be more specific, for every problem in the MATH task, around 24\% of the skills, as shown in Table~\ref{Tab:math_results}, applied in the reasoning steps stem from the LLM's internal pre-trained knowledge (see Table~\ref{Tab:math_top_skills} for the most frequently used internal skills). The ability to harness both in-context skills and inherent capabilities is crucial for addressing complex reasoning problems, which typically require varied compositions across a broad spectrum of foundational skills. Manually enumerating every required skill within a prompt context is often impractical. Meanwhile, LLMs have accumulated a vast reservoir of knowledge and skills during their pre-training. Leveraging these internal competencies can unlock significant potential, allowing LLMs to tackle even more complex challenges. 



\begin{figure*}[t]
\begin{center}
\centerline{\includegraphics[width=1.0\columnwidth]{figures/prompt_construction.png}}
\caption{Two approaches to creating our SKiC prompt for a given task, depending on how we distill the skills. (a) We \emph{manually} summarize the basic skills from the sample problems associated with the task, and then construct the compositional examplars on how to compose these skills to tackle the problem. (b) We prompt the LLMs to \emph{automatically} generate the necessary basic skills, followed by human review and adjustments. Then we manually craft the compositionl examplars by grounding their reasoning steps onto either the provided in-context skills or the inherent skills within the LLMs, where the existence of the inherent skills in LLMs is verified by zero-shot prompting.
} 
\label{fig:construction}
\end{center}
\vskip -0.2in
\end{figure*}




\begin{figure*}[t]
\begin{center}
\centerline{\includegraphics[width=1.0\columnwidth]{figures/example_math.png}}
\caption{An example of the generated solution on the MATH task using our SKiC prompting. The skills  $<$Angle Bisector Theorem$>$ and $<$Heron's Formula$>$ are neither provided in the SKiC prompting context (see Figure \ref{Tab:math_skill}) nor used in any given examplars. LLMs harness the internal skills in their pre-trained knowledge to solve the problem.} \label{Tab:example_compose_math_skill_5}
\end{center}
\vskip -0.2in
\end{figure*}




















