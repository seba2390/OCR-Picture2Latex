\section{Introduction}
\label{sec: introduction}

Pre-trained large language models (LLMs) have achieved great success in solving natural language processing (NLP) tasks \citep{brown2020language, Radford2019LanguageMA, smith2022using, chowdhery2022palm, lewkowycz2022solving, sanh2021multitask, wei2021finetuned, mishra2022cross, chung2022scaling, ouyang2022training,openai2023gpt4,touvron2023llama}. When the size of the model continuously scales up, LLMs exhibit strong zero-shot and few-shot performance on a wide range of NLP tasks \citep{brown2020language,wei2021finetuned,chowdhery2022palm,zhou2022least,cot_wei_sc,li2022advance,wang2022rationale,kojima2022large,shi2022language,magister2022teaching,ho2022large,nye2021show,dua2022successive,openai2023gpt4} --- a salient behavior characterized by the scaling law \citep{kaplan2020scaling,hoffmann2022training} and emergent abilities \citep{wei2022emergent}. However, LLMs still struggle with compositional generalization, i.e., the ability to use existing known skills to solve more complex problems than they have seen before (i.e., ``easy-to-hard'' generalization)~\citep{zhou2022least,dziri2023faith}. In this paper, we will focus on developing a prompting strategy for LLMs that can effectively unlock their generic compositional generalization capabilities.



Ideally, if an LLM has already learned a rich set of skills, it should be able to solve any problem whose solutions are composable from these skills. To unlock such great potential, the key is to teach the LLMs how to use these skills to construct a solution to any unseen, more difficult problem. Towards this goal, there have been a series of prompting strategies being developed to improve the reasoning and compositionality capabilities. Notably, chain-of-thought (CoT) prompting \citep{wei2022chain} significantly improves the reasoning performance of LLMs by demonstrating how to approach a complex problem through a sequence of simple and basic steps. Follow-ups such as Least-to-Most prompting \citep{zhou2022least} and decomposed prompting \citep{khot2022decomposed} propose a two-stage strategy, which first decomposes the original problem into a set of subproblems, and then solve and combine each of them sequentially. Although these methods significantly boost the performance over standard prompting in solving many challenging compositional generalization tasks, they still cannot perform systematic generalization well enough over problems that are significantly harder than the ones they have seen. Moreover, least-to-most prompting and decomposed prompting are restricted to solving a certain class of tasks, where each problem can be decomposed as a sequence of subproblems. And for problems with general computation graphs \citep{dziri2023faith}, it is generally less intuitive, if not possible, to construct the prompting exemplars.




In this paper, we develop an effective \emph{one-stage} prompting strategy, named \textbf{SK}ills-\textbf{i}n-\textbf{C}ontext (SKiC) prompting, to unlock the general compositional generalization capability in LLMs. The key insight is to teach the LLM to explicitly ground each of its reasoning steps on the (more elementary) skills. Specifically, the SKiC prompt is constructed from three main blocks. The first block contains the skills that LLMs may need to use in order to solve a more complex problem, which include both descriptions of the skills and the instructions (with a few examples) on how to use them. These skills can be distilled either manually or automatically via prompting the LLMs. The second part consists of a few (generally two in most of our cases) examplars that demonstrate how to explicitly compose skills into a solution to a more complex problem. The last part is the problem to be solved. Interestingly, with both the skills and their explicit compositions presented in the context, the LLMs successfully learn how to ground each reasoning step on the knowledge and skills that they have already mastered, yielding the desirable general compositional generalization capabilities. Notably, unlike the Least-to-Most or decomposed prompting, our proposed approach is a one-stage prompting method, without the need to call LLMs multiple times. Therefore, it can be easily used in a plug-and-play manner, as the CoT prompting and the standard prompting.




We evaluate our proposed SKiC prompting on a wide range of challenging compositional generalization tasks. Our experiments show that SKiC prompting achieves state-of-the-art performance on all of these benchmarks, and it even achieves near-perfect generalization on unseen harder problems on some of the datasets. Moreover, the improvement margins compared to the previous methods are significant. For example, SKiC outperforms previous state-of-the-art prompting strategies on unseen longer cases by 16.3\% on last-letters \citep{zhou2022least}, 25.5\% on addition, 45.0\% on multiplication \citep{dziri2023faith},9.93\% on Commaqa-E \citep{khot2021hey}, 36.0\% on dynamic programming \citep{dziri2023faith}, 2.7\% on GSM8K\citep{cobbe2021training}, and 12.1\% on MATH \citep{hendrycks2021measuring}. Notably, our results on GSM8K and MATH further reveal that SKiC prompting allows the LLMs to generalize beyond the skills provided in the context and solve problems by using the vast reservoir of the internal skills they acquired during the prior pretraining stage. It clearly demonstrates that SKiC prompting unleashes strong synergies between skills and their composition capabilities, which teaches LLMs to generalize to harder problems than they have seen and to problems that require innovative compositions of existing knowledge.


