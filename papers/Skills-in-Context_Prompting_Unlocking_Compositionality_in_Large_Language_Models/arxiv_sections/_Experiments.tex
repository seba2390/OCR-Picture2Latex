\section{Experiments}
\label{sec: experiments}
In this section, we show the superior compositional capabilities of our SKiC prompting by evaluating it in two settings:


\begin{itemize}[leftmargin=0.6cm]
  \item \textbf{Composition over in-context skills}, where all the essential skills needed to solve the problems are provided in the context. The tasks we evaluate in this setting  include symbolic manipulation \citep{wei2022chain,zhou2022least,khot2022decomposed}, arithmetic operation \citep{dziri2023faith}, question answering \citep{khot2022decomposed}, and dynamic programming \citep{dziri2023faith}. In this setting, we mainly examine the ability to generalize from easy demonstration examplars to more difficult testing problems (i.e., easy-to-hard generalization).
  
  \item \textbf{Generalization beyond in-context skills}, where models also need to harness skills beyond what have been provided in the context and tap into the internal skills for math reasoning like GSM8K~\citep{wei2022chain,zhou2022least} and MATH~\citep{hendrycks2021measuring} problems. In this context, the primary challenge lies in achieving diverse compositions across a wide range of foundational skills to solve a complex reasoning problem.
\end{itemize}


\subsection{Composition over In-Context Skills: Easy-to-Hard Generalization}

In this section, we begin by evaluating our SKiC prompting strategy on tasks that require only a limited skill set, yet pose challenges in terms of easy-to-hard generalization capabilities. Under these circumstances, we construct our SKiC prompts manually, adhering to the first methodology outlined in Section~\ref{Sec:skic_construction}. We mainly consider foundation models including LLAMA-65B \citep{touvron2023llama1}, text-davinvi-003 \citep{brown2020language}, ChatGPT and GPT-4~\citep{openai2023gpt4}. Additional experiments on LLAMA2~\citep{touvron2023llama} can be found in Appendix~\ref{Sec:llama2}.

\subsubsection{Symbolic Manipulation: Last Letters}
Following \citeauthor{zhou2022least}, we first assess the compositionality in LLMs through the last-letter-concatenation task. For a given list of words, the LLM needs to generate an output that is the concatenation of the last letter from each word in the list. We compare our SKiC with zero/few-shot standard prompting (4-shot) \citep{brown2020language}, CoT~\citep{wei2022chain} and Least-to-Most prompting (LtM) \citep{zhou2022least} on different large language models, including LLAMA-65B \citep{touvron2023llama1}, text-davinvi-003 \citep{brown2020language,ouyang2022training}, and ChatGPT. And we evaluate them on different subsets of testing problems that include 1, 2, 4, 6, 8, 10, 12 words\footnote{From \url{https://github.com/first20hours/google-10000-english/tree/master}.}, respectively. The examplars in all the prompts are constructed from the cases with 1 or 2 words. Therefore, the evaluations on the test subsets with 1, 2 words are in-distribution, and the ones on 4, 6, 8, 10, 12 words are out-of-distribution. 

A SKiC prompt contains the skills and two examples of how to compose these skills as shown in Figure~\ref{Tab:last_letter_skill} and Figure~\ref{Tab:compose_last_letter_skill}. The model is given the needed skills such as putting the given words to a list and getting the last letter of one word, and then two examples of how to compose these skills to take the last letters of two given words. 




\begin{table}[th]
 \caption{Accuracy of different prompting methods on different evaluation subsets of the last-letter-concatenation task. The testing problems with 1 and 2 words are in-distribution evaluation, while the ones with $4\sim 12$ words are (harder) out-of-distribution evaluations. } \label{Tab:last_letter_results}
\centering
\small
\begin{tabular}{c|c|c|cc|ccccc} \toprule
\textbf{Model}                & \textbf{Prompting} &\textbf{\#-shots}     & \multicolumn{1}{c}{\textbf{1}} & \multicolumn{1}{c|}{\textbf{2}} & \multicolumn{1}{c}{\textbf{4}} & \multicolumn{1}{c}{\textbf{6}} & \multicolumn{1}{c}{\textbf{8}} & \multicolumn{1}{c}{\textbf{10}} & \multicolumn{1}{c}{\textbf{12}} \\ \midrule \midrule
\multirow{5}{*}{LLAMA-65B} & zero-shot  &0        & 0                     & 0                     & 0                     & 0                     & 0                     & 0                      & 0                      \\
                           & 4-shots   &4      & 72.0                    & 66.0                    & 50.0                    & 26.0                    & 10.0                   & 6.0                      & 0                      \\
                           & CoT   &4        & 76.0                    & 70.0                    & 58.0                    & 42.0                    & 30.0                    & 26.0                     & 20.0                     \\
                           & LtM &4  & 76.0                    & 72.0                    & 66.0                   & 50.0                   & 46.0                    & 36.0                     & 25.0                     \\
                           & SKiC  &2 & \textbf{81.0}           & \textbf{97.0}           & \textbf{77.0}           & \textbf{59.0}           & \textbf{56.0}           & \textbf{48.0}            & \textbf{36.0}            \\  \midrule
\multirow{5}{*}{text-davinci-003}  & zero-shot   &0        & 0                     & 0                     & 0                     & 0                     & 0                     & 0                      & 0                      \\
                           & 4-shots  &4     & 99.0                    & 97.0                    & 89.0                    & 68.0                    & 45.0                    & 27.0                     & 10.0                     \\
                           & CoT  &4        & 100.0                   & 99.0                    & 90.0                    & 75.0                    & 52.0                    & 39.0                     & 31.0                     \\
                           & LtM &4 & 100.0                   & 99.0                    & 94.0                    & 90.0                    & 87.0                    & 84.0                     & 80.0                     \\
                           & SKiC  &2 & \textbf{100.0}          & \textbf{100.0}          & \textbf{100.0}          & \textbf{100.0}          & \textbf{100.0}          & \textbf{99.0}           & \textbf{98.0}            \\  \midrule
\multirow{5}{*}{ChatGPT}   & zero-shot &0           & 99.0                    & 98.0                    & 93.0                    & 88.0                    & 84.0                    & 80.0                     & 77.0                     \\
                           & 4-shots  &4      & 100.0                   & 100.0                   & 95.0                    & 92.0                    & 90.0                    & 86.0                     & 85.0                     \\
                           & CoT    &4       & 100.0                   & 100.0                   & 97.0                    & 95.0                    & 92.0                    & 88.0                     & 85.0                     \\
                           & LtM &4 & 100.0                   & 100.0                   & 99.0                    & 95.0                    & 92.0                    & 92.0                     & 88.0                     \\
                           & SKiC  &2 & \textbf{100.0}          & \textbf{100.0}          & \textbf{100.0}          & \textbf{100.0}          & \textbf{100.0}          & \textbf{100.0}           & \textbf{100.0}       \\ \bottomrule    
\end{tabular}
\end{table}

% $\dag$ represents our method.

The results are reported in Table~\ref{Tab:last_letter_results}. We observe that standard zero/few-shot prompting generalizes poorly on the testing problems that are harder than the examplars in the prompting context. For example, 4-shot standard prompting only achieves 10\% accuracy with text-davinci-003 when solving testing problems that involves 12 words. Chain-of-Thoughts and Least-to-Most prompting improve the overall performance but still degrade quickly over longer inputs. Our Skills-in-Context prompting  significantly boosts the accuracy in all the test cases especially when there are more input words --- it achieves nearly perfect generalization to harder problems with text-davinvi-003 and ChatGPT. This suggests that by showing the basic skills and teaching the models how to use the skills (with just \emph{two} examples), our designed Skills-in-Context prompting achieves better compositionality. An example of the generated answer when using SKiC prompting to concatenate the last letters of 10 words can be found in Figure~\ref{Tab:example_last_letter_skill}.



\subsubsection{Arithmetic Operation}
Following \citeauthor{dziri2023faith}, we evaluate the compositional capabilities on two arithmetic operation tasks: addition and multiplication. These two tasks involves complicated composition over skills such as one-digit addition or multiplication, carry over, concatenation and etc.\citep{dziri2023faith}, making it difficult especially for long form addition or multiplication. We compare our Skills-in-Context prompting (SKiC) with zero/few-shot standard prompting \citep{brown2020language} and Chain-of-Thoughts prompting (CoT) \citep{wei2022chain} on different foundation models including LLAMA-65B, text-davinvi-003, and ChatGPT. We exclude the Least-to-Most prompting \citep{zhou2022least} as it is difficult to design linear problem decomposition for addition or multiplication task. We also include text-davinci-003 finetuned with scratchpad method \citep{nye2021show,dziri2023faith} on the multiplication task for comparison.


\begin{table}[t]
\caption{Accuracy of different prompting methods on the task of adding two numbers with different digits (2,3,4,5,6,7) and multiplying two numbers with different digits (2,3,4,5). For the addition task, the prompting examplars are constructed to demonstrate the addition between two numbers with 2 or 3 digits. Therefore, the results for adding numbers with $4 \sim 7$ digits measure the desirable compositional generalization capabilities over harder problems. For the multiplication task, the prompting examplars are constructed to demonstrate how to multiply two numbers with 2 or 3 digits. Therefore, the results for multiplying numbers with 4 and 5 digits  measure the compositional generalization capability over harder problems. $\dag$ denotes our method.} \label{Tab:add_mul_results}
\centering
\scalebox{0.88}{
\begin{tabular}{c|c|c|cc|cccc||cc|cc} \toprule
\multirow{2}{*}{\textbf{Model}}     & \multirow{2}{*}{\textbf{Prompting}} &\multirow{2}{*}{\textbf{\#-shots}} & \multicolumn{6}{|c||}{\textbf{Addition}} & \multicolumn{4}{|c}{\textbf{Multiplication}}   \\  \cmidrule{4-13}

 & & &\multicolumn{1}{c}{\textbf{2}} & \multicolumn{1}{c|}{\textbf{3}} & \textbf{4}   & \textbf{5}   & \textbf{6}   & \textbf{7} &\multicolumn{1}{c}{\textbf{2}} & \multicolumn{1}{c|}{\textbf{3}} & \textbf{4}   & \textbf{5} \\ \midrule \midrule
\multirow{4}{*}{LLAMA-65B} & zero-shot    &0              & 58.0                             & 40.5                             & 22.5           & 8.0            & 0            & 0            & 28.0                             & 17.0                             & 0                              & 0                              \\
                           & 4-shots  &4            & 64.5                             & 46.5                             & 28.0           & 10.0            & 0            & 0            & 24.0                             & 18.0                             & 0                              & 0                              \\
                           & CoT   &4             & 60.0                             & 52.5                             & 24.0           & 12.0           & 1.0            & 0           & 22.0                             & 21.0                             & 0                              & 0                              \\
                           & SKiC$\dag$  &2     & \textbf{82.5}                    & \textbf{74.5}                    & \textbf{66.5}  & \textbf{52.0}  & \textbf{38.0}  & \textbf{22.0}   & \textbf{50.0}                    & \textbf{42.0}                    & \textbf{12.0}                    & \textbf{8.0}                     \\   \midrule
\multirow{5}{*}{text-davinci-003}  & zero-shot  &0                & 100.0                            & 100.0                            & 98.0           & 87.5           & 74.5           & 54.0           & 76.0                             & 14.5                             & 0                              & 0                              \\
                           & 4-shots &4               & 100.0                            & 100.0                            & 98.0           & 92.0           & 80.5           & 58.5           & 82.0                             & 18.0                             & 0                              & 0                              \\
                           & CoT    &4               & 100.0                            & 100.0                            & 92.0           & 68.5           & 42.0           & 38.0            & 86.0                             & 20.5                             & 2.0                              & 0                              \\ 
                             & finetuned &0 &- &- &- & - &- & -& 99.0 &55.0 & 1.0  &0.0 \\ 
                           & SKiC$\dag$  &2   & \textbf{100.0}                   & \textbf{100.0}                   & \textbf{99.0}  & \textbf{98.0}  & \textbf{99.0}  & \textbf{98.5}   & \textbf{100.0}                   & \textbf{58.0}                    & \textbf{42.5}                    & \textbf{36.0}                    \\  \midrule
\multirow{4}{*}{ChatGPT}   & zero-shot  &0                & 100.0                            & 100.0                            & 100.0          & 92.0           & 86.5           & 78.0           & 99.0                            & 55.0                            & 1.0                             & 0                              \\
                           & 4-shots   &4             & 100.0                            & 100.0                            & 100.0          & 94.0           & 90.5           & 83.5           & 99.0                             & 58.0                             & 1.0                              & 0                              \\
                           & CoT     &4              & 100.0                            & 100.0                            & 98.5           & 90.0           & 87.5           & 80.0           & 99.0                             & 54.5                             & 13.0                              & 2.0                              \\
                           & SKiC$\dag$ &2      & \textbf{100.0}                   & \textbf{100.0}                   & \textbf{100.0} & \textbf{100.0} & \textbf{100.0} & \textbf{100.0} & \textbf{100.0}                   & \textbf{82.0}                    & \textbf{72.0}                    & \textbf{48.5}                    \\ \bottomrule
\end{tabular} 
}
\end{table}






\paragraph{Addition} We construct different subsets of testing problems, which ask to output the sum of two numbers with 2,3,4,5,6,7 digits, respectively. The given in-context examplars are only constructed to demonstrate the addition of two numbers with 2-digits or 3-digits. Consequently, the results for 4,5,6,7-digits summation are out-of-distribution evaluation. Figures~\ref{Tab:simple_add_skill}--\ref{Tab:compose_simple_add_skill} show the basic skills and, for brevity, one compositional examplar, respectively. We first present the basic skills like extracting digits from a number and then show the model how to use these skills to add two numbers with two examples. The results are shown in Table~\ref{Tab:add_mul_results}. Even though large language models such as text-davinci-003 and ChatGPT can perform well in adding smaller numbers in zero-shot and few-shots settings, they often fail to add larger numbers accurately such as adding two 7-digits numbers. Chain-of-Thoughts prompting does not improve the capability significantly. When utilizing our proposed Skills-in-Context prompting, there are consistent performance improvements on all the models (achieving over 68.9\% improvements on 7-digits addition with text-davinci-003 compared to baselines) and even achieve 100\% accuracy with ChatGPT. This underscores the significance of concurrently presenting both the skills and their compositional examplars within a unified prompt context to enhance compositional generalization. An example of the generated answer using SKiC prompting for solving 6-digits addition can be found in Figure~\ref{Tab:example_simple_add_skill}.




\paragraph{Multiplication} 
Next, we evaluate the compositional generalization performance on the multiplication task. Specifically, we construct different subsets of evaluation problems that ask for the product of two numbers with 2,3,4,5 digits, respectively. The given in-context examplars in all the prompts are constructed to demonstrate 2-digit and 3-digit multiplications. Therefore, the results for 4,5-digits multiplications measure the compositional generalization to unseen harder problems.
The construction of our Skills-in-Context prompting is shown in Figure~\ref{Tab:simple_mul_skill} and Figure~\ref{Tab:compose_simple_mul_skill}, which illustrate the skills and the compositional examplar, respectively. The evaluation results are reported in Table~\ref{Tab:add_mul_results}. All the models with standard prompts or Chain-of-Thoughts prompts can not handle the multiplication, especially for larger number (e.g., 0\% accuracy for ChatGPT when multiplying two 5-digits numbers). After explicit showing the models with the necessary skills to compute the product of two numbers as well as the detailed process of composing these skills, our SKiC prompting significantly improve the multiplication accuracy. It highlights the superiority of our prompting approach to tackle compositional and out-of-distribution testing cases. Our error analysis reveals that most of the errors in SKiC prompting are caused by missing the multi-digit addition capability, which can be incorporated as a basic skill (Figure~\ref{Tab:simple_add_skill} and Figure~\ref{Tab:compose_simple_add_skill}) in the prompting context. When equipped with such extra skills, ChatGPT with SKiC can achieves 100\% accuracy for both 2-digit and 3-digit multiplications.\footnote{Further extending it to more digits will require longer context window of the language models.} We show an example of the generated answer for solving 4-digits multiplication in Figure~\ref{Tab:example_simple_mul_skill}.











\subsubsection{Long-Context Question Answering: CommaQA-E}  
To evaluate the compositional generalization in the reading comprehension setting, following \citeauthor{khot2022decomposed}, we evaluate different prompting strategies on CommaQA-E~\citep{khot2021hey}. For given facts of a set of synthetically generated entities, the models need to answer the multi-hop questions which are composed of multiple reasoning steps, e.g., \textit{What movies have people from the country Stridery acted in?}. Besides the standard zero/few-shot prompting \citep{brown2020language} and the Chain-of-Thoughts prompting (CoT) \citep{wei2022chain}, we also compare our Skills-in-Context (SKiC) prompting to Decomp prompting\footnote{Reproduced using the original code from: \url{https://github.com/allenai/DecomP/tree/main}} \citep{khot2022decomposed}. We evaluate the results on different foundation models: LLAMA-65B, text-davinvi-003, and ChatGPT. The construction of the SKiC prompting for CommaQA-E is described in Figure~\ref{Tab:qa_skill} and~\ref{Tab:compose_qa_skill}, which show the skills and the examplars of how to compose the skills, respectively. Notably, both the ability to break down complex questions into simple ones and the operation to answer each simple questions are also treated as (basic) skills --- see Figure \ref{Tab:qa_skill}. Compared to the multi-stage prompting strategies like least-to-most or DECOMP prompting, such basic skills and their compositional examplars (Figure \ref{Tab:compose_qa_skill}) are placed in the same SKiC prompt context. Consequently, the LLM is able to flexibly apply the question decomposition skill and simple question answering skills to reach the final answer within 1-stage of prompting. The results on the CommaQA-E dataset are summarized in Table~\ref{Tab:qa_results} (measured in Exact Match). We observe that, with multiple stages of question decomposition and answering, Decomp improves the performance over few-shots prompting and chain-of-thoughts prompting. Nevertheless, our SKiC prompting further boosts the accuracy of answering compositional questions significantly (+7\%) by using just one-stage prompting. This is a further manifestion of the advantage of concurrently demonstrating the skills and their compositions for unleashing the compositionality of LLMs. In Figure~\ref{fig:error_example}, we show that errors made in early stages in Decomp prompting result in wrong predictions while our SKiC prompting accurately answer different questions. We show an example of the generated answer on the Commaqa-E test set in Figure~\ref{Tab:example_compose_qa_skill}.




\begin{table}[t]
\caption{Performance of different prompting methods on Commaqa-E datasets (measured in Exact Match). The rows of ``Compositional Generalization'' reports the results on the new (unseen) compositional questions from the compositional generalization split. $\dag$ denotes our method.} \label{Tab:qa_results}
\centering
\small
\begin{tabular}{c|c|c|ccc} \toprule
\textbf{Commaqa-E} & \textbf{Prompting} & \textbf{\#-shots} & \multicolumn{1}{l}{\textbf{LLAMA-65B}} & \multicolumn{1}{l}{\textbf{text-davinci-003}} & \multicolumn{1}{l}{\textbf{ChatGPT}} \\ \midrule \midrule
\multirow{5}{*}{\begin{tabular}[c]{@{}c@{}}\textbf{Commaqa-E} \\ \textbf{Test} \end{tabular}} & zero-shot & 0 & 12.0 & 34.0 & 42.0 \\
                      & 4-shots & 4 & 15.0 & 42.0 & 47.0 \\
                      & CoT & 4 & 27.0 & 44.0 & 55.0 \\
                      & Decomp & 12 & 32.0 & 58.0 & 64.0 \\
                      & SKiC$\dag$ & 2 & \textbf{44.0} & \textbf{66.0} & \textbf{70.0} \\ \midrule
\multirow{5}{*}{ \begin{tabular}[c]{@{}c@{}}\textbf{Compositional} \\ \textbf{Generalization} \end{tabular}} & zero-shot & 0 & 16.3 & 26.8 & 30.6 \\
                           & 4-shots & 4 & 24.6 & 33.5 & 40.3 \\
                           & CoT & 4 & 30.8 & 38.2 & 46.4 \\
                           & Decomp & 12 & 40.4 & 66.6 & 73.5 \\
                           & SKiC$\dag$ & 2 & \textbf{52.0} & \textbf{74.8} & \textbf{80.8} \\ \bottomrule        
\end{tabular} 
\end{table}



% \usepackage{multirow}
\begin{table}[t]
\caption{Accuracy of different prompting methods on the dynamic programming task with input sequence lengths being 4,5,6,7,8, respectively. The in-context examplars for all the prompts are constructed with sequence lengths of 4 and 5. Therefore, the results for sequence lengths of 6,7,8 measures the out-of-distribution generalization to increasingly harder problems. $\dag$ denotes our method.} \label{Tab:dp_results}
\centering
\begin{tabular}{c|c|c|cc|ccc} \toprule
\textbf{Model}               & \textbf{Prompting}&\textbf{\#-shots} & \multicolumn{1}{c}{\textbf{4}} & \textbf{5}  & \textbf{6}  & \textbf{7}  & \textbf{8}  \\ \midrule\midrule 

\multirow{5}{*}{text-davinci-003} & zero-shot  &0        & 10.5                              & 4.0           & 4.0           & 0.0           & 0.0           \\
                          & 4-shots     &4       & 32.5                             & 18.0          & 10.0          & 4.0           & 0.0           \\
                          & CoT     &4           & 58.0                             & 22.0          & 15.0          & 8.0           & 2.0           \\
                          &finetuned & 0 &\textbf{100.0} &\textbf{100.0} &22.0 &14.0 &8.0\\
                          & SKiC$\dag$   &2             & 78.0                    & 62.5 & \textbf{54.5} & \textbf{48.0} & \textbf{42.5} \\ \midrule
\multirow{4}{*}{ChatGPT}  & zero-shot   &0       & 18.0                             & 10.0          & 6.0           & 4.0           & 0.0           \\
                          & 4-shot    &4         & 44.5                            & 18.0          & 10.0          & 4.0           & 0.0           \\
                          & CoT    &4            & 82.5                             & 76.0          & 72.0        & 64.0           & 55.5           \\
                          & SKiC$\dag$    &2            & \textbf{98.0}                    & \textbf{96.0} & \textbf{95.0} & \textbf{94.0} & \textbf{92.0} \\ \midrule
\multirow{4}{*}{GPT-4}     & zero-shot   &0       & 58.0                             & 42.5          & 35.5          & 28.0          & 12.0          \\
                          & 4-shots     &4       & 76.5                             & 70.5          & 58.0          & 55.0          & 42.0          \\
                          & CoT       &4         & 94.0                             & 91.0         & 88.0          & 83.5          & 72.0          \\
                          & SKiC$\dag$  &2             & \textbf{100.0}                    & \textbf{100.0} & \textbf{100.0} & \textbf{99.0} & \textbf{98.0} \\ \bottomrule
\end{tabular} 
\end{table}




\subsubsection{Dynamic Programming}
We then further evaluate the compositional generalization capabilities of Skills-in-Context (SKiC) prompting in solving a classic dynamic programming problem \citep{dziri2023faith}: \textit{Given a sequence of integers, find a subsequence with the highest sum, such that no two numbers in the subsequence are adjacent in the original sequence.} We compare our SKiC prompting (SKiC) with standard zero/few-shot prompting \citep{brown2020language}, and Chain-of-Thoughts prompting (CoT)\footnote{The reasoning steps are constructed based on the scratchpad prompts used in \citet{dziri2023faith}.}  \citep{wei2022chain} on different LLMs (text-davinvi-003, ChatGPT and GPT-4). In addition, we also compare with the baseline of finetuned text-davinci-003 with scratchpad (reported in \citet{dziri2023faith}).
Likewise, we evaluate them on different subsets of testing instances with sequence length of 4, 5, 6, 7, 8, respectively.\footnote{The numbers are within the range [-5,5]} The in-context examplars are constructed with sequence length of 4 and 5. Therefore, the testing subsets with sequence length of 4 and 5 are in-distribution evaluation and the ones with length 6, 7, and 8 are for out-of-distribution evaluation. The construction of SKiC prompt is characterized in Figure ~\ref{Tab:dp_skill} and~\ref{Tab:compose_dp_skill}, which show the skills and their compositions examplars, respectively. Specifically, in the SKiC prompt, the models are presented with the skills to get the length of a list, find the max number for a given list and add two single digit numbers, followed by two compositional examplars about how to compose these skills to solve the dynamic programming problems with sequence length 4 and 5. Table~\ref{Tab:dp_results} shows the results (measured in accuracy). Compared to the previous prompting techniques such as Chain-of-Thoughts, our proposed SKiC again achieve the best performance, with a large improvement margin on the out-of-distribution compositionality (e.g., improving the accuracy by a large margin of $40.5\%$ using text-davinci-003 for sequence length of $8$). In addition, compared to the finetuned text-davinci-003 with scratchpad, SKiC prompting is also significantly better in the out-of-distribution regime, although its performance at the in-distribution regime is worse.\footnote{This is expected as the it is finetuned directly on input sequences with length 4 and 5, while our method is not finetuned at all.} Notably, with a stronger foundation model (i.e., GPT-4), SKiC prompting even achieves near perfect generalization ($98\%$), which also improves significantly over CoT by $26\%$. By incorporating basic skills into the prompts --- such as extracting the length of a list and identifying the maximum number within it --- the models are guided to reason and address problems based on these foundational skills. Consequently, it performs the reasoning steps more accurately and could generalize better to the harder examples by following similar patterns to compose the basic skills. An example of the generated answer on the DP task for a sequence of 8 numbers can be found in Figure~\ref{Tab:example_compose_dp_skill}.



























\subsection{Generalization Beyond In-Context Skills: Complex Reasoning}
In this section, we further evaluate whether our SKiC prompting could allow LLMs to generalize beyond the skills provided in the prompt context and invoke the massive set of internal skills and knowledge that are acquired during pre-training. 
Such capability is vital in solving complex reasoning problems (e.g., math), which require varied compositions over a vast amount of foundational skills. And it is impractical to enumerate all the skills in context.

\begin{figure*}[t]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.8\columnwidth]{figures/gsm8k.png}}
\caption{The accuracy of different prompting techniques on GSM8K tasks (using different LLMs).} 
\label{fig:gsm8k_results}
\end{center}
\vskip -0.2in
\end{figure*}

\subsubsection{GSM8K}
We first apply our Skills-in-Context prompting to GSM8K \citep{cobbe2021training}, which requires multiple math-related skills to solve complex math world problems. We construct our SKiC prompt by using the first approach in Section~\ref{Sec:skic_construction},
% For this task, since it is infeasible to enumerate all the needed skills to solve all the problems in GSM8k, 
which includes a limited skill set together with eight compositional examplars to teach the LLMs how to use them. Figure~\ref{Tab:gsm8k_skill} and Figure~\ref{Tab:compose_gsm8k_skill} show the constructed skill set and one compositional examplar, respectively. We compare our SKiC with Chain-of-Thoughts prompting (CoT) \citep{wei2022chain}, Least-to-Most prompting (LtM) \citep{zhou2022least}, ComplexCot \citep{fu2022complexity} and PHP \citep{zheng2023progressive} on different foundation models (i.e., text-davinvi-003, ChatGPT and GPT-4). We evaluate the accuracy on the GSM8K test set, which are shown in the Figure~\ref{fig:gsm8k_results} \footnote{The results are re-implemented with the provided prompts from the original works. Note that GPT-4's performance might drop over time on math related tasks as is observed in \citet{chen2023chatgpts}, which might make our reproduced number lower than the ones reported in the original papers (e.g., PHP results with GPT-4).}. Note that, it is generally impractical to enumerate all the skills in the prompt context. However, even with incomplete skill set in our SKiC prompts, we still observe a significant accuracy boost compared to previous state-of-the-art prompting methods across all foundation models (even better than multi-stage methods such as PHP, which modifies and corrects the predictions through multiple rounds). Significantly, we observe several intriguing cases of generalization: (i) the generated reasoning steps effectively utilize the provided skills that are not demonstrated in the compositional examples (see Figure~\ref{Tab:example_compose_gsm8k_skill_1} for an example), (ii) the generated reasoning steps successfully employ skills that are not included in the prompts but may exist within the pre-trained knowledge of the LLM (see Figure~\ref{Tab:example_compose_gsm8k_skill_2} and \ref{Tab:example_compose_gsm8k_skill_3} for examples). These discoveries suggest that, with SKiC prompting, LLMs can be taught to use the skills provided in the context as well as from their pre-existing internal (pretrained) knowledge to solve math problems via compositionality. 



\definecolor{red5}{rgb}{0.55,0.95,0.55}

\begin{table}[t]
\caption{Testing accuracy on the MATH benchmark. We compare our SKiC prompting with different prompting strategies: CoT~\citep{wei2022chain}, Scratchpad \citep{nye2021show}, Learning-to-Program(LtP)\citep{guo2023learning}, and ComplexCoT \citep{fu2022complexity}. In addition, we also include different ensemble strategies that are commonly combined together with these baselines: majority voting (maj1@k) \citep{lewkowycz2022solving}, Self-Consistency (SC) \citep{cot_wei_sc}, and Progressive-Hint Prompting (PHP) \citep{zheng2023progressive}. We use $\dag$ to represent our method. Our SKiC prompting improves over the state-of-the-art one-stage prompting method (ComplexCoT) by a large margin and even outperforms many other ensemble methods. In SKiC prompting, we also report the \emph{internal skill activation rate}, which measures the percentage of skills utilized in the output reasoning steps for each question that originate from pre-trained knowledge (rather than being included in the SKiC prompt context). This rate clearly indicates that SKiC prompting enables LLMs to generalize beyond the in-context skills, 
tapping into the vast reservoir of internal skills they amassed during the prior pretraining stage and leveraging two types of skills.
% and leverage massive amount of their pre-existing internal skills acquired during the earlier pretraining stage.
} \label{Tab:math_results}
\centering
% \tiny
\scalebox{0.66}{%
\begin{tabular}{c|c|c|ccccccc|c} \toprule
\textbf{Model}        & \textbf{Prompting }      & \textbf{Ensemble} & \textbf{Pre-Algebra}   & \textbf{Geometry}      & \textbf{Inter-Algebra} & \textbf{Algebra}       & \textbf{Probability}   & \textbf{Pre-Calculus}  & \textbf{NumTheory}     & \textbf{Overall}       \\  \midrule  \midrule

PaLM-2  & CoT             & SC       & -             & -             & -             & -             & -             & -             & -             & 48.8          \\ 
Minerva-540B & CoT, Scratchpad & maj1@k    & 71.1          & 42.0          & 27.1          & 72.7          & 43.5          & 34.5          & 36.3          & 50.3          \\ 
ChatGPT      & ComplexCoT      & PHP      & 57.7          & 25.4          & 17.1          & 49.1          & 33.7          & 16.1          & 35.1          & 36.5          \\  
GPT-4        & ComplexCoT      & PHP      & 73.8          & 41.9          & 26.3          & 73.4          & 56.3          & 29.8          & 55.7          & 53.9          \\  \midrule \midrule

PaLM-2        & CoT             & \XSolidBrush     & -             & -             & -             & -             & -             & -             & -             & 34.3          \\
Minerva-540B & CoT, Scratchpad & \XSolidBrush     & 54.9          & 26.7          & 13.6          & 51.2          & 27.9          & 18.0          & 21.2          & 33.6          \\ \midrule

\multirow{4}{*}{ChatGPT}        & CoT, LtP      & \XSolidBrush     & 52.3          & 22.5          & 16.9          & 49.6          & 30.2          & 16.3          & 29.8         & 31.1          \\
& ComplexCoT      & \XSolidBrush     & 53.8          & 22.3          & 14.6          & 49.1          & 29.7          & 16.8          & 33.4          & 34.1          \\
      & SKiC$\dag$          & \XSolidBrush     & 62.0 \small{\colorbox{red5}{$\uparrow8.2$}}           & 30.1 \small{\colorbox{red5}{$\uparrow7.8$}}          & 17.8 \small{\colorbox{red5}{$\uparrow3.2$}}        & 57.9  \small{\colorbox{red5}{$\uparrow8.8$}}        & 38.2 \small{\colorbox{red5}{$\uparrow8.5$}}         & 23.0 \small{\colorbox{red5}{$\uparrow6.2$}}         & 35.5  \small{\colorbox{red5}{$\uparrow2.1$}}        & 40.6 \small{\colorbox{red5}{$\uparrow6.5$}}         \\ \cmidrule{2-11}
        
        & \multicolumn{2}{c|}{\textit{Internal Skill Activation Rate}}    & \textit{6.5} & \textit{19.0} & \textit{13.2} & \textit{5.7} & \textit{9.1} & \textit{45.2} & \textit{7.8} & \textit{14.9} \\ \midrule 

        
\multirow{4}{*}{GPT-4}        & CoT             & \XSolidBrush     & -             & -             & -             & -             & -             & -             & -             & 42.2          \\
        & ComplexCoT      & \XSolidBrush     & 71.6          & 36.5          & 23.4          & 70.8          & 53.1          & 26.7          & 49.6          & 50.3          \\
        & SKiC$\dag$           & \XSolidBrush     & \textbf{79.7} \small{\colorbox{red5}{$\uparrow8.1$}}  & \textbf{43.6} \small{\colorbox{red5}{$\uparrow7.1$}}  & \textbf{29.5} \small{\colorbox{red5}{$\uparrow6.1$}}  & \textbf{74.6} \small{\colorbox{red5}{$\uparrow3.8$}}  & \textbf{58.2} \small{\colorbox{red5}{$\uparrow5.1$}}  & \textbf{36.6} \small{\colorbox{red5}{$\uparrow9.9$}}  & \textbf{55.9} \small{\colorbox{red5}{$\uparrow6.3$}}  & \textbf{56.4} \small{\colorbox{red5}{$\uparrow6.1$}}  \\  \cmidrule{2-11}
        
        & \multicolumn{2}{c|}{\textit{Internal Skill Activation Rate}}    & \textit{12.7} & \textit{37.0} & \textit{33.4} & \textit{16.0} & \textit{4.4} & \textit{65.5} & \textit{12.1} & \textit{24.3} \\ \bottomrule     
\end{tabular}
}
\end{table}


\subsubsection{MATH}
We then apply our Skills-in-Context prompting to MATH~\citep{hendrycks2021measuring}, which is a significantly more challenging benchmark on mathematical reasoning. It encompasses problems in Algebra, Counting and Probability, Geometry, Intermediate Algebra, Number Theory, PreAlgebra, and PreCalculus. Due to the large variety of foundational capabilities needed for solving these math problems, it is infeasible to distill and enumerate the needed skills manually. Therefore, we adopt the second approach as described in Section~\ref{Sec:skic_construction}, where we prompt the LLM to generate the skills and then craft the compositional examples manually. Specifically, we first prompt the LLM (i.e., the same LLM that we will use to solve the problems) to generate a list of skills for each subject category in the MATH dataset (e.g., ``Counting and Probability'') with the instruction ``Basic skills in $[$subject$]$''. Then we further ask the model to generate the description of each skill, and the resulting skill set is listed in Figure~\ref{Tab:math_skill}. In Figure~\ref{Tab:compose_math_skill}, we show a compositional examplar that demonstrates how to utilize the skills to solve a problem in MATH dataset. Note from this example that we ground a part of the reasoning steps to in-context skills such as ``Combination'' and ``Sub'' and anchor others to internal skills (e.g., ``Pascal's Triangle''). In our experiment, we provide the model with seven examplars (one example per category in the MATH dataset). We compare our SKiC prompting with different prompting strategies: CoT \citep{wei2022chain}, Scratchpad \citep{nye2021show}, Learning-to-Program(LtP) \citep{guo2023learning}, and ComplexCoT \citep{fu2022complexity} on two representative foundation models: ChatGPT and GPT-4 \footnote{We use the same model to construct the SKiC skills and to do the inference. That is, we prompt ChatGPT to construct the SKiC when testing with ChatGPT and we prompt GPT-4 to construct the SKiC when testing with GPT-4.}. In addition, we also include different ensemble strategies that are commonly combined together with these baselines: majority voting (maj1@k) \citep{lewkowycz2022solving}, Self-Consistency (SC) \citep{cot_wei_sc}, and Progressive-Hint Prompting (PHP) \citep{zheng2023progressive}. The accuracy on the MATH test sets is reported in Table~\ref{Tab:math_results}. With SKiC prompting, models could explicitly ground the reasoning steps to the skills in the context as well as their internal knowledge to resolve diverse math problems. As a result, our SKiC significantly outperforms the state-of-the-art prompting methods on all the sub-categories in MATH test set with only \textbf{one} round of generation, and it even outperforms the approaches that ensemble the outputs from multiple rounds of generations (e.g., PHP). In Table~\ref{Tab:math_results}, we also show the \emph{internal skill activation rate} that measures the percentage of skills utilized in the generated reasoning steps for each question that originate from pre-trained knowledge (rather than being introduced in the SKiC prompt context). It further verifies that our SKiC prompting allows the LLMs to generalize beyond the in-context skills and invoke the massive reservoir of internal capabilities in LLMs (e.g., 24\% of skills utilized in the output reasoning steps are from the GPT-4 internal knowledge) --- see Figure~\ref{Tab:example_compose_math_skill_5}, \ref{Tab:example_compose_gsm8k_skill_1}, ~\ref{Tab:example_compose_gsm8k_skill_2},~\ref{Tab:example_compose_gsm8k_skill_3} and~\ref{Tab:example_compose_math_skill_4} for more examples of the generated solutions for the MATH problems, where the reasoning process carried out by the LLM (GPT-4) effectively utilize both in-context and internal skills. In Table~\ref{Tab:math_top_skills}, we also report the most frequently used in-context and internal skills for solving MATH problems.








% Please add the following required packages to your document preamble:
% \usepackage{multirow}
\begin{table}[t]
\caption{The most frequently used skills by GPT-4 for solving MATH benchmark with SKiC prompting. The skills can be from the context of the SKiC prompts (denoted as ``in-context'' in the table) or from the internal knowledge acquired during the pretraining stage (denoted as ``internal'').} \label{Tab:math_top_skills}
\scalebox{0.8}{
\begin{tabular}{c|c|l} \toprule
\centering
\textbf{Category}              & \textbf{Source} & \textbf{Top Used Skills}                                                                                                                                                                                                            \\ \midrule \midrule
\multirow{3}{*}{Pre-Algebra}   & In-context            & Div, Mul, Add, Sub, Solve Equation, Area, Exp, Counting Principle, Radicals, Prime Numbers                                                                                                                                          \\ \cmidrule{2-3}
                               & Internal        & \begin{tabular}[c]{@{}l@{}}Pythagorean Theorem, Rounding, Divisibility Rules, Percentage, Angles, Simply Fraction, \\ Mean, Ratio, Triangle Angle Sum, Order of Operations\end{tabular}                                             \\  \midrule \midrule
\multirow{3}{*}{Geometry}      & In-context            & Area, Mul, Div, Add, Sub, Solve Equation, Volume, Radicals, Exp, Perimeter                                                                                                                                                          \\ \cmidrule{2-3}
                               & Internal        & \begin{tabular}[c]{@{}l@{}}Pythagorean Theorem, Trigonometry, Triangle, Triangle Inequality, Similar Triangles, \\ Circle, Geometry, Triangle Angle Sum, Angle Bisector Theorem, Trigonometric Ratios\end{tabular}                  \\ \midrule \midrule
\multirow{3}{*}{Inter-Algebra} & In-context            & Factoring, Solve Equation, Add, Mul, Sub, Complex Number, Inequality, Quadratic Formula, Div, Exp                                                                                                                                   \\ \cmidrule{2-3}
                               & Internal        & \begin{tabular}[c]{@{}l@{}}Substitution, Completing the Square, Polynomial, Logarithm, AM-GM Inequality, \\ Polynomial Division, Absolute Value, Summation, Sequences, Simplify\end{tabular}                                        \\ \midrule \midrule
\multirow{3}{*}{Algebra}       & In-context            & Add, Mul, Solve Equation, Sub, Div, Exp, Factoring, Quadratic Formula, Radicals, Distance Formula                                                                                                                                   \\ \cmidrule{2-3}
                               & Internal        & \begin{tabular}[c]{@{}l@{}}Absolute Value, Slope, Logarithm, Arithmetic Sequence, Completing the Square, Interval Notation, \\ Inverse Function, Substitution, Midpoint Formula, Ceiling Function\end{tabular}                      \\ \midrule \midrule
\multirow{3}{*}{Probability}   & In-context            & Factorial, Combination, Counting Principle, Probability, Add, Sub, Permutations, Mul, Div, Exp                                                                                                                                      \\ \cmidrule{2-3}
                               & Internal        & \begin{tabular}[c]{@{}l@{}}Simplify Fraction, Binomial Theorem, Expected Value, Arithmetic Sequence, Sum of Arithmetic Series, \\ Counting, Stars and Bars, Divisibility Rules, Binomial Probability, Perfect Squares\end{tabular}  \\ \midrule \midrule
\multirow{3}{*}{Pre-Calculus}  & In-context            & Solve Equation, Add, Mul, Sub, Complex Number, Div, Factoring, Radicals, Area, Distance Formula                                                                                                                                     \\ \cmidrule{2-3}
                               & Internal        & \begin{tabular}[c]{@{}l@{}}Trigonometric Identities, Trigonometry, Dot Product, Matrix Multiplication, Pythagorean Theorem, \\ Cross Product, Inverse Trigonometric Functions, Determinant, Vector Projection, Vectors\end{tabular} \\ \midrule \midrule
\multirow{3}{*}{NumTheory}     & In-context            & Add, Mod, Base Conversion, Mul, Congruences, Div, Sub, Factoring, Prime Number, GCD                                                                                                                                                 \\ \cmidrule{2-3}
                               & Internal        & \begin{tabular}[c]{@{}l@{}}Divisors, Divisibility Rules, Units Digit, Prime Fraction, Chinese Remainder Theorem, Arithmetic \\ Sequence, Exponents, Cyclic Patterns, Perfect Squares, Modular Arithmetic\end{tabular}    \\ \bottomrule           
\end{tabular}
}
\end{table}



\begin{table}[t]
\caption{Testing accuracy and internal skill activation rate on the MATH benchmark. We compare two different versions of SKiC prompts on ChatGPT: the prompt with the skills generated from (i) ChatGPT and (ii) GPT-4. The \emph{internal skill activation rate} refers to the average proportion of skills utilized per question that originate from pre-trained knowledge (i.e., internal skills) rather than from the SKiC prompt context (i.e., the in-context skills).} \label{Tab:math_ablation_results}
\centering
% \tiny
\scalebox{0.72}{
\begin{tabular}{c|c|ccccccc|c} \toprule
    \textbf{Metric}   &\textbf{Source of SKiC} & \textbf{Pre-Algebra}   & \textbf{Geometry}      & \textbf{Inter-Algebra} & \textbf{Algebra}       & \textbf{Probability}   & \textbf{Pre-Calculus}  & \textbf{NumTheory}     & \textbf{Overall}        \\  \midrule  \midrule


\multirow{2}{*}{Accuracy} &GPT-4        & 60.7          & 27.8          & 16.8          & \textbf{58.2}          & 33.3          & 19.0          & 34.2          & 38.9          \\  

& ChatGPT  & \textbf{62.0} & \textbf{30.1} & \textbf{17.8} & 57.9 & \textbf{38.2} & \textbf{23.0} & \textbf{35.5} & \textbf{40.6} \\ \midrule 

\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Internal Skill \\ Activation Rate\end{tabular}} &GPT-4        & 5.9 & 18.5 & 11.2 & \textbf{6.6} & 7.0 &43.8 &6.2 & 12.5   \\   
        
    &ChatGPT               & \textbf{6.5} & \textbf{19.0} & \textbf{13.2} & 5.7 & \textbf{9.1} & \textbf{45.2} & \textbf{7.8} & \textbf{14.9} \\ \bottomrule
\end{tabular}
}
\end{table}




\paragraph{Ablation Study: different sources of the in-context skills} One important question we want to understand is whether it is beneficial to generate the in-context skills from the same foundation model used for prediction? Our hypothesis is that in-context skills generated from the same foundation model can initiate stronger synergize with the internal knowledge, due to their higher alignment. To test this hypothesis, we prompt the ChatGPT model using the SKiC prompt constructed from GPT-4 (i.e., the in-context skills are generated by GPT-4). The accuracy and the internal skill activation rate on MATH test set are reported in Table~\ref{Tab:math_ablation_results}. With the skills prompted from itself, we observe both improved accuracy and higher internal skill activation rate, even though the skills prompted from GPT-4 generally have higher quality. This suggests that (i) aligning the model that is used to prompt the in-context skills and the model that is used to generate answers helps the models' capability to link and utilize the internal pretrained skills, and (ii) activating more internal skills generally leads to higher performance gains, especially when solving complex problems that require compositions over a wide range of basic skills.


 


\subsection{Error Analysis}


\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/errors.png}
    \caption{Distributions of different types of errors in Multiplication, Question Answering, GSM8K and MATH tasks.}
    \label{fig:errors}
\end{figure}



We further perform error analysis on the tasks that are still far away from achieving (nearly) perfect generalization when applying SKiC prompting on ChatGPT --- multiplication, question answering, GSM8K and MATH. For each task, we randomly sample 50 error cases \footnote{For MATH dataset, we randomly sample 5 error cases per category, resulting in 35 error cases in total.} \citep{zhou2022least} and conduct an examination of the types of errors involved. We summarize five types of common errors: (i) seen basic skills: errors arise due to a lack of mastery of the skills in context, (ii) unseen basic skills: errors caused by the absence of necessary skills in context, particularly when these skills do not exist in the pre-trained knowledge of the LLM, (iii) incorrect composition: errors of incorrect composition or reasoning using the basic skills, (iv) incorrect copying: copying or merging errors between different steps, (v) others: other errors such as incorrect ground truth labels in the test set. 



Their distributions are visualized in Figure~\ref{fig:errors}. We observe that (i) the most common errors arise from unseen basic skills (for example, 83\% of the errors in the Multiplication task are due to the absence of the skill to add large numbers), (ii) a lack of mastery of the basic skills leads to more errors when there are more complex or more basic skills to be used (for example, the question decomposition capability in the CommaQA-E task is generally a complex skill, and the GSM8K and MATH dataset requires more basic skills), (iii) incorrect composition is a major error type for tasks that require more complex reasoning steps such as GSM8K (e.g., 45\% of the errors are due to incorrect reasoning steps such as misinterpreting the questions or incorrectly reasoning about the questions), (iv) copying errors become more prevalent when there are more reasoning steps with longer context, and (v) math reasoning generally requires a wider variety of skill compositions, and the way of composition varies significantly from one problem to another, making it considerably harder to master the appropriate skill composition for each problem. Therefore, there are several key directions to further improve SKiC: (1) providing the model with high-quality basic skills and illustrations to improve the execution quality of these basic skills, (2) expanding the range of task-related basic skills to prevent errors caused by unseen skill, (3) providing more examples of how to compose basic skills, especially for more complex tasks, and (4) utilizing better foundation models that can avoid copying errors in long context and that have a more extensive set of well-mastered skills in their pre-existing pretrained knowledge.






