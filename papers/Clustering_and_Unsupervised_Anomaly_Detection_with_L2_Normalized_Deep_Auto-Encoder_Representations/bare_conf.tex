
%% bare_conf.tex
%% V1.4b
%% 2015/08/26
%% by Michael Shell
%% See:
%% http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.8b or later) with an IEEE
%% conference paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/pkg/ieeetran
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE! 
%% User assumes all risk.
%% In no event shall the IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%*************************************************************************


% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. The IEEE's font choices and paper sizes can   ***
% *** trigger bugs that do not appear when using other class files.       ***                          ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/



\documentclass[conference]{IEEEtran}
\usepackage{adjustbox}
\usepackage{amssymb,amsmath}
\usepackage{subcaption}
\usepackage{graphicx}
% Some Computer Society conferences also require the compsoc mode option,
% but others use the standard conference format.
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[conference]{../sty/IEEEtran}





% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/pkg/ifpdf
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.






% *** CITATION PACKAGES ***
%
%\usepackage{cite}
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of the IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off
% such as if a citation ever needs to be enclosed in parenthesis.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 5.0 (2009-03-20) and later if using hyperref.sty.
% The latest version can be obtained at:
% http://www.ctan.org/pkg/cite
% The documentation is contained in the cite.sty file itself.






% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  % \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation
% can be obtained at: 
% http://www.ctan.org/pkg/graphicx
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found at:
% http://www.ctan.org/pkg/epslatex
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). The IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex





% *** MATH PACKAGES ***
%
%\usepackage{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics.
%
% Note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/amsmath





% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as the IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/pkg/algorithms
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/pkg/algorithmicx




% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/array


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.




% *** SUBFIGURE PACKAGES ***
%\ifCLASSOPTIONcompsoc
%  \usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
%\else
%  \usepackage[caption=false,font=footnotesize]{subfig}
%\fi
% subfig.sty, written by Steven Douglas Cochran, is the modern replacement
% for subfigure.sty, the latter of which is no longer maintained and is
% incompatible with some LaTeX packages including fixltx2e. However,
% subfig.sty requires and automatically loads Axel Sommerfeldt's caption.sty
% which will override IEEEtran.cls' handling of captions and this will result
% in non-IEEE style figure/table captions. To prevent this problem, be sure
% and invoke subfig.sty's "caption=false" package option (available since
% subfig.sty version 1.3, 2005/06/28) as this is will preserve IEEEtran.cls
% handling of captions.
% Note that the Computer Society format requires a larger sans serif font
% than the serif footnote size font used in traditional IEEE formatting
% and thus the need to invoke different subfig.sty package options depending
% on whether compsoc mode has been enabled.
%
% The latest version and documentation of subfig.sty can be obtained at:
% http://www.ctan.org/pkg/subfig




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure.
% Be aware that LaTeX2e kernels dated 2015 and later have fixltx2e.sty's
% corrections already built into the system in which case a warning will
% be issued if an attempt is made to load fixltx2e.sty as it is no longer
% needed.
% The latest version and documentation can be found at:
% http://www.ctan.org/pkg/fixltx2e


%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/stfloats
% Do not use the stfloats baselinefloat ability as the IEEE does not allow
% \baselineskip to stretch. Authors submitting work to the IEEE should note
% that the IEEE rarely uses double column equations and that authors should try
% to avoid such use. Do not be tempted to use the cuted.sty or midfloat.sty
% packages (also by Sigitas Tolusis) as the IEEE does not format its papers in
% such ways.
% Do not attempt to use stfloats with fixltx2e as they are incompatible.
% Instead, use Morten Hogholm'a dblfloatfix which combines the features
% of both fixltx2e and stfloats:
%
% \usepackage{dblfloatfix}
% The latest version can be found at:
% http://www.ctan.org/pkg/dblfloatfix




% *** PDF, URL AND HYPERLINK PACKAGES ***
%
%\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/url
% Basically, \url{my_url_here}.




% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
%
% paper title
% Titles are generally capitalized except for words such as a, an, and, as,
% at, but, by, for, in, nor, of, on, or, the, to and up, which are usually
% not capitalized unless they are the first or last word of the title.
% Linebreaks \\ can be used within to get better formatting as desired.
% Do not put math or special symbols in the title.
\title{Clustering and Unsupervised Anomaly Detection with $l_2$ Normalized Deep Auto-Encoder Representations}


% author names and affiliations
% use a multiple column layout for up to three different
% affiliations
\author{\IEEEauthorblockN{Caglar Aytekin, Xingyang Ni, Francesco Cricri and Emre Aksu}
\IEEEauthorblockA{Nokia Technologies, Tampere, Finland\\
Corresponding Author e-mail: caglar.aytekin@nokia.com}
}

% conference papers do not typically use \thanks and this command
% is locked out in conference mode. If really needed, such as for
% the acknowledgment of grants, issue a \IEEEoverridecommandlockouts
% after \documentclass

% for over three affiliations, or if they all won't fit within the width
% of the page, use this alternative format:
% 
%\author{\IEEEauthorblockN{Michael Shell\IEEEauthorrefmark{1},
%Homer Simpson\IEEEauthorrefmark{2},
%James Kirk\IEEEauthorrefmark{3}, 
%Montgomery Scott\IEEEauthorrefmark{3} and
%Eldon Tyrell\IEEEauthorrefmark{4}}
%\IEEEauthorblockA{\IEEEauthorrefmark{1}School of Electrical and Computer Engineering\\
%Georgia Institute of Technology,
%Atlanta, Georgia 30332--0250\\ Email: see http://www.michaelshell.org/contact.html}
%\IEEEauthorblockA{\IEEEauthorrefmark{2}Twentieth Century Fox, Springfield, USA\\
%Email: homer@thesimpsons.com}
%\IEEEauthorblockA{\IEEEauthorrefmark{3}Starfleet Academy, San Francisco, California 96678-2391\\
%Telephone: (800) 555--1212, Fax: (888) 555--1212}
%\IEEEauthorblockA{\IEEEauthorrefmark{4}Tyrell Inc., 123 Replicant Street, Los Angeles, California 90210--4321}}




% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}




% make the title area
\maketitle

% As a general rule, do not put math, special symbols or citations
% in the abstract
\begin{abstract}
Clustering is essential to many tasks in pattern recognition and computer vision. With the advent of deep learning, there is an increasing interest in learning deep unsupervised representations for clustering analysis. Many works on this domain rely on variants of auto-encoders and use the encoder outputs as representations/features for clustering. In this paper, we show that an $l_2$ normalization constraint on these representations during auto-encoder training, makes the representations more separable and compact in the Euclidean space after training. This greatly improves the clustering accuracy when $k$-means clustering is employed on the representations. We also propose a clustering based unsupervised anomaly detection method using $l_2$ normalized deep auto-encoder representations. We show the effect of $l_2$ normalization on anomaly detection accuracy. We further show that the proposed anomaly detection method greatly improves accuracy compared to previously proposed deep methods such as reconstruction error based anomaly detection. 
\end{abstract}

% no keywords




% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle



\section{Introduction}
% no \IEEEPARstart
Cluster analysis is essential to many applications in computer vision and pattern recognition. 
Given this fact and the recent advent of deep learning, there is an increasing interest in learning deep unsupervised representations for clustering analysis \cite{DEC,IDEC,GMVAE,DCEC}. 
Most of the methods that perform clustering on deep representations, make use of auto-encoder representations (output of the encoder part) and define clustering losses on them. 
The focus of previous works have been on the choice of the auto-encoder type and architecture and the clustering loss. 
In DEC \cite{DEC}, first a dense auto-encoder is trained with minimizing reconstruction error. 
Then, as a clustering optimization stage, the method iterates between computing an auxiliary target distribution from auto-encoder representations and minimizing the Kullback-Leibler divergence to it. 
In IDEC \cite{IDEC}, it is argued that the clustering loss of DEC corrupts the feature space, therefore IDEC proposes to jointly optimize the clustering loss and reconstruction loss of the auto-encoder. 
DCEC \cite{DCEC} argues the inefficiency of using dense auto-encoders for image clustering, therefore adopts a convolutional auto-encoder and shows that it improves the clustering accuracy of DEC and IDEC. 
GMVAE \cite{GMVAE} adopts a variational auto-encoder in order to learn unsupervised representations and simply applies K-means clustering on representations.

In this manuscript, we show that regardless of the auto-encoder type (dense or convolutional), constraining the auto-encoder representations to be on the unit-ball, i.e. to be $l_2$ normalized, during auto-encoder training, greatly improves the clustering accuracy. 
%Moreover, we show that this constraint eliminates the need to define an additional clustering loss. 
We show that a simple $k$-means clustering on the auto-encoder representations trained with our constraint already gives improved accuracy with a large margin compared to baselines with or without additional clustering losses.
Motivated by the high performance of our clustering method on deep representations, we propose an unsupervised anomaly detection method based on this clustering.
We show that our anomaly detection method greatly improves on other deep anomaly detection strategies such as reconstruction error based ones.
We also investigate the effect of $l_2$ normalization constraint during training on the anomaly detection accuracy and show that it leads to superior results compared to not applying the constraint.
%Mention somewhere that training autoencoder as is and then applyinh l2 normaliztion before kmeans gives inferior results.

\section{Related Work}
\subsection{Deep Unsupervised Anomaly Detection}

Unsupervised anomaly detection tries to find anomalies in the data without using any annotation \cite{ANOM}. 
Recently, deep learning methods have also been used for this task \cite{DRAE, AVAE}. These works train auto-encoders on the entire data and use reconstruction loss as an indicator of anomaly. 
%ADGAN uses generative adversarial networks and defines an anomalous sample as the one for which there exists no good representation in the latent space of the generator. 
DRAE \cite{DRAE} trains auto-encoders and uses reconstruction error as an anomaly indicator. 
Moreover, DRAE proposes a method to make the reconstruction error distributions of the normal and abnormal classes even more separable so that it is easier to detect anomalies. 
AVAE \cite{AVAE} trains both conventional and variational auto-encoders and use reconstruction error as an anomaly indicator. 

The general assumption of the above works is that since the anomaly data is smaller in ratio than the normal data, the auto-encoder would not learn to reconstruct it accurately. 

The above assumption seems to work in a specific definition of anomaly where the normal samples are drawn from a single class only and anomaly classes have been selected from many other classes \cite{DRAE}.
However, the assumption fails in another anomaly type where the normal samples are drawn from multiple classes and anomaly class is sampled from a specific class \cite{AVAE}.

In this paper we propose an unsupervised anomaly detection method based on clustering on deep auto-encoder representations and show that it gives a superior performance than reconstruction error based anomaly.


\begin{figure*}

\centering
  \subcaptionbox{No normalization\label{fig1:a}}{\includegraphics[width=2.3in]{normal.png}}
  \subcaptionbox{$l_2$ normalization after training\label{fig1:b}}{\includegraphics[width=2.3in]{normall2.png}}
  \subcaptionbox{$l_2$ normalization during training\label{fig1:c}}{\includegraphics[width=2.3in]{l2.png}}
  
\caption{Illustration of t-SNE encoding of auto-encoder representations for MNIST dataset to two dimensions. Best viewed in color.} \label{fig:1}
\end{figure*}

\subsection{Regularization and Normalization in Neural Networks}

Due to the high number of parameters, neural networks have a risk of over-fitting to the training data. 
This sometimes reduces the generalization ability of the learned network.
In order to deal with over-fitting, mostly regularization methods are employed.
One of the most widely used regularization technique is weight norm regularization. 
Here the aim is to add an additional regularization loss to the neural network error, which gives high penalty to weights that have high norms.
Both $l_1$ and $l_2$ norm can be exploited.

Recently some normalization techniques for neural networks emerged such as \cite{BNORM, LNORM}.
Batch normalization \cite{BNORM}, aims to find a statistical mean and variance for the activations which are calculated and updated according to batch statistics.
The activations are normalized according to these statistics.
In layer normalization \cite{LNORM}, the mean and variance are computed from all of the summed inputs to the neurons on a layer on a single training sample. 
This overcomes the batch-size dependency drawback of batch-normalization.
Although these methods were mainly proposed as tricks to make the neural network training faster by conditioning each layer's input, it is argued that they may also have a regularization effect due to their varying estimations of parameters for standardization at each epoch.

In our proposed method, the unit ball constraint that we put on activations is a normalization technique.
However, unlike layer or batch normalization, the unit ball constraint is parameter-free as it simply sets the norm of each activation vector to 1.
Therefore, it is free from the parameter estimation stochasticity.
Yet, it may still act as a regularization method due to its hard constraint on some activations to be of fixed norm.
This slightly resembles the $l_2$ norm regularization.
A key difference is that in $l_2$ norm regularization, the norm is of the weights, but in our case it is applied on the activations.
Another key difference is that we fix the activation norms to 1, whereas $l_2$ norm regularization penalizes to large weight norms and does not fix the norms to any value.
%We will show that $l_2$ normalization works much better than batch and layer normalization in terms of the clustering accuracy, when we apply k-means on the normalized activations.



\section{Proposed Method}

\subsection{Clustering on $l_2$ Normalized Deep Auto-Encoder Representations}

We represent the auto-encoder representations for the input $I$ as $E(I)$ and the reconstructed input as the $D(E(I))$. The representations are generally obtained via several dense or convolutional layers applied on the input $I$. In each layer, usually there are filtering and an activation operations, and optionally pooling operations. Let $f_i$ be the computations applied to the input at layer $i$, then the encoded representations for an $n$-layer encoder are obtained as in Eq. \ref{eqn1}.

\begin{equation}
\label{eqn1}
E(I)=f_n(f_{n-1}(...f_1(I)))
\end{equation}

The reconstruction part of the auto-encoder applies on $E(I)$ and is obtained via several dense or deconvolutional layers. In each layer, usually there are filtering and activation operations and optionally un-pooling or up-sampling operations. Let $g_i$ be the computations applied to the auto-encoder representations, then the reconstructed signal for an $m$-layer decoder is obtained as as in Eq. \ref{eqn2}.

\begin{equation}
\label{eqn2}
D(E(I))=g_m(g_{m-1}(...g_1(E(I))))
\end{equation}

The auto-encoder training is conducted in order to reduce the reconstruction error (loss) given in Eq. \ref{eqn3}.

\begin{equation}
\label{eqn3}
L=\frac{1}{|J|}\sum_{j \in J} (I_j-D(E(I_j)))^2
\end{equation}

%In order to reduce the over-fitting which is a general problem with deep learning methods having millions of parameters, there are commonly adopted regularization strategies. 
%For auto-encoders, regularization on weight norms are widely used, where the loss function is updated as follows.
%
%\begin{equation}
%\label{eqn4}
%L=\frac{1}{J}\sum_j (I_j-D(I_j))^2+\lambda\sum_l |w_l|_p
%\end{equation}
%
%In Eq. \ref{eqn4}, $w_l$ are the weights in layer $l$, $p$ is the norm order and $\lambda$ controls the contribution of weight norm regularization in the total loss. Usually, $p$ is either selected as 1 or 2.

Here, we propose an additional step conducted on the auto-encoder representations $E(I)$. 
In particular, we apply $l_2$ normalization on $E(I)$. 
This corresponds to adding a hard constraint on the representations to be on the unit ball.
%This is not to be confused with the weight regularization discussed above as there are two main differences.
%First, in weight regularization the aim is to keep the norms of the weights as small as possible, whereas we propose to normalize the activations/representations -not the weights- such that they have a fixed norm of 1.
The loss function with our introduced constraint can then be written as in Eq. \ref{eqn4}, where $L_c$ and $E_c$ are loss and encoded representations with our introduced constraint.

\begin{figure*}

\centering
  \subcaptionbox{Clustering Method\label{method:a}}{\includegraphics[width=3.2in]{fig1.png}}
  \hfill
\subcaptionbox{Normality Score Method\label{method:b}}{\includegraphics[width=3.6in]{fig2.png}}
  
\caption{Illustration of proposed methods in inference phase.}
\end{figure*}


\begin{equation}
\label{eqn4}
\begin{aligned}
L_c=\frac{1}{|J|}\sum_{j \in J} (I_j-D(E_c(I_j)))^2 ,\\
E_c(I)=\frac{E(I)}{\lVert E(I) \rVert_2}
\end{aligned}
\end{equation}


%Our motivation with the above step is two-fold. 
%First, we believe that enforcing the unit norm on representations would act as a regularization for the entire auto-encoder. 
We believe that $l_2$ normalized features (representations) are more suitable for clustering purposes, especially for the methods that use Euclidean distance such as conventional $k$-means. 
This is because the distances between the vectors would be independent of their length, and would instead depend on the angle between the vectors. 
As a positive side effect, enforcing the unit norm on representations would act as a regularization for the entire auto-encoder.

In Fig. \ref{fig:1}, we illustrate t-SNE \cite{TSNE} encoding (to 2 dimensions) of the auto-encoder representations of networks with same architecture. 
All auto-encoders were trained on MNIST \cite{MNIST} dataset training split. 
The representations corresponding to Fig. \ref{fig1:a} are from the auto-encoder that was trained by the loss in Eq. \ref{eqn3}.
The same representations with $l_2$ normalization applied after training are illustrated in Fig. \ref{fig1:b}.
Finally, the representations with $l_2$ constraint during training , i.e. training with loss Eq. \ref{eqn4}, are illustrated in Fig. \ref{fig1:c}.

It is observed from the figures that the $l_2$ normalization during training (Fig. \ref{fig1:c}) results into more separable clusters. One example is the distributions of digit 7 in MNIST dataset. Note that the numbers are indicated with color codes where the color bar is available in Fig. \ref{fig:1}. It is clearly observed that with no normalization during training (Fig. \ref{fig1:a}), digit 7 is divided into 2 parts where the small part is surrounded by 8,9,6 and 2 digits. With normalization applied after training (Fig. \ref{fig1:b}) this effect becomes even more evident. So, we clearly observe here that applying normalization after training does not help at all. But, with $l_2$ normalization constraint during training (Fig. \ref{fig1:c}), we see a clear separation of digit 7 as a single cluster from the rest of the numbers. Moreover, it can be observed that the clusters are more compact in Fig. \ref{fig1:c} compared to others.


After training the auto-encoder with the loss function in Eq. \ref{eqn4}, the clustering is simply performed by $k$-means algorithm. No more clustering loss is applied. Our clustering method is illustrated in Fig. \ref{method:a}.



%We would like to emphasize here that performing $l_2$ normalization on the representations of an auto-encoder which is trained without constraint should not be confused by representations of an auto-encoder which is trained with the $l_2$ normalization constraint on the representations. The prior corresponds to applying the loss in Eq. \ref{eqn3} during training and then performing $l_2$ normalization on representations, whereas the latter corresponds to applying the loss in Eq. \ref{eqn4} during training. In the latter one, the representations are already $l_2$ normalized. In order to emphasize this difference we also illustrate the t-SNE encoding (to 2 dimensions) of the auto-encoder representations of performing $l_2$ normalization after the auto-encoder training in Fig. \ref{fig1:b}. It can be observed that $l_2$ normalization constraint during training leads to much more separable and compact clusters. Performing $l_2$ normalization after plain auto-encoder training may result into even worse clusters compared to no normalization case. This is visible from the above example of cluster corresponding to digit 7.





%\centering
%
%\begin{subfigure}{0.3\textwidth}
%\includegraphics[width=\linewidth]{normal.png}
%\caption{No normalization} \label{fig:1a}
%\end{subfigure}
%\hspace*{\fill} % separation between the subfigures
%\begin{subfigure}{0.3\textwidth}
%\includegraphics[width=\linewidth]{normall2.png}
%\caption{$l_2$ normalization after training} \label{fig:1b}
%\end{subfigure}
%\hspace*{\fill} % separation between the subfigures
%\begin{subfigure}{0.3\textwidth}
%\includegraphics[width=\linewidth]{l2.png}
%\caption{$l_2$ normalization during training} \label{fig:1c}
%\end{subfigure}
%\caption{Illustration of t-SNE encoding of auto-encoder representations to two dimensions.} \label{fig:1}
%\end{figure*}



\subsection{Unsupervised Anomaly Detection using $l_2$ Normalized Deep Auto-Encoder Representations} 
\label{anomalymethod}
Here, we propose a clustering based unsupervised anomaly detection.
We train an auto-encoder on the entire dataset including normal and abnormal samples and no annotation or supervision is used.
The auto-encoder is simply trained with the loss in Eq. \ref{eqn4}.
After training, the $l_2$ normalized auto-encoder representations are clustered with $k$-means algorithm.
We assume the anomaly cases to be considerably smaller in number than any normal clusters.
Note that this assumption does not put any constraint on the dataset, but it simply follows the general definition of anomaly.
Therefore, the centroids obtained by the $k$-means method can be considered to be representations of normal clusters by some errors that are caused by anomaly samples in the dataset.
To each sample $i$, we assign the normality score $v_i$ in Eq. \ref{eqn5}.

\begin{equation}
\label{eqn5}
v_i=\max_{j} (E_c(I_i) \cdot \frac{C_j}{\lVert C_j \rVert_2})
\end{equation}
In Eq. \ref{eqn5}, $C_j$ is a cluster centroid and $\cdot$ is the dot product operator.
Notice that we $l_2$ normalize the cluster centroids.
Since representations $E_c(I_i)$ are already $l_2$ normalized, $v_i \in [0,1]$ holds.
The measure in Eq. \ref{eqn5} is intuitive considering that we expect high similarities of normal samples to normal classes.
Our normality scoring method is illustrated in Fig. \ref{method:b}.

The normality score can be used for anomaly detection in a straightforward manner.
Simply, the abnormal samples can be detected as the ones having $v_i<\tau$ , where $\tau \in [0,1]$ is a threshold.


\section{Experimental Results}
\subsection{Clustering}

\subsubsection*{Evaluation Metrics}
We use the widely used evaluation for unsupervised clustering accuracy \cite{DEC} given in Eq. \ref{acc}.
\begin{equation}
\label{acc}
acc=\max_m \frac{\sum_{i=1}^n \pmb{1} \{l_i=m(c_i) \}}{n}
\end{equation}
In Eq. \ref{acc}, $l_i$ is the ground truth labeling of sample $i$, $c_i$ is the cluster assignment according to the one-to-one mapping defined by $m$, where $m$ ranges over all possible one-to-one mappings between clusters generated by the algorithm and the ground truth labels.
The maximization in Eq. \ref{acc} can be performed by Hungarian algorithm \cite{HUN}.

We compare the clustering accuracy of auto-encoder representations with and without $l_2$ normalization constraint.
We make this comparison in dense and convolutional auto-encoders.
For dense auto-encoder, we use MNIST \cite{MNIST} and for convolutional auto-encoders, we use MNIST \cite{MNIST} and USPS \cite{USPS} datasets. 
This is due to the availability of results in the works that use dense and convolutional auto-encoders.

For dense auto-encoder, we use the network structure which is used both in DEC \cite{DEC} and IDEC \cite{IDEC}. 
In encoding part there are 4 layers with $500-500-2000-10$ hidden neurons and in decoding part there are $2000-500-500-d$ neurons, where $d$ is the dimension of the input. 
We re-implement the auto-encoder training and with leaky relu \cite{LRELU} activations after each hidden layer except for the last one and trained the auto-encoder end to end for 100 epochs. 
We select the best model with lowest reconstruction error.
As it can be observed from Table \ref{AE}, we obtain a very similar clustering accuracy when we apply $k$-means on auto-encoder representations compared to the original paper of DEC \cite{DEC}. 
Note here that results indicated with * corresponds to our own implementation.
Other results for baselines are borrowed from original papers.
Table \ref{AE} shows that when we train the auto-encoder with our $l_2$ normalization constraint on the representations, we achieve a much better clustering accuracy when we apply $k$-means on the representations. 
We denote our method as AE-$l_2$ which stands for auto-encoder with $l_2$ normalization.
Moreover, our clustering accuracy is even better than the methods that define a separately designed clustering loss on the representations (DEC and IDEC).

Next, we make experiments for the convolutional auto-encoder. 
For this, we make use of the model structure introduced in DCEC \cite{DCEC}. 
This model consists of  5x5, 5x5 and 3x3 convolutional filters in the encoding layers respectively. 
There are 32,64 and 128 filters in encoding layers respectively. 
Convolutions are applied with 2x2 strides and with relu \cite{RELU} activations. 
After the convolutional layers, the activations are flattened and there is a dense layer of dimension 10. 
This is followed by another dense layer and reshaping. 
Decoder part consists of 64,32 and 1 deconvolutional filters  of size 3x3, 5x5 and 5x5 respectively. 
Relu activations were applied after each convolution, except for the last one.
The network was trained for 200 epochs as in the original paper of DCEC \cite{DCEC}. 
In Table \ref{CAE}, we show clustering accuracy of $k$-means applied on convolutional autoencoder representations. 
We were able to obtain similar results as in the original paper (DCEC). 
Note here that results indicated with * corresponds to our own implementation.
Other results for baselines are borrowed from original papers.
It can be observed from Table \ref{CAE} that when we train the convolutional autoencoder with our $l_2$ normalization constraint on representations, we achieve a much better performance. 
We denote our method as CAE-$l_2$ which stands for convolutional auto-encoder with $l_2$ normalization.
Our performance is superior to DCEC which introduces additional clustering loss.


\begin{table}[!t]
% increase table row spacing, adjust to taste

\caption{Clustering on Dense Auto-Encoder Representations}
\label{AE}
\centering
% Some packages, such as MDW tools, offer better commands for making tables
% than the plain LaTeX2e tabular which is used here.
\begin{adjustbox}{width=0.47\textwidth}

\begin{tabular}{|c|c|c|c|c|c|}
\hline
&AE* & AE & DEC & IDEC & AE-$l_2$\\ 
&$k$-means & $k$-means &  &  & $k$-means\\ 
\hline
MNIST&81.43 & 81.82 & 86.55 & 88.06 & \textbf{90.20}\\

\hline
\end{tabular}
\end{adjustbox}
\end{table}


\begin{table}[!t]
% increase table row spacing, adjust to taste

\caption{Clustering on Convolutional Auto-Encoder Representations}
\label{CAE}
\centering
% Some packages, such as MDW tools, offer better commands for making tables
% than the plain LaTeX2e tabular which is used here.
\begin{adjustbox}{width=0.4\textwidth}

\begin{tabular}{|c|c|c|c|c|}
\hline
&CAE* & CAE & DCEC & CAE-$l_2$\\ 
&$k$-means & $k$-means &  & $k$-means\\ 

\hline
MNIST&84.83 & 84.90 & 88.97 & \textbf{95.11}\\
\hline
USPS&73.521 & 74.15 & 79.00 & \textbf{91.35}\\
\hline
\end{tabular}
\end{adjustbox}

\end{table}

\
\subsubsection*{$l_2$ versus Batch and Layer Normalization}

Due to $l_2$ normalization step in our clustering method, we compare it with applying other normalization techniques training. 
In particular we train two separate networks by using batch \cite{BNORM} and layer \cite{LNORM} normalization, instead of $l_2$ normalization.
All other setup for the experiments are the same.
Batch size of 256 is used for all methods in order to have a large enough batch for batch normalization.
Our method performs superior to both baselines by a large margin, as the accuracies in Table \ref{regul} indicate.
More importantly it is noticed that neither batch nor layer normalization provides a noticeable accuracy increase over the baseline (CAE+$k$-means).
Moreover in MNIST dataset, layer and batch normalization results into a significant accuracy decrease.
This is an important indicator showing that the performance upgrade of our method is not a result of a input conditioning, but it is a result of the specific normalization type that is more fit for clustering in Euclidean space.

\begin{table}[!t]
% increase table row spacing, adjust to taste

\caption{Comparison of Normalization Methods}
\label{regul}
\centering
% Some packages, such as MDW tools, offer better commands for making tables
% than the plain LaTeX2e tabular which is used here.
\begin{adjustbox}{width=0.4\textwidth}

\begin{tabular}{|c|c|c|c|}
\hline
 & batch-norm & layer-norm & $l_2$-norm\\ 
\hline
MNIST & 70.67  & 70.83 & \textbf{95.11}\\
\hline
USPS & 74.95 & 75.263 & \textbf{91.35}\\
\hline
\end{tabular}
\end{adjustbox}
\end{table}



\subsection{Anomaly Detection}

\subsubsection*{Evaluation Metrics}
An anomaly detection method often generates an anomaly score, not a hard classification result. 
Therefore, a common evaluation strategy in anomaly detection is to threshold this anomaly score and form a receiver operating curve where each point is the true positive and false positive rate of the anomaly detection result corresponding to a threshold. 
Then, the area under the curve (AUC) of RoC curve is used as an evaluation of the anomaly detection method \cite{ANOM}. 

Here, we evaluate our method introduced in Section \ref{anomalymethod}.
The evaluation setup and implementation of our method are as follows.
In MNIST training dataset, we select a digit class as anomaly class and keep a random 10$\%$ of that class in the dataset while the remaining 90$\%$ is ignored. 
We leave the rest of the classes as is. 
Then, we use the convolutional autoencoder structure in DCEC \cite{DCEC} and train it with our $l_2$ normalization constraint on representations. 
Finally, we apply $k$-means clustering on the representations and keep the centroids. 
In our experiments we use k=9 for $k$-means, since we assume that we know the number of normal classes in the data. 
For MNIST test dataset, we calculate the auto-encoder representations. 
As a normality measure for each sample, we calculate the corresponding representation's maximum similarity to pre-calculated cluster centroids as in \ref{eqn5}.
It should be noted that we repeat the above procedure by choosing a different class to be anomaly class, for all possible classes. 

We also evaluate two baselines. 
In the first baseline, we exactly repeat the above procedure, but without $l_2$ normalization constraint on representations. 
In the second baseline, again we train the auto-encoder with $l_2$ normalization constraint on representations.
Then, on the test set, we calculate the reconstruction error per sample and define that as anomaly score.
Using reconstruction error based anomaly detection follows the works in AVAE \cite{AVAE} and DRAE \cite{DRAE}.
The setups in AVAE and DRAE are different than ours. 
In AVAE, the training is only conducted on normal data, so the method is not entirely unsupervised in that sense.
In DRAE, the anomaly definition is different: only a single class is kept as normal and samples from other classes are treated as anomaly.
That setup presents a much easier case and therefore reconstruction error based anomaly detection produces acceptable results.
Next, we show that in our setup this is not the case.

For each method we plot a RoC curve via thresholding the normality (or anomaly) score with multiple thresholds. 
Then, we evaluate the area under the RoC curve (AUC) for measuring the anomaly detection performance. 
The training and test datasets for all methods are the same. 
Due to the random selection of 10$\%$ of the anomaly class to be kept, performance can change according to the partition that is randomly selected. Therefore, we run the method 10 times for different random partitions and report the mean AUC.

It can be observed from Table \ref{anom} that our clustering based anomaly detection method drastically outperforms the reconstruction error based anomaly detection for CAE neural network structure. 

It is worth noting here an interesting observation from Table \ref{anom}: for digits 1, 7 and 9, reconstruction error based anomaly detection gets a very inferior performance. 
This is most evident in digit 1. 
The reason for this is that the digit 1 is very easy to reconstruct (only 1 stroke) and even though an auto-encoder is trained on much less examples of this digit, it can reconstruct it quite well. 
This shows a clear drawback of the reconstruction error based anomaly detection. 
However, in our clustering based method, we achieve a very high accuracy in all the digits.

The effect of our proposed $l_2$ normalization constraint on representations during training can also be observed from Table \ref{anom}.
In $9/10$ cases, i.e. digits selected as anomaly, anomaly detection with the network trained with $l_2$ normalization constraint on representations performs much better than the one without. 
Only in digit $9$, we observe an inferior accuracy of our method.
Compared to other digits, we observe less performance for digits 4 and 9. 
We argue that this might be happening due to very similar appearance of these digits in some handwritings.
Therefore, the method may confuse these numbers with each other during clustering.

\begin{table}[!t]
% increase table row spacing, adjust to taste

\caption{Anomaly Detection with Auto-Encoder Representations}
\label{anom}
\centering
% Some packages, such as MDW tools, offer better commands for making tables
% than the plain LaTeX2e tabular which is used here.
\centering

\begin{adjustbox}{width=0.35\textwidth}

\begin{tabular}{|c|c|c|c|c|}
\hline
Anom.  & CAE. & CAE & CAE-$l_2$\\ 
Digit  & (recons) & (cluster) & (cluster)\\ 
\hline
0 &  0.7025 & 0.7998 & \textbf{0.9615} \\
\hline
1 &  0.0782 & 0.8871 & \textbf{0.9673}\\
\hline
2 &  0.879 & 0.7512 & \textbf{0.9790}\\
\hline
3 & 0.8324 & 0.8449 & \textbf{0.9382}\\
\hline
4 & 0.7149 & 0.4988 & \textbf{0.7825}\\
\hline
5 &  0.8359 & 0.7635 & \textbf{0.9136}\\
\hline
6 & 0.6925 & 0.7896 & \textbf{0.9497}\\
\hline
7 &  0.5767 & 0.7421 & \textbf{0.9100}\\
\hline
8 &  0.8912 & 0.9200 & \textbf{0.9237}\\
\hline
9 &  0.514 & \textbf{0.8944} & 0.7495\\
\hline
\end{tabular}
\end{adjustbox}
\end{table}


In Table \ref{anom2}, we compare our method to another method \cite{AVAE} that performs reconstruction error based anomaly detection, but using dense auto-encoders. 
There is also a variational auto-encoder based version of the method. 
It should be noted that this method trains auto-encoders only on normal data. 
This presents a much easier task compared to our case where we also include anomalous samples during training. 
Thus our case is entirely unsupervised.
Still, $9/10$ cases, our method outperforms both variants of the method with a large margin.
Only in digit 4, we observe an inferior performance of our method compared to VAE method. 

\begin{table}[!t]
% increase table row spacing, adjust to taste

\caption{Anomaly Detection with Auto-Encoder Representations}
\label{anom2}
\centering
% Some packages, such as MDW tools, offer better commands for making tables
% than the plain LaTeX2e tabular which is used here.
\begin{adjustbox}{width=0.35\textwidth}

\begin{tabular}{|c|c|c|c|c|}
\hline
Anom. & AE. & VAE. & CAE-$l_2$\\ 
(Digit) & (recons.) & (recons.) & cluster\\ 
\hline
0 & 0.825 & 0.917& \textbf{0.9615} \\
\hline
1 & 0.135 & 0.136& \textbf{0.9673}\\
\hline
2 & 0.874 & 0.921  & \textbf{0.9790}\\
\hline
3 & 0.761 & 0.781  & \textbf{0.9382}\\
\hline
4 & 0.727 & \textbf{0.808} &  0.7825\\
\hline
5 & 0.792 & 0.862 &  \textbf{0.9136}\\
\hline
6 & 0.812 & 0.848 & \textbf{0.9497}\\
\hline
7 & 0.508 & 0.596 &\textbf{0.9100}\\
\hline
8 & 0.869 & 0.895  & \textbf{0.9237}\\
\hline
9 & 0.548 & 0.545  & \textbf{0.7495}\\
\hline
\end{tabular}
\end{adjustbox}

\end{table}


% An example of a floating figure using the graphicx package.
% Note that \label must occur AFTER (or within) \caption.
% For figures, \caption should occur after the \includegraphics.
% Note that IEEEtran v1.7 and later has special internal code that
% is designed to preserve the operation of \label within \caption
% even when the captionsoff option is in effect. However, because
% of issues like this, it may be the safest practice to put all your
% \label just after \caption rather than within \caption{}.
%
% Reminder: the "draftcls" or "draftclsnofoot", not "draft", class
% option should be used if it is desired that the figures are to be
% displayed while in draft mode.
%
%\begin{figure}[!t]
%\centering
%\includegraphics[width=2.5in]{myfigure}
% where an .eps filename suffix will be assumed under latex, 
% and a .pdf suffix will be assumed for pdflatex; or what has been declared
% via \DeclareGraphicsExtensions.
%\caption{Simulation results for the network.}
%\label{fig_sim}
%\end{figure}

% Note that the IEEE typically puts floats only at the top, even when this
% results in a large percentage of a column being occupied by floats.


% An example of a double column floating figure using two subfigures.
% (The subfig.sty package must be loaded for this to work.)
% The subfigure \label commands are set within each subfloat command,
% and the \label for the overall figure must come after \caption.
% \hfil is used as a separator to get equal spacing.
% Watch out that the combined width of all the subfigures on a 
% line do not exceed the text width or a line break will occur.
%
%\begin{figure*}[!t]
%\centering
%\subfloat[Case I]{\includegraphics[width=2.5in]{box}%
%\label{fig_first_case}}
%\hfil
%\subfloat[Case II]{\includegraphics[width=2.5in]{box}%
%\label{fig_second_case}}
%\caption{Simulation results for the network.}
%\label{fig_sim}
%\end{figure*}
%
% Note that often IEEE papers with subfigures do not employ subfigure
% captions (using the optional argument to \subfloat[]), but instead will
% reference/describe all of them (a), (b), etc., within the main caption.
% Be aware that for subfig.sty to generate the (a), (b), etc., subfigure
% labels, the optional argument to \subfloat must be present. If a
% subcaption is not desired, just leave its contents blank,
% e.g., \subfloat[].


% An example of a floating table. Note that, for IEEE style tables, the
% \caption command should come BEFORE the table and, given that table
% captions serve much like titles, are usually capitalized except for words
% such as a, an, and, as, at, but, by, for, in, nor, of, on, or, the, to
% and up, which are usually not capitalized unless they are the first or
% last word of the caption. Table text will default to \footnotesize as
% the IEEE normally uses this smaller font for tables.
% The \label must come after \caption as always.
%
%\begin{table}[!t]
%% increase table row spacing, adjust to taste
%\renewcommand{\arraystretch}{1.3}
% if using array.sty, it might be a good idea to tweak the value of
% \extrarowheight as needed to properly center the text within the cells
%\caption{An Example of a Table}
%\label{table_example}
%\centering
%% Some packages, such as MDW tools, offer better commands for making tables
%% than the plain LaTeX2e tabular which is used here.
%\begin{tabular}{|c||c|}
%\hline
%One & Two\\
%\hline
%Three & Four\\
%\hline
%\end{tabular}
%\end{table}


% Note that the IEEE does not put floats in the very first column
% - or typically anywhere on the first page for that matter. Also,
% in-text middle ("here") positioning is typically not used, but it
% is allowed and encouraged for Computer Society conferences (but
% not Computer Society journals). Most IEEE journals/conferences use
% top floats exclusively. 
% Note that, LaTeX2e, unlike IEEE journals/conferences, places
% footnotes above bottom floats. This can be corrected via the
% \fnbelowfloat command of the stfloats package.




\section{Conclusion}
In this paper, we have applied a $l_2$ normalization constraint to deep autoencoder representations during autoencoder training and observed that the representations obtained in this way clusters well in Euclidean space.
Therefore, applying a simple $k$-means clustering on these representations gives high clustering accuracies and works better than methods defining additional clustering losses.
We have also shown that the high performance is not due to any conditioning applied on the representations but it is due to selection of a particular normalization that leads to more separable clusters in Euclidean space.
We have proposed an unsupervised anomaly detection method on $l_2$ normalized deep auto-encoder representations.
We have shown that the proposed $l_2$ normalization constraint drastically increases the anomaly detection method's performance. 
Finally, we have shown that the commonly adopted deep anomaly detection method based on the reconstruction error performs weak in a definition of anomaly, whereas our method performs superior.





% conference papers do not normally have an appendix


% use section* for acknowledgment
%\section*{Acknowledgment}


%The authors would like to thank...





% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://mirror.ctan.org/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
%\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)
\begin{thebibliography}{1}

\bibitem{DEC}
J. Xie, R. Girshick and A. Farhadi, \emph{Unsupervised deep embedding for clustering analysis}, International Conference on Machine Learning, pp. 478-487, June, 2016.

\bibitem{IDEC}
X. Guo, L. Gao, X. Liu and J. Yin, \emph{Improved deep embedded clustering with local structure preservation}, International Joint Conference on Artificial Intelligence, pp. 1753-1759, June, 2017.

\bibitem{GMVAE}
N. Dilokthanakul, P. A. Mediano, M. Garnelo, M- C- Lee, H. Salimbeni, K- Arulkumaran and M. Shanahan, \emph{Deep unsupervised clustering with gaussian mixture variational autoencoders}, arXiv preprint arXiv:1611.02648, 2016.

\bibitem{DCEC}
X. Huo, X. Liu, E. Zheand J. Yin, \emph{Deep Clustering with Convolutional Autoencoders}, International Conference on Neural Information Processing, pp. 373-382, 2017.

\bibitem{DRAE}
Y. Xia, X. Cao, F. Wen, G. Hua and J. Sun, \emph{Learning discriminative reconstructions for unsupervised outlier removal}, Proceedings of the IEEE International Conference on Computer Vision, pp. 1511-1519, 2015.

\bibitem{AVAE}
J. An and S. Cho, \emph{Variational autoencoder based anomaly detection using reconstruction probability}, SNU Data Mining Center, Tech. Rep., 2015.

\bibitem{ANOM}
M. Goldstein and S. Uchida, \emph{A comparative evaluation of unsupervised anomaly detection algorithms for multivariate data}, PloS one, vol 11, no. 4, 2016.

\bibitem{BNORM}
S. Ioffe and C. Szegedy, \emph{Batch normalization: Accelerating deep network training by reducing internal covariate shift}, International conference on machine learning, pp. 448-456, 2015.

\bibitem{LNORM}
J. L. Ba, J. R. Kiros and G. E. Hinton, \emph{Layer normalization}, arXiv preprint arXiv:1607.06450, 2016.

\bibitem{TSNE}
L. V. D. Maaten and G. Hinton, \emph{Visualizing data using t-SNE}, Journal of machine learning research, pp. 2579-2605, 2008.

\bibitem{MNIST}
Y. LeCun, L. Bottou, Y. Bengio and P. Haffner, \emph{Gradient-based learning applied to document recognition}, Proceedings of the IEEE, vol. 86, no. 11, pp. 2278-2324, 1998.

\bibitem{HUN}
H. W. Kuhn, \emph{The Hungarian method for the assignment problem}, Naval Research Logistics, 2(1-2), pp. 83-97, 1955.

\bibitem{LRELU}
V. Nair and G. E. Hinton, \emph{Empirical evaluation of rectified activations in convolutional network}, arXiv preprint arXiv:1505.00853, 2015.

\bibitem{RELU}
B. Xu, N. Wang, T. Chen and M. Li, \emph{Rectified linear units improve restricted boltzmann machines}, International Conference on Machine Learning, pp. 807-814, 2010.

\bibitem{USPS}
Y. LeCun, O. Matan, B. Boser, J. D. Denker, D. Henderson, R. E. Howard, W. Hubbard, L. D. Jacket and H. S. Baird, \emph{Handwritten zip code recognition with multilayer networks}. International Conference on Pattern Recognition, vol. 2, pp. 35-40, 1990.


\end{thebibliography}


% that's all folks
\end{document}


