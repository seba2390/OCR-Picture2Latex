\begin{appendices}

\section{Simple versions of the algorithms}
\label{appendix:simple_algs}

\begin{algorithm}
\caption{Online algorithm}\label{online_simple}
\begin{algorithmic}
\State Initialize \textsc{Student} learning algorithm
\State Initialize expected return $Q(a)=0$ for all $N$ tasks
\For{t=1,\ldots,T}
\State Choose task $a_t$ based on $|Q|$ using $\epsilon$-greedy or Boltzmann policy
\State Train \textsc{Student} using task $a_t$ and observe reward $r_t = x_t^{(a_t)} - x_{t'}^{(a_t)}$
\State Update expected return $Q(a_t) = \alpha r_t + (1 - \alpha) Q(a_t)$
\EndFor
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{Naive algorithm}\label{naive_simple}
\begin{algorithmic}
\State Initialize \textsc{Student} learning algorithm
\State Initialize expected return $Q(a)=0$ for all $N$ tasks
\For{t=1,...,T}
\State Choose task $a_t$ based on $|Q|$ using $\epsilon$-greedy or Boltzmann policy
\State Reset $D=\emptyset$
\For{k=1,...,K}
\State Train \textsc{Student} using task $a_t$ and observe score $o_t = x_t^{(a_t)}$
\State Store score $o_t$ in list $D$
\EndFor
\State Apply linear regression to $D$ and extract the coefficient as $r_t$
\State Update expected return $Q(a_t) = \alpha r_t + (1 - \alpha) Q(a_t)$
\EndFor
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{Window algorithm}\label{window_simple}
\begin{algorithmic}
\State Initialize \textsc{Student} learning algorithm
\State Initialize FIFO buffers $D(a)$ and $E(a)$ with length $K$ for all $N$ tasks
\State Initialize expected return $Q(a)=0$ for all $N$ tasks
\For{t=1,\ldots,T}
\State Choose task $a_t$ based on $|Q|$ using $\epsilon$-greedy or Boltzmann policy
\State Train \textsc{Student} using task $a_t$ and observe score $o_t = x_t^{(a_t)}$
\State Store score $o_t$ in $D(a_t)$ and timestep $t$ in $E(a_t)$
\State Use linear regression to predict $D(a_t)$ from $E(a_t)$ and use the coef. as $r_t$
%\State Update expected return $Q(a_t) := r_t$
\State Update expected return $Q(a_t) = \alpha r_t + (1 - \alpha) Q(a_t)$
\EndFor
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{Sampling algorithm}\label{sampling_simple}
\begin{algorithmic}
\State Initialize \textsc{Student} learning algorithm
\State Initialize FIFO buffers $D(a)$ with length $K$ for all $N$ tasks
\For{t=1,\ldots,T}
\State Sample reward $\tilde{r}_a$ from $D(a)$ for each task (if $|D(a)|=0$ then $\tilde{r}_a=1$)
\State Choose task $a_t = \argmax_a |\tilde{r}_a|$
\State Train \textsc{Student} using task $a_t$ and observe reward $r_t = x_t^{(a_t)} - x_{t'}^{(a_t)}$
\State Store reward $r_t$ in $D(a_t)$
\EndFor
\end{algorithmic}
\end{algorithm}

\newpage
\section{Batch versions of the algorithms}
\label{appendix:batch_algs}

\begin{algorithm}
\caption{Online algorithm}\label{online_batch}
\begin{algorithmic}
\State Initialize \textsc{Student} learning algorithm
\State Initialize expected return $Q(a)=0$ for all $N$ tasks
\For{t=1,\ldots,T}
\State Create prob. dist. $\vec{a_t}=(p_t^{(1)}, ..., p_t^{(N)})$ based on $|Q|$ using $\epsilon$-greedy or Boltzmann policy
\State Train \textsc{Student} using prob. dist. $\vec{a_t}$ and observe scores $\vec{o_t} = (x_t^{(1)}, ..., x_t^{(N)})$
\State Calculate score changes $\vec{r_t} = \vec{o_t} - \vec{o_{t-1}}$
%\State Calculate score change $\hat{r}_t = o_t - o_{t-1}$
%\State Calculate corrected reward $r_t = \hat{r}_t / a_t$ ($a_t$ is prob. dist.)
\State Update expected return $\vec{Q} = \alpha \vec{r_t} + (1 - \alpha) \vec{Q}$
\EndFor
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{Naive algorithm}\label{online_naive}
\begin{algorithmic}
\State Initialize \textsc{Student} learning algorithm
\State Initialize expected return $Q(a)=0$ for all $N$ tasks
\For{t=1,\ldots,T}
\State Create prob. dist. $\vec{a_t}=(p_t^{(1)}, ..., p_t^{(N)})$ based on $|Q|$ using $\epsilon$-greedy or Boltzmann policy
\State Reset $D(a)=\emptyset$ for all tasks
\For{k=1,\ldots,K}
\State Train \textsc{Student} using prob. dist. $\vec{a_t}$ and observe scores $\vec{o_t} = (x_t^{(1)}, ..., x_t^{(N)})$
\State Store score $o_t^{(a)}$ in list $D(a)$ for each task $a$
\EndFor
\State Apply linear regression to each $D(a)$ and extract the coefficients as vector $\vec{r_t}$
%\State Apply linear regression to each $D(a)$ and extract the coefficients as $\hat{r}_t$
%\State Calculate corrected rewards $r_t = \hat{r}_t / a_t$ ($a_t$ is prob. dist.)
\State Update expected return $\vec{Q} = \alpha \vec{r_t} + (1 - \alpha) \vec{Q}$
\EndFor
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{Window algorithm}\label{online_window}
\begin{algorithmic}
\State Initialize \textsc{Student} learning algorithm
\State Initialize FIFO buffers $D(a)$ with length $K$ for all $N$ tasks
\State Initialize expected return $Q(a)=0$ for all $N$ tasks
\For{t=1,\ldots,T}
\State Create prob. dist. $\vec{a_t}=(p_t^{(1)}, ..., p_t^{(N)})$ based on $|Q|$ using $\epsilon$-greedy or Boltzmann policy
\State Train \textsc{Student} using prob. dist. $\vec{a_t}$ and observe scores $\vec{o_t} = (x_t^{(1)}, ..., x_t^{(N)})$
\State Store score $o_t^{(a)}$ in $D(a)$ for all tasks $a$
\State Apply linear regression to each $D(a)$ and extract the coefficients as vector $\vec{r_t}$
%\State Apply linear regression to each $D(a)$ and extract the coefficients as $\hat{r}_t$
%\State Calculate corrected rewards $r_t = \hat{r}_t / a_t$ ($a_t$ is prob. dist.)
\State Update expected return $\vec{Q} = \alpha \vec{r_t} + (1 - \alpha) \vec{Q}$
%\State Update expected return $Q = r_t$
\EndFor
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{Sampling algorithm}\label{online_sampling}
\begin{algorithmic}
\State Initialize \textsc{Student} learning algorithm
\State Initialize FIFO buffers $D(a)$ with length $K$ for all $N$ tasks
\For{t=1,\ldots,T}
\State Sample reward $\tilde{r}_a$ from $D(a)$ for each task (if $|D(a)|=0$ then $\tilde{r}_a=1$)
\State Create one-hot prob. dist. $\vec{\tilde{a}_t}=(p_t^{(1)}, ..., p_t^{(N)})$ based on $\argmax\nolimits_a |\tilde{r}_a|$
\State Mix in uniform dist. : $\vec{a_t} = (1 - \epsilon) \vec{\tilde{a}_t} + \epsilon/N$
\State Train \textsc{Student} using prob. dist. $\vec{a_t}$ and observe scores $\vec{o_t} = (x_t^{(1)}, ..., x_t^{(N)})$
\State Calculate score changes $\vec{r_t} = \vec{o_t} - \vec{o_{t-1}}$
%\State Calculate score change $\hat{r}_t = o_t - o_{t-1}$
%\State Calculate corrected rewards $r_t = \hat{r}_t / a_t$ ($a_t$ is prob. dist.)
\State Store reward $r_t^{(a)}$ in $D(a)$ for each task $a$
\EndFor
\end{algorithmic}
\end{algorithm}

\clearpage
\section{Decimal Number Addition Training Details}
\label{appendix:addition}

Our reimplementation of decimal addition is based on Keras \citep{chollet2015keras}. The encoder and decoder are both LSTMs with 128 units. In contrast to the original implementation, the hidden state is not passed from encoder to decoder, instead the last output of the encoder is provided to all inputs of the decoder. One curriculum training step consists of training on 40,960 samples. Validation set consists of 4,096 samples and 4,096 is also the batch size. Adam optimizer \citep{kingma2014adam} is used for training with default learning rate of 0.001. Both input and output are padded to a fixed size.

In the experiments we used the number of steps until 99\% validation set accuracy is reached as a comparison metric. The exploration coefficient $\epsilon$ was fixed to 0.1, the temperature $\tau$ was fixed to 0.0004, the learning rate $\alpha$ was 0.1, and the window size $K$ was 10 in all experiments.
 
\section{Minecraft Training Details}
\label{appendix:minecraft}

The Minecraft task consisted of navigating through randomly generated mazes. The maze ends with a target block and the agent gets 1,000 points by touching it. Each move costs -0.1 and dying in lava or getting a timeout yields -1,000 points. Timeout is 30 seconds (1,500 steps) in the first task and 45 seconds (2,250 steps) in the subsequent tasks.

For learning we used the \textit{proximal policy optimization} (PPO) algorithm \citep{schulman2017proximal} implemented using Keras \citep{chollet2015keras} and optimized for real-time environments. The policy network used four convolutional layers and one LSTM layer. Input to the network was $40\times 30$ color image and outputs were two Gaussian actions: move forward/backward and turn left/right. In addition the policy network had state value output, which was used as the baseline. Figure \ref{f14} shows the network architecture.

\begin{figure}[h]
  \includegraphics[scale=0.4]{figures/minecraft_network}
\caption{Network architecture used for Minecraft.}
\label{f14}
\end{figure}

For training we used a setup with 10 parallel Minecraft instances. The agent code was separated into runners, that interact with the environment, and a trainer, that performs batch training on GPU, similar to \cite{babaeizadeh2016reinforcement}. Runners regularly update their snapshot of the current policy weights, but they only perform prediction (forward pass), never training. After a fixed number of steps they use FIFO buffers to send collected states, actions and rewards to the trainer. Trainer collects those experiences from all runners, assembles them into batches and performs training. FIFO buffers shield the runners and the trainer from occasional hiccups. This also means that the trainer is not completely on-policy, but this problem is handled by the importance sampling in PPO.

\begin{figure}[h]
  \includegraphics[scale=0.4]{figures/minecraft_training}
\caption{Training scheme used for Minecraft.}
\label{f14}
\end{figure}

During training we also used frame skipping, i.e. processed only every 5th frame. This sped up the learning considerably and the resulting policy also worked without frame skip. Also, we used auxiliary loss for predicting the depth as suggested in \citep{mirowski2016learning}. Surprisingly this resulted only in minor improvements.

For automatic curriculum learning we only implemented the Window algorithm for the Minecraft task, because other algorithms rely on score change, which is not straightforward to calculate for parallel training scheme. Window size was defined in timesteps and fixed to 10,000 in the experiments, exploration rate was set to 0.1.

The idea of the first task in the curriculum was to make the agent associate the target with a reward. In practice this task proved to be too simple - the agent could achieve almost the same reward by doing backwards circles in the room. For this reason we added penalty for moving backwards to the policy loss function. This fixed the problem in most cases, but we occasionally still had to discard some unsuccessful runs. Results only reflect the successful runs.

We also had some preliminary success combining continuous (Gaussian) actions with binary (Bernoulli) actions for "jump" and "use" controls, as shown on figure \ref{f14}. This allowed the agent to learn to cope also with rooms that involve doors, switches or jumping obstacles, see \url{https://youtu.be/e1oKiPlAv74}.

\end{appendices}