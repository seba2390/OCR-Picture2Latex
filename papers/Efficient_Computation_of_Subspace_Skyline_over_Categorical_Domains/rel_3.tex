\section{Related Work}\label{sec:relWork}
\vspace{1mm}

\begin{table}[!tb]
\centering
\caption{Taxonomy of Skyline Algorithms}\label{tab:runningExampleSubspaceSkyline}
\begin{tabular}{p{1.4cm}|p{1.6cm}|p{1.8cm}|p{1.8cm}}
    
    \hline 
    \multicolumn{3}{c|}{Fixed Attribute Skyline} & Subspace Skyline\\
    \hline
    
    Sorting-based & \multicolumn{2}{c|}{Partition-based} & \multirow{3}{1.8cm}{Skycube~\cite{yuan2005efficient}, Skyey~\cite{pei2005catching}, Subsky~\cite{tao2006subsky}, CSC~\cite{xia2012online}, FMC~\cite{maabout2016skycube}} \\ \cline{1-3}
    
    \multirow{2}{1.4cm}{BNL~\cite{borzsony2001skyline}, SFS~\cite{chomicki2003skyline}, LESS~\cite{godfrey2005maximal}, SaLSa~\cite{bartolini2008efficient}} & Index & No Index & \\ \cline{2-3}
    
     & NN~\cite{kossmann2002shooting}, BBS~\cite{papadias2003optimal}, ZSearch~\cite{lee2007approaching} & D\&C~\cite{borzsony2001skyline}, OSPS~\cite{zhang2009scalable}, BSkyTree~\cite{lee2014scalable} & \\
    \hline
\end{tabular}
\end{table}

In database context, the skyline operator was first introduced in \cite{borzsony2001skyline}. The authors proposed two algorithms, Block-nested-loops (BNL) and Divide and Conquer (D\&C), that can work on dataset large enough to fit into the main memory. Since then many work has been done that aims improving the performance of  skyline computation in different scenarios. Algorithms on skyline computation can be classified into two categories: \textit{fixed attribute skyline algorithms}, and \textit{subspace skyline algorithms}. Algorithms in the first category consider the problem of computing skyline over the entire dataset. Whereas subspace skyline algorithms focus on computing skyline over an arbitrary set of attributes. Fixed attribute skyline algorithms can be further categorized into two groups: \textit{sorting-based algorithms}, and \textit{partition-based algorithms}.

\subsection{Fixed attribute skyline algorithms}
Most of the works found in literature focus on computing skyline over the entire dataset. Specifically, the goal is to find skyline objects that aren't dominated by any other object in the dataset. In addition, the algorithms also assume that the attributes considered for computing skylines will always be same. Such algorithms are not appropriate in online settings.

\vspace{1mm}
\noindent{\bf Sorting-based Algorithms:} Sorting based algorithms aim to improve the performance of skyline computation by trying to discarded the non skyline objects by performing small number of dominance check. The SFS algorithm proposed in ~\cite{chomicki2003skyline, chomicki2005skyline} first sorts the points using a monotonic scoring function. The authors have shown that, given an monotonic scoring function $F(\cdot)$, if $F(t') \leq F(t) \Rightarrow t' \nsucc t$. Ordering tuples using a monotonic scoring function brings tuples that more likely to be skyline at the front of the list. It also ensures that tuples that not skyline will not be inserted into the candidate skyline list. Thus, reducing the total number of dominance check.~\cite{godfrey2005maximal} proposed LESS that improves the performance of skyline computation by discarding portion of non skyline points in external sorting phase. The authors proved the LESS has $O(n)$ average case running time.  ~\cite{bartolini2008efficient} proposed a sorting based skyline algorithm called SaLSa. SaLSa also tries to improve the performance of skyline computation by trying stop early. It maintains a \textit{stopping point}, selected from the skylines that have already been discovered. The algorithm stops when all the points not yet accessed by the algorithm are dominated by the stopping point. The authors also showed that the correctness and as well as performance of the algorithm depends on the choice of scoring function. The number of tuples needed to be accessed can be minimized by sorting data according to ``minimum coordinate'', minC, criterion.

\noindent{\bf Drawbacks:} The sorting based algorithms are not every efficient for answering subspace skyline queries. This is because, for each subspace skyline query, they require all the tuples in dataset to be sorted according a an appropriate sorting function first. Hence, making these algorithms ineffective for online applications.


\vspace{1mm}
\noindent{\bf Partition-based Algorithms:} The D\&C~\cite{borzsony2001skyline} algorithm recursively partitions the dataset into a set of disjoint regions. For each partition, local skylines are computed by considering objects only in that region. The final skyline set is then computed by considering skylines of all the partitions.~\cite{zhang2009scalable} improves the performance of skyline computation by performing region level dominance test. The algorithm first selects an object as \textit{pivot element} and the partitions the $d$ dimensional space into $2^d$ regions. The dominance relationship among the regions form a lattice structure. Thus for each region $R$, we only need to compare the objects in that region with the other objects that belongs to region $R'$ such that $R'$ dominates or partially dominates $R$. BSkyTree proposed in~\cite{lee2014scalable} selects an object in dataset as pivot element by considering both \textit{dominance} and \textit{incomparability}. Specially, the effectiveness of an object as pivot element is defined as the summation of i) number of objects dominated by it and ii) number of objects pairs that are incomparable. Object with the highest value is selected as pivot element. Algorithms like NN~\cite{kossmann2002shooting}, BBS~\cite{papadias2003optimal} and ZSearch~\cite{lee2007approaching} utilize indexing structures such as R-tree, ZB-tree for efficient region level dominance tests.

{\bf Drawbacks:}  While index based algorithms exhibit better performance compared to other algorithms, they are not appropriate for subspace subspace skyline queries. This is because, they require indexing for all possible attribute combination, which is exponential over $d$. Moreover, all the partition-based algorithms are specifically designed for databases with {\em numeric attributes}. Even though it is possible to adapt these algorithms for categorical database, the performance of the algorithms degrades in that process. For example, consider a dataset with two numeric attributes, each having domain in range $[0, 1]$. Tuples in that dataset can be considered as points in rectangle $R$ with $(0, 0)$ and $(1, 1)$ as corner points. Partition-based algorithms mentioned above will select one of the objects in the dataset as pivot element and partition the data space into four disjoint regions: $R_0$ (bottom-left), $R_1$ (bottom-right), $R_2$ (top-left), and $R_3$ (top-right). Objects in region $R_3$ are dominated by objects in $R_0$. Moreover, Objects in $R_2$ ad $R_3$ are incomparable. Thus, no dominance check is required among them. However, for categorical dataset, the objects can only be on the edges of rectangle $R$. Thus, a pivot element can only partition the space into two regions. None of this two regions is dominated by the other one, nor they are incomparable. Thus, diminishing the gains of partition-based algorithms. The problem becomes worse for boolean dataset. Moreover, these algorithms require accessing the tuples in dataset at-least once. Hence, might be inefficient for online applications.


\subsection{Subspace Skyline Algorithms}
~\cite{yuan2005efficient, pei2005catching} studied the problem of subspace skyline computation in parallel. The authors have shown that the set of all possible skyline queries over an attribute set forms a lattice called SKYCUBE. Each node in the lattice is called cuboid and represents a specific subspace skyline query. The idea was to precompute skylines for all the cuboids in SKYCUBE. In~\cite{yuan2005efficient}, two algorithms TDS (Top-Down Skyline) and BUS (Bottom-Up Skyline) are proposed. TDS computes skylines for each cuboid using the results of its parent cuboids. Whereas, in BUS, the cuboids are materialized in bottom up manner. Instead of pre-materializing skylines for each possible subspace,~\cite{tao2006subsky} aims to answer subspace skyline query on-the-fly (reduce space requirement). However, the algorithm performs effectively only for subspace queries low-cardinality (i.e., 2 or 3)~\cite{xia2006refreshing}.

~\cite{xia2012online} proposed a Compressed SkyCube (CSC) structure to reduce the space requirement of subspace skyline algorithm. Instead of materializing the all the cuboids of skycube, CSC maintains only a portion of cuboids that are enough to answer subspace skyline queries. It utilizes concept of \textit{minimum skyline} to reduce the space requirement of skycube. Given a set of attribute $\mathcal{X} \subseteq \mathcal{A}$, the \textit{minimum skyline} of $\mathcal{X}$, denoted as $min\_sky(\mathcal{X})$ is the set $\{t | t \in \mathcal{S}_\mathcal{X} \text{ and } \exists! \mathcal{Y} \subset \mathcal{X} \text{ s.t. } t \in \mathcal{S}_\mathcal{Y}  \}$. CSC only stores the cuboids that are not empty, i.e., $min\_sky(\mathcal{X}) \neq \emptyset$. Given a subspace skyline query $\mathcal{Q}$, the skylines over $\mathcal{Q}$ is computed by running skyline algorithm over tuple set $T' = \{ t | t \in  \textit{min\_sky }(\mathcal{Q'}) \text { s.t. } \mathcal{Q'} \subseteq \mathcal{Q} \}$. After $T'$ is computed, we can run any existing skyline algorithm to find the skylines. 

~\cite{maabout2016skycube} tries to reduce the space requirement of skycube by utilizing functional dependencies among subspaces. The algorithm proposed by the authors is called Full Materialization with Closed subspaces (FMC). Functional dependency $\mathcal{X} \rightarrow \mathcal{Y}$ is satisfied by $D$ iff for every pair of tuple $t_1, t_2 \in D$, if $t_1[\mathcal{X}] = t_2[\mathcal{X}]$ then $t_1[\mathcal{Y}] = t_2[\mathcal{Y}]$. The authors have shown that functional dependencies implies inclusions between subspace skylines. Specifically, if $\mathcal{X} \rightarrow \mathcal{Y}$, then $\mathcal{S}_\mathcal{X} \subseteq \mathcal{S}_{\mathcal{XY}}$, where $\mathcal{XY}$ is the union of attribute set $\mathcal{X}$ and $\mathcal{Y}$. 
%Hence, storing the skylines for subspace $\mathcal{XY}$ is enough for computing skyline over subspace $\mathcal{X}$. 
Using functional dependency, the notion of closed subspace, $\Gamma$ is then defined. %Specifically, $\Gamma = \{ \mathcal{X} | \exists! \mathcal{Y} \subseteq \mathcal{A} \text{ s.t. } \mathcal{X} \cap \mathcal{Y} = \emptyset \text{ and } \mathcal{X} \rightarrow \mathcal{Y} \text{ is satisfied by } \mathcal{D} \}$. Hence we only need to store skylines for closed subspaces. 
%The space requirement can be further minimized by removing every closed subspace $X$ whose skyline in included in that of some $Y$, where $X \subset Y$. 
The final set of subspaces that needed to be materialized is called Minimal Information-Complete Subskycube (MICS). Given a subspace skyline query $\mathcal{Q}$, the algorithm then finds the largest subspace $\mathcal{Q^+}$ in MICS such that $\mathcal{Q} \rightarrow \mathcal{Q}^+$. Only the tuples in $S_{\mathcal{Q}^+}$ are used to compute skylines over $\mathcal{Q}$.

\vspace{1mm}
\noindent{\bf Drawbacks:} Skycube algorithms have time complexity exponential over $d$. This is because, we need to traverse all the cuboids of skycube. Even though we only need to compute the skycube only once at the beginning, the exponential precomputation cost makes the skycube algorithms inappropriate for web applications that has many attributes. Moreover, the initial skycube algorithms required skyline results for each cuboid to be materialized, making the space requirement inordinately high.

Recent works on skycube aims to reduce the space required by the algorithm. For example, CSC~\cite{xia2012online} utilizes the \textit{minimum skyline} concept to store only the tuples that are necessary to answer any subspace skyline query. This gain in space requirement comes at the cost of increase of query execution time. The performance of the algorithm decreases as $|\mathcal{Q}|$ increases. This is because the number of subspace CSC need to search to compute $\mathcal{S}_{\mathcal{Q}}$ is exponential over $|\mathcal{Q}|$. Moreover, for low-cardinality attributes, the size of $T'$ is close to $n$ (i.e., the number of tuples in database). This diminishes the advantage of pre-computation. For examples, consider a boolean dataset where each attribute can have value either $0$ or $1$. For such dataset, CSC will only store cuboids at level $1$ (cuboid corresponds to single attribute). For each level-1  cuboid $X$, $min\_sky(X) = \{t | t[X] = 1 \}$. Now, given a skyline query $\mathcal{Q}$, we will compute $T'$ form all the level-1 cuboids that corresponds to an attribute in $\mathcal{Q}$. Therefor $T' = \{t | t \in \textit{ min\_sky} (A) \text{ and } A \in \mathcal{Q} \}$. The expected size of $T'$ is then $n (1 - \sum_{A \in Q}(1-p_A))$, where $p_A$ is the probability of binary attribute $A$ being 1. For a subspace skyline query of length $6$, where all the attributes have $0.5$ probability of being $1$, $T' = 0.98n$, almost equal to the input size. This makes the skyline computation part quadratic on $n$. Similarly the performance of FMC decrease drastically with the reduction of attribute domains size. As pointed out by the authors, the size of the MICS (i.e., number of subspaces need to be materialized) increases exponentially with reduction of domain size. This is because the total number of functional dependencies in $D$, decreases with decrease of attribute domain value. For example, in case of boolean dataset, when $n$ is large, it is highly likely to find a pair of tuple $t_1, t_2$ such that has same value combination on attribute set $\mathcal{X}$ but different value combination on attribute set $\mathcal{Y}$, thus violating the condition of functional dependencies. Therefore, in worst case it needs to materialize all the cuboids. The authors also proposed an algorithm to compute subspace skylines form the skylines of top-most cuboid (skylines over full-space). However, it is shown taht the approach is only effective when the size of skyline over entire attribute set is small.

\vspace{1mm}
\noindent{\bf Skycube vs TA-SKY:} While the skycube is very effective for answering subspace skyline query, the exponential computation cost and inordinate space requirement makes it only applicable to applications with small number of attributes. TA-SKY, the algorithm we proposed in this paper falls in between two extreme approaches for answering subspace skyline query: i) \textit{no precomputations}: skyline queries are computed on-fly and ii) \textit{materialize everything}: skylines are materialize for all the cuboids in skycube. Using little amount of indexing, TA-SKY can answer subspace skyline queries very quickly. Especially when the attributes in skyline query is comparatively small. Note that, the space requirement of TA-SKY is very small compared to skycube algorithms. We need to maintain a sorted list for each attribute. Note that, the sorted lists doesn't need to store the actual tuples of dataset. Storing only the $tupleId$s is sufficient.

In relational databases similar approaches are used to answer user queries. Datacube~\cite{gray1997data} is a well known approach for answering the aggregate queries very quickly, which requires inordinate amount of precomputation and space. Whereas in ~\cite{das2006answering} \textcolor{red}{add more ref.} the authors consider the problem of answering as many aggregate queries as possible using small amount of materialization.




