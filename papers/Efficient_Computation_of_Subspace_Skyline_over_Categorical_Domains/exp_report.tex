\begin{figure*}[!ht]
  \begin{minipage}[t]{0.23\linewidth}
    \centering
    \includegraphics[scale=0.46]{figures/TimeVsDimensionalityFullSpace.pdf}
    \vspace{-6mm}
    \caption{Varying query size}
    \label{fig:algorithms}
  \end{minipage}
  \hspace{1mm}
  \begin{minipage}[t]{0.23\linewidth}
    \centering
    \includegraphics[scale=0.46]{figures/TimeVsDimensionality.pdf}
    \vspace{-6mm}
    \caption{Varying query size}
    \label{fig:syn_TimeVsDimension_Dist}
  \end{minipage}
  \hspace{3mm}
  \begin{minipage}[t]{0.23\linewidth}
    \centering
    \includegraphics[scale=0.46]{figures/TimeVsN.pdf}
    \vspace{-6mm}
    \caption{Varying number of tuples} 
    \label{fig:syn_TimeVsN}
  \end{minipage}
  \begin{minipage}[t]{0.25\linewidth}
    \centering
    \includegraphics[scale=0.46]{figures/TimeVsCardinality.pdf}
    \vspace{-2mm}
    \caption{Varying cardinality}
    \label{fig:syn_TimeVsC}
  \end{minipage}
\end{figure*}


\begin{figure*}[!ht]
  \begin{minipage}[t]{0.25\linewidth}
    \centering
    \includegraphics[scale=0.46]{figures/TimeVsOutputTuple.pdf}
    %\vspace{-2mm}
    \caption{Time vs number of skylines returned}
    \label{fig:syn_TimeVsNumberOfSkylines}
  \end{minipage}
  \hspace{2mm}
  \begin{minipage}[t]{0.25\linewidth}
    \centering
    \includegraphics[scale=0.46]{figures/NumberOfTuplesVsOutputTuples.pdf}
    %\vspace{-2mm}
    \caption{Tuples accessed vs number of skylines returned}
    \label{fig:syn_NumberOfTuplesVsSkylines}
  \end{minipage}
  \begin{minipage}[t]{0.21\linewidth}
    \centering
    \includegraphics[scale=0.5]{figures/TimeVsDimensionalityAirbnb.pdf}
    %\vspace{-6mm}
    \caption{AirBnB: Varying query size}
    \label{fig:algorithmsAirbnb}
  \end{minipage}
  \hspace{1mm}
  \begin{minipage}[t]{0.23\linewidth}
    \centering
    \includegraphics[scale=0.46]{figures/TimeVsDimensionalityAirbnbTASky.pdf}
    %\vspace{-6mm}
    \caption{AirBnB: TA-SKY performance v.s. Skyline size}
    \label{fig:Airbnbm}
  \end{minipage}
\end{figure*}

\section{Experimental Evaluation}\label{sec:experiments}
\subsection{Experimental Setup}

In this section, we describe our experimental results. In addition to the theoretical analysis presented in \S\ref{sec:3} and \S\ref{sec:subsky}, we compared our algorithms experimentally against existing state-of-the-art algorithms.  Our experiments were run over synthetic data, as well as two real-world datasets collected from \emph{AirBnB}\footnote{\small{http://www.airbnb.com/}} and \emph{Zillow}\footnote{\small{http://zillow.com/}}.
%The synthetic data was used to evaluate the effectiveness of the proposed methods over varying characteristics of the dataset. 

\vspace{1mm}
\noindent {\bf Synthetic Datasets:} 
In order to study the performance of the proposed algorithms in different scenarios, we generated a number of {\bf Zipfian datasets}, each containing 2M tuples and 30 attributes. Specifically, we created datasets with attribute cardinality ranging from  $2 - 8$. In this environment, the frequency of an attribute value is inversely proportional to its rank.  Therefore, the number of tuples having a higher (i.e., better) attribute value is less than then number of tuples with a comparatively lower attribute value. We used a Python package for generating these datasets. For each attribute, we specify its distribution over the corresponding domain by controlling the $z$ value. Two attributes having the same cardinality but different $z$ values will have different distributions. Specifically, the attribute with lower $z$ value will have a higher number of tuples having higher attribute value. Unless otherwise specified, we set the $z$ values of the attributes evenly distributed in the range $(1, 2]$ for generating synthetic datasets.


\vspace{1mm}
{\bf Choice of dataset:} we used Zipfian datasets as they reflect more precisely situation with real categorical datasets. Specifically, in real-world applications, for a specific attribute, the number of objects having higher attribute values (i.e., better) is likely to be less than the number of objects with lower attribute values. For example, in AirBnB, \emph{3 bed room} hosts are less frequent than hosts having a \emph{single bed room}. Similarly, in Craigslist, \emph{sedans} are more prevalent than \emph{sports cars}. Moreover, in real-world applications, the distributions of attributes are different from one another. For example, in our AirBnB dataset, approximately 600k out of the 2M hosts have amenity \emph{Cable TV}. Whereas, the approximate number of hosts with amenity \emph{Hot Tub} is only 200k.



\vspace{1mm}
\noindent {\bf AirBnB Dataset:} Probably one of the best fits for the application of this paper is AirBnB. It is a peer-to-peer location-based marketplace in which people can rent their properties or look for an abode for a temporary stay. We collected the information of approximately 2 million \emph{real} properties around the globe, shared on this website. AirBnB has a total number of 41 attributes for each host that captures the features and amenities provided by the hosts. Among all the attributes, 36 of them are boolean (categorical with domain size 2) attributes, such as \emph{Breakfast}, \emph{Cable TV}, \emph{Gym}, and \emph{Internet}, while 5 are categorical attributes, such as \emph{Number of Bedrooms}, and \emph{Number of Beds} etc. We tested our proposed algorithms against this dataset to see their performance on real-world applications.


\vspace{1mm}
\noindent {\bf Zillow Dataset:}
Zillow is a popular online real estate website that helps users to find houses and apartments for sale/rent. We crawled approximately 240k houses listed for sale in Texas and Florida state. For each listing, we collected 9 attributes that are present in all the houses. Out of 9 attributes, 7 of them are categorical, such as \emph{House Type}, \emph{Number of Beds}, \emph{Number of Baths}, \emph{Parking Space} etc., and two are numeric - \emph{House size} (in sqft), and \emph{Price}. The domain cardinalities of the categorical attributes varies from 3 to 30. Using discretization we mapped the numeric attributes into the categorical domain, each of cardinality 20. 


\vspace{1mm}
\noindent {\bf Algorithms Evaluated:}
We tested the proposed algorithms, namely ST-S, ST-P, TOP-DOWN, and TA-SKY as well as the state-of-art algorithms LS~\cite{morse2007efficient}, SaLSa~\cite{bartolini2008efficient} and BSkyTree \cite{lee2014scalable} that are applicable to our problem settings. 


\vspace{1mm}
\noindent {\bf Performance Measures:} We consider running time as the main performance measure of the algorithms proposed in this paper. In addition, we also investigate the key features of ST-S, ST-P and TA-SKY algorithm and demonstrate how they behave under a variety of settings. Each data point is obtained as the average of 25 runs.


\vspace{1mm}
\noindent{\bf Hardware and Platform:} All our experiments were performed on a quad-core 3.5 GHz Intel i7 machine running Ubuntu 14.04 with 16 GB of RAM. The algorithms were implemented in Python.




\subsection{Experiments over Synthetic Datasets}

\vspace{1mm}
\noindent{\bf Effect of Query Size $m^\prime$} \textbf{:} We start by comparing the performance of our algorithms with existing state-of-art algorithms that exhibit the best performance in their respective domain. Note that, unlike TA-SKY, the rest of the algorithms do not leverage any indexing structure. The goal of this experiment is to demonstrate how utilizing a small amount of precomputation (compared to the inordinate amount of space required by Skycube algorithms) can improve the performance of subspace skyline computation. Moreover, the precomputation cost is independent of the skyline query. This is because we only need to build the sorted lists once at the beginning. For this experiment, we set $n=500$k and vary $m'$ between $6-24$. In order to match real-world scenarios, we selected attributes with cardinality $c$ ranging between $2-6$. Specifically, $50\%$ of the selected attributes have cardinality $2$, $30\%$ have cardinality $4$, and $20\%$ have cardinality $6$. Figure~\ref{fig:algorithms} shows the experiment result. We can see that when $m^\prime$ is small, TA-SKY outperforms other algorithms. This is because, with small query size, TA-SKY can discover all the skylines by accessing only a small portion of the tuples in the dataset. However as $m'$ increases, the likelihood of a tuple dominating another tuple decreases. Hence, the total number of tuples accessed by TA-SKY before the stopping condition is satisfied also increases. Hence, the performance gap between TA-SKY and ST-S starts to decrease. Both ST-S and ST-P exhibits better performance compared to their baseline algorithms (SaLSa and BSkyTree). Algorithms such as ST-P, BSkyTree, and LS do not scale for larger values of $m'$. This is because all these algorithms operate by constructing a lattice over the query space which grows exponentially. Moreover, even though TOP-DOWN initially performed well, it did not not complete successfully for $m^\prime > 4$.

%Figure~\ref{fig:syn_TimeVsDimension_TopDownVsTA-SKY} shows the impact of $m^\prime$ on TA-SKY and TOP-DOWN, two index based algorithms proposed in \S\ref{sec:subsky}. For this experiment, we use the dataset with $n = 1$M, $m = 40$ and $c$ ranging between $2-6$. We then issue the subspace skyline queries by randomly selecting $m^\prime$ attributes ($2 \leq m^\prime \leq 16$). For both algorithms, the performance decreased as we increase the query size. However, the execution time of TOP-DOWN increases at an exponential rate. TOP-DOWN did not not complete successfully for $m^\prime > 4$. For TA-SKY, the growth is still polynomial over $n$ as shown in \S\ref{sec:TASKY-performance}. In the following experiments, we shall only consider TOP-DOWN and ST-S as they exhibit better performance compared to the other algorithms.


Figure~\ref{fig:syn_TimeVsDimension_Dist} demonstrates the effect $m^\prime$ and $z$ on the performance of TA-SKY and ST-S. For this experiment, we created two datasets with cardinality $c = 6$ and different $z$ values. In the first dataset, all the attributes have same $z$ value (i.e., $z=1.01$), whereas, for the second dataset, $z$ values of the attributes are evenly distributed within the range $(1, 2]$. By setting $z = 1.01$ for all attributes, we increase the frequency of tuples having preferable (i.e., higher) attribute values. Hence, the skyline size of the first dataset is less than the skyline size of the second dataset. This is because tuples with preferable attribute values are likely to dominate more non-skyline tuples, resulting in a small skyline size. Moreover, this also increases the likelihood of the stopping condition being satisfied at an early stage of the iteration. Hence, TA-SKY needs less time for the dataset with $z = 1.01$. In summary, TA-SKY performs better on datasets where more tuples have preferable attribute values.
The right-y-axis of Figure~\ref{fig:syn_TimeVsDimension_Dist} shows the skyline size for each query length. One can see that as the query size increased, the chance of tuples dominating each other decreased, which resulted in a significant increase in the skyline size. Please note that the increases in the execution time of TA-SKY are due to the increase in the skyline size which is bounded by $n$. Moreover, as $m^\prime$ increases, there is an initial decrease in skyline size. This is because when $m^\prime$ is small (i.e., 2), the likelihood of a tuple having highest value (i.e., preferable) on all attribute is large.



\vspace{1mm}
\noindent{\bf Effect of Dataset Size ($n$):} Figure~\ref{fig:syn_TimeVsN} shows the impact of $n$ on the performance of TA-SKY and ST-S. For this experiment, we used dataset with cardinality $c=6$, $m^\prime=12$ and varied $n$ from $500$K to $2$M. As we increase the value of $n$, the number of skyline tuples increases. With the increase of skyline size, both TA-SKY and ST-S needs to process more tuple before satisfying the stop condition. Therefore, total execution time increases with the increase of $n$.
%Moreover, in accordance with our previous discussion, the execution time of TA-SKY is less for the dataset having attributes distributed according to $z=1.01$.


\vspace{1mm}
\noindent{\bf Effect of Attribute Cardinality ($c$):} In our next experiment, we investigate how changing attribute cardinality affects the execution time of TA-SKY and ST-S. We set the dataset size to $n=1$M while setting the query size to $m^\prime = 12$, and vary the attribute cardinality $c$ from $4$ to $8$. Figure~\ref{fig:syn_TimeVsC} shows the experiment result. Increasing the cardinality of the attributes increases the total number of skyline tuples. Therefore, effects the total execution time of TA-SKY and ST-S.

\begin{figure*}[!ht]
  \vspace{-6mm}
  \begin{minipage}[t]{0.23\linewidth}
    \centering
    \includegraphics[scale=0.46]{figures/TimeVsNAirbnb.pdf}
    %\vspace{-2mm}
    \caption{AirBnB: Varying the number of tuples}
    \label{fig:Airbnbn}    
  \end{minipage}
  \begin{minipage}[t]{0.23\linewidth}
    \centering
    \includegraphics[scale=0.45]{figures/TimeVsOutputTupleAirbnb.pdf}
    %\vspace{-2mm}
    \caption{AirBnB: Time vs the number of skylines returned} 
    \label{fig:Airbnbp1}
  \end{minipage}
  \hspace{1mm}
  \begin{minipage}[t]{0.25\linewidth}
    \centering
    \includegraphics[scale=0.45]{figures/NumberOfTuplesVsOutputTuplesAirbnb.pdf}
    %\vspace{-2mm}
    \caption{AirBnB: Number of accessed tuples vs the number of skylines}
    \label{fig:Airbnbp2}
  \end{minipage}
  \begin{minipage}[t]{0.25\linewidth}
    \centering
    \includegraphics[scale=0.45]{figures/TimeVsDimensionalityZillow.pdf}
    %\vspace{-2mm}
    \caption{Zillow: Varying query size}
    \label{fig:Zillowm}
  \end{minipage}
\end{figure*}

\begin{figure*}[!ht]
  \begin{minipage}[t]{0.3\linewidth}
    \centering
    \includegraphics[scale=0.46]{figures/TimeVsNZillow.pdf}
    %\vspace{-2mm}
    \caption{Zillow: Varying the number of tuples}
    \label{fig:TimeVsNZillow}
  \end{minipage}
  \hspace{2mm}
  \begin{minipage}[t]{0.3\linewidth}
    \centering
    \includegraphics[scale=0.46]{figures/TimeVsOutputTupleZillow.pdf}
    %\vspace{-2mm}
    \caption{Zillow: Time vs the number of skylines returned}
    \label{fig:NumberOfTuplesVsSkylines}
  \end{minipage}
  \begin{minipage}[t]{0.3\linewidth}
    \centering
    \includegraphics[scale=0.5]{figures/NumberOfTuplesVsOutputTuplesZillow.pdf}
    %\vspace{-6mm}
    \caption{Zillow: Number of accessed tuples vs the number of skylines}
    \label{fig:NumberOfTuplesVsOutputTuplesZillow}
  \end{minipage}
\end{figure*}


\vspace{1mm}
\noindent{\bf Progressive Behavior of TA-SKY:} Figure~\ref{fig:syn_TimeVsNumberOfSkylines} and~\ref{fig:syn_NumberOfTuplesVsSkylines} demonstrates the incremental performance of TA-SKY for discovering the new skylines for a specific query of size $m^\prime=12$, while $n=1$M and all the attributes having cardinality $c=12$. Figure~\ref{fig:syn_TimeVsNumberOfSkylines} shows the CPU time as a function of the skyline size returned. We can see that even though the full skyline discovery takes $250$ seconds, within the first $50$ seconds TA-SKY outputs more than $50\%$  of the skyline tuples. Figure~\ref{fig:syn_NumberOfTuplesVsSkylines} presents the number of tuples TA-SKY accessed as a function of skyline tuples discovered so far. The skyline contains more than $33$k tuples. In order to discover all the skylines, TA-SKY needs to access almost $700$K (70\%) tuples. However, we can see that more than $80$\% of the skyline tuples can be discovered by accessing less that $30$\% tuples.


\subsection{Experiments over AirBnB Dataset}\label{subsec:expAirbnb}
In this experiment, we test the performance of our final algorithm, TA-SKY, against the real Airbnb dataset. We especially study (i) the effects of varying $m'$ and $n$ on the performance of the algorithm and (ii) the progressive behavior of it.

\vspace{1mm}
\noindent{\bf Effect of Varying Query Size ($m'$):} In our first experiment on AirBnB dataset, we compared the performance of different algorithms proposed in the paper with existing works.
We varied the number of attributes in the query (i.e., $m'$) from $2$ to $24$ while setting the number of tuples to 1,800,000.
Figure~\ref{fig:algorithmsAirbnb} shows the experiment result.
Similar to our experiment on the synthetic dataset (Figure~\ref{fig:algorithms}), TA-SKY and ST-S perform better than the remaining algorithms.
Even though initially performing well, TOP-DOWN did not scale after query length $4$. This is because, with $m^\prime > 4$, the skyline hosts shift to the middle of the corresponding query lattice, requiring TOP-DOWN to query many lattice nodes.
Figure~\ref{fig:Airbnbm} shows the relation between the performance of TA-SKY and the skyline size.
Unlike the generally accepted rule of thumb that the skyline size grows exponentially as the number of attributes increases, in this experiment, we see that the skyline size originally started to decrease as the query size increased and then started to increase again after query size $12$. The reason for that is because when the query size is small and $n$ is relatively large, the chance of having many tuples with (almost) all attributes in $\mathcal{Q}$ being 1 (for Boolean attributes) is high. None of these tuples are dominated and form the skyline. However, as the query size increases, the likelihood of having a tuple in the dataset that corresponds to the top node of the lattice decreases. Hence, if the query size gets sufficiently large, we will not see any tuple corresponding to the top node. From then the skyline size will increase with the increase of query size.
%On the other hand, as expected by the theoretical analysis, TA-SKY scaled well for increasing the skyline size.% For instance, it only required around 10 seconds to discover the skyline of a query of size $20$ over almost 2M tuples.


\vspace{1mm}
\noindent{\bf Effect of Varying Dataset Size ($n$):}
In this experiment, we varied the dataset size from 500,000 to 1,800,000 tuples, while setting $m'$ to $20$.
Figure~\ref{fig:Airbnbn} shows the performance of TA-SKY and ST-S in this case. Once can see that between these two algorithms, the cost of ST-S grows faster. Moreover, even though in the worst case TA-SKY is quadratically dependent on $n$, it performs significantly better in practice. Especially in this experiment, a factor of 4 increase in the dataset size only increased the execution time by less than a factor of 3.


\vspace{1mm}
\noindent{\bf Progressive Behavior of TA-SKY:}
As explained in \S\ref{sec:TASky}, TA-SKY is a progressive algorithm, i.e., tuples that are inserted into the candidate skyline set are guaranteed to be in $\mathcal{S}_\mathcal{Q}$.
This characteristic of TA-SKY makes it suitable for real world (especially web) applications, where, rather than delaying the result until the algorithm ends, partial results can gradually be returned to the user. Moreover, we can see that TA-SKY tends to discover a large portion of the skyline quickly within a short execution time with a few number of tuple accesses (as a measure of cost in the web applications).
To study this property of the algorithm, in this experiment, we set $n=1,800,000$ and $m'=20$ and monitored the execution time, as well as the number of tuple accesses, as the new skyline tuples are discovered. 
Figures~\ref{fig:Airbnbp1} and~\ref{fig:Airbnbp2} show the experiment results for the execution time and the number of accessed tuples, respectively.
One can see in the figure that TA-SKY performed well in discovering a large number of tuples quickly. For example, (i) as shown in Figure~\ref{fig:Airbnbp1}, it discovered more than $\frac{2}{3}$ of the skylines in less that $3$ seconds, and (ii) as shown in Figure~\ref{fig:Airbnbp2}, more than half of the skylines were discovered by only accessing less than $2\%$ of the tuples ($20,000$ tuples).



\subsection{Experiments over Zillow Dataset}\label{subsec:expZillow}

We performed the similar set of experiments on Zillow dataset.

\vspace{1mm}
In our first experiment, we varied the number of attributes from 2 to 9 while the $n$ is set to 236,194. The experiment result is presented in Figure \ref{fig:Zillowm}. Similar to our previous experiments, ST-S and TA-SKY outperforms the remaining algorithms. This result also shows the effectiveness of ST-S and TA-SKY on categorical attributes with large domain size. For the next experiment, we varied the dataset size ($n$) from 50,000 to 240,000 tuples, while setting $m^\prime$ to 9. Figure \ref{fig:TimeVsNZillow} shows the performance of ST and TA-SKY for this experiment. Figure \ref{fig:NumberOfTuplesVsSkylines} and \ref{fig:NumberOfTuplesVsOutputTuplesZillow} demonstrate the progressive behavior of TA-SKY for $m^\prime = 9$ and $n= 236,194$. We can see that 90\% of skylines are discovered withing the first second and by accessing only 1\% tuples.
 

