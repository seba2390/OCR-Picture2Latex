\section{Preliminaries}

\subsection{Differential Privacy}

% \textbf{\begin{definition}[Total variation distance]
% The total variation distance between two random variables $Y$ and $Z$ is defined by:
% \begin{align*}
%     \TV(Y, Z) \stackrel{\text { def }}{=} \max _{S}|\operatorname{Pr}[Y \in S]-\operatorname{Pr}[Z \in S]|
% \end{align*}
% \end{definition}}

% \iffalse
% \begin{definition}[Kullback-Leibler (KL) divergence]
% Let $\rho,\cNu$ be probability distributions on $\cK\subseteq \R^d$. Assume $\rho,\cNu$ have full support and smooth densities. 
% The Kullback-Leibler (KL) divergence of $\rho$ with respect to $\cNu$ is defined by
% \begin{align*}
%     H_\cNu(\rho)=\int_{\cK}\rho(\omega)\log \frac{\rho(\omega)}{\cNu(\omega)}\d \omega.
% \end{align*}
% \end{definition}

% \begin{definition}[$\alpha$-Rényi Divergence]
% For two probability distributions $P$ and $Q$ over $\cK\subseteq\R^d$ and $\alpha$, the Rényi divergence of order $\alpha$ ($\alpha>0$ and $\alpha\cNeq 1$) is
% \begin{align*}
%     R_{\alpha}(P\| Q)=\frac{1}{\alpha-1}\log \E_{\omega\sim Q}[(\frac{P(\omega)}{Q(\omega)})^\alpha].
% \end{align*}
% For $\alpha=1$, we let $R_{1}(P\| Q):=\sup_{0<\alpha<1}R_{\alpha}(P\mid Q)$.
% We use $E_{\alpha}(P\|Q)$ to denote the $\alpha^{th}$ moment of the likelihood ratio between $P$ and $Q$:
% \begin{align*}
%     E_{\alpha}(P\|Q)=\E_{\omega\sim Q}[(\frac{P(\omega)}{Q(\omega)})^\alpha].
% \end{align*}
% \end{definition}
% By Theorem 5 in \cite{EH14}, we know $R_1(P\|Q)=H_Q(P)$.
% \fi

% \iffalse
% \begin{definition}[Rényi differential privacy \cite{Mir17}]
% We say a randomized mechanism $\M:\cK\rightarrow \mathrm{Range}(\M)$ has $\epsilon$-Rényi differential privacy of order $\alpha$, or $(\alpha,\epsilon)$-RDP for short, if for any adjacent databases $S,S'$, one has
% \begin{align*}
%     R_\alpha(\M(S)\|\M(S'))\leq \epsilon.
% \end{align*}
% \end{definition}
% \fi

% \begin{definition}[Neighbouring databases]
% We say two databases $S,S'$ are neighbouring if they only differ by one element and the difference between $F(\omega;S)$ and $F(\omega;S')$ is $2G/|S|$-Lipschitz over $\cK\subseteq\R^d$, where
% \begin{align*}
%     F(x;S)=\frac{1}{|S|}\sum_{s_i\in S}f(\omega;s_i).
% \end{align*}
% \end{definition}


A DP algorithm $\M$ usually satisfies a collection of $(\eps,\delta)$-DP guarantees for each $\epsilon$, i.e., for each $\epsilon$ there exists some smallest $\delta$ for which $\M$ is $(\eps,\delta)$-DP. By collecting all of them together, we can form the privacy curve or privacy profile which fully characterizes the privacy of a DP algorithm.
\begin{definition}[Privacy Curve]
Given two random variables $X,Y$ supported on some set $\Omega$, define the privacy curve $\delta(X\|Y):\R_{\geq 0}\rightarrow [0,1]$ as:
\begin{align*}
    \delta(X\|Y)(\epsilon)=\sup_{{S}\subset \Omega} \Pr[Y\in {S}]-e^{\epsilon}\Pr[X\in {S}].
\end{align*}
\end{definition}

One can explicitly calculate the privacy curve of a Gaussian mechanism as
\begin{equation}
\label{eqn:Gaussian_privacycurve}
   \deltacurve{\cN(0,1)}{\cN(s,1)}(\eps)= \Phi\lp -\frac{\eps}{s}+\frac{s}{2} \rp - e^\eps \Phi\lp -\frac{\eps}{s}-\frac{s}{2} \rp 
\end{equation}
 where $\Phi(\cdot)$ is the Gaussian cumulative distribution function (CDF)~\cite{BalleW18}. 
%  One can also prove the following upper bound on the privacy curve by using the upper bound $\Phi(-t)\leq \exp(-t^2/2)/2$ for $t\ge 0,$ % Double check in a computer, the tight bound has a extra 1/2. (I find it in wiki)
% \begin{equation}
% \label{eqn:Gaussian_privacycurve_approx}
%   \deltacurve{\cN(0,1)}{\cN(s,1)}(\eps)  \le \frac{1}{2} \exp\lp-\frac{1}{2}\lp\frac{\eps}{s}-\frac{s}{2}\rp^2\rp  \text{ if } \eps \geq \frac{s^2}{2}.
% \end{equation}


We say a differentially private mechanism $\M$ has privacy curve $\delta:\R_{\ge0}\rightarrow[0,1]$ if for every $\epsilon\geq0$, $\M$ is $(\epsilon,\delta(\epsilon))$-differentially private, i.e., $\delta(\M(\cD)\| \M(\cD'))(\epsilon)\leq \delta(\eps)$ for all neighbouring databases $\cD,\cD'$.
We will also need the notion of tradeoff function introduced in~\cite{dong2019gaussian} which is an equivalent way to describe the privacy curve $\delta(P\|Q)$.
\begin{definition}[Tradeoff function]
Given two (continuous) distributions $P,Q$, we define the trade-off function\footnote{Tradeoff curves in~\cite{dong2019gaussian} are defined using type I and type II errors. The definition given here is equivalent to their definition for continuous distributions.} $T(P\|Q):[0,1]\to [0,1]$ as $$T(P\|Q)(z)= \inf_{S:P(S)=1-z}Q(S).$$

It is easy to compute explicitly the tradeoff function for Gaussian mechanism~\cite{dong2019gaussian},
\begin{equation}
    \label{eqn:gaussian_tradeoff}
    T(\cN(0,1)\|\cN(s,1))(z)=\Phi(\Phi^{-1}(1-z)-s).
\end{equation}
Note that perfect privacy is equivalent to the tradeoff function $\mathrm{Id}(z)=1-z$ and the closer a tradeoff function is to $\mathrm{Id}$, better the privacy. The tradeoff function $T(P\|Q)$ and the privacy curve $\delta(P\|Q)$ are related via convex duality. Therefore to compare privacy curves, it is enough to compare tradeoff curves.

\begin{proposition}[\cite{dong2019gaussian}]
\label{prop:delta_tradeoff}
$\delta(P\|Q)\le \delta(P'\|Q')$ iff $T(P\|Q)\ge T(P'\|Q')$
\end{proposition}


\end{definition}

\subsection{Optimization}
Here we collect some properties of functions which are useful for optimization and sampling.
\begin{definition}[$L$-Lipschitz Continuity]
A function $f:{\cal K}\rightarrow \R$ is $L$-Lipschitz continuous over the domain ${\cal K}\subset \R^{d}$ if the following holds for all $\omega,\omega'\in {\cal K}:|f(\omega)-f(\omega')|\leq L\|\omega-\omega'\|_2$. 
\end{definition}

% never used
%\begin{definition}[$\beta$-Smoothness]
%A function $f:{\cal K}\rightarrow \R$ is $\beta$-smooth over the domain ${\cal K}\subset \R^{d}$ if for all $\omega,\omega'\in{\cal K}$, $\|\nabla f(\omega)-\nabla f(\omega')\|_2\leq \beta \|\omega-\omega'\|_2$.
%\end{definition}

\begin{definition}[$\mu$-Strongly convex]
A differentiable function $f:\cK\rightarrow \R$ is called strongly convex with parameter $\mu>0$ if ${\cal K}\subset \R^{d}$ is convex and the following inequality holds for all points $\omega,\omega'\in {\cal K}$,
% one def is enough here
%\[
%\langle \nabla f(\omega)-\nabla f(\omega'), \omega-\omega' \rangle\geq \mu \|\omega-\omega'\|_2^2.
%\]
%Equivalently,
\[
f(\omega')\geq f(\omega) +\inpro{\nabla f(\omega)}{\omega'-\omega}+\frac{\mu}{2}\|\omega'-\omega\|_2^2.
\]
\end{definition}

\begin{definition}[Log-concave measure and density]
A density function $f:\cK \rightarrow \R_{\geq 0}$ is log-concave if $\int_{\cK} f(x) dx = 1$ and $f(x) = \exp(-F(x))$ for some convex function $F$. We call $f$ is $\mu$-strongly log-concave if $F$ is $\mu$-strongly convex. Similarly, we call $\pi$ a log-concave measure if its density function is log-concave, and we call $\pi$ is a $\mu$-strongly log-concave measure if its density function is $\mu$-strongly log-concave.
\end{definition}


\subsection{Distribution Distance and Divergence}
We present some distribution distances or divergences mentioned or used in this work.
\label{sec:dis_measure}
\begin{definition}{\cite[R{\'e}nyi Divergence]{Ren61}}
Suppose $1<\alpha<\infty$ and $\pi,\nu$ are measures with $\pi\ll\nu$.
The R{\'e}nyi divergence of order $\alpha$ between $\pi$ and $\nu$ is defined as
\begin{align*}
    \mathrm{D}_{\alpha}(\pi\|\nu)=\frac{1}{\alpha}\log\int\lp\frac{\pi(x)}{\nu(x)}\rp^{\alpha}\nu(x)\d x.
\end{align*}
We follow the convention that $\frac{0}{0}=0$. R{\'e}nyi Divergence of orders $\alpha=1,\infty$ are defined by continuity.
For $\alpha=1$, the limit in R{\'e}nyi Divergence equals to the Kullback-Leibler divergence of $\pi$ from $\nu$, which is defined as following:
\end{definition}

\begin{definition}[Kullback–Leibler divergence]
The Kullback–Leibler divergence between probability measures $\pi$ and $\nu$ is defined by 
\begin{align*}
    \mathrm{D}_{KL}(\pi\|\nu)=\int  \log\lp\frac{\pi}{\nu}\rp\d \pi.
\end{align*}
\end{definition}

\begin{definition}[Wasserstein distance]
Let $\pi,\nu$ be two probability distributions on $\R^d$.
The second Wasserstein distance $\mathrm{W}_{2}$ between $\pi$ and $\nu$ is defined by
\begin{align*}
    \mathrm{W}_2(\pi,\nu)=\big( \inf_{\gamma\in\Gamma(\pi,\nu)}\int_{\R^d\times\R^d}\|x-y\|_2^2\d \gamma(x,y) \big)^{1/2},
\end{align*}
where $\Gamma(\pi,\nu)$ is the set of all couplings of $\pi$ and $\nu$.
\end{definition}

\begin{definition}[Total variation distance]
The total variation distance between two probability measures $\pi$ and $\nu$ on a sigma-algebra $\mathcal{F}$ of subsets of the sample space $\Omega$ is defined via
\begin{align*}
    \mathrm{TV}(\pi,\nu)=\sup_{S\in \mathcal{F}}|\pi(S)-\nu(S)|.
\end{align*}
\end{definition}


\subsection{Isoperimetric Inequality for Strongly Log-concave Distributions}
The cumulative distribution function (CDF) of one-dimensional standard Gaussian distribution will be denoted by $\Phi(x)=\Pr_{y\sim\cN(0,1)}[y\le x]$. The following Lemma relates the expanding property of log-concave measures with $\Phi$.
%Note that by the symmetry of the Gaussian distribution, $\Phi(x)+\Phi(-x)=1$ and $\Phi^{-1}(1-z)=-\Phi^{-1}(z).$
\begin{proposition}[Theorem 1.1. in \cite{Led99}]
\label{prop:isoperimetry}
Let $\pi$ be a $\mu$-strongly log-concave measure supported on a convex set $\cK \subseteq \R^d$. Let $A\subset\cK$ by any subset such that $\pi(A)=z$. For any point $x\in\R^d$, define $d(x,A)=\inf_{y\in A}\|x-y\|_2$. Let $A_r = \lc x: d(x,A)\le r\rc$. Then if $A_r\subseteq\cK$, for every $r\ge 0$, $$\pi(A_r) \ge \Phi(\Phi^{-1}(z)+r\sqrt{\mu}).$$
\end{proposition}
The property above implies the concentration of Lipschitz functions over log-concave measures.
\begin{corollary}
\label{cor:isoperimetry_lip}
Let $\pi$ be a $\mu$-strongly log-concave measure supported on a convex set $\cK\subseteq\R^d$. Suppose $\alpha:\cK\to \R$ is $G$-Lipschitz. For $z\in [0,1]$, define $m(z)\in \R$ such that $\Pr_{x\sim \pi}[\alpha(x)\le m(z)]=z$. Then for every $r\ge 0$,
$$\Pr_{x\sim \pi}[\alpha(x) \ge m(z)+r] \le \Phi\lp \Phi^{-1}(1-z)-\frac{r\sqrt{\mu}}{G}\rp,$$
$$\Pr_{x\sim \pi}[\alpha(x) \le m(z)-r] \le \Phi\lp \Phi^{-1}(z)-\frac{r\sqrt{\mu}}{G}\rp.$$
\end{corollary}
\begin{proof}
Fix some $z\in [0,1]$. Let $A=\{x\in \cK:\alpha(x)\le m(z)\}$, so $\pi(A)=z$. Let $A_r=\{x:d(x,A)\le r\}.$ Since $\alpha$ is $G$-Lipschitz, $\alpha(x)\ge m(z)+r$ implies that $d(x,A)\ge r/G.$ Therefore $\lc x: \alpha(x) \ge m(z)+r\rc \subset \lc x: d(x,A) \ge r/G\rc = \overline{A_{r/G}}$ and so 
\begin{align*}
\Pr_{x\sim \pi}[\alpha(x) \ge m(z)+r] &\le \pi(\overline{A_{r/G}}) \\
&= 1- \pi(A_{r/G})\\
&\le 1 - \Phi\lp\Phi^{-1}(z)+\frac{r\sqrt{\mu}}{G}\rp\\
&= \Phi\lp-\Phi^{-1}(z)-\frac{r\sqrt{\mu}}{G}\rp.
\end{align*}
We obtain the other inequality by applying the above inequality to $-\alpha(x).$
\end{proof}



% \begin{theorem}[Sampling]

% \end{theorem}

