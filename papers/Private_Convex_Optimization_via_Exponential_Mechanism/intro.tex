\section{Introduction}
Differential Privacy (DP), introduced in~\cite{DMNS06,DKMMN06}, is increasingly becoming the universally accepted standard in privacy protection. We see an increasing array of adoptions in industry~\cite{Apple17,EPK14,BEM+17,DKY17} and more recently the US census bureau \cite{Abo16,KCK+18}. 
Differential privacy allows us to quantify the privacy loss of an algorithm and is defined as follows.

\begin{definition}[$(\epsilon,\delta)$-DP]
A randomized mechanism $\M$ is $(\epsilon,\delta)$-differentially private if for any neighboring databases $\cD,\cD'$ and any subset $S$ of outputs, one has
\begin{align*}
    \Pr[\M(\cD)\in S]\leq e^\epsilon\Pr[\M(\cD')\in S] +\delta.
\end{align*} In this paper, we say $\cD$ and $\cD'$ are neighboring databases if they agree on all the user inputs except for a single user's input.
\end{definition}

Privacy concerns are particularly acute in machine learning and optimization using private user data. Suppose we want to minimize some loss function $F(x;\cD):\cK\to \R$ for some domain $\cK$ where $\cD$ is some database.
We want to output a solution $x^{priv}$ using differentially private mechanism $\cM$ such that we minimize the \emph{excess empirical risk}
\begin{equation}
\label{eqn:empirical_risk}
\E_{\cM}[F(x^{priv};\cD)]-F(x^*;\cD),    
\end{equation}
 where $x^*\in \cK$ is the true minimizer of $F(x;\cD)$. 
 
 \paragraph{Exponential Mechanism} One of the first mechanisms invented in differential privacy, the \emph{exponential mechanism}, was proposed by \cite{MT07} precisely to solve this. It involves sampling $x^{priv}$ from the density 
\begin{equation}
    \label{eqn:exponential_mechanism}
    \pi_\cD(x)\propto \exp\lp-kF(x;\cD)\rp.
\end{equation}
 Here $k$ controls the privacy-vs-utility tradeoff, large $k$ ensures that we get a good solution but less privacy and small $k$ ensures that we get good privacy but we lose utility. Suppose $\Delta_F = \sup_{\cD\sim\cD'} \sup_x |F(x;\cD)-F(x;\cD')|$ is the sensitivity of $F$, where the supremum is over all neighboring databases $\cD,\cD'$. Then choosing $k=\frac{\eps}{2\Delta_F}$, the exponential mechanism satisfies $(\eps,0)$-DP. 
 
 Exponential mechanism is widely used both in theory and in practice, such as in mechanism design \cite{HK12}, convex optimization \cite{BST14,MV21}, statistics \cite{WZ10,WM10,AKR+19}, machine learning and AI \cite{ZP19}. Even for infinite and continuous domains, exponential mechanism can be implemented efficiently for many problems \cite{HT10,CSS13,KT13,BV19,CKS20}. There are also several variants and generalizations of the exponential mechanism which can improve its utility based on different assumptions \cite{TS13,BNS13,RS16,LT19}. See \cite{LT19} for a survey of these results.
 
 
% One of the important case that exponential mechanism is known to be sub-optimal is convex optimization. Under $(\eps,\delta)$-DP, \cite{BST14,bftt19,bfgt20} shows that noisy stochastic gradient descent achieves the empirical risk roughly $\sqrt{d}/n$ for $\cK\subset \R^d$ with $n$ users. However, 
 
% Exponential mechanism can be efficiently implemented for both finite domains and infinite and continuous domains \cite{HT10,CSS13,KT13,BV19,CKS20}.
 
 %when $\cK$ is a finite.
 
 
 %Suppose $\Delta_F = \sup_{\cD\sim\cD'} \sup_x |F(x;\cD)-F(x;\cD')|$ is the sensitivity of $F$, where the supremum is over all neighboring databases $\cD,\cD'$. Then choosing $k=\frac{\eps}{2\Delta_F}$, the exponential mechanism satisfies $(\eps,0)$-DP. 
 %For convex loss $F$, the empirical risk in (\ref{eqn:empirical_risk}) is bounded by $\frac{d}{k}=O\lp\frac{d\Delta_F}{\eps}\rp$ and this is also known to be optimal under pure differential privacy~\cite{BST14}. 
 
 
% In this paper, we study on the case $F$ is convex and $\cK$ is a convex set. We show one can significantly improve the bound by adding a $\ell^2_2$ regularizer term to $F$ under $(\eps,\delta)$-DP.
 
 
% One can significantly improve the empirical risk from $O_\eps(d)$ to $O_{\eps,\delta}(\sqrt{d})$ under convexity assumptions on $F$ and by asking for $(\eps,\delta)$-DP instead of $(\eps,0)$-DP. 
%In this paper, we st
% We will now define this setting formally. 
 \paragraph{DP Empirical Risk Minimization (DP-ERM)}
 In many applications, the loss function is given by the average of the loss of each user:
   \begin{align}
\label{eq:DPERM}
    F(x;\cD):=\frac{1}{n}\sum_{i=1}^n f(x; s_i).
\end{align} 
 where $\cD = \{s_1,s_2,\cdots,s_n\}$ is the collection of users $s_i$ and $f(x; s_i)$ is the loss function of user $s_i$. 
 
 Throughout this paper, we assume $f(x;s)$ is convex and $f(x;s)-f(x;s')$ is $G$-Lipschitz for all $s,s'$, and $\cK\subset \R^d$ is convex with diameter $D$.\footnote{Some of our results can handle the unconstrained domain, such as $\cK=\R^d$.} We call the problem of minimizing the excess empirical risk in \eqref{eq:DPERM} as DP Empirical Risk Minimization (DP-ERM). This setting is well studied by the DP community with many exciting results \cite{CM08,rbht09,cms11,jt14,BST14,kj16,fts17,zzmw17,Wang18,ins+19,bftt19,FKT20,KLL21,bgn21,LL21,AFKT21,sstt21,MBST21,GTU22}.\footnote{Most of the literature uses a stronger assumption that $f(x;s)$ is $G$-Lipschitz, while some of our results only need to assume the difference $f(x;s)-f(x;s')$ is $G$-Lipschitz.}
 
In particular, \cite{BST14} shows that exponential mechanism in (\ref{eqn:exponential_mechanism}) achieves the optimal excess empirical risk of $O\lp \frac{GDd}{n\eps}\rp$ under $(\eps,0)$-DP. On the other hand, \cite{BST14,bftt19,bfgt20} show that \emph{noisy gradient descent} on $F(x;\cD)$ achieves an excess empirical risk of 
\begin{equation}
\label{eqn:optimal_empirical_risk}
O\lp \frac{GD\sqrt{d \log(1/\delta)}}{n\eps}\rp    
\end{equation}
under $(\eps,\delta)$-DP, which is also shown to be optimal~\cite{BST14}. This is a significant $\sqrt{d}$ improvement over the exponential mechanism. 

Exponential mechanism is a universally powerful tool in differential privacy.
However, nearly all of the previous works on DP-ERM rely on noisy gradient descent or its variants to achieve the significant $\sqrt{d}$ improvement over exponential mechanism under $(\eps,\delta)$-DP. 
One natural question is whether noisy gradient descent has some extra ability that exponential mechanism lacks or we didn't use exponential mechanism optimally in this setting. This brings us to the first question.
\begin{question}
Can we obtain the optimal empirical risk in \eqref{eqn:empirical_risk} under $(\eps,\delta)$-DP using exponential mechanism?
\end{question}

\paragraph{DP Stochastic Convex Optimization (DP-SCO)} Beyond the privacy guarantee and the empirical risk guarantee, another important guarantee is the generalization guarantee. Formally, we assume the users are sampled from an unknown distribution $\cP$ over convex functions. We define the loss function as
\begin{align}
\label{eq:DPSCO}
    \HF(x)=\E_{s\sim \cP}[f(x;s)].
\end{align}
We want to design a DP mechanism $\cM$ which outputs $x^{priv}$ given users $\cD = \{s_1,s_2,\dots,s_n\}$ independently sampled from $\cP$ and minimize the \emph{excess population loss}
\begin{equation}
\label{eqn:population_loss}
    \E_{\cM,\cD\sim \cP}[\HF(x^{priv})] - \HF(x^*)
\end{equation}
where $x^*$ is the minimizer of $\HF(x)$. We call the problem of minimizing the excess population loss in \eqref{eqn:population_loss} as DP Stochastic Convex Optimization (DP-SCO).
By a suitable modification of noisy stochastic gradient descent, \cite{bftt19,FKT20} show that one can achieve the optimal population loss of
\begin{equation}
\label{eqn:optimal_population_loss}
O\lp GD \lp\frac{1}{\sqrt{n}}+\frac{\sqrt{d\log(1/\delta)}}{\eps n}\rp\rp.
\end{equation}
\cite{bftt19} bounds the generalization error by showing that running SGD on smooth functions is stable and \cite{FKT20} proposes an iterative localization technique.
Note that only the algorithm for smooth functions in \cite{bftt19} can achieve both optimal empirical risk and optimal population loss at the same time, with the price of taking more gradient queries and loss of efficiency.
%$O\left(\min \left\{n^{3 / 2}, n^{5 / 2} / d\right\}\right)$ which uses  gradient queries and is less efficient than the linear rate achieved by \cite{FKT20}.
%algorithms for non-smooth functions does not simultaneously give the optimal empirically risk and the optimal population loss and 
It is unclear to us how one can obtain both using current techniques for non-smooth functions.
%\Yintat{make sure it is not a lie}. 
This brings us to the second question. 
\begin{question}
Can we achieve both the optimal empirical risk and the optimal population loss for non-smooth functions with the same algorithm?
\end{question}
%\Gopi{Cite the appropriate papers.}

\paragraph{Sampling} Without extra smoothness assumptions on $f$, currently, there is no optimally efficient algorithm for both problems. For example, with oracle access to gradients of $f$, the previous best algorithms for DP-SCO use:
\begin{itemize}
    \item $\widetilde{O}(n d)$ queries to $\nabla f(x;s)$ (by combining \cite{FKT20}, Moreau-Yosida regularization and cutting plane methods),
    \item $\widetilde{O}(\min(n^{3/2},n^{2}/\sqrt{d}))$ queries to $\nabla f(x;s)$ \cite{AFKT21},
    \item $\widetilde{O}(\min(n^{5/4}d^{1/8},n^{3/2}/d^{1/8}))$ queries to $\nabla f(x;s)$ \cite{KLL21}.
\end{itemize}
Combining these results, this gives an algorithm for DP-SCO that uses 
$$\widetilde{O}(\min(nd, n^{5/4}d^{1/8}, n^{3/2}/d^{1/8},n^{2}/\sqrt{d}))$$
many queries to $\nabla f(x;s)$. Although the information lower bound for non-smooth functions with the gradient queries is open, it is unlikely that the answer involves four different cases. 

In this paper, we focus on the function value query (zeroth order query) on $f(x;s)$. This query is weaker than gradient query as it obtains $d$ times less information. They are used in many practical applications such as clinical trials and ads placement when the gradient is not available and is also useful in bandit problems.
This brings us to the third question.

\begin{question}
Can we obtain an algorithm with optimal query complexity for DP-SCO for zeroth order query model?
\end{question}



%it is open how to develop time optimal algorithm 
%Developing time efficient algorithms for both 


%Suppose that $F(x;\cD)$ is convex and for $F(x;\cD)-F(x;\cD')$ is $G$-Lipchitz for any two neighboring databases $\cD,\cD'$. For example, this happens when $F(x;\cD)=\frac{1}{n}\sum_{i=1}^n f_i(x)$ and each $f_i$ is convex and Lipschitz. Then there are $(\eps,\delta)$-DP mechanisms than the exponential mechanism which achieve an excess  risk of 
%\begin{equation}
%\label{eqn:optimal_empirical_risk}
%\E_{\cM}[F(x^{priv})]-F(x^*)\le %\lp\frac{GD\sqrt{d\log(1/\delta)}}{n\eps }\rp    
%\end{equation}
% where $D=\norm{\cK}_2$ is the diameter of the domain $\cK$.% \Gopi{cite relevant papers}.
% There are also lower bounds showing that this is optimal \cite{BST14}. This is much better than the excess  risk of 
% $O\lp \frac{GDd}{n\eps}\rp$achieved by the exponential mechanism under $(\eps,0)$-DP. Though they are strictly not comparable, setting $\delta=o\lp\frac{1}{n}\rp$ (where $n$ is the number of users) is good enough to get privacy and so $\sqrt{d\log(1/\delta)}\ll d$ for large dimension $d$. If additionally, $F(x;\cD)$ is $\mu$-strongly convex, then we can handle both constrained (bounded $\cK$) and unconstrained (e.g. $\cK=\R^d$) case  and achieve an excess risk of 
% \begin{equation}
%    \label{eqn:optimal_empirical_risk_stronglyconvex}
%    \E_{\cM}[F(x^{priv})]-F(x^*)\le \lp\frac{G^2d\log(1/\delta)}{\mu n^2\eps^2}\rp,    
%\end{equation}
%which is also shown to be optimal \cite{BST14}.
%These mechanisms are very different from the exponential mechanism and broadly fall into the following categories:
% \begin{itemize}
%     \item \textbf{Output Perturbation}: Under strong convexity assumptions on $f_i$, one can show that the optimal solution $x^*$ doesn't move too much for neighboring databases. So by adding ....
%     \item \textbf{Objective Perturbation}:
%     \item \textbf{Gradient Perturbation}:
% \end{itemize}
%One of the mechanisms which achieves the (nearly) optimal excess risk in (\ref{eqn:optimal_empirical_risk}) and (\ref{eqn:optimal_empirical_risk_stronglyconvex}) is \emph{gradient perturbation}, also called \emph{noisy gradient descent} \cite{BST14,bftt19,bfgt20}. In this mechanism, we do (stochastic) gradient descent on $F(x;\cD)$ and add Gaussian noise to the gradient before each step. Since each step is a Gaussian mechanism (or a subsampled Gaussian mechanism), we can compute the privacy loss in each step. Finally, we can use advanced composition theorem or the moments accountant for DP \cite{DRV10,ACG+16}%(\Gopi{cite})
% to compose all the privacy losses across steps. All these prior algorithms achieving the optimal risk require access to gradients of $F(x;\cD)$.\Gopi{Verify this is true}


\subsection{Our Contributions}
In this paper, we give a positive answer to all these questions using the \emph{Regularized Exponential Mechanism}. If we add an $\ell_2^2$ regularizer to $F$ and sample $x^{priv}$ from the density 
\begin{equation}
\label{eqn:our_mechanism}
\exp\lp-k\lp F(x;\cD)+\mu \norm{x}_2^2/2\rp\rp,
\end{equation}
then, for a suitable choice of $\mu$ and $k$, we recover the optimal excess risk in (\ref{eqn:optimal_empirical_risk}) for DP-ERM and optimal population loss in (\ref{eqn:optimal_population_loss}) for DP-SCO. Finally, we give an algorithm to sample $x^{priv}$ from the density \eqref{eqn:our_mechanism} with nearly optimal number of queries to $f(x;s)$ (See Figure \ref{fig:sample_runtime}). To the best of our knowledge, our algorithm is the first whose query complexity has \emph{polylogarithmic dependence} in both dimension and accuracy (in TV distance).

%Note that our mechanism is essentially how statisticians perform modeling: model the data by a log-density $f(x;s)$, add some regularizer, and learn the model parameters by a sample. Therefore, our result provides insights on the importance and advantages of a good regularizer, and a guideline on how to choose it. \Gopi{I didn't understand this paragraph.}

%Previously, sampling algorithms involves a large polynomial runtime for non-smooth objective (See Figure \ref{fig:sample_runtime}). Surprisingly, our algorithm is the first with the query complexity \emph{polylogarithmic dependence} in both dimension and accuracy and the first that is optimal in any function class we aware of.

Formally, our result is follows:
\begin{theorem}[DP-ERM, Informal]
Let $\cK$ be a convex set with diameter $D$ and $\{f(\cdot;s)\}$
be a family of convex functions on $\cK$ where $f(\cdot;s)-f(\cdot;s')$ is $G$-Lipschitz for all $s,s'$. Given a
database $\mathcal{D}=\{s_{1},s_{2},\cdots,s_{n}\}$, for any $\epsilon,\delta\in(0,\frac{1}{10})$,
\footnote{See Theorem~\ref{thm:DPERM} for general conclusions for all $\eps>0$}
the regularized exponential mechanism 
\[
x^{(priv)}\propto\exp\lp-k\cdot\lp\frac{1}{n}\sum_{i=1}^{n}f(x;s_{i})+\frac{\mu}{2}\|x\|_{2}^{2}\rp\rp
\]
is $(\epsilon,\delta)$-DP with expected excess empirical loss
\[
\frac{2GD\sqrt{d\log(1/\delta)}}{\epsilon n}
\]
for some appropriate choices of $k$ and $\mu$. Furthermore, if $f(\cdot;s)$ is $G$-Lipschitz for all $s$, 
we can sample $x^{(priv)}$ using $O(\frac{\eps^2n^2}{\log(1/\delta)}\log^2(\frac{nd}{\delta}))$ queries in expectation to the values of $f(x;s)$.
\end{theorem}

\begin{theorem}[DP-SCO, Informal]
Let $\cK$ be a convex set with diameter $D$ and $\{f(\cdot;s)\}$
be a family of convex functions on $\cK$ where $f(\cdot;s)-f(\cdot;s')$ is $G$-Lipschitz for all $s,s'$. 
Given a
database $\mathcal{D}=\{s_{1},s_{2},\cdots,s_{n}\}$ of samples from
some unknown distribution $\mathcal{P}$. For any $\epsilon,\delta\in(0,\frac{1}{10})$,\footnote{
See Theorem~\ref{thm:dpsco_impl} for general conclusions for all $\eps>0$.
}
the regularized exponential mechanism 
\[
x^{(priv)}\propto\exp\lp-k\cdot\lp\frac{1}{n}\sum_{i=1}^{n}f(x;s_{i})+\frac{\mu}{2}\|x\|_{2}^{2}\rp\rp
\]
is $(\epsilon,\delta)$-DP with expected excess population loss
\[
\frac{2GD}{\sqrt{n}}+\frac{2GD\sqrt{d\log(1/\delta)}}{\epsilon n}
\]
for some appropriate choice of $k$ and $\mu$. Furthermore, if $f(\cdot;s)$ is $G$-Lipschitz for all $s$, 
we can sample $x^{(priv)}$ using $O(\min\{\frac{\eps^2n^2}{\log(1/\delta)},nd\}\log^2(\frac{nd}{\delta}))$ queries in expectation to the
values of $f(x;s)$ and the expected number of queries is optimal up to logarithmic terms. 
\end{theorem}

For DP-SCO, we provide a nearly matching information-theoretic lower bound on the number of value queries (Section~\ref{sec:infolower}), proving the optimality of our sampling algorithm.
Moreover, when $f$ is already strongly convex, our proof shows the exponential mechanism (without adding a regularizer) itself simultaneously achieves both the optimal excess empirical risk and optimal population loss. 

In a {\em concurrent and independent} work, \cite{GTU22} study the DP properties of Langevin Diffusion, and provide optimal/best known private empirical risk and population loss under both pure-DP ($\delta=0$) and approximate-DP ($\delta>0$) constraints.
Utility/privacy trade-off of non-convex functions is also discussed.

%Unfortunately, if $f$ is just convex, we need to pick different $\mu$ and different $k$ for DP-ERM and DP-SCO. We are not sure how to obtain the optimal excess empirical risk and optimal population loss with the same algorithm and the same parameters. We leave this as an open problem.



% \begin{theorem}[Lower bound for DP-SCO]
% For any (non-private) algorithm which makes less than $O\lp\min\{\frac{\eps^2n^2}{\log(1/\delta)},nd\}\rp$ function value queries, there exists a convex domain $\cK\subset \R^d$ of diameter $D$, a distribution $\cP$ supported over $G$-Lipschitz linear functions $f(x;s)\defeq\langle x,s\rangle$ such that the output $\hx$ of the algorithm has excess population loss
% \begin{align*}
%     \E_{s\sim\cP}[\langle \hx,s\rangle]-\min_{x\in\cK}\E_{s\sim\cP}[\langle x,s\rangle]\geq \Omega\lp \frac{GD}{\sqrt{1+\log(n)/d}}\cdot \min \lc 1,\frac{\sqrt{\log(1/\delta)d}}{\eps n}+\frac{1}{\sqrt{n}}\rc \rp.
% \end{align*}

% \end{theorem}

 %Finally, we are not sure if the constant ($3$ and $4$ in the loss term) in the theorems above are optimal.  3 is not optimal, we can easily make the proof 2.0001

% \begin{algorithm2e}
% \SetKwInOut{Input}{input}
% \SetKwInOut{Output}{output}
% \caption{Regularized Exponential Mechanism \label{algo:regularized_exp}}
% \Input{Convex function $F$, parameters $k,\mu\in \R^{>0}$}
% \Output{ A sample $x^{priv}$ from the density $\pi_\cD(x) \propto \exp\lp-k\lp F(x;\cD)+\mu \norm{x}_2^2/2\rp\rp$}
% \end{algorithm2e}

