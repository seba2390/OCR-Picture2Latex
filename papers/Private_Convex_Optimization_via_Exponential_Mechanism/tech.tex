\section{Techniques}

The main contribution of this paper is the discovery that adding regularization terms in exponential mechanism leads to optimal algorithms for DP-ERM and DP-SCO. For this, we develop some important tools that could be of independent interest. We now briefly discuss each of the main tools.

% Our proofs mainly rely on some less well-known techniques and involves surprising twists.\Gopi{This isn't a movie, lol. Need to use more formal language in papers.}
% In this section, we discuss some techniques that maybe of independent interest. 

\subsection{Gaussian Differential Privacy (GDP) of Regularized Exponential Mechanism}

To analyze the privacy of the regularized exponential mechanism, we need to bound the privacy curve between a strongly log-concave distribution and its Lipschitz perturbation in the exponent. \cite{MASN16} gave a nearly tight (up to constants) privacy guarantee of exponential mechanism if the distribution $\exp(-k F(x;\cD))$ satisfies Logarithmic Sobolev inequality (LSI). Since strongly log-concave distributions satisfy LSI, their result immediately gives the $(\epsilon,\delta)$-DP guarantee of our algorithm. However, this gives a sub-optimal privacy bound because it does not fully take advantage of the strongly log-concave property.

Instead, we show directly that the privacy curve between a strongly log-concave distribution and its Lipschitz perturbation in the exponent is upper bounded by the privacy curve of an appropriate Gaussian mechanism. This new proof uses the notion of tradeoff function introduced in~\cite{dong2019gaussian} and the isoperimetric inequality for strongly log-concave distribution.

\begin{theorem}
\label{thm:privacy_technical}
Given convex set $\cK\subseteq \R^d$ and $\mu$-strongly convex functions $F,\Tilde{F}$ over $\cK$. Let $P,Q$ be distributions over $\cK$ such that $P(x)\propto e^{-F(x)}$ and $Q(x)\propto e^{-\Tilde{F}(x)}$.
If $\Tilde{F}-F$ is $G$-Lipschitz over $\cK$, then for all $\eps>0$,
\begin{align*}
    \deltacurve{P}{Q}(\epsilon) 
    \leq \deltacurve{\cN\lp 0,1\rp}{\cN\lp\frac{G}{\sqrt{\mu}},1\rp}(\epsilon).
\end{align*}
\end{theorem}
This proves that the privacy curve for distinguishing between $P,Q$ is upper bounded the privacy curve of a Gaussian mechanism with sensitivity $G/\sqrt{\mu}$ and noise scale 1.

\paragraph{Tightness:} Note that Theorem~\ref{thm:privacy_technical} is completely tight because it contains the privacy of Gaussian mechanism as a special case. If $F(x)=\norm{x}_2^2/2$ and $\Tilde{F}(x)=\norm{x-a}_2^2/2$ for some $a\in \R^d$, then $\Tilde{F}(x)-F(x)=-\inpro{x}{a}+\norm{a}_2^2/2$ is $G$-Lipschitz with $G=\norm{a}_2$ and $F,\Tilde{F}$ are $1$-strongly convex. And $P=\cN(0,I_d)$ and $Q=\cN(a,I_d)$. Therefore:
$$\deltacurve{P}{Q}=\deltacurve{\cN(0,I_d)}{\cN(a,I_d)}=\deltacurve{\cN(0,1)}{\cN\lp\norm{a}_2,1\rp}$$ which is precisely the upper bound guaranteed by the theorem.

%\noindent Theorem~\ref{thm:privacy_technical} implies the following corollaries for DP-ERM.

% Yin Tat: There are too many theorem
%\begin{corollary}
%Suppose $F(x;\cD)$ is convex over some convex domain $\cK\subset \R^d$ with diameter $D$ and $F(x;\cD)-F(x;\cD')$ is $G$-Lipschitz for all neighboring databases $\cD,\cD'.$ Then the mechanism (\ref{eqn:our_mechanism}) satisfies $(\eps,\delta(\eps))$-DP for all $\eps$ where
%\begin{align*}
% \delta(\eps)\leq \deltacurve{\cN(0,1)}{\cN\lp\frac{G\sqrt{k}}{\sqrt{\mu}},1\rp}(\epsilon).
%\end{align*}
%Moreover, the expected excess empirical loss is bounded by %$\frac{d}{k}+\mu D^2$.
%\end{corollary}

% Yin Tat: There are too many theorem
%\begin{corollary}
%Suppose $F(x;\cD)$ is $\mu$-strongly convex over some convex domain $\cK\subseteq \R^d$ with diameter $D$ and $F(x;\cD)-F(x;\cD')$ is $G$-Lipschitz for all neighboring databases $\cD,\cD'.$ Then the exponential mechanism (\ref{eqn:exponential_mechanism}) satisfies $(\eps,\delta(\eps))$-DP for all $\eps$ where
%\begin{align*}
% \delta(\eps)\leq \deltacurve{\cN(0,1)}{\cN\lp\frac{G\sqrt{k}}{\sqrt{\mu}},1\rp}(\epsilon).
%\end{align*}
%Moreover, the expected excess empirical loss is bounded by $\frac{d}{k}$.
%\end{corollary}


%\begin{corollary}
%Suppose that $F(x;\cD)$ is convex and for $F(x;\cD)-F(x;\cD')$ is $G$-Lipschitz for any two neighboring databases $\cD,\cD'$. Then we have following:
%\begin{enumerate}
%    \item Sampling $x^{priv}$ from the density in (\ref{eqn:our_mechanism}) with $$k= \frac{c\eps^2\mu}{G^2\log(1/\delta)} \text{ and } \mu=\frac{G\sqrt{d\log(1/\delta)}}{\eps D},$$ for some sufficiently small constant $c>0$, satisfies $(\eps,\delta)$-DP and achieves the optimal excess risk in (\ref{eqn:optimal_empirical_risk}). 
%    \item If $F(x;\cD)$ is already $\mu$-strongly convex, then sampling from the exponential mechanism in (\ref{eqn:exponential_mechanism}) with $k=\frac{c\eps^2\mu}{G^2\log(1/\delta)}$, for some sufficiently small constant $c>0$, satisfies $(\eps,\delta)$-DP and achieves the optimal excess risk in (\ref{eqn:optimal_empirical_risk_stronglyconvex}).
%\end{enumerate}
% Moreover, in both cases, we achieve $\gamma$-Gaussian Differential Privacy (GDP) with $\gamma=\frac{G\sqrt{k}}{\sqrt{\mu}}$, i.e., $$\deltacurve{\pi_\cD}{\pi_\cD'} \ge \deltacurve{N(0,1)}{N(\gamma,1)}.$$ Or in other words, we get as much privacy as a Gaussian mechanism with sensitivity $\gamma$ and noise scale $1.$
%\end{corollary}



% ================================================================

% Since proposed by \cite{MT07}, the simple exponential mechanism has been a fundamental building block to construct a randomized estimator which satisfies pure differential privacy ($(\eps,0)$-differential privacy).
% For some utility score $F:\cK\times\Xi^{n}\rightarrow \R$ with bounded sensitivity $\Delta F\defeq \max_{x\in\cK}\max_{S,S'}|F(x;S)-F(x;S')|$ where $S$ and $S'$ are neighboring, the exponential mechanism selects and outputs an element $r\in\cR$ with probability  proportional to $\exp(\frac{\eps F(x;S)}{2\Delta F})$ for given data-set $S$.
% With the simple structure and easy to analyze, exponential mechanism is widely used, such as mechanism design \cite{HK12}, convex optimization \cite{BST14}, statistic \cite{WZ10,WM10,AKR+19}, artificial intelligence \cite{ZP19} and so on.

% Over the past decade, the research on exponential mechanism has never stopped, and many exciting developments have been made.
% Exponential mechanism can be implemented efficiently over a finite data-set of appropriate size, but how to implement it over super polynomial size or even infinite and continuous domain will incur new challenge, which are discussed by \cite{HT10,CSS13,KT13,BV19,CKS20}.
% The utility of the exponential mechanism depends heavily on the worst case sensitivity, but it is very common that there are very few base cases where the sensitivity is very large, and other instances have low sensitivity. There are many interesting results \cite{TS13,BNS13,RS16,LT19} on how to generalize the exponential mechanism and make use of the different sensitivity to get better utility under weaker privacy condition ($(\eps,\delta)$-differential privacy for $\delta>0$). Read a more detail discussion therein \cite{LT19}.

% The simple mechanism suggests a very good and general framework for pure differential privacy. A very natural and important question is whether there is some simple and similar mechanism which can be used to present approximate differential privacy?
% Though pure DP provides stronger privacy protection from definition, the approximate DP also has its advantages.
% The typical polynomial small setting of $\delta=o(1/n)$ is usually good enough to protect privacy in practice, and there are many problems which can have much better utility guarantee under the requirement of approximate DP than pure DP, for example, the private learning and sanitization \cite{BNS13}, the empirical risk minimization \cite{BST14}, and  statistical queries \cite{SU15}.

% Motivated by this, we study the substitution of exponential mechanism for approximate DP, and find there is actually a simple mechanism which can yields approximate DP.
% Roughly speaking, if $F$ is $\mu$-strongly convex and $F(\cdot,S)-F(\cdot,S')$ is $G$-Lipschitz over $x\in\K$, then sampling $x\in \cK$ with the probability proportional to distribution $\exp(-F(x;S))$ is $(\eps,\delta)$-DP where $\delta\leq \deltacurve{\cN\lp 0,1\rp}{\cN\lp\frac{G}{\sqrt{\mu}},1\rp}(\epsilon)$, which is the privacy curve of Gaussian mechanism.
% We think this can be a quite standard result, and the bound in our conclusion is tight.
% To deal with the general case when the objective function is not strongly convex, we can calibrate it with some carefully chosen regularization terms, and that's the reason we name it by Regularization Exponential Mechanism.
% \begin{theorem}
% Maybe give a statement here about our main result.
% \end{theorem}

%Our proof is pretty simple, as well. \Daogao{Say something, Please.}

%Exponential Mechanism (Generalized Exponential mechanism) \cite{RS16,LT19}
%We show that the sample complexity of these tasks under approximate differential privacy can be significantly lower than that under pure differential privacy

\subsection{Generalization Error of Sampling}
Many important and fundamental problems in machine learning, optimization and operations research are special cases of
SCO, and ERM is a classic and widely-used approach to solve it, though their relationships are not well-understood.
If one can solve the ERM problem optimally and get the exact optimal solution $x^*$ to minimizing $F(\cdot;\cD)$ (see Equation~\ref{eq:DPERM}), then \cite{SSSSS09} showed $x^*$ will also be a good solution to the SCO for strongly convex functions.
But in most situations, solving ERM optimally costs too much or even impossible. 
Can we find a approximately good solution to ERM and hope that it is also a good solution for SCO?
\cite{Fel16} provides a negative answer and shows there is no good uniform convergence between $F(\cdot;\cD)$ and $\HF$, that is there always exists $x\in\cK$ such that $|F(x;\cD)-\HF(x)|$ is large.
This fact forces us to find approximate solution to ERM with very high accuracy, which makes the algorithms inefficient.

Prior works proposed a few interesting ways to overcome this difficulty, such as the uniform stability in \cite{HRS16} and the iterative localization technique in \cite{AFKT21}.
Roughly speaking, uniform stability means that if running algorithms on neighboring datasets lead to similar output distributions, then the generalization error of the ERM algorithm is bounded.
Thus a good solution to ERM obtained by a stable algorithm is also a good solution for SCO.
\cite{bftt19} makes use of the stability of running SGD on smooth functions to get a tight bound on the population loss for DP-SCO.


Recall $F(x;\cD)$ and $\HF(x)$ are defined in Equation~\eqref{eq:DPERM} and \eqref{eq:DPSCO} respectively. 
Our result enriches the toolbox of bounding the generalization error and provides new insights for this problem.
\begin{theorem}
Suppose $\{f_i\}$ is a family of $\mu$-strongly convex functions over $\cK$ and $f_i-f_{i'}$ is $G$-Lipschitz for any two functions $f_i,f_{i'}$ in the family.
For any $k>0$ and suppose the $n$ samples in data set $\cD$ are drawn i.i.d from the underlying distribution, then by sampling $x^{(sol)}$ from density $\propto e^{-kF(x^{(sol)};\cD)}$, the population loss satisfies
\begin{align*}
    \E[\HF(x^{(sol)})]-\min_{x\in\cK}\HF(x)\leq \frac{G^2}{\mu n}+ \frac{d}{k}.
\end{align*}
\end{theorem}

Considering two neighboring datasets $\cD$ and $\cD'$, our result is based on bounding the Wasserstein distance between the distributions proportional to $e^{-kF(x;\cD)}$ and $e^{-kF(x;\cD')}$, which means the sampling scheme is stable and leads to the $\frac{G^2}{\mu n}$ term in generalization error.
% The proof makes use of the fact that any divergence measure that decreases under post-processing such as KL divergence.
The other term $\frac{d}{k}$ is excess empirical loss of the sampling mechanism.
One advantage of our result is that it works for both smooth and non-smooth functions. 
Moreover, we may choose the value $k$ carefully and get a solution with both optimal empirical loss and optimal population loss.

% Write the statement of generalization error here (Theorem 6.10, make it as a restatable, change the statement such that it is just about generalization, but not about DP so that other people can use).
% Also, explain the proof Theorem 6.10.


\subsection{Non-smooth Sampling and DP Convex Optimization }
Implementing the exponential mechanism involves sampling from a log-concave distribution.
When the negative log-density function $F$ is smooth, i.e. the gradient of $F$ is Lipschitz,
there are many efficient algorithms for this sampling tasks
such as \cite{D17,LSV18,MMW+19,CV19,DMM19,shen2019randomized,CDW+20,LST20}.
For example, if $F=\frac{1}{n}\sum_{i=1}^{n}f_{i}$
and each $f_{i}$ is $1$-strongly convex with $\kappa$-Lipschitz
gradient,\footnote{For convenience, we used $f_{i}$ to denote the function $f(\cdot;s_i)$ in this and Section \ref{sec:sampling}.} we can sample $x\sim\exp(-F(x))$ in $\widetilde{O}(n+\kappa\max(d,\sqrt{nd})\log(1/\delta))$
iterations with $\delta$ error in total variation distance and each iteration involves computing one $\nabla f_{i}(x)$ \cite{LST21}.
Note that this is nearly linear time when $n\gg\kappa^2d$ and the $\delta$ error in
total variation distance can be translated to an extra $\delta$ error in the $(\epsilon,\delta)$-DP
guarantee.

\begin{center}
\begin{figure}[ht]
\begin{centering}
\begin{tabular}{|c|c|c|c|}
\hline 
 & Complexity & Oracle & Guarantee\tabularnewline
\hline 
\hline 
\cite{BST14} & $d^{O(1)}$ & $F(x)$ & $\mathrm{D}_{\infty}\leq\epsilon$\tabularnewline
\hline 
\cite{CDJB20} & $G^{O(1)}d^{5/2}/\epsilon^{4}$ & $\nabla F(x)$ & $\mathrm{W}_{2}\leq\delta$\tabularnewline
\hline 
\cite{JLLV21} + \cite{C21} & $d^{3}$ & $F(x)$ & $\mathrm{TV}\leq\delta$\tabularnewline
\hline 
\cite{GT20}& $\frac{\alpha^{2}G^{4}d}{\epsilon^{2}}$ & $\nabla F(x)$ & $\mathrm{D}_{\alpha}\leq\epsilon$\tabularnewline
\hline 
\cite{LC21} & $\frac{G^{2}}{\delta}$ & $\nabla F(x)$ & $\mathrm{TV}\leq\delta$\tabularnewline
\hline 
This & $G^{2}$ & $f_{i}(x)$ & $\mathrm{TV}\leq\delta$\tabularnewline
\hline 
\end{tabular}
\par\end{centering}
\caption{The complexity of sampling from $\exp(-F(x))$ where $F=\frac{1}{n}\sum_{i}f_{i}$
is $1$-strongly convex and $f_{i}$ are $G$-Lipschitz and convex.
For applications in differential privacy, $\epsilon$ is a constant
and $\delta=n^{-\Theta(1)}$. Polylogarithmic terms are omitted. Only the last result uses the summation structure and queries only one $f_{i}$ each step. \label{fig:sample_runtime}}
\end{figure}
\vspace{-7mm}
\par\end{center}

Unfortunately, when the functions $f_i$ are only Lipschitz but not smooth, this problem is more difficult.
In Table \ref{fig:sample_runtime}, we summarize some existing results on this topic. They use different guarantees such as Renyi divergence $\mathrm{D}_{\alpha}$ of order $\alpha$, Wasserstein distance $\mathrm{W}_{2}$ and total variation distance $\mathrm{TV}$ (defined in subsection~\ref{sec:dis_measure}). For applications in differential privacy, we need either polynomially small $\mathrm{W}_{2}$ or $\mathrm{TV}$ distance, or $\epsilon$ small $\mathrm{D}_{\alpha}$ distance. 


All previous results for non-smooth function use oracle access to $F$ or $\nabla F$ (instead of $f_i$) and have iterative complexity at least $d$ iterations for $\mathrm{W}_{2}$ or TV distance smaller than $1/d$. Because of this, our algorithm is significantly faster than
the previous algorithms and can handle the case when $F$ is expectation of (infinitely many)  $f_i$ directly.
For example, to get the optimal private empirical loss with typical settings where $\epsilon=\Theta(1)$
and $\delta=1/n^{\Theta(1)}$, the previous best samplers use $\widetilde{O}(n^{4}d)$ many queries to $\nabla f_{i}(x)$ by \cite{GT20} or $\widetilde{O}(nd^{3})$
many queries to $f_{i}(x)$ by combining \cite{JLLV21} and \cite{C21}. 
% The extra $n$ factors are because of the need to choose a large scaling factor $k$ in the distribution $\exp(-kF)$ to make it private. 
In comparison, our algorithm only takes $\widetilde{O}(n^{2})$
many $f_{i}(x)$. 

Our result is based on the alternating sampler proposed in \cite{LST21} and a new rejection sampling scheme.

%{[}Copy the main statement here{]}
%To sample from $\exp(-F(x))$ where $F\defeq\E_{i\in I}f_i+\psi(x)$ where $\psi(x)$ is $\mu$-strongly convex and each $f_i$ is $G$-Lipschitz and convex, our algorithm needs $\Tilde{O}(\frac{G^2}{\mu}\log^2(1/\delta))$ (ignore other logarithmic terms) iterations, and in each iteration it queries $O(1)$ many values of $f_i$ in expectation and finally outputs a sample, total variation distance between whose distribution and the $\exp(-F)$ is bounded by $\delta$.
%{[}restatable of the sampling result{]}
\begin{theorem}
%\label{thm:sampler}
Given a $\mu$-strongly convex function $\psi(x)$ defined on a convex set $\cK \subseteq \R^{d}$ and $+\infty$ outside. Given a family of $G$-Lipschitz convex functions $\{f_{i}(x)\}_{i\in I}$ defined on $\cK$ and an initial point $x_0\in \cK$.
Define the function $\widehat{F}(x)=\E_{i\in I}f_{i}(x)+\psi(x)$ and 
the distance $D=\|x_{0}-x^{*}\|_{2}$ for some $x^{*}=\arg\min_{x\in\cK}\HF(x)$.
For any $\delta\in(0,1/2)$, we can generate a random point $x$ that
has $\delta$ total variation distance to the distribution proportional to $\exp(-\widehat{F}(x))$ in
\[
T:=\Theta\lp\frac{G^{2}}{\mu}\log^{2}\lp\frac{G^{2}(d/\mu+D^{2})}{\delta}\rp\rp\text{ steps}.
\]
Furthermore, each steps accesses only $O(1)$ many $f_{i}(x)$ and samples from $\exp(-\psi(x) - \frac{1}{2\eta} \|x-y\|^2_2)$ for $O(1)$ many $y$
in expectation with $\eta = \Theta(G^{-2}/\log(T/\delta))$.
\end{theorem}


% Given a family of convex functions $\{f(\cdot,s)\}_{s\in\Xi}$ over a convex set $\cK\subseteq \R^d$.
% In the Empirical Risk Minimization (ERM) problem, we are given a data set $S=\{s_1,s_2,\cdots,s_n\}$ over the universe $\Xi$ with the objective to
% \begin{align}\label{eq:DPERM}
%     \text{minimize } F(x;S)\defeq \frac{1}{n}\sum_{s_i\in S}f(x,s_i) \text{   over } x\in \cK
% \end{align}

% And we define the excess empirical loss with respect to a solution $x$ by $F(x;S)-F^*$ where $F^*=\min_{x\in\cK}F(x;S)$.
\begin{comment}
The Stochastic Convex Optimization (SCO) problem is closely related to ERM.
In the SCO problem, we suppose the $n$ samples are drawn i.i.d. from some underlying distribution $\mathcal{P}$ and the objective is to
\begin{align}\label{eq:DPSCO}
    \text{minimize } \hat{F}(x)\defeq \E_{s\sim \calP}f(x,s) \text{   over } x\in \cK.
\end{align}
We refer $\hat{F}(x)-\hat{F}^*$ as the population loss where $\hat{F}^*=\min_{x\in\cK}\hat{F}(x)$.
In the private setting, we want to design $(\epsilon,\delta)$ algorithms for both problems.

As ERM and SCO are two fundamental and important problems in optimization, machine learning and many other related fields,
DP-ERM and DP-SCO have been studied by the DP community and there are many exciting results \cite{CM08,rbht09,cms11,jt14,BST14,kj16,fts17,zzmw17,ins+19,bftt19,FKT20,KLL21,bgn21,AFKT21,sstt21}.

Our main result can lead to the following result about DP-ERM directly. A main improvement of our result are the explicit small constants in the statement.

\begin{theorem}[Informal, DP-ERM]
For $1>\eps>0$ and $1>\delta>0$, and let convex set $s\cK\subset\R^d$ and $\{f(\cdot,s)\}_{s\in\Xi}$ be a family of $G$-Lipschitz functions and $\mu$-strong convex over $\cK$. Setting $k=\frac{\eps^2n^2\mu}{8G^2\log(1.25/\delta)}$, and sampling with probability proportional to $\exp(-kF(x;S))$ is $(\eps,\delta)$-differentially private with expected excess empirical loss $\frac{8G^2d\log(1.25/\delta)}{\eps^2n^2\mu}$.

Moreover, if $\cK$ has diameter $D$ and $\{f(\cdot,s)\}_{s\in\Xi}$ are $G$-Lipschitz and convex, setting $k=\frac{\eps^2n^2\mu}{8G^2\log(1.25/\delta)}$ where $\mu=\frac{4G\sqrt{d\log(1.25/\delta)}}{\eps n D}$, sampling with probability proportional to $\exp(-kF(x;S)+\mu\|x\|_2^2/2)$ is $(\eps,\delta)$-differentially private with expected excess empirical loss $\frac{4GD\sqrt{d\log(1.25/\delta)}}{\eps n}$.
\end{theorem}


% With this sampling scheme, we can have the following efficient algorithm for DP-ERM:

% \begin{theorem}[Implementation for DP-ERM]
% For any constants $1\geq \eps> 0$ and $1>\delta>0$, there is an efficient sampler to solve DP-ERM which has the following guarantees:
% \begin{itemize}
%     \item The scheme is $(\eps,\delta)$-differentially private;
%     \item The expected excess empirical loss is bounded by
%     $
%         \frac{4GD\sqrt{d\log(1.26/\delta)}}{\eps n}
%     $
%     \item The scheme takes 
%     $
%         \Theta(\frac{\eps^2n^2}{\log(1/\delta)}\log^2(\frac{\eps^2n^3d}{\delta\log(n/\delta)}))
%     $
%     queries of the values on $f_i(x)$ in expectation.
% \end{itemize}
% \end{theorem}

%{[}Write down our statement for the empirical loss case{]}
\cite{FKT20} suggested a iterative localization technique to reduce the DP-SCO to DP-ERM, which is formalized a little to a framework by \cite{KLL21}. With this framework, we can have the following result about DP-SCO.

\begin{theorem}[DP-SCO]
There is an efficient algorithm to solve DP-SCO which has the following guarantees:
\begin{itemize}
    \item The algorithm is $(\eps,\delta)$-differentially private;
    \item The expected population loss for general convex case is bounded by
    $
        O(GD(\frac{1}{\sqrt{n}}+\frac{\sqrt{d\log(1/\delta)}}{\eps n})).
    $
    The expected population loss for strongly-convex case is bounded by
    $
        O(\frac{G^2}{\mu}(\frac{1}{N}+\frac{d\log(1/\delta)}{\eps^2n^2})).
    $
    \item The algorithm takes 
    $
        O(\frac{\min\{\eps^2n^2,\eps nd\}}{\log(1/\delta)}\log^2(\frac{nd\eps\}}{\delta}))
    $
    queries of the values of $f_i(\cdot)$ in expectation.
\end{itemize}
\end{theorem}



Note that it is still open how to get a private algorithm with optimal loss using $\widetilde{O}(n)$ many $\nabla f_{i}(x)$. The previous best uses $\widetilde{O}(\min(n^{5/4}d^{1/8},n^{3/2}/d^{1/8}))$ many queries of
$\nabla f_{i}(x)$ \cite{KLL21} or $\widetilde{O}(\min(n^{3/2},n^{2}/\sqrt{d}))$
many $\nabla f_{i}(x)$ \cite{AFKT21}. However, our algorithm uses
less information from the functions, $\widetilde{O}(n^{2})$ bits
instead of $\widetilde{O}(n^{2.375})$ bits (when $n=d$). We are
not sure if there are super linear information lower bound for function
value oracle. We left the problem as an open problem. 

We draw a picture to compare the current results on query complexity and our results for DP-SCO:
\begin{wrapfigure}{h}{.5\textwidth}
    \centering
    \includegraphics[width = .5\textwidth]{figs/running_time.pdf}
    %\hfill
    %\caption{Reductions between ERM and SCO for general convex and strongly convex cases.}
   % As the lower bound of excess population loss is $x(GD(\frac{1}{\sqrt{N}}+\frac{\sqrt{d}}{N\epsilon}))$ while the lower bound of empirical risk is $x(\frac{GD\sqrt{d}}{N\epsilon})$, we do not know how to reduce from ERM to SCO. }
   \caption{Comparison among our results and the recent results in \cite{AFKT21,KLL21} for the non-trivial regime ($d\leq N^2)$. Suppose $\epsilon,\delta$ are small constants. Note that we are querying the functions values $f_i(x)$ while the previous two works query gradient $\nabla f_i(x)$.
   }
    \label{fig:compare_result}
\end{wrapfigure}

In practice, if we can compute gradient oracle, then we can usually compute the function value. But the opposite may not be true (for
example when $f_{i}(x)$ involves human interaction). Therefore, we suspect our weaker oracle requirement may lead to new applications.

%{[}Write down our statement for the Excess Population Loss{]}
%{[}Draw the plot to highlight we are faster{]}

\subsection{Other related work}
There are some related results in \cite{MASN16}, where they also study about the distribution $\exp(-kF(x;S))$ gives $(\eps,\delta)$-differential privacy for the DP-ERM problem. 
Their proof is based on bounding KL-divergence by Logarithmic Sobolev inequality, and the result is not tight with unnecessary restrictions between $\eps$ and $\delta$ as well.
%\Daogao{Seems their bound is only off by some constant to us}
%Specifically, they should set $k=O(\frac{n\mu\eps^2}{G^2\log(1/\delta)})$ in the strongly convex DP-ERM, while the value we require can be as large as $\frac{\eps^2n^2\mu}{8G^2\log(1.25/\delta)}$.


\end{comment}