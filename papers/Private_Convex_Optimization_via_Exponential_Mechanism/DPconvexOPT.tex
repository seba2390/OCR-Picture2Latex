\section{DP Convex Optimization}
In this section we present our results about DP-ERM and DP-SCO.
%We show our results about DP-ERM first, and discuss how to bound the generalization error and get the optimal result about DP-SCO afterwards. 

%See Cor 1 in \url{https://pubsonline.informs.org/doi/pdf/10.1287/moor.2017.0906}
% \begin{proof}
% %\Daogao{Maybe put the proof into the appendix}
% %We adopt the standard ``peeling'' arguments which have been used a lot in previous works, e.g. \cite{KV06,BST14}.


% Let $x^*=\arg\min_{x\in\cK}F(x)$.
% Without loss of generality, let $F(x^*)=0$
% and it suffices to consider a differential cone $\Omega$ on direction $v$ and centered at $x^*$.
% We will bound the expected loss conditioned on $x\in\Omega\cap\cK$.


% Suppose $\E_{\nu}[F(x)\mid x\in \Omega\cap \cK]=a$.
% We want to prove that $a\leq dT$ and hence we can assume that $a>0$.
% % Let $t^*$ be a point such that $F(x^*+tv)=a$.
% % On this cone we define a new function
% % \begin{align*}
% %     \Tilde{F}(x^*+tv)=\frac{t}{t^*}a.
% % \end{align*}

% % Note that $a=\E_{x\sim e^{-F/T}}[F(x)\mid x\in \Omega\cap \cK]\leq \E_{x\sim e^{-\Tilde{F}/T}}[\Tilde{F}(x)\mid x\in \Omega\in \cK]$ due to the convexity of the $F$ and $\cK$.
% % For $0\leq t\leq t^*$, we know that $\Tilde{F}(tv+x^*)\geq F(tv+x^*)$ while $t\geq t^*$, $\Tilde{F}(tv+x^*)\leq F(tv+x^*)$.
% % Compared to the original distribution $\nu$, the distribution proportional to $e^{-\Tilde{F}/T}$ will have more mass in points with $\Tilde{F}(x)\leq a$ and less mass in points with $\Tilde{F}(x)\geq a$.

% % Note that $\Tilde{F}$ is linear and by \cite{KV06}, we have
% % \begin{align*}
% %     \E_{x\sim e^{-\Tilde{F}/T}}[\Tilde{F}(x)\mid x\in \Omega\in \cK]\leq dT.
% % \end{align*}


% Define the set $H(y)=\{x\in \cK\cap \Omega\mid F(x)=y\}$.
% It suffices to consider $\E_{\nu\mid_{ \Omega\cap\cK}}[F(x)]$.
% We have
% \begin{align*}
%     \E_{\nu\mid_{ \Omega\cap\cK}}[F(x)]=&\frac{\int_{0}^{\infty}ye^{-y/T}\volume_{d-1}(H(y))\d y}{\int_{0}^{\infty}e^{-y/T}\volume_{d-1}(H(y))\d y}
% \end{align*}
% \Gopi{Elaborate a little more. By convexity of what? Are you using some theorem like Brunn-Minkowski implicitly here.}
% By the convexity, it is easy to observe that
% $\volume_{d-1}(H(y))\geq (\frac{y}{a})^{n-1}\volume_{d-1}(H(a))$ for $y\leq a$ and $\volume_{d-1}(H(y))\leq (\frac{y}{a})^{n-1}\volume_{d-1}(H(a))$ for $y\geq a$.
% Removing mass in points with $F(x)\leq a$ and adding mass with $F(x)\geq a$ can only increase the expectation.
% Hence we have
% \begin{align*}
%     \E_{\nu\mid_{\Omega\cap\cK}}[F(x)]\leq& \frac{\int_{0}^{\infty}y e^{-y/T}\volume_{d-1}(H(a))(\frac{y}{a})^{d-1}\d y}{\int_{0}^{\infty} e^{-y/T}\volume_{d-1}(H(a))(\frac{y}{a})^{d-1}\d y}
%     = \frac{\int_{0}^{\infty}y^de^{-y/T}\d y}{\int_{0}^{\infty}y^{d-1}e^{-y/T}\d y}
%     =\frac{d!T^{d+1}}{(d-1)!T^d}
%     = dT.
% \end{align*}
% Hence we complete the proof.
% \end{proof}

\subsection{DP-ERM}
In this subsection, we state our result for the DP-ERM problem \eqref{eq:DPERM}.
Briefly speaking, our main result (Theorem~\ref{thm:privacy_technical}) shows that sampling from $\exp(-kF(x;\cD))$ for some appropriately chosen $k$ is $(\eps,\delta)$-DP and achieves the optimal empirical risk in (\ref{eqn:optimal_empirical_risk}).
Our sampling scheme in Section~\ref{sec:sampling} provides an efficient implementation. We start with the following lemma which shows the utility guarantee for the sampling mechanism.
%Here, we give a proof for the convex setting with the tight constant. The proof is similar to \cite{KV06,BST14}.

\begin{lemma}[Utility Guarantee, {\cite[Corollary 1]{DKL18}}]
\label{lm:utility_tech}
Suppose $k>0$ and $F$ is a convex function over the convex set $\cK\subseteq \R^d$. If we sample $x$ according to distribution $\nu$ whose density is proportional to $\exp(-k F(x))$, then we have
\begin{align*}
    \E_{\nu}[F(x)]\leq \min_{x\in\cK}F(x)+\frac{d}{k}.
\end{align*}
\end{lemma}

This is first shown by \cite{KV06} for any linear function $F$, and \cite{BST14} extends it to any convex function $F$ with a slightly worse constant. 

\begin{theorem}[DP-ERM]\label{thm:DPERM}
Let $\epsilon>0$, $\cK\subseteq \R^d$ be a convex set of diameter $D$ and $\{f(\cdot;s)\}_{s\in\mathcal{D}}$ be a family of convex functions over $\cK$ such that $f(x;s)-f(x;s')$ is $G$-Lipschitz for all $s,s'$.
For any data-set $\cD$ and $k>0$, sampling $x^{(priv)}$ with probability proportional to $\exp\left(-k(F(x;\cD)+\mu\|x\|_2^2/2)\right)$ is $(\epsilon,\delta(\eps))$-differentially private, where
\begin{align*}
 \delta(\eps)\leq \deltacurve{\cN(0,1)}{\cN\lp\frac{G\sqrt{k}}{n\sqrt{\mu}},1\rp}(\epsilon).
\end{align*}
The excess empirical risk is bounded by {$\frac{d}{k}+\frac{\mu D^2}{2}$}.
Moreover, if $\{f(\cdot,s)\}_{s\in\mathcal{D}}$ are already $\mu$-strongly convex, then sampling 
$x^{(priv)}$ with probability proportional to $\exp(-kF(x;\cD))$ is $(\eps,\delta(\eps))$-differentially private where 
\begin{align*}
 \delta(\eps)\leq \deltacurve{\cN(0,1)}{\cN\lp\frac{G\sqrt{k}}{n\sqrt{\mu}},1\rp}(\epsilon).
 \end{align*}
 The excess empirical risk is bounded by $\frac{d}{k}$.
\end{theorem}
% \Gopi{Do we have the factor 2 in the privacy bound?}
\begin{proof}
The privacy guarantee follows directly from our main result Theorem~\ref{thm:privacy_technical}, and the bound on excess empirical loss can be proved by Lemma~\ref{lm:utility_tech}.
\end{proof}
%\begin{remark}
%If we use the result about Gaussian mechanism, {\cite[Theorem 3.22]{DR14}}, we can set $k=\frac{\eps^2n^2\mu}{8G^2\log(1.25/\delta)}$ in the strongly convex case with expected excess empirical loss $\frac{8G^2d\log(1.25/\delta)}{\eps^2n^2\mu} $. In the general convex case, we can set $k=\frac{\eps^2n^2\mu}{8G^2\log(1.25/\delta)}$ where $\mu=\frac{4G\sqrt{d\log(1.25/\delta)}}{\eps n D}$ and get expected excess empirical loss $\frac{4GD\sqrt{d\log(1.25/\delta)}}{\eps n}$.
%\end{remark}

%\Gopi{The bound from {\cite[Theorem 3.22]{DR14}} is only stated in the regime $\eps\in (0,1).$ Find an other reference or use the bounds from Section 3.}
%Tat  I think just stating for eps between 0,1 is okay just for statement simplicity

Before we state the implementation results on DP-ERM, we need the following technical lemma:
\begin{lemma}
\label{lm:privacy_curve_bound}
For any constants $1/2>\delta>0$ and $\eps>0$, if $|s|\leq \sqrt{2\log(1/(2\delta))+2\eps}-\sqrt{2\log(1/(2\delta))}$,
one has
\begin{align*}
  \deltacurve{\cN(0,1)}{\cN(s,1)}\leq \delta.  
\end{align*}
\end{lemma}
\begin{proof}
By Equation~(\ref{eqn:Gaussian_privacycurve}), we know that
\begin{align*}
    \deltacurve{\cN(0,1)}{\cN(s,1)}(\eps)\leq \Phi\lp -\frac{\eps}{s}+\frac{s}{2} \rp.
\end{align*}
Without loss of generality, we assume $s\geq0$ and
want to find an appropriate value of $s$ such that $\Phi\lp -\frac{\eps}{s}+\frac{s}{2} \rp\leq \delta$. 
Denote $t\defeq \Phi^{-1}(1-\delta)$ and since $1-\Phi(t)\le \frac{1}{2}\exp(-t^2/2)$ for $t>0$, we know that $t\leq \sqrt{2\log(1/(2\delta))}$.
It is equivalent to solve the equation $\frac{\eps}{s}-\frac{s}{2}\geq t$, which is equivalent to $0\leq s\leq \sqrt{t^2+2\eps}-t$.
Note that $\sqrt{t^2+2\eps}-t$ decreases as $t$ increases, which implies that we can set $s\leq \sqrt{2\log(1/(2\delta))+2\eps}-\sqrt{2\log(1/(2\delta))}$.
\end{proof}

Combining the sampling scheme (Theorem~\ref{thm:sampler}) and our analysis on DP-ERM, we can get the efficient implementation results on DP-ERM directly.

 

\begin{theorem}[DP-ERM Implementation]
\label{thm:DPERM_impl}
With same assumptions in Theorem~\ref{thm:DPERM}, and assume $f(\cdot;s)$ is $G$-Lipschitz over $\cK$ for all $s$.
For any constants $1/10> \delta>0$ and $ \eps> 0$, there is an efficient sampler to solve DP-ERM which has the following guarantees:
\begin{itemize}
    \item The scheme is $(\eps,\delta)$-differentially private;
    \item The expected excess empirical loss is bounded by $\frac{GD\sqrt{d}}{n(\sqrt{\log(1/\delta)+\eps}-\sqrt{\log(1/\delta)})}$.
    In particular, if $\eps< 1/10$, the expected excess empirical loss is bounded by
    $
        \frac{2GD\sqrt{d\log(1/\delta)}}{\eps n}.
    $
    If $\eps \geq \log(1/\delta)$, the expected excess empirical loss is bounded by $ O(\frac{GD\sqrt{d}}{n\sqrt{\eps}})$.
    \item The scheme takes 
    \begin{align*}
        \Theta\lp\frac{\eps^2n^2}{\log(1/\delta)}\log^2(\frac{nd\eps}{\delta})\rp
    \end{align*}
    queries to the values on $f(x;s)$ in expectation and takes the same number of samples from some Gaussian restricted to the convex set $\mathcal{K}$.
\end{itemize}
\end{theorem}

\begin{proof}
% {\cite[Theorem 3.22]{DR14}} shows that $\delta(N(0,1)||N(\Delta,1))(\epsilon)\leq\delta$
% if 
% \[
% \frac{\sqrt{2\ln(1.25/\delta)}\Delta}{\epsilon}\leq1.
% \]

By Lemma~\ref{lm:privacy_curve_bound}, we can set $s=\sqrt{2\log(3/(4\delta))+2\eps}-\sqrt{2\log(3/(4\delta))}$ to make $\deltacurve{\cN(0,1)}{\cN(s,1)}\leq2\delta/3$.
For our setting, Theorem \ref{thm:DPERM} shows that we have $s=\frac{G\sqrt{k}}{n\sqrt{\mu}}$
and hence we can take
\begin{align*}
    k=\frac{2\mu n^{2}\lp\sqrt{\log(3/(4\delta))+\eps}-\sqrt{\log(3/(4\delta))}\rp^2}{G^{2}}.
\end{align*}
Putting it into the excess empirical loss bound of $\frac{d}{k}+\frac{\mu D^{2}}{2}$
and setting $\mu=\frac{G\sqrt{d}}{ n D\lp\sqrt{\log(3/(4\delta))+\eps}-\sqrt{\log(3/(4\delta))}\rp}$,
we get the result on the empirical loss.

Particularly, consider the case when $\eps<1/10$.
We know the excess empirical loss is bounded by $\frac{GD\sqrt{d}}{n(\sqrt{\log(3/(4\delta))+\eps}-\sqrt{\log(3/(4\delta))})}$.
Note that $1+\frac{x}{2}-\frac{x^2}{8}\leq \sqrt{1+x}\leq 1+\frac{x}{2}$ for $x\geq 0$.
Under the assumption that $\delta,\eps\in(0,\frac{1}{10})$, we know $\frac{GD\sqrt{d}}{n(\sqrt{\log(3/(4\delta))+\eps}-\sqrt{\log(3/(4\delta))})}\leq \frac{2GD\sqrt{d\log(4/(5\delta))}}{n\eps}$.
The case when $\eps\geq \log(1/\delta)$ also follows similarly.
% Particularly, if $\eps\leq \log(3/(4\delta))$, we know $0\leq s\leq \frac{\eps}{\sqrt{2\log(1/\delta)}}\leq \sqrt{2\log(3/(4\delta))+2\eps}-\sqrt{2\log(3/(4\delta))}\leq \sqrt{t^2+2\eps}-t$.
% Hence we can set $k$.
% \gnote{Can remove the text from here}
% Particularly, consider the case when $\eps<1/2$.
% By Equation~\eqref{eqn:Gaussian_privacycurve_approx}, we know that 
% \begin{align*}
%     \deltacurve{\cN(0,1)}{\cN(s,1)}(\eps)  \le \frac{1}{2} \exp\lp-\frac{1}{2}\lp\frac{\eps}{s}-\frac{s}{2}\rp^2\rp  \text{ if } \eps \geq \frac{s^2}{2}.
% \end{align*}
% Hence we know $\deltacurve{\cN(0,1)}{\cN(s,1)}(\eps)\leq 4\delta/5$ if
% $
%     \frac{s\sqrt{2\log(3/(4\delta))}}{\eps}\leq 1
% $
% under the assumptions that $\eps< 1/2$ and $\delta< 1/2$.
% For our setting, Theorem \ref{thm:DPERM} shows that we have $s=\frac{2G\sqrt{k}}{n\sqrt{\mu}}$
% and hence we can take
% \[
% k=\frac{\epsilon^{2}n^{2}\mu}{8G^{2}\log(3/(4\delta))}.
% \]
% Putting it into the excess empirical loss bound of $\frac{d}{k}+\frac{\mu D^{2}}{2}$
% and setting $\mu=\frac{4G\sqrt{d\log(3/(4\delta))}}{\epsilon n D}$,
% we get the result on the empirical loss.
% The case when $\eps\geq \log(1/\delta)$ follows directly by the calculation.
% \gnote{to here. Instead just use upper bounds on the general bound just derived to get the special cases.}

To make it algorithmic, we apply Theorem~\ref{thm:sampler} with the accuracy on the total variation distance to be $\min\{\delta/3,\frac{1}{cn^c\eps}\}$ for some large enough constant $c$. This leads to $(\epsilon,\delta)$-DP and an extra empirical loss and hence we use $\log(1/\delta)$ rather than $\log(3/(4\delta))$ or $\log(4/(5\delta))$ in the final loss term. 

%Note that $1+\frac{x}{2}-\frac{x^2}{8}\leq \sqrt{1+x}\leq 1+\frac{x}{2}$ for $x\geq 0$. \gnote{Why is this fact mentioned here?}
The running time follows from Theorem~\ref{thm:sampler}.
\end{proof}


\subsection{DP-SCO and Generalization Error}
As mentioned before, one can reduce the DP-SCO \eqref{eq:DPSCO} to DP-ERM \eqref{eq:DPERM} by the iterative localization technique proposed by \cite{FKT20}.
But this method forces us to design different algorithms for DP-ERM and DP-SCO, and may lead to a large constant in the final loss.
In this section, we show that the exponential mechanism can achieve both the optimal empirical risk for DP-ERM and the optimal population loss for DP-SCO by simply changing the parameters.
The bound on the generalization error works beyond differential privacy and can be useful for other (non-private) optimization settings.


% We define the log-Sobolev inequality first.
% \begin{definition}[Log-Sobolev Inequality]
% We say a distribution $\nu$ satisfies a logarithmic Sobolev inequality with a constant $C$ if for all smooth
% function $g:\R^n\rightarrow \R$ with $\E_{\nu}[g^2]<\infty$, one has
% \begin{equation}
%     \label{eq:log-sobolev}
%      \Ent_\nu[g^2] \leq 2C \mathbb{E}_{\nu}\left[\|\nabla g\|_2^{2}\right]
% \end{equation}
% where $\Ent_\nu[f]=\mathbb{E}_{\nu}\left[f \log \lp \frac{f}{\E_\nu f}\rp\right]$.
% \end{definition}

% We have the following useful lemma for Log-Sobolev Inequality (LSI):
% \begin{lemma}[\cite{BE85}]
% \label{lm:strongly_convexity_LSI}
% If $\nu$ is $\mu$-strongly log-concave, then $\nu$ satisfies LSI with constant $C=1/\mu$.
%If $\nu$ is obtained by restricting a standard Gaussian distribution with variance $\sigma^2$ to some subset $\cK\subset \R^d$, then $\nu$ satisfies the log-Sobolev inequality (\ref{eqn:log-sobolev}) with $C=\sigma^2.$
% \end{lemma}

% The Talagrand transportation inequality is closely related to LSI. 
The proof will make use of one famous inequality: \emph{Talagrand transportation inequality}.
Recall for two probability distributions $\nu_1,\nu_2$, the Wasserstein distance is equivalently defined as $$W_2(\nu_1,\nu_2)=\inf_\Gamma \lp\E_{(x_1,x_2)\sim \Gamma} \norm{x_1-x_2}_2^2\rp^{1/2},$$ where the infimum is over all couplings $\Gamma$ of $\nu_1,\nu_2.$
\begin{theorem}[Talagrand transportation inequality]{\cite[Theorem 1]{OV00}}
\label{thm:TTI}
Let $\d\pi\propto e^{-F(x)}\d x$ be a $\mu$-strongly log-concave probability measure on $\cK\subseteq\R^d$ with finite moments of order 2. 
For all probability measure $\nu$ absolutely continuous w.r.t. $\pi$ and with finite moments of order 2,
we have
\begin{align*}
\mathrm{W}_2(\nu,\pi)\leq\sqrt{\frac{2}{\mu}\mathrm{D}_{KL}(\nu,\pi)}.   
\end{align*}
\end{theorem}

% To bound the KL divergence of the objective distributions, we have the following lemma.


% \begin{lemma}
% \label{lm:LSI_KL}
% Suppose $F,\TF$ are two $\mu$-strongly convex functions over $\cK\subseteq \R^d$, and $F-\TF$ is $G$-Lipschitz over $\cK$.
% For any $k>0$, if we let $\pi\propto e^{-kF}$ and $\nu\propto e^{-k\TF}$ be two probability distributions on $\cK$, then we have
% \begin{align*}
%     \mathrm{D}_{\alpha}(\pi,\nu)\leq \mathrm{D}_{\alpha}\lp\cN(0,1),\cN(\frac{G\sqrt{k}}{\sqrt{\mu}},1)\rp,\forall \alpha\in[0,\infty].
% \end{align*}
% \end{lemma}

% \begin{proof}
% Recall in Theorem~\ref{thm:privacy_technical_tradeoff}, we proved for all $z\in[0,1]$, one has
% \begin{align*}
%     \tradeoff{\pi}{\nu}(z)\ge \tradeoff{\cN\lp 0,1\rp}{\cN\lp\frac{G\sqrt{k}}{\sqrt{\mu}},1\rp}(z).
% \end{align*}

% By Theorem 2.10 in \cite{dong2019gaussian}, there exists a randomized algorithm $\Proc$ such that $\Proc\lp\cN\lp 0,1\rp\rp=\pi$ and $\Proc\lp\cN\lp\frac{G\sqrt{k}}{\sqrt{\mu}},1\rp\rp=\nu$.
% Due to the data processing inequality (See \cite{EH14}, for example), the statement holds.
% \end{proof}

% \begin{proof}
% Let $g\defeq \frac{\d \pi}{\d \nu}$ and one has
% \begin{align*}
%     \mathrm{D}_{KL}(\pi,\nu)=&\E_{\pi}[\log g]\\
%     =& \E_{\nu}[g \log g] \\
%     \leq & \E_{\nu}[g \log g]-\E_{\nu}[g]\log\E_{\nu}[ g],
% \end{align*}
% where the last inequality follows by that $\E_{\nu}[g]=1$ and $ \log\E_{\nu}[g]= 0$.
% Note that both $F$ and $\TF$ are $\mu$-strongly convex. 
% By Lemma~\ref{lm:strongly_convexity_LSI} and log-Sobolev inequality (\ref{eq:log-sobolev}), we have
% % \Gopi{Don't we have  $$\E_{\nu}[g \log g]-\E_{\nu}[g]\log\E_{\nu}[g]
%     % \leq \frac{2}{k\mu}\E_{\nu}[\|\nabla \sqrt{g}\|_2^2]$$ by LSI?}
% \begin{align*}
%     &~\E_{\nu}[g \log g]-\E_{\nu}[g]\log\E_{\nu}[ g]\\
%     \leq&~ \frac{2}{k\mu}\E_{\nu}[\|\nabla \sqrt{g}\|_2^2]\\
%     = & ~\frac{2}{k\mu}\E_{\nu}[\|\frac{\sqrt{g}}{2}\nabla \log g\|_2^2]\\
%     \leq & ~\frac{1}{2k\mu}(Gk)^2\E_{\nu}[g]\\
%     =& ~\frac{G^2k}{2\mu},
% \end{align*}
% where the fourth line follows by $\|\nabla \log g\|_2^2\leq (Gk)^2$ due to the Lipschitz assumption.
% Hence we complete the proof.
% \end{proof}

To prove our main result on bounding the generalization error of sampling mechanism, we need the following lemma.


\begin{lemma}[Lemma 7 in \cite{BE02}]
\label{lm:generalization_error_erm}
For any learning algorithm $\cA$ and dataset $\cD=\{s_1,\cdots,s_n\}$ drawn i.i.d from the underlying distribution $\cP$, let $\cD'$ be a neighboring dataset formed by replacing a random element of $\cD$ with a freshly sampled $s'\sim \cP$. If $\cA(\cD)$ is the output of $\cA$ with $\cD$, then
\begin{align*}
    \E_{\cD}[\HF(\cA(\cD))-F(\cA(\cD);\cD)]=\E_{\cD,s'\sim\cP,\cA}\Big[f(\cA(\cD);s')-f(\cA(\cD');s') \Big].
\end{align*}
\end{lemma}

Now we begin to state and prove our main result on the generalization error.
\begin{theorem}
\label{thm:generalization_error}
Suppose $\{f(\cdot,s)\}$ is a family $\mu$-strongly convex functions over $\cK$ such that $f(x;s)-f(x;s')$ is $G$-Lipschitz for all $s,s'$.
For any $k>0$ and dataset $\cD=\{s_1,s_2,\cdots,s_n\}$ drawn i.i.d from the underlying distribution $\cP$, let $\cD'$ be a neighboring dataset formed by replacing a random element of $\cD$ with a freshly sampled $s'\sim \cP$,
\begin{align*}
    \mathrm{W}_{2}(\pi_\cD,\pi_{\cD'})\leq \frac{G}{n\mu}.
\end{align*}
If we sample our solution from density $\pi_\cD(x) \propto e^{-kF(x;\cD)}$, we can bound the excess population loss as:
\begin{align*}
    \E_{\cD,x\sim \pi_\cD}[\HF(x)]-\min_{x\in\cK}\HF(x)\leq \frac{G^2}{\mu n}+ \frac{d}{k}.
\end{align*}
\end{theorem}

\begin{proof}
Recall that
\begin{align*}
    F(x;\cD)=\frac{1}{n}\sum_{s_i\in\cD}f(x;s_i).
\end{align*}
We form a neighboring data set $\cD'$ by replacing a random element of $\cD$ by a freshly sampled $s'\sim \cP.$
Let $\pi_\cD\propto e^{-kF(x;\cD)}$ and $\pi_{\cD'}\propto e^{-kF(x;\cD')}$. By Corollary~\ref{cor:divergence_privacy}, we have
\begin{align*}
    \mathrm{D}_{KL}(\pi_\cD,\pi_{\cD'})\leq \frac{G^2k}{2n^2\mu}.
\end{align*}
By the assumptions, we know both $F(x;\cD)$ and $F(x;\cD')$ are $\mu$-strongly convex and by Theorem~\ref{thm:TTI}, we have
\begin{align*}
    \mathrm{W}_2(\pi_\cD,\pi_{\cD'})\leq \sqrt{\frac{2}{k\mu}\mathrm{D}_{KL}(\pi_\cD,\pi_{\cD'})} \leq \frac{G}{n\mu}.
\end{align*}
By Lemma~\ref{lm:generalization_error_erm} and properties of Wasserstein distance, we have
\begin{align*}
    \E_{\cD,x\sim \pi_{\cD}}[\HF(x)-F(x;\cD)]=&~\E_{\cD,s'\sim \cP}\lb\E_{x\sim \pi_\cD}f(x;s')-\E_{x'\sim \pi_{\cD'}}f(x';s')\rb\\
    =&~\E_{\cD,s'\sim \cP}\lb\E_{x\sim \pi_\cD}\lb f(x;s')-f(x;s'')\rb -\E_{x'\sim \pi_{\cD'}}\lb f(x';s')-f(x';s'')\rb \rb \tag{where $s''$ is chosen arbitrarily, note that $\E_{\cD,x\sim \pi_\cD}[f(x;s'')]=\E_{\cD',x'\sim \pi_{\cD'}}[f(x';s'')]$}\\
    \leq &~G\cdot \mathrm{W}_{2}(\pi_\cD,\pi_{\cD'}) \tag{$f(x;s')-f(x;s'')$ is $G$-Lipschitz}\\
    \leq &~ \frac{G^2}{n\mu}.
\end{align*}
Hence, we know that
\begin{align*}
    \E_{\cD,x\sim\pi_{\cD}}[\hat{F}(x)]-\min_{x\in\cK}\HF(x)\leq &~ \E_{\cD,x\sim\pi_{\cD}}[\hat{F}(x)]-\E_{\cD}[\min_{x\in\cK}F(x;\cD)]\\
    \leq &~ \E_{\cD,x\sim\pi_{\cD}}[\hat{F}(x)-F(x;\cD)]+\E_{\cD,x\sim\pi_{\cD}}[F(x;\cD)-\min_{x\in\cK}F(x;\cD)]\\
    \leq &~ \frac{G^2}{n\mu}+\E_{\cD,x\sim\pi_{\cD}}[F(x;\cD)-\min_{x\in\cK}F(x;\cD)]\\
    \leq &~ \frac{G^2}{n\mu}+\frac{d}{k},
\end{align*}
where the last inequality follows from Lemma~\ref{lm:utility_tech}.
\end{proof}

With the bounds on generalization error, we can get our first result on DP-SCO.
\begin{theorem}[DP-SCO]
\label{thm:dpsco}
Let $\epsilon>0$, $\cK\subseteq \R^d$ be a convex set of diameter $D$ and $\{f(\cdot;s)\}_{s\in\mathcal{D}}$ be a family of convex functions over $\cK$ such that $f(x;s)-f(x;s')$ is $G$-Lipschitz for all $s,s'$.
For any data-set $\cD$ and $k>0$, sampling $x^{(priv)}$ with probability proportional to $\exp\left(-k(F(x;\cD)+\mu\|x\|_2^2/2)\right)$ is $(\epsilon,\delta(\eps))$-differentially private, where
\begin{align*}
 \delta(\eps)\leq \deltacurve{\cN(0,1)}{\cN\lp\frac{G\sqrt{k}}{n\sqrt{\mu}},1\rp}(\epsilon).
\end{align*}
If users in the data-set $\cD$ are drawn i.i.d. from the underlying distribution $\cP$, the excess population loss is bounded by $\frac{G}{n\mu}+\frac{d}{k}+\frac{\mu D^2}{2}$. 
Moreover, if $\{f(\cdot;s)\}_{s\in\mathcal{D}}$ are already $\mu$-strongly convex, then sampling 
$x^{(priv)}$ with probability proportional to $\exp(-kF(x;\cD))$ is $(\eps,\delta(\eps))$-differentially private where 
\begin{align*}
 \delta(\eps)\leq \deltacurve{\cN(0,1)}{\cN\lp\frac{G\sqrt{k}}{n\sqrt{\mu}},1\rp}(\epsilon).
 \end{align*}
 The excess population loss is bounded by $\frac{G}{n\mu}+\frac{d}{k}$.
% \Gopi{Add what happens if $f(\cdot;s)$ are already $\mu$-strongly convex.}
\end{theorem}
% Combining our result on DP-ERM (Theorem~\ref{thm:DPERM_impl}) and the bound on generalization error (Theorem~\ref{thm:generalization_error}), we can get our result about DP-SCO.

\begin{proof}
The first part about privacy is a restatement of our result on DP-ERM (Theorem~\ref{thm:DPERM_impl}).
The excess population loss (See Equation~\eqref{eqn:population_loss}) follows from the bound on generalization error (Theorem~\ref{thm:generalization_error}) and utility guarantee (Lemma~\ref{lm:utility_tech}).
\end{proof}
%%%%%%%%%%%
%%%%%%%%%%%%
\begin{comment}

===================\textcolor{red}{OLD TEXT BEGIN}===========================================
\begin{lemma}[Lemma 7 in \cite{BE02}]
\label{lm:generalization_error_erm}
For any learning algorithm $\cA$ and neighboring data-sets $\cD=\{s_1,\cdots,s_n\}$, $\cD^i=\{s_1,\cdots,s_{i-1},s_i',s_{i+1},\cdots,s_n\}$ drawn i.i.d from the underlying distribution $\cP$, let $\cA(\cD)$ be the output of $\cA$ with $\cD$. 
One has
\begin{align*}
    \E_{\cD}[\HF(\cA(\cD))-F(\cA(\cD);\cD)]=\E_{\cD,s_i'\sim\cP}\Big[\E[f(\cA(\cD);s_i')-f(\cA(\cD^{i});s_i')] \Big].
\end{align*}
\end{lemma}

Now we begin to state and prove our main result on the generalization error.
\begin{theorem}
\label{thm:generalization_error}
Suppose $\{f(\cdot,s)\}$ is a family of $G$-Lipschitz and $\mu$-strongly convex functions over $\cK$.
For any $k>0$ and suppose the $n$ samples $\cD=\{s_1,s_2,\cdots,s_n\}$ are drawn i.i.d from the underlying distribution $\cP$, then if we draw a sample $x^{(sol)}$ from density $\propto e^{-kF(x;\cD)}$, we have
\begin{align*}
    \E[\HF(x^{(sol)})]-\min_{x\in\cK}\HF(x)\leq \frac{2G^2}{\mu n}+ \frac{d}{k}.
\end{align*}
\end{theorem}

\begin{proof}
Recall that $\cD=\{s_1,\cdots,s_n\}$.
\begin{align*}
    F(x;\cD)=\frac{1}{n}\sum_{s_i\in\cD}f(x;s_i).
\end{align*}
We consider any neighboring data set $\cD^i=\{s_1,\cdots,s_{i-1},s_i',s_{i+1},\cdots,s_n\}$.

Let $\pi\propto e^{-kF(x;\cD)}$ and $\hat{\pi}\propto \exp^{-kF(x;\cD^i)}$.
By the assumptions, we know both $F(x;\cD)$ and $F(x;\cD^i)$ are $\mu$-strongly convex and by Theorem~\ref{thm:TTI}, we have
\begin{align*}
    \mathrm{W}_2(\pi,\hpi)\leq \sqrt{\frac{2}{k\mu}\mathrm{D}_{KL}(\pi,\hpi)}.
\end{align*}

As $kF(x;\cD)-kF(x;\cD^i)$ is $2Gk/n$ Lipschitz,
%\Yintat{Is this a 2Gk/n?}, 
by Lemma~\ref{lm:LSI_KL} we have
\begin{align*}
    \mathrm{D}_{KL}(\pi,\hpi)\leq \frac{2G^2k}{n^2\mu}.
\end{align*}
Thus we know that 
\begin{align*}
    \mathrm{W}_{2}(\pi,\hpi)\leq \frac{2G}{n\mu}.
\end{align*}

By Lemma~\ref{lm:generalization_error_erm} , the assumption on Lipschitz continuity and definition of Wasserstein distance, we have
\begin{align*}
    \E_{\cD,x\sim \pi}[\HF(x)-F(x;\cD)]=&~\E_{\cD,s_i'}[\E_{x\sim \pi}f(x;s_i')-\E_{x'\sim \hpi}f(x';s_i')]\\
    =&~\E_{\cD,s_i',s''\sim \cP}[\E_{x\sim \pi}\lp f(x;s_i')-f(x;s'')\rp -\E_{x'\sim \hpi}\lp f(x';s_i')-f(x';s'')\rp]\\
    \leq &~G\cdot \mathrm{W}_{2}(\pi,\hpi) \tag{$f(x;s_i')-f(x;s'')$ is $G$-Lipschitz}\\
    \leq &~ \frac{2G^2}{n\mu}.
\end{align*}
Hence, we know that
\begin{align*}
    \E_{\cD,x\sim\pi}[\hat{F}(x)]-\min_{x\in\cK}\HF(x)\leq &~ \E_{\cD,x\sim\pi}[\hat{F}(x)]-\E_{\cD}[\min_{x\in\cK}F(x;\cD)]\\
    \leq &~ \E_{\cD,x\sim\pi}[\hat{F}(x)-F(x;\cD)]+\E_{\cD,x\sim\pi}[F(x;\cD)-\min_{x\in\cK}F(x;\cD)]\\
    \leq &~ \frac{2G^2}{n\mu}+\E_{\cD,x\sim\pi}[F(x;\cD)-\min_{x\in\cK}F(x;\cD)]\\
    \leq &~ \frac{2G^2}{n\mu}+\frac{d}{k},
\end{align*}
where the last inequality follows from Lemma~\ref{lm:utility_tech}.
\end{proof}
======================= \textcolor{red}{OLD TEXT END} ===================================================
\end{comment}



% As suggested in \cite{FKT20,KLL21,AFKT21}, we can reduce the DP-SCO to DP-ERM by the localization technique. 
% As mentioned before, 
% Based on the technique, \cite{KLL21} gave a framework (Algorithm~\ref{alg:local}) below:
% \begin{algorithm2e}
% \caption{Iterative Localized Algorithm Framework $\cA'$}
% \label{alg:local}
% {\bf Input:} A family of $G$-Lipschitz and $\mu$-strongly convex function $f:\cK\times\Xi\rightarrow \R$, initial point $x_0\in \K$ and privacy parameter $\epsilon,\delta$.

% {\bf Process:}
% Set $k=\lceil \log n \rceil$\;
% \For{$i=1,\cdots,k$}
% {
% Set $\epsilon_i=\epsilon/2^i,n_i=n/2^{i},\eta_i=\eta/2^{5i},\delta_i=\delta/2^i$\;

% Apply $(\epsilon_i,\delta_i)$-DP ERM algorithm $\cA_{\epsilon_i,\delta_i}$ over $\cK_{i}=\left\{x \in \cK:\left\|x-x_{i-1}\right\|_{2} \leq 2G\eta_in_i\right\}$ \label{ln:sub-procedure_DP_ERM}\\ with the function
% $\HF_i(x)=\frac{1}{n_i}\sum_{j\in S_i}f(x,x_j)+\frac{1}{\eta_in_i}\|x-x_{i-1}\|^2$ where $S_i$ consists of $n_i$ samples drawn i.i.d. from $\mathcal{P}$\;

% Let $x_i$ be the output of the ERM algorithm\;

% }
% {\bf Return:} The final iterate $x_k$\;
% \end{algorithm2e}

% Adopting our scheme for DP-ERM in the Line~\ref{ln:sub-procedure_DP_ERM} in Algorithm~\ref{alg:local} and use the Theorem~5.1 in \cite{KLL21}, we have the following results directly:
We give an implementation result of our DP-SCO result.
\begin{theorem}[DP-SCO Implementation]
\label{thm:dpsco_impl}
With same assumptions in Theorem~\ref{thm:dpsco}, and assume $f(\cdot;s)$ is $G$-Lipschitz over $\cK$ for all $s$.
For $0<\delta<\frac{1}{10}$ and $0 < \eps<\frac{1}{10}$, there is an efficient algorithm to solve DP-SCO which has the following guarantees:
\begin{itemize}
    \item The algorithm is $(\eps,\delta)$-differentially private;
    \item The expected population loss is bounded by
    \begin{align*}
        GD\lp\frac{2\sqrt{\log(1/\delta)d}}{\eps n}+\frac{2}{\sqrt{n}}\rp,
    \end{align*}
    where $c>0$ is an arbitrary constant to be chosen. 
    %The expected population loss for strongly-convex case is bounded by
    %\begin{align*}
    %    O(\frac{G^2}{\mu}(\frac{1}{n}+\frac{d\log(1/\delta)}{\eps^2n^2})).
    %\end{align*}
    \item The algorithm takes 
    \begin{align*}
        O\lp\min\lc\frac{\eps^2n^2}{\log(1/\delta)},nd\rc\log^2\lp\frac{\eps nd}{\delta}\rp\rp
    \end{align*}
    queries of the values of $f(\cdot,s_i)$ in expectation and takes the same number of samples from some Gaussian restricted to the convex set $\mathcal{K}$.
\end{itemize}
\end{theorem}
\begin{remark}
As for the non-typical case when $\eps\geq 1/10$, one can use the bound in Theorem~\ref{thm:DPERM_impl} and the bound on generalization error (Theorem~\ref{thm:generalization_error}) .
Particularly, one can achieve expected population loss $O\lp GD\lp\frac{\sqrt{d}/n}{\sqrt{\log(1/\delta)+\eps}-\sqrt{\log(1/\delta)}}+\frac{1}{\sqrt{n}}\rp\rp$.
\end{remark}

\begin{proof}
By Theorem~\ref{thm:dpsco}, sampling from $\exp(-k(F(x;\cD)+\mu \|x\|_2^2/2))$ when $k\leq \frac{\eps^2n^2\mu}{2G^2\log(3/(4\delta))}$ is $(\eps,2\delta/3)$-DP.
Besides, 
we can set  $k=\frac{\mu}{G^2}\min\{\frac{\epsilon^{2}n^{2}}{2\log(3/(4\delta))},2nd\}$ for arbitrarily large constant $c>0$ to make the mechanism $(\eps,2\delta/3)$-differentially private, achieving tight population loss and decrease the running time.
Then the population loss is upper bounded by
\begin{align*}
    \frac{d}{k}+\frac{\mu D^2}{2}+\frac{G^2}{\mu n}=& \frac{G^2}{\mu}\max\lc\frac{2\log(3/(4\delta))d}{\eps^2n^2},\frac{1}{2n}\rc+\frac{\mu D^2}{2}+\frac{G^2}{\mu n}.
\end{align*}
% where the generalization error term $\frac{2G^2}{\mu n}$ follows from bounding Wasserstein distance by Theorem~\ref{thm:generalization_error} and making use of Lemma~\ref{lm:generalization_error_erm} the assumption that $f(;s')$ is $G$-Lipschitz for any $s'$.

By setting $\mu=\frac{G}{D}\sqrt{2(\frac{2\log(3/(4\delta))d}{\eps^2 n^2}+\frac{1}{2n})}$, the population loss is upper bounded by
\begin{align*}
    GD\sqrt{\frac{4\log(3/(4\delta))d}{\eps^2 n^2}+\frac{1}{n}}+GD\sqrt{\frac{1}{n}}\leq GD\lp\frac{2\sqrt{\log(3/(4\delta))d}}{\eps n}+\frac{2}{\sqrt{n}}\rp.
\end{align*}

To make it algorithmic, we also apply Theorem~\ref{thm:sampler} with the accuracy on the total variation distance to be $\min\{\delta/3,\frac{1}{cn^c}\}$ for some large enough constant $c$. This leads to an extra empirical loss and hence we use $\log(1/\delta)$ rather than $\log(3/(4\delta))$ in the final loss term. The runtime follows from Theorem~\ref{thm:sampler}.
\end{proof}

\begin{comment}
\begin{remark}
An alternative way to achieve $O(nd)$ first order oracle (query sub-gradient) complexity is to run proximal point algorithm on non-smooth functions, which can be shown with similar contractility as running stochastic gradient descent on smooth functions. Thus replacing the SGD in \cite{FKT20} by proximal point algorithm, we can achieve $\Tilde{O}(nd)$ gradient queries for DP-SCO. But this method takes gradient queries while we query values, and it requires more memory space $\Omega(d^2)$ compared to our algorithm.
\end{remark}

\end{comment}
