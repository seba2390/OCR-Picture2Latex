\section{Information-theoretic Lower Bound for DP-SCO}
\label{sec:infolower}

 In this section, we prove an information-theoretic lower bound for the query complexity required for DP-SCO (with value queries), which matches (up to some logarithmic terms) the query complexity achieved by our algorithm (in Theorem \ref{thm:dpsco_impl}).
Our proof is similar to the previous works like \cite{ACCD12,DJWW15} with some modifications.


% In fact, the query complexity of our (non-private) sampling scheme and algorithm for solving DP-SCO matches the  information-theoretic minimax lower bounds (up to some logarithmic terms).
% Our proof is similar to the previous works like \cite{ACCD12,DJWW15} with some modifications.
% For completeness, we present the proof here.
%We will demonstrate the result of DP-SCO first.

Before stating the lower bound, we define some notations.
Recall that we are given a set $\cD$ of $n$ samples (users) $\{s_1,\cdots,s_n\}$.
Let $\mathbb{A}_k$ be the collection of all algorithms that observe a sequence of $k$ data points $(Y^1,\cdots,Y^k)$ with $Y^t=f(X^t;S^t)$ where %$s^t$ is an independent sample randomly drawn from the underlying distribution $\mathcal{P}$, 
$S^t\in \cD$ and $X^t\in \cK$ are chosen arbitrarily and adaptively by the algorithm (and possibly using some randomness).

For the lower bound, we only consider linear functions, that is we define $f(x;s)\defeq\langle x,s\rangle$. And let $\cP_G$ be the collection of all distributions such that if $\cP\in\cP_G$, then $\E_{s\sim\cP}\|s\|_2^2\leq G^2$.

% \begin{align*}
%     \cF_G\defeq\{(f,\mathcal{P}):f(x;s)=\langle x,s\rangle,\E_{s\sim \cP}[\|s\|^2_2]\leq G^2\}
% \end{align*}
% be a class of linear functionals.
And we define the optimality gap
\begin{align*}
    \eps_k(\cA,\cP,\cK)\defeq&~ \E_{\cD\sim\cP^n,\cA}[\HF(\hat{x}(\cD))]-\inf_{x\in \cK}\hat{F}(x),
\end{align*}
where $\HF(x) = \E_{s\sim\cP} f(x; s)$, $\hat{x}$ is the output the algorithm $\cA$ given the input dataset $\cD$ and the expectation is over the dataset $\cD\sim \cP^n$ and the randomness of the algorithm $\cA.$ Note that we can rewrite the optimality gap as:
\begin{align*}
    \eps_k(\cA,\cP,\cK)=&~ \E_{\cD\sim\cP^n,\cA}[\HF(\hat{x}(\cD))]-\inf_{x\in \cK}\hat{F}(x)\\
    =& \E_{s\sim\cP}\Big[\E_{\cD\sim\cP^n,\cA}f(\hat{x}(\cD);s)]\Big]-\inf_{x\in\cK}\E_{s\sim\cP}[f(x;s)]\\
    =& \E_{s\sim\cP,\cD\sim\cP^n,\cA}[\hat{x}(\cD)^\top s]-\inf_{x\in\cK}\E_{s\sim \cP}[x^\top s].
\end{align*}
The minimax error is defined by
\begin{align*}
    \eps_k^*(\cP_G,\cK)\defeq \inf_{\cA\in \mathbb{A}_k}\sup_{\mathcal{P}\in \cP_G}\eps_k(\cA,\cP,\cK).
\end{align*}
%where the expectation is taken over the observations $(Y^1,\cdots,Y^k)$ and any additional randomness in $\cA$.

\begin{theorem}
%\Yintat{Make this statement easier to understand. Maybe add something like: In particular, there is no algorithm that use something something and get error something something}
\label{thm:info_bound}
Let $\cK$ be the $\ell_2$ ball of diameter $D$ in $\R^d$, then
\begin{align*}
    \eps_k^*(\cP_G,\cK)\geq \frac{GD}{16}\min\left\{1,\sqrt{\frac{d}{4k}}\right\}.
\end{align*}
In particular, for any (randomized) algorithm $\cA$ which can observe a sequence of data points $(Y^1,\cdots,Y^k)$ with $Y^t=f(X^t;S^t)$ where $S^t\in\cD=\{s_1,s_2,\dots,s_n\}$ and $X^t\in\cK$ are chosen arbitrarily and adaptively by $\cA$,
there exists a distribution $\cP$ over convex functions such that $\E_{s\sim \mathcal{P}}[\|\nabla f(x,s)\|_2^2]\leq G^2$ for all $x\in \cK$, such that the output $\hx$ of the algorithm satisfies
\begin{align*}
    \E_{s\sim\cP}\Big[ \E_{\cD\sim\cP^n,\cA}f(\hx;s)]\Big]-\min_{x\in\cK}\E_{s\sim\cP}[f(x;s)]\geq \frac{GD}{16}\min\left\{1,\sqrt{\frac{d}{4k}}\right\}.
\end{align*}
\end{theorem}


% There is one difference between the condition in Theorem~\ref{thm:info_bound_technical} and our desired lower bound.
% We are under the assumption that the convex functions are $G$-Lipschitz, i.e.,  $ \|\nabla_x f(x;s)\|\leq G$ for all $x,s$, while the lower bound in Theorem~\ref{thm:info_bound_technical} gives a distribution $\cP$ over convex functions such that $\E_{s\sim \cP}[\|\nabla_x f(x;s) \|_2^2]\leq G^2$ which is a weaker condition. In the lower bound the functions $f(x;s)=\inpro{x}{s}$ are linear, and so $\nabla_x f(x;s)=s.$ Since the distribution $\cP$  
% To deal with this difference, we can lose a logarithmic term $\log(dn)$ to get a new information lower bound, i.e., we can truncate the Gaussian distribution used in the proof and conditional on the event that $\|\nabla f(x;s) \|_2^2\leq  O(G^2 \log (nd))$ for each $s$ in the dataset, which occurs with probability at least $1-\frac{1}{\mathrm{poly}(nd)}$ and the truncation does not influence the remaining proofs.
% \gnote{Can we remove the above paragraph?}
% we can assume the algorithm can recover the underlying $v$ exactly which leads to a zero error.
%  Note that the probability of a bad event that at least one $s_i\in\cD$ sa can be made polynomially small, and 
% Thus it serves a new bound for our case.

%At the beginning, let me introduce the lower bound in \cite{DJWW15} at first.Their main statement is about getting two values each query, and can be extended to query $d$ values.


%Without loss of generality, we consider the case when $d\leq n$ first.
% \begin{theorem}
% \label{thm:info_bound}
% For any (randomized) algorithm $\cA$ which can observe a sequence of $k$ data points $(Y^1,\cdots,Y^k)$ with $Y^t=f(X^t;S^t)$ where $S^t\in\cD=\{s_1,s_2,\dots,s_n\}$ and $X^t\in\cK$ are chosen arbitrarily and adaptively by $\cA$,
% there exists a distribution $\cP$ over convex functions such that $\E_{s\sim \mathcal{P}}[\|\nabla f(x,s)\|_2^2]\leq G^2$ for all $x\in \cK$, such that the output $\hx$ of the algorithm satisfies
% \begin{align*}
%     \E_{s\sim\cP}\Big[ \E_{\cD\sim\cP^n,\cA}f(\hx;s)]\Big]-\min_{x\in\cK}\E_{s\sim\cP}[f(x;s)]\geq \frac{1}{40}\frac{GD}{\sqrt{k}}\min\{\sqrt{d},\sqrt{k}\}.
% \end{align*}
% \end{theorem}

\subsection{Proof of Theorem \ref{thm:info_bound}}
We reduce the optimization problem into a series of binary hypothesis tests.
Recall we are considering linear functions $f(x;s)\defeq\langle x,s\rangle$.
Let $\cV=\{-1,1\}^d$ be a Boolean hyper-cube and for each $v\in \cV$, let $\cN_{v}=\cN(\delta v,\sigma^2I_{d})$ be a Gaussian distribution for some parameters to be chosen such that $\HF_v(x)\defeq \E_{s\sim\cN_v}[f(x;s)]=\delta\langle x,v\rangle$. Note that $$\E_{s\sim \cN_v}[\|\nabla f(x,s)\|_2^2]=\E_{s\sim \cN_v}[\norm{s}_2^2]=(\delta^2+\sigma^2)d.$$ Therefore $G=\sqrt{d(\delta^2+\sigma^2)}.$
%Then by taking $nd$ queries of function like $f(e_i;s_j)$, we can determine the exact values of the $n$ samples $\{s_1,\cdots,s_n\}$.

Clearly the lower bound should scale linearly with $D$. Therefore without loss of generality, we can assume that the diameter $D=2$ and define $\cK=\{x\in\R^d:\|x\|_2\leq 1\}$ to be the unit ball.
As in \cite{ACCD12}, we suppose that $v$ is uniformly sampled from $\cV=\{-1,1\}^d$.
Note that if we can find a good solution to $\HF_v(x)$, we need to determine the signs of vector $v$ well. Particularly, we have the following claim:
\begin{claim}[\cite{DJWW15}]
\label{clm:error_determine_sign}
For each $v\in \cV$, let $x^v$ minimize $\HF_v$ over $\cK$ and obviously we know that $x^v=-v/\sqrt{d}$. 
For any solution $\hat{x}\in \R^d$, we have 
\begin{align*}
    \HF_v(\hat{x})-\HF_v(x^v)\geq\frac{\delta}{2\sqrt{d}}
    \sum_{j=1}^{d}\indicator\{\sign(\hat{x}_j)\neq \sign(x^v_j) 
    \},
\end{align*}
where the function $\sign(\cdot)$ is defined as:
\begin{align*}
    \sign(\hx_j)=\left\{\begin{array}{cc}
       + &  \text{ if } \hx_j>0 \\
         0 & \text{ if } \hx_j=0\\
         - & \text{ otherwise}
         \end{array}
         \right.
\end{align*}
\end{claim}

Claim~\ref{clm:error_determine_sign} provides a method to lower bound the minimax error.
Specifically, 
we define the hamming distance between any two vectors $x,y\in\R^d$ as $d_H(x,y)=\sum_{j=1}\indicator\{\sign(x_j)\neq \sign(y_j)\}$, and
we have
\begin{align}
\label{eq:minimax_to_testing}
    \eps_k^*(\cP_G,\cK)\geq \frac{\delta}{2\sqrt{d}}\{\inf_{\hat{v}}\E[d_H(\hv,v)]\},
\end{align}
where $\hv$ denotes the output of any algorithm mapping from the observation $(Y^1,\cdots,Y^k)$ to $\{-1,1\}^d$, and the probability is taken over the distribution of the underlying $v$, the observation $(Y^1,\cdots,Y^k) $ and any additional randomness in the algorithm.

% Let $S=\{j:v_j=1\}$, and we define the testing error of a solution $\hat{S}$ for estimating $S$ as
% \begin{align*}
%     \E[\hat{S}\Delta S]\defeq\sum_{j=1}^{d}\Pr[\hat{S}_j\neq S_j].
% \end{align*}

By Equation~\eqref{eq:minimax_to_testing}, it suffices to lower bound the value of the testing error $\E[d_H(\hv,v)]$. 
As discussed in \cite{ACCD12,DJWW15}, the randomness in the algorithm can not help, and we can assume the algorithm is deterministic, i.e. $(X^t,S^t)$ is a deterministic function of $Y^{[t-1]}$.\footnote{We use $Y^{[t]}$ to denote the first $t$ observations, i.e. $(Y^1,\cdots,Y^t)$}
The argument is basically based on the easy direction of Yao's principle.
% Indeed, recall $\cV=\{-1,+1\}^d$ is a finite set indexing a subset $\{\HF_v,\cN_v\}$.
% Then
% \begin{align*}
%     \sup_{\mathcal{P}\in \cP_G}\E[\eps_k(\cA,\cP,\cK)]\geq \frac{1}{|\cV|}\sum_{v\in \cV}\E_{\cN_v}[\HF_v(\hx)-\inf_{x\in\cK}\HF_v(x)]].
% \end{align*}
% At each iteration of any algorithm $\cA$, we can write $(x^t,S^t)=H_t(Y^{[t-1]},U^t)$ where we use $U^t$ is a random variable independent of $Y^{[t-1]}$ and denotes the randomness in $\cA$, and $H_t$ is a deterministic function.
% By the properties of expectations, we have
% \begin{align*}
%     &~\frac{1}{|\cV|}\sum_{v\in\cV}\E_{\cN_v}[\HF(\hx)-\inf_{x\in\cK}\HF_v(x)]\\
%     =&~\frac{1}{|\cV|}\sum_{v\in\cV}\E[\E_{\cN_v}[\HF(\hx)-\inf_{x\in\cK}\HF(x)\mid U^{[k]}]]\\
%     \ge & \inf_{u^{[k]}}\frac{1}{|\cV|}\sum_{v\in\cV}\E[\E_{\cN_v}[\HF(\hx)-\inf_{x\in\cK}\HF(x)\mid U^{[k]}=u^{[k]}]],
% \end{align*}
% which implies we can incorporate the best randomness into the algorithm $\cA$ and assume it is deterministic.

Now we continue our proof of the lower bound. We will make use of the property of the Bayes risk.

\begin{comment}
\begin{definition}[Bayes Risk, \cite{LR05}]
Suppose parameter $\theta$ is assumed
known only that it lies in a certain set $\Theta$ and consider a set of probability distributions $P=\{P_\theta:\theta\in\Theta\}$.
Given $\theta$, we can get observations $Y$ and can output an estimator $\hat{\theta}(Y)$ %\Yintat{what is X?} 
by a testing function $\hat{\theta}$.
We use a loss function $l(\theta,\hat{\theta})$ to quantifies the quality of the estimator.
For a prior distribution $\pi$ on $\Theta$, the average risk is defined as
\begin{align*}
    B_{\pi}(\hat{\theta})=\E_{\theta\sim\pi,Y\sim P_\theta}l(\theta,\hat{\theta}).
\end{align*}
The Bayes risk is the minimum that the average risk can achieve, i.e.
\begin{align*}
    B_{\pi}^*=\inf_{\hat{\theta}}B_{\pi}(\hat{\theta}).
\end{align*}
\end{definition}

\begin{lemma}[{\cite[Lemma 1]{ACCD12}}]
\label{lm:Bayes_risk}
Consider the problem of testing hypothesis $H_{-1}:v \sim \mathbb{P}_{-1}$ and $H_1:v\sim \mathbb{P}_1$, where $H_{-1}$ and $H_1$ are occurred with prior probability $\pi_{-1}$ and $\pi_1$ respectively prior to the experiment. Under the 0-1 loss, the Bayes risk $B$ satisfies
\begin{align*}
    B\geq \min(\pi_{-1},\pi_1)(1-\|\mathbb{P}_1-\mathbb{P}_{-1}\|_{\mathrm{TV}}).
\end{align*}
\end{lemma}
\end{comment}

\begin{lemma}[{\cite[Lemma 1]{ACCD12}}]
\label{lm:Bayes_risk}
Consider the problem of testing hypothesis $H_{-1}:v \sim \mathbb{P}_{-1}$ and $H_1:v\sim \mathbb{P}_1$, where $H_{-1}$ and $H_1$ occur with prior probability $\pi_{-1}$ and $\pi_1\defeq1-\pi_{-1}$ respectively prior to the experiment. 
For any algorithm that takes one sample $v$ and outputs $\hat{i}:v\rightarrow\{-1,1\}$, we define the Bayes risk $B$ be the minimum average probability that algorithm fails ($v$ is not sampled from $H_{\hat{i}(v)}$).
That is $B=\inf_{\hat{i}}\pi_{-1}\Pr[\hat{i}(v)=1\mid v\sim \mathbb{P}_{-1}]+\pi_1\Pr[\hat{i}(v)=0\mid v\sim \mathbb{P}_1]$.
Then, we have
\begin{align*}
    B\geq \min(\pi_{-1},\pi_1)(1-\|\mathbb{P}_1-\mathbb{P}_{-1}\|_{\mathrm{TV}}).
\end{align*}
\end{lemma}

\begin{lemma}
\label{lm:error_binary_test}
Suppose that $v$ is uniformly sampled from $\cV=\{-1,1\}^d$, then any estimate $\hat{v}$
%outputted by algorithm which satisfies the Orthogonal Query assumption
obeys
\begin{align*}
    \E[d_H(\hv,v)]\geq
    \frac{d}{2}\lp 1-\frac{\delta\sqrt{k}}{\sigma\sqrt{d}}\rp.
\end{align*}
\end{lemma}

\begin{proof}
Let $\pi_{-1}=\pi_1=1/2$.
For each $j$, define $\mathbb{P}_{-1,j}=\bbP(Y^{[k]}\mid v_j=-1)$ and $\bbP_{1,j}=\bbP(Y^{[k]}\mid v_j=1)$ to be distributions over the observations $(Y^1,\cdots,Y^k)$ conditional on $v_j\neq 1$ and $v_j=1$ respectively.
Let $B_j$ be the Bayes risk of the decision problem for $j$-th coordinate of $v$ between $H_{-1,j}: v_j=-1$ and $H_{1,j}: v_j=1$.
We have that
\begin{align*}
    \E[d_H(\hv,v)]
    \geq& \sum_{j=1}^{d}B_j\\
    \geq& \pi_1\sum_{j=1}^{d}(1-\|\bbP_{1,j}-\bbP_{-1,j}\|_{\mathrm{TV}})\\
    \geq& \frac{d}{2}\lp 1-\frac{1}{\sqrt{d}}\sqrt{\sum_{j=1}^{d}\|\bbP_{1,j}-\bbP_{-1,j}\|^2_{\mathrm{TV}}}\rp,
\end{align*}
where the first inequality follows from the definition of Bayes risk $B_j$, the second inequality follows by Lemma~\ref{lm:Bayes_risk} and the last inequality follows by the Cauchy-Schwartz inequality.

To complete the proof, it suffices to show that
\begin{align}
\label{eq:bounded_TVD}
    \sum_{j=1}^{d}\|\bbP_{1,j}-\bbP_{-1,j}\|_{\mathrm{TV}}^2\leq \frac{\delta^2}{\sigma^2}k.
\end{align}

Assuming Equation~\eqref{eq:bounded_TVD} first, which will be established later.
Then we know that
\begin{align*}
    \E[d_H(\hv,v)]\geq \frac{d}{2}(1-\frac{\delta\sqrt{k}}{\sigma\sqrt{d}}).
\end{align*}
\end{proof}

We will complete the proof of Lemma~\ref{lm:error_binary_test} by showing the following bounded total variation distance.
\begin{claim}
\begin{align*}
    \sum_{j=1}^{d}\|\bbP_{1,j}-\bbP_{-1,j}\|_{\mathrm{TV}}^2\leq \frac{\delta^2}{\sigma^2}k.
\end{align*}
\end{claim}

\begin{proof}
Applying Pinsker's inequality, we know $\|\bbP_{1,j}-\bbP_{-1,j}\|_{\mathrm{TV}}^2\leq \frac{1}{2}\mathrm{D}_{KL}(\bbP_{-1,j}\|\bbP_{1,j})$.
To bound the KL divergence between $\bbP_{-1,j}$ and $\bbP_{1,j}$ over all possible $Y^{[k]}$, consider $v'=(v_1,\cdots,v_{j-1},v_{j+1},\cdots,v_d)$, and define $\bbP_{-1,j,v'}(Y^{[k]})\defeq \bbP(Y^{[k]}\mid v_j=-1,v')$ to be the distribution conditional on $v_j=-1$ and $v'$.
%\gnote{Use $\bbP_{-1,j,v'}$ notation, since it also depends on $j.$}
We have
\begin{align*}
    \bbP_{-1,j}(Y^{[k]})= \sum_{v'}\Pr[v']\bbP_{-1,j,v'}(Y^{[k]}).
\end{align*}

The convexity of the KL divergence suggests that
\begin{align*}
    \mathrm{D}_{KL}(\bbP_{-1,j}\|\bbP_{1,j})\leq \sum_{v'}\Pr[v']\mathrm{D}_{KL}(\bbP_{-1,j,v'}\|\bbP_{1,j,v'}).
\end{align*}
Fixing any possible $v'$, we want to bound the KL divergence $\mathrm{D}_{KL}(\bbP_{-1,j,v'}\|\bbP_{1,j,v'})$.

Recall we are considering deterministic algorithms and
$(X^t,S^t)$ is a deterministic function of $Y^{[t-1]}$.
Let $Q_i\in \R^{d\times k}$ be a (random) matrix, which records the set of points the algorithm queries for the user $s_i$.
Specifically, for $t$-th step, if the algorithm queries $(X^t,S^t)$, then $Q_{i}^{t}=X^t$ if $S^t=s_i$, otherwise $Q_{i}^{t}=0$, where $Q_i^t$ is the $t$-th column of $Q_i$.

As we are considering linear functions, without loss of generality we can assume  $\langle Q_{i}^j,Q_{i}^{j'}\rangle=0$ for each $i$ and any $j\neq j'$, and $\|Q_{i}^t\|_2\in\{0,1\}$ for any $i$ and $t$.
We name this assumption \textsc{Orthogonal Query}.
Roughly speaking, for any algorithm, we can modify it to satisfy the Orthogonal Query.
Whenever the algorithm wants to query some point, we can use Gramâ€“Schmidt process to query another point and satisfy Orthogonal Query, and recover the function value at the original point queried by the algorithm.
% To establish this, observe that for any algorithm $\cA$, we can find another algorithm $\cA'$ which satisfies the Orthogonal Query assumption, and the distributions of the outputs of $\cA$ and $\cA'$ are the same almost everywhere.
% Give an example, suppose $j$ is the minimum integer that $\cA$ has re-queried information of some user, say $s_i$.
% That is $s^{t_1}\neq s^{t_2}$ for $t_1,t_2\leq j-1$ and $S^j=S^t=s_i$ for some $t\leq j-1$.
% Then we can construct $\cA'$ by taking $\cA$ as an black box: for the first $j-1$-th step, $\cA'$ queries $(X^t/\|X^t\|_2,S^t)$ for the first $j-1$ steps and inputs the values of $f(X^t/\|X^t\|_2,S^t)\cdot \|X^t\|_2$ to $\cA$.
% For $j$th step, $\cA'$ queries $(z';S^t)$ where $z'=\frac{x^j-\frac{\langle x^j,x^i\rangle }{\|x^i\|^2_2}\cdot x^i}{\|z-\frac{\langle x^j,x^i\rangle }{\|x^i\|^2_2}\cdot x^i\|_2}$, then recovers the value of $f(x^j;s^j)$ and inputs it to $\cA$ to observe what's the next query.
% $\cA'$ can repeat this procedure, and output whatever $\cA$ outputs finally.

By the chain-rule of KL-divergence, if we define $P_{-1,j,v'}(Y^t\mid Y^{[t-1]})$ to be the distribution of $t$th observation $Y^t$ conditional on $v'$, $v_j=-1$ and $Y^{[t-1]}$, then we have
\begin{align*}
    \mathrm{D}_{KL}(\bbP_{-1,j,v'}\|\bbP_{1,j,v'})=\sum_{t=1}^{k}\int_{\cY^{t-1}} \mathrm{D}_{KL}(P_{-1,j,v'}(Y^t\mid Y^{[t-1]}=y)\|P_{1,j,v'}(Y^t\mid Y^{[t-1]}=y)\d P_{-1,j,v'}(y).
\end{align*}

Fix $Y^{[t-1]}$ such that $Y^{[t-1]}=y$.
Since the algorithm is deterministic and $(X^t,S^t)$ is fixed given $Y^{[t-1]}$.
Let $S^t=s_i$ so $X^t=Q_i^t$.
%Denote the choice of the algorithm is $(Q_{i}^t,s_i)$ for $t$-th step conditional on $Y^{[t-1]}=y$.

Note that the $n$ users in $\cD$ are i.i.d. sampled.
Then $\mathrm{D}_{KL}(P_{-1,j,v'}(Y^t\mid Y^{[t-1]}=y)\|P_{1,j,v'}(Y^t\mid Y^{[t-1]}=y)$ only depends on the randomness of $s_i$ and the first $t$ columns of $Q_{i}$, which is denoted by $Q_{i}^{[t]}$.
We use $Y^{t}_{j}$ to denote the observation corresponding to user $s_j$ for the $t$th query (if $S^t\neq s_j$, we have $Y^{t}_j=0$).
Note that the observation $Y^{[t]}_i=Q_i^{[t]\top} s_i$ where $s_i\sim \cN(\delta v,\sigma^2 I_d)$. Then we know $Y^{[t]}_i$ is normally distributed with mean $\delta Q_{i}^{[t]\top} v$ and co-variance $\sigma^2 Q_{i}^{[t]\top} Q_{i}^{[t]}$.

Recall that the KL divergence between two normal distributions is $\mathrm{D}_{KL}(\cN(\mu_1,\Sigma)\|\cN(\mu_2,\Sigma))=\frac{1}{2}(\mu_1-\mu_2)^{\top}\Sigma^{-1}(\mu_1-\mu_2)$. Recall that we have the Orthogonal Query assumption and thus $Q_{i}^{[t]\top} Q_{i}^{[t]}\in\{0,1\}^{t\times t}$ is a diagonal matrix.
By the conditional distributions of Gaussian, we know $Y^t_i$ only depends on the $Q_{i}^t$ and it is independent of $Q_{i}^{[t-1]}$.

Hence we have
\begin{align*}
    &\mathrm{D}_{KL}(P_{-1,j,v'}(Y^t\mid Y^{[t-1]}=y)\|P_{1,j,v'}(Y^t\mid Y^{[t-1]}=y))\\
    =&\mathrm{D}_{KL}(P_{-1,j,v'}(Y^t_i\mid Y^{[t-1]}=y)\|P_{1,j,v'}(Y^t_i\mid Y^{[t-1]}=y))\\
    =&\frac{1}{2} (2\delta Q_{i}^t(j))^2/\sigma^2,
\end{align*}
where $Q_{i}^t(j)$ is the $j$-th coordinate of $Q_{i}^t$.
Summing over the terms, one has
\begin{align*}
\sum_{j=1}^{d}\|\bbP_{1,j}-\bbP_{-1,j}\|_{\mathrm{TV}}^2\leq &
    \frac{1}{2}\mathrm{D}_{KL}(\bbP_{-1,j}\|\bbP_{1,j})\\
    \leq&\frac{1}{2} \sum_{t=1}^{k}\sum_{j=1}^{d}\sum_{i=1}^{n}\E[\frac{1}{2} (2\delta Q_{i}^t(j))^2/\sigma^2] \\
    \leq& \frac{\delta^2}{\sigma^2}k,
\end{align*}
where the last line follows from the fact that for each $t$,$ \sum_{i=1}^{n}\|Q_{i}^t\|_2^2=\sum_{i=1}^{n}\sum_{j=1}^{d}(Q_{i}^t(j))^2=1$ as we only query one user for $t$-th step.

This completes the proof.
\end{proof}

Having Lemma~\ref{lm:error_binary_test}, we can complete the proof of Theorem~\ref{thm:info_bound}.

\begin{proof}{of Theorem~\ref{thm:info_bound}.}
As discussed before, we know
\begin{align*}
    \HF_v(\hat{x})-\HF_v(x^v)\geq\frac{\delta}{2\sqrt{d}}\sum_{j=1}^{d}\indicator\{\sign(\hat{x}_j)\neq \sign(x^v_j) \},
\end{align*}
and hence we know that
\begin{align*}
     \eps_k^*(\cP_G,\cK)\geq & \frac{\delta}{2\sqrt{d}}\inf_{\hat{v}}\E[d_H(\hat{v},v)]\\
    \geq & \frac{\delta \sqrt{d}}{4}\lp 1-\frac{\delta\sqrt{k}}{\sigma\sqrt{d}}\rp,
\end{align*}
where the last line follows from Lemma~\ref{lm:error_binary_test}.
%As we assume $\cK$ is the unit ball and thus $D=2$.
We now set $\delta=\frac{\sigma \sqrt{d}}{2\sqrt{k}}$ and $\sigma=\frac{G}{\sqrt{d+d^2/4k}}$, so that $d(\sigma^2+\delta^2)=G^2$.
Hence one has
\begin{align*}
 \eps_k^*(\cP_G,\cK)
\geq \frac{\delta\sqrt{d}}{8} 
= \frac{D\delta\sqrt{d}}{16}
= \frac{GD}{16\sqrt{1+\frac{4k}{d}}}
\ge \frac{GD}{16}\min\left\{1,\sqrt{\frac{d}{4k}}\right\}.
%\ge GD\frac{\min\{\sqrt{d},\sqrt{k}\}}{40\sqrt{k}}.
\end{align*}
%\gnote{Check the last line or modify the lower bound to $ \frac{GD}{16}\min\{1,\sqrt{\frac{d}{4k}}\}$ everywhere.}
Thus we complete the proof.
\end{proof}

\begin{corollary}[Lower bound for DP-SCO]
\label{cor:DPSCOlower}
For any (non-private) algorithm which makes less than $O\lp\min\{\frac{\eps^2n^2}{\log(1/\delta)},nd\}\rp$ function value queries, there exist a convex domain $\cK\subset \R^d$ of diameter $D$, a distribution $\cP$ supported on $G$-Lipschitz linear functions $f(x;s)\defeq\langle x,s\rangle$, such that the output $\hx$ of the algorithm satisfies that
\begin{align*}
    \E_{s\sim\cP}[\langle \hx,s\rangle]-\min_{x\in\cK}\E_{s\sim\cP}[\langle x,s\rangle]\geq \Omega\lp \frac{G D}{\sqrt{1+\log(n)/d}} \cdot \min\lc\frac{\sqrt{\log(1/\delta)d}}{\eps n}+\frac{1}{\sqrt{n}},1\rc \rp.
\end{align*}
\end{corollary}
\begin{proof}
% WLOG, we can assume that $\frac{\sqrt{\log(1/\delta)d}}{\eps n}+\frac{1}{\sqrt{n}}=O(1)$. This is a reasonable assumption as if $\frac{\sqrt{\log(1/\delta)d}}{\eps n}+\frac{1}{\sqrt{n}}=\Omega(1)$, uniformly randomly output a point in $\cK$ is a good DP solution.

Note that Theorem~\ref{thm:info_bound} almost gives us what we want, except that the Lipschitz constant of the functions in the hard distribution is bounded only on average by $G$. To get distributions over $G$-Lipschitz functions, we just condition on the bad event not happening.

Recall that we are considering the set of distributions $\cN_v=\cN(\delta v,\sigma^2 I_d)$ for which $\E_{s\sim\cN_v}\|s\|_2^2\le G^2=d(\delta^2+\sigma^2)$.
And we proved that 
$\inf_{\cA\in \mathbb{A}_k}\sup_{v\in \cV}\E_{s\sim \cN_v,\cA}[\HF_v(\hat{x}_k)-\HF_v^*]\ge \frac{GD}{16}\min\left\{1,\sqrt{\frac{d}{4k}}\right\}$ in Theorem~\ref{thm:info_bound}, where $\hat{x}_k$ is the output of $\cA$ with $k$ observations $Y^{[k]}$.
%\gnote{What is $f,\cF_G,\hat{x}_k$ in the above equation?}
To prove Corollary~\ref{cor:DPSCOlower}, we need to modify the distribution of $s$ to satisfy the Lipschitz continuity.

In particularly, for some constant $c$, we know 
\begin{align*}
    &\E[\HF_v(\hat{x}_k)-\HF_v^*]\\
    =&\E\Big[ \HF_v(\hat{x}_k)-\HF_v^*\mid \max_{s_i\in \cD}\|s_i\|_2\leq cG\sqrt{1+\log(nd)/d}\Big]\Pr\Big[\max_{s_i\in \cD}\|s_i\|_2\leq cG \sqrt{1+\log(nd)/d}\Big]+\\
    &~~\E\Big[ \HF_v(\hat{x}_k)-\HF_v^*\mid \max_{s_i\in \cD}\|s_i\|_2> cG\sqrt{1+\log(nd)/d}\Big]\Pr\Big[\max_{s_i\in \cD}\|s_i\|_2> cG\sqrt{1+\log(nd)/d}\Big].
\end{align*}
By the concentration of spherical Gaussians, we know if $s\sim\cN(\delta v,\sigma^2 I_d)$, then 
\begin{align*}
    \Pr\Big[\|s-\delta v\|_2^2\leq \sigma^2 d(1+2\sqrt{\ln(1/\eta)/d}+2\ln(1/\eta)/d)\Big]\geq 1-\eta.
\end{align*}
% \Gopi{Do we really need to lose $\log(nd)$ factor here? looks like a constant is enough...}
% \Daogao{We want $\eta\leq 1/\poly(nd)$. I use log in case that $n\gg d$. maybe I just modify it.}
We can choose the constant $c$ large enough, such that $\Pr[\max_{s_i\in \cD}\|s_i\|_2\leq cG\sqrt{1+\log(nd)/d}]\geq 1-1/\poly(nd)$, which implies
\begin{align*}
    \inf_{\cA\in \mathbb{A}_k}\sup_{v\in \cV}\E_{\cD \sim \cN_v^n,\cA}\Big[\HF_v(\hat{x}_k)-\HF_v^*\mid \max_{s_i\in \cD}\|s_i\|_2\leq cG\sqrt{1+\log(nd)/d}\Big]\geq \Omega(GD\frac{\min\{\sqrt{d},\sqrt{k}\}}{\sqrt{k}}).
\end{align*}
If we use the distributions conditioned on $\max_{s_i\in \cD}\|s_i\|_2\leq cG\sqrt{1+\log(nd)/d}$ rather than the Gaussians, and scale the constant to satisfy the assumption on Lipschitz continuity, we can prove the statement.
Particularly, let $G'=cG(\sqrt{1+\log(nd)/d})$. 
If the algorithm can only make $k=O\lp\min\{\frac{\eps^2n^2}{\log(1/\delta)},nd\}\rp$ observations, we know 
\begin{align*}
    &\inf_{\cA\in \mathbb{A}_k}\sup_{v\in \cV}\E_{\cD \sim \cN_v^n,\cA}\Big[\HF_v(\hat{x}_k)-\HF_v^*\mid \max_{s_i\in \cD}\|s_i\|_2\leq G'\Big]\\
    \geq&
    \Omega\lp GD\cdot \min\lc(\frac{\sqrt{\log(1/\delta)d}}{\eps n}+\frac{1}{\sqrt{n}}),1\rc \rp  \\
    =&\Omega\lp \frac{G' D}{\sqrt{1+\log(nd)/d}}\cdot \min\lc\frac{\sqrt{\log(1/\delta)d}}{\eps n}+\frac{1}{\sqrt{n}},1\rc  \rp,
\end{align*}
% \Gopi{There is a gap in the proof. We don't get the required result exactly.}
which proves the lower bound claimed in the Corollary statement.
\end{proof}

\begin{corollary}[Lower bound for sampling scheme]
\label{cor:Samplinglower}
Given any $G > 0$ and $\mu > 0$. For any algorithm which takes function values queries less than $O\lp\frac{G^2}{\mu}/(1+\log(G^2/\mu)/d)\rp$ times, there is a family of $G$-Lipschitz linear functions $\{f_i(x)\}_{i\in I}$ defined on some $\ell_2$ ball $\cK\subset\R^d$, such that the total variation distance between the distribution of the output of the algorithm and the distribution proportional to $\exp(-\E_{i\in I}f_i(x)-\mu \|x\|^2 / 2)$ is at least $\min(1/2, \sqrt{d \mu / G^2})$.
\end{corollary}
\begin{proof}
By a similar argument in the proof of Corollary~\ref{cor:DPSCOlower}, for any algorithm which can only make $k$ observations, there are a family of $G$-Lipschitz linear functions restricted on an $\ell_2$ ball $\cK$ of diameter $D$ centered at $\mathbf{0}$ such that  
\begin{align}
\label{eq:lower_sampling_SCO}
    \E\Big[\HF_v(\hat{x}_k)-\HF_v^*\Big]
    \ge&\Omega\lp \frac{G D}{\sqrt{1+\log(k)/d}} \cdot \min\lc \sqrt{\frac{d}{k}},1\rc \rp,
\end{align}
where $\HF_v^*=\min_{x\in\cK}\HF_v(x)$ and $\hx_k\in\cK$ is the output of $\cA$.

%Without loss of generality, we can shift the functions and assume the zero point $\mathbf{0}$ is the center of $\cK$.
Suppose we have a sampling algorithm that takes $k$ queries. We use it to sample from $x^{(sol)}$ proportional to $p(x):=\exp(-\HF_v(x)-\frac{\mu}{2} \|x\|^2)$ on $\cK$ with total variation distance $\eta\leq \min(1/2, \sqrt{d \mu / G^2})$. 
% Note that $\HF_v(x)$ is some $G$-Lipschitz linear function, and by Lemma~\ref{lm:utility_tech}, we know $\E[\|x^{(sol)}\|]\leq O(\sqrt{d/\mu})$.
% To ensure $x^{(sol)}$ is bounded, we can define the bounded variant $\overline{x^{(sol)}} = x^{(sol)}$ if $\|x^{(sol)}\|^2 \leq O(d \log(1/\eta) / \mu)$ and $\mathbf{0}$ otherwise. Note that $p$ is $\mu$-strongly convex, we have $\|x\|^2 \leq O(d \log(1/\eta)/ \mu)$ with probability $1-\eta$ by Lemma~\ref{lem:gaussian_concentration} and hence $\overline{x^{(sol)}}$ has total variation distance $2 \eta$ from $p$.

Lemma \ref{lm:utility_tech} shows that
\begin{align*}
    \E[\HF_{v}(x^{(sol)})+\frac{\mu}{2}\|x^{(sol)}\|^{2}]\leq\min_{x\in \cK}\lp\HF_{v}(x)+\frac{\mu}{2}\|x\|^{2}\rp+O(d) + O(\eta) \cdot (GD+\mu D^2),
\end{align*}
where the last term involving $\eta$ is due to the total variation distance between $x^{(sol)}$ and $p$. Setting $D=\sqrt{d/\mu}$ and using the diameter of $\cK$ is $D$ and $\eta \leq \min(1/2, \sqrt{d \mu / G^2})$, we have
\begin{align*}
\E[\HF_{v}(x^{(sol)})] & \leq\min_{x\in\cK}\HF_{v}(x)+\frac{\mu}{2}D^{2}+O(d+\eta\cdot (GD+\mu D^2))\\
& \leq \min_{x\in\cK}\HF_{v}(x)+O(d).
\end{align*}
Note that we set $D = \sqrt{d/\mu}$. Comparing with \eqref{eq:lower_sampling_SCO}, we have
\[
\frac{G\sqrt{d/\mu}}{\sqrt{1+\log(k)/d}}\min\left\{ \sqrt{\frac{d}{k}},1\right\} \leq O(d).
\]

If $d\leq G^{2}/\mu\leq \exp(d)$, we have
\[
G\sqrt{d/\mu}\sqrt{\frac{d}{k}}\leq O(d)
\]
and hence $k=\Omega(G^{2}/\mu)$.
If $G^{2}/\mu\geq\exp(d)$,
we have
\[
\frac{G\sqrt{d/\mu}}{\sqrt{\log(k)/d}}\sqrt{\frac{d}{k}}\leq O(d)
\]
and hence $k=\Omega(\frac{G^{2}d/\mu}{\log(G^{2}/\mu)})$.
If $G^{2}/\mu\leq d$, we can construct our function only on the first
$O(G^{2}/\mu)$ dimensions to get a lower bound $k=\Omega(G^{2}/\mu).$
Combining all cases gives the result.
\end{proof}


% \begin{corollary}[Lower bound for sampling scheme]
% \label{cor:Samplinglower}
% Given any $G > 0$ and $\mu > 0$. For any algorithm which takes function values queries less than $O\lp\frac{G^2}{\mu}/(1+\log(G^2/\mu)/d)\rp$ times, there is a family of $G$-Lipschitz linear functions $\{f_i(x)\}_{i\in I}$, such that the total variation distance between the distribution of the output of the algorithm and the distribution proportional to $\exp(-\E_{i\in I}f_i(x)-\mu \|x\|^2 / 2)$ is at least $\mu/G^2$.
% \end{corollary}
% \begin{proof}
% By a similar argument in the proof of Corollary~\ref{cor:DPSCOlower}, for any algorithm which can only make $k$ observations, there are a family of $G$-Lipschitz linear functions restricted on an $\ell_2$ ball $\cK$ of diameter $D$ such that  
% \begin{align}
% \label{eq:lower_sampling_SCO}
%     \E\Big[\HF_v(\hat{x}_k)-\HF_v^*\Big]
%     \ge&\Omega\lp \frac{G D}{\sqrt{1+\log(k)/d}} \cdot \min\lc \sqrt{\frac{d}{k}},1\rc \rp,
% \end{align}
% where $\HF_v^*=\min_{x\in\cK}\HF_v(x)$ and $\hx_k\in\cK$ is the output of $\cA$.

% Suppose we have a sampling algorithm that takes $k$ queries. We use it to sample from $x^{(sol)}$ proportional to $p(x):=\exp(-\HF_v(x)-\frac{\mu}{2} \|x\|^2)$ with total variation distance $\eta$. 
% Without loss of generality, we can shift the functions and assume the zero point $\mathbf{0}\in\cK$ and minimizes $\HF_v(x)+\frac{\mu}{2} \|x\|^2$.
% Note that $\HF_v(x)$ is some $G$-Lipschitz linear function, and by Lemma~\ref{lm:utility_tech}, we know $\E[\|x^{(sol)}\|]\leq O(\sqrt{d/\mu})$.
% To ensure $x^{(sol)}$ is bounded, we can define the bounded variant $\overline{x^{(sol)}} = x^{(sol)}$ if $\|x^{(sol)}\|^2 \leq O(d \log(1/\eta) / \mu)$ and $\mathbf{0}$ otherwise. Note that $p$ is $\mu$-strongly convex, we have $\|x\|^2 \leq O(d \log(1/\eta)/ \mu)$ with probability $1-\eta$ by Lemma~\ref{lem:gaussian_concentration}
% and hence $\overline{x^{(sol)}}$ has total variation distance $2 \eta$ from $p$.

% Lemma \ref{lm:utility_tech} shows that
% \begin{align*}
%     \E[\HF_{v}(\overline{x^{(sol)}})+\frac{\mu}{2}\|\overline{x^{(sol)}}\|^{2}]\leq\min_{x\in \R^d}\lp\HF_{v}(x)+\frac{\mu}{2}\|x\|^{2}\rp+O(d) + O(\eta) \cdot (G \sqrt{d \log(1/\eta) / \mu} + d \log(1/\eta))
% \end{align*}
% where the last term involving $\eta$ is due to the total variation distance between $\overline{x^{(sol)}}$ and $p$. Optimizing on the right hand side on $\cK$ gets $\overline{x^{(sol)}_{\cK}}$ and using the diameter of $\cK$ is $D$, we have \Daogao{Seems problematic, as we do not know how to get a solution in $\cK$?}
% \begin{align*}
% \E[\HF_{v}(\overline{x^{(sol)}_{\cK}})] & \leq\min_{x\in\cK}\HF_{v}(x)+\frac{\mu}{2}D^{2}+O(d+\eta\cdot G\sqrt{d\log(1/\eta)/\mu})\\
%  & \leq\min_{x\in\cK}\HF_{v}(x)+\frac{\mu}{2}D^{2}+O(d+G\sqrt{d\eta/\mu})\\
%  & \leq\min_{x\in\cK}\HF_{v}(x)+\frac{\mu}{2}D^{2}+O(d)
% \end{align*}
% \Gopi{Isn't it enough to have $\eta G \sqrt{d\log(1/\eta)/\mu}\le d$ i.e. $\eta \le \Tilde{O}( \frac{\sqrt{\mu d}}{G}).$}
% where we used $\eta \leq \mu / G^2$ at the end. Putting $D = \sqrt{d/\mu}$ and comparing with \eqref{eq:lower_sampling_SCO}, we have
% \[
% \frac{G\sqrt{d/\mu}}{\sqrt{1+\log(k)/d}}\min\left\{ \sqrt{\frac{d}{k}},1\right\} \leq O(d).
% \]

% If $d\leq G^{2}/\mu\leq\exp(d)$, we have
% \[
% G\sqrt{d/\mu}\sqrt{\frac{d}{k}}\leq O(d)
% \]
% and hence $k=\Omega(G^{2}/\mu).$ If $G^{2}/\mu\geq\exp(d)$,
% we have
% \[
% \frac{G\sqrt{d/\mu}}{\sqrt{\log(k)/d}}\sqrt{\frac{d}{k}}\leq O(d)
% \]
% and hence $k=\Omega(\frac{G^{2}d/\mu}{\log(G^{2}/\mu)})$.
% If $G^{2}/\mu\leq d$, we can construct our function only on the first
% $O(\log(G^{2}/\mu))$ dimensions to get a lower bound $k=\Omega(G^{2}/\mu).$
% Combining all cases gives the result.
% \end{proof}




%Picking $\mu = c^2 d / D^2$ for some $c \geq 1$ to be determined, we have
%\begin{align*}
%\E\HF_{v}(x^{(sol)})	\leq \HF_v^* +O(c^{2}d+\frac{\eta}{\sqrt{c}}GD).
%\end{align*}

%we can design a regularized mechanism by sampling from $x^{(sol)}$ proportional to $\exp(-c(F(\cdot;\cD)+\psi(\cdot)))$ where  $\psi(x)=\frac{\mu}{2}\|x\|_2^2$ is a regularization term and $c=O(\mu nd/G^2)$.
%By Theorem~\ref{thm:generalization_error} and our guarantee on the total variation distance $\gamma<\eta$ between our final output and the distribution proportional to $\exp(-c(F(\cdot;\cD)+\psi(\cdot)))$, we know that for any $v\in \cV$ one has
%\begin{align*}
%    \E[\HF_v(x^{(sol)})-\min_{x\in\cK}\HF_v(x)]\leq O\lp GD(\frac{1}{\sqrt{n}}+\eta)\rp.
%\end{align*}
%Then if we choose $n=\Theta(1/\eta^2)$, we can get expected excess population loss $O(GD/\sqrt{n})$ with $O(nd/\log(nd))$ queries. If we set the constants hidden carefully, we can break Equation~\eqref{eq:lower_sampling_SCO}, which is contradiction.
%So the corollary holds and we complete the proof.

% Corollary~\ref{cor:Samplinglower} can be deduced directly by combining the Corollary~\ref{cor:DPSCOlower} and our results (Theorem~\ref{thm:generalization_error}) about generalization error of exponential mechanism.

% Particularly, 
% If we have such a scheme that works for all $G$-Lipschitz linear functions,
% we can try to sample $x^{(sol)}$ proportional to $\exp(-k(F(\cdot;\cD)+\psi(\cdot)))$ where $k=O(\frac{\mu}{G^2}\min\{\frac{\eps^2n^2}{\log(1/\delta)},nd \})$ with $O(\min\{\frac{\eps^2n^2}{\log(1/\delta)},nd \}/\log(nd))$ queries.
% By Theorem~\ref{thm:generalization_error} and our guarantee on the total variation distance $\gamma<\eta$ between our final output and the distribution proportional to $\exp(-k(F(\cdot;\cD)+\psi(\cdot)))$, we know that
% \begin{align*}
%     \E[\HF(x^{(sol)})-\min_{x\in\cK}\HF(x)]\leq O\lp GD(\frac{\sqrt{\log(1/\delta)d}}{\eps n}+\frac{1}{\sqrt{n}}+\eta)\rp.
% \end{align*}
% Then if we choose $n=\Theta(1/\eta^2)$, we can get expected excess population loss $O(GD(\frac{\sqrt{\log(1/\delta)d}}{\eps n}+\frac{1}{\sqrt{n}}))$ with $O(\min\{\frac{\eps^2n^2}{\log(1/\delta)},nd \}/\log(nd))$ queries..
% If we set the constants hidden carefully and can break corollary~\ref{cor:DPSCOlower}, which is contradiction.
% \gnote{Why are $\epsilon,\delta$ appearing here? Write the proof without using $\epsilon,\delta$} 

%Corollary~\ref{cor:DPSCOlower} can be deduced from the same proof of Theorem~\ref{thm:info_bound} and take care of the Lipschitz condition of the linear functions, and


%To address the first difference, for each step $t$, we can consider the $\cA$ which can choose $d$ points $(x_1^t,\cdots,x_d^t)$ to be the $(e_1,\cdots,e_d)$ where $e_j$ is the vector of size $d$ containing all zeros except for a 1 in the $j$-th position, and thus can recover the $S^t$.
%Thus our algorithm does not have any advantage over $\cA$.The second difference can be addressed by using an alternative argument with a truncated Gaussian.Thus we know the lower bound in \cite{DJWW15} can be extended to our case.With queries $\min\{n^2,nd\}$, the lower bound should be $\Omega(GD(\frac{1}{\sqrt{n}}+\frac{\sqrt{d}}{n}))$ by choosing $m=\min\{n,d\}$ and $k=n$.