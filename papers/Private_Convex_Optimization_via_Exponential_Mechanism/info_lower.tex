\section{Information-theoretic Lower Bound for DP-SCO}
\label{sec:infolower}

 In this section, we prove an information-theoretic lower bound for the query complexity required for DP-SCO (with value queries), which matches (up to some logarithmic terms) the query complexity achieved by our algorithm (in Theorem \ref{thm:dpsco_impl}).
Our proof is similar to the previous works like \cite{ACCD12,DJWW15} with some modifications.


% In fact, the query complexity of our (non-private) sampling scheme and algorithm for solving DP-SCO matches the  information-theoretic minimax lower bounds (up to some logarithmic terms).
% Our proof is similar to the previous works like \cite{ACCD12,DJWW15} with some modifications.
% For completeness, we present the proof here.
%We will demonstrate the result of DP-SCO first.

Before stating the lower bound, we define some notations.
Recall that we are given a set $\cD$ of $n$ samples (users) $\{s_1,\cdots,s_n\}$.
Let $\mathbb{A}_k$ be the collection of all algorithms that observe a sequence of $k$ data points $(Y^1,\cdots,Y^k)$ with $Y^t=f(X^t;S^t)$ where %$s^t$ is an independent sample randomly drawn from the underlying distribution $\mathcal{P}$, 
$S^t\in \cD$ and $X^t\in \cK$ are chosen arbitrarily and adaptively by the algorithm (and possibly using some randomness).

For the lower bound, we only consider linear functions, that is we define $f(x;s)\defeq\langle x,s\rangle$. And let $\cP_G$ be the collection of all distributions such that if $\cP\in\cP_G$, then $\E_{s\sim\cP}\|s\|_2^2\leq G^2$.

% \begin{align*}
%     \cF_G\defeq\{(f,\mathcal{P}):f(x;s)=\langle x,s\rangle,\E_{s\sim \cP}[\|s\|^2_2]\leq G^2\}
% \end{align*}
% be a class of linear functionals.
And we define the optimality gap
\begin{align*}
    \eps_k(\cA,\cP,\cK)\defeq&~ \E_{\cD\sim\cP^n,\cA}[\HF(\hat{x}(\cD))]-\inf_{x\in \cK}\hat{F}(x),
\end{align*}
where $\HF(x) = \E_{s\sim\cP} f(x; s)$, $\hat{x}$ is the output the algorithm $\cA$ given the input dataset $\cD$ and the expectation is over the dataset $\cD\sim \cP^n$ and the randomness of the algorithm $\cA.$ Note that we can rewrite the optimality gap as:
\begin{align*}
    \eps_k(\cA,\cP,\cK)=&~ \E_{\cD\sim\cP^n,\cA}[\HF(\hat{x}(\cD))]-\inf_{x\in \cK}\hat{F}(x)\\
    =& \E_{s\sim\cP}\Big[\E_{\cD\sim\cP^n,\cA}f(\hat{x}(\cD);s)]\Big]-\inf_{x\in\cK}\E_{s\sim\cP}[f(x;s)]\\
    =& \E_{s\sim\cP,\cD\sim\cP^n,\cA}[\hat{x}(\cD)^\top s]-\inf_{x\in\cK}\E_{s\sim \cP}[x^\top s].
\end{align*}
The minimax error is defined by
\begin{align*}
    \eps_k^*(\cP_G,\cK)\defeq \inf_{\cA\in \mathbb{A}_k}\sup_{\mathcal{P}\in \cP_G}\eps_k(\cA,\cP,\cK).
\end{align*}
%where the expectation is taken over the observations $(Y^1,\cdots,Y^k)$ and any additional randomness in $\cA$.

\begin{theorem}
%\Yintat{Make this statement easier to understand. Maybe add something like: In particular, there is no algorithm that use something something and get error something something}
\label{thm:info_bound}
Let $\cK$ be the $\ell_2$ ball of diameter $D$ in $\R^d$, then
\begin{align*}
    \eps_k^*(\cP_G,\cK)\geq \frac{GD}{16}\min\left\{1,\sqrt{\frac{d}{4k}}\right\}.
\end{align*}
In particular, for any (randomized) algorithm $\cA$ which can observe a sequence of data points $(Y^1,\cdots,Y^k)$ with $Y^t=f(X^t;S^t)$ where $S^t\in\cD=\{s_1,s_2,\dots,s_n\}$ and $X^t\in\cK$ are chosen arbitrarily and adaptively by $\cA$,
there exists a distribution $\cP$ over convex functions such that $\E_{s\sim \mathcal{P}}[\|\nabla f(x,s)\|_2^2]\leq G^2$ for all $x\in \cK$, such that the output $\hx$ of the algorithm satisfies
\begin{align*}
    \E_{s\sim\cP}\Big[ \E_{\cD\sim\cP^n,\cA}f(\hx;s)]\Big]-\min_{x\in\cK}\E_{s\sim\cP}[f(x;s)]\geq \frac{GD}{16}\min\left\{1,\sqrt{\frac{d}{4k}}\right\}.
\end{align*}
\end{theorem}


% There is one difference between the condition in Theorem~\ref{thm:info_bound_technical} and our desired lower bound.
% We are under the assumption that the convex functions are $G$-Lipschitz, i.e.,  $ \|\nabla_x f(x;s)\|\leq G$ for all $x,s$, while the lower bound in Theorem~\ref{thm:info_bound_technical} gives a distribution $\cP$ over convex functions such that $\E_{s\sim \cP}[\|\nabla_x f(x;s) \|_2^2]\leq G^2$ which is a weaker condition. In the lower bound the functions $f(x;s)=\inpro{x}{s}$ are linear, and so $\nabla_x f(x;s)=s.$ Since the distribution $\cP$  
% To deal with this difference, we can lose a logarithmic term $\log(dn)$ to get a new information lower bound, i.e., we can truncate the Gaussian distribution used in the proof and conditional on the event that $\|\nabla f(x;s) \|_2^2\leq  O(G^2 \log (nd))$ for each $s$ in the dataset, which occurs with probability at least $1-\frac{1}{\mathrm{poly}(nd)}$ and the truncation does not influence the remaining proofs.
% \gnote{Can we remove the above paragraph?}
% we can assume the algorithm can recover the underlying $v$ exactly which leads to a zero error.
%  Note that the probability of a bad event that at least one $s_i\in\cD$ sa can be made polynomially small, and 
% Thus it serves a new bound for our case.

%At the beginning, let me introduce the lower bound in \cite{DJWW15} at first.Their main statement is about getting two values each query, and can be extended to query $d$ values.


%Without loss of generality, we consider the case when $d\leq n$ first.
% \begin{theorem}
% \label{thm:info_bound}
% For any (randomized) algorithm $\cA$ which can observe a sequence of $k$ data points $(Y^1,\cdots,Y^k)$ with $Y^t=f(X^t;S^t)$ where $S^t\in\cD=\{s_1,s_2,\dots,s_n\}$ and $X^t\in\cK$ are chosen arbitrarily and adaptively by $\cA$,
% there exists a distribution $\cP$ over convex functions such that $\E_{s\sim \mathcal{P}}[\|\nabla f(x,s)\|_2^2]\leq G^2$ for all $x\in \cK$, such that the output $\hx$ of the algorithm satisfies
% \begin{align*}
%     \E_{s\sim\cP}\Big[ \E_{\cD\sim\cP^n,\cA}f(\hx;s)]\Big]-\min_{x\in\cK}\E_{s\sim\cP}[f(x;s)]\geq \frac{1}{40}\frac{GD}{\sqrt{k}}\min\{\sqrt{d},\sqrt{k}\}.
% \end{align*}
% \end{theorem}

\subsection{Proof of Theorem \ref{thm:info_bound}}
We reduce the optimization problem into a series of binary hypothesis tests.
Recall we are considering linear functions $f(x;s)\defeq\langle x,s\rangle$.
Let $\cV=\{-1,1\}^d$ be a Boolean hyper-cube and for each $v\in \cV$, let $\cN_{v}=\cN(\delta v,\sigma^2I_{d})$ be a Gaussian distribution for some parameters to be chosen such that $\HF_v(x)\defeq \E_{s\sim\cN_v}[f(x;s)]=\delta\langle x,v\rangle$. Note that $$\E_{s\sim \cN_v}[\|\nabla f(x,s)\|_2^2]=\E_{s\sim \cN_v}[\norm{s}_2^2]=(\delta^2+\sigma^2)d.$$ Therefore $G=\sqrt{d(\delta^2+\sigma^2)}.$
%Then by taking $nd$ queries of function like $f(e_i;s_j)$, we can determine the exact values of the $n$ samples $\{s_1,\cdots,s_n\}$.

Clearly the lower bound should scale linearly with $D$. Therefore without loss of generality, we can assume that the diameter $D=2$ and define $\cK=\{x\in\R^d:\|x\|_2\leq 1\}$ to be the unit ball.
As in \cite{ACCD12}, we suppose that $v$ is uniformly sampled from $\cV=\{-1,1\}^d$.
Note that if we can find a good solution to $\HF_v(x)$, we need to determine the signs of vector $v$ well. Particularly, we have the following claim:
\begin{claim}[\cite{DJWW15}]
\label{clm:error_determine_sign}
For each $v\in \cV$, let $x^v$ minimize $\HF_v$ over $\cK$ and obviously we know that $x^v=-v/\sqrt{d}$. 
For any solution $\hat{x}\in \R^d$, we have 
\begin{align*}
    \HF_v(\hat{x})-\HF_v(x^v)\geq\frac{\delta}{2\sqrt{d}}
    \sum_{j=1}^{d}\indicator\{\sign(\hat{x}_j)\neq \sign(x^v_j) 
    \},
\end{align*}
where the function $\sign(\cdot)$ is defined as:
\begin{align*}
    \sign(\hx_j)=\left\{\begin{array}{cc}
       + &  \text{ if } \hx_j>0 \\
         0 & \text{ if } \hx_j=0\\
         - & \text{ otherwise}
         \end{array}
         \right.
\end{align*}
\end{claim}

Claim~\ref{clm:error_determine_sign} provides a method to lower bound the minimax error.
Specifically, 
we define the hamming distance between any two vectors $x,y\in\R^d$ as $d_H(x,y)=\sum_{j=1}\indicator\{\sign(x_j)\neq \sign(y_j)\}$, and
we have
\begin{align}
\label{eq:minimax_to_testing}
    \eps_k^*(\cP_G,\cK)\geq \frac{\delta}{2\sqrt{d}}\{\inf_{\hat{v}}\E[d_H(\hv,v)]\},
\end{align}
where $\hv$ denotes the output of any algorithm mapping from the observation $(Y^1,\cdots,Y^k)$ to $\{-1,1\}^d$, and the probability is taken over the distribution of the underlying $v$, the observation $(Y^1,\cdots,Y^k) $ and any additional randomness in the algorithm.

% Let $S=\{j:v_j=1\}$, and we define the testing error of a solution $\hat{S}$ for estimating $S$ as
% \begin{align*}
%     \E[\hat{S}\Delta S]\defeq\sum_{j=1}^{d}\Pr[\hat{S}_j\neq S_j].
% \end{align*}

By Equation~\eqref{eq:minimax_to_testing}, it suffices to lower bound the value of the testing error $\E[d_H(\hv,v)]$. 
As discussed in \cite{ACCD12,DJWW15}, the randomness in the algorithm can not help, and we can assume the algorithm is deterministic, i.e. $(X^t,S^t)$ is a deterministic function of $Y^{[t-1]}$.\footnote{We use $Y^{[t]}$ to denote the first $t$ observations, i.e. $(Y^1,\cdots,Y^t)$}
The argument is basically based on the easy direction of Yao's principle.
% Indeed, recall $\cV=\{-1,+1\}^d$ is a finite set indexing a subset $\{\HF_v,\cN_v\}$.
% Then
% \begin{align*}
%     \sup_{\mathcal{P}\in \cP_G}\E[\eps_k(\cA,\cP,\cK)]\geq \frac{1}{|\cV|}\sum_{v\in \cV}\E_{\cN_v}[\HF_v(\hx)-\inf_{x\in\cK}\HF_v(x)]].
% \end{align*}
% At each iteration of any algorithm $\cA$, we can write $(x^t,S^t)=H_t(Y^{[t-1]},U^t)$ where we use $U^t$ is a random variable independent of $Y^{[t-1]}$ and denotes the randomness in $\cA$, and $H_t$ is a deterministic function.
% By the properties of expectations, we have
% \begin{align*}
%     &~\frac{1}{|\cV|}\sum_{v\in\cV}\E_{\cN_v}[\HF(\hx)-\inf_{x\in\cK}\HF_v(x)]\\
%     =&~\frac{1}{|\cV|}\sum_{v\in\cV}\E[\E_{\cN_v}[\HF(\hx)-\inf_{x\in\cK}\HF(x)\mid U^{[k]}]]\\
%     \ge & \inf_{u^{[k]}}\frac{1}{|\cV|}\sum_{v\in\cV}\E[\E_{\cN_v}[\HF(\hx)-\inf_{x\in\cK}\HF(x)\mid U^{[k]}=u^{[k]}]],
% \end{align*}
% which implies we can incorporate the best randomness into the algorithm $\cA$ and assume it is deterministic.

Now we continue our proof of the lower bound. We will make use of the property of the Bayes risk.

\begin{comment}
\begin{definition}[Bayes Risk, \cite{LR05}]
Suppose parameter $\theta$ is assumed
known only that it lies in a certain set $\Theta$ and consider a set of probability distributions $P=\{P_\theta:\theta\in\Theta\}$.
Given $\theta$, we can get observations $Y$ and can output an estimator $\hat{\theta}(Y)$ %\Yintat{what is X?} 
by a testing function $\hat{\theta}$.
We use a loss function $l(\theta,\hat{\theta})$ to quantifies the quality of the estimator.
For a prior distribution $\pi$ on $\Theta$, the average risk is defined as
\begin{align*}
    B_{\pi}(\hat{\theta})=\E_{\theta\sim\pi,Y\sim P_\theta}l(\theta,\hat{\theta}).
\end{align*}
The Bayes risk is the minimum that the average risk can achieve, i.e.
\begin{align*}
    B_{\pi}^*=\inf_{\hat{\theta}}B_{\pi}(\hat{\theta}).
\end{align*}
\end{definition}

\begin{lemma}[{\cite[Lemma 1]{ACCD12}}]
\label{lm:Bayes_risk}
Consider the problem of testing hypothesis $H_{-1}:v \sim \mathbb{P}_{-1}$ and $H_1:v\sim \mathbb{P}_1$, where $H_{-1}$ and $H_1$ are occurred with prior probability $\pi_{-1}$ and $\pi_1$ respectively prior to the experiment. Under the 0-1 loss, the Bayes risk $B$ satisfies
\begin{align*}
    B\geq \min(\pi_{-1},\pi_1)(1-\|\mathbb{P}_1-\mathbb{P}_{-1}\|_{\mathrm{TV}}).
\end{align*}
\end{lemma}
\end{comment}

\begin{lemma}[{\cite[Lemma 1]{ACCD12}}]
\label{lm:Bayes_risk}
Consider the problem of testing hypothesis $H_{-1}:v \sim \mathbb{P}_{-1}$ and $H_1:v\sim \mathbb{P}_1$, where $H_{-1}$ and $H_1$ occur with prior probability $\pi_{-1}$ and $\pi_1\defeq1-\pi_{-1}$ respectively prior to the experiment. 
For any algorithm that takes one sample $v$ and outputs $\hat{i}:v\rightarrow\{-1,1\}$, we define the Bayes risk $B$ be the minimum average probability that algorithm fails ($v$ is not sampled from $H_{\hat{i}(v)}$).
That is $B=\inf_{\hat{i}}\pi_{-1}\Pr[\hat{i}(v)=1\mid v\sim \mathbb{P}_{-1}]+\pi_1\Pr[\hat{i}(v)=0\mid v\sim \mathbb{P}_1]$.
Then, we have
\begin{align*}
    B\geq \min(\pi_{-1},\pi_1)(1-\|\mathbb{P}_1-\mathbb{P}_{-1}\|_{\mathrm{TV}}).
\end{align*}
\end{lemma}

\begin{lemma}
\label{lm:error_binary_test}
Suppose that $v$ is uniformly sampled from $\cV=\{-1,1\}^d$, then any estimate $\hat{v}$
%outputted by algorithm which satisfies the Orthogonal Query assumption
obeys
\begin{align*}
    \E[d_H(\hv,v)]\geq
    \frac{d}{2}\lp 1-\frac{\delta\sqrt{k}}{\sigma\sqrt{d}}\rp.
\end{align*}
\end{lemma}

\begin{proof}
Let $\pi_{-1}=\pi_1=1/2$.
For each $j$, define $\mathbb{P}_{-1,j}=\bbP(Y^{[k]}\mid v_j=-1)$ and $\bbP_{1,j}=\bbP(Y^{[k]}\mid v_j=1)$ to be distributions over the observations $(Y^1,\cdots,Y^k)$ conditional on $v_j\neq 1$ and $v_j=1$ respectively.
Let $B_j$ be the Bayes risk of the decision problem for $j$-th coordinate of $v$ between $H_{-1,j}: v_j=-1$ and $H_{1,j}: v_j=1$.
We have that
\begin{align*}
    \E[d_H(\hv,v)]
    \geq& \sum_{j=1}^{d}B_j\\
    \geq& \pi_1\sum_{j=1}^{d}(1-\|\bbP_{1,j}-\bbP_{-1,j}\|_{\mathrm{TV}})\\
    \geq& \frac{d}{2}\lp 1-\frac{1}{\sqrt{d}}\sqrt{\sum_{j=1}^{d}\|\bbP_{1,j}-\bbP_{-1,j}\|^2_{\mathrm{TV}}}\rp,
\end{align*}
where the first inequality follows from the definition of Bayes risk $B_j$, the second inequality follows by Lemma~\ref{lm:Bayes_risk} and the last inequality follows by the Cauchy-Schwartz inequality.

To complete the proof, it suffices to show that
\begin{align}
\label{eq:bounded_TVD}
    \sum_{j=1}^{d}\|\bbP_{1,j}-\bbP_{-1,j}\|_{\mathrm{TV}}^2\leq \frac{\delta^2}{\sigma^2}k.
\end{align}

Assuming Equation~\eqref{eq:bounded_TVD} first, which will be established later.
Then we know that
\begin{align*}
    \E[d_H(\hv,v)]\geq \frac{d}{2}(1-\frac{\delta\sqrt{k}}{\sigma\sqrt{d}}).
\end{align*}
\end{proof}

We will complete the proof of Lemma~\ref{lm:error_binary_test} by showing the following bounded total variation distance.
\begin{claim}
\begin{align*}
    \sum_{j=1}^{d}\|\bbP_{1,j}-\bbP_{-1,j}\|_{\mathrm{TV}}^2\leq \frac{\delta^2}{\sigma^2}k.
\end{align*}
\end{claim}

\begin{proof}
Applying Pinsker's inequality, we know $\|\bbP_{1,j}-\bbP_{-1,j}\|_{\mathrm{TV}}^2\leq \frac{1}{2}\mathrm{D}_{KL}(\bbP_{-1,j}\|\bbP_{1,j})$.
To bound the KL divergence between $\bbP_{-1,j}$ and $\bbP_{1,j}$ over all possible $Y^{[k]}$, consider $v'=(v_1,\cdots,v_{j-1},v_{j+1},\cdots,v_d)$, and define $\bbP_{-1,j,v'}(Y^{[k]})\defeq \bbP(Y^{[k]}\mid v_j=-1,v')$ to be the distribution conditional on $v_j=-1$ and $v'$.
%\gnote{Use $\bbP_{-1,j,v'}$ notation, since it also depends on $j.$}
We have
\begin{align*}
    \bbP_{-1,j}(Y^{[k]})= \sum_{v'}\Pr[v']\bbP_{-1,j,v'}(Y^{[k]}).
\end{align*}

The convexity of the KL divergence suggests that
\begin{align*}
    \mathrm{D}_{KL}(\bbP_{-1,j}\|\bbP_{1,j})\leq \sum_{v'}\Pr[v']\mathrm{D}_{KL}(\bbP_{-1,j,v'}\|\bbP_{1,j,v'}).
\end{align*}
Fixing any possible $v'$, we want to bound the KL divergence $\mathrm{D}_{KL}(\bbP_{-1,j,v'}\|\bbP_{1,j,v'})$.

Recall we are considering deterministic algorithms and
$(X^t,S^t)$ is a deterministic function of $Y^{[t-1]}$.
Let $Q_i\in \R^{d\times k}$ be a (random) matrix, which records the set of points the algorithm queries for the user $s_i$.
Specifically, for $t$-th step, if the algorithm queries $(X^t,S^t)$, then $Q_{i}^{t}=X^t$ if $S^t=s_i$, otherwise $Q_{i}^{t}=0$, where $Q_i^t$ is the $t$-th column of $Q_i$.

As we are considering linear functions, without loss of generality we can assume  $\langle Q_{i}^j,Q_{i}^{j'}\rangle=0$ for each $i$ and any $j\neq j'$, and $\|Q_{i}^t\|_2\in\{0,1\}$ for any $i$ and $t$.
We name this assumption \textsc{Orthogonal Query}.
Roughly speaking, for any algorithm, we can modify it to satisfy the Orthogonal Query.
Whenever the algorithm wants to query some point, we can use Gram–Schmidt process to query another point and satisfy Orthogonal Query, and recover the function value at the original point queried by the algorithm.
% To establish this, observe that for any algorithm $\cA$, we can find another algorithm $\cA'$ which satisfies the Orthogonal Query assumption, and the distributions of the outputs of $\cA$ and $\cA'$ are the same almost everywhere.
% Give an example, suppose $j$ is the minimum integer that $\cA$ has re-queried information of some user, say $s_i$.
% That is $s^{t_1}\neq s^{t_2}$ for $t_1,t_2\leq j-1$ and $S^j=S^t=s_i$ for some $t\leq j-1$.
% Then we can construct $\cA'$ by taking $\cA$ as an black box: for the first $j-1$-th step, $\cA'$ queries $(X^t/\|X^t\|_2,S^t)$ for the first $j-1$ steps and inputs the values of $f(X^t/\|X^t\|_2,S^t)\cdot \|X^t\|_2$ to $\cA$.
% For $j$th step, $\cA'$ queries $(z';S^t)$ where $z'=\frac{x^j-\frac{\langle x^j,x^i\rangle }{\|x^i\|^2_2}\cdot x^i}{\|z-\frac{\langle x^j,x^i\rangle }{\|x^i\|^2_2}\cdot x^i\|_2}$, then recovers the value of $f(x^j;s^j)$ and inputs it to $\cA$ to observe what's the next query.
% $\cA'$ can repeat this procedure, and output whatever $\cA$ outputs finally.

By the chain-rule of KL-divergence, if we define $P_{-1,j,v'}(Y^t\mid Y^{[t-1]})$ to be the distribution of $t$th observation $Y^t$ conditional on $v'$, $v_j=-1$ and $Y^{[t-1]}$, then we have
\begin{align*}
    \mathrm{D}_{KL}(\bbP_{-1,j,v'}\|\bbP_{1,j,v'})=\sum_{t=1}^{k}\int_{\cY^{t-1}} \mathrm{D}_{KL}(P_{-1,j,v'}(Y^t\mid Y^{[t-1]}=y)\|P_{1,j,v'}(Y^t\mid Y^{[t-1]}=y)\d P_{-1,j,v'}(y).
\end{align*}

Fix $Y^{[t-1]}$ such that $Y^{[t-1]}=y$.
Since the algorithm is deterministic and $(X^t,S^t)$ is fixed given $Y^{[t-1]}$.
Let $S^t=s_i$ so $X^t=Q_i^t$.
%Denote the choice of the algorithm is $(Q_{i}^t,s_i)$ for $t$-th step conditional on $Y^{[t-1]}=y$.

Note that the $n$ users in $\cD$ are i.i.d. sampled.
Then $\mathrm{D}_{KL}(P_{-1,j,v'}(Y^t\mid Y^{[t-1]}=y)\|P_{1,j,v'}(Y^t\mid Y^{[t-1]}=y)$ only depends on the randomness of $s_i$ and the first $t$ columns of $Q_{i}$, which is denoted by $Q_{i}^{[t]}$.
We use $Y^{t}_{j}$ to denote the observation corresponding to user $s_j$ for the $t$th query (if $S^t\neq s_j$, we have $Y^{t}_j=0$).
Note that the observation $Y^{[t]}_i=Q_i^{[t]\top} s_i$ where $s_i\sim \cN(\delta v,\sigma^2 I_d)$. Then we know $Y^{[t]}_i$ is normally distributed with mean $\delta Q_{i}^{[t]\top} v$ and co-variance $\sigma^2 Q_{i}^{[t]\top} Q_{i}^{[t]}$.

Recall that the KL divergence between two normal distributions is $\mathrm{D}_{KL}(\cN(\mu_1,\Sigma)\|\cN(\mu_2,\Sigma))=\frac{1}{2}(\mu_1-\mu_2)^{\top}\Sigma^{-1}(\mu_1-\mu_2)$. Recall that we have the Orthogonal Query assumption and thus $Q_{i}^{[t]\top} Q_{i}^{[t]}\in\{0,1\}^{t\times t}$ is a diagonal matrix.
By the conditional distributions of Gaussian, we know $Y^t_i$ only depends on the $Q_{i}^t$ and it is independent of $Q_{i}^{[t-1]}$.

Hence we have
\begin{align*}
    &\mathrm{D}_{KL}(P_{-1,j,v'}(Y^t\mid Y^{[t-1]}=y)\|P_{1,j,v'}(Y^t\mid Y^{[t-1]}=y))\\
    =&\mathrm{D}_{KL}(P_{-1,j,v'}(Y^t_i\mid Y^{[t-1]}=y)\|P_{1,j,v'}(Y^t_i\mid Y^{[t-1]}=y))\\
    =&\frac{1}{2} (2\delta Q_{i}^t(j))^2/\sigma^2,
\end{align*}
where $Q_{i}^t(j)$ is the $j$-th coordinate of $Q_{i}^t$.
Summing over the terms, one has
\begin{align*}
\sum_{j=1}^{d}\|\bbP_{1,j}-\bbP_{-1,j}\|_{\mathrm{TV}}^2\leq &
    \frac{1}{2}\mathrm{D}_{KL}(\bbP_{-1,j}\|\bbP_{1,j})\\
    \leq&\frac{1}{2} \sum_{t=1}^{k}\sum_{j=1}^{d}\sum_{i=1}^{n}\E[\frac{1}{2} (2\delta Q_{i}^t(j))^2/\sigma^2] \\
    \leq& \frac{\delta^2}{\sigma^2}k,
\end{align*}
where the last line follows from the fact that for each $t$,$ \sum_{i=1}^{n}\|Q_{i}^t\|_2^2=\sum_{i=1}^{n}\sum_{j=1}^{d}(Q_{i}^t(j))^2=1$ as we only query one user for $t$-th step.

This completes the proof.
\end{proof}

Having Lemma~\ref{lm:error_binary_test}, we can complete the proof of Theorem~\ref{thm:info_bound}.

\begin{proof}{of Theorem~\ref{thm:info_bound}.}
As discussed before, we know
\begin{align*}
    \HF_v(\hat{x})-\HF_v(x^v)\geq\frac{\delta}{2\sqrt{d}}\sum_{j=1}^{d}\indicator\{\sign(\hat{x}_j)\neq \sign(x^v_j) \},
\end{align*}
and hence we know that
\begin{align*}
     \eps_k^*(\cP_G,\cK)\geq & \frac{\delta}{2\sqrt{d}}\inf_{\hat{v}}\E[d_H(\hat{v},v)]\\
    \geq & \frac{\delta \sqrt{d}}{4}\lp 1-\frac{\delta\sqrt{k}}{\sigma\sqrt{d}}\rp,
\end{align*}
where the last line follows from Lemma~\ref{lm:error_binary_test}.
%As we assume $\cK$ is the unit ball and thus $D=2$.
We now set $\delta=\frac{\sigma \sqrt{d}}{2\sqrt{k}}$ and $\sigma=\frac{G}{\sqrt{d+d^2/4k}}$, so that $d(\sigma^2+\delta^2)=G^2$.
Hence one has
\begin{align*}
 \eps_k^*(\cP_G,\cK)
\geq \frac{\delta\sqrt{d}}{8} 
= \frac{D\delta\sqrt{d}}{16}
= \frac{GD}{16\sqrt{1+\frac{4k}{d}}}
\ge \frac{GD}{16}\min\left\{1,\sqrt{\frac{d}{4k}}\right\}.
%\ge GD\frac{\min\{\sqrt{d},\sqrt{k}\}}{40\sqrt{k}}.
\end{align*}
%\gnote{Check the last line or modify the lower bound to $ \frac{GD}{16}\min\{1,\sqrt{\frac{d}{4k}}\}$ everywhere.}
Thus we complete the proof.
\end{proof}

\begin{corollary}[Lower bound for DP-SCO]
\label{cor:DPSCOlower}
For any (non-private) algorithm which makes less than $O\lp\min\{\frac{\eps^2n^2}{\log(1/\delta)},nd\}\rp$ function value queries, there exist a convex domain $\cK\subset \R^d$ of diameter $D$, a distribution $\cP$ supported on $G$-Lipschitz linear functions $f(x;s)\defeq\langle x,s\rangle$, such that the output $\hx$ of the algorithm satisfies that
\begin{align*}
    \E_{s\sim\cP}[\langle \hx,s\rangle]-\min_{x\in\cK}\E_{s\sim\cP}[\langle x,s\rangle]\geq \Omega\lp \frac{G D}{\sqrt{1+\log(n)/d}} \cdot \min\lc\frac{\sqrt{\log(1/\delta)d}}{\eps n}+\frac{1}{\sqrt{n}},1\rc \rp.
\end{align*}
\end{corollary}
\begin{proof}
% WLOG, we can assume that $\frac{\sqrt{\log(1/\delta)d}}{\eps n}+\frac{1}{\sqrt{n}}=O(1)$. This is a reasonable assumption as if $\frac{\sqrt{\log(1/\delta)d}}{\eps n}+\frac{1}{\sqrt{n}}=\Omega(1)$, uniformly randomly output a point in $\cK$ is a good DP solution.

Note that Theorem~\ref{thm:info_bound} almost gives us what we want, except that the Lipschitz constant of the functions in the hard distribution is bounded only on average by $G$. To get distributions over $G$-Lipschitz functions, we just condition on the bad event not happening.

Recall that we are considering the set of distributions $\cN_v=\cN(\delta v,\sigma^2 I_d)$ for which $\E_{s\sim\cN_v}\|s\|_2^2\le G^2=d(\delta^2+\sigma^2)$.
And we proved that 
$\inf_{\cA\in \mathbb{A}_k}\sup_{v\in \cV}\E_{s\sim \cN_v,\cA}[\HF_v(\hat{x}_k)-\HF_v^*]\ge \frac{GD}{16}\min\left\{1,\sqrt{\frac{d}{4k}}\right\}$ in Theorem~\ref{thm:info_bound}, where $\hat{x}_k$ is the output of $\cA$ with $k$ observations $Y^{[k]}$.
%\gnote{What is $f,\cF_G,\hat{x}_k$ in the above equation?}
To prove Corollary~\ref{cor:DPSCOlower}, we need to modify the distribution of $s$ to satisfy the Lipschitz continuity.

In particularly, for some constant $c$, we know 
\begin{align*}
    &\E[\HF_v(\hat{x}_k)-\HF_v^*]\\
    =&\E\Big[ \HF_v(\hat{x}_k)-\HF_v^*\mid \max_{s_i\in \cD}\|s_i\|_2\leq cG\sqrt{1+\log(nd)/d}\Big]\Pr\Big[\max_{s_i\in \cD}\|s_i\|_2\leq cG \sqrt{1+\log(nd)/d}\Big]+\\
    &~~\E\Big[ \HF_v(\hat{x}_k)-\HF_v^*\mid \max_{s_i\in \cD}\|s_i\|_2> cG\sqrt{1+\log(nd)/d}\Big]\Pr\Big[\max_{s_i\in \cD}\|s_i\|_2> cG\sqrt{1+\log(nd)/d}\Big].
\end{align*}
By the concentration of spherical Gaussians, we know if $s\sim\cN(\delta v,\sigma^2 I_d)$, then 
\begin{align*}
    \Pr\Big[\|s-\delta v\|_2^2\leq \sigma^2 d(1+2\sqrt{\ln(1/\eta)/d}+2\ln(1/\eta)/d)\Big]\geq 1-\eta.
\end{align*}
% \Gopi{Do we really need to lose $\log(nd)$ factor here? looks like a constant is enough...}
% \Daogao{We want $\eta\leq 1/\poly(nd)$. I use log in case that $n\gg d$. maybe I just modify it.}
We can choose the constant $c$ large enough, such that $\Pr[\max_{s_i\in \cD}\|s_i\|_2\leq cG\sqrt{1+\log(nd)/d}]\geq 1-1/\poly(nd)$, which implies
\begin{align*}
    \inf_{\cA\in \mathbb{A}_k}\sup_{v\in \cV}\E_{\cD \sim \cN_v^n,\cA}\Big[\HF_v(\hat{x}_k)-\HF_v^*\mid \max_{s_i\in \cD}\|s_i\|_2\leq cG\sqrt{1+\log(nd)/d}\Big]\geq \Omega(GD\frac{\min\{\sqrt{d},\sqrt{k}\}}{\sqrt{k}}).
\end{align*}
If we use the distributions conditioned on $\max_{s_i\in \cD}\|s_i\|_2\leq cG\sqrt{1+\log(nd)/d}$ rather than the Gaussians, and scale the constant to satisfy the assumption on Lipschitz continuity, we can prove the statement.
Particularly, let $G'=cG(\sqrt{1+\log(nd)/d})$. 
If the algorithm can only make $k=O\lp\min\{\frac{\eps^2n^2}{\log(1/\delta)},nd\}\rp$ observations, we know 
\begin{align*}
    &\inf_{\cA\in \mathbb{A}_k}\sup_{v\in \cV}\E_{\cD \sim \cN_v^n,\cA}\Big[\HF_v(\hat{x}_k)-\HF_v^*\mid \max_{s_i\in \cD}\|s_i\|_2\leq G'\Big]\\
    \geq&
    \Omega\lp GD\cdot \min\lc(\frac{\sqrt{\log(1/\delta)d}}{\eps n}+\frac{1}{\sqrt{n}}),1\rc \rp  \\
    =&\Omega\lp \frac{G' D}{\sqrt{1+\log(nd)/d}}\cdot \min\lc\frac{\sqrt{\log(1/\delta)d}}{\eps n}+\frac{1}{\sqrt{n}},1\rc  \rp,
\end{align*}
% \Gopi{There is a gap in the proof. We don't get the required result exactly.}
which proves the lower bound claimed in the Corollary statement.
\end{proof}

\begin{corollary}[Lower bound for sampling scheme]
\label{cor:Samplinglower}
Given any $G > 0$ and $\mu > 0$. For any algorithm which takes function values queries less than $O\lp\frac{G^2}{\mu}/(1+\log(G^2/\mu)/d)\rp$ times, there is a family of $G$-Lipschitz linear functions $\{f_i(x)\}_{i\in I}$ defined on some $\ell_2$ ball $\cK\subset\R^d$, such that the total variation distance between the distribution of the output of the algorithm and the distribution proportional to $\exp(-\E_{i\in I}f_i(x)-\mu \|x\|^2 / 2)$ is at least $\min(1/2, \sqrt{d \mu / G^2})$.
\end{corollary}
\begin{proof}
By a similar argument in the proof of Corollary~\ref{cor:DPSCOlower}, for any algorithm which can only make $k$ observations, there are a family of $G$-Lipschitz linear functions restricted on an $\ell_2$ ball $\cK$ of diameter $D$ centered at $\mathbf{0}$ such that  
\begin{align}
\label{eq:lower_sampling_SCO}
    \E\Big[\HF_v(\hat{x}_k)-\HF_v^*\Big]
    \ge&\Omega\lp \frac{G D}{\sqrt{1+\log(k)/d}} \cdot \min\lc \sqrt{\frac{d}{k}},1\rc \rp,
\end{align}
where $\HF_v^*=\min_{x\in\cK}\HF_v(x)$ and $\hx_k\in\cK$ is the output of $\cA$.

%Without loss of generality, we can shift the functions and assume the zero point $\mathbf{0}$ is the center of $\cK$.
Suppose we have a sampling algorithm that takes $k$ queries. We use it to sample from $x^{(sol)}$ proportional to $p(x):=\exp(-\HF_v(x)-\frac{\mu}{2} \|x\|^2)$ on $\cK$ with total variation distance $\eta\leq \min(1/2, \sqrt{d \mu / G^2})$. 
% Note that $\HF_v(x)$ is some $G$-Lipschitz linear function, and by Lemma~\ref{lm:utility_tech}, we know $\E[\|x^{(sol)}\|]\leq O(\sqrt{d/\mu})$.
% To ensure $x^{(sol)}$ is bounded, we can define the bounded variant $\overline{x^{(sol)}} = x^{(sol)}$ if $\|x^{(sol)}\|^2 \leq O(d \log(1/\eta) / \mu)$ and $\mathbf{0}$ otherwise. Note that $p$ is $\mu$-strongly convex, we have $\|x\|^2 \leq O(d \log(1/\eta)/ \mu)$ with probability $1-\eta$ by Lemma~\ref{lem:gaussian_concentration} and hence $\overline{x^{(sol)}}$ has total variation distance $2 \eta$ from $p$.

Lemma \ref{lm:utility_tech} shows that
\begin{align*}
    \E[\HF_{v}(x^{(sol)})+\frac{\mu}{2}\|x^{(sol)}\|^{2}]\leq\min_{x\in \cK}\lp\HF_{v}(x)+\frac{\mu}{2}\|x\|^{2}\rp+O(d) + O(\eta) \cdot (GD+\mu D^2),
\end{align*}
where the last term involving $\eta$ is due to the total variation distance between $x^{(sol)}$ and $p$. Setting $D=\sqrt{d/\mu}$ and using the diameter of $\cK$ is $D$ and $\eta \leq \min(1/2, \sqrt{d \mu / G^2})$, we have
\begin{align*}
\E[\HF_{v}(x^{(sol)})] & \leq\min_{x\in\cK}\HF_{v}(x)+\frac{\mu}{2}D^{2}+O(d+\eta\cdot (GD+\mu D^2))\\
& \leq \min_{x\in\cK}\HF_{v}(x)+O(d).
\end{align*}
Note that we set $D = \sqrt{d/\mu}$. Comparing with \eqref{eq:lower_sampling_SCO}, we have
\[
\frac{G\sqrt{d/\mu}}{\sqrt{1+\log(k)/d}}\min\left\{ \sqrt{\frac{d}{k}},1\right\} \leq O(d).
\]

If $d\leq G^{2}/\mu\leq \exp(d)$, we have
\[
G\sqrt{d/\mu}\sqrt{\frac{d}{k}}\leq O(d)
\]
and hence $k=\Omega(G^{2}/\mu)$.
If $G^{2}/\mu\geq\exp(d)$,
we have
\[
\frac{G\sqrt{d/\mu}}{\sqrt{\log(k)/d}}\sqrt{\frac{d}{k}}\leq O(d)
\]
and hence $k=\Omega(\frac{G^{2}d/\mu}{\log(G^{2}/\mu)})$.
If $G^{2}/\mu\leq d$, we can construct our function only on the first
$O(G^{2}/\mu)$ dimensions to get a lower bound $k=\Omega(G^{2}/\mu).$
Combining all cases gives the result.
\end{proof}


% \begin{corollary}[Lower bound for sampling scheme]
% \label{cor:Samplinglower}
% Given any $G > 0$ and $\mu > 0$. For any algorithm which takes function values queries less than $O\lp\frac{G^2}{\mu}/(1+\log(G^2/\mu)/d)\rp$ times, there is a family of $G$-Lipschitz linear functions $\{f_i(x)\}_{i\in I}$, such that the total variation distance between the distribution of the output of the algorithm and the distribution proportional to $\exp(-\E_{i\in I}f_i(x)-\mu \|x\|^2 / 2)$ is at least $\mu/G^2$.
% \end{corollary}
% \begin{proof}
% By a similar argument in the proof of Corollary~\ref{cor:DPSCOlower}, for any algorithm which can only make $k$ observations, there are a family of $G$-Lipschitz linear functions restricted on an $\ell_2$ ball $\cK$ of diameter $D$ such that  
% \begin{align}
% \label{eq:lower_sampling_SCO}
%     \E\Big[\HF_v(\hat{x}_k)-\HF_v^*\Big]
%     \ge&\Omega\lp \frac{G D}{\sqrt{1+\log(k)/d}} \cdot \min\lc \sqrt{\frac{d}{k}},1\rc \rp,
% \end{align}
% where $\HF_v^*=\min_{x\in\cK}\HF_v(x)$ and $\hx_k\in\cK$ is the output of $\cA$.

% Suppose we have a sampling algorithm that takes $k$ queries. We use it to sample from $x^{(sol)}$ proportional to $p(x):=\exp(-\HF_v(x)-\frac{\mu}{2} \|x\|^2)$ with total variation distance $\eta$. 
% Without loss of generality, we can shift the functions and assume the zero point $\mathbf{0}\in\cK$ and minimizes $\HF_v(x)+\frac{\mu}{2} \|x\|^2$.
% Note that $\HF_v(x)$ is some $G$-Lipschitz linear function, and by Lemma~\ref{lm:utility_tech}, we know $\E[\|x^{(sol)}\|]\leq O(\sqrt{d/\mu})$.
% To ensure $x^{(sol)}$ is bounded, we can define the bounded variant $\overline{x^{(sol)}} = x^{(sol)}$ if $\|x^{(sol)}\|^2 \leq O(d \log(1/\eta) / \mu)$ and $\mathbf{0}$ otherwise. Note that $p$ is $\mu$-strongly convex, we have $\|x\|^2 \leq O(d \log(1/\eta)/ \mu)$ with probability $1-\eta$ by Lemma~\ref{lem:gaussian_concentration}
% and hence $\overline{x^{(sol)}}$ has total variation distance $2 \eta$ from $p$.

% Lemma \ref{lm:utility_tech} shows that
% \begin{align*}
%     \E[\HF_{v}(\overline{x^{(sol)}})+\frac{\mu}{2}\|\overline{x^{(sol)}}\|^{2}]\leq\min_{x\in \R^d}\lp\HF_{v}(x)+\frac{\mu}{2}\|x\|^{2}\rp+O(d) + O(\eta) \cdot (G \sqrt{d \log(1/\eta) / \mu} + d \log(1/\eta))
% \end{align*}
% where the last term involving $\eta$ is due to the total variation distance between $\overline{x^{(sol)}}$ and $p$. Optimizing on the right hand side on $\cK$ gets $\overline{x^{(sol)}_{\cK}}$ and using the diameter of $\cK$ is $D$, we have \Daogao{Seems problematic, as we do not know how to get a solution in $\cK$?}
% \begin{align*}
% \E[\HF_{v}(\overline{x^{(sol)}_{\cK}})] & \leq\min_{x\in\cK}\HF_{v}(x)+\frac{\mu}{2}D^{2}+O(d+\eta\cdot G\sqrt{d\log(1/\eta)/\mu})\\
%  & \leq\min_{x\in\cK}\HF_{v}(x)+\frac{\mu}{2}D^{2}+O(d+G\sqrt{d\eta/\mu})\\
%  & \leq\min_{x\in\cK}\HF_{v}(x)+\frac{\mu}{2}D^{2}+O(d)
% \end{align*}
% \Gopi{Isn't it enough to have $\eta G \sqrt{d\log(1/\eta)/\mu}\le d$ i.e. $\eta \le \Tilde{O}( \frac{\sqrt{\mu d}}{G}).$}
% where we used $\eta \leq \mu / G^2$ at the end. Putting $D = \sqrt{d/\mu}$ and comparing with \eqref{eq:lower_sampling_SCO}, we have
% \[
% \frac{G\sqrt{d/\mu}}{\sqrt{1+\log(k)/d}}\min\left\{ \sqrt{\frac{d}{k}},1\right\} \leq O(d).
% \]

% If $d\leq G^{2}/\mu\leq\exp(d)$, we have
% \[
% G\sqrt{d/\mu}\sqrt{\frac{d}{k}}\leq O(d)
% \]
% and hence $k=\Omega(G^{2}/\mu).$ If $G^{2}/\mu\geq\exp(d)$,
% we have
% \[
% \frac{G\sqrt{d/\mu}}{\sqrt{\log(k)/d}}\sqrt{\frac{d}{k}}\leq O(d)
% \]
% and hence $k=\Omega(\frac{G^{2}d/\mu}{\log(G^{2}/\mu)})$.
% If $G^{2}/\mu\leq d$, we can construct our function only on the first
% $O(\log(G^{2}/\mu))$ dimensions to get a lower bound $k=\Omega(G^{2}/\mu).$
% Combining all cases gives the result.
% \end{proof}




%Picking $\mu = c^2 d / D^2$ for some $c \geq 1$ to be determined, we have
%\begin{align*}
%\E\HF_{v}(x^{(sol)})	\leq \HF_v^* +O(c^{2}d+\frac{\eta}{\sqrt{c}}GD).
%\end{align*}

%we can design a regularized mechanism by sampling from $x^{(sol)}$ proportional to $\exp(-c(F(\cdot;\cD)+\psi(\cdot)))$ where  $\psi(x)=\frac{\mu}{2}\|x\|_2^2$ is a regularization term and $c=O(\mu nd/G^2)$.
%By Theorem~\ref{thm:generalization_error} and our guarantee on the total variation distance $\gamma<\eta$ between our final output and the distribution proportional to $\exp(-c(F(\cdot;\cD)+\psi(\cdot)))$, we know that for any $v\in \cV$ one has
%\begin{align*}
%    \E[\HF_v(x^{(sol)})-\min_{x\in\cK}\HF_v(x)]\leq O\lp GD(\frac{1}{\sqrt{n}}+\eta)\rp.
%\end{align*}
%Then if we choose $n=\Theta(1/\eta^2)$, we can get expected excess population loss $O(GD/\sqrt{n})$ with $O(nd/\log(nd))$ queries. If we set the constants hidden carefully, we can break Equation~\eqref{eq:lower_sampling_SCO}, which is contradiction.
%So the corollary holds and we complete the proof.

% Corollary~\ref{cor:Samplinglower} can be deduced directly by combining the Corollary~\ref{cor:DPSCOlower} and our results (Theorem~\ref{thm:generalization_error}) about generalization error of exponential mechanism.

% Particularly, 
% If we have such a scheme that works for all $G$-Lipschitz linear functions,
% we can try to sample $x^{(sol)}$ proportional to $\exp(-k(F(\cdot;\cD)+\psi(\cdot)))$ where $k=O(\frac{\mu}{G^2}\min\{\frac{\eps^2n^2}{\log(1/\delta)},nd \})$ with $O(\min\{\frac{\eps^2n^2}{\log(1/\delta)},nd \}/\log(nd))$ queries.
% By Theorem~\ref{thm:generalization_error} and our guarantee on the total variation distance $\gamma<\eta$ between our final output and the distribution proportional to $\exp(-k(F(\cdot;\cD)+\psi(\cdot)))$, we know that
% \begin{align*}
%     \E[\HF(x^{(sol)})-\min_{x\in\cK}\HF(x)]\leq O\lp GD(\frac{\sqrt{\log(1/\delta)d}}{\eps n}+\frac{1}{\sqrt{n}}+\eta)\rp.
% \end{align*}
% Then if we choose $n=\Theta(1/\eta^2)$, we can get expected excess population loss $O(GD(\frac{\sqrt{\log(1/\delta)d}}{\eps n}+\frac{1}{\sqrt{n}}))$ with $O(\min\{\frac{\eps^2n^2}{\log(1/\delta)},nd \}/\log(nd))$ queries..
% If we set the constants hidden carefully and can break corollary~\ref{cor:DPSCOlower}, which is contradiction.
% \gnote{Why are $\epsilon,\delta$ appearing here? Write the proof without using $\epsilon,\delta$} 

%Corollary~\ref{cor:DPSCOlower} can be deduced from the same proof of Theorem~\ref{thm:info_bound} and take care of the Lipschitz condition of the linear functions, and


%To address the first difference, for each step $t$, we can consider the $\cA$ which can choose $d$ points $(x_1^t,\cdots,x_d^t)$ to be the $(e_1,\cdots,e_d)$ where $e_j$ is the vector of size $d$ containing all zeros except for a 1 in the $j$-th position, and thus can recover the $S^t$.
%Thus our algorithm does not have any advantage over $\cA$.The second difference can be addressed by using an alternative argument with a truncated Gaussian.Thus we know the lower bound in \cite{DJWW15} can be extended to our case.With queries $\min\{n^2,nd\}$, the lower bound should be $\Omega(GD(\frac{1}{\sqrt{n}}+\frac{\sqrt{d}}{n}))$ by choosing $m=\min\{n,d\}$ and $k=n$.