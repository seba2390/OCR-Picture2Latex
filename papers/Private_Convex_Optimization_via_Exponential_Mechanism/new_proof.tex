\section{GDP of Regularized Exponential Mechanism}
In this section, we prove our DP result (Theorem~\ref{thm:privacy_technical}). 
The proof uses the isoperimetric inequality for strongly log-concave measures~\cite{Led99}. Intuitively, the privacy loss random variable will be $G$-Lipschitz under the hypothesis and isoperimetric inequality implies that any Lipschitz function will be as concentrated as a Gaussian with appropriate standard deviation. This allows us compare the privacy curve $\deltacurve{P}{Q}$ to that of a Gaussian mechanism. In our proof, it is actually more convenient to compare tradeoff curves ($\tradeoff{P}{Q}$) which are equivalent to privacy curves via convex duality (Proposition~\ref{prop:delta_tradeoff} and Theorem~\ref{thm:privacy_technical}).

%By Proposition~\ref{prop:delta_tradeoff}, Theorem~\ref{thm:privacy_technical} is equivalent to the following theorem.
%The proof is based on the Isoperemetric inequality, which presents the concentration property of strongly log-concave measure and the worst case is exactly the Gaussian measure.
%Thus the privacy curve can be bounded by the one of Gaussian mechanism.

\begin{theorem}
\label{thm:privacy_technical_tradeoff}
Given convex set $\cK\subseteq \R^d$ and $\mu$-strongly convex functions $F,\Tilde{F}$ over $\cK$. Let $P,Q$ be distributions over $\cK$ such that $P(x)\propto e^{-F(x)}$ and $Q(x)\propto e^{-\Tilde{F}(x)}$.
If $\Tilde{F}-F$ is $G$-Lipschitz over $\cK$, then for all $z\in[0,1]$,
\begin{align*}
    \tradeoff{P}{Q}(z) 
    \ge \tradeoff{\cN\lp 0,1\rp}{\cN\lp\frac{G}{\sqrt{\mu}},1\rp}(z).
\end{align*}
\end{theorem}

% \begin{proof}
% Let $\alpha(x)=\Tilde{F}(x)-F(x)+C$ for some constant $C\in \R$ so that $Q(x)=e^{-\alpha(x)} P(x)$. We have $T(P\|Q)(z)=\inf_{S:P(S)=z} Q(S)$. Note that the infimum is achieved when we choose 
% $$S=\lc x:\log\lp \frac{Q(x)}{P(x)}\rp = -\alpha(x) \le -m(z)\rc$$ for some $m(z)$ chosen such that 
% $$P(S)=\Pr_{x\sim P}[\alpha(x)\ge m(z)]=z.$$ Therefore:
% \begin{align*}
% T(P\|Q)(z)&= \int_{x: \alpha(x)\ge m(z)} Q(x) dx\\
% &= \int_{x: \alpha(x)\ge m(z)} e^{-\alpha(x)} P(x) dx\\
% &= \int_{t=m(z)}^\infty e^{-t} \lp -\frac{d\Pr_{x\sim P}\lb\alpha(x)\ge t\rb}{dt}\rp dx\\
% &=  \left.-e^{-t}\Pr_{x\sim P}\lb\alpha(x)\ge t\rb\right\vert_{m(z)}^\infty - \int_{t=m(z)}^\infty e^{-t} \Pr_{x\sim P}\lb\alpha(x)\ge t\rb dx\\
% &=  ze^{-m(z)} - e^{-m(z)}\int_{t=0}^\infty e^{-t} \Pr_{x\sim P}\lb\alpha(x)\ge t+m(z)\rb dx\\
% \end{align*}
% By isoperimetric inequality\Gopi{Refer to the theorem}, we have
% \begin{align*}
% \Pr_{x\sim P}\lb\alpha(x)\ge t+m(z)\rb &\le \Pr[N(0,1)\ge t+\Phi^{-1}(1-z)]\\
% &=1-\Phi\lp t+ \Phi^{-1}(1-z)\rp\\
% &=\Phi(\Phi^{-1}(z)-t) \tag{$\Phi^{-1}(1-z)=-\Phi^{-1}(z)$ and $\Phi(-x)+\Phi(x)=1$}.
% \end{align*}
% Plugging this into the expression for $T(P\|Q)$, we get:
% \begin{align*}
%     T(P\|Q)&\ge ze^{-m(z)} - e^{-m(z)}\int_{t=0}^\infty e^{-t} \Phi(\Phi^{-1}(z)-t) dx\\
%     &= ze^{-m(z)} - e^{-m(z)}\lp z - \exp\lp\frac{1}{2}-\Phi^{-1}(z)\rp \Phi(\Phi^{-1}(z)-1) \rp \tag{\Gopi{Integrate by parts}}\\
%     &= \exp\lp\frac{1}{2}-\Phi^{-1}(z)-m(z)\rp \Phi(\Phi^{-1}(z)-1)\\
% \end{align*}

% \end{proof}

\begin{proof}
% WLOG, we can assume $G=\mu=1$.
% One can easily bring in specific parameters with the same arguments.
% \Gopi{Explain why}
Let $\gamma=G/\sqrt{\mu}.$
Let $\alpha(x)=\Tilde{F}(x)-F(x)$ so that $Q(x)\propto e^{-\alpha(x)} P(x)$. Recall that we have $T(P\|Q)(z)=\inf_{S:P(S)=1-z} Q(S)$. Note that the infimum is achieved when we choose 
$S=\lc x\in\cK: \alpha(x) \ge m(z)\rc$ for some $m(z)$ chosen such that 
$P(S)=\Pr_{x\sim P}[\alpha(x)\ge m(z)]=1-z$ (Neyman-Pearson lemma). Therefore:
\begin{align*}
T(P\|Q)(z)&= \int_{x\in S} Q(x) \d x\\
&= \frac{\int_{x\in S} e^{-\alpha(x)} P(x) \d x}{\int_{x\in\cK} e^{-\alpha(x)} P(x) \d x}\\
%&= \frac{\E_P[e^{-\alpha}\Ind_S]}{\E_P[e^{-\alpha}\Ind_S] + \E_P[e^{-\alpha}\Ind_{\barS}]}\\
&= \lp 1+\frac{\E_P[e^{-\alpha}\Ind_{\barS}]}{\E_P[e^{-\alpha}\Ind_S]}\rp^{-1}
\end{align*}
% \begin{align*}
%     T(P\|Q)(z)&=\lp 1+\frac{\E_P[e^{-\alpha}\Ind_{\barS}]}{\E_P[e^{-\alpha}\Ind_S]}\rp^{-1}.
% \end{align*}
We will now lower bound $\E_P[e^{-\alpha}\Ind_S]$. Let the random variable $Y=\alpha(x)$ where $x\sim P.$ Let $f_Y(\cdot)$ be the PDF of $Y$.
\begin{align*}
\E_P[e^{-\alpha(x)}\Ind_S]&=\int_{x: \alpha(x)\ge m(z)}e^{-\alpha(x)}P(x) \d x=\E[e^{-Y}\Ind(Y\ge m(z))]=\int_{m(z)}^\infty e^{-t} f_Y(t) dt\\
&= \int_{t=0}^\infty e^{-t-m(z)} \lp -\frac{\d\Pr_{x\sim P}\lb\alpha(x)\ge t+m(z)\rb}{\d t}\rp \d t\\
&=  e^{-m(z)}\lp \left.-e^{-t}\Pr_{x\sim P}\lb\alpha(x)\ge t+m(z)\rb\right\vert_{0}^\infty - \int_{t=0}^\infty e^{-t} \Pr_{x\sim P}\lb\alpha(x)\ge t+m(z)\rb \d t\rp\\
&=  (1-z)e^{-m(z)} - e^{-m(z)}\int_{t=0}^\infty e^{-t} \Pr_{x\sim P}\lb\alpha(x)\ge t+m(z)\rb \d t\\
&\ge (1-z)e^{-m(z)} - e^{-m(z)}\int_{t=0}^\infty e^{-t} \Phi(\Phi^{-1}(1-z)-t/\gamma) \d t \tag{Corollary~\ref{cor:isoperimetry_lip}}\\
&= (1-z)e^{-m(z)} - e^{-m(z)}\lp (1-z) - \exp\lp\frac{\gamma^2}{2}-\Phi^{-1}(1-z)\gamma\rp \Phi(\Phi^{-1}(1-z)-\gamma) \rp \tag{Claim \ref{claim:some_gaussian_integrals}}\\
&= \exp\lp\frac{\gamma^2}{2}+\Phi^{-1}(z)\gamma-m(z)\rp \Phi(-\Phi^{-1}(z)-\gamma)
\end{align*}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



We will now upper bound $\E_P[e^{-\alpha}\Ind_{\barS}]$ in a similar way.
\begin{align*}
\E_P[e^{-\alpha(x)}\Ind_{\barS}]&=\int_{x: \alpha(x)\le m(z)}e^{-\alpha(x)}P(x) \d x\\
&= \int_{t=0}^{\infty} e^{-m(z)+t} \lp -\frac{d\Pr_{x\sim P}\lb\alpha(x)\le m(z)-t\rb}{dt}\rp \d t\\
&=  e^{-m(z)}\lp\left.-e^{t}\Pr_{x\sim P}\lb\alpha(x)\le m(z)-t\rb\right\vert_{0}^\infty + \int_{t=0}^\infty e^{t} \Pr_{x\sim P}\lb\alpha(x)\le m(z)-t\rb \d t \rp\\
&=  ze^{-m(z)} + e^{-m(z)}\int_{t=0}^\infty e^{t} \Pr_{x\sim P}\lb\alpha(x)\le m(z)-t\rb \d t\\
&\le ze^{-m(z)} + e^{-m(z)}\int_{t=0}^\infty e^{t} \Phi(\Phi^{-1}(z)-t/\gamma) \d t \tag{Corollary~\ref{cor:isoperimetry_lip}}\\
    &= ze^{-m(z)} + e^{-m(z)}\lp -z + \exp\lp\frac{\gamma^2}{2}+\Phi^{-1}(z)\gamma\rp \Phi(\Phi^{-1}(z)+\gamma) \rp \tag{Claim~\ref{claim:some_gaussian_integrals}}\\
    &= \exp\lp\frac{\gamma^2}{2}+\Phi^{-1}(z)\gamma-m(z)\rp \Phi(\Phi^{-1}(z)+\gamma)
\end{align*}


Combining the two bounds, we get:
\begin{align*}
T(P\|Q)(z)&= \lp 1+\frac{\E_P[e^{-\alpha}\Ind_{\barS}]}{\E_P[e^{-\alpha}\Ind_S]}\rp^{-1}\\
&\ge \lp 1 + \frac{\Phi(\Phi^{-1}(z)+\gamma)}{\Phi(-\Phi^{-1}(z)-\gamma)} \rp^{-1}\\
&= \Phi(-\Phi^{-1}(z)-\gamma) \tag{Using $\Phi(x)+\Phi(-x)=1$}\\
&= \tradeoff{N(0,1)}{N(\gamma,1)} \tag{Eqn (\ref{eqn:gaussian_tradeoff})}.
\end{align*}
\end{proof}

We finish by calculating the integrals that showed up in the proof.
\begin{claim}
\label{claim:some_gaussian_integrals}
$$\int_0^\infty e^{-t} \Phi\lp a-\frac{t}{\gamma}\rp \d t = \Phi(a)-e^{\frac{\gamma^2}{2}-a\gamma}\Phi(a-\gamma)$$
$$\int_0^\infty e^{t} \Phi\lp a-\frac{t}{\gamma}\rp  \d t = -\Phi(a)+e^{\frac{\gamma^2}{2}+a\gamma}\Phi(a+\gamma)$$
\end{claim}
\begin{proof}
\begin{align*}
    \int_0^\infty e^{-t} \Phi(a-t/\gamma) \d t &= \left.-e^{-t}\Phi(a-t/\gamma)\right\vert_0^\infty - \int_0^\infty e^{-t} \frac{e^{-(a-t/\gamma)^2/2}}{\gamma\sqrt{2\pi}} \d t\\
     &= \Phi(a) - \int_0^\infty e^{\gamma^2/2-a\gamma} \frac{e^{-(t-(\gamma a-\gamma^2))^2/2}}{\gamma\sqrt{2\pi}} \d t\\
     %&= \Phi(a) -  e^{\gamma^2/2-a\gamma} \lp 1- \Phi(-(a-\gamma))\rp\\
     &= \Phi(a) -  e^{\gamma^2/2-a\gamma} \Phi(a-\gamma).
\end{align*}
\begin{align*}
    \int_0^\infty e^{t} \Phi(a-t/\gamma) \d t &= \left.e^{t}\Phi(a-t/\gamma)\right\vert_0^\infty + \int_0^\infty e^{t} \frac{e^{-(a-t/\gamma)^2/2}}{\gamma\sqrt{2\pi}} \d t\\
     &= -\Phi(a) + \int_0^\infty e^{\gamma^2/2+a\gamma} \frac{e^{-(t-(a\gamma+\gamma^2))^2/2\gamma^2}}{\gamma\sqrt{2\pi}} \d t\\
     %&= -\Phi(a) +  e^{\gamma^2/2+a\gamma} \lp 1- \Phi(-a-\gamma)\rp\\
     &= -\Phi(a)  +  e^{\gamma^2/2+a\gamma} \Phi(a+\gamma).
\end{align*}
\end{proof}

As a corollary to Theorem~\ref{thm:privacy_technical_tradeoff}, we can bound any divergence measure that decreases under post-processing such as Renyi divergence or KL divergence. In particular, this also implies Renyi Differential Privacy~\cite{Mir17} of our algorithm.


\begin{corollary}
\label{cor:divergence_privacy}
Suppose $F,\TF$ are two $\mu$-strongly convex functions over $\cK\subseteq \R^d$, and $F-\TF$ is $G$-Lipschitz over $\cK$.
For any $k>0$, if we let $P\propto e^{-kF}$ and $Q\propto e^{-k\TF}$ be two probability distributions on $\cK$, then we have
\begin{align*}
    \mathrm{D}(P\|Q)\leq \mathrm{D}\lp\cN(0,1)\|\cN\lp \frac{G\sqrt{k}}{\sqrt{\mu}},1\rp\rp
\end{align*}
for any divergence measure $\mathrm{D}$ which decreases under post-processing. In particular, $$\mathrm{D}_{\alpha}(P\|Q)\le \frac{\alpha k G^2}{2\mu} \text{ and }\mathrm{D}_{KL}(P\|Q)\le \frac{kG^2}{2\mu}.$$
\end{corollary}

\begin{proof}
% Recall in Theorem~\ref{thm:privacy_technical_tradeoff}, we proved for all $z\in[0,1]$, one has
% \begin{align*}
%     \tradeoff{\pi}{\nu}(z)\ge \tradeoff{\cN\lp 0,1\rp}{\cN\lp\frac{G\sqrt{k}}{\sqrt{\mu}},1\rp}(z).
% \end{align*}

By Theorem 2.10 in \cite{dong2019gaussian}, if $T(P\|Q) \ge T(X\|Y)$, then there exists a randomized algorithm $M$ such that $M(X)=P$ and $M(Y)=Q$. Therefore for any divergence measure which decreases under post-processing we have,
$$\mathrm{D}(P\|Q)= \mathrm{D}(M(X)\|M(Y)) \le \mathrm{D}(X\|Y).$$ The rest follows from Theorem~\ref{thm:privacy_technical_tradeoff}. It is well-known that Renyi divergence and KL divergence decrease with post-processing (see \cite{EH14}, for example). We can also compute $\mathrm{D}_\alpha(\cN(0,1),\cN(s,1))=\alpha s^2/2$ and $\mathrm{D}_{KL}(\cN(0,1),\cN(s,1))=s^2/2$~\cite{Mir17}.
\end{proof}

% Actually the bounds we get on Renyi divergence and Wasserstein distance are tight, where the tightness can be shown by Gaussian case.