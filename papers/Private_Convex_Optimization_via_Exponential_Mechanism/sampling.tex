\section{Efficient Non-smooth Sampling}\label{sec:sampling}
In this section, we will present an efficient sampling scheme for (non-smooth) functions to complement our main result first.
% Afterwards, as an example of the wide applications of our main result, we extend our result to differentially convex optimization, specifically DP-ERM and DP-SCO.
%\subsection{Sampling Scheme}
%\label{sec:sampling}s
Specifically, we study the following problem about sampling from a (non-smooth) log-concave distribution.
\begin{problem}\label{problem:sample}
Given a $\mu$-strongly convex function $\psi(x)$ defined on a convex set $\cK \subseteq \R^{d}$ and $+\infty$ outside. Given a family of $G$-Lipschitz convex functions $\{f_{i}(x)\}_{i\in I}$ defined on $\cK$. Our goal is to sample a point
$x\in \cK$ with probability proportionally to $\exp(-\widehat{F}(x))$ where
\[
\widehat{F}(x)=\E_{i\in I}f_{i}(x)+\psi(x).
\]
\end{problem}

Our sampler is based on the alternating sampling algorithm in \cite{LST21}
(See algorithm \ref{algo:AlternatingSampler}). This algorithm reduces
the problem of sampling from $\exp(-\widehat{F}(x))$ to sampling from $\exp(-\widehat{F}(x)-\frac{1}{2\eta}\|x-y\|^{2})$
for some fixed $\eta$ and for roughly $\frac{1}{\eta\mu}$ many different
$y$. When the step size $\eta$ is very small, the later problem
is easier because the distribution is almost like a Gaussian distribution.
For our problem, we will pick the largest step size $\eta$ such that
we can sample $\exp(-\widehat{F}(x)-\frac{1}{2\eta}\|x-y\|^{2})$ using only
$\widetilde{O}(1)$ many steps. 

\begin{algorithm2e}
\caption{Alternating Sampler \label{algo:AlternatingSampler}}

\textbf{Input:} $\mu$-strongly convex function $\widehat{F}$, step size $\eta>0$, initial
point $x_{0}$

\For{$t\in[T]$}{

$y_{t}\leftarrow x_{t-1}+\sqrt{\eta}\cdot\zeta$ where $\zeta\sim \cN(0, I_{d})$.

Sample $x_{t}\propto\exp(-\widehat{F}(x)-\frac{1}{2\eta}\|x-y_{t}\|_{2}^{2})$.\label{line:sample_oracle}

}

\textbf{Return} $x_{T}$

\end{algorithm2e}
\begin{theorem}[{\cite[Theorem 1]{LST21}}]
\label{thm:alternating_sampler}Given a $\mu$-strongly convex function
$F$ defined on $\cK$ with an initial point $x_{0}$. Let the distance $D=\|x_{0}-x^{*}\|_{2}$
for any $x^{*}=\arg\min_{x \in \cK}\widehat{F}(x)$. Suppose the step size $\eta\leq\frac{1}{\mu}$,
the target accuracy $\delta>0$ and the number of step $T\geq\Theta(\frac{1}{\eta\mu}\log(\frac{d/\mu+D^{2}}{\eta\delta}))$.
Then, Algorithm \ref{algo:AlternatingSampler} returns a random point
$x_{T}$ that has $\delta$ total variation distance to the distribution proportional to $\exp(-\widehat{F}(x))$.
\end{theorem}

Now, we show that Line \ref{line:sample_oracle} in Algorithm~\ref{algo:AlternatingSampler} can be implemented
by a simple rejection sampling. The idea is to pick step size $\eta$
small enough such that $\widehat{F}(x)$ is essentially a constant function
for a random $x\sim \cN(y,\eta\cdot I_{d})$. The precise algorithm
is given in Algorithm \ref{algo:AlternatingSamplerImpl}.

\begin{algorithm2e}

\caption{Implementation of Line \ref{line:sample_oracle} \label{algo:AlternatingSamplerImpl}}

\textbf{Input}: convex function $\widehat{F}(x)=\E_{i\in I}f_{i}(x)+ \psi(x)$,
step size $\eta>0$, current point $y$

\Repeat{$u\leq\frac{1}{2}\rho$}{

Sample $x,z$ from the distribution $\propto\exp(-\psi(x)-\frac{1}{2\eta}\|x-y\|^{2}_2)$

Set $\rho\leftarrow1$

\For{$\alpha=1,2,\cdots$}{

$\rho\leftarrow\rho+\Pi_{i=1}^{\alpha}(f_{j_{i}}(z)-f_{j_{i}}(x))$
where $j_{i}$ are random indices in $I$ 

With probability $\frac{\alpha}{1+\alpha}$, \textbf{break}

}

Sample $u$ uniformly from $[0,1]$.

}

\textbf{Return} $x$

\end{algorithm2e}

Since $F$ has the $\psi$ term, instead of sampling $x$ from $\cN(y,\eta\cdot I_{d})$,
we sample from $\exp(-\psi(x)-\frac{1}{2\eta}\|x-y\|^{2})$ in Algorithm
\ref{algo:AlternatingSamplerImpl}. The following lemma shows how
to decompose the distribution $\exp(-\widehat{F}(x)-\frac{1}{2\eta}\|x-y\|^{2})$
into the distribution mentioned above and the distribution $\exp(-\E_{i\in I}f_{i}(x))$. It
also calculates the distribution given by the algorithm. 
\begin{lem}
\label{lem:dpi}Let $\pi$ be the distribution proportional to $\exp(-\widehat{F}(x)-\frac{1}{2\eta}\|x-y\|_{2}^{2})$
and let $\mathcal{G}$ be the distribution proportional to $\exp(-\psi(x)-\frac{1}{2\eta}\|x-y\|^{2})$.
Then, we have that
\[
\frac{d\pi}{dx}=\frac{d\mathcal{G}}{dx}\cdot\frac{\exp(-\E_{i\in I}f_{i}(x))}{\E_{x\sim\mathcal{G}}\exp(-\E_{i\in I}f_{i}(x))}.
\]
Let $\widetilde{\pi}$ be the distribution returns by Algorithm \ref{algo:AlternatingSamplerImpl}.
Then, we have that
\[
\frac{d\widetilde{\pi}}{dx}=\frac{d\mathcal{G}}{dx}\cdot\frac{\E(\overline{\rho}|x)}{\E(\overline{\rho})}
\]
where $\overline{\rho} = \min(\max(\rho,0),2)$ is the truncation of $\rho$ in
Algorithm \ref{algo:AlternatingSamplerImpl} to $[0,2]$, $\E(\overline{\rho}|x)$ is the expected value of $\overline{\rho}$ conditional on $x$, and
$\E(\overline{\rho} )=\E_{x\sim\mathcal{G}}\E(\overline{\rho}|x)$. Furthermore,
we have that
\[
\E(\rho|x)=\exp(-\E_{i\in I}f_{i}(x))\cdot\E_{z\sim\mathcal{G}}\exp(\E_{i\in I}f_{i}(z)).
\]
\end{lem}

\begin{proof}
For the true distribution $\pi$, we have
\begin{align*}
\frac{d\pi}{dx} & =\frac{\exp(-\E_{i\in I}f_{i}(x)-\psi(x)-\frac{1}{2\eta}\|x-y\|_{2}^{2})}{\int\exp(-\E_{i\in I}f_{i}(x)-\psi(x)-\frac{1}{2\eta}\|x-y\|_{2}^{2})dx} \\
& = \frac{\exp(-\E_{i\in I}f_{i}(x))\frac{d\mathcal{G}}{dx}}{\int\exp(-\E_{i\in I}f_{i}(x))\frac{d\mathcal{G}}{dx}dx}=\frac{d\mathcal{G}}{dx}\cdot\frac{\exp(-\E_{i\in I}f_{i}(x))}{\E_{x\sim\mathcal{G}}\exp(-\E_{i\in I}f_{i}(x))}.
\end{align*}

For the distribution $\widetilde{\pi}$ by the algorithm, we sample
$x\sim\mathcal{G}$, then accept the sample if $u\leq\frac{1}{2}\rho$.
Hence, we have
\[
\frac{d\widetilde{\pi}}{dx}=\frac{d\mathcal{G}}{dx}\frac{\Pr(u\leq\frac{1}{2}\rho|x)}{\Pr(u\leq\frac{1}{2}\rho)}.
\]
Since $u$ is uniform between $0$ and $1$, we have the result.

Finally, for the expectation of $\rho$, we note that 
\[
\E\Pi_{i=1}^{\alpha}(f_{j_{i}}(z)-f_{j_{i}}(x))=(\E_{i\in I}(f_{i}(z)-f_{i}(x)))^{\alpha}
\]
and that the probability that the loop pass step $\alpha$ is exactly
$\frac{1}{\alpha!}$. Hence, we have 
\[
\E(\rho|x,z)=1+\sum_{\alpha=1}^{\infty}\frac{1}{\alpha!}(\E_{i\in I}(f_{i}(z)-f_{i}(x)))^{\alpha}=\exp(\E_{i\in I}(f_{i}(z)-f_{i}(x)).
\]
Taking expectation over $z$ gives the result.
\end{proof}
Note that if we always had $0 \leq \rho\leq2$, then $\E(\overline{\rho}|x)=\E(\rho|x)\propto\exp(-\E_{i\in I}f_{i}(x))$
and hence $\frac{d\pi}{dx}=\frac{d\widetilde{\pi}}{dx}$. Therefore,
the only thing left is to show that $0\leq \rho\leq2$ with high probability
and that it does not induces too much error in total variation distance.
To do this, we use Gaussian concentration to prove that $\E_{i\in I}f_{i}(x)$
is almost a constant over random $x\sim\mathcal{G}$.
\begin{lem}[{Gaussian concentration \cite[Eq 1.21]{Led99}}]
\label{lem:gaussian_concentration}Let $X \sim \exp(-\widehat{F})$ for some $1/\eta$-strongly convex $\widehat{F}$ and $\ell$
is a $G$-Lipschitz function. Then, for all $t\geq0$,
\[
\Pr[\ell(X)-\mathbb{E}[\ell(X)]\geq t]\leq e^{-t^{2}/(2\eta G^2)}.
\]
\end{lem}

Now, we are already to prove our main result. This shows that if $\eta\ll G^{-2}$,
then the algorithm indeed implements Line \ref{line:sample_oracle}
correctly up to small error.
\begin{lem}
\label{lem:sub_problem}If the step size $\eta\leq C\log^{-1}(1/\delta_{\mathrm{inner}})G^{-2}$
for some small enough $C$ and the inner accuracy $\delta_{\mathrm{inner}}\in(0,1/2)$,
then Algorithm \ref{algo:AlternatingSamplerImpl} returns a random
point $x$ that has $\delta_{\mathrm{inner}}$ total variation distance
to the distribution proportional to $\exp(-\widehat{F}(x)-\frac{1}{2\eta}\|x-y\|_{2}^{2})$. Furthermore,
the algorithm accesses only $O(1)$ many $f_{i}(x)$ in expectation and samples from $\exp(-\psi(x) - \frac{1}{2\eta} \|x-y\|^2_2)$ for $O(1)$ many $y$.
\end{lem}

\begin{proof}
Let $\pi$ be the distribution given by $c\cdot\exp(-\widehat{F}(x)-\frac{1}{2\eta}\|x-y\|_{2}^{2})$
and $\widetilde{\pi}$ is the distribution outputted by the algorithm.
By Lemma \ref{lem:dpi}, we have
\begin{align*}
d_{\mathrm{TV}}(\pi,\widetilde{\pi}) & =\int_{\R^{d}}\left|\frac{d\mathcal{G}}{dx}\frac{\exp(-\E_{i\in I}f_{i}(x))}{\E_{x\sim\mathcal{G}}\exp(-\E_{i\in I}f_{i}(x))}-\frac{d\mathcal{G}}{dx}\frac{\E(\overline{\rho}|x)}{\E(\overline{\rho})}\right|dx\\
 & =\E_{x\sim\mathcal{G}}\left|\frac{\exp(-\E_{i\in I}f_{i}(x))}{\E_{x\sim\mathcal{G}}\exp(-\E_{i\in I}f_{i}(x))}-\frac{\E(\overline{\rho}|x)}{\E(\overline{\rho})}\right|.
\end{align*}
Let $X$ be the random variable $\E(\rho|x)$ and $\widetilde{X}$
be the random variable $\E(\overline{\rho}|x)$. Lemma \ref{lem:dpi}
shows that $X=\exp(-\E_{i\in I}f_{i}(x))\cdot\E_{z\sim\mathcal{G}}\exp(\E_{i\in I}f_{i}(z))$
and hence
\[
\frac{\exp(-\E_{i\in I}f_{i}(x))}{\E_{x\sim\mathcal{G}}\exp(-\E_{i\in I}f_{i}(x))}=\frac{X}{\E_{x\sim \G}X}.
\]
Therefore, we have
\begin{align}
d_{\mathrm{TV}}(\pi,\widetilde{\pi}) & =\E\left|\frac{X}{\E X}-\frac{\widetilde{X}}{\E\widetilde{X}}\right|\leq\E\left|\frac{X}{\E X}-\frac{\widetilde{X}}{\E X}\right|+\E\left|\frac{\widetilde{X}}{\E X}-\frac{\widetilde{X}}{\E\widetilde{X}}\right|\leq2\frac{\E|X-\widetilde{X}|}{|\E X|}.\label{eq:dTV1}
\end{align}

We simplify the right hand side by lower bounding $\E X$. By Lemma
\ref{lem:gaussian_concentration} and the fact that the negative log-density of $\mathcal{G}$ is $1/\eta$-strongly convex, we have that $\E_{i\in I}f_{i}(z)\geq\E_{x\sim\mathcal{G}}\E_{i\in I}f_{i}(x)-2G \sqrt{\eta}$
with probability $\geq1-e^{-2}$. Hence, we have
\begin{align*}
\E X & =\E_{x\sim\mathcal{G}}\exp(-\E_{i\in I}f_{i}(x))\cdot\E_{z\sim\mathcal{G}}\exp(\E_{i\in I}f_{i}(z))\\
 & \geq\exp(-\E_{x\sim\mathcal{G}}\E_{i\in I}f_{i}(x))\cdot\E_{z\sim\mathcal{G}}\exp(\E_{i\in I}f_{i}(z))\\
 & =\E_{z\sim\mathcal{G}}\exp(\E_{i\in I}f_{i}(z)-\E_{x\sim\mathcal{G}}\E_{i\in I}f_{i}(x))\\
 & \geq(1-e^{-2})\exp(-2G \sqrt{\eta}).
\end{align*}
Using $\eta\leq G^{-2}/8$,
we have $\E [X]\geq\frac{2}{3}$. Using this, (\ref{eq:dTV1}), $X=\E(\rho|x)$
and $\widetilde{X}=\E(\overline{\rho}|x)$, we have
\[
d_{\mathrm{TV}}(\pi,\widetilde{\pi})\leq 3 \cdot\E|X-\widetilde{X}|\leq 3\cdot\E(|\rho|\cdot1_{\rho \notin [0,2]}).
\]

% \Daogao{...}
% Now we bound the expectation $\E(|\rho|\cdot1_{\rho \notin [0,2]})$.
% By a calculation similar to Lemma \ref{lem:dpi}, we have
% \begin{align*}
%     \E(|\rho|\cdot1_{\rho \notin [0,2]})\le&~\E(|\rho|)\\
%     \le & ~ \E_{x,z}\exp(\E_{i\in I}|f_i(z)-f_i(x)|).
% \end{align*}

% Thus
% \begin{align*}
%     \E_{x,z}\exp(\E_{i\in I}|f_i(z)-f_i(x)|)\le& \int_{0}^{\infty} \Pr_{x,y}[\exp(\E_{i\in I}|f_i(z)-f_i(x)|)\ge t] \d t\\
%     \leq & 1+\int_{1}^{\infty} 6e^{-\log^2 t/(64\eta G^2)}\d t\\
%     \leq& 1+ \int_{1}^{\infty}6e^{}
% \end{align*}

% \Daogao{.....Expectation too large. Can we bound the tail probability instead?}

We split the $\rho$ into two terms $\rho_{\leq L}$ and $\rho_{>L}$.
The first term $\rho_{\leq L}$ is the sum of all terms added to $\rho$
when $\alpha\leq L$ (including the initial term $1$). The second
term $\rho_{>L}$ is the sum when $\alpha>L$. Hence, we have $\rho=\rho_{>L}+\rho_{\leq L}$
and hence
\begin{equation}
d_{\mathrm{TV}}(\pi,\widetilde{\pi})\leq3\cdot\E(|\rho_{>L}|\cdot1_{\rho \notin [0,2]})+3\cdot\E(|\rho_{\leq L}|\cdot1_{\rho \notin [0,2]}).\label{eq:dTV2}
\end{equation}

For the term $\rho_{>L}$, by a calculation similar to Lemma \ref{lem:dpi},
we have
\begin{align*}
\E(|\rho_{>L}|\cdot1_{\rho \notin [0,2]}) & \leq\E|\rho_{>L}|\leq\E_{x,z}\Phi(\E_{i\in I}|f_{i}(z)-f_{i}(x)|),
\end{align*}
% \Daogao{Actually, the bound seems to be $\E|\rho_{>L}|\leq\E_{x,z}\Phi(\E_{i\in I}|f_{i}(z)-f_{i}(x)|).$ }
where $\Phi(t)=\sum_{\alpha=L+1}^{\infty}\frac{t^{\alpha}}{\alpha!}$ is a power series in $t$ with all positive coefficients.
By picking $L>C\log(1/\delta_{\mathrm{inner}})$ for some large constant
$C$, we have $\Phi(t)\leq\frac{\delta_{\mathrm{inner}}}{16}$
for all $|t|\leq1$. Let $\Delta$ be the random variable $\E_{i\in I}|f_{i}(z)-f_{i}(x)|$ whose randomness comes from $x$ and $z$.
Then, we have
\[
\E(|\rho_{>L}|\cdot1_{\rho \notin [0,2]})\leq\frac{\delta_{\mathrm{inner}}}{16}+\E e^{\Delta}1_{\Delta\geq1}\leq\frac{\delta_{\mathrm{inner}}}{16}+\sum_{k=1}^{\infty}e^{k+1}\Pr_{x,z}(\Delta\geq k).
\]

Denote a function $h_{x,z}(t):=\Pr_{i\in I}[|f_i(z)-f_i(x)|\ge t]$.
Since each $f_i$ is $G$-Lipschitz, Lemma~\ref{lem:gaussian_concentration} shows that
\begin{align*}
    \Pr_{x,z}[|f_i(z)-f_i(x)|\geq t]\leq 4e^{-t^2/(8\eta G^2)},
\end{align*}
which implies 
\begin{align*}
    \E_{x,z}[h_{x,z}(t)] = \Pr_{x,z,i}[|f_i(z)-f_i(x)|\geq t]\leq 4e^{-t^2/(8\eta G^2)}.
\end{align*}
By Markov inequality, for any $k>0$, we know
\begin{align*}
    \Pr_{x,z}[h_{x,z}(t)\geq e^{-k}]\leq 4e^{k-t^2/(8\eta G^2)}.
\end{align*}
As $|f_i(z)-f_i(x)|\leq G\|x-z\|_2$, if $h_{x,z}(t)=\Pr_{i\in I}[|f_i(z)-f_i(x)|\ge t]\leq e^{-t^2/(16\eta G^2)}$, we know $$\E_{i\in I}|f_i(z)-f_i(x)|\leq t +e^{-t^2/(16\eta G^2)}\cdot G\|x-z\|_2.$$
Hence, one has
\begin{align*}
     \Pr_{x,z}\Big[\E_{i\in I}|f_i(z)-f_i(x)|\ge t+e^{-t^2/(16\eta G^2)}G\|x-z\|_2 \Big]
    \leq &~ \Pr_{x,z}[h_{x,z}(t)\ge e^{-t^2/(16\eta G^2)}]\\
    \leq & ~ 4e^{-t^2/(16\eta G^2)}.
\end{align*}
By Gaussian Concentration, we know
\begin{align*}
    \Pr_{x,z}[\|x-z\|_2\ge t]
    \leq &~ \Pr_{x,z}[\|x-\E x\|_2\ge t/2 \text{ or }\|z-\E z\|\ge t/2]\\
    \leq & ~ 2e^{-t^2/(8\eta)}.
\end{align*}
Thus we know
\begin{align*}
    &~ \Pr_{x,z}[\E_{i\in I}|f_i(z)-f_i(x)|\ge 2t]\\
    =& ~\Pr_{x,z}[\E_{i\in I}|f_i(z)-f_i(x)|\ge 2t, \|x-z\|_2\ge t/G]+\Pr_{x,z}[\E_{i\in I}|f_i(z)-f_i(x)|\geq 2t,\|x-z\|_2<t/G]\\
    \leq & ~2e^{-t^2/(8G^2\eta)}+\Pr_{x,z}[\E_{i\in I}|f_i(z)-f_i(x)|\geq 2t,\|x-z\|_2<t/G]\\
    \leq &~  2e^{-t^2/(8G^2\eta)} +\Pr_{x,z}[\E_{i\in I}|f_i(z)-f_i(x)|\ge t+e^{-t^2/(16\eta G^2)}G\|x-z\|_2]\\
    \leq & ~ 6e^{-t^2/(16\eta G^2)}.
\end{align*}
% \Daogao{new :}We want to bound the tail probability of $\Delta$. We can lose a $\log(|I|)$ term for union bound, or lose $\sqrt{d}$ to bound the distance between $\|x-z\|$.
% Now we try to get the original result for infinitely large size of $I$.
% \Daogao{Old:}
% Since $\E_{i\in I}f_{i}(x)$ is a $G$-Lipschitz function, Lemma \ref{lem:gaussian_concentration}
% shows that
% \[
% \Pr_{x,z}[|\E_{i\in I}(f_{i}(z)-f_{i}(x))|\geq t]\leq4e^{-t^{2}/(8 \eta G^2)}.
% \]
Hence, we have $\Pr(\Delta\geq k)\leq 6\exp(-k^{2}/(64G^{2}\eta))$ and 
\begin{equation}
\E(|\rho_{>L}|\cdot1_{\rho \notin [0,2]})\leq\frac{\delta_{\mathrm{inner}}}{16}+17\sum_{k=1}^{\infty}e^{k-\frac{k^{2}}{64G^{2}\eta}} \leq\frac{\delta_{\mathrm{inner}}}{9},\label{eq:dTv3}
\end{equation}
where we used $\eta\leq2^{-6}G^{-2}/\log(400/\delta_{\mathrm{inner}})$ at the end.

As for the term $\rho_{\leq L}$, we know that
\begin{align}
\label{eq:split_dTVterm}
& \E(|\rho_{\leq L}|\cdot 1_{\rho \notin [0,2]})\nonumber\\
= & \E(|\rho_{\leq L}|\cdot 1_{\rho \notin [0,2]}\cdot 1_{|\rho_{\leq L}|\leq 2^L})+\E(|\rho_{\leq L}|\cdot1_{\rho \notin [0,2]}\cdot 1_{|\rho_{\leq L}|\geq 2^L})\nonumber \\
\leq & \Pr[\rho\notin[0,2]]\cdot 2^L+ \sum_{k=1}^{\infty}2^{(k+1)L}\Pr(|\rho_{\leq L}|\ge2^{kL}).
\end{align}
Note that the term $\rho_{\leq L}$ involves only less than $\frac{L^{2}}{2}$
many $f_{i}(x)$ and $f_{i}(z)$. Lemma \ref{lem:gaussian_concentration}
shows that for any $i$, we have
\[
\Pr_{x\sim\mathcal{G}}(|f_{i}(x)-\E_{x\sim\mathcal{G}}f_{i}(x)|\geq t )\leq2e^{-t^{2}/(2 \eta G^2)}.
\]
By union bound, this shows 
\[
\Pr_{x,z\sim\mathcal{G}}(|f_{i}(x)-f_{i}(z)|\geq\frac{1}{4}2^{k}\text{ for any such }i)\leq L^{2}\exp(-\frac{4^{k}}{32\eta G^{2}}).
\]
Under the event $|f_{i}(x)-f_{i}(z)|\leq\frac{1}{3}2^{k}$
for all $i$ appears in $\rho_{\leq L}$, we have
\[
|\rho_{\leq L}|\leq1+\sum_{\alpha=1}^{L}\Pi_{i=1}^{\alpha}|f_{j_{i,\alpha}}(z)-f_{j_{i,\alpha}}(x)|\leq1+\sum_{\alpha=1}^{L}(\frac{2^{k}}{3})^\alpha\leq2^{kL}.
\]
Therefore, we have $\Pr(|\rho_{\leq L}|>2^{kL})\leq L^{2}\exp(-\frac{4^{k}}{32\eta G^{2}})$
and
\[
\sum_{k=1}^{\infty}2^{(k+1)L}\Pr(|\rho_{\leq L}|>2^{kL})\leq\sum_{k=1}^{\infty}2^{(k+1)L}L^{2}\exp(-\frac{4^{k}}{32\eta G^{2}})\leq\sum_{k=1}^{\infty}2^{4kL}\exp(-\frac{4^{k}}{32\eta G^{2}}).
\]
Picking $\eta\leq2^{-8}G^{-2}L^{-1}$, we have that
\begin{equation}
\sum_{k=1}^{\infty}2^{(k+1)L}\Pr(|\rho_{\leq L}|>2^{kL})\leq\sum_{k=1}^{\infty}2^{4kL}\exp(-2\cdot4^{k}L)\leq\sum_{k=1}^{\infty}2^{-kL}\leq\frac{\delta_{\mathrm{inner}}}{9}\label{eq:dTV4}
\end{equation}
by picking $L>C\log(1/\delta_{\mathrm{inner}})$ for large enough
$C$.

It remains to bound the term $\Pr[\rho\notin[0,2]]\cdot 2^L$.
We know the probability the algorithm enters the $(L+1)$-th phase is at most $\frac{1}{L!}\leq \frac{2}{2^L}$.
Hence we know $\Pr[\rho \notin[0,2]]\leq \frac{2}{2^L}+\Pr[\rho_{\leq L}\notin[0,2]]$.
Similarly, by Gaussian Concentration and union bound, we have
\begin{align*}
    \Pr_{x,z\sim\mathcal{G}}(|f_{i}(x)-f_{i}(z)|\geq1/2\text{ for any such }i)\leq L^{2}\exp(-\frac{1}{8\eta G^{2}}).
\end{align*}
Under the event that $|f_i(x)-f_i(z)|\leq 1/2$ for all $i$ appears in $\rho_{\leq L}$, we have
\begin{align*}
    1-\sum_{\alpha=1}^{L}\Pi_{i=1}^{\alpha}|f_{j_{i,\alpha}}(z)-f_{j_{i,\alpha}}(x)|\leq \rho_{\le L}\le 1+\sum_{\alpha=1}^{L}\Pi_{i=1}^{\alpha}|f_{j_{i,\alpha}}(z)-f_{j_{i,\alpha}}(x)|,
\end{align*}
which implies $0\le \rho_{\leq L}\leq 2$.
Then we know $\Pr[\rho_{\le L}\notin[0,2]]\leq L^2\exp(-\frac{1}{8\eta G^2})$.
By our setting of parameters and that $L = C \log(1/\delta_{\mathrm{inner}})$ for some large constant $C$, we know
\begin{align}
\label{eq:dTV5}
    \Pr[\rho\notin[0,2]]\cdot 2^L\leq 2^L(L^2\exp(-\frac{1}{8\eta G^2})+\frac{2}{2^L})\leq \frac{\delta_{\mathrm{inner}}}{9}.
\end{align}

Combining (\ref{eq:dTV2}), (\ref{eq:dTv3}), (\ref{eq:split_dTVterm}), (\ref{eq:dTV4}) and (\ref{eq:dTV5}),
we have the result $d_{\mathrm{TV}}(\pi,\widetilde{\pi})\leq\delta_{\mathrm{inner}}$.

Finally, the accept probability is given by $\E\widetilde{X}/2$ and $\E\widetilde{X}\geq\E X-\E|X-\widetilde{X}|\geq\frac{2}{3}-\frac{\delta_{\mathrm{inner}}}{3}\geq\frac{1}{3}$.
Hence, the number of access is $O(1)$.
\end{proof}
Combining Theorem \ref{thm:alternating_sampler} and Lemma \ref{lem:sub_problem},
we have the following result:
\begin{theorem}
\label{thm:sampler}
Given a $\mu$-strongly convex function $\psi(x)$ defined on a convex set $\cK \subseteq \R^{d}$ and $+\infty$ outside. Given a family of $G$-Lipschitz convex functions $\{f_{i}(x)\}_{i\in I}$ defined on $\cK$.
Define the function $\widehat{F}(x)=\E_{i\in I}f_{i}(x)+\psi(x)$ and 
the distance $D=\|x_{0}-x^{*}\|_{2}$ for some $x^{*}=\arg\min_{x}\widehat{F}(x)$.
For any $\delta\in(0,1/2)$, if we can get samples from $\exp(-\psi(x)-\frac{\|x-y\|_2^2}{2\eta}) $ for any $y\in\R^d$ and $\eta>0$, we can find a random point $x$ that
has $\delta$ total variation distance to the distribution proportional to $\exp(-\widehat{F}(x))$ in
\[
T:=\Theta(\frac{G^{2}}{\mu}\log^{2}(\frac{G^{2}(d/\mu+D^{2})}{\delta}))\text{ steps}.
\]
Furthermore, each steps accesses only $O(1)$ many $f_{i}(x)$ in
expectation and samples from $\exp(-\psi(x) - \frac{1}{2\eta} \|x-y\|^2_2)$ for $O(1)$ many $y$ with $\eta = \Theta(G^{-2}/\log(T/\delta))$.
\end{theorem}

\begin{proof}
This follows from applying Lemma \ref{lem:sub_problem} to implement
Line \ref{line:sample_oracle}. Note that the distribution implemented
has total variation distance $\delta_{\mathrm{inner}}$ to the required
one. By setting $\delta_{\mathrm{inner}}=\delta/(2T)$, this only
gives an extra $\delta/2$ error in total variation distance. Finally,
setting $\eta=\Theta(G^{-2}/\log(1/\delta_{\mathrm{inner}}))$, Theorem
\ref{thm:alternating_sampler} shows that Algorithm \ref{algo:AlternatingSamplerImpl}
outputs the correct distribution up to $\delta/2$ error in total
variation distance. This gives the result.
\end{proof}

In the most important case of interest when $\psi(x)$ is $\ell_2^2$ regularizer, one can see $\exp(-\psi(x) - \frac{1}{2\eta} \|x-y\|^2_2)$ is a truncated Gaussian distribution, and there are many results on how to sample from truncated Gaussian, e.g. \cite{KD99}.
For more general case, there are also efficient algorithms to do the sampling, such as the Projected Langevin Monte Carlo \cite{BEL18}.
In fact our sampling scheme matches the information-theoretical lower bound on the value query complexity up to some logarithmic terms, which can be reduced from the result in \cite{DJWW15} with some modifications.
See Section~\ref{sec:infolower} for a detailed discussion.