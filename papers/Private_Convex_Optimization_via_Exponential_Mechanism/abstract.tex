In this paper, we study private optimization problems for non-smooth convex functions $F(x)=\mathbb{E}_i f_i(x)$ on $\mathbb{R}^d$.
We show that modifying the exponential mechanism by adding an $\ell_2^2$ regularizer to $F(x)$ and sampling from $\pi(x)\propto \exp(-k(F(x)+\mu\|x\|_2^2/2))$ recovers both the known optimal empirical risk and population loss under $(\eps,\delta)$-DP. Furthermore, we show how to implement this mechanism using $\widetilde{O}(n \min(d, n))$ queries to $f_i(x)$ for the DP-SCO where $n$ is the number of samples/users and $d$ is the ambient dimension.
We also give a (nearly) matching lower bound $\widetilde{\Omega}(n \min(d, n))$ on the number of evaluation queries. 

Our results utilize the following tools that are of independent interest:
\begin{itemize}
    \item We prove Gaussian Differential Privacy (GDP) of the exponential mechanism if the loss function is strongly convex and the perturbation is Lipschitz. Our privacy bound is \emph{optimal} as it includes the privacy of Gaussian mechanism as a special case and is proved using the isoperimetric inequality for strongly log-concave measures.
    \item We show how to sample from $\exp(-F(x)-\mu \|x\|^2_2/2)$ for $G$-Lipschitz $F$ with $\eta$ error in total variation (TV) distance using $\widetilde{O}((G^2/\mu) \log^2(d/\eta))$ unbiased queries to $F(x)$. This is the first sampler whose query complexity has \emph{polylogarithmic dependence} on both dimension $d$ and accuracy $\eta$.
\end{itemize}


%We study the private convex optimization problem for non-smooth convex functions. To minimize $F(x)=\mathbb{E}_i f_i(x)$ for convex $f_i$, the exponential mechanism samples $x^{priv}$ from the density $\pi(x)\propto \exp(-kF(x))$. It is well-known that this has optimal empirical risk under $(\eps,0)$-DP [Bassily, Smith, Thakurta 14] but is sub-optimal under $(\eps,\delta)$-DP.

%Thus we give a simple and unified algorithm which \emph{optimally} solves both DP Empirical Risk Minimization (ERM) and DP Stochastic Convex Optimization (SCO) problems, while previous work had different algorithms for these problems.

% I also want to say the following. But I don't like super long abstract. So, maybe good to highlight in the intro.
% \begin{itemize}
% \item Our mechanism achieves the optimal empirical risk and population loss simultaneously.
% \item In some sense, our mechanism is exactly what statistician doing for decades. Do sampling and add regularizer.
% \item Our mechanism is mathematically simple compared to previous algorithms.
% \item Our guarantee has very small constant. It may be optimal even with the constant?
% \item Our algorithm even match fastest algorithm using gradient oracle for some regime. This is surprising because gradient oracle uses $d$ times more information.
% \item Value oracle is potentially useful for many ML settings like ads, RL, etc. Seb knows this a lot. I only know the math, not applications. So, this open up the possible applications.
% \end{itemize}

% \Daogao{The result about SCO is particularly exciting to me, as people only know the localization framework, which is complicated, and needs to run projected gradient descent on a domain decreasing over time which is annoying and may significantly impact accuracy.}


%We prove Gaussian Differential Privacy (GDP) of the exponential mechanism for private convex optimization under strong convexity assumption. Suppose each user contributes a $G$-Lipschitz function $f_i:\mathcal{K}\to \R$ over some convex set $\cK\subset \R^d$ with diameter $D$. To minimize $F(x)=\frac{1}{n}\sum_{i=1}^n f_i(x) + \theta(x)$ (where $\theta$ is some convex regularizer), the exponential mechanism samples $x^{priv}$ from the density $\pi(x)\propto \exp(-kF(x))$. It is well-known that this achieves $(\eps,0)$-DP with $\eps=\frac{2GDk}{n}$. We show that if in addition $F$ is $\mu$-strongly convex, then the exponential mechanism achieves $\gamma$-GDP with $\gamma=\frac{2G\sqrt{k}}{n\sqrt{\mu}}$. In other words, it achieves the same privacy guarantees as a Gaussian mechanism with sensitivity $\gamma$ and noise scale $1.$ Our privacy bound is \emph{optimal} as it includes the Gaussian mechanism as a special case, and is proved using the \emph{isoperimetric inequality} for strongly log-concave measures. As a corollary, this immediately implies $(\epsilon,\delta)$-DP with $\epsilon = O\left(\frac{G\sqrt{k\log(1/\delta)}}{n\sqrt{\mu}}\right)$ which was previously shown by Minami et al. (NeurIPS 2016). This also implies that the exponential mechanism (by a simple modification) can achieve the optimal empirical risk of $O\left(\frac{GD\sqrt{d\log(1/\delta)}}{\epsilon n}\right)$ under $(\eps,\delta)$-DP.

% The key ingredient of this paper is the following generalization of the Gaussian mechanism. We show that the privacy curve between a strongly log-concave distribution and its Lipschitz perturbation in the exponent is bounded by the privacy curve of a Gaussian mechanism. Our privacy bound is \emph{optimal} as it includes the Gaussian mechanism as a special case, and is proved using the \emph{isoperimetric inequality} for strongly log-concave measures.

%In addition, we show how to sample from $\exp(-F(x))$ for $G$-Lipschitz and $\mu$-strongly convex $F$ in time $\widetilde{O}((G^2/\mu) \log^2(d/\eta))$ steps where $\eta$ is the accuracy in total variation distance. The new sampler combines of the alternating sampler of Lee et al. (COLT 2020) and a new rejection scheme. Each step involves only one stochastic value oracle to $F$. This is the first sampler for the non-smooth case where the number of steps has \emph{polylogarithmic dependence} in both dimension and accuracy. As a corollary, this gives the fastest known algorithm for private convex optimization in some parameter regimes. 

%============================== OLD ABSTRACT ========================================================
% We propose a Gaussian analogue to the exponential mechanism for differentially private convex optimization. Suppose each user contributes a $G$-Lipschitz function $f_i:\mathcal{K}\to \R$ over some convex set $\cK\subset \R^d$ with diameter $D$. To minimize $F(x)=\frac{1}{n}\sum_{i=1}^n f_i(x)$, the exponential mechanism samples $x^{priv}$ from the density $\pi(x)\propto \exp(-kF(x))$. This achieves $(\eps,0)$-DP with an excess empirical risk of $O\left(\frac{GDd}{\epsilon n}\right)$ for some appropriate choice of $k$ [Bassily, Smith, Thakurta 14]. In this paper, we show that by merely adding an $\ell_2^2$-regularizer to $F(x)$ and sampling from $\pi(x)\propto \exp(-k(F(x)+\mu\|x\|_2^2/2))$, recovers the known optimal empirical risk of $O\left(\frac{GD\sqrt{d\log(1/\delta)}}{\epsilon n}\right)$ for the $(\eps,\delta)$-DP settings. If $F$ is already $\mu$-strongly convex, then the exponential mechanism itself achieves the optimal empirical excess risk of $O\left(\frac{dG^2\log(1/\delta)}{n^2\mu\eps^2}\right)$ which was not known previously.

% The key ingredient of this paper is the following generalization of the Gaussian mechanism. We show that the privacy curve between a strongly log-concave distribution and its Lipschitz perturbation in the exponent is bounded by the privacy curve of a Gaussian mechanism. Our privacy bound is \emph{optimal} as it includes the Gaussian mechanism as a special case, and is proved using the isoperimetric inequality for strongly log-concave measures.

% In addition, we show how to sample from $\exp(-F(x) + \mu\|x\|_2^2/2)$ for $G$-Lipschitz convex $F$ in time $\widetilde{O}((G^2/\mu) \log^2(d/\eta))$ steps where $\eta$ is the accuracy in total variation distance. The new sampler combines of the alternating sampler [Lee, Shen, Tian 21] and a new rejection scheme. Each step involves only one stochastic value oracle to $F$. This is the first sampler for the non-smooth case where the number of steps has \emph{polylogarithmic dependence} in both dimension and accuracy. As a corollary, this gives the fastest known algorithm for private convex optimization in some parameter regimes. 

%=================================================================================
%satisfies $\gamma$-Gaussian Differential Privacy with $\gamma=\frac{2G\sqrt{k}}{N\sqrt{\mu}}$, i.e., it has the same privacy curve as a Gaussian mechanism with sensitivity $\gamma.$ Moreover it achieves the optimal empirical risk of $O\left(\frac{GD\sqrt{d\log(1/\delta)}}{\epsilon N}\right)$, which is much better than that of the exponential mechanism for large dimensions. When $F(x)$ is already $\mu$-strongly convex, then the exponential mechanism itself achieves these much better privacy guarantees which was not known previously. Our privacy bounds are \emph{optimal} and obtained using the isoperimetric inequality for strongly log-concave measures.% When $f_i=\frac{1}{2}\|x-a_i\|_2^2$, our mechanism reduces to releasing the optimal solution $\frac{1}{N}\sum_{i=1}^N a_i$ with Gaussian noise.

%We also give an efficient algorithm for sampling from $\pi(x)\propto \exp(-k(F(x)+\mu\|x\|_2^2/2))$ which runs in time $O\left(\frac{kG^2}{\mu}\right)=O(\gamma^2 N^2)$ with only oracle access to the functions $f_i(\cdot).$



% We propose a Gaussian analogue to the exponential mechanism for differentially private convex optimization. Suppose each user contributes a $G$-Lipschitz function $f_i:\mathcal{K}\to \R$ over some convex set $\cK\subset \R^d$ with diameter $D$. To minimize $F(x)=\frac{1}{N}\sum_{i=1}^N f_i(x)$, the exponential mechanism samples $x^{priv}$ from the density $\pi(x)\propto \exp(-kF(x))$. This achieves $(\eps,0)$-DP with an excess empirical risk of $O\left(\frac{GDd}{N\epsilon}\right)$ for some appropriate choice of $k$

% In this paper, we show that by adding an $\ell_2^2$-regularizer to $F(x)$ and sampling from $\pi(x)\propto \exp(-k(F(x)+\mu\|x\|_2^2))$ satisfies $\gamma$-Gaussian Differential Privacy with $\gamma=\frac{2G\sqrt{k}}{N\sqrt{\mu}}$, i.e., it has the same privacy curve as a Gaussian mechanism with sensitivity $\gamma.$ Moreover it achieves the optimal empirical risk of $O\left(\frac{GD\sqrt{d\log(1/\delta)}}{\epsilon N}\right)$, which is much better than that of the exponential mechanism for large dimensions. When $F(x)$ is already $\mu$-strongly convex, then the exponential mechanism itself achieves these much better privacy guarantees which was not known previously. Our privacy bounds are \emph{optimal} and obtained using the isoperimetric inequality for strongly log-concave measures.% When $f_i=\frac{1}{2}\|x-a_i\|_2^2$, our mechanism reduces to releasing the optimal solution $\frac{1}{N}\sum_{i=1}^N a_i$ with Gaussian noise.

% We also give an efficient algorithm for sampling from $\pi(x)\propto \exp(-k(F(x)+\mu\|x\|_2^2))$ which runs in time $O\left(\frac{kG^2}{\mu}\right)=O(\gamma^2 N^2)$ with only oracle access to the functions $f_i(\cdot).$