\documentclass{llncs}

\usepackage{graphicx} % figures

\usepackage{amsmath} % math
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{accents}
\newcommand{\ubar}[1]{\underaccent{\bar}{#1}}

\usepackage[pdftex,dvipsnames]{xcolor}  % Coloured text etc.

\usepackage{listings} % code listings
\lstset{frame=tb,
  language=Python,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}

% Marking equal contribution.
\makeatletter
\newcommand{\printfnsymbol}[1]{%
  \textsuperscript{\@fnsymbol{#1}}%
}
\makeatother
%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\title{Explainable nonlinear modelling of multiple time series with invertible neural networks\thanks{
The work in this paper was supported by the SFI Offshore Mechatronics
grant 237896/O30.
}
}
\titlerunning{Explainable VAR modeling with invertible NNs}

\author{Luis Miguel Lopez-Ramos\thanks{Equal contribution in terms of working hours.}%\inst{1,2}
\orcidID{0000-0001-8072-3994} \and
\\ 
Kevin Roy\printfnsymbol{2}%\inst{1,2}
%\orcidID{1111-2222-3333-4444} 
\and
Baltasar Beferull-Lozano%\inst{1,2}
\orcidID{0000-0002-0902-6245}
}

\authorrunning{Lopez-Ramos & Roy et al.}

\institute{SFI Offshore Mechatronics Center, University of Agder
\and
Intelligent Signal Processing and Wireless Networks (WISENET) Center
\and
Department of ICT, University of Agder, Grimstad, Norway
}
\maketitle

\begin{abstract}
    A method for nonlinear topology identification is proposed, based on the assumption that a collection of time series are generated in two steps: i) a vector autoregressive process in a latent space, and ii) a nonlinear, component-wise, monotonically increasing observation mapping. The latter mappings are assumed invertible, and are modeled as shallow neural networks, so that their inverse can be numerically evaluated, and their parameters can be learned using a technique inspired in deep learning. Due to the function inversion, the backpropagation step is not straightforward, and this paper explains the steps needed to calculate the gradients applying implicit differentiation. Whereas the model explainability is the same as that for linear VAR processes, preliminary numerical tests show that the prediction error becomes smaller.
\end{abstract}

\keywords{Vector autoregressive model
\and nonlinear
\and network topology inference
\and invertible neural network
}

\input{Introduction.tex}
\input{background.tex}
\input{modeling.tex}
\input{experiments.tex}

\section{Conclusion}

A method for inferring nonlinear VAR models has been proposed and validated. The modeling assumption that the observed data are the outputs of nodal nonlinearities applied to the individual time series of a linear VAR process lying in an unknown latent vector space. Since the number of parameters that determine the topology does not increase, the model interpretability remains the same as that with linear VAR modeling, making the proposed model amenable for Granger causality testing and network topology identification. The optimization method, similar to that of DNN training, can be extended with state-of-the-art tools to accelerate training and avoid undesired effects such as convergence to unstable points and overfitting.

\subsubsection{Acknowledgement:}

The authors would like to thank Emilio Ruiz Moreno for helping us manage a more elegant derivation of the gradient of $g_i(\cdot)$.

\bibliographystyle{IEEEtran}
\bibliography{intap}

\end{document}