\section{Background}
 \subsection{Graph Topology Identification}
 
Estimating topology of a system means finding the dependencies between network data time series. These dependencies may not be physically observable; rather, there can be logical connections between data nodes that are not physically connected, but which may be (indirectly) logically connected due to, e.g. control loops. %in the overall dynamic system being monitored. 
Topology inference has the potential to contribute to the algorithmic foundations to solve important problems in signal processing (e.g. prediction, data completion, etc..) and data-driven control.

\begin{figure}[h]
\centering
\includegraphics[width=0.6\textwidth]{figures/node1a.jpg}
\caption{{Illustration of an N-node network with directed edges% (in blue) 
}} 
\label{fig:node_edges}
\end{figure}

While the simplest techniques such as correlation graphs \cite{jin2020correlated} cannot determine the direction of interactions, one may also employ to this end structural equation models (SEM) or Bayesian networks \cite{yanuar2014estimation}. However, such methods account only for memory-less interactions. On the other hand, causality in the Granger \cite{granger1988concept} sense is based on the idea that the cause precedes the effect in time, and knowledge about the cause helps predicting the effect more accurately. The way Granger causality is defined makes it interesting, from a conceptual point of view, for understanding data dependencies; however, it is often computationally intractable. Thus, alternative causality definitions, such as those based on vector autoregressive (VAR) models \cite{granger1988concept,zaman2020online} are typically preferred in practical scenarios. The simplest possible VAR model is a linear VAR model. 

Consider a collection of $N$ sensors, where $y_n[t]$ denotes the measurement of the $n$-th sensor at time $t$. A $P$-th order linear VAR model can be formulated as 
\begin{align} \label{eq:var_matrix}
      y[t]=\sum_{p=1}^{P} A_p y[t-p]+u[t],  \quad \quad P \leq t \leq T
\end{align}
where $y[t]=[y_1[t],........,y_N[t]]^T$, $A_p\ \in\ R^{N\times N}$, p = 1,\ldots, P, are the matrices of VAR parameters (see Fig. 2) , $T$ is observation time period, and $u[t]={[u}_1[t],.........,u_N[t]]$ is an innovation process typically modeled as a Gaussian, temporally white random process. With $a_{n,n^\prime}^{(p)}$ being the $(n,n^\prime)$ entry of the matrix $A_p$, the r.h.s above takes the form:
% IT MAY BE A GOOD IDEA LATER TO REDUCE A BIT THE SPACING
 \begin{align} \label{eq:var_scalar}
       y_n[t] = \sum_{n^\prime=1}^{N}\sum_{p=1}^{P}{a_{n,n^\prime}^{(p)}y_{n^\prime}}[t-p]+\ u_n[t], \quad \quad P \leq t \leq T
	\end{align}
for $n = 1,\ldots, N$, %where   $a_{n,n^\prime}  = [a_{n,n^\prime}^{(1)},....,\ a_{n,n^\prime}^{(p)}]^{T}$ is the impulse response from node $n^\prime$ to node $n$; this will be a zero vector when there is no edge from node $n^\prime$ to node $n$. 
The problem of identifying a linear VAR causality model reduces to estimating the VAR coefficient matrices $\{A_p\}_{p=1}^P$ given the observations $\{y[t]\}_{t=0}^{T-1}$. The VAR causality \cite{lutkepohl2005} is determined from the support of the VAR matrix parameters and can be interpreted as a surrogate (yet not strictly equivalent) for Granger causality\footnote{Notice that VAR models encode lagged interactions, and other linear models such as structural equation models (SEM) or structural VAR (SVAR) are available if interactions at a small time scale are required. In this paper, for the sake of simplicity, we focus on learning non-linear VAR models. However, our algorithm designs can also accomodate the SEM and SVAR frameworks without much difficulty.}.
\iffalse
%\vspace{0px}
\begin{figure}[h]
\centering
\includegraphics[width=0.7\textwidth]{figures/figvar.png}
\caption{{ Tensor $A$ collecting the VAR parameter matrices \cite{zaman2020online}.  }}
\label{fig:A_tensor}
\end{figure}
\fi
%Notice that VAR models rely on uniformly-sampled data, and sometimes, one is interested in finding interactions that occur at a time scale that is smaller than the sampling rate; we refer to these interactions as “instantaneous causal relations”, which a standard VAR model cannot capture. In this sense, an alternative model that captures both lagged and instantaneous causal relations is the structural VAR (SVAR), a slightly modified model that unifies both SEM and VAR. SEM, VAR and SVAR models are widely used to study linear dependencies among the graph connected time-series. 



\subsection{Nonlinear function approximation}

The main advantages of linear modeling are its simplicity, the low variance of the estimators (at the cost of a higher bias compared to more expressive methods), and the fact that linear estimation problems often lead naturally to convex optimization problems, which can be solved efficiently.

However, there are several challenges related to inferring linear, stationary models from real-world data. Many instances such as financial data, brain signals, industrial sensors, etc. exhibit highly nonlinear interactions, and only nonlinear models have the expressive capacity to capture complex dependencies (assuming that those are identifiable and enough data are provided for the learning). Some existing methods have tried to capture nonlinear interactions using kernel-based function approximators (see  \cite{shen2019nonlinear,money2021online} and references therein).
%\textcolor{red}{Add references to Rohan's paper and 1 or 2 more references there-in}.
In the most general non-linear case, each data variable $y_n[t]$ can be represented as a non-linear function of several multi-variate data time series as:
    \begin{align} \label{eq:nonvar}
	y_n[t] =   h_n(y_{t-1},\ldots, y_{t-P}) + u_n[t],
	\end{align}
    where $y_{t-p} = [y_1[t-p],y_2[t-p],.....,y_N[t-p]]^{T}$, and $h(\cdot)$ is a non-linear function.

However, from a practical perspective, this model is too general to be useful in real applications, because the class of possible nonlinear functions is unrestricted and, therefore, the estimators will suffer from high variance. Notice also that learning such a model would require in general an amount of data that may not be available in realistic scenarios, and requiring a prohibitive complexity. A typical solution is to restrict the the modeling to a subset of nonlinear functions, either in a parametric (NN) or nonparametric (kernel) way.

Our goal in this paper is to learn nonlinear dependencies with some underlying structure making it possible to learn them with limited complexity, with an expressive slightly higher than linear models.
