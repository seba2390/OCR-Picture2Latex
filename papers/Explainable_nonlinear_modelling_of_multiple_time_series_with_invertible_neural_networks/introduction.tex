
\section{Introduction}
Multi-dimensional time series data are observed in many real-world systems, where some of the time series are influenced by other time series. The interrelations among the time series can be encoded in a graph structure, and identifying such structure or topology is of great interest in multiple applications \cite{giannakis2018topology}.  The inferred topology can provide insights about the underlying system and can assist in inference tasks such as prediction and anomaly detection. 

In real-world applications such as neuroscience and genomics, signal interrelations are often inherently nonlinear \cite{chen2018dynamic,fujita2010granger,shen2019nonlinear}. In these cases, using linear models may lead to inconsistent estimation of causal interactions \cite{tank2017interpretable}. We propose deep learning based methods by applying feed-forward invertible neural networks. This project proposes a low-complexity nonlinear topology identification method that is competitive with other nonlinear methods explaining time series data from a heterogeneous set of sensors.

\subsection{State of the art and contribution}

The use of linear VAR models for topology identification have been well-studied. A comprehensive review of topology identification algorithms was recently published \cite{giannakis2018topology}, where the issue of nonlinearity is discussed together with other challenges such as dynamics (meaning estimating time-varying models). 

In \cite{zaman2020online}, an efficient algorithm to estimate linear VAR coefficients from streaming data is proposed. Although the linear VAR model is not expressive enough for certain applications, it allows clear performance analysis, and is subject to continuous technical developments, such as a novel criterion for automatic order selection \cite{nassif2021automatic}, VAR estimation considering distributions different to the Gaussian, such as Student's $t$ \cite{zhou2021parameter}, or strategies to deal with missing data \cite{zhou2021parameter,ioannidis2019semiblind,zaman2020online}.

Regarding non-linear topology identification based on the VAR model, kernels are used in  \cite{shen2018online,money2021online} to linearize the nonlinear dependencies by mapping variables to a higher-dimensional Hilbert space. The growth of computational complexity and memory requirements (a.k.a.  “curse  of dimensionality”) associated with kernel representations is circumvented in \cite{shen2018online,money2021online} by  restricting  the  numeric  calculation  to  a  limited number  of  time-series  samples  using  a  time  window,  which results in suboptimal performance. A semiparametric model is proposed for the same task in \cite{farnoosh2017semiparametric}.

A different class of nonlinear topology identification methods are based on deep feedforward or recurrent NNs \cite{tank2017interpretable,tank2021neural} combined with sparsity-inducing penalties on the weights at one layer, labeled as "Granger-causality layer".

Recent work \cite{morioka2021independent} considers a nonlinear VAR framework where the innovations are not necessarily additive, and proposes estimation algorithms and identifiability results based on the assumption that the innovations are independent.

%In \cite{nassif2021automatic}, autoregressive modeling applied to electroencephalography (EEG) sleep-stage classification, and a novel criterion for automatic order selection is demonstrated.

%In \cite{zhou2021parameter}, a ML algorithm is proposed for estimation of VAR models with non-Gaussian distribution (specifically Student's $t$) with the additional challenge of missing data.

% Cite Rohan's online nonlinear topology ID

All the aforementioned nonlinear modeling techniques are based on estimating nonlinear functions that predict the future time series values in the measurement space, which entails high complexity and is not amenable to predicting multiple time instants ahead. The main contribution of our work is a modeling assumption that accounts for mild nonlinear relations that are independent of the (linear) multivariate structure, and reduces the complexity associated with long-term predictions, as explained in detail in Sec. \ref{sec:modelling}.
