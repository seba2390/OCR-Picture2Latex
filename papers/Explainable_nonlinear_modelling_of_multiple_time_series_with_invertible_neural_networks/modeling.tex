\section{Modelling}
\label{sec:modelling}

The linear coefficients in (1) are tailored to assessing only linear mediating dependencies. To overcome this
limitation, this work considers a non-linear model by introducing a set of node dependent nonlinear functions $\{f_i\}_{i=1}^{N}$. 
Previous works on nonlinear topology identification \cite{shen2019nonlinear,tank2017interpretable,money2021online} estimate nonlinear multivariate models without necessarily assuming linear dependencies in an underlying space; rather, they directly estimate non-linear functions from and into the real measurement space without assuming an underlying structure. In our work, we assume that the multivariate data can be explained as the nonlinear output of a set of observation functions $\{f_i\}_{i=1}^{N}$ with a VAR process as an input. Each function $f_i$ represents a different non-linear distortion at the $i$-th node.

Given data time series, the task is to jointly learn the non-linearities together with a VAR topology in a feature space which is linear in nature, where the outputs of the functions $\{f_i\}_{i=1}^{N}$ belong to. Such functions are required to be invertible, so that sensor measurements can be mapped into the latent feature space, where the linear topology (coefficients) can be used to generate predictions, which can be taken back to the real space through $\{f_i\}_{i=1}^{N}$. In our model, prediction involves the composition of several functions, which can be modeled as neural networks. The nonlinear observation function at each node can be parameterized by a NN that is in turn a universal function approximator \cite{cybenko1989approximation}. 
%\textcolor{red}{ADD REFERENCE TO UNIVERSAL REPRESENTATION THEOREM}. 
Consequently, the topology and non-linear per-node transformations can be seen in aggregation as a DNN, and its parameters can be estimated using appropriate deep learning techniques.

\begin{figure}[h]
\vspace{-0.6cm}
\centering
\includegraphics[width=0.8\textwidth]{figures/lowres/circle_only.jpg}
\vspace{-0.5cm}
\caption{{Causal dependencies among a set of time series are linear in the latent space represented by the green circle. However, the variables in the latent space are not available, only nonlinear observations (output of the functions $f_i$) are available.}\vspace{-0.3cm}} 
\label{fig:swn3}
\end{figure}


The idea is illustrated in Figure \ref{fig:swn3}. The green circle represents the underlying latent vector space. The exterior of the circle is the space where the sensor measurements lie, which need not be a vector space. The blue lines show the linear dependency between the time series inside the latent space. The red line from each time series shows the transformation to the measurement space. Each sensor is associated with a different nonlinear function. Specifically, if $y_i[t]$ denotes the $i$-th time series in the latent space, the measurement (observation) is modeled as $z_{i}[t]=f_{i}\left(y_{i}[t]\right)$. The function $f_i$  is parameterized as a neural network layer with $M$ units, expressed as follows:
\begin{align} 
\label{eq:f_model}
    f_{i}\left(y_{i}\right)=\sum_{j=1}^{M} \alpha_{ij} \sigma\left(w_{ij}y_{i}-k_{ij}\right)+b_{i}
\end{align}

For the function $f_i$ to be monotonically increasing (which guarantees invertibility), it suffices to ensure that $\alpha_{ij}$ and  $w_{ji}$ are positive $\forall j$. The pre-image of $f_i$ is the whole set of real numbers, but the image is an interval $(\ubar{z}_i, \bar{z}_i)$, which is in accordance ro the fact that sensor data are usually restricted to a dynamic range. If the range is not available a priori but sufficient data is available, bounds for the operation interval can be easily inferred.

Let us remark three important advantages in the proposed model: 
\begin{itemize}
    \item It is substantially more expressive than the linear model, while capturing non-linear dependencies with lower complexity than other non-linear models. 
    \item It allows to predict with longer time horizons ahead within the linear latent space. Under a generic non-linear model, the variance of a long-term prediction explodes with the time horizon.
    \item Each non-linear nodal mapping can also adapt and capture any possible drift or irregularity in the sensor measurement, thus, it can directly incorporate imperfections in the sensor measurement itself due to, e.g. lack of calibration.    
\end{itemize}

\subsection{Prediction}

\label{sec:prediction}

Given accurate estimates of the nonlinear functions $\{f_i\}_{i=1}^N$, their inverses, and the parameters of the VAR model, future measurements can be easily predicted. Numerical evaluation of the inverse of $f_i$ as defined in \eqref{eq:f_model} can easily be done with a bisection algorithm. 

Let us define $g_i = f_i^{-1}$. Then, the prediction consists of three steps, the first one being mapping the previous samples back into the latent vector space:
\begin{subequations}
    \begin{equation} \label{eq:forward_g}
        \tilde{y}_{i}[t-p] =g_{i}\left(z_{i}[t-p]\right)
    \end{equation}
Then, the VAR model parameters are used to predict the signal value at time t (also in the latent space):
    \begin{equation} \label{eq:forward_A}
        \hat{y}_{i}[t] =\sum_{p=1}^{p}\sum_{j=1}^{n}  a_{i j}^{(p)} \tilde{y}_{j} [t-p] 
    \end{equation}
Finally, the predicted measurement at each node is obtained by applying $f_i$ to the latent prediction:
    \begin{equation} \label{eq:forward_f}
        \hat{z}_{i}[t] =f_{i}\left(\hat{y}_{i}[t]\right) 
    \end{equation}
\end{subequations}

\begin{figure}[t]
\vspace{-1.7cm}
\hspace{-1.2cm}
\vspace{-0cm}
\includegraphics[width=1.2\textwidth]{figures/network.png}
\vspace{-2.8cm}
\caption{{ Schematic for modeling Granger causality for a toy example with 2 sensors. }} 
\label{fig:example_2sensors}
\end{figure}

These prediction steps can be intuitively visualized as a neural network. The next section formulates an optimization problem intended to learn the parameters of such a neural network. For a simple example with 2 sensors, the network structure is shown in Figure \ref{fig:example_2sensors}.

\section{Problem formulation}

The functional optimization problem consists in minimizing $\|{z}[t]-\hat z\|_{2}^{2}$ (where $z[t]$ is a vector collecting the measurements for all N sensors at time $t$), subject to the constraint of $f_i$ being invertible $\forall i$, and the image of $f_i$ being $(\ubar{z}_i, \bar{z}_i)$. The saturating values can be obtained from the nominal range of the corresponding sensors, or can be inferred from data. 
 
 Incorporating equation (1),  the optimization problem can be written as:
\begin{subequations}
\label{eq:optimization_problem}
\begin{align}
    \min _{f, A} \;\;& \| {z}[t]-f\Big(\sum_{p=1}^{p} A^{(p)}\big[g(z[t-p])\big]\Big) \|_{2}^{2} 
    \\
    \textrm{s. to:}\;\  
    & \sum_{j=1}^{M}\alpha_{j i} 
      = \bar{z}_i %max(z_{i}[t-p])
        - \ubar{z}_i %min(z_{i}[t-p]) 
        \; \forall i
        \label{eq:constraint_alpharange}
    \\
    & b_{i} = \bar{z}_i%min(z_{i}[t-p]) 
        \; \forall i
        \label{eq:constraint_barz}
    \\
    & \alpha_{j i} \geq0 
        \; \forall i, j
        \label{eq:constraint_alphapos}
        \\
    & w_{j i} \geq0 
        ; \forall i, j
        \label{eq:constraint_wpos}
\end{align}
 \end{subequations}
% \textcolor{red}{comment about the max and min saying that the function f saturates at z bar and  below if you know a priori you impose  directly or else infer from the data}     
         
The functional optimization over $f_i$ is tantamount to optimizing over $\alpha_{j i}$,$w_{j i}$,  $k_{j i}$ and $b_{i}$. The main challenge to solve this problem is that there is no closed form for the inverse function $g_i$. This is addressed in the ensuing section.

\iffalse
\begin{lstlisting}
// Algorithm for function g implementation
def g(x,i):
  niter = 10000
  vy = 0
  for j in range(niter):
    vy = vy - (ginverse(vy,i)-x)/gprime(vy,i)  
  return vy
def gprime(x,i): 
  a=0.01
  for j in range(m):
            a = a + alpha[j][i] * sigmoid(x-k[j][i]) * (1-sigmoid(x-k[j][i])) 
  return a
def ginverse(x,i): 
  a = 0   
  for j in range(m):
      a = a + alpha[j][i] * sigmoid(x-k[j][i]) + b[i][0]
  return a
\end{lstlisting}
\fi



%\subsection{Implementation as a neural network}



\section{Learning algorithm}

Without a closed form for $g$, we cannot directly obtaining gradients with automatic differentiation such as Pytorch,%\footnote{One could try to differentiate through all iterations of the Newton method that numerically inverts $f_i$, but there is no guarantee of obtaining a stable gradient estimate, and the computational cost is probably high.}
as is typically done in deep learning with a stochastic gradient-based optimization algorithm. Fortunately, once $\{g_i(\cdot)\}$ is numerically evaluated, the gradient at that point can be calculated with a relatively simple algorithm, derived via implicit differentiation in Sec. \ref{ss:backprop}. Once that gradient is available, the rest of the steps of the backpropagation algorithm are rather standard.

\subsection{Forward equations}
The forward propagation equations are given by the same steps that are used to predict next values of the time series $z$:

\begin{subequations}
    \begin{equation} \label{eq:forward_g}
        \tilde{y}_{i}[t-p] =g_{i}\left(z_{i}[t-p], \theta_{i}\right)
    \end{equation}
    \begin{equation} \label{eq:forward_A}
        \hat{y}_{i}[t] =\sum_{p=1}^{p}\sum_{j=1}^{n}  a_{i j}^{(p)} \tilde{y}_{j} [t-p] 
    \end{equation}
    \begin{equation} \label{eq:forward_f}
        \hat{z}_{i}[t] =f_{i}\left(\hat{y}_{i}[t], \theta_{i}\right) 
    \end{equation}
    \begin{equation} \label{eq:forward_cost}
        C[t] =\sum_{n=1}^{N}\left(z_{n}[t]-\hat{z}_{n}[t]\right)^{2}_\cdot
    \end{equation}
\end{subequations}
Here, the dependency of the nonlinear functions with the neural network parameters is made explicit, where $$\theta_{i}=\left[\begin{array}{l}\alpha_{ i} \\  w_i\\ k_{ i} \\ b_{i}\end{array}\right] \text{ and } 
\alpha_{i}=\left[\begin{array}{c}
\alpha_{i 1} \\
\alpha_{i 2} \\
\vdots \\
\alpha_{i M}
\end{array}\right],w_{i}=\left[\begin{array}{c}
k_{i 1} \\
k_{i 2} \\
\vdots \\
k_{i M}
\end{array}\right], k_{i}=\left[\begin{array}{c}
k_{i 1} \\
k_{i 2} \\
\vdots \\
k_{i M}
\end{array}\right]_. $$

\subsection{Backpropagation equations}
\label{ss:backprop}
The goal of backpropagation is to  calculate the gradient of the cost function with respect to the VAR parameters and the node dependent function parameters $\theta_i$.

%The dimension of the parameter vector $\theta_{i}$ is for the i th sensor  will be (2M+1). 
The gradient of the cost is obtained by applying the chain rule as following:
%\setcounter{equation}{9}
\begin{equation} \label{chainrule1}
\begin{array}{c}

\frac{d C[t]}{d \theta_{i}}=\sum_{n=1}^{N} \frac{\partial C}{\partial \hat{z}_{n}[t]}  \frac{\hat{z}_{n}[t]}{\partial \theta_{i}} \\
\text { where } \frac{\partial C}{\partial \hat{z}_{n}[t]}=2(\hat{z}_n[t]-z_n[t]) = S_n
\end{array}
\end{equation}


\begin{equation} \label{chainrule2}
\frac{\partial \hat{z}_{n}[t]}{\partial \theta_{i}}=\frac{\partial f_{n}}{\partial \hat{y}_{n}}  \frac{\partial \hat{y}_{n}}{\partial \theta_{i}}+\frac{\partial f_{n}}{\partial \theta_{n}}  \frac{\partial \theta_{n}}{\partial \theta_{i}} \\
\quad 
\end{equation}
$$
\text { where } \frac{\partial \theta_{n}}{\partial \theta_{i}}=\left\{
\begin{array}{l}
I, n = i \\
0, n  \neq i
\end{array}\right.
$$
Substituting equation \eqref{chainrule1} into \eqref{chainrule2} yields
\begin{equation} \label{derivativecost1}
\frac{d C[t]}{d \theta_{i}}=\sum_{n=1}^{N} S_{n}\left(\frac{\partial f_{n}}{\partial \hat{y}_{n}}  \frac{\partial \hat{y}_{n}}{\partial \theta_{i}}+\frac{\partial f_{n}}{\partial \theta_{n}}  \frac{\partial \theta_{n}}{\partial \theta_{i}}\right)_\cdot
\end{equation}
Equation\eqref{derivativecost1} can be simplified as:

\begin{equation} \label{derivativecost2}
    \frac{d C[t]}{d \theta_{i}}
    =
    S_{i}
    \frac{\partial f_{i}}{\partial \theta_{i}}
    +\sum_{n=1}^{N} S_{n}\frac{\partial f_{n}}{\partial \hat{y}_{n}}  \frac{\partial \hat{y_n}}{\partial \theta_{i}}.
\end{equation}


$\text { The next step is to derive } \frac{\partial \hat{y}_{n}}{\partial \theta_{i}} \text { and } \frac{\partial f_{i}}{\partial \theta_{i}}$ of  equation \eqref{derivativecost2}:


\begin{equation} \label{gradientyhat}
    \frac{\partial \hat{y}_{n}[t]}{\partial \theta_i}
    =
    \sum_{p=1}^{P}\sum_{j=1}^{N}  
        a_{n j}^{(p)} 
        \frac{\partial}{\partial \theta_{j}} \tilde{y}_{j}[t-p]
        \frac{\partial \theta_{j}}{\partial \theta_{i}} .
\end{equation}

With $f_{i}^{\prime}\left(z\right)= 
\frac{\partial f_i\left(z, \theta_{i}\right)}{\partial\left(z\right)}, $  
expanding $\tilde{y}_j[t-p]$ in equation \eqref{gradientyhat} changes \eqref{derivativecost2} to:

%\begin{equation} \label{derivativecost3}
%\text { So } \frac{d C[t]}{d \theta_{i}}=S_{i}\left(\frac{\partial f_{i}}{\partial \theta_{i}}\right)+\sum_{p=1}^{P} \sum_{n=1}^{N} S_{n} f_{n}^{\prime}(\hat{y}_n[t])  a_{n i}^{(p)} \frac{\partial}{\partial \theta_{i}} g_{i}\left(z_{i}[t-p],\theta_{i}\right)_\cdot
%\end{equation}

%equation \eqref{derivativecost3} becomes:

\begin{equation} \label{derivativecost4}
 \frac{d C[t]}{d \theta_{i}}=S_{i}\left(\frac{\partial f_{i}}{\partial \theta_{i}}\right)+\sum_{n=1}^{N} S_{n}\left(f_{n}^{\prime}(\hat{y}_{n}[t]) \sum_{p=1}^{P} a_{n i}^{(p)} \frac{\partial}{\partial \theta_{i}} g_{i}\left(z_{i}[t-p],\theta_{i}\right)\right)_\cdot
\end{equation}

Here, the vector 
$$\frac{\partial f_i\left(z, \theta_{i}\right)}{\partial \theta_{i}} %\text{ in equation } \eqref{derivativecost4} 
= \left[
\frac{\partial f_i\left(z, \theta_{i}\right)}{\partial \alpha_{i}} 
\frac{\partial f_i\left(z, \theta_{i}\right)}{\partial w_{i}} 
\frac{\partial f_i\left(z, \theta_{i}\right)}{\partial k_{i}} 
\frac{\partial f_i\left(z, \theta_{i}\right)}{\partial b_{i}} \right]$$ can be obtained by standard or automated differentiation via, e.g., Pytorch \cite{NEURIPS2019_9015}.


However, \eqref{derivativecost4} involves the calculation of $\frac{\partial g_i(z, \theta_{i})}{\partial \theta_{i}}$, which is not straightforward to obtain. Since $g_i(z)$ can be computed numerically, the derivative can be obtained by implicit differentiation, realizing that the composition of $f_i$ and $g_i$ remains invariant, so that its total derivative is zero:

\begin{equation} \label{dfwrttheta1}
\frac{d}{d \theta_{i}}\left[f_i\left(g_i\left(z, \theta_{i}\right), \theta_{i}\right)\right]=0
\end{equation}

\begin{equation} \label{dfwrttheta2}
\Rightarrow \frac{\partial f_i\left(g_i\left(z, \theta_{i}\right), \theta_{i}\right)}{\partial g\left(z, \theta_{i}\right)} \frac{\partial g\left(z, \theta_{i}\right)}{\partial \theta_{i}}+\frac{\partial f_i\left(z, \theta_{i}\right)}{\partial \theta_{i}}=0
\end{equation}

\begin{equation} \label{dfwrttheta3}
\Rightarrow {f^{\prime}_i(g_i(z,\theta_{i}))} \frac{\partial g\left(z, \theta_{i}\right)}{\partial \theta_{i}}+\frac{\partial f_i\left(z, \theta_{i}\right)}{\partial \theta_{i}}=0
\end{equation}


\begin{equation} \label{dgwrttheta1}
\text { Hence } \frac{\partial g_i\left(z, \theta_{i}\right)}{\partial \theta_{i}}=
-\big\{f^{\prime}_i(g_i(z,\theta_{i}))\big\}^{-1}{\left(\frac{\partial f_i\left(z, \theta_{i}\right)}{\partial \theta_{i}}\right)}_\cdot 
\end{equation}
%
%$$\text{where }\frac{\partial g_i\left(z, \theta_{i}\right)}{\partial \theta_{i}} =  \left[\begin{array}{l}
%\frac{\partial g_i\left(z, \theta_{i}\right)}{\partial \alpha_{i}} \\
%\frac{\left.\partial g_i\left(z\right, \theta_{i}\right)}{\partial k_{i}} \\
%\frac{\partial g_i(z,\theta_{i})}{\partial b_{i}}
%\end{array}\right]_\cdot$$
%

The gradient of $C_T$ w.r.t. the VAR coefficient $a^{(p)}_{ij}$ is calculated as follows: 

\begin{equation} \label{dCwrtthetaa}
\frac{d C[t]}{d a^{(p)}_{i j}}=\sum_{n=1}^{N} S_{n}  \frac{\partial f_{n}}{\partial \hat y_{n}}  \frac{\partial \hat{y}_{n}}{\partial a_{i j}^{(p)}}
\end{equation}

\begin{equation} \label{dCwrtthetaa1} \nonumber
\frac{\partial \hat{y}_{n}[t]}{\partial a_{i j}^{(p)}}=\frac{\partial}{\partial a_{i j}^{(p)}} \sum_{p^\prime=1}^{P} \sum_{q=1}^{N} a_{n q}^{(p^\prime)} \tilde{y}_{q}[t-p]
\end{equation}

\begin{equation} \label{dCwrtthetaa2}
\begin{array}{c}
\text { where } 
\frac{\partial a_{n q}^{(p^\prime)}}{\partial a_{i j}^{(p)}}=\left\{\begin{array}{l}
1, n=i, p = p^\prime, \text { and } q=j \\
0, \text {otherwise}
\end{array}\right.
\end{array}
\end{equation}

%\begin{equation} \label{dCwrtthetaa3}
%\frac{\partial \hat{y}_{n}[t]}{\partial a_{i j}^{(p)}}=\left\{\begin{array}{c}
%\sum_{p=1}^{P} \tilde{y}_{j}[t-p], i=n \\
%0, \quad i \neq n
%\end{array}\right.
%\end{equation}

\begin{equation} \label{dCwrtthetaa4}
\frac{d C[t]}{d a_{i j}^{(p)}}=S_i f_{i}^{\prime}\left(\hat{y}_{i}[t]\right) \tilde{y}_{j}[t-p]_\cdot 
\end{equation}

Even though the backpropagation cannot be done in a fully automated way, it can be realized by implementing equations \eqref{dgwrttheta1} and \eqref{derivativecost4} after automatically obtaining the necessary expressions.

\subsection{Parameter optimization}

The elements in $\{A^{(p)}\}_{p=1}^P,$ and $\{\theta_i\}_{i=1}^{N}$ can be seen as the parameters of a NN. Recall from Fig. \ref{fig:example_2sensors} that the prediction procedure resembles a typical feedforward NN as it interleaves component-wise nonlinearities with multidimensional linear mappings. The only difference is that one of the layers computes the inverse of a given function, and its backward step has been derived. Moreover, the cost function in \eqref{eq:optimization_problem} is the mean squared error (MSE).

The aforementioned facts support the strategy of learning the parameters using state-of-the-art NN training techniques. A first implementation has been developed using stochastic gradient descent (SGD) and its adaptive-moment variant Adam \cite{kingma2014adam}. Constraints \eqref{eq:constraint_alpharange}-\eqref{eq:constraint_wpos} are imposed by projecting the output of the optimizer into the feasible set at each iteration.

The approach is flexible enough to be extended with neural training regularization techniques such as dropout %\cite{dropout} 
or adding a penalty based on the L1 or L2 norm of the coefficients, to address the issue of over-fitting and/or promote sparsity. The batch normalization technique can be proposed to improve the training speed and stability. %Last but not least, future developments include improving the interpretability by imposing sparsity. 

\iffalse
\begin{lstlisting}
// Algorithm for back propagation implementation
def backward_propagation(H,alpha,b,A,k,X_train, z_pred):
for i in range(n):
          for p in range(n):
              a1=0
              for j in range(len(X_train)):
                  a1  = a1-2*(X_train[j][p] - z_pred[j][p])*(g(X_train[j][i],i))*(f(H[j][p],p))
              dA[i][p] =a1
    for i in range(n): 
        for p in range(n):                        
            for j in range(len(X_train)):
                dalpha[j][i] = dalpha[j][i] -2*(X_train[j][i] - z_pred[j][i])  *(f(H[j][p],p)*A[i][p]*dalphag(X_train[j][i],i)) -2*(X_train[j][i] - z_pred[j][i])*sigmoid(H[j][i]-k[j][i])
    for i in range(n): 
        for p in range(n):                       
            for j in range(len(X_train)):
                dk[j][i] = dk[j][i] -2*(X_train[j][i] - z_pred[j][i]) * (f(H[j][p],p)*A[i][p]*dkg(X_train[j][i],i)) -2*(X_train[j][i] - z_pred[j][i])*alpha[j][i]*sigmoid(H[j][i]-k[j][i])
                *(1-sigmoid(H[j][i]-k[j][i]))
    for i in range(n): 
        a1 = 0
        for p in range(n):                        
            for j in range(len(X_train)):
                a1  = a1 -2*(X_train[j][i] - z_pred[j][i])  *(f(H[j][p],p)*A[i][p]*dbg(X_train[j][i],i)) -2*(X_train[j][i] - z_pred[j][i])*alpha[j][i]*sigmoid(H[j][i]-k[j][i])
        a1 = db[i][0]            
    return 0

\end{lstlisting}
\fi