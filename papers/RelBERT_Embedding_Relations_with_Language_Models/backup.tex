
% \begin{figure}[!t]
%     \centering
%     \begin{subfigure}[b]{0.25\columnwidth}
%         \centering
%         \includegraphics[width=\columnwidth]{figure/case_reasoning.png}
%         \caption{Case-based reasoning}
%     \end{subfigure}
%     \hspace{1cm}
%     \begin{subfigure}[b]{0.55\columnwidth}
%         \centering
%         \includegraphics[width=\columnwidth]{figure/ana_reasoning.png}
%         \caption{Analogical reasoning}
%     \end{subfigure}
%     \caption{A comparisons of reasoning a phrase ``note'' between (a) case-based reasoning and (b) analogical reasoning. }
%     \label{fig:relbert:case-analogical-reasoning}
% \end{figure}

% Analogies (or relational similarity) plays a central role in computational creativity \cite{goel2019computational}, legal reasoning \cite{ashley1988arguing,walton2010similarity} and instance-based learning \cite{miclet2008analogical}. A specific reasoning ability enabled by understanding analogies, is analogical reasoning \cite{prade2017analogical}, where one can recognize a concept from different perspective through the analogies\nb{This sentence is very vague; I don't get what this is trying to say.}. \autoref{fig:relbert:case-analogical-reasoning} shows the comparisons of analogical reasoning against case-based reasoning that characterizes the concept only by its nearest neighbour in a semantic representation space\nb{It wasn't clear to me why we are making this comparison with case-based reasoning. If we want to keep this, we need a much more detailed explanation of analogical reasoning and case based reasoning. Also, I'm not convinced it makes sense to contrast the two against each other, since analogies can be used as part of a case based reasoning system.}. 


%Language models are pre-trained on a large scale corpus in an unsupervised way such as causal language modeling \cite{GPT}, masked language modeling \cite{devlin-etal-2019-bert}, or denoising auto-encoder \cite{2020t5}), and then with fine-tuning or zero/few-shot learning, LMs leverage the inductive bias learnt through the pre-training implicitly to solve the target NLP tasks. As the LMs rely on a deep layers of transformer-based architecture, The internal representations of LMs are usually not directly interpretable, and LMs are rather treated as black box. Due to such lack of understanding to the knowledge in LMs, although LMs can significantly improve the traditional baseline in many NLP tasks, one can generate adversarial attacks on LMs to fool the prediction \cite{jin2020bert,welbl-etal-2020-undersensitivity,morris-etal-2020-textattack}. It has been also shown that the prediction of fine-tuned LMs can be biased by a specific set of phrases, and even one can switch the prediction by adding those phrases in the input sentence \cite{wallace-etal-2019-universal,song-etal-2021-universal}. Another example is the spurious correlation observed in LM fine-tuning \cite{clark-etal-2019-dont,tu-etal-2020-empirical,wang-culotta-2020-identifying}, where LMs prefers to rely on a correlation with surface feature of the input text than to capture semantics in the input text at solving downstream task, when the data size is limited and the size of the LM is small. In causal LMs, it has been known that LMs can generate counter-factual \cite{wang-sennrich-2020-exposure}, or repetitive words \cite{Holtzman2020The} in some edge cases such as out-of-domain, which is empirically proved as partially because of the exposure bias caused by the teacher forcing loss objective at training causal LMs \cite{Welleck2020Neural}.
% \nb{I removed the first section, since it wasn't really clear how the works discussed in that section related to this paper. We should try to extend the related work section, but it still needs to remain focused on relational knowledge.} 

%trained on fundamental NLP tasks such as semantic parsing \cite{hewitt-manning-2019-structural} or natural language inference \cite{mccoy-etal-2019-right}, on the desired feature from LMs which can be the output embedding or attention matrix for example. 
%The choice of classifiers and the metric to measure the knowledge given the classifier depends on the methods but typically the criterion is based on the accuracy \cite{hewitt-manning-2019-structural,pimentel-etal-2020-information} or the minimum description length of the model \cite{voita-titov-2020-information}.
%More recently, large LMs are scaled to more than a few billions of parameters \cite{gpt-j,zhang2022opt,chowdhery2022palm,https://doi.org/10.48550/arxiv.2210.11416,tay2023ul2,GPT3,rae2021language,hoffmann2022empirical}, and 

% \subsection{The Choice of Aggregation Functions}\label{sec:the-choice-of-aggrgegation}

% \begin{table}[!t]
% \centering
% \begin{tabular}{lccc}\toprule
%            & \emph{average w.o. mask} & \emph{mask}   & \emph{average} \\\midrule
% SAT        & 59.9                     & \textbf{62.0} & 59.1           \\
% U2         & 59.6                     & \textbf{62.7} & 55.7           \\
% U4         & 57.4                     & \textbf{59.3} & 56.3           \\
% BATS       & 70.3                     & \textbf{72.1} & 68.6           \\
% Google     & 89.2                     & \textbf{91.2} & 88.0           \\
% SCAN       & \textbf{25.9}            & 25.2          & 25.2           \\
% NELL       & 62.0                     & \textbf{67.2} & 63.8           \\
% T-REX      & \textbf{66.7}            & \textbf{66.7} & 65.6           \\
% ConceptNet & 39.8                     & \textbf{43.9} & 39.4           \\ \midrule
% Average    & 59.0                     & \textbf{61.1} & 58.0           \\ \midrule
% BLESS      & 90.0                     & \textbf{91.4} & 90.0           \\
% CogALexV   & 83.7                     & \textbf{85.8} & 82.7           \\
% EVALution  & 64.2                     & \textbf{67.1} & 64.6           \\
% K\&H+N      & 94.0                     & \textbf{94.1} & 94.1           \\
% ROOT09     & 88.2                     & \textbf{89.9} & 87.5           \\ \midrule
% Average    & 84.0                     & \textbf{85.6} & 83.8          \\ \bottomrule
% \end{tabular}
% \end{table}


% \subsubsection{Batch Size of InfoNCE}\label{sec:batch-size-of-infonce}

% \begin{table}[t]
\centering
\begin{tabular}{lrrrr}
\toprule
Batch Size &   100 &   200 &   300 &   400 \\
\midrule
\multicolumn{4}{l}{\textit{Analogy Questions}}                                \\
SAT                         &  \textbf{74.3} &  73.0 &  70.1 &  73.3 \\
U2                               &  62.7 &  65.4 &  \textbf{70.6} &  67.5 \\
U4                               &  62.3 &  \textbf{64.1} &  63.0 &  63.0 \\
BATS                             &  76.1 &  76.8 &  78.9 &  \textbf{80.9} \\
Google                           &  92.0 &  94.0 &  94.0 &  \textbf{95.2} \\
SCAN                             &  25.6 &  26.5 &  26.6 &  \textbf{27.2} \\
NELL       &  65.2 &  \textbf{65.8} &  63.8 &  \textbf{65.8} \\
T-REX      &  63.4 &  61.7 &  62.8 &  \textbf{64.5} \\
ConceptNet &  40.9 &  43.0 &  44.3 &  \textbf{47.5} \\ \midrule
Average                     &  62.5 &  63.4 &  63.8 &  \textbf{65.0} \\ \midrule
\multicolumn{4}{l}{\textit{Lexical Relation Classification}}                  \\
BLESS                            &  91.1 &  \textbf{91.7} &  91.5 &  \textbf{91.7} \\
CogALexV                         &  59.2 &  \textbf{69.3} &  67.7 &  67.4 \\
EVALution                        &  66.4 &  65.5 &  \textbf{68.4} &  67.8 \\
K\&H+N                            &  \textbf{88.2} &  87.1 &  86.8 &  86.6 \\
ROOT09                           &  89.9 &  88.7 &  \textbf{90.4} &  \textbf{90.4} \\ \midrule
Average                         &  79.0 &  80.5 &  \textbf{81.0} &  80.8 \\
\bottomrule
\end{tabular}
\caption{The results on analogy questions (accuracy) and lexical relation classification (micro F1 score) with different batch size of InfoNCE, where the best result across models in each dataset is highlighted by bold characters. }
\label{tab:infonce-batch-size}
\end{table}

% \subsubsection{Classification Loss}
% Following SBERT \citep{reimers-gurevych-2019-sentence}, we use a classifier to predict whether two word pairs belong to the same relation. 
% The classifier is jointly trained with the LM using the negative log likelihood loss function:
% \begin{align}
%     L_{{\rm cls}} = -\log(g(\bm{x}_a, \bm{x}_p)) - \log(1-g(\bm{x}_a, \bm{x}_n))
% \end{align}
% where
% \begin{align}
%     g(\bm{u}, \bm{v}) = \text{sigmoid}(W \cdot \left[ \bm{u} \oplus \bm{v} \oplus |\bm{v} - \bm{u}| \right]^{T} )
% \end{align}
% with $W\in \mathbb{R}^{3\times d}$, $\bm{u},\bm{v}\in \mathbb{R}^d$, $|\cdot|$ the element-wise absolute difference, and $\oplus$ concatenation.
% Classification loss itself is not used alone but together with either of Triplet loss, InfoNCE or InfoLOOB.


% \subsubsection{Analogical Proportion Score}\label{sec:analogical-proportion-score}
% Analogical proportion score (AP score) \citep{ushio-etal-2021-bert} is a general framework to solve an analogy question by LMs.
% Given a query word pair and a candidate word pair, first AP score transforms those four words (the member of the query and the candidate word pairs) into a sentence with a template, and compute perplexity over the sentence.
% As a property of analogical proportion, there are positive and negative permutations of those four words, where the positive permutations can keep the analogy, while the negative permutations changes the relation \citep{prade2017analogical,barbot2019analogy}, so the AP score takes the relative improvement of the perplexity from the negative to the positive permutations instead of the absolute perplexity to reduce the effect of high frequency words. The candidate with the best AP score is then chosen as the prediction.
% \subsection{Baselines for Relation Mapping Problem}\label{sec:baseline-for-relation-mapping-problem}
% Latent relation mapping engine (LRME) \citep{turney2008latent} is an extension of LRA to solve the relation mapping problem that is define as
% \begin{align}
%     Y = \argmax_{\tilde{Y} \in {\rm P}(B)} \sum_{(i, j) \in \mathcal{I}} 
%     \left( {\rm cos} (r^{'}(a_i, a_j), r^{'}(\tilde{y}_i, \tilde{y}_j)) + p(a_i, \tilde{y}_i) + p(a_j, \tilde{y}_j) \right)
% \end{align}
% where $r^{'}$ is the LRA word pair embedding and $p$ is 1 if the part-of-speech tags of two inputs are same, and otherwise 0.

% \subsection{Prompts Learning}
% Instead of using a manual template, prompting can be optimized through various ways such as mining patterns from a corpus \citep{bouraoui2020inducing}, paraphrasing \citep{jiang-etal-2020-know},
% training an additional LM for template generation \citep{haviv-etal-2021-bertese,gao2020making},
% exploring template that minimize a loss objective by discrete optimization \citep{shin-etal-2020-autoprompt},
% and learning embeddings directly for the optimal template \citep{shin-etal-2020-autoprompt,liu2021gpt}.
% In our work, we focus on AutoPrompt \citep{shin-etal-2020-autoprompt} and P-tuning \citep{liu2021gpt}, given its conceptual simplicity and its strong performance reported on various benchmarks.
% Note that both methods rely on training data and no extra corpus is needed unlike other prompt optimization. We will use the same training data and loss function that we use for fine-tuning the LM; see Section \ref{sec:finetuning}.

% \subsubsection{AutoPrompt}
% AutoPrompt initializes the prompt as a fixed-length template: 
% \begin{align}
%     T = (z_1, \dots, z_{\pi}, \textbf{[h]}, z_{\pi +1}, \dots, z_{\pi + \beta}, \textbf{[t]}, z_{\pi + \beta + 1}, \dots, z_{\pi + \beta + \gamma} )
% \end{align}
% where $\pi$, $\beta$, and $\gamma$ are hyper-parameters, which determine the length of the template.
% The tokens of the form $z_i$ are called trigger tokens, which are initialized as \texttt{<mask>}.
% The method then iteratively finds the best token to replace each mask, based on the gradient of the task-specific loss function.
% We note that in most implementations of AutoPrompt, the vocabulary to sample trigger tokens is restricted to that of the training data. However, given the nature of our training data (i.e., pairs of words and not sentences), we consider the full pre-trained LM's vocabulary.

% \subsubsection{P-tuning} 
% P-tuning employs the same template initialization as AutoPrompt but its trigger tokens are newly introduced special tokens with trainable embeddings $\bm{\hat{e}}_{1:\pi + \beta + \gamma}$, which are learned using a task-specific loss function while the weights of LMs are frozen.
% To enable interactions across positions, a stacked bidirectional LSTM is used to generate the embeddings, i.e.:
% \begin{align}
%     \bm{\hat{e}}_{1:\pi + \beta + \gamma} = \text{LSTM}(\bm{\hat{e}^{'}}_{1:\pi + \beta + \gamma})
% \end{align}
% The input vectors $\bm{\hat{e}^{'}}_{1:\pi + \beta + \gamma}$ are jointly trained with the parameters of the LSTM.
% Once this training process is complete, we use the vectors $\bm{\hat{e}}_{1:\pi + \beta + \gamma}$ as the embeddings of the trigger tokens in the learned template.
% After these trigger embeddings have been learned, we thus no longer need the LSTMs.


% % $\in\mathcal{C}$
% % $r_{\mathcal{C}} \in \mathcal{C} \subset \mathcal{R}$
% % $r\in\mathcal{R}$ where $\mathcal{R}$ is the set of all relation types.
% The RelBERT training dataset is constructed by word pairs with top/middle/low-level relation comparisons from the relation similarity dataset that correspond to the granularity in the contrast of relations in the dataset.
% Let us define $\mathcal{P} \in \mathcal{C}$ as a parent relation category, % and $r_{\mathcal{P}}$ as a relation type in $\mathcal{P}$.
% and an anchor word pair $(h,t) \in r \in \mathcal{P}$.
% The top-level relation consists of positive word pairs from the same parent category
% $(h_p, h_p) \in r_p \in \mathcal{P}$ 
% and negative word pairs from other parent categories 
% $(h_n, h_n) \in r_n \in \hat{\mathcal{P}}, \hat{\mathcal{P}} \neq \mathcal{P}$.
% Then, the middle-level relation is based on the same parent relation as the anchor word pair, but the positive word pairs are from the same relation type $r\in\mathcal{P}$, while the negative word pairs are from different relation types under the same parent category $r_n\in\mathcal{P}, r_n \neq r$.
% Finally, the low-level relation focuses on the most fine-grained relation types, where both of positive and negative word pairs are from the same relation type $r$ as the anchor word pair, but we compare the word pairs with higher typicality score and ones with lower score.
% Assuming the rank of the anchor word pair, positive word pair, and negative word pair within $r$ as $m$-th, $m_p$-th, and $m_p$-th, the rank of anchor word pair should at least satisfies $m\leq 5$, and the rank of positive word pair should have a positive typicality score and satisfies $m < m_p$, while the rank of negative word pair is taken from word pairs with a rank lower than $m_p + 10 \leq m_n$.


% Optionally, the training dataset can have explicit negative word pairs that do not belong to the relation type, which can be added to the negative samples at computing loss value.
