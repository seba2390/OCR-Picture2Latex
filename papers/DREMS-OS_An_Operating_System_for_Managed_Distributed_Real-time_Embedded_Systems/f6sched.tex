%!TEX root = main.tex
% $Id: f6sched.tex 3898 2013-10-22 04:52:42Z csanad $
%\section{\iap\ OS Scheduler}
%\label{sec:scheduler}
%This section delves into the details of the \iap\ OS scheduler and its
%capabilities.

\subsection{Partitioning Support}
\label{sec:scheduler}
The system guarantees spatial isolation between actors  by (a)
providing  a separate address space for each actor; (b) enforcing that an
I/O device can be accessed by only one actor at a time; and (c)
facilitating temporal isolation between processes by the scheduler.
Spatial isolation is implemented by the Memory Management Unit of the CPU,  
while temporal isolation is provided via ARINC-653~\cite{ARINC-653} style
\textit{temporal partitions}, implemented in the OS scheduler. 

%-- a periodically repeating fixed interval of the CPU's time exclusively assigned to a group of 
% cooperating actors of the same  application. 
% Abhishek, in the intro we have mentioned limitations with such a
% partitioning scheme and cited the very same reference. So here we need
% to say a sentence saying that there is a slight difference as
% explained in Section~\ref{sec:scheduler}.

A temporal partition is characterized by two parameters: period and
duration.  The period reflects how often the tasks of the
partition will be guaranteed CPU allocation.  The duration governs the
length of the CPU allocation window in each cycle.  Given the period
and duration of all temporal partitions, an execution schedule can be
generated by solving a series of constraints, see~\cite {ACM_SPE:10}.
A feasible solution, \emph{e.g.} Figure~\ref{fig:validschedule}, comprises
a repeating frame of windows, where each window is assigned to a
partition.  These windows are called \emph{minor frames}.  The length
of a window assigned to a partition is always the same as the duration
of that partition.  The repeating frame of minor frames, known as the \emph{major frame},
has a length called the \emph{hyperperiod}.  The hyperperiod is the lowest common multiple
of the partition periods.  


\subsection{Criticality Levels Supported by the \iap\ OS Scheduler}
\label{sec:criticality_levels}
The \iap\ OS scheduler can manage CPU's time
for tasks on three different criticality levels:
\emph{Critical}, \emph{Application} and \emph{Best Effort}.
The \emph{Critical} tasks provide kernel level services and system
management services. These tasks will be scheduled based on their
priority whenever they are ready. \emph{Application} tasks are
mission specific and are isolated from each other. These tasks are
constrained by temporal partitioning and can be preempted by tasks of the
\emph{Critical} level. Finally, \emph{Best Effort} tasks are executed
whenever no tasks of any higher criticality level are available.

Note that actors in an application can have different criticality levels,
but all tasks associated with an actor must have the same criticality
level, \emph{i.e.} an actor cannot have both \emph{Critical} tasks and
\emph{Application} tasks.

\subsection{Multiple partitions}
\label{sec:datastructure}
To support the different levels of criticality, we extend the \textit{runqueue} data structure of the Linux kernel \cite{garg2009real}. A runqueue maintains a list of tasks eligible for scheduling. %It consists of a bit array with one bit for each priority level and a list containing the tasks ready to be scheduled at that level. The bit at a level is set to one when there are tasks at that level, a 0 value indicates an empty queue at that level. 
In a multicore system, this structure is replicated per CPU. In a fully preemptive mode, the scheduling decision is made by  evaluating which task should be executed next on a CPU when an interrupt handler exits, when a system call returns, or when the scheduler function is explicitly invoked to preempt the current process.
We created one runqueue per temporal partition per CPU. Currently, the system can support 64 {\it Temporal partitions}, also referred to as Application partitions in the sequel. One extra runqueue is created for the critical tasks. These tasks are said to belong to the {\it System partition}.  The Best effort tasks are managed through the Linux Completely Fair Scheduler (default) runqueue and are considered for execution as part of the System partition when no other tasks are eligible to run. 



\subsection{CPU Cap and Work Conserving Behavior}
\label{sec:CPUCAP}
The schedulability of the \emph{Application} level tasks is
constrained by the current load coming from the \emph{Critical} tasks
and the temporal partitioning used on the \emph{Application} level.
Should the load of the \emph{Critical} tasks exceed a threshold the system
will not be able to schedule tasks on the \emph{Application} level. A
formal analysis of the response-time of the \emph{Application} level
tasks will not be provided in this paper, however, we present a 
description of the method we will use to address the analysis which
will build on available results from
\cite{BaruahRTA4MCS, PartitionedRTA-Almeida04, 
HierarchicalRTA-Lipari05}.

The submitted load function $H_i(t)$ determines the maximum load
submitted to a partition by the task $\tau_i$ itself after its
release together with all higher priority tasks belonging to
the same partition. The availability function $A_S(t)$ returns for
each time instant the cumulative computation time available for the
partition to execute tasks. In the original model~\cite{PartitionedRTA-Almeida04}
$A_S(t)$ is the availability function of a periodic server.
 The response-time of a task $\tau_i$ is the time
when $H_i(t)$ intersects the availability function $A_S(t)$ for the
first time.  In our system $A_S(t)$ is decreased by the load of the
available \emph{Critical} tasks which, if unbounded, could block the
application level tasks forever. This motivates us to enforce a bound
on the load of the \emph{Critical} tasks. This bound is referred to as
{\bf CPU cap}.
% It is our future goal to come up with a bound backed by a theoretical analysis.

In \iap\ OS, the CPU cap can be applied to tasks on the
\emph{Critical} and \emph{Application} level to provide scheduling
fairness within a partition or hyperperiod. Between criticality
levels, the CPU cap provides the ability to prevent higher
criticality tasks from starving lower criticality tasks of the CPU.
On the \emph{Application} level, the CPU cap can be used to bound
the CPU consumption of higher priority tasks to allow the execution
of lower priority tasks inside the same partition. If the CPU cap
enforcement is enabled, then it is possible to set a maximum CPU
time that a task can use, measured over a configurable number of
major frame cycles.

%In addition to partition scheduling, each application task or system task can have a
%CPU cap resource assigned to it to provide scheduling fairness within
%a partition or hyperperiod, if the CPU cap enforcement is enabled.  The CPU cap
%provides the ability to prevent high priority application tasks from
%starving lower priority tasks of the CPU.
%The enforcement of the CPU cap is work conserving within partitions. 

%For application tasks which share a temporal partition, the scheduler
%provides a work conserving ceiling on each task's utilization of the
%CPU. This ceiling, the CPU cap, restricts the task to a percentage of
%time on the CPU over a configurable period.  Tasks which share a
%partition are scheduled in a work conserving manner, \emph{i.e.} if a
%task has reached its CPU cap but there are no other runnable tasks in
%the partition, the scheduler will continue scheduling the task past
%its ceiling. Additionally, the scheduler allows the temporal
%partitioning schedule to be dynamically updated. Finally, the \iap\
%OS scheduler guarantees that resource over-utilization by a set of
%tasks in one partition will not affect tasks running in another
%partition. Additionally, the scheduler supports a maximum cpu cap
%ceiling that can be imposed on a group of critical level tasks. This
%is necessary to provide a guaranteed CPU time slot to application
%tasks. 

\iffalse
The CPU cap resource for a task is converted into a ceiling of
execution time, which is measured over $N$ major frames. The number
of major frames over which the CPU cap ceiling is calculated is
configurable at compilation time. When CPU cap is enabled, the
scheduler maintains a counter for all tasks. The scheduler also
maintains the current execution time of each task since the start of
the CPU cap window. Note that at the beginning of each CPU cap
window, the execution time of the task is reset to zero whereas the
counter is reset after the number of major frame cycles over which
the cap was specified elapses. 

The counter is used when making a scheduling decision, which requires
consideration of whether a task is ready and whether the task has
surpassed its CPU cap quota.  At every invocation of the scheduler,
the execution time for the task is updated and compared against the
execution time ceiling of the currently running task. If the task
has surpassed its CPU cap quota within the CPU cap window, its
$disabled$ flag is set to $true$.  
%This flag is evaluated for the
%currently running task when the main scheduling function is executed
%and is set when the task reaches its CPU cap.  

%For application tasks which share a temporal partition, the scheduler
%provides a work conserving ceiling on each task's utilization of the
%CPU. This ceiling, the CPU cap, restricts the task to a percentage of
%time on the CPU over a configurable period.  Tasks which share a
%partition are scheduled in a work conserving manner, \emph{i.e.} if a
%task has reached its CPU cap but there are no other runnable tasks in
%the partition, the scheduler will continue scheduling the task past
%its ceiling. Additionally, the scheduler allows the temporal
%partitioning schedule to be dynamically updated. Finally, the \iap\
%OS scheduler guarantees that resource over-utilization by a set of
%tasks in one partition will not affect tasks running in another
%partition. Additionally, the scheduler supports a maximum cpu cap
%ceiling that can be imposed on a group of critical level tasks. This
%is necessary to provide a guaranteed CPU time slot to application
%tasks. 
\fi

The CPU cap is enforced in a work conserving manner, \textit{i.e.}, 
if a task has reached its CPU cap but there are no other
available tasks, the scheduler will continue scheduling the task past
its ceiling. In case of \emph{Critical} tasks, when the CPU cap is reached,
the task is not marked ready for execution unless
(a) there is no other ready task in the system; or 
(b) the CPU cap accounting is reset.
This behavior ensures that the kernel tasks, such as those belonging
to network communication, do not
overload the system, for example in a denial-of-service attack.
For the tasks on the \emph{Application} level, the CPU cap is
specified as a percentage of the total duration of the partition,
the number of major frames, and the number of CPU cores
available all multiplied together. When an \emph{Application} task reaches
the CPU cap, it is not eligible to be scheduled again unless 
the following is true: either
(a) there are no \emph{Critical} tasks to schedule and there are no other ready tasks in the partition; or (b) the CPU cap accounting has been reset.

\subsection{Dynamic Major Frame Configuration}
\label{sec:reconfiguration}

\iffalse
This section describes the mechanism used to configure (or reconfigure during a mission) the partition
scheduler, Procedure~\ref{algo:majorframe}.
Table~\ref{table:variable} summarizes the key symbols used in this
and related subsections. 
\begin{table}[ht]
\caption{\iap\ Symbols used in Section \ref{sec:scheduler}}
\footnotesize
\begin{tabular}{| c | p{0.3\textwidth} |}
\hline
 APP\_INACTIVE &The scheduler state in which tasks in temporal partitions are not scheduled \\\hline
 APP\_ACTIVE &Inverse of APP\_INACTIVE\\\hline
$firstrun$& A global variable, set whenever the major frame has been changed\\\hline
$mfl$&A global circular linked list of minor frames used by the scheduler\\\hline
$cur\_frame$&Current minor frame.\\\hline
$HP\_start$&Global variable, stores the start time of a new major frame.\\\hline
\end{tabular}
\label{table:variable}
\end{table}
\fi


%The scheduler states are described in table \ref{table:variable}. Just after boot sequence the scheduler is in  APP\_INACTIVE state. In this state

During the configuration process that can be repeated at
any time without rebooting the node, the kernel receives the major
frame structure that contains a list of minor frames and it also
contains the length of the hyperperiod, partition periodicity, and
duration. Note that major frame reconfiguration can only be
performed by an actor with suitable capabilities.  More details on the
\iap\ capability model can be found in~\cite{ISIS_F6_Aerospace:12}.

Before the frames are set up, the process configuring the frame has to
ensure that the following three constraints are met: (C0) The
hyperperiod must be the least common multiple of partition periods;
(C1) The offset between the major frame start and the first minor
frame of a partition must be less than or equal to the partition
period:  $(\forall p \in \mathbb{P})(O_{1}^{p} \leq \phi(p))$; (C2)
Time between any two executions should be equal to the partition
period: $(\forall p \in
\mathbb{P})(k\in[1,N(p)-1])(O_{k+1}^{p}=O_{k}^{p}+ \phi(p))$, where
$\mathbb{P}$ is the set of all partitions, $N(p)$ is the number of
partitions, $\phi(p)$ is the period of partition $p$ and $\Delta(p)$
is the duration of the partition $p$. $O^p_i$ is the offset of
$i^{th}$ minor frame for partition $p$ from the start of the major
frame, $H$ is the hyperperiod. 

\iffalse
\begin{algorithm}[t]
\caption{Update Major frame}
\label{algo:majorframe}
\begin{algorithmic}[1]
\footnotesize
\INPUT $frame$ \COMMENT{A sorted but not necessarily contiguous major frame structure} 
\REQUIRE $Valid(mf)$
\STATE $Reassign ~Task ~to ~CPU ~0$
\STATE $Acquire~ update~ frame~ spin lock,~ disable~ preemption/interrupts$
\STATE $frame \leftarrow  Fill\_Empty(frame)$ 
\STATE $\bf{Atomic:}$$state \leftarrow  APP\_INACTIVE$
\STATE $ firstrun \leftarrow true$
\STATE $ mfl \leftarrow frame.minorframelist$
\STATE $\bf{Atomic:}$$state \leftarrow  APP\_ACTIVE$
\STATE $Release~ update~ frame~ spin lock,~ enable~ preemption/interrupts$
\end{algorithmic}
\end{algorithm}
\fi
%During configuration of the partition schedule, the scheduler receives a 
%list of minor frames which comprise the schedule.  These minor frames
%are checked against certain constraints, shown below, to verify they
%form a valid schedule. 
%
%\begin{description}
%\item [C0] The start for all partitions must happen before the period ends : $(\forall p \in \mathbb{P})(O_{1}^{p} \leq \phi(p))$.
%\item [C1] The start for all partitions must happen before the period ends : $(\forall p \in \mathbb{P})(O_{1}^{p} \leq \phi(p))$.
%\item [C2] Time between any two executions should be equal to partition period : $(\forall p \in \mathbb{P})(k\in [1,N(p)-1])(O_{k+1}^{p}=O_{k}^{p}+ \phi(p))$.
%\item [C3] The last start must finish before the hyperperiod ends : $(\forall p \in \mathbb{P})(O_{N(p)}^{p}+\Delta(p) \leq H)$
%\item [C4] A partition cannot overlap : $(\forall p \in \mathbb{P})(\forall z \in \mathbb{P})$$(k\in [1,N(p)])(j\in [1,N(z)])$ $(O_{k}^{p} \leq O_{j}^{z} \implies O_{j}^{z} \geq O_{k}^{p} +\Delta(p))$
%\end{description}
%
%
%
% Note that the minor frames need not be contiguous,
%as the algorithm, Procedure~\ref{algo:majorframe}, fills in any gaps 
%automatically.  
%
%Let $\mathbb{P}$ be the set of all partitions in a node. Let $\phi(p) \in \mathbb{R} \cap [0,\infty) $ denote the period of partition $p \in \mathbb{P}$. Let $\Delta(p) \in \mathbb{R} \cap [0,\phi(p)]$ denote the duration of time that a partition needs to be executed every  $\phi(p)$ time units. Then hyperperiod $H$ is given as $H=LCM(\phi(\mathbb{P}))$\footnote{Here  $\phi(\mathbb{P})$ is a used as a succinct  representation of set $\{x|x=\phi(p) \wedge p \in \mathbb{P} \}$. We will use this short representation for other sets also. }, where LCM is the abbreviation for the least common multiple.  The constraints for a valid scheduler are
%
%\begin{description}
%\item [C1] The start for all partitions must happen before the period ends : $(\forall p \in \mathbb{P})(O_{1}^{p} \leq \phi(p))$.
%\item [C2] Time between any two executions should be equal to partition period : $(\forall p \in \mathbb{P})(k\in [1,N(p)-1])(O_{k+1}^{p}=O_{k}^{p}+ \phi(p))$.
%\item [C3] The last start must finish before the hyperperiod ends : $(\forall p \in \mathbb{P})(O_{N(p)}^{p}+\Delta(p) \leq H)$
%\item [C4] A partition cannot overlap : $(\forall p \in \mathbb{P})(\forall z \in \mathbb{P})$$(k\in [1,N(p)])(j\in [1,N(z)])$ $(O_{k}^{p} \leq O_{j}^{z} \implies O_{j}^{z} \geq O_{k}^{p} +\Delta(p))$
%\end{description}

The kernel checks two additional constraints: (1) All minor frames
finish before the end of the hyperperiod: $(\forall i)(O_{i}.start+O_{i}.duration
\leq H)$ and (2) minor frames cannot overlap, i.e. given a sorted minor
frame list (based on their offsets): $(\forall i <
N(O))(O_{i}.start+O_{i}.duration \leq O_{i+1})$, where $N(O)$ is the number
of minor frames.   Note that the minor frames need not be contiguous,
as the update procedure fills in any gaps automatically.

If the constraints are satisfied, then the task is moved to the first core, \emph{CPU0} if it is not already on \emph{CPU0}. 
This is done because the global tick (explained in next subsection) used for implementing the major 
frame schedule is also executed on \emph{CPU0}. By moving the task to \emph{CPU0} and disabling interrupts, 
the scheduler ensures that the current frame is not changed while the major frame is being updated. 
At this point the task also obtains a spin lock to  ensure that no other task can update the major frame at 
the same time. In this procedure the scheduler state is also set to \texttt{APP\_INACTIVE} (see Table \ref{table:variable}), to stop the scheduling of all application
tasks across other cores. The main scheduling loop reads the scheduler state before scheduling application tasks. A  scenario showing dynamic reconfiguration can be seen in Figure~\ref{fig:dynamic_reconfig}. 

\begin{table}[ht]
\centering
\caption{The states of the DREMS Scheduler}
\footnotesize
\begin{tabular}{| c | p{0.3\textwidth} |}
\hline
 APP\_INACTIVE &Tasks in temporal partitions are not run \\\hline
 APP\_ACTIVE &Inverse of APP\_INACTIVE\\\hline
\end{tabular}
\label{table:variable}
\end{table}

\iffalse
the scheduler state is set to
\texttt{APP\_INACTIVE}, to stop the scheduling of all application
tasks so the partition structure can be updated.  Note that while the
scheduler state is set to \texttt{APP\_INACTIVE}, the $mfl$ in
Procedure~\ref{algo:globaltick} will be $null$, so the partition
scheduling will be halted.   The scheduler state is set to
\texttt{APP\_ACTIVE} to begin scheduling these application tasks per
the schedule.   
To ensure that multiple processes cannot try to change the partition
schedule at the same time, Procedure~\ref{algo:majorframe} uses a 
spinlock to ensure that multiple processors cannot execute this code 
simultaneously.  Additionally, all other processors except for \emph{CPU0} only
ever atomically read the major frame structure to ensure data structure consistency.
\fi

%Why is the important to have synchronized hyper periohd.
%parts of the same application are distributed across the node.
% low latency communication.
%Every node will run the related tasks at the same time.
%Note that, though it is not shown in the algorithm, 

It is also possible to set the global tick (that counts the hyperperiods) to be started with an offset. 
This delay can be used to synchronize the start of the hyperperiods across nodes of the cluster. This is necessary to 
ensure that all nodes schedule related temporal partitions at the same time.
%The activation of this schedule can be set to occur at a specific
%time, so the partition schedules on multiple computing nodes can be
%synchronized.
This ensures that for an application that is distributed
across multiple nodes, its \emph{Application} level tasks run at
approximately the same time on all the nodes which enables low latency
communication between dependent tasks across the node level. 

%Unlike many temporally partitioned schedulers, \iap\ OS provides
%functionality for dynamic reconfiguration of the temporal partition
%schedule.  This capability allows the system to switch between scenarios
%while running.  The dynamic reconfiguration is achieved by atomically
%setting the scheduler state to \texttt{APP\_INACTIVE}, which stops all
%application actors contained within partitions from being scheduled.
%While in this state, a new schedule can be loaded through system calls
%during run-time.  Once updated, the new partition schedule can be
%started by changing the scheduler state back to \texttt{APP\_ACTIVE}.
%\textbf {Example:}  A  scenario showing dynamic reconfiguration can be seen in
%Figure~\ref{fig:dynamic_reconfig}. 



\begin{figure}[t]
\centering
\includegraphics[width=0.48\textwidth]{dynamic_reconfig}
\caption{Two single-threaded processes run in separate partitions with a duration of $60 ms$ each. The schedule is dynamically reconfigured so that each partition duration is doubled. 
A \emph{Critical} task is responsible for calling the update\_major\_frame system call. Duration of the active partition is cut short at the point when update\_major\_frame function is called. 
}
\label{fig:dynamic_reconfig}
%\vspace{-0.2in}
\end{figure}


%The main scheduling function then
%includes following additional steps:

%\begin{enumerate}
%\item Pick the highest priority task from the criticality level if
%  it has not exhausted the CPU quota.
%\item If the highest priority task has exhausted the CPU cap, then
%  check if the window over which cap was being measured should be
%  reset.  If the window is reset then this task is scheduled.
%\item Otherwise, the above two steps are repeated unless all ready
%  tasks in a level have exhausted their CPU cap. In that case the
%  scheduler executes the task in the same criticality level that is
%  ready. 
%\end{enumerate}

%
%\section{Scheduler}
%\label{sec:scheduling}
	
%a small paragraph should explain that CPU cap is turned into a per actor ceiling of execution time measured over $N$ major frames, where N is a configurable parameter defined at compilation time.
% for each task a counter of execution time is maintained that is added over all threads/all CPUs of the process/ and is reset every N global tick (is discussed in the next section.)
% CPU cap is a feature that can be completely disabled.
\vspace{-0.1in}
\subsection{Main Scheduling Loop}	
\label{sec:scheduling}

%Figure~\ref{fig:scheduler}  shows the high-level overview of the \iap\
%scheduler.  
 A periodic tick running at $250$ Hz\footnote{The kernel tick value is also called 'jiffy' and can be set to a different value when the kernel image is compiled}  is used to ensure
that a scheduling decision is triggered at least every $4$ ms.  This
tick runs with the base clock of \emph{CPU0} and executes a procedure called $GlobalTick$
%Procedure~\ref{algo:globaltick} 
in the interrupt context only on
\emph{CPU0}.   
%This ensures that all
  %CPU switches the current partition at
%approximately the same time, to within one global tick of the
%scheduler. 
 This procedure enforces the partition scheduling and
updates the current minor frame and hyperperiod start time
(\texttt{HP\_start}).  The partition schedule is determined by
a circular linked list of minor frames which comprise
the major frame.  Each entry in this list contains that partition's duration,
so the scheduler can easily calculate when to switch to the next minor frame. 

%\begin{figure}[htb]
%\centering
%\includegraphics[width=0.5\textwidth]{scheduler}
%\caption{DREMS OS Scheduler}
%\label{fig:scheduler}
%\end{figure}

  \iffalse

\begin{algorithm}[t]
\caption{Global Tick}
\label{algo:globaltick}
\begin{algorithmic}[1]
\footnotesize
\IF{ \COMMENT{Current CPU is CPU0}}
\IF {$firstrun ~and~mfl \neq null $} 
\STATE  $ firstrun \leftarrow false$
\STATE  $ HP\_start \leftarrow Sched\_clock()$\COMMENT{ Sched\_clock() provides the current uptime measured based on elapsed jiffies}
\STATE  $ MF\_start \leftarrow  HP\_start$
\STATE  $ cur\_frame \leftarrow HEAD(mfl)$
\STATE  $next\_switch \leftarrow HP\_start+cur\_frame.duration$
\ENDIF
\IF {$Sched\_clock() \geq next\_switch$ } 
\STATE  $ cur\_frame \leftarrow cur\_frame.next$
\STATE  $next\_switch \leftarrow next\_switch+cur\_frame.duration$
\IF {$cur\_frame ==  HEAD(mfl)$ } 
\STATE $ HP\_start \leftarrow Sched\_clock()$
\ENDIF
\ENDIF
\ENDIF
\end{algorithmic}
\end{algorithm}
\fi

After the global tick handles the partition switching, the function to
get the next  runnable task is invoked. This function combines the
\emph{mixed criticality} scheduling with the \emph{temporal partition}
scheduling. For mixed
criticality scheduling, the \emph{Critical} system tasks should preempt
the \emph{Application} tasks, which themselves should preempt the
\emph{Best Effort} tasks. This policy is implemented by  \emph{Pick\_Next\_Task} subroutine, which is called first for the system partition.
Only if there are no runnable \emph{Critical} system tasks and the
scheduler state is not inactive, i.e. the application partitions are allowed to run\footnote{The OS provides support for pausing all application partitions and ensuring that only system partition is executed}, will
\emph{Pick\_Next\_Task} be called for the \emph{Application} tasks.
Thus, the scheduler does not schedule any \emph{Application} tasks during
a major frame reconfiguration. Similarly \emph{Pick\_Next\_Task} will
only be called for the \emph{Best Effort} tasks if there are both no
runnable \emph{Critical} tasks and no runnable \emph{Application} tasks.

\iffalse
\begin{algorithm}[t]
\caption{Main Scheduler Function - Called when task wishes to give up the CPU or a CPU tick occurs}
\label{algo:main_sched}
\begin{algorithmic}[1]
\footnotesize
\REQUIRE $TIF\_NEED\_RESCHED$ flag on the task is set by scheduler\_tick(). Preemption is enabled.
\STATE $Disable(Preemption)$
\STATE $ RQ \leftarrow Get\_CPU\_RQ(Current CPU)$
\STATE $prev\_task \leftarrow RQ.curr\_task $

% PICK NEXT TASK
\STATE $[index,next\_task] \leftarrow Pick\_Next\_Task(RQ, sys\_partition)$

\IF {$index >= MAX\_RT\_PRIO$ and $state \neq APP\_INACTIVE$ }
\STATE $[index,next\_task] \leftarrow Pick\_Next\_Task(RQ, cur\_frame.partition)$
\IF {$index >= MAX\_RT\_PRIO$}
\STATE $[index,next\_task] \leftarrow Pick\_Next\_Best\_Effort\_Task()$
\ENDIF
\ENDIF

\STATE $Update\_Exec\_Time(prev\_task)$

\STATE $RQ.curr\_task \leftarrow next\_task$

\IF {\COMMENT{CPU Cap Enabled}}
\STATE $Update\_Stats(prev\_task)$
\STATE $Update\_Disabled\_Bit(prev\_task)$
\ENDIF
\IF {$prev\_task!=next\_task$}
\STATE $Context\_Switch\{RQ, prev\_task, next\_task\}$
\ENDIF
\STATE $Enable(Preemption)$

\end{algorithmic}

\end{algorithm}

\vspace{-0.1in}
\fi
\subsection{Pick\_Next\_Task and CPU Cap}
The \emph{Pick\_Next\_Task} function returns  either the highest
priority task from the current temporal partition (or the system
partition, as application) or an empty list of there are no runnable
tasks.  
%MAX\_RT\_PRIO$ and the empty list if there
%are no runnable tasks in the runqueue.  
 If CPU cap is disabled, the
\emph{Pick\_Next\_Task} algorithm returns the first task from the specified
runqueue. For the best effort class, the default algorithm for the
Completely Fair Scheduler policy in the Linux Kernel
\cite{mauerer2008} is used.

  If the CPU cap is enabled,
the \emph{Pick\_Next\_Task} algorithm iterates through the task list
at the highest priority index of the runqueue, because unlike the
Linux scheduler, the tasks may have had their disabled bit set by the
scheduler if it had enforced their CPU cap.  If the algorithm finds a
disabled task in the task list, it checks to see when it was disabled;
if the task was disabled in the previous CPU cap window, it reenables the
task and sets it as the $next\_task$.  If, however, the task
was disabled in the current CPU cap window, the algorithm continues
iterating through the task list until it finds a task which is
enabled.  If the algorithm finds no enabled task, it returns the first
task from the list if the current runqueue belongs to an application partition. 

%If the current runqueue belongs to the system or critical partition then  it returns  an  empty list if there are no tasks 
%since the CPU cap for critical tasks is a hard limit; 
%see section \ref{sec:CPUCAP} for a discussion of this behavior.

This iteration through the task list when CPU cap
enforcement is enabled increases the complexity of the scheduling algorithm to
$O(n)$, where $n$ is the number of tasks in that temporal partition,
compared to the Linux scheduler's complexity of $O(1)$.  Note that
this complexity is incurred when CPU cap enforcement is
enabled and there is at least one actor that has partial CPU cap (less
than 100\%).  In the worst case, if all actors are given a partial CPU
cap, the scheduler performance may degrade necessitating more
efficient data structures. 

\iffalse
\begin{algorithm}[t]
\caption{Pick Next Task from RunQueue}
\label{algo:pick_next_task}
\begin{algorithmic}[1]
\footnotesize
\INPUT $RQ$ \COMMENT{The scheduler runqueue}; $partition$ \COMMENT{The currently active partition}
\STATE $prio\_array \leftarrow RQ.PartitionRQ[partition]$
\STATE $next\_task \leftarrow null$
\STATE $next\_index \leftarrow MAX\_RT\_PRIO$
\STATE $index \leftarrow 0$
\WHILE{$index < MAX\_RT\_PRIO$}
\STATE $index \leftarrow FindFirstBit(prio\_array.bitmap + index)$ \COMMENT{find first enabled bit after index}
\IF {$index >= MAX\_RT\_PRIO$}
\RETURN  $MAX\_RT\_PRIO,null$
\ENDIF
\STATE $runlist \leftarrow prio\_array.queue + index$
\COMMENT{runlist is a doubly linked list containing all the tasks at that priority level}
\IF {$next\_task == null$}
\STATE $next\_task \leftarrow runlist[0]$
\STATE $next\_index \leftarrow index$
\ENDIF
\IF {\COMMENT{CPU Cap Enabled}}
\FOR{$task$ in $runlist$}
\IF {$task.disabled == true$}
\IF {$task.last\_disabled\_time < CPUCAP\_WIN\_start$}
\STATE $task.disabled \leftarrow \FALSE$ 
\STATE $next\_task \leftarrow task$  
\STATE $next\_index \leftarrow index$
\RETURN $next\_index,next\_task$
\ENDIF
\ELSE
\STATE $next\_task \leftarrow task$
\STATE $next\_index \leftarrow index$
\RETURN $next\_index,next\_task$
\ENDIF
\ENDFOR
\ELSE 
 \RETURN $next\_index,next\_task$\COMMENT{CPU CAP is DISABLED}
\ENDIF
\ENDWHILE
\COMMENT{This implies that all tasks are disabled due to CPU cap.}
\IF {$partition == sys\_partition$}
\RETURN  $MAX\_RT\_PRIO,null$ \COMMENT{the CPU cap for critical tasks is a hard limit.}
\ENDIF
\RETURN $next\_index,next\_task$ \COMMENT{returns the highest priority task if all tasks are disabled or returns a null task with MAX\_RT\_PRIO.}
\end{algorithmic}
\end{algorithm} 
\fi


\begin{figure}[t]
\centering
\includegraphics[width=0.5\textwidth]{scenario1}
\caption{Single Threaded processes 1000 and 1001 share a partition with a duration of $60 ms$.
Process 1000 has 100\% CPU cap and priority 70; process 1001 has 20\% CPU cap, and higher priority 72.
Since process 1001 has a CPU cap less than 100\%, a ceiling is calculated for this process: $20\%$ of $60 ms$ = $12 ms$. The average jitter was calculated to be 2.136 ms with a maximum jitter of 4.0001 ms. 
}
\label{fig:scenario1}
\vspace{-0.2in}
\end{figure}

To complete the enforcement of the CPU cap, the scheduler updates the
statistics tracked about the task and then updates the disabled bit of
the task accordingly.
%as seen in lines $13-16$ of
%Procedure~\ref{algo:main_sched}. 
%\textbf {Example:} 
Figure~\ref{fig:scenario1}, shows
the above mentioned scheduler decisions when CPU cap is placed on
processes that share a temporal partition.  
%The behavior of the scheduler can be observed and analyzed by simply varying the thread priority and CPU cap attributes of both these processes. 
To facilitate analysis, the scheduler uses a logging framework that
updates a log every time a context switch happens.  Figure~\ref{fig:scenario1} clearly shows the lower priority actor
executing after the higher priority actor has reached its CPU cap. 

% If
% the CPU cap of the higher priority process is set to 100\%, it has no
% ceiling and therefore consumes all of the CPU time within the
% partition (not shown in the figure). From the scheduler log, the average jitter was calculated to be 2.136
% ms with a maximum jitter of 4.0001 ms. This jitter is consistent with
% the value of jiffy used, 4 ms, indicating that the scheduler would
% occasionally take an extra jiffy of time to switch to the next minor
% frame. Also, no overshoot was observed in the thread activity for all
% processes - neither process executed outside its temporal partition.


%Figure~\ref{scenario1_thread} shows the observed thread activity. Process 1001, being higher priority, is scheduled as soon as its partition is active. But this process is also limited by its ceiling and does not execute for more than 12 ms, giving CPU time for process 1000. The process execution trace can be visualized in Figure~\ref{scenario1_exec}. The peak execution time for process 1000 was observed to be 44 ms and the peak execution time for process 1001 was 12.005 ms. Since the partition duration is up to 64 ms (accounting for jitter), this indicates a scheduler overhead of up to 8 ms.

%With a minor change to the above scenario, another important property of the scheduling logic can be observed. If the CPU cap of process 1001 is set to 100\%, then process 1001 becomes the highest priority process with no effective ceiling. The design of the CPU cap logic is such that, since both processes 1000 and 1001 have a CPU cap of 100\%, the process with highest priority gets preference for scheduling. It was therefore expected both to be scheduled first and to take up the entire minor frame, as was observed in the scheduler log - only process 1001 was active for the entire duration of the test. This observation can be visualized in Figure~\ref{scenario2_thread}.

%\begin{figure}[ht]
%\centering
%\includegraphics[width=0.5\textwidth]{scenario2_thread}
%\caption{Scenario 2: Thread Activity}
%\label{scenario2_thread}
%\end{figure}

%The features of the \iap\ OS scheduler will be discussed in more detail in the following paragraphs.
%\subsection{Mixed Criticality tasks and Temporal Partitioning}
%\label{sec:mixedCriticality}

%As discussed in Section~\ref{sec:task_model} the scheduler supports
%four different categories of tasks. Out of these four categories, the
%\emph{System} category is strictly reserved for background kernel
%tasks. Tasks belonging to this category are scheduled whenever they
%are ready. However, they are subject to a cap that restricts the
%maximum number of CPU cycles that they can use within a hyper
%period. We discuss the CPU cap in Section~\ref{sec:cpucap}.

%\emph{Critical, System} and \emph{Application} tasks are implemented under a
%modified RT scheduling class. While \emph{Critical} and \emph{System} tasks are
%maintained in a real-time runqueue per CPU, the \emph{Application} tasks are
%managed in a partition specific runqueue. Currently our architecture
%can support $63$ partitions.  The \emph{Best Effort} tasks are mapped to the
%Linux CFS runqueue.

%Figure \ref{fig:scheduler} describes the overall architecture. A
%periodic tick running at $250$ Hz is used in the scheduler, which 
%triggers a scheduling decision at least every $4$ ms.  This
%tick is used to update a global data structure that stores the
%currently active minor frame. Procedure~\ref{algo:globaltick} shows
%the enforcement of this temporal partitioning. The scheduler maintains
%a circular linked list of minor frames ($mfl$) that make up this
%schedule. Keeping track of the period and duration of every partition,
%the scheduler calculates the point in time when the partitions need to
%switch and forces this behavior by changing the currently active frame
%to the next minor frame on the $mfl$.



%\subsection{Main Scheduling Loop}
%\label{sec:schedulingLoop}
%Procedure~\ref{algo:main_sched} describes the operations of the main
%scheduling loop. The act of picking the next task and switching to it
%is implemented via this procedure. It is invoked whenever a task is to
%be preempted. Before finding the next task to switch to, a check is
%made to see if this function is called because the CPU cap ceiling of
%the previous process has been reached (described in
%Section~\ref{sec:cpucap}). If so, this process is disabled by the
%scheduler and the next task is selected from the runqueues in the
%following order (a) System/Critical runqueue, (b) the current
%partition's runqueue, and (c) the Best effort runqueue.

%\subsection{CPU Utilization Ceiling}
%\label{sec:cpucap}
%The purpose of the CPU cap is to allow a mechanism by which high
%priority application tasks can be ensured not to starve lower priority
%tasks of the CPU. If the CPU cap is enabled as a feature, then it is
%possible to set a maximum CPU time that tasks belonging to a process
%can use, measured over a configurable number of major frame
%cycles. This cap is applied only to tasks in the \emph{System} and
%\emph{Application} categories. The cap in the \emph{Application} category is specified
%as a percentage of the total duration of the partition multiplied by
%the number of major frame cycles multiplied by the number of CPU cores
%available. By default, CPU cap for \emph{System} tasks is set to be 5 percent
%of a hyperperiod.

%With the CPU cap enabled the scheduling decisions between tasks that
%are based on the CPU time used counter which is maintained per
%process. This counter is reset after the number of major frame cycle
%over which the cap was specified elapses. This counter is used to make a
%scheduling decision, which requires consideration of not only whether a
%task is ready but also whether its $disabled$ flag is set to true. This
%flag is evaluated for the currently running task when the main
%scheduling function is evaluated and is set when the task when it reaches 
%its CPU cap. The main scheduling function then
%includes following additional steps:

%\begin{enumerate}
%\item Pick the highest priority task from the criticality level if
%  it has not exhausted the CPU quota.
%\item If the highest priority task has exhausted the CPU cap, then
%  check if the window over which cap was being measured should be
%  reset.  If the window is reset then this task is scheduled.
%\item Otherwise, the above two steps are repeated unless all ready
%  tasks in a level have exhausted their CPU cap. In that case the
%  scheduler executes the task in the same criticality level that is
%  ready. 
%\end{enumerate}
