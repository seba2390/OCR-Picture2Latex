% EXEMPLE DE CONTRIBUTION AU GRETSI

% Classe `gretsi`
% Définir la langue utilisée (`francais` ou `english`)
\documentclass[francais]{gretsi}

% Encodage du fichier source
\usepackage[utf8]{inputenc}

% Encodage des caractères dans le PDF
\usepackage[T1]{fontenc}
\usepackage[french]{babel}


% Packages mathématiques
\usepackage{amsmath, amssymb, amsfonts}

\usepackage{mdframed}
\usepackage{acronym}
%\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{dsfont,bbm}
\usepackage{url}
\usepackage{color}
\newcommand{\tcb}{\textcolor{black}}
\newcommand{\ul}{\underline}
\newcommand{\ds}{\displaystyle}
\newcommand{\mc}{\mathcal}
\newcommand{\mb}{\mathbf}
\newcommand{\mr}{\mathrm}
\newcommand{\mbb}{\mathbb}
\newcommand{\eqdef}{\triangleq}
\DeclareMathOperator{\Tr}{Tr}
\newcommand{\calX}{\mathcal{X}}
\newcommand{\calY}{\mathcal{Y}}
\newcommand{\calT}{\mathcal{T}}
\newcommand{\ol}{\overline}
\newcommand{\R}{\mathbb{R}}
\newcommand{\PP}{\mathcal{P}}
\newcommand{\bs}{\boldsymbol}
\newcommand{\tc}{\textcolor}
\newcommand{\tb}{\textbf}
\newcommand{\deq}{\stackrel{d}{=}}
\newcommand{\Nr}{{n_r}}
\newcommand{\Nt}{{n_t}}
\renewcommand{\d}{\mathrm{d}}
\newcommand{\Id}{{\mb I}}
\newcommand{\tr}{\mathrm{tr}}
\newcommand{\diag}{\mathrm{diag}}
\newcommand{\idest}{i.e., }
\newcommand{\eg}{e.g., }

\newcommand{\bigO}[1]{\ensuremath{\mathop{}\mathopen{}O\mathopen{}\left(#1\right)}}
\newcommand{\smallO}[1]{\ensuremath{\mathop{}\mathopen{}o\mathopen{}\left(#1\right)}}
\newcommand{\ind}[1]{\mathds{1}_{\left\lbrace #1 \right\rbrace}}
\newcommand{\tcr}{\textcolor{red}}

\usepackage{fancyhdr}
 
\graphicspath{{graphics/}}

\acrodef{iid}[i.i.d.]{independent identically distributed}
\acrodef{wrt}[w.r.t.]{with respect to}

\acrodef{SINR}[SINR]{signal-to-interference plus noise ratio}
\acrodef{SNR}[SNR]{signal-to-noise ratio}
\acrodef{DMC}[DMC]{discrete memoryless channel}
\acrodef{BSC}[BSC]{binary symmetric channel}
\acrodef{KKT}[KKT]{Karush-Kuhn-Tucker}
\acrodef{NPC}[NPC]{Nash-equilibrium power control}
\acrodef{SPC}[SPC]{Semi-coordinated power control}
\acrodef{CCPC}[CCPC]{Costless-communication power control}
\acrodef{CPC}[CPC]{Coded power control}


\newtheorem{Lemma}{Lemma}
\newtheorem{Corollary}{Corollary}
\newtheorem{Proposition}{Proposition}
\newtheorem{propposition}{Proposition}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{definition}[theorem]{Definition}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{proposition}[theorem]{Proposition}

\begin{document}

% ===================================================================================== %

% Titre
\titre{Improving MIMO channel estimation via receive power feedback}


% Auteurs
\auteurs{
  % Syntaxe : \auteur{<prénom>}{<nom>}{<adresse électronique>}{<indice d'affiliation>}
  \auteur{Chao}{Zhang}{}{1}
  \auteur{Hang}{Zou}{}{2}
  \auteur{Samson}{Lasaulce}{}{3}
  \auteur{Lucas}{Saludjian}{}{4}
}

% Affiliations
\affils{
  % Syntaxe : \affil{<indice d'affiliation>}{<nom du labo et adresse>}
  \affil{}{$^1$Central South University \& $^2$TII \& $^3$CRAN-Nancy \& $^4$RTE France
  }
}

% Si tous les auteurs ont la même adresse : ne pas mettre de numéro d'affiliation. Exemple :
% \auteurs{
  % \auteur{Michel}{Dupont}{mdupont@uni.fr}{}
  % \auteur{Danielle}{Durand}{dd@uni.com}{}
% }
% \affils{
%   \affil{}{Laboratoire Traitement des Signaux,
%         1 rue de la parole, BP 00000,
%         99000 Nouvelleville Cedex 00, France
%   }
% }

% Résumé en français
\resume{Estimer l'\'{e}tat du canal est un probl\`{e}me important dans les r\'{e}seaux sans fil. \`{A} cette fin, il importe d'exploiter toutes les informations disponibles pour am\'{e}liorer autant que possible la pr\'{e}cision de l'estimation de canal. Il s'av\`{e}re que le probl\`{e}me de l'exploitation de l'information associ\'{e}e au feedback de puissance re\c{c}u (par exemple, l'indicateur de force du signal re\c{c}u -RSSI-) n'a pas \'{e}t\'{e} identifi\'{e} et r\'{e}solu; dans cette configuration, on suppose que l'\'{e}metteur re\c{c}oit des commentaires de tous les r\'{e}cepteurs pr\'{e}sents. Pour r\'{e}soudre ce probl\`{e}me, des outils d'estimation classiques peuvent \^{e}tre utilis\'{e}s. L'utilisation du MMSE correspondant est toujours b\'{e}n\'{e}fique, tandis que la pertinence de l'utilisation de l'estimateur MAP d\'{e}pendrait du rapport signal sur bruit (SNR) de fonctionnement.}

% Résumé en anglais
\abstract{Estimating the channel state is known to be an important problem in wireless networks. To this end, it matters to exploit all the available information to improve channel estimation accuracy as much as possible. It turns out that the problem of exploiting the information associated with the receive power feedback (e.g., the received signal strength indicator -RSSI-) has not been identified and solved; in this setup, the transmitter is assumed to receive feedback from all the receivers in presence. As shown in this paper, to solve this problem, classical estimation tools can be used. Using the corresponding MMSE is shown to be always beneficial, whereas the relevance of using the MAP estimator would depend on the operating SNR. }

\maketitle

% ===================================================================================== %

\section{Introduction}
\label{sec:intro}

The acquisition of channel information is essential to optimize the system performance in many wireless networks such as orthogonal frequency-division multiplexing (OFDM) and multiple input multiple output (MIMO) (see e.g., \cite{Li-2002,Fang-TWC-2017,Choi-TSP-2014,Inter-TWC-2019}). Generally the channel information is not perfectly known in practical systems and needs to be estimated by sending pilot sequences \cite{caire-tit-2010} or using blind channel estimation \cite{blind-2002}. Improving the quality of channel estimation is a well studied problem for both academic researchers as well as engineers in the communication industry.  {Recent studies of channel estimation through downlink training concentrated on massive MIMO systems.  The overhead of training sequence is huge in such systems with a large number of antennas which makes the channel estimation even more challenging.   The behavior of MMSE estimator
of  low-rank channel covariance matrix is studied in \cite{Fang-TWC-2017}.  It is shown that training overhead can be substantially reduced by exploiting the low-rank property of the channel covariance matrix in the asymptotic low-noise regime. In \cite{Choi-TSP-2014}, an open and closed-loop training
framework using successive channel prediction/estimation at the user for FDD massive MIMO systems is proposed. Similarly, a small amount of
feedback is required in low signal-to-noise ratio (SNR) regime with the immense transmit antennas or with inaccurate prior channel estimates setup for closed-loop systems. More interestingly, in \cite{Inter-TWC-2019}, downlink
beamforming training, although increases pilot overhead and
introduces additional pilot contamination, improves significantly
the achievable downlink rate.} Most of the works aims at finding efficient training schemes to improve the channel estimate, especially in low SNR regime. However, apart from the training phase, the channel estimation can also be enhanced during data transmission, which is the purpose of this paper. 

In this paper, by exploiting the receive power feedback \cite{Zhang-TWC-2017}, also referred to as received signal strength indicator (RSSI), we propose a novel technique to better estimate the channel in MIMO systems. {Since feedback resources are extremely limited in practical systems, it is important to efficiently use these power domain feedback resources. For example, authors in \cite{Panda-ICACCI-2016} consider a sub-optimal scheduling scheme for a multiuser MIMO (MU-MIMO) to decrease the load on the feedback. Received signal-to-noise-plus-interference-ratio (SINR) at each user is quantized for both fixed thresholds and adaptable ones both achieving a remarkable reduction of feedback bits. It is worth mentioning that the usage of such information could be highly case-depending.  Authors in \cite{Su-TVT-2015} show that the quantization method for a single cell could highly degrade the performance of coordinated multipoint (CoMP) system for quantization error vector being no longer isotropic.} To this end, we propose to use receive power feedback at a much lower rate compared to the classical channel state information (CSI)  feedback rate. We will focus on two  specific estimators: minimum mean square error (MMSE) and maximum a posteriori (MAP) estimators. Both estimators have been widely used to acquire CSI but only through pilot training, e.g., in \cite{caire-tit-2010}\cite{vincent-book}.
%We provide a new MMSE estimator which uses RSSI measurements, and show that this results in a better estimate than the classical MMSE estimator from \cite{vincent-book}. Also, a new MAP estimator exploiting RSSI feedback is proposed to   improve the estimation accuracy. Finally, we show the improvements provided by the new estimators proposed in this paper through numerical simulations. We  observe reduction of distortion is up to $20\%$ for the MMSE estimator and up to $50\%$ for the MAP estimator, validating our proposed approach. 


\section{Problem statement}

We consider a MIMO broadcast channel consisting of a base station (BS) equipped with $N$  transmit antennas and $K$ user equipments (UEs) with single antenna. The channel model can be described by 
\begin{equation}
y_j = \mathbf{h}_{j}^{\mathrm{H}} \mathbf{x} + z_j
\end{equation}
where $y_j$ represents the received signal at UE $j$,  $\mathbf{h}_j=(h_{1j},\dots,h_{Nj})^{\mathrm{T}}\in \mathbb{C}^N$ is the vector of channel coefficients from BS to UE $j$, $\mathbf{x}=(x_1,\dots,x_N)^{\mathrm{T}}$ is the vector of independent transmitted symbols at BS and $z_j$ is the received Gaussian noise at UE $j$ following $z_j\sim \mathcal{CN}(0,N_0)$. The channel matrix, $\mathbf{H}=[\mathbf{h}_1,\dots,\mathbf{h}_K]$, is assumed to follow a block fading model, that is, $\mathbf{H}$ remains constant by block and more precisely over each transmitted frame. Indeed, like modern cellular networks standards, we assume that the transmission take places in the form of frames, each frame being itself divided into $M$ time-slots, and each time slot comprises $T$ symbols. In frequency division duplex (FDD) wireless systems, one can assume that  each UE $j\in\{1,\dots,K\}$ estimates $\mathbf{h}_{j}$ from downlink training symbols and feeds the estimated channel information back to the BS. This is the setting we assume here. The conventional approach to estimate the channel is to use pilot signals and training in the signal domain only. Assume orthogonal pilot signals with power $P$. If $\beta$ symbols are used for training, the observation at Receiver $j$ that is, $\bold{s}_j=(s_{1j},\dots,s_{Nj})^T$ is given by
\begin{equation}
\bold{s}_j = \sqrt{\beta P} \mathbf{h}_j  + \bold{z}_j
\end{equation}
where the estimation noise $\bold{z}_j=(z_{1j},z_{2j},\dots,z_{Nj})^T\sim \mathcal{CN}(0,N_0\bold{I}_N)$.  The signal-to-noise ratio (SNR) in the training phase is defined as $\mathrm{SNR}=\frac{\beta P}{N_0}$. The observations $\bold{s}_j$ can be used to estimate $\mathbf{h}_j$. The approach retained in this paper is to de-noise the above estimates not only by using the channel statistics (as done in previous works such as \cite{lasaulce-vtc-2001}) but also by using some information that can be acquired in the power domain, namely the receive power feedback samples sent by the UEs to the BS. Indeed, in wireless systems such as 3G and 5G systems, for each transmitted time slot $m'$, the BS receives a feedback sample of the power received by UE $j$. This feedback is used e.g., for power control or scheduling at the BS. Our key observation is that this information can also be used to improve channel estimation, and more precisely the modulus information accuracy. Suppose the transmitted symbols at BS are not correlated, i.e. $\mathbb{E}[x_i(m')x_j(m')^{\mathrm{H}}]=0$ for $i\neq j$, the average receive power at UE $j$ for time slot $m'$ can be written as
\begin{equation}
R_{j,m'}= \sum_{i=1}^N g_{ij} P_i(m') + N_0
\label{eq:RSSI_feedback}
\end{equation}
where $g_{ij}=|h_{ij}|^2$ represents the channel gain and $P_i(t) = \mathbb{E}[|x_i(m')|^2]$ represents the transmit power of the $i$-th antenna at time-slot $m'$. Our goal is to find new estimates $\bold{h}_j^{\text{X}:m}$ by exploiting this power domain information, where $\text{X}\in\{\text{MMSE}, \text{MAP}\}$ indicates the type of considered estimation criterion, and $m$ the number of time slots exploited. Without loss of generality, we assume that $\bold{h}_j\sim \mathcal{CN}(0,\bold{D}_j)$ where $\bold{D}_j=\diag\{\sigma_{1j}^2,\dots,\sigma_{Nj}^2\}$.  
\section{Novel estimates with receive power feedback}
In this section, we explain our technique for the single band scenario. The extension to the multi-band case is straightforward since the estimation over each band can be processed independently.  Apart from the observation $\mathbf{s}_j$ during the downlink training phase, we use receive power feedback introduced by (\ref{eq:RSSI_feedback}) as an additional information to improve the estimation quality.
\subsection{MMSE estimator}

%If the MMSE estimator is of the form (\ref{eq:mmseform}), minimizing the MMSE can be formulated as the following optimization problem
%\begin{equation}
%\underset{\bold{A}_j}{\min} \quad \mathbb{E}_{\bold{h}_j,\bold{z}_j}[|\bold{A}_j\bold{s}_j-\bold{h}_j|^2]
%\end{equation}
%By taking the derivative of $\mathbb{E}_{\bold{h}_j,\bold{z}_j}[|\bold{A}_j\bold{s}_j-\bold{h}_j|^2]$ w.r.t. $\bold{A}_j$, the MMSE estimator can be obtained in the following manner:
%\begin{equation}
%\begin{split}
%&\frac{\partial \mathbb{E}_{\bold{h}_j,\bold{z}_j}[|\bold{A}_j\bold{s}_j-\bold{h}_j|^2]}{\partial \bold{A}_j}\\
%=&\frac{\partial\mathbb{E}_{\bold{h}_j,\bold{z}_j}[\bold{s}_j^{\mathrm{H}}\bold{A}_j^{\mathrm{H}}\bold{A}_j\bold{s}_j-\bold{s}_j^{\mathrm{H}}\bold{A}_j^{\mathrm{H}}\bold{h}_j-\bold{h}_j^{\mathrm{H}}\bold{A}_j\bold{s}_j]}{\partial \bold{A}_j}\\
%=&\frac{\partial\mathbb{E}_{\bold{h}_j,\bold{z}_j}\{\Tr[\bold{s}_j^{\mathrm{H}}\bold{A}_j^{\mathrm{H}}\bold{A}_j\bold{s}_j-\bold{s}_j^{\mathrm{H}}\bold{A}_j^{\mathrm{H}}\bold{h}_j-\bold{h}_j^{\mathrm{H}}\bold{A}_j\bold{s}_j]\}}{\partial \bold{A}_j}\\
%=&\bold{A}_j^{*}\mathbb{E}_{\bold{h}_j,\bold{z}_j}[\bold{s}_j^{*}\bold{s}_j^{\mathrm{T}}]-2\mathbb{E}_{\bold{h}_j,\bold{z}_j}[\bold{h}_j^{*}\bold{s}_j^{\mathrm{T}}]
%\end{split}
%\end{equation}
%where $(.)^{*}$ is  the conjugate operator, $(.)^{\mathrm{T}}$ is  the transpose operator and $(.)^{\mathrm{H}}$ is  the conjugate transpose operator.
%
%The optimum $\bold{A}_j^{\mathrm{OPT:0}}$ can be obtained by setting the derivative to zero, i.e.
%\begin{equation}
%\bold{A}_j^{\mathrm{OPT:0}}=\mathbb{E}_{\bold{h}_j,\bold{z}_j}[\bold{h}_j\bold{s}_j^{\mathrm{H}}]\mathbb{E}_{\bold{h}_j,\bold{z}_j}[\bold{s}_j\bold{s}_j^{\mathrm{H}}]^{-1}
%\label{eq:optimalA}
%\end{equation}
%
%In the absence of any additional information such as the RSSI, we can simply write 
%\begin{equation}
%\mathbb{E}_{\bold{h}_j,\bold{z}_j}[\bold{h}_j\bold{s}_j^{\mathrm{H}}]\mathbb{E}_{\bold{h}_j,\bold{z}_j}[\bold{s}_j\bold{s}_j^{\mathrm{H}}]^{-1}=\frac{\sqrt{\beta P}}{\beta P+N_0} \bold{I}_{K}
%\label{eq:basicmmse}
%\end{equation}
%
%This result is well known and has been provided in \cite{caire-tit-2010} and \cite{vincent-book}. However, if the RSSI is known, then the expectation in (\ref{eq:optimalA}) must be conditioned with respect to this information. The RSSI at time $t$ and receiver $j$ is a random variable $R_{j,t}$ as provided in (\ref{eq:defrssi}). This random variable is observed and takes a realization $r_{j,t}$. Denote by $\mathbf{R}_j^{(m)} = (R_{j,1},\dots,R_{j,m})$ the vector of random RSSI observed at $j$ over $m$ time slots and by $\mathbf{r}_j^{(m)} = (r_{j,1},\dots,r_{j,m})$ a vector of RSSI realizations. Then, the optimal MMSE estimator knowing $\mathbf{r}_j^{(m)}$ is simply given by
%\begin{equation}
%\bold{A}_j^{\mathrm{OPT:m}}=\mathbb{E}_{\bold{h}_j,\bold{z}_j|\mathbf{R}_j^{(m)}=\mathbf{r}_j^{(m)}}[\bold{h}_j\bold{s}_j^{\mathrm{H}}]\mathbb{E}_{\bold{h}_j,\bold{z}_j|\mathbf{R}_j^{(m)}=\mathbf{r}_j^{(m)}}[\bold{s}_j\bold{s}_j^{\mathrm{H}}]^{-1}
%\label{eq:optimalAwithm}
%\end{equation}
%
%Note that
%\begin{equation}
%\begin{split}
%r_{j,i}=&\sum_k g_{kj}P_k(i)+N_0\\
%=&\sum_k |h_{kj}|^2P_k(i)+N_0\\
%=& \bold{h}_j^{\mathrm{H}}\bold{P}(i)\bold{h}_j+N_0
%\end{split}
%\end{equation}
%where $\bold{P}(i)=\diag (P_1(i),\dots,P_K(i))$. This results in
%\begin{equation}
%\bold{A}_j^{\mathrm{OPT:m}} = \frac{ \sqrt{\beta P}\mathbb{E}_{|\mathbf{R}_j^{(m)}=\mathbf{r}_j^{(m)}}[\bold{h}_j\bold{h}_j^{\mathrm{H}}] } {\sqrt{\beta P}\mathbb{E}_{|\mathbf{R}_j^{(m)}=\mathbf{r}_j^{(m)}}[\bold{h}_j\bold{h}_j^{\mathrm{H}}]+N_0\bold{I}_K }
%\label{eq:optimalAwithm2}
%\end{equation}

Denote by $\mathbf{R}_j^{(m)} = (R_{j,1},\dots,R_{j,m})$ the vector of random RSSI observed at $j$ over $m$ time slots.  Correspondingly, $\mathbf{r}_j^{(m)} = (r_{j,1},\dots,r_{j,m})$ denotes a vector of RSSI realizations. The optimal MMSE estimator knowing $\mathbf{r}_j^{(m)}$ can be written as
\begin{equation}\bold{h}_j^{\mathrm{MMSE}:m}=\bold{A}_j^{\mathrm{OPT:m}}\bold{s}_j\end{equation}
where $\bold{A}_j^{\mathrm{OPT:m}}$ is obtained from:
\begin{equation}
 \bold{A}_j^{\mathrm{OPT:m}}\in\underset{\bold{A}_j}{\arg\min} \quad \mathbb{E}_{\bold{h}_j,\bold{z}_j|\mathbf{R}_j^{(m)}=\mathbf{r}_j^{(m)}}[|\bold{A}_j\bold{s}_j-\bold{h}_j|^2]
\end{equation}
The corresponding optimal solution $\bold{A}_j^{\mathrm{OPT:m}}$ expressed as:
\begin{equation}
\bold{A}_j^{\mathrm{OPT:m}}=\mathbb{E}_{\bold{h}_j,\bold{z}_j|\mathbf{R}_j^{(m)}=\mathbf{r}_j^{(m)}}[\bold{h}_j\bold{s}_j^{\mathrm{H}}]\mathbb{E}_{\bold{h}_j,\bold{z}_j|\mathbf{R}_j^{(m)}=\mathbf{r}_j^{(m)}}[\bold{s}_j\bold{s}_j^{\mathrm{H}}]^{-1}
\label{eq:optimalAwithm}
\end{equation}

With our model, this could be rewritten as:

\begin{equation}
\bold{A}_j^{\mathrm{OPT:m}} = { \sqrt{\beta P}\mathbb{E}_{\bold{h}_j|\mathbf{R}_j^{(m)}=\mathbf{r}_j^{(m)}}[\bold{h}_j\bold{h}_j^{\mathrm{H}}] }\bold{W}^{-1}
\label{eq:optimalAwithm2}
\end{equation}
where $W={\beta P}\mathbb{E}_{\bold{h}_j|\mathbf{R}_j^{(m)}=\mathbf{r}_j^{(m)}}[\bold{h}_j\bold{h}_j^{\mathrm{H}}]+N_0\bold{I}_K$.


The $\mathbb{E}_{\bold{h}_j|\mathbf{R}_j^{(m)}=\mathbf{r}_j^{(m)}}[\bold{h}_j\bold{h}_j^{\mathrm{H}}]$ term in (\ref{eq:optimalAwithm2}) is not always easy to express. Therefore, we also provide details on how such an estimator can be designed in practice. First, it can be verified that the non-diagonal elements of $\mathbb{E}_{\bold{h}_j|\mathbf{R}_j^{(m)}=\mathbf{r}_j^{(m)}}[\bold{h}_j\bold{h}_j^{\mathrm{H}}]$ equal to zero, i.e.
\begin{equation}
\mathbb{E}_{\bold{h}_j|\mathbf{R}_j^{(m)}=\mathbf{r}_j^{(m)}}[h_{ij}h_{kj}^{*}]=0\quad(k\neq i)
\label{eq:zero-element}
\end{equation}
From (\ref{eq:zero-element}), it can be seen that $\bold{A}_j^{\mathrm{OPT:m}} $ is a diagonal matrix. Thus the MMSE estimator expresses:
\begin{equation}
\label{eq:mmse_gen}
{h}_{ij}^{\mathrm{MMSE}:m}=\frac{ \sqrt{\beta P}\mathbb{E}_{\bold{h}_j|\mathbf{R}_j^{(m)}=\mathbf{r}_j^{(m)}}[|{h}_{ij}|^2] }{{\beta P}\mathbb{E}_{\bold{h}_j|\mathbf{R}_j^{(m)}=\mathbf{r}_j^{(m)}}[|{h}_{ij}|^2]+N_0} s_{ij}
\end{equation}
The classical MMSE estimator can be obtained as a special case 
\begin{equation}
{h}_{ij}^{\mathrm{MMSE}:0}=\frac{ \sqrt{\beta P}\sigma_{ij}^2 }{{\beta P}\sigma_{ij}^2+N_0} s_{ij}.
\label{eq:MMSE_conventional}
\end{equation} 
Knowing the optimal estimator, we can calculate the conditional MSE (conditioned to $\mathbf{R}_j^{(m)}=\mathbf{r}_j^{(m)}$) as a result of using the MMSE given by (\ref{eq:optimalAwithm2}). This conditional MSE is denoted by $\mathrm{D}_{j}(\mathbf{r}_j^{(m)})$ when $m$ RSSI observations are available, and can be evaluated as $
\mathrm{D}_{j}(\mathbf{r}_j^{(m)})= \mathbb{E}_{|\mathbf{R}_j^{(m)}=\mathbf{r}_j^{(m)}}[|\bold{A}_j\bold{s}_j-\bold{h}_j|^2]$. Finally, the real MSE of the estimator with $m$ RSSI measurements can be expressed as $\Delta_{j;m}=\mathbb{E}_{\mathbf{r}_j^{(m)}}[\mathrm{D}_{j}(\mathbf{r}_j^{(m)})]$. We use $ D^*_{j} $ and $\Delta_{j;m}^*$ to denote the conditional and real MSEs when using the optimal estimator given in (\ref{eq:optimalAwithm2}). This yields:
\begin{align} 
 \label{eq:distor2}
\mathrm{D}_{j}^{*}(\mathbf{r}_j^{(m)})=&\sum_{i=1}^{N}\frac{N_0\mathbb{E}_{|\mathbf{R}_j^{(m)}=\mathbf{r}_j^{(m)}}[|h_{ij}|^2]}{\beta P\mathbb{E}_{|\mathbf{R}_j^{(m)}=\mathbf{r}_j^{(m)}}[|h_{ij}|^2]+N_0}
\\
 \label{eq:distor}
\Delta_{j;m}^{*}=&\mathbb{E}^{*}_{\mathbf{r}_j^{(m)}}[\mathrm{D}^{*}_{j}(\mathbf{r}_j^{(m)})]
\end{align}

Next, we prove that the performance in terms of the real MSE given in (\ref{eq:distor}) is at least as good as the classical estimator for the proposed estimator. We also prove that lesser MSE can be achieved if we use more feedback samples. We formalize this result with the following proposition.

\begin{proposition}
The MSE resulting from the proposed estimator in (\ref{eq:optimalAwithm2}) is a decreasing function of the number of RSSI feedbacks available at any receiver $j$, i.e., $\Delta^*_{j;m} \geq \Delta^*_{j;m+1}$. Additionally, the MSE is lower bounded by a constant
\begin{equation}
\Delta^*_{j;m} \geq \sum_{i=1}^N \mathbb{E}_{h_{ij}}\left[\frac{N_0|h_{ij}|^2}{\beta P|h_{ij}|^2+N_0}\right]
\end{equation}
\end{proposition}

%By introducing low-rate power domain feedback at the end of each time slot from UEs to BS, the channel estimate MSE can be reduced by exploiting this prior information.
%\textcolor{red}{Interpretations for this proposition?}


%The $\mathbb{E}_{|\mathbf{R}_j^{(m)}=\mathbf{r}_j^{(m)}}[\bold{h}_j\bold{h}_j^{\mathrm{H}}]$ term in (\ref{eq:optimalAwithm2}) is not always easy to evaluate, therefore, we provide details on how such an estimator can be designed in practice. First, it can be easily verified that the non-diagonal elements of $\mathbb{E}_{|\mathbf{R}_j^{(m)}=\mathbf{r}_j^{(m)}}[\bold{h}_j\bold{h}_j^{\mathrm{H}}]$ equal to zero, i.e.
%\begin{equation}
%\mathbb{E}_{|\mathbf{R}_j^{(m)}=\mathbf{r}_j^{(m)}}[h_{ij}h_{kj}^{*}]=0\quad(k\neq i)
%\end{equation}
%
%Additionally, note that the chain gain $g_{kj}=|h_{kj}|^2$ follows the exponential distribution with expectation $1$. Hence, the diagonal elements can be expressed as:
%\begin{equation}
%\begin{split}
%&\mathbb{E}_{|\mathbf{R}_j^{(m)}=\mathbf{r}_j^{(m)}}[h_{kj} h_{kj}^{*}]\\
%=&\displaystyle\int_{C_{m}}g_{kj} \exp(-g_{k1}-\dots-g_{kK})\mathrm{d}g_{k1}\dots\mathrm{d}g_{kK}
%\end{split} \label{eq:diagele}
%\end{equation}
%where $C_{m}=\{\bold{g}_j|\forall\,\, 1\leq i\leq m,\quad\bold{g}_j\bold{P}(i)\bold{1}_{K}+N_0=r_{j,i} \}$ with $\bold{g}_j=(g_{1j},\dots,g_{Kj})$ and $\bold{1}_K=(1,\dots,1)^{\mathrm{T}}$  of dimension $K\times 1$. 
%
%The diagonal elements can be easily evaluated if
%\begin{enumerate}
%\item $g_{kj}$ is known for all $k$, or
%\item $g_{kj}$ is known for some $k \in S$, $S \subseteq \mathcal{K}$ and no information other than the statistics in available on $g_{ij}$ with $i = \mathcal{K} \setminus S$. 
%\end{enumerate}
%
%The second case can be satisfied if the $k$ transmitters for $k \in S$ are the only active transmitters while obtaining the RSSI, and they are active in a time-division multiple access (TDMA) manner, i.e., no two transmitters are active simultaneously. This will allow any receiver $j$ to obtain the measurement $P g_{kj} + N_0$ for $k \in S$, thereby evaluating $g_{kj}$ easily. 
%
%The first case can also be satisfied if all $K$ devices operate in TDMA. However, we might also use techniques like that mentioned in \cite{ourTWC}, so that each receiver $j$ obtains $g_{kj}$ for all $k$ where all transmitters will use a pre-designed power level that is known to all receivers in order to obtain $g_{kj}$ from a set of $K$ RSSI measurements (resulting in $K$ equations and $K$ unknowns). 
%
%The first case is especially interesting as the resulting distortion can be simplified to be
%\begin{equation}
%\Delta^{\text{MMSE}*}_{j;m} = \sum_{k=1}^K \mathbb{E}_{h_{kj}}\left[\frac{N_0|h_{kj}|^2}{\beta P|h_{kj}|^2+N_0}\right]
%\end{equation}
%
%We can also prove that the performance in terms of the real distortion given in (\ref{eq:distor}), for the proposed estimator in (\ref{eq:optimalAwithm2}) is at least as good as the classical estimator. Further, it can be demonstrated that with with each additional RSSI measurement available, the new distortion is at most the distortion before, i.e., $\Delta^{\text{MMSE}*}_{j;m+1}\leq \Delta^{\text{MMSE}*}_{j;m}  $. The proofs are omitted due to lack of space available. Next, we present the improved MAP estimate.
%
%
%\begin{proposition}
%The distortion resulting from the proposed estimator in (\ref{eq:optimalAwithm2}) is a decreasing function of the number of RSSI feedbacks available at any receiver $j$, i.e., $\Delta^*_{j;m} \geq \Delta^*_{j;m+1}$. Additionally, the distortion is lower bounded by a constant
%\begin{equation}
%\Delta^*_{j;m} \geq \sum_{k=1}^K \mathbb{E}_{h_{kj}}\left[\frac{N_0|h_{kj}|^2}{\beta P|h_{kj}|^2+N_0}\right]
%\end{equation} \label{prop1}
%\end{proposition}
%
%\begin{proof}
%Define the conditional p.d.f. of $r_{j,m+1}$ knowing $\mathbf{r}_j^{(m)}$ as $f(r_{j,m+1}|\mathbf{r}_j^{(m)})$, the distortion with $m+1$ feedbacks can be rewritten as: 
%\begin{equation}
%\begin{split}
%&\Delta_{j;m+1}^{*}\\
%=&\mathbb{E}_{\mathbf{r}_j^{(m+1)}}[\mathrm{D}^{*}_{j}(\mathbf{r}_j^{(m+1)})]\\
%=&\mathbb{E}_{\mathbf{r}_j^{(m)}}[\displaystyle\int_{r_{j,m+1}}f(r_{j,m+1}|\mathbf{r}_j^{(m)})\mathrm{D}^{*}_{j}(\mathbf{r}_j^{(m+1)})\mathrm{d}r_{j,m+1}]\\
%=&\sum_{k=1}^{K}\mathbb{E}_{\mathbf{r}_j^{(m)}}[\displaystyle\underset{{r_{j,m+1}}}{\int}\frac{f(r_{j,m+1}|\mathbf{r}_j^{(m)})}{\frac{\beta P}{N_0}+\frac{1}{\mathbb{E}_{|\mathbf{R}_j^{(m+1)}=\mathbf{r}_j^{(m+1)}}[|h_{kj}|^2]}}\mathrm{d}r_{j,m+1}]\\
%\end{split}
%\end{equation}
%
%
%Denote  the p.d.f. of $\mathbf{R}_j^{(m+1)}=\mathbf{r}_j^{(m+1)}$ as $f_{m+1}(\mathbf{r}_j^{(m+1)})$ and the p.d.f. of  $\mathbf{R}_j^{(m)}=\mathbf{r}_j^{(m)}$ as $f_{m}(\mathbf{r}_j^{(m)})$, then it can be checked that 
%\begin{equation}
%\frac{f(r_{j,m+1}|\mathbf{r}_j^{(m)})}{f_{m+1}(\mathbf{r}_j^{(m+1)})}=\frac{1}{f_{m}(\mathbf{r}_j^{(m)})}
%\end{equation}
%Assume the p.d.f. of $\bold{h}_j$ is $\phi_j(\bold{h}_j)$, note that for every $k\in\{1,\dots, K\}$, we have
%\begin{equation}
%\begin{split}
%\label{eq:pr_division}
%&\displaystyle\int_{r_{j,m+1}}f(r_{j,m+1}|\mathbf{r}_j^{(m)})\mathbb{E}_{|\mathbf{R}_j^{(m+1)}=\mathbf{r}_j^{(m+1)}}[|h_{kj}|^2]\mathrm{d}r_{j,m+1}\\
%=&\displaystyle\underset{{r_{j,m+1}}}{\int}f(r_{j,m+1}|\mathbf{r}_j^{(m)})\frac{\underset{ \tiny \mathbf{R}_j^{(m+1)}=\mathbf{r}_j^{(m+1)}}{\int}|h_{kj}|^2\phi_j(\bold{h}_j)\mathrm{d}\bold{h}_j}{f_{m+1}(\mathbf{r}_j^{(m+1)})}\mathrm{d}r_{j,m+1}\\
%=&\frac{\displaystyle\int_{r_{j,m+1}}{\displaystyle \underset{\mathbf{R}_j^{(m+1)}=\mathbf{r}_j^{(m+1)}}{\int}|h_{kj}|^2\phi_j(\bold{h}_j)\mathrm{d}\bold{h}_j}\mathrm{d}r_{j,m+1}}{{f_{m}(\mathbf{r}_j^{(m)})}}\\
%=&\frac{{\displaystyle\int_{\mathbf{R}_j^{(m)}=\mathbf{r}_j^{(m)}}|h_{kj}|^2\phi_j(\bold{h}_j)\mathrm{d}\bold{h}_j}}{{f_{m}(\mathbf{r}_j^{(m)})}}\\
%=&\mathbb{E}_{|\mathbf{R}_j^{(m)}=\mathbf{r}_j^{(m)}}[|h_{kj}|^2]
%\end{split}
%\end{equation}
%
%
%
%Additionally, it can be checked from  (\ref{eq:distor2})  that $\mathrm{D}_{j}(\mathbf{r}_{j}^{(m+1)})$ is a concave function with respect to $\mathbb{E}_{|\mathbf{R}_j^{(m+1)}=\mathbf{r}_j^{(m+1)}}[|h_{kj}|^2]$. According to (\ref{eq:pr_division}) and the concavity, it can be obtained that:
%\begin{equation}
%\begin{split}
%&\displaystyle\int_{r_{j,m+1}}\frac{f(r_{j,m+1}|\mathbf{r}_j^{(m)})N_0\mathbb{E}_{|\mathbf{R}_j^{(m+1)}=\mathbf{r}_j^{(m+1)}}[|h_{kj}|^2]}{\beta P\mathbb{E}_{|\mathbf{R}_j^{(m+1)}=\mathbf{r}_j^{(m+1)}}[|h_{kj}|^2]+N_0}\mathrm{d}r_{j,m+1}\\
%\leq& \frac{N_0\mathbb{E}_{|\mathbf{R}_j^{(m)}=\mathbf{r}_j^{(m)}}[|h_{kj}|^2]}{\beta P\mathbb{E}_{|\mathbf{R}_j^{(m)}=\mathbf{r}_j^{(m)}}[|h_{kj}|^2]+N_0}
%\end{split}
%\end{equation}
%which yields
%\begin{equation}
%\begin{split}
%&\Delta_{j;m+1}^{*}\\
%\leq& \sum_{k=1}^{K}\mathbb{E}_{\mathbf{r}_j^{(m)}}[\frac{N_0\mathbb{E}_{|\mathbf{R}_j^{(m)}=\mathbf{r}_j^{(m)}}[|h_{kj}|^2]}{\beta P\mathbb{E}_{|\mathbf{R}_j^{(m)}=\mathbf{r}_j^{(m)}}[|h_{kj}|^2]+N_0}]\\
%=&\Delta_{j;m}^{*}
%\end{split}
%\end{equation}
%
%$\Delta_m^*$ will decrease when more useful prior information is acquired. Hence, when  all the channel gains can be obtained from the feedback, the distortion will be minimized. According to (\ref {eq:distor2}), by knowing all the channel gain, the distortion of the MMSE estimator can be expressed as $\sum_{k=1}^K \mathbb{E}_{h_{kj}}\left[\frac{N_0|h_{kj}|^2}{\beta P|h_{kj}|^2+N_0}\right]$, which is therefore the lower bound on $\Delta_{j;m}^*$.
%\end{proof}
%
%With Proposition \ref{prop1} we have shown that the proposed estimator (\ref{eq:optimalAwithm2}) has a better performance than the classical estimator. However, evaluating the conditional expectation is not always easily solvable. Therefore, in the following subsection, we provide details on a specific power control schemes that would result in a feasible estimator.


\subsection{MAP estimator}

%With $m$ RSSI measurements given by $\mathbf{r}_j^{(m)}$ available at the $j$-th receiver, the MAP estimate will be now given by
%\begin{equation}
%\widehat{h}_{ij}^{\text{MAP}}= \arg \max_{h_{ij}} f_{ij}(h_{ij}|\bold{s}_j ,\mathbf{r}_j^{(m)} )
%\label{eq:newbasicmap}
%\end{equation}
%when $m=0$, i.e., no RSSI measurements are available, this becomes the classical MAP, which in the case of training as described in our system model, and $f_{ij}(.)$ according to $h_{ij} \sim C \mathcal{N}(0,1)$, becomes
%\begin{equation}
%\widehat{h}_{ij}^{\text{MAP}} = \frac{\sqrt{\beta P}}{N_0+ \beta P } s_{ij}
%\label{eq:newbasicmap2}
%\end{equation}
%which coincides with the MMSE when $m=0$. However, when the RSSI estimates are known, the optimization must be done under the condition that $R_{j,1}=r_{j,1},\dots,R_{j,m}=r_{j,m}$, i.e.
%
%
%
%
%% The posteriori probability can be expressed as:
%%\begin{equation}
%%\begin{split}
%%\Pr(\bold{h}|\bold{s},\bold{y}_m)&=\frac{\Pr(\bold{s}|\bold{h},\bold{y}_m)\Pr(\bold{h}|\bold{y}_m)}{\Pr(\bold{s}|\bold{y}_m)}\\
%%&\sim\Pr(\bold{s}|\bold{h},\bold{y}_m)\Pr(\bold{h}|\bold{y}_m)\\
%%&=\Pr(\bold{s}|\bold{h})\Pr(\bold{h}|\bold{y}_m)\\
%%&\sim\Pr(\bold{s}|\bold{h})\Pr(\bold{h})\delta_{\bold{y}_m}\\
%%\end{split}
%%\end{equation}
%%where $\delta_{\bold{y}_m}=1$ when $\{\bold{h}^{\mathrm{H}}\bold{P}(i)\bold{h}+N_0=r(i), \,\, \,\,1\leq i\leq m\}$ and $\delta_{\bold{y}_m}=0$ otherwise. According to the p.d.f. of $\bold{z}$ and ${\bold{h}}$, the posteriori probability can be rewritten as:
%%\begin{equation}
%%\begin{split}
%%\Pr(\bold{h}|\bold{s},\bold{y}_m)&=\frac{\Pr(\bold{s}|\bold{h},\bold{y}_m)\Pr(\bold{h}|\bold{y}_m)}{\Pr(\bold{s}|\bold{y}_m)}\\
%%&\sim\Pr(\bold{s}|\bold{h})\Pr(\bold{h})\delta_{\bold{y}_m}\\
%%&\sim\Pr(\bold{s}-\sqrt{\beta P}\bold{h}|\bold{h})\Pr(\bold{h})\delta_{\bold{y}_m}\\
%%&\sim\Pr(\bold{z}|\bold{h})\Pr(\bold{h})\delta_{\bold{y}_m}\\
%%&=\exp{\left(-\frac{(\bold{s}-\sqrt{\beta P}\bold{h})^H(\bold{s}-\sqrt{\beta P}\bold{h})}{N_0}\right)}\exp{\left(-\bold{h}^H\bold{h}\right)}\delta_{\bold{y}_m}
%%\end{split}
%%\end{equation}
%%Hence, to maximize the $\Pr(\bold{h}|\bold{s},\bold{y}_m)$ is equivalent to the following optimization problem:
%\begin{equation}
%\begin{split}
%&\underset{\bold{h}_j}{\min}\quad \frac{(\bold{s}_j-\sqrt{\beta P}\bold{h}_j)^H(\bold{s}_j-\sqrt{\beta P}\bold{h}_j)}{N_0}+\bold{h}_j^H\bold{h}_j\\
%&\mathrm{s.t.} \quad \bold{h}_j^{\mathrm{H}}\bold{P}(k)\bold{h}_j+N_0=r_{j,k}, \,\, \forall \,\, k \in \{1,2,\dots,m\}
%\end{split}
%\end{equation}
%This problem can be solved by using Lagrange method. The optimal solution $\widehat{\bold{h}}^{\text{MAP}}_j$ can be written as
%\begin{equation}\label{eq:21}
%\begin{split}
%&\widehat{\bold{h}}^{\text{MAP}}_j=\frac{\sqrt{\beta P}[(1+\frac{\beta P}{N_0})I_{K}+\sum_{n=1}^{m}\lambda_n \bold{P}(k)]^{-1}}{N_0}\bold{s}_j\\
%\end{split}
%\end{equation}
%where $\lambda_k$ can be obtained from the constraint
%\begin{equation}\label{eq:22}
%\lambda_k (\tilde{\bold{h}_j}^{H}\bold{P}(k) \tilde{\bold{h}_j}-r_{j,k}+N_0)=0
%\end{equation}
%for all $k \in \{1,2,\dots,m\}$.
%%%Plug (\ref{eq:21}) into (\ref{eq:22}), the $\lambda_1,\dots,\lambda_m$ can be obtained by solving $m$ equations
%%\begin{equation}\label{eq:23}
%%\sum_k^{K}\frac{\beta P s_k^2 P_k(i)}{[(N_0+{\beta P})+N_0\sum_{j=1}^{m}\lambda_j P_k(j)]^2}=r_i-N_0
%%\end{equation}
%%where $i\in\{1,\dots,m\}$ and $s_k$ is the $k$-th element of the received signal $\bold{s}$.
%However, there are several solutions for the above set of equations and the complexity is too high to get relevant solutions. Hence, for the sake of simplicity, we assume that by combining the feedbacks (or with channel selection transmission policy), $|h_{nj}|^2$ or $g_{nj}$ can be perfectly known for $n \in S$ where $S \subseteq \mathcal{K}$ and $|S| \leq m$, as explained in the previous subsection. This results in a simpler optimization problem given by
%\begin{equation}
%\begin{split}
%&\underset{\bold{h}_j}{\min}\quad \frac{(\bold{s}_j-\sqrt{\beta P}\bold{h}_j)^H(\bold{s}_j-\sqrt{\beta P}\bold{h}_j)}{N_0}+\bold{h}_j^H\bold{h}_j\\
%&\mathrm{s.t.} \quad |h_{nj}|=g_{nj}, \,\,\forall \,\,n \in S
%\end{split}
%\end{equation}
%The solution of the simplified problem can be written as:
%
%\begin{equation}\label{eq:25}
%\begin{split}
%\widehat{\bold{h}}^{\text{MAP}}_j=\frac{\sqrt{\beta P}[(1+\frac{\beta P}{N_0})\bold{I}_{K}+\bold{Q}_{\lambda}]^{-1}}{N_0}\bold{s}_j\\
%\end{split}
%\end{equation}
%\begin{equation}\label{eq:26}
%\frac{\beta P |s_{nj}|^2 }{[(N_0+{\beta P})+N_0\lambda_n]^2}=g_{nj}
%\end{equation}
%where $\bold{Q}_{\lambda}=\diag(q_1,\dots,q_K)$ with $q_k=\lambda_k$ when $k \in S$ and $q_k=0$ otherwise.
%With (\ref{eq:26}), for $n\in S$, we have:
%\begin{equation}
%\lambda_n=\frac{\displaystyle{\pm\sqrt{\frac{\beta P |s_{nj}|^2}{g_{nj}}}-(N_0+{\beta P})}}{N_0}
%\end{equation}
%To determine the sign of $\lambda_n$ which minimizes the cost function in (\ref{eq:25}), the optimal $\lambda_n$ can be expressed as:
%\begin{equation}
%\lambda_n=\frac{\displaystyle{ \sqrt{\frac{\beta P |s_{nj}|^2}{g_{nj}}}-(N_0+{\beta P})}}{N_0}
%\end{equation}
%for all $n \in S$. Therefore, the MAP estimator can be simplified and rewritten as:
%\begin{equation}
%\widehat{h}_{nj}^{\text{MAP}}= g_{nj} \frac{s_{nj}}{|s_{nj}|}
%\end{equation}
%for all $n \in S$ and $\widehat{h}_{ij}^{\text{MAP}}$ given by (\ref{eq:newbasicmap2}) when $i \in \mathcal{K} \setminus S$. 



As with the MMSE estimator, we can use the information provided by RSSI measurements to propose a novel MAP estimator which reduces the MSE. Denoting $m$ RSSI measurements available at the $j$-th receiver by $\mathbf{r}_j^{(m)}$, the MAP estimate can be given by
{\footnotesize \begin{equation}
\begin{split}
\widehat{h}_{ij}^{\text{MAP:m}}&\in \arg \max_{h_{ij}} f_{h|s,r}(h_{ij}|{s}_{ij} ,\mathbf{r}_j^{(m)} )\\
&\in \arg \max_{h_{ij}} f_{s|h,r}(s_{ij}|h_{ij},\mathbf{r}_j^{(m)} )f_{r|h}(\mathbf{r}_j^{(m)}|h_{ij} )f_{h}(\mathbf{r}_j^{(m)}|h_{ij} )\\
&\in \arg \max_{h_{ij}} f_{r|h}(\mathbf{r}_j^{(m)}|h_{ij} )\exp(\frac{|s_{ij}-\sqrt{\beta P h_{ij}}|^2}{N_0})\exp(\frac{|h_{ij}|^2}{\sigma_{ij}^2})
\end{split}
\label{eq:newbasicmap}
\end{equation}}
When no RSSI measurements are available i.e., $m=0$, this reduces to the classical MAP as follows 
\begin{equation}
\widehat{h}_{ij}^{\text{MAP}:0} = \frac{\sqrt{\beta P}\sigma_{ij}^2}{N_0+ \beta P\sigma_{ij}^2 } s_{ij},
\label{eq:MAP_conventional}
\end{equation}
which also coincides with the classical MMSE. However, when the RSSI estimates are known, $f_{r|h}(\mathbf{r}_j^{(m)}|h_{ij} )$ becomes a Dirac function. Therfore, the optimization must be performed under the constraints that $R_{j}^{(1)}=r_{j}^{(1)},\dots,R_{j}^{(m)}=r_{j}^{(m)}$. The optimization problem can be rewritten as:

% The posteriori probability can be expressed as:
%\begin{equation}
%\begin{split}
%\Pr(\bold{h}|\bold{s},\bold{y}_m)&=\frac{\Pr(\bold{s}|\bold{h},\bold{y}_m)\Pr(\bold{h}|\bold{y}_m)}{\Pr(\bold{s}|\bold{y}_m)}\\
%&\sim\Pr(\bold{s}|\bold{h},\bold{y}_m)\Pr(\bold{h}|\bold{y}_m)\\
%&=\Pr(\bold{s}|\bold{h})\Pr(\bold{h}|\bold{y}_m)\\
%&\sim\Pr(\bold{s}|\bold{h})\Pr(\bold{h})\delta_{\bold{y}_m}\\
%\end{split}
%\end{equation}
%where $\delta_{\bold{y}_m}=1$ when $\{\bold{h}^{\mathrm{H}}\bold{P}(i)\bold{h}+N_0=r(i), \,\, \,\,1\leq i\leq m\}$ and $\delta_{\bold{y}_m}=0$ otherwise. According to the p.d.f. of $\bold{z}$ and ${\bold{h}}$, the posteriori probability can be rewritten as:
%\begin{equation}
%\begin{split}
%\Pr(\bold{h}|\bold{s},\bold{y}_m)&=\frac{\Pr(\bold{s}|\bold{h},\bold{y}_m)\Pr(\bold{h}|\bold{y}_m)}{\Pr(\bold{s}|\bold{y}_m)}\\
%&\sim\Pr(\bold{s}|\bold{h})\Pr(\bold{h})\delta_{\bold{y}_m}\\
%&\sim\Pr(\bold{s}-\sqrt{\beta P}\bold{h}|\bold{h})\Pr(\bold{h})\delta_{\bold{y}_m}\\
%&\sim\Pr(\bold{z}|\bold{h})\Pr(\bold{h})\delta_{\bold{y}_m}\\
%&=\exp{\left(-\frac{(\bold{s}-\sqrt{\beta P}\bold{h})^H(\bold{s}-\sqrt{\beta P}\bold{h})}{N_0}\right)}\exp{\left(-\bold{h}^H\bold{h}\right)}\delta_{\bold{y}_m}
%\end{split}
%\end{equation}
%Hence, to maximize the $\Pr(\bold{h}|\bold{s},\bold{y}_m)$ is equivalent to the following optimization problem:
\begin{equation}
\begin{split}
&\underset{\bold{h}_j}{\min}\quad \frac{(\bold{s}_j-\sqrt{\beta P}\bold{h}_j)^H(\bold{s}_j-\sqrt{\beta P}\bold{h}_j)}{N_0}+{\bold{h}_j^H\bold{D}_j^{-1}\bold{h}_j}\\
&\mathrm{s.t.} \quad \bold{h}_j^{\mathrm{H}}\bold{P}(n)\bold{h}_j+N_0=r_{j}^{(n)}, \,\, \forall \,\, n \in \{1,2,\dots,m\}
\end{split}
\label{eq:MAP_op_origin}
\end{equation}
where  $\bold{P}(n)=\diag (P_1(n),\dots,P_N(n))$. This problem can be solved by using the Lagrange multiplier method. The optimal solution $\widehat{\bold{h}}^{\text{MAP:m}}_{ij}$ can be written as
\begin{equation}\label{eq:21}
\widehat{\bold{h}}^{\text{MAP:m}}_{ij}=\frac{\sqrt{\beta P}}{N_0(\frac{1}{\sigma^2_{ij}}+\frac{\beta P}{N_0}+\sum_{n=1}^{m}\lambda_n P_i(n))}{s}_{ij}
\end{equation}
where $\lambda_n$ can be obtained from the constraint $
\lambda_n ({\bold{h}_j}^{H}\bold{P}(n) {\bold{h}_j}-r_{j}^{(n)}+N_0)=0$ for all $n \in \{1,2,\dots,m\}$.
%%Plug (\ref{eq:21}) into (\ref{eq:22}), the $\lambda_1,\dots,\lambda_m$ can be obtained by solving $m$ equations
%\begin{equation}\label{eq:23}
%\sum_k^{K}\frac{\beta P s_k^2 P_k(i)}{[(N_0+{\beta P})+N_0\sum_{j=1}^{m}\lambda_j P_k(j)]^2}=r_i-N_0
%\end{equation}
%where $i\in\{1,\dots,m\}$ and $s_k$ is the $k$-th element of the received signal $\bold{s}$.
\textcolor{black}{If the number of timeslots $m$ is less than the number of antennas, then there are possibly infinite solutions for the above system of equations. To circumvent this problem,  we assume that we can perfectly know $|h_{ij}|^2$ or $g_{ij}$ for $i \in S$ by exploiting $m$ RSSI measurements where $S$ is defined as follows:} $S=\{i\in\{1,\dots,N\} |\quad |h_{ij}|^2\,\,\text{can be obtained from RSSI} \}$. Based on this relaxation, the RSSI measurements are treated as the measurements of channel gains/magnitude $ |h_{ij}|^2$, and the optimization problem defined by (\ref{eq:MAP_op_origin}) can be simplified as:
\begin{equation}
\begin{split}
&\underset{\bold{h}_j}{\min}\quad \frac{(\bold{s}_j-\sqrt{\beta P}\bold{h}_j)^H(\bold{s}_j-\sqrt{\beta P}\bold{h}_j)}{N_0}+{\bold{h}_j^H\bold{D}_j^{-1}\bold{h}_j}\\
&\mathrm{s.t.} \quad |h_{ij}|^2=g_{ij}, \,\,\forall \,\,i \in S
\end{split}
\end{equation}
The solution of the simplified problem can be found by solving the following equations:
\begin{equation}\label{eq:25}
\widehat{{h}}^{\text{MAP:m}}_{ij}=\frac{\sqrt{\beta P}}{N_0(\frac{1}{\sigma^2_{ij}}+\frac{\beta P}{N_0}+q_i)}{s}_{ij}\\
\end{equation}
\begin{equation}\label{eq:26}
\frac{\beta P |s_{ij}|^2 }{[(\frac{N_0}{\sigma_{ij}^2}+{\beta P})+N_0\lambda_i]^2}=g_{ij} , \,\,\forall \,\,i \in S
\end{equation}
where  $q_i=\lambda_i$ when $i \in S$ and $q_i=0$ otherwise.
With (\ref{eq:26}), for $i\in S$, we have:
\begin{equation}
\lambda_i=\frac{\displaystyle{\pm\sqrt{\frac{\beta P |s_{ij}|^2}{g_{ij}}}-(\frac{N_0}{\sigma_{ij}^2}+{\beta P})}}{N_0}.
\end{equation}
%To determine the sign of $\lambda_i$ which minimizes the cost function in (\ref{eq:25}), the optimal $\lambda_i$ can be expressed as:
%\begin{equation}
%\lambda_i=\frac{\displaystyle{ \sqrt{\frac{\beta P |s_{ij}|^2}{g_{ij}}}-(\frac{N_0}{\sigma_{ij}^2}+{\beta P})}}{N_0}
%\end{equation}
%for all $i \in S$. 
Therefore, the MAP estimator can be simplified and rewritten as:
\begin{equation}
\widehat{h}_{ij}^{\text{MAP:m}}=
\left\{
\begin{array}{ll}
\sqrt{g_{ij}} \frac{s_{ij}}{|s_{ij}|} & \hbox{ $i\in S$} \\\\
\widehat{h}_{ij}^{\text{MAP}:0} & \hbox{ otherwise} \\\\
\end{array}
\right.
\end{equation}
The proposed MAP will ensure the maximum a posteriori probability. However the minimization of the MSE is not always guaranteed. Indeed, it can be seen from the following proposition that the proposed MAP brings more MSE in the low SNR regime.
\begin{proposition}
Assuming all the channel gains can be obtained from the RSSI measurements, i.e., $S=\{1,\dots,N\}$, the following equalities can be derived
{\footnotesize \begin{equation}
\underset{\mathrm{SNR}\rightarrow 0}{\lim}\mathbb{E}_{h_{ij},z_{ij}}[|\widehat{h}_{ij}^{\text{MAP:m}}-h_{ij}|^2]=2\underset{\mathrm{SNR}\rightarrow 0}{\lim}\mathbb{E}_{h_{ij},z_{ij}}[|\widehat{h}_{ij}^{\text{MAP}}-h_{ij}|^2]
\end{equation}
\begin{equation}
\underset{\mathrm{SNR}\rightarrow \infty}{\lim}\mathbb{E}_{h_{ij},z_{ij}}[|\widehat{h}_{ij}^{\text{MAP:m}}-h_{ij}|^2]=\frac{1}{2}\underset{\mathrm{SNR}\rightarrow \infty}{\lim}\mathbb{E}_{h_{ij},z_{ij}}[|\widehat{h}_{ij}^{\text{MAP}}-h_{ij}|^2]
\end{equation}}
\end{proposition}

%According to this proposition, it can be seen the proposed MAP is worse than the classical MAP in low SNR regime (See Fig.~\ref{fig:MAP}). Nonetheless, it can be seen in the following section that the proposed MAP reduces MSE considerably when the SNR is not low and converges to  about 50\% in high SNR regime.

%\subsection{Connections between the proposed MMSE and MAP}
%Here we discuss the connections between the two different estimators. Since the MAP assumes the knowledge of $g_{ij}$, we will compare MMSE with MAP when $g_{ij}$ is known. In this scenario, the MMSE and MAP can be rewritten as:
%\begin{equation}
%{h}_{ij}^{\mathrm{MMSE}:m}=\frac{ \sqrt{\beta P}g_{ij}}{\sqrt{\beta P}g_{ij}+N_0} s_{ij}
%\end{equation}
%\begin{equation}
%{h}_{ij}^{\mathrm{MAP}:m}=\sqrt{g_{ij}} \frac{s_{ij}}{|s_{ij}|}
%\end{equation}
%
%\begin{proposition}
%When $g_{ij}$ is known, the proposed MMSE and the proposed MAP coincide if and only if $|s_{ij}|=\sqrt{\beta Pg_{ij}}+\frac{N_0}{\sqrt{\beta Pg_{ij}}}$.
%\end{proposition}
%\begin{proof}
%Trivial.
%\end{proof}
%XXX: remove this subsection?
\section{Numerical results}
\label{sec:num}
In the simulations, we fix number of transmit antennas as$N=4$, number of users as $K=4$ and observe the distortion performance at different SNR. For ease of exposition, we assume $\bold{D}_j$ is an identity matrix in the simulations. i.e. $\bold{D}_j=\bold{I}_N$. As seen from (\ref{eq:MMSE_conventional}) and (\ref{eq:MAP_conventional}), the classical MMSE coincides with the classical MAP in our model. We define the relative MSE reduction as \begin{equation}
\frac{\Delta_{j;0}^{*}-\Delta_{j;m}^{\mathrm{X}*}}{\Delta_{j;0}^{*}}\times 100\%
\end{equation}
 with $\text{X}\in\{\text{MMSE}, \text{MAP}\}$. We assume that from each feedback, we can perfectly reconstruct one channel gain, i.e., with $m$ feedbacks, Receiver $j$ can acquire $g_{1j},\dots,g_{mj}$.  
 
 First, we study the influence of the prior information on the MMSE estimator. We compare the reduced MSE in terms of number of feedback samples for different SNR. Fig.~1 shows that the MSE can be mitigated by using the RSSI measurements and the MSE decreases with more measurements available. In the low SNR regime, the MMSE estimator does not perform well since $\mathbf{s}_j$ is multiplied by a constant matrix $\bold{A}_j$. Hence, even if we can improve the selection of $\bold{A}_j$ by knowing the RSSI, the MSE can not be reduced much due to the high noise level. On the other hand, in the high SNR regime, the classical MMSE estimator is already accurate. This results in limited gains for the high SNR regime too. Our technique can bring more improvements in the mid-SNR range. 
 
 We conduct a similar analysis for the new MAP estimator. The results are shown in Fig.~2. From Fig.~2, we see that unlike in the MMSE case, the MSE does not necessarily reduce with higher number of RSSI feedback samples. Indeed, as proved in Prop.~III.2, the MSE for the new MAP might actually increase in the low SNR regime. It has been checked that the degradation in estimation is close to 100\% when SNR is less than -40 dB. However, when SNR increases, the RSSI measurements can lead to a significant reduction in MSE and it can even outperform the new MMSE. 

 \begin{figure}[!h]
   \begin{center}
        \includegraphics[width=0.44\textwidth]{ICASSP_MMSE.eps}
    \end{center}
  \caption{The MMSE estimator brings a significant improvements in the range [-10dB,+10dB]}
   \label{fig:MMSE}
\end{figure}
\begin{figure}[!h]
   \begin{center}
        \includegraphics[width=0.44\textwidth]{ICASSP_MAP.eps}
    \end{center}
  \caption{The MAP estimator performs very well for sufficiently high SNRs}
   \label{fig:MAP}
\end{figure}




\section{Conclusion and future works}

Under some classical assumptions on the residual noise produced by least-squares estimates, it is known that the de-noising matrices obtained by using MMSE and MAP de-noising coincide \cite{lasaulce-vtc-2001}. In the presence of additional prior coming from the power domain, this result is no longer true, which explains that MMSE and MAP performance do not perform similarly here. In terms of MSE performance, it is recommended to use the proposed MMSE in the moderate SNR regime and the proposed MAP in the high SNR regime. By doing so, RSSI feedback can be used not only for resource allocation purposes (as currently done) but also for improving channel estimation. In future works, we intend to extend this framework to goal-oriented communication systems, where estimation quality is not only to recover the signal itself but also to serve the system to better accomplish the goal \cite{Zhang-AE-2021}\cite{Zhang-JSAC-2022}\cite{Zhang-IoTM-2022}.


\section*{Acknowledgement}
This work is partly supported by PGMO project fund from the Fondation Hadamard des Math\'ematiques.
% ===================================================================================== %

\bibliographystyle{unsrt}
\begin{thebibliography}{10}
\footnotesize
\bibitem{Li-2002}
Y. Li, "Simplified channel estimation for OFDM systems with multiple transmit antennas", IEEE Trans. on Wireless Comm., 1(1), 67-75, 2002.
\bibitem{Fang-TWC-2017} J. Fang, X. Li, H. Li and F. Gao, "Low-Rank Covariance-Assisted Downlink Training and Channel Estimation for FDD Massive MIMO Systems," IEEE Trans. Wireless Comm., vol. 16, no. 3, 2017.
\bibitem{Choi-TSP-2014} J. Choi, D. J. Love and P. Bidigare, "Downlink Training Techniques for FDD Massive MIMO Systems: Open-Loop and Closed-Loop Training With Memory," in IEEE J. of Selected Topics in Sig. Proc., vol. 8, no. 5, pp. 802-814, Oct. 2014.
\bibitem{Inter-TWC-2019} G. Interdonato, H. Q. Ngo, P. Frenger and E. G. Larsson, "Downlink Training in Cell-Free Massive MIMO: A Blessing in Disguise," IEEE Trans.  Wireless Comm., vol. 18, no. 11, pp. 5153-5169, Nov. 2019.
\bibitem{caire-tit-2010}G. Caire, N. Jindal, , M. Kobayashi,  and N. Ravindran, "Multiuser MIMO achievable rates with downlink training and channel state feedback", IEEE Trans. on Info. Theory, 56(6), pp.2845-2866, 2010.
\bibitem{blind-2002}
B. Muquet, M. De Courville, and P.  Duhamel, "Subspace-based blind and semi-blind channel estimation for OFDM systems",  IEEE Trans. on Sig. Proc., 50(7), 1699-1712, 2002.
\bibitem{Zhang-TWC-2017} C. Zhang, V. Varma, S. Lasaulce, and R. Visoz, "Interference Coordination via Power Domain Channel Estimation", IEEE Trans. on Wireless Comm., Vol. 16, No. 10, pp. 6779-6794, 2017.
\bibitem{Panda-ICACCI-2016} A. V. Panda, S. K. Mishra, S. S. Dash and N. Bansal, "A suboptimal scheduling scheme for MIMO broadcast channels with quantized SINR," 2016 ICACCI, Jaipur, 2016, pp. 2175-2179.  
\bibitem{Su-TVT-2015} D. Su, C. Yang, G. Wang and M. Lei, "SINR Estimation in Limited Feedback Coordinated Multipoint Systems," in IEEE Trans. on Veh. Tech., vol. 64, no. 5, pp. 2180-2185, May 2015.
\bibitem{vincent-book} H. V. Poor,  "An introduction to signal detection and estimation". Springer Science and Business Media, 2013.
\bibitem{lasaulce-vtc-2001} S. Lasaulce, P. Loubaton, E. Moulines, and S. Buljor, "Training-based channel estimation and de-noising for the UMTS TDD mode", IEEE VTC Fall 2001, Vol. 3, pp. 1908-1911.
\bibitem{Zhang-AE-2021}
 C. Zhang, S. Lasaulce, M. Hennebel, L. Saludjian, P. Panciatici, and H. V. Poor, "Decision-making oriented clustering: Application to pricing
and power consumption scheduling,'' \textit{Appl. Energy}, vol. 297, 2021.
\bibitem{Zhang-JSAC-2022}
H. Zou, C. Zhang, S. Lasaulce, L. Saludjian, and H. V. Poor, "Goal-oriented quantization: Analysis, design, and application to resource allocation,'' \textit{IEEE J. Sel. Areas Commun.}, vol. 41, no. 1, pp. 42-54, Jan. 2023.
\bibitem{Zhang-IoTM-2022}
C. Zhang, H. Zou, S. Lasaulce, W. Saad, M. Kountouris and M. Bennis, "Goal-Oriented Communications for the IoT and Application to Data Compression," \textit{IEEE IoT Mag.}, vol. 5, no. 4, pp. 58-63, December 2022.
\end{thebibliography}

% ===================================================================================== %

\end{document}
