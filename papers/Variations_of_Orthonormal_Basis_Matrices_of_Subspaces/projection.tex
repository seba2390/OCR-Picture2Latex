\documentclass[11pt]{article}
\usepackage{color}
\textwidth 15.5cm
\textheight 21.5cm
\hoffset -1.35cm
\voffset -1.25cm
\synctex=1

%\def\TeXHOME{../..}
%\input \TeXHOME/defnAMS.tex

%\usepackage{amsmath,amsfonts,amsthm}
%\usepackage{amssymb,latexsym}
%\usepackage{kbordermatrix}
%\usepackage{algorithm}
%\usepackage{algorithmic}
%%\RequirePackage{doi}
%\usepackage{hyperref}
%\numberwithin{algorithm}{section}

\usepackage{graphicx}
\usepackage{amsmath,amsfonts,amsthm}
\usepackage{amssymb,latexsym}
\usepackage{fixmath}
\usepackage{mathrsfs,amsbsy}
\usepackage{dsfont}
\usepackage{enumerate}
\usepackage{algorithm,algorithmic}
\usepackage{kbordermatrix}
\usepackage{xcolor}
\usepackage{soul,xcolor}
\setstcolor{red}
\def\bbC{\mathbb{C}}
\def\bbO{\mathbb{O}}
\def\bbR{\mathbb{R}}
\def\bbX{\mathbb{X}}

\def\one{\pmb{1}}

\def\cR{{\cal R}}
\def\cU{{\cal U}}
\def\cV{{\cal V}}
\def\cX{{\cal X}}

\def\sss{\scriptscriptstyle}

\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\dist}{dist}
\DeclareMathOperator{\eig}{eig}
\DeclareMathOperator*{\opt}{opt}
\DeclareMathOperator{\ui}{ui}
\DeclareMathOperator{\Sq}{Sq}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\re}{Re}
\DeclareMathOperator{\tr}{tr}

\DeclareMathOperator{\F}{F}
\DeclareMathOperator{\HH}{H}
\DeclareMathOperator{\T}{T}
\DeclareMathOperator{\UI}{ui}


\def\wtd{\widetilde}
\def\what{\widehat}

\def\hadm{{\tt hadamard}}
\def\randn{{\tt randn}}
\def\orth{{\tt orth}}
\def\bestr{\mbox{\scriptsize\rm best-$r$}}

\newtheorem{proposition}{Proposition}[section]
\newtheorem{theorem}{Theorem}[section]
%\newtheorem{algorithm}{Algorithm}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{conjecture}{Conjecture}[section]
\newtheorem{exercise}{Exercise}[section]

\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{remark}{Remark}[section]
\newtheorem{example}{Example}[section]


%\renewcommand{\theequation}{\thesection.\arabic{equation}}
%\renewcommand{\thefigure}{\thesection.\arabic{figure}}
%\renewcommand{\thetable}{\thesection.\arabic{table}}

\numberwithin{equation}{section}
\numberwithin{figure}{section}
\numberwithin{table}{section}

\title{Variations of Orthonormal Basis Matrices of Subspaces}

\author{Zhongming Teng\thanks{%
   College of Computer and Information Science, Fujian Agriculture and Forestry University, Fuzhou, 350002, P. R. China.
Email: {\tt zhmteng@fafu.edu.cn}}
\and  Ren-Cang Li\thanks{%
   Department of Mathematics, University of Texas at Arlington, Arlington, TX 76019-0408,
USA.  E-mail: {\tt rcli@uta.edu}.  Supported in part by NSF DMS-2009689.}
}
\date{\today}

\begin{document}
%\setcounter{section}{1}
\maketitle

\begin{abstract}
An orthonormal basis matrix $X$ of a subspace ${\cal X}$ is known not to be unique, unless there are some kinds of normalization requirements.
One of them is to require that $X^{\T}D$ is positive semi-definite, where $D$ is a
constant matrix of apt size. It is  a natural one in multi-view subspace learning
models in which $X$ serves as a projection matrix and is determined by
a maximization problem over the Stiefel manifold whose objective function contains
and increases with $\tr(X^{\T}D)$.
%
This paper is concerned with bounding the change in orthonormal basis matrix $X$ as subspace ${\cal X}$ varies
under the requirement that $X^{\T}D$ stays positive semi-definite.
% is the positive semi-definite, where $D$ is a
%constant matrix and has the same size as $X$.
The results are useful in
convergence analysis of the NEPv approach (nonlinear eigenvalue problem
with eigenvector dependency) to solve the maximization problem.
%It can be proved that at maximality, $X^{\T}D$ is positive semi-definite.
%The basic idea of the NEPv approach is to iteratively solve
%associated NEPv via the Self-Consistent-Field-type (SCF-type) iteration, whose convergence analysis  often begins
%with showing that the ranges of iterative approximations converge to a target subspace,
%but it remains to analyze if the approximations as matrices converge to a particular
%orthonormal basis matrix of the target subspace and quantify the convergence.
%
%
%Recently a few optimization problems on the Stiefel manifold
%
%The key of a successful subspace learning model is to find a high quality projection matrix to project high dimensional
%data to a low dimensional subspace. Often the projection matrix is determined by an optimization problem over
%the Stiefel manifold.
%
%A few recent subspace learning models appear as maximization problems over
%the Stiefel manifold whose optimal solutions are the sought projection matrices that are later used to
%project high dimensional data to low dimensional subspaces. Numerically,
%These projection matrices are often determined up to their ranges numerically, more so
%in the convergence analysis of iterative numerical methods, where it is often shown that the ranges of
%computed approximations converge. There are cases that this is the best one can do as in the linear discriminant analysis (LDA) where
%the objective function to be optimized is actually a function over subspaces, i.e., taking the same values at any
%orthonormal basis matrix of a subspace. But recently, we have encountered a few objective functions that
%contain the term $\tr(X^{\T}D)$ and increase with $\tr(X^{\T}D)$.
%
%For computing a global maximizer $X$ whose columns span the subspace $\cX$,
%in addition to SCF iterations for the associated eigenvector-dependent nonlinear eigenvalue problem,
%the basis alignment operation is also a key step to keep $X^{\T}D$ being symmetric positive semidefinite.
%When an invariant subspace $\wtd\cX$ gotten by SCF iterations is close to $\cX$ in terms of their canonical angles,
%in this paper, we quantify the upper bounds on the difference between $X$ and the basis matrix $\wtd X$ of
%the subspace $\wtd\cX$ such that $\wtd X^{\T}D$ is symmetric positive semidefinite.
%These bounds will be characterized by a general unitarily invariant norm of the canonical angles between $\wtd\cX$ and $\cX$.
%The work extends the perturbation theory for the orthonormal basis matrices approximations
%and helps assess how accurate the approximate maximizer $\wtd X$ is when $\wtd\cX$ move towards $\cX$ in SCF iterations.
\end{abstract}

\medskip
{\small
{\bf Key words.} Unitarily invariant norm, canonical angle, subspace,
orthonormal basis matrix,  optimization on Stiefel manifold.

\medskip
{\bf Mathematics subject classifications (2010).} 15A45, 65F35
}

\section{Introduction}\label{sec:intro}
\iffalse
Dimensionality reduction is a cornerstone technique for analyzing high-dimensional data.
Linear dimensionality reduction (LDR) is the most widely used one because of its simple geometric interpretation and
computational convenience \cite{cugh:2015}. In a nutshell, LDR
seeks a projection matrix $P\in\bbR^{n\times k}$ to project high-dimensional vector
$\bx\in\bbR^n$ to a (much) lower-dimensional vector $\bz=P^{\T}\bx\in\bbR^k$ where  $k\ll n$. Usually projection matrix $P$
is an optimizer of some optimization problem on the Stiefel manifold:
\begin{equation}\label{eq:OptSTM}
\max_{X^{\T}X=I_k} f(X),
\end{equation}
where $f(X)$ is the objective function and the Stiefel manifold
$$
\bbO^{n\times k}:=\{X\in\bbR^{n\times k}\,:\, X^{\T}X=I_k,\,\mbox{the $k\times k$ identity matrix}\}.
$$
%$I_k$ is the $k\times k$ identity matrix.
Examples include
the orthogonal linear discriminant analysis (OLDA) \cite{ngbs:2010,zhln:2010,zhln:2013},
the orthogonal canonical correlation analysis (OCCA) \cite{zhwb:2022},
and the unbalanced Procrustes problem \cite{chtr:2001,edas:1999,elpa:1999,godi:2004,huca:1962,zhys:2020,zhdu:2006},
among others \cite{ball:2022,wazl:2022,wazl:2022a,zhys:2020}. In those applications, objective function has strong
linear algebra characteristics, for example it often involves some large scale matrices constructed from
high-dimensional data, and its first order optimality condition, also known as the KKT condition, takes the form or can be made equivalent to
some nonlinear eigenvalue problem with eigenvector dependency (NEPv), a term that was first coined by \cite{cazb:2018}:
\begin{equation}\label{eq:NEPv-form}
H(X)X=X\Omega,\quad X^{\T}X=I_k,
\end{equation}
where $H(X)$, dependent of $X$, is symmetric. NEPv of such form are not new, however, and in fact before
\cite{zhln:2010,zhln:2013} where OLDA was solved through NEPv, they mostly come from solving the discretized
Korn-Sham equation from the density functional theory \cite{hoko:64,kosh:1965,robl:2012,yaml:2009}.
Numerically, NEPv~\ref{eq:NEPv-form} is often solved by the self-consistent-field (SCF) iteration:
given $X_0\in\bbO^{n\times k}$,
\begin{equation}\label{eq:SCF0}
\mbox{iteratively solve $H(X_{i-1})X_i=X_i\Omega_i$ for $X_i$},
\end{equation}
until convergence, where each $H(X_{i-1})X_i=X_i\Omega_i$ is a symmetric eigenvalue problem \cite{parl:1998}.

Except for OLDA,
many objective functions $f$ of the involved optimization problems contains $\tr(X^{\T}D)$ in such way that
$f(X)$ increases as $\tr(X^{\T}D)$
\fi
Recently in \cite{luli:2022}, the following optimization problem over the Stiefel manifold $\bbO^{n\times k}$
\begin{equation}\label{eq:OptSTM}
\max_{X\in\bbO^{n\times k}} f(X)\quad\mbox{with}\,\, f(X):=\phi(X)+\psi(X)\times\tr(X^{\T}D)
\end{equation}
%\eqref{eq:OptSTM} with
%\begin{equation}\label{eq:f(X)-luli}
%f(X):=\phi(X)+\psi(X)\times\tr(X^{\T}D)
%\end{equation}
is  considered, where $D$ is a constant matrix, the Stiefel manifold
$$
\bbO^{n\times k}=\{X\in\bbR^{n\times k}\,:\, X^{\T}X=I_k\,\,(\mbox{the $k\times k$ identity matrix})\},
$$
$\phi$ and $\psi$ are two unitarily invariant functions in the sense
that $\phi(XQ)\equiv\phi(X)$ and $\psi(XQ)\equiv\psi(X)$ for any $Q\in\bbO^{k\times k}$ and $\psi(X)>0$.
It is an abstraction of a few concrete problems arising from subspace learning
\cite{wazl:2022a,wazl:2022,zhys:2020,zhwb:2022},
where objective functions  contain $\tr(X^{\T}D)$ and
increase with $\tr(X^{\T}D)$.
Along the lines of earlier research in \cite{wazl:2022,zhys:2020,zhwb:2022},
the authors of \cite{luli:2022} started by transforming the KKT condition for \eqref{eq:OptSTM}
into an NEPv (nonlinear eigenvalue problem with eigenvector dependency)
\begin{equation}\label{eq:NEPv-form}
H(X)X=X\Omega,\quad X\in\bbO^{n\times k},
\end{equation}
where $H(X)$, dependent of $X$, is symmetric. NEPv of this form are not new, however, and in fact before
\cite{zhln:2010,zhln:2013} where orthogonal linear discriminant analysis (OLDA)
was first solved through NEPv, they mostly come from solving discretized
Kohn-Sham equations from the density functional theory \cite{hoko:64,kosh:1965,robl:2012,yaml:2009}.
Numerically, NEPv~\eqref{eq:NEPv-form} is often solved by the so-called {\em self-consistent-field\/} (SCF) iteration.
In \cite{luli:2022}, NEPv~\eqref{eq:NEPv-form} is solved by an SCF-type iteration:
given $X_0\in\bbO^{n\times k}$,
\begin{equation}\label{eq:SCF1}
\mbox{iteratively solve $H(X_{i-1})\what X_i=\what X_i\Omega_i$ for $\what X_i$ which is postprocessed to get $X_i$},
\end{equation}
until convergence, where the postprocessing yields $X_i=\what X_iQ_i$ for some $Q_i\in\bbO^{k\times k}$ such that
$X_i^{\T}D\succeq 0$ (positive semidefinite). $Q_i$ is often taken to be an orthogonal polar factor of
$X_i^{\T}D$ \cite{luli:2022,wazl:2022,zhys:2020,zhwb:2022},
owing to the fact that $f(X)$  is monotonically increasing
in $\tr(X^{\T}D)$ and that $Q_*\in\bbO^{k\times k}$ such that
$(XQ_*)^{\T}D=Q_*^{\T}(X^{\T}D)\succeq 0$ (positive semidefinite) ensures \cite{luli:2022,wazl:2022}
\begin{equation}\label{eq:tr-incr}
\tr([XQ_*]^{\T}D)=\tr(Q_*^{\T}[X^{\T}D])=\max_{Q\in\bbO^{k\times k}}\tr(Q^{\T}[X^{\T}D])\ge\tr( X^{\T}D),
\end{equation}
and the inequality is strict if $X^{\T}D\not\succeq 0$.

The SCF-type iteration \eqref{eq:SCF1} differs from
the classical SCF  for solving discretized
Kohn-Sham equations in its postprocessing from $\what X_i$ to $X_i$, which is not needed in the classical SCF for NEPv
that is unitarily invariant, i.e., $H(XQ)\equiv H(X)$ for any $Q\in\bbO^{k\times k}$. Before \cite{luli:2022},
SCF-type \eqref{eq:SCF1} had appeared in \cite{wazl:2022,zhys:2020,zhwb:2022}. Often indiscriminately, we use SCF to
refer to both the classical SCF and SCF-type iteration when no confusion arises.

An immediate consequence of \eqref{eq:tr-incr} is that $X_*^{\T}D\succeq 0$ for  any maximizer $X_*$
of maximization problem~\eqref{eq:OptSTM}. Another important characterization of maximizer $X_*$
is that \cite[Theorem 3.1]{luli:2022}
\begin{equation}\label{cond:rank}
\rank(X_*^{\T}D)=\rank(D).
\end{equation}
As a result, for any $X\in\bbO^{n\times k}$ such that the column space of $X$, denoted by  $\cR(X)$, is sufficiently close to $\cR(X_*)$, we have
$\rank(X^{\T}D)=\rank(D)$ \cite[Lemma 5.1]{luli:2022} which implies the continuity
of the canonical orthogonal polar factor
of $X^{\T}D$ for $\cR(X)$ near $\cR(X_*)$ \cite{high:2008,li:1993b}.

One of the key issues for SCF-type iteration \eqref{eq:SCF1}, as an iterative scheme, is whether
the generated sequence of approximations converge to the intended target. In the case when
optimization problem~\eqref{eq:OptSTM} is involved, that target is one of its maximizers. Because of technical
limitation, existing results on convergence are really about convergence-in-subspace, i.e., the convergence of
$\cR(X_i)$ to some $k$-dimensional subspace $\cX_*:=\cR(X_*)$ with exact $X_*$ unknown of course.
In other words, existing results may guarantee that $\cR(X_i)$ converges to
$\cX_*$ and produce estimates on the distance between subspaces $\cR(X_i)$ and
$\cX_*$ at convergence, but do not yield bounds on $\|X_i-X_*\|$ where $\|\cdot\|$ is some matrix norm.
In the case of OLDA or any objective function $f$ that is
unitarily invariant, this is the best  we can do because if $X_*$ is an optimizer then so is $X_*Q$ for any
$Q\in\bbO^{k\times k}$, but for $f$ as in \eqref{eq:OptSTM}, the optimizer $X_*$
is provably unique, provided $\rank(X_*^{\T}D)=k$,  within the orbit
\begin{equation}\label{eq:orbit4X*}
\bbX_*:=\{X_*Q\,:\, Q\in\bbO^{k\times k}\}
\end{equation}
whose elements share the same subspace $\cX_*=\cR(X_*)$, but $X_*$ is only
partially unique when $\rank(X_*^{\T}D)<k$ \cite{wazl:2022,luli:2022}. Notice that
$\rank(X^{\T}D)$ is a constant for all $X\in\bbX_*$, independent of any particular orthonormal basis matrix for $\cX_*$.
In view of this discussion, we may regard $D$ as some kind of decider that picks up particular $X_*$ from the orbit
\eqref{eq:orbit4X*}.

The goal of this paper is to answer the following mathematical question:
\begin{equation}\label{eq:TheQuestion}
\framebox{
\parbox{12.7cm}{Given two $k$-dimensional subspaces $\cX$ and $\wtd\cX$ of $\bbR^n$,
let $X,\,\wtd X\in\bbO^{n\times k}$ be their orthonormal basis matrices, respectively,
such that $X^{\T}D,\,\wtd X^{\T}D\succeq 0$, and assume $\rank(X^{\T}D)=\rank(\wtd X^{\T}D)$.
How do we bound the difference between $X$ and $\wtd X$ in terms of
the difference between the subspaces $\cX$ and $\wtd\cX$?}
}
\end{equation}
As to the issue raised moments ago for the convergence of $X_i$ in SCF, our main result can be
used to
bound $\|X_i-X_*\|$ in terms of the distance between the subspaces $\cR(X_i)$ and
$\cX_*$, by letting $X=X_*$ and $\wtd X=X_i$. The first condition $X_i^{\T}D\succeq 0$ holds by design and
$X_*^{\T}D\succeq 0$ is a necessary condition for a maximizer, and
$\rank(X_i^{\T}D)=\rank(X_*^{\T}D)$ near convergence is due to \eqref{cond:rank} of
\cite[Lemma 5.1]{luli:2022}
%To that end, we abstract out a
%A prerequisite condition that makes it possible is the rank-preserving condition
%\eqref{cond:rank}, which in particular ensures that $\rank(X^{\T}D)$ is the same if $\cR(X)$ is sufficiently close
%to $\cR_*$.


{\bf Notation.}
$\bbR^{m\times n}$ is the set of $m\times n$ real matrices, $\bbR^n=\bbR^{n\times 1}$ and $\bbR=\bbR^1$. $I_n\in\bbR^{n\times n}$ is
the identity matrix.
% and $\one_n\in\bbR^n$ is the vector of all ones.
%$\|\bx\|_2$ is the 2-norm of vector $\bx\in \bbR^n$.
For $B\in\bbR^{m\times n}$,
$\cR(B)$ is the column subspace, spanned by its columns, and its singular values are denoted by
$\sigma_i(B)$ for $i=1,\ldots,\min(m,n)$ arranged in the nonincreasing order,
%The thin SVD of $B$ is the one $B=U\Sigma V^{\T}$ such that $\Sigma\succ 0$
and
$$
\|B\|_2=\sigma_1(B),\,\,
\|B\|_{\F}=\sqrt{\sum_{i=1}^{{\rm rank}(B)}[\sigma_i(B)]^2},\,\,
\|B\|_{\tr}=\sum_{i=1}^{{\rm rank}(B)}\sigma_i(B)
$$
are the spectral, Frobenius, and trace norms of $B$, respectively.
$B^{\T}$ is the transpose of $B$. The trace norm is also known as the nuclear norm.
For a symmetric $A\in\bbR^{n\times n}$,
%\st{$\eig(A)=\{\lambda_i(A)\}_{i=1}^n$ denotes the set of its eigenvalues (counted by multiplicities)
%arranged in the nonincreasing order;}\marginpar{\textcolor{red}{seem to be not used later}}
$A\succ 0 ~(\succeq 0)$ means that $A$ is positive definite (semi-definite).
MATLAB-like notation is used to access the entries of a matrix or vector:
$X_{(i:j,k:l)}$ denotes the submatrix of a matrix $X$, consisting of the intersections of
rows $i$ to $j$ and columns $k$ to $l$, and when $i : j$ is replaced by $:$, it means all rows.
%, similarly for columns;
%$v_{(k)}$ refers the $k$th entry of a vector $v$ and $v_{(i:j)}$
%is the subvector of $v$ consisting of the $i$th to $j$th entries inclusive.

%\marginpar{\tiny Stiefel Manifold}

\iffalse
For example, the orthogonal linear discriminant analysis (OLDA) determines $X$ by
$$
\max_{X^{\T}X=I_k}\frac {\tr(X^{\T}S_bX)}{\tr(X^{\T}S_wX)},
$$
a maximization problem on  the Stiefel manifold
$$
\bbO^{n\times k}=\left\{X\in\bbR^{n\times k}\ |\  X^{\T}X=I_k\right\},
$$
where $S_b$ and $S_w$ are two scattering matrices, symmetric and at least positive semidefinite;
The orthogonal canonical correlation analysis (OCCA) determines two projection matrices $X_i\in\bbO^{n_i\times k}$ $(i=1,2)$ by
$$
\max_{X^{\T}X=I_k}\frac {X_1^{\T}C_{12}X_2}{\sqrt{\tr(X_1^{\T}A_1X_1)\tr(X_2^{\T}A_2X_2)}},
$$
a maximization problem on the product of the Stiefel manifolds $\bbO^{n_i\times k}$ for $i=1,2$.

The idea is to project
high dimesnion
\fi

\section{Preliminaries}
In this section, we collect a few known results that we will need in our later developments.

\subsection{Canonical angles between subspaces}
Given two $k$-dimensional subspaces $\cX$ and $\wtd\cX$ of $\bbR^{n}$,
let $X\in\bbO^{n\times k}$ and $\wtd X\in\bbO^{n\times k}$ be their orthonormal basis matrices,
respectively, i.e.,
\[
X^{\T}X=I_k,\ \cR(X)=\cX\quad\mbox{and}\quad \wtd X^{\T}\wtd X=I_k,\ \cR(\wtd X)=\wtd \cX.
\]
%where $\cR(X)$ and $\cR(\wtd X)$ are the column spaces of $\cX$ and $\wtd\cX$, respectively.
Denote by $\omega_i$ for $1\le i\le k$ the singular values of $X^{\T}\wtd X$ in descending order, i.e.,
$\omega_1\ge\dots\ge\omega_k$. The $k$ canonical angles $\theta_i(\cX,\wtd\cX)$ between $\cX$ and $\wtd\cX$
are defined as
\[
0\le\theta_i(\cX,\wtd\cX):=\arccos(\omega_{k-i+1})\le\frac{\pi}{2},\quad\mbox{for\ $1\le i\le k$} .
\]
%where $\theta_1(\cX,\wtd\cX)\ge\dots\ge\theta_k(\cX,\wtd\cX)$.
Set
\begin{equation}\label{eq:angle-def}
\Theta(\cX,\wtd\cX)=\diag\left(\theta_1(\cX,\wtd\cX),\dots,\theta_k(\cX,\wtd\cX)\right)\in\bbR^{k\times k}.
\end{equation}
It can be seen that the angle matrix $\Theta(\cX,\wtd\cX)$ in~\eqref{eq:angle-def} is independent of
choosing orthonormal basis matrices of $\cX$ and $\wtd\cX$.

In this paper, any unitarily invariant norm $\|\cdot\|_{\UI}$ \cite{stsu:1990} we refer to is assumed to be dimension-free in the sense
that it can be applied to matrices of any size consistently such as the matrix spectral and Frobenius norm.
Less stringently, we may limit our unitarily invariant norms that are induced by
a symmetric gauge function $\Phi$ on $\bbR^N$ \cite[section~II.4]{stsu:1990} with sufficiently large $N$ such that
all matrices $B$ of interest have no more than $N$ rows and columns, and then
we let \cite[p.79]{stsu:1990}
$$
\|B\|_{\UI}=\Phi(\sigma_1(B),\ldots,\sigma_r(B), 0,\ldots,0),
$$
i.e., appending $0$ to the set of singular values of $B$ to make $N$ of them.
It is known that for matrices $A$, $B$ and $C$ of compatible size
we have
\begin{equation}\label{eq:UI-2:consistent}
\|ABC\|_{\UI}\le\|A\|_2\|B\|_{\UI}\|C\|_2.
\end{equation}

Sun~\cite[p.95]{sun:1987} proved that for
any unitarily invariant norm $\|\cdot\|_{\UI}$,
$\|\sin\Theta(\cX,{\cal Y})\|_{\UI}$
defines a unitarily invariant metric on the Grassmann manifold
consisting of all $k$-dimensional subspaces
of $\bbR^n$. A convenient way to work with $\|\sin\Theta(\cX,{\cal Y})\|_{\UI}$ is as follows.
Let $X_{\perp},\,\wtd X_{\perp}\in\bbO^{n\times (n-k)}$
such that $[X, X_{\perp}]\in\bbO^{n\times n}$ and $[\wtd X, \wtd X_{\perp}]\in\bbO^{n\times n}$, respectively. Then
\begin{equation}\label{eq:def-comp-sin}
\|\sin\Theta(\cX,\wtd\cX)\|_{\UI}=\|X_{\perp}^{\T}\wtd X\|_{\UI}
=\|\wtd X_{\perp}^{\T}X\|_{\UI}.
\end{equation}



%\marginpar{\tiny check \Red{\cite[p.96]{stsu:1990}} again.}
\begin{lemma}[{\cite[Lemma~4.1]{zhli:2014b}}]\label{le:max-angle}
There exists an orthogonal matrix $Q\in\bbO^{k\times k}$ such that
\begin{equation}\label{eq:angle1}
    \|\sin\Theta(\cX,\wtd\cX)\|_{\UI}\le\|X-\wtd XQ\|_{\UI}\le\sqrt 2\,\|\sin\Theta(\cX,\wtd\cX)\|_{\UI}.
\end{equation}
%In addition, we have
%\begin{equation}\label{eq:angle2}
%    \|X-\wtd XQ\|_{\F}=\min_{W\in\bbO^{k\times k}}\|X-\wtd XW\|_{\F}.
%\end{equation}
\end{lemma}

\subsection{SVD Perturbation}
For any matrix $B$ of apt size, we will use $B_{\bestr}$ to denote its {\em best rank-$r$ approximation\/}
obtained by zeroing out all of its singular values except the first $r$ largest ones in its SVD. It can be shown,
using Fan's theorem \cite[p.86]{stsu:1990}, that
for any unitarily invariant norm $\|\,\cdot\,\|_{\UI}$, $\|(\,\cdot\,)_{\bestr}\|_{\UI}$ for $r\ge 1$ is also
a unitarily invariant norm. The consistency inequalities in \eqref{eq:UI-2:consistent} can be sharpened a little:
\begin{equation}\label{eq:UI-2:consistent'}
\|ABC\|_{\UI}\le\| A\|_2\|B_{\bestr}\|_{\UI}\|C\|_2\quad\mbox{if $\min\{\rank(A),\rank(B),\rank(C)\}\le r$}.
%  \begin{cases}
% \| A\|_2\|B_{\bestr}\|_{\UI}, \quad &\mbox{if $\rank(A)\le r$}, \\
% \|A_{\bestr}\|_{\UI}\|B\|_2, \quad &\mbox{if $\rank(B)\le r$}.
%  \end{cases}
\end{equation}
The next lemma is a corollary of the classical Wedin's result of
\cite[(3.1)]{wedi:1972} and \eqref{eq:UI-2:consistent'}.
%but slightly improved for the case of the lemma.

\begin{lemma}\label{lm:SVD-pert:wedin}
Let $B,\,\wtd B=B+F\in\bbR^{m\times n}$ such that $\rank(B)=\rank(\wtd B)=r$ and let
their singular value decompositions be
%\marginpar{\tiny state the theorem for $\rank(B)=r$ first and then ffor $\rank(B)=\rank(\wtd B)$?}
\begin{equation}\label{eq:Bsvd}
    B=U\Sigma V^{\T}\quad\mbox{and}\quad
\wtd B=\wtd U\wtd\Sigma \wtd V^{\T},
\end{equation}
where $\Sigma_{(1:r,1:r)}\succ 0$ and $\wtd\Sigma_{(1:r,1:r)}\succ 0$.
Then we have
%\begin{multline}\label{eq:sin}
%\max\left\{\|\sin\Theta(\cU,\wtd\cU)\|_{\UI},\,\|\sin\Theta(\cV,\wtd\cV)\|_{\UI}\right\} \\
%  \le\frac{\|F_{\bestr}\|_{\UI}}{\max\{\sigma_r(B),\sigma_r(\wtd B)\}}
%                \le\frac{\|F\|_{\UI}}{\max\{\sigma_r(B),\sigma_r(\wtd B)\}}.
%\end{multline}
\begin{subequations}\label{eq:sin}
\begin{align}
\max\left\{\|\sin\Theta(\cU,\wtd\cU)\|_{\UI},\,\|\sin\Theta(\cV,\wtd\cV)\|_{\UI}\right\}
                &\le\frac{\|F_{\bestr}\|_{\UI}}{\max\{\sigma_r(B),\sigma_r(\wtd B)\}} \label{eq:sin-a} \\
                &\le\frac{\|F\|_{\UI}}{\max\{\sigma_r(B),\sigma_r(\wtd B)\}}. \label{eq:sin-b}
%\label{eq:sin-b}\|\sin\Theta(\cV,\wtd\cV)\|_{\UI}
%                \le\frac{\|F_{\bestr}\|_{\UI}}{\max\{\sigma_r(B),\sigma_r(\wtd B)\}}
%                \le\frac{\|F\|_{\UI}}{\max\{\sigma_r(B),\sigma_r(\wtd B)\}},
\end{align}
\end{subequations}
where $\cU=\cR(U_{(:,1:r)})$,
$\wtd\cU=\cR(\wtd U_{(:,1:r)})$,
$\cV=\cR(V_{(:,1:r)})$, and
$\wtd\cV=\cR(\wtd V_{(:,1:r)})$.
\end{lemma}

\begin{proof}
Let
\begin{align*}
R&:=B\wtd V_{(:,1:r)}-\wtd U_{(:,1:r)}\wtd\Sigma_{(1:r,1:r)}=(B-\wtd B)\wtd V_{(:,1:r)},\\
S&:=B^{\T}\wtd U_{(:,1:r)}-\wtd V_{(:,1:r)}\wtd\Sigma_{(1:r,1:r)}=(B-\wtd B)^{\T}\wtd U_{(:,1:r)}.
\end{align*}
By \eqref{eq:UI-2:consistent'}, we get
$\|R\|_{\UI}\le\|F_{\bestr}\|_{\UI}$ and $\|S\|_{\UI}\le\|F_{\bestr}\|_{\UI}$.
Hence, with the help of the classical Wedin's result of \cite[(3.1)]{wedi:1972} (see also \cite[Fact 4, p.21-7]{li:2014HLA})
for the case, we have
%\begin{equation}\label{eq:wedin}
%\max\left\{\|\sin\Theta(\cU,\wtd\cU)\|_{\UI},\;
%\|\sin\Theta(\cV,\wtd\cV)\|_{\UI}\right\}
%\le\frac{\max\{\|R\|_{\UI},\|S\|_{\UI}\}}{\sigma_r(\wtd B)}.
%\end{equation}
\begin{align}
\max\left\{\|\sin\Theta(\cU,\wtd\cU)\|_{\UI},\,\|\sin\Theta(\cV,\wtd\cV)\|_{\UI}\right\}
  &\le\frac{\max\{\|R\|_{\UI},\|S\|_{\UI}\}}{\sigma_r(\wtd B)} \nonumber \\
  &\le\frac{\|F_{\bestr}\|_{\UI}}{\sigma_r(\wtd B)}. \label{eq:wedin}
\end{align}
Switching the roles of $B$ and $\wtd B$ in \eqref{eq:wedin}, we get
\begin{equation}\label{eq:wedin'}
\max\left\{\|\sin\Theta(\cU,\wtd\cU)\|_{\UI},\,\|\sin\Theta(\cV,\wtd\cV)\|_{\UI}\right\}
\le\frac{\|F_{\bestr}\|_{\UI}}{\sigma_r(B)}.
\end{equation}
Inequalities in \eqref{eq:sin} are the consequences of \eqref{eq:wedin} and \eqref{eq:wedin'}.
%
%Let $B^{\dagger}$ and $\wtd B^{\dagger}$ be the Moore-Penrose inverses of $B$ and $\wtd B$, respectively. Then
%\[
%\Pi_{\cU}=BB^{\dagger}\quad\mbox{and}\quad \Pi_{\wtd\cU}=\wtd B\wtd B^{\dagger}
%\]
%are the orthogonal projections onto the subspaces $\cU$ and $\wtd\cU$, respectively.
%Let $\Pi_{\cU^{\perp}}=I-\Pi_{\cU}$, the orthogonal projection onto the orthogonal complement of $\cU$. Then
%we have $\Pi_{\cU^{\perp}}B=0$ and
%\begin{align}\label{eq:pf-sin-a1}
%\notag    \|\sin\Theta(\cU,\wtd\cU)\|_{\UI}
%\notag    &=\|\Pi_{\cU^{\perp}}\Pi_{\wtd\cU}\|_{\UI}=\|\Pi_{\cU^{\perp}}\wtd B\wtd B^{\dagger}\|_{\UI}\\
%\notag    &=\|\Pi_{\cU^{\perp}}(B+F)\wtd B^{\dagger}\|_{\UI}\\
%\notag          &=\|\Pi_{\cU^{\perp}}F\wtd B^{\dagger}\|_{\UI}\\
%          &\le\|\Pi_{\cU^{\perp}}\|_2\|F_{\bestr}\|_{\UI}\|\wtd B^{\dagger}\|_2
%          =\frac{\|F_{\bestr}\|_{\UI}}{\sigma_r(\wtd B)},
%\end{align}
%where we have used \eqref{eq:UI-2:consistent'}.
%%Notice that $\rank(\wtd B^{\dagger})=r$ which implies $\rank(\Pi_{\cU^{\perp}}F\wtd B^{\dagger})\le r$.
%%Let $\Phi(\,\cdot\,)$ be the symmetric gauge function detailed in~\cite[II.3]{stsu:1990}
%%associated with the unitarily invariant norm $\|\cdot\|_{\UI}$, and let
%%\[\gamma_1\ge\gamma_2\ge\dots\ge\gamma_{\min\{m,n\}}\ge 0 \quad\mbox{and}\quad
%%\tau_1\ge\tau_2\ge\dots\ge\tau_{\min\{m,n\}}\ge 0
%%\]
%%be the singular values of the matrices $\Pi_{\cU^{\perp}}F\wtd B^{\dagger}$ and $F$, respectively.
%%Therefore, we have
%%\begin{align} \label{eq:pf-sin-a2}
%%\notag \|\Pi_{\cU^{\perp}}F\wtd B^{\dagger}\|_{\UI}&=\Phi(\gamma_1,\gamma_2,\dots,\gamma_r)\\
%%\notag &\le\Phi\left(\|\wtd B^{\dagger}\|_2\,\tau_1, \|\wtd B^{\dagger}\|_2\,\tau_2, \dots, \|\wtd B^{\dagger}\|_2\,\tau_r\right)\\
%%\notag &\le\|\wtd B^{\dagger}\|_2\,\Phi\left(\tau_1,\tau_2,\dots,\tau_r\right)\\
%%&=\frac{\|F_{\bestr}\|_{\UI}}{\sigma_r(\wtd B)}.
%%\end{align}
%Similarly, let $\Pi_{\wtd\cU^{\perp}}=I-\Pi_{\wtd\cU}$, the orthogonal projection onto
%the orthogonal complement of $\wtd\cU$. Then we have $\Pi_{\wtd\cU^{\perp}}\wtd B=0$ and
%\begin{align}
%\notag  \|\sin\Theta(\cU,\wtd\cU)\|_{\UI}
%\notag  &=\|\Pi_{\wtd\cU^{\perp}}\Pi_{\cU}\|_{\UI}
%\notag  =\|\Pi_{\wtd\cU^{\perp}}B B^{\dagger}\|_{\UI}\\
%\notag  &=\|\Pi_{\wtd\cU^{\perp}}(\wtd B-F)B^{\dagger}\|_{\UI}\\
%\notag  &=\|\Pi_{\wtd\cU^{\perp}}(-F)B^{\dagger}\|_{\UI}\\
%        &\le\frac{\|F_{\bestr}\|_{\UI}}{\sigma_r(B)}. \label{eq:pf-sin-b}
%\end{align}
%Inequality \eqref{eq:sin-a} is a consequence of~\eqref{eq:pf-sin-a1} and~\eqref{eq:pf-sin-b}.
%Similarly, we can prove~\eqref{eq:sin-b}.
\end{proof}

%The results in Lemma~\ref{lm:SVD-pert:wedin} can be compactly written as
%\begin{equation}\label{eq:sin-max-r}
%\max\left\{\|\sin\Theta(\cU,\wtd\cU)\|_{\UI},\;
%\|\sin\Theta(\cV,\wtd\cV)\|_{\UI}\right\}
%\le\frac{\|F_{\bestr}\|_{\UI}}{\max\{\sigma_r(B),\sigma_r(\wtd B)\}}.
%\end{equation}
%Note that $\|F_{\bestr}\|_{\UI}\le \|F\|_{\UI}$. It follows from \eqref{eq:sin-max-r} that
%\begin{equation}\label{eq:sin-max-r'}
%\max\left\{\|\sin\Theta(\cU,\wtd\cU)\|_{\UI},\;
%\|\sin\Theta(\cV,\wtd\cV)\|_{\UI}\right\}
%\le\frac{\|F\|_{\UI}}{\max\{\sigma_r(B),\sigma_r(\wtd B)\}},
%\end{equation}
%which is a corollary of Wedin~\cite[(3.1)]{wedi:1972}.

\iffalse
\begin{remark}\label{rk:SVD-pert:wedin}
In Lemma~\ref{lm:SVD-pert:wedin}, it is assumed both $\rank(B)=\rank(\wtd B)=r$.
What will happen if we relax the condition to just $\rank(B)=r$ but possibly
$\rank(\wtd B)>r$? Recall the derivations in \eqref{eq:pf-sin-b}. Now $\Pi_{\wtd\cU^{\perp}}\wtd B=0$ no longer, but still
\begin{align}
\|\sin\Theta(\cU,\wtd\cU)\|_{\UI}
  &=\|\Pi_{\wtd\cU^{\perp}}(\wtd B-F)B^{\dagger}\|_{\UI} \nonumber\\
  &\le\|(\Pi_{\wtd\cU^{\perp}}\wtd B)_{\bestr}\|_{\UI}\|B^{\dagger}\|_2+\|(\Pi_{\wtd\cU^{\perp}}F)_{\bestr}\|_{\UI}\|B^{\dagger}\|_2.
       \label{eq:SVD-pert:wedin:rk-1}
%   \\
%  &\le\|(\Pi_{\wtd\cU^{\perp}}\wtd B)_{\bestr}\|_{\UI}+
\end{align}
Notice that $\wtd B=B+F$ and denote the singular values of $B$ and $\wtd B$ by
$$
\sigma_1\ge\cdots\ge\sigma_r> 0=\cdots=0, \quad
\wtd\sigma_1\ge\cdots\ge\wtd\sigma_{\min\{m,n\}}\ge 0,
$$
respectively. Noticing that $\|(\,\cdot\,)_{\bestr}\|_{\UI}$ is a unitarily invariant norm and that
the singular values of $\Pi_{\wtd\cU^{\perp}}\wtd B$ are $\wtd\sigma_i$ for $i=r+1,\cdots,\min\{m,n\}$,
we have, by Mirsky's theorem \cite[p.204]{stsu:1990},
\begin{align}
\|(\Pi_{\wtd\cU^{\perp}}\wtd B)_{\bestr}\|_{\UI}
   &=\big\|\big[\diag(\wtd\sigma_{r+1},\ldots,\wtd\sigma_{\min\{m,n\}})\big]_{\bestr}\big\|_{\UI} \nonumber\\
   &\le\big\|\big[\diag(\wtd\sigma_1-\sigma_1,\ldots,\wtd\sigma_r-\sigma_r,\wtd\sigma_{r+1},\ldots,\wtd\sigma_{\min\{m,n\}})\big]_{\bestr}\big\|_{\UI}
       \nonumber\\
   &\le\|F_{\bestr}\|_{\UI}. \label{eq:SVD-pert:wedin:rk-2}
\end{align}
Combine \eqref{eq:SVD-pert:wedin:rk-2} with \eqref{eq:SVD-pert:wedin:rk-1} to get
\begin{equation}\label{eq:SVD-pert:wedin:rk-3}
\|\sin\Theta(\cU,\wtd\cU)\|_{\UI}\le\frac{2\,\|F_{\bestr}\|_{\UI}}{\sigma_r(B)}.
\end{equation}
Similarly, we can bound $\|\sin\Theta(\cV,\wtd\cV)\|_{\UI}$ by the same right-hand side of \eqref{eq:SVD-pert:wedin:rk-3},
and finally
\begin{equation}\label{eq:SVD-pert:wedin:rk-4}
\max\{\|\sin\Theta(\cU,\wtd\cU)\|_{\UI},\|\sin\Theta(\cV,\wtd\cV)\|_{\UI}\}\le\frac{2\,\|F_{\bestr}\|_{\UI}}{\sigma_r(B)}.
\end{equation}
This inequality extends \cite[Theorem~5]{luhz:2021} which is for the matrix Schatten norm only.
%%in which the authors give a singular subspace perturbation bound as
%\[
%\max\left\{\|\sin\Theta(\cU,\wtd\cU)\|_{\Sq},\;
%\|\sin\Theta(\cV,\wtd\cV)\|_{\Sq}\right\}
%                    \le\frac{2\,\|F_{\bestr}\|_{\Sq}}{\sigma_r(B)}
%\]
%under \Red{the assumption of $B$ being rank-$r$, while $\wtd B$ is not required to be rank-$r$},
%where $\|\cdot\|_{\Sq}$ is the matrix Schatten-$q$ norm which is a specific type of unitarily invariant norm.
%In~\eqref{eq:sin-max-r}, we use a general unitarily invariant norms, and obtain a sharper perturbation bound
%because both $B$ and $\wtd B$ have rank $r$.
Comparing with \eqref{eq:sin-max-r}, \eqref{eq:SVD-pert:wedin:rk-4} is likely weaker (upon reasonably assuming
$\sigma_r(B)\approx\sigma_r(\wtd B)$), but the latter holds without needing $\sigma_{r+1}(\wtd B)=0$.
%has an advantage of not involving $\sigma_r(\wtd B)$.
\end{remark}
\fi


\subsection{Polar decomposition}
%Consider matrices in  $\bbR^{n\times m}$ where $n\ge m$.
Any $B\in\bbR^{n\times m}$ $(n\ge m)$ can be decomposed as $B=QH$, called a polar decomposition \cite[p.449]{hojo:2013},
where $Q\in\bbO^{n\times m}$ and $H=(B^{\T}B)^{1/2}\succeq 0$ is the unique positive semidefinite square root of $B^{\T}B$.
It is known that orthogonal factor $Q$  is unique if and only if $\rank(B)=m$ \cite{li:2014HLA}. When $\rank(B)<m$,
there is  the so-called {\em canonical polar decomposition\/} $B=QH$ in which $Q\in\bbR^{n\times m}$
is a partial isometry and satisfies $\cR(Q^{\T})=\cR(H)$ and again $H=(B^{\T}B)^{1/2}$.
In the canonical polar decomposition, $Q$ is unique
(see \cite[p.220]{begr:2003}, \cite[chapter~8]{high:2008}, \cite{li:1993b}).

%Let the SVD of $B\in\bbR^{n\times m}$ be \cite{govl:2013}
%\begin{equation}\label{eq:svd-B}
%B=\kbordermatrix{ &\sss r &\sss n-r\\
%                                 & U_1 & U_2}\times
%                      \kbordermatrix{ &\sss r &\sss m-r \\
%                                    \sss r & \Sigma_1 & \\
%                                    \sss n-r & & 0}\times
%                      \kbordermatrix{ &\\
%                               \sss r  & V_1^{\T} \\
%                               \sss m-r  & V_2^{\T}},
%\end{equation}
%where $r=\rank(B)$. For the canonical polar decomposition of $B=QH$, $Q=U_1V_1^{\T}$, we have

\begin{lemma}[{\cite[Theorem~1]{li:1995}}, {\cite[Theorem~3.4]{lisu:2003}}] \label{le:polar}
Suppose that $B\in\bbR^{n\times m}$  ($n\ge m$) is perturbed to $\wtd B$
such that $\rank(\wtd B)=\rank(B)=r$. Let the SVD of $B$ be given by
\begin{equation}\label{eq:svd-B}
B=\kbordermatrix{ &\sss r &\sss n-r\\
                                 & U_1 & U_2}\times
                      \kbordermatrix{ &\sss r &\sss m-r \\
                                    \sss r & \Sigma_1 & \\
                                    \sss n-r & & 0}\times
                      \kbordermatrix{ &\\
                               \sss r  & V_1^{\T} \\
                               \sss m-r  & V_2^{\T}},
\end{equation}
where $r=\rank(B)$, and similarly
the SVD of $\wtd B$ takes the form as in \eqref{eq:svd-B} except with
a tilde on each of the symbols there. Then
\begin{equation}\label{eq:QtQ}
 Q= U_1 V_1^{\T}
\quad\mbox{and}\quad
\wtd Q=\wtd U_1\wtd V_1^{\T}
\end{equation}
are
the unique partial isometry factors of the canonical polar decompositions
of $B$ and $\wtd B$, respectively, and
\begin{equation}\label{eq:polar-iu}
\|Q-\wtd Q\|_{\UI}\le
\begin{cases}
    \dfrac{2}{\sigma_n(B)+\sigma_n(\wtd B)}\,\|\wtd B-B\|_{\UI}, &\mbox{if $r=n=m$}; \\[1em]
    \left(\dfrac{2}{\sigma_r(B)+\sigma_r(\wtd B)}+\dfrac{2}{\max\{\sigma_r(B),\sigma_r(\wtd B)\}}\right)\,
               \|\wtd B-B\|_{\UI},
    &\mbox{otherwise}.
\end{cases}
\end{equation}
This inequality can be improved for the matrix spectral and Frobenius norm
in the case when $n>m$ or $r<n$:
%  $\|\cdot\|_{\UI}=\|\cdot\|_{\F}$ and $\|\cdot\|_{\UI}=\|\cdot\|_2$:
\begin{subequations}\label{eq:polar-fnorm-2norm}
\begin{align}
\|Q-\wtd Q\|_{\F}
           &\le\dfrac{2}{\sigma_r(B)+\sigma_r(\wtd B)}\,\|\wtd B-B\|_{\F}, \label{eq:polar-fnorm} \\
\|Q-\wtd Q\|_2
          &\le \sqrt{\dfrac{4}{\left[\sigma_r(B)+\sigma_r(\wtd B)\right]^2}
                     +\dfrac{2}{\left[\max\{\sigma_r(B),\sigma_r(\wtd B)\}\right]^2}}\,\|\wtd B-B\|_2. \label{eq:polar-2norm}
\end{align}
\end{subequations}
\end{lemma}

The next lemma characterizes $X\in\bbO^{n\times k}$ such that $X^{\T}D\succeq 0$ into a sum of two terms, one of which depends on $\cR(X)$ only.

\begin{lemma}[{\cite[Theorem~3.2]{wazl:2022}}] \label{le:mp}
Given a $k$-dimensional subspace $\cX$ of $\bbR^n$, let $X_{\diamond}\in\bbO^{n\times k}$ with $\cR(X_{\diamond})=\cX$,
and let $r=\rank(X_{\diamond}^{\T}D)$ where $D\in\bbR^{n\times k}$. Let the SVD of $X_{\diamond}^{\T}D$ be
\begin{equation}\label{eq:svd-xdd}
X_{\diamond}^{\T}D=\kbordermatrix{ &\sss r &\sss k-r\\
                                 & U_1 & U_2}\times
                      \kbordermatrix{ &\sss r &\sss k-r \\
                                    \sss r & \Sigma_1 & \\
                                    \sss k-r & & 0}\times
                      \kbordermatrix{ &\\
                               \sss r  & V_1^{\T} \\
                               \sss k-r  & V_2^{\T}}.
\end{equation}
Any $X\in\bbO^{n\times k}$ with $\cR(X)=\cX$ such that $X^{\T}D\succeq 0$ takes the form
\[
X=X_{\diamond}U_1V_1^{\T}+X_{\diamond}U_2WV_2^{\T},
\]
where the first term, although constructed from $X_{\diamond}$, depends only on $\cX$, while
the second term has a freedom in $W\in\bbO^{(k-r)\times (k-r)}$. In particular, if $r=k$, then
$X=X_{\diamond}UV^{\T}$ is unique, given $\cX$.
\end{lemma}

\iffalse
Therefore, in most cases,
the difference between $X$ and $\wtd X$ does not move to 0 as the distance
between $\cX$ and $\wtd\cX$ going to 0. Even if $\cX=\wtd\cX$, we only get the result $\wtd X=XQ$ with $Q\in\bbO^{k\times k}$.
The purpose of this note is to give bounds for the difference between the orthonormal matrices $X$
and $\wtd X$ in terms of the canonical angles between the subspaces $\cX$ and $\wtd\cX$ in the assumption
\begin{equation}\label{eq:assp}
\framebox{\parbox{10cm}{\centering $X^{\T}D\succeq 0$, $\wtd X^{\T}D\succeq 0$ and $\rank(X^{\T}D)=\rank(\wtd X^{\T}D)$}
}
\end{equation}
where $D\in\bbR^{n\times k}$, $X^{\T}D\succeq 0$ and $\wtd X^{\T}D\succeq 0$ denote the matrices $X^{\T}D$ and $\wtd X^{\T}D$
being symmetric positive semidefinite.  As we state above, the assumption~\eqref{eq:assp} is a necessary condition
in our further estimations.  It play an important role on pinning the basis matrix for a given subspace.
If $X^{\T}D\succeq 0$ and $\wtd X^{\T}D\succeq 0$, by~\cite[Section~3]{wazl:22}, we have
\begin{subequations}\label{eq:prbx}
\begin{align}
\label{eq:prbx1} X&=\arg\max_{X\in\bbO^{n\times k},\,\cR(P)=\cX}\tr(P^{\T}D),\\
\label{eq:prbx2} \wtd X&=\arg\max_{X\in\bbO^{n\times k},\,\cR(P)=\wtd\cX}\tr(P^{\T}D).
\end{align}
\end{subequations}
In fact, \eqref{eq:prbx1} is also treated as a classical matrix optimization problem
\begin{equation}\label{eq:max-opt-x-2}
    \max_{P\in\bbO^{k\times k}}\tr\left(P^{\T}(X^{\T}D)\right)
\end{equation}
which is extensively discussed in recent work~\cite[Lemma~3.1]{luli:22} as well.
By~\cite[Lemma~3.1]{luli:22}, the global maximizer of~\eqref{eq:max-opt-x-2} is the polar factor $Q$ of $X^{\T}D$,
and $Q=I_k$ is the unique polar factor when $X^{\T}D\succ 0$ (positive definite) which implies~\eqref{eq:prbx1}.
It is a little more complexity to interpret~\eqref{eq:prbx1} for the general case $X^{\T}D\succeq 0$
since the polar factor of $X^{\T}D$ is not unique any more, but $Q=I_k$ is still one of them.

Our motivation for this paper is from recent work on the following optimization problem over
the Stiefel manifold~\cite{luli:22,wazl:22,zhwb:22} as
\begin{equation}\label{eq:f(X)-luli}
    \max_{Z\in\bbO^{n\times k}}f(Z)\quad\mbox{and}\quad f(Z):=\phi(Z)+\psi(Z)\times\tr(Z^{\T}D),
\end{equation}
where $\phi(Z)$ and $\psi(Z)$ are continuously differentiable functions in $Z\in\bbR^{n\times k}$ and unitarily invariant
\footnote{In fact, the term ``unitary'' is come from complex matrices.
As the term ``unitarily invariant norm'' appeared in this paper,
we use this name here for the convention to our further analysis.
In addition, our results in this paper also hold for the complex case.},
i.e., $\phi(Z)=\phi(ZP)$ and $\psi(Z)=\psi(ZP)$ for any $Z\in\bbO^{n\times k}$ and $P\in\bbO^{k\times k}$,
and $\psi(Z)$ is usually a positive function, i.e., $\psi(Z)>0$, for all $Z\in\bbO^{n\times k}$.
However, due to the presence of the term $\tr(Z^{\T}D)$, the function $f(Z)$ is not unitarily invariant anymore.
Such a problem usually arises in  multi-view subspace learning~\cite{cugh:15,sumd:19},
such as the multi-set canonical correlation analysis~\cite{visp:07},
unbalanced orthogonal Procrustes problems~\cite{elpa:99,godi:04,zhys:20},
and generalized multi-view analysis~\cite{shkd:12}, to product an orthogonal linear transformation,
i.e., a global maximizer $X$ of~\eqref{eq:f(X)-luli}, for the original high-dimensional data such that the low-dimensional
data representation based on $X$ preserves some features of interest in the high-dimensional data.
The KKT condition of such maximization problem~\eqref{eq:f(X)-luli} can be transformed as a nonlinear eigenvalue problem
with eigenvector dependency (NEPv) as
\begin{equation} \label{eq:nepv}
    E(Z)Z=Z\Lambda, \quad Z\in\bbO^{n\times k},
\end{equation}
where $\Lambda=Z^{\T}E(Z)Z$ and $E(Z)$ is a symmetric matrix-value function defined by
\begin{equation}\label{eq:hg}
    E(Z)=E_{\phi}(Z)+\tr(Z^{\T}D)E_{\psi}(Z)+\psi(X)(DX^{\T}+XD^{\T})
\end{equation}
with
\[\begin{cases}
    \dfrac{\partial\phi(Z)}{\partial Z}=E_{\phi}(Z)Z,\\[1em]
    \dfrac{\partial\psi(Z)}{\partial Z}=E_{\psi}(Z)Z.\\
\end{cases}
\]

The self-consistent field (SCF) iteration is usually used to solve NEPv~\eqref{eq:nepv}.
Given an initial guess $X_0\in\bbO^{n\times k}$, at the $i$th SCF iteration for $i\ge 1$, one computes
the following symmetric eigenvalue problem
\begin{equation} \label{eq:scf-eig}
    E(X_{i-1})\wht X_i=\wht X_i\Lambda_i,
\end{equation}
where $\wht X_i\in\bbO^{n\times k}$ is the eigenvector matrix corresponding to the $k$ largest eigenvalues of $E(X_{i-1})$.
Remember that, unlike the trace ratio optimization~\cite{zhli:15a, zhln:2010},
the optimization problem~\eqref{eq:f(X)-luli} does not have the unitary invariance property.
Nevertheless, the orthonormal base of the invariant subspace of~\eqref{eq:scf-eig} is not unique,
which leads that $\wht X_iP$ with any $P\in\bbO^{k\times k}$ is also a wanted eigenvector matrix.
Thus, to calculate the maximizer of problem~\eqref{eq:f(X)-luli},
a basis alignment operation based on the matrix $D$ is a necessary step to align an orthonormal basis matrix
for the invariant subspace associated with the $k$ largest eigenvalues of $E(X_{i-1})$.
As stated in~\cite{luli:22,wazl:22}, let $X\in\bbO^{n\times k}$ be a global maximizer of~\eqref{eq:f(X)-luli},
then $X$ satisfies the necessary conditions as follows:
\begin{enumerate}[(a)]
    \item $X$ is an orthonormal basis matrix of the invariant subspace corresponding to the $k$ largest eigenvalues of $E(X)$;
    \item $X^{\T}D\succeq 0$.
\end{enumerate}
Hence, setting $X_i=\wht X_i\wht Q$ with $\wht Q$ being a polar factor of $\wht X_i^{\T}D$
as the post step of~\eqref{eq:scf-eig} is a natural choice for making $X_i^{\T}D\succeq 0$,
and then $X_i$ is used as the next approximation to the maximizer of~\eqref{eq:f(X)-luli}.

Suppose $\lambda_k\left(E(X)\right)>\lambda_{k+1}\left(E(X)\right)$, by~\cite[Theorem~4.3]{wazl:22}, then the sequence
$\{\cX_i\}_{i=0}^{\infty}$ converges to $\cX$, where $\cX_i=\cR(X_i)$.
That means for every given tolerance $\varepsilon>0$, there exists a natural number $\ell$ such that
for the number of iterations $i>\ell$, we have
\begin{equation}\label{eq:sin-eps}
\|\sin\Theta(\cX,\cX_i)\|_{\UI}<\varepsilon.
\end{equation}
Suppose $X$ is a global maximizer of~\eqref{eq:f(X)-luli},  it follows by~\cite[Lemma~5.1]{luli:22} and~\eqref{eq:sin-eps} that
\[\rank(X_i^{\T}D)=\rank(X^{\T}D)\quad\mbox{for}\ i>\ell. \]
In such cases, one naturally wants to know how good $X_i$ approximates to $X$.
In this paper, we give several results to quantify how much such an approximation is.
These results are helpful in understanding how the approximated $X_i$ move to the maximizer $X$
when the subspace $\cX_i$ converges to $\cX$.

The rest of the paper is organized as follows.
In Section~\ref{sec:main}, we first give some essential lemmas for our later analysis,
and then prove our main results by considering $\rank(X^{\T}D)=\rank(\wtd X^{\T}D)=k$
and $\rank(X^{\T}D)=\rank(\wtd X^{\T}D)<k$, respectively.
Section~\ref{sec:num} contains some numerical examples presented to support our theoretical analysis.
Finally, our conclusion is given in Section~\ref{sec:col}.

\textbf{Notation.}
$I_n$ is the $n\times n$ identity matrix. The superscript ``$^{\T}$'' takes the transpose of a matrix or vector.
For a symmetric matrix $A\in\bbR^{n\times n}$, $A\succ 0$ $(A\succeq 0)$ means that $A$ is positive definite (semidefinite),
and $\lambda_1(A)\ge\dots\ge\lambda_n(A)$ denotes the eigenvalues of $A$ arranged in descending order.
For $Y\in\bbR^{m\times n}$, $Y_{ (:,i:j)}$ is the submatrix consisting the $i$-th to the $j$-th columns of $Y$,
$\sigma_1(Y)\ge\dots\ge\sigma_{\min\{m,n\}}(Y)\ge 0$ is the singular values of $Y$ in descending order,
$\cR(Y)$ and $\rank(Y)$ denote the column space and rank of $Y$, respectively,
and $\|Y\|_2$, $\|Y\|_{\F}$, $\|Y\|_{\tr}$ and $\|Y\|_{\UI}$ indicate the spectral norm, Frobenius norm, nuclear norm
and unitarily invariant norm, where
\begin{equation}\label{eq:nc-norm}
\|Y\|_{\tr}=\sum_{j=1}^{\min\{m,n\}}\sigma_j(Y).
\end{equation}
For matrices or scalars $Y_j$ for $1\le j\le m$, $\diag(Y_1,\dots,Y_m)$ is the block diagonal matrix
\[
\begin{bmatrix}
    Y_1 &  &\\
    &   \ddots &\\
    &    & Y_m\\
\end{bmatrix}.
\]
\fi

\section{Main result}\label{sec:main}
In this section, we will present our main result that answers the question in \eqref{eq:TheQuestion}.
Given $k$-dimensional subspace $\cX$ of $\bbR^n$, consider
%To that end, we start by explaining the results in \cite{wazl:2022} that characterize
all $X\in\bbO^{n\times k}$ such that
\begin{equation}\label{eq:charz-X}
\cR(X)=\cX, \quad X^{\T}D\succeq 0.
\end{equation}
The first condition $\cR(X)=\cX$ merely says that $X$
is an orthonormal basis matrix of $\cX$ and it is not unique, and in fact, it has the degree of freedom:
$k^2-\frac 12 k(k+1)=\frac 12 k(k-1)$. It is the  second characterization $X^{\T}D\succeq 0$ that will decide
which one or ones among the orthonormal basis matrices of $\cX$ should  be.
Let $r=\rank(X^{\T}D)$ and $r'=k-r$, and let the SVD of $X^{\T}D$ be
\begin{equation}\label{eq:SVD:XD}
X^{\T}D=V\Sigma V^{\T}\equiv\kbordermatrix{ &\sss r &\sss r'\\
                                 & V_1 & V_2}\times
                      \kbordermatrix{ &\sss r &\sss r' \\
                                    \sss r & \Sigma_1 & \\
                                    \sss r' & & 0}\times
                      \kbordermatrix{ &\\
                               \sss r  & V_1^{\T} \\
                               \sss r'  & V_2^{\T}}.
\end{equation}
By Lemma~\ref{le:mp}, we know that  $Y\in\bbO^{n\times k}$
such that $\cR(Y)=\cX$ and $Y^{\T}D\succeq 0$ if and only if
\begin{equation}\label{eq:xset}
Y\in\bbX:=\left\{XV\begin{bmatrix}
                     I_r &  \\
                      & W
                   \end{bmatrix}V=
XV_1V_1^{\T}+XV_2WV_2^{\T}\, :\,W\in\bbO^{r'\times r'}\right\}.
\end{equation}
When $\rank(X^{\T}D)=k$, the second term $XV_2WV_2^{\T}$ disappears and $\bbX=\{X\}$, which means that $X$ is unique.
But when $\rank(X^{\T}D)<k$, $\bbX$ is parameterized by a matrix variable $W\in\bbO^{r'\times r'}$ that
has the degree of freedom $\frac 12r'(r'-1)$. Since any element in $\bbX$ could be taken as $X$ to begin with, $X$ is not
uniquely decided by $X^{\T}D\succeq 0$. It worths emphasizing that $XV_1V_1^{\T}$ in \eqref{eq:xset}
depends on $\cX$ only, although it is constructed with the help of a particular orthonormal basis matrix $X$ of
$\cX$.

The same can be said about $\wtd X\in\bbO^{n\times k}$ such that
\begin{equation}\label{eq:charz-tX}
\cR(\wtd X)=\wtd\cX, \quad \wtd X^{\T}D\succeq 0.
\end{equation}
In view of those, it only makes sense to bound $\|X-\wtd X\|$ when $\rank(X^{\T}D)=\rank(\wtd X^{\T}D)=k$ but to bound
$\min \|\wtd X-Y\|$ subject to $Y\in\bbX$ when $\rank(X^{\T}D)=\rank(\wtd X^{\T}D)<k$.


Let $r=\rank(X^{\T}D)=\rank(\wtd X^{\T}D)$, and express the SVD of
$\wtd X^{\T}D$ in the same way as in \eqref{eq:SVD:XD}, except putting a {\em tilde\/} on
each of the symbols $V_i$ and $\Sigma_1$ there. Write
\begin{equation}\label{eq:Sigma1}
\Sigma_1=\diag(\sigma_1,\sigma_2,\ldots,\sigma_r),\quad
\sigma_1\ge\cdots\ge\sigma_r>0.
\end{equation}
and, similarly for $\wtd\Sigma_1$.
Our main result of this paper is stated in the following theorem.

\begin{theorem} \label{thm:main}
Given $D\in\bbR^{n\times k}$ and $k$-dimensional subspaces $\cX$ and $\wtd\cX$ of $\bbR^n$,
let $X,\,\wtd X\in\bbO^{n\times k}$ such that both \eqref{eq:charz-X} and \eqref{eq:charz-tX} hold.
Suppose that $\rank(X^{\T}D)=\rank(\wtd X^{\T}D)=:r$.
Then for any unitarily invariant norm $\|\cdot\|_{\UI}$
%, and let the set $\bbX$ be given in~\eqref{eq:xset} and
%the singular value decompositions of $X^{\T}D$ and $\wtd X^{\T}D$ be defined in~\eqref{eq:svd-xd-txd}. Then
\begin{equation} \label{ineq:main}
\min_{Y\in\bbX}\|\wtd X-Y\|_{\UI}\le\eta\,\|\sin\Theta(\cX,\wtd\cX)\|_{\UI},
\end{equation}
where $\bbX$ is defined as in~\eqref{eq:xset}, and
\begin{equation}\label{eq:main-eta}
\eta=
\begin{cases}
  \sqrt 2\left(1+\dfrac{2\,\|D\|_2}{\sigma_k+\tilde\sigma_k}\right),  &  \mbox{if $r=k$},\\[1em]
  \sqrt 2\left(1+\dfrac{2\,\|D\|_2}{\sigma_r+\tilde\sigma_r}\right)+\dfrac{(2\sqrt 2+4)\,\|D\|_2}{\max\{\sigma_r,\tilde\sigma_r\}},&\mbox{if $r<k$}.
\end{cases}
\end{equation}
Inequality \eqref{ineq:main} can be improved for  the matrix spectral and Frobenius norm,
in the case when $r<k$, with a smaller $\eta$ given by
\begin{equation}\label{eq:main-eta'}
\eta=
\begin{cases}
  \sqrt 2\left(1+\dfrac{2\,\|D\|_2}{\sigma_r+\tilde\sigma_r}\right)
         +\dfrac{4\,\|D\|_2}{\max\{\sigma_r,\tilde\sigma_r\}},
                    &  \mbox{for $\|\cdot\|_{\UI}=\|\cdot\|_{\F}$},\\[1em]
  \sqrt 2+\sqrt{\dfrac{8\,\|D\|_2^2}{(\sigma_r+\tilde\sigma_r)^2}
             +\dfrac{4\,\|D\|_2^2}{[\max\{\sigma_r,\tilde\sigma_r\}]^2}}
             +\dfrac{4\,\|D\|_2}{\max\{\sigma_r,\tilde\sigma_r\}},
                    &\mbox{for $\|\cdot\|_{\UI}=\|\cdot\|_2$}.
\end{cases}
\end{equation}
\end{theorem}

\begin{remark}\label{rk:main}
There are a few comments in order.
\begin{enumerate}[(a)]
\item For the case $r=k$, the left-hand side of~\eqref{ineq:main} is really $\|\wtd X-X\|_{\UI}$, yielding
      \begin{equation} \label{eq:main;r=k}
      \|\wtd X-X\|_{\UI}\le\eta\,\|\sin\Theta(\cX,\wtd\cX)\|_{\UI},
      \end{equation}
      because then $\bbX=\{X\}$
      as we previously explained.

\item The coefficient $\eta$ is smallest when $r=k$, for any general unitarily invariant norm and for the
      two specific ones: the matrix spectral and Frobenius norm.
      Both values for $\eta$ in \eqref{eq:main-eta'} are smaller than
      the ones in \eqref{eq:main-eta} for the case $r<k$. This is easily seen for the first value in \eqref{eq:main-eta'};
      for the second value, we may use
      $\sqrt{a^2+b^2}\le a+b$ for all $a,\,b\ge 0$ to see the fact.

%\item Note that in Theorem~\ref{thm:main}, we use $\tilde\sigma_i$ for $1\le i\le r$
%being the singular values of the symmetric matrix $\wtd X^{\T}D$, whereas those applying in~\eqref{eq:upb1}
%and~\eqref{eq:upb-ui-nofull} are the singular values of $\what X^{\T}D$. In fact, $\what X^{\T}D$
%and $\wtd X^{\T}D$ have the same singular values because of $\what X=\wtd X\wtd G$ with $\wtd G^{\T}\wtd G=I_k$.

\item Similarly to the definition of set $\bbX$, we may define, associated with $\wtd X$,
      \begin{equation}\label{eq:txset}
      \wtd\bbX=\left\{\wtd X\wtd V\begin{bmatrix}
                           I_r &  \\
                            & W
                         \end{bmatrix}\wtd V=\wtd X\wtd V_1\wtd V_1^{\T}+\wtd X\wtd V_2 W\wtd V_2^{\T}\,:\, W\in\bbO^{r'\times r'}\right\}.
      \end{equation}
      As we explained before, the term $\wtd X\wtd V_1\wtd V_1^{\T}$ depends on $\wtd\cX$ only, and
      $\wtd X$ is just one of the elements in $\wtd\bbX$ and can be any one in the set as far as the conclusion
      of Theorem~\ref{thm:main} is concerned. Hence \eqref{ineq:main} leads to a bound on
      the Hausdorff distance~\cite[Section~11.1]{pete:2006}
      between $\bbX$ and $\wtd\bbX$
      \begin{equation}\label{eq:haus-dist}
      \dist(\bbX,\wtd\bbX)=\max_{\wtd Y\in\wtd\bbX}\,\min_{Y\in\bbX}\|\wtd Y-Y\|_{\UI}\le\eta\,\|\sin\Theta(\cX,\wtd\cX)\|_{\UI},
      \end{equation}
      which can be interpreted as for any point $\wtd Y$ in $\wtd\bbX$
      there is a point $Y$ in $\bbX$ that is no further than $\eta\|\sin\Theta(\cX,\wtd\cX)\|_{\UI}$ away from $\wtd Y$.
\end{enumerate}
\end{remark}

The rest of this section is devoted to the proof of Theorem~\ref{thm:main}. For that purpose, we notice that,
by Lemma~\ref{le:max-angle}, there exists an orthogonal matrix $Q\in\bbO^{k\times k}$
such that $\what X=\wtd X Q^{\T}\in\bbO^{n\times k}$ satisfies
\begin{equation}\label{eq:(X,tX)}
    \|X-\what X\|_{\UI}\le\sqrt 2\,\|\sin\Theta(\cX,\wtd\cX)\|_{\UI}.
\end{equation}
Note $\cR(\what X)=\wtd\cX$ and also $\wtd X=\what XQ$. Using \eqref{eq:(X,tX)}, we get
\begin{equation}\label{eq:xd-x0td}
\|X^{\T}D-\what X^{\T}D\|_{\UI}\le\|X^{\T}-\what X^{\T}\|_{\UI}\|D\|_2
\le\sqrt 2\,\|D\|_2\,\|\sin\Theta(\cX,\wtd\cX)\|_{\UI}.
\end{equation}



\subsection{Case $\rank(X^{\T}D)=\rank(\wtd X^{\T}D)=k$}
In this case, $X^{\T}D\succ 0$ and $\wtd X^{\T}D\succ 0$, and both $X$ and $\wtd X$ are unique.
Therefore, we can bound $\|X-\wtd X\|_{\UI}$.
%The following lemma on the perturbation bounds of the generalized polar
%decomposition is necessary for our later developments.


%Let
%Since $\rank(\what X^{\T}D)=k$, let the SVD of $\what X^{\T}D$ be
%$\what X^{\T}D=\wtd U\wtd\Sigma\wtd V^{\T}$ with $\wtd\Sigma=\diag(\tilde\sigma_1,\dots,\tilde\sigma_k)$.
%Then, by $\wtd X^{\T}D\succ 0$ and Lemma~\ref{le:mp}, we have
%\[
%\wtd X=\what X\wtd U\wtd V^{\T}.
%\]
Observe that $\what X^{\T}D=Q(\wtd X^{\T}D)$ which is the polar decomposition of $\what X^{\T}D$ because $Q\in\bbO^{k\times k}$ and
$\wtd X^{\T}D\succeq 0$, while
$X^{\T}D=I_k\cdot(X^{\T}D)$ is the polar decomposition of $X^{\T}D$. Hence by Lemma~\ref{le:polar}, we have
$$
\|I_k-Q\|_{\UI}\le\frac{2}{\sigma_k+\tilde\sigma_k}\|X^{\T}D-\what X^{\T}D\|_{\UI}
     \le\frac{2\,\|D\|_2}{\sigma_k+\tilde\sigma_k}\|X^{\T}-\what X^{\T}\|_{\UI}.
$$
Finally, we have, using \eqref{eq:(X,tX)},
\begin{align}
\|X-\wtd X\|_{\UI}&=\|X-\what XQ\|_{\UI} \nonumber\\
 &=\|X-\what X+\what X-\what XQ\|_{\UI} \nonumber\\
 &\le\|X-\what X\|_{\UI}+\|\what X\|_2\|I_k-Q\|_{\UI} \nonumber\\
 &\le\|X-\what X\|_{\UI}
       +\frac{2\,\|D\|_2}{\sigma_k+\tilde\sigma_k}\|X^{\T}-\what X^{\T}\|_{\UI} \nonumber\\
&\le\left(1+\frac{2\,\|D\|_2}{\sigma_k+\tilde\sigma_k}\right)\sqrt 2\,\|\sin\Theta(\cX,\wtd\cX)\|_{\UI},
  \label{eq:upb1}
\end{align}
yielding \eqref{ineq:main} for the case.

\subsection{$\rank(X^{\T}D)=\rank(\wtd X^{\T}D)<k$}
%As stated in Lemma~\ref{le:mp}, if $\rank(X^{\T}D)=\rank(\wtd X^{\T}D)<k$,
%any entry of the set $\bbX$ defined in~\eqref{eq:xset} satisfies the assumption~\eqref{eq:assp}.
%In such cases, it naturally hopes to monitor whether such $\wtd X$ keeping $\wtd X^{\T}D\succeq 0$
%moves towards the set $\bbX$ by estimating $\min_{Y\in\bbX}\|\wtd X-Y\|_{\UI}$
%when $\|\sin\Theta(\cX,\wtd\cX)\|_{\UI}\rightarrow 0$.
%For bounding such approximations, we first present the following lemma which can be regarded as
%a special case of the perturbation bounds in connection with SVD~\cite{wedi:1972}.


In the current case, both $X$ and $\wtd X$ are not uniquely determined by $X^{\T}D\succeq 0$ and $\wtd X^{\T}D\succeq 0$.
Hence it only make sense to bound $\|\wtd X-Y\|_{\UI}$ subject to $Y\in\bbX$.

Recall $\what X=\wtd X Q^{\T}$ introduced to satisfy \eqref{eq:(X,tX)}. Evidently,
$\rank(\what X^{\T}D)=\rank(\wtd X^{\T}D)=\rank(X^{\T}D)=r<k$. The SVD of $\what X^{\T}D=Q(\wtd X^{\T}D)$ can be given as
\begin{equation}\label{eq:svd-hatXD}
\what X^{\T}D=(Q\wtd U)\wtd\Sigma\wtd V^{\T}\equiv\kbordermatrix{ &\sss r &\sss r'\\
                                 & Q\wtd U_1 & Q\wtd U_2}\times
                      \kbordermatrix{ &\sss r &\sss r' \\
                                    \sss r & \wtd\Sigma_1 & \\
                                    \sss r' & & 0}\times
                      \kbordermatrix{ &\\
                               \sss r  & \wtd V_1^{\T} \\
                               \sss r'  & \wtd V_2^{\T}},
\end{equation}
%which is similar to the one in \eqref{eq:SVD:XD}, except that the left singular vector matrices $\what U_i$ are different from
%the right ones because possibly $\what X^{\T}D\not\succeq 0$.
and let $\what U=Q\wtd U$, $\what U_1=Q\wtd U_1$, and $\what U_2=Q\wtd U_2$.
By Lemma~\ref{le:mp},
there exists a $\wtd W\in\bbO^{r'\times r'}$ such that
\begin{equation}\label{eq:def-tx-nofullrank}
\wtd X=\what X\what U\begin{bmatrix}
                     I_r &  \\
                      & \wtd W
                   \end{bmatrix}\wtd V^{\T}=\what X\what U_1\wtd V_1^{\T}+\what X\what U_2 \wtd W\wtd V_2^{\T}.
\end{equation}
We have, by~\eqref{eq:xset} and \eqref{eq:def-tx-nofullrank},
\begin{align}
\min_{Y\in\bbX}\|\wtd X-Y\|_{\UI}
    &=\min_{W\in\bbO^{r'\times r'}}
        \left\|\what X\what U\begin{bmatrix}
                     I_r &  \\
                      & \wtd W
                   \end{bmatrix}\wtd V^{\T}- XV\begin{bmatrix}
                     I_r &  \\
                      & W
                   \end{bmatrix}V^{\T}\right\|_{\UI} \notag\\
  &\le\min_{W\in\bbO^{r'\times r'}}\left(\left\|\what X\what U\begin{bmatrix}
                     I_r &  \\
                      & \wtd W
                   \end{bmatrix}\wtd V^{\T}- \what XV\begin{bmatrix}
                     I_r &  \\
                      & W
                   \end{bmatrix}V^{\T}\right\|_{\UI}\right. \notag \\
  &\qquad\qquad\qquad \left.+\left\|\what XV\begin{bmatrix}
                     I_r &  \\
                      & W
                   \end{bmatrix}V^{\T}- XV\begin{bmatrix}
                     I_r &  \\
                      & W
                   \end{bmatrix}V^{\T}\right\|_{\UI}\right)  \notag\\
  &=\|\what X- X\|_{\UI}+\min_{W\in\bbO^{r'\times r'}}\left\|\what X\what U\begin{bmatrix}
                     I_r &  \\
                      & \wtd W
                   \end{bmatrix}\wtd V^{\T}- \what XV\begin{bmatrix}
                     I_r &  \\
                      & W
                   \end{bmatrix}V^{\T}\right\|_{\UI}  \notag\\
  &\le\|\what X- X\|_{\UI}+\min_{W\in\bbO^{r'\times r'}}\|\what U_1\wtd V_1^{\T}-V_1V_1^{\T}
        +\what U_2 \wtd W\wtd V_2^{\T}-V_2WV_2^{\T}\|_{\UI}\notag \\
  &\le\|\what X- X\|_{\UI}+\|\what U_1\wtd V_1^{\T}-V_1V_1^{\T}\|_{\UI}
      +\min_{W\in\bbO^{r'\times r'}}\|\what U_2\wtd W\wtd V_2^{\T}- V_2WV_2^{\T}\|_{\UI}.
         \label{eq:dist-all}
\end{align}
Now, we shall bound the three terms in the right-hand side of \eqref{eq:dist-all} in terms of
$\|\sin\Theta(\cX,\wtd\cX)\|_{\UI}$.
%Before doing that, we have, by \eqref{eq:(X,tX)}, that
%\begin{equation}\label{eq:xd-x0td}
%\|X^{\T}D-\what X^{\T}D\|_{\UI}\le\|D\|_2\,\|X^{\T}-\what X^{\T}\|_{\UI}
%\le\sqrt 2\,\|D\|_2\,\|\sin\Theta(\cX,\wtd\cX)\|_{\UI}.
%\end{equation}
First $\|\what X- X\|_{\UI}$ has already been taken care of by \eqref{eq:(X,tX)}.
Next, it can be seen that $\what U_1\wtd V_1^{\T}$ and $V_1V_1^{\T}$ are the canonical isometry polar factors of
$\what X^{\T}D$ and $X^{\T}D$, respectively.
By Lemma~\ref{le:polar} and \eqref{eq:xd-x0td}, we have
\begin{align}\label{eq:dist-1}
 \notag \|\what U_1\wtd V_1^{\T}-V_1V_1^{\T}\|_{\UI}&\le\left(\frac{2}{\sigma_r+\tilde\sigma_r}
 +\frac{2}{\max\{ \sigma_r,\tilde\sigma_r\}}\right)\,\|X^{\T}D-\what X^{\T}D\|_{\UI}\\
&\le \left(\frac{2}{\sigma_r+\tilde\sigma_r}+\frac{2}{\max\{\sigma_r,\tilde\sigma_r\}}\right)
  \sqrt 2\,\|D\|_2\,\|\sin\Theta(\cX,\wtd\cX)\|_{ \ui}.
\end{align}
Finally, let $\what\cU_i=\cR(\what U_i)$ and $\cV_i=\cR(V_i)$ for $i=1,2$.
%Next, we manage to evaluate the last term on the right hand side of~\eqref{eq:dist-all}.
It follows from~\eqref{eq:def-comp-sin} and Lemma~\ref{lm:SVD-pert:wedin} that
\begin{align}\label{eq:sinu2v2}
\notag \|\sin\Theta(\cV_2,\what\cU_2)\|_{\UI}&=\|V_1^{\T}\what U_2\|_{\UI}=\|\what U_2^{\T}V_1\|_{\UI}\\
\notag &=\|\sin\Theta(\cV_1,\what\cU_1)\|_{\UI}
         \le\frac{\|X^{\T}D-\what X^{\T}D\|_{\UI}}{\max\{ \sigma_r,\tilde\sigma_r\}} \\
&\le\frac{\sqrt 2\,\|D\|_2\,}{\max\{\sigma_r,\tilde\sigma_r\}}\|\sin\Theta(\cX,\wtd\cX)\|_{ \ui},
\end{align}
where the last inequality holds because of~\eqref{eq:xd-x0td}.
Note that $\what U_2\wtd W,\,V_2\in\bbO^{k\times r'}$ satisfying $\cR(\what U_2\wtd W)=\what\cU_2$ and $\cR(V_2)=\cV_2$.
Hence, by Lemma~\ref{le:max-angle},
there exists an orthogonal matrix $W_1\in\bbO^{r'\times r'}$ such that
\begin{equation} \label{eq:u2}
   \|V_2W_1-\what U_2\wtd W\|_{\UI}\le\sqrt 2\,\|\sin\Theta(\cV_2,\what\cU_2)\|_{ \ui}\le
   \frac{2\,\|D\|_2\,}{\max\{ \sigma_r,\tilde\sigma_r\}}\|\sin\Theta(\cX,\wtd\cX)\|_{ \ui}.
\end{equation}
Similarly, there exists $W_2\in\bbO^{r'\times r'}$ satisfying
\begin{equation} \label{eq:v2}
   \|V_2W_2-\wtd V_2\|_{\UI}\le\sqrt 2\,\|\sin\Theta(\cV_2,\wtd\cV_2)\|_{ \ui}\le
   \frac{2\,\|D\|_2\,}{\max\{ \sigma_r,\tilde\sigma_r\}}\|\sin\Theta(\cX,\wtd\cX)\|_{ \ui}.
\end{equation}
Keeping \eqref{eq:u2} and \eqref{eq:v2} in mind, we have
\begin{align}
\min_{W\in\bbO^{r'\times r'}}&\|\what U_2\wtd W\wtd V_2^{\T}- V_2WV_2^{\T}\|_{\UI} \notag \\
         &\le\|\what U_2\wtd W\wtd V_2^{\T}- V_2W_1W_2^{\T}V_2^{\T}\|_{\UI}  \notag\\
 &=\|\what U_2\wtd W\wtd V_2^{\T}- V_2W_1\wtd V_2^{\T}+V_2W_1\wtd V_2^{\T}-V_2W_1W_2^{\T}V_2^{\T}\|_{\UI} \notag \\
 &\le\|\what U_2\wtd W\wtd V_2^{\T}- V_2W_1\wtd V_2^{\T}\|_{\UI}
       +\|V_2W_1\wtd V_2^{\T}-V_2W_1W_2^{\T}V_2^{\T}\|_{\UI} \notag \\
 &\le\|\what U_2\wtd W- V_2W_1\|_{\UI}+\|\wtd V_2^{\T}-W_2^{\T} V_2^{\T}\|_{\UI} \nonumber\\
       &\le\frac{4\,\|D\|_2\,}{\max\{ \sigma_r,\tilde\sigma_r\}}\|\sin\Theta(\cX,\wtd\cX)\|_{\UI}.
           \label{eq:dist-2}
\end{align}
%\marginpar{\textcolor{red}{(3.22) is not used later}}

%The last inequality holds because of~\eqref{eq:u2} and~\eqref{eq:v2}.
Together with~\eqref{eq:(X,tX)},~\eqref{eq:dist-all}, \eqref{eq:dist-1} and \eqref{eq:dist-2}, we have
\begin{align}
\min_{Y\in\bbX}\|\wtd X-Y\|_{\UI}
    &\le\left[\sqrt 2+\left(\frac{2}{\sigma_r+\tilde\sigma_r}
         +\frac{2}{\max\{\sigma_r,\tilde\sigma_r\}}\right)\sqrt 2\,\|D\|_2\right. \notag \\
    &\hspace{4cm}\left.     +\frac{4\,\|D\|_2}{\max\{ \sigma_r,\tilde\sigma_r\}}\right]\|\sin\Theta(\cX,\wtd\cX)\|_{\UI} \notag\\
    &\le\left(\sqrt 2+\frac{2\sqrt 2\,\|D\|_2}{\sigma_r+\tilde\sigma_r}
    +\frac{(2\sqrt 2+4)\,\|D\|_2}{\max\{\sigma_r,\tilde\sigma_r\}}\right)\|\sin\Theta(\cX,\wtd\cX)\|_{\UI}.
        \label{eq:upb-ui-nofull}
\end{align}
%We summarize what we do in the following theorem.
Inequalities \eqref{eq:upb1} and \eqref{eq:upb-ui-nofull} yield \eqref{ineq:main} for any general unitarily invariant norm.

Inequality \eqref{ineq:main} can be improved for two particular unitarily invariant norms, the matrix spectral and Frobenius norm.
In our case, the improvements are made possible by using better bounds than \eqref{eq:dist-1} when it comes to
the two particular norms, thanks to Lemma~\ref{le:polar}.

By Lemma~\ref{le:polar},
inequality~\eqref{eq:dist-1} can be improved, in the case of the Frobenius norm, to
\begin{align}\label{eq:dist-f1}
\notag \|\what U_1\wtd V_1^{\T}-V_1V_1^{\T}\|_{\F}&\le\frac{2}{\sigma_r+\tilde\sigma_r}\,
\|X^{\T}D-\what X^{\T}D\|_{\F}\\
&\le \frac{2\sqrt 2\,\|D\|_2}{\sigma_r+\tilde\sigma_r}\,\|\sin\Theta(\cX,\wtd\cX)\|_{\F}.
\end{align}
Therefore, together \eqref{eq:dist-all},~\eqref{eq:dist-2} and~\eqref{eq:dist-f1} lead to
\begin{equation}\label{eq:upb-fnorm-nofull}
\min_{Y\in\bbX}\|\wtd X-Y\|_{\F}\le\left(\sqrt 2+\frac{2\sqrt 2\,\|D\|_2}{\sigma_r+\tilde\sigma_r}
+\frac{4\,\|D\|_2}{\max\{\sigma_r,\tilde\sigma_r\}}\right)\,\|\sin\Theta(\cX,\wtd\cX)\|_{\F}.
\end{equation}
Similarly, when $\|\cdot\|_{\UI}=\|\cdot\|_2$, we have by~\eqref{eq:polar-2norm}
\begin{equation}\label{eq:upb-2norm-nofull}
\min_{Y\in\bbX}\|\wtd X-Y\|_2
    \le\left(\sqrt 2+\sqrt{\dfrac{8\,\|D\|_2^2}{(\sigma_r+\tilde\sigma_r)^2}
             +\dfrac{4\,\|D\|_2^2}{\max\{\sigma_r^2,\tilde\sigma_r^2\}}}
             +\frac{4\,\|D\|_2}{\max\{\sigma_r,\tilde\sigma_r\}}\right)
             \,\|\sin\Theta(\cX,\wtd\cX)\|_2.
\end{equation}
Inequalities \eqref{eq:upb-fnorm-nofull} and \eqref{eq:upb-2norm-nofull} yield
\eqref{ineq:main} with  improved $\eta$ given as in \eqref{eq:main-eta'}.

\begin{remark}\label{rk:main'}
Slight improvements to \eqref{ineq:main} for any general unitarily invariant norm are also possible from another direction.
Assuming $r=\rank(X^{\T}D)=\rank(\wtd X^{\T}D)<k$, we can have
\[
\max\big\{\|\sin\Theta(\cV_1,\what\cU_1)\|_{\UI},\; \|\sin\Theta(\cV_1,\wtd\cV_1)\|_{\UI}\big\}
\le\frac{\big\|\big(X^{\T}D-\what X^{\T}D\big)_{\bestr}\big\|_{\UI}}{\max\{\sigma_r,\tilde\sigma_r\}}
\]
by~\eqref{eq:sin-a},  and improve \eqref{eq:xd-x0td} to
\begin{align}
\big\|\big(X^{\T}D-\what X^{\T}D\big)_{\bestr}\big\|_{\UI}
    &\le \|D\|_2\,\big\|\big(X^{\T}-\what X^{\T}\big)_{\bestr}\big\|_{\UI} \nonumber \\
    &\le\sqrt 2\,\|D\|_2\,\big\|\big[\sin\Theta(\cX,\wtd\cX)\big]_{\bestr}\big\|_{\UI}.  \label{eq:xd-x0td'}
\end{align}
%\begin{equation}
%\big\|\big(X^{\T}D-\what X^{\T}D\big)_{\bestr}\big\|_{\UI}
%    \le \|D\|_2\,\big\|\big(X^{\T}-\what X^{\T}\big)_{\bestr}\big\|_{\UI}
%   \le\sqrt 2\,\|D\|_2\,\big\|\big[\sin\Theta(\cX,\wtd\cX)\big]_{\bestr}\big\|_{\UI}.
%\end{equation}
Note
$\big\|\big[\sin\Theta(\cX,\wtd\cX)\big]_{\bestr}\big\|_{\UI}
=\big\|\big(X_{\perp}^{\T}\wtd X\big)_{\bestr}\big\|_{\UI}$ in~\eqref{eq:sinu2v2} to obtain
\[
\|\sin\Theta(\cV_2,\what\cU_2)\|_{\UI}\le\frac{\sqrt 2\,\|D\|_2\,}{\max\{\sigma_r,\tilde\sigma_r\}}
\big\|\big[\sin\Theta(\cX,\wtd\cX)\big]_{\bestr}\big\|_{\UI}.
\]
Similarly, improvements to \eqref{eq:u2} and~\eqref{eq:v2} can be obtained as follows:
\begin{align*}
   \|V_2W_1-\what U_2\wtd W\|_{\UI}&\le\sqrt 2\,\|\sin\Theta(\cV_2,\what\cU_2)\|_{\UI}
\le\frac{2\,\|D\|_2\,}{\max\{\sigma_r,\tilde\sigma_r\}}
\big\|\big[\sin\Theta(\cX,\wtd\cX)\big]_{\bestr}\big\|_{\UI},\\
   \|V_2W_2-\wtd V_2\|_{\UI}&\le\sqrt 2\,\|\sin\Theta(\cV_2,\wtd\cV_2)\|_{\UI}
\le\frac{2\,\|D\|_2\,}{\max\{\sigma_r,\tilde\sigma_r\}}
\big\|\big[\sin\Theta(\cX,\wtd\cX)\big]_{\bestr}\big\|_{\UI},
\end{align*}
consequently a slightly sharper bound on $\min_{Y\in\bbX}\|\wtd X-Y\|_{\UI}$ than \eqref{ineq:main} by replacing
$\sin\Theta(\cX,\wtd\cX)$ there with $\big[\sin\Theta(\cX,\wtd\cX)\big]_{\bestr}$.
\end{remark}

\section{Numerical examples}\label{sec:num}
In this section, we conduct  numerical experiments
%on a synthetic problem and a trace ratio optimization problem
to demonstrate the effectiveness of the main result in this paper.

%\begin{example}\label{eg1}
%To make the numerical example repeatable,
Let $M=\frac 1{\sqrt n}\,\hadm(n)$, where $\hadm$ is a  MATLAB function that generates a Hadamard matrix, which
is orthogonal. Let
$$
X_{\diamond}=M_{ (:,1:k)},\quad
\wtd X_{\diamond}=\sqrt{1-\delta^2}\,M_{(:,1:k)}Q_1+\delta M_{(:,k+1:2k)}Q_2,
$$
%we apply a Hadamard matrix by the MATLAB function ``$\hadm(n)$'' to construct an orthogonal matrix. Specifically, let
%\begin{align*}
%    M&=\frac{\hadm(n)}{\sqrt{n}},\quad X_{\diamond}=M_{ (:,1:k)},\\
%   \wtd X_{\diamond}&=\sqrt{1-\delta^2}\,M_{(:,1:k)}Q_1+\delta M_{(:,k+1:2k)}Q_2,
%\end{align*}
where $\delta$ is a parameter to control the distance between $\cX=\cR(X_{\diamond})$
and $\wtd\cX=\cR(\wtd X_{\diamond})$, $Q_1, Q_2\in\bbO^{k\times k}$ generated by
MATLAB's built-in functions $\orth$ and $\randn$ as $\orth(\randn(k))$. It can be calculated that
$X_{\diamond}^{\T}\wtd X_{\diamond}=\sqrt{1-\delta^2} Q_1$ whose singular values are
$\sqrt{1-\delta^2}$ of multiplicity $k$ and, hence,  the $k$ canonical angles $\theta_i$
between $\cX$ and $\wtd\cX$ are all the same with $\cos\theta_i=\sqrt{1-\delta^2}$, yielding
\begin{equation}\label{eq:sin-value}
\|\sin\Theta(\cX,\wtd\cX)\|_2=\delta,\,
\|\sin\Theta(\cX,\wtd\cX)\|_{\F}=\sqrt k\,\delta,\,
\|\sin\Theta(\cX,\wtd\cX)\|_{\tr}= k\,\delta.
\end{equation}
They all go to $0$ as $\delta$ does, but as orthonormal basis matrices of $\cX$ and $\wtd\cX$,
respectively, $X_{\diamond}$ and $\wtd X_{\diamond}$ are nowhere near.

Our main result in this paper shows that any $D\in\bbR^{n\times k}$ such that $X^{\T}D\succ 0,\,\wtd X^{\T}D\succ 0$
can nail down particular orthonormal basis matrices $X$ of $\cX$ and $\wtd X$  of $\wtd\cX$,
respectively, and that ensures $X-\wtd X=O(\delta)$. In what follows we will first numerically demonstrate
the sharpness of this upper bound for the
matrix norms $\|\cdot\|_{\UI}=\|\cdot\|_2,\, \|\cdot\|_{\F}$, and $\|\cdot\|_{\tr}$, as
$\delta\to 0$.
Specifically, let
$X=X_{\diamond}UV^{\T}$ and $\wtd X=\wtd X_{\diamond}\wtd U\wtd V^{\T}$
where $UV^{\T}$ and $\wtd U\wtd V^{\T}$ are the orthogonal polar factors of $X_{\diamond}^{\T}D$
and $\wtd X_{\diamond}^{\T}D$, respectively. Consider
$n=96$, $k=5$, and
\begin{equation}\label{eq:D-fullrank}
D=
\begin{bmatrix}
    1 & 0 & 0 & 0 & 0\\
    0 & 1 & 0 & 0 & 0\\
    0 & 0 & 1 & 0 & 0\\
    0 & 0 & 0 & 1 & 0\\
    0 & 0 & 0 & 0 & 1\\
    \frac 6 {8n} &  \frac 6 {8n+1} &  \frac 6 {8n+2} &  \frac 6 {8n+3} &  \frac 6 {8n+4} \\
    \vdots & \vdots & \vdots & \vdots &  \vdots\\
    \frac n {8n} &  \frac n {8n+1} &  \frac n {8n+2} &  \frac n {8n+3} &  \frac n {8n+4}
\end{bmatrix}\textcolor{red}{.}
\end{equation}
With this $D$, we have, by Theorem~\ref{ineq:main},
\begin{equation}\label{eq:egs:bd1}
\|X-\wtd X\|_{\UI}\le\xi_{\UI}:=\left(\sqrt 2+\frac{2\sqrt 2\,\|D\|_2}{\sigma_k+\tilde\sigma_k}\right)
                \times\|\sin\Theta(\cX,\wtd\cX)\|_{\UI}.
\end{equation}
In Figure~\ref{fig1}, we present three plots, each of which contains
the upper bound $\xi_{\UI}$ in \eqref{eq:egs:bd1} and the exact $\|X-\wtd X\|_{\UI}$ for the three norms, respectively.
It is observed that the upper bounds are tight and indicative of the true difference $\|X-\wtd X\|_{\UI}$.

\begin{figure}[t]
\begin{center}
\includegraphics[height=0.20\textheight,width=0.328\textwidth]{fig111.eps}
\includegraphics[height=0.20\textheight,width=0.328\textwidth]{fig112.eps}
\includegraphics[height=0.20\textheight,width=0.328\textwidth]{fig113.eps}
\end{center}
\vspace{-10pt}
\caption{\small
         The full rank case: $r=k$ with $D$ as in \eqref{eq:D-fullrank}.
         Upper bound $\xi_{\UI}$ in \eqref{eq:egs:bd1} and the exact $\|X-\wtd X\|_{\UI}$
         as $\delta$ varies from $10^{-12}$ to $10^{-2}$.
         {\em Left:\/} the spectral norm; {\em Middle:\/} the Frobenius norm;
         {\em Right:\/} the trace norm.}
\label{fig1}
\end{figure}

%\marginpar{\tiny Figs. 4.1, 4.2, 4.3: use $\|\cdot\|_{\tr}$}
%
%to align basis matrices for the subspaces $\cR(X_{\diamond})$ and $\cR(\wtd X_{\diamond})$, respectively.
%It is clear that in such a particular case, $\rank(X_{\diamond}^{\T}D)=\rank(\wtd X_{\diamond}^{\T}D)=k$.
%We set $X=X_{\diamond}UV^{\T}$ and $\wtd X=\wtd X_{\diamond}\wtd U\wtd V^{\T}$
%where $UV^{\T}$ and $\wtd U\wtd V^{\T}$ are polar factors of $X_{\diamond}^{\T}D$
%and $\wtd X_{\diamond}^{\T}D$, respectively, and measure
%the following errors $\|X-\wtd X\|_2$, $\|X-\wtd X\|_{\F}$, $\|X-\wtd X\|_{\tr}$
%and their associated upper bounds in~\eqref{eq:upb1},
%i.e.,
%\begin{align*}
%    \xi_2&=\left(\sqrt 2+\frac{2\sqrt 2\,\|D\|_2}{\sigma_k+\tilde\sigma_k}\right)\times\|\sin\Theta(\cX,\wtd\cX)\|_2,\\
%    \xi_{\F}&=\left(\sqrt 2+\frac{2\sqrt 2\,\|D\|_2}{\sigma_k+\tilde\sigma_k}\right)\times\|\sin\Theta(\cX,\wtd\cX)\|_{\F},\\
%    \xi_{\tr}&=\left(\sqrt 2+\frac{2\sqrt 2\,\|D\|_2}{\sigma_k+\tilde\sigma_k}\right)\times\|\sin\Theta(\cX,\wtd\cX)\|_{\tr},
%\end{align*}
%where $\|\cdot\|_{\tr}$ is the nuclear norm given by~\eqref{eq:nc-norm}.
%By simply calculating the matrix $X_{\perp}^{\T}\wtd X$, we have
%\[\|\sin\Theta(\cX,\wtd\cX)\|_2=\delta,\ \|\sin\Theta(\cX,\wtd\cX)\|_{\F}=\sqrt{k}\,\delta\
%\mbox{and}\ \|\sin\Theta(\cX,\wtd\cX)\|_{\tr}=k\,\delta.\]
%We vary the parameter $\delta$ and plot the corresponding numerical results in Figure~\ref{fig1}.
%Inspection of Figure~\ref{fig1} shows that $\|X-\wtd X\|_2\rightarrow 0$, $\|X-\wtd X\|_{\F}\rightarrow 0$ and
%$\|X-\wtd X\|_{\tr}\rightarrow 0$ as $\delta\rightarrow 0$
%which is different to $\lim_{\delta\rightarrow 0}\|X_{\diamond}-\wtd X_{\diamond}\|_{\UI}$.
%In addition, the plots suggest that $\xi_2$, $\xi_{\F}$ and $\xi_{\tr}$ provide sharp upper bounds
%for $\|X-\wtd X\|_2$, $\|X-\wtd X\|_{\F}$ and $\|X-\wtd X\|_{\tr}$, respectively.

Next, we consider the rank-deficient case: $\rank(X^{\T}D)=\rank(\wtd X^{\T}D)=:r<k$.
By Theorem~\ref{thm:main}, we have
\begin{subequations}\label{eq:def-xi-nofr}
\begin{equation}\label{eq:egs:bd2}
\min_{Y\in\bbX}\|\wtd X-Y\|_{\UI}\le\xi_{\UI},
\end{equation}
where for the
matrix norms $\|\cdot\|_{\UI}=\|\cdot\|_2,\, \|\cdot\|_{\F}$, and $\|\cdot\|_{\tr}$
\begin{align}
\xi_2&=\left(\sqrt 2+\sqrt{\dfrac{8\,\|D\|_2^2}{(\sigma_r+\tilde\sigma_r)^2}
       +\dfrac{4\,\|D\|_2^2}{\max\{\sigma_r^2,\tilde\sigma_r^2\}}}+\frac{4\,\|D\|_2}{\max\{\sigma_r,\tilde\sigma_r\}}\right)
         \,\|\sin\Theta(\cX,\wtd\cX)\|_2, \label{eq:def-xi-nofr:2}\\
\xi_{\F}&=\left(\sqrt 2+\frac{2\sqrt 2\,\|D\|_2}{\sigma_r+\tilde\sigma_r}+\frac{4\,\|D\|_2}
          {\max\{\sigma_r,\tilde\sigma_r\}}\right)\times\|\sin\Theta(\cX,\wtd\cX)\|_{\F}, \label{eq:def-xi-nofr:F}\\
\xi_{\tr}&=\left(\sqrt 2+\frac{2\sqrt 2\,\|D\|_2}{\sigma_r+\tilde\sigma_r}+\frac{(2\sqrt{2}+4)\,\|D\|_2}
          {\max\{\sigma_r,\tilde\sigma_r\}}\right)\times\|\sin\Theta(\cX,\wtd\cX)\|_{\tr}. \label{eq:def-xi-nofr:tr}
\end{align}
\end{subequations}



For $r=k-1$, we simply take the same $D$
in \eqref{eq:D-fullrank} but reset
its last column to $0$. By Lemma~\ref{le:mp}, there are only two $X$ that satisfy $X^{\T}D\succeq 0$ and
$\cR(X)=\cX:=\cR(X_{\diamond})$:
$$
X=X_{\diamond}U_{(:,1:k-1)}V_{(:,1:k-1)}^{\T}+X_{\diamond}U_{(:,k)}V_{(:,k)}^{\T}, \quad
X_-=X_{\diamond}U_{(:,1:k-1)}V_{(:,1:k-1)}^{\T}-X_{\diamond}U_{(:,k)}V_{(:,k)}^{\T}.
$$
The same can be said for $\wtd X$ that satisfies $\wtd X^{\T}D\succeq 0$ and $\cR(\wtd X)=\wtd\cX:=\cR(\wtd X_{\diamond})$.
Hence
\begin{equation}\label{eq:egs:r=k-1}
\min_{Y\in\bbX}\|\wtd X-Y\|_{\UI}=\min\{\|\wtd X-X\|_{\UI}, \|\wtd X-X_-\|_{\UI}\}.
\end{equation}
In Figure~\ref{fig2}, we again present three plots, each of which contains
the upper bound $\xi_{\UI}$ and the exact quantity in \eqref{eq:egs:r=k-1} for the three norms, respectively.
We observe similar behaviors to those in Figure~\ref{fig1} for the full-rank case.

%The associated numerical results are reported in Figure~\ref{fig2}.
%By Figure~\ref{fig2},
%we observe a similar numerical performance of the upper bounds~\eqref{eq:upb-2norm-nofull},
%\eqref{eq:upb-fnorm-nofull} and~\eqref{ineq:main} as Figure~\ref{fig1}.
%Similar comments we made for Figure~\ref{fig1} are still valid here as well.
\begin{figure}
\begin{center}
\includegraphics[height=0.20\textheight,width=0.328\textwidth]{fig121.eps}
\includegraphics[height=0.20\textheight,width=0.328\textwidth]{fig122.eps}
\includegraphics[height=0.20\textheight,width=0.328\textwidth]{fig123.eps}
\end{center}
\vspace{-10pt}
\caption{\small
         The rank-deficient case: $r=k-1$ by resetting the last column of $D$ in \eqref{eq:D-fullrank} to $0$.
         Upper bound $\xi_{\UI}$ in \eqref{eq:def-xi-nofr} and the exact $\min\|\wtd X-Y\|_{\UI}$ in
         \eqref{eq:egs:r=k-1}
         as $\delta$ varies from $10^{-12}$ to $10^{-2}$.
         {\em Left:\/} the spectral norm; {\em Middle:\/} the Frobenius norm;
         {\em Right:\/} the trace norm.}
\label{fig2}
\end{figure}

When $r<k-1$, the set $\bbX=\left\{X_{\sss\diamond}U_1V_1^{\T}+X_{\sss\diamond}U_2WV_2^{\T}\,:\, W\in\bbO^{(k-r)\times(k-r)}\right\}$
contains infinitely many elements. Fortunately,  for the Frobenius norm, we have
\begin{align*}
    \min_{Y\in\bbX}\|\wtd X-Y\|_{\F}^2&=\min_{W\in\bbO^{(k-r)\times(k-r)}}\left\|\wtd X-X_{\sss\diamond}U
                                         \begin{bmatrix}
                                            I_r &  \\
                                                &  W
                                            \end{bmatrix}V^{\T}\right\|_{\F}^2\\
                                       &=\min_{W\in\bbO^{(k-r)\times(k-r)}}\left\|\wtd XV-X_{\sss\diamond}U
                                         \begin{bmatrix}
                                            I_r &  \\
                                                &  W
                                         \end{bmatrix}\right\|_{\F}^2\\
                                       &=\min_{W\in\bbO^{(k-r)\times(k-r)}}\left\|
                                       \begin{bmatrix}
                                           \wtd XV_1, &\wtd XV_2
                                       \end{bmatrix}-\begin{bmatrix}
                                                        X_{\sss\diamond}U_1, & X_{\sss\diamond}U_2W
                                                      \end{bmatrix}
                                        \right\|_{\F}^2\\
                                       &=\|\wtd XV_1-X_{\sss\diamond}U_1\|_{\F}^2+\min_{W\in\bbO^{(k-r)\times(k-r)}}
                                       \|\wtd XV_2-X_{\sss\diamond}U_2W\|_{\F}^2,
\end{align*}
where the last term can be expressed as
\begin{equation*}
    \min_{W\in\bbO^{(k-r)\times(k-r)}}\|\wtd XV_2-X_{\sss\diamond}U_2W\|_{\F}^2=2(k-r)^2-
    \max_{W\in\bbO^{(k-r)\times(k-r)}}\tr(W^{\T}U_2^{\T}X_{\sss\diamond}^{\T}\wtd XV_2).
\end{equation*}
By von Neumann's trace inequality~\cite[section II.3.1]{stsu:1990}, the optimizer $W_{\sss\opt}$ for
\[
\max_{W\in\bbO^{(k-r)\times(k-r)}}\tr(W^{\T}U_2^{\T}X_{\sss\diamond}^{\T}\wtd XV_2)
\]
is the orthogonal polar factor of $U_2^{\T}X_{\sss\diamond}^{\T}\wtd XV_2$, and finally
\begin{equation}\label{eq:opt-form}
\min_{Y\in\bbX}\|\wtd X-Y\|_{\F}=\|\wtd X-Y_{\opt}\|_{\F},
\end{equation}
where $Y_{\opt}=X_{\sss\diamond}U_1V_1^{\T}+X_{\sss\diamond}U_2W_{\opt}V_2^{\T}$.
For norms other than $\|\cdot\|_{\F}$, it is not easy to calculate $\min\|\wtd X-Y\|_{\UI}$ subject to $Y\in\bbX$. For those norms,
we can tightly bound the exact $\min_Y\|\wtd X-Y\|_{\UI}$ tightly with the help of this $Y_{\opt}$ as follows:
%However, based on the equivalence of norms,
%we have
\begin{subequations} \label{eq:bound-by-eql}
    \begin{align}
\frac 1{\sqrt k}\|\wtd X-Y_{\opt}\|_{\F}\le&\min_{Y\in\bbX}\|\wtd X-Y\|_2\le\|\wtd X-Y_{\opt}\|_2, %\le\|\wtd X-Y_{\opt}\|_{\F}\le\xi_{\F},
        \label{eq:bound-by-eql-2} \\
\|\wtd X-Y_{\opt}\|_{\F}\le&\min_{Y\in\bbX}\|\wtd X-Y\|_{\tr}\le\|\wtd X-Y_{\opt}\|_{\tr}, %\le\sqrt{k}\,\|\wtd X-Y_{\opt}\|_{\F}\le\sqrt{k}\,\xi_{\F},
        \label{eq:bound-by-eql-ncl}
    \end{align}
\end{subequations}
%where $\xi_{\F}$ is defined in~\eqref{eq:def-xi-nofr:F}.
because $Y_{\opt}\in\bbX$ and for any $Y\in\bbX$,
$$
\frac 1{\sqrt k}\|\wtd X-Y\|_{\F}\le\|\wtd X-Y\|_2, \quad
\|\wtd X-Y\|_{\F}\le\|\wtd X-Y\|_{\tr}.
$$
%Our following experiment take $r=k-2$. To do that,
%we simply set the last two columns of $D$ to zeros.
%We compute $\|\wtd X-Y_{\opt}\|_2$, $\|\wtd X-Y_{\opt}\|_{\F}$ and $\|\wtd X-Y_{\opt}\|_{\tr}$
%together with their upper bounds as in~\eqref{eq:bound-by-eql},
%i.e., $\xi_{\F}$, $\xi_{\F}$ and $\sqrt{k}\,\xi_{\F}$, respectively,
%and
We plot in Figure~\ref{fig13} upper bounds $\xi_{\UI}$ for $\min_Y\|\wtd X-Y\|_{\UI}$ the three norms,
their lower and upper bounds in \eqref{eq:bound-by-eql}.
It is noted  that
our upper bounds $\xi_{\UI}$ are again very good and go to $0$ at the same rates as the true ones.
%%$\|\wtd X-Y_{\opt}\|_2\rightarrow 0$ and $\|\wtd X-Y_{\opt}\|_{\tr}\rightarrow 0$
%%as $\delta\rightarrow 0$.
%In addition, it is noted in Figure~\ref{fig13} that the upper bound $\sqrt{k}\,\xi_{\F}$ on $\min\limits_{Y\in\bbX}\|\wtd X-Y\|_{\tr}$
%by~\eqref{eq:bound-by-eql-ncl} is little smaller than $\xi_{\tr}$. There is a good reason for that.
%In fact, for the example, $\|\sin\Theta(\cX,\wtd\cX)\|_{\tr}=\sqrt{k}\,\|\sin\Theta(\cX,\wtd\cX)\|_{\F}$
%by~\eqref{eq:sin-value} and hence
%which implies
%\begin{align*}
%    \sqrt{k}\,\xi_{\F}&=\left(\sqrt 2+\frac{2\sqrt 2\,\|D\|_2}{\sigma_r+\tilde\sigma_r}+\frac{4\,\|D\|_2}
%                {\max\{\sigma_r,\tilde\sigma_r\}}\right)\times\sqrt{k}\,\|\sin\Theta(\cX,\wtd\cX)\|_{\F}\\
%                          &=\left(\sqrt 2+\frac{2\sqrt 2\,\|D\|_2}{\sigma_r+\tilde\sigma_r}+\frac{4\,\|D\|_2}
%                            {\max\{\sigma_r,\tilde\sigma_r\}}\right)\times\|\sin\Theta(\cX,\wtd\cX)\|_{\tr}<\xi_{\tr}.
%\end{align*}
%However, we also observe that for some other examples not reported here or seeing Figure~\ref{fig23}.,
%$\xi_{\tr}<\sqrt{k}\,\xi_{\F}$. It depends on the details of $\|\sin\Theta(\cX,\wtd\cX)\|_{\tr}$
%and $\|\sin\Theta(\cX,\wtd\cX)\|_{\F}$.
\begin{figure}
\begin{center}
\includegraphics[height=0.20\textheight,width=0.328\textwidth]{fig131.eps}
\includegraphics[height=0.20\textheight,width=0.328\textwidth]{fig132.eps}
\includegraphics[height=0.20\textheight,width=0.328\textwidth]{fig133.eps}
\end{center}
\vspace{-10pt}
\caption{The rank-deficient case: $r=k-2$ by resetting the last two column of $D$ in \eqref{eq:D-fullrank} to $0$.
$\|\wtd X-Y_{\opt}\|_{\UI}$,  $\xi_{\UI}$ in \eqref{eq:def-xi-nofr} and bounds by \eqref{eq:bound-by-eql}.
%, and $\sqrt{k}\,\xi_{\F}$.
         {\em Left:\/} the spectral norm; {\em Middle:\/} the Frobenius norm;
         {\em Right:\/} the trace norm.
}
\label{fig13}
\end{figure}
%\end{example}

\iffalse
\begin{example}\label{eg2}
In this example, we consider the optimization problem \eqref{eq:OptSTM} on the Stiefel manifold
with $f$ in the form of~\eqref{eq:f(X)-luli} given by
\begin{equation} \label{eq:trop}
f(X)=\phi(X)+\psi(X)\tr(Z^{\T}D),
\end{equation}
with
\[
\phi(Z)=\frac{\tr(Z^{\T}AZ)}{\tr(Z^{\T}BZ)}\quad\mbox{and}\quad\psi(Z)=\frac{1}{\tr(Z^{\T}BZ)},
\]
where $A\in\bbR^{n\times n}$ and $B\in\bbR^{n\times n}$ are both symmetric matrices with $B\succ 0$.
As shown in~\cite[Theorem~2.3]{wazl:2022}, if $X\in\bbO^{n\times k}$ is a local or global maximizer of~\eqref{eq:trop},
then $X$ satisfies $X^{\T}D\succeq 0$, and $X$ is the orthonormal basis matrix of the invariant subspace
corresponding to the $k$ largest eigenvalues of NEPv \eqref{eq:NEPv-form} with
\begin{equation} \label{eq:nepv2}
    E(Z)Z=Z\Lambda, \quad Z\in\bbO^{n\times k},
\end{equation}
where $\Lambda=Z^{\T}E(Z)Z$ and $E(Z)$ is
\[E(Z)=\frac{2}{\tr(Z^{\T}BZ)}\left[A+\frac{ZD^{\T}+DZ^{\T}}{2}-f(Z)B\right].\]
Here, E(Z) is gotten by using~\eqref{eq:hg} and
\[\begin{cases}
    \dfrac{\partial\phi(Z)}{\partial Z}=E_{\phi}(Z)Z,\quad\mbox{with}\quad E_{\phi}(Z)=2\psi(Z)[A-\phi(Z)B],\\[1em]
    \dfrac{\partial\psi(Z)}{\partial Z}=E_{\psi}(Z)Z,\quad\mbox{with}\quad E_{\psi}(Z)=-2[\psi(Z)]^2 B.\\
\end{cases}
\]

Take $n=50$, $k=3$,
\[
A=\kbordermatrix{        &\sss k  & \sss n-k \\
               \sss k    & A_{11} & 0 \\
               \sss n-k & 0      & 0 },\quad
B=\kbordermatrix{          &\sss k  & \sss n-k\\
                  \sss k   & 0      & 0 \\
                  \sss n-k & 0      & B_{22}},
               \quad\mbox{and}\quad
D=\kbordermatrix{          &\sss k  \\
                  \sss k   & \wtd R   \\
                  \sss n-k & 0     },
\]
with $A_{11}=R\diag(4,3,2,1)R^{\T}$, $B_{22}=\diag(0.001,\dots,0.046)$ and $\wtd R=R\diag(4,3,2,1)$ where $R=\hadm(4)/2$.
In such a case, clearly, the wanted orthonormal basis matrix is $X=\begin{bmatrix}
R \\
0
\end{bmatrix}$ since
    \[E(X)X=X\diag(8,6,4,2) \quad\mbox{and}\quad X^{\T}D=\diag(4,3,2,1)\succ 0.\]
We use the following self-consistent field (SCF) iteration to
computing the maximizer of the trace ratio optimization problem~\eqref{eq:trop}.
\begin{algorithm}
    \caption{An SCF iteration for problem~\eqref{eq:trop}}\label{alg:scf}
\begin{algorithmic}[1]
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\REQUIRE $X_0\in\bbO^{n\times k}$ satisfying $X_0^{\T}D\succeq 0$.
\ENSURE a maximizer of~\eqref{eq:trop}.
%\nlset{(\expandafter{\romannumeral2})}
\FOR{$i=1,2,\dots$,\mbox{\rm until convergence}}
\STATE construct $E_i=E(X_{i-1})$;
\STATE compute the partial eigen-decomposition  $E_i\wht X_i=\wht X_i\Lambda_i$
for the first $k$ largest eigenvalues and their associated eigenvectors of $E_i$;
\STATE compute SVD: $\wht X_i^{\T}D=U_i\Sigma_iV_i^{\T}$;
\STATE $X_i=\wht X_iU_iV_i^{\T}$;
\ENDFOR
\RETURN the last $X_i$.
\end{algorithmic}
\end{algorithm}
\end{example}
It is noted that line 4 is a necessary step in Algorithm~\ref{alg:scf} which makes $X_i^{\T}D\succ 0$
if $\rank(X_i^{\T}D)=k$ or $X_i^{\T}D\succeq 0$ when $\rank(X_i^{\T}D)=r<k$.
As the number of iterations $i$ increases, we compute the errors
 $\|X-X_i\|_2$, $\|X-X_i\|_{\F}$, $\|X-X_i\|_{\tr}$ and their associated upper bounds
\begin{align*}
  \xi_2&=\left(\sqrt 2+\frac{2\sqrt 2\,\|D\|_2}{\sigma_k+\tilde\sigma_k}\right)\times\|\sin\Theta(\cX,\cX_i)\|_2,\\
  \xi_{\F}&=\left(\sqrt 2+\frac{2\sqrt 2\,\|D\|_2}{\sigma_k+\tilde\sigma_k}\right)\times\|\sin\Theta(\cX,\cX_i)\|_{\F},\\
  \xi_{\tr}&=\left(\sqrt 2+\frac{2\sqrt 2\,\|D\|_2}{\sigma_k+\tilde\sigma_k}\right)\times\|\sin\Theta(\cX,\cX_i)\|_{\tr},
\end{align*}
where $\cX_i=\cR(X_i)$ and collect the numerical results in Figure~\ref{fig3}. Furthermore, for testing
the numerical performance of these bounds when $\rank(X^{\T}D)=\rank(X_i^{\T}D)<k$, we reset
$\wtd R=R\diag(4,3,2,0)$ which leads to $\rank(X^{\T}D)=\rank(X_i^{\T}D)=k-1$. Similarly to Example~\ref{eg1}, let
\[X_{\dagger}=X{I_k}_{(:,1:k-1)}{I_k}_{(:,1:k-1)}^{\T}-X{I_k}_{(:,k)}{I_k}_{(:,k)}^{\T},\]
and calculate
\begin{align*}
    &\min_{Y\in\bbX}\|X_i-Y\|_2=\min\{\|X_i-X\|_2, \|X_i-X_{\dagger}\|_2\},\\
&\min_{Y\in\bbX}\|X_i-Y\|_{\F}=\min\{\|X_i-X\|_{\F}, \|X_i-X_{\dagger}\|_{\F}\},\\
&\min_{Y\in\bbX}\|X_i-Y\|_{\tr}=\min\{\|X_i-X\|_{\tr}, \|X_i-X_{\dagger}\|_{\tr}\},\\
    &\xi_2=\left(\sqrt 2+\sqrt{\dfrac{8\,\|D\|_2^2}{(\sigma_r+\tilde\sigma_r)^2}
+\dfrac{4\,\|D\|_2^2}{\max\{\sigma_r^2,\tilde\sigma_r^2\}}}+\frac{4\,\|D\|_2}{\max\{\sigma_r,\tilde\sigma_r\}}\right)
\times\|\sin\Theta(\cX,\cX_i)\|_2,\\
   &\xi_{\F}=\left(\sqrt 2+\frac{2\sqrt 2\,\|D\|_2}{\sigma_r+\tilde\sigma_r}+\frac{4\,\|D\|_2}
                {\max\{\sigma_r,\tilde\sigma_r\}}\right)\times\|\sin\Theta(\cX,\cX_i)\|_{\F},\\
   &\xi_{\tr}=\left(\sqrt 2+\frac{2\sqrt 2\,\|D\|_2}{\sigma_r+\tilde\sigma_r}+\frac{(2\sqrt{2}+4)\,\|D\|_2}
                {\max\{\sigma_r,\tilde\sigma_r\}}\right)\times\|\sin\Theta(\cX,\cX_i)\|_{\tr},
\end{align*}
in Figure~\ref{fig4}. Figures~\ref{fig3} and~\ref{fig4} illustrate that the errors $\min_{Y\in\bbX}\|X_i-Y\|_2\rightarrow 0$,
$\min_{Y\in\bbX}\|X_i-Y\|_{\F}\rightarrow 0$ and $\min_{Y\in\bbX}\|X_i-Y\|_{\tr}\rightarrow 0$
as the number of iterations $i$ increasing, and the upper bounds given by Theorem~\ref{thm:main} match the errors
$\min_{Y\in\bbX}\|X_i-Y\|_{\UI}$ very well.
\begin{figure}
\begin{center}
\includegraphics[height=0.20\textheight,width=0.328\textwidth]{FIGs/fig31.eps}
\includegraphics[height=0.20\textheight,width=0.328\textwidth]{FIGs/fig32.eps}
\includegraphics[height=0.20\textheight,width=0.328\textwidth]{FIGs/fig33.eps}
\end{center}
\vspace{-10pt}
\caption{The errors $\|X-X_i\|_2$, $\|X-X_i\|_{\F}$, $\|X-X_i\|_{\tr}$
and their upper bounds $\xi_2$, $\xi_{\F}$ and $\xi_{\tr}$ in SCF iterations.} \label{fig3}
\end{figure}

\begin{figure}
\begin{center}
\includegraphics[height=0.20\textheight,width=0.328\textwidth]{FIGs/fig41.eps}
\includegraphics[height=0.20\textheight,width=0.328\textwidth]{FIGs/fig42.eps}
\includegraphics[height=0.20\textheight,width=0.328\textwidth]{FIGs/fig43.eps}
\end{center}
\vspace{-10pt}
\caption{The errors $\min_{Y\in\bbX}\|X_i-Y\|_2$, $\min_{Y\in\bbX}\|X_i-Y\|_{\F}$, $\min_{Y\in\bbX}\|X_i-Y\|_{\tr}$
and their upper bounds $\xi_2$, $\xi_{\F}$ and $\xi_{\tr}$ when $\rank(X^{\T}D)=\rank(X_i^{\T}D)<k$ in SCF iterations.}
\label{fig4}
\end{figure}
\fi


\section{Concluding remarks}\label{sec:col}
Let $\cX$ be a $k$-dimensional subspace of $\bbR^n$, and $D\in\bbR^{n\times k}$,
and let $X$ be an orthonormal basis matrix of $\cX$.
If $X^{\T}D\succ 0$, then $X$ is unique among all
orthonormal basis matrices of $\cX$. However if $X^{\T}D\succeq 0$ and $r=\rank(X^{\T}D)<k$, then
$X$ can be any one from set $\bbX$ in \eqref{eq:xset} of orthonormal basis matrices of $\cX$.
These results are recently obtained in \cite{luli:2022,wazl:2022}. In this paper, however,
we study how quantitatively $X$ changes as $\cX$ changes in both cases.

Specifically, suppose that $\cX$ is changed to $\wtd\cX$ and their difference is measured by
$\sin\Theta(\cX,\wtd\cX)$, the
sines of their canonical angles, and let $\wtd X$ be an orthonormal basis matrix of $\wtd\cX$.
In the case when both $X^{\T}D\succ 0,\,\wtd X^{\T}D\succ 0$, we
established upper bounds on $\|X-\wtd X\|$ in terms of $\|\sin\Theta(\cX,\wtd\cX)\|$ for the spectral, Frobenius
and, more generally, any unitarily invariant norm,
while in the case when both $X^{\T}D\succeq 0,\,\wtd X^{\T}D\succeq 0$ and also $\rank(X^{\T}D)=\rank(\wtd X^{\T}D)<k$,
our bounds are essentially on the Hausdorff distances of two sets $\bbX$ and $\wtd\bbX$ (see \eqref{eq:haus-dist}).
Numerical tests are conducted to  demonstrate the sharpness of our bounds.

The result in this paper can be used to understand the convergence of the SCF iteration in the NEPv approach
to solve maximization problems over the Stiefel manifold whose objective functions contain and increase with $\tr(X^{\T}D)$
\cite{luli:2022,wazl:2022,zhys:2020}
and even to assess approximation accuracy during SCF iterations.

%These
%
%subject to $\cR(X)=\cX$
%
%$X,\,\wtd X\in\bbR^{n\times k}$ are orthonormal
%
%Given two $k$-dimensional subspaces $\cX$ and $\wtd\cX$ of $\bbR^n$ and $D\in\bbR^{n\times k}$,
%subject to $\cR(X)=\cX,\,\cR(\wtd X)=\wtd\cX$ where $X,\,\wtd X\in\bbR^{n\times k}$ are orthonormal,
%it is known that $X$ and $\wtd X$ are uniquely determined.
%
%subject to $X^{\T}D\succeq 0,\,\wtd X^{\T}D\succeq 0$ and $\cR(X)=\cX,\,\cR(\wtd X)=\wtd\cX$,
%we have established upper bounds on $\|X-\wtd X\|$ for the spectral, Frobenius
%and, more generally, any unitarily invariant norm in terms the canonical angles between the two subspaces $\cX$ and $\wtd\cX$.
%
%Given two subspaces $\cX$ and $\wtd\cX$ of $\bbR^{n}$ with dimension $k$ which are close in terms of their canonical angles,
%in this paper, we obtain several bounds, presented in Theorem~\ref{thm:main} and Remark~\ref{rk:main},
%for the difference between the basis matrices $X$ and $\wtd X$
%in the condition of $X^{\T}D\succeq 0$, $\wtd X^{\T}D\succeq 0$ and $\rank(X^{\T}D)=\rank(\wtd X^{\T}D)=r$.
%The proof of our main results is partitioned by the cases $r=k$ and $r<k$. Particularly, when $r<k$,
%the basis matrices $X$ and $\wtd X$ satisfying the assumption~\eqref{eq:assp} are not unique.
%We regard bounds on the Hausdorff distance of two sets $\bbX$ and $\wtd\bbX$ given by~\eqref{eq:haus-dist}
%in terms of $\|\sin\Theta(\cX,\wtd\cX)\|_{\UI}$.
%These results are tailored to SCF iterations for a particular type of optimization over the Stiefel manifold
%as defined in~\eqref{eq:f(X)-luli} for estimating how accuracy of the approximated maximizer when $\wtd\cX$ is close to $\cX$.


Although our analysis so far focuses on the real number field, all developments can be
extended to the complex number field straightforwardly. To that end, only a few minor modifications are needed, namely,
replacing all $\bbR$ by $\bbC$, all matrix/vector transposes by complex conjugate transposes and $\tr(\,\cdot\,)$ by
$\re\left(\tr(\,\cdot\,)\right)$, where $\re(\,\cdot\,)$ takes the real part of a complex number.

%{\small
%\bibliographystyle{plainurl}
%\bibliography{PloarFactor}
%}

{\small
%\bibliographystyle{plain}
%\bibliography{\TeXHOME/BIB/strings,\TeXHOME/BIB/mxptrefs,\TeXHOME/BIB/li,\TeXHOME/BIB/ML,\TeXHOME/BIB/opt}
%\bibliography{\TeXHOME/BIB/strings,nsf2022a}
\input projectionSAV.bbl
}

\end{document}
