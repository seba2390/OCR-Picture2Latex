\section{Conclusion}

This paper proposes an optimization scheme for variational quantum circuits by combining the gradient estimate obtained from simultaneous perturbation stochastic approximation with gradient-based optimizers like SGD, Adam, AMSGrad, or RMSProp. We demonstrate with noiseless simulations on simple regression tasks that using the SPSA-approximated gradient improves both the convergence rate by a factor of three and the final absolute error by a factor of more than two compared to the parameter-shift rule. We further observe that the combined SPSA-AMSGrad optimizer consistently outperforms all other methods, including standard SPSA.

Adam, AMSGrad, and RMSProp clearly outperform SGD for SPSA-inferred gradients when considering shot- and hardware noise. The gap in performance grows to a factor of 1.5 for the full noise model. The performance boost between methods remains the same even when error mitigation addresses the hardware noise.

We conclude that combining the computationally cheap to obtain gradient estimate of SPSA with modern gradient-based optimizers drastically speeds up the training process of VQCs and leads to improved convergence.

% funding is already mentioned on the first page
%The research presented in this paper is supported by the Bavarian Ministry of Economic Affairs, Regional Development and Energy with funds from the Hightech Agenda Bayern.