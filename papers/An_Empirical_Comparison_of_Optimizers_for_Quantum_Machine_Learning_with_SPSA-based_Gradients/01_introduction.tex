\section{Introduction}
\label{sec:introduction}

\IEEEPARstart{M}{achine} learning (ML) has been the key driver for a great variety of recent technological advancements, for example in computer vision \cite{Voulodimos2018}, natural language processing \cite{Otter2017}, automatic translation \cite{Vaswani2017}, self-driving cars \cite{Baheri2020}, and medical diagnostics \cite{fatima2017survey}. However, machine learning algorithms often require a considerable amount of computational resources and data in the training stage. The rapidly emerging field of \gls{QML} aims to tackle these challenges by employing a completely different computational paradigm, namely quantum computing. 

Quantum computing gained a lot of attention since it offers speedup in terms of computational complexity for a range of problems, including factoring numbers \cite{Shor94}, unstructured search \cite{Grover96}, or solving systems of linear equations \cite{Harrow09}. Still, the realization of a fault-tolerant quantum computer is yet to be achieved, with current hardware suffering from low qubit counts, coherence times, and high gate- and readout-error rates \cite{preskill2018quantum}. As these limitations restrict the size of the quantum circuits that can be executed, the quantum computing community turned to hybrid quantum-classical algorithms, such as \glspl{VQA} \cite{cerezo2021variational}, in the hope of surpassing classical computing capabilities. They have been applied in the fields of chemistry \cite{OMalley2016}, nuclear physics \cite{Kiss2022}, combinatorial optimization \cite{Farhi2014}, and machine learning \cite{farhi2018classification}, which will be the focus of this work.

Like in classical ML, choosing the correct optimizer for the learning task is a key factor in keeping computational costs as low as possible. In the current literature, there are two popular choices. On the one hand, one can either use the parameter-shift rule \cite{Schuld19} to evaluate gradients on the quantum hardware and use a gradient-based optimizer, like Adam \cite{ADAM}; on the other hand, one can use gradient-free optimization methods that do not need access to exact derivatives, like \gls{SPSA} \cite{Spall92}. The latter only requires estimating two expectation values per update step, regardless of the number of parameters that need to be optimized in the circuit. In contrast, the parameter-shift rule requires \(2p\) estimations, where \(p\) is the number of parameters.

%In this paper we present the first quantitative comparison of the performance of the Adam, RMSProp AMSGrad %and SGD (with momentum) optimizers with both parameter-shift rule inferred and SPSA approximated gradients. 
In this paper, we present the first quantitative comparison of the following optimizers: (1) Adam, (2) RMSProp, (3) AMSGrad, (4) SGD, and (5) SGD with momentum in conjunction with the gradients estimated using parameter-shift or SPSA rules.
%We evaluate the two approaches 
We evaluate each combination of these approaches on multiple regression tasks from the \textit{scikit-learn} library and on real-world data. Numerical experiments were done on the qasm simulator offered by the IBM Quantum services \cite{ibmq2022}. Both ideal and noisy conditions mimicking the ibmq\_ehningen device are employed to study the effect of noise. Additionally, we examine how the use of error mitigation \cite{Cai22} on the noisy simulation impacts the performance of the optimizers.

The paper is structured as follows: Sec. \ref{sec:related_work} provides an overview of related literature. The relevant theoretical background is introduced in Sec. \ref{sec:Theory}. Sec. \ref{sec:vqc} introduces variational quantum circutis. Sec. \ref{sec:gradient_estimation} explains how gradients can be estimated on quantum hardware. In Sec. \ref{sec:Optimizers} a brief overview of the different optimizers that are used in this paper is given. This is followed by a short high-level introduction to quantum error mitigation in Sec. \ref{sec:error_mitigation}. A detailed description of numerical experiments is given in Sec. \ref{sec:Methods} and subsequently the results thereof are presented in Sec. \ref{sec:Results}.

\section{Related Work}
\label{sec:related_work}
SGD with \gls{SPSA}-based gradients (which is just called \gls{SPSA} in the literature) has recently been compared against other gradient-free and gradient-based optimizers in various settings. The available literature suggests, that \gls{SPSA} is able to outperform other state-of-the-art gradient-based and gradient-free optimization methods in terms of final error, in a wide range of variational quantum computing tasks like finding energy spectra and ground state energies \cite{Sa2023, Bonet2021, Oliv2022, Mihalikova2022} or solving linear systems of equations \cite{Pellow-Jarman2021}, both in noiseless and noisy simulations.

%SPSA has recently been compared against other gradient-free and gradient-based optimizers in various settings. In \cite{Sa2023} the SPSA and COBYLA optimizers are used to solve for the spectral gap in a BCS hamiltonian. While SPSA yields better results on shallower circuits in the noiseless simulation, COBYLA outperforms SPSA for deeper circuits and when thermal relaxation noise is taken into consideration.

%A wide variety of optimizers, including SPSA, COBYLA, Powell's method, Adam, AMSGrad and BFGS are studied by \cite{Pellow-Jarman2021} in the setting of ideal and noisy simulation of a variational quantum linear solver. It is concluded that SPSA and Powell's method are the best performing gradient-free methods, while BFGS is the best gradient-based one. Between these methods however, there is no clear distinction in performance, even in the noisy setting. It must however be noted that the gradient-based optimization was done using analytical gradients and not with the parameter-shift rule.

%In \cite{Bonet2021} SPSA is shown to consistently outperform COBYLA, SLSQP and CMA-ES for finding the ground state of various hamiltonians under the influence of shot noise. However, after hyperparameter tuning CMA-ES reached a comparable performance. Similar results are obtained by \cite{Oliv2022} for finding the groundstate energy of the \(\mathrm{H}_2\) molecule with a modified version of SPSA and the NFT method scoring the best results. Additionally, the results from \cite{Mihalikova2022} indicate that SPSA outperforms COBYLA, Nelder-Mead and Powell's method in the noiseless setting. When gate- and measurement-errors are considered, SPSA and COBYLA give similar mean results, but the former offers less variance.

However, the picture is not always that clear. Singh et al. \cite{Singh2022} observed, that the \gls{SPSA} optimizer converged to significantly worse results than all other considered methods when evaluating the ground-state energy and dipole moments of different molecules on a noiseless simulator. While it is affected less by noise than most other optimizers, as can be seen from the improved relative performance to the other methods in the noisy simulation, it is still outperformed by Powell's method, even in the noisy setting.

The core idea of \gls{SPSA}, namely approximating gradients by simultaneous perturbations, has also been used to design new optimizers, that are tailored specifically to the training of variational quantum circuits.
For example, a quantum version of the natural gradient optimization scheme can be built from a second-order variant of \gls{SPSA} that approximates the quantum Fischer information matrix \cite{Gacon2021, Gidi2022}.
%For example \cite{Gacon2021, Gidi2022} use a second-order variant of SPSA to approximate the quantum Fischer information matrix to build a quantum version of the natural gradient optimization scheme.
In addition to that, while SPSA does not require the explicit computation of the loss functions gradient, internally it still computes an approximation to it. Therefore it is possible to use the approximated gradient from \gls{SPSA} with well-established gradient-based optimizers. This idea has been mentioned in \cite{Ebadi2022}, but no quantitative comparisons to standard SPSA or the parameter-shift rule are given. While \cite{Hoffmann2022} provides such a quantitative comparison, it only considers the \gls{SGD} optimizer.
