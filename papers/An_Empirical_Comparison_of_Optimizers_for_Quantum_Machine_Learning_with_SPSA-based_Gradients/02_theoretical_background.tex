\section{Theoretical Background}
\label{sec:Theory}
To give theoretical background, we start by introducing \glspl{VQC} and continue by presenting the parameter-shift rule and SPSA. Furthermore, we give a detailed list of the optimizers we examine in this work and close with a look at \gls{QEM}.

\subsection{Variational Quantum Circuits}
\label{sec:vqc}
A \gls{VQC} can be formally described as a sequence of parameterized gates $U(\bm{x}, \bm{\theta})$, that prepares a quantum state depending on the initial state $| \psi_0 \rangle$, input $\bm{x}$ and variational parameters $\bm{\theta}$. The output is obtained by computing the expectation value of an observable $A$ \cite{Schuld19}
\begin{equation}
\label{eq:expectation_value}
    f(\bm{x}, \bm{\theta}) := \langle \psi_0 | U^\dagger(\bm{x}, \bm{\theta}) A U(\bm{x}, \bm{\theta}) | \psi_0 \rangle .
\end{equation}
In the following, we will drop the dependency on the input $\bm{x}$ to facilitate the notation.
During the training of this circuit, we want to minimize a loss function $L(f(\bm{\theta}), y)$ with respect to some target values $y$ by tuning the circuit parameters $\bm{\theta}$. For example, this can be realized by iterative gradient descent steps:

\begin{equation}
\label{eq:gradient_step}
    \hat{\bm{\theta}}^{k+1} = \hat{\bm{\theta}}^k - \alpha^k \nabla L(\hat{\bm{\theta}}^k), %\overset{\tiny a)}{=} \hat{\bm{\theta}}^k - \alpha^k \nabla f(\hat{\bm{\theta}}^k)
\end{equation}
where $\hat{\bm{\theta}}^k$ denotes the updated parameter vector, $\alpha^k$ the learning rate and $\nabla L(\hat{\bm{\theta}}^k)$ the gradient at iteration $k$.  
%Without loss of generality we set $L(f(\bm{\theta}), y) = f(\bm{\theta})$ in a), where for more general cases the chain rule can be applied.

\subsection{Gradient Computation}
\label{sec:gradient_estimation}
In order to determine this gradient, we need to compute partial derivatives $\partial_{\theta_i} f(\bm{\theta})$ of the expectation in \cref{eq:expectation_value}, which arise due to the chain rule. Since the classical computational cost of computing the unitary $U$ increases exponentially for larger circuits, it is necessary to obtain gradients directly from the quantum hardware.
%This is where the parameter-shift rule is typically employed to yield estimates of these analytical partial derivatives. %from two estimations of $f(\bm{\theta})$ with shifted parameters. %analytical partial derivatives by computing $f(\bm{\theta})$ twice with shifted parameter $\theta_i$.

%Approaches like automatic gradient computation through backpropagation, as is common in Neural Networks, have no immediate analog in quantum computations, since measurements are irreversible.

\subsubsection{Parameter-shift rule}
%For the introduction of the parameter-shift rule it is sufficient to consider a single qubit parameterized gate $U_G(\theta)$ with a Hermitian generator $G$ with two distinct eigenvalues $e_0$ and $e_1$ and a real valued constant $a$ as:
\iffalse
Following the derivation of the partial derivative $\partial_{\theta_i} f(\bm{\theta})$ in \cite{Schuld19} and \cite{Crooks19}, we can decompose the unitary $U(\bm{\theta})$ into a product $U(\bm{\theta}) = V\mathcal{G}(\theta_i)W$ and regard all other parameters as constant: %, for which the product rule of calculus holds: %into the state $|\psi' \rangle = $ and following ones into the observable $A$.
\begin{equation}
\label{eq:partial_f}
    \partial_{\theta_i} f(\bm{\theta}) = \partial_{\theta_i} \langle \psi' | \mathcal{G}^\dagger(\theta_i) B \mathcal{G}({\theta_i}) | \psi' \rangle
\end{equation}
Where $B = V^\dagger A V$ and $| \psi' \rangle = W | \psi \rangle $.

If we only consider gates $\mathcal{G}(\theta_i)$ with a Hermitian generator $G$ that has two distinct eigenvalues $e_0$ and $e_1$ we can write:
\begin{equation}
    \mathcal{G}(\theta_i) = e^{-ia\theta_i G} = I \mathrm{cos}(r \theta_i) - i \frac{a}{r} G \mathrm{sin}(r \theta_i)
\end{equation}
with $r = \frac{a}{2}(e_1 - e_0)$ and \(I\) the identity operator. From this follows:
\begin{equation}
\label{eq:special_case}
    \mathcal{G}(\pm \frac{\pi}{4r}) = \frac{1}{\sqrt{2}}(I \mp i \frac{a}{r}G)
\end{equation}
We observe that:
\begin{equation}
\label{eq:partial_G}
    \partial_{\theta_i} \mathcal{G}(\theta_i) = -iaGe^{-ia\theta_iG}
\end{equation}
After substituting \eqref{eq:partial_G} and \eqref{eq:special_case} into \cref{eq:partial_f} and rearranging we get the parameter-shift rule:
\begin{equation}
\label{eq:param_shift}
    \partial_{\theta_i} f(\bm{\theta}) = r \left[ f(\bm{\theta} + \bm{s}) -  f(\bm{\theta} - \bm{s})\right]
\end{equation}
With the components of shift $\bm{s}$ being $s_i = \frac{\pi}{4r}$ and $s_j = 0 \; \forall \; j \neq i$.
%For a detailed derivation we refer to \cite{Schuld19}.
\fi

%If we only consider gates with a Hermitian generator that has two distinct eigenvalues $e_0$ and $e_1$ the parameter-shift rule states
The parameter-shift rule enables the computation of exact partial derivatives of \cref{eq:expectation_value} from two expectation values per parameter.
Following refs. \cite{Schuld19} and \cite{Crooks19}, in order to compute the partial derivative \(\partial_{\theta_i}f(\bm{\theta})\) with respect to a single parameter \(\theta_i\), we factor the circuit unitary $U(\bm{\theta}) = V\mathcal{G}(\theta_i)W$ into parts, which are constant w.r.t. \(\theta_i\), and a unitary \(\mathcal{G}(\theta_i)\), which actually depends on \(\theta_i\). Furthermore, we assume that $\mathcal{G}(\theta_i) = e^{-ia\theta_i G}$ with the hermitian generator $G$ having only two distinct eigenvalues $e_0$ and $e_1$. The parameter-shift rule then states: %pauli version schreiben und dann sagen, man kann andere gates entsprechend zerlegen. oder \cite{} showed that it can be generalized for a broader class of gates by decompsing the,...
\begin{equation}
\label{eq:param_shift}
    \partial_{\theta_i} f(\bm{\theta}) = r \left[ f(\bm{\theta} + \bm{s}) -  f(\bm{\theta} - \bm{s})\right],
\end{equation}
with $r = \frac{a}{2}(e_1 - e_0)$ and the components of shift $\bm{s}$ being $s_i = \frac{\pi}{4r}$ and $s_j = 0 \; \forall \; j \neq i$.

From this we conclude that $\partial_{\theta_i} f(\bm{\theta})$ can be computed directly through two expectation value estimations with shifted parameters. Based on this, the whole gradient can be computed by estimating $2p$ distinct expectation values, where $p = |\bm{\theta}|$ denotes the number of parameters in the trainable circuit.

On quantum hardware, expectation values are formed by averaging noisy measurement outputs over multiple circuit runs, called shots. The finite sampling statistics adversely impacts the computed gradient and therefore the whole training process. In order to get a reliable estimate, multiple shots are required per expectation, making this approach intractable even for medium-sized circuits.%In the context of optimization this corresponds to obtaining noisy samples of $L(f(\bm{\theta}), y)$ and calculating their sample mean. 
%method described in Sec. \ref{sec:spsa}, which estimates the gradient solely based on noisy samples of the loss function and randomly perturbing the parameter vector.

%If multiple gates depend on $\theta_i$ the derivative is obtained by shifting each gate separately and summing the results.
%The consideration of gates $\mathcal{G}(\theta_i)$ as described above suffices since a broad class of parameterized gates can be decomposed into products of standard gates for which the parameter-shift rule holds \cite{Crooks19}.
%Besides the similarity of equation \eqref{eq:param_shift} with the finite difference method from numerical optimization, the parameter-shift rule gives the analytical derivative of equation \eqref{eq:expectation_value}. 
% Gavin E. Crooks, Mitarai, Schuld
\subsubsection{Stochastic Perturbation Simultaneous Approximation}
\label{sec:spsa}
\gls{SPSA} is an algorithm designed for circumstances, where only such noisy samples of the loss function are available. It is a gradient-free stochastic optimization algorithm that follows the steepest descent direction on average \cite{Spall98}. %It is a gradient-free stochastic optimization algorithm that enables gradient approximation from noisy samples of the loss function, following the steepest descent direction on average \cite{Spall98}.

To achieve this, the gradient $\nabla L(\hat{\bm{\theta}}^k)$ in the parameter update step (cf. \cref{eq:gradient_step}) is replaced by a gradient approximation $\hat{\bm{g}}^k(\hat{\bm{\theta}}^k) \approx \nabla L(\hat{\bm{\theta}}^k)$, which is obtained by:
% we could remove ^-1 since we are drawing from +1, -1
\begin{equation*}
\label{eq:spsa}
    \hat{\bm{g}}^k(\hat{\bm{\theta}}^k) = \frac{L(\hat{\bm{\theta}}^k + c^k\bm{\Delta}^k) - L(\hat{\bm{\theta}}^k - c^k\bm{\Delta}^k)}{2c^k} \begin{bmatrix}
	(\Delta^k_1)^{-1} \\
	(\Delta^k_2)^{-1} \\
	\vdots \\
	(\Delta^k_p)^{-1}\\
	\end{bmatrix},
\end{equation*}
where $c^k$ denotes a small positive scaling factor and $\bm{\Delta}^k$ a random perturbation vector at iteration \(k\).
The entries $\Delta^k_i$ of the perturbation vector are drawn uniformly and independently from the set $\{-1,1\}$.

\iffalse \cite{Spall92} gives conditions on the convergence, depending on the distribution of the perturbation vector, learning rate $\alpha^k$, scaling factor $c^k$ and the statistical relation of perturbation vector and loss function samples. \fi

We observe that all parameters are perturbed at the same time. This means that the gradient approximation can be done by estimating exactly two expectation values, independent of the number of parameters. This can amount to a drastic reduction in training time through computational savings per parameter update step. However, these savings are only effective if they are not canceled out by an overall increase in updates required to reach convergence. Recent work by Kungurtsev et al.\ suggests, that the overall asymptotic iteration complexity of \gls{SPSA} is the same as of \gls{SGD} \cite{Kungurtsev2022}. They also observe similar convergence rates empirically. This means \gls{SPSA} can provide real, practical speedup in \glspl{VQA}.
%Crucially, recent work suggests, that the overall iteration complexity of \gls{SPSA} is the same as of \gls{SGD} \cite{Kungurtsev2022}, meaning \gls{SPSA} offers a practical speedup over gradient descent with the parameter-shift rule.
%Random search direction only following the direction of steepest descent in average.
%Perturbation vector is i.i.d. following a symmetric Bernoulli distribution where the entries are drawn uniformly from the set $\{-1,1\}$

\subsection{Optimizers}
\label{sec:Optimizers}
The way the parameter update is performed based on the estimated or approximated gradient is determined by an optimizer. A very basic optimizer is the gradient descent step from \cref{eq:gradient_step}. More sophisticated schemes do not only use the current gradient, but keep a record of gradients from previous iterations $H^k = \{\nabla L(\hat{\bm{\theta}}^s)\}^k_{s=0}$ \cite{Choi19}.
%In order to determine how the parameter update is performed based on the estimated or approximated gradient, more sophisticated schemes instead of the basic gradient descent step in \eqref{eq:gradient_step} can be used. 
%An alternative to the basic gradient update steps in \eqref{} are sophisticated schemes called optimizers.
%Optimizer are algorithms that determine how the parameter update is performed during minimization of the loss function $L(f(\bm{\theta}), y)$. Usually, this is based on a set of first-order derivatives or estimates thereof and the loss from current and previous iterations, $H^k = \{\hat{\bm{\theta}}^s, \nabla L(\hat{\bm{\theta}}^s), L(\hat{\bm{\theta}}^s)\}^k_{s=0}$ \cite{Choi19}.
%First order methods
Formally, they can be expressed as
\begin{equation}
    \hat{\bm{\theta}}^{k+1} = \mathcal{M}(H^k, \bm{\phi}^k),
\end{equation}
where the update rule $\mathcal{M}$ gives the updated parameter vector $\hat{\bm{\theta}}^{k+1}$ based on the record $H^k$ and optimizer hyperparameters $\bm{\phi}^k$ e.g. learning rate $\alpha^k$.

The optimizers we examine in combination with parameter-shift rule and SPSA are the following.
% How to cite, just overview paper or cite every original paper
\subsubsection{Stochastic Gradient Descent}
\gls{SGD} uses the same update rule given in \cref{eq:gradient_step}. The stochasticity enters when the gradient is computed over a subset of the training dataset (mini-batches), rather than the entire set. %$\{y_i\}^N_{i=0}$.

\subsubsection{SGD with momentum}
%There are different impelementations
To accelerate SGD a running mean estimate $\bm{\mu}^k$ of the gradient is included, which is updated via
\begin{equation*}
    \bm{\mu}^{k+1} = \gamma \bm{\mu}^k + \nabla L(\hat{\bm{\theta}}^k).
\end{equation*}
This mean is typically called velocity and is intended to accumulate persistent descent directions across parameter updates
\begin{equation}
    \hat{\bm{\theta}}^{k+1} = \hat{\bm{\theta}}^k - \alpha^k \bm{\mu}^{k+1},
\end{equation}
where the momentum $\gamma \in [0, 1]$ is a constant hyperparameter \cite{sutskever2013importance}.

\subsubsection{Adam}
Adaptive moment estimation (Adam) has an individual and adaptive learning rate per parameter, which is based on running estimates of first and second moments
\begin{align*}
    \bm{\mu}^{k+1} &= \beta_1 \bm{\mu}^k + (1 -\beta_1) \nabla L(\hat{\bm{\theta}}^k) \\
    \bm{\sigma}^{k+1} &= \beta_2 \bm{\sigma}^k + (1 -\beta_2) \nabla L(\hat{\bm{\theta}}^k) \odot \nabla L(\hat{\bm{\theta}}^k),
\end{align*}
of the gradient \cite{ADAM}. Here, $\bm{\mu}^k$ denotes the mean estimate and $\bm{\sigma}^k$ the uncentered variance based on the element-wise square $\nabla L(\hat{\bm{\theta}}^k) \odot \nabla L(\hat{\bm{\theta}}^k)$.

For a small number of iterations, these estimates are biased due to their initialisation to zero. To compensate this, correction factors are introduced
\begin{align*}
    \hat{\bm{\mu}}^{k+1} &= \frac{\bm{\mu}^{k+1}}{1 -(\beta_1)^{k+1}} \\
    \hat{\bm{\sigma}}^{k+1} &= \frac{\bm{\sigma}^{k+1}}{1 -(\beta_2)^{k+1}},
\end{align*}
where the decay rates $\beta_1, \beta_2 \in [0,1)$ are taken to the $(k+1)$-th power.
The resulting parameter update is then given by normalizing the mean by the standard deviation $\sqrt{\hat{\bm{\sigma}}^{k+1}}$ and subtracting this normalized mean from the current parameters
\begin{equation}
\label{eq:adam}
    \hat{\bm{\theta}}^{k+1} = \hat{\bm{\theta}}^k - \frac{\alpha \hat{\bm{\mu}}^{k+1}}{\sqrt{\hat{\bm{\sigma}}^{k+1}} + \epsilon}.
\end{equation}
Here, the step size $\alpha$ is a positive scaling factor.

Reddi et al. show that convergence is not generally guaranteed \cite{Reddi19}. Nonetheless, Adam is found to be effective in many practical scenarios.

\subsubsection{AMSGrad}
This optimizer corrects convergence problems of Adam by maintaing the maximum of the second moment over previous iterations
\begin{equation*}
    \hat{\bm{\sigma}}^{k+1}_{\mathrm{max}} = \max(\hat{\bm{\sigma}}^{k}_{\mathrm{max}}, \hat{\bm{\sigma}}^{k+1}),
\end{equation*}
and replacing $\hat{\bm{\sigma}}^{k+1}$ in \cref{eq:adam} with this maximum value \cite{Reddi19}
\begin{equation}
    \hat{\bm{\theta}}^{k+1} = \hat{\bm{\theta}}^k - \frac{\alpha \hat{\bm{\mu}}^{k+1}}{\sqrt{\hat{\bm{\sigma}}^{k+1}_{\mathrm{max}}} + \epsilon}.
\end{equation}

\subsubsection{RMSProp}
This modified version of SGD with momentum keeps a running estimate of the uncentered variance (mean square)
\begin{equation*}
    \bm{\sigma}^{k+1} = \rho \bm{\sigma}^k + (1 -\rho) \nabla L(\hat{\bm{\theta}}^k) \odot \nabla L(\hat{\bm{\theta}}^k),
\end{equation*}
and normalizes the gradient by the standard deviation (root mean square) \cite{tieleman2012lecture}
\begin{equation*}
    \bm{\mu}^{k+1} = \gamma \bm{\mu}^k + \frac{\alpha^k}{\sqrt{\bm{\sigma}^{k+1}} + \epsilon} \nabla L(\hat{\bm{\theta}}^k).
\end{equation*}
%\begin{equation}
%    \hat{\bm{\theta}}^{k+1} = \hat{\bm{\theta}}^k - \bm{\mu}^{k+1}.
%\end{equation}

\subsection{Quantum Error Mitigation}
\label{sec:error_mitigation}
A challenge with near-term quantum hardware is its susceptibility to noise. This noise introduces a bias into expectation values, such as in \cref{eq:expectation_value}. For ML models based on a VQC this bias should be eliminated as much as possible, since their accuracy depends directly on the estimated expectation. 

Methods that reduce noise impact are called Quantum Error Mitigation (QEM) \cite{Cai22}. In contrast to Quantum Error Correction (QEC) \cite{lidar2013qec}, which currently suffers from a large gate overhead, QEM performes post-processing that does not aim to reduce the effect of noise per shot, but over the whole ensemble of shots. Thus, QEM typically comes with a tradeoff, where for a reduced estimator bias the variance is increased \cite{Cai22}. This can be compensated by an increased number of samples, causing a sampling overhead compared to the ideal noise-free scenario.
In this paper, we employ zero-noise extrapolation \cite{temme2017error}, which uses data gathered from boosted noise levels to extrapolate expectation values to the zero-noise limit. It has already been successfully implemented in different variational algorithms \cite{kandala2019error, li2017efficient, dumitrescu2018cloud, kim2023scalable} both in the simulator and on real hardware.