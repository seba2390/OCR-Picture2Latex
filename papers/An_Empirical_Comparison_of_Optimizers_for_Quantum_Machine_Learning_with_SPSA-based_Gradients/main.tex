\pdfoutput=1
\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\PassOptionsToPackage{table, x11names, dvipsnames, svgnames}{xcolor}
\usepackage{cite}
\usepackage{bbm}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage[english]{babel}
\usepackage[acronym,nonumberlist]{glossaries}
\usepackage{glossaries-prefix}
\usepackage{pgfplots}
\pgfplotsset{compat=1.17}
\usepgfplotslibrary{fillbetween}
\usepackage{amssymb}
\usepackage{import}
\usepackage{bm}
\usepackage{siunitx}
% conflicts with IEEE format
% \usepackage{subcaption}
\usepackage{multirow}
\usepackage[]{yquant}
\usepackage{hyperref}
\hypersetup{%
%colorlinks=true, 
linktocpage=true, 
colorlinks=false,
pdfborder={0 0 0}, %pdfstartpage=3, pdfstartview=FitV,%
breaklinks=true, pdfpagemode=UseNone, pageanchor=true, pdfpagemode=UseOutlines,%
plainpages=false, bookmarksnumbered, bookmarksopen=true, bookmarksopenlevel=1,%
hypertexnames=true, pdfhighlight=/O,%hyperfootnotes=true,%nesting=true,%frenchlinks,%
pdftitle={An Empirical Comparison of Optimizers for
Quantum Machine Learning with SPSA-based
Gradients},%
pdfauthor={Maniraman Periyasam},%
pdfsubject={QML},%
pdfkeywords={},%
pdfcreator={pdfLaTeX},%
pdfproducer={LaTeX with hyperref}%
}

% for reviewers reply
%\usepackage{rebuttal}

% For inline enumerations
\usepackage[inline]{enumitem}

\usepackage[capitalise]{cleveref}

%\makenoidxglossaries
\newacronym[shortplural=GMMs]{GMM}{GMM}{Gaussian mixture model}
\newacronym[shortplural=HMMs]{HMM}{HMM}{hidden Markov model}
\newacronym[shortplural=DNNs]{DNN}{DNN}{deep neural network}
\newacronym[shortplural=SVDs]{SVD}{SVD}{singular value decomposition}
\newacronym[
    prefixfirst={a\ },% prefix used on first use
    prefix={an\ }% prefix used on subsequent use
]{MCTS}{MCTS}{Monte Carlo tree search}
\newacronym[prefixfirst={a\ },prefix={an\ }]{MDP}{MDP}{Markov decision process}
\newacronym{CMDP}{CMDP}{constrained Markov decision process}
\newacronym{RL}{RL}{reinforcement learning}
\newacronym[shortplural=DTs]{DT}{DT}{decision tree}
\newacronym{SMT}{SMT}{satisfiability modulo theories}
\newacronym{IL}{IL}{Imitation Learning}
\newacronym[shortplural=CNNs]{CNN}{CNN}{convolutional neural network}
\newacronym[shortplural=DQNs]{DQN}{DQN}{deep Q-network}
\newacronym{AI}{AI}{artificial intelligence}
\newacronym{PPO}{PPO}{proximal policy optimization}
\newacronym{ML}{ML}{machine learning}
\newacronym{QML}{QML}{quantum machine learning}
\newacronym{NISQ}{NISQ}{noisy intermediate scale quantum}
\newacronym{QC}{QC}{quantum circuit}
\newacronym{VQC}{VQC}{variational quantum circuit}
\newacronym{VQA}{VQA}{variational quantum algorithm}
\newacronym{MNIST}{MNIST}{modified national institute of standards and technology}
\newacronym{FIM}{FIM}{Fisher information matrix}
\newacronym{IDU}{IDU}{incremental data-uploading}
\newacronym{DRU}{DRU}{data re-uploading}
\newacronym{QRL}{QRL}{quantum reinforcement learning}
\newacronym{SPSA}{SPSA}{simultaneous perturbation stochastic approximation}
\newacronym{SGD}{SGD}{stochastic gradient descent}
\newacronym{QEM}{QEM}{quantum error mitigation}

%-- layout -----------------------------
\renewcommand{\floatpagefraction}{.9}    % default: .5
\renewcommand{\topfraction}{0.99}         % 90% of page top can be a float (Standard 0.7)
\setcounter{topnumber}{15}
\setcounter{dbltopnumber}{15}
\renewcommand{\bottomfraction}{0.99}      % 90% of page bottom can be a float (Standard 0.3)
\setcounter{bottomnumber}{15}
\setcounter{totalnumber}{99}
\renewcommand{\textfraction}{0.01}        % only 10% of page must to be text (Standard 0.2)
%
\clubpenalty=10000
\widowpenalty=10000
\linepenalty=10
\hyphenpenalty=50
\pretolerance=100
\tolerance=1000
\hfuzz=2pt
\vfuzz=1pt
\exhyphenpenalty=50
%\allowdisplaybreaks

% \usepackage{lineno,todonotes}
% \newcommand{\todoALL}[1]{\todo[color=orange!20,inline]{@ALL: #1}}
% \newcommand{\todoMani}[1]{\todo[color=blue!20,inline]{@Mani: #1}}
% \newcommand{\todoNico}[1]{\todo[color=red!20,inline]{@Nico: #1}}
% \newcommand{\todoAxel}[1]{\todo[color=green!20,inline]{@Axel: #1}}
% \newcommand{\todoChris}[1]{\todo[color=magenta!20]{@Chris: #1}}
% \newcommand{\todoChristian}[1]{\todo[color=cyan!20,inline]{@Christian: #1}}
% \newcommand{\todoALLIn}[1]{\todo[inline,color=orange!20]{@ALL: #1}}


\begin{document}

%\onecolumn
%\input{reviewReply}

%\setcounter{figure}{0}
%\setcounter{table}{0}
%\twocolumn


\title{An Empirical Comparison of Optimizers for Quantum Machine Learning with SPSA-based Gradients
%{\footnotesize \textsuperscript{*}Note: Sub-titles are not captured in Xplore and should not be used}
\thanks{

* These authors contributed equally (name order randomised). \\
The research is supported by the Bavarian Ministry of Economic Affairs, Regional Development and Energy with funds from the Hightech Agenda Bayern via the project BayQS.\\
email address for correspondence: 
maniraman.periyasamy@iis.fraunhofer.de}
}

\author{
%
\IEEEauthorblockN{Marco Wiedmann*, Marc HÃ¶lle*, Maniraman Periyasamy*, Nico Meyer, 
Christian Ufrecht, Daniel D.\ Scherer, \\ Axel Plinge, and Christopher Mutschler}
\IEEEauthorblockA{\textit{Fraunhofer IIS, Fraunhofer Institute for Integrated Circuits IIS},
Nuremberg, Germany \\\vspace{1mm}}
}



\maketitle

\begin{abstract}
\Glspl{VQA} have attracted a lot of attention from the quantum computing community for the last few years. Their hybrid quantum-classical nature with relatively shallow quantum circuits makes them a promising platform for demonstrating the capabilities of \gls{NISQ} devices. Although the classical machine learning community focuses on gradient-based parameter optimization, finding near-exact gradients for \glspl{VQC} with the parameter-shift rule introduces a large sampling overhead. Therefore, gradient-free optimizers have gained popularity in quantum machine learning circles. Among the most promising candidates is the \gls{SPSA} algorithm, due to its low computational cost and inherent noise resilience. We introduce a novel approach that uses the approximated gradient from \gls{SPSA} in combination with state-of-the-art gradient-based classical optimizers. We demonstrate numerically that this outperforms both standard \gls{SPSA} and the parameter-shift rule in terms of convergence rate and absolute error in simple regression tasks. The improvement of our novel approach over \gls{SPSA} with \gls{SGD} is even amplified when shot- and hardware-noise are taken into account. We also demonstrate that error mitigation does not significantly affect our results.
\end{abstract}

\begin{IEEEkeywords}
variational quantum computing, quantum error mitigation, SPSA, gradient free optimization, classical optimizers, quantum regression.
\end{IEEEkeywords}

\glsresetall
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%% Sections %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{01_introduction}
\input{02_theoretical_background}
\input{03_method}
\input{04_results}
%\input{04_theoretical_consideration}
\input{05_conclusion}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\let\oldthebibliography=\thebibliography
%\let\endoldthebibliography=\endthebibliography
%\renewenvironment{thebibliography}[1]{%
%\begin{oldthebibliography}{#1}%
%\setlength{\parskip}{1ex plus 1ex minus 0.5ex}%
%\setlength{\itemsep}{6pt}%
%\setstretch{1.0} % ZEILENABSTAND
%}%
%{%
%\end{oldthebibliography}%
%}
\newpage
\bibliographystyle{IEEEtran}
\bibliography{references}

\end{document}
