\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mwe}% http://ctan.org/pkg/mwe
\usepackage{adjustbox}
\usepackage{subcaption}
\usepackage{booktabs}
\usepackage[dvipsnames,svgnames,x11names]{xcolor}
\usepackage{cuted}
\usepackage{capt-of}
\usepackage{kotex}

% added
\usepackage{multirow}

\newcommand{\myparagraph}[1]{\vspace{2pt}\noindent{\bf #1}}

\def\authornote#1#2#3{{\textcolor{#2}{\textsl{\small[#1: #3]}}}}
\newcommand{\wonmin}[1]{\authornote{Wonmin}{Blue}{#1}} % Wonmin
\newcommand{\sang}[1]{\authornote{Sang}{Green}{#1}}
\newcommand{\spk}[1]{\authornote{spk}{red}{#1}}
% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}
\cvprfinalcopy

\DeclareMathOperator*{\argmin}{argmin}

% \cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{7227} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
% \ifcvprfinal\pagestyle{empty}\fi
\begin{document}
%%%%%%%%% TITLE
\title{Sound-Guided Semantic Image Manipulation}

 \author{
Seung Hyun Lee$^1$, Wonseok Roh$^1$, 
Wonmin Byeon$^4$, 
Sang Ho Yoon$^{3}$,
Chanyoung Kim$^1$,\\
  Jinkyu Kim$^{2*}$, and Sangpil Kim$^{1}$\thanks{Corresponding authors.}\\
  $^1$Department of Artificial Intelligence, Korea University\\
  $^2$Department of Computer Science and Engineering, Korea University\\
  $^3$Graduate School of Culture Technology, KAIST\\
  $^4$NVIDIA Research, NVIDIA Corporation\\
  %\texttt{\{doranee12, nahyuk0113, kochanha, petacwj\}@gmail.com}\\
  %\texttt{jinkyukim@korea.ac.kr, gsvsrasid@gmail.com, spk7@korea.ac.kr
}

\maketitle
% \twocolumn[{%
% \renewcommand\twocolumn[1][]{#1}%
% \maketitle
% \includegraphics[width=\textwidth]{figure1_submission.pdf}
% \caption{Our sound-guided image manipulation examples. Our model manipulates input images (top row) based on the user-provided sound inputs (e.g. fire crackling, siren, underwater bubbling, or wind noise), which produces sound-driven manipulated results (bottom row).
% }}]

% \begin{figure*}[t]
%   \centering
%   \includegraphics[width=\textwidth]{figure1_submission.pdf}
%   \caption{Our sound-guided image manipulation examples. Our model manipulates input images (top row) based on the user-provided sound inputs (e.g. fire crackling, siren, underwater bubbling, or wind noise), which produces sound-driven manipulated results (bottom row).% More diverse examples are provided as supplemental materials.
%   }
%   \label{fig:fig1}
% \end{figure*}
%\thispagestyle{empty}
\begin{strip}\centering
\vspace{-1.59cm}
\includegraphics[width=\textwidth]{figure1_submission.pdf}
\captionof{figure}
{
Modified images with sound-guided semantic image manipulation.
Our method manipulates source images~(top row) given user-provided sound~(middle row) into semantic images~(last row).
}\label{fig:fig1}\vspace{-0.5em}
\end{strip}
% and produces sound-guided manipulated results (bottom row).

\begin{abstract}
%\vspace{-0.485cm}
\vspace{-1em}
The recent success of the generative model shows that leveraging the multi-modal embedding space can manipulate an image using text information. However, manipulating an image with other sources rather than text, such as sound, is not easy due to the dynamic characteristics of the sources. Especially, sound can convey vivid emotions and dynamic expressions of the real world. Here, we propose a framework that directly encodes sound into the multi-modal~(image-text) embedding space and manipulates an image from the space. Our audio encoder is trained to produce a latent representation from an audio input, which is forced to be aligned with image and text representations in the multi-modal embedding space. We use a direct latent optimization method based on aligned embeddings for sound-guided image manipulation.
\iffalse
We also show that our method can mix text and audio modalities, which enrich the variety of the image modification.
We verify the effectiveness of our sound-guided image manipulation quantitatively and qualitatively.  
\fi
We also show that our method can mix different modalities, i.e., text and audio, which enrich the variety of the image modification. The experiments on zero-shot audio classification and semantic-level image classification show that our proposed model outperforms other text and sound-guided state-of-the-art methods. 
%We further show several applications that verify the effectiveness of our sound-guided image manipulation.

%Furthermore, we perform zero-shot audio classification and outperformed the other state-of-the-art methods.
%that proposed sound-guided image manipulation approach can produce semantically meaningful images.
\end{abstract}
%\vspace{-0.4em}
% \begin{figure}[t!]
%   \centering
%   \includegraphics[width=\linewidth]{examples_v3.pdf}
%   \caption{An overview of our proposed approach, the Sound-Guided Image Manipulation step. We use a direct code optimization approach. Here, a source latent code is modified in response to a user-provided audio input.}
%   \label{fig:fig2}
% \end{figure}


% \begin{figure*}
% \begin{center}
% \includegraphics[width=\textwidth]{figure1_submission.pdf}
% \end{center}
%   \caption{Our sound-guided image manipulation examples. Our model manipulates input images (top row) based on the user-provided sound inputs (e.g., fire crackling, siren, underwater bubbling, or wind noise) and produces sound-driven manipulated results (bottom row).}% More diverse examples are provided as supplemental materials.}
% \label{fig:fig1}
% \end{figure*}


%%%%%%%%% BODY TEXT
\section{Introduction}
\vspace{-0.5em}
% \wonmin{The first paragraph is somewhat obvious and user-controlled manipulation is too general as there are already many ML-based image manipulation techniques these days. What about structuring the intro. this way?: 1) introducing image manipulation/image-image style transfer 2) introducing text-based image manipulation 3)challenges of text-based image manipulation 4) other modality/multi-modal image manipulation 5) challenges/issues of these previous works 6) Introducing our work 7) Compare this work with previous ones --- why audio guidance is better than previous ones? does this address the issues we mentioned above?. Additionally, it would be good to add 1) why audio-driven image manipulation is difficult 2) how this paper solves it. }

% Introducing image manipulation/image-image style transfer  & Introducing text-based image manipulation



 % Other modality/multi-modal manipulation
Image editing has been widely studied in the field of computer vision due to 
its usefulness in photo-realistic editing applications, social media image sharing, and image-based advertisement.
An image can be used to transfer its style into the target image~\cite{gatys2016image, gatys2016neural}.
Also, modifying specific parts in the human face image, such as hairstyle or color, is useful in image editing applications~\cite{xia2021tedigan,Patashnik_2021_ICCV}.
The purpose of semantic image manipulation is to generate a novel image that contains both source image identification and semantic information of user intention. 
In this paper, we tackle the semantic image manipulation task, which is the task of modifying an image with user-provided semantic cues.
To apply the user intention into the image, a mixture of sketches and text is used to perform image manipulation and synthesis~\cite{park2019semantic,xia2021tedigan}.
User intention can be applied by drawing a paint~\cite{park2019semantic} or writing text with semantic meanings~\cite{xia2021tedigan,gatys2016neural}.
% \wonmin{'clue' is an ambiguous word. I see that you use 'clue' a lot in next 2 paragraphs. It's better to be more specific, e.g., multi-model/extra inputs such as xxx} in text with multi-modal synthesis. 
%GauGAN~\cite{park2019semantic} performs successful image manipulation with mask information according to the class.
\iffalse 
TediGAN~\cite{xia2021tedigan} also gives extra-inputs to text-based image manipulation by embedding other modalities, such as mask and sketch, in the same space as text. More specifically, TediGAN projects the multi-modal embedding of the mask onto $\mathcal{W}$ Space of StyleGAN. 
Multi-modal image manipulation is performed by style mixing latent code obtained from content and text-driven latent code. However, extra inputs mainly support refining shape or boundary of objects in the image, and such objects are often predefined.
Also, non of these works have use sound directly to edit the image.
\fi

Text-based image manipulation methods are proposed to edit the image conditionally~\cite{el2019tell, jiang2021language, li2020manigan, nam2018tagan,xia2021tedigan}.
%Manipulating an image with text produces results containing desired visual properties.
These works modify target contents in the image based on the text information.
%remaining other unrelated parts of the original image. %preserves the unrelated parts of the original image. 
Among the text-based image manipulation methods, StyleCLIP~\cite{Patashnik_2021_ICCV} considered leveraging the representational power of Contrastive Language-Image Pre-training (CLIP)~\cite{radford2learning} models to produce text-relevant manipulations with given text input. StyleCLIP maintains high quality image generation ability using StyleGAN~\cite{jeong2021tr} while allowing insertion of semantic text into the image. 
% \wonmin{StyleCLIP maintains high quality image generation ability using StyleGAN while ....}

 % Semantic image manipulation requires modifying the image appearance in response to a user-controlled way, while preserving contextual information as well as the realism of the result. 
 % Producing semantically meaningful images are further challenging as it involves laborious manual human examination for each desired manipulation. Recently, several approaches have been proposed for manipulating images through guidance of diverse styles \wonmin{'guidance of diverse styles' is ambiguous.}~\cite{dong2017semantic, nam2018text, li2020manigan, patashnik2021styleclip, xia2021tedigan}.
 
% Introducing text-based image manipulation
 % Manipulating a given image with text produces results that contain the desired visual properties. The manipulated result only changes certain parts of the image and preserves the unrelated textual content of the original image~\cite{xia2021tedigan}. For instance, a text-driven image manipulation method called StyleCLIP~\cite{patashnik2021styleclip} considered leveraging the representational power of Contrastive Language-Image Pre-training (CLIP)~\cite{radford2learning} models to produce text-relevant manipulations given text input. StyleCLIP uses a generator of StyleGAN, and it maintains high quality image generation ability while inserting semantic text into the image. % An image defines static information obtained from a specific point in the scene. The trained CLIP representation between image and text manipulates static information such as a person's age, gender, and hairstyle well in StyleCLIP.


% Challaenges of text-based image manipulation
However, text-based image manipulation has an inherent limitation when applying sound semantics into the image, 
due to the lack of handling vivid sound, which has infinite variation.
%Text can only represent static semantic information where dynamics of context is absent. 
Since the text is the form of discrete character, expressing the spectrum that has a continuous and dynamic context of sound is extremely difficult in our world. 
% \wonmin{'slightly' is an ambiguous term. What about 'Text only provides static semantic information. It does not include dynamics/change of contexts'.} 
For example, every ``thunder'' generates different loudness and characteristic of ``\textit{sound of thunder}''.
The discreetness of a text message prevent expressing the detailed difference of the sound around us.
%but only single context could be represented with text such as ``\textit{weak wind, strong wind}''. 
Therefore, the text-based image manipulation model has limitations in transferring specific, vivid sound semantics into the source image for the image modification. 
%of learns to transfer only a single concept to the image. 
% \wonmin{It's unclear from this paragraph why we need to capture 'continuous change' to manipulate images since you don't do manipulation in time. Make this point clear. }
% In the wild environment, countless images are matched with one text keyword, making it challenging to provide desirable guidance for text-based image manipulation.
% Furthermore, the hierarchical structure of text semantic allows image guidance to focus on the top-level concept even if there are multiple user-provided text keywords. 
% \wonmin{This paragraph is the most important motivation in this paper, but the current wording is a bit abstract (like 'text is insufficient as a clue' 'top-level concept'). Let's be more specific about the problem. Something like this (rephrase it based on your need): However, text-based image manipulation is still challenging. A single word or phrase often includes multiple concepts or meanings in real-world scenarios such as xxx, but text can encode only a single concept. Therefore, the text-based image manipulation model learns to transfer only a single concept to the image. }
%
% \wonmin{The first sentence also says there are extra inputs in addition to text. The next sentences should be how TediGAN and GauGAN incorporate these inputs to image manipulation. If 'how' is not so important, just remove the sentences. They don't provide any new information.}
% Challenges/issues of these previous works
 % However, this mask can only give shape information to image manipulation, and the conditions of the mask for generation are also limited.
 % \wonmin{'However, these extra inputs mainly help to refine shape or boundary of objects in the image, and such objects are often predefined.' Does it make sense?}
 % Human perception is multi-dimensional, such as sight, hearing, touch, taste, and smell. The trend from single-modality learning to multi-modal learning, along with the active development of artificial intelligence technology in recent years, is essential role in better machine recognition~\cite{zhu2021deep}. Among them, 
%
 % \sang{Bit unclear about next 3 sentences' connection}
 % 
 % \sang{what is context connecting these two sentences}
%The unique rich labels of multiple sound segments, along with specific contexts, reinforce image manipulation guidance.
% \sang{why temporal context suddenly pops up?}
%The acoustic characteristics of an audio signal allow us to know the context of the events that occurred in the scene.

Sound provides polyphonic information of the scene and contains multiple sound events~\cite{9524590}.
That is why watching a movie with sound is more realistic than reading a book.
Our daily environment is filled with diverse sound sources and a complex blend of audio signals~\cite{9524590}.
Therefore, sound, which we focus on, is a necessary modality for image manipulation.
% \wonmin{This sentence is redundant. Instead, you can add one sentence about how these diverse sound sources and complex signals help with image manipulation.}
 
 Several studies~\cite{chen2017deep, hao2018cmcgan, oh2019speech2face, qiu2018image, wan2019towards, zhu2021deep} have attempted to visualize the meaning of sound, but it is still challenging to reflect sound events in high-resolution images due to two reasons. 
 % \wonmin{Extend what 'sound events' means.} 
 The first reason is the lack of a suitable high-resolution audio-visual dataset. Audio-visual benchmark video datasets~\cite{caba2015activitynet, kay2017kinetics, soomro2012ucf101} for GAN training has generally lower resolution than high-resolution image datasets including Flickr-Faces-HQ (FFHQ)~\cite{karras2019style} and The Large-scale Scene Understanding Challenge (LSUN)~\cite{yu2015lsun}. There is no dataset with as many audio-visual pairs as the number of image-text pairs used for CLIP training. CLIP uses 400 million image-text pair data to learn the relationship between very large and diverse image and text modalities, whereas audio-visual pair data is still insufficient. Secondly, it is difficult to discover potential correlations between auditory and visual modalities~\cite{zhu2021deep}. 
 % \sang{simply state different characteristics. bit ambiguous what different characteristics are from the reference}
Extracting appropriate temporal context, tone, and theme from the sound is difficult.
 % \wonmin{Second, as auditory and visual modalities have different characteristics, it is difficult to discover potential correlations between them~\cite{zhu2021deep}. Also, extracting the appropriate temporal context, tone, and theme from the sound is difficult.}
 % \wonmin{Another difficulty could be extracting the temporal context, tone, theme, etc from sound.}
% To overcome those problems, we exploit CLIP's prior knowledge for sound-based image application to obtain a rich representation of audio from the audio-text and the audio-visual data.
 
 
% Introducing our work 
To overcome these challenges of manipulating images with sound semantics, we introduce a novel image manipulation method driven by sound semantics~(see Fig.~\ref{fig:contrastivelearning}). As shown in Fig.~\ref{fig:fig1}, an image of an old car is manipulated into an old car with a fire truck-like exterior appearance when adding a siren sound. 
Our model consists of two main stages: (i) the CLIP-based Multi-modal Representation Learning, where an audio encoder is trained to produce a latent representation aligned with textual and visual semantics by leveraging the representation power of pre-trained CLIP models.
% \sang{is the last part "by leveraging the representation power of pre-trained CLIP models" essential? Is it better to highlight this or not? Sentence becomes too long with multiple messages. Ignore this comment if this part has to be  highlighted} 
(ii) the Sound-Guided Image Manipulation, where we use the direct latent code optimization to produce a semantically meaningful image in response to a user-provided sound.

% Compare this work with previous ones 

% Our method maintains the high-quality generation quality of StyleGAN and manipulates the image according to the meaning of sound. 
Our experimental results show that the proposed method supports a variety of sound sources with a better reflection of given audio information when transferring image styles. The sound-based approach supports more diverse and detailed information related to scenes compared to text-based image manipulation methods. 

% In this step, we obtain audio embeddings that exploit CLIP's prior knowledge through contrastive learning on the cross-modality combination of augmented audio and text. As a result, the audio modality shares a new embedding space of the image and text domains. We perform image manipulation by optimizing the latent code so that the CLIP embedding of the image generated from the StyleGAN latent code is close to embedding the user-provided input audio in the same embedding space. 

% With our proposed adaptive layer masking, the constraint for direct latent optimization in the source latent code moves the latent code by a different magnitude for each style layer. 
% Finally, the perspective of style-mixing text and audio, which are two different modals, is introduced. To mix the different styles of the two modals, a specific layer is selected from each latent code guided by the dynamic characteristics of the audio domain and the color and texture of the text domain.

% To the best of our knowledge, our work is the first to explore generating semantically meaningful image manipulations from various sound sources.

% Compare this work with previous ones 
% Our method maintains the high-quality generation quality of StyleGAN and manipulates the image according to the meaning of sound. Our study proves that our method includes the semantic of sound in image style better than the existing methods that transform the image style with audio information in LSUN and FFHQ datasets. We showcase more diverse examples in the Supplemental Figure 1.

Our main contributions are listed as follows: 
\begin{itemize}
    \vspace{-0.7em}
    \item We propose multi-modal contrastive losses to expand the CLIP-based embedding space. Moreover, we introduce contrastive learning on augmented audio data, which helps to learn a more robust representation. Here, we achieve state-of-the-art performance for a zero-shot audio classification task.\vspace{-0.7em}
    % \wonmin{Expand this more about what sound-alignment process is and what you propose in this paper.} 
    \item We propose semantic-level image manipulation solely based on the given audio features, including temporal context, tone, and volume.\vspace{-0.7em}%Features that only audio has, such as temporal context, tone, and volume, are shown in image manipulation.
    \item We propose the sound-guided code optimization steps with adaptive layer masking for putting sound meaning into images, enhancing the realism of the output.\vspace{-0.7em}
    % \wonmin{What is 'dynamic image manipulation'? Is it about adaptive masking? or sound guided code optimization? Mention what it is explicitly.} 
    % using style-mixing latent codes guided by multi-modal data~(audio and text).
    % We propose the sound-guided code
\end{itemize}

\begin{figure*}
    \begin{center}
        \includegraphics[width=\textwidth]{main_figure1.pdf}
    \end{center}
    \vspace{-1.3em}
    \caption{Our model consists of two main steps: (a) the {\em CLIP-based Contrastive Latent Representation Learning} step and (b) the {\em Sound-Guided Image Manipulation} step. In (a), we train a set of encoders with three different modalities~(audio, text, and image) to produce the matched latent representations. The latent representations for a positive triplet pair~(e.g., audio input: ``Explosion'', text: ``explosion'', and corresponding image) are mapped close together, while that of negative pair samples further away in the~(CLIP-based) embedding space~(left).  In (b), we use a direct code optimization approach where a source latent code is modified in response to user-provided audio, producing a sound-guided image manipulation result~(right).}
    \label{fig:contrastivelearning}
    \vspace{-1.3em}
\end{figure*}
%An overview of our proposed approach, which consists of two main steps: (a)~the CLIP-based Contrastive Latent Representation Learning step and (b) the Sound-Guided Image Manipulation step. In (a), we train a set of encoders with three different modalities (i.e. audio, text, and image) to produce the matched latent representations. Exemplary representations for a triplet pair (audio input: ``Explosion'', text input: ``explosion'', and corresponding image) are pulled together in the embedding~(CLIP) space~(left). In (b), we use a direct code optimization approach. Here, a source latent code is modified in response to a user-provided audio input~(right).


% \begin{figure}[t!]
%   \centering
%   \includegraphics[width=\linewidth]{main_figure1.pdf}
%   \caption{An overview of our proposed approach, which consists of two main steps: (a)~the CLIP-based Contrastive Latent Representation Learning step and (b) the Sound-Guided Image Manipulation step. In (a), we train a set of encoders with different modalities (i.e. audio, text and image) to produce the matched latent representations. Exemplary representations for a triplet pair (audio input: “Explosion”, text input: "explosion", and corresponding image) are pulled together in the embedding~(CLIP) space~(top). In (b), we use a direct code optimization approach. Here, a source latent code is modified in response to a user-provided audio input~(bottom).}
%   \label{fig:contrastivelearning}
% \end{figure}

\section{Related Work}

\myparagraph{Text-guided Image Manipulation.} Text-guided image manipulation is the most widely studied among guidance based tasks.
% SISGAN~\cite{dong2017semantic} employed the GAN-based encoder-decoder structure to preserve the features of the image while presenting image manipulations corresponding to the text description. 
% TAGAN~\cite{nam2018text} classify attributes independently and provide feedback to the generator with several local text-adaptive discriminators. ManiGAN~\cite{li2020manigan} proposes a multi-stage network for cross-modality representations of text and image through a text-image affine combination module. 
Several studies~\cite{dong2017semantic, li2020manigan, nam2018tagan} employed the GAN-based encoder-decoder structure to preserve the features of the image while presenting image manipulations corresponding to the text description. 
StyleCLIP~\cite{Patashnik_2021_ICCV} and TediGAN~\cite{xia2021tedigan} utilize the latent space of the pre-trained StyleGAN and the prior knowledge from CLIP~\cite{radford2learning}. StyleCLIP performed image manipulation using a user-provided text prompt. TediGAN enabled image generation and manipulation using GAN inversion technique using multi-modal mapping. Beyond text and images, a sound can express a complex context appearing in a scene, and there is correspondence between a sound and an event occurring in the scene.

\myparagraph{Sound-guided Image Manipulation.} Sound contains temporal dynamic information of a scene, which can be used as an imagery source for image manipulation. 
Some approaches have been introduced for the sound-guided image manipulation task.
However, the previous works mainly focus on music (instead of using sound semantics), which includes music-to-visual style transfer with cross-modal learning strategy~\cite{lee2020crossing} and a neural music visualizer by mapping music embeddings to visual embeddings from StyleGAN~\cite{jeong2021tr}. To manipulate the image according to the sound, % \textit{Audio-reactive StyleGAN}~\cite{brouwer2020audio} controls the latent space of StyleGAN with digital filtering and Nsynth~\cite{oord2016wavenet} of audio and 
\textit{Tr$\ddot{a}$umerAI}~\cite{jeong2021tr} visually expresses music by latent transfer mapping of music to StyleGAN's style embedding. 

However, the above studies have limitations in focusing only on the reaction, not the semantic of the sound, in the direction of navigation in the latent space of StyleGAN. \textit{Crossing you in style}~\cite{lee2020crossing} uses the period to define the semantic relationship between sound and visual domain, but there is still a limitation which only can transfer the image style.
%in that it is hard to generate semantically meaningful high-resolution images.
Our proposed method can isolate the modification area in the source image, such as modifying the emotion of the face while preserving the color of the hair.
%Instead of using music, we focus on sound, which convey rich semantic information. 

% Moreover, we leverage the representation power of existing CLIP models to train our audio encoder, which gives benefit of producing semantically meaningful results.  

\myparagraph{Interpreting Latent Space in StyleGAN.} The intermediate latent space in pre-trained StyleGAN~\cite{karras2019style} solves the disentanglement issue and allows the generated images to be manipulated meaningfully according to changes in the latent space. 
% GANSpace~\cite{harkonen2020ganspace} and StyleSpace~\cite{wu2021stylespace} allow image manipulation with interpretable controls from a pre-trained GAN generator.
Extended latent space $\mathcal{W}+$ allows image manipulation with interpretable controls from a pre-trained GAN generator~\cite{abdal2019image2stylegan, karras2019style, karras2020analyzing}. % A different intermediate latent vector w is fed to each of the generator’s layers.
% However, these works have not invested on the audio sequences. 
For latent space analysis in audio sequences, \textit{Audio-reactive StyleGAN}~\cite{brouwer2020audio} generates an image every time step by calculating the magnitude of the audio signal and moving it in the latent space of StyleGAN. However, the method cannot control the meaning of sound in the latent space. StyleGAN's motion in the latent space is only mapped to the magnitude of the sound. There is a novelty in that we manipulate images with the properties of sound. % Our approach controls StyleGAN2's generator given an input audio to perform an sound-guided semantic image manipulation.

\myparagraph{Audio-visual Representation Learning.} Cross-modal representation learning obtains relationships between different modalities in audio-visual tasks such as video retrieval and text-image cross-modal tasks such as image captioning and visual question answering. Audio-visual representation learning studies~\cite{DBLP:journals/corr/AytarVT17,nagrani2018learnable, suris2018cross} aim to map both modalities to the same embedding space. The correlation between modalities is learned by contrastive learning between composite audio-visual pairs~\cite{chen2021distilling, mazumder2021avgzslnet, sun2020learning}. 

However, audio-visual representation learning is still challenging because there is no adequate data as much as CLIP~\cite{radford2learning} for learning the correlation between different modalities.
CLIP learned the relationship between image and text embedding by multi-modal self-supervised learning of 400 million image-text pairs and showed zero-shot inference performance comparable to supervised learning in most image-text benchmark datasets.  

In this paper, the audio encoder not only exploits the representation ability of CLIP but also learns supervisory signals from the audio data itself through self-supervised manners. As a result, our method obtains an audio-specific representation for sound-guided image manipulation.

% \begin{figure}[t]
%   \centering
%   \includegraphics[width=0.8\linewidth]{cvpr_figure3.pdf}
%   \caption{The CLIP-based Contrastive Latent Representation Learning step. Exemplary representations for a triplet pair (audio input: “Giggling”, text input: "people giggling", and corresponding image) are pulled together in the embedding~(CLIP).}
%   \label{fig:fig2}\vspace{-1em}
% \end{figure}
% \begin{figure}[t]
%   \centering
%   \includegraphics[width=0.85\linewidth]{turk1.png}
%   \caption{Comparison of text-driven and audio-driven manipulation in Amazon Mechanical Turk.}
%   \label{fig:turkfig}
% \end{figure}
% \begin{figure}
%      \centering
%      \begin{subfigure}[b]{0.3\linewidth}
%          \centering
%          \includegraphics[width=\linewidth]{turk1.png}
%          \caption{Section1}
%          \label{fig:section1}
%      \end{subfigure}
%      \hfill
%      \begin{subfigure}[b]{0.3\linewidth}
%          \centering
%          \includegraphics[width=\linewidth]{turk1.png}
%          \caption{Section2}
%          \label{fig:section2}
%      \end{subfigure}
%      \hfillhttps://www.overleaf.com/project/615059f9f3e740cb3e2fb641
%      \centering
%      \begin{subfigure}[b]{0.3\linewidth}
%          \centering
%          \includegraphics[width=\linewidth]{turk1.png}
%          \caption{Section3}
%          \label{fig:section3}
%      \end{subfigure}
%      \hfill
%      \caption{Comparison of text-driven and audio-driven manipulation in Amazon Mechanical Turk.}
%         \label{fig:turkfig}
% \end{figure}

\section{Method}
% \wonmin{'which first trains..' is a description of our method right? It sounds like styleCLIP has audio encoder.}
% \wonmin{If overlap between styleCLIP and the proposed one is big enough, it would be better to add one paragraph explaining styleCLIP and use the subsections to explain the new parts.}
We follow the existing text-guided image manipulation model, StyleCLIP~\cite{Patashnik_2021_ICCV}. Our model and StyleCLIP manipulate the latent code of StyleGAN using joint embedding space between modalities. However, our model extends the CLIP~\cite{radford2learning} embedding space to the audio embedding space, which was not embedded before. We also introduce novel contrastive losses and adaptive masking for sound-guided image manipulation.
% \wonmin{However, our model extends the CLIP embedding space to audio which... (explain why this is good).} \wonmin{You introduce new contractive losses and adaptive masking for sound-guided manipulation right? You can briefly mention these  here. Something like: We also introduce .... } 
Our model consists of two main steps: (i)~the CLIP-based Multi-modal Latent Representation Learning and (ii)~the Sound-guided Image Manipulation. First, we train audio, text, and image encoders to generate new latent representations. To do so, we train the audio encoder using the InfoNCE loss~\cite{oord2018representation, alayrac2020self, zhang2020contrastive} to produce a latent representation that is aligned with the representations from the pre-trained CLIP's text and image encoders. Such aligned representations can be used for image manipulation with the provided audio input. After the pre-training step, we use encoders to manipulate images according to a target sound input~(e.g., images with different facial expressions can be manipulated with different sound inputs).

\subsection{Multi-modal Latent Representation Learning}
As shown in Fig.~\ref{fig:contrastivelearning}~(a), we train a set of encoders with three different modalities \{audio, text, and image\} to produce the matched representations in the embedding space. Specifically, given audio, text, and image inputs, i.e. $x_a$, $x_t$, and $x_v$, we use three different encoders to obtain a set of $d$-dimensional latent representations, i.e. $\bf{a}$, $\bf{t}$, and ${\bf{v}}\in\mathcal{R}^{d}$, respectively. These latent representations are learned via a typical contrastive learning approach following the work by Radford~\etal~\cite{radford2learning} -- the latent representations for a positive triplet pair are mapped close together in the embedding space, while that of negative pair samples further away. Learning such a joint representation from scratch is, however, generally challenging due to the lack of multi-modal datasets, which can provide positive and negative pairs. Thus, we instead leverage the pre-trained CLIP model, which optimized a visual-textual joint representation by contrastive learning. Then, we train an audio encoder to produce an aligned representation by using contrastive learning. Details are explained in the next section. Note that we obtain a latent representation $\hat{{\bf{a}}}\in\mathcal{R}^{d}$ from an augmented audio input $\hat{x}_a$, which is shown useful to improve the quality of the latent representation as this is a common practice in the self-supervised representation learning. %We discuss this in Section 3.1. % -- Audio Self-supervised learning.

% v1 %
%Given audio, text and image inputs, $x_a, x_t$, and $x_v$, we use separate encoders to obtain the $d$-dimensional normalized embeddings, $\bf{a}$, $\bf{t}$, and $\bf{v}$, respectively. 
%Additionally, augmented audio input $\hat{x}_a$ is also used for multi-modal contrastive learning.
%Also, we use augmented audio input $\hat{x}_a$ to make the same audio samples similar and different audio samples dissimilar via contrastive learning between the original and the augmented audio. $\hat{x}_a$ passes the audio encoder and produces the augmented audio embedding $\bf{\hat{a}}$.
% \wonmin{via contrastive learning between the original and the augmented audio.}. 
% \wonmin{$\hat{x}_a$ also passes the audio encoder and produces the augmented audio embedding $\hat{a}$. }
% \wonmin{Let's add some description about the inputs/outputs of encoder: "Given image, text, and audio inputs, $x_v, x_v$, and $x_a$, we use separate encoders to obtain the $d$-dimensional embeddings, $v$, $t$, and $a$, respectively. Additionally, augmented audio input $x^{hat}_a$ is also used for ..... "}


% The higher the cosine similarity of the vectors of the normalized embedding between each modality, the higher the semantic similarity between the encoder inputs. \wonmin{This sentence doesn't fit in this paragraph. }

% We get all same $d$-dimensional embeddings from image, text and audio encoder. \wonmin{redundant} 

%\myparagraph{Multi-modal Contrastive Loss.}
\myparagraph{Matching Multi-modal Representations via Contrastive Loss.}
We use the InfoNCE loss~\cite{alayrac2020self} to map positive audio-text pairs close together in the CLIP-based joint embedding space, while negative pairs further away. Formally, given a minibatch of $N$ audio-image representation pairs $\{{\bf{a}}_i, {\bf{t}}_j\}$ for $i\in\{1, 2, \dots, N\}$, we first compute the following audio-to-text loss function for the $i$-th pair:
\begin{equation}
    l_{i}^{(a\rightarrow t)}=-\text{log}\cfrac{\exp(\langle{\bf{a}}_i, {\bf{t}}_j\rangle/\tau) }{\sum_{\textnormal{j=1}}^N\exp(\langle{\bf{a}}_i, {\bf{t}}_j\rangle/\tau)}
    \label{loss:mini_con}
\end{equation}
where $\langle{\bf{a}}_i, {\bf{t}}_j\rangle$ represents the cosine similarity, i.e. $\langle{\bf{a}}_i, {\bf{t}}_j\rangle = {\bf{a}}_i^\intercal{\bf{t}}_j/\|{\bf{a}}_i\|\|{\bf{t}}_j\|$ and $\tau$ is a temperature parameter. This loss function is the log loss of an $N$-way classifier that wants to predict $\{{\bf{a}}_i, {\bf{t}}_j\}$ as the true representation pair. As the loss function is asymmetric, we define the following similar text-to-audio contrastive loss:
\begin{equation}
    l_{i}^{(t\rightarrow a)}=-\text{log}\cfrac{\exp(\langle{\bf{t}}_i, {\bf{a}}_j\rangle/\tau) }{\sum_{\textnormal{j=1}}^N\exp(\langle{\bf{t}}_i, {\bf{a}}_j\rangle/\tau)}
    \label{loss:mini_con2}
\end{equation}
Concretely, we minimize the following loss function $\mathcal{L}_{\textnormal{nce}}$ as a sum of the two losses $l_{i}^{(a\rightarrow t)}$ and $l_{i}^{(t\rightarrow a)}$ for all positive audio-text representation pairs in each minibatch of size $N$:
\begin{equation}
    \begin{aligned}
    \mathcal{L}_{\textnormal{nce}}^{(a \leftrightarrow t)}=\cfrac{1}{N}\sum_{i=1}^N (l_{i}^{(a\rightarrow t)} + l_{i}^{(t\rightarrow a)})
    \end{aligned}
    \label{loss:nce}
\end{equation}

% We use the Normalized temperature-scaled Cross Entropy loss (NCE loss)~\cite{alayrac2020self} to map positive audio-text pairs close together in the CLIP-based joint embedding space, while negative pairs further away. Formally, given $N$ audio-image pairs $({\bf{a}}_i, {\bf{t}}_j)$ for $i\in\{1, 2, \dots, N\}$, we compute the following loss function $\mathcal{L}_{\textnormal{nce}}$ to optimize the pairwise cosine similarity between the normalized audio embedding ${\hat{\bf a}}_i={\bf a}_i/||{\bf a}_i||_2$ and the normalized text embedding ${\hat{\bf t}}_i={\bf t}_i/||{\bf t}_i||_2$:
% \begin{equation}
%     \mathcal{L}_{\textnormal{nce}}^{(a \leftrightarrow t)}=-\cfrac{1}{N}\displaystyle\sum_{i} \big( \big)
%     \label{loss:nce}
% \end{equation}
% \begin{equation}
%     \resizebox{\linewidth}{!}{%
%         \mathcal{L}_{\textnormal{nce}}^{(a \leftrightarrow t)}=-\cfrac{1}{N}\displaystyle\sum_{i} \text{log}\cfrac{\exp({\hat{\bf a}}_i^\intercal {\hat{\bf t}}_j / \tau)}{\sum_{j}\exp({\hat{\bf a}}_i^\intercal{\hat{\bf t}}_j / \tau)} + \text{log}\cfrac{\exp({\hat{\bf t}}_i^\intercal {\hat{\bf a}}_j / \tau)}{\sum_{j}\exp({\hat{\bf t}}_i^\intercal{\hat{\bf a}}_j / \tau)}
%     }
%     \label{loss:nce}
% \end{equation}
%
%
% \wonmin{Overall, the section is not easy to follow. 1) paragraph titles are not very intuitive and do not match well with the contents. 2) it is important to first explain the input of the network and also inputs/outputs of each module. 3) starting the paragraph with explaining a loss (without any prior information) is not easily understandable. Better to explain the procedure and architecture before the losses. 4)  Match the texts/title/paragraph order with Figure 2. It will help readers to follow the texts better.}
%\wonmin{It look like all losses can be in one paragraph. What about dividing the paragraphs into 1) multi-modal data encoding (containing encoder architecture, input, data augmentation, etc) 2) sound-guided contrastive learning (all losses). }

% \wonmin{Our objective is to learn/create a common shared embedding space by multi-modal inputs, audio, image and texts.}
% \wonmin{Before introducing the loss function, first explain what inputs are ($x_a, x_t$), how $a, t$ are obtained from the input}
% $\bf a$ and $\bf t$ denote $d$-dimensional embeddings obtained by audio encoder and text encoder.

% \wonmin{Before or after the equation, it would be good to explain what each term means (in words) and what it does. }
% \begin{equation}
%     \begin{aligned}
%     % \mathcal{L}_{\textnormal{con}} = \sum\limits^N_{i=1}\sum\limits^N_{j=1}y\cdot\mathbb(\log({\bf a}_i\cdot {\bf t}_j) + \log({\bf a}_i \cdot \hat{{\bf a}}_j)) + \\
%     % (1-y)\cdot\mathbb(\log(1-{\bf a}_i\cdot {\bf t}_j) + \log(1-{\bf a}_i\cdot \hat{{\bf a}}_j))
%     \mathcal{L}_{\textnormal{con}} = \text{NCE}(x_a, x_t)
%     \end{aligned}
%     \label{loss:audio}
% \end{equation}
% \wonmin{In the Eq 1-3, the loss function takes $a$ and $t$. (not $x_a$ and $x_t$).}
% \begin{equation}
%     \begin{aligned}
%     \mathcal{L}_{\textnormal{nce}}(\bf{a}, \bf{t})=-\text{log}(\cfrac{\text{exp}({\bf a}^\intercal{\bf t} / \tau)}{\text{exp}({\bf a}^\intercal{\bf t}/ \tau)+\sum_{\textnormal{z$\sim\mathcal{N}$(x)}}\text{exp}({{\bf{a'}}}^\intercal{\bf{t'}} / \tau)})
%     \end{aligned}
%     \label{loss:nce}
% \end{equation}

% v1 %
% Our objective is to learn/create a common shared embedding space by multi-modal inputs, audio, text and image.
% We employ the Normalized temperature-scaled Cross Entropy loss (NCE loss)~\cite{alayrac2020self} to make positive pairs similar and negative pairs dissimilar in the CLIP-based~\cite{radford2learning} joint embedding space. Here, two types of datasets are used: an audio-text pairs dataset and an audio-image pairs dataset. Given the pair $(\bf{a_i}, \bf{t_j})$, the contrastive loss for the audio-to-text $i$-th pair is as follows:
% \begin{equation}
%     \begin{aligned}
%     l_{i}^{(a\rightarrow t)}=-\text{log}\cfrac{\exp(\bf{a}_i^\intercal \bf{t}_j) / \tau}{\sum_{\textnormal{j=1}}^N\exp({{\bf{a}_i}}^\intercal{\bf{t}_j} / \tau)}
%     \end{aligned}
%     \label{loss:mini_con}
% \end{equation}
% where optimize cosine similarity between the normalized audio embedding ${{\bf a}_i}\in\mathbb{R}^{d}$ and the normalized text embedding ${\bf{t}_j}\in\mathbb{R}^{d}$ from multi-modal data encoding. 
% % $\mathcal{N}(x)$ denotes negative pairs of audio and text. 
% $\tau$ is a temperature hyperparameter for NCE loss. The contrastive loss for the text-to-audio~($t \rightarrow a$) $i$-th pair $(\bf{t_i}, \bf{a_j})$ is as follows:
% \begin{equation}
%     \begin{aligned}
%     l_{i}^{(t\rightarrow a)}=-\text{log}\cfrac{\exp(\bf{t}_i^\intercal \bf{a}_j) / \tau}{\sum_{\textnormal{j=1}}^N\exp({{\bf{t}_i}}^\intercal{\bf{a}_j} / \tau)}
%     \end{aligned}
%     \label{loss:mini_con2}
% \end{equation}
% Sampling $N$ mini-batch yields the same $N$ positive pairs and $N^2-N$ negative pairs. 
% Likewise, we minimize the NCE loss $\mathcal{L}_{\textnormal{nce}}$ for audio-to-text~($a \rightarrow t$) $i$-th pair as follows:
% \begin{equation}
%     \begin{aligned}
%     \mathcal{L}_{\textnormal{nce}}^{(a \leftrightarrow t)}=\cfrac{1}{N}\sum_{i=1}^N (l_{i}^{(a\rightarrow t)} + l_{i}^{(t\rightarrow a)})
%     \end{aligned}
%     \label{loss:nce}
% \end{equation}
% Embedding networks of different modalities learn to classify euclidean embedding dot products as positive and negative. Latent representations of a positive (e.g., sound of fire and a text of ``fire crackling'') and a negative~(e.g., sound of siren and a text of ``raining'') multi-modal pairs are pulled together and pushed apart from each other, respectively.  %Here,  the  latent  representations of a positive multi-modal pair (e.g., sound of fire and a text of “fire crackling”) from audio and text encoders are pulled  together  while  latent  representations  of  a  negative pair are pushed apart from each other
 
%\myparagraph{Audio Self-supervised Learning.}
\myparagraph{Applying Self-supervised Representation Learning for Audio Inputs.}
Self-supervised learning approaches rely on a contrastive loss that encourages representations of the same-class different views to be close in the embedding space, while that of different-class views to be pushed away from each other. We apply this technique to improve the quality of audio representations by minimizing the following $\mathcal{L}_{\textnormal{self}}^{(a \leftrightarrow \hat{a})}$:
\begin{equation}
    \mathcal{L}_{\textnormal{self}}^{(a \leftrightarrow \hat{a})}=\cfrac{1}{N}\sum_{i=1}^N (l_{i}^{(a\rightarrow \hat{a})} + l_{i}^{(\hat{a}\rightarrow a)})
    \label{loss:self}
\end{equation}
where $l_{i}^{(a\rightarrow \hat{a})}$ and $l_{i}^{(\hat{a}\rightarrow a)}$ are defined in a similar way as in Eq.~\ref{loss:mini_con} and \ref{loss:mini_con2}. This loss function is useful to learn subtle differences over sound inputs as it needs to maximize the mutual information between two different views of the same inputs but to minimize the mutual information between two views of the different inputs. For example, as shown in Fig.~\ref{fig:new_figure}, an audio sample $a_i$ forms a negative pair with $\hat{a}_j$ for $i\neq j$, which induces a diffusive effect in the embedding space. 


% v1 %
% Contrastive learning between audio and text makes the audio embedding space depend on the CLIP text embedding space. 
% We do not only perform contrastive learning between different modalities, but also perform contrastive learning on augmented audio data within the same modality to enrich the audio representation.
% We add a self-supervised learning loss with audio as input to represent unique situations within the scene~(see Fig.~\ref{fig:new_figure}). From the $N$ mini-batch, the audio sample itself learns similarly and the other samples dissimilarly as follows:
% \begin{equation}
%     \begin{aligned}
%     % \mathcal{L}_{\textnormal{con}} = \sum\limits^N_{i=1}\sum\limits^N_{j=1}y\cdot\mathbb(\log({\bf a}_i\cdot {\bf t}_j) + \log({\bf a}_i \cdot \hat{{\bf a}}_j)) + \\
%     % (1-y)\cdot\mathbb(\log(1-{\bf a}_i\cdot {\bf t}_j) + \log(1-{\bf a}_i\cdot \hat{{\bf a}}_j))
%     \mathcal{L}_{\textnormal{cross}}^{(a \leftrightarrow t)} = \lambda \cdot \mathcal{L}_{\textnormal{nce}}^{( a \leftrightarrow t)} + (1 - \lambda) \cdot \mathcal{L}_{\textnormal{self}}^{(a \leftrightarrow \hat{a})}
%     \end{aligned}
%     \label{loss:con_ver2}
% \end{equation}
% where $\lambda$ is a hyperparameter. $\mathcal{L}_{\textnormal{cross}}^{(a \leftrightarrow t)}$ is obtained by adding the $\mathcal{L}_{\textnormal{nce}}^{(a \leftrightarrow t)}$ between audio-text and the $\mathcal{L}_{\textnormal{self}}^{(a \leftrightarrow \hat{a})}$ which makes the audio sample and its augmented audio similar and dissimilar between the different audio samples. The $\mathcal{L}_{\textnormal{self}}^{(a \leftrightarrow \hat{a})}$ is the audio self-supervised learning loss which calculates NCE loss between $a$ and $\hat{a}$.
% After we sample N pairs of $\bf{a}$ and $\bf{t}$, we obtain an augmented sample $\bf{\hat{a}}$ and $\bf{\hat{t}}$ from each instance. 
% There are three types of pairs~($\bf{a}$, $\bf{\hat{a}}$), ($\bf{a}$, $\bf{\hat{t}}$), ($\bf{\hat{a}}$, $\bf{t}$), and contrastive learning is performed for each type in Equation~\ref{loss:con_ver2}. 

\myparagraph{Data Augmentation.}
We further apply the data augmentation strategy to improve the quality of representations and to overcome the lack of large-scale audio-text multimodal datasets. For audio inputs, we apply the SpecAugment~\cite{park19e_interspeech}, which visually augments Mel-spectrogram acoustic features by warping the features and masking blocks of frequency channels. For text inputs, we augment text data by (i) replacing words with synonyms, (ii) applying a random permutation of words, and (iii) inserting random words. Note that, for (i) we find synonyms of the given word from WordNet~\cite{fellbaum2010wordnet} and insert the synonym anywhere randomly in the given text input. For example, we augment original texts {\it ``rowboat, canoe, kayak rowing"} to produce new text {\it ``row canoe, kayak quarrel rowboat."}

% v1 %
%We use a data augmentation strategy for audio and text inputs. For audio augmentation, SpecAugment~\cite{park19e_interspeech} is performed on a audio sample. SpecAugment is a policy for visually augmenting Mel-spectrogram acoustic features and consists of warping the features, masking blocks of frequency channels. The text prompt describes the occurrence of dynamic actions or events, such as ``playing violin'' or ``baby laughing''. Furthermore, we augmented text data by (i) replacing words with synonyms, (ii) applying a random permutation of words, and (iii) inserting random words. To augment the text messages, we find synonyms of given word from WordNet~\cite{fellbaum2010wordnet} and insert the synonym anywhere in the text messages. For example, we augment original texts (``rowboat, canoe, kayak rowing") to produce new texts (``row canoe, kayak quarrel rowboat").

\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{cvpr_figure12.pdf}
  \vspace{-1.3em}
  \caption{Multi-modal contrastive learning with audio self-supervised loss. %$\mathcal{L}_{\textnormal{nce}}^{(a \leftrightarrow t)}$ minimizes the contrastive loss between audio and text pair~(${\bf{a}}_i$, ${\bf{t}}_j$) and $\mathcal{L}_{\textnormal{self}}^{(a \leftrightarrow \hat{a})}$ minimizes the self-supervised loss with itself~(${\bf{a}}_i$, $\hat{\bf a}_j$).
  % Even if the sampled audio has the same text label, to consider each audio sample's unique situation, audio samples other than themselves are considered as negative pairs to minimize loss.
  %Comparison of Sound-guided Manipulation. Thefirst row is the sound of fire crackling,  the second row isthe sound of raining, and the results of manipulation withTraumerAI, Crossing you in style, and our model are listedfrom left to right
  }
  \vspace{-1.3em}
  \label{fig:new_figure}
\end{figure}
\myparagraph{Loss Function.}
To summarize, we minimize the following loss function $\mathcal{L}_\text{total}$:
%update audio, text and image encoder weights by minimizing $\mathcal{L}_{\textnormal{total}}$ for the (image, audio) and (audio, text) pairs as follows:
% We employ following contrastive loss function $\mathcal{L}_{\textnormal{con}}$ to train our audio encoder:
\begin{equation}
    \begin{aligned}
    % \mathcal{L}_{\textnormal{con}} = \sum\limits^N_{i=1}\sum\limits^N_{j=1}y\cdot\mathbb(\log({\bf a}_i\cdot {\bf t}_j) + \log({\bf a}_i \cdot \hat{{\bf a}}_j)) + \\
    % (1-y)\cdot\mathbb(\log(1-{\bf a}_i\cdot {\bf t}_j) + \log(1-{\bf a}_i\cdot \hat{{\bf a}}_j))
    \mathcal{L}_{\textnormal{total}} = \mathcal{L}_{\textnormal{nce}}^{( a \leftrightarrow v)} + \mathcal{L}_{\textnormal{nce}}^{( a \leftrightarrow t)} + \mathcal{L}_{\textnormal{self}}^{( a \leftrightarrow \hat{a})}
    \end{aligned}
    \label{loss:con}
\end{equation}
% We use an indicator variable $y$ where we set $y=1$ for the positive pairs, otherwise y = 0. $N$ represents the batch size and $\cdot$ is the Euclidean dot product. 
%where the $\mathcal{L}_{\textnormal{total}}$ is a sum of the $\mathcal{L}_{\textnormal{nce}}^{( a \leftrightarrow v)}$ for (audio, image) pairs, the $\mathcal{L}_{\textnormal{cross}}^{( a \leftrightarrow t)}$ for (audio, text) pairs, and the $\mathcal{L}_{\textnormal{self}}^{( a \leftrightarrow t)}$ for (audio, text) pairs.% to align the tri-modal embedding space. 
% We use over 200k video sources from VGGSound dataset~\cite{chen2020vggsound} and over 2.1M from Audioset~\cite{gemmeke2017audio} for training our audio encoder. \wonmin{The sentence about datasets should go to the experiment section.} 
% \begin{equation}
%     \begin{aligned}
%     % \mathcal{L}_{\textnormal{con}} = \sum\limits^N_{i=1}\sum\limits^N_{j=1}y\cdot\mathbb(\log({\bf a}_i\cdot {\bf t}_j) + \log({\bf a}_i \cdot \hat{{\bf a}}_j)) + \\
%     % (1-y)\cdot\mathbb(\log(1-{\bf a}_i\cdot {\bf t}_j) + \log(1-{\bf a}_i\cdot \hat{{\bf a}}_j))
%     \mathcal{L}_{\textnormal{total}} = \mathcal{L}_{\textnormal{mul}} + \lambda_{aa} \cdot \mathcal{L}_{\textnormal{nce}}(\bf{a}, \bf{\hat{a}}) 
%     \end{aligned}
%     \label{loss:con_ver2}
% \end{equation}
%$\mathcal{L}_{\textnormal{total}}$ shows the total contrastive loss when sampling batches of audio, text and image triplet pairs. 
 
 
\subsection{Sound-guided Image Manipulation}
After learning the multi-modal joint embedding space by minimizing Eq.~\ref{loss:con}, we use a direct latent code optimization method to manipulate the given image similar to StyleCLIP~\cite{Patashnik_2021_ICCV}. As shown in Fig.~\ref{fig:contrastivelearning} (b), our model minimizes the distance between a given source latent code and an audio-driven latent code in the learned joint embedding space to produce sound-guided manipulated images. Moreover, we propose a {\it Adaptive Layer Masking} technique, which adaptively manipulates the latent code. 

% v1 %
%Latent optimization process for image manipulation is similar to the original StyleCLIP~\cite{Patashnik_2021_ICCV}. There are two different parts. First, the magnitude of the manipulation is adaptively controlled by masking the layer in the optimization process. Second, our model calculates the CLIP loss between the audio  and the generated image. Sound-guided latent code is optimized by minimizing distance computed in CLIP-based multi-modal share latent space.
 
% Second, our model calculates the CLIP loss between the audio $a$ and the fake image, $G(w_a)$. $w_a$ is optimized by minimizing $\mathcal{L}_{man}$ computed in CLIP-based multi-modal share latent space.
 
\myparagraph{Direct Latent Code Optimization.} We employ the direct latent code optimization for sound-guided image manipulation by solving the following optimization problem:
\begin{equation}
    \begin{aligned}
    \mathcal{L}_{man} =\  & \underset{w_a \in \mathcal{W}+}\argmin\;{d_{\textnormal{cosine}}(G(w_a),a)} +
    \lambda_{\textnormal{ID}}{\mathcal{L}_{ID}}(w_a) \\
     & \hspace{3.57 cm} + \lambda_{sim}||{g} \cdot {(w_a - w_s)||_2}\\ 
    \end{aligned}
    \label{loss:man}
\end{equation}
where a given source latent code $w_s\in\mathcal{W}$~(the
intermediate latent space in StyleGAN), audio-driven latent code $w_a\in\mathcal{W}+$. $\lambda_{sim}$ and $\lambda_{ID}$ are hyperparameters. $g$ is a trainable vector to mask the specific style layer adaptively.
%for regularization and preserving a face identity, respectively. 
$\mathcal{L}_{\textnormal{ID}}$ and $G$ are the identity loss and StyleGAN-based generator, respectively. The source latent code $w_s$ means the randomly generated latent code from $G$ or the latent code obtained from the existing input image through GAN inversion~\cite{richardson2021encoding, 10.1145/3450626.3459838}. With such an optimization scheme, we minimize the cosine distance $d_{\textnormal{cosine}}(G(w_a),a)$ between the embedding vectors of the manipulated image $G(w_a)$ and the audio input $a$.
%Therefore, the input image is modified in response to a user-provided audio by Eq.~\ref{loss:man}. 
% \wonmin{$g, \lambda_{\textnormal{ID}}, \mathcal{L}_{ID}$ are not defined.} \wonmin{The loss is similar to the original styleCLIP except the use of audio features and g (masking), right? It would be good to clearly mention that the loss is adapted from styleCLIP and explain the difference.} 

% \sang{Similar to above, please define all parameters from equation (4) which are not explained in below paragraph. $\lambda_{sim}$}

\myparagraph{Identity Loss.}
The similarity to the input image is also controlled by the identity loss function $\mathcal{L}_{\textnormal{ID}}$, which is defined:
\begin{equation}
    \mathcal{L}_{\text{ID}}(w_a) = 1 - \langle R(G(w_s), R(G(w_a))) \rangle
\end{equation}
where $R$ is the pre-trained ArcFace~\cite{deng2019arcface} model for face recognition, thus this loss function minimizes the cosine similarity $\langle R(G(w_s), R(G(w_a))) \rangle$ between its arguments in the latent space of the ArcFace network. This allows manipulating facial expressions without changing the personal identity. Note that we disable the identity loss by setting $\lambda_{\textnormal{ID}}=0$ for all other image manipulations.


% The source image identity is preserved by the loss function~$\mathcal{L}_{\textnormal{ID}}$ which changes a person's emotional characteristics (e.g., giggling, crying) while preserving the personal identity~\cite{Patashnik_2021_ICCV}. 
% The loss function $\mathcal{L}_{ID}$ maintains human identity by minimizing the distance between embeddings of the pre-trained Arcface network~\cite{deng2019arcface}. For scene and object image manipulation, we disable the identity loss by setting $\lambda_{\textnormal{ID}}=0$. 
% %\wonmin{$w_s$ and $w_a$ are never defined or mentioned how they are obtained.} 
% With such an optimization scheme, we minimize the cosine distance between the embedding vectors of the manipulated image $G(w_a)$ and the audio input $a$.
% Therefore, the input image is modified in response to a user-provided audio by Eq.~\ref{loss:man}. 
 
 
 
 

\myparagraph{Adaptive Layer Masking.} 
% A simple approach to regulate the similarity to the input image is the $L_2$ distance in latent space.
%Instead of direct $L_2$ regularization between $w_a$ and $w_s$ 
We control style changes with adaptive layer masking.
% \wonmin{Before talking about the layer masking, explain why we don't use $L_2$ or why we propose a different one.}
$L_2$ regularization is effective in keeping the image generated from the moved latent code from being different from the original~\cite{Patashnik_2021_ICCV}. However, StyleGAN's latent code has different properties per each layer, so different weights should be applied to each layer if the user-provided attribute changes. We use layerwise masking to keep compact content information within style latent code. % $\lambda_{sim}$ is a hyperparameter for regularization. 
In StyleGAN2~\cite{karras2020analyzing}, the latent code represents as $ w \in \mathbb{R}^{L \times D}$, where $L$ is the number of the network layers, and $D$ is the latent code's dimension size. We declare a parameter vector $g$ in $L$ dimension.  In latent optimization step, $g$ and $w$ are multiplied per layer. $g$ is iteratively updated, which adaptively manipulates the latent code. 





\myparagraph{Sound and Text Multi-modal Style Mixing.} Multi-modal manipulation of audio and text is based on style mixing of StyleGAN. Different layers of $w$ latent code in StyleGAN represent different properties. 
Because audio and text share the same new multi-modal embedding space, selecting a specific layer of each latent code guided by audio and text can manipulate the image using properties of audio and text. 
% We select the layer of audio-driven manipulated latent code to reflect the coarse features and the layer of text-driven manipulated latent code to reflect the fine features. 

% \begin{figure}[t]
%   \centering
%   \includegraphics[width=0.8\linewidth]{cvpr_figure4.pdf}
%   \caption{Sound-Guided Image Manipulation step. we use a direct code optimization approach. Here, a source latent code is modified in response to a user-provided audio input.}
%   \label{fig:fig2}\vspace{-1em}
% \end{figure}



\section{Experiments}
% \wonmin{Don't forget to add 'the' before '[name] dataset', e.g., the FFHQ dataset.}
% \wonmin{Don't use 'x' with numbers. Use $\times$, e.g., $1024 \times 1024$.}


%\myparagraph{Architecture.} 
%We use CLIP's large-scale pre-trained Vision Transformer (ViT)~\cite{dosovitskiy2021an} as the image encoder and the Transformer as the text encoder~\cite{radford2019language}. We use ResNet-50 as an audio encoder.  The output dimension of the audio encoder is 512, which is the same dimension as the image and text encoder of CLIP~\cite{radford2learning}. Our audio encoder takes melspectrogram acoustic features as an input and produces a $d$-dimensional latent representation. Augmented audio embedding $\hat{a}$ as audio encoder input is used for $\mathcal{L}_{nce}(a, \hat{a})$.

%We use StyleGAN2's pre-trained generator for manipulation. The size of StyleGAN2's latent code depends on the resolution of the learned image. For example, when the resolution is $1024 \times 1024$, the dimension of the latent code is $18 \times 512$ and $256 \times 256$ for $14 \times 512$. 
% \wonmin{Add some sentences about augmented audio encoding. }




\myparagraph{Implementation Details.}
Following CLIP~\cite{radford2learning}, we use the Vision Transformer (ViT)~\cite{dosovitskiy2021an} for our image encoder and the Transformer~\cite{radford2019language} for our text encoder. Note that we use a pre-trained model from \cite{radford2learning}. For our audio encoder, we use ResNet50~\cite{hershey2017cnn} by following~\cite{hershey2017cnn}, where we employ the same output dimension, 512, as the image and text encoder. First, we convert audio inputs to Mel-spectrogram acoustic features. Then, our audio encoder takes these features as an input to produce a 512-dimensional latent representation. The details about the train dataset are explained in supplemental material. For the manipulation step, we leverage StyleGAN2~\cite{karras2020analyzing}'s pre-trained generator. We set the size of latent code based on the resolution of the learned image. Here, we set $18 \times 512$ for images of size $1024 \times 1024$ and $14 \times 512$ for $256 \times 256$.

We train our model for 50 epochs using the Stochastic Gradient Descent (SGD) with the cosine cyclic learning rate scheduler~\cite{smith2017cyclical}. We set the learning rate to $10^{-3}$ with the momentum $0.9$ and weight decay $10^{-4}$. The batch size is set to 384. For audio augmentation, we use SpecAugment~\cite{park19e_interspeech} with the frequency mask ratio of $0.15$ and time masking ratio of $0.3$. For direct latent code optimization, $\lambda_{sim}$ and $\lambda_{ID}$ in in Eq. (\ref{loss:man}) are set to $0.008$ and $0.004$ for the FFHQ dataset; and $0.002$ and $0$ for the LSUN dataset.

% Multi-modal contrastive learning creates a multi-modal embedding space with $N$ batches of data sampled from Audio Set and VGG-Sound. For more information on train datasets, see supplemental material. 
% % \wonmin{If the whole encoder architecture is same as CLIP, mention it (and cite the paper).} 
% We use the batch size of 384 and train the model using the Stochastic Gradient Decent (SGD) optimizer with the cosine cyclic learning rate scheduler~\cite{smith2017cyclical}. The learning rate is $10^{-3}$, momentum is $0.9$, and weight decay is $10^{-4}$. And the epoch for learning is 50. For audio augmentation, we used SpecAugment~\cite{park2019specaugment} with the frequency mask ratio of $0.15$ and time masking ratio of $0.3$. Both $\lambda$ and $\lambda_{av}$ in Equations~\ref{loss:con_ver2} and~\ref{loss:con} are 0.5.

% We set same hyperparmeters for text-guided and sound-guided latent optimization. %\wonmin{What does it mean? same as what?} \sang{We set same hyperparmeters for text-guided and sound-guided latent optimization for fair comparison?} 
% For direct latent code optimization, $\lambda_{sim}$ and $\lambda_{ID}$ are set to $0.008$ and $0.004$ for the FFHQ dataset; and $0.002$ and $0$ for the LSUN dataset.





\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{cvpr_figure5.pdf}
  \vspace{-1.5em}
  \caption{Comparison of sound-guided manipulation results. Given fire crackling (top) and raining (bottom) audio inputs, we manipulate the input image with Tr\"{a}umerAI~\cite{jeong2021tr}, Crossing you in style~\cite{lee2020crossing}, and our method.}
  \vspace{-1.5em}
  \label{fig:cvprfig5}
\end{figure}



\subsection{Qualitative Analysis}
% In Figure~\ref{fig:cvprfig1}~(a), we provide examples of our audio-driven manipulation results (middle row). We adopt six different audio clips including ``baby crying'', ``people giggling'', ``noise blowing'', ``explosion'', ``fire crackling'', and ``thunderstorm''. Moreover, we compare our result with text-driven approach~(bottom row). We observe that the proposed method successfully carry out image manipulation with the audio input while preserving identity and key visual attributes. Figure~\ref{fig:cvprfig1}~(b) demonstrates that proposed audio-driven approach could be used with text-driven manipulation as well. For instance, a text prompt ``Black woman'' and an audio input ``people giggling'' could manipulate a white woman portrait into a giggling black woman portrait.}




%Sound-guided manipulation comparison. We apply \textit{Fire crackling} and \textit{Raining} sound to \textit{Tr$\ddot{a}$umerAI~\cite{jeong2021tr}, Crossing you in style}~\cite{lee2020crossing}, and our sound-guided manipulation method~(left to right)
%Comparison of Sound-guided Manipulation. Thefirst row is the sound of fire crackling,  the second row isthe sound of raining, and the results of manipulation withTraumerAI, Crossing you in style, and our model are listedfrom left to right


\myparagraph{Sound-guided Image Manipulation.} 
We first compare our sound-guided image manipulation model with existing sound-based style-transfer models including Tr\"{a}umerAI~\cite{jeong2021tr} and Crossing you in Style~\cite{lee2020crossing}. Fig.~\ref{fig:cvprfig5} showcases image manipulation results in response to given audio inputs, including fire crackling and raining. We observe that our model produces a better quality of manipulated images where existing models often fail to capture semantic information of the given audio input~(See 2\textsuperscript{nd} and 3\textsuperscript{rd} columns). 

%In Figure~\ref{fig:cvprfig5}, we compare our sound-guided image manipulation model qualitatively with previous sound-based style-transfer models including Tr$\ddot{a}$umerAI~\cite{jeong2021tr} and Crossing you in Style~\cite{lee2020crossing}. The first column represents the source image and subsequent columns illustrate the image manipulation results with three different models. \textit{Tr$\ddot{a}$umerAI} responds well to the size of the audio signal and \textit{Crossing you in Style} induces style changes in color and texture of sound. However, these models do not manipulate image in accordance with audio-relevant properties. Rather than making audio-reactive model, we focus on devising a model that reflects the sementic understanding of sound into the image.
 %allow model to Our model goes beyond simply being reactive to audio, allowing the semantics of the sound to appear in the image.




%Comparison of text-driven and audio-driven manipulation results. Rows represent the manipulation results with the input image, TediGAN, StyleCLIP, and our model, respectively. Attributes include baby crying, people coughing, people giggling, and people screaming.


\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{cvpr_figure2.pdf}
  \vspace{-1.5em}
  \caption{Given the (a) input image, we compare the image manipulation results between (b-c) text-driven image manipulation approaches (i.e. TediGAN~\cite{xia2021tedigan} and StyleCLIP~\cite{Patashnik_2021_ICCV}) and (d) ours. Attributes for driving such manipulations include baby crying, people coughing, people giggling, and people screaming.}
  \label{fig:cvprfig2}
  \vspace{-1.5em}
\end{figure}

\myparagraph{Comparison of Text-guided Image Manipulation.}
%We visually compare sound-guided image and text-guided image manipulation. (Seems not needed)
We use the latest text-guided image manipulation models as a baseline, including TediGAN and the latent optimization technology of StyleCLIP. As shown in Fig.~\ref{fig:cvprfig2}, the proposed sound-guided image manipulation~(proposed method) shows more radical results than text-guided manipulation~(TediGAN~\cite{xia2021tedigan} and StyleCLIP~\cite{Patashnik_2021_ICCV}). Unlike text-guided methods, the audio-guided approach achieves natural image style transfer while capable of reflecting multiple labels.
%Beyond text, audio expresses the change of image style more naturally when the attribute has multiple labels.
 For example, TediGAN emphasizes crying, whereas StyleCLIP focuses on the baby when ``baby crying'' context is given. On the contrary, our proposed method is capable of handling ``baby'' and ``crying'' simultaneously. %For example, when TediGAN and StyleCLIP manipulate images with "baby crying", TediGAN focuses on crying, and StyleCLIP focuses on baby, but our method manipulates images of baby and crying at the same time. 

We demonstrate that each audio sample has its own context, which makes the guidance richer than text~(Fig.~\ref{fig:difference}). If the magnitude of \textit{Thunder} is altered or a specific attribute like \textit{Rain} is added to the audio, the manipulation context becomes more diverse than text-guided image manipulation.

We visualize the direction vector with t-SNE~\cite{van2008visualizing} in a supplemental document. By subtracting the vectors of the latent code guided by each modality and the source latent code, we show the distribution of manipulating direction. 
We select the attributes in VGG-Sound~\cite{chen2020vggsound} and randomly manipulate the audio and text prompts.
Although we randomly sample the audio and text in the same labels, the sound-guided latent code shows a more significant transition than the text-guided latent code. We use various text synonyms for a fair comparison, but text-guided latent code seems less effective with changes. % Even if audio and text have the same class in CLIP space, audio-driven latent code has a more considerable semantic change than text-driven latent code because the distance between instances in CLIP space is more remarkable than text. 
% Data distribution in CLIP space affects the direction of manipulation in StyleGAN latent space. 
% urthermore, the dispersion of the expression of audio embedding is extensive and continuous, which appears as a result of drastic and dramatic manipulation of the image.}


\begin{figure}[t!]
  \centering
  \includegraphics[width=\linewidth]{cvpr_figure11.pdf}
  \vspace{-1.3em}
  \caption{Comparison of manipulation results between ours (top) and the existing text-driven manipulation approach, StyleCLIP~\cite{Patashnik_2021_ICCV} (bottom). Unlike the text-driven approach, ours can produce more diverse manipulation results in response to different intensities of raining, i.e. raining, raining with weak thunder, and raining with strong thunder.}
  \label{fig:difference}
  \vspace{-0.8em}
\end{figure}

\begin{figure}[t!]
  \centering
  \includegraphics[width=\linewidth]{figure9_submission.pdf}
  \caption{An example of  image style mixing jointly with the audio (\textit{people giggling}) and text input (\textit{black woman}).}
  \label{fig:stylemixing}
   \vspace{-2em}
\end{figure}


\myparagraph{Multi-modal Image Manipulation.} 
Our method ensures that audio, text, and image share the same embedding space. To demonstrate that multi-modal embedding lies in a same latent space, we interpolated text and sound-guided latent code~(see supplementary document). 
%shows images generated by the StyleGAN2 generator of interpolated latent code guided by text and audio, which are different modalities.
Constructing multi-modal shareable latent space enables joint modification of the target image
%controllability of StyleGAN latent code 
with user-provided text and audio inputs from the same embedding space.
We further perform multi-modal style mixing experiments by selecting a specific layer of latent code and mixing style with audio and text. 
We find that the sound source can effectively manipulate facial emotion aspects such as ``giggling" on the face and text information controls the background color of the target image (Fig.~\ref{fig:stylemixing}). 
For the style-mixing details, we follow TediGAN's StyleGAN layerwise analysis~\cite{xia2021tedigan}. In the 18 $\times$ 512 latent code, the style-mixing technique selects the 1st to 9\textsuperscript{th} layers of the sound-guided latent code and the 10\textsuperscript{th} to 18\textsuperscript{th} layers of the text-guided latent code to mix the dynamic characteristics of sound and human properties of text.





\myparagraph{Effect of Adaptive Layer Masking.}
In StyleGAN~\cite{jeong2021tr}, it is necessary to adaptively regularize style layer since each layer of latent code has different style attributes. For each layer of latent code, it multiplies the trainable parameter that controls the diversity
%between 0~(diverse) and 1~(close to original) 
during regularization. %Here, the closer to 1 represents the closer to the original whereas the closer to 0 means the more it changes.
The ablation study shows a qualitative comparison of the mechanism for applying adaptive layer masking to the style layer, as illustrated in Fig.~\ref{fig:cvprfig6}. The adaptive masking rectifies the direction by changing the latent code based on the semantic cue. When applying the gate function, sound-guided image manipulation is semantically reasonable. For example, a thunderstorm is a blend of thunder and rain sound. Although thunder and lightning are not seen in the second row, lightning and rain appear in the last row. Manipulation results according to $\lambda_{sim}$ and $\lambda_{ID}$ hyperparameters are added to the supplemental material.
%The adaptive masking rectifies the direction by changing the latent code that affects the style that the semantic cue best reveals. When applying the gate function, sound-guided image manipulation is semantically reasonable. For example, a thunderstorm is a sound of thunder and rain. Therefore, thunder and lightning are not seen in the second row, but in the last row, lightning and rain appear.





% In Figure~\ref{fig:cvprfig1}~(a), we provide examples of our audio-driven manipulation results (middle row). We adopt six different audio clips including ``baby crying'', ``people giggling'', ``noise blowing'', ``explosion'', ``fire crackling'', and ``thunderstorm''. Moreover, we compare our result with text-driven approach~(bottom row). We observe that the proposed method successfully carry out image manipulation with the audio input while preserving identity and key visual attributes. Figure~\ref{fig:examples}(b) demonstrates that proposed audio-driven approach could be used with text-driven manipulation as well. For instance, a text prompt ``Black woman'' and an audio input ``people giggling'' could manipulate a white woman portrait into a giggling black woman portrait. We showcase more diverse examples in the supplemental material (see Supplemental Figure 1 and Figure 2).


%Difference between audio and text-driven image manipulation. Even in the same thunder category, each instance of audio has a different context.




% \begin{figure}
%      \centering
%      \begin{subfigure}[b]{0.3\linewidth}
%          \centering
%          \includegraphics[width=\linewidth]{scream.png}
%          \caption{Scream}
%          \label{fig:scream}
%      \end{subfigure}
%      \hfill
%      \begin{subfigure}[b]{0.3\linewidth}
%          \centering
%          \includegraphics[width=\linewidth]{sob.png}
%          \caption{Sobbing}
%          \label{fig:sob}
%      \end{subfigure}
%      \hfill
%      \begin{subfigure}[b]{0.3\linewidth}
%          \centering
%          \includegraphics[width=\linewidth]{noseblowing.png}
%          \caption{Nose blowing}
%          \label{fig:noseblowing}
%      \end{subfigure}
%      \hfill
%      \centering
%      \begin{subfigure}[b]{0.3\linewidth}
%          \centering
%          \includegraphics[width=\linewidth]{giggle.png}
%          \caption{Giggling}
%          \label{fig:giggling}
%      \end{subfigure}
%      \hfill
%      \begin{subfigure}[b]{0.3\linewidth}
%          \centering
%          \includegraphics[width=\linewidth]{cough.png}
%          \caption{Coughing}
%          \label{fig:giggling}
%      \end{subfigure}
%      \hfill
%      \begin{subfigure}[b]{0.3\linewidth}
%          \centering
%          \includegraphics[width=\linewidth]{babycrying.png}
%          \caption{Baby crying}
%          \label{fig:babycrying}
%      \end{subfigure}
%         \caption{Visualization of audio and text-guided manipulation directions with t-SNE~\cite{van2008visualizing} (red: text, blue: audio).}
%         \label{fig:cvprfig}
% \end{figure}








%Comparision of audio encoder classification performance. Each LR and ZS stands for logistic regression with normalized audio embedding and zero-shot inference.

\subsection{Quantitative Evaluation}
\myparagraph{Zero-shot Transfer.} % To evaluate the quality and the discriminability of the audio-based embedding space, we perform zero-shot audio classification. As a baseline model, we use the ResNet50 classifier and train the classifier in a supervised way from randomly initialized weights. As shown in Table~\ref{zero}, our audio encoder shows better classification performance than the baseline. 
We compare our model to the supervised method and the existing zero-shot audio classification method. First, we compare audio embeddings trained by supervised methods such as logistic regression, ResNet50~\cite{hershey2017cnn} supervised by random initialization of weights as a baseline model, and AudioCLIP~\cite{guzhov2021audioclip}. We consider AudioCLIP as supervised learning method since it fine-tunes the evaluation dataset using the audio head in the paper. Even though logistic regression is used without additional fine-tuning on the ResNet50 backbone, it is comparable to AudioCLIP using ESResNeXt~\cite{guzhov2021esresne} as the backbone. Secondly, we compare the zero-shot audio classification accuracy with Wav2clip~\cite{wu2021wav2clip}. Table~\ref{zero} shows that our model outperforms previous studies in each task. Our proposed loss learns three modalities in the CLIP embedding space and obtains a more rich audio representation through the contrastive loss between audio samples whereas Wav2clip only learns the relationship between audio and visual.
% \wonmin{What about putting the dataset part in the datasets paragraph and use this paragraph as 'architecture' to explain the details of the model architecture. So, 'zero-shot transfer' paragraph can be moved to the 'quality analysis'.}

\myparagraph{Semantic Accuracy of Manipulation.} We quantitatively analyze the effectiveness of our proposed audio-driven image manipulation approach. First, we measure performance on the semantic-level classification task. Given the audio embeddings from our pre-trained audio encoder, we train a linear classifier to recognize eight semantic labels including giggling, sobbing, nose-blowing, fire crackling, wind noise, underwater bubbling, explosion, and thunderstorm. We use StyleGAN2~\cite{karras2020analyzing} weights pre-trained from the FFHQ~\cite{karras2019style} dataset when guiding with giggling, sobbing, and nose blowing attributes to compare the semantic-level classification accuracy between text and audio. Also, when guiding with fire crackling, wind noise, underwater bubbling, explosion, and thunderstorm attributes, the weights of StyleGAN2 pre-trained with the LSUN (church)~\cite{yu2015lsun} dataset are used. As shown in Fig.~\ref{fig:userstudy}~(a), we generally outperform existing text-driven manipulation approach with better semantically-rich latent representation. 

\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{cvpr_figure6.pdf}
  \vspace{-1.3em}
  \caption{Ablation study of adaptive layer masking. The first row is the input image, the second row is the manipulation result when the gate function is not applied, and the third row is the sound-guided image manipulation result after the gate function is applied. }
  \label{fig:cvprfig6}
  \vspace{-1.3em}
\end{figure}

\begin{table}[t!]
    \caption{Comparison of the quality of audio representations between ours and alternatives. We report classification accuracy (top-1 in \%) of a linear classifier on the ESC-50~\cite{piczak2015esc} and the Urban sound 8k~\cite{Salamon:UrbanSound:ACMMM:14} datasets as well as their zero-shot inference results. {\it{Abbr.}} $S$: supervised setting.}
    \label{zero}
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{@{}lcccc@{}}
        \toprule
        \multirow{2}{*}{Model} & \multirow{2}{*}{$S$} & \multirow{2}{*}{Zero-shot} & \multicolumn{2}{c}{Dataset} \\ \cmidrule{4-5}
        & & & ESC-50 & Urban sound 8k \\ \midrule
        ResNet50~\cite{hershey2017cnn} & \checkmark & - & 66.8 \% & \textbf{71.3 \%} \\ 
        AudioCLIP~\cite{guzhov2021audioclip} & \checkmark & - & 69.4 \% & 68.8 \% \\ \midrule
        Ours w/o $\mathcal{L}_{nce}^{(a \leftrightarrow \hat{a})}$ & - & - & 58.7 \% & 63.3 \% \\
        Ours & - & - & \textbf{72.2 \%} & 66.8 \% \\\midrule
        Wav2clip~\cite{wu2021wav2clip} & - & \checkmark & 41.4 \% & 40.4 \% \\
        % Audio encoder(ZS) & 64.4 \% & 66.8
        Ours w/o $\mathcal{L}_{nce}^{(a \leftrightarrow \hat{a})}$ & - & \checkmark & 49.4 \% & 45.6 \% \\
        Ours & - & \checkmark & \textbf{57.8 \%} & \textbf{45.7\%} \\
        \bottomrule
    \end{tabular}}
    \vspace{-0.5em}
\end{table}


% \begin{figure*}[t]
%   \centering
%   \includegraphics[width=0.9\linewidth]{figure7_submission.pdf}
%   \caption{It shows that interpolation between latent codes is possible even if latent codes are guided for different modals and attributes in the latent space of StyleGAN2 pre-trained with FFHQ.}
%   \label{fig:stylemixing_lsun}
% \end{figure*}

\myparagraph{Distribution of Manipulation Direction.} We can see how much the latent code has changed by the cosine similarity between the source latent code and the manipulated latent code. We compare the cosine similarity between text-guided and sound-guided latent representations. We evaluate the mean and variance of the cosine similarity between $w_s$, a source latent code, $w_a$, an audio-driven latent code, and $w_t$, a text-driven latent code. The latent representations generally exhibit a high-level characteristic of the content~(see sup.). In the latent space of StyleGAN2, the sound-guided latent code moves more from the source latent code than the text-guided latent code, and the image generated from the sound-guided latent code is more diverse and dramatic than the text-guided method.

\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{cvpr_figure13.pdf}
  \caption{Quantitative evaluation and user study results. (a)~Downstream task evaluation to compare the quality of representations between ours and text-driven manipulation approaches on the FFHQ~\cite{karras2019style} dataset. A linear classifier is trained to predict 8 semantic labels, such as giggling, sobbing, etc. Participants answered a questionnaire including (b)~naturalness (``Which of the images is the best?'') and (c)~perceptual realism (``Do you think the provided image looks naturally manipulated?''). For perceptual realism, we use a 5-point Likert scale.}
  \label{fig:cvprfig13}
  \label{fig:userstudy}
  \vspace{-1.3em}
\end{figure}

\iffalse
\begin{table}[t!]
  \caption{Downstream task evaluation to compare the quality of representations between ours and text-driven manipulation approaches on the FFHQ~\cite{karras2019style} dataset. A linear classifier is trained to predict 8 semantic labels, such as giggling, sobbing, etc. }
  \vspace{-1.0em}
  \centering
  \label{accuracy}
  \resizebox{.5\linewidth}{!}{
  \begin{tabular}{@{}lc@{}}
    \toprule
    Model & Accuracy (in \%)\\ \midrule
    TediGAN~\cite{xia2021tedigan} & 84.8 \\
    StyleCLIP~\cite{Patashnik_2021_ICCV} & 92.7 \\
    Ours & \textbf{98.3} \\
    \bottomrule
  \end{tabular}}
\end{table}
\fi

\iffalse
\begin{table}[t]
    \caption{User study result. Participants answered two questions regarding naturalness (``Which of the images is the best?'') and perceptual realism (``Do you think the provided image looks naturally manipulated?''). For perceptual realism, we use a five-level Likert scale.}
    \label{table:section1}
    %\vspace{-1.3em}
    \centering
    \resizebox{.7\linewidth}{!}{
    \begin{tabular}{@{}lcc@{}}
    \toprule
    Model & Perceptual Realism & Naturalness \\
    %& {\it{``Which of the images is the best?''}} & {\it{``looks naturally manipulated?''}}\\
    \midrule
    TediGAN~\cite{xia2021tedigan}  & 20.4 \% & 2.435\\
    StyleCLIP~\cite{Patashnik_2021_ICCV} & 20.2 \% & 3.255 \\
    Ours & \textbf{59.4 \%} & \textbf{4.045} \\
    \bottomrule
    \end{tabular}}
    \vspace{-1.3em}
\end{table}
\fi

%Human evaluation results. Participant selects the image with the best attribute and generation quality among the images created by each model(first column). Participants give realistic scores between 1 and 5 points~(second column). A score closer to 5 means the more realistic image.

% \begin{table}[h!]
%     \begin{center}
    
%     \begin{tabular}{@{}lc@{}}
%     \toprule
%     Model & Realistic Score \\
%     \midrule
%     TediGAN~\cite{xia2021tedigan}  &  2.435 \\
%     StyleCLIP~\cite{patashnik2021styleclip} & 3.255 \\
%     Ours & \textbf{4.045} \\
%     \bottomrule
%     \end{tabular}
%     \end{center}
%     \caption{Realistic score. Participants give realistic scores between 1 and 5 points. A score closer to 5 means a more realistic image.}
%     \label{table:section3}
% \end{table}


% \begin{itemize}
%     \item We vote with AMT to see if the semantics of the manipulated human face and sound user-provided sound match. 
%     \item We sample six random images from three models and examine which attribute fits best.
%     \item We rank the images sampled from the three models according to how realistic they are.
% \end{itemize}


%\vspace{-0.285cm}
\subsection{User Study}
We recruit 100 participants from Amazon Mechanical Turk (AMT) for evaluating our proposed method. 
We show participants three manipulated images that are generated by TediGAN~\cite{xia2021tedigan}, StyleCLIP~\cite{Patashnik_2021_ICCV}, and our model. Participants answer the following questionnaire: (i) Perceptual Realism-~\textit{Which of the images is the best?} and (ii) Naturalness-~\textit{Do you think the provided image looks naturally manipulated?} For naturalness, we employ Likert scale ranging from 1~(low naturalness) to 5~(high naturalness). Fig.~\ref{fig:userstudy}~(b) and Fig.~\ref{fig:userstudy}~(c) show that our method significantly outperforms other state-of-the-art approaches~(TediGAN and StyleCLIP) in terms of \textit{Perceptual Realism} and \textit{Naturalness}. The large portion of participants~($59.4$\%) chose generated image by our model as the best. Moreover, the result also shows that our method generated more natural images than other text-driven manipulation approaches. Details are illustrated in supplementary document.%which is higher than those from other existing text-driven manipulation approaches. 

%Previous works carried out quantitative user evaluation on the generation quality of an image~\cite{altwaijry2016learning,choi2018stargan, philion2020learning, shaham2019singan, zhou2018interpretable}. % with  studies design people to quantitatively evaluate the generation quality of an image
%We conducted two user studies using Amazon Mechanical Turk~(AMT) for quantitative evaluation of sound-guided image manipulation from 100 random users. In the first study, participants select the best-generated image based on each attribute and generation quality among images generated by TediGAN~\cite{xia2021tedigan}, StyleCLIP~\cite{patashnik2021styleclip}, and our model. In the second study, participants also report how realistic the manipulated image is in scale of 1 (less realistic) to 5 (more realistic).  

%As shown in the Table~\ref{table:section1}, our model wins the majority of votes for best manipulating attributes in AMT. Sound has a significant advantage in delivering information necessary for image manipulation. Also, our model has a higher realistic score than the other two models, which shows that our model produces the most realistic results after manipulation.

%More user studies are explained in supplemental material.
% Participants are instructed to pick the attribute that best matches the given image. 

% Table~\ref{table:section2} shows that the result of the image guided by sound is most relevant to the audio. In the last study, participants choose from 1 to 5 how realistic the manipulated image is. (The higher the score, the more realistic). Table~\ref{table:section1} indicates that our model is the most realistic.


%\subsection{Ablation Study}
% We conduct an ablation study on adaptive layer masking and the hyperparameters of latent optimization.




% \myparagraph{Identity Loss.}


% \begin{figure}[t!]
%   \centering
%   \includegraphics[width=\linewidth]{cvpr_audio.png}
%   \caption{Audio and text embedding in CLIP space. We show the text embedding of the label and audio embedding of audio instance in CLIP space with t-SNE visualization from a given (audio, text) pair of ESC-50 datasets.}
%   \label{fig:cvprfig}
% \end{figure}


% \begin{figure*}[t!]
%   \centering
%   \includegraphics[width=0.95\linewidth]{figure7_submission.pdf}
%   \caption{In the latent space of pre-trained StyleGAN2 with FFHQ, the latent code is guided by text and audio modal for the attribute of "people giggling, people sobbing," and the interpolation result between the changed latent codes is shown.}
%   \label{fig:stylemixing_ffhq}
% \end{figure*}




\section{Applications}
\myparagraph{Sound-Guided Artistic Paintings Manipulation.} 
We propose a novel sound-guided image manipulation approach for artistic paintings. We employ StyleGAN2~\cite{karras2020analyzing} generator which is pre-trained with the fine-art paintings dataset called WikiArt~\cite{saleh2016large}. As shown in Fig.~\ref{fig:cvprfig8}, our model could produce various manipulations for art paintings guided by given audio inputs. We observe that an audio input can successfully provide a semantic cue to manipulate artistic paintings. Given a fire crackling sound, a painting is manipulated with fire crackling. We also measured the manipulation quality for artistic painting using the Wikiart dataset using AMT. The responses showed that audio~(73.3\%) is better than text~(26.7\%) in terms of manipulation.

\myparagraph{Music Style Transfer.}
Our method has a potential to reflect the mood of the music into the image style. Fig.~\ref{fig:cvprfig8} illustrates the results of image style transfer with various music genres. The source latent code is close to the keywords of each music, so the mood of the music appears in the image. For instance, \textit{Funny} music manipulates the image with a fairy-tale style whereas \textit{Latin} music manipulates the image with red-color theme which reflects \textit{passion} characteristic. 
%Funny music manipulates the given image with a scene from fairy tale, and Latin music manipulates the given image with a red color of passion representing Latin.

\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{cvpr_figure8.pdf}
  \caption{Examples of sound-guided artistic paintings manipulation and music style transfer using our method.}
  \label{fig:cvprfig8}
  \vspace{-1.3em}
\end{figure}

\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{cvpr_figure7.pdf}
  \vspace{-1.3em}
  \caption{Failure cases of manipulation with our method.}
  \label{fig:cvprfig7}
  \vspace{-1.3em}
\end{figure}

\section{Discussion and Conclusion}
We propose a method to manipulate images based on the semantic-level understanding from the given audio input. %for guiding the image manipulation direction with the intrinsic meaning of given audio input. 
We take the user-provided audio input into the latent space of StyleGAN2~\cite{karras2020analyzing} and the CLIP~\cite{radford2learning} embedding space. Then, the latent code is aligned with the audio to enable meaningful image manipulation while reflecting the context from the audio. 
Our model produces responsive manipulations based on various audio inputs such as wind, fire, explosion, thunderstorm, rain, giggling, and nose blowing. We observe that an audio input can successfully provide a semantic cue to manipulate images accordingly. However, it would be challenging to preserve the identity for all cases due to the drastic change in image style~(see Fig.~\ref{fig:cvprfig7}).
Our method of traversing multi-modal embedding space can be used in many applications with multi-modal contexts.



% \begin{figure*}[b]
%     \centering 
%     \includegraphics[width=\textwidth]{figure5_submission.pdf}
%     \caption{Results of text-driven manipulation and audio-driven manipulation from FFHQ dataset ~\cite{karras2019style}.}
%         \label{fig:fig6}
% \end{figure*}

\newpage
{
\small
\bibliographystyle{ieee}
\bibliography{egbib}
}

\end{document}