\section{Continuous-Time Generalized AdaGrad}
\label{sec:adagrad}

In this section, we propose a set of autonomous ordinary differential equations. Using first-order Euler discretization, we show that the proposed set of differential equations coincides with a general version of the AdaGrad algorithm, which we refer to as the Generalized AdaGrad (G-AdaGrad). The proposed differential equations include the original AdaGrad as a special case.

We make the following assumptions in order to present our algorithms and their convergence results.

\begin{assumption} \label{assump_1}
Assume that the minimum of function $f$ exists and is finite. In other words, $\mnorm{\min_{x \in \R^d} f(x)} < \infty$.
\end{assumption}

\begin{assumption} \label{assump_2}
Assume that $f$ is twice differentiable over its domain $\R^d$ and the entries in the Hessian matrix $\nabla^2 f(x)$ are bounded above for all $x\in \R^d$.
\end{assumption}

The above assumptions about the {\em objective function} $f$ are mild and standard in the literature of gradient-based optimization. Assumption~\ref{assump_2} is equivalent to the gradient $\nabla f$ being Lipschitz continuous, which is often referred as the function $f$ being {\em smooth}~\cite{li2019convergence, reddi2018adaptive,de2018convergence}.


\subsection{Description of Generalized AdaGrad}
\label{sub:algo_adagrad}

We propose the Generalized AdaGrad (G-AdaGrad) method which is parameterized by a positive real scalar $\alpha$.
For each dimension $i \in \{1,\ldots,d\}$ and $t \geq 0$, consider the following pair of differential equations
\begin{align}
    \Dot{x}_{ci}(t) & = \norm{\nabla_i f(x(t))}^2, \label{eqn:xc_evol} \\
    \Dot{x}_i(t) & = -\dfrac{\nabla_i f(x(t))}{\left(x_{ci}(t)\right)^\alpha},  \label{eqn:x_evol}
\end{align}
with initial conditions $x_c(0) \in \R^d$ and $x(0) \in \R^d$. We assume that the initial condition $\{x_{ci}(0) > 0 : i = 1,\ldots,d\}$.  The variable ${x}_{ci}, \forall i$ can be abstracted as dynamic controller state. 

The above pair of differential equations~\eqref{eqn:xc_evol}-\eqref{eqn:x_evol} can be seen as a continuous-time variation of the following algorithm, when~\eqref{eqn:xc_evol}-\eqref{eqn:x_evol} are discretized with a fixed sampling time $\delta > 0$. For each $i \in \{1,\ldots,d\}$ and $k \in \{0,1,\ldots\}$,
\begin{align}
    x_{ci}((k+1)\delta) & = x_{ci}(k\delta) + \delta \norm{\nabla_i f(x(k\delta))}^2, \label{eqn:xc_dscrt} \\
    x_{i}((k+1)\delta) & = x_{i}(k\delta) - \delta \dfrac{\nabla_i f(x(k\delta))}{\left(x_{ci}(k\delta)\right)^\alpha}.  \label{eqn:x_dscrt}
\end{align}

This fact can be seen from the following argument.
From Taylor series expansion of $x_{ci}((k+1)\delta)$ and $x_{i}((k+1)\delta)$ we obtain that,
\begin{align*}
    x_{ci}((k+1)\delta) & = x_{ci}(k\delta) + \delta \Dot{x}_{ci}(k\delta) + \bigO(\delta^2), \\
    x_{i}((k+1)\delta) & = x_{i}(k\delta) + \delta \Dot{x}_{i}(k\delta) + \bigO(\delta^2).
\end{align*}
Upon substituting from above,~\eqref{eqn:xc_dscrt}-\eqref{eqn:x_dscrt} can be rewritten as
\begin{align*}
    \delta \Dot{x}_{ci}(k\delta) + \bigO(\delta^2) & = \delta \norm{\nabla_i f(x(k\delta))}^2, \, i \in \{1,\ldots,d\}, \\
    \delta \Dot{x}_{i}(k\delta) + \bigO(\delta^2) & = - \delta \dfrac{\nabla_i f(x(k\delta))}{\left(x_{ci}(k\delta)\right)^\alpha}, \, i \in \{1,\ldots,d\}.
\end{align*}
Defining $t=k\delta$, in the limit $\delta \to 0$, the above equations coincide with~\eqref{eqn:xc_evol}-\eqref{eqn:x_evol}. 

Note that,~\eqref{eqn:xc_dscrt}-\eqref{eqn:x_dscrt} represents a generalization of the AdaGrad algorithm discussed in Section~\ref{sec:intro} with {\em step-size} $\eta = \delta$ and an additional parameter $\alpha$. The controller states $x_c(t)$ in continuous-time corresponds to the variable $b_k$ in discrete-time of the AdaGrad algorithm. When we set $\alpha=0.5$,~\eqref{eqn:xc_dscrt}-\eqref{eqn:x_dscrt} correspond to the original AdaGrad algorithm. Introducing the parameter $\alpha$ can further improve its convergence. This is discussed in the following subsection.


\subsection{Convergence of Generalized AdaGrad}
\label{sub:conv_adagrad}

Define the set of {\em critical points} of the {\em objective function} $f$ as
\begin{align}
    X^* = \{x\in \R^d : \nabla f(x) = 0_d\}. \label{eqn:xstar}
\end{align}
Theorem~\ref{thm:adagrad} below presents a key result on the convergence of the G-AdaGrad algorithm~\eqref{eqn:xc_evol}-\eqref{eqn:x_evol} in continuous-time to a {\em critical point} in $X^*$.

\begin{theorem} \label{thm:adagrad}
Consider the pair of differential equations~\eqref{eqn:xc_evol}-\eqref{eqn:x_evol} with initial conditions $x_c(0) \in \R^d$ and $x(0) \in \R^d$ such that $\{x_{ci}(0) > 0 : i = 1,\ldots,d\}$. Let the parameter $\alpha \in (0,1)$. If Assumptions~\ref{assump_1}-\ref{assump_2} hold, then
\begin{align}
    \lim_{t \to \infty} \nabla f(x(t)) = 0_d. \label{eqn:zero_grad_1}
\end{align}
Moreover, for all $t \geq 0$, we have
\begin{align}
   f(x(t)) = f(x(0)) + \sum_{i=1}^d \dfrac{\left(x_{ci}(0)\right)^{1-\alpha} - \left(x_{ci}(0) + \int_0^t \norm{\nabla_i f(x(s))}^2 ds\right)^{1-\alpha}}{1-\alpha}. \label{eqn:ft}
\end{align}
\end{theorem}

\begin{proof}
The time-derivative of $f$ along the trajectories $x(t)$  of~\eqref{eqn:x_evol} is given by
\begin{align*}
    \Dot{f}(x(t)) & = (\nabla f(x(t))^T \Dot{x}(t) = \sum_{i=1}^d \nabla_i f(x(t)) \Dot{x}_{i}(t).
\end{align*}
Substituting~\eqref{eqn:x_evol} yields,
\begin{align*}
    \Dot{f}(x(t)) & =  - \sum_{i=1}^d \dfrac{\norm{\nabla_i f(x(t))}^2}{\left(x_{ci}(t)\right)^\alpha}.
\end{align*}
Further utilizing~\eqref{eqn:xc_evol} we get,
\begin{align}
    \Dot{f}(x(t)) & =  - \sum_{i=1}^d \dfrac{\Dot{x}_{ci}(t)}{\left(x_{ci}(t)\right)^\alpha}. \label{eqn:fdot}
\end{align}
Integrating both sides above with respect to (w.r.t) $t$ from $0$ to $t$, we get
\begin{align}
   & f(x(t))-f(x(0)) = - \sum_{i=1}^d \int_0^t \dfrac{\Dot{x}_{ci}(s)}{\left(x_{ci}(s)\right)^\alpha} ds. \label{eqn:f_int}
\end{align}
Since $\alpha < 1$, upon evaluating the integral we have
\begin{align}
    f(x(t)) = f(x(0)) + \sum_{i=1}^d \dfrac{\left(x_{ci}(0)\right)^{1-\alpha} - \left(x_{ci}(t)\right)^{1-\alpha}}{1-\alpha}. \label{eqn:f_eval}
\end{align}
Integrating both sides of~\eqref{eqn:xc_evol} w.r.t $t$ from $0$ to $t$, we have
\begin{align*}
    x_{ci}(t) = x_{ci}(0) + \int_0^t \norm{\nabla_i f(x(s))}^2 ds, ~ i \in \{1,\ldots,d\}.
\end{align*}
Using the above equation in~\eqref{eqn:f_eval} proves~\eqref{eqn:ft}.

Since $x_{ci}(0) > 0$, we have $x_{ci}(t) > 0$.
The above equation implies that $x_{ci}(t)$ is non-decreasing w.r.t $t$, which combined with~\eqref{eqn:ft} and $\alpha\in (0,1)$ implies that $f(x(t))$ in non-increasing w.r.t. $t$. From Assumption~\ref{assump_1}, $f$ is bounded below. Thus, $\lim_{t \to \infty} f(x(t))$ is finite. From~\eqref{eqn:f_eval} then it follows that, $\lim_{t \to \infty} x_{c}(t)$ is finite. Thus, the above equation implies that $\norm{\nabla f(x(t))}$ is square-integrable w.r.t $t$. Hence, $\nabla f(x(t))$ is bounded above. 

Since $\nabla f(x(t))$ is bounded and $x_{ci}(t) > 0$,  from~\eqref{eqn:x_evol} we have that $\Dot{x}(t)$ is bounded above. Now, the time-derivative of $\norm{\nabla f}^2$ along the trajectories $x(t)$ is given by
\begin{align*}
    \dfrac{d}{dt} \norm{\nabla f(x(t))}^2 = 2\nabla f(x(t))^T \nabla^2 f(x(t)) \Dot{x}(t).
\end{align*}
We have shown that $\nabla f(x(t))$ and $\Dot{x}(t)$ are bounded above. From Assumption~\ref{assump_2}, we have all the entries in $\nabla^2 f(x(t))$ bounded above. Then, from the above equation we have $\dfrac{d}{dt} \norm{\nabla f(x(t))}^2$ bounded above. Thus, $\norm{\nabla f(x(t))}^2$ is uniformly continuous.

We have shown that, $\norm{\nabla f(x(t))}$ is square-integrable and $\norm{\nabla f(x(t))}^2$ is uniformly continuous. From Barbalat's lemma~\cite{barbalat1959systemes} it follows that $\lim_{t \to \infty} \norm{\nabla f(x(t))}^2 = 0$. This proves~\eqref{eqn:zero_grad_1}.

\end{proof}

Theorem~\ref{thm:adagrad} implies that the G-AdaGrad algorithm, proposed in~\eqref{eqn:xc_evol}-\eqref{eqn:x_evol}, converges to a {\em critical point} in $X^*$ of the non-convex optimization problem~\eqref{eqn:opt_1}. Furthermore,~\eqref{eqn:ft} implies that the convergence of G-AdaGrad is affected by the algorithm parameter $\alpha$. As we will show through simulations in Section~\ref{sec:exp}, $\alpha=0.5$, which corresponds to the original AdaGrad method~\cite{duchi2011adaptive}, is not the optimal value of $\alpha$. 

Another significance of the above proof is that, it explains why the exponent $\alpha$ of $x_c(t)$ (equivalently, $b_k$ in discrete-time) in the update equation of the estimate $x(t)$ is limited to $\alpha < 1$. If $\alpha > 1$,~\eqref{eqn:ft} implies that $f(x(t))$ will be increasing in $t$. If $\alpha = 1$, evaluating the integral in~\eqref{eqn:f_int} we have
\begin{align*}
   & f(x(t)) = f(x(0)) + \sum_{i=1}^d \log \left(\frac{x_{ci}(0)}{x_{ci}(t)}\right).
\end{align*}
Thus, $f$ decreases at a slower rate for $\alpha = 1$ compared to $\alpha < 1$, because of logarithmic decrements in case of $\alpha =1$ compared to exponential decrements.

Note that, the parameter $\epsilon$ in~\cite{li2019convergence} plays the same role as $\alpha$. However, the convergence results in~\cite{li2019convergence} is only for $\epsilon \in (0,0.5]$ which corresponds to $\alpha \in (0.5,1]$. Thus, our analysis is more general compared to~\cite{li2019convergence}. In addition, our analysis in Theorem~\ref{thm:adagrad} explains the significance of the parameter $\alpha$, as discussed in the previous paragraph.
