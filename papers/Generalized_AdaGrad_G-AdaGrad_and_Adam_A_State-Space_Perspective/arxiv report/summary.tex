\section{Conclusion}

We proposed a fast optimizer, named Generalized AdaGrad or G-AdaGrad, a generalization of the prototypical AdaGrad algorithm. 
We acquired state-space frameworks of the G-AdaGrad algorithm and the Adam algorithm, governed by a set of ordinary differential equations. From the proposed state-space viewpoint, we presented simple convergence proofs of G-AdaGrad and Adam for non-convex optimization problems. Our analysis of G-AdaGrad provided further insights into the AdaGrad method. The theoretical results have been validated empirically on the \textit{MNIST} dataset. Future work involves analyzing variations of Adam, such as AdaShift~\cite{zhou2018adashift}, Nadam~\cite{dozat2016incorporating}, AdaMax~\cite{kingma2014adam}, that do not have theoretical guarantees, in the state-space framework proposed in this paper.