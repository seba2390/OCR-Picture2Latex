\section{Introduction}
\label{sec:intro}

In this paper, we consider the minimization problem
\begin{align}
    \min_{x \in \R^d} f(x), \label{eqn:opt_1}
\end{align}
where the {\em objective function} $f: \R^d \to \R^+$ is smooth and possibly non-convex. In machine learning, $f$ is typically approximated by the average of a large number of loss functions, each loss function being associated with individual training examples or mini-batches, and $x$ typically represents the unknown weights of a model. The goal is to find a {\em critical point} of the aggregate loss function $f$.

Several first-order iterative methods exist for solving the optimization problem~\eqref{eqn:opt_1}. First-order methods are preferred when the available data-size is large~\cite{bottou2018optimization}. Besides expensive computations, another drawback of second-order methods, such as Newton's method~\cite{kelley1999iterative}, is that for linear models these methods cannot be implemented over a distributed network, as the agents do not share their data points with the server~\cite{chakrabarti2020iterative}.

The classical gradient-descent method is the basic prototype of first-order optimization methods~\cite{bertsekas1989parallel}. Its stochastic version, known as the stochastic gradient-descent (SGD), has become a popular method for solving machine learning problems, especially the large-scale problems~\cite{bottou2018optimization}. Several accelerated variants of SGD have been proposed since the past decade~\cite{duchi2011adaptive, kingma2014adam, zeiler2012adadelta, reddi2019convergence, dozat2016incorporating}. Two of such notable methods are the adaptive gradient-descent method (AdaGrad)~\cite{duchi2011adaptive}, and the adaptive momentum estimation method (Adam)~\cite{kingma2014adam}. Both AdaGrad and Adam methods maintain an estimate of a local minima in~\eqref{eqn:opt_1} and update it iteratively using the gradient of the {\em objective function} multiplied by an adaptive {\em learning rate}.

AdaGrad is a prominent optimization method that achieves significant performance gains compared to SGD. As the name suggests, AdaGrad adaptively updates the {\em learning rate} based on the information of all the previous gradients. Specifically~\cite{duchi2011adaptive}, for each iteration $k \in \{0,1,\ldots\}$, let $x_k = [x_{k,1},\ldots,x_{k,d}]^T$ denote the estimate of a local minima in~\eqref{eqn:opt_1} maintained by AdaGrad. In addition, AdaGrad maintains a set of real valued scalar parameters denoted by $\{b_{k,i}: i=1,\ldots,d\}$. The algorithm is initialized with arbitrarily chosen initial estimate $x_0 \in \R^d$ and $\{b_{k,i} > 0: i=1,\ldots,d\}$. Let the gradient of the {\em objective function} evaluated at $x \in \R^d$ be denoted as $\nabla f(x) \in \R^d$, and its $i$-th element in be denoted as $\nabla_i f(x)$ for each dimension $i \in  \{1,\ldots,d\}$. At each iteration $k \in \{0,1,\ldots\}$, each of the parameters $\{b_{k,i}: i=1,\ldots,d\}$ are updated according to $b_{k+1,i}^2 = b_{k,i}^2 + \norm{\nabla_i f(x_k)}^2$. At the same iteration $k$, the estimate is updated to $x_{k+1,i} = x_{k,i} - \eta \frac{\nabla_i f(x_k)}{b_{k+1,i}}$ for each $i \in  \{1,\ldots,d\}$. The real valued scalar parameter $\eta > 0$ is called the {\em step-size}. Thus, the {\em learning rate} in AdaGrad is adaptively weighted along each dimension by the sum of squares of the past gradients. AdaGrad has been shown to particularly effective for sparse gradients~\cite{NIPS2013_2812e5cf}, but has under-performed for some applications~\cite{wilson2017marginal}.

The Adam algorithm has been observed to compare favorably with other optimization methods for a wide range of optimization problems, including deep learning~\cite{radford2015unsupervised, peters2018deep, wu2016google}. Like AdaGrad, Adam also updates the {\em learning rate} based on the information of past gradients. However, unlike AdaGrad, Adam effectively updates the {\em learning rate} based on only a moving window of the past gradients. Specifically~\cite{kingma2014adam}, Adam maintains two sets of $d$-dimensional vectors, respectively denoted by $\mu_k = [\mu_{k,1},\ldots,\mu_{k,d}]^T$ and $v_k = [v_{k,1},\ldots,v_{k,d}]^T$. $\mu_k$ and $v_k$ are respectively known as the biased first moment estimate and biased second raw moment estimate. These vectors are initialized with $\mu_0 = 0_d$ and $\{v_{k,i} > 0: i=1,\ldots,d\}$. Three parameters $\eta > 0$, $\beta_1 \in [0,1)$, and $\beta_2 \in [0,1)$ are chosen before the iterations begin. At each iteration $k \in \{0,1,\ldots\}$, the vectors $\mu_k$ and $v_k$ are updated according to $\mu_{k+1,i} = \beta_1 \mu_{k,i} + (1-\beta_1) \nabla_i f(x_k)$ and $v_{k+1,i} = \beta_2 v_{k,i} + (1-\beta_2) \norm{\nabla_i f(x_k)}^2$ along each dimension $i \in  \{1,\ldots,d\}$. Next, the estimate $x_k$ is updated to $x_{k+1,i} = x_{k,i} - \eta \frac{\sqrt{1-\beta_2^k}}{1-\beta_1^k} \frac{\mu_{k+1,i}}{\sqrt{v_{k+1,i}}}$ for each $i \in  \{1,\ldots,d\}$. The factor $\frac{\sqrt{1-\beta_2^k}}{1-\beta_1^k}$ is responsible for the initial bias correction, as proposed in the original Adam algorithm~\cite{kingma2014adam}.  Thus, the learning rate in Adam is weighted by the exponentially moving averages of the past gradients.

Several algorithms have been proposed to improve upon the convergence of the Adam method, such as AdaShift~\cite{zhou2018adashift}, Nadam~\cite{dozat2016incorporating}, AdaMax~\cite{kingma2014adam}. Although these algorithms have demonstrated good performance in practice, they do not have theoretical convergence guarantees. While AMSGrad has been shown to perform better than Adam on CIFAR-10 dataset~\cite{reddi2019convergence}, other experiments suggest AMSGrad be similar or worse than Adam. The recently proposed AdaBelief~\cite{zhuang2020adabelief} is another variation of Adam with a theoretical convergence guarantee. Note that the RMSprop method is a special case of Adam with the parameter $\beta_1=0$~\cite{reddi2018adaptive}.

We aim to present simplified proofs of convergence of the AdaGrad and Adam algorithms to a {\em critical point} for non-convex {\em objective functions} in the deterministic settings. The first convergence guarantee of a generalized AdaGrad method for non-convex functions was proved recently in~\cite{li2019convergence}, where the additional parameter $\epsilon \geq 0$ generalizes the AdaGrad method. However, the parameter $\epsilon$ in~\cite{li2019convergence} has been assumed to be strictly positive for the convergence guarantee, which excludes the case of the original AdaGrad method~\cite{duchi2011adaptive} where $\epsilon = 0$. We first propose a more general AdaGrad model, coined G-AdaGrad, that subsumes the work in~\cite{li2019convergence}.  Our model and corresponding convergence proof allow the parameter $\epsilon$ to be negative, as well as the case of the original AdaGrad. Besides, our proof provides intuition behind how this generalization of AdaGrad impacts its convergence. The analysis for AdaGrad in~\cite{defossez2020simple} assumes the gradients to be uniformly bounded. We do not make such an assumption. Other works also analyze the convergence of AdaGrad-like algorithms for non-convex objective functions, notable among them being WNGrad~\cite{wu2018wngrad} and AdaGrad-Norm~\cite{ward2019adagrad}. 
Note that all of the aforementioned analyses of AdaGrad and AdaGrad-like algorithms are in discrete-time. We analyze AdaGrad in the continuous-time domain.

Previous works that demonstrate convergence of the Adam algorithm for non-convex {\em objective functions} include~\cite{reddi2018adaptive, de2018convergence, tong2019calibrating, barakat2020convergence, chen2018convergence, barakat2021convergence}. In~\cite{reddi2018adaptive}, the proof for Adam is provided when the algorithm parameter $\beta_1 = 0$. We consider the general parameter settings where $\beta_1 \geq 0$. An Adam-like algorithm has been proposed and analyzed in~\cite{defossez2020simple}. The proofs in~\cite{reddi2018adaptive, de2018convergence, tong2019calibrating, barakat2020convergence, chen2018convergence} do not consider the initial bias correction steps in the original Adam~\cite{kingma2014adam}. Our analysis of Adam considers the bias correction steps. The analyses in~\cite{defossez2020simple, reddi2018adaptive, de2018convergence, tong2019calibrating, chen2018convergence}
assume uniformly bounded gradients. We do not make such an assumption. The aforementioned analyses of Adam are in discrete-time. A continuous-time version of Adam has been proposed in~\cite{barakat2021convergence}, which includes the bias correction steps. However, compared to the convergence proof in~\cite{barakat2021convergence}, our proof for Adam is simpler. In addition,~\cite{barakat2021convergence} assumes that the parameters $\beta_1$ and $\beta_2$ in the Adam algorithm are functions of the {\em step-size} $\eta$ such that $\beta_1$ and $\beta_2$ tends to one as the {\em step-size} $\eta \to 0$. We do not make such an assumption in our analysis.


\subsection{Summary of Our Contributions}
\label{sub:contri}

\begin{itemize}
    \item In this paper, we first propose a more general AdaGrad algorithm, which we refer to as Generalized AdaGrad (G-AdaGrad). The proposed optimizer improves upon the convergence rate of the original AdaGrad algorithm. The original AdaGrad, discussed in Section~\ref{sec:intro}, is a special case of the proposed G-AdaGrad algorithm.
    \item We propose two state-space models, each for the G-AdaGrad algorithm and the original Adam algorithm, in continuous time-domain. The proposed state-space models are an autonomous and non-autonomous system of ordinary differential equations, respectively, for G-AdaGrad and Adam. The non-autonomy of the model for Adam is due to initial bias correction steps.
    \item Using a simple analysis of the proposed state-space models, we prove the convergence of the G-AdaGrad and the Adam algorithm to a {\em critical point} of the possibly non-convex optimization problem~\eqref{eqn:opt_1} in the deterministic settings. Our analysis requires minimal assumptions about the optimization problem~\eqref{eqn:opt_1}.
\end{itemize}
 
Compared to the existing works that analyze the convergence of the AdaGrad or the Adam algorithm for non-convex {\em objective functions}, the \textbf{major contributions} of our presented analysis are as follows.

\begin{enumerate}
    \item The gradient $\nabla f$ need not be uniformly bounded, unlike~\cite{defossez2020simple, reddi2018adaptive, de2018convergence, tong2019calibrating, chen2018convergence}.
    \item Includes the original AdaGrad algorithm and a more generalized version with intuition behind the generalization, compared to~\cite{li2019convergence}.
    \item Explanation for the choice of exponent of $b_k$ in the estimation update of AdaGrad, unlike~\cite{li2019convergence}.
    \item Includes initial bias correction steps in Adam, unlike~\cite{reddi2018adaptive, de2018convergence, tong2019calibrating, barakat2020convergence, chen2018convergence}.
    \item Analysis in continuous-time domain, unlike~\cite{li2019convergence, defossez2020simple, reddi2018adaptive, de2018convergence, tong2019calibrating, barakat2020convergence, chen2018convergence}.
    \item $\beta_1$ and $\beta_2$ in Adam need not be functions of the step-size $\eta$, unlike~\cite{barakat2021convergence}. 
    \item Simple proof of convergence, compared to the continuous-time version in~\cite{barakat2021convergence}.
\end{enumerate}







