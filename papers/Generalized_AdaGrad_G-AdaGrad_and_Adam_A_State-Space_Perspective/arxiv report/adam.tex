\section{Continuous-Time Adam}
\label{sec:adam}

In this section, we propose a set of non-autonomous ordinary differential equations. Using first-order Euler discretization, the proposed set of differential equations coincides with the original Adam algorithm. 

\subsection{Description of Adam}
\label{sub:algo_adam}

In order to present our algorithm, define two real valued scalar parameters $\lambda_1 \in (0,1)$ and $\lambda_2 \in (0,1)$.
Define a function $\alpha: [1,\infty) \to \R$ as
\begin{align}
    \alpha(t) &= \dfrac{1-(1-\lambda_1)^t}{\sqrt{1-(1-\lambda_2)^t}}, \, t\geq 1. \label{eqn:alpha}
\end{align}
For each $i \in \{1,\ldots,d\}$ and $t \geq 1$, consider the following sets of differential equations
\begin{align}
    \Dot{\mu}_i(t) & = -\lambda_1 \mu_i(t) + \lambda_1 \nabla_i f(x(t)), \label{eqn:mu_evol} \\
    \Dot{v}_i(t) & = -\lambda_2 v_i(t) + \lambda_2 \norm{\nabla_i f(x(t))}^2, \label{eqn:v_evol} \\
    \Dot{x}_i(t) & = - \dfrac{1}{\alpha(t)} \dfrac{\mu_i(t)}{\sqrt{v_i(t)}}, \label{eqn:xm_evol}
\end{align}
with initial conditions $\mu(1) \in \R^d$, $v(1) \in \R^d$, and $x(1) \in \R^d$. We assume that the initial condition $\{v_i(1) > 0 : i = 1,\ldots,d\}$. The variables $\mu$ and $v$ can be abstracted as dynamic controller states. 

Following the argument in Section~\ref{sub:algo_adagrad}, the above pair of differential equations~\eqref{eqn:mu_evol}-\eqref{eqn:xm_evol} can be seen as a continuous-time variation of the following algorithm, when~\eqref{eqn:mu_evol}-\eqref{eqn:xm_evol} are discretized with a sampling time $\delta > 0$. For each $i \in \{1,\ldots,d\}$ and $k \in \{1,2,\ldots\}$,
\begin{align}
    \mu_i((k+1)\delta) & = (1-\delta \lambda_1) \mu_i(k\delta) + \delta \lambda_1 \nabla_i f(x(k\delta)), \label{eqn:mu_dscrt}\\
    v_i((k+1)\delta) & = (1-\delta \lambda_2) \mu_i(k\delta) + \delta \lambda_2 \norm{\nabla_i f(x(k\delta))}^2, \label{eqn:v_dscrt}\\
    x_{i}((k+1)\delta) & = x_{i}(k\delta) - \dfrac{\delta}{\alpha(k\delta)} \dfrac{\mu_i(k\delta)}{\sqrt{v_i(k\delta)}}.  \label{eqn:xm_dscrt}
\end{align}

Note that,~\eqref{eqn:mu_dscrt}-\eqref{eqn:xm_dscrt} represents the Adam algorithm discussed in Section~\ref{sec:intro}, with the parameters $\beta_1 = 1-\delta\lambda_1$, $\beta_2 = 1-\delta\lambda_2$ and {\em step-size} $\eta = \delta$. The term $\alpha(t)$ in~\eqref{eqn:xm_evol} of captures the initial bias corrections in Adam. This term renders the system of our differential equations~\eqref{eqn:mu_evol}-\eqref{eqn:xm_evol} as non-autonomous.

In the next subsection, we present the convergence of our proposed state-space model in~\eqref{eqn:mu_evol}-\eqref{eqn:xm_evol}.


\subsection{Convergence of Adam}
\label{sub:conv_adam}

Recall the definition of the set of {\em critical points} $X^*$ from~\eqref{eqn:xstar} in Section~\ref{sub:conv_adagrad}.
Theorem~\ref{thm:adam} below proves the convergence of the Adam algorithm~\eqref{eqn:mu_evol}-\eqref{eqn:xm_evol} in continuous-time domain to a {\em critical point} in $X^*$.


\begin{theorem} \label{thm:adam}
Consider the set of differential equations~\eqref{eqn:mu_evol}-\eqref{eqn:xm_evol} with initial conditions $\mu(1) = 0_d$, $v(1) \in \R^d$ and $x(1) \in \R^d$ such that $\{v_i(1) > 0 : i = 1,\ldots,d\}$. Let the parameters $\lambda_1$ and $\lambda_2$ satisfy
\begin{align}
    0 < \lambda_2 < \lambda_1 < 1. \label{eqn:lambda_cond}
\end{align}
If Assumptions~\ref{assump_1}-\ref{assump_2} hold, then $\lim_{t \to \infty} \nabla f(x(t)) = 0_d$.
\end{theorem}

\begin{proof}

The time-derivative of $f$ along the trajectories $x(t)$  of~\eqref{eqn:xm_evol} is given by
\begin{align*}
    \Dot{f}(x(t)) &= \sum_{i=1}^d \nabla_i f(x(t)) \Dot{x}_{i}(t) = - \sum_{i=1}^d \dfrac{\nabla_i f(x(t))}{\alpha(t)} \dfrac{\mu_i(t)}{\sqrt{v_i(t)}}.
\end{align*}
Multiplying with $\alpha(t)$ on both sides above we get
\begin{align*}
    \alpha(t)\Dot{f}(x(t)) = - \sum_{i=1}^d \nabla_i f(x(t)) \dfrac{\mu_i(t)}{\sqrt{v_i(t)}}.
\end{align*}
Upon integrating both sides above w.r.t. $t$ from $1$ to $t$ and substituting from~\eqref{eqn:mu_evol} we have
\begin{align}
    & \int_1^t \alpha(s)\Dot{f}(x(s))ds = - \sum_{i=1}^d \int_1^t \dfrac{(\Dot{\mu}_i(s) + \lambda_1 \mu_i(s)) \mu_i(s)}{\lambda_1 \sqrt{v_i(s)}}ds \nonumber\\
    &= - \sum_{i=1}^d \int_1^t \dfrac{\mu_i(s)\Dot{\mu}_i(s)}{\lambda_1 \sqrt{v_i(s)}}ds - \sum_{i=1}^d \int_1^t \dfrac{\mu_i(s)^2}{\sqrt{v_i(s)}}ds. \label{eqn:total_int}
\end{align}
Integrating by parts we have the first term on R.H.S. as
\begin{align*}
    & \int_1^t \dfrac{\mu_i(s)\Dot{\mu}_i(s)}{\sqrt{v_i(s)}}ds
    = \left[\dfrac{\mu_i(s)^2}{2\sqrt{v_i(s)}}\right]_1^t + \dfrac{1}{4} \int_1^t \mu_i(s)^2 v_i(s)^{-1.5} \Dot{v}_i(s) ds.
\end{align*}
Upon substituting above from~\eqref{eqn:v_evol}, and using that $\mu(1)=0_d$ we have
\begin{align*}
    \int_1^t \dfrac{\mu_i(s)\Dot{\mu}_i(s)}{\sqrt{v_i(s)}}ds
    &= \dfrac{\mu_i(t)^2}{2\sqrt{v_i(t)}} - \dfrac{\lambda_2}{4}\int_1^t \mu_i(s)^2 v_i(s)^{-0.5} ds + \dfrac{\lambda_2}{4}\int_1^t \mu_i(s)^2 v_i(s)^{-1.5} \norm{\nabla_i f(x(s))}^2 ds.
\end{align*}
Upon substituting above in~\eqref{eqn:total_int} we obtain that
\begin{align}
    \int_1^t \alpha(s)\Dot{f}(x(s))ds
    = & - \sum_{i=1}^d \dfrac{\mu_i(t)^2}{2\lambda_1\sqrt{v_i(t)}} - \sum_{i=1}^d \left(1-\dfrac{\lambda_2}{4\lambda_1}\right)\int_1^t \mu_i(s)^2 v_i(s)^{-0.5} ds \nonumber \\
    & - \sum_{i=1}^d \dfrac{\lambda_2}{4\lambda_1}\int_1^t \mu_i(s)^2 v_i(s)^{-1.5} \norm{\nabla_i f(x(s))}^2 ds. \label{eqn:int_expand}
\end{align}

We define, $\gamma_1 = 1-\lambda_1$ and $\gamma_2 = 1-\lambda_2$.
Upon differentiating both sides of~\eqref{eqn:alpha} w.r.t $t$ we get $\Dot{\alpha}(t) = \dfrac{\gamma_2^t(1-\gamma_1^t)\log{\gamma_2} - 2\gamma_1^t(1-\gamma_2^t)\log{\gamma_1}}{2(1-\gamma_2^t)^{1.5}}$.
So we have
\begin{align}
    \Dot{\alpha}(t) < 0 \iff \left(\dfrac{\gamma_2}{\gamma_1}\right)^t \dfrac{1-\gamma_1^t}{1-\gamma_2^t} > 2\dfrac{\log{\gamma_1}}{\log{\gamma_2}}. \label{eqn:cond}
\end{align}
From the condition $\lambda_1 > \lambda_2$ in~\eqref{eqn:lambda_cond}, we have $1 > \gamma_2 > \gamma_1 > 0$. Then, $\left(\dfrac{\gamma_2}{\gamma_1}\right)^t$ and $\dfrac{1-\gamma_1^t}{1-\gamma_2^t}$ are, respectively, increasing and decreasing functions of $t$. Since $1 > \gamma_2 > \gamma_1 > 0$, we have $\lim_{t \to \infty} \left(\dfrac{\gamma_2}{\gamma_1}\right)^t \to \infty$ and $\lim_{t \to \infty} \dfrac{1-\gamma_1^t}{1-\gamma_2^t} = 1$. Thus, $\left(\dfrac{\gamma_2}{\gamma_1}\right)^t \dfrac{1-\gamma_1^t}{1-\gamma_2^t}$ is an increasing function of $t$. Then, there exists $T < \infty$ such that~\eqref{eqn:cond} holds for all $t \geq T$. 
Integrating by parts we rewrite the L.H.S. in~\eqref{eqn:int_expand} as
\begin{align*}
    & \int_1^t \alpha(s)\Dot{f}(x(s))ds = \left[\alpha(s)f(x(s))\right]_1^t - \int_1^t \Dot{\alpha}(s)f(x(s))ds.
\end{align*}
Upon substituting from above in~\eqref{eqn:int_expand}, for $t \geq T$,
\begin{align}
    & \alpha(t)f(x(t)) + \sum_{i=1}^d \dfrac{\mu_i(t)^2}{2\lambda_1\sqrt{v_i(t)}} = \alpha(1)f(x(1)) + \int_1^{T} \Dot{\alpha}(s)f(x(s))ds + \int_{T}^t \Dot{\alpha}(s)f(x(s))ds \nonumber \\
    & - \sum_{i=1}^d \left(1-\dfrac{\lambda_2}{4\lambda_1}\right)\int_1^t \mu_i(s)^2 v_i(s)^{-0.5} ds - \sum_{i=1}^d \dfrac{\lambda_2}{4\lambda_1}\int_1^t \mu_i(s)^2 v_i(s)^{-1.5} \norm{\nabla_i f(x(s))}^2 ds. \label{eqn:alphaV}
\end{align}
Due to~\eqref{eqn:lambda_cond} and~\eqref{eqn:cond}, the R.H.S. in~\eqref{eqn:alphaV} is decreasing in $t \geq T$. Then, the L.H.S. in~\eqref{eqn:alphaV} is also decreasing in $t \geq T$. From~\eqref{eqn:v_evol} and $v_i(1) > 0$, $v_i(t) > 0$. Since $\mu_i(t)$ and $v_i(t)$ are continuous and $v_i(t) > 0$, $\dfrac{\mu_i(t)^2}{2\lambda_1\sqrt{v_i(t)}}$ is continuous. Also, $\alpha(t)f(x(t))$ is continuous. Thus, considering the compact interval $[1,T]$, $\alpha(T)f(x(T)) + \sum_{i=1}^d \dfrac{\mu_i(T)^2}{2\lambda_1\sqrt{v_i(T)}} =: M_T$ is finite. Since the L.H.S. in~\eqref{eqn:alphaV} is decreasing in $t \geq T$, we have the L.H.S. in~\eqref{eqn:alphaV} bounded above by $M_T$ for all $t \geq T$. 

% Integrating by parts we rewrite the L.H.S. above as
% \begin{align}
%     & \int_1^t \alpha(s)\Dot{f}(x(s))ds = \left[\alpha(s)f(x(s))\right]_1^t - \int_1^t \Dot{\alpha}(s)f(x(s))ds. \label{eqn:lhs}
% \end{align}
% Define, $\gamma_1 = 1-\lambda_1$ and $\gamma_2 = 1-\lambda_2$.
% Upon differentiating both sides of~\eqref{eqn:alpha} w.r.t $t$ we get
% \begin{align*}
%     \Dot{\alpha}(t) = \dfrac{\gamma_2^t(1-\gamma_1^t)\log{\gamma_2} - 2\gamma_1^t(1-\gamma_2^t)\log{\gamma_1}}{2(1-\gamma_2^t)^{1.5}}.
% \end{align*}
% So we have
% \begin{align}
%     \Dot{\alpha}(t) < 0 \iff \left(\dfrac{\gamma_2}{\gamma_1}\right)^t \dfrac{1-\gamma_1^t}{1-\gamma_2^t} > 2\dfrac{\log{\gamma_1}}{\log{\gamma_2}}. \label{eqn:cond}
% \end{align}
% From the condition $\lambda_1 > \lambda_2$ in~\eqref{eqn:lambda_cond}, we have $1 > \gamma_2 > \gamma_1 > 0$. Then, $\left(\dfrac{\gamma_2}{\gamma_1}\right)^t$ and $\dfrac{1-\gamma_1^t}{1-\gamma_2^t}$ are, respectively, increasing and decreasing functions of $t$. Since $1 > \gamma_2 > \gamma_1 > 0$, we have $\lim_{t \to \infty} \left(\dfrac{\gamma_2}{\gamma_1}\right)^t \to \infty$ and $\lim_{t \to \infty} \dfrac{1-\gamma_1^t}{1-\gamma_2^t} = 1$. Thus, $\left(\dfrac{\gamma_2}{\gamma_1}\right)^t \dfrac{1-\gamma_1^t}{1-\gamma_2^t}$ is an increasing function of $t$. Then, there exists $T_1 < \infty$ such that~\eqref{eqn:cond} holds for all $t \geq T_1$.
% From the condition $\lambda_1 > \frac{\lambda_2}{4}+0.5$ in~\eqref{eqn:lambda_cond}, we have $\left(1-\dfrac{\lambda_2}{4\lambda_1}\right) > \dfrac{1}{2\lambda_1}$. Note that, there exists sufficiently large
% $T_2 < \infty$ such that $\int_1^t \mu_i(s)^2 v_i(s)^{-0.5} ds \geq \mu_i(t)^2 v_i(t)^{-0.5}$ for all $t \geq T_2$.
% So we have, for all $i \in \{1,\ldots,d\}$ and $t \geq T_2$,
% \begin{align}
%     \left(1-\dfrac{\lambda_2}{4\lambda_1}\right)\int_1^t \mu_i(s)^2 v_i(s)^{-0.5} ds > \dfrac{1}{2\lambda_1} \mu_i(t)^2 v_i(t)^{-0.5}. \label{eqn:int_dom}
% \end{align}
% Since the above holds for all $t \geq T_2$, we can choose $T_2 \geq T_1$.
% Upon substituting from~\eqref{eqn:lhs} in~\eqref{eqn:int_expand}, for all $t \geq T_2$ we have
% \begin{align}
%     & \alpha(t)f(x(t)) \nonumber \\
%     & = \alpha(1)f(x(1)) + \int_1^{T_2} \Dot{\alpha}(s)f(x(s))ds + \int_{T_2}^t \Dot{\alpha}(s)f(x(s))ds \nonumber \\
%     & - \sum_{i=1}^d \dfrac{\mu_i(t)^2}{2\lambda_1\sqrt{v_i(t)}} - \sum_{i=1}^d \left(1-\dfrac{\lambda_2}{4\lambda_1}\right)\int_1^t \mu_i(s)^2 v_i(s)^{-0.5} ds \nonumber \\
%     & - \sum_{i=1}^d \dfrac{\lambda_2}{4\lambda_1}\int_1^t \mu_i(s)^2 v_i(s)^{-1.5} \norm{\nabla_i f(x(s))}^2 ds. \label{eqn:alphaV}
% \end{align}
% We have shown that~\eqref{eqn:cond} holds for all $t \geq T_1$. Thus,~\eqref{eqn:cond} holds for all $t \geq T_2$, which means $\Dot{\alpha}(t)<0$ for all $t \geq T_2$.  Thus, combining with~\eqref{eqn:int_dom}, the R.H.S. in~\eqref{eqn:alphaV} is decreasing in $t$, for all $t \geq T_2$. Then, from~\eqref{eqn:alphaV} we have that $\alpha(t)f(x(t))$ is decreasing in $t$, hence bounded above, for all $t \geq T_2$. 

From~\eqref{eqn:alphaV} then we have that $\mu_i(t)^2 v_i(t)^{-1.5} \norm{\nabla_i f(x(t))}^2$ and $\mu_i(t)^2 v_i(t)^{-0.5}$ are integrable w.r.t. $t$ and bounded above. It implies that, $\norm{\nabla_i f(x(t))}$ is bounded unless $\mu_i(t) = 0$ or $v_i(t) = \infty$. From~\eqref{eqn:xm_evol}, either of the conditions $\mu_i(t) = 0$ and $v_i(t) = \infty$ implies that $\dot{x}_i(t) = 0$ and, hence, $\frac{d}{dt}\nabla_i f(x(t)) = 0$. Due to continuity of $\nabla_i f$ and $\norm{\nabla f(x(1))} < \infty$, we then have $\norm{\nabla_i f(x(t))}$ is bounded above for all $t$.
Integrating both sides of~\eqref{eqn:mu_evol} and~\eqref{eqn:v_evol} w.r.t $t$ from $1$ to $t$, we have for $i \in \{1,\ldots,d\}$,
\begin{align*}
    \mu_i(t) & = \lambda_1 \int_1^t e^{-\lambda_1(t-s)} \nabla_i f(x(s)) ds, \\
    v_i(t) & = \lambda_2 \int_1^t e^{-\lambda_2(t-s)} \norm{\nabla_i f(x(s))}^2 ds + v_i(1)e^{-\lambda_2t}.
\end{align*}
Since $\norm{\nabla f(x(t))}$ is bounded above and $\lambda_1,\lambda_2 > 0$, the above equations implies that $\mu_i(t)$ and $v_i(t)$ are bounded above. Moreover, $v_i(t) > 0$ as $v_i(1) > 0$. From~\eqref{eqn:xm_evol} then we have, $\Dot{x}(t)$ is bounded above. From~\eqref{eqn:mu_evol} and~\eqref{eqn:xm_evol}, $\mu_i(t) = 0$ implies that $\dot{\mu}_i(t) = \lambda_1 \nabla_i f(x(t))$ and $\dot{x}_i(t) = 0$. Thus, $\mu_i(t)$ can be zero only at isolated points $t$. Otherwise, for some $h>0$ there exists an interval $(t-h,t+h)$ such that $\mu_i(s) = 0$ for all $s \in (t-h,t+h)$. In that case, $\dot{\mu}_i(s) = 0$ for all $s \in (t-h,t+h)$. Since $\dot{\mu}_i(s) = \lambda_1 \nabla_i f(x(s))$ for all $s \in (t-h,t+h)$, we then have $\nabla_i f(x(s)) = 0$ for all $s \in (t-h,t+h)$, which proves the theorem.

We have shown above that $\mu_i(t) = 0$ only at isolated points and $v_i(t)$ is bounded above. So, $\dfrac{1}{\mu_i(t)^2 v_i(t)^{-0.5}}$ is bounded above except at isolated points.
Since $\mu_i(t)^2 v_i(t)^{-0.5}$ is integrable and $\dfrac{1}{\mu_i(t)^2 v_i(t)^{-0.5}}$ is bounded above except at isolated points, we have $\dfrac{1}{\mu_i(t)^2 v_i(t)^{-0.5}}$ is integrable. Since $v_i(t)$ is bounded above and $\dfrac{1}{\mu_i(t)^2 v_i(t)^{-0.5}}$ is integrable, we have $\dfrac{1}{\mu_i(t)^2 v_i(t)^{-1.5}}$ integrable.
Now, we apply Cauchy-Schwartz inequality on the functions $\mu_i(t) v_i(t)^{-0.75} \norm{\nabla_i f(x(t))}$ and $\dfrac{1}{\mu_i(t) v_i(t)^{-0.75}}$. Since we have $\mu_i(t)^2 v_i(t)^{-1.5} \norm{\nabla_i f(x(t))}^2$ and $\dfrac{1}{\mu_i(t)^2 v_i(t)^{-1.5}}$ integrable, the Cauchy-Schwartz inequality implies that $\norm{\nabla_i f(x(t))}$ is integrable. Since $\norm{\nabla_i f(x(t))}$ is bounded and integrable, it is also square-integrable. Thus, $\norm{\nabla f(x(t)}^2$ is integrable.

So we have shown that $\norm{\nabla f(x(t)}^2$ is integrable and $\Dot{x}(t)$ is bounded above.
Following the same argument in last two paragraphs in the proof of Theorem~\ref{thm:adagrad}, under Assumption~\ref{assump_2}, we conclude that $\lim_{t \to \infty} \nabla f(x(t)) = 0_d$.
\end{proof}


























% Now, the second derivative of $f$ w.r.t $t$ along the trajectories $x(t)$ is given by
% \begin{align}
%     \Ddot{f}(x(t)) & = (\nabla f(x(t))^T \Ddot{x}(t) + \Dot{x}(t)^T \nabla^2 f(x(t)) \Dot{x}(t). \label{eqn:ftt}
% \end{align}
% Differentiating both sides of~\eqref{eqn:x_evol} w.r.t. $t$, we have
% \begin{align}
%     \Ddot{x}_i(t) = & - \dfrac{\partial^2 f(x(t))}{\partial x \partial x_i} \Dot{x}(t)\dfrac{1}{\left(x_{ci}(t)\right)^\alpha}\nonumber \\ & + \alpha \nabla_i f(x(t)) x_{ci}(t)^{-(1+\alpha)}\Dot{x}_{ci}(t). \label{eqn:xtt}
% \end{align}
% We have shown that $\nabla f(x(t))$, $\Dot{x}(t)$, and $x_c(t)$ are bounded above. As $\nabla f(x(t))$ is bounded, $x(t)$ is also bounded above. From Assumption~\ref{assump_3}, then we have all the entries in $\nabla^2 f(x(t))$ bounded above. Thus, from~\eqref{eqn:xtt} and~\eqref{eqn:ftt} it follows that $\Ddot{x}(t)$ and $\Ddot{f}(x(t))$ are bounded above.

% Since we have shown that $\lim_{t \to \infty} x_{c}(t)$ is finite and $\Ddot{f}(x(t))$ is bounded above, from Barbarat's lemma~\cite{khalil_2002} we obtain that $\lim_{t \to \infty} \nabla f(x(t)) = 0_d$.


% Consider an arbitrary critical point $x^* \in X^*$. Define a function $V:\R^d \to \R$ such that
% \begin{align}
%     V(y) = f(y+x^*) - f^*, ~ \forall y \in \R^d. \label{eqn:vdef}
% \end{align}
% From Assumption~\ref{assump_1}, it follows that $V$ is continuously differentiable in $\R^d$.
% Then, $\nabla V(0_d) = \nabla f(x^*)$. From the definition~\eqref{eqn:xstar} then we have 
% \begin{align}
%     \nabla V(0_d) = 0_d. \label{eqn:zero_grad}
% \end{align}
% From Assumption~\ref{assump_2} and the definition of $V$ in~\eqref{eqn:vdef}, we have $V(y) \geq 0$ for all $y \in \R^d$.

% Define,
% \begin{align}
%     z(t) = x(t)-x^*, \, \forall t \geq 0.
% \end{align}