\begin{figure*}[htb!]
\centering
\begin{subfigure}{.5\textwidth}
  \begin{center}
  \includegraphics[width = \textwidth]{mnist_alpha.jpg}
  \caption{Generalized AdaGrad}
  \label{fig:mnist_adagrad}
  \end{center}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \begin{center}
  \includegraphics[width = \textwidth]{mnist_ctadam.jpg}
  \caption{Adam}
  \label{fig:mnist_adam}
  \end{center}
\end{subfigure}
\caption{\footnotesize{\it Optimality gap $\frac{1}{2}\norm{Ax(t)-B}^2-f^*$ for linear regression problem with MNIST dataset, under the algorithms (a) G-AdaGrad and (b) Adam. For the G-AdaGrad algorithm, $x_c(0) = x(0) = [0.01,\ldots,0.01]^T$, and $\alpha$ is represented by different colors. For the Adam algorithm, $\mu(1) = [0,\ldots,0]^T$, $v(1) = x(1) = [0.01,\ldots,0.01]^T$, $\lambda_2 = 0.0067$, and $\lambda_1$ is represented by different colors.}}
\label{fig:mnist}
\end{figure*}

\begin{figure*}[htb!]
\centering
\begin{subfigure}{.5\textwidth}
  \begin{center}
  \includegraphics[width = \textwidth]{scatter_plot.jpg}
  \caption{Training Data}
  \label{fig:train}
  \end{center}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \begin{center}
  \includegraphics[width = \textwidth]{scatter_test.jpg}
  \caption{Test Data}
  \label{fig:test}
  \end{center}
\end{subfigure}
\caption{\footnotesize{\it Decision boundary in the $a_1-a_2$ plane, obtained from training a linear regression model for classification of digit-1 and digit-5. All the data points from (a) MNIST training set and (b) MNIST test set are plotted in $a_1-a_2$ plane. Digit-1 and digit-5 are represented by different colors.}}
\label{fig:scatter}
\end{figure*}

\begin{figure*}[htb!]
\centering
\begin{subfigure}{.5\textwidth}
  \begin{center}
  \includegraphics[width = \textwidth]{ctadagrad.jpg}
  \caption{Generalized AdaGrad}
  \label{fig:log_adagrad}
  \end{center}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \begin{center}
  \includegraphics[width = \textwidth]{ctadam.jpg}
  \caption{Adam}
  \label{fig:log_adam}
  \end{center}
\end{subfigure}
\caption{\footnotesize{\it Training loss of logistic regression model for classifying digit-1 and digit-5 from the MNIST dataset, under the algorithms (a) G-AdaGrad and (b) Adam. For the G-AdaGrad algorithm, $x_c(0) = x(0) = [0.01,\ldots,0.01]^T$, and $\alpha$ is represented by different colors. For the Adam algorithm, $\mu(1) = [0,\ldots,0]^T$, $v(1) = x(1) = [0.01,\ldots,0.01]^T$, $\lambda_2 = 0.0067$, and $\lambda_1$ is represented by different colors.}}
\label{fig:logreg}
\end{figure*}

\section{Experimental Results}
\label{sec:exp}

In this section, we present our experimental results validating the convergence guarantees from Section~\ref{sub:conv_adagrad} and Section~\ref{sub:conv_adam}. We consider the problem of recognizing handwritten digit one and digit five. 

Although it is a binary classification problem between the digits one and five, we solve it as a regression problem first. The obtained linear regression model can be a good initial decision boundary (ref. Fig.~\ref{fig:scatter}) for classification algorithms.
We conduct experiments for minimizing the {\em objective function} $f(x)=\frac{1}{2}\norm{Ax-B}^2$. The training data points $(A,B)$ are obtained from the \textit{``MNIST''}~\cite{MNIST} dataset as follows. We select $5000$ arbitrary training instances labeled as either the digit one or the digit five. For each instance, we calculate two quantities, namely the average intensity of an image and the average symmetry of an image~\cite{abu2012learning}. Let the column vectors $a_1$ and $a_2$ respectively denote the average intensity and the average symmetry of those $5000$ instances. We perform a quadratic {\em feature transform} of the data $(a_1,a_2)$. Then, our input matrix before pre-processing is $\Tilde{A} = \begin{bmatrix} a_1 & a_2 & a_1.^2 & a_1.*a_2 & a_2.^2 \end{bmatrix}$. Here, $(.*)$ represents element-wise multiplication and $(.^2)$ represents element-wise squares. This raw input matrix $\Tilde{A}$ is then pre-processed as follows. Each column of $\Tilde{A}$ is shifted by the mean value of the corresponding column and then divided by the standard deviation of that column. Finally, a $5000$-dimensional column vector of unity is appended to this pre-processed matrix. This is our final input matrix $A$ of dimension $(5000 \times 6)$. Next we consider the logistic regression model and conduct experiments for minimizing the cross-entropy error on the raw training data.

We train both of these models with the G-AdaGrad algorithm~\eqref{eqn:xc_evol}-\eqref{eqn:x_evol} and the Adam algorithm~\eqref{eqn:mu_evol}-\eqref{eqn:xm_evol}. We initialize the algorithms according to the conditions in Theorem~\ref{thm:adagrad} and Theorem~\ref{thm:adam}. Specifically, we initialize the G-AdaGrad algorithm with $x_c(0) = x(0) = [0.01,\ldots,0.01]^T$, and the Adam algorithm with $\mu(1) = [0,\ldots,0]^T$, $v(1) = x(1) = [0.01,\ldots,0.01]^T$. Moreover, we set $\lambda_2 = 0.0067$ for Adam.

G-AdaGrad converges for different values of $\alpha$ (ref. Fig.~\ref{fig:mnist_adagrad} and Fig.~\ref{fig:log_adagrad}). We observe that the convergence is faster when $\alpha$ is smaller. Thus, the coefficient $\alpha = 0.5$, which corresponds to the original AdaGrad method, is not the optimal choice. In addition, $\alpha=1$ leads to poor convergence, as we have theoretically explained in Section~\ref{sub:conv_adagrad}.

Fig.~\ref{fig:mnist_adam} and Fig.~\ref{fig:log_adam} show the effect of the relative values of $\lambda_1$ and $\lambda_2$ on the convergence of Adam algorithm. The standard choices for $\beta_1$ and $\beta_2$ in discrete-time Adam are respectively $0.9$ and $0.999$~\cite{kingma2014adam}. With a sampling time $\delta = 0.15$, from the relation between discrete-time and continuous-time Adam we have $\lambda_1 = 0.67$ and $\lambda_2 = 0.0067$ (ref. Section~\ref{sub:algo_adam}). Thus, our result in Fig.~\ref{fig:mnist_adam} and Fig.~\ref{fig:log_adam} agrees with the standard choices of these two parameters. A smaller or larger $\frac{\lambda_1}{\lambda_2}$ leads to oscillations or slows down the convergence. Note that, the condition~\eqref{eqn:lambda_cond} in Theorem~\ref{thm:adam} is satisfied with these standard parameter values. 