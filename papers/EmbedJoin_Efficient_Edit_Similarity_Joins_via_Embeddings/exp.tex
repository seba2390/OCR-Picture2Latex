\section{Experiments}
\label{sec:exp}

In this section we present our experimental studies.  After listing the datasets and tested algorithms, we first give an overview of the performance of \ebdjoin+. We then compare it with the existing best algorithms.  Finally, we show the scalability of \ebdjoin+ in the ranges that the existing best algorithms cannot reach.

\subsection{The Setup}
\label{sec:setup}


\paragraph{Datasets} 
We tested the algorithms in three publicly available real world datasets.

\medskip
\noindent \uniref: a dataset of UniRef90 protein sequence data from UniProt project.\footnote{Available in \url{http://www.uniprot.org/}
%\ (downloaded in Sep.\ 2016).
} Each sequence is an array of amino acids coded in uppercase letters. We first remove sequences whose lengths are smaller than 200, and then extract the first 400,000 protein sequences. 

\medskip
\noindent \trec:  a dataset of references from Medline (an online medical information database) consisting of titles and abstracts from 270 medical journals.\footnote{Available in \url{http://trec.nist.gov/data/t9_filtering.html}
%\ (downloaded in Sep.\ 2016).
} We first extract and concatenate title, author, and abstract fields, and then convert punctuations into white spaces and letters into their upper cases. 

\medskip
\noindent \genoaa, \genoa, \genob, \genoc, \genod, \genoe, \genof:  datasets of human genomes of 50 individuals obtained from the {\em personal genomes project},\footnote{Available in \url{http://personalgenomes.org/}} and the reference sequence is obtained from GRCh37 assembly.   We choose to use Chromosome 20.  For \genoaa\ we partition the long DNA sequences into shorter substrings according to the indices of the reference sequence, so that the shift is small in similar pairs. For all other genome datasets we select substrings with {\em random} starting positions.  The names of datasets can be read as `GEN $\circ$ number of strings ($20k$ to $320k$) $\circ$ string length (S $\approx$ 5k, M $\approx$ 10k, L $\approx$ 20k)'. 

We summarize the statistics of our datasets in Table~\ref{tab:stat}. 
The distributions of the string lengths of the \uniref\ and \trec\ datasets are plotted in Figure~\ref{fig:unidis}.
% and Figure~\ref{fig:trecdis}.

\begin{table}[t]
\centering 
\begin{tabular}{lccccc} 
\hline
Datasets &$n$ &Avg Len &Min Len & Max Len & $|\Sigma|$\\  \hline 
\uniref &400000  &445 &200  &35213 &25\\  \hline
\trec &233435  &1217 &80   &3947 &37    \\  \hline
\genoaa &50000   &5000  &4844 & 5109 &4   \\  \hline
\genoa &50000   &5000  &4829 & 5152 &4   \\  \hline
\genob &20000   &5000  &4829 &5109 &4   \\  \hline
\genoc &20000   &10000  &9843 &10154 &4   \\  \hline
\genod &20000   &20000  &19821 &20109 &4   \\  \hline
%\genod &20000   &79998  &79813  &80189 &4   \\  \hline
\genoe &80000   &5000  &4814  &5109 &4   \\  \hline
\genof &320000   &5000  &4811  &5154 &4   \\  \hline
\end{tabular}
\caption{Statistics of tested datasets}
\label{tab:stat}
\end{table}

\begin{figure}[t]
\centering
\includegraphics[height = 1.6in]{strdistri.eps}
\caption{String length distributions of \uniref\ and \trec\ datasets}
\label{fig:unidis}
\end{figure}

%\begin{figure}[t]
%\centering
%\includegraphics[height = 1.8in]{Haoyu/plot/trec.eps}
%\caption{String length distribution  of \trec\  dataset}
%\label{fig:trecdis}
%\end{figure}

\paragraph{Tested Algorithms} 
%We implemented our algorithm \ebdjoin\ in C++, and complied using GCC 5.4.0 with O3 flag.  We compared our algorithms with \pass \cite{LDW11},  \edjoin \cite{XWL08}, \adpjoin \cite{WLF12}, and \qchunk \cite{QWL11}, whose binary codes were downloaded from the authors' project websites.
We now list all the algorithms that we have used in our experiments.
We choose these competing algorithms based on the recommendations of the experimental study \cite{JLFL14} and the similarity search/join competition \cite{WDG14}. We believe that these are the best existing algorithms for edit similarity joins.  
%We refer readers to the full version of this paper for brief descriptions of these competing algorithms.

\medskip
\noindent \ebdjoin, \ebdjoin+: our purposed algorithms.  Note again that when $\lceil K/\Delta \rceil = 1$ \ebdjoin+ degenerates to \ebdjoin.  We implemented our algorithms in C++ and complied using GCC 5.4.0 with O3 flag.

\medskip
\noindent \pass \cite{LDW11}: an exact algorithm for similarity joins use a partition-based framework.  The basic idea of \pass\ is to use the pigeon-hole principle: given an edit distance threshold $K$, \pass\ partitions each string into $K+1$ segments. Two similar strings must share at least one segment.  The \pass\ has the best time performance for similarly joins on long strings according to the report \cite{JLFL14} and competition \cite{WDG14}.   We obtained the implementation of \pass\ from the authors.

\medskip
\noindent \edjoin \cite{XWL08}: an exact algorithm for similarity joins based on prefix filtering.  The idea of prefix filtering is that given an edit distance threshold $K$, we generate $q$-grams for each string, sort them based on a global ordering, and then choose the first $qK+1$ grams as the string's signatures. Two similar strings must have at least one common signature. The \edjoin\ further improves the prefix filtering by {\em Position Filtering} and {\em Content Filtering}. We download the binary codes from the authors' project website.\footnote{\url{http://www.cse.unsw.edu.au/~weiw/project/simjoin.html\#\_download}.} To make the comparison fair, for each dataset and each threshold value $K$ we always report the best time performance among different parameters $q$.

\medskip
\noindent \adpjoin \cite{WLF12}:  an exact algorithm for similarity joins based on prefix filtering.  It improves the original prefix filtering by learning the tradeoff between number of signatures and the  filtering power, instead of using a fixed number of $q$-grams. We download the binary codes from the authors' project website.\footnote{\url{https://www2.cs.sfu.ca/~jnwang/projects/adapt/}.} There are three filtering methods used in \cite{WLF12}, named {\em Gram}, {\em IndexGram} and {\em IndexChunk}. We found that {\em Gram} always has the best time performance. We thus report the best time performance among different parameters $q$ using the {\em Gram} filter.

\medskip
\noindent \qchunk \cite{QWL11}: an exact algorithm for similarity joins based on prefix filtering.  It improves the prefix filter by introducing {\em $q$-chunk} which is $q$-gram with starting positions at $i \cdot q +1$ for $i\in \{0, 1, \ldots, \frac{l-1}{q}\}$, where $l$ is the string length. \qchunk\ then employs effective filters based on $q$-chunk. We download the binary codes from the authors' project website.\footnote{\url{http://www.cse.unsw.edu.au/~weiw/project/simjoin.html\#\_download} and \url{http://www.cse.unsw.edu.au/~jqin/}.} There are two filtering methods used in \cite{QWL11}, named {\em IndexGram} and {\em IndexChunk}. We found that {\em IndexChunk} always has the better time performance. We thus report the best time performance among different parameters $q$ using the {\em IndexChunk} filter.


\paragraph{Measurements}
We report three types of measurements in our experiments: {\em accuracy}, {\em memory usage} and {\em running time}.  Recall that \ebdjoin\ and \ebdjoin+ only have false negatives; the {\em accuracy} we report is number of output pairs returned by \ebdjoin\ and \ebdjoin+ divided by the ground truth returned by other exact competing algorithms.  The memory usage we report is the maximum memory usage of a program during its execution. 

As mentioned, the competing algorithms may use different filtering methods or different parameters. We always choose the {\em best} combinations for comparisons.  To make the comparison fair we have counted the time used for all the preprocessing steps.
%For \ebdjoin\ and \ebdpara, we report the running time only for the run with accuracy greater than $95\%$ in the \uniref\ and \trec\ datasets, and greater than $99\%$ in all the genome datasets.

\paragraph{Computing Environment}
All experiments were conducted on a Dell PowerEdge T630 server with 2 Intel Xeon E5-2667 v4 3.2GHz CPU with 8 cores each, and 256GB memory.


\begin{figure}[t]
\centering
\includegraphics[height = 2in]{trun.eps}
\caption{The influence of length of Truncation $L$ on the {\em minimum} normalized Hamming distance $Ham(x,y)/L$, for $1000$ random selected similar pairs (with $ED(x,y) \le 150$) and dissimilar pairs (with $ED(x,y) > 150$) on \genoa\ dataset. The parameters are $r=z=5$.}
\label{fig:trun}
\end{figure}


\subsection{Performance Overview of \ebdjoin+}
\label{sec:embed-exp}

In this section we present an overview of the performance of \ebdjoin+.   All the results for \ebdjoin+\ are the average of five independent runs. 

\paragraph{Length of truncation}
As discussed in Section~\ref{sec:speedup}, we use {\em truncation} to speed up the CGK-embedding.  This is useful since when the distance threshold $K$ or number of candidates are small, the embedding will dominate the total running time.  In the following we show how different choices of the truncation lengths affect distance gaps between similar and dissimilar pairs after the CGK-embedding.


Figure~\ref{fig:trun} presents how the length of truncation $L$ influences the {\em minimum normalized Hamming distance} of similar and dissimilar pairs on the \genoa\ dataset. The minimum normalized Hamming distance of a pair is the minimum value of normalized Hamming distance ($Ham(x,y)/L$) over all pairs of substrings and all embeddings.  The total string length after CGK-embedding is $15000$; we thus truncate strings from $2500$ characters to $15000$ characters. 
From the plot we notice that the normalized Hamming distances of similar pairs are almost the same under different $L$ values, and increase a little when $L = 2500$. On the other hand, the normalized Hamming distances of dissimilar pairs are almost the same when $L \le 10000$, and decrease a lot when $L > 10000$; this is because most characters after the $10000$-th digit are ``$\perp$'', which do not contribute to the Hamming distances. The plot recommends us to choose $L$ between $5000$ and $10000$, or, between $avg(\S)$ and $2 avg(\S)$.  In our experiments we will use truncation length $L=avg(\S)$ on genome datasets in which the string lengths are very close, and $L=2 avg(\S)$ on other datasets where the string lengths vary.




\begin{table*}[t]
\centering
\scalebox{0.8}{
  \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
    \hline
    \multirow{3}{*}{Accuracy} &
      \multicolumn{3}{c|}{$r=5$} &
      \multicolumn{3}{c|}{$r=7$} &
      \multicolumn{3}{c|}{$r=9$} \\

& $z=3$  &$z=5$ &  $z=7$ & $z=3$  &$z=5$ &  $z=7$ & $z=3$  &$z=5$ & $z=7$ \\
    \hline
    $m=5$ & 94.5\% & 97.4\% &98.6\% & 96.9\% &99.0\% &99.5\% & 98.5\% & 99.4\% & 99.7\% \\
    \hline
     $m=7$ & 91.6\% & 94.0\% & 95.6\% & 95.2\% & 97.2\% & 98.4\% & 96.4\% & 98.4\% & 99.1\% \\
    \hline
     $m=9$ & 90.1\% & 90.8\% & 92.9\% & 90.7\% & 94.7\% & 96.1\% & 92.9\% & 96.2\% & 97.6\% \\
    \hline
  \end{tabular}
  }
  \caption{Accuracy of \ebdjoin+, \uniref\ dataset, $K=20, \Delta = 50$}
\label{tab:uniacc}
\end{table*}


\begin{table*}[t]
\centering
\scalebox{0.8}{
  \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
    \hline
    \multirow{3}{*}{Accuracy} &
      \multicolumn{3}{c|}{$r=5$} &
      \multicolumn{3}{c|}{$r=7$} &
      \multicolumn{3}{c|}{$r=9$} \\

& $z=3$  &$z=5$ &  $z=7$ & $z=3$  &$z=5$ &  $z=7$ & $z=3$  &$z=5$ & $z=7$ \\
    \hline
    $m=8$ & 91.3\% & 94.2\% &95.6\% & 91.3\% &94.2\% &95.6\% & 95.6\% & 95.6\% & 98.6\% \\
    \hline
     $m=10$ & 90.0\% & 92.8\% & 92.8\% & 91.3\% & 94.2\% & 94.2\% & 92.8\% & 94.2\% & 95.6\% \\
    \hline
     $m=12$ & 90.0\% & 90.0\% & 91.3\% & 90.0\% & 90.0\% & 91.3\% & 91.3\% & 92.8\% & 94.2\% \\
    \hline
  \end{tabular}
  }
  \caption{Accuracy of \ebdjoin+, \trec\ dataset, $K=40, \Delta = 50$}
\label{tab:trecacc}
\end{table*}

\begin{table*}[!ht]
\centering
\scalebox{0.8}{
  \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
    \hline
    \multirow{3}{*}{Accuracy} &
      \multicolumn{3}{c|}{$r=5$} &
      \multicolumn{3}{c|}{$r=7$} &
      \multicolumn{3}{c|}{$r=9$} \\

& $z=8$  &$z=12$ &  $z=16$ & $z=8$  &$z=12$ &  $z=16$ & $z=8$  &$z=12$ &  $z=16$  \\
    \hline
    $m=11$ & 99.0\% & 99.2\% &99.4\% & 99.2\% &99.8\% &99.9\% & 99.7\% & 99.9\% & 100.0\% \\
    \hline
     $m=13$ & 97.4\% & 97.7\% & 98.0\% & 98.9\% & 99.6\% & 99.7\% & 99.6\% & 99.9\% & 99.9\% \\
    \hline
     $m=15$ & 96.1\% & 96.7\% &98.3\% & 98.0\% & 98.2\% & 99.3\% & 98.8\% & 99.4\% & 99.6\% \\
    \hline
  \end{tabular}
  }
  \caption{Accuracy of \ebdjoin+, \genoa\ dataset, $K=100, \Delta = 50$}
\label{tab:genoacc}
\end{table*}

\begin{table*}[!ht]
\centering
\scalebox{0.8}{
  \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
    \hline
    \multirow{3}{*}{Accuracy} &
      \multicolumn{3}{c|}{$r=5$} &
      \multicolumn{3}{c|}{$r=7$} &
      \multicolumn{3}{c|}{$r=9$} \\

& $z=8$  &$z=12$ &  $z=16$ & $z=8$  &$z=12$ &  $z=16$ & $z=8$  &$z=12$ &  $z=16$  \\
    \hline
    $\Delta=25$ & 99.8\% & 99.9\% &100.0\% & 100.0\% &100.0\% &100.0\% & 100.0\% & 100.0\% & 100.0\% \\
    \hline
     $\Delta=34$ & 99.8\% & 99.9\% & 100.0\% & 100.0\% & 100.0\% & 100.0\%& 100.0\% & 100.0\% & 100.0\% \\
    \hline
     $\Delta=50$ & 97.4\% & 97.7\% & 98.0\% & 98.9\% & 99.6\% & 99.7\% & 99.6\% & 99.9\% & 99.9\% \\
    \hline
  \end{tabular}
  }
  \caption{Accuracy of \ebdjoin+, \genoa\ dataset, $K=100, m = 13$}
\label{tab:genoaccdel}
\end{table*}


\paragraph{Accuracy}
In Table~\ref{tab:uniacc}, \ref{tab:trecacc} and \ref{tab:genoacc} we study how different parameters $(r, z, m)$ influence the accuracy of \ebdjoin+. 
We vary $r$ in $\{5,7,9\}$, $z$ in $\{3,5,7\}$ for \trec\ and \uniref, and $z$ in $\{8,12,16\}$ for \genoa. We choose slightly different values for $m$ on different datasets (the choices of $m$ largely depend on the string length and the distance threshold $K$).  

We observe that the accuracy of \ebdjoin+\ is $90.1 \sim 99.7\%$ on \uniref, $90.0 \sim 98.6\%$ on \trec, 
and $96.1 \sim 100.0\%$ in \genoa.  

We note that the accuracy of \ebdjoin+\ increases with $r$ and $z$, and decreases with $m$. This is consistent with the theory.  When $r$ and $z$ increase, we use more hash functions (recall that the total number of hash functions used is $r \cdot z$), and thus each pair of strings have more chance to be hashed into the same bucket in at least one of the hash tables. Similarly, when $m$ decreases, each LSH function has larger collision probability.  Of course, the increase of the collision probability will always introduce more false positives, and consequently increase the verification time.  Using more hash functions/tables will also increase the space usage.  



In Table~\ref{tab:genoaccdel} we study how the parameter $\Delta$ influences the accuracy of \ebdjoin+. We vary $\Delta$ in $\{25, 34, 50\}$ so that the number of substrings for each string are  $\{4, 3, 2\}$. We observe that the accuracy of \ebdjoin+\ decreases when $\Delta$ increases. This is because when $\Delta$ increases, the length of shifts between similar pairs may increase, which makes the chance of hashing them into the same bucket to be smaller. 



\paragraph{Time and Space}
In Table~\ref{tab:genotime-1} we study how different parameters $(r, z, m)$ influence the running time of \ebdjoin+\ in the \genoa\ dataset.  We note that the running time increases when $r$ and $z$ increase, decreases when $m$ increases.  This is just the opposite to what we have observed for accuracy, and is consistent to the theory that increasing the collision probability will introduce more false positives/candidates and thus increase the verification time.  

 
In Table~\ref{tab:genomem} we study how different parameters $(r, z, m)$ influence the memory of \ebdjoin+\ in the \genoa\ dataset. 
We observe that the memory usage increases when $r$ and $z$ increase. This is because when $r$ and $z$ increase we need to store more hash tables {\em and} we will have more candidate pairs to verify.  When $m$ increases, the memory usage stays the same or slightly increases.  There are two kinds of mutually exclusive forces that affect this.  On the one hand, when $m$ increases the size of each hash signature increases.  On the other hand, when $m$ increases the number of candidate pairs decreases. From what we have observed, the first force generally dominates the second.


Figure~\ref{fig:timen} and Figure~\ref{fig:timek} depict the running time of \ebdjoin+\ on (1) reading the input and CGK-embedding, (2) performing LSH, and (3) verification. We vary the number of input strings $n$ and the distance threshold $K$.  We observe that when $n,K$ increases, the time usages of all the three parts increase.  In all cases, the input reading and embedding is the bottleneck.  The first two parts are more sensitive to $n$, which have a higher increasing rate when $n$ increases, and are almost stable when $K$ increases. The verification time increases rapidly when both $n,K$ increase.


\begin{table*}[t]
\centering
\scalebox{0.8}{
  \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
    \hline
    \multirow{3}{*}{Time(s)} &
      \multicolumn{3}{c|}{$r=5$} &
      \multicolumn{3}{c|}{$r=7$} &
      \multicolumn{3}{c|}{$r=9$} \\

& $z=8$  &$z=12$ &  $z=16$ & $z=8$  &$z=12$ &  $z=16$ & $z=8$  &$z=12$ &  $z=16$  \\
    \hline
    $m=11$ & 18.3 & 20.3 & 22.8& 25.4& 28.3 & 31.1 & 32.3 & 36.1 & 39.2 \\
    \hline
     $m=13$ & 17.8 & 19.9 & 21.9 & 24.9 & 27.7 & 30.1 & 31.4 & 35.7 & 38.5 \\
    \hline
     $m=15$ & 17.7 & 19.8 & 21.6 & 24.8 & 27.3 & 30.0 & 31.2 & 35.2 & 38.4 \\
    \hline
  \end{tabular}
  }
  \caption{Running time of \ebdjoin+, \genoa\ dataset, $K=100, \Delta = 50$}
\label{tab:genotime-1}
\end{table*}

\begin{table*}[t]
\centering
\scalebox{0.8}{
  \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
    \hline
    \multirow{3}{*}{GB} &
      \multicolumn{3}{c|}{$r=5$} &
      \multicolumn{3}{c|}{$r=7$} &
      \multicolumn{3}{c|}{$r=9$} \\

& $z=8$  &$z=12$ &  $z=16$ & $z=8$  &$z=12$ &  $z=16$ & $z=8$  &$z=12$ &  $z=16$  \\
    \hline
    $m=11$ & 2.1 & 2.6 & 3.1 & 2.4 & 3.3 & 3.9 & 2.9 & 4.0 & 4.7\\
    \hline
     $m=13$ & 2.1 & 2.6 & 3.1 & 2.5 & 3.3 & 3.9 & 2.9 & 4.0 & 4.7 \\
    \hline
     $m=15$ & 2.3 & 2.6 & 3.5 & 2.8 & 3.3 & 4.5 & 3.3 & 4.0 & 5.6 \\
    \hline
  \end{tabular}
  }
  \caption{Memory usage of \ebdjoin+, \genoa\ dataset, $K=100, \Delta = 50$. }
\label{tab:genomem}
\end{table*}

\begin{figure*}[!t]
\centering
\begin{minipage}[d]{0.3\linewidth}
\centering
\includegraphics[width=1\textwidth]{timenuniref.eps}
\centerline{\uniref\ ($K=20$)}
\end{minipage}
\begin{minipage}[d]{0.3\linewidth}
\centering
\includegraphics[width=1\textwidth]{timentrec.eps}
\centerline{\trec\ ($K=40$)}
\end{minipage}
\begin{minipage}[d]{0.3\linewidth}
\centering
\includegraphics[width=1\textwidth]{timen.eps}
\centerline{\genoa\ ($K=100$)}
\end{minipage}
\caption{Running time of different parts of \ebdjoin+, varying $n$.}
\label{fig:timen}
\end{figure*}



\begin{figure*}[!ht]
\centering
\begin{minipage}[d]{0.3\linewidth}
\centering
\includegraphics[width=1\textwidth]{timekuniref.eps}
\centerline{\uniref}
\end{minipage}
\begin{minipage}[d]{0.3\linewidth}
\centering
\includegraphics[width=1\textwidth]{timektrec.eps}
\centerline{\trec}
\end{minipage}
\begin{minipage}[d]{0.3\linewidth}
\centering
\includegraphics[width=1\textwidth]{timek.eps}
\centerline{\genoa}
\end{minipage}
\caption{Running time of different parts of \ebdjoin+, varying $K$.}
\label{fig:timek}
\end{figure*}



\begin{figure*}[!ht]
\centering
\begin{minipage}[d]{0.4\linewidth}
\centering
\includegraphics[width=0.8\textwidth]{paruniref.eps}
\centerline{\uniref}
\end{minipage}
\begin{minipage}[d]{0.4\linewidth}
\centering
\includegraphics[width=0.8\textwidth]{partrec.eps}
\centerline{\trec}
\end{minipage}
\caption{Grouping vs Truncation, $r=5$, $z=5$, $m=15$.}
\label{fig:part}
\end{figure*}

 \begin{table*}[!ht]
\centering
\scalebox{0.8}{
  \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
    \hline
    \multirow{3}{*}{Time(s)} &
      \multicolumn{3}{c|}{$r=5$} &
      \multicolumn{3}{c|}{$r=7$} &
      \multicolumn{3}{c|}{$r=9$} \\

& $z=8$  &$z=12$ &  $z=16$ & $z=8$  &$z=12$ &  $z=16$ & $z=8$  &$z=12$ &  $z=16$  \\
    \hline
    $\Delta=25$  & 30.2 & 32.5 & 34.8 & 41.6& 45.1 & 48.4 & 53.2 & 58.0 & 62.3 \\
    \hline
     $\Delta=34$  & 26.1 & 28.6 & 31.7 & 36.5 & 40.8 & 45.0 & 46.7 & 51.6 & 57.3 \\
    \hline
     $\Delta=50$  & 17.7 & 19.8 & 21.6 & 24.8 & 27.3 & 30.0 & 31.2 & 35.2 & 38.4 \\
    \hline
  \end{tabular}
  }
  \caption{Running time of \ebdjoin+, \genoa\ dataset, $K=100, m = 13$}
\label{tab:genotimedel}
\end{table*}

Figure~\ref{fig:part} shows the running time of \ebdjoin+\ on datasets with strings of different lengths (\uniref\ and \trec), using the grouping method and the truncation method respectively.  It is clear that truncation is always better than grouping.  
%This is mainly because in grouping each candidate pair {\em may be} verified twice, and this is also why in all the other experiments
 We thus always use truncation-based \ebdjoin\ and \ebdjoin+ in our (other) experiments.  
 
 
In Table~\ref{tab:genotimedel} we study how the parameter $\Delta$ influences the running time of \ebdjoin+. We vary $\Delta$ in $\{25, 34, 50\}$ so that the number of substrings for each string are $\{4, 3, 2\}$. We observe that the running time increases when $\Delta$ decreases. This is because when $\Delta$ decreases, there are more substrings to embed and hash for each string, and more candidates to verify. 




\paragraph{The Filtering Quality}
Table~\ref{tab:can} shows how different parameters $(r, z, m)$ influence the number of candidates generated by \ebdjoin+.  We use \genoa\ as the test dataset.   We observe that the number of candidates is consistent to the running time, that is, the number increases when $r$ and $z$ increase, and decreases when $m$ increases.  From the table we can see that under different parameters, our numbers of candidates are about $2.80 \sim 5.07$ times of the ground truth $6317$. 

\begin{table*}[!ht]
\centering
\scalebox{0.8}{
  \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
    \hline
    \multirow{3}{*}{\# Candidates} &
      \multicolumn{3}{c|}{$r=5$} &
      \multicolumn{3}{c|}{$r=7$} &
      \multicolumn{3}{c|}{$r=9$} \\

& $z=8$  &$z=12$ &  $z=16$ & $z=8$  &$z=12$ &  $z=16$ & $z=8$  &$z=12$ &  $z=16$  \\
    \hline
    $m=11$ & 21498  & 22650  & 24493  & 22652  & 24593  & 28266  & 24002  & 28948   & 32050  \\
    \hline
     $m=13$ & 19245  &20791   & 21256   & 20387  & 21516  & 22163  & 21747  & 22496  & 23390  \\
    \hline
     $m=15$ & 17686  & 19072  & 20269  & 19723   & 21038 & 21746   & 20253  & 21730 & 22377   \\
    \hline
  \end{tabular}
  }
\caption{Number of candidate pairs after filtering of \ebdjoin+, \genoa\ dataset, $K=100, \Delta = 50$; ground truth is $6317$.}
\label{tab:can}
\end{table*}



\subsection{A Comparison with Existing Algorithms}
\label{sec:comparison}

In this section we compare \ebdjoin\ and \ebdjoin+\ with the existing best algorithms introduced in Section~\ref{sec:setup}.  
We note that in some figures some data points for competing algorithms are missing, which is either because these algorithms have implementation limitations (returned wrong answers or triggered memory overflow) or they cannot finish in 24 hours in our computing environment.  


\begin{figure*}[!t]
\centering
\begin{minipage}[d]{0.4\linewidth}
\centering
\includegraphics[width=0.9\textwidth]{unirefktime.eps}
\centerline{\uniref}
\end{minipage}
\begin{minipage}[d]{0.4\linewidth}
\centering
\includegraphics[width=0.9\textwidth]{trecktime.eps}
\centerline{\trec}
\end{minipage}
\begin{minipage}[d]{0.4\linewidth}
\centering
\includegraphics[width=0.9\textwidth]{dnaktimepre.eps}
\centerline{\genoaa}
\end{minipage}
\begin{minipage}[d]{0.4\linewidth}
\centering
\includegraphics[width=0.9\textwidth]{dnaktime.eps}
\centerline{\genoa}
\end{minipage}
\caption{Running time, varying $K$.  Percentages on the curves for \ebdjoin/\ebdjoin+\ are their accuracy
%The accuracy of \ebdpara\ is always the same as that of \ebdjoin\ for each tested $K$ value so we do not repeat.  
}
\label{fig:ktime}
\end{figure*}



\paragraph{Scalability on the Threshold Distance}
Figure~\ref{fig:ktime} shows the running time of different algorithms when varying the distance threshold $K$ on \uniref, \trec, \genoaa\ and \genoa.   In all experiments we always guarantee that the accuracy of  \ebdjoin\ and \ebdjoin+\ is above $95\%$ on \uniref\ and \trec, and above $99\%$ on \genoaa. We use the same parameters for both algorithms on \genoa\ and \genoaa.  On \uniref\ and \trec\ datasets, where we choose $T = 1$ for \ebdjoin+, \ebdjoin\ and \ebdjoin+\ become the same algorithm, and thus have same accuracy, memory usage and running time.

We observe that \ebdjoin\ and \ebdjoin+\ always have the best time performances: the running time of \ebdjoin\ is better than the best existing algorithm by a factor of $9.2$ on \uniref\  ($K = 20$), 10.2 in \trec\ ($K = 50$), $88.7$ on \genoaa\ ($K = 150$), and $21.4$ on \genoa\ ($K = 150$);  the running time of \ebdjoin+\ is better than the best existing algorithm by a factor of $9.2$ on \uniref\  ($K = 20$), 10.2 in \trec\ ($K = 50$), $23.9$ on \genoaa\ ($K = 150$), and $5.9$ on \genoa\ ($K = 150$). 


However, the accuracy of \ebdjoin\ is as low as $39.3\%$ on \genoa\ when $K = 150$. The main reason is that the pairwise edit distance distributes almost uniformly on \genoa\ (and on other random reads genome datasets as well). On the rest of the datasets, there are clear gaps between similar and dissimilar pairs.  See Figure~\ref{fig:dist} for the details.
When the distance gap exists, the distortion generated by the CGK-embedding becomes less critical. Otherwise, in order to maintain a high accuracy, we have to make sure that there are not many false negatives by maintaining a large candidate set, which can be done by adjusting the parameters in LSH.  However, this will make the verification step very expensive, and is thus not a good idea overall.  

The above issue is resolved in \ebdjoin+.  The motivation of proposing \ebdjoin+, as presented in Section~\ref{sec:improved}, is to reduce the shift between a pair of strings so as to reduce the distortion of the CGK-embedding.  Note that when the shift is reduced, the edit distance of the remaining pair of substrings is smaller than original one, which helps to remove false negatives {\em without} changing the LSH module by much (compared with the idea of trying to modify the original \ebdjoin\ mentioned above).  After such a procedure the number of false positives in the candidate set will still increase, but only at a modest amount.





\begin{figure*}[t]
\centering
\begin{minipage}[d]{0.4\linewidth}
\centering
\includegraphics[width=0.9\textwidth]{unirefdist.eps}
\centerline{\uniref}
\end{minipage}
\begin{minipage}[d]{0.4\linewidth}
\centering
\includegraphics[width=0.9\textwidth]{trecdist.eps}
\centerline{\trec}
\end{minipage}
\begin{minipage}[d]{0.4\linewidth}
\centering
\includegraphics[width=0.9\textwidth]{dnaolddist.eps}
\centerline{\genoaa}
\end{minipage}
\begin{minipage}[d]{0.4\linewidth}
\centering
\includegraphics[width=0.9\textwidth]{dnadist.eps}
\centerline{\genoa}
\end{minipage}
\caption{Distance distribution of datasets.}
\label{fig:dist}
\end{figure*}

The \pass\ algorithm does not scale well on $K$: when $K$ increases, the running time jumps sharply.  This may due to the fact that the time complexity in the filtering step of \pass\ is $O(n K^3)$ -- a cubic dependence on $K$.  The other three algorithms, \edjoin, \adpjoin\ and \qchunk, are all based on $q$-gram or its variants; they generally have similar running time curves, which rise much slower compared with \pass\ when $K$ increases.  One exception is that on the \uniref\ dataset the running time of \qchunk\ increases sharply when $K$ passes $20$, which may due to the sudden increase of the number of candidate pairs that \qchunk\ produces.  On \genoaa\ and \genoa, the running time of \edjoin\ is too large ($>10000$s when $K = 50$) and thus does not fit the figure, and \adpjoin\ reports erroneous results.

%We would like to comment that the {\em output size} has little to do with the running time of the algorithms.  For example, for the five distance thresholds on the \uniref\ dataset in Figure~\ref{fig:ktime}, the corresponding output sizes are $707, 1377, 2154, 3184, 5864$; the numbers increase quickly.  On the other hand, for the five distance thresholds on the \trec\ dataset the corresponding output sizes are $52, 63, 64, 69, 70$; the numbers increase slowly.  But the running time curves of the test algorithms on the two datasets generally follow the similar trends.  The major components that affect the running time are the time spent on the filtering and the number of {\em candidate pairs} after the filtering step for verification.  See Figure~\ref{fig:timen} and \ref{fig:timek} for statistics on the running time of \ebdjoin+\ on different modules.


\begin{figure*}[t]
\centering
\begin{minipage}[d]{0.4\linewidth}
\centering
\includegraphics[width=0.9\textwidth]{unirefkmem.eps}
\centerline{\uniref}
\end{minipage}
\begin{minipage}[d]{0.4\linewidth}
\centering
\includegraphics[width=0.9\textwidth]{treckmem.eps}
\centerline{\trec}
\end{minipage}
\begin{minipage}[d]{0.4\linewidth}
\centering
\includegraphics[width=0.9\textwidth]{dnakmempre.eps}
\centerline{\genoaa}
\end{minipage}
\begin{minipage}[d]{0.4\linewidth}
\centering
\includegraphics[width=0.9\textwidth]{dnakmem.eps}
\centerline{\genoa}
\end{minipage}
\caption{Memory usage, varying $K$. 
%The accuracy of \ebdpara\ is always the same as that of \ebdjoin\ for each tested $K$ value so we do not repeat.  
}
\label{fig:kmem}
\end{figure*}


Figure~\ref{fig:kmem} shows the memory usages of different algorithms in the same settings as Figure~\ref{fig:ktime}.  The memory used by \ebdjoin\ is the smallest among all in most cases, and \ebdjoin+\ uses a little bit more memory when $K$ is relatively large. Note that the memory usage of \ebdjoin+\ has a linear dependency on $K$, which is because the number of substrings for each string is $\lceil K/\Delta \rceil$ and we need to store signatures for each of them.  The memory usage of \pass\ is also small at the beginning, but deteriorates fast when $K$ increases.  The three $q$-gram based algorithms have similar trends in memory usage.  



\begin{figure*}[t]
\centering
\begin{minipage}[d]{0.4\linewidth}
\centering
\includegraphics[width=0.9\textwidth]{unirefntime.eps}
\centerline{\uniref\ ($K=20$)}
\end{minipage}
\begin{minipage}[d]{0.4\linewidth}
\centering
\includegraphics[width=0.9\textwidth]{trecntime.eps}
\centerline{\trec\ ($K=40$)}
\end{minipage}
\begin{minipage}[d]{0.4\linewidth}
\centering
\includegraphics[width=0.9\textwidth]{dnantimepre.eps}
\centerline{\genoaa\ ($K=100$)}
\end{minipage}
\begin{minipage}[d]{0.4\linewidth}
\centering
\includegraphics[width=0.9\textwidth]{dnantime.eps}
\centerline{\genoa\ ($K=100$)}
\end{minipage}
\caption{Running time, varying $n$.  Percentages on the curves for \ebdjoin/\ebdjoin+\  are their accuracy}
\label{fig:ntime}
\end{figure*}

\paragraph{Scalability on the Input Size}
Figure~\ref{fig:ntime} shows the running time of different algorithms on the \uniref, \trec, \genoaa\ and \genoa\ datasets when varying input size $n$. The trends of the running time of all algorithms are similar; they increase with respect to $n$.   It is clear that \ebdjoin\ and \ebdjoin+\ perform much better than all the other algorithms: \ebdjoin\ performs better than the best existing algorithm by a factor of $9.2$ on \uniref\ ($N = 4 \times 10^5$), $11.5$ on \trec\ ($N = 2 \times 10^5$), $69.7$ on \genoaa\ ($N = 5 \times 10^4$), and $7.7$ on \genoa\ ($N = 5 \times 10^4$);  the running time of \ebdjoin+\ is better than the best existing algorithm by a factor of $9.2$ on \uniref\ ($N = 4 \times 10^5$), $11.5$ on \trec\ ($N = 2 \times 10^5$), $22.1$ on \genoaa\ ($N = 5 \times 10^4$), and $4.8$ on \genoa\ ($N = 5 \times 10^4$) . 

Figure~\ref{fig:nmem} shows the memory usages of different algorithms in the same settings as Figure~\ref{fig:ntime}. The trends of the memory usages of all algorithms are similar; they increase almost linearly with respect to $n$. 





\begin{figure*}[t]
\centering
\begin{minipage}[d]{0.4\linewidth}
\centering
\includegraphics[width=0.9\textwidth]{unirefnmem.eps}
\centerline{\uniref\ ($K=20$)}
\end{minipage}
\begin{minipage}[d]{0.4\linewidth}
\centering
\includegraphics[width=0.9\textwidth]{trecnmem.eps}
\centerline{\trec\ ($K=40$)}
\end{minipage}
\begin{minipage}[d]{0.4\linewidth}
\centering
\includegraphics[width=0.9\textwidth]{dnanmempre.eps}
\centerline{\genoaa\ ($K=100$)}
\end{minipage}
\begin{minipage}[d]{0.4\linewidth}
\centering
\includegraphics[width=0.9\textwidth]{dnanmem.eps}
\centerline{\genoa\ ($K=100$)}
\end{minipage}
\caption{Memory usage, varying $n$.}
\label{fig:nmem}
\end{figure*}





\paragraph{The Ultimate Scalability of EmbedJoin+}   
Finally, we present a set of experiments that distinguish \ebdjoin+\ from all the competing algorithms. We test all the algorithms on longer strings (length ranges from 5,000 to 20,000) with larger distance thresholds ($1\% \sim 20\%$ of the corresponding string length).  The numbers of strings in the datasets range from 20,000 to 320,000.  For \ebdjoin\ we fix $r = z = 7$, and set $m= 15 - \lfloor \log_2 x \rfloor$ where $x \%$ is the threshold.  For \ebdjoin+\ we fix $r =  7,z = 16, \Delta = 50$, and set $m= 15 - \lfloor \log_2 x \rfloor$ where $x \%$ is the threshold.  Result points are only depicted for those that can finish in 24 hours, {\em and} return correct answers.  
%We observe that most of the competing algorithms cannot return correct answers within 24 hours in most of these regions, or simply failed due to issues such as memory overflow.  

When varying the string length $N$ (see Figure~\ref{fig:scalek}), there are three other algorithms that can produce data points in the \genob\ dataset: \edjoin\ can report answer up to the $2\%$ distance threshold, and \pass\ and \qchunk\ can go up to $8\%$.  We observe a sharp time jump of \qchunk\  from $4\%$ to $8\%$ -- at the $8\%$ distance threshold \qchunk\ barely finished within 24 hours.  
%This is similar to the performance of \qchunk\ on the \uniref\ datasets from the (absolute) distance threshold $20$ to $25$ (about $5.6\%$ of the average string length). 
On \genod, unfortunately, the program for \qchunk\ that we have used cannot produce any data point due to memory overflow.  \pass\ only succeeds at the $2\%$ distance threshold.  
%All the computing algorithms cannot produce any data point on \genod\ within 24 hours.

When varying the number of input strings $n$ (see Figure~\ref{fig:scalen}; the first subfigure of Figure~\ref{fig:scalen} is simply a  repeat of the first subfigure of Figure~\ref{fig:scalek}), all the other computing algorithms cannot produce anything on \genof.  \pass\ manages to produce results on \genoe\ up to $4\%$ distance threshold.  On the other hand, \ebdjoin\ and \ebdjoin+\ scales smoothly on all the datasets. 

The accuracy of \ebdjoin\ decreases sharply with $K$, while \ebdjoin+\ always maintains a good accuracy. The accuracy of \ebdjoin+\ even increases with $K$.  This is because we use a fixed $\Delta$ value for different thresholds $K$, and as a result the number of substrings for each string increases with $K$, which means that the chance for a pair of strings to be chosen as a candidate increases, and consequently the number of false negatives decreases.    We observe that on $\genoe$ and $\genof$ datasets, \ebdjoin+\ has a better time performance than \ebdjoin\ when $K$ is large, even that it needs to spend more time on embedding and hashing. This is because \ebdjoin+\ requires similar pairs to have a pair of substrings with at least $T$ hash signature matches, which decreases number of false positives and consequently saves the verification time. 

\begin{figure*}[t]
\centering
\begin{minipage}[d]{0.32\linewidth}
\centering
\includegraphics[width=1\textwidth]{dnasmall.eps}
\centerline{\genob}
\end{minipage}
\begin{minipage}[d]{0.32\linewidth}
\centering
\includegraphics[width=1\textwidth]{dnamidk.eps}
\centerline{\genoc}
\end{minipage}
\begin{minipage}[d]{0.32\linewidth}
\centering
\includegraphics[width=1\textwidth]{dnalongk.eps}
\centerline{\genod}
\end{minipage}
\caption{Scalability on string length. Percentages on the curves for \ebdjoin/\ebdjoin+\  are their accuracy}
\label{fig:scalek}
\end{figure*}

\begin{figure*}[t]
\centering
\begin{minipage}[d]{0.32\linewidth}
\centering
\includegraphics[width=1\textwidth]{dnasmall.eps}
\centerline{\genob}
\end{minipage}
\begin{minipage}[d]{0.32\linewidth}
\centering
\includegraphics[width=1\textwidth]{dna80k.eps}
\centerline{\genoe}
\end{minipage}
\begin{minipage}[d]{0.32\linewidth}
\centering
\includegraphics[width=1\textwidth]{dna32k.eps}
\centerline{\genof}
\end{minipage}
\caption{Scalability on number of strings. Percentages on the curves for \ebdjoin/\ebdjoin+\  are their accuracy. }
\label{fig:scalen}
\end{figure*}


To summarize, it is clear that on large datasets with long string,  \ebdjoin+\ performs much better than all the competing algorithms, and scales well up to distance threshold $20\%$.  Unfortunately, we do not know the exact accuracy of \ebdjoin+\ in many points where other exact computation algorithms cannot finish, but from the trends that we have observed on shorter strings and smaller distance thresholds, we would expect that its accuracy will be consistently high. 










