\section{The \ebdjoin\ Algorithm}
\label{sec:algo}

\label{sec:embed}

Now we are ready to describe our basic algorithm \ebdjoin, which is presented in Algorithm~\ref{alg:embed-join} using Algorithm~\ref{alg:preprocessing} as a subroutine. We explain them in words below.

\begin{algorithm}[t]
\caption{Preprocessing ($\mathcal{S}$, $r$, $z$, $m$)}
\label{alg:preprocessing}
\begin{algorithmic}[1]
\Require Set of input strings $\mathcal{S} = \{s_1, \ldots, s_n\}$, and parameters $r$, $z$ and $m$ described in Table~\ref{tab:notation}
\Ensure  Strings in $\mathcal{S}$ in the sorted order, strings after CGK-embedding $\{t_i^\ell\ |\ \ell \in [r], i \in [n]\}$, and hash tables $\{\D_j^{\ell} \ |\ \ell \in [r], j \in [z]\}$.
\State Sort $\mathcal{S}$ first by string length increasingly, and second by the alphabetical order. 

\ForEach{$\ell \in [r]$}
	\ForEach{$j \in [z]$}
	\State Initialize hash table $\D_j^{\ell} $ by generating a random hash function $f_j^{\ell} \in \F(m)$  
	\EndFor
\EndFor

\ForEach{$\ell \in [r]$}
\State Generate a random string $R^{\ell} \in \{0,1\}^{3 N \abs{\Sigma}}$
\ForEach{$s_i\in \mathcal{S}$}
\State $t_i^{\ell}  \leftarrow $ CGK-Embedding($s_i$, $R^{\ell}$) 
\EndFor
\EndFor
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[t]
\caption{EmbedJoin ($\mathcal{S}$, $K$, $r$, $z$, $m$)}
\label{alg:embed-join}
\begin{algorithmic}[1]
\Require Set of input strings $\mathcal{S} = \{s_1, \ldots, s_n\}$, distance threshold $K$, and parameters $r$, $z$ and $m$ described in Table~\ref{tab:notation}
\Ensure  $\O \gets \{(s_i, s_j)\ |\ s_i, s_j \in \S; i \neq j; \text{ED}(s_i, s_j) \le K\}$ 
\State Preprocessing$(\S, r, z, m)$  
\Comment Using Algorithm~\ref{alg:preprocessing}
\State  $\mathcal{C} \leftarrow \emptyset $  
\Comment Collection of candidate pairs

\ForEach{$s_i \in \mathcal{S} \text{ (in the sorted order)}$}
	\ForEach{$\ell \in [r]$}
		\ForEach{$j \in [z]$}
			\ForEach{string $s$ stored in the $f_j^\ell (t_i^\ell)\text{-th bucket of table} \ \D_j^\ell $}
				\If{$\abs{s_i} - \abs{s} \le K$} \label{ln:a-1}
				\State $\mathcal{C}  \leftarrow \mathcal{C}  \cup (s, s_i)$ 
\Else
				\State Remove $s$ from $\D_j^\ell$ \label{ln:a-2}
				\EndIf
			\EndFor
			\State Store $s_i$ in the $f_j^\ell (t_i^\ell)$-th bucket of $\D_j^\ell$
		\EndFor
	\EndFor
	\State Remove duplicate pairs in $\mathcal{C}$ \label{ln:a-3}
\EndFor

\ForEach {$(x, y) \in \mathcal{C}$}
	\If{$\text{ED}(x, y) \le K$}
	\Comment Using the algorithm in \cite{Ukkonen85}
	\State $\mathcal{O} \leftarrow  \mathcal{O} \cup (x, y)$
	\EndIf
\EndFor
\end{algorithmic}
\end{algorithm}

In the preprocessing we generate $r \times z$ hash tables $\D_j^\ell\ (\ell \in [r], j \in [z])$ implicitly by sampling  $r \times z$ random hash functions $f_j^\ell\ (\ell \in [r], j \in [z])$ from $\F(m)$ (defined in Section~\ref{sec:LSH}).  We then CGK-embed each string $s_i \in \S$ for $r$ times, getting $t_i^\ell\ (\ell \in [r])$.

Similar to previous algorithms, \ebdjoin\ has two stages: it first finds a small set of candidate pairs, and then verifies each of them using exact edit-distance computation via dynamic programming.  We use the algorithm for computing edit distance in~\cite{Ukkonen85} for the second step.  In the rest of this section we explain the first filtering step.

The main idea of the filtering step is fairly straightforward. We use LSH to find all pairs $(s_i, s_j)$ for which there exists an $\ell \in [r]$ such that $t_i^\ell$ and $t_j^\ell$ are hashed into the same bucket by at least one of the hash functions $f_j^\ell \in \F(m)\ (j \in [z])$.  In other words, for at least one of the $r$ CGK-embeddings, the output pairs corresponding to $s_i$ and $s_j$ are identified to be similar by at least one of the $z$ LSH functions.  Recall that we do $r$ repetitions of CGK-embedding to achieve a good distortion ratio (see the discussion in Section~\ref{sec:CGK}), and we use $z$ LSH functions from $\F(m)$ to amplify the gap between $p_1$ and $p_2$ in the definition of LSH to reduce false positives/negatives (see the discussion in Section~\ref{sec:LSH}).

In the actual implementation, we use a sliding window to speed-up the filtering: We first sort the input strings in $\S$ according to their lengths increasingly (breaking ties by the alphabetical orders of the strings).  We then process them one by one. If $s_i \in S$ is hashed into some bucket $B$ in the hash table, when fetching each string $s$ in $B$ we first test whether $\abs{s_i} - \abs{s} \le K$ (Line~\ref{ln:a-1}).  If not, we can immediately conclude $\text{ED}(s, s_i) > K$, and consequently $\text{ED}(s, s_{i'}) > K \ (i' > i)$ for all the future strings $s_{i'} \in \S$, since we know for sure that $\abs{s_{i'}} - \abs{s} > K$ due to the sorted order. We thus can safely delete $s$ from bucket $B$ (Line~\ref{ln:a-2}).  Otherwise we add $(s, s_i)$ to our candidate set $\mathcal{C}$.  After these we store $s_i$ in bucket $B$ for future comparisons.  Note that each pair $(s_i, s_j)$ can potentially be added into $\mathcal{C}$ multiple times by different LSH collisions, we thus do a deduplication at Line~\ref{ln:a-3}.

There are two implementation details that we shall mention.  First, in the preprocessing we do not need to generate the whole $t_i^\ell$, but just those $m$ bits that will be used by each of the $z$ LSH functions. This reduces the space usage from $3N \cdot r \cdot n$ to $z \cdot m \cdot r \cdot n$.  Second, It is time/space prohibited to generate the hash table $\D_j^\ell$ whose size is $\abs{\Sigma}^m$.  We adopt the standard two-level hashing implementation of LSH:  For a signature in $\Sigma^m$, we first convert it into a vector $u \in \{1, \ldots, \abs{\Sigma}\}^m$ in the natural way.  We then generate a random vector $v \in \{0, \ldots, P - 1\}^m$ where $P > 1,000,000$ is a prime we choose that fits our datasets in experiments.  Finally, the second level hash function returns $\langle u, v\rangle \bmod P$, where $\langle \cdot, \cdot \rangle$ denotes the inner product.

\begin{table}[t]
\centering
\begin{minipage}[]{0.3\textwidth}
\centering
\begin{tabular}{ |l|l| }
  \hline
  \multicolumn{2}{|c|}{Strings} \\
  \hline
  $s_1$ & ACGTGACGTG \\
  $s_2$ & ACGTCGCGTG\\
  $s_3$ & ACTTACCTG \\
  $s_4$ & ATCGATCGGT\\
  \hline
\end{tabular}
\centerline{(a)}
\end{minipage}
\begin{minipage}[]{0.3\textwidth}
\centering
\begin{tabular}{ |l|l| }
  \hline
  \multicolumn{2}{|c|}{LSH functions } \\
  \hline
  $f_1^1$ & ($h_2$,$h_9$) \\   \hline
  $f_2^1$ &  ($h_1$,$h_4$) \\   \hline
  $f_1^2$ &  ($h_2$,$h_5$)  \\   \hline
  $f_2^2$ &  ($h_7$,$h_3$) \\
  \hline
\end{tabular}
\centerline{(b)}
\end{minipage}
\medskip
\caption{A collection of strings and LSH functions}  
\label{tab:strs}
\end{table}



\begin{table}[t]
\centering
\begin{minipage}[]{0.65\textwidth}
\centering
\begin{tabular}{l*{10}{c}r}
\ $j$              & 1 & 2 & 3 & 4 & 5  & 6 & 7 & 8  & 9 & 10 & $\dots$  \\
\hline
$\pi_j(A)$    & 0 & 1 & 0 & 0 & 1 & 0 & 1 & 1 & 0 & 1  & $\dots$\\
$\pi_j(C)$     & 1 & 1 & 0 & 1 & 1 & 1 & 1 & 0 & 0 & 0 & $\dots$ \\
$\pi_j(G)$    & 0 & 1 & 1 & 1 & 0& 0 & 0 & 1 & 1 & 1  & $\dots$\\
$\pi_j(T)$     & 1 & 0 & 0 & 0 & 1 & 0 & 1 & 1 & 0 & 1  & $\dots$ \\
\end{tabular}
\centerline{(a) random string $R^1$}
\smallskip
\end{minipage}

\begin{minipage}[]{0.65\textwidth}
\centering
\begin{tabular}{l*{10}{c}r}
\ $j$              & 1 & 2 & 3 & 4 & 5  & 6 & 7 & 8  & 9 & 10 & $\dots$  \\
\hline
$\pi_j(A)$    & 1 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 1 & 0  & $\dots$\\
$\pi_j(C)$     & 1 & 1 & 0 & 0 & 1 & 1 & 1 & 1 & 0 & 0 & $\dots$ \\
$\pi_j(G)$    & 1 & 0 & 1 & 1 & 0& 0 & 1 & 1 & 0 & 1  & $\dots$\\
$\pi_j(T)$     & 1 & 0 & 0 & 1 & 0 & 1 & 1 & 1 & 0 & 0  & $\dots$ \\
\end{tabular}
\centerline{(b) random string $R^2$}
\end{minipage}
\medskip
 \caption{Random strings for two CGK embeddings; represented as the equivalent $\pi_j(\cdot)$'s} 
\label{tab:CGK}
\end{table}



\begin{table}[t]
\centering
\begin{minipage}[]{0.4\textwidth}
\centering
\begin{tabular}{ |l|l| } 
  \hline
  \multicolumn{2}{|c|}{Strings after embedding} \\
  \hline
  $t_1^1$ & AACCGGGGTT $\dots$ \\
  $t_1^2$ & ACGTGGGAAC $\dots$\\
  $t_2^1$ & AACCGGGGTT $\dots$\\
  $t_2^2$ & ACGTCGGCGG $\dots$\\
  $t_3^1$ & AACCTTTACC $\dots$\\
  $t_3^2$ & ACTTTTAAAC $\dots$\\
  $t_4^1$ & AATTTCGGAA $\dots$\\
  $t_4^2$ & ATTTCGGAAT $\dots$\\
  \hline
\end{tabular}
\centerline{(a)}
\end{minipage}
\begin{minipage}[]{0.6\textwidth}
\centering
\begin{tabular}{l*{3}{c}r}
\ $i$              &$f_1^1(t_i^1)$  &$f_2^1(t_i^1)$ & $f_1^2(t_i^2)$ & $f_2^2(t_i^2)$   \\
\hline
$1$    & (A,T) & (A,C) & (C,G) & (G,G) \\
$2$     & (A,T) & (A,C) & (C,C) & (G,G)  \\
$3$    & (A,C) & (A,C) & (C,T) & (A,T) \\
$4$     & (A,A) & (A,T) & (T,C) & (G,T)  \\
\end{tabular}
\centerline{(b)}
\end{minipage}
\medskip
\caption{(a) Strings after embedding; (b) Signatures of LSH functions}  
\label{tab:strs2}
\end{table}


%\begin{table}[t]
%\centering
%\begin{tabular}{ |l|l| } 
%  \hline
%  \multicolumn{2}{|c|}{Strings after embedding} \\
%  \hline
%  $t_1^1$ & AACCGGGGTT $\dots$ \\
%  $t_1^2$ & ACGTGGGAAC $\dots$\\
%  $t_2^1$ & AACCGGGGTT $\dots$\\
%  $t_2^2$ & ACGTCGGCGG $\dots$\\
%  $t_3^1$ & AACCTTTACC $\dots$\\
%  $t_3^2$ & ACTTTTAAAC $\dots$\\
%  $t_4^1$ & AATTTCGGAA $\dots$\\
%  $t_4^2$ & ATTTCGGAAT $\dots$\\
%  \hline
%\end{tabular}
%\caption{Strings after embedding}  
%\label{tab:strs2}
%\end{table}

%\begin{table}[t]
%\centering
%\begin{tabular}{l*{3}{c}r}
%\ $i$              &$f_1^1(t_i^1)$  &$f_2^1(t_i^1)$ & $f_1^2(t_i^2)$ & $f_2^2(t_i^2)$   \\
%\hline
%$1$    & (A,T) & (A,C) & (C,G) & (G,G) \\
%$2$     & (A,T) & (A,C) & (C,C) & (G,G)  \\
%$3$    & (A,C) & (A,C) & (C,T) & (A,T) \\
%$4$     & (A,A) & (A,T) & (T,C) & (G,T)  \\
%\end{tabular}
%\caption{Signatures of LSH functions}  
%\label{tab:lshvalue}
%\end{table}

\paragraph{A Running Example}  
Table~\ref{tab:strs} shows the collection of input strings, and the set of LSH functions we use. Set distance threshold $K=3$. We choose parameters $r=2, z=2, m=2$ for \ebdjoin.  
Table~\ref{tab:CGK} shows two random strings $R^1, R^2$ (represented as the equivalent $\pi_j(\cdot)$'s; see Algorithm~\ref{alg:CGK}) that we use for the two rounds of CGK-embeddings. Table~\ref{tab:strs2}(a) shows the strings after CGK-embedding, and Table~\ref{tab:strs2}(b) shows the signatures of LSH functions. From Table~\ref{tab:strs2}(b) we find that $f_1^1(t_1^1)=f_1^1(t_2^1)=(A,T)$, $f_2^1(t_1^1)=f_2^1(t_2^1)=f_2^1(t_3^1)=(A,C), f_2^2(t_1^2)=f_2^2(t_2^2)=(G,G)$, and thus $ (s_1,s_2),(s_1,s_3),(s_2,s_3)$ are candidate pairs.  Finally after the verification step, we output $(s_1,s_2)$, $(s_1,s_3)$ as the results of similarity joins. 

\paragraph{Choices of parameters}  There are three parameters $m, z, r$ in \ebdjoin\ that we need to specify.  Recall that $m$ is the length of the LSH signature, or, the number of primitive hash functions $h \in \H$ we use in each $f \in \F(m)$; and $z$ is the number of LSHs we use for each string generated by CGK-embedding.  The larger $z$ and $m$ are, the better LSH performs in terms of accuracy and filtering effectiveness.  The product $m \cdot z$ will contribute to the total running time of the algorithm.  On the other hand, $r$ is number of CGK-embeddings we perform for each input string.  The larger $r$ we use, the smaller distortion we will get (see Figure~\ref{fig:CGK-distortion}).  

The concrete choices of $m, z$ and $r$ depend on the data size, distance thresholds, computation time/space budget and accuracy requirements.  For our datasets we have tested a number of parameter combinations.  We refer readers to Section~\ref{sec:embed-exp} for some statistics.
We have observed that $r = z = 7$, and $m = \log_2 N - \lfloor \log_2 x \rfloor$ where $x \% = K/N$ is the {\em relative} edit distance threshold, are good choices to balance the resource usage and the accuracy.

\paragraph{Running time}  The preprocessing step takes time $O(r \cdot z \cdot P + r \cdot n \cdot 3N \abs{\Sigma})$.  The time cost of LSH-based filtering depends on the effectiveness of the sliding window pruning; in the worst case it is $O(n r z m)$ where $m$ counts the cost of evaluating a hash function $f \in \F(m)$. Finally, the verification step costs $O(N K \cdot Z)$ where $Z$ is the number of candidate pairs after LSH-based filtering.


\subsection{Further Speed-up}
\label{sec:speedup}

Note that in the CGK-Embedding (Algorithm~\ref{alg:CGK}), we always pad the output strings $x'$ up to length $3N$, where $N = \max_{i \in [n]} \{\abs{s_i}\}$.  This approach is not very efficient for datasets containing strings with very different lengths (for example, our datasets \uniref\ and \trec; see Section~\ref{sec:setup}), since we need to pad a large number of `$\perp$' to the output strings which can be a waste of time. For example, for two strings $s_1$ and $s_2$ where $\abs{s_1}, \abs{s_2} \ll N$, if we map them to bit vectors $s'_1$ and $s'_2$ of size $3N$, then most of the aligned pairs in $s'_1$ and $s'_2$ are $(\perp, \perp)$s which carry almost no information.  Then if we use bit-sampling LSH for the Hamming distance we need a lot of samples in order to hit the interesting region, that is, the coordinates of strings in $s'_1$ and $s'_2$ where at least one of the two characters is {\em not} `$\perp$'.  
This is time and space expensive. 
%To avoid this situation, we should try to minimize the number of ``$\perp$''s in the output strings of the embedding.  
We propose two ways to handle this issue. 

\vspace{-0.5mm} 

\paragraph{Grouping}
We first partition the set of strings of $\S$ to $(N'/K - 1)$ groups where $N' = \lceil N/K \rceil \cdot K$.  The $i$-th group contains all the strings of lengths $((i-1) K, (i+1) K]$. Note that each string will be included in two groups (i.e., the redundancy), and every pair of strings with distance at most $k$ will both be included in at least one of the groups. We then apply \ebdjoin\ on each group, and union the outputs at the end.  Due to the redundancy this approach may end up evaluating at most twice of the total number of candidates.

\vspace{-0.5mm} 

\paragraph{Truncation}
The second method is to use truncation, that is, we truncate each embedded string to predefined threshold $L$.  We then apply \ebdjoin\ on all the truncated strings.  Note that after truncation we essentially assume that all the bits after the $L$-th position in the embedded strings are the same, and thus truncation will {\em not} increase the Hamming distance of any pair of strings, and consequently will not introduce any false negative.  It can introduce some false positives but this is not a problem since we have a verification step at the end to remove all the false positives.

From the theory of the CGK-embedding we know that the number of the embedding steps is tightly concentrated around $2N$ where $N$ is the length of the original string (and then possibly many `$\perp$' will be appended afterwards). This indicates that for a datasets of strings of different lengths,  setting $L = 2 avg(\S)$ where $avg(\S)$ is the average length of the strings in $\S$ may be a good choice.  From our experimental results (see Section~\ref{sec:embed-exp}) we noticed that we can also be a little bit more aggressive to set $L = avg(\S)$.
\smallskip

Our experimental results (see Section~\ref{sec:embed-exp}) show that truncation always has the better performance than grouping on our tested datasets.  Therefore in the rest of the paper we always use truncation.



