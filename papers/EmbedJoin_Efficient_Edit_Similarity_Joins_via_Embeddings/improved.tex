\begin{algorithm}[t]
\caption{Preprocessing+ ($\mathcal{S}$, $K$, $r$, $z$, $m$, $\Delta$)}
\label{alg:preprocessing2}
\begin{algorithmic}[1]
\Require Set of input strings $\mathcal{S} = \{s_1, \ldots, s_n\}$, distance threshold $K$, and parameters $r$, $z$, $m$ and $\Delta$ described in Table~\ref{tab:notation}
\Ensure  Strings in $\mathcal{S}$ in the sorted order, strings after CGK-embedding $\{t_{i,k}^\ell\ |\ \ell \in [r], i \in [n], k \in [\lceil K/\Delta \rceil]\}$, and hash tables $\{\D_j^{\ell} \ |\ \ell \in [r], j \in [z]\}$.
\State Sort $\mathcal{S}$ first by string length increasingly, and second by the alphabetical order. 

\ForEach{$\ell \in [r]$}
	\ForEach{$j \in [z]$}
	\State Initialize hash table $\D_j^{\ell} $ by generating a random hash function $f_j^{\ell} \in \F(m)$  
	\EndFor
\EndFor

\ForEach{$\ell \in [r]$}
\State Generate a random string $R^{\ell} \in \{0,1\}^{3 N \abs{\Sigma}}$
\ForEach{$s_i\in \mathcal{S}$}
\ForEach{$k\in [\lceil K/\Delta \rceil]$}
\State $s_{i,k} \gets s_i[(k-1)\Delta+1, \abs{s_i}]$ \label{line:c-1}
\State $t_{i,k}^{\ell}  \leftarrow $ CGK-Embedding($s_{i,k}$, $R^{\ell}$) \label{line:c-2}
\EndFor
\EndFor
\EndFor
\end{algorithmic}
\end{algorithm}


\begin{algorithm}[]
\caption{EmbedJoin+ ($\mathcal{S}$, $K$, $r$, $z$, $m$, $\Delta$, $T$)}
\label{alg:embed-joins+}
\begin{algorithmic}[1]
\Require Set of input strings $\mathcal{S} = \{s_1, \ldots, s_n\}$, distance threshold $K$, and parameters $r$, $z$, $m$, $\Delta$ and $T$ described in Table~\ref{tab:notation}
\Ensure  $\O \gets \{(s_i, s_j)\ |\ s_i, s_j \in \S; i \neq j; \text{ED}(s_i, s_j) \le K\}$ 
\State Preprocessing+ ($\mathcal{S}$, $K$, $r$, $z$, $m$, $\Delta$) 
\Comment Using Algorithm~\ref{alg:preprocessing2}
\State  $\mathcal{C} \leftarrow \emptyset $  
\Comment Collection of candidate pairs

\ForEach{$s_i \in \mathcal{S} \text{ (in the sorted order)}$}
	\ForEach{$\ell \in [r]$}
		\ForEach{$j \in [z]$}
			\ForEach{$k\in [\lceil K/\Delta \rceil]$}
			\ForEach{tuple $(s,k') $ ($s \neq s_i$)  stored in the $f_j^\ell (t_{i,k}^\ell)\text{-th bucket of table} \ \D_j^\ell $}
				\If{$\abs{s_i} - \abs{s} \le K$} \label{ln:a-1}
				\State $\mathcal{C}  \leftarrow \mathcal{C}  \cup (s, s_i, k', k)$ \label{line:d-1}
\Else
				\State Remove $(s,k')$ from $\D_j^\ell$ \label{ln:a-2}
				\EndIf
			\EndFor
			\State Store $(s_{i},k)$ in the $f_j^\ell (t_{i,k}^\ell)$-th bucket of $\D_j^\ell$ \label{line:d-2}
		\EndFor
		\EndFor
	\EndFor
\EndFor
\State Count the frequency of each tuple $(x, y,k_x,k_y)$ in  $\mathcal{C}$
\ForEach {$(x, y,k_x,k_y) \in \mathcal{C}$ with count $\ge T$ \label{line:d-3}}
	\If{$\text{ED}(x, y) \le K$}
	\State $\mathcal{O} \leftarrow  \mathcal{O} \cup (x, y)$
	\EndIf
\State Remove all tuples $(x, y, \cdot, \cdot)$ in $\mathcal{C}$ \label{line:d-4}
\Comment We only need one pair of substrings of $(x, y)$ with count at least $T$
\EndFor
\end{algorithmic}
\end{algorithm}
\section{The \ebdjoin+ Algorithm}
\label{sec:improved}



An important application of similar joins is to find similar pairs of strings in a biological datasets that consists of {\em random} reads of the human genomes or protein sequences. A sufficient number of similar pairs of reads can be used to reconstruct the original genome or protein sequence~\cite{GYB10}.
In those datasets, for two strings $x$ and $y$ who are overall similar, there could be a long prefix of insertions at the beginning of one of the strings in the optimal alignment of $x$ and $y$, which we call the {\em shift}.  More precisely, given two strings $x[1..N]$ and $y[1..M]$, we define the shift between $x$ and $y$ to be $\sft(x, y) = \max\{\sft_1, \sft_2\}$ where 
$$\sft_1 = \max_{t \in [N]}\{\text{ED}(x[1..N], y[1..M]) = t + \text{ED}(x[t+1..N], y[1..M])\},\quad \text{and}$$ 
$$\sft_2 = \max_{t \in [M]}\{\text{ED}(x[1..N], y[1..M]) = t + \text{ED}(x[1..N], y[t+1..M])\}$$
When applying \ebdjoin\ directly to find similar pairs of strings on such datasets under large thresholds, the shift may contribute most of the edits which will be further ``amplified'' by the CGK-embedding, since consecutive errors is one of the worst cases for the distortion of the CGK-embedding.  This phenomenon may introduce a large number of false negative, and consequently reduce the accuracy of the join results.

In this section we propose an improved version of \ebdjoin\ called \ebdjoin+ to handle string shifts.  \ebdjoin+ contains several new ideas which we will illustrate below.

%\paragraph{Embedding from Multiple Positions}
A natural way to handle shifts is to start the CGK-embedding from multiple positions of the strings.  Given a parameter $\Delta$ which we will set later, for each string $s_i$, we consider $\lceil K / \Delta \rceil$ substrings which are suffixes of $s_i$ with starting positions $1, \Delta+1, \ldots, (\lceil K/\Delta \rceil - 1)\Delta + 1$;  we denote these substrings by $s_{i,1}, \ldots, s_{i,{\lceil K/\Delta \rceil}}$.  By embedding all the substrings, we can guarantee that for any pair of strings $(s_i, s_j)$ such that $\text{ED}(s_i, s_j) \le K$, there is a pair of substrings $(s_{i,p}, s_{j,q})$ such that  $\sft(s_{i,p}, s_{j,q}) \le \Delta/2$.  

However, the direct implementation of this idea will cause the number of false positives in the set of candidate pairs (after the CGK-embedding and LSH) to increase significantly, and consequently make the verification the bottleneck.  In order to reduce the number of false positives, we require a candidate $(s_i, s_j)$ to have a pair of substrings $(s_{i,p}, s_{i,q})$ with at least $T \in [z]$ matched hash signatures in the process of LSH (recall that $z$ is number of hash functions we use in LSH).  Intuitively, when $T > 1$, this requirement will make it harder for a pair to be selected as a candidate.  More precisely, let $p$ be the collision probability of a pair of substrings under a single hash function, then the probability that the two substrings have at least $T$ common hash signatures is 
$$p_T = 1- \sum_{i=0}^{T-1} \binom zi p^{i} (1-p)^{z-i}.$$

\begin{figure}[t]
\centering
\includegraphics[height = 1.6in]{sig.eps}
\caption{The probability of having $T=1,2,3$ matched hash signatures for similar and dissimilar pairs under different numbers of hash functions $z$.  We assume that the collision probability for a similar pair is $0.9^{13} = 0.25$, and that for a dissimilar pair is $0.7^{13} = 0.01$, where $m = 13$ is the number of bits we use in the bit-sampling LSH for the Hamming distance.}
\label{fig:numsig}
\end{figure}

We plot $p_T$ for two different $p$ values in Figure~\ref{fig:numsig}.  It can be seen that when $T$ becomes larger, the gap of probabilities between similar and dissimilar pairs becomes bigger.  However, for larger $T$ we will need more hash functions to guarantee that the number of false negatives is small, which will increase the time of performing LSH.  In practice, we observed that when $K/\Delta > 1$ (i.e., we will produce at least $2$ substrings for each string), then setting $T = 2$ is a good choice.  Otherwise if $K/\Delta \le 1$,  then we set $T = 1$, and \ebdjoin+ degenerates to \ebdjoin.



The pseudocode of \ebdjoin+ is very similar to that of \ebdjoin; see Algorithm~\ref{alg:preprocessing2} and Algorithm~\ref{alg:embed-joins+}.  In the preprocessing (Algorithm~\ref{alg:preprocessing2}), the only difference is that we need to embed for each string $s_i$ the substrings $s_{i,1}, \ldots, s_{i,{\lceil K/\Delta \rceil}}$ (Line~\ref{line:c-1}-\ref{line:c-2}). In the main algorithm (Algorithm~\ref{alg:embed-joins+}), for each substring $s_{i,k}$ generated from string $s_i$, we record both its original string and its substring index in the hash table, that is, $(s_i, k)$ (Line~\ref{line:d-2}).  For each pair of substrings in the same hash table, we record the match using their original strings and their indices, in the form of $(s_i, s_j, k_{s_i}, k_{s_j})$ (Line~\ref{line:d-1}).  At the end we need to count and verify for each pair $(x, y)$ whether at least one of their substring pairs have at least $T$ matches (Line~\ref{line:d-3}-\ref{line:d-4}).

\paragraph{Choices of parameters}
Compared with \ebdjoin, we have one more parameter to choose in the algorithm \ebdjoin+, that is, the ``step length'' $\Delta$ for creating substrings.  From the theory of CGK-embedding, with a good probability a consecutive set of insertions of length $\Delta$ will introduce $c \Delta^2$ (for some constant $c$) Hamming errors after the embedding.  Since we truncate each string at the position $avg(\S)$, it is meaningful to ensure that $c \Delta^2 \le avg(\S)$. On the other hand, we would like to set $\Delta$ as large as possible since $\lceil K/\Delta \rceil$ substrings generated for each string $s_i$ will contribute to both time and space of the algorithm. We thus choose $\Delta \approx \sqrt{avg(\S)}$ or a bit smaller.   

As already mentioned, the variable $T$ is determined by $K$ and $\Delta$: When $\lceil K/\Delta \rceil > 1$ we set $T = 2$; otherwise \ebdjoin+ degenerates to \ebdjoin.

Similar to \ebdjoin, in \ebdjoin+ we set $r = 7$ and $m = \log_2 N - \lfloor \log_2 x \rfloor$ where $x \% = K/N$ is the relative edit distance threshold.  For the value of $z$, we set $z = 16$ when $T = 2$, and $z = 7$ when $T = 1$.
This is according to the fact that when $T$ increases, we have to increase the number of hash functions in LSH to achieve a good accuracy. 

\paragraph{Running time}
The preprocessing step takes time $O(r \cdot z \cdot P + r \cdot \lceil K/\Delta \rceil \cdot n \cdot 3N \abs{\Sigma})$.  The time cost of LSH-based filtering again depends on the effectiveness of the sliding window pruning; in the worst case it is $O(n r z m \cdot \lceil K/\Delta \rceil)$ where $m$ counts the cost of evaluating a hash function $f \in \F(m)$. Finally, the verification step costs $O(N K \cdot Z)$ where $Z$ is the number of candidate pairs after LSH-based filtering.



