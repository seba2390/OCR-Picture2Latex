\section{Tools}
\label{sec:tool}

%In this section we present our algorithm \ebdjoin.  
Before presenting our algorithm, we would like to introduce a few tools that we shall use in \ebdjoin+, including the CGK-embedding, the LSH for the Hamming distance, and an algorithm for exact edit distance computation. We list in Table~\ref{tab:notation} a set of notations that will be used in the presentation.


%\begin{adjustwidth}{-1cm}{-1cm}
\begin{table}[t]
\centering
\begin{tabular}{|p{.12\textwidth}| p{.85\textwidth}| m{.01\textwidth}|} 
\hline
Notation & Definition\\ 
\hline
$[n]$ & $[n] = \{1, 2, \ldots, n\}$ \\
\hline
$K$ & Edit distance threshold\\ 
\hline
$\S$ & The set of input strings \\ 
\hline
$s_i$ & The $i$-th string in $\S$ \\ 
\hline
$\abs{x}$ & Length of string $x$ \\ 
\hline
$n$ & Number of input strings, i.e.,  $n = \abs{\S}$\\ 
\hline
$N$ & Maximum length of strings in $\S$\\ 
\hline
$\Sigma$ & Alphabet of strings in $\S$ \\ 
\hline
$r$ & Number of CGK-embeddings for each input string\\ 
\hline
$t_i^{\ell}$ & The output string generated by the $\ell$-th CGK-embedding of $s_i$ \\ 
\hline
$z$ & Number of hash functions used in LSH for each string generated by CGK-embedding\\ 
\hline
$m$ & Length of the LSH signature\\ 
\hline
$f_j^{\ell} $ &   $f_j^{\ell} : \Sigma^N \to \Sigma^m$, the $j$-th ($j\in[z]$)  LSH function for each string generated by the $\ell$-th CGK-embedding
\\ 
\hline
$\mathcal{D}_j^{\ell}$ & The hash table corresponding to the LSH function $f_j^{\ell}$ \\ 
\hline
$\Delta$ & A parameter for dealing with shifts  \\ 
\hline
$s_{i,k}$ & The $k$-th substring of $s_i$ starting at the $((k-1)\Delta+1)$-th character\\ 
\hline
$t_{i,k}^\ell$ & The output string generated by the $\ell$-th CGK-embedding of $s_{i,k}$\\ 
\hline
$T$ & The threshold of the number of matched hash signatures for a pair of substrings\\ 
\hline
\end{tabular}
\caption{Summary of Notations}
\label{tab:notation}
\end{table}
%\end{adjustwidth}


\subsection{The CGK-Embedding}
\label{sec:CGK}

We describe the CGK-embedding in Algorithm~\ref{alg:CGK}. Below we  illustrate the main idea behind the CGK-embedding, which we believe is useful and important to understand the intuition of \ebdjoin+.  We note that the original algorithm in \cite{CGK16} was only described for binary strings, and it was mentioned that we can encode an alphabet $\Sigma$ into binary codes using $\log\abs{\Sigma}$ bits for each character.  In our rewrite (Algorithm~\ref{alg:CGK}) we choose to use the alphabet $\Sigma$ directly without the encoding.  This may give some performance gain when the size of the alphabet is small. 


\begin{algorithm}[t]
\begin{algorithmic}[1]
\Require A string $x \in \Sigma^\eta$ for some $\eta \le N$, and a random string $R \in \{0,1\}^{ 3 N |\Sigma|}$
\Ensure A string $x' \in \Sigma^{3N}$
\smallskip

%\State Pad $x$ with ``$\perp$'' in the tail to make it of length $N$; denote the new string by $\tilde{x}$

\State Interpret $R$ as a set of functions 

$\pi_1,\dots,\pi_{3N}:  \Sigma \rightarrow \{0,1\}$; for the $k$-th char $\sigma_k$ in $\Sigma$, 

$\pi_j(\sigma_k) = R[(j-1) \cdot \abs{\Sigma} + k]$
\State $i \leftarrow 1$
\State $x' \leftarrow \emptyset$ 
\For{$j \in [3N]$} 
	\If{$i\le \abs{x}$}
		\State $x' \leftarrow x' \odot x[i]$ 
		\Comment the ``$\odot$'' denotes concatenation
		\State $i \leftarrow i+\pi_j(x[i])$
	\Else
		\State $x' \leftarrow x' \odot \perp$
		\Comment ``$\perp$'' can be an arbitrary character outside $\Sigma$
	\EndIf
\EndFor 
\end{algorithmic}
\caption{CGK-Embedding($s$, $R$) \cite{CGK16}}
\label{alg:CGK}
\end{algorithm}


Let $N$ be the maximum length of all input strings in $\S$. The CGK-embedding maps a string $x \in \S$ to an output string $x' \in \Sigma^{3N}$ using a random bit string $R \in \{0,1\}^{3N\abs{\Sigma}}$.  We maintain a counter $i \in [1 .. \abs{x}]$ pointing to the input string $x$, initialized to be $1$.  The embedding proceeds by steps $j = 1, \ldots, 3N$. At the $j$-th step, we first copy $x[i]$ to $x'[j]$. Next, with probability $1/2$, we increase $i$ by $1$, and with the rest of the probability we keep $i$ to be the same.  At the point when $i > \abs{x}$, if $j$ is still no more than $3N$, we simply pad an arbitrary character outside the dictionary $\Sigma$ (denoted by ``$\perp$'' in Algorithm~\ref{alg:CGK}) to make the length of $x'$ to be $3N$.  In practice this may introduce quite some overhead for short strings in the case that the string lengths vary significantly. We will discuss in Section~\ref{sec:speedup} how to efficiently deal with input strings of very different lengths.

Now consider two input stings $x$ and $y$.  We use $i_0$ and $i_1$ as two counters pointing to $x$ and $y$ respectively.   At the $j$-th step, we first copy $x[i_0]$ to $x'[j]$, and $y[i_1]$ to $y'[j]$, and then decide whether to increment $i_0$ and $i_1$ using the random bit string $R$.  There are four possibilities: (1) only $i_0$ increments; (2) only $i_1$ increments; (3) both $i_0$ and $i_1$ increment; and (4) neither $i_0$ nor $i_1$ increments.  Let $d = i_0 - i_1$ be the position shift of the two counters/pointers on the two strings.  Note that if $x[i_0] = y[i_1]$, then only the cases (3) and (4) can happen, so that $d$ will remain the same.  Otherwise if $x[i_0] \neq y[i_1]$, then each case can happen with probability $1/4$ -- whether $i_0$ or $i_1$ will increment depends on the two random hash values $\pi_j(x[i_0])$ and $\pi_j(y[i_1])$. Thus with probability $1/4$, $1/2$ and $1/4$, the value $d$ will increment, remain the same, or decrement, respectively.  Ignoring the case when the value $d$ remains the same, we can view $d$ as a (different) {\em simple random walk} on the integer line with $0$ as the origin.  

We now try to illustrate the high level idea of why CGK-embedding gives an $O(K)$ distortion.  Let $u = \abs{x}$ and $v = \abs{y}$.  Suppose that at some step $j$, letting $p = i_0(j)$ (the value of $i_0$ at step $j$) and $q = i_1(j)$, we have two tails $x[p .. u] = \alpha \circ \tau$ and $y = y[q .. v] = \tau$ where $\alpha, \tau$ are two substrings and $\abs{\alpha} = k \le K$. That is, we have $k$ consecutive deletions in the optimal alignment of the two tails.  Now if after a few random walk steps, at step $j' > j$, we have $p' = i_0(j') \ge p + k$, $q' = i_1(j') \ge q$ and $p' - q' = (p - q) + k$, then the two tails $x[p'..u]$ and $y[q'..v]$ can be perfectly aligned, and consequently the pairs of characters in the output strings $x', y'$ will always be the same; in other words, they will {\em not} contribute to the Hamming distance from step $j'$. 

Now observe that since the value of $d$ changes according to a simple random walk, by the theory of random walk, with probability $0.999$ it takes at most $O(k^2)$ steps for $d$ to go from $(p - q)$ to $(p' - q')$ where $\abs{(p - q) - (p' - q')} = k$.  Therefore the number of steps $j$ where $x'[j] \neq y'[j]$ is bounded by $O(k^2)$.  
%In general, every time $k$ edits occur when ``walking'' on $x$ and $y$, with probability $0.999$ it will take at most $O(k^2)$ steps to ``resolve'' the differences and go back to the right alignment track.  
This is roughly why $\text{Ham}(x', y')$ can be bounded by $O(K^2)$ if $\text{ED}(x, y) \le K$, and consequently the distortion can be bounded by $O(K)$.

%On the other hand, it is easy to show that $\text{Ham}(x', y') \ge \text{ED}(x, y) / 2$ holds with probability $1 - o(1)$. Combining the two we conclude that the embedding has an $O(K)$ distortion.

\paragraph{Small Distortion is Good for Edit Similarity Join}  We now explain why the distortion of the embedding matters. If we have an embedding $f$ such that for any pair of input strings $(x, y)$, the distortion of the embedding is upper bounded by $D$, then the set $\{(x, y)\ |\ \text{Ham}(f(x), f(y)) \le D \cdot K\}$ will include all pairs $(x, y)$ such that $\text{ED}(x, y) \le K$.  Therefore a small $D$ can help to reduce the number of false positives, and consequently reduce the verification time which typically dominates the total running time.

\medskip

\noindent{\bf Why CGK-embedding Does Better in Practice?}
Although the worst-case distortion of CGK-embedding can be large when $\text{ED}(x, y)$ is large, we have observed that its practical performance on the datasets that we have tested is much better.  While it is difficult to fully understand this phenomenon without a thorough investigation of the actual properties of the datasets, we can think of the following reasons.

First, if a set of $z$ edits fall into an interval of length $O(z)$, {\em and} the difference between the numbers of insertions and deletions among the $z$ edits is at most $O(\sqrt{z})$ (substitutions do not matter), then with probability $0.999$ after $O(z)$ walk steps the random walk will re-synchronize.  In other words, the distortion of the embedding is $O(1)$ with probability $0.999$ on this cluster of edits.  We have observed that in our protein/genome datasets (Section~\ref{sec:setup}) the edits are often clustered into small intervals; in each cluster most edits are substitutions, and consequently the difference between the numbers of insertions and deletions is small. 

Second, in the task of differentiating similar pairs of strings and dissimilar pairs of strings, as long as the distance gap between strings is preserved after the embedding, the distortion of CGK-embedding will not affect the performance by much. In particular, when the distortion of CGK-embedding is $\Theta(k)$ (which is very likely when edits are well separated), the embedding actually {\em amplifies} the distance gap between similar and dissimilar pairs, which makes the next LSH step easier.

%We believe that our datasets have (some of) these properties.  First, we have observed that in the protein/DNA datasets the edits are often clustered into small intervals; in each cluster most edits are substitutions, and consequently the difference between the numbers of insertions and deletions is small.  Second, in our datasets, in most cases, the distance gaps between similar pairs and dissimilar pairs are large so that the distortion of CGK is not a big issue.
 
To further improve the effectiveness of the CGK-embedding, we run the embedding multiple times and then take the one with the minimum Hamming distance. That is, we choose the run with the best distortion.  This is just a heuristic, and cannot improve the distortion by much in theory, but we have observed that for the real-world datasets that we have tested, repeating and then taking the minimum does help to reduce the distortion. In Figure~\ref{fig:CGK-distortion} we depicted the best distortions under different numbers of runs of the CGK-embedding on a real-world genome dataset.
%\ (see Section~\ref{sec:setup} for a detailed description of our datasets).  

\begin{figure}[t]
\centering
\includegraphics[height = 1.6in]{distortion.eps}
\caption{The CDF of the best distortions of 1000 random pairs strings from the \genoa\ dataset, under different numbers of CGK-embeddings (value $r$)}
\label{fig:CGK-distortion}
\end{figure}


\subsection{LSH for the Hamming Distance}
\label{sec:LSH}

Our second tool is the LSH for the Hamming distance, introduced in~\cite{IM98,GIM99} for solving nearest neighbor problems.  We first give the definition of LSH.  By $h \in_r \H$ we mean sampling a hash function $h$ randomly from a hash family $\H$. 

\begin{definition}(Locality Sensitive Hashing \cite{GIM99})
Let $U$ be the item universe, and $d(\cdot,\cdot)$ be a distance function. We say a hash family $\mathcal{H}$ is $(l, u, p_1,p_2)$-sensitive if for any $x, y \in U$
\begin{itemize}
\item if $d(x, y)\le l$, then $\Pr_{h\in_r \H}[h(x) = h(y)] \ge p_1$,
\item if $d(x, y)\ge u$, then $\Pr_{h\in_r \H}[h(x) = h(y)] \le p_2$.
\end{itemize}
\end{definition}

We will make use of the following vanilla version of LSH for the Hamming distance.
\begin{theorem}(Bit-sampling LSH for Hamming  \cite{GIM99})
For the Hamming distance over vectors in $\Sigma^N$, for any $d > 0, c > 1$, the family $$\mathcal{H}_N = \{v_i: v_i(b_1, \dots, b_N) = b_i\ |\ i \in [N]\}$$ is  $(d, cd, 1-{d}/{N}, 1-{cd}/{N})$-sensitive.
\end{theorem}

We can use the standard AND-OR amplification method\footnote{See, for example, \url{https://en.wikipedia.org/wiki/Locality-sensitive_hashing}.} to amplify the gap between $p_1$ and $p_2$.
We first concatenate $m$ ($m$ is a parameter) hash functions, and define 
\begin{eqnarray*}
f = h_1 \circ h_2 \circ \ldots \circ h_m \text{ where } \forall i \in [m], h_i \in_r \H,
\end{eqnarray*}
such that for $x \in U$, $f(x) = (h_1(x), h_2(x), \ldots, h_m(x))$ is a vector of $m$ bits.  Let $\F(m)$ be the set of all such hash functions $f$.
We then define (for a parameter $z$)
\begin{eqnarray*}
g = f_1 \vee f_2 \vee \ldots \vee f_z, \text{ where } \forall j \in [z], f_j \in_r \F(m),
\end{eqnarray*}
such that for $x, y \in U$ $g(x) = g(y)$ if and only if there is at least one $j \in [z]$ for which $f_j(x) = f_j(y)$.  Easy calculation shows that $g$ is
\begin{eqnarray*}
\left(d, cd, 1 - \left(1 - \left({d}/{N}\right)^m\right)^z, 1 - \left(1 - \left( {cd}/{N} \right)^m \right)^z \right) \text{-sensitive.}
\end{eqnarray*}
 By appropriately choosing the parameters $m$ and $z$,  we can amplify the gap between $p_1$ and $p_2$ so as to reduce the numbers of false positives/negatives.  
%Note that the total number of primitive hash functions $h \in \H$ we have used is $\beta = m \cdot z$, which will contribute to the running time.  

%We comment that this vanilla version of LSH is enough for our applications, and its good time performance fits our needs well.

\subsection{Exact Edit Distance Computation for Verification}
\label{sec:exact-ED}

We will use the classic algorithm by Ukkonen~\cite{Ukkonen85} for computing threshold edit distance as our verification algorithm.  In the high level, defining the diagonal $d$ of a matrix $D$ to be the set of all entries $D_{i, i+d}$,
the algorithm tries to fill {\em a subset of} the entries in the $2K+1$ diagonals $\{-K, \ldots, K\}$ in the $N \times N$ dynamic programming matrix, which are sufficient to give the final output.  The worst-case running time of this algorithm is $O(N K)$. But if one of the strings is a random string, then the algorithm only uses $O(N + K^2)$ time in expectation \cite{Myers86}.   In \cite{Myers86}, Myers also proposed another algorithm using suffix-tree whose worse-case running time is $O(N+K^2)$. However, we found that suffix-tree is computational expensive in practice and has no advantage over a ``brute force'' table filing \cite{Ukkonen85}. 

We also note that Belazzougui and Zhang~\cite{BZ16} (and independently, Chakraborty et al.~\cite{CGK16b}) showed that the $O(N + K^2)$ running time is also achievable in the simultaneous streaming model where we can only scan each string once in the coordinated fashion.  However, the algorithms in \cite{BZ16,CGK16b} still needs to use suffix-tree.  Chakraborty et al.~\cite{CGK16b} also proposed an algorithm with $N + O(K^3)$ running time in the simultaneous streaming model {\em without} using suffix-tree, but this bound would be large when the distance threshold $K$ is large, say, $20\%$ of the string length $N$.

In an earlier version of this paper~\cite{ZZ17} we used the algorithm in~\cite{LDW11} for computing edit distance in the verification step.  We later found that it is more efficient to use Ukkonen's algorithm.



