\section{Related Work}
\label{sec:related}

\paragraph{Similarity Joins for Edit Distance}
The edit similarity join problem has been studied extensively in the literature.  We refer the readers to \cite{JLFL14} for a comprehensive survey.  A widely adopted approach to this problem is to first generate for each string a set of signatures/substrings. For example, in the $q$-gram signature, we generate all substrings of length $q$ (e.g., when $q=2$, the $2$-grams of {\tt ACCAT} is \{{\tt AC, CC, CA, AT}\}). We then perform a filtering step based on the frequencies, positions and/or the contents of these substrings.  The filtering step will give a set of candidate (similar) pairs, for each of which we use a dynamic programming algorithm for edit distance to verify its {\em exact} similarity.  Concrete algorithms of signature-based approach include {\tt GramCount} \cite{GJKMS01}, {\tt AllPair} \cite{BMS07}, {\tt FastSS} \cite{BHSH07}, 
{\tt ListMerger} \cite{LLL08}, {\tt EDJoin} \cite{XWL08}, {\tt QChunk} \cite{QWL11}, {\tt VChunk} \cite{WQX13}, {\tt PassJoin} \cite{LDW11}, and {\tt AdaptJoin} \cite{WLF12}.  We will briefly describe in Section~\ref{sec:setup} the best ones among these algorithms which we use as competitors to \ebdjoin+ in our experiments.

While different signature-based algorithms use different filtering methods, their common feature is to first compute some upper or lower bounds, and then prune those pairs $(x, y)$ for which $g(sig(x), sig(y))$ is above or below the predetermined upper/lower bounds, where $g$ is a predefined function, and $sig(x), sig(y)$ are signatures of $x$ and $y$ respectively.  The main drawback of signature-based approach is that the information about the sequence ordering is somewhat lost when converting strings to a set of substrings.  Another issue is that the precomputed upper/lower bounds may be too loose for effective pruning.  

There are a few other approaches for computing edit similarity joins, such as trie-based algorithm {\tt TrieJoin}~\cite{WLF10}, tree-based algorithm {\tt M-Tree}~\cite{CPZ97}, enumeration-based algorithm {\tt PartEnum}~\cite{AGK06}.  However, as reported in \cite{JLFL14}, these algorithms are not very effective on datasets of long strings.


%We have already discussed related work on edit similarity join in the introduction.  In this section we briefly discuss other works that are related to this paper.

\paragraph{Similarity Joins for Other Metrics} 
Similarity joins have been studied for a number of other metrics~\cite{GJKMS01,AGK06,BMS07,LLL08,XWLY08,WLF12,LDW11,ZLG11}, including Cosine, Jaccard, Overlap and Dice.  A survey of these works is beyond the scope of this paper, and we again refer reader to \cite{JLFL14} for an overview.   



\paragraph{Other Related Work on Edit Distance}
Edit distance is also a notoriously difficult metric for sketching and embeddings, and very little is known in these frontiers.  As mentioned, embedding enables us to study the similarity join problem in an easier metric space. On the other hand, if we can efficiently obtain small sketches of the input strings, then we can solve the similarity join problem on smaller inputs.
Ostrovsky and Rabani proposed an embedding from the edit metric to the $\ell_1$ metric with an $\exp(O(\sqrt{\log N\log\log N}))$ distortion~\cite{OR07} where $N$ is the length of the string. A corresponding distortion lower bound of $\Omega(\log N)$ has been obtained by Kraughgamer and Rabani~\cite{KR09}.  Recently Chakraborty et al.\ gives a weak embedding to the Hamming space~\cite{CGK16} with an $O(K)$ distortion,\footnote{In a weak embedding, the distortion holds for {\em each} pair of strings with constant probability, say, 0.99. In contrast, in a strong embedding, with probability $0.99$ the distortion holds for {\em all} pairs of strings simultaneously.} which serves as the main tool in our algorithm.   For sketching, very recently Belazzougui and Zhang~\cite{BZ16} proposed the first almost linear time sketching algorithm that gives a sketch of sublinear size (more precisely, $O(K^8 \log^5 N)$), which, unfortunately, is still too large to be useful in practice in its current form. 

We will briefly survey algorithms for computing edit distance in the RAM and simultaneous streaming models in Section~\ref{sec:exact-ED}.

%Computing edit distance in the RAM model has been studied for decades.  It is well-known that the edit distance verification problem under distance threshold $K$ can be solved in time $O(N K)$ by dynamic programming~\cite{Ukkonen85}.  It has been further improved to $O(N + K^2)$ \cite{Myers86}.
%, but the algorithm in \cite{LMS98} employs suffix trees and thus may not have advantage in practice.
