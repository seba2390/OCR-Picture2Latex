\section{Introduction}
\label{sec:intro}

Given a collection of strings, the task of similarity join is to find all pairs of strings whose similarities are above a predetermined threshold, where the similarity of two strings is measured by a specific distance function. Similarity join is a fundamental problem in data cleaning and integration (e.g., data deduplication), bioinformatics (e.g., find similar protein/DNA sequences), collaborative filtering (e.g., find user pairs of similar interests), natural language processing (e.g., automatic spelling corrections), etc. It has been studied extensively in the literature (see \cite{JLFL14} for a survey), and has been identified as one of the primitive operators for database systems~\cite{CGK06}.

In this paper we study similarity join under edit distance.  The edit distance between two strings $x$ and $y$, denoted by $\text{ED}(x, y)$, is defined to be the minimum number of edit operations (insertion, deletion and substitution) to transfer $x$ to $y$.  Formally, given a collection of strings $\S = \{s_1, s_2, \ldots, s_n\}$ over alphabet $\Sigma$, a similarity threshold $K$, edit similarity (self)join outputs
$$\{(s_i, s_j)\ |\ s_i, s_j \in \S; i \neq j; \text{ED}(s_i, s_j) \le K\}.$$ For example, given strings  {\tt ACCAT}, {\tt CCAAT}, {\tt GCCCT}, {\tt CACGA}, {\tt AACGG} and $K = 2$, the output pairs will be  ({\tt ACCAT, CCAAT}), ({\tt ACCAT, GCCCT}), ({\tt CACGA, AACGG}).

Compared with the Hamming distance
%\footnote{Given two strings $x, y \in \Sigma^N$, Ham$(x, y) = \sum_{i=1}^N \mathbf{1}(x_i \neq y_i)$.} 
and token-based distances such as Cosine,
%\footnote{Given two vectors $x = (x_1, \ldots, x_N)$ and $y = (y_1, \ldots, y_N)$ in the $N$-dimensional Euclidean space, where $x_i$ and $y_i$ represent the frequencies of the $i$-th token in $x$ and $y$ respectively, COS$(x, y) = \frac{\langle x, y \rangle}{\norm{x} \norm{y}}$ where ``$\langle \cdot, \cdot \rangle$'' is the inner product operation and ``$\norm{\cdot}$'' stands for the Euclidean norm.}, 
Jaccard,
%\footnote{Given two sets of tokens $s$ and $t$, JAC$(s, t) = \frac{\abs{s \cap t}}{\abs{s \cup t}}$.}, 
Overlap
%\footnote{Given two sets of tokens $s$ and $t$, OLP$(s, t) = \abs{s \cap t}$.} 
and Dice,
%\footnote{Given two sets of tokens $s$ and $t$, DICE$(s, t) = \frac{2\abs{s \cap t}}{\abs{s} + \abs{t}}$.}, 
edit distance retains the information of the orderings of characters, and captures the best alignment of the two strings, which is critical to applications in bioinformatics, natural language processing and information retrieval.  On the other hand, edit distance is computationally more expensive than Hamming and token-based distances: computing edit distance takes at least quadratic time under the SETH conjecture~\cite{BI15}, while Hamming, Cosine, Jaccard, Overlap and Dice can be computed in linear time.  

Due to its difficulty and usefulness, a large portion of the similarity join literature has been devoted to edit distance~\cite{GJKMS01,AGK06,BMS07,BHSH07,LLL08,XWL08,WLF10,QWL11,WQX13,LDW11,WLF12}.  However, we have observed that all the existing approaches fall short on long strings and relatively large thresholds.  In the recent string similarity search/join competition, it was reported that ``an error rate of $20\% \sim 25\%$ pushes today's techniques to the limit''~\cite{WDG14}.  By  $20\%$ errors we mean that the distance threshold is set to be $20\%$ of the string length.  In fact the limit is reached much earlier on strings that are longer than those tested in the competition.  
%Our experiments have shown that the previous best algorithms cannot finish in 10 hours on a collection of 20,000 DNA sequences each of length 20,000 and 1\% error rate in our computing environments.\footnote{Our experiments were conducted on a Dell PowerEdge T630 sever with 2 Intel Xeon E5-2667 v4 3.2GHz CPUs and 256GB memory.}
%\qinsays{need to change the statistics?}

However, long strings and large thresholds are critical to many applications. For example, documents can contain hundreds of thousands of characters; the lengths of DNA sequences range from thousands to billions of bases. If we set a threshold that is too small, then we may end up getting zero output pair which is certainly not interesting. %-- it is folklore in bioinformatics that the edit distances between human DNAs are mostly in the range of $1\% \sim 10\%$ of the string lengths, and those between different spices are even higher.

\paragraph{Our Contribution}
The main contribution of this paper is a novel approach of computing edit similarity joins that scales very well with the string length and the distance threshold.  Different from all previous approaches which directly perform computations on the edit distance, we first embed the input strings from the edit space to the Hamming space, and then perform a filtering in the Hamming space using locality sensitive hashing.  

Our main algorithm, named \ebdjoin+, is randomized and may introduce a small number of errors (95\% - 99\% recall, 100\% precision in all of our experiments), but it significantly outperforms all the previous algorithms in both running time and memory usage on long strings and large thresholds.  In particular,  \ebdjoin+ scales very well up to error rate 20\% on large datasets which is beyond the reach of existing algorithms.
%For instance, on the dataset mentioned above (a collection of 20,000 strings each of length 20,000 and the error threshold 1\%), \ebdjoin\ finished in less than 6 minutes with precision of 100\% and recall of 99.2\%. Moreover, \ebdjoin\ scales very well up to error threshold 20\% which is far beyond the reach of existing algorithms.
%\qinsays{need to change the statistics?}



\paragraph{Overview of Our Approach}  
Given two strings $x, y \in \Sigma^N$, the Hamming distance between $x$ and $y$ is defined to be $\text{Ham}(x, y) = \sum_{i=1}^N \mathbf{1}(x_i \neq y_i)$.
Our approach is built on the recent advance of metric embeddings for edit distance, and is very different from all of the previous approaches.  In \cite{CGK16}, it has been shown that there exists an embedding function $f : \Sigma^N \to \Sigma^{3N}$ such that given $x, y \in \Sigma^N$,  we have with probability $1 - o(1)$ that~\footnote{The analysis in \cite{CGK16} in fact only gives ${\text{ED}(x, y)}/2 \le  \text{Ham}(f(x), f(y))$. However, as we shall describe in Algorithm~\ref{alg:CGK}, if we pad the embedded strings using a character that is not in the dictionary, then it is easy to show that $\text{ED}(x, y) \le \text{Ham}(f(x), f(y))$ with probability $1 - o(1)$.}
\begin{eqnarray*}
\label{eq:a-1}
{\text{ED}(x, y)} &\le&  \text{Ham}(f(x), f(y)),
\end{eqnarray*}
and with probability at least $0.999$ that
\begin{eqnarray*}
\label{eq:a-2}
\text{Ham}(f(x), f(y)) &\le& O\left((\text{ED}(x, y))^2\right).
\end{eqnarray*}
We call this scheme the {\em CGK-embedding}, named after the initials of the authors in \cite{CGK16}.  The details of the embedding algorithm will be illustrated in Section~\ref{sec:CGK}.  We call 
$$D(x, y) = \text{Ham}(f(x), f(y)) / \text{ED}(x, y)$$
the {\em distortion} of the CGK-embedding on input $(x, y)$.  Note that 
if $\text{ED}(x, y) \le K$, then $1 \le D(x, y) \le O(K)$ with probability at least $0.99$.  

%In fact, in our experiments the distortion never went below $1$ (i.e., we always have $\text{Ham}(f(x), f(y)) \ge \text{ED}(x, y)$), and thus we assume that $1 \le D(x, y) \le O(K)$ in the rest of the paper for all inputs $(x, y)$ with $\text{ED}(x, y) \le K$.

The high level idea of our approach is fairly simple: we first embed using CGK all the strings from the edit space to the Hamming space, and then perform a filtering step on the resulting vectors in the Hamming space using locality sensitive hashing (LSH) \cite{IM98,GIM99}.  LSH has the property that it will map a pair of items of small Hamming distance to the same bucket in the hash table with good probability, and map a pair of items of large Hamming distance to different buckets with good probability.  The final step is to verify for each hash bucket $B$, and for all the strings hashed into $B$, whether their pairwise edit distances are at most $K$ or not, by an exact dynamic programming based edit distance computation.  This finishes the high level description of our basic algorithm which we name \ebdjoin. \ebdjoin\ works very well on datasets where there is a non-trivial gap between distances of similar pairs and dissimilar pairs, but does not give satisfactory accuracy on datasets where the gap is very small (e.g., random reads of DNA sequences).  We thus further improve \ebdjoin\  by adding a couple of new ideas to deal with string shifts, and obtain \ebdjoin+ which works well on all the datasets that we have tested.
  
One may observe that the worst-case distortion of the CGK-embedding can be fairly large if the threshold $K$ is large. However, we have observed that the practical performance of CGK-embedding is much better. We will give more discussions on this phenomenon in Section~\ref{sec:CGK}.   To further reduce the distortion, we choose to run the embedding multiple times, and then for each pair of strings we choose the run with the minimum Hamming distance for the filtering.  This minimization step does not have to be performed explicitly since we do not have to compute Ham$(f(x), f(y))$ for all pairs of strings which is time consuming. We instead integrate this step with LSH for a fast filtering.  

Finally, we note that since LSH is a dimension reduction step, LSH-based filtering naturally fits long strings  (e.g., DNA sequences) which are our main interest.  For short strings LSH-based filtering may not be the most effective approach and one may want to use different filtering methods.  We also note Satuluri et al.~\cite{SP12} used LSH-based filtering for computing similarity joins under the Jaccard distance and the Cosine distance.   Unfortunately there is no efficient LSH for edit distance, which is the motivation for us to first embed the strings to vectors in the Hamming space and then perform LSH.  
%On the other hand, in Section~\ref{sec:setup} we show experimentally that the idea of first converting strings to $q$-gram vectors and then performing filtering will give low accuracy (Figure~\ref{fig:set}). 

%Finally, we comment that our approach can be easily extended to general distance functions for sequence alignment where the cost of matching a pair of characters $u, v \in \Sigma$ is determined by a cost matrix, which can be more useful for applications in bioinformatics.  To this end, suppose that each value in the cost matrix and the size of $\Sigma$ are constants, we can encode each $u \in \Sigma$ to a binary vector $b(u)$ of constant length, such that $\text{cost}(u, v) = \text{Ham}(b(u), b(v))$; this can be done easily by solving a set of quadratic equations.  We also need to replace the dynamic programming for edit distance with that for sequence alignment in the verification step. 

%One shall see from our experimental results that our approach outperforms previous algorithms by orders of magnitudes in time, while only introduces at most 1\% false negatives for genome datasets, and at most 5\% false negatives in other datasets with smaller string lengths.

%Easy analysis shows that the filtering step our algorithm runs in linear time which is {\em independent} on the threshold $K$.  From the experimental results we also notice that our filtering step is much more efficient than all the previous algorithms on long strings.

A preliminary version of this article appeared in \cite{ZZ17}, where only the basic version of \ebdjoin+, namely \ebdjoin, was proposed.   
%We noticed that \ebdjoin\ does not perform very well on datasets where similar strings have non-trivial shifts (such as random reads of DNA sequences), which is the motivation for us to propose \ebdjoin+.  
Compared with \cite{ZZ17}, Section~\ref{sec:exact-ED} is newly added where have changed the algorithm for computing exact edit distance in the verification phase of \ebdjoin\ and \ebdjoin+. Part of Section~\ref{sec:speedup} has been rewritten. 
Section~\ref{sec:improved} for \ebdjoin+ is entirely new.  All the experiments in Section~\ref{sec:exp} have been redone, in particular, for the new algorithm \ebdjoin+.  


\paragraph{Roadmap} The rest of this paper is organized as follows.  In Section~\ref{sec:related} we survey related work on edit similarity joins.  In Section~\ref{sec:tool} we describe a set of tools that we make use of in our algorithms.
In Section~\ref{sec:algo} we describe \ebdjoin\ which is a basic version of \ebdjoin+, and then in Section~\ref{sec:improved} we show our main algorithm \ebdjoin+.  We present experimental studies in Section~\ref{sec:exp}, and conclude the paper in Section~\ref{sec:conclude}.   




 


