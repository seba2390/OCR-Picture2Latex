\section{related work} \label{sec:related}

  As a classical problem in the area of data quality, entity resolution has been extensively studied in the literature~\cite{christen2012data, elmagarmid2007duplicate, christophides2015entity}. The proposed techniques include those based on rules~\cite{fan2009reasoning, li2015rule, singh2017generating}, probabilistic theory~\cite{fellegi1969theory, singla2006entity} and machine learning~\cite{sarawagi2002interactive, kouki2017collective, arasu2010active, bellare2012active}. However, these traditional techniques lack effective mechanisms for quality control; ergo, they fail in ensuring high-quality guarantees.

  Active learning-based approaches~\cite{arasu2010active, bellare2012active} have been proposed in order to satisfy the precision requirement for ER. The authors of \cite{arasu2010active} proposed a technique that can optimize the recall while ensuring a pre-specified precision goal. The authors in~\cite{bellare2012active} proposed an improved algorithm that approximately maximizes the recall under a precision constraint. Considering that these techniques share the same classification paradigm with traditional machine learning-based ones; the former cannot enforce comprehensive quality guarantees specified by both precision and recall metrics as HUMO does.

   The progressive paradigm for ER~\cite{whang2013pay, altowim2014progressive} has also been proposed for the application scenarios in which ER should be processed efficiently, but it does not necessarily guarantee high-quality results. Taking a pay-as-you-go approach, it studied how to maximize the result's quality given a pre-specified resolution budget, which was defined based on the machine computation cost. A similar iterative algorithm, SiGMa, was proposed in \cite{lacoste2013sigma}. It can leverage both the structure information and string similarity measures to resolve entity alignment across different knowledge bases. Note that built on machine computation, these techniques could not be applied to enforce quality guarantees either.

   It has been well recognized that pure machine algorithms may not be able to produce satisfactory results in many practical scenarios~\cite{li2016crowdsourced}. Many researchers~\cite{wang2012crowder, whang2013question, vesdapunt2014crowdsourcing, gokhale2014corleone, mozafari2014scaling, wang2015crowd, chai2016cost, verroios2017waldo} have studied how to crowdsource an ER workload. For instance, recently, the authors of~\cite{chai2016cost} proposed a cost-effective framework that employs the partial order relationship on instance pairs to reduce the number of asked pairs. Similarly, the authors in~\cite{verroios2017waldo} provided solutions to take advantage of both pairwise and multi-item interfaces in a crowdsourcing setting. While, these works addressed the challenges specific to crowdsourcing; we instead investigate a different problem: how to divide a workload between the human and the machine such that the user-specified quality guarantees can be met. In this paper, we assume that human workload can be performed with high quality; yet we do not investigate the problems targeted by existing interactive and crowdsourcing solutions. Note that the workload assigned to the human by HUMO can be naturally processed in a crowdsourcing manner. Our work can thus be considered orthogonal to existing works on crowdsourcing. It is interesting to investigate how to seamlessly integrate a crowdsourcing platform into HUMO in future work.

