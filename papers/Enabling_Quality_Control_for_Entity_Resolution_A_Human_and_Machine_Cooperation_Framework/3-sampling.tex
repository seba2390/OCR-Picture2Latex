\section{Sampling-based Approach} \label{sec:aggressive}

  The baseline approach estimates the match proportions of $D_-$ and $D_+$ by the observed match proportions of the intervals in $D_H$. However, it can be noticeable that the match proportion of $D_-$ is usually significantly smaller than that of $D_H$, while the match proportion of $D_+$ is usually considerably larger than that of $D_H$. Strictly speaking, the baseline approach may overestimate the match proportion of $D_-$, and may also underestimate the match proportion of $D_+$. As a result, it would require considerably more than necessary human cost to enforce quality guarantees. To alleviate this limitation, we propose a more aggressive sampling-based approach in this section. Compared with the baseline approach, it is more aggressive in that it estimates the match proportions of $D_-$ and $D_+$ by directly sampling them.

  The sampling-based approach divides $D$ into multiple disjoint unit subsets and estimates their match proportions by sampling. We first present an all-sampling solution that samples all the subsets. To reduce human cost, we also present an improved partial-sampling solution that only requires sampling a portion of the subsets.

\begin{figure}[!htb]
\setlength{\abovecaptionskip}{\figcaptionspace}
\centering
\includegraphics[width=\linewidth]{figures/sampling.pdf}
\caption{The demonstration of sampling-based solution.}
\label{sampling_based_demonstration}
\end{figure}


\subsection{All-Sampling Solution} \label{sec:all-sampling}

   Suppose that $D$ is divided into $m$ disjoint subsets, $D=D_1\cup\cdots\cup D_m$, and the subsets are ordered by their similarity values. If $i<j$, then $\forall d\in D_i$ and $\forall d'\in D_j$, we have $sim(d)\leq sim(d')$, in which $sim(d)$ denotes the similarity value of $d$. With the notation of $D_i$, we can represent $D_H$ by a union of subsets, $D_H=D_i\cup D_{i+1}\cdots\cup D_j$, in which $D_i$ is the lower bound subset of $D_H$ while $D_j$ is its upper bound subset. We also denote the sampled match proportion of $D_i$ by $\mathsf{R}_i$. We first consider the hypothetical case that the estimate of $\mathsf{R}_i$ is accurate, and then integrate sampling errors into bound computation.

  When the estimate of $\mathsf{R}_i$ is hypothetically accurate, the achieved recall level of a HUMO solution solely depends on $D_H$'s lower bound. Therefore, the all-sampling solution first identifies $D_H$'s lower bound subset to meet the constraint on recall, then identifies its upper bound subset to meet the precision constraint. With the lower bound of $D_H$ set at $D_i$, the achieved recall level can be estimated by
\begin{equation}
\label{eq:all-recall}
  recall(D,S)=\frac{\sum_{i\leq k\leq m}{|D_k|\cdot \mathsf{R}_k}}{\sum_{1\leq k\leq m}{|D_k|\cdot \mathsf{R}_k}}.
\end{equation}
Therefore, to minimize the size of $D_H$ while ensuring the recall level $\beta$, the search process initially sets the lower bound to $D_1$, and then iteratively moves it right from $D_k$ to $D_{k+1}$ until the estimated recall level specified in Eq.~\ref{eq:all-recall} falls below $\beta$.

  The search process then deals with the precision constraint in a similar way by incrementally identifying the upper bound of $D_H$. Suppose that the lower bound of $D_H$ has been identified to be $D_i$. With its upper bound set at $D_j$, the achieved precision level can be estimated by
\begin{equation}
\label{eq:all-precision}
  precision(D,S)=\frac{\sum_{i\leq k\leq m}{|D_k|\cdot \mathsf{R}_k}}{\sum_{i\leq k\leq j}{|D_k|\cdot \mathsf{R}_k}+\sum_{j+1\leq k\leq m}{|D_k|}}
\end{equation}
Therefore, to minimize the size of $D_H$ while ensuring the precision level $\alpha$, the search process initially sets the upper bound to $D_m$, and then iteratively moves it left from $D_{k}$ to $D_{k-1}$ until the estimated precision level specified in Eq.~\ref{eq:all-precision} falls below $\alpha$.



  Now we describe how to integrate sampling errors into bound computation. For fulfilling the confidence level, we resort to the theory of stratified random sampling \cite{cochran1977sampling} to estimate sampling error margins. We denote the total number of pairs in $D$ by $n$ and the number of pairs in the subset $D_i$ by $n_i$. Based on the sampled match proportion estimate of $D_i$, we can compute the mean of the match proportion of $D$ and its estimated standard deviation, which are denoted by ${\bar{\mathsf{R}}_D}$ and $\sigma_D$ respectively. The details on how to compute ${\bar{\mathsf{R}}_D}$ and $\sigma_D$ can be found in~\cite{chen2017humoreport}. Given the confidence level $\theta$, the total number of matching pairs in $D$ falls reasonably within the interval
\begin{equation}
  [n\cdot(\bar{\mathsf{R}}_D-t_{(1-\theta, d.f.)}\cdot\sigma_{D}), n\cdot(\bar{\mathsf{R}}_D+t_{(1-\theta, d.f.)}\cdot\sigma_{D})],
\label{eq:confidenceintervals}
\end{equation}
in which $t_{(1-\theta, d.f.)}$ is {\em Student's t value} for {\em d.f.} degrees of freedom and the confidence level $\theta$ for two-sided critical regions. In Eq.~\ref{eq:confidenceintervals}, as typical in stratified sampling \cite{cochran1977sampling}, we use Student's t value to take account of the sampling error due to limited sample size. Suppose that a random variable $T$ has a Student's t-distribution, a Student's t value for confidence level $\theta$ for two-sided critical regions is the value, let's say $\tilde{t}$, that satisfies $P(-\tilde{t} < T < \tilde{t})=\theta$, where $P(\cdot)$ represents the probability.

  Next, we apply the analysis results of confidence error margins in the recall and precision estimates as presented in Eq.~\ref{eq:all-recall} and ~\ref{eq:all-precision}. According to Eq.~\ref{eq:all-recall}, the lower bound of the recall estimate can be guaranteed by setting a lower bound on $n_{[i,m]}^{+}$ and an upper bound on $n_{[1,i-1]}^{+}$, in which $n_{[i,j]}^{+}$ denotes the total number of matching pairs in the subset union, $D_i\cup D_{i+1}\cdots\cup D_j$. Suppose that the lower bound of $D_H$ is set at $D_i$. Given the confidence level $\theta$ and the recall level $\beta$, the HUMO solution meets the recall requirement if
\begin{equation}
\label{eq:recall-condition}
  \beta\leq \frac{lb(n_{[i,m]}^{+}, \sqrt{\theta})}{ub(n_{[1,i-1]}^{+}, \sqrt{\theta}) + lb(n_{[i,m]}^{+}, \sqrt{\theta})},
\end{equation}
in which $lb(n_{[i,m]}^{+}, \sqrt{\theta})$ denotes the lower bound of $n_{[i,m]}^{+}$ with the confidence $\sqrt{\theta}$, and $ub(n_{[1,i-1]}^{+}, \sqrt{\theta})$ denotes the upper bound of $n_{[1,i-1]}^{+}$ with the confidence $\sqrt{\theta}$. Since the bound estimations on $n_{[i,m]}^{+}$ and $n_{[1,i-1]}^{+}$ are independent, the lower bound of the recall level specified in Eq.~\ref{eq:recall-condition} has the desired confidence $\theta$.

  Similarly, suppose that the lower and upper bounds of $D_H$ are set at $D_i$ and $D_j$ respectively. Given the confidence level $\theta$ and the precision level $\alpha$, the HUMO solution meets the precision requirement if
\begin{equation}
\label{eq:precision-condition}
  \alpha\leq \frac{lb(n_{[i,j]}^{+}, \sqrt{\theta}) + lb(n_{[j+1,m]}^{+}, \sqrt{\theta})}{lb(n_{[i,j]}^{+}, \sqrt{\theta}) + n_{[j+1,m]}}.
\end{equation}
Since the bound estimations on $n_{[i,j]}^{+}$ and $n_{[j+1,m]}^{+}$ are independent, the lower bound of the precision level specified in Eq.~\ref{eq:precision-condition} has the desired confidence $\theta$.

  The all-sampling search process iteratively searches for the lower and upper bounds of $D_H$. It first identifies the maximal value of $i$ such that the condition specified in Eq.~\ref{eq:recall-condition} is satisfied. It begins with $i=1$ and then iteratively moves the lower bound right from $D_i$ to $D_{i+1}$. Similarly, with the lower bound of $D_H$ set at $D_i$, it identifies the minimal value of $j$ such that the condition specified in Eq.~\ref{eq:precision-condition} is satisfied. It begins with $j=m$ and then iteratively moves the upper bound left from $D_j$ to $D_{j-1}$. More details on the search process are however omitted here due to space constraints. They can be found in our technical report \cite{chen2017humoreport}.

  The worst-case computational complexity of the all-sampling search process can be represented by ${\bf O}(n+m^2)$, in which $n$ denotes the total number of pairs in $D$ and $m$ denotes the total number of subsets. Finally, we conclude this subsection with the following theorem, whose proof follows naturally from our above analysis:
\begin{theorem}
  Given an ER workload of $D$, a confidence level $\theta$, a precision level $\alpha$ and a recall level $\beta$, the all-sampling search process returns a HUMO solution that can ensure the precision and recall levels of $\alpha$ and $\beta$ respectively with the confidence $\theta$.
\end{theorem}


\subsection{Partial-Sampling Solution} \label{sec:partial-sampling}

 Note that samples should be labeled by the human and the optimization objective of HUMO is to minimize human cost. The all-sampling solution has to sample every subset; therefore its human cost consumed on labeling samples is usually prohibitive. In this subsection, we propose an improved solution that only needs to sample a portion of the subsets. It achieves the purpose by approximating the match proportions of unsampled subsets based on those observed on sampled ones. We use the Gaussian process (GP) \cite{rasmussen2006gaussian}, which is a classical technique for non-parametric regression. GP assumes that the match proportions of subsets have a joint Gaussian distribution. It can smoothly integrate sampling error margins into the approximation process.

   Given $k$ sampled subsets, we denote their observed match proportions by $\mathsf{R} = [\mathsf{R}_1, \mathsf{R}_2, \ldots, \mathsf{R}_k]^T$, and their corresponding average similarity values by $V = [v_1, v_2, \ldots, v_k]^T$. The Gaussian process estimates the match proportion, $\mathsf{R}_*$, of a new similarity value, $v_*$, based on $\mathsf{R}$, the observed match proportions of $V$. According to the assumption of GP, the random variables $[V^T, v_*]^T$ satisfy a joint Gaussian distribution, which can be represented by
  \begin{equation}
  \begin{bmatrix} V \\ v_* \end{bmatrix} \sim
  \mathcal{N}\left(0, \begin{bmatrix} \mathbf{K}(V, V) & \mathbf{K}(V, v_*) \\ \mathbf{K}(v_*, V) & \mathbf{K}(v_*, v_*) \end{bmatrix}\right),
  \label{eq:jointdistribution}
  \end{equation}
in which $\mathbf{K}(\cdot , \cdot)$ represents the covariance matrix. The details of how to compute the covariance matrix $\mathbf{K}(\cdot , \cdot)$ can be found in~\cite{chen2017humoreport}. Based on Eq.~\ref{eq:jointdistribution}, the mean of the match proportion of $v_*$, $\mathsf{R}_*$, can be represented by
  \begin{equation}
  \bar{\mathsf{R}}_* = \mathbf{K}(v_*, V) \cdot \mathbf{K}^{-1}(V, V) \cdot \mathsf{R}.
  \label{eq:gpr:mean}
  \end{equation}
The variance of $\mathsf{R}_*$ can be also represented by
\begin{equation}
  \sigma_{\mathsf{R}_*}^2 = \mathbf{K}(v_*, v_*) - \mathbf{K}(v_*, V) \cdot \mathbf{K}^{-1}(V, V) \cdot \mathbf{K}(V, v_*).
\label{eq:gpr:variance}
\end{equation}
Accordingly, the distribution of $\mathsf{R}_*$, the match proportion of $v_*$, can be represented by the following Gaussian function
\begin{equation}
  \mathsf{R}_* \sim \mathcal{N}\left(\bar{\mathsf{R}}_*, \sigma_{\mathsf{R}_*}^2\right).
\end{equation}

  Now we are ready to describe how to aggregate the estimations of multiple subsets. Note that the distribution of each subset's match proportion satisfies a Gaussian function. Given the $t$ subsets of $D_*$, $D_*$ = $\{D_*^1,D_*^2,\ldots,D_*^t\}$, we denote their corresponding numbers of pairs by $\{n_*^1, n_*^2,\ldots, n_*^t\}$, and their similarity values by $V_*=[v_*^1, v_*^2, \ldots, v_*^t]^T$. Then, the total number of match pairs in $D_*$, denoted by $n_*$, satisfies a Gaussian distribution. Its mean can be represented by
\begin{equation}
  \bar{n}_* = \sum_{i=1}^{t}n_*^i\cdot\bar{\mathsf{R}}_*^i,
\end{equation}
in which $\bar{\mathsf{R}}_*^i$ represents the mean of the match proportion of $D_*^i$. Its standard deviation can also be represented by
\begin{equation}
  \sigma_{D_*} = \sqrt{\sum_{1\leq i\leq t,1\leq j\leq t}n_*^i\cdot n_*^j\cdot cov(v_*^i, v_*^j)},
\end{equation}
in which $cov(v_*^i, v_*^j)$ is the covariance between two estimates and its value is the {\em (i,j)-th} element in the covariance matrix $\mathbf{K}(V_*,V_*)-\mathbf{K}(V_*,V)\cdot\mathbf{K^{-1}}(V,V)\cdot\mathbf{K}(V,V_*)$. Therefore, given the confidence level $\theta$, the corresponding confidence interval of the number of match pairs in $D_*$ can be represented by
\begin{equation}
  [\bar{n}_* - \mathcal{Z}_{(1-\theta)} \cdot \sigma_{D_*}, \bar{n}_* + \mathcal{Z}_{(1-\theta)} \cdot \sigma_{D_*}],
\label{eq:gpr:confidenceintervals}
\end{equation}
in which $\mathcal{Z}_{(1-\theta)}$ is the $(1-\frac{1-\theta}{2})$ point of {\em standard normal distribution}.


\begin{algorithm}
\setlength{\textfloatsep}{0pt}
\caption{Gaussian Regression of Match Proportion Function}
\label{alg:fit-gp}
\KwIn{Sorted disjoint subsets $\{D_1, D_2, ..., D_m\}$; Sampling cost range $[p^l, p^u]$; Error threshold $\varepsilon$.}
\KwOut{The function of match proportion, $F_k$.}
$j \gets m\cdot p^l$\;
$TrainSet \gets$ select j equidistance subsets $\{D_{i_1},$ $D_{i_2},$ $...,$ $D_{i_j}\}$\;
$\mathsf{V}, \mathsf{R} \gets$ sample every subset in $TrainSet$ to get their match proportion estimates\;
$F_k \gets$ use $\mathsf{V}, \mathsf{R}$ to train Gaussian process model\;
$IndexQueue \gets [(i_1, i_2), ..., (i_k, i_{k+1}), ..., (i_{j-1}, i_j)]$\;
\While{$IndexQueue$ is not empty \\ \qquad and $|TrainSet| < m\cdot p^u$}
{
    $(i_k, i_{k+1}) \gets IndexQueue.pop()$\;
    $D_x \gets$ the middle subset between $D_{i_k}$ and $D_{i_{k+1}}$\;
    $\mathsf{R}_x \gets$ match proportion of $D_x$ estimated by sampling\;
    \If{$|F_k(v_x) - \mathsf{R}_x| \geq \varepsilon$}
    {
        $IndexQueue.append([(i_k, x),(x, i_{k+1})])$\;
    }
    Add $D_x, v_x, \mathsf{R}_x$ to $TrainSet, \mathsf{V}, \mathsf{R}$ respectively\;
    $F_k \gets$ use $\mathsf{V}, \mathsf{R}$ to train Gaussian process model\;
}
return $F_k$.
\end{algorithm}

  The partial-sampling search process consists of two phases. It trains the function of match proportion by Gaussian regression in the first phase, it then searches for the lower and upper bounds of $D_H$ based on the trained function in the second phase. The function training's procedure is sketched in Algorithm\ref{alg:fit-gp}. Note that $D$ is divided into $m$ disjoint subsets \{$D_1$, $D_2$, $\ldots$, $D_m$\}. To balance approximation accuracy and sampling cost, it presets a range, $[p^l, p^u]$ (e.g. $[1\%, 5\%]$), for the proportion of sampled subsets among all subsets. Initially, the training set consists of $j$ sampled subsets, \{$D_{i_1}$, $D_{i_2}$, $\ldots$, $D_{i_j}$\}, in which $j=m\times p^l$ and $\forall 1\leq k\leq j-2$, $i_{k+1}-i_k=i_{k+2}-i_{k+1}$. In each iteration, the algorithm first trains an approximation function, denoted by $F_k$, by Gaussian regression based on the sampled subsets. It then uses $F_k$ to estimate the match proportion of a subset that is located in the middle point between two neighbouring sampled subsets. Suppose that $D_x$ denotes the subset between the sampled subsets $D_{i_k}$ and $D_{i_{k+1}}$. If the difference between the estimated value based on $F_k$ and the observed match proportion based on sampling exceeds a small threshold $\epsilon$, the algorithm would add $D_x$ into the training set; otherwise, it would not sample any other subset between $D_{i_k}$ and $D_{i_{k+1}}$ (except $D_x$) in the following iterations. Finally, the algorithm trains the function with the updated training set. This cycle of sampling and training is iteratively invoked until the trained function achieves a good approximation or the sampling cost reaches the upper bound of the pre-specified range (i.e. $p^u$).

  Similar to the procedure for all-sampling solution, the partial-sampling search process first identifies the maximal lower bound of $D_H$ to meet the recall requirement, and then identifies the minimal upper bound of $D_H$ to meet the precision requirement. The only difference is that the lower bounds of the achieved recall and precision levels of a HUMO solution should be estimated by the confidence intervals specified in Eq.~\ref{eq:gpr:confidenceintervals}.

  The worst-case computational complexity of Alg.~\ref{alg:fit-gp} is in the order of ${\bf O}(k^4)$, in which $k$ denotes the number of sampled unit subsets. The worst-case computational complexity of the search process can be represented by ${\bf O}(m\cdot k^2+m^3)$. Therefore, the worst-case computational complexity of the partial-sampling solution can be represented by ${\bf O}(n+m^3+m\cdot k^2+k^4)$. It can be observed that the effectiveness of the partial-sampling solution in ensuring quality guarantees depends on the accuracy of the Gaussian approximation. As shown by our empirical evaluation in Section~\ref{sec:experiment}, the partial-sampling solution is highly effective due to the powerfulness and robustness of the Gaussian process.
