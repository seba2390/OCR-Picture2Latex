\section{Problem Setting} \label{sec:setting}

  Entity resolution's main purpose is to determine whether two records are equivalent. Two records are deemed equivalent if and only if they correspond to the same real-world entity. We denote an ER workload by $D$, $D=\{d_1, d_2, \cdots, d_n\}$, in which $d_i$ represents an instance pair.  An ER solution corresponds to a label assignment $L$ for $D$, $L=\{l_1, l_2, \cdots, l_n\}$, in which $l_i=1$ if $d_i$ is labeled as {\em match} and $l_i=0$ if it is labeled as {\em unmatch}. In this paper, $d_i$ is called a matching pair if its two records are equivalent; otherwise, it is called an unmatching pair.

  As usual, we measure the quality of an ER solution by the metrics of precision and recall. Precision is the fraction of matching pairs among the pairs labeled as {\em match}, while recall is the fraction of correctly labeled matching pairs among all the matching pairs. Formally, we denote the ground-truth labeling solution for $D$ by $\hat{L}$, $\hat{L} = \{\hat{l}_1, \hat{l}_2, \cdots, \hat{l}_n\}$, in which $\hat{l}_i=1$ if $d_i$ is a matching pair and $\hat{l}_i=0$ otherwise. Given a labeling solution $L$, we use $D_{tp}$ to denote its set of true positive pairs, $D_{tp}=\{d_i|\hat{l}_i=1 \wedge l_i=1 \}$, $D_{fp}$ to denote its set of false positive pairs, $D_{fp}=\{d_i|\hat{l}_i=0 \wedge l_i=1 \}$, and $D_{fn}$ to denote its set of false negative pairs, $D_{fn}=\{d_i|\hat{l}_i=1 \wedge l_i=0\}$. Accordingly, the achieved precision level of $L$ can be represented by
\begin{equation} \label{eq:precision}
    \emph{precision(D,L)} = \frac{|D_{tp}|}{|D_{tp}| + |D_{fp}|}.
\end{equation}
Similarly, the achieved recall level of $L$ can be represented by
\begin{equation} \label{eq:recall}
    \emph{recall(D,L)} = \frac{|D_{tp}|}{|D_{tp}| + |D_{fn}|}.
\end{equation}

   Formally, the problem of entity resolution with quality guarantees specified at both precision and recall fronts is defined as follows:
\begin{definition}
\label{problemsetting}
{\bf [Entity Resolution with Quality Guarantees]}  Given a set of instance pairs, $D=\{d_1, d_2, \cdots, d_n\}$, the problem of entity resolution with quality guarantees is to give a labeling solution $L$ for $D$ provided with a confidence level $\theta$, $precision(D,L)\geq\alpha$ and $recall(D,L)\geq\beta$, in which $\alpha$ and $\beta$ denote the user-specified precision and recall values respectively.
\end{definition}


\section{HUMO Framework} \label{sec:framework}

  In this section, we first give an overview on HUMO, then introduce its optimization problem.


\subsection{Framework Overview}

    The primary idea behind HUMO is to enforce quality guarantees by dividing an ER workload between the human and the machine. It assigns easy instances, which can be automatically labeled with high accuracy, to the machine, while leaving more challenging instances for human-operated manual verification.

		
		Suppose that each instance pair in $D$ can be evaluated by a machine metric. This metric can be pair similarity or other classification metrics (e.g. match probability \cite{fellegi1969theory} and Support Vector Machine distance \cite{kopcke2010evaluation}). Note that entity resolution by classification usually categorizes pairs into $match$ and $unmatch$ based on a selected metric. Given a machine metric, HUMO assumes that $D$ statistically satisfies monotonicity of precision. Given a set of instance pairs, its precision refers to the proportion of matching pairs among all pairs. Intuitively, the monotonicity assumption of precision states that the higher (resp. lower) metric values a set of pairs have, the more probably they are matching pairs (resp. unmaching pairs). It can be observed that given a machine metric, the monotonicity assumption of precision underlies its effectiveness as a classification metric. {\em For simplicity of presentation, we use pair similarity as a machine metric example in this paper. However, HUMO is similarly effective with other machine metrics}. For instance, with the metric of SVM, each pair can be measured by its distance to a classification plane; with the metric of match probability, each pair can be measured by its estimated probability.
			
		
 Formally, we define the monotonicity assumption of precision, which was first proposed in \cite{arasu2010active}, as follows:
\begin{assumption}[Monotonicity of Precision]
  A value interval $I_i$ is dominated by another interval $I_j$, denoted by $I_i\preceq I_j$, if every value in $I_i$ is less than every value in $I_j$. We say that precision is monotonic with respect to a pair metric if for any two value intervals $I_i\preceq I_j$ in [0,1], we have $\mathsf{R}(I_i)\leq\mathsf{R}(I_j)$, in which $\mathsf{R}(I_i)$ denotes the precision of the set of instance pairs whose metric values are located in $I_i$.
\label{monotonicity}
\end{assumption}


\begin{figure}[h]
\setlength{\abovecaptionskip}{\figcaptionspace}
\centering
\includegraphics[width=\linewidth]{figures/framework.pdf}
\caption{The HUMO framework.}
\label{fig_basic_idea}
\end{figure}


  With the metric of pair similarity, the underlying intuition of Assumption \ref{monotonicity} is that the more similar two records are, the more likely they refer to the same real-world entity. According to the monotonicity assumption, a pair with high similarity has a correspondingly high probability of being a matching pair. A pair with low similarity instead has a correspondingly low probability of being a matching pair. These two groups of instance pairs can be supposed to be easy in that they can be automatically labeled by the machine with high accuracy. In comparison, the instance pairs having medium similarities are more challenging because labeling them either way by machine would introduce more considerable errors.


  The HUMO framework is shown in Figure~\ref{fig_basic_idea}. It divides the similarity interval [0,1] into three disjoint intervals, $I_-$, $I_H$ and $I_+$, in which $I_-$=[0,$v^-$), $I_H$=[$v^-$,$v^+$] and $I_+$=($v^+$,1], and correspondingly $D$ into three disjoint subsets, $D_-$, $D_H$ and $D_+$. It automatically labels the pairs in $D_-$ as {\em unmatch}, the pairs in $D_+$ as {\em match}, and assigns the pairs in $D_H$ to the human for manual verification. It can be observed that HUMO can flexibly enforce quality guarantees by adjusting the range of $D_H$. In the extreme case of $D_H=\emptyset$, HUMO boils down to a straightforward machine-based classification technique. With the assumption that the human performs better than the machine on a quality basis, enlarging the range of $D_H$ would result in improved quality. In the opposite extreme case of $D_H=D$, HUMO achieves the best performance, which is the same as the human's.

  Generally, given a HUMO solution $S$ consisting of $D_-$, $D_H$ and $D_+$, the lower bound of its achieved precision level can be represented by
\begin{equation}
   precision_l(S)=\frac{N^+_l(D_+)+N^+_l(D_H)}{N(D_+)+N(D_H)},
\label{eq:precision-bound}
\end{equation}
in which $N(\cdot)$ denotes the total number of pairs in a set and $N^+_l(\cdot)$ denotes the lower bound of the total number of matching pairs in a set. Similarly, the lower bound of its achieved recall level can be represented by
\begin{equation}
  recall_l(S)=\frac{N^+_l(D_+)+N^+_l(D_H)}{N^+_l(D_+)+N^+_l(D_H)+N^+_u(D_-)},
\label{eq:recall-bound}
\end{equation}
in which $N^+_u(\cdot)$ denotes the upper bound of the total number of matching pairs in a set. In this paper, for the sake of presentation simplicity, we assume that the pairs in $D_H$ can be manually labeled accurately (100\% accuracy with 100\% confidence). With that being said, we emphasize that HUMO's effectiveness does not depend on said assumption, since it can work properly provided that quality guarantees can be enforced on $D_H$. In the case that human errors are introduced in $D_H$, the lower bounds of the achieved precision and recall can be similarly estimated based on Eq.~\ref{eq:precision-bound} and Eq.~\ref{eq:recall-bound}. Nonetheless, it is worthy to point out that under the assumption that the human yields higher quality matches than the machine, the best quality guarantees HUMO can achieve are no better than human attained ones on $D_H$.


\subsection{Optimization Problem}

  Since human labor is in practice much more expensive than machine computation, HUMO aims to minimize human cost provided that user-specified quality requirements can be satisfied. By quantifying human cost by the number of manually inspected instance pairs in $D_H$, we formally define HUMO's optimization problem as follows:

\begin{definition}
\label{optimization}
{\bf [Minimizing Human Cost in HUMO].} Given a set of instance pairs, $D$, a confidence level $\theta$, a precision level $\alpha$ and a recall level $\beta$, HUMO's optimization problem is represented by
\begin{equation}
\begin{split}
& \quad \underset{S_i}{argmin} (|D_H(S_i)|)\\
& subject \quad to \quad P(precision(D,S_i)\geq\alpha)\geq\theta , \\
& \hspace{0.7in} P(recall(D,S_i)\geq\beta)\geq\theta ,
\end{split}
\label{eq:minimization}
\end{equation}
in which $S_i$ denotes a HUMO solution, $D_H(S_i)$ denotes the set of instance pairs assigned to the human by $S_i$, $precision(D,S_i)$ denotes the achieved precision by $S_i$, $recall(D,S_i)$ denotes the achieved recall by $S_i$, and $P(\cdot)$ denotes the probability of a required quality level being met.
\end{definition}

  Note that in Definition~\ref{optimization}, $P(\cdot)$, or the probability of satisfying a certain required quality level, is statistically equivalent to the confidence level $\theta$ defined in Definition.~\ref{problemsetting}. It can be observed that HUMO achieves a 100\% precision and recall levels in the extreme case when all the instance pairs are assigned to the human (i.e. $D_H$=$D$). In general, its achieved precision and recall levels tend to decrease as $D_H$ becomes smaller. However, the problem of searching for the minimum size $D_H$ is challenging due to the fact that the ground-truth match proportions of $D_-$ and $D_+$ are unknown. In the following sections, we propose three search approaches: a conservative baseline one purely based on the monotonicity assumption of precision (Section~\ref{sec:conservative}), a more aggressive sampling-based one (Section~\ref{sec:aggressive}), and a hybrid one that benefits from the strengths of both previous approaches (Section~\ref{sec:hybrid}). They estimate the match proportions of $D_-$ and $D_+$ based on different assumptions.


\iffalse	
	According to Eq.~\ref{eq:precision} and ~\ref{eq:recall}, $precision(D,S_i)$ can be represented by
\begin{equation}
  \frac{|D_H|\cdot \mathsf{R}(D_H)+|D_+|\cdot \mathsf{R}(D_+)}{|D_H|\cdot \mathsf{R}(D_H)+|D_+|},
\end{equation}
in which $\mathsf{R}(D_*)$ denotes the ground-truth match proportion of the pair instances in $D_*$. Similarly, $recall(D,S_i)$ can be represented by
\begin{equation}
  \frac{|D_H|\cdot \mathsf{R}(D_H)+|D_+|\cdot \mathsf{R}(D_+)}{|D_-|\cdot \mathsf{R}(D_-)+|D_H|\cdot \mathsf{R}(D_H)+|D_+|\cdot \mathsf{R}(D_+)}.
\end{equation}
\fi












