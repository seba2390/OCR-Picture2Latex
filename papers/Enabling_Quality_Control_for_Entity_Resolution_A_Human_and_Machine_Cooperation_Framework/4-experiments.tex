\section{\vspace{-0.1cm}Experimental Evaluation} \label{sec:experiment}

  This section empirically evaluates HUMO's performance by performing a comparative study. We have implemented three proposed optimization approaches for HUMO:
\begin{itemize}
\item Baseline (denoted by BASE). It represents the optimization approach purely based on the monotonicity assumption of precision presented in Section~\ref{sec:conservative};
\item Sampling-based (denoted by SAMP). Since the all-sampling solution performs worse than the partial-sampling one, SAMP represents the partial-sampling solution presented in Section~\ref{sec:partial-sampling}. The comparative evaluation results between the all-sampling and partial-sampling solutions can be found in technical report~\cite{chen2017humoreport}.
\item Hybrid  (denoted by HYBR). It represents the hybrid approach presented in Section~\ref{sec:hybrid}.
\end{itemize}

   		In all HUMO's implementations, we divide an ER workload $D$ into disjoint subsets, each of which contains the same number of instance pairs. The number of instance pairs contained by each subset is set to be 200. Due to the distribution irregularity of matching pairs, BASE estimates the match proportion bounds of $D_-$ and $D_+$ by using the average match proportion of multiple consecutive subsets in $D_H$ instead of a single one. For practical implementation, we suggest that the number of consecutive subsets should be set between 3 and 10. Note that as its value increases, BASE becomes more conservative. To balance sampling cost and accuracy of match proportion approximation, SAMP sets both lower and upper limits on sampling cost, which is measured by the proportion of sampled subsets among all the subsets. In our implementation, the sampling proportion range is set to be between 1\% and 5\%.

 Note that most of the existing techniques for ER cannot enforce quality guarantees. Therefore, we implement a classical technique based on Support Vector Machine (SVM) \cite{kopcke2010evaluation} and present its results for quality reference. More recently proposed techniques (e.g. \cite{kouki2017collective} and \cite{lacoste2013sigma}) may have achieved better performance. However, it should be clear that as the SVM-based technique, they can not enforce quality guarantees. The performance difference between the existing classification techniques is beyond the scope of this paper. We instead compare HUMO with state-of-the-art active learning based approaches \cite{arasu2010active, bellare2012active} (denoted by ACTL), which can at least enforce precision. ACTL maximizes recall level while ensuring a user-specified precision level. It estimates the achieved precision level of a given labeling solution by sampling. It also requires manual verification. We compare HUMO and ACTL on achieved quality and required human cost.

\begin{figure}
\setlength{\abovecaptionskip}{\figcaptionspace}
\centering
\subfigure[DS Dataset.]
{\includegraphics[width=0.48\linewidth]{figures/DS_distribution.pdf}}
\subfigure[AB Dataset.]
{\includegraphics[width=0.48\linewidth]{figures/AB_distribution.pdf}}
\caption{The distributions of matching pairs in two real datasets.}
\label{fig:matching-distribution}
\vspace{-0.8cm}
\end{figure}

  The rest of this section is organized as follows: Subsection~\ref{sec:datasets} describes our experimental setup. Subsection~\ref{sec:HUMO-experiment} evaluates the performance of different optimization approaches for HUMO. Subsection~\ref{sec:other-comparison} compares HUMO with ACTL. Finally, Subsection~\ref{sec:scalability} evaluates its efficiency and scalability.
\vspace{-0.25cm} 
\subsection{\vspace{-0.15cm}Experimental Setup}
\label{sec:datasets}

  We use two real datasets and one synthetic dataset in our evaluation. The experiments on real datasets can demonstrate the proposed solutions' performance in real application scenarios. The experiments on synthetic datasets can instead test their performance sensitivity to different data characteristics. The details of the two real datasets~\cite{kopcke2010evaluation} are described as follows:
\begin{itemize}
\item DBLP-Scholar\footnote{\url{https://dbs.uni-leipzig.de/file/DBLP-Scholar.zip}} (denoted by DS): The DBLP dataset contains 2616 publication entities from DBLP publications and the Scholar dataset contains 64263 publication entities from Google Scholar. The experiments match the DBLP entries with the Scholar entries.
\item Abt-Buy\footnote{\url{https://dbs.uni-leipzig.de/file/Abt-Buy.zip}} (denoted by AB): It contains 1081 product entities from Abt.com and 1092 product entities from Buy.com. The experiments match the Abt entries with the Buy entries.
\end{itemize}


  On both datasets, we compute pair similarity by aggregating attribute similarities with weights \cite{christen2012data}.  Specifically, on the DS dataset, Jaccard similarity of the attributes {\em title} and {\em authors}, and Jaro-Winkler distance of the attribute {\em venue} are used; on the AB dataset, Jaccard similarity of the attributes {\em product name} and {\em product description} are used. The weight of each attribute is determined by the number of its distinct attribute values. As in \cite{arasu2010active}, we use the blocking technique to filter the instance pairs unlikely to match. Specifically, the workload of DS contains the instance pairs whose aggregated similarity values are no less than 0.2. Similarly, the aggregated similarity value threshold for the AB workload is set to be 0.05. After blocking, the DS dataset has 100077 pairs and 5267 among them are matching pairs; the AB dataset has 313040 pairs and 1085 among them are matching pairs.

 The distributions of matching pairs in the two real datasets are presented in Figure~\ref{fig:matching-distribution}, in which the X-axis represents pair similarity value and the Y-axis represents the number of matching pairs. It can be observed that in DS, the majority of matching pairs has high similarity values; in AB, many matching pairs however have median and low similarity values. Therefore, in terms of classification accuracy, AB is a more challenging workload than DS.



   The performance of the SVM-based technique on the metrics of precision, recall and F1 are presented in Table~\ref{tab:svm-results}. Note that similar results have also been reported in \cite{kopcke2010evaluation}. However, the performance of the SVM-based solution is highly dependent on the selected features and training data. Here we only use them for quality reference. It can be observed that the classification quality on DS is better than that on AB. This observation is consistent with the two datasets' matching pairs distributions presented in Figure~\ref{fig:matching-distribution}.

    \begin{table}
    \caption{The SVM-based classification results on DS and AB.}
    \vspace{-0.1cm}
    \centering
    \label{tab:svm-results}
    \begin{tabular}{|c|c|c|c|}
    \hline
    Dataset & Precision & Recall & F1 Score \\
    \hline
    DS & 0.87 & 0.76 & 0.81 \\
    \hline
    AB & 0.47 & 0.35 & 0.40 \\
    \hline
    \end{tabular}
    \vspace{-0.1cm}
    \end{table}


\begin{figure}
\setlength{\abovecaptionskip}{\figcaptionspace}
\centering
{\includegraphics[width=0.55\linewidth]{figures/synthetic/logisticFunc.pdf}}
\caption{Logistic function.}
\label{fig:logistic-func}
\vspace{-0.65cm}
\end{figure}

  The generator for synthetic datasets uses the logistic function to simulate the function of match proportion with regard to pair similarity. The logistic function is represented by
\begin{equation}
  \frac{0.95}{1+e^{(-\tau(v-0.55))}}
\end{equation}
in which $v$ denotes pair similarity and the parameter $\tau$ specifies the steepness of the logistic curve. Some examples of the logistic function are also shown in Figure~\ref{fig:logistic-func}. As the value of $\tau$ decreases, the curve becomes less steep; the generated ER workload would be more challenging. The generator also has the parameter $\sigma$, which specifies the variances of the subsets' match proportions. A larger value of $\sigma$ would result in more distribution irregularity; the generated ER workload would be more challenging.

  Note that in our experiments, we have the ground-truth labels for all the test pairs. The ground-truth labels are originally hidden; whenever manual verification is called for, they are provided to the program. Existing crowdsourcing platforms can obviously be used to perform manual verification. Integrating HUMO with crowdsourcing platforms is an interesting future work. It is however beyond the scope of this paper.
	
  On efficiency evaluation, all experiments were run on a commercial machine running Windows 10, equipped with an Intel Core i5 2.30GHz and 16GB of RAM.
	
\subsection{Evaluation of HUMO Optimization} \label{sec:HUMO-experiment}

\subsubsection{On Real Datasets}

\begin{figure}
\setlength{\abovecaptionskip}{\figcaptionspace}
\centering
\subfigure[DS dataset.]
{\includegraphics[width=0.48\linewidth]{figures/dblp-scholar/dsmanual_g.pdf}}
\subfigure[AB dataset.]
{\includegraphics[width=0.48\linewidth]{figures/abt-buy/abmanual_g.pdf}}
\caption{Comparison of human cost on two real datasets (with $\theta =0.9$).}
\label{fig:realdata-evaluation}
\end{figure}

\begin{table}
\vspace{-0.2cm}
\caption{The quality levels achieved by BASE on DS and AB.}
\vspace{-0.1cm}
\centering
\label{tab:con-quality}
\begin{tabular}{|c|c|c|}
\hline
Quality & \multicolumn{2}{c|}{Quality Levels of Results} \\
\cline{2-3}
Requirement & DS & AB \\
\hline
$\alpha=0.70$ & $\bar{\alpha}=0.9679$ & $\bar{\alpha}=0.9843$ \\
$\beta=0.70$ & $\bar{\beta}=0.9725$ & $\bar{\beta}=0.9244$ \\
\hline
$\alpha=0.75$ & $\bar{\alpha}=0.9732$ & $\bar{\alpha}=0.9843$ \\
$\beta=0.75$ & $\bar{\beta}=0.9738$ & $\bar{\beta}=0.9244$ \\
\hline
$\alpha=0.80$ & $\bar{\alpha}=0.9786$ & $\bar{\alpha}=0.9845$ \\
$\beta=0.80$ & $\bar{\beta}=0.9738$ & $\bar{\beta}=0.9382$ \\
\hline
$\alpha=0.85$ & $\bar{\alpha}=0.9786$ & $\bar{\alpha}=1.0\ \ \ \ $ \\
$\beta=0.85$ & $\bar{\beta}=0.9744$ & $\bar{\beta}=0.9521$ \\
\hline
$\alpha=0.90$ & $\bar{\alpha}=0.9883$ & $\bar{\alpha}=1.0\ \ \ \ $ \\
$\beta=0.90$ & $\bar{\beta}=0.9744$ & $\bar{\beta}=0.9521$ \\
\hline
$\alpha=0.95$ & $\bar{\alpha}=0.9946$ & $\bar{\alpha}=1.0\ \ \ \ $ \\
$\beta=0.95$ & $\bar{\beta}=0.9852$ & $\bar{\beta}=0.9659$ \\
\hline
\end{tabular}
\vspace{-0.5cm}
\end{table}

\begin{table}
\vspace{-0.4cm}
\caption{The quality levels achieved by SAMP on DS and AB.}
\vspace{-0.1cm}
\centering
\label{tab:agg-quality}
\begin{tabular}{|c|c|c|c|c|}
\hline
Quality & \multicolumn{2}{c|}{Quality Levels of Results} & \multicolumn{2}{c|}{Success rate}\\
\cline{2-5}
Requirement & DS & AB  & \ DS\ \  & AB  \\
\hline
$\alpha=0.70$ & $\bar{\alpha}=0.8649$ & $\bar{\alpha}=0.9282$ & \multirow{2}{*}{100} & \multirow{2}{*}{100} \\
$\beta=0.70$ & $\bar{\beta}=0.8365$ & $\bar{\beta}=0.8849$ & & \\
\hline
$\alpha=0.75$ & $\bar{\alpha}=0.8347$ & $\bar{\alpha}=0.9597$ & \multirow{2}{*}{100} & \multirow{2}{*}{100}\\
$\beta=0.75$ & $\bar{\beta}=0.8574$ & $\bar{\beta}=0.9046$ & &\\
\hline
$\alpha=0.80$ & $\bar{\alpha}=0.8544$ & $\bar{\alpha}=0.9635$ & \multirow{2}{*}{100} & \multirow{2}{*}{100}\\
$\beta=0.80$ & $\bar{\beta}=0.8980$ & $\bar{\beta}=0.9158$ & &\\
\hline
$\alpha=0.85$ & $\bar{\alpha}=0.9011$ & $\bar{\alpha}=0.9726$ & \multirow{2}{*}{96} & \multirow{2}{*}{100}\\
$\beta=0.85$ & $\bar{\beta}=0.9205$ & $\bar{\beta}=0.9253$ & &\\
\hline
$\alpha=0.90$ & $\bar{\alpha}=0.9489$ & $\bar{\alpha}=0.9907$ & \multirow{2}{*}{97} & \multirow{2}{*}{100} \\
$\beta=0.90$ & $\bar{\beta}=0.9436$ & $\bar{\beta}=0.9398$ & &\\
\hline
$\alpha=0.95$ & $\bar{\alpha}=0.9834$ & $\bar{\alpha}=0.9977$ & \multirow{2}{*}{98} & \multirow{2}{*}{100} \\
$\beta=0.95$ & $\bar{\beta}=0.9683$ & $\bar{\beta}=0.9574$ & &\\
\hline
\end{tabular}
\end{table}

\begin{table}
\caption{The quality levels achieved by HYBR on DS and AB.}
\vspace{-0.1cm}
\centering
\label{tab:hyb-quality}
\begin{tabular}{|c|c|c|c|c|}
\hline
Quality & \multicolumn{2}{c|}{Quality Levels of Results} & \multicolumn{2}{c|}{Success rate}\\
\cline{2-5}
Requirement & DS & AB  & \ DS\ \  & AB  \\
\hline
$\alpha=0.70$ & $\bar{\alpha}=0.8649$ & $\bar{\alpha}=0.9304$ & \multirow{2}{*}{100} & \multirow{2}{*}{100} \\
$\beta=0.70$ & $\bar{\beta}=0.8365$ & $\bar{\beta}=0.8306$ & & \\
\hline
$\alpha=0.75$ & $\bar{\alpha}=0.8347$ & $\bar{\alpha}=0.9717$ & \multirow{2}{*}{100} & \multirow{2}{*}{100}\\
$\beta=0.75$ & $\bar{\beta}=0.8573$ & $\bar{\beta}=0.8589$ & &\\
\hline
$\alpha=0.80$ & $\bar{\alpha}=0.8535$ & $\bar{\alpha}=0.9632$ & \multirow{2}{*}{100} & \multirow{2}{*}{100}\\
$\beta=0.80$ & $\bar{\beta}=0.8937$ & $\bar{\beta}=0.8946$ & &\\
\hline
$\alpha=0.85$ & $\bar{\alpha}=0.9015$ & $\bar{\alpha}=0.9898$ & \multirow{2}{*}{95} & \multirow{2}{*}{100}\\
$\beta=0.85$ & $\bar{\beta}=0.9171$ & $\bar{\beta}=0.9160$ & &\\
\hline
$\alpha=0.90$ & $\bar{\alpha}=0.9487$ & $\bar{\alpha}=0.9957$ & \multirow{2}{*}{97} & \multirow{2}{*}{100} \\
$\beta=0.90$ & $\bar{\beta}=0.9425$ & $\bar{\beta}=0.9327$ & &\\
\hline
$\alpha=0.95$ & $\bar{\alpha}=0.9834$ & $\bar{\alpha}=0.9991$ & \multirow{2}{*}{97} & \multirow{2}{*}{100} \\
$\beta=0.95$ & $\bar{\beta}=0.9679$ & $\bar{\beta}=0.9521$ & &\\
\hline
\end{tabular}
\end{table}

\begin{figure}
\setlength{\abovecaptionskip}{\figcaptionspace}
\begin{minipage}[t]{0.5\textwidth}
\centering
\subfigure[Human Cost.]
{\includegraphics[width=0.48\linewidth]{figures/dblp-scholar/ds_labor_conf.pdf}}
\subfigure[Success rate.]
{\includegraphics[width=0.48\linewidth]{figures/dblp-scholar/ds_succ_conf.pdf}}
\caption{Varying confidence level on DS (with $\alpha=0.9$ and $\beta=0.9$).}
\label{fig:ds-confidence}
\vspace{0.4cm}
\end{minipage}
%\end{figure}

%\begin{figure}
%\setlength{\abovecaptionskip}{\figcaptionspace}
\begin{minipage}[t]{0.5\textwidth}
\centering
\subfigure[Human Cost.]
{\includegraphics[width=0.48\linewidth]{figures/abt-buy/ab_labor_conf.pdf}}
\subfigure[Success rate.]
{\includegraphics[width=0.48\linewidth]{figures/abt-buy/ab_succ_conf.pdf}}
\caption{Varying confidence level on AB (with $\alpha=0.9$ and $\beta=0.9$).}
\label{fig:ab-confidence}
\end{minipage}
\vspace{-0.55cm}
\end{figure}

  The comparative results on the two real datasets are presented in Figure~\ref{fig:realdata-evaluation}. On both datasets,  the confidence level for SAMP and HYBR is set to 0.9. Note that for SAMP and HYBR, different runs may generate different solutions due to sampling randomness. Their reported results are therefore the averages over 100 runs. It can be observed that on both datasets, the baseline approach (BASE) requires more human cost than the partial-sampling approach (SAMP). This is mainly due to BASE's conservative estimations of the match proportions of $D_-$ and $D_+$. The more aggressive SAMP approach achieves better performance by effectively reducing their estimation margins. On DS, HYBR performs roughly the same as SAMP; on AB, HYBR however clearly outperforms SAMP. The results on AB show that HYBR can achieve better performance than SAMP by using the better of both BASE and SAMP estimates. It can also be observed that given the same quality requirement, AB requires more human cost than DS. This result should not be surprising given that AB is a more challenging workload than DS. Finally, it is worthy to point out that on both datasets, the required human cost only increases modestly with quality requirement. With both precision and recall guarantees set at 0.9, DS and AB require only around 7\% and 12\% manual work respectively if performed by HYBR.

  We also report the achieved quality levels of different approaches. Note that BASE generates only one HUMO solution on each dataset. Its achieved quality levels on DS and AB are presented in Table~\ref{tab:con-quality}. It can be observed that all the BASE solutions successfully meet the specified quality requirement. Similarly, the achieved quality levels of SAMP and HYBR on DS and AB are presented in Table~\ref{tab:agg-quality} and Table~\ref{tab:hyb-quality} respectively. For SAMP and HYBR, we also report their success rates (to meet quality requirement) of multiple runs besides the averaged precision and recall levels. It can be observed that on both averaged quality and success rate, SAMP and HYBR achieve levels well above what was required in most cases.

  Finally, we evaluate how the required human cost and the success rate of SAMP and HYBR vary with different confidence levels. The required precision and recall levels are both set to 0.9. The detailed results on DS and AB are presented in Figure~\ref{fig:ds-confidence} and Figure~\ref{fig:ab-confidence} respectively. It is clearly observable that the required human cost only increases modestly with the confidence level. SAMP and HYBR's achieved success rates are always above the specified confidence levels. In most cases, the margins between them are considerable. These experimental results demonstrate the robustness of the Gaussian process in approximating match proportions in real application scenarios.

\subsubsection{On Synthetic Datasets}


  Firstly, we fix the parameter value of $\sigma$ at 0.1 and vary the parameter value of $\tau$ from 8 to 18 to test the approaches' performance on the datasets with different match proportion functions. Secondly, we fix the parameter value of $\tau$ at 14 and vary the parameter value of $\sigma$ from 0.1 to 0.5 to test their performance sensitivities to different match proportion irregularities. In both cases, the required precision and recall levels are set to be 0.9. The confidence level of SAMP and HYBR is set at 0.9.

   The detailed evaluation results for the first case are presented in Figure~\ref{fig:synthetic-experiment}. As expected, all the approaches require lesser manual work as $\tau$ is set larger. The results also clearly show that HYBR can effectively use the better of both BASE and SAMP estimates to improve performance. When $\tau\leq10$, BASE requires less manual work than SAMP. When $\tau> 10$, BASE instead requires more manual work than SAMP. However, HYBR can achieve whichever better of BASE and SAMP at all the settings of $\tau$. All the achieved precisions and recalls are observed to be above the required level of 0.9.


\begin{figure}
\setlength{\abovecaptionskip}{\figcaptionspace}
\centering
\subfigure[Human Cost.]
{\includegraphics[width=0.3\linewidth]{figures/synthetic/noise01Manual_g.pdf}}
\subfigure[Precision level.]
{\label{fig:synthetic-noise01Precision}
\includegraphics[width=0.3\linewidth]{figures/synthetic/noise01Precision_g.pdf}}
\subfigure[Recall level.]
{\label{fig:synthetic-noise01Recall}
\includegraphics[width=0.3\linewidth]{figures/synthetic/noise01Recall_g.pdf}}
\caption{Varying $\tau$ (steepness) of the logistic curve on the synthetic datasets.}
\label{fig:synthetic-experiment}
\vspace{-0.3cm}
\end{figure}

\begin{figure}
\setlength{\abovecaptionskip}{\figcaptionspace}
\centering
\subfigure[Human Cost.]
{\includegraphics[width=0.3\linewidth]{figures/synthetic/varynoiseManual_g.pdf}}
\subfigure[Precision level.]
{\includegraphics[width=0.3\linewidth]{figures/synthetic/varynoisePrecision_g.pdf}}
\subfigure[Recall level.]
{\includegraphics[width=0.3\linewidth]{figures/synthetic/varynoiseRecall_g.pdf}}
\caption{Varying $\sigma$ (variance) of match proportions on the synthetic datasets.}
\label{fig:synthetic-noise-level}
\vspace{-0.55cm}
\end{figure}

 The detailed evaluation results for the second case are presented in Figure~\ref{fig:synthetic-noise-level}. As expected, the required manual workload for SAMP and HYBR generally increases as $\sigma$ is set larger. Similar to what was observed in Figure~\ref{fig:synthetic-experiment}, HYBR achieves the best performance among them by effectively using the better of both BASE and SAMP estimates. With $\sigma\leq 0.4$, all the three approaches can meet the quality requirement. With $\sigma=0.5$, SAMP still manages to meet the quality requirement, but BASE and HYBR fails on precision. This is due to the fact that with $\sigma=0.5$, the monotonicity assumption of precision does not hold true anymore on the synthetic dataset. The effectiveness of SAMP to enforce quality guarantees in the big-variance case of $\sigma=0.5$ also validates the performance resilience of the Gaussian process.

\subsection{Comparison with State-Of-The-Art} \label{sec:other-comparison}

\begin{table}
\begin{minipage}[t]{0.5\textwidth}
\centering
\caption{Performance comparison between HUMO and ACTL on DS. }
\vspace{-0.1cm}
\label{tab:compare-act-ds}
\begin{tabular}{|c|c|c|c|c|c|}
\hline
Target & \multicolumn{2}{c|}{Achieved Recall} & \multicolumn{2}{c|}{$\psi (\%)$} & \multirow{2}{*}{$\frac{\Delta \psi}{100 \cdot \Delta Recall}$}\\
\cline{2-5}
Precision & HUMO & ACTL & HUMO & ACTL & \\
\hline
0.75 & 0.8573 & 0.8210 & 4.94 & 4.08 & 0.2373\\
0.80 & 0.8937 & 0.7953 & 5.52 & 4.10 & 0.1439\\
0.85 & 0.9171 & 0.7786 & 6.20 & 3.73 & 0.1779\\
0.90 & 0.9425 & 0.7124 & 7.34 & 3.63 & 0.1614\\
0.95 & 0.9679 & 0.6502 & 10.05 & 3.01 & 0.2217\\
\hline
\end{tabular}
\vspace{0.5cm}
\end{minipage}
%\end{table}
%\begin{table}
\begin{minipage}[t]{0.5\textwidth}
\centering
\caption{Performance comparison between HUMO and ACTL on AB.}
\vspace{-0.1cm}
\label{tab:compare-act-ab}
\begin{tabular}{|c|c|c|c|c|c|}
\hline
Target & \multicolumn{2}{c|}{Achieved Recall} & \multicolumn{2}{c|}{$\psi (\%)$} & \multirow{2}{*}{$\frac{\Delta \psi}{100 \cdot \Delta Recall}$}\\
\cline{2-5}
Precision & HUMO & ACTL & HUMO & ACTL & \\
\hline
0.75 & 0.8589 & 0.1968 & 6.83 & 0.30 & 0.0985\\
0.80 & 0.8946 & 0.1594 & 7.91 & 0.26 & 0.1040\\
0.85 & 0.9160 & 0.1379 & 9.31 & 0.28 & 0.1161\\
0.90 & 0.9327 & 0.1173 & 11.82 & 0.20 & 0.1426\\
0.95 & 0.9521 & 0.0966 & 16.60 & 0.19 & 0.1918\\
\hline
\end{tabular}
\end{minipage}
\vspace{-0.5cm}
\end{table}

 In this subsection, we compare HUMO with the state-of-the-art alternative (ACTL) based on active learning on the two real datasets. We have implemented both techniques proposed in \cite{arasu2010active} and \cite{bellare2012active} respectively. Our experiments showed that they perform similarly on the achieved quality and required manual work. Their detailed performance comparisons can be found in our technical report \cite{chen2017humoreport}. Here, we present the comparative evaluation results between HUMO and the technique proposed in \cite{arasu2010active}. As \cite{arasu2010active}, we employ Jaccard similarity on attributes used in Subsection~\ref{sec:HUMO-experiment} as the similarity space for ACTL. On DS, the used attributes are {\em title} and {\em authors}; on AB, they are {\em product name} and {\em product description}. ACTL uses sampling to estimate the achieved precision level of a given classification solution; therefore it also requires manual work.


 The performance comparisons between HUMO and ACTL on the DS and AB are presented in Table~\ref{tab:compare-act-ds} and Table~\ref{tab:compare-act-ab} respectively, in which $\psi$ represents the percentage of manual work, and $\Delta$ denotes the performance difference between the two methods on a specified metric. The required precision and recall levels are set to be the same for HUMO. Note that ACTL can not enforce recall level. At each given precision level, we record HUMO and ACTL's differences on achieved recall and consumed human cost. It can be observed that the achieved recall level of ACTL generally decreases with the specified precision level. In all the test cases, HUMO achieves higher recall levels than ACTL. We also record the additional human cost required by HUMO for the absolute recall improvement of $1\%$ over ACTL (at the last columns of Table~\ref{tab:compare-act-ds} and Table~\ref{tab:compare-act-ab}). It can be observed that the cost generally increases with the required precision level. With both precision and recall set at the high level of 0.9, the cost is as low as 0.1614\% on DS and 0.1426\% on AB.

  Note that given the same precision requirement, ACTL and HUMO might actually achieve different precisions. Therefore, we also compare their performance on the metric of F1 and record the additional human cost required by HUMO for the absolute F1 improvement of $1\%$ over ACTL. The detailed results on both datasets are presented in Figure~\ref{fig:compare-act}. Similar to what was observed in Table~\ref{tab:compare-act-ds} and Table~\ref{tab:compare-act-ab}, the additional human cost generally increases with the specified precision level. On DS, the additional human cost of HUMO for 1\% increase in F1 score is maximally 0.35\%. On AB, it is as low as 0.21\%.
Along with the results presented in Table~\ref{tab:compare-act-ds} and Table~\ref{tab:compare-act-ab}, these results clearly demonstrate that compared with ACTL, HUMO can effectively improve the resolution quality with reasonable ROI in terms of human cost.

\begin{figure}
\setlength{\abovecaptionskip}{\figcaptionspace}
\centering
\includegraphics[width=0.55\linewidth]{figures/compareACT_F1.pdf}
\caption{The percentage of manual work incurred by HUMO for 1\% absolute improvement in F1 score over ACTL.}
\label{fig:compare-act}
\vspace{-0.2cm}
\end{figure}

\begin{figure}
\setlength{\abovecaptionskip}{\figcaptionspace}
\centering
{\includegraphics[width=0.55\linewidth]{figures/scalability_g.pdf}}
\caption{Varying data size on the synthetic datasets.}
\label{fig:scalability}
\vspace{-0.6cm}
\end{figure}


\subsection{Efficiency Evaluation} \label{sec:scalability}

\begin{table}
\vspace{-0.4cm}
\centering
\caption{Efficiency evaluation on DS and AB datasets.}
\vspace{-0.1cm}
\label{tab:efficiency-evaluation}
\begin{tabular}{|c|c|c|c|c|}
\hline
\multirow{2}{*}{Dataset} & \multirow{2}{*}{$\#$ of pairs} & \multicolumn{3}{c|}{Runtime (Sec)} \\
\cline{3-5}
&& BASE & SAMP & HYBR \\
\hline
DS & 100077 & 0.969 & 6.506 & 7.580 \\
\hline
AB & 313040 & 3.062 & 20.921 & 53.503 \\
\hline
\end{tabular}
%\vspace{-0.4cm}
\end{table}

The machine runtime of HUMO on two real datasets are presented in Table~\ref{tab:efficiency-evaluation}. Note that the reported runtime does not include the time consumed by data preprocessing and the latency incurred by human verification. It can be observed that as indicated by their analyzed complexity, SAMP and HYBR consume considerably more time than BASE. We also evaluate their scalability with varying data size. We generated the test datasets of different sizes using the synthetic data generator. The detailed results are presented in Figure~\ref{fig:scalability}. It can be observed that the runtime of BASE increases only marginally with the increasing dataset size. The runtimes of SAMP and HYBR increase more dramatically but in a polynomial-time manner as dictated by their analyzed complexity. Since efficiency of human work can be expected to be much lower than that of machine computation, machine efficiency of HUMO should not be a concern in practical scenarios.

