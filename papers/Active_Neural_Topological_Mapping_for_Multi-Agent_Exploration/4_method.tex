
\section{Methodology}
% Agent $k$ learns the policy, $\pi_k$, to produce a distribution of actions, $a_{t}^k\sim\pi_{k}(a_{t}^k|o_{t}^k)$, at each time step $t$. 
We follow the paradigm of centralized training and decentralized execution (CTDE) with partial observations of $N$ agents, where agent $k$ receives local observation, $o_{t}^k$, at timestep $t$. The overview of {\name} is depicted in \cref{fig:Overview}. Each agent shares the same Topological Mapper, Hierarchical Topological Planner, Local Planner, and Local Policy while making decisions independently. The Topological Mapper of each agent utilizes a visual encoder and distance-based heuristics to construct a topological map based on RGB-D images and estimated poses from sensory signals. For better cooperation, the Topological Mapper transforms all individual maps into the same coordination and merges them. The Hierarchical Topological Planner (\planner) receives graphs that respectively contain current agent information, main node, and ghost nodes, along with corresponding historical graphs containing agent trajectories, selected main nodes, and selected ghost nodes from the merged topological map.
{\planner} utilizes GNN on these graphs to hierarchically capture spatio-temporal and intra-agent information. It then selects a ghost node as a global goal at each global step.
Finally, the Local Planner plans a reachable path to the predicted global goal via the Fast Marching Method (FMM)~\cite{fmm}, and the Local Policy generates environmental actions based on the reachable path. We remark that the Local Planner and the Local Policy do not involve multi-agent interactions, so they are directly adopted from ANS~\cite{ans}.\looseness=-1


% The Local Planner does the local planning via FMM~\cite{fmm}, and the Local Policy gives out environmental actions to reach the selected global goal for each agent.  

% \subsection{Local Planner and Local Policy}



%The hyperparameters $m$ and $\lambda$ are based on domain knowledge, and the maximum of $m$ is $m_{max}$. Ghost nodes are evenly attached to each main node at a distance $\lambda$ and will be deleted if located in the explored areas. The number of remaining ghost nodes is $m$. Moreover, we have studied the sensibility of these hyperparameters before, and the results show that $m$ and $\lambda$ have little influence on accomplishment while affecting efficiency. More concretely, agents achieve over 90\% \emph{Coverage} ratio at different $m_{max}$ and $\lambda$, while $m_{max}=12$ and $\lambda=3$ produce the fastest exploration and are therefore chosen for the paper. We will include details in Appendix.


\subsection{Topological Mapper\label{sec:mapper}}
We introduce a Topological Mapper to provide a merged topological map, as shown in \cref{fig:Overview}. Inspired by \cite{topo-map2,norl}, we consider two types of nodes in the topological map: main nodes and ghost nodes. 
Main nodes are located where agents have already explored. Ghost nodes are located in the unexplored area and adjacent to the corresponding main node. \looseness=-1

In map construction, the Topological Mapper relies on a visual encoder and distance-based heuristics. Firstly, we adopt an encoder~\cite{vgm} to predict visual embeddings of panoramic RGB-D images. The topological map constructs main nodes based on the cosine similarity between visual embeddings. Specifically, when the cosine similarity between the visual embedding at the current location and that of existing main nodes is below a threshold (i.e., 0.75 in our work), a new main node is constructed at that location and connected to the main node that the agent most recently localized. However, images of some spatially irrelevant locations may be similar (e.g., images of corners and corridors both contain a large portion of the wall), resulting in incorrect connections between main nodes. To deal with this problem, we delete the connection of main nodes with far-distant FMM distance at each global step. 
Once a main node is constructed, $m$ new ghost nodes are uniformly adjacent to this main node at a distance $\lambda$. Ghost nodes can be removed if they are located in the explored areas identified by the current agent's predicted metric map via SLAM. We remark that the metric map is just for identifying explored regions and not for global planning. Besides, a ghost node can be converted to a main node if the agent passes through it. Therefore, the potential number of remaining ghost nodes ranges from 0 to $m$$\times$$N_{main}$, where $N_{main}$ is the number of main nodes. In our work, we choose $\lambda=3$ meters and $m=12$. Moreover, we delete spurious ghost nodes and their connected edges based on the FMM distance between two ghost nodes belonging to two different main nodes and between a main node and a ghost node belonging to another main node.\looseness=-1

For better cooperation, we transform all individual topological maps into the same coordinate system and merge them based on the estimated poses of the agents at each global step. During merging, if the distance between two main nodes from different maps is below a threshold (i.e., 3 meters in our work), we randomly remove one of the nodes and redirect its connected edges to the remaining node.




% Moreover, we study the sensibility of these hyperparameters, and the results show that $m$ and $\lambda$ have little influence on accomplishment while affecting efficiency. More concretely, agents all achieve over 90\% \emph{Coverage} ratio at different $m_{max}$ and $\lambda$, while $m_{max}=12$ and $\lambda=3$ produce the fastest exploration and are therefore chosen for our work.  We provide more details about the results in the Appendix.

% Each node contains the grid position and the semantic label to represent the type of the node is described in Sec. 4.2.1.
% The hyperparameters $m$ and $\lambda$ are based on domain knowledge, and the maximum of $m$ is $m_{max}$. Ghost nodes are evenly attached to each main node at a distance $\lambda$ and will be deleted if located in the explored areas. The number of remaining ghost nodes is $m$. Moreover, we have studied the sensibility of these hyperparameters before, and the results show that $m$ and $\lambda$ have little influence on accomplishment while affecting efficiency. More concretely, agents achieve over 90\% \emph{Coverage} ratio at different $m_{max}$ and $\lambda$, while $m_{max}=12$ and $\lambda=3$ produce the fastest exploration and are therefore chosen for the paper. We will include details in Appendix.
% To build topological maps for the multi-agent exploration task, we designed graph-formation mechanisms that are different from \cite{topo-map2}. Specifically, it utilizes a visual encoder and distance-based heuristics to construct individual maps based on depth images and pose sensory signals, then transforms maps into the same coordination to build a merged graph.



% Afterwards, six fully connected graphs are derived from the merged topological map for global planning in {\planner}. 

% \subsubsection{Graph Builder}
% We consider two types of nodes in a topological map: stellar nodes and orbital nodes. Stellar nodes are located where agents have already passed, while orbital nodes are unexplored and are attached to stellar nodes. The information of each node is the position and the category. For a more precise topological map, we follow the below instructions: (a) a stellar node is added when there is no stellar node surrounding the agent with a radius of $r$. (b) a stellar node substitutes an orbital node within a radius of $r$ to the agent. (c) $N_{ob}$ orbital nodes are evenly distributed around a stellar node with angle $o$ and distance $d$.

% \subsubsection{Graph Filter and Graph Merger}
% The graph filter is executed in every global decision-making step for selecting and thinning out valid orbital nodes as candidates for global goals. We remove redundant orbital nodes based on the following rules: (a) a orbital node is moved when a stellar node is within distance $d$. (b) a orbital node is cleared once another orbital node is within distance $d$. (c) an orbital node is masked when the predicted obstacle blocks the path from the orbital node to its corresponding stellar node.
% %and the region between the obstacle and the stellar node has been explored by over $60\%$.
 %At the same time, the merged graphs are fed into the Graph Filter to ensure sparsity.   

%{\planner} takes four merged graphs from Topological Mapper and selects a global goal for each agent.
 
\begin{figure*}[ht]
	\centering
    \includegraphics[width=0.95\linewidth]{figures/architecture.png}
	\centering \caption{Workflow of \emph{Hierarchical Topological Planner} (\planner), including a Memory Fusion, a Main Node Selector, a Ghost Node Selector and an Action Generator. {\planner} is under decentralized decision-making setting. We take Agent $k$ as an example.}
\label{fig:htp}
\vspace{-6mm}
\end{figure*}
%{\planner} handles graph-based inputs with variable node numbers for stable training and effective planning. 
\subsection{Hierarchical Topological Planner}
The Hierarchical Topological Planner (\planner) selects a ghost node as a global goal at each global step. However, it is difficult for MARL to directly explore the (near) optimal strategy due to the large and varying search space associated with the number of ghost nodes. To address this issue, we propose a hierarchical network that matches agents to main nodes and then to the corresponding ghost nodes. Therefore, the {\planner} can be easily optimized in the reduced search space with much fewer candidate nodes and potentially provide a more appropriate probability distribution of ghost nodes in a coarse-to-fine manner.
The workflow of the {\planner} is illustrated in \cref{fig:htp}.
Firstly, at each global step, we extract an agent-info graph, $G_{a}$, a main node graph, $G_{m}$, a ghost node graph, $G_{g}$, and three corresponding historical graphs, $\hat{G}$, from a merged topological map. Subsequently, we update each $G$ with its historical graph in the Memory Fusion to aggregate current and historical information. The Main Node Selector computes matching scores between the agent and the main nodes via the attention mechanism~\cite{attention}. 
Afterward, the Ghost Node Selector calculates the probability distribution of ghost nodes based on ghost node features and the matching results from the Main Node Selector. 
Finally, the Action Generator takes in the probability distribution of ghost nodes and determines the most appropriate ghost node as the global goal at each global decision-making step.\looseness=-1


%Therefore, the observation space can be formulated as $R^{(4\times max\times 2)}+N^{(4\times max\times 2)}$, where $max$ represents the maximum number of nodes for each graph.
We remark that $G_{a}$ contains the current agent information, while $G_{m}$ contains current main node features, and $G_{g}$ includes current ghost node features. Besides, in historical graphs, $\hat{G_{a}}$ contains agent trajectories, $\hat{G_{m}}$ contains selected main node features, and $\hat{G_{g}}$ includes selected ghost node features from past global steps.
Each node consists of its grid position, $(x,y)\in R^2, x,y\in [0,map\ size]$, and a semantic label, $(s_1,s_2)\in N^2, N=\left\{0,1\right\}$.
This semantic label represents the type of node, including agent nodes, historical agent nodes, map nodes, and historical map nodes.


%Specifically, we apply the planning to the stellar and orbital nodes in a coarse-to-fine fashion.
\subsubsection{Memory Fusion}
Exploration is a memory-based task~\cite{memory_based} which heavily relies on historical information.
Therefore, this module incorporates each graph with its historical counterpart to mitigate the risk of memory loss. Concretely, we first use a weight-shared multi-layer perception (MLP) layer to encode node features due to their consistent information types across different graphs. Then, the Memory Fusion merges each graph with its corresponding historical graph and yields an updated graph, $\tilde{G}$, via the self-attention and the cross-attention mechanisms~\cite{attention}. The output dimensions of the last cross-attention layer match those of the original node features, ensuring that the shape of $\tilde{G}$ remains consistent with that of $G$.\looseness=-1

%For each graph $G= \langle V,E  \rangle$, each node $v$ comprises a grid position $(x,y)\in R^2, x,y\in [0,map\ size]$ and a semantic label $(s_1,s_2)\in N^2, N=\left\{0,1 \right\}$. 
% performs graph fusion for each graph pair (i.e., the graph and its corresponding historical graph) to 
%$\tilde{G}_{m}$, and $\tilde{G_{g}}$


% To aggregate the historical and current information, 
% {\color{red}To aggregate the historical and current information, we first use a weight-shared Multi-Layer Perception (MLP) layer to encode all node features. Then, the Graph Fusion incorporates each graph pair and yields the updated graphs, $\tilde{G}_{a}$, $\tilde{G}_{m}$, and $\tilde{G_{g}}$, by adopting the self-attention and the cross-attention mechanism~\cite{attention}. In the self-attention stage, we update all graphs with their own information:
% \begin{equation}
%     \begin{aligned}
%         G^{(t+1)} = G^{(t)} + f_{self}(G^t,self\_attn(G))
%     \end{aligned}
% \end{equation}
% \begin{equation}
%     \begin{aligned}
%         self\_attn(G) = Softmax(Q_GK^T_G)V_G \label{self_attn}
%     \end{aligned}
% \end{equation}

% Here $G^t$ represents node features in the graph at time step $t$ and $self\_attn(G)$ represents the self-attention operator in equation \ref{self_attn}. $f_{self}$ is an MLP layer where the inputs are the concatenation of $G^t$ and $self\_attn(G)$. $Q_G$, $K_G$, and $V_G$ are linear projections of node features. Then in the cross-attention stage, we update each graph with its corresponding historical graph:
% \begin{equation}
%     \begin{aligned}
%         \tilde{G} = G + f_{cross}(G,cross\_attn(G,\hat{G}))
%     \end{aligned}
% \end{equation}
% \begin{equation}
%     \begin{aligned}
%         &cross\_attn(G,\hat{G}) = \\
%         &Softmax(f_{encode}(Q_G,K^T_{\hat{G}},d_{fmm}))V_{\hat{G}} \label{cross_attn}
%     \end{aligned}
% \end{equation}

% Here $f_{cross}$ and $f_{encode}$ represent different MLP layers, and the inputs of $f_{encode}$ are the concatenation of $Q_G$, $K^T_{\hat{G}}$ and $d_{fmm}$. $cross\_attn$ represents the cross-attention operator in equation \ref{cross_attn} and the $d_{fmm}$ is the FMM distance between the given node pair. $Q_G$ is the linear projection of graph $G$ and $K_{\hat{G}}$, $V_{\hat{G}}$ are linear projections of the historical graph $\hat{G}$.}
%\subsubsection{Probabilistic Node Selector}
%Given $\tilde{G}_{goal}$ and $\tilde{G}_{agent}$, the Probabilistic Node Selector can assign a global goal to each agent. However, during an exploration episode, the number of orbital nodes may change rapidly, increasing the learning difficulty of selecting an orbital node for each individual agent. To address this problem, we proposed a hierarchical network containing two modules: a stellar Node Selector and an orbital Node Selector. These two modules perform planning on the stellar and orbital nodes in a coarse-to-fine manner.

%The {probabilistic node selector} contains two modules: a stellar node selector and an orbital node selector. Firstly, the stellar node selector utilizes $\tilde{G}_{agent}$ and the stellar nodes in $\tilde{G}_{goal}$ to calculate the stellar scores. Afterwards, $\tilde{G}_{agent}$, the orbital nodes in $\tilde{G}_{goal}$ and the stellar scores are sent to the orbital node selector to predict the orbital scores, which are used to assign the global goals. As the building blocks in our \planner, we first introduce the individual encoder and the relation encoder.

\subsubsection{Main Node Selector}
The Main Node Selector is introduced for the high-level goal selection and yields the matching scores between the agent and the main nodes. It predicts the next preferred region to explore since a main node with a higher matching score implies a higher probability of selecting its corresponding ghost nodes as the global goal. More concretely, this module receives $\tilde{G}_{a}$ and $\tilde{G}_{m}$ and then leverages an Individual Encoder and a Relation Encoder to infer the matching scores, $S_{m, re}$. The Individual Encoder perceives the states of the agents and the spatial information of the main nodes, while the Relation Encoder captures the correlation between the agents and the main nodes. \looseness=-1

% The scores are then sent to the Ghost Node Selector to provide guidance for the matching between agents and ghost nodes.

\textbf{Individual Encoder:} The Individual Encoder captures relationships between any two nodes in the same graph and updates these nodes. As shown in \cref{fig:ind_encoder}, this module first calculates the normalized matching scores of any two nodes:
\begin{equation}
S_{m, in} = Softmax(W_{Q}X \times W_{K}X).
\end{equation}
Here $X$ denotes the node features, while $W_{Q}$ and $W_{K}$ represent the linear projections of $X$. The $S_{m, in}$ is a matrix where each element, $S_{m, in}^{(i,j)}$, refers to the matching score between node $i$ and node $j$. 


After that, each node is updated by an MLP layer, $f_{in}$, with a residual connection. The input to $f_{in}$ is the concatenation of each node feature and the weighted sum of its neighboring features. The update of each node is formulated as: \looseness=-1
\begin{equation}
    \begin{aligned}
        X^{(t+1)} = X^t
        +f_{in}(X^t,  W_{V}X^t \times S_{m, in}^T),
    \end{aligned}
\end{equation}
where $X^{t}$ represents the node features at time step $t$, and $W_{V}$ is the linear projection of $X^t$. The matching scores between nodes in a graph, $S_{m, in}$, serve as the weights for neighboring nodes. 

\textbf{Relation Encoder:} The Relation Encoder captures correlations between any two nodes from different graphs. The architecture of the Relation Encoder is shown in \cref{fig:rel_encoder}. Considering the node features in two different graphs as $Y$ and $Z$, we calculate the normalized matching scores for each node pair $(y,z)$ by taking a softmax operator over the outputs of an MLP layer, $f_{dis}$. The input to $f_{dis}$ is the concatenation of $W_{Q}Y$, $W_{K}Z$, and $d_{fmm}$, where $d_{fmm}$ is the FMM distance between the given node pair. The calculation of the scores is formulated as:
\begin{equation}
    S_{m, re} = Softmax( f_{dis}( W_{Q}Y,
    W_{K}Z, d_{fmm})).
\end{equation}
Afterward, we update all nodes via an MLP layer, $f_{re}$, with a residual connection:
\begin{equation}
Y^{(t+1)} =  Y^t + f_{re}(Y^t,  W_{V}Z^t \times S_{m, re}^T).
\end{equation}
Here $W$ represents linear projections, while $Y^t$ and $Z^t$ denote different node features at time step $t$. Note that $S_{m, re}^{(k)}$ denotes the matching score between agent $k$ and the main nodes and is then sent to the Ghost Node Selector for agent $k$.

\begin{figure}[ht]
\vspace{-1mm}
\captionsetup{justification=centering}
	\centering
    \subfigure[\label{fig:ind_encoder}Individual Encoder]
        {\centering        {\includegraphics[height=4.5cm]{figures/ind_encoder.png}
            }
    }
    \subfigure[\label{fig:rel_encoder}Relation Encoder]
        {\centering        {\includegraphics[height=4.5cm]{figures/rel_encoder.png}
            }
    }
    \vspace{-2mm}
    \caption{\raggedright{Network architecture of the Individual Encoder and the Relation Encoder.}}
    \vspace{-3mm}
    \label{fig:encoder}
\end{figure} 
\subsubsection{Ghost Node Selector}
For the low-level goal selection, the Ghost Node Selector provides a probability distribution of ghost nodes and predicts the next location to explore based on the preferred region. Similar to the Main Node Selector, the Ghost Node Selector first receives $\tilde{G}_{a}$ and $\tilde{G}_{g}$ and provides ghost node scores, $S_{g, re}$, via an Individual Encoder and a Relation Encoder. The Individual Encoder captures intra-agent interactions and spatial information of ghost nodes, while the Relation Encoder infers the correlations between agents and ghost nodes. Additionally, we update $S_{g, re}^{(k,j)}$ by multiplying it with $S_{m, re}^{(k,h)}$ to aggregate information from main and ghost nodes for more appropriate goal selection:
\begin{equation}
    \hat{S}_{g, re}^{(k,j)} =  S_{g, re}^{(k,j)} \times S_{m, re}^{(k,h)}.
\label{eq:multiply}
\end{equation}
Here, $k$ is a specific agent, $j$ is the main node, and $h$ denotes the corresponding ghost node of the main node $j$. $\hat{S}_{g, re}^{(k)}$ is the final matching score of agent $k$ and ghost nodes.


\subsubsection{Action Generator}
The Action Generator selects a global goal from all ghost nodes at each global step. We regard $\hat{S}_{g, re}$ as the probability distribution over the ghost nodes. The action space for the {\planner} is $A=\{a|a\in ghost\ nodes\}$, where $a$ is a discrete variable sampled from a categorical distribution, indicating the index of the selected ghost node.


% Since the inputs of the last relation encoder are the main nodes and the agent-info nodes, the $S_{g, re}$ represents each agent's preference for all ghost nodes, which are candidates for the global goals.



% Specifically, the Local Planner plans a path to the global goal via FMM~\cite{fmm} and generates a sequence of reachable short-term goals. The Local Policy takes in the agent's current local observation and the relative angle and distance from the agent to the nearest short-term goal, and then outputs an environmental action.


\subsection{Reinforcement Learning Design}
We train the {\planner} by using multi-agent proximal policy optimization (MAPPO)~\cite{mappo}, which is a multi-agent variant of proximal policy optimization (PPO)~\cite{ppo}.

% We consider {\planner} as the policy network and use multi-agent proximal policy optimization (MAPPO)~\cite{mappo}, a multi-agent variant of proximal policy optimization (PPO)~\cite{ppo}, as the policy optimizer.
\textbf{Reward Function: }To promote efficient and cooperative exploration, we introduce four types of rewards. The coverage reward, $R_{cov}$, represents the increase of the explored area to encourage thorough exploration. Besides, the success reward, $R_{suc}$, offers a bonus for reaching the target coverage ratio. The overlap penalty, $R_{o}$, denotes the overlapped area passed by agents to encourage cooperation. Finally, for efficient exploration, agents receive a constant time penalty, $R_{t}$, at each timestep until they attain the target coverage ratio. As a result, the reward function, $R_{total}$, is a linear combination of these kinds of rewards.


% :
% \begin{equation}
% R_{total}=\alpha R_{cov}+\beta R_{suc}+\gamma R_{o}+\theta R_{t},
% \end{equation}


% where $\alpha$ and $\beta$ are the positive coefficients of $R_{cov}$ and $R_{suc}$, while $\gamma$ and $\theta$ are the negative coefficients of $R_{o}$ and $R_{t}$, respectively.\