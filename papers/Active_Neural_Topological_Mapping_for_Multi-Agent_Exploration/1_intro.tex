
\section{Introduction}
\label{sec:intro}


%1、visual exploration的重要性，应用
%2、传统方法，着重讲在拓扑上的
%3、rl学习法，着重讲在拓扑上的
%4、我们的方法：
%5、结果
Exploration~\cite{ramakrishnan2021exploration} is one of the fundamental building blocks for developing intelligent autonomous agents, and is widely applied in autonomous driving~\cite{autonomousdriving}, disaster rescue~\cite{rescue}, and planetary exploration~\cite{DBLP:journals/corr/abs-2002-00515}. 
In this paper, we focus on multi-agent exploration, where agents simultaneously explore an unknown scene via sensory signals. To achieve efficient exploration, we typically employ a workflow of autonomous map construction and collaborative planning. 

The spatial arrangement of metric maps~\cite{ans,singleagent-RL1} can vary significantly between scenes, which hinders the generalization ability for exploration. Metric maps also perform poorly in efficiency, as they are with high communication traffic and have difficulty scaling to larger environments. In contrast, topological maps, which contain abstract but essential information, require less communication traffic and are less sensitive to changes in scene structure. Therefore, applying topological maps~\cite{vgm,topo-map2} offers significant generalization potential and high efficiency.


Topology-based exploration tasks commonly utilize classical methods~\cite{normalized_frontier,zhang2022mr} for planning due to their minimal training time and direct deployment for evaluation. However, they often suffer from numerous handcrafted parameters and rule-based coordination strategies, which limits their effectiveness. In contrast, DRL has shown potential for topological exploration~\cite{topo_exploration, topo_multi2} due to its ability to model arbitrarily complex strategies and execute real-time actions. However, these methods are based on pre-built graphs~\cite{topo_exploration,topo-map4} or tested on simple grid maps~\cite{topo_multi2}. 
Applying active topological mapping in RL-based multi-agent exploration is confronted with the following limitations: (a) the number of nodes in the merged graph is large and constantly changing during exploration, leading to unstable RL training and suboptimal results in such a large and varying search space; (b) capturing complex relationships between agents and topological maps is difficult, resulting in an unbalanced workload distribution among agents.


%Active topological mapping in complex scenarios is only applied in the navigation~\cite{topo-map2,vgm}, a downstream task of exploration~\cite{singleagent-RL1}, which has a pre-defined goal to be achieved and does not require the graph to represent the entire environment. 

%inefficient goal assignment. with less inference computation
% . A trained RL-based agent is a deep neural network that 

% Exploration focuses not only on the navigation to the goals but also on the prediction of the goals for a completely exploration .

%现有的方法用拓扑地图进行探索的都是预先建好图的或者有先验语义知识的，或者现有learning的active mapping的但是都是用于导航任务上，因为导航任务更简单，她不需要建图完全。而探索任务需要建图完全，除了要往一个目标方向导航外，还包括了对各区域的探索分配。故探索的下游任务为导航down-stream navigation tasks。这现有的这些导航任务不能直接上游适应exploration with active mapping。那么将topo地图用于rl-based的多机探索上有几大难点：


%only keep the information of important places, which aligns with human cognition. the scenarios under


To address the above challenges, we propose \emph{\underline{M}ulti-\underline{A}gent \underline{N}eural \underline{T}opological \underline{M}apping} (\name), an RL-based topological solution for multi-agent exploration. We adopt a modular exploration strategy that divides the planning process into two phases. In the first phase, the global planner infers global goals in each global decision-making step. Subsequently, the local planner predicts the environmental actions to encourage the agents to reach the global goals. {\name} comprises a Topological Mapper to build topological maps, a Hierarchical Topological Planner (\planner) to infer the global goals, and a Local Planner and a Local Policy to generate environmental actions. The Topological Mapper employs a visual encoder and distance-based heuristics to construct graphs with main nodes (i.e., explored areas) and ghost nodes (i.e., unexplored areas). To build more accurate graphs, each agent also maintains a predicted metric map to identify explored areas and prune graphs. We remark that the metric maps are not shared among agents and are not used for global planning, thus promising reduced communication traffic and enhanced generalization capabilities. The RL-based global planner, the Hierarchical Topological Planner ({\planner}), is the most crucial component that selects a main node and then chooses a corresponding ghost node as a global goal for each agent at each global step. This hierarchical goal selection significantly reduces the search space with much fewer candidate nodes, thus addressing challenge (a).
Furthermore, the {\planner} leverages graph neural networks to capture the relationship between agents and topological maps, solving challenge (b).


% give a more appropriate probability distribution of nodes.
%  The Topological Mapper will convert ghost nodes into main nodes once agents reach ghost nodes. Therefore, as global goal candidates, ghost nodes enable agents to explore the unseen area.
%Meanwhile, only the valid orbital nodes are kept as candidates for global goals.

We conduct extensive experiments in the physically realistic simulator, Habitat~\cite{habitat}. The experimental results demonstrate that {\name} has at least 7.63\% fewer steps than RL-based baselines, and at least 26.40\% fewer steps than planning-based competitors in unseen scenarios.

% that of all ghost nodes, is easily optimized and compare {\name} with a collection of planning-based and RL-based baselines.





