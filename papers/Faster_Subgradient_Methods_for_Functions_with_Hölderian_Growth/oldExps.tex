\section{Numerical Experiments}\label{sec_numerical}
In this section we present simulations to demonstrate some of the theoretical findings in this manuscript. We consider two examples satisfying HEB($c,\theta$) with $\theta=1$ to test our proposed descending stairs stepsize choice in DS-SG and our ``double descending stairs" method for unknown $c$, DS2-SG. 
\subsection{Least-Absolute Deviations Regression}
Consider the following problem:
\begin{eqnarray}\label{prob_l1_l1}
\min_x \|Ex-b\|_1:\quad\|x\|_1\leq\tau.
\end{eqnarray}
This objective function is often used in regression problems and in machine learning \cite{hastie2009elements,wang2006regularized,wang2013l1,gao2010asymptotic}. 
% where one observes $b = Ex+\eta$ and would like to recover $x$, given that $\eta$ is some unknown noise term. If $\eta$ is Gaussian, then the maximum likelihood estimator is the least-squares minimizer. However, if the noise is known to contain several outliers, or equivalently is sampled from a distribution with ``heavier tails" then the Gaussian, the least absolute deviation loss is a more robust choice as the resulting estimator is less sensitive to outliers \cite{portnoy1997gaussian}. The $\ell_1$ box constraint is used to encourage a sparse solution $x$. In the context of regression, enforcing sparsity makes sense when only a small subset of the features is actually correlated with the target variable \cite{hastie2009elements}. The statistical estimation properties of (\ref{prob_l1_l1}) were discussed in \cite{wang2006regularized,wang2013l1,gao2010asymptotic}. 
Besides the subgradient techniques considered in this manuscript, there are a few other methods which can tackle Prob. (\ref{prob_l1_l1}). The problem can be written as a linear program and solved via any LP solver. A popular option is an interior point method. These are second order methods that rely on computing second order information and solving potentially large linear systems at each iteration. In general they are not competitive with subgradient methods on large scale problems. Simplex methods \cite{barrodale1973improved} are another option. While their typical performance is good, these methods have exponential computational complexity in the worst case. The alternating direction method of multipliers (ADMM) is another approach to solving Prob. (\ref{prob_l1_l1}), however it involves solving a quadratic program at each iteration, placing it in the same complexity class as the interior point methods \cite{eckstein2012augmented}. The primal-dual splitting method of \cite{chambolle2011first} is a first-order method which can tackle Prob. (\ref{prob_l1_l1}). 
%On the theoretical side, the primal-dual method is known to obtain an $O(1/k)$ convergence rate of objective function values. However in our experiments on Prob. (\ref{prob_l1_l1}) it appears to obtain linear convergence. 
The main drawback of the method is that one must know the largest singular value of $E$ in order to choose the stepsizes correctly. As such, it is not directly comparable with the subgradient methods developed in this manuscript which do not require this information. The paper \cite{wang2006regularized} introduces a method for solving Prob. (\ref{prob_l1_l1}) which is similar to the LARS method for solving the LASSO \cite{efron2004least}. The method solves Prob. (\ref{prob_l1_l1}) for an increasing sequence of $\tau$. At every iteration it solves a linear system, using the previous solution in a smart way. However, as far as we are aware, the iteration complexity of this method is unknown. Edgeworth's algorithm is a coordinate descent method for Prob. (\ref{prob_l1_l1}) which has shown promising empirical performance \cite{wu2008coordinate}. However unlike the subgradient methods considered here, the method is not guaranteed to converge to a minimizer. In fact specific examples exist where Edgeworth's algorithm converges to a non-optimal point \cite{li2004maximum}. %While it may be possible to develop line-search variants, we will not pursue that here. 
%In this experiment we consider first-order approaches to solving  can be seen as a more scalable alternative to interior point methods such as \cite{portnoy1997gaussian}. 

Problem (\ref{prob_l1_l1}) is a polyhedral optimization problem therefore HEB($c,\theta$) is satisfied for all $x$ with $\theta=1$ \cite{yang2015rsg}. However, it is not easy to compute $c$. Note that the constraint set is compact thus DS2-SG is applicable. Projection onto the $\ell_1$ ball can be done in linear time in expectation via the method of \cite{duchi2008efficient}. 

To test the subgradient methods we first consider a random instance of Problem (\ref{prob_l1_l1}). We set $m=100$ and $n=50$ and construct $E$ of size $m\times n$ with i.i.d. $\mathcal{N}(0,1)$ entries. We construct $b$ of size $m\times 1$ with  i.i.d. $\mathcal{N}(0,1)$ entries. We set $\tau=1$. All tested algorithms were initialized to the same point.

To start we test the convergence rates predicted by Theorem \ref{ThmLargeTheta} for decaying stepsizes. We consider two stepsizes $\alpha_k = 0.1 k^{-0.99}$, and $\alpha_k = 0.01 k^{-0.5}$, where the constants were tuned to achieve good performance. In Fig. \ref{fig_decay} we plot the log of $d(x_k,\calX_h)^2$ versus $\log_{10} k$, where $k$ is the number of iterations. An optimal solution $x^*$ is estimated by running DS-SG until it converges to within numerical precision. Looking at the figure it appears that for $k>1000$ the convergence rates are as predicted in Theorem \ref{ThmLargeTheta}. Specifically for the first parameter choice, $d(x_k,\calX_h)^2 \approx O(k^{-1.98})$ and for the second $d(x_k,\calX_h)^2 \approx O(k^{-1})$.

Next we test the performance of DS-SG, RSG \cite{yang2015rsg}, and Shor's method of \cite[Sec. 2.3]{shor2012minimization} (which is very similar to Goffin's stepsize \cite{goffin1977convergence}), alongside the two decaying stepsizes discussed in Fig. \ref{fig_decay}.  For DS-SG we used $\beta_{ds}=4$, $\epsilon=10^{-5}$, $\Omega_{\calC}=4\tau^2$, and $G=\sum_{i=1}^n\|E_i\|$ where $E_i$ is the $i$th column of $E$. For the other methods we chose the parameters in the way suggested by the authors.
 Since $c$ is difficult to estimate, we tuned it to get the best performance in each algorithm (see below for our approach, DS2-SG, which does not need $c$). For DS-SG, RSG, and Shor's algorithm, these were $c=22,15$, and $11$ respectively. 

The log of $d(x_k,\calX_h)^2$ for each of these algorithms is plotted in Fig. \ref{fig_compAll} versus the number $k$ of subgradient evaluations.
Fig. \ref{fig_compAll} confirms that DS-SG has a linear convergence rate, verifying Theorem \ref{thmRestart}. It's performance is very similar to Shor's method. While RSG does appear to obtain linear convergence, it's rate is slower than DS-SG and Shor's method. % Also observe that for the first $15000$ iterations, the diminishing stepsize with $\alpha_k=O(k^{-1})$ is the best performing method. This is because the three linearly convergent methods are all highly sensitive to the condition number $G/c$, which can be large. This suggests that decaying stepsize rules can still play a role on highly ill-conditioned polyhedral optimization problems. 
%The figure confirms that DS-SG obtains a linear convergence rate. However the performance is even better than predicted by our theory. For  $\epsilon=10^{-12}$ the number of iterations to guarantee $d(x_k,\calX_h)^2\leq\epsilon$ is $M\cdot K \approx 55000$. However we see from the plot that this tolerance is reached after less than half this many iterations. We suspect that this is because for the early iterations the error bound constant is very pessimistic, and a larger $c$ is valid when far from the solution. This allows the algorithm to make much more progress in the early iterations than is expected. 

\begin{figure}
\centering
\includegraphics[width=3in]{DistNonSum.eps}
% where an .eps filename suffix will be assumed under latex, 
% and a .pdf suffix will be assumed for pdflatex; or what has been declared
% via \DeclareGraphicsExtensions.
\caption{Problem (\ref{prob_l1_l1}): Log of square distance to the (unique) solution vs log of number of subgradient evaluations for two decaying stepsizes.}
\label{fig_decay}
\end{figure}

\begin{figure}
\centering
%\includegraphics[width=3in]{dist_no\alpha_1dapt.eps}
\includegraphics[width=3in]{dist_comparisonAll.eps}

% where an .eps filename suffix will be assumed under latex, 
% and a .pdf suffix will be assumed for pdflatex; or what has been declared
% via \DeclareGraphicsExtensions.
\caption{Problem (\ref{prob_l1_l1}): Log of square distance to the (unique) solution vs number of subgradient evaluations for DS-SG, RSG, and two decaying stepsizes.}
\label{fig_compAll}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=3in]{FuncAdapt.eps}
% where an .eps filename suffix will be assumed under latex, 
% and a .pdf suffix will be assumed for pdflatex; or what has been declared
% via \DeclareGraphicsExtensions.
\caption{Problem (\ref{prob_l1_l1}): Log of $h(x)-h^*$  vs number of subgradient evaluations for DS-SG, RSG, and Shor's method all with $c=100$, R\textsuperscript{2}SG, DS2-SG with the initial $c_1=G=160$, and two decaying stepsizes.}
\label{fig_ad}
\end{figure}

As was mentioned we had to tune $c$ to get good performance of DS-SG, RSG, and Shor's method. We now compare these three methods with our proposed 'doubling trick' variant DS2-SG, which does not need the value of $c$. We also compare with the method R\textsuperscript{2}SG proposed in \cite{yang2015rsg}. Note that this method only works for $\theta<1$ so following the advice of \cite{yang2015rsg}, we use the approximate value of $\hat{\theta}=0.8$, which was chosen because it performed well.  We initialize DS2-SG with the same parameters as DS-SG but with $c_1 = G = 160$. To demonstrate the effect of poorly chosen $c$ in DS-SG, RSG, and Shor's method, we set $c=100$ for all these methods (recall the tuned values were smaller). The results are given in Fig. \ref{fig_ad}. We compare function values and for each algorithm we keep track of the iterate with the smallest function value so far. We see that DS-SG, RSG, and Shor's method converge to suboptimal solutions due to the incorrect value of $c$. However DS2-SG finds the correct solution to within an objective function error of $10^{-10}$. R\textsuperscript{2}SG has slower convergence, which is not surprising since it is not guaranteed to obtain linear convergence when $\theta=1$. It is also encouraging that DS2-SG is faster than the decaying stepsizes $\alpha_k=O( k^{-1})$ and $\alpha_k=O(k^{-0.5})$, since this choice also does not require knowledge of $c$.

\subsection{Least-Absolute Deviations Regression on the ``space.ga" Dataset}

We also apply Prob. (\ref{prob_l1_l1}) on a real dataset. We use the normalized space.ga dataset downloaded from the \emph{libsvm} website.\footnote{\url{https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/}.} We use a subset of the dataset with $m=100$ and $n=6$, and set $\tau=5$.

Since $c$ is unknown, we compare subgradient methods which do not require it. Thus we compare two decaying stepsizes, $\alpha_k= k^{-1}$ and $\alpha_k=0.1 k^{-0.5}$, and DS2-SG. Note that  R\textsuperscript{2}SG also does not require $c$ but we could not tune it to be competitive on this problem. For DS2-SG, we estimate $G=\sum_{i=1}^n \|E_i\|$ and $\Omega_{\calC}=4\tau^2$ as in the synthetic experiment. We use $\beta_{ds}=2$ and $\epsilon=10^{-12}$. The objective function vs iteration-number is plotted in Fig. \ref{fig_realRegress}. One can see that the decaying stepsizes are faster than DS2-SG in the early iterations but DS2-SG is much faster in the later iterations. The decaying stepsizes were highly sensitive to the choice of $\alpha_1$ which had to be tuned. On the other hand DS-SG was effected by the choice of $\beta_{ds}$. Smaller values of $\beta_{ds}$ lead to better performance early-on, while larger values give better convergence in the latter iterations. In general $\beta_{ds}\in[1.5,4]$ worked well in all of our experiments. 

\begin{figure}
\centering
\includegraphics[width=3in]{RealRegression.eps}
% where an .eps filename suffix will be assumed under latex, 
% and a .pdf suffix will be assumed for pdflatex; or what has been declared
% via \DeclareGraphicsExtensions.
\caption{Problem (\ref{prob_l1_l1}) applied to space.ga dataset: Log of $h(x)-h^*$  vs number of subgradient evaluations for DS-SG, $\alpha_k=k^{-1}$, and $\alpha_k=0.1 k^{-0.5}$.}
\label{fig_realRegress}
\end{figure}

\subsection{Sparse SVM}
The $\ell_1$-regularized Support Vector Machine (SVM) Problem \cite{zhu20031} is
\begin{eqnarray*}
	\min_{x\in\mathbb{R}^n}\sum_{i=1}^m \max\{0,1- y_i c_i^\top x\}  +\rho\|x\|_1
\end{eqnarray*}
for a dataset $\{c_i,y_i\}_{i=1}^m$ with $c_i\in\mathbb{R}^n$ and $y_i\in\{\pm 1\}$. We will consider the equivalent constrained version
\begin{eqnarray}\label{probSVM}
\min_{x\in\mathbb{R}^n}\sum_{i=1}^m \max\{0,1- y_i c_i^\top x\} : \|x\|_1\leq\tau.
\end{eqnarray}
Since the objective function is polyhedral it satisfies HEB with $\theta=1$ for some unknown $c>0$. Once again since $c$ is unknown, we only consider DS2-SG, R\textsuperscript{2}SG \cite{yang2015rsg}, and the following decaying stepsizes: $\alpha_k=0.1 k^{-1}$ and $\alpha_k=0.01 k^{-0.5}$, where the constants $0.1$ and $0.01$ were tuned to give fast convergence. R\textsuperscript{2}SG only works for $\theta<1$ so cannot be directly applied to this problem. Instead we selected $\hat{\theta}<1$ which gave the fastest convergence. Surprisingly, $\hat{\theta}=0.5$ performed the best even though one might expect $\hat{\theta}\approx 1$ to perform better. For DS2-SG we initialize with $c_1=G$ where $G=\sum_{i=1}^m\|c_i\|$. We used $\beta_{ds}=2$, $\epsilon=10^{-5}$, and $\Omega_{\calC}=4\tau^2$. All four algorithms had the same starting point. 

A random instance of Prob. (\ref{probSVM}) was generated as follows: $n=50$, $m=100$, the entries of $c_i$ are drawn from $\mathcal{N}(0,1)$, the $y_i=\pm 1$ with equal probability, and $\tau=2$. The results are plotted in Fig. \ref{figSVM}. We see that our proposal, DS2-SG, outperforms the others. 

\subsection{Sparse SVM on the ``glass.scale" Dataset}
To test Prob. (\ref{probSVM}) on real data, we download the \emph{glass.scale} dataset from the libsvm website. For this dataset, $n=9$ and $m=214$. There are $6$ different labels so we group labels ``1", ``2", and ``3" together into class: $y=-1$, and labels ``5", ``6", and ``7" into class: $y=1$. We solve Prob. (\ref{probSVM})  with $\tau=2$. 

Again we compare the subgradient methods which do not require $c$, namely DS2-SG, R\textsuperscript{2}SG, and two decaying stepsizes. For DS2-SG we use the same parameters as in the synthetic experiment, except $\beta_{ds}=4$ and $\epsilon=10^{-8}$. The  objective function vs iteration-number is plotted in Figure \ref{fig_realSVM}. Once again we see that DS2-SG outperforms the two decaying stepsizes as well as R\textsuperscript{2}SG. 


\begin{figure}[h!]
\centering
\includegraphics[width=3in]{SVM.eps}
% where an .eps filename suffix will be assumed under latex, 
% and a .pdf suffix will be assumed for pdflatex; or what has been declared
% via \DeclareGraphicsExtensions.
\caption{Problem (\ref{probSVM}) with randomly generated data: Log of $h(x)-h^*$  vs number of subgradient evaluations for DS2-SG,   R\textsuperscript{2}SG, and two decaying stepsizes.}
\label{figSVM}
\end{figure}

\begin{figure}[h!]
\centering
\includegraphics[width=3in]{realSVM.eps}
% where an .eps filename suffix will be assumed under latex, 
% and a .pdf suffix will be assumed for pdflatex; or what has been declared
% via \DeclareGraphicsExtensions.
\caption{Problem (\ref{probSVM}) for the ``glass.scale" dataset: Log of $h(x)-h^*$  vs number of subgradient evaluations for DS2-SG,   R\textsuperscript{2}SG, and two decaying stepsizes.}
\label{fig_realSVM}
\end{figure}

