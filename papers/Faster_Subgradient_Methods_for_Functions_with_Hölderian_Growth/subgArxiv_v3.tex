% This is a sample LaTeX file for a JOTA paper. A sample figure file (Fig_1.pdf) is required to typeset this file.
%A standard way of writing LaTeX files is to give everything a label: sections, formulas, figures, references, etc. Labeling makes it easy to modify a LaTeX file, but it is often difficult to create and remember the labels. Labeling is not used in this example.

%If in CSL
%\documentclass[smallextended]{C:/Users/prjohns2.UOFI/Dropbox/ResearchAndwork_constant_folder/LatexFiles/Springer/svjour3} 
%\newcommand{\PathDropbox}{C:/Users/prjohns2.UOFI/Dropbox}

%If on Laptop
\documentclass[smallextended]{svjour3} 
%\newcommand{\PathDropbox}{C:/Users/Pat}

\usepackage{etoolbox}
\newtoggle{in_office}

\toggletrue{in_office}
\togglefalse{in_office}


% Use in laptop

\usepackage{defs}
\usepackage{prjohns2MacrosTA}



\allowdisplaybreaks
\newcommand{\calN}{\mathcal{N}}

% The option smallextended is the standard JOTA format
% The option referee  makes the paper double-spaced.
% The option envcountsect numbers theorems, etc, by section.
% svjour3 is the document class for Springer journals.  
\smartqed 
%This command right justifies \qed throughout the paper. 

\usepackage{url}
%This package is used to insert figures.
%\journalname{COAP}
\usepackage{amssymb}
\usepackage{mathtools}
%\usepackage{algorithm}
%\usepackage{algorithmic}
\usepackage[ruled,lined]{algorithm2e}
\usepackage{amsmath}
\usepackage{enumerate}\usepackage{epstopdf}

\usepackage{fixltx2e}
%corresponding author symbol
\usepackage{marvosym}
\newcommand{\envelope}{(\raisebox{-.5pt}{\scalebox{1.45}{\Letter}}\kern-1.7pt \hspace{0.7mm})}

%
%\newcommand{\calP}{\mathcal{P}}
%\newcommand{\calQ}{\mathcal{Q}}
%\newcommand{\dom}{\text{dom}\,\,}
\newcommand*\samethanks[1][\value{footnote}]{\footnotemark[#1]}
%\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}
%\usepackage{refcheck}
\newenvironment{assumption1}[1][Assumption ${\mathbf 1}$]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}

\newenvironment{assumption2}[1][Assumption ${\mathbf 2}$]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}

\newtheorem{assump}{Assumption}
%\newcommand*\samethanks[1][\value{footnote}]{\footnotemark[#1]}
%\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}
%\usepackage{refcheck}



\begin{document}

%All acknowledgements should be placed in the back of the paper before References.

\title{Faster Subgradient Methods for Functions with H\"olderian Growth}

%\subtitle{Using  the  LaTex Template}

%\author{Patrick R. Johnstone\footnote{Department of Management Science and Information Systems, Rutgers Business School Newark and New Brunwick, Rutgers University  patrick.r.johnstone gmail.com} \and  Pierre Moulin\footnote{ Coordinated Science Laboratory, University of Illinois,Urbana, IL 61801, USA}}

\author{Patrick R. Johnstone\and  Pierre Moulin}

\institute{
Patrick R. Johnstone \at 
Department of Management Sciences and Information Systems, Rutgers Business School Newark and New Brunswick, Rutgers University\\
\email{patrick.r.johnstone@gmail.com}
\and 
Pierre Moulin \at 
Coordinated Science Laboratory, University of Illinois,Urbana, IL 61801, USA\\
\email{pmoulin@illinois.edu} 
}



\date{Received: date / Accepted: date}
% The correct dates will be entered by the editor


\maketitle
\begin{abstract}
The purpose of this manuscript is to derive new convergence results for several subgradient methods applied to minimizing nonsmooth convex functions with H\"olderian growth. The growth condition is satisfied in many applications and includes functions with quadratic growth and weakly sharp minima as special cases. To this end there are three main contributions. First, for a constant and sufficiently small stepsize, we show that the subgradient method achieves linear convergence up to a certain region including the optimal set, with error of the order of the stepsize. Second, if appropriate problem parameters are known, we derive a decaying stepsize which obtains a much faster convergence rate than is suggested by the classical $O(1/\sqrt{k})$ result for the subgradient method. Thirdly we develop a novel ``descending stairs" stepsize which obtains this faster convergence rate and also obtains linear convergence for the special case of weakly sharp functions.
%While this was previously known for smooth convex functions satisfying the Polyak-Lojasiewicz inequality, we extend it to the more general KL inequality and do not require smoothness. 
%Unlike most of the existing literature on subgradient methods, these convergence rates refer to the distance to the solution set rather than the objective function values. 
We also develop an adaptive variant of the ``descending stairs" stepsize which achieves the same convergence rate without requiring an error bound constant which is difficult to estimate in practice.
%As a consequence of our analysis we show that when the KL exponent is less than or equal to $1/2$ restarting is unnecessary as a particular fixed stepsize achieves the same convergence rate as a restarted method. 
%With the aid of simulations, we show that using decaying but nonsummable stepsizes is more robust to uncertainty in the problem parameters than this restart method. 
%We also derive new convergence rates for stochastic problems. 

%\keywords{proximal gradient methods \and forward-backward splitting \and inertial methods \and FISTA \and $\ell_1$-regularization \and local linear convergence}
%\subclass{65K05\and 65K10 \and 90C06 \and 90C25}
\end{abstract}

\section{Introduction}
\subsection{Motivation and Background}
In this manuscript we consider the following problem:
\begin{eqnarray}\label{Prob1}
\min_{x\in\calC} h(x),
\end{eqnarray}
where $\calC$ is a convex, closed, and nonempty subset of a real Hilbert space $\calH$, and $h:\calH\to\mathbb{R}$ is a convex and closed function. We do not assume $h$ is smooth or strongly convex. Problem (\ref{Prob1}) arises in many applications such as image processing, machine learning, compressed sensing, statistics, and computer vision \cite{hastie2009elements,agrb1992maximum,yang2015rsg}.

We focus on the class of \emph{subgradient methods} for solving this problem, which were first studied in the 1960s \cite{shor2012minimization,goffin1977convergence}. Since then, these methods have been used extensively because of their simplicity and low per-iteration complexity \cite{shor2012minimization,goffin1977convergence,rosenberg1988geometrically,nedic2010effect,nedic2001convergence,nemirovski2009robust}. Such methods only evaluate a subgradient of the function at each iteration. 
However in general they have a slow worst-case convergence rate of $h(\hat{x}_k)-\min_{x\in\calC}h(x)\leq O(1/\sqrt{k})$ after $k$ subgradient evaluations for a particular averaged point $\hat{x}_k$. In this manuscript we show how a structural assumption for Problem (\ref{Prob1}) that is commonly satisfied in practice yields faster subgradient methods.

The structural assumption we consider is the \emph{H\"older error bound} (throughout referred to as either HEB, HEB$(c,\theta)$, or H\"olderian growth). We assume that $h$ satisfies
\begin{eqnarray*}\label{errorBound}
\hspace{3cm}h(x)-h^*\geq c d(x,\calX_h)^{\frac{1}{\theta}},\quad\forall x\in\mathcal{C},\hspace{2.5cm}(\text{HEB})
\end{eqnarray*}
where
\begin{itemize}
	\item $\theta\in(0,1]$ is the ``error bound exponent",
	\item $c>0$ is the ``error bound constant",
	\item $h^*=\min_{x\in\calC}h(x)$ is the optimal value,
\item 
$\calX_h\triangleq\{x\in\calC:h(x)=h^*\}$ is the solution set (assumed to be nonempty), and
\item 
 $d(x,\calX_h)=\inf_{x^*\in\calX_h}\|x-x^*\|$. 
\end{itemize}
In general, an ``error bound" is an upper bound on the distance of a point to the optimal set by some residual function. The study of error bounds has a long tradition in optimization, sensitivity analysis, systems of inequalities, projection methods, and convergence rate estimation \cite{li2013global,tseng2010approximation,zhou2015unified,xu2016accelerate,bolte2015error,burke1993weak,zhang2013gradient,pang1997error,luo1993error,ferris1991finite,burke2002weak,karimi2016linear,beck2015linearly} In recent years there has been much renewed interest in the topic.  HEB is often referred to as the \emph{{\L}ojaziewicz error bound} \cite{bolte2007lojasiewicz} and is also related to the \emph{Kurdyka--\L ojaziewicz (KL) inequality} \cite{bolte2015error}. In fact in \cite{bolte2015error} it was shown that the KL inequality is equivalent to HEB for convex, closed, and proper functions.


There are three main motivations for studying the behavior of algorithms for problems satisfying  HEB. Firstly HEB holds for problems arising in many applications. In fact for a semialgebraic function, HEB is guaranteed to hold on a compact set for some $\theta$ and $c$ \cite{bolte2015error}. Secondly, many algorithms have been shown to achieve significantly faster convergence behavior when HEB is satisfied. Thirdly, under HEB it has been possible to develop even faster methods. 

The two most common instances of HEB in practice are $\theta=1/2$ and $\theta=1$. The $\theta=1/2$ case is often referred to as the \emph{quadratic growth condition} (QG) \cite{karimi2016linear}. The $\theta=1$ case is often referred to by saying the function has \emph{weakly sharp minima} (WS) \cite{burke2002weak}. The function itself may also be called a weakly sharp function. There are also a small number of applications where $\theta\neq 1/2$ or $1$, such as $L_d$ regression with $d\neq 1,2$ \cite{agrb1992maximum}. 
 
Due to its prevalence in applications, many recent papers have studied QG (the $\theta=1/2$ case). QG has been used to show a \emph{linear} convergence rate of the objective function values for various algorithms, such as the proximal gradient method, that would otherwise only guarantee sublinear convergence \cite{zhang2016new,beck2015linearly,zhou2015unified,karimi2016linear}. 
Many papers have discovered connections between QG and other error bounds and conditions known in the literature. Most importantly it was shown in \cite[Appendix A]{karimi2016linear} that for convex functions, QG is equivalent to the \emph{Luo-Tseng} error bound \cite{luo1993error}, the \emph{Polyak-{\L}ojaziewicz} condition \cite{karimi2016linear}, and the \emph{restricted secant inequality} \cite{zhang2013gradient}. 

Weakly sharp functions (i.e. HEB with $\theta=1$) have been studied in many papers, for example \cite{burke2002weak,ferris1991finite,pang1997error,shor2012minimization,nedic2010effect,poljak1978nonlinear,yang2015rsg,supittayapornpong2016staggered,attouch2013convergence}. For such functions \cite{ferris1991finite} showed that the proximal point method converges to a minimum in a \emph{finite} number of iterations. This is interesting because this method would otherwise only have an $O(1/k)$ rate. %Descent methods, which guarantee a decrease in the objective at each iteration, have been studied extensively under the KL inequality in many papers such as \cite{noll2014convergence,kurdyka1998gradients,li2016calculus,frankel2014splitting,Johnstone_ConvKL}.  The presence of the KL inequality implies much faster convergence rates for descent type methods than the usual $O(1/k)$. For example, finite convergence of descent type methods under WS is shown in \cite{attouch2013convergence,frankel2014splitting}.



\subsection{Our Contributions}
Recall the definition of the subgradient of $h$ at $x$ \cite[Def. 16.1]{bauschke2011convex}:
\begin{eqnarray*}
\partial h(x) \triangleq \{g\in\mathcal{H}:h(y)\geq h(x)+\langle g,y-x\rangle, \forall y\in\mathcal{H}\}.
\label{eq:ineq3}
\end{eqnarray*}
Define the \emph{subgradient method} as 
\begin{eqnarray}\label{iterSG}
 x_{k+1}=P_{\calC}(x_k-\alpha_k g_k):\quad\forall k\geq 1, g_k\in\partial h(x_k),\,\, x_1\in \calC,
 \end{eqnarray}
where $P_{\calC}$ denotes the projection onto $\calC$ and the choice of the \emph{stepsize} $\alpha_k>0$ is left unspecified. 
 Despite the long history of analysis of subgradient methods, the simplest stepsize choices for (\ref{iterSG}) have not been studied for objective functions satisfying HEB. These are the constant stepsize, $\alpha_k=\alpha$, and the decaying stepsize, $\alpha_k=\alpha_1 k^{-p}$ for $p>0$. This brings us to our contributions in this manuscript. 

Firstly we determine the convergence rate of a constant stepsize choice which previously had only been determined for the special case of $\theta=1/2$ (see \cite[Prop. 2.4]{nedic2001convergence}). Interestingly, \emph{for any} $\theta\in(0,1]$ the method obtains a linear convergence rate for $d(x_k,\calX_h)$, up to a specific tolerance level of order $O(\alpha^{\theta})$. %This result is encouraging since the constant stepsize choice is often used as a heuristic in many applications.

%Secondly, we determine the convergence rate of both summable and nonsummable decaying polynomial stepsize choices. 
%For a correctly chosen $p\in(0,1)$, we determine the convergence rate to be
%\begin{eqnarray}\label{ConvRate1}
%d(x_k,\calX_h)=O(k^{\frac{\theta}{2(\theta-1)}}):\quad 0<\theta\leq \frac{1}{2}
%\end{eqnarray}
%and
%\begin{eqnarray}\label{ConvRate2}
% %\end{eqnarray}
%Previously, these results had only been obtained for the case where $\theta=1/2$. For $\theta=1$ the paper \cite{lim2011convergence} obtains an asymptotic convergence rate  for $p=1$ with an additional global QG assumption. The big advantage of the nonsummable stepsizes is that, for $\theta\geq 1/2$, they require no information about the problem's parameters in order to guarantee convergence. 
Secondly, we derive decaying stepsizes which obtain much faster rates than the classical subgradient method if appropriate problem parameters are available.
The classical analysis of the subgradient method leads to the rate
\begin{eqnarray*}
h(\hat{x}_k)-h^*\leq O(k^{-\frac{1}{2}}),
\end{eqnarray*}
where $\hat{x}_k$ is a specific average of the previous iterates and $\alpha_k=O(1/\sqrt{k})$ \cite{nemirovski2009robust}. Combining this with HEB yields
\begin{eqnarray*}\label{ConvClassic}
d(\hat{x}_k,\calX_h)\leq O(k^{-\frac{\theta}{2}}).
\end{eqnarray*}
This rate is slower than the result of our specialized analysis. We show that with stepsize $\alpha_k=\alpha_1 k^{-p}$ and the proper choice of $p$ and $\alpha_1$, the subgradient method can obtain the convergence rate
\begin{eqnarray}\label{optDecay}
d(x_k,\calX_h)\leq O(k^{-\frac{\theta}{2(1-\theta)}}),\quad\forall \theta<1.
\end{eqnarray}
It can be seen that the absolute value of the exponent is a factor $1/(1-\theta)$ larger in our analysis. 
%If the problem parameters are not known, we show that the decaying stepsize $\alpha_k=\alpha_1 k^{-p}$ for \emph{any} $p\in(0,1)$ and $\alpha_1>0$ obtains the rate $O(k^{-2 p \theta})$ for all $\theta\in(0,1]$. While this is a slower rate than (\ref{optDecay}), this choice is robust to uncertainty about the problem parameters. 

Our third major contribution is  a new ``descending stairs" stepsize choice for the subgradient method (DS-SG). The method achieves the convergence rate given in (\ref{optDecay}) for $\theta<1$. In addition, for the case $\theta=1$ it achieves linear convergence. Unlike the methods of \cite{renegar2015framework,renegar2016efficient} and \cite[Exercise 6.3.3]{bertsekas1999nonlinear}, which also obtain linear convergence when $\theta=1$, our proposal does not require knowledge of $h^*$. The methods of \cite{supittayapornpong2016staggered,shor2012minimization,goffin1977convergence} have a similar complexity for $\theta=1$ but cannot handle $\theta<1$. The Restarted Subgradient method (RSG) \cite{yang2015rsg} obtains the same iteration complexity but requires averaging which is disadvantageous in applications where the solution is sparse (or low rank) because it can spoil this property \cite{davis2017three}. (In Section \ref{Discuss} we discuss other problems with averaging.) An advantage of RSG is it only requires that HEB be satisfied locally, i.e. on a sufficiently-large level set of $h$. However in the important case where $\theta=1$ this makes no difference, because if HEB holds with $\theta=1$ on any compact set, then it holds globally \cite{burke1993weak}. Furthermore for many applications with $\theta<1$, HEB is satisfied globally \cite{bolte2015error,karimi2016linear}.   %Our method retains the same iteration complexity even when the subgradients are corrupted, provided the noise is small relative to the sharpness constant $c$. %Shor's method of \cite{shor2012minimization} (see also the similar method of \cite{goffin1977convergence}) also obtains linear convergence in the $\theta=1$ case but ca.

%We also consider summable decaying stepsizes. We show that, when the constraint set is bounded and one has access to suitable problem parameters such as an upper bound on the initial distance to the solution set and a lower bound on $c$, these stepsizes also obtain the iteration complexity $O(\epsilon^{1-\frac{1}{\theta}})$ when $\theta<1$.% As far as we are aware

DS-SG, RSG, and several other methods \cite{goffin1977convergence,shor2012minimization} require knowledge of the constant $c$ in HEB which can be hard to estimate in practice. This motivates us to develop our final major contribution: a ``doubling trick" for DS-SG which automatically adapts to the unknown error bound constant and still obtains the same iteration complexity, up to a small constant. We call this method  the ``doubling trick descending stairs subgradient method" (DS2-SG).  The competing methods of \cite{yang2015rsg,supittayapornpong2016staggered,shor2012minimization,goffin1977convergence} all require knowledge of $c$. The authors of \cite{yang2015rsg} proposed an adaptive method which does not require $c$, however it only works for $\theta<1$. % While Shor \cite{shor2012minimization} discusses a heuristic for dealing with unknown $c$, no analysis is given. %The method of \cite{supittayapornpong2016staggered} is similar to ours but only covers the case $\theta=1$.  %however the iteration complexity of their method depends linearly on the initial distance $d(x_1,\calX_h)$, whereas our method has an iteration complexity which depends on $\log d(x_1,\calX_h)$. 
 


In summary, our contributions under HEB are as follows: 
\begin{enumerate}
\item We show that the subgradient method with a constant stepsize obtains linear convergence for $d(x_k,\calX_h)$ to within a region of the optimal set for all $\theta\in(0,1]$. 
%\item We find a constant stepsize depending on $c$ and $G$ for finding a point such that $d(x_k,\calX_h)\leq\epsilon$  and give its iteration complexity, which is only logarithmically dependent on the initial distance $d(x_1,\calX_h)$. 
\item We derive a decaying stepsize with faster convergence rate than the classical subgradient method.
\item We develop a new ``Descending Stairs" stepsize with iteration complexity $O(\epsilon^{1-\frac{1}{\theta}})$ when $\theta<1$ and $\ln\frac{1}{\epsilon}$ when $\theta=1$ for finding a point such that $d(x_k,\calX_h)^2\leq\epsilon$. We also develop an adaptive variant which does not need $c$ but retains the same iteration complexity up to a small constant. 
\end{enumerate}
%{ |p{0.75cm}|p{2cm}|p{2.5cm}|p{2cm}|p{2.5cm}|}
 \begin{table}
  	\caption{Summary of our contributions for constant, decaying (polynomial), DS-SG, and DS2-SG stepsizes. The given convergence rates are for $d(x_k,\calX_h)^2$. We list the cases $\theta=1$ and $\theta<1$ seperately. Goffin \cite{goffin1977convergence} developed a geometrically decaying stepsize which obtains geometric convergence rate for the case $\theta=1$ with known $c$ (see also \cite[Sec. 2.3]{shor2012minimization}.}\label{tableux}
  	\begin{tabular}{|c||c|c|c|c|}
  		\hline
    &constant& decaying & DS-SG & DS2-SG\\		
    \hline
  	$\theta=1$& $q^k+O(\alpha^{2\theta})$ & $O(q^k)$, Goffin \cite{goffin1977convergence}&$O(q^k)$&$O(q^k)$, $c$ not required\\[2ex]
  	\hline 
  	$\theta<1$ & $q^k+O(\alpha^{2\theta})$ & $O\left(k^{\frac{\theta}{\theta-1}}\right)$ 
  	&$O\left(k^{\frac{\theta}{\theta-1}}\right)$ & $O\left(k^{\frac{\theta}{\theta-1}}\right)$, $c$ not required
  	\\[2ex]
  	\hline
  	\end{tabular}
  	
  \end{table}
  Our contributions are summarized in Table \ref{tableux}.
  
The outline for the manuscript is as follows. In Sec. \ref{Sec_subgHist} we discuss some previously known results for subgradient methods applied to functions satisfying HEB. In Sec. \ref{secKeyRec} we derive the key recursion which describes the subgradient method under HEB and allows us to obtain convergence rates. In Sec. \ref{sec_const} we determine the behavior of a constant stepsize. In Sec. \ref{sec_itercomp} we derive a constant stepsize with explicit iteration complexity. In Sec. \ref{secRest} we develop our proposed DS-SG. In Sec. \ref{secAdapt} we develop the variant, DS2-SG, which does not require the error bound constant. In Sec. \ref{sec_sum} we derive a decaying stepsize with faster convergence rate than the classical decaying stepsize. In Sec. \ref{Sec_decay}, we derive convergence rates under HEB for some classical, decaying, and nonsummable stepsizes. These results are proved in Sec. \ref{secProofDecay}. Finally, Sec. \ref{sec_numerical} features numerical experiments to test some of the theoretical findings of this paper. 

\section{Prior Work on Subgradient Methods under HEB}\label{Sec_subgHist}
%Over the past three decades many algorithms have been studied under HEB and shown to have improved convergence rates. Until recently \emph{subgradient} methods have mostly escaped attention. However over the past few years several papers have examined ways to improve the convergence behavior of subgradient methods when (\ref{errorBound}) is satisfied. 

There were a few early works that studied the subgradient method under conditions related to HEB with $\theta=1$. In \cite[Thm 2.7, Sec. 2.3]{shor2012minimization}, Shor proposed a geometrically decaying stepsize which obtains a linear convergence rate under a condition equivalent to HEB with $\theta=1$. The stepsize depends on explicit knowledge of the error bound constant $c$, a bound on the subgradients, and the initial distance $d(x_1,\calX_h)$. Goffin \cite{goffin1977convergence} extended the analysis of \cite{shor2012minimization} to a slightly more general notion than HEB.\footnote{Our analysis also holds for Goffin's condition.}  Note that our optimal decaying stepsize, derived in Sec. \ref{sec_sum}, is a natural extension of Goffin's geometrically-decaying stepsize to $\theta<1$.  Rosenburg \cite{rosenberg1988geometrically} extended Goffin's results to constrained problems. In \cite{poljak1978nonlinear}, Polyak showed that Goffin's  method still converges linearly when the subgradients are corrupted by bounded, deterministic noise.

The paper \cite{nedic2010effect} also considers functions satisfying HEB with $\theta=1$ with (deterministically) noisy subgradients. For constant stepsizes, they show convergence of $\lim\inf h(x_k)$ to $h^*$ plus a tolerance level depending on noise. For diminishing stepsizes, they show that $\lim\inf h(x_k)$ actually converges to $h^*$ despite the noise. However \cite{nedic2010effect} does not discuss \emph{convergence rates}, which is the topic of our work.

As mentioned in the introduction, \cite{yang2015rsg} introduced the \emph{restarted subgradient method} (RSG) for when $h$ satisfies HEB. The method implements a predetermined number of averaged subgradient iterations with a constant stepsize and then restarts the averaging and uses a new, smaller stepsize. The authors show that after $O(\epsilon^{2(\theta-1)}\log\frac{1}{\epsilon})$ iterations the method is guaranteed to find a point such that $h(x_k)-h^*\leq\epsilon$. For $\theta=1$ this is a logarithmic iteration complexity. This improves the iteration complexity of the classical subgradient method which is $O(\epsilon^{-2})$. Differences between our results and RSG will be discussed in Sec. \ref{secDD_discuss}. 

The recent paper \cite{xu2016accelerate} extends RSG to stochastic optimization. In particular they provide a similar restart scheme that can also handle stochastic subgradient calls, and guarantees $h(x)-h^*\leq\epsilon$ with high probability. The iteration complexity is the same as for RSG, up to a constant. However, this constant is large leading to a large number of inner iterations, making it potentially difficult to implement the method in practice. 

For WS functions, the paper \cite{supittayapornpong2016staggered} introduced a method similar to RSG except it does not require averaging at the end of each constant stepsize phase. The method also obtains a logarithmic iteration complexity in the $\theta=1$ case.  This method is essentially a special case of our proposed DS-SG for $\theta=1$. 
%The main drawback is that the iteration complexity is proportional to the initial distance $d(x_1,\calX_h)$, rather than $\log d(x_1,\calX_h)$, as in the case of RSG. 

The paper \cite{gilpin2012first} is concerned with a two-person zero-sum game equilibrium problem with a linear payoff structure. The authors show that finding the solution to the equilibrium problem is equivalent to a WS minimization problem. Using this fact, they derive a method based on Nesterov's smoothing technique with logarithmic iteration complexity. This is superior to the $O(1/\epsilon)$ of standard Nesterov smoothing. Connections between our results and \cite{gilpin2012first} are discussed in Section \ref{secDD_discuss}.  

The work \cite{lim2011convergence} studies stochastic subgradient descent under the assumption that the function satisfies WS locally and QG globally. They show a faster convergence rate of the iterates to a minimizer, both in expectation and with high probability, than is known under the classical analysis. 

The work \cite{freund2015new} proposes a new subgradient method for functions satisfying a similar condition to HEB but with $h^*$ replaced by a strict lower bound on $h^*$. Like RSG, this algorithm has a logarithmic dependence on the initial distance to the solution set. However it still obtains an $O(1/\epsilon^2)$ iteration complexity, which is the same as the classical subgradient method.

In \cite{renegar2015framework,renegar2016efficient} Renegar presented a framework for converting a convex conic program to a general convex problem with an affine constraint, to which projected subgradient methods can be applied. He further showed how this can be applied to general convex optimization problems, such as Prob. (\ref{Prob1}), by representing them as a conic problem. For the special case where the objective and constraint set is polyhedral, one of the subgradient methods proposed by Renegar has a logarithmic iteration complexity \cite[Cor. 3.4]{renegar2015framework}. The main drawback of this method is that it requires knowledge of the optimal value, $h^*$. It also requires a point in the interior of the constraint set. Similarly the stepsizes proposed in Thm. 2 of \cite[Sec 5.3.]{PolyakIntro} and \cite[Prop. 2.11]{nedic2001convergence} depend on exact knowledge of $h^*$ and also obtain a logarithmic iteration complexity under WS. 

The work \cite{noll2014convergence} explores subgradient-type algorithms for nonsmooth nonconvex functions satisfying the KL inequality. A procedure was developed for selecting a subgradient at each iteration which results in a decrease in objective value, thereby leading to convergence to a critical point. The selection procedure typically involves either storing a collection of past subgradients and solving a convex program, or suitably backtracking the stepsize until a certain condition is met.

For WS functions, it is known that there are subgradient methods which obtain linear convergence \cite{goffin1977convergence,shor2012minimization,yang2015rsg}. A different assumption, known as partial smoothness, has been used to show \emph{local} linear convergence of proximal gradient methods \cite{hare2004identifying,liang2017activity}. We mention that the partial smoothness property is different to WS: it applies to composite optimization problems with objective: $F = f+h$ where $h$ must be smooth but $f$ may be nonsmooth. Unlike subgradient methods, in proximal gradient methods the nonsmooth part $f$ is addressed via its proximal operator. 

In recent times, convergence analyses for the subgradient method have focused on the objective function rather than the distance of the iterates from the optimal set. However in the early period of development, there were many works focusing on the distance (e.g. \cite{nedic2001convergence,shor2012minimization,poljak1978nonlinear,goffin1977convergence}). The subgradient method is not a descent method with respect to function values, however it is with respect to the distances to the optimal set. Thus the distance is a natural metric to study for the subgradient method.  Furthermore, for some applications, the distance to the solution set arguably matters more than the objective function value. For example in machine learning, the objective function is only a surrogate for the actual objective of interest -- expected prediction error. 

Without further assumptions, \cite[p. 167--168]{PolyakIntro} showed that the convergence rate of the distance of the iterates of the subgradient method to the optimal set can be made arbitrarily slow.
This is true even for smooth convex problems. In this case, gradient descent with a constant stepsize obtains an $O(1/k)$ \emph{objective function} convergence rate, however the iterates can be made to converge arbitrarily slowly to a minimizer. It is our use of HEB which allows us to derive less pessimistic convergence rates for the distance to the optimal set.
 

\section{The Key Recursion}\label{secKeyRec}
 In this section we derive the recursion which describes the evolution of the squared error $d(x_k,\calX_h)^2$ for the iterates of the standard subgradient method under HEB. The same recursion has been derived many times before for the special cases $\theta=\{1/2,1\}$ (e.g. \cite{goffin1977convergence,shor2012minimization,nedic2001convergence}).
\subsection{Assumptions}
\label{secMathDef}
 The optimality condition for Prob. (\ref{Prob1}) can be found in \cite[Prop. 26.5]{bauschke2011convex}. Note that we do not explicitly use this optimality criterion anywhere in our analysis.
For Prob. (\ref{Prob1}), throughout the manuscript we will assume that $\calC\subseteq\dom(\partial h)$, so that for any query point $x\in\calC$ it is possible to find a $g\in\partial h(x)$. If $h$ is convex and closed, the solution set $\calX_h=\{x:h(x)=h^*\}$ is convex and closed \cite{bauschke2011convex}.
Here are the precise assumptions we will use throughout the manuscript.

{\bf Assumption 3.} (Problem (\ref{Prob1})).
Assume $\calC$ is convex, closed, and nonempty. Assume $h$ is convex, closed, and satisfies HEB$(c,\theta)$. Assume $\calX_h$ is nonempty. Assume $\calC\subseteq\dom(\partial h)$. Assume there exists a constant $G$ such that $\|g\|\leq G$ for all $g\in\partial h(x)$ and $x\in\calC$. 

Throughout the manuscript let $\kappa\triangleq G/c$. 
\subsection{The Recursion under HEB}


 \begin{proposition}\label{Prop_keyRecur}
 Suppose Assumption 3 holds. Then for all $k\geq 1$ for the iterates $\{x_k\}$ of (\ref{iterSG})
 \begin{eqnarray}\label{KeyRecursion}
 d(x_{k+1},\calX_h)^2
 &\leq&
  d(x_k,\calX_h)^2
  -2\alpha_k c(d(x_k,\calX_h)^2)^{\frac{1}{2\theta}}
  +\alpha_k^2 G^2.
 \end{eqnarray}
 \end{proposition}
 \begin{proof}
 For the point $x_k$ let $x_k^*$ be the unique projection of $x_k$ onto $\calX_h$.  
 For $k\geq 1$,
 \begin{eqnarray}
 d(x_{k+1},\calX_h)^2
 &=&
 \|x_{k+1}-x_{k+1}^*\|^2
 \nonumber\\
 &\leq&
 \|x_{k+1}-x_k^*\|^2
 \nonumber\\\nonumber
 &\leq&
 d(x_k,\calX_h)^2
 -2\alpha_k\langle g_k,x_k - x_k^*\rangle
 +\alpha_k^2\|g_k\|^2
 \\\nonumber
 &\leq&
 d(x_k,\calX_h)^2
 -2\alpha_k\left(h(x_k)-h^*\right)
 +\alpha_k^2 G^2
 \\\nonumber
 &\leq&
 d(x_k,\calX_h)^2
 -2\alpha_k c(d(x_k,\calX_h)^2)^{\frac{1}{2\theta}}
 +\alpha_k^2 G^2.
 \end{eqnarray}
In the first inequality, we used the fact that $x_{k+1}^*$ is the closest point to $x_{k+1}$ in $\calX_h$. In the second inequality, we used the nonexpansiveness of the projection operator. In the third, we used the convexity of $h$ and in the final inequality we used the error bound. 
 \end{proof}
 
Let $e_k \triangleq d(x_k,\calX_h)^2$ and $\gamma=\frac{1}{2\theta}\in[\frac{1}{2},+\infty)$ then for all $k\geq 1$
 \begin{eqnarray}\label{ABiggy}
 0\leq e_{k+1}\leq e_k - 2\alpha_k c e_k^{\gamma} +\alpha_k^2 G^2.
 \end{eqnarray}
 The main effort of our analysis is in deriving convergence rates for this recursion for various stepsizes.
 
 We note that the key recursion (\ref{KeyRecursion}) can also be derived with different constants in the following extensions: 
 \begin{enumerate}
 	\item For $\theta=1$ a small (relative to $c$) amount of deterministic noise can be added to the subgradient  \cite{nedic2010effect}, 
 	\item 
 	A more general condition than HEB (with $\theta=1$), used in \cite{goffin1977convergence}, can be considered,
 	 	\item Instead of \eqref{iterSG} one can consider
 	the \emph{incremental} subgradient method \cite{nedic2001convergence}, 
 the \emph{proximal} subgradient method \cite{cruz2017proximal},
\begin{align*}
  x_{k+1}= \prox_{\alpha_k f}(x_k-\alpha_k g_k):\quad\forall k\geq 1, g_k\in\partial h(x_k),\,\, x_1\in \dom(\partial h),
\end{align*}
 for minimizing $F(x) = f(x)+h(x)$, so long as the composite function $F$ satisfies HEB and $\dom(\partial h)\subseteq\dom(f)$,
 or the \emph{relaxed} projected subgradient method:
  \begin{align*}
 x_{k+1}=(1-\theta_k)x_k + \theta_kP_\calC(x_k-\alpha_k g_k):\quad\forall k\geq 1, g_k\in\partial h(x_k),\,\, x_1\in \calC,
 \end{align*} 
 so long as $0<\underline{\theta}\leq\theta_k\leq 1$. 
  \end{enumerate}
 \label{secMy}
 
 Extensions 1-2 are discussed in more detail in Sec. \ref{secExtend}.
 
 \section{Constant Stepsize}\label{sec_const}
 Consider the projected subgradient method with \emph{constant}, or fixed, stepsize $\alpha$ given in Algorithm FixedSG.
  \begin{algorithm}
  \caption{(FixedSG)}\label{fix}
  \begin{algorithmic}[1]
  \REQUIRE $K>0$, $\alpha>0$, $x_1\in\calC$
  \FOR{$k=1,2,\ldots,K$}
           \STATE $x_{k+1}= P_{\calC}\left(x_k-\alpha g_k\right):\quad g_k\in\partial h(x_k)$
       \ENDFOR 
       \RETURN $x_{k+1}$
  \end{algorithmic}
  \end{algorithm}
% for $k\geq 1$
% \begin{eqnarray}\label{fix}
% x_{k+1}=P_{\mathcal{C}}\left(x_k-\alpha g_k\right):\quad g_k\in\partial h(x_k),\quad %x_1\in\calC
% \end{eqnarray}
% where $\alpha>0$.
 Previously it was known that if $\theta=1/2$ then this method achieves linear convergence to within a region of the solution set \cite{nedic2001convergence,karimi2016linear}. We show in the next theorem that linear convergence to within a certain region of $\calX_h$ occurs for any $\theta\in(0,1]$ provided $\alpha$ is sufficiently small.
 
 \begin{theorem}\label{ThmFix}
Suppose Assumption 3 holds. Let $e_* = \left(\frac{\alpha G^2}{2c}\right)^{2\theta}$. 
 \begin{enumerate}
 \item For all $k\geq 1$  the iterates of FixedSG satisfy
 \begin{eqnarray}\label{eqBounded}
 d(x_k,\calX_h)^2\leq \max\left\{d(x_1,\calX_h)^2,e_* + \alpha^2 G^2\right\}.
 \end{eqnarray}
 \item If $0<\theta\leq \frac{1}{2}$ 
 and
 \begin{eqnarray}\label{fixed0}
 0<\alpha 
 \leq 
 2^{\frac{1-2\theta}{2(1-\theta)}}\theta^{\frac{1}{2(1-\theta)}}
 G^{\frac{2\theta-1}{1-\theta}}
 c^{\frac{\theta}{\theta-1}}
 %\leq 
 %\left(\frac{2^{\gamma-1} G^{2(1-\gamma)}}{\gamma c}\right)^{\frac{1}{2\gamma-1}}
 \end{eqnarray}
then
 for all $k\geq 1$ the iterates of FixedSG satisfy
 \begin{eqnarray}\label{fixed1}
d(x_k,\calX_h)^2 - e_*\leq q_1^{k-1}
(d(x_1,\calX_h)^2-e_*)
 \end{eqnarray}
 where
 \begin{eqnarray}\label{fixed2}
 q_1=\left(1-\frac{1}{\theta}\alpha c e_*^{\frac{1-2\theta}{2\theta}}\right)\in[0,1).
 \end{eqnarray}

 \item If  $\frac{1}{2}\leq\theta\leq 1$, suppose there exists $D\geq 0$ s.t.  $d(x_k,\calX_h)^2\leq D$ for all $k$, and the stepsize is chosen s.t.
 \begin{eqnarray}\label{linConv}
 0<\alpha\leq  \frac{\theta D^{1-\frac{1}{2\theta}}}{c},
 \end{eqnarray}
 then for all $k\geq 1$ the iterates of FixedSG satisfy
 \begin{eqnarray}\label{ghh}
 d(x_k,\calX_h)^2 - e_*\leq \max\{q_2^{k-1}(d(x_1,\calX_h)^2-e_*),\alpha^2 G^2\},
 \end{eqnarray}
 where
 $$
 q_2=
1-\frac{\alpha c D^{\frac{1}{2\theta}-1}}{\theta}\in[0,1).
 $$
 \end{enumerate}
 \end{theorem}
Note that in part 3 of Theorem \ref{ThmFix}, we assume the existence of a bound $D$ s.t. $d(x_k,\calX_h)^2\leq D$ for all $k\in\mathbb{N}$. Such a bound was provided in part 1 of the theorem. However for the sake of notational clarity we prove part 3 with a generic upper bound $D$. 

 \input{Proof_Fixed1}
 %For the special case of $\theta=\frac{1}{2}$, our Theorem \ref{ThmFix} implies linear convergence up to a tolerance level of $\frac{\alpha G^2}{2 c}$ provided $\alpha\leq \frac{1}{c}$. This is exactly the same stepsize constraint derived in \cite[Thm 4]{karimi2016linear}. However our tolerance level is better since we do not assume the function is Lipschitz smooth whereas the tolerance derived in \cite{karimi2016linear} is proportional to the Lipschitz smoothness constant. Thus our result can be viewed as a generalization and improvement of \cite[Thm 4]{karimi2016linear}. %It should be noted that their analysis holds for stochastic subgradient methods, whereas our analysis thus far has been deterministic. In section \ref{secStochastic} we show that all our analysis so far can be extended to stochastic oracles so long as $0<\theta\leq\frac{1}{2}$. 

 
 
 \section{Iteration Complexity for Constant Stepsize}\label{sec_itercomp}
 Using the results of the previous section we can derive the iteration complexity of a constant stepsize for finding a point such that $d(x_k,\calX_h)^2\leq\epsilon$. 
  %The iteration complexity is the same (up to a factor of $\log\frac{1}{\epsilon}$) as that of a decaying stepsize sequence with the optimal choice for $d$, as given in Theorems \ref{thmDimSum}, \ref{ThmLargeTheta}, and \ref{thmD1}. 
   The basic idea in the following theorem is to pick $\alpha=O(\epsilon^{\frac{1}{2\theta}})$, so that $e_*$ defined in Theorem \ref{ThmFix} is equal to $\epsilon$. Then the iteration complexity can be determined from the linear convergence rate of $d(x_k,\calX_h)^2$ to $e_*$.

\input{Thm_and_Proof_Fixed2_itercomp}
  
  Rather surprisingly, Theorem \ref{ThmFixIterComp} shows that a restarting strategy is not necessary for $\theta\leq\frac{1}{2}$. This is because for $\theta\leq\frac{1}{2}$ the iteration complexity for a constant stepsize is equal to the complexity of RSG derived in \cite{yang2015rsg}. It is also matched by the optimal decaying stepsize we will derive in Sec. \ref{sec_sum}. 
     To compare with RSG in more detail, \cite{yang2015rsg} showed that RSG requires $O(\epsilon'^{2(\theta-1)})$ iterations (suppressing constants and a $\ln\frac{1}{\epsilon}$ factor) to achieve $h(x)-h^*\leq\epsilon'$. Now, using the error bound, in order to guarantee $d(x_k,\calX_h)^2\leq\epsilon$, we need $h(x)-h^*\leq \epsilon'=\epsilon^{\frac{1}{2\theta}}$. Using this in the iteration complexity from \cite{yang2015rsg} yields the expression $O(\epsilon^{1-\frac{1}{\theta}})$, which is the same as what we derived for the constant stepsize for $\theta\leq 1/2$. However, for $\theta>\frac{1}{2}$, RSG, our DS-SG method, and our optimal decaying stepsize are significantly faster than the constant stepsize choice.
     For $\theta=1/2$, the iteration complexity of the constant stepsize derived in Theorem \ref{ThmFixIterComp} depends on $\ln d(x_1,\calX_h)$, and has the same dependence on $\epsilon$ as the other methods. This remarkable property makes it preferable to the other more sophisticated methods in this case. %Note that both the constant stepsize and RSG have a logarithmic dependency on the initial distance $d(x_1,\calX_h)$. 
    %This is an important finding as there are many applications where $\theta=\frac{1}{2}$ \cite{bolte2015error,li2016calculus}. 
    
    The comparison with the classical result for the subgradient method is as follows. It is easy to show that for the subgradient method with a constant stepsize $\alpha$:
    \begin{eqnarray*}
    \frac{1}{k}\sum_{i=1}^k(h(x_i)-h^*)
    \leq
    \frac{d(x_1,\calX_h)^2}{2\alpha k}+\frac{\alpha}{2}G^2.
    \end{eqnarray*}
    Setting 
    $$\alpha=\frac{c\epsilon^{\frac{1}{2\theta}}}{G^2}
    $$
     and 
    \begin{eqnarray*}\label{classicalIter}
   k\geq \kappa^2 d(x_1,\calX_h)^2\epsilon^{-1/\theta}
    \end{eqnarray*} 
    implies 
    $$
    h(x_k^{av})-h^*\leq \frac{1}{k}\sum_{i=1}^k(h(x_i)-h^*)\leq  c\epsilon^{1/2\theta}
    $$ 
    where $x_k^{av}=\frac{1}{k}\sum_{i=1}^k x_i$. Now using the error bound, this yields $d(x_k^{av},\calX_h)^2\leq\epsilon$. With respect to $\epsilon$, this classical iteration complexity is clearly worse than the result of Theorem \ref{ThmFix} for all $\theta\in(0,1]$. Furthermore, the dependence on $d(x_1,\calX_h)$ is worse. For $\theta\leq1/2$, the fixed stepsize depends on $\ln d(x_1,\calX_h)$, whereas the classical stepsize has iteration complexity which depends linearly on $d(x_1,\calX_h)$. 
    
    % For the special case of $\theta=1$, in the next section we derive a new stepsize for $\theta=1$ which ensures $d(x_k,\calX_h)$ remains bounded, and obtains a linear convergence rate. 
    
    %As mentioned in the introduction, \cite{supittayapornpong2016staggered} also proved a $O(1/\epsilon)$ iteration complexity for $d(x_k,\calX_h)$ in the case where $\theta=1$. However the dependence on $d(x_k,\calX_h)$ for their analysis is proportional, not logarithmic. 
    
    We note that as $\theta\to 0$ the iteration complexity can be made arbitrarily large. This is not suprising, as it has been proved in \cite[p. 167-168]{PolyakIntro} that the convergence rate of $x_k\to x^*$ can be made arbitrarily bad for gradient methods. 


  
 \section{A ``Descending Stairs" Stepsize with Better Iteration Complexity for $1/2\leq \theta\leq 1$}
 \label{secRest}
 \subsection{The Method}
  In this section we propose a new stepsize for the subgradient method (DS-SG) which obtains a better iteration complexity than the fixed stepsize for functions satisfying HEB with $1/2\leq\theta\leq 1$. In fact for $\theta=1$ the iteration complexity is logarithmic, i.e. $O(\ln\frac{1}{\epsilon})$. 
  The basic idea is to use a constant stepsize in the subgradient method and every $K$ iterations reduce the stepsize by a factor of $\beta_{ds}^{\frac{1}{2\theta}}>1$. Also the number of iterations $K$ increases by a factor $\beta_{ds}^{\frac{1}{\theta}-1}$. Our analysis allows us to determine good choices for the initial stepsize and number of iterations which lead to an improved rate. 
 % Our analysis is based on the iteration complexities derived in Theorem \ref{ThmFixIterComp} for the optimal constant stepsize. 
 
 
 The algorithm is similar to RSG \cite{yang2015rsg}. However our  method has some important advantages, which will be discussed in Sec. \ref{secDD_discuss}, and a different analysis. As was mentioned earlier, the method of \cite[Sec. V]{supittayapornpong2016staggered} is essentially a special case of DS-SG for $\theta=1$. 
  
  
  
  
  DS-SG requires an upper bound on the distance of the starting point to the solution, i.e. $\Omega_1\geq d(x_{\text{init}},\calX_h)^2$. If $\calC$ is bounded then one can use the diameter of $\calC$. If a lower bound on the optimal value is known, i.e. $h_{l}\leq h^*$, then by the error bound $d(x_1,\calX_h)\leq c^{-\theta}\left(h(x_1)-h^*\right)^\theta\leq c^{-\theta}\left(h(x_1)-h_l\right)^\theta$ implies we can use $\Omega_1=c^{-2\theta}\left(h(x_1)-h_l\right)^{2\theta}$.
 
 \begin{algorithm}
 
   \caption{(DS-SG) Descending Stairs Subgradient Method for $1/2\leq \theta\leq 1$}
   \begin{algorithmic}[1]
   \REQUIRE $\beta_{ds}$, $M$, $x_{\text{init}}$, $\Omega_1$, $G$, $c$, $\theta$.
   \STATE  $\kappa=\frac{G}{c}$ 
   \STATE \label{Kline1} $\tilde{K}_1=
     \theta
 \kappa^2\beta_{ds}^{\frac{1}{2\theta}}\ln\left(2\beta_{ds}\right)
      \Omega_1^{1-\frac{1}{\theta}}
         $
   \STATE $K_1 = \lceil\tilde{K}_1\rceil$
   \STATE\label{alphaline1} $\alpha(1)=\frac{2c}{G^2}\left(\frac{\Omega_1}{2\beta_{ds}}\right)^{\frac{1}{2\theta}}$
   \STATE $\hat{x}_0=x_{\text{init}}$
   \FOR{$m=1,2,\ldots,M$}
      \STATE    $\hat{x}_m = \text{FixedSG}(K_m,\alpha(m),\hat{x}_{m-1})$\label{LineXhat}
      \STATE \label{alphaline2} $\alpha(m+1) = \beta_{ds}^{-\frac{1}{2\theta}}\alpha(m)$
      \STATE $K_{m+1}=\left\lceil\beta_{ds}^{\frac{m(1-\theta)}{\theta}}\tilde{K}_1\right\rceil$\label{Kline2}
   \ENDFOR
   \RETURN $\hat{x}_M$ 
   \end{algorithmic}
   \label{ReSG}
\end{algorithm}
 
\input{Proof_DS_SG}

   
  \subsection{Discussion}\label{Discuss}\label{secDD_discuss}
%  The asymptotic requirement that $\Omega_1 = O\left(\kappa^{\frac{2\theta}{1-\theta}}\right)$ is mild. As $\theta\to 1$, the requirement vanishes. If $\theta=1/2$, then this reduces to $\Omega_1 = O(\kappa^2)$. Since typically $\kappa\gg1$, this mild.
%  
  
The optimal choice for $\beta_{ds}$ can be found by minimizing the iteration complexities given in (\ref{Th1ResultA}) and (\ref{scoreA})  w.r.t. $\beta_{ds}$. However the closed form expression is complicated and not particularly enlightening. Solving it numerically, we find it is typically between $2$ and $2.5$.

%If $\theta=1$ the number of inner iterations remains constant. The iteration complexity of DS-SG is proportional to $\Omega_1^{\frac{1}{\theta}-1}$ via the constant $G^2$. For $\theta=1$ this dependence disappears. 
  %For the case $\theta=1$,  DS-SG, RSG of \cite{yang2015rsg}, and the method of \cite{supittayapornpong2016staggered} all obtain a logaritmic complexity with different choices for the parameters $M$, $K$, and $\alpha$, and either with or without averaging. %A downside of \cite{supittayapornpong2016staggered} is that the iteration complexity is proportional to $d(x_1,\calX_h)$. In contrast, our iteration complexity, and the one for RSG in \cite{yang2015rsg}, is proportional to $\ln d(x_1,\calX_h)$. %It is difficult to determine the effect of $G$ and $c$ on the iteration complexity of the algorithm in \cite{supittayapornpong2016staggered}. 
  %In  contrast, the iteration complexity in Theorem \ref{thmRestart} is arguably simple and easily interpreted.
  
Regarding RSG \cite{yang2015rsg}, the iteration complexity is very similar to ours, even though the analysis is different. There are several points to note in comparing the two. First is that their error metric is $h(x)-h^*$. 
 %Now if $h$ is convex with bounded subgradients then  
  %\begin{eqnarray*}
  %h(x)-h^*&\leq&|\langle g,x-x^*\rangle|\quad\forall g\in\partial h(x),x^*\in\calX_h
  %\\
  %&\leq&
  %\|g\|\|x-x^*\|\quad\forall g\in\partial h(x),x^*\in\calX_h
  %\\
  %&\leq&
  %G\|x-x^*\|\quad\forall x^*\in\calX_h.
  %\end{eqnarray*}
  %In particular choosing $x^*$ to be the projection of $x$ onto $\calX_h$ yields
  %$h(x)-h^*\leq G d(x,\calX_h)$. 
%Combining this with the error bound
%  \begin{eqnarray*}\label{LiscitzCont}
%  c d(x,\calX_h)^\theta\leq h(x)-h^*\leq G d(x,\calX_h).
%  \end{eqnarray*}
 On the other hand our error metric is $d(x_k,\calX_h)^2$. Furthermore their iteration complexity is for finding $h(x)-h^*\leq 2\epsilon$. To do a fair comparison, we can convert their error metric to $d(x_k,\calX_h)^2$ by using $\epsilon'=2^{-1}\epsilon^{\frac{1}{2\theta}}$ in their iteration complexity. As we mentioned earlier, their iteration complexity is $O(\epsilon'^{2(\theta-1)}\ln\frac{1}{\epsilon'})$. Thus, if we make the substitution, we see that their iteration complexity is the same as ours except they have an extra $\log\frac{1}{\epsilon}$ term.
 The dependence on $\kappa = G/c$ is the same. 
 
 With respect to their algorithm implementation as given in \cite[Algorithm 2]{yang2015rsg}, the major difference to DS-SG is that \cite{yang2015rsg} requires averaging to be done after every inner loop. As mention before, this may be undesirable on problems where nonergodic methods are preferable. For instance, in problems where $\calC$ enforces sparsity or low-rank, the averaging phase spoils this property \cite{davis2017three}. Another situation in which averaging is undesirable is when learning with reproducing kernels \cite{kivinen2004online}. In such problems, the variable is represented as a linear combination of a kernel evaluated at different points. After $t$ iterations of the subgradient method, the solution is $\sum_{i=1}^{t-1} \alpha_i k(x_i,\cdot)$ where $k:\calH\times\calH\to\mathbb{R}$ is the kernel function. Thus it is necessary to store the $t-1$ points $\{x_i\}$ after $t$ iterations which is infeasible. The key to making the method practical is that for certain objectives the coefficients $\alpha_i$ decay geometrically and the early iterations can be safely ignored. Thus only a small fraction of the last $t$ points are recorded. However, if averaging is used, the earlier coefficients are no longer negligible which compromises the feasibility of the method. Another advantage of our approach over \cite{yang2015rsg} will arise in the next section, where we develop a method for adapting to unknown $c$. 
 

 
 % An advantage of RSG over DS-SG is that RSG only requires the error bound to be satisfied on a local region such that $h(x)-h^*\leq\epsilon$, where $\epsilon$ is the target accuracy. However if the function satisfies HEB with $\theta=1$ on a local region, then it is automatically satisfied on the entire space. This quite intuitive observation can be shown by considering the equivalent subgradient characterization of WS functions given in \cite{burke1993weak}.  We note that for $\theta<1$, the iteration complexity of DS-SG has worse dependency on $\Omega_1$ than RSG. % For $\theta=1/2$, it appears the fixed
%For $\theta=1$ the analysis of DS-SG In both cases the base algorithm finds a point such that $d(x_k,\calX_h)\leq\epsilon$ after a number of iterations which is proportional to a power of the ratio $d(x_1,\calX_h)/\epsilon$. This allows a decrease of the distance to the solution set by a constant factor in a constant number of iterations, resulting in an overall logarithmic iteration complexity. It is interesting that these two different applications lead to the same phenomenon, suggesting there may be other applications. 



%In closing this section we note that it is easy to optimize $\beta_{ds}$ by choosing it to minimize the iteration complexity as given in Theorem \ref{thmRestart}. A numerical solver gives 
%$\beta_{ds}^*= 1.74$ XX to two decimal places.

 
 \section{Double Descending Stairs Stepsize  Method for Unknown $c$}\label{secAdapt}
  \subsection{The Method}
In our method DS-SG (Algorithm \ref{ReSG}), the initial number of inner iterations is
 \begin{eqnarray}
 \label{Kdef}K_1=
   \left\lceil 
   \theta\kappa^2\beta_{ds}^{\frac{1}{2\theta}}\ln\left(2\beta_{ds}\right)
   \Omega_1^{1-\frac{1}{\theta}}
      \right\rceil,
 \end{eqnarray}
 where $\kappa=G/c$. The initial stepsize $\alpha(1)$, given in line \ref{alphaline1}, and the lower bound on $\beta_{ds}$, given in  \eqref{beta_bound}, also depend on $c$.
 If a lower bound for $c$ is known, then using this value in (\ref{alphaline1}), (\ref{beta_bound}), and (\ref{Kdef}) ensures convergence. However in many problems $c$ is unknown.  Furthermore if $c$ is greatly underestimated then this will lead to many more inner iterations and a much smaller initial stepsize than is necessary. For the case where no accurate lower bound for $c$ is known, we propose the following ``doubling trick" which still guarantees essentially the same iteration complexity. The analysis only holds when $\calC$ is bounded. Let the diameter of $\calC$ be $\Omega_{\calC} = \max_{x,x'\in\calC}\|x-x'\|^2$. The basic idea is to repeat DS-SG with a new $c$ which is half the old estimate, which quadruples the number of inner iterations and halves the initial stepsize. In this way it takes only $O(\log_2(\frac{c_1}{c}))$ trial choices for for the error bound constant until it lower bounds the true constant. Furthermore, if the initial estimate $c_1$ is much larger than the true $c$, then the number of inner iterations is relatively small, which is why the overall iteration complexity comes out to be only a factor of $(4/3)$ times larger than that of DS-SG. This means it is advantageous to use a large  overestimate of $c$. In fact one can safely use the initial estimate $c_1 = G\Omega_{\calC}^{\frac{1}{2}-\frac{1}{2\theta}}$. We call the method the ``Doubling trick Descending Stairs" subgradient method (DS2-SG).
 
 \begin{algorithm}
   \caption{Double Descending Stairs subgradient method for unknown $c$ (DS2-SG), $\frac{1}{2}\leq\theta\leq 1$}
   \begin{algorithmic}[1]
   \REQUIRE $\beta_{ds}$, $G$, $M$, $c_1$, $\Omega_{\calC}, x_1$, \emph{stopping criterion}.
   \STATE $l= 1$
   \WHILE{\emph{stopping criterion} not satisfied}
      \STATE $\tilde{x}_l= $DS-SG($\beta_{ds},M,\tilde{x}_{l-1},\Omega_{\calC},G,c_l,\theta$)
      \STATE $c_{l+1}=c_l/2$
      %\IF{$\beta_{ds}$ does not satisfy (\ref{beta_bound_2_1}) with $c_{l+1}$ in place of $c_1$}
      %\STATE Set $\beta_{ds}$ equal to R.H.S. of (\ref{beta_bound_2_1}) with $c_{l+1}$ in place of $c_1$.
      %\ENDIF
      \STATE $l= l+1$
   \ENDWHILE 
   \RETURN $\tilde{x}_{l-1}$
   \end{algorithmic}
   \label{AdReSG}
   \end{algorithm}
   
\input{ThmAndProof_DS2_SG}
\subsection{Discussion}
%The choice $c_1= G\Omega^{1-\frac{1}{\theta}}$ has iteration complexity $O(\kappa^2\Omega_1^{\frac{1}{\theta}-1}\epsilon^{\frac{1}{\theta}-1})$. This is worse than the iteration complexity of DS-SG (which knows $c$) which is $O((\kappa^2+\Omega_1^{\frac{1}{\theta}-1})\epsilon^{\frac{1}{\theta}-1})$. However when $\theta=1$ they coincide and there is no penalty associated with not knowing $c$.

 %The competing methods for $\theta=1$ which also obtain a %$O(\log\frac{1}{\epsilon})$ complexity cannot handle unknown $c$. This is the major advantage of DS2-SG.
    The authors of RSG \cite{yang2015rsg} proposed a variant, R\textsuperscript{2}SG, which can adapt to unknown $c$ when $\theta<1$. It also uses geometrically increasing number of inner iterations, however the initial stepsize remains the same. An advantage of that method is it does not require the constraint set to be bounded. However since their analysis is only valid for $\theta<1$, it cannot be directly applied to important problems such as polyhedral convex optimization, and requires using a surrogate $\theta<1$.
    
    For $\theta=1$, the subgradient methods of \cite[Sec. 2.3]{shor2012minimization} and \cite{goffin1977convergence} choose geometrically decaying stepsizes which depend on the error bound constant $c$. It is plausible that our ``doubling trick" idea can be employed to accelerate these methods when $c$ is unknown, by starting with an estimate for $c$ and repeatedly halving it. This should lead to linear convergence with only a slightly larger iteration complexity than the original methods. Thus our doubling trick can be thought of as a ``meta-acceleration" technique with potentially large scope. 
   
 A drawback of DS2-SG is it does not have an explicit stopping rule. In particular, the number of ``wrapper" iterations, $L$, depends on the true error bound constant $c$, which is unknown. This is also the main drawback of R\textsuperscript{2}SG \cite{yang2015rsg} (along with the fact it cannot be applied when $\theta=1$). As was suggested in \cite{yang2015rsg}, we suggest using an independent stopping criterion. For example on a machine learning problem, one could use the error on a  small validation set as an indication the algorithm has converged. If a lower bound $h_{LB}\leq h^*$ is known, then $c^{-\theta}\left(h(x_k)-h_{LB}\right)^\theta<\sqrt{\epsilon}$ can be used as a stopping criterion. This is because  $d(x_k,\calX_h)\leq c^{-\theta}\left(h(x_k)-h_{LB}\right)^\theta$.  Furthermore since, $c d(x_k,\calX_h)^{\frac{1}{\theta}-1}\leq\|g\|$ for $g\in\partial h(x)$, the norm of the subgradient could be used as a stopping criterion for $\theta<1$. Another possibility is to use the fact that $c d(x_k,\calX_h)\leq \|g\|^\theta \Omega_{\calC}^\theta$. Exploring these stopping criteria is a topic for future work. %We point out that this is a common property of complexity estimates. For example for smooth and strongly convex functions, gradient descent and Newton's method have an iteration complexity which depends on the condition number, which is rarely known. Instead one is forced to use an independent stopping criterion.
   
   In practice for DS2-SG, we often observe an increase in the objective function value whenever a new trial error bound constant is used resulting in a larger stepsize. It is therefore a good strategy to keep track of the iterate $\tilde{x}_l$ with the smallest objective function value so far. This does not change the overall iteration complexity and only requires storing one additional iterate.  
   
   %In closing this section we note that our idea for periodically decreasing the estimate of $c$ can also be applied to the restart method of \cite{yang2015rsg} to create a new method which adapts to unknown $c$.

 
 
 
 
 
\section{Faster Rates for Decaying Stepsizes for $\theta<1$}\label{sec_sum}
If $\theta<1$, an upper bound for $G$ is known, a lower bound for $c$ is known, and the constraint set is compact, then it is possible to obtain the same iteration complexity as DS-SG using decaying stepsizes. We consider $\theta\geq 1/2$ and $\theta<1/2$ in separate theorems. 

\begin{theorem}\label{OptimalP}\label{ThmOptDecay}
Suppose Assumption 3 holds and $\frac{1}{2}\leq \theta< 1$. Suppose $\|x-y\|^2\leq\Omega_{\calC}$ for all $x,y\in\calC$. 
Choose $c$ small enough (or $G$ large enough) so that 
\begin{eqnarray}
\kappa\label{kappaCond}
\geq
\sqrt{3}\Omega_{\calC}^{\frac{1-\theta}{2\theta}}.
\end{eqnarray}
For the iterates of the subgradient method (\ref{iterSG}), let $\alpha_k = \alpha_1 k^{-p}$ where
\begin{eqnarray}\label{defp}
p = \frac{1}{2(1-\theta)}
\end{eqnarray}
and 
\begin{eqnarray}\label{alph1Choice}
\alpha_1 = \frac{c}{G^2}\left(\frac{\theta\kappa^2}{1-\theta}\right)^p.
\end{eqnarray}
Then, for all $k\geq \lceil\frac{2\theta}{1-\theta}\rceil$
\begin{eqnarray}\label{OptConvRate}
d(x_k,\calX_h)^2 \leq  \left(
\frac{\theta }{1-\theta}
\right)^{\frac{\theta}{1-\theta}} \left(\frac{k}{\kappa^2}\right)^{\frac{\theta}{\theta-1}}.
\end{eqnarray}
\end{theorem}
\begin{proof} 
\input{ThetaOver05DecayingProof}
\end{proof} 

The convergence rate given in (\ref{OptConvRate}) yields the following iteration complexity: The subgradient method with this stepsize yields a point such that $d(x_k,\calX_h)^2\leq\epsilon$ for all
\begin{eqnarray*}
k\geq \frac{2\theta}{1-\theta} \max\{\kappa^2,3\Omega_{\calC}^{\frac{1}{\theta}-1}\} \epsilon^{1-\frac{1}{\theta}}.
\end{eqnarray*}
This is equal (up to constants) to the iteration complexity derived for DS-SG in Theorem \ref{thmRestart}. The main drawback versus DS-SG is that the analysis only holds for a bounded constraint set. It is also trivial to embed this stepsize into the ``doubling" framework used in DS2-SG so that one does not need a lower bound for $c$. Since the analysis is the same as given in Theorem \ref{thmAdapt}, we omit the details. The proof of Theorem \ref{ThmOptDecay} is inspired by \cite{goffin1977convergence} which considered geometrically decaying stepsizes when $\theta=1$. Theorem \ref{ThmOptDecay} is a natural extension of \cite{goffin1977convergence} to $\theta<1$. 

The optimal stepsize given in Theorem \ref{ThmOptDecay} requires knowledge of $G$, $c$, and $\Omega_{\calC}$ in order to set $\alpha_1$. In the longer version of this paper \cite{johnstone2017faster} we show that the stepsizes $\alpha_k=\alpha_1 k^{-p}$ with $p<1$ are convergent for any $\alpha_1>0$ when $\theta\geq 1/2$. 
 
\input{MP_ThetaLessThan05Decaying}


\input{nonsum} 


\input{oldExps}

\input{extensions}

\input{longProofsArxivOnly}
%\section{Possible Directions for Future Work}
%In this paper we have derived new convergence rates for various subgradient methods for minimizing nonsmooth functions satisfying a H\"{o}lderian growth condition. 
%In the future, it would be interesting to study under and over relaxed versions of the iteration \eqref{iterSG}. It would also be interesting to study the ODE which is the limit of \eqref{iterSG} as the stepsize goes to $0$, again under a H\"{o}lderian growth condition. 









\bibliographystyle{spmpsci}
\bibliography{refs}
%%%
%%%





\end{document}






