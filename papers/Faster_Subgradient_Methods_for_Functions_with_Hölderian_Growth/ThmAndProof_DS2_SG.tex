\begin{theorem}
\label{thmAdapt} Suppose Assumption 3 holds and $1/2\leq\theta\leq 1$. Suppose $\|x-y\|^2\leq \Omega_{\calC}$ for all $x,y\in\calC$ . Let $\kappa_1=G/c_1$.
If $\theta<1$, choose $\beta_{ds}>1$ s.t.
\begin{eqnarray}\label{beta_bound_2_1}
\beta_{ds}&\geq& 
\max\left\{\frac{1}{2}
\left(\frac{\kappa_1^2}{4}\right)^{\frac{\theta}{\theta-1}}\Omega_\calC,
\theta^{-2\theta}\kappa_1^{-4\theta}\Omega_\calC^{2(1-\theta)}
\right\}.
\end{eqnarray}
If $\theta=1$, choose $c_1$ so that $\kappa_1\geq 2$ and choose any $\beta_{ds}>1$.
%(Note that if $\theta=1$ and $\kappa_1\geq 1$, (\ref{beta_bound_2_1}) reduces to the simple requirement that $\beta_{ds}>1$.) 
Fix $\epsilon>0$ and choose 
\begin{eqnarray}\label{Mreq}
M\geq\left\lceil\frac{\ln\frac{\Omega_{\calC}}{\epsilon}}{\ln\beta_{ds}}\right\rceil.
\end{eqnarray}
For the output of Algorithm DS2-SG, if $l\geq L= \max\{0,\lceil\log_2 c_1/c\rceil\}+1$, then $d(\tilde{x}_l,\calX_h)^2\leq\epsilon$. The number of subgradient evaluations is upper bounded by the following quantities (where $\overline{\kappa}=\max\{\kappa,\kappa_1\}$):
\begin{enumerate}
	\item If $\theta=1$:
\begin{eqnarray}
\frac{4}{3}\left(\beta_{ds}^{\frac{1}{2}} \overline{\kappa}^2 \ln(2\beta_{ds})+\left(\frac{\overline{\kappa}}{\kappa_1}\right)^2
+
\log_2\left(\frac{\overline{\kappa}}{\kappa_1}\right)+1
\right)
\left(\frac{\ln\frac{\Omega_{\calC}}{\epsilon}}{\ln\beta_{ds}}+1\right)
\label{adaptIterComp1}
\end{eqnarray}
which simplifies to
\begin{eqnarray*}
O\left(\overline{\kappa}^2\ln\frac{\Omega_{\calC}}{\epsilon}\right)
\end{eqnarray*}
as $\kappa,\kappa_1,\Omega_{\calC}\to\infty$, and $\epsilon\to 0$. 
\item 
If $\frac{1}{2}\leq \theta<1$:
%\begin{eqnarray}
%\label{adaptIterComp2}
% \tilde{O}\left(\max\left\{\overline{\kappa}^2,
% \overline{\kappa}^{2}
% \kappa_1^{\frac{1}{\theta-1}}
% \Omega_{\calC}^{\frac{1}{2\theta}},
% \left(\frac{\overline{\kappa}}{\kappa_1}\right)^2\Omega_{\calC}^{\frac{1}{\theta}-1}\right\}\epsilon^{1-\frac{1}{\theta}}\right)\text{ if }\theta<1,
%\end{eqnarray}
\begin{align}\label{adaptIterComp2}
\frac{4\theta
	\beta_{ds}^{\frac{3}{2\theta}-1}\ln(2\beta_{ds})}
{3(\beta_{ds}^{\frac{1}{\theta}-1}-1)
} 
\overline{\kappa}^2
\epsilon^{1-\frac{1}{\theta}}
+
\left(\frac{4\overline{\kappa}^2}{3\kappa_1^2}
+
\log_2\left(\frac{\overline{\kappa}}{\kappa_1}\right)+2
\right)
\left(
\frac{\ln\frac{\Omega_{\calC}}{\epsilon}}{\ln\beta_{ds}}+1  
\right).
\end{align}
If $\kappa_1$ is chosen large enough so that $\Omega_{\calC} = O(\kappa_1^{\frac{2\theta}{1-\theta}})$,
this simplifies to 
\begin{align}\label{bigO3}
O\left(
\max\{\overline{\kappa}^2,\Omega_{\calC}^{\frac{1}{\theta}-1}\}\epsilon^{1-\frac{1}{\theta}}
\right)
\end{align}
as $\kappa,\kappa_1,\Omega_{\calC}\to\infty$, and $\epsilon\to 0$. 

\end{enumerate}
Note that if $c_1=G\Omega_{\calC}^{\frac{1}{2}-\frac{1}{2\theta}}$, then $\overline{\kappa}=\kappa$. 

%\begin{eqnarray*}
 % \frac{4}{3}\max\left\{\left(\frac{c}{c_1}\right)^2,1\right\}
%\end{eqnarray*}
\end{theorem}
\begin{proof}
%If $c_l\leq c$, for any $l$ then, $d(\tilde{x}_l,\calX_h)^2\leq\epsilon$ by Theorem \ref{thmRestart}. So we assume $c_l> c$ for $l=1,2\ldots,L-1$. 
For all $l$ it is clear that since the iterates remain in the constraint set $\calC$,  $d(\tilde{x}_l,\calX_h)^2\leq \Omega_{\calC}$. Now by the choice of $L$, $c_l\leq c$ for all $l\geq L$. Therefore we can apply Theorem \ref{thmRestart} to the iterations within the while loop when $l\geq L$, which implies $d(\tilde{x}_l,\calX_h)^2\leq\epsilon$ for $l\geq L$. Note that, since the R.H.S. of (\ref{beta_bound_2_1}) decreases if you replace $c_1$ with a smaller error bound constant, $\beta_{ds}$ will satisfy (\ref{beta_bound}) with $c_l$ in place of $c_1$ for all $l\geq 2$. 

   We now determine the overall iteration complexity. Let $K_j^l$ for $l=1,2,\ldots, L$ and $j=1,2,\ldots M$ be the number of iterations passed to FixedSG within the $j$th call to FixedSG in DS-SG, during the $l$th loop in DS2-SG. For all such $l$ and $j$, $K_j^l = \lceil\tilde{K}_j^l\rceil$ where $\tilde{K}_j^1=\tilde{K}_j$ defined in \eqref{defKtild}, and $\tilde{K}_j^l = 4^{l-1}\tilde{K}_j^1$. 
  Thus using the fact that $\ceil x< x+1$, the total number of subgradient calls of DS2-SG can be upper bounded as
  \begin{eqnarray}
%  	&&(K^1_1+K^1_2+\ldots K^1_M)+(K^2_1+K^2_2+\ldots K^2_M)
%  	+\ldots+(K^L_1+K^L_2+\ldots+K_M^L)
%  	\nonumber\\
    \sum_{l=1}^L\sum_{j=1}^M K_j^l
  	&<&
    \sum_{l=1}^L\sum_{j=1}^M\tilde{K}_j^l +  LM
  	\nonumber\\
  	&=&
  	\left(1+4+16+\ldots +4^{L-1}\right)\sum_{j=1}^M\tilde{K}_j^1+  LM
  	\nonumber\\
  	&=&
  	\frac{(4^L-1)}{3}\sum_{j=1}^M\tilde{K}_j^1+LM
  	\nonumber\\
  	&=&
  	\frac{4}{3}
  	\max\left\{\left(\frac{c_1}{c}\right)^2,1\right\}
  	\sum_{j=1}^M\tilde{K}_j^1
  	\nonumber\\
  	&&+(\max\{0,\lceil\log_2 c_1/c\rceil\}+1)
  	\left\lceil\frac{\ln\frac{\Omega_{\calC}}{\epsilon}}{\ln\beta_{ds}}\right\rceil
  	.\label{eqFInal}
  \end{eqnarray}
By noting that 
$$
  	\max\left\{\left(\frac{c_1}{c}\right)^2,1\right\}
  	=
\frac{\overline{\kappa}^2}{\kappa_1^2}
 $$
 and with the aid of \eqref{Th1ResultA} and \eqref{scoreA}, \eqref{eqFInal} reduces to the iteration complexities given in (\ref{adaptIterComp1}) and \eqref{adaptIterComp2}.

 Now $$
 c d(x,\calX_h)^{\frac{1}{\theta}}
 \leq h(x)-h^*\leq \langle g,x-x^*\rangle\leq \|g\|\|x-x^*\|
 $$
 for all $x\in\calC$, $g\in\partial h(x)$, and $x^*\in\calX_h$. If $x^*=\proj_{\calX_h}(x)$ then this implies
 $$
 c d(x,\calH_h)^{\frac{1}{\theta}}\leq Gd(x,\calH_h)\implies c\leq Gd(x,\calH_h)^{1-\frac{1}{\theta}}\quad\forall x\in\calC.
 $$
 Minimizing the R.H.S. yields $c\leq G\Omega_{\calC}^{\frac{1}{2}-\frac{1}{2\theta}}$.
 Therefore the choice $c_1=G\Omega_{\calC}^{\frac{1}{2}-\frac{1}{2\theta}}$ guarantees $\kappa_1\leq\kappa$. 
For $\theta=1$, choosing $c_1=G$ implies $\kappa_1=1$, which violates the requirement: $\kappa_1\geq 2$. Thus one should instead choose $c_1=G/2$. 
\end{proof}