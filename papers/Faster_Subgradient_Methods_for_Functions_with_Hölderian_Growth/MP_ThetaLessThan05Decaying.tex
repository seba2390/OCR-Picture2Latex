We can obtain the same rate for the choice of $\alpha_1$ and $p$ in Theorem \ref{ThmOptDecay} when $\theta<1/2$. In this case, the convergence rate holds for all $k\geq 2$ under a slightly different condition on $\kappa$. 

\begin{theorem}\label{OptimalP2}
Suppose Assumption 3 holds and $0< \theta< \frac{1}{2}$. 
Suppose $\|x-y\|^2\leq\Omega_{\calC}$ for all $x,y\in\calC$. 
Choose $c$ small enough (or $G$ large enough) so that 
\begin{eqnarray}
\kappa^2\label{secondKappaCond}
\geq
\frac{2(1-\theta)}{\theta}\Omega_\calC^{\frac{1-\theta}{\theta}}.
\end{eqnarray}
For the iterates of the subgradient method (\ref{iterSG}), let $\alpha_k = \alpha_1 k^{-p}$ 
%where
%\begin{eqnarray*}
%p = \frac{1}{2(1-\theta)}
%\end{eqnarray*}
where $p$
and $\alpha_1$ are defined in \eqref{defp}  and (\ref{alph1Choice}).
Then, for all $k\geq 2$, $d(x_k,\calX)^2$ satisfies (\ref{OptConvRate}).
\end{theorem}
\begin{proof}
Recall $\gamma=1/(2\theta)$ and note that $\gamma> 1$ since $\theta< 1/2$. Recall 
$$
b=\frac{1}{2\gamma-1}\leq 1\text{ and } p=\gamma b.
$$
As with the proof of Theorem \ref{OptimalP}, this will be a proof by induction. We wish to prove that $e_k\leq C_e k^{-b}$ for all $k\geq 2$ for the constant defined as 
$
C_e = \left(\kappa^2 b\right)^b.
$
 The initial condition is 
$
e_{2}\leq C_e 2^{-b}
$
which is implied by
%\begin{eqnarray}\label{C_initCond2}
%e_1 - 2\alpha_1 c e_1^\gamma + \alpha_1^2 G^2 \leq C_e 2^{-b}.
%\end{eqnarray}
$
C_e\geq\Omega_\calC 2^b\label{C_initCond2}.
$
This in turn is implied by (\ref{secondKappaCond}).

Now we assume $e_k=aC_e k^{-b}$ for some $k\geq 2$ and $a\in[0,1]$ and will show that $e_{k+1}\leq C_e(k+1)^{-b}$. Using  the inductive assumption in the main recursion (\ref{recurting}) yields the following inequality, which we would like to enforce for all $a\in[0,1]$:
\begin{eqnarray}\label{astar}
e_{k+1}&\leq& a C_e k^{-b}
+
\left(
\alpha_1^2 G^2
-
2 \alpha_1 c a^\gamma C_e^\gamma
\right)
k^{-2p}
\nonumber\\
&\leq&
C_e (k+1)^{-b},
\end{eqnarray}
where we once again used the fact that $p+\gamma b=2 p$. 
We require (\ref{astar}) to hold for all $a\in[0,1]$. The L.H.S. is concave in $a$ (since $\gamma> 1$), so we will compute the maximizer w.r.t. $a$. Let $D_1 = \alpha_1^2 G^2 k^{-2p}$, $D_2 = C_e k^{-b}$, and $D_3= 2\alpha_1 c C_e^\gamma k ^{-2\gamma b}$. Then let
\begin{eqnarray*}
f(a)=D_1 + D_2 a-D_3 a^\gamma
\end{eqnarray*}
which is the L.H.S. of (\ref{astar}).
Let $a_*$ be the solution to 
$$
0 = f'(a_*) = D_2 - \gamma D_3 a_*^{\gamma - 1},
$$ 
which implies
\begin{eqnarray*}
a_* &=& \left(\frac{D_2}{\gamma D_3}\right)^{\frac{1}{\gamma-1}}
\\
&=&
C_e^{-1}
(2\alpha_1\gamma c)
^{\frac{1}{1-\gamma}}
k^{\frac{1}{\gamma-1}}
= 
C_e^{-1}D_4
\alpha_1^{\frac{1}{1-\gamma}}
k^{\frac{1}{\gamma-1}}
\end{eqnarray*}
where $D_4 = (2\gamma c)^{\frac{1}{1-\gamma}}$. But recall that $a\in[0,1]$ therefore the maximizer of $f(a)$ in $[0,1]$ is given by
\begin{eqnarray*}
\min\left\{1,C_e^{-1}D_4
\alpha_1^{\frac{1}{1-\gamma}}
k^{\frac{1}{\gamma-1}}\right\}.
\end{eqnarray*}
Thus if 
\begin{eqnarray}\label{secondKcond}
k\geq (C_e D_4^{-1})^{\gamma-1}\alpha_1
\end{eqnarray}
then the maximizer in $[0,1]$ is equal to $1$. Substituting the values for $\alpha_1$ and $C_e$ into (\ref{secondKcond}) yields
\begin{eqnarray*}
k&\geq& (C_e D_4^{-1})^{\gamma-1}\frac{c}{G^2}C_e^\gamma
=
\frac{2\gamma}{2\gamma-1}.
\end{eqnarray*}
Since $\gamma> 1$ this is implied by $k\geq 2$. Thus we only need to consider $a=1$ in (\ref{astar}).

% For $\gamma=1$, the function $f$ is linear in $a$, therefore either $0$ or $1$ is a maximizer. 
%Substituting $a=0$ into (\ref{astar}) yields
%\begin{eqnarray}\label{astar2}
%\alpha_1^2 G^2
%k^{-2p}
%\leq
%C_e (k+1)^{-b}.
%\end{eqnarray}
%The analysis of this inequality was carried out in the proof of Theorem \ref{ThmOptDecay} and yielded the condition: $k\geq 2 b$. Since $b = (2\gamma - 1)^{-1}=1$, this is equivalent to $k\geq 2$. 

The analysis with $a=1$ substituted into (\ref{astar}) was carried out in the proof of Theorem \ref{ThmOptDecay}. Recall that for this choice of stepsize and constant, the inequality (\ref{astar}) is satisfied with $a=1$ for all $k\geq 1$, which completes the proof.  



\end{proof}