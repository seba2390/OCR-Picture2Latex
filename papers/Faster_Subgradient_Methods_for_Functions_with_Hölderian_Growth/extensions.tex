\section{Extensions}\label{secExtend}
As previously mentioned, the key recursion (\ref{KeyRecursion}) can also be derived in the following situations: 1) when a small amount of noise is added to the subgradient, 2) for the incremental subgradient method, 3) under a more general condition than HEB, introduced by Goffin \cite{goffin1977convergence},  4) for the proximal subgradient method, and 5) for relaxed versions of the subgradient method. We now discuss the first three of these in more detail. 

\subsection{Deterministic Noise in the Subgradient when $\theta=1$}
    For the weakly sharp case ($\theta=1$), the subgradient method exhibits resilience to bounded noise. This has been observed in \cite{nedic2010effect,poljak1978nonlinear}. Suppose that at each iteration we have access to a noisy subgradient:
    \begin{eqnarray*}
    \tilde{g}_k = g_k+r_k:g_k\in\partial h(x_k),\|r_k\|\leq R
    \end{eqnarray*}
    and as before the method iterates for all $k\geq 0$
    \begin{eqnarray*}
     x_{k+1}=P_{\calC}(x_k - \alpha_k\tilde{g}_k).
    \end{eqnarray*}
One can repeat the analysis of Sec. \ref{secMy} to show
     \begin{eqnarray*}
     d(x_{k+1},\calX_h)^2
     &\leq&
     d(x_k,\calX_h)^2
     -2\alpha_k  d(x_k,\calX_h)(c-R)
     +2\alpha_k^2 (R^2+G^2).
     \end{eqnarray*}
     We see that this is exactly the same recursion as (\ref{ABiggy}) with the error bound constant $c$ replaced by $c-R$, and $G^2$ replaced by $2(G^2+R^2)$. Thus, if $R<c$, all of the results presented throughout for $\theta=1$ hold with a new error bound constant $\tilde{c}=c-R$, and bound on the subgradients $\tilde{G}^2 = 2(G^2+R^2)$. In particular this refers to Theorems \ref{ThmFix}, \ref{ThmFixIterComp}, \ref{thmRestart}, \ref{thmAdapt}, and \ref{ThmLargeTheta}.
     %In contrast, the analysis of RSG in \cite{yang2015rsg}  does not consider the case when the subgradient is corrupted. 
 
 \subsection{Incremental Subgradient Methods}
 Suppose $h(x)=\sum_{i=1}^m h_i(x)$. Such objective functions which are a finite sum of terms often arise in machine learning in the guise of \emph{empirical risk minimization} \cite{hastie2009elements}. For such problems the \emph{incremental} subgradient method can be used \cite{nedic2001convergence}. This method proceeds by computing the subgradient with respect to each individual function $h_i$ in a fixed order. More precisely the method proceeds for $k\geq 1$ with $x_1\in\calC$ as
 \begin{eqnarray}\label{increment1}
 x_{k+1}&=&\psi_{m,k}
 \\\label{increment2}
 \psi_{i,k}&=& P_\calC(\psi_{i-1,k}-\alpha_k g_{i,k}),g_{i,k}\in\partial h_i(\psi_{i-1,k}),\,\,\, i=1,\ldots,m
 \\\label{increment3}
 \psi_{0,k}&=&x_k.
 \end{eqnarray}
  This method has been analyzed extensively in \cite{nedic2001convergence}.
  \begin{proposition}[\cite{nedic2001convergence}]
Suppose Assumption 3 holds. Then for all $k\geq 1$ the iterates of (\ref{increment1})--(\ref{increment3}) satisfy
  \end{proposition}
 \begin{eqnarray*}
 d(x_{k+1},\calX)^2
 \leq
  d(x_{k},\calX)^2
  - 2\alpha_k c d(x_k,\calX)^{\frac{1}{\theta}}
  +
  \alpha_k^2 m^2 G^2.
 \end{eqnarray*}
 
This is the same as the main recursion we analyze in (\ref{ABiggy}) with $G^2$ replaced by $m^2 G^2$. Thus all our results in the following sections apply to the incremental subgradient method (\ref{increment1})--(\ref{increment3}) with this change in constants. 

 \subsection{Goffin's Condition Number}\label{goff_cond}  
  Goffin \cite{goffin1977convergence} discussed a condition number for quantifying the convergence rate of subgradient methods. The condition number is a generalization of the ordinary notion defined for a smooth strongly convex function as the ratio of the Lipschitz constant of the gradient to the strong convexity parameter. In contrast Goffin's condition number requires neither smoothness or strong convexity. The condition number is also more general than Shor's eccentricity measure \cite{shor2012minimization}. The condition number for a convex function $h$ is defined as
  \begin{eqnarray}\label{goffinCond}
  \mu_{h}=\inf\left\{\frac{\langle u ,x-x^*_p\rangle}{\|u\|\|x-x_p^*\|}: x\in\calC\backslash \calX_h,u\in\partial h(x),x_p^*=\proj_{\calX_h}(x)\right\}.
  \end{eqnarray}
 By convexity and the Cauchy-Schwarz inequality $0\leq\mu_h\leq 1$. Goffin showed that if $h$ satisfies HEB$(c,\theta)$ with $\theta=1$ and $\|g\|\leq G$ for all $g\in\partial h(x),x\in\calC$, then it satisfies (\ref{goffinCond}) with
  \begin{eqnarray*}
  \mu_h\geq\frac{c}{G}=\frac{1}{\kappa}
  \end{eqnarray*}
  which proves that functions satisfying (\ref{goffinCond}) with $\mu_h>0$ are more general than weakly sharp functions. 
  
  Our results for $\theta=1$ throughout this manuscript can be extended to functions satisfying (\ref{goffinCond}) with $\mu_h>0$ if we make a slight modification to the subgradient method.
  \begin{lemma}[\cite{goffin1977convergence}]
  Let $\{x_k\}$ be a sequence satisfying
  \begin{eqnarray}\label{normalizedSG}
  x_{k+1}=P_\calC\left(x_k-\alpha_k\frac{g_k}{\|g_k\|}\right):\forall k\geq 1,g_k\in\partial h(x_k),x_1\in\calC.
  \end{eqnarray}
  If $\calX_h$ is nonempty and $h$ is convex, closed, and proper (CCP) and satisfies (\ref{goffinCond}) with $\mu_h>0$, then for all $k\geq 1$
  \begin{eqnarray*}
  d(x_{k+1},\calX_h)^2\leq d(x_k,\calX_h)^2-2\alpha_k\mu_h d(x_k,\calX_h)+\alpha_k^2.
  \end{eqnarray*}\label{lemGoff}
  \end{lemma}  
  This is the same recursion as (\ref{ABiggy}) with $G=1$, $\theta=1$,  and $c=\mu_h$. Thus all the results derived in this manuscript for HEB with $\theta=1$ can be derived for the scheme (\ref{normalizedSG}) applied to functions satisfying (\ref{goffinCond}) so long as $c$ is replaced by $\mu_h$ and $G=1$. Also note that Lemma \ref{lemGoff} does not require that the subgradients are uniformly bounded over $\calC$. 
 


