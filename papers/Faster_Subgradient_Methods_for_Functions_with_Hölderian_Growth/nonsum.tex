\section{Convergence Rates for Classical Nonsummable Stepsizes}\label{Sec_decay}
We now turn our attention to nonsummable but square summable stepsize sequences for the subgradient method under HEB. These stepsizes are used frequently for the stochastic and deterministic subgradient method, however their behavior under HEB has not been studied in detail with the exception of \cite{lim2011convergence,supittayapornpong2016staggered}. We will see that these nonsummable stepsizes are slower than the ``descending stairs" stepsizes and summable stepsizes when $\theta> 1/2$. However, in this case the nonsummable stepsizes have the advantage that they do not require $G$, $c$, and $\Omega_1$. We will first state and discuss our results. The proofs are in Section \ref{secProofDecay}. 
\subsection{Results for $\theta\in(0,\frac{1}{2})$}

\begin{theorem}\label{thmDimSum}
Suppose Assumption 3 holds and $0<\theta<1/2$. Let $\alpha_k = \alpha_1 k^{-p}$. Let
\begin{eqnarray}
C_1 &\triangleq& \label{C1Def}
2^{2p\theta+1}
\left(
\left(\frac{\alpha_1 G^2}{c}\right)^{2\theta}
+
\alpha_1^2 G^2
\right)
\\\nonumber
C_2
&\triangleq&
\left(\frac{\alpha_1(1-2\theta)}{2\theta(1-p)}\right)^{\frac{2\theta}{2\theta-1}}.
\end{eqnarray}
Then if
\begin{eqnarray}\label{dcond}
\frac{1}{2(1-\theta)}\leq p\leq 1
\end{eqnarray}
and $\alpha_1$ is chosen so that
\begin{eqnarray}
C_1
&\leq&
\left(\frac{2\theta(1-p)}{\alpha_1(1-2\theta)}\right)^{\frac{2\theta}{1-2\theta}}
(k_0+1)
^{\frac{2\theta(2p(1-\theta)-1)}{1-2\theta}}\label{cc},
\\
\alpha_1
&\leq&
\frac{2\theta(1-p)d(x_1,\calX_h)^{\frac{2\theta-1}{\theta}}}{1-2\theta}
,
\label{cc2}
\end{eqnarray}
then for all $k\geq k_0$
\begin{eqnarray}\label{lowTheta1}
d(x_k,\calX_h)^2
\leq
\max\{C_1,C_2\}
\max\left\{
k^{-2p\theta},
k^{\frac{2\theta(1-p)}{2\theta-1}}
\right\}.
\end{eqnarray}


\end{theorem}
\begin{proof}
Sec. \ref{secProofDecay}.
\end{proof}
\begin{comment}
\begin{eqnarray}\label{lowTheta1}
d(x_k,\calX_h)^2
\leq
\max\left\{
C_1 k^{-2d\theta},
C_3
\left(
\frac{1}{2}+C_2k^{1-d}
\right)^{\frac{2\theta}{2\theta-1}}
\right\}.
\end{eqnarray}
\end{comment}

%Theorem \ref{thmDimSum} says that, under the error bound condition, the subgradient method with a slightly smaller stepsize sequence than the classical $\alpha_1 k^{-1/2}$ choice achieves a nonergodic sublinear convergence rate.

In the following corollary we give the optimal choice for $p$ that makes the two arguments to the max function in (\ref{lowTheta1}) equal. 
%and
%\begin{eqnarray*}
%C_1 = 
%\max\left\{
%C_1,
%\left(
%\frac{2\theta(1-d)}{1-2\theta}
%\right)^{\frac{2\theta}{1-2\theta}}
%\right\}.
%\end{eqnarray*}
\begin{corollary}\label{CorDimSum}
In the setting of Theorem \ref{thmDimSum} with $0<\theta<\frac{1}{2}$ and $C_1$ defined in (\ref{C1Def}),  if $p=\frac{1}{2(1-\theta)}$, and $\alpha_1$ is chosen so that (\ref{cc2}) holds and
\begin{eqnarray}\label{SecondAConstant}
\alpha_1^{\frac{2\theta}{1-2\theta}}
C_1\leq \left(\frac{\theta}{1-\theta}\right)^{\frac{2\theta}{1-2\theta}}
\end{eqnarray}
then for all $k\geq 1$
\begin{eqnarray*}\label{lowTheta2}
d(x_k,\calX_h)^2\leq 
\alpha_1^{\frac{2\theta}{2\theta-1}}
\left(\frac{\theta}{1-\theta}\right)^{\frac{2\theta}{1-2\theta}} 
k^{\frac{-\theta}{1-\theta}}. 
\end{eqnarray*}
If $\alpha_1$ is chosen so that (\ref{SecondAConstant}) is satisfied with equality, then 
\begin{eqnarray*}\label{lowTheta3}
d(x_k,\calX_h)^2\leq 
C_1
k^{\frac{-\theta}{1-\theta}}. 
\end{eqnarray*}

\end{corollary}
\begin{proof}
Sec. \ref{secProofDecay}.
\end{proof}

Our derived convergence rate $O(k^{\frac{-\theta}{1-\theta}})$ is faster than the naive application of the classical $O(1/\sqrt{k})$ function value convergence rate, which with the use of HEB results in a rate $d(\hat{x}_k,\calX_h)^2=O(k^{-\theta})$ at the averaged point $\hat{x}_k=\sum\alpha_k x_k/\sum\alpha_k$. Furthermore our result is nonergodic (no averaging is required). 
Thus we see that for $\theta<1/2$ decaying polynomial stepsize sequences can achieve the same convergence rate as RSG \cite{yang2015rsg} and the constant stepsize we derived in Theorem \ref{ThmFixIterComp}. %his choice depends %Furthermore the rate in Corollary \ref{CorDimSum} can be obtained with less information than the restart scheme of \cite{yang2015rsg} as well as the constant stepsize. Indeed, Theorem \ref{thmDimSum} only needs us to choose $\alpha_1$ to satisfy (\ref{SecondAConstant}), which requires an upper bound for $G$, and a lower bound for $c$. In contrast, the method of \cite{yang2015rsg} requires this information along with a lower bound on $h^*$. The constant stepsize of Theorem \ref{ThmFixIterComp} requires an upper bound on $d(x_1,\calX_h)^2$. All methods require $\theta$.



\subsection{Results for $\theta\in[\frac{1}{2},1]$}
We now consider nonsummable stepsizes for $\theta\geq 1/2$. The primary advantage of the following stepsize is that it does not require knowledge of $G,c$, or $d(x_1,\calX)^2$. 
\begin{theorem}\label{ThmLargeTheta}
Suppose Assumption 3 holds and $1/2\leq \theta\leq 1$.
Suppose $\alpha_k = \alpha_1 k^{-p}$ for some $p\in(0,1)$ and $\alpha_1>0$. Let $C_1$ be as defined in (\ref{C1Def}), 
\begin{eqnarray*}
C_3 &\triangleq& C_1^{\frac{1+2p(\theta-1)}{1-p}}
\left(
\frac{\alpha_1(1-2^{p-1}) c e}{4p\theta}
\right)^{-\frac{2 p\theta}{1-p}}
\\
C_4 &\triangleq& 16 \left(\frac{8\theta C_1}{\alpha_1 c e}\right)^{2\theta}
\\
C_5 &\triangleq& d(x_1,\calX_h)^{\frac{2+4p(\theta-1)}{1-p}}\left(
\frac{\alpha_1 c e}{4p\theta}
\right)^{-\frac{2 p\theta}{1-p}}.
\end{eqnarray*}
Then for all $k\geq 4$
\begin{eqnarray}\label{BigThetaResult}
d(x_k,\calX_h)^2\leq 4\max\{C_1,C_3,C_4,C_5\}k^{-2p\theta}.
\end{eqnarray}
\end{theorem}
\begin{proof}
Sec. \ref{secProofDecay}.
\end{proof}

Once again this improves on the known classical \emph{ergodic}  convergence rate of $O(k^{-\theta})$. As $p\to1$ the method can get arbitrarily close to the best rate $O(k^{-2\theta})$, however $p=1$ is not covered by our analysis other than the special case $\theta=\frac{1}{2}$ discussed in Theorem \ref{thmD1} and Proposition \ref{PropThta05} below. The decaying stepsize does not require knowledge of $\theta$, $c$, $G$, $h^*$, or $d(x_1,\calX_h)$ to set the parameters $\alpha_1$ and $p$. 
The result holds for arbitrary $\alpha_1>0$ and $p\in(0,1)$. Nevertheless, the constants are affected by the choice of $\alpha_1$ and $p$ as well as practical performance.

The convergence rate for the decaying stepsizes is much slower than DS-SG, the summable stepsizes in Sec. \ref{sec_sum}, and RSG \cite{yang2015rsg}. These methods obtain the rate $O\left(k^{\frac{\theta}{\theta-1}}\right)$ for $\theta>1/2$. 

%On the other hand Theorems \ref{thmDimSum} and \ref{ThmFixIterComp} imply restarting is unnecessary for $\theta\leq1/2$ as either the constant choice or the decaying polynomial choice have the same convergence rate as RSG. 

The case $\theta=1$ in Theorem \ref{ThmLargeTheta} can be compared with the main result of \cite{lim2011convergence} which also proves $O(1/k^2)$ rate of convergence for $d(x_k,\calX_h)^2$. A difference is their result only holds for sufficiently large $k$. They also assume the function satisfies the quadratic growth condition (i.e. $\theta=1/2$ error bound) globally. For problems where $\calC$ is compact, this does not matter, since QG is implied by WS on a compact set. An advantage of \cite{lim2011convergence} is that it holds for stochastic gradient descent.


\subsection{Results for $\theta=\frac{1}{2}$}
For the special case of $\theta=\frac{1}{2}$ our analysis extends to the choice $p=1$.
\begin{theorem}\label{thmD1}
Suppose Assumption 3 holds and $\theta=1/2$. 
Suppose $\alpha_k = \alpha_1 k^{-1}$ and
\begin{eqnarray*}
\alpha_1\leq\frac{1}{c}.
\end{eqnarray*}
Then for $k\geq 1$
\begin{eqnarray}\label{ThmD1Result}
d(x_k,\calX_h)^2
\leq \max\left\{\frac{2 \alpha_1 G^2}{c},d(x_1,\calX_h)^2\right\} k^{-c \alpha_1}.
\end{eqnarray}
\end{theorem}
\begin{proof}
Sec. \ref{secProofDecay}.
\end{proof}

Strongly convex functions with strong convexity parameter $\mu_{sc}$ satisfy the error bound with $\theta=\frac{1}{2}$ and $c=\frac{\mu_{sc}}{2}$. In this case $C_1=\frac{8 G^2}{c^2}$. Thus, for the choice $\alpha_1=\frac{2}{\mu_{sc}}$ we have proved that
\begin{eqnarray*}
d(x_k,\calX_h)^2\leq \frac{1}{k}\max\left\{d(x_1,\calX_h)^2,\frac{32 G^2}{\mu_{sc}^2}\right\}.
\end{eqnarray*}
This result can be compared with several papers. The result \cite[Theorem 6.2]{bubeck2015convex} finds an $O(1/k)$ convergence rate for $h(\hat{x}_k)-h^*$ for a particular averaged point $\hat{x}_k$ under strong convexity. This, combined with HEB implies an $O(1/k)$ rate for $d(\hat{x}_k,\calX_h)^2$.  The work \cite[Thm 1]{nedic2014stochastic} obtained a nonergodic $O(1/k)$ rate for $d(x_k,\calX_h)^2$ in stochastic mirror descent under strong convexity for a similar stepsize sequence to Theorem \ref{thmD1}. The result \cite[Prop. 2.8]{nedic2001convergence} provides convergence rates for the (incremental) subgradient method with stepsize $\alpha_k=\alpha_1k^{-1}$ for all values of $\alpha_1$ under QG. This is more general than Theorem \ref{thmD1} as they cover the case where $\alpha_1>1/c$. However, for $\alpha_1=1/c$, \cite[Prop. 2.8]{nedic2001convergence} only proves $O(\log k/k)$ convergence whereas Theorem \ref{thmD1} implies $O(1/k)$ convergence. 
The result of \cite[Eq. (2.9)]{nemirovski2009robust} says that for strongly convex functions with parameter $\mu_{sc}$, the subgradient method achieves a nonergodic $O(1/k)$ convergence so long as $\alpha_1>\frac{1}{2\mu_{sc}}$.  In contrast we do not require strong convexity but only the weaker error bound. The result can also be compared to \cite[Thm. 4]{karimi2016linear} which proved an $O(1/k)$ rate for the objective function gap under QG. However they additionally require Lipschitz smoothness. Both \cite{nemirovski2009robust} and \cite{karimi2016linear} considered the stochastic subgradient method. 
%In Section \ref{secStochastic} we show that it is trivial to extend our analysis to the stochastic case for $0<\theta\leq\frac{1}{2}$ with the same convergence rate.

We also provide another choice of stepsize which guarantees a convergence rate of $O(1/k)$ for $d(x_k,\calX_h)^2$ in the case where $\theta=\frac{1}{2}$. This proof is a direct adaptation of \cite[Thm. 4]{karimi2016linear}. Unlike \cite[Thm. 4]{karimi2016linear}, it does not require smoothness of the objective.
\begin{proposition}\label{PropThta05}
In the setting of Theorem \ref{thmD1}, consider the subgradient method with 
\begin{eqnarray*}
\alpha_k =\frac{2k+1}{2c(k+1)^2}.
\end{eqnarray*}
Then for all $k$
\begin{eqnarray*}
d(x_{k+1},\calX_h)^2 
\leq
\frac{d(x_1,\calX_h)^2}{(k+1)^2}
+
\frac{G^2}{c^2(k+1)}.
\end{eqnarray*}
\end{proposition}
\begin{proof}
Sec. \ref{secProofDecay}.
\end{proof}