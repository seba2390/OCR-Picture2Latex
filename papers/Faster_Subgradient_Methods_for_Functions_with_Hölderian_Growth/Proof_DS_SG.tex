 \begin{theorem}\label{thmRestart}
Suppose Assumption 3 holds and $\frac{1}{2}\leq \theta\leq 1$. Choose $x_{\text{init}}\in\calC$ and $\Omega_1$ such that $d(x_{\text{init}},\calX_h)^2\leq\Omega_1$. If $\theta<1$, choose $\beta_{ds}>1$ so that
\begin{eqnarray}\label{beta_bound}
\beta_{ds}&\geq& 
\max\left\{\frac{1}{2}
\left(\frac{\kappa^2}{4}\right)^{\frac{\theta}{\theta-1}}\Omega_1,
\theta^{-2\theta}\kappa^{-4\theta}\Omega_1^{2(1-\theta)}
\right\}.
\end{eqnarray}
If $\theta=1$, assume $\kappa\geq 2$ and choose any $\beta_{ds}>1$.
  Fix $\epsilon>0$ and choose $M\geq\left\lceil\frac{\ln\frac{\Omega_{1}}{\epsilon}}{\ln\beta_{ds}}\right\rceil$. Then for $\hat{x}_M$ returned by Algorithm DS-SG, $d(\hat{x}_M,\calX_h)^2\leq \epsilon$. The iteration complexity is as follows:
  \begin{enumerate}
  \item If $\theta=1$ this requires fewer than
  \begin{eqnarray}
     &&\left(\beta_{ds}^{\frac{1}{2}}  \kappa^2 \ln(2\beta_{ds})+1\right)
   \left(\frac{\ln\frac{\Omega_1}{\epsilon}}{\ln\beta_{ds}}+1\right)
  \label{Th1ResultA}\label{Th1Result}
  \end{eqnarray}
  subgradient evaluations. This simplifies to 
  \begin{align}\label{bigO1}
  O\left(\kappa^2\ln\frac{\Omega_1}{\epsilon}\right)
  \end{align}
  as $\kappa,\Omega_1\to\infty$, and $\epsilon\to 0$.
  \item If $\frac{1}{2}\leq\theta<1$, this requires fewer than
  \begin{eqnarray}
\frac{\theta
	\beta_{ds}^{\frac{3}{2\theta}-1}\ln(2\beta_{ds})}{\beta_{ds}^{\frac{1}{\theta}-1}-1
} 
\kappa^2
\epsilon^{1-\frac{1}{\theta}}
+
\frac{\ln\frac{\Omega_1}{\epsilon}}{\ln\beta_{ds}}+1  \label{scoreA}
  \end{eqnarray}
    subgradient evaluations. If $\kappa$ is chosen large enough so that $\Omega_1 = O(\kappa^{\frac{2\theta}{1-\theta}})$,
this simplifies to 
  \begin{align}\label{bigO}
  O\left(
\max\{\kappa^2,\Omega^{\frac{1}{\theta}-1}\}\epsilon^{1-\frac{1}{\theta}}
  \right)
  \end{align}
  as $\kappa,\Omega_1\to\infty$, and $\epsilon\to 0$. 
  

%   If
%$\beta_{ds}$ satisfies \eqref{beta_bound} with equality whenever the r.h.s. is greater than $1$, then \eqref{scoreA} simplifies to 
%\begin{align}\label{score}
%\tilde{O}\left(\max\left\{\kappa^2,\kappa^{\frac{2\theta-1}{\theta-1}}\Omega_1^{\frac{1}{2\theta}},\Omega_1^{\frac{1}{\theta}-1}\right\}\epsilon^{1-\frac{1}{\theta}}\right)
%\end{align}
%subgradient evaluations,
%     where $\tilde{O}$ suppresses constants and terms which depend on $\log\kappa$ or $\log\Omega_1$.
      \end{enumerate}
 \end{theorem}
 \begin{proof}
 We need some new notation. For $\hat{x}_m$ defined in line \ref{LineXhat} of DS-SG, let $\hat{e}_m=d(\hat{x}_m,\calX_h)^2$. We will use a sequence of tolerances $\{\epsilon_m\}$ defined as $\epsilon_m=\beta_{ds}^{-m}\Omega_1$. Another sequence $\{D_m\}$ is chosen as 
 $$D_m = 2\beta_{ds}\epsilon_m.$$ 
   For each $m\geq 1$, the set $\{\epsilon_m/2,D_m,\alpha(m)\}$ will be used in statement 2 of Theorem \ref{ThmFixIterComp} in place of $\{\epsilon,\hat{D},\alpha\}$. Furthermore we will show that $K_m$ is greater than the corresponding expression for $K$ in \eqref{requiredK}. This will show that $\hat{e}_m\leq 2(\epsilon_m/2)=\epsilon_m$.
   


   We now show that $\{\epsilon_m/2,D_m,\alpha(m)\}$ satisfies (\ref{stepsizeFixed}), \eqref{DinitBound}, (\ref{Dbound}),  and (\ref{epsBound2}), and that $K_m$ is greater than $K$ given in (\ref{requiredK}). Now
the stepsize $\alpha(m)$, defined on lines \ref{alphaline1} and \ref{alphaline2} of DS-SG, can be written as
   \begin{eqnarray*}
   \alpha(m) = \frac{2 c}{ G^2}\left(\frac{\epsilon_m}{2}\right)^{\frac{1}{2\theta}}.
   \end{eqnarray*}
       Thus $\alpha(m)$ satisfies (\ref{stepsizeFixed}) for all $m\geq 1$. % Also note the recursive relationship between $\alpha(m)$ and $\alpha(m-1)$, which is implemented on Line 6 of DS-SG. 
          Next we prove that for $\frac{1}{2}\leq \theta<1$, condition (\ref{beta_bound}) ensures that \eqref{Dbound}--(\ref{epsBound2}) are satisfied for all $m\geq 1$. We also show that for $\theta=1$, \eqref{Dbound} is implied by $\kappa\geq 2$ (recall that \eqref{epsBound2} is only required for $\frac{1}{2}\leq\theta<1$). 
          
             To establish \eqref{Dbound}, we will prove that both arguments in the $\min$ in \eqref{Dbound} individually satisfy the inequality when $\hat{D}$ and $\epsilon$ are replaced by $D_m$ and $\epsilon_{m}/2$.
          Since $\beta_{ds}>1$, it is clear that the first argument in the $\min$ in \eqref{Dbound} satisfies the inequality.
Now for the second argument in the $\min$ in (\ref{Dbound}) to satisfy the inequality we require
\begin{eqnarray*}
	\frac{\epsilon_m}{2}\leq 
	\left(\frac{\theta\kappa^2}{2}\right)^{2\theta}
	D_m^{2\theta-1} = 
	\frac{1}{2}\left(\theta\kappa^2\right)^{2\theta}
	\beta_{ds}^{2\theta-1}\epsilon_m^{2\theta-1}.
\end{eqnarray*}
Using $\epsilon_m=\beta_{ds}^{-m}\Omega_1$ and rearranging this yields
\begin{eqnarray}
	\beta_{ds}^{2m(1-\theta)+2\theta-1}\geq 
	\theta^{-2\theta}\kappa^{-4\theta}
	\Omega_1^{2(1-\theta)}.\label{missed}
\end{eqnarray}
In order to hold for all $m\geq 1$ it suffices to show it holds for $m=1$, which is implied by the second argument in the max in (\ref{beta_bound}).
In the case $\theta=1$,
\eqref{missed} reduces to 
\begin{align*}
\beta_{ds}\geq \frac{1}{\kappa^4}.
\end{align*}
Since $\kappa\geq 2$, any $\beta_{ds}>1$ satisfies this. 

          
          
           Now \eqref{epsBound2} is only required when $\frac{1}{2}\leq\theta<1$. In this case, \eqref{epsBound2} requires that
   \begin{eqnarray*}
   \frac{\epsilon_m}{2} = \frac{1}{2}\beta_{ds}^{-m}\Omega_1
      \leq
      \left(\frac{\kappa^2}{4}\right)^{\frac{\theta}{1-\theta}}.
   \end{eqnarray*}
   In order for this to be satisfied for all $m$, it suffices to show that it holds for $m=1$. This is implied by the first argument in the max function in (\ref{beta_bound}). 
%    using the assumption that $\kappa\geq 2$, we note that the second argument in the $\max$ in (\ref{beta_bound}) reduces to
%   \begin{eqnarray*}
%   \beta_{ds}
%   \geq
%   \frac{1}{\kappa^4}
%   \end{eqnarray*}
%and therefore any $\beta_{ds}>1$ will suffice. 
   
   
   

    
    Finally we prove by induction that \eqref{DinitBound} holds and that $K_m$ is greater than $K$ defined in (\ref{requiredK}). For $m=1$, $D_1$ clearly satisfies \eqref{DinitBound}.   Also $K_1$, given in Line 1 of Algorithm DS-SG, satisfies (\ref{requiredK}).  Altogether this implies $\hat{e}_1\leq\epsilon_1$ by Theorem \ref{ThmFixIterComp}. 
    
   
   
   Next, assume \eqref{DinitBound}  is true and $K_{m-1}$ is greater than $K$ in \eqref{requiredK} at iteration $m-1$. Since we have established \eqref{stepsizeFixed}, \eqref{Dbound}, and \eqref{epsBound2} hold for all $m\geq 1$, part 2 of Theorem \ref{ThmFix} implies that $\hat{e}_{m-1}\leq\epsilon_{m-1}$. At iteration $m$, FixedSG is initialized at $\hat{x}_{m-1}$, and $d(\hat{x}_{m-1},\calX)^2\leq\epsilon_{m-1}$, thus 
   $$
   D_m=2\beta_{ds}\epsilon_m = 2\epsilon_{m-1}\geq d(\hat{x}_{m-1},\calX)^2
   $$ 
   which establishes \eqref{DinitBound} at iteration $m$.
   Next, substituting $D_m$ and $\epsilon_m/2$ in for $\hat{D}$ and $\epsilon$ in \eqref{requiredK}, we see that $K_m$ needs to be greater than
    \begin{eqnarray*}
\frac{1}{2}\theta\kappa^2
    \ln\left(\frac{2d(\hat{x}_{m-1},\calX_h)^2}{\epsilon_m}\right)
    D_m^{1-\frac{1}{2\theta}}(\epsilon_m/2)^{-\frac{1}{2\theta}}\label{thisOne}
    \end{eqnarray*}
    which is indeed true since
$K_m$ can be re-expressed as
    \begin{align*}
    K_m&=\left\lceil 
    \theta\kappa^2\beta_{ds}^{\frac{1}{2\theta}}\ln(2\beta_{ds})\Omega_1^{1-\frac{1}{\theta}}\beta_{ds}^{-(m-1)(1-\frac{1}{\theta})}
    \right\rceil 
    \\    
    &=
    \left\lceil\theta\kappa^2
    \beta_{ds}^{1-\frac{1}{2\theta}}\ln\left(2\beta_{ds}\right)
    \epsilon_m^{1-\frac{1}{\theta}}\right\rceil
    \\
    &\geq 
    \frac{1}{2}\theta\kappa^2
    \ln\left(\frac{2d(\hat{x}_{m-1},\calX_h)^2}{\epsilon_m}\right)
    (2\beta_{ds}\epsilon_m)^{1-\frac{1}{2\theta}}(\epsilon_m/2)^{-\frac{1}{2\theta}}.
    \end{align*} 
    
    
    We have shown that $\{\epsilon_m/2,D_m,\alpha(m)\}$ satisfies (\ref{stepsizeFixed}), \eqref{DinitBound}, (\ref{Dbound}), and (\ref{epsBound2}), and that $K_m$ is greater than $K$ defined in (\ref{requiredK}).
 Thus by part 2 of Theorem \ref{ThmFix}, for all $m\geq 1$ $\hat{e}_m\leq 2(\epsilon_m/2)=\epsilon_{m}$. 
  Finally the choice $M = \left\lceil \frac{\ln\frac{\Omega_1}{\epsilon}}{\ln\beta_{ds}}\right\rceil$ implies $\epsilon_M=\beta_{ds}^{-M}\Omega_1\leq\epsilon$.
  
 If $\theta=1$, the total number of subgradient evaluations is
 \begin{eqnarray*}
 M K_1
&\leq& 
 \left(\kappa^2 \beta_{ds}^{\frac{1}{2}}\ln(2\beta_{ds})+1\right)
\left(\frac{\ln\frac{\Omega_1}{\epsilon}}{\ln\beta_{ds}}+1\right)
 \end{eqnarray*}
where we have used $\lceil x \rceil< x+1$. 
Further note that for $\theta=1$, $\beta_{ds}$ is a constant that can be chosen independently of $\kappa, \Omega$, and $\epsilon$,  which implies \eqref{bigO1}.

 We now establish the iteration complexity when $\frac{1}{2}\leq\theta<1$.
For $m\geq 0$, let 
\begin{align}\label{defKtild}
\tilde{K}_{m+1} =      \beta_{ds}^{\frac{m(1-\theta)}{\theta}}\theta
\kappa^2\beta_{ds}^{\frac{1}{2\theta}}\ln\left(2\beta_{ds}\right)
\Omega_1^{1-\frac{1}{\theta}}
\end{align}
 
then $K_m = \lceil \tilde{K}_m\rceil$ where $K_m$ is defined on Line \ref{Kline2} of Algorithm \ref{ReSG}. 
If $\theta<1$ 
the total number of subgradient evaluations is
 \begin{eqnarray}\nonumber
    K_1 + K_2 +\ldots+ K_M
    &=&
 \lceil \tilde{K}_1\rceil +\lceil \tilde{K}_2\rceil +\ldots +\lceil \tilde{K}_M\rceil
 \\\nonumber
 &<&
   \tilde{K}_1 + \tilde{K}_2 +\ldots+ \tilde{K}_M
  +
  M
  \\\nonumber 
 &=&
 \tilde{K}_1
\left(
1+\beta_{ds}^{\frac{1}{\theta}-1}
+
(\beta_{ds}^{\frac{1}{\theta}-1})^2
+
\ldots
+
(\beta_{ds}^{\frac{1}{\theta-1}})^{M-1}
\right)
\\\nonumber
&&+M
\\\nonumber
&=&
\tilde{K}_1\frac{(\beta_{ds}^{\frac{1}{\theta}-1})^M - 1}{\beta_{ds}^{\frac{1}{\theta}-1}-1}
+
M
\\\label{totalComp} 
&\leq&
\tilde{K}_1\frac{(\beta_{ds}^{\frac{1}{\theta}-1})^M}{\beta_{ds}^{\frac{1}{\theta}-1}-1}
+M
.
 \end{eqnarray}
Now since
\begin{align}
M\leq \frac{\ln\frac{\Omega_1}{\epsilon}}{\ln\beta_{ds}}+1\label{Mupper}
\end{align} 
it follows that
\begin{eqnarray}
(\beta_{ds}^{\frac{1}{\theta}-1})^M\leq 
\beta_{ds}^{\frac{1}{\theta}-1}
\left(\frac{\Omega_1}{\epsilon}\right)^{\frac{1}{\theta}-1}.\label{do}
\end{eqnarray}
%Also since $\lceil x\rceil \leq x+1$
%\begin{eqnarray}\label{it}
%K_1 \leq \theta \kappa^2 \beta_{ds}^{\frac{1}{2\theta}}\ln(2\beta_{ds})\Omega_1^{1-\frac{1}{\theta}}+1.
%\end{eqnarray}
Finally, substitute \eqref{Mupper}, (\ref{do}), and the expression for $\tilde{K}_1$ into  (\ref{totalComp}) to obtain the iteration complexity 
\begin{eqnarray}\label{eq25new} 
\frac{\theta
	\beta_{ds}^{\frac{3}{2\theta}-1}\ln(2\beta_{ds})}{\beta_{ds}^{\frac{1}{\theta}-1}-1
} 
\kappa^2
\epsilon^{1-\frac{1}{\theta}}
+
\frac{\ln\frac{\Omega_1}{\epsilon}}{\ln\beta_{ds}}+1
\end{eqnarray}
total subgradient evaluations,
which is \eqref{scoreA}. 

Now onto \eqref{bigO}. We derive the limiting behavior of \eqref{eq25new} as $\epsilon\to 0$, and $\Omega_1$ and $\kappa$ approach $\infty$. 
In order to do this, we will prove that if $\Omega_1 = O(\kappa^{\frac{2\theta}{1-\theta}})$, the requisite lower bound on $\beta_{ds}$ in \eqref{beta_bound} is $O(1)$, which implies that $\beta_{ds}$ can be chosen as an $O(1)$ constant. 
 If $\kappa$ is too small, then it is enlarged to size $\Theta(\Omega^{\frac{1-\theta}{2\theta}})$ so that this does hold.
 
Considering each argument in the $\max$ in \eqref{beta_bound}, the first is
\begin{align*}
\frac{1}{2}
\left(\frac{\kappa^2}{4}\right)^{\frac{\theta}{\theta-1}}\Omega_1
=
O\left( 
\kappa^{\frac{2\theta}{\theta-1}}\Omega_1
\right) 
 =  O(1)
\end{align*}
and the second is
\begin{align*}
\theta^{-2\theta}\kappa^{-4\theta}\Omega_1^{2(1-\theta)}
=
O
\left(
\kappa^{-4\theta}\Omega_1^{2(1-\theta)}
\right)
 = O(1)
\end{align*}
where we have used the assumption that $\Omega_1 = O(\kappa^{\frac{2\theta}{1-\theta}})$.
Since $\beta_{ds}$ is $O(1)$ under this assumption, \eqref{eq25new} implies the number of subgradient evaluations behaves as 
$
O(\kappa^2\epsilon^{1-\frac{1}{\theta}}).
$
Since $\kappa$ may have to be enlargened to $\Theta(\Omega^{\frac{1-\theta}{2\theta}})$, this implies the subgradient evaluations actually behave as
$
O(\max\{\kappa^2,\Omega^{\frac{1-\theta}{\theta}}\}\epsilon^{1-\frac{1}{\theta}}), 
$
which yields \eqref{bigO}.



 
  

   \end{proof}
 