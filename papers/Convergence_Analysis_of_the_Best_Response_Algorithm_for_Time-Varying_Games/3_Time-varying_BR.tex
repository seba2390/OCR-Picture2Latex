\section{Time-varying games}\label{sec:TV_BR}
In this section, we analyze time-varying games $\mathcal{G}_t$ where the cost functions of the agents change over time. Since the equilibrium of these games also varies, in what follows we analyze the ability of the best response algorithm \eqref{eq:TVBR:update} to generate actions that track the evolving equilibrium. 

If the game $\mathcal{G}_t$ changes significantly, it is reasonable to expect that it will be hard to track the evolving equilibrium. Therefore, as in related literature \cite{duvocelle2022multiagent,zhang2022no}, we assume that both the equilibrium variation $V_T$ in \eqref{eq:def:VT} and the function variation $W_{i,T}$ in \eqref{eq:def:WT} are sub-linear in $T$, for $i=1,\ldots,N$.
% i.e., $V_T = \mathcal{O}(T)$ and $\mathcal{W}_{i,T}=\mathcal{O}(T)$.

In what follows, we analyze the equilibrium tracking error of the best response algorithm \eqref{eq:TVBR:update} in terms of the equilibrium variation.
\begin{theorem}\label{theorem:BRTV}
Suppose that the time-varying game $\mathcal{G}_t$ is $m_t$-strongly monotone and $\nabla_i \mathcal{C}_{i,t}(x_i,x_{-i})$ is $L_t$-Lipschitz continuous in $x_{-i}$ for every $x_i \in \mathcal{X}_i$ with parameter $m_t>L_t \sqrt{N-1}$, for $\forall t$. Then, the best response algorithm \eqref{eq:TVBR:update} satisfies that
\begin{align}\label{eq:BRTV:convergence}
    {\rm{Err}}(T) \leq \frac{\left\| x_{1}- x_{1}^{*}\right\|^2}{1-\rho_m} + \frac{V_T}{(1-\rho_m)^2}=\mathcal{O}\left( 1+ V_T\right),
\end{align}
where $\rho_m:= \mathop{\rm{max}}_t \left\{ \frac{L_t\sqrt{N-1}}{m_t} \right\}$.
\end{theorem}

\begin{proof}
Applying the same arguments as in Proposition \ref{prop:BR} to the cost function $\mathcal{C}_{i,t}$, we can obtain an inequality similar to \eqref{eq:BR_temp5} as
\begin{align}\label{eq:BRTV_temp4}
    \left\| x_{t+1}-x_{t}^{*}  \right\| 
    \leq  \rho_t  \left\| x_{t} - x_{t}^{*}\right\|,
\end{align}
where $\rho_t:=  \frac{L_t\sqrt{N-1}}{m_t} $.
Observe that 
\begin{align*}
    &\left\| x_{t+1}-x_{t+1}^{*}  \right\|^2 = \left\| x_{t+1}- x_{t}^{*} + x_{t}^{*}- x_{t+1}^{*}  \right\|^2 \nonumber \\
    \leq &(1+\lambda) \left\| x_{t+1}- x_{t}^{*}\right\|^2 + (1+\frac{1}{\lambda}) \left\|x_{t}^{*}- x_{t+1}^{*}  \right\|^2,
\end{align*}
for $\forall \lambda >0$. Setting $\lambda = \frac{1}{\rho_t}-1>0$ yields 
\begin{align}\label{eq:BRTV_temp5}
    &\left\| x_{t+1}-x_{t+1}^{*}  \right\|^2 
    \leq \frac{1}{\rho_t } \left\| x_{t+1}- x_{t}^{*}\right\|^2 + \frac{1}{1-\rho_t } \left\|x_{t}^{*}- x_{t+1}^{*}  \right\|^2 \nonumber \\
    & \leq  \rho_t \left\| x_{t}- x_{t}^{*}\right\|^2 + \frac{1}{1-\rho_t } \left\|x_{t}^{*}- x_{t+1}^{*}  \right\|^2 \nonumber \\
    & \leq  \rho_m \left\| x_{t}- x_{t}^{*}\right\|^2 + \frac{1}{1-\rho_m } \left\|x_{t}^{*}- x_{t+1}^{*}  \right\|^2,
\end{align}
where the second inequality follows from \eqref{eq:BRTV_temp4} and the last inequality is due to the fact that $\rho_t \leq \rho_m <1$. Rearranging and summing \eqref{eq:BRTV_temp5} over $t=1,\ldots,T$, we have that
\begin{align*}
    &(1-\rho_m) \sum_{t=1}^T \left\| x_{t}- x_{t}^{*}\right\|^2 \nonumber \\
    \leq & \sum_{t=1}^T \left(\left\| x_{t}- x_{t}^{*}\right\|^2 - \left\| x_{t+1}-x_{t+1}^{*}  \right\|^2 + \frac{\left\|x_{t}^{*}- x_{t+1}^{*}  \right\|^2}{1-\rho_m } \right) \nonumber \\
    \leq & \left\| x_{1}- x_{1}^{*}\right\|^2 +\frac{1}{1-\rho_m } \sum_{t=1}^T\left\|x_{t}^{*}- x_{t+1}^{*}  \right\|^2 \nonumber \\
    \leq & \left\| x_{1}- x_{1}^{*}\right\|^2 +  \frac{1}{1-\rho_m } V_T.
\end{align*}
Dividing both sides of the above inequality by $(1-\rho_m)$ completes the proof.
\end{proof}

Theorem~\ref{theorem:BRTV} shows that $V_T$ dominates the equilibrium tracking error. If $V_T$ is sub-linear in $T$, so is the equilibrium tracking error.
%
In what follows, we analyze the dynamic regret of each agent in terms of the equilibrium variation and the function variation.
\begin{theorem}\label{theorem:BRTV:no_regret}
Suppose that the time-varying game $\mathcal{G}_t$ is $m_t$-strongly monotone, $\nabla_i \mathcal{C}_{i,t}(x_i,x_{-i})$ is $L_t$-Lipschitz continuous in $x_{-i}$ for every $x_i \in \mathcal{X}_i$ with parameter $m_t>L_t \sqrt{N-1}$, and the cost $\mathcal{C}_{i,t}(x)$ is $L_0$-Lipschitz continuous in $x_{-i}$ for every $x_i \in \mathcal{X}_i$ for $\forall t$. Then, the dynamic regret of the best response algorithm \eqref{eq:TVBR:update} satisfies
\begin{align}
    {\rm{DR}}_i(T)  = \mathcal{O}\left( W_{i,t} +\sqrt{TV_T} \right), \; i=1,\ldots,N.
\end{align}
\end{theorem}

\begin{proof}
Using the update rule of the best response algorithm \eqref{eq:TVBR:update}, we have 
\begin{align}
    &{\rm{DR}}_i(T) = \sum_{t=1}^T \Big( \mathcal{C}_{i,t}(x_t) -\mathcal{C}_{i,t}(x_{i,t+1},x_{-i,t})\Big) \nonumber \\
    =& \sum_{t=1}^T \Big( \mathcal{C}_{i,t}(x_t) - \mathcal{C}_{i,t+1}(x_{t+1}) + \mathcal{C}_{i,t+1}(x_{t+1}) \nonumber \\ 
    &- \mathcal{C}_{i,t}(x_{t+1}) + \mathcal{C}_{i,t}(x_{t+1}) - \mathcal{C}_{i,t}(x_{i,t+1},x_{-i,t}) \Big) \nonumber \\ 
    \leq & \mathcal{C}_{i,1}(x_1) +  W_{i,T} + \sum_{t=1}^T \Big(\mathcal{C}_{i,t}(x_{t+1}) - \mathcal{C}_{i,t}(x_{i,t+1},x_{-i,t}) \Big) \nonumber \\ 
    \leq & \mathcal{C}_{i,1}(x_1)+ W_{i,T} + L_0 \sum_{t=1}^T\left\| x_{-i,t+1}-x_{-i,t}\right\| \nonumber \\ 
    \leq & \mathcal{C}_{i,1}(x_1)+ W_{i,T} + L_0 \sum_{t=1}^T \left\| x_{t+1}-x_{t}\right\|.
\end{align}
%
Using the inequality \eqref{eq:BRTV_temp4} and the fact that $\rho_t \leq \rho_m <1$, we have
\begin{align*}
    &\sum_{t=1}^T\left\| x_{t+1}-x_{t}\right\|^2 =\sum_{t=1}^T\left\| x_{t+1}- x_t^{*} +x_t^{*} - x_{t}\right\|^2 \nonumber \\
    & \leq \sum_{t=1}^T \big( (1+\frac{1}{\rho_m}) \left\| x_{t+1}- x_t^{*}\right\|^2 + (1+\rho_m)  \left\|  x_t^{*} - x_{t}\right\|^2 \big) \nonumber \\
    & \leq (\rho_m+1)^2 \sum_{t=1}^T \left\|  x_{t} - x_t^{*} \right\|^2,
\end{align*}
which further yields
\begin{align}
    &{\rm{DR}}_i(T)  \nonumber \\
    \leq & \mathcal{C}_{i,1}(x_1)+ W_{i,T} + L_0 \sqrt{T} \sqrt{\sum_{t=1}^T \left\| x_{t+1}-x_{t}\right\|^2} \nonumber \\
    \leq &  \mathcal{C}_{i,1}(x_1)+ W_{i,T} + L_0 \sqrt{T} \sqrt{(\rho_m+1)^2 \sum_{t=1}^T \left\|  x_{t} - x_t^{*} \right\|^2} \nonumber \\
    =& \mathcal{O}\left( W_{i,t} +\sqrt{TV_T} \right),
\end{align}
where in the last inequality we use the results from Theorem~\ref{theorem:BRTV}.
The proof is complete.
\end{proof}

Theorem~\ref{theorem:BRTV:no_regret} shows that the dynamic regret is sublinear in $T$ if the variation of the game satisfies $W_{i,T}=\mathcal{O}(T^a)$ and $V_{T}=\mathcal{O}(T^b)$ with $a,b\in[0,1)$. 
%
\begin{remark}
(Connection between dynamic regret and equilibrium tracking error). In the single agent case, equilibrium tracking error is equivalent to the dynamic regret. However, this is not true for games involving multiple agents. This is due to the fact that the function $\mathcal{C}_{i,t}(\cdot,x_{-i,t})$ is time-varying due to changes in the function $\mathcal{C}_{i,t}$ itself and changes in other agents' actions $x_{-i,t}$.
To see this, consider the class of time-varying games with time-varying cost functions but constant equilibrium, i.e., $V_T=0$, $W_{i,T}=\mathcal{O}(T^a)$ for some $a>0$. In this case, we have $ {\rm{Err}}(T) = \mathcal{O}(1)$ but  ${\rm{DR}}_i(T) = \mathcal{O}(T^a)$.
\end{remark}
