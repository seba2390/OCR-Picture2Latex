\section{Introduction}
Online convex games study the interplay between game theory and online learning, and find many applications ranging from traffic routing \cite{sessa2019no} to economic market optimization \cite{wang2022risk,lin2020finite}. 
%
In these games, agents simultaneously take actions to minimize their loss functions, which depend on the other agents' actions.
%


Generally, every agent in an online convex game adapts its actions to the actions of other agents in a dynamic manner with the objective to minimize its regret, defined as the cumulative difference in performance between the agent's online actions and the best single action in hindsight.
%
An algorithm is said to achieve no-regret learning if every agent's regret generated by this algorithm is sub-linear in the total number of episodes.
%
If the agents in an online game reach a stationary point from which no agent has an incentive to deviate, then we say the game has reached a Nash equilibrium.
%
There is a growing literature \cite{tatarenko2018learning,bravo2018bandit,drusvyatskiy2021improved,mertikopoulos2019learning,wang2022zeroth} that analyzes the Nash equilibrium convergence in strongly monotone games which admit a unique Nash equilibrium as shown in~\cite{rosen1965existence}.


In non-cooperative games, a common strategy used by competitive agents that selfishly minimize their own cost functions is the best response algorithm since it produces the most favorable outcome given the other agents plays.
%
The best response algorithm has been shown to converge under a spectral condition associated with the best-response map \cite{shanbhag2016inexact,facchinei201012}.
%
In general, best response algorithms have been studied for several classes of games, including supermodular games \cite{milgrom1990rationalizability}, potential games \cite{pass2019course,swenson2018best,lei2017randomized} and zero-sum games \cite{leslie2020best}. 
%
For example, \cite{swenson2018best} shows that in almost every potential game with finite actions, the best response dynamics converges to the unique Nash equilibrium with linear rate. 
%
Similarly, \cite{leslie2020best} shows the convergence of several best response dynamics in two-player zero-sum games.




In this paper, we study the regret and equilibrium tracking error of the best response algorithm for time-varying games. Specifically, we consider a class of strongly monotone games \cite{rosen1965existence,bravo2018bandit}, which guarantee the uniqueness of the well-defined Nash equilibrium. To the best of our knowledge, the best response algorithm has not been explored in the literature for time-varying games. 
%
Instead, time-varying games have been analyzed using  gradient-based algorithms for, e.g., strongly monotone games \cite{duvocelle2022multiagent} and zero-sum games \cite{zhang2022no}. Specifically, \cite{duvocelle2022multiagent} analyzes the Nash equilibrium convergence and the equilibrium tracking properties of the mirror descent algorithm for games that converge and diverge, respectively. In \cite{zhang2022no}, a gradient-type algorithm is proposed that achieves performance guarantees under three different measures.
As gradient-based algorithms are fundamentally different compared to the best response method, the techniques developed in these works cannot be applied here to analyze the best response algorithm.



To address this challenge, we first start with time-invariant games. Specifically, we assume games that satisfy the so-called strong monotonicity condition with parameter $m>0$, which guarantees the uniqueness of the Nash equilibrium \cite{rosen1965existence}. We provide a sufficient condition $m>L\sqrt{N-1}$ under which the best response algorithm achieves linear convergence to the static Nash equilibrium, where $L$ is the Lipschitz constant related to the gradient of the individual loss functions and $N$ is the number of agents. 
Moreover, we show numerically that when this condition fails to hold, the best response algorithm may oscillate. 
Compared to \cite{facchinei201012}, here we characterize the convergence in terms of the strong monotonicity parameter. For simple problems, we can show that our proposed condition is equivalent to the spectral condition proposed in \cite{facchinei201012}. 
%
Then, we analyze the best response algorithm for time-varying games where the Nash equilibrium evolves over time. Specifically, under similar conditions as for time-invariant games, we show that the average distance from the evolving equilibrium is bounded by the equilibrium variation. 
We also show that the dynamic regret is bounded by the cumulative variations of the loss functions.





%
The rest of the paper is organized as follows. In Section~\ref{sec:problem}, we provide some preliminaries and formally define the problem. In Section~\ref{sec:BR}, we present the regret and equilibrium convergence of the best response algorithm for time-invariant games. In Section~\ref{sec:TV_BR}, we extend our result to time-varying games and analyze the equilibrium tracking error and the dynamic regret. In Section~\ref{sec:simulation}, numerical experiments on  a Cournot game are presented to verify our method. Finally, in Section~\ref{sec:conclusion}, we conclude the paper.