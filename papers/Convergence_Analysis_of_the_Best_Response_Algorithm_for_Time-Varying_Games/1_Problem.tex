\section{Preliminaries and Problem Definition}\label{sec:problem}
\subsection{Online Convex Games}
Consider an online convex game $\mathcal{G}$ with $N$ agents, whose goal is to learn their best individual actions that minimize their local loss functions.
%
For each agent  $i\in \mathcal{N}=\{1,\ldots,N\}$, denote by $\mathcal{C}_i(x_i,x_{-i}) : \mathcal{X} \rightarrow \mathbb{R}$ its individual loss function, where $x_i \in \mathcal{X}_i$ is the action of agent $i$, $x_{-i}$ are the actions of all agents excluding agent $i$, and we define by $\mathcal{X} =\Pi_{i=1}^N\mathcal{X}_i$ the joint action space since each agent takes actions independently. 
%
For ease of notation, we collect all agents' actions in a vector $x:=(x_1,\ldots,x_N)$. 
%
We assume that $\mathcal{C}_i(x)$ is convex in $x_i$ for all $x_{-i} \in \mathcal{X}_{-i}$, where $\mathcal{X}_{-i}$ is the joint action space excluding agent $i$.
%
% In addition, we assume that the diameter of the convex set $\mathcal{X}_i$ is bounded by $D$, for all $i=1,\ldots,N$. 
The goal of every agent~$i$ is to determine the action $x_i$ that minimizes its individual cost function, i.e., 
\begin{align}\label{eq:def:game}
    \mathop{{\rm{min}}}_{x_i \in \mathcal{X}_i} \mathcal{C}_{i}(x_i,x_{-i}).
\end{align}
As shown in \cite{rosen1965existence}, convex games always have at least one Nash equilibrium. In what follows, we denote by $x^{*}$ a Nash equilibrium of the game \eqref{eq:def:game}. Then, for each agent $i$, we have $\mathcal{C}_i(x^{*})\leq \mathcal{C}_i(x_i,x_{-i}^{*}),$ $\forall x_i \in \mathcal{X}_i$, $i\in\mathcal{N}$. At this Nash equilibrium point, agents are strategically stable in the sense that each agent lacks incentive to change its action.
%
Since the agents' loss functions are convex, the Nash equilibrium can also be characterized by the first-order optimality condition, i.e., $\langle \nabla_{x_i} \mathcal{C}_i(x^{*}), x_i - x_i^{*} \rangle \geq 0, \; \forall x_i \in \mathcal{X}_i, i\in\mathcal{N},$ where $\nabla_{x_i} \mathcal{C}_i(x)$ is the partial derivative of the loss function with respect to each agent's action.  We write $\nabla_{i} \mathcal{C}_i(x)$ instead of $\nabla_{x_i} \mathcal{C}_i(x)$ whenever it is clear from the context. 

% Throughout the paper, we make the following assumptions on the convex loss functions. 


In general, it is not easy to show convergence to a Nash equilibrium for games with multiple Nash Equilibria. 
%
For this reason, recent studies often focus on games that are so-called strongly monotone and are well-known to have a unique Nash equilibrium \cite{rosen1965existence}. 
%
The game \eqref{eq:def:game} is said to be $m$-strongly monotone if for $\forall x,x'\in \mathcal{X}$ we have that 
\begin{align}\label{eq:strong_monotone}
    \sum_{i=1}^N \langle \nabla_i \mathcal{C}_i(x) -\nabla_i \mathcal{C}_i(x'),x_i-x_i' \rangle \geq m \left\|x -x' \right\|^2.
\end{align}
%
The ability of the agents to efficiently learn their optimal actions can be quantified using the notion of (static) regret that captures the cumulative loss of the learned online actions compared to the best actions in hindsight, and can be formally defined as
\begin{align}\label{eq:def:regret:game}
    {\rm{SR}}_i(T)= \sum_{t=1}^T \mathcal{C}_i(x_t) - \mathop{\rm{min}}_{x_i} \sum_{t=1}^T\mathcal{C}_i(x_i,x_{-i,t}),
\end{align}
for sequences of actions $\{x_{i,t} \}_{t=1}^T, i=1,\ldots,N$.
An algorithm is said to be no-regret if the regret of each agent is sub-linear in the total number of episodes $T$, i.e., ${\rm{SR}}_i(T)=\mathcal{O}(T^a), a\in[0,1)$, $\forall i \in \mathcal{N}$.

\subsection{Problem Definition}
In this work, we consider the time-varying game $\mathcal{G}_t$ where at episode $t$ each agent aims to minimize its time-varying cost function, i.e.,
\begin{align}\label{eq:def:TV:game}
    \mathop{{\rm{min}}}_{x_i \in \mathcal{X}_i} \mathcal{C}_{i,t}(x_i,x_{-i}).
\end{align}
Then, we can define the best response algorithm for time-varying games as
\begin{align}\label{eq:TVBR:update}
    x_{i,t+1} = \mathop{\rm{arg min}}_{x_i \in \mathcal{X}_i} \mathcal{C}_{i,t} (x_i, x_{-i,t}).
\end{align}
%
To attain the best response action $x_{i,t+1}$, for each agent $i$, we assume the cost function $\mathcal{C}_{i,t}$ is known and all other agents' previous actions are provided. This is not a very strong assumption. For example, in supply chain problems \cite{cachon2006game}, $\mathcal{C}_{i,t}$ can represent an agent's local revenue model that depends on all competitors' actions and unknown market demands. At the beginning of episode $t+1$, the agents may not be able to observe the other agents' actions and precisely predict the market demands. However, previous actions and demands can be obtained from public revenue reports. 
%
Thus, it is reasonable to implement a strategy where the agents take actions that best respond to the other agents' actions from the previous episode. 
%
In addition, we assume that at every episode $t$, the time-varying game with the cost function $\mathcal{C}_{i,t}$ is strongly monotone and thus has a unique Nash equilibrium, which we denote by $x_t^{*}$.
To analyze the performance of the best response algorithm \eqref{eq:TVBR:update} for time-varying games, we define the equilibrium tracking error 
\begin{align}\label{eq:BRTV:trackingerror}
    {\rm{Err}}(T):=\sum_{t=1}^T\left\| x_t - x_t^{*}\right\|^2,
\end{align}
and the dynamic regret
\begin{align}\label{eq:BRTV:dynamic:regret}
    {\rm{DR}}_i(T) := \sum_{t=1}^T \Big( \mathcal{C}_{i,t}(x_t) - \mathop{\rm{min}}_{y_i} \mathcal{C}_{i,t}(y_i,x_{-i,t})\Big),
\end{align}
where $T$ is the total number of episodes.
If the game $\mathcal{G}_t$ changes significantly over time, it is reasonable to expect that it may become impossible to track the evolving equilibrium. 
The time-varying  problem becomes meaningful only when the variation of the game $\mathcal{G}_t$ is reasonably small.
To capture the effect of the variation of the game $\mathcal{G}_t$ on the performance of the best response algorithm, we first define the equilibrium variation
%
\begin{align}\label{eq:def:VT}
    V_T:=\sum_{t=1}^T\left\|x_{t}^{*}- x_{t+1}^{*}  \right\|^2,
\end{align}
%
which tracks the changes of Nash equilibria.
%
It is possible that the cost function $ \mathcal{C}_{i,t}$ changes over time but the equilibrium stays constant, i.e., $V_T=0$. 
To further capture the variations of the cost functions, we define the function variation 
\begin{align}\label{eq:def:WT}
    W_{i,T} = \sum_{t=1}^T \sup_{x\in\mathcal{X}}|C_{i,t}(x) - C_{i,t+1}(x)|.
\end{align}
%
% Clearly, if both the equilibrium variation and function variation equal to zero, the game becomes time-invariant. 
Our goal in this paper is to analyze the equilibrium tracking error and the dynamic regret of the best response algorithm \eqref{eq:TVBR:update} for time-varying games. To do so, we start with the analysis of time-invariant games and then extend our results to the time-varying case.


