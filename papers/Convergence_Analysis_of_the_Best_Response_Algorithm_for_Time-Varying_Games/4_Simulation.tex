\section{Numerical Experiments}\label{sec:simulation}
In this section, we validate our analysis on a Cournot game for both time-invariant and time-varying losses.
\subsection{Time-invariant game}
We first focus on the time-invariant case.
We consider a Cournot game with two agents whose goal is to minimize their local losses by appropriately setting the production quantity $x_i$, $i=1,2$.
The loss function of each agent is given by $\mathcal{C}_i(x) = x_i(\frac{a_i x_i}{2} + b_i x_{-i} - e_i)+ 1$, where $a_i>0$ , $b_i$, $e_i$ are constant parameters, and $x_{-i}$ denotes the production quantity of the opponent of agent~$i$. 
It is easy to show that $\nabla_i \mathcal{C}_i(x) = a_i x_i + b_i x_{-i} -e_i$. Recalling that $L$ is the Lipschitz constant of the function $\nabla_i \mathcal{C}_i(x)$ with respect to $x_{-i}$, we have $L={\rm{max}}\{ |b_1|,|b_2|\}$.
%
Define $g(x) = (\nabla_1 \mathcal{C}_1(x), \nabla_2 \mathcal{C}_2(x))$ and let $G(x)$ denote the Jacobian of $g(x)$, i.e., $G(x)=[a_1,b_1;b_2,a_2]$. 
According to \cite{rosen1965existence}, the strong monotonicity parameter $m$  coincides with the smallest eigenvalue of the matrix $\frac{G(x)+G'(x)}{2}$. 

We validate our methods for three different selections of parameters $\theta^k:=(a_1^k,a_2^k,b_1^k,b_2^k,e_1^k,e_2^k)$ for $k=1,2,3$. Specifically, We select $\theta^1 = (1,1,0.6,-0.5,1.2,0.8)$, $\theta^2 = (1,1,1,-1,1.2,0.8)$ and $\theta^3 = (1,1,2,-1,1.2,0.8)$. It is easy to verify that $\theta^1$, $\theta^2$ and $\theta^3$ correspond to the cases $m>L\sqrt{N-1}$, $m=L\sqrt{N-1}$, and $m<L\sqrt{N-1}$, respectively. The convergence results are shown in Figure~\ref{fig_TI}. We observe that when $m>L\sqrt{N-1}$, the best response converges with exponential rate. When $m\leq L\sqrt{N-1}$, the best response algorithm fails to converge, which indicates the tightness of our theoretical results.

\begin{figure}[t] 
\begin{center}
\centerline{\includegraphics[width=0.9\columnwidth]{Time-invariant.pdf}}
\caption{Convergence of the best response algorithm for time-invariant games.}
\label{fig_TI}
\end{center}
\vskip -0.2in
\end{figure}



\subsection{Time-varying games}
For the time-varying case, the loss function of agent $i$ is defined as $\mathcal{C}_{i,t}(x) = x_i(\frac{a_i x_i}{2} + b_{i,t} x_{-i} - e_{i,t})+ 1$, where $a_i=2$, $i=1,2$, and $b_{i,t}$, $e_{i,t}$ are time-varying parameters. The time-varying parameters are selected as 
\begin{align*}
    b_{i,t}=\left\{ \begin{array}{cc} 0.3+ 0.1\times (-1)^t  & t\in[1,T^{0.6}] \\
    0.3 & t\in(T^{0.6},T] \end{array},\right.
\end{align*}
\begin{align*}
    e_{i,t}=\left\{ \begin{array}{cc} 0.4 & t\in[1,T^{0.6}] \\
    0.4+ 0.1\times (-1)^t t^{-1/4}  & t\in(T^{0.6},T] \end{array}.\right.
\end{align*}
%
We select $T=1000$ and thus $T^{0.6}\approx63$.
It can be verified that the selection of parameters yields $m_t\geq L_t \sqrt{N-1}$ for $\forall t$, and $V_T = \mathcal{O}(T^{3/4})$, $W_{i,T} = \mathcal{O}(T^{3/4})$, $i=1,2$. Figures \ref{fig_tracking}--\ref{fig_DR}  illustrate the equilibrium tracking  error and the dynamic regret of the best response algorithm, respectively. We observe that, when $t\in[1,T^{0.6}]$, both the equilibrium tracking error and the dynamic regret grow rapidly due to the oscillations of $b_{i,t}$; when $t\in(T^{0.6},T]$, they grow slowly since $b_{i,t}$ is a constant and the variation of $e_{i,t}$ is decreasing over time.
Moreover, both the equilibrium tracking error and the dynamic regret are sub-linear in the total number of episodes, which supports our theoretical results.


\begin{figure}[t]
\begin{center}
\centerline{\includegraphics[width=0.9\columnwidth]{Tracking.pdf}}
\caption{Equilibrium tracking error of the best response algorithm for time-varying games. }
\label{fig_tracking}
\end{center}
\vskip -0.3in
\end{figure}

\begin{figure}[t]
\begin{center}
\centerline{\includegraphics[width=0.9\columnwidth]{DR.pdf}}
\caption{Dynamic regret of the best response algorithm for time-varying games.}
\label{fig_DR}
\end{center}
\vskip -0.3in
\end{figure}