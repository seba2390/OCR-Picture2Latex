% Template for ICASSP-2021 paper; to be used with:
%          spconf.sty  - ICASSP/ICIP LaTeX style file, and
%          IEEEbib.bst - IEEE bibliography style file.
% --------------------------------------------------------------------------
\documentclass{article}

\usepackage[hidelinks]{hyperref}
\usepackage{spconf, amsmath, bm, graphicx, cite, siunitx, booktabs, listings, inconsolata, multicol, enumitem, float}
\usepackage[acronym]{glossaries}
\usepackage[capitalize]{cleveref}
\usepackage{tikz}
\usepackage{doi}

%\usepackage{showframe}

\sisetup{detect-weight=true}

\input{acronyms.tex}

% Example definitions.
% --------------------
\def\x{{\mathbf x}}
\def\L{{\cal L}}

% Title.
% ------
\title{POSITNN: TRAINING DEEP NEURAL NETWORKS WITH MIXED LOW-PRECISION POSIT}
%
% Single address.
% ---------------
%\name{Author(s) Name(s)\thanks{Thanks to XYZ agency for funding.}}
%\address{Author Affiliation(s)}
\name{Gonçalo Raposo \qquad Pedro Tomás \qquad Nuno Roma }
\address{INESC-ID, Instituto Superior Técnico, Universidade de Lisboa, Portugal}
%
% For example:
% ------------
%\address{School\\
%	Department\\
%	Address}
%
% Two addresses (uncomment and modify for two-address case).
% ----------------------------------------------------------
%\twoauthors
%  {A. Author-one, B. Author-two\sthanks{Thanks to XYZ agency for funding.}}
%	{School A-B\\
%	Department A-B\\
%	Address A-B}
%  {C. Author-three, D. Author-four\sthanks{The fourth author performed the work
%	while at ...}}
%	{School C-D\\
%	Department C-D\\
%	Address C-D}
%
%\renewcommand{\baselinestretch}{0.96}

\newcommand\mynotice[1]{
	\begin{tikzpicture}[remember picture,overlay]
	\node[anchor=north,yshift=-10pt] at (current page.north) {\parbox{\dimexpr\textwidth-\fboxsep-\fboxrule\relax}{#1}};
	\end{tikzpicture}
}

\newcommand\mycopyrightnotice[1]{
	\begin{tikzpicture}[remember picture,overlay]
	\node[anchor=south,yshift=10pt] at (current page.south) {\parbox{\dimexpr\textwidth-\fboxsep-\fboxrule\relax}{#1}};
	\end{tikzpicture}
}

\begin{document}
	%\ninept
	%
	\maketitle
	%
	\mynotice{\centering\small Published in ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). \doi{10.1109/ICASSP39728.2021.9413919}}
	\mycopyrightnotice{
		\footnotesize \textcopyright~2021 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works.}
	\begin{abstract}
		Low-precision formats have proven to be an efficient way to reduce not only the memory footprint but also the hardware resources and power consumption of deep learning computations.  Under this premise, the posit numerical format appears to be a highly viable substitute for the IEEE floating-point, but its application to neural networks training still requires further research. Some preliminary results have shown that 8-bit (and even smaller) posits may be used for inference and 16-bit for training, while maintaining the model accuracy. The presented research aims to evaluate the feasibility to train deep convolutional neural networks using posits. For such purpose, a software framework was developed to use simulated posits and quires in end-to-end training and inference. This implementation allows using any bit size, configuration, and even mixed precision, suitable for different precision requirements in various stages.
		The obtained results suggest that 8-bit posits can substitute 32-bit floats during training with no negative impact on the resulting loss and accuracy.
	\end{abstract}
	%
	\begin{keywords}
		Posit numerical format, low-precision arithmetic, deep neural networks, training, inference
	\end{keywords}
	%
	%\vspace{-0.5\baselineskip}
	\section{Introduction}
	\label{sec:intro}
	%\vspace{-0.5\baselineskip}
	
	\gls{dl} is, nowadays, one of the hottest topics in signal processing research, spanning across multiple applications. This is a highly demanding computational field, since, in many cases, better performance and generality result in increased complexity and deeper models \cite{Thompson2020}. For example, the recently published language model GPT-3, the largest ever trained network with 175 billion parameters, would require 355 years and \$4.6M to train on a Tesla V100 cloud instance\cite{Li2020}. Therefore, it is increasingly important to optimize the energy consumption required by the training process. Although algorithmic approaches may contribute to these goals, computing architectures advances are also fundamental \cite{Schmidhuber2015}.
	
	The computations involved in \gls{dl} mostly use the \glsunset{ieee754}\acrshort{ieee754} \gls{sp} \gls{fp} format \cite{IEEE2019}, with 32 bits. However, recent research has achieved comparable precision with smaller numerical formats. The novel posit format \cite{Gustafson2017}, designed as a direct drop-in replacement for float (i.e., IEEE SP FP), provides a wider dynamic range, higher accuracy, and simpler hardware. Moreover, each posit format has a corresponding exact accumulator, named quire, which is particularly useful for the frequent dot products in \gls{dl}.
	
	Contrasting with the \acrshort{ieee754} FP, the posit numerical format may be used with any size and has been shown to be able to provide more accurate operations than floats, while using fewer bits. Posits may even use sizes that are not multiples of 8, which could be exploited in \gls{fpga} or \gls{asic} to obtain optimal efficiency and performance.
	
	%\begin{itemize}
	%\item many deep learning applications nowadays
	%\item computations are expensive
	%\item deeper networks and models don't just take a long time to be trained but consume a lot of energy
	%\item computing with low-precision numerical formats is a promising method to accelerate the computations
	%\item deep learning involves many iterations and 32-bit float precision isn't always necessary
	%\item Gustafson proposed posit as a substitute to float \cite{Gustafson2017}
	%\item when used correctly, posits are more accurate than floats and with fewer bits
	%\item quire is the proposed accumulator for posits, useful for operations such as matrix multiplications and convolutions
	%\item posits may have any bit size, so FPGA or ASIC units could be designed to operate with custom sizes
	%\end{itemize}
	
	%    \subsection*{Related work}
	
	However, most published studies regarding the application of the posit format to \glspl{dnn} rely on the inference stage \cite{Cococcioni2018, Johnson2018, Langroudi2018, Carmichael2019, Carmichael2019a, Langroudi2019, Langroudi2020}. The models are trained using floats and are later quantized to posits to be used for inference. Nevertheless, the inference phase tends to be less sensitive to errors than the training phase, making it easier to achieve good performance using \{5..8\}-bit posits.
	
	In contrast, exploiting the use of posits during the training phase is a more compelling topic since this is the most computationally demanding stage. The first time posits were used in this context was in \cite{Montero2019}, by training a \gls{fcnn} for a binary classification problem using \{8, 10, 12, 16, 32\}-bit posits. Later, in \cite{Langroudi2019b, Langroudi2019a}, a \gls{fcnn} was trained for MNIST and Fashion MNIST using \{16, 32\}-bit posits. In \cite{Lu2019, Lu2020}, \glspl{cnn} were trained using a mix of \{8, 16\}-bit posits, but still relying on floats for the first epoch and layer computations. More recently, in \cite{Murillo2020}, a \gls{cnn} was trained for CIFAR-10 but using only \{16, 32\}-bit posits.
	
	%\begin{itemize}
	%\item Inference \cite{Cococcioni2018, Johnson2018, Langroudi2018, Carmichael2019, Carmichael2019a, Langroudi2019, Langroudi2020}
	
	%\item Training \cite{Montero2019, Langroudi2019b, Langroudi2019a, Lu2019, Lu2020, Murillo2020}
	%\end{itemize}
	
	%    \subsection*{Original contributions}
	
	Under the premise of these previous works, the research that is now presented goes a step further by extending the implementation of \glspl{dnn} in a more general and feature-rich approach. Hence, the original contributions of this paper are:
	
	\begin{itemize}[leftmargin=*]
		\item \textbf{open-source framework}\footnote{\href{https://github.com/hpc-ulisboa/posit-neuralnet}{Available at: https://github.com/hpc-ulisboa/posit-neuralnet}} to natively perform inference and training with posits of any precision (number of bits and exponent size) and quires; it was developed in C++ and adopts a similar API as PyTorch, with multithread support;
		
		\item adaptation of the framework to support \textbf{mixed-precision}, with different stages (forward, backward, gradient, optimizer, and loss) operating under different posit formats;
		
		%\item \textbf{identification of the \gls{cnn} training stages} that are more sensible to errors and thus require wider formats (optimizer and loss).
		
		%\item identification of the \textbf{more sensible \gls{cnn} training stages} (optimizer and loss), which required wider formats.
		
		\item training \glspl{cnn} with only \textbf{8 to 12-bit posits} without impacting on the achieved model accuracy.
	\end{itemize}
	
	%\begin{itemize}
	%\item framework for inference and training of neural networks using posits with an API and flow similar to PyTorch
	%\item frameworks in C++ and multithreaded to speed up computations
	%\item posits and quires are software simulated with a parameterized library that allows to use any posit configuration, from 3 bits until 256 bits or more
	%\item frameworks allows to use mixed precision (different posit configurations) on any stage or layer
	%\item evaluation of training and inference using posits and quires with CNNs
	%\item exploiting the use of low-precision posits and mixed precision during training and inference. this allows to reduce the precision of certain stages or layers to 8-bits or less during training.
	%\item CNNs: LeNet-5 and AlexNet
	%\item datasets: MNIST, Fashion MNIST, CIFAR-10, CIFAR-100
	%\end{itemize}
	
	\section{Posit numbering system}
	
	Among the several different numbering formats that have been proposed to represent real numbers~\cite{Sousa2020}, the \gls{ieee754} single-precision floating-point (float) is the most widely adopted. It decomposes a number into a sign (1-bit), exponent (8-bits) and mantissa (23-bits):
	\begin{equation}
	f = \left(-1\right)^\text{sign} \times \text{2}^{\text{exponent}-127} \times \text{mantissa}.
	\label{eq:float}
	\end{equation}
	However, it has also been observed that many application domains do not need nor make use of the total accuracy and wide dynamic range that is made available by \gls{ieee754}, often compromising the resulting system optimization in terms of hardware resources, performance, and energy efficiency. One of such domains is \gls{dnn} training, where most of the computations are zero-centered.
	
	To overcome these issues, the Posit numbering system~\cite{Gustafson2017} was recently proposed as a new alternative to \gls{ieee754}. Posit is characterized by a fixed size/\gls{nbits} and an \gls{es}, being composed by the following fields: sign (1-bit), regime (variable bits), exponent (0..\gls{es}-bits), and fraction (remaining bits)~\cite{Group2018}. It is decoded as in \cref{eq:posit}.
	%\vspace*{-\baselineskip}
	\begin{equation}
	p = \left(-1\right)^\text{sign} \times 2^{2^\textit{es} \times k} \times 2^\text{exponent} \times \left( 1 + \text{fraction} \right).
	\label{eq:posit}
	\end{equation}
	
	When the number is negative, the two's complement has to be applied before decoding the other fields. The regime bits are decoded by measuring $k$, determined by their run-length.
	
	%Contrasting to IEEE FP numbers, posits only have 2 special values: $0$ and $\pm\infty=\mathrm{NaR}$ (Not a Real). The exception handling is also much simpler since posits do not overflow nor underflow and have only one exception: $\mathrm{NaR}$.
	
	A particular characteristic of Posit, and perhaps the most interesting aspect for \acrshort{dnn} applications, refers to the distribution of its values, resembling a log-normal distribution (see \cref{fig:posit_distribution}), which is similar to the normal distribution of the values commonly found in \glspl{dnn}.
	%
	%The posit format results in numbers near to $\pm1$ having more accuracy than very large or very small numbers -- tapered accuracy. The distribution of its values resembles a log-normal distribution, as shown in \cref{fig:posit_distribution}. Most of the data involved in \glspl{dnn} approximate to normal distributions, so posit appears to be an appropriate format.
	%
	Another interesting point is the definition of the quire, a Kulisch-like large accumulator~\cite{Kulisch2012} designed to contain exact sums of products of posits. \cref{tab:posit_quire} shows the recommended posit and quire configurations.
	
	
	%\begin{itemize}
	%\item explain posit numerical format
	%\item posit dynamic range
	%\item advantages compared to floats
	%\item explain quire
	%\end{itemize}
	
	% \section{Numerical formats}
	
	%     While multiple formats have been proposed to represent real number \cite{Sousa2020}, the most common is the IEEE SP FP, which decomposes a number in a sign (1 bit), exponent (8 bits) and mantissa (23 bits):
	
	%     \begin{equation}
	%         f = \left(-1\right)^\text{sign} \times \text{2}^{\text{exponent}-127} \times \text{mantissa}.
	%         \label{eq:float}
	%     \end{equation}
	%     It offers an evenly distributed accuracy and a wide dynamic range. However, certain computations might not need as much accuracy, such as \gls{dnn} training, where most of the computations are zero-centered. Computations with large formats also spend more energy and time, not to mention the redundant/special values and exception handling that float entails. 
	
	%     %\begin{itemize}
	%         %\item explain float numerical format
	%         %\item float dynamic range
	%         %\item it might be too powerful to some applications
	%     %\end{itemize}
	
	%     \subsection{Posit}
	
	%         To overcome the IEEE FP issues, John L. Gustafson proposed in 2017 the posit format \cite{Gustafson2017}. It is characterized \cite{Group2018} by a fixed size/\gls{nbits} and \gls{es}, and composed by the fields: sign (1 bit), regime (variable bits), exponent (0..\gls{es} bits), and fraction (remaining bits). If the number is negative, one has to take the two's complement before decoding the other fields. The regime bits are decoded by measuring $k$, determined by their run-length. Lastly, the posit may be decoded as in \cref{eq:posit}.
	% \iffalse
	%         \begin{table}[htb]
	% 		 	\centering
	% 		 	\caption{Example of run-length $k$ for 3-bit regime field.}
	% 		 	\label{tab:regime_bits}
	% 		 	\begin{tabular}{lcccccc} \toprule
	% 		 		Binary & 000 & 001 & 01x & 10x & 110 & 111 \\ \midrule
	% 		 		$k$ & -3 & -2 & -1 & 0 & 1 & 2 \\ \bottomrule
	% 		 	\end{tabular}
	% 		\end{table}
	% \fi
	%         \begin{equation}
	%             p = \left(-1\right)^\text{sign} \times 2^{2^\textit{es} \times k} \times 2^\text{exponent} \times \left( 1 + \text{fraction} \right).
	%             \label{eq:posit}
	%         \end{equation}
	
	% 		Contrasting to IEEE FP numbers, posits only have 2 special values: $0$ and $\pm\infty=\mathrm{NaR}$ (Not a Real). The exception handling is also much more simpler since posits do not overflow nor underflow and have only one exception: $\mathrm{NaR}$.
	
	% 		Moreover and perhaps the most interesting point for \acrshort{dnn} applications is that its distribution of values resembles a log-normal distribution (see \cref{fig:posit_distribution}), which is very similar to the normal distribution of values commonly found in \acrshort{dnn}.
	
	% 		%The posit format results in numbers near to $\pm1$ having more accuracy than very large or very small numbers -- tapered accuracy. The distribution of its values resembles a log-normal distribution, as shown in \cref{fig:posit_distribution}. Most of the data involved in \glspl{dnn} approximate to normal distributions, so posit appears to be an appropriate format.
	
	% 		Another interesting point is the definition of the quire, a Kulisch-like large accumulator \cite{Kulisch2012} designed to contain exact sums of products of posits. \cref{tab:posit_quire} shows the recommend posit and quire configurations \cite{Group2018}.
	% 		\begin{figure}[htb]
	%             \centering
	%             \includegraphics[width=1\columnwidth]{Figures/posit_8_0_hist.pdf}
	%             \caption{Distribution of posit(8, 0) in linear and log scale.}
	%             \label{fig:posit_distribution}
	%         \end{figure}
	%         \begin{table}[htb]
	%             \centering
	%             \caption{Main properties of posit formats according to \cite{Group2018}.}
	%             \label{tab:posit_quire}
	%             \begin{tabular}{@{}lcccc@{}}
	%                 \toprule
	%                 nbits & $8$ & $16$ & $32$ & $64$ \\ \midrule
	%                 es & $0$ & $1$ & $2$ & $3$ \\
	%                 dynamic range & $2^{\pm 6}$ & $2^{\pm 28}$ & $2^{\pm 120}$ & $2^{\pm 496}$ \\
	%                 quire bits & $32$ & $128$ & $512$ & $2048$ \\
	%                 dot product limit & $127$ & $32767$ & $2^{31}-1$ & $2^{63}-1$ \\ \bottomrule
	%             \end{tabular}
	%         \end{table}
	
	%         %\begin{itemize}
	%             %\item explain posit numerical format
	%             %\item posit dynamic range
	%             %\item advantages compared to floats
	%             %\item explain quire
	%         %\end{itemize}
	
	\section{Deep learning posit framework}
	
	Current \gls{dnn} frameworks (such as PyTorch and TensorFlow/Keras) do not natively support the posit data type. As a result, the whole set of functions and operators would need to be reimplemented, in order to take advantage of this new numbering system. As such, it was decided to develop an entirely new framework, from scratch, in order to ensure better control of its inner operations and exploit them for the posit data format.
	
	\subsection{PositNN Framework}
	
	The developed framework, named PositNN, was based on the PyTorch API for C++ (LibTorch), thus inheriting its program functions and data flow. As a result, any user familiar with PyTorch may easily port their networks and models to PositNN. As an example, a comparison between PyTorch and the proposed framework regarding the declaration of a 1-layer model is shown in \cref{fig:framework} (left and center). The overall structure and functions are very similar, the only difference being the declaration of the backward function, since the proposed framework does not currently support automatic differentiation.
	
	Despite being compared against a full-fledged framework, like PyTorch, the proposed framework is also capable of performing \gls{dnn} inference and training with the most common models and functions. A complete list of the supported functionalities is shown in \cref{fig:framework} (right), which allow implementing all the stages illustrated in \cref{fig:diagram}. Thus, common \glspl{cnn}, such as LeNet-5, CifarNet, AlexNet, and others, are fully supported. Moreover, the framework allows the user to extend it with custom functions or combine it with existing ones (e.g.\ from PyTorch).
	
	\begin{table}[t]
		\vspace*{-0.5\baselineskip}
		\centering
		\caption{Main properties of posit formats according to \cite{Group2018}.}
		\label{tab:posit_quire}
		\begin{tabular}{@{}lcccc@{}}
			\toprule
			nbits & $8$ & $16$ & $32$ & $64$ \\ \midrule
			es & $0$ & $1$ & $2$ & $3$ \\
			dynamic range & $2^{\pm 6}$ & $2^{\pm 28}$ & $2^{\pm 120}$ & $2^{\pm 496}$ \\
			quire bits & $32$ & $128$ & $512$ & $2048$ \\
			dot product limit & $127$ & $32767$ & $2^{31}-1$ & $2^{63}-1$ \\ \bottomrule
		\end{tabular}
	\end{table}        
	\begin{figure}[t]
		\centering
		\includegraphics[width=1\columnwidth]{Figures/posit_8_0_hist_v3.pdf}
		\vspace*{-1.5\baselineskip}
		\caption{Distribution of posit(8, 0) in linear and log scale.}
		\vspace*{-0.5\baselineskip}
		\label{fig:posit_distribution}
	\end{figure}
	
	\begin{figure*}[htb]
		%\vspace*{-\baselineskip}
		\begin{minipage}[b]{.69\textwidth}
			%\scalebox{0.9}{\input{code.tex}}
			{\input{code.tex}}
		\end{minipage}%
		\hfill
		\fbox{\begin{minipage}[b]{.28\textwidth}
				\footnotesize
				\begin{itemize}[leftmargin=*, itemindent=-2ex, itemsep=0.5em]
					\item {\bf Activation functions:}\\ ReLU, Sigmoid, TanH
					\item {\bf Layers:}\\  Linear, Convolutional, \\ Average and Maximum Pooling,\\ Batch Normalization, Dropout
					\item {\bf Loss functions:}\\ Cross Entropy,\\ \gls{mse}
					\item {\bf Optimizer:}\\ \gls{sgd}
					\item {\bf Utilities:}\\ StdTensor, convert PyTorch tensors, mixed precision tensor, save and load model, scaled gradients
				\end{itemize}
		\end{minipage}}
		\caption{Comparison of PyTorch (left) and the proposed framework (center). Implemented functionalities of PositNN (right).}
		\vspace*{-1\baselineskip}
		\label{fig:framework}
	\end{figure*}
	
	%\input{functionalities.tex}
	
	\begin{figure}[htb]
		\centering
		\includegraphics[width=.9\columnwidth]{Figures/diagram.pdf}
		\vspace*{-0.5\baselineskip}
		\caption{\gls{dnn} training diagram, starting at the dataset. The various $p_i$, with $i=\{1..5\}$, represent the different posit precisions that may be used throughout the proposed framework.}
		\vspace*{-0.5\baselineskip}
		\label{fig:diagram}
	\end{figure}
	
	\subsection{Posit variables}
	
	Among the several libraries already available to implement posit operators in software \cite{NGATeam2019}, the Universal\footnote{Available at: \href{https://github.com/stillwater-sc/universal}{https://github.com/stillwater-sc/universal}} library was selected, thanks to its comprehensive support for any posit configuration and quires. Furthermore, C++ classes and function templates are generically used to implement different posits. Therefore, declaring a posit(8, 0) variable \texttt{p} equal to 1 is as simple as:
	\begin{lstlisting}[language=C++, basicstyle=\small\ttfamily, frame=leftline]
#include <universal/posit/posit>
sw::unum::posit<8, 0> p = 1;
	\end{lstlisting}
	Moreover, all the main operations specified in the Posit Standard \cite{Group2018} are fully supported and implemented. Furthermore, the proposed framework adopts bitwise operations whenever possible, thus avoiding operating with intermediate float representations, since this could introduce errors regarding a native implementation.
	
	%\begin{itemize}
	%\item explain universal library
	%\item allows for any posit and quire
	%\end{itemize}
	
	\subsection{Implementation}
	
	Posit tensors are stored as StdTensors, a class implemented using only the C++ Standard Library. Data is internally stored in a one-dimensional dynamic vector and the multidimensional strides are automatically accounted for.
	
	Given that some stages are more sensitive to numerical errors, the proposed framework supports different precisions per stage, as depicted in the arrows of \cref{fig:diagram}. Although not illustrated, it even allows the model to use different precisions per layer. To accomplish that, the weights are stored in a class where members are copies with different posit configurations. Hence, each layer and stage converts its posit tensors to the appropriate precisions and seamlessly updates the copies after every change. It also has the option to use quires for the accumulations, significantly improving the accuracy of matrix multiplications, convolutions, etc.
	
	In order to take the maximum advantage of the \acrshort{cpu}, most functions were conveniently parallelized and implemented with multithread support, thus dividing each mini-batch by different workers. In matrix multiplication, this corresponds to splitting the left operand by rows, performing the computation, and then concatenating the results. The threads were implemented using std::thread.
	
	The proposed framework could also be adapted to support other data types, since most functions are independent of the posit format, except those that use the quire to accumulate.
	
	%\begin{itemize}
	%\item diagram of framework
	%\item explain why existing frameworks couldn't be used
	%\item briefly explain forward, backward, gradient, loss, optimizer
	%\item how mixed precision is implemented
	%\item multithreading
	%\item implemented layers and functions
	%\item could be adapted with a small effort to other custom formats
	%\end{itemize}
	
	%\input{tables.tex}
	
	\vspace{-0.5\baselineskip}
	\section{Experimental Evaluation}
	\vspace{-0.5\baselineskip}
	
	By making use of the developed framework, the presented research started by studying how much can the posit precision be decreased without penalizing the \gls{dnn} model accuracy. Then, the best configuration was used to train a deeper model on a more complex dataset. In this evaluation, small accuracy differences ($<1\%$) were assumed to be caused solely by the randomness of the training process and not exactly by lack of precision of the numerical format.
	
	For the initial evaluation, the 5-layer \gls{cnn} LeNet-5 was trained on Fashion MNIST (a more complex dataset than the ordinary MNIST) during 10 epochs. Just as in \cite{Langroudi2019a, Murillo2020}, posit(16, 1) was first used everywhere and decreased until posit(8, 0) (see \cref{tab:same_posit_without_quire}).
	\begin{table}[b]
		\vspace*{-\baselineskip}
		\centering
		\caption{Accuracy of LeNet-5 trained on Fashion MNIST using posit and without quire, using float for reference.}
		\label{tab:same_posit_without_quire}
		\begin{tabular}{@{}l|c|cccc@{}}
			\toprule
			Posit & \textbf{Float} & $(16, 1)$ & $(12, 1)$ & $(10, 1)$ & $(8, 0)$  \\ \midrule
			Accuracy [\%] & \textbf{\num{90.42}} & \num{90.87} & \num{90.15} & \num{88.15} & \num{10.00} \\ \bottomrule
		\end{tabular}
	\end{table}
	
	As expected, posit(16, 1) achieves a float-like accuracy, and narrower precisions, such as posit(12, 1) and posit(10, 1), are also usable for training, the latter incurring in some accuracy loss. However, when trained using posit(8, 0), the model accuracy does not improve and fixes at $\SI{10}{\percent}$ (equivalent to randomly classifying a 10-class dataset), probably due to the narrow dynamic range (as seen in \cref{fig:posit_distribution}). This hypothesis was subsequently evaluated by using a different exponent size (\gls{es}) and using quires for the accumulations (see \cref{tab:es_quire}). The obtained results confirmed the hypothesis, showing that the precision of the 8-bit model slightly increases when using quires, especially when the posit exponent size is $\textit{es}=2$.
	\begin{table}[t]
		%\vspace*{-\baselineskip}
		\centering
		\caption{Accuracy of LeNet-5 trained on Fashion MNIST using posit and quire. Posit8 is tested with different \gls{es}.}
		\label{tab:es_quire}
		\resizebox{\columnwidth}{!}{\begin{tabular}{@{}l|c|cccc@{}}
				\toprule
				Posit with quire & \textbf{Float} & $(10, 1)$ & $(8, 0)$ & $(8, 1)$ & $(8, 2)$ \\ \midrule
				Accuracy [\%] & \textbf{\num{90.42}} & \num{88.40} & \num{13.84} & \num{12.86} & \num{19.39} \\ \bottomrule
		\end{tabular}}
	\end{table}
	
	Another common problem that is particularly noted while using 8-bit posit precisions is the vanishing gradient problem -- the gradients become smaller and smaller as the model converges. This is particularly problematic when the model weights are updated with low-precision posits, since they do not have enough resolution for small numbers. As suggested in \cite{Lu2020}, using 16-bit posits for the optimizer and loss is usually enough to allow models to train with low-precision posits. With this observation in mind, this model was trained with a different precision for the optimizer and loss, while using posit(8, 2) everywhere else (see \cref{tab:mixed}). The posit exponent size \gls{es} was fixed at 2, since it gave the best results and simplified the conversion between posits with different \gls{nbits}.
	\begin{table}[t]
		\vspace*{-0.5\baselineskip}
		\centering
		\caption{Accuracy of LeNet-5 trained on Fashion MNIST using posit, quire, and mixed precision. Configuration OxLy means Optimizer (O) with posit(x, 2) and Loss (L) with posit(y, 2), and everything else with posit(8, 2).} %Posit(8, 2) is used everywhere except for the stages specified in the configuration.
		\label{tab:mixed}
		\resizebox{\columnwidth}{!}{\begin{tabular}{@{}l|c|cccc@{}}
				\toprule
				Configuration & \textbf{Float} & O12L8 & O12L12 & O12L10 & O10L10 \\ \midrule
				Accuracy [\%] & \textbf{\num{90.42}} & \num{88.40} & \num{90.07} & \num{90.25} & \num{88.08} \\ \bottomrule
		\end{tabular}}
		%\vspace*{-\baselineskip}
	\end{table}
	
	The obtained results showcase the feasibility of using 8-bit posits, achieving an accuracy very close to 32-bits \acrshort{ieee754}. In particular, while solely computing the optimizer with posit(12, 2) is not enough to achieve a float-like accuracy, when the loss precision is also increased, the model is able to train without any accuracy penalization and using, at most, 12-bit posits. Conversely, if posit(10, 2) is used for both the optimizer and loss, the final accuracy slightly decreases. Therefore, the configuration with 12 bits for optimizer and 10 bits for loss (O12L10 in \cref{tab:mixed}) offers the best compromise in terms of low-precision and overall model accuracy. This configuration will be referred to as posit(8, 2)*, since the loss function and weight update, both computed with higher precision, only represent about \SI{15}{\percent} of the operations that are performed while training the considered models. 
	
	Given the promising results for the Fashion MNIST dataset, the posit(8, 2)* configuration was also used to train LeNet-5 on MNIST and CifarNet on CIFAR-10, validating the proposed mixed configuration. The resulting accuracies are compared against float in \cref{tab:results}. Moreover, a plot of the training progress of LeNet-5 on Fashion MNIST is shown in \cref{fig:training}, comparing different posit configurations and float.
	
	\begin{table}[t]
		%\vspace*{-\baselineskip}
		\centering
		\caption{Accuracy of \glspl{cnn} trained on MNIST, Fashion MNIST, and CIFAR-10 using float and posit(8, 2)*.}
		\label{tab:results}
		\begin{tabular}{@{}lccc@{}}
			\toprule
			Dataset & MNIST & Fashion MNIST & CIFAR-10 \\
			CNN & LeNet-5 & LeNet-5 & CifarNet \\ \midrule
			Float [\%] & \num{99.19} & \num{90.42} & \num{70.29} \\
			Posit(8, 2)* [\%] & \num{99.17} & \num{90.25} & \num{68.65} \\ \bottomrule
		\end{tabular}
	\end{table}
	
	%\cref{fig:training} shows the training loss and testing accuracy of LeNet-5 trained on Fashion MNIST throughout epochs. In the top plot is shown how the posit(8, 0) is not able to train since its loss does not decrease. 
	
	\begin{figure}[t]
		\centering
		\includegraphics[width=1\columnwidth]{Figures/plot1.pdf}
		\vspace*{-1\baselineskip}
		\caption{Training loss and testing accuracy of LeNet-5 trained on Fashion MNIST using float and different posit precisions. Posit(8, 2)* corresponds to configuration O12L10 of \cref{tab:mixed}.}
		\label{fig:training}
	\end{figure}
	
	%\begin{itemize}
	%\item explain used \gls{cnn} and dataset
	%\item first all same posit without quire
	%\item all same posit with quire
	%\item all same posit with quire, different exponent
	%\item A common problem during training is the vanishing gradients problem -- the gradients become smaller and smaller as the model converges. Therefore, one may implement the Optimizer (weights update) with posit16 and the rest with posit8.
	%\item different optimizer
	%\item different optimizer and loss
	%\item decrease posit
	%\item decrease forward posit
	%\item other nets and datasets with best configuration
	%\item compare against float
	%\end{itemize}
	
	\section{Conclusion}
	
	A new \gls{dnn} framework (PositNN) supporting both training and inference using any posit precision is proposed. The mixed precision feature allows adjusting the posit precision used in each stage of the training network, thus achieving results similar to float. Common \glspl{cnn} were trained with the majority of the operations performed using posit(8, 2) and showed no significant loss of accuracy on datasets such as MNIST, Fashion MNIST, and CIFAR-10.
	
	%Future work should evaluate the use of different precisions per layer and train deeper models. Hardware implementations of posit for \glspl{dnn} should be evaluated against float implementations in terms of performance and energy consumption. Moreover, posit variants such as Adaptive Posit or others that allow underflow might suit \glspl{dnn} well. 
	
	Future work shall make use of this knowledge and framework to devise adaptable hardware implementations of posit units that may exploit this feasibility to implement low-resource and low-power \gls{dnn} implementations while keeping the same model accuracy.
	
	%\begin{itemize}
	%\item float may be substituted by posit16 without any second thoughts
	%\item using mixed precision, it's possible to train with 8-bit posit or less instead of float
	%\item test hardware implementations of posit with deep learning training
	%\item not allowing underflow might undermine posits in deep learning
	%\end{itemize}
	
	%ADD URLS TO REFERENCES
	
	\section*{Acknowledgments}
	
	Work supported by national funds through Fundação para a Ciência e a Tecnologia (FCT), under the projects\linebreak UIDB/50021/2020 and PTDC/EEI-HAC/30485/2017, and student merit scholarship funded by Fundação Calouste Gulbenkian (FCG).
	
	\vfill\pagebreak
	
	\small
	\bibliographystyle{IEEEbib}
	\bibliography{refs}
	
\end{document}