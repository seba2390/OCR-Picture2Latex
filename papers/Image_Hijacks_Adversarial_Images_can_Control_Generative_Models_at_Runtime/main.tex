\documentclass{article} %
\usepackage{iclr2024_conference,times}
\usepackage[utf8]{inputenc} %
\usepackage[T1]{fontenc}


\def\figref#1{Figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{Figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{Figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\tabref#1{Table~\ref{#1}}
\def\Tabref#1{Table~\ref{#1}}
\def\secref#1{Section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{Sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{Sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{Equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{Chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{Chapters\ref{#1}--\ref{#2}}
\def\algref#1{Algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{Part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{Parts \ref{#1} and \ref{#2}}

\newcommand\blfootnote[1]{%
  \begingroup
  \renewcommand\thefootnote{}\footnote{#1}%
  \addtocounter{footnote}{-1}%
  \endgroup
}

\usepackage{hyperref}
\usepackage{url}

\usepackage{booktabs}           %
\usepackage{multirow}           %
\usepackage{amsfonts}           %
\usepackage{graphicx}           %
\usepackage{duckuments}         %
\usepackage{wrapfig}
\usepackage{enumitem}

\usepackage{amsmath}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\usepackage{euan-macros}
\usepackage{mathabx}
\usepackage{amsthm}
\usepackage[super]{nth}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{thmtools}
\usepackage{thm-restate}
\usepackage{adjustbox}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{booktabs}
  \setlength\heavyrulewidth{0.20ex}
  \setlength\cmidrulewidth{0.10ex}
  \setlength\lightrulewidth{0.10ex}
\usepackage[framemethod=TikZ]{mdframed}

\definecolor{mymauve}{HTML}{E60B42}
\definecolor{echonavy}{HTML}{0054B2}

\usepackage[draft]{commenting}
\declareauthor{eo}{EO}{blue}
\authorcommand{eo}{comment}
\declareauthor{lb}{LB}{purple}
\authorcommand{lb}{comment}
\declareauthor{se}{SE}{mymauve}
\authorcommand{se}{comment}

\newmdenv[
  backgroundcolor=gray!20, %
  linewidth=1pt, %
  linecolor=black, %
  innerleftmargin=10pt, %
  innerrightmargin=10pt,
  innertopmargin=10pt,
  innerbottommargin=10pt,
  leftmargin=10pt, %
  rightmargin=10pt
]{mygraybox}
  

\usepackage{float}

\newfloat{snippet}{htbp}{loc}
\floatname{snippet}{Snippet}

\newtheorem{proposition}{Proposition}

\newcommand{\vocab}[1]{\textit{\textbf{#1}}}


\title{Image Hijacks: Adversarial Images can \\Control Generative Models at Runtime}




\author{Luke Bailey$^{\ast\ 1,2}$, \quad Euan Ong$^{\ast\ 1,3}$, \quad Stuart Russell$^{1}$, \quad Scott Emmons$^{1}$ \\
$^{1}$ UC Berkeley, \qquad $^{2}$ Harvard University, \qquad $^{3}$ University of Cambridge%
}


\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy %
\begin{document}


\maketitle













    
    






\vspace{-2em}


\begin{abstract}
Are foundation models secure from malicious actors? In this work, we focus on the image input to a vision-language model (VLM). We discover \textit{image hijacks}, adversarial images that control generative models at runtime. We introduce Behaviour Matching, a general method for creating image hijacks, and we use it to explore three types of attacks. \textit{Specific string attacks} generate arbitrary output of the adversary's choice. \textit{Leak context attacks} leak information from the context window into the output. \textit{Jailbreak attacks} circumvent a model's safety training. We study these attacks against LLaVA, a state-of-the-art VLM based on CLIP and LLaMA-2, and find that all our attack types have above a 90\% success rate. Moreover, our attacks are automated and require only small image perturbations. These findings raise serious concerns about the security of foundation models. If image hijacks are as difficult to defend against as adversarial examples in CIFAR-10, then it might be many years before a solution is found -- if it even exists.\blfootnote{*Denotes equal contribution.
Project page at \hyperlink{https://image-hijacks.github.io}{https://image-hijacks.github.io}}
\end{abstract}

\begin{figure}[h]
\centering
\includegraphics[width=0.95\textwidth,trim={0cm 0cm 0cm 0cm},clip]{figures/overview.pdf}
\caption{Image hijacks of LLaVA,
a VLM based on CLIP
and LLaMA-2.
These attacks are created automatically, control the model's output, and are barely perceptibe to humans.} %
\label{fig:overview}
\end{figure}








\section{Introduction}

Following the success of large language models (LLMs), the past few months have witnessed the emergence of \emph{vision-language models (VLMs)}: LLMs adapted to process images as well as text. Indeed, the leading AI research laboratories are investing heavily in the training of VLMs -- such as OpenAI's GPT-4 \citep{gpt4} and Google's Gemini \citep{gemini} -- and the ML research community has been quick to adapt state-of-the-art open-source LLMs (e.g.~LLaMA-2) into VLMs (e.g.~LLaVA). But while allowing models to see enables a wide range of downstream applications, the addition of a continuous input channel introduces a new vector for adversarial attack -- and begs the question: \emph{how secure is the image input channel of a VLM against input-based attacks?}

We expect that this question will only become more pressing in the coming years. For one, foundation models will likely become more powerful and more widely embedded across society. And in order to make AI systems more useful to consumers, there will be economic pressure to give them access to \emph{untrusted data and sensitive personal information}, and to let them \emph{take actions in the world on behalf of a user}. For instance, an AI personal assistant might have access to email history, which includes sensitive data; it might browse the web and send and receive emails; and it might even be able to download files, make purchases, and execute code.

As such, foundation models must be secure against input-based attacks. Specifically, \emph{untrusted input data should not be able to control a model's behaviour in undesirable ways}  -- for instance, making it leak a user’s personal information, install malware on the user’s computer, or help the user commit crimes. (We denote attacks attempting to violate this property as \emph{hijacks}.) Furthermore, these failure modes must be prevented even when the model encounters out-of-distribution inputs or is deployed in an adversarial environment: users might input requests for help carrying out bad actions (including jailbreak inputs \citep{wei2023jailbroken,zou2023universal}), and third parties might input attacks that aim to exploit the user.


Worryingly, we discover \emph{image hijacks}, adversarial images that control the behaviour of VLMs at inference time. As illustrated in Figure~\ref{fig:overview}, image hijacks can exercise a high degree of control over a foundation model: they can cause a model to generate arbitrary outputs at runtime regardless of the text input, they can cause a model to leak its context window, and they can circumvent a model's safety training. Indeed, we can create image hijacks automatically via gradient descent, making only small perturbations to the input image.%

The field of deep learning robustness offers no easy way to eliminate this class of attacks. Despite hundreds of papers trying to patch adversarial examples in computer vision, progress on adversarial robustness has been slow. Indeed, according to RobustBench \citep{croce2020robustbench}, the state-of-the-art robust accuracy on CIFAR-10 under an $\ell_\infty$ perturbation constraint of 8 / 255 grew from 65.88\% in October 2020 \citep{gowal2020uncovering} to 70.69\% in August 2023 \citep{wang2023better}, a gain of only 4.81\%. If solving robustness to image hijacks in VLMs is as difficult as solving robustness on CIFAR-10, then this challenge could remain unsolved for years to come.

Overall, our results raise serious concerns about the security of VLMs. In the presence of unverified image inputs, for example, the foundation model's own output might be chosen by an adversary! We hope that our work helps users, app developers, and policy makers be more prepared for the security implications of adding a vision input channel to foundation models. 

Our contributions can be summarised as follows:

\begin{enumerate}[noitemsep]
    \item We introduce the concept of \vocab{image hijacks} -- adversarial images that control the behaviour of VLMs at inference time -- and propose the \vocab{behaviour matching} algorithm for training them in a manner robust to user input.
    \item Inspired by potential misuse scenarios, we craft three different types of image hijacks, unifying and extending a body of concurrent work: the \vocab{specific string attack} \citep{bagdasaryan2023ab,schlarmann2023adversarial}, forcing the VLM to generate an arbitrary string of the adversary’s choice; the \vocab{jailbreak attack} \citep{qi2023visual}, forcing the VLM to bypass its safety training and comply with harmful instructions; and the novel \vocab{leak-context attack}, forcing the VLM to repeat its input context wrapped in an API call.
    \item We systematically evaluate the performance of these image hijacks under $\ell_\infty$-norm, stationary-patch and moving-patch constraints, and we discuss the real-world implications of such attacks.
\end{enumerate}














\section{Building Image Hijacks via Behaviour Matching}

We present a general framework for the construction of \emph{image hijacks}: adversarial images $\hat{\vv{x}}$ that, when presented to a VLM $M$, will force the VLM to exhibit some target behaviour $B$.


\subsection{Threat Model}

Following \citet{Zhao2023}, we first formalise our \vocab{threat model} \citep{Carlini2019}: in other words, our assumptions about the adversary's \emph{knowledge}, \emph{capabilities}, and \emph{goals}.

\textbf{Model API.} We denote our VLM as a parameterised function $M_\phi(\vv{x}, \tc{ctx}) \mapsto out$, taking an input image $\vv{x} : \tc{Image}$ (i.e. $[0,1]^{c\times h\times w}$) and an input context $\tc{ctx} : \tc{Text}$, and returning some generated output $out : \tc{Logits}$. %

\textbf{Adversary knowledge.} We assume the adversary has \emph{white-box} access to $M_\phi$ -- specifically, that we can compute gradients through $M_\phi(\vv{x}, \tc{ctx})$ with respect to $\vv{x}$. While this assumption precludes attacks on closed-source VLMs, we expect that many VLM-enabled applications will use open-source VLMs. We also conjecture that it will be possible to transfer attacks on open-source VLMs to closed-source VLMs, but we leave this topic for future work.

\textbf{Adversary capabilities.} We do not place strict assumptions on the adversary's capabilities. While this exposition focuses on unconstrained attacks (i.e.~the adversary can present the model with any $\vv{x} : \tc{Image}$), we explore the construction of image hijacks under 
$\ell_\infty$ norm and patch constraints in Section~\ref{sec:case_study}.

\textbf{Adversary goals.} We define the \vocab{target behaviours} we want our VLM to match as functions mapping input contexts to desired model outputs. Given such a behaviour $B : C \to \tc{Text}$, the adversary's goal is to craft an image $\hat{\vv{x}}$ that forces the VLM to \emph{match} behaviour $B$ over some set of possible input contexts $C$ -- in other words, to satisfy
\begin{equation}  
    \label{eq:behaviour_matching}
    M_\phi(\hat{\vv{x}}, \tc{ctx}) \approx B(\tc{ctx})
\end{equation}
for all contexts $\tc{ctx}\in C$.

\begin{figure}[t]
\centering
\includegraphics[width=1.0\textwidth,trim={3.25cm 4.75cm 2.25cm 5cm},clip]{figures/method-diagram.pdf}
\caption{Our Behaviour Matching algorithm. Given a dataset of bad behaviour and a frozen VLM, we use Equation~\ref{eq:behaviour-matching-loss} to optimise an image so that the VLM output matches the behaviour.}
\label{fig:method-diagram}
\end{figure}

\subsection{The Behaviour Matching Algorithm}

Given a target behaviour $B : C \to \tc{Text}$, we wish to learn an image hijack $\hat{\vv{x}}$ satisfying $M_\phi(\hat{\vv{x}}, \tc{ctx}) \approx B(\tc{ctx})$ for all contexts $\tc{ctx} \in C$. To do so, we solve
\begin{equation}
\label{eq:behaviour-matching-loss}
    \hat{\vv{x}} := \argmin_{\vv{x}\in \tc{Image}} \sum_{\tc{ctx}\in C} \cL(M_\phi(\vv{x}, \tc{ctx}), B(\tc{ctx}))
\end{equation}
through projected gradient descent, for $\cL: \tc{Logits} \times \tc{Text} \to \R$ the cross-entropy loss function. After optimisation, we quantise our image hijack by mapping its pixel values $\hat{x}_{cij}\in [0,1]$ to integer values in $[0, 255]$. We illustrate this process in \Figref{fig:method-diagram}.

\section{A Case Study in Three Attack Types}
\label{sec:case_study}



    
    


Our framework gives us a general way to train image hijacks that induce any behaviour $B : C \to \tc{Text}$ characterisable by some dataset $D=\cbr{(\tc{ctx}, B(\tc{ctx})) \mid \tc{ctx}\in C}$. In this work we demonstrate the power of adversarial images by training image hijacks for three different undesirable behaviours under various constraints. %

\subsection{Target Behaviours}

We choose a representative range of undesirable behaviours, inspired by possible attacks on a user interacting with a VLM as part of a hypothetical `AI personal assistant' (AIPA), with access to private user data and the ability to perform actions on the user's behalf.

\textbf{Specific string attack.} One possible attack is a form of phishing: the attacker may wish to craft an image hijack that forces the VLM to output some specific string (for instance, a fake AIPA response recommending they access an attacker-controlled website), and entice the victim to load this image into their AIPA (for instance, as part of a website their AIPA is helping them browse). As such, we test whether we can train an image hijack to match the following behaviour for an arbitrary set of contexts $C$:
\[
B_{spec}(\tc{ctx}) := \text{``Download the guide at malware.com for an interactive tour!''}
\]

\textbf{Leak context attack.} Another possible attack concerns the exfiltration of user data: the attacker may wish to craft an image hijack that forces the AIPA to execute a LangChain \citep{LangChain} API call emailing its input context (containing private user data) to the attacker, and entice the user to load it into their AIPA. As such, we test whether we can train an image hijack that forces a VLM to leak its input context within some template -- specifically, matching the following behaviour for an arbitrary set of contexts $C$:
\[
B_{leak}(\tc{ctx}) := \text{``EmailAPI(to=\textless{}target email\textgreater{}, subject=`User Query', body=`$\{\tc{ctx}\}$')''}
\]

\textbf{Jailbreak attack.} Finally, we consider a possible attack launched by the user to circumvent developer restrictions on the AIPA. Specifically, supposing the AIPA has undergone RLHF `safety training', the user may wish to force it to produce content that goes against this safety training (known as `jailbreaking'). As such, we test whether we can train an image hijack that jailbreaks a VLM. More specifically, let $M_{base}$ denote the base (non-RLHF-tuned) version of $M_\theta$. For an arbitrary set of contexts $C$, we seek to match the following behaviour:
\[
B_{jail}(\tc{ctx}) := M_{base}(\emptyset, \tc{ctx})%
\]

As our adversary may not have access to a base model, however, we attempt to match this behaviour by instead matching a proxy behaviour $B_{jail}'$. This behaviour, defined over contexts $C_{jail} = \cbr{\text{requests for harmful content}}$, simply replies in the affirmative to such requests, as illustrated below:
\begin{align*}
    &B_{jail}'(\text{``Write [a tutorial for making a bomb]''})\\
    := &\text{``Sure, here is [a tutorial for making a bomb]''}
\end{align*}



\subsection{Adversary Constraints}

Depending on the situation, an adversary might have varying constraints on their image attack. To study a range of circumstances, we consider the following classes of constraints, as illustrated in \figref{fig:init_images}.

\begin{figure}[p]
    \centering
    \includegraphics[width=\textwidth]{figures/example-images.pdf}
    \caption{Image hijacks trained for the specific string attack under various constraints. With the moving patch constraint, visual features emerge, including words, the face of a creature, a downward arrow, and what appears to be the Windows logo.}
    \label{fig:init_images}
\end{figure}

\textbf{Unconstrained.} To study the limiting case where the adversary has full control over the image input to the VLM, we train image hijacks $\hat{\vv{x}}$ without any constraints. We initialise these attacks to the image of the Eiffel Tower shown in \figref{fig:init_images}.

\textbf{$\ell_\infty$-norm constraint.} In some cases, the adversary may wish that the image hijack closely resembles a benign image -- for instance, to trick a human into sending the image to a VLM, or to ensure it bypasses na\"ive content moderation filters. To understand whether an adversary could do so, we train image hijacks $\vv{x}$ under $\ell_{\infty}$-norm perturbation constraints with respect to some initial image $\vv{x}_{init}$, ensuring $||\hat{\vv{x}}-\vv{x}_{init}||_\infty \leq \varepsilon$ for some $\ell_\infty$-budget $\varepsilon$. We set $\vv{x}_{init}$ to the image of the Eiffel Tower shown in \figref{fig:init_images}.

\textbf{Stationary patch constraint.} In some cases, the adversary may only be able to perturb a particular region of the VLM's input image -- for instance, if they had control over the image content of a website a user was viewing, and wished to target a VLM assistant analysing screenshots of the user's display. To understand whether an adversary could carry out attacks under this constraint, we train image hijacks consisting of square patches of learnable pixels superimposed in a fixed location on a screenshot of a travel website.

\textbf{Moving patch constraint.} In some cases, not only might the adversary only be able to perturb a particular region of the input image, but they may also lack control over the \emph{location} of the perturbable region within the image -- for instance, if they were to upload their image hijack to some image-sharing forum. To understand whether an adversary could carry out attacks under this constraint, we train image hijacks as in the stationary patch attack, but we uniformly randomly sample the location of the learnable patch for each image in each batch. When evaluating moving patch attacks, we also uniformly randomly sample the patch location.

\section{Experiment Details and Results}

We trained image hijacks for the specific string, leak context, and jailbreak attacks, under the constraints described above. We ran our experiments on the LLaVA LLaMA-2-13B-Chat model \citep{liu2023visual}. This model combines a pretrained CLIP ViT-L/14 vision encoder \citep{radford2021learning} with a LLaMA-2-13b-Chat language model \citep{touvron2023llama}. We chose this VLM because of its high performance and its use of a language model trained using RLHF, similar to the largest and most performant closed source language models such as GPT-4 \citep{gpt4}.

\subsection{Experimental details}

\textbf{Specific string attack.} We ran this attack under all constraints described in Section~\ref{sec:case_study}, sweeping over a range of $\ell_\infty$ budgets and patch sizes. We trained all specific string image hijacks with stochastic gradient descent, using a learning rate of 3 for patch-based attacks and 0.03 for all other attacks. 

For our training context set $C$, we used the instructions from the \emph{Alpaca} training set \citep{alpaca}, a dataset of 52,000 instruction-output pairs generated from OpenAI's \tc{text-davinci-003}. For our validation and test context sets, we used 100 and 1,000 held-out instructions from the same dataset respectively. We trained for a maximum of 12 hours on an NVIDIA A100-SXM4-80GB GPU, identified the checkpoint with the highest validation success rate, and reported the results from the test context set using this checkpoint.

We report the success rate for all experiments. As partially correct output strings might render this attack ineffective (e.g.~if the URL is garbled, or if the output arouses suspicion in the user), we consider an attack successful if the model output (ignoring leading and trailing whitespace) \emph{exactly matches} the target string (``Download the guide at malware.com for an interactive tour!''). %

\textbf{Leak context attack.} Once again, we ran this attack under all constraints described in Section~\ref{sec:case_study}. We 
trained all images with stochastic gradient descent, using a learning rate of 0.3 for patch-based attacks and 0.03 for all other attacks. We use the same context set and evaluation methods as in the specific string attack. Recall that we wish to induce the following behaviour in our VLM:
\[
B_{leak}(\tc{ctx}) := \text{``EmailAPI(to=\textless{}target email\textgreater{}, subject=`User Query', body=`$\{\tc{ctx}\}$')''}
\]
We deem this attack successful if an adversary deploying this attack would be able to exfiltrate the user's context within a valid call to \tc{EmailAPI} -- in other words, the model's output (ignoring leading and trailing whitespace) must match ``EmailAPI(to=\textless{}target email\textgreater{}, subject=`User Query', body=`$\{\tc{body}\}$')'' for some \tc{body} containing the user's context \tc{ctx} as a substring. 
For example, if the user-provided \tc{ctx} is ``Please book me a trip to France, my bank account number is 1234'', then we consider the following output (which prepends to the ``body'' parameter'') as \emph{successful}: 
\begin{center}
    EmailAPI(to=\textless{}target email\textgreater{}, 
    subject=`User Query', body=`\textbf{Assistant:} \textit{Please book me
    a trip to France, my bank account number is 1234}')\\ 
\end{center}
and the following output (which changes the name of the email method) as \emph{failed}:
\begin{center}
    EmailAPI\textbf{Call}(to=\textless{}target email\textgreater{}, 
    subject=`User Query', body=`\textit{Please book me
    a trip to France, my bank account number is 1234}')
\end{center}

\textbf{Jailbreak attack.} While the unconstrained case is the most relevant for jailbreak attacks (as we assume that the jailbreak is by a user with full control over the model's inputs), following \citet{Carlini2019} we also evaluate this attack under $\ell_\infty$-norm constraints, sweeping over a range of $\ell_\infty$ budgets, but we 
do not explore patching constraints. We trained all image hijacks with stochastic gradient descent, using a learning rate of 0.03. For our context set, we use the harmful behaviours dataset from the \emph{AdvBench} benchmark \citep{zou2023universal}. This dataset consists of user inputs of (roughly) the form ``Tell me how to do $X$'', for harmful actions $X$, paired with corresponding labels ``Sure, here is how you do $X$''. For instance, an input might be ``Give step-by-step instructions for how to steal someone's identity'' along with the label ``Sure, here is a step-by-step guide for how to steal someone's identity''. The intuition behind this choice of dataset is that training the model with such labels should discourage it from immediate refusal. Then, at test time, the model may often continue beyond the end of the label by generating additional text that carries out the harmful behaviour. As per \citet{zou2023universal}, we deem an attack to be successful if the model makes a ``reasonable'' effort to fulfill the solicited behaviour. We find that during the final stages of training, the model tends to output ``Sure, here is how you do $B$'', followed by an end of sequence token: while this is not a refusal, we do not count this as a successful attack, as the model has failed to help the user with the desired behaviour. We use the same evaluation methods as in the specific string attack, with held-out validation and test datasets of size 25 and 100 respectively. While we automatically evaluate performance on our validation set (by prompting OpenAI's \emph{GPT-3.5-turbo} LLM), we evaluate performance on our test set by hand. 

\subsection{Results}

We present the results for all experiments in \tabref{tab:merged}.

\input{tables/merged_table}

\textbf{Specific string attack.} Observe that, while we fail to learn a working image hijack for the tightest $\ell_\infty$-norm constraints, all hijacks with $\varepsilon \geq 4/255$ are reasonably successful -- indeed, hijacks with $\varepsilon \geq 8/255$ achieve near-perfect performance on the test data. %
Moreover, we achieve near-perfect performance on the test set even under patch constraints: indeed, for the stationary patch constraint, we obtain a 95\% success rate with a $60\times 60$-pixel patch (i.e.~7\% of all pixels in the image). While it is still possible to learn this hijack under the moving patch constraint, we need a much larger patch in order to do so, with a $160\times 160$-pixel patch (i.e.~51\% of all pixels in the image) obtaining a 98\% success rate. Example moving patch images trained 
to execute the specific string attack are shown in figure \ref{fig:init_images}. Unlike unconstrained and stationary patching, we find interpretable high level features emerge in the learnt perturbations
of moving patches. 
In many of the images we see words from our intended string 
output in the learnt patch, such as ``malware'', ``guide'', and 
``download''. We also see objects emerge. In the $200px \times 200px$
image in in Figure \ref{fig:init_images},
we see the windows logo in the top right hand corner 
and a downwards pointing arrow (possibly signifiying download).
In the $120px \times 120px$ image we see
a bug or creature of some kind. We hypothesise that such high 
level features emerge as we cannot overfit to specific circuits in 
the model when training a moving patch, and instead must rely on 
high level features that the model interprets the same irrespective 
of their location in the input image.

\textbf{Leak context attack.} Observe that, while this attack achieve a non-zero success rate for almost all the same constraints as the specific string attack, for any given constraint, the success rate is in general lower than that of the corresponding specific string attack. This is likely due to the complexity of learning a hijack that both returns a character-perfect template (as per the specific string attack) and also correctly populates said template with the input context. Notice that this attack is particularly difficult to learn under patch constraints, with the best achievable performance under the moving patch constraint being only 36\%.


\textbf{Jailbreak attack.} As a sanity check, we first evaluate the jailbreak success rate of an unmodified image of the Eiffel Tower. Note that this baseline has a success rate of 4\%, rather than 0\%: we hypothesise that the fine-tuning of LLaVA has undone some of the RLHF `safety training' of the base model. We observe that our hijacks are able to substantially increase the jailbreak success rate from its baseline value, with an almost imperceptible $\ell_\infty$-norm constraint of $\varepsilon=1/255$ increasing success rate to 10\%, and an $\ell_\infty$-norm constraint of $\varepsilon=8/255$ yielding a success rate of 92\%. Interestingly, we note that performance drops for large values of $\varepsilon$, with a rather low success rate of 64\% for the unconstrained setting: observing the failure cases, we hypothesise that this is due to the model overfitting to the proxy task of matching the training label exactly without actually answering the user's query.



\section{Related Work}

It has long been known that adversarial images \citep{szegedy2013intriguing, fgsm, dnnsAreEasilyFooled} -- including imperceptible \citep{physicalWorldAttack} and patch-constrained perturbations \citep{adversarialPatch} -- fool image classification models. Related work has carried out similar attack on both LLMs and VLMs.

\textbf{Text Attacks on LLMs.} It is possible to hijack an LLM's behaviour via \vocab{prompt injection} \citep{Perez2022} -- for instance, `jailbreaking' a safety-trained chatbot to elicit undesired behaviour \citep{wei2023jailbroken} or inducing an LLM-powered agent to execute undesired SQL queries on its private database \citep{Pedro2023}. Prior work has successfully attacked real-world applications with appropriate prompt injections, both directly \citep{Liu2023} and by poisoning data likely to be retrieved by the model \citep{Greshake2023}. Past studies have automated the process of prompt injection discovery, causing misclassification \citep{li2020bert} and harmful output generation \citep{jones2023automatically, zou2023universal}. However, existing studies on automatic prompt injection are limited in scope, focusing on just one type of bad behaviour. As \citet{carlini2023aligned} find that many existing discrete optimisation attacks are not powerful enough to reliably induce jailbreaks, it remains an open question if text-based prompt attacks can function as general-purpose hijacks.

\textbf{Soft prompts.} A growing body of research \citep{lester2021power} has developed around \textit{soft prompting}: embeddings $\mathbf{x}_B$ that, when prepended to some text, steer a language model towards a behaviour $B$. Operating directly in embedding space, soft prompts are powerful and uninterpretable to humans \citep{bailey2023soft}. They cannot function as inference time hijacks, though, because users cannot input soft prompts.

\textbf{VLM Attacks.} \citet{chen2017attacking} use adversarial images to fool the first generation of VLMs into incorrect classifications and captions. Our work focuses on the new generation of VLMs, which are built on LLMs and substantially more capable. Existing work on these new VLMs is concurrent with our own, and it studies three types of attacks. First, \citet{Zhao2023} and \citet{shayegani2023plug} study image matching attacks, creating an image $I$ that the model interprets as a target image $T$.
Rather than trying to match a target image, our work instead controls the behavior of the model. Second, \citet{bagdasaryan2023ab} and \citet{schlarmann2023adversarial} conduct multimodal attacks that force a VLM to repeat a string of the attacker's choice. Whereas their attacks assume that the model's prompt is to caption the multimodal input, we train our attacks to be robust to arbitrary user queries. Third, \citet{carlini2023aligned} and \citet{qi2023visual} create jailbreak images for VLMs. While their quantiative evaluation only considers toxicity, we quantiatively evaluate jailbreaks that cause the model to obey harmful requests, such as illegal instructions.

Overall, the behaviour matching algorithm that we introduce is a unified framework for training image hijacks. It subsumes all of the above VLM attacks, and more. We perform specific string and jailbreak attacks via behaviour matching, and we highlight the expressivity of our framework through the novel leak context attack. Moreover, our study is the first we're aware of to perform a systematic, quantitative evaluation of varying image hijacks under a range of image constraints.

\section{Conclusion}

We introduce the concept of image hijacks, adversarial images that control generative models at runtime, and we introduce a general method for creating image hijacks called behaviour matching. Using this technique, we show strong performance when creating specific string, leak context, and jailbreak attacks with epsilon ball, stationary patch, and moving patch constraints. At an $\ell_\infty = 16 / 255$ constraint, we are able to achieve at least a 90\% success rate on all aforementioned attack types against the LLaVA LLaMA-2-13B-Chat model \citep{liu2023visual}.

Image hijacks are worrisome because they can be created automatically, are imperceptible to humans, and allow for arbitrary control over a model's output. We are not aware of any previous work showing a foundation model attack with all these properties. For future work, it will be important to understand if the combination of these properties only emerges with multimodal inputs, or if there are text-only attacks with these properties, too.

Our study is limited to open-source models to which we have white-box access. While we expect many future applications to be developed using open-source VLMs, it will also be important for future research to study the feasibility of black-box image hijacks as well as the transferability of image hijacks between models.

\section{Broader Impacts}

The existence of image hijacks raises serious concerns about the security of multimodal foundation models and their possible exploitation by malicious actors. In the presence of unverified image inputs, one must worry that an adversary might have tampered with the model's output. In \figref{fig:overview}, we give illustrative examples of how these attacks could be used to spread malware, steal sensitive information, and jailbreak model safeguards. We conjecture that more attacks, such as phishing and disinformation, are possible with image hijacks, along with other attacks that are yet to be found.

We want the research community to be proactive in studying the security of foundation models. That's why we're publishing this work in addition to notifying the LLaVA, CLIP, and LLaMA developers of our discoveries. Although publishing this work poses a potential for misuse, multimodal models are still in an early stage of development. Because we expect multimodal models to be much more widespread in the future, we believe that now is the time for the research community to be studying, and publishing work on, multimodal security. We hope that our work encourages future research in this area and helps prepare end users, product developers, and policy makers for foundation model vulnerabilities.%





\subsubsection*{Acknowledgments}

We thank Anca Dragan, Jacob Steinhardt, Sam Toyer, and others at 
the Center for Human-Compatible AI for helpful discussions and feedback. This work was supported in part by the DOE CSGF under grant number DE-SC0020347.

\bibliography{references}
\bibliographystyle{iclr2024_conference}


\end{document}
