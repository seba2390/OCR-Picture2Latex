\section{Kalman Filter for state estimation} \label{sec:kf}

We derive the KF from the perspective of probability theory.
We therefore start with a small review of the background concepts. 
The reader who is familiar with probability theory may skip them and resume in \cref{sec:linear_system}.

\subsection{Basic Probability Theory}
\label{sec:background_probability}

A random variable, for example $X$, may be assigned a value representing the outcomes of an experiment. 
We use $p(X=x)$ to denote the probability that the random variable $X$ has value $x$. 
The probabilities of all outcomes sum up to $1$. 
Sometimes we write $p(x)$ instead of $p(X=x)$, for brevity.

As we will show later, one of the important problems in state estimation is the computation of a conditional probability function. 
For instance, if $x$ is the state vector we want to estimate, and $y$ is the measured data, the conditional probability $p(X=x|Y=y)$ (or $p(x|y)$) represents what is the probability of state $X=x$ given the measurement $y$.
The following represent well known equations involving the conditional probability. 
\begin{equation}
	\label{equ:conditional_prob}
	p(x|y) = \frac{p(x\cap y)}{p(y)} = \frac{p(x, y)}{p(y)}, \hspace{3em}
  p(x|y) = \frac{p(y|x)p(x)}{p(y)}.     
\end{equation}
where 
$p(x)$ is called the prior (probability of state $X=x$ without any additional information);
$p(x|y)$ is called the posterior;
$p(y|x)$ is called the likelihood; and
$p(y)$ is called the marginal likelihood which is the total probability of observing $Y=y$.

\begin{remark}\label{remark:ct_prob}
	For a continuous random variable, we use $p(x)$ to denote a Probability Density Function (PDF) of random variable $X=x$ and the probability of $X=x$ is $Pr(x)=p(x)\Delta x$, where $\Delta x > 0$ is a sufficient small value. 
	In this tutorial, we will mostly use $p(x)$ to represent the PDF for a continuous random variable. 
\end{remark}

The joint probability can be generalized to multiple random variables. 
According to the definition of conditional probability in \cref{equ:conditional_prob}, we obtain
\begin{subequations} \label{equ:joint_prob_multiple_random_variables}
	\begin{align}
		p(x_n|x_{n-1},\ldots,x_1) &= p(x_n,x_{n-1},\ldots,x_1)/p(x_{n-1},\ldots,x_1)\\
		p(x_n,x_{n-1},\ldots,x_1)&=	p(x_n|x_{n-1},\ldots,x_1)p(x_{n-1},\ldots,x_1).\label{equ:joint_prob_expand_rule}
	\end{align}
\end{subequations}

An example of a PDF for a continuous random variable is the Gaussian distribution, commonly used to model process and measurement noise.
One of the advantages of Gaussian distribution is that if the prior and the likelihood are Gaussian distributed, the posterior is also Gaussian distributed. Such distribution makes the calculation of expectation and variance simpler, which makes possible the derivation of the KF.
The univariate Gaussian distribution with a mean $\mu$ and variance $\sigma^2$ is defined as
\begin{equation}\label{equ:gaussian}
	p(x)=(2\pi \sigma^2 )^{-\frac{1}{2}}
	\exp\{-\frac{1}{2} \frac{(x-\mu)^2}{\sigma^2}\}.
\end{equation}
If the mean $\mu$ and variance $\sigma^2$ are unknown, we define a conditional probability of $X=x$ conditioned on $\mu$ and $\sigma^2$ with the notation $p(x|\mu,\sigma^2)$.
In addition, we write
$X \sim \mathcal{N}(\mu,\sigma^2)$
to denote that $X$ follows a Gaussian distribution with a mean $\mu$ and variance $\sigma^2$.
The multivariate Gaussian distribution is characterized by a mean vector $\mu$ with the same dimension of state vector $x$ and a covariance matrix $\Sigma$, having the form of  
\begin{equation} \label{equ:multivariate_gaussian}
	p(x)=\det (2\pi \Sigma)^{-\frac{1}{2}}
	\exp\{-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)\}.
\end{equation}
 


\subsection{Linear Discrete-Time Dynamical System with Gaussian Uncertainty} \label{sec:linear_system}

As introduced in \cref{sec:incubator_system}, and exemplified in \cref{ex:incubator_noise}, the general form of a linear discrete-time dynamical system with noise is given as:
\begin{equation}\label{equ:state_space_equation_noise}
		{x}_k =Ax_{k-1}+Bu_k+\epsilon_k, \hspace{3em} 
    y_k =Cx_k+\delta_k,
\end{equation}
where $x_k$ and $x_{k-1}$ are state vectors, $u_k$ is the input vector, and $y_k$ is the measurement. 
This means that previous states such as $x_{k-2}$ or $x_{k-8}$ do not contribute directly to the calculation of state $x_k$ and measurement $y_k$, but they contributed implicitly through $x_{k-1}$. 
In addition, we have the following assumption:
\begin{compactitem}
\item \Cref{equ:state_space_equation_noise} is the best predictor of state $x_k$.
\item The process error $\epsilon_k$ and measurement error $\delta_k$ satisfy $\epsilon_k \sim \mathcal{N}(0,R_k)$ and $\delta_k \sim \mathcal{N}(0,Q_k)$, respectively.
\item The initial state $x_0$ satisfies $x_0 \sim \mathcal{N}(\mu_{x_0},\Sigma_{x_0})$.
\end{compactitem}

We now proceed to show that, under the above assumptions, all quantities in \cref{equ:state_space_equation_noise} follow Gaussian distributions.
This is an important result because it is very easy to obtain the most likely value of a Gaussian distribution, i.e. its mean.
If presented with an analytical expression as in \cref{equ:gaussian}, we can obtain the mean by differentiating the expression and equating it to zero.
Moreover, the covariance is obtained by calculating the second derivative of \cref{equ:gaussian}.

We rewrite \cref{equ:state_space_equation_noise} in terms of $\epsilon_k$ and $\delta_k$:
\begin{equation} \label{equ:transformation_noise}
		\epsilon_k  = {x}_k - (Ax_{k-1}+Bu_k), \hspace{3em}
		\delta_k = y_k - C x_k.
\end{equation}
According to \cref{equ:multivariate_gaussian}, the PDF of $\delta_k$ is
\begin{equation} \label{equ:noise_distribution}
	p(\delta_k) = \det (2\pi Q_k)^{-\frac{1}{2}}\exp\{ -\frac{1}{2}(\delta_k-0)^TQ_k^{-1}(\delta_k-0)\}.
\end{equation}
Substituting \cref{equ:transformation_noise} into \cref{equ:noise_distribution} yields
\begin{equation}
	p(\delta_k) = p(y_k - C x_k) 
	= \det (2\pi Q_k)^{-\frac{1}{2}}\exp\{ -\frac{1}{2}(y_k -Cx_k)^TQ_k^{-1}(y_k -Cx_k)\}
\end{equation}
Which is the expression for the Gaussian distribution $y_k \sim \mathcal{N}(Cx_k,Q_k)$.
Since $x_k$ is an unknown, we write $p(y_k | x_k)$.
Applying the same reasoning to $\epsilon_k$, we get the expression for $x_k \sim \mathcal{N}(Ax_{k-1}+Bu_k,R_k)$.
This shows that the states, measurements, and noise, are all Gaussian distributions.

\begin{remark}\label{rem:distributions}
	All variables in \cref{equ:state_space_equation_noise} are Gaussian distributed, with parameters:
	\begin{equation}
		\begin{split}
			\epsilon_k \sim \mathcal{N}(0,R_k)\\
			\delta_k \sim \mathcal{N}(0,Q_k)\\
			x_k \sim \mathcal{N}(Ax_{k-1}+Bu_k,R_k)\\
			y_k \sim \mathcal{N}(Cx_k,Q_k)
		\end{split}
	\end{equation}
\end{remark}




\subsection{Maximum A Posteriori Estimation}
\label{sec:map}

One of the tasks of state estimation is to calculate the most likely value of an unknown quantity, given its probability distribution (recall \cref{sec:incubator_system}).
For example, in the incubator system, the unknown quantity can be the state of the incubator system (temperature of the heater and the air inside the box), and the known quantity is the measured air temperature.
Maximum a posteriori (MAP) estimate is the estimation of an unknown quantity from a distribution of the unknown quantity. 
It is the most probable state $x$ given measurement data $y$, formulated as:
\begin{equation} \label{equ:map_pre}
	\hat{x} = \argmax_{x} p(x|y).
\end{equation}
Note that, according to \cref{remark:ct_prob}, when the random variable $X$ is continuous, the estimation becomes $\hat{x} = \argmax_{x} p(x|y)\Delta x$.
However, since $\Delta x$ does not depend on $x$, $\argmax_{x} p(x|y)\Delta x$ is equivalent to $\argmax_{x} p(x|y)$.

If we do not have an analytical expression for \cref{equ:map_pre}, we may attempt to reformulate it into an equivalent problem, comprised of terms for which we have analytical expressions.
Substituting Bayes rule in \cref{equ:conditional_prob} into \cref{equ:map_pre}, we obtain the MAP objective function
\begin{equation} \label{equ:map_middle}
	\hat{x} = \argmax_{x} \frac{p(y|x)p(x)}{p(y)},
\end{equation}
and because $p(y)$ is independent of $X$, \cref{equ:map_middle} is simplified to
\begin{equation} \label{equ:map}
	\hat{x} = \argmax_{x} p(y|x) p(x).
\end{equation} 

\subsection{Objective Function of Kalman Filter}

Suppose we have collected a series of historical inputs, $u_{1:k}=u_1,u_2,u_3,\ldots,u_{k-1}$, and measurement outputs, $y_{1:k-1}=y_1,y_2,y_3,\ldots,y_{k-1}$, from a linear discrete time system as in \cref{equ:state_space_equation_noise}.
At time step $k$, we apply an input $u_k$ to the system. 
Then the state of the system advances to the state $x_k$ from state $x_{k-1}$, and we obtain measurement data $y_k$. 
The ultimate outcome of the KF is the most probable state $x$ using the posterior distribution given all data.

Applying the MAP discussed in \cref{sec:map}, we formulate the following
\begin{equation} \label{equ:bel_xt}
	\hat{x}_k = \argmax_{x_k} p(x_k|y_{1:k},u_{1:k}).
\end{equation}

In what follows, we describe the procedure to turn \cref{equ:bel_xt} into a form similar to \cref{equ:map}.
Based on \cref{equ:joint_prob_multiple_random_variables} and \cref{equ:map}, we expand \cref{equ:bel_xt} to
\begin{equation} \label{equ:xt_beyasian_inference}
	\begin{split} 
		\hat{x_k} &= \argmax_{x_k} p(x_k|y_k, y_{1:k-1},u_{1:k}) 
		 =\argmax_{x_k} \frac{p(y_k|x_k,y_{1:k-1},u_{1:k})p(x_k|y_{1:k-1},u_{1:k})}{p(y_k|y_{1:k-1},u_{1:k})}\\
		&=\argmax_{x_k} p(y_k|x_k,y_{1:k-1},u_{1:k})p(x_k|y_{1:k-1},u_{1:k}).
	\end{split}
\end{equation}

According to \cref{rem:distributions}, the distribution of current measurement $y_k$ only depends on the current state $x_k$.
We can therefore conclude that
$p(y_k|x_k,y_{1:k-1},u_{1:k}) = p(y_k|x_k)$
and simplify \cref{equ:xt_beyasian_inference} to:
\begin{equation} \label{equ:map_states}
	\hat{x_k} = \argmax_{x_k} p(y_k|x_k)p(x_k|y_{1:k-1},u_{1:k}).
\end{equation}

Our goal is to derive a recursive calculation of the KF.
So we aim at reducing the problem in \cref{equ:bel_xt} to something that involves the term $p(x_{k-1}|y_{1:k-1},u_{1:k-1})$.
In \cref{equ:map_states}, the term $p(y_k|x_k)$ is already at its simplest form, since it matches exactly the distributions in \cref{rem:distributions}, so we will focus on the term $p(x_k|y_{1:k-1},u_{1:k})$.
Based on \cref{rem:distributions}, we can see that $x_k$ depends on $x_{k-1}$ and $u_k$, so we are looking for a way to relate $y_{1:k-1}$ to $x_{k-1}$.
This is where Chapman-Kolmogorov equation \cite{ross2014a} comes into play.
Applying this equation to $p(x_k|y_{1:k-1},u_{1:k})$ gives:
\begin{equation}\label{eq:chapman}
		p(x_k|y_{1:k-1},u_{1:k}) = \int p(x_k, x_{k-1}|y_{1:k-1},u_{1:k})\,dx_{k-1}.
\end{equation}
Applying Bayes theorem, the term inside the integral can be simplified to 
\begin{equation}
	\begin{split}
		p(x_k, x_{k-1}|y_{1:k-1},u_{1:k}) &= p(x_k|x_{k-1},y_{1:k-1},u_{1:k})p(x_{k-1}|y_{1:k-1},u_{1:k})\\ & =
    p(x_k|x_{k-1},u_{k})p(x_{k-1}|y_{1:k-1},u_{1:k}).
	\end{split}
\end{equation}
Because future input $u_k$ can not affect the past state $x_{k-1}$, we can safely omit it from $p(x_{k-1}|y_{1:k-1},u_{1:k})$, therefore obtaining the term $p(x_{k-1}|y_{1:k-1},u_{1:k-1})$.
To summarise, \cref{eq:chapman} can be simplified to:
\begin{equation} \label{eq:chapman_2}
	p(x_k|y_{1:k-1},u_{1:k}) = \int p(x_k|x_{k-1},u_{k})p(x_{k-1}|y_{1:k-1},u_{1:k-1}) \,dx_{k-1}.
\end{equation}

Finally, we insert \cref{eq:chapman_2} into \cref{equ:map_states}, obtaining the following equivalent formulation:
\begin{equation} \label{equ:obj_expand}
	\begin{split}
		\hat{x_k} &= \argmax_{x_k} p(x_k|y_{1:k},u_{1:k}) \\ &= \argmax_{x} p(y_k|x_k)\int p(x_k|x_{k-1},u_{k})p(x_{k-1}|y_{1:k-1},u_{1:k-1}) \,dx_{k-1}.
	\end{split}
\end{equation} 

The derivation above shows the objective function for KF based on MAP.
Here we summarize what we know about this problem:
\begin{inparaitem}
\item $p(y_k|x_k)$ is the PDF of $\mathcal{N}(Cx_k,Q_k)$ (\cref{rem:distributions}); and
\item $p(x_k|x_{k-1},u_{k})$ is the PDF of $\mathcal{N}(Ax_{k-1}+Bu_k,R_k)$ (\cref{rem:distributions}).
\end{inparaitem}
In addition, here is what we need to know in order to solve the problem: does $p(x_k|y_{1:k},u_{1:k})$ match the PDF of a Gaussian distribution? If it does, the solution is just its mean.

In the interest of solving the problem, let us for now assume that $p(x_k|y_{1:k},u_{1:k})$ indeed matches the PDF of a Gaussian distribution with mean $\mu_{x_{k}}$ and $\Sigma_{x_{k}}$:
$\mathcal{N}(\mu_{x_{k}}, \Sigma_{x_{k}})$.
It follows that the term $p(x_{k-1}|y_{1:k-1},u_{1:k-1})$ in \cref{equ:obj_expand} matches the PDF of $\mathcal{N}(\mu_{x_{k-1}}, \Sigma_{x_{k-1}})$.
As \cref{sec:prediction_phase} will show, these facts are instrumental in showing that the integral term in \cref{equ:obj_expand} will match the PDF of a Gaussian distribution scaled by a factor $\eta$, with mean and covariance as follows: $ \mathcal{N}( \overline{\mu}_{x_k}=Bu_k + A\mu_{x_{k-1}} , \overline{\Sigma}_{x_k} = (R_k + A\Sigma_{x_{k-1}}A^T)^{-1})$.

Then, in \cref{sec:solving_opt_problem}, we show how under the above conclusions, the mean $\mu_{x_{k}}$ and covariance $\Sigma_{x_{k}}$ are computed, from the knowledge of $\mu_{x_{k-1}}, \Sigma_{x_{k-1}}$.
This calculation is the KF.

\subsection{Prediction Phase} \label{sec:prediction_phase} 

In this section, we focus on the term 
$\int p(x_k|x_{k-1},u_{k})p(x_{k-1}|y_{1:k-1},u_{1:k-1}) \,dx_{k-1}$.  
This term represents the prediction phase of KF because it utilizes the previous estimates state $x_{k-1}$ with state transition function to predict state $x_{k}$.
The term $p(x_{k-1}|y_{1:k-1},u_{1:k-1})$ is the posterior of state $x_{k-1}$ given historical data.

Regarding the initial state $x_0$, the posterior is simply $p(x_0)$ since we do not have access to historical data. According to \cref{equ:obj_expand}, 
the posterior of state $x_1$ is 
$
		p(x_1|y_1,u_{1}) \propto p(y_1|x_1)\int p(x_1|x_{0},u_{1})p(x_{0}) \,dx_{0},
$
where $\propto$ means \emph{proportional to}.
% \claudio{What does the notation $\propto$ mean?}
Based on our assumptions in \cref{sec:linear_system}, all the terms, $ p(y_1|x_1)$, $ p(x_1|x_{0},u_{1})$, and $p(x_{0})$ , are Gaussian distributed. 
Thus the posterior of state $x_1$ is also Gaussian distributed. 
By induction, all the posterior of states are Gaussian distributed as well as $p(x_{k-1}|y_{1:k-1},u_{1:k-1})$.

Let us assume the mean and covariance of $p(x_{k-1}|y_{1:k-1},u_{1:k-1})$ to be $\mu_{x_{k-1}}$ and $\Sigma_{x_{k-1}}$.
Furthermore, we use the Gaussian distribution in $\int p(x_k|x_{k-1},u_{k})p(x_{k-1}|y_{1:k-1},u_{1:k-1}) \,dx_{k-1}$ and let $\tilde{x_{k}}=\int p(x_k|x_{k-1},u_{k})p(x_{k-1}|y_{1:k-1},u_{1:k-1}) \,dx_{k-1}$, leading to
\begin{equation} \label{equ:tilde_x}
	\begin{split}
		\tilde{x_{k}} &=\int p(x_k|x_{k-1},u_{k})p(x_{k-1}|y_{1:k-1},u_{1:k-1}) \,dx_{k-1}\\
		&= \gamma \int \exp \! \{
		\begin{aligned}[t]
			 -\frac{1}{2}({x}_k - Ax_{k-1}-&Bu_k)^TR_k^{-1}({x}_k - Ax_{k-1}-Bu_k)\\
			 & -\frac{1}{2}(x_{k-1} -\mu_{x_{k-1}})^T \Sigma_{x_{k-1}}^{-1}(x_{k-1} -\mu_{x_{k-1}})  \}\,dx_{k-1},
		\end{aligned}
	\end{split}
\end{equation}
where $\gamma = \det (2\pi R_k)^{-\frac{1}{2}}\det \Sigma_{x_{k-1}}^{-\frac{1}{2}}$. 
Such $\gamma$ plays the role of normalization. 

Let 
$
	L_k = \frac{1}{2}({x}_k - Ax_{k-1}-Bu_k)^TR_t^{-1}({x}_k - Ax_{k-1}-Bu_k) + \frac{1}{2}(x_{k-1} -\mu_{x_{k-1}})^T  \Sigma_{x_{k-1}}^{-1}(x_{k-1} -\mu_{x_{k-1}}).
$
Then, we have
\begin{equation} \label{equ:predicted_x_k}
	\tilde{x_{k}} =\gamma \int \exp\{-L_k\} \,dx_{k-1}, 
\text{where $L_k$ is quadratic in $x_{k-1}$ and $x_{k}$.}
\end{equation}
We can express $L_k$ in terms of $loss(x_{k-1},x_k)$ and $loss(x_k)$, then we obtain
\begin{equation} \label{equ:L_k}
	L_k = loss(x_{k-1},x_k) + loss(x_k),
\end{equation}
where $loss(x_{k-1},x_k)$ incorporates terms relative to $x_{k-1}$ and $x_k$, $loss(x_k)$ only contains items about $x_k$. 
The decomposition of $L_k$ separates the terms $x_k$ and $x_{k-1}$. 
Such decomposition simplifies \cref{equ:predicted_x_k} to 
\begin{equation} \label{equ:bel_xk_predicted}
	\begin{aligned}
		\tilde{x_{k}} &=\gamma \int \exp\{-L_k\} \,dx_{k-1} =\gamma \int \exp\{-loss(x_{k-1},x_k) - loss(x_k)\} \,dx_{k-1}\\
		&=\gamma \exp\{-loss(x_k)\}  \int \exp\{-loss(x_{k-1},x_k) \} \,dx_{k-1}.
	\end{aligned}
\end{equation}

$loss(x_k)$ and $loss(x_{k-1},x_k)$ are both quadratic in $x_k$ and $x_{k-1}$, which have the form of a Gaussian distribution but without normalization. 
Thus we will rearrange $loss(x_k)$ and $loss(x_{k-1},x_k)$ in terms of mean and covariance of a Gaussian distribution. 

First, we analyze $loss(x_{k-1},x_k)$. 
The covariance of $x_{k-1}$ in $loss(x_{k-1},x_k)$ is the inverse of coefficient of $x_{k-1}^2$ in $L_k$. 
By taking the second derivative of $L_k$ with respect to $x_{k-1}$, we acquire the covariance of $x_{k-1}$ in $loss(x_{k-1},x_k)$. 
Setting the first derivative of $L_k$ with respect to $x_{k-1}$ to zero gives the mean value of $x_{k-1}$ in $loss(x_{k-1},x_k)$. Then, $loss(x_{k-1},x_k)$ is expressed as 
\begin{equation}
	\begin{split}
		\mathit{loss}&(x_{k-1},x_k) =  \frac{1}{2}( x_{k-1} - (A^TR_k^{-1}A+\Sigma_{x_{k-1}}^{-1})	[A^TR_k^{-1}(x_k-Bu_k)+\Sigma_{x_{k-1}}^{-1}\mu_{x_{k-1}}] )^T \\ & (A^TR_k^{-1}A+\Sigma_{x_{k-1}}^{-1})^{-1} 
		(x_{k-1} - (A^TR_k^{-1}A+\Sigma_{x_{k-1}}^{-1})	[A^TR_k^{-1}(x_k-Bu_k)+\Sigma_{x_{k-1}}^{-1}\mu_{x_{k-1}}] ).
	\end{split}
\end{equation}
% \begin{multline}
% 	\mathit{loss}(x_{k-1},x_k) =  \frac{1}{2}( x_{k-1} - (A^TR_k^{-1}A+\Sigma_{x_{k-1}}^{-1})	[A^TR_k^{-1}(x_k-Bu_k)+\Sigma_{x_{k-1}}^{-1}\mu_{x_{k-1}}] )^T (A^TR_k^{-1}A+\Sigma_{x_{k-1}}^{-1})^{-1}
% 		(x_{k-1} - (A^TR_k^{-1}A+\Sigma_{x_{k-1}}^{-1})	[A^TR_k^{-1}(x_k-Bu_k)+\Sigma_{x_{k-1}}^{-1}\mu_{x_{k-1}}] ).
% \end{multline}

We know that the integral of a Gaussian distribution of \cref{equ:multivariate_gaussian} should equal to 1. 
Furthermore, the covariance of $x_{k-1}$ in $loss(x_{k-1},x_k)$ is $\Psi = A^TR_k^{-1}A+\Sigma_{x_{k-1}}^{-1}$. Thus we conclude that
$
\int \det\{2\pi \Psi \} ^{-\frac{1}{2} }  \exp\{-L_k(x_{k-1},x_k) \} \,dx_{k-1} = 1
$
and therefore
$
\int  \exp\{-L_k(x_{k-1},x_k) \} \,dx_{k-1} = \det\{2\pi \Psi \} ^{\frac{1}{2} }.
$
Because $\det\{2\pi \Psi \} ^{\frac{1}{2}}$ has no contribution in maximizing the probability of estimated state $x_k$ in \cref{equ:obj_expand}, we incorporate it into the $\gamma$ in \cref{equ:L_k}. Equation \cref{equ:bel_xk_predicted} then becomes 
$
\tilde{x_{k}} = \gamma \exp\{-loss(x_k)\}.
$

Now we look at $loss(x_k)$. Since we already know the function of $loss(x_{k-1},x_k)$, we can obtain the function of $loss(x_k) = L_k - loss(x_{k-1},x_k)$.
The first derivative of $loss(x_k)$ with respect to $x_{k-1}$ is
\begin{equation} \label{equ:1_derivative_loss_xk}
	\begin{split}
		\frac{\partial loss(x_k)}{\partial x_k} =[R_k^{-1}-R_k^{-1}A(A^TR_k^{-1}A + \Sigma_{x_{k-1}}^{-1})^{-1} A^TR_k^{-1}] (x_k-Bu_k)
		-R_k^{-1}A\\(A^TR_k^{-1}A + \Sigma_{x_{k-1}}^{-1})^{-1}\Sigma_{x_{k-1}}^{-1}\mu_{x_{k-1}}.
	\end{split}
\end{equation}
According to the inversion lemma \cite{higham2002}, $(R+PQP^T)^{-1}=R^{-1}-R^{-1}P(Q^{-1}+P^TR^{-1}P)^{-1}P^TR^{-1} $, so we simplify \cref{equ:1_derivative_loss_xk} to
$
	\frac{\partial loss(x_k)}{\partial x_k} = (R_k + A\Sigma_{x_{k-1}}A^T)^{-1} (x_k-Bu_k)	-	R_k^{-1}A(A^TR_k^{-1}A + \Sigma_{x_{k-1}}^{-1})^{-1}\Sigma_{x_{k-1}}^{-1}\mu_{x_{k-1}}.
$

Setting the first derivation to zero gives us the mean value of $x_k$ in $loss(x_k)$:
\begin{subequations}
	\begin{align}
		(R_k& + A\Sigma_{x_{k-1}}A^T)^{-1} (x_k-Bu_k) =R_k^{-1}A(A^TR_k^{-1}A + \Sigma_{x_{k-1}}^{-1})^{-1}\Sigma_{x_{k-1}}^{-1}\mu_{x_{k-1}}\\
		\begin{split}
			x_k &=Bu_k+ (R_k + A\Sigma_{x_{k-1}}A^T)R_k^{-1}A(A^TR_k^{-1}A + \Sigma_{x_{k-1}}^{-1})^{-1}\Sigma_{x_{k-1}}^{-1}\mu_{x_{k-1}}\\
			&=  Bu_k + A(I+ \Sigma_{x_{k-1}}A^TR_k^{-1}A)  (\Sigma_{x_{k-1}}A^TR_k^{-1}A + I)^{-1}\mu_{x_{k-1}} 
			=Bu_k + A\mu_{x_{k-1}}. \label{equ:prediction_mean}
		\end{split}
	\end{align}
\end{subequations}

The second derivation of $loss(x_k)$ leads us to the inverse covariance of $x_k$ in $loss(x_k)$:
\begin{equation} \label{equ:prediction_variance}
		\frac{\partial^2 loss(x_k)}{\partial x_k^2} =(R_k + A\Sigma_{x_{k-1}}A^T)^{-1}=\overline{\Sigma}_{x_k}^{-1}.
\end{equation}

Finally, the prediction $\tilde{x_{k}}$ in \cref{equ:tilde_x} becomes a scaled Gaussian distribution. 
Such distribution has a mean of $\overline{\mu}_{x_k}=Bu_k + A\mu_{x_{k-1}}$ and a covariance of 
$\overline{\Sigma}_{x_k}=R_k + A\Sigma_{x_{k-1}}A^T$. 

Our goal is the solve \cref{equ:obj_expand}. So far we have solved the prediction phase of \cref{equ:tilde_x} in \cref{equ:obj_expand}. Next, we focus on solving $p(y_k|x_k)$ in \cref{equ:obj_expand} that is the measurement phase. 

\subsection{Measurement Phase}
\label{sec:solving_opt_problem}

From \cref{sec:linear_system}, we know that $p(y_k|x_k)$ follows a Gaussian distribution. Furthermore, we concluded that the $\tilde{x_k}$ in \cref{sec:prediction_phase} is also a scaled Gaussian distribution. Thus we can expand \cref{equ:obj_expand} using the Gaussian distribution as 
\begin{equation}
	\begin{split}
	\hat{x} &= \argmax_{x} p(y_k|x_k)\int p(x_k|x_{k-1},u_{k})p(x_{k-1}|y_{1:k-1},u_{1:k-1}) \,dx_{k-1}\\
	&=\argmax_{x} \eta \exp \{ {-\frac{1}{2} (y_k-Cx_k)^T Q_k^{-1} (y_k-Cx_k)} \} \exp \{{-\frac{1}{2} (x_k-\overline{\mu}_{x_k})^T \overline{\Sigma}_{x_k}^{-1} (x_k-\overline{\mu}_{x_k})} \},
	\end{split} 
\end{equation}
where the $\eta$ incorporates the scalar coefficients of the Gaussian distributions, $\overline{\mu}_{x_k}$ and $\overline{\Sigma}_{x_k}$ are the mean and the covariance in \cref{equ:prediction_mean} and \cref{equ:prediction_variance} from prediction phase. 
Let 
\begin{equation} \label{equ:cost_function_jk}
	J_k=\frac{1}{2} (y_k-Cx_k)^T Q_k^{-1} (y_k-Cx_k) + \frac{1}{2} (x_k-\overline{\mu}_{x_k})^T \overline{\Sigma}_{x_k}^{-1} (x_k-\overline{\mu}_{x_k}),
\end{equation}
then the objective function has the form of
$ \label{equ:incoporated_Jk}
	\hat{x} = \argmax_{x} \eta \exp \{ -J_k \}.
$
Because $\eta \exp \{ -J_k \}$ is the product of a Gaussian distribution with a scalar, $\hat{x} = \argmax_{x} \eta \exp \{ -J_k \}$ is the mean value of the distribution. Our goal boils down to obtain the mean value. In addition, in the prediction phase we also utilized the covariance. Therefore we need to calculate the covariance for next iteration as well. 

By calculating the first and second derivatives of \cref{equ:cost_function_jk} with respect to $x_k$, we acquire the mean $\mu_{x_{k}}$ and covariance $\Sigma_{x_{k}}$:
\begin{subequations}
	\begin{align}
		\begin{split} \label{equ:posterior_covariance}
			\Sigma_{x_{k}}^{-1}&=\frac{\partial^2 J}{\partial x_k^2} =C^TQ_k^{-1}C + \overline{\Sigma}_{x_k}^{-1} 
		\end{split}, \hspace{3em}
		\frac{\partial J}{\partial x_k} = -C^TQ_k^{-1}(y_k-Cx_k)+\overline{\Sigma}_{x_k}^{-1}(x_k-\overline{\mu}_{x_k}) =0\\
		\mu_{x_{k}}&=x_k=\overline{\mu}_{x_k} + \Sigma_{x_k}C^TQ_k^{-1}(y_k-C\overline{\mu}_{x_k}) \label{equ:update_eqution}.
	\end{align}
\end{subequations}
So far we get the most probable estimated state $x_k$ in \cref{equ:update_eqution}. 
Moreover we have the covariance in \cref{equ:posterior_covariance} for next iteration. 

In practice solving for the inversion of \cref{equ:posterior_covariance} might be time-consuming. 
To avoid calculating the inversion we change the way to calculate the covariance. 
According to Inversion Lemma \cite{higham2002}, the expression of \cref{equ:posterior_covariance} is rewritten to
$
		\Sigma_{x_k} = (C^TQ_k^{-1}C + \overline{\Sigma}_{x_k}^{-1})^{-1}
		=\overline{\Sigma}_{x_k} - \overline{\Sigma}_{x_k} C^T (Q_k + C\overline{\Sigma}_{x_k}C^T)^{-1}C\overline{\Sigma}_{x_k}
		=[I-\overline{\Sigma}_{x_k} C^T (Q_k + C\overline{\Sigma}_{x_k}C^T)^{-1}C]\overline{\Sigma}_{x_k}
		=(I-K_kC)\overline{\Sigma}_{x_k},
$
where $K_k=\overline{\Sigma}_{x_k} C^T (Q_k + C\overline{\Sigma}_{x_k}C^T)^{-1}$.
Furthermore, it can be proven that $K_k=\overline{\Sigma}_{x_k} C^T (Q_k + C\overline{\Sigma}_{x_k}C^T)^{-1}=\Sigma_{x_k}C^TQ_k^{-1}$, hence \cref{equ:update_eqution} is 
$
	x_k =\overline{\mu}_{x_k} + K_k(y_k-C\overline{\mu}_{x_k}).
$


\paragraph{Summary.}
The goal of KF is to estimate the current state of a system given historical measurements and inputs. 
By utilizing  Bayes rule and MAP, we obtain the objective function of \cref{equ:obj_expand} in a recursive form.
To solve the maximization problem, we split it into two phases, a prediction phase and a measurement phase. 
In the prediction phase we concluded that $\tilde{x_k}$ is Gaussian distributed. Combining such Gaussian distribution with another Gaussian distribution of $p(y_k|x_k)$ in the measurement phase gives us the result of the solution to \cref{equ:obj_expand}.
The KF steps are summarized in \cref{alg:kf}.

\begin{algorithm}[tbh]
  Given $\mu_{x_{k-1}}$, $\Sigma_{x_{k-1}}$, $u_k$, $y_k$ \;
  $\overline{\mu}_{x_k} = Bu_k + A\mu_{x_{k-1}}$\;
  $\overline{\Sigma}_{x_k} = (R_k + A\Sigma_{x_{k-1}}A^T)$\;
  $ K_k=\overline{\Sigma}_{x_k} C^T (Q_k + C\overline{\Sigma}_{x_k}C^T)^{-1}$\;
  $\mu_{x_{k}} =\overline{\mu}_{x_k} + K_k(y_k-C\overline{\mu}_{x_k})$\;
  $\Sigma_{x_k} = (I-K_kC)\overline{\Sigma}_{x_k} $\;
  return $\mu_{x_{k}}$ and $\Sigma_{x_k}$\;
  \caption{The Kalman filter algorithm.}
  \label{alg:kf}
\end{algorithm}



















