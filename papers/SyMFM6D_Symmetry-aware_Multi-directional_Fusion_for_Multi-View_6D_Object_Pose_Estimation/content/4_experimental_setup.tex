
\section{Experiments}

To demonstrate the performance of our method in comparison to related approaches, we perform extensive experiments on four very challenging datasets.



\subsection{Datasets}

The {\bf YCB-Video dataset} \cite{posecnn} contains a total of 133,827 RGB-D images showing 92 scenes composed of three to nine objects from the 21 Yale-CMU-Berkeley (YCB) objects \cite{ycb}.
Additionally, there are 80,000 synthetic non-sequential \mbox{RGB-D} frames showing a random subset of the YCB objects placed at random positions.

However, most frames from YCB-Video are very similar because they originate from videos with 30 frames per second recorded by a handheld camera that was moved slowly. The videos also do not show the scene from all sides but just from similar perspectives. Furthermore, the scenes do not include strong occlusions, and hence, most object poses are simple to estimate from a single perspective. 
Therefore, we additionally consider the recently proposed photorealistic synthetic datasets {\bf MV-YCB FixCam} and {\bf MV-YCB WiggleCam} \cite{mv6d} as they contain much more difficult scenes with strong occlusions and diverse camera perspectives.
Both datasets depict 8,333 cluttered scenes composed of eleven non-symmetric YCB objects which are randomly arranged so that strong occlusions occur. Each scene is photorealistically rendered from three very different perspectives providing 24,999 RGB-D images with accurate ground truth annotations. Unlike FixCam which uses fixed camera positions while providing accurate camera poses, WiggleCam has varying camera poses which are inaccurately annotated on purpose.

Since FixCam and WiggleCam contain only non-symmetric objects, we created an additional photorealistic synthetic dataset with symmetric and non-symmetric objects called {\bf MV-YCB SymMovCam} using Blender with physically based rendering and domain randomization as in \cite{mv6d}. It also depicts 8,333 cluttered scenes, but they are composed of 8 -- 16 objects randomly chosen from the 21 YCB objects which results in very strong occlusions. For each scene, we created four cameras at changing positions around the scene with the restriction that in each quadrant there is only one camera so that the perspectives are very distinct. This results in a total of 33,332 annotated RGB-D images.



\subsection{Training Procedure}

For training our model in single-view mode on YCB-Video, we randomly use the synthetic and real images of YCB-Video with a ratio of 4:1. Since consecutive real frames are very similar, we consider only every seventh real frame. For training a multi-view model, we start from the corresponding single-view checkpoint and continue training with batches of real YCB-Video frames. 
For training on FixCam and WiggleCam we follow \cite{mv6d} and use random permutations of the three available camera views. For SymMovCam, we take a random subset of three views from the available four views.



\subsection{Evaluation Metrics}
\label{sec_eval_metrics}

We evaluated our method using the area-under-curve (AUC) metrics for ADD-S and \mbox{ADD(-S)} and the precision metrics ADD-S \textless ~\SI{2}{cm} and \mbox{ADD(-S)} \textless ~\SI{2}{cm} as these metrics are most commonly used in related work \cite{cosypose, ffb6d, mv6d}. 



\subsection{Baseline Methods}

We compare our methods with many established and some very recent methods namely 
DenseFusion \cite{densefusion}, CosyPose \cite{cosypose}, PVN3D \cite{pvn3d}, FFB6D \cite{ffb6d}, ES6D \cite{es6d}, and MV6D \cite{mv6d}.
