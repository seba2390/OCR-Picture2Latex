
\subsection{Results on YCB-Video}

\begin{table}[b]
    \centering
\begin{tabular}{l|ccc}
    \toprule
                           &    ADD-S  &    ADD(-S) \\\midrule
DenseFusion (per-pixel)    &     91.2  &     82.9   \\ 
DenseFusion (iterative)    &     93.2  &     86.1   \\
CosyPose                   &     89.8  &     84.5   \\
PVN3D                      &     95.5  &     91.8   \\     
FFB6D                      &     96.6  &     92.7   \\     
ES6D                       &     93.6  &     89.0   \\   
SyMFM6D                    & \tb{96.8} & \tb{94.1}  \\ 
\bottomrule
\end{tabular}
    \caption{Single-view results on YCB-Video using the AUC metrics for ADD-S and \mbox{ADD(-S)}. The best results are printed in bold.}
    \label{tab_ycbv_sv}
\end{table}

\cref{tab_ycbv_sv} compares the single-view performance of our SyMFM6D network with all baseline methods using the AUC of ADD-S and \mbox{ADD(-S)} on YCB-Video. Please note that MV6D corresponds to PVN6D in the single-view scenario. The results show that our approach copes very well with the dynamic camera setup of YCB-Video while outperforming all methods significantly. On the symmetry-aware \mbox{ADD(-S)} AUC metric, SyMFM6D outperforms the current state-of-the-art FFB6D by even \SI{1.5}{\%}. 
Please note that unlike DenseFusion (iterative) and CosyPose, our approach does not perform computationally expensive post processing or iterative refinement procedures.


To examine the effect of our symmetry-aware training procedure, we provide an object-wise evaluation of the three best single-view methods on YCB-Video in \cref{fig_ycb_sv_objects}. Please note that in single-view mode, our model architecture is the same as FFB6D except for our novel symmetry-aware loss function. 
The results show that not only most symmetric objects (highlighted in bold) are estimated more accurate but also most non-symmetric objects.
This indicates that there is a synergy effect which improves the keypoint detection for non-symmetric objects due to an improvement of the keypoint detection for symmetric objects.

\begin{table}[tbp]
    \vspace{2mm}
    \centering
    \begin{tabular}{l|ccc}
        \toprule 
        Object class  		   &   PVN3D  &   FFB6D  &  SyMFM6D  \\\midrule
        Master chef can        &    80.5  &    80.6  &\tb{80.7} \\
        Cracker box            &    94.8  &    94.6  &\tb{94.9} \\
        Sugar box              &    96.3  &\tb{96.6} &\tb{96.6} \\
        Tomato soup can        &    88.5  &\tb{89.6} &    87.9  \\
        Mustard bottle         &    96.2  &    97.0  &\tb{97.8} \\
        Tuna fish can          &    89.3  &    88.9  &\tb{92.3} \\
        Pudding box            &\tb{95.7} &    94.6  &    93.3  \\
        Gelatin box            &    96.1  &\tb{96.9} &    96.1  \\
        Potted meat can        &    88.6  &    88.1  &\tb{90.0} \\
        Banana                 &    93.7  &    94.9  &\tb{95.2} \\
        Pitcher base           &    96.5  &    96.9  &\tb{97.5} \\
        Bleach cleanser        &    93.2  &\tb{94.8} &    93.9  \\
        \tb{Bowl}              &    90.2  &    96.3  &\tb{96.4} \\
        Mug                    &    95.4  &    94.2  &\tb{95.7} \\
        Power drill            &    95.1  &    95.9  &\tb{96.4} \\
        \tb{Wood block}        &    90.4  &    92.6  &\tb{95.2} \\
        Scissors               &    92.7  &    95.7  &\tb{95.8} \\
        Large marker           &\tb{91.8} &    89.1  &    90.0  \\
        \tb{Large clamp}       &    93.6  &    96.8  &\tb{96.9} \\
        \tb{Extra large clamp} &    88.4  &\tb{96.0} &    95.3  \\
        \tb{Foam brick}        &    96.8  &    97.3  &\tb{97.6} \\\midrule
        ALL                    &    91.8  &    92.7  &\tb{94.1} \\\bottomrule
    \end{tabular} 
	\caption{Single-view results on YCB-Video evaluated for each object class individually using the \mbox{ADD(-S)} AUC metric. Symmetric objects and the best results are printed in bold.}
	\label{fig_ycb_sv_objects}
\end{table}

\cref{fig_ycbv_sv} shows a visualization of three scenes of YCB-Video with 6D pose ground truth, predictions of FFB6D, and predictions of our SyMFM6D network using only the depicted view. It can be seen that both FFB6D and SyMFM6D estimate very accurate poses as the scenes of YCB-Video contain only a few objects and not many occlusions. However, SyMFM6D predicts even more accurate poses than FFB6D due to our proposed symmetry-aware training procedure.

\begin{figure*}[htbp]
        \vspace{2mm}
	\centering
	\begin{minipage}{0.24\textwidth}
		\centering
		\textbf{Original View}
	\end{minipage}%
	\begin{minipage}{0.24\textwidth}
		\centering
		\textbf{Ground Truth}
	\end{minipage}%
	\begin{minipage}{0.24\textwidth}
		\centering
		\textbf{FFB6D}
	\end{minipage}%
	\begin{minipage}{0.24\textwidth}
		\centering
		\textbf{SyMFM6D}
	\end{minipage}% 
	\setkeys{Gin}{width=0.24\linewidth}
	\includegraphics{figures/ycb_sv/0055_001042_rgb_1800.png}\,%
	\includegraphics{figures/ycb_sv/0055_001042_gt_1800.png}\,%
	\includegraphics{figures/ycb_sv/0055_001042_FFB6D_1view_1800.png}\,%
	\includegraphics{figures/ycb_sv/0055_001042_SyMV6D_16sym_1view.png}\,%
	\vspace{0.7mm}
	
	\includegraphics{figures/ycb_sv/0051_000636_rgb_864.png}\,%
	\includegraphics{figures/ycb_sv/0051_000636_gt_864.png}\,%
	\includegraphics{figures/ycb_sv/0051_000636_FFB6D_1view_864.png}\,%
	\includegraphics{figures/ycb_sv/0051_000636_SyMV6D_16sym_1view_864.png}\,%
	\vspace{0.7mm}
	
	\includegraphics{figures/ycb_sv/0058_000553_rgb_2461.png}\,%
	\includegraphics{figures/ycb_sv/0058_000553_gt_2461.png}\,%
	\includegraphics{figures/ycb_sv/0058_000553_FFB6D_1view_2461.png}\,%
	\includegraphics{figures/ycb_sv/0058_000553_SyMV6D_16sym_1view.png}\,%
	\caption{Comparison of 6D pose predictions on single frames of the YCB-Video dataset.}
	\label{fig_ycbv_sv}
\end{figure*}

\cref{tab_ycbv_mv} compares our multi-view results with all multi-view baseline methods on YCB-Video using three and five input views.
We see that our approach with disabled symmetry training procedure already outperforms all previous multi-view methods significantly. Enabling the symmetry awareness further improves the results slightly. However, 
using more views does not improve the accuracy as most views of YCB-Video are very similar in which case additional views do not provide beneficial information while the learning problem of fusing different views becomes slightly harder.

\begin{table}[!hbt]
    \tabcolsep=1.35mm
    \centering
\begin{tabular}{l|cc|cc}
    \toprule 
                  & \multicolumn{2}{c|}{ADD-S}
                                          & \multicolumn{2}{c}{ADD(-S)} \\
                  &  3 views  &  5 views  &  3 views  &  5 views  \\\midrule
CosyPose          &     92.3  &     93.4  &     87.7  &     88.8  \\
MV6D              &     91.2  &     91.1  &     85.6  &     84.0  \\
SyMFM6D (no sym)  &     95.2  &     95.2  &     91.5  &     91.4  \\
SyMFM6D           & \tb{95.4} & \tb{95.4} & \tb{91.7} & \tb{91.6} \\
\bottomrule
\end{tabular}
    \caption{Quantitative multi-view results on YCB-Video. The best results are printed in bold.}
    \label{tab_ycbv_mv}
\end{table}



\subsection{Results on MV-YCB FixCam, WiggleCam and SymMovCam}

We show the quantitative results on the datasets MV-YCB FixCam, MV-YCB WiggleCam, and MV-YCB SymMovCam in \cref{tab_fixCam_wiggleCam}. It includes a comparison with two modified CosyPose (CP) versions with and without known camera poses as presented by \cite{mv6d}.
Our SyMFM6D network yields the best results on all metrics on all three datasets. This shows that SyMFM6D copes very well with the strong occlusions in the datasets. The results on WiggleCam are just slightly worse than on FixCam which demonstrates that our approach is robust towards inaccurately known camera poses.

On the novel SymMovCam dataset, our method outperforms the baselines by a much larger margin than on FixCam and WiggleCam. This is due to the symmetric objects in the datasets on which the keypoint estimation of the baseline methods is inaccurate. The results also prove that our approach is robust to very dynamic camera setups where the cameras are mounted at varying positions.


\begin{table*}[h]
	\tabcolsep=1.0mm
	\centering
	\begin{tabular}{r|cccccc|cccccc|ccccc}
    \toprule
                           &     \multicolumn{6}{c|}{MV-YCB FixCam}                        &      \multicolumn{6}{c|}{MV-YCB WiggleCam}                    &      \multicolumn{5}{c}{MV-YCB SymMovCam}            \\
                           &  PVN3D   &   FFB6D  &   CP   &    CP    &   MV6D   &   Ours   &  PVN3D   & FFB6D    &   CP   &   CP     &  MV6D    &  Ours    &  PVN3D   & FFB6D    &   Ours   &    MV6D  &  Ours    \\ 
    Number of views        &   1      &     1    &   3    &    3     &   3      &    3     &    1     & 1        &   3    &   3      &   3      &    3     &    1     &    1     &     1    &     3    &    3     \\
    Known cam poses        &\checkmark&\checkmark&$\times$&\checkmark&\checkmark&\checkmark&\checkmark&\checkmark&$\times$&\checkmark&\checkmark&\checkmark&\checkmark&\checkmark&\checkmark&\checkmark&\checkmark\\
    \midrule                                                                                      
    ADD-S AUC              &  81.3    &   82.3   &  90.8  &   91.9   &  96.9    &\tb{97.3} &   80.8   &   81.9   & 90.0   &  91.3    &    96.2  &\tb{96.7} &   75.0   &   79.9   &   80.6   &   92.8   & \tb{94.2}\\
    ADD(-S) AUC            &  74.9    &   76.3   &  82.4  &   84.6   &  94.8    &\tb{95.6} &   74.0   &   75.5   & 81.0   &  83.4    &    93.0  &\tb{94.2} &   68.5   &   75.6   &   76.7   &   88.7   & \tb{91.6}\\
    ADD-S \textless   ~\SI{2}{cm} &  82.1    &   83.6   &  92.9  &   93.0   &  98.8    &\tb{98.9} &   82.0   &   83.4   & 92.3   &  92.6    &    98.7  &\tb{98.8} &   77.2   &   81.1   &   81.9   &   96.3   & \tb{96.6}\\
    ADD(-S) \textless ~\SI{2}{cm} &  73.0    &   74.8   &  80.6  &   82.4   &  96.5    &\tb{96.8} &   72.4   &   74.0   & 78.9   &  81.6    &\tb{96.0} &\tb{96.0} &   64.5   &   74.5   &   76.3   &   91.6   & \tb{93.6}\\
    \bottomrule
	\end{tabular}
	\caption{Quantitative results on the datasets MV-YCB FixCam (left), MV-YCB WiggleCam (middle), and MV-YCB SymMovCam (right). The baseline CosyPose (CP) uses PVN3D as backend network as described in \cite{mv6d}. The best results per dataset are printed in bold.}
	\label{tab_fixCam_wiggleCam}
\end{table*}



\subsection{Keypoint Visualization}

\cref{fig_ycbv_sv_keypoints} shows predicted keypoints of FFB6D and SyMFM6D in a YCB-Video scene. We additionally visualize the keypoint proposals of each object in individual colors.
The resulting predicted keypoints are white, the target keypoints are black. You can see that both FFB6D and SyMFM6D predict very accurate keypoints on all non-symmetric objects. However, FFB6D fails to predict accurate keypoints on the large clamp which has one discrete rotational symmetry. This shortcoming of FFB6D is also apparent on other symmetric objects. We believe that this is caused by the ambiguities of the object poses resulting in ambiguous target keypoints which results in averaging over the multiple solutions given by the symmetry. Therefore, the training loss is minimized when predicting keypoints on the symmetric axis rather than predicting them on the desired target locations. SyMFM6D in contrast overcomes this problem by our novel symmetry-aware training procedure as it can be seen in \cref{fig_ycbv_sv_keypoints_SyMFM6D}.

\begin{figure}[!tbh]
  \centering
\begin{subfigure}[b]{0.49\columnwidth}
  \includegraphics[width=1.0\columnwidth]{figures/0054_000204_FFB6D_1view_keypoints_cropped.jpg}
   \caption{FFB6D}
   \label{fig_ycbv_sv_keypoints_FFB6D}
\end{subfigure}
\begin{subfigure}[b]{0.49\columnwidth}
  \centering
  \includegraphics[width=1.0\columnwidth]{figures/0054_000204_SyMFM6D_16sym_1view_keypoints_BestSym_cropped.jpg}
   \caption{SyMFM6D}
   \label{fig_ycbv_sv_keypoints_SyMFM6D}
   \end{subfigure}
	\caption{Visualization of the predicted keypoints on single frames of the YCB-Video dataset.} 
   \label{fig_ycbv_sv_keypoints}
\end{figure}


\subsection{Implementation Details and Runtime}

We trained our network up to seven days on four NVIDIA Tesla V100 GPUs with \SI{32}{GB} of memory. 
The network architecture of our SyMFM6D approach has 3.5 million trainable parameters and requires about \SI{46}{ms} for processing a single RGB-D image on a single GPU. 
Mean shift clustering and least-squares fitting for computing a 6D pose require additional \SI{14}{ms} per object. 
Please visit our previously mentioned GitHub repository for code, datasets, and further details.
