\section{Introduction}

Estimating the 6D poses of objects is an essential computer vision task which is widely used in robotics \cite{posecnn, pvn3d, ffb6d}, automated driving \cite{avod, gu2021ecpc_icp}, and augmented reality \cite{arSurvey, su2019ar}.
In recent years, 6D pose estimators have made significant progress based on deep neural network architectures which rely on a single RGB image \cite{posecnn, so_pose, zebrapose}, on a single point cloud \cite{pointvotenet2020, votingAttentionPoseEst22},
or fuse both \cite{densefusion, pvn3d, ffb6d}. 
Single-view methods, however, have problems detecting objects which are occluded by other objects.
These problems can be overcome by considering data from multiple perspectives.
Fusing multi-view data can significantly improve the accuracy and robustness of environmental understanding in complex scenarios, which can enable more flexible production and assembly processes, among other applications.
There are already a few methods that consider multi-view data \cite{zeng2017multi, li2018unified, cosypose} which are, however, computationally expensive and not designed for scenes with strong occlusions. Moreover, most methods suffer from symmetric objects as they have multiple 6D poses with same visual and geometric appearance, causing most learning-based estimators to average over these multiple solutions.

We present a novel \textbf{Sy}mmetry-aware \textbf{M}ulti-directional \textbf{F}usion approach for \textbf{M}ulti-view \textbf{6D} pose estimation called SyMFM6D which overcomes the previously mentioned issues. \cref{fig_eye_catcher} shows an overview of our system. 
SyMFM6D exploits the visual and geometric information from an arbitrary number of RGB-D images depicting a scene from multiple perspectives. We propose a deep multi-directional fusion network which fuses the multi-view RGB-D data efficiently and learns a compact representation of the entire scene. Our approach predicts the 6D poses of all objects in the scene simultaneously based on keypoint detection, instance semantic segmentation, and least-squares fitting. Furthermore, we present a novel symmetry-aware training procedure including a novel objective function which significantly improves the keypoint detection. 

\begin{figure}[t]
  \centering 
  \includegraphics[page=1, trim = 10mm 35mm 7mm 36mm, clip, width=1.0\columnwidth]{figures/SyMFM6D_eyecatcher.pdf}
   \caption{Overview of our proposed SyMFM6D approach. We present a novel deep multi-directional fusion network which merges RGB-D data from multiple cameras. SyMFM6D predicts the 6D poses of all objects in the scene while considering object symmetries. It copes with very cluttered scenes and outperforms the state-of-the-art in single-view and multi-view 6D pose estimation.}
   \label{fig_eye_catcher}
\end{figure}

Our experiments demonstrate a large benefit of the proposed symmetry-aware training procedure, improving the accuracy of both symmetric and non-symmetric objects due to synergy effects. Thus, our approach outperforms the state-of-the-art in single-view 6D pose estimation. SyMFM6D also outperforms the state-of-the-art multi-view approach while being computationally more efficient. We furthermore show that our approach works accurately in both fixed and dynamic camera settings. Moreover, our method is robust towards inaccurate camera calibration by compensating imprecise camera pose information when using multiple views.


Our main contributions are:

\begin{enumerate*}[label=\enumlabel]
    \item We propose a novel multi-directional multi-view fusion network for efficient representation learning of multiple RGB-D frames and present a novel multi-view 6D pose estimation method based on it.
    \item We present a novel symmetry-aware training procedure for 3D keypoint detection based on a symmetry-aware objective function.
    \item We present a novel synthetic dataset with photorealistic multi-view \mbox{RGB-D} data and labels for 6D pose estimation as well as instance semantic segmentation.
	\item We demonstrate significant improvements and synergy effects due to our symmetry-aware training procedure on challenging datasets including symmetric and non-symmetric objects.
	\item Our method outperforms the state-of-the-art in single-view and multi-view 6D object pose estimation. We further demonstrate the robustness of our approach towards inaccurate camera calibration and dynamic camera setups. 
\end{enumerate*}
