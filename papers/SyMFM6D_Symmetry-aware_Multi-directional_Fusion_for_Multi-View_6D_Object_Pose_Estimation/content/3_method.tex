\section{Proposed Method: SyMFM6D}

We propose a deep multi-directional fusion approach called SyMFM6D that estimates the 6D object poses of all objects in a cluttered scene based on multiple RGB-D images while considering object symmetries. 
In this section, we define the task of multi-view 6D object pose estimation and present our multi-view deep fusion architecture.

\begin{figure*}[tbh]
  \vspace{2mm}
  \centering
  \includegraphics[page=1, trim = 5mm 40mm 5mm 42mm, clip,  width=1.0\linewidth]{figures/SyMFM6D_architecture4_2.pdf}
   \caption{Network architecture of SyMFM6D which fuses $N$ RGB-D input images. Our method converts the $N$ depth images to a single point cloud which is processed by an encoder-decoder point cloud network. The $N$ RGB images are processed by an encoder-decoder CNN. Every hierarchy contains a point-to-pixel fusion module and a pixel-to-point fusion module for deep multi-directional multi-view fusion. We utilize three MLPs with four layers each to regress 3D keypoint offsets, center point offsets, and semantic labels based on the final features. The 6D object poses are computed as in \cite{pvn3d} based on mean shift clustering and least-squares fitting. We train our network by minimizing our proposed symmetry-aware multi-task loss function using precomputed object symmetries. $N_p$ is the number of points in the point cloud. $H$ and $W$ are height and width of the RGB images.}
   \label{fig_architecture}
   \vspace{-2mm}
\end{figure*}


6D object pose estimation describes the task of predicting a rigid transformation $\boldsymbol p = [\boldsymbol R |  \boldsymbol t] \in SE(3)$ which transforms the coordinates of an observed object from the object coordinate system into the camera coordinate system. This transformation is called 6D object pose because it is composed of a 3D rotation $\boldsymbol R \in SO(3)$ and a 3D translation $\boldsymbol t \in \mathbb{R}^3$. 
The designated aim of our approach is to jointly estimate the 6D poses of all objects in a given cluttered scene using multiple RGB-D images which depict the scene from multiple perspectives. We assume the 3D models of the objects and the camera poses to be known as proposed by \cite{mv6d}.



\subsection{Network Overview}

Our symmetry-aware multi-view network consists of three stages which are visualized in \cref{fig_architecture}. 
The first stage receives one or multiple RGB-D images and extracts visual features as well as geometric features which are fused to a joint representation of the scene. 
The second stage performs a detection of predefined 3D keypoints and an instance semantic segmentation.
Based on the keypoints and the information to which object the keypoints belong, we compute the 6D object poses with a least-squares fitting algorithm \cite{leastSquares} in the third stage.



\subsection{Multi-View Feature Extraction}

To efficiently predict keypoints and semantic labels, the first stage of our approach learns a compact representation of the given scene by extracting and merging features from all available RGB-D images in a deep multi-directional fusion manner. For that, we first separate the set of RGB images $\text{RGB}_1, ..., \text{RGB}_N$ from their corresponding depth images $\text{Dpt}_1$, ..., $\text{Dpt}_N$. The $N$ depth images are converted into point clouds, transformed into the coordinate system of the first camera, and merged to a single point cloud using the known camera poses as in \cite{mv6d}. 
Unlike \cite{mv6d}, we employ a point cloud network based on RandLA-Net \cite{hu2020randla} with an encoder-decoder architecture using skip connections.
The point cloud network learns geometric features from the fused point cloud and considers visual features from the multi-directional point-to-pixel fusion modules as described in \cref{sec_multi_view_fusion}.

The $N$ RGB images are independently processed by a CNN with encoder-decoder architecture using the same weights for all $N$ views. The CNN learns visual features while considering geometric features from the multi-directional pixel-to-point fusion modules. We followed \cite{ffb6d} and build the encoder upon a ResNet-34 \cite{resnet} pretrained on ImageNet~\cite{imagenet} and the decoder upon a PSPNet \cite{pspnet}. 

After the encoding and decoding procedures including several multi-view feature fusions, we collect the visual features from each view corresponding to the final geometric feature map and concatenate them. The output is a compact feature tensor containing the relevant information about the entire scene which is used for keypoint detection and instance semantic segmentation as described in \cref{sec_keypoint_detection_and_segmentation}.


\begin{figure*}[tbh]
  \vspace{2mm} 
  \centering  
\begin{subfigure}[b]{0.48\textwidth}
  \includegraphics[page=1, trim = 1mm 6mm 6mm 6mm, clip,  width=1.0\linewidth]{figures/p2r_8.pdf}
   \caption{Point-to-pixel fusion module.~~~~}
   \label{fig_pt2px_fusion}
\end{subfigure}
\begin{subfigure}[b]{0.48\textwidth}
  \centering  
  \includegraphics[page=1, trim = 1mm 6mm 6mm 6mm, clip,  width=1.0\linewidth]{figures/r2p_8.pdf}
   \caption{Pixel-to-point fusion module.~~~~~}
   \label{fig_px2pt_fusion}
   \end{subfigure}
      \caption{Overview of our proposed multi-directional multi-view fusion modules. They combine pixel-wise visual features and point-wise geometric features by exploiting the correspondence between pixels and points using the nearest neighbor algorithm. We compute the resulting features using multiple shared MLPs with a single layer and max-pooling.
      For simplification, we depict an example with $N=2$ views and $K_\text{i}=K_\text{p}=3$ nearest neighbors. The points of ellipsis (...) illustrate the generalization for an arbitrary number of views $N$. Please refer to \cite{ffb6d} for better understanding the basic operations.
      }
   \label{fig_fusion_modules}
   \vspace{-1mm}
\end{figure*}



\subsection{Multi-View Feature Fusion}
\label{sec_multi_view_fusion}
In order to efficiently fuse the visual and geometric features from multiple views, we extend the fusion modules of FFB6D~\cite{ffb6d} from bi-directional fusion to \emph{multi-directional fusion}. We present two types of multi-directional fusion modules which are illustrated in \cref{fig_fusion_modules}.
Both types of fusion modules take the pixel-wise visual feature maps and the point-wise geometric feature maps from each view, combine them, and compute a new feature map.
This process requires a correspondence between pixel-wise and point-wise features which we obtain by computing an XYZ map for each RGB feature map based on the depth data of each pixel using the camera intrinsic matrix as in \cite{ffb6d}. To deal with the changing dimensions at different layers, we use the centers of the convolutional kernels as new coordinates of the feature maps and resize the XYZ map to the same size using nearest interpolation as proposed in \cite{ffb6d}.

The \emph{point-to-pixel} fusion module in \cref{fig_pt2px_fusion} computes a 
fused feature map $\bb F_\text{f}$ based on the image features $\bb F_{\text{i}}(v)$ of all views $v \in \{1, \ldots, N\}$.
We collect the $K_\text{p}$ nearest point features $\bb F_{\text{p}_k}(v)$ with $k \in \{1, \ldots, K_\text{p}\}$ from the point cloud for each pixel-wise feature and each view independently by computing the nearest neighbors according to the Euclidean distance in the XYZ map. Subsequently, we process them by a shared MLP before aggregating them by max-pooling, i.e.,
\begin{align} 
    \widetilde{\bb F}_{\text{p}}(v) = \max_{k \in \{1, \ldots, K_\text{p}\}} 
    \Big( \text{MLP}_\text{p}(\bb F_{\text{p}_k}(v)) \Big).
    \label{eq_p2r}
\end{align}
Finally, we apply a second shared MLP to fuse all features $\bb F_\text{i}$ and 
$\widetilde{\bb F}_{\text{p}}$ as 
$\bb F_{\text{f}} = \text{MLP}_\text{fp}(\widetilde{\bb F}_{\text{p}} \oplus \bb F_\text{i})$ where $\oplus$ denotes the concatenate operation.


The \emph{pixel-to-point} fusion module in \cref{fig_px2pt_fusion} collects the $K_\text{i}$ nearest image features $\bb F_{\text{i}_k}(\textrm{i2v}(i_k))$ with $k\in\{1, ..., K_\text{i}\}$. $\textrm{i2v}(i_k)$ is a mapping that maps the index of an image feature to its corresponding view. This procedure is performed for each point feature vector $\bb F_\text{p}(n)$.
We aggregate the collected image features by max-pooling and apply a shared MLP, i.e.,
\begin{align}
    \widetilde{\bb F}_{\text{i}} = \text{MLP}_\text{i} 
    \left( \max_{k \in \{1, \ldots, K_\text{i}\}} 
    \Big( \bb F_{\text{i}_k}(\textrm{i2v}(i_k)) \Big)  
    \right).
    \label{eq_r2p}
\end{align}
One more shared MLP fuses the resulting image features $\widetilde{\bb F}_{\text{i}}$ with the point features $\bb F_\text{p}$ as 
$\bb F_{\text{f}} = \text{MLP}_\text{fi}(\widetilde{\bb F}_{\text{i}} \oplus \bb F_\text{p})$.




\subsection{Keypoint Detection and Segmentation}
\label{sec_keypoint_detection_and_segmentation}
The second stage of our SyMFM6D network contains modules for 3D keypoint detection and instance semantic segmentation following \cite{mv6d}. However, unlike \cite{mv6d}, we use the SIFT-FPS algorithm \cite{lowe1999sift} as proposed by FFB6D \cite{ffb6d} to define eight target keypoints for each object class. SIFT-FPS yields keypoints with salient features which are easier to detect.
Based on the extracted features, we apply two shared MLPs to estimate the translation offsets from each point of the fused point cloud to each target keypoint and to each object center.
We obtain the actual point proposals by adding the translation offsets to the respective points of the fused point cloud. 
Applying the mean shift clustering algorithm \cite{cheng1995meanshift} results in predictions for the keypoints and the object centers.
We employ one more shared MLP 
for estimating the object class of each point in the fused point cloud as in \cite{pvn3d}.



\subsection{6D Pose Computation via Least-Squares Fitting}

Following \cite{pvn3d}, we use the least-squares fitting algorithm \cite{leastSquares} to compute the 6D poses of all objects based on the estimated keypoints. As the $M$ estimated keypoints $\boldsymbol{\widehat{k}}_1, ..., \boldsymbol{\widehat{k}}_M$ are in the coordinate system of the first camera and the target keypoints $\boldsymbol k_1, ..., \boldsymbol k_M$ are in the object coordinate system, least-squares fitting calculates the rotation matrix $\boldsymbol R$ and the translation vector $\boldsymbol t$ of the 6D pose by minimizing the squared loss
\begin{equation}
    L_\text{Least-squares} = \sum_{i=1}^M \norm{\boldsymbol{\widehat{k}_i} - (\boldsymbol R \boldsymbol k_i + \boldsymbol t)}_2^2.
\end{equation}



\subsection{Symmetry-aware Keypoint Detection}

Most related work, including \cite{pvn3d, ffb6d}, and \cite{mv6d} does not specifically consider object symmetries. 
However, symmetries lead to ambiguities in the predicted keypoints as multiple 6D poses can have the same visual and geometric appearance. 
Therefore, we introduce a novel symmetry-aware training procedure for the 3D keypoint detection including a novel symmetry-aware objective function to make the network predicting either the original set of target keypoints for an object or a rotated version of the set corresponding to one object symmetry. Either way, we can still apply the least-squares fitting which efficiently computes an estimate of the target 6D pose or a rotated version corresponding to an object symmetry. To do so, we precompute the set $\boldsymbol{S}_I$ of all rotational symmetric transformations for the given object instance $I$ with a stochastic gradient
descent algorithm \cite{sgdr}.
Given the known mesh of an object and an initial estimate for the symmetry axis, we transform the object mesh along the symmetry axis estimate and optimize the symmetry axis iteratively by minimizing the ADD-S metric \cite{hinterstoisser2012model}.
Reflectional symmetries which can be represented as rotational symmetries are handled as rotational symmetries. 
Other reflectional symmetries are ignored, since the reflection cannot be expressed as an Euclidean transformation.
To consider continuous rotational symmetries, we discretize them into 16 discrete rotational symmetry transformations.

We extend the keypoints loss function of \cite{pvn3d} to become symmetry-aware such that it predicts the keypoints of the closest symmetric transformation, i.e. 
\begin{equation}
    L_\text{kp}(\mathcal{I}) = \frac{1}{N_I} 
    \min_{\boldsymbol{S} \in \boldsymbol{S}_I} 
    \sum_{i \in \mathcal{I}} \sum_{j=1}^M 
    \norm{\boldsymbol{x}_{ij} - \boldsymbol{S}\boldsymbol{\widehat{x}}_{ij}}_2, 
\label{eq_keypoint_loss}
\end{equation}
where $N_I$ is the number of points in the point cloud for object instance $I$, $M$ is the number of target keypoints per object, and $\mathcal{I}$ is the set of all point indices that belong to object instance $I$.  
The vector $\boldsymbol{\widehat{x}}_{ij}$ is the predicted keypoint offset for the $i$-th point and the $j$-th keypoint while $\boldsymbol{x}_{ij}$ is the corresponding ground truth. 



\subsection{Objective Function}

We train our network by minimizing the multi-task loss function
\begin{equation}
 \label{eq_total_loss}
    L_\text{multi-task} = \lambda_1 L_\text{kp} 
    + \lambda_2 L_\text{semantic}  
    +  \lambda_3 L_\text{cp},
\end{equation}
where $L_\text{kp}$ is our symmetry-aware keypoint loss from \cref{eq_keypoint_loss}.
$L_\text{cp}$ is an L1 loss for the center point prediction, $L_\text{semantic}$ is a Focal loss \cite{focalLoss} for the instance semantic segmentation, and $\lambda_1=2$, $\lambda_2=1$, and $\lambda_3=1$ are the weights for the individual loss functions as in \cite{ffb6d}.
