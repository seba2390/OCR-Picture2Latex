\section{Related Work}

Over the last few years, there has been significant progress in the area of 6D pose estimation. We now discuss the most important milestones subdivided into single-view methods, multi-view methods, and symmetry-aware methods.



\subsection{Single-View 6D Pose Estimation}

The methods in this family require only a single input modality, which can be RGB, point cloud, or RGB-D. Traditional pose estimators using a \emph{single RGB image} are mostly feature-based \cite{lowe1999sift, lowe2004distinctive, rosten2006machine, moped, collet2009object, pvnet} or based on template matching \cite{ComparingImgsTPAMI93, gu2010discriminative, 2011gradientResponseMaps, cao2016realTime6d}. Especially the former group of methods are often multi-staged and first extract local features from the given RGB image before matching the 2D-3D-correspondences to estimate the object's pose using a Perspective-n-Point (PnP) algorithm \cite{ransac}. End-to-end trainable neural networks directly predict object poses without requiring multiple stages \cite{2015viewpointsKeypoints, deepim, posecnn, gupta2019cullnet, ssd6d, tekin2018real, gdrnet, so_pose, zebrapose}. 
These methods share similar ideas to exploit differentiable PnP or differentiable rendering techniques.

The recent advance of LiDAR and depth sensors promoted the proposal of methods based on a \emph{single point cloud} \cite{chen2020survey6d, fernandes2021pointCloudSurvey}. These methods apply either 3D convolutions \cite{song2014sliding, li2017_3dFCN}, or variations of PointNet \cite{pointNet} as backbone \cite{voxelnet, second, pointpillars}. The authors of \cite{qi2019voteNet} and \cite{xie2021VENet} introduce and further improve voting techniques for 3D object detection.
However, since point cloud based methods cannot extract texture information, their application range is limited.

In contrast, \emph{RGB-D based approaches} can combine the advantages of both modalities. For instance, \cite{avod} and \cite{mv3d} fuse an RGB image with a LiDAR point cloud by applying networks for convolutional feature extraction and for generating 3D object proposals.
The approaches proposed in \cite{pointfusion}, \cite{densefusion}, and \cite{pvn3d} separately process the RGB image by a CNN and the point cloud by a PointNet-based network before fusing the appearance features and the geometric features with a dense fusion network.
In \cite{pvn3d} and \cite{ffb6d} the authors employ a deep Hough voting network for 3D keypoint detection before estimating 6D poses by least-squares fitting \cite{leastSquares}. 
However, most previous methods do not consider object symmetries and suffer from strong occlusions.



\subsection{Multi-View 6D Pose Estimation}

Multi-view pose estimators consider multiple RGB(-D) frames showing the same scene from different perspectives in order to reduce the effect of occlusions and to improve the 6D pose estimation accuracy.
The approach proposed in \cite{zeng2017multi} first segments all frames with a CNN before aligning the known 3D models with the segmented object point cloud to estimate their poses.
The authors of \cite{li2018unified} present an end-to-end trainable CNN-based architecture based on a single RGB or RGB-D image. They perform the single-view pose estimation multiple times with images from different viewpoints before selecting the best hypothesis using a voting score that suppresses outliers. 
In \cite{cosypose} the authors propose a three-stage approach which first employs a CNN for generating object candidate proposals for each view independently. Secondly, they conduct a candidate matching considering the predictions of all views before finally performing a refinement procedure based on object-level bundle adjustment \cite{triggs2000bundle}.
The approach of \cite{mv6d} directly fuses the features from multiple RGB-D views before predicting the poses based on keypoints and least-squares fitting \cite{leastSquares}. However, the method uses a computationally expensive feature extraction and fusion network which does not consider object symmetries and it is evaluated on only synthetic datasets.



\subsection{Symmetry-aware 6D Pose Estimation}

Symmetric objects are known to be a challenge for 6D pose estimation approaches due to ambiguities \cite{objectSym6D_3DV19}. Different techniques have been proposed to address this issue. The authors of \cite{objectSym6D_3DV19} and \cite{rad2017bb8} propose to utilize an additional output channel to classify the type of symmetry and its domain range. 
In \cite{pix2poseICCV19}, a loss is introduced that is the smallest error among symmetric pose proposals in a finite pool of symmetric poses. 
In \cite{eposCVPR20} the authors propose to use compact surface fragments as a compositional way to represent objects. As a result, this representation can easily allow handling of symmetries. 
The authors of \cite{zhang2020symmetry6d} employ an additional symmetry prediction as output, and an extra refining step of predicted symmetry via an optimization function. 
A novel output space representation for CNNs is presented in  \cite{symCnnICRA21} where symmetrical equivalent poses are mapped to the same values. In \cite{es6d} the authors introduce a compact shape representation based on grouped primitives to handle symmetries.
However, non of these methods outperforms the keypoint-based methods 
\cite{pvn3d} and \cite{ffb6d}, even though they do not consider object symmetries. In contrast, our method extends current keypoint based methods to consider object symmetries, and consequently outperforms all previous methods on single and multi-view scenes.
