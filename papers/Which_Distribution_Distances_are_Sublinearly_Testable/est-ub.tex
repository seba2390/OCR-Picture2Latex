\section{Upper Bounds Based on Estimation}
\label{sec:est-ub}

We start by showing a simple meta-algorithm -- in short, it says that if a testing problem is well-defined (i.e., has appropriate separation between the cases) and we can estimate one of the distances, it can be converted to a testing algorithm.
\begin{theorem}\label{thm:est-ub}
Suppose there exists an $m(n, \ve)$-sample algorithm which, given sample access to distributions $p$ and $q$ over $[n]$ and parameter $\ve$, estimates some distance $d(p,q)$ up to an additive $\ve$ with probability at least $2/3$.
Consider distances $d_X(\cdot, \cdot), d_Y(\cdot, \cdot)$ and $\ve_1, \ve_2 > 0$ such that $ d_Y(p,q) \geq \ve_2 \rightarrow d_X(p,q) > 3\ve_1/2$ and $d_X(p,q) \leq \ve_1 \rightarrow d_Y(p,q) < 2\ve_2/3$, and $d(\cdot, \cdot)$ is either $d_X(\cdot, \cdot)$ or $d_Y(\cdot, \cdot)$.

Then there exists an algorithm for equivalence testing between $p$ and $q$ distinguishing the cases:
\begin{itemize}
\item $d_X(p,q) \leq \ve_1$;
\item $d_Y(p,q) \geq \ve_2$.
\end{itemize}
The algorithm uses either $m(n, O(\ve_1))$ or $m(n, O(\ve_2))$ samples, depending on whether $d = d_X$ or $d_Y$.
\end{theorem}
\begin{proof}
Suppose that $d = d_X$, the other case follows similarly.
Using the $m(n, \ve_1/4)$ samples, obtain an estimate $\hat \tau$ of $d_X(p,q)$, accurate up to an additive $\ve_1/4$.
If $\hat \tau \leq 5\ve_1/4$, output that $d_X(p,q) \leq \ve_1$, else output that $d_Y(p,q) \geq \ve_2$. 
Conditioning on the correctness of the estimation algorithm, correctness for the case when $d_X(p,q) \leq \ve_1$ is immediate, and correctness for the case when $d_Y(p,q) \geq \ve_2$ follows from the separation between the cases.
\end{proof}

It is folklore that a distribution over $[n]$ can be $\ve$-learned in $\ell_2$-distance with $O(1/\ve^2)$ samples (see, i.e., \cite{ChanDVV14, Waggoner15} for a reference).
By triangle inequality, this implies that we can estimate the $\ell_2$ distance between $p$ and $q$ up to an additive $O(\ve)$ with $O(1/\ve^2)$ samples, leading to the following corollary.

\begin{corollary}\label{cor:l2-est}
There exists an algorithm for equivalence testing between $p$ and $q$ distinguishing the cases:
\begin{itemize}
\item $d(p,q) \leq f(n, \ve)$;
\item $\dlt(p,q) \geq \ve$,
\end{itemize}
where $d(\cdot, \cdot)$ is a distance and $f(n, \ve)$ is such that $\dlt(p,q) \geq \ve \rightarrow d(p,q) \geq 3f(n, \ve)/2$ and $d(p,q) \leq f(n, \ve) \rightarrow \dlt(p,q) \leq 2\ve/3$.  
The algorithm uses $O(1/\ve^2)$ samples.
\end{corollary}

Finally, we note that total variation distance between $p$ and $q$ can be additively estimated up to a constant using $O(n/\log n)$ samples~\cite{LehmannC06, ValiantV11b, JiaoHW16}, leading to the following corollary:
\begin{corollary}\label{cor:tv-est}
For constant $\ve > 0$, there exists an algorithm for equivalence testing between $p$ and $q$ distinguishing the cases:
\begin{itemize}
\item $\dtv(p,q) \leq \ve^2/4$;
\item $\dh(p,q) \geq \ve/\sqrt{2}$.
\end{itemize}
The algorithm uses $O(n/\log n)$ samples.
\end{corollary}
