\section{Lower Bounds}
\label{sec:lb}
We start with a simple lower bound, showing that identity testing with respect to KL divergence is impossible.
A similar observation was made in~\cite{BatuFRSW00}.
\begin{theorem}\label{thm:untestable}
No finite sample test can perform identity testing between $p$ and $q$ distinguishing the cases:
\begin{itemize}
\item $p = q$;
\item $\dkl(p,q) \geq \ve^2$.
\end{itemize}
\end{theorem}
\begin{proof}
Simply take $q = (1, 0)$ and let~$p$ be either $(1, 0)$ or $(1-\delta, \delta)$, for~$\delta > 0$ tending to zero.
Then $p = q$ in the first case and $\dkl(p,q) = \infty$ in the second, but distinguishing between these two possibilities for~$p$
takes $\Omega(\delta^{-1})\rightarrow \infty$ samples.
\end{proof}

Next, we prove our lower bound for KL tolerant identity testing.

\begin{theorem}\label{thm:ones-lb}
There exist constants $0 < s < c$, such that any algorithm for identity testing between $p$ and $q$ distinguishing the cases:
\begin{itemize}
\item $\dkl(p,q) \leq s$;
\item $\dtv(p,q) \geq c$;
\end{itemize}
requires $\Omega(n/\log n)$ samples.
\end{theorem}
\begin{proof}
Let $q = (\tfrac{1}{n}, \ldots, \tfrac{1}{n})$ be the uniform distribution.
Let $R(\cdot, \cdot)$ denote the \emph{relative earthmover distance} (see~\cite{ValiantV10a} for the definition).
By Theorem~$1$ of~\cite{ValiantV10a},
for any $\delta < \frac{1}{4}$
there exist sets of distributions~$\calC$ and~$\calF$ (for \emph{close} and \emph{far})
such that:
\begin{itemize}
\item For every $p \in \calC$, $R(p, q) = O(\delta | \log \delta|)$.
\item For every $p \in \calF$ there exists a distribution~$r$ which is uniform over~$n/2$ elements such that $R(p, r) = O(\delta | \log \delta|)$.
\item Distinguishing between $p \in \calC$ and $p \in \calF$ requires $\Omega(\frac{\delta n}{\log(n)})$ samples.
\end{itemize}
Now, if $p \in \calC$ then
\begin{equation*}
\dkl(p,q)
= \sum_{i=1}^n p_i \log\left(\frac{p_i}{1/n}\right)
= \log(n) - H(p)
\leq O(\delta |\log(\delta)|),
\end{equation*}
where $H(p)$ is the Shannon entropy of~$p$,
and here we used the fact that $|H(p) - H(q)| \leq R(p, q)$, which follows from Fact~$5$ of~\cite{ValiantV10a}.
On the other hand, if $q \in \calF$, let~$r$ be the corresponding distribution which is uniform over~$n/2$ elements.
Then
\begin{equation*}
\frac{1}{2}
= \dtv(p,q)
\leq \dtv(q,p) + \dtv(p,r)
\leq \dtv(q,p) + O(\delta | \log \delta|),
\end{equation*}
where we used the triangle inequality
and the fact that $\dtv(p,r) \leq R(p, r)$ (see~\cite{ValiantV10a} page 4).
As a result, if we set~$\delta$ to be some small constant,
$s = O(\delta |\log(\delta)|)$,
and $c = \frac{1}{2} - O(\delta | \log\delta|)$,
then this argument shows that distinguish $\dkl(p,q) \leq s$ versus $\dtv(p,q) \geq c$
requires $\Omega(n/\log n)$ samples.
\end{proof}

Finally, we conclude with our lower bound for $\chi^2$-tolerant equivalence testing.

\begin{theorem}\label{thm:twos-lb}
There exists a constant $\ve > 0$ such that any algorithm for equivalence testing between $p$ and $q$ distinguishing the cases:
\begin{itemize}
\item $\dxs(p,q) \leq \ve^2/4$;
\item $\dtv(p,q) \geq \ve$;
\end{itemize}
requires $\Omega(n/\log n)$ samples.
\end{theorem}
\begin{proof}
We reduce the problem of distinguishing $\dh(p,q) \leq \frac{1}{\sqrt{48}} \epsilon$ from $\dtv(p,q) \geq 3\epsilon$ to this.
Define the distributions
\begin{equation*}
p' = \frac{2}{3} p + \frac{1}{3} q, \qquad q' = \frac{1}{3} p + \frac{2}{3} q.
\end{equation*}
Then $m$ samples from~$p'$ and~$q'$ can be simulated by $m$ samples from~$p$ and~$q$.
Furthermore,
\begin{equation*}
\dh(p',q') \leq \frac{1}{\sqrt{48}} \epsilon, \qquad \dtv(p',q') = \frac{1}{3} \dtv(p,q) \geq \epsilon,
\end{equation*}
where we used the fact that Hellinger distance satisfies the data processing inequality.
But then, in the ``close" case,
\begin{equation*}
\dxs(p',q')
= \sum_{i=1}^n \frac{(p'_i - q'_i)^2}{q'_i}
\leq 3 \sum_{i=1}^n \frac{(p'_i - q'_i)^2}{p'_i + q'_i}
\leq 12 \dh^2(p',q') \leq \frac{1}{4} \epsilon^2,
\end{equation*}
where we used the fact that $p'_i \leq 2q'_i$ and Proposition~\ref{prp:didnt-know-it-was-hellinger}.
Hence, this problem, which requires $\Omega(n/\log n)$ samples (by the relationship between total variation and Hellinger distance, and the lower bound for testing total variation-close versus -far of~\cite{ValiantV10a}), reduces to the problem in the proposition, and so that requires $\Omega(n/\log n)$ samples as well.
\end{proof}
