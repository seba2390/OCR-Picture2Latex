\section{Preliminaries}
\label{sec:preliminaries}
In this paper, we will focus on discrete probability distributions over $[n]$.
For a distribution $p$, we will use the notation $p_i$ to denote the mass $p$ places on symbol $i$.
For a set $S \subseteq [n]$ and a distribution $p$ over $[n]$, $p_S$ is the vector $p$ restricted to the coordinates in $S$.
We will call this a \emph{restriction} of distribution $p$.

 
The following probability distances and divergences are of interest to us:
\begin{definition}
The \emph{total variation distance} between $p$ and $q$ is defined as
$$\dtv(p,q) = \max_{S \subseteq [n]} p(S) - q(S) = \frac12\sum_{i \in [n]} \left|p_i - q_i\right| = \|p - q\|_1 \in [0,1].$$
\end{definition}

\begin{definition}
The \emph{KL divergence} between $p$ and $q$ is defined as
$$\dkl(p,q) = \sum_{i \in [n]} p_i \log \left(\frac{p_i}{q_i}\right) \in [0, \infty).$$
This definition uses the convention that $0 \log 0 = 0$.
\end{definition}

\begin{definition}
The \emph{Hellinger distance} between $p$ and $q$ is defined as 
$$\dh(p,q) = \frac{1}{\sqrt{2}}\sqrt{\sum_{i \in [n]} \left(\sqrt{p_i} - \sqrt{q_i}\right)^2} \in [0,1].$$
\end{definition}

\begin{definition}
The \emph{$\chi^2$-divergence} (or \emph{chi-squared divergence}) between $p$ and $q$ is defined as
$$\dxs(p,q) = \sum_{i \in [n]} \frac{(p_i - q_i)^2}{q_i} \in [0,\infty).$$
\end{definition}

\begin{definition}
The \emph{$\ell_2$ distance} between $p$ and $q$ is defined as
$$\dlt(p,q) = \sqrt{\sum_{i \in [n]} (p_i - q_i)^2} = \|p-q\|_2 \in [0,1].$$
\end{definition}
\noindent
We also define these distances for restrictions of distributions $p_S$ and $q_S$ by replacing the summations over $i \in [n]$ with summations over $i \in S$.

We have the following relationships between these distances.
These are well-known for distributions, i.e., see \cite{GibbsS02}, but we prove them more generally for restrictions of distributions in Section~\ref{sec:distanceinequalities}. 
\begin{proposition}
\label{prop:distanceinequalities}
Letting $p_S$ and $q_S$ be restrictions of distributions $p$ and $q$ to $S \subseteq [n]$,
$$\dh^2(p_S,q_S) \leq \dtv(p_S,q_S) \leq \sqrt{2}\dh(p_S,q_S) \leq \sqrt{\sum_{i \in S} (q_i - p_i)  + \dkl(p_S,q_S)} \leq \sqrt{\dxs(p_S,q_S)}.$$
\end{proposition}

We recall that $\dlt$ fits into the picture by its relationship with total variation distance:
\begin{proposition}
\label{prop:ltinequalities}
Letting $p$ and $q$ be distributions over $[n]$,
$$\dlt(p,q) \leq 2\dtv(p,q) \leq \sqrt{n}\dlt(p,q).$$
\end{proposition}
The second inequality follows from Cauchy-Schwarz.

We will also need to following bound for Hellinger distance:
\begin{proposition}\label{prp:didnt-know-it-was-hellinger}
$\displaystyle 2 \dh^2(p,q) \leq \sum_{i=1}^n \frac{(p_i - q_i)^2}{p_i+q_i} \leq 4\dh^2(p,q)$.
\end{proposition}
\begin{proof}
Expanding the Hellinger-squared distance,
\begin{equation*}
\dh^2(p,q) = \frac12 \sum_{i=1}^n (\sqrt{p_i} - \sqrt{q_i})^2 = \frac12 \sum_{i=1}^n \frac{(p_i-q_i)^2}{(\sqrt{p_i}+\sqrt{q_i})^2}.
\end{equation*}
The fact now follows because $(p_i + q_i) \leq (\sqrt{p_i}+\sqrt{q_i})^2 \leq 2(p_i + q_i)$.
\end{proof}
\noindent
The quantity $\sum_{i=1}^n (p_i-q_i)^2/(p_i+q_i)$ is sometimes called the \emph{triangle distance}.
However, we see here that it is essentially the Hellinger distance (up to constant factors).

\begin{proposition}\label{prop:mixing}
Given a number $\delta \in [0,1]$ and a discrete distribution~$r  = (r_1, \ldots, r_n)$, define
\begin{equation*}
r^{+\delta} := (1-\delta) \cdot r + \delta \cdot (\tfrac{1}{n} , \ldots, \tfrac{1}{n}).
\end{equation*}
Then given two discrete distributions $p=(p_1, \ldots, p_n)$ and $q=(q_1, \ldots, q_n)$,
\begin{equation*}
\dtv(p^{+\delta}, q^{+\delta}) = (1-\delta) \dtv(p,q),
\quad
\dlt(p^{+\delta},q^{+\delta})= (1-\delta) \dlt(p,q).
\end{equation*}
In addition, $\dh(p^{+\delta},q^{+\delta}) \geq \dh(p,q) - 2\sqrt{\delta}$.
\end{proposition}
\begin{proof}
The statements for total variation and~$\ell_2$ distance are immediate.  As for the Hellinger distance, we have by the triangle inequality that
\begin{equation*}
\dh(p,q) \leq \dh(p,p^{+\delta}) + \dh(p^{+\delta},q^{+\delta}) + \dh(q^{+\delta},q).
\end{equation*}
We can bound the first term by
\begin{equation*}
\dh^2(p,p^{+\delta})
\leq \dtv( p, p^{+\delta})
= \tfrac{1}{2} \cdot\Vert \delta \cdot p - \delta \cdot (\tfrac{1}{n}, \ldots, \tfrac{1}{n}) \Vert_1
\leq \delta,
\end{equation*}
where the last step is by the triangle inequality,
and a similar argument bounds the third term by~$\sqrt{\delta}$ as well.
Thus, $\dh(p^{+\delta},q^{+\delta}) \geq \dh(p,q) - 2\sqrt{\delta}$.
\end{proof}
A similar technique was employed in~\cite{Goldreich16}.

At times, our algorithms will employ \emph{Poisson sampling}.
Instead of taking $m$ samples from a distribution $p$, we instead take $\mathrm{Poisson}(m)$ samples.
As a result, letting $N_i$ be the number of occurences of symbol $i$, all $N_i$ will be independent and distributed as $\mathrm{Poisson}(m \cdot p_i)$.
We note that this method of sampling is for purposes of analysis -- concentration bounds imply that $\mathrm{Poi}(m) = O(m)$ with high probability, so such an algorithm can be converted to one with a fixed budget of samples at a constant-factor increase in the sample complexity.
