\section{Introduction} \label{sec:intro}

The arch problem in science is determining whether observations of some phenomenon conform to a conjectured model. Often, phenomena of interest are probabilistic in nature, and so are our models of these phenomena; hence, testing their validity becomes a statistical hypothesis testing problem. In mathematical notation, suppose that we have access to samples from some unknown distribution $p$ over some set $\Sigma$ of size $n$. We also have a hypothesis distribution $q$, and our goal is to distinguish whether $p=q$ or $p \neq q$. For instance, we may want to test whether the sizes of some population of insects are normally distributed around their mean by sampling insects and measuring their sizes.

Of course, our models are usually imperfect. In our insect example, perhaps our estimation of the mean and variance of the insect sizes is a bit off. Furthermore, the sizes will clearly always be positive numbers. Yet a Normal distribution could still be a good fit. To get a meaningful testing problem some slack may be introduced, turning the problem into that of distinguishing whether $d_1(p,q) \le \epsilon_1$ versus $d_2(p,q)\ge\epsilon_2$, for some distance measures $d_1(\cdot,\cdot)$ and $d_2(\cdot,\cdot)$ between distributions over $\Sigma$ and some choice of $\epsilon_1$ and $\epsilon_2$ which may potentially depend on $\Sigma$ or even $q$. Regardless, for the problem to be well-defined, the sets of distributions ${\cal C} = \{p~|~d_1(p,q) \le \epsilon_1 \}$ and ${\cal F} = \{p~|~d_2(p,q) \ge \epsilon_2 \}$ should be disjoint. In fact, as our goal is to distinguish between $p \in {\cal C}$ and $p \in {\cal F}$ from samples, we cannot possibly draw the right conclusion with probability $1$ or detect the most minute deviations of $p$ from ${\cal C}$ or ${\cal F}$. So our guarantee should be probabilistic, and there should be some ``gap'' between the sets $\cal C$ and $\cal F$. In sum, the problem is the following:

\medskip \framebox{
\begin{minipage}{15.5cm}
{\em $(d_1,d_2)$-Identity Testing}: Given an explicit description of a distribution $q$ over $\Sigma$, sample access to a distribution $p$ over $\Sigma$, and bounds $\epsilon_1 \ge 0$, and $\epsilon_2, \delta >0$, distinguish with probability at least $1-\delta$ between $d_1(p,q)\le \epsilon_1$ and $d_2(p,q) \ge \epsilon_2$, whenever $p$ satisfies one of these two inequalities.
\end{minipage}}

\medskip \noindent A related problem is when we have sample access to both $p$ and $q$. For example, we might be interested in whether two populations of insects have distributions that are close or far. The resulting problem is the following:

\medskip \framebox{
\begin{minipage}{15.5cm}
{\em $(d_1,d_2)$-Equivalence (or Closeness) Testing}: Given sample access to distributions $p$ and $q$ over $\Sigma$, and bounds $\epsilon_1 \ge 0$, and $\epsilon_2, \delta >0$, distinguish with probability at least $1-\delta$ between $d_1(p,q)\le \epsilon_1$ and $d_2(p,q) \ge \epsilon_2$, whenever $p,q$ satisfy one of these two inequalities.
\end{minipage}}

\medskip The above questions are of course fundamental, and widely studied since the beginning of statistics.  However, most tests only detect certain types of deviations of $p$ from $q$, or are designed for distributions in parametric families. Moreover, most of the emphasis has been on the asymptotic sample regime. To address these challenges, there has been a surge of recent interest in information theory, property testing, and sublinear-time algorithms aiming at finite sample and $d_1$-close vs. $d_2$-far distinguishers, as in the formulations above; see e.g.\ \cite{BatuFFKRW01,BatuKR04,Paninski08,ValiantV17,AcharyaDK15,CanonneDGR16,DiakonikolasK16}. This line of work has culminated in computationally efficient and sample optimal testers for several choices of distances $d_1$ and $d_2$, as well as error parameters $\epsilon_1$ and $\epsilon_2$, for example:
\begin{itemize}
\item for identity testing, when: 
\begin{itemize}
\item $d_2$ is taken to be the total variation distance, and $\epsilon_1=0$~\cite{BatuFFKRW01, Paninski08,ValiantV17};

\item $d_1$ is taken to be the $\chi^2$-divergence, $d_2$ is taken to be the total variation distance, and $\epsilon_1=(\epsilon_2)^2/4$~\cite{AcharyaDK15, DiakonikolasK16};
\end{itemize}

\item for equivalence testing, when $d_2$ is taken to be the total variation distance, and $\epsilon_1=0$~\cite{BatuFRSW13, Valiant11, ChanDVV14}.

\end{itemize}
There are also several other sub-optimal results known for other combinations of $d_1$, $d_2$, $\epsilon_1$ and $\epsilon_2$, and for many combinations there are no known testers. A more extensive discussion of the literature is provided in Section~\ref{sec:related-work}.

\medskip The goal of this paper is to {\em provide a complete mapping of the optimal sample complexity required to obtain computationally efficient testers for identity testing and equivalence testing under the most commonly used notions of distances $d_1$ and $d_2$.} Our results are summarized in Tables~\ref{tab:id}, \ref{tab:eq}, and \ref{tab:l2} and discussed in detail in Section~\ref{sec:results}. In particular, we obtain computationally efficient and sample optimal testers for distances $d_1$ and $d_2$ ranging in the set \{$\ell_2$-distance, total variation distance, Hellinger distance, Kullback-Leibler divergence, $\chi^2$-divergence\},\footnote{These distances are nicely nested, as discussed in Section~\ref{sec:preliminaries}, from the weaker $\ell_2$ to the stronger $\chi^2$-divergence.} and for combinations of these distances and choice of errors $\epsilon_1$ and $\epsilon_2$ which give rise to meaningful testing problems as discussed above. The sample complexities stated in the tables are for probability of error $1/3$. Throwing in extra factors of $O(\log 1/\delta)$ boosts the probability of error to $1-\delta$, as usual.\footnote{Namely, one can repeat the test $O(\log 1/\d)$ times and output the majority result. One can analyze the resulting probability of success by the Chernoff bound.}

Our motivation for this work is primarily the fundamental nature of identity and equivalence testing, as well as of the distances under which we study these problems. It is also the fact that, even though distribution testing is by now a mature subfield of information theory, property testing, and sublinear-time algorithms, several of the testing questions that we consider have had unknown statuses prior to our work. This gap is accentuated by the fact that, as we establish, closely related distances may have radically different behavior. To give a quick example, it is easy to see that $\chi^2$-divergence is the second-order Taylor expansion of KL-divergence. Yet, as we show, the sample complexity for identity testing changes radically when $d_2$ is taken to be total variation or Hellinger distance, and $d_1$ transitions from $\chi^2$ to KL or weaker distances; see~Table~\ref{tab:id}. Prior to this work we knew about a transition somewhere between $\chi^2$-divergence and total variation distance, but our work identifies a more refined understanding of the point of transition. Similar fragility phenomena are identified by our work for equivalence testing, when we switch from total variation to Hellinger distance, as seen in Tables~\ref{tab:eq} and~\ref{tab:l2}. 

Adding to the fundamental nature of the problems we consider here, we should also emphasize that a clear understanding of the different tradeoffs mapped out by our work is critical at this point for the further development of the distribution testing field, as recent experience has established. Let us provide a couple of recent examples, drawing from our prior work. Acharya, Daskalakis, and Kamath~\cite{AcharyaDK15} study whether properties of distributions, such as unimodality or log-concavity, can be tested in total variation distance. Namely, given sample access to a distribution $p$, how many samples are needed to test whether it has some property (modeled by a set ${\cal P}$ of distributions) or whether it is far from having the property, i.e.~$d_{\rm TV}(p,{\cal P})>\epsilon$, for some  error $\epsilon$? Their approach is to first learn a proxy distribution $\hat{p} \in {\cal P}$ that satisfies $d'(p,\hat{p}) \le \epsilon'$ for some distance $d'$, whenever $p \in {\cal P}$, then reduce the property testing problem to $(d',d_{\rm TV})$-identity testing  of $p$ to $\hat{p}$. Interestingly, rather than picking $d'$ to be total variation distance, they take it to be $\chi^2$-divergence, which leads to optimal testers of sample complexity $O(\sqrt{n}/\epsilon^2)$ for several ${\cal P}$'s such as monotone, unimodal, and log-concave distributions over $[n]$. Had they picked $d'$ to be total variation distance, they would be stuck with a $\Omega(n/\log n)$ sample complexity in the resulting identity testing problem, as Table~\ref{tab:id} illustrates, which would lead to a suboptimal overall tester. The choice of $\chi^2$-divergence in the work of Acharya et al.~was somewhat ad hoc. By providing a full mapping of the sample complexity tradeoffs in the use of different distances, we expect to help future work in identifying better where the bottlenecks and opportunities lie.

Another example supporting our expectation can be found in recent work of Daskalakis and Pan~\cite{DaskalakisP17}. They study equivalence testing of Bayesian networks under total variation distance. Bayesian networks are flexible models expressing combinatorial structure in high-dimensional distributions in terms of a directed acyclic graph (DAG) specifying their conditional dependence structure. The challenge in testing Bayes nets is that their support scales exponentially in the number of nodes, and hence naive applications of known equivalence tests lead to sample complexities that are exponential in the number of nodes, even when the in-degree $\delta$ of the underlying DAGs is bounded. To address this challenge, Daskalakis and Pan establish ``localization-of-distance'' results of the following form, for various choices of distance $d$: ``If two Bayes nets $P$ and $Q$ are $\epsilon$-far in total variation distance, then there exists a small set of nodes $S$ (whose size is $\Delta+1$, where $\Delta$ is again the maximum in-degree of the underlying DAG where $P$ and $Q$ are defined) such that the marginal distributions of $P$ and $Q$ over the nodes of set $S$ are $\epsilon'$-far under distance $d$.'' When they take $d$ to be total variation distance, they can show $\epsilon'=\Omega(\epsilon/m)$, where $m$ is the number of nodes in the underlying DAG (i.e.~the dimension). Given this localization of distance, to test whether two Bayes nets $P$ and $Q$ satisfy $P=Q$ vs. $d_{\rm TV}(P,Q) \ge \epsilon$, it suffices to test, for all relevant marginals $P_S$ and $Q_S$ whether $P_S=Q_S$ vs. $d_{\rm TV}(P_S,Q_S)=\Omega(\epsilon/m)$. From Table~\ref{tab:eq} it follows that this requires sample size superlinear in $m$, which is suboptimal. Interestingly, when they take $d$ to be the square Hellinger distance, they can establish a localization-of-distance result with $\epsilon'=\epsilon^2/2m$. By Table~\ref{tab:eq}, to test each $S$ they need sample complexity that is linear in $m$, leading to an overall dependence of the sample complexity on $m$ that is $\tilde{O}(m)$,\footnote{The extra log factors are to guarantee that the tests performed on all sets $S$ of size $\delta+1$ succeed.} which is optimal up to log factors. Again, switching to a different distance results in near-optimal overall sample complexity, and our table is guidance as to where the bottlenecks and opportunities lie. 

%Bringing our point home, for identity testing of Bayesian networks we can improve the dependence of Daskalakis and Pan's sample complexity on the degree to the optimal sample dependence $\Theta(|\Sigma|^{1/2(\delta+1)} m /\epsilon^2)$. (Their results are already optimal for closeness testing.)

Finally, we comment that tolerant testing (i.e., when $\ve_1 > 0$) is perhaps one of the most interesting questions in the design of practically useful testers.
Indeed, as mentioned before, in many statistical settings there may be model misspecification.
For example, why should one expect to be receiving samples from \emph{precisely} the uniform distribution?
As such, one may desire that a tester is \emph{robust} to small errors, and accepts all distributions which are \emph{close} to uniform.
Unfortunately, Valiant and Valiant~\cite{ValiantV11a} ruled out the possibility of a strongly sublinear tester which has total variation tolerance, showing that such a problem requires $\Theta\left(\frac{n}{\log n}\right)$ samples.
However, as shown by Acharya, Daskalakis, and Kamath~\cite{AcharyaDK15}, $\chi^2$-tolerance is possible with only $O\left(\frac{\sqrt{n}}{\ve^2}\right)$ samples.
This raises the following question: Which distances can a tester be tolerant to, while maintaining a strongly sublinear sample complexity? We outline what is possible.

\subsection{Results} \label{sec:results}
Our results are pictorially presented in Tables~\ref{tab:id}, \ref{tab:eq}, and \ref{tab:l2}.
We note that these tables are intended to provide only references to the \emph{sample complexity} of each testing problem, rather than exhaustively cover all prior work. 
As such, several references are deferred to Section~\ref{sec:related-work}.
In Tables~\ref{tab:id} and \ref{tab:eq}, each cell contains the complexity of testing whether two distributions are close in the distance for that row, versus far in the distance for that column.\footnote{Note that we chose constants in our theorem statements for simplicity of presentation, and they may not match the constants presented in the table. This can be remedied by appropriate changing of constants in the algorithms and constant factor increases in the sample complexity.}
These distances and their relationships are covered in detail in Section~\ref{sec:preliminaries}, but we note that the distances are scaled and transformed such that problems become harder as we traverse the table down or to the right.
In other words, lower bounds hold for cells which are down or to the right in the table, and upper bounds hold for cells which are up or to the left; problems with the same complexity are shaded with the same color.
The dark grey boxes indicate problems which are not well-defined, i.e. two distributions could simultaneously be close in KL and far in $\chi^2$-divergence.
\input{idtable}
\input{equivtable}
\input{l2table}


We highlight some of our results:
\begin{enumerate}
\item We give an $O(\sqrt{n}/\ve^2)$ sample algorithm for identity testing whether $\dxs(p,q) \leq \ve^2/4$ or $\dh(p,q) \geq \ve/\sqrt{2}$ (Theorem \ref{thm:ones-csq-h}).
This is the first algorithm which achieves the optimal dependence on both $n$ and $\ve$ for identity testing with respect to Hellinger distance (even non-tolerantly).
We note that a $O(\sqrt{n}/\ve^4)$ algorithm was known, due to optimal identity testers for total variation distance and the quadratic relationship between total variation and Hellinger distance.
\item In the case of identity testing, a stronger form of tolerance (i.e., KL divergence instead of $\chi^2$) causes the sample complexity to jump to $\Omega\left(n/\log n\right)$ (Theorem~\ref{thm:ones-lb}).
We find this a bit surprising, as $\chi^2$-divergence is the second-order Taylor expansion of KL divergence, so one might expect that the testing problems have comparable complexities.
\item In the case of equivalence testing, \emph{even $\chi^2$-tolerance} comes at the cost of an $\Omega\left(n/\log n\right)$ sample complexity (Theorem~\ref{thm:twos-lb}).
This is a qualitative difference from identity testing, where $\chi^2$-tolerance came at no cost.
\item However, in both identity and equivalence testing, $\ell_2$ tolerance comes at no additional cost (Theorems~\ref{thm:ones-tv}, \ref{thm:ones-h}, \ref{thm:twos-tv}, and \ref{thm:twos-h}).
Thus, in many cases, $\ell_2$ tolerance is the best one can do if one wishes to maintain a strongly sublinear sample complexity.
\end{enumerate}

From a technical standpoint, our algorithms are $\chi^2$-statistical tests, and most closely resemble those of~\cite{AcharyaDK15} and~\cite{ChanDVV14} (similar $\chi^2$-tests were employed in~\cite{ValiantV17, DiakonikolasKN15a, CanonneDGR16}).
However, crucial changes are required to satisfy the more stringent requirements of testing with respect to Hellinger distance.
In our identity tester for Hellinger, we deal with this different distance measure by pruning light domain elements of $q$ less aggressively than~\cite{AcharyaDK15},
in combination with a preliminary test to reject early if the difference between $p$ and $q$ is contained exclusively within the set of light elements -- this is a new issue that cannot arise when testing with respect to total variation distance.
In our equivalence tester for Hellinger, we follow an approach, similar to~\cite{ChanDVV14} and~\cite{DiakonikolasK16},
of analyzing the light and heavy domain elements separately, with the challenge that the algorithm does not know which elements are which.
Finally, to achieve $\ell_2$ tolerance in these cases, we use a ``mixing" strategy
in which instead of testing based solely on samples from~$p$ and~$q$, we mix in some number (depending on our application) of samples from the uniform distribution.
At a high level, the purpose of mixing is to make our distributions \emph{well-conditioned},
i.e.\ to ensure that all probability values are sufficiently large.
Such a strategy was recently employed by Goldreich in~\cite{Goldreich16} for uniformity testing.

\subsubsection{Comments on $\ell_2$-tolerance}
$\ell_2$ tolerance has been indirectly considered in~\cite{GoldreichR00, BatuFFKRW01, BatuFRSW13} through their weak tolerance for total variation distance and the relationship with $\ell_2$ distance, though these results have suboptimal sample complexity.
Our equivalence testing results improve upon~\cite{ChanDVV14} by adding $\ell_2$-tolerance.
We note that~\cite{DiakonikolasK16} also provides $\ell_2$-tolerant testers (as well as~\cite{DiakonikolasKN15a} for the case of uniformity), comparable to those obtained in Theorems~\ref{thm:ones-tv},~\ref{thm:ones-h}, and~\ref{thm:twos-h}, though this tolerance is not explicitly analyzed in their paper.
This can be seen by noting that the underlying tester from~\cite{ChanDVV14} is tolerant, and the ``flattening'' operation they apply reduces the $\ell_2$-distance between the distributions.
The testers in~\cite{DiakonikolasK16} are those of Propositions 2.7, 2.10, and 2.15, combined with the observation of Remark 2.8.
We rederive these results for completeness, and to show a direct way of proving $\ell_2$-tolerance.
Note that Theorem~\ref{thm:twos-h} also improves upon Proposition 2.15 of~\cite{DiakonikolasK16} by removing log factors in the sample complexity.

\subsubsection{Comments on the $\Theta(n/\log n)$ Results}
\label{sec:nlogn}
Our upper bounds in the bottom-left portion of the table are based off the total variation distance estimation algorithm of Jiao, Han, and Weissman~\cite{JiaoHW16}, where an $\Theta(n/\log n)$ complexity is only derived for $\ve \geq 1/\poly(n)$.
Similarly, in~\cite{ValiantV10a}, the lower bounds are only valid for constant $\ve$.
We believe that the precise characterization  is a very interesting open problem.
In the present work, we focus on the case of constant $\ve$ for these testing problems.

We wish to draw attention to the bottom row of the table, and note that the two testing problems are $\dtv(p,q) \leq \ve/2$ versus $\dtv(p,q) \geq \ve$, and $\dtv(p,q) \leq \ve^2/4$ versus $\dh(p,q) \geq \ve/\sqrt{2}$.
This difference in parameterization is required to make the two cases in the testing problem disjoint.
With this parameterization, we conjecture that the latter problem has a greater dependence on $\ve$ as it goes to $0$ (namely, $\ve^{-4}$ versus $\ve^{-2}$), so we colour the box a slightly darker shade of orange.

\subsection{Related Work}
\label{sec:related-work}
\input{related}

\subsection{Organization}
The organization of this paper is as follows.
In Section~\ref{sec:preliminaries}, we state preliminaries and notation used in this paper.
In Sections~\ref{sec:ones-ub} and~\ref{sec:twos-ub}, we prove upper bounds for identity testing and equivalence testing (respectively) based on $\chi^2$-style statistics.
In Section~\ref{sec:est-ub}, we prove upper bounds for distribution testing based on distance estimation.
Finally, in Section~\ref{sec:lb}, we prove testing lower bounds.
