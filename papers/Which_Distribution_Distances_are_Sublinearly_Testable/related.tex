The most classic distribution testing question is uniformity testing, which is identity testing when $\ve_1 = 0$, $d_2$ is total variation distance, and $q$ is the uniform distribution.
This was first studied in theoretical computer science in~\cite{GoldreichR00}.
Paninski gave an optimal algorithm (for when $\ve_2$ is not too small) with a complexity of $O(\sqrt{n}/\ve^2)$ and a matching lower bound~\cite{Paninski08}.
More generally, letting $q$ be an arbitrary distribution, exact total variation identity testing was studied~\cite{BatuFFKRW01}, and an (instance) optimal algorithm was given by Valiant and Valiant~\cite{ValiantV17}, with the same complexity of $O(\sqrt{n}/\ve^2)$.
Optimal algorithms for this problem were rediscovered several times, see i.e.~\cite{DiakonikolasKN15a, AcharyaDK15, DiakonikolasK16, DiakonikolasGPP16}.

Equivalence (or closeness) testing was studied in~\cite{BatuFRSW13}, in the same setting ($\ve_1 = 0$, $d_2$ is total variation distance).
A lower bound of $\Omega(n^{2/3})$ was given by~\cite{Valiant11}.
Tight upper and lower bounds were given in~\cite{ChanDVV14}, which shows interesting behavior of the sample complexity as the parameter $\ve$ goes from large to small.
This problem was also studied in the setting where one has unequal sample sizes from the two distributions~\cite{BhattacharyaV15,DiakonikolasK16}.
When the distance $d_1$ is Hellinger, the complexity is qualitatively different, as shown by~\cite{DiakonikolasK16}.
They prove a nearly-optimal upper bound and a tight lower bound for this problem.

\cite{Waggoner15, DoBaNNR11} also consider testing problems with other distances, namely $\ell_p$ distances and earth mover's distance (also known as Wasserstein distance), respectively.

Tolerant identity testing (where $\ve_1 = O(\ve)$ and $d_1$ is total variation distance) was studied in~\cite{ValiantV10a,ValiantV10b,ValiantV11a,ValiantV11b}, through the (equivalent) lens of estimating total variation distance between distributions.
In these works, $\Theta\left(n/\log n\right)$ bounds were proven for the sample complexity.
Several other related problems (i.e., support size and entropy estimation) share the same sample complexity, and have enjoyed significant study~\cite{AcharyaOST17, WuY16, AcharyaDOS17}.
The closest related results to our work are those on estimating distances between distributions~\cite{JiaoHW16, JiaoKHW17, HanJW16}.

$\chi^2$-tolerance (when $d_1$ is $\chi^2$-divergence and $\ve_1 = O(\ve^2)$) was introduced and applied by~\cite{AcharyaDK15} for testing families of distributions, i.e., testing if a distribution is monotone or far from being monotone.
It was shown that this tolerance comes at no additional cost over vanilla identity testing; that is, the sample complexity is still $O(\sqrt{n}/\ve^2)$.
Testing such families of distributions was also studied by~\cite{CanonneDGR16}.

Testing with respect to Hellinger distance was applied in~\cite{DaskalakisP17} for testing Bayes networks.
Since lower bounds of~\cite{AcharyaDK15} show that distribution testing suffers from the curse of dimensionality, further structural assumptions must be made if one wishes to test multivariate distributions.
This ``high-dimensional frontier'' has also been studied on graphical models by~\cite{DaskalakisDK18} and~\cite{CanonneDKS17} (for Ising models and Bayesian networks, respectively).

Our work focuses on characterizing the complexity of identity and equivalence testing in the worst case over pairs $p$ and $q$.
Related works attempt to nail down the sample complexity of identity testing on an \emph{instance-by-instance} basis~\cite{ValiantV17, JiaoHW16, DiakonikolasK16, BlaisCG17} -- that is, reducing the sample complexity depending on which distribution $q$ is given as input (and sometimes depending on $p$ as well).
We consider this to be an interesting open question for different distances $d_1$ and $d_2$.
For example, Theorem~\ref{thm:untestable} states that identity testing is impossible when $d_2$ is the KL divergence.
However, if $q$ is the uniform distribution, then the complexity becomes $\Theta(\sqrt{n})$.
An instance-by-instance analysis would allow one to bypass some of these strong lower bounds.

This is only a fraction of recent results; we direct the reader to~\cite{Canonne15} for an excellent recent survey of distribution testing.

