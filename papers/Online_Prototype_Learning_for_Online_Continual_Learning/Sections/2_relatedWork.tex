\section{Related Work}
\paragraph{Continual learning.}
Continual learning methods can be roughly summarized into three categories: regularization-based, parameter-isolation-based, and replay-based methods. Regularization-based methods~\cite{EWC, regular1, regular2, regular3} add extra regularization constraints on network parameters to mitigate forgetting. Parameter-isolation-based methods~\cite{PNN, para-iso1, para-iso2, pathNet} avoid forgetting by dynamically allocating parameters or modifying the architecture of the network. Replay-based methods~\cite{ER, MIR, GSS, AGEM, DER++, GDumb} maintain and update a memory bank (buffer) that stores exemplars of past tasks. 
Among them, replay-based methods are the most popular for their simplicity yet efficiency. Experience Replay~\cite{ER} randomly samples from the buffer. MIR~\cite{MIR} retrieves buffer samples by comparing the interference of losses. Furthermore, in the online setting, ASER~\cite{ASER} introduces a buffer management theory based on the Shapley value. SCR~\cite{SCR} utilizes supervised contrastive loss~\cite{SupCL} for training and the nearest-class-mean classifier for testing. OCM~\cite{OCM} prevents forgetting through mutual information maximization.

Unlike these methods that focus on selecting which samples to store or learning features only by instances, our work rethinks the catastrophic forgetting from a new shortcut learning perspective, and proposes to learn representative and discriminative features through online prototypes.


\paragraph{Knowledge distillation in continual learning.}
Another solution to catastrophic forgetting is to preserve previous knowledge by self-distillation~\cite{iCaRL, DER++, kd1, Co2L, protoAug, OCM}. iCaRL~\cite{iCaRL} constrains changes of learned knowledge by distillation and employs class prototypes for nearest neighbor prediction. Co$^2$L~\cite{Co2L} proposes a self-distillation loss to preserve learned features. PASS~\cite{protoAug} maintains the decision boundaries of old classes by distilling old prototypes.
However, it is hard to distill useful knowledge when previous models are not learned well.
In contrast, we propose a general feedback mechanism to enhance the discrimination of classes that are prone to misclassification, which overcomes the limitations on knowledge distillation.



\paragraph{Prototypes in continual learning.}
Some previous methods~\cite{iCaRL, SCR, protoAug} attempt to utilize prototypes to mitigate catastrophic forgetting. As mentioned above, iCaRL and SCR employ class prototypes as classifiers, and PASS distills old prototypes to retain learned knowledge. Nevertheless, computing prototypes with all samples is extremely expensive for training. There are also some works considering the use of prototypes in the online scenario. CoPE~\cite{online_pro_ema} designs the prototypes with a high momentum-based update for each observed batch. A recent work~\cite{online_pro_accum} estimates class prototypes on all seen data using mean update criteria. However, regardless of momentum update or mean update, accumulating previous features as prototypes may be detrimental to future learning, since the features learned in old classes may not be discriminative when encountering new classes due to shortcut learning. 
In contrast, the proposed online prototypes only utilize the data visible at the current time step, which significantly decreases the computational cost and 
is more suitable for online CL.



\paragraph{Contrastive learning.}
Inspired by breakthroughs in self-supervised learning~\cite{CPC, MoCo, SimCLR, BYOL, SwAV, ProPos}, many studies~\cite{SCR, ER_AML, OCM, Co2L, online_pro_accum} in CL use contrastive learning to learn generalized features. An early work~\cite{ssl4onlineCL} analyzes and reveals the impact of contrastive learning on online CL. Among them, the work most related to ours is PCL~\cite{PCL}, which calculates infoNCE loss~\cite{CPC} between instance and prototype.
The most significant difference is that the loss in \methodname only considers online prototypes, and there is no involvement of instances. 
Please refer to Appendix~\ref{appendix:PCL} for detailed comparisons between our \methodname and PCL.
