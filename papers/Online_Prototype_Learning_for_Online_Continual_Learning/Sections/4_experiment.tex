
\input{Figures/TSNE_motivation}
\input{Tables/Accuracy}

\section{Experiments}
\subsection{Experimental Setup}
\paragraph{Datasets.}
We use three image classification benchmark datasets, including \textbf{CIFAR-10}~\cite{cifar10_100}, \textbf{CIFAR-100}~\cite{cifar10_100}, and \textbf{TinyImageNet}~\cite{tinyImageNet}, to evaluate the performance of online CIL methods. 
Following~\cite{ASER, SCR, DVC}, we split CIFAR-10 into 5 disjoint tasks, where each task has 2 disjoint classes, 10,000 samples for training, and 2,000 samples for testing, and split CIFAR-100 into 10 disjoint tasks, where each task has 10 disjoint classes, 5,000 samples for training, and 1,000 samples for testing.
Following~\cite{OCM}, we split TinyImageNet into 100 disjoint tasks, where each task has 2 disjoint classes, 1,000 samples for training, and 100 samples for testing.
Note that the order of tasks is fixed in all experimental settings.

\paragraph{Baselines.}
We compare our \frameworkName with 13 baselines, including 10 replay-based online CL baselines: {AGEM}~\cite{AGEM}, {MIR}~\cite{MIR}, {GSS}~\cite{GSS}, {ER}~\cite{ER}, {GDumb}~\cite{GDumb}, {ASER}~\cite{ASER}, {SCR}~\cite{SCR}, {CoPE}~\cite{online_pro_ema}, {DVC}~\cite{DVC}, and {OCM}~\cite{OCM}; 3 offline CL baselines that use knowledge distillation by running them in one epoch: {iCaRL}~\cite{iCaRL}, {DER++}~\cite{DER++}, and PASS~\cite{protoAug}. Note that PASS is a non-exemplar method.

\paragraph{Evaluation metrics.}
We use Average Accuracy and Average Forgetting~\cite{ASER, DVC} to measure the performance of our framework in online CIL. Average Accuracy evaluates the accuracy of the test sets from all seen tasks, defined as $\text {Average Accuracy} =\frac{1}{T} \sum_{j=1}^T a_{T, j},$
where $a_{i, j}$ is the accuracy on task $j$ after the model is trained from task $1$ to $i$.
Average Forgetting represents how much the model forgets about each task after being trained on the final task, defined as
$\text { Average Forgetting } =\frac{1}{T-1} \sum_{j=1}^{T-1} f_{T, j}, 
\text { where } f_{i, j}=\max _{k \in\{1, \ldots, i-1\}} a_{k, j}-a_{i, j}.$

\paragraph{Implementation details.}
We use ResNet18~\cite{ResNet} as the backbone $f$ and a linear layer as the projection head $g$ like~\cite{SCR, OCM, Co2L}; the hidden dim in $g$ is set to 128 as~\cite{SimCLR}. We also employ a linear layer as the classifier $\varphi$. We train the model from scratch with Adam optimizer and an initial learning rate of $5\times10^{-4}$ for all datasets. The weight decay is set to $1.0\times10^{-4}$. Following~\cite{ASER, DVC}, we set the batch size $N$ as 10, and following~\cite{OCM} the replay batch size $m$ is set to 64. 
For CIFAR-10, we set the ratio of \dataaugname $\alpha = 0.25$. For CIFAR-100 and TinyImageNet, $\alpha$ is set to $0.1$. The temperature $\tau = 0.5$ and $\tau^{\prime} = 0.07$.
For baselines, we also use ResNet18 as their backbone and set the same batch size and replay batch size for fair comparisons.
We reproduce all baselines in the same environment with their source code and default settings; see Appendix~\ref{appendix:baselines} for implementation details about all baselines.
We report the average results across 15 runs for all experiments.



\paragraph{Data augmentation.}
Similar to data augmentations used in SimCLR~\cite{SimCLR}, we use resized-crop, horizontal-flip, and gray-scale as our data augmentations. For all baselines, we also use these augmentations. In addition, for DER++\cite{DER++}, SCR~\cite{SCR}, and DVC~\cite{DVC}, we follow their default settings and use their own extra data augmentations. OCM~\cite{OCM} uses extra rotation augmentations, which are also used in \frameworkName.


\subsection{Motivation Justification}
\label{pre_exp}
\paragraph{Shortcut learning in online CL.}
Shortcut learning is severe in online CL since the model cannot learn sufficient representative features due to the single-pass data stream. To intuitively demonstrate this issue,  
we conduct GradCAM++~\cite{Grad-cam++} on the training set of CIFAR-10 ($M=0.2k$) after the model is trained incrementally, as shown in Fig.~\ref{fig:heatmap}.
Each row in Fig.~\ref{fig:heatmap} represents a task with two classes.
We can observe that although ER and DVC predict the correct class, the models actually take shortcuts and focus on some object-unrelated features. 
An interesting phenomenon is that ER tends to take shortcuts in each task. For example, ER learns the sky on both the airplane class in task 1 (the first row) and the bird class in task 2 (the second row) . Thus, ER forgets almost all the knowledge of the old classes.  
DVC maximizes the mutual information between instances like contrastive learning~\cite{SimCLR, MoCo}, which only partially alleviates shortcut learning in online CL. 
In contrast, \frameworkName focuses on the representative features of the objects themselves. The results confirm that learning representative features is crucial against shortcut learning; see Appendix~\ref{appendix:more_visual} for more visual explanations.


\input{Tables/Forget.tex}
\input{Figures/IncrementalACC_ConfusionMatrix}


\paragraph{Class confusion in online CL.}
Fig.~\ref{fig:tsne_motivation} provides the $t$-SNE~\cite{tsne} visualization results for ER and \frameworkName on the test set of CIFAR-10 ($M=0.2k$). 
We can draw intuitive observations as follows. 
(1) There is serious class confusion in ER.
When the new task (task 2) arrives, features learned in task 1 are not discriminative for task 2, leading to class confusion and decreased performance in old classes.
(2) Shortcut learning may cause class confusion. For example, the performance of ER decreases more on airplanes compared to automobiles, probably because birds in the new task have more similar backgrounds to airplanes, as shown in Fig.~\ref{fig:heatmap}.
(3) \frameworkName achieves better discrimination both on task 1 and task 2. The results demonstrate that \frameworkName can maintain discrimination of all seen classes and significantly mitigate forgetting by 
combining the proposed \methodname and \dataaugname.






\subsection{Results and Analysis}
\label{result}
\paragraph{Performance of average accuracy.}
Table~\ref{tab:acc} presents the results of average accuracy with different memory bank sizes ($M$) on three benchmark datasets. Our \frameworkName consistently outperforms all baselines on three datasets.
Remarkably, the performance improvement of \frameworkName is more significant when the memory bank size is relatively small; this is critical for online CL with limited resources. For example, compared to the second-best method OCM, \frameworkName achieves about 10$\%$ and 6$\%$ improvement on CIFAR-10 when $M$ is 100 and 200, respectively. 
The results show that our \frameworkName can learn more representative and discriminative features with a limited memory bank.
Compared to baselines that use knowledge distillation (iCaRL, DER++, PASS, OCM), our \frameworkName achieves better performance by leveraging the feedback of online prototypes.  
Besides, \frameworkName significantly outperforms PASS and CoPE that also use prototypes, showing that online prototypes are more suitable for online CL. 


We find that the performance improvement tends to be gentle when $M$ increases.
The reason is that as $M$ increases, the samples in the memory bank become more diverse, and the model can extract sufficient information from massive samples to distinguish seen classes. 
In addition, many baselines perform poorly on CIFAR-100 and TinyImageNet due to a dramatic increase in the number of tasks. In contrast, \frameworkName still performs well and improves accuracy over the second best.



\paragraph{Performance of average forgetting.}
We report the Average Forgetting results of our \frameworkName and all baselines on three benchmark datasets in Table~\ref{tab:forget}. The results confirm that \frameworkName can effectively mitigate catastrophic forgetting. 
For CIFAR-10 and CIFAR-100, \frameworkName achieves the lowest average forgetting compared to all replay-based baselines. 
For TinyImageNet, our result is a little higher than iCaRL and CoPE but better than the latest methods DVC and OCM. 
The reason is that iCaRL uses a nearest class mean classifier, but we use softmax and FC layer during the test phase, and CoPE slowly updates prototypes with a high momentum.
However, as shown in Table~\ref{tab:acc}, \frameworkName provides more accurate classification results than iCaRL and CoPE. 
It is a fact that when the maximum accuracy of a task is small, the forgetting on this task is naturally rare, even if the model completely forgets what it learned.





\paragraph{Performance of each incremental step.}
We evaluate the average incremental performance~\cite{DER++, DVC} on CIFAR-10 ($M=0.1k$) and CIFAR-100 ($M=0.5k$), which indicates the accuracy over all seen tasks at each incremental step. 
Fig.~\ref{fig:incrementalAcc} shows that \frameworkName achieves better accuracy and effectively mitigates forgetting while the performance of most baselines degrades rapidly with the arrival of new classes.

\paragraph{Confusion matrices at the end of learning.}
We report the confusion matrices of our \frameworkName and the second-best method OCM, as shown in Fig.~\ref{fig:confusionMatrix}. 
After learning the last task (\ie, the last two classes), OCM forgets the knowledge of early tasks (classes 0 to 3). 
In contrast, \frameworkName performs relatively well in all classes, especially in the first task (classes 0 and 1), outperforming OCM by 27.8\% average improvements.
The results show that learning representative and discriminative features is crucial to mitigate catastrophic forgetting; see Appendix~\ref{appendix:extra_exp} for extra experimental results.  




\subsection{Ablation Studies}
\label{ablation}

\input{Tables/Ablation.tex}

\paragraph{Effects of each component.} Table~\ref{tab:ablation} presents the ablation results of each component. Obviously, \methodname and \dataaugname can consistently improve the average accuracy of classification. 
We can observe that the effect of \methodname is more significant on more tasks while \dataaugname plays a crucial role when the memory bank size is limited. Moreover, when combining \methodname and \dataaugname, the performance is further improved, which indicates that both can benefit from each other. For example, \dataaugname boosts \methodname by about 6$\%$ improvements on CIFAR-10 ($M=0.1k$), and the performance of \dataaugname is improved by about 3$\%$ on CIFAR-100 ($M=0.5k$) by combining \methodname.


\paragraph{Equilibrium in \methodname.}
When learning new classes, the data of new classes is involved in both $\mathcal{L}^{\mathrm{new}}_{\mathrm{pro}}$ and $\mathcal{L}^{\mathrm{seen}}_{\mathrm{pro}}$ of \methodname, where $\mathcal{L}^{\mathrm{new}}_{\mathrm{pro}}$ only focuses on learning new knowledge while $\mathcal{L}^{\mathrm{seen}}_{\mathrm{pro}}$ tends to alleviate forgetting on seen classes.
To explore the best way of learning new classes, we consider three scenarios for \methodname in Table~\ref{tab:ablation}.
The results show that only learning new knowledge (w/o $\mathcal{L}^{\mathrm{seen}}_{\mathrm{pro}}$) or only consolidating the previous knowledge (w/o $\mathcal{L}^{\mathrm{new}}_{\mathrm{pro}}$) can significantly degrade the performance, which indicates that both are indispensable for online CL.
Furthermore, when $\mathcal{L}^{\mathrm{seen}}_{\mathrm{pro}}$ only considers old classes and ignores new classes ($\mathcal{L}^{\mathrm{seen}}_{\mathrm{pro}}$ w/o $\mathcal{C}^\mathrm{new}$), the performance also decreases. These results show that the equilibrium of all seen classes (\methodname) can achieve the best performance and is crucial for online CL.


\paragraph{Effects of \dataaugname.} 
To verify the advantage of \dataaugname, we compare it with the completely random mixup
in Table~\ref{tab:ablation_mixup}.
\input{Tables/Mixup_APF}
\dataaugname outperforms random mixup in all three scenarios. Notably, \dataaugname works significantly when the memory bank size is small, which shows that the feedback can prevent class confusion due to a restricted memory bank; see Appendix~\ref{appendix:ablations} for extra ablation studies.



\subsection{Validation of Online Prototypes}
\label{prove_onlinePrototypes}
\input{Figures/CosineSimilarity}
Fig.~\ref{fig:cosine_similarity} shows the cosine similarity between online prototypes and global prototypes (prototypes of the entire memory bank) at each time step.
For the first mini-batch of each task, online prototypes are equal to global prototypes (similarity is 1, omitted in Fig.~\ref{fig:cosine_similarity}).
In the first task, online and global prototypes are updated synchronously with the model updates, resulting in high similarity. 
In subsequent tasks, the model initially learns inadequate features of new classes, causing online prototypes to be inconsistent with global prototypes and low similarity, which shows that accumulating early features as prototypes may be harmful to new tasks. However, the similarity will improve as the model learns, because the model gradually learns representative features of new classes.
Furthermore, the similarity on old classes is only slightly lower, showing that online prototypes are resistant to forgetting. 
