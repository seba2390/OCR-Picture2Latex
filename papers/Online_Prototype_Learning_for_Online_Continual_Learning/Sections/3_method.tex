\section{Method}
Fig.~\ref{fig:framework} presents the illustration of the proposed \frameworkName.
In this section,  
we start by providing the problem definition of online CIL. Then, we describe the definition of the online prototype, the proposed online prototype equilibrium, and the proposed adaptive prototypical feedback. Finally, we propose an online prototype learning framework.

\subsection{Problem Definition}
Formally, online CIL considers a continuous sequence of tasks from a single-pass data stream $\mathfrak{D}=\left\{\mathcal{D}_1, \ldots, \mathcal{D}_T \right\} $, where $\mathcal{D}_t = \left\{ x_{i}, y_{i} \right\} ^{N_t}_{i=1} $ is the dataset of task $t$, and $T$ is the total number of tasks. Dataset $\mathcal{D}_t$ contains $N_t$ labeled samples, $y_{i}$ is the class label of sample $x_{i}$ and $y_{i} \in \mathcal{C}_t$, where $\mathcal{C}_t$ is the class set of task $t$ and the class sets of different tasks are disjoint. 
For replay-based methods, a memory bank is used to store a small subset of seen data, and we also maintain a memory bank $\mathcal{M}$ in our method.
At each time step of task $t$, the model receives a mini-batch data $X \cup X^\mathrm{b}$ for training, where $X$ and $X^\mathrm{b}$ are drawn from the i.i.d distribution $\mathcal{D}_t$ and the memory bank $\mathcal{M}$, respectively. 
Moreover, we adopt the single-head evaluation setup~\cite{EWC}, where a unified classifier must choose labels from all seen classes at inference due to unavailable task identifiers. 
The goal of online CIL is to train a unified model on data seen only once while predicting well on both new and old classes.

\subsection{Online Prototype Definition}
Prior to introducing the online prototypes, we first present the network architecture of our \frameworkName. Suppose that the model consists of three components: an encoder network $f$, a projection head $g$, and a classifier $\varphi$. Each sample $x$ in incoming data $X$ (a mini-batch data from new classes) is mapped to a projected vectorial embedding (representation) $\mathbf{z}$ by encoder $f$ and projector $g$:
\begin{align}
\label{eq:cal_z}
    \mathbf{z} = g(f(\operatorname{aug}(x);\theta_f);\theta_g),
\end{align}
where $\operatorname{aug}$ represents the data augmentation operation, $\theta_f$ and $\theta_g$ represent the parameters of $f$ and $g$, respectively, and $\mathbf{z}$ is $\ell_2$-normalized. 
Similar to Eq.~\eqref{eq:cal_z}, we use $\mathbf{z}^\mathrm{b}$ to denote the representation of replay data $X^\mathrm{b}$ (a mini-batch data from seen classes in the memory bank). 

At each time step of task $t$, the online prototype of each class is defined as the mean representation in a mini-batch:
\begin{align}
\label{eq:cal_p}
    \mathbf{p}_i = \frac{1}{n_i}\sum\nolimits_j\mathbf{z}_j\cdot \mathbbm{1}\{y_j = i\},
\end{align}
where $n_i$ is the number of samples for class $i$ in a mini-batch, and $\mathbbm{1}$ is the indicator function. 
We can get a set of $K$ online prototypes  in $X$, $\mathcal{P} = \left\{ \mathbf{p}_{i} \right\} ^{K}_{i=1}$, and a set of $K^\mathrm{b}$ online prototypes in $X^\mathrm{b}$, $\mathcal{P}^\mathrm{b} = \left\{ \mathbf{p}_i^\mathrm{b} \right\} ^{K^\mathrm{b}}_{i=1}$.
Note that $K = |\mathcal{P}| \leq |\mathcal{C}_t|$ and $K^\mathrm{b} = |\mathcal{P}^\mathrm{b}| \leq \sum_{i=1}^{t}|\mathcal{C}_i| $, where $|\cdot|$ denotes the cardinal number.



\subsection{Online Prototype Equilibrium}
The introduced online prototypes can provide representative features and avoid class-unrelated information.  
These characteristics are exactly the key to counteracting shortcut learning in online CL.
Besides, maintaining the discrimination among seen classes is also essential to mitigate catastrophic forgetting.
Based on these, we attempt to learn representative features of each class by pulling online prototypes $\mathcal{P}$ and their augmented views $\widehat{\mathcal{P}}$ closer in the embedding space, and learn discriminative features between classes by pushing online prototypes of different classes away, formally defined as a contrastive loss:
\begin{align}
\label{eq:proto_infoNCE}
    \ell(\mathcal{P},\widehat{\mathcal{P}})\!=\!
    % \frac{-1}{K}
    \frac{-1}{|\mathcal{P}|}\sum_{i=1}^{|\mathcal{P}|}\!\log\! 
    \tfrac
    {\exp \big(\tfrac{{\mathbf{p}_i^\mathrm{T} \widehat{\mathbf{p}}_i}}{\tau}\big)}
    {
    \sum\limits_{j} \exp \big(\tfrac{{\mathbf{p}_i^\mathrm{T} \widehat{\mathbf{p}}_j}}{\tau}\big)
    +\!
    \sum\limits_{\substack{j \neq i}} \exp \big(\tfrac{{\mathbf{p}_i^\mathrm{T} \mathbf{p}_j}}{\tau}\big) 
    },
\end{align}
where 
$\tau$ is the temperature hyper-parameter, 
$\mathcal{P}$ and $\widehat{\mathcal{P}}$ are $\ell_2$-normalized. To compute the contrastive loss across all positive pairs in both $(\mathcal{P}, \widehat{\mathcal{P}})$ and $(\widehat{\mathcal{P}}, \mathcal{P})$, we define $\mathcal{L}_{\mathrm{pro}}$ as the final contrastive loss over online prototypes:
\begin{align}
    \mathcal{L}_{\mathrm{pro}}(\mathcal{P},\widehat{\mathcal{P}}) = 
    \frac{1}{2}
    \left[\ell(\mathcal{P}, \widehat{\mathcal{P}}) + \ell(\widehat{\mathcal{P}}, \mathcal{P})\right].
\end{align}



Considering the learning of new classes and the consolidation of learned knowledge simultaneously in online CL, we propose Online Prototype Equilibrium (\methodname) to 
learn representative and discriminative features on both new and seen classes by employing $\mathcal{L}_{\mathrm{pro}}$:
\begin{equation}
    \begin{aligned}
    \mathcal{L}_{\mathrm{\methodname}}
    &=
    \mathcal{L}^{\mathrm{new}}_{\mathrm{pro}}(\mathcal{P},\widehat{\mathcal{P}}) + \mathcal{L}^{\mathrm{seen}}_{\mathrm{pro}}(\mathcal{P}^\mathrm{b},\widehat{\mathcal{P}}^\mathrm{b}),
    \end{aligned}
\end{equation}
where
$\mathcal{L}^{\mathrm{new}}_{\mathrm{pro}}$ focuses on learning knowledge from \emph{new} classes, and $\mathcal{L}^{\mathrm{seen}}_{\mathrm{pro}}$ is dedicated to preserving learned knowledge of all \emph{seen} classes.
\textit{This process is similar to a zero-sum game, 
and \methodname aims to achieve the equilibrium to play a win-win game.}
Concretely,
as the model learns, the knowledge of new classes is gained and added to the prototypes over the memory bank $\mathcal{M}$, causing $\mathcal{L}^{\mathrm{seen}}_{\mathrm{pro}}$ gradually changes to the equilibrium that separates all seen classes well, including new ones. 
This variation is crucial to mitigate forgetting and is consistent with the goal of CIL.



\subsection{Adaptive Prototypical Feedback} 
Although \methodname can bring an overall equilibrium, it tends to treat each class \emph{equally}. 
In fact, the degree of confusion varies among classes, 
and the model should focus purposefully on confused classes to consolidate learned knowledge. 
To this end, we propose Adaptive Prototypical Feedback (\dataaugname) with the feedback of online prototypes to sense the classes that are prone to be misclassified and then enhance their decision boundaries.
 
For each class pair in the memory bank $\mathcal{M}$, \dataaugname calculates the distances between online prototypes of all seen classes from the previous time step, showing the class confusion status by these distances. The closer the two prototypes are, the easier to be misclassified. 
Based on this analysis, 
our idea is to enhance the boundaries for those classes. Therefore, we convert the prototype distance matrix to a probability distribution $P$ over the classes via a symmetric Gaussian kernel, defined as follows:
\begin{align}
\label{eq:cal_d}
    P_{i, j} \propto \exp (-\| \mathbf{p}_i^\mathrm{b} - \mathbf{p}_j^\mathrm{b} \|_2^2),
\end{align}
where $i,j \in \{ 1, \ldots, |\mathcal{P}^\mathrm{b}| \}$ and $i \neq j$. 
Then, 
all probabilities are normalized to a probability mass function that sums to one.
\dataaugname returns probabilities to $\mathcal{M}$ for guiding the next sampling process and enhancing decision boundaries of easily misclassified classes. 


Our adaptive prototypical feedback is implemented as a sampling-based mixup. Specifically, 
\dataaugname adaptively selects more samples from easily misclassified classes in $\mathcal{M}$ for mixup~\cite{Mixup} according to the probability distribution $P$. 
Considering not over-penalizing the equilibrium of current online prototypes, we introduce a two-stage sampling strategy for replay data $X^\mathrm{b}$ of size $m$. 
First, we select $n_{\mathrm{\dataaugname}}$ samples  
with $P$, and a larger $P_{a,b}$ means more sampling from classes $a$ and $b$. Here, $n_{\mathrm{\dataaugname}} = \alpha \cdot m$, and $\alpha$ is the ratio of \dataaugname.
Second, the remaining $m-n_{\mathrm{\dataaugname}}$ samples are uniformly randomly selected from the entire memory bank to avoid the model only focusing on easily misclassified classes and disrupting the established equilibrium. 




\subsection{Overall Framework of \frameworkName}
The overall structure of \frameworkName is shown in Fig.~\ref{fig:framework}. \frameworkName comprises two key components based on proposed online prototypes: Online Prototype Equilibrium (\methodname) and Adaptive Prototypical Feedback (\dataaugname). 
With the two components, 
the model can learn representative features against shortcut learning, and 
all seen classes maintain discriminative when learning new classes. 
However, classes may not be compact, because the online prototypes cannot cover full instance-level information.
To further achieve intra-class compactness, 
we employ supervised contrastive learning~\cite{SupCL} to learn instance-wise representations:
\begin{equation}
\begin{aligned}
    \mathcal{L}_{\mathrm{INS}}
    &=
    \sum_{i=1}^{2N} \frac{-1}{\left|I_i\right|} \sum_{j \in I_i} \log \frac{\exp \left(\mathrm{sim}(\mathbf{z}_i, \mathbf{z}_j) / \tau^{\prime}\right)}{\sum\limits_{k \neq i} \exp \left(\mathrm{sim}(\mathbf{z}_i, \mathbf{z}_k) / \tau^{\prime}\right)}
    \\
    &+
    \sum_{i=1}^{2m} \frac{-1}{\left|I_i^{\mathrm{b}}\right|} \sum_{j \in I_i^{\mathrm{b}}} \log \frac{\exp (\mathrm{sim}(\mathbf{z}_i^{\mathrm{b}}, \mathbf{z}_j^{\mathrm{b}}) / \tau^{\prime})}{\sum\limits_{k \neq i} \exp \left(\mathrm{sim}(\mathbf{z}_i^{\mathrm{b}}, \mathbf{z}_k^{\mathrm{b}}) / \tau^{\prime}\right)},
\end{aligned}
\end{equation}
where $I_i=\left\{j \in\{1, \ldots, 2 N\} \mid j \neq i, y_j=y_i\right\}$ and $I_i^\mathrm{{b}}=\left\{j \in\{1, \ldots, 2m\} \mid j \neq i, y_j^\mathrm{b}=y_i^\mathrm{b}\right\}$ are the set of positive samples indexes to $\mathbf{z}_i$ and $\mathbf{z}_i^\mathrm{{b}}$, respectively. $y_i^\mathrm{b}$ is the class label of input $x_i^\mathrm{b}$ from $X^\mathrm{b}$. $N$ is the batch size of $X$. $\tau^{\prime}$ is the temperature hyperparameter.
The similarity function $\mathrm{sim}$ is computed in the same way as Eq.~(9) in OCM~\cite{OCM}.

Thus, the total loss of our \frameworkName framework is given as:
\begin{align}
    \mathcal{L}_{\mathrm{\frameworkName}}=\mathcal{L}_{\mathrm{\methodname}} + \mathcal{L}_{\mathrm{INS}} + \mathcal{L}_{\mathrm{CE}},
\end{align}
where $\mathcal{L}_{\mathrm{CE}} = \mathrm{CE}(y^\mathrm{b}, \varphi(f(\operatorname{aug}(x^\mathrm{b}))))$ is the cross-entropy loss; see Appendix~\ref{appendix:algorithm} for detailed training algorithms.

Following other replay-based methods~\cite{ER, SCR, OCM}, we update the memory bank in each time step by uniformly randomly selecting samples from $X$ to push into $\mathcal{M}$ and, if $\mathcal{M}$ is full, pulling an equal number of samples out of $\mathcal{M}$.
