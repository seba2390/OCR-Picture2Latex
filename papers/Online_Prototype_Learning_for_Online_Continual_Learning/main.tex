\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
% \usepackage[review]{cvpr}      % To produce the REVIEW version
% \usepackage{cvpr}              % To produce the CAMERA-READY version
% \usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{bbm}
\usepackage[caption=false]{subfig}
\usepackage{enumitem}
\usepackage{algorithmicx}
\usepackage[ruled]{algorithm2e}
\usepackage{algpseudocode}
\usepackage{appendix}

\newcommand{\frameworkName}{OnPro\xspace}
\newcommand{\methodname}{OPE\xspace}
\newcommand{\dataaugname}{APF\xspace}
\newcommand{\std}[1]{\footnotesize{#1}}
\newcommand{\vct}[1]{\boldsymbol{#1}}
\newlength\savewidth\newcommand\shline{\noalign{\global\savewidth\arrayrulewidth\global\arrayrulewidth1.25pt}\hline\noalign{\global\arrayrulewidth\savewidth}}



% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
%\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}
\usepackage[pagebackref=true, breaklinks=true,colorlinks,citecolor=citecolor, linkcolor=linkcolor, bookmarks=false]{hyperref}
\definecolor{citecolor}{HTML}{1F801F}
\definecolor{linkcolor}{HTML}{ED1C24}
\iccvfinalcopy % *** Uncomment this line for the final submission

\renewcommand{\paragraph}[1]{\noindent\textbf{#1}}

\def\iccvPaperID{xxxx} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
% \ificcvfinal\pagestyle{empty}\fi

\begin{document}

%%%%%%%%% TITLE
\title{Online Prototype Learning for Online Continual Learning}


\author{Yujie Wei$^{1}$\qquad Jiaxin Ye$^{1}$\qquad Zhizhong Huang$^{2}$\qquad Junping Zhang$^{2}$\qquad Hongming Shan$^{1,3,4}$\thanks{Corresponding author}
\\
$^{1}$ Institute of Science and Technology for Brain-inspired Intelligence, Fudan University\\
$^{2}$ School of Computer Science,
Fudan University\\
$^{3}$ MOE Frontiers Center for Brain Science, Fudan University\\
$^{4}$ Shanghai Center for Brain Science and Brain-inspired Technology\\
{\tt\small \{yjwei22, jxye22\}@m.fudan.edu.cn},\quad
{\tt\small \{zzhuang19, jpzhang, hmshan\}@fudan.edu.cn}
}

\maketitle
% Remove page # from the first page of camera-ready.
\ificcvfinal\thispagestyle{empty}\fi


%%%%%%%%% ABSTRACT
\input{Sections/0_abstract.tex}

%%%%%%%%% BODY TEXT
\input{Sections/1_intro.tex}
\input{Sections/2_relatedWork.tex}
\input{Sections/3_method.tex}
\input{Sections/4_experiment.tex}
\input{Sections/5_conclusion.tex}

%\clearpage
{\small
\bibliographystyle{ieee_fullname}
\begin{thebibliography}{10}\itemsep=-1pt

\bibitem{regular1}
Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny, Marcus Rohrbach, and
  Tinne Tuytelaars.
\newblock Memory aware synapses: Learning what (not) to forget.
\newblock In {\em Proceedings of the European Conference on Computer Vision
  (ECCV)}, pages 139--154, 2018.

\bibitem{MIR}
Rahaf Aljundi, Eugene Belilovsky, Tinne Tuytelaars, Laurent Charlin, Massimo
  Caccia, Min Lin, and Lucas Page-Caccia.
\newblock Online continual learning with maximal interfered retrieval.
\newblock In {\em Advances in Neural Information Processing Systems},
  volume~32, 2019.

\bibitem{GSS}
Rahaf Aljundi, Min Lin, Baptiste Goujaud, and Yoshua Bengio.
\newblock Gradient based sample selection for online continual learning.
\newblock {\em Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem{DER++}
Pietro Buzzega, Matteo Boschini, Angelo Porrello, Davide Abati, and Simone
  Calderara.
\newblock Dark experience for general continual learning: a strong, simple
  baseline.
\newblock {\em Advances in Neural Information Processing Systems},
  33:15920--15930, 2020.

\bibitem{ER_AML}
Lucas Caccia, Rahaf Aljundi, Nader Asadi, Tinne Tuytelaars, Joelle Pineau, and
  Eugene Belilovsky.
\newblock New insights on reducing abrupt representation change in online
  continual learning.
\newblock {\em arXiv:2203.03798}, 2022.

\bibitem{SwAV}
Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and
  Armand Joulin.
\newblock Unsupervised learning of visual features by contrasting cluster
  assignments.
\newblock {\em Advances in Neural Information Processing Systems},
  33:9912--9924, 2020.

\bibitem{Co2L}
Hyuntak Cha, Jaeho Lee, and Jinwoo Shin.
\newblock {Co\({}^{\mbox{2}}\)L}: Contrastive continual learning.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 9516--9525, 2021.

\bibitem{Grad-cam++}
Aditya Chattopadhay, Anirban Sarkar, Prantik Howlader, and Vineeth~N
  Balasubramanian.
\newblock {Grad-CAM++}: Generalized gradient-based visual explanations for deep
  convolutional networks.
\newblock In {\em 2018 IEEE Winter Conference on Applications of Computer
  Vision (WACV)}, pages 839--847, 2018.

\bibitem{EWC}
Arslan Chaudhry, Puneet~K Dokania, Thalaiyasingam Ajanthan, and Philip~HS Torr.
\newblock Riemannian walk for incremental learning: Understanding forgetting
  and intransigence.
\newblock In {\em Proceedings of the European Conference on Computer Vision
  (ECCV)}, pages 532--547, 2018.

\bibitem{AGEM}
Arslan Chaudhry, Marc'Aurelio Ranzato, Marcus Rohrbach, and Mohamed Elhoseiny.
\newblock Efficient lifelong learning with {A-GEM}.
\newblock {\em arXiv:1812.00420}, 2018.

\bibitem{ER}
Arslan Chaudhry, Marcus Rohrbach, Mohamed Elhoseiny, Thalaiyasingam Ajanthan,
  Puneet~K Dokania, Philip~HS Torr, and Marc'Aurelio Ranzato.
\newblock On tiny episodic memories in continual learning.
\newblock {\em arXiv:1902.10486}, 2019.

\bibitem{SimCLR}
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton.
\newblock A simple framework for contrastive learning of visual
  representations.
\newblock In {\em International Conference on Machine Learning}, pages
  1597--1607, 2020.

\bibitem{onlineCL1}
Aristotelis Chrysakis and Marie-Francine Moens.
\newblock Online continual learning from imbalanced data.
\newblock In {\em International Conference on Machine Learning}, pages
  1952--1961, 2020.

\bibitem{survey3}
Matthias De~Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, et~al.
\newblock A continual learning survey: Defying forgetting in classification
  tasks.
\newblock {\em IEEE Transactions on Pattern Analysis and Machine Intelligence},
  44(7):3366--3385, 2021.

\bibitem{online_pro_ema}
Matthias De~Lange and Tinne Tuytelaars.
\newblock Continual prototype evolution: Learning online from non-stationary
  data streams.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 8250--8259, 2021.

\bibitem{ViT}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
  Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
  Heigold, Sylvain Gelly, et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock {\em arXiv:2010.11929}, 2020.

\bibitem{kd_work}
Zhiyuan Fang, Jianfeng Wang, Lijuan Wang, Lei Zhang, Yezhou Yang, and Zicheng
  Liu.
\newblock Seed: Self-supervised distillation for visual representation.
\newblock {\em arXiv:2101.04731}, 2021.

\bibitem{pathNet}
Chrisantha Fernando, Dylan Banarse, Charles Blundell, Yori Zwols, David Ha,
  Andrei~A. Rusu, Alexander Pritzel, and Daan Wierstra.
\newblock {PathNet}: Evolution channels gradient descent in super neural
  networks.
\newblock {\em arXiv 1701.08734}, 2017.

\bibitem{CL_CIL1}
Enrico Fini, Victor G~Turrisi Da~Costa, Xavier Alameda-Pineda, Elisa Ricci,
  Karteek Alahari, and Julien Mairal.
\newblock Self-supervised models are continual learners.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 9621--9630, 2022.

\bibitem{catastrophic}
Robert~M French.
\newblock Catastrophic forgetting in connectionist networks.
\newblock {\em Trends in Cognitive Sciences}, 3(4):128--135, 1999.

\bibitem{ssl4onlineCL}
Jhair Gallardo, Tyler~L Hayes, and Christopher Kanan.
\newblock Self-supervised training enhances online continual learning.
\newblock {\em arXiv:2103.14010}, 2021.

\bibitem{shortcut}
Robert Geirhos, J{\"o}rn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel,
  Wieland Brendel, Matthias Bethge, and Felix~A Wichmann.
\newblock Shortcut learning in deep neural networks.
\newblock {\em Nature Machine Intelligence}, 2(11):665--673, 2020.

\bibitem{catastrophic2}
Ian~J Goodfellow, Mehdi Mirza, Da Xiao, Aaron Courville, and Yoshua Bengio.
\newblock An empirical investigation of catastrophic forgetting in
  gradient-based neural networks.
\newblock {\em arXiv:1312.6211}, 2013.

\bibitem{BYOL}
Jean-Bastien Grill, Florian Strub, Florent Altch{\'e}, Corentin Tallec, Pierre
  Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila~Pires, Zhaohan
  Guo, Mohammad Gheshlaghi~Azar, et~al.
\newblock Bootstrap your own latent-a new approach to self-supervised learning.
\newblock {\em Advances in Neural Information Processing Systems},
  33:21271--21284, 2020.

\bibitem{DVC}
Yanan Gu, Xu Yang, Kun Wei, and Cheng Deng.
\newblock Not just selection, but exploration: Online class-incremental
  continual learning via dual view consistency.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 7442--7451, 2022.

\bibitem{OCM}
Yiduo Guo, Bing Liu, and Dongyan Zhao.
\newblock Online continual learning through mutual information maximization.
\newblock In {\em International Conference on Machine Learning}, pages
  8109--8126, 2022.

\bibitem{onlineCL2}
Jiangpeng He, Runyu Mao, Zeman Shao, and Fengqing Zhu.
\newblock Incremental learning in online scenario.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 13926--13935, 2020.

\bibitem{online_pro_accum}
Jiangpeng He and Fengqing Zhu.
\newblock Exemplar-free online continual learning.
\newblock In {\em 2022 IEEE International Conference on Image Processing
  (ICIP)}, pages 541--545, 2022.

\bibitem{MoCo}
Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick.
\newblock Momentum contrast for unsupervised visual representation learning.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 9729--9738, 2020.

\bibitem{ResNet}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 770--778, 2016.

\bibitem{regular3}
Xu He and Herbert Jaeger.
\newblock Overcoming catastrophic interference using conceptor-aided
  backpropagation.
\newblock In {\em International Conference on Learning Representations}, 2018.

\bibitem{kd_work2}
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean.
\newblock Distilling the knowledge in a neural network.
\newblock {\em arXiv:1503.02531}, 2015.

\bibitem{Imagenet_spilt}
Saihui Hou, Xinyu Pan, Chen~Change Loy, Zilei Wang, and Dahua Lin.
\newblock Learning a unified classifier incrementally via rebalancing.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 831--839, 2019.

\bibitem{ProPos}
Zhizhong Huang, Jie Chen, Junping Zhang, and Hongming Shan.
\newblock Learning representation for clustering via prototype scattering and
  positive sampling.
\newblock {\em IEEE Transactions on Pattern Analysis and Machine Intelligence}, 45(6):7509--7524
  2023.

\bibitem{SupCL}
Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip
  Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan.
\newblock Supervised contrastive learning.
\newblock {\em Advances in Neural Information Processing Systems},
  33:18661--18673, 2020.

\bibitem{cifar10_100}
Alex Krizhevsky, Geoffrey Hinton, et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem{DNN}
Alex Krizhevsky, Ilya Sutskever, and Geoffrey~E Hinton.
\newblock {ImageNet} classification with deep convolutional neural networks.
\newblock {\em Communications of the ACM}, 60(6):84--90, 2017.

\bibitem{tinyImageNet}
Ya Le and Xuan Yang.
\newblock Tiny {ImageNet} visual recognition challenge.
\newblock {\em CS 231N}, 7(7):3, 2015.

\bibitem{para-iso2}
Soochan Lee, Junsoo Ha, Dongsu Zhang, and Gunhee Kim.
\newblock A neural Dirichlet process mixture model for task-free continual
  learning.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{regular2}
Sang{-}Woo Lee, Jin{-}Hwa Kim, Jaehyun Jun, Jung{-}Woo Ha, and Byoung{-}Tak
  Zhang.
\newblock Overcoming catastrophic forgetting by incremental moment matching.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  4652--4662, 2017.

\bibitem{PCL}
Junnan Li, Pan Zhou, Caiming Xiong, and Steven C.~H. Hoi.
\newblock Prototypical contrastive learning of unsupervised representations.
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem{kd1}
Zhizhong Li and Derek Hoiem.
\newblock Learning without forgetting.
\newblock {\em IEEE Transactions on Pattern Analysis and Machine Intelligence},
  40(12):2935--2947, 2017.

\bibitem{online_survey}
Zheda Mai, Ruiwen Li, Jihwan Jeong, David Quispe, Hyunwoo Kim, and Scott
  Sanner.
\newblock Online continual learning in image classification: An empirical
  survey.
\newblock {\em Neurocomputing}, 469:28--51, 2022.

\bibitem{SCR}
Zheda Mai, Ruiwen Li, Hyunwoo Kim, and Scott Sanner.
\newblock Supervised contrastive replay: Revisiting the nearest class mean
  classifier in online class-incremental continual learning.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition (CVPR) Workshops}, pages 3589--3599, 2021.

\bibitem{survey2}
Marc Masana, Xialei Liu, Bart{\l}omiej Twardowski, Mikel Menta, Andrew~D
  Bagdanov, and Joost van~de Weijer.
\newblock Class-incremental learning: survey and performance evaluation on
  image classification.
\newblock {\em IEEE Transactions on Pattern Analysis and Machine Intelligence},
  2022.

\bibitem{CPC}
Aaron van~den Oord, Yazhe Li, and Oriol Vinyals.
\newblock Representation learning with contrastive predictive coding.
\newblock {\em arXiv:1807.03748}, 2018.

\bibitem{survey1}
German~I Parisi, Ronald Kemker, Jose~L Part, Christopher Kanan, and Stefan
  Wermter.
\newblock Continual lifelong learning with neural networks: A review.
\newblock {\em Neural networks}, 113:54--71, 2019.

\bibitem{GDumb}
Ameya Prabhu, Philip~HS Torr, and Puneet~K Dokania.
\newblock GDumb: A simple approach that questions our progress in continual
  learning.
\newblock In {\em Proceedings of the European Conference on Computer Vision
  (ECCV)}, pages 524--540, 2020.

\bibitem{iCaRL}
Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, and Christoph~H
  Lampert.
\newblock {iCaRL}: Incremental classifier and representation learning.
\newblock In {\em Proceedings of the IEEE conference on Computer Vision and
  Pattern Recognition}, pages 2001--2010, 2017.

\bibitem{PNN}
Andrei~A Rusu, Neil~C Rabinowitz, Guillaume Desjardins, Hubert Soyer, James
  Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell.
\newblock Progressive neural networks.
\newblock {\em arXiv:1606.04671}, 2016.

\bibitem{para-iso1}
Joan Serra, Didac Suris, Marius Miron, and Alexandros Karatzoglou.
\newblock Overcoming catastrophic forgetting with hard attention to the task.
\newblock In {\em International Conference on Machine Learning}, pages
  4548--4557, 2018.

\bibitem{ASER}
Dongsub Shim, Zheda Mai, Jihwan Jeong, Scott Sanner, Hyunwoo Kim, and Jongseong
  Jang.
\newblock Online class-incremental continual learning with adversarial Shapley
  value.
\newblock In {\em Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~35, pages 9630--9638, 2021.

\bibitem{VGG}
Karen Simonyan and Andrew Zisserman.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock {\em arXiv:1409.1556}, 2014.

\bibitem{tsne}
Laurens Van~der Maaten and Geoffrey Hinton.
\newblock Visualizing data using {t-SNE}.
\newblock {\em Journal of Machine Learning Research}, 9(11), 2008.

\bibitem{Mixup}
Hongyi Zhang, Moustapha Cisse, Yann~N Dauphin, and David Lopez-Paz.
\newblock mixup: Beyond empirical risk minimization.
\newblock {\em arXiv:1710.09412}, 2017.

\bibitem{protoAug}
Fei Zhu, Xu-Yao Zhang, Chuang Wang, Fei Yin, and Cheng-Lin Liu.
\newblock Prototype augmentation and self-supervision for incremental learning.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 5871--5880, 2021.

\end{thebibliography}

}

\clearpage

\input{Appendix}

\end{document}