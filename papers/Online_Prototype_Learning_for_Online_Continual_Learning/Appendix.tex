\renewcommand{\thetable}{A\arabic{table}}
\renewcommand{\thefigure}{A\arabic{figure}}
\renewcommand{\theequation}{A\arabic{equation}}
\setcounter{figure}{0}
\setcounter{table}{0}  
\setcounter{equation}{0}  

\appendix
\section*{Appendix}
\section{Difference from PCL}
\label{appendix:PCL}
PCL~\cite{PCL} bridges instance-level contrastive learning with clustering based on unsupervised representation learning. We discuss the differences between PCL and \methodname in the following three parts.

\paragraph{(1) Difference in learning settings.}
PCL is an unsupervised contrastive learning method while \methodname explicitly leverages class labels to compute online prototypes. Thus, \methodname belongs to the supervised setting.

\paragraph{(2) Difference in prototype calculation.}
At each time step (iteration), PCL uses all samples of classes to obtain prototypes by performing K-means clustering.
In contrast, \methodname just utilizes a mini-batch of training data to calculate online prototypes.

\paragraph{(3) Difference in contrastive form (most significant differences).}
The anchor of \methodname as well as its positive and negative samples are online prototypes, which means no instance is involved, while PCL takes instance-level representation as the anchor and cluster centers as the positive and negative samples. 
Specifically,
\methodname regards an online prototype and its augmented view as a positive pair; online prototypes of different classes are regarded as negative pairs. 
PCL clusters samples $M$ times, then regards a representation $\mathbf{z}$ of one image (instance) and its cluster center $\mathbf{c}$ as a positive pair; $\mathbf{z}$ and other cluster centers as negative pairs, formally defined as:
\begin{align}
\label{eq:PCL_infoNCE}
    \mathcal{L}_{\mathrm{PCL}}
    \!=-\sum_{i=1}^{2N}\left( \!\frac{1}{M} \sum_{m=1}^M\!\log\! 
    \frac
    {\exp (\frac{\mathbf{z}_i^\mathrm{T} \mathbf{c}_i^m}{\tau^m})}
    {\sum_{j=0}^{r} 
    \exp (\frac{\mathbf{z}_i^\mathrm{T} \mathbf{c}_j^m}{\tau^m})}\right),
\end{align}
where $N$ is the batch size, $r$ is the number of negative samples, and $\tau^m$ is the temperature hyper-parameter.

In addition, at each iteration, PCL needs to cluster all samples $M$ times, which is very expensive for training, while our \methodname only needs to compute online prototypes once.






\section{Extra Experimental Results}
\label{appendix:extra_exp}
\subsection{More Visual Explanations}
\label{appendix:more_visual}
To further demonstrate the shortcut learning in online CL, we randomly select several images from all (ten) classes in the training set of CIFAR-10 and provide their visual explanations by GradCAM++~\cite{Grad-cam++}, as shown in Fig.~\ref{fig:moreVisual}.
The results confirm that shortcut learning is widespread in online CL. Although ER~\cite{ER} and DVC~\cite{DVC} predict the correct class, they still focus on some oversimplified and object-unrelated features. In contrast, our \frameworkName learns representative features of classes.
\input{Figures/AppendixFigs/MoreVisual}




\subsection{Knowledge Distillation on ER}
As analyzed in the main paper, it is hard to distill useful knowledge due to shortcut learning. To demonstrate this, we apply the knowledge distillation in \cite{protoAug} to ER, and the results are shown in Table~\ref{tab:KD_ER}.
The performance of ER decreases after using knowledge distillation, and a larger memory bank does not result in significant performance gains.
\input{Tables/AppendixTables/knowledgeDistillation_ER}


\subsection{Experiments on Larger Datasets}
We conduct extra experiments on ImageNet-100 and ImageNet-1k. ImageNet-100
is a subset of ImageNet-1k with randomly sampled 100 classes; we follow~\cite{Imagenet_spilt} to use the fixed random seed (1993) for dataset generation. We set the number of tasks to 50, the batch size and the buffer batch size to 10, and the memory bank size to 1k for ImageNet-100 and 5k for ImageNet-1k. For a fair comparison, all methods use the same data augmentations, including resized-crop, horizontal-flip, and gray-scale. 
The mean Average Accuracy over 3 runs are reported in Table~\ref{tab:imagenet100_1k}, suggesting: (i) on larger datasets, our OnPro still achieves the best performance and is more stable (lower STD); and (ii) the performance on larger datasets varies greatly. For example, on ImageNet-1k, DVC fails, ER is unstable (large STD), and SCR performs even worse than ER.
\input{Tables/AppendixTables/largerDatasets}


\subsection{Visualization of All Classes} 
\input{Figures/AppendixFigs/TSNE}
\input{Tables/AppendixTables/ablation_CE}
To demonstrate the impact of our \frameworkName on classification, we provide the visualization of \frameworkName and OCM for all classes in the {test set} on CIFAR-10 ($M=0.2k$), as shown in Fig.~\ref{fig:visual_cifar10_all}. It is intuitive that the closer the prototypes of the two classes are, the more confused these two classes become. Obviously, OCM does not avoid class confusion, especially for the three animal classes of \texttt{Bird}, \texttt{Cat}, and \texttt{Dog}, while \frameworkName achieves clear inter-class dispersion. 
Furthermore, compared to OCM, \frameworkName can perceive semantically similar classes and present their relationships in the embedding space. Specifically, for the two classes of \texttt{Automobile} and \texttt{Truck}, their distributions are adjacent in \frameworkName because they have more similar semantics compared to other classes. 
However, OCM cannot capture the semantics relationships, causing the two classes to be relatively far apart. 
The results suggest that \frameworkName can achieve an equilibrium status that separates all
seen classes well by learning representative and discriminative features with online prototypes.





\section{Extra Ablation Studies}
\label{appendix:ablations}
\subsection{Class Balance on Cross-Entropy Loss} 
\label{appendix:ablation_CE}
In Table~\ref{tab:ablation_CE}, we find that the way to calculate the cross-entropy (CE) loss can significantly affect the performance of \frameworkName, where $\mathcal{L}_\mathrm{CE}(\mathrm{both}) = l(y \cup y^\mathrm{b}, \varphi(f(x \cup x^\mathrm{b})))$ and $\mathcal{L}_\mathrm{CE}(\mathrm{sepa}) = l(y, \varphi(f(x))) + l(y^\mathrm{b}, \varphi(f(x^\mathrm{b})))$. Here we omit $\operatorname{aug}$ for simplicity. Both $\mathcal{L}_\mathrm{CE}(\mathrm{both})$ and $\mathcal{L}_\mathrm{CE}(\mathrm{sepa})$ degrade the performance because adding the data of new classes will bring serious class imbalance, causing the classifier to easily overfit to new classes and forget previous knowledge.



\subsection{Effects of Rotation Augmentation} 
As mentioned in the main paper, besides resized-crop, horizontal-flip, and gray-scale, OCM and \frameworkName use Rotation augmentation (Rot) like~\cite{protoAug}. To explore the effects of Rot, we employ it for some SOTA baselines, as shown in Table~\ref{tab:dataAug}. We find that using Rot can improve the performance of baselines except for SCR. However, they are still inferior to \frameworkName.
\input{Tables/AppendixTables/dataAug}




\input{Tables/AppendixTables/alpha_APF}

\subsection{Effects of the \dataaugname Ratio $\alpha$}
Encouraging the model to have a tendency to focus on confused classes is helpful for mitigating catastrophic forgetting. However, excessive focus on these classes may disrupt the established equilibrium. 
Therefore, we study the trade-off factor $\alpha$ on CIFAR-10 (${M=0.2k}$) and CIFAR-100 (${M=0.5k}$), and the results are shown in Table~\ref{tab:alpha}.
On the one hand, when $\alpha$ is too small, the \dataaugname reduces to the random selection and takes little account of easily misclassified classes.
On the other hand, too large $\alpha$ causes focusing too much on confused classes and ignoring general cases. Based on the experimental results, we set $\alpha=0.25$ on CIFAR-10 and $\alpha=0.1$ on CIFAR-100 and TinyImageNet.


\subsection{Effects of Projection Head $g$}
We employ a projection head $g$ to get representations, which is widely-used in contrastive learning~\cite{SimCLR}. For baselines, SCR~\cite{SCR}, DVC~\cite{DVC}, and OCM~\cite{OCM} also use a projection head to get representations. To explore the effects of the projector $g$ in \frameworkName, we conduct the experiment in Table~\ref{tab:noProjector}. 
The result shows that projector $g$ can only bring a slight performance improvement, and also illustrates that the performance of \frameworkName comes mainly from our proposed components.
\input{Tables/AppendixTables/ablation_noProjector}




\subsection{Effects of Memory Bank Batch Size $m$}
Fig.~\ref{fig:memoryBankBatchSize} shows the effects of memory bank batch size. We can observe that the performance of \frameworkName improves as the memory bank batch size increases. 
However, the training time also grows with larger memory bank batch sizes. 
Following~\cite{OCM}, we set the memory bank batch size to 64.
\input{Figures/AppendixFigs/MemoryBatchSize}



\section{Training Algorithms of \frameworkName and \dataaugname}
\label{appendix:algorithm}
The training procedures of the proposed \frameworkName and \dataaugname are presented in Algorithms~\ref{alg2} and~\ref{alg_aug}, respectively. The source code will be made publicly available upon the acceptance of this work.
\input{Algorithms/algorithm_OnPro}
\input{Algorithms/algorithm_APF}



\section{Implementation Details about Baselines}
\label{appendix:baselines}
\input{Tables/AppendixTables/baselines}
The hyperparameters of \frameworkName are given in the main paper. Here we discuss in detail how each baseline is implemented.

For all baselines, we follow their original paper and default settings to set the hyperparameters. We set the random seed to 0 and run the experiment 15 times in the same program to get the results.

For iCaRL, AGEM, and ER, we use the SGD optimizer and set the learning rate to 0.1. We uniformly randomly select samples to update the memory bank and replay.

For DER++, we use the SGD optimizer and set the learning rate to 0.03. We fix $\alpha$ to 0.1 and $\beta$ to 0.5.

For PASS, we use the Adam optimizer and set the learning rate to 0.001. The weight decay is set to 2e-4. We set the loss weights $\lambda$ and $\gamma$ to 10 and fix the temperature as 0.1.

For GSS, we use the SGD optimizer and set the learning rate to 0.1. The number of batches randomly sampled from the memory bank to estimate the maximal gradients cosine similarity score is set to 64, and the random sampling batch size for calculating the score is also set to 64.

For MIR, we use the SGD optimizer and set the learning rate to 0.1. The number of subsamples is set as 100.

For GDumb, we use the SGD optimizer and set the learning rate to 0.1. The value for gradient clipping is set to 10. The minimal learning rate is set to 0.0005, and the epochs to train for the memory bank are 70.

For ASER, we use the SGD optimizer and set the learning rate to 0.1. The number of nearest neighbors to perform ASER is set to 3. We use mean values of Adversarial SV and Cooperative SV, and set the maximum number of samples per class for random sampling to 1.5. We use the SV-based methods for memory update and retrieval as given in the original paper.

For SCR, we use the SGD optimizer and set the learning rate to 0.1. We set the temperature to 0.07 and employ a linear layer with a hidden size of 128 as the projection head.

For CoPE, we use the SGD optimizer and set the learning rate to 0.001. We set the temperature to 1. The momentum of the moving average updates for the prototypes is set to 0.99. We use dynamic buffer allocation instead of a fixed class-based memory as given in the original paper.

For DVC, we use the SGD optimizer and set the learning rate to 0.1. The number of candidate samples for retrieval is set to 50. For CIFAR-100 and TinyImageNet, we set loss weights $\lambda_1 = \lambda_2 = 1$, $\lambda_3 = 4$. For CIFAR-10, $\lambda_1 = \lambda_2 = 1$, $\lambda_3 = 2$.

For OCM, we use the Adam optimizer and set the learning rate to 0.001. The weight decay is set as 0.0001. We set the temperature to 0.07 and employ a linear layer with a hidden size of 128 as the projection head. $\lambda$ is set to 0.5. We set $\alpha$ to 1 and $\beta$ to 2 for contrastive loss and set $\alpha$ to 0 and $\beta$ to 2 for supervised contrastive loss as given in the original paper of OCM.

We refer to the links in Table~\ref{tab:baselines} to reproduce the results.


\section{Execution Time}
\input{Figures/AppendixFigs/TrainingTime}
Fig.~\ref{fig:trainingTime} shows the training time of all methods on CIFAR-10. \frameworkName is faster than OCM~\cite{OCM} and GSS~\cite{GSS}. We find that rotation augmentation (Rot) is the main reason for the increase in training time. 
When rotation augmentation is not used, the training time of \frameworkName is significantly reduced and is close to most of the baselines.
Furthermore, \frameworkName achieves the best performance compared to all baselines.
