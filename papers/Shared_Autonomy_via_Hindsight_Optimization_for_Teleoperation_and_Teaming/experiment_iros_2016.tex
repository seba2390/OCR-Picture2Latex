\subsection{Human-Robot Teaming Experiment}
\label{sec:experiment_iros_2016}

\graphicspath{{./}{./images_iros_2016/}}

We apply our shared autonomy framework to a human-robot teaming task of gift-wrapping, where the user and robot must both perform a task on each box to be gift wrapped. Our goal restriction set enforces that they cannot perform a task on the same box at the same time. 

In a user study, we compare three methods: our shared autonomy framework, referred to as \emph{policy}, a standard predict-then-act system, referred to as \emph{plan}, and a non-adaptive system where the robot executes a fixed sequence of motions, referred to as \emph{fixed}.

%We implemented the system on a real-world shared workspace collaboration. First, we show that an implementation of our algorithm successfully operates in real time with naive users. Second, we investigate the effectiveness of the POMDP representation with hindsight optimization by comparing our approach to a system that monitors user goals, but separates the goal prediction and assistance phases.
%
%To achieve this second goal, we measured the fluency and efficiency of the human-robot collaboration across three conditions: (i) when using our \emph{POMDP} model with hindsight optimization; (ii) when using a state of the art \emph{predict-then-act} controller that must first predict goals, then act when goal prediction reaches a certain confidence; and (iii) when using a \emph{control} algorithm that executes a fixed sequence without accounting for human goals.

\subsubsection{Metrics}

\emph{Task fluency} involves seamless coordination of action. One measure for task fluency is the minimum distance between the human and robot end effectors during a trial. This was measured automatically by a Kinect mounted on the robot's head, operating at 30Hz. Our second fluency measure is the proportion of trial time spent in collision. Collisions occur when the distance between the robot's end effector and the human's hand goes below a certain threshold. We determined that 8cm was a reasonable collision threshold based on observations before beginning the study. 

\emph{Task efficiency} relates to the speed with which the task is completed. Objective measures for task efficiency were total task duration for robot and for human, the amount of human idle time during the trial, and the proportion of trial time spent idling. Idling is defined as time a participant spends with their hands still (i.e., not completing the task). For example, idling occurs when the human has to wait for the robot to stamp a box before they can tie the ribbon on it. We only considered idling time while the robot was executing its tasks, so idle behaviors that occurred after the robot was finished stamping the boxes---which could not have been caused by the robot's behavior---were not taken into account.

We also measured subjective \emph{human satisfaction} with each method through a seven-point Likert scale survey evaluating perceived safety (four questions) and sense of collaboration (four questions). The questions were:
\begin{enumerate}
  \item ``HERB was a good partner''
  \item ``I think HERB and I worked well as a team''
  \item ``I'm dissatisfied with how HERB and I worked together''
  \item ``I trust HERB''
  \item ``I felt that HERB kept a safe distance from me''
  \item ``HERB got in my way''
  \item ``HERB moved too fast''
  \item ``I felt uncomfortable working so close to HERB''
\end{enumerate}

\subsubsection{Hypotheses}

We hypothesize that:

\newhypothset

\hypothcounter{Task fluency will be improved with our policy method compared with the plan and fixed methods}{hypoth:teaming_1}

\hypothcounter{Task efficiency will be improved with our policy method compared with the plan and fixed methods}{hypoth:teaming_2}

\hypothcounter{People will subjectively prefer the policy method to the plan or fixed methods}{hypoth:teaming_3}


\subsubsection{Experimental Design}
We developed a gift-wrapping task (\cref{fig:collab_exper_setup}). A row of four boxes was arranged on a table between the human and the robot; each box had a ribbon underneath it. The robot's task was to stamp the top of each box with a marker it held in its hand. The human's task was to tie a bow from the ribbon around each box. By nature of the task, the goals had to be selected serially, though ordering was unspecified. Though participants were not explicitly instructed to avoid the robot, tying the bow while the robot was stamping the box was challenging because the robot's hand interfered, which provided a natural disincentive toward selecting the same goal simultaneously.
%
\begin{figure}
	\centering
	\includegraphics[width=0.9\columnwidth]{iros_exper_setup.jpg}
  \caption{Participants performed a collaborative gift-wrapping task with HERB to evaluate our POMDP based reactive system against a state of the art predict-then-act method, and a non-adaptive fixed sequence of robot goals.}
	\label{fig:collab_exper_setup}
\end{figure}
%


\subsubsection{Implementation}
We implemented the three control methods on HERB~\citet{srinivasa_2012}, a bi-manual mobile manipulator with two Barrett WAM arms. A Kinect was used for skeleton tracking and object detection. Motion planning was performed using CHOMP, except for our policy method in which motion planning works according to \cref{sec:framework}.

The stamping marker was pre-loaded in HERB's hand. A stamping action began at a home position, the robot extended its arm toward a box, stamped the box with the marker, and retracted its arm back to the home position.

To implement the fixed method, the system simply calculated a random ordering of the four boxes, then performed a stamping action for each box. To implement the predict-then-act method, the system ran the human goal prediction algorithm from \cref{sec:framework_prediction} until a certain confidence was reached (50\%), then selected a goal that was not within the restricted set $\goalrestrictionset$ and performed a stamping action on that goal. There was no additional human goal monitoring once the goal action was selected. In contrast, our policy implementation performed as described in \cref{sec:human_robot_teaming}, accounting continually for adapting human goals and seamlessly re-planning when the human's goal changed.


\subsubsection{Procedure}
We conducted a within-subjects study with one independent variable (control method) that had 3 conditions (policy, plan, and fixed). Each performed the gift-wrapping task three times, once with each robot control method. To counteract the effects of novelty and practice, we counterbalanced on the order of conditions.

We recruited 28 participants (14 female, 14 male; mean age 24, SD 6) from the local community. Each participant was compensated \$5 for their time. After providing consent, participants were introduced to the task by a researcher. They then performed the three gift-wrapping trials sequentially. Immediately after each trial, before continuing to the next one, participants completed an eight question Likert-scale survey to evaluate their collaboration with HERB on that trial. At the end of the study, participants provided verbal feedback about the three methods. All trials and feedback were video recorded.


\subsubsection{Results}
\label{sec:results_iros_2016}
Two participants were excluded from all analyses for noncompliance during the study (not following directions). Additionally, for the fluency objective measures, five other participants were excluded due to Kinect tracking errors that affected the automatic calculation of minimum distance and time under collision threshold. Other analyses were based on video data and were not affected by Kinect tracking errors.

\begin{figure}[t]
	\centering
	\begin{subfigure}[t]{0.49\columnwidth}
		\includegraphics{collab_mindist.pdf}
		\caption{Minimum Distance}
		\label{fig:result_mindist}
	\end{subfigure}
	\hfill
	\begin{subfigure}[t]{0.49\columnwidth}
		\includegraphics{collab_percentcollision.pdf}
		\caption{Percent in Collision}
		\label{fig:result_percentcollision}
	\end{subfigure}
	\caption{Distance metrics: no difference between methods for minimum distance during interaction, but the policy method yields significantly $(p<0.05)$ less time in collision between human and robot.}
	\label{fig:distance}
\end{figure}

\begin{figure*}[t]
	\begin{subfigure}[b]{0.32\textwidth}
		\includegraphics{collab_humantime.pdf}
		\caption{Human Trial Duration}
		\label{fig:result_humantime}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.32\textwidth}
		\includegraphics{collab_robottime.pdf}
		\caption{Robot Trial Duration}
		\label{fig:result_robottime}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.32\textwidth}
		\includegraphics{collab_totaltime.pdf}
		\caption{Total Trial Duration}
		\label{fig:result_totaltime}
	\end{subfigure}
	\caption{Duration metrics, with pairs that differed significantly during post-analysis are plotted, where ${*}$ indicates $p<0.05$ and ${*}{*}{*}$ that $p<0.001$. Human trial time was approximately the same across all methods, but robot time increased with the computational requirements of the method. Total time thus also increased with algorithmic complexity.}
	\label{fig:duration}
\end{figure*}

\begin{figure}[t]
	\begin{subfigure}[b]{0.49\columnwidth}
		\includegraphics{collab_idletime.pdf}
		\caption{Human Idle Time}
		\label{fig:result_idletime}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.49\columnwidth}
		\includegraphics{collab_idlepercentage.pdf}
		\caption{Human Idle Percentage}
		\label{fig:result_idlepercentage}
	\end{subfigure}
	\caption{Idle time metrics: policy yielded significantly $(p<0.05)$ less absolute idle time than the fixed method.}
	\label{fig:idle}
\end{figure}

We assess our hypotheses using a significance level of $\alpha=0.05$. For data that violated the assumption of sphericity, we used a Greenhouse-Geisser correction. If a significant main effect was found, a post-hoc analysis was used to identify which conditions were statistically different from each other, with Holm-Bonferroni corrections for multiple comparisons.

% H1 - fluency
To evaluate \cref{hypoth:teaming_1} (fluency), we conducted a repeated measures ANOVA testing the effects of method type (policy, plan, and fixed) on our two measures of human-robot distance: the minimum distance between participant and robot end effectors during each trial, and the proportion of trial time spent with end effector distance below the 8cm collision threshold (\cref{fig:distance}). The minimum distance metric was not significant ($F(2,40) = 1.405, p = 0.257$). However, proportion of trial time spent in collision was significantly affected by method type ($F(2,40) = 3.639, p = 0.035$). Interestingly, the policy method never entered under the collision threshold. Post-hoc pairwise comparisons with a Holm-Bonferroni correction reveal that the policy method yielded significantly ($p = 0.027$) less time in collision than the plan method (policy $M=0.0\%, SD=0$; plan $M=0.44\%, SD = 0.7$). 
% This one from iros paper: %To evaluate \cref{hypoth:teaming_1} (fluency), we conducted a repeated measures ANOVA testing the effects of algorithm type (POMDP, predict-then-act, and control) on our two measures of human-robot distance: the minimum distance between participant and robot end effectors during each trial, and the proportion of trial time spent with end effector distance below the 8cm collision threshold (\cref{fig:distance}). The minimum distance metric was not significant ($F(2,40) = 1.405, p = 0.257$). However, proportion of trial time spent in collision was significantly affected by algorithm type ($F(2,40) = 3.364, p = 0.045$). Interestingly, the POMDP algorithm never entered under the collision threshold. Post-hoc pairwise comparisons with a Bonferroni correction reveal that the POMDP algorithm yielded significantly ($p = 0.033$) less time in collision than the predict-than-act algorithm (POMDP $M=0.0\%, SD=0$; predict-then-act $M=0.44\%, SD = 0.7$). 

Therefore, \cref{hypoth:teaming_1} is partially supported: the policy method actually yielded no collisions during the trials, whereas the plan method yielded collisions during 0.4\% of the trial time on average. This confirms the intuition behind the differences in the two methods: the policy continually monitors human goals, and thus never collides with the human, whereas the plan method commits to an action once a confidence level has been reached, and is not adaptable to changing human goals.

%H2 - efficiency
To evaluate \cref{hypoth:teaming_2} (efficiency), we conducted a similar repeated measures ANOVA for the effect of method type on task durations for robot and human (\cref{fig:duration}), as well as human time spent idling (\cref{fig:idle}).
Human task duration was highly variable and no significant effect for method was found ($F(2,50)=2.259, p = 0.115$). On the other hand, robot task duration was significantly affected by method condition ($F(2,50)=79.653, p<0.001$). Post-hoc pairwise comparisons with a Bonferroni correction reveal that differences between all conditions are significant at the $p < 0.001$ level. Unsurprisingly, robot task completion time was shortest in the fixed condition, in which the robot simply executed its actions without monitoring human goals ($M=46.4s, SD=3.5s$). It was significantly longer with the plan method, which had to wait until prediction reached a confidence threshold to begin its action ($M=56.7s, SD=6.0$). Robot task time was still longer for the policy method, which continually monitored human goals and smoothly replanned motions when required, slowing down the overall trajectory execution ($M=64.6s, SD=5.3$). 
%\spnote{can we say something in relation to the robot time to answer to reviewr7223? A possible aswer is: the robot time increases moving from control strategy to POMDP while human average time descreases. Even if totat time increases, human satisfaction is improved, that is the goal of the paper. } 

Total task duration (the maximum of human and robot time) also showed a statistically significant difference ($F(2,50)=4.887, p = 0.012$). Post-hoc tests with a Bonferroni-Holm correction show that both fixed ($M=58.6s, SD=14.1$) and plan ($M=60.6s, SD=7.1$) performed significantly ($p = 0.026$ and $p=0.032$, respectively) faster than policy ($M=65.9s, SD=6.3$). This is due to the slower execution time of the policy method, which dominates the total execution time. 
% This one from IROS paper: %Total task duration (the maximum of human and robot time) also showed a statistically significant difference ($F(2,50)=4.887, p = 0.012$). Post-hoc tests with Bonferroni correction show that control ($M=58.6s, SD=14.1$) performed significantly ($p < 0.05$) faster than the other two algorithms (predict-then-act: $M=60.6s, SD=7.1$; POMDP: $M=65.9s, SD=6.3$). However, there is no statistically significant difference between predict-then-act and POMDP algorithms. In other words, though algorithm execution time slows down robot actions, people's faster performance with the POMDP somewhat redeems this by eliminating differences between POMDP and predict-then-act algorithms.

Total idle time was also significantly affected by method type ($F(2,50)=3.809, p=0.029$). Post-hoc pairwise comparisons with Bonferroni correction reveal that the policy method yielded significantly ($p = 0.048$) less idle time than the fixed condition (policy $M=0.46s, SD=0.93$, fixed $M=1.62s, SD=2.1$). Idle time percentage (total idle time divided by human trial completion time) was also significant ($F(2,50)=3.258, p=0.047$). Post-hoc pairwise tests with Bonferroni-Holm correction finds no significance between pairs. In other words, the policy method performed significantly better than the fixed method for reducing human idling time, while the plan method did not.

%As with total idle time, post-hoc pairwise tests with Bonferroni correction show that the POMDP yielded shorter idle percentages than the control (POMDP $M=0.9\%, SD=1.9$, control $M=2.8\%, SD=3.6$).

Therefore, \cref{hypoth:teaming_2} is partially supported: although total human task time was not significantly influenced by method condition, the total robot task time and human idle time were all significantly affected by which method was running on the robot. The robot task time was slower using the policy method, but human idling was significantly reduced by the policy method.

To evaluate \cref{hypoth:teaming_3} (subjective responses), we first conducted a Chronbach's alpha test to assure that the eight survey questions were internally consistent. The four questions asked in the negative (e.g., ``I'm dissatisfied with how HERB and I worked together'') were reverse coded so their scales matched the positive questions. The result of the test showed high consistency ($\alpha=0.849$), so we proceeded with our analysis by averaging together the participant ratings across all eight questions. 

During the experiment, participants sometimes saw collisions with the robot. We predict that collisions will be an important covariate on the subjective ratings of the three methods. In order to account for whether a collision occurred on each trial in our within-subjects design, we cannot conduct a simple repeated measures ANOVA. Instead, we conduct a linear mixed model analysis, with average rating as our dependent variable; method (policy, plan, and fixed), collision (present or absent), and their interaction as fixed factors; and method condition as a repeated measure and participant ID as a covariate to account for the fact that participant ratings were not independent across the three conditions. Table \ref{tab:subjective} shows details of the scores for each method broken down by whether a collision occurred.

% \begin{figure}
% 	\centering
% 	\includegraphics[width=0.5\columnwidth]{img/graph_subjective.pdf}
% 	\caption{}
% 	\label{fig:subjectiveresults}
% \end{figure}
\begin{table}
	\centering
	\begin{tabular}{lcccc}
		\toprule
		& \multicolumn{2}{c}{No Collision} & \multicolumn{2}{c}{Collision}\\
		\cmidrule(r){2-3} \cmidrule(l){4-5}
		& \emph{mean (SD)} & \emph{N} & \emph{mean (SD)} & \emph{N} \\ 
		\cmidrule(r){2-2}\cmidrule(lr){3-3}\cmidrule(lr){4-4} \cmidrule(l){5-5}
		Fixed 		&	5.625 (1.28)	& 14	& 4.448	(1.23)	& 12	\\
		Plan      & 	5.389 (1.05)	& 18	& 4.875	(1.28)	& 8		\\
		Policy 		&	5.308 (0.94)	& 26	& ---			& 0		\\
		\bottomrule
	\end{tabular}
	\caption{Subjective ratings for each method condition, separated by whether a collision occurred during that trial.}
	\label{tab:subjective}
\end{table}

We found that collision had a significant effect on ratings ($F(1,47.933)=6.055, p=0.018$), but method did not ($F(1,47.933)=0.312, p = 0.733$). No interaction was found. In other words, ratings were significantly affected by whether or not a participant saw a collision, but not by which method they saw independent of that collision. Therefore, \cref{hypoth:teaming_3} is not directly supported. However, our analysis shows that collisions lead to poor ratings, and our results above show that the policy method yields fewer collisions. We believe a more efficient implementation of our policy method to enable faster robot task completion, while maintaining fewer collisions, may result in users preferring the policy method.

%We conducted a repeated measures ANOVA to evaluate the effect of algorithm condition on subjective ratings of the human-robot collaboration. However, no significant effect was found ($F(2,50)=0.527, p=0.594$). Thus, H3 is not supported. 


% \begin{figure*}[h!]
% 	\centering
% 	\includegraphics[width=1.75\columnwidth]{img/time.png}
% 	\caption{Herb execution time, human execution time, and human idle time during task executions}
% 	\label{fig:dist2}
% \end{figure*}


% \begin{figure*}[h!]
% 	\centering
% 	\includegraphics[width=1.8\columnwidth]{img/dist.png}
% 	\caption{Minimum distance between the human and the robot and per cent time under the threshold}
% 	\label{fig:dist}
% \end{figure*}


%Holm-Bonferroni corrected values
%\begin{table*}
%  \newcommand{\algopolicy}{Policy}
%  \newcommand{\algoplan}{Plan}
%  \newcommand{\algocontrol}{Fixed}
%  \newcommand{\sigresult}[1]{\bm{#1}}
%  \newcommand{\spssnoval}{\sigresult{<\!0.001}}
%  \newcommand{\nonsigresult}{\text{NS}}
%  \centering
%  %\addtolength\tabcolsep{-2.2pt}
%  \begin{tabular}{|c|c|c|c|}
%    \hline
%    %Algorithm & Direct-Blend & Direct-Policy & Direct-Autonomy & Blend-Policy & Blend-Autonomy & Policy-Autonomy
%    Metric & \algopolicy-\algoplan & \algopolicy-\algocontrol & \algoplan-\algocontrol \\
%    \hline
%    Min Dist & $\nonsigresult$ & $\nonsigresult$ & $\nonsigresult$ \\
%    Percent Collision & $\sigresult{.027}$ & $\nonsigresult$ & $\nonsigresult$ \\
%    Human Trial Time & $\nonsigresult$ & $\nonsigresult$ & $\nonsigresult$ \\
%    Robot Trial Time & $\spssnoval$ & $\spssnoval$ & $\spssnoval$ \\
%    Total Trial Time & $\sigresult{0.032}$ & $\sigresult{0.026}$ & $\nonsigresult$ \\
%    Human Idle Time & $\nonsigresult$ & $\sigresult{0.048}$ & $\nonsigresult$ \\
%    Human Idle Percentage& $\nonsigresult$ & $\nonsigresult$ & $\nonsigresult$ \\
%    \hline
%    \end{tabular} 
%    \caption{Post-Hoc p-value for every pair of algorithms for each hypothesis. All results are from a repeated measures ANOVA, with reported values using a Holm-Bonferroni correction.}
%    \label{tab:p_val_results_collab}
%\end{table*}

