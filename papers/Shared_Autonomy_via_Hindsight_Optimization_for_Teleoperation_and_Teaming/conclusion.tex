\section{Discussion and Conclusion}
\label{sec:conclusion}

In this work, we present a method for shared autonomy that does not rely on predicting a single user goal, but assists for a distribution over goals. Our motivation was a lack of assistance when using predict-then-act methods - in our own experiment (\cref{sec:experiment_hri_2016}), resulting in no assistance for $69\%$ of execution time. To assist for any distribution over goals, we formulate shared autonomy as a POMDP with uncertainty over user goals. To provide assistance in real-time over continuous state and action spaces, we used hindsight optimization~\citep{littman_1995,chong_2000,yoon_2008} to approximate solutions. We tested our method on two shared-control teleoperation scenarios, and one human-robot teaming scenario. Compared to predict-then-act methods, our method achieves goals faster, requires less user input, decreases user idling time, and results in fewer user-robot collisions.

In our shared control teleoperation experiments, we found user preference differed for each task, even though our method outperformed a predict-then-act method across all objective measures for both tasks. This is not entirely surprising, as prior works have also been mixed on whether users prefer more control authority or better task completion~\cite{you_2011, kim_2012, dragan_2013_assistive}. In our studies, user's tended to prefer a predict-then-act approach for the simpler grasping scenario, though not significantly so. For the more complex eating task, users significantly preferred our shared autonomy method to a predict-then-act method. In fact, our method and blending were the only pair of algorithms that had a significant difference across all objective measures and the subjective measuring of like and rank (\cref{tab:eating_p_val_results}).

However, we believe this difference of rating cannot simply be explained by task difficulty and timing, as the experiments had other important differences. %The control interface differed, where users had one control mode with a 3-DOF input (X,Y,Z) for grasping, and users used a standard assistive arms input with modal control and 2-DOF (see \cref{fig:control_modes}) for feeding.
The grasping task required minimal rotation, and relied entirely on assistance to achieve it. Using blending, the user could focus on teleoperating the arm near the object, at which point the predictor would confidently predict the user goal, and assistance would orient the hand. For the feeding task, however, orienting the fork was necessary before moving the arm, at which point the predictor could confidently predict the user goal. For this task, predict-then-act methods usually did not reach their confidence threshold until users completed the most difficult portion of the task - cycling control modes to rotate and orient the fork. These mode switches have been identified as a significant contributor to operator difficulty and time consumption~\citep{herlant_2016}. This inability to confidently predict a goal until the fork was oriented caused predict-then-act methods to provide no assistance for the first $29.4$ seconds on average - which is greater then the total average time of our method ($18.5s$). We believe users were more willing to give up control authority if they did not need to do multiple mode switches and orient the fork, which subjectively felt much more tedious then moving the position.

%Additionally, the precision required in the feeding task was significantly greater than the grasping task. 

%Like our grasping experiment, the user's task was very simple during our human teaming experiment. The total task duration was dominated by the robot task time, making the decrease in human trial with our method provide no gain on total trial time. However, we note that users who collided with a robot, which never occurred when working with our method but occurred often when working with both the fixed and predict-then-act method, did rate the methods differently.

In all experiments, we used a simple distance-based cost function, for which we could compute value functions in closed form. This enabled us to compute prediction and assistance 50 times a second, making the system feel responsive and reactive. However, this simple cost function could only provide simple assistance, with the objective of minimizing the time to reach a goal. Our new insights into possible differences of user costs for rotation and mode switches as compared to translation can be incorporated into the cost function, with the goal of minimizing user effort.

For human-robot teaming, the total task time was dominated by the robot, with the user generally finishing before the robot. In situations like this, augmenting the cost function to be more aggressive with robot motion, even at the cost of responsiveness to the user, may be beneficial. Additionally, incorporating more optimal robot policies may enable faster robot motions within the current framework.

Finally, though we believe these results show great promise for shared control teleoperation and teaming, we note users varied greatly in their preferences and desires. Prior works in shared control teleoperation have been mixed on whether users prefer control authority or more assistance~\cite{you_2011, kim_2012, dragan_2013_assistive}. Our own experiments were also mixed, depending on the task. Even within a task, users had high variance, with users fairly split for grasping (\cref{fig:survey_means}), and a high variance for user responses for full autonomy for eating (\cref{fig:survey_questions}). For teaming, users were similarly mixed in their rating for an algorithm depending on whether or not they collided with the robot (\cref{tab:subjective}). This variance suggests a need for the algorithm to adapt to each individual user, learning their particular preferences. New work by \citet{nikolaidis_2017_shared} captures these ideas through the user's \emph{adaptability}, but we believe even richer user models and their incorporation into the system action selection would make shared autonomy systems better collaborators.


 


%users spent an average of $29.4$ seconds ($66.4\%$ of the total time) before a predict-then-act method was confident enough to provide any assist.
%In our feeding experiment, our POMDP based method completed the task ($18.5$ seconds on average) before a predict-then-act method even started assisting ($29.4$ seconds on average).
%predict-then-act $48.6$ seconds to complete the task.~\cite{you_2011, kim_2012, dragan_2013_assistive}
