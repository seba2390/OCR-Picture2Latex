\section{Shared Control Teleoperation}
\label{sec:shared_teleop}


\graphicspath{{./}{./images_rss_2015/}}

\begin{figure*}[t]
\centering
% \begin{subfigure}{0.24\textwidth}
%   \centering 
%   \includegraphics[width=1.0\textwidth, trim=250 150 200 190, clip=true]{valfunc_left_noback/rss_1.png}
%  \includegraphics[width=0.7\textwidth, trim=150 515 640 150, clip=true]{valuefunc_text/valuefunc_slides_1.pdf}
%   \caption{ }
% \label{fig:valfunc_1}
% \end{subfigure}
  \begin{subfigure}{0.32\textwidth}
    \centering 
    \begin{tikzpicture}[every node/.style={anchor=south west,inner sep=0pt}, x=1mm, y=1mm,]    
      \node {\includegraphics[width=1.0\textwidth, trim=250 150 200 190, clip=true]{rss_2_blacktext.png}} ;
      \node [opacity=0.6]{\includegraphics[width=1.0\textwidth, trim=250 150 200 190, clip=true]{rss_1_noback.png} };
    \end{tikzpicture}
    \includegraphics[width=0.7\textwidth, trim=150 515 640 150, clip=true]{valuefunc_slides_2.pdf}
    \caption{ }
  \label{fig:valfunc_2}
 \end{subfigure}
 \begin{subfigure}{0.32\textwidth}
   \centering 
   \begin{tikzpicture}[every node/.style={anchor=south west,inner sep=0pt}, x=1mm, y=1mm,]    
     \node {\includegraphics[width=1.0\textwidth, trim=250 150 200 190, clip=true]{rss_3_2_blacktext.png}} ;
     \node [opacity=0.6]{\includegraphics[width=1.0\textwidth, trim=250 150 200 190, clip=true]{rss_2_noback.png} };
    \end{tikzpicture}
    \includegraphics[width=0.7\textwidth, trim=150 515 640 150, clip=true]{valuefunc_slides_3.pdf}
 \caption{ }
 \label{fig:valfunc_3}
 \end{subfigure}
 \begin{subfigure}{0.32\textwidth}
   \centering 
   \begin{tikzpicture}[every node/.style={anchor=south west,inner sep=0pt}, x=1mm, y=1mm,]    
     \node {\includegraphics[width=1.0\textwidth, trim=250 150 200 190, clip=true]{rss_4_2_blacktext.png}} ;
     \node [opacity=0.6]{\includegraphics[width=1.0\textwidth, trim=250 150 200 190, clip=true]{rss_3_2_noback.png} };
 \end{tikzpicture}
  \includegraphics[width=0.7\textwidth, trim=150 515 640 150, clip=true]{valuefunc_slides_4.pdf}
 \caption{ }
 \label{fig:valfunc_4}
 \end{subfigure}
  \label{fig:valfunc}
  \caption{Estimated goal probabilities and value function for object grasping. Top row: the probability of each goal object and a 2-dimensional slice of the estimated value function. The transparent end-effector corresponds to the initial state, and the opaque end-effector to the next state. Bottom row: the user input and robot control vectors which caused this motion. (\subref{fig:valfunc_2}) Without user input, the robot automatically goes to the position with lowest value, while estimated probabilities and value function are unchanged. (\subref{fig:valfunc_3}) As the user inputs ``forward'', the end-effector moves forward, the probability of goals in that direction increase, and the estimated value function shifts in that direction. (\subref{fig:valfunc_4}) As the user inputs ``left'', the goal probabilities and value function shift in that direction. Note that as the probability of one object dominates the others, the system automatically rotates the end-effector for grasping that object.}
\end{figure*}


We apply our shared autonomy framework to two shared control teleoperation tasks: a simpler task of object grasping (\cref{sec:experiment_rss_2015}) and a more complicated task of feeding (\cref{sec:experiment_hri_2016}). Formally, the state $\stateenv$ corresponds to the end-effector pose of the robot, each goal $\goal$ an object in the world, and each target $\target$ a pose for achieving that goal (e.g. pre-grasp pose). The transition function $\transitionallargs$ deterministically transitions the state by applying both $\actionuser$ and $\actionrobot$ as end-effector velocities. We map user joystick inputs to $\actionuser$ as if the user were controlling the robot through direct teleoperation.

For both tasks, we hand-specify a simple user cost function, $\costtarguser$, from which everything is derived. Let $d$ be the distance between the robot state $\stateenv' = \transitionuser(\stateenv, \actionuser)$ and target $\target$:
 \begin{align*}
   \costtarguser(\stateenv, \actionuser) &= \left\{ \begin{array}{cc} \alpha & d > \delta \\ \frac{\alpha}{\delta} d & d\leq \delta \end{array} \right.
 \end{align*}

That is, a linear cost near a target $(d \leq \delta)$, and a constant cost otherwise. This is based on our observation that users make fast, constant progress towards their goal when far away, and slow down for alignment when near their goal. This is by no means the best cost function, but it does provide a baseline for performance. We might expect, for example, that incorporating collision avoidance into our cost function may enable better performance~\citep{you_2011}. We use this cost function, as it enables closed-form value function computation, enabling inference and execution at 50Hz.

For prediction, when the distance is far away from any target $(d > \delta)$, our algorithm shifts probability towards goals relative to how much progress the user action makes towards the target. If the user stays close to a particular target $(d \leq \delta)$, probability mass automatically shifts to that goal, as the cost for that goal is less than all others.

We set $\costtargrobot(\stateenv, \actionrobot, \actionuser) = \costtarguser(\stateenv, \actionrobot)$, causing the robot to optimize for the user cost function directly\footnote{In our prior work~\citep{javdani_2015_rss}, we used $\costtargrobot(\stateenv, \actionrobot, \actionuser) = \costtarguser(\stateenv, \actionrobot) + (\actionrobot - \actionuser)^2$ in a different framework where only the robot action transitions the state. Both formulations are identical after linearization. Let $\actionrobot^*$ be the optimal optimal robot action in this framework. The additional term $(\actionrobot - \actionuser)^2$ leads to executing the action $\actionuser + \actionrobot^*$, equivalent to first executing the user action $\actionuser$, then $\actionrobot^*$, as in this framework.}, and behave similar to how we observe users behaved. When far away from goals $(d > \delta)$, it makes progress towards all goals in proportion to their probability of being the user's goal. When near a target $(d \leq \delta)$ that has high probability, our system reduces assistance as it approaches the final target pose, letting users adjust the final pose if they wish.

%\subsubsection*{Hindsight Optimization for Shared Control Teleoperation}

We believe hindsight optimization is a suitable POMDP approximation for shared control teleoperation. A key requirement for shared control teleoperation is efficient computation, in order to make the system feel responsive. With hindsight optimization, we can provide assistance at 50Hz, even with continuous state and action spaces.

The primary drawback of hindsight optimization is the lack of explicit information gathering~\citep{littman_1995}: it assumes all information is revealed at the next timestep, negating any benefit to information gathering. As we assume the user provides inputs at all times, we gain information automatically when it matters. When the optimal action is the same for multiple goals, we take that action. When the optimal action differs, our model gains information proportional to how suboptimal the user action is for each goal, shifting probability mass towards the user goal, and providing more assistance to that goal.

For shared control teleoperation, explicit information gathering would move the user to a location where their actions between goals were maximally different. Prior works suggest that treating users as an oracle is frustrating~\citep{guillory_2011_noise, amershi_2014}, and this method naturally avoids it.


% Evaluations segue
We evaluated this system in two experiments, comparing our POMDP based method, referred to as \emph{policy}, to a conventional predict-then-act approach based on~\citet{dragan_2013_assistive}, referred to as \emph{blend} (\cref{fig:blend_diagram}). In our feeding experiment, we additionally compare to direct teleoperation, referred to as \emph{direct}, and full autonomy, referred to as \emph{autonomy}.

The \emph{blend} baseline of \citet{dragan_2013_assistive} requires estimating the predictor's confidence of the most probable goals, which controls how user action and autonomous assistance are arbitrated (\cref{fig:blend_diagram}). We use the distance-based measure used in the experiments of~\citet{dragan_2013_assistive}, $\text{conf} = \max\left(0, 1-\frac{d}{D}\right)$, where $d$ is the distance to the nearest target, and $D$ is some threshold past which confidence is zero.

\input{experiment_rss_2015}
\input{experiment_hri_2016}
%\input{shared_teleop_discussion}
