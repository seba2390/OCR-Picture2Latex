\subsection{Hindsight Optimization}
\label{sec:framework_hindsight}

Computing the optimal solution for a POMDP with continuous states and actions is generally intractable. Instead, we approximate this quantity through \emph{Hindsight Optimization}~\citep{chong_2000,yoon_2008}, or QMDP~\citep{littman_1995}. This approximation estimates the value function by switching the order of the min and expectation in \cref{eq:v_belief}:
\begin{align*}
  \vhs(\belief) &= \expctover{\belief}{\min_{\policyrobot} \vrobot^{\policyrobot}(\state)}\\
  &= \expctover{\goal}{\vgoal(\stateenv)}\\
  \qhs(\beliefactions) &= \expctover{\belief}{\costrobot(\stateactions) + \expctover{\state'}{\vhs(\state')}}\\
  &= \expctover{\goal}{\qgoal(\stateenvactions)}
\end{align*}

Where we explicitly take the expectation over $\goal \in \Goal$, as we assume that is the only uncertain part of the state.

Conceptually, this approximation corresponds to assuming that all uncertainty will be resolved at the next timestep, and computing the optimal cost-to-go. As this is the best case scenario for our uncertainty, this is a lower bound of the cost-to-go, $\vhs(\belief) \leq \vopt(\belief)$. Hindsight optimization has demonstrated effectiveness in other domains~\citep{yoon_2007, yoon_2008}. However, as it assumes uncertainty will be resolved, it never explicitly gathers information~\citep{littman_1995}, and thus performs poorly when this is necessary.

We believe this method is suitable for shared autonomy for many reasons. Conceptually, we assume the user provides inputs at all times, and therefore we gain information without explicit information gathering. Works in other domains with similar properties have shown that this approximation performs comparably to methods that consider explicit information gathering~\citep{koval_2014}. Computationally, computing $\qhs$ can be done with continuous state and action spaces, enabling fast reaction to user inputs. 
%say that this is a lower bound on cost-to-go?

%Let $\qpomdp(\belief, \actionrobot, \actionuser)$ be the action-value function of the POMDP, estimating the cost-to-go of taking action $\actionrobot$ when in belief $\belief$ with user input $\actionuser$, and acting optimally thereafter. In our setting, uncertainty is only over goals, $\belief(\staterobgoal) = \belief(\goal) = p(\goal | \trajtot)$.

%Let $\qgoal(\staterobot, \actionrobot, \actionuser)$ correspond to the action-value for goal $\goal$, estimating the cost-to-go of taking action $\actionrobot$ when in state $\staterobot$ with user input $\actionuser$, and acting optimally for goal $\goal$ thereafter. The QMDP approximation is~\citep{littman_1995}:
%\begin{align*}
%  \qpomdp(\belief, \actionrobot, \actionuser) &= \sum_{\goal} \belief(\goal) \qgoal(\staterobot, \actionrobot, \actionuser)
%\end{align*}

Computing $\qgoal$ for shared autonomy requires utilizing the user policy $\policyusergoal$, which can make computation difficult. This can be alleviated with the following approximations:
\subsubsection*{Stochastic user with robot}
Estimate $\actionuser$ using $\policyusergoal$ at each time step, e.g. by sampling, and utilize the full cost function $\costrobotgoal(\stateenvactions)$ and transition function $\transitionallargs$ to compute $\qgoal$. This would be the standard QMDP approach for our POMDP.

\subsubsection*{Deterministic user with robot}
Estimate $\actionuser$ as the most likely $\actionuser$ from $\policyusergoal$ at each time step, and utilize the full cost function $\costrobotgoal(\stateenvactions)$ and transition function $\transitionallargs$ to compute $\qgoal$. This uses our policy predictor, as above, but does so deterministically, and is thus more computationally efficient.

\subsubsection*{Robot takes over}
Assume the user will stop supplying inputs, and the robot will complete the task. This enables us to use the cost function $\costrobotgoal(\stateenv, 0, \actionrobot)$ and transition function $\transition(\stateenv' \given \stateenv, 0, \actionrobot)$ to compute $\qgoal$. For many cost functions, we can analytically compute this value, e.g. cost of always moving towards the goal at some velocity. An additional benefit of this method is that it makes no assumptions about the user policy $\policyusergoal$, making it more robust to modelling errors. We use this method in our experiments.

Finally, as we often cannot calculate $\argmax_{\actionrobot} \qhs(\beliefactions)$ directly, we use a first-order approximation, which leads to us to following the gradient of $\qhs(\beliefactions)$.
%In cases were an action exists to assist for all goals, this approximation will take that action. When there aren't any such actions, the output will look similar to a blending between the user control and our assistance strategy, solving for the parameters of blending based on the cost functions. This sort of blending has been shown to be effective in the past~\citep{dragan_2013_assistive}. See \figref{fig:teledata}.


%add something about 1st order approximation for continuous systems?

%Maybe more specifics for our system? 
%-First order approx for qmdp
%-we optimize directly for user's value function
%---actually, we aren't fully solving the POMDP assuming user is optimal
