\subsection{Shared Control Teleoperation}
\label{sec:related_shared_teleop}


\graphicspath{{./}{./images/}{./images_hri_2016/}}

Shared control teleoperation has been used to assist disabled users using robotic arms~\citep{kim_2006_bci, kim_2012, mcmullen_2014, katyal_2014, schroeer_2015, muelling_2015} or wheelchairs~\citep{argall_2016,carlson_2012}, operate robots remotely~\citep{shen_2004, you_2011, leeper_2012}, decrease operator fatigue in surgical settings~\citep{park_2001, marayong_2003, kragic_2005, aarno_2005_virtualfixtures, li_2007}, and many other applications. As such, there are a great many methods catering to the specific needs of each domain.

One common paradigm launches a fully autonomous takeover when some trigger is activated, such as a user command~\citep{shen_2004, bien_2004, simpson_2005, kim_2012}, or when a goal predictor exceeds some confidence threshold~\citep{fagg_2004, kofman_2005, mcmullen_2014, katyal_2014}. Others have utilized similar triggers to initiate a subtask in a sequence~\citep{schroeer_2015, jain_argall_2015}. While these systems are effective at accomplishing the task, studies have shown that users often prefer having more control~\citep{kim_2012}.
%In BCI settings, \citet{mcmullen_2014} and \citet{katyal_2014} combine BCI signals with eye tracking and environmental sensing to determine an intended object, and take over when confident.

Another line of work utilizes high level user commands, and relies on autonomy to generate robot motions. Systems have been developed to enable users to specify an end-effector path in 2D, which the robot follows with full configuration space plans~\citep{you_2011, hauser_2013}. Point-and-click interfaces have been used for object grasping with varying levels of autonomy~\citep{leeper_2012}. Eye gaze has been utilized to select a target object and grasp position~\citep{bien_2004}.

Another paradigm augments user inputs minimally to maintain some desired property, e.g. collision avoidance, without necessarily knowing exactly what goal the user wants to achieve. Sensing and complaint controllers have been used increase safety during teleoperation~\citep{kim_2006_bci, vogel_2014}. \emph{Potential field} methods have been employed to push users away from obstacles~\citep{crandall_2002} and towards goals~\citep{aigner_1997}. For assistive robotics using modal control, where users control subsets of the degrees-of-freedom of the robot in discrete modes (\cref{fig:control_modes}), \citet{herlant_2016} demonstrate a method for automatic time-optimal mode switching.

\begin{figure}
  \begin{subfigure}[b]{.32\linewidth}
    \includegraphics[width=\linewidth]{mico_modes1}
    \caption{Mode 1}
  \end{subfigure}
  \begin{subfigure}[b]{.32\linewidth}
    \includegraphics[width=\linewidth, trim=0 4 0 0, clip=true]{mico_modes2}
    \caption{Mode 2}
  \end{subfigure}
  \begin{subfigure}[b]{.32\linewidth}
    \includegraphics[width=\linewidth, trim=0 6 0 4, clip=true]{mico_modes3}
    \caption{Mode 3}
  \end{subfigure}
  \caption{Modal control used in our feeding experiment on the Kinova MICO, with three control modes and a 2 degree-of-freedom input device. Fewer input DOFs means more modes are required to control the robot.}
  \label{fig:control_modes}
\end{figure}

Similarly, methods have been developed to augment user inputs to follow some constraint. \emph{Virtual fixtures}, commonly used in surgical robotics settings, are employed to project user commands onto path constraints (e.g. straight lines only)~\citep{park_2001,li_2003, marayong_2003, kragic_2005, aarno_2005_virtualfixtures, li_2007}. \citet{mehr_2016} learn constraints online during execution, and apply constraints softly by combining constraint satisfaction with user commands. While these methods benefit from not needing to predict the user's goal, they generally rely on a high degree-of-freedom input, making their use limited for assistive robotics, where disabled users can operate few DOF at a time and thus rely on modal control~\citep{herlant_2016}.

\emph{Blending} methods~\citep{dragan_2013_assistive} attempt to bridge the gap between highly assistive methods with little user control, and minimal assistance with higher user burden. User actions and full autonomy are treated as two independent sources, which are combined by some \emph{arbitration} function that determines the relative contribution of each (\cref{fig:blend_diagram}). \citet{dragan_2013_assistive} show that many methods of shared control teleoperation (e.g. autonomous takeover, potential field methods, virtual fixtures) can be generalized as blending with a particular arbitration function. 

Blending is one of the most used shared control teleopration paradigms due to computational efficiency, simplicity, and empirical effectiveness~\citep{li_2011, carlson_2012, dragan_2013_assistive, muelling_2015, gopinath_2016}. However, blending has two key drawbacks.
First, as two independent decisions are being combined without evaluating the action that will be executed, catastrophic failure can result even when each independent decision would succeed~\citep{trautman_2015}. Second, these systems rely on a \emph{predict-then-act} framework, predicting the single goal the user is trying to achieve before providing any assistance. Often, assistance will not be provided for large portions of execution while the system has low confidence in its prediction, as we found in our feeding experiment (\cref{sec:experiment_hri_2016}). %Despite these drawbacks, blending is one of the most used shared autonomy paradigms due to computational efficiency, simplicity, and empirical effectiveness~\citep{li_2011, carlson_2012, dragan_2013_assistive, muelling_2015, gopinath_2016}.

Recently, \citet{hauser_2013} presented a system which provides assistance for a distribution over goals. Like our method, this policy-based method minimizes an expected cost-to-go while receiving user inputs (\cref{fig:policy_diagram}). The system iteratively plans trajectories given the current user goal distribution, executes the plan for some time, and updates the distribution given user inputs. In order to efficiently compute the trajectory, it is assumed that the cost function corresponds to squared distance, resulting in the calculation decomposing over goals. Our model generalizes these notions, enabling the use of any cost function for which a value function can be computed.

In this work, we assume the user does not change their goal or actions based on autonomous assistance, putting the burden of goal inference entirely on the system. \citet{nikolaidis_2017_shared} present a game-theoretic approach to shared control teleoperation, where the user adapts to the autonomous system. Each user has an \emph{adaptability}, modelling how likely the user is to change goals based on autonomous assistance. They use a POMDP to learn this adaptability during execution. While more general, this model is computationally intractable for continuous state and actions.

%-mutual adaptation to shared autonomy. One interesting finding - fixed policy without taking into account user commands to select item had lowest trust, though best performance in terms of reward


\begin{figure}
 \begin{tikzpicture} 

 %icons
 \node[inner sep=-2pt] (user) at (0,4)
     {\includegraphics[width=.11\textwidth]{user_icon.pdf}};
 \node[below] at (user.south) (user_label) {$\policyuser$};
 \node[inner sep=-3pt, below=0.75cm of user] (robot)
     {\includegraphics[width=.11\textwidth]{robot_icon.pdf}};
 \node[below] at (robot.south) (robot_label) {$\policyrobot$};

 % plot
 \def \plotoriginx {2.0}
 \def \plotoriginy {2.1}
 \def \plotlength {1.5}
 \coordinate (plot_origin) at (\plotoriginx, \plotoriginy);
 \coordinate (plot_arb_1) at (\plotoriginx + \plotlength/2, \plotoriginy+\plotlength*2/3);
 \coordinate (plot_arb_2) at (\plotoriginx + \plotlength, \plotoriginy+\plotlength*2/3);
 \draw (plot_origin) -- node[below] {\small Confidence} ++ (\plotlength,0);
 \draw (plot_origin) -- node[above, rotate=90] (plot_vert) {\small $\arbitration$} ++ (0,\plotlength);
 \draw[color=blue, thick] (plot_origin) -- (plot_arb_1);
 \draw[color=blue, thick] (plot_arb_1) -- (plot_arb_2);

 %\draw[opacity=0] (plot_origin) rectangle ++ (2, 2)  node[pos=0.5, above, opacity=1] {Arbitration};
 \node [draw,rectangle,minimum width=\plotlength cm,minimum height=\plotlength cm,anchor=south west, opacity=0., label={[label distance=0.1cm]above: Arbitration}] (plot_frame) at (plot_origin) {};

 \node[inner sep=0pt,right=2.3cm of plot_frame] (robot_arm)
     {\includegraphics[trim=0 85 0 0, width=.10\textwidth, clip=true]{mico_arm_editted.png}};

 \draw[->, thick] (user) -- node[above] {$\actionuser$} ([xshift=-0.2cm, yshift=-0.2cm]plot_frame.north west);
 \draw[->, thick] (robot) -- node[below] {$\actionrobot$}  ([xshift=-0.2cm, yshift=0.2cm]plot_frame.south west);
 \draw[->, thick] ([xshift=0.1cm]plot_frame.east) -- node[above] {\small $\arbitration a + (1-\arbitration)u$}  (robot_arm);


 \end{tikzpicture}
 \caption{Blend method for shared control teleoperation. The user and robot are both modelled as separate policies $\policyuser$ and $\policyrobot$, each independently providing actions $\actionuser$ and $\actionrobot$ for a single goal. These actions are combined through a specified arbitration function, which generally uses some confidence measure to augment the magnitude of assistance. This combined action is executed on the robot.}
 \label{fig:blend_diagram}
\end{figure}

\begin{figure}
  \begin{tikzpicture} 

 \def \distbetween {1.4}

  %icons
  \node[inner sep=-2pt] (user) at (0,0)
      {\includegraphics[width=.11\textwidth]{user_icon.pdf}};
  \node[below] at (user.south) (user_label) {$\policyuser$};
  \node[inner sep=-3pt, right=\distbetween cm of user] (robot)
      {\includegraphics[width=.11\textwidth]{robot_icon.pdf}};
  \node[below] at (robot.south) (robot_label) {$\policyrobot$};

  \node[below] at (user_label.south) (user_cost_label) {$\costuser(\staterobgoal, \actionuser)$};
  \node[below] at (robot_label.south) (robot_cost_label) {$\costrobot(\staterobgoal, \actionuser, \actionrobot)$};

  \def \shiftarmim {0.2}
  \node[inner sep=0pt,right=\distbetween cm of robot, yshift=-\shiftarmim cm] (robot_arm)
      {\includegraphics[trim=0 85 0 00, width=.10\textwidth, clip=true]{mico_arm_editted.png}};

  \draw[->, thick] (user)  -- node[above] {$\actionuser$} (robot);
  \draw[->, thick] (robot)  -- node[above] {$\actionuser, \actionrobot$} ([yshift=\shiftarmim cm]robot_arm.west);

  \end{tikzpicture}
 \caption{Policy method for shared control teleoperation. The user is modelled as a policy $\policyuser$, which selects user input $\actionuser$ to minimizes the expected sum of user costs $\costuser(\stateenv, \actionuser)$. The user input $\actionuser$ is provided to the system policy $\policyrobot$, which then selects action $\actionrobot$ to minimize its expected sum of costs cost $\costrobot(\state, \actionuser, \actionrobot)$. Both actions are passed to the robot for execution. Unlike the blend method, the user and robot actions are not treated separately, which can lead to catastrophic failure~\citep{trautman_2015}. Instead, the robot action $\actionrobot$ is optimized given the user action $\actionuser$.}
  \label{fig:policy_diagram}
\end{figure}
