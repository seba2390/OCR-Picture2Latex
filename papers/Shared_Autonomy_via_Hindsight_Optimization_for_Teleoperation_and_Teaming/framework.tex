\section{Framework}
\label{sec:framework}


\graphicspath{{./}{./images_rss_2015/}{./images_hri_2016/}}


%Under this:
% Shared autonomy assumes $\transition(\stateenv' \given \stateenv, \actionuser, \actionrobot) = \transition(\stateenv' \given \stateenv, 0, \actionrobot)$ - that is, the user does not directly affect the state.
% collab doesn't really care about the user state in cost 


We present our framework for minimizing a cost function for shared autonomy with an unknown user goal. We assume the user's goal is fixed, and they take actions to achieve that goal without considering autonomous assistance. These actions are used to predict the user's goal based on how optimal the action is for each goal (\cref{sec:framework_prediction}). Our system uses this distribution to minimize the expected cost-to-go  (\cref{sec:framework_unknown_goal}). As solving for the optimal action is infeasible, we use hindsight optimization to approximate a solution (\cref{sec:framework_hindsight}). For reference, see \cref{table:variable_definitions} in \cref{sec:variable_definitions} for variable definitions.

\subsection{Cost minimization with a known goal}
\label{sec:framework_known_goal}

We first formulate the problem for a known user goal, which we will use in our solution with an unknown goal. We model this problem as a Markov Decision Process (MDP). 

Formally, let $\stateenv \in \Stateenv$ be the environment state (e.g. human and robot pose). Let $\actionuser \in \Actionuser$ be the user actions, and $\actionrobot \in \Actionrobot$ the robot actions. Both agents can affect the environment state - if the user takes action $\actionuser$ and the robot takes action $\actionrobot$ while in state $\stateenv$, the environment stochastically transitions to a new state $\stateenv'$ through $\transitionallargs$. 

We assume the user has an intended goal $\goal \in \Goal$ which does not change during execution. We augment the environment state with this goal, defined by $\state = \left(\stateenv, \goal\right) \in \Stateenv \times \Goal$. We overload our transition function to model the transition in environment state without changing the goal, $\transition( (\stateenv', g) \given (\stateenv, g), \actionuser, \actionrobot) = \transitionallargs$.

%In our scenario, the user wants to move the robot to one goal in a discrete set of goals $\goal \in \Goal$.
We assume access to a user policy for each goal $\policyuser(\actionuser \given \state) = \policyusergoal(\actionuser \given \stateenv) = p(\actionuser \given \stateenv, \goal)$. We model this policy using the maximum entropy inverse optimal control (MaxEnt IOC) framework of~\citet{ziebart_2008}, where the policy corresponds to stochastically optimizing a cost function $\costuser(\state, \actionuser) = \costusergoal(\stateenv, \actionuser)$. We assume the user selects actions based only on $\state$, the current environment state and their intended goal, and does not model any actions that the robot might take. Details are in \cref{sec:framework_prediction}.

The robot selects actions to minimize a cost function dependent on the user goal and action $\costrobot(\state, \actionuser, \actionrobot) = \costrobotgoal(\stateenv, \actionuser, \actionrobot)$. At each time step, we assume the user first selects an action, which the robot observes before selecting $\actionrobot$. The robot selects actions based on the state and user inputs through a policy $\policyrobot(\actionrobot \given \state, \actionuser) = p(\actionrobot \given \state, \actionuser)$. We define the value function for a robot policy $\vrobot^{\policyrobot}$ as the expected cost-to-go from a particular state, assuming some user policy $\policyuser$:
\begin{align*}
  \vrobot^{\policyrobot}(\state) &= \expctarg{\sumtime \costrobot(\state_t, \actionuser_t, \actionrobot_t) \given \state_0 = \state}\\
  \actionuser_t &\sim \policyuser(\cdot \given \state_t)\\
  \actionrobot_t &\sim \policyrobot(\cdot \given \state_t, \actionuser_t)\\
  \state_{t+1} &\sim \transition(\cdot \given \state_t, \actionuser_t, \actionrobot_t)
\end{align*}
%\vrobot^{\policyrobot}(\state) &= \expctover{\policyuser, \policyrobot, \transition}{\sumtime \costrobot(\state_t, \actionuser_t, \actionrobot_t) \given \state_0 = \state}\\

The optimal value function $\vopt$ is the cost-to-go for the best robot policy:
\begin{align*}
  \vopt(\state) &= \min_{\policyrobot} \vrobot^{\policyrobot}(\state)
\end{align*}

The action-value function $\qopt$ computes the immediate cost of taking action $\actionrobot$ after observing $\actionuser$, and following the optimal policy thereafter:
\begin{align*}
  \qopt(\stateactions) &= \costrobot(\stateactions) + \expctarg{\vopt(\state')}
\end{align*}
%\qopt(\stateactions) &= \costrobot(\stateactions) + \expctover{\state'}{\vopt(\state')}
Where $\state' \sim \transition(\cdot \given \stateactions)$. The optimal robot action is given by $\argmin_\actionrobot \qopt(\stateactions)$.

In order to make explicit the dependence on the user goal, we often write these quantities as:
\begin{align*}
  \vgoal(\stateenv) &= \vopt(\state)\\
  \qgoal(\stateenvactions) &= \qopt(\stateactions)
\end{align*}

Computing the optimal policy and corresponding action-value function is a common objective in reinforcement learning. We assume access to this function in our framework, and describe our particular implementation in the experiments.

%The action-value function $\qrobot$ is defined as the immediate cost of taking action $\actionrobot$ after observing $\actionuser$, plus the cost of following $\policyrobot$ thereafter:
%\begin{align*}
%  \qrobot^{\policyrobot}(\stateactions) &= \costrobot(\stateactions) + \expctover{\state'}{\vrobot^{\policyrobot}(\state')}
%\end{align*}
%
%We define the optimal value and action-value functions as the cost-to-go for the best robot policy:
%\begin{align*}
%  \vopt(\state) &= \min_{\policyrobot} \vrobot^{\policyrobot}(\state)\\
%  \qopt(\stateactions) &= \costrobot(\stateactions) + \expctover{\state'}{\vopt(\state)}
%\end{align*}


\subsection{Cost Minimization with an unknown goal}
\label{sec:framework_unknown_goal}

We formulate the problem of minimizing a cost function with an unknown user goal as a Partially Observable Markov Decision Process (POMDP). A POMDP maps a distribution over states, known as the \emph{belief} $\belief$, to actions. We assume that all uncertainty is over the user's goal, and the environment state is known. This subclass of POMDPs, where uncertainty is constant, has been studied as a Hidden Goal MDP~\citep{fern_2010}, and as a POMDP-lite~\citep{chen_2016}.

In this framework, we infer a distribution of the user's goal by observing the user actions $\actionuser$. Similar to the known-goal setting (\cref{sec:framework_known_goal}), we define the value function of a belief as:
\begin{align*}
  \vrobot^{\policyrobot}(\belief) &= \expctarg{\sumtime \costrobot(\state_t, \actionuser_t, \actionrobot_t)  \given \belief_0 = \belief} \\
  \state_t &\sim \belief_t\\
  \actionuser_t &\sim \policyuser(\cdot \given \state_t)\\
  \actionrobot_t &\sim \policyrobot(\cdot \given \state_t, \actionuser_t)\\
  \belief_{t+1} &\sim \transitionbelief(\cdot \given \belief_t, \actionuser_t, \actionrobot_t)
\end{align*}
%\vrobot^{\policyrobot}(\belief) &= \expctover{\belief, \policyuser, \policyrobot, \transition}{\sumtime \costrobot(\state_t, \actionuser_t, \actionrobot_t)  \given \belief_0 = \belief} \\
Where the belief transition $\transitionbelief$ corresponds to transitioning the known environment state $\stateenv$ according to $\transition$, and updating our belief over the user's goal as described in $\cref{sec:framework_prediction}$. We can define quantities similar to above over beliefs:
\begin{align}
  \vopt(\belief) &= \min_{\policyrobot} \vrobot^{\policyrobot}(\belief) \label{eq:v_belief}\\
  \qopt(\beliefactions) &= \expctarg{\costrobot(\belief, \actionuser, \actionrobot) + \expctover{\belief'}{\vopt(\belief')}} \nonumber
\end{align}
%\qopt(\beliefactions) &= \expctover{\belief}{\costrobot(\belief, \actionuser, \actionrobot) + \expctover{\belief'}{\vopt(\belief')}}%\\&= \expctover{\belief}{\costrobot(\belief, \actionuser, \actionrobot)} + \min_\policyrobot \expctover{\belief}{\expctover{\belief'}{\vopt(\belief')}}


%\begin{align*}
%  \qrobot^{\policyrobot}(\beliefactions) &= \expctover{\belief}{\costrobot(\beliefactions) + \expctover{\belief'}{\vrobot^{\policyrobot}(\belief')}}
%\end{align*}




%As the robot does not know the user's goal a priori, we infer this goal from the user actions $\actionuser$ based on our models $\policyuser(\state)$.

%We can model the robots objective of minimizing a cost function with uncertainty using a Partially Observable Markov Decision Process (POMDP) with uncertainty over the user's goal. A POMDP maps a distribution over states, known as the \emph{belief} $\belief$, to actions. We assume that all uncertainty is over the user's goal, and the environment state is known, as in a Hidden Goal MDP~\citep{fern_2010}. Note that allowing the cost to depend on the observation $\actionuser$ is non-standard, but important for shared autonomy, as prior works suggest that users prefer maintaining control authority~\citep{kim_2012}. Our shared autonomy POMDP is defined by the tuple $\left(\Staterobgoal, \Actionrobot, \transition, \costrobot, \Actionuser, \policyuser, \right)$. The optimal solution to this POMDP minimizes the expected accumulated cost $\costrobot$. As this is intractable to compute, we utilize Hindsight Optimization to select actions, described in \cref{sec:framework_hindsight}.





%%We model the robot as a deterministic dynamical system with transition function $\transition: \Stateenv \times \Actionrobot \rightarrow \Stateenv$. %  where applying action $\actionrobot$ in state $\stateenv$ results in state $\stateenv'$.
%%The user supplies continuous inputs $\actionuser \in \Actionuser$ via an interface (e.g. joystick, mouse). These user inputs map to robot actions through a known deterministic function $\userinputtoaction: \Actionuser \rightarrow \Actionrobot$, corresponding to the effect of \emph{direct teleoperation}.
%
%In our scenario, the user wants to move the robot to one goal in a discrete set of goals $\goal \in \Goal$. We assume access to a stochastic user policy for each goal $\policyusergoal(\stateenv) = p(\actionuser | \stateenv, \goal)$, usually learned from user demonstrations. %Here, the user assumes inputs get mapped directly to actions through $\userinputtoaction$ - thus, they assume direct teleoperation.
%In our system, we model this policy using the maximum entropy inverse optimal control (MaxEnt IOC) framework of~\citet{ziebart_2008}, which assumes the user is approximately optimizing some cost function for their intended goal $g$, $\costusergoal: \Stateenv \times \Actionuser \rightarrow \mathcal{R}$. This model corresponds to a goal specific Markov Decision Process (MDP), defined by the tuple $\left(\Stateenv, \Actionuser, \transition, \costusergoal\right)$. We discuss details in \cref{sec:framework_prediction}. 
%
%Unlike the user, our system does not know the intended goal. We model this with a Partially Observable Markov Decision Process (POMDP) with uncertainty over the user's goal. A POMDP maps a distribution over states, known as the \emph{belief} $\belief$, to actions. Define the system state $\staterobgoal \in \Staterobgoal$ as the robot state augmented by a goal, $\staterobgoal = (\stateenv, \goal)$ and $\Staterobgoal = \Stateenv \times \Goal$. In a slight abuse of notation, we overload our transition function such that $\transition: \Staterobgoal \times \Actionrobot \rightarrow \Staterobgoal$, which corresponds to transitioning the robot state as above, but keeping the goal the same.
%
%In our POMDP, we assume the robot state is known, and all uncertainty is over the user's goal. Observations in our POMDP correspond to user inputs $\actionuser \in \Actionuser$. Given a sequence of user inputs, we infer a distribution over system states (equivalently a distribution over goals) using an observation model $\pomdpohm$. This corresponds to computing $\policyusergoal(\stateenv)$ for each goal, and applying Bayes' rule. We provide details in \cref{sec:framework_prediction}.
%
%The system uses cost function $\costrobot: \Staterobgoal \times \Actionrobot \times \Actionuser \rightarrow \mathcal{R}$, corresponding to the cost of taking robot action $\actionrobot$ when in system state $\staterobgoal$ and the user has input $\actionuser$. Note that allowing the cost to depend on the observation $\actionuser$ is non-standard, but important for shared autonomy, as prior works suggest that users prefer maintaining control authority~\citep{kim_2012}. This formulation enables us to penalize robot actions which deviate from $\userinputtoaction(\actionuser)$. Our shared autonomy POMDP is defined by the tuple $\left(\Staterobgoal, \Actionrobot, \transition, \costrobot, \Actionuser, \pomdpohm \right)$. The optimal solution to this POMDP minimizes the expected accumulated cost $\costrobot$. As this is intractable to compute, we utilize Hindsight Optimization to select actions, described in %\cref{sec:hindsight}.





\input{framework_hindsight}
\input{framework_prediction}
\input{framework_multitarget}
