\section{Human-Robot Teaming}
\label{sec:human_robot_teaming}

%copy in specifics of formulation from iros paper

In human-robot teaming, the user and robot want to achieve a set of related goals. Formally, we assume a set of user goals $\usergoal \in \userGoal$ and robot goals $\robotgoal \in \robotGoal$, where both want to achieve all goals. However, there may be constraints on how these goals can be achieved (e.g. user and robot cannot simultaneously use the same object~\citep{hoffman_2007}). We apply a conservative model for these constraints through a \emph{goal restriction set} $\goalrestrictionset = \left\{ (\usergoal, \robotgoal) : \text{Cannot achieve $\usergoal$ and $\robotgoal$ simultaneously}\right\}$. In order to efficiently collaborate with the user, our objective is to simultaneously predict the human's intended goal, and achieve a robot goal not in the restricted set. We remove the achieved goals from their corresponding goal sets, and repeat this process until all robot goals are achieved.

The state $\stateenv$ corresponds to the state of both the user and robot, where $\actionuser$ affects the user portion of state, and $\actionrobot$ affects the robot portion. The transition function $\transitionallargs$ deterministically transitions the state by applying $\actionuser$ and $\actionrobot$ sequentially.

For prediction, we used the same cost function for $\costtarguser$ as in our shared teleoperation experiments (\cref{sec:shared_teleop}). Let $d$ be the distance between the robot state $\stateenv' = \transitionuser(\stateenv, \actionuser)$\footnote{We sometimes instead observe $\stateenv'$ directly (e.g. sensing the pose of the user hand)} and target $\target$:
\begin{align*}
  \costtarguser(\stateenv, \actionuser) &= \left\{ \begin{array}{cc} \alpha & d > \delta \\ \frac{\alpha}{\delta} d & d\leq \delta \end{array} \right.
\end{align*}
Which behaves identically to our shared control teleoperation setting: when the distance is far away from any target $(d > \delta)$, probability shifts towards goals relative to how much progress the user makes towards them. When the user stays close to a particular target $(d \leq \delta)$, probability mass shifts to that goal, as the cost for that goal is less than all others.

Unlike our shared control teleoperation setting, our robot cost function does not aim to achieve the same goal as the user, but rather any goal not in the restricted set. As in our shared autonomy framework, let $\goal$ be the user's goal. The cost function for a particular user goal is:
\begin{align*}
  \costgoalrobot(\stateenv, \actionuser, \actionrobot) &= \min_{\robotgoal \text{\;s.t.\;} (\goal, \robotgoal) \not\in \goalrestrictionset} \costuser_{\robotgoal}(\stateenv, \actionrobot)
\end{align*}
Where $\costgoaluser$ uses the cost for each target $\costtarguser$ to compute the cost function as described in \cref{sec:framework_multitarget}. Additionally, note that the $\min$ over cost functions looks identical to the $\min$ over targets to compute the cost for a goal. Thus, for deterministic transition functions, we can use the same proof for computing the value function of a goal given the value function for all targets (\cref{sec:framework_multigarget_assistance}) to compute the value function for a robot goal given the value function for all user goals:
\begin{align*}
  \vgoalrobot(\stateenv) &= \min_{\robotgoal \text{\;s.t.\;} (\goal, \robotgoal) \not\in \goalrestrictionset} \vuser_{\robotgoal}(\stateenv)
\end{align*}

This simple cost function provides us a baseline for performance. We might expect better collaboration performance by incorporating costs for collision avoidance with the user~\citep{mainprice_2013, lasota_2015}, social acceptability of actions~\citep{sisbot_2007}, and user visibility and reachability~\citep{sisbot_2010, pandey_2010, mainprice_2011}. We use this cost function to test the viability of our framework as it enables closed-form computation of the value function.

This cost and value function causes the robot to go to any goal currently in it's goal set $\robotgoal \in \robotGoal$ which is not in the restriction set of the user goal $\goal$. Under this model, the robot makes progress towards goals that are unlikely to be in the restricted set and have low cost-to-go. As the form of the cost function is identical to that which we used in shared control teleoperation, the robot behaves similarly: making constant progress when far away $(d > \delta)$, and slowing down for alignment when near $(d \leq \delta)$. The robot terminates and completes the task once some condition is met (e.g. $d \leq \epsilon$).


\subsubsection*{Hindsight Optimization for Human-Robot Teaming}

Similar to shared control teleoperation, we believe hindsight optimization is a suitable POMDP approximation for human-robot teaming. The efficient computation enables us to respond quickly to changing user goals, even with continuous state and action spaces. For our formulation of human-robot teaming, explicit information gathering is not possible: As we assume the user and robot affect different parts of state space, robot actions are unable to explicitly gather information about the user's goal. Instead, we gain information freely from user actions.


%To implement the non-adaptive control algorithm, the system simply calculated a random ordering of the four boxes, then performed a stamping action for each box. To implement the predict-then-act algorithm, the system ran the human goal prediction algorithm from \cref{sec:framework_prediction} until a certain confidence was reached (50\%), then selected a goal that was not within the restricted set $\goalrestrictionset$ and performed a stamping action on that goal. There was no additional human goal monitoring once the goal action was selected. In contrast, our POMDP implementation performed as described in \cref{sec:framework}, accounting continually for adapting human goals and seamlessly re-planning when the human's goal changed.



\input{experiment_iros_2016}

