\section{Introduction}
\label{sec:intro}


\graphicspath{{./}{./images/}{./images_rss_2015/}}


Human-robot collaboration studies interactions between humans and robots sharing a workspace. One instance of collaboration arises in \emph{shared autonomy}, where both the user and robotic system act simultaneously to achieve shared goals. For example, in \emph{shared control teleoperation}~\citep{goertz_1963, rosenberg_1993, aigner_1997, debus_2000, dragan_2013_assistive}, both the user and system control a single entity, the robot, in order to achieve the user's goal. In \emph{human-robot teaming}, the user and system act independently to achieve a set of related goals~\citep{hoffman_2007, arai_2010, dragan_2013_legible, koppula_2013, mainprice_2013, gombolay_2014, nikolaidis_2017_mutual}. 

While each instance of shared autonomy has many unique requirements, they share a key common challenge - for the autonomous system to be an effective collaborator, it needs to know the user's goal. For example, feeding with shared control teleoperation, an important task for assistive robotics~\citep{chung_2013}, requires knowing what the user wants to eat (\cref{subfig:eating_ambiguous_goal}). Wrapping gifts with a human-robot team requires knowing which gift the user will wrap to avoid getting in their way and hogging shared resources (\cref{subfig:teaming_ambiguous_goal}). 


\begin{figure}[t]
\centering
\begin{subfigure}{0.24\textwidth}
  \includegraphics[width=1.0\textwidth, trim=0 0 0 0, clip=true]{eating_ambiguous_goal.png}
  \caption{Shared Control Teleop}
  \label{subfig:eating_ambiguous_goal}
\end{subfigure}
\hfill
\begin{subfigure}{0.24\textwidth}
  \includegraphics[width=1.0\textwidth, trim=0 0 0 0, clip=true]{teaming_ambiguous_goal.png}
  \caption{Human-Robot Teaming}
  \label{subfig:teaming_ambiguous_goal}
\end{subfigure}
\caption{We can provide useful assistance even when we do not know the user's goal. (\subref{subfig:eating_ambiguous_goal}) Our feeding experiment, where the user wants to eat one of the bites of food on the plate. With an unknown goal, our method autonomously orients the fork and moves towards all bites. In contrast, predict-then-act methods only helped position the fork at the end of execution. Users commented that the initial assistance orienting the fork and getting close to all bites was the most helpful, as this was the most time consuming portion of the task. (\subref{subfig:teaming_ambiguous_goal}) Our teaming experiment, where the user wraps a box, and the robot must stamp a different box. Here, the user's motion so far suggests their goal is likely either the green or white box. Though we cannot confidently predict their single goal, our method starts making progress for the other boxes.}
 \label{fig:ambiguous_goals}
\end{figure} 



%While each application has many unique requirements, they share a key common challenge - for the autonomous system to be an effective collaborator, it needs to know the user's goal. For feeding with shared control teleoperation, the system should know what the user wants to eat \cref{subfig:eating_ambiguous_goal}. For human-robot team gift wrapping, the system should know which gift the user will wrap to avoid getting in their way \cref{subfig:teaming_ambiguous_goal}. 

%\sjnote{Any more about difficulty or challenges in these scenarios? Teleop: low degree of freedom input with high degree of freedom robot, noisy input devices. Teaming? Maybe put this motivation in the section corresponding to each scenario?}% In this work, we present a general framework for shared autonomy, and specific applications to these scenarios.
%These instances of shared autonomy share a common requirement of simultaneously predicting the user's goal, and a
%\hanote{Yes, I agree that this info would be useful to motivate. Would also like to emphasize the similarities of teaming and teleop, since this paper tries to unify them.}
%\spnote{I agree and I would discuss it here as suggested by Henny. Teaming: to grant human safety and comfort when both the human and the robot are acting in the same environment, acceptability issues (the human has to accept the robot coworker), the robot should not slow down the human  }

%Many prior works suggest that should not rely on explicit mechanisms for communication (e.g. buttons)
%In shared autonomy, the utility of autonomous assistance depends on the user's goal, making it challenging to provide assistance when the user goal is unknown. We could require that users specify their goals explicitly prior to execution. 

In general, the system does not know the user's goal apriori. We could alleviate this issue by requiring users to explicitly specify their goals (e.g. through voice commands). However, there are often a continuum of goals to choose from (e.g. location to place an object, size to cut a bite of food), making it impossible for users to precisely specify their goals. Furthermore, prior works suggest requiring explicit communication leads to ineffective collaboration~\citep{vanhooydonck_2003, goodrich_2003, green_2007}. Instead, implicit information should be used to make collaboration seamless. In shared autonomy, this suggests utilizing sensing of the environment and user actions to infer the user's goal. This idea has been successfully applied for shared control teleoperation~\citep{li_2003,yu_2005,kragic_2005,kofman_2005,aarno_2008,carlson_2012, dragan_2013_assistive, hauser_2013, muelling_2015} and human-robot teaming~\citep{hoffman_2007, nguyen_2011, macindoe_2012, mainprice_2013, koppula_2013, lasota_2015}.  


%As the utility of assistance actions depends on the user's goal, most shared autonomy methods take a \emph{predict-then-act} approach. These system only predict at first, running an intent inference algorithm until a single user goal is predicted with high probability. The system then assists given that goal.
%Given that single goal, the system then provides assistance. While this works well in simple scenarios with few goals, in many real-world scenarios (e.g. cluttered scenes), we may not confidently predict the user's goal until the end of execution.

%benefits: lack of harmful assistance (good when we prefer no assistance to bad assistance?)

As providing effective assistance requires knowing the user's goal, most shared autonomy methods do not assist when the goal is unknown. These works split shared autonomy into two parts: 1) predict the user's goal with high probability, and 2) assist for that single goal, potentially using prediction confidence to regulate assistance. We refer to this approach as \emph{predict-then-act}. While this has been effective in simple scenarios with few goals~\citep{yu_2005, kofman_2005, carlson_2012, dragan_2013_assistive, koppula_2013, muelling_2015}, it is often impossible to predict the user's goal until the end of execution (e.g. cluttered scenes), causing these methods to provide little assistance. Addressing this lack of assistance is of great practical importance - with uncertainty over only goals in our feeding experiment, a predict-then-act method provided assistance for only $31\%$ of the time on average, taking $29.4$ seconds on average before the confidence threshold was initially reached.
%users spent an average of $29.4$ seconds ($66.4\%$ of the total time) before a predict-then-act method was confident enough to provide any assist.

%if we want to mention that we assume user does not adapt to assistance in intro, do so before mentioning POMDP
In this work, we present a general framework for goal-directed shared autonomy that does not rely on predicting a single user goal (\cref{fig:robot_human_model}). We assume the user's goal is fixed (e.g. they want a particular bite of food), and the autonomous system should adapt to the user goal\footnote{While we assume the goal is fixed, we do not assume how the user will achieve that goal (e.g. grasp location) is fixed.}. Our key insight is that there are useful assistance actions for \emph{distributions over goals}, even when confidence for a particular goal is low (e.g. move towards multiple goals) (\cref{fig:ambiguous_goals}). We formalize this notion by modelling our problem as a Partially Observable Markov Decision Process (POMDP)~\citep{kaelbling_1998_pomdp}, treating the user's goal as hidden state. When the system is uncertain of the user goal, our framework naturally optimizes for an assistance action that is helpful for many goals. When the system confidently predicts a single user goal, our framework focuses assistance given that goal (\cref{fig:teledata}). %In our feeding experiment, our POMDP based method completed the task ($18.5$ seconds on average) before a predict-then-act method even started assisting ($29.4$ seconds on average).
%predict-then-act $48.6$ seconds to complete the task.

As our state and action spaces are both continuous, solving for the optimal action in our POMDP is intractable. Instead, we approximate using QMDP~\citep{littman_1995}, also referred to as hindsight optimization~\citep{chong_2000,yoon_2008}. This approximation has many properties suitable for shared autonomy: it is computationally efficient, works well when information is gathered easily~\citep{koval_2014}, and will not oppose the user to gather information. The result is a system that minimizes the expected cost-to-go to assist for any distribution over goals. 

We apply our framework in user study evaluations for both shared control teleoperation and human-robot teaming. For shared control teleoperation, users performed two tasks: a simpler object grasping task (\cref{sec:experiment_rss_2015}), and a more difficult feeding task (\cref{sec:experiment_hri_2016}). In both cases, we find that our POMDP based method enabled users to achieve goals faster and with less joystick input than a state-of-the-art predict-then-act method~\citep{dragan_2013_assistive}. Subjective user preference differed by task, with no statistical difference for the simpler object grasping task, and users preferring our POMDP method for the more difficult feeding task. 
%An examination of user comments also reveals insights into how users respond to failures depending on the level of autonomy. 

For human-robot teaming (\cref{sec:experiment_iros_2016}), the user and robot performed a collaborative gift-wrapping task, where both agents had to manipulate the same set of objects while avoiding collisions. We found that users spent less time idling and less time in collision while collaborating with a robot using our method. However, results for total task completion time are mixed, as predict-then-act methods are able to take advantage of more optimized motion planners, enabling faster execution once the user goal is confidently predicted. 


%We believe these results demonstrate the effectiveness of our POMDP based framework for shared autonomy scenarios, even with a very simple cost function. 



\begin{figure}[t]
\centering
\includegraphics[width=0.49\textwidth, trim=34 156 45 64, clip=true]{fig1_system_ppt.pdf}
\caption{ Our shared autonomy framework. We assume the user executes a policy for their single goal, depicted as a heatmap plotting the value function at each position. Our shared autonomy system models all possible user goals and their corresponding policies. From user actions $\actionuser$, a distribution over goals is inferred. Using this distribution and the value functions for each goal, the system selects an action $\actionrobot$. The world transitions from $\stateenv$ to $\stateenv'$. The user and shared autonomy system both observe this state, and repeat action selection.}
 \label{fig:robot_human_model}
\end{figure} 

\begin{figure}[t]
\centering
 \begin{subfigure}{0.240\textwidth}
   \centering 
   \hspace*{1mm} \includegraphics[width=0.92\textwidth, trim=400 50 400 450, clip=true]{screen_data2.png}
   \hspace*{-2mm} \includegraphics[trim=1 3 0 -3, clip=true]{tele_data1_floating.pdf} \hfill
   \caption{}
 \label{fig:teledata_cen}
 \end{subfigure}
 \hfill
 \begin{subfigure}{0.240\textwidth}
   \centering
   \hspace*{1mm} \includegraphics[width=0.92\textwidth, trim=400 50 400 450, clip=true]{screen_data3.png}
   \hspace*{-1mm}\includegraphics[trim=1 3 0 -3, clip=true]{tele_data3_floating.pdf} \hfill
  \caption{}
 \label{fig:teledata_back}
 \end{subfigure}
 \caption{Arbitration as a function of confidence with two goals. Confidence $=\max_g p(g) - \min_g p(g)$, which ranges from $0$ (equal probability) to $1$ (all probability on one goal). (\subref{fig:teledata_cen}) The hand is directly between the two goals, where no action assists for both goals. As confidence for one goal increases, assistance increases. (\subref{fig:teledata_back}) From here, going forward assists for both goals, enabling the assistance policy to make progress even with $0$ confidence.}
\label{fig:teledata}
\end{figure} 




%Compared to predict-then-act methods, this system enabled goals to be achieved faster, with less user effort, and less robot idling time.
%
%
%%Robotic teleoperation enables a user to achieve their goal by providing inputs into a robotic system.
%Teleoperation systems are used in many settings, such as assistive robotics, surgical robotics, remotely operated vehicles, space robotics, and industrial assembly. In practice, most systems use \emph{direct teleoperation}, where user inputs are mapped directly to robot actions, putting the burden of control entirely on the user.
%However, input interfaces are noisy, and often have fewer degrees of freedom than the robot they control. This makes operation tedious, and many goals impossible to achieve.
%%For each setting, many different input interfaces have been developed, varying in their cost, intuitiveness of use, and ability to enable users to achieve their goals.
%
%\emph{Shared autonomy} seeks to alleviate these issues by combining teleoperation with autonomous assistance. This idea has successfully been applied in many settings, such as assisting disabled users to control robotic arms~\citep{kim_2006_bci, kim_2012, mcmullen_2014, katyal_2014, schroeer_2015, muelling_2015} or wheelchairs~\citep{simpson_2005, li_2011, carlson_2012}, operate robots remotely~\citep{shen_2004, you_2011, leeper_2012}, decrease operator fatigue in surgical settings~\citep{park_2001, marayong_2003, kragic_2005, aarno_2005_virtualfixtures, li_2007}, and many more.
%
%%However, combining manual teleoperation with autonomous assistance creates a tension between user control authority and autonomy.
%While these systems show promise, prior studies are mixed in terms of when and how to provide autonomous assistance. On the one hand, more autonomy leads to greater efficiency and accuracy~\citep{leeper_2012}, which often correlates with higher user satisfaction~\citep{you_2011, dragan_2013_assistive}. On the other hand, users have expressed preference for having more control~\citep{kim_2012}, even at the cost of efficiency and easy of use~\cite{lawitzky_2013, javdani_2015_rss, gopinath_2016}.
%
%\citet{dragan_2013_assistive} hypothesize that this disparity depends on task difficulty, where ``users might appreciate assistance if the task is very hard for them''. \citet{kim_2012} believe another key factor is the user themselves, hypothesizing that ``while able-bodied users may prefer to cede autonomy to robots, disabled users tend to see the robot not merely as an agent to retrieve objects but also as a quintessential tool to reassert their domain of interaction with their environment''. 
%
%Studies have investigated how users respond to different \emph{magnitudes} of autonomy for a specific method~\citep{li_2011, dragan_2013_assistive, gopinath_2016}. In this work, we compare different methods of shared autonomy for a task requiring precise placement of a robotic arm.
%%However, when given the option of changing autonomy levels within a shared autonomy scheme, \citet{gopinath_2016} found that users with spinal cord injuries asked for more assistance then able bodied users for the same task.
%
%A major source of difficulty for many teloperation systems is that the input interface often has fewer degrees of freedom then that robot it controls. \emph{Modal control} addresses this issue through the interface, asking the user to toggle between different control modes, each of which controls a subset of the degrees of freedom~\citep{rosier_1991, maheu_2011, herlant_2016, gopinath_2016}. While this makes operation more challenging and time consuming, it is crucial for assistive arms, as users have decreased mobility~\cite{herlant_2016, gopinath_2016}.
%
%%In assistive robotics, it was found that difficulty due to switching modes causes common tasks such as eating to require so much effort that users abandon these tasks all together~\citep{herlant_2016}.
%
%%Most studies of shared autonomy for assistive arms focus on pick-and-place tasks~\cite{you_2011, dragan_2013_assistive, javdani_2015_rss, gopinath_2016}. 
%
%%We study the task of using a fork to retrieve a bite of food using modal control \cref{fig:study_ex}. Eating has been identified as one of the most important tasks for assistive robotic arms~\citep{chung_2013}. However, interviews with current users of assistive arms has found that using the arm for eating requires so much effort that users do not even attempt it~\citep{herlant_2016}. In contrast to many previous studies of shared autonomy systems, this task requires precise placement of both the position and orientation of the arm.
%
%We study four different methods for this task - direct teleoperation, full autonomy, and two methods for shared autonomy. The first is linear blending~\cite{dragan_2013_assistive}, which combines user inputs with a robot policy based on how confident the system is in the predicted user goal. As a result, assistance is often not provided until the end of execution, helping the user do the final alignment. The second uses a Partially Observable Markov Decision Process (POMDP)~\cite{kaelbling_1998_pomdp} to provide assistance throughout execution, even when the system is not confident in any particular goal~\cite{javdani_2015_rss}. Assistance is provided throughout, even when the user provides no inputs. In a simple pick-and-place task, this has led to user dissatisfaction, where users perceived the robot as having a ``mind of it's own''~\cite{javdani_2015_rss}.
%
%As the task is quite difficult, we expected that failures would be common. Indeed, we found that with direct teleoperation, users failed about 40\% of the time, and full autonomy 20\% of the time. We hypothesized that trial success would be correlated with user preference. Surprisingly, this was not always the case, as users preferred direct teleoperation over full autonomy, even though success rate was lower and task completion time was higher. Notably, user comments for the autonomous system mention task failure more then direct teleoperation, suggesting that users penalize autonomy for failure more than themselves. 
%
%We also found that the method for assistance affected both user preference and task efficiency. The blend method, which does not assist for the majority of execution time, was ranked as the worst system on average, suggesting it provides a poor tradeoff between autonomy and user control for this task. In contrast, the POMDP based method was ranked as the best on average, suggesting users were willing to give up some control for assistance.
%
%%\citep{maheu_2011} assistive arms can decrease caregiver need by 41\%
%



