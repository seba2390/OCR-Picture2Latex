\subsection{Human-Robot Teaming}
\label{sec:related_collaboration}

%\citet{hoffman_2007} test a system where the robot minimizes a cost function which includes predictions of the human collaborator.


In human-robot teaming, robot action selection that models and optimizes for the human teammate leads to better collaboration. \citet{hoffman_2007} show that using predictions of a human collaborator during action selection led to more efficient task completion and more favorable perception of robot contribution to team success. \citet{lasota_2015} show that planning to avoid portions of the workspace the user will occupy led to faster task completion, less user and robot idling time, greater user satisfaction, and greater perceived safety and comfort. \citet{arai_2010} show that users feel high mental strain when a robot collaborator moves too close or too quickly.

Motion planners have been augmented to include user models and collaboration constraints. For static users, researchers have incorporated collaboration constraints such as safety and social acceptability~\citep{sisbot_2007}, and task constraints such as user visibility and reachability~\citep{sisbot_2010, pandey_2010, mainprice_2011}. For moving users, \citet{mainprice_2013} use a Gaussian mixture model to predict user motion, and select a robot goal that avoids the predicted user locations.

%\citet{losata_2014} develop a system to localize humans in the robot workspace, and slow down the robot if the distance to users falls below a threshold.

%Many works in human-robot teaming focus on developing human aware motion planners. For static users, \citet{sisbot_2007} present a motion planner that considers the safety, vision field, and socially acceptable of robot motions. They extend this work to include kinematic and task constraints~\citep{sisbot_2010}. \citet{mainprice_2011} present a motion planner for maintaining distance to humans while considering task constraints such as visibility.


Similar ideas have been used to avoid moving pedestrians. \citet{ziebart_2009} learn a predictor of pedestrian motion, and use this to predict the probability a location will be occupied at each time step. They build a time-varying cost map, penalizing locations likely to be occupied, and optimize trajectories for this cost. \citet{chung_2011} use A* search to predict pedestrian motions, including a model of uncertainty, and plan paths using these predictions. \citet{bandy_2012} use fixed models for pedestrian motions, and focus on utilizing a POMDP framework with SARSOP~\citep{kurniawati_2008} for selecting good actions. Like our approach, this enables them to reason over the entire distribution of potential goals. They show this outperforms utilizing only the maximum likelihood estimate of goal prediction for avoidance. 

Others develop methods for how the human-robot team should be structured. \citet{gombolay_2014} study the effects of having the user and robot assign goals to each other. They find that users were willing to cede decision making to the robot if it resulted in greater team fluency~\citep{gombolay_2014}. However, \citet{gombolay_2017} later show that having the autonomous entity assign goals led to less situational awareness. Inspired by training schemes for human-human teams, \citet{stefanos_2013} present a human-robot cross training method, where the user and robot iteratively switch roles to learn a shared plan. Their model leads to greater team fluency, more concurrent motions, greater perceived robot performance, and greater user trust. \citet{koppula_2013} use conditional random fields to predict the user goal (e.g. grasp cup), and have a robot achieve a complementary goal (e.g. pour water into cup).% Much like our setting, the robot goal is highly dependent on the user goal.

%gombolay_2014: Users also preferred looser couplings between goals when they did not assign them. 





Others have studied how robot motions can influence the belief of users. \citet{sisbot_2010} fix the gaze of the robot on its goal to communicate intent. \citet{dragan_2013_legible} incorporate legibility into the motion planner for a robotic arm, causing the robot to exaggerate its motion to communicate intent. They show this leads to more quickly and accurately predicting the robot intent~\citep{dragan_2013_legible_hri}. \citet{rezvani_2016} study the effects of conveying a robot's state (e.g. confidence in action selection, anomaly in detection) directly on a user interface for autonomous driving. %They find conveying information leads to greater user awareness and trust.

Recent works have gone one step further, selecting robot actions that not only change the perceptions of users, but also their actions. \citet{nikolaidis_2017_mutual} model how likely users are to adopt the robot's policy based on robot actions. They utilize a POMDP to simultaneously learn this user adaptability while steering users to more optimal goals to achieve greater reward. \citet{nikolaidis_2017_game} present a more general game theoretic approach where users change their actions based on robot actions, while not completely adopting the robot's policy. Similarly, \citet{sadigh_2016} generate motions for an autonomous car using predictions of how other drivers will respond, enabling them to change the behavior of other users, and infer the internal user state~\citep{sadigh_iros2016}.

Teaming with an autonomous agent has also been studied outside of robotics. \citet{fern_2010} have studied MDPs and POMDPs for interactive assistants that suggest actions to users, who then accept or reject each action. They show that optimal action selection even in this simplified model is PSPACE-complete. However, a simple greedy policy has bounded regret. \citet{nguyen_2011} and \citet{macindoe_2012} apply POMDPs to cooperative games, where autonomous agents simultaneously infer human intentions and take assistance actions. Like our approach, they model users as stochastically optimizing an MDP, and solve for assistance actions with a POMDP. In contrast to these works, our state and action spaces are continuous.

