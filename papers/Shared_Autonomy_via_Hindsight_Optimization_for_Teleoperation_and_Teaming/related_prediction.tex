\subsection{User Prediction}
\label{sec:related_prediction}

%domains such as surgical robotics~\cite{kragic_2005}, virtual telepresence~\cite{schrempf_2007}, general robotic manipulators~\cite{dragan_2013_assistive, hauser_2013}, assistive wheelchairs~\cite{carlson_2012}.

A variety of models and methods have been used for intent prediction. Hidden markov model (HMM) based methods~\citep{li_2003,kragic_2005, aarno_2005_virtualfixtures, aarno_2008} predict subtasks or intent during execution, treating the intent as latent state. \citet{schrempf_2007} use a Bayesian network constructed with expert knowledge. \citet{koppula_2013} extend conditional random fields (CRFs) with object affordance to predict potential human motions. \citet{wang_2013_intentioninference} learn a generative predictor by extending Gaussian Process Dynamical Models (GPDMs) with a latent variable for intention. \citet{hauser_2013} utilizes a Gaussian mixture model over task types (e.g. reach, grasp), and predicts both the task type and continuous parameters for that type (e.g. movements) using Gaussian mixture autoregression.

%Surgical settings~\cite{kragic_2005, aarno_2005_virtualfixtures} (HMMs for subtasks. Aarno work usually straight lines)
%Extend that work to more complicated skills (primitives and tasks layerd)~\cite{aarno_2008}
%Virtual telepresence~\cite{schrempf_2007} (Bayesian network constructed with expert knowledge)

Many successful works in shared autonomy utilize of maximum entropy inverse optimal control (MaxEnt IOC)~\citep{ziebart_2008} for user goal prediction. Briefly, the user is modelled as a stochastic policy approximately optimizing some cost function. By minimizing the worst-case predictive loss, \citet{ziebart_2008} derive a model where trajectory probability decreases exponentially with cost. They then derive a method for inferring a distribution over goals from user inputs, where probabilities correspond to how efficiently the inputs achieve each goal~\citep{ziebart_2009}. A key advantage of this framework for shared autonomy is that the we can directly optimize for the cost function used to model the user.
%As this framework explicitly models a cost function for the user, which we directly optimize in our shared autonomy policy.

Exact, global inference over these distributions is computationally infeasible in continuous state and action spaces. Instead, \citet{levine_2012} provide a method that considers the expert demonstrations as only locally optimal, and utilize Laplace's method about the expert demonstration to estimate the log likelihood during learning. Similarly, \citet{dragan_2013_assistive} use Laplace's method about the optimal trajectory between any two points to approximate the distribution over goals during shared control teleoperation. \citet{finn_2016} simultaneously learn a cost function and policy consistent with user demonstrations using deep neural networks, utilizing importance sampling to approximate inference with few samples. Inspired by Generative Adversarial Nets~\citep{goodfellow_2014}, \citet{ho_2016} directly learn a policy to mimic the user through Generative Adversarial Imitation Learning.

We use the approximation of \citet{dragan_2013_assistive} in our framework due to empirical evidence of effectiveness in shared autonomy systems~\citep{dragan_2013_assistive, muelling_2015}. 


