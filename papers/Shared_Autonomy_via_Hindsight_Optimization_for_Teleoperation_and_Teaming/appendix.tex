\begin{appendices}

\begin{table*}[!t]
\centering
\begin{tabular}{rl}
\toprule
Symbol & Description\\ \midrule
$\stateenv \in \Stateenv$ & Environment state, e.g. robot and human pose\\
$\goal \in \Goal$ & User goal\\
$\stateenvgoal \in \Stateenvgoal$ & $\stateenvgoal = (\stateenv, \goal)$. State and user goal\\
$\actionuser \in \Actionuser$ & User action\\
$\actionrobot \in \Actionrobot$ & Robot action\\
%$\userinputtoaction(\actionuser) \in \Actionrobot$ & Direct teleoperation action from $\actionuser$\\
$\costuser(\state, \actionuser) = \costusergoal(\stateenv, \actionuser)$ & Cost function for each user goal\\
$\costrobot(\state, \actionuser, \actionrobot) = \costrobotgoal(\stateenv, \actionuser, \actionrobot)$ & Robot cost function for each goal\\
$\transitionallargs$ & Transition function of environment state\\
$\transition( (\stateenv', g) \given (\stateenv, g), \actionuser, \actionrobot) = \transition( \stateenv' \given \stateenv, \actionuser, \actionrobot)$ & User goal does not change with transition\\
$\transitionuser( \stateenv' \given \stateenv, \actionuser) = \transition( \stateenv' \given \stateenv, \actionuser, 0)$ & User transition function assumes the user is in full control\\
$\vgoal(\stateenv) = \vopt(\state)$ & The value function for a user goal and environment state\\
$\qgoal(\stateenvactions) = \qopt(\stateactions)$ & The action-value function for a user goal and environment state\\
%$\state \in \Stateenv$ & System state, where $\state = (\stateenv, \goal)$ and $ \State = \Stateenv \times \Goal$\\
%$\costusergoal(\stateenv, \actionuser)$ & User cost of input $\actionuser$ in robot state $\stateenv$ with goal $\goal$\\
%$\costrobot(\state, \actionrobot, \actionuser)$ & System cost of action $\actionrobot$ in state $\state$ with user input $\actionuser$\\
$\belief$ & Belief, or distribution over states in our POMDP.\\
$\transitionbelief(\belief' \given \belief, \actionuser, \actionrobot)$ & Transition function of belief state\\
$\vrobot^{\policyrobot}(\belief)$ & Value function for following policy $\policyrobot$ given belief $\belief$\\
$\qrobot^{\policyrobot}(\belief, \actionuser, \actionrobot)$ & Action-Value for taking actions $\actionuser$ and $\actionrobot$ and following $\policyrobot$ thereafter\\
$\vhs(\belief)$ & Value given by Hindsight Optimization approximation\\
$\qhs(\belief, \actionuser, \actionrobot)$ & Action-Value given by Hindsight Optimization approximation\\
\bottomrule
\end{tabular}
\captionof{table}{Variable definitions}
\label{table:variable_definitions}
\end{table*}


\section{Variable Definitions} 
\label{sec:variable_definitions}

For reference, we provide a table of variable definitions in \cref{table:variable_definitions}.

\section{Multi-Target MDPs Proofs}
\label{sec:mingoal_thms}

Below we provide the proofs for decomposing the value functions for MDPs with multiple targets, as introduced in~\cref{sec:framework_multitarget}.


\subsection{\Cref{thm:mingoal_assist}: Decomposing value functions}
Here, we show the proof for our theorem that we can decompose the value functions over that the targets for deterministic MDPs. The proofs here are written for our shared autonomy scenario. However, the same results hold for any deterministic MDP:
\valfundecompose*
\begin{proof}
We show how the standard value iteration algorithm, computing $\qgoal$ and $\vgoal$ backwards, breaks down at each time step. At the final timestep T, we get:
\begin{align*}
  \qgoalt{T}(\stateenvactions) &= \costgoal(\stateenvactions)\\
  &= \costtarg(\stateenvactions) \qquad \text{for any $\target$}\\
  \vgoalt{T}(\stateenv) &= \min_{\actionrobot} \costgoal(\stateenvactions) \qquad \actionuser = \policyuser(\stateenv)\\
  &= \min_{\actionrobot} \min_\target \costtarg(\stateenvactions) \\
  &= \min_\target \vtargt{T}(\stateenv)
\end{align*}
Let $\target^* = \argmin \vtarg(\stateenv')$ as before. Now, we show the recursive step:
\begin{align*}
  \qgoalt{t-1}(\stateenvactions) &= \costgoal(\stateenvactions) + \vgoalt{t}(\stateenv')\\
  &= \costtargstar(\stateenvactions) + \min_\target \vtargt{t}(\stateenv') \hspace{1.3em}\\% \target^* = \argmin \vtarg(\stateenv')\\
  &= \costtargstar(\stateenvactions) + \vtargstart{t}(\stateenv')\\% \qquad \target^* = \argmin \vtarg(\stateenv')\\
  &= \qtargstar(\stateenvactions)\\
  \vgoalt{t-1}(\stateenv) &= \min_{\actionrobot} \qgoalt{t-1}(\stateenvactions)  \qquad \actionuser = \policyuser(\stateenv)\\
  &=  \min_{\actionrobot} \costtargstar(\stateenvactions) + \vtargstart{t}(\stateenv')\\% \hspace{1.0em} \target^* = \argmin \vtarg(\stateenv')\\
  & \geq  \min_{\actionrobot} \min_\target \left( \costtarg(\stateenvactions) + \vtargt{t}(\stateenv') \right)\\
  &= \min_\target \vtargt{t-1}(\stateenv)
\end{align*}


Additionally, we know that $\vgoal(\staterobot) \leq \min_{\target} \vtarg(\staterobot)$, since $\vtarg(\staterobot)$ measures the cost-to-go for a specific target, and the total cost-to-go is bounded by this value for a deterministic system. Therefore, $\vgoal(\staterobot) = \min_{\target} \vtarg(\staterobot)$.
\end{proof}


\subsection{\Cref{thm:mingoal_pred}: Decomposing soft value functions}
Here, we show the proof for our theorem that we can decompose the soft value functions over that the targets for deterministic MDPs:
\softvalfundecompose*

\begin{proof}
  %For simplicity of exposition, we assume deterministic transition function $\transitionuser$. Unlike \cref{thm:mingoal_assist}, this proof also holds for stochastic dynamics.
As the cost is additive along the trajectory, we can expand out $\exp(-\costtarg(\traj))$ and marginalize over future inputs to get the probability of an input now:
\begin{align*}
  \policyuser(\actionuser_t,\target| \staterobot_t) &= \frac{ \exp(-\costtarg(\staterobot_t, \actionuser_t)) \int \exp(-\costtarg(\trajatp{\staterobot_{t+1}})) } {\sum_{\target'}\int \exp(-\costtargprime(\trajat{\staterobot_{t}}))} 
\end{align*}
Where the integrals are over all trajectories. By definition, $\exp(-\vtargsoftt{t}(\staterobot_t)) = \int \exp(-\costtarg(\trajat{\staterobot_t}))$:
%$\exp(-\vtargsoftt{t} (\staterobot_t)) = \int \exp(-\costtarg(\trajat{\staterobot_t}))$:
\begin{align*}
  &= \frac{ \exp(-\costtarg(\staterobot_t, \actionuser_t)) \exp(-\vtargsoftt{t+1}(\staterobot_{t+1}))} {\sum_{\target'} \exp(-\vsoft_{\target',t}(\staterobot_{t}) )} \\
  %&= \frac{ \exp(-(\costtarg(\staterobot_t, \actionuser_t) + \vtargsoft(\staterobot_{t+1}))} {\sum_{\target'} \exp(-\vsoft_{\target'}(\staterobot_{t}) )} \\
  &= \frac{ \exp(-\qtargsoftt{t}(\staterobot_t, \actionuser_t))} {\sum_{\target'} \exp(-\vsoft_{\target',t}(\staterobot_{t}) )} 
\end{align*}
Marginalizing out $\target$ and simplifying:
\begin{align*}
  & \policyuser(\actionuser_t| \staterobot_t) = \frac{\sum_\target \exp( -\qtargsoftt{t}(\staterobot_t, \actionuser_t))} {\sum_{\target} \exp(-\vtargsoftt{t}(\staterobot_{t}) )} \\
  &= \exp \left( \log \left( \frac{\sum_\target \exp( -\qtargsoftt{t}(\staterobot_t, \actionuser_t))} {\sum_{\target} \exp(-\vtargsoftt{t}(\staterobot_{t}) )} \right) \right)\\
  %&= \exp \left( \log  \sum_\target \exp( -\qtargsoft(\staterobot_t, \actionuser_t))  - \log \sum_{\target} \exp(-\vtargsoft(\staterobot_{t}) )  \right)\\
  &= \exp \left( \softmin_\target \vtargsoftt{t}(\staterobot_t) - \softmin_\target \qtargsoft{t}(\staterobot_t, \actionuser_t) \right)
\end{align*}
As $\vgoalsoftt{t}$ and $\qgoalsoftt{t}$ are defined such that $\policyuser_t(\actionuser | \staterobot, \goal) = \exp(\vgoalsoftt{t}(\staterobot) -\qgoalsoftt{t}(\staterobot, \actionuser))$, our proof is complete.
\end{proof}


%%%%%%%%%%%%%%%%%
% For Proof
%%%%%%%%%%%%%%%%%
\newcommand{\inreg}[1]{n_{#1}}
\newcommand{\inreghat}[1]{\widehat{n}_{#1}}
\newcommand{\numwithobs}[1]{n^{#1}}
\newcommand{\numwithobshat}[1]{\widehat{n}^{#1}}
\newcommand{\inregandobs}[2]{n_{#1}^{#2}}
\newcommand{\inregandobshat}[2]{\widehat{n}_{#1}^{#2}}
\newcommand{\totalhypoths}{N}
\newcommand{\totalhypothshat}{\widehat{N}}

\newcommand{\edgermind}{l}
\newcommand{\inregrm}{\inreg{\edgermind}}
\newcommand{\subregrm}{\subreg{\edgermind}}


\newcommand{\hyperedgesetk}{\hyperedgeset_\edgermind}
\newcommand{\hyperedgesetnok}{\overline{\hyperedgeset_{\edgermind}}}

\newcommand{\minhyperedgeset}{\hyperedgeset^{\min}}
\newcommand{\minhyperedgesetk}{\minhyperedgeset_\edgermind}
\newcommand{\minhyperedgesetnok}{\overline{\minhyperedgeset_{\edgermind}}}
\newcommand{\nominhyperedgeset}{\hyperedgeset^{\overline{\min}}}
\newcommand{\nominhyperedgesetk}{\nominhyperedgeset_\edgermind}
\newcommand{\nominhyperedgesetnok}{\overline{\nominhyperedgeset_{\edgermind}}}
%\newcommand{\numk}{ {\#k(\hyperedge)} }
\newcommand{\numk}{ {\vert\hyperedge_\edgermind\vert} }

\newcommand*\circled[1]{\tikz[baseline=(char.base)]{\node[shape=circle,draw,inner sep=2pt] (char) {#1};}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Sums and prods used often
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\sumhe}{\sum_{\hyperedge \in \hyperedgeset}}
\newcommand{\sumhek}{\sum_{\hyperedge \in \hyperedgesetk}}
\newcommand{\sumhenok}{\sum_{\hyperedge \in \hyperedgesetnok}}
\newcommand{\summinhenok}{\sum_{\hyperedge \in \minhyperedgesetnok}}
\newcommand{\sumnominhenok}{\sum_{\hyperedge \in \nominhyperedgesetnok}}
\newcommand{\sumallobs}{\sum_{\observationitem \in \observationset}} 
\newcommand{\sumobsnotc}{\sum_{\observationitem \in \observationset \backslash c}} 

\newcommand{\prodedge}{\prod_{i \in \hyperedge}}
\newcommand{\prodedgenok}{\prod_{i \in \hyperedge, i \neq l}}




\end{appendices}
