%Shared autonomy frameworks combine user actions and autonomous assistance to achieve shared goals. 
In shared autonomy, a user and autonomous system work together to achieve shared goals. To collaborate effectively, the autonomous system must know the user's goal. As such, most prior works follow a predict-then-act model, first predicting the user's goal with high confidence, then assisting given that goal. Unfortunately, confidently predicting the user's goal may not be possible until they have nearly achieved it, causing predict-then-act methods to provide little assistance. However, the system can often provide useful assistance even when confidence for any single goal is low (e.g. move towards multiple goals). In this work, we formalize this insight by modelling shared autonomy as a Partially Observable Markov Decision Process (POMDP), providing assistance that minimizes the expected cost-to-go with an unknown goal. As solving this POMDP optimally is intractable, we use hindsight optimization to approximate. We apply our framework to both shared-control teleoperation and human-robot teaming. Compared to predict-then-act methods, our method achieves goals faster, requires less user input, decreases user idling time, and results in fewer user-robot collisions.

%In shared autonomy, a user and autonomous system share goals, making the utility of autonomous actions depend on the user's goal. Often, the system does not know apriori which goal the user will try to achieve. Without this explicit knowledge, most prior works follow a predict-then-act model, first predicting the user's goal with high probability, then assisting given that goal. Unfortunately, we often cannot confidently predict the user's goal until the end of execution, causing predict-then-act models to provide little assistance. However, it is often possible to provide useful assistance even when confidence for any single goal is low (e.g. move towards multiple goals). In this work, we formalize this insight by modelling shared autonomy as a Partially Observable Markov Decision Process (POMDP), providing assistance that minimizes the expected cost-to-go with an unknown goal. As computing the optimal action in this POMDP is intractable, we use hindsight optimization to approximate the solution. We apply our framework to both shared-control teleoperation and human-robot teaming. Compared to predict-then-act methods, we find our method achieves goals faster, with less user input, and decreases the idling time of both the user and robot.

