\subsection{User Prediction}
\label{sec:framework_prediction}

In order to infer the user's goal, we rely on a model $\policyusergoal$ to provide the distribution of user actions at state $\stateenv$ for user goal $\goal$. In principle, we could use any generative predictor for this model, e.g.~\citep{koppula_2013, wang_2013_intentioninference}. We choose to use maximum entropy inverse optimal control (MaxEnt IOC)~\citep{ziebart_2008}, as it explicitly models a user cost function $\costusergoal$. We optimize this directly by defining $\costrobotgoal$ as a function of $\costusergoal$.

In this work, we assume the user does not model robot actions. We use this assumption to define an MDP with states $\stateenv \in \Stateenv$ and user actions $\actionuser \in \Actionuser$ as before, transition $\transitionuser(\stateenv' \given \stateenv, \actionuser) = \transition(\stateenv' \given \stateenv, \actionuser, 0)$, and cost $\costusergoal(\stateenv, \actionuser)$. MaxEnt IOC computes a stochastically optimal policy for this MDP.

The distribution of actions at a single state are computed based on how optimal that action is for minimizing cost over a horizon $T$. Define a sequence of environment states and user inputs as $\traj = \left\{ \stateenv_0, \actionuser_0, \cdots, \stateenv_T, \actionuser_T \right\}$. Note that sequences are not required to be trajectories, in that $\stateenv_{t+1}$ is not necessarily the result of applying $\actionuser_t$ in state $\stateenv_t$. Define the cost of a sequence as the sum of costs of all state-input pairs, $\costgoaluser(\traj) = \sum_{t} \costgoaluser(\stateenv_t, \actionuser_t)$. Let $\trajtot$ be a sequence from time $0$ to $t$, and $\trajat{\stateenv}$ a sequence of from time $t$ to $T$, starting at $\stateenv$.

\citet{ziebart_thesis} shows that minimizing the worst-case predictive loss results in a model where the probability of a sequence decreases exponentially with cost, $p(\traj \given \goal) \propto \exp(-\costgoaluser(\traj))$. Importantly, one can efficiently learn a cost function consistent with this model from demonstrations~\citep{ziebart_2008}.

Computationally, the difficulty in computing $p(\traj \given \goal)$ lies in the normalizing constant $\int_{\traj} \exp(-\costgoaluser(\traj))$, known as the partition function. Evaluating this explicitly would require enumerating all sequences and calculating their cost. However, as the cost of a sequence is the sum of costs of all state-action pairs, dynamic programming can be utilized to compute this through soft-minimum value iteration when the state is discrete~\citep{ziebart_2009,ziebart_2012}:
\begin{align*}
  \qgoalsoftt{t}(\stateenv, \actionuser) &= \costgoaluser(\stateenv, \actionuser) + \expctarg{\vgoalsoftt{t+1}(\stateenv')}\\
  \vgoalsoftt{t}(\stateenv) &= \softmin_{\actionuser} \qgoalsoftt{t}(\stateenv, \actionuser)
\end{align*}
Where $\softmin_{x} f(x) = - \log \int_{x} \exp(-f(x)) dx$ and $\stateenv' \sim \transitionuser(\cdot \given \stateenv, \actionuser)$.

The log partition function is given by the soft value function, $\vgoalsoftt{t}(\stateenv) = - \log \int_{\trajat{\stateenv}} \exp\left(-\costgoaluser(\trajat{\stateenv})\right)$, where the integral is over all sequences starting at $\stateenv$ and time $t$. Furthermore, the probability of a single input at a given environment state is given by $\policyuser_t(\actionuser \given \stateenv, \goal) = \exp(\vgoalsoftt{t}(\stateenv) -\qgoalsoftt{t}(\stateenv, \actionuser))$~\citep{ziebart_2009}.

%make more clear that while our user policy doesn't consider robot assistance, it still affects this positive feedback thing
Many works derive a simplification that enables them to only look at the start and current states, ignoring the inputs in between~\citep{ziebart_2012, dragan_2013_assistive}. Key to this assumption is that $\traj$ corresponds to a trajectory, where applying action $\actionuser_t$ at $\stateenv_t$ results in $\stateenv_{t+1}$. However, if the system is providing assistance, this may not be the case. In particular, if the assistance strategy believes the user's goal is $\goal$, the assistance strategy will select actions to minimize $\costusergoal$. Applying these simplifications will result positive feedback, where the robot makes itself more confident about goals it already believes are likely. In order to avoid this, we ensure that the prediction comes from user inputs only, and not robot actions:
\begin{align*}
  p(\traj \given \goal) &= \prod_t \policyuser_t(\actionuser_{t} \given \stateenv_t, \goal)
\end{align*}
%Where the user applied input $\actionuser_t$ at state $\state_t$.
To compute the probability of a goal given the partial sequence up to $t$, we apply Bayes' rule:
\begin{align*}
  p(\goal \given \trajtot) &= \frac{p(\trajtot \given \goal) p(\goal) }{\sum_{\goal'} p(\trajtot \given \goal') p(\goal')}
\end{align*}
This corresponds to our POMDP observation model, used to transition our belief over goals through $\transitionbelief$.


\subsubsection{Continuous state and action approximation}
Soft-minimum value iteration is able to find the exact partition function when states and actions are discrete. However, it is computationally intractable to apply in continuous state and action spaces. Instead, we follow \citet{dragan_2013_assistive} and use a second order approximation about the optimal trajectory. They show that, assuming a constant Hessian, we can replace the difficult to compute soft-min functions $\vgoalsoft$ and $\qgoalsoft$ with the min value and action-value functions $\vgoaluser$ and $\qgoaluser$:
\begin{align*}
  \policyuser_t(\actionuser \given \stateenv, \goal) &= \exp(\vgoaluser(\stateenv) -\qgoaluser(\stateenv, \actionuser))
\end{align*}
Recent works have explored extensions of the MaxEnt IOC model for continuous spaces~\citep{boularias_2011, levine_2012, finn_2016}. We leave experiments using these methods for learning and prediction as future work.

