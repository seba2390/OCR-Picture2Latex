\subsection{Multi-Target MDP}
\label{sec:framework_multitarget}

There are often multiple ways to achieve a goal. We refer to each of these ways as a \emph{target}. For a single goal (e.g. object to grasp), let the set of targets (e.g. grasp poses) be $\target \in \Target$. We assume each target has a cost function $\costtarg$, from which we compute the corresponding value and action-value functions $\vtarg$ and $\qtarg$, and soft-value functions $\vtargsoft$ and $\qtargsoft$. We derive the quantities for goals, $\vgoal, \qgoal, \vgoalsoft, \qgoalsoft$, as functions of these target functions.

We state the theorems below, and provide proofs in the appendix (\cref{sec:mingoal_thms}).

\subsubsection{Multi-Target Assistance}
\label{sec:framework_multigarget_assistance}
We assign the cost of a state-action pair to be the cost for the target with the minimum cost-to-go after this state:
\begin{align}
  \costgoal(\stateenvactions) &= \costtargstar(\stateenvactions) \quad \target* = \argmin_\target \vtarg(\stateenv') \label{eq:goal_target_cost}
\end{align}
Where $\stateenv'$ is the environment state after actions $\actionuser$ and $\actionrobot$ are applied at state $\stateenv$. For the following theorem, we require that our user policy be deterministic, which we already assume in our approximations when computing robot actions in \cref{sec:framework_hindsight}.
\begin{restatable}{theorem}{valfundecompose}
\label{thm:mingoal_assist}
Let $\vtarg$ be the value function for target $\target$. Define the cost for the goal as in \cref{eq:goal_target_cost}. For an MDP with deterministic transitions, and a deterministic user policy $\policyuser$, the value and action-value functions $\vgoal$ and $\qgoal$ can be computed as:
\begin{align*}
  \qgoal(\stateenvactions) &= \qtargstar(\stateenvactions) \qquad \target^* = \argmin_\target \vtarg(\stateenv') \\
  \vgoal(\stateenv) &= \min_\target \vtarg(\stateenv)
\end{align*}
\end{restatable}

\subsubsection{Multi-Target Prediction}
\label{sec:framework_multigarget_prediction}
Here, we don't assign the goal cost to be the cost of a single target $\costtarg$, but instead use a distribution over targets.%based on the cost-to-go.
\begin{restatable}{theorem}{softvalfundecompose}
  \label{thm:mingoal_pred}
  Define the probability of a trajectory and target as $p(\traj, \target) \propto \exp(-\costtarg(\traj))$. Let $\vtargsoft$ and $\qtargsoft$ be the soft-value functions for target $\target$. For an MDP with deterministic transitions, the soft value functions for goal $\goal$, $\vgoalsoft$ and $\qgoalsoft$, can be computed as:
\begin{align*}
  \vgoalsoft(\stateenv) &= \softmin_\target \vtargsoft(\stateenv)\\
  \qgoalsoft(\stateenv, \actionuser) &= \softmin_\target \qtargsoft(\stateenv, \actionuser)
\end{align*}
\end{restatable}

%
%Marginalizing out g:
%\begin{align*}
%  p(a_t | s) &= \sum_g p(a_t, g | s)\\
%  &= \frac{ \sum_g \exp(-Q_g^{t}(s_t, a_t))} {\sum_{g'}\exp(-V_{g'}^{t}(s_{t}))}
%\end{align*}
%
%We can also write this out as:
%\begin{align*}
%  \exp\left( \log\left( p(a_t | s) \right) \right)&= \exp\left( \log\left(  \frac{ \sum_g \exp(-Q_g^{t}(s_t, a_t))} {\sum_{g'}\exp(-V_{g'}^{t}(s_{t}))}\right) \right)\\
%  &= \exp\left( \log\left(  \sum_g \exp(-Q_g^{t}(s_t, a_t)) \right) - \log\left(\sum_{g'}\exp(-V_{g'}^{t}(s_{t})) \right) \right)\\
%  &= \exp\left( \softmin_g V_{g}^{t}(s_{t}) - \softmin_g Q_g^{t}(s_t, a_t)\right)
%\end{align*}
%

\begin{figure}[t]
\centering
 \begin{subfigure}{0.24\textwidth}
   \centering 
   \includegraphics[width=0.97\textwidth, trim=440 250 500 210, clip=true]{rss_multigoal_1.png}
  \caption{}
 \label{fig:multigoal_1}
 \end{subfigure}
 \begin{subfigure}{0.24\textwidth}
   \centering 
   \includegraphics[width=0.97\textwidth, trim=440 250 500 210, clip=true]{rss_multigoal_2.png}
  \caption{}
 \label{fig:multigoal_2}
 \end{subfigure}
 \begin{subfigure}{0.24\textwidth}
   \centering 
   \includegraphics[width=0.97\textwidth, trim=440 250 500 210, clip=true]{rss_multigoal_3_arb.png}
  \caption{}
 \label{fig:multigoal_3_arb}
 \end{subfigure}
 \begin{subfigure}{0.24\textwidth}
   \centering 
   \includegraphics[width=0.97\textwidth, trim=440 250 500 210, clip=true]{rss_multigoal_3_pred.png}
  \caption{}
 \label{fig:multigoal_3_pred}
 \end{subfigure}
 \caption{Value function for a goal (grasp the ball) decomposed into value functions of targets (grasp poses). (\subref{fig:multigoal_1}, \subref{fig:multigoal_2}) Two targets and their corresponding value function $\vtarg$. In this example, there are 16 targets for the goal. (\subref{fig:multigoal_3_arb}) The value function of a goal $\vgoal$ used for assistance, corresponding to the minimum of all 16 target value functions (\subref{fig:multigoal_3_pred}) The soft-min value function $\vgoalsoft$ used for prediction, corresponding to the soft-min of all 16 target value functions.}
 \label{fig:multigoal}
\end{figure}


