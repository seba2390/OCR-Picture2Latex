% !TEX root =  ../main.tex

\section{Algorithms \& Results}
\label{sec:algorithm}

We start with a high-level description of our general framework, called \selfsparring, which is inspired by the Sparring algorithm from \citet{ailon2014reducing}.  The high-level strategy is to reduce the multi-dueling bandits problem to a multi-armed bandit (MAB) problem that can be solved using a MAB algorithm, and ideally lift existing MAB guarantees to the multi-dueling setting.

Algorithm \ref{alg:ss} describes the \selfsparring approach. 
\selfsparring uses a stochastic MAB algorithm such as Thompson sampling as a subroutine to independently sample the set of $m$ arms, $S_t$ to duel.  The distribution of $S_t$ is generally not degenerate (e.g., all the same arm) unless the algorithm has converged.   In contrast, the Sparring algorithm uses $m$ MAB algorithms to control the choice of the each arm, which essentially reduces the conventional dueling bandits problem to two multi-armed bandit problems ``sparring'' against each other. %Extending Sparring to the multi-dueling setting requires instantiating $m$ MAB algorithms ($m=2$ in the original setting), which can be inefficient.

%which essentially reduces the conventional dueling bandits problem to two multi-armed bandit problems sparring against each other.  In contrast, \selfsparring using a single multi-armed bandit algorithm, but selects  "Self-Sparring." Instead of the more common class of UCB methods, we use Thompson sampling as the SBM, which allows for the sharing of information among players. We make extensive use of the Thompson sampling SBM's modular functions defined in Algorithm~\ref{alg:ts}. We present pseudo-code of \selfsparring in Algorithm~\ref{alg:ss}, and next explain its workings in more detail.

 \begin{algorithm}[tb]
     \caption{\selfsparring}
     \label{alg:ss}
 \begin{algorithmic}[1]
     \INPUT arms $1, \ldots, K$ in space $S$, $m$ the number of arms drawn at each iteration, $\eta$ the learning rate
     \STATE Set prior $D_0$ over $S$
     \FOR{$t=1,2,\ldots $}
       \FOR{$j = 1, \ldots, m$}
         \STATE select arm $i_j(t)$ using $D_{t-1}$ \label{lin:sample}
 	   \ENDFOR
	   \STATE Play $m$ arms $\{i_j(t)\}_j$ and observe $m\times m$ pairwise feedback matrix $R = \{r_{ij} \in \{0,1,\emptyset\}\}_{m \times m}$
       \STATE update $D_{t-1}$ using $R$ to obtain $D_t$
     \ENDFOR
 \end{algorithmic}
 \end{algorithm}

\selfsparring takes as input $S$ the total set of arms, $m$ the number of arms to be dueled at each iteration, and $\eta$ the learning rate for posterior updates. $S$ can be a finite set of $K$ arms for independent setting, or a  continuous action space of arms for kernelized setting. A prior distribution $D_0$ is used to initialize the sampling process over $S$. In the $t$-th iteration, \selfsparring selects $m$ arms by  sampling over the distribution $D_{t-1}$ as shown in line~\ref{lin:sample} of Algorithm~\ref{alg:ss}. The preference feedback can be any type of comparisons ranging from full comparison over the $m$ arms (a full matrix for $R$, aka `all pairs'') to single comparison of one pair (just two valid entries in $R$). The posterior distribution over arms $D_t$ then gets updated by $R$ and the prior $D_{t-1}$.

We specialize \selfsparring in two ways.  The first, \multisparring (Algorithm~\ref{alg:ms}), is the independent-armed version of \selfsparring.  The second,  \kersparring (Algorithm~\ref{alg:ks}), uses Gaussian processes to make predictions about preference function $f$ based on noisy evaluations over comparisons. We emphasize here that \selfsparring is very modular approach, and is thus easy to implement and extend.

\subsection{Independent Arms Case}
\label{sec:multisparring}
\multisparring (Algorithm \ref{alg:ms}) instantiates \selfsparring using Beta-Bernoulli Thompson sampling.  
%The algorithm iteratively samples $m$ arms from the current distribution by Thompson Sampling and receives preference feedback.  \multisparring takes the total set of $K$ arms, $m$ the number of arms chosen at each iteration, and $\eta$ the learning rate for posterior updates as inputs. A prior of $K$ Beta distributions $D_0$ is set over the $K$ arms as initialization of the sampling process. Within the $t$'th iteration, \multisparring treats the multi-dueling problem as a $m$-player game. Each player selects its own arm by Thompson sampling over the Beta distributions $D_{t-1}$. Play the $m$ arms and evaluate the preference over the arms. 
The posterior Beta distributions  $D_t$ over the arms are updated by the preference feedback within the iteration and the prior Beta distributions $D_{t-1}$.

We present a no-regret guarantee of \multisparring in Theorem \ref{thm:ms} below.  We now provide a high-level outline of the main components leading to the result.  Detail proofs are deferred to the supplementary material.

%\multisparring has empirically been shown to enjoy state-of-the-art performance as shown in Section~\ref{sec:experiments}. We also provide theoretical guarantees for the performance of \multisparring below. For clarity of exposition,  The proving methods are briefly shown in the main context and the detailed proofs deferred to Appendix~\ref{app:a}.

Our first step is to prove that \multisparring is asymptotically consistent, i.e., it is guaranteed (with high probability) to converge to the best bandit.  In order to guarantee consistency, we first show that all arms are sampled infinitely often in the limit.

\begin{lemma}
\label{lem:io}
Running \multisparring with infinite time horizon will sample each arm infinitely often.
\end{lemma}

In other words, Thompson sampling style algorithms do not eliminate any arms. 
Lemma~\ref{lem:io} also guarantees concentration of any statistical estimates for each arm as $t\rightarrow \infty$.
We next show that the sampling of \multisparring will concentrate around the optimal arm.  

\begin{theorem}
\label{thm:conv}
Under Approximate Linearity, \multisparring converges to the optimal arm $b_1$ as running time $t\rightarrow \infty$: $\lim_{t\rightarrow \infty} \mathbb{P}(b_t = b_1) = 1$.
\end{theorem}


\begin{algorithm}[tb]
     \caption{\multisparring}
     \label{alg:ms}
 \begin{algorithmic}[1]
 	 \INPUT $m$ the number of arms drawn at each iteration, $\eta$ the learning rate
     \STATE For each arm $i=1,2,\cdots, K$, set $S_i=0$, $F_i=0$.
     \FOR{$t=1,2,\ldots $}
     	\FOR{$j = 1, \ldots, m$}
          \STATE For each arm $i=1,2,\cdots, K$, sample $\theta_{i}$ from $Beta(S_i+1,F_i+1)$
          \STATE Select $i_j(t) := \argmax_i{\theta_i(t)}$
    	\ENDFOR
        \STATE Play $m$ arms $\{i_j(t)\}_j$, observe pairwise feedback matrix $R = \{r_{jk} \in \{0,1,\emptyset\}\}_{m \times m}$
        \FOR{$j,k = 1, \ldots, m$}
          %\STATE $r_i(t) \leftarrow \sum_{j\neq i} r_{ij}$
          \IF{$r_{jk} \neq \emptyset$}
          \STATE $S_j \leftarrow S_j + \eta\cdot r_{jk}$, 
          $F_j \leftarrow F_j + \eta(1 - r_{jk})$
          \ENDIF
        \ENDFOR
     \ENDFOR
 \end{algorithmic}
 \end{algorithm}
 
 
\begin{figure*}[t!]
\centering
\subfloat[$5$ iterations]{\includegraphics[width=0.33\textwidth]{figures/gp5.eps}}
%\qquad
\subfloat[$20$ iterations]{\includegraphics[width=0.33\textwidth]{figures/gp20.eps}}
\subfloat[$100$ iterations]{\includegraphics[width=0.33\textwidth]{figures/gp100.eps}}
\caption{Evolution of a GP preference function in \kersparring; dashed lines correspond to the mean and shaded areas to $\pm 2$ standard deviations. The underlying utility function was sampled randomly from a GP with a squared exponential kernel with lengthscale parameter 0.2, and the resulting preference function is shown in blue. The GP finds the best arm with high confidence.}
\label{fig:ks}
\end{figure*}

Theorem~\ref{thm:conv} implies that \multisparring is asymptotically no-regret. As $t\rightarrow \infty$, the Beta distribution for each arm $i$ is converging to  $P(b_i \succ b_1)$, which implies converging to only choosing the optimal arm.

Most existing dueling bandits algorithm chooses one arm as a ``reference'' arm and the other arm as a competing arm for exploration/exploitation (in the $m=2$ setting). If the distribution over reference arms never changes, then the competing arm is playing against a fixed ``environment'', i.e., it is a standard MAB problem. For general $m$, we can analogously consider choosing only one arm against a fixed distribution over all the other arms.  Using Thompson sampling, the following lemma holds.

\begin{lemma}
	\label{lem:fixed}
Under Approximate Linearity, selecting only one arm  via Thompson sampling against a fixed distribution over the remaining arms leads to optimal regret w.r.t. choosing that arm.
\end{lemma}

%Although the incurred regret for the reference arms are not guaranteed to be optimal in this setting, the incurred regret of the single competing arm is optimal. L
Lemma~\ref{lem:fixed} and Theorem \ref{thm:conv} motivate the idea of analyzing the regret of each individual arm against near-fixed (i.e.,  converging) environments. %which we know happens from Theorem \ref{thm:conv}.  This insight is used to prove our main result.
%We implement it in our proposed algorithms \multisparring, for the multi-dueling bandits problem.

\begin{theorem}
\label{thm:ms}
Under Approximate Linearity, \multisparring converges to the optimal arm with asymptotically optimal no-regret rate of $\mathcal{O}(K\ln(T)/\Delta)$.
\end{theorem}

Theorem \ref{thm:ms} shows an no-regret guarantee for \multisparring that asymptotically matches the optimal rate of $\mathcal{O}(K\ln(T)/\Delta)$ up to constant factors. In other words, once $t>C$ for some problem-dependent constant $C$, the regret of \multisparring matches information-theoretic bounds up to constant factors (see \citet{yue2012k} for lower bound analysis).\footnote{A finite-time guarantee requires more a refined analysis of $C$, and is an interesting direction for future work.}
The proof technique follows two major steps:
(1) prove the convergence of \multisparring as shown in Theorem \ref{thm:conv}; and
(2) bound the expected total regret for sufficiently large $T$.



 \begin{algorithm}[tb]
     \caption{\kersparring}
     \label{alg:ks}
 \begin{algorithmic}[1]
 	 \INPUT Input space $S$, GP prior $(\mu_0, \sigma_0)$, $m$ the number of arms drawn at each iteration   
     \FOR{$t=1,2,\ldots $}
     	\FOR{$j = 1, \ldots, m$}
          \STATE Sample $f_j$ from $(\mu_{t-1}, \sigma_{t-1})$
          \STATE Select $i_j(t) := \argmax_x{f_j(x)}$
    	\ENDFOR
        \STATE Play $m$ arms $\{i_j(t)\}_j$, observe pairwise feedback matrix $R = \{r_{jk} \in \{0,1,\emptyset\}\}_{m \times m}$
        \FOR{$j, k = 1, \ldots, m$}
          \IF{$r_{jk}\neq\emptyset$}
          \STATE apply Bayesian update using $(i_j(t), r_{jk})$ to obtain $(\mu_t, \sigma_t)$
          \ENDIF
        \ENDFOR
     \ENDFOR
 \end{algorithmic}
 \end{algorithm}


\subsection{Dependent Arms Case}
% We use Gaussian processes and kernels (see Section \ref{sec:gp}) to model dependencies between arms. 
% In this scenario, GP predictions are not straightforward (in contrast to the regression case), since the posterior distribution is analytically intractable and approximations at required. 
We use Gaussian processes (see Section \ref{sec:gp}) to model dependencies among arms. 
Applying Gaussian processes is not straightforward, since the underlying utility function is not directly observable or does not exist. 
We instead use Gaussian processes to model a specific the preference function. In Gaussian process notation, the preference function $f(b)$ represents the preference of choosing $b$ over the perfect ``environment'' of competing arms. Like in the independent arms case (Section \ref{sec:multisparring}), the perfect environment corresponds to having all the remaining arms be deterministically selected as the best arm $b_1$, yielding $f(b) = P(b \succ b_1)$. We model $f(b)$ as a sample from a Gaussian process $GP(\mu(b),k(b, b'))$. Note that this setup is analogous to the independent arms case, which uses a Beta prior to estimate the probability of each arm defeating the environment (and converges to competing against the best environment). %Our goal is to optimize the initially unknown $f$ cumulatively. 

%Figure~\ref{fig:ks} illustrates the optimization of preference function with dependent arms. 



Algorithm \ref{alg:ks} describes \kersparring, which instantiates \selfsparring using a Gaussian process Thompson sampling algorithm.
%is shown in \kersparring (Algorithm~\ref{alg:ks}).  
The input space $S$ can be continuous. 
%A GP prior distribution is set over $S$. W
At each iteration $t$, $m$ arms are sampled using the Gaussian process prior $D_{t-1}$. The posterior $D_t$ is then updated by the responses $R$ and the prior. 

Figure~\ref{fig:ks} illustrates the optimization process in a one-dimensional example. The underlying preference function against the best environment is shown in blue. Dashed lines are the mean function of GP. Shaded areas are $\pm 2$ standard deviations regions (high confidence regions). Figures \ref{fig:ks}(a)(b)(c) represent running \kersparring algorithm at 5, 20, and 100 iterations. The GP model can be observed to be converging to the preference function against the best environment.

We conjecture that it is possible to prove no-regret guarantees that scale w.r.t. the dimensionality of the kernel. However, there does not yet exist suitable regret analyses for Gaussian Process Thompson Sampling in the kernelized MAB setting to leverage.


%%%%%%%%%%%%%%%%%%%%
% \begin{theorem}
%   \label{thm:iwss}
% The expectation of total regret of playing \algo in a dueling bandit problem is:
% $$\mathbb{E}[R(T)] = \mathcal{O}\left( \frac{K}{\gamma\Delta \cdot \ln\omega}\right) \ln^2(T)$$
% where $\Delta$ is the preference between the best two arms and $\gamma$ is the parameter of approximate linearity.
% \end{theorem}

% \begin{theorem}
%   \label{thm:iws}
% The expected total regret of playing \sparringiw in a dueling bandit problem is:
% $$\mathbb{E}[R(T)] = \mathcal{O}\left( \frac{K}{\gamma\Delta \cdot \ln\omega}\right) \ln^2(T)$$
% where $\Delta$ is the preference between the best two arms and $\gamma$ is the parameter of approximate linearity.
% \end{theorem}

% \begin{figure}
% \centering
% \includegraphics{figures/biases.eps}
% \vspace{-0.1in}
% \caption{Bias ($\mu^* - \bar{\mu}_t$) vs. iterations}
% \label{fig:bias}
% \end{figure}

% \paragraph{Results for One-Sided Drifting Bandits.}

% It is necessary to analyze the regret of a one-sided dueling bandit facing a drifting ``environment''. From the perspective of each learning agent, this "environment" is not static, but rather drifts with the decision making of the other agent. Furthermore, this "environment" is drifting or converging towards some value. For example, consider the bias of one side of Sparring in the utility-based bandits case, defined as $b_t = \mu^* - \bar{\mu}_t $, where $\bar{\mu}_t$ is the mean bandit at time $t$. We can see from Figure~\ref{fig:bias} that this bias rapidly converges, or drifts, to some fixed value close to zero.


%\begin{lemma}
%	\label{lem:conv}
%\selfsparring is guaranteed to converge to the optimal decision pair with high probability.
%\end{lemma}

% \begin{lemma}
% \label{lem:drift_converge}
% If approximate linearity holds, competing with a drifting but converging distribution of arms guarantees convergence for Thompson Sampling.
% \end{lemma}


%  \begin{algorithm}[tb]
%      \caption{\selfsparring}
%      \label{alg:ss}
%  \begin{algorithmic}[1]
%      \STATE $TS \leftarrow$ new Thompson Sampling SBM
%      \STATE $TS$.\textbf{init}(), $t\leftarrow 0$
%      \WHILE{$t \geq 0$}
%  		\STATE $i_1$ = $TS$.\textbf{queryAction}()
%             	\STATE $i_2$ = $TS$.\textbf{queryAction}()
% 		\STATE play $i_1, i_2$, observe rewards $r_1, r_2$
% 		\STATE $TS$.\textbf{feedback}($i_1$, $r_1$)
%         	\STATE $TS$.\textbf{feedback}($i_2$, $r_2$)
%  	\STATE $t \leftarrow t+1$
%      \ENDWHILE
%  \end{algorithmic}
%  \end{algorithm}
 
%  \begin{algorithm}[tb]
%      \caption{\selfsparring}
%      \label{alg:ss}
%  \begin{algorithmic}[1]
%      \STATE $TS \leftarrow$ new Thompson Sampling SBM
%      \STATE $TS$.\textbf{init}(), $t\leftarrow 0$
%      \FOR{$t=1,2,\ldots $}
%        \FOR{$j = 1, \ldots, d$}
%          \STATE $i_j(t) := TS$.\textbf{queryAction}()
%  	   \ENDFOR
% 	   \STATE Play $d$ arms $\{i_j(t)\}_j$, observe pairwise feedback $\{r_{ij}\}$
%        \FOR{$j,k = 1, \ldots, d$}
%           %\STATE $r_i(t) \leftarrow \sum_{j\neq i} r_{ij}$
%           \IF{$r_{jk}$ nonempty}
%           \STATE $TS$.\textbf{feedback}($i_j$, $r_{jk}$)
%           \STATE $TS$.\textbf{feedback}($i_j$, $1-r_{jk}$)
%           \ENDIF
%         \ENDFOR
%      \ENDFOR
%  \end{algorithmic}
%  \end{algorithm} 

%  \begin{algorithm}[tb]
%      \caption{\multisparring}
%      \label{alg:ms}
%  \begin{algorithmic}[1]
%  	 \INPUT $d$ the number of arms drawn at each iteration, $\eta$ the learning rate
%      	\FUNCTION{\textbf{init}()}
% %    	\STATE Store $D \equiv \{ S_1, \cdots, S_K; F_1, \cdots, F_K\}$
% 		\STATE Store $D \equiv \{ S_1, \cdots, S_K; F_1, \cdots, F_K\} \leftarrow \{\textbf{0}\}$
%     	\ENDFUNCTION   
%     \FUNCTION{\textbf{queryAction}()}
%     	\STATE For each arm $i=1,2,\cdots, K$:\\
% 	sample $\theta_{i}$ from $Beta(S_i+1,F_i+1)$
%         \STATE \textbf{return} $i = \arg\max_i \theta_{i}$
%     \ENDFUNCTION
%     \FUNCTION{\textbf{feedback}($i, r$)}
%         \STATE $S_i \leftarrow S_i + r$
%         \STATE $F_i \leftarrow F_i + 1-r$
%     \ENDFUNCTION
%      \STATE $TS \leftarrow$ new Thompson Sampling SBM
%      \STATE $TS$.\textbf{init}(), $t\leftarrow 0$
%      \FOR{$t=1,2,\ldots $}
%        \FOR{$j = 1, \ldots, d$}
%          \STATE $i_j(t) := TS$.\textbf{queryAction}()
%  	   \ENDFOR
% 	   \STATE Play $d$ arms $\{i_j(t)\}_j$, observe pairwise feedback $\{r_{ij}\}$
%        \FOR{$j,k = 1, \ldots, d$}
%           %\STATE $r_i(t) \leftarrow \sum_{j\neq i} r_{ij}$
%           \IF{$r_{jk}$ nonempty}
%           \STATE $TS$.\textbf{feedback}($i_j$, $r_{jk}$)
%           \STATE $TS$.\textbf{feedback}($i_j$, $1-r_{jk}$)
%           \ENDIF
%         \ENDFOR
%      \ENDFOR
%  \end{algorithmic}
%  \end{algorithm}
 