% !TEX root =  ../main.tex
\section{Introduction}

In many online learning settings, particularly those that involve human feedback, reliable feedback is often limited to pairwise preferences (e.g., ``is A better than B?''). Examples include implicit or subjective feedback for information retrieval and various recommender systems  \citep{chapelle2012large,sui2014clinical}.  This setup motivates the dueling bandits problem \citep{yue2012k}, which formalizes the problem of online regret minimization via preference feedback.

The original dueling bandits setting ignores many real world considerations. For instance, in personalized clinical recommendation settings \citep{sui2014clinical}, it is often more practical for subjects to provide preference feedback on several actions (or treatments) simultaneously rather than just two.  Furthermore, the action space can be very large, possibly infinite, but often has a low-dimensional dependency structure.
%There is more than one way to define feedback, e.g., a full rank comparison or winner takes all. 
%Recently, single-pair dueling methods has been generalized to multi-dueling \citep{brost2016multi} which permits more than two arms to be compared in a single comparison. 

In this paper, we address both of these challenges in a unified framework, which we call \textit{multi-dueling bandits with dependent arms}.  We extend the original dueling bandits problem by simultaneously dueling multiple arms as well as modeling dependencies between arms using a kernel.  Explicitly formalizing these real-world characteristics provides an opportunity to develop principled algorithms that are much more efficient than algorithms designed for the original setting.  For instance, most dueling bandits algorithms suffer regret that scales linearly with the number of arms, which is not practical when the number of arms is very large or infinite.  

For this setting, we propose the \selfsparring algorithm, inspired by the Sparring algorithm from \cite{ailon2014reducing}, which algorithmically reduces the multi-dueling bandits problem into a conventional muilti-armed bandit problem that can be solved using a stochastic bandit algorithm such as Thompson Sampling \citep{chapelle2011empirical,russo2014learning}. Our approach can naturally incorporate dependencies using a Gaussian process prior with an appropriate kernel.

While there have been some prior work on multi-dueling \citep{brost2016multi} and learning from pairwise preferences over kernels \citep{gonzalez2016bayesian}, to the best of our knowledge, our approach is the first to address to both in a unified framework.  We are also the first to provide a regret analysis of the multi-dueling setting.  We further demonstrate the effectiveness of our approach over conventional dueling bandits approaches in a wide range of simulation experiments.

%Prior work on multi-dueling has shown its privilege over the original settings empirically. That algorithm needs to compare the whole set of arms in early iterations which prohibits its usage in practice. It also has no control over the number of arms chosen at each iteration by using simultaneous comparisons of an unrestricted number of arms. We propose a novel extension of dueling bandits - multi-dueling, which naturally extends to a variety of partial ordered feedback structures including comparisons, ranks, and any arbitrary comparative feedback at each iteration. Our experimental results show that the \multisparring algorithm yields significant improvement in performance compared to state of-the-art dueling bandit algorithms.

%Furthermore, we also study the dueling bandits problem where arms may have dependent structures.
%Some problems need to optimize unknown functions where pairwise preferences rather than direct feedback values are available \citep{gonzalez2016bayesian}. Our setup is different to the well studied Bayesian optimization where real valued feedback is available. In our setting, the utility function is implicit and are only accessible by pairwise comparisons. There exist a variety of real wold scenarios in which the underlying utility function needs to be optimized by only preferential returns. 
%It has been demonstrated that humans are better at evaluating differences rather than absolute function values.
 %\kersparring is a new global optimization approach able to find the optimum of a latent function that can only be queried through pairwise comparisons. \kersparring generalizes previous discrete dueling approaches by modeling the probability of the the winner of each duel by means of Gaussian process model with a Bernoulli likelihood. Moreover, \kersparring can deal with multi-dueling inputs in the kernelized input space as \multisparring does for the independent case. We show the benefit of \kersparring in experiments too. 


% One of the best performing algorithms is the \sparring~algorithm  \citep{ailon2014reducing}, which uses two separate multi-armed bandit algorithms to choose the two arms to be compared at each time step, and essentially treats the dueling bandits problem as a competition between two learning agents.
% Operationally, \sparring uses two separate multi-armed bandit algorithms to choose the two arms to be compared at each time step. 
% \selfsparring builds upon \sparring by essentially treating the two multi-armed bandit algorithms as a single bandit algorithm that controls both/multiple players.

%We propose the first algorithmic framework which has the ability to handle all problem settings mentioned above. Our algorithm builds upon the sparring algorithm of \citet{ailon2014reducing}. The original \sparring algorithm has only been rigorously analyzed when using multi-armed bandit algorithms with adversarial guarantees \citep{dudik2015contextual}, such as EXP3 \citep{auer2002nonstochastic}. However, in practice, EXP3 is typically overly conservative and suffers much poorer performance compared to more efficient bandit algorithms designed for stochastic settings, such as UCB1 \citep{auer2002finite} and Thompson Sampling \citep{chapelle2011empirical}.

% The key difficulty in analyzing \sparring using stochastic bandit algorithms is the fact that the ``environment'' that each bandit algorithm is playing against changes over time (due to the other bandit algorithm changing its decisions). 

%\paragraph{Our contributions.}
%In this paper, we show how to view the multi-dueling bandits problem as a multi-player game with stochastic rewards and drifting dynamics. We propose the first algorithmic framework \selfsparring which has the ability to handle dueling/multi-dueling bandits with dependent/independent arms. We provide the first near-optimal no-regret guarantee for \multisparring with independent arms. We also extensively evaluate the performance of \selfsparring on a wide range of simulation settings and real datasets. Experimental results shows \selfsparring achieving state-of-the-art performances under different problem settings. 

