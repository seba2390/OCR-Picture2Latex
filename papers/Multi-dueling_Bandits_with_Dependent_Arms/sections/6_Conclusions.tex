% !TEX root =  ../main.tex
\section{Conclusions}
% We explored how to view the dueling bandits problem as a two-player game with stochastic rewards and slowly drifting dynamics. Through this viewpoint, we proposed a variant of \sparring, called \algo, which enjoys near-optimal no-regret guarantees.  Our proof methodology relies on replacing the slowly drifting distribution of arms with a periodically-updated fixed virtual reference distribution via importance weighting.  Our setting also fully generalizes the linear utility-based dueling bandits setting \citep{ailon2014reducing}.

% We also show empirically that \algo enjoys state-of-the-art performance. We further propose a natural practical variant of \algo, \selfsparring, which significantly outperforms all previous state-of-the-art dueling bandits algorithms. Our regret analysis of slowly drifting stochastic bandit games may also be of independent interest beyond dueling bandits.

% Moving forward, it would be interesting to provide an analysis of \selfsparring, without having to resort to importance weighting, as well as an analysis of \sparring.

We studied multi-dueling bandits with dependent arms.  This setting extends the original dueling bandits setting by dueling multiple arms per iteration rather than just two, and modeling low-dimensional dependencies between arms rather than treat each arm independently.  Both extensions are motivated by practical real-world considerations such as in personalized clinical treatment \citep{sui2014clinical}.
%We explored how to view the dueling bandits problem as a multi-player game with stochastic rewards and drifting dynamics. Through this viewpoint, we proposed \selfsparring, which enjoys near-optimal no-regret guarantees. 
%Furthermore, our setting fully generalizes the linear utility-based dueling bandits setting \citep{ailon2014reducing}.  
We proposed \selfsparring, which is simple and easy to extend, e.g., by integrating with kernels to model dependencies across arms.  
%With kernelized input space, \selfsparring combines the good properties of the dueling bandits setting with the advantages of having a probabilistic model that is able to capture correlations across the arms. The key idea is to learn a preference function in the space of the duels by using a Gaussian process. This allows us to select comparisons efficiently and improve the state-of-the-art performance.
Our experimental results demonstrated significant reduction in regret compared to state-of-the-art dueling bandit algorithms. Generally, relative benefits compared to dueling bandits increased with the number of arms being compared. For \selfsparring, the incurred regret did not increase substantially as the number of arms increased.

Our approach can be extended in several important directions.  Most notably, the theoretical analysis could be improved.  For instance, it would be more desirable to provide explicit finite-time regret guarantees rather than asymptotic ones.  Furthermore, an analysis of the kernelized multi-dueling setting is also lacking.  From a more practical perspective, we assumed that the choice of arms does not impact the feedback mechanism (e.g., all pairs), which is not true in practice (e.g., humans can have a hard time distinguishing very different arms).
