% !TEX root = ../main.tex

\section{Background}

\subsection{Dueling Bandits}
\label{sec:db}
The original dueling bandits problem is a sequential optimization problem with relative feedback. 
Let $\mathcal{B} = \{b_1,\ldots,b_K\}$ be the set of $K$ bandits (or arms). At each iteration, the algorithm duels or compares a single pair of arms $b_i, b_j$ from the set of $K$ arms ($b_i$ and $b_j$ can be identical). The outcome of each duel between $b_i$ and $b_j$ is an independent sample of a Bernoulli random variable. We define the probability that arm $b_i$ beats $b_j$ as: $$P(b_i \succ b_j) = \phi(b_i,b_j) + 1/2,$$
where $\phi(b_i,b_j)\in [-1/2,1/2]$ denotes the stochastic preference between $b_i$ and $b_j$, thus $b_i \succ b_j \Leftrightarrow \phi(b_i,b_j) > 0$.
We assume there is a total ordering, and WLOG that $b_i \succ b_j \Leftrightarrow i < j$.

The setting proceeds in a sequence of iterations or rounds. At each iteration $t$, the decision maker must choose a pair of bandits $b_t^{(1)}$ and $b_t^{(2)}$ to compare, and observes the outcome of that comparison. The quality of the decision making is then quantified using a notion of cumulative regret of $T$ iterations:
\begin{eqnarray}
R_T = \sum_{t=1}^T \left[ \phi(b_1,b_t^{(1)}) + \phi(b_1, b_t^{(2)})\right].\label{eqn:regret}
\end{eqnarray}
When the algorithm has converged to the best arm $b_1$, then it can simply duel $b_1$ against itself, thus incurring no additional regret.
In the recommender systems setting, one can interpret \eqref{eqn:regret} as the how much the user(s) would have preferred the best bandit over the the ones presented by the algorithm.  

%\begin{table}[t]
%\caption{Example $\textsc{Prob}(Row>Col)-1/2$ preferences.}
%\label{tab:pref_matrix}
%\vspace{-0.1in}
%\centering
%\begin{tabular}{|c|cccccc|}
%\hline
% & A & B & C & D & E & F\\
% \hline
% A & 0 & 0.04 & 0.05 & 0.07 & 0.10 & 0.12\\
% B & -0.04 & 0 & 0.04 & 0.06 & 0.08 & 0.10\\
% C & -0.05 & -0.04 & 0 & 0.01 &  0.04& 0.09\\
% D & -0.07 &  -0.06 & -0.01 & 0 & 0.02 & 0.05\\
% E & -0.10 & -0.08 &  -0.04 & -0.02 & 0 & 0.03\\
% F & -0.12 & -0.10 & -0.09 & -0.05 & -0.03 & 0\\
% \hline
% \end{tabular}
%\end{table}

%%In Table \ref{tab:pref_matrix}, if we view each %dimension of the matrix as the behavior of a separate agent, then one can view the preference matrix itself as %the payoff matrix for the row agent. Since there is %exactly one winner and one loser, the payoff matrix for %the column agent is exactly the negation, hence a zero-%sum game.  %Furthermore, Table \ref{tab:pref_matrix} has %a unique (pure) Nash equilibrium of both agents choosing %the Condorcet winner (arm A).

To date, there have been several algorithms proposed for the stochastic dueling bandits problem, including Interleaved Filter \citep{yue2012k}, Beat the Mean \citep{yue2011beat}, SAVAGE \citep{urvoy2013generic}, RUCB \citep{zoghi2014relative,zoghi2015mergerucb}, Sparring \citep{ailon2014reducing,dudik2015contextual}, RMED \citep{komiyama2015regret}, and DTS \citep{wu2016doublets}.  Our proposed approach, \selfsparring, is inspired by Sparring, which along with RUCB-style algorithms are the best performing methods.  In contrast to Sparring, which has no theoretical guarantees, we provide no-regret guarantees for \selfsparring, and demonstrate significantly better performance in the multi-dueling setting. 

Previous work on extending the original dueling bandits setting have been largely restricted to settings that duel a single pair of arms at a time. These include continuous-armed convex dueling bandits \citep{yue2009interactively}, contextual dueling bandits which also introduces the von Neumann winner solution concept \citep{dudik2015contextual}, sparse dueling bandits that focuses on the Borda winner solution concept \citep{jamieson2015sparse}, Copeland dueling bandits that focuses on the Copeland winner solution concept \citep{zoghi2015copeland}, and adversarial dueling bandits \citep{gajane2015relative}.
In contrast, our work studies the complementary directions of how to formalize multiple duels simultaneously, as well as how to reduce the dimensionality of modeling the action space using a low-dimensional similarity kernel. %It would be interesting to study how to extend our analysis to these other settings as well.

Recently, there have been increasing interest in studying personalization settings that simultaneously elicit multiple pairwise comparisons.  Example settings include information retrieval \citep{hofmann2011probabilistic,schuth2014multileaved,schuth2016multileave} and clinical treatment \citep{sui2014clinical}. There have also been some previous work on multi-dueling bandits settings  \citep{brost2016multi,sui2014clinical,schuth2016multileave}, however the previous approaches are limited in their scope and lack rigorous theoretical guarantees.  In contrast, our approach can handle a wide range of multi-dueling mechanisms, has near-optimal regret guarantees, and can be easily composed with kernels to model dependent arms.

%Of these, RUCB and \sparring consistently achieve the best empirical performance \citep{ailon2014reducing}.  Our work builds upon \sparring to arrive at a new algorithm, which we call \selfsparring, that significantly outperforms the existing in empirical evaluations. 
%We also show that \selfsparring enjoys near-optimal no-regret guarantees.
%We also present the first rigorous stochastic no-regret analysis of \sparring, which can be extended to apply to \selfsparring as well.
%\selfsparring relies on the following assumption:

\subsection{Multi-armed Bandits}
Our proposed algorithm,
\selfsparring, utilizes a  multi-armed bandit (MAB) algorithm as a subroutine, and so we provide here a brief formal description of the conventional MAB problem for completeness.
The stochastic MAB problem \citep{robbins52} refers to an iterative decision making problem where the algorithm repeatedly chooses among K actions (or bandits or arms).  In contrast to the dueling bandits setting, where the feedback is relative between two arms, here, we receive an absolute reward that depends on the arm selected. We assume WLOG that every reward is bounded between $[0,1]$.\footnote{So long as the rewards are bounded, one can shift and re-scale them to fit within $[0,1]$.} The goal then is to minimize the cumulative regret compared to the best arm:
\begin{eqnarray}
% R_T^{\mab} = \sum_{t=1}^T \left[\mu_1 - \mu(b_t)\right],\label{eqn:mab_regret}
R_T^{\text{MAB}} = \sum_{t=1}^T \left[\mu^1 - \mu(b_t)\right],
\label{eqn:mab_regret}
\end{eqnarray}
where $b_t$ denotes the arm chosen at time $t$, $\mu(b)$ denotes the expected reward of arm $b$, and $\mu^1 = \argmax_b \mu(b)$.
Popular algorithms for the stochastic setting include UCB (upper confidence bound) algorithms \citep{auer2002finite}, and Thompson Sampling
\citep{chapelle2011empirical,russo2014learning}.  

In the adversarial setting, the rewards are chosen in an adversarial fashion, rather than sampled independently from some underlying distribution.  In this case, regret \eqref{eqn:mab_regret} is rephrased as the difference in the sum of rewards. The predominant algorithm for the adversarial setting is EXP3 \citep{auer2002nonstochastic}.



\subsection{Thompson Sampling}
\label{sec:ts}

The specific MAB algorithm used by our \selfsparring approach is Thompson Sampling.
Thompson Sampling is a stochastic algorithm that maintains a distribution over the arms, and chooses arms by sampling  \citep{chapelle2011empirical}. This distribution is updated using reward feedback.  The entropy of the distribution thus corresponds to  uncertainty regarding which is the best arm, and flatter distributions lead to more exploration.

\newcommand{\muh}{\hat{\mu}}

\begin{algorithm}[t]
    \caption{Thompson Sampling for Bernoulli Bandits}
    \label{alg:ts}
{
\begin{algorithmic}[1]
	\STATE For each arm $i=1,2,\cdots, K$, set $S_i=0$, $F_i=0$.
    \FOR{$t=1,2,\ldots $}
    	\STATE For each arm $i=1,2,\cdots, K$, sample $\theta_{i}$ from $Beta(S_i+1,F_i+1)$
        \STATE Play arm $i(t) := \argmax_i{\theta_i(t)}$, observe reward $r_t$
        \STATE $S_i \leftarrow S_i + r_t$, $F_i \leftarrow F_i + 1 - r_t$
    \ENDFOR
\end{algorithmic}
}
\end{algorithm}

%Our algorithms rely on Thompson Sampling as a subroutine; hence we define a Thompson Sampling Singleton Bandit Machine (SBM) in \algoref{alg:basic}. 
Consider the Bernoulli bandits setting where observed rewards are either 1 (win) or 0 (loss).  Let $S_i$ and $F_i$ denote the historical number of wins and losses of arm $i$, and let $D_t$ denote the set of all parameters at round $t$:
$$D_t= \{ S_1, \cdots, S_K; F_1, \cdots, F_K\}_t.$$
For brevity, we often represent $D_t$ by $D$, since only the current iteration matters at run-time. The sampling process of Beta-Bernoulli Thompson Sampling given $D$ is:
\begin{itemize}
\vspace{-0.1in}
\item For each arm $i$, sample $\theta_i \sim Beta(S_i+1,F_i+1)$.
\vspace{-0.1in}
\item Choose the arm with maximal $\theta_i$.
\end{itemize}
In other words, we model the average utility of each arm using a Beta prior, and rewards for arm $i$ as Bernoulli distributed according to latent mean utility $\theta_i$.  As we observe more rewards, we can compute the posterior, which is also Beta distributed by conjugation between Beta and Bernoulli.  The sampling process above can be shown to be sampling for the following distribution:
\begin{eqnarray}
P(i|D) = P(i =\argmax_b \theta_b|D).
\label{eqn:ts}
\end{eqnarray}
Thus, any arm $i$ is chosen with probability that it has  maximal reward under the Beta posterior. Algorithm~\ref{alg:ts} describes the Beta-Bernoulli Thompson Sampling algorithm, which we use as a subroutine for our approach.
Thompson Sampling enjoys near-optimal regret guarantees in the stochastic MAB setting, as given by the lemma below (which is a direct consequence of main theorems in \citet{agrawal2012,kaufmann2012}).

% Algorithm~\ref{alg:basic} describes the relevant components of the version of Thompson Sampling we use for \selfsparring, and Algorithm~\ref{alg:ts} shows how to use these subroutines for the conventional MAB problem.
% For the basic MAB problem, the components used are \textbf{init()}, \textbf{queryAction()}, and \textbf{feedback($i$,$r$)}.

% \textbf{init}(): initializes the observation set $D$ to be empty. %The SBM stores $D$ and updates it as it receives feedback.
 
% \textbf{queryAction}(): samples from the $K$ arms according to the Beta posterior induced by $D$.

% \textbf{feedback}($i, r$): takes arm $i$, reward $r$, and updates $D$. In the standard MAB setting, we simply increment $S_i$ by 1 if $r=1$ and $F_i$ by 1 if $r=0$.

%We revisit the Thompson sampling method for Bernoulli bandits as shown in 
 
%(Descriptions for the modular functions are at the end of this section.) The algorithm for Bernoulli bandits maintains Bayesian priors on the Bernoulli means $\mu_i$’s. Beta distribution is the conjugate prior for Bernoulli rewards. It forms a family of continuous probability distributions on the interval $(0, 1)$. The probability density function of the beta distributions, $Beta(\alpha, \beta)$ with parameters $\alpha>0$, $\beta>0$, is given by $f(x;\alpha, \beta) =\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(、\beta)}x^{\alpha-1}(1-x)^{\beta-1}$. The mean of $Beta(\alpha, \beta)$ is $\frac{\alpha}{\alpha+\beta}$. Larger $\alpha$ and $\beta$ leads to tighter concentration of $Beta(\alpha, \beta)$ around the mean. If the prior is a $Beta(\alpha, \beta)$ distribution, then after observing a Bernoulli trial, the posterior distribution is either $Beta(\alpha+1, \beta)$ or $Beta(\alpha, \beta+1)$, depending on whether the trial resulted in a success or failure, respectively. The Thompson Sampling algorithm initially assumes arm $i$ to have prior $Beta(1,1)$ on $\mu_i$, which is natural because $Beta(1,1)$ is the uniform distribution on $(0, 1)$. At time $t$, having observed $S_{i(t)}$ successes (reward $= 1$) and $F_{i(t)}$ failures (reward = 0) in $k_{i(t)} = S_{i(t)} + F_{i(t)}$ plays of arm $i$, the algorithm updates the distribution on $\mu_i$ as $Beta(S_{i(t)}+1,F_{i(t)}+1)$. The algorithm then samples from these posterior distributions of the $\mu_i$’s, and plays an arm according to the probability of its mean being the largest. 


\begin{lemma}
	\label{lem:ts}
For the K-armed stochastic MAB problem, Thompson Sampling has expected regret:
$\mathbb{E}[R_T^{\text{MAB}}] = \mathcal{O}\left(\frac{K}{\Delta}\ln T \right)$, where $\Delta$ is the difference between expected rewards of the best two arms.
\end{lemma}


% The algorithms we study in this paper can, in principle, use any of the previously mentioned MAB algorithms as sub. 
% Previous work showed that \sparring enjoys state-of-the-art empirical performance using UCB1 \citep{ailon2014reducing}, and near-optimal no-regret guarantees using EXP3 \citep{dudik2015contextual} (but with much worse empirical performance).


\subsection{Gaussian Processes \& Kernels}
\label{sec:gp}

 Normally, when one observes measurements about one arm (in both dueling bandits and conventional multi-armed bandits), one cannot use that measurement to infer anything about other arms -- i.e., the arms are independent.  This limitation necessarily implies that regret scales linearly w.r.t. the number of arms $K$, since each arm must be explored at least once to collect at least one measurement about it.  We will use Gaussian processes and kernels to model dependencies between arms.

For simplicity, we present Gaussian processes in the context of multi-armed bandits.  We will describe how to apply them to multi-dueling bandits in Section \ref{sec:problem}
A Gaussian process (GP) is a probability measure over functions such that any linear restriction is multivariate Gaussian. A GP is fully determined by its mean and a positive definite covariance operator, also known as a kernel. 
A $GP(\mu(b),k(b, b'))$ is a probability distribution across a class of ``smooth'' functions, which is parameterized by a kernel function $k(b, b')$ that characterizes the smoothness of $f$.  One can think of $f$ has corresponding to the reward function in the standard MAB setting. 
%In standard regression cases with Gaussian likelihoods, closed forms for the posterior mean and variance are available.
%In the preference learning, like the one we face here, the basic idea behind Gaussian process modeling is to place a GP prior over some latent preference function $f$ that captures the underlying utility of playing each arm in the input space. 

We assume WLOG~that $\mu(b)=0$, and that our observations are perturbed by i.i.d.~Gaussian noise, i.e., for samples at points $A_T=[b_1 \dots b_T]$, we have $y_t = f(b_t) + n_t$ where $n_t \sim T(0, \sigma^2)$ (e will relax this later). The posterior over $f$ is then also Gaussian with mean $\mu_T( b)$, covariance $k_T(b, b)$ and variance $\sigma_T^2(b, b')$ that satisfy:
\begin{align*}
\mu_T(b) &= k_T(b)^T(\K_T + \sigma^2I)^{-1}y_T\\
k_T(b, b') &= k(b, b') - k_T(x)^T(\K_T + \sigma^2I)^{-1}k_T(b')\\
\sigma_T^2(b) &= k_T(b, b),
\end{align*}
where $k_T(b) = [k(b_1, b) \dots k(b_T, b)]^T$ and $\K_T$ is the positive definite kernel matrix $[k(x, x')]_bx, b' \in A_T]$.

Posterior inference updates the mean reward estimates for all the arms that share dependencies (as specified by the kernel) with the arms selected for measurement.  Thus one can show that MAB algorithms using Gaussian processes have regret that scale linearly w.r.t. the dimensionality of the kernel rather than the number of arms (which can now be infinite) \citep{srinivas10}.



