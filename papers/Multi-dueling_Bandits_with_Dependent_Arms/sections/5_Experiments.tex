
% !TEX root = ../main.tex

\begin{table*}[t]
\centering
\begin{small}
\begin{tabular}{|l|l|}
\hline
Name  & Distribution of Utilities of arms \\ \hline
\hline
1good & 1 arm with utility 0.8, 15 arms with utility 0.2  \\ \hline
%2good & 1 arm with utility 0.8, 1 arms with utility 0.7, 14 arms with utility 0.2  \\ \hline
%6good & 1 arm with utility 0.8, 5 arms with utility 0.7, 10 arms with utility 0.2   \\ \hline
arith & 1 arm with utility 0.8, 15 arms forming an arithmetic sequence between 0.7 and 0.2 \\ \hline
%geom  & 1 arm with utility 0.8, 15 arms forming a geometric sequence between 0.7 and 0.2 \\ \hline
\end{tabular}
\end{small}
\caption{16-arm synthetic datasets used for experiments.}
\label{tab:arms}
\vspace{-0.1in}
\end{table*}


\section{Experiments}\label{sec:experiments}

%In this section, we extensively evaluate the performances of \selfsparring algorithms on both synthetic and real-world datasets.
\subsection{Simulation Settings \& Datasets}
\label{sec:data}

\textbf{Synthetic Functions.} We evaluated on a range of 16-arm synthetic settings derived from  the utility-based dueling bandits setting of \citet{ailon2014reducing}. For the multi-dueling setting, we used the following preference functions:
\vspace{-0.1in}
\begin{table}[H]
\centering
\begin{tabular}{lc}
linear:  & $\phi(x, y) - 1/2 = (1+x-y)/2$          \\
%natural: & $\phi(x, y) - 1/2 = x/(x+y)$            \\
logit:   & $\phi(x, y) - 1/2 = (1+\exp{(y-x)})^{-1}$
\end{tabular}
\end{table}
\vspace{-0.2in}
% \vspace{-0.1in}
and the utility functions shown in Table \ref{tab:arms} (generalized from those in \citet{ailon2014reducing}). 
Note that although these preference functions do not satisfy approximate linearity over their entire domains, they do for the utility samples (over the a finite subset of arms).


\textbf{MSLR Dataset.}
%We also tested \multisparring against Multi-Dueling Bandit Algorithm on the 
Following the evaluation setup of \citet{brost2016multi}, we also  used the Microsoft Learning to Rank (MSLR) WEB30k dataset, which consists of over 3 million query-document pairs labeled with relevance scores \citep{liu2007letor}. Each pair is  scored along 136 features, which can be treated as rankers (arms). For any subset of arms, we can estimate a preference matrix using the expected probability over the entire dataset of one arm beating another using top-10 interleaving and a perfect-click model. 
%Similar to \citet{brost2016multi}, 
We simulate user feedback by using team-draft multileaving \citep{schuth2014multileaved}.

%\paragraph{Vanilla Dueling Bandits.}
\subsection{Vanilla Dueling Bandits Experiments}
%We evaluated fifteen 16-arm synthetic scenarios, generated from the three preference functions used in \citet{ailon2014reducing}:
We first compare against the vanilla dueling bandits setting of dueling a single pair of arms at a time.  These experiments are included as a sanity check to confirm that \selfsparring (with $m=2$) is a competitive algorithm in the original dueling bandits setting, and are not the main focus of our empirical analysis.


%\subsection{Vanilla Dueling Bandits Experiments}
We empirically evaluate against a range of conventional dueling bandit algorithms, including:
\begin{itemize}
\vspace{-0.1in}
\item \textbf{Interleaved Filter (IF)} \citep{yue2012k}
\vspace{-.1in}
\item \textbf{Beat the Mean (BTM)} \citep{yue2011beat}
\vspace{-.1in}
\item \textbf{RUCB} \citep{zoghi2014relative}
\vspace{-.1in}
\item \textbf{MergeRUCB} \citep{zoghi2015mergerucb}
\vspace{-.1in}
\item \textbf{Sparring + UCB1} \citep{ailon2014reducing}
\vspace{-.1in}
\item \textbf{Sparring + EXP3} \citep{dudik2015contextual}
\vspace{-.1in}
\item \textbf{RMED1} \citep{komiyama2015regret}
\vspace{-.1in}
\item \textbf{Double Thompson Sampling} \citep{wu2016doublets}
\end{itemize}
For Double Thompson Sampling and \multisparring, we set the learning rates to be 2.5 and 3.5 as optimized over a separate dataset of uniformly sampled utility functions. We use $\alpha=0.51$ for RUCB/MergeRUCB, $\gamma=1$ for BTM, and $f(K) = 0.3K^{1.01}$ for RMED1.


\textbf{Results.}
For each scenario, we run each algorithm 100 times for 20000 iterations. For brevity, we show in Figure \ref{fig:best9} the average regret of one synthetic simulation along with shaded one standard-deviation areas.  We observe that \selfsparring is competitive with the best performing methods in the original dueling bandits setting.  More complete experiments that replicate \citet{ailon2014reducing} are provided in the supplementary material, and demonstrate the consistency of this result.


%\paragraph{\multisparring vs. Double Thompson Sampling}
%As seen in figures \ref{fig:best9} and \ref{fig:grid}, 
Double Thompson Sampling (DTS) is the best performing approach in Figure \ref{fig:best9}, which is a fairly consistent result in the extended results in the supplementary material.
%Double Thompson Sampling performs slightly better than \multisparring in most settings; 
However, given their high variances they are essentially comparable w.r.t. all other algorithms. 
%\ref{fig:best9}. 
Furthermore, \multisparring has the advantage of being easily extensible to the more realistic multi-dueling and kernelized settings, which is not true of DTS.


% \begin{figure}[t!]
% \centering
% \includegraphics[]{figures/stdcomparefinal.eps}
% \vspace{-0.1in}
% \caption{Expectations of cumulative regrets of top 4 algorithms. Dashed lines represent one standard deviation error curves.}
% \label{fig:best}
% \end{figure}

% \begin{figure}[H]
% \centering
% \includegraphics[scale=0.5]{figures/top4.pdf}
% \vspace{-0.1in}
% \caption{Average regret for top four algorithms on logit/arith case. Shaded regions correspond to one standard deviation.}
% \label{fig:best}
% \end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=0.5\textwidth]{figures/top9.eps}
\vspace{-0.1in}
\caption{Vanilla dueling bandits setting.  Average regret for top nine algorithms on logit/arith. Shaded regions correspond to one standard deviation.}
\label{fig:best9}
\end{figure}

\subsection{Multi-Dueling Bandits Experiments}

We next evaluate the multi-dueling setting with independent arms.  We compare against the main existing approaches that are applicable to the multi-dueling setting, including the MDB algorithm \citep{brost2016multi}, and the multi-dueling extension of Sparring, which we refer to as MultiSparring \citep{ailon2014reducing}.  
Following \cite{brost2016multi}, we use $\alpha=0.5$ and $\beta=1.5$ for the MDB algorithm. For \multisparring, we set learning rate to be the default 1.  Note that the vast majority dueling bandits algorithms are not easily applicable to the multi-dueling setting.  For instance, RUCB-style algorithms treat the two arms asymmetrically, which is not easily generalized to multi-dueling.

\textbf{Results on Synthetic Experiments.}
We test $m=4$ on the linear 1good and arith datasets in Figure \ref{fig:multi1good} and Figure \ref{fig:multiarith}, respectively. We observe that \multisparring significantly outperforms competing approaches.

\textbf{Results on MSLR Dataset.} Following the simulation setting of \cite{brost2016multi} on the MSLR dataset (see Section \ref{sec:data}), we compared against the MDB algorithm over the same collection of 50 randomly sampled 16-arm subsets. We ensured that each 16-arm subset had a Condorcet winner; in general it is likely for any random subset of arms in the MSLR dataset to have a Condorcet winner \citep{zoghi2015copeland}.  
Figure \ref{fig:multimslr} shows the results, where we again see that \multisparring enjoys significantly better performance.  %these results suggest the that \multisparring is an effective approach for the multi-dueling setting.  %Of course, the most realistic setting is the k


\begin{figure}[t]
\centering
\includegraphics[width=0.4\textwidth]{figures/multi1goodn.eps}
\vspace{-0.1in}
\caption{Multi-dueling regret for linear/1good setting}
\label{fig:multi1good}
\vspace{-0.1in}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=0.4\textwidth]{figures/multiarithn.eps}
\vspace{-0.1in}
\caption{Multi-dueling regret for linear/arith setting}
\label{fig:multiarith}
\vspace{-0.05in}
\end{figure}



% We used the perfect click model for both feedback and preference matrix estimation, the latter of which we took to be the expected probability over the entire dataset of arm $i$ beating arm $j$ using top-5 interleaving. For feedback, we used team-draft multileaving to order documents and sample rewards.

\begin{figure}[t]
\centering
\includegraphics[width=0.42\textwidth]{figures/multimslr.eps}
\vspace{-0.1in}
\caption{Multi-dueling regret for MSLR-30K experiments}
\vspace{-0.14in}
\label{fig:multimslr}
\end{figure}

\begin{figure}[b]
\centering
\includegraphics[width=0.38\textwidth]{figures/gpsynthn.eps}
\vspace{-0.15in}
\caption{2-dueling regret for kernelized setting with synthetic preferences}
\vspace{-0.05in}
\label{fig:gpsynthetic}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=0.38\textwidth]{figures/gpforresterfull.eps}
\vspace{-0.1in}
\caption{2-dueling regret for kernelized setting with Forrester objective function}
\vspace{-0.05in}
\label{fig:gpforrester}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=0.38\textwidth]{figures/gpsixhumpn.eps}
\vspace{-0.1in}
\caption{2-dueling regret for kernelized setting with Six-Hump Camel objective function}
\vspace{-0.1in}
\label{fig:gpsixhump}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=0.39\textwidth]{figures/multigpforrester4n.eps}
\vspace{-0.1in}
\caption{Multi-dueling regret for kernelized setting with Forrester objective function}
\label{fig:multiforrester}
\vspace{-0.05in}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=0.38\textwidth]{figures/multigpsixhump4n.eps}
\vspace{-0.1in}
\caption{Multi-dueling regret for kernelized setting with Six-Hump Camel objective function}
\label{fig:multisixhump}
\vspace{-0.05in}
\end{figure}


%\subsection{Multi-dueling Experiments}
%\paragraph{Results and Analysis.}


%\paragraph{Multi-Dueling Bandits}
%\multisparring in general outperforms the Multi-Dueling Bandit algorithm, since the latter incurs a substantial amount of regret in the early phase. Our algorithm achieves the state-of-the-art on both synthetic and real-world datasets.

%\subsection{Dependent Arms Experiments}


\subsection{Kernelized (Multi-)Dueling Experiments}

We finally evaluate the kernelized setting for both the 2-dueling and the multi-dueling case.
We evaluate \kersparring against BOPPER \citep{gonzalez2016bayesian} and Sparring \citep{ailon2014reducing} with GP-UCB \citep{srinivas10}. BOPPER is a Bayesian optimization method can be applied to kernelized 2-dueling setting (but not multi-dueling).  Sparring with GP-UCB, which refer to as GP-Sparring, is essentially a variant of our \kersparring approach but maintains a $m$ GP-UCB bandit algorithms (one controlling each choice of arm to be dueled), rather than just a single one.

\kersparring and GP-Sparring use GPs that model the preference function, i.e. are one-sided, whereas BOPPER uses a GP to model the entire preference matrix. Following \citet{srinivas10}, we use a squared exponential kernel with lengthscale parameter 0.2 for both GP-Sparring and \kersparring, and use a squared exponential kernel with parameter 1 for BOPPER. We initialize all GPs with a zero-mean prior, and use sampling noise variance $\sigma^2=0.025$. For GP-Sparring, we use the scaled-down version of $\beta_t$ as suggested by \citet{srinivas10}.
 
We use the Forrester and Six-Hump Camel functions as utility functions on $[0,1]$ and $[0,1]^2$, respectively, as in \citet{gonzalez2016bayesian}. Similarly, we use the same uniform discretizations of 30 and 64 points for the Forrester and Six-Hump Camel settings respectively, and use the logit link function to generate preferences.

Since the BOPPER algorithm is computationally expensive, we only include it in the Forrester setting, and run each algorithm 20 times for 100 iterations. In the Six-Hump Camel setting, we run \kersparring and GP-Sparring for 500 iterations 100 times each. Results are presented in Figures \ref{fig:gpforrester} and \ref{fig:gpsixhump}, where we observe much better performance from \kersparring against both BOPPER and GP-Sparring.

%\kersparring outperforms both Sparring with GP-UCB and BOPPER in nearly all settings and is equal to Sparring in only the synthetic case. In the multi-dueling kernelized case, \kersparring outperforms Sparring significantly. 

In the kernelized multi-dueling setting, we compare against GP-Sparring.  We run each algorithm for 100 iterations 50 times on the Forrester and Six-Hump Camel functions, and plot their regrets in Figures \ref{fig:multiforrester} and \ref{fig:multisixhump} respectively. We use $m=4$ for both algorithms, and the same discretization as in the standard dueling case.  We again observe significant performance gains of our \kersparring approach.

% From Figure~\ref{fig:grid}, we see that \selfsparring is consistently among the best algorithms, and outperform previous state-of-the-art Sparring UCB1 in every scenario. Only RUCB and DTS are competitive with \selfsparring.

% We also note in Figure~\ref{fig:omega} that the performance of \algo increases as $\omega$ decreases. In fact, as $\omega$ approaches 1, \algo becomes \selfsparring with minimal recalibration and importance weighting at every iteration. This comes at the expense of theoretical guarantees, since our regret bound in Theorem~\ref{thm:iwss} blows up as $\omega$ approaches 1. Furthermore, \algo suffers from high variance due to the importance-weighting scheme, as seen in Figure~\ref{fig:best}. However, we also observe that \selfsparring has variance comparable to that of Sparring UCB and RUCB, so due to its lower regret mean and variance we recommend using \selfsparring in practice.

