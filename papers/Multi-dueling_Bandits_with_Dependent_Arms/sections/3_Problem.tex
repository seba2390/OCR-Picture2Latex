% !TEX root = ../main.tex


\section{Multi-dueling Bandits}
\label{sec:problem}

%\subsection{Multi-dueling Bandits}
We now formalize the multi-dueling bandits problem. We inherit all notation from original dueling bandits setting (Section \ref{sec:db}).
The key difference is that the algorithm now selects a (multi-)set $S_t$ of arms at each iteration $t$, and observes outcomes of duels between some pairs of arms in $S_t$. For example, in information retrieval this can be implemented via multi-leaving \citep{schuth2014multileaved} the ranked lists of the subset, $S_t$, of rankers and then inferring the relative quality of the lists (and the corresponding rankers) from user feedback. 

In general, we assume the number of arms being dueled at each iteration is some fixed constant $m = |S_t|$.  When $m=2$, the problem reduces to the original dueling bandits setting. Extending the regret formulation from the original setting \eqref{eqn:regret}, we can write the regret as:
\begin{eqnarray}
R_T = \sum_{t=1}^T \sum_{b\in S_t} \phi(b_1,b).\label{eqn:regret2}
\end{eqnarray}

%Let $P = [p_{ij}]$ denote a matrix of probabilities that arm $i$ wins in a pairwise comparison with arm $j$. 
%As we have already mentioned, in pairwise comparisons the best arm is not always well-defined (recall the example with A being better than B, B better than C, and C better than A). We follow the assumption in most dueling bandit literature and assume that there exists a Condorcet winner, which is a unique arm $b_1$ satisfying $p_{1j} > 1/2$ for all $j \neq 1$. That is, the Condorcet winner $1$ is pairwise better than any other arm $j$. The quality of all arms is then defined by their regret, $r(j) = p_{1j} - 1/2$, which is a shifted probability of losing to the best arm (this definition also coincides with dueling bandits). Smaller regret corresponds to better quality and the regret of playing the best arm is zero.

The goal then is to select subsets of arms ${S_t}$ so that the cumulative regret \eqref{eqn:regret2} is minimized. Intuitively, all arms have to be selected a small number of times in order to be explored, but the goal of the algorithm is to minimize the number of times when suboptimal arms are selected.
When the algorithm has converged to the best arm $b_1$, then it can simply choose $S_t$ to only contain $b_1$, thus incurring no additional regret.
%On average, simultaneous exploration has lower regret than sequential comparison.

Our setting differs from \citet{brost2016multi} in two ways.  First, we play a fixed, rather than variable, number of arms at each iteration. Furthermore, we focus on total regret, rather than the instantaneous average regret in a single iteration; in many applications (e.g., \citet{sui2014clinical}), playing each arm incurs its own regret .

\textbf{Feedback Mechanisms.} Simultaneously dueling multiple arms opens up multiple options for collecting feedback. For example, in some applications it may be viable to collect all pairwise feedback for all chosen arms  $S_t$.  In other applications, it is more realistic to only observe the ``winner'' of $S_t$, in which we observe feedback that one $b\in S_t$ wins against all other arms in $S_t$, but nothing about pairwise preferences between the other arms.  %More generally, we assume that the feedback mechanism returns some 



%\textcolor{red}{\textbf{(Yisong: this paragraph needs to be re-written.  It needs crisp details on exactly what the $f$ is modeling.)}}
%\ys{rewrote:v1}


\textbf{Approximate Linearity.}
One assumption that we leverage in developing our approach is \textit{approximate linearity}, which fully generalizes the linear utility-based dueling bandits setting studied in \citet{ailon2014reducing}.
For any triplet of bandits $b_i \succ b_j \succ b_k$ and some constant $\gamma > 0$: 
\begin{eqnarray}
\phi(b_i,b_k) - \phi(b_j,b_k) \geq \gamma\phi(b_i,b_j).
\label{eqn:al}
\end{eqnarray}

%\textbf{Illustration of a special case of Approximate Linearity}:
%The Approximate Linearity is as simple as shown in \eqnref{eqn:al}. 
To understand Approximate Linearity, consider the special case when the preference function follows the form $\phi(b_i, b_j) = \Phi(u_i - u_j)$, where $u_i$ is a bounded utility measure of $b_i$. Approximate linearity of $\phi(\cdot, \cdot)$ is equivalent to having $\Phi(\cdot)$ be not far from some linear function on its bounded support (see Figure \ref{fig:al}), and is satisfied by any continuous monotonic increasing function.  When $\Phi$ is linear, then our setting reduces to the utility-based dueling bandits setting of \citet{ailon2014reducing}.\footnote{Compared to the assumptions of \cite{yue2012k}, Approximate Linearity is a stricter requirement than strong stochastic transitivity, and is a complementary requirement to stochastic triangle inequality. In particular, stochastic triangle inequality requires that the curve in Figure \ref{fig:al} exhibits diminishing returns in the top-right quadrant (i.e., is sub-linear), whereas Approximate Linearity requires that the curve be not too far from linear.}



\begin{figure}[t]
%\vskip 0.2in
\centering
\includegraphics[width=0.92\columnwidth]{figures/approxlin.eps}
\vspace{-0.2in}
\caption{Illustration of Approximate Linearity. The curve represents $\Phi(\cdot)$ with support on $[-1, 1]$.  Monotonicity guarantees Approximate Linearity for some $\gamma$.}
\label{fig:al}
\vskip -0.2in
\end{figure}


%\begin{table}[t]
%\caption{$\textsc{Prob}(Row>Col)-1/2$ preference matrix from \citept{yue2011beat}.}
%\label{tab:pref_matrix}
%\centering
%\begin{tabular}{|c|cccccc|}
%\hline
% & A & B & C & D & E & F\\
% \hline
% A & 0 & 0.05 & 0.05 & 0.04 & 0.11 & 0.11\\
% B & -0.05 & 0 & 0.05 & 0.06 & 0.08 & 0.10\\
% C & -0.05 & -0.05 & 0 & 0.04 &  0.01& 0.06\\
% D & -0.04 &  -0.04 & -0.04 & 0 & 0.04 & 0.00\\
% E & -0.11 & -0.08 &  -0.01 & -0.04 & 0 & 0.01\\
% F & -0.11 & -0.10 & -0.06 & -0.00 & -0.01 & 0\\
% \hline
% \end{tabular}
%\end{table}

% \newcommand{\self}{\texttt{self}}

%When Table \ref{tab:pref_matrix} is unknown a priori, one can formulate the one-sided online learning problem for each agent as a MAB problem, with regret defined relative to the reward obtained from the Nash equilibrum solution. It is known that no-regret learning agents in a two-player zero-sum game are guaranteed to converge to a Nash equilibrium \citep{friedman1998learning}. 
%Furthermore, in the case where there is a Condorcet winner (as is the case in our setting), the Nash equilibrium is a pure Nash, i.e., a deterministic strategy (e.g., arm A in Table \ref{tab:pref_matrix}).  %Thus, one can directly relate no-regret learning for the dueling bandits problem to efficient convergence to the Nash equilibrium in the corresponding two-player game.  
% Indeed, the analysis of \sparring using EXP3 \citep{dudik2015contextual} almost exactly follows the  analysis of how no-regret online learning converges to a Nash equilibrium in zero-sum games.

%The majority of existing dueling bandits algorithms have different strategies for the picking of two arms at each round. Stochastic two-player games are widely used for modeling discrete systems operating in an unknown environment. The dueling bandits problem can also be seen as two players playing with antagonistic objectives, where each player aims at maximizing the probability of wins. The main difficulty in analyzing such algorithms is the fact that the ``environment'' from the perspective of each learning agent is not static, but rather drifts with the decision making of the other agent. We will show how to incorporate Thompson Sampling with single posterior distribution to guarantee convergence and to prove no-regret guarantees.

%Since we assume the total order $b_1 \succ \cdots \succ b_K$ exists, the decision pair $(b_1, b_1)$ is the only Nash equilibrium of the game. 

% \begin{algorithm}[tb]
%     \caption{Thompson Sampling Subroutines}
%     \label{alg:basic}
% {
% \begin{algorithmic}[1]
% 	\FUNCTION{\textbf{init}()}
% %    	\STATE Store $D \equiv \{ S_1, \cdots, S_K; F_1, \cdots, F_K\}$
% 		\STATE Store $D \equiv \{ S_1, \cdots, S_K; F_1, \cdots, F_K\} \leftarrow \{\textbf{0}\}$
%     	\ENDFUNCTION   
%     \FUNCTION{\textbf{queryAction}()}
%     	\STATE For each arm $i=1,2,\cdots, K$:\\
% 	sample $\theta_{i}$ from $Beta(S_i+1,F_i+1)$
%         \STATE \textbf{return} $i = \arg\max_i \theta_{i}$
%     \ENDFUNCTION
%     \FUNCTION{\textbf{feedback}($i, r$)}
%         \STATE $S_i \leftarrow S_i + r$
%         \STATE $F_i \leftarrow F_i + 1-r$
%     \ENDFUNCTION
% \end{algorithmic}
% }
% \end{algorithm}

