\section{Introduction}
The importance of interpretability and explainability of machine-learned decisions has recently been an area of active interest, with the EU even declaring what has been called a ``right to an explanation" \cite{EUexplanationright}.  In traditional machine learning contexts, the focus of interpretability has been two-fold, first on the receiver of the decision (``why was I rejected for this job?") and second on the model creator (``why is my model giving these answers?").

Here, we extend this interest in interpretability to active learning, a domain in which the explanation is additionally of interest to the labeler (``why am I being asked these questions and why is it worth it to answer?").  Since active learning is generally applied in scenarios such as drug discovery where it is expensive (whether in terms of time or money) to label a query, the labeler in these contexts is often a domain expert in their own right (e.g., a chemist).  Given this, a query explanation can serve as a way to both justify an expensive request and allow the domain expert to give feedback to the model.

\subsection{Results}
We demonstrate how active learning choices can be made more interpretable to non-experts.  Using per-query explanations of uncertainty, we develop a system that allows experts to choose whether to label a query.  This allows experts to incorporate domain knowledge and their own interests into the labeling process.  In addition, we introduce a quantified notion of \emph{uncertainty bias}, the idea that an algorithm may be less certain about its decisions on some data clusters than others.  In the context of decision-making about people, this may mean that some protected groups (e.g., race or gender) may receive less favorable decisions due to risk aversion \cite{EUexplanationright}.  In the context of active learning, this means that these groups are more likely to be targeted for exploratory queries in order to improve the model.  We combine this idea with the explanations generated per query to describe the groups most targeted by uncertainty bias.  More broadly, these techniques allow us to make active learning interpretable to expert labelers, so that queries and query batches can be explained and the uncertainty bias can be tracked via interpretable clusters.

\section{Related Work}
\mypara{Active Learning}
Active learning has a long history detailed in a comprehensive survey by \citet{settles.tr09}.  Our work will focus on explaining query uncertainty.  Uncertainty querying for active learning was first proposed by \citet{Gale}. Since then, it has become perhaps the most common strategy for active learning and  several strategies for quantifying uncertainty have been developed \cite{settles.tr09}. Strategies used to quantify uncertainty for actively learning multi-class classification problems include selecting the sample with the minimum maximum-class probability, selecting the sample with the minimum difference in probabilities between the two most probable classes, and choosing the sample with maximal label entropy.  All three of the above strategies are equivalent for the binary classification tasks we will focus on in this paper \cite{settles.tr09}.

Related to our focus on the added impact of a domain expert on an active learning system, \cite{BaldridgePalmer2009} focus on evaluating the strength of active versus passive learning with expert versus novice labelers.  They found that the domain expert was able to take advantage of the more effective active learning setting, while the novice labeler did not provide the expected increase in performance from active learning.  While that work focused on the impact of an expert on the labeling process, we will focus on the impact of an expert on the \emph{choice of query} to label.  Perhaps the closest work to what we propose here is the work of \cite{Glass2006Explaining}, which focuses on explaining preference learning results to users of a scheduling system.  However, that work aims to explain the way the preferences have been updated in a way that is specific to the model and domain problem, while we focus on explaining the choice of query using a general model in any application area. 

\mypara{Interpretability}
Another area of direct importance to this paper is interpretable machine learning.  Recent work on interpretability has included both local explanations about an individual's decision \cite{LIME} and global explanations about the model's actions overall, including interpretable techniques in clustering \cite{interpretableClustering}, integer programming \cite{SLIMrecidivism}, rule lists \cite{WangRudin2015Falling}, and methods for understanding deep nets \cite{Zeiler2014Visualizing, Le2013Building} in addition to historical work on decision trees \cite{Quinlan1993C4.5} and random forests \cite{breiman2001random}.

\mypara{LIME}
We will build specifically on a method for creating local explanations introduced in \cite{LIME}. Local Interpretable Model-Agnostic Explanations (LIME) is a framework for generating locally faithful explanations for an otherwise opaque machine learning model. LIME does this by taking a given point and perturbing it to generate many hypothetical points in the neighborhood of a query point and then training an interpretable model on this hypothetical neighborhood. More specifically, LIME generates an explanation for the prediction $f(x)$ for a selected point $x$ and a given a global classifier $f$. To generate this explanation, LIME requires a number of samples $N$, a similarity kernel $\pi$, a discretizer $d$, and a number of attributes $K$ to use for the explanation. LIME begins by generating $N$ vectors with Gaussian random noise scaled to the mean variance of the dataset as a whole. The discretization method $d$ transforms points from a continuous representation to a categorical one by reducing the values into bins. A local regressor is fit to the generated points and the global model's predicted class probabilities. This model is used to select features by iteratively re-fitting and greedily adding them to the model until $K$ features are found. \footnote{Alternatively, the features with the highest weights could be used or, if utilizing LASSO, the LASSO regularization path could be used.}  The similarity kernel $\pi$ is used to upweight points near the query point.Finally, LIME returns the chosen $K$ features and their weights in the final local model as the explanation for $f(x)$.
