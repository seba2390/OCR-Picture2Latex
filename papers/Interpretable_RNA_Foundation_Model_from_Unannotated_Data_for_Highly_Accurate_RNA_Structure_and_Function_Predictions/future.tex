% Future is bright~\cite{big}.
To take advantage of the abundant unannotated RNA data, we propose an RNA foundation model trained on 23 million RNA sequences via self-supervised learning, which can be employed in both structural and functional downstream applications. Detailed analysis shows that RNA-FM also encodes the evolutionary information implicitly, which can be used to derive the evolutionary trend of lncRNAs and SARS-CoV-2 variants. Several further experiments, from structure prediction to gene expression regulation modeling, are conducted, and the results strongly prove the effectiveness of our pre-trained model. Particularly in structural-related experiments, models which include our RNA-FM embeddings can significantly improve the performance among various tasks spreading from simple to complex. When dealing with a complex task with a relatively large-scale dataset, it is more likely to achieve admirable performance by fine-tuning our RNA-FM and downstream modules together. In the case of simple task with small-scale datasets, it is better to utilize transfer learning to avoid over-fitting. On all accounts, our RNA-FM indeed encodes the RNA structural patterns and can offer explicit information useful for RNA structure predictions.

However, the improvement brought by RNA-FM in the functional tasks seems more slight compared with the gain in the structural tasks. The underlying reason may be the sequence distribution differences between these function-related applications and our pre-training dataset. Besides, the relation between the RNA structure and its function is too complicated to represent directly. Even though without colossal performance improvement, our embedding can still be beneficial for these downstream tasks. We aim to provide more impressive results regarding these functional-related tasks in the future. %\yu{In fact, still impressive, although not that impressive than the structural part.}

% The surge of computational approaches, especially artificial intelligence methods, has provided many ideas for solving biological problems, including natural language processing tools for modeling protein structures and functions. In this study, inspired by the application of NLP models, together with taking RNA sequences as text and applying a large-scale RNA dataset with 23 million sequences, we trained an foundation model using unsupervised learning, such model can be employed in both structural and functional downstream tasks. Previous approaches for identifying RNA structure or function could only handle specific tasks, such as UFold \cite{fu2021ufold} for RNA secondary structure prediction and PrismNet \cite{sun2021predicting} for RNA-protein interaction, however, due to the data or functionality limitation of these tools, it is difficult to apply them to another task. We have 23 million RNA sequences among various types in our training dataset, compared to other methods, which tend to have only a few thousand data points for training. It's apparent that our model is more universal in dealing with RNA-related tasks; in our example, RNA foundation model improves the performance of the corresponding SOTA approaches in six downstream tasks.

% We applied our pre-trained model to 4 RNA structure-related tasks and 2 RNA function-related tasks, which have been the most popular RNA tasks in last decade. For context, although diverse RNA structures and functions have a long history of experimental discovery









