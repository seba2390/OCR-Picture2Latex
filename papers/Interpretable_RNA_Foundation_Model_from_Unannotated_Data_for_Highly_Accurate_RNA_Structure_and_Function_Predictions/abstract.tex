% Inspired by the success of the application of NLP in the protein field, we train an RNA language model, RNA-FM, on 26 million sequences from the RNAcentral dataset by unsupervised learning. 
Non-coding RNA structure and function are essential to understanding various biological processes, such as cell signaling, gene expression, and post-transcriptional regulations. These are all among the core problems in the RNA field.
%Similar to protein structure prediction,  
%RNA structure and function prediction , and its performance could be further improved.
With the rapid growth of sequencing technology, we have accumulated a massive amount of unannotated RNA sequences. On the other hand, expensive experimental observatory results in only limited numbers of annotated data and 3D structures. Hence, it is still challenging to design computational methods for predicting their structures and functions. The lack of annotated data and systematic study causes inferior performance.
% utilizing them to develop practical models.
To resolve the issue, we propose a novel RNA foundation model (RNA-FM) to take advantage of all the 23 million non-coding RNA sequences through self-supervised learning. Within this approach, we discover that the pre-trained RNA-FM could infer sequential and evolutionary information of non-coding RNAs without using any labels. Furthermore, we demonstrate RNA-FM's effectiveness by applying it to the downstream secondary/3D structure prediction, SARS-CoV-2 genome structure and evolution prediction, protein-RNA binding preference modeling, and gene expression regulation modeling. The comprehensive experiments show that the proposed method improves the RNA structural and functional modelling results significantly and consistently. Despite only being trained with unlabelled data, RNA-FM can serve as the foundational model for the field. 

% Our results exceed previous state-of-the-art approaches without substantial task-specific architecture modifications.

%The trained model can extract such information from the unannotated RNA sequences, producing effective RNA rzpresentations. Although the RNA-FM model is not trained with the RNA structural information, the representation generated by RNA-FM can significantly improve the performance of models on various structure-related prediction tasks.
% we propose a novel RNA foundation model (RNA-FM), the first attempt to train a general language model on all the non-coding RNA sequences by self-supervised learning. After training upon 27 million non-coding RNA sequences collected and pre-processed from the RNAcentral dataset, the proposed foundation model could extract biological information from RNA sequences and produce effective representations. 
% The model learns the structural patterns just from sequences without any other annotations. 
% \textcolor{blue}{Because the proposed model is built upon self-supervised learning, it learns the sequential patterns only from the RNA sequences itself without any additional annotations, which makes the model produce effective general RNA representations that can be fine-tuned and widely applied to various real-world downstream RNA-related prediction tasks.} \siqi{same with the previous sentence, could be removed}
% because the obtaining of the annotations of RNA data is usually very costly.
% The embedding generated by the model improves the performance of models across different structure-related prediction tasks.
% Although the RNA-FM model is not trained with the RNA structural information, thanks to the large-scale pre-training on a huge number of RNA sequences, which contain different distributions and sequential patterns, the embedding generated by RNA-FM can significantly improve the performance of models on various structure-related prediction tasks. \siqi{still similar to previous sentence}
% When finetuning RNA-FM with the downstream model together, we can obtain state-of-the-art prediction performance. 

%By using general deep learning frameworks, such as ResNets, the model built upon our pre-trained RNA-FM outperforms the previous state-of-the-art methods, which are carefully designed for the downstream tasks, by up to 20\%, no matter on the F1 score of RNA secondary structure prediction, the long-range top precision of 3D closeness prediction, or the PMCC of the distance map prediction. Moreover, extensive experimental results demonstrate that the RNA-FM model is also very promising to benefit the RNA function-related prediction tasks, such as 5â€™ UTR-based mean ribosome loading prediction and protein-RNA interaction prediction. As a novel model learning to extract useful information based on the unannotated RNA sequences, RNA-FM can serve as a foundation for RNA structure and function prediction. 

