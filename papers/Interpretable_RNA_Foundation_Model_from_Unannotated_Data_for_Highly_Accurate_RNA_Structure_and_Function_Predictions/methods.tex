% \subsection{Awesome methods}
% \begin{figure}[!t]
%   \begin{center}
%     \includegraphics[scale=0.4]{figs/tardis.png}
%   \end{center}
% \caption{A simple sketch of the TARDIS.}
% \label{fig:tardis}
% \end{figure}



% In this paper, we introduce the deep Transformer encoder method \cite{vaswani2017attention, devlin2018bert, rives2021biological} to process the input data, namely RNA sequences.
% Different from the recurrent neural networks \cite{chung2014empirical} and the convolutional neural networks \cite{ shen2020deep}, which have some requirements on the order of the input data, the Transformer encoder model does not have such assumptions. Instead, the Transformer encoder model utilized the position embeddings, which is more general and can be applied more widely.
% Since the RNA sequences are usually very long, these sequences can not be effectively handled by the traditional recurrent neural networks \cite{hidasi2018recurrent}, or its variants, such as the Gated Recurrent Unit and the Long short-term memory \cite{yang2017attention}, because of the problem of the vanishing gradient signals for especially long sequences modelling in recurrent neural networks.
% The Transformer encoder method can effectively deal with such vanishing gradient issues by making use of the attention mechanism \cite{vaswani2017attention, devlin2018bert}.
% Therefore, in this research, we select the Transformer method rather than recurrent neural networks to model the RNA sequences.
% In the following sub-sections, we will discuss the model architecture and the attention mechanism utilized in the Transformer method. 

%cjy

% 1.overview: brief summary of motivation and our method (model, dataset, training scheme)
%Original
%To effectively advance the development of the RNA study, we aim to build a unified foundation model to generate general representations for RNA sequences, which could benefit various tasks with insufficient annotated data and significantly improve their performances. 
%

% \textcolor{blue}{Ash: Jan-23}
\paragraph{Overview of RNA-FM.}
In order to advance the development of RNA studies, we aim to build a unified foundation model in providing rich and meaningful representations inferred from standalone sequential information. We expect such representations to significantly boost various downstream tasks' performances when sufficient annotated data are unavailable. Inspired by recent great success in natural language processing, computer vision, and bioinformatics \cite{vaswani2017attention}\cite{dosovitskiy2020image}\cite{rives2021biological}, we explore the possibilities of a general transformer architecture in RNA-related studies. Thus, our framework is built upon the bidirectional transformer language model proposed in BERT \cite{devlin2018bert}, followed by the unsupervised training scheme. We named our framework RNA-FM, suggesting a foundational model for all the RNA-related studies.

This section first illustrates how we construct the large-scale ncRNA dataset, followed by model and training details. Next, we investigate a general strategy for several RNA function-related or structure-related downstream applications.
%Ash Jan-23

%Therefore, we use a Transformer \cite{vaswani2017attention, devlin2018bert, rives2021biological} model as backbone since it emerges as a powerful general model in multiple domains, such as natural language processing, computer vision and bioinformatics.\siqi{add some citation}. 


%In this section, we first present how we build the large-scale ncRNA dataset, then introduce the detailed model architecture and objection function. Finally, we propose a general framework for a set of RNA function-related or structure-related downstream tasks.

%\textcolor{blue}{%More importantly, considering that the obtaining of the annotations of RNA sequences is usually very costly in reality, we construct the RNA-FM method upon the unsupervised learning mechanism, which enables it to automatically learn the distribution and the sequential patterns and important structure information from the RNA sequences themselves without requiring any additional annotations.
%The proposed model can encode the distribution and the sequential pattern and functional information into its representations, and therefore contribute to a majority of structure-related and function-related tasks, which we named as RNA foundation model (RNA-FM).}\siqi{this part has been mentioned many times}

% 2.Dataset (Intro + Preprocessing)
\paragraph{Large-scale pre-training dataset.}

%\subsection{Large-scale Pre-training Dataset}
% \textcolor{blue}{Ash: Jan-24}
Our large-scale dataset used for pre-training phase is collected from RNAcentral \cite{rnacentral2021rnacentral}, the to-date largest dataset of the ncRNA. This dataset is indeed a comprehensive ncRNA sequences collection, representing all the ncRNA types from a broad range of organisms. It combines ncRNA sequences across 47 different databases, adding up to around 27 million RNA sequences in total.

We then pre-process all ncRNA sequences by replacing `T's with `U's since they are both complementary to adenine and similar in structure (`T's used for representing thymine in DNA while `U's stands for uracil in RNA.). This results in a dataset involving 4 main types of bases (16 counted types of combination in total, `A', `C', `G', `U', `R', `Y', `K', `M', `S', `W', `B', `D', `H', `V', `N', `-'). Moreover, to reduce redundancy without hurting the capacity of our dataset (reserve sequences as more as possible), we eliminate identical sequences by applying cd-hit-est \cite{fu2012cd} with a cut-off at 100\%.

After the above pre-processing steps, a final large-scale dataset consisting of 23.7 million ncRNA sequences is obtained. We named it RNAcentral100, which will be used to train our RNA foundation model in a self-supervised manner. Please refer to Supplementary Table.1-3 for the distribution of nucleotides and sequence lengths.
%Ash
%The pre-trained dataset of our foundation model is collected from RNAcentral, which is the to-date largest dataset of the non-coding RNA. The RNAcentral dataset is a comprehensive ncRNA sequence collection, representing all the ncRNA types from a broad range of organisms. This dataset combines ncRNA sequences from 47 different databases, including around 27 million RNA sequences in total.
%We pre-process all the sequences by replacing “T”s with “U”s\siqi{explain why or add a reference?}, which resulted a dataset with 20 types of amino acid in total. In addition, to preserve as many sequences as possible, we merely eliminate the identical sequences by applying cd-hit-est \cite{fu2012cd} with a cut-off at 100\%. After pre-processing, we obtain a large-scale dataset with ?\siqi{fill the number} million sequences, named RNAcentral100, which is used to train the proposed RNA foundation model. Please refer to table/figure \siqi{please refer table/figure number} for the distribution of 20 amino acids and sequence length.


% 3.RNA Foundation Model
\paragraph{RNA foundation model training details.}

% \textcolor{blue}{Ash: Jan-24}
Our RNA-FM's framework is a stack of $12$ transformer encoder blocks proposed in BERT \cite{devlin2018bert}\cite{vaswani2017attention}. Each encoder block \cite{devlin2018bert} consists of a $640$ hidden size feed-forward layer and a $20$ multi-heads self-attention layer. Layer normalization \cite{devlin2018bert} and residual connections are applied before and after every block, respectively.


For a RNA sequence with length $L$, RNA-FM takes raw sequential tokens as input, and an embedding layer maps each nucleotide token into a $640$-dimensional vector, thus resulting in an $L\times 640$ embedding matrix. The embedding matrix then proceeds through each encoder block, which includes multi-head self-attention modules and feed-forward layers. The output tensors from encoder blocks have the exact same size with the input, and a final Softmax layer is concatenated above to predict corresponding tokens including our selected 16 nucleotides and 4 specific functional identifiers. 

During the pre-training phase, we followed a self-supervised training manner in BERT \cite{devlin2018bert}. Around 15\% of nucleotide tokens are randomly replaced with a special mask token. ( If the $i$-th token is chosen, we replace the $i$-th token with (1) the [MASK] token 80\% of the time (2) a random token 10\% of the time (3) the unchanged $i$-th token 10\% of the time). We then train the model with masked language modeling (MLM) \cite{devlin2018bert} by predicting the original masked token with cross-entropy loss. Such a training strategy can be formulated as an objective function as follow: 

\begin{equation}
\mathcal{L}_{MLM}= \mathbb{E}_{x \sim \mathcal{X}}\mathbb{E}_{x_{\mathcal{M}}\sim x}\sum_{i \in \mathcal{M}} - \log p(x_i|x_{/\mathcal{M}}).  \label{EQ.1}
\end{equation}

Here in the above equation, a set of indices $\mathcal{M}$ are randomly sampled from each of the input sequences $x$ ($15\%$ among the whole sequence), and masked by replacing the true token at each index $i$ to some other mask tokens.
Next, for each masked token, when the masked sequence ($x_{/\mathcal{M}}$) is given as context, our adopted objective function will minimize the negative log-likelihood of the corresponding true nucleotide $x_i$.
Our adopted objective function can capture dependencies between masked proportion and the remaining parts of the input sequence, which enables accurate predictions for masked positions. Thus, RNA-FM trained via Eq.~(\ref{EQ.1}) drives the network to gain deep understanding and rich representation of each sequential token.

%To coordinate with the 26 million \siqi{the number in previous section is in question mark} sequences \siqi{coordinate means?}, we build the foundation model based on Transformer, the most prevalent architecture due to its high-capacity. The RNA-FM consists of 12 transformer layers with 640 embedding dimension and 20 attention heads. And it takes pure RNA sequences with nucleotide characters as input. More detailed, for a sequence $x$ with length $L$, we first tokenize each nucleotide from a character to an integer. Each token is then mapped into a 640-dimensional vector as the initial embedding. The embedding matrix then passes through the transformer layers, including a multi-head self-attention module and a feed-forward network (Please refer to appendix A for more details). The self-attention module explicitly incorporates the sequence embeddings from the previous layer using attention mechanism and eventually builds up a complex representation that models residue-residue interactions.

%In the pre-training stage, we apply the masked language modeling loss (MLM) \cite{devlin2018bert} to train the model. More specifically, 15\%  of the nucleotide are randomly replaced with a special mask token and RNA-FM aims to reconstruct these corrupted RNA sequences to the original ones. Therefore, RNA-FM is trained without using any additional annotated data. The objective function for MLM can be formulated as below:

%\siqi{the above equation seems wrong}


In summary, we trained RNA-FM on eight A100 GPUs of 80 GB memories for one month. We adopted an inverse square root learning rate schedule for the training of the neural network, with a 0.0001 base learning rate, a 0.01 weight decay, and 10000 warm-up steps. Similar to the previous studies, we also set the maximum length of the input sequences as 1024 to reduce the memory consumption and increase the batch size to speed up the training of the neural network.

\paragraph{RNA foundation model-generated embeddings analysis.}
After the training stage, we can get the embeddings for all the input RNA sequences generated by our RNA foundation model. In order to test if these embeddings have fully mined the structural and functional information belongs to RNA, we build RNA Atlas by subsample RNAcentral100 with a maximum of 10,000 samples per each RNA types. Each instance from different RNA families can be represented by a 640 dimensional vector by averaging across its RNA-FM embedding with shape of $L*640$ at each position in its sequence, where $L$ denotes the sequence length. Then, we apply UMAP \cite{mcinnes2018umap} to reduce these 640 dimensional vectors into 2 dimensional ones, and project them on a plane. Furthermore, we explore if the embeddings contain evolutionary information of RNA by applying trajectory inference, which is implemented by VIA \cite{stassen2021generalized}. We take embeddings of lncRNA as inputs, and get the stream-plot of RNA evolutionary trend.


% 4.Downtream Application 
\paragraph{Downstream training strategies.}
% \textcolor{blue}{Ash 13-Mar}
Once the pre-trained RNA-FM model encoded with RNA's informative structural and functional patterns is obtained, we can integrate it into various downstream applications via two schemes: feature-based training and fine-tuning. For the feature-based scheme, we freeze the parameters inside RNA-FM and feed them to downstream models by switching the output module. In contrast, the fine-tuning scheme will require training RNA-FM together with downstream modules rather than freezing parameters inside RNA-FM, which usually performs better than the feature-based training scheme.
% Regarding the downstream tasks fields that RNA-FM can handle, we think they mainly focus on structure-related tasks because the RNA primary structure has a very close relationship with its high-order structure. Because the structure also partially determines function, RNA-FM can deal with the function-related tasks to some extent.


Our proposed RNA-FM can effectively tackle both the RNA structure prediction and function modeling. We first investigate structure-related applications, including RNA secondary structure prediction, RNA 3D closeness prediction, and RNA distance prediction. 
% We expect to achieve SOTA on a broad field of evaluation metrics since RNA primary structure carries strong connections with its high-order structure. 
Additionally, we investigate the downstream function-related tasks, including COVID-19 virus modeling, RNA-protein interaction prediction, and gene expression regulation modeling. 
% Since RNA's primary structure can also partially determine its functions, RNA-FM is also expected to further boost its performance. 
We give more details about how to set up our methodology to perform in the major downstream applications.

%Once we get our pre-trained RNA-FM model, we can combine the RNA-FM into a variety of downstream tasks in mainly two schemes, feature-based training and fine-tuning. For the feature-based scheme, we should freeze our RNA-FM parameters and only feed into the downstream models for specific tasks with the output embeddings of RNA-FM, which encode the informative structural and even the functional patterns. In contrast, the fine-tuning scheme will train the RNA-FM and downstream models together rather than fixing RNA-FM and training downstream models alone, which usually performs better than the feature-based training scheme.
% Regarding the downstream tasks fields that RNA-FM can handle, we think they mainly focus on structure-related tasks because the RNA primary structure has a very close relationship with its high-order structure. Because the structure also partially determines function, RNA-FM can deal with the function-related tasks to some extent.
%The proposed RNA-FM is able to effectively deal with two types of downstream tasks, including RNA structure prediction tasks and and RNA function-related tasks. 
%Firstly, we focus on the structure-related tasks (including RNA Secondary Structure Prediction, RNA 3D Closeness Prediction, and RNA Distance Prediction) because the RNA primary structure has a very close relationship with its high-order structure. 
%Secondly, because the structure of RNA can also partially determine the functions of RNA, RNA-FM can also be utilized to deal with the function-related tasks (including UTR Function Prediction and RNA-protein Interaction Prediction) to some extent.
%The descriptions of the downstream tasks that are utilized to estimate the performance of RNA-FM are as follows:
% To estimate the utility of the proposed neural network for real-world applications, we evaluate the performance of the proposed method for various downstream tasks, including RNA Structure Prediction Tasks (such as RNA Secondary Structure Prediction, RNA 3D Closeness Prediction, and RNA Distance Prediction) and RNA Function-Related Tasks (such as UTR Function Prediction and RNA-protein Interaction Prediction)

\textbf{RNA second-order structure modeling.} 
% \textcolor{blue}{Ash} 
% \Ash{3-26}
RNA high-order structures can usually be represented by a 2D matrix. We would focus on three related studies to highlight the capability of RNA-FM: secondary structure, contact map, and distance map prediction. Secondary structure reflects its hydrogen bonds in primary sequence. While contact and distance maps concentrate more on pairwise tertiary inter-nucleotide interactions. Here, we adopt a simple 2D ResNet as a unified downstream prediction module for all the structure prediction tasks rather than creating an elaborately-designed framework for each sub-task. Similar to ESM-1b \cite{rives2021biological}, our deep residual network consists of 32 blocks, where each contains two convolution layers with a filter size of 64. The input of the ResNet32 is the outer concatenation of output embeddings obtained from query sequences. This module is utilized across all the RNA structure prediction tasks unless we specify (See Supplementary for more details of ResNet32). We show that RNA-FM can achieve significantly better performance than the state-of-the-art models with such simple downstream modules.
% In the following result section, we will investigate how our RNA-FM with such downstream training strategy works on structure-related tasks: RNA secondary structure prediction, RNA 3d closeness prediction, and RNA distance map prediction. We will further prove that our RNA-FM can also achieve significantly better performance than the state-of-the-art models with such simple downstream modules.

%\textcolor{blue}{Ash 13-Mar} Structure prediction is always the key among various RNA-related tasks because RNA structure usually determines its function. However, because of the high cost in the wet-lab experiments and the RNA structural instability, only a tiny fraction (\textless0.001\%) of the structure-known ncRNAs has been determined by experiments \cite{zhao2021review}. Therefore, more and more computational methods \cite{singh2019rna, chen2020rna,sato2021rna,fu2021ufold} are proposed for RNA structure prediction. In this paper, we aim at utilizing our pre-trained model to improve the performances of current high-order RNA structure prediction.
%\paragraph{Rationale}
%This research is motivated by the fundamental consideration that the high-order structures of molecules are highly related to their primary structure \cite{celander1991visualizing}. As a result, our proposed model is expected to improve the prediction performance effectively as well, although it is merely pre-trained with pure sequences in an unsupervised way.


%\paragraph{RNA Secondary Structure Prediction}  RNA can rapidly form its secondary structure from its primary structure by pairing bases with hydrogen bonds. The RNA secondary structure is much more stable than its tertiary structure and more accessible in the cells, making itself an essential role for the high-order structure prediction and even function prediction \cite{zhao2021review}. we compare the performance of our proposed model with state-of-the-art methods.

%\paragraph{RNA 3D Closeness Prediction} 
%Secondary structure only reveals parts of relationships between base pairs, which cannot provide as much valuable information as 3D structure. And to obtain precise RNA 3D structure modeling results, many challenging tasks are proposed to generate strict constraints for downstream modeling, especially RNA 3d closeness prediction. The RNA 3D closeness utilizes a 2D matrix to represent pairwise tertiary inter-nucleotide interactions rather than the relationDear ship in a 2D ﬂat described in secondary structure predictions, which thus clearly indicate that arbitrary two bases have tertiary interaction if their distance is under a certain threshold \cite{sun2021rna}\cite{wang2017accurate}. 
%The distance is defined as the minimal atomic distance of arbitrary bases, and the threshold is set as 8A°.

%\paragraph{RNA Distance Prediction} 
%In the past few years, more and more complex protein structure prediction tasks have been well studied. For instance, trRosetta \cite{yang2020improved} can predict the distance between two amino acids and the orientation formed by their atom planes, and Alphafold \cite{alquraishi2019alphafold} even directly predict the 3D structure of target proteins with high precision. To benefit the prediction of 3D structure, we further evaluate the performance of RNA-FM in a relative new task, which is predicting the distance of arbitrary bases in the primary sequence, namely RNA Distance Prediction. This regression task is more informative to subsequent 3D folding methods although it's more complicated than RNA secondary structure and 3D closeness prediction tasks. The base pair distance measures the continuous distance between arbitrary base pairs, so such regression task contribute more for the prediction of RNA 3D structure.


\textbf{End-to-end RNA 3D structure prediction.}
Similar to AlphaFold2 in protein research, we aim to first establish such an end-to-end differentiable model for RNA 3D structure prediction. The model takes advantage of 4 Evoformers as its backbone, and we stack an equivariant graph transformer (EGNN) on top as a 3D atom coordinate predictor. The data are collected via the up-to-date Protein Data Bank, including raw RNA sequences with corresponding 3D structures. 1036 sequences are used for training, and the left 100 are used for validation and testing purposes. We utilize distance RMSD as our objective for training, and we optimize the model for more than 10000 steps to get an initial version. To validate the effectiveness of our RNA-FM embeddings, we compare it with the pure RNA sequences as the inputs of the 3D framework.

\textbf{SARS-CoV-2 virus genome embedding extraction.} 
We aim to extract representations for severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) using RNA-FM. Considering the length of the whole genome (around 30K nucleotides) is far longer than the input limitation (1022 nucleotides) of RNA-FM, here, we apply a different feature extraction strategy. To be specific, we employ a fixed-length window of 1022nt on a non-overlap sub-section of the whole genome to extract the RNA-FM embeddings. Then, we aggregate embeddings by taking the average over them, and the final standard length vector is used as the RNA-FM embedding for the whole genome. We then apply trajectory inference with the genome-wise RNA-FM embeddings by VIA \cite{stassen2021generalized}. For each variant, we sample maximum 100 instances from the all sequences download from \href{https://www.ncbi.nlm.nih.gov/labs/virus/vssi/#/virus?SeqType_s=Nucleotide&VirusLineage_ss=Severe%20acute%20respiratory%20syndrome%20coronavirus%202,%20taxid:2697049&Completeness_s=complete}{SARS-CoV-2 Data Hub}.
We set $k$ in the $k$-nearest neighbors algorithm of VIA as 120 and the root as the Alpha variant.

\textbf{Protein-RNA interaction.} 
% \yu{We need three sentences to describe what is Protein-RNA interaction, why we want to study it, and what we want to predict. Also, what is the input and what is the output.} 
Protein-RNA interactions play important roles in a plenty of activities, such as cell-signaling, post-transcriptional regulations and protein synthesis \citep{wei2021protein}. Therefore, considering the importance of protein-RNA interactions in RNA function, we evaluate how well RNA-FM mine ncRNA function information by predicting RNA binding proteins corresponding to RNAs. In this application, we apply Hela cell as the dataset for the task and devide it into 17 different protein sub-sets. Within each one, we predict whether the input RNA can bind with the protein. Our downstream prediction module perform similarly as the PrismNet \cite{sun2021predicting} pipeline. Firstly, we reproduce PrismNet results, including two different types: One trained from raw sequences as baselines, and another one trained with \textit{in vivo} secondary structures. We then replace the actual secondary structures in PrismNet with our generated RNA-FM embeddings for all sequences while keeping the whole downstream PrismNet architecture unchanged. In this way, we can test if using RNA-FM embeddings can accurately predict protein-RNA interactions as the real RNA secondary structures.


\textbf{mRNA untranslated region's function.} 
% \textcolor{blue}{Ash} 
% \yu{We need three sentences to describe what is mRNA untranslated region, why we want to study it, and what we want to predict. Also, what is the input and what is the output.}
The 5' untranslated region is the region of a messenger RNA (mRNA). Although a 5'UTR is a part of a mRNA, which does not belong to ncRNAs, and RNA-FM is trained with ncRNAs, we attempt to test whether our pre-trained RNA-FM can handle these kinds of special non-coding sequences, and expect RNA-FM to finally aid with modeling the relationship between UTR and target protein expression. For the input UTR, we predict its corresponding mean ribosome load (MRL), which can reflect how a UTR regulates the target protein expression level. Specifically, we adopt the same pipeline and model as Paul et al. \cite{sample2019human} developed in order to testify the effectiveness of our RNA-FM embeddings. Their model is well-designed by performing a grid search, whose best framework is constructed as three 1D convolutional layers with 120 filters and a ReLU activation for each layer. The third convolution layer will output one channel and L length features, which will be fed into two fully-connected layers with one output node as the final prediction. The original inputs of the model are simply the one-hot representation of raw sequences (4 dims). Therefore, to stay as close to their original architecture as possible, we replace the inputs with our embeddings. Finally, we obtain three models with different inputs, including pure sequence (Seq) in the form of one-hot encoding (4 dims), pure RNA-FM embedding (RNA-FM) of 640 dims, and the combination of these two (RNA-FM + Seq). Further, we apply a linear projection to reduce the embedding dimension from 640 to 4 for matching the one-hot embedding dimension.

%\paragraph{RNA-protein Interaction Prediction} 
%Protein-RNA interactions play important roles in various cellular activities, such as posttranscriptional regulations and protein synthesis. Therefore, we also evaluate the performance of our designed network in the embeddings of a Protein-RNA interaction prediction pipeline. PrismNet is an effective tool to predict RBP-RNA binding sites based on the information from RNA sequence and secondary structures, which takes RNA sequences and in vivo secondary structures as inputs and can output a score for every binding site by evaluating every nucleotide position to determine whether it’s a binding site \citep{sun2021predicting}. Thus, we evaluate our method in PrismNet pipeline for the RNA-protein interaction prediction tasks.






