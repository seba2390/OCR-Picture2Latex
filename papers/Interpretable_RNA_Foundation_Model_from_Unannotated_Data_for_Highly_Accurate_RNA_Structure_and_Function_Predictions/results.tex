% Results are great~\cite{small}. MRGA!
% Analysis of embeddings generated by RNA foundation model
% {\jiayang{RNA-FM assists in establishing meaningful RNA Atlas.} 

%\paragraph{RNA-FM encodes structure properties of RNA in the representation space.} \siqi{ does cluster itself indicate it could extract structural and functional information?}
%\paragraph{Creating a unified foundation model to encode RNA sequential information.}
%\yu{Please add a section: Overview of RNA-FM, expanding Figure 1. The intuition behind our method. The general procedure of the results part. Overview of the results.}
%\paragraph{Learning from large-scale unlabeled RNA sequences.}
%Inspired by the successful application of transformer structure in NLP, we applied 12 transformer-based bidirectional encoder blocks that proposed in BERT \cite{devlin2018bert} to construct the RNA foundation model. In order to take advantage of huge amount of unlabelled ncRNA data and avoid relying on label information, we adopted RNA-FM to do self-supervised learning on 23 million sequences from RNAcentral database \cite{rnacentral2021rnacentral}, as Fig.\ref{Fig.overview} shows.
%At the training stage, around 15$\%$ of
%nucleotide tokens are randomly replaced with special mask tokens, and we take the model to predict the masked ones, which forces it to learn context information. Then, we get our RNA foundation model. For each input RNA sequence of length $L$, RNA-FM generates an $L\times640$ embedding matrix. These embeddings are expected to contain fruitful messages of RNA universe. Therefore, to test the hypothesis, we firstly analyze embeddings by building RNA Atlas, afterward, we apply them to various structural-related and functional-related downstream tasks. 
%\Ash{3-27}


\paragraph{Learning from large-scale unlabeled non-coding RNA sequences.}
As shown in Figure \ref{Fig.overview}, in order to take advantage of a massive amount of unlabeled ncRNA data and avoid relying on label information, we propose our RNA foundation model (RNA-FM) based on the BERT \cite{devlin2018bert} language model architecture. It is built upon 12 transformer-based bidirectional encoder blocks and trained on 23 million sequences from the RNAcentral database in a self-supervised manner.
After training, RNA-FM produces a $L\times640$ embedding matrix for each RNA sequence with length $L$. These embeddings are expected to contain rich information within the ncRNA universe. We verified the effectiveness of RNA-FM on various applications. Firstly, to investigate what has been learned by the model and the physical meaning of the model outputs, we analyze the derived embeddings directly and examine how ncRNAs of similar function and structure gather in a 2-dimensional plane, resulting in the RNA Atlas in Figure \ref{Fig.visual}. Also, the embedding from RNA-FM can be used to infer the long non-coding RNA (lncRNA) evolutionary trend, which suggests that the evolutionary information has been learned by our model implicitly. Furthermore, models using RNA-FM embeddings could improve over state-of-the-art approaches consistently on various structural-related and functional-related downstream prediction problems, including both SARS-CoV-2 study and gene expression regulation modeling.


\begin{figure}[!t]
\centering
\includegraphics[width=0.90\textwidth]{figs/visualization/visualization.png} 
\caption{\textbf{RNA-FM encodes multi-scale patterns.} \textbf{a.} RNA Atlas with all ncRNA types in RNAcentral100 using different embedding ways. 
\textbf{b.(1)} Embedding projections of housekeeping RNA (rRNA, tRNA) and regulatory RNA (lncRNA, snoRNA, miRNA, siRNA, snRNA, piRNA).
\textbf{b.(2)} Detailed distribution of the regulatory RNAs. 
\textbf{b.(3)} Embedding projections of long ncRNA (sequence length $>$200, rRNA, tmRNA, \textit{etc.}) and small ncRNA (sequence length $<$200, tRNA, sncRNA, \textit{etc.}). 
\textbf{b.(4)} Detailed distribution of long RNAs (most of the small RNAs have been analyzed in b.(2)). 
\textbf{c.} Trajectory inference of lncRNA evolutionary trend based on the RNA-FM embeddings.
}
\label{Fig.visual}
\end{figure}

\paragraph{RNA-FM learns multi-dimension biological information of RNA universe.}

To demystify what has been learned by the large-scale RNA-FM model from the million-scale data and the physical meaning of the model outputs, we take a closer look into RNA biological information contained in the RNA-FM embedding, including its structural/functional properties and evolutionary information. Such analysis shows the interpretability of the RNA-FM model.

\textbf{RNA functions and structures} vary across different RNA types, and RNA-FM is expected to encode these rich properties within our generated embeddings from pure RNA sequences. We build RNA Atlas by employing the proposed encoder on the known RNA universe to analyze their patterns. Next, UMAP \cite{mcinnes2018umap} is applied to reduce the dimension of embeddings and project them into a 2-dimensional plane. To compare and visualize how the learning process shapes the representations, we take both generated representations before and after model pre-training into consideration. Furthermore, random initialized RNA-FM (Random) and one-hot encoding (One-Hot) are also introduced for visualization purposes.
% \jiayang{intraclass}
The results are shown in Figure \ref{Fig.visual}a. In the pre-trained RNA-FM embedding space (left), visual inspection reveals that RNA types are organized by structure and function properties with clear boundaries between clusters. In contrast, the projection of the Random model (middle) presents some extremely vague clustering structure; while projections of One-Hot encoding (right) are barely distinguishable with no apparent structure information, implying that RNA-FM has learned structural or functional information beyond their primary structure, such that instances with similar properties are grouped.

% \jiayang{different view} 
We take it a step further to discuss ncRNAs from different categorical views, such as housekeeping ncRNAs and regulatory ncRNAs, long ncRNAs ($>$200 nucleotides), and small ncRNAs ($<=$200 nucleotides). In Figures \ref{Fig.visual}b(1) and b(3), RNA-FM well discriminates housekeeping and regulatory categories but struggles to deal with short and long ncRNAs. This might suggest that RNA-FM encoding emphasizes more on structural or functional similarity rather than length since RNAs with different lengths could share the same functions and RNAs with similar lengths could differ significantly.
% \jiayang{interclass \& length effect}
In Figures \ref{Fig.visual}b(2) and b(4), when we look closer into some sub-classes within a limited length range (less or greater than 200) in the RNA-FM part. These RNA embeddings aggregate or separate according to the similarity of their structures and functions; it proves again that RNA-FM establishes the RNA Atlas by recognizing the structures and functions of RNAs rather than their length. %discover that interclass patterns of embeddings are the same as the above intraclass ones for both small and long RNAs. 

\textbf{RNA evolutionary information} is also explored in our studies. We apply trajectory inference (pseudotemporal ordering) \cite{saelens2019comparison}, which is commonly used in single-cell transcriptomics, to a subset of lncRNA with their RNA-FM embedding as input. RNAs in the subset can be classified according to different types of species. Here we obtain their evolutionary relationship from an evolutionary study of lncRNA repertoires and expression patterns \cite{necsulea2014evolution}. We first generate RNA-FM embeddings for the lncRNA subset, then trajectory inference is carried out via VIA \cite{stassen2021generalized} and the stream-plot is shown in Figure \ref{Fig.visual}c. We discover that although it is hard for RNA-FM to distinguish these RNAs into different species, the embeddings are able to present a roughly accurate evolutionary trend of different species corresponding to their ground-truth timeline. The result is surprising because we do not include such evolutionary features during training and only use the pure RNA sequences. This result testified that RNA-FM deeply mined the implicit genetic message and encoded the outputs with evolutionary information.

% We first directly analyse the embedding space formed by our RNA-FM and generate RNA Atlas based on the RNAcentral. To further evaluate the effectiveness of our RNA-FM, we apply it to a variety of downstream tasks, including structure-related and function-related tasks. As a result, the RNA-FM can improve almost 20\% over SOTAs in the structure prediction tasks. In addition, the RNA-FM can also contribute to function-related tasks to some extent.

% Unsupervised language models are usually deemed to encode the obvious underlying patterns from vast amounts of sequences. To find whether our RNA-FM can capture RNA sequence patterns and understand how the pre-training shapes the representations, we compare the changes of the embedding spaces formed by RNA-FM on RNAcentral100 before and after the pre-training phase. The analyses are conducted from two scales, including sequence-level and gram-level.

% \paragraph{Embeddings encode RNA Sequence Type}


% In order to compare the shaped representations before and after the pre-training phase, we project the embedding of them into two-dimensions using UMAP.


% \paragraph{Embeddings encode RNA N-gram pattern} supp


\paragraph{RNA-FM benefits both RNA secondary structure prediction and 3D modelling results.} Structure understanding is always the key among various RNA-related applications since RNA structure usually determines its function. However, only a tiny fraction (\textless0.001\%) of the structure-known ncRNAs has been determined by experiments \cite{zhao2021review} due to the high cost of wet-lab experiments and RNA structural instability. To tackle this problem, more and more computational approaches \cite{singh2019rna, chen2020rna,sato2021rna,fu2021ufold} have been proposed for RNA structure prediction. We investigate RNA-FM's performance on several structure prediction tasks, including secondary structure prediction, 3D closeness prediction, and RNA map distance prediction. We also try to perform 3D reconstruction and prediction beyond the predicted secondary structures.
%\paragraph{Rationale}
%This research is motivated by the fundamental consideration that the high-order structures of molecules are highly related to their primary structure \cite{celander1991visualizing}. As a result, our proposed model is expected to improve the prediction performance effectively as well, although it is merely pre-trained with pure sequences in an unsupervised way.

%\paragraph{Downstream Module \& Training Scheme}
% RNA high-order structure can be represented by a 2D matrix, so a 2D ResNet is very well suited to predict these structures.The 2D ResNet is the most prevalent model in the computer vision field and has been successfully applied to deal with many bio information tasks, such as structure prediction of protein [] or RNA []. 
%RNA high-order structures can usually be represented by a 2D matrix. To emphasize the capability of RNA-FM, we use a simple 2D ResNet as a unified downstream model for nearly all RNA structure downstream tasks rather than create the elaborately-designed framework for each sub-task. Following the strategy utilized in ESM1b \cite{rives2021biological}, we build a deep residual network with 32 blocks. Each block contains two convolution layers with a filter size of 64. The input of the ResNet32 is the outer concatenation of the output embeddings of the query sequence. This module is utilized across all the following RNA structure prediction tasks unless we specify their framework. (see Supplementary for more details of ResNet32) The following involved RNA structure-related tasks: RNA secondary structure prediction, RNA 3d closeness prediction, and RNA distance prediction. Although with the simple downstream module, our RNA-FM can also achieve significantly better performance than state-of-the-art models.

\textbf{RNA secondary structure} can be rapidly formed from its primary sequence by pairing bases with hydrogen bonds. Secondary structure is much more stable and more accessible in cells than its tertiary form, making it an essential role for the high-order structure prediction or even function prediction \cite{zhao2021review}. This section performs a comprehensive comparison of RNA-FM and other popular RNA secondary structure prediction methods, as well as a head-to-head comparison with one of the SOTA methods, UFold \cite{fu2021ufold}.

%\textit{Evaluation}
We conduct experiments on several benchmarks commonly used in E2Efold, SPOT-RNA and UFold.
(1) RNAStralign \cite{tan2017turbofold}, which consists of 37149 structures from 8 RNA types, is one of the most comprehensive collections of RNA structures in the field.
(2) ArchiveII \cite{sloma2016exact}, which consists of 3975 RNA structures from 10 RNA types, is also a widely-used benchmark dataset for many classical RNA folding methods.
(3) bpRNA-1m \cite{singh2019rna}. The dataset is preprocessed by removing sequence similarity with 80\% sequence-identity cut-off and restricting their maximum sequence length below 500. The preprocessed dataset contains 13,419 sequences and is randomly split into 10,814 RNAs for training (TR0), 1300 for validation (VL0), and 1,305 for testing (TS0).
% (4) PDB \cite{singh2019rna}. The dataset for transfer learning is obtained by downloading high-resolution ($<3.5$\r{A}) RNAs from PDB on March 2, 2019. After applying CD-HIT-EST with 80\% cut-off and BLAST-N, we finally obtain 120, 30, and 67 RNAs for training (TR1), validation (VL1), and independent test (TS1), respectively.
For evaluation and testing, we take the usage of Ufold \cite{fu2021ufold} data and make a fair comparison. The well-trained model is evaluated on ArchiveII600 (a subset with a length less than 600) and TS0.

%Here, we only their offered subset of length less than 600 due to the input limitation of our model and regard them as RNAStralign600 and ArchiveII600. The RNA-FM and ResNet32 are trained on the RNAStralign600 and directly evaluated on ArchiveII600 without re-training.

\begin{table}[t]
\centering
\caption{\textbf{RNA secondary structure prediction performance.} Our method beats the other 12 SOTA methods on the two datasets across all evaluation criteria except for being slightly behind Ufold on the recall score. RNA-FM is not specific to RNA secondary structure prediction. However, it has learned rich information about the secondary structure and done a much better job in RNA secondary structure prediction than other models.}
\label{Tab.ss}
\begin{threeparttable}
\begin{tabular}{ccccccc} 
\toprule
\multirow{2}{*}{Method}     & \multicolumn{3}{c}{ArchiveII600 (3911)} & \multicolumn{3}{c}{bpRNA TS0 (1305)}  \\ 
\cmidrule(lr){2-4}\cmidrule(lr){5-7}
                            & Pre\tnote{a}   & Rec   & F1s                     & Pre   & Rec   & F1s                   \\ 
%\midrule
% RNA-FM (All dataset)        & 0.918 & 0.942 & 0.928                   & 0.700 & 0.712 & 0.696                 \\ 
\midrule
RNA-FM & \textbf{0.936} & \textbf{0.951} & \textbf{0.941}                   & \textbf{0.718} & 0.713 & \textbf{0.704}                \\
%RNA-FM (Pretrain + Finetune) & \textbf{0.924} & \textbf{0.958} & \textbf{0.939}                   & \textbf{0.684} & 0.725 & \textbf{0.694}                 \\
% RNA-FM (Pretrain + Feature)  & 0.873 & 0.913 & 0.890                   & 0.708 & 0.609 & 0.639                 \\
% RNA-FM (Random + Finetune)   & 0.861 & 0.910 & 0.882                   & 0.668 & 0.578 & 0.600                 \\
% RNA-FM (Random + Feature)    & 0.860 & 0.910 & 0.882                   & 0.632 & 0.572 & 0.577                 \\ 
\midrule
UFold                       & 0.890 & 0.926 & 0.905                   & 0.607 & \textbf{0.741} & 0.654                 \\
E2Efold                     & 0.738 & 0.665 & 0.690                   & 0.140 & 0.129 & 0.130                 \\
LinearFold                  & 0.641 & 0.617 & 0.621                   & 0.561 & 0.581 & 0.550                 \\
Mfold                       & 0.428 & 0.383 & 0.401                   & 0.501 & 0.627 & 0.538                 \\
RNAstructure                & 0.563 & 0.615 & 0.585                   & 0.494 & 0.622 & 0.533                 \\
RNAfold                     & 0.565 & 0.627 & 0.592                   & 0.494 & 0.631 & 0.536                 \\
CONTRAfold                  & 0.607 & 0.679 & 0.638                   & 0.528 & 0.655 & 0.567                 \\
SPOT-RNA                    & 0.743 & 0.726 & 0.711 
& 0.594 & 0.693 & 0.619                 \\
RNAsoft	                    & 0.665 & 0.594 & 0.622
 & 0.497 & 0.626 & 0.535                 \\
MXfold2	                    & 0.788 & 0.760 & 0.768
 & 0.519 & 0.646 & 0.558                 \\
Contextfold	                & 0.873 & 0.821 & 0.842 	
 & 0.529 & 0.607 & 0.546                 \\
Eternafold	                & 0.667 & 0.622 & 0.636 	
 & 0.516 & 0.666 & 0.563                 \\

\bottomrule
\end{tabular}
\begin{tablenotes}
        \footnotesize
        \item[a] Pre, Rec, and F1s are the macro averages of the precision, recall and F1-score, respectively.  %此处加入注释*信息
      \end{tablenotes}
\end{threeparttable}
\end{table}



Table \ref{Tab.ss} presents the accuracy of proposed RNA-FM and other advanced approaches \cite{fu2021ufold, huang2019linearfold,zuker2003mfold, reuter2010rnastructure,chen2020rna,singh2019rna,do2006contrafold,sato2021rna,lorenz2011viennarna,zakov2011rich,andronescu2003rnasoft, wayment2020rna} to secondary structure prediction.
RNA-FM outperforms all the other approaches concerning almost all metrics. In addition, RNA-FM far exceeds SPOT-RNA by a total of \textbf{22.8} points and \textbf{7.5} points on the ArchieveII600 and TS0, and distinctly higher than the SOTA (UFold) by a total of \textbf{3.4} points and \textbf{4.0} points on the ArchieveII600 and TS0, respectively, despite UFold also utilizes prior knowledge to model the probability of pairing. The superior performance demonstrates the advantages of underlying structural information encoded by RNA-FM.




Furthermore, we also conduct a head-to-head comparison of RNA-FM with UFold on ArchiveII600. Appendix Figure \ref{Fig.UFold1}(a) shows the F1 score distribution across all samples in ArchieveII600, comparing the RNA-FM with UFold, corresponding to the y-axis and x-axis in the scatter plot, respectively. The RNA-FM matches or exceeds the UFold on \textbf{85.5\%} of instances of all RNA types in the form of most points over the diagonal. We also explore the F1 score on different lengths of the input sequence, as shown in Appendix Figure \ref{Fig.UFold1}(b). Regardless of the length of input RNA sequences, RNA-FM always outperforms UFold, especially when the RNA length is over 150, suggesting that our model better predicts the secondary structure of longer RNA sequences.
Appendix Figure \ref{Fig.UFold2} presents the binary maps predicted by the model with a threshold of 0.5 and a graph view of the secondary structure predictions of two randomly selected examples. The probability maps from RNA-FM (second column) are more robust, less noisy, and much closer to the ground truth (first column) compared to those of UFold (third column). Regarding the graph-view converted from the binarized probability map, RNA-FM also generates secondary structures more similar to the ground truth than UFold.

%It demonstrates the superiority of our RNA-FM over other methods, attributed to the unsupervised pre-training on the large dataset and underlying structural information encoded by the model.
%E2Efold \cite{chen2020rna} involves two benchmarks in their experiments: 
%(1) RNAStralign \cite{tan2017turbofold} consists of 37149 structures from 8 RNA types, which is one of the most comprehensive collections of RNA structures in the market;
%(2) ArchiveII \cite{sloma2016exact} contains 3975 RNA structures from 10 RNA types, which is also a widely-used benchmark dataset for many classical RNA folding methods. 
%Here, we only their offered subset of length less than 600 due to the input limitation of our model and regard them as RNAStralign600 and ArchiveII600. The RNA-FM and ResNet32 are trained on the RNAStralign600 and directly evaluated on ArchiveII600 without re-training.

%Tab.\ref{Tab.e2efold} compares the RNA secondary structure prediction performances of E2Efold and our RNA-FMs with different initialization strategies and training schemes.
%It can be seen from the table that all of our models under different conditions outperform the E2Efold method, which demonstrates that the downstream ResNet predictor itself is superior to E2Efold.
%The pre-trained RNA-FMs always perform better than those with random initialization in different conditions, either under fine-tuning or feature-based training. 
%On the other hand, fine-tuning always precedes feature-based training across different parameter initialization. The combination of pre-trained parameters and fine-tuning result in the best performance, significantly increasing the F1 score over the E2Efold by a total of \textbf{12} points and \textbf{20} points on the RNAStralign and ArchieveII test sets without re-training, respectively.

%Fig.\ref{Fig.e2e1}(a) shows the F1 score distribution across all samples in ArchieveII, comparing the RNA-FM with E2Efold, corresponding to the y-axis and x-axis in the scatter plot, respectively. The RNA-FM matches or exceeds the E2Efold on \textbf{94.4\%} of instances of all RNA types in the form of most points over the diagonal. We also explore the relationship between the F1 score and the length of the input sequence, as shown in Fig.\ref{Fig.e2e1}(b). The RNA-FM outperforms E2Efold across all sequence lengths, especially when the RNA length is over 150.
%Fig.\ref{Fig.e2e2} presents the probability maps predicted by the model and graph view of the secondary structure predictions of two randomly selected examples. The probability maps from RNA-FM (second column) are more robust with less noise and closer to the ground truth (first column) than those from E2Efold (third column). Regarding the graph-view converted from the binarized probability map with a threshold of 0.5, RNA-FM also generates secondary structures more similar to the ground truth than E2Efold.


% \begin{table}
% \centering
% \caption{RNA secondary structure prediction performance on the E2Efold benchmark datasets. The pre-trained RNA-FM surpasses E2Efold on the F1 score either under feature-based training or fine-tuning. Note that the best RNA-FM can improve the F1 score over E2Efold by about 20\% on the test set of RNAstralign600 and the all set of ArchieveII600.}
% \begin{tabular}{ccccccccc} 
% \toprule
% \multirow{2}{*}{Model}   & \multirow{2}{*}{Initialization} & \multirow{2}{*}{Scheme}  &
% \multicolumn{3}{c}{RNAStralign600} & \multicolumn{3}{c}{ArchieveII600}  \\ 
% \cmidrule(lr){4-6}\cmidrule(lr){7-9}
% &   &  & Pre   & Rec   & F1s    & Pre   & Rec   & F1s  \\ 
% \midrule
% E2Efold & Random & - & 87.2 & 82.4 & 84.1 & 73.8 & 66.5 & 69.0\\ 
% UFold & Random & - & 78.7 & 82.5 & 80.3 & 68.9 & 71.3 & 69.7 \\
% \cmidrule(r){1-9}
% \multirow{4}{*}{RNA-FM}
% & Pre-train & Fine-tune & 95.0 & 98.9 & \textbf{96.7} & 86.0 & 95.6 & \textbf{89.8} \\
% & Random & Fine-tune & 84.1 & 99.3 & 89.0 & 62.8 & 96.3 & 71.9\\
% & Pre-train & Feature-based & 85.2 & 99.5 & 
% \underline{89.7} & 64.3 & 96.8 & \underline{72.7}\\
% & Random & Feature-based & 83.5 & 99.2 & 88.7 & 62.2 & 96.3 & 71.5\\
% \bottomrule
% \end{tabular}
% \label{Tab.e2efold}
% \end{table}









% \begin{figure}[htbp]
% \centering
% \includegraphics[width=0.7\textwidth]{figs/E2Efold/e2efold_5s.png} 
% \caption{examples} 
% \label{Fig.e2efold_5s}
% \end{figure}

% \begin{figure}[htbp]
% \centering
% \includegraphics[width=0.7\textwidth]{figs/E2Efold/e2efold_srp.png} 
% \caption{examples} 
% \label{Fig.e2efold_srp}
% \end{figure}


% \begin{figure}[htbp]
% \centering
% \includegraphics[width=0.9\textwidth]{figs/E2Efold/E2Escatter.png} 
% \caption{examples} 
% \label{Fig.e2efold_scatter}
% \end{figure}

% \begin{figure}[htbp]
% \centering
% \includegraphics[width=0.7\textwidth]{figs/E2Efold/E2Elength.png} 
% \caption{examples} 
% \label{Fig.e2efold_length}
% \end{figure}

% \begin{figure}[htbp]
% \centering
% \includegraphics[width=0.7\textwidth]{figs/SPOT-RNA/spot8208.png} 
% \caption{examples} 
% \label{Fig.spot8208}
% \end{figure}

% \begin{figure}[htbp]
% \centering
% \includegraphics[width=0.7\textwidth]{figs/SPOT-RNA/spot35615.png} 
% \caption{examples} 
% \label{Fig.spot35615}
% \end{figure}

% \begin{figure}[htbp]
% \centering
% \includegraphics[width=0.7\textwidth]{figs/SPOT-RNA/Spotscatter.png} 
% \caption{examples} 
% \label{Fig.spotrna_scatter}
% \end{figure}


% \begin{figure}[htbp]
% \centering
% \includegraphics[width=0.7\textwidth]{figs/SPOT-RNA/Spotlength.png} 
% \caption{examples} 
% \label{Fig.spotrna_length}
% \end{figure}

% \begin{figure}[htbp]
% \centering
% \includegraphics[width=0.7\textwidth]{figs/RNA-CONTACT/1u0b_A.png} 
% \caption{examples} 
% \label{Fig.1u0b_A}
% \end{figure}

% \begin{figure}[htbp]
% \centering
% \includegraphics[width=0.7\textwidth]{figs/RNA-CONTACT/1y27_X.png} 
% \caption{examples} 
% \label{Fig.1y27_X}
% \end{figure}

% \begin{figure}[htbp]
% \centering
% \includegraphics[width=0.7\textwidth]{figs/RNA-CONTACT/6az3_8.png} 
% \caption{examples} 
% \label{Fig.6az3_8}
% \end{figure}

% \begin{figure}[htbp]
% \centering
% \includegraphics[width=0.7\textwidth]{figs/RNA-CONTACT/RNAContactscatter.png} 
% \caption{examples} 
% \label{Fig.rnacontact_scatter}
% \end{figure}

% \begin{figure}[htbp]
% \centering
% \includegraphics[width=0.7\textwidth]{figs/RNA-CONTACT/RNAContactlength.png} 
% \caption{examples} 
% \label{Fig.rnacontactlength}
% \end{figure}

%\paragraph{Comparation with SPOT-RNA}
%To compare with SPOT-RNA \cite{singh2019rna}, we use their processed bpRNA-1m as our benchmark. The dataset is preprocessed by removing sequence similarity with 80\% sequence-identity cutoff and restricting their maximum sequence length below 500. The preprocessed dataset contains 13,419 sequences and is randomly split into 10,814 RNAs for training (TR0), 1300 for validation (VL0), and 1,305 for testing (TS0).

%Tab.\ref{Tab.spot} compares the RNA secondary structure prediction performances of SPOT-RNA and our RNA-FMs with different initialization strategies and training schemes. 
%Unlike the results in E2Efold experiments, RNA-FMs with random initialization are inferior to SPOT-RNA, which indicates our downstream module is slightly less suitable than their framework on this dataset. Although we have a framework with fleabite, we can get consistent results: Initializing RNA-FM with pre-training parameters is better than random initialization, and fine-tuning is much better than feature-based training, especially with pre-trained RNA-FM. Finally, our RNA-FM with the best configuration, pre-trained parameters and fine-tuning, surpasses SPOT-RNA by almost \textbf{6\%} of F1 score and \textbf{7\%} of MCC \cite{singh2019rna} on the validation and test set.

%Fig.\ref{Fig.spot}(a) shows the MCC distribution across all samples in TS0, comparing the RNA-FM with SPOT-RNA, corresponding to the y-axis and x-axis in the scatter plot, respectively. The RNA-FM matches or exceeds the SPOT-RNA on \textbf{97.2\%} of instances of all RNA types in the form of most points over the diagonal. We also explore the relationship between the MCC and the length of the input sequence, as shown in Fig.\ref{Fig.spot}(b). The RNA-FM outperforms SPOT-RNA across all sequence lengths.
%Fig.\ref{Fig.spot}(c) presents the probability maps predicted by the model and graph view of the secondary structure predictions of two randomly selected examples in the TS0. The graph-view secondary structure is generated by a binarized probability map with a threshold of 0.325, making the model achieve the best F1 score on the validation set (VL0).
%RNA-FM (second column) generates secondary structures nearly identical to the ground truth (first column) with subtle differences in the probability maps and visualizations, while the SPOT-RNA's predictions (third column) deviate from the ground truth.

% \begin{table}
% \centering
% \caption{RNA secondary structure prediction performance on bpRNA dataset. We evaluate the RNA-FM-based methods following the SPOT-RNA paradigm. The pre-trained RNA-FM outperforms SPOT-RNA (scores from their paper) on both the F1 score and the MCC, either under feature-based training or fine-tuning. Regarding MCC, our best method can improve over SPOT-RNA by up to 7\%.}
% \begin{tabular}{ccccccccccc} 
% \toprule
% \multirow{2}{*}{Method}  &
% \multirow{2}{*}{Initialization} & \multirow{2}{*}{Scheme}  &
% \multicolumn{4}{c}{Valid} &
% \multicolumn{4}{c}{Test} \\ 
% \cmidrule(r){4-7}\cmidrule(lr){8-11}
% &   &  & Pre   & Rec   & F1s   & MCC   & Pre   & Rec   & F1s   & MCC    \\ 
% \midrule
% SPOT-RNA & Random  & -  & 71.2 & 56.3 & 62.9 & 63.2 & 70.9 & 56.0  & 62.6 & 62.9  \\ 
% UFold & Random & - & - & - & - & - & 60.7 & 74.1 & 65.4 & 66.2 \\
% \midrule
% \multirow{4}{*}{RNA-FM} 
% & Pre-train & Fine-tune & 75.2 & 67.2 & \textbf{69.1} & \textbf{70.0} & 75.7 & 67.3 & \textbf{69.4} & \textbf{70.2} \\
% & Random & Fine-tune & 63.7 & 59.2 & 59.3 & 60.1 & 63.6 & 60.2 & 59.6 & 60.5  \\
% & Pre-train & Feature-based & 67.6 & 63.8 &
% \underline{64.0}  & \underline{64.7} & 67.7 & 64.0  & \underline{64.3} & \underline{64.9}  \\
% & Random & Feature-based & 59.4 & 59.5 & 56.9 & 58.0  & 59.3 & 60.1 & 57.1 & 58.2  \\
% \bottomrule
% \end{tabular}
% \label{Tab.spot}
% \end{table}

% \begin{figure}[!th]
% \centering
% \includegraphics[width=0.9\textwidth]{figs/SPOT-RNA/spot.png} 
% \caption{\textbf{Detailed head-to-head comparison between RNA-FM and SPOT-RNA on the bpRNA dataset.} We follow the testing paradigm from SPOT-RNA. \textbf{a.} Scatter plots of MCCs with the performance of RNA-FM as the y-axis and the performance SPOT-RNA as the x-axis. Each point represents an RNA structure. Almost all the points are above the diagonal, which means RNA-FM outperforms SPOT-RNA on nearly all the instances. \textbf{b.} MCCs as a function of RNA sequence lengths. RNA-FM outperforms SPOT-RNA across all the lengths. \textbf{c.} Probability maps and graph view of secondary structure predictions of two randomly selected examples. RNA-FM generates 
% secondary structures nearly identical to the ground truth with subtle differences in the probability maps and visualizations, while the SPOT-RNA's predictions deviate from the ground truth.} 
% \label{Fig.spot}
% \end{figure}

% \begin{table}
% \centering
% \caption{New results of RNA secondary structure prediction}
% \begin{tabular}{ccc} 
% \toprule
% Model                           & ArchieveII-393 (600) & bpRNA-1m Test (TS0: 1305)  \\ 
% \midrule
% RNA-FM (Pretrain  Finetune)     & \textbf{0.936}                & \textbf{0.702}                      \\
% RNA-FM (Pretrain  Feature-base) & 0.897                & 0.649                      \\ 
% \cmidrule(r){1-3}
% UFold                           & 0.905                & 0.654                      \\
% SPOT-RNA                        & 0.711                & 0.619                      \\
% RNAStructure                    & 0.628                & 0.532                      \\
% RNAsoft                         & 0.622                & 0.535                      \\
% RNAfold                         & 0.63                 & 0.536                      \\
% MXfold2                         & 0.769                & 0.558                      \\ 
% Mfold                           & 0.62                 & 0.538                      \\
% Linearfold                      & 0.647                & 0.551                      \\
% Eternafold                      & 0.636                & 0.563                      \\
% e2efold                         & 0.554                & 0.189                      \\
% Contrafold                      & 0.665                & 0.567                      \\
% Contextfold                     & 0.842                & 0.539                      \\
% \bottomrule
% \end{tabular}
% \end{table}





%\subsubsection{RNA 3D Closeness Prediction}
\textbf{RNA 3D closeness} indicates that arbitrary two bases have tertiary interaction if their distance is under a certain threshold, which originates from the “contact” concept in the protein field.
Although secondary structure can reveal parts of the relationship between base pairs of RNA, it is merely a prior result and usually a constraint applied to the subsequent structure modelling. To obtain more precise structures, researchers propose many informative and challenging tasks for generating more strict constraints for downstream modelling methods.
RNA 3D closeness utilizes a 2D matrix to represent pairwise tertiary inter-nucleotide interactions rather than the 2D flat relationship in secondary structure. The distance is defined as the minimal atomic distance of arbitrary bases, and the threshold is set as 8\r{A}.

%\textit{Evaluation} 
We select the benchmark datasets used by RNAcontact \cite{sun2021rna}, which is constructed based on a set of non-redundant RNA 3D structures from Leontis and Zirbel (2012) (Version 3.99, 2019-11-06), containing 1786 entries with resolution $<4$\r{A} initially. Following preprocessing steps \cite{sun2021rna}, we remove sequences with length $<32nt$ or $>1000nt$, with redundancy over $80\%$ as well as with too few positive points ($<5$). Finally, 221 sequences left are used for training (which we denoted as TR221), and 80 sequences for testing (denoted as TE80). The ground truth is computed from their PDB files following the steps above. The other features involved in the RNAcontact pipeline include the covariance of MSA and the secondary structure predicted by the PETfold \cite{seemann2008unifying} based on MSA.
Appendix Figure \ref{Fig.rnacontact} compares RNA 3D closeness prediction performance of RNAcontact and RNA-FMs with different initialization strategies and training schemes. Table \ref{Tab.rnacontact} then presents the long-range top precisions of different models on the TE80 in detail. We focus on switching input representation features on the same ResNet32 architecture to achieve fair comparisons. Note that we only train ResNet32 once instead of averaging an ensemble of 100 models mentioned in RNAcontact \cite{sun2021rna}. A simple ResNet32 with RNA-FM embeddings achieves SOTA in all aspects and a great improvement over RNAcontact. Then we find that the long-range Top-L precision \cite{wang2017accurate} improves \textbf{7} points when using our RNA-RM embeddings rather than using the covariance and PETfold prediction results. To pursue better performance on such a small dataset, we also adapt the transfer learning by initializing ResNet32 with the parameters pre-trained on bpRNA-1m in the above secondary prediction task. The transfer learning improves the performance significantly by another \textbf{20} points. Obviously, for the small-scale dataset, the pre-trained parameter is critical for both the backbone and downstream model. In addition, The long-range Top-L precision of the model with RNA-FM embedding is always higher than those with MSA covariances or the PETfold secondary structure, which indicates our embeddings from pure sequences present much more useful information than these features from MSA and further eliminate the time-consuming multiple sequence alignment generation step.

\begin{table}
\centering
\caption{\textbf{RNA 3D closeness prediction on the RNAcontact Test80 dataset (long-range top-precision).} The first row shows the results of RNAcontact with the sequence encoding as input (ensemble result of 100 models). The rest rows contain the results predicted by ResNet32 models with different feature inputs. The model with the RNA-FM embedding has already outperformed models with all the other features significantly (20\% performance improvement on Top-L precision over RNAcontact), and its performance can be further boosted dramatically by transfer learning (33\% performance improvement Top-L precision over RNAcontact). Unlike other features generated from MSA data, RNA-FM embedding is obtained from pure sequences, eliminating the time-consuming step of doing multiple sequence alignment.}
\label{Tab.rnacontact}
\begin{threeparttable}
\begin{tabular}{ccccccccc} 
\toprule
\multirow{2}{*}{Features} & \multirow{2}{*}{Source} & \multirow{2}{*}{Model} &
\multicolumn{4}{c}{Long-Range Top Precision}  \\ 
\cmidrule(lr){4-7}
&       &       & L/10 & L/5  & L/2  & L/1    \\ 
\midrule
%\multirow{3}{*}{RNAcontact} 
Seq\tnote{a}     & Seq   & RNAcontact (100 ensemble)   & 0.48 & 0.45 & 0.40 & 0.33   \\
%& Cov       & MSA   & 100   & No   & 0.81 & 0.80 & 0.73 & 0.59   \\
%& Cov + SS  & MSA   & 100   & No   & 0.89 & 0.88 & 0.81 & 0.66   \\ 
\midrule
%\multirow{4}{*}{ResNet32}   
Cov      & MSA   & \multirow{3}{*}{ResNet32 (random)}  & 0.57 & 0.54 & 0.45 & 0.34   \\
Cov + SS\tnote{b}   & MSA   &  & 0.62 & 0.61 & 0.54 & 0.46   \\
RNA-FM\tnote{c} & Seq   &    & \underline{0.68} & \underline{0.66} & \underline{0.62} & \underline{0.53}   \\
\midrule
RNA-FM & Seq    & ResNet32 (transfer)    & \textbf{0.88} & \textbf{0.85} & \textbf{0.79} & \textbf{0.66}   \\
\bottomrule

\end{tabular}
\begin{tablenotes}
        \footnotesize
        \item[a] \textit{Seq} means sequence one-hot encoding
        \item[b] \textit{Cov} means MSA covariances; \textit{SS} means secondary structure predicted by the PETfold based on MSA; \textit{+} means a combination of features by a channel-wise concatenation.
        \item[c]  \textit{RNA-FM} means the RNA-FM embeddings. 
      \end{tablenotes}
\end{threeparttable}

\end{table}




Appendix Figure \ref{Fig.rnacontact}(a) shows the long-range Top-L precision distribution across all samples in TE80, comparing the ResNet32 with different input features. The y-axis of the plot represents RNA-FM embedding with transfer learning (\textit{RNA-FM(TL)}), and the x-axis represents the combination of MSA covariance and secondary structure predicted by the PETfold as input (\textit{Cov$+$SS}), which RNAcontact requires to generate from RNA MSA data. The RNA-FM with transfer learning matches or exceeds the MSA feature combination on \textbf{77.5\%} of instances of all RNA types in the form of most points over the diagonal. We also explore the relationship between the Top-L precision and the input RNA sequence length, as shown in Appendix Figure \ref{Fig.rnacontact}(b). The RNA-FM embedding with transfer learning outperforms the MSA features across all sequence lengths.

% Fig.\ref{Fig.rnacontact}(a) shows the long-range Top-L precision distribution across all samples in TE80, comparing the ResNet32 with different input features. The y-axis of the plot represents RNA-FM embedding (\textit{RNA-FM}), and the x-axis represents the combination of MSA covariance and secondary structure predicted by the PETfold as input (\textit{Cov$+$SS}), which RNAcontact requires to generate from RNA MSA data. The RNA-FM matches or exceeds the MSA feature combination on \textbf{61.3\%} of instances of all RNA types in the form of most points over the diagonal. We also explore the relationship between the Top-L precision and the input RNA sequence length, as shown in Fig.\ref{Fig.rnacontact}(b). The RNA-FM embedding outperforms the MSA features across all sequence lengths.

Appendix Figure \ref{Fig.rnacontact}(c) presents the predicted probability maps of two randomly selected examples in the TS0. With the standalone RNA-FM embedding (\textit{RNA-FM}, third column) as the input, the downstream model has already generated visualizations much closer to the ground truth (first column) than other features. Furthermore, we can perform far better than the other methods by applying above mentioned transfer learning (second column).

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.74\textwidth]{figs/dist/distance+ren.png}
    \caption{\textbf{RNA 3D distance prediction performance on the RNAcontact TE80 dataset and 3D reconstruction of RNA.} We use U-Net as the downstream base model with different input features. \textit{Seq}: sequence one-hot encoding; \textit{Cov} : MSA covariances; \textit{SS}: secondary structure from E2Efold; \textit{Emb}: embedding from RNA-FM; $+$: features combination via channel-wise concatenation. \textbf{a.} MSE Scatter plots with the performance of \textit{RNA-FM$+$Seq} as y-axis and \textit{SS$+$Seq} as x-axis. Each point represents an RNA structure. Almost all points are below the diagonal, which indicates that the RNA-FM embedding is superior over other features on nearly all instances. \textbf{b.} ${R^2}$ measurement as a function of RNA sequence lengths. \textit{RNA-FM$+$Seq} significantly outperforms that of \textit{SS$+$Seq} across all the lengths \textbf{c.} Probability maps of two randomly selected examples. Integrating RNA-FM embedding into input can significantly improve the model's performance. \textbf{d.} The probability maps and binary maps of an instance from PDB (5m73-1-A) are generated by different predictors. The graph views are obtained by jViz.Rna 4.0 \cite{shabash2017numerical}. Finally, the 3D structures are optimized and reconstructed by 3dRNA.}
    \label{fig:dist}
\end{figure}



%\subsubsection{RNA Distance Prediction}

\textbf{RNA distance map} defines the distance of arbitrary bases in the primary sequence. In the past few years, more and more complex protein structure prediction tasks have been thoroughly studied. For instance, trRosetta \cite{yang2020improved} can predict the distance between two amino acids and the orientation formed by their atom planes, and Alphafold \cite{alquraishi2019alphafold} can even directly predict the 3D structure of target proteins with high precision. However, 3D structure prediction in the RNA field is still an underdeveloped yet critical task. To ultimately approach this objective, we define a relatively new task for predicting this distance regression task, which can offer more information to downstream 3D folding methods than RNA secondary structure prediction \cite{singh2019rna} and 3D closeness prediction \cite{sun2021rna} mentioned above.

\begin{table}[!t]
\centering
\caption{\textbf{RNA 3D distance prediction performance on the RNAcontact TE80 dataset.} All the experiments are based on U-Net with different inputs. The model with standalone RNA-FM embedding can obtain a lower MSE than the model with sequence encoding and secondary structure information. When combined with sequence encoding, the RNA-FM embedding outperforms all the other feature combinations across different evaluation criteria. When combining sequence encoding, MSA covariances, and the RNA-FM embedding, we can reach the awe-inspiring performance of PMCC as high as 0.8313.}
\label{table:dist}
\begin{threeparttable}

\begin{tabular}{ccccc} 
\toprule
%Model &
Features & MSE   & R\textsuperscript{2} & PA(\%)   & PMCC    \\ 
\midrule
%\multirow{6}{*}{U-Net}  
Seq\tnote{a}             & 0.0615  & 0.3652  & 45.77  & 0.4024   \\
SS $+$ Seq        & 0.0387  & 0.6875  & 81.45  & 0.7826   \\
SS $+$ Cov $+$ Seq    & 0.0338  & 0.7821  & 85.44  & 0.8218   \\ 
\midrule  %\cmidrule(lr){1-5}
RNA-FM             & 0.0353  & 0.7542  & 84.62  & 0.8143   \\
RNA-FM $+$ Seq      & \underline{0.0322}  & \underline{0.7824}  & \underline{86.13}  & \underline{0.8261}   \\
RNA-FM $+$ Cov $+$ Seq   & \textbf{0.0319}  &
\textbf{0.7921} & \textbf{88.83}  & \textbf{0.8313}     \\
\bottomrule

\end{tabular}
\begin{tablenotes}
        \footnotesize
        \item[a]\textit{Seq} means sequences using one-hot encoding. \textit{Cov} means MSA covariances. \textit{SS} means secondary structures predicted by E2Efold. \textit{RNA-FM} means RNA-FM embedding. \textit{+} means a combination of features.
      \end{tablenotes}
\end{threeparttable}
\end{table}

%\textit{Evaluation} 
The dataset used for RNA distance prediction is the same as the benchmark used in the RNAcontact, as described in the previous paragraph. We generate distance maps for RNA sequences from their PDB files according to the minimal atomic distance of arbitrary bases. Then we limit the distance from 0 to 20 \r{A} and regard the value over 20 as 20. Finally, we use 20 to normalize the distance values and obtain a normalized distance map with elements falling into $[0,1]$.
Table \ref{table:dist} summarizes the distance prediction performance of the model with different inputs on TE80. The U-Net with RNA-FM embeddings as input significantly outperforms sequences across different evaluation criteria with almost a 39\% increase in $R^2$, a 39\% increase in Pixel Accuracy (PA), a 41\% increase in (PMCC), and 0.026 (42\%) decrease in MSE. Furthermore, when simply combining RNA-FM embeddings with sequences, we already obtained a model with a slightly better MSE over approaches that take advantage of all features, such as secondary structure (\textit{SS}) and MSA covariance (\textit{Cov}). It suggests that our generated RNA-FM embedding contains the most explicitly helpful information for this task. Notice that RNA-FM is a pure single-sequence method, eliminating the time-consuming MSA searching step. Moreover, when combining RNA-FM embeddings with sequences and MSA covariances, we obtain the best model with the lowest MSE at 0.0319. Evaluated $R^2$ also achieves the highest value of 43\% over the standalone sequence.
%1\% over the combination of other features (\textit{SS$+$Cov$+$Seq}.   %\yu{Highligh again that our method is a pure single-sequence method, which eliminates the time-consuming MSA searching step.}
%\paragraph{Downstream Module \& Training Scheme} 
%Considering the difficulty of this pairwise regression task, we build our downstream module upon classical U-Net architecture \cite{ronneberger2015u} with channels numbers of $[64,128,256,128,1026]$ for this task. We train the model with a batch size of $=1$, an optimizer of Adam, and a learning rate of $=0.001$. \cite{DBLP:journals/corr/KingmaB14}. We trained around $50$ epochs and selected the best validation results for each input feature. To evaluate the effectiveness of our embeddings, we try several input types, including sequences, RNA-FM embeddings, the secondary structures predicted by E2Efold, and their combinations. To match the dimension with other features, we also employ an MLP to map our embedding dimension from 640 to 128. Furthermore, the secondary structure features (SS) used in this section are obtained from E2Efold \cite{chen2020rna}.

% \begin{figure}[h!]
%     \centering
%     \includegraphics[width=1.0\textwidth]{figs/dist/dist.png}
%     \caption{Distance prediction examples.}
%     \label{fig:dist}
% \end{figure}
% \begin{figure}[h!]
%     \centering
%     \includegraphics[width=1.00\textwidth]{figs/dist/dist_len.png}
%     \caption{Distance prediction evaluation, MSE and R-squared.}
%     \label{fig:dist_metric}
% \end{figure}
%\paragraph{Results} 


Detailed analysis is conducted between two models with different input features. One is the combination of sequences and RNA-FM embeddings (\textit{RNA-FM$+$Seq}), and the other is the combination of sequences and the predicted secondary structures (\textit{SS$+$Seq}). Figure \ref{fig:dist}(a) shows the MSE of these two models across all instances in TE80, with \textit{RNA-FM$+$Seq} as the y-axis and \textit{SS$+$Seq} as the x-axis. The \textit{RNA-FM$+$Seq} is better than the \textit{SS$+$Seq} on \textbf{94.2\%} of instances in the form of most points below the diagonal. We also explore the relationship between the $R^2$ and the input RNA sequence length, as shown in Figure \ref{fig:dist}(b). When combined with sequence one-hot encoding, RNA-FM embedding outperforms the predicted secondary structure across all the RNA lengths.

As shown in Figure \ref{fig:dist}(c), we can see that our embedding feature enables the model to capture specific details of distance data while only providing sequential and secondary-structure data is not sufficient. The standalone sequential data does the worst in our experiment and only captures distance values on the diagonal.





%%%%% collapse
% \begin{figure}[!htbp]
%     \centering
%     \includegraphics[width=0.8\textwidth]{figs/dist/distance.png}
%     \caption{\textbf{RNA 3D distance prediction performance on the RNAcontact TE80 dataset.} We use U-Net as the downstream base model with different input features. \textit{Seq} means one-hot encoding of sequence; \textit{Cov} means MSA covariances; \textit{SS} means secondary structure predicted by the E2Efold; \textit{Emb} means the feature from RNA-FM; $+$ means a combination of features by a channel-wise concatenation. \textbf{a.} Scatter plots of MSE (the lower, the better) with the performance of \textit{RNA-FM$+$Seq} as the y-axis and that of \textit{SS$+$Seq} as the x-axis. Each point represents an RNA structure. Almost all the points are below the diagonal, which means that the RNA-FM embedding beats the other features on nearly all the instances. \textbf{b.} ${R^2}$ (the higher, the better) as a function of RNA sequence lengths. \textit{RNA-FM$+$Seq} significantly outperforms that of \textit{SS$+$Seq} across all the lengths \textbf{c.} Probability maps of two randomly selected examples. Integrating the RNA-FM embedding into the input can significantly improve the model's performance compared with the standalone sequence encoding or the combination of sequence encoding and other features.}
%     \label{fig:dist}
% \end{figure}


% \begin{figure}[htbp] %[!th]
% \centering
% \includegraphics[width=1\textwidth]{figs/3D-model/ren+5m73.pdf} 
% \caption{\textbf{3D reconstruction of RNA.} The probability maps and binary maps are generated by different predictors. The graph views are obtained by jViz.Rna 4.0 \cite{shabash2017numerical}. The 3D structures are modelled by 3dRNA. \textbf{a.} An instance from PDB (5m73-1-A). \textbf{b.} The DCS-PK of Zika Virus.}
% \label{Fig.3dmodel}
% \end{figure}

\textbf{RNA 3D reconstruction} is the ultimate goal of RNA structure prediction. This section presents the results by combing RNA-FM with existing optimizing tools to obtain 3D approximates. Specifically, secondary structures are generated by the proposed RNA-FM and UFold. Then, their 3D structures can be optimized with RNA 3D modelling tools, including 3dRNA and FARFAR2 \cite{watkins2020farfar2}. We employ 3dRNA here to optimize the 3D structure upon its secondary structure.  
By applying our model to the PDB dataset \cite{singh2019rna}, we fine-tune the secondary structure predictor. The performance is shown in Supplementary Table 5. One of the examples in the PDB test set, namely TS1, is shown in Figure \ref{fig:dist}d. Notice that RNA-FM produces around $7.91$ RMSD, which is significantly better than the results produced by UFold ($25.70$). Interestingly, even the ground truth secondary structure produces a higher RMSD ($13.96$), suggesting that the error may actually come from the 3D structure modelling process. 
Moreover, we also apply it to the DCS-PK element in the 5' UTR flanking region of the Zika virus (ZIKV) \cite{li2018integrative}. The DCS-PK is a pseudoknot found in the coding region \cite{liu2013novel}, which helps enhance genome cyclization during replication. Due to the lack of ground truth for some 3D structures, RMSD for each prediction is unavailable, but RNA-FM produces more precise secondary structures than UFold for these targets, as shown in Appendix  Figure \ref{Fig.3dmodel-ren}.


To eliminate the deviation from the above 3D optimizing process, we also developed an end-to-end differentiable RNA 3D structures prediction model for evaluating the RNA-FM embedding by comparing them with the raw sequence inputs. 
As shown in Supplementary Table 6, on all the RNApuzzle structures, RNA-FM representations improve RNA 3D structure prediction greatly, with the average RMSD being around 4\AA{}.
It is consistent with all the above prediction tasks, suggesting that RNA-FM leads to more accurate RNA structure prediction.




\begin{figure}[htbp] %[!th]
\centering
\includegraphics[width=0.85\textwidth]{figs/3D-model/covid_samples_all.png} 
\caption{\textbf{RNA-FM predicts SARS-CoV-2 genome regulatory element secondary structures and the virus variant evolutionary trend.} \textbf{a.} Diagram of key regulatory segments sampled from the whole SARS-CoV-2 genome. \textbf{b.} Violin plots of the secondary structure prediction performance of segments mentioned above. RNA-FM can precisely predict the secondary structure of these segments. \textbf{c.} The visualization of RNA secondary structure predictions in 5'UTR. The prediction results are almost the same as the ground truth. \textbf{d.} The phylogenetic tree generated by FastME \cite{desper2002fast}. We treat it as the ground truth of the evolutionary trend of SARS-CoV-2, from Alpha types to Omicron variants. \textbf{e.} The trajectory inference of COVID-19 evolutionary trend with the RNA-FM genome-level embeddings. The results are highly consistent with the ground truth.}
\label{Fig.covid}
\end{figure}

%\siqi{there are only two targets, explain more details for both of them? RMSD info is not available in REN's lab target \jiayang{no ground truth} Ash: Jiayang, take a look}

\paragraph{RNA-FM facilities SARS-CoV-2 genome secondary structure and evolution study.}
COVID-19 has caused significant losses in properties and life in the past years, and detailed studies of the virus genome structure and its evolution are vital to prevent the next pandemic. We conduct such an investigation and apply RNA-FM to the whole genome of Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2), the causal pathogen of the epidemic \cite{wu2020new}.
Firstly, we utilize the well-trained RNA-FM to predict the secondary structures of the key regulatory segments of the SARS-CoV-2 reference genome (Refseq accession number NC\_045512.2). As shown in Figures \ref{Fig.covid}(a) and (b), we sample 3'UTR, 5'UTR, and other segments from the entire genome with the length of 29870 based on the work of Cao \textit{et al} \cite{cao2021architecture}. Our model precisely predicts the majority of them. The results indicate that our model can effectively perform RNA secondary structure prediction task on an independent test set, with desirable generalization property. The predictions of the fragments (in black boxes) in 5'UTR are visualized in Figure \ref{Fig.covid}(c), which are almost the same as the ground truth.
Secondly, we explore the evolution of different COVID variants by applying RNA-FM to the whole genomes. Although RNA-FM is not initially designed for whole genome modelling, we assume that aggregation of the RNA-FM embedding extracted from fragments of the whole genome can still characterize the genome, benefiting the study of the virus genome evolution. The calculation of genome-level RNA-FM embedding can be seen in the Method section. As shown in Figure \ref{Fig.covid}(e), the trajectory inference with the RNA-FM embedding of the virus genome is roughly in line with the phylogenetic tree generated by FastME \cite{desper2002fast}, which is considered the ground truth (Figure \ref{Fig.covid}(d)). The predictive evolution trend of COVID-19 begins with the Alpha type and ends up with the newest Omicron variant by April 2022, especially from Omicron 21K to Omicron 21L. 
Notice that RNA-FM is trained using merely the ncRNA sequences and unsupervised learning. We directly apply the trained model to the COVID-19 dataset without any fine-tuning or using any labelling information about the virus. It suggests that the regulatory elements of the virus genome could be vital for the virus variant evolution. Also, the RNA-FM framework can dig up core structure messages and evolutionary trend information of COVID-19 and its variants. Further development of the model has the potential to facilitate the research of COVID-19 and other pandemics.


%\paragraph{RNA Function Prediction }
%\paragraph{mRNA untranslated region's function prediction.}

%\begin{figure}[htbp]
%\centering
%\includegraphics[width=0.7\textwidth]{figs/UTR/UTRboxMAE.png} 
%\caption{examples} 
%\label{Fig.UTRboxMAE}
%\end{figure}

%\begin{figure}[htbp]
%\centering
%\includegraphics[width=0.7\textwidth]{figs/UTR/UTRboxMSE.png} 
%\caption{examples} 
%\label{Fig.UTRboxMSE}
%\end{figure}

%\subsection{RNA-protein Interaction Prediction}



% \yu{The logic of this application is not clear. We want to show that RNA-FM feature can achieve similar performnace as the in vivo RNA SS feature. Please point out this logic.}

\begin{table}[!t]
\centering
\caption{\textbf{ RBP-RNA binding prediction AUPRCs on the HeLa dataset with different RBPs.} CNN models with different input features are compared in this experiment. \textit{RNA-FM$+$Seq} and \textit{RealSS$+$Seq} outperform \textit{Seq} on all RBPs except for METTL14. Although the \textit{RealSS$+$Seq} achieves a higher mean AUPRCs than \textit{RNA-FM$+$Seq}, the latter surpasses the former on nearly half of RBPs, which demonstrates the effectiveness of the RNA-FM embedding.}
\label{RBP-tab}
\begin{threeparttable}

\begin{tabular}{cccc}
\toprule
RBPs          & Seq\tnote{a}         & RNA-FM $+$ Seq\tnote{b}   & RealSS $+$ Seq\tnote{c}   \\ 
\midrule
ELAVL1     & 0.946       & \textbf{0.950}          & 0.938       \\ 
UPF1       & 0.933       & 0.934         & \textbf{0.956}       \\
TIA1       & 0.917       & \textbf{0.929}         & 0.925       \\
METTL14    & \textbf{0.889}       & 0.866         & 0.876       \\
HNRNPC     & 0.874       & \textbf{0.883}         & 0.873       \\
CSTF2      & 0.860        & 0.867         & \textbf{0.870}        \\
TIAL1      & 0.853       & 0.852         & \textbf{0.869}       \\
U2AF65     & 0.852       & \textbf{0.899}         & 0.886       \\
eIF4AIII   & 0.818       & 0.814         & \textbf{0.829}       \\
WTAP       & 0.804       & 0.801         & \textbf{0.843}       \\
HNRNPU     & 0.798       & \textbf{0.829}         & 0.821       \\
PTBP1      & 0.793       & 0.799         & \textbf{0.840}        \\
PTBP1PTBP2 & 0.743       & 0.749         & \textbf{0.775}       \\
EIF4A3     & 0.738       & 0.751         & \textbf{0.758}       \\
U2AF2      & 0.712       & 0.705         & \textbf{0.764}       \\
METTL3     & 0.709       & \textbf{0.750}          & 0.704       \\
YTHDF2     & 0.610        & \textbf{0.638}         & 0.629       \\
\midrule
Mean    & 0.815 & \underline{0.824}   & \textbf{0.833} \\
\bottomrule
\end{tabular}
\begin{tablenotes}
        \footnotesize
        \item[a] \textit{Seq} means one-hot encoding of sequence (4 dims); \item[b] \textit{RNA-FM$+$Seq} combines the RNA-FM embedding (640 dims reduced to 1 dim) and the sequential one-hot encoding; \item[c] \textit{RealSS$+$Seq} means the combination of the structural score by the icSHAPE experiment with the sequence data.  %此处加入注释*信息
      \end{tablenotes}
\end{threeparttable}



\end{table}

% \begin{figure}[h!]
% \centering
% \subfigure[histogram]{
% \label{RBP1}
% \includegraphics[width=0.5\textwidth]{figs/PrismNet/Histogram.pdf}}
% \subfigure[violin plot]{
% \label{RBP2}
% \includegraphics[width=0.4\textwidth]{figs/PrismNet/violin.pdf}}
% \caption{Compared performance}
% \label{RBP}
% \end{figure}
\begin{figure}[!t]
    \centering
    \includegraphics[width=0.9\textwidth]{figs/interaction/Interaction.png}
    \caption{\textbf{RNA-FM embedding facilitates protein-RNA interaction prediction.} \textbf{a.} The deep learning framework for RBP-RNA interaction prediction in this experiment. The CNN model initially takes the combination of sequences and the structural scores offered by icSHAPE as input, while we substitute the experimental icSHAPE score with the RNA-FM embedding. \textbf{b.} Violin plots of AUPRCs of the model with different input features. Combining RNA-FM embedding with sequence encoding (\textit{RNA-FM$+$Seq}) can improve the median and the quartiles of AUPRCs over sequence standalone, approaching the performance generated by the experiment-measured structural information (\textit{RealSS$+$Seq}). \textbf{c.} Histogram plots of AUPRCs on different proteins with the \textit{Seq} as the baseline, corresponding to the vertical line across the origin point. The \textit{RNA-FM$+$Seq} outperforms the \textit{Seq} in most cases and sometimes achieves even better performance than the \textit{RealSS$+$Seq}, which shows the effectiveness of the RNA-FM features.}
    \label{fig:RBP}
\end{figure}

\paragraph{RNA-FM carries secondary structure information for RNA-protein interaction modelling.} Protein-RNA interactions are of vital importance in various cellular activities, including cell-signalling, post-transcriptional regulations, and protein synthesis \citep{wei2021protein}. We reproduce PrismNet \cite{sun2021predicting}, which includes \emph{in vivo} RNA secondary structure profiles for RNA-protein interaction prediction.  After that, we adopt RNA in the HeLa cell as the dataset for RNA binding protein prediction application and divide them into several sub-datasets according to different corresponding RBPs. The secondary structures of RNA are generated by icSHAPE (\emph{in vivo} click selective 2'-hydroxyl acylation and profiling experiment) \citep{spitale2015structural} in the HeLa cell environment. Then, we use our method to generate corresponding embeddings for all the sequences to replace the real secondary structures in PrismNet and make a comparison, evaluating the difference between the outputs from RNA-FM and the \emph{in vivo} secondary structure profiles.

For evaluation, we calculate AUPRCs on three streamlines (sequence only, sequence with real secondary structure, and sequence with RNA-FM) for comparison, as shown in Table \ref{RBP-tab} and Figure \ref{fig:RBP}(c). RNA-FM embeddings with sequences achieve the best performance on nearly half of the subsets. Their performance is even comparable to the real secondary structure with sequences, suggesting that embeddings from RNA-FM provide sufficient information as real secondary structures. Furthermore, taking advantage of RNA-FM embeddings helps the original model improve performance over models only with sequential information. Figure \ref{fig:RBP}(b) shows AUPRC violin plots for the three mentioned combinations as well as three other methods including RCK \citep{orenstein2016rck}, DeepBind \citep{alipanahi2015predicting}, GraphProt \citep{maticzka2014graphprot}. Our ``RNA-FM+Seq'' achieves close results compared to PrismNet with ``Real SS+Seq''. The RNA-protein interaction prediction results further illustrate that our embedding can learn sufficient information about secondary structures from raw sequences, which benefits the downstream functional prediction.



% PrismNet \citep{sun2021predicting} is an effective tool to predict RBP-RNA binding sites based on the information from RNA sequence and secondary structures. Taking RNA sequences and in vivo secondary structures as inputs, PrismNet models can output a score for every binding site by evaluating every nucleotide position to determine whether it’s a binding site. The whole architecture of PrismNet is shown in Fig.\ref{fig:RBP}(a).

% \paragraph{Dataset}
% % They import binding sites of 168 RBPs including 134 RBPs from the ENCODE Project and 56 RBPs from POSTAR. Regrading secondary structures, they utilize icSHAPE to generate a comprehensive data set of RNA secondary structures in seven cell types: K562, HepG2, HEK293, HEK 293T, HeLa, H9, and mES cells. So the data sets consist of multiple sub-data set for different RBP and RNA cell types. For every RBP with an available CLIP experiment, PrismNet trained a model learning both from RNA sequences and secondary structures for a certain cell type.
% HeLa cell is one of the most important cells in scientific research. It’s the oldest and most commonly used human cell line. RNA dataset in HeLa cell is very appropriate for us to evaluate our RNA-FM embeddings method in PrismNet pipeline. The secondary structures are generated by icSHAPE (\emph{in vivo} click selective 2'-hydroxyl acylation and profiling experiment) \citep{spitale2015structural} in the HeLa cell enviroment. The dataset contains several sub-dataset for different corresponding RBPs like ELAVL1, UPF1, etc. imported from the ENCODE Project \citep{van2020large} and POSTAR \citep{hu2017postar}. For every RBP with an available CLIP experiment, PrismNet trained a model learning both from RNA sequences and secondary structures to predict RBP binding on RNAs. 


% \paragraph{Results}
% First we reproduce PrismNet results including two types of models. One is only trained from sequences as baselines, the other is trained from sequences and in vivo secondary structures as the best performance of PrismNet. Then we use our method to generate corresponding embeddings for all the sequences to replace the real secondary structures while keeping the whole neural network unchanged. After training, we compare the performance of the three streamlines: only sequence, sequence and real SS, sequence and RNA-FM. We calculated AUPRCs on each sub-dataset for comparison shown in Tab.\ref{RBP-tab} and Fig.\ref{fig:RBP}(c). The performance of combining sequences and real secondary structures is the best, while the performance of RNA-FM embeddings is competitive with the best. It means the embeddings from RNA-FM provide effective information for secondary structure. The overall performance outperform that only using sequences. Fig.\ref{fig:RBP}(b) are AUPRC violin plots for the three mentioned combinations as well as three other methods including: RCK \citep{orenstein2016rck}, DeepBind \citep{alipanahi2015predicting}, GraphProt \citep{maticzka2014graphprot}. We can see PrismNet for 'Real SS+Seq' achieves the best while the performance of PrismNet for 'RNA-FM+Seq' are close.  






\begin{figure}[!t]
\centering
\includegraphics[width=0.9\textwidth]{figs/UTR/UTR-new.png}
\caption{\textbf{5’ UTR-based mean ribosome loading (MRL) prediction.} \textbf{a.} The deep learning framework for predicting MRL, which is a metric for evaluating the protein expression level regulated by UTR. \textbf{b.} Histogram plots of MSE on models of different input features with the one of \textit{Seq} as the baseline, corresponding to the horizontal line across the origin point. The \textit{RNA-FM} outperforms the \textit{Seq} significantly on the Random Set. When combined with the secondary structure (SS) and 3D structure (3DS) information, which is predicted based on RNA-FM, our method can further improve the performance on the Human Set. \textbf{c.} ${R^2}$ as a function of RNA sequence lengths. 
The model with information based on RNA-FM significantly outperforms \textit{Seq} model across all the lengths.
}
% \caption{\textbf{5’ UTR-based mean ribosome loading (MRL) prediction.} \textbf{a.} The deep learning framework for predicting MRL, which is a metric for evaluating the protein expression level regulated by UTR. 
% \textbf{b.} Violin plots of absolute error (the lower, the better) between the ground truth and predictions of the downstream model with different input features. \textit{Seq} means the one-hot encoding of the sequence; \textit{RNA-FM} means RNA-FM embedding; \textit{RNA-FM$+$Seq} means the channel-wise concatenation of these two features. \textit{RNA-FM} and \textit{RNA-FM$+$Seq} both obtain lower MSE and higher $R^2$ than \textit{Seq} on either the synthetic data (Random7600) or the real data (Human7600). \textbf{c.} ${R^2}$ as a function of RNA sequence lengths. \textit{RNA-FM} significantly outperforms \textit{Seq} across all the lengths. \textbf{d.} Prediction performance box plots grouped by different start codons context status. The \textit{RNA-FM$+$Seq} model performs slightly better than \textit{RNA-FM} in the context of none-uAUGs or OOF uURF regarding the absolute error, while \textit{RNA-FM} is better in the context of in-frame or out-frame uAUGs. \textbf{e.} Scatter plot of MRLs of all RNA samples with \textit{RNA-FM} predictions as the y-axis and the ground truth as the x-axis. The RNA sequences in the context of uAUG with lower MRLs than those without uAUG.
% }
\label{Fig.utr}
\end{figure}
\paragraph{RNA-FM generalizes to mRNA untranslated region’s function.} We further assume that RNA-FM could directly benefit the gene expression regulation modeling, which is one of the ultimate goals in the related studies, because function partially depends on structures. 
The 5’ untranslated region is the region of a messenger RNA (mRNA) located upstream of the initiation codon. This region is important for the translation regulation by different mechanisms in viruses, prokaryotes and eukaryotes. The sequence of 5’ UTR is a primary determinant of mRNA translation efficiency, especially the concomitant coding sequence (CDS), which is responsible for target protein expression. Although RNA-FM is trained with ncRNAs and the 5'UTR is a part of an mRNA (not belong to ncRNAs), we test the versatility of RNA-FM to generalize it on handling implicit non-coding sequences of an mRNA and aid with modelling the relationship between UTR and target protein expression.
With the assistance of massively parallel reporter assays and polysome profiling methods \cite{sample2019human}, which can measure the corresponding mean ribosome load (MRL) for each UTR, we can evaluate how a UTR regulates the target protein expression level of specific CDS by predicting the MRL of a UTR.
 
%  \jiayang{Although a 5'UTR is a part of a mRNA, which does not belong to ncRNAs, we attempt to test whether our pre-trained RNA-FM can handle these kinds of unseen non-coding sequences, and expect RNA-FM to finally aid with modeling the relationship between UTR and target protein expression.} %We adopt the pipeline and model as Paul et al. \cite{sample2019human} developed. The original inputs of their model are just the one-hot embeddings of sequences (4 dims). To see the effect of our embedding generated by RNA foundation model on this function-related task, we attempt to replace or add the sequences with our embeddings. Finally, we obtain three different types of inputs, including pure sequence (Seq) in the form of one-hot encoding (4 dims), pure RNA-FM embedding (RNA-FM) of 640 dims, and the combination of these two (RNA-FM + Seq). When the input includes embedding, we will apply a linear projection to reduce the embedding dimension from 640 to 4 for matching the one-hot embedding dimension.
%  \yu{Please incorporate Jiayang's point. The result is very impressive. Our method can generalize to non-nc regulatory RNA. It's very difficult. This point should be further highlighted.}

We utilize a large-scale synthetic Human 5’UTR library \cite{sample2019human} as the dataset for the UTR function prediction task. The dataset consists of 83,919 5’UTRs of 75 different lengths and their corresponding MRLs. 7600 sequences are sampled equally at each length as a validation set, while the remainder is adopted for training. An additional dataset consisting of 7600 real human 5’UTRs with the same length distribution provided by the library is used for validation to measure the generalization of models. %We adopt the pipeline and model as Paul et al. \cite{sample2019human} developed. The original inputs of their model are just the one-hot embeddings of sequences (4 dims). To see the effect of our embedding generated by RNA foundation model on this function-related task, we attempt to replace or add the sequences with our embeddings. Finally, we obtain three different types of inputs, including pure sequence (Seq) in the form of one-hot encoding (4 dims), pure RNA-FM embedding (RNA-FM) of 640 dims, and the combination of these two (RNA-FM + Seq). When the input includes embedding, we will apply a linear projection to reduce the embedding dimension from 640 to 4 for matching the one-hot embedding dimension.
We discover that model performances on the real human set are inferior to random set due to their data distribution difference, as illustrated in Table \ref{Tab.utr}. For both datasets, the model with RNA-FM embeddings is better than the model with pure sequences. On the synthetic set, model based on RNA-FM embedding can achieve $R^2$ = 0.875 and MSE = 0.247. On the human set, it  can achieve $R^2$ = 0.816 and MSE = 0.264. In addition, we assume that if we add more structure information to the model, we can further improve the modeling accuracy. So, we add the secondary structure information and 3D structure information, both predicted based on RNA-FM, into the model. As shown in Table \ref{Tab.utr} and \ref{Fig.utr}(b), the prediction accuracy is indeed further improved. 
% Embeddings generated by our model offer more explicit information valid for protein expression prediction than only pure sequences, which is shown in Figure \ref{Fig.utr}(b). 
Besides, performance gains are consistent across all the lengths and contexts, as shown in Figure \ref{Fig.utr}(c). This application further demonstrates the generalization of RNA-FM and its practical usage for real biological problems, even if the problem is not purely related to non-coding RNAs.

% Translation initiation mainly depends on start codons and their context or position relative to a CDS, and different contexts yield different protein expression levels. Therefore, we further analyze the performance of the models on human sets under different start codon contexts, including 5’ UTR with in-frame uAUGs, out-of-frame uAUGs, out-of-frame uORFs, and none. As shown in Figure \ref{Fig.utr}(d), the median and the quartiles of the model with RNA-FM embeddings (\textit{RNA-FM}) or the combination of RNA-FM and sequence one-hot embeddings (\textit{RNA-FM$+$Seq}) always stay lower than the model with standalone sequence (\textit{Seq}) across all status, consistent with their global metric. We can also observe more subtle differences between the \textit{RNA-FM} and \textit{RNA-FM$+$Seq} in a more detailed perspective. \textit{RNA-FM$+$Seq} performs even slightly better than \textit{RNA-FM} on a square error in the case of none-uAUGs and OOF uURF. These results demonstrate that RNA-FM embeddings provide more comprehensive information than pure RNA sequences, even generalizing to the regulatory UTRs on mRNAs.



% \paragraph{Background}
% The 5' untranslated region (UTR) sequence is a primary determinant of the translation efficiency of the concomitant coding sequence (CDS) responsible for a specific function. Earlier methods can hardly accurately predict protein expression from the 5' UTR sequence alone. The weakness limits the ability to estimate the effects of genome-encoded variants and the ability to engineer 5' UTRs for precise translation control. Recently, a new polysome profiling method has been developed to measure the attribution of 5' UTR regulation in this protein expression progress \cite{sample2019human}. The method will initially create a synthetic dataset with whole gene sequences consisting of a random 5' UTR, a constant region containing the CDS for target protein expression, and a 3' UTR. Then, HEK293T cells were transfected with IVT library mRNA, and polysome fractions in these cells were collected and sequenced after 12h. For a given UTR, the relative counts per fraction were multiplied by the number of ribosomes associated with each fraction. Finally, the resulting values were summed to obtain a measured mean ribosome load (MRL) for evaluating the target protein expression level of specific CDS. Through analyzing the relationship between 5' UTR and MRL measurement described by this synthetic dataset, we can find how a 5' UTR regulates the target protein expression. (see the task framework in Fig.\ref{Fig.utr}(a)) In this section, we aim to use our pre-trained model to aid with modelling this relationship and furtherly predicting the mean ribosome loadings (MRLs) of human 5' UTR variants in the wild.

% \paragraph{Dataset}
% Human 5' UTR sequences vary in length from tens to thousands of nucleotides with a median length of 218 nucleotides. To cover as many real sequences as possible, Paul \cite{sample2019human} create a large-scale synthetic 5' UTR library based on the above pipeline. Each sequence in the dataset consists of a random 5' UTR and a constant region containing the CDS for enhanced green fluorescent protein (eGFP) and a 3' UTR. Specifically, the 5' UTR of each construct began with 25 nucleotides of defined sequence used for PCR amplification, followed by an entirely random nucleotide sequence whose length ranges from 25 to 100, which would increase the coverage of human 5' UTRs to 29\%. After polysome profiling and RNA sequencing, the library finally retained 83,919 distinct 5' UTRs of 75 different lengths and their corresponding MRLs. A subset of 7600 sequences are sampled equally at each length from this synthetic dataset as a validation set, and the remaining sequences are used for training. To measure the generalization of this model on real data, they also offer an additional dataset consisting of 7600 real human 5' UTRs with the same length distribution as the validation set.

%\paragraph{Downstream Module \& Training Scheme}
%To validate the effectiveness of our RNA-FM embeddings, we adopt the same pipeline and model as the original paper. Their model is well-designed by performing a grid search, whose best framework was as follows: Three 1D convolutional layers with 120 filters and a ReLU activation for each layer. The Third convolution layer will output 1 channel and L length features, and they will be fed into two fully-connected layers with one output node as the final prediction. The original inputs of the model are just the one-hot embeddings of sequences (4 dims). To see the effect of our embedding on this function-related task, we attempt to replace or add the sequences with our embeddings. Finally, we obtain three models with different inputs, including pure sequence (Seq) in the form of one-hot encoding (4 dims), pure RNA-FM embedding (RNA-FM) of 640 dims, and the combination of these two (RNA-FM + Seq). When the input includes embedding, we will apply a linear projection to reduce the embedding dimension from 640 to 4 for matching the one-hot embedding dimension.

% \paragraph{Results}
% Tab.\ref{Tab.utr} exhibits the MRL prediction performance of the MRL-CNN model with different input types. The model performances on the human set are always inferior to those on the random set because of their data distribution difference. Among all the trials, the model with pure RNA-FM embeddings obtains the best performance on both the synthetic set ($R^2$ = 0.859, MSE = 0.308) and human set ($R^2$ = 0.791, MSE = 0.308), while the model with pure sequences gains the worst performance on these two sets (random: $R^2$ = 0.859, MSE = 0.308; human: $R^2$ = 0.859, MSE = 0.308). This result proves that our embedding can offer more explicit information valid for protein expression prediction than sequence, which can be inferred from t-test results shown in the violin plot in Fig.\ref{Fig.utr}(b). However, a strange result remains that the performance of the model with RNA-FM embedding will slightly decrease in terms of $R^2$ and MSE, when we include the sequence into the input, although there is a subtle improvement in the MAE metric. The possible reason for this phenomenon may be due to the changing degree of the model framework. Besides, we also observe that the model with RNA-FM embeddings surpasses the model with pure sequences across all the lengths, as shown in Fig.\ref{Fig.utr}(c).

% Translation initiation mainly depends on start codons and their context and position relative to a CDS, and different contexts yield different protein expression levels. Therefore, we furtherly analyze the performance of the models on the human set under different start codon contexts, including 5’ UTR with in-frame uAUGs, out-of-frame uAUGs, out-of-frame uORFs, and none of them. As shown in Fig \ref{Fig.utr}(d), the median and the quartiles of the model with RNA-FM embeddings (\textit{RNA-FM}) or the combination of RNA-FM and sequence one-hot embeddings embeddings (\textit{RNA-FM$+$Seq}) always keep lower than the model with sequence embeddings standalone (\textit{Seq}) across all the statuses, consistent with their global metric. We can also observe more subtle differences between the \textit{RNA-FM} and \textit{RNA-FM$+$Seq} in a more detailed perspective. The \textit{RNA-FM$+$Seq} performed even slightly better than \textit{RNA-FM} on square error in the case of none-uAUGs and OOF uURF, while the latter keeps on top in the cases of in-frame and out-frame uAUGs.

% \begin{table}[!t]
% \centering
% \caption{\textbf{Mean ribosome loading (MRL) prediction performance on the Random7600 and Human7600 datasets.} Replacing the original input of MRL-CNN from sequence encoding (\textit{Seq}) to RNA-FM embedding (\textit{RNA-FM}) improves the performance. The combination of RNA-FM embedding and sequence encoding (\textit{RNA-FM$+$Seq}) does not outperform the RNA-FM embedding alone. We suppose that, for this task, the sequencing encoding information does not complement the RNA-FM embedding.}
% \label{Tab.utr}
% \begin{threeparttable}

% \begin{tabular}{ccccccc} 
% \toprule
% %Model  & 
% \multirow{2}{*}{Features} &\multicolumn{3}{c}{Random7600} &\multicolumn{3}{c}{Human7600} \\
% \cmidrule(r){2-4}\cmidrule(lr){5-7}
% & R\textsuperscript{2} & MAE & MSE & R\textsuperscript{2} & MAE   & MSE \\ 
% \midrule
% %\multirow{2}{*}{MRL-CNN}   & 
% Seq\tnote{a} &  0.826  & 0.432 & 0.351 &  0.761 & 0.426 & 0.344  \\ 
% %\midrule
% %\multirow{4}{*}{RNA-ESM} 
% RNA-FM & \textbf{0.853} & \textbf{0.404} & \textbf{0.306} & \textbf{0.791} & 0.404 & \textbf{0.308}  \\ 
% %\midrule
% RNA-FM $+$ Seq   & 0.846 & 0.406 & 0.313 & 0.784 & \textbf{0.402} & 0.312  \\
% \bottomrule
% \end{tabular}

% \begin{tablenotes}
%         \footnotesize
%         \item[a] \textit{Seq} means sequence encoding.
%         \item[b] \textit{RNA-FM} means RNA-FM embedding.
%       \end{tablenotes}
      
% \end{threeparttable}


% \end{table}


\begin{table}[!t]
\centering
\caption{\textbf{Mean ribosome loading (MRL) prediction performance on the Random7600 and Human7600 datasets.} Replacing the original input of MRL-CNN from sequence encoding (\textit{Seq}) to RNA-FM embedding (\textit{RNA-FM}) improves the performance.}
\label{Tab.utr}
\begin{threeparttable}

\begin{tabular}{ccccccc} 
\toprule
%Model  & 
\multirow{2}{*}{Features} &\multicolumn{3}{c}{Random7600} &\multicolumn{3}{c}{Human7600} \\
\cmidrule(r){2-4}\cmidrule(lr){5-7}
& R\textsuperscript{2} & MAE & MSE & R\textsuperscript{2} & MAE   & MSE \\ 
\midrule
%\multirow{2}{*}{MRL-CNN}   & 
Seq\tnote{a} &  0.860  & 0.371 & 0.277 &  0.814 & 0.375 & 0.269  \\ 
%\midrule
%\multirow{4}{*}{RNA-ESM} 
Seq $+$ SS\tnote{b} & 0.866 & 0.369 & 0.266 & 0.820& 0.370 & 0.261  \\ 
RNA-FM\tnote{c} & 0.876 & 0.360 & 0.247 & 0.816 & 0.377 & 0.264  \\ 
%\midrule
3DS\tnote{d} & 0.864 & 0.375 & 0.271 & 0.813 & 0.379 & 0.267 \\
Seq $+$ SS $+$ RNA-FM & 0.876 & 0.361 & 0.245 & 0.811 & 0.392 & 0.287  \\
Seq $+$ SS $+$ 3DS $+$ RNA-FM  & \textbf{0.882} & \textbf{0.353} & \textbf{0.236} & \textbf{0.824} & \textbf{0.368} & \textbf{0.256}  \\
\bottomrule
\end{tabular}

\begin{tablenotes}
        \footnotesize
        \item[a] \textit{Seq} means sequence encoding.
        \item[b] \textit{SS} means secondary structure, usually formatting an embedding (L$*$16) together with \textit{Seq}.
        \item[c] \textit{RNA-FM} means RNA-FM embedding.
        \item[d] \textit{3DS} means embedding extracted from 3D structure prediction framework.
      \end{tablenotes}
      
\end{threeparttable}


\end{table}