\subsection{Image data}  
Retrospective analysis was performed using CT and MR images from 20 prostate cancer patients (61 to 80 years old). The CTs were acquired on a 64-slice CT scanner (Sensation, Siemens Medical Solutions, Erlangen, Germany) using the following settings: 120 kVp, 400 mA, and 1.5 mm or 3 mm slice thickness, with in-plane spatial resolutions varying from 0.85 $\times$ 0.85 mm$^{2}$ to 1.27 $\times$ 1.27 mm$^{2}$. For each patient, an MR image was acquired on the same day as the CT with a non-contrast T1-weighted 2D turbo spin echo sequence (echo time: 12 ms or 13 ms, repetition time: 523 ms to 784 ms, flip angle: 150o) on a 1.5 T MR scanner (Sonata, Siemens Healthcare, Erlangen, Germany). MR images had slice thickness of 5 mm and in-plane spatial resolutions ranging from 0.71 $\times$ 0.71 mm$^{2}$ to 0.94 $\times$ 0.94 mm$^{2}$.  Thirty slices covering the prostate region were extracted from MR images and resampled to dimensions of 256 $\times$ 256 $\times$ 30. The final voxel size of MR images varies from 1.25 $\times$ 1.11 $\times$ 5 mm$^{3}$ to 1.41 $\times$ 1.41 $\times$ 5 mm$^{3}$.


\subsection{Preprocessing}  % \label{} allows reference to this section
Figure~\ref{fig1} outlines the image preprocessing and CNN model training workflows, respectively. N4 bias field correction\cite{RN30} and histogram-based normalization\cite{RN31} were performed on the MR images to minimize the inter-patient intensity variation. A body mask of each patient, which was used for restricting loss evaluation and sCT accuracy assessment, was generated from the bias-corrected MR image using Otsu`s thresholding\cite{RN32} followed by opening and closing morphological operations. To account for organ movement and patient setup variations between CT and MR images, the CT was registered to the bias-corrected MR image using rigid and affine registrations, followed by a multi-resolution B-spline registration (Elastix\cite{RN33}). Each deformed CT (dCT) was resampled to match the MR image resolution. Each dCT was visually compared to its paired MR image to assure that the images were properly registered.


\begin{figure}
\begin{center}
\begin{tabular}{c}
\includegraphics[height=12.5cm]{figure1.pdf}
\end{tabular}
\end{center}
\caption 
{\label{fig1}
The overall workflow of sCT generation. (a) In the preprocessing stage, N4 bias correction was applied to the MRI to get the bias-correct MRI (bc-MRI). The CT was then deformably registered to the bc-MRI to get the paired MRI-deformed CT (dCT). The body mask and normalized MRI (nMRI) were acquired from the bc-MRI for each patient. (b) In the training stage, the sCT was generated by feeding the nMRI into the CNN model. The loss was computed as the mean absolute error between the sCT and dCT within the body mask and then minimized by updating variables of the CNN model using backpropagation and stochastic gradient descent.} 
\end{figure} 

\subsection{2D and 3D CNN models} 
The proposed 2D model was modified from SegNet{\cite{RN27}}, a state-of-the-art deep learning architecture for semantic segmentation, and extended to 3D. 2D MR slices and 3D MR volumes were fed into the corresponding CNN models which were trained to output 2D sCT slices and 3D sCT volumes, respectively. Figure~\ref{fig2} shows the architecture of the 2D model.  

\begin{figure}
\begin{center}
\begin{tabular}{c}
\includegraphics[height=12.5cm]{figure2.pdf}
\end{tabular}
\end{center}
\caption 
{\label{fig2}
The overall 2D CNN model architecture. A slice from the normalized MRI is input into the model. Each blue box represents a set of feature maps whose dimensions and number are shown. Each orange arrow represents a convolutional (conv) layer followed by instance normalization and the activation function (rectified linear unit, ReLu). In the encoder network, a maxpool operation with a 2 $\times$ 2 window and at a stride of 2, shown by green arrows, is applied to reduce the spatial resolution of feature maps, while the deconvolutional (deconv) layer followed by instance normalization layer, shown by black arrows, is used to upsample feature maps. A residual shortcut, shown by gray arrows, is achieved by adding high-resolution feature maps in the encoder network to up-sampled feature maps in the decoder network. Finally, a conv layer consisting of 1Ã—1 filters is used to generate a 2D sCT.} 
\end{figure} 

Like SegNet, the 2D model has encoder and decoder networks. The encoder network, consisting of 13 convolutional layers, is identical to the convolutional layers in the VGG16 model{\cite{RN29}}, except that filters in the first convolutional layer have a depth of 1 rather than 3, because of the scalar nature of MR and CT.  Each encoding convolutional layer performed convolution of its input with a set of 3 $\times$ 3 trainable filters at a stride of 1. Zero padding was used to produce feature maps with the same resolution as the inputs. These feature maps were normalized using instance normalization\cite{RN34} to reduce internal covariate shifts and then operated by the element-wise activation function $max(0,x)$, termed the Rectified Linear Unit (ReLU). The feature maps were downsampled by applying a maxpooling layer with a 2 $\times$ 2 window and a stride of 2. The sequence of several convolutional layers and max pooling layers act to extract local and global features and increase translation invariance. 

The decoder network, consisting of a hierarchy of decoders, was used to upsample low-resolution feature maps and gradually reconstruct the sCT. Each decoding convolutional layer corresponded to an encoding convolutional layer, except for the final convolutional layer that had a set of 1 $\times$ 1 learnable filters with a stride of 1.


Three modifications to SegNet\cite{RN27} were made to develop the proposed 2D CNN model. First, the unpooling layers in the original SegNet\cite{RN27} were replaced with fractionally-strided convolutional layers (also known as deconvolutional layers). Unlike unpooling layers, which use memorized pooling indices from maxpooling layers to produce sparse high-resolution feature maps, fractionally-strided convolutional layers can be trained to produce dense high-resolution feature maps.\cite{RN35} Second, residual shortcuts, which element-wise add encoder feature maps to corresponding upsampled feature maps, were introduced for faster convergence. This was inspired by ResNet{\cite{RN36}}. Third, instance normalization\cite{RN34} was employed rather than batch normalization\cite{RN37} to deal with the small batch size. 

The 3D model shared the same architecture as the 2D model except that all 2D operations were replaced with their corresponding 3D counterparts.

The filters in the convolutional layers and fractionally-strided convolutional layers had sets of weights and biases, which were trained by minimizing a loss function. The loss function was defined as the mean absolute error (MAE) between the sCT and deformed CT (dCT) within the body mask;
\begin{equation}
\label{eq1}
loss = \frac{1}{N}\sum_{i=1}^{N}|sCT_{i}-CT_{i}|
\end{equation}
where N was the number of voxels inside the body masks of MR images, and $sCT_{i}$ and $CT_{i}$ represented the HU values of the $i^{th}$ voxel in the sCT and dCT, respectively.

\subsection{Model optimization details}  
Both the 2D and 3D CNN models were implemented using Tensorflow\cite{RN38} packages. The Adam stochastic gradient descent method\cite{RN39} with default parameters, except for the learning rate that was set at 0.01, was used for minimizing the loss function (Equation~\ref{eq1}). At each iteration, a mini-batch of 2D images or 3D volumes was randomly selected from the training set. The batch size was limited by GPU memory. A mini-batch of 15 training slices was used to run the 2D model on an 8 GB NVIDIA GeForce GTX 1080 GPU. The 3D model was run on a 12 GB NVIDIA GeForce GTX Titan X GPU with a mini-batch of 1 training volume. The reduced batch size and large memory GPU card were necessary for implementing the 3D model due to its greater memory consumption. On-the-fly data augmentation (random shift and rotation) was performed on each set of MR images, body masks, and dCTs to reduce overfitting. For both the 2D and 3D models, the random translation was up to 15 pixels in the x and y directions, and the random rotation angle in the x-y plane was confined within $\pm$5$^{o}$. Rotations with random angles within $\pm$2$^{o}$ in the x-z and y-z planes were applied to the 3D images. The 2D and 3D model weights were initialized using He initialization{\cite{RN40}}, and the biases were initialized to 0. 

\subsection{Model evaluation}  
Five-fold-cross-validation was performed to evaluate model performance. The 20 patient-cohort was randomly divided into five groups. Each time validation was performed, four groups were used as the training set to optimize the model. The optimized model was then used to generate sCTs of patients in the remaining group. For the 2D (3D) model, four groups of four patients provided 480 (16) training samples. Using the batch size of 15 (1), it took 32 (16) iterations to go over all samples in the training set for the 2D (3D) model, which was considered as one epoch. 

CNN model accuracy was evaluated by using voxel-wise MAE between the sCT and dCT for three regions: 1) the whole body; 2) a soft tissue region generated by thresholding the dCT with a range [-100,150) HU; and 3) a bone region generated by thresholding the dCT at 150 HU, i.e., [150,$\infty$) HU.

CNN model accuracy was also evaluated by calculating the dice similarity coefficient (DSC), recall, and precision for the bone region. They were defined as:
\begin{equation}
\label{eq2}
DSC = \frac{2(V_{sCT} \cap V_{dCT})}{V_{sCT} + V_{dCT}},
recall = \frac{V_{sCT} \cap V_{dCT}}{V_{dCT}},
precison = \frac{V_{sCT} \cap V_{dCT}}{V_{sCT}}
\end{equation}
where V was the bone-region volume.
Wilcoxon signed-rank tests were performed on the evaluation metrics to test the difference between the performance of 2D and 3D models. $P < 0.05$ was considered statistically significant.
