\label{sect:intro}
Magnetic resonance imaging (MRI) is often integrated into radiotherapy treatment planning\cite{RN1}, particularly for tumors in regions like the brain, head and neck, and prostate\cite{RN2}. The superior soft tissue contrast of MR images facilitates precise delineations of tumors and organs at risk\cite{RN3,RN4}. MR images can also provide guidance for adaptive radiation therapy\cite{RN5,RN6}. The standard MRI-guided clinical workflow includes acquisition of a planning computed tomography (CT). The CT Hounsfield Unit (HU) map, essentially a scaled linear attenuation map, is used to generate both digital reconstructed radiographs for subsequent patient positioning and electron density maps for dose calculation. 

The need to acquire the CT when employing MR images for contouring has several disadvantages. Acquiring a CT increases unwanted radiation exposure, clinical workload, and financial cost\cite{RN7}. In addition, co-registering CT and MR images is required for transferring delineation structures from the MR image to the CT. This process introduces a systematic uncertainty, which is estimated to be 2 mm to 5 mm in various sites, that propagates throughout the treatment\cite{RN8}. MR-only radiotherapy can avoid these pitfalls. 

To achieve MR-only radiotherapy, synthetic HU maps, termed synthetic CT (sCT) images, must be accurately generated from the MR images. To date, there are three types of methods developed for this: atlas-based, voxel-based and hybrid\cite{RN8}. In atlas-based methods, a set of one or multiple co-registered MRI-CT images are deformably registered to a patient`s MR image\cite{RN9,RN10,RN11}. The resulting transformation can then be applied on the CT-atlas to generate the sCT. Atlas-based approaches can be time-consuming, particularly when the atlases are large, and often fail if the patient has very different anatomy from what is represented by the atlas. 

Voxel-based methods convert individual MR voxel intensities to HU values using bulk density assignments or machine learning models. Bulk density techniques assign the patient`s electron density either to water or to pre-defined electron densities within selected MR-segmented tissue types.\cite{RN12,RN13,RN14,RN15} These methods may lead to dose discrepancies and often have limited value in generating positioning reference images. Machine learning methods use paired MRI-CT images to train models that associate MRI intensities with HU values. It is challenging for models to distinguish air from bone in conventional MR images as both tissues exhibit weak signals due to their small T2 values. Some learning methods required manual bone segmentation\cite{RN16,RN17} in conventional MR images or require acquisition of specialized MR sequences like ultrashort echo time sequence\cite{RN18,RN19,RN20} for separating bone and air. Some methods used multiple MR images acquired with additional sequences designed to distinguish different tissue types.\cite{RN21,RN22,RN23}. Adding sequences can increase workload and extend scan time. 

Hybrid methods combine elements of voxel-based and atlas-based approaches.\cite{RN11,RN23} A detailed summary of previous approaches can be found in the review paper by Karlsson $et \ al$ {\cite{RN8}}. 

Recently, deep learning models\cite{RN24} proposed to estimate sCTs from MR images have demonstrated promising results. Nie $et \ al.$ \cite{RN25} presented a 3D convolutional neural network (CNN) model with three convolutional layers. It was trained to convert 3D patches of pelvic MR images to corresponding 3D sCT patches. The sCT was then generated by averaging the HU values of overlapping sCT patches. An updated model with an adversarial network\cite{RN26} was later proposed to improve the sCT quality. Training on patches rather than whole volumes reduces the required number of CNN model parameters and saves computational resources. However, using patches might miss larger scale (relative to patch size) image features. A SegNet-like 2D CNN model with 27 convolutional layers was proposed by Han for brain sCT generation. This more complex CNN model could capture long-range information and generate brain sCTs slice by slice without dividing images into patches\cite{RN27,RN28}. 

Ignoring other model- and data-specific variations, it is reasonable to expect that 3D models should have better performance than their corresponding 2D models. Since 3D models use entire image volumes rather than individual slices, they can exploit more information (e.g. relationships between consecutive slices). Han identified two potential drawbacks of using 3D models: 3D models need more parameters, potentially requiring more training data to achieve robust performance, and 3D models are difficult to implement on commonly-available GPU cards due to their large memory consumption.\cite{RN28} Han therefore used a 2D model rather than a 3D model for brain sCT generation because of the limited available training data and GPU memory\cite{RN28}. Another benefit of using Han`s 2D model\cite{RN28} is that its half weights can be initialized using the pre-trained VGG16 model{\cite{RN29}}. These weights can be used to assist the training process. No such pre-trained weights are available for a 3D model. 

In this paper, we investigated the performance of generating sCTs using CNN models in the male pelvis, which has greater anatomic variation than the brain.  We modified a SegNet-2D CNN model by implementing instance normalization\cite{RN34} and residual shortcuts\cite{RN36} to speed training. We extended the 2D model to 3D to test whether a similar size patient-cohort as in Han\cite{RN28} would be enough to effectively train a 3D model and compared 2D and 3D model performance. We incorporated on-the-fly data augmentation and a modified loss function to enhance model performance. Both models were trained from scratch without implementing transfer learning. Their performance was evaluated and compared using geometric and voxel-wise metrics.

