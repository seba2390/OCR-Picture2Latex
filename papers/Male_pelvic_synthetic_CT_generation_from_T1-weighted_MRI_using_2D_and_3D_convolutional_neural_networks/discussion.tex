In this paper, 2D and 3D CNN models were used to generate pelvic sCTs from T1-weighted MR images. Our sCT generation methods were fully automated, requiring no deformable registration or manual segmentation of bone tissues. As shown in Figure~\ref{fig3}, the 2D and 3D CNN models generated high quality sCTs. MAE curves shown in Figure~\ref{fig4} indicated that both models could precisely estimate soft-tissue HU values but had difficulty in reproducing air and high-density bone tissues. 

The MAEs within the body contour across all patients were 40.5 $\pm$ 5.4 HU and 37.6 $\pm$ 5.1 HU for the 2D and 3D models, respectively. The time required for generating a pelvic sCT using our CNN models was about 5.5 s. Our MAE results are comparable to previous studies. Kim $et \ al.$\cite{RN41} presented a voxel-based weighted summation method that produced an MAE of 74.3 $\pm$ 3.9 HU. However, manual contouring of bone tissues required for this method can be tedious and time-consuming. An MAE of 40.5 $\pm$ 8.2 HU was achieved by Dowling $et \ al.$\cite{RN11} using an average MRI-CT atlas from 38 patients. Andreasen $et \ al.$\cite{RN42} reported an MAE of 54 $\pm$ 8 HU using an atlas-based method with pattern recognition, and its prediction time was about 20.8 min. Another random forest model proposed by Andreasen $et \ al.$\cite{RN43} generated sCTs with an MAE of 58 $pm$ 9 HU. A hybrid method suggested by Siversson $et \ al.$ \cite{RN45} obtained an MAE of 36.5 $\pm$ 4.1 HU when ignoring errors introduced by gas cavities. This hybrid method was implemented in the cloud-based commercial software MriPlanner (Spectronic Medical AB, Helsingborg, Sweden), which required 50 to 80 min to generate a sCT.\cite{RN45} The patch-based 3D context-aware generative adversarial network presented by Nie $et \ al.$\cite{RN26} achieved an MAE of 39.0 $\pm$ 4.6 HU. 

Our CNN models reproduced low-density bone as shown in Figure ~\ref{fig4}. The bone-region DSCs were 0.81 $\pm$ 0.04 and 0.82 $\pm$ 0.04 from the 2D and 3D models, respectively. These results are comparable to reported DSC results of 0.79 $\pm$ 0.12\cite{RN10} and 0.91$\pm$0.03{\cite{RN11}}, where the authors compared bone contours manually drawn on the sCT and CT.

It was feasible to train the proposed 3D model with 16 image volumes from scratch. Results of the Wilcoxon signed-rank tests shown in Table~\ref{tab1} demonstrated a statistically significant improvement in overall MAE, bone DSC, and bone precision of the 3D model compared to the 2D model. However, as shown in Figure~\ref{fig4}, the 2D model seemed to perform better in estimating the high-density bone HU values. It should be noted that smaller overall MAEs do not guarantee improved sCT dose calculation and patient positioning performance. While the models performed well, we will continue to acquire more patient data to potentially improve model accuracy and further test model differences.

As this was a retrospective study, the MR image voxel sizes were not matched, resulting in different voxel intensities between images. This may have affected the sCT generation accuracy although we applied intensity normalization. A potential study could examine how voxel size variations affects sCT estimation. 

The proposed 3D model can be implemented on a 12 GB GPU to process volumetric images with dimensions of 256 $\times$ 256 $\times$ 30. More GPU memory would be required to process higher resolution 3D images. Considering the limited access to multi-GPU systems, a 3D architecture with fewer convolutional layers could be considered to deal with higher resolutions. However, the performance could be affected by the reduced parameters and smaller receptive fields of the less complex model. Another approach would be to extract 30-slice sub-volumes from CT and MR images for training the 3D model. The sCT could then be generated by averaging 30-slice sCT sub-volumes produced by the model. 

A number of techniques could be investigated for improving model performance.  Nie $et \ al.$\cite{RN26} showed that introducing an additional adversarial discriminator improved overall sCT quality. The same approach could be adapted in our proposed 2D and 3D CNN models.  Non-rigid deformation\cite{RN44} could also be applied to both CT and MR images in the process of the on-the-fly data augmentation to produce more training pairs. Multiple MR images acquired with different sequences could be fed into models to provide more information for distinguishing different tissues. Multi-GPU systems with more memory would enable the exploration of larger batch sizes for training CNN models, which could reduce variances in gradient estimation and accelerate the training. 

