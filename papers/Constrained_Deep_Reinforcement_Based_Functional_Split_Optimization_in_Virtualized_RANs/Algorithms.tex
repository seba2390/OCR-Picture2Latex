\section{Constrained Deep Reinforcement based Functional Split Optimization Framework} \label{sec:solution}
%\vspace{-1mm}

\secrev{We leverage a constrained deep RL paradigm to solve our vRAN splitting problem by treating the vRAN system as a black-box environment, which makes minimal assumptions about the underlying system. Consequently, our RL agent does not need to know the information about the formulation in \eqref{eq:setx}-\eqref{eq: total-cost} to decide the splits. Our agent relies on the scalar reward and penalization returned from the environment to assess the quality of the solutions.}
%
%Our vRAN splitting problem also comprises combinatorially large discrete action spaces as the split configuration should simultaneously be deployed to multiple BSs. Therefore, we propose a CDRS framework to follow NCO with a deep RL paradigm to handle this challenge \cite{neural_bello,solozabal_constrained,vnf_drl_solozabal}
%
%
\secrev{At each episode, our agent observes a \textit{state} of incoming a sequence of all BS functions drawn from the \textit{environment} of vRANs, takes an \textit{action} to decide the splits for all the BSs, and expects to receive feedback signals of the \textit{reward} (total network cost) and \textit{penalization} (for violating the constraints). Our state comprises of a sequence information of BS functions: $\mathcal{F} = \{ \mathcal{F}_n \}_{n=1}^{N}$,  where $\mathcal{F}_n$ is a set of BS-$n$ functions. Given the input state, our agent assigns $\mathcal{O} \!\!=\!\! \{o_n \! \in \! \{0,1,2,3\}, \forall n \in \mathcal{N} \}$ as a set of selected splits for all the BSs, which decides the placement of BS functions at the CU or DUs. Our objective is to minimize the total network cost while enforcing the constraint requirements. Given the selected splits, our agent expects to receive scalar values from the environment consisting of: \textit{i)} $J(\mathcal{O}|\mathcal{F})$, the total induced cost and \textit{ii)} $\xi(\mathcal{O}|\mathcal{F})$, the weighted sum of penalization. Further, we consider a particular RL algorithm using one-step constrained policy optimization and neural network architecture, where the interactions are narrowed to a single time step at every episode, and our agent learns iteratively over episodes.}

%CDRS follows the deep RL paradigm based on the constrained policy optimization to solve \secrev{the split problem in vRANs through} a Policy Gradient \cite{rl_sutton} with Lagrangian relaxation method \cite{Bertsekas}. In addition to the reward (cost) signal, we also give penalization for every constraint violation that guides the policy to constraint satisfaction. In this sense, the penalty coefficient (Lagrangian multiplier) setting is a multi-objective problem where there is a different optimum solution for each configuration. To this end, we follow two penalty coefficient updates: 1) CDRS-Fixed that uses a fixed penalty coefficient (reward shaping) \cite{vnf_drl_solozabal, cmpd_solo} and 2) CDRS-Ada that applies an adaptive penalty coefficient (updated in the ascent direction)  \cite{pdo_risk}.


\secrev{Our goal is to design a stochastic policy $\pi_\theta(\mathcal{O}|\mathcal{F})$ parameterized by a neural network with weights $\theta$ to predict the splits for all the BSs to minimize the total cost while satisfying constraint requirements. However, we have the $N$ BSs that need to deploy the splits together, where each has four possible split options. Each split decision is also interdependent as the BSs share the same network links and computing servers. Consequently, our problem has a combinatorially large discrete action space with a total of $4^N$ possible actions. Such a curse dimensionality in high dimensional spaces can be avoided by modelling complicated joint probability distributions using the chain rule decomposition. Therefore, we design our policy based on a chain rule by factorizing the output probability, parameterized by a neural network with weights $\theta$ as:
	%
	\begin{align}
		\pi_\theta(\mathcal{O}| \mathcal{F}) = \prod_{n=1}^{N} \pi_\theta(o_n| o_{(<n)}, \mathcal{F}_n). 
	\end{align}
	%
	%
	This policy strategy assigns a higher probability to the splits for having a lower cost and vice versa for every BS, which also can be represented by individual softmax modules (e.g., at the output layer). Motivated by \cite{seq2seq, attention_bahdanau} that uses neural networks \thirdrev{to estimate} the same factorization \thirdrev{of our stochastic policy} for machine translation, we design our policy network using an encoder-decoder sequence-to-sequence model based on LSTM networks. Our policy network architecture, which also utilizes an attention mechanism, captures the dependency and correlation between split decisions. This architecture allows our policy to read input information from all BS functions, then maps them into split selections for all the BSs.
	%
	%We provide the summarize and illustration of CDRS training operation in Algorithm 1 and Fig xxx, respectively. 
	In the training, we use a batch of $B$ i.i.d samples on the stochastic policy to select the splits and generate several pretraining models. In the test, we perform an inference through a search strategy by greedy decoding or temperature sampling.%The detail of CDRS operation is discussed as follows. 
	} 

%
%In supervised learning, the performance of the model depends on the quality of the supervised labels, where finding high-quality labelled data (e.g., optimal label) is expensive and may not be possible. Therefore, we follow NCO framework with deep RL paradigm \cite{neural_bello,solozabal_constrained,vnf_drl_solozabal}. We define \textit{environment} as a vRAN system consisting of CU, DUs and network links. The \textit{states} is defined as a sequence of all BS functions. Our RL \textit{agent} generates \textit{action} that corresponds to a set of decisions to choose the functional split configuration for every BS. This action decides which functions are deployed at the DU and CU for every BS. The environment will evaluate for every action taken and give a feedback signal from the environment. This feedback consists of \textit{reward} (the total network cost) and \textit{penalization} (constraint violation). As opposed to supervised learning, RL can deliver an appropriate paradigm for training and updating the neural network parameters to improve the performance of the model for the functional split problem. It simply relies on the reward and penalty feedbacks (interaction with the environment) instead of the high-quality labelled data.  Since our problem is combinatorial that has highly dimensional action space, we leverage model-free policy-based RL that optimizes the weight parameter $\theta$ that infers a policy strategy to deploy the split configuration. It is worth noting that we have a constrained environment; hence, we also consider a constraint relaxation technique in the cost function of our policy-based method to deal with constraint dissatisfaction. 
%%Fig xxx depics the general RL scheme for our problem.
%
%Our approach follows the deep RL paradigm based on the constrained policy optimization and neural network architecture to solve the functional split problem in the vRAN. We utilize Policy Gradient \cite{rl_sutton} with Lagrangian relaxation method \cite{Bertsekas}. In addition to the reward (cost) signal, we also give penalization for every constraint violation that guides the policy to constraint satisfaction. In this sense, the penalty coefficient (Lagrangian multiplier) setting is a multi-objective problem where there is a different optimum solution for each configuration. To this end, we follow two penalty coefficient updates: 1) CDRS-Fixed that uses a fixed penalty coefficient (reward shaping) \cite{vnf_drl_solozabal, cmpd_solo} and 2) CDRS-Ada that applies an adaptive penalty coefficient (updated in the ascent direction)  \cite{pdo_risk}. A neural network architecture formed by an encoder-decoder sequence-to-sequence model \cite{seq2seq,attention_bahdanau} based on stacked LSTM is also utilized that approximate the stochastic policy over the solution. 
%
%Our agent receives input of set of BS functions $\mathcal{F} = \{ \mathcal{F}_n \}_{n=1}^{N} \!\!$,  where $\mathcal{F}_n \!=\! \{f_0,f_1,f_2,f_3 \}$ is a set of functions for BS-$n$. In the output, we expect to receive $\mathcal{O} \!\!=\!\! \{o_n \}_{n=1}^{N}$ as a set of selected configuration for all BSs. It addresses the split configuration of BS-$n$ with $o_n \! \in \! \{0,1,2,3\}$. 
%%In relation to Eq. \eqref{eq:setx}, $o_n$ activates the respected split configuration variables for BS-$n$ following indicator function: $x_{0n} \!=\! \mathbbm{1}_{(o_n \!= 0)}, x_{1n} \!=\! \mathbbm{1}_{(o_n \!= 1)}, x_{2n} \!=\! \mathbbm{1}_{(o_n \!= 2)},$ and $ x_{3n} \!=\! \mathbbm{1}_{(o_n \!= 3)} $.  
%We also use the neural network with weight parameter $\theta$ that infers a policy strategy $\pi_\theta(\mathcal{O}| \mathcal{F}, \theta)$ to deploy the split configuration. 
%
%
%In practice, our approach does not have to know the defined problem in Section \ref{sec:problem}. Our agent interacts with the environment expecting to receive a reward (network cost) and penalization (constraints violation); then learn from this interaction to find the optimal solution. At the test time, we perform an inference process through search strategies by a greedy decoding or temperature sampling method. 


  
\vspace{-2mm}
\subsection{\secrev{Policy} Network Architecture}
%\vspace{-1mm}
%Our system is bounded under computational and link capacity, where each BS has distinct network parameters. Hence, the BS input sequence (to which BS needs to decide first) affects the solution. To this end, 
Our policy network infers a strategy to deploy the splits for all the BSs, given a sequence information of BS functions as an input $\mathcal{F} = \{\mathcal{F}_1, ...., \mathcal{F}_N \}$. It is constructed from an encoder decoder sequence-to-sequence model with an attention mechanism based on LSTM networks \cite{seq2seq,attention_bahdanau}.
\secrev{We also consider a batch training by drawing} a batch of $B$ i.i.d samples with different sequence order \secrev{to encourage the exploration further}. 

\secrev{\textbf{LSTM structure.} We leverage LSTM networks, a particular RNN architecture \cite{lstm},  to construct  our sequence-to-sequence model that maps the input BS functions into split decisions for all the BSs.
	An LSTM cell has three main structures comprising of: \textit{(i)} a forget gate that receives the cell state input and learns how long should memorize or forget from the past; \textit{(ii)} an input gate that aggregates the current input and the output of past steps, then feeds them to the activation function; and \textit{(iii)} an output gate that provides the LSTM output from the combination of current cell state and the output of input gate. The relationship of these blocks can be expressed as:
	\begin{align}
		&\bm{\hat{f}}_n = \sigma \big( W_f \big[\bm{h}_{n-1}^T, \bm{s}_{n}^T  \big]^T + \bm{b}_f \big), \\
		%
		&\bm{\hat{r}}_n = \sigma \big( W_r \big[\bm{h}_{n-1}^T, \bm{s}_{n}^T   \big]^T + \bm{b}_r \big), \\
		%
		&\bm{\tilde{c}}_n = \tanh \big( W_c \big[\bm{h}_{n-1}^T, \bm{s}_{n}^T   \big]^T + \bm{b}_c  \big), \\
		%
		&\bm{\hat{c}}_n = \bm{\hat{f}}_n *  \bm{\hat{c}}_{n-1} + \bm{\hat{r}}_n  * \bm{\tilde{c}}_n, \\
		%
		%
		&\bm{\hat{o}}_n = \sigma \big( W_o \big[\bm{h}_{n-1}^T, \bm{s}_{n}^T   \big]^T + \bm{b}_o  \big), \\
		& \bm{h}_n = \bm{\hat{o}}_n * \tanh(\bm{\hat{c}}_n),
	\end{align}
	where function $\sigma(x) \triangleq \frac{1}{1 + \exp(-x)} $ is the sigmoid function and symbol $*$ is element-wise multiplication. The weight and bias matrices for the respective forget, input and output gates of the LSTM cell are represented by $W_f, W_r, W_c, W_o$  and $\bm{b}_f, \bm{b}_r, \bm{b}_c, \bm{b}_o$.
	%
	%
	Multiple LSTM layers can be further stacked one on top of another (a stacked LSTM) to create a deeper model, which may obtain more accurate prediction. Each LSTM cell reads an input of embedding vector representation $\bm{s}_n \in [ -1,1]^E$ translated from each input $\mathcal{F}_n$, where $E$ is the embedding size.
	The structure of an LSTM cell is illustrated in Fig \ref{fig:lstm} and utilized to construct our sequence-to-sequence model.}
	%
	\begin{figure}[t!] 
		\centering
		\includegraphics[width=0.45 \textwidth]{images/lstm.pdf}   
		\caption{\small \secrev{A generic architecture of an LSTM cell.}  } 
		\label{fig:lstm}
		\vspace{-3mm}
	\end{figure}
	%
%Fig. \ref{fig:neural} illustrates the overall architecture of our policy network.
%
%Additionally, \secrev{the attention mechanism in our policy network allows to capture the dependency and correlation between each split decision, such as the shared network link and computing resources and the routing and computational weighting costs.}
%The architecture is depicted in Fig. \ref{fig:nn}. \fm{need more rephrasing}

\secrev{\textbf{Policy Network.}} Our policy network is built from an encoder-decoder sequence-to-sequence model based on LSTM networks. \secrev{One main drawback of vanilla sequence model is generally unable to learn accurately long sequence. Therefore, the vanilla model may not be able learn our problem with large number of BSs.
An attention mechanism comes to address this issue as it considers all the hidden state from all input sequences.} The encoder read the entire input sequence to a fixed-length vector. The decoder decides \secrev{the deployed split of each BS} at each step from an output function based on its own previous state combined with an attention over the encoder hidden states \cite{attention_bahdanau}. The decoder network hidden state is defined with a function: $\bm{h}_t = f(\bm{h}_{t-1}, \bm{\bar{h}}_{t-1}, \bm{c}_t)$, \secrev{where $\bm{c}_t$ and $\bar{\bm{h}}_{t}$ are the context vector and the source hidden state at time step $t$}. 
Our model derives the context vector $\bm{c}_t$ that captures relevant source information that helps to predict the splits. The main idea is to use \secrev{an attention mechanism}, where the context vector $\bm{c}_t$ takes consideration of all the hidden states of the encoder and the alignment vector $\bm{a}_{t}$: 
\begin{align}
	\bm{c}_t = \sum_{k \in \mathcal{N}} \bm{a}_{tk} \bm{\bar{h}}_k.
\end{align}
%
% 
Note that the alignment vector has an equal size to the number of steps in the source side, which can be calculated by comparing the current target hidden state of decoder $\bm{h}_t$ with each source hidden state $\bm{\bar{h}}_k$ as:
\begin{align} \label{eq:softmax}
	 \bm{a}_{tk} = \frac{\exp(\text{score}(\bm{h}_t,\bm{\bar{h}}_k))}{\sum_{k'=1}^{N} \exp(\text{score}(\bm{h}_t,\bm{\bar{h}}_k')))}
\end{align}
This \secrev{alignment} model gives a score $\bm{a}_{tk}$ which describes how well the pair of input at position $k$ and the output at position $t$. The \secrev{alignment} score is parameterized by a feed-forward network where the network is trained jointly with the other models \cite{attention_bahdanau}. The score function is defined by a non-linear activation function following Bahdanau's additive style:
\begin{align}\label{eq:score}
	\text{score}(\bm{h}_t,\bm{\bar{h}}_k) = \bm{v}_a^{\top} (\tanh(\bm{w}_1 \bm{h}_t +  \bm{w}_2 \bm{\bar{h}}_k )),
\end{align}
%
%
where $\bm{v}_a^{\top} \in \mathbb{R}^{n}, \bm{w}_1 \!\in\! \mathbb{R}^{n \times n} $ and $\bm{w}_2 \!\in\! \mathbb{R}^{n \times n}$ are \secrev{defined as the weight matrices to be learned in the alignment model, and $n$ is the size of hidden layers}. The overall architecture of our policy network is illustrated in Fig. \ref{fig:neural}.
%
\begin{figure}[t!] 
	\centering
	\includegraphics[width=0.49 \textwidth]{images/NN-v2.pdf}   
	\caption{\small \secrev{\textbf{Policy Network.} CDRS utilizes a neural network architecture to approximate the stochastic policy over the solution. It is constructed by an encoder-decoder sequence-to-sequence model with attention mechanism based on LSTM networks.} }
	\label{fig:neural}
	\vspace{-3mm}
\end{figure}
%
%
\vspace{-2mm}
\subsection{Constrained Policy Gradient with Baseline}
%\vspace{-1mm}
%
%
%
%
%
%
\secrev{We train the above neural network model using a constrained policy gradient method with a self competing baseline.}
We define the objective of $\mathbb{P}$ as an expected reward that is obtained for every vector of weights $\theta$. Hence, the expected cost $J$ in associated with the selected split $o_n$ given BS-$n$ functions \secrev{is denoted as}:
%
\begin{align}
	J^\pi(\theta|\mathcal{F}_n) = \underset{o_n \sim \pi(.|\mathcal{F}_n) }{\mathbb{E}} [ J(o_n) ],
\end{align}
%
and we have the expected of total cost from all BSs:
%
\begin{align} \label{eq:total_cost_theta}
J^\pi(\theta) = \underset{o_n \sim \mathcal{O} }{\mathbb{E}} [ J(\theta|\mathcal{O}) ].
\end{align}
%
The vRAN system has constraints of delay requirement and computational and link capacity. 
%
%we put these to the constraint disatisfication that associated with our policy with: 
%\begin{align} \label{eq:disatisfication}
%J_C^\pi(\theta) = \underset{o_n \sim \mathcal{O} }{\mathbb{E}} [ J(\theta|\mathcal{O}) ]. 
%\end{align}
%
Therefore, our original problem turns to a primal problem as:
%
\begin{align} 
\mathbb{P}_{1\text{P}}: \ \underset{\pi \sim \Pi }{\text{min}} \  J^\pi(\theta); \ \ \text{s.t.} \ \secrev{J_{C_i}^\pi(\theta) \leq 0, \forall i}, \notag
\end{align}
%
where \secrev{we define $J_{C}^\pi(\theta) = \big(J_{C_{i}}^\pi(\theta), \forall i \big)$} as a function of constraint dissatisfaction to capture the penalization that the environment returns for violating each $i$ constraint requirement, e.g., computing, link, delay. 
In this problem, we consider parametrized stochastic policy using a neural network. In order to ensure the convergence of our policy to constraint \secrev{satisfaction}, we follow \cite{reward_constraint} and make assumptions:
%
\begin{assumption} \label{assumption:cost}
	$J^\pi$ is bounded for all policies $\pi \in \Pi$.
\end{assumption}
\begin{assumption} \label{assumption:localminima}
	Each local minima of $J_{C}^\pi(\theta)$  is a feasible solution.
\end{assumption}
\noindent
Assumption \ref{assumption:localminima} describes that any local minima $\pi_\theta$ satisfies all constraints, e.g., $J_{C_i}^\pi(\theta) \leq 0, \forall i$. It is the minimal requirement that guarantees the convergence of a gradient algorithm to a feasible solution. The stricter assumptions, e.g., convexity, may guarantee the optimal solution.
%

%%%%%%%% DUAL FUNCTION START HERE %%%%%%%%%%%%%%%
Next, we reformulate $\mathbb{P}_{1\text{P}}$ to unconstraied problem with Lagrange relaxation method \cite{Bertsekas}. The penalty signal is also included aside \secrev{from the} original objective for infeasibility, which leads to a sub-optimality for infeasible solutions. Given $\mathbb{P}_{\text{1P}}$, we have the dual function:
%
\begin{align} 
	\label{eq:dual_function}
	g({\mu}) = \underset{\theta }{\text{min}} \  J_L^\pi({\mu},\theta) &= \underset{\theta }{\text{min}} \  J^\pi(\theta) + \sum_{i} \mu_i J_{C_{i}}^\pi(\theta) \notag \\
	&=\underset{\theta }{\text{min}} \  J^\pi(\theta) + J_\zeta^\pi(\xi),
\end{align}
where $\mu \!=\! (\mu_i, \forall i), J_L^\pi({\mu},\theta)$ and $J_\zeta^\pi(\xi)$ are the  penalty coefficients (Lagrange multipliers), Lagrange objective function and the expected penalization, respectively. Then, we define the dual problem:
%
\begin{align}
\mathbb{P}_{1\text{D}}: \ \underset{{\mu} }{\text{max}} \  g({\mu}). \notag
\end{align}
%
%
$\mathbb{P}_{1\text{D}}$ aims to find a local optima or a saddle point $(\theta({\mu}^*), {\mu}^*)$, which is a feasible solution. The feasible solution is a solution that satisfies:  $J_{C_{i}}^\pi(\theta) \leq 0, \forall i$. 
%
% 
%
To compute the weights $\theta$ that optimize the objective, we use Monte-Carlo policy gradient and stochastic gradient descent by the following update:
\begin{align}\label{eq:update1}
	\theta_{k+1} = \theta_{k} - \eta_a(k) 	 \nabla_\theta J_L^\pi({\mu},\theta), 
\end{align}
%
where $\eta_a(k) $ is the step-size. The gradient $\nabla_\theta J_L^\pi({\mu},\theta)$  with regards to weights $\theta$ can be calculated using a log-likelihood method as:
%
%
\begin{align}
\nabla_\theta J_L^\pi(\theta) = \underset{\mathcal{O} \sim \pi_\theta(.|\mathcal{F}) }{\mathbb{E}} [ L(\mathcal{O}|\mathcal{F}) \ \nabla_\theta \log \pi_\theta(\mathcal{O}|\mathcal{F}) ].
\end{align} 
%
%
$L(\mathcal{O}|\mathcal{F})$ represents the total cost with penalization obtained from: 
	 $L(\mathcal{O}|\mathcal{F}) = J(\mathcal{O}|\mathcal{F}) + \xi (\mathcal{O}|\mathcal{F}) $, where
$J(\mathcal{O}|\mathcal{F})$ is the total network cost in each iteration and $\xi (\mathcal{O}|\mathcal{F}) = {\mu} C(\mathcal{O}|\mathcal{F})$ is the weighted sum of constraint dissatisfaction of $C(\mathcal{O}|\mathcal{F})$. 

The penalty coefficient ${\mu}$ is set manually \cite{vnf_drl_solozabal,cmpd_solo} for CDRS-Fixed within a range $[0, \mu_{\text{max}} ]$\footnote{If Assumption \ref{assumption:localminima} is satisfied, $\mu_{\text{max}}$ can be set to $\infty$ \cite{reward_constraint}.}. In this case, the selection of ${\mu}$ can be set following intuition approach in \cite{vnf_drl_solozabal} (Appendix C), i.e., agent will not pay attention to penalty if $\mu = 0$, and it will only converge to penalization if $\mu = \infty$. Hence, selecting the appropriate penalty coefficient is important in this case. Otherwise, we can follow a less intuitive approach by adaptively updating the penalty coefficient (CDRS-Ada). CDRS-Ada is updated based on the primal-dual optimization (PDO) method inspired from \cite{pdo_risk}. Hence, we update the penalty coefficient in the ascent direction as:
%
\begin{align} \label{eq:update2}
{\mu}_{k+1} &= {\mu}_{k} + \eta_d(k) \nabla_\mu J_L^\pi({\mu},\theta) \\
& = {\mu}_{k} + \eta_d(k) (  J_{C}^\pi(\theta))_+, 
\end{align} 
where $\eta_d(k)$ is the step-size (Dual) and \secrev{$\nabla_\mu J_L^\pi({\mu},\theta) = \mathbb{E}_{\mathcal{O} \sim \pi_\theta(.|\mathcal{F})} [ C(\mathcal{O}|\mathcal{F}) ]$ is the gradient with respect to $\mu$}. The penalty coefficient ${\mu}_{k}$ is updated for every $k$-th iteration and will converge to a fixed value once the constraints are satisfied \cite{reward_constraint,pdo_risk}. 
%
%
Then, Monte-Carlo sampling can be applied to approximate \secrev{$J_L^\pi(\theta)$} by drawing  $B$ i.i.d samples $ \!\mathcal{F}^1,...,\mathcal{F}^B \!\sim\! \mathcal{F}$, which can be written:
\begin{align} \label{eq:lag_grads}
\!	\nabla_{\!\theta} J_L^\pi(\theta) \! \approx \! \frac{1}{B} \! \sum_{i=1}^{B} \! \! \Big(\! L(\mathcal{O}^i | \mathcal{F}^i) \! - \! b_{\theta_v}(\mathcal{F}^i)\! \Big) \! \nabla_{\!\theta} \! \log \! \pi_\theta(\mathcal{O}^i | \mathcal{F}^i), \!\!
\end{align} 
\secrev{where $b_{\theta_v}(\mathcal{F}^i)$ is the baseline estimation given the state input of $i$-th batch, parameterized by a neural network structure with weights $\theta_v$.}

\textbf{Baseline estimator.} The baseline choice can be from an exponential moving average of the reward over time that captures the improving policy in training. Although it succeeds in the Christofides algorithm, it does not perform well because it can not differentiate between inputs \cite{neural_bello}. To this end, we use a parametric baseline $b_{\theta_v}$ to estimate the expected total cost with penalization that typically improves the learning performance. \secrev{We estimate the baseline through} an auxiliary network built from an LSTM encoder connected to a multilayer perceptron output layer. \secrev{The auxiliary network (parameterized by $\theta_v$) that approximates the expected cost with penalization from input $\mathcal{F}$ is trained with stochastic gradient descent.} It employs a mean squared error (MSE) objective, calculated from the prediction of $b_{\theta_v}$ and the total cost with penalization $L(\mathcal{O}^i | \mathcal{F}^i)$, and sampled by the most recent policy (obtained from the environment). We formulate \secrev{the auxiliary network goal is to minimize the below loss function:}
%$\mathbb{E}_{\mathcal{O} \sim \pi(.|\mathcal{F})} L(\mathcal{O}|\mathcal{F})$
\begin{align} \label{eq:aux_mse}
	\mathcal{L}(\theta_v) = \frac{1}{B} \sum_{i=1}^{B} \left\| b_{\theta_v}(\mathcal{F}^i) - L(\mathcal{O}^i | \mathcal{F}^i) \right\|_2^2.
\end{align}
%
Fig. \ref{fig:baseline} illustrates the architecture of the auxiliary network for estimating the baseline. 
%
%
%
\begin{figure}[t!] 
	\centering
	\includegraphics[width=0.24 \textwidth]{images/baseline.pdf}   
	\caption{\small\secrev{\textbf{Baseline Estimator.} The self-competing baseline of CDRS is estimated using an auxiliary network constructed from an LSTM encoder connected to a multilayer perceptron output linear layer.}  } 
	\label{fig:baseline}
	%\vspace{-1mm}
\end{figure}
%%
%
%
%  
%
%
\begin{figure}[t!] 
	\centering
	\includegraphics[width=0.49 \textwidth]{images/RL_diagram.pdf}   
	\caption{\small\textbf{CDRS Diagram.} CDRS is trained using a single time step Monte-Carlo policy gradient algorithm, where at every epoch, the interactions with the environment are narrowed to a single time step. Our agent learns the policy iteratively over epochs.} 
	\label{fig:rl_diagram}
	\vspace{-3mm}
\end{figure}
%
%
\begin{algorithm}[t!]  \caption{CDRS Training}
	\label{algo:cdrs}
	%\myproc{TRAIN(Learning Set $\mathcal{F}$, batch size $B$)}
	\SetAlgoLined
	\DontPrintSemicolon
	\KwInput{$K$ (Num of epoch), $B$ (Batch size), $\mathcal{F}$ (Learning set)}
	\KwInitialize{ assign agent and critic (baseline) networks with random weights $\theta$ and $\theta_v. \;$} 
	%
	%	 
	\For{ $ k=1, ..., K$}  
	{
		$d\theta$ $\leftarrow$ 0 \% Reset gradient \\
		$\mathcal{F}^i \sim $ \text{SampleInput} $(\mathcal{F})$ for $i \in \{1,...,B \}$. \;
		$\mathcal{O}^i \sim $ SampleSolution $(\pi_\theta(.|\mathcal{F}))$ for $i \in \{1,...,B \}$. \;
		$b^i \leftarrow b_{\theta_v} (\mathcal{F}^i)$ for $i \in \{1,...,B \}$. \;
		Compute $L(\mathcal{O}^i)$ for $i \in \{1,...,B \}$. \;
		$g_\theta \leftarrow \frac{1}{B} \! \sum_{i=1}^{B} \! \! \Big(\! L(\mathcal{O}^i) \! - \! b^{i}\! \Big) \! \nabla_{\!\theta} \! \log \! \pi_\theta(\mathcal{O}^i | \mathcal{F}^i)$ from \eqref{eq:lag_grads}. \;
		$\theta \leftarrow$ Adam($\theta, g_\theta$) \%Run Adam algorithm \;
		$\mathcal{L}_v \leftarrow \frac{1}{B} \sum_{i=1}^{B} \left\| b^{i} - L(\mathcal{O}^i) \right\|_2^2 $ from \eqref{eq:aux_mse}. \;
		$\theta_v \leftarrow$ Adam($\theta_v, \mathcal{L}_v$) \%Run Adam algorithm \;
		{\color{black} Update ${\mu}$ from \eqref{eq:update2} \%CDRS-Ada\\ }
		{\color{black} Set ${\mu} = \max(0,{\mu})$ \%CDRS-Ada}
	}
	\Return $\theta, \theta_v, \mu$
	\;
\end{algorithm}
%
%

\secrev{To sum up, our training procedures are summarized in Algorithm \ref{algo:cdrs} and illustrated in Fig. \ref{fig:rl_diagram}, which run iteratively by $K$ episodes (epochs) based on a single time-step Monte-Carlo policy gradient with a baseline estimator.}
The sequence of policy updates will converge to a locally optimal policy and the penalty coefficient updates (e.g., CDRS-Ada) will converge to a fixed value when all constraints are satisfied; see also \cite{pdo_risk,reward_constraint}. 


%

%for the standard convergence proof of stochastic approximation algorithm with constraints. 

%\subsection{Training Algorithm}
%Algorithm 1 summarizes our training procedure of single time-step Monte-Carlo Policy Gradient with baseline estimator, which runs until $T$ epochs. We also include the penalty coefficient update in this procedure. Our training firstly requires the training set from a set of all BS functions $\mathcal{F}$, the number of minibatch $B$, and the number of epochs $T$. For initalization, we randomly give the weight values for our agent $\theta$ and baseline $\theta_v$. Algorithm 1 runs iteratively until $T$ epochs (Step 1). It resets the gradient ($d\theta $) by assigning zero value (Step 2). Then, it randomly generates i.i.d samples from the training set, e.g., $\mathcal{F}^1,...,\mathcal{F}^B \sim \mathcal{F}$ (Step 3).

%Next, we discuss the convergence of Algorithm 1. 

%It almost surely converges to a fixed values, which is a feasible solution (local optimal)
%\begin{theorem} \label{theo:penalty}
%	The penalty coefficient updates of CDRS-Ada in Algorithm 1 will converge to a fixed value once all constrains are satisfied.
%\end{theorem}
%\noindent
%\textit{Proof.}
%
%
%\begin{theorem} \label{theo:algo1}
%	The policy updates in Algorithm 1 almost surely converges to a locally optimal policy and hold in our case.
%\end{theorem}
%\noindent
%\textit{Proof.} We can proof it following standard procedure for stochastic approximation algorithm...


% Next, we prove that our approach will converge to a fixed values, which is a feasible solution (local minima) for our problem.
%%
%%
%\begin{theorem}
%	Algorithm 1 converges to a feasible solution (local optimality) 
%\end{theorem}
%%
%\textit{Proof.} We prove the convergence of Algorithm 1 for our case following  Theorem xxx of \cite{}. 1) The dual function is always convex despite the primal problem is non-convex \cite{}. Hence, our dual function in \eqref{eq:dual_function}  is also a convex function, so it is also Lipschitz continuous. 

\vspace{-1mm}
\subsection{Searching Strategy}
%\vspace{-1mm}
At the test time, evaluating the total network cost is inexpensive \secrev{as it only requires a forward pass from the policy network to decide the splits}. Our agent can add a search procedure during the inference process by considering solution candidates from multiple \secrev{pretraining} models to select the splits. It can help to reduce the inferred policy suffering from a severe suboptimality. 
%In this part, we employ two different search strategies: greedy decoding and \secrev{temperature sampling} \cite{neural_bello}. 
We employ two different search strategies by greedy decoding and \secrev{temperature sampling} \cite{neural_bello}.

\textbf{Greedy decoding.} It is the simplest search strategy. The idea is to \secrev{greedily} select the splits with the \secrev{highest} probability for having the lowest cost \secrev{from multiple pretraining models during the inference time}. %At the inference time, the greedy output from each model is evaluated to \secrev{choose} the best one \cite{vnf_drl_solozabal}. 
Then, we can extend CDRS to CDRS-Fixed-G, which uses a fixed penalty coefficient with greedy decoding and CDRS-Ada-G that uses an adaptive penalty coefficient with greedy decoding. 

\textbf{Temperature sampling.} This method samples through stochastic policy for \secrev{each pretraining model to generate several candidate solutions} then \secrev{decides} the splits with the lowest total cost \secrev{among them} \cite{neural_bello,vnf_drl_solozabal}. As opposed to the heuristic solvers, it does not sample the different split options. Instead, \secrev{it samples through the stochastic policy} and controls the sparsity of the output distribution \secrev{by} a temperature hyperparameter $T$. The softmax function in \eqref{eq:softmax} is modified to $\bm{a}_{tk} = \frac{\exp\big( \text{score}(\bm{h}_t,\bm{\bar{h}}_k)/T \big)}{\sum_{k'=1}^{N} \exp\big(\text{score}(\bm{h}_t,\bm{\bar{h}}_k')/T ) \big)}$ (softmax temperature). \secrev{In the training}, the temperature hyperparameter $T$ is \secrev{set} to 1. \secrev{Meanwhile, we modify to $T>1$ during the test}, hence the output distribution becomes less step, \secrev{which} prevents the model from being overconfident. With this method, we can extend CDRS to CDRS-Fixed-T (fixed penalty coefficient, temperature sampling) and CDRS-Ada-T (adaptive penalty coefficient, temperature sampling). \secrev{Note that this method requires additional time, which depends on the number of samples.}


 %It considers multiple candidate solutions, then infers the best solution. The approach is to sample candidate solutions from stochastic policy, then select the split configuration with the lowest total cost. A temperature hyperparameter controls the diversity of the sampling to attain an improvement in finding the best solution. The detailed algorithm is described in \cite{neural_bello}.







