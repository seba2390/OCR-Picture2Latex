\begin{table*}[h!]
\centering
\scalebox{1.0}{
  
    \begin{tabular}{l|ccc|cc}
   \Xhline{2\arrayrulewidth}
          & \multicolumn{5}{c}{\textbf{Average Downstream Accuracy --- \texttt{Seen Tasks}}} \\
          \cline{2-6}
          & \multicolumn{3}{c|}{\textbf{100 classes / 40k images}} & \multicolumn{2}{c}{\textbf{237 classes / 100k images}} \\
          \cline{2-6}
    \multicolumn{1}{c|}{\textbf{Pretraining Dataset}} & \thead{\textbf{5NN}} & \thead{\textbf{Linear Probing} } & \thead{\textbf{Finetuning}} & \thead{\textbf{Linear Probing}} & \thead{\textbf{Finetuning}} \\
   \Xhline{2\arrayrulewidth}
    Scratch &  -  &  -  & 64.85 & -  & 64.85 \\
    Random & 25.30 & 54.06 & 70.77 & 55.14 & 72.18 \\
    Domain Randomization & 19.42 & 35.31 & 62.96 & 45.31 & 68.51 \\
    Imagenet$^{*}$ & \underline{28.91} & \textbf{63.12} & \underline{74.26} & \textbf{68.44} & \textbf{77.61} \\
    \texttt{\textbf{\ours}} & \textbf{30.46} & \underline{62.70} & \textbf{75.34} & \underline{62.71} & \underline{76.87} \\
   \Xhline{2\arrayrulewidth}
    \end{tabular}%
} \vspace{-2mm}
  \caption{Comparing the downstream accuracy on seen tasks for the Task2Sim chosen pretraining dataset and other baselines. Simulation parameters found on seen tasks by \ours generates synthetic pretraining data that is better for downstream tasks than other approaches like using Random simulation parameters or Domain Randomization. Pre-training with \ours's data is also competitive with pre-training on images from Imagenet. $^{*}$Imagenet has been subsampled to the same number of classes and images as indicated at the top of the column. boldface=highest, underline=$2^{nd}$ highest in column.}
  \label{tab:main_seen}%
  \vspace{-2mm}
\end{table*}%

