\section{Introduction} \label{sec:intro}

\input{figures/fig1}

Using large-scale labeled (like ImageNet~\cite{deng2009imagenet}) or weakly-labeled (like JFT-300M~\cite{chollet2017xception,hinton2015distilling}, Instagram-3.5B~\cite{mahajan2018exploring}) datasets collected from the web has been the go-to approach for pre-training classifiers for \emph{downstream} tasks with a relative scarcity of labeled data. 
Prior works have demonstrated that as we move to bigger datasets for pre-training, downstream accuracy improves on average~\cite{sun2017revisiting,mahajan2018exploring}.
However, large-scale real image datasets bear the additional cost of curating labels, in addition to other concerns like privacy or copyright. Furthermore, large datasets like JFT-300M and Instagram-3.5B are not publicly available posing a bottleneck in reproducibility and fair comparison of algorithms.


Synthetic images generated via graphics engines provide an alternative quelling a substantial portion of these concerns. With 3D models and scenes, potentially infinite images can be generated by varying various scene or image-capture parameters. Although synthetic data has been used for transfer learning in various specialized tasks~\cite{su2015render, richter2016playing, tobin2017domain, anderson2021sim}, there has not been prior research dedicated to its transferability to a range of different recognition tasks from different domains (see \cref{fig:fig1}). In conducting this first of its kind (to the best of our knowledge) study, we first ask the question : in synthetic pretraining for different downstream classification tasks, does a one-size-fits-all solution (\ie, a universal pre-trained model for all tasks) work well?

With graphics engines, we can control various \emph{simulation} parameters (lighting, pose, materials, etc.). So, in an experiment, we introduced more variations successively from different parameters into a pretraining dataset of 100k synthetic images from 237 different classes (as many categories as are available in Three-D-World \cite{gan2020threedworld}). We pre-trained a ResNet-50 \cite{he2016deep} on these, and evaluated this backbone with linear probing on different downstream tasks. The results are in Table \ref{tab:motivation_results}. We see that some parameters like random object materials result in improved performance for some downstream tasks like SVHN and DTD, while hurting performance for other tasks like EuroSAT and Sketch. In general different pre-training data properties seem to favor different downstream tasks.

\input{tables/motivation_results}

To maximize the benefit of pre-training, different optimal simulation parameters can be found for each specific downstream task. Because of the combinatorially large set of different simulation parameter configurations, a brute force search is out of the question. However, this might still suggest that some, presumably expensive, learning process is needed for each downstream task for an optimal synthetic image set for pre-training. We show this is not the case. 

We introduce \ours, a unified model that maps a downstream task representation to optimal simulation parameters for pre-training data generation to maximize downstream accuracy. Using vector representations for a set of downstream tasks (in the form of Task2Vec \cite{achille2019task2vec}), we train \ours to find and thus learn a mapping to optimal parameters for each task from the set. Once trained on this set of ``seen'' tasks, \ours can also use Task2Vec representations of novel ``unseen'' tasks to predict simulation parameters that would be best for their pre-training datasets. This efficient one-shot prediction for novel tasks is of significant practical value, if developed as an end-user application that can automatically generate and provide pre-training data, given some downstream examples.


Our extensive experiments using $20$ downstream classification datasets show that on seen tasks, given a number of images per category, \ours's output parameters generate pre-training datasets that are much better for downstream performance than approaches like domain randomization~\cite{yue2019domain,anderson2021sim,khirodkar2019domain} that are not task-adaptive. Moreover, we show \ours also generalizes well to unseen tasks, maintaining an edge over non-adaptive approaches while being competitive with Imagenet pre-training.

In summary,
(i) We address a novel, and very practical, problem---how to optimally leverage synthetic data to task-adaptively pre-train deep learning models for transfer to diverse downstream tasks.
To the best of our knowledge, this is the first time such a problem is being addressed in transfer learning research.  
(ii) We propose \ours, a unified parametric model that learns to map Task2Vec representations of downstream tasks to simulation parameters for optimal pre-training. 
(iii) \ours can generalize to novel ``unseen'' tasks, not encountered during training, a feature of significant practical value as an application. 
(iv) We provide a thorough analysis of the behavior of downstream accuracy with different sizes of pre-training data (in number of classes, object-meshes or simply images) and with different downstream evaluation methods. 


