\appendix

\section*{Appendices}

\section{Task2Vec}
We used Task2Vec~\cite{achille2019task2vec} representations for downstream tasks for our Task2Sim model. Task2Vec of a task consists of diagonal elements of the Fisher information matrix (FIM) of the outputs with respect to the network parameters over the distribution of downstream task examples (Refer to Section 2 of \cite{achille2019task2vec} for more details). For this purpose, following \cite{achille2019task2vec}, we used a single Imagenet pre-trained probe network with only the classifier layer trained on specific tasks (using the training set of examples for that task). In our experiments, a Resnet-18 probe network was used, resulting in a 9600-dimensional Task2Vec task representation.

\vspace{1mm}
\noindent \textbf{How much downstream data do we need access to?} In the case of models pre-trained using an approach that is not task-adaptive, there is no need to access any downstream data while pre-training. Given that task-adaptive approaches need a downstream task representation we used Task2Vec. Here, following~\cite{achille2019task2vec} we used all labeled examples from the training set of the downstream task to represent its distribution (in computing the FIM). However, we show that the FIM can be estimated by using fewer examples from the downstream task and the resulting Task2Vec vectors can be used to train Task2Sim with no degradation in performance (see \cref{tab:diff_task2vec}). This property also makes Task2Sim more practical since a user need not wait to collect labels for all data pertaining to their downstream application in order to generate pre-training data using Task2Sim.

\begin{table}[]
    \centering
    \begin{tabular}{c|cc}
    \multicolumn{1}{c|}{\multirow{2}[1]{*}{\thead{\textbf{Fraction of data} \\ \textbf{used for Task2Vec}}}} & \multicolumn{2}{c}{\textbf{Avg Downstream Acc.}} \\
          & Seen Tasks & Unseen Tasks \\
    \midrule
    100\% & 30.46 & 53.06 \\
    50\%  & 30.69 & 52.70 \\
    20\%  & 30.72 & 53.11 \\
    10\%  & 31.18 & 53.57 \\
    \end{tabular}%
    \caption{Average downstream performance (evaluated with 5NN classifier and using 40k images from 100 classes for pre-training) over seen and unseen tasks using different fractions of downstream training data (randomly subsampled) used to compute Task2Vec task representations for Task2Sim model. Task2Sim performance does not degrade when fewer downstream examples are used for computing Task2Vec.}
    \label{tab:diff_task2vec}
\end{table}

\section{Similarity between Learned Features}
\input{figures/supp/cka}

We used centered kernel alignment (CKA)~\cite{kornblith2019similarity} to find the similarity between features learned by the Resnet-50 backbone pre-trained on different image sets containing 100k images from 237 classes. \cref{fig:cka}, shows these similarities computed using the output features at different stages of the backbone (Stages 1-4 are intermediate outputs after different convolutional blocks in the resnet).

A few interesting phenomena surface: Task2Sim features (\ie features produced by a model pre-trained on Task2Sim generated dataset) are more similar to Imagenet features, than Domain Randomization. Thus Task2Sim in some manner, mimics features learned on real images better. 
We can also see that features early on in the network are largely similar across all kinds of pre-training and they only start differentiating at later stages, suggesting high similarity in lower level features (\eg edges, curves, textures, \etc) across different pre-training datasets. Also, as might be expected, features post downstream finetuning are more similar to each other than before, while still quite far away from being identical.


\section{Additional Results}

\subsection{Effect of Different Backbones} \label{subsec:diff_backbones}

\input{figures/supp/diff_backbones_seen}
\input{figures/supp/diff_backbones_unseen}

In \cref{fig:diff_backbones_seen,,fig:diff_backbones_unseen}, we show the average downstream performance over the seen and unseen tasks respectively, using different Resnet backbones (of different sizes). For this study, we used the same pre-training procedure across all backbones. We see that results are largely consistent with different backbones and for all of them Task2Sim performance is competitive with Imagenet pre-training and is much better than Domain Randomization. We also see that typically methods improve average downstream performance with the use of a larger backbone in the classifier. Moving from Resnet-50 to Resnet-101, Task2Sim performance breaks this trend and is lower indicating that the larger backbone could overfit in this case. This might be expected since Task2Sim was trained to optimize the performance of a Resnet-50 backbone.




\subsection{Task2Sim Results---Linear Probing} 

\input{figures/supp/main_lineval}

\cref{fig:main_lineval} shows the downstream accuracy with linear probing for different seen and unseen datasets where pre-training dataset has 100k images from all 237 classes. These complement \cref{fig:main_all_seen,,fig:main_all_unseen}, where downstream evaluation used full network finetuning. 

\subsection{Varying Pre-training Data Size}

\input{figures/supp/diff_num_cls_lineval}

\input{figures/supp/diff_num_objs_lineval}

\input{figures/supp/diff_num_imgs_lineval}

\noindent \textbf{Linear Probing.} \cref{fig:diff_num_cls_lineval,,fig:diff_num_objs_lineval,,fig:diff_num_imgs_lineval} are counterparts (with downstream evaluation done with linear probing) of \cref{fig:diff_num_cls,,fig:diff_num_objs,,fig:diff_num_imgs} respectively. We see that primarily similar findings as the main paper hold and in \cref{fig:diff_num_cls_lineval}, different backbones improve at a similar rate with more classes (and images for pre-training). In \cref{fig:diff_num_objs_lineval}, we see that both methods of synthetic pre-training improve their features with more object models, with Domain Randomization improving at a slightly higher rate. 

In \cref{fig:diff_num_imgs_lineval} we see some differences: There is a more severe saturating behavior of downstream performance, which even decreases by a little after a certain point for the synthetic pre-training data. This is likely because the feature extractor overfits to the pre-training task and a linear classifier on these features cannot perform as well. Both from \cref{fig:diff_num_cls_lineval} and from the curve for Imagenet-1K in \cref{fig:diff_num_imgs_lineval} we see that this saturating/overfitting behavior is somewhat alleviated by more classes in pre-training data. Another observation of note in \cref{fig:diff_num_imgs_lineval} is that the feature extractor pre-trained on Domain Randomization starts overfitting \emph{before} it matches the performance of Task2Sim. 
With \cref{fig:diff_num_imgs}, we mentioned that with more images a non-adaptive approach like domain randomization could improve its performance faster and sometimes equal a task-adaptive approach like Task2Sim. \cref{fig:diff_num_imgs_lineval} shows that although a non-adaptive approach may improve faster, it may not always match performance of its adaptive counterpart.

\vspace{1mm}
\noindent \textbf{Unseen Tasks.} \cref{fig:diff_num_cls_unseen,,fig:diff_num_objs_unseen,,fig:diff_num_imgs_unseen} show effect of above variations averaged over unseen tasks. We can see that similar trends hold in this case, as in case of seen datasets.

\subsection{Comparison with Large scale Pre-training (CLIP)} CLIP~\cite{radford2021learning} pre-trains on ~400M image-text pairs. Such large datasets when curated from the web, are bound to have privacy and other ethical concerns, as discussed in the paper. CLIP pre-training is also much more expensive than its counterparts using our synthetic data. We conducted an experiment finetuning a Resnet-50 model using pre-trained weights from CLIP on our tasks, while noting that this CLIP pre-trained Resnet-50 is different from the standard model used by us and uses more parameters (38M in CLIP Resnet50 vs 25M in standard Resnet50). The result was 77.33\% avg. accuracy on seen tasks and 91.56\% avg. accuracy on unseen tasks, which is comparable to the best Task2Sim performance (79.10\% over seen tasks and 91.50\% over unseen tasks).





\section{Synthetic Image Generation} \label{sec:syn-data}

\input{figures/supp/sim_variations}

We used Three-D-World (TDW)~\cite{gan2020threedworld} for synthetic image generation. It is a platform built using the Unity3D engine, and besides a python interface, provides asset bundles which include 3D object models, interactive 3D scenes, and HDRI skyboxes (360$^\circ$ images of real scenes accompanied with lighting information). TDW is available under a BSD 2-Clause "Simplified" License.

For our implementation, we used all 2322 object models from 237 different classes available in TDW. We use a generator that imports one object into a simple scene with an HDRI-skybox background. It then, changes different properties of the scene/object/camera based on 8 simulation parameters as mentioned in \cref{subsec:expts_details}. Whenever different variations corresponding to a simulation parameter are to be included, values are chosen uniformly at random within an appropriate range (via a careful choice of the extremes). \cref{fig:sim_variations} has 8 rows corresponding to each of the simulation parameters used for Task2Sim. Each row shows using 5 images, the variations corresponding to its specific simulation parameter. 
Generating 1M images using our generator with all 2322 objects, takes around 12 hours on an Nvidia Tesla-V100 GPU. Given the number of objects we used in our implementation, a bottleneck in image generation is the speed of loading object meshes into Unity3D. Hence, we used a subset of 780 objects from 100 classes with relatively simpler meshes, for generating the data used for training Task2Sim. The 8 parameters we used result in a total of $2^8 = 256$ different possibilities and so we pre-generated these 256 sets of 40k images each for faster and smoother training of the Task2Sim model. Each of these 256 sets took $\sim$30 mins to generate on a Tesla-V100 GPU.

\section{Training and Evaluation} \label{sec:train_details}
We based our implementation of different classifiers for pre-training and downstream evaluation on pytorch-image-models~\cite{rw2019timm}. For all experiments except those in \cref{subsec:diff_backbones}, we used a Resnet-50 backbone for our classifier. For all datasets while pre-training, we used the following parameters: we trained for 100 epochs using an AdamW optimizer, using a learning rate 0.001 and a batch size of 1024. The learning rate used a linear warmup for 20 epochs and a cosine annealing schedule following warmup. We use regularization methods like label-smoothing, cutmix~\cite{yun2019cutmix} and mixup~\cite{zhang2017mixup} following a training strategy from \cite{rw2019timm}. We used image augmentation in the form of RandAugment~\cite{cubuk2020randaugment} while pre-training.

For downstream evaluation, we followed a procedure similar to \cite{islam2021broad}. For both evaluations using linear probing and full-network finetuning, we used 50 epochs of training using an SGD optimizer with learning rate decayed by a tenth at 25 and 37 epochs. No additional regularizers or data augmentation approaches were used. For each downstream task, we did a coarse hyperparameter grid-search over learning rate $\in \{10^{-5}, 10^{-4}, 10^{-3}, 10^{-2}\}$, optimizer weight decay $\in \{0, 10^{-5}\}$ and training batch size $\in \{32, 128\}$. We found by comparing backbones pre-trained on Imagenet and a large synthetic set generated with Domain Randomization, that with the above grid, for each specific downstream task and evaluation method, a particular set of hyperparameters worked best irrespective of the pre-training data. This was found using a separate validation split created from the downstream training set with 30\% of the examples. Given this finding, we fixed these hyperparameters for a given downstream task and evaluation method for all remaining experiments.

\section{Details of Downstream Tasks} \label{sec:downstream_details}
\cref{tab:dataset_details} shows the number of classes in each of the 20 downstream tasks we used. It also shows the number of images in the training and test splits for each.

\begin{table}[h]
    \centering
    \scalebox{0.8}{
    \begin{tabular}{l| l|r r r }
        \toprule
        Category & Dataset & Train Size & Test Size & Classes \\ 
        \midrule
        \multirow{3}{*}{Natural}
        & CropDisease~\cite{mohanty2016cropdisease} & 43456 & 10849 & 38 \\%&  \\
        & Flowers~\cite{nilsback2008automatedflowers102} & 1020 & 6149 & 102 \\%&  \\
        & DeepWeeds~\cite{DeepWeeds2019} & 12252 & 5257 & 9 \\%&  \\
        & CUB~\cite{WahCUB_200_2011} & 5994 & 5794 & 200 \\%&  \\
        \midrule
        \multirow{2}{*}{Satellite}
        & EuroSAT~\cite{helber2019eurosat} & 18900 & 8100 & 10 \\%&  \\
        & Resisc45~\cite{cheng2017remoteresisc45} & 22005 & 9495 & 45 \\%&  \\
        & AID~\cite{xia2017aid} & 6993 & 3007 & 30 \\%&  \\
        & CactusAerial~\cite{lopez2019columnarCactusAerial} & 17500 & 4000 & 2 \\%&  \\
        \midrule
        \multirow{2}{*}{Symbolic}
        & Omniglot~\cite{lake2015humanomniglot} & 9226 & 3954 & 1623 \\%&  \\
        & SVHN~\cite{netzer2011readingsvhn} & 73257 & 26032 & 10 \\%&  \\
        & USPS~\cite{hull1994databaseUSPS} & 7291 & 2007 & 10 \\%&  \\
        \midrule
        \multirow{2}{*}{Medical}
        & ISIC~\cite{codella2019skinisic} & 7007 & 3008 & 7  \\%&  \\
        & ChestX~\cite{wang2017chestx} & 18090 & 7758 & 7 \\%&  \\
        & ChestXPneumonia~\cite{kermany2018identifyingChestXP} & 5216 & 624 & 2 \\
        \midrule
        \multirow{2}{*}{Illustrative}
        & Kaokore~\cite{tian2020kaokore} & 6568 & 821 & 8 \\%&  \\
        & Sketch~\cite{wang2019learningsketch} & 35000 & 15889 & 1000 \\%&  \\
        & PACS-C~\cite{li2017deeperPACS} & 2107 & 237 & 7 \\%&  \\
        & PACS-S~\cite{li2017deeperPACS} & 3531 & 398 & 7 \\%&  \\
        \midrule
        \multirow{2}{*}{Texture}
        & DTD~\cite{cimpoi2014DTD} & 3760 & 1880 & 47 \\%&  \\
        & FMD~\cite{zhang2019poissonFMD} & 1400 & 600 & 10 \\%&  \\
        \bottomrule
    \end{tabular}
    }
    \caption{Number of classes in each downstream task and number of images in each training and test split.}
    \label{tab:dataset_details}
\end{table}

\input{figures/supp/diff_num_cls_unseen}

\input{figures/supp/diff_num_objs_unseen}

\input{figures/supp/diff_num_imgs_unseen}


\section{Limitations} \label{sec:limitations}
In this paper, we constrained our demonstration to a relatively low number of datasets and simulation parameters, limited by data generation, pre-training and evaluation speed. If these processes can be made more efficient, in future work, we can expect to use more simulation parameters (with possibly more discrete options or even real-valued ranges), and use more datasets for training \ours, allowing it to be more effective in deployment as a practical application.

While a large portion of contemporary representation learning research focuses on self-supervision to avoid using labels, we hope our demonstration with Task2Sim motivates further research in using simulated data from graphics engines for this purpose, with focus on adaptive generation for downstream application. 

\section{Societal Impact}
In the introduction, we discussed model pre-training using large real image datasets was what paved the way for a gamut of transfer learning research. Using real images is however riddled with curation costs and others concerns around privacy, copyright, ethical usage, etc. The fact that downstream performance on average correlates positively with the size of pre-training data, created a race for curating bigger datasets. Corporations with large resources are able to invest in such large-scale curation and create datasets for their exclusive use (\eg JFT-300M~\cite{chollet2017xception,hinton2015distilling}, or Instagram-3.5B~\cite{mahajan2018exploring}), which are unavailable to a range of research on downstream applications.

Using synthetic data for pre-training can drastically reduce these costs, because potentially infinite images can be rendered once 3D models and scenes are available, by varying various simulation parameters. In this paper, we demonstrated that the optimal use of such a simulation engine can be found in restricting certain variations, and that different restrictions benefit different downstream tasks. Our Task2Sim approach, can be used as the basis for a pre-training data generator, which as an end-user application can allow research on a wide range of downstream applications to have access to the benefits of pre-training on large-scale scale data. This does not create any direct impacts on average individuals, but could do so through the advancement in downstream applications. One particular case, as an example, could be the advancement in visual recognition systems in the medical domain, possibly making the diagnosis of illnesses faster and cheaper.