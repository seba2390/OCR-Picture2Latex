\vspace{-1mm}
\section{Related Work} \label{sec:related_work}

\noindent {\bf Training with Synthetic Data.} Methods that learn from synthetic data have been extensively studied since the early days of computer vision~\cite{nevatia1977description,little1988analysis}. In recent years,  many approaches that rely on synthetic data representations have been proposed for image classification~\cite{mikami2021scaling,gan2020threedworld}, object detection~\cite{peng2015learning,prakash2019structured}, semantic segmentation~\cite{wang2020differential,ros2016synthia}, action recognition~\cite{varol2021synthetic,roberto2017procedural}, visual reasoning~\cite{johnson2017clevr}, and embodied perception~\cite{savva2019habitat,kolve2017ai2,xia2018gibson}. While most of these rely on some graphics engines to generate synthetic images mimicking real ones, it has been observed that images seemingly consistent of noise can still be useful for representation learning~\cite{baradad2021learning}.
Unlike previous work, we focus on a different problem: how to build task-adaptive pre-trained models from synthetic data that can transfer to a wide range of downstream datasets from various domains.

\input{figures/controller-approach}

\vspace{1mm}
\noindent {\bf Synthetic to Real Transfer.} The majority of methods proposed to bridge the {\em reality gap} (between simulation and real data) are based on domain adaptation~\cite{csurka2017domain}. These include reconstruction-based techniques, using encoder-decoder models or GANs to improve the realism of synthetic data~\cite{richter2021enhancing,hoffman2018cycada,shrivastava2017learning},  discrepancy-based methods, designed to align features between the two domains~\cite{rozantsev2018beyond,zhang2019curriculum}, and adversarial approaches, which rely on a domain discriminator to encourage domain-independent feature learning~\cite{ren2018cross, ganin2016domain,tzeng2017adversarial}. Contrasting from these techniques, our work aims at building pre-trained models from synthetic data and does not assume the same label set for source and target domains. 
The most prevalent approach in a setting similar to ours, is domain randomization~\cite{tobin2017domain, prakash2019structured,yue2019domain,anderson2021sim,khirodkar2019domain}, which learns pre-trained models from datasets generated by randomly varying simulator parameters. In contrast, \ours \textit{learns} simulator parameters to generate synthetic datasets that maximize transfer learning performance.

\vspace{1mm}
\noindent {\bf Optimization of Simulator Parameters.} Recently, a few approaches have been proposed to learn synthetic data generation by optimizing simulator parameters~\cite{ruiz2018learning,kim2021drivegan,yang2020learning,behl2020autosimulate}. SPIRAL~\cite{ganin2018synthesizing}, AVO~\cite{louppe2019adversarial} and Attr. Desc.~\cite{yue2019domain} minimize the distance between distributions of simulated data and real data. Learning to Simulate~\cite{ruiz2018learning} optimizes simulator parameters using policy gradients that maximize validation accuracy for a specific task, while Auto-Sim~\cite{behl2020autosimulate} speeds up the search process using a differentiable approximation of the objective.
Meta-Sim~\cite{kar2019meta,devaranjan2020meta} learns to modify attributes obtained from probabilistic scene grammars for data generation.  These methods are specifically tailored to applications in autonomous driving, whereas our goal is to transfer synthetic data representations to a wide range of downstream tasks. Notably, our proposed approach is significantly different from previous methods, as it maps task representations to simulation parameters through a unified parametric model, enabling one-shot synthetic data generation, even for unseen tasks, without requiring expensive training.

\vspace{1mm}
\noindent {\bf Conditional Computation.} Albeit not apparent, our method is also related to dynamic neural network models that adaptively change computation depending on the input~\cite{han2021dynamic}. These methods have been effectively used to skip computation in deep neural networks conditioned on the input~\cite{wu2018blockdrop,wang2018skipnet,veit2018convolutional}, perform adaptive fine-tuning~\cite{guo2019spottune}, and dynamically allocate computation across frames for efficient video analysis~\cite{wu2019adaframe,meng2020ar}. In particular, Adashare~\cite{sun2019adashare} learns different computational pathways for each task within a single multi-task network model, with the goal of improving efficiency and minimizing negative interference in multi-task learning. Analogously, our approach learns different {\em data simulation pathways} (by adaptively deciding which rendering parameters to use) for each task, using a single parametric model, with the goal of generating task-specific pre-training data.

