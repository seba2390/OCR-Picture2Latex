
\section{Experiments} \label{sec:expts}

\subsection{Details} \label{subsec:expts_details}

\noindent \textbf{Downstream Tasks.} We use a set of 20 classification tasks with
12 tasks from \cite{islam2021broad} as the set of ``seen'' tasks for our model and a separate set of 8 tasks as the ``unseen'' set.
All our tasks can be broadly categorized into the following 6 classes (S:seen, U:unseen): 
\begin{itemize}
\vspace{-0.5em}
\itemsep-0.2em
\item Natural Images: CropDisease(S)\cite{mohanty2016cropdisease}, Flowers102(S)\cite{nilsback2008automatedflowers102}, DeepWeeds(S)\cite{DeepWeeds2019}, CUB(U)\cite{WahCUB_200_2011}
\item Aerial Images: EuroSAT(S)\cite{helber2019eurosat}, Resisc45(S)\cite{cheng2017remoteresisc45}, AID(U)\cite{xia2017aid}, CactusAerial(U)\cite{lopez2019columnarCactusAerial}
\item Symbolic Images: SVHN(S)\cite{netzer2011readingsvhn}, Omniglot(S)\cite{lake2015humanomniglot}, USPS(U)\cite{hull1994databaseUSPS}
\item Medical Images: ISIC(S)\cite{codella2019skinisic}, ChestX(S)\cite{wang2017chestx}, ChestXPneumonia(U)\cite{kermany2018identifyingChestXP}
\item Illustrative Images: Kaokore(S)\cite{tian2020kaokore}, Sketch(S)\cite{wang2019learningsketch}, Pacs-C(U), Pacs-S(U)\cite{li2017deeperPACS}
\item Texture Images: DTD(S)\cite{cimpoi2014DTD}, FMD(U)\cite{zhang2019poissonFMD}
\end{itemize}
\vspace{-0.3em}

\noindent \textbf{Task2Sim Details.} We used a Resnet-18 probe network to generate 9600-dimensional Task2Vec representations of downstream tasks. The Task2Sim model is a multi-layer perceptron with 2 hidden layers, having ReLU activations. The model shares its first two layers for all $M$ heads, and branches after that. It is trained for 1000 epochs on seen tasks, with a batch-size 4 and 5 self-imitation steps (\ie $n=4, T_{si} = 5$ and $T = 1000$). We used a Resnet-50 model for pre-training and downstream evaluation for \ours's rewards. Details of all datasets, pre-training and evaluation procedures are included in Appendix \ref{sec:train_details}.


\vspace{1mm}
\noindent \textbf{Synthetic Data Generation.} We use Three-D-World (TDW) \cite{gan2020threedworld} for synthetic image generation. The platform provides 2322 different object models from 237 different classes, 57 of which overlap with Imagenet. Using TDW, we generate synthetic images of single objects from the aforementioned set (see Figure~\ref{fig:fig1} for examples). 

In this paper, we experiment with a parameterization of the pretraining dataset where $M = 8$ and $k_i = 2 ~\forall~ i \in [M]$ (using terminology from Section \ref{sec:method}). The 8 parameters are: 
\begin{itemize}
\vspace{-0.3em}
\itemsep-0.2em
    \item Object Rotation : If $1$, multiple poses of an object are shown in the dataset, else, an object appears in a canonical pose in each image.
    \item Object Distance (from camera) : If $1$, object distance from the camera is varied randomly within a certain range, else, it is kept fixed.
    \item Lighting Intensity : If $1$, intensity of the main lighting source (sun-like point light source at a distance) is varied, else it is fixed.
    \item Lighting Color : If $1$, RGB color of the main lighting source is varied, else it is fixed.
    \item Lighting Direction : If $1$, the direction of the main light source is varied, else it is a constant.
    \item Focus Blur : If $1$, camera focus point and aperture are randomly perturbed to induce blurriness in the image, else, all image contents are always in focus.
    \item Background : If $1$, background of the object changes in each image, else it is held fixed.
    \item Materials : If $1$, in each image, each component of an object is given a random material out of 140 different materials, else objects have their default materials. 
\end{itemize}
\vspace{-0.5em}
Hence in our experiments, for each of the above 8 parameters, \ours decided whether or not different variations of it, would exhibit in the dataset. For speed of dataset generation while training \ours, we used a subset of 780 objects with simple meshes, from 100 different categories and generated 400 images per category for pre-training.



\subsection{\name~Results}

\noindent \textbf{Baselines.} We compared Task2Sim's downstream performance with the following baselines (pre-training datasets):
(1) Random : For each downstream dataset, chooses a random 8-length bit string as the set of simulation parameters. 
(2) Domain Randomization : Uses a 1 in each simulation parameter, thus using all variations from simulation in each image. 
\interfootnotelinepenalty=10000
(3) Imagenet : Uses a subset of Imagenet with equal number of classes and images as other baselines\footnote{ We also compared pre-training using Imagenet with 1K classes and an equal number of images, but this was poorer on average in downstream performance than the subset with fewer classes. \cref{tab:main_seen,,tab:main_unseen} and \cref{fig:main_all_seen,,fig:main_all_unseen} do not include it for succinctness.}.
(4)~Scratch : Does not involve any pre-training of the classifier's feature extractor, training a randomly initialized classifier, with only downstream task data.



\input{tables/main_seen}
\input{tables/main_unseen}

\input{figures/main_all_seen}
\input{figures/main_all_unseen}

\vspace{1mm}
\noindent \textbf{Performance on Seen Tasks.} \cref{tab:main_seen} shows accuracies averaged over 12 seen downstream tasks for \ours and all baselines using different evaluation methods for a Resnet-50 backbone. For the last two columns, we included all objects of TDW from 237 categories, and kept the number of images at roughly 400 per class, resulting in about 100k images total, regenerating a new dataset with the simulation parameters corresponding to the different synthetic image generation methods. On average, over the 12 seen tasks, simulation parameters that \ours finds are better than Domain Randomization and Random selection and are competitive with Imagenet pre-training, both for the subset of classes that Task2Sim is trained using, and when a larger set of classes is used. \cref{fig:main_all_seen} shows accuracies for the 12 seen datasets for different methods, on the 237 category 100k image pre-training set.



\vspace{1mm}
\noindent \textbf{Performance on Unseen Tasks.} \cref{tab:main_unseen} shows average downstream accuracy over 8 unseen datasets, of a Resnet-50 pretrained on different datasets. 
We see that Task2Sim generalizes well, and is still better than Domain Randomization and Random simulation parameter selection. Moreover, it is marginally better on average than Imagenet pretraining for these tasks. \cref{fig:main_all_unseen} shows the accuracies from the last column of \cref{tab:main_unseen} over the 8 individual unseen tasks.




\subsection{Analysis} \label{subsec:analysis}
\noindent \textbf{Task2Sim Outputs.}
\cref{fig:t2s_outputs} shows the output distribution from the trained Task2Sim model for different seen and unseen tasks. Each output shows the probability assigned by the model to the output 1 in that particular simulation parameter. From the outputs, we see the model determines that in general for the set of tasks considered, it is better to see a single pose of objects rather than multiple poses, and that it is better to have scene lighting intensity variations in different images than have lighting of constant intensity in all images. In general, adding material variability was determined to be worse for most datasets, except for SVHN. Comparing predictions for seen vs unseen tasks, we see that Task2Sim does its best to generalize to unseen tasks by relating them to the seen tasks. 
For \eg, outputs for ChestXPneumonia are similar to ChestX, while outputs of CactusAerial are similar to those of EuroSAT, both being aerial/satellite image datasets. A similar trend is also seen in PacsS and Sketch both of which contain hand-sketches, and for CUB and CropDisease, both natural image datasets.

\input{figures/t2s_outputs}

Another inspection shows Task2Sim makes decisions that are quite logical for certain tasks. For instance, Task2Sim turns off the ``Light Color'' parameter for CUB. Here, color plays a major role in distinguishing different birds, thus needing a classifier representation that should not be invariant to color changes. Indeed, from \cref{fig:nn}, we see that the neighbors of Task2Sim are of similar colors. 


\input{figures/diff_num_cls}

\input{figures/diff_num_objs}

\input{figures/diff_num_imgs}

\input{figures/nn}

\vspace{1mm}
\noindent \textbf{Effect of Number of Pretraining Classes.}
In \cref{fig:diff_num_cls}, we plot the average accuracy with full network finetuning on the 12 seen downstream tasks. On the x-axis, we vary the number of classes used for pre-training, with 1000 images per class on average (200 classes=200k images). We see all pre-training methods improve with more classes (and correspondingly more images) at about similar rates. Task2Sim stays better than Domain Randomization and competitive with (about 2\% shy of) pre-training with an equivalent subset (in number of classes and images) of Imagenet.



\vspace{1mm}
\noindent \textbf{Effect of Number of Different Objects per Class.} In TDW, we have 2322 object meshes from 237 different categories. In \cref{fig:diff_num_objs}, we vary the number of object meshes used per category. The point right-most on the x-axis has 200k images with all objects used, and moving to the left, the number of images reduces proportionately as a fraction of these objects are used (the number of categories being the same). We find that with increasing number of different objects used for each category, Domain Randomization improves downstream performance at a slightly higher rate than our proposed Task2Sim. 


\vspace{1mm}
\noindent \textbf{Effect of Number of Pretraining Images.} In \cref{fig:diff_num_imgs}, we show average downstream task accuracy, for the 12 seen tasks, with different number of images used for pretraining. All methods, except Imagenet-1K and Scratch, use 237 image categories, with synthetic datasets using all available object models. Imagenet-237 is a subset of Imagenet-1K containing 237 categories that were randomly picked.
We see Task2Sim is highly effective in the regime where fewer images are available for pre-training, and is even slightly better than pre-training with Imagenet at 50k images. It maintains its advantage over non-adaptive pretraining up to a significant extrapolation of 500k images, having only trained using smaller datasets (of 100 classes and 40k images). At 1M images, it is still competitive with Imagenet pre-training and is much better than training from scratch. 

We also observe that all methods improve when more pre-training images are available, although the rate of improvement decreases as we move along positive X-direction.
Initially, Domain Randomization improves at a higher rate than Task2Sim and at 1M pretraining images, matches its performance. This is likely because at a higher number of images, even when there are all variations possible from simulation in each image (corresponding to Domain Randomization), the deep feature extractor grows robust to the variations which may not add any value to the representation for specific downstream tasks. 

Our hypothesis is that at a fixed number of categories there may exist some point in number of pre-training images when the above robustness can be good enough to match our Task2Sim's downstream performance. With a 237-category limit from TDW and using the set of variations from our 8 chosen parameters, 1M images seems to be this point. However as the number of classes increases, this point shifts towards higher number of images. As evidence, consider \cref{fig:diff_num_cls}, where we see that as more classes of objects are added with more data, different methods improve at similar rates. Moving further along positive X, if this holds with more classes, Task2Sim maintains its edge over Domain Randomization even at higher numbers of images. This suggests a non-adaptive pre-training method like Domain Randomization has potential to be as effective on average as Task2Sim, but only at the cost of more pre-training images. However, this cost would keep increasing as pre-training data encompasses more object categories, and would be unknown without experimentation. 

For additional results and discussions, we refer readers to the Appendix.
















