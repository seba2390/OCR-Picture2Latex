





\vspace{-1mm}
\section{Conclusion} \label{sec:conclusion}
\vspace{-1mm}

We saw the approach that is optimal for downstream performance in using synthetic data for pre-training is to specifically adapt the synthetic data to different downstream tasks. In this paper, we parameterized our synthetic data via different simulation parameters from graphics engines, and introduced \ours, which learns to map downstream task representations to optimal simulation parameters for synthetic pretraining data for the task. We showed \ours can be trained on a set of ``seen'' tasks and can then generalize to novel ``unseen'' tasks predicting parameters for them in one-shot, making it highly practical for synthetic pre-training data generation. While a large portion of contemporary representation learning research focuses on self-supervision to avoid using labels, we hope our demonstration with Task2Sim motivates further research in using simulated data from graphics engines for this purpose, with focus on adaptive generation for downstream application.

 



\vspace{1mm}
\noindent\textbf{Acknowledgements.} This material is based upon work supported by the Defense Advanced Research Projects Agency (DARPA) under Contract No. FA8750-19-C-1001. Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the Defense Advanced Research Projects Agency (DARPA). This work was also supported by Army Research Office Grant W911NF2110246, National Science Foundation grants CCF-2007350 and CCF-1955981, and the Hariri institute at Boston University. We would also like to thank the developers of TDW: Seth Alter, Abhishek Bhandwaldar and Jeremy Schwartz, for their assistance with the platform and its use.