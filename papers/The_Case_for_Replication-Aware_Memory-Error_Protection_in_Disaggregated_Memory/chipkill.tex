
\begin{figure}[tb]
\centering
\includegraphics[width=3.5in]{fig/overhead}
\caption{DUE, NDE, and storage overhead for different chipkill protection schemes.
The solid rectangle (in the top-left figure) marks the DUE and storage overhead of the original chipkill design~\cite{zhang:pm-chipkill:micro:2018}.
NDE is shown only for baseline chipkill as it is independent of replication and identical for all chipkill schemes.
}
\label{fig:overhead}
\vspace{-0.4cm}
\end{figure}

\section{Replication-Aware Chipkill-Correct}

We use our analytical model to study trade offs between reliability and storage overhead for a recent chipkill design~\cite{zhang:pm-chipkill:micro:2018}. 
The design achieves storage efficiency through a two-tier protection scheme: 
(i) a performance tier reuses the chip failure protection bits to opportunistically correct bit errors at high performance, and 
(ii) a storage-optimized tier uses long ECC codewords to correct at low storage cost bit errors that are detected but uncorrected by the performance tier.
The storage-optimized tier uses a BCH(2312,2048,22) code for each ECC codeword. BCH(n,k,t) uses a codeword of length $n=k+t(\left \lceil{log_2 (k)} \right \rceil+1)$ to correct $t$ bad bits when protecting $k$ bits of data~\cite{zhang:pm-chipkill:micro:2018}.
%BCH(n,k,t) code uses a codeword of length $n$ to protect $k$ bits of data by correcting up to $t$ bad bits. 
%The codeword length $n$ is $k+t(\ceil{log_2(k)+1})$. 

We show how we use our model to further optimize the storage-optimized tier. First, we need to compute the base cache-line failure probabilities of the storage-optimized tier.
We compute the cache-line failure probability due to DUE as the 
the probability that the storage-optimized tier fails to correct multiple bit errors in the BCH codeword (which happens when there are at least $t$ bit errors):

\begin{equation*}
\pcdue= \sum_{i=t+1}^{n}\binom{n}{i}{RBER}^i\cdot{RBER}^{n-i}
\end{equation*}

\ignore{
We compute the cache-line failure probability due to DUE as the product of two terms:
the probability that the performance tier fails to correct a bit error (whose value equals to 0.018 as is taken from the original design~\cite{zhang:pm-chipkill:micro:2018}) and 
the probability that the storage-optimized tier fails to correct multiple bit errors in the BCH codeword (which happens when there are at least $t$ bit errors):

\begin{equation*}
\pcdue= 0.018 \times \sum_{i=t+1}^{n}\binom{n}{i}{RBER}^i\cdot{RBER}^{n-i}
\end{equation*}
}

\noindent We compute the cache-line failure probability due to NDE following the analysis of Kim and Lee~
\cite{kim:undetected-error-bch:ieee-tc:1996}.
We assume an NVM technology with $RBER=2\times10^{-4}$, as in~\cite{zhang:pm-chipkill:micro:2018}.

We then use our model to estimate the combined DUE rate resulting from using available replicas to correct DUEs. 
We study three protection schemes: a baseline scheme that relies solely on chipkill (without redundancy) to protect blocks and two application-level redundancy schemes.
For the two redundancy schemes, we choose parameters so that they can tolerate up to two replica failures (following standard practice),
that is N=3 for primary-backup replication and N=5 and K=3 for erasure coding.
For all three schemes, we vary storage overhead by varying the strength of the BCH code used by the storage-optimized tier to protect individual replica blocks.
\revisionhighlight{
We vary strength by varying the number of $t$ bit errors that can be corrected by the BCH code.
}
We assume uniform access to all logical blocks and that all physical blocks are equally vulnerable to memory errors.
\revisionhighlight{
All replicas use the same ECC.
}

Figure~\ref{fig:overhead} plots combined DUE rate and NDE rate of individual physical blocks as a function of storage overhead.
For each replication scheme, the storage overhead is calculated over a corresponding baseline that employs the same replication scheme but without chipkill protection. 
For the top two figures, we use a physical block size equal to the cache line size, that is 64 bytes.
For the bottom-left figure, we vary the block size and plot the storage overhead sustained to achieve the same level of DUE as the original chipkill design.
\revisionhighlight{
For the bottom-right figure, we vary the number of replicas and plot the storage overhead sustained to achieve the same level of DUE as the original chipkill design.
}

We observe that both primary-backup replication and erasure coding can achieve the same level of DUE as the original ckipkill design ($\sim10^{-33}$ DUE rate), albeit at about $9\%$ less overhead.
\revisionhighlight{
For a target SDC rate of $10^{-22}$~\cite{zhang:pm-chipkill:micro:2018}, we need to provision an extra $2.4\%$ overhead, bringing the storage overhead savings down to $6.6\%$.
}
Although not shown, the relative performance overhead is negligible, less than $10^{-11}$.
Moreover, we observe diminishing returns in storage-overhead-savings as we increase the number of replicas, suggesting that \ramp could be more beneficial with low replication factors. Overall, these results confirm our main hypothesis: by weakening the protection of each individual replica, we can lower the storage overhead while we can rely on the combined protection conferred by multiple replicas to meet a stronger protection target.

