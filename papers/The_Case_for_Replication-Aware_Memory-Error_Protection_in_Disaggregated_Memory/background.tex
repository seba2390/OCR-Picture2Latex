\section{Disaggregated Memory}\label{sec:background}

\subsection{Enabling technologies and architecture}
Disaggregated memory builds on recent technology advances on two fronts.
First, non-volatile memory (NVM) technologies, such as phase-change memory (PCM), resistive random access memory (ReRAM), and commercially available Intel Optane DC Persistent Memory (DCPMM), provide byte-addressable persistent storage accessible via load/store instructions, rather than I/O requests. In addition to non-volatility, these technologies provide the potential for increased memory density and increased energy efficiency relative to DRAM. DCPMM has 2x higher read latency and 8x lower write bandwidth than DRAM, but it is up to 10x faster than Flash. %and supports up to 3 TB per processor. 
Second, high-performance interconnects provide sub-microsecond access latency to remote memory~\cite{knebel:genz:hotchips:2019,pinto:thymesis:micro:2020}, while future interconnects based on silicon photonics are expected to further reduce latency~\cite{knebel:genz:hotchips:2019}.

These two technology advances provide the building blocks for constructing disaggregated memory architectures, where decentralized compute and memory nodes are interconnected by a high-performance system interconnect. 
Compute nodes mainly provide processing capability, but they also include a small amount of DRAM memory used as a local cache.
Memory nodes provide memory capacity in the form of NVM by attaching standard NVM subsystems to the network. 
Although the microarchitecture design of NVM subsystems is more complex than conventional DRAM subsystems\ignore{~\cite{wang:model-nvm:micro:2020}}, at a high level, NVM subsystems follow similar chip structure and system organization as DRAM subsystems: 
a memory controller is connected to memory modules through one or more channels, and each module provides an interface for accessing data stored across multiple chips, with chips comprising arrays of NVM bit cells. 

\subsection{Memory failures}
\label{sec:failure-model}
Disaggregation provides separate fault domains between processing and memory, meaning that the failure of a compute node does not render disaggregated memory unavailable, and vice versa, that is when a memory node fails, compute and other memory nodes continue to function. 
In this work, we focus on memory node failures caused by memory errors.

Memory errors may occur due to a variety of reasons.
First, memory errors may occur due to NVM bit cell errors. 
Bit errors are random in nature and can be caused by permanent faults due to limited and variable endurance, and transient faults due to resistance drift and read disturb\ignore{~\cite{yoon:freep:hpca:2011}}. 
Raw bit error rate (RBER) in PCM and ReRAM is significantly higher than in DRAM and ranges from $10^{-3}$ to $10^{-5}$~\cite{zhang:pm-chipkill:micro:2018}, depending on the technology and time since last write or refresh.
%
Second, memory errors may also occur when other components of the memory subsystem fail. 
Since NVM subsystems follow similar organization as DRAM subsystems, NVM subsystems will likely suffer from similar failures, including memory controller and memory channel failures due to \ignore{transient failures due to signal disturbances, and permanent failures due to }
faults in logic and transmission circuitry. 

To protect against memory errors, NVM subsystems maintain error correcting codes (ECC) computed over data. 
These codes can detect and correct a small number of errors.
For example, single error correction double error detection (SEC-DEC) uses parity to detect up to two-bit errors or correct a single-bit error.
Chipkill uses wider ECC to protect against multi-bit errors and chip failures.
Detectable but uncorrectable memory errors (DUE), which are detected but cannot be corrected by ECC, can cause memory node failures.
Non-detectable memory errors (NDE), which are non-detected and potentially miscorrected by ECC, do not cause memory node failures but may cause silent data corruption (SDC), which is also higly undesirable. 
For dense NVM with high RBER, simply extending existing memory protection mechanisms with stronger codes to achieve a low uncorrectable bit error rate (UBER) and low SDC rate incurs prohibitive storage overheads~\cite{zhang:pm-chipkill:micro:2018}.
These overheads remain significant ($\sim27\%$) despite recent efforts on improving storage
efficiency~\cite{zhang:pm-chipkill:micro:2018}. 

\ignore{
\subsection{Memory errors and their handling}
\label{sec:failure-model}
Disaggregation provides separate fault domains between processing and memory, meaning that the failure of a compute node does not render disaggregated memory unavailable, and vice versa, that is when a memory node fails, compute and other memory nodes continue to function.

In this work, we focus on memory node failures. These may happen due to several reasons. First, a memory node may fail due to a random NVM bit cell error. Bit cells are susceptible to permanent failures due to limited and variable endurance, and transient failures due to resistance drift and read disturb~\cite{yoon:freep:hpca:2011}. RBER in PCM and ReRAM is significantly higher than in DRAM, ranging from $10^{-3}$ to $10^{-5}$~\cite{zhang:pm-chipkill:micro:2018}.
%
Second, a memory node may fail due to a memory subsystem failure. 
At a very high level, NVM subsystems follow similar chip structure and system organization as DRAM subsystems, comprising a memory controller that is connected to multiple memory chips through one or more channels.
However, their microarchitecture design is more complex than conventional DRAM subsystems~\cite{wang:model-nvm:micro:2020}.
Hence, NVM subsystems will likely suffer from similar or more complex failures, including transient failures due to signal disturbances, and permanent failures due to faults in logic and transmission circuitry. 
%
Finally, a memory node may fail due to 
%failure in the power distribution units or 
failure in the network-interface card (NIC) that connects a memory node with the rest of the system.
}

\ignore{
Memory nodes incorporate hardware-level error-correcting code (ECC) mechanisms to protect against memory errors. Memory nodes fail when such protection mechanisms detect but cannot correct an underlying memory error. 

Tolerating NVM cell failures involves device- and architecture-level techniques that mitigate endurance-related permanent failures through write-efficient coding, memory remapping  and embedded redirection of failed lines8, , and mitigate transient failures through ECC8. However, maintaining a correctable error rate below 10-15 that is necessary with petabyte memory sizes expected in rack-scale DM requires using strong BCH ECC with high energy and die area overheads, up to 30% . Despite hardwareâ€™s best efforts, undetected memory errors may still trickle into software and cause disastrous silent data corruption12. 


Silent data corruption?

memory node failures can happen due to: cell, chip, controller, nic, failures
hardware-level protection techniques addrsss cell, chip failures
software-level replication addresses uncorrected errors


technology: persistent memory, nvram, 
failure model:
- disaggregation failures: compute nodes, memory nodes: memory controllers, bit errors, 
- memory bit errors: cite rber of pcm/reram

compute and memory nodes fail independently

How do memory nodes fail?

Bit errors in NVM, memory controller errors, circuit errors (how do these differ from bit errors?)
- sources of bit errors: nvm cells, circuit lines, etc. Here, we focus on nvm cells

ECC addresses bit errors

assumption: 
-memory nodes fail similarly to servers today?
-applications will rely on replication and erasure coding to recover from errors that are not recoverable with ECC 
-investigate bit errors as the primary failure model; won't model impact of memory controller and circuit failures. most likely, that will follow dram circuits. leave this as future work.

Hardware-level protection techniques, such as parity and chipkill-correct, employ redundancy to guard against bit corruption due to memory cell and/or chip failures. 

However, memory nodes can still fail due to memory controller 


Memory nodes can fail either due to 

why replication for performance? To avoid a single memory node becoming a performance bottleneck
why replication for availability? To tolerate failures that render the complete memory node unavailable and which cannot be addressed by bit and chip protection techniques. This includes memory controller failures, power delivery/supply?, NIC failures? 

Explore co-design of memory protection techniques for such applications
}