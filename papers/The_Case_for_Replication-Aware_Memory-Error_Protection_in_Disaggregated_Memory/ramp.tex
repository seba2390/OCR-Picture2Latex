\section{Replication-Aware Memory-error Protection}
\label{sec:shepherd}

We propose a flexible software-defined protection architecture called \emph{\textbf{R}eplication-\textbf{A}ware \textbf{M}emory-error \textbf{P}rotection} (\ramp)
to efficiently tolerate memory errors due to random bit cell errors in disaggregated memory.
We focus on random bit cell errors, as we expect these to represent the majority of memory errors because of the high RBER of high-density NVM.

\begin{figure}[!t]
\centering
\includegraphics[width=3in]{fig/ramp}
\caption{\ramp architecture. }
\label{fig:ramp-architecture}
\vspace{-0.25cm}
\end{figure}

% replication => gives choices to repair
% checksum => improves detection strength/ability
% so we can weaken the hardware protection or vice versa.

\ramp enables co-designing application-level replication together with hardware-level memory protection to improve storage efficiency. 
\ignore{Maintaining multiple replicas across memory nodes enables the power of many choices. Instead of trying hard to eliminate uncorrectable memory errors within a single memory node using strong but expensive codes, \ramp accepts the possibility of uncorrectable errors.}
Applications that maintain multiple replicas across memory nodes can employ weaker but lower-storage-overhead ECC within individual replicas.
While weaker ECC increases failure rate of individual replicas,  
applications can rely on the multiple choices offered by available replicas in other memory nodes to correct a memory error. 
\ignore{Similarly, applications can rely on checksums to detect silent corruptions that may evade the weaker ECC.}
For example, Figure~\ref{fig:ramp-architecture} shows three applications A, B, and C with different degrees of replication and levels of protection.
Application A maintains three replicas per data item so it uses weak ECC, relying on the collective protection of multiple replicas to tolerate the increased per-replica error rate.
\revisionhighlight{Application C uses no replication so it deploys strong ECC as it relies exclusively on ECC to tolerate memory errors.}
%\fixme{Cite \cite{tai:flash-uber:atc:2019}}

\subsection{Architecture}

Figure~\ref{fig:ramp-architecture} shows \ramp's system architecture. 
Compute nodes communicate with memory nodes through a control plane and a data plane.
Each memory node exposes its local NVM by attaching its memory controller (MC) to the control and data plane via a network interface card (not shown).
An application running on a compute node uses the control plane to dynamically configure hardware-level memory error protection at the memory nodes to provide a target UBER and SDC rate.
\revisionhighlight{
The application configures memory protection method (\eg, SEC-DEC, chipkill) and code strength at the granularity of individual pages.
\ramp supports page-granularity protection by augmenting the virtual memory page table and TLB to include protection information for each page. 
\ignore{
similarly to Virtualized ECC~\cite{yoon:virtualized-ecc:asplos:2010}.
}
\ignore{
This allows software to tune hardware ECC protection strength, trading off storage overhead, hardware UBER and SDC rate. 
Configuration capability can range from letting software select a memory protection scheme from a fixed set of protection schemes (\eg, SEC-DEC, chipkill) to providing complete flexibility in choosing the protection method and level~\cite{yoon:virtualized-ecc:asplos:2010}.
}
After configuration, the application uses the data plane to access data stored in the memory nodes using fast one-sided remote DMA (RDMA) reads and writes.
The memory controller processing the RDMA request uses the page protection information to identify which protection technique to use for any given memory access.
}



\ignore{
    Memory nodes expose any uncorrectable memory errors to compute nodes.
    Compute nodes tolerate such errors using software-level replication.
}

%\ramp provides fine-grained failure reporting and handling. 
Memory nodes report DUEs to compute nodes for further handling. 
\ignore{
In contrast to previous work~\cite{shan:legoos:osdi:2018} where a memory node becomes unavailable upon a DUE, memory nodes expose DUEs to compute nodes for further handling. 
}
\ignore{
In contrast to previous work where uncorrectable memory errors crash a memory node, thus making all replicas stored on that node unavailable, our fine-grained failure model enables the replication protocol to continue using replicas stored in other memory regions unaffected by the memory error.
}
\ignore{
    The memory controller uses hardware ECC to detect and correct memory errors.
    The controller transparently corrects correctable errors, and silently returns invalid data for undetectable errors.
    For detectable but uncorrectable errors, the controller raises a hardware exception in response to uncorrectable memory errors. 
}
After reporting, a memory node remains operational and continues to serve memory accesses to non-failed memory regions, thus improving availability.
Memory nodes leverage existing hardware error reporting mechanisms, such as Intel Machine Check Architecture (MCA), to report DUEs. 
\ignore{
    For error reporting mechanisms, that do not provide a mechanism for detecting store failures, like Intel Machine Check Architecture (MCA), the NIC issues an additional read after a write to check success of the write.
}
\ignore{
    A lightweight service processor on the memory node handles the exception and returns an error as a response to the RDMA request by piggybacking on the existing error reporting mechanism of RDMA.
}
\ignore{
    To serve control plane operations and support fine-grain error reporting, the memory nodes include a lightweight service processor.
}

\subsection{Tolerating memory errors through replication}

\ramp targets applications that already employ replication for performance and availability, enabling them to leverage replication to correct DUE errors efficiently.
\ramp leaves the implementation of the replication method to the application for flexibility, dictating only some minimal requirements.
\ignore{
    Because targeted applications already employ replication for performance and availability (\cref{sec:ramp:idea}), we do not expect this requirement to significantly burden applications. 
}

For each replicated data item, an application maintains multiple replicas across memory nodes.
Applications map each replica to a memory node and memory region, and configure the hardware protection strength of each replica to meet a target UBER and SDC rate.
Applications may track and blacklist failed memory regions to avoid mapping replicas to regions with known errors.
When an application trying to access a data item faces a DUE, it attempts to correct the memory error using another replica. 
\ignore{
On read failure, it redirects the request to another replica.
On write failure, if the memory node remains operational, then it may attempt to write to another memory region within the same memory node. Otherwise it the memory node fails completely, software issues the writes to another memory node, and also remaps/migrates (asynchronously) all other replicas of the failed memory node. 
}

\revisionhighlight{
Applications can implement any block-level static homogeneous replication method, including primary-backup replication, chain replication, quorum-based replication, and MDS erasure coding. Static requires a fixed number of replicas whose protection strength does not change dynamically, thus relieving \ramp from having to support frequent protection changes. Homogeneous requires all replicas to have the same protection strength, thus simplifying replica strength reasoning.
}

\ignore{LegoOS~\cite{shan:legoos:osdi:2018}, a recent disaggregated OS, replicates memory contents across multiple memory nodes. When hardware raises a machine check exception (MCE) due to an uncorrectable memory error, the OS, instead of crashing immediately, first tries to recover from the error by serving the corresponding memory request using another memory replica.}
\ignore{
operation:
-compute nodes access memory using RDMA; rdma offers robust failure semantics compared to ld/st; piggyback on existing error reporting mechanism
-when an RDMA causes a memory side error, memory nodes do not crash, report operation failure, compute nodes recover by trying another replica, memory nodes remain functional
-memory nodes do not crash; instead poison affected region and continue servicing other requests (rely on an extended from of intel machine check architecture); current mce raises exception on read errors only. report both reads and non-posted-writes. we expect replication protocols to use non-posted writes to ensure writes reach nvm (cite snia)
}

\ignore{
Application software running on compute nodes may tolerate memory errors using application-level redundancy in the form of replication and checksumming.
\ramp does not dictate or implement a specific redundancy scheme, leaving the implementation to the application for maximum flexibility. 
Because targeted applications already employ redundancy for performance and availability (\cref{sec:ramp:idea}), we do not expect this requirement to significantly burden applications. 

Applications may use replication to tolerate DUEs. 
For each replicated data item, an application maintains multiple replicas across memory nodes.
Applications map each replica to a memory node and memory region, and configure the hardware protection strength of each replica to meet a target UBER and SDC rate.
Applications may also track and blacklist failed memory regions to avoid mapping replicas to regions with known errors.
When an application trying to access a data item faces a DUE, it attempts to correct the memory error using another replica. 

Applications may use checksumming to tolerate NDEs, including non-detectable bit cell errors and scribbles, that would otherwise silently corrupt data.
With checksumming, an application maintains a checksum for each data item.
When the application writes a data item, it computes and stores a corresponding checksum. When the application later reads the data, it may recompute the checksum and verify that the computed checksum matches the stored checksum.
}
\ignore{Application-level checksumming increases CPU utilization, but provides end-to-end protection against silent data corruption.}

\ignore{
Software running on a compute node accesses disaggregated memory using one-sided remote DMA (RDMA) reads and writes. 
When the network interface card (NIC) at a memory node receives an RDMA request, it performs a local DMA request to the node's memory controller, which in turn issues memory accesses to memory media.
The controller uses hardware ECC to detect and correct memory errors, and leverages existing hardware error reporting mechanisms, such as Intel Machine Check Architecture (MCA), to report DUEs. 
The controller transparently corrects correctable errors, and silently returns invalid data for undetectable errors.
For DUEs, the controller raises a hardware exception in response to uncorrectable memory errors. 
A lightweight service processor on the memory node handles the exception and returns an error as a response to the RDMA request by piggybacking on the existing error reporting mechanism of RDMA. 
After reporting the error, the memory node continues normal operation by servicing other pending RDMA requests. 
For error reporting mechanisms, that do not provide a mechanism for detecting store failures, like Intel Machine Check Architecture (MCA), the NIC issues an additional read after a write to check success of the write. 
}

\ignore{
memory failure path:
- apply local error detection and correction
- expose detected but uncorrected errors to software: complement reliable connection with additional error message to indicate ECC failure
- Related work: 
  - https://tools.ietf.org/id/draft-talpey-rdma-commit-01.html
  - Persistent memory over fabrics
  - Enabling Efficient RDMA-based Synchronous Mirroring of Persistent Memory Transactions
}
