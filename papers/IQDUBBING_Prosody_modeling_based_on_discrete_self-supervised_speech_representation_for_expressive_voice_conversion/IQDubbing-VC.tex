% Template for ICASSP-2021 paper; to be used with:
%          spconf.sty  - ICASSP/ICIP LaTeX style file, and
%          IEEEbib.bst - IEEE bibliography style file.
% --------------------------------------------------------------------------
\documentclass{article}
\usepackage{spconf,amsmath,graphicx}
\usepackage{color}
\usepackage{hyperref}
\usepackage{makecell}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{multirow}

% Example definitions.
% --------------------
\def\x{{\mathbf x}}
\def\L{{\cal L}}

% Title.
% ------
\title{IQDUBBING: Prosody modeling based on discrete self-supervised speech representation for expressive voice conversion}
%
% Single address.
% ---------------
%\name{Wendong Gan\textsuperscript{1}, Bolong Wen\textsuperscript{1}, Ying Yan\textsuperscript{1}, Haitao Chen\textsuperscript{1}, Zhichao Wang\textsuperscript{2}, \\\emph{Hongqiang Du\textsuperscript{2}}, \emph{Lei Xie\textsuperscript{2*}}\thanks{*Corresponding author}, \emph{Kaixuan Guo\textsuperscript{1}}, \emph{Hai Li\textsuperscript{1}}}

%\name{\normalsize {Wendong Gan\textsuperscript{1}, Bolong Wen\textsuperscript{1}, Ying Yan\textsuperscript{1}, Haitao Chen\textsuperscript{1}, Zhichao Wang\textsuperscript{2}, \emph{Hongqiang Du\textsuperscript{2}}, \emph{Lei Xie\textsuperscript{2*}}\thanks{*Corresponding author}, \emph{Kaixuan Guo\textsuperscript{1}}, \emph{Hai Li\textsuperscript{1}}}}
\name{\begin{tabular}{c}Wendong Gan$^1$, Bolong Wen$^1$,Ying Yan$^1$, Haitao Chen$^1$, Zhichao Wang$^2$ \\
Hongqiang Du$^2$, Lei Xie$^{2*}$\thanks{*Corresponding author}, Kaixuan Guo$^1$, Hai Li$^1$\end{tabular}}

%\name{\begin{tabular}{c}Fan Yu$^{1,4}$, Shiliang Zhang$^{1}$,Yihui Fu$^4$, Lei Xie$^4$, Siqi Zheng$^1$, Zhihao Du$^1$, \\
%Weilong Huang$^1$, Pengcheng Guo$^4$, Zhijie Yan$^1$, Bin Ma$^2$, Xin Xu$^3$, Hui Bu$^3$\end{tabular}}

%\name{\small {Wendong Gan\textsuperscript{1},Bolong Wen\textsuperscript{1},Ying Yan\textsuperscript{1},Haitao Chen\textsuperscript{1},Zhichao Wang\textsuperscript{2},\emph{Hongqiang Du\textsuperscript{2}},\emph{Lei Xie\textsuperscript{2*}}\thanks{*Corresponding author},\emph{Kaixuan Guo\textsuperscript{1}},\emph{Hai Li\textsuperscript{1}}}}

%\name{Wendong Gan$^1$, Bolong Wen$^1$, Ying Yan$^1$, Haitao Chen$^1$, Zhichao Wang$^2$, Hongqiang Du$^2$, Lei Xie$^2$, Kaixuan Guo$^1$, Hai Li$^1$}
%\name{Zhichao Wang$^1$, Qicong Xie$^1$, Tao Li$^1$, Hongqiang Du$^1$, Lei Xie$^1$, Pengcheng Zhu$^2$, Mengxiao Bi$^2$}

%\address{\textsuperscript{1}IQIYI Inc, China\\
%\textsuperscript{2}Audio, Speech and Language Processing Group (ASLP@NPU), School of Computer Science,\\
%Northwestern Polytechnical University, Xi’an, China}
\address{
  $^1$IQIYI Inc, Chengdu, China\\
  $^2$Audio, Speech and Language Processing Group (ASLP@NPU)\\School of Computer Science,
  Northwestern Polytechnical University, Xi’an, China\\
  }

%
% For example:
% ------------
%\address{School\\
%	Department\\
%	Address}
%
% Two addresses (uncomment and modify for two-address case).
% ---------------------------------------------------------- 

%\twoauthors
%  {A. Author-one, B. Author-two\sthanks{Thanks to XYZ agency for funding.}}
%	{School A-B\\
%	Department A-B\\
%	Address A-B}
%  {C. Author-three, D. Author-four\sthanks{The fourth author performed the work
%	while at ...}}
%	{School C-D\\
%	Department C-D\\
%	Address C-D}
%
\begin{document}
\ninept
%\small
%
\maketitle
%
\begin{abstract}
Prosody modeling is important, but still challenging in expressive voice conversion. As prosody is difficult to model, and other factors, e.g., speaker, environment and content, which are entangled with prosody in speech, should be removed in prosody modeling. In this paper, we present \textit{IQDubbing} to solve this problem for expressive voice conversion. To model prosody, we leverage the recent advances in discrete self-supervised speech representation (DSSR). Specifically, prosody vector is first extracted from pre-trained VQ-Wav2Vec model, where rich prosody information is embedded while most speaker and environment information are removed effectively by quantization. To further filter out the redundant information except prosody, such as content and partial speaker information, we propose two kinds of prosody filters to sample prosody from the prosody vector. Experiments show that \textit{IQDubbing} is superior to baseline and comparison systems in terms of speech quality while maintaining prosody consistency and speaker similarity.
\end{abstract}
%
\begin{keywords}
Expressive voice conversion, Prosody modeling, Discrete representation, Self-supervised, Prosody filter 
\end{keywords}
%
\section{Introduction}
\label{sec:intro}

Voice conversion (VC)~\cite{ezzine2017comparative} aims to modify a speech signal
uttered by a source speaker to sound as if it is uttered by a
target speaker while retaining the linguistic information. Various approaches have been proposed~\cite{sisman2020overview} for voice conversion. As parallel data~\cite{ezzine2017comparative} is expensive to collect, non-parallel methods~\cite{hsu2016voice,kaneko2018cyclegan,sun2016phonetic} have received significant attention. Among them, phonetic posteriorgram (PPG)~\cite{sun2016phonetic} based method is one of the most popular implementations. PPG is extracted from automatic speech recognition (ASR), which is regarded as speaker independent and can be used to represent linguistic content.  Inspired by PPG, the encoder output vector of an end-to-end ASR model~\cite{liu2021fastsvc} and the  bottleneck features (BN)~\cite{lian2021towards}, have been applied to voice conversion. Recently, discrete representation extracted from self-supervised model VQ-Wav2Vec (VQW2V)~\cite{baevski2019vq} is used to build a voice conversion system~\cite{huang2021any}.


Despite recent progress, modeling prosody from expressive speech~\cite{du2021expressive} for style transfer with voice conversion framework is still a challenging task. Besides linguistic information, transferring the source prosody to the target is vital for many voice conversion tasks, including automatic \textit{dubbing}\footnote{This is how our system IQDubbing named -- IQ is from IQIYI Inc, and dubbing is our target VC application.} for movies in which conversations are emotional in nature. Modeling prosody is not a trivial task; furthermore, it is necessary to remove speaker and content related information from prosody representation. Prosody information can be simply extracted from Mel spectrum (Mel)~\cite{skerry2018towards}. However, the extracted prosody contains speaker information and environment noise~\cite{wang2018style}, especially when the prosody is extracted from character conversations in movies, where speech is inevitably mixed with background speech like music and noise. To solve this problem, adversarial neural network is introduced~\cite{li2021ppg}, while it is hard to optimize. Another way to extract prosody is to use BN as input~\cite{wang2021enriching}, which achieves good results in terms of speech quality. However, the major information embedded in BN is linguistic content information, whose contribution to prosody is limited, resulting in converted speech lacking of expressiveness.

Recent studies~\cite{huang2021any, 2019Unsupervised} confirm that discrete self-supervised speech representation (DSSR) is effective to remove  speaker and environment information, but the speech quality of the converted speech is still affected. In this paper, we leverage the advances of discrete self-supervised speech representation for prosody modeling and aim to further disentangle the prosody information from the discrete representation. In the proposed system, named \textit{IQDubbing}, we first use a \textit{content encoder} to extract the linguistic information from ASR bottleneck features. Second, discrete representation which contains rich prosody information is extracted from a pre-trained VQ-Wav2Vec model. As a result, most of speaker and environment information is removed effectively by quantization. We then design a \textit{prosody encoder} to extract prosody vector from discrete representation. However, it is found that the prosody vector still contains redundant information expect prosody, such as content and partial speaker information. Thus we propose two kinds of \textit{prosody filter} to further sample prosody from the prosody vector. Third, we use a individual speaker encoder to represent the speaker identity. Finally, the decoder takes content, prosody and speaker vectors as input and the output is mel spectrum for target waveform generation. Experiment results show that \textit{IQDubbing} outperforms  baseline and comparison systems in terms of speech quality while maintaining good prosody consistency and speaker similarity.

%Voice conversion (VC) aims to convert speech of source speaker to that of target speaker (TA) while retaining the content \cite{ezzine2017comparative}. For this task, various voice conversion approaches have been proposed \cite{sisman2020overview}. In early studies, many methods are based on feature alignment \cite{kobayashi2018sprocket}. However, these methods are limited to parallel data, which is difficult to obtain. In order to solve this problem, non-parallel data methods have been explored \cite{desai2010spectral}. Among them, the method based on the phonetic posteriorgram feature of automatic speech recognition (ASR) acoustic model is currently widely used \cite{sun2016phonetic}. This framework includes two stages: recognition and synthesis. Firstly, content information of the source speech is extracted from ASR. Then, the content information is used to synthesize the speech of TA. Recently, the end-to-end ASR encoder output vector (EV) \cite{gulati2020conformer} has also gradually attracted more attention \cite{liu2021fastsvc}. 

%The mainstream voice conversion model can achieve good results for neutral speech, but it still faces many challenges on expressive speech \cite{du2021expressive} such as dubbing. The major problem is how to achieve good speech quality, preserve prosody and maintain speaker similarity at the same time. In order to model prosody for expressive speech, melspectrum (Mel) is utilized to get prosody information from source speech \cite{skerry2018towards}. However, this method introduces much speaker information and environment noise \cite{wang2018style}. To solve this problem, bottleneck dimension   is reduced \cite{lian2021towards} or adversarial network is added \cite{li2021ppg}. But speech quality of the converted speech is greatly related to the source speech. In order to improve the speech quality, bottleneck features (BN) of pre-trained ASR model is used to extract prosody \cite{wang2021enriching}. It verifies that BN contains much prosody information. However, the content and prosody information are extracted from the same BN. Recently, self-supervised model has received more and more attention in processing of speech information \cite{baevski2020wav2vec}. VQ-Wav2Vec (VQW2V) \cite{baevski2019vq} is used to build a voice conversion system \cite{huang2021any}. It verifies that quantization can remove major information of speaker and environment \cite{wu2020one}. But discrete self-supervised model in prosody modeling has not been studied. 


%In this paper, we propose \textit{IQDubbing}, leveraging the recent advances in discrete self-supervised speech representation (DSSR) for prosody modeling in expressive voice conversion. It is designed to solve the challenge, which is to achieve well speech quality while maintaining prosody consistency and speaker similarity. Firstly, we use EV to extract content, and try to extract more prosody information from self-supervised speech representation. Quantization is utilized to get DSSR. Specifically, VQW2V is selected as DSSR. While rich prosody is extracted, the speaker  and environment information are removed effectively. However, it is found that prosody vector from DSSR contains redundant information expect prosody. And then, we propose phone level prosody sieve (PLPS). This method is based on the phone-speech alignment sequence pair to guide the extraction of phone level prosody information. It is better than random downsample prosody sieve (RDPS)\cite{dai2021information}. RDPS leads to much loss of prosody and is sensitive to noise. Experiment results show that \textit{IQDubbing} is superior to baseline and comparison systems in terms of speech quality while maintaining prosody consistency and speaker similarity.
\begin{figure*}[htb]

\begin{minipage}[b]{1.0\linewidth}
  \centering
  \centerline{\includegraphics[width=17cm]{fig1}}
%  \vspace{2.0cm}
\end{minipage}
%
\caption{Overview of the components of the proposed voice conversion model. First, prosody modeling is based on DSSR. DSSR is discrete self-supervised speech representation. Besides, two kinds of prosody filters: random downsample prosody filter(RDPF) and aligned downsample prosody filter(ADPF), are compared.}
\label{fig:res}
%
\end{figure*}

The rest of the paper is organized as follows. Section 2 gives an overview on the \textit{IQDubbing} system. The prosody modeling based on DSSR is detailed in Section 3. Section 4 presents the experiments and the comparison of our proposed approach against baseline and comparison systems. Section 5 concludes this paper.

\section{System Overview}
\label{sec:format}

The architecture of IQDubbding is shown in Fig.~1. In general the system follows an encoder-decoder framework, where three individual encoders are adopted, in charge of content extraction, prosody extraction and speaker representation respectively. Specifically for the content extraction, an end-to-end ASR model is first adopted to take source audio as input and its encoder output, or the bottleneck feature (BN), is fed into the content encoder, resulting in a content vector representing the linguistic information. As for the prosody modeling, a pre-trained VQ-Wav2Vec model is adopted to process source audio and output the discrete representation -- VQW2V. The VQW2V is then fed into the prosody encoder, resulting in the prosody vector. To further filter out prosody-unrelated information from the prosody vector, we specifically design a prosody filter to get the filtered prosody vector. The decoder takes content vector, filtered prosody vector and speaker vector as input to reconstruct mel spectrum. Finally, Parallel WaveGAN~\cite{yamamoto2020parallel} is used to synthesize the converted speech.  



%It consists of VQ-Wav2Vec model, E2E ASR encoder model, VQW2V prosody encoder, content encoder, prosody sieve module, and decoder. VQ-Wav2Vec and ASR are pretrained models, which are used to extract VQW2V and BN respectively. The frame-level prosody vector is extracted by VQW2V prosody encoder. Prosody vector is then sieved by prosody sieve module. The content vector is extracted from BN by content encoder. The decoder takes content vector, sieved prosody vector and speaker ID as input to reconstruct melspectrum. Finally, Parallel WaveGAN~\cite{yamamoto2020parallel} is used to  synthesized the converted speech. 

%We use ${\rm E}_{c}(.)$ to denote content encoder,  ${\rm E}_{p}(.)$ to denote VQW2V prosody encoder, ${\rm S}_{p}(.)$ to denote prosody sieve module and ${\rm D}(.)$ to denote decoder. ${V}$, ${B}$, ${I}$ are VQW2V, BN, and speaker ID respectively. Besides, ${P}_{v}$ , ${P}_{sv}$, ${C}_{v}$ are prosody vector, sieved prosody vector and content vector respectively. The system function can be described as:

%\begin{equation}
%C_{v} = {\rm E}_{c}(B)
%\end{equation}
%\begin{equation}
% P_{v} = {\rm E}_{p}(V)
%\end{equation}
%\begin{equation}
% P_{sv} = {\rm S}_{p}(P_{v})
%\end{equation}
%\begin{equation} 
%Mel_{recon} = {\rm Dec}(P_{sv}, C_{v}, I)
%\end{equation}
%where $Mel_{recon}$ is the reconstructed melspectrum. Voice conversion model is optimized with reconstruction loss. %The loss function can be described as:

%\begin{equation} 
%L_{recon} = \left\|Mel_{recon}-Mel_{target}\right\|_2^2
%\end{equation}
%where $Mel_{target}$ denotes the target melspectrum, and $L_{recon}$ is the reconstruction loss.
% Content encoder refers to the encoder of Tacotron2 \cite{shen2018natural}. VQW2V prosody encoder will be introduced in detail in section 2.1. Prosody sieve module will be introduced in detail in section 2.2. Decoder is based on the decoder of Tacotron2 \cite{shen2018natural} and an autoregressive model without attention is used.



% \vspace{-15pt}
\section{Prosody Modeling based on DSSR}

Our aim is to transfer not only the linguistic content, but also the style of the source speech to the target speaker. It was previously recognized as a challenging task as linguistic content, speaker identity as well as prosody are entangled in the speech signal. In this paper, we specifically propose a prosody module to output a `pure' prosody vector,  without the linguistic and speaker information, fed into the decoder to result in the converted speech with target speaker's timbre and source speaker's style.

\subsection{VQW2V-based Prosody Encoder}
\label{ssec:subhead}



% In the early studies, Mel is used as the input feature of prosody encoder \cite{skerry2018towards}. But Mel contains much speaker and environment information, which affects speech quality and similarity. Inspired by the VQWav2Vec-VC \cite{huang2021any}, DSSR contains rich prosody information, and quantization removes major speaker and environment information. So we designed a prosody encoder whose input is from a pre-trained VQWav2Vec model.

Recent study adopting VQWav2Vec in VC has shown that the quantization works as a disentanglement operation which helps to remove speaker and environment information, while maintaining rich prosody information from the source speech~\cite{huang2021any, 2019Unsupervised}. Hence based the VQW2V vector outputted by the VQWav2Vec model, we use a prosody encoder to further generate a prosody vector.


\begin{figure}[htb]

\begin{minipage}[b]{1.0\linewidth}
  \centering
  \centerline{\includegraphics[width=8.5cm]{fig2}}
%  \vspace{2.0cm}
\end{minipage}

%
\caption{VQW2V based prosody encoder. The VQW2V indices are from pre-trained VQ-Wav2Vec model~\cite{2019Unsupervised}.}
\label{fig:res}
%
\end{figure}


% As is shown in Fig.2, firstly we take VQW2V indices as the prosody encoder input. In the pre-trained VQWav2Vec model, the codebook size is configured as 320, the group size is configured as 2. Secondly, we separate the indices of two groups and used them to look up two 128-dim embedding tables. The two queried embedding vectors are concatenated. Finally, the generated frame-wise 256-dim vector is fed into a max-pooling layer, followed by six 2-D convolution layers and a bidirectional GRU network, which refers to \cite{li2021ppg}. The dimension of VQW2V prosody encoder output is set as 4. To our knowledge, this is the first paper in which VQW2V is used for prosody modeling. The purpose of this design is to improve speech quality and speaker similarity. 

As shown in Fig.2, first the VQW2V prosody encoder takes the indices of VQW2V as input. The codebook size of VQWav2Vec model is set as 320. Second, we divide the indices into two groups along the time and dimension axis. Two groups are used to query from two 128-dim embedding tables to obtain embedding vector. The two queried embedding vectors are concatenated to form frame-wise 256-dim vector. Finally, the  256-dim vector is fed into a max-pooling layer, followed by six 2-D convolution layers and a bidirectional GRU network, which refers to~\cite{li2021ppg}. The  output dimension of VQW2V prosody encoder is set as 4. To our knowledge, this is the first paper to study VQW2V for prosody modeling. We believe that VQW2V is able to improve speech quality and speaker similarity by removing speaker and environment information. 

\subsection{Prosody Filter}
\label{ssec:subhead}

% To remove content information and improve speaker similarity, as is shown in Fig.1, we compare 3 kinds of prosody sieve: P1, P2 and \textit{IQDubbing}. In P1, prosody vector is passed through. P2 is random downsample prosody sieve (RDPS) \cite{wu2020one} which is shown in Fig.3 (a). \textit{IQDubbing} is phone level prosody sieve (PLPS) which is shown in Fig.3 (b). The phone-speech alignment sequence pair is used to align the prosody vector. 


As shown in Fig.1,  we design a module, named as \textit{prosody filter}, to further remove content and speaker information contained in prosody vector. Specifically, we compare two kinds of prosody filter: \textit{random downsample prosody filter} (RDPF) and \textit{aligned downsample prosody filter} (ADPF), which filter out the redundant information by downsamping prosody vector along time axis. 


\textbf{Random downsample prosody filter (RDPF)}. Inspired by~\cite{dai2021information},  we design RDPF, which is shown in Fig.3 (a). First, frame level prosody vectors are equally distributed to different phones. A group of vectors corresponds to a phone. Then, we randomly select one prosody vector with a fixed rate $\tau$ from each group as the downsampled vector. Specifically, the downsampled vector is picked out at timestamps $\{\tau-1, 2\tau-1, 3\tau-1, ...\}$. Finally, the downsampled vector is upsampled to form filtered prosody vector, the length of which is the same as prosody vector.


\textbf{Aligned downsample prosody filter (ADPF)}. The second prosody filter ADPF is shown in Fig.~3~(b). First, we use third-party pre-trained model MontrealCorpusTools~\footnote{https://github.com/MontrealCorpusTools/Montreal-Forced-Aligner} to force-align  prosody vectors to the phones to get a time-aligned frame level sequence. Force alignment gives accurate time-aligned sequence.  Then, the prosody vectors are fed into GRU to get corresponding hidden state in frame level. The lengths of hidden states and prosody vectors are same. Therefore, the aligned sequence still suits for the hidden states. Then the last hidden state of each phone is regarded as the downsampled vector. Finally, similar to RDPF, downsampled vector is upsampled to restore the original length of the prosody vector. 


Both RDPF and ADPF adopt downsampling technique to remove content and speaker information. Prosody vector is extracted from discrete self-supervised speech representation (DSSR). DSSR is effective to remove  speaker and environment information~\cite{huang2021any, 2019Unsupervised}. Dowmsampling technique compresses prosody vector, which means that a part of the information is lost during the compressing process. As the content and speaker have been represented  by speaker id and content vector, the remaining filtered prosody vector should provide the decoder with sufficient prosody information that is necessary for perfect reconstruction~\cite{qian2019autovc}. Moreover, content changes dramatically among frames in one utterance~\cite{chou2019one}, and each frame level prosody vector may contain the corresponding content. However, the downsampled vector is selected from the corresponding prosody vectors of each phone. Therefore, content information can be further removed.

Comparing RDPF with ADPF, downsampling in RDPF is random, which may lead to corruption of prosody. For example, in Fig.~3, the $4^{th}$ frame belongs to the third phone ``AA2", but the upsampled vector after RDPF is from the second phone. Furthermore, the downsampled vector is just one selected prosody vector, which may be unstable at run-time. For example, the selected prosody vector happens to be noisy. In contrast, ADPF overcomes the above two shortcomings and  achieves more accurate frame level prosody information while removing the redundant information.


% \subsection{Prosody Sieve Module}
% \label{ssec:subhead}

% % To remove content information and improve speaker similarity, as is shown in Fig.1, we compare 3 kinds of prosody sieve: P1, P2 and \textit{IQDubbing}. In P1, prosody vector is passed through. P2 is random downsample prosody sieve (RDPS) \cite{wu2020one} which is shown in Fig.3 (a). \textit{IQDubbing} is phone level prosody sieve (PLPS) which is shown in Fig.3 (b). The phone-speech alignment sequence pair is used to align the prosody vector. 

% As shown in Fig.1, to remove content and speaker information, we compare 3 kinds of prosody sieve: P1, P2 and \textit{IQDubbing}. For P1, prosody vector is used as the sieved prosody vector. After limiting the output dimension of prosody encoder, P2 and P3 are both designed to sieve prosody vector along time axis.

% For P2, as is shown in Fig.3 (a), random downsample prosody sieve (RDPS)~\cite{dai2021information} is used to sieve prosody vector. Prosody vector is downsampled by a fixed rate $\tau$ along time axis. Specifically, the grouped vector is picked out at timestamps like$\{\tau-1, 2\tau-1, 3\tau-1, ...\}$ to generate the downsampled vector. Then, the downsampled vector is upsampled to generate sieved prosody vector, the length of which is the same with prosody vector. 

% For \textit{IQDubbing},  as is shown in Fig.3 (b),phone level prosody sieve (PLPS) is adopted to sieve prosody vector . We use third-part pretrained model\footnote{https://github.com/MontrealCorpusTools/Montreal-Forced-Aligner} to get the phone alignment sequence. Then, GRU is used to calculate the prosody information of various length. Each phone gets a frame of downsampled vector from the outputs of GRU. Finally, the downsampled vector of each phone is phone-length wise upsampled and restored to the original length of the prosody vector.

% Compared with P2 and \textit{IQDubbing}, prosody sieve in P2 is random, which may lead to corruption of prosody. For example, the 4th frame belongs to the third phone, but its upsampled vector after sieve module is from second phone. Furthermore, the downsampled vector is just from one frame of the prosody vector. It may be unstable and be sensitive to random interference. For example, the selected frame happens to be noise. In contrast, \textit{IQDubbing} overcomes the above two shortcomings and  achieves more accurate phone level prosody information while removing the redundant information along time axis.

\begin{figure*}[htb]
\begin{minipage}[b]{1.0\linewidth}
  \centering
  \centerline{\includegraphics[width=17cm]{fig3_a}}
%  \vspace{2.0cm}
  \centerline{(a) Random downsample prosody filter (RDPF). Prosody Vector is grouped by a fixed rate $\tau$. }\medskip
\end{minipage}
%
\begin{minipage}[b]{1.0\linewidth}
  \centering
  \centerline{\includegraphics[width=17cm]{fig3_b}}
%  \vspace{1.5cm}
  \centerline{(b) Align downsample prosody filter (PLPF). Prosody Vector is aligned by a phone alignment sequence.}\medskip
\end{minipage}

%
\caption{Prosody filter. Here Mandarin syllable ``hua2" is chosen as an example, which includes 3 phones, ``HH", ``UW2" and ``AA2". The $1^{st}$ and $2^{nd}$ frame belong to ``HH". The $3^{rd}$ frame belongs to ``AA2". The $4^{th}$, $5^{th}$ and $6^{th}$ frame belong to "AA2". Please focus on the filtered prosody vector of each phone.} 
\label{fig:res}
%
\end{figure*}



% We use third-part pretrained model to get alignment pair. Then, GRU is used to calculate the prosody information of various length. Each phone gets a downsampled vector from the outputs of GRU. Finally, the downsampled vector of each phone is phone-length wise upsampled and restored to the original length of the prosody vector.

% As is shown in Fig.3 (a), prosody sieve in P2 is random, which may lead to corruption of prosody. For example, the 4th frame is from third phone, but its upsampled vector after sieve module is from second phone. Furthermore, the downsampled vector is just from one frame. It may be unstable and be sensitive to random interference. For example, the selected frame happens to be noise. In contrast, \textit{IQDubbing} overcomes these two shortcomings. While achieving the purpose of sieving information, more suitable prosody is obtained.

\section{EXPERIMENTS}
\label{sec:pagestyle}

% \subsection{Dataset and Experimental Setup}
% \label{ssec:subhead}

% ESD dataset \cite{zhou2021emotional} is used to train the voice conversion model. This dataset contains 35000 utterances spoken by 10 native Mandarin speakers and 10 English speakers with 5 emotional states (neutral, happy, angry, sad and surprise). For conversion test, 260 utterances are used for evaluation. It contains 10 male speakers and 10 female speakers. The style includes ordinary reading and dubbing speech. We resample the training set to 24kHz. We use 80-dim Mel as reconstruct target, which computed in 50ms window length and 10ms frame shift.

% We trained an end-to-end ASR to get EV with 4000 hours mandarin corpus consisting of open source dataset\footnote{http://www.openslr.org/68/,  http://www.openslr.org/62/} and dataset from tv series. 1024-dim EV is used as the content information representation. VQW2V is from third-part pretrained model \cite{baevski2019vq}, which is trained by the 960h Librispeech dataset \cite{panayotov2015librispeech}. Parallel WaveGAN is used to reconstruct Mel into   waveform, which is trained on the ESD dataset.

% In order to verify the performance of our proposed approach, we implement the following system:
% \vspace{-5pt}
% \begin{itemize}
% \setlength{\itemsep}{0pt}
%     \item \textbf{BL}: baseline system, which refers to Tacotron2, composed of an encoder and an auto-regressive decoder. Only EV is the input of the system.
% 	\item \textbf{CS}: comparison system, which is similar to \cite{lian2021towards}. It is based on \textbf{BL}, and Mel is used as prosody feature.
% 	\item \textbf{P1}: proposed system 1, in which Mel is replaced with VQW2V in \textbf{CS}. 
% 	\item \textbf{P2}: proposed system 2, in which random downsample prosody sieve (RDPS) with fixed rate 32 is added to prosody vector in \textbf{P1}.
% 	\item \textbf{\textit{IQDubbing}}: finally proposed system, in which phone level prosody sieve (RDPS) is added to prosody vector in \textbf{P1}.
% \end{itemize}
% \vspace{-5pt}

% We use Adam optimizer to optimize our model with learning rate decay, which starts from 0.001 and decays every 10 epochs in decay rate 0.7. In the training stage, the model is trained for 140 epochs and batch size is 32.

% \subsection{Prosody Consistency Evaluation}
% \label{ssec:subhead}

% AB test is conducted to verify prosody consistency. We randomly select 15 utterances from 260 utterances in each system and provide them to 12 listeners for testing. The samples can been found here\footnote{https://wblgers.github.io/IQDUBBING-VC.github.io/}. Listeners are asked to choose better utterance on prosody consistency from paired samples. The higher the score, the better prosody consistency.
% \vspace{-0.3cm}
% \begin{figure}[htb]

% \begin{minipage}[b]{1.0\linewidth}
%   \centering
%   \centerline{\includegraphics[width=8.5cm]{fig4}}
% %  \vspace{2.0cm}
% \end{minipage}
% \vspace{-0.6cm}
% %
% \caption{Prosody consistency AB test results(A: Fair: B).}
% \label{fig:res}
% %
% \end{figure}

% The results are shown in Fig.4. P1 is significantly better than BL. It verifies that the VQW2V prosody encoder has effectively improved the prosody consistency. Besides, \textit{IQDubbing} is better than P2. \textit{IQDubbing}, CS and P1 are close. It is shown that \textit{IQDubbing} can achieve the purpose of maintaining well prosody consistency. The effect of \textit{IQDubbing} in speaker similarity and speech quality will be verified respectively in following section 3.3 and 3.5.

% \subsection{Speaker Similarity Evaluation}
% \label{ssec:subhead}
% In order to verify the performance of each system in terms of speaker similarity, we introduce False Acceptance Rate of Target (FAR) as the indicator of similarity evaluation \cite{das2020predictions}. We use a third-party pre-trained speaker encoder\footnote{https://github.com/resemble-ai/Resemblyzer}  of Speaker Verification (SV) system \cite{wan2018generalized} to extract speaker embedding from each utterance. We select 140 speakers from Aishell-3 and ESD datasets, each with 10 utterances, as the registered utterances. Utterances of each speaker is used to calculate the average embeddings. Utterances synthesized pass the SV system as long as the cosine similarity exceeds a threshold. The threshold is obtained by computing the equal error rate over the whole registered utterances. The equal error rate is 5.41\%. The higher the FAR score, the better the similarity.
% \vspace{-10pt}
% \begin{table}[!htbp]
% \centering
% \caption{Speaker similarity evaluation.}
% \begin{tabular}{c|c|c|c|c|c}
% \hline
% \textbf{}&\textbf{BL}&\textbf{CS}&\textbf{P1}&\textbf{P2}&\textbf{\textit{IQDubbing}}\\
% \hline
% \textbf{FAR}&\textbf{0.833}&0.617&\textbf{0.818}&0.837&\textbf{0.830}\\
% \hline
% \end{tabular}
% \end{table}

% The FAR scores of different systems are shown in Table 1. The scores of CS and P1 are lower than BL, which shows that prosody module reduces the performance of speaker similarity. However, the score of P1 is higher than CS, which shows that the VQW2V has a better effect on speaker similarity. The scores of P2 and \textit{IQDubbing} are both higher than P1 and close to BL. The above results show that the P2 and \textit{IQDubbing} we proposed have more obvious effect in improving the speaker similarity.

% \subsection{Necessity Analysis of Prosody Sieve}
% \label{ssec:subhead}

% We conduct following test to verify the necessity and contribution of prosody sieve module. The details are as follows:
% \begin{itemize}
% \setlength{\itemsep}{0pt}

% \vspace{-0.3cm}
% \item \textbf{Only EV}: value of VQW2V is set to 0 in the  corresponding system. 
% \item  \textbf{Only VQW2V}: value of EV is set to 0 in the corresponding system.
% \item \textbf{EV+VQW2V}: EV and VQW2V are used normally in the corresponding system.
% \end{itemize}
% \vspace{-0.1cm}

% The pictures in Fig.5 are the spectrograms of the utterances generated by P1, P2 or \textit{IQDubbing}. They are used to show the content or prosody information of speech.  As is shown in Fig.5, the speech of \textbf{Only EV} is worse than \textbf{Only VQW2V} in P1. It shows that too much content information is leaked from VQW2V in P1. However, the content information should be obtained from EV. It verifies the necessity of prosody sieve module. It is also found that \textbf{Only EV} is very close to \textbf{EV+VQW2V} in P2. The contribution of VQW2V is limited. Furthermore, we find that both EV and VQW2V play an important role in \textit{IQDubbing}. Obviously, the effect of prosody sieve module is verified in \textit{IQDubbing}, which is consistent with the evalution results in section3.2 and 3.3.
% \vspace{-5pt}
% \begin{figure}[htb]

% \begin{minipage}[b]{1.0\linewidth}
%   \centering
%   \centerline{\includegraphics[width=8.5cm]{fig5}}
% %  \vspace{2.0cm}
% \end{minipage}

% %
% \caption{Necessity and contribution analysis of prosody sieve.}
% \label{fig:res}
% %
% \end{figure}
% \vspace{2pt}
% \subsection{speech Quality Evaluation}
% \label{ssec:subhead}
% After verifying the performance of \textit{IQDubbing} on maintaining prosody consistency and speaker similarity, we conduct speech quality evaluation to measure conversion performance. The 15 sentences from each system in sections 3.2 are provided to 12 listeners for testing. 
% \vspace{-10pt}
% \begin{table}[!htbp]
% \centering
% \caption{MOS results of speech quality.}
% \begin{tabular}{c|c|c}
% \hline
% \textbf{ID}&\textbf{Model}&\textbf{speech Quality}\\
% \hline
% \textbf{TA}&Target speaker speech&4.217\\
% \hline
% \textbf{BL}& Baseline& 3.733\\
% \hline
% \textbf{CS}& Comparison system& 3.267\\
% \hline
% \textbf{P1}& EV + VQW2V& 3.750\\
% \hline
% \textbf{P2}& EV + VQW2V + RDPS& 3.417\\
% \hline
% \textbf{\textit{IQDubbing}}& EV + VQW2V + PLPS& \textbf{3.867}\\
% \hline
% \end{tabular}
% \end{table}


% In the test, the listeners are asked to evaluate the speech quality of the converted speech, and the evaluation is carried out on a 5-point scale. In addition, the speech from the target TA is also used for evaluation. The evaluation results are shown in Table 2. We can find that the score of P1 is higher than BL, and the score of P2 is lower than BL and P1. The score of \textit{IQDubbing} is highest, which indicates that it achieves best results of speech quality.

% \vspace{-5pt}

\subsection{Dataset and Experimental Setup}
\label{ssec:subhead}

We conduct voice conversion experiments on ESD dataset~\cite{zhou2021emotional}. The ESD dataset covers 5 emotion categories (neutral, happy, angry, sad and surprise), and consists of 350 parallel utterances spoken by 10 native Chinese and 10 native English speakers. For conversion test, 260 utterances are used for evaluation, which contain 10 male speakers and 10 female speakers. The style includes ordinary reading and dubbing speech. All audio files are downsampled to 24kHz. 80 dim Mel is extracted with 50ms frame length and 10ms frame shift.

We trained an end-to-end ASR with 4000 hours mandarin corpus to get 1024-dim BN. The corpus consists of open source dataset\footnote{http://www.openslr.org/68/,  http://www.openslr.org/62/} and internal dataset from TV series which covers different background noises. VQW2V is extracted from third-part pre-trained model~\cite{baevski2019vq}, which is trained with the 960h Librispeech dataset~\cite{panayotov2015librispeech}.  Parallel WaveGAN~\cite{yamamoto2020parallel} is used to reconstruct waveform, which is trained on the ESD dataset.


In order to verify the performance of our proposed approach, we implement the following systems:
\begin{itemize}
\setlength{\itemsep}{0pt}
    \item \textbf{BL}: Baseline system~\cite{sun2016phonetic} is similar to IQDubbing, but is without prosody modeling. Only BN is used as the input of the system.
	\item \textbf{CS}: We implement a comparison system proposed in~\cite{lian2021towards}, which is similar to IQDubbing, but prosody feature is Mel and prosody modeling is without prosody filter. Besides, prosody encoder only includes a max-pooling layer, six 2-D convolution layers and a bidirectional GRU network.
	\item \textbf{IQDubbing-Base}: the proposed system shown in Fig.1,  in which the prosody encoder is VQW2V-based Prosody Encoder introduced in section3.1. Besides, IQDubbing-Base is without prosody filter.  
	\item \textbf{IQDubbing-RDPF}: IQDubbing-Base adopts random downsample prosody filter (RDPF) introduced in Section 3.2. Fixed rate $\tau$ is set as 32, which follows~\cite{dai2021information}. 
	\item \textbf{IQDubbing-ADPF}: IQDubbing-Base adopts aligned downsample prosody filter (ADPF) introduced in Section 3.2.
\end{itemize}

% \begin{itemize}
% \setlength{\itemsep}{0pt}
%     \item \textbf{BL}: Baseline system is similar to IQDubbing but is without prosody modeling. Tacotron2 is used as the baseline system. which is composed of an encoder and an auto-regressive decoder without attention. Only BN is used as the input of the system.
% 	\item \textbf{CS}: This is the comparison system, which is similar to~\cite{lian2021towards}. Comparison system is based on BL, and Mel is used as prosody feature.
% 	\item \textbf{IQDubbing}: Mel is replaced with VQW2V in CS. 
% 	\item \textbf{IQDubbing-RDPF}: Random downsample prosody filter (RDPF) is added to prosody vector in IQDubbing.
% 	\item \textbf{IQDubbing-ADPF}: Aligned downsample prosody filter (ADPF) is added to prosody vector in IQDubbing.
% \end{itemize}


For IQDubbing, we utilize the encoder of Tacotron2~\cite{shen2018natural} as the content encoder and  follows the original configurations. An auto-regressive decoder~\cite{wang2021enriching} is used as our decoder. All the voice conversion models are optimized with Adam optimizer with learning rate decay, which starts from 0.001 and decays every 10 epochs with decay rate 0.7. During the training stage, the models are trained for 140 epochs and batch size is 32.

% In detail, we use the encoder part of Tacotron2~\cite{shen2018natural} as the content encoder. An autoregressive model without attention mechanism is used as decoder~\cite{wang2021enriching}. All the voice conversion models are optimized with Adam optimizer with learning rate decay, which starts from 0.001 and decays every 10 epochs with decay rate 0.7. During the training stage, the models are trained for 140 epochs and batch size is 32.

\subsection{Prosody Consistency Evaluation}
\label{ssec:subhead}

AB test is conducted to verify prosody consistency. All the converted speech are used for listening test. 12  listeners participate in listening test. Listeners are asked to choose the utterance  that is close to source speech in terms of  prosody consistency from paired samples. Higher score indicates that the result is better. The samples can be found here\footnote{https://wblgers.github.io/IQDUBBING-VC.github.io/}.

The results are shown in Fig.4. IQDubbing-Base is significantly better than BL, which verifies that the VQW2V-based prosody encoder has effectively improved the prosody consistency. Besides, IQDubbing-ADPF is better than IQDubbing-Base, IQDubbing-RDPF and CS respectively. The results confirms that IQDubbing-ADPF successfully models the prosody and make the prosody consistent with that of source speech. 

\begin{figure}[!h]
%\begin{minipage}[b]{1.0\linewidth}
  \centering
  \centerline{\includegraphics[width=8.5cm]{fig4}}
%  \vspace{2.0cm}
%\end{minipage}
\label{fig:res}
\caption{AB test results of prosody consistency.}
\end{figure}

%The effect of \textit{IQDubbing} in speaker similarity and speech quality will be verified respectively in following section 3.3 and 3.5.

\subsection{Speaker Similarity Evaluation}
\label{ssec:subhead}
In order to verify the performance of each system in terms of speaker similarity, we introduce false acceptance rate of target (FAR) as the indicator of speaker similarity evaluation~\cite{das2020predictions}. The third-party pre-trained speaker verification (SV) system~\footnote{https://github.com/resemble-ai/Resemblyzer} to extract speaker embedding from each utterance~\cite{wan2018generalized}.  We select 140 speakers from Aishell-3~\cite{shi2020aishell} and ESD datasets for evaluation. Ten utterances from each speaker are utilized as the registered utterances to calculate the average speaker embedding.  Higher FAR score indicates that the results of speaker similarity is better.


% \begin{table}[!htbp]
% \centering
% \caption{FAR results of speaker similarity evaluation.}
% \begin{tabular}{c|c|c|c|c|c}
% \hline
% \textbf{}&BL&CS&IQDubbing&\makecell[c]{IQDubbing-\\RDPF}&\makecell[c]{IQDubbing-\\ADPF}\\
% \hline
% FAR&0.833&0.617&0.818&0.837&\textbf{0.830}\\
% \hline
% \end{tabular}
% \end{table}

\begin{table}[!htbp]
\centering
\caption{FAR results of speaker similarity evaluation.}\vspace{5pt}
\begin{tabular}{l|c}
\toprule
System&FAR\\
\midrule
BL&0.833\\
\hline
CS&0.617\\
\hline
IQDubbing-Base&0.818\\
\hline
IQDubbing-RDPF&0.837\\
\hline
IQDubbing-ADPF&\textbf{0.830}\\
\bottomrule
\end{tabular}
\end{table}

The FAR scores of different systems are shown in Table 1. The scores of CS and IQDubbing-Base are lower than BL, which shows that prosody module reduces the performance of speaker similarity as the extracted prosody contains redundant information, such as speaker related information of source speaker. However, the score of IQDubbing-Base is obviously higher than CS, which shows that the VQW2V improves  speaker similarity by quantization. The scores of IQDubbing-RDPF and IQDubbing-ADPF are  higher than IQDubbing-Base and close to BL. The above results show that the prosody filter is effective to improve the speaker similarity.

% \subsection{Ablation Analysis of Prosody Filter}
% \label{ssec:subhead}

% IQDubbing, IQDubbing-RDPF, and IQDubbing-ADPF are compared under the following conditions to verify the contribution of prosody filter. The details of conditions are shown as follows:
% \begin{itemize}
% \setlength{\itemsep}{0pt}

% \item \textbf{Only BN}: The inputs of both VQW2V prosody encoder and content encoder are BN. 
% \item  \textbf{Only VQW2V}: The inputs of both VQW2V prosody encoder and content encoder are VQW2V.
% \item \textbf{BN+VQW2V}: VQW2V prosody encoder takes VQW2V as input, and content encoder takes VQW2V as input.
% \end{itemize}

% Fig.5 shows the spectrograms of converted speech by three proposed systems under different conditions.

% The spectrograms shown in Fig.5 are from the utterances generated by P1, P2 or \textit{IQDubbing} respectively. They are used to show the content or prosody information of speech.  As is shown in Fig.5, the spectrogram of Only BN is worse than Only VQW2V with P1. It shows that content information is leaked from VQW2V in P1. However, the content information should be obtained from BN. It verifies the necessity of prosody filter. It is also found that Only BN is very close to BN+VQW2V in P2, which illustrates the contribution of VQW2V is limited. Furthermore, we find that both BN and VQW2V play an important role in \textit{IQDubbing}. Obviously, the effect of prosody filter module is verified in \textit{IQDubbing}, which is consistent with the evalution results in Section 3.2 and 3.3.

% \begin{figure}[htb]

% \begin{minipage}[b]{1.0\linewidth}
%   \centering
%   \centerline{\includegraphics[width=8.5cm]{fig5}}
% %  \vspace{2.0cm}
% \end{minipage}

% %
% \caption{Ablation analysis of prosody filter. The spectrograms are used to show the content or prosody information.}
% \label{fig:res}
%
% \end{figure}
\subsection{Speech Quality Evaluation}
\label{ssec:subhead}
We conduct  mean opinion score (MOS) tests to evaluate speech quality.  The speech from the target speaker is also used for this evaluation.  Each listener
is asked to give opinion score on a five-point scale (5: excellent, 4: good, 3: fair, 2: poor, 1: bad). 
% \begin{table}[!htbp]
% \centering
% \caption{MOS results of speech quality.}
% \begin{tabular}{c|c|c|c|c|c|c}
% \hline
% \textbf{}&GT&BL&CS&IQDubbing&\makecell[c]{IQDubbing\\-RDPF}&\makecell[c]{IQDubbing\\-ADPF}\\
% \hline
% \makecell[c]{Speech \\Quality}&4.217&3.733&3.267&3.750&3.417&\textbf{3.867}\\
% \hline
% \end{tabular}
% \end{table}

\begin{table}[!htbp]
\centering
\caption{MOS results of speech quality.}
\vspace{5pt}
\begin{tabular}{l|c}
\toprule
ID&Speech Quality\\
\midrule
Ground Truth& 4.217\\
\hline
BL& 3.733\\
\hline
CS& 3.267\\
\hline
IQDubbing-Base& 3.750\\
\hline
IQDubbing-RDPF& 3.417\\
\hline
IQDubbing-ADPF& \textbf{3.867}\\
\bottomrule
\end{tabular}
\end{table}

% \begin{table}[!htbp]
% \centering
% \caption{MOS results of speech quality.}
% \begin{tabular}{c|l|c}
% \hline
% ID&Model&speech Quality\\
% \hline
% TA&Target speaker speech&4.217\\
% \hline
% BL& Baseline& 3.733\\
% \hline
% CS& Comparison system& 3.267\\
% \hline
% IQDubbing& BN + VQW2V & 3.750\\
% \hline
% IQDubbing-RDPF& BN + VQW2V + RDPS& 3.417\\
% \hline
% IQDubbing-ADPF& BN + VQW2V + PLPS& \textbf{3.867}\\
% \hline
% \end{tabular}
% \end{table}


%In the test, the listeners are asked to evaluate the speech quality of the converted speech, and the evaluation is carried out on a 5-point scale. In addition, the speech from the target TA is also used for evaluation. 

The results of MOS tests are shown in Table 2. We find that the score of IQDubbing-Base is higher than BL and CS. However, the score of IQDubbing-RDPF is lower than IQDubbing-Base as RDPF is not stable at runtime. IQDubbing-ADPF achieves the  highest MOS score, which indicates that ADPF helps to improve speech quality by successfully removing content and speaker information in prosody.

\section{CONCLUSION}
\label{sec:typestyle}

% In this study, we propose \textit{IQDubbing}, leveraging the recent advances in DSSR for prosody modeling in expressive voice conversion. It is designed to solve the challenge, which is to achieve well speech quality while preserving prosody consistency and speaker similarity. For the first time, DSSR is adopted to model prosody, which solves the shortcomings of Mel. Furthermore, to solve information leakage of DSSR, PLPS is proposed to sieve the prosody information in a more accurate way.  It is better than RDPS, which causes much loss of prosody and is affected by random interference. Experiments show that for expressive speech, \textit{IQDubbing}, is superior to baseline and comparison systems in terms of speech quality while maintaining prosody consistency and speaker similarity.

Transferring prosody from source to target is vital for expressive voice conversion. However, this is a challenging task as prosody is entangled with other factors including content, speaker and environment information. To solve this problem, we propose \textit{IQDubbing} which models speaker, content and prosody as individual encoders and more importantly, a prosody modeling framework based on discrete self-supervised representation (DSSR) is introduced, aiming to extract rich prosody and remove prosody-unrelated information. Firstly, by using VQ-Wav2Vec for prosody modeling, rich prosody is maintained, while speaker and environment information are removed effectively. And then, since prosody vector is still entangled with redundant information expect prosody, such as content and partial speaker information, we particularly propose two kinds of \textit{prosody filter} to further sample prosody from the prosody vector.  Experiments show that \textit{IQDubbing} is superior to baseline and comparison systems in terms of speech quality and maintains prosody consistency and speaker similarity. Moreover, aligned downsample prosody filter (ADPF) is more effective than random downsample prosody filter (RDPF).

\vfill\pagebreak



% References should be produced using the bibtex program from suitable
% BiBTeX files (here: strings, refs, manuals). The IEEEbib.bst bibliography
% style file from IEEE produces unsorted bibliography list.
% -------------------------------------------------------------------------
\bibliographystyle{IEEEbib}
\small\bibliography{strings,refs}

\end{document}
