
In this section, we demonstrate and analyse the performance of the proposed architectures on the courier task.
We first show the performance of our agents in large city environments, next their generalisation capabilities on a held-out set of goals. Finally, we investigate whether the proposed approach allows transfer of an agent trained on a set of regions to a new and previously unseen region.

\begin{figure}[th]
% \vskip 0.2in
\begin{center}
\begin{subfigure}{.33\textwidth}
  \centering
  \includegraphics[width=1.0\linewidth]{figures/learning_nyu.png}
  \caption{NYU (New York City)}
  \label{fig:main_results_nyu}
\end{subfigure}%
\begin{subfigure}{.33\textwidth}
  \centering
  \includegraphics[width=1.0\linewidth]{figures/learning_london.png}
  \caption{Central London}
  \label{fig:main_results_london}
\end{subfigure}%
\begin{subfigure}{.33\textwidth}
  \centering
  \includegraphics[width=1.0\linewidth]{figures/reward_shaping.png}
  \caption{Effect of reward shaping}
  \label{fig:reward_shaping}
\end{subfigure}%
\caption{
Average per-episode rewards (y axis) are plotted vs. learning steps (x axis) for the courier task. We compare the \emph{GoalNav} agent, the \emph{CityNav} agent, and the \emph{CityNav} agent without skip connection on the NYU environment \textbf{(a)}, and the \emph{CityNav} agent in London \textbf{(b)}. We also give \emph{Oracle} performance and a \emph{Heuristic} agent. A curriculum is used in London---we indicate the end of phase 1 (up to 500m) and the end of phase 2 (5000m). %Results on Paris (trained as London) are comparable and the agent achieved mean goal reward 426.
\textbf{(c)} Results of the \emph{CityNav} agent on NYU, comparing radii of early rewards (ER) vs. ER with random coins vs. curriculum with ER 200m and no coins.
}
\label{fig:main_results}
\end{center}
\vskip -0.25in
\end{figure}


\subsection{Courier Navigation in Large, Diverse City Environments}
\label{sec:large}


\begin{figure}[b]
% \vskip 0.2in
\begin{center}
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.99\linewidth]{figures/dist_steps.png}
  \caption{}
  \label{fig:steps}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.99\linewidth]{figures/trajectories_values.pdf}
  \caption{}
  \label{fig:value_london_nyu}
\end{subfigure}
\caption{
\textbf{(a)} Number of steps required for the \emph{CityNav} agent to reach a goal from 100 start locations vs. the straight-line distance to the goal in metres. \textbf{(b)} \emph{CityNav} performance in London (left panes) and NYU (right panes). \emph{Top}: examples of the agent's trajectory during one 1000-step episode, showing successful consecutive goal acquisitions. The arrows show the direction of travel of the agent. \emph{Bottom}: We visualise the agent's value function over 100 trajectories with random starting points and the same goal. Thicker and warmer colour lines correspond to higher value functions.
}
\end{center}
\vskip -0.25in
\end{figure}

We first show that the \emph{CityNav} agent, trained with curriculum learning, succeeds in learning the courier task in New York, London and Paris. We replicated experiments with 5 random seeds and plot the mean and standard deviation of the reward statistic throughout the experimental results. Throughout the paper, and for ease of comparison with experiments that include reward shaping, we report only the rewards at the goal destination (\emph{goal rewards}). Figure~\ref{fig:main_results} compares different agents and shows that the CityNav architecture with the dual LSTM pathways and the heading prediction task attains a higher performance and is more stable than the simpler GoalNav agent. We also trained a CityNav agent without the skip connection from the vision layers to the policy LSTM. While this hurts the performance in single-city training, we consider it because of the multi-city transfer scenario (see Section~\ref{sec:transfer}) where funeling all visual information through the locale-specific LSTM seems to regularise the interface between the goal LSTM and the policy LSTM.
We also consider two baselines which give lower (\emph{Heuristic}) and upper (\emph{Oracle}) bounds on the performance.
\emph{Heuristic} is a random walk on the street graph, where the agent turns in a random direction if it cannot move forward; if at an intersection it will turn with a probability $p=0.95$.
\emph{Oracle} uses the full graph to compute the optimal path using breath-first search.

We visualise trajectories from the trained agent over two 1000 step episodes (Fig.~\ref{fig:value_london_nyu} (top row)). In London, we see that the agent crosses a bridge to get to the first goal, then travels to goal 2, and the episode ends before it can reach the third goal. Figure~\ref{fig:value_london_nyu} (bottom row) shows the value function of the agent as it repeatedly navigates to a chosen destination (respectively, St Paul's Cathedral in London and Washington Square in New York). 

To understand whether the agent has learned a policy over the full extent of the environment, we plot the number of steps required by the agent to get to the goal. As the number grows linearly with the straight-line distance to that goal, this result suggests that the agent has successfully learnt the navigation policy on both cities (Fig.~\ref{fig:steps}). 




\subsection{Impact of Reward Shaping and Curriculum Learning}
\label{sec:shaping}
To better understand the environment, we present further experiments on reward, curriculum. Additional analysis, including architecture ablations, the robustness of the agent to the choice of goal representations, and position and goal decoding, are presented in the Supplementary Material.

Our navigation task assigns a goal to the agent; once the agent successfully navigates to the goal, a new goal is given to the agent. The long distance separating the agent from the goal makes this a difficult RL problem with sparse rewards. To simplify this challenging task, we investigate giving early rewards (\emph{reward shaping}) to the agent before it reaches the goal (we define goals with a 100m radius), or to add random rewards (\emph{coins}) to encourage exploration \cite{beattie2016deepmind,mirowski2016learning}. Figure \ref{fig:reward_shaping} suggests that \emph{coins} by themselves are ineffective as our task does not benefit from wide explorations. At the same time, large radii of reward shaping help as they greatly simplify the problem. We prefer curriculum learning to reward shaping on large areas because the former approach keeps agent training consistent with its experience at test time and also reduces the risk of learning degenerate strategies such as ascending the gradient of increasing rewards to reach the goal, rather than learn to read the goal specification $g_t$.

As a trade-off between task realism and feasibility, and guided by the results in Fig. \ref{fig:reward_shaping}, we decide to keep a small amount of reward shaping (200m away from the goal) combined with curriculum learning. The specific reward function we use is: $r_t = \max ( 0, \min ( 1, (d_{ER} - d^g_t) / 100 ) ) \times r^g$, where $d^g_t$ is the distance from the current position of the agent to the goal, $d_{ER}=200$ and $r^g$ is the reward that the agent will receive if it reaches the goal. Early rewards are given only once per panorama / node, and only if the distance $d^g_t$ to the goal is decreasing (in order to avoid the agent developing a behavior of harvesting early rewards around the goal rather than going directly towards the goal).

We choose a curriculum that starts by sampling the goal within a radius of 500m from the agent's location, and progressively grows that disc until it reaches the maximum distance an agent could travel within the environment (e.g., 3.5km, and 5km in the NYU and London environments respectively) by the end of the training. Note that this does not preclude the agent from going astray in the opposite direction several kilometres away from the goal, and that the goal may occasionally be sampled close to the agent. Hence, our curriculum scheme naturally combines easy with difficult cases \cite{zaremba2014learning}, with the latter becoming more common over the period of time.


\subsection{Generalization on Held-out Goals}
\label{sec:heldout}


\begin{figure}[thb]
\begin{floatrow}
\ffigbox{%
  \includegraphics[width=.6\linewidth]{figures/nyu_heldout.png}
}{%
  \caption{Illustration of medium-sized held-out grid with gray corresponding to training destinations, black corresponding to held-out test destinations. Landmark locations are marked in red.
}%
\label{fig:heldout}
}
\capbtabbox{%
\begin{small}
\begin{sc}
\begin{tabular}{r|c|ccc}
\toprule
Grid & Train & \multicolumn{3}{c}{Test}  \\
Size & Rew & Rew & Fail & $T_{\frac{1}{2}}$ \\
% Dataset & Grid & Train & Test &  &  \\
%  & Size & Rew. & Rew. & Fail & $T_{\frac{1}{2}}$ \\
\midrule
fine & 655 & 567 & 11\% & 229 \\
medium  & 637 & 293 & 20\% & 184 \\
coarse   & 623 & 164 & 38\% & 243 \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
}{%
\caption{\emph{CityNav} agent generalization performance (reward and fail metrics) on a set of held-out goal locations.
%shows that the agent performs worse as the held-out area increases. 
We also compute the \emph{half-trip time} ($T_{\frac{1}{2}}$), to reach halfway to the goal.
%) to understand the lower performance. 
}
\label{tab:heldout}
}
\end{floatrow}
\end{figure}



Navigation agents should, ideally, be able to generalise to unseen environments~\cite{dhiman2018critical}. While the nature of our courier task precludes zero-shot navigation in a new city without retraining, we test the \emph{CityNav} agent's ability to exploit local linearities of the goal representation to handle unseen goal locations. We mask $25\%$ of the possible goals and train on the remaining ones (Fig.~\ref{fig:heldout}). At test time we evaluate the agent only on its ability to reach goals in the held-out areas. Note that the agent is still able to traverse \emph{through} these areas, it just never samples a goal there.
More precisely, the held-out areas are squares
sized $0.01^{\degree}$, $0.005^{\degree}$ or $0.0025^{\degree}$ of latitude and longitude (roughly 1km$\times$1km, 0.5km$\times$0.5km and 0.25km$\times$0.25km). We call these grids respectively \emph{coarse} (with few and large held-out areas), \emph{medium} and \emph{fine} (with many small held-out areas).

In the experiments, we train the \emph{CityNav} agent for 1B steps, and next freeze the weights of the agent and evaluate its performance on held-out areas for 100M steps. Table \ref{tab:heldout} shows decreasing performance of the agents as the held-out area size increases. 
We believe that the performance drops on the large held-out areas (medium and coarse grid size) because the model cannot process new or unseen local landmark-based goal specifications, which is due to our landmark-based goal representation: as Figure~\ref{fig:heldout} shows, some coarse grid held-out areas cover multiple landmarks.
To gain further understanding, in addition to the \emph{Test Reward} metric, we also use missed goals (\emph{Fail}) and half-trip time ($T_{\frac{1}{2}}$) metrics. The missed goals metric measures the percentage of times goals were not reached. The half-trip time measures the number of agent steps necessary to cover half the distance separating the agent from the goal. While the agent misses more goal destinations on larger held-out grids, it still manages to travel half the distance to the goal within a similar time, which suggests that the agent has an approximate held-out goal representation that enables it to head towards it until it gets close to the goal and the representation is no longer useful for the final approach.

\subsection{Transfer in Multi-city Experiments}
\label{sec:transfer}
A critical test for our proposed method is to demonstrate that it can provide a mechanism for transfer to new cities. By definition, the courier task requires a degree of memorization of the map, and what we focused on was not zero-shot transfer, but rather the capability of models to generalize quickly, learning to separate general ability from local knowledge when migrating to a new map. Our motivation for transfer learning experiments comes from the goal of continual learning, which is about learning new skills without forgetting older skills.
As with humans, when our agent visits a new city we would expect it to have to learn a new set of landmarks, but not have to re-learn its visual representation, its behaviours, etc. Specifically, we expect the agent to take advantage of existing visual features (convnet) and movement primitives (policy LSTM). Therefore, using the \emph{MultiCityNav} agent, we train on a number of cities (actually regions in New York City), freeze both the policy LSTM and the convolutional encoder, and then train a new locale-specific pathway (the goal LSTM) on a new city. The gradient that is computed by optimising the RL loss is passed through the policy LSTM without affecting it and then applied only to the new pathway. 

We compare the performance using three different training regimes, illustrated in Fig.~\ref{fig:transfer_diagram}: Training on only the target city (\emph{single training}); training on multiple cities, including the target city, together (\emph{joint training}); and joint training on all but the target city, followed by training on the target city with the rest of the architecture frozen (\emph{pre-train and transfer}).
In these experiments, we use the whole Manhattan environment as shown in Figure \ref{fig:nyc}, and consisting of the following regions ``Wall Street'', ``NYU'', ``Midtown'', ``Central Park'' and ``Harlem''. The target city is always the Wall Street environment, and we evaluate the effects of pre-training on 2, 3 or 4 of the other environments. We also compare performance if the skip connection between the convolutional encoder and the policy LSTM is removed.

 We can see from the results in Figure \ref{fig:transfer} that not only is transfer possible, but that its effectiveness increases with the number of the regions the network is trained on. Remarkably, the agent that is pre-trained on 4 regions and then transferred to Wall Street achieves comparable performance to an agent trained jointly on all the regions, and only slightly worse than single-city training on Wall Street alone\footnote{We observed that we could train a model jointly on 4 cities in fewer steps than when training 4 single-city models.}. This result supports our intuition that training on a larger set of environments results in successful transfer. We also note that in the single-city scenario it is better to train an agent with a skip-connection, but this trend is reversed in the multi-city transfer scenario. We hypothesise that isolating the locale-specific LSTM as a bottleneck is more challenging but reduces overfitting of the convolutional features and enforces a more general interface to the policy LSTM. While the transfer learning performance of the agent is lower than the stronger agent trained jointly on all the areas, the agent significantly outperforms the baselines and demonstrates goal-dependent navigation. 

\begin{figure}[t]
% \vskip 0.2in
\begin{center}
\begin{subfigure}{.55\textwidth}
  \centering
  \includegraphics[width=.99\linewidth]{figures/streetlearn_transfer_schematic.pdf}
  \caption{Diagram of transfer learning experiments.}
  \label{fig:transfer_diagram}
\end{subfigure}%
\begin{subfigure}{.45\textwidth}
  \centering
  \includegraphics[width=.99\linewidth]{figures/transfer.png}
  \caption{Transfer learning performance.}
  \label{fig:transfer}
\end{subfigure}

\caption{
Left: Illustration of training regimes: (a) training on a single city (equivalent to CityNav); (b) joint training over multiple cities with a dedicated per-city pathway and shared convolutional net and policy LSTM; (c) joint pre-training on a number of cities followed by training on a target city with convolutional net and policy LSTM frozen (only the target city pathway is optimised).
Right: Joint multi-city training and transfer learning performance of variants of the \emph{MultiCityNav} agent, evaluated only on the target city (Wall Street).
}
\label{fig:transfer_experiments}
\end{center}
\vskip -0.25in
\end{figure}
