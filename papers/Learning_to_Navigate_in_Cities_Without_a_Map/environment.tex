
This section presents an interactive environment, named \emph{StreetLearn}, constructed using Google Street View, which provides a public API\footnote{\url{https://developers.google.com/maps/documentation/streetview/}}. 
Street View provides a set of geolocated $360^{\degree}$ panoramic images which form the nodes of an undirected graph. We selected a number of large regions in New York City, Paris and London that contain between 7,000 and 65,500 nodes (and between 7,200 and 128,600 edges, respectively), have a mean node spacing of 10m, and cover a range of up to 5km (see Fig.~\ref{fig:nyc}).
We do not simplify the underlying connectivity, thus there are congested areas with complex occluded intersections, tunnels and footpaths, and other ephemera. Although the graph is used to construct the environment, the agent only sees the raw RGB images (see Fig.~\ref{fig:streetview}).


\subsection{Agent Interface and the Courier Task}
\label{sec:agent}


\begin{figure}[t]
%\vskip -0.1in
\begin{center}
\begin{subfigure}{.45\textwidth}
  \centering
  \includegraphics[width=.9\linewidth]{figures/goal_description.pdf}
  \caption{Goal description using landmarks.}
  \label{fig:goals}
\end{subfigure}%
\begin{subfigure}{.55\textwidth}
  \centering
  \includegraphics[width=.99\linewidth]{figures/streetlearn_architectures_v2_small_labels.pdf}
  \caption{Comparison of architectures.}
  \label{fig:architecture}
\end{subfigure}
\caption{
\textbf{(a)} In the illustration of the goal description, we show a set of 5 nearby landmarks and 4 distant ones; the code $g_i$ is a vector with a softmax-normalised distance to each landmark.
\textbf{(b)} \emph{Left:} GoalNav is a convolutional encoder plus policy LSTM  with goal description input. \emph{Middle:} CityNav is a single-city navigation architecture with a separate goal LSTM and optional auxiliary heading ($\theta$). \emph{Right:} MultiCityNav is a multi-city architecture with individual goal LSTM pathways for each city.
}
\label{fig:goals_architecture}
\end{center}
% \vskip -0.4in
\vskip -0.25in
\end{figure}

An RL environment needs to specify the start space, observations, and action space of the agent as well as the task reward. The agent has two inputs: the image  ${\bf x_t}$, which is a cropped, $60^\degree$ square, RGB image that is scaled to $84\times84$ pixels (i.e.\ not the entire panorama), and the goal description $g_t$. The action space is composed of five discrete actions: ``slow'' rotate left or right ($\pm 22.5^\degree$), ``fast'' rotate left or right ($\pm 67.5^\degree$), or move forward---this action becomes a \texttt{noop} if there is not an edge in view from the current agent pose. If there are multiple edges in the view cone of the agent, then the most central one is chosen. 

There are many options for how to specify the goal to the agent, from images to agent-relative directions, to text descriptions or addresses. We choose to represent the current goal in terms of its proximity to a set $\mathcal{L}$ of fixed landmarks: $\mathcal{L}=\{(Lat_k,Long_k)\}_k$, specified using the Lat/Long (latitude and longitude) coordinate system. To represent a goal at $(Lat^g_t,Long^g_t)$ we take a softmax over the distances to the $k$ landmarks (see Fig.~\ref{fig:goals}), thus for distances $\{d^g_{t,k}\}_k$ the goal vector contains $g_{t,i} = \exp(-\alpha d^g_{t,i}) / \sum_k \exp(-\alpha d^g_{t,k})$
for the $ith$ landmark with $\alpha =0.002$ (which we chose through cross-validation). This forms a goal description with certain desirable qualities: it is a scalable representation that extends easily to new regions, it does not rely on any arbitrary scaling of map coordinates, and it has intuitive meaning---humans and animals also navigate with respect to fixed landmarks. Note that landmarks are fixed per map and we used the same list of landmarks across all experiments; $g_t$ is computed using the distance to all landmarks, but by feeding these distances through a non-linearity, the contribution of distant landmarks is reduced to zero.
In the Supplementary material, we show that the locally-continuous landmark-based representation of the goal performs as well as the linear scalar representation $(Lat^g_t,Long^g_t)$. Since the landmark-based representation performs well while being independent of the coordinate system and thus more scalable, we use this representation as canonical.
Note that the goal description is not relative to the agent's position and only changes when a new goal is sampled. Locations of the 644 manually defined landmarks in New York, London and Paris are given in the Supplementary material, where we also show that the density of landmarks does not impact the agent performance.


In the \emph{courier} task, which we define as the problem of navigating to a series of random locations in a city, the agent starts each episode from a randomly sampled position and orientation. If the agent gets within 100m of the goal (approximately one city block), the next goal is randomly chosen and input to the agent. 
Each episode ends after 1000 agent steps.  The reward that the agent gets upon reaching a goal is proportional to the shortest path between the goal and the agent's position when the goal is first assigned; much like a delivery service, the agent receives a higher reward for longer journeys. Note that we do not reward agents for taking detours, but rather that the reward in a given level is a function of the optimal distance from start to goal location. As the goals get more distant during the training curriculum, per-episode reward statistics should ideally reach and stay at a plateau performance level if the agent can equally reach closer and further goals.
