\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2018

% ready for submission
% \usepackage{neurips_2018}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2018}

% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2018}
     \usepackage[final,nonatbib]{neurips_2018}

% to avoid loading the natbib package, add option nonatbib:
%     \usepackage[nonatbib]{neurips_2018}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{graphicx}
\usepackage{amsmath,amssymb} % define this before the line numbering.
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{color}
\usepackage{xspace}
\usepackage{mathabx}
%\usepackage{gensymb}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{floatrow}
\usepackage{soul}
\newfloatcommand{capbtabbox}{table}[][\FBwidth]

\title{Learning to Navigate in Cities Without a Map}

\author{
  Piotr Mirowski, Matthew Koichi Grimes, Mateusz Malinowski, Karl Moritz Hermann, \\
  \bf{Keith Anderson, Denis Teplyashin, Karen Simonyan, Koray Kavukcuoglu,} \\
  \bf{Andrew Zisserman, Raia Hadsell}\\
  DeepMind\\
  London, United Kingdom\\
  \texttt{\{piotrmirowski, mkg, mateuszm, kmh, keithanderson, \}@google.com} \\
  \texttt{\{teplyashin, simonyan, korayk, zisserman, raia\}@google.com} \\
}

\input{define}

\begin{document}

\maketitle

% Proposed edits
\newcommand{\ed}[1]{\textcolor{cyan}{\bf \small #1}}
% Piotr
\newcommand{\piotr}[1]{\textcolor{cyan}{\bf \small [#1 --PM]}}
% Raia
\newcommand{\rh}[1]{\textcolor{blue}{\bf \small [#1 --RH]}}
% Mateusz
\newcommand{\mati}[1]{\textcolor{green}{\bf \small [#1 --MM]}}
% Karen
\newcommand{\karen}[1]{\textcolor{black}{\bf \small [#1 --Karen]}}
% Andrew Zisserman
\newcommand{\az}[1]{\textcolor{red}{\bf \small [#1 --AZ]}}
% Karl Moritz
\definecolor{darkblue}{RGB}{0, 0, 83}
\newcommand{\kmh}[1]{\textcolor{darkblue}{\bf\small [#1 --KMH]}}


\begin{abstract}
Navigating through unstructured environments is a basic capability of intelligent creatures, and thus is of fundamental interest in the study and development of artificial intelligence. Long-range navigation is a complex cognitive task that relies on developing an internal representation of space, grounded by recognisable landmarks and robust visual processing, that can simultaneously support continuous self-localisation (``I am \emph{here}'') and a representation of the goal (``I am going \emph{there}''). Building upon recent research that applies deep reinforcement learning to maze navigation problems, we present an end-to-end deep reinforcement learning approach that can be applied on a city scale. Recognising that successful navigation relies on integration of general policies with locale-specific knowledge, we propose a dual pathway architecture that allows locale-specific features to be encapsulated, while still enabling transfer to multiple cities. A key contribution of this paper is an interactive navigation environment that uses Google Street View for its photographic content and worldwide coverage. Our baselines demonstrate that deep reinforcement learning agents can learn to navigate in multiple cities and to traverse to target destinations that may be kilometres away. The project webpage \url{http://streetlearn.cc} contains a video summarizing our research and showing the trained agent in diverse city environments and on the transfer task, the form to request the StreetLearn dataset and links to further resources. The StreetLearn environment code is available at \url{https://github.com/deepmind/streetlearn}.

\end{abstract}

\section{Introduction}
\label{sec:intro}
\input{introduction}

\section{Related Work}
\label{sec:related}
\input{related_work}

\section{Environment}
\label{sec:environment}
\input{environment}

\section{Methods}
\label{sec:methods}
\input{methods}

\section{Results}
\label{sec:experiments}
\input{experiments}

\section{Conclusion}
\label{sec:conclusion}
\input{conclusion}

% % Acknowledgements should only appear in the accepted version.
\section*{Acknowledgements}

The authors wish to acknowledge Andras Banki-Horvath for open-sourcing the StreetLearn environment, Lasse Espeholt and Hubert Soyer for technical help with the IMPALA algorithm, Razvan Pascanu, Ross Goroshin, Pushmeet Kohli and Nando de Freitas for their feedback, Chloe Hillier, Razia Ahamed and Vishal Maini for help with the project, and the Google Street View team (Tilman Reinhardt, Wenfeng Li, Ben Mears, Karen Guo, Oliver Metzger, Jayanth Nayak) as well as Richard Ives and Ashwin Kakarla for their support in accessing the data.

\bibliography{streetlearn}
\bibliographystyle{plain}

\newpage
\appendix{}

\section{Video of the Agent Trajectories and Observations}
\label{supp:video}
\input{video}

\section{Further Analysis}
\label{supp:analysis}
\input{analysis}

\section{Implementation Details}
\label{supp:implementation}
\input{implementation}

\section{Environment}
\label{supp:environment}
\input{landmarks}

\end{document}
