\subsection{Neural Network Architecture}
\label{supp:architecture}

For all the experiments in the paper we use the standard vision model for Deep RL \cite{mnih2016asynchronous} with 2 convolutional layers followed by a fully connected layer. The baseline \emph{GoalNav} architecture has a single recurrent layer (LSTM), from which we predict the policy and value function, similarly to \cite{mnih2016asynchronous}.

The convolutional layers are as follows. The first convolutional layer has a kernel of size 8x8 and a stride of 4x4, and 16 feature maps. The second layer has a kernel of size 4x4 and a stride of 2x2, and 32 feature maps. The fully connected layer has 256 units, and outputs 256-dimensional visual features $f_t$. Rectified nonlinearities (ReLU) separate the layers.

The convnet is connected to the policy LSTM (in case of two-LSTM architectures, we call it a \emph{Skip} connection). The policy LSTM has additional inputs: past reward $r_{t-1}$ and previous action ${\bf a}_{t-1}$ expressed as a one-hot vector of dimension 5 (one for each action: forward, turn left or right by $22.5^{\degree}$, turn left or right by $67.5^{\degree}$).

The goal information $g_t$ is provided as an extra input, either to the policy LSTM (\emph{GoalNav} agent) or to each \emph{goal} LSTM in the \emph{CityNav} and \emph{MultiCityNav} agents. In case of landmark-based goals, $g_t$ is a vector of 644 elements (see Section \ref{supp:environment} for the complete list of landmark locations in the New York and London environments). In the case of Lat/Long scalars, $g_t$ is a 2-dimensional vector of Lat and Long coordinates normalized to be between 0 and 1 in the environment of interest. In the case of binned Lat/Long coordinates, we bin the normalized scalar coordinates using 35 bins for Lat and 35 bins for Long in the NYU environment, each bin representing 100m, and the vector $g_t$ contains 70 elements.

The goal LSTM also takes 256-dimensional inputs from to the convnet. The goal LSTM contains 256 hidden units and is followed by a \emph{tanh} nonlinearity, a dropout layer with probability $p=0.5$, then a 64-dimensional linear layer and finally a \emph{tanh} layer. It is this (\emph{CityNav}) or these (\emph{MultiCityNav}) 64-dimensional outputs that are connected to the policy LSTM. We chose to use this bottleneck, consisting of a dropout, linear layer from 256 to 64, followed by a nonlinearity, in order to force the representations in the goal LSTM to be more robust to noise and to send only a small amount of information (possibly related to the egocentric position of the agent w.r.t. the goal) to the policy LSTM. Please note that the \emph{CityNav} agent can still be trained to solve the navigation task without that layer.

Similarly to \cite{mnih2016asynchronous}, the policy LSTM contains 256 hidden units, followed by two parallel layers: one linear layer going from 256 to 1 and outputing the value function, and one linear layer going from 256 to 5 (the number of actions), and a softmax nonlinearity, outputting the policy.

The heading $\theta_t$ prediction auxiliary task is done using an MPL with a hidden layer of 128 units, connected to the hidden units of each goal LSTM in the \emph{CityNav} and \emph{MultiCityNav} agents, and outputs a softmax of 16-dimensional vectors, corresponding to 16 binned directions towards North. The auxiliary task is optimized using a multinomial loss.

\subsection{Learning Hyperparameters}
\label{supp:hyperparameters}

The costs for all auxiliary heading prediction tasks, of the value prediction, of the entropy regularization and of the policy loss are added before being sent to the RMSProp gradient learning algorithm \cite{tieleman2012lecture} (momentum 0, discounting factor 0.99, $\epsilon=0.1$, initial learning rate 0.001). The weight of heading prediction is 1, the entropy cost is 0.004 and the value baseline weight is 0.5.

In all our experiments, we train our agent with IMPALA \cite{espeholt2018impala}, an actor-critic implementation of deep reinforcement learning that decouples acting and learning. In our experiments, IMPALA results in similar performance to A3C \cite{mnih2016asynchronous} on a single city task, but as it has been demonstrated to handle better multi-task learning than A3C, we prefer it to A3C for our multi-city and transfer learning experiments. We use 256 actors for \emph{CityNav} and 512 actors for \emph{MultiCityNav}, with batch sizes of 256 or 512 respectively, and sequences are unrolled to length 50. We used a learning rate of 0.001, linearly annealed to 0 after 2B steps (NYU), 4B steps (London) or 8B steps (multi-city and transfer learning experiments). The discounting coefficient in the Bellman equation is 0.99. Rewards are clipped at 1 for the purpose of gradient calculations.

\subsection{Curriculum Learning}
\label{supp:curriculum}

Because of the distributed nature of the learning algorithm, it was easier to implement the duration of phase 1 and phase 2 of curriculum learning using the Wall clock of the actors and learners rather than by sharing the total number of steps with the actors, which explains why phase durations are expressed in terms of days, rather than in a given number of steps. With our software implementation, hardware and batch size as well as number of actors, the distributed learning algorithm runs at about 6000 environment steps/sec, and a day of training corresponds to about 500M steps. In terms of gradient steps, given than we use unrolls of length 50 steps and batch sizes of 256 or 512, each gradient step corresponds to either $50\times256=12800$ or $50\times512=25600$ environment steps, and is taken every 2s or 4s respectively for a speed of 6000 environment steps/sec.

