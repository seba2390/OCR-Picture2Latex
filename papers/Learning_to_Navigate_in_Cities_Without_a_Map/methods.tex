\label{sec:problem}
We formalise the learning problem as a Markov Decision Process, with
state space $\mathcal{S}$, action space $\mathcal{A}$, environment $\mathcal{E}$, and a set of possible goals $\mathcal{G}$.
The reward function depends on  the current goal and state: $R: \mathcal{S} \bigtimes \mathcal{G} \bigtimes \mathcal{A} \to \mathbb{R}$. 
The usual reinforcement learning objective is to find the policy that maximises the expected return defined as the sum of discounted rewards starting from state $s_0$ with discount $\gamma$. 
In this navigation task, the expected return from a state $s_t$ also depends on the series of sampled goals $\{g_{k}\}_k$. The policy is a distribution over actions given the current state $s_t$ and the goal $g_t$: $\pi(a|s,g) = Pr(a_t=a |s_t=s, g_t=g)$. We define the value function to be the expected return for the agent that is sampling actions from policy $\pi$ from state $s_t$ with goal $g_t$:  $V^{\pi}(s,g)=E[R_t]= E[\, \sum_{k=0}^{\infty} \gamma^k r_{t+k} |s_t=s,g_t=g]$.


We hypothesise the courier task should benefit from two types of learning: general, and locale-specific. A navigating agent not only needs an internal representation that is general, to support cognitive processes such as scene understanding, but also needs to organise and remember the features and structures that are unique to a place. Therefore, to support both types of learning,
we focus on  neural architectures with multiple pathways. 

\subsection{Architectures}
\label{sec:architecture}


The policy and the value function are both parameterised by a neural network which shares all layers except the final linear outputs. The agent operates on raw pixel images ${\bf x_t}$, which are passed through a convolutional network as in \cite{mnih2016asynchronous}. A Long Short-Term Memory (LSTM) \cite{hochreiter1997long} receives the output of the convolutional encoder as well as the past reward $r_{t-1}$ and previous action $a_{t-1}$.  The three different architectures are described below. Additional architectural details are given in the Supplementary Material.

The baseline {\bf GoalNav} architecture (Fig.~\ref{fig:architecture}a) has a convolutional encoder and \emph{policy LSTM}. The key difference from the canonical A3C agent \cite{mnih2016asynchronous} is that the goal description $g_t$ is input to the policy LSTM (along with the previous action and reward).
    
The {\bf CityNav} architecture (Fig.~\ref{fig:architecture}b) combines the previous architecture with an additional LSTM, called the \emph{goal LSTM}, which receives visual features as well as the goal description.  The CityNav agent also adds an auxiliary heading ($\theta$)  prediction task on the outputs of the goal LSTM.
    
The {\bf MultiCityNav} architecture (Fig.~\ref{fig:architecture}c) extends the CityNav agent to learn in different cities. The remit of the goal LSTM is to encode and encapsulate locale-specific features and topology such that multiple pathways may be added, one per city or region. Moreover, after training on a number of cities, we demonstrate that the convolutional encoder and the policy LSTM become general enough that only a new goal LSTM needs to be trained for new cities, a benefit of the modular approach~\cite{devin2017learning}. 

Figure~\ref{fig:architecture} illustrates that the goal descriptor $g_t$ is not seen by the policy LSTM but only by the locale-specific LSTM in the {\bf CityNav} and {\bf MultiCityNav} architectures (the baseline {\bf GoalNav} agent has only one LSTM, so we directly input $g_t$). This separation forces the locale-specific LSTM to interpret the absolute goal position coordinates, with the hope that it then sends relative goal information (\emph{directions}) to the policy LSTM. This hypothesis is tested in section 2.3 of the supplementary material.

As shown in \cite{jaderberg2016reinforcement,mirowski2016learning,dosovitskiy2016learning,lample_aaai17}, auxiliary tasks can speed up learning by providing extra gradients as well as relevant information. We employ a very natural auxiliary task: the prediction of the agent's heading $\theta_t$, defined as an angle between the north direction and the agent's pose, using a multinomial classification loss on binned angles. The optional heading prediction is an intuitive way to provide additional gradients for training the convnet. The agent can learn to navigate without it, but we believe that heading prediction helps learning the geometry of the environment; the Supplementary material provides a detailed architecture ablation analysis and agent implementation details.

To train the agents, we use IMPALA \cite{espeholt2018impala}, an actor-critic implementation that decouples acting and learning. In our experiments, IMPALA results in similar performance to A3C \cite{mnih2016asynchronous}. We use 256 actors for \emph{CityNav} and 512 actors for \emph{MultiCityNav}, with batch sizes of 256 or 512 respectively, and sequences are unrolled to length 50. 


\subsection{Curriculum Learning}
\label{sec:curriculum}

Curriculum learning gradually increases the complexity of the learning task by presenting progressively more difficult examples to the learning algorithm \cite{bengio2009curriculum, graves2017automated,zaremba2014learning}. We use a curriculum to help the agent learn to find increasingly distant destinations. Similar to RL problems such as Montezuma's Revenge, the courier task suffers from very sparse rewards; unlike that game, we are able to define a natural curriculum scheme. We start by sampling each new goal to be within 500m of the agent's position (phase 1).
In phase 2, we progressively grow the maximum range of allowed destinations to cover the full graph (3.5km in the smaller New York areas, or 5km for central London or Paris).
