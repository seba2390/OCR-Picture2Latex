\subsection{Architecture Ablation Analysis}
\label{supp:ablation}

\begin{figure}[ht]
% \vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.5\columnwidth]{figures/ablations.png}}
\caption{Learning curves of the \emph{CityNav} agent (2LSTM+Skip+HD) on NYU, comparing different ablations, all they way down to \emph{GoalNav} (LSTM). 2LSTM architectures have a global pathway LSTM and a policy LSTM with optional Skip connection between the convnet and the policy LSTM. HD is the heading prediction auxiliary task.
}
\label{fig:ablations}
\end{center}
\vskip -0.2in
\end{figure}

In this analysis, we focus on agents trained with a two-day learning curriculum and early rewards at 200m, in the NYU environment. Here, we study how the learning benefits from various auxiliary tasks as well as we provide additional ablation studies where we investigate various design choices. We quantify the performance in terms of average reward per episode obtained at goal acquisition.
Each experiment was repeated 5 times with different seeds. We report average final reward and plot the mean and standard deviation of the reward statistic.
As Fig. \ref{fig:ablations} shows, the auxiliary task of heading (\emph{HD}) prediction helps in achieving better performance in the navigation task for the \emph{GoalNav} architecture, and, in conjunction with a skip connection from the convnet to the policy LSTM, for the 2-LSTM architecture. The \emph{CityNav} agent significantly outperforms our main baseline \emph{GoalNav} (LSTM in Fig. \ref{fig:ablations}), which is a goal-conditioned variant of the standard RL baseline \cite{mnih2016asynchronous}. \emph{CityNav} perfoms on par with \emph{GoalNav} with heading prediction, but the latter cannot adapt to new cities without re-training or adding city-specific components, whereas the \emph{MultiCityNav} agent with locale-specific LSTM pathways can, as demonstrated in the paper's section on transfer learning. Our weakest baseline (\emph{CityNav} no vision) performs poorly as the agent cannot exploit visual cues while performing navigation tasks.
In our investigation, we do not consider other auxiliary tasks introduced in prior works \cite{jaderberg2016reinforcement, mirowski2016learning} as they are either unsuitable for our task, do not perform well, or require extra information that we consider too strong. Specifically, we did not implement the reward prediction auxiliary task on the convnet from \cite{jaderberg2016reinforcement}, as the goal is not visible from pixels, and the motion model of the agent with widely changing visual input is not appropriate for the pixel control tasks in that same paper. From \cite{mirowski2016learning}, we kept the 2-LSTM architecture and substituted depth prediction (which we cannot perform on this dataset) by heading and neighbor traversability prediction. We did not implement loop-closure prediction as it performed poorly in the original paper and uses privileged map information.

\subsection{Goal Representation}
\label{supp:goal}

\begin{figure}[!ht]
% \vskip 0.2in
\begin{center}
\includegraphics[width=.5\linewidth]{figures/goal_representation.png}
\caption{
Learning curves for \emph{CityNav} agents with different goal representations: landmark-based, as well as latitude and longitude classification-based and regression-based.
}
\label{fig:goal}
\end{center}
\vskip -0.2in
\end{figure}

As described in Section 3.1 of the paper, our task uses a goal description which is a vector of normalised distances to a set of fixed landmarks. Reducing the density of the landmarks to half, a quarter or an eighth ($50\%$, $25\%$, $12.5\%$) does not significantly reduce the performance (Fig. \ref{fig:goal}).
We also investigate some alternative representations: a) latitude and longitude scalar coordinates normalised to be between 0 and 1 (\emph{Lat/long scalar} in Figure \ref{fig:goal}), and b) a binned representation \emph{Lat/long binned} using 35 bins for X and Y each, with each bin covering 100m. The Lat/long scalar goal representations performs best.

Fig. \ref{fig:goal} compares the performance of the \emph{CityNav} agent for different goal representations $g_t$ on the NYU environment. The most intuitive one consists in normalized latitude and longitude coordinates, or in binned representation of latitude and longitude (we used 35 bins for X and 35 bins for Y, where each bin covers 100m, or 80 bins for each coordinate).

An alternative goal representation is expressed in terms of the distances $\{d^g_{t,k}\}_k$ from the goal position $(x^g_t, y^g_t)$ to a set of arbitrary landmarks $\{x_k, y_k\}_k$. We defined $g_{t,i} = \frac{\exp(-\alpha d^g_{t,k})}{\sum_k \exp(-\alpha d^g_{t,k})}$ and tuned $\alpha=0.002$ using grid search. We manually defined 644 landmarks covering New York, Paris and London, which we use throughout the experiments and which are illustrated on Fig.2a. We observe that reducing the density of the landmarks to half, a quarter or an eighth has a slightly detrimental effect on performance because some areas are sparsely covered by landmarks. Because the landmark representation is independent of the coordinate system, we choose it and use it in all the other experimnets in this paper.

Finally, we also train a  \emph{Goal-less CityNav} agent by removing inputs $g_t$. The poor performance of this agent (Fig.~\ref{fig:goal}) confirms that the performance of our method cannot be attributed to clever street graph exploration alone. \emph{Goal-less CityNav} learns to run in circles of increasing radii---a reasonable, greedy behaviour that is a good baseline to the other agents.

Since the landmark-based representation performs well while being independent of the coordinate system and thus more scalable, we use this representation as canonical.


\subsection{Allocentric and Egocentric Goal Representation}
\label{supp:representation}

We do an analysis of the activations of the 256 hidden units of the region-specific LSTM, by training decoders (2-layer multi-layer perceptrons, MLP, with 128 units on the hidden layer and rectified nonlinearity transfer functions) for the allocentric position of the agent and of the goal as well as for the egocentric direction towards the goal. Allocentric decoders are multinomial classifiers over the joint Lat/Long position, and have 50$\times$50 bins (London) or 35$\times$35 bins (NYU), each bin covering an area of size 100m$\times$100m. The egocentric decoder has 16 orientation bins. Fig.\ref{fig:decoding} illustrates the noisy decoding of the agent's position along 3 trajectories and the decoding of the goal (here, St Paul's), overlayed with the ground truth trajectories and goal location. The average error of the egocentric goal direction decoding is about $60^{\degree}$ (as compared to $90^{\degree}$ for a random predictor), suggesting some decoding but not a cartesian manifold representation in the hidden units of the LSTM.   

\begin{figure}[ht]
% \vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.5\columnwidth]{figures/pos_goal_decoding.png}}
\caption{Decoding of the agent position (blue dots) and goal position (cyan stars) over 3 trajectories (in red) with a goal at St Paul's Cathedral, in London (in black).}
\label{fig:decoding}
\end{center}
\vskip -0.4in
\end{figure}

\subsection{Reward Shaping: Goal Rewards vs. Rewards}
\label{supp:rewards}

The agent is considered to have reached the goal if it is within 100m of the goal, and the reward shaping consists in giving the agent early rewards as it is within 200m of the goal. Early rewards are shaped as following:
\[
r_t = \max \left( 0, \min \left( 1, \frac{200 - d^g_t}{100} \right) \right) \times r^g
\]
where $d^g_t$ is the distance from the current position of the agent to the goal and $r^g$ is the reward that the agent will receive if it reaches the goal. Early rewards are given only once per panorama / node, and only if the distance $d^g_t$ to the goal is decreasing (in order to avoid the agent developing a behavior of harvesting early rewards around the goal rather than going directly towards the goal). However, depending on the path taken by the agent towards the goal, it could earn slightly more rewards if it takes a longer path to the goal rather than a shorter path. Throughout the paper, and for ease of comparison with experiments that include reward shaping, we report only the rewards at the goal destination (\emph{goal rewards}).