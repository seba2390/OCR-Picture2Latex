Reward-driven navigation in a real-world environment is related to research in various areas of deep learning, reinforcement learning, navigation and planning.

{\bf Learning from real-world imagery.}
Localising from only an image may seem impossible, but humans can integrate visual cues to geolocate a given image with surprising  accuracy, motivating machine learning approaches. For instance, convolutional neural networks (CNNs) achieve competitive scores on the geolocation task~\cite{weyand2016planet} and CNN+LSTM architectures improve on this~\cite{donahue2015long, malinowski2017ask}.
robust location-based image retrieval \cite{arandjelovic2016netvlad}. 
Several methods \cite{berriel2018heading,khosla2014looking}, including DeepNav \cite{brahmbhatt2017deepnav}, use datasets collected using Street View or Open Street Maps and solve navigation-related tasks using supervision. RatSLAM demonstrates localisation and path planning over long distances using a biologically-inspired architecture \cite{milford2004ratslam}.
The aforementioned methods rely on supervised training with ground truth labels: with the exception of the compass, we do not provide labels in our environment. 

{\bf Deep RL methods for navigation.}
Many RL-based approaches for navigation rely on simulators which have the benefit of features like procedurally generated variations but tend to be visually simple and unrealistic \cite{beattie2016deepmind,kempka2016vizdoom,tessler_aaai17}. To support sparse reward signals in these environments, recent navigational agents use auxiliary tasks in training~\cite{mirowski2016learning,jaderberg2016reinforcement,lample_aaai17}. Other methods learn to predict future measurements or to follow simple text instructions \cite{dosovitskiy2016learning,hill2017understanding,hermann2017grounded,chaplot2017gated}; in our case, the goal is designated using proximity to local landmarks. Deep RL has also been used for active localisation \cite{chaplot2018active}. Similar to our proposed architecture, \cite{zhu_icra2017} show goal-conditional indoor navigation with a simulated robot and environment. 

To bridge the gap between simulation and reality, researchers have developed more realistic, higher-fidelity simulated environments \cite{dosovitskiy2017carla,kolve2017ai2,shah2018airsim,wu2018building}. However, in spite of their increasing photo-realism, the inherent problems of simulated environments lie in the limited diversity of the environments and the antiseptic quality of the observations. Photographic environments have been used to train agents on short navigation problem in indoor scenes with limited scale \cite{chang2017matterport3d,anderson2017vision,bruce2017one,mo2018adobeindoornav}. Our real-world dataset is diverse and visually realistic, comprising scenes with vegetation, pedestrians or vehicles, diverse weather conditions and covering large geographic areas. However, we note that there are obvious limitations of our environment: it does not contain dynamic elements, the action space is necessarily discrete as it must jump between panoramas, and the street topology cannot be arbitrarily altered.

{\bf Deep RL for path planning and mapping.} 
Several recent approaches have used memory or other explicit neural structures to support end-to-end learning of planning or mapping. These include Neural SLAM \cite{zhang2017neural} that proposes an RL agent with an external memory to represent an occupancy map and a SLAM-inspired algorithm, Neural Map \cite{parisotto2017neural} which proposes a structured 2D memory for navigation, Memory Augmented Control Networks \cite{khan2017memory}, which uses a hierarchical control strategy, and MERLIN, a general architecture that achieves superhuman results in novel navigation tasks \cite{wayne2018unsupervised}. Other work \cite{brunner2017teaching,chaplot2018active} explicitly provides a global map that is input to the agent. 
The architecture in \cite{gupta2017unifying} uses an explicit neural mapper and planner for navigation tasks as well as registered pairs of landmark images and poses. Similar to \cite{gupta2017cognitive,zhang2017neural}, they use extra memory that represents the ego-centric agent position. Another recent work proposes a graph network solution \cite{savinov2018semi}. The focus of our paper is to demonstrate that simpler architectures can explore and memorise very large environments using target-driven visual navigation with a goal-conditional policy.


