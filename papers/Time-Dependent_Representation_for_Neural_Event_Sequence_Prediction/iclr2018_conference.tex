\documentclass{article} % For LaTeX2e
\usepackage{iclr2018_workshop,times}
\usepackage{hyperref}
\usepackage{url}
\usepackage{amsmath}
\usepackage{graphicx}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\newcommand*{\prob}{\mathsf{\textit{P}}}
\DeclareMathOperator*{\argmin}{arg\,min}


\title{Time-Dependent Representation for Neural Event Sequence Prediction}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Yang Li \\
Google AI\\
Mountain View, CA 94043, USA \\
\texttt{liyang@google.com} \\
\And
Nan Du \\
Google Brain \\
Mountain View, CA 94043, USA \\
\texttt{dunan@google.com} \\
\AND
Samy Bengio \\
Google Brain \\
Mountain View, CA 94043, USA \\
\texttt{bengio@google.com}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.

\begin{document}


\maketitle

\begin{abstract}
  Existing sequence prediction methods are mostly concerned with time-independent sequences, in which the actual time span between events is irrelevant and the distance between events is simply the difference between their order positions in the sequence. While this time-independent view of sequences is applicable for data such as natural languages, e.g., dealing with words in a sentence, it is inappropriate and inefficient for many real world events that are observed and collected at unequally spaced points of time as they naturally arise, e.g., when a person goes to a grocery store or makes a phone call. The time span between events can carry important information about the sequence dependence of human behaviors. In this work, we propose a set of methods for using time in sequence prediction. Because neural sequence models such as RNN are more amenable for handling token-like input, we propose two methods for time-dependent event representation, based on the intuition on how time is tokenized in everyday life and previous work on embedding contextualization. We also introduce two methods for using next event duration as regularization for training a sequence prediction model. We discuss these methods based on recurrent neural nets. We evaluate these methods as well as baseline models on five datasets that resemble a variety of sequence prediction tasks. The experiments revealed that the proposed methods offer accuracy gain over baseline models in a range of settings.

\end{abstract}

% \section{Introduction}

% Event sequence prediction is a task to predict the next event\footnote{We use "event" in this paper for real world observations instead of "token" that is often used in sequence problems, e.g., words in a sentence. But they are equivalent to a sequence model.} based on a sequence of previously occurred events. Event sequence prediction has a broad range of applications, e.g., next word prediction in language modeling \citep{DBLP:journals/corr/JozefowiczVSSW16}, next place prediction based on the previously visited places, or next app to launch given the usage history. Recurrent neural networks (RNNs) can memorize and persist the history of a sequence in its internal state to influence the future output of the network \citep{graves2012supervised}, which has become a natural choice for many sequence prediction tasks.

% Sequence prediction tasks today primarily deal with time independent sequences, which a sequence consists of a series of tokens (events) that can be indexed by their order position in the sequence. These sequences are either inherently time-independent, e.g, each word in a sentence, or resulted from sampling a sequential behavior at an equally-spaced point in time, e.g., busy or not busy for an hourly traffic update. In a time-independent sequence, the distance between events is measured as the difference of their order positions while the actual elapsed time is irrelevant.

% Although the time-independent view of sequences is applicable to many sequence prediction tasks, a large class of sequence problems involve real world events that are observed and collected at unequally spaced points of time when these events naturally arise. For example, the time interval between consecutive phone calls a person makes can vary significantly. The duration of events and the interval between them potentially carry important information about a natural sequential phenomenon. However, how to effectively use time in sequence prediction largely remains unexplored.

% It would be extremely inefficient, for both computation and learning, to sample a long-range behavior such as making phone calls, at a fixed rate, which will result in many "idle" events when no events actually occur. These idle events arbitrarily increase the computation time and space needed for learning and make it harder to back-propagate errors through time. An alternative is to bring time as an additional input into a neural sequence model, which is much more convenient and compact than interpolating it as events. However, it is notoriously challenging for recurrent neural networks to directly handle continuous input that has a wide value range, as shown in our experiments. Meanwhile, we found there is little work for dealing with continuous real time in neural sequence prediction. Thus, it is important to devise methods that can leverage time in a neural sequence prediction model. 

% We are inspired by the fact that humans are very good at characterizing time span as high-level concepts. For example, we would say "watching TV for a little while" instead of using the exact minutes and seconds to describe the duration. We also notice that these high-level descriptions about time are event dependent. For example, watching movies for 30 minutes might feel much shorter than waiting in the line for the same amount of time. Thus, it is desirable to architect a neural sequence model to learn and incorporate such time-dependent event representation. 

% We here propose two methods for time-dependent event representation in a neural sequence prediction model: time masking of event embedding and event-time joint embedding. These methods are also motivated by the recent findings on word embedding contextualization in neural machine translation \citep{DBLP:journals/corr/ChoiCB16}. Choi et al. revealed that by contextualizing the embedding of each word based on all the words in a sentence it alleviates a recurrent neural network to disambiguate words for different contexts thus improves translation quality. In the same vein, we here use the time span associated with an event to better characterize the event by manipulating its embedding to give a recurrent model additional resolving power for sequence prediction. 

% We evaluated the proposed methods as well as baseline methods on two sequence prediction tasks: predicting next app launch based on a mobile user's app usage history, and song recommendation based on a person's listen history. The baseline methods include a vanilla RNN model that does not use time and the one that uses time directly as a continuous input to a recurrent layer. The experiments reveal that the proposed methods consistently improved prediction accuracy in comparison with the baseline models.

% \section{Background}

% \subsection{Sequence Prediction}

% Many natural phenomena involve sequential behaviors where each observation of the behavior cannot be treated as an independently and identically distributed (iid) sample \citep{Dietterich:2002:MLS:645890.671269}. Rather, both the input features and output labels of these observations are strongly correlated sequences \citep{graves2012supervised}. It is important for a model to capture the sequential patterns in these behaviors. Examples of early methods include Hidden Markov Models (HMM) and Conditional Random Fields (CRF) \citep{Lafferty:2001:CRF:645530.655813}. 

% One important weakness in the early approaches is the difficulty to capture long-range dependencies in sequences. Markovian property that many early methods rely on assumes the current state of the behavior only depends on the last or recent few states. Over the past decade, Recurrent neural networks (RNN) especially with Long-Short Term Memory (LSTM) \citep{Hochreiter:1997:LSM:1246443.1246450} have become popular in solving a variety of sequence prediction problems, including neural machine translation \citep{DBLP:journals/corr/BahdanauCB14}, image captioning \citep{DBLP:journals/corr/XuBKCCSZB15} and speech recognition \citep{DBLP:journals/corr/SoltauLS16}.

% \subsection{Neural Event Sequence Prediction}

% Although the methods we are going to propose are applicable to general sequence prediction problems that involve continuous time spans, we scope our exploration in this paper for the class of problems regarding event sequence prediction. The basic task of event sequence prediction\footnote{We also use "sequence prediction" for brevity in the rest of the paper.} is to predict the next event, $e_{t}$, given the sequence of previously occurred events, $e_1, e_2, ..., e_{t-1}$. This computes the conditional probability distribution over all the possible events in a fixed event set $V$ (the vocabulary) based on the event history.

% \begin{equation*}
%     \prob(e_t=e|e_{1:t-1}), e\in{V} 
% \end{equation*}

% This can be estimated by using a recurrent neural network with parameters $\theta$ and introducing a hidden state vector, $h_t$.
% \begin{equation*}
%     \prob(e_t=e|h_t;\theta)
% \end{equation*}
% $h_t$ is a function of the previous hidden state, $h_{t-1}$, and the previous event, $e_{t-1}$. $h_t$ essentially captures the state of the sequence regarding the previous events as a continuous vector.

% \begin{equation}
%     h_t=f(h_{t-1},e_{t-1};\theta)
% \end{equation}

% $\prob(e_t=e|h_t;\theta)$ is typically realized as a linear or non-linear projection of the state vector onto a vector of scores, one score for each possible event in $V$. These scores are then normalized into the probability space using a softmax transformation. $f(h,e;\theta)$ is often implemented as a non-linear transformation, e.g., using a set of long-short term memory (LSTM) units \citep{Hochreiter:1997:LSM:1246443.1246450}, that takes the previous state and the previous event and outputs the current hidden state. The model can be trained by maximizing the log likelihood of generating the correct event sequences. The training is often achieved by minimizing the cross-entropy loss defined over the target sequence and the predicted one through stochastic gradient descent and back-propagation through time.

% There is one aspect worth clarification. Many sequence prediction frameworks, especially neural machine translation, use the model output (the prediction) at the previous step as the input in the current step \citep{DBLP:journals/corr/BengioVJS15}. In contrast, this is not needed for an event sequence prediction model. As the sequence unfolds, each event becomes observed thus the input to the network.

% To feed an event into a recurrent neural network, the event, a categorical variable, needs to be represented as a continuous vector. A common way to achieve this is to use embedding \citep{Bengio:2003:NPL:944919.944966}.

% \begin{equation}
%     x_t=1(e_t)E^x
% \end{equation}

% where $1(e_t)$ is a one-hot vector. For the $j$th event in the vocabulary $V$, $e^j$, its one-hot vector has $0$s for all the entries except the $j$th entry being $1$. $E^x\in{R^{|V|\times{E}}}$ is the embedding matrix, where $|V|$ is the number of unique events (the vocabulary size) and $E$ is the embedding dimension.

% The use of embedding provides a dense representation for an event that improves learning \citep{Turian:2010:WRS:1858681.1858721}. Through training, the embedding vector of an event encodes its meaning relative to other events. Events that are similar tend to have embedding vectors closer to each other in the embedding space than those that are not. The entire model is illustrated graphically in Figure 1.

% \begin{figure}[h]
%   \centering
%   \includegraphics[width=0.6\linewidth]{basic_rnn.png}
%   \caption{A schematic illustration of a basic architecture of neural sequence prediction with recurrent neural networks and embedding. There is also an optional nonlinear projection after the recurrent layer. $e_i$ denotes an observed event fed to the network as input and $e_{i+1}^{'}$ represents the prediction at step $i$.}
% \end{figure}

\section{Introduction}

Event sequence prediction is a task to predict the next event\footnote{We use "event" in this paper for real world observations instead of "token" that is often used in sequence problems, e.g., words in a sentence. But they are equivalent to a sequence model.} based on a sequence of previously occurred events. Event sequence prediction has a broad range of applications, e.g., next word prediction in language modeling \citep{DBLP:journals/corr/JozefowiczVSSW16}, next place prediction based on the previously visited places, or next app to launch given the usage history. Depending on how the temporal information is modeled, event sequence prediction often decomposes into the following two categories: discrete-time event sequence prediction and continuous-time event sequence prediction. 

Discrete-time event sequence prediction primarily deals with sequences that consist of a series of tokens (events) where each token can be indexed by its order position in the sequence. Thus such a sequence evolves synchronously in natural unit-time steps. These sequences are either inherently time-independent, e.g, each word in a sentence, or resulted from sampling a sequential behavior at an equally-spaced point in time, e.g., busy or not busy for an hourly traffic update. In a discrete-time event sequence, the distance between events is measured as the difference of their order positions. As a consequence, for discrete-time event sequence modeling, the primary goal is to predict what event will happen next. 

Continuous-time event sequence prediction mainly attends to the sequences where the events occur asynchronously. For example, the time interval between consecutive clinical visits of a patient may potentially vary largely. The duration between consecutive log-in events into an online service can change from time to time. Therefore, one primary goal of continuous-time event sequence prediction is to predict when the next event will happen in the near future.

Although these two tasks focus on different aspects of a future event, how to learn a proper representation for the temporal information in the past is crucial to both of them. More specifically, even though for a few discrete-time event sequence prediction tasks (e.g., neural machine translation), they do not involve an explicit temporal information for each event (token), a proper representation of the position in the sequence is still of great importance, not to mention the more general cases where each event is particularly associated with a timestamp. For example, the next destination people want to go to often depends on what other places they have gone to and how long they have stayed in each place in the past. When the next clinical visit \citep{Choi16} will occur for a patient depends on the time of the most recent visits and the respective duration between them. Therefore, the temporal information of events and the interval between them are crucial to the event sequence prediction in general. However, how to effectively use and represent time in sequence prediction still largely remains under explored.

A natural and straightforward solution is to bring time as an additional input into an existing sequence model (e.g., recurrent neural networks). However, it is notoriously challenging for recurrent neural networks to directly handle continuous input that has a wide value range, as what is shown in our experiments. Alternatively, we are inspired by the fact that humans are very good at characterizing time span as high-level concepts. For example, we would say "watching TV for a little while" instead of using the exact minutes and seconds to describe the duration. We also notice that these high-level descriptions about time are event dependent. For example, watching movies for 30 minutes might feel much shorter than waiting in the line for the same amount of time. Thus, it is desirable to learn and incorporate these time-dependent event representations in general. Our paper offers the following contributions:
\begin{itemize}
\item{We propose two methods for \textit{time-dependent event representation} in a neural sequence prediction model: time masking of event embedding and event-time joint embedding. We use the time span associated with an event to better characterize the event by manipulating its embedding to give a recurrent model additional resolving power for sequence prediction.}

\item{We propose to use \textit{next event duration as a regularizer} for training a recurrent sequence prediction model. Specifically, we define two flavors of duration-based regularization: one is based on the negative log likelihood of duration prediction error and the other measures the cross entropy loss of duration prediction in a projected categorical space.}

\item We evaluated these proposed methods as well as several baseline methods on five datasets (four are public). These datasets span a diverse range of sequence behaviors, including mobile app usage, song listening pattern, and medical history. The baseline methods include vanilla RNN models and those found in the recent literature. These experiments offer valuable findings about how these methods improve prediction accuracy in a variety of settings.
\end{itemize}

\section{Background}
In recent years, recurrent neural networks (RNN) especially with Long-Short Term Memory (LSTM) \citep{Hochreiter:1997:LSM:1246443.1246450} have become popular in solving a variety of discrete-time event sequence prediction problems, including neural machine translation \citep{DBLP:journals/corr/BahdanauCB14}, image captioning \citep{DBLP:journals/corr/XuBKCCSZB15} and speech recognition \citep{DBLP:journals/corr/SoltauLS16}. In a nutshell, given the sequence of previously occurred events, $\{e_1, e_2, ..., e_{t}\}$, the conditional probability $\prob(e_{t+1}|\{e_1, e_2, ..., e_{t}\})=\prob(e_{t+1}|h_{t},\theta)$ of the next event $e_{t+1}$ is estimated by using a recurrent neural network with parameters $\theta$ and the hidden state vector $h_{t}=f(h_{t-1},e_{t},\theta)$ which is assumed to encode the information of the past events. 

To feed an event into a recurrent neural network, the event, often described as a categorical variable, needs to be represented in a continuous vector space. A common way to achieve this is to use embedding \citep{Bengio:2003:NPL:944919.944966} $x_t=1(e_t)E^x$ where $1(e_t)$ is a one-hot vector. For the $j$th event in the vocabulary $V$, $e^j$, its one-hot vector has $0$s for all the entries except the $j$th entry being $1$. $E^x\in{R^{|V|\times{E}}}$ is the embedding matrix, where $|V|$ is the number of unique events (the vocabulary size) and $E$ is the embedding dimension. The use of embedding provides a dense representation for an event that improves learning \citep{Turian:2010:WRS:1858681.1858721}. Through training, the embedding vector of an event encodes its meaning relative to other events. Events that are similar tend to have embedding vectors closer to each other in the embedding space than those that are not.

On the other hand, temporal point processes are mathematical abstractions for the continuous-time event sequence prediction task by explicitly modeling the inter-event interval as a continuous random variable. Since the occurrence of an event may be triggered by what happened in the past, we can essentially specify different models for the timing of the next event given what we have already known so far. Very recently, \citep{DuDaiTri16, MeiEis17, XiaYan17, XiaYanFar17} focus on expanding the flexibility of temporal point processes using recurrent neural networks where the prediction of the next event time is based on the current hidden state $h_{t}$ of RNN. However, all of these work use the direct concatenation between the inter-event interval and the respective event embedding as the input to the recurrent layer where the representation of the temporal information is limited. 

Recent work \citep{46488} employed "latent crossing" of event and time interval based on pairwise multiplication of their embedding vectors. However, the discretization function of time in their work is fixed instead of being learned from data. Because it is not clear how to properly represent time as input, in this work, we intend to let the model learn a proper representation for encoding temporal information in a sequence. In addition, we propose a collection of methods for using time, not only as input but also as a regularizer. Rather than proposing a standalone model, our approach should be considered an representation approach for time that can be used by general event sequence prediction models, including models proposed previously \citep{DuDaiTri16, MeiEis17}. 

% These methods are motivated by the recent findings on word embedding contextualization in neural machine translation \citep{DBLP:journals/corr/ChoiCB16}. Choi et al. revealed that by contextualizing the embedding of each word based on all the words in a sentence it alleviates a recurrent neural network to disambiguate words for different contexts thus improves translation quality. In the same vein, we use the time span associated with an event to better characterize the event by manipulating its embedding to give a recurrent model additional resolving power for sequence prediction.

\section{Time-Dependent Event Representation}

%The above architecture is applicable for addressing a range of sequence prediction tasks where the actual time span between events is irrelevant. For example, the sequential distance between two words in a sentence is simply the difference of their order position in the sequence. Similarly, the distance between two hourly traffic updates is the number of updates between them. To address sequences that have events collected at unequally spaced points of time only when they occur, e.g., when a person calls a friend, we need to treat the time span of an event as a continuous variable in the sequence prediction, for a number of reasons as discussed in the introduction.
%
There are two notions about time spans in a sequential behavior: duration and intervals. Duration is how long an event lasts, e.g., listening to music for an half hour, and an interval is the time span between two adjacent events. To unify both types of time spans, we treat the idle period when no event is occurring (e.g., the person is not using any app for an app usage sequence) as a special event. Thus, duration becomes an inherent property of an event--the interval between two events is the duration of an idle event (see Figure \ref{timeline}). 

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.6\linewidth]{duration.pdf}
  \caption{\label{timeline}An interval is treated as the "duration" of an idle event.}
\end{figure}

With this, $h_t=f(h_{t-1},e_{t},d_{t};\theta)$ where $d_{t}$ is the duration of event $e_{t}$. We here propose two methods to bring continuous time, $d_{t}$, into a neural sequence prediction model. Both achieve time-dependent event representation by manipulating event embedding vectors using time. Our methods are schematically illustrated in Figure \ref{architecture}. 

%Intuitively, time as a continuous quantity can carry rich information about a sequential behavior. For example, how long a person has watched YouTube might affect his or her next action. One simple way to add the duration time of an event to the model is to concatenate the time value with the embedding vector of the event as the input to the recurrent layer. However, continuous input with a large numerical range is notoriously difficult to use in recurrent neural networks that are more amenable for handling token-like input. %


\begin{figure}[ht]
  \centering
  \includegraphics[width=0.6\linewidth]{tdrnn.pdf}
  \caption{\label{architecture}A time-dependent RNN for event sequence prediction. $d_{t}$ is used to generate time-dependent event embedding. Next event duration can be used as a regularizer, which can be applied to the recurrent layer and/or any post recurrent layer.}
\end{figure}

\subsection{Contextualizing Event Embedding with Time Mask}

Recent work by \citep{DBLP:journals/corr/ChoiCB16} revealed that in neural machine translation the embedding vector of a word encodes multiple meanings of the word. As a result, it requires a recurrent layer to sacrifice its capacity to disambiguate a word based on its context, instead of focusing on its main task for learning the higher-level compositional structure of a sentence. To address this problem, they used a mask computed based on all the words in a sentence to contextualize the embedding of a target word.

Based on this recent work, we propose a method to learn a time mask to "contextualize" event embedding, by which we hope a time-dependent embedding would give the recurrent layer additional resolving power. Similar to the word mask proposed by Choi et al. \citep{DBLP:journals/corr/ChoiCB16}, we first compute a time context vector for duration, $c^d$. 

\begin{equation}
    c^d=\phi(\log(d_t); \theta)
\end{equation}

$\phi$ is a nonlinear transformation of $d_t$ and is implemented as a feedforward neural network parameterized by $\theta$. $d_t$ is log transformed before it is fed to $\phi$ to effectively cover the wide numerical range of duration values, e.g., it can range from seconds to hours for app usage events.

We compute a time mask by linearly transforming $c^d$ with weights $W_d\in{\mathbb{R}^{C\times{E}}}$ and bias $b_d\in{\mathbb{R}^E}$, which is followed by a sigmoid nonlinear activation, $\sigma$, to generate a mask $m_d\in{\mathbb{R}^E}$ and $\mathbb{R}^E\rightarrow[0,1]$. $C$ is the size of the time context vector, and $E$ is the event embedding dimension.

\begin{equation}
    \label{mask}
    m_d=\sigma(c^dW_d+b_d)
\end{equation}

We then apply the mask to an event embedding by performing an element-wise multiplication, $\odot$, between the embedding vector and the mask. Finally, the product is fed to the recurrent layer.

\begin{equation}
    x_t\leftarrow{x_t\odot{m_d}}
\end{equation}

\subsection{Event-Time Joint Embedding}

Humans developed many ways to tokenize continuous time in everyday life. For example, we would say "talk to someone briefly" instead of using exact minutes and seconds to characterize the length of the conversation. Such a kind of tokenization is extensively used in natural languages. In addition, our perception about the duration also depends on the specific event that we are experiencing. Based on these intuitions, we propose a method to first encode the duration of an event using \emph{soft} one-hot encoding and then use the encoding to form the joint embedding with the event. 

To do so, we first project the scalar duration value onto a vector space, where $W_d\in{\mathbb{R}^{1\times{P}}}$ is the weight matrix, $b_d\in{\mathbb{R}^{P}}$ is the bias vector, and $P$ is the projection size.

\begin{equation}
\label{projection}
    p^d=d_tW_d+b_d
\end{equation}

We then compute the soft one-hot encoding, $s^d$, of a duration value by applying a softmax function to the projection vector, $p^d$. Softmax has been typically used in the output layer \citep{graves2012supervised} and in the attention mechanisms \citep{DBLP:journals/corr/BahdanauCB14, DBLP:journals/corr/XuBKCCSZB15} for selecting one out of many. The $i$th entry of the encoding vector is calculated as the following and $p^d_i$ is the $i$th entry in $p^d$.

\begin{equation}
\label{softmax}
    s_i^d=\dfrac{\exp(p^d_i)}{\sum_{k=1}^{P}\exp(p^d_k)}
\end{equation}

All the entries in the soft one-hot encoding are positive. Similar to a regular one-hot encoding, $\sum_{i=1}^{P}s_i^d=1$. We then project the soft one-hot encoding onto a time embedding space, $g_d$. It has the same dimension as the event embedding. $E^s\in{R^{P\times{E}}}$ is the embedding matrix.

\begin{equation}
    g_d=s^dE^s
\end{equation}

Embedding for a regular one-hot encoding essentially takes a single row of the embedding matrix that is corresponding to the non-zero entry as the embedding vector. In contrast, embedding for a soft one-hot encoding computes a weighted sum over all the rows in the embedding matrix. Finally, we form the joint embedding of an event and its duration by taking the mean of their embedding vectors, which is then fed to the recurrent layer.

\begin{equation}
    x_t\leftarrow\dfrac{{x_t+g_d}}{2}
\end{equation}

\section{Next Event Duration as a Regularizer}

While our goal here is to predict next event, it can help learning by introducing an additional loss component based on the prediction of the next event duration (see Figure \ref{architecture}). The duration prediction of the next event at step $t$, $d_{t+1}^\prime$, is computed from a linear transformation of the recurrent layer. A loss defined on the prediction error of $d_{t+1}^\prime$ provides additional information during back propagation, acting like a regularizer. Optionally, one can use the concatenation of the recurrent layer output and a hidden layer on the path for event prediction to regularize more layers. We discuss two alternatives for the loss function over $d_{t+1}^\prime$.

\subsection{Negative Log Likelihood of Time Prediction Error}

A common way for the loss over a continuous value is to use the squared error. Here, it is $(d_{t+1}^\prime-d_{t+1})^2$ where $d_{t+1}$ is the observed duration of the next event. However, such a loss needs to be at the same scale as that of of event prediction, which is typically a log likelihood of some form. Hinton and Van Camp \citep{Hinton:1993:KNN:168304.168306} have shown that minimizing the squared error can be in fact formulated as maximizing the probability density of a zero-mean Gaussian distribution. Note that this does not require duration to obey a Gaussian distribution but rather the prediction error. We define our regularizer, $R_{t}^N$, as the negative log likelihood of duration prediction error at step $t$. 
\begin{equation}
    R_{t}^N=\dfrac{(d_{t+1}^\prime-d_{t+1})^2}{2\sigma_{i}^2}
\end{equation}
The variance, $\sigma_{i}$, is seeded with an initial value (e.g., the variance of duration values in the training data) and updated iteratively during training based on the duration prediction error distribution of the learned model at each update $i$.

\subsection{Cross Entropy Loss on Time Projection}

In Section 3.2, we proposed to use softmax to project a continuous duration value onto a categorical space. Using the same technique, by projecting both $d_{t+1}^\prime$ and $d_{t+1}$ onto a categorical space, we can then compute a cross entropy loss based on the two projections as another regularizer $R_{t}^X$. 

\begin{equation}
    R_{t}^X=-\sum_{k=1}^{P}{Proj_{k}(d_{t+1})\log{Proj_{k}(d_{t+1}^\prime)}}
\end{equation}

$Proj$ is the softmax projection process we defined in Equation \ref{projection} and \ref{softmax}, $Proj_{k}$ is the $k$th entry in the projection vector. When event-time joint embedding and $R_{t}^X$ are both used, the embedding and the regularizer can use the same projection function, i.e., sharing the same projection weights (Equation \ref{projection}).

% \section{Experiments}

%  We evaluated the proposed methods as well as baseline models on five sequence prediction tasks, including next app prediction based on previously used apps and music recommendation based on personal listen history. The purpose of these experiments is not to find the best possible model for each task, because there are other domain-specific signals to be considered for better prediction quality (e.g., the physical location of a user when using an app). Rather, we intend to find out the effects of having continuous time as an additional input in a basic sequence prediction model.

% \subsection{Next App Prediction}

% \paragraph{Task and Data Preparation} Mobile users often use a large number of apps, ranging from tens to hundreds. It is time consuming to find a target app on mobile devices. One promising way to address this problem is to predict the next app a user will use based on their app usage history. Being able to predict next apps also allows the mobile platform to preload an app in memory to speed up its startup.

% We examine the proposed methods on a dataset that contains app usage sequences collected from 18,644 Android users. Each app event has both duration and the elapsed time since the last app usage in seconds. To avoid users who participated in the data collection only briefly, we exclude sequences that have fewer than 50 app launches or if the time span of the sequence is shorter than a week. This resulted in 5,891 app usage sequences, one from each unique user. These sequences include 2,863,095 app usage events and the longest sequence spanned 551 days. We split the dataset on users into the training (80\%), validation (10\%) and test (10\%) such that each user is only in one of these partitions. Hence there is no intersection of users between training, validation and test sets. For an event that has fewer than 5 occurrences in the training dataset, we assign it the OOV id for out of vocabulary. In total, there are 7,327 events in the vocabulary, including 7,325 unique apps, the idle event and the OOV.

% \paragraph{Model Configuration} We determined an optimal model architecture and hyper parameters based on the training and the validation datasets on a distributed parallel tuning infrastructure. We used LSTM units \citep{Hochreiter:1997:LSM:1246443.1246450} for the recurrent layer, and Rectified Linear Units (ReLu) \citep{icml2010_NairH10} for the activation function in the nonlinear projection layer. The event embedding dimension, the number of LSTM units, and the nonlinear projection layer size are all 128. There are 9 models compared. The baseline model is when no time is used and is referred to as \emph{Time Independent}. The rest models are time dependent: \emph{Log(time)} for feeding time (log transformed) directly into the recurrent layer along event embedding, \emph{Time Masking} (Section 3.1) and \emph{Joint Embedding} (Section 3.2) for generating time-dependent event embedding as input to the recurrent layer, and RMTPP for the model introduced previously \citep{DuDaiTri16}. We also include four regularized models based on $R_{t}^X$ and $R_{t}^N$ defined earlier. For Time Masking, the size of the time context vector, $C$, is 32, and we use ReLu for the activation function in $\phi$ in Equation \ref{mask}. For Joint Embedding, we chose the projection size, $P$ (Equation \ref{projection}), to be 30. For all these models, we use a two-layer hierarchical softmax \citep{Morin05hierarchicalprobabilistic} for the output layer.

% \paragraph{Training} We trained these models by minimizing the cross-entropy loss, plus the regularization loss if duration regularizer is used, over all the sequences in the training dataset and selected the best parameters for each model with early stopping based on the validation dataset. We used truncated back-propagation through time with the number of unroll to be 30. We used an adaptive gradient descent optimizer \citep{DBLP:journals/corr/abs-1212-5701}, using a learning rate of 0.024 with a threshold for gradient clipping of 1.0, and a batch size of 32. We decided not to use dropout as it didn't seem to improve accuracy on this task. We implemented the models in TensorFlow \citep{tensorflow}. The training was conducted on a distributed learning infrastructure \citep{Dean:2012:LSD:2999134.2999271} with 50 GPU cores where updates are applied asynchronously across multiple replicas.

% \paragraph{Evaluation} We evaluated each trained model based on the test dataset. We calculated prediction accuracy based on how accurately a model predicts the next app at each step in the sequence, excluding the idle events. It is automatically counted as an error if the target event is an OOV. The experimental results are reported in Table 1.

% \begin{table}[h!]
%   \caption{Prediction accuracy on test dataset for next app prediction in percentages.}
%   \label{sample-table}
%   \centering
%   \begin{tabular}{lll}
%     \toprule
%     \cmidrule{1-3}
%     Model & Precision@1 & Precision@5 \\
%     \midrule
%     Time Independent  & 30.29 & 13.07  \\
%     Log(time) & 31.03 & 12.98 \\
%     RMTPP & 31.31 & 12.9 \\
%     Time Masking (TM) & 31.29 & 13.13  \\
%     TM + $R_{t}^X$ & 31.3 & \textbf{13.15} \\
%     TM + $R_{t}^N$ & 31.41 & 13.1 \\
%     Joint Embedding (JE) & 31.3 & 13.07  \\
%     JE + $R_{t}^X$ & \textbf{31.53} & 13.09 \\
%     JE + $R_{t}^N$ & 31.45} & 13.13 \\
%     \bottomrule
%   \end{tabular}
% \end{table}

% \subsection{Music Recommendation}

% \paragraph{Task and Data Preparation} Based on a public dataset that includes the listening habits of 992 users \citep{lastfm, Celma:Springer2010}, we created a music recommendation sequence dataset. The sequence prediction task for the music dataset is to recommend 5 unique songs that the user has not listened given the user's listen history. Each listen event in the original dataset has a timestamp. We removed sequences that are shorter than 50 and songs that have fewer than 50 listens. Based on these listen habits, we generate a collection of examples where each example consists of a listen history and a set of 5 unique songs to recommend. To do so, we split each original listen sequence into segments (examples). To extract an example, we first take the 40 events out in order from the beginning of the sequence as the listen history, and then take more events out from the beginning of the sequence until we find 5 unique songs that have not occurred in the listen history. We do so repeatedly to extract each example until we exhaust all the original sequences. This data processing resulted in 221,920 sequence examples with 71,619 unique songs (the vocabulary size). We then allocate these sequence examples for the training (80\%), validation (10\%) and test (10\%). Because the original dataset does not have the duration information for each listen event, we did not inject the additional idle event in the sequence to differentiate duration versus intervals. 

% \paragraph{Model Configuration} Similar to the app prediction experiment, we determined model architecture and hyper parameters based on the training and validation dataset. We chose the embedding size as 128 and LSTM size as 256 for all the model conditions. We did not use the nonlinear projection layer after the LSTM layer for this task. We evaluate 9 models similar to the next app prediction experiment. We chose 32 for the time context vector size and 30 for the time projection size. Again, we use a two-layer hierarchical softmax \citep{Morin05hierarchicalprobabilistic} as the output layer of all these models.

% \paragraph{Training} We trained these models, in a similar way to the app prediction task, for producing all the listen sequences in the training dataset. The training here used the full sequence back-propagation through time with 2\% dropout ratio on the recurrent layer for better generalization. We used the Adam optimizer by \citep{DBLP:journals/corr/KingmaB14} for adaptive learning with a learning rate of 0.00005 and a gradient clipping threshold at 1.0. The mini-batch size is 256. Again, we implemented these models in TensorFlow \citep{tensorflow}. The training was conducted on a distributed learning infrastructure \citep{Dean:2012:LSD:2999134.2999271} with 50 GPU cores.

% \paragraph{Evaluation} We evaluated each trained model based on the test dataset. For music recommendation, it is more important to retrieve a collection of songs of interest that the user has not listened rather than predicting the next song the user will listen. Thus, we used a different evaluation metric from the one used for the app prediction task. We calculated how accurately a model can retrieve songs based on the 5 unique songs in the recommendation set of each example, after feeding the entire history (40 listen events) into the sequence model. Because the recommendations do not occur in the listen history, we filtered the predictions by removing those that have occurred in the listen history. The experimental results are reported in Table 2.

% \begin{table}[h!]
%   \caption{Prediction accuracy on test dataset for music recommendation. The numbers are percentages. MAP@K stands for Mean Average Precision at K, and P@K for Precision at K.}
%   \label{sample-table}
%   \centering
%   \begin{tabular}{lllllll}
%     \toprule
%     \cmidrule{1-2}
%     Model & MAP5 & MAP10 & MAP20 & P@5 & P@10 & P@20 \\
%     \midrule
%     Time Independent  & 11.59 &	13.18 & 13.83 &	13.82 & 8.75 &	5.25 \\
%     Log(time) & 11.41 &	12.85 &	13.51 &	13.53 &	8.63 &	5.19 \\
%     RMTPP & 11.51 & 12.93 & 13.59 & 13.62 & 8.66 & 5.19 \\
%     Time Masking (TM) & 11.74 &	13.18 & 13.83 &	13.79 &	8.81 & 5.28  \\
%     TM + $R_{t}^X$ & 11.71 & 13.17 & 13.82 & 13.81 & 8.76 &	5.25 \\
%     TM + $R_{t}^N$ & 11.69 & 13.16 & 13.8 & 13.81 & 8.76 &	5.30 \\
%     Joint Embedding (JE) & 11.82 & 13.37 & 14.06 & 13.95 &	8.97 & 5.40  \\
%     JE + $R_{t}^X$ & \textbf{12.02} & \textbf{13.51} & \textbf{14.2} & \textbf{14.12} & \textbf{9.01} & \textbf{5.43} \\
%     JE + $R_{t}^N$ & 11.9 &	13.41 & 14.11 & 14.05 & 8.98 &	5.40 \\
%     \bottomrule
%   \end{tabular}
% \end{table}

 \section{Experiments}
In this section, we evaluate the effectiveness of our proposed approaches on the following five real-world datasets across a diverse range of domains.
\begin{itemize}
\item Electrical Medical Records. MIMIC II medical dataset
is a collection of de-identified clinical visit records of Intensive
Care Unit patients for seven years. The filtered dataset released by \citep{DuDaiTri16} include 650 patients and 204 diseases. The goal is to predict which major disease will happen to a given patient.
\item Stack Overflow Dataset. The Stack Overflow dataset includes two years of user awards on a question-answering website. The awarded badges are treated as the events. \citep{DuDaiTri16} collected 6,000 users with a total of 480,000 events. The goal is to predict the next badge a user will receive.
\item Financial Transaction Dataset. \citep{DuDaiTri16} collected a long stream of high frequency transactions for a single stock from NYSE where the events correspond to the "buy" and "sell" actions. The task is to predict the next action a user might take.
\item App Usage Dataset. Mobile users often use a large number of apps, ranging from tens to hundreds. It is time consuming to find a target app on mobile devices. One promising way to address this problem is to predict the next app a user will use based on their app usage history. Being able to predict next apps also allows the mobile platform to preload an app in memory to speed up its startup. We have collected 5,891 app usage sequences comprising of 2.8 million app usage events. The task is to predict the next app that will be used for a given user.
\item Music Recommendation. The music dataset represents the longitudinal listening habits of 992 users \citep{lastfm, Celma:Springer2010} involving millions of listening events. The goal is to predict the next five unique songs that the user has not listened given the user's listen history.
% Based on a public dataset that includes the listening habits of 992 users \citep{lastfm, Celma:Springer2010}, we created a music recommendation sequence dataset. The sequence prediction task for the music dataset is to recommend 5 unique songs that the user has not listened given the user's listen history. Each listen event in the original dataset has a timestamp. We removed sequences that are shorter than 50 and songs that have fewer than 50 listens. Based on these listen habits, we generate a collection of examples where each example consists of a listen history and a set of 5 unique songs to recommend. To do so, we split each original listen sequence into segments (examples). To extract an example, we first take the 40 events out in order from the beginning of the sequence as the listen history, and then take more events out from the beginning of the sequence until we find 5 unique songs that have not occurred in the listen history. We do so repeatedly to extract each example until we exhaust all the original sequences. This data processing resulted in 221,920 sequence examples with 71,619 unique songs (the vocabulary size). We then allocate these sequence examples for the training (80\%), validation (10\%) and test (10\%). Because the original dataset does not have the duration information for each listen event, we did not inject the additional idle event in the sequence to differentiate duration versus intervals. 

\end{itemize}

\subsection{Data Preparation}
For the MIMIC II, Stack Overflow, and Financial data, we follow \citep{DuDaiTri16} to pre-process the data and seek to predict every single held-out event from the history. We evaluate the prediction accuracy with the binary 0-1 loss. 

For the app usage data, to avoid users who participated in the data collection only briefly, we exclude sequences that have fewer than 50 app launches or if the time span of the sequence is shorter than a week. This resulted in 5,891 app usage sequences, one from each unique user. These sequences include 2,863,095 app usage events and the longest sequence spanned 551 days. We split the dataset on users into the training (80\%), validation (10\%) and test (10\%) such that each user is only in one of these partitions. Hence there is no intersection of users between training, validation and test sets. For
an event that has fewer than 5 occurrences in the training dataset, we assign it the OOV id for out of vocabulary. In total, there are 7,327 events in the vocabulary, including 7,325 unique apps, the idle event and the OOV (out of vocabulary). In practice, predicting the next 5 apps is often desired so we use Precision@K to evaluate the performance. 

For the music recommendation, each listen event has a timestamp. We removed sequences that are shorter than 50 and songs that have fewer than 50 listens. We thus generate a collection of examples where each example consists of a listen history and a set of 5 unique songs to recommend. To do so, we split each original listen sequence into segments. We first take the 40 events out in order from the beginning of the sequence as the listen history, and then take more events out from the beginning of the sequence until we find 5 unique songs that have not occurred in the listen history. We do so repeatedly to extract each example until we exhaust all the original sequences. This data processing resulted in 221,920 sequence examples with 71,619 unique songs (the vocabulary size). We then allocate these sequence examples for the training (80\%), validation (10\%) and test (10\%). Because the original dataset does not have the duration information for each listen event, we did not inject the additional idle event in the sequence to differentiate duration versus intervals. Because in practice, the ranking order of the recommended music often matters, we further use MAP@K and Prevision@K to evaluate the performance.

\subsection{Model Configurations}
We compare with the following five models: \emph{NoTime} in which a simple LSTM sequence model is used; \emph{TimeConcat} in which we feed time (log transformed) directly into the recurrent layer along the event embedding; \emph{TimeMask} (Section 3.1) and \emph{TimeJoint} (Section 3.2) for generating time-dependent event embedding as input to the recurrent layer; and 
\emph{RMTPP} for the model introduced previously by \citep{DuDaiTri16}. Moreover, we also include four regularized models based on $R_{t}^X$ and $R_{t}^N$ defined earlier. For \emph{TimeMask}, the size of the time context vector is $C=32$, and we use ReLu for the activation function in $\phi$ in Equation \ref{mask}. For \emph{TimeJoint}, we chose the projection size, $P=30$ (Equation \ref{projection}). For the App Usage and Music Recommendation experiments, we use a two-layer hierarchical softmax \citep{Morin05hierarchicalprobabilistic} for the output layer due to the large vocabulary size, while we use a full sofmax for the rest experiments.

For the MIMIC II, Stack Overflow, and Financial data, we follow \citep{DuDaiTri16} for \emph{RMTPP}'s model parameters. For the app usage data, we determined the parameters of each model based on the training and the validation datasets on a distributed parallel tuning infrastructure. We used LSTM units \citep{Hochreiter:1997:LSM:1246443.1246450} for the recurrent layer, and Rectified Linear Units (ReLu) \citep{icml2010_NairH10} for the activation function in the nonlinear projection layer. The event embedding dimension, the number of LSTM units, and the nonlinear projection layer size are all set to 128. For the music recommendation data, we use a setting similar to the app prediction experiment where we chose the embedding size as 128 and LSTM size as 256. We did not use the nonlinear projection layer after the LSTM layer for this task because it does not seem to help. We implemented all the models in TensorFlow \citep{tensorflow}. 

\subsection{Training and Testing}

For the experiments based on MIMIC II, Stack Overflow and Financial Transaction datasets, we use the same training and testing strategy of \citep{DuDaiTri16}. For App Usage and Music Recommendation tasks, we selected the model architecture and hyper parameters with early stopping based on the validation dataset of each task, and report the performance of each model based on the test dataset. 

For the App Usage experiment, we used truncated back-propagation through time with the number of unroll to be 30. We used an adaptive gradient descent optimizer \citep{DBLP:journals/corr/abs-1212-5701}, using a learning rate of 0.024 with a threshold for gradient clipping of 1.0, and a batch size of 32. We decided not to use dropout as it did not seem to improve accuracy on this task.  

For the Music Recommendation experiment, we used the full sequence back-propagation through time with 2\% dropout ratio on the recurrent layer for better generalization. We used the Adam optimizer by \citep{DBLP:journals/corr/KingmaB14} for adaptive learning with a learning rate of 0.00005 and a gradient clipping threshold at 1.0. The mini-batch size is 256. 

We trained the models by minimizing the cross-entropy loss, plus the regularization loss if the duration regularizer is used, over all the sequences in the training dataset. The training for App Usage and Music Recommendation was conducted on a distributed learning infrastructure \citep{Dean:2012:LSD:2999134.2999271} with 50 GPU cores where updates are applied asynchronously across multiple replicas.

\subsection{Experimental Results}
\begin{figure*}[t]
	\centering
	\renewcommand{\tabcolsep}{2pt}
	\begin{tabular}{ccc}
		\includegraphics[height=0.26\columnwidth]{mimic2} & 
		\includegraphics[height=0.26\columnwidth]{so} & 
		\includegraphics[height=0.26\columnwidth]{bookorder} 
		\\
		(a) MIMIC II & (b) Stack Overflow & (c) Financial Data 
	\end{tabular}
	\vspace{-2mm}
	\caption{\label{real} Prediction accuracy on (a) MIMIC II, (b) Stack Overflow, and (c) Financial Data.}
\end{figure*}
\begin{table}[h!]
   \caption{Prediction accuracy on test dataset for next app prediction in percentages.\label{tbl_app}}
   \label{sample-table}
   \centering
   \begin{tabular}{lll}
     \toprule
     \cmidrule{1-3}
     Model & Precision@1 & Precision@5 \\
     \midrule
     NoTime  & 30.29 & 13.07  \\
     TimeConcat & 31.03 & 12.98 \\
     RMTPP & 31.31 & 12.9 \\
     TimeMask & 31.29 & 13.13  \\
     TimeMask + $R_{t}^X$ & 31.3 & \textbf{13.15} \\
     TimeMask + $R_{t}^N$ & 31.41 & 13.1 \\
     TimeJoint & 31.3 & 13.07  \\
     TimeJoint + $R_{t}^X$ & \textbf{31.53} & 13.09 \\
     TimeJoint + $R_{t}^N$ & 31.45 & 13.13 \\
     \bottomrule
   \end{tabular}
\end{table}
\begin{table}[h!]
   \caption{Prediction accuracy on test dataset for music recommendation. The numbers are percentages.\label{tbl_music}}
   \label{sample-table}
   \centering
   \begin{tabular}{lllllll}
     \toprule
     \cmidrule{1-2}
     Model & MAP5 & MAP10 & MAP20 & Precision@5 & Precision@10 & Precision@20 \\
     \midrule
     NoTime & 11.59 &	13.18 & 13.83 &	13.82 & 8.75 &	5.25 \\
     TimeConcat & 11.41 &	12.85 &	13.51 &	13.53 &	8.63 &	5.19 \\
     RMTPP & 11.51 & 12.93 & 13.59 & 13.62 & 8.66 & 5.19 \\
     TimeMask & 11.74 &	13.18 & 13.83 &	13.79 &	8.81 & 5.28  \\
     TimeMask + $R_{t}^X$ & 11.71 & 13.17 & 13.82 & 13.81 & 8.76 &	5.25 \\
     TimeMask + $R_{t}^N$ & 11.69 & 13.16 & 13.8 & 13.81 & 8.76 &	5.30 \\
     TimeJoint & 11.82 & 13.37 & 14.06 & 13.95 &	8.97 & 5.40  \\
     TimeJoint + $R_{t}^X$ & \textbf{12.02} & \textbf{13.51} & \textbf{14.2} & \textbf{14.12} & \textbf{9.01} & \textbf{5.43} \\
     TimeJoint + $R_{t}^N$ & 11.9 &	13.41 & 14.11 & 14.05 & 8.98 &	5.40 \\
     \bottomrule
   \end{tabular}
\end{table}

\emph{Effectiveness of Temporal Representation}. Figure \ref{real} presents the comparisons between all the models on three released public datasets. We can observe a consistent performance gain with using the proposed methods for time-dependent event embedding compared to the \emph{NoTime} baseline and the simple \emph{TimeConcat} approach. \emph{TimeJoint} significantly outperformed all other methods on both the Stack Overflow and the Financial dataset, with p<0.05 using Paired T-test. But none of the methods for using time is able to improve accuracy on the MIMIC II dataset.  This indicates that using time might not always help. However, when it does, our methods such as \emph{TimeJoint} enable more efficient representation of time than simply using the scalar value of time in RNN models.

Our methods also outperformed \emph{RMTPP} for event prediction. The performance gain of our models are more pronounced on the App Usage and Music Recommendation datasets as shown in Table \ref{tbl_app} and \ref{tbl_music}. \emph{TimeJoint} seems to outperform the rest on most measures and \emph{TimeMask} also performs well compared to other previous methods. We also notice that using time directly without representing them appropriately in RNN, i.e., \emph{TimeConcat}, can sometime hurt the performance.

\emph{Effectiveness of Event Duration Regularization}. We demonstrate the performance boosting gained from our proposed temporal regularization in Table \ref{tbl_app} and \ref{tbl_music}, respectively. We can observe that our proposed regularizers can bring additional performance gain on many cases. In particular, the cross-entropy regularizer, $R_{t}^X$, is able to give consistent performance gain with the temporal embedding approaches.

\emph{Learned Time Representation}. Our motivation in this work is to let the model learn a proper representation of time from data. We here briefly discuss what the TimeJoint approach learns about how to project a scalar value of time into a soft one-hot encoding \ref{proj_vis}. It seems that for small time periods, e.g., shorter than 20 seconds for the Next App prediction task, more dimensions are needed to express the differences of continuous time values. As the time period grows, we need less dimensions for representing time, e.g., two of the curves have converged to the same small values.

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.8\linewidth]{proj5.png}
  \caption{\label{proj_vis}The projection of time learned by TimeJoint with $P=5$. The X axis is in seconds and the Y axis is the projection of a time in each dimension defined in Equation 5.}
\end{figure}

\section{Conclusions}
We proposed a set of methods for leveraging the temporal information for event sequence prediction. Based on our intuition about how humans tokenize time spans as well as previous work on contextual representation of words, we proposed two methods for time-dependent event representation. They transform a regular event embedding with learned time masking and form time-event joint embedding based on learned soft one-hot encoding. We also introduced two methods for using next duration as a way of regularization for training a sequence prediction model. Experiments on a diverse range of real data demonstrate consistent performance gain by blending time into the event representation before it is fed to a recurrent neural network.

% We evaluated these methods on five event sequence prediction tasks, including next mobile app usage prediction, and music recommendation based on the user's listen history. The experiments revealed that it improves event prediction accuracy by blending time into the event representation before it is fed to a recurrent neural network. In contrast, feeding continuous time value directly to a recurrent layer does not seem to always help. We also found using next event duration as regularization can further improve prediction accuracy for several tasks.

\bibliography{iclr2018_conference}
\bibliographystyle{iclr2018_conference}

% \section{Appendix}

% \begin{figure}[h]
%   \centering
%   \includegraphics[width=0.6\linewidth]{time_dist.jpg}
%   \caption{The cumulative distribution of event duration for mobile app usage on based on an Android dataset. The X axis, duration seconds, is plotted at $\log_{10}$ scale.}
% \end{figure}

\end{document}
