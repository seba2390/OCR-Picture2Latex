\documentclass{article} % For LaTeX2e
\usepackage{iclr2018_conference,times}
\usepackage{hyperref}
\usepackage{url}
\usepackage{xspace}
\usepackage{booktabs}
\usepackage{enumitem}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{graphicx,wrapfig}
\usepackage[hang,flushmargin]{footmisc}

\usepackage{xcolor}
\hypersetup{
    colorlinks,
    linkcolor={red!50!black},
    citecolor={blue!50!black},
    urlcolor={blue!80!black}
}

\title{Seq2SQL: Generating Structured Queries from Natural Language using Reinforcement Learning}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Victor Zhong, Caiming Xiong, Richard Socher \\
Salesforce Research \\
Palo Alto, CA \\
\texttt{\{vzhong,cxiong,rsocher\}@salesforce.com}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.

\begin{document}

\input{definitions}


\maketitle

\begin{abstract}
Relational databases store a significant amount of the worldâ€™s data.
However, accessing this data currently requires users to understand a query language such as SQL. We propose \model, a deep neural network for translating natural language questions to corresponding SQL queries.
Our model uses rewards from in-the-loop query execution over the database to learn a policy to generate the query, which contains unordered parts that are less suitable for optimization via cross entropy loss.
Moreover, \model leverages the structure of SQL to prune the space of generated queries and significantly simplify the generation problem.
In addition to the model, we release \dataset, a dataset of \numinstances hand-annotated examples of questions and SQL queries distributed across \numtables tables from Wikipedia that is an order of magnitude larger than comparable datasets.
By applying policy-based reinforcement learning with a query execution environment to \dataset, \model outperforms a state-of-the-art semantic parser, improving execution accuracy from \perfeaseqtoseq to \perfeaours and logical form accuracy from \perflaseqtoseq to \perflaours.
\end{abstract}


\section{Introduction}

%A vast amount of today's information is stored in relational databases, which provide the foundation of applications such as medical records \citep{hillestad2005can}, financial markets \citep{beck2000new}, and customer relations management \citep{ngai2009application}.
Relational databases store a vast amount of today's information and provide the foundation of applications such as medical records \citep{hillestad2005can}, financial markets \citep{beck2000new}, and customer relations management \citep{ngai2009application}.
However, accessing relational databases requires an understanding of query languages such as SQL, which, while powerful, is difficult to master.
Natural language interfaces (NLI), a research area at the intersection of natural language processing and human-computer interactions, seeks to provide means for humans to interact with computers through the use of natural language~\citep{IAndroutsopoulos1995NaturalLI}.
We investigate one particular aspect of NLI applied to relational databases: translating natural language questions to SQL queries.

Our main contributions in this work are two-fold.
First, we introduce \model, a deep neural network for translating natural language questions to corresponding SQL queries.
\model, shown in Figure~\ref{fig:seq2sql}, consists of three components that leverage the structure of SQL to prune the output space of generated queries.
Moreover, it uses policy-based reinforcement learning (RL) to generate the conditions of the query, which are unsuitable for optimization using cross entropy loss due to their unordered nature.
We train \model using a mixed objective, combining cross entropy losses and RL rewards from in-the-loop query execution on a database.
These characteristics allow \model to achieve state-of-the-art results on query generation.


Next, we release \dataset, a corpus of \numinstances hand-annotated instances of natural language questions, SQL queries, and SQL tables extracted from \numtables HTML tables from Wikipedia.
\dataset is an order of magnitude larger than previous semantic parsing datasets that provide logical forms along with natural language utterances.
We release the tables used in \dataset both in raw JSON format as well as in the form of a SQL database.
Along with \dataset, we release a query execution engine for the database used for in-the-loop query execution to learn the policy.
On \dataset, \model outperforms a previously state-of-the-art semantic parsing model by~\citet{dong-neural_semantic_parsing}, which obtains \perfeaseqtoseq execution accuracy, as well as an augmented pointer network baseline, which obtains \perfeapointer execution accuracy.
By leveraging the inherent structure of SQL queries and applying policy gradient methods using reward signals from live query execution, \model achieves state-of-the-art performance on \dataset, obtaining \perfeaours execution accuracy.


\begin{figure}[t!] 
  %\vspace{-5mm}
  \begin{center}
	\includegraphics[width=0.75\textwidth]{Seq2SQL.pdf}
  \end{center}
  \vspace{-2mm}
  \caption{
\model takes as input a question and the columns of a table.
It generates the corresponding SQL query, which, during training, is executed against a database.
The result of the execution is utilized as the reward to train the reinforcement learning algorithm.
}\label{fig:seq2sql}
  \vspace{-0mm}
\end{figure}


\section{Model}
The \dataset task is to generate a SQL query from a natural language question and table schema.
Our baseline model is the attentional sequence to sequence neural semantic parser proposed by~\citet{dong-neural_semantic_parsing} that achieves state-of-the-art performance on a host of semantic parsing datasets without using hand-engineered grammar.
However, the output space of the softmax in their Seq2Seq model is unnecessarily large for this task.
In particular, we can limit the output space of the generated sequence to the union of the table schema, question utterance, and SQL key words.
The resulting model is similar to a pointer network~\citep{vinyals-pointer_networks} with augmented inputs.
We first describe the augmented pointer network model, then address its limitations in our definition of \model, particularly with respect to generating unordered query conditions.


\begin{figure}[t!]
  \vspace{-2mm}
  \begin{center}
	\includegraphics[width=\textwidth]{example.pdf}
  \end{center}
  \vspace{-5mm}
  \caption{
  An example in \dataset. The inputs consist of a table and a question. The outputs consist of a ground truth SQL query and the corresponding result from execution.
}\label{fig:example}
  \vspace{-5mm}
\end{figure}


% aug pointer
\subsection{Augmented Pointer Network}
\label{section:augpointer}


The augmented pointer network generates the SQL query token-by-token by selecting from an input sequence.
In our case, the input sequence is the concatenation of the column names, required for the selection column and the condition columns of the query,
the question, required for the conditions of the query,
and the limited vocabulary of the SQL language such as \texttt{SELECT}, \texttt{COUNT} etc.
In the example shown in Figure~\ref{fig:example},
the column name tokens consist of ``Pick'', ``\#'', ``CFL'', ``Team'' etc.;
the question tokens consist of ``How'', ``many'', ``CFL'', ``teams'' etc.;
the SQL tokens consist of \texttt{SELECT}, \texttt{WHERE}, \texttt{COUNT}, \texttt{MIN}, \texttt{MAX} etc.
With this augmented input sequence, the pointer network can produce the SQL query by selecting exclusively from the input.

Suppose we have a list of $N$ table columns and a question such as in Figure~\ref{fig:example}, and want to produce the corresponding SQL query.
Let $x_j\column = [x\column_{j, 1}, x\column_{j, 2}, ... x\column_{j, T_j}]$ denote the sequence of words in the name of the $j$th column, where $x\column_{j, i}$ represents the $i$th word in the $j$th column and $T_j$ represents the total number of words in the $j$th column.
Similarly, let $x\question$ and $x\sql$ respectively denote the sequence of words in the question and the set of unique words in the SQL vocabulary.

We define the input sequence $x$ as the concatenation of all the column names, the question, and the SQL vocabulary:
\begin{align}
x = \left[ \sentinelcol ; x_1\column ; x_2\column ; ... ; x_{N}\column; \sentinelsql; x\sql ; \sentinelquestion; x\question \right]
\end{align}
where $[a ; b]$ denotes the concatenation between the sequences $a$ and $b$ and we add sentinel tokens between neighbouring sequences to demarcate the boundaries.

The network first encodes $x$ using a two-layer, bidirectional Long Short-Term Memory network~\citep{Hochreiter1997LongSM}.
The input to the encoder are the embeddings corresponding to words in the input sequence.
We denote the output of the encoder by $h\enc$, where $h\enc_t$ is the state of the encoder corresponding to the $t^{\text{th}}$ word in the input sequence.
For brevity, we do not write out the LSTM equations, which are described by~\citet{Hochreiter1997LongSM}.
We then apply a pointer network similar to that proposed by~\citet{vinyals-pointer_networks} to the input encodings $h\enc$.

The decoder network uses a two layer, unidirectional LSTM.
During each decoder step $s$, the decoder LSTM takes as input $y_{s-1}$, the query token generated during the previous decoding step, and outputs the state $g_s$.
Next, the decoder produces a scalar attention score $\alpha\pointer_{s, t}$ for each position $t$ of the input sequence:
%
\begin{align}
\label{equation:decoderattn}
\alpha\pointer_{s, t} = W\pointer {\rm \tanh} \left( U\pointer g_s + V\pointer h_t \right)
\end{align}
We choose the input token with the highest score as the next token of the generated SQL query, $y_s = {\rm argmax}(\alpha\pointer_s)$.

% structured pointer + RL
\subsection{\model}

\begin{wrapfigure}[15]{R}{0.55\textwidth}
\vspace{-5mm}
  \begin{center}
	\includegraphics[width=0.55\textwidth]{structured.pdf}
  \end{center}
\vspace{-2mm}
  \caption{
The \model model has three components, corresponding to the three parts of a SQL query (right).
The input to the model are the question (top left) and the table column names (bottom left).
}\label{fig:structured}
\end{wrapfigure}

While the augmented pointer network can solve the SQL generation problem, it does not leverage the structure inherent in SQL.
Typically, a SQL query such as that shown in Figure~\ref{fig:structured} consists of three components.
The first component is the aggregation operator, in this case \texttt{COUNT}, which produces a summary of the rows selected by the query.
Alternatively the query may request no summary statistics, in which case an aggregation operator is not provided.
The second component is the \texttt{SELECT} column(s), in this case \texttt{Engine}, which identifies the column(s) that are to be included in the returned results.
The third component is the \texttt{WHERE} clause of the query, in this case \texttt{WHERE Driver = Val Musetti}, which contains conditions by which to filter the rows.
Here, we keep rows in which the driver is ``Val Musetti''.

\model, as shown in Figure~\ref{fig:structured}, has three parts that correspond to the aggregation operator, the \texttt{SELECT} column, and the \texttt{WHERE} clause.
First, the network classifies an aggregation operation for the query, with the addition of a null operation that corresponds to no aggregation.
Next, the network points to a column in the input table corresponding to the \texttt{SELECT} column.
Finally, the network generates the conditions for the query using a pointer network.
The first two components are supervised using cross entropy loss, whereas the third generation component is trained using policy gradient to address the unordered nature of query conditions (we explain this in the subsequent~\textbf{WHERE Clause} section).
Utilizing the structure of SQL allows \model to further prune the output space of queries, which leads to higher performance than Seq2Seq and the augmented pointer network.


\paragraph{Aggregation Operation.}
%
The aggregation operation depends on the question.
For the example shown in Figure~\ref{fig:structured}, the correct operator is \texttt{COUNT} because the question asks for ``How many''.
To compute the aggregation operation, we first compute the scalar attention score, $\alpha\inp_t = W\inp h\enc_t$, for each $t$th token in the input sequence.
%
We normalize the vector of scores $\alpha\inp = [\alpha\inp_1, \alpha\inp_2, ...]$ to produce a distribution over the input encodings, $\beta\inp = \softmax{\alpha\inp}$.
The input representation $\kappa\aggregation$ is the sum over the input encodings $h\enc$ weighted by the normalized scores $\beta\inp$:
%
\begin{align}
\label{equation:aggrepr}
\kappa\aggregation = \sum_t \beta\inp_t h\enc_t
\end{align}
%
Let $\alpha\aggregation$ denote the scores over the aggregation operations such as \texttt{COUNT}, \texttt{MIN}, \texttt{MAX}, and the no-aggregation operation \texttt{NULL}.
We compute $\alpha\aggregation$ by applying a multi-layer perceptron to the input representation $\kappa\aggregation$:
%
\vspace{-4mm}
\begin{align}
\alpha\aggregation = W\aggregation \tanh \left( V\aggregation \kappa\aggregation + b\aggregation \right) + c\aggregation
\end{align}
\vspace{-2mm}

% The dimensions of $\kappa\aggregation$, $W\aggregation$, $V\aggregation$, $b\aggregation$, $c\aggregation$ are respectively $2\dhid$, $1 \times \dhid$, $\dhid \times 2\dhid$, $\dhid$ and $\dhid$.
We apply the softmax function to obtain the distribution over the set of possible aggregation operations $\beta\aggregation = \softmax{\alpha\aggregation}$.
We use cross entropy loss $\loss\aggregation$ for the aggregation operation.


\paragraph{SELECT Column.}
%
The selection column depends on the table columns as well as the question.
Namely, for the example in Figure~\ref{fig:structured}, ``How many engine types'' indicates that we need to retrieve the ``Engine'' column.
\texttt{SELECT} column prediction is then a matching problem, solvable using a pointer:
given the list of column representations and a question representation, we select the column that best matches the question.

In order to produce the representations for the columns, we first encode each column name with a LSTM.
The representation of a particular column $j$, $e\column_j$, is given by:

%
\begin{align}
h\column_{j, t} = \lstm{ \emb{x\column_{j,t}} }{ h\column_{j, {t-1}} }
\qquad
e\column_j = h\column_{j, T_j}
\end{align}
%
Here, $h\column_{j, t}$ denotes the $t$th encoder state of the $j$th column.
We take the last encoder state to be $e\column_j$, column $j$'s representation.
% Both $h\column_{j, t}$ and $e\column_j$ have dimension $\dhid$.

To construct a representation for the question, we compute another input representation $\kappa\select$ using the same architecture as for $\kappa\aggregation$ (Equation~\ref{equation:aggrepr}) but with untied weights.
%
Finally, we apply a multi-layer perceptron over the column representations, conditioned on the input representation, to compute the a score for each column $j$:

\begin{equation}
\alpha\select_j = W\select \tanh \left( V\select \kappa\select + V\column e\column_j \right)
\end{equation}

We normalize the scores with a softmax function to produce a distribution over the possible \texttt{SELECT} columns $\beta\select = \softmax{\alpha\select}$.
For the example shown in Figure~\ref{fig:structured}, the distribution is over the columns ``Entrant'', ``Constructor'', ``Chassis'', ``Engine'', ``No'', and the ground truth \texttt{SELECT} column ``Driver''.
We train the \texttt{SELECT} network using cross entropy loss $\loss\select$.


\paragraph{WHERE Clause.}
\label{section:rl}
%
We can train the \texttt{WHERE} clause using a pointer decoder similar to that described in Section~\ref{section:augpointer}.
However, there is a limitation in using the cross entropy loss to optimize the network:
the \texttt{WHERE} conditions of a query can be swapped and the query yield the same result.
Suppose we have the question ``which males are older than 18'' and the queries 
\texttt{SELECT name FROM insurance WHERE age > 18 AND gender = "male"}
and
\texttt{SELECT name FROM insurance WHERE gender = "male" AND age > 18}.
Both queries obtain the correct execution result despite not having exact string match.
If the former is provided as the ground truth, using cross entropy loss to supervise the generation would then wrongly penalize the latter.
To address this problem, we apply reinforcement learning to learn a policy to directly optimize the expected correctness of the execution result (Equation~\ref{equation:reward}).

Instead of teacher forcing at each step of query generation, we sample from the output distribution to obtain the next token.
At the end of the generation procedure, we execute the generated SQL query against the database to obtain a reward.
Let $y = [y^1, y^2, ..., y^T]$ denote the sequence of generated tokens in the \texttt{WHERE} clause.
Let $\generated$ denote the query generated by the model and $\groundtruth$ denote the ground truth query corresponding to the question.
We define the reward $\reward$ as

\vspace{-2mm}
\begin{equation}
\label{equation:reward}
\reward = 
\begin{cases}
    -2, & \text{if } \generated \text{ is not a valid SQL query}\\
    -1, & \text{if } \generated \text{ is a valid SQL query and executes to an incorrect result}\\
    +1, & \text{if } \generated \text{ is a valid SQL query and executes to the correct result}
\end{cases}
\end{equation}
\vspace{-2mm}

The loss, $\loss\where = -\mathbb{E}_y [\reward]$, is the negative expected reward over possible \texttt{WHERE} clauses.
We derive the policy gradient for $\loss\where$ as shown by~\citet{sutton2000policy} and~\citet{Schulman2015GradientEU}.

\vspace{-5mm}
\begin{eqnarray}
\nabla \loss\where_\Theta &=& - \nabla_\Theta \left( \mathbb{E}_{y \sim p_y} \left[ \reward \right] \right)
% = - \mathbb{E}_{y \sim p_y} \left[ \reward \nabla_\Theta \log p_y \left(y ; \Theta \right) \right]
\\
&=& - \mathbb{E}_{y \sim p_y} \left[ \reward \nabla_\Theta \sum_t \left( \log p_y \left(y_t ; \Theta \right) \right) \right]
\\
\label{eq:monte_carlo}
&\approx& - \reward \nabla_\Theta \sum_t \left( \log p_y \left(y_t ; \Theta \right) \right)
\end{eqnarray}
\vspace{-4mm}

Here, $p_y(y_t)$ denotes the probability of choosing token $y_t$ during time step $t$.
In equation~\ref{eq:monte_carlo}, we approximate the expected gradient using a single Monte-Carlo sample $y$.
% \citet{Schulman2015GradientEU} show that sampling during the forward pass of the network can be followed by the corresponding injection of a synthetic gradient, which is a function of the reward, during the backward pass in order to compute estimated parameter gradient.

\paragraph{Mixed Objective Function.}
We train the model using gradient descent to minimize the objective function $\loss = \loss\aggregation + \loss\select + \loss\where$.
Consequently, the total gradient is the equally weighted sum of the gradients from the cross entropy loss in predicting the \texttt{SELECT} column, from the cross entropy loss in predicting the aggregation operation, and from policy learning.



\section{\dataset}
% \vspace{-2mm}


\begin{wrapfigure}[13]{R}{0.50\textwidth}
\vspace{-7mm}
  \begin{center}
	\includegraphics[width=0.50\textwidth]{stats_types.pdf}
  \end{center}
\vspace{-6mm}
  \caption{
  Distribution of questions in \dataset.
}\label{fig:stat_types}
\end{wrapfigure}



\dataset is a collection of questions, corresponding SQL queries, and SQL tables.
A single example in \dataset, shown in Figure~\ref{fig:example}, contains a table, a SQL query, and the natural language question corresponding to the SQL query.
Table~\ref{table:dataset_comparison} shows how \dataset compares to related datasets.
Namely, \dataset is the largest hand-annotated semantic parsing dataset to date - it is an order of magnitude larger than other datasets that have logical forms, either in terms of the number of examples or the number of tables.
The queries in \dataset span over a large number of tables and hence presents an unique challenge: the model must be able to not only generalize to new queries, but to new table schema.
Finally, \dataset contains realistic data extracted from the web.
This is evident in the distributions of the number of columns, the lengths of questions, and the length of queries, respectively shown in Figure~\ref{fig:stat_sizes}.
Another indicator of the variety of questions in the dataset is the distribution of question types, shown in Figure~\ref{fig:stat_types}.

\begin{wraptable}[21]{R}{0.5\textwidth}
\vspace{-3mm}
\centering
\begin{tabular}{lrlr}
\toprule
Dataset & Size & LF & Schema \\
\midrule
\textbf{\dataset} & \textbf{\numinstances}     & \textbf{yes}      & \textbf{\numtables}    \\
Geoquery & 880  & yes & 8    \\
ATIS & 5871 & yes\textsuperscript{*} & 141  \\
Freebase917 & 917 & yes & 81\textsuperscript{*}  \\
Overnight & 26098 & yes & 8  \\
WebQuestions & 5810 & no & 2420  \\
WikiTableQuestions & 22033 & no & 2108  \\
\bottomrule
\end{tabular}
\caption{
Comparison between \dataset and existing datasets.
The datasets are
GeoQuery880~\citep{Tang2001UsingMC},
ATIS~\citep{Price1990ATIS},
Free917~\citep{cai2013LargescaleSP}, 
Overnight~\citep{Wang2015BuildingAS},
WebQuestions~\citep{berant2013SemanticPO},
and WikiTableQuestions~\citep{Pasupat2015CompositionalSP}.
``Size'' denotes the number of examples in the dataset.
``LF'' indicates whether it has annotated logical forms. 
``Schema'' denotes the number of tables.
ATIS is presented as a slot filling task.
Each Freebase API page is counted as a separate domain.
}
\label{table:dataset_comparison}
\end{wraptable}


\begin{figure}[ht]
\vspace{-2mm}
  \begin{center}
	\includegraphics[width=0.9\textwidth]{stats_lengths.pdf}
  \end{center}
  \vspace{-5mm}
  \caption{
  Distribution of table, question, query sizes in \dataset.
}\label{fig:stat_sizes}
\vspace{-2mm}
\end{figure}


We collect \dataset by crowd-sourcing on Amazon Mechanical Turk in two phases.
First, a worker paraphrases a generated question for a table.
We form the generated question using a template, filled using a randomly generated SQL query.
We ensure the validity and complexity of the tables by keeping only those that are legitimate database tables and sufficiently large in the number of rows and columns.
Next, two other workers verify that the paraphrase has the same meaning as the generated question.
We discard paraphrases that do not show enough variation, as measured by the character edit distance from the generated question, as well as those both workers deemed incorrect during verification.
Section~\ref{section:data_collection} of the Appendix contains more details on the collection of \dataset.
We make available examples of the interface used during the paraphrase phase
and during the verification phase
in the supplementary materials.
The dataset is available for download at \url{https://github.com/salesforce/WikiSQL}.



\label{section:execution_engine}
The tables, their paraphrases, and SQL queries are randomly slotted into train, dev, and test splits, such that each table is present in exactly one split. In addition to the raw tables, queries, results, and natural utterances, we also release a corresponding SQL database and query execution engine.


\subsection{Evaluation}
\label{section:evaluation}
\vspace{-2mm}

Let $N$ denote the total number of examples in the dataset,
$N_{\rm ex}$ the number of queries that, when executed, result in the correct result, and 
$N_{\rm lf}$ the number of queries has exact string match with the ground truth query used to collect the paraphrase.
We evaluate using the execution accuracy metric $\accex = \frac{N_{\rm ex}}{N}$.
One downside of $\accex$ is that it is possible to construct a SQL query that does not correspond to the question but nevertheless obtains the same result.
For example, the two queries \texttt{SELECT COUNT(name) WHERE SSN = 123} and \texttt{SELECT COUNT(SSN) WHERE SSN = 123} produce the same result if no two people with different names share the SSN 123.
Hence, we also use the logical form accuracy $\acclo = \frac{N_{\rm lf}}{N}$.
However, as we showed in Section~\ref{section:rl}, $\acclo$ incorrectly penalizes queries that achieve the correct result but do not have exact string match with the ground truth query.
Due to these observations, we use both metrics to evaluate the models.


\section{Experiments}
\vspace{-2mm}

We tokenize the dataset using Stanford CoreNLP~\citep{corenlp}.
We use the normalized tokens for training and revert into original gloss before outputting the query so that generated queries are executable on the database.
We use fixed GloVe word embeddings~\citep{Pennington2014GloveGV} and character n-gram embeddings~\citep{arXiv-2016-HASHIMOTO-jmt}.
Let $w^{\rm g}_x$ denote the GloVe embedding and $w^{\rm c}_x$ the character embedding for word $x$.
Here, $w^{\rm c}_x$ is the mean of the embeddings of all the character n-grams in $x$.
For words that have neither word nor character embeddings, we assign the zero vector.
All networks are run for a maximum of 300 epochs with early stopping on dev split execution accuracy.
We train using ADAM~\citep{Kingma2014AdamAM} and regularize using dropout~\citep{Srivastava2014DropoutAS}.
All recurrent layers have a hidden size of 200 units and are followed by a dropout of 0.3.
We implement all models using PyTorch~\footnote{\url{https://pytorch.org}}.
To train \model, we first train a version in which the \texttt{WHERE} clause is supervised via teacher forcing (i.e. the policy is not learned from scratch) and then continue training using reinforcement learning.
In order to obtain the rewards described in Section~\ref{section:rl}, we use the query execution engine described in Section~\ref{section:execution_engine}.



\subsection{Result}
\vspace{-2mm}

We compare results against the attentional sequence to sequence neural semantic parser proposed by~\citet{dong-neural_semantic_parsing}.
This model achieves state of the art results on a variety of semantic parsing datasets, outperforming a host of non-neural semantic parsers despite not using hand-engineered grammars.
To make this baseline even more competitive on our new dataset, we augment their input with the table schema such that the model can generalize to new tables.
We describe this baseline in detail in Section 2 of the Appendix.
Table~\ref{table:model-results} compares the performance of the three models.

Reducing the output space by utilizing the augmented pointer network improves upon the baseline by \perfdiffpointervsseqtoseq.
Leveraging the structure of SQL queries leads to another improvement of \perfdiffoursnorlvspointer, as is shown by the performance of \model without RL compared to the augmented pointer network. Finally, training using reinforcement learning based on rewards from in-the-loop query executions on a database leads to another performance increase of \perfdiffoursvsoursnorl, as is shown by the performance of the full \model model.



\subsection{Analysis}
\vspace{-2mm}

\paragraph{Limiting the output space via pointer network leads to more accurate conditions.}
%rs: This paragraph seems to better titled: "Pointers are helpful" based on the examples 
Compared to the baseline, the augmented pointer network generates higher quality \texttt{WHERE} clause.
For example, for ``in how many districts was a successor seated on march 4, 1850?'', the baseline generates the condition \texttt{successor seated = seated march 4} whereas \model generates \texttt{successor seated = seated march 4 1850}.
Similarly, for ``what's doug battaglia's pick number?'', the baseline generates \texttt{Player = doug} whereas \model generates \texttt{Player = doug battaglia}.
%rs: These two examples are repetitive and could be shortened/ one is also enough probably.
The conditions tend to contain rare words (e.g. ``1850''), but the baseline is inclined to produce common words in the training corpus, such as ``march'' and ``4'' for date, or ``doug'' for name.
The pointer is less affected since it selects exclusively from the input.

\begin{table}[t!]
\vspace{-3mm}
\centering
\begin{tabular}{lllll}
\toprule
Model                & Dev $\acclo$ & Dev $\accex$ & Test $\acclo$ & Test $\accex$ \\
\midrule              
Baseline~\citep{dong-neural_semantic_parsing}  & \perflaseqtoseqdev & \perfeaseqtoseqdev & \perflaseqtoseq & \perfeaseqtoseq   \\
Aug Ptr Network & \perflapointerdev & \perfeapointerdev & \perflapointer & \perfeapointer     \\
\midrule
\model (no RL)       & \perflaoursnorldev & \perfeaoursnorldev & \perflaoursnorl & \perfeaoursnorl   \\
\textbf{\model}      & \textbf{\perflaoursdev} & \textbf{\perfeaoursdev}  & \textbf{\perflaours} & \textbf{\perfeaours}       \\
\bottomrule
\end{tabular}
\vspace{1mm}
\caption{
Performance on \dataset.
Both metrics are defined in Section~\ref{section:evaluation}.
For \model (no RL), the \texttt{WHERE} clause is supervised via teacher forcing as opposed to reinforcement learning.
}
\vspace{-7mm}
\label{table:model-results}
\end{table}

\begin{wraptable}[6]{R}{0.52\textwidth}
\vspace{-5mm}
\centering
\begin{tabular}{llll}
\toprule
Model   & Precision & Recall & F1   \\
\midrule
Aug Ptr Network & 66.3\%      & 64.4\%   & 65.4\% \\
\model & 72.6\%  & 66.2\%   & 69.2\% \\
\bottomrule
\end{tabular}
\caption{
Performance on the \texttt{COUNT} operator.
%rs TODO unclear what exactly the aug. pointer refers to in this table. used "network" in previous table. at least gotta stay consistent.
}
\label{table:count_stats}
\end{wraptable}


%rs: What structure? what exactly is the model change? maybe simplify to "adding column names to potential pointer inputs" ? i would try to refer to equations or at least subsections of the model when giving these numbers.
\paragraph{Incorporating structure reduces invalid queries.}
Seq2SQL without RL directly predicts selection and aggregation and reduces invalid SQL queries generated from 7.9\% to 4.8\%.
A large quantity of invalid queries result from column names --
the generated query refers to selection columns that are not present in the table.
This is particularly helpful when the column name contain many tokens, such as ``Miles (km)'', which has 4 tokens.
Introducing a classifier for the aggregation also reduces the error rate.
Table~\ref{table:count_stats} shows that adding the aggregation classifier improves the precision, recall, and F1 for predicting the \texttt{COUNT} operator.
For more queries produced by the different models, please see Section 3 of the Appendix.

\paragraph{RL generates higher quality \texttt{WHERE} clause that are ordered differently than ground truth.}
Training with policy-based RL obtains correct results in which the order of conditions is differs from the ground truth query.
For example, for ``in what district was the democratic candidate first elected in 1992?'', the ground truth conditions are \texttt{First elected = 1992 AND Party = Democratic} whereas \model generates \texttt{Party = Democratic AND First elected = 1992}.
When \model is correct and \model without RL is not, the latter tends to produce an incorrect \texttt{WHERE} clause.
For example, for the rather complex question ``what is the race name of the 12th round trenton, new jersey race where a.j. foyt had the pole position?'',
\model trained without RL generates
\texttt{WHERE rnd = 12 and track = a.j. foyt AND pole position = a.j. foyt}
whereas \model trained with RL correctly generates 
\texttt{WHERE rnd = 12 AND pole position = a.j. foyt}.


%\textbf{Difficult table schema present a challenge.}
%In numerous HTML tables, the column names do not accurately portray the column contents and hence confuse the model.

%For example, in one of the tables in the dev set, the number of voters from the Bronx is listed under a column title ``the bronx''.
%When asked the question ``how many voters from the bronx voted for the socialist party'', the model mistakenly generate the aggregation operator \texttt{COUNT} due to the phrase ``how many''.
%However, due to the way this table is organized, the number of voters are actually listed in the column and the \texttt{COUNT} operation is superfluous.
%Another difficult example is the question ``in what year did a plymouth vehicle win on february 9?'', because the model does not understand that ``plymouth'' refers to a manufacturer of vehicles.

\section{Related Work}
\vspace{-2mm}

\paragraph{Semantic Parsing.}
In semantic parsing for question answering (QA), natural language questions are parsed into logical forms that are then executed on a knowledge graph~\citep{zelle1996LearningTP,wong2007LearningSG,zettlemoyer2005LearningTM,Zettlemoyer2007OnlineLO}.
Other works in semantic parsing focus on learning parsers without relying on annotated logical forms by leveraging conversational logs~\citep{Artzi2011BootstrappingSP}, demonstrations~\citep{Artzi2013WeaklySL}, distant supervision~\citep{cai2013LargescaleSP,Reddy2014LargescaleSP}, and question-answer pairs~\citep{Liang2011LearningDC}.
Semantic parsing systems are typically constrained to a single schema and require hand-curated grammars to perform well\footnote{For simplicity, we define table schema as the names of the columns in the table.}.
~\citet{Pasupat2015CompositionalSP} addresses the single-schema limitation by proposing the floating parser, which generalizes to unseen web tables on the WikiTableQuestions task.
Our approach is similar in that it generalizes to new table schema.
However, we do not require access to table content, conversion of table to an additional graph, nor hand-engineered features/grammar.

\vspace{-2mm}
\paragraph{Semantic parsing datasets.}
Previous semantic parsing systems were designed to answer complex and compositional questions over closed-domain, fixed-schema datasets such as GeoQuery~\citep{Tang2001UsingMC} and ATIS~\citep{Price1990ATIS}.
Researchers also investigated QA over subsets of large-scale knowledge graphs such as DBPedia~\citep{Starc2017JointLO} and Freebase~\citep{cai2013LargescaleSP,berant2013SemanticPO}.
The dataset ``Overnight'' \citep{Wang2015BuildingAS} uses a similar crowd-sourcing process to build a dataset of natural language question, logical form pairs, but has only 8 domains.
WikiTableQuestions ~\citep{Pasupat2015CompositionalSP} is a collection of question and answers, also over a large quantity of tables extracted from Wikipedia.
However, it does not provide logical forms whereas \dataset does.
In addition, \dataset focuses on generating SQL queries for questions over relational database tables and only uses table content during evaluation.

\vspace{-2mm}
\paragraph{Representation learning for sequence generation.}
\citet{dong-neural_semantic_parsing}'s attentional sequence to sequence neural semantic parser, which we use as the baseline, achieves state-of-the-art results on a variety of semantic parsing datasets despite not utilizing hand-engineered grammar.
Unlike their model, \model uses pointer based generation akin to~\citet{vinyals-pointer_networks} to achieve higher performance, especially in generating queries with rare words and column names.
Pointer models have also been successfully applied to tasks such as language modeling~\citep{Merity2017}, summarization~\citep{gu-copying_in_seq2seq}, combinatorial optimization~\citep{bello-neural_combinatorial_optimization}, and question answering~\citep{seo-bidirectional_attention_flow,xiong-dynamic_coattention}.
Other interesting neural semantic parsing models are the Neural Programmer~\citep{neelakantan2016LearningAN} and the Neural Enquirer~\citep{YinLLK15}.
~\citet{mou2016coupling} proposed a distributed neural executor based on the Neural Enquirer, which efficiently executes queries and incorporates execution rewards in reinforcement learning.
Our approach is different in that we do not access table content, which may be unavailable due to privacy concerns.
We also do not hand-engineer model architecture for query execution and instead leverage existing database engines to perform efficient query execution.
Furthermore, in contrast to~\citet{dong-neural_semantic_parsing} and~\citet{neelakantan2016LearningAN}, we use policy-based RL in a fashion similar to~\citet{ChenNeuralSymbolic}, ~\citet{mou2016coupling}, and~\citet{GuuPLL17}, which helps \model achieve state-of-the-art performance.
Unlike~\citet{mou2016coupling} and~\citet{YinLLK15}, we generalize across natural language questions and table schemas instead of across synthetic questions on a single schema.


\vspace{-2mm}
\paragraph{Natural language interface for databases.}
One prominent works in natural language interfaces is PRECISE~\citep{popescu-nli}, which translates questions to SQL queries and identifies questions that it is not confident about.
~\citet{Giordani2012TranslatingQT} translate questions to SQL by first generating candidate queries from a grammar then ranking them using tree kernels.
Both of these approaches rely on high quality grammar and are not suitable for tasks that require generalization to new schema.
~\citet{Iyer2017NeuralSemanticUserFeedback} also translate to SQL, but with a Seq2Seq model that is further improved with human feedback.
\model outperforms Seq2Seq and uses reinforcement learning instead of human feedback during training.


\vspace{-1mm}
\section{Conclusion}
\vspace{-2mm}
We proposed \model, a deep neural network for translating questions to SQL queries.
Our model leverages the structure of SQL queries to reduce the output space of the model.
To train \model, we applied in-the-loop query execution to learn a policy for generating the conditions of the SQL query, which is unordered and unsuitable for optimization via cross entropy loss.
We also introduced \dataset, a dataset of questions and SQL queries that is an order of magnitude larger than comparable datasets.
Finally, we showed that \model outperforms a state-of-the-art semantic parser on \dataset, improving execution accuracy from \perfeaseqtoseq to \perfeaours and logical form accuracy from \perflaseqtoseq to \perflaours.



\bibliography{iclr2018_conference}
\bibliographystyle{iclr2018_conference}


\appendix
\include{appendix}

\end{document}
