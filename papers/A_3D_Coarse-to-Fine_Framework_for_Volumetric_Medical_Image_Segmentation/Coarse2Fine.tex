\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{3dv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

%%%% Add other packages here
\usepackage{epsfig}
\usepackage{array}
\usepackage{longtable}
\usepackage{booktabs}
\usepackage{bm}
\usepackage{gensymb}
\usepackage{mathtools}
\usepackage{subfigure}
\usepackage{spverbatim}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{booktabs}
\usepackage{float}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}


\threedvfinalcopy % *** Uncomment this line for the final submission

\def\threedvPaperID{4} % *** Enter the 3DV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ifthreedvfinal\pagestyle{empty}\fi
\begin{document}

%%%%%%%%% TITLE
% a more general name?
\title{A 3D Coarse-to-Fine Framework for Volumetric Medical Image Segmentation}

\author{Zhuotun Zhu\textsuperscript{1}, Yingda Xia\textsuperscript{1},
Wei Shen\textsuperscript{1,2}, Elliot K. Fishman\textsuperscript{3}, Alan L. Yuille\textsuperscript{1} \\
\textsuperscript{1}Johns Hopkins University, 
\textsuperscript{2}Shanghai University, 
\textsuperscript{3}Johns Hopkins University School of Medicine\\
{\tt\small \{ztzhu, yxia25, ayuille1\}@jhu.edu}\quad
{\tt\small wei.shen@t.shu.edu.cn}\quad
{\tt\small efishman@jhmi.edu}\quad \\
}

\maketitle
% \thispagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}
	In this paper, we adopt 3D Convolutional Neural Networks to segment volumetric medical images. Although deep neural networks have been proven to be very effective on many 2D vision tasks, it is still challenging to apply them to 3D tasks due to the limited amount of annotated 3D data and limited computational resources. We propose a novel 3D-based coarse-to-fine framework to {\bf\textit{effectively}} and {\bf\textit{efficiently}} tackle these challenges. The proposed 3D-based framework outperforms the 2D counterpart to a large margin since it can leverage the rich spatial information along all three axes. We conduct experiments on two datasets which include healthy and pathological pancreases respectively, and achieve the current state-of-the-art in terms of Dice-S{\o}rensen Coefficient (DSC). On the NIH pancreas segmentation dataset, we outperform the previous best by an average of over $2\%$, and the worst case is improved by $7\%$ to reach almost $70\%$, which indicates the reliability of our framework in clinical applications.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
Driven by the huge demands for computer-aided diagnosis systems, automatic organ segmentation from medical images, such as computed tomography (CT) and magnetic resonance imaging (MRI), has become an active research topic in both the medical image processing and computer vision communities. It is a prerequisite step for many clinical applications, such as diabetes inspection, organic cancer diagnosis, and surgical planning. Therefore, it is well worth exploring automatic segmentation systems to accelerate the computer-aided diagnosis in medical image analysis.


In this paper, we focus on pancreas segmentation from CT scans, one of the most challenging organ segmentation problems~\cite{zhou2017fixed}\cite{roth2015deeporgan}. As shown in Fig.~\ref{fig:CTPancreas}, the main difficulties stem from three aspects: 1) the small size of the pancreas in the whole abdominal CT volume; 2) the large variations in texture, location, shape and size of the pancreas;  3) the abnormalities, like pancreatic cysts, can alter the appearance of pancreases a lot.


Following the rapid development of deep neural networks~\cite{KrizhevskySH12}\cite{SimonyanZ14a} and their successes in many computer vision tasks, such as semantic segmentation~\cite{LongSD15}\cite{ChenPKMY17}, edge detection~\cite{ShenWWBZ15}\cite{XieT15}\cite{ShenWJWY17} and 3D shape retrieval~\cite{zhu2016deep}\cite{fang20153d}, many deep learning based methods have been proposed for pancreas segmentation and achieved considerable progress~\cite{zhou2017fixed}\cite{roth2015deeporgan}\cite{roth2016spatial}. However, these methods are based on 2D fully convolutional networks (FCNs)~\cite{LongSD15}, which perform segmentation slice by slice while CT volumes are indeed 3D data. Although these 2D methods use strategies to fuse the output from different 2D views to obtain 3D segmentation results, they inevitably lose some 3D context, which is important for capturing the discriminative features of the pancreas with respect to background regions.

\begin{figure}[t]
\centering
\includegraphics[width=1\linewidth]{figures/IllusCTPancreas3.pdf}\\
   \caption{An illustration of normal pancreases on NIH dataset~\cite{roth2015deeporgan} and abnormal cystic pancreases on JHMI dataset~\cite{zhou2017deep} shown in the first and second row respectively. Normal pancreas regions are masked as red and abnormal pancreas regions are marked as blue. The pancreas usually occupies a small region in a whole CT scan. Best viewed in color.}
\label{fig:CTPancreas}
\end{figure}

Using 3D deep networks for organ segmentation is a recent trend but not yet applied to the pancreas.
An obstacle is that training 3D deep networks suffers from the ``out of memory" problem. 2D FCNs can accept a whole 2D slice as input, but 3D FCNs cannot be fed a whole 3D volume due to the limited GPU memory size. A common solution is to train 3D FCNs from small sub-volumes and test them in a sliding-window manner~\cite{milletari2016v}\cite{bui20173d}\cite{cciccek20163d}\cite{chen2017voxresnet}\cite{yu2017automatic}, \emph{i.e.}, performing 3D segmentation on densely and uniformly sampled sub-volumes one by one. Usually, these neighboring sampled sub-volumes overlap with each other to improve the robustness of the final 3D results. It is worth noting that the overlap size is a trade-off between the segmentation accuracy and the time cost. Setting a larger/smaller overlap size generally leads to a better/worse segmentation accuracy but takes more/less time during testing.

To address these issues, we propose a concise and effective framework based on 3D deep networks for pancreas segmentation, which can simultaneously achieve high segmentation accuracy and low time cost. Our framework is formulated in a coarse-to-fine manner. In the training stage, we first train a 3D FCN from the sub-volumes sampled from an entire CT volume. We call this {\bf\textit{ResDSN Coarse}}
 model, which aims to obtain the rough location of the target pancreas from the whole CT volume by making full use of the overall 3D context. Then, we train another 3D FCN from the sub-volumes sampled only from the ground truth bounding boxes of the target pancreas. We call this the {\bf\textit{ResDSN Fine}} model, which can refine the segmentation based on the coarse result. In the testing stage, we first apply the coarse model in the sliding-window manner to a whole CT volume to extract the most probable location of the pancreas. Since we only need a rough location for the target pancreas in this step, the overlap size is set to a small value. Afterwards, we apply the fine model in the sliding-window manner to the coarse pancreas region, but by setting a larger overlap size. Thus, we can efficiently obtain a fine segmentation result and we call the coarse-to-fine framework by {\bf\textit{ResDSN C2F}}. 
 
Note that, the meaning of ``coarse-to-fine'' in our framework is twofold. First, it means the input region of interests (RoIs) for {\bf\textit{ResDSN Coarse}} model and {\bf\textit{ResDSN Fine}} model are different, i.e., a whole CT volume for the former one and a rough bounding box of the target pancreas for the latter one. We refer to this as coarse-to-fine RoIs, which is designed to achieve better segmentation performance. The coarse step removes a large amount of the unrelated background region, then with a relatively smaller region to be sampled as input, the fine step can much more easily learn cues which distinguish the pancreas from the local background, i.e., exploit local context which makes it easier to obtain a more accurate segmentation result. Second, it means the overlap sizes used for {\bf\textit{ResDSN Coarse}} model and {\bf\textit{ResDSN Fine}} model during inference are different, i.e., small and large overlap sizes for them, respectively. We refer to this as coarse-to-fine overlap sizes, which is designed for efficient 3D inference.

To our best knowledge, we are one of the first studies to segment the challenging {\bf\textit{normal}} and {\bf\textit{abnormal}} pancreases using 3D networks which leverage the rich spatial information. The effectiveness and efficiency of the proposed 3D coarse-to-fine framework are demonstrated on two pancreas segmentation datasets where we achieve the state-of-the-art with relative low time cost. It is worth mentioning that, although our focus is pancrease segmentation, our framework is generic and can be directly applied to segmenting other medical organs. 

\begin{figure*}[t]
  \centering
  \includegraphics[width=1\linewidth]{figures/ResDSNIntroTesting.pdf}\\
   \caption{Flowchart of the proposed 3D coarse-to-fine segmentation system in the testing phase. We first apply ``ResDSN Coarse" with a small overlapped sliding window to obtain a rough pancreas region and then use the ``ResDSN Fine" model to refine the results with a large overlapped sliding window.
   Best viewed in color.}
\label{fig:Intro}
\end{figure*}

\section{Related Work}
The medical image analysis community is facing a revolution brought by the fast development of deep networks~\cite{KrizhevskySH12}\cite{SimonyanZ14a}. Deep Convolutional Neural Networks (CNNs) based methods have dominated the research area of volumetric medical image segmentation in the last few years. Generally speaking, CNN-based methods for volumetric medical image segmentation can be divided into two major categories: 2D CNNs based and 3D CNNs based. 

\subsection{2D CNNs for Volumetric Segmentation}
2D CNNs based methods~\cite{roth2015deeporgan}\cite{roth2016spatial}\cite{HavaeiDWBCBPJL17}\cite{MoeskopsWVGLVI17}\cite{ronneberger2015u} performed volumetric segmentation slice by slice from different views, and then fused the 2D segmentation results to obtain a 3D Volumetric Segmentation result. In the early stage, the 2D segmentation based models were trained from image patches and tested in a patch by patch manner~\cite{roth2015deeporgan}, which is time consuming. Since the introduction of fully convolution networks (FCNs)~\cite{LongSD15}, almost all the 2D segmentation methods are built upon 2D FCNs to perform holistic slice segmentation during both training and testing. Havaei \textit{et al}~\cite{HavaeiDWBCBPJL17} proposed a two-pathway FCN architecture, which exploited both local features as well as more global contextual features simultaneously by the two pathways. Roth \textit{et al}~\cite{roth2016spatial} performed Pancreas segmentation by a holistic learning approach, which first segment pancreas regions by holistically-nested networks~\cite{XieT15} and then refine them by the boundary maps obtained by robust spatial aggregation using random forest. The U-Net~\cite{ronneberger2015u} is one of the most popular FCN architectures for medical image segmentation, which is a encoder-decoder network, but with additional short connection between encoder and decoder paths. Based on the fact that a pancreas only takes up a small fraction of the whole scan, Zhou \emph{et al.}~\cite{zhou2017fixed} proposed to find the rough pancreas region and then learn a FCN based fixed-point model to refine the pancreas region iteratively. Their method is also based on a coarse-to-fine framework, but it only considered coarse-to-fine RoIs. Besides coarse-to-fine RoIs, our coarse-to-fine method also takes coarse-to-fine overlap sizes into account, which is designed specifically for efficient 3D inference. 

\subsection{3D CNNs for Volumetric Segmentation}
Although 2D CNNs based methods achieved considerable progresses, they are not optimal for medical image segmentation, as they cannot make full use of the 3D context encoded in volumetric data. Several 3D CNNs based segmentation methods have been proposed. The 3D U-Net~\cite{cciccek20163d} extended the previous 2D U-Net architecture~\cite{ronneberger2015u} by replacing all 2D operations with their 3D counterparts. Based on the architecture of the 3D U-Net, the V-Net~\cite{milletari2016v} introduced residual structures~\cite{he2016deep} (short term skip connection) into each stage of the network. Chen \textit{et al}~\cite{chen2017voxresnet} proposed a deep voxel-wise residual network for 3D brain segmentation. Both I2I-3D~\cite{merkow2016dense} and 3D-DSN~\cite{dou20173d} included auxiliary supervision via side outputs into their 3D deep networks. Despite the success of 3D CNNs as a technique for segmenting the target organs, such as prostate~\cite{milletari2016v} and kidney~\cite{cciccek20163d}, very few techniques have been developed for leveraging 3D spatial information on the challenging pancreas segmentation. Gibson \emph{et al.}~\cite{gibson2018automatic} proposed the DenseVNet which is however constrained to have shallow encoders due to the computationally demanding dense connections. Roth \emph{et al.}~\cite{roth2018towards} extended 3D U-Net to segment pancreas, which has the following shortcomings, 1) the input of their networks is fixed to $120\times120\times120$, which is very computationally demanding due to this large volume size, 2) the rough pancreas bounding box is resampled to a fixed size as their networks input, which loses information and flexibility, and cannot deal with the intrinsic large variations of pancreas in shape and size. Therefore, we propose our 3D coarse-to-fine framework that works on both normal and abnormal to ensure both low computation cost and high pancreas segmentation accuracy.


\section{Method}

In this section, we elaborate our proposed 3D coarse-to-fine framework which includes a \textit{\textbf{coarse}} stage and a \textit{\textbf{fine}} stage afterwards. We first formulate a segmentation model that can be generalized to both \textit{\textbf{coarse}} stage and \textit{\textbf{fine}} stage. Later in Sec.~\ref{Sec:CoarseStage} and Sec.~\ref{Sec:FineStage}, we will customize the segmentation model to these two stages, separately.

We denote a 3D CT-scan volume by $\mathbf{X}$. This is associated with a human-labeled per-voxel annotation $\mathbf{Y}$, where both $\mathbf{X}$ and $\mathbf{Y}$ have size $W\times H\times D$, which corresponds to axial, sagittal and coronal views, separately. The ground-truth segmentation mask $\mathbf{Y}$ has a binary value $y_i$, $i = 1, \cdots, WHD$, at each spatial location $i$ where $y_i = 1$ indicates that $x_i$ is a pancreas voxel. 
Denote a segmentation model by $\mathbb{M}: \mathbf{P} = {\mathbf{f}\!\left(\mathbf{X}; \boldsymbol{\Theta}\right)}$, where $\boldsymbol{\Theta}$ indicates model parameters and $\mathbf{P}$ means the binary prediction volume. Specifically in a neural network with $L$ layers and parameters $\boldsymbol{\Theta} = \{\mathcal{W}, \mathcal{B}\}$, $\mathcal{W}$ is a set of weights and $\mathcal{B}$ is a set of biases, where $\mathcal{W} = \{\mathbf{W}^1, \mathbf{W}^2, \cdots, \mathbf{W}^L\}$ and $\mathcal{B} = \{\mathbf{B}^1, \mathbf{B}^2, \cdots, \mathbf{B}^L\}$. Given that $p(y_i | x_i; \boldsymbol{\Theta})$ represents the predicted probability of a voxel $x_i$ being what is the labeled class at the final layer of the output, the negative log-likelihood loss can be formulated as:

\begin{equation}\label{Eq:SoftmaxLoss}
\mathcal{L} = \mathcal{L}(\mathbf{X}; \boldsymbol{\Theta}) = -\sum_{x_i\in \mathbf{X}}{\log(p(y_i | x_i; \boldsymbol{\Theta}))}.
\end{equation}
It is also known as the cross entropy loss in our binary segmentation setting. By thresholding $p(y_i | x_i; \boldsymbol{\Theta})$, we can obtain the binary segmentation mask $\mathbf{P}$.

We also add some auxiliary layers to such a neural network (will be called mainstream network in the rest of the paper), which produces side outputs under deep supervision~\cite{lee2015deeply}. These auxiliary layers form a branch network and facilitate the feature learning at lower layer of the mainstream network. Each branch network shares the weights of the first $d$ layers from the mainstream network, which is denoted by $\boldsymbol{\Theta}_d = \{\mathcal{W}_d , \mathcal{B}_d\}$. Apart from the shared weights, it owns weights $\widehat{\boldsymbol{\Theta}}_d$ to output the per-voxel prediction. Similarly, the loss of an auxiliary network can be formulated as:
% {\small}
\begin{equation}\label{Eq:SoftmaxLoss}
\mathcal{L}_d(\mathbf{X}; \boldsymbol{\Theta}_d, \widehat{\boldsymbol{\Theta}}_d) = \sum_{x_i\in \mathbf{X}}{-\log(p(y_i | x_i; \boldsymbol{\Theta}_d, \widehat{\boldsymbol{\Theta}}_d))},
\end{equation}
which is abbreviated as $\mathcal{L}_d$. Finally, stochastic gradient descent is applied to minimize the negative log-likelihood, which is given by following regularized objective function:
\begin{equation}\label{Eq:OverallObj}
\mathcal{L}_{overall} = \mathcal{L} + \sum_{d\in \mathcal{D}}\xi_d\mathcal{L}_d + \lambda ({\Vert \boldsymbol{\Theta} \Vert}^2 + \sum_{d\in \mathcal{D}}{\Vert \widehat{\boldsymbol{\Theta}}_d \Vert)}^2,
\end{equation}
where $\mathcal{D}$ is a set of branch networks for auxiliary supervisions, $\xi_d$ balances the importance of each auxiliary network and $l_2$ regularization is added to the objective to prevent the networks from overfitting. For conciseness concerns in the following sections, we keep a segmentation model that is obtained from the overall function described in Eq.~\ref{Eq:OverallObj} denoted by $\mathbb{M}: \mathbf{P} = {\mathbf{f}\!\left(\mathbf{X}; \boldsymbol{\Theta}\right)}$, where $\boldsymbol{\Theta}$ includes parameters of the mainstream network and auxiliary networks.


\subsection{Coarse Stage}\label{Sec:CoarseStage}
In the \textit{\textbf{coarse}} stage, the input of ``ResDSN Coarse" is sampled from the whole CT-scan volume denoted by $\mathbf{X}^\textrm{C}$, on which the \textit{\textbf{coarse}} segmentation model $\mathbb{M}^\textrm{C}: \mathbf{P}^\textrm{C} = {\mathbf{f}^\textrm{C}\!\left(\mathbf{X}^\textrm{C}; \boldsymbol{\Theta}^\textrm{C}\right)}$ is trained on. All the $\textrm{C}$ superscripts depict the \textit{\textbf{coarse}} stage. The goal of this stage is to efficiently produce a rough binary segmentation $\mathbf{P}^\textrm{C}$ from the complex background, which can get rid of regions that are segmented as non-pancreas with a high confidence to obtain an approximate pancreas volume. Based on this approximate pancreas volume, we can crop from the original input $\mathbf{X}^\textrm{C}$ with a rectangular cube derived from $\mathbf{P}^\textrm{C}$ to obtain a smaller 3D image space $\mathbf{X}^\textrm{F}$, which is surrounded by simplified and less variable context compared with $\mathbf{X}^\textrm{C}$. The mathematic definition of $\mathbf{X}^\textrm{F}$ is formulated as:
\begin{equation}\label{Eq:CoarseSegmentation}
\mathbf{X}^\textrm{F} = \textrm{Crop}[\mathbf{X}^\textrm{C}\otimes\mathbf{P}^\textrm{C}; \mathbf{P}^\textrm{C}, m],
\end{equation}
where $\otimes$ means an element-wise product. The function $\textrm{Crop}[\mathbf{X};\mathbf{P}, m]$ denotes cropping $\mathbf{X}$ via a rectangular cube that covers all the $1$'s voxels of a binary volume $\mathbf{P}$ added by a padding margin $m$ along three axes. Given $\mathbf{P}$, the functional constraint imposed on $\mathbf{X}$ is that they have exactly the same dimensionality in 3D space. The padding parameter $m$ is empirically determined in experiments, where it is used to better segment the boundary voxels of pancreas during the \textit{\textbf{fine}} stage. The $\textrm{Crop}$ operation acts as a dimensionality reduction to facilitate the fine segmentation, which is crucial to cut down the consuming time of segmentation. It is well-worth noting that the 3D locations of the rectangular cube which specifies where to crop $\mathbf{X}^\textrm{F}$ from $\mathbf{X}^\textrm{C}$ is recorded to map the \textit{\textbf{fine}} segmentation results back their positions in the full CT scan.

\subsection{Fine Stage}\label{Sec:FineStage}
In the \textit{\textbf{fine}} stage, the input of the ConvNet is sampled from the cropped volume $\mathbf{X}^\textrm{F}$, on which we train the \textit{\textbf{fine}} segmentation model $\mathbb{M}^\textrm{F}: \mathbf{P}^\textrm{F} = {\mathbf{f}^\textrm{F}\!\left(\mathbf{X}^\textrm{F}; \boldsymbol{\Theta}^\textrm{F}\right)}$, where the $\textrm{F}$ superscripts indicate the \textit{\textbf{fine}} stage. The goal of this stage is to refine the coarse segmentation results from previous stage. In practice, $\mathbf{P}^\textrm{F}$ has the same volumetric size of $\mathbf{X}^\textrm{F}$, which is smaller than the original size of $\mathbf{X}^\textrm{C}$. 

\subsection{Coarse-to-Fine Segmentation}\label{Sec:Coarse2Fine}
Our segmentation task is to give a volumetric prediction on every voxel of $\mathbf{X}^\textrm{C}$, so we need to map the $\mathbf{P}^\textrm{F}$ back to exactly the same size of $\mathbf{X}^\textrm{C}$ given by:
\begin{equation}\label{Eq:C2FSegmentation}
\mathbf{P}^\textrm{C2F} = \textrm{DeCrop}[\mathbf{P}^\textrm{F}\odot\mathbf{P}^\textrm{C}; \mathbf{X}^\textrm{F}, \mathbf{X}^\textrm{C}],
\end{equation}
where $\mathbf{P}^\textrm{C2F}$ denotes the final volumetric segmentation, and $\odot$ means an element-wise replacement, and $\textrm{DeCrop}$ operation defined on $\mathbf{P}^\textrm{F}, \mathbf{P}^\textrm{C}, \mathbf{X}^\textrm{F} \textrm{ and } \mathbf{X}^\textrm{C}$ is to replace a pre-defined rectangular cube inside $\mathbf{P}^\textrm{C}$ by $\mathbf{P}^\textrm{F}$, where the replacement locations are given by the definition of cropping $\mathbf{X}^\textrm{F}$ from $\mathbf{X}^\textrm{C}$ given in Eq.~\ref{Eq:CoarseSegmentation}.

All in all, our entire 3D-based coarse-to-fine segmentation framework during testing is illustrated in Fig~\ref{fig:Intro}. 

\begin{figure*}
  \centering
  \includegraphics[width=1\linewidth]{figures/NetworkStructure5.pdf}\\
  \caption{Illustration of our 3D convolutional neural network for volumetric segmentation. The encoder path is composed from ``Conv1a" to ``Conv4b" while the decoder path is from ``DeConv3a" to ``Res/Conv1b". Each convolution or deconvolution layer consists of one convolution followed by a BatchNorm and a ReLU. To clarify, ``Conv1a, 32, $3\times 3 \times 3$" means the convolution operation with $32$ channels and a kernel size of $3\times 3 \times 3$. ``Pooling 1, max, $2$" means the max pooling operation with kernel size of $2\times 2 \times 2$ and a stride of two. Long residual connections are illustrated by the blue concrete lines. Blocks with same color mean the same operations. Best viewed in color.}\label{fig:ResDSN}
\end{figure*}

\subsection{Network Architecture}\label{Sec:NetworkArchietcture}
As shown in Fig.~\ref{fig:ResDSN}, we provide an illustration of our convolutional network architecture. Inspired by V-Net~\cite{milletari2016v}, 3D U-Net~\cite{cciccek20163d}, and VoxResNet~\cite{chen2017voxresnet}, we have an encoder path followed by a decoder path each with four resolution steps. The left part of network acts as a feature extractor to learn higher and higher level of representations while the right part of network decompresses compact features into finer and finer resolution to predict the per-voxel segmentation. The padding and stride of each layer (Conv, Pooling, DeConv) are carefully designed to make sure the densely predicted output is the same size as the input.

The encoder sub-network on the left is divided into different steps that work on different resolutions. Each step consists of one to two convolutions, where each convolution is composed of $3\times 3\times 3$ convolution followed by a batch normalization (BN~\cite{ioffe2015batch}) and a rectified linear unit (ReLU~\cite{nair2010rectified}) to reach better convergence, and then a max pooling layer with a kernel size of $2\times 2\times 2$ and strides of two to reduce resolutions and learn more compact features. The downsampling operation implemented by max-pooling can reduce the size of the intermediate feature maps while increasing the size of the receptive fields. Having fewer size of activations makes it possible to double the number of channels during feature aggregation given the limited computational resource. 

The decoder sub-network on the right is composed of several steps that operate on different resolutions as well. Each step has two convolutions with each one followed by a BatchNorm and a ReLU, and afterwards a Deconvolution with a kernel size of $4\times 4 \times 4$ and strides of two is connected to expand the feature maps and finally predict the segmentation mask at the last layer. The upsampling operation that is carried out by deconvolution enlarges the resolution between each step, which increases the size of the intermediate activations so that we need to halve the number of channels due to the limited memory of the GPU card.

Apart from the left and right sub-networks, we impose a residual connection~\cite{he2016deep} to bridge short-cut connections of features between low-level layers and high-level layers. During the forward phase, the low-level cues extracted by networks are directly added to the high-level cues, which can help elaborate the fine-scaled segmentation, \emph{e.g.}, small parts close to the boundary which may be ignored during the feature aggregation due to the large size of receptive field at high-level layers. 
As for the backward phase, the supervision cues at high-level layers can be back-propagated through the short-cut way via the residual connections. This type of mechanism can prevent networks from gradient vanishing and exploding~\cite{glorot2010understanding}, which hampers network convergence during training.

We have one mainstream loss layer connected from ``Res/Conv1b" and another two auxiliary loss layers connected from ``Conv2b" and ``Conv3b" to the ground truth label, respectively. For the mainstream loss in ``Res/Conv1b" as the last layer which has the same size of data flow as one of the input, a $1 \times 1 \times 1$ convolution is followed to reduce the number of channels to the number of label classes which is $2$ in our case. As for the two auxiliary loss layers, deconvolution layers are connected to upsample feature maps to be the same as input.

The deep supervision imposed by auxiliary losses provides robustness to hyper-parameters choice, in that the low-level layers are guided by the direct segmentation loss, leading to faster convergence rate. Throughout this work, we have two auxiliary branches where the default parameters are $\xi_1 = 0.2$ and $\xi_2 = 0.4$ in Eq.~\ref{Eq:OverallObj} to control the importance of deep supervisions compared with the major supervision from the mainstream loss for all segmentation networks.

\begin{table}[htb]
\footnotesize
\begin{center}
\begin{tabular}{lcccc}\toprule
Method 							& Long Res 	& Short Res & Deep Super & Loss\\
\hline
ResDSN (Ours) 							& Sum		& No		& Yes 		&CE \\
\hline
FResDSN 						& Sum		& Sum		& Yes 		&CE \\
SResDSN 						& No		& Sum		& Yes 		&CE \\
\hline
3D U-Net~\cite{cciccek20163d}	& Concat	& No		& No		&CE\\
V-Net~\cite{milletari2016v} 	& Concat	& Sum		& No		&DSC\\
VoxResNet~\cite{chen2017voxresnet}	& No 	& Sum		& Yes		&CE\\
MixedResNet~\cite{yu2017volumetric}	& Sum	& Sum 		& Yes		&CE\\
3D DSN~\cite{dou20173d} 		& No		& No		& Yes 		&CE \\
3D HED~\cite{merkow2016dense} 	& Concat	& No		& Yes 		&CE \\
\bottomrule
\end{tabular}
\end{center}
\caption{
    Configurations comparison of different 3D segmentation networks on medical image analysis.
    For all the abbreviated phrases, ``Long Res" means long residual connection, ``Short Res" means short residual connection, ``Deep Super" means deep supervision implemented by auxiliary loss layers, ``Concat" means concatenation, ``DSC" means Dice-S{\o}rensen Coefficient and ``CE" means cross-entropy. For residual connection, it has two types: concatenation (``Concat") or element-wise sum (``Sum").
}
\label{Tab:Config3DSegmentation}
\end{table}

As shown in Table~\ref{Tab:Config3DSegmentation}, we give the detailed comparisons of network configurations in terms of four aspects: long residual connection, short residual connection, deep supervision and loss function. Our backbone network architecture, named as ``ResDSN", is proposed with different strategies in terms of combinations of long residual connection and short residual connection compared with VoxResNet~\cite{chen2017voxresnet}, 3D HED~\cite{merkow2016dense}, 3D DSN~\cite{dou20173d} and MixedResNet~\cite{yu2017volumetric}. In this table, we also depict ``FResDSN" and ``SResDSN", where ``FResDSN" and ``SResDSN" are similar to MixedResNet~\cite{yu2017volumetric} and VoxResNet~\cite{chen2017voxresnet}, respectively. As confirmed by our quantitative experiments in Sec.~\ref{Sec:DisResConnection}, instead of adding short residual connections to the network, \emph{e.g.}, ``FResDSN" and ``SResDSN", we only choose the long residual element-wise sum, which can be more computationally efficient while even performing better than the ``FResDSN" architecture which is equipped with both long and short residual connections. Moreover, ResDSN has noticeable differences with respect to the V-Net~\cite{milletari2016v} and 3D U-Net~\cite{cciccek20163d}. On the one hand, compared with 3D U-Net and V-Net which concatenate the lower-level local features to higher-level global features, we adopt the element-wise sum between these features, which outputs less number of channels for efficient computation. On the other hand, we introduce deep supervision via auxiliary losses into the network to yield better convergence.

\section{Experiments}
In this section, we first describe in detail how we conduct training and testing in the \textit{\textbf{coarse}} and \textit{\textbf{fine}} stages, respectively. Then we are going to compare our proposed method with previous state-of-the-art on two pancreas datasets: NIH pancreas dataset~\cite{roth2015deeporgan} and JHMI pathological pancreas dataset~\cite{zhou2017deep}.
\subsection{Network Training and Testing}\label{Sec:NetworkTrTs}
All our experiments were run on a desktop equipped with the NVIDIA TITAN X (Pascal) GPU and deep neural networks were implemented based on the CAFFE~\cite{jia2014caffe} platform customized to support 3D operations for all necessary layers, \emph{e.g.}, ``convolution", ``deconvolution" and ``pooling", \emph{etc}. For the data pre-processing, we simply truncated the raw intensity values to be in $[-100, 240]$ and then normalized each raw CT case to have zero mean and unit variance to decrease the data variance caused by the physical processes~\cite{gravel2004method} of medical images. As for the data augmentation in the training phase, unlike sophisticated processing used by others, \emph{e.g.}, elastic deformation~\cite{milletari2016v}\cite{ronneberger2015u}, we utilized simple but effective augmentations on all training patches, \emph{i.e.}, rotation ($90\degree, 180\degree, \textrm{ and } 270\degree$) and flip in all three axes (axial, sagittal and coronal), to increase the number of 3D training samples which can alleviate the scarce of CT scans with expensive human annotations. Note that different CT cases have different physical resolutions, but we keep their resolutions unchanged. The input size of all our networks is denoted by $W_I\times H_I\times D_I$, where $ W_I = H_I = D_I =64$.

For the \textit{\textbf{coarse}} stage, we randomly sampled $64 \times 64 \times 64$ sub-volumes from the whole CT scan in the training phase. In this case, a sub-volume can either cover a portion of pancreas voxels or be cropped from regions with non-pancreas voxels at all, which acts as a hard negative mining to reduce the false positive. In the testing phase, a sliding window was carried out to the whole CT volume with a \textit{\textbf{coarse}} stepsize that has small overlaps within each neighboring sub-volume. Specifically, for a testing volume with a size of $W \times H \times D$, we have a total number of $(\floor{\frac{W}{W_I}} + n) \times (\floor{\frac{H}{H_I}} + n) \times (\floor{\frac{D}{D_I}} + n)$ sub-volumes to be fed into the network and then combined to obtain the final prediction, where $n$ is a parameter to control the sliding overlaps that a larger $n$ results in a larger overlap and vice versa. In the \textit{\textbf{coarse}} stage for the low time cost concern, we set $n = 6$ to efficiently locate the rough region of pancreas $\mathbf{X}^\textrm{F}$ defined in Eq.~\ref{Eq:CoarseSegmentation} from the whole CT scan $\mathbf{X}^\textrm{C}$.

For the \textit{\textbf{fine}} stage, we randomly cropped $64 \times 64 \times 64$ sub-volumes constrained to be from the pancreas regions defined by ground-truth labels during training. In this case, a training sub-volume was assured to cover pancreatic voxels, which was specifically designed to be capable of segmentation refinement. In the testing phase, we only applied the sliding window on $\mathbf{X}^\textrm{F}$ with a size of $W_F\times H_F\times D_F$. The total number of sub-volumes to be tested is $(\floor{\frac{W_F}{W_I}} + n)\times (\floor{\frac{H_F}{H_I}} + n)\times (\floor{\frac{D_F}{D_I}} + n)$. In the \textit{\textbf{fine}} stage for the high accuracy performance concern, we set $n = 12$ to accurately estimate the pancreatic mask $\mathbf{P}^\textrm{F}$ from the rough segmentation volume $\mathbf{X}^\textrm{F}$. In the end, we mapped the $\mathbf{P}^\textrm{F}$ back to $\mathbf{P}^\textrm{C}$ to obtain $\mathbf{P}^\textrm{C2F}$ for the final pancreas segmentation as given in Eq.~\ref{Eq:C2FSegmentation}, where the mapping location is given by the cropped location of $\mathbf{X}^\textrm{F}$ from $\mathbf{X}^\textrm{C}$.

After we get the final binary segmentation mask, we denote $\mathcal{P}$ and $\mathcal{Y}$ to be the set of pancreas voxels in the prediction and ground truth, separately, \emph{i.e.}, $\mathcal{P} = \{i | p_i = 1\}$ and $\mathcal{Y} = \{i | y_i = 1\}$. The evaluation metric is defined by the Dice-S{\o}rensen Coefficient (DSC) formulated as $\text{DSC}(\mathcal{P}, \mathcal{Y}) = \frac{2\times |\mathcal{P}\cap \mathcal{Y}|}{|\mathcal{P}| + |\mathcal{Y}|}$. This evaluation measurement ranges in $[0, 1]$ where $1$ means a perfect prediction.

\subsection{NIH Pancreas Dataset}
% a threshold of
We conduct experiments on the NIH pancreas segmentation dataset~\cite{roth2015deeporgan}, which contains $82$ contrast-enhanced abdominal CT volumes provided by an experienced radiologist. The size of CT volumes is $512\times512\times D$, where $D\in [181, 466]$ and their spatial resolutions are $w\times h\times d$, where $d = 1.0\textrm{mm}$ and $w = h$ that ranges from $0.5\textrm{mm}$ to $1.0\textrm{mm}$. Data pre-processing and data augmentation were described in Sec.~\ref{Sec:NetworkTrTs}. Note that we did not normalize the spatial resolution into the same one since we wanted to impose the networks to learn to deal with the variations between different volumetric cases. Following the training protocol~\cite{roth2015deeporgan}, we perform $4$-fold cross-validation in a random split from $82$ patients for training and testing folds, where each testing fold has $21, 21, 20$ and $20$ cases, respectively. We trained networks illustrated in Fig.~\ref{fig:ResDSN} by SGD optimizer with a $16$ mini-batch, a $0.9$ momentum, a base learning rate to be $0.01$ via polynomial decay (the power is $0.9$) in a total of $80\rm{,}000$ iterations, and the weight decay $0.0005$. Both training networks in the \textit{\textbf{coarse}} and \textit{\textbf{fine}} stages shared the same training parameter settings except that they took a $64 \times 64 \times 64$ input sampled from different underlying distributions described in Sec.~\ref{Sec:NetworkTrTs}, which included the details of testing settings as well. We average the score map of overlapped regions from the sliding window and throw away small isolated predictions whose portions are smaller than $0.2$ of the total prediction, which can remove small false positives. For DSC evaluation, we report the average with standard deviation, max and min statistics over all $82$ testing cases as shown in Table~\ref{Tab:NIHC2FSegmentation}.

\begin{table}[tb]
\footnotesize
\begin{center}
\begin{tabular}{lccc}\toprule
Method           & {Mean DSC}       		& {Max DSC}    		& {Min DSC} \\
\hline
{ResDSN C2F (Ours)}  		& $\bm{84.59\pm{4.86}}\%$ 				& $\bm{91.45}\%$       	&$\bm{69.62}\%$ \\
{ResDSN Coarse (Ours)}  		& $83.18\pm{6.02}\%$ 				& $91.33\%$       	&$58.44\%$ \\
\hline
{Cai \emph{et al.}~\cite{cai2017improving}}   & $82.4\pm{6.7}\%$          & $90.1\%$          & $60.0\%$          \\
{Zhou \emph{et al.}~\cite{zhou2017fixed}} 	& $82.37\pm{5.68}\%$ 				&$90.85\%$ 			& $62.43\%$\\
{Dou\footnotemark[1] \emph{et al.}~\cite{dou20173d}} &$82.25\pm{5.91}\%$ 				&$90.32\%$			&$62.53\%$ \\
{Roth \emph{et al.}~\cite{roth2016spatial}}		&$78.01\pm{8.20}\%$ 				&$88.65\%$			&$34.11\%$ \\
{Yu\footnotemark[1] \emph{et al.}~\cite{yu2017automatic}} &$71.96\pm{15.34}\%$ 				&$89.27\%$			&$0\%$ \\
\bottomrule
\end{tabular}
\end{center}
\caption{
    Evaluation of different methods on the NIH dataset. Our proposed framework achieves state-of-the-art by a large margin compared with previous state-of-the-arts.
}
\label{Tab:NIHC2FSegmentation}
\end{table}

\footnotetext[1]{The results are reported by our runs using the same cross-validation splits where code is available from their GitHub: \url{https://github.com/yulequan/HeartSeg}.}

First of all, our overall coarse-to-fine framework outperforms previous state-of-the-art by nearly $2.2\%$ (Cai \emph{et al.}~\cite{cai2017improving} and Zhou \emph{et al.}~\cite{zhou2017fixed}) in terms of average DSC, which is a large improvement. The lower standard deviation of DSC shows that our method is the most stable and robust across all different CT cases. Although the enhancement of max DSC of our framework is small due to the saturation, the improvement of min DSC over the second best (Dou \emph{et al.}~\cite{dou20173d}) is from $62.53\%$ to $69.62\%$, which is a more than $7\%$ advancement. The worst case almost reaches $70\%$, which is a reasonable and acceptable segmentation result. After coarse-to-fine, the segmentation result of the worst case is improved by more than $11\%$ after the 3D-based refinement from the 3D-based coarse result. The overall average DSC was also improved by $1.41\%$, which proves the effectiveness of our framework.

\begin{figure*}
\centering
	\includegraphics[width=0.85\linewidth]{figures/NIHC2F3.pdf}
   \caption{Examples of segmentation results reported by ``ResDSN Coarse" and ``ResDSN C2F" on a same slice in the axial view from NIH case $\#33$, $\#63$ and $\#74$, respectively. Numbers after ``Coarse" or ``C2F" mean testing DSC. Red, green and yellow indicate the ground truth, prediction and overlapped regions, respectively. Best viewed in color.
}
\label{fig:NIHC2F3}
\end{figure*}

s shown in Fig~\ref{fig:NIHC2F3}, we report the segmentation results by ``ResDSN Coarse" and ``ResDSN C2F" on the same slice for comparison. Note that yellow regions are the correctly predicted pancreas. For the NIH case $\#33$, which is the min DSC case reported by both ``ResDSN Coarse" and ``ResDSN C2F", the ``ResDSN C2F" successfully predict more correct pancreas regions at the bottom, which is obviously missed by ``ResDSN Coarse". If the coarse segmentation is bad, \emph{e.g.}, case $\#33$ and $\#63$, our 3D coarse-to-fine can significantly improve the segmentation results by as much as $10\%$ in DSC. However, if the coarse segmentation is already very good, \emph{e.g.}, case $\#74$, our proposed method cannot improve too much. We conclude that our proposed ``ResDSN C2F" shows its advancement over 2D methods by aggregating rich spatial information and is more powerful than other 3D methods on the challenging pancreas segmentation task.


\subsection{JHMI Pathological Pancreas Dataset}
We verified our proposed idea on the JHMI pathological cyst dataset~\cite{zhou2017deep} of abdominal CT scans as well. Different from the NIH healthy pancreas dataset, this dataset includes pathological cysts where some can be or can become cancerous. The pancreatic cancer stage largely influences the morphology of the pancreas~\cite{lasboo2010morphological} that makes this dataset extremely challenging for considering the large variants.

This dataset has a total number of $131$ contrast-enhanced abdominal CT volumes with human-labeled pancreas annotations. The size of CT volumes is $512\times512\times D$, where $D\in [358, 1121]$ that spans a wider variety of thickness than one of the NIH dataset. Following the training protocol~\cite{zhou2017deep}, we conducted $4$-fold cross validation on this dataset where each testing fold has $33, 33, 32$ and $33$ cases, respectively. We trained networks illustrated in Fig.~\ref{fig:ResDSN} in both the $\textit{\textbf{coarse}}$ and $\textit{\textbf{fine}}$ stage with the same training settings as on the NIH except that we trained a total of $300\rm{,}000$ iterations on this pathological dataset since a pancreas with cysts is more difficult to segment than a normal case. In the testing phase, we vote the prediction map of overlapped regions from the sliding window and ignore small isolated pancreas predictions whose portions are smaller than $0.05$ of the total prediction. As shown in Table.~\ref{Tab:JHUC2FSegmentation}, we compare our framework with only one available published results on this dataset. ``ResDSN C2F" achieves an average ${80.56\%}$ DSC that consistently outperforms the 2D based coarse-to-fine method~\cite{zhou2017deep}, which confirms the advantage of leveraging the rich spatial information along three axes. What's more, the ``ResDSN C2F" improves the ``ResDSN Coarse" by ${2.60\%}$ in terms of the mean DSC, which is a remarkable improvement that proves the effectiveness of the proposed 3D coarse-to-fine framework. Both ~\cite{zhou2017deep} and our method have multiple failure cases whose testing DSC are $0$, which indicates the segmentation of pathological organs is a more tough task. Due to these failure cases, we observe a large deviation on this pathological pancreas dataset compared with results on the NIH healthy pancreas dataset. 

 \begin{table}[htb]
 \footnotesize
 \begin{center}
 \begin{tabular}{lc l c}\toprule
 Method           & {Mean DSC} \\
 \hline
 {ResDSN C2F (Ours)}  		& $\bm{80.56\pm{13.36}}\%$ \\ 
 {ResDSN Coarse (Ours)}  		& $77.96\pm{13.36}\%$ \\
 \hline
 {Zhou \emph{et al.}~\cite{zhou2017deep}} 	& $79.23\pm{9.72}\%$\\
 \bottomrule
 \end{tabular}
 \end{center}
 \caption{
     Evaluations on the JHMI pathological pancreas. 
 }
 \label{Tab:JHUC2FSegmentation}
 \vspace{-0.2in}
 \end{table}


\section{Discussion}
In this section, we conduct the ablation studies about residual connection, time efficiency and deep supervision to further investigate the effectiveness and efficiency of our proposed framework for pancreas segmentation.

\subsection{Residual Connection}\label{Sec:DisResConnection}
We discuss how different combinations of residual connections contribute to the pancreas segmentation task on the NIH dataset. All the residual connections are implemented in the element-wise sum and they shared exactly the same deep supervision connections, cross-validation splits, data input, training and testing settings except that the residual structure is different from each other. As given in Table.~\ref{Tab:NIHResSegmentation}, we compare four configurations of residual connections of 3D based networks only in the $\textit{\textbf{coarse}}$ stage. The major differences between our backbone network ``ResDSN" with respect to ``FResDSN", ``SResDSN" and ``DSN" are depicted in Table.~\ref{Tab:Config3DSegmentation}. ``ResDSN" outperforms other network architectures in terms of average DSC and a small standard deviation even through the network is not as sophisticated as ``FResDSN", which is the reason we adopt ``ResDSN" for efficiency concerns in the \textit{\textbf{coarse}} stage.

\begin{table}[tb]
\footnotesize
\begin{center}
\begin{tabular}{lccc}\toprule
Method           & {Mean DSC}       		& {Max DSC}    		& {Min DSC} \\
\hline
{ResDSN Coarse (Ours)}  		& $\bm{83.18\pm{6.02}}\%$ 				& $91.33\%$       	&$58.44\%$ \\
\hline
{FResDSN Coarse} 	& $83.11\pm{6.53}\%$ 				&$91.34\%$ 			& $61.97\%$\\
{SResDSN Coarse} &$82.82\pm{5.97}\%$ 				&$90.33\%$			&$62.43\%$ \\
{DSN~\cite{dou20173d} Coarse}		&$82.25\pm{5.91}\%$ 				&$90.32\%$			&$62.53\%$ \\
\bottomrule
\end{tabular}
\end{center}
\caption{
    Evaluation of different residual connections on NIH.
}
\label{Tab:NIHResSegmentation}
\end{table}

\subsection{Time Efficiency}\label{Sec:DisTimeCost}
We discuss the time efficiency of the proposed coarse-to-fine framework with a smaller overlap in the \textit{\textbf{coarse}} stage for the low consuming time concern while a larger one in the \textit{\textbf{fine}} stage for the high prediction accuracy concern. The overlap size depends on how large we choose $n$ defined in Sec~\ref{Sec:NetworkTrTs}. We choose $n=6$ during the coarse stage while $n=12$ during the fine stage. Experimental results are shown in Table~\ref{Tab:NIHTimeCost}. ``ResDSN Coarse" is the most efficient while the accuracy is the worst among three methods, which makes sense that we care more of the efficiency to obtain a rough pancreas segmentation. ``ResDSN Fine" is to use a large overlap on an entire CT scan to do the segmentation which is the most time-consuming. In our coarse-to-fine framework, we combine the two advantages together to propose ``ResDSN C2F" which can achieve the best segmentation results while the average testing time cost for each case is reduced by $36\%$ from $382$s to $245$s compared with ``ResDSN Fine". In comparison, it takes an experienced board certified Abdominal Radiologist 20 mins for one case, which verifies the clinical use of our framework.

\begin{table}[tb]
\footnotesize
\begin{center}
\begin{tabular}{lccc}\toprule
Method				& Mean DSC			& $n$ &Testing Time (s) \\
\hline
{ResDSN C2F (Ours)}  & $\bm{84.59\pm{4.86}}\%$ & $6\&12$	& $245$\\
{ResDSN Coarse (Ours)}  & $83.18\pm{6.02}\%$ & $6$	& $\bm{111}$\\
{ResDSN Fine (Ours)}	& $83.96\pm{5.65}\%$ & $12$	& $382$\\
\bottomrule
\end{tabular}
\end{center}
\caption{
    Average time cost in the testing phase, where $n$ controls the overlap size of sliding windows during the inference. 
}
\vspace{-0.4cm}
\label{Tab:NIHTimeCost}
\end{table}

\subsection{Deep Supervision}
We discuss how effective of the auxiliary losses to demonstrate the impact of the deep supervision on our 3D coarse-to-fine framework. Basically, we train our mainstream networks without any auxiliary losses for both coarse and fine stages, denoted as ``Res C2F", while keeping all other settings as the same, \emph{e.g.}, cross-validation splits, data pre-processing and post-processing. As shown in Table~\ref{Tab:DeepSupervision}, ``ResDSN C2F" outperforms ``Res C2F" by $17.79\%$ to a large extent on min DSC and $0.53\%$ better on average DSC though it's a little bit worse on max DSC. We conclude that 3D coarse-to-fine with deep supervisions performs better and especially more stable on the pancreas segmentation.

\begin{table}[tb]
\footnotesize
\begin{center}
\begin{tabular}{lccc}\toprule
Method           & {Mean DSC}       		& {Max DSC}    		& {Min DSC} \\
\hline
{ResDSN C2F (Ours)}  		& $\bm{84.59\pm{4.86}}\%$ 				& $91.45\%$       	&$\bm{69.62}\%$ \\
{Res C2F} &$84.06\pm{6.51}\%$ 				&$\bm{91.72}\%$			&$51.83\%$ \\
\bottomrule
\end{tabular}
\end{center}
\caption{
    Discussions of the deep supervision on NIH.
}
\vspace{-0.4cm}
\label{Tab:DeepSupervision}
\end{table}

\vspace{-0.3cm}
\section{Conclusion}
In this work, we propose a novel 3D network called ``ResDSN" integrated with a coarse-to-fine framework to simultaneously achieve high segmentation accuracy and low time cost. The backbone network ``ResDSN" is carefully designed to only have long residual connections for efficient inference. To our best knowledge, we are one of the first works to segment the challenging pancreas using 3D networks which leverage the rich spatial information to achieve the state-of-the-art. On widely-used datasets, the worst segmentation case is experimentally improved a lot by our coarse-to-fine framework. What's more, our coarse-to-fine framework can work on both normal and abnormal pancreases to achieve good segmentation accuracy.

Though this work mainly focuses on segmentation for the pancreas, we can naturally apply the proposed idea to other small organs, \emph{e.g.}, spleen, duodenum and gallbladder, \emph{etc},  In the future, we will target on error causes that lead to inaccurate segmentation to make our framework more stable, and extend our 3D coarse-to-fine framework to cyst segmentation which can cause cancerous tumors, and the very important tumor segmentation~\cite{zhu2018multi} task.\\
{\bf Acknowledgements} This work was supported by the Lustgarten Foundation for Pancreatic Cancer Research, and National Natural Science Foundation of China No. 61672336. We appreciate enormous help from Seyoun Park, Lingxi Xie, Yuyin Zhou, Yan Wang, Fengze Liu, and valuable discussions from Qing Liu, Yan Zheng, Chenxi Liu, Zhe Ren.

\newpage
{\small
\bibliographystyle{ieee}
\bibliography{egbib}
}

\end{document}
