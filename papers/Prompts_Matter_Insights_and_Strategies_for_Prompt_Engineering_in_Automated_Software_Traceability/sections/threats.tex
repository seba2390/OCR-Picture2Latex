While this initial study provides promising evidence that prompt engineering can enhance LLMs for software traceability tasks, several threats could limit the validity of our findings. First, we evaluated only three open-source projects and only provide a detailed analysis of one, limiting the generalization of our findings. However, we selected projects that spanned multiple domains, artifact types, and sizes to improve generalizability. We also constructed trace queries that were representative of their parent distribution. Second, existing traceability datasets are typically incomplete, as truly considering every candidate link in a project grows $\mathcal{O}(n^2)$ with the number of artifacts. The LLMs identified potential missing traces, but we could not fully validate their accuracy without a project expert. Third, our study used a limited set of LLMs which may not represent the full space of the current state-of-the-art. However, we chose the leading LLMs from our initial explorations with publicly available commercial models. Clearly, there are many extension to this study considering more datasets, different LLMs, and other prompt engineering methods. We leave the full exploration of the problem space to future work and focus on showing the potential these models have towards advancing automated software traceability.