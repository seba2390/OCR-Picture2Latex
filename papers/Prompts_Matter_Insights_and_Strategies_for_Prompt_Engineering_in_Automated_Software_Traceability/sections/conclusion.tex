\input{tables/metrics}

Throughout our experiments, we addressed multiple questions and derived several key takeaways regarding using LLMs for trace-link prediction.

\subsection{Key Takeaways}

\begin{itemize}
\item Small modifications to prompts can lead to significant differences in model outputs, emphasizing the importance of carefully crafting prompts.

\item The performance of a given prompt in comparison to alternative phrasings can vary across datasets and models, though some general techniques like chain-of-thought reasoning tend to produce a more consistent performance.

\item LLMs frequently identify different artifact relationships by than those selected by human tracers. Prompts should specify the targeted usage of the traceability links (e.g. change impact analysis, hierarchical composition) to better align the model's output with the desired outcome.

\item Specifying the targeted usage has the additional benefit of opening the door for creating different trace views - a possible advantage over purely similarity-based methods.

% \item LLMs can also provide explanations for their decision to trace two artifacts, offering useful insights for practitioners.

\item Requiring models to show intermediate reasoning steps boosts performance on some tasks and builds in explanations into the decision making process. This is useful to both to those establishing the trace links and those using them. 

\item List ranking style prompts are highly sensitive to the order of artifacts presented in the prompt. This variability was mitigated by pre-sorting by VSM scores.

\item Overall, carefully tailored prompts are needed to harness the versatility of LLMs for the task of traceability and to produce outputs that are consistent with the goals of traceability engineers and researchers.

\end{itemize}

Throughout this process, one of our biggest takeaways was how minor adjustments to prompts could have dramatic impacts on the results. Subtle changes, such as pluralizing words, interchanging prepositions, or reordering phrases, could alter the outcomes. These findings underscore the inherent challenge of engineering robust prompts. In future research, we aim to explore strategies that mitigate such variability and delve into the effectiveness of different prompts across different models.

Further, due to the limited number of trace queries we analyzed per dataset as well as our integration of chain-of-thought, we were able to review trace predictions in depth. Interestingly, we were often surprised by the strength of many false positives, forcing us to re-think the accurate and complete nature of these datasets. Reviewing predictions for even our smallest subset (265 combinations) became an arduous task.  In reality, industrial projects range from 50K to 500K potential trace links, making it extremely challenging to have complete and standardized tracing practices. However, examining the predictions of a few selected trace links may still provide traceability experts with the insights they need to refine prompts in a way that improves performance across the project.

\subsection{Do LLMs possess knowledge necessary for tracing projects with domain-specific vocabulary?}
Our conversations with Claude revealed that it contained sufficient knowledge to draw many correct conclusions about the CM1 system, irrespective of the acronyms or jargon used. Furthermore, we were able to obtain high MAP scores without performing any additional pre-training.
Nevertheless, we plan to experiment with pre-training in the future to see if it can provide a performance boost. Additionally, we hope to test the model's knowledge on a wider range of datasets. It is important to note that since the datasets in this paper were all publicly available at the time of the model's creation, we cannot eliminate the possibility that the model had previous exposure to them. Thus, we are particularly interested to see how the model performs on an entirely new dataset.


\subsection{Can LLMs provide reasonable explanations for their decisions?}
By probing the model to elicit explanations for many of its mispredictions, we found that it could provide an in-depth analysis of its decision. Whether or not these explanations are accurate reflections of the reasoning behind the model's decision is beyond the scope of this paper, but we did find that when we adjusted the prompts based on the model's explanation, we were often able to change its answer. 


\subsection{If so, can these explanations be utilized to improve prompts?}
The ability to alter the model's decision by using its explanations proved to be a useful tool for improving prompts. Engaging in conversations with the model enabled an increased understanding of its interpretation of a given prompt, facilitating an iterative approach to refine prompts. Gradually adjusting the prompts in this way can be used to find a prompt that better aligns the model's understanding with the objectives of the tracer.


\subsection{Can reasoning be used to improve responses?}
By asking the model to formally articulate its thinking in response to probing questions, the model was able to make a more well-informed final judgment about the relationship between the artifacts in the classification. This also offers the advantage of allowing the task to be broken down into smaller pieces, where the model first evaluates the relationship between the artifacts and then makes a final decision. Further, chain-of-thought reasoning has the potential to improve the ranking task and should be evaluated in future work.


\subsection{How can LLMs be leveraged to generate
software traceability links?}
In our experiments, we explored two different tasks which could be used to predict trace links from pairs of software artifacts: classification and 
ranking. While ranking allows for a nuanced expression of confidence in a prediction, classification  offers the advantage of needing a smaller context window and enables the discovery of diverse relationship types. By adapting our prompts to describe various relationships, we captured distinct links. For instance, when inquiring whether two artifacts were part of the same feature, we discovered different links than when asking if they shared functionality. This can be used to present multiple ``views" of traceability, where each view highlights different relationships within the system. This may be particularly valuable for change propagation where the prompt can focus on determining whether a modification to one artifact necessitates a change in the other. Additionally, multiple prompts may be combined to capture the many different relationships present in the project. This presents an avenue for future investigation.

An alternative way in which LLMs can be used for trace link prediction is by comparing the similarity of artifact embeddings. As mentioned previously, we opted not to explore this method in this paper, but future works might benefit from comparing this approach to those discussed in this paper.


\subsection{Concluding Remarks}
Overall, our experiments demonstrated that large language models show promise for tracing software systems. As opposed to previous approaches for automated traceability, LLMs can perform well without pre-training and are able to offer detailed explanations of their decisions. These explanations are not only useful for helping an engineer make an informed decision about a trace-link but can guide the process of selecting an appropriate prompt for the tracing task. Through iterative prompt refinement, the models can be used to classify trace links and establish a diverse set of relationships between project artifacts. The models are also capable of ranking target artifacts based on how related they are to a source artifact, albeit with aid from VSM. Ranking can allow engineers to sift through a prioritized list of candidate links and potentially reducing the review time required.

While this paper showcases the power of LLMs for traceability, it also highlights many of the lingering challenges in engineering effective prompts for the models. Careful tailoring of prompts can help to reach high performance for each project but this was ultimately a time-consuming task that may not always be feasible. Although the community might one day discover a "silver bullet" prompt, a more practical path forward may be to identify common patterns that make prompts most effective for certain projects and tracing objectives. Discovering such patterns could enable partially automating this process so that it can be seamlessly integrated into current traceability workflows. There remains much future work that must be done to gain a comprehensive understanding of how LLMs can best be utilized to enhance the field of traceability.

