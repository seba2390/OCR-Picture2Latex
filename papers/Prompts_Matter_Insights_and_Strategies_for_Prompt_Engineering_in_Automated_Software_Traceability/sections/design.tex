For the preliminary investigation reported in this paper, we analyzed three software engineering datasets: CM1, iTrust, and Dronology. We selected these datasets to span natural language and programming language artifacts as well as diverse application domains (embedded systems, healthcare, UAVs). 

For each dataset, we selected only a subset of its data to use in our study in order to increase the depth of our analysis, reduce run-time, and decrease cost. To select the links, we first calculated the number of children artifacts traced to each parent and then identified the minimum, maximum and median number of links. Using these categories, we identified five parent artifacts: one with the fewest child links, three with the median number of child links, and one with the maximum number of child links. In cases where multiple parent artifacts tied for the minimum, median, or maximum, we randomly sampled from those tied parents. This allowed us to create a set of trace queries that were representative of the project's link distribution of its trace queries. Table \ref{tab:datasets} describes the selected queries for each system noting the parent and child types, the number of potential trace links (candidates), and the number of those links that were actually true.

Prior to the start of our experiments, we tested OpenAI's \emph{text-davinci-003} model for predicting trace links, and found that, while it required slightly different prompts, it had comparable capabilities to Anthropic's Claude instant model (\emph{claude-instant-v1}). Due to its lower cost and increased speed, we selected Claude for the remainder of our experiments. We also explored utilizing embeddings to compute similarity scores between artifacts, similar to the original Vector Space Model (VSM) approaches \cite{traceability_vsm}. We examined the ada-embedding model developed by OpenAI (\emph{text-embedding-ada-002}), however, the results obtained from this investigation did not show a significant advantage over VSM. Therefore, we decided to leverage the generative capabilities of the models for trace link predictions within this paper. Nevertheless, we acknowledge the need for future endeavors to conduct a more comprehensive analysis of the advantages and disadvantages associated with utilizing embeddings for generating trace links.

 Additionally, we obtained summaries of all code artifacts to use in our experiments. We accomplished this by prompting the model to provide a several sentences focusing on the high-level functionality of the code. Although this removed some information, the resulting summaries contained most of the relevant details and reduced the number of tokens required for each tracing prompt.

For our first approach, we prompted the model to classify every source and target artifact pair. Each prompt followed a similar format, consisting primarily of a question and instructions to answer `yes' or `no', followed by the content of the source artifact numbered as `1' and the target artifact numbered as `2'. When a prompt directly referenced the source or target in the question, it used (1) to indicate the source or (2) to indicate the target, corresponding to the numbers of the artifact content (e.g., ``Is (1) related to (2)?"). Each question was posed such that an answer of `yes' was indicative of a link between the answers, while `no' indicated that the artifacts were not linked. The resulting candidate links are then evaluated against the ground truth links using common classification metrics such as precision and recall. 

Precision is the ratio of the number of correctly identified relevant trace links to the total number of trace links identified by the system. Recall, on the other hand, measures the ratio of the correctly identified relevant trace links to the total number of relevant trace links in the system. This is shown below where TP is the true positives, FP is false positives, and FN is false negatives.
\begin{align*}
\text{Precision} &= \frac{\text{TP}}{\text{TP} + \text{FP}} &
\text{Recall}    &= \frac{\text{TP}}{\text{TP} + \text{FN}}
\end{align*}

For our ranking approach, we prompted the model to rank all targets for each source artifact. In this case, the model was given the content of the source artifact and the ID and body of each target, separated by newlines. The model was instructed to return the artifact IDs in the order of relevance (from most to least) in a comma delimited list. Given the non-deterministic nature of responses from each model, there were times when the model neglected to include some artifact IDs. This problem was unique to the ranking task, as the model correctly output yes and no each time for the classification task. In these cases we randomly appended the missing ids to the end of the list for our evaluation. We calculate the Mean Average Precision of these rankings to showcase their performance. It provides a measure of the effectiveness of the ranking algorithm in identifying relevant trace links between software artifacts. To calculate MAP, the precision is computed at different levels of recall. The average precision is then calculated as the average of the precision values at each recall level. Finally, the mean of the average precision values across trace queries is taken to obtain the MAP score. The equation for MAP is obtained by taking the mean of the average precision values across different queries or datasets:

\begin{equation*}
\text{MAP} = \frac{1}{N} \sum_{i=1}^{N} \text{Average Precision}_i
\end{equation*}

where $N$ is the number of queries or datasets.

Throughout our process of generating trace-links, we have several conversations with the model to test its prior knowledge, understand its responses, and to brainstorm potential prompts and improvements to prompts. We include many of these in our paper. It is important to note that these exchanges occurred independently - the model could not reference previous conversations when responding to subsequent questions or when making new traceability classifications. Each conversation represented an isolated context, rather than a continuing thread. This methodology enabled us to systematically probe the model's knowledge without reliance on prior memorization.

% \human{Hello, what is your name?}
% \assistant{Hello, my name is Claude.}

% As shown above, the content in the gray \textbf{Human} speech bubble is the prompt that we gave the model while the purple \textbf{Assistant} speech bubble is the model's response. For brevity, we often omit insignificant portions of the model's response and indicate this using ellipses. 