Due to space constraints, we focus our analysis on our prompt engineering process for the CM1 dataset. While we followed a similar prompting process with other datasets, we found that many of the challenges we faced were exemplified in CM1. We specifically chose CM1 to focus on because it has posed a unique challenge to researchers in the past \cite{bert_pl, bert_nl}. In the end, we show the performance of our best approach on all the datasets to highlight that many of the lessons we learned from CM1 were transferable to the other datasets as well.

\subsection{Domain-specific Knowledge}
Before running trace-link prediction with the models, we first wanted to understand what prior knowledge the model had about the system being evaluated. Previously, BERT models often required extensive pre-training on domain-specific documentation to interpret the vocabulary within a project \cite{bert_pl, bert_nl}. CM1, in particular, includes a myriad of jargon and acronyms that have long presented a challenge to previous methods of automatic traceability \cite{cm1, rodriguez2023understanding}. Since one strength of LLMs is their ability to perform well without any training \cite{gpt3, gpt4}, we wanted to explore whether Claude possessed enough prior knowledge to succeed on the CM1 dataset. To do this, we had the following conversation with it about one of the CM1 artifacts containing a large number of acronyms.

\human{What does this artifact mean? \\
The DPU-TMALI shall utilize SCM-DCI-SR, along with ERRNO provided by DPU-DCI to decode errors and place them on an error queue for DPU-CCM.}

\assistant{
... Some guesses at the meaning:  \newline \newline DPU may stand for Data Processing Unit ... SCM-DCI-SR seems to be some kind of protocol or interface used for communication between DPU-TMALI and DPU-DCI. \newline \newline ERRNO is a standard way for software components to return error codes. So DPU-DCI seems to be providing error codes to DPU-TMALI ... DPU-TMALI is receiving error data from DPU-DCI, decoding or interpreting those errors, and then queuing them up to be sent to DPU-CCM. \newline \newline DPU-CCM seems to be a component that is receiving error data, perhaps for monitoring purposes...}

As shown in the conversation above, the model was able to leverage its general knowledge to reason about what each component did from only the artifact content. Although we did not extensively investigate the model's knowledge of each acronym or vocabulary word in the dataset, this example shows that Claude can interpret the meaning of complex acronyms in some cases where previous models could not without fine-tuning \cite{bert_nl, bert_pl}. 

\subsection{Classification Prompt Discovery}

Next, we began the process of creating a prompt to use for our trace link prediction. As discussed previously, we provide highlights only for our experiments with CM1 but performed a similar analysis across datasets. Results for each prompt can be found in Table \ref{tab:cm1_prompt_metrics}.
Our initial prompt for the model was general:
\human{Is (1) related to (2)? Answer ‘yes’ or ‘no’. \\}

The content of the two artifacts was included in the prompt following the format explained in Section \ref{sec:design}. On CM1, this prompt resulted in the model correctly identifying 6 out of 13 links and mispredicting 27 of the 225 true negative links as positive links for precision of 18\% and recall of 46\%.

In order to understand why the prompt failed on over half of the true links, we randomly selected one of the false negatives and prompted the model to explain why it responded `no' for the link. For reference, the artifacts in the link are shown in Table \ref{tab:investigated_links} in the row for Link \#1.  

\assistant{...The first paragraph is very high level, while the second paragraph goes into more technical implementation details. If they were related, you would expect a more consistent level of technical depth...}

Inspired by this response, we inserted ``Ignoring the different abstraction levels of the artifacts..." before our question. Interestingly, this link was correctly predicted when we used the modified prompt. Throughout our experiments, this would be a re-occurring problem where the model often failed to predict links between artifacts with different levels of details/abstractions. We therefore began instructing it to ignore abstraction levels in many of our following prompts. When we re-ran this modified prompt on the dataset, it correctly identified 7 out of the 13 links (one more than before) but also predicted 86 false positives. We again prompted it to explain its decision for one of the remaining false negatives (Link \#2 in \ref{tab:investigated_links}). Its response and our subsequent questioning are shown below:

\assistant{...while there is a small overlap in the error handling mention, the main substance and purposes of the artifacts are distinct. I would still assess them as not strongly related...}

\human{Would there be a traceability link between artifact 1 and 2?}

\assistant{There could potentially be a weak traceability link between the two artifacts based on the minor overlap in error handling/queueing...}

We found that asking it directly if the artifacts were linked received a better response, and further prompting revealed that Claude had a relatively robust understanding of traceability, thus we decided to update our prompt as follows: 
\human{Is there a traceability link between (1) and (2)?}

 We suspected that this prompt would be more specific, potentially eliminating some of the false positives, and this was indeed the case with Claude only predicting one false positive. However, it now correctly identified only 4 of the 13 links. This time when questioned about why it incorrectly believed Link \#3 (Table \ref{tab:investigated_links}) was un-traced, Claude's confusion seemed to come from it not realizing the artifacts were talking about the same system:

\assistant{
 ...The first requirement mentions specific tasks...that need to report heartbeats, while the second requirement talks about tasks in general without specifying which tasks. So it's not clear if the tasks mentioned in the first requirement are the same as those referred to in the second...}

We found this to be another common point of confusion for Claude so we updated our prompt to include ``Below are artifacts from the same software system" at the start of our prompt. 
\human{Below are artifacts from the same software system. Is there a traceabilty link between (1) and (2)?}
With this additional information, Claude identified all of the project trace-links but wrongly predicted 98 negative links to be positive. Since Claude now mistakenly believed many artifacts were linked, we randomly selected a false positive (Link \#4 in Table \ref{tab:investigated_links}) to investigate further. This lead us to gain the following insight into the misprediction: 
 
\assistant{
Yes, there is a traceability link between (1) and (2). (1) mentions that the DPU-TMALI places errors on an error queue for the DPU-CCM. \newline \newline (2) describes the error collection and reporting mechanism of the DPU-CCM, specifically referring to enqueueing errors. \newline \newline So (2) provides an implementation detail for the error queueing functionality briefly mentioned in (1). Therefore, there is a traceability link between these two artifacts in the software system.}


We found nothing in Claude's response to be inherently wrong, so we speculated that the mistake came not from a misunderstanding but rather from a misalignment between Claude and the tracers of CM1 on what should be considered a trace-link. Thus, we began to generate prompts that focused on the specific relationships between the requirements and regulations of CM1.
\human{Below are artifacts from the same software system. Ignoring different levels of abstraction, can (2) be decomposed from (1)? Answer `yes' or `no'.}

This led to the model identifying 8 of the trace links with only 14 false positives. These missed trace links seemed to result from a confusion surrounding the model's understanding of decomposition, where the model believed that no new information could be added to a decomposed requirement. This was indicated by its response below for Link \#1:

\assistant{... So (2) contains new information that cannot be derived or decomposed from (1) alone. The answer is 'no'. }

Replacing the question with ``does (2) fulfill (1)?" resulted in the model missing 7 links for a similar reason, including Link \#1 again:

\assistant{...(2) lacks critical details and scope needed to conclusively fulfill (1)...}

Despite missing 7 links, the model was able to correctly rule out all but 3 true negatives. Although both of these prompts were too restrictive (i.e. not all the true links were captured), they seemed to align Claude with at least some of the relationships established in CM1 as indicated by its success at eliminating false links.

Some other prompts we tried included:
\begin{enumerate}[label=\arabic*.]
    \item \textit{are (1) and (2) part of the same feature and share functionality?}
    \item \textit{does (2) implement system functionality from (1)?}
    \item \textit{are (1) and (2) dependent on the same system capability?}
\end{enumerate}
where each prompt began with ``Below are artifacts from the same software system. Ignoring different levels of abstraction" and concluded with "Answer `yes' or `no'." Each subsequent prompt successfully predicted one more positive link than the one before, but led to an increasing number of false positives ranging from 28 all the way to 48 for \#3. 

Our final prompt of this structure was a combination of several previous prompts that had shown promise:

\human{Below are artifacts from the same software system. Ignoring different levels of abstraction, can (2) be one of the hierarchical decompositions of (1)? Consider whether (2) implements a part of (1).}

By adding ``one of the hierarchical" and ``a part of", we hoped to make the prompt less restrictive so that the model would allow new information to be added to decomposed artifacts. This time the model correctly predicted all but one true link. Its 44 false positives still fell short of our goal of less than 20 but were significantly less than the 98 returned in the previous approach that achieved these high levels of recall.

\subsection{Introduction of Chain-of-Thought Reasoning}
As our prompt evolved, we saw improvements in the model's link-prediction abilities, but the results were still falling short of our goals. It seemed evident that we needed to approach the task differently. As a result, we decided to break down the task into steps to aid the model in reasoning about the relationship between the artifacts before arriving at a final decision. This process, known as "chain of thought", has previously shown to be effective in helping LLMs solve complex tasks \cite{wei_chain--thought_2023}. To test the impact of incorporating reasoning into the process, we reran our original prompt with this additional step at the beginning of the prompt:

\human{You are working on a software system and are given a high-level requirement and a low-level requirement. Your job is to determine if they are traced. Give one reason why (1) might be related to (2) enclosed in \brackets{related}\brackets{/related} and one reason why (1) might be un-related to (2) enclosed in \brackets{unrelated}\brackets{/unrelated} but ignore levels of abstraction. Then based on these reasons, do you think (1) is related to (2)?  Answer should be 'yes' or 'no' enclosed in \brackets{answer}\brackets{/answer}.}

Having it enclose each separate response in xml tags allowed us to easily parse its answer despite the added complexity. An example of the model's response for link \#5 in Table \ref{tab:investigated_links} is shown below:

\assistant{\brackets{related} Both specify requirements related to processing commands within a certain time \brackets{/related}
\brackets{unrelated} Requirement 1 specifies a time constraint while requirement 2 provides a general description of a command and control subsystem\brackets{/unrelated} \brackets{answer}yes\brackets{/answer}} 

Recall increased by nearly 50\% while precision rose by 14\%. With the added reasoning step, the model missed only 1 true link and misidentified just 25 false positives. With both questions (1) and (3) from Table \ref{tab:cm1_prompt_metrics}, the introduction of incremental reasoning helped the model find a higher portion of the true connections in the data. Due to time constraints, we were unable to test the reasoning on the remaining questions but we believe this is an interesting avenue for future work.

Encouraged by this initial success, we decided to have the model answer each of our questions as intermediate steps before finally determining whether the artifacts were related. We hoped this approach would help the model explore different ways in which the artifacts could be connected. It also allowed us to use a simple ranking system in which more `yes' responses would increase the likelihood that the artifacts were linked. By quantifying the model's degree of support for a relationship through the ranking system, we could evaluate not just whether it predicted a link but also how confident it was in that prediction based on the reasoning exhibited in its responses. 

\human{I am giving you two software artifacts from a system.
Your job is to determine if there is a traceability link.
Answer whether (2) implements a part of (1) with yes or no enclosed in \brackets{implements}\brackets{/implements}.
Answer whether (2) is a hierarchical decomposition of (1) with yes or no enclosed in \brackets{decomposed}\brackets{/decomposed}.
Answer whether (2) fulfills (1) with yes or no enclosed in \brackets{fulfills}\brackets{/fulfills}.
Answer whether (2) and (1) are part of the same feature and shares functionality with yes or no enclosed in \brackets{feature}\brackets{/feature}.
Answer whether (2) and (1) are dependent on the same system capability with yes or no enclosed in \brackets{capability}\brackets{/capability}.
Use your answers to give one reason why (1) might be related to (2) enclosed in \brackets{related}\brackets{/related}
and one reason why (1) might be un-related to (2) enclosed in \brackets{unrelated}\brackets{/unrelated}
Now answer is (1) related to (2) with yes or no enclosed in \brackets{traced}\brackets{/traced}.}

\subsection{Ranking Prompt Discovery}
Despite not outperforming other classification prompts, ranking the artifacts by the number of `yes' and `no' answers, did provide the opportunity to establish a threshold retrospectively, allowing us to categorize items based on the strength of the model's prediction instead of relying on a single yes/no choice. This, combined with Claude's new 100k context window, inspired us to experiment with an entirely new strategy.

For our next experiment, we gave Claude the following instructions:
\human{
\# Task \newline 
Rank all related artifacts from most to least related to the source.\newline \newline
Source: [SOURCE ARTIFACT] \newline \newline \# Artifacts \newline \newline \brackets{artifact}\newline 
\brackets{id}...\brackets{/id}\newline
\brackets{body}...\brackets{/body}\newline
\brackets{/artifact} \newline \newline
\# Instructions \newline Rank the artifact bodies from most to least relevant to the source. Provide the ranked artifacts as comma delimited list of artifact ids where the first element relates to the source the most and the last element does so the least. 
}

\input{tables/cm1_prompt_metrics}

By providing the model with more context about the system in the prompt and allowing it to compare all targets when making its decision, we hoped to see a performance boost. Unfortunately, the task was not as simple as we had hoped, and we, like previous researchers, identified another nuance with the prompts - order matters \cite{qin_large_2023}. When we presented the target artifacts in a random order, performance was barely above random; however, ordering artifacts that were more likely to be linked at the top, delivered significantly higher performance. It seemed that unless there was some pattern already established, the task would overwhelm the model. Because of this, we decided to rank the target artifacts based on their VSM similarity to the source. Then, we presented the model with targets in this order. With this initialization, the model improved upon the original VSM ranking.  Furthermore, While discussions throughout the paper have focused on the CM1 dataset, we applied this approach to the three other datasets presented in Table \ref{tab:datasets} and report results for all four datasets in Table \ref{tab:performance_metrics}.

\subsection{Summary of Results}
Overall, our results demonstrated that the ranking task could be a useful approach to automated traceability, but it may require additional steps and further prompt refinement to reach the necessary performance. In the future, we plan to explore ways of decomposing the overall task into simpler, incremental steps to reduce complexity for the model as we did for the classification task. It should also be noted that the ranking task necessitated a large context window, which may pose a challenge for certain open-source models. Consequently, classification remains a valuable alternative when ranking is infeasible. Furthermore, classification opens up avenues for diverse applications of traceability, such as ``trace views" that we discuss further in Section \ref{sec:conclusion}. 

