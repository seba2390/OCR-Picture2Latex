\section{Current State of the Art}

Up until quite recently, all existing 3PCF algorithms scaled as $\mathcal{O}(N^3)$. However, very recent work by~\cite{SE3ptalg} (summarized below) developed a new approach to the isotropic 3PCF, using Legendre polynomials and radial coefficients as a basis for the isotropic 3PCF's dependence on triangle opening angle and side lengths. This algorithm showed that using spherical harmonic (SH) expansions of the density field around each galaxy in the survey, the 3PCF's multipole moments (another name for the radial expansion coefficients) could be obtained scaling as $\mathcal{O}(N^2$).  


In this section we first summarize developments previous to the Legendre approach. We then briefly discuss the Legendre approach for the isotropic 3PCF.
Finally, we outline previous work on high-performance implementations of the 2PCF and 3PCF.

\subsection{State of the art prior to Legendre polynomial approach}
Prior to the development of the Legendre polynomial approach, 3PCF algorithms \cite{Moore2001, Gray2004, Jarvis2004, Nichol2006, Gardner2007, March2013} fundamentally scaled as the number of possible triangles in a survey. More precisely, for a 3PCF measurement out to some maximum scale $R_{\rm max}$, there would be $N(nV_{R_{\rm max}})^2$ , with $n$ the survey number density and $V_{R_{\rm max}}$ the spherical volume within $R_{\rm max}$. 

\cite{Moore2001, Gray2004, Jarvis2004, Nichol2006, Gardner2007, March2013} trace the development of a 3PCF algorithm based on k-d trees, most recently described in \cite{March2013}. This algorithm used ``marked'' k-d trees that cached additional information: the number of galaxies within each node of the tree as well as the bounding box of the node. 

While somewhat faster than a naive triplet count, the k-d tree-based algorithms are most effective if the typical galaxy separation is much smaller than the desired binning in triangle side length used to report the 3PCF. If this is the case, then many galaxies can be handled at once using a node of the k-d tree without going further down the tree. 

However, typical galaxy surveys geared for cosmology are sparse. For example,  the Baryon Oscillation Spectroscopic Survey (BOSS) has an average separation between galaxies of $13~\text{Mpc}/h$, but the desired bin width is typically $\sim\! 10~\text{Mpc}/h$. Consequently k-d tree algorithms do not perform well for cosmological-scale 3PCF measurements. 
The largest-scale use-case tested in \cite{March2013} is a triangle configuration  with three sides of $5.6~\text{Mpc}/h$  each, significantly smaller than the large triangles ($\sim\!50-200~\text{Mpc}/h$) most useful for cosmology. Furthermore, these k-d tree algorithms still fundamentally scale as $N(nV_{\rm R_{\rm max}})^2\propto N^3$ \cite{March2013}. 


\subsection{Legendre polynomial approach}

The current state of the art algorithm to measure the isotropic 3PCF is presented in \cite{SE3ptalg}. This algorithm formulates the isotropic 3PCF as
\begin{equation*}
\zeta(r_1, r_2;\hat{r}_1\cdot\hat{r}_2) = \sum_{\ell}\zeta_{\ell}(r_1,r_2)P_{\ell}(\hat{r}_1\cdot\hat{r}_2),
\end{equation*}
where $P_{\ell}$ is a Legendre polynomial. We wish to obtain the radial coefficients (``multipole moments'') $\zeta_{\ell}$, which describe the 3PCF's dependence on triangle side lengths $r_1$ and $r_2$.

The algorithm estimates the multipole moments around each galaxy in the survey, and averages them. For each galaxy in the survey, the algorithm bins the density around it into spherical shells, corresponding to bins in triangle side length. It then expands the angular clustering on each shell into spherical harmonics. Using the spherical harmonic addition theorem, one can then sum the spherical harmonics  over spins to recover the multipole moments. This algorithm scales as $\mathcal{O}(N^2)$.

We emphasize that the Legendre basis is symmetric under rotations by construction because they preserve the dot product $\hat{r}_1\cdot\hat{r}_2$. Thus the Legendre polynomial-based algorithm does not track {\it anisotropies} in the 3PCF. 

However, as  discussed in Section~\ref{sec:overview}, redshift-space distortions induce anisotropies that carry significant information on the growth rate of structure and hence the theory of gravity. The new algorithm implemented in this work  generalizes the Legendre polynomial approach to include anisotropies, as described in ~\cite{SE3ptAniso} and in Section~\ref{sec:a3PCF}. 


\subsection{Current SoA implementations}
To date, the Legendre polynomial algorithm for the isotropic 3PCF provides the best baseline comparison for the current work. However we caution that the algorithm in this work tracks different quantities that offer additional information over the isotropic 3PCF, so the quantitative comparison should serve only as a guide.

The performance numbers for the Legendre algorithm quoted in \cite{SE3ptalg} used a dataset consisting of 642,619 randomly distributed particles in a realistic sky survey geometry; the calculation for the true distribution of galaxies would likely have similar performance. In that work, the isotropic 3PCF 
was run in 170 seconds on a 6-core 4.2 GHz i7-3930K. The core computation of the multipoles sustains about $30\%$ of peak performance for this processor, and the full code sustains about $21\%$ of peak. 
The implementation presented in \cite{SE3ptalg} included a number of optimizations to exploit AVX registers, as well as a simple gridding scheme to accelerate the finding of all secondary galaxies within $R_{\rm max}$ of a given primary.


In this work, we use our implementation of the novel anisotropic 3PCF algorithm  to process a dataset 3 orders of magnitude larger, on 4 orders of magnitude more nodes. There have been no published attempts to measure the 3PCF at scale on massively parallel architectures. 
However, we do note that previous work has exploited of GPU capabilities for the 2PCF, focusing on the brute force calculation of the distances between all galaxy pairs ~\cite{Bard2012,Ponce2012}. 
Unpublished work using GPUs to accelerate the brute-force 3PCF on GPUs also exists~\cite{Bard2014,Bellis2015}, but has not been scaled to datasets beyond 50,000 galaxies or beyond a single GPU. 

We are aware of one significant effort to optimize the 2PCF for HPC machines~\cite{Chhugani2012} that focused on a novel SIMD-friendly algorithm for histogram updates. This effort calculated the 2PCF for a dataset of 1.7 billion galaxies in 5.3 hours, using 25,600 Intel Xeon cores. 

