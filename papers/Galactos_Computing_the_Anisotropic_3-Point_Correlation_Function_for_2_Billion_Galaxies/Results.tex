\section{Performance Results}
\subsection{Single node performance}
\label{sec:single-node-performance}

We discuss the single-node performance of Galactos here.
Figure~\ref{fig:single-node} shows the performance breakdown of the code
running the Outer Rim dataset with 225,000 galaxies on a single node. Of the
portions shown in the figure, only the k-d tree construction (which includes
partitioning and halo exchange) involves MPI communication. Hence, the code is
bound by single node performance. We see that the majority of the time ($55\%$)
is spent in the multipole accumulation function. This function (described in
Section \ref{subsec:single-node-optimizations}) calculates the 286 multipole
contributions of each pair and is vectorized over pairs. From
\ref{subsubsec:vectorization} we see that a pair of galaxies consumes 576 FLOPS; we
measure empirically that each pair in the k-d tree search contributes roughly
37 FLOPs, leading to an average of 609 FLOPs per galaxy pair for the entire
computation. The efficiency of the single-node code is bound by the instruction
mix (ratio of store to FMA instructions) and the size of the vector register
file. The multipole accumulation function achieves 1017 GF in double precision,
which is $39\%$ of a single node's peak performance. We note that the multipole
accumulation function runs entirely in double precision, but the k-d tree
search is performed in single-precision due to its insensitivity to the precision
of galaxy locations.

\begin{figure}
\centering
\includegraphics[width=\columnwidth]{Figures/single-node-breakdown.pdf}
    \caption{Runtime breakdown of single node performance on the Outer Rim 225,000 galaxy dataset.}
\label{fig:single-node}
\end{figure}




\subsubsection{Thread Scaling}

Figure \ref{fig:thread-scaling} shows the thread scaling of the full application on 10,000 Outer Rim galaxies on a single node. As we increase the number of physical cores from 1 to 68, we achieve very good scaling with the time-to-solution decreasing by $58\times$. However, for a given number of physical cores, we see only a marginal improvement ($35\%$) in performance when using hyperthreading.
In fact, the performance of the k-d tree search deteriorates slightly when using hyperthreading. Overall, we achieve $65\times$ thread scaling when comparing 272 threads to 1 thread.

\begin{figure}
\centering
\includegraphics[width=\columnwidth]{Figures/thread-scaling.pdf}
    \caption{Thread scaling on a single Xeon Phi node with 68 cores, using 10,000 galaxies from the Outer Rim dataset. Each plot indicates a different number of hyperthreads ("HT") active on each physical core. The rightmost data point represents code performance on 68 cores, but is unlabeled on the x-axis for visual clarity.}
\label{fig:thread-scaling}
\end{figure}




\subsection{Weak scaling}
\label{sec:weak-scaling}

The performance of the Galactos algorithm is sensitive to the average number density of galaxies.
Therefore, in order to capture the ``true'' performance behavior of the algorithm on smaller problem sets for weak scaling measurements, we constructed problem sets with the same number density as the full Outer Rim dataset (roughly 0.071 galaxies $[\text{Mpc}/h]^{-3}$). 
The sizes of these problem sets are shown in Table~\ref{tab:weak_scaling_problem_sets}.

Figure~\ref{fig:weak-scaling} depicts the weak scaling behavior of Galactos on Cori.
With a 32-fold increase in the number of nodes---from 128 to 8,192 nodes---the time to solution increases by 9\%.
That Galactos exhibits such efficient weak scaling is expected, since is it designed to decompose the problem in order to eliminate communication between MPI processes during the bulk of the computation (as described in Section \ref{subsec:multinode_scaling}).


\begin{figure}
\centering
\includegraphics[width=\columnwidth]{Figures/WeakScaling_r90_mixed_prec.pdf}
    \caption{Weak scaling of Galactos code on Cori, using the Outer Rim datasets shown in Table~\ref{tab:weak_scaling_problem_sets}.
             The wall clock times reported here measure end-to-end code execution time, including I/O, k-d tree construction and querying, and multipole computation.}
\label{fig:weak-scaling}
\end{figure}





\subsection{Strong scaling}
\label{sec:strong-scaling}

To measure the strong scaling behavior of Galactos, we use the problem set created for 128 compute nodes (28.8 million galaxies, $734.5~\text{Mpc}/h$ cubic box length; see Table~\ref{tab:weak_scaling_problem_sets}), and compute the 3PCF using 128 to 8,192 nodes.
The results are shown in Figure~\ref{fig:strong-scaling}.

We see that the algorithm strong scales efficiently.
Increasing the number of nodes by 64$\times$ (128 to 8,192 nodes) reduces the time-to-solution by a factor of 27 (994s to 37s).
This result illustrates the efficient load balancing scheme of the k-d tree implementation in Galactos. 
The small deviation from perfect strong scaling is largely due to load imbalances induced by short range variations in the galaxy density---the number of primary galaxies per node was balanced to 0.1\%, but we observed up to 60\% variation in the number of primary/secondary pairs when sub-dividing the 128-node dataset between larger numbers of nodes.
In contrast, the weak scaling runs exhibited less than 10\% variation in the number of primary/secondary pairs per node, which is consistent with the expectation that the density of galaxies becomes more uniform on larger length scales.



\begin{figure}
\centering
\includegraphics[width=\columnwidth]{Figures/StrongScaling_r90_mixed_prec.pdf}
    \caption{Strong scaling of Galactos code on Cori, using the Outer Rim dataset corresponding to 128 nodes (28.8 million galaxies; see Table~\ref{tab:weak_scaling_problem_sets}).}
\label{fig:strong-scaling}
\end{figure}


\subsection{Time to solution and peak performance for Outer Rim dataset}

We ran Galactos over the full Cori system on all 9,636 available nodes in both
mixed and double precision. (In mixed precision, the k-d tree is computed in
single precision and everything else is in double precision.) We can estimate
the sustained FLOPS rate for Galactos by extrapolating our empirical
measurements of FLOPS per pair from Section~\ref{sec:single-node-performance}.
The time to solution to compute the 3PCF for 2 billion galaxies in mixed
precision is 982.4 sec; in pure double precision, the time to solution is
1070.6 sec. (This is slightly faster than expected from our weak scaling study,
since we based our weak scaling datasets on a conservative estimate of $9,000$
available compute nodes. In our full-system runs each compute node consequently
processed the 3PCF for less than 225,000 primary galaxies.) In the full Outer
Rim calculation there are \num{8.17e15} galaxy pairs. Therefore, with the k-d
tree search running in double precision, we achieve an overall FLOPS rate of
4.65~PF; in mixed precision mode, we achieve 5.06~PF. There is a 9\%
improvement in overall runtime in mixed precision mode.

As a sanity check, we see that the node with the smallest number of galaxy
pairs (\num{7.06e11}) in the full run completed its multipole computation in
644.2~\si{\second}. Assuming a flop rate of 1.017~TF, we can estimate that
the node spends $61\%$ of its runtime in the multipole accumulation kernel. For
the node with the largest number of galaxy pairs (\num{9.88e11}), the
corresponding fraction is $58\%$. Note that these agree very well with the
$55\%$ measured in Section \ref{sec:single-node-performance}.

Although we obtained a precise measurement of peak performance on a single node
(1.017~TF, or 39\% of peak in double precision), extrapolating this value to
obtain overall peak application performance is non-trivial. A direct
extrapolation would suggest a peak FLOPS rate of 9.8~PF across 9636 nodes; in
reality, the actual peak was likely only marginally higher than the sustained
average of 5.06~PF (in mixed precision). This is because the multipole addition
kernel execution is highly asynchronous among nodes -- each node executes the
kernel as soon as it finds 128 galaxy pairs within its domain of the k-d tree
-- and the probability that a significant number of nodes are executing that
kernel simultaneously is vanishingly small. One could adjust the algorithm to
obtain a higher peak FLOPS rate by enlarging the galaxy ``bucket'' size such
that each node executes the multipole kernel only after it has found \emph{all}
possible galaxy pairs, thus ensuring that all nodes are executing the kernel at
once. However, this approach would also require a larger memory footprint,
introduce unnecessary synchronization, and exhibit a lower overall FLOPS rate.
