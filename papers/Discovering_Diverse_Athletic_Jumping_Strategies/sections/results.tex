\input{figures/highjump-strategy-2}
\input{figures/obstacle-jump-strategy-1}
\input{figures/obstacle-jump-strategy-2}
\input{figures/strategies.tex}

\input{figures/peak-poses}
\input{figures/compare-mocap}

\input{figures/high-jump-variations}
\input{figures/ablation}
\input{figures/mars}

\section{Results}
\label{sec:results}

We demonstrate multiple strategies discovered through our framework for high jumping and obstacle jumping in Section~\ref{sec:Experiments-Diverse-Strategies}. We validate the effectiveness of BDS and P-VAE in Section~\ref{sec:Experiments-Comparison-and-Ablation-Study}. Comparison with motion capture examples, and interesting variations of learned policies are given in Section~\ref{sec:Results-variations}. All results are best seen in the supplementary videos in order to judge the quality of the synthesized motions.

\subsection{Diverse Strategies}
\label{sec:Experiments-Diverse-Strategies}

\subsubsection{High Jumps}\label{sec:results:high-jump}
In our experiments, six different high jump strategies are discovered during the Stage 1 initial state exploration within the first ten BDS samples: Fosbury Flop, Western Roll (facing up), Straddle, Front Kick, Side Jump, Side Dive. The first three are high jump techniques standard in the sports literature. The last three strategies are not commonly used in sporting events, but still physically valid so we name them according to their visual characteristics. The other four samples generated either repetitions or failures. Strategy repetitions are generally not avoidable due to model errors and large flat regions in the motion space. Since the evaluation of one BDS sample takes about six hours, the Stage 1 exploration takes about 60 hours in total. The discovered distinct strategies at $z_\text{freeze}=100cm$ are further optimized separately to reach their maximum difficulty level, which takes another 20 hours. Representative take-off state feature values of the discovered strategies can be found in Appendix~\ref{app:takeoffStates}. {We also show the DRL learning and curriculum scheduling curves for two strategies in Appendix~\ref{app:curves}.}

In Stage 2, we perform novel policy search for five DRL iterations from each good initial state of Stage 1. Training is warm started with the associated Stage 1 policy for efficient learning. The total time required for Stage 2 is roughly 60 hours. More strategies are discovered in Stage 2, but most are repetitions and only two of them are novel strategies not discovered in Stage 1: Western Roll (facing sideways) and Scissor Kick. Western Roll (sideways) shares the same initial state with Western Roll (up). Scissor Kick shares the same initial state with Front Kick. The strategies discovered in each stage are summarized in Figure~\ref{fig:strategies}. We visualize all eight distinct strategies in Figure~\ref{fig:teaser} and Figure~\ref{fig:highJumps}. We also visualize their peak poses in Figure~\ref{fig:High-jump-peak-poses}.

While the final learned control policies are stochastic in nature, the majority of the results shown in our supplementary video are the deterministic version of those policies, i.e., using the mean of the learned policy action distributions. In the video we further show multiple simulations from the final stochastic policies, to help give insight into the true final endpoint of the optimization. As one might expect for a difficult task such as a maximal-height high jump, these stochastic control policies will also fail for many of the runs, similar to a professional athlete.

\subsubsection{Obstacle Jumps}\label{sec:results:obstacle-jump}
Figure~\ref{fig:obstacleJumps1} shows the six different obstacle jump strategies discovered in Stage 1 within the first 17 BDS samples: Front Kick, Side Kick, Twist Jump (clockwise), Twist Jump (counterclockwise), Straddle and Dive Turn. More strategies are discovered in Stage 2, but only two of them are novel as shown in Figure~\ref{fig:obstacleJumps2}: Western Roll and Twist Turn. Western Roll shares the initial state with Twist Jump (clockwise). Twist Turn shares the initial state with Dive Turn. The two stages together take about 180 hours. We encourage readers to watch the supplementary video for better visual perception of the learned strategies.

Although our obstacle jump task is not an Olympic event, it is analogous to a long jump in that it seeks to jump a maximum-length jumped. 
Setting the obstacle height to zero yields a standard long jump task. The framework discovers several strategies, including one similar to the standard long jump adopted in competitive events, with the strong caveat that the distance achieved is limited by the speed of the run up. Please refer to the supplementary video for the long jump results.

\subsection{Validation and Ablation Study}
\label{sec:Experiments-Comparison-and-Ablation-Study}

\subsubsection{BDS vs. Random Search}
\label{sec:comparison-bds-random-search}
We validate the sample efficiency of BDS compared with a random search baseline. Within the first ten samples of initial states exploration in the high jump task, BDS discovered six distinct strategies as discussed in Section~\ref{sec:results:high-jump}. Given the same computational budget, random search only discovered three distinct strategies: Straddle, Side Jump, and one strategy similar to Scissor Kick. Most samples result in repetitions of these three strategies, due to the presence of large flat regions in the strategy space where different initial states lead to the same strategy. In contrast, BDS largely avoids sampling the flat regions thanks to the acquisition function for diversity optimization and guided exploration of the surrogate model.

\subsubsection{Motion Quality with/without P-VAE}
We justify the usage of P-VAE for improving motion naturalness with results shown in Figure~\ref{fig:ablation}. Without P-VAE, the character can still learn physically valid skills to complete the tasks successfully, but the resulting motions usually exhibit unnatural behavior. In the absence of a natural action space constrained by the P-VAE, the character can freely explore any arbitrary pose during the course of the motion to complete the task, which is unlikely to be within the natural pose manifold all the time.

\subsection{Comparison and Variations}
\label{sec:Results-variations}

\subsubsection{Synthesized High Jumps vs. Motion Capture}
\label{sec:synthesized-mocap}
We capture motion capture examples from a university athlete in a commercial motion capture studio for three well-known high jump strategies: Scissor Kick, Straddle, and Fosbury Flop. We retarget the mocap examples onto our virtual athlete, which is shorter than the real athlete as shown in Table~\ref{tb:modelParams}. We visualize keyframes sampled from our simulated jumps and the retargeted mocap jumps in Figure~\ref{fig:compare-mocap}. Note that the bar heights are set to the maximum heights achievable by our trained policies, while the bar heights for the mocap examples are just the bar heights used at the actual capture session. We did not set the mocap bar heights at the athlete's personal record height, as we wanted to ensure his safety and comfort while jumping in a tight mocap suit with a lot of markers on.

\subsubsection{High Jump Variations}
\label{sec:synthesized-variations}

In addition to discovering multiple motion strategies, our framework can easily support physically valid motion variations. We show four high jump variations generated from our framework in Figure~\ref{fig:variations}. We generate the first three variations by taking the initial state of the Fosbury Flop strategy discovered in Stage 1, and retrain the jumping policy with additional constraints starting from  a random initial policy. Figure~\ref{fig:variation-weakerLeg} shows a jump with a weaker take-off leg, where the torque limits are reduced to $60\%$ of its original values. Figure~\ref{fig:variation-inflexibleSpine} shows a character jumping with a spine that does not permit backward arching. Figure~\ref{fig:variation-fixedKnee} shows a character jumping with a fixed knee joint. All these variations clear lower maximum heights, and are visually different from the original Fosbury Flop in Figure~\ref{fig:highjump-flop}. For the jump in Figure~\ref{fig:variation-landonFoot}, we take the initial state of the Front Kick, and train with an additional constraint that requires landing on feet. In Figure~\ref{fig:High-jump-mars} we also show a high jump trained on Mars, where the gravity $g=3.711m/s^2$ is lower, from the initial state of the Fosbury flop discovered on Earth.



 
