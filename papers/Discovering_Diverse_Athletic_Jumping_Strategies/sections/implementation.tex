\section{Task Setup and Implementation}
\label{sec:Experiments}

We demonstrate diverse strategy discovery for two challenging motor tasks: high jumping and obstacle jumping. We also tackle several variations of these tasks. We describe task specific settings in Section~\ref{sec:Experiments-Task-Setup}, and implementation details in Section~\ref{sec:Experiments-Implementation}. 

\subsection{Task Setup}
\label{sec:Experiments-Task-Setup}
The high jump task follows the Olympics rules, where the simulated athlete takes off with one leg, clears the crossbar, and lands on a crash mat. We model the landing area as a rigid block for simplicity. The crossbar is modeled as a rigid wall vertically extending from the ground to the target height to prevent the character from cheating during early training, i.e., passing through beneath the bar. A rollout is considered successful and terminated when the character lands on the rigid box with all body parts at least $20$ $cm$ away from the wall. A rollout is considered as a failure and terminated immediately, if any body part touches the wall, or any body part other than the take-off foot touches the ground, or if the jump does not succeed within two seconds after the take-off.

The obstacle jump shares most of the settings of the high jump. The character takes off with one leg, clears a box-shaped obstacle of $50$ $cm$ in height with variable widths, then lands on a crash mat. The character is required to complete the task within two seconds as well, and not allowed to touch the obstacle with any body part.

\subsubsection{Run-up Learning}
\label{sec:Experiments-Runup}
A full high jump or obstacle jump consists of three phases: run-up, take-off and landing. Our framework described so far can discover good initial states at take-off that lead to diverse jumping strategies. What is lacking is the matching run-up control policies that can prepare the character to reach these good take-off states at the end of the run. We train the run-up controllers with DeepMimic \cite{Peng:2018:DeepMimic}, where the DRL learning reward consists of a task reward and an imitation reward. The task reward encourages the end state of the run-up to match the desired take-off state of the jump. The imitation reward guides the simulation to match the style of the reference run. We use a curved sprint as the reference run-up for high jump, and a straight sprint for the obstacle jump run-up. For high jump, the explored initial state space is four-dimensional: the desired approach angle $\alpha$, the $X$ and $Z$ components of the root angular velocity $\omega$, and the magnitude of the $Z$ component of the root linear velocity $v_z$ in a facing-direction invariant frame. We fix the desired root $Y$ angular velocity to $3 \text{rad}/\text{s}$, which is taken from the reference curved sprint. In summary, the task reward $r_\text{G}$ for the run-up control of a high jump is defined as
\begin{equation}
    r_\text{G} = \text{exp}\left(-\frac{1}{3}\cdot||\omega - \bar{\omega}||_1 - 0.7\cdot(v_z - \bar{v}_z)^2\right),
\end{equation}
where $\bar{\omega}$ and $\bar{v}_z$ are the corresponding targets for $\omega$ and $v_z$. $\alpha$ does not appear in the reward function as we simply rotate the high jump suite in the environment to realize different approach angles. For the obstacle jump, we explore a three-dimensional take-off state space consisting of the root angular velocities along all axes. Therefore the run-up control task reward $r_\text{G}$ is given by
\begin{equation}
    r_\text{G} = \text{exp}(-\frac{1}{3}\cdot||\omega - \bar{\omega}||_1).
\end{equation}

\input{tables/curriculum}
\subsubsection{Reward Function}
\label{sec:Experiments-Reward}
We use the same reward function structure for both high jumps and obstacle jumps, where the character gets a sparse reward only when it successfully completes the task. The full reward function is defined as in Equation~\ref{eq:stage1-reward} for Stage 1. For Stage 2, the novelty bonus $r_\text{novelty}$ as discussed in Section~\ref{sec:methods-phase2} is added:
\begin{equation}
    r = r_\text{task}\cdot r_\text{naturalness}\cdot r_\text{novelty}.
    \label{eq:stage2-reward}
\end{equation}
$r_\text{naturalness}$ is the motion naturalness term discussed in Section~\ref{sec:methods-PVAE}. For both stages, the task reward consists of three terms:
\begin{equation}
    r_\text{task} = r_{\text{complete}} \cdot r_{\omega} \cdot r_\text{safety}.
\end{equation}
$r_\text{complete}$ is a binary reward precisely corresponding to task completion. $r_{\omega} = \text{exp}(-0.02 ||\omega||)$ penalizes excessive root rotations where $||\omega||$ is the average magnitude of the root angular velocities across the episode. $r_\text{safety}$ is a term to penalize unsafe head-first landings. We set it to $0.7$ for unsafe landings and $1.0$ otherwise. $r_\text{safety}$ can also be further engineered to generate more landing styles, such as a landing on feet as shown in Figure~\ref{fig:variations}.

\subsubsection{Curriculum and Scheduling}
\label{sec:Experiments-Curriculum}
The high jump is a challenging motor skill that requires years of training even for professional athletes. We therefore adopt curriculum-based learning to gradually increase the task difficulty $z$, defined as the crossbar height in high jumps or the obstacle width in obstacle jumps. Detailed curriculum settings are given in Table~\ref{tb:curriculum}, where $z_\text{min}$ and $z_\text{max}$ specify the range of $z$, and $\Delta z$ is the increment when moving to a higher difficulty level.

We adaptively schedule the curriculum to increase the task difficulty according to the DRL training performance. At each training iteration, the average sample reward is added to a reward accumulator. We increase $z$ by $\Delta z$ whenever the accumulated reward exceeds a threshold $R_T$, and then reset the reward accumulator. Detailed settings for $\Delta z$ and $R_T$ are listed in Table~\ref{tb:curriculum}. The curriculum could also be scheduled following a simpler scheme adopted in \cite{Xie2020allsteps}, where task difficulty is increased when the average sample reward in each iteration exceeds a threshold. We found that for athletic motions, such average sample reward threshold is hard to define uniformly for different strategies in different training stages.

Throughout training, the control frequency $f_\text{control}$ and the P-VAE offset penalty coefficient $c_\text{offset}$ in Equation \ref{eq:r-pvae} are also scheduled according to the task difficulty, in order to encourage exploration and accelerate training in early stages. We set $f_\text{control} = 10 + 20 \cdot \text{Clip}(\rho, 0, 1)$ and $c_\text{offset} = 48 - 33 \cdot \text{Clip}(\rho, 0, 1)$, where $\rho = 2z-1$ for high jumps and $\rho = z$ for obstacle jumps. We find that in practice the training performance does not depend sensitively on these hyperparameters.

\input{tables/model_parameter}

\subsubsection{Strategy Features}
\label{sec:Experiments-Strategy-Features}
We choose low-dimensional and visually discriminate features $f$ of learned strategies for effective diversity measurement of discovered strategies. In the sports literature, high jump techniques are usually characterized by the body orientation when the athlete clears the bar at his peak position. The rest of the body limbs are then coordinated in the optimal way to clear the bar as high as possible. Therefore we use the root orientation when the character's CoM lies in the vertical crossbar plane as $f$. This three-dimensional root orientation serves well as a Stage 2 feature for high jumps. For Stage 1, this feature can be further reduced to one dimension, as we will show in Section \ref{sec:Experiments-Diverse-Strategies}. More specifically, we only measure the angle between the character's root direction and the global up vector, which corresponds to whether the character clears the bar facing upward or downward. Such a feature does not require a non-Euclidean GP output space that we need to handle in Stage 1. We use the same set of features for obstacle jumps, except that root orientations are measured when the character's CoM lies in the center vertical plane of the obstacle.

Note that it is not necessary to train to completion, i.e., the maximum task difficulty, to evaluate the feature diversity, since the overall jumping strategy usually remains unchanged after a given level of difficulty, which we denote by $z_\text{freeze}$. Based on empirical observations, we terminate the training after reaching $z_\text{freeze}=100cm$ for both high jump and obstacle jump tasks for strategy discovery. 

\subsubsection{GP Priors and Kernels}
\label{sec:Experiments-GP-Priors-Kernels}
We set GP prior $m(\cdot)$ and kernel $k(\cdot,\cdot)$ for BDS based on common practices in the Bayesian optimization literature. Without any knowledge on the strategy feature distribution, we set the prior mean $m(\cdot)$ to be the mean of the value range of a feature. Among the many common choices for kernel functions, we adopt the Mat\'ern$^5/_2$ kernel \cite{matern1960spatial,klein2017fast}, defined as:
\begin{equation}
    k_{^5/_2}(x, x') = \theta(1 + \sqrt{5}d_{\lambda}(x,x') + \frac{5}{3}d^{2}_{\lambda}(x, x'))e^{-\sqrt{5}d_{\lambda}(x, x')} 
\end{equation}
where $\theta$ and $\lambda$ are learnable parameters of the GP. 
$d_{\lambda}(x,x') = (x-x')^{T}\diag(\lambda)(x-x')$ is the Mahalanobis distance.

\subsection{Implementation}
\label{sec:Experiments-Implementation}

We implemented our system in PyTorch \cite{PyTorch} and PyBullet \cite{coumans2016pybullet}. The simulated athlete has 28 internal DoFs and 34 DoFs in total. We run the simulation at 600~$Hz$. {Torque limits for the hips, knees and ankles are taken from Biomechanics estimations for a human athlete performing a Fosbury flop \cite{Okuyama03}. Torque limits for other joints are kept the same as \cite{Peng:2018:DeepMimic}. Joint angle limits are implemented by penalty forces.} We captured three standard high jumps from a university athlete, whose body measurements are given in Table~\ref{tb:modelParams}. For comparison, we also list these measurements for our virtual athlete. 

For DRL training, we set $\lambda=0.95$ for both TD($\lambda$) and GAE($\lambda$). We set the discounter factor $\gamma=1.0$ since our tasks have short horizon and sparse rewards. The PPO clip threshold is set to $0.02$. The learning rate is $2.5\times 10^{-5}$ for the policy network and $1.0\times 10^{-2}$ for the value network. In each training iteration, we sample 4096 state-action tuples in parallel and perform five policy updates with a mini-batch size of 256. For Stage 1 diverse strategy discovery, we implement BDS using GPFlow \cite{GPflow2020multioutput-gp} with both $N_\text{exp}$ and $N_\text{opt}$ set to three. $d_\text{threshold}$ in Stage 2 novel policy seeking is set to $\pi/2$. Our experiments are performed on a Dell Precision 7920 Tower workstation, with dual Intel Xeon Gold 6248R CPUs (3.0 GHz, 48 cores) and an Nvidia Quadro RTX 6000 GPU. Simulations are run on the CPUs. One strategy evaluation for a single initial state, i.e. Line 9 in Algorithm~\ref{algorithm::BDS}, typically takes about six hours. Network updates are performed on the GPU.