\section{LEARNING NATURAL STRATEGIES}
\label{sec:discovery}
Given a character model, an environment, and a task objective, we aim to learn feasible natural-looking motion strategies using deep reinforcement learning. We first describe our DRL formulation in Section~\ref{sec:methods-DRL-formulation}. To improve the learned motion quality, we propose a Pose Variational Autoencoder (P-VAE) to constrain the policy actions in Section~\ref{sec:methods-PVAE}.

\subsection{DRL Formulation}
\label{sec:methods-DRL-formulation}
Our strategy learning task is formulated as a standard reinforcement learning problem, where the character interacts with the environment to learn a control policy which maximizes a long-term reward. The control policy $\pi_\theta(a|s)$ parameterized by $\theta$ models the conditional distribution over action $a \in \mathcal{A}$ given the character state $s \in \mathcal{S}$. At each time step $t$, the character interacts with the environment with action $a_t$ sampled from $\pi(a|s)$ based on the current state $s_t$. The environment then responds with a new state $s_{t+1}$ according to the transition dynamics $p(s_{t+1}|s_t,a_t)$, along with a reward signal $r_t$. The goal of reinforcement learning is to learn the optimal policy parameters $\theta^*$ which maximizes the expected return defined as
\begin{equation}
    J(\theta) = \mathbb{E}_{\tau\sim p_\theta(\tau)}\left[
        \sum_{t=0}^T{\gamma^t r_t}
    \right],
\end{equation}
where $T$ is the episode length, $\gamma \leq 1$ is a discount factor, and $p_\theta(\tau)$ is the probability of observing trajectory $\tau = \{s_0, a_0, s_1, ..., a_{T-1}, s_T\}$ given the current policy $\pi_\theta(a|s)$.

\paragraph{States}
The state $s$ describes the character configuration. We use a similar set of pose and velocity features as those proposed in DeepMimic \cite{Peng:2018:DeepMimic}, including relative positions of each link with respect to the root, their rotations parameterized in quaternions, along with their linear and angular velocities. Different from DeepMimic, our features are computed directly in the global frame without direction-invariant transformations for the studied jump tasks. The justification is that input features should distinguish states with different relative transformations between the character and the environment obstacle such as the crossbar. In principle, we could also use direction-invariant features as in DeepMimic, and include the relative transformation to the obstacle into the feature set. However, as proved in \cite{Ma19}, there are no direction-invariant features that are always singularity free. Direction-invariant features change wildly whenever the character's facing direction approaches the chosen motion direction, which is usually the global up-direction or the $Y$-axis. For high jump techniques such as the Fosbury flop, singularities are frequently encountered as the athlete clears the bar facing upward. Therefore, we opt to use global features for simplicity and robustness. Another difference from DeepMimic is that time-dependent phase variables are not included in our feature set. Actions are chosen purely based on the dynamic state of the character.

\paragraph{Initial States}
\label{sec:methods-initial-states}
The initial state $s_0$ is the state in which an agent begins each episode in DRL training. We explore a chosen low-dimensional feature space ($3\sim4D$) of the take-off states for learning diverse jumping strategies. As shown by previous work \cite{ma2021spacetime}, the take-off moment is a critical point of jumping motions, where the volume of the feasible region of the dynamic skill is the smallest. In another word, bad initial states will fail fast, which in a way help our exploration framework to find good ones quicker. Alternatively, we could place the agent in a fixed initial pose to start with, such as a static pose before the run-up. This is problematic for several reasons. First, different jumping strategies need different length for the run-up. The planar position and facing direction of the root is still a three dimensional space to be explored. Second, the run-up strategies and the jumping strategies do not correlate in a one-to-one fashion. Visually, the run-up strategies do not look as diverse as the jumping strategies. Lastly, starting the jumps from a static pose lengthens the learning horizon, and makes our learning framework based on DRL training even more costly. Therefore we choose to focus on just the jumping part of the jumps in this work, and leave the run-up control learning to DeepMimic, which is one of the state-of-the-art imitation-based DRL learning methods. More details are given in Section~\ref{sec:Experiments-Runup}.    

\paragraph{Actions}
The action $a$ is a target pose described by internal joint rotations. We parameterize 1D revolute joint rotations by scalar angles, and 3D spherical joint rotations by exponential maps \cite{Grassia:1998:ExpMap}. Given a target pose and the current character state, joint torques are computed through the Stable Proportional Derivative (SPD) controllers \cite{Tan:2011:SPD}. Our control frequency $f_{\text{control}}$ ranges from 10 $Hz$ to 30 $Hz$ depending on both the task and the curriculum. For challenging tasks like high jumps, it helps to quickly improve initial policies through stochastic evaluations at early training stages. A low-frequency policy enables faster learning by reducing the needed control steps, or in another word, the dimensionality and complexity of the actions $(a_0,...,a_T)$. This is in spirit similar to the 10 $Hz$ control fragments used in SAMCON-type controllers \cite{Liu16}. Successful low-frequency policies can then be gradually transferred to high-frequency ones according to a curriculum to achieve finer controls and thus smoother motions. We discuss the choice of control frequency in more detail in Section~\ref{sec:Experiments-Curriculum}.
\paragraph{Reward}
We use a reward function consisting of the product of two terms for all our strategy discovery tasks as follows:
\begin{equation}
\label{eq:stage1-reward}
    r = r_{\text{task}} \cdot r_{\text{naturalness}} 
\end{equation}    
where $r_{\text{task}}$ is the task objective and $r_{\text{naturalness}}$ is a naturalness reward term computed from the P-VAE to be described in Section \ref{sec:methods-PVAE}. For diverse strategy discovery, a simple $r_{\text{task}}$ which precisely captures the task objective is preferred. For example in high jumping, the agent receives a sparse reward signal at the end of the jump after it successfully clears the bar. In principle, we could transform the sparse reward into a dense reward to reduce the learning difficulty, such as to reward CoM positions higher than a parabolic trajectory estimated from the bar height. However in practice, such dense guidance reward can mislead the training to a bad local optimum, where the character learns to jump high in place rather than clears the bar in a coordinated fashion. Moreover, the CoM height and the bar height may not correlate in a simple way. For example, the CoM passes underneath the crossbar in Fosbury flops. As a result, a shaped dense reward function could harm the diversity of the learned strategies. We will discuss reward function settings for each task in more details in Section~\ref{sec:Experiments-Reward}.

\paragraph{Policy Representation}
We use a fully-connected neural network parameterized by weights $\theta$ to represent the control policy $\pi_\theta(a|s)$. Similar to the settings in \cite{Peng:2018:DeepMimic}, the network has two hidden layers with $1024$ and $512$ units respectively. ReLU activations are applied for all hidden units. Our policy maps a given state $s$ to a Gaussian distribution over actions $a=\mathcal{N}(\mu(s), \Sigma)$. The mean $\mu(s)$ is determined by the network output. The covariance matrix $\Sigma=\sigma I$ is diagonal, where $I$ is the identity matrix and $\sigma$ is a scalar variable measuring the action noise. We apply an annealing strategy to linearly decrease $\sigma$ from $0.5$ to $0.1$ in the first $1.0\times 10^7$ simulation steps, to encourage more exploration in early training and more exploitation in late training.

\paragraph{Training}
We train our policies with the Proximal Policy Optimization (PPO) method \cite{Schulman:2017:PPO}. PPO involves training both a policy network and a value function network. The value network architecture is similar to the policy network, except that there is only one single linear unit in the output layer. We train the value network with TD($\lambda$) multi-step returns. We estimate the advantage of the PPO policy gradient by the Generalized Advantage Estimator GAE($\lambda$) \cite{Generalized-Advantage-Estimation}.

\subsection{Pose Variational Autoencoder}\label{sec:methods-PVAE}
The dimension of natural human poses is usually much lower than the true degrees of freedom of the character model. We propose a generative model to produce natural PD target poses at each control step. More specifically, we train a Pose Variational Autoencoder (P-VAE) from captured natural human poses, and then sample its latent space to produce desired PD target poses for control. Here a pose only encodes internal joint rotations without the global root transformations. We use publicly available human motion capture databases to train our P-VAE. Note that none of these databases consist of high jumps or obstacle jumps specifically, but they already provide enough poses for us to learn the natural human pose manifold successfully.

\paragraph{P-VAE Architecture and Training}
Our P-VAE adopts the standard Beta Variational Autoencoder ($\beta$-VAE) architecture \cite{beta-VAE}. The encoder maps an input feature $x$ to a low-dimensional latent space, parameterized by a Gaussian distribution with a mean $\mu_x$ and a diagonal covariance $\Sigma_x$. The decoder maps a latent vector sampled from the Gaussian distribution back to the original feature space as $x'$. The training objective is to minimize the following loss function:
\begin{equation}
    \mathcal{L} = \mathcal{L}_\text{MSE}(x,x') + \beta \cdot \text{KL}(\mathcal{N}(\mu_x, \Sigma_x), \mathcal{N}(0, I)),
\end{equation}
where the first term is the MSE (Mean Squared Error) reconstruction loss, and the second term shapes the latent variable distribution to a standard Gaussian by measuring their Kulback-Leibler divergence. We set $\beta = 1.0\times10^{-5}$ in our experiments, so that the two terms in the loss function are within the same order of magnitude numerically.

We train the P-VAE on a dataset consisting of roughly $20,000$ poses obtained from the CMU and SFU motion capture databases. We include a large variety of motion skills, including walking, running, jumping, breakdancing, cartwheels, flips, kicks, martial arts, etc. The input features consist of all link and joint positions relative to the root in the local root frames, and all joint rotations with respect to their parents. We parameterize joint rotations by a $6$D representation for better continuity, as described in \cite{zhou2019continuity, Ling20}.

We model both the encoder and the decoder as fully connected neural networks with two hidden layers, each having 256 units with $tanh$ activation. We perform PCA (Principal Component Analysis) on the training data and choose $d_{\text{latent}} = 13$ to cover $85\%$ of the training data variance, where $d_{\text{latent}}$ is the dimension of the latent variable. We use the Adam optimizer to update network weights \cite{kingma2014adam}, with the learning rate set to $1.0\times10^{-4}$. Using a mini-batch size of $128$, the training takes $80$ epochs within $2$ minutes on an NVIDIA GeForce GTX 1080 GPU and an Intel i7-8700k CPU. We use this single pre-trained P-VAE for all our strategy discovery tasks to be described.

\paragraph{Composite PD Targets}
PD controllers provide actuation based on positional errors. So in order to reach the desired pose, the actual target pose needs to be offset by a certain amount. Such offsets are usually small to just counter-act the gravity for free limbs. However, for joints that interact with the environment, such as the lower body joints for weight support and ground takeoff, large offsets are needed to generate powerful ground reaction forces to propel the body forward or into the air. Such complementary offsets combined with the default P-VAE targets help realize natural poses during physics-based simulations. Our action space $\mathcal{A}$ is therefore $d_{\text{latent}} + d_{\text{offset}}$ dimensional, where $d_{\text{latent}}$ is the dimension of the P-VAE latent space, and $d_{\text{offset}}$ is the dimension of the DoFs that we wish to apply offsets for. We simply apply offsets to all internal joints in this work. Given $a=(a_{\text{latent}},a_{\text{offset}}) \in \mathcal{A}$ sampled from the policy $\pi_\theta(a|s)$, where $a_{\text{latent}}$ and $a_{\text{offset}}$ correspond to the latent and offset part of $a$ respectively, the final PD target is computed by $D_{\text{pose}}(a_{\text{latent}}) + a_{\text{offset}}$. Here $D_{\text{pose}}(\cdot)$ is a function that decodes the latent vector $a_{\text{latent}}$ to full-body joint rotations. We minimize the usage of rotation offsets by a penalty term as follows:
\begin{equation}
\label{eq:r-pvae}
    r_{\text{naturalness}} = 1 - \clip\left(\left(\frac{||a_{\text{offset}}||_1}{c_{\text{offset}}}\right)^2, 0, 1\right) ,
\end{equation}
where $c_{\text{offset}}$ is the maximum offset allowed. For tasks with only a sparse reward signal at the end, $||a_{\text{offset}}||_1$ in Equation~\ref{eq:r-pvae} is replaced by the average offset norm $\frac{1}{T}\sum_{t=0}^T{||a^{(t)}_{\text{offset}}||_1}$ across the entire episode. We use $L1$-norm rather than the commonly adopted $L2$-norm to encourage sparse solutions with fewer non-zero components \cite{tibshirani1996regression, chen2001atomic}, as our goal is to only apply offsets to essential joints to complete the task while staying close to the natural pose manifold prescribed by the P-VAE.