\appendix

\input{tables/take-off-states}

\begin{figure}[t]
    \centering
    \begin{subfigure}[b]{\linewidth}
        \centering
        \scalebox{0.56}{\input{images/learning-curves/rwd.pgf}}
    \end{subfigure}
    \begin{subfigure}[b]{\linewidth}
        \centering
        \scalebox{0.56}{\input{images/learning-curves/curriculum.pgf}}
    \end{subfigure}
    \caption{{Stage 1 DRL learning and curriculum scheduling curves for two high jump strategies. As DRL learning is stochastic, the curves shown are the average of five training runs. The shaded regions indicates the standard deviation.}}
    \label{fig:learning-curves}
\end{figure}

\section{Representative Take-off State Features}
\label{app:takeoffStates}
We list representative take-off state features discovered through BDS in Table~\ref{tb:take-off-states} for high jumps and Table~\ref{tb:take-off-states-box} for obstacle jumps. The approach angle $\alpha$ for high jumps is defined as the wall orientation in a facing-direction invariant frame. The orientation of the wall is given by the line $x\text{sin}\alpha - z\text{cos}\alpha = 0$. 

\section{Learning Curves}
\label{app:curves}
{We plot Stage 1 DRL learning and curriculum scheduling curves for two high jump strategies in Figure~\ref{fig:learning-curves}. An initial solution for the starting bar height $0.5m$ can be learned relatively quickly. After a certain bar height has been reached (around $1.4m$), the return starts to drop  because larger action offsets are needed to jump higher, which decreases the $r_{naturalness}$ in Equation~\ref{eq:r-pvae} and therefore the overall return in Equation~\ref{eq:stage1-reward}. Subjectively speaking, the learned motions remain as natural for high crossbars, as the lower return is due to the penalty on action offsets.}
