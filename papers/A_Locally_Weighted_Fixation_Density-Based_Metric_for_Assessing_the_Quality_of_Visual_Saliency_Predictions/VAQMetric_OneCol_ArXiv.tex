
%% bare_jrnl.tex
%% V1.4
%% 2012/12/27
%% by Michael Shell
%% see http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.8 or later) with an IEEE journal paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/tex-archive/macros/latex/contrib/IEEEtran/
%% and
%% http://www.ieee.org/



% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. IEEE's font choices can trigger bugs that do  ***
% *** not appear when using other class files.                            ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/


%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE! 
%% User assumes all risk.
%% In no event shall IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%
%% File list of work: IEEEtran.cls, IEEEtran_HOWTO.pdf, bare_adv.tex,
%%                    bare_conf.tex, bare_jrnl.tex, bare_jrnl_compsoc.tex,
%%                    bare_jrnl_transmag.tex
%%*************************************************************************

% Note that the a4paper option is mainly intended so that authors in
% countries using A4 can easily print to A4 and see how their papers will
% look in print - the typesetting of the document will not typically be
% affected with changes in paper size (but the bottom and side margins will).
% Use the testflow package mentioned above to verify correct handling of
% both paper sizes by the user's LaTeX system.
%
% Also note that the "draftcls" or "draftclsnofoot", not "draft", option
% should be used if it is desired that the figures are to be displayed in
% draft mode.
%
%\documentclass[journal,onecolumn]{IEEEtran}
\documentclass[12pt,onecolumn,journal,	draftclsnofoot]{IEEEtran}
%\documentclass[12pt,onecolumn,journal]{IEEEtran}
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[journal]{../sty/IEEEtran}

% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/tex-archive/macros/latex/contrib/oberdiek/
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.






% *** CITATION PACKAGES ***
%
%\usepackage{cite}
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off
% such as if a citation ever needs to be enclosed in parenthesis.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 4.0 (2003-05-27) and later if using hyperref.sty. cite.sty does
% not currently provide for hyperlinked citations.
% The latest version can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/cite/
% The documentation is contained in the cite.sty file itself.






% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
 \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation
% can be obtained at: 
% http://www.ctan.org/tex-archive/macros/latex/required/graphics/
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found at:
% http://www.ctan.org/tex-archive/info/epslatex/
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex





% *** MATH PACKAGES ***
%
%\usepackage[cmex10]{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics. If using
% it, be sure to load this package with the cmex10 option to ensure that
% only type 1 fonts will utilized at all point sizes. Without this option,
% it is possible that some math symbols, particularly those within
% footnotes, will be rendered in bitmap form which will result in a
% document that can not be IEEE Xplore compliant!
%
% Also, note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/amslatex/math/





% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/algorithms/
% There is also a support site at:
% http://algorithms.berlios.de/index.html
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/tex-archive/macros/latex/contrib/algorithmicx/




% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/tools/


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.




% *** SUBFIGURE PACKAGES ***
%\ifCLASSOPTIONcompsoc
%  \usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
%\else
%  \usepackage[caption=false,font=footnotesize]{subfig}
%\fi
% subfig.sty, written by Steven Douglas Cochran, is the modern replacement
% for subfigure.sty, the latter of which is no longer maintained and is
% incompatible with some LaTeX packages including fixltx2e. However,
% subfig.sty requires and automatically loads Axel Sommerfeldt's caption.sty
% which will override IEEEtran.cls' handling of captions and this will result
% in non-IEEE style figure/table captions. To prevent this problem, be sure
% and invoke subfig.sty's "caption=false" package option (available since
% subfig.sty version 1.3, 2005/06/28) as this is will preserve IEEEtran.cls
% handling of captions.
% Note that the Computer Society format requires a larger sans serif font
% than the serif footnote size font used in traditional IEEE formatting
% and thus the need to invoke different subfig.sty package options depending
% on whether compsoc mode has been enabled.
%
% The latest version and documentation of subfig.sty can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/subfig/




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure. The latest version and documentation can be found at:
% http://www.ctan.org/tex-archive/macros/latex/base/


%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/sttools/
% Do not use the stfloats baselinefloat ability as IEEE does not allow
% \baselineskip to stretch. Authors submitting work to the IEEE should note
% that IEEE rarely uses double column equations and that authors should try
% to avoid such use. Do not be tempted to use the cuted.sty or midfloat.sty
% packages (also by Sigitas Tolusis) as IEEE does not format its papers in
% such ways.
% Do not attempt to use stfloats with fixltx2e as they are incompatible.
% Instead, use Morten Hogholm'a dblfloatfix which combines the features
% of both fixltx2e and stfloats:
%
% \usepackage{dblfloatfix}
% The latest version can be found at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/dblfloatfix/




%\ifCLASSOPTIONcaptionsoff
%  \usepackage[nomarkers]{endfloat}
% \let\MYoriglatexcaption\caption
% \renewcommand{\caption}[2][\relax]{\MYoriglatexcaption[#2]{#2}}
%\fi
% endfloat.sty was written by James Darrell McCauley, Jeff Goldberg and 
% Axel Sommerfeldt. This package may be useful when used in conjunction with 
% IEEEtran.cls'  captionsoff option. Some IEEE journals/societies require that
% submissions have lists of figures/tables at the end of the paper and that
% figures/tables without any captions are placed on a page by themselves at
% the end of the document. If needed, the draftcls IEEEtran class option or
% \CLASSINPUTbaselinestretch interface can be used to increase the line
% spacing as well. Be sure and use the nomarkers option of endfloat to
% prevent endfloat from "marking" where the figures would have been placed
% in the text. The two hack lines of code above are a slight modification of
% that suggested by in the endfloat docs (section 8.4.1) to ensure that
% the full captions always appear in the list of figures/tables - even if
% the user used the short optional argument of \caption[]{}.
% IEEE papers do not typically make use of \caption[]'s optional argument,
% so this should not be an issue. A similar trick can be used to disable
% captions of packages such as subfig.sty that lack options to turn off
% the subcaptions:
% For subfig.sty:
% \let\MYorigsubfloat\subfloat
% \renewcommand{\subfloat}[2][\relax]{\MYorigsubfloat[]{#2}}
% However, the above trick will not work if both optional arguments of
% the \subfloat command are used. Furthermore, there needs to be a
% description of each subfigure *somewhere* and endfloat does not add
% subfigure captions to its list of figures. Thus, the best approach is to
% avoid the use of subfigure captions (many IEEE journals avoid them anyway)
% and instead reference/explain all the subfigures within the main caption.
% The latest version of endfloat.sty and its documentation can obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/endfloat/
%
% The IEEEtran \ifCLASSOPTIONcaptionsoff conditional can also be used
% later in the document, say, to conditionally put the References on a 
% page by themselves.




% *** PDF, URL AND HYPERLINK PACKAGES ***
%
%\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/url/
% Basically, \url{my_url_here}.




% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}
 \usepackage{amsmath}
% \usepackage{mathtools}
\usepackage{subfig}
\usepackage{pgfplots}
 \usepackage{amsfonts} 
\usepackage{rotating}
\usepackage{array}
\usepackage{epstopdf}
\usepackage{tikz} % Necessary package for tikz
%\usepackage{printlen}
%\usepackage{layouts}
\usepackage{booktabs}
\usepackage{float}
\usepackage[final]{changes}
%\usepackage{changes}
\usepackage{soul}
\usetikzlibrary{plotmarks}
\pgfplotsset{ % Here we specify options for all figures in the document
	compat=1.7, % Which version of pgfplots do we want to use?
	legend style = {font=\small\sffamily}, % Legends in a sans-serif font
	label style = {font=\small\sffamily} % Labels in a sans-serif font
}
\newcolumntype{C}{ >{\centering\arraybackslash} m{2cm} }
\newcolumntype{D}{ >{\centering\arraybackslash} m{0.2cm} }


\newlength\figureheight
\newlength\figurewidth

% Alter some LaTeX defaults for better treatment of figures:
% See p.105 of "TeX Unbound" for suggested values.
% See pp. 199-200 of Lamport's "LaTeX" book for details.
%   General parameters, for ALL pages:
\renewcommand{\topfraction}{0.9}	% max fraction of floats at top
\renewcommand{\bottomfraction}{0.8}	% max fraction of floats at bottom
%   Parameters for TEXT pages (not float pages):
\setcounter{topnumber}{2}
\setcounter{bottomnumber}{2}
\setcounter{totalnumber}{4}     % 2 may work better
\setcounter{dbltopnumber}{2}    % for 2-column pages
\renewcommand{\dbltopfraction}{0.9}	% fit big float above 2-col. text
\renewcommand{\textfraction}{0.07}	% allow minimal text w. figs
%   Parameters for FLOAT pages (not text pages):
\renewcommand{\floatpagefraction}{0.7}	% require fuller float pages
% N.B.: floatpagefraction MUST be less than topfraction !!
\renewcommand{\dblfloatpagefraction}{0.7}	% require fuller float pages

% remember to use [htp] or [htpb] for placement
%\tikzset{every picture/.style={font issue=\tiny},
%	font issue/.style={execute at begin picture={#1\selectfont}}
%}
\begin{document}
\definechangesauthor[name={Milind Gide}, color=red]{MG}
\setremarkmarkup{(#2 -- #1)}
%
% paper title
% can use linebreaks \\ within to get better formatting as desired
% Do not put math or special symbols in the title
\title{A Locally Weighted Fixation Density-Based Metric for Assessing the Quality of Visual Saliency Predictions}
%
%
% author names and IEEE memberships
% note positions of commas and nonbreaking spaces ( ~ ) LaTeX will not break
% a structure at a ~ so this keeps an author's name from being broken across
% two lines.
% use \thanks{} to gain access to the first footnote area
% a separate \thanks must be used for each paragraph as LaTeX2e's \thanks
% was not built to handle multiple paragraphs
%

\author{Milind~S.~Gide       
        and~Lina~J.~Karam,~\IEEEmembership{Fellow,~IEEE}% <-this % stops a space
\thanks{M.~S.~Gide and L.~J.~Karam are with the School
	of Electrical, Computer and Energy Engineering, Arizona State University, Tempe, AZ, 85287-5706 USA~(e-mail:~mgide,~karam@asu.edu)}% <-this % stops a space
%\thanks{L. Karam is with the Department of Electrical and Computer Engineering, Arizona State University, Tempe, AZ, 85281 USA~(e-mail:~karam@asu.edu)}% <-this % stops a space
%\thanks{Manuscript received April 19, 2005; revised December 27, 2012.}
}

% note the % following the last \IEEEmembership and also \thanks - 
% these prevent an unwanted space from occurring between the last author name
% and the end of the author line. i.e., if you had this:
% 
% \author{....lastname \thanks{...} \thanks{...} }
%                     ^------------^------------^----Do not want these spaces!
%
% a space would be appended to the last name and could cause every name on that
% line to be shifted left slightly. This is one of those "LaTeX things". For
% instance, "\textbf{A} \textbf{B}" will typeset as "A B" not "AB". To get
% "AB" then you have to do: "\textbf{A}\textbf{B}"
% \thanks is no different in this regard, so shield the last } of each \thanks
% that ends a line with a % and do not let a space in before the next \thanks.
% Spaces after \IEEEmembership other than the last one are OK (and needed) as
% you are supposed to have spaces between the names. For what it is worth,
% this is a minor point as most people would not even notice if the said evil
% space somehow managed to creep in.



% The paper headers
\markboth{Final version published in IEEE Transactions on Image Processing 2016}%
{Shell \MakeLowercase{\textit{et al.}}: Bare Demo of IEEEtran.cls for Journals}
% The only time the second header will appear is for the odd numbered pages
% after the title page when using the twoside option.
% 
% *** Note that you probably will NOT want to include the author's ***
% *** name in the headers of peer review papers.                   ***
% You can use \ifCLASSOPTIONpeerreview for conditional compilation here if
% you desire.




% If you want to put a publisher's ID mark on the page you can do it like
% this:
%\IEEEpubid{0000--0000/00\$00.00~\copyright~2012 IEEE}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.



% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}




% make the title area
\maketitle

% As a general rule, do not put math, special symbols or citations
% in the abstract or keywords.
\begin{abstract}
%		 \printinunitsof{in}\prntlen{\textwidth}
With the increased focus on visual attention (VA) in the last decade, a large number of computational visual saliency methods have been developed over the past few years. These models are traditionally evaluated by using performance evaluation metrics that quantify the match between predicted saliency and fixation data obtained from eye-tracking experiments on human observers. Though a considerable number of such metrics have been proposed in the literature, there are notable problems in them. In this work, we discuss shortcomings in existing metrics through illustrative examples and propose a new metric that uses local weights based on fixation density which overcomes these flaws. To compare the performance of our proposed metric  at assessing the quality of saliency prediction with other existing metrics, we construct a ground-truth subjective database in which saliency maps obtained from 17 different VA models are evaluated by 16 human observers on a 5-point categorical scale in terms of their visual resemblance with corresponding ground-truth fixation density maps obtained from eye-tracking data. The metrics are evaluated by correlating metric scores with the human subjective ratings. The correlation results show that the proposed evaluation metric outperforms all other popular existing metrics. Additionally, the constructed database and corresponding subjective ratings provide an insight into which of the existing metrics and future metrics are better at estimating the quality of saliency prediction and can be used as a benchmark. 
\end{abstract}

% Srenivas abstract for reference 
%This paper presents a no-reference perceptual
%metric that quantiﬁes the degree of perceived regularity in
%textures. The metric is based on the similarity of visual
%attention (VA) of the textural primitives and the periodic spatial
%distribution of foveated ﬁxation regions throughout the image.
%A ground-truth eye-tracking database for textures is also gener-
%ated as part of this paper and is used to evaluate the performance
%of the most popular VA models. Using the saliency map generated
%by the best VA model, the proposed texture regularity metric
%is computed. It is shown through subjective testing that the
%proposed metric has a strong correlation with the mean opinion
%score for the perceived regularity of textures. The proposed
%texture regularity metric can be used to improve the quality
%and performance of many image processing applications like
%texture synthesis, texture compression, and content-based image
%retrieval

%

% Note that keywords are not normally used for peerreview papers.
\begin{IEEEkeywords}
Visual Attention, Saliency, Quality Assessment, Visual Attention Models.
\end{IEEEkeywords}

% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle



\section{Introduction}
% The very first letter is a 2 line initial drop letter followed
% by the rest of the first word in caps.
% 
% form to use if the first word consists of a single letter:
% \IEEEPARstart{A}{demo} file is ....
% 
% form to use if you need the single drop letter followed by
% normal text (unknown if ever used by IEEE):
% \IEEEPARstart{A}{}demo file is ....
% 
% Some journals put the first two words in caps:
% \IEEEPARstart{T}{his demo} file is ....
% 
% Here we have the typical use of a "T" for an initial drop letter
% and "HIS" in caps to complete the first word.
\IEEEPARstart{V}{isual} attention (VA) is the broad area of research that aims to explain the mechanisms by which the human visual system (HVS) filters the vast amount of visual information captured by the retina.
VA has applications in a large number of diverse areas like object recognition, image segmentation, compression, selective reduced-power visual processing, to name a few. As a result, there has been a lot of focus recently on developing computational VA models.  The VA mechanism is considered to be a combination of instantaneous pre-attentive, bottom-up processes that depend on low-level cues, and much slower, top-down, cognitive processes that depend on high-level precepts. Most of the existing models are based on bottom-up concepts and output what is known as a saliency map that gives the visual importance of each pixel location. Given the large number of VA models to choose from, it is necessary to evaluate these models. VA models are traditionally evaluated by comparing the saliency maps with eye-tracking data that is obtained from human observers. Several performance metrics that objectively quantify the match between the predicted saliency map and eye-tracking data have been introduced over the past decade for evaluating VA models (see ~\cite{borjireview} for a detailed review). A number of studies like those by Toet~\cite{Toet}, Borji et al.~\cite{borjieval}, and Judd et al.~\cite{judd2012benchmark} have evaluated state-of-the-art VA models using a subset of these metrics. However, none of these studies have evaluated the effectiveness of existing metrics in assessing the quality of VA models and ignore certain notable flaws in them.  The motivation for the proposed metric is to provide a more accurate assessment of the quality of visual saliency prediction than existing metrics, which can aid in a better comparative evaluation of VA models.  The proposed metric can also act as an improved measure of cost for training VA models that use machine learning.  Yet another application for the proposed metric is faithful objective comparison of eye-tracking equipments. 
Given the importance of accurate evaluation of VA models, there have been a few papers in recent years that discuss metrics. LeMeur and Baccino~\cite{LeMeur2013} gave an overview of existing performance metrics in literature and discussed their strengths and weaknesses. Riche et al.~\cite{riche2013saliency} provided a taxonomy for existing metrics and also studied the correlation between the metrics. They showed that each metric alone is not sufficient to evaluate a VA model and suggested the use of a combination of metrics to get a better estimate of performance.  Recently, Gide et al.~\cite{MilindSamTPAMI} discussed known flaws in existing metrics  through examples and proposed a metric, $sNSS$, that resolves the center-bias problem in the Normalized Scanpath Saliency ($NSS$) metric~\cite{NSS} through shuffling.  However, none of these works provide a common benchmark to compare the performance metrics. 

The key contribution of this paper is to propose a novel metric that assigns locally adaptive weights to fixation points based on local fixation density and thus gives more importance to the visually relevant fixations in the ground-truth eye-tracking data.  We also address the problem of a lack of a benchmark for evaluating existing and future performance metrics by constructing a subjective database in which ratings on a 5-point categorical scale by human observers are used to rate saliency maps of several VA models based on their visual resemblance to ground-truth saliency maps. The average ratings or mean opinion scores (MOS) are then correlated with the performance metric scores to evaluate the metrics.

This paper is organized as follows. In Section~\ref{sec:shortcomings} we highlight the known problems in existing popular metrics~\cite{mit-saliency-benchmark} through illustrative examples.  We then propose a new metric that uses locally adaptive weights for fixation points  in  Section~\ref{sec:proposedmetrics}. The details of the subjective study are presented in Section~\ref{sec:subjectivestudy}, and the correlation results for the existing and proposed metrics are presented in Section~\ref{sec:results}. Finally, we conclude the paper in Section~\ref{sec:conclusion} and also provide directions for future research. 

\section{Existing metrics and their shortcomings}
\label{sec:shortcomings}
\begin{table}
	\centering
	\footnotesize
	\caption{Evaluated Metrics.}
	\begin{tabular}{|c|c|c|}
		\hline
		Metric Name & Category & Ground-truth \\
		\hline
		$AUC_{Borji}$~\cite{borjieval} & Location-based & Fixation Points \\
		$AUC_{Judd}$~\cite{judd2012benchmark}  & Location-based & Fixation Points\\
		$sAUC$~\cite{borjieval} & Location-based & Fixation Points \\
		$WF_\beta$~\cite{margolin2014evaluate} & Location-based & Fixation Density Map \\
		$NSS$~\cite{borjieval}&Value-based & Fixation Points \\
		$sNSS$~\cite{MilindSamTPAMI}&Value-based & Fixation Points \\
		$CC$~\cite{borjieval}& Distribution-based & Fixation Density Map \\
		$SIM$~\cite{judd2012benchmark}& Distribution-based & Fixation Density Map\\
		$EMD$~\cite{judd2012benchmark}& Distribution-based & Fixation Density Map \\     
		\added[id=MG]{$MAE$~\cite{MAE}} & \added[id=MG]{Distribution-based}& \added[id=MG]{Fixation Density Map}\\
		\hline
	\end{tabular}
	\label{tab:existingmetrics}
\end{table}


Existing metrics can be classified into the following three major categories: value-based, location-based, and distribution-based, depending on the type of similarity measure used to compare the predicted saliency map to the eye-tracking data~\cite{riche2013saliency}. The value-based metrics focus on the predicted saliency map values at fixation points, the location-based metrics focus on how well the salient regions in the predicted saliency maps match with the locations of the fixation points, and the distribution-based metrics focus on the differences in the statistical distributions of the predicted saliency maps and fixation points. In addition to these categories, metrics can also be  classified based on the type  of ground-truth used. Some metrics use only the fixation locations from the eye-tracking data whereas others use a ground-truth saliency map (GSM) which is obtained by convolving a 2D Gaussian with the fixations and normalizing the resulting map. \added[id=MG]{Several recent studies have used different types metrics to benchmark VA models.  Toet~\cite{Toet} evaluated several VA models by using the Spearman's correlation coefficient. More recently, Borji et al.~\cite{borjieval} used the $AUC_{Borji}$, $CC$ and $NSS$ measures and Judd et al.~\cite{judd2012benchmark} used the $AUC_{Judd}$, $Similarity$ and $EMD$ metrics to evaluate several VA models. The MIT saliency benchmark project~\cite{mit-saliency-benchmark} is an up-to-date online benchmarking resource that lists the performance of all the recent state-of-the-art VA models using seven popular evaluation metrics that are a combination of those used in~\cite{borjieval} and~\cite{judd2012benchmark}.} The metrics used by the MIT Saliency Benchmark~\cite{mit-saliency-benchmark} along with recently proposed metrics  $WF_{\beta}$~\cite{margolin2014evaluate} and $sNSS$~\cite{MilindSamTPAMI}  \added[id=MG]{in addition to a baseline metric $MAE$~\cite{MAE}} are listed in Table~\ref{tab:existingmetrics} along with  the categories they belong to, as well as the type of ground-truth used.  
\begin{figure}[t]
	\centering
	\footnotesize
	\begin{tabular}{cccc}
		\includegraphics[width=0.2\textwidth]{figures/AIMSAUC_failcase.jpg} &
		\includegraphics[width=0.2\textwidth]{figures/AIMfdm3.jpg} &
		\includegraphics[width=0.2\textwidth]{figures/AIMCenter.jpg} & 
		\includegraphics[width=0.2\textwidth]{figures/AIMpredmapAIM3.jpg} \\
		\textbf{(a)} Original Image &
		\textbf{(b)} Ground Truth & 
		\textbf{(c)} Centered Gaussian &
		\textbf{(d)} AIM~\cite{AIM} 
	\end{tabular}
	\footnotesize
	\begin{tabular}{|c|c c|}
		%		
		\hline
		Non-shuffled Metrics & (c) & (d) \\ \hline
		$AUC_{Borji}$~\cite{borjieval} & 0.8195 & 0.7509 \\	
		$AUC_{Judd}$~\cite{Judd_2012} & 0.8264 & 0.7760 \\
		%		$AUC_{FDM}$~\cite{LeMeur2013} &  0.7949  &  0.6447\\
		$CC$~\cite{Judd_2012} & 0.4522 & 0.4359 \\ 
		$SIM$~\cite{Judd_2012} & 0.3415 &  0.3398 \\
		%		$KLD_{FDM}$~\cite{borjireview} &  3.6590 & 1.6450 \\
		%		$KLD_{Fix}$~\cite{LeMeur2013} &  11.6082  &  13.2479 \\
		$EMD$~\cite{Judd_2012} &  3.2479 & 3.5294 \\
		$NSS$~\cite{NSS} & 1.2347  & 1.3745 	 \\
		%		$Perc$~\cite{riche2013saliency} & 0.8228 & 0.7688 \\ 
		%		$Pf$~\cite{riche2013saliency} & 0.6991 & 0.0531 \\	
		%		Proposed $WNSS_1$  & 1.4312   & 1.5055 \\
		%		Proposed $WNSS_2$  &  1.7285  &  1.7612 \\
		\hline
		Shuffled metrics & (c) & (d) \\	\hline		
		$sAUC$~\cite{borjieval} & 0.5979 & 0.6689 \\
		%		$SKLD$~\cite{SUN} & 7.4188 & 11.9814 \\
		$sNSS$~\cite{MilindSamTPAMI} &  0.4853 & 0.8911\\
		Proposed $sWNSS$ &  0.2144  & 0.9104  \\
		
		\hline			
	\end{tabular}
	\caption{Center bias problem in existing metrics that is rectified by the shuffled metrics. For $EMD$, a lower score indicates better performance; for the other metrics, a higher score indicates better performance.}
	\label{fig:CenterBias}
\end{figure}

\added[id=MG]{The first notable and well-analyzed problem with existing metrics is the problem of center-bias~\cite{borjireview,MilindSamTPAMI,LeMeur2013}. This problem arises due to an inherent tendency of images and photographs to contain objects of interest in central regions as compared to peripheral regions.  Most metrics that do not factor the center-bias in their formulation tend to incorrectly reward models that independent of content assign higher importance to central regions and lower importance to peripheral regions. One way of tackling this issue is through ``shuffling'' in which ground-truth fixations for all other images in the dataset are randomly sampled and high saliency predictions at such locations are penalized. Consequently, models that blindly reward central regions are penalized to a greater extent by the shuffling process, and receive a much lower score than more discriminative models~\cite{borjireview}. An illustration of the effect of center-bias on shuffled and non-shuffled metrics is shown in Figure~\ref{fig:CenterBias}. As shown in Figure~\ref{fig:CenterBias}, the}
\deleted[id=MG]{The} non-shuffled metrics like $AUC_{Borji}$, $AUC_{Judd}$, $CC$, $EMD$, $SIM$, and  $NSS$ tend to give higher scores to models that assign higher saliency to central regions as compared to the boundaries \deleted[id=MG]{(see Figure~\ref{fig:CenterBias})}. As a result, these incorrectly result in higher performance scores for a centered Gaussian blob (Figure~\ref{fig:CenterBias}(c)) as compared to a saliency map from a VA model (Figure~\ref{fig:CenterBias}(d)). \deleted[id=MG]{One solution to this problem is shuffling the metrics~\cite{SUN}. As seen in Figure~\ref{fig:CenterBias}} \added[id=MG]{On the other hand, the shuffled metrics} \deleted[id=MG]{$sAUC$ assigns}\added[id=MG]{assign} a better score to the AIM~\cite{AIM} saliency map over the centered Gaussian map. 
\begin{figure}[t]
	% shows the interpolation flaw for AUC-borji and SAUC
	\centering
	\footnotesize
	\begin{tabular}{cccc}
		\includegraphics[width=0.2\textwidth]{figures/AUCBorjiSalMaporig.png} & 
		\includegraphics[width=0.2\textwidth]{figures/AUCBorjiROCorig.png} & 
		\includegraphics[width=0.2\textwidth]{figures/AUCJuddROCorig.png} & 
		\includegraphics[width=0.2\textwidth]{figures/AUCShuffROCorig.png}\\
		%	(a) Original Saliency Map  & (b) $AUC_{Borji}$ ROC curve for (a)\\
		\includegraphics[width=0.2\textwidth]{figures/AUCBorjiSalMapblur.png} &
		\includegraphics[width=0.2\textwidth]{figures/AUCBorjiROCblur.png} &
		\includegraphics[width=0.2\textwidth]{figures/AUCJuddROCblur.png} &
		\includegraphics[width=0.2\textwidth]{figures/AUCShuffROCblur.png} \\
		%	 (c) Blurred Saliency Map & (d) $AUC_{Borji}$ ROC curve for (c)
		
	\end{tabular}
	\caption{AUC Interpolation flaw. Top Row: (Left to Right) Ground-truth saliency map with ground-truth fixations and corresponding ROC curves for $AUC_{Borji}$, $AUC_{Judd}$ and $sAUC$. Bottom Row: (Left to Right) Fuzzier version of the ground-truth saliency map with ground-truth fixations and corresponding ROC curves for $AUC_{Borji}$, $AUC_{Judd}$ and $sAUC$.}
	
	\label{fig:interpolationflaw}
\end{figure}

\begin{figure}[t]
	\centering
	\footnotesize
	\begin{tabular}{cc}
		\includegraphics[width=0.3\textwidth]{SamFigures/SNSS/1.jpg} & 
		\includegraphics[width=0.3\textwidth]{SamFigures/SNSS/d1.jpg} \\
		\textbf{(a)}Image &
		\textbf{(b)}Ground-truth	
	\end{tabular}
	\begin{tabular}{ccc}
		\includegraphics[width=0.3\textwidth]{SamFigures/SNSS/sal2.png} & 
		\includegraphics[width=0.3\textwidth]{SamFigures/SNSS/sal1.png} &
		\includegraphics[width=0.3\textwidth]{SamFigures/SNSS/sal3.png} \\
		\textbf{(c)} &
		\textbf{(d)} & 
		\textbf{(e)} 
	\end{tabular}
	\footnotesize
	\begin{tabular}{| c | c c c |}
		\hline
		& (c) & (d) & (e)\\ 
		\hline
		$AUC_{Borji}$~\cite{borjieval} & 0.9104 &  0.9167 &  0.7955 \\
		$AUC_{Judd}$~\cite{Judd_2012} & 0.9164 & 0.9250 & 0.8030 \\
		$sAUC$~\cite{borjieval} & 0.8212 & 0.8113 &  0.4667 \\
		$NSS$~\cite{NSS}& 1.9630 & 2.2586 & 1.0794 \\
		%		Proposed $WNSS_1$  &  2.5263       &     2.2044      &    1.0488      \\
		Proposed $WNSS$ &     2.7984     &   3.1348   &    0.9889      \\
		$sNSS$~\cite{MilindSamTPAMI} &  1.3573 &  1.5864 & -0.1247 \\
		%		Proposed $WSNSS_1$ &   1.6293 & 1.3678 &   -0.1362 \\
		Proposed $sWNSS$ & 2.1901 & 2.5390  &  -0.0273 \\
		\hline		 			
	\end{tabular}
	\caption{The two saliency maps (c) and (d) have nearly identical $sAUC$, $AUC_{Borji}$ and $AUC_{Judd}$ scores, however it is clear that (d) is a much ``better'' saliency map.  $NSS$, $sNSS$  and the proposed $WNSS$ and $sWNSS$ metrics do not have this problem as they assign a significantly higher score to (d) than (c). A centered Gaussian blob (e) will perform well using $NSS$ and the proposed $WNSS$ metrics, however using the $sNSS$ and the proposed shuffled $sWNSS$ the same Gaussian blob receives a low score as expected.}
	\label{fig:snss}
\end{figure}

However, the AUC metrics including the $sAUC$ suffer from \added[id=MG]{another notable flaw} \deleted[id=MG]{what is} known as the interpolation flaw (described in detail in~\cite{margolin2014evaluate}). As seen in Figure~\ref{fig:interpolationflaw}, $AUC_{Borji}$, $AUC_{Judd}$ and $sAUC$ are less sensitive to false-positives. \added[id=MG]{As a result, a ``fuzzy'' ground-truth saliency map created by increasing the background activity in the neighborhood of a true-positive peak incorrectly gets higher or almost similar scores than the actual ground-truth saliency when using the AUC-based metrics.} \deleted[id=MG]{introduced by making the saliency map more fuzzy.}  
The other metrics $NSS$~\cite{NSS}, $CC$~\cite{borjieval}, $EMD$~\cite{mit-saliency-benchmark} and $SIM$~\cite{mit-saliency-benchmark} do not exhibit the interpolation flaw but do suffer from the center-bias problem as seen in Figure~\ref{fig:CenterBias}. %additional results can be shown here, once i fix the center bias problem with wnss i can show a bar chart that shows the percentage of times the nss, cc, emd and sim metrics give a centered map a higher score as compared to state-of-the-art models is higher than those for the shuffled metrics. 
Of these metrics, only $NSS$ is a viable candidate to be shuffled to tackle the center-bias issue as suggested in~\cite{MilindSamTPAMI}.  This metric termed Shuffled NSS or $sNSS$ for short is given by
\begin{equation}
sNSS = NSS(p) - NSS(r)
\label{eq:SNSS}
\end{equation}
\noindent where $p$ and $r$ denote, respectively, the ground-truth fixation points for the image and the randomly sampled non-fixation points from the set of fixation points for other images in the dataset and
\begin{equation}
NSS(x) = \frac{1}{N}\sum\limits_{x\in X}\frac{S(x) - \mu_s}{\sigma_s}.
\label{eq:NSS}
\end{equation}
In (\ref{eq:NSS}), $\mu_s$ and $\sigma_s$ are, respectively, the mean and standard deviation of the predicted saliency map $S$ and $N$ is the number of points in the set $X$. The random sampling for the non-fixation points $r$ is repeated a number of times, typically 100, and the final result is the average of scores obtained for each of these trials. The $sNSS$ metric improves on the $sAUC$ scores by correctly assigning a better score to the saliency map in Figure~\ref{fig:snss}(d) as compared to the one in Figure~\ref{fig:snss}(c). It also improves upon $NSS$ by giving the centered Gaussian map in Figure~\ref{fig:snss}(e) a low score. 

\begin{figure}[t]
	\centering
	\footnotesize
 \resizebox{\textwidth}{!}{
	\begin{tabular}{ccccc}
		\includegraphics[width=0.18\textwidth]{figures/AIMSAUC_failcase.jpg} & 
		\includegraphics[width=0.18\textwidth]{figures/fixPtMapVisual.png} & 
		\includegraphics[width=0.18\textwidth]{figures/AIMfdm3.jpg} &
		\includegraphics[width=0.18\textwidth]{figures/salMapGood.png} & 
		\includegraphics[width=0.18\textwidth]{figures/salMapBad.png} \\ 		
		\textbf{(a)} \added[id=MG]{Original Image} & 
		\textbf{(b)} \added[id=MG]{Fixation Map} & 
		\textbf{(c)} \added[id=MG]{Ground-Truth Saliency} &	\textbf{(d)}\added[id=MG]{ Saliency Map 1} &  
		\textbf{(e)}\added[id=MG]{ Saliency Map 2} \\	
	\end{tabular}}
	\footnotesize
	\begin{tabular}{|c|cc|}
		\hline
		Metric & (d) & (e) \\
		\hline
		$NSS$~\cite{NSS} & 2.4245 & 2.4605 \\ 
		$sNSS$~\cite{MilindSamTPAMI}   & 1.4715 &  1.8223  \\
		Proposed $WNSS$ &  3.1503
		&2.5531 \\
		Proposed $sWNSS$  & 2.2327 & 1.9541 \\
		\hline		
	\end{tabular}
	\caption{Problem with $NSS$ and $sNSS$:  The $NSS$ and $sNSS$ metrics give equal weight to every fixation point and ignore density. As a result, they incorrectly give higher scores to Saliency Map 2 (e) as compared to  Saliency Map 1 (d). The proposed $WNSS$ and its shuffled variant $sWNSS$  weight the fixations based on their local density and assign a higher score to map (d) as expected.}	
	\label{fig:NSSFlaw}	
\end{figure} 
For the $sAUC$, the locations used for determining false-positives are sampled from the distribution of fixations for all other images.  Because of the center-bias inherent in most eye-tracking datasets, these locations tend to be in the central portion of the image. As a result, if false-positives crop up in regions away from the center, $sAUC$ is not able to penalize them. In contrast, because of the zero-mean unit-standard deviation normalization in $sNSS$, blurrier maps are penalized as a result of which $sNSS$ is able to correctly assign a lower score to fuzzy maps such as map (c) in Figure~\ref{fig:snss}. However, a drawback of the  $NSS$ and $sNSS$ metrics is that in their computation all fixations are given equal weights and fixation density is ignored.  Figure~\ref{fig:NSSFlaw} illustrates this drawback through two created saliency maps. Though map (d) is much better than map (e) in Figure~\ref{fig:NSSFlaw}, it gets lower $NSS$ and $sNSS$ scores than map (e).  This happens because in the $NSS$ formulation,  when the normalized saliency values are averaged,  each fixation location contributes equally to the average. 
\section{Proposed Metric}
\label{sec:proposedmetrics}
  Figure\added[id=MG]{~\ref{fig:DBSCANClusters}} illustrates that fixations that are closely clustered together lie on actual objects in the scene and are most important for identifying salient regions as compared to others that are scattered around and lie on background areas.  
  However, $sNSS$ and $NSS$ both do not discriminate between relevant fixations that belong to a dense cluster and represent objects, from fixations  that are sparse and usually fall on background regions and could be considered as outliers. One way to remedy this is to assign weights to each fixation point based on its importance. If $W(p)$ is the weight assigned to  the fixation point $p\in P$, where $P$ is the set of all fixation points, the proposed metric termed as weighted $NSS$ or $WNSS$ for short is defined as 
\begin{equation}
WNSS = \frac{1}{\sum\limits_{p \in P}W(p)}\sum\limits_{p \in P} \frac{W(p)(S(p) - \mu_s)}{\sigma_s} 
\label{eq:WNSS}
\end{equation}
where $\mu_s$ and $\sigma_s$ are the mean and standard deviation, respectively, of the predicted saliency map $S$,  and $P$ denotes the set of ground-truth fixation points for the image.
\begin{figure}[t]
\begin{tabular}{cc}
\subfloat[Original Fixations]{\includegraphics[width=0.45\textwidth]{figures/allfix.png}}
& \subfloat[Clustered Fixations using DBSCAN]{\includegraphics[width=0.45\textwidth]{figures/ClusterWeightsDBSCAN.png}}
\end{tabular}
\caption{\added[id=MG]{(a) Importance of fixation density for identifying relevant fixations. The more important fixations are those that are clustered tightly together as they lie on the actual salient regions. The sparsely clustered fixations tend to lie on less salient background regions. (b) Fixation weights assigned based on the number of fixations in each cluster. Different symbols and colors represent different clusters with weights for each cluster shown.}}
\label{fig:DBSCANClusters}
\end{figure}
%\begin{figure}[t]
%	\centering
%	\includegraphics[width=0.45\textwidth]{figures/allfix.png}
%	\caption{Importance of fixation density for identifying relevant fixations. The more important fixations are those that are clustered tightly together as they lie on the actual salient regions. The sparsely clustered fixations tend to lie on less salient background regions.}. 
%	\label{fig:fixationdensity}
%\end{figure}
%\begin{figure}
%	\centering
%	\includegraphics[width=0.45\textwidth]{figures/ClusterWeightsDBSCAN.png}
%	\caption{Fixation weights assigned based on the number of fixations in each cluster. Different colors represent different clusters with weights for each cluster shown.}
%	\label{fig:DBSCANClusters}
%\end{figure}
 To obtain appropriate weights for each fixation, we use the fact that fixations that are in higher density clusters are more important and should be weighted higher than those in low density clusters. For this purpose, we use the density-based spatial clustering of applications with noise (DBSCAN) algorithm~\cite{DBSCAN} for clustering the fixations based on their density to obtain fixation clusters. We then assign every fixation in a particular cluster a weight equal to the number of fixations in that cluster. Mathematically, if $P$ is the set of all fixation points, and if $C = \{c_1,c_2,...,c_N\}$ is the set of clusters obtained after applying the DBSCAN algorithm, such that $C$ is a partition of set $P$, then the weights are given by
 \begin{equation}
   W(p) = |c_i|, \forall p \in c_i 
 \end{equation} 
 where $|.|$ represents the $l_0$ norm corresponding to set cardinality (or the number of elements in the set).  
 The DBSCAN algorithm has two parameters, the minimum distance $\epsilon$ within which points are considered as belonging to the same cluster and the minimum number of points required to form a dense cluster $minPts$. We choose the $\epsilon$ parameter such that it is equal to the diameter of a circle that is subtended by one degree of visual angle for the eye-tracking setup for the Toronto dataset~\cite{AIM,VisualAngleDatabases}.  The $minPts$ parameter is chosen to be 3 so that isolated clusters with 2 or less points are rejected.  For rejected clusters, the weights are considered to be zero (as they represent clusters with zero points) and saliency values at such outlier fixations are ignored during the score computation. 
 An illustration of the weighting scheme is shown in Figure~\ref{fig:DBSCANClusters}(a)  where the different clusters of fixation points are shown using different colors, and the weight for each cluster is shown.  On comparison with the original fixations seen in Figure~\ref{fig:DBSCANClusters}(b) one can see that most of the outlier fixations on the carpet that are distant from the objects of interest are rejected and hence do not influence the score. 
 
 A shuffled version of the proposed metric that does not exhibit center-bias can be obtained in a manner similar to the $sNSS$ metric (\ref{eq:SNSS}) as follows: 
\begin{equation}
sWNSS= WNSS(p) - NSS(r) \\
\label{eq:sWNSS}
\end{equation}
where equal weights are assumed for the random set of fixation points $r$. \added[id=MG]{Equal weights are chosen for the random set of fixation points because, unlike the ``good'' fixation points, random fixations that are chosen from the set of fixations for other images cannot be weighted by a density based criteria and are treated equally in order to capture the centered distribution of fixations for the database to nullify center bias.}  
As shown in Figures~\ref{fig:snss} and~\ref{fig:NSSFlaw}, the proposed $WNSS$ and $sWNSS$ metrics give a higher score to the better map. The proposed $sWNSS$ metric is also able to correctly assign the lowest score to the centered Gaussian blob (e) in  Figure~\ref{fig:snss}.   

\section{Subjective Evaluation of VA Models}
\label{sec:subjectivestudy}
Even though a large number of metrics have been proposed in the literature for evaluating VA models, currently, there is no ground-truth subjective database that validates these metrics.  To address this need and evaluate the performance of our proposed metric,  a Visual Attention Quality (VAQ) database is constructed as part of this work.  The constructed database consists of saliency maps that are obtained from state-of-the-art VA models  and their corresponding ground-truth saliency maps. \added[id=MG] {A ground-truth saliency map is obtained by first aggregating the fixation locations obtained by eye-tracking for all subjects to get a fixation map. The obtained fixation map is then convolved with a 2D Gaussian kernel with a standard deviation $\sigma$ proportional to one degree of visual angle followed by normalization~\cite{LeMeur2013}. Thus, a ground-truth saliency map represents the likelihood that a pixel will be attended to by a human observer. As a result, ground-truth saliency maps are more suitable for at-a-glance visual comparisons as opposed to fixation points~\cite{AIM}.} Subjective ratings are obtained by asking human subjects to rate the similarity of the predicted saliency map to the corresponding ground-truth saliency map on a 5-point scale (5-Excellent, 4-Good, 3-Fair, 2-Poor, and 1-Bad). The two aspects the subjects are asked to focus on are the how well the locations of the highest intensity values in the ground-truth match those in the predicted saliency map and the amount of false-positive activity, i.e. high activity in the predicted saliency map that falls on regions of low activity in the ground-truth. The subjects are given a training session and are shown examples of each rating type from excellent to bad. Figure~\ref{fig:training} shows samples of the training images (one for each category) shown to the subjects. As seen in Figure~\ref{fig:training}, in the ``Excellent'' category the high saliency regions align very well with those in the ground-truth map and have minimal false-positive activity. The misalignment of high saliency regions and amount of false-positive activity increases for the ``Good'' and ``Fair'' categories. For the ``Poor'' to ``Bad'' categories the highest saliency regions in the ground truth and the predicted saliency maps are totally misaligned and the false positive activity increases from high to very-high, respectively.  

\begin{figure}[t]
	\centering
	\footnotesize
	\begin{tabular}{DCCC}
		& Original Image & Ground-Truth & Predicted Saliency \\
		\begin{turn}{90} Excellent \end{turn} & \includegraphics[width=0.12\textwidth]{figures/training_excellent.jpg} & 
		\includegraphics[width=0.12\textwidth]{figures/training_excellent_fdm.png} &
		\includegraphics[width=0.12\textwidth]{figures/training_excellent_pmap.png} \\
		\begin{turn}{90} Good \end{turn} & \includegraphics[width=0.12\textwidth]{figures/training_good.jpg} & 
		\includegraphics[width=0.12\textwidth]{figures/training_good_fdm.png} &
		\includegraphics[width=0.12\textwidth]{figures/training_good_pmap.png} \\
		\begin{turn}{90} Fair \end{turn} & \includegraphics[width=0.12\textwidth]{figures/training_fair.jpg} & 
		\includegraphics[width=0.12\textwidth]{figures/training_fair_fdm.png} &
		\includegraphics[width=0.12\textwidth]{figures/training_fair_pmap.png} \\
		\begin{turn}{90} Poor \end{turn} & \includegraphics[width=0.12\textwidth]{figures/training_poor.jpg} & 
		\includegraphics[width=0.12\textwidth]{figures/training_poor_fdm.png} &
		\includegraphics[width=0.12\textwidth]{figures/training_poor_pmap.png} \\
		\begin{turn}{90} Bad \end{turn} & \includegraphics[width=0.12\textwidth]{figures/training_bad.jpg} & 
		\includegraphics[width=0.12\textwidth]{figures/training_bad_fdm.png} &
		\includegraphics[width=0.12\textwidth]{figures/training_bad_pmap.png} \\
		
		
	\end{tabular}
	\caption{Training samples from each category shown to subjects before taking the main test.}
	\label{fig:training}
\end{figure}

\begin{figure}
	\centering
	\includegraphics[width=0.5\textwidth]{figures/histogram_std_aim.png}
	\caption{Histogram of standard deviations for all normalized ground-truth saliency maps in the Toronto dataset~\cite{AIM}.}
	\label{fig:stdhist}
	
\end{figure}
\begin{figure}[t]
	\centering
	\includegraphics[width=0.8\textwidth]{figures/gui_main.png}
	\caption{GUI used for obtaining subjective ratings.}
	\label{fig:GUI}	
\end{figure}
\begin{figure}[t]
	\centering
	\begin{tabular}{ccc}
		
		\includegraphics[width=0.12\textwidth]{figures/57.jpg} & 
		\includegraphics[width=0.12\textwidth]{figures/7.jpg} &
		\includegraphics[width=0.12\textwidth]{figures/67.jpg} \\
		\includegraphics[width=0.12\textwidth]{figures/1.jpg} & 
		\includegraphics[width=0.12\textwidth]{figures/13.jpg} &
		\includegraphics[width=0.12\textwidth]{figures/10.jpg} \\
		\includegraphics[width=0.12\textwidth]{figures/115.jpg} & 
		\includegraphics[width=0.12\textwidth]{figures/103.jpg} &
		\includegraphics[width=0.12\textwidth]{figures/20.jpg} \\
		\includegraphics[width=0.12\textwidth]{figures/119.jpg} & 
		\includegraphics[width=0.12\textwidth]{figures/55.jpg} &
		\includegraphics[width=0.12\textwidth]{figures/89.jpg} 
		
	\end{tabular}
	\caption{The 12 chosen images from the Toronto~\cite{AIM} dataset used in the main subjective test. }
	\label{fig:chosenOrigImgs}
\end{figure}

\begin{figure}[t]
	\centering
	\begin{tabular}{ccc}
		
		\includegraphics[width=0.12\textwidth]{figures/d57.jpg} & 
		\includegraphics[width=0.12\textwidth]{figures/d7.jpg} &
		\includegraphics[width=0.12\textwidth]{figures/d67.jpg} \\
		\includegraphics[width=0.12\textwidth]{figures/d1.jpg} & 
		\includegraphics[width=0.12\textwidth]{figures/d13.jpg} &
		\includegraphics[width=0.12\textwidth]{figures/d10.jpg} \\
		\includegraphics[width=0.12\textwidth]{figures/d115.jpg} & 
		\includegraphics[width=0.12\textwidth]{figures/d103.jpg} &
		\includegraphics[width=0.12\textwidth]{figures/d20.jpg} \\
		\includegraphics[width=0.12\textwidth]{figures/d119.jpg} & 
		\includegraphics[width=0.12\textwidth]{figures/d55.jpg} &
		\includegraphics[width=0.12\textwidth]{figures/d89.jpg} 
		
	\end{tabular}
	\caption{The ground-truth saliency maps for the 12 chosen images from the Toronto~\cite{AIM} dataset used in the main subjective test. }
	\label{fig:chosenFDM}
\end{figure}

The images shown in the training as well as main sessions were taken from the \added[id=MG]{popular} Toronto eye-tracking database~\cite{AIM}. \added[id=MG]{The images in that database have all the same size, which makes the computation of the shuffled metrics easier}. The images used in the training session were different from those in the main test. To ensure variety in the images shown in the main test, the ground truth maps for all the images were analyzed based on their standard deviation as it is a good measure of the spread of salient regions in an image. Figure~\ref{fig:stdhist} shows a histogram of the standard deviations of the ground-truth saliency maps of the images. Based on the 4 noticeable peaks in the histogram, we cluster the images in the dataset based on their standard deviations by using kmeans. Then, the 3 images with standard deviation nearest to the cluster centroids are chosen for each cluster. This gives us the 12 images that are used in the main test. The GUI used for the subjective testing and the colormap used is shown in Figure~\ref{fig:GUI}. Figure~\ref{fig:chosenOrigImgs} shows the images that are chosen and Figure~\ref{fig:chosenFDM} shows their ground-truth saliency maps. We then compute the predicted saliency maps for each of these 12 images using the following 17 state-of-the-art VA models:  GAFFE~\cite{GAFFE}, ITTI~\cite{Itti}, GBVS~\cite{GBVS}, AIM~\cite{AIM},  HouNIPS~\cite{HouNIPS}, GR~\cite{GR}, SDSR~\cite{SDSR}, SUN~\cite{SUN}, 
Torralba~\cite{AudeTorralba},      
FES~\cite{FES}, SigSal~\cite{SigSal}, SpectRes~\cite{SpectRes},
AWS~\cite{AWS}, BMS~\cite{BMS}, Context~\cite{Context}, CovSal~\cite{CovSal},
and RandomCS~\cite{RandomCS}. We also evaluate the ``center''  model which is an image independent model that consists of a centered Gaussian blob. In addition, the original ground truth saliency maps are also added to the list of images shown. We expect the ``center'' model to get lower scores in most cases and the original ground truth saliency maps to get the highest score. The total pairs of ground-truth saliency and test saliency maps shown are 228. These are presented to each subject in a randomized order.  Both the ground-truth and predicted saliency maps are shown with the `jet' colormap that indicates high intensity values in red and low intensity values in blue to make it easy for the subjects to assess the maps. 
 The maps were shown to 16 subjects \added[id=MG]{with age ranging between 22 to 33,} who were checked for both color blindness as well as visual acuity. \added[id=MG]{Out of the 16 participants, 1 subject was working in the area of visual attention, 7 subjects were working in the area of computer vision but not specifically in the area of visual attention, and 8 subjects were working in areas completely unrelated to computer vision. Out of the 16 subects,  6  were female and 10 were male.} The ratings for each predicted saliency map shown were averaged over 16 subjects to get a mean opinion score. These mean opinion scores (MOS) were then correlated with the scores obtained from popular VA performance metrics in addition to the proposed WNSS and sWNSS metrics.  Figure~\ref{fig:scoreDist} shows the distribution of the MOS scores obtained for the predicted saliency maps given by VA models. It illustrates that in only about 16\% of the cases, models received a subjective rating of good or excellent. It also shows that the saliency maps shown cover the entire range of ratings from Excellent to Poor.   The VAQ database will be provided online to download for free for the research community to benchmark metrics developed in the future. 

Figure~\ref{fig:OverallMOS}  shows the ranking of all models in terms of the mean subjective rating obtained for each VA model over all subjects for the VAQ database and shows which VA models are preferred by the human observers for the images in the VAQ database.  
\begin{figure}[t]
	\centering
	\includegraphics[width=0.5\linewidth]{figures/MOS_distrib.png}
	\caption{Distribution of MOS scores.}
	\label{fig:scoreDist}
\end{figure}
%\clearpage
\begin{figure}[t]
	\centering
	\setlength{\figureheight}{0.5\linewidth}
	\setlength{\figurewidth}{0.6\linewidth}
	\input{figures/MOSAllImages.tikz}
	\caption{MOS taken over all predicted saliency maps for each VA model and arranged in descending order.}
	\label{fig:OverallMOS}
\end{figure} 



\section{Metrics Performance Evaluation Results}
\label{sec:results}
\begin{table}[t]
	\centering
	\caption{Correlation results using the VAQ database.}
	\label{tab:correlationresult}
	
	\begin{tabular}{|c|c|c|c|}
   
   \hline
      \textbf{Non-Shuffled Metrics} & SROCC & KROCC & PLCC \\
    \hline
	 $AUC_{Borji}$~\cite{borjieval} & 0.5617   & 0.3952  & 0.5493 \\
	 $AUC_{Judd}$~\cite{Judd_2012} &  0.5883 & 0.4222  &  0.5846 \\
	 $WF_{\beta}$~\cite{margolin2014evaluate} & 0.3126 & 0.2113 & 0.2958 \\   
%	 $AUC_{FDM}$~\cite{LeMeur2013} &  -0.0309 & -0.0042 & -0.2321 \\
	 $NSS$~\cite{NSS}   & 0.7563 & 0.5810 & 0.8297 \\
	 $EMD$~\cite{Judd_2012}   & 0.4470 & 0.3168 & 0.5683 \\ 
%	 $KLD_{FDM}$~\cite{borjireview}   & 0.5200 & 0.3700  & 0.6076 \\ 
%	 $KLD_{Fix}$~\cite{LeMeur2013} & 0.3362 & 0.2301 & 0.3184 \\
	 $CC$~\cite{Judd_2012}    & 0.7461 &  0.5726 & 0.8216 \\
	 $SIM$~\cite{Judd_2012}   & 0.5891  &  0.4246 & 0.6739 \\
	 \added[id=MG]{$MAE$~\cite{MAE}} &\added[id=MG]{0.5063} & \added[id=MG]{0.3599}& \added[id=MG]{0.4803}\\ 
     Proposed $WNSS$  & \bf{0.7858} & \bf{0.6178} & \bf{0.8687}\\
     \hline
        \textbf{Shuffled Metrics}  & SROCC & KROCC & PLCC \\
        \hline
     $sAUC$~\cite{borjieval}	 & 0.5455 & 0.3871 & 0.5631 \\    
	 $sNSS$~\cite{MilindSamTPAMI}	& 0.6526 & 0.4843 & 0.7533\\
	 Proposed $sWNSS$ & \bf{0.7624}& \bf{0.5891} &  \bf{0.8553} \\  
    \hline
	\end{tabular}
	
\end{table}


%\begin{table}[t]
%\centering
%\caption{\added[id=MG]{Agreement in model ranking for all metrics with MOS using Kendall's Coefficient of Concordance (KCC) }}
%\label{tab:KCC}
%\begin{tabular}{|c|c|}
%	\hline
%	Metric & Kendall's Coefficient of Concordance (KCC) \\
%	\hline 
%	$sNSS$~\cite{MilindSamTPAMI}&0.5634 \\
%	Proposed $WNSS$&0.5626\\
%	Proposed $sWNSS$&0.5564 \\
%	$NSS$~\cite{NSS}&0.5546 \\
%	$CC$~\cite{Judd_2012}&0.5530 \\
%	$AUC_{Judd}$~\cite{Judd_2012}&0.5275 \\
%	$sAUC$~\cite{borjieval}&0.5212 \\
%	$WF_{\beta}$~\cite{margolin2014evaluate}&0.5179 \\
%	$AUC_{Borji}$~\cite{borjieval}&0.5126 \\
%	 $SIM$~\cite{Judd_2012}&0.4902 \\
%	$MAE$~\cite{MAE}&0.4679 \\
%	$EMD$~\cite{Judd_2012}&0.4571 \\	
%	\hline
%\end{tabular}
%\end{table}

This section discusses the correlation results  between the subjective ratings and the metric scores. To evaluate how good a performance metric is, we compare the scores given by each metric to each of the considered models  with the average scores given by the subjects to the same models. To correlate the scores we use the widely used correlation measures of  Spearman Rank Order Correlation Coeffficient (SROCC), Kendall Rank Correlation Coefficient (KROCC) and Pearson Linear Correlation Coefficient (PLCC).
 \added[id=MG]{The SROCC and KROCC are rank correlation coefficients, and enable us to compare the ranking given to the VA models by a VA metric with the ranking given by the MOS scores. The PLCC is a linear correlation coefficient that measures how linear the relationship between the metric scores and MOS score is.} 
 The metric scores are normalized by the metric score obtained for the ground-truth saliency map that serves as an upper-bound on most metrics before performing the correlation.   For the $EMD$ metric for which a lower score is better and the best possible score is zero this normalization was not performed, and subjective scores were inverted by subtracting the scores from the maximum subject score of 5 to obtain positive correlation scores. For the $WF_{\beta}$ measure~\cite{margolin2014evaluate}, which requires the ground-truth to be a binary mask, we threshold the ground-truth saliency map by its standard deviation as suggested in~\cite{Moorthy_Bovik}. \added[id=MG]{For the $MAE$ metric, instead of using a binary ground-truth map as in~\cite{MAE}, we use a real-valued ground-truth saliency map since, by definition, the $MAE$ can be computed for two real-valued maps.} Table~\ref{tab:correlationresult} shows the result of the correlations for the  VAQ  database for all the existing metrics listed in Table~\ref{tab:existingmetrics} and our proposed metric (the shuffled $sWNSS$ and non-shuffled $WNSS$ versions). \added[id=MG]{The results for shuffled and non-shuffled metrics are reported separately because there is no explicit way to remove the center bias effect from the ground-truth saliency maps in the subjective study. As a result, human ratings will tend to be better for the saliency maps that boost central regions over peripheral regions. This leads to non-shuffled metrics like $NSS$ and $WNSS$ which tend to reward maps with more central than peripheral activity correlating better with human scores compared to their shuffled versions $sNSS$ and $sWNSS$.} Figure~\ref{fig:scatterplots} shows the scatter plots corresponding to the existing and proposed metrics. From the existing metrics used in the MIT saliency benchmark~\cite{mit-saliency-benchmark}, the $NSS$~\cite{NSS} and $CC$~\cite{borjieval} metrics perform significantly better than the other metrics with the $NSS$ performing the best among them. The $AUC_{Borji}$~\cite{borjieval} and its derivative $sAUC$~\cite{borjieval} metric that suffer from the most number of flaws perform the worst among the MIT benchmark metrics. The $WF_{\beta}$~\cite{margolin2014evaluate} metric also performs poorly.  The proposed  $WNSS$ metric gives the best correlation in the non-shuffled metrics and correspondingly the proposed $sWNSS$ metric gives the best performance among the shuffled metrics.
%\added[id=MG]{In addition to correlation of scores, the concordance of model rankings given by each metric to those given by the MOS scores was analyzed using Kendall's coefficient of concordance (KCC) as in~\cite{riche2013saliency}. As rankings given by each metric change based on the reference image considered, the KCC was computed for rankings obtained by sorting MOS and metric scores for the 17 predicted saliency maps generated for each reference image  and the average KCC was considered. Table~\ref{tab:KCC} shows the KCC values for each of the metrics sorted in the descending order which shows that in terms of  model rankings, the the proposed $WNSS$ and $sWNSS$ metrics perform do well with the $sNSS$ metric performing the best.}

% 'snss'    'wnssdbscan'    'wsnssdbscan'    'nss'    'cc'    'aucjudd'    'aucshuff'    'WFb' 'aucborji'    'sim'    'MAE'    'emd'

\section{Conclusion}
\label{sec:conclusion}
This paper proposes a locally weighted fixation-density based performance metric for assessing the quality of saliency predictions for VA models.  A subjective ground-truth Visual Attention Quality (VAQ) database is created to evaluate the performance of the proposed metric and other existing metrics.  Results of the evaluation show that the proposed metrics ($WNSS$ and its shuffled version $sWNSS$) outperform the widely used $sAUC$, $AUC_{Borji}$ and $AUC_{Judd}$ measures as well as other popular metrics used in the MIT Benchmark~\cite{mit-saliency-benchmark} in terms of their agreement with the subjective ratings. The subjective database is made available online to the research community as a performance metric evaluation benchmark on which future metrics can be tested.
% if have a single appendix:
%\appendix[Proof of the Zonklar Equations]
% or
%\appendix  % for no appendix heading
% do not use \section anymore after \appendix, only \section*
% is possibly needed

% use appendices with more than one appendix
% then use \section to start each appendix
% you must declare a \section before using any
% \subsection or using \label (\appendices by itself
% starts a section numbered zero.)
%


%\appendices
%\section{Proof of the First Zonklar Equation}
%Appendix one text goes here.
%
%% you can choose not to have a title for an appendix
%% if you want by leaving the argument blank
%\section{}
%Appendix two text goes here.
%
%
%% use section* for acknowledgement
%\section*{Acknowledgment}


%The authors would like to thank...


% Can use something like this to put references on a page
% by themselves when using endfloat and the captionsoff option.
\ifCLASSOPTIONcaptionsoff
  \newpage
\fi



% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://www.ctan.org/tex-archive/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
%\clearpage
\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,refs}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)
%\begin{thebibliography}{1}
%
%\bibitem{IEEEhowto:kopka}
%H.~Kopka and P.~W. Daly, \emph{A Guide to \LaTeX}, 3rd~ed.\hskip 1em plus
%  0.5em minus 0.4em\relax Harlow, England: Addison-Wesley, 1999.
%
%\end{thebibliography}

% biography section
% 
% If you have an EPS/PDF photo (graphicx package needed) extra braces are
% needed around the contents of the optional argument to biography to prevent
% the LaTeX parser from getting confused when it sees the complicated
% \includegraphics command within an optional argument. (You could create
% your own custom macro containing the \includegraphics command to make things
% simpler here.)
%\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{mshell}}]{Michael Shell}
% or if you just want to reserve a space for a photo:

%\begin{IEEEbiography}{Michael Shell}
%Biography text here.
%\end{IEEEbiography}

% if you will not have a photo at all:
%\begin{IEEEbiographynophoto}{John Doe}
%Biography text here.
%\end{IEEEbiographynophoto}

% insert where needed to balance the two columns on the last page with
% biographies
%\newpage

%\begin{IEEEbiographynophoto}{Jane Doe}
%Biography text here.
%\end{IEEEbiographynophoto}

% You can push biographies down or up by placing
% a \vfill before or after them. The appropriate
% use of \vfill depends on what kind of text is
% on the last page and whether or not the columns
% are being equalized.

%\vfill

% Can be used to pull up biographies so that the bottom of the last one
% is flush with the other column.
%\enlargethispage{-5in}
%\clearpage
%\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{photos/milind.jpg}}]{Milind S. Gide} received the B.E degree in Electronics and Telecommunications from the University of Mumbai, Mumbai, India in 2006, and the M.S. degree in Electrical Engineering from the University of Houston, Houston, TX, USA in 2009 . He is currently a Ph.D. student in the Image, Video, \& Usability and a teaching assistant in the School of Electrical, Computer \& Energy Engineering at Arizona State University. His research interests include visual attention, image quality, texture-based video coding and super-resolution. During Summer 2011, he worked as an intern at Sharp Labs of America, Camas, Washington in the area of texture-based video coding. Currently he is working as a research assistant in the area of object detection for automated driver assist systems with Intel, Chandler, Arizona. 
%\end{IEEEbiography}
%\vspace{-2 in}		
%\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{photos/drkaram.jpg}}]{Lina J. Karam} (Fellow, IEEE) received the B.E. degree in computer and communications engineering from the American University of Beirut, Beirut, Lebanon, in 1989 and the M.S. and Ph.D. degrees in electrical engineering from the Georgia Institute of Technology, Atlanta, GA, USA, in 1992 and 1995, respectively.		
%She is a Full Professor in the School of Electrical, Computer \& Energy Engineering, Arizona State University, Phoenix, AZ, USA, where she directs the Image, Video, \& Usabilty (IVU) Research Laboratory. Her industrial experience includes image and video compression development at AT\&T Bell Labs, Murray Hill, NJ, USA, multidimensional data processing and visualization at Schlumberger, biomedical image processing with Muscale, and collaboration on computer vision, image/video processing, compression, and transmission projects with industries including Intel, Google, Qualcomm, NTT, Motorola, General Dynamics, and NASA. She has over 100 technical publications and she is a coinventor on a number of patents.	
%
%Dr. Karam was awarded a U.S. National Science Foundation CAREER Award, a NASA Technical Innovation Award, and the 2012 Intel Outstanding Researcher Award. She was also awarded the Outstanding Faculty Award by the IEEE Phoenix Section in 2012. She has served on several journal editorial boards, several conference organization committees, and several IEEE technical committees. She served as the lead giuest editor of the Proceedings of the IEEE, Special Issue on Perceptual-Based Media Processing. She served as the Technical Program Chair of the 2009 IEEE International Conference on Image Processing, as the General Chair of the 2011 IEEE International DSP/SPE Workshops, and as the Lead Guest Editor of the IEEE JOURNAL ON SELECTED TOPICS IN SIGNAL PROCESSING, Special Issue on Visual Quality Assessment. She has cofounded two international workshops (VPQM and QoMEX). She is currently serving as the General Chair of the 2016 IEEE International Conference on Image Processing. She is also serving on the IEEE Publications Board Strategic Planning Committee. She is a member of the IEEE Signal Processing Society's Multimedia Signal Processing Technical Committee and the IEEE Circuits and Systems Society's DSP Technical Committee. She is a member of the Signal Processing, Circuits and Systems, and Communications societies of the IEEE.
%\end{IEEEbiography}
%	
  
\newpage
%\twocolumn
%\fontsize{10pt}{14pt}
\begin{figure*}[t]
	\centering
	\setlength{\figureheight}{0.1\linewidth}
	\setlength{\figurewidth}{0.2\linewidth}
	\begin{tabular}{ccc}
		\hspace*{-0.3in}
		\input{figures/scatterplot_aucborji.tikz} & \hspace*{-0.2in}
		\input{figures/scatterplot_aucjudd.tikz} &  \hspace*{-0.2in}
		\input{figures/scatterplot_aucshuff.tikz} \\
		(a) $AUC_{Borji}$~\cite{borjieval}&  (b) $AUC_{Judd}$~\cite{Judd_2012}& (c) $sAUC$~\cite{borjieval} \\
		\hspace*{-0.3in}
		\input{figures/scatterplot_cc.tikz} & \hspace*{-0.2in}
		\input{figures/scatterplot_emd.tikz} & \hspace*{-0.2in}
		\input{figures/scatterplot_nss.tikz} \\
		(d) $CC$~\cite{Judd_2012} & (e) $EMD$~\cite{Judd_2012}  & (f) $NSS$~\cite{NSS}  \\
		%	\input{figures/scatterplot_kld.tikz} &
		%	\input{figures/scatterplot_kldfix.tikz} &
		%	\input{figures/scatterplot_nss.tikz} \\
		%	(g) $KLD_{FDM}$~\cite{LeMeur2013} & (h) $KLD_{Fix}$~\cite{LeMeur2013} & (i)    \\
		\hspace*{-0.3in}
		\input{figures/scatterplot_sim.tikz} & \hspace*{-0.2in}
		\input{figures/scatterplot_snss.tikz} & \hspace*{-0.2in} 
		\input{figures/scatterplot_Wfb.tikz} \\
		(j) $SIM$~\cite{Judd_2012}  & (k)  $sNSS$~\cite{MilindSamTPAMI} & (l)  $WF_{\beta}~\cite{margolin2014evaluate}$ \\
		%	\input{figures/scatterplot_snss.tikz} &
		%	\input{figures/scatterplot_snss.tikz} & 
		%	\input{figures/scatterplot_wnssfixcount.tikz} \\
		%	(m) $SKLD$~\cite{SUN} & (n) $SNSS$~\cite{MilindSamTPAMI} & (o)    \\
		\hspace*{-0.3in} 
		\input{figures/scatterplot_MAE.tikz} & \hspace*{-0.2in}
		\input{figures/scatterplot_wnssdbscan.tikz} & \hspace*{-0.2in}
		\input{figures/scatterplot_wsnssdbscan.tikz} 
		\\
		(m)\added[id=MG]{$MAE$~\cite{MAE}} & (n) Proposed $WNSS$  & (o) Proposed $sWNSS$ \\   
	\end{tabular} 
	\caption{Scatter plots for all metrics with correlation scores displayed for the VAQ database. Most of the existing metrics along with the widely used AUC based metrics like $sAUC$~\cite{borjieval}, $AUC_{Borji}$~\cite{borjieval} and $AUC_{Judd}$~\cite{Judd_2012} do not show high correlation with subjective scores. Among the non-shuffled metrics, existing metrics $CC$~\cite{borjieval} and $NSS$~\cite{borjieval} exhibit better correlation scores with subjective scores and the proposed metric $WNSS$ performs the best.  Amongst the existing shuffled metrics, the recently proposed $sNSS$~\cite{MilindSamTPAMI} metric performs well with the proposed  $sWNSS$ performing the best.}
	\label{fig:scatterplots}		
\end{figure*}


% that's all folks
\end{document}


