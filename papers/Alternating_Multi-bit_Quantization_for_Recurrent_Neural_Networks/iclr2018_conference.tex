\documentclass{article} % For LaTeX2e
\usepackage{iclr2018_conference,times}
\usepackage{hyperref}
\usepackage{url}
\usepackage{times}
\usepackage{helvet}
\usepackage{courier}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
%\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
%\usepackage[longnamesfirst]{natbib}
%\addtolength{\bibsep}{0em}
%\setlength{\bibhang}{0em}
\usepackage{url}
%\usepackage{pdfsync}
\usepackage{CJKutf8}
\usepackage{xcolor}
\usepackage{subfigure}
\usepackage{listings}
\usepackage[cmex10]{amsmath}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{bbding}
\usepackage{amsfonts,amssymb}
\usepackage{bm}
\usepackage{booktabs}
%\usepackage{algorithm}
%\usepackage{algorithmic}
%\usepackage{algorithm}
%\usepackage{algpseudocode}
\usepackage[ruled,linesnumbered]{algorithm2e}
\usepackage{epstopdf}
\usepackage{enumerate}
%\usepackage[colorlinks,linkcolor=red]{hyperref}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{remark}{Remark}
\newtheorem{proposition}{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}{Definition}
\newcommand{\y}{\mathbf{y}}
\newcommand{\x}{\mathbf{x}}
\newcommand{\dk}{\mathbf{k}}
\newcommand{\dl}{\mathbf{l}}
\newcommand{\e}{\mathbf{e}}
\newcommand{\dr}{\mathbf{r}}
\newcommand{\db}{\mathbf{b}}
\newcommand{\g}{\mathbf{g}}
\newcommand{\w}{\mathbf{w}}
\newcommand{\z}{\mathbf{z}}
%\newcommand{\dr}{\mathbf{r}}
\newcommand{\dd}{\mathbf{d}}
\newcommand{\bx}{\mathbf{X}}
\newcommand{\by}{\mathbf{Y}}
\newcommand{\bz}{\mathbf{Z}}
\newcommand{\be}{\mathbf{E}}
\newcommand{\bl}{\mathbf{L}}
\newcommand{\bs}{\mathbf{S}}
\newcommand{\ba}{\mathbf{A}}
\newcommand{\bb}{\mathbf{B}}
\newcommand{\bd}{\mathbf{D}}
\newcommand{\bh}{\mathbf{H}}
\newcommand{\bi}{\mathbf{I}}
\newcommand{\bj}{\mathbf{J}}
\newcommand{\bk}{\mathbf{K}}
\newcommand{\bq}{\mathbf{Q}}
\newcommand{\bw}{\mathbf{W}}
\newcommand{\bu}{\mathbf{U}}
\newcommand{\bv}{\mathbf{V}}
\newcommand{\br}{\mathbf{R}}
%\renewcommand{\u}{\mathbf{u}}
%\renewcommand{\v}{\mathbf{v}}
\newcommand{\p}{\mathbf{p}}
\renewcommand{\b}{\mathbf{b}}
\newcommand{\bflambda}{\mathbf{\lambda}}
\newcommand{\A}{\mathcal{A}}
\newcommand{\B}{\mathcal{B}}
\newcommand{\X}{\mathbf{X}}
\newcommand{\W}{\mathbf{W}}
\newcommand{\sgn}{\mbox{sgn}}
\newcommand{\bzero}{\mathbf{0}}
\newcommand{\rank}{\mbox{rank}}
\newcommand{\prox}{\mbox{prox}}
\newcommand{\<}{\left\langle}
\renewcommand{\>}{\right\rangle}
\newcommand{\lbar}{\left\|}
\newcommand{\rbar}{\right\|}
%\newcommand{\(}{\left(}
%\newcommand{\)}{\right)}
\newcommand{\fracpartial}[2]{\frac{\partial #1}{\partial  #2}}
\newcommand{\tU}{\mathcal{U}}
\newcommand{\tX}{\mathcal{X}}
\newcommand{\tC}{\mathcal{C}}
\newcommand{\tM}{\mathcal{M}}
\newcommand{\tW}{\mathcal{W}}
\newcommand{\tT}{\mathcal{T}}
\newcommand{\tE}{\mathcal{E}}
\newcommand{\tS}{\mathcal{S}}


%\usepackage[linesnumbered,noline,noend]{algorithm2e}

\SetNlSty{}{}{}

\let\oldnl\nl% Store \nl in \oldnl
\newcommand\nonl{%
\renewcommand{\nl}{\let\nl\oldnl}}% Remove line number for one line


\title{Alternating Multi-bit Quantization for \\ Recurrent Neural Networks}
\iclrfinalcopy
% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.


\author{\small{Chen Xu$^{1,}$\thanks{Work performed while interning at Alibaba  search algorithm team.}$,$ Jianqiang Yao$^2,$ Zhouchen Lin$^{1,3,}$\thanks{Corresponding author.}$,$ Wenwu Ou$^2,$ Yuanbin Cao$^4,$ Zhirong Wang$^2,$}
	%\vspace{-3mm}
	\\ \textbf{\small{Hongbin Zha}}$^{1,3}$ \\
	$^1$ Key Laboratory of Machine Perception (MOE), School of EECS, Peking University, China\\
	$^2$ Search Algorithm Team, Alibaba Group, China \\
	$^3$ Cooperative Medianet Innovation Center, Shanghai Jiao Tong University, China\\
	$^4$ AI-LAB, Alibaba Group, China \\
\texttt{\small xuen@pku.edu.cn,tianduo@taobao.com,zlin@pku.edu.cn,santong.oww@taobao.com}\\
\texttt{\small lingzun.cyb@alibaba-inc.com, qingfeng@taobao.com,zha@cis.pku.edu.cn} \\
}


\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}



\begin{document}


\maketitle

\begin{abstract}
Recurrent neural networks have achieved excellent performance in many applications. However, on portable devices with limited resources, the models are often too large to deploy. For applications on the server with large scale concurrent requests, the latency during inference can also be very critical for costly computing resources. In this work, we address these problems by quantizing the network, both weights and activations, into multiple binary codes $\{-1,+1\}$. We formulate the quantization as an optimization problem. Under the key observation that once the quantization coefficients are fixed the binary codes can be derived efficiently by binary search tree, alternating minimization is then applied.  We test the quantization for two well-known RNNs, i.e., long short term memory (LSTM) and gated recurrent unit (GRU), on the language models. Compared with the full-precision counter part, by $2$-bit quantization we can achieve $\scriptsize{\sim} 16 \times$ memory saving and  $\scriptsize{\sim} 6 \times$ real inference acceleration on CPUs, with only a reasonable loss in the accuracy. By $3$-bit quantization, we can achieve almost no loss in the accuracy or even surpass the original model, with $\scriptsize{\sim} 10.5 \times$ memory saving and $\scriptsize{\sim}3 \times$ real inference acceleration. Both results beat the exiting quantization works with large margins.  We extend our alternating quantization to image classification tasks. In both RNNs and feedforward neural networks, the method also achieves  excellent performance.
\end{abstract}

\section{Introduction}
Recurrent neural networks (RNNs) are specific type of neural networks which are designed to model the sequence data. In last decades, various RNN architectures have been proposed, such as Long-Short-Term Memory (LSTM) \citep{lstm} and Gated Recurrent Units \cite{gru}. They have enabled the RNNs to achieve 
state-of-art performance in many applications, e.g., language models \citep{languagemodel}, neural machine translation \citep{sequence1, sequence2}, automatic speech recognition \citep{speech}, image captions \citep{imagecaption}, etc. However, the models often build on high dimensional input/output,e.g., large vocabulary in language models, or very deep inner recurrent networks, making the models have too many parameters to deploy on portable devices with limited resources. In addition, RNNs can only be executed sequentially with dependence on current hidden states. This causes large latency during inference. For applications in the server with large scale concurrent requests, e.g., on-line machine translation and speech recognition, large latency leads to limited requests processed per machine to meet the stringent response time requirements. Thus much more costly computing resources are in demand for RNN based models.

To alleviate the above problems, several techniques can be employed, i.e., low rank approximation \citep{lowrank1,lowrank2,lowrank3,lowrank4}, sparsity \citep{sparse0,sparse1,pruning,sparse2}, and quantization. All of them are build on the redundancy of current networks and can be combined. In this work, we mainly focus on quantization based methods. More precisely, we are to quantize all parameters into multiple binary codes $\{-1,+1\}$.

The idea of quantizing both weights and activations is firstly proposed by \citep{bnn}. It has shown that even $1$-bit binarization can achieve reasonably good performance in some visual classification tasks. Compared with the full precision counterpart, binary weights reduce the memory by a factor of $ 32$.  And the costly arithmetic operations between weights and activations can then be replaced by cheap XNOR and bitcount operations\citep{bnn}, which potentially leads to much acceleration.  \citet{xnor} further incorporate a real coefficient to compensate for the binarization error. They apply the method to the challenging ImageNet dataset and achieve better performance than pure binarization in \citep{bnn}. However, it is still of large gap compared with the full precision networks. To bridge this gap, some recent works \citep{qnn, dorefa, balanced} further employ quantization with more bits and achieve plausible performance. Meanwhile, quite an amount of works, e.g., \citep{binaryconnect,ternary, tternary, refinedgreedy},  quantize the weights only. Although much memory saving can be achieved, the acceleration is very limited in modern computing devices \citep{xnor}. 

Among all existing quantization works, most of them focus on convolutional neural networks (CNNs) while pay less attention to RNNs.  As mentioned earlier, the latter is also very demanding. Recently, \citep{lab} showed that binarized LSTM with preconditioned coefficients can achieve promising performance in some easy tasks such as predicting the next character. However, for RNNs with large input/output, e.g., large vocabulary in language models, it is still very challenging for quantization. Both works of \citet{qnn} and \citet{balanced} test the effectiveness of their multi-bit quantized RNNs to predict the next word.  Although using up to $4$-bits, the results with quantization still have noticeable gap with those with full precision. This motivates us to find a better method to quantize RNNs. The main contribution of this work is as follows:



\begin{enumerate}[(a)]
	\item We formulate the multi-bit quantization as an optimization problem. The binary codes $\{-1, +1\}$ are learned instead of rule-based. For the first time, we observe that the codes can be optimally derived by the binary search tree once the coefficients are knowns in advance, see, e.g., Algorithm \ref{bstalg}.  Thus the whole optimization is eased by removing the discrete unknowns, which are very difficult to handle.  
		 
	\item We propose to use alternating minimization to tackle the quantization problem. By separating the binary codes and real coefficients into two parts, we can solve the subproblem efficiently when one part is fixed. With proper initialization, we only need two alternating cycles to get high precision approximation, which
	is effective enough to even quantize the activations on-line.   
	
	\item  
	We systematically evaluate the effectiveness of our alternating quantization on language models. Two well-known RNN structures, i.e., LSTM and GRU, are tested with different quantization bits. Compared with the full-precision counterpart, by $2$-bit quantization we can achieve $\scriptsize{\sim}16 \times$ memory saving and   $\scriptsize{\sim}6 \times$ real inference acceleration on CPUs, with a reasonable loss on the accuracy. By $3$-bit quantization, we can achieve almost no loss in accuracy or even surpass the original model with $\scriptsize{\sim}10.5 \times$ memory saving and $\scriptsize{\sim}3 \times$ real inference acceleration. Both results beat the exiting quantization works with large margins. To illustrate that our alternating quantization is very general to extend,  we apply it to image classification tasks. In both RNNs and feedforward neural networks, the technique still achieves very plausible performance.       
\end{enumerate}

\section{Existing Multi-bit Quantization Methods} \label{background}

Before introducing our proposed multi-bit quantization, we first summarize existing works as follows:
\begin{enumerate}[(a)]
	\item Uniform quantization method \citep{xnor,qnn} firstly scales its value in the range $x \in [-1,1]$. Then it adopts the following $k$-bit quantization:
	\begin{equation} \label{uniformquantization}
	q_k(x)=2\left(\frac{{\rm round}[(2^{k}-1)(\frac{x+1}{2})]}{2^{k}-1}-\frac{1}{2}\right),
	\end{equation}
	 after which the method scales back to the original range. Such quantization is rule based thus is very easy to implement. The intrinsic benefit is that when computing inner product of two quantized vectors, it can employ cheap bit shift and count operations to replace costly multiplications and additions operations. However, the method can be far from optimum when quantizing non-uniform data, which is believed to be the trained weights and activations of deep neural network \citep{balanced}. 
	
	\item Balanced quantization \citep{balanced} alleviates the drawbacks of the uniform quantization by firstly equalizing the data.  The method constructs $2^k$ intervals which contain roughly the same percentage of the data. Then it linearly maps the center of each interval to the corresponding quantization code in \eqref{uniformquantization}. Although sounding more reasonable than the uniform one, the affine transform on the centers can still be suboptimal. In addition,  there is no guarantee that the evenly spaced partition is more suitable if compared with the non-evenly spaced partition for a specific data distribution.  
 
	\item  Greedy approximation \citep{refinedgreedy} instead tries to learn the quantization by tackling the following problem:
	\begin{equation} \label{kbitweight}
	\min_{\{\alpha_i, \b_i\}_{i=1}^k} \left\|\w -  \sum_{i=1}^{k} \alpha_i \b_i \right\|^2, \quad \text{with }\quad  \b_i \in \{ -1, +1\}^{n}.
	\end{equation}
	For $k=1$, the above problem has a closed-form solution \citep{xnor}. Greedy approximation extends to $k$-bit ($k> 1$) quantization by sequentially minimizing the residue. That is 
	\begin{equation} \label{sequential}
		   \min_{\alpha_i, \b_i} \left\|\dr_{i-1} - \alpha_i \b_i \right\|^2, \quad \text{with} \quad \dr_{i-1} = \w - \sum_{j=1}^{i-1} \alpha_j \b_j.
	   \end{equation}
	   Then the optimal solution is given as
	   \begin{equation}\label{solution}
	   \alpha_i = \frac{1}{n}\|\dr_{i-1}\|_1 \quad \text{and} \quad \b_i = {\rm sign}(\dr_{i-1}).
	   \end{equation} 
	    
	   Greedy approximation is very efficient to implement in modern computing devices. Although not able to reach a high precision solution, the formulation of minimizing quantization error is very promising.  
	  
	\item Refined greedy approximation \citep{refinedgreedy}  extends to further decrease the quantization error. In the $j$-th iteration after minimizing problem \eqref{sequential}, the method adds one extra step to refine all computed $\{\alpha_{i}\}_{i=1}^j$ with the least squares solution:
	\begin{equation}\label{leastsquare}
	[\alpha_1, \ldots, \alpha_j]=\left( (\mathbf{B}_j^T \mathbf{B}_j)^{-1} \mathbf{B}_j^T \w \right)^T, \quad \text{with}\quad  \mathbf{B}_j = [\b_1, \ldots, \b_j],
	\end{equation}
	In experiments of quantizing the weights of CNN, the refined approximation is verified to be better than the original greedy one. However, as we will show later,  the refined method is still far from  satisfactory for quantization accuracy. 
\end{enumerate}
\begin{figure}[t!]
	\centering
	\includegraphics[width = 0.6\textwidth]{pic/line1.pdf} \caption{Illustration of the optimal $2$-bit quantization when $\alpha_1$ and $\alpha_2$ ($\alpha_1 \geq \alpha_2$) are known in advance. The values are  quantized into $-\alpha_1 - \alpha_2$, $-\alpha_1 + \alpha_2$, $\alpha_1 - \alpha_2$, and $\alpha_1 + \alpha_2$, respectively. And the partition intervals are optimally separated by the middle points of  adjacent quantization codes, i.e., $-\alpha_1$, $0$, and $\alpha_1$, correspondingly.}
	\label{interval}   
\end{figure}
Besides the general multi-bit quantization as summarized above, \citet{ternary} propose ternary quantization by extending $1$-bit binarization with one more feasible state, $0$.
It does quantization by tackling $\min_{\alpha, \mathbf{t}}\|\mathbf{w} - \alpha \mathbf{t} \|_2^2$ with $\mathbf{t} \in  \{-1,0,+1\}^n$. However, no efficient algorithm is proposed in \citep{ternary}. They instead empirically set the entries $w$ with absolute scales less than $0.7/n \|\mathbf{w}\|_1$ to $0$ and binarize the left entries as \eqref{solution}. In fact, ternary quantization is a special case of the $2$-bit quantization in \eqref{kbitweight}, with an additional constraint that $\alpha_1 = \alpha_2$. When the binary codes are fixed, the optimal coefficient $\alpha_1$ (or $\alpha_2$) can be derived by least squares solution similar to \eqref{leastsquare}. 

In parallel to the binarized quantization discussed here, vector quantization is applied to compress the weights for feedforward neural networks \citep{vectorquantization,pruning}. Different from ours where all weights are directly constraint to $\{-1, +1\}$, vector quantization learns a small codebook by applying k-means clustering to the weights or conducting product quantization. The weights are then reconstructed by indexing the codebook. It has been shown that by such a technique, the number of parameters can be reduced by an order of magnitude with limited accuracy loss \citep{vectorquantization}. It is possible that the multi-bit quantized binary weight can be further compressed by using the product quantization.


\section{Our Alternating Multi-bit Quantization}


Now we  introduce our quantization method. We tackle the same minimization problem as \eqref{kbitweight}. For simplicity, we firstly consider the problem with $k = 2$.  Suppose that $\alpha_1$ and $\alpha_2$ are known in advance with $\alpha_1 \geq \alpha_2 \geq 0$, then the quantization codes are restricted to $\mathbf{v} = \{-\alpha_1 - \alpha_2, -\alpha_1 + \alpha_2, \alpha_1 - \alpha_2, \alpha_1 + \alpha_2\}$. For any entry $w$ of $\w$ in problem \eqref{kbitweight}, its quantization code is determined by the least distance to all codes. Consequently, we  can partition the number axis into $4$ intervals. And each interval corresponds to one particular quantization code. The common point of two adjacent intervals then becomes the middle point of the two quantization codes, i.e., $-\alpha_1$, $0$, and $\alpha_1$. Fig. \ref{interval} gives an illustration.  


\begin{figure}[t!]
	\centering
	\includegraphics[width = 0.6\textwidth]{pic/tree3.pdf} \caption{Illustration of binary search tree to determine the optimal quantization.}
	\label{bstpic}   
\end{figure}


\begin{algorithm}[t!]
	\label{bstalg}
	\SetKwComment{comment}{\{}{\}}
	\SetNoFillComment
	\caption{Binary Search Tree (BST) to determine to optimal code}
	\BlankLine 
	\SetKwFunction{printlcs}{\textbf{BST}}
	\nonl\printlcs{$w,\mathbf{v}$}\\ 
	\comment{$w$ is the real value to be quantized} 
	\comment{$\mathbf{v}$ is the vector of quantization codes in ascending order}  
	\Indp
	\BlankLine
	$m = {\rm length}(\mathbf{v})$ \\   
	\If{$m == 1 $  }{
		%\comment{Reaching the leaf node.} 
		\KwRet{$\mathbf{v}_1$} \\ }
	\eIf{$w \geq (v_{m/2} + v_{m/2+1})/2$}{
		%\comment{Searching on the right subtree.}
		\printlcs{$w,\mathbf{v}_{m/2 +1: m}$}\\
	}
	{ %\comment{Searching on the left subtree.}
		\printlcs{
			$w,\mathbf{v}_{1: m/2}$}}
\end{algorithm}

\begin{algorithm}[t!]
	%\SetKwComment{comment}{// }{}
	\SetKwComment{comment}{\{}{\}}
	\SetNoFillComment
	\SetKwInOut{REQUIRE}{Require}
	\SetKwInOut{ENSURE}{Ensure}
	
	\caption{Alternating Multi-bit Quantization}
	\label{altalg}
	\REQUIRE{Full precision weight $\w \in \mathbb{R}^n$, number of bits $k$, total iterations $T$}
	\ENSURE{$\{\alpha_i, \b_i\}_{i=1}^{k}$}
	\BlankLine
	Greedy Initialize $\{\alpha_i, \b_i\}_{i=1}^{k}$ as \eqref{solution} \\
	\For{${\rm iter} \gets1$ \KwTo $T$}{
		Update $\{\alpha_i\}_{i=1}^{k}$ as \eqref{leastsquare}\\
		Construct $\mathbf{v}$ of all feasible codes in  accending order \\
		Update $\{\b_i\}_{i=1}^{k}$ as Algorithm \ref{bstalg}.
	}	
\end{algorithm}
For the general $k$-bit quantization, suppose that $\{\alpha_i\}_{i=1}^k$ are known and we have all possible codes in ascending order, i.e., $\mathbf{v} = \{-\sum_{i=1}^k \alpha_i,~ \mathbf{\ldots}~,  \sum_{i=1}^k\alpha_i \}$. Similarly, we can partition the number axis into $2^k$ intervals, in which the boundaries are determined by the centers of two adjacent codes in $\mathbf{v}$, i.e., $\{(v_i + v_{i+1})/2\}_{i=1}^{2^k-1}$. However, directly comparing per entry with all the boundaries needs $2^k$ comparisons, which is very inefficient. Instead, we can make use of the ascending property in $\mathbf{v}$. Hierarchically, we partition the codes of $\mathbf{v}$ evenly into two ordered sub-sets, i.e., $\mathbf{v}_{1: m/2}$  and $\mathbf{v}_{m/2 +1: m}$ with $m$ defined as the length of $\mathbf{v}$. If $w < (v_{m/2} + v_{m/2+1})/2 $, its feasible codes are then optimally restricted to $\mathbf{v}_{1: m/2}$. And if $w \geq (v_{m/2} + v_{m/2+1})/2$ , its feasible codes become $\mathbf{v}_{m/2 +1: m}$. By recursively evenly partition the ordered feasible codes, we can then efficiently determine the optimal code for per entry by only $k$ comparisons. The whole procedure is in fact a binary search tree. We summarize it in Algorithm \ref{bstalg}. Note that once getting the quantization code, it is straightforward to map to the binary code $\b$. Also, by maintaining a mask vector with the same size as $\w$  to indicate the partitions, we could operate BST for all entries simultaneously. To give a better illustration, we give a binary tree example for $k =2$ in Fig. \ref{bstpic}. Note that for $k=2$, we can even derive the optimal codes by a closed form solution, i.e., $\b_1 = {\rm sign}(\w)$ and $\b_2 = {\rm sign}(\w - \alpha_1 \b_1)$ with $\alpha_1 \geq \alpha_2 \geq 0$.


Under the above observation, let us reconsider the refined greedy approximation \citep{refinedgreedy} introduced in Section \ref{background}. After modification on the computed $\{\alpha_{i}\}_{i=1}^{j}$ as \eqref{leastsquare},  $\{\b_{i} \}_{i=2}^j$ are no longer optimal while the method keeps all of them fixed. To improve the refined greedy approximation,  alternating minimizing $\{\alpha_i\}_{i=1}^k$ and $\{\b_i\}_{i=1}^k$ becomes a natural choice. Once getting $\{\b_i\}_{i=1}^k$ as described above, we can optimize $\{\alpha_i\}_{i=1}^k$ as \eqref{leastsquare}. In real experiments, we find that by greedy initialization as \eqref{solution}, only two alternating cycles is good enough to find high precision quantization. For better illustration, we summarize our alternating minimization in Algorithm \ref{altalg}.  For updating $\{\alpha_{i}\}_{i=1}^{k}$, we need $2k^2n$ binary operations and $kn$ non-binary operations. Combining  $kn$ non-binary operations to determine the binary code, for total $T$ alternating cycles, we thus need $2Tk^2n$ binary operations and $2(T+1)kn$ non-binary operations to quantize $\w \in \mathbb{R}^n$ into $k$-bit, with the extra $2kn$ corresponding to greedy initialization. 
  
\section{Apply Alternating Multi-bit Quantization to RNNs}
\textbf{Implementation}.  We firstly introduce the implementation details for quantizing RNN. For simplicity, we consider the one layer
 LSTM for language model. The goal is to predict the next  word indexed by $t$ in a sequence of one-hot word tokens $(y_1^*,\ldots,y_N^* )$ as follows:
\begin{equation}\label{lstm_equation}
\begin{split}
& \x_t =   \W_e^T \y_{t-1}^*,  \qquad    \\
&\mathbf{i}_t, \mathbf{f}_t, \mathbf{o}_t, \mathbf{g}_t = \sigma(\W_i \x_t + \db_i + \W_h \mathbf{h}_{t-1} + \db_h), \\
&\mathbf{c}_t = \mathbf{f}_t \odot \mathbf{c}_{t-1} + \mathbf{i}_t \odot \mathbf{g}_t,  \quad  \mathbf{h}_t =  \mathbf{o}_t \odot \tanh(\mathbf{c}_t), \\
&\y_t = \text{softmax}(\W_s \mathbf{h}_t + \db_s ). \\
\end{split}
\end{equation} 
where $\sigma$ represents the activation function. In the above formulation, the multiplication between the weight matrices and the vectors $\x_t$ and $\mathbf{h}_t$ occupy most of the computation. This is also where we apply quantization to. For the weight matrices, We do not apply quantization on the full but rather row by row.
During the matrix vector product, we can firstly execute the binary multiplication. Then element-wisely multiply the obtained binary vector with the high precision scaling coefficients. Thus little extra computation results while much more freedom is brought to better approximate the weights. We give an illustration on the left part of Fig. \ref{product}. Due to one-hot word tokens, $\x_t$ corresponds to one specific row in the quantized $\W_e$.  It needs no more quantization. Different from the weight matrices, $\mathbf{h}_t$ depends on the input, which needs to be quantized on-line during inference.  For consistent notation with existing work, e.g., \citep{qnn, balanced}, we also call quantizing on $\mathbf{h}_t$ as quantizing on activation.  


\begin{figure}[t!]
	\centering
	\includegraphics[width = 1\textwidth]{pic/plot3.pdf} \caption{Illustration of quantized matrix vector multiplication (left part). The matrix is quantized row by row, which provides more freedom to approximate while adds little extra computation. By reformulating as the right part, we can make full use of the intrinsic parallel binary matrix vector multiplication for further acceleration.}
	\label{product}   
\end{figure}


For $\W \in \mathbb{R}^{m \times n}$ and $\mathbf{h}_t \in \mathbb{R}^{n}$, the standard matrix-vector product needs $2mn$ operations.  For the quantized product between $k_w$-bit $\W$ and $k_h$-bit $\mathbf{h_t}$, we have $2k_wk_hmn + 4k_h^2n$ binary operations and $6k_hn + 2 k_wk_hm$ non-binary operations, where $6k_hn$ corresponds to the cost of alternating approximation ($T= 2$) and $2 k_wk_hm$ corresponds to the final product with coefficients. As the binary multiplication operates in $1$ bit, whereas the full precision multiplication operates in $32$ bits, despite the feasible implementations, the acceleration can be $32\times$ in theory. For alternating quantization here, the overall theoretical acceleration is thus  computed as $\gamma = \frac{2mn}{\frac{1}{32}(2k_wk_hmn + 4k_n^2n) + 6k_hn + 2 k_wk_hm}$.
Suppose that LSTM has hidden states $n = 1024$, then we have $\W_h \in \mathbb{R}^{4096 \times 1024}$. The acceleration ratio becomes roughly $7.5 \times $ for $(k_h,k_w) = (2,2)$ and $ 3.5 \times$ for $(k_h, k_w) = (3,3)$.  In addition to binary operations, the acceleration in real implementations can be largely affected by the size of the matrix, where much memory reduce can result in better utilizing in the limited faster cache. We implement the binary multiplication kernel in CPUs. Compared with  the much optimized Intel Math Kernel Library (MKL) on full precision matrix vector multiplication, we can roughly achieve $6 \times $ for $(k_h,k_w) = (2,2)$ and $3 \times$ for $(k_h, k_w) = (3,3)$. For more details, please refer to Appendix \ref{multiplicationinCPU}. 


As indicated in the left part of Fig. \ref{product}, the binary multiplication can be conducted sequentially by associativity. Although the operation is suitable for parallel computing by synchronously conducting the multiplication, this needs extra effort for parallelization. We instead concatenate the binary codes as shown in the right part of Fig. \ref{product}. Under such modification, we are able to make full use of the much optimized inner parallel matrix multiplication, which gives the possibility for further acceleration. The final result is then obtained by adding all partitioned vectors together, which has little extra computation. 

\textbf{Training.} As firstly proposed by \citet{binaryconnect}, during the training of quantized neural network, directly adding the moderately small gradients to quantized weights will result in no change on it. So they maintain a full precision weight to accumulate the gradients then apply quantization in every mini-batch.  In fact,  the whole procedure can be mathematically formulated as a bi-level optimization \citep{bilevelreview} problem: 
\begin{equation}
\begin{split}
\min_{\w, \{\alpha_i, \b_i \}_{i=1}^k } \ & f\left(\sum_{i=1}^{k} \alpha_i \b_i\right)\\
s.t. \quad  \{\alpha_i, \b_i \}_{i=1}^k = &\mathop{\arg\min}\limits_{\{ \alpha_i', \b_i' \}_{i=1}^k }  \left\|\w -  \sum_{i=1}^{k} \alpha_i' \b_i' \right\|^2. 
\end{split}
\end{equation}
Denote the quantized weight as $\hat{\w} = \sum_{i=1}^{k} \alpha_i \b_i$.  
In the forward propagation, we derive $\hat{\w}$ from the full precision $\w$ in the lower-level problem  and  apply it to the upper-level function $f(\cdot)$, i.e., RNN in this paper. During the backward propagation, the derivative $\frac{\partial f}{\partial \hat{\w}}$  is propagated back to $\w$  through the lower-level function. Due to the discreteness of $\b_i$,  it is very hard to model the implicit dependence of $\hat{\w}$ on $\w$. So we also adopt the ``straight-through estimate'' as \citep{binaryconnect}, i.e., $\frac{\partial f}{\partial \w} = \frac{\partial f}{\partial \hat{\w}}$. To compute the derivative on the quantized hidden state $\mathbf{h}_t$, the same trick is applied.  During the training, we find the same phenomenon as \citet{qnn} that some entries of $\w$ can grow very large, which become outliers and harm the quantization. Here we simply clip $\w$ in the range of $[-1, 1]$.  
 
\section{Experiments on the Language Models}
In this section, we conduct quantization experiments on language models. The two most well-known recurrent neural networks, i.e., LSTM \citep{lstm} and GRU  \citep{gru}, are evaluated. As they are to predict the next word, the performance is measured by perplexity per word (PPW) metric.  For all experiments, we initialize with the pre-trained model and using vanilla SGD. The initial learning rate is set to $20$. Every epoch we evaluate on the validation dataset and record the best value. When the validation error exceeds the best record, we decrease learning rate by a factor of $1.2$. Training is terminated once the learning rate less than $0.001$ or reaching the maximum epochs, i.e., $80$.   The gradient norm is clipped in the range $[-0.25, 0.25]$.  We unroll the network for $30$ time steps and regularize it with the standard dropout (probability of dropping out units equals to $0.5$) \citep{dropoutrnn}. For simplicity of notation, we denote the methods using uniform, balanced, greedy, refined greedy, and our alternating quantization as Uniform, Balanced, Greedy, Refined, and Alternating, respectively.    
 

\begin{table}[!t]
	\caption{Measurement on the approximation of different quantization methods, e.g., Uniform \citep{qnn}, Balanced \citep{balanced}, Greedy \citep{refinedgreedy}, Refined \citep{refinedgreedy}, and our Alternating method, see Section \ref{background}. We apply these methods to quantize the full precision pre-trained weight of LSTM on the PTB dataset. The best values are in bold. W-bits represents the number of weight bits and FP denotes full precision.} 
	\renewcommand{\arraystretch}{1.3}
	\label{ptbquantization}
	\begin{center}
		\begin{tabular}{|c|c|c|c||c|c|c|c|c|}
			\hline
			\multicolumn{4}{|c||}{\small{Relative MSE}}  & \multicolumn{4}{|c|}{\small{Testing PPW}} \\\cline{1-8} 
			\small{W-Bits} &$2 $ & $3$& $ 4$ & 
			$2 $ & $3$& $4$&   \small{FP}
			\\\hline\hline		 
		\small{Uniform}	&$1.070$ &  $0.404$ &  $0.302$ & $   283.2 $&  $   227.3 $&  $   216.3 $&  \multirow{5}{*}{$89.8$} \\ \cline{1-7}
		\small{Balanced}	&$0.891$ &  $0.745$ &  $0.702$ & $ 10287.6 $&  $  9106.4 $&  $  8539.8 $&  \\ \cline{1-7}
		\small{Greedy}	&$0.146$ &  $0.071$ &  $0.042$ & $   118.9 $&  $    99.4 $&  $    95.0 $&  \\ \cline{1-7}
		\small{Refined}	&$0.137$ &  $0.060$ &  $0.030$ & $   105.3 $&  $    95.4 $&  $    93.1 $&  \\ \cline{1-7}   
		\small{Alternating (ours)} &$\mathbf{0.125}$ &  $\mathbf{0.043}$ &  $\mathbf{0.019}$ & $   \mathbf{103.1} $&  $    \mathbf{93.8} $&  $    \mathbf{91.4} $&  \\ \hline
		\end{tabular}
	\end{center}
\end{table}


\begin{table}[!t]
	\caption{ Quantization on the full precision pre-trained weight of GRU on the PTB dataset.} 
	\renewcommand{\arraystretch}{1.3}
	\label{ptbgruquantization}
	\begin{center}
		\begin{tabular}{|c|c|c|c||c|c|c|c|c|}
			\hline
			\multicolumn{4}{|c||}{\small{Relative MSE}}  & \multicolumn{4}{|c|}{\small{Testing PPW}} \\\cline{1-8} 
			%\multicol{*}{5}{$92.5$}\\
			\small{W-Bits} &$2 $ & $3$& $ 4$ & 
			$2 $ & $3$& $4$&   \small{FP}
			\\\hline\hline		 
			\small{Uniform}	&$6.138$ &  $3.920$ &  $3.553$ & $3161906.6 $&  $771259.6 $&  $715781.9 $&  \multirow{5}{*}{$92.5$} \\ \cline{1-7}
			\small{Balanced} &$1.206$ &  $1.054$ &  $1.006$ & $  2980.4 $&  $  3396.3 $&  $  3434.1 $&  \\ \cline{1-7}
			\small{Greedy}	&$0.377$ &  $0.325$ &  $0.304$ & $   135.7 $&  $   105.5 $&  $    99.2 $&  \\ \cline{1-7}
			\small{Refined}	&$0.128$ &  $0.055$ &  $0.030$ & $   111.6 $&  $    99.1 $&  $    97.0 $&  \\ \cline{1-7}   
			\small{Alternating (ours)} &$\mathbf{0.120}$ &  $\mathbf{0.044}$ &  $\mathbf{0.021}$ & $   \mathbf{110.3} $&  $    \mathbf{97.3} $&  $    \mathbf{95.2} $&  \\ \hline
		\end{tabular}
	\end{center}
\end{table}





\begin{table}[!t]
	\caption{Testing PPW of multi-bit quantized LSTM and GRU on the PTB dataset.  W-Bits and A-Bits represent the number of weight and activation bits, respectively.}
	\renewcommand{\arraystretch}{1.3}
	\label{ptbresult}
	\begin{center}
		\begin{tabular}{|c|c|c|c|c|c||c|c|c|c|c|}
			\hline
			\multicolumn{6}{|c||}{\small{LSTM}}  & \multicolumn{5}{|c|}{\small{GRU}} \\\cline{1-11}
			\small{W-Bits / A-Bits} &$2/2$ & $2/3$& $3/3$& $4/4$ & \small{FP/FP} &
			$2/2$ & $2/3$& $3/3$& $4/4$ & \small{FP/FP}   
			\\\hline\hline		 
			\small{Uniform} &$-$ &  $220$ &  $-$  & $100$  & $97$ &
			$-$ &  $-$ &  $-$  & $-$  & $-$
			\\ \hline
			\small{Balanced} & $126$ &  $123$ & $-$&  $114$  & $107$ &
			$142$ &  $-$ &  $-$&  $116$  & $100$
			\\ \hline
			\small{Refined}&  $100.3$ &  $95.6$ & $91.3$ &  $-$& \multirow{2}{*}{$89.8$} &
			$105.1 $ &  $100.3$ & $95.9$ &  $-$ & \multirow{2}{*}{$92.5$}  
			\\ \cline{1-5} \cline{7-10}
			\small{Alternating} \small{(ours)} & $\mathbf{95.8} $ &  $\mathbf{91.9}$ & $ \mathbf{87.9}$ & $-$ &  &
			$\mathbf{101.2} $ &  $\mathbf{97.0}$ & $ \mathbf{92.9}$ & $-$  &   
			\\\hline
		\end{tabular}
	\end{center}
\end{table}


\textbf{Peen Tree Bank.} We first conduct experiments on the Peen Tree Bank (PTB) corpus \citep{ptbdataset}, using the standard preprocessed splits with a $10$K size vocabulary \citep{ptb}. The
PTB dataset contains $929$K training tokens, $73$K validation tokens, and $82$K test tokens.  For fair comparison with existing works, we also use LSTM and GRU with $1$ hidden layer of size $300$. To have a glance at the approximation ability of different quantization methods as detailed in Section \ref{background}, we firstly conduct experiments by directly quantizing the trained full precision weight (neither quantization on activation nor retraining). Results on LSTM and GRU are shown in Table \ref{ptbquantization} and Table \ref{ptbgruquantization}, respectively. The left parts record the relative mean squared error of quantized weight matrices with full precision one. We can see that our proposed Alternating can get much lower error across all varying bit. We also measure the testing PPW for the quantized weight as shown in the right parts of Table \ref{ptbquantization} and \ref{ptbgruquantization}.  The results are in consistent with the left part, where less errors result in lower testing PPW.   
Note that Uniform and Balanced quantization are rule-based and not aim at minimizing the error. Thus they can have much worse result by direct approximation. We also repeat the experiment on other datasets. For both LSTM and GRU, the results are very similar to here.    
 
 We then conduct experiments by quantizing both weights and activations.  We train with the batch size $20$.  The final result is shown in Table \ref{ptbresult}. Besides comparing with the existing works, we also conduct experiment for Refined as a competitive baseline. We do not include Greedy as it is already shown to be much inferior to the refined one, see, e.g., Table \ref{ptbquantization} and \ref{ptbgruquantization}.  
 As Table \ref{ptbresult} shows, our full precision model can attain lower PPW than the existing works. However, when considering the gap between quantized model with the full precision one, our alternating quantized neural network is still far better than existing works, i.e., Uniform \citep{qnn} and Balanced \citep{balanced}.  Compared with Refined, our Alternating quantization can achieve compatible performance using $1$-bit less quantization on weights or activations. In other words, under the same tolerance of accuracy drop, Alternating executes faster and uses less memory than Refined. We can see that our $3/3$ weights/activations quantized LSTM can achieve even better performance than full precision one. A possible explanation is due to the regularization introduced by quantization \citep{qnn}. 
\begin{table}[!t]
	\caption{Testing PPW of multi-bit quantized LSTM and GRU on the WikiText-2 dataset.}
	\renewcommand{\arraystretch}{1.3}
	\label{wikidata-2}
	\begin{center}
		\begin{tabular}{|c|c|c|c|c||c|c|c|c|}
			\hline
			\multicolumn{5}{|c||}{\small{LSTM}}  & \multicolumn{4}{|c|}{\small{GRU}} \\\cline{1-9}
			\small{W-Bits / A-Bits} &$2/2$ & $2/3$& $3/3$ & \small{FP/FP} &
			$2/2$ & $2/3$& $3/3$&   \small{FP/FP}   
			\\\hline\hline		 
			\small{Refined}&  $108.7 $ &  $105.8$ & $102.2$ & \multirow{2}{*}{$100.1$} &
			$117.2 $ &  $114.1$ & $111.8$ & \multirow{2}{*}{$106.7$}  
			\\\cline{1-4} \cline{6-8} 
			\small{Alternating (ours)} & $\mathbf{106.1} $ &  $\mathbf{102.7}$ & $ \mathbf{98.7}$ &  &
			$\mathbf{113.7} $ &  $\mathbf{110.2}$ & $ \mathbf{106.4}$ &  
			\\\hline
		\end{tabular}
	\end{center}
\end{table}

\begin{table}[t!]
	\caption{Testing PPW of multi-bit quantized LSTM and GRU on the Text8 dataset.}
	\renewcommand{\arraystretch}{1.3}
	\label{text8}
	\begin{center}
		\begin{tabular}{|c|c|c|c|c||c|c|c|c|}
			\hline
			\multicolumn{5}{|c||}{\small{LSTM}}  & \multicolumn{4}{|c|}{\small{GRU}} \\\cline{1-9}
			\small{W-Bits / A-Bits} &$2/2$ & $2/3$& $3/3$ & \small{FP/FP} &
			$2/2$ & $2/3$& $3/3$&   \small{FP/FP}   
			\\\hline\hline		 
			\small{Refined}&  $135.6 $ &  $122.3$ & $110.2$ & \multirow{2}{*}{$101.1$} &
			$135.8$ &  $126.9$ & $ 118.3$ & \multirow{2}{*}{$111.6$}  
			\\\cline{1-4} \cline{6-8} 
			\small{Alternating (ours)} & $\mathbf{108.8} $ &  $\mathbf{105.1}$ & $ \mathbf{98.8}$ &  &
			$\mathbf{124.5} $ &  $\mathbf{118.7}$ & $ \mathbf{114.0}$ &    
			\\\hline
		\end{tabular}
	\end{center}
\end{table}

 
\textbf{WikiText-2} \citep{wikitext-2} is a dataset
released recently as an alternative to PTB. It contains $2088$K training, $217$K validation, and $245$K test tokens, and has a vocabulary of $33$K words, which is roughly $2$ times larger in dataset size, and $3$ times larger in vocabulary than PTB. We train with one layer's hidden state of size $512$ and set the batch size to $100$. The result is shown in Table \ref{wikidata-2}. Similar to PTB, our Alternating can use $1$-bit less quantization to attain compatible or even lower PPW than Refined.

\textbf{Text8.} In order to determine whether Alternating remains effective with a larger dataset, we perform
experiments on the Text8 corpus \citep{text8dataset}. Here we follow the same setting as \citep{noisernn}. The first $90$M characters are used for training, the next $5$M for
validation, and the final 5M for testing, resulting in $15.3$M training tokens, $848$K validation tokens,
and $855$K test tokens. We also preprocess the data by mapping all words which appear 10 or fewer times to the unknown token, resulting in a $42$K size vocabulary. We train LSTM and GRU with one  hidden layer of size $1024$ and set the batch size to $100$. The result is shown in Table \ref{text8}. For LSTM on the left part, Alternating achieves excellent performance. By only $2$-bit quantization on weights and activations, it exceeds Refined with $3$-bit. The $2$-bit result is even better than that reported in \citep{noisernn}, where LSTM adding noising schemes for regularization can only attain $110.6$ testing PPW. For GRU on the right part, although Alternating is much better than Refined, the $3$-bit quantization still has gap with full precision one. We attribute that to the unified setting of hyper-parameters across all experiments. With specifically tuned hyper-parameters on this dataset, one may make up for that gap. 

Note that our alternating quantization is a general technique. It is not only suitable for language models here. For a comprehensive verification, we apply it to image classification tasks. In both RNNs and feedforward neural networks, our alternating quantization also achieves the lowest testing error among all compared methods. Due to space limitation, we deter the results to Appendix \ref{imageclassification}.    




\section{Conclusions}
In this work, we address the limitations of RNNs, i.e., large memory and high latency, by quantization.  We formulate the quantization by minimizing the approximation error. Under the key observation that some parameters can be singled out when others fixed, a simple yet effective alternating method is proposed.  We apply it to quantize LSTM and GRU on language models. By $2$-bit weights and activations, we achieve only a reasonably accuracy loss compared with full precision one, with $\scriptsize{\sim}16\times$ reduction in memory and $\scriptsize{\sim}6 \times$ real acceleration on CPUs.  By $3$-bit quantization, we can attain compatible or even better result than the full precision one, with $\scriptsize{\sim}10.5\times$ reduction in memory and $\scriptsize{\sim}3 \times $ real acceleration. Both beat existing works with a large margin.  We also apply our alternating quantization to image classification tasks. In both RNNs and feedforward neural networks, the method can still achieve very plausible performance. 
%It is not difficult to  incorporate the low-rank approximation and sparsity for further compression and acceleration.
\section{Acknowledgements}
We would like to thank the reviewers for their suggestions on the manuscript. Zhouchen Lin is supported by National Basic Research Program of China (973 Program) (grant no. 2015CB352502), National Natural Science Foundation (NSF) of China (grant nos. 61625301 and 61731018), Qualcomm, and Microsoft Research Asia.  Hongbin Zha is supported by Natural Science Foundation (NSF)  of China (No. 61632003).


\bibliography{iclr2018_conference}
\bibliographystyle{iclr2018_conference}
\newpage

\appendix
\chapter{\LARGE APPENDIX}
%
\section{Binary Matrix Vector Multiplication in CPUs}
\label{multiplicationinCPU}
\begin{table}[h!]
	\caption{Computing time of the binary matrix vector multiplication in CPUs, where Quant represents the cost to execute our alternating quantization on-line.}
	\renewcommand{\arraystretch}{1.3}
	\label{acceleration}
	\begin{center}
		\begin{tabular}{|c|c|c|c|c|c|c|}
			\hline
			\small{Weight Size} & \small{W-Bits / A-Bits} & \small{Total (ms)} & \small{Quant (ms)} & \small{Quant / Total} & \small{Acceleration} \\
			\hline
			\multirow{3}{*}{\centering $4096 \times 1024$}
			& $2/2$ & $0.35$ & $0.07$ & $20\%$ & $5.6 \times$ \\
			\cline{2-6}
			& $3/3$ & $0.72$  & $0.11$ & $15\%$ & $2.7 \times$ \\
			\cline{2-6}
			& FP/FP & $1.95$  & $-$ & $-$ & $1.0 \times$ \\
			\hline
			\hline
			\multirow{3}{*}{\centering $42000 \times 1024$}
			& $2/2$ & $3.17$  & $0.07$ & $2.2\%$ & $6.0 \times$ \\
			\cline{2-6}
			& $3/3$ & $6.46$  & $0.11$ & $1.7\%$ & $3.0 \times$ \\
			\cline{2-6}
			& FP/FP & $19.10$ & $-$ & $-$ & $1.0 \times$ \\
			\hline
		\end{tabular}
	\end{center}
\end{table}

In this section, we discuss the implementation of the binary multiplication kernel in CPUs. The binary multiplication is divided into two steps: Entry-wise XNOR operation (corresponding to entry-wise product in the full precision multiplication) and bit count operation for accumulation (corresponding to compute the sum of all multiplied entries in the full precision multiplication). We test it on Intel Xeon E5-2682 v4 @ 2.50 GHz CPU. For the XNOR operation, we use the Single instruction, multiple data (SIMD) $\it{\_mm256\_xor\_ps}$, which can execute $256$ bit simultaneously. For the bit count operation, we use the function $\it{\_popcnt64}$ (Note that this step can further be accelerated by the up-coming instruction $\it{\_mm512\_popcnt\_epi64}$, which can execute $512$ bits simultaneously. Similarly, the XNOR operation can also be further accelerated by the up-coming $\it{\_mm512\_xor\_ps}$ instruction to execute $512$ bits simultaneously). We compare with the much optimized Intel Math Kernel Library (MKL) on full precision matrix vector multiplication and execute all codes in the single-thread mode. We conduct two scales of experiments: a matrix of size $4096 \times 1024$ multiplying a vector of size $1024$ and a matrix of size $42000 \times 1024$ multiplying a vector of size $1024$, which respectively correspond to the hidden state product $\mathbf{W}_h \mathbf{h}_{t-1}$ and the softmax layer $\mathbf{W}_s \mathbf{h}_t$ for Text8 dataset during inference with batch size of $1$ (See Eq. \eqref{lstm_equation}). The results are shown in Table \ref{acceleration}. We can see that our alternating quantization step only accounts for a small portion of the total executing time, especially for the larger scale matrix vector multiplication. Compared with the full precision one, the binary multiplication can roughly achieve $6\times$ acceleration with $2$-bit quantization and   $3\times$ acceleration with $3$-bit quantization. Note that this is only a simple test on CPU. Our alternating quantization method can also be extended to GPU, ASIC, and FPGA.

\section{Image Classification}\label{imageclassification}

\textbf{Sequential MNIST.} As a simple illustration to show that our alternating quantization is not limited for texts, we conduct experiments on the sequential MNIST classification task \citep{bnrnn}. The dataset consists of a training set of $60$K and a test set of $10$K $28 \times 28$ gray-scale images. Here  we divide the last $5000$ training images for validation. In every time, we sequentially use one row of the image as the input ($28 \times 1$), which results in a total of $28$ time steps. We use $1$ hidden layer’s LSTM of size $128$ and the same optimization hyper-parameters as the language models. Besides the weights and activations, the inputs are quantized. The testing error rates for $1$-bit input, $2$-bit weight, and $2$-bit activation are shown in \ref{sequentialmnist}, where our alternating quantized method still achieves plausible performance in this task. 


\begin{table}[!t]
	\caption{Testing error rate of LSTM on MNIST with $1$-bit input, $2$-bit weight, and $2$-bit activation.}
	\renewcommand{\arraystretch}{1.3}
	\label{sequentialmnist}
	\begin{center}
		\begin{tabular}{l c}
			\toprule[1.5pt]
			Methods &Testing Error Rate \\
			\hline
			Full Precision            &   $1.10~\%$ \\
			\hline
			Refined \citep{refinedgreedy} \qquad\qquad\qquad  & $1.39~\%$ \\
			Alternating (ours)        &    $\mathbf{1.19}~\%$ \\
			\hline
		\end{tabular}
	\end{center}
\end{table}




\textbf{MLP on MNIST.} The alternating quantization proposed in this work is a general technique. It is not only suitable for RNNs, but also for feed-forward neural networks. As an example,  we firstly conduct a classification task on MNIST and compare with existing work \citep{greedy2}. The method proposed in \citep{greedy2} is intrinsically a greedy multi-bit quantization method. For fair comparison, we follow the same setting. We use the MLP consisting of $3$ hidden layers of $4096$ units and an L2-SVM output layer. No convolution, preprocessing, data augmentation or pre-training is used. We also use ADAM \citep{adam} with an exponentially decaying learning rate and Batch Normalization \citep{bn} with a batch size 100. The testing error rates for $2$-bit input, $2$-bit weight, and $1$-bit activation are shown in Table \ref{mnist}. Among all the compared multi-bit quantization methods, our alternating one achieves the lowest testing error.

\begin{table}[!t]
	\caption{Testing error rate of MLP on MNIST with $2$-bit input, $2$-bit weight, and $1$-bit activation.}
	\renewcommand{\arraystretch}{1.3}
	\label{mnist}
	\begin{center}
		\begin{tabular}{l c}
			\toprule[1.5pt]
			Methods & Testing Error Rate \\
			\hline
			Full Precision                                &$0.97~\%$ \\ \hline
			Greedy (reported in \citep{greedy2})           &  $1.25~\%$\\	
			Refined \citep{refinedgreedy} \qquad\qquad\qquad  & $1.22~\%$  \\
			Alternating (ours)        &    $\mathbf{1.13}~\%$ \\ \hline			
		\end{tabular}
	\end{center}
\end{table}

\textbf{CNN on CIFAR-10.} We then conduct experiments on CIFAR-10 and follow the same setting as \citep{lab}. That is, we use 45000 images for training, another 5000 for validation, and the remaining 10000 for testing. The images are preprocessed with global contrast normalization and ZCA whitening. We also use the VGG-like architecture \citep{vgg}:

$(2\times128$ C3)$-$MP2$-$($2\times256$ C3)$-$MP2$-$($2\times512$  C3)$-$MP2$-$($2\times1024$ FC)$-$10 SVM

where C3 is a $3\times3$ convolution layer, and MP2 is a $2\times2$ max-pooling layer. Batch Normalization, with a mini-batch size of $50$, and ADAM are used. The maximum number of epochs is $200$. The learning rate starts at $0.02$ and decays by a factor of $0.5$ after every $30$ epochs. The testing error rates for $2$-bit weight and $1$-bit activation are shown in Table \ref{cifar}, where our alternating method again achieves the lowest test error rate among all compared quantization methods. 

\begin{table}[!t]
	\caption{Testing error rate of CNN on CIFAR-10 with $2$-bit weight and $1$-bit activation.}
	\renewcommand{\arraystretch}{1.3}
	\label{cifar}
	\begin{center}
		\begin{tabular}{l c}
			\toprule[1.5pt]
			Methods &Testing Error Rate \\
			\hline
			Full Precision (reported in \citep{lab}) &   $11.90~\%$ \\
			\hline
			XNOR-Net ($1$-bit weight $\&$ activation, reported in \citep{lab}) \qquad &$12.62~\%$ \\	
			Refined \citep{refinedgreedy} \qquad\qquad\qquad  & $12.08~\%$  \\
			Alternating (ours)        &    $\mathbf{11.70~\%}$ \\
			\hline
		\end{tabular}
	\end{center}
\end{table}
\end{document}
