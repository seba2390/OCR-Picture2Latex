\section{Environment}
\label{sec:environment}

Following the reinforcement learning literature, we model GUI interaction as a Markov Decision Process (MDP): at each time step, our agent receives an observation and selects an action.
We develop a common environment framework with shared observation and action formats for browser-based tasks.
Similarly to prior work on web-based agents \citep{liu2018reinforcement}, we use Selenium to programmatically interact with the Google Chrome browser.

\paragraph{Observations} To form an observation, we first take a screenshot of the current browser window using Selenium and then augment it with additional information. First, if not already present, we render the natural language instruction on the top of the screenshot, following \citet{lee2022pix2struct}.
Second, as Selenium screenshots do not include cursors (which are typically rendered by the operating system), we draw a cursor on the screenshot to indicate the mouse pointer position. Finally, we render an indicator of whether the mouse button is currently pressed down, which is useful for dragging actions.

\paragraph{Actions}  Our action space consists of raw mouse and keyboard actions, as shown in Figure~\ref{fig:agent_env}, where \texttt{X} and \texttt{Y} refer to discrete coordinate bins, \texttt{K} is one or more keys, \texttt{M} is an optional modifier key such as ``shift'', and \texttt{Z} refers to a vertical scroll amount, also represented as a discrete bin.\footnote{We chose discrete bins because they enable a simple encoding of actions as tokens. Alternatives could include continuously-valued coordinates or relative movements with foveated binning~\citep{baker2022video}.} The \texttt{begin\_drag} and \texttt{end\_drag} actions can be used to execute ``click and drag'' actions. We use a configurable number of coordinate buckets per vertical and horizontal axis. Importantly, the DOM information is not provided by the environment and is therefore not used in any way to define observations or actions.

\paragraph{Episodes and Rewards} Episodes continue until a terminal state or a configurable number of maximum steps is reached. For the environments we consider, the agent only receives a reward at a terminal state. This can be a binary reward based on whether the task was completed successfully or 
a partial reward based on how well the task was completed. 

