\section{Related Work}
\label{sec:related-work}

We focus on agents that interact with GUIs, such as operating system dialogs or web pages, to accomplish a given task.
Many early approaches relied on the structured information from the GUIs \citep{zettlemoyer1999visual,allen2007plow,branavan2010reading}. This information could range from a flat list of GUI components and their properties, to the full hierarchical structure of the components (\eg the DOM tree).
The output space also depends on this structured information, often using GUI components as action targets (\eg clicking button \#7). As discussed in \S\ref{sec:introduction}, such structured information might not always be available, or might not align with what visually appears to the users.


When \citet{shi2017world} introduced the \emph{World of Bits} tasks, which was the precursor to MiniWob++ \citep{liu2018reinforcement},
they proposed a model based on a convolutional neural network that takes both visual and structured inputs and then performs generic low-level computer actions (\eg clicking at a coordinate or pressing a key), similarly to \ours.
However, the model performed poorly compared to humans.
Follow-up work studied specialized architectures for incorporating structured DOM information and restricted the action space to clicking and typing predetermined texts on DOM elements \citep{liu2018reinforcement,gur2018learning,jia2019dom}.
\citet{humphreys2022data} reconsidered incorporating both visual and structured information as well as a low-level action space that aligns better to the human demonstrations. We discussed their approach, CC-Net, in \S\ref{sec:ablations}. \citet{humphreys2022data} also explored the benefits of large-scale human demonstrations, and we build on their work to utilize a large number of human demonstrations to train \ours.
This paper shows that \ours, a model with pixel-only inputs, can outperform humans on MiniWob++ and match the state-of-the-art approaches that rely on DOM information.

Automating web-based tasks using large language models (LLMs) has also been broadly explored. For instance, WebGPT uses a text-based web browsing environment to search and navigate the web \citep{nakano2021webgpt}.
More relatedly, recent work has investigated prompting LLMs to produce agents that can generalize to tasks based on a small number of in-context examples.
\citet{yao2023react} proposed ReAct, a few-shot prompted LLM, which uses observations derived from HTML and a custom action space to make predictions based on explicit reasoning steps.
Similarly, \citet{kim2023language} proposed RCI, a prompted LLM that iteratively critiques and refines its outputs, also using HTML inputs and custom action spaces.
These approaches achieve competitive performance on WebShop and MiniWob++, respectively, and are extremely sample-efficient, relying on just a handful of demonstrations per task. 
\citet{gur2022understanding} treated raw HTML as a string and fed it to LLMs pretrained on natural language. After fine-tuning them on demonstrations, 
the models improved MiniWob++ task success rate and sample efficiency compared to models that take DOM-based inputs and specialized architectures.
Finally, WebGUM \citep{webgum23}, discussed in \S\ref{sec:ablations}, extends HTML-based models to integrate a vision encoder pretrained on ImageNet-21K.

Other work has focused on tasks related to mobile apps. \citet{li2022spotlight} considered a model with pixel-based inputs similar to that of \citet{lee2022pix2struct}, and included evaluations on tasks related to grounding instructions to screenshots, but did not consider interactive environments. Some work has considered instruction following tasks in mobile app environments \citep{li2020mapping,burns2022interactive}, but has generally not studied observation and action formats similar to ours, instead relying on inputs based on the Android view hierarchy.
We focused on web-based GUIs so that we could use a consistent environment framework for simplicity.
Besides GUIs, several works on video game agents also considered visual-only input and low-level actions.
For example, most works on Atari games used the screenshot as visual input and predicted the controller buttons to press \citep{Mnih2015HumanlevelCT}.
More recently, \citet{baker2022video}, which focuses on learning from unlabeled videos, proposes an agent for Minecraft that uses pixel-based inputs paired with keyboard and mouse actions, similarly to \ours. 
