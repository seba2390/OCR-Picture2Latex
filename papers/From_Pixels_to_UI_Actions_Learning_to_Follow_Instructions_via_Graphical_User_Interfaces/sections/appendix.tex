
\newpage

\section{Additional Dataset Details}
\label{sec:appendix-a}
\subsection{MiniWob++ Supported Tasks}

MiniWob++ consists of 104 tasks. Most prior work~\citep{shi2017world, liu2018reinforcement, gur2018learning, jia2019dom} has evaluated performance on only a subset of these tasks, with the notable exception of~\citet{humphreys2022data}, which evaluated on all 104 tasks. We evaluated on 59 of these 104 tasks, based on our best effort attempt to (1) design a general purpose set of actions that could be implemented using Selenium and (2) convert the demonstrations collected by~\citet{humphreys2022data} to our observation and action format. While further development of the conversion process and Selenium-based actions could potentially support more tasks, the 59 tasks we support still include a wide range of instructions and interactions. Note that determining the set of 59 tasks was based solely on the feasibility of conversion to our observation and action format, and \emph{not} based on model performance. Below we offer further details. 

Several tasks in MiniWob++ feature animated elements. These tasks can require sampling observations in a real-time manner in order to capture the information needed to select the correct action. Also, the effects of an action may be delayed and therefore not captured by an observation sampled immediately after the action has executed. MiniWob++ provides a \texttt{-nodelay} version for several tasks which removes such animations. We train and evaluate on the \texttt{-nodelay} version of these tasks (%
\texttt{choose-date},
\texttt{click-collapsible-2},
\texttt{click-collapsible},
\texttt{click-pie},
\texttt{use-autocomplete}). We exclude \texttt{choose-date-easy} and \texttt{choose-date-medium} which offer simpler versions of \texttt{choose-date} but do not have a corresponding \texttt{-nodelay} version. Additionally, we exclude \texttt{chase-circle}, \texttt{drag-cube}, \texttt{moving-items}, and \texttt{simon-says}, which feature animation without a \texttt{-nodelay} version.

Many MiniWob++ tasks also involve vertical scrolling. In the human demonstrations, this can be implemented using a scroll wheel, or various clicking or dragging interactions with a vertical scroll bar rendered on the right side of a scrollable element. Mapping such interactions to actions that lead to equivalent scrolling in our Selenium-based environment is non-trivial. 
Therefore, for simplicity, we excluded tasks that involve scrolling: \texttt{book-flight},
\texttt{click-scroll-list},
\texttt{email-inbox},
\texttt{email-inbox-nl-turk},
\texttt{read-table},
\texttt{read-table-2},
\texttt{scroll-text},
\texttt{scroll-text-2},
\texttt{search-engine},
\texttt{social-media},
\texttt{social-media-all},
\texttt{social-media-some},
\texttt{terminal}.

Demonstrations for many MiniWob++ tasks also include copying and pasting text. In many cases, this was executed in the human demonstrations by double clicking a text string and then clicking and dragging it into an input field. Such an interaction is not supported in Selenium, which made it challenging to support these tasks. This led us to exclude the following tasks: 
\texttt{login-user-popup},
\texttt{copy-paste},
\texttt{copy-paste-2},
\texttt{email-inbox-forward},
\texttt{email-inbox-forward-nl},
\texttt{email-inbox-forward-nl-turk},
\texttt{email-inbox-noscroll},
\texttt{email-inbox-reply},
\texttt{email-inbox-star-reply},
\texttt{enter-password}, 
\texttt{enter-text}, 
\texttt{enter-text-dynamic}, 
\texttt{find-word}, 
\texttt{login-user}, 
\texttt{multi-layouts}, 
\texttt{multi-orderings}.

Finally, we excluded several other tasks for various other reasons. The \texttt{choose-list} task uses the HTML \texttt{<select>} tag to implement a drop-down menu, which is not supported properly by our Selenium-based environment. The \texttt{click-menu} and \texttt{click-menu-2} tasks require unsupported mouseover effects. Demonstrations for the \texttt{text-editor} task features click and drag interactions to highlight text which do not have the same effect when executed in Selenium. There also appeared to be differences in how Selenium implemented the number input field for \texttt{guess-number}. Finally, we excluded several tasks due to low demonstration conversion success rates (\texttt{focus-text}, \texttt{focus-text-2}, \texttt{use-spinner}). Upon further investigation, this was due to episodes completing immediately after a ``pointer down'' event without a complete click for \texttt{focus-text} and \texttt{focus-text-2}, and due to frequent double clicking for \texttt{use-spinner}. 
\subsection{MiniWob++ Rendering Differences}

There are differences between the rendering of observations in the human demonstrations from \citet{humphreys2022data} and the rendering of environment state in our Selenium-based environment. We show an example in Figure~\ref{fig:render_diff}, which shows subtle differences, \eg in font style and in element sizes and positions.

\begin{figure}[h!]
\centering
\begin{subfigure}[h]{0.45\textwidth}
\centering
\includegraphics[keepaspectratio,height=4cm]{images/compare_env_ours.png}
\end{subfigure}
\begin{subfigure}[h]{0.45\textwidth}
\centering
\includegraphics[keepaspectratio,height=4cm]{images/compare_env_humphreys_env.png} % width=\columnwidth,height=8cm,
\end{subfigure}
\caption{Comparison of  differences between the screenshots of the human demonstrations for MiniWob++ from \cite{humphreys2022data} (right) with how the same environment state is rendered in our Selenium-based environment (left).} 
\label{fig:render_diff}
\end{figure}




\section{Additional Technical Details}
\label{sec:appendix-b}

\subsection{Beam Search}
\label{sec:appendix-beam}
As mentioned in \S\ref{sec:agent}, we use beam search over tokens in the text decoder to produce a set of top-$k$ actions for a given state, along with their approximate probabilities. We refer to these as approximate probabilities because they are subject to a length normalization factor~\citep{wu2016google} of $0.6$ during beam search, following \citet{raffel2019exploring}. For MiniWob and WebShop, our experiments used $k=8$ and $k=10$, respectively.

\subsection{Tree Search}
\label{sec:appendix-mcts} Here we describe the details of the tree search approach described in \S\ref{sec:agent-training}. We adopt Monte Carlo Tree Search (MCTS)~\citep{Coulom2006EfficientSA}, and follow prior work which has integrated MCTS with neural networks \citep{silver2017mastering,anthony2017thinking}, which we apply to MiniWob++ environments. We performed a minimal amount of tuning to determine an approach that yielded improvements in mean score over the greedy policy, even for the most challenging tasks.

\paragraph{Problem Setting} We consider an environment with states $\mathcal{S}$ and actions $\mathcal{A}$. The reward function, $r(s)$, returns a scalar corresponding to the reward given for transitioning to state $s \in \mathcal{S}$, and is described below.
MiniWob++ environments are randomly generated, but transitions are deterministic within an environment generated by a particular random seed. The transition function, $f(s,a)$, returns the state resulting from taking action $a \in \mathcal{A}$ in state $s \in \mathcal{S}$.

\paragraph{Surrogate reward} Rather than using the raw reward directly provided by the MiniWob++ environment, we consider a surrogate reward: $r(s) = \alpha_{s} + r^t(s)$, where $\alpha_{s}$ provides a small negative reward that encourages shorter trajectories without unnecessary actions. $r^t(s)$ is the raw reward from the MiniWob++ environment if $s$ is a terminal state and the raw reward is $> 0.8$, or $0$ otherwise. We use $\alpha_{S} = -\frac{1}{30}$. As all tasks can be completed within 30 steps, this is small enough to ensure a positive reward is possible for all tasks. Additionally, the penalty is small enough such that in practice the agent should not be incentivized to sacrifice raw reward to reduce the number of steps taken.

\paragraph{Value network} The value function $v^\pi(s)$ for a given policy $\pi$ 
is the expected future rewards from state $s$ if actions are selected according to policy $\pi$. The optimal value function, $v^*(s)$, is the expected future rewards if optimal actions are chosen. We attempt to learn an approximation of this function, $\hat{v}_\phi(s) \approx v^*(s)$, parameterized as a \textsc{Pix2Struct}-initialized model with parameters $\phi$, which we refer to as the \emph{value network}. The model is trained on transitions from the human demonstrations, which demonstrate close to optimal behavior in many cases. For every state in the human demonstrations, we compute the actual future rewards for the given episode, according to the surrogate reward. We map these future rewards to discrete bins and represent them as integers in the \textsc{Pix2Struct} decoder. At inference time, we approximate the mean of the distribution over these discrete bins by considering the top-$n$ predictions from the model using beam search (with $n=3$), weighted proportional to their respective probabilities.

\paragraph{Policy network} For consistency with prior work, we will refer to the \textsc{Pix2Struct} model tuned to generate actions (\ie \textsc{Pix2Act}) as the \emph{policy network}, with parameters $\theta$. The greedy policy $\pi_\theta(s)$ selects the action $a$ with the highest approximate probability $p_\theta(a|s)$ in the top-$k$ beam (see \S\ref{sec:appendix-beam}), subject to the conditions described in $\S\ref{sec:agent}$.

\paragraph{Search policy} We can use lookahead search to implement a policy, $\pi^*_\theta(s)$, which leverages interactions with the environment ($f(s,a)$ and $r(s)$) to select actions in a more optimal way than the greedy policy $\pi_\theta(s)$. Both the policy network and value network are used to constrain and prioritize the search. 

MCTS performs $K$ rounds of traversing a search tree with nodes corresponding to states, and edges corresponding to actions. 
Due to the computational cost of the policy and value networks, we use a modest number of rounds, $K=16$, for our experiments. The search tree is initialized with a single root node for state $s$. Each round starts at $s$ and traverses the tree. At each step $t$ of a given round, an action $a_t$ is selected for state $s_t$, where $a_t = \max_a Q(s_t,a) + U(s_t,a)$. $Q(s_t,a)$ is an average reward over all rounds that have traversed the associated edge. It is based on actual accumulated rewards during tree traversal and the value estimates of leaf states (described below). $U(s_t,a) = c * p_\theta(a|s) * \frac{\sqrt{N(s_t)}}{1+n(s_t,a)}$ is a term that encourages exploration, where $n(s_t,a)$ is the number of times action $a$ has been selected from state $s_t$, $N(s_t)$ is the total number of times state $s_t$ has been visited, and $c$ is a scalar hyperparameter that we set to $0.1$. 
Following ~\cite{silver2017mastering}, we use the policy network to bias this exploration term. To constrain the search, we only consider the top-$k$ actions according to the policy network, where $k=8$ in our experiments. 

If we select an action $a_t$ for state $s_t$ which has never been previously selected from $s_t$, then the simulation ends and we add a new leaf state, $s_L = f(s_t,a)$, to the search tree. If $s_L$ is not a terminal state, then we estimate its value (\ie future returns) using both the value network and a rollout with the greedy policy. Specifically, following \citet{silver2017mastering}, we estimate its value as $\lambda * \hat{v}_\phi(s_L) + (1 - \lambda) * v^{\pi_\theta}(s_L)$ where $v^{\pi_\theta}(s_L)$ is equal to the actual returns from following the policy $\pi_\theta$ starting at $s_L$ for a maximum of $20$ steps, with actual returns clipped to a minimum value of $0$.
Is there $\lambda$ is a mixing parameter that we set to $0.1$. For challenging environments, rollouts may be unlikely to find a terminal state with positive reward, and in such cases rollouts may not be very informative. On the other hand, the value network can provide poor value estimates for certain states, especially if they are not well represented in the human demonstrations. By combining both methods we aim to provide a better approximation of the value of leaf states. Returns are propagated up the tree to each parent $s'$ to update $Q(s',a)$. As $Q(s_L,a)$ is undefined prior to selecting $a$ from $s_L$ for the first time, we initialize $Q(s_L,a)$ for each action to be equal to the initial value estimate of $s_L$ plus $\alpha_s$.

To understand the impact of rollouts and value estimates using the value network, in Table~\ref{tab:rollout_vs_value_network} we compare mean scores over 12 challenging MiniWob++ tasks for different values of $\lambda$: 0 (rollout only), 0.1 (both rollout and value network), and 1 (value network only). We also include the mean score using the greedy policy for reference. These results use the policy network and value network trained on the human demonstrations. The results show that using a combination of rollouts and the value network gives the best results. The value network is primarily useful for challenging tasks that require longer trajectories, such as \texttt{number-checkboxes}, relative to using rollouts only.

\begin{table}[h!]
\centering

\scalebox{0.8}{
\begin{tabular}{cccc}
\toprule
Greedy Policy & $\lambda=0$ (rollout only) & $\lambda=0.1$ & $\lambda=1$ (value network only) \\
\midrule
28.8 & 74.2 & \bf 78.3 & 57.4  \\
\bottomrule
\end{tabular}
}
\vspace{0.07 in}
\caption{Mean scores for different policies over 12 challenging MiniWob++ tasks.}
\label{tab:rollout_vs_value_network}
\end{table}

Once we have completed $K$ rounds, $\pi^*_\theta(s)$ selects the most visited action $a$ for state $s$, and we begin the process again at the subsequent state. We reuse the search tree for subsequent time steps for efficiency, so we require only $K - n(s,a)$ additional rounds for the subsequent state.

\paragraph{Policy improvement} We can sample trajectories with $\pi^*_\theta$, then update $\theta$ by training $\pi_\theta(s)$ to approximate $\pi^*_\theta(s)$ for each $s$ in the sampled trajectories. This then also improves $\pi^*_\theta(s)$, as $\theta$ informs how the search space is constrained and prioritized. Therefore, we can continue to iteratively improve $\pi_\theta(s)$. To produce these trajectories, we randomly sample MiniWob++ tasks and seeds, and select actions according to $\pi^*_\theta$. We then filter trajectories where the raw reward is $< 0.8$. We then tune $\theta$ on these new trajectories. For simplicity, we keep the value network (\ie $\phi$) fixed.

We initially found that tuning on trajectories from MCTS could be unstable, leading to an early loss spike. To resolve this, we slightly decreased the learning rate (from $1e-3$ to $5e-4$) and increased the number of warmup steps (from $1000$ to $4000$) relative to the hyperparameters used for behavioral cloning.

\subsection{Compute Details} We fine-tuned models using 64 Google Cloud TPU v3 cores.

\section{Additional Results}
\label{sec:appendix-c}

\subsection{Variance Estimates}

We evaluated results for MiniWob++ based on 100 randomly selected seeds for each of the 59 tasks. To understand how results vary depending on which 100 seeds per task are used for evaluation, we ran 3 trials with different evaluation seeds for the strongest \ours model reported in Table~\ref{fig:kshot_main}, yielding mean scores of $96.2$, $96.4$, and $96.1$; the standard deviation across these trials was $0.15$. For WebShop, there is a standard test set consisting of 500 instances, so selecting seeds for evaluation is not necessary.

\subsection{MiniWob++ Results Per Task}

We show the performance of \ours (ours) on each of the 59 MiniWob++ tasks we study, compared to other approaches, in Table~\ref{tab:miniwob-per-task}. We compare with human crowdworker performance reported by \citet{humphreys2022data}, CC-Net \citep{humphreys2022data}, DOM-Q-Net \citep{jia2019dom}, DOMNET with workflow-guided execution \citep{liu2018reinforcement}, QWeb \citep{gur2018learning}, RCI \citep{kim2023language}, WebN-T5-3B \citep{gur2022understanding}, and WebGUM \citep{furuta2023instruction}. We also report scores for \ours and CC-Net with behavioral cloning (BC) only. We do not include scores for GlobalCNN \citep{shi2017world}, which reported only human normalized success rates. Other than \citet{humphreys2022data}, prior work has primarily reported success rate (\ie the percentage of episodes with positive rewards), which can be equivalently mapped to the scores we report for tasks without partial rewards.

\input{tables_and_figures/miniwob_per_task}

