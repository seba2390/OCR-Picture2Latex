\section{Benchmarks and Demonstrations}
\label{sec:datasets}

We adapt two benchmarks, MiniWob++ and WebShop, to our environment framework (\S\ref{sec:environment}) which consists of pixel-based observations and generic low-level actions. We also map previously collected human demonstrations for these benchmarks to our observation and action spaces. 

\subsection{MiniWob++}
\label{sec:datasets-miniwob}

MiniWob++ \citep{liu2018reinforcement} is a set of over a hundred web-browser based tasks. 
See Figures~\ref{fig:agent_env}~and~\ref{fig:episode_examples} for task examples. Each task consists of an algorithm for generating variations of the task and an instruction template, controlled by a random seed, with up to billions of possible configurations per task.
The task instruction is given as (mostly) natural language text in the top yellow part, which in our framework can only be accessed visually. An automatic reward is given at the end of the task.

\paragraph{Human Demonstrations} We use the human demonstrations collected by \citet{humphreys2022data}. However, their demonstrations were collected using an X11-based environment, which is different from our Selenium-based environment. This results in different renderings of the same underlying environment state, introducing a shift between the screenshots seen during training and those observed at test time. Additionally, we need to map from their real-time X11-based action sequences to our action space. We were able to perform this mapping with a reasonable degree of success for 59 tasks. Notably, not all behaviors in the human demonstrations are supported in our Selenium-based environment. For example, Selenium does not implement the ability to highlight text and drag it into a text field, and such an action is widely used in the human demonstrations for tasks where text is copied and pasted. Additionally, while our environment framework intends to cover the basic functionality of most web interfaces, aspects of some MiniWob++ tasks, such as capturing real-time observations for animated elements, are not supported. See Appendix~\ref{sec:appendix-a} for additional details.\footnote{Other prior work has used the demonstrations from \citet{liu2018reinforcement}, which cover a different subset of MiniWob++ tasks. However, these demonstrations do not include screenshots or sufficient information to replay the episodes in a browser environment to collect new screenshots, and therefore cannot be applied to our setting.}

Starting with approximately 1.3 million demonstrations across the 59 supported tasks, we filtered demonstrations with a reward of $<0.8$, or approximately 6\% of demonstrations. We were able to successfully convert 81\% of the remaining demonstrations to our action space. We reserve 10\% of the data for a development set. Demonstrations contain approximately 3 steps per task on average, although this varies considerably across tasks.

\paragraph{Evaluation}

We report the mean score across seeds and tasks. The score is the MiniWob++ raw reward (without time decay) mapped from the original range $[-1,1]$ to the range $[0,100]$.  The score is equivalent to the success rate (\ie the proportion of episodes in which the agent receives a positive reward) for tasks with binary rewards. For episodes that do not complete due to reaching a maximum number of allowed steps, we assume a score of $0$. For each task, we compute the mean over 100 random seeds, and then compute the mean over 59 MiniWob++ tasks.

\subsection{WebShop}

WebShop \citep{yao2022webshop} is a web-based shopping environment with over 1.1 million products from Amazon. The task is to find and purchase a product based on a human-authored text instruction. Finding a suitable product requires entering search queries, clicking on results, and determining the relevance of various products to the instruction. 
An automatic reward is computed based on similarity between the purchased product and the gold target product.

\paragraph{Human Demonstrations}
We use the 1,566 human demonstrations (with a train/development/test split of 1012/54/500) collected in \citet{yao2022webshop}. As with the MiniWob++ demonstrations, we need to map between the observation and action sequences used in their setup to our framework. \citet{yao2022webshop} used high-level actions (\eg ``\texttt{search}'' or ``\texttt{click[item]}''), each of which could map to multiple lower-level actions in our environment.
Specifically, for all actions involving a mouse click, we determine the coordinates of the center of the corresponding HTML element. For WebShop, the entire screen content is not always visible due to page heights exceeding the viewport dimensions. If the clicked element lies outside the visible area, we add scroll actions until the element is visible. Finally, we map \texttt{search} actions to two actions in our environment: clicking on the center of the search box and entering the search query followed by the \emph{enter} key. We render the HTML inputs in the human demonstrations using our browser to obtain screenshots. Additionally we found that rendering the last 5 actions (separated by \texttt{<s>}) on top of the screenshot to be helpful.

\paragraph{Evaluation}
Consistent with previous work, we report Task Score, which is the average reward across 500 test instructions.

\section{Experiments and Analysis}

\input{tables_and_figures/fig_main_barchart}

\subsection{Training Details}

 We updated all model parameters during fine-tuning, including both the image encoder and text decoder. We used the Adafactor optimizer~\citep{shazeer2018adafactor}  with a learning rate of 0.01.
 
\label{sec:train-details}
\paragraph{MiniWoB++} We finetuned a single model jointly on episodes from all tasks for a total of 26K steps using a batch size of 512, input/output sequence lengths of 512/16. We also evaluated using the tree search procedure described in \S\ref{sec:agent-training} to improve our agent. We performed 2 iterations of policy improvement with tree search, collecting a total of 826K episodes across all tasks, and tuning for a further 26K steps.

\paragraph{WebShop} We used only the provided human demonstrations to train our model.\footnote{We did not explore applying RL techniques to WebShop in this work. Prior work~\citep{yao2022webshop} has not shown as significant an advantage to applying RL on WebShop relative to the large improvements shown by prior work on MiniWob++, which offers a near limitless variety of environments with reward signals for training.} Due to its larger resolution and text-heavy data, we used a higher input sequence length of 4096. We also found it useful to perform intermediate finetuning on MiniWoB++, followed by 10K steps of further finetuning on WebShop using a batch size of 256 (see \S\ref{sec:ablations} for details). 

\input{tables_and_figures/fig_ours_vs_human}

\subsection{Main Results}
\label{sec:main-results} We report the results of our models on MiniWob++ and WebShop in Figure~\ref{fig:kshot_main}. For MiniWob++, we also provide task-level comparisons between \ours and human crowdworkers in Figure~\ref{fig:head2tail}. There is limited prior work studying these tasks without access to DOM and HTML information. For MiniWob++, the only comparable baselines are from the CC-Net model of \citet{humphreys2022data}, which mentions an ablation experiment where performance dropped by 75\% from their primary results when the models conditioned on only screenshots without DOM information. As they did not provide per-task numbers for this ablation, we estimate the performance of CC-Net without DOM information by assuming that the drop in performance on the subset of tasks we study was also 75\%. Regardless, it is clear that \ours significantly outperforms CC-Net on this setting. The difference in performance can be largely attributed to the screenshot parsing pre-training of ~\citet{lee2022pix2struct}. For WebShop, there is no prior work exploring such a setting, so we establish the first baseline. 

\subsection{Ablations and Analysis}
\label{sec:ablations}
\paragraph{Pre-training ablations} To study the impact of the pre-training on our model's ability to effectively learn to follow instructions via GUIs, we evaluate model performance without the pre-training procedure. For these experiments, we only compared performance of models trained using behavioral cloning. The results are shown in Figure~\ref{fig:kshot_main}, and demonstrate that pre-training is critical for our model's performance.

\paragraph{Comparison with models that use DOM or HTML as input} We can also compare our results without access to DOM or HTML to previous methods that utilized these resources, including those which also leverage DOM information to construct specialized action spaces. The performance of the best model from prior work leveraging DOM or HTML information is shown in Figure~\ref{fig:kshot_main}.

For MiniWob++, the best model on this setting is CC-Net~\citep{humphreys2022data} trained with BC and RL and with access to both DOM and pixel-based observations.\footnote{We compute mean scores for CC-Net by averaging their reported per-task results over the 59 tasks we study.} 
\ours achieves comparable performance to their best model, while relying on only a subset of the information used by CC-Net, and using a comparable number of human demonstrations for training.
\ours also outperforms CC-Net when each model is trained only with behavioral cloning, as CC-Net performance on this setting drops to 38.7 (results not shown in the Figure). Notably, CC-Net scores also drop by approximately 10\% when the model is not given access to a dictionary of input strings provided by the environment. As shown in Figure~\ref{fig:kshot_main}, the key to our model's ability to achieve comparable performance without relying on DOM-based inputs is pixel-based pre-training. Another difference is that CC-Net uses a real time setting, which enables some forms of interaction not supported by our environment, and therefore can support a larger set of MiniWob++ tasks. On the other hand, for BC, CC-Net does not need to handle the shift in rendering format and potentially noisy action space conversion. 

For WebShop, the best model on this setting is WebGUM~\citep{furuta2023instruction}, which leverages the HTML source,
a custom action space for the shopping domain, and a Flan-T5-XL~\citep{chung2022scaling} backbone.
WebGUM outperforms \ours when compared on this setting. Some of this gap can be attributed to their simplified high-level action space, direct access to the relevant text on the page, and ability to transfer from Flan-T5's pretraining scale and instruction finetuning. Comparable improvements to the scale and pretraining of pixel-based models could reduce this gap.


We discuss other approaches that leverage DOM or HTML information further in \S\ref{sec:related-work}. We also offer a complete comparison across all MiniWob++ tasks in Appendix~\ref{sec:appendix-c}.

\paragraph{Evaluating transfer across tasks}
Training a pretrained, pixel-based model to interact with a GUI can intuitively lead to better generalization to new tasks that use common GUI design principles. To study this, we evaluate the ability of \ours{} (without RL) to generalize to tasks unseen during training. Specifically, we hold out 9 out of 59 tasks and train on the remaining 50.\footnote{We manually pick the 9 tasks to verify they include only actions or elements that would be reasonable to generalize to from the training tasks. The tasks are \texttt{click-checkboxes-large}, \texttt{click-color}, \texttt{click-tab-2}, \texttt{click-tab-2-hard}, \texttt{count-shape}, \texttt{drag-shapes}, \texttt{use-color-wheel-2}, \texttt{use-slider-2}.}  We then evaluate performance on the held-out tasks, comparing initializing with \textsc{Pix2Struct} to random initialization.
Table~\ref{tab:heldout} illustrates that \ours{} can reach a mean score of 28.3 on held out tasks compared to 65.5 when training on those tasks. Conversely, mean score is 7.6 when \textsc{Pix2Struct} initialization is not used. This shows that combining pretraining with a general GUI interface can lead to non-trivial generalization to held out tasks.

\input{tables_and_figures/combined_heldout_rl}

For WebShop, we find that finetuning directly on WebShop (without intermediate finetuning on MiniWoB++ as mentioned in ~\ref{sec:train-details}) results in a drop of 4.0 in Task Score, demonstrating transfer learning benefits across these datasets.

\paragraph{Tree search analysis} Table~\ref{tab:tree-search} shows the improvement in MiniWob++ scores by training on episodes generated by tree search. After an initial round of training on episodes generated by tree search, the effectiveness of tree search also improves due to improvements in the underlying model used to guide the search. The best greedy policy achieves performance close to the best tree search policy, but does not require access to reward signals or additional exploration at inference time. Our results indicate that we could further improve performance with more iterations of policy improvement via tree search.
