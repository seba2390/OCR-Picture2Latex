\section{Introduction}
\label{sec:introduction}

Systems that can follow instructions to complete tasks through graphical user interfaces (GUIs) can help automate tedious tasks,
improve accessibility, and expand the usefulness of digital assistants by allowing them to interact with tools and services.
Despite the visual nature of GUIs, prior work has primarily focused on utilizing structured representations of the user interfaces (such as HTML sources, Document Object Model (DOM) trees, and Android view hierarchies) as well as custom, task-specific representations of high-level actions based on these structured representations (see \S\ref{sec:related-work}).
Recent efforts have achieved positive outcomes thanks to the advances of powerful language models \citep{gur2022understanding,kim2023language, yao2022webshop}.

While structured and task-specific representations may be useful, they are not always available -- some examples are web applications that use extensive scripting, sandboxed environments where access to DOM is limited, and mobile applications which often do not expose the underlying structure to external modules. Even when structured application source data is available, it may be hard to interpret due to obfuscation and misalignment with what actually appears on the GUIs.
Finally, aligning human demonstrations with task-dependent actions is often challenging.

In contrast, people interact with GUIs by perceiving the visual input and using generic mouse and keyboard actions, without needing to inspect the application's source code for cues on its functionality. They can quickly learn to interact with new applications that offer familiar visual interfaces, regardless of differences in implementation technologies. In this paper we ask: \emph{Can we build an agent that can complete tasks for users while relying solely on pixel-level visual representations of the GUI state, and generic low-level actions?}

\begin{figure}[t!]
    \centering
    \includegraphics[width=\columnwidth,height=12cm,keepaspectratio]{images/agent_env_new.003.pdf}
    \caption{\textbf{Our agent learns to follow instructions via Graphical User Interfaces (GUIs).} Unlike most prior work studying instruction following for GUI-based tasks, our agent does not rely on text-based observations corresponding to DOM trees or HTML source code, or task-specific actions. Instead, our agent receives pixel-based observations and generates outputs corresponding to mouse and keyboard actions. The possible actions are encoded as text and shown on the top of the figure. We show examples of observations from various episodes for two benchmarks, MiniWob++ (top row) and WebShop (bottom row), that we adapt to study within the context of our general Chrome-based environment framework. See details in \S\ref{sec:environment}.} 
    \label{fig:agent_env}
\end{figure}

Learning based on pixel-only inputs proved effective for game playing environments such as Atari \citep{Mnih2015HumanlevelCT}. However, for GUI-based instruction following tasks, learning from pixel-only inputs coupled with general low-level actions leads to several challenges.
Interpreting GUIs visually requires understanding the interface layout, recognizing and interpreting visually-situated natural language, identifying visual elements, and predicting their functions and methods of interaction. A generic action space also poses the challenge of a more complex mapping between high-level textual instructions and corresponding sequences of low-level actions. As an example of the increased difficulty in this setting, on the MiniWob++ benchmark \citep{shi2017world,liu2018reinforcement} of web GUI interaction, CC-Net \citep{humphreys2022data} demonstrates human-level accuracy when accessing both screenshots and DOM structure, but its performance drops by 75\% when the DOM information is removed from the agent's observations.

Here we present \ours, a model that relies solely on pixel-based screenshots as input and selects actions corresponding to basic mouse and keyboard functionalities.\footnote{Code and models are available at \url{https://github.com/google-deepmind/pix2act}.}
We build on {\textsc{Pix2Struct}}~\citep{lee2022pix2struct}, a Transformer-based \citep{vaswani2017attention} image-to-text model pre-trained to map screenshots to structured  representations derived from HTML on web-scale data.
\ours tunes this model using a combination of human demonstrations and environment interactions, applying tree search to iteratively generate new expert trajectories for training. We develop a general browser-based environment framework, and adapt two benchmark datasets, MiniWob++ and WebShop~\citep{yao2022webshop}, to our setting with a unified, general purpose observation and action format.

On MiniWob++, \ours outperforms human crowdworkers and improves task score nearly 4x compared to the best prior results in our proposed setting (CC-Net without DOM).  Ablations show that a key ingredient for \ours's performance is the pixel-based pre-training of {\textsc{Pix2Struct}}. 

Our contributions are as follows:
\begin{enumerate}[leftmargin=*,topsep=1pt]
\item We show, for the first time, that an agent using pixel-only inputs and a generic action space can outperform human crowdworkers on the MiniWob++ benchmark, significantly improving over prior work on this setting, and reaching performance comparable to that of state-of-the-art agents that access DOM information and use a comparable number of human demonstrations.
\item We adapt the WebShop benchmark to our setting, using pixel-based observations and general low-level actions. We establish the first baseline
on this setting, although there is still a performance gap relative to larger language models using HTML-based inputs and task-specific actions.
\item We show that \textsc{Pix2Struct}'s pre-training via screenshot parsing is effective for GUI-based instruction following with pixel-based inputs. In the behavioral cloning setting, pre-training improves task scores from 17.1 to 66.5 on MiniWob++ and from 1.1 to 46.7 on WebShop. 
\item We demonstrate the successful application of tree search as a relatively simple method for policy improvement for MiniWob++.
\end{enumerate}

