
\section{Proposed Agent}
\label{sec:agent}

Our agent, \ours, is based on the \textsc{Pix2Struct}
model \citep{lee2022pix2struct}, which uses an image Transformer encoder and a text Transformer decoder. The architecture is based on Vision Transformer~\citep{vit} and T5~\citep{raffel2019exploring}. \textsc{Pix2Struct} is pre-trained on a \emph{screenshot parsing} task: predicting simplified HTMLs from screenshots with visually-masked regions. Such pre-training was proven effective for tasks related to understanding user interfaces in a non-interactive setting, such as screen summarization and widget captioning~\citep{screen2words, li-etal-2020-widget}.
We use the \textsc{Pix2Struct} base
variant with 282M parameters (12 encoder and 12
decoder layers; hidden size 768) for all our experiments. The model is called once per time step.

\input{tables_and_figures/example_episode}


\paragraph{Input} The only input to the model is pixel-based observation from the environment. We can also condition on multiple previous observations by concatenating multiple frames. In preliminary experiments, we did not observe significant gains from conditioning on past observations for MiniWob++, and thus we only use the screenshot of the current step in our experiments.
We reuse \textsc{Pix2Struct}'s image processing by scaling input images up or down so as to extract the maximal number of fixed-size patches that still fit within the sequence length limit. We use resolutions of 160$\times$210 and 800$\times$600 for MiniWoB++ and WebShop, respectively.

\paragraph{Output} We encode actions as text tokens, which are predicted autoregressively by the Transformer decoder. We use beam search over tokens to output the $k$-best actions (see Appendix \ref{sec:appendix-beam} for details).

\paragraph{Greedy Policy} For interacting with the environment, we adopt a standard greedy policy, selecting the highest scoring action at each step, with one modification. To help prevent the agent from getting stuck in cycles, we track which actions have been taken for a given observation, and select the highest probability action in the beam that has not previously been taken given the current observation, which provides a modest increase in performance.

\subsection{Training}
\label{sec:agent-training}

We explore two methods for training models to follow instructions via GUIs. First, similarly to prior work, we use Behavioral Cloning (BC), where we train our model using standard supervised learning to predict the given action for each observation in a set of human demonstrations. Second, given access to environments with reward signals, prior work has also explored Reinforcement Learning (RL) to further improve agent performance. As an alternative to common reinforcement learning algorithms such as REINFORCE~\citep{Williams2004SimpleSG} and PPO~\citep{Schulman2017ProximalPO}, we apply tree search as a simple method for policy improvement.

\paragraph{Tree Search} For a given set of model parameters, tree search leverages the deterministic nature of the environment to look ahead at the consequences of possible actions to determine a more optimal policy than greedily selecting actions.

We adopt Monte Carlo Tree Search (MCTS)~\citep{Coulom2006EfficientSA}, which outperformed more naive search algorithms in initial experiments, and has been successfully integrated with neural network policies in prior work \citep{silver2017mastering,anthony2017thinking}. Similarly to this prior work, we train a model to estimate a \emph{value function}, which predicts the value (i.e., estimated future rewards) of a given state. We use a surrogate reward which penalizes the number of steps taken to encourage concise trajectories without unnecessary actions. We implement this value function approximator using the same \textsc{Pix2Struct} architecture used for our agent.\footnote{While it may be more efficient to share an encoder between these two \textsc{Pix2Struct}-based models that condition on the same inputs, we trained separate models for simplicity.} However, instead of predicting actions, this model predicts state-values mapped to discrete buckets. To estimate the value of leaf states during MCTS, we use a combination of this value function approximator and rollouts using our greedy policy, similarly to \citet{silver2017mastering}. See Appendix~\ref{sec:appendix-b} for additional technical details.

We can then use successful episodes found with this stronger tree search policy to improve our model. As this stronger model then yields a more effective tree search policy, we can continue to iteratively improve our model using this method. Notably, this approach requires no modifications to the fine-tuning procedure of \ours{}, as, for simplicity, we tune on episodes from the tree search policy using standard supervised learning.

\commentout{
\paragraph{Tree Search Policy}~\kristout{I thought that for something to be a policy, it would just have to select one action from a state without first exploring a whole bunch of actions, but I have not read the literature enough; we should doubel check}\pete{Tree search is used to select an action given a state, but I will re-write this section for clarity.} We can use tree search which leverages lookahead to yield a stronger policy for a given model than simply greedily selecting actions. We leverage a value function approximator, also implemented using an image to text model initialized from \textsc{Pix2Struct}, to improve the efficiency of the search procedure. Specifically, we adopt Monte Carlo Tree Search (MCTS), which outperformed more naive search algorithms in our initial experiments. We use both rollouts (using the greedy policy) and a value function approximator to estimate the value of leaf nodes. [Reference details in appendix]

\paragraph{Surrogate Reward} We attempt to maximize a surrogate reward during MCTS, and train our value function approximator to estimate this surrogate reward. The surrogate reward provides a small negative reward at each step, which encourages trajectories that are concise and do not include unnecessary actions, which can potentially complicate the learning of optimal policies. 

\paragraph{Value Function Approximation} The value function approximator receives the same pixel-based input as the policy network, and uses the same architecture. While it may be more computationally efficient to share a single model (or at least a single encoder) between the policy model and the value function approximator, we implement these as two separate models for simplicity. The value function approximator is trained on the same human demonstrations as our initial policy. It is trained to the state-value function, i.e. to predict future rewards (under the surrogate objective described above). To predict numerical values without modifications to the model's text-based decoder, we map the range of possible rewards to 30 discrete buckets, represented as integers in the model output. The model learns to predict a distribution over integers in this range. To generate state-value estimates, we approximate the mean of this distribution by considering a beam of top-k predictions.

\paragraph{Policy Improvement} We can then tune our model on successful episodes using the tree search policy. We can repeat this procedure iteratively, as improvements to our model further improve the effectiveness of the tree search policy, which can further improve our model.}
