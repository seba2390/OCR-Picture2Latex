\begin{table}[t!]
\centering
\begin{minipage}{.47\textwidth}

% \begin{table}
\centering

\scalebox{0.8}{
\begin{tabular}{lcc}
\toprule
Pre-training & Included & Heldout \\
\midrule
Yes & 65.5 & 28.3  \\
No & 11.0 & 7.6 \\
\bottomrule
\end{tabular}
}
\vspace{0.07 in}
\caption{We selected 9 MiniWob++ tasks and evaluated mean scores when they are \emph{heldout} from the training set. Pretraining leads to non-trivial generalization (28.3) to held out tasks that were unobserved at training time compared to a randomly initialized model (7.6). We also include scores when the tasks are \emph{included} during training for reference.}
\label{tab:heldout}
% \end{table}
  
  
  
\end{minipage}%
\hspace{0.03\textwidth}
\begin{minipage}{.47\textwidth}

% \begin{table}
\centering
\scalebox{0.8}{
\begin{tabular}{lccc}
\toprule
& \multicolumn{3}{c}{Iteration} \\
 \cmidrule(lr){2-4}
Policy & 0 & 1 & 2 \\
\midrule
Greedy & 66.5 & 93.1 & 96.2 \\
Tree Search & 91.7 & 98.4 & --- \\
\bottomrule
\end{tabular}
}
\vspace{0.07 in}
\caption{We compare average MiniWob++ scores using the greedy policy with one that uses tree search and lookahead, given the same underlying model. The model is initially trained on human demonstrations and iteratively improved by training on episodes generated by the tree search policy.}
\label{tab:tree-search}
% \end{table}

\end{minipage}
\end{table}