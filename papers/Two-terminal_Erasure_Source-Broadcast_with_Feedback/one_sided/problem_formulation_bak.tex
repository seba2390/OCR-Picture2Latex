\subsection{Problem Formulation}
\label{sec:system_model_one_sided}

%\subsection{{}System Model}
%\section{Problem Formulation}

%We first define the problem in Section~\ref{subsec:problem_definition}.  In Section~\ref{subsec:metrics}, we then discuss metrics that are of interest in the evaluation of broadcast codes and show how a solution to the problem in Section~\ref{subsec:problem_definition} can be used to address these metrics.

%\input{2_problem_formulation/problem_definition}
%\input{2_problem_formulation/metrics}

%%%\begin{figure*}
%%\begin{figure}
%%	\centering
%%%	\input{3/out/fig/schematic}
%%%	\includegraphics[scale=0.6]{3/out/fig/schematica}
%%%	\includegraphics[scale=0.75]{out/fig/schematic}
%%%	\includegraphics[scale=0.8]{2_problem_formulation/fig/schematicE}
%%	\includegraphics{2_problem_formulation/fig/schematicE}
%%%	\includegraphics[width=0.6\textwidth]{outer_bound.png}
%%	\caption{Broadcasting an equiprobable binary source over an erasure broadcast channel.}
%%	\label{fig:schematic}
%%%\end{figure*}
%%\end{figure}

%We begin by presenting our joint source-channel coding formulation which is illustrated in Fig.~\ref{fig:schematic}.

%\begin{figure*}
%%%%%\begin{figure}
%%%%%	\centering
%%%%%%	\input{fig/schematicE}
%%%%%%	\includegraphics[scale=0.95]{fig/schematic}
%%%%%	\includegraphics[scale=0.75]{fig/schematic}
%%%%%%	\includegraphics[scale=0.75]{SysModel/fig/schematic_arrow}
%%%%%	\caption{Broadcasting an equiprobable binary source over an erasure broadcast channel.}
%%%%%	\label{fig:schematic}
%%%%%%\end{figure*}
%%%%%\end{figure}

%The problem is illustrated in Fig.~\ref{fig:schematic}.  
The problem is illustrated in Figure~\ref{fig:system_model_one_sided}.  The problem involving \emph{one-sided} feedback is a variation of the problem defined in Section~\ref{sec:system_model_feedback} with the exception that of the two receivers in the broadcast network, only the receiver with the lower erasure rate has a feedback channel available.  That is, at time $T$, we assume that only $\{Z_1(t)\}_{t=1, 2, \ldots, T-1}$ is available to the transmitter and both receivers rather than having $\{Z_1(t), Z_2(t)\}_{t=1, 2, \ldots, T-1}$ available.  We therefore redefine 

We consider the problem of communicating a binary memoryless source $\{S(t)\}_{t=1,2, \ldots}$ to two users over an erasure broadcast channel with one-sided feedback.  Of the two receivers in the broadcast network, only the receiver with the lower erasure rate has a feedback channel available. %Let $\mathcal{S}^{N}$ be the set of all $N$-vectors with components in $\mathcal{S}$, and denote $(S(1), S(2), \ldots , S(N))$ as $S^{N}$.  For convenience, we will also denote the set $\{1,2,\dots,N\}$ as $[N]$.
%
\begin{figure}
	\centering
%	\input{3/fig/schematic}
%	\includegraphics[scale=0.8]{3/one_sided/fig/system_model_one_sided}
	\includegraphics[scale=1]{one_sided/fig/system_model_one_sided}
%%	\includegraphics[width=0.6\textwidth]{outer_bound.png}
	\caption{Broadcasting an equiprobable binary source over an erasure broadcast channel with a feedback channel asymmetrically available to only the stronger user.}
	\label{fig:system_model_one_sided}
\end{figure}
%
The source produces equiprobable symbols in $\mathcal{S}=\{0,1\}$ and is communicated by an encoding function that produces the channel input sequence $X^{W} = (X(1),  \dots , X(W))$, where $X(t)$ denotes the $t^{\mathrm{th}}$ channel input taken from the alphabet $\mathcal{X} = \{0, 1\}$.  We assume that 
$X(t)$ is a function of the source as well as the channel outputs of the stronger user prior to time $t$.  
%The encoding function produces the channel input sequence $X^{W} = (X(1),  \dots , X(W))$, where $X(t)$ denotes the $t^{\mathrm{th}}$ channel input taken from the alphabet $\mathcal{X} = \{0, 1\}$.  %Note here that our notation defines $X^{n}$ as the first $n$ channel symbols sent. %which is also taken from the alphabet $\mathcal{S}$.  %

% Original
%Let $Y_{i}(t)$ be the channel output observed by user $i$ on the $t^{\mathrm{th}}$ channel use for $i \in [n]$ and $t \in [W]$.  Our channel model is an erasure channel.  In particular, let $\epsilon_i$ denote the erasure rate of the channel corresponding to user~$i$.  Without loss of generality, we will assume that $0 < \epsilon_{1} < \epsilon_{2} < \ldots < \epsilon_{n} < 1$.  Then $Y_{i}(t)$ exactly reproduces the channel input $X(t)$ with probability $(1 - \epsilon_i)$ and otherwise indicates an erasure event, which happens with probability $\epsilon_i$.  We let $Y_{i}(t)$ take on values in the alphabet $\mathcal{Y} = \{0, 1, \star\}$ so that an erasure event is represented by `$\star$,' the erasure symbol.  We associate user~$i$ with the state sequence $(N_i(t))_{t \in [W]}$, which is defined such that $N_{i}(t) = 1$ if $Y_i(t)$ was erased and $N_i(t) = 0$ otherwise.  The channel we consider is memoryless in the sense that $N_{i}(t)$ is drawn i.i.d.\ from a $\operatorname{Bern}(1 - \epsilon_{i})$ distribution.

Let $Y_{i}(t)$ be the channel output observed by user $i$ on the $t^{\mathrm{th}}$ channel use for $i \in \{1, 2\}$. %$\mathcal{U}= \{1, 2, 3\}$ and $t \in [W] = \{1, \ldots, W\}$.  %Our channel model is a binary erasure broadcast channel. % as shown in Fig.~\ref{fig:schematic}.  
%In particular, let $\epsilon_i$ denote the erasure rate of the channel corresponding to user~$i$, where we assume that $0 < \epsilon_{1} < \epsilon_{2} < \ldots < \epsilon_{n} < 1$.  This is without loss of generality since we can address {{}all} users that experience identical erasure rates by serving the {{}one} with the most stringent distortion requirement.  
%In particular, if $\epsilon_i$ denotes the erasure rate of the channel corresponding to user~$i$, our model specifies that $Y_{i}(t)$ exactly reproduces the channel input $X(t)$ with probability $(1 - \epsilon_i)$ and otherwise indicates an erasure event, which happens with probability $\epsilon_i$.  
We let $Y_{i}(t)$ take on values in the alphabet $\mathcal{Y} = \{0, 1, \star\}$ so that an erasure event is represented by `$\star$'.   
%
We associate user~$i$ with the state sequence $\{Z_i(t)\}_{t \in [W]}$, which represents the noise on user~$i$'s channel, where $Z_i(t) \in \mathcal{Z} = \{0,1\}$,  and  $Y_i(t)$ will be erased if $Z_{i}(t) = 1$ and $Y_{i}(t) = X(t)$ if $Z_i(t) = 0$.  The channel we consider is memoryless in the sense that $(Z_{1}(t), Z_2(t))$ is drawn i.i.d.\ from the probability mass function given by   
%For $i, j \in \{1, 2\}$, let $p_{E}(i, j) = \textrm{Pr}(Z_1(t) = i - 1, Z_2(t) = j - 1)$, and let $P_{E} \in \mathbb{R}^{2 \times 2}$ be the stochastic matrix such that $P_{E} = [p_{E}(i, j)]$

\begin{subequations}
\label{eq:pmf_z1z2}
\begin{align}
	\textrm{Pr}(Z_1 = 1, Z_2 = 1) &= \epsot \\
	\textrm{Pr}(Z_1 = 1, Z_2 = 0) &= \epsilon_1 - \epsot \\
	\textrm{Pr}(Z_1 = 0, Z_2 = 1) &= \epsilon_2 - \epsot \\
	\textrm{Pr}(Z_1 = 0, Z_2 = 0) &= 1 - \epsilon_1 - \epsilon_2 + \epsot,			
\end{align}
\end{subequations}
where $\epsot\in (0, 1)$ is the probability that an erasure simultaneously occurs on both channels and $\epsilon_i \in (0, 1)$ denotes the erasure rate of the channel corresponding to user~$i$, where we assume $\epsilon_1 < \epsilon_2$.
%$\epsilon_i$ denotes the erasure rate of the channel corresponding to user~$i$, and we assume $\epsilon_1 < \epsilon_2$.
%\textcolor{blue}{We assume that the erasure rates are known to both the transmitter and receiver. If such channel knowledge is not possible, we can take the standard approach of interpreting a broadcast channel as an abstraction for a \emph{compound channel}~\cite{CT}.  In such a situation, the compound channel is a channel that randomly takes on one of many potential states for the duration of transmission.  We model this by considering a broadcast channel whose constituent channels correspond to these potential channel states.}
%{}{Note that in our setup, the erasure rates for each user are assumed to be known. However, our setup also models the 
%compound channel~\cite{CT}, where an erasure rate is not known and instead belong to a collection of possible states, with each 
%state corresponding to one virtual user in our system.}

%Let $Y_{i}(t)$ be the channel output observed by user $i$ on the $t^{\mathrm{th}}$ channel use for $i \in [n]$ and $t \in [W]$.  Our channel model is an erasure channel.  In particular, $Y_{i}(t)$ either exactly reproduces the channel input $X(t)$ or otherwise indicates an erasure event.  We let $Y_{i}(t)$ take on values in the alphabet $\mathcal{Y} = \{0, 1, \star\}$ so that an erasure event is represented by `$\star$,' the erasure symbol.  Let $\epsilon_i$ denote the erasure rate of the channel corresponding to user~$i$.  Without loss of generality, we assume that $0 < \epsilon_{1} < \epsilon_{2} < \ldots < \epsilon_{n} < 1$.  We associate user~$i$ with the state sequence $(N_i(t))_{t \in [W]}$, where $N_i(t) \in \{0,1\}$.  The state sequence represents the noise on user~$i$'s channel and we will have that $Y_i(t)$ is erased if $N_{i}(t) = 1$ and $Y_{i}(t) = X(t)$ if $N_i(t) = 0$.  The channel we consider is memoryless in the sense that $N_{i}(t)$ is drawn i.i.d.\ from a $\operatorname{Bern}(1 - \epsilon_{i})$ distribution.

%\begin{equation}
%%\label{eq:channel_model}
%	Y_{i}(t) = X(t) \cdot N_{i}(t),
%\end{equation}
%
%where $N_{i}(l)$ is a $\operatorname{Bern}(1 - \epsilon_{i})$ random variable representing the noise at user $i$'s $l^{\mathrm{th}}$ channel output.  The channel is memoryless in the sense that $N_{i}(l)$ is drawn i.i.d.\ from a $\operatorname{Bern}(1 - \epsilon_{i})$ distribution.  %Specifically, the statistics of $N_{i}(l)$ are such that
%
%\begin{equation}
%\label{eq:nstat}
%	N_{i}(l) =
%	\begin{cases}
%		0 & \text{with probability} \ \epsilon_{i}, \\
%		1 & \text{with probability} \ 1 - \epsilon_{i}.
%	\end{cases}
%\end{equation}
%
%Thus, $Y_{i}(l)$ takes on values in the alphabet $\mathcal{Y} = \{-1, 0, +1\}$ and we interpret $Y_{i}(l)$ as the output of an erasure channel, with erasure probability $\epsilon_{i}$, when $X(l)$ is the channel input (see Section~\ref{sec:erased_rv} for a discussion of the erasure channel model).
%
%Thus, $Y_{i}(l)$ takes on values in the alphabet $\mathcal{Y} = \{-1, 0, +1\}$.  If we define `0' as the erasure symbol, we can interpret $Y_{i}(l)$ as the output of an erasure channel with erasure probability $\epsilon_{i}$ when $X(l)$ is the channel input. Without loss of generality, we will assume that $0 < \epsilon_{1} < \epsilon_{2} < \ldots < \epsilon_{n} < 1$. %Users on channels with the same erasure probability are represented by a user of the smallest distortion

%Now although the length-$n$ channel input is broadcasted to both users, we will assume that due to each user's delay constraint, the $i^{\mathrm{th}}$ user reconstructs the source after observing only the first $n_{i}$ channel input symbols, denoted as $X^{n_{i}}$, where $n_{i} \leq n$.  Specifically, we have that $n = \max(n_{1}, n_{2})$.

The problem we consider involves causal feedback from the stronger user that is universally available.  That is, at time $T$, we assume that $\{Z_1(t)\}_{t=1, 2, \ldots, T-1}$ is available to the transmitter and both receivers. 
After $W$ channel uses, user $i$ utilizes the feedback and his own channel output to reconstruct the source as a length-$N$ sequence, denoted as $\hat{S}_{i}^{N}$.  We will be interested in a fractional recovery requirement so that each symbol in $\hat{S}_{i}^{N}$ either faithfully recovers the corresponding symbol in $S^{N}$, or otherwise a failure is indicated with an erasure symbol, i.e., we do not allow for any bit flips.

More precisely, we choose the reconstruction alphabet $\mathcal{\hat{S}}$ to be an augmented version of the source alphabet so that $\mathcal{\hat{S}} = \{0, 1, \star\}$, where the additional `$\star$' symbol indicates an erasure symbol.  Let $\mathcal{D} = [0,1]$ and $d_i \in \mathcal{D}$ be the distortion user $i$ requires.  We then express the constraint that an achievable code ensures that each user $i \in \{1, 2\}$ achieves a fractional recovery of $1 - d_{i}$ with the following definition.

%by first defining the set $D_{i}(\hat{S}_{i}^{N}) = \{l \in [N] : \hat{S}_{i}(l) = 0\}$.

%Note that the we have taken the reconstruction alphabet to be an augmented version of the source alphabet by adding the additional `0' symbol, which we interpret as an erasure symbol.  In our reconstruction, we require that $\hat{S}_{i}^{N}$ is such that for $l \in [N]$, $\hat{S}_{i}(l) = S(l)$ or $\hat{S}_{i}(l) = 0$.  In other words, we do not allow any bit flips in our reconstruction so that $\hat{S}_{i}(l)$ either faithfully reproduces $S(l)$ or otherwise outputs an erasure symbol.


%%%On a symbol-by-symbol basis, we measure the reconstruction's fidelity with the erasure distortion $d_{E}: \mathcal{S} \times \hat{\mathcal{S}} \to \{0,1, \infty\}$ given by
%%%%Depending on the distortion measure we use, $\mathcal{\hat{S}}$ can either be a binary source alphabet or the extended alphabet $\mathcal{\hat{S}_{E}} = \{-1, 0, 1\}$, where `0' denotes the source erasure symbol.
%%%
%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%
%%%%On the other hand, when the output alphabet can include the source erasure symbol as in the second case, then on a symbol-by-symbol comparison, the reconstruction's fidelity is measured with the erasure distortion $d_{E} : \mathcal{S} \times \mathcal{\hat{S}_{E}} \to \{0, 1, \infty\}$, given by
%%%
%%%\begin{equation}
%%%\label{eq:symbol_distortion1}
%%%	d_{E}(s, \hat{s}) =
%%%	\begin{cases}
%%%		0 & \text{if } \hat{s} = s, \\
%%%		1 & \text{if } \hat{s} = 0 \\
%%%		\infty & \text{otherwise}.
%%%	\end{cases}
%%%\end{equation}
%%%%
%%%The per-letter distortion of a vector is then defined as $d(s^{k}, \hat{s}^{k}) = \frac{1}{k} \Sigma_{m=1}^{k} d(s_{m}, \hat{s}_{m})$.
%%%
%%%%\begin{equation}
%%%%\label{eq:vector_distortion1}
%%%%	d(s^{k}, \hat{s}^{k}) = \frac{1}{k} \sum\limits_{i=1}^k d(s_{i}, \hat{s}_{i}).
%%%%\end{equation}
%%%%
%%%%where $d(\cdot, \cdot)$ can be either $d_{H}(\cdot, \cdot)$ or $d_{E}(\cdot, \cdot)$.
%%%
%%%%We point out that with the erasure distortion measure,  a finite distortion value indicates that we are certain about the accuracy of the non-erased symbols in our reconstruction, while the actual value of the distortion is a measure of the proportion of bits that we are unsure about, i.e., that have been erased in the reconstruction.
%%%
%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%
%%%
%%%We point out that with the erasure distortion measure,  a finite distortion value indicates that we are certain about the accuracy of the non-erased symbols in our reconstruction, while the actual value of the distortion is a measure of the proportion of bits that we are unsure about, i.e., that have been erased in the reconstruction.  We now define the components of our problem.

\begin{mydef}
\label{def:code_one_sided}
	An $(N, W, d_{1}, d_{2})$ code for source $S$ on the erasure broadcast channel with \emph{one-sided} feedback consists of	
	\begin{enumerate}
		\item A sequence of encoding functions $f_{i, N} : \mathcal{S}^{N} \times  \mathcal{Z}^{i-1} \to \mathcal{X}$ for $i \in [W]$, s.t.\ $X(i) = f_{i, N}(S^{N}, Z_1^{i -1})$
		
		\item Two decoding functions $g_{i,N} : \mathcal{Y}^{W} \times \mathcal{Z}^{W} \to \mathcal{\hat{S}}^{N}$ s.t.\ $\hat{S}_{i}^{N} = g_{i,N}(Y_{i}^{W}, Z_1^{W})$, $i \in \{1, 2\}$, and
		\begin{enumerate}
			\item $\hat{S}_{i}^{N}$ is such that for $t \in [N]$, if $\hat{S}_{i}(t) \neq S(t)$, then $\hat{S}_{i}(t) = \star$,
%			\item $\mathbb{E}   \left\vert{D_{i}(\hat{S}_{i}^{N})}\right\vert \leq d_{i}$
			\item $\mathbb{E}   \left\vert{\{t \in [N] \mid \hat{S}_{i}(t) = \star\}}\right\vert \leq N d_{i}$,
		\end{enumerate}
		 	
%	$\mathbb{E}d(S^{k}, g_{i}( X^{n} \cdot N_{i}^{n})) \leq D_{i}$ holds for $i \in \{1,2\}$		
	\end{enumerate}
	
%	where $\mathbb{E}(\cdot)$ is the expectation operation and $\vert A \vert$ denotes the cardinality of set $A$.
\end{mydef}

%\begin{mydef}
%\label{def:code}
%	An $(N, W, d_{1}, d_{2}, d_{3})$ code for source $S$ on the erasure broadcast channel consists of
%	\begin{enumerate}
%		\item a sequence of encoding functions $f_{i, N} : \mathcal{S}^{N} \times  \prod_{j = 1}^{3} \mathcal{N}^{i-1} \to \mathcal{X}$ for $i \in [W]$, such that $X(i) = f_{i, N}(S^{N}, N_1^{i -1}, N_2^{i -1}, N_3^{i - 1})$, and
%		
%		\item three decoding functions $g_{i,N} : \mathcal{Y}^{W} \times  \prod_{j = 1}^{3} \mathcal{N}^{W} \to \mathcal{\hat{S}}^{N}$ s.t.\ $\hat{S}_{i}^{N} = g_{i,N}(Y_{i}^{W}, N_1^{W}, N_2^{W}, N_3^{W})$, $i \in \{1, 2, 3\}$,
%		\begin{enumerate}
%			\item $\hat{S}_{i}^{N}$ is such that for $t \in [N]$, if $\hat{S}_{i}(t) \neq S(t)$, then $\hat{S}_{i}(t) = \star$,
%%			\item $\mathbb{E}   \left\vert{D_{i}(\hat{S}_{i}^{N})}\right\vert \leq d_{i}$
%			\item $\mathbb{E}   \left\vert{\{t \in [N] \mid \hat{S}_{i}(t) = \star\}}\right\vert \leq N d_{i}$,
%		\end{enumerate}
%		 	
%%	$\mathbb{E}d(S^{k}, g_{i}( X^{n} \cdot N_{i}^{n})) \leq D_{i}$ holds for $i \in \{1,2\}$		
%	\end{enumerate}
%	
%%	where $\mathbb{E}(\cdot)$ is the expectation operation and $\vert A \vert$ denotes the cardinality of set $A$.
%\end{mydef}

%A point is now made about the modelling of latencies in our problem.  We define the \emph{latency} or \emph{bandwidth expansion factor} $b \in [0, \infty )$, as the number of channel uses per source symbol that are delivered over the broadcast channel, i.e., $b \triangleq n/k$.  This is to say that $b \cdot k$ channel uses are required before both users can reconstruct $S^{k}$ subject to their distortion constraints.  Our problem is now defined as characterizing the achievable latency region under a given pair of distortion constraints as per the next definition.

%A point is now made about the modelling of delays in our problem.  We define the bandwidth expansion factor $b \in [0, \infty )$, as $b = n/k$.  We interpret $b$ as the normalized latency (in units of ``channel uses per source symbol''), before both users can reconstruct $S^{k}$ subject to their distortion constraints.  Our problem is now defined as characterizing the achievable distortion region under a given bandwidth expansion factor as per the next definition.
%
We next define the {\it latency} that a given code requires before all users can recover their desired fraction of the source.
\begin{mydef}
	The latency, $w$, of an~$(N, W, d_{1}, d_{2})$ code is the number of channel uses per source symbol that the code requires to meet all distortion demands, i.e., $w = W/N$.
\end{mydef}
%
Our goal is to characterize the achievable latencies under a prescribed distortion vector,
as per the following definition.
\begin{mydef}
\label{def:achievable_one_sided}
	Latency $w$ is said to be $(d_{1}, d_{2})$-achievable over the erasure broadcast channel with one-sided feedback if for every $\delta > 0$, there exists for sufficiently large $N$, an $(N, wN, \hat{d}_{1}, \hat{d}_{2})$ code such that for all $i \in \{1, 2\}$, $d_{i}+\delta \geq \hat{d}_{i}$.
	
%	\begin{equation}
%		D_{i}+\delta \geq d_{i},  \quad i \in \{1, 2\}.
%	\end{equation}	
%	Alternatively, we may occasionally say that the tuple $(b, D_{1}, D_{2})$ is achievable if the latency $b$ is $(D_{1}, D_{2})$-achievable.
	
%The achievable latency region is the set of all achievable latencies under the prescribed distortion vector.

\end{mydef}

%In the remaining sections, we consider the cases $n=2$ and $n=3$.  We will also make a mention whenever the strategies we use for these cases can be generalized for $n$ users.  
%While this may seem to be an oversimplification, it is worth noting that to the authors' best knowledge, a solution is also not known for the related channel coding problem involving $n$ users~\cite{GGT}.

%\begin{remark}
%\label{rem:deps}
%Throughout this paper we will assume that for each user $i \in [n]$, we have that $d_i < \epsilon_i$. Any user with $d_i \ge \epsilon_i$ will be trivially satisfied by the systematic portion of our segmentation-based coding scheme.  Furthermore, we will show in Lemma~\ref{lem:bn1} that within our class of coding schemes, such a systematic portion can be transmitted without loss of optimality when at least one user satisfies $d_i < \epsilon_i$. Finally, if every user satisfies $d_i \ge \epsilon_i$, a simple uncoded transmission scheme is easily shown to be optimal.
%\end{remark}
%
%\begin{remark}
%	While our system model has assumed binary alphabets for both the source and channel input sequences, our results can be easily extended to larger alphabet sizes for the purpose of applying our results to packet erasure networks.
%\end{remark}


%For convenience, throughout this paper, for any positive integer $k$, we shorthand the set $\{1,2,\dots,k\}$ as $[k].$
%
%Consider broadcasting a length-$N$ binary sequence $S = \{s(j)\}_{j\in[N]}$ from a source to $n$ receivers over independent memoryless binary erasure channels. For $i\in[n]$, denote the erasure rate of the channel between the source and receiver $i$ as $\epsilon_i$. Applying some encoding scheme to $S$, the source generates and broadcasts the coded binary sequence $X=\{x_j\}_{j\ge1}$. Each receiver $i$ retrieves from the channel a sequence $Y^i = \{y_{j}^i\}_{j\ge1}$ where $y_{j}^i \in \{0,1,e\}$, $e$ indicating an erasure. Denote $Y^{i,W} = \{y_{j}^i\}_{j\in[W]}$ as the truncation of $Y^i$ up to the $W$th symbol. Receiver $i$ reconstructs from $Y^{i,W}$ a copy $\hat S^{i,W}=\{\hat s_{j}^{i,W}\}_{j\in[N]},$ where $\hat s_{j}^{i,W}\in\{0,1,e\}.$
%
%%\subsection{Erasure Distortion}
%We consider a symbol-by-symbol based erasure reconstruction distortion measure $D_e: \{0,1\}\times \{0,1,e\}\mapsto \mathds R$ defined as
%\begin{align*}
%D_E(s,\hat s) = \left\{\begin{array}{lr} 0,& \textnormal{if } \hat s=s\\ 1, & \textnormal{if } \hat s\ne s, \hat s\ne e\\ \infty,& \textnormal{if } \hat s=e \end{array}\right.
%\end{align*}
%%for symbols $s\in\{0,1\}$ and $\hat s\in\{0,1,e\}$,
%and
%\begin{align*}
%D(S,\hat S^{i,W}) =  \frac{1}{N}\sum_{j\in[N]} D_E(s_j,\hat s_{j}^{i,W}).
%\end{align*}
%for the source sequence $S$ and the sequence $\hat S^{i,W}$ reconstructed by user $i$ from the first $W$ symbols of the channel output sequence $Y^{i,W}$.
%
%Now our problem is to upper bound and lower bound the minimum $W$, or $w=W/N,$ which we refer to as the {\it latency}, that guarantees $\mathds E[ D(S,\hat S^{i,W})]\le d_i$ for all $i\in[n]$,
%where $d_i$ is the given distortion requirement of user $i$.
%
%Clearly, $w$ is trivially lower bounded by $\max_{i\in[n]}\{\frac{1-d_i}{1-\epsilon_i}\}$. In the next section, we provide an upper bound of $w$ and the achieving coding scheme. We show that the upper bound is achieved with a coding scheme that sends a part of the source sequence uncoded, partitions the rest of the source sequence into a number of segments, and encodes each segment by a systematic optimal erasure code. The optimal segmentation can be found by solving a linear programming problem. We also show in Sec.~\ref{sec:jscc} that the segmentation-based scheme is equivalent to the Mittal-Phamdo~\cite{} scheme for erasure channels.

%\input{SysModel/individual_code}




%\input{SysModel/JSCC_approach3}








