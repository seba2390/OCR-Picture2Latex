\subsection{Minmax Latency Optimality}
\label{sec:one_sided_optimality}

In this section, we show that it is possible to choose values for $\omegaparam, \gammaparam \in [0, 1]$ from Section~\ref{subsubsec:inner_bound} so that the lower bound for the minmax latency in~\eqref{eq:wplusd} is achieved.  We first calculate the expected number of \emph{unknown} variables involved in transmissions to the weaker user from Phase~\Rmnum{2} onwards.  

First, since we send random linear combinations of the symbols in $\Fset$ in Phase~\Rmnum{2}, we initially expect this to contribute $|\Fset|$ variables.  However, some of the symbols in $\Fset$ have already been received by user~2 in Phase~\Rmnum{1}.  Let $\nFUnknown$ be the number of symbols in $\Fset$ \emph{not} received by user~2 in Phase~\Rmnum{1}.  Given a channel noise realization $(Z_{1}^{W}, Z_{2}^{W}) = (z_{1}^{W}, z_{2}^{W})$, we can calculate the expected value of $\nFUnknown$ as

\setcounter{cnt}{1}
\begin{align}
	\mathbb{E}(\nFUnknown | (Z_{1}^{W}, Z_{2}^{W}) = (z_{1}^{W}, z_{2}^{W})) &= \sum_{s \in \Fset} \textrm{Pr}(\textrm{$s$ not received by user~2 in Phase~\Rmnum{1}})\\
	&\stackrel{(\alph{cnt})}{=} \sum_{s \in \Cset} \textrm{Pr}(\textrm{$s$ not received by user~2 in Phase~\Rmnum{1}}) \\ \nonumber
	&\qquad  + \sum_{s' \in \BsetOmega} \textrm{Pr}(\textrm{$s'$ not received by user~2 in Phase~\Rmnum{1}})\\ 
	\addtocounter{cnt}{1}
	&\stackrel{(\alph{cnt})}{=} \sum_{s \in \Cset} \textrm{Pr}(Z_2 = 1 | Z_1 = 0) + \sum_{s' \in \BsetOmega} \textrm{Pr}(Z_2 = 1 | Z_1 = 1) \\ 
	\addtocounter{cnt}{1}
	&\stackrel{(\alph{cnt})}{=} \sum_{s \in \Cset} \left(\frac{\epsilon_2 - \epsot}{1 - \epsilon_1}\right) + \sum_{s' \in \BsetOmega} \left( \frac{\epsot}{\epsilon_1}\right) \\ 
	\addtocounter{cnt}{1}
%	&\stackrel{(\alph{cnt})}{=}  N\left( \omegaparam(\epsilon_1 - d_1) + \gamma(1 - \epsilon_1) \right)\epsilon_2
	\label{eq:last_line_set}
	&= |\Cset| \left(\frac{\epsilon_2 - \epsot}{1 - \epsilon_1} \right) + |\BsetOmega|\left( \frac{\epsot}{\epsilon_1}\right),
\end{align}
where 
\begin{enumerate}[(a)]
	\item follows from the fact that $\Fset = \Cset \cup \BsetOmega$ and $\Cset$ and $\BsetOmega$ are disjoint by construction
	\item follows from the fact that by construction, all symbols in $\Cset$ have been received by user~1 and all symbols in $\BsetOmega$ were not received by user~1
	\item we have calculated the conditional probabilities from~\eqref{eq:pmf_z1z2}.
\end{enumerate}

The cardinality of sets $\Cset$ and $\BsetOmega$ depends on the channel noise variables $(Z_{1}^{W}, Z_{2}^{W})$.  By taking the expectation over the channel noise, we can calculate the unconditional expected value of $\nFUnknown$ as

\setcounter{cnt}{1}
\begin{align}
	\label{eq:nFUnknown}
	\mathbb{E}\nFUnknown &\stackrel{(\alph{cnt})}{=} N \gammaparam (1 - \epsilon_1) \left(\frac{\epsilon_2 - \epsot}{1 - \epsilon_1} \right) + N \omegaparam(\epsilon_1 - d_1) \left( \frac{\epsot}{\epsilon_1}\right),
\end{align}
where 
\begin{enumerate}[(a)]
	\item follows from~\eqref{eq:last_line_set} and by construction of the sets (see Section~\ref{subsubsec:inner_bound} and Figure~\ref{fig:set_construction}).
\end{enumerate}

%the probability that any symbol in $\Fset$ was not already received by the weaker user in Phase~\Rmnum{1} is equal to $\epsilon_2$.  Therefore the expected number of \emph{unknown} variables within $\Fset$ is given by $\nFUnknown$, where
%
%\setcounter{cnt}{1}
%\begin{align}
%	\mathbb{E}\nFUnknown &= \mathbb{E}|\Fset|\epsilon_2 \\
%	&= \mathbb{E}|\BsetOmega \cup \Cset|\epsilon_2 \\
%	&\stackrel{(\alph{cnt})}{=} \left( \mathbb{E}|\BsetOmega| + \mathbb{E} |\Cset| \right) \epsilon_2	 \\
%	\addtocounter{cnt}{1}
%	\label{eq:nFUnknown}	
%	&\stackrel{(\alph{cnt})}{=}  N\left( \omegaparam(\epsilon_1 - d_1) + \gamma(1 - \epsilon_1) \right)\epsilon_2
%\end{align}
%where 
%\begin{enumerate}[(a)]
%	\item follows from the fact that $\BsetOmegaC$ and $\Cset$ are disjoint by construction
%	\item follows by construction (see Section~\ref{subsubsec:inner_bound} and Figure~\ref{fig:set_construction}).
%\end{enumerate}

The use of repetition coding for the symbols in $\BsetOmegaC$ in Phase~\Rmnum{2} further adds additional unknown variables to the coding scheme.  On average, the expected number of symbols repeated is $\mathbb{E}|\BsetOmegaC| = N(1  - \omegaparam)(\epsilon_1 - d_1)$, of which, again, only a fraction of $\textrm{Pr}(Z_2 = 1 | Z_1 = 1) = \epsot/\epsilon_1$ were not already received by the weaker user in Phase~\Rmnum{1}.  By Lemma~\ref{lem:repetition}, the number of additional \emph{unknown} variables introduced to the weaker user as a result of the repetition scheme is therefore given by $\nBUnknown$, where

\setcounter{cnt}{1}
\begin{align}
	\label{eq:nBUnknown}
	\mathbb{E}\nBUnknown &= N(1  - \omegaparam)(\epsilon_1 - d_1)\left(\frac{\epsot}{\epsilon_1}\right) \left(\frac{1 - \epsilon_2}{1 - \epsot} \right).
\end{align}

Let $\LHSfuncparam$ be the expected fraction of all source symbols that are involved in transmissions to the weaker user from Phase~\Rmnum{2} onwards that have not yet been decoded prior to Phase~\Rmnum{2}.  We have that $\LHSfuncparam$ is the normalized sum of~\eqref{eq:nFUnknown} and~\eqref{eq:nBUnknown}, i.e., 

\begin{align}
	\LHSfuncparam &= \frac{\mathbb{E}\nFUnknown + \mathbb{E}\nBUnknown}{N} \\
	&= \kgamma \gammaparam + \komega \omegaparam	+ \kk,
	\label{eq:LHSfuncparam}
\end{align}
where 
\begin{subequations}
\begin{align}
	\kgamma &= \epsilon_2 - \epsot, \label{eq:kgamma} \\	
	\komega &= (\epsilon_1 - d_1) \left(\frac{\epsot}{\epsilon_1}\right) \left(\frac{\epsilon_2 - \epsot}{1 - \epsot} \right), 	\label{eq:komega}\\
	\kk &= (\epsilon_1 - d_1) \left(\frac{\epsot}{\epsilon_1}\right) \left(\frac{1 - \epsilon_2}{1 - \epsot} \right).
	\label{eq:kk}
\end{align}
\end{subequations}

Having calculated the number of unknown variables sent to the weaker user from Phase~\Rmnum{2} onwards, we now consider the number of equations he receives in Phases~\Rmnum{2} and~\Rmnum{3}.  From Section~\ref{subsubsec:inner_bound}, we know that during these phases, the total number of transmissions was simply equal to the number of trials needed to send $N(\epsilon_1 - d_1)$ equations to the stronger user with feedback.  The number of transmissions in Phases~\Rmnum{2} through~\Rmnum{3}  is therefore distributed according to a negative binomial distribution and the mean number of transmissions in this period is $W_{2,3} = N(\epsilon_1 - d_1)/(1 -\epsilon_1)$.  Of these transmissions, the expected number received by the weaker user is equal to $W_{2, 3}(1 - \epsilon_2)$.  We rewrite the expression for $W_{2, 3}(1 - \epsilon_2)$, the expected number of transmissions received by user~2 in Phases~\Rmnum{2} through~\Rmnum{3}, as $NC_{2, 3}$ where

%We therefore define the capacity of the weaker user's channel during Phases~\Rmnum{2} through~\Rmnum{3} as $C_{2,3}$, where
\begin{align}
	C_{2,3} = \frac{(\epsilon_1 - d_1)(1 - \epsilon_2)}{1 - \epsilon_1}.
%	\bstar &= \frac{(1 - \epsilon_1) + d_1(1 - \epsilon_2)}{1 - \epsilon_1\epsilon_2}
\end{align}
%and the expected number of equations received by the weaker user in Phases~\Rmnum{2} through~\Rmnum{3} is given by $NC_{2, 3}$.  
We next compare $N\LHSfuncparam$, the amount of source symbols destined for the weaker user, with $NC_{2, 3}$,  the expected number of equations received over the weaker user's channel during Phases~\Rmnum{2} and~\Rmnum{3}.  

As mentioned in Section~\ref{subsubsec:inner_bound}, the weaker user requires an additional $N(\epsilon_2 - d_2)$ symbols to be sent from Phase~\Rmnum{2} onwards.  Therefore, it is necessary that $\LHSfuncparam \geq \epsilon_2 - d_2$.  However, if $\LHSfuncparam$ is much greater than $\epsilon_2 - d_2$, we encounter the problem explained in the introduction of this section in which the weaker user is forced to decode unnecessary symbols thus introducing delay.  Say that we are able to find values of $\gammaparam', \omegaparam' \in [0, 1]$ such that $\LHSfuncprime = \epsilon_2 - d_2$.  We consider two cases when this is so -- when $\LHSfuncprime \leq C_{2, 3}$ and when $\LHSfuncprime > C_{2, 3}$.  We show that in both cases, we can achieve the optimal minmax  latency so long as $\LHSfuncprime = \epsilon_2 - d_2$.

In the first case, when $\LHSfuncprime \leq C_{2, 3}$, we wish to send less information over the channel than what the channel can support.  Therefore, we expect that the weaker user should be able to decode all source symbols before the conclusion of Phase~\Rmnum{3}.  However, in general, if the weaker user achieves distortion $d_2$ after decoding, it will be at a latency $w$, where $w > \wtwo$.  That is, in general, the weaker user may not achieve an \emph{individual} point-to-point optimal latency.

To see why this is so, recall from Section~\ref{subsubsec:inner_bound} that in Phase~\Rmnum{2} of our coding scheme, we transmit $\bOmegaT + v(t)$ at every time instant $t$, where $v(t)$ is a new random linear combination of the source symbols in $\Fset$ generated at every time $t$.  Since $\LHSfuncprime \leq C_{2, 3}$, there is the possibility that at some point, the weaker user is able to decode all symbols in $\Fset$ even before Phase~\Rmnum{2} has concluded.  Say that this is the case and the stronger user has stalled on receiving a particular symbol $\bOmegaC \in \BsetOmegaC$ being repeated.  Let us further assume that the weaker user has already received $\bOmegaC$.  Then while $\bOmegaC + v(t)$ is being transmitted, all transmissions to the weaker user are redundant.
%and thus the reception of a channel symbol will not lead to the acquisition of an independent equation that can be used to decode an additional source symbol.  
After $\bOmegaC$ is received by the stronger user and the transmitter moves on to  $\bOmegaPrime$, the next symbol in $\BsetOmegaC$ to be sent via the modified repetition scheme, the weaker user can continue to receive innovative information.  However, the set of transmissions received while $\bOmegaC$ is being repeated prevents the weaker user from achieving an optimal \emph{individual} latency.

However, we show that the optimal \emph{minmax} latency can still be achieved.  Notice that the moment all symbols in $\Fset$ can be decoded by the weaker user, the random linear combination $v(t)$ can be subtracted from any transmission $\bOmegaT + v(t)$ in Phase~\Rmnum{2}.  Therefore the remainder of Phase~\Rmnum{2} effectively consists of uncoded transmissions from the weaker user's perspective, and he is eventually able to decode all $\LHSfuncprime$ symbols.  Thus, so long as $\LHSfuncprime = \epsilon_2 - d_2$, the weaker user will decode the necessary amount of symbols before the conclusion of Phase~\Rmnum{3}, while the stronger user decodes at an optimal latency the moment Phase~\Rmnum{3} terminates.  In this case, the stronger user is the bottleneck of the system and in fact, the condition $\LHSfuncprime \leq C_{2, 3}$ is equivalent to $\wone\geq \wtwo$.  

%For example, say that $\bOmegaPrime$ is the last symbol in $\BsetOmegaC$ about to be sent via the modified repetition coding at time $t_2$.  

%Although the weaker user receives $NC_{2, 3}$ equations during Phases~\Rmnum{2} through~\Rmnum{3}, it may not necessarily be the case that all equations are independent.  Recall from Section~\ref{subsubsec:inner_bound} that in Phase~\Rmnum{2} of our coding scheme, we transmit $\bOmegaT + v(t)$ at every time instant $t$, where $v(t)$ is a new random linear combination of the source symbols in $\Fset$ generated at every time $t$.  There is the possibility that at some point, the weaker user is able to decode all symbols in $\Fset$ even before Phase~\Rmnum{2} has concluded.  

%Notice however, that the moment the weaker user can decode all source symbols in $\Fset$, the only remaining symbols missing are those in $\BsetOmegaC$.  Therefore, during Phase~\Rmnum{2} when $\bOmegaT + v(t)$ is being repeated, the weaker user can subtract off $v(t)$, and can thereby 

%may not necessarily lead to an \emph{independent} equation received.  

%Now, although the weaker user receives $NC_{2, 3}$ equations during Phases~\Rmnum{2} through~\Rmnum{3}, it may not necessarily be the case that all equations are independent.  Recall from Section~\ref{subsubsec:inner_bound} that in Phase~\Rmnum{2} of our coding scheme, we transmit $\bOmegaT + v(t)$ at every time instant $t$, where $v(t)$ is a new random linear combination of source symbols in $\Fset$ generated at every time $t$.  There is the possibility that at some point, the weaker user is able to decode all symbols in $\Fset$ even before Phase~\Rmnum{2} has concluded.  If this is the case and the stronger user has stalled on receiving a particular symbol being repeated that the weaker user has already received, then all transmissions to the weaker user will be redundant, and thus the reception of a channel symbol will not lead to the acquisition of an independent equation that can be used to decode an additional source symbol.  

%we will have received more equations than unknown variables and so the weaker user can decode at the conclusion of Phase~\Rmnum{3}.  In this case, the stronger user is the bottleneck of the system and the condition $\LHSfuncprime \leq C_{2, 3}$ is equivalent to $w_{1}(d_1) \geq w_2(d_2)$.  

On the other hand, if $\LHSfuncprime > C_{2, 3}$, the weaker user has more unknown variables than equations and so he cannot yet decode at the conclusion of Phase~\Rmnum{3}.  However, every transmission he has received so far is ``innovative'' in the sense that it provides an independent equation that can be used to decode the $N\LHSfuncprime$ source symbols.  In order to decode, we simply need to send additional equations to the weaker user, and since $\LHSfuncprime = \epsilon_2 - d_2$, there will not be any unnecessary source symbols sent.  Since the stronger user is point-to-point optimal at the conclusion of Phase~\Rmnum{3}, we have therefore sent a total of $N\wone$ transmissions up to that point.  Since, from the weaker user's perspective, we have hitherto been sending random linear combinations of $N\LHSfuncprime$ variables, we simply need to continue doing so for another $N(\wtwo - \wone)$ transmissions in Phase~\Rmnum{4} before he receives the remaining number of equations required and achieves point-to-point optimal performance.

\begin{table}
	\begin{center}
		\begin{tabular}{| c | c |}
%    \caption{The justification for the ordering}
			\hline
			\multicolumn{2}{|c|}{{\bf Ordering of Boundaries for $d_1$}} \\
			\hline
			{\bf Inequality} & {\bf Justification}   \\ \hline
			$\donedaggall < \doneddaggall$ & $(1 - \epsot)^2 > 0$ \\ \hline 
			$\doneddaggall < \epsilon_1$ & $\epsot < 1$, $\epsilon_1 > 0$ \\ \hline 
			\hline
		\end{tabular}
	\end{center}
	\caption{We justify the ordering of the region boundaries for $d_1$.  In the left column, we have the ordering between two boundary points, and in the right column, we show the necessary and sufficient condition that justifies the ordering.}	
	\label{tab:d1_boundaries}	
\end{table}


We therefore see that regardless of whether $\LHSfuncparam$ is greater or less than $C_{2, 3}$, we can achieve an optimal minmax latency so long as we can find $\gammaparam, \omegaparam \in [0, 1]$ such that $\LHSfuncparam = \epsilon_2 - d_2$.   We focus on finding these values of $\gammaparam$ and $\omegaparam$ in the next sections. In doing so, we consider three cases cases for $d_1$.  
%Let 
%\begin{subequations}
%\begin{align}
%	\label{eq:donedagg}
%	\donedagg &= \frac{\epsilon_1}{\epsot}(2 \epsot - 1) \\
%	\label{eq:doneddagg}
%	\doneddagg &= \frac{\epsilon_1}{\epsot}(2 \epsot - 1) \\
%\end{align}
%\end{subequations}
%
The first is when $0 \leq d_1 \leq \donedaggall$, the second when $\donedaggall < d_1 < \doneddaggall$, and third when $\doneddaggall \leq d_1 < \epsilon_1$.  We justify the position of these boundary points with Table~\ref{tab:d1_boundaries}.  For example, in the first row of Table~\ref{tab:d1_boundaries}, we justify that the boundary point $\donedaggall$ is less than the boundary point $\doneddaggall$ with the necessary and sufficient condition that $(1 - \epsot)^2 > 0$.  

After dividing the values of $d_1$ into regions, we then further consider regions of $d_2/\epsilon_2$, where each region requires a distinct choice for the values of $\gammaparam$ and $\omegaparam$.  We note that from Remark~1 in~\cite{TLKS_TIT16} that for $i \in \{1, 2\}$, we assume that $d_i/\epsilon_i < 1$, otherwise an uncoded transmission strategy can achieve~\eqref{eq:wplusd}.  Therefore, we consider only values of $d_2/\epsilon_2 \in [0, 1]$.  In the following sections, the regions of $\depstwo$ will depend on the boundaries $\cstar$, $\mydstar$, $\astar$ and $\bstar$, which we define as

%Within each case that $d_1$ falls into, we further consider regions for $d_2$ that require separate choices of $\gammaparam$ and $\omegaparam$ so that $\LHSfuncparam = \epsilon_2 - d_2$.  These regions depend on the boundaries $\astar$ and $\bstar$, which we define as

\begin{align}
	\cstar &= \left(\frac{\epsot}{\epsilon_2}\right) \left(\frac{d_1}{\epsilon_1}\right), \\
	\mydstar &= \frac{d_1\epsot + \epsilon_1(\epsilon_2 - \epsot)}{\epsilon_1\epsilon_2}, \\	
	\astar &= \frac{\epsot (d_1 (1 - \epsilon_2) + \epsilon_1(\epsilon_2 - \epsot))}{(1 - \epsot)\epsilon_1 \epsilon_2},\\
	\bstar &= \frac{d_1\epsot(1 - \epsilon_2) + \epsilon_1(\epsilon_2 - \epsot)}{(1 - \epsot) \epsilon_1 \epsilon_2}.
\end{align}

%\begin{align}
%	\astar &= \frac{\epsilon_1\epsilon_2(1 - \epsilon_1) + d_1(1 - \epsilon_2)}{1 - \epsilon_1\epsilon_2}, \\
%	\bstar &= \frac{(1 - \epsilon_1) + d_1(1 - \epsilon_2)}{1 - \epsilon_1\epsilon_2}.
%\end{align}

\input{one_sided/optimality_regions}