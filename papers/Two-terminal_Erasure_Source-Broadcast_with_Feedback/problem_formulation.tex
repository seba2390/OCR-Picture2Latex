\section{{}{System Model}}
%\subsection{{}System Model}
%\section{Problem Formulation}
\label{sec:system_model}

%We first define the problem in Section~\ref{subsec:problem_definition}.  In Section~\ref{subsec:metrics}, we then discuss metrics that are of interest in the evaluation of broadcast codes and show how a solution to the problem in Section~\ref{subsec:problem_definition} can be used to address these metrics.

%\input{2_problem_formulation/problem_definition}
%\input{2_problem_formulation/metrics}

%%%\begin{figure*}
%%\begin{figure}
%%	\centering
%%%	\input{3/out/fig/schematic}
%%%	\includegraphics[scale=0.6]{3/out/fig/schematica}
%%%	\includegraphics[scale=0.75]{out/fig/schematic}
%%%	\includegraphics[scale=0.8]{2_problem_formulation/fig/schematicE}
%%	\includegraphics{2_problem_formulation/fig/schematicE}
%%%	\includegraphics[width=0.6\textwidth]{outer_bound.png}
%%	\caption{Broadcasting an equiprobable binary source over an erasure broadcast channel.}
%%	\label{fig:schematic}
%%%\end{figure*}
%%\end{figure}

%We begin by presenting our joint source-channel coding formulation which is illustrated in Fig.~\ref{fig:schematic}.

%\begin{figure*}
\begin{figure}
	\centering
%	\input{fig/schematicE}
%	\includegraphics[scale=0.95]{fig/schematic}
%	\includegraphics[scale=0.75]{fig/schematicE_two_bec}
	\includegraphics[scale=1]{fig/schematicE_two_bec}
%	\includegraphics[scale=0.75]{SysModel/fig/schematic_arrow}
	\caption{Broadcasting an equiprobable binary source over an erasure broadcast channel.}
	\label{fig:schematic}
%\end{figure*}
\end{figure}

The problem is illustrated in Fig.~\ref{fig:schematic}.  We consider {{}a} binary memoryless source $\{S(t)\}_{t=1,2, \ldots}$ that produces equiprobable symbols in the alphabet $\mathcal{S}=\{0,1\}$ and that we wish to communicate to two users over an erasure broadcast channel.  %Let $\mathcal{S}^{N}$ be the set of all $N$-vectors with components in $\mathcal{S}$, and denote $(S(1), S(2), \ldots , S(N))$ as $S^{N}$.  For convenience, we will also denote the set $\{1,2,\dots,N\}$ as $[N]$.
%
%\begin{figure}
%	\centering
%%	\input{3/fig/schematic}
%	\includegraphics[scale=0.8]{fig/schematicE}
%%%	\includegraphics[width=0.6\textwidth]{outer_bound.png}
%	\caption{Broadcasting an equiprobable binary source over an erasure broadcast channel with individual bandwidth mismatches.}
%	\label{fig:schematicE}
%\end{figure}
%
The source is communicated by a block-encoding function that maps a length-$m$ source sequence, $S^{m}$, to a length-$n$ channel input sequence, $X^{n} = (X(1), X(2), \ldots , X(n))$, where $X(t)$ denotes the $t^{\mathrm{th}}$ channel input taken from the alphabet $\mathcal{X} = \{0, 1\}$.  %Note here that our notation defines $X^{n}$ as the first $n$ channel symbols sent. %which is also taken from the alphabet $\mathcal{S}$.  %

% Original
%Let $Y_{i}(t)$ be the channel output observed by user $i$ on the $t^{\mathrm{th}}$ channel use for $i \in [n]$ and $t \in [W]$.  Our channel model is an erasure channel.  In particular, let $\epsilon_i$ denote the erasure rate of the channel corresponding to user~$i$.  Without loss of generality, we will assume that $0 < \epsilon_{1} < \epsilon_{2} < \ldots < \epsilon_{n} < 1$.  Then $Y_{i}(t)$ exactly reproduces the channel input $X(t)$ with probability $(1 - \epsilon_i)$ and otherwise indicates an erasure event, which happens with probability $\epsilon_i$.  We let $Y_{i}(t)$ take on values in the alphabet $\mathcal{Y} = \{0, 1, \star\}$ so that an erasure event is represented by `$\star$,' the erasure symbol.  We associate user~$i$ with the state sequence $(N_i(t))_{t \in [W]}$, which is defined such that $N_{i}(t) = 1$ if $Y_i(t)$ was erased and $N_i(t) = 0$ otherwise.  The channel we consider is memoryless in the sense that $N_{i}(t)$ is drawn i.i.d.\ from a $\operatorname{Bern}(1 - \epsilon_{i})$ distribution.

Let $Y_{i}(t)$ be the channel output observed by user $i$ on the $t^{\mathrm{th}}$ channel use for $i \in \{1, 2\}$ and $t \in [n]$.  Our channel model is a binary erasure broadcast channel as shown in Fig.~\ref{fig:schematic}.  In particular, let $\epsilon_i$ denote the erasure rate of the channel corresponding to user~$i$, where we assume that $0 < \epsilon_{1} < \epsilon_{2}$, and $\epsilon_2 = 1 - 1/M$ for some integer $M > 1$.  Our model specifies that $Y_{i}(t)$ exactly reproduces the channel input $X(t)$ with probability $(1 - \epsilon_i)$ and otherwise indicates an erasure event, which happens with probability $\epsilon_i$.  We let $Y_{i}(t)$ take on values in the alphabet $\mathcal{Y} = \{0, 1, \star\}$ so that an erasure event is represented by `$\star$,' the erasure symbol.  
We can associate user~$i$ with the channel-noise state sequence $(N_i(t))_{t \in [n]}$, where $N_i(t) \in \{0,1\}$, and we will have that $Y_i(t)$ is erased if $N_{i}(t) = 1$ and $Y_{i}(t) = X(t)$ if $N_i(t) = 0$.  The channel we consider is memoryless in the sense that $N_{i}(t)$ is drawn i.i.d.\ from a $\operatorname{Bern}(\epsilon_{i})$ distribution.
%\textcolor{blue}{We assume that the erasure rates are known to both the transmitter and receiver. If such channel knowledge is not possible, we can take the standard approach of interpreting a broadcast channel as an abstraction for a \emph{compound channel}~\cite{CT}.  In such a situation, the compound channel is a channel that randomly takes on one of many potential states for the duration of transmission.  We model this by considering a broadcast channel whose constituent channels correspond to these potential channel states.}
%{}{Note that in our setup, the erasure rates for each user are assumed to be known. However, our setup also models the 
%compound channel~\cite{CT}, where the erasure rate is not known and instead belong to a collection of possible states, with each 
%state corresponding to one virtual user in our system.}

%Let $Y_{i}(t)$ be the channel output observed by user $i$ on the $t^{\mathrm{th}}$ channel use for $i \in [n]$ and $t \in [W]$.  Our channel model is an erasure channel.  In particular, $Y_{i}(t)$ either exactly reproduces the channel input $X(t)$ or otherwise indicates an erasure event.  We let $Y_{i}(t)$ take on values in the alphabet $\mathcal{Y} = \{0, 1, \star\}$ so that an erasure event is represented by `$\star$,' the erasure symbol.  Let $\epsilon_i$ denote the erasure rate of the channel corresponding to user~$i$.  Without loss of generality, we assume that $0 < \epsilon_{1} < \epsilon_{2} < \ldots < \epsilon_{n} < 1$.  We associate user~$i$ with the state sequence $(N_i(t))_{t \in [W]}$, where $N_i(t) \in \{0,1\}$.  The state sequence represents the noise on user~$i$'s channel and we will have that $Y_i(t)$ is erased if $N_{i}(t) = 1$ and $Y_{i}(t) = X(t)$ if $N_i(t) = 0$.  The channel we consider is memoryless in the sense that $N_{i}(t)$ is drawn i.i.d.\ from a $\operatorname{Bern}(1 - \epsilon_{i})$ distribution.

%\begin{equation}
%%\label{eq:channel_model}
%	Y_{i}(t) = X(t) \cdot N_{i}(t),
%\end{equation}
%
%where $N_{i}(l)$ is a $\operatorname{Bern}(1 - \epsilon_{i})$ random variable representing the noise at user $i$'s $l^{\mathrm{th}}$ channel output.  The channel is memoryless in the sense that $N_{i}(l)$ is drawn i.i.d.\ from a $\operatorname{Bern}(1 - \epsilon_{i})$ distribution.  %Specifically, the statistics of $N_{i}(l)$ are such that
%
%\begin{equation}
%\label{eq:nstat}
%	N_{i}(l) =
%	\begin{cases}
%		0 & \text{with probability} \ \epsilon_{i}, \\
%		1 & \text{with probability} \ 1 - \epsilon_{i}.
%	\end{cases}
%\end{equation}
%
%Thus, $Y_{i}(l)$ takes on values in the alphabet $\mathcal{Y} = \{-1, 0, +1\}$ and we interpret $Y_{i}(l)$ as the output of an erasure channel, with erasure probability $\epsilon_{i}$, when $X(l)$ is the channel input (see Section~\ref{sec:erased_rv} for a discussion of the erasure channel model).
%
%Thus, $Y_{i}(l)$ takes on values in the alphabet $\mathcal{Y} = \{-1, 0, +1\}$.  If we define `0' as the erasure symbol, we can interpret $Y_{i}(l)$ as the output of an erasure channel with erasure probability $\epsilon_{i}$ when $X(l)$ is the channel input. Without loss of generality, we will assume that $0 < \epsilon_{1} < \epsilon_{2} < \ldots < \epsilon_{n} < 1$. %Users on channels with the same erasure probability are represented by a user of the smallest distortion

%Now although the length-$n$ channel input is broadcasted to both users, we will assume that due to each user's delay constraint, the $i^{\mathrm{th}}$ user reconstructs the source after observing only the first $n_{i}$ channel input symbols, denoted as $X^{n_{i}}$, where $n_{i} \leq n$.  Specifically, we have that $n = \max(n_{1}, n_{2})$.

Having observed his channel output, user $i$ then uses it to reconstruct the source as a length-$m$ sequence, denoted as $\hat{S}_{i}^{m}$.  We will be interested in a fractional recovery requirement so that each symbol in $\hat{S}_{i}^{m}$ either faithfully recovers the corresponding symbol in $S^{m}$, or otherwise a failure is indicated with an erasure symbol, i.e., we do not allow for any bit flips.

More precisely, we choose the reconstruction alphabet $\mathcal{\hat{S}}$ to be an augmented version of the source alphabet so that $\mathcal{\hat{S}} = \{0, 1, \star\}$, where the additional `$\star$' symbol indicates an erasure symbol.  We then express the constraint that an achievable code ensures that each user $i \in \{1, 2\}$ achieves a fractional recovery of $1 - d_{i}$, where $d_{i}  \in [0, 1]$, with the following definition.




%by first defining the set $D_{i}(\hat{S}_{i}^{N}) = \{l \in [N] : \hat{S}_{i}(l) = 0\}$.

%Note that the we have taken the reconstruction alphabet to be an augmented version of the source alphabet by adding the additional `0' symbol, which we interpret as an erasure symbol.  In our reconstruction, we require that $\hat{S}_{i}^{N}$ is such that for $l \in [N]$, $\hat{S}_{i}(l) = S(l)$ or $\hat{S}_{i}(l) = 0$.  In other words, we do not allow any bit flips in our reconstruction so that $\hat{S}_{i}(l)$ either faithfully reproduces $S(l)$ or otherwise outputs an erasure symbol.


%%%On a symbol-by-symbol basis, we measure the reconstruction's fidelity with the erasure distortion $d_{E}: \mathcal{S} \times \hat{\mathcal{S}} \to \{0,1, \infty\}$ given by
%%%%Depending on the distortion measure we use, $\mathcal{\hat{S}}$ can either be a binary source alphabet or the extended alphabet $\mathcal{\hat{S}_{E}} = \{-1, 0, 1\}$, where `0' denotes the source erasure symbol.
%%%
%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%
%%%%On the other hand, when the output alphabet can include the source erasure symbol as in the second case, then on a symbol-by-symbol comparison, the reconstruction's fidelity is measured with the erasure distortion $d_{E} : \mathcal{S} \times \mathcal{\hat{S}_{E}} \to \{0, 1, \infty\}$, given by
%%%
%%%\begin{equation}
%%%\label{eq:symbol_distortion1}
%%%	d_{E}(s, \hat{s}) =
%%%	\begin{cases}
%%%		0 & \text{if } \hat{s} = s, \\
%%%		1 & \text{if } \hat{s} = 0 \\
%%%		\infty & \text{otherwise}.
%%%	\end{cases}
%%%\end{equation}
%%%%
%%%The per-letter distortion of a vector is then defined as $d(s^{k}, \hat{s}^{k}) = \frac{1}{k} \Sigma_{m=1}^{k} d(s_{m}, \hat{s}_{m})$.
%%%
%%%%\begin{equation}
%%%%\label{eq:vector_distortion1}
%%%%	d(s^{k}, \hat{s}^{k}) = \frac{1}{k} \sum\limits_{i=1}^k d(s_{i}, \hat{s}_{i}).
%%%%\end{equation}
%%%%
%%%%where $d(\cdot, \cdot)$ can be either $d_{H}(\cdot, \cdot)$ or $d_{E}(\cdot, \cdot)$.
%%%
%%%%We point out that with the erasure distortion measure,  a finite distortion value indicates that we are certain about the accuracy of the non-erased symbols in our reconstruction, while the actual value of the distortion is a measure of the proportion of bits that we are unsure about, i.e., that have been erased in the reconstruction.
%%%
%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%
%%%
%%%We point out that with the erasure distortion measure,  a finite distortion value indicates that we are certain about the accuracy of the non-erased symbols in our reconstruction, while the actual value of the distortion is a measure of the proportion of bits that we are unsure about, i.e., that have been erased in the reconstruction.  We now define the components of our problem.

\begin{mydef}
\label{def:code}
	An $(m, n, d_{1}, d_{2})$ code for source $S$ on the erasure broadcast channel consists of
	
	\begin{enumerate}
		\item an encoding function $f_{m} : \mathcal{S}^{m} \to \mathcal{X}^{n}$ such that $X^{n} = f_{m}(S^{m})$, and
		
		\item Two decoding functions $g_{i,m} : \mathcal{Y}^{n} \to \mathcal{\hat{S}}^{m}$ such that $\hat{S}_{i}^{m} = g_{i,m}(Y_{i}^{n})$ and for each $i \in \{1, 2\}$,
		\begin{enumerate}
			\item $\hat{S}_{i}^{m}$ is such that for $t \in [m]$, if $\hat{S}_{i}(t) \neq S(t)$, then $\hat{S}_{i}(t) = \star$,
%			\item $\mathbb{E}   \left\vert{D_{i}(\hat{S}_{i}^{N})}\right\vert \leq d_{i}$
			\item $\mathbb{E}   \left\vert{\{t \in [m] \mid \hat{S}_{i}(t) = \star\}}\right\vert \leq m d_{i}$,
		\end{enumerate}
		 	
%	$\mathbb{E}d(S^{k}, g_{i}( X^{n} \cdot N_{i}^{n})) \leq D_{i}$ holds for $i \in \{1,2\}$		
	\end{enumerate}
	
	where $\mathbb{E}(\cdot)$ is the expectation operation and $\vert A \vert$ denotes the cardinality of set $A$.
\end{mydef}

\begin{remark}
	The fractional recovery requirement of our problem can also be captured by considering the erasure distortion $d_{E}(\cdot, \cdot)$, and requiring that the average $m$-letter distortion between the source and reconstruction sequences be less than a prescribed value.   For $s \in \mathcal{S}$, and $\hat{s} \in \mathcal{\hat{S}}$, the erasure distortion measure is given by
\begin{equation}
\label{eq:symbol_distortion1}
	d_{E}(s, \hat{s}) =
	\begin{cases}
		0 & \text{if } \hat{s} = s, \\
		1 & \text{if } \hat{s} = \star \\
		\infty & \text{otherwise}.
	\end{cases}
\end{equation}

\end{remark}

%A point is now made about the modelling of latencies in our problem.  We define the \emph{latency} or \emph{bandwidth expansion factor} $b \in [0, \infty )$, as the number of channel uses per source symbol that are delivered over the broadcast channel, i.e., $b \triangleq n/k$.  This is to say that $b \cdot k$ channel uses are required before both users can reconstruct $S^{k}$ subject to their distortion constraints.  Our problem is now defined as characterizing the achievable latency region under a given pair of distortion constraints as per the next definition.

%A point is now made about the modelling of delays in our problem.  We define the bandwidth expansion factor $b \in [0, \infty )$, as $b = n/k$.  We interpret $b$ as the normalized latency (in units of ``channel uses per source symbol''), before both users can reconstruct $S^{k}$ subject to their distortion constraints.  Our problem is now defined as characterizing the achievable distortion region under a given bandwidth expansion factor as per the next definition.

For a given code, we next define the {\it latency} that the code requires before all users can recover their desired fraction of the source.  We then state our problem as characterizing the achievable distortion pairs under a prescribed latency as per the following definitions.

\begin{mydef}
	The latency, $b$, of an~$(m, n, d_{1}, d_{2})$ code is the number of channel uses per source symbol that the code requires to meet all distortion demands, i.e., $b = n/m$.
\end{mydef}

\begin{mydef}
\label{def:achievable}
	The distortion pair $(d_{1}, d_{2})$ is said to be achievable with latency $b$ over the erasure broadcast channel if for every $\delta > 0$, there exists for sufficiently large $m$, an $(m, b\cdot m, \hat{d}_{1}, \hat{d}_{2})$ code such that for all $i \in \{1, 2\}$, $d_{i}+\delta \geq \hat{d}_{i}$.
\end{mydef}

%\begin{mydef}
%\label{def:achievable}
%	Latency $b$ is said to be $(d_{1}, d_{2})$-achievable over the erasure broadcast channel if for every $\delta > 0$, there exists for sufficiently large $m$, an $(m, b\cdot m, \hat{d}_{1}, \hat{d}_{2})$ code such that for all $i \in \{1, 2\}$, $d_{i}+\delta \geq \hat{d}_{i}$.
%	
%%	\begin{equation}
%%		D_{i}+\delta \geq d_{i},  \quad i \in \{1, 2\}.
%%	\end{equation}	
%%	Alternatively, we may occasionally say that the tuple $(b, D_{1}, D_{2})$ is achievable if the latency $b$ is $(D_{1}, D_{2})$-achievable.
%	
%%The achievable latency region is the set of all achievable latencies under the prescribed distortion vector.
%
%\end{mydef}

\begin{remark}
	In our work, we assume that the latency $b > 1$, since otherwise, finding matching inner and outer bounds are trivial~\cite{TLKS_TIT16} for the case when $b \in (0, 1]$.
\end{remark}

Finally, in our outer bound, we consider only the class of non-erasure-randomized codes, which assumes that the positions of non-erased symbols in the reconstruction of $\SHatK{2}$ given $\YW$ depends only on the channel noise realization $\NW$, and not on the actual channel output \YW.  
That is, we assume that the decoder for the weaker user, user~2, is such that given \NW, which specifies the positions of the erasures in the channel output, the positions of the reconstructed source symbols are determined.  We formally define this below.
%That is, we assume that given \nN{0} and the position of the erasures in the channel output, \nN{i} can be deterministically constructed and consequently, the positions of the reconstructed source symbols, as indicated by $R_i$ are deterministic so that $R_i = f(N_0^n)$.

\begin{mydef}
\label{def:non_erasure_randomized}
	Let $\YW$ be the channel output of user~2.  An $(m, n, d_1, d_2)$ code is said to be non-erasure-randomized if for every $\nN{0} \in \{0,1\}^n$, there exists a predetermined set $R(\nN{0}) \subseteq [m]$ such that the reconstruction 
%	$\tSM{0} = g_{i, m}(\YW)$ satisfies
	$\SHatK{2} = g_{i, m}(\YW)$ satisfies
	
	\begin{enumerate}
		\item $\SHatK{2}(k) = \star$ for all $k \in [m] \setminus R(\nN{0})$
		\item $\SHatK{2}(j) = S^m(j)$ for all $j \in R(\nN{0})$.
	\end{enumerate}
\end{mydef}

%\begin{remark}
%\label{rem:deps}
%Throughout this paper we will assume that for each user $i \in [n]$, we have that $d_i < \epsilon_i$. Any user $j$ with $d_j \ge \epsilon_j$ will be trivially satisfied by the systematic portion of our segmentation-based coding scheme.  Furthermore, we will show in Lemma~\ref{lem:bn1} that within our class of coding schemes, such a systematic portion can be transmitted without loss of optimality when at least one user satisfies $d_i < \epsilon_i$. Finally, if every user satisfies $d_i \ge \epsilon_i$, a simple uncoded transmission scheme is easily shown to be optimal.
%\end{remark}

%\begin{remark}
%	While our system model has assumed binary alphabets for both source and channel input sequences, our results can easily be extended to larger alphabet sizes.  {Provided we keep source and channel input alphabets identical in size, our results could then extend to packet erasure networks.}
%\end{remark}


%For convenience, throughout this paper, for any positive integer $k$, we shorthand the set $\{1,2,\dots,k\}$ as $[k].$
%
%Consider broadcasting a length-$N$ binary sequence $S = \{s(j)\}_{j\in[N]}$ from a source to $n$ receivers over independent memoryless binary erasure channels. For $i\in[n]$, denote the erasure rate of the channel between the source and receiver $i$ as $\epsilon_i$. Applying some encoding scheme to $S$, the source generates and broadcasts the coded binary sequence $X=\{x_j\}_{j\ge1}$. Each receiver $i$ retrieves from the channel a sequence $Y^i = \{y_{j}^i\}_{j\ge1}$ where $y_{j}^i \in \{0,1,e\}$, $e$ indicating an erasure. Denote $Y^{i,W} = \{y_{j}^i\}_{j\in[W]}$ as the truncation of $Y^i$ up to the $W$th symbol. Receiver $i$ reconstructs from $Y^{i,W}$ a copy $\hat S^{i,W}=\{\hat s_{j}^{i,W}\}_{j\in[N]},$ where $\hat s_{j}^{i,W}\in\{0,1,e\}.$
%
%%\subsection{Erasure Distortion}
%We consider a symbol-by-symbol based erasure reconstruction distortion measure $D_e: \{0,1\}\times \{0,1,e\}\mapsto \mathds R$ defined as
%\begin{align*}
%D_E(s,\hat s) = \left\{\begin{array}{lr} 0,& \textnormal{if } \hat s=s\\ 1, & \textnormal{if } \hat s\ne s, \hat s\ne e\\ \infty,& \textnormal{if } \hat s=e \end{array}\right.
%\end{align*}
%%for symbols $s\in\{0,1\}$ and $\hat s\in\{0,1,e\}$,
%and
%\begin{align*}
%D(S,\hat S^{i,W}) =  \frac{1}{N}\sum_{j\in[N]} D_E(s_j,\hat s_{j}^{i,W}).
%\end{align*}
%for the source sequence $S$ and the sequence $\hat S^{i,W}$ reconstructed by user $i$ from the first $W$ symbols of the channel output sequence $Y^{i,W}$.
%
%Now our problem is to upper bound and lower bound the minimum $W$, or $w=W/N,$ which we refer to as the {\it latency}, that guarantees $\mathds E[ D(S,\hat S^{i,W})]\le d_i$ for all $i\in[n]$,
%where $d_i$ is the given distortion requirement of user $i$.
%
%Clearly, $w$ is trivially lower bounded by $\max_{i\in[n]}\{\frac{1-d_i}{1-\epsilon_i}\}$. In the next section, we provide an upper bound of $w$ and the achieving coding scheme. We show that the upper bound is achieved with a coding scheme that sends a part of the source sequence uncoded, partitions the rest of the source sequence into a number of segments, and encodes each segment by a systematic optimal erasure code. The optimal segmentation can be found by solving a linear programming problem. We also show in Sec.~\ref{sec:jscc} that the segmentation-based scheme is equivalent to the Mittal-Phamdo~\cite{} scheme for erasure channels.

%\input{SysModel/individual_code}

%\input{SysModel/JSCC_approach3}








