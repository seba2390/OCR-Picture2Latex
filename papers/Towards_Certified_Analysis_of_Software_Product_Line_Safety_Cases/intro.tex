\vspace{-0.2in}
\section{Introduction}
\label{sec:intro}
\vspace{-0.1in}
%Software systems evolve over time, and each change to a system might affect its safety. 
The development of safety-critical systems usually involves a rigorous safety engineering process. A primary artifact resulting from that is a \term{safety case}, identifying potential safety hazards, their mitigation goals, and pieces of evidence required to show that goals have been achieved.
Safety cases, together with other system artifacts, are usually inspected and analyzed by tools as a part of the safety engineering process. In safety-critical domains, correctness of those tools is essential to the integrity of the whole process. \term{Correctness certification} of tools w.r.t. their specifications becomes of extremely high value in this context.
%Safety case elements typically refer to the design/implementation artifacts for which safety hazards are to be mitigated, and from which mitigation evidence is to be collected and validated. Traceability links between the safety model and a system model allow for analysis algorithms involving both models.

In many cases, families of safety-critical software products are developed together in the form of  \term{Software Product Lines (SPLs)}. Different product variants of an SPL have different \term{features}, i.e., externally visible attributes such
as a piece of functionality, support for a particular peripheral device, or a performance optimization. Each feature can be either present or absent in each of the product variants of an SPL.
Given this combinatorial nature of feature composition, 
%the number of products grows exponentially with the number of features. As a result, 
analyzing the safety of each product instance individually in a \term{brute-force} fashion is usually intractable. 

Several source-code and model-based analysis tools have been \term{lifted} to product lines~\cite{Thum:2014,Kastner:2012,Gazzillo:2012,Salay:2014,Bodden:2013,Classen:2013,Shahin:2019,Shahin:2020} in the sense that they can be applied efficiently to the whole product line at once, leveraging the commonalities between individual products, and thus generating aggregated results for the complete set of products. Those results have to be correct with respect to applying the analysis to each product individually. 
%and when analyzing safety-critical systems fulfilling this \term{correctness} requirement needs to be guaranteed. 
However, to the best of our knowledge, lifting of safety analyses has not been attempted before. 
%In addition, correctness validation of different analysis lifting approaches varies widely from testing all the way to formal proofs. Automatic analysis of safety cases is expected to meet a high standard of correctness validation, and the same holds when applying a lifted safety analysis to product lines. As a result, manual inspection and testing are typically inadequate when certifying the correctness of lifted safety-critical algorithms.

In this paper, we present a systematic methodology to correct-by-construction lifting of safety case analysis algorithms to software product lines. This includes infrastructure building blocks for implementing lifted algorithms, and proving their correctness with respect to their single-product counterparts. We use the Lean interactive theorem prover~\cite{deMoura:2015} to formalize the correctness criteria of lifting, implement our lifting infrastructure, and prove the correctness of lifted algorithms. A Lean proof is machine-checked, so it can be used as a \term{correctness certificate} of the property being proven.

We demonstrate our approach on a \term{Change Impact Assessment (CIA)} algorithm~\cite{Kokaly:2017} that takes a system model, an assurance case, traceability links in between, and a modification to the system model as inputs, and determines the set of safety case elements that need to be revised or rechecked.

%Instead, we need to find a way to efficiently apply an impact assessment algorithm to the entire product line at once, leveraging the degree of commonality across different product variants, and identifying product line safety case elements impacted.
%The integrity of the safety case needs to be maintained across modifications to the system. In the worst case, the entire safety case needs to be reviewed after each change.

%Several approaches have been proposed to localize the impact of changes only to specific parts of the safety case.
%For example, \term{Change Impact Assessment (CIA)} algorithms (e.g., \cite{Kelly:2001,Kokaly:2017}) identify how local changes to specific system elements affect the structure and validity of the safety case, given traceability links between the two. The impacted safety case elements need to be revised or rechecked, and the rest of the safety case can be reused as is. Soundness of impact assessment algorithms is essential in safety-critical systems, because missing the revision of a safety goal impacted by a system modification might result in the propagation of catastrophic software bugs all the way to deployment. Precision of impact assessment is also important, because the fewer false positives are identified by the algorithm, the less time is spent reviewing the safety case after a system modification.

%Impact assessment algorithms can only work on the artifacts of a single software product at a time. However, in many cases a family of software products is developed together in the form of a \term{Software Product Line (SPL)}.
%An SPL is a family of related software products developed together.
% from a common set of artifacts, in addition to feature-specific artifacts. 


%\begin{sidewaysfigure}[p]
%	\centering
%	\includegraphics[width=\textwidth]{ac}
%	\caption{An annotated GSN assurance case of the LMS software product line.}
%	\vspace{-0.2in}
%	\label{fig:ac}	
%\end{sidewaysfigure}

\begin{figure}[t]
	\centering
	\includegraphics[width=\textwidth]{motivating-ex}
	\caption{Lifted change impact assessment when the "Visual" class is modified. A dashed ellipse around the Visual class denotes a modification, and dashed ellipses around safety case elements indicate that they need to be rechecked as a result.}
	\label{fig:ex}	
	\vspace{-0.3in}
\end{figure}

\vskip 0.1in
\noindent
{\bf Motivating Example.}
%\subsection{Motivating Example}
Consider the \term{Lane Management System (LMS)} system outlined in~\cite{Chechik:2020}. 
%Fig.~\ref{fig:classdiag} shows a UML class diagram of that product line, using colored annotations for different features.
LMS can be thought of as a product line with several features, including: \term{Lane Departure Warning System (LDWS)}, \term{Audio warning (Audio)}, and \term{Visual warning (Visual)}. For simplicity of presentation, we assume that all feature combinations are allowed.
Fig.~\ref{fig:ex} shows a snippet of the class diagram of the LMS product line, and the corresponding snippet of its GSN~\cite{Kelly:2004} assurance case, with traceability links in between the two. 
%Fig.~\ref{fig:ac} shows a Goal Structuring Notation (GSN) assurance case of the LMS product line. In addition to other elements, 
%A GSN assurance case is composed of four kinds of elements: goals, solutions, strategies and contexts~\cite{Kelly:2004}.
%A \name{Goal} (denoted by a rectangle) is a safety assertion that needs to be satisfied by some evidence. 
%A \name{Solution} (denoted by a circle) represents a piece of evidence about a property of the system that typically has to hold to satisfy a goal.
%A \name{Strategy} (denoted by a parallelogram) is used to decompose goals (conclusions) into subgoals (premises). 
%Finally, a \name{Context} element (denoted by a rectangle with round vertical edges) describes assumptions on the elements it connects to.

We use colored annotations to map class diagram and GSN elements to features. For example, elements colored in green belong to the \name{Audio} feature, and those in orange belong to the \name{LDWS} feature. Base system elements (existing in all products) are in yellow. 
%In this example, for simplicity we assign a color to each individual feature, but 
In general each element can be annotated by a propositional formula over features (usually referred to as a \term{presence condition}).

Consider a modification to the  \name{Visual} class. The problem CIA algorithms try to solve is figuring out how that modification of a system element would impact the safety case. 
We distinguish between two ways in which a change to the system can impact safety case elements~\cite{Kokaly:2017}: 
(1) \name{revise} -- the \emph{content} of the element (e.g., definition of a goal, or description of a solution) may have to be revised because it referred to a system element that has changed and the semantics of the content may have changed, and 
(2) \name{recheck} -- the \emph{state} of the element (e.g., whether a goal is satisfied, or a solution is available) must be rechecked because it may have changed.

In a product line setting, in addition to figuring out which elements are impacted, we also need to identify the product variants in which they are. In Fig.~\ref{fig:ex}, goals \name{G19} and \name{G20} are directly impacted by modifications to class \name{Visual} because of the direct traceability links. %between the class diagram and the safety case. 
Both classes need to be rechecked as a result, but only in products where the \name{Visual} feature is included.
% and this is one extra piece of information a lifted impact assessment algorithm is expected to output. 
In the same set of products, pieces of evidence linked to those goals (\name{Sn4}, \name{Sn11}, \name{Sn18}) need to be rechecked as well.
%only in the presence of the \name{Visual} feature. 
Note that although \name{G20}, \name{Sn4}, and \name{Sn18} belong to all product variants, we do not need to recheck them in product variants not including the \name{Visual} feature.

A CIA tool lifted to product lines has to preserve the exact semantics of its single-product counterpart. In other words, using the lifted tool should output exactly the union of outputs of the single-product tool applied to each product variant. A software bug in the lifted tool might result in false positives (elements marked as impacted while they should not). Even worse, a bug might result in overlooking an impacted element, potentially resulting in safety incidents.

%Software systems evolve all the time, and whenever a software artifact is modified, we need to make sure the modification does not negatively impact other artifacts. From a safety perspective, it is particularly essential to ensure the integrity of the assurance case of a system subject to system modifications. In a product line setting, this is more challenging because a modification in a single artifact might impact the whole set of products in which this artifact belongs.

%However, since an artifact is usually explicitly annotated by some representation of the set of products in which it exists (e.g., the color annotations on Fig.~\ref{fig:classdiag} and Fig.~\ref{fig:ac}), we already have the pieces of information needed to narrow down the set of products impacted by a modification. We also have \term{Changed Impact Assessment (CIA)} algorithms that calculate the set of elements of an assurance case impacted by a modification in the corresponding system, given a traceability map between system elements and assurance case elements. CIA algorithms work in the setting of a single product though. In this paper we \term{lift} a CIA algorithm~\cite{Kokaly:2017} to support product line variability in its inputs (system models, assurance cases, and the traceability map between the two), and propagate that variability as it computes its intermediate results, until it eventually returns CIA results for the whole product line.

%Impact assessment algorithms (e.g., \cite{Kokaly:2017}) assume their inputs to represent a single product. It is possible to generate all products of a product line, and apply the impact assessment algorithm to each of them one at a time, and then merge the results. However, since the number of products grows exponentially with the number of features, this \term{brute-force} approach is intractable, and it does not exploit the commonalities among the different product variants. Several software analyses (e.g., \cite{Shahin:2019,Gazzillo:2012,Kastner:2012,Bodden:2013}) have been \term{lifted} to product lines, i.e., they have been redesigned to efficiently process the entire product line, and produce outputs for all product variants at once, leveraging the shared artifacts between those variants and being much more efficient than the brute force approach. In this paper, we follow the lifting approach, applying it to change impact assessment.  
\vskip 0.05in
\noindent
{\bf Contributions.}
In this paper we
(1) outline a methodology for lifting safety analyses to safety cases of software product lines, and present a generic infrastructure for certified lifting (data structures and correctness criteria) using the Lean interactive theorem prover; and
(2) demonstrate our methodology on a CIA algorithm lifted to software product lines, i.e., supporting the input of feature-specific modifications, and outputting feature-specific annotations of safety case elements. In addition,
(3) we formalize the single-configuration CIA algorithm from~\cite{Kokaly:2017} using Lean;
(4) we outline a sketch of the correctness proof of the lifted CIA algorithm with respect to the single-configuration one (full Lean proof available online); and
(5) we discuss extending MMINT-A~\cite{Fung:2018} model management framework with lifted safety algorithms, including lifted CIA.

\vskip 0.05in
\noindent
{\bf Organization.}
The rest of this paper is organized as follows:
In Sec.~\ref{sec:background}, we provide background on safety cases and SPLs. 
We outline the correctness criteria, methodology, and infrastructure needed to formally lift safety case algorithms in Sec.~\ref{sec:methodology}.
In Sec.~\ref{sec:cia} we formalize the original single-configuration CIA algorithm, its lifted counterpart, and outline the lifting correctness proof.
%In Sec.~\ref{sec:lifting}, , and apply them to lifting the CIA algorithm. Sec.~\ref{sec:examples} illustrates the lifted CIA algorithm on a few examples, 
Sec.~\ref{sec:impl} explains how lifted algorithms can be integrated into existing model management tools.  Sec.~\ref{sec:related} compares our approach to related work, and Sec.~\ref{sec:conclusion} concludes.
