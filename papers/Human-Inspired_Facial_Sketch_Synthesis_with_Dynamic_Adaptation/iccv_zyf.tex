\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{bm}
\usepackage{subcaption} 
\usepackage[accsupp]{axessibility}  % Improves PDF readability for those with disabilities.

%\usepackage{subfig}
% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\iccvfinalcopy % *** Uncomment this line for the final submission

\def\iccvPaperID{11934} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ificcvfinal\pagestyle{empty}\fi

\begin{document}

%%%%%%%%% TITLE
%\title{Depth-Informed and Style-Controllable Facial Sketch Synthesis via Dynamic Adaptation}
\title{Human-Inspired Facial Sketch Synthesis with Dynamic Adaptation}
%\title{Human-Inspired Facial Sketch Synthesis with \\ Informative and Dynamic Adaptation}

%\author{Fei Gao\\
%Xidian University\\
%Xi'an 710126, China\\
%{\tt\small fgao@xidian.edu.cn}
%% For a paper whose authors are all at the same institution,
%% omit the following lines up until the closing ``}''.
%% Additional authors and addresses can be added with ``\and'',
%% just like the second author.
%% To save space, use either the email address or home page, not both
%\and
%Yifan Zhu, Chang Jiang\\
%Hangzhou Dianzi University\\
%Hangzhou 310018, China\\
%{\tt\small  2961695289@qq.com, jc233@hdu.edu.cn}
%\and
%Nannan Wang \footnote{Corresponding author.} \\
%Xidian University\\
%Xi'an 710126, China\\
%{\tt\small nnwang@xidian.edu.cn}
%}

\author{
	Fei Gao$^1$, 
	Yifan Zhu$^2$, 
	Chang Jiang$^2$, 
	Nannan Wang$^{3*}$ \\
	$^1$Hangzhou Institute of Technology, Xidian University 
	$^2$Hangzhou Dianzi University
	$^3$Xidian University  \\
	{\tt\small fgao@xidian.edu.cn, 
	2961695289@qq.com, 
	jc233@hdu.edu.cn,
	nnwang@xidian.edu.cn }
}

%\author{Fei Gao\\
%Hangzhou Institute of Technology, Xidian University\\
%Xian 710126, China\\
%{\tt\small fgao@xidian.edu.cn}
%% For a paper whose authors are all at the same institution,
%% omit the following lines up until the closing ``}''.
%% Additional authors and addresses can be added with ``\and'',
%% just like the second author.
%% To save space, use either the email address or home page, not both
%\and
%Yifan Zhu\\
%School of Computer Science, Hangzhou Dianzi University\\
%Hangzhou 310018, China\\
%{\tt\small secondauthor@i2.org}
%\and
%Chang Jiang\\
%School of Computer Science, Hangzhou Dianzi University\\
%Hangzhou 310018, China\\
%{\tt\small jc233@hdu.edu.cn}
%\and
%Nannan Wang\\
%ISN State Key Laboratory, Xidian University\\
%Xian 710126, China\\
%{\tt\small nnwang@xidian.edu.cn}
%}

\maketitle
% Remove page # from the first page of camera-ready.
\ificcvfinal\thispagestyle{empty}\fi

{
	\let\thefootnote\relax\footnotetext{* Corresponding Author}
	%\blfootnote{*Corresponding Author}
}

%%%%%%%%% ABSTRACT
\begin{abstract}
Facial sketch synthesis (FSS) aims to generate a vivid sketch portrait from a given facial photo. Existing FSS methods merely rely on 2D representations of facial semantic or appearance. However, professional human artists usually use outlines or shadings to covey 3D geometry. Thus facial 3D geometry (e.g. depth map) is extremely important for FSS. Besides, different artists may use diverse drawing techniques and create multiple styles of sketches; but the style is globally consistent in a sketch. Inspired by such observations, in this paper, we propose a novel \textit{Human-Inspired Dynamic Adaptation} (HIDA) method. Specially, we propose to dynamically modulate neuron activations based on a joint consideration of both facial 3D geometry and 2D appearance, as well as globally consistent style control. Besides, we use deformable convolutions at coarse-scales to align deep features, for generating abstract and distinct outlines. Experiments show that HIDA can generate high-quality sketches in multiple styles, and significantly outperforms previous methods, over a large range of challenging faces. Besides, HIDA allows precise style control of the synthesized sketch, and generalizes well to natural scenes and other artistic styles. Our code and results have been released online at: \url{https://github.com/AiArt-HDU/HIDA}. 
\end{abstract}



%%%%%%%%% BODY TEXT
\section{Introduction}
\label{sec:intro}

Making computers create arts like human beings, is a longstanding and challenging topic, in the artificial intelligence (AI) area \cite{artai22}. 
To this end, researchers have made great efforts and proposed numerous methods, such as neural style transfer (NST) \cite{Huang2017AdaIN} and image-to-image translation (I2IT) \cite{Isola2017Pix2Pix, Zhu2017CycleGAN}. These methods mainly tackle cluttered image styles, such as oil paintings \cite{Huang2017AdaIN}. In this paper, we are interested in creating artistic sketches from facial photos, which is referred to as face sketch synthesis (FSS) \cite{Wang2013Transductive}. 
%FSS has a wide range of applications in areas of social security \cite{pang2018cross} and digital entertainment \cite{li2020sketchman}. 

%For now, great efforts have been made and a huge number of methods have been proposed. 
For now, there has been significant progress in FSS inspired by the excellent success of Generative Adversarial Networks (GANs) \cite{Isola2017Pix2Pix}. 
Specially, researchers have proposed various techniques, including embedding image prior \cite{Zhang2018IJCAI}, semi-supervised learning \cite{Chen2018Semi}, self-attention/transformer based methods \cite{gao2020incremental, zhu2021sketch,duan2020multi}, hierarchical GANs \cite{Peng2019DeepPGM, Zhang2019TIP, Fan2021FS2K}, composition assistance \cite{gao2020cagan}, and semantic adaptive normalization \cite{li2021genre}, to boost the quality of synthesized sketches. 
However, all these methods merely use 2D appearance or semantic representations of the input photo. 
They may fail to handle serious variations in appearance, such as the pose, lighting, expression, and skin color. %It is still challenging to generate high-quality sketches for faces in-the-wild.


\begin{figure}
  \includegraphics[width=\linewidth]{images/fig_teaser.pdf}
  % \vspace{-0.7cm}
  \caption{Illustration of facial photos, depth maps, multi-style facial sketches drawn by human artists \cite{Fan2021FS2K}, and the corresponding results synthesized by our method.}
  %\Description{Enjoying the baseball game from the third-base
	  % seats. Ichiro Suzuki preparing to bat.}
	  % \vspace{-0.5cm}
  \label{fig:teaser}
\end{figure}


%%To tackle this challenge, we propose a depth-informed and style-controllable (HIDA) method, inspired by how human artists draw a sketch. 
%We observe that facial 3D geometry plays a significant role in human artists' drawing process. 
%Specially, human artists mainly use abstract and deformable outlines to convey facial structure and mood.
%Besides, they use adaptive shading techniques, e.g. hatching and rending, to present depth, appearance, and lighting \cite{li2019im2pencil}. 
%%Besides, the sketchy outlines drawn by human artists are typically abstract and deformable. 
%%Besides, they usually use spatially adaptive shading techniques, e.g. hatching and rending, to covey both facial 3D geometry and 2D appearance.
%In addition, different artists may use diverse drawing methods and create multiple styles of sketches. 
%In other words, they may use divergent textures to represent the same facial area. Notably,  the style in each sketch is globally consistent. Fig. \ref{fig:teaser} shows three styles of sketches drawn by artists \cite{Fan2021FS2K}. Obviously, style1 is extremely abstract and mainly contains sketchy outlines. In contrast, style3 depicts facial 3D geometry with a lot of shading textures.   % Hence, we seek to guide generation of sketch portraits by using facial 3D geometry information with controllable styles. 

To tackle this challenge, we propose a novel method, inspired by how human artists draw a sketch. We observe that facial 3D geometry plays a significant role in human artists' drawing process. Besides, a professional human artist considers comprehensive information, including facial 3D geometry, 2D appearance, and the artistic style, to execute a sketch portrait.  
We summarize the drawing methodologies of human artists \cite{li2019im2pencil} into the following four folds:
\begin{itemize}
% \vspace{-9pt}
	\item \textbf{Local 3D geometry conveyor}: First, artists typically use abstract and deformable outlines to characterize major geometry, and use different shading methodologies, e.g. hatching, blending, and stippling, to convey local 3D structures \cite{chan2022informativedraw}. 
% \vspace{-9pt}
	\item \textbf{Local 2D appearance representation}: Second, artists may use different shading or tonal techniques to represent local 2D facial appearance, so as to depict variations in lighting, color, texture, etc.
% \vspace{-9pt}
	\item \textbf{Sketches in diverse styles}: Third, different artists may use diverse drawing methods and create multiple styles of sketches. In other words, they may use divergent textures to represent the same facial area. Fig. \ref{fig:teaser} shows three styles of sketches drawn by artists \cite{Fan2021FS2K}. Obviously, Style1 is extremely abstract and mainly contains sketchy outlines. In contrast, Style3 depicts facial 3D geometry with a lot of shading textures.
% \vspace{-8pt}
	\item \textbf{Globally consistent style}: Finally, the style of pencil-drawing is usually consistent in a single sketch. As shown in Fig. \ref{fig:teaser}, although there are distinct inter-style divergences, the style of pencil-drawing is globally consistent across different regions inside each sketch.
% \vspace{-8pt}
\end{itemize}
%It is necessary to incorporate all these information for generating quality sketch portraits.

%In this paper, we build these observations into several computation modules, and propose a novel FSS method.
%Specially, we propose modulating neuron activations dynamically and adaptively, according to the facial 3D geometry, 2D appearance, and a globally-consistent style control. 
%To this end, we 
%first, we use deformable convolutions to align deep features at coarse scales for generating abstract and distinct sketchy outlines.
%the following an adaptive normalization module and a dynamic activation function, which adaptively modulate neuron activations according to the facial depth, appearance representations, and style map. 
%Besides, we use deformable convolutions at coarse-scales to align features for generating abstract and distinct outlines. 

Inspired by these observations, we seek to guide the synthesis of sketch portraits by using comprehensive information, including facial 3D geometry and 2D appearance, as well as global style control.  
In the implementation, given a facial photo, we use the depth map to represent its 3D geometry, and use the encoding features to represent its 2D appearance. Afterwards, we combine them with a style map to dynamically modulate deep features for generating a sketch. 
Inspired by the success of SPADE \cite{Park2019GauGAN} in style control \cite{cocosnet} and the local flexibility of dynamic neural networks \cite{han2021dynamic}, we propose to dynamically modulate neuron activations, based on a joint consideration of all these information. 
Such modulation is conducted though both dynamic normalization and activation. 
Specially, we propose a novel dynamic activation function, termed Informative ACON (InfoACON), and a dynamic normalization module, termed DySPADE. 
%To this end, we a novel dynamic normalization module and a novel dynamic activation function. 
%SPADE has shown strong capacity in semantic image synthesis \cite{Park2019GauGAN} and style control \cite{cocosnet}. We thus , with the same input mask, SPADE produces similar 
%We first use the comprehensive information to modulate local features in the manner of SPADE. 
%We first propose a dynamic SPADE module, by using dynamic activation instead of fixed activation, e.g. ReLU, in  learn spatially-adaptive modulating parameters 
%First, we design a novel normalization module and a novel dynamic activation function. The normalization module learns spatially-adaptive modulating parameters to transform a neuron activation at each position . 
%The dynamic activation function automatically decides whether a neuron is active or not, based on local 3D geometry and appearance, as well as the global style. In this way, the generator 
%Facial areas with similar geometries might be presented with different appearance in the corresponding sketch.    
%Specially, 
In addition, we use deformable convolutions \cite{Dai2017Deformable} to align deep features \cite{FaPN2021ICCV} at coarse scales for generating abstract and distinct sketchy outlines. 
Initially, the dynamic adaptation and deformation simulate the flexibility and abstract process of human artists during drawing.

%1) Inspired by drawing methodologies of human artistes, we propose a depth-informed and style-controllable (HIDA) method for facial sketch synthesis (FSS). As far as we know, this is the first time of using facial 3D geometry in the FSS field. 
%2) We propose a novel normalization module, which adaptively and dynamically modulate neuron activations according to comprehensive facial information, including the facial 3D geometry, 2D appearance, and global style control. 
%3) We propose a novel dynamic activation function, which automatically decides whether a neuron is active or not, based on the comprehensive facial information.
%4) Our method can produce high-quality sketches in multiple styles over a wide range of challenging data. Beside, it outperforms previous methods both quantitatively and qualitatively. 

Based on the above mentioned contributions, we build a Human-Inspired Dynamic Adaptation (HIDA) method for FSS.
We conduct experiments on several challenging datasets, including the FS2K \cite{Fan2021FS2K}, the FFHQ \cite{Karras2018StyleGAN}, and a collection of faces in-the-wild. Our method outperforms state-of-the-art (SOTA) methods both qualitatively and quantitatively. Besides, our method allows precise style control and can produce high-quality sketches in multiple styles. Even for faces with serious variations, the synthesized sketches present realistic textures and preserve facial geometric details. In addition, extensive ablation studies demonstrate the effectiveness of the proposed dynamic and adaptive modulation techniques. Finally, our model, although trained for faces, can generate high-quality sketches for natural scenes. 

%The rest of the paper is organized as follows. We first briefly summary previous related work in section \ref{sec:related}, and then detail the proposed HIDA method in section \ref{sec:method}. Afterwards, we introduce and analyze extensive experiments in section \ref{sec:exp}. Finally, we conclude this paper in section \ref{sec:conclusion}.

%-------------------------------------------------------------------------
\section{Related Works}
\label{sec:related}
Our work is related to GANs-based FSS methods. Besides, our method is highly inspired by semantic adaptive normalization \cite{Park2019GauGAN} and dynamic activation \cite{ma2021acon}. 

\textbf{GANs-based FSS.}
\label{ssec:fss}
The latest FSS methods are typically based on GANs \cite{Isola2017Pix2Pix, Gui2020ReviewGAN}, where the mapping from a facial photo to a sketch is modeled as an image-to-image translation task \cite{Isola2017Pix2Pix}.
%Initially, Wang et al. \cite{wang2017bpgan} use Pix2Pix \cite{Isola2017Pix2Pix} to generate an primary sketch and then use back-projection to boost quality. 
%To ensure the consistency between input photos and synthesized sketches, Wang et al. \cite{wang2017multgan} introduce the cycle-consistency constraint \cite{Zhu2017CycleGAN} and use multi-scale discriminators to boost quality of details.
%Zhu et al. \cite{Zhu2019ColGAN} and Zhang et al. \cite{Zhang2019MAL} collaboratively learn the mapping from photos to sketches and that form sketches to photos, where both photos and sketches are mapped to a latent common feature space. 
%Shang et al. \cite{shang2021bridging} use line-drawings as a implicit common space, and propose a unsupervised FSS framework. 
%Yan et al. \cite{yan2021isgan} use an united identity recognition loss and a cyclic-synthesized loss to enforce the consistency between photos and synthesized sketches. Cao et al. \cite{cao2022face} propose to use full-scale identity supervision and stacked generators to boost quality of synthesized sketches. Cao et al. \cite{cao2022SDGAN} propose a self-discriminative cycle generative adversarial network (SDGAN), where the target domain and source domain are discriminated over the bottleneck layers of generators. 
%To boost performance on faces in-the-wild, researchers propose a number of techniques, including embedding image priors \cite{Zhang2018IJCAI}, semi-supervised learning \cite{Chen2018Semi}, self-attention/transformer based methods \cite{gao2020incremental, zhu2021sketch,duan2020multi}, hierarchical GANs \cite{Peng2019DeepPGM, Zhang2019TIP, Zhang2019TCYB, du2020mhgan, li2021face}. For example, to tackle the challenge of unpleasing lighting conditions, Zhang et al. \cite{Zhang2018IJCAI} add a learnable illumination enhancement module into GANs. To boost the generalization ability of FSS methods, Chen et al. \cite{Chen2018Semi} propose to train the model by using both paired and unpaired data, in a semi-supervised learning manner. 
%Inspired by the success of Transformer \cite{Vaswani2017Attention}, several works \cite{gao2020incremental, zhu2021sketch, duan2020multi} introduce self-attention to GANs to model correlations between local facial areas and thus improve quality of sketches. 
%
%In addition, researchers propose to boost FSS performance by using hierarchical GANs \cite{Peng2019DeepPGM, Zhang2019TIP, Zhang2019TCYB, du2020mhgan, li2021face}. These methods use multiple local generators to translate facial patches into sketches, and a global generator to translate the whole photo. Afterwards, the synthesized results are fused and refined by a following network. Similar architectures have been used for generating pen-drawings \cite{Ran2020APD2, YiLLR22}. These methods usually use facial landmarks to detect local regions of facial organs. 
Some latest methods use 2D semantic information to guide the generation process. For example, Yu et al. \cite{gao2020cagan} propose a stacked composition-aided GANs to boost quality of details. Inspired by the great success of spatially adaptive (de)normalization (SPADE)  \cite{Park2019GauGAN} in semantic image generation, Wang et al. \cite{zhu2021sketch} and Qi et al. \cite{qi2022biphasic} spatially modulate decoding features according to facial parsing masks. Li et al. \cite{li2021genre} propose an enhanced SPADE (eSPADE) by using both facial parsing masks and encoding features for feature modulation. 

Recently, researchers seek to solve the challenge of unconstrained faces by constructing large datasets. Fan et al. \cite{Fan2021FS2K} release a challenging FS2K dataset, which consists of multi-style sketches for faces with diverse variations. % Besides, they use hierarchical GANs and expand style vectors to intermediate features for generating multi-style sketches. 
Nie et al. \cite{nie2021unconstrained} propose a novel WildSketch dataset and a Perception-Adaptive Network (PANet). In PANet, deformable feature alignment (DFA) and patch-level adaptive convolution are used. Different from \cite{nie2021unconstrained}, we analyze the effects of DFA, and only use DFA over the coarse scales. Besides, we propose to dynamically modulate neuron activations based on facial depth and artistic style.

%Recently, researchers seek to solve the challenge of unconstrained faces by constructing large datasets. Fan et al. \cite{Fan2021FS2K} release a challenging FS2K dataset, which consists of multi-style sketches for faces with diverse variations. Besides, they use hierarchical GANs and expand style vectors to intermediate features for generating multi-style sketches. Nie et al. \cite{nie2021unconstrained} propose a novel WildSketch dataset and a Perception-Adaptive Network (PANet). In PANet, deformable feature alignment (DFA) and patch-level adaptive convolution are used to boost performance on faces under unconstrained conditions. Different from \cite{nie2021unconstrained}, we analyze the effects of DFA on synthesized sketches, and only use DFA over the coarse scales. Besides, we propose to dynamically modulate neuron activations based on facial depth and artistic style.

%Existing methods all neglect correlations between sketch textures and 3D facial geometry. In this paper, we seek to character such correlations and mimic the drawing methodology of human artists. 

\textbf{Semantic Adaptive Normalization.} 
\label{ssec:spade}
Recently, Park et al. \cite{Park2019GauGAN} propose to modulate deep features based on semantic layouts for semantic image synthesis. In SPADE, deep features are modulated based on semantic layouts. Afterwards, Zhu et al. \cite{zhu2020sean} propose Semantic Region-Adaptive Normalization (SEAN) to control the style of each semantic region individually. 
% Similarly, Lv et al. \cite{lv2021learning} propose semantic-activation normalization, where modulating parameters are learned from style features.
To boost the efficiency of SPADE, Tan et al. \cite{tan2021diverse} propose a Class-Adaptive (DE)Normalization (CLADE) layer by replacing the modulation networks with class-level modulating parameters. 
All these adaptive normalization layers use 2D semantic maps and show amazing performance in generating photo-realistic images \cite{lv2021learning} and face sketches \cite{gao2020cagan}. 
%Hu et al. \cite{hu2020facesr3d} also propose to use a rendered 3D face image, instead of facial parsing masks, in SPADE for face image super-resolution reconstruction. 
%In contrast, we use comprehensive facial information, including 3D geometry, appearance representations, and a global style control map, to learn the modulating parameters. 
In this paper, we use pix-wise dynamic activation in the normalization block, so that the modulating parameters would flexibly adapt to local information. 
Experimental results show that the dynamic normalization are essential for detailed synthesis of facial sketches.  % depth map directly to modulate the photo-to-sketch translation process. Besides, we use encoding features of an input photo to automatically modify potential errors in depth maps, and use style codes to control styles of facial sketches. Thus our adaptive modulation layer automatically learn spatially-adaptive transformations based on facial 3D geometry, appearance, and global style type. 



\begin{figure*}[ht]
	\centering
%	\subfloat[a][]{\includegraphics[width=0.56\linewidth]{images/fig_pipeline.pdf} \label{fig:pipeline}} 
%	\subfloat[b][]{\includegraphics[width=0.44\linewidth]{images/fig_ida.pdf} \label{fig:DySPADE}} 
	\includegraphics[width=1\linewidth]{images/fig_pipelines.pdf} 
	% \vspace{-20pt}
	\caption{Pipeline of the proposed \textit{Human-Inspired Dynamic Adaptation} (HIDA) method for facial sketch synthesis. (a) The overall generator architecture, (b) an decoding layer with DySPADE, InfoACON, and DOG.}
	\label{fig:pipeline}
	% \vspace{-0.2cm}
\end{figure*}



\textbf{Dynamic Activations.}
%There have been various activation functions, such as Rectified Linear Unit (ReLU) \cite{hahnloser2000relu}, leaky ReLU \cite{maas2013leakyrelu}, Maxout \cite{goodfellow2013maxout}, and Swish \cite{swish}. 
Recently, Chen et al. \cite{dyrelu} propose a Dynamic ReLU (DY-ReLU) function, where parameters in Leaky ReLU are learned from all input elements. Ma et al. \cite{ma2021acon} propose a costumed activation function, termed ACON, which automatically decides whether a neuron is active or not. ACON has several variants, among which the pixel-wise version of metaACON shows remarkable performance. Give a neuron activation $x$, the output of metaACON is formulated as:
\begin{equation}
	y = (p_1 - p_2) \cdot \sigma(\theta(p_1 - p_2)x) + p_2x,
	\label{eq:acon} 
\end{equation}
where $\theta = \sigma(x)$, $\sigma$ is a Sigmoid function, $p_1$, and $p_2$ are learnable parameters.  
In this paper, we use metaACON, instead of ReLU or Leaky ReLU, in part of our networks. Besides, we propose to learn spatially-adaptive parameter $\theta$ according to the 3D geometry, 2D appearance, and global style control. Our activation function proves boosting the performance and allowing precise style control. 


\section{The Proposed}
\label{sec:method}

%\subsection{Overview}
%\label{ssec:overview}

We aim to translate a facial photo $\mathbf{X}$ to a sketch $\mathbf{Y}_s$, in style $s$, drawn by an artist. Here $s=1,2,...,S$ is a style label, $S$ is the total number of styles. %In practice, we denote the style in a one-hot vector.  
In this work, we seek to guide the synthesis of sketch portraits by using comprehensive facial information, including both 3D geometry and 2D appearance, as well as the global style control. 
Given a facial photo, we use the corresponding depth map $\mathbf{D}$ to represent its 3D geometry. Afterwards, we combine them with a global style map $\mathbf{S}$ to decode a facial sketch. In this way, our goal is formulated as learning a mapping from $\{\mathbf{X}, \mathbf{D}, \mathbf{S}\}$ to $Y_s$, i.e. $G: \{\mathbf{X}, \mathbf{D}, \mathbf{S}\} \mapsto \mathbf{Y}_s$. 


To supervise our model, it is necessary to obtain depth maps for input facial photos. However, it is usually impossible to obtain ground truth depth information in practical applications. 
%Fortunately, recent methods are very successful at producing depth maps for facial photos \cite{DECA}. 
Therefore we use state-of-the-art (SOTA) depth prediction methods to estimate the depth map of an input facial photo. In practice, we use 3DDFA \cite{3DDFA} as the depth predictor, because it has been widely used and shown excellent performance in various 3D face reconstruction tasks. 

The overall pipeline of our model is as shown in Fig. \ref{fig:pipeline}. It contains an off-line facial depth predictor $P$, a generator $G$, and a patch-wise discriminator $D$. 
% The generator $G$ follows the U-Net architecture \cite{Isola2017Pix2Pix}. It consists of an encoder and an decoder. The encoder transfer both $\mathbf{X}$ and $\mathbf{D}$ to intermediate features $E^l$, where $l=1,2,...,L$ indicates the $l^{th}$ layer. Afterwards, the decoder produces a sketch, $\hat{\mathbf{Y}}_s = G(\mathbf{X},\mathbf{D},\mathbf{S})$. 
In addition to a facial sketch, we enforce $G$ to reconstruct the input depth map $\mathbf{D}$ from features representing the sketch. In this way, the generated sketch $\hat{\mathbf{Y}}_s$ would convey the 3D geometry of $\mathbf{X}$. 
%This constraint conveys the drawing methodology of human artists, since artists usually use different shading methods to represent 3D shape.
Besides, we boost the capacity of generator by using a dynamic normalization module and a  dynamic activation function. 
Finally, to formulate the abstraction methodology of human artists in drawing sketchy outlines, we propose using deformable convolutions to align features at coarse scales. Details will be introduced bellow. 


\subsection{Informative and Dynamic Adaptation (IDA)}
%\subsection{Depth-and-Style Adaptive Normalization}
\label{ssec:IDA}

%We observe that human artists choose adaptive drawing techniques according to the facial depth and artistic style. Inspired by such observation, we 
To simulate the drawing methodology of human artist, we first propose a novel \emph{Informative and Dynamic Adaptation (IDA)} module, to modulate deep features based on a combination of the facial depth map $\mathbf{D}$, the style map $\mathbf{S}$, and the appearance representations $\mathbf{A}$, i.e. $\{ \mathbf{D}, \mathbf{S}, \mathbf{A} \}$. Specially, we propose a novel dynamic activation function, termed Informative ACON (InfoACON), and a dynamic normalization module, termed DySPADE. 

%Specially, we learn adapting parameters from a combination of the facial depth map $\mathbf{D}$, the style map $\mathbf{S}$, and the appearance representations $\mathbf{A}$, i.e. $\{ \mathbf{D}, \mathbf{S}, \mathbf{A} \}$. 

%To avoid confusion, we refer to our activation method as \texttt{InfoACON}. 

 


\textbf{Informative ACON (InfoACON).}
\label{ssec:dsaa}
The original metaACON function automatically allows whether a neuron is active or not, based on its value, as previously presented in Eq. \ref{eq:acon}. 
During the drawing process, a human artist typically decides whether to draw a stroke or not based on the 3D geometry, 2D appearance, and style type. Inspired by this observation, we propose to learn the parameter $\theta$ in Eq. \ref{eq:acon} from $\{ \mathbf{D}, \mathbf{S}, \mathbf{A} \}$, i.e.
\begin{equation}
		\theta = \sigma ( \phi_\theta(\mathrm{Cat}( \mathbf{D}, \mathbf{S}, \mathbf{A})),
\end{equation}
where $\phi_\theta$ is a two-layer Convolutional network (Fig. \ref{eq:DySPADE}). % of architecture Conv-metaACON-Conv. 
%\begin{equation}
%	\begin{aligned}
%		\theta & = \sigma ( \phi(\mathrm{Cat}( \mathbf{D}, \mathbf{S}, \mathbf{A})), \\
%		y & = (p_1 - p_2) \cdot \sigma(\theta \cdot (p_1 - p_2)x) + p_2x, 
%	\end{aligned}
%\end{equation}
%where $x$ denotes a neuron activation, $y$ is the output, $\phi$ is a shallow network of architecture \texttt{Conv-metaACON-Conv}. 
We refer to the modified metaACON function as \textit{Informative ACON} (InfoACON). In our networks, we apply this InfoACON function in all the decoding layers. In this way, the decoder would pixel-wisely decides whether to depict a stroke, or the type of a stroke, in a generated sketch. 


% In eSPADE, the activation is normalized in the instance-wise and channel-wise manner \cite{ulyanov2017IN}, and then modulated with learned scale and bias. 

\textbf{Dynamic Normalization (DySPADE).}
%Inspired by the capacity of SPADE in controlling both structure and style in image generation, we also use $\{ \mathbf{D}, \mathbf{S}, \mathbf{A} \}$ to modulate 
Following \cite{Park2019GauGAN}, we additionally transform neuron activations by shifting the mean values and scaling the standard deviations, in the instance-wise and channel-wise manner \cite{ulyanov2017IN}.  Different from the original SPADE, we use dynamic activation here to introduce more flexibility on the learned modulating parameters.
%Specially, we learn modulating parameters from $\{ \mathbf{D}, \mathbf{S}, \mathbf{A} \}$. 
Let $\mathbf{F} \in \mathbb{R}^{C \times H \times W}$ denote the input features of the current DySPADE module. $H$, $W$, and $C$ are the height, width and the number of channels. The activation value at site $(c,h,w)$ is modulated as: 
\begin{equation}
	\tilde{f}_{c,h,w} = \gamma_{c,h,w}(\mathbf{D}, \mathbf{S},\mathbf{A}) \frac{f_{c,h,w}-\mu_{c}}{\sigma_{c}} + \beta_{c,h,w}(\mathbf{D}, \mathbf{S},\mathbf{A}),
	\label{eq:DySPADE}
\end{equation}
where $f_{c,h,w}$ and $\tilde{f}_{c,h,w}$ are the input and modulated activation at site $(c, h, w)$, respectively. $\mu_{c}$ and $\sigma_{c}$ are the mean and standard deviation of $f_{c,h,w}$ in the $c$-th channel. 
%\begin{equation}
%	\begin{aligned}
%		\mu_c & = \frac{1}{WH} \sum_{w,h} f_{c,h,w}, \\
%		\sigma_c & = \frac{1}{WH} \sum_{w,h} ( f_{c,h,w} - \mu_c )^2.
%	\end{aligned}
%	\label{eq:in}
%\end{equation}
%\begin{equation}
%		\mu_c = \frac{1}{WH} \sum_{w,h} f_{c,h,w}, 
%		\sigma_c = \frac{1}{WH} \sum_{w,h} ( f_{c,h,w} - \mu_c )^2.
%	\label{eq:in}
%\end{equation}
$\gamma_{c,h,w}(\mathbf{D}, \mathbf{S},\mathbf{A})$ and $\beta_{c,h,w}(\mathbf{D}, \mathbf{S},\mathbf{A})$ are learned scale and bias parameters at site $(c,h,w)$. 
%In the implementation, we learn these parameters through a two-layer branched convolutional network \cite{Park2019GauGAN} from a concatenation of $\{ \mathbf{D}, \mathbf{S}, \mathbf{A} \}$. %Different from the original SPADE, we 
%Specially, we concatenate $\{ \mathbf{D}, \mathbf{S}, \mathbf{A} \}$, and first use a Convolutional layer with an activation layer, and then use two branch Convolutional layers to produce $\bm{\gamma}$ and $\bm{\beta}$, respectively. 
%We formulate this process as:
%\begin{equation}
%	\begin{aligned}
%		\bm{\gamma} & = \mathrm{Conv} (\mathrm{metaACON} ( \mathrm{Conv} ( \mathrm{Cat}( \mathbf{D}, \mathbf{S}, \mathbf{A})) )), \\
%		\bm{\beta} & = \mathrm{Conv} ( \mathrm{metaACON} ( \mathrm{Conv} (\mathrm{Cat}( \mathbf{D}, \mathbf{S}, \mathbf{A}))) ).
%	\end{aligned}
%	\label{eq:gambet}
%\end{equation}


%Correspondingly, the DySPADE operation shown in Eq. \ref{eq:DySPADE} can be reformulated in terms of matrices:
%\begin{equation}
%	\tilde{\mathbf{F}} = \mathrm{DySPADE}(\mathbf{F}) = \mathrm{IN} (\mathbf{F}) \otimes \bm{\gamma} \oplus \bm{\beta}, 
%	\label{eq:IDNmat}
%\end{equation}
%where $\mathrm{IN}$ denotes Instance Normalization (IN); $\otimes$ and $\oplus$ denote element-wise product and addition, respectively.


%Specially, there are three learnable parameters in metaACON, i.e. $\theta$, $p_1$, and $p_2$. $\theta$ is learned from the input features of metaACON. The output of metaACON is formulated as:
%\begin{equation}
%y = (p_1 - p_2) \cdot \sigma(\theta(p_1 - p_2)x) + p_2x, 
%\end{equation}
%where $x$ denotes a neuron activation, $y$ is the output of InfoACON. 

 
As shown in Fig. \ref{fig:pipeline}, we use a two-layer and three-branched Convolutional network to predict the parameter $\theta$ in InfoACON, and the modulating parameters $\bm{\gamma}$ and $\bm{\beta}$ in DySPADE. To improve the flexibility of the adaptation block, we use metaACON (Eq. \ref{eq:acon}) \cite{ma2021acon} instead of ReLU, after the first Convolutional layer. In this way, the modulating factors would pixel-wisely adapt to an integration of the facial 3D geometry, 2D appearance, and global style.  

In IDA, the activation at each position is modulated according to a joint consideration of local facial 3D geometry, appearance, and artistic style. This mechanism is consistent with the drawing methodology of human artists. To execute a facial sketch, an artist usually uses diverse textures to represent 3D geometry or illustration variations. Besides, the style of all pencil strokes are consistent inside a single sketch. As a result, IDA is promising to produce realistic sketchy textures in globally consistent style. 



\subsection{Deformable Outline Generation (DOG)}
\label{ssec:dfa}

Human artists usually draw abstract lines to capture facial geometric structures, such as the boundaries of facial organs, and facial mood. To this end, the resulting outlines typically convey such structures abstractly, instead of pixel-wisely tracing them. In other words, there are geometric deformations between the input photo and the sketches drawn by artists. 
To simulate such an abstraction drawing methodology, we propose to align decoding features at coarse scales. In this way, the generated sketches would present abstract and distinct outlines, instead of scattered outlines with a lot of subtle variations.
 
In practice, we use deformable convolution (DCN) \cite{Dai2017Deformable} instead of standard Transposed Convolution over the first and second decoding layers. 
%Specially, we first enlarge the size of input features twice via bilinear interpolation, and then feed it into a deformable convolutional layer. In deformable convolution, operations are executed in an irregular grid, which is achieved by augmenting a regular grid, e.g. $\mathcal{R}=\{(-1,-1),(-1,0),...,(1,1)\}$ with offsets, $\{ \Delta \mathbf{p}_n | n = 1,...,9 \}$. For each location $\mathbf{p}_0$ on the output feature map $\mathbf{y}$, we have
%\begin{equation}
%	\mathbf{y}(\mathbf{p}_0) = \sum_{\mathbf{p}_n \in \mathcal{R}} \mathbf{w}(\mathbf{p}_n) \cdot \mathbf{x}(\mathbf{p}_0+\mathbf{p}_n+\Delta \mathbf{p}_n).
%	\label{eq:dfmconv}
%\end{equation}
%In other words, the convolution at each site $\mathbf{p}_0$ is operated on irregular and offset locations $\mathbf{p}_0 + \Delta \mathbf{p}_n$.
As will be presented in the ablation study (Section \ref{ssec:exp_ablation}), this deformable outline generation (DOG) module significantly boosts the clarity of generated outlines. Besides, DOG enables the network produce abstract sketches (e.g. Style1 in the FS2K dataset), which contains a sparse set of sketchy line drawings.  


\subsection{Overall Generator Architecture}
\label{ssec:gen}

Our generator follows the U-Net architecture \cite{Isola2017Pix2Pix} in whole. 
%Assume there are totally $L$ encoding layers and $L$ decoding layers, $l=1,2,...,L$. Let $\mathbf{E}^{l} \in \mathbb{R}^{C_{en}^l \times H_{en}^l \times W_{en}^l}$ and $\mathbf{K}^{l} \in \mathbb{R}^{C_{de}^l \times H_{de}^l \times W_{de}^l}$ denote the initial outputs of the $l$-th encoding layer and decoding layer, respectively. $C_*$, $H_*$, and $W_*$ are the number of channels, height, and width, correspondingly. 
%Following the U-Net architecture, we transport output of the $(n-l)$-th encoding layer, i.e. $\mathbf{E}^{n-l}$, to the $l$-th decoding layer. $\mathbf{E}^{n-l}$ and $\mathbf{K}^{l-1}$ are concatenated and fed into the $l$-th decoding layer.
%\textbf{Appearance Encoder.}
In the encoder, the facial photo $\mathbf{X}$ and the depth map $\mathbf{D}$ are first fed into a Convolutional layer, separately. Afterwards, the corresponding feature maps are concatenated and fed into the following encoding layers. Each encoding layer follows a Conv-metaReLU-IN architecture, and down-samples the size of feature maps by 1/2. The encoding features are adopted as appearance representations, $\mathbf{A}$.

%\begin{figure}
%	\begin{center}
%		\includegraphics[width=1\linewidth]{images/fig_ida.pdf}
%	\end{center}
%	 % \vspace{-0.3cm}
%	\caption{Pipeline of an decoding layer with DySPADE, InfoACON, and DOG.}
%	\label{fig:DySPADE}
%	 % \vspace{-0.4cm}
%\end{figure}



%\textbf{Sketch Decoder with DySPADE.}
In the decoder, we expand an DySPADE block to every decoding layer, except the last one. 
Fig. \ref{fig:pipeline} illustrates the pipeline of a decoding layer with DySPADE. 
Over the $l$-th decoding layer, let $\mathbf{D}^{l}$ be the corresponding depth map, $\mathbf{S}^{l}$ the style map, and $\mathbf{A}^{l}$ the appearance features.
We down-sample the original depth map $\mathbf{D}$ to $\mathbf{D}^{l}$ by building a Gaussian Pyramid, and expand the one-hot style vector $\mathbf{s}$ to $\mathbf{S}^{l}$. 
%To obtain $\mathbf{D}^{l}$, we down-sample the original depth map $\mathbf{D}$ to the size of $H_{de}^l \times W_{de}^l$ by building a Gaussian Pyramid. 
%Besides, we expand the one-hot style vector $\mathbf{s}$ to the size of $3 \times H_{en}^l \times W_{en}^l$. The resulting style map is $\mathbf{S}^{l}$.
Besides, we obtain $\mathbf{A}^l$ by upsampling $\mathbf{E}^{l-1}$ through a Transposed-Convlotional (TrConv) layer, followed by a metaACON activation layer.
%, i.e. 
%\begin{equation}
%	\mathbf{A}^l = \mathrm{metaACON} (\mathrm{TrConv}(\mathbf{E}^{n-l})) ).
%\end{equation}
We finally apply the residual connection to obtain the output of the $l$-th decoding layer: 
%$\mathbf{K}^{l} = \mathbf{H}^l \oplus \tilde{\mathbf{F}}^l$, with $\tilde{\mathbf{F}}^l = \mathrm{DySPADE}(\mathbf{F}^l)$,
\begin{equation}
	\mathbf{K}^{l} = \mathbf{H}^l \oplus \tilde{\mathbf{F}}^l, \text{~with~} \tilde{\mathbf{F}}^l = \mathrm{DySPADE}(\mathbf{F}^l), 
\end{equation}
where $\oplus$ denotes element-wise addition. 
$\mathbf{H}^l$ is the initial upsampled feature map, output by a DOG layer (over the $1^{st}$ and $2^{nd}$ decoding layer) or a TrConv layer (over the rest layers).
$\mathbf{K}^{l}$ is fed into subsequent layers for generating final predictions.

\subsection{Loss Functions}
\label{ssec:loss}
To train our model, we use the following loss functions. 

\textbf{Geometric loss.}
First, we use a geometric constraint to supervise depth reconstructions from features of sketches.
The geometric loss is the L2 distance between the input depth map and the reconstructed one: 
%$\mathcal{L}_{geo} = \Vert \hat{\mathbf{D}} - \mathbf{D} \Vert_2^2$.
\begin{equation}
	\mathcal{L}_{geo} = \Vert \hat{\mathbf{D}} - \mathbf{D} \Vert_2^2.
\end{equation}
% In this way, the synthesized sketch $\hat{\mathbf{Y}}_s$ would present the same 3D geometry of the input facial photo $\mathbf{X}$. This constraint conveys the drawing methodology of human artists, since artists usually use different shading methods to effectively convey 3D shape. This geometric loss would make the synthesised textures effectively representing local 3D geometries and 2D appearances. Besides, in stead of using a cycle consistency, i.e. reconstructing the input photo from the synthesised sketch, the geometric loss obstacle the domain gaps between photos and sketches \cite{chan2022drawings}.

\textbf{Textural loss.}
The synthesized sketch $\hat{\mathbf{Y}}_s$ should present similar textures as that drawn by an artist $\mathbf{Y}_s$. In this work, we constrain $\hat{\mathbf{Y}}_s$ and $\mathbf{Y}_s$ to have similar pixel-wise adjacent correlations \cite{li2021genre}. To this end, we calculate their gradients by using the Sobel operator, and calculate the average Cosine distance between them. Let $\mathbf{g}_{i,j} = [g^x_{i,j}, g^y_{i,j}]^T$ denote the $x$-directional and $y$-directional gradients of $\mathbf{Y}_s$ at site $(i,j)$; and $\mathbf{f}_{i,j} = [f^x_{i,j}, f^y_{i,j}]^T$ the corresponding gradients in $\hat{\mathbf{Y}}_s$. 
The textural loss is formulated as: 
%$\mathcal{L}_{tex} = \frac{1}{MN} \sum_{i,j}  \frac{\mathbf{g}_{i,j}^T \mathbf{f}_{i,j}}{\Vert \mathbf{g}_{i,j} \Vert \cdot \Vert \mathbf{f}_{i,j} \Vert}$, 
%The cosine distance between them is calculated by 
\begin{equation}
	\mathcal{L}_{tex} = \frac{1}{MN} \sum_{i,j}  \frac{\mathbf{g}_{i,j}^T \mathbf{f}_{i,j}}{\Vert \mathbf{g}_{i,j} \Vert \cdot \Vert \mathbf{f}_{i,j} \Vert},
\end{equation}
where $\Vert \cdot \Vert$ denotes the magnitude of a vector, $M$ and $N$ are the width and height of the sketch.
%The textural loss is formulated as:
%\begin{equation}
%	\mathcal{L}_{tex} = \frac{1}{MN} \sum_{i,j} \ell_{tex, i, j},
%\end{equation}
%where $M$ and $N$ are the width and height of the sketch.


\textbf{Pixel loss.}
In addition, we use the pixel-wise reconstruction loss between the synthesized sketch $\hat{\mathbf{Y}}_s$ and the target sketch $\mathbf{Y}_s$, i.e. % $\mathcal{L}_{pix} = \Vert \hat{\mathbf{Y}_s} - \mathbf{Y}_s \Vert_1$.
\begin{equation}
	\mathcal{L}_{pix} = \Vert \hat{\mathbf{Y}_s} - \mathbf{Y}_s \Vert_1.
\end{equation}

\textbf{Adversarial loss.}
Finally, we use adversarial loss to measure whether a pair of depth map and synthesized sketch is real or fake. Here, we use the Cross Entropy loss, i.e. 
%$\mathcal{L}_{adv} = -\log D(\mathbf{D}, \mathbf{Y}_s) 
%	- \log (1-D(\hat{\mathbf{D}}, \hat{\mathbf{Y}}_s))$.
\begin{equation}
	\mathcal{L}_{adv} = -\log D(\mathbf{D}, \mathbf{Y}_s) 
	- \log (1-D(\hat{\mathbf{D}}, \hat{\mathbf{Y}}_s)).
\end{equation}

\textbf{Full objective.}
We use a combination of all the aforementioned losses as our full objective:
\begin{equation}
	\mathcal{L}_{all} = \mathcal{L}_{adv} + \lambda_1 \mathcal{L}_{pix} + \lambda_2 \mathcal{L}_{tex} + \lambda_3 \mathcal{L}_{geo},
\end{equation}
where $\lambda_1$, $\lambda_2$, and $\lambda_3$ are weighting factors. We train the generator $G$ and the discriminator $D$ in an alternative manner, to minimize $\mathcal{L}_{all}$. 

%Note that, we don't use a style classification constraint in the training process. In previous works, such as FSGAN \cite{Fan2021FS2K} and UPDG \cite{YiLLR22}, a style classifier is extended to the discriminator to distinguish styles of synthesized portraits. Correspondingly, a style classification loss is used to optimize both the generator and the discriminator. We preliminarily train MobileNetv3 \cite{mobilenetv3}, VGG19 \cite{vgg}, and our discriminator, for classifying three sketch styles in the FS2K dataset \cite{Fan2021FS2K}, respectively. However, all these networks fail to distinguish Style2 with the other two styles. Besides, when we use a style classification loss in our model, the generator fails to produce pencil-drawing sketches. Inspiringly, without such a style classification module, our model can generate high-quality sketches and allow precise control of the style (Section \ref{ssec:exp_robust}). % The corresponding results will be presented in Section \ref{ssec:exp_style}.

% ---------------------------------------------------- %
\section{Experiments}
\label{sec:exp}
We present a thorough experimental comparison on the challenging FS2K dataset \cite{Fan2021FS2K}. Besides, we conduct a series of ablation study to analyse impacts of the proposed DySPADE, InfoACON, and DOG modules. 



\begin{figure*}
	\centering
	\includegraphics[width=1\linewidth]{images/exp_comp.pdf}
%	% \vspace{-0.2cm}
	\caption{Comparison with SOTAs on the FS2K dataset.}
	\label{fig:fs2k}
%	% \vspace{-0.3cm}
\end{figure*}

\subsection{Experimental Settings}
\label{ssec:exp_setting}

\textbf{Data.}
\label{ssec:exp_data}
We conduct experiments on the challenging FS2K dataset. The FS2K dataset is the largest publicly released FSS dataset, consisting of 2,104 photo-sketch pairs from a wide range of image backgrounds, skin colors, sketch styles, and lighting conditions. These sketches are mainly in three styles. Following standard settings \cite{Fan2021FS2K}, we have 1,058 photo-sketch pairs for training, and 1,046 pairs for testing. For each style, we have 357/351/350 training pairs, and 619/381/46 testing pairs, from Style1 to Style3, respectively. All the images are aligned and resized to $250 \times 250$. In the inference stage, we use the same style of sketch as the ground truth in default.  
% (2) \textbf{The CUFS dataset} \cite{Wang2009Face} consists of 606 face photos and their corresponding sketches drawn by artists. Following standard settings \cite{Wang2017RSLCR}, 268 face photo-sketch pairs are used for training, and the rest 338 pairs are for testing. All the images are aligned and resized to $256 \times 256$. We treat all sketches on the CUFS dataset being in the same style. (3) 
In addition, we collect a number of challenging Faces in-the-wild from FFHQ \cite{Karras2018StyleGAN} and Web. We align and resize these images in the same way as those in the FS2K dataset. 

% \textbf{APDrawing dataset.} APDrawing dataset \cite{YiLLR19} contains 140 pairs of face photos and corresponding pen-drawings. All portrait drawings are of the same style. All images and drawings are aligned and cropped to $512 \times 512$ size. 

%\textbf{Preprocessing.} 
% All the images in the CUFS dataset are aligned and cropped to $256 \times 256$ size, relying on three points: two eye centers and the mouth center. For face photos beyond these datasets (Section \ref{ssec:exp_robust}), we align them in the same way. 

\textbf{Comparison Methods}
\label{ssec:exp_compmethod}
In this section, we compare our method with various state-of-the-art (SOTA) ones, including FSGAN \cite{Fan2021FS2K}, GENRE \cite{li2021genre}, SCA-GAN \cite{gao2020cagan}, and MDAL \cite{Zhang2019MAL}. Besides, we compare with several advanced GANs, including CycleGAN \cite{Zhu2017CycleGAN}, Pix2Pix \cite{Isola2017Pix2Pix}, and Pix2PixHD \cite{Wang2017Pix2PixHD}. We use results and codes of these methods released by the corresponding authors \cite{Fan2021FS2K}. All these methods and ours follow the same experimental settings. 

	


\textbf{Criteria}
\label{ssec:exp_criteria}
In this work, we choose four performance indices as the criteria, i.e. the \textit{Fr\'{e}chet Inception distance} (FID) \cite{Heusel2017FID}, \textit{Learned Perceptual Image Patch Similarity} (LPIPS) metric \cite{zhang2018lpips}, \textit{Structure Co-Occurrence Texture} (SCOOT) metric \cite{fan2019scoot}, and \textit{Feature Similarity Measure} (FSIM) \cite{Zhang2011FSIM}.  
Lower values of FID and LPIPS indicate higher realism of synthesized sketches. In contrast, greater values of SCOOT and FSIM generally indicate higher similarity between a synthesized sketch and the corresponding sketch drawn by an artist. We here report the average LPIPS, SCOOT, and FSIM values across all the test samples, respectively. In the following sections, $\downarrow$ indicates that lower value is better, while $\uparrow$ higher is better. 





\textbf{Implementation Details}
\label{ssec:implement}
We implemented our model in PyTorch. All experiments are performed on a computer with a Titan 3090 GPU. We use a batch size of 4, a learning rate of $1e-4$. We use the Adam Optimizer, and train the model for 800 epochs on the training set. Our code will be released after peer review. % are available at: \url{https://github.com/AiArt-HDU/HIDA}.


%\begin{figure}
%	\centering
%	\includegraphics[width=1\linewidth]{images/fig_fs2k.pdf}
%	% \vspace{-0.7cm}
%	\caption{Comparison with SOTAs on the FS2K dataset.  (a) Input photo, (b) depth, (c) sketch drawn by artists, synthesized sketches by (d) HIDA (Ours), (e) Pix2Pix, (f) Pix2PixHD, (g) FSGAN, (h) SCA-GAN, (i) MDAL, (j) CycleGAN, and (k) GENRE.}
%	\label{fig:fs2k}
%	% \vspace{-0.3cm}
%\end{figure}


\subsection{Qualitative Comparison with SOTAs}
\label{ssec:exp_quali}

We further qualitatively compare with SOTA FSS methods. Fig. \ref{fig:fs2k} illustrates synthesized sketches on the FS2K dataset. Although our method can generate multiple styles of sketches, here we only show the synthesized sketch in the same style as the ground truth. 
For the face in constrained condition (the first row), most methods successfully generate a quality sketch. For the face with extreme lighting condition (the second row) or pose variation (the third row), most synthesized sketches present unpleasant geometric deformations and fail to precisely reproduce the style. 
Although sketches generated by CycleGAN seems acceptable, the textures aren't like pencil-drawings. 
The sketches generated by FSGAN show the same styles as the ground truths, since FSGAN contains a style control module. However, these sketches show unpleasant structural distortions. This might be caused by the geometric deformations between facial photos and free-hand sketches drawn by artists, in the training data. 
GENRE successfully produces quality sketches, but they are all almost in the same style, since no style information is considered in GENRE. 

% \vspace{-0.1cm}
In contrast, our HIDA generates high-quality sketches in all three styles. Specially, our synthesized sketches preserve the geometries of input faces. This implies that, HIDA doesn't overfit to the training samples and combats geometric deformations. We achieve such success mainly due to the informatively adaptive normalization module, i.e. DySPADE, and the constraint of reconstructing the input depth. 
Besides, our synthesized sketches present the same style of strokes as the corresponding ground truths. The drawing textures are consistent inside each sketch. The style consistency demonstrates the effectiveness of our global style control mechanism through DySPADE and InfoACON. Based on all these observations, we conclude that our HIDA model can generate high-quality and style-consistent sketches.    
	

%\begin{figure*}
%\centering
%\includegraphics[width=0.9\linewidth]{images/exp_cufs.pdf}
%   % % \vspace{-0.7cm}
%\caption{Synthesized sketches on the CUFS dataset.}
%\label{fig:cufs}
%   % % \vspace{-0.3cm}
%\end{figure*}



\begin{table}
	\centering
	\caption{Comparison with SOTAs on the FS2K dataset.}
	\label{tab:pfm_fs2k}
	\renewcommand\arraystretch{1.2}
	\resizebox{0.45\textwidth}{!}{
		\begin{tabular}{l|cccc}
			\toprule
			&	FID$\downarrow$	&	LPIPS$\downarrow$	&	SCOOT$\uparrow$	&	FSIM$\uparrow$	\\
			\midrule	
			Pix2Pix \cite{Isola2017Pix2Pix}	&	\underline{18.34}	&	0.304	&	0.493	&	0.541 	\\
			Pix2PixHD \cite{Wang2017Pix2PixHD}	&	32.03	&	0.468	&	0.374	&	0.531 	\\
			CycleGAN \cite{Zhu2017CycleGAN}	&	26.49	&	0.505	&	0.348	&	0.501 	\\
			MDAL \cite{Zhang2019MAL}	&	50.18	&	0.492	&	0.355	&	0.530 	\\
			SCA-GAN \cite{gao2020cagan}	&	39.63	&	0.305	&	\textbf{0.600} 	&	\textbf{0.782} 	\\
			FSGAN \cite{Fan2021FS2K}	&	34.88	&	0.483	&	0.405	&	\underline{0.610} 	\\
			GENRE \cite{Park2019GauGAN}	&	20.67	&	\underline{0.302}	&	0.483	&	0.534 	\\
			HIDA (Ours)	&	\textbf{15.06}	&	\textbf{0.263}	&	\underline{0.575}	&	0.551 	\\
			\bottomrule
		\end{tabular}
	}
	% \vspace{-0.2cm}
\end{table}

\begin{figure}
	\centering
	\includegraphics[width=1\linewidth]{images/exp_fs2kstyle.pdf}
	% % \vspace{-0.7cm}
	\caption{FID and LPIPS values w.r.t. each style in FS2K.}
	\label{fig:fs2kstyle}
	% \vspace{-0.3cm}
\end{figure}


\subsection{Quantitative Comparison with SOTAs}
\label{ssec:exp_quanta}

\textbf{Overall Performance.}
Table \ref{tab:pfm_fs2k} shows the quantitative performance criteria of each method on the whole FS2K testing dataset. 
Obviously, our method achieves the lowest FID and LPIPS values. 
In contrast to previous benchmark method, FSGAN, our HIDA dramatically decrease both FID and LPIPS by about 20 and 0.22, respectively. 
Besides, compared to SOTA 2D-semantic driven methods, i.e. SCA-GAN and GENRE, HIDA decreases FID by about 24 and 5, respectively. HIDA also decreases LPIPS by about 0.04, i.e. 10\% relatively. 
Such dramatic decreases of both FID and LPIPS mean that our method produces the most realistic sketches in terms of style and stroke.

In addition, HIDA achieves the second best value of SCOOT, which is significantly better than FSGAN and GENRE, but slightly lower than SCA-GAN. 
Such a high value of SCOOT means that the sketches produced by our method are similar to those drawn by artists in terms of structure and textures. 
%
Finally, HIDA achieves the third best FSIM value. Recall that there are geometric deformations between facial photos and sketches drawn by artists. Thus an excessively high value of FSIM might indicate the potential that: a FSS model overfits to the training data, and cannot precisely preserve facial structures in the translation process. Correspondingly, as shown in Fig. \ref{fig:fs2k}, both SCA-GAN and FSGAN produce deformable sketches. In contrast, HIDA preserves the structure of input faces. 
%indistinct outlines over the regions of eye and mouth. In contrast, our HIDA model produces distinct outlines and precisely preserve facial structures.


\textbf{Performance on Each Style.}
We further analyse the performance of FSS methods on each style subset. Since both FID and LPIPS measure the realism of synthesized sketches in terms of style and textures, we report them in Fig. \ref{fig:fs2kstyle}. Obviously, our HIDA model consistently achieves the lowest FID and LPIPS values, across all the styles. Especially, our method significantly outperforms previous SOTA method, FSGAN, according to both criteria. Such distinct superiority over existing methods demonstrates that our method effectively learns the style information and allows precise control over the style of synthesized sketches. 

\subsection{User Study}
\label{ssec:exp_subj}

We further conduct a series of subjective study to evaluate the performance of HIDA, in contrast to existing methods. Specially, we have 10 participators, all of whom are not professional artists. For each participator, we show them 1,000 randomly selected samples from the testing set in FS2K. Each time, we show a facial photo, the corresponding sketch drawn by an artist, and 8 synthesized sketches produced by different methods. Participators are requested to choose the best sketch, according to (1) the similarity between a synthesized sketch and the ground truth, and (2) the quality of a sketch, based on their own preferences. Finally, we collect totally 10,000 preference labels. 

Fig. \ref{fig:subject} shows the average preference percent about each model, and the standard deviation among different participants. Obviously, our method dramatically outperforms all the other methods. In average, subjective participators think our model generates the best sketch over 70\% of facial photos. 
%GENRE and Pix2Pix perform almost the same in this subjective study. Besides, there are subtle divergence among different subjective observers. 
The subjective comparison result demonstrate that our method significantly outperforms SOTAs in generating high-quality and style-specific facial sketches. In addition, the sketches synthesized by our HIDA model meet the preference of most users.  


	
\begin{figure}
\centering
\includegraphics[width=1\linewidth]{images/fig_subj.pdf}
% % \vspace{-0.7cm}
\caption{Average subjective preference percent  (\textit{Avg.}) and the standard deviations among different subjects (\textit{St.D.}).}
\label{fig:subject}
% \vspace{-0.5cm}
\end{figure}

%In summary, both the qualitative and quantitative comparisons demonstrate that our HIDA model outperforms previous SOTAs in generating high quality facial sketches. Besides, HIDA can generates multi-style sketches with consistent facial structures and realistic pencil-drawing textures. 



\subsection{Ablation Study}
\label{ssec:exp_ablation}

We first conduct a series of ablation study on the FS2K dataset. To this end, we build several model variants, by gradually adding different modules to the base model, i.e. Pix2Pix \cite{Isola2017Pix2Pix}. The modules we aims to analyse include the use of depth map $\mathbf{D}$ as auxiliary input, the DySPADE transformation, the InfoACON function, and the DOG layer. 


\textbf{Qualitative Analysis.}
Fig. \ref{fig:exp_ablation} illustrates sketches produced by these model variants. 
The second column shows the depth maps predicted by 3DDFA. These maps convey well with the corresponding facial geometry in general. 
The third column shows sketches generated by the base model (i.e. Model-A). Obviously, these sketches occasionally show chaotic facial structures. Besides, there is no distinct difference between the generated two sketches in terms of style.  
In contrast, using the DySPADE module (i.e. Model-C) enables the model precisely preserving tiny facial structures. For example, the shapes of eyebrows in both examples become consistent between the synthesized sketches and the input photos. %This might because the decoding features are modulated by the appearance features and depth maps. 

If we further use the InfoACON function in the decoder (i.e. Model-D), the generator produces more details. For example, the textures precisely present the 3D structure of lips. Besides, the major boundary of eyeglass is generated. The synthesized sketches of these two examples also show different types of strokes over the same semantic regions, e.g. lips. % The possible reason for such improvement is that: InfoACON allows each neuron to activate or not according to the facial geometry, appearance, and the style. 
Finally, using DOG (i.e. the full model) enables the model generating abstract and distinct outlines. For example, the result in the top row is consistent with Style1 in terms of line drawings. All the other model variants produce obvious rendering textures to present 3D geometry. 
%Besides, the sketch of the bottom example shows consecutive boundary of the eyeglass. In contrast, Model-C produces snatchy line drawings over the same region. 
Such comparisons demonstrate our motivation of using DOG to simulate the abstraction process of human artists. 


\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{images/fig_ablation.pdf}
	% \vspace{-0.6cm}
	\caption{Comparison between different model variants in the ablation study, on the FS2K dataset.}
	\label{fig:exp_ablation}
	% \vspace{-0.2cm}
\end{figure}

\begin{table}
	\centering
	\tabcolsep=1.5pt
%	\footnotesize
	\caption{Quantitative results of the ablation study on the FS2K dataset.}
	\label{tab:ablation}
%	\renewcommand\arraystretch{1.2}
%	\resizebox{0.45\textwidth}{!}{
		\begin{tabular}{l|cccc}
			\toprule
			&	FID$\downarrow$	&	LPIPS$\downarrow$	&	SCOOT$\uparrow$	&	FSIM$\uparrow$	\\
			\midrule
			% Pix2pix	&	18.34	&	0.304	&	0.493	&	0.541	\\
			Base ~~(Model-A) 	&	18.34	&	0.304	&	0.493	&	0.541	\\
			+ Depth ~(Model-B)	&	20.1	&	0.307	&	0.489	&	\underline{0.543} \\
			+ DySPADE ~(Model-C)	&	18.30	&	0.298	&	0.479	&	0.539	\\
			+ InfoACON ~(Model-D)	&	\underline{17.41}	&	\underline{0.291}	&	\underline{0.493}	&	0.536	\\
			+ DOG (full)	&	\textbf{15.06}	&	\textbf{0.263}	&	\textbf{0.575}	&	\textbf{0.551}	\\
			\bottomrule
		\end{tabular}
%	}
	% \vspace{-0.4cm}
\end{table}


\begin{figure*}
	\centering
	\includegraphics[width=0.9\linewidth]{images/fig_vis.pdf}
	% \vspace{-0.8cm}
	\caption{Visualization of activations and adaptation parameters, w.r.t. different model variants of HIDA.}
	\label{fig:vis}
%	\vspace{-0.2cm}
\end{figure*}

\textbf{Quantitative Analysis.}
Table \ref{tab:ablation} lists the performance criteria achieved by these model variants. 
Using depth alone, although improves the geometrical structures (Fig. \ref{fig:exp_ablation}), doesn't consistently contribute to quantitative performance. 
As we gradually add the DySPADE, InfoACON, and DOG modules, both FID and LPIPS consistently decrease. At the same time, Model-C and Model-D achieve comparable SCOOT and FSIM values, in contrast to Model-A. This means that both DySPADE and InfoACON improve the realism of the generated sketch portraits, without significantly changing facial structures. Inspiringly, our full model achieves the best performance in terms of all the quantitative criteria. 

\textbf{Parameter Visualization.} 
We further analyse the impact of each module by removing it from our full model. Fig. \ref{fig:vis} visualizes activation maps and adaptation parameters w.r.t. the corresponding model variants. We can see that depth helps learning effective geometric representations (Full \textit{vs}. w/o Depth). Besides, the proposed dynamic adaptation (DySPADE and InfoACON) boosts the representations, and migrates the artefacts introduced by the incomplete depth map. 
Based on previous analysis, we conclude that our method achieves such inspiring performance, due to a combination of depth and the IDA modules.  


%\subsection{Analysis of Model Settings}

\textbf{Analysis of InfoACON.}
\label{ssec:exp_ACON}
We further analyze the impacts of dynamic activation functions, including metaACON and the proposed InfoACON. To this end, we build model variants based on Model-B, by (1) using ReLU in the encoder and LeakyReLU in the decoder and discriminator; (2) using metaACON \cite{ma2021acon} in all layers; and (3) using InfoACON in the decoder and metaACON in the other layers.  % Fig. \ref{fig:exp_ACON} and Table \ref{tab:dinfoACON} show the corresponding results. 
%
As shown in Fig. \ref{fig:exp_ACON}, 
%using ReLU/LeakyReLU produces indistinct outlines. Using metaACON leads to much better outlines, but with shading inside lips. In contrast,
InfoACON makes the generator merely produce distinct sketchy outlines over the mouth region, which is most similar to the ground truth, in terms of style. 
%Besides, according the second example, InfoACON leads to smooth and realistic blending textures inside clips. 
As shown in Table \ref{tab:dinfoACON}, InfoACON achieves the lowest FID and LPIPS, as well as highly comparable SCOOT and FSIM. 
%Besides, both metaACON and InfoACON achieve much higher SCOOT than ReLU/LeakyReLU.
Besides, both metaACON and InfoACON outperform ReLU/LeakyReLU. This means that dynamic activation significantly improves the consistency between the synthesized sketches and those drawn by human artists, in terms of textures. 
%Finally, compared to ReLU/LeakyReLU, metaACON decreases LPIPS but increases FID. In contrast, InfoACON decreases both FID and LPIPS significantly. 
%Such comparison demonstrates that InfoACON precisely controls neuron activations based on a joint consideration of local 3D geometry, 2D appearance, and global style. % This demonstrating our motivations of InfoACON. 
%% \vspace{-0.4cm}

\begin{figure}
	\centering
	\includegraphics[width=1\linewidth]{images/exp_acon.pdf}
%	% \vspace{-0.3cm}
	\caption{Comparison between activation functions.}
	\label{fig:exp_ACON}
%	% \vspace{-0.4cm}
\end{figure}

\begin{table}
	%\tabcolsep=4pt
	\centering
	\caption{Comparison between different activation functions.}
%	\footnotesize
	\label{tab:dinfoACON}
%	\renewcommand\arraystretch{1.2}
%	\resizebox{0.45\textwidth}{!}{
		\begin{tabular}{l|cccc}
			\toprule
			&	FID$\downarrow$	&	LPIPS$\downarrow$	&	SCOOT$\uparrow$	&	FSIM$\uparrow$	\\
			\midrule
			(Leaky)ReLU	&	\underline{18.30}	&	0.298	&	0.479	&	\underline{0.539}	\\
			metaACON	&	19.05	&	\underline{0.292}	&	\textbf{0.498}	&	\textbf{0.541}	\\
			InfoACON	&	\textbf{17.41}	&	\textbf{0.291}	&	\underline{0.493}	&	0.536	\\
			\bottomrule
		\end{tabular}
%		% \vspace{-0.8cm}
%	}
\end{table}



\textbf{Analysis of DOG.}
\label{ssec:exp_dfa}
In our framework, we apply deformable convolutions only at coarse-scale layers, i.e. the top 2 layers in the decoder. To verify such motivation, we conduct variants of our final model, by using DOG at top 2 layers (\textit{top}2), middle 3 layers (\textit{mid}3), bottom 2 layers (\textit{btm}2), and all layers (\textit{all}), respectively. % Fig. \ref{fig:exp_dfa} and Fig \ref{fig:exp_dfaacurve} show the corresponding results.
In this experiment, HIDA w/o DOG is the base model. %As shown in Table \ref{tab:dfa}, merely using DOG in the top 2 layers decreases FID and LPIPS by 2.3 and 0.03, respectively; both are greater than $>10\%$ relatively. Besides, \textit{top}2 significantly improves both SCOOT and FSIM. 
%Using DOG over the middle layers also slightly boosts the performance. However, using DOG over the bottom layers will hurt the performance. 
%
Fig. \ref{fig:exp_dfa} illustrates the corresponding synthesized sketches. Obviously, the sketches synthesized with DOG present distincter geometric outlines than those without DOG. 
% Using DOG over the top or middle layers enables the model generating sketches in Style1. 
If we apply DOG over the bottom layer, the model fails to generate sketches in Style1. 
This might due to the fact that human painters usually abstract in large areas rather than small ones. 
Besides, the sketch synthesized by \textit{top}2 has the most consistent style compared to the ground truth.
Using DOG over all decoding layers leads to an integrated effects on the synthesized sketch, e.g. confused styles and distinct boundaries. 
%This observation is consistent with the performance criteria shown in Table \ref{tab:dfa}. 
We therefore merely use DOG over the top 2 decoding layers in our final model. 

\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{images/exp_dog.pdf}
	%% \vspace{-0.8cm}
	\caption{Comparison between different settings of DOG.}
	\label{fig:exp_dfa}
	% \vspace{-0.3cm}
\end{figure}


%\begin{figure}
%\centering
%\includegraphics[width=\linewidth]{images/exp_dfacurve.pdf}
%%% \vspace{-0.8cm}
%\caption{Curves of performance criteria, when DOG is used over different decoding layers.}
%\label{fig:exp_dfaacurve}
%%% \vspace{-0.4cm}
%\end{figure}	

%\begin{table}
%	\centering
%	\caption{Criteria values w.r.t. different settings of DOG.}
%	\label{tab:dfa}
%	\renewcommand\arraystretch{1.2}
%	\resizebox{0.45\textwidth}{!}{
%		\begin{tabular}{l|cccc}
%			\toprule
%			&	FID$\downarrow$	&	LPIPS$\downarrow$	&	SCOOT$\uparrow$	&	FSIM$\uparrow$	\\
%			\midrule			
%			HIDA w/o DOG				&	17.41	&	0.291	&	0.493	&	0.536 \\
%			HIDA w/ DOG \textit{all}	&	18.87	&	0.298	&	0.490  &	\underline{0.540} 	\\
%			HIDA w/ DOG \textit{btm}2	&	20.34	&	0.297	&	0.491	&	0.537	\\
%			HIDA w/ DOG \textit{mid}3	&	\underline{16.68}	&	\underline{0.291}	&	\underline{0.495}	&	0.539	\\
%			HIDA w/ DOG \textit{top}2 	&	\textbf{15.06}	&	\textbf{0.263}	&	\textbf{0.575}	&	\textbf{0.551} \\
%			\bottomrule
%		\end{tabular}
%	}
%% \vspace{-0.3cm}
%\end{table}

%\subsection{Analysis of DOG and InfoACON}
%\label{ssec:exp_analysis}

%\subsubsection{DySPADE over different layers}
%\label{ssec:exp_DySPADE}


%\begin{table}
%  \caption{Criteria values when we use DySPADE on different decoding layers (on the CUFS dataset).}
%  \label{tab:freq}
%  \begin{tabular}{l|cc|ccc|c}
	%    \toprule
	%	&	FID	&	LPIPS	&	SCOOT	&	FSIM	&	SSIM & NLDA	\\
	%\midrule											
	%\textit{top}2	&	31.1	&	\textbf{31.1}	&	\textbf{0.200} 	&	\textbf{0.629}	&	\textbf{0.721}	&	\textbf{0.492}	\\
	%\textit{mid}3	&	30.6	&	31.3	&	0.226	&	0.603	&	0.718	&	0.474	\\
	%\textit{btm}2	&	\textbf{28.4}	&	37.0 	&	0.234	&	0.578	&	0.709	&	0.474	\\		
	%\textit{all}	&	32.5	&	35.1	&	0.228	&	0.609	&	0.714	&	0.477	\\
	%  \bottomrule
	%\end{tabular}
	%\end{table}
	
	
%\textbf{CUFS dataset.} 
%Table \ref{tab:pfm_cufs} shows the quantitative evaluation metrics of each method on the CUFS dataset. 
%
%
%\begin{table}
%  \caption{Performance criteria on the CUFS dataset.}
%  \label{tab:pfm_cufs}
%  \begin{tabular}{l|cccc}
%    \toprule
%&	FID$\downarrow$	&	LPIPS$\downarrow$	&	SCOOT$\uparrow$	&	FSIM$\uparrow$	\\
%\midrule	
%Pix2Pix \cite{Isola2017Pix2Pix}	&	27.89 	&	0.193 	&	\textbf{0.657}	&	0.722	\\
%CycleGAN \cite{Zhu2017CycleGAN}	&	39.59 	&	0.330 	&	0.494	&	0.682	\\
%MDAL \cite{Zhang2019MAL}	&	41.16 	&	0.236 	&	0.557	&	\textbf{0.725}	\\
%SDGAN \cite{cao2022SDGAN}	&	32.57 	&	\textbf{0.178} 	&	0.438	&	0.601	\\
%GauGAN \cite{Park2019GauGAN}	&	36.19 	&	0.221 	&	0.577	&	0.724	\\
%SCA-GAN \cite{gao2020cagan}	&	31.78 	&	0.403 	&	0.597	&	0.716	\\
%GENRE \cite{Park2019GauGAN}	&	\textbf{24.40} 	&	0.190 	&	0.616	&	0.724	\\
%HIDA (Ours)	&	26.90 	&	0.208 	&	0.616	&	0.718	\\
%  \bottomrule
%\end{tabular}
%\end{table}


% \textbf{APDrawing dataset.} 
% Table \ref{tab:pfm_fs2k} shows the quantitative evaluation metrics of each method on the APDrawing dataset. 

% \begin{table*}
	%   \caption{Quantative Comparison with existing methods on the APDrawing dataset.}
	%   \label{tab:freq}
	%   \begin{tabular}{l|ccc|ccc|c}
		%     \toprule
		%     Model & FID & SIFID & LPIPS & SCOOT & SSIM & FSIM &  KLDA \\
		%     \midrule
		%     Pix2Pix &  \\
		%     Pix2PixHD &  \\
		%     CycleGAN &  \\
		%     APDrawingGAN &  \\
		%     APDrawingGANv2  &  \\
		%     GENRE  &  \\
		%     DSAM-GAN(Ours)  &  \\
		%   \bottomrule
		% \end{tabular}
	% \end{table*}



%\subsection{Style Control and Mixture}
%\label{ssec:exp_style}
%
%We here analyse the style control capacity of our model. To this end, we interpolate between the one-hot style label vectors, and extend them to style maps for generation. Fig. \ref{fig:exp_style} shows that, the synthesised sketches show a combination of different styles. For example, the sketch of $[0.6, 0.2, 0.2]$ preserves the abstract characteristic of Style1, but shows stronger outlines and detailed hair regions like Style2 and Style3. Similarly, $[0.2, 0.6, 0.2]$ shows more shading areas than $[0.2, 0.2, 0.6]$. While $[0.2, 0.2, 0.6]$ shows a slight grey background, which is similar to Style2. 
%
%In addition, we further quantitatively evaluate the performance of style control. Specially, we randomly choose 1,000 facial photos from the FFHQ dataset \cite{Karras2018StyleGAN}, and generate sketches using the style codes shown in Fig. \ref{fig:exp_style}. Afterwards, we calculate the FID and LPIPS between synthesised sketches and all the testing sketches in the FS2K dataset (denoted by \emph{all}). Besides, we calculated the FID and LPIPS between synthesised sketches and each style subset, respectively (denoted by \emph{Style1, Style2, Style3} sequentially in Fig. \ref{fig:exp_style}). Inspiringly, both  FID and LPIPS inversely change with the value of style label. For example, $[1,0,0]$ achieves the lowest FID and LPIPS values, and $[0.6,0.2,0.2]$ achieves the second lowest values. Recall that lower FID and LPIPS values indicates higher consistency between two distributions in terms of style. Such phenomena means the synthesised sketches consistently change with the the input style codes.
%
%Based on these observations, we can safely conclude that our HIDA model allows precise and fine-grained style control in synthesised sketches.  
%
%\begin{figure*}
%\centering
%\includegraphics[width=\linewidth]{images/exp_style.pdf}
%% % \vspace{-0.8cm}
%\caption{Style control and mixture by our model.}
%\label{fig:exp_style}
%%% \vspace{-0.4cm}
%\end{figure*}
%
%\begin{figure}
%\centering
%\includegraphics[width=\linewidth]{images/exp_stylecurve.pdf}
%% % \vspace{-0.8cm}
%\caption{Performance criteria about style control and mixture on faces in-the-wild.}
%\label{fig:exp_style}
%%% \vspace{-0.4cm}
%\end{figure}


\subsection{Generalization Ability}
\label{ssec:exp_robust}
To evaluate the generalization ability of our framework, we apply the previously learned HIDA model to challenging faces in-the-wild and natural images. Here, we compare with Pix2Pix, SCA-GAN, and GENRE, because the models of MDAL and FSGAN haven't been released. All models are learned from the training set of the FS2K dataset.




\begin{figure}
	\centering
	\includegraphics[width=1\linewidth]{images/fig_wild.pdf}
	% \vspace{-0.6cm}
	\caption{Performance of our method on faces in-the-wild.}
	\label{fig:wild}
	 % \vspace{-0.3cm}
\end{figure}

\begin{figure}
%	\centering
	\includegraphics[width=1\linewidth]{images/exp_nature.pdf}
	% \vspace{-0.5cm}
	\caption{Performance of our model on natural images.}
	\label{fig:natural}
	 % \vspace{-0.3cm}
\end{figure}

\textbf{Performance on Faces In-the-wild.}
Fig. \ref{fig:wild} illustrates synthesized sketches on unconstrained faces. These faces have extreme variations in occlusion, pose, lighting, and tone. Generally speaking, our method produces high-quality sketches, in multiple styles, for both examples. Inspiringly, for the example shown in the bottom row, our HIDA model successfully depicts the eyes in shading areas. 
%GENRE, SCA-GAN, and Pix2Pix show suboptimal performance. However, 
In contrast, the other methods fail to generate some quality details, e.g. eyes of both examples. Moreover, they produce geometric deformations over the mouth of the bottom example. 
%CycleGAN fails to produce realistic sketches. 
Finally, our synthesized sketches vividly characterize the moods shown in the photographic faces. 


\textbf{Extension to Natural Images.}
We here apply our previously learned model to several natural images, collected from the Web. Here, we use MiDas \cite{ranftl2020MiDas} instead of 3DDFA for depth estimation. Fig. \ref{fig:natural} shows that our model still produces high-quality sketches, in multiple styles. The synthesized sketches vividly present the geometry and appearance of natural images. 

\textbf{Extension to other Image-to-Image translation tasks.} 
We additionally apply our method to pen-drawing generation  (with paired data on the APDrawing dataset \cite{YiLLR19}) and exemplar-based image translation (with unpaired data on the MetFace dataset \cite{karras2020training}). 
In the former task, we train and test our full model following standard settings. In the latter task, we use CoCosNet \cite{cocosnet} as the baseline, and modify it by (1) using depth, and (2) replacing the standard SPADE modules in CoCosNet by DySPADE and InfoACON. As shown in Table \ref{tab:addcomp}, our method outperforms previous SOTA methods, in terms of most performance indices. Fig. \ref{fig:metface} shows that our method generates distinct and accurate facial structures, compared to the other methods. Such results demonstrate that the proposed techniques are robust and applicable to other image translation tasks. 

%\begin{figure}
%	\centering
%	\includegraphics[width=1\linewidth]{images/fig_apd.pdf}
%	% % \vspace{-0.8cm}
%	\caption{Comparison with SOTAs on APDrawing.}
%	\label{fig:apd}
%	% \vspace{-0.2cm}
%\end{figure}

\begin{figure}
	\centering
	\includegraphics[width=1\linewidth]{images/fig_penoil.pdf}
	% % \vspace{-0.8cm}
	\caption{Generated pen-drawings and oil-paintings, on the APDrawing and MetFace datasets.}
	\label{fig:metface}
	% \vspace{-0.2cm}
\end{figure}


\begin{table}
	\tabcolsep=1.5pt
	\footnotesize
	\centering
	\caption{Comparison with SOTAs on the APDrawing and MetFace datasets.}
	\label{tab:addcomp}
%	\renewcommand\arraystretch{1.2}
%	\resizebox{0.48\textwidth}{!}{
		\begin{tabular}{lcc|lccc}
			\toprule
%			\multicolumn{3}{c|}{\textbf{APDrawing}} & \multicolumn{4}{c}{\textbf{MetaFace}} \\
%			
\textbf{APDrawing}	&	FID$\downarrow$		&	LPIPS $\downarrow$	& \textbf{MetaFace} &	FID$\downarrow$	&	LPIPS$\downarrow$ &	Sem.$\uparrow$	\\
			\midrule	
			DSMAP \cite{chang2020domain}	&	71.38	&	0.466 	 & GauGAN \cite{Park2019GauGAN}	&	76.42	&	0.391	 &	0.915	\\
			TUNIT \cite{Baek2021tunit}	&	91.64	&	0.458 &  DyNaST \cite{liu2022dynast}	&	29.25	&	0.375	&	0.917		\\
			Pix2Pix \cite{Isola2017Pix2Pix}	&	80.11	&	0.250  & Cocosnet \cite{cocosnet}	&	34.14	&	0.355	&		0.930		\\
			Pix2PixHD \cite{Wang2017Pix2PixHD}	&	\underline{60.55}	&	\underline{0.206}  & 	CocosnetV2 \cite{zhang2021cocosnetv2}	&	27.98	&	0.296	&	0.939	\\
            U$^2$-Net \cite{qin2020u2net}	&	77.19	&	0.232  & 	UNITE \cite{zhan2021unite}	&	35.91	&	0.356	&	0.930	\\
            HIDA (Ours)	&	\textbf{56.58}	&	\textbf{0.194} 	 &  HIDA (Ours)	&	\textbf{22.62}    &	\textbf{0.174}	&	   \textbf{0.981}  	  \\
			\bottomrule
		\end{tabular}
%	}
	% \vspace{-0.6cm}
\end{table}

%\begin{table}
%%\tabcolsep=2pt
%	\footnotesize
%	\centering
%	\caption{Comparison on APDrawing and MetFace.}
%	\label{tab:addcomp}
%%	\renewcommand\arraystretch{1.2}
%%	\resizebox{0.48\textwidth}{!}{
%		\begin{tabular}{l|cccc}
%			\toprule
%%			& \multicolumn{4}{c|}{APDrawing} & \multicolumn{4}{c}{MetaFace} \\
%\textbf{APDrawing}	&	FID$\downarrow$	&	C-FID$\downarrow$	&	SIFID $\downarrow$	&	LPIPS $\downarrow$	\\
%			\midrule	
%%			AdaIN \cite{Isola2017Pix2Pix}	&	137.25	&	130.68	&	0.64	&	0.3665 	\\
%%			DRIT \cite{Wang2017Pix2PixHD}	&	64.86	&	63.79	&	0.21	&	0.4726 	\\
%			DSMAP \cite{chang2020domain}	&	71.38	&	68.69	&	0.16	&	0.4661 	\\
%%			CycleGAN \cite{Zhang2019MAL}	&	103.12	&	99.84	&	0.31	&	0.5066 	\\
%%			MUNIT \cite{gao2020cagan}	&	77.59	&	74.53	&	0.17 	&	0.4675 	\\
%			Pix2Pix \cite{Isola2017Pix2Pix}	&	80.11	&	77.97	&	\underline{0.14}	&	0.2504 	\\
%			Pix2PixHD \cite{Wang2017Pix2PixHD}	&	\underline{60.55}	&	\underline{59.22}	&	\textbf{0.11}	&	\underline{0.2055} 	\\
%			TUNIT \cite{Baek2021tunit}	&	91.64	&	88.65	&	0.23	&	0.4584 	\\
%            U$^2$-Net \cite{qin2020u2net}	&	77.19	&	78.67	&  0.44	 &	0.2320 	\\
%            % BestSOTA 	&	60.6	&	59.2	&	\textbf{22.3}	&	0.206 &	27.98	&	0.296		&	19.7    &  	0.939  \\  
%            HIDA (Ours)	&	\textbf{56.58}	&	\textbf{56.35}	&	0.53 	&	\textbf{0.1935} 	  \\
%			\midrule
%			\midrule
%			\textbf{MetFace}		&	FID$\downarrow$	&	LPIPS$\downarrow$	&	SWD$\downarrow$    &	Semantic$\uparrow$	\\
%			\midrule	
%			SPADE \cite{Park2019GauGAN}	&	76.42	&	0.391	&	30.4	0 &	0.9151	\\
%			DyNaST \cite{liu2022dynast}	&	29.25	&	0.375	&	23.07	&	0.9168	\\
%			Cocosnet \cite{cocosnet}	&	34.14	&	0.355	&	18.09	&	0.9295	\\
%			CocosnetV2 \cite{zhang2021cocosnetv2}	&	27.98	&	0.296	&	19.72	&	0.9391	\\
%			UNITE \cite{zhan2021unite}	&	35.91	&	0.356	&	16.30	&	0.9301	\\
%			HIDA (Ours)	&	\textbf{22.62}    &	\textbf{0.174}	&	\textbf{10.28}	&	   \textbf{0.9814}  	  \\
%			\bottomrule
%		\end{tabular}
%%	}
%	% \vspace{-0.6cm}
%\end{table}


%\begin{table}
%	
%	\centering
%	\caption{Comparison with SOTAs on APDrawing and MetFace. \texttt{BestSOTA} indicates the best performance achieved by SOTA methods, i.e. \{DRIT, DSMAP, CycleGAN, MUNIT, TUNIT, Pix2Pix, Pix2PixHD, and U$^2$Net\} on APDrawing, and \{GauGAN, DynaST, CoCosNet, CoCosNet-v2, and UNITE\} on MetFace.}
%	\label{tab:addcomp}
%	\renewcommand\arraystretch{1.2}
%	\resizebox{0.48\textwidth}{!}{
%		\begin{tabular}{l|cccc|cccc}
%			\hline
%			& \multicolumn{4}{c|}{APDrawing} & \multicolumn{4}{c}{MetaFace} \\
%			&	FID$\downarrow$	&	C-FID$\downarrow$	&	SWD $\downarrow$	&	LPIPS $\downarrow$	&	FID$\downarrow$	&	LPIPS$\downarrow$	&	SWD$\downarrow$    &	Semantic$\uparrow$	\\
%			\hline	
%%			AdaIN \cite{Isola2017Pix2Pix}	&	137.25	&	130.68	&	0.64	&	0.3665 	\\
%%			DRIT \cite{Wang2017Pix2PixHD}	&	64.86	&	63.79	&	0.21	&	0.4726 	\\
%%			DSMAP \cite{Zhu2017CycleGAN}	&	71.38	&	68.69	&	0.16	&	0.4661 	\\
%%			CycleGAN \cite{Zhang2019MAL}	&	103.12	&	99.84	&	0.31	&	0.5066 	\\
%%			MUNIT \cite{gao2020cagan}	&	77.59	&	74.53	&	0.17 	&	0.4675 	\\
%%			TUNIT \cite{Fan2021FS2K}	&	91.64	&	88.65	&	0.23	&	0.4584 	\\
%%			Pix2Pix \cite{Park2019GauGAN}	&	80.11	&	77.97	&	\underline{0.14}	&	0.2504 	\\
%%			Pix2PixHD	&	\underline{60.55}	&	\underline{59.22}	&	\textbf{0.11}	&	\underline{0.2055} 	\\
%%                U2Net	&	77.19	&	78.67	&  0.44	 &	0.232 	\\
%                BestSOTA 	&	60.6	&	59.2	&	\textbf{22.3}	&	0.206 &	27.98	&	0.296		&	19.7    &  	0.939  \\  
%                Ours	&	\textbf{56.6}	&	\textbf{56.4}	&	23.7	&	\textbf{0.194} 	&	\textbf{22.6}    &	\textbf{0.174}	&	\textbf{10.3}	&	   \textbf{0.981}  	  \\
%			\hline
%		\end{tabular}
%	}
%	% \vspace{-0.6cm}
%\end{table}

% ----------------------------------------------------- %
\section{Conclusions}
\label{sec:conclusion}

In this work, we use comprehensive facial information for synthesizing sketchy portraits. Technically, we propose two informative and dynamic adaptation methods, including a normalization module and an activation function. Extensive experiments show that our method, termed HIDA, can generate high-quality and style-controllable sketches, over a wide range of challenging samples. 
%Since depth information has shown inspiring capacity in generating high-quality artistic images \cite{cheng2019structure, chan2022informativedraw}, it's promising to extend the depth adaptation module for generating other artistic images, such as oil paintings. 
%To this end, it is necessary to design adaptation mechanisms for unpaired image-to-image translation tasks. 
Our work also implies promising applications of dynamic adaptation, or dynamic networks, in more image generation tasks. 
Besides, it is promising to boost the performance of FSS models by combining multi-source datasets. We will explore such works in the near future.


{\small
\bibliographystyle{ieee_fullname}
\bibliography{ref}
}

\end{document}