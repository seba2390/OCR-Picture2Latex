% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
\usepackage[final]{EMNLP2022}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}
% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}

% \usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}


\usepackage{listings}
\usepackage{color}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstset{
backgroundcolor=\color{backcolour},
  keepspaces=false,
  language=Python,
  aboveskip=3mm,
  belowskip=1mm,
  showstringspaces=false,
  captionpos=b, 
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3,
  belowcaptionskip=2\baselineskip,
  frame=tb
}
\usepackage{todonotes}
% Definitions of handy macros can go here
\usepackage{fancyvrb}
\usepackage{pifont}
\usepackage{scrextend}
% \usepackage{times}
% \usepackage{float}
% \usepackage{graphicx}
% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\newcommand{\cmark}{\centering \ding{51}}%
\newcommand{\xmark}{ \ding{55}}
\newcommand{\dataset}{{\cal D}}
\newcommand{\fracpartial}[2]{\frac{\partial #1}{\partial  #2}}
\newcommand{\Ical}{\mathcal{I}}
\newcommand{\Ucal}{\mathcal{U}}
\newcommand{\Lcal}{\mathcal{L}}
\newcommand{\Vcal}{\mathcal{V}}
\newcommand{\Tcal}{\mathcal{T}}
\newcommand{\Xcal}{\mathcal{X}}
\newcommand{\Ycal}{\mathcal{Y}}
\newcommand{\Scal}{\mathcal{S}}
\newcommand{\bfl}{\mathbf l}
\newcommand{\bfx}{\mathbf x}
\newcommand{\wisdom} {\textsc{Wisdom}}
\newcommand{\snuba} {\textsc{Snuba}}
\newcommand{\cage} {\textsc{Cage}}
\newcommand{\snorkel} {\textsc{Snorkel}}
\newcommand{\spear}{\mbox{\textsc{Spear}}}
% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

% \title{Instructions for EMNLP 2022 Proceedings}
\title{\textsc{SPEAR} : Semi-supervised Data Programming in Python}
\author{Guttu Sai Abhishek$^{1}\thanks{~~Authors contributed equally}$,  Harshad Ingole$^{1}\footnotemark[1]$, Parth Laturia$^{1}\footnotemark[1]$, Vineeth Dorna$^{1}\footnotemark[1]$,  \\ \textbf{Ayush Maheshwari$^{1}\footnotemark[1]$, Rishabh Iyer$^{2}$, Ganesh Ramakrishnan$^{1}$}  \\
$^1$Indian Institute of Technology Bombay \\  $^2$The University of Texas at Dallas
}
% \affiliation{%
%   \institution{$^1$Indian Institute of Technology Bombay, $^2$The University of Texas at Dallas}
%   }
%   \email{{gsaiabhishek, harshadingole, parthlaturia, vineethdorna, ayusham,  ganesh}@cse.iitb.ac.in,  rishabh.iyer@utdallas.edu }

\begin{document}

\maketitle
% \author{Guttu Sai Abhishek\textsuperscript{1*} \email  gsaiabhishek@cse.iitb.ac.in
%     %   \addr Department of Statistics
%       \AND
%       Harshad Ingole\textsuperscript{1*} \email harshadingole@cse.iitb.ac.in
%       \AND
%       Parth Laturia\textsuperscript{1*} \email  parthlaturia@cse.iitb.ac.in
%       \AND
%       Vineeth Dorna\textsuperscript{1*} \email vineethdorna@cse.iitb.ac.in 
%       \AND
%       Ayush Maheshwari\textsuperscript{1} \email ayusham@cse.iitb.ac.in
%       \AND
%       Ganesh Ramakrishnan\textsuperscript{1} \email ganesh@cse.iitb.ac.in
%       \AND
%       Rishabh Iyer\textsuperscript{2} \email rishabh.iyer@utdallas.edu
%       \AND
%       \addr \textsuperscript{1}Department of Computer Science and Engineering, IIT Bombay
%       \AND
%       \addr \textsuperscript{2}University of Texas at Dallas
%       \AND
%       \addr \textsuperscript{*}Equal contribution. Authors ordered alphabetically.
%       }
      
% \editor{}


\begin{abstract}%
We present \spear, an open-source python library for data programming with semi supervision. The package implements several recent data programming approaches including facility to programmatically label and build training data. SPEAR facilitates \textit{weak supervision} in the form of heuristics (or rules) and association of \textit{noisy} labels to the training dataset. These \textit{noisy} labels are aggregated to assign labels to the unlabeled data for downstream tasks. We have implemented several label aggregation approaches  that aggregate the \textit{noisy} labels and then train using the \textit{noisily} labeled set in a cascaded manner. Our implementation also includes other approaches that \textit{jointly} aggregate and train the model for text classification tasks. Thus, in our python package, we integrate several cascade and joint data-programming approaches while also  providing the facility of data programming by letting the user define labeling functions or rules.
The code and tutorial notebooks are available at \url{https://github.com/decile-team/spear}. Further, extensive documentation can be found at \url{https://spear-decile.readthedocs.io/}. Video tutorials demonstrating the usage of our package are available  \href{https://youtube.com/playlist?list=PLW8agt_HvkVnOJoJAqBpaerFb-z-ZlqlP}{here}. We also present some real-world use cases of \spear.
%ha been used for machine learnt textual post-editing in efforts such as in translation as well as i patient diagnosis.

\end{abstract}

\maketitle

% \begin{keywords}
%   low cost labeling, data programming, labeling functions, machine learning, semi-supervision
% \end{keywords}

\section{Introduction}
Supervised machine learning approaches require large amounts of labeled data to train robust machine learning models. 
For classification tasks such as spam detection, (movie) genre categorization, sequence labelling, and so on, modern machine learning systems rely heavily on human-annotated \textit{gold} labels. Creating labeled data can be a time-consuming and expensive procedure that necessitates a significant amount of human effort.
% Modern machine learning systems rely excessively on human-annotated \textit{gold}  labels for classification tasks such as spam detection, (movie) genre classification, sequence labeling, {\em etc}. Creating labeled data is a time-intensive process, requiring sizeable human effort and cost. 
%\todo{Ayush - not clear. Hence commenting out.} This, in conjunction with the model's  heavy dependence on vast amounts of labelled data for its training, makes it difficult to achieve comparable results on new tasks. 
To reduce dependence on human-annotated labels, various techniques such as semi-supervision, distant supervision, and crowdsourcing have been proposed.
% Combined with the heavy dependence of model training on large amounts of labeled data, this serves as a deterrent from achieving comparable performances on new tasks. Therefore, various techniques such as semi-supervision, distant supervision, and crowdsourcing have been proposed to reduce dependency on human-annotated labels.
% In particular, several recent data programming approaches \citep{bach2019snorkel, spear, oishik, awasthi2020learning} have proposed the use of \textit{human-crafted} labeling functions to \textit{weakly} associate labels with the training data. Users encode supervision as rules/guides/heuristics in the form of labeling functions (LFs) that assign noisy labels to the unlabeled data, thus reducing dependence on human labeled data.
In order to help reduce the subjectivity and drudgery in the labeling process, several recent data programming approaches~\citep{bach2019snorkel,  oishik, awasthi2020learning, spear} have proposed the use of \textit{human-crafted} labelling functions or automatic LFs~\citep{maheshwari2022learning} to \textit{weakly} associate labels with the training data. Users encode supervision in the form of labelling functions (LFs), which assign noisy labels to unlabeled data, reducing dependence on human labeled data. LFs can defined as first-order logic rules as a composition of semantic role attributes \cite{sen2020learning} or syntactic grammar rules \cite{sahay2021rule}.

\begin{figure*}[!t]
    \centering
    \includegraphics[width=0.95\linewidth]{JMLR_and_Spear.pdf}
    \caption{Flow of the \textsc{SPEAR} library.}
    \label{fig:flow}
\end{figure*}
%\todo{Either cite all the inductive logic programming work and rule induction work or drop. Why single out only Snuba? Also perhaps all these can be cited in the future work section} Another approach, \snuba \citep{varma2018snuba} induces labelling function \textit{automatically} from the text features without requiring human supervision to develop LFs. 
% \todo{As discussed, a flow chart for the execution (with files needed for exchange) will be helpful} 

While most data-programming approaches cited above provide their source code in the public domain, a unified package providing access to all data programming approaches is however missing. In this work, we describe \spear, a python package that implements several existing data programming approaches while also providing a platform for integrating and benchmarking newer ones. Inspired by frameworks such as Snorkel~\citep{lison-etal-2021-skweak, ratner2017snorkel, zhang2021wrench} and %automation 
algorithm based labeling in Matlab\footnote{https://www.mathworks.com/help/vision/ug/create-automation-algorithm-for-labeling.html}, we provide a facility for users to define LFs. Further, we develop and integrate several recent data programming models that uses these LFs. We provide many easy-to-use jupyter notebooks and video tutorials for helping new users get quickly started. Though we provide  implementation on 5 text datasets, our package can be easily integrated with vision and speech datasets as well. The users can get started by installing the package using the below command.
\begin{lstlisting}
pip install decile-spear
\end{lstlisting}
In Table \ref{tab:comparisonTable}, we compare our library with other existing packages such as Wrench \cite{zhang2021wrench}, SkWeak\cite{lison-etal-2021-skweak}, Imply Loss \cite{awasthi2020learning}, Snorkel \cite{bach2019snorkel} and Matlab. Wrench \cite{zhang2021wrench} provides facility for semi-supervised and unsupervised label aggregation approaches, however, it does not provide mechanism to find useful subset of unlabeled data and defining continuous LFs. SkWeak \cite{lison-etal-2021-skweak} does not integrate semi-supervised LA approaches in the package. \spear{} addresses the shortcomings of existing packages by providing features such as designing of discrete and continuous LFs, integrating unsupervised and semi-supervised aggregation approaches and facility to choose labeled set using subset selection approaches.


\section{Package Flow}
The \spear\ package consists of three components (and they are applied in the same order): (i) Designing LFs, (ii) applying LFs, and (iii) applying a label aggregator (LA). \\
% Initially, we develop LFs in the form of regex rules or  . 
Initially, the user is expected to declare an \textit{enum} class listing all the class labels. The \textit{enum} class associates the numeric class label with the readable class name.
As part of (i), \spear\ provides the facility for manually creating LFs. LFs can be in the form of regex rules as well. Additionally, we also provide the facility of declaring a \textit{@preprocessor} decorator to use an external library such as \textit{spacy}\footnote{https://spacy.io}, nltk, {\em{etc.}} which can be optionally invoked by the LFs.
%Thereafter, as part of (ii), we use \texttt{apply()} on the unlabeled set (and labeled set) that returns a LF matrix. The matrix is then provided as input to one of the selected label aggregation approaches, as shown in Figure\ref{fig:flow}.
% LFs by  several  DA component provides facility to develop human heuristics. Each heuristic can either return a  a class label or an abstain label. In this package, LF can use external libraries such as \textit{spacy} by declaring a \textit{@preprocessor} decorator.
 %The enum class prevents ambiguity by avoiding declaring numeric class labels. 
Thereafter, as part of (ii), the LFs can be applied on the unlabeled (and labeled) set using an \textit{apply} function that returns a matrix of dimension \#LFs $\times$ \#instances.
The matrix is then provided as input to the selected label aggregator (LA)  in (iii), as shown in Figure\ref{fig:flow}.
We integrate several LA options into \spear. Each LA aggregates multiple noisy labels (obtained from the LFs) to associate a single class label with an instance. Additionally, we have also implemented in \spear, several joint learning approaches that employ semi-supervision and feature information.
The high-level flow of the \spear\ library is presented in Figure \ref{fig:flow}.
% The  label the data using LFs and generate a pickle file and a json file to store labels given by LFs and information about the labels(from enum class) respectively. Now these files are passed to train the models and predict the final labels to the unlabeled data. The following sections details the labeling part using LFs and the next section to it goes over the models.

    %\scalebox{0.5}[0.25]{stuff}
    
    \begin{table*}[!h]
    \centering
    \begin{center}
    % \hspace*{-1.5cm}
    \begin{tabular}{|p{0.21\textwidth}|p{0.15\textwidth}|p{0.12\textwidth}|p{0.1\textwidth}|p{0.1\textwidth}|p{0.17\textwidth}|}
    \hline
         Package & Designing \& applying LFs & Continuous LFs & Unsup LA & Semi-sup LA & Labeled-data subset selection  \\
         \hline
         Snorkel\cite{ratner2017snorkel}  & \centering \cmark  & \centering \xmark &  \centering \xmark & \centering \cmark  &  {\hspace {3em}\xmark}  \\
         Imply Loss \citep{awasthi2020learning}  & \centering \xmark  & \centering \xmark & \centering \xmark  & \centering  \cmark & {\hspace {3em}\xmark}   \\
         Matlab  & \centering \cmark  & \centering \xmark & \centering \xmark &  \centering \xmark & {\hspace {3em}\xmark}     \\
         SkWeak \cite{lison-etal-2021-skweak}  & \centering \cmark  & \centering \cmark & \centering \cmark &  \centering \xmark & {\hspace {3em}\xmark}     \\
         Wrench \cite{zhang2021wrench}  & \centering \cmark  & \centering \xmark & \centering \cmark &  \centering \cmark & {\hspace {3em}\xmark}     \\
         \spear  & \centering \cmark  & \centering \cmark & \centering \cmark & \centering \cmark & {\hspace {3.2em}\cmark }  \\
         \hline
    \end{tabular}
    \caption{Comparison of \spear\ against available packages.
    % Snorkel provides support for designing and applying LFs and semi-supervised LA approaches but does not have facility for continuous LFs, unsupervised LA and labeled-data subset selection. 
    \label{tab:comparisonTable}}
    \end{center}
\end{table*}


\section{Designing and Applying LFs}
User interacts with the library by designing labeling functions. Similar to \citet{ratner2017snorkel}, labeling functions are python functions which take a candidate as an input and either associates class label or abstains. However, continuous LFs returns a continuous score in addition to the class label. These continuous LFs are more
natural to program and lead to improved recall \cite{oishik}.
\subsection{Designing LFs}
% Spear provides a sub-package named labeling using which users can define heuristics in a programmatically fashion. These heuristics, also called labeling functions(LF), facilitate the labeling of an unlabeled dataset. In addition to this, the user can define a continuous scorer, which associates a confidence score with the label assigned by the LF.
% \subsection{Labeling functions}
\spear\ uses a \texttt{@labeling\_function()} decorator to define a labeling function. Each LF, when applied on an instance, can either return a class label or not return anything, \textit{i.e.} abstain. The LF decorator has an additional argument that accepts a list of preprocessors. Each preprocessor can be either  declared  as  a  pre-defined  function  or  can employ external libraries. The pre-processor transforms the data point before applying the labeling function. 


%\begin{Verbatim}[fontsize=\small]
%@preprocessor()
%def to_lower(x):
%  return x.lower()
%\end{Verbatim}
% \begin{Verbatim}[fontsize=\small]
\begin{lstlisting}
@labeling_function(cont_scorer, resources, preprocessors, label)
def CLF1(x,**kwargs):
  return label if kwargs["continuous_score"] >= threshold else ABSTAIN
\end{lstlisting}



The LF can express pattern matching rules in the form of heuristics, distant supervision by using external knowledge bases and other data resources to label datapoints. LFs on SMS dataset can be seen in the example notebook \href{https://github.com/decile-team/spear/blob/main/notebooks/SMS_SPAM/sms_labeling.ipynb}{here}.

\paragraph{Continuous LFs:} 
In the discrete LFs, users construct heuristic patterns based on dictionary lookups or thresholded distance for the classification tasks. However, the keywords in hand-crafted dictionaries might be incomplete. \citet{oishik} proposed a comprehensive alternative that design continuous valued LFs that return scores derived from soft match between words in the sentence and
the dictionary.


\spear\ provides the facility to declare continuous LFs, each of which returns the associated label along with a confidence score using the \texttt{@continuous\_scorer} decorator. 
%Using the  \texttt{@continuous\_scorer} decorator, a user can define a heuristic to assign a score to the label triggered by the LF. 
The continuous score can be accessed in the LF definition through the keyword argument \texttt{continuous\_score}. As evident from Table~\ref{tab:comparisonTable}, no other existing package provisions for both semi-supervised aggregation and subset selection modules.
%The continuous score is utilised in two algorithms, namely, CAGE and JL.
% \begin{Verbatim}[fontsize=\small]
% @continuous_scorer()
% def word_similarity(sentence,**kwargs):
%     similarity = 0.0
%     words = preprocess(sentence.split())
%     word_vectors = get_word_vectors(words)
%     for w in kwargs['keywords']:
%         similarity = min(max(similarity,get_similarity(word_vectors,w)),1.0)
%     return similarity
% \end{Verbatim}
% \begin{Verbatim}[fontsize=\small]
\begin{lstlisting}
@continuous_scorer()
def similarity(sentence,**kwargs):
  word_vecs = featurizer(sentence)
  keyword_vecs = featurizer(kwargs["keywords"])
  return similarity(word_vecs,keyword_vecs)
\end{lstlisting}



\subsection{Applying LFs}%PreLabels
Once LFs are defined, users can analyse labeling functions by calculating coverage, overlap, conflicts, empirical accuracy for each LF which helps to re-iterate on the process by refining new LFs. The metrics can be visualised within the \spear\ tool, either in the form of a table or graphs as shown in Figure \ref{fig:metric}.

PreLabels is the master class which encapsulates a set of LFs, the dataset to label and enum of class labels. PreLabels facilitates the process of applying the LFs on the dataset, and of analysing and refining the LF set. We provide functions to store labels assigned by LFs % and class information 
and associated meta-data such as mapping of class name to numeric class labels on the disk in the form json file(s). The pre-labeling performed using the LFs can be consolidated into labeling performed using several consensus models described in Section~\ref{sec:models}.
% \begin{Verbatim}[fontsize=\small]
\begin{lstlisting}
sms_pre_labels = PreLabels(name="sms", data=X_V, gold_labels=Y_V, 
data_feats=X_feats_V, rules=rules, labels_enum=ClassLabels, num_classes=2)
\end{lstlisting}

% \subsection{Analysis of labeling functions}
% Wrt \url{https://github.com/decile-team/spear/blob/main/labeling/data/SMS_SPAM/enum_sms.ipynb} shouldn't y axis of conflict and overlap be on the same/shared y axis? Or if difficult, both can be in the same plot. 

\begin{figure*}[!h]
    \centering
    \includegraphics[width=0.8\linewidth]{spear-lf-stat.pdf}
    \caption{LF analysis on the SMS dataset presented in the form of graph visualization within the \spear\ tool. The statistics include precision, coverage, conflict and empirical accuracy for each LF.}
    \label{fig:metric}
\end{figure*}

\section{Models} \label{sec:models}
% We could have simple wrapper functions to show ablation test results in the form of plots (say for varying #LFs, TrainSet, UnlabelledSet, ValSet).
% The label matrix can be stored in a pickle file and the enum class details is stored in a json file for a quick access. We feed these files to the model for label aggregation and training. \par
We implement several data-programming approaches in this demonstration that includes simple baselines such as fully-supervised, semi-supervised and unsupervised approaches.

\subsection{Joint Learning \cite{spear}}
    
The joint learning (JL) module implements a semi-supervised data programming paradigm that learns a joint model over LFs and features. JL has two key components, {\em viz.}, feature model (fm) and graphical model (gm) and their sum is used as a training objective.
% that effectively uses the rules/labelling functions along with semi-supervised loss functions on the feature space(and hence, a feature matrix is to be provided by user). \\ \\
During training, the JL requires labeled ($\Lcal$), validation ($\Vcal$), test ($\Tcal$) sets consisting of true labels and an unlabeled ($\Ucal$) set whose true labels are to be inferred. The model API closely follows that of \textit{scikit-learn}~\citep{pedregosa2011scikit} to make the package easily accessible to the machine learning audience. The primary functions are: (1) \texttt{fit\_and\_predict\_proba}, which trains using the prelabels assigned by LFs and true labels of $\Lcal$ data and predicts the probabilities of labels for each instance of $\Ucal$ data 
(2) \texttt{fit\_and\_predict}, similar to the previous one but which predicts labels of $\Ucal$ using maximum posterior probabilities 
(3) \texttt{predict\_(fm/gm)\_proba}, predicts the probabilities, using feature model(fm)/graphical model(gm) 
(4) \texttt{predict\_(fm/gm)}, predicts labels using fm/gm based on learned parameters.
We also provide functions \texttt{save} or \texttt{load\_params} to save or load the trained parameters.


As another unique feature ({\em c.f.} Table~\ref{tab:comparisonTable}), our library supports a \textit{subset-selection framework} that makes the best use of human-annotation efforts. The $\Lcal$ set can be chosen using submodular functions such as facility location, max cover, { \em etc.} We utilise the submodlib\footnote{\url{https://github.com/decile-team/submodlib}} library for the subset selection algorithms.
% We provide functions to choose a subset of unlabeled data such that it complements the LFs. This subset can then be human labeled and passed to JL member functions as L data. Choosing an appropriate subset makes best use of the human labeling efforts. 
The function alternatives for subset selection are \texttt{rand\_subset, unsup\_subset, sup\_subset\_indices, sup\_subset\_save\_files}. 
% As in the case of JL, we implement \texttt{fit\_and\_predict} and \texttt{fit\_and\_predict\_proba} for training and prediction for below approaches as well.
\subsection{Only-$\Lcal$}
In this, the classifier $P(y|\bfx)$ is trained only on the labeled data. Following \citet{spear}, we provide facility to  use either Logistic Regression or a 2-layered neural network. Our package is flexible to allow other architectures to be plugged-in as well.


\begin{figure*}[t]
    \centering
    \includegraphics[width=.8\linewidth]{chart.pdf}
    \caption{Experiments on SMS, IMDB and MIT-R dataset and comparison with various approaches. We use JL combined with supervised subset selection for obtaining numbers. }
    \label{fig:chart}
\end{figure*}

\subsection{\cage~\citep{oishik}}
This accepts both continuous and discrete LFs. Further, each LF has an associated quality guide component, that refers to the fraction of times the LF predicts the correct label; this stabilises training in absence of $\Vcal$ set. In our package, \cage\ accepts $\Ucal$ and $\Tcal$ sets during training. \cage{} has member functions similar to (except there are no fm or gm variants to \texttt{predict\_proba}, \texttt{predict} functions in Cage) JL module, with different arguments, serving the same purpose. It should be noted that this model doesn't need labeled($\Lcal$) or validation($\Vcal$) data.  %Below we list other approaches implemented in our packages. These approaches need a labeled($L$), an unlabeled ($U$), a validation($V$) set during training. \\

\subsection{Learning to Reweight (L2R) \citep{l2r}}
This method is an online meta-learning approach for reweighting training examples with a mix of $\Ucal$ and $\Lcal$. It leverages validation set to determine and adaptively assigns importance weights to examples based on the gradient direction. This does not employ additional parameters to weigh or denoise individual rules.

\subsection{$\mathbf{\Lcal+\Ucal_{Snorkel}}$~\citep{ratner2017snorkel}} 

This method trains a supervised classifier on $\Lcal$ set and Snorkel's generative model on $\Ucal$ set. Snorkel is a generative model that models class probabilities based on discrete LFs for consensus on the noisy and conflicting labels. It assigns a linear weight to each rule based on an agreement objective and label examples in $\Ucal$.

\subsection{Posterior Regularization (PR)~\citep{Hu2016Harnessing}}
This is a method that enables to simultaneously learn from $\Lcal$ and logic rules by jointly learning a rule and feature network in a teacher-student setup. The student network learns parameter $\theta$ using the $\Lcal$ set and teacher networks attempts to imitates the student network in a joint learning manner. The teacher network encodes logic rules as a regularization term in the overall loss objective.

\subsection{Imply Loss~\citep{awasthi2020learning}}
This approach uses additional information in the form of labeled rule exemplars and trains with a denoised rule-label loss.  They leverage both rules and labeled data by mapping each rule with exemplars of correct firings (i.e., instantiations) of that rule. Their joint training algorithms
denoise over-generalized rules and train a classification model. It has two main components:
% On the other hand, \textbf{High Level Supervision Module} \citep{awasthi2020learning} that contains approaches like Imply-loss, L2R, PR and Only-L requires a labeled($L$), an unlabeled ($U$) and a validation($V$) set. The motivation of this model is to learn rules to reduce the effort of humans in labeling and supervision of data while at the same time, consider the expertise of humans in the domain to combine the scalability of rules with quality of instance labels and make the supervision process both natural for humans and synergistic for learning. 
\begin{enumerate}
\item {Rule Network}: It learns to predict whether a given rule has overgeneralized on a given sample using latent coverage variables. 
\item  Classification Network: It is trained on $\Lcal$ and $\Ucal$ to predict the output label and maximize the accuracy on unseen test instances using a soft implication loss. 
\end{enumerate}

This module contains the following primary classes:
\begin{enumerate}
\item  DataFeeder - It will essentially take all the parameters as input and create a data feeder class with all these parameters as its attributes. 
\item HighLevelSupervisionNetwork (HLS) - It will take the 2 networks, the mode or the approach that needs to be used to train the model, the required parameters, the directory storing model checkpoints at different instances and the instances and labels from the labeled dataset ($\Lcal$) and create an object named "hls".
\end{enumerate}
HLS object will have many member functions of which the 2 significant are:\\
(a) \textbf{hls.train}: This function, when called with the required mode, will train the 2 network attributes of the object.\\
(b) \textbf{hls.test}: It supports 3 types of testing:\\
\indent  (i) test\_w: this will test the rule network and the related model of the object.\\
\indent  (ii) test\_f: this will test the classification network and the related model of the object.\\
\indent  (iii) test\_all: this will test both the networks and models of the class.


% \subsection{Imply Loss}
% \subsection{L2R}

% \todo{Move project development into covering note}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.85\linewidth]{framework.pdf}
    \caption{Mutiple LFs generated from post-editor edits based on semantic and lexical features while editing science (domain-specific) document in English.}
    \label{fig:framework}
\end{figure*}
\section{Experiments}

We prepared \href{https://github.com/decile-team/spear/tree/main/notebooks/}{jupyter tutorial notebooks} for two standard text classification datasets, namely SMS, YouTube and TREC. We took LFs on these datasets from \citet{awasthi2020learning} and train using approaches implemented in this paper.  Figure \ref{fig:chart} shows performance of various approaches implemented using our package on additional two datasets, MIT-R and IMDB. We can integrate image classification tasks by defining appropriate feature extraction module and rules.



\section {Use Cases}

\spear\ is employed in project UDAAN\footnote{\url{https://www.udaanproject.org/}} for reducing post editing efforts. UDAAN \cite{udaan} is a post-editing workbench to translate content in native languages. Based on the post editor's patterns of changes to the target language document, candidate labeling functions are generated (based on a combination of heuristics and linguistic patterns) by the UDAAN workbench ({\em c.f.} Figure~\ref{fig:framework} for examples of LFs). Based on these LFs, \spear\ gets invoked on a combination of the edited ({\em i.e.}, labeled) data and the not yet edited ({\em i.e.}, unlabeled) data to present consolidated edits to the post-editor. This use case has been presented in the flow chart in Figure~\ref{fig:framework}. We present the appropriate incorporation of \spear\ into the post-editing environment of an ecosystem such as for translation (UDAAN) or even for Optical Character Recognition\footnote{\url{https://www.cse.iitb.ac.in/~ocr/}} or Automatic Speech Recognition (ASR). %he/she is prompted with the list of edits that need to be made based on the current page. The prompt provides an option for replacement of the edited words across all pages. The list of all these edits made till now can be part of the labeled set and replacements that need to be done in the next part of the document is part of the unlabeled set.
%From the labeled set of the post editor we develop heuristics based on  lexical, syntactic and semantic features. We train the cage model for consensus among the different heuristics developed and label the unlabeled data i.e next part of the document which is not being edited yet.

As a part of COVID-19 third wave preparedness, \spear\ was used by the Municipal Corporation of Greater Mumbai (MCGM)’s Health Ward\footnote{\url{https://colab.research.google.com/drive/1tNUObqSDypUos7YNvnqvemALlkrrsB0z}} for predicting the COVID-19 status of patients to help in preliminary diagnosis. 

\subsection{Demonstration Case}
For the demonstration use case, apart from the use cases outlined in the previous section, we can choose a text classification dataset and form regex or continuous rules by observing few data points. Once LFs are developed, we can easily compare the LFs with any of the semi- and un-supervised algorithms present in the package. 

% \section{Project Development}
% For ease of use, we use well-known and established 3rd-party packages to increase system stability. For instance, we build documentation using \textit{sphinx} documentation generator\footnote{\url{https://www.sphinx-doc.org}}. We also use standard open-source packages for development and visualisation such as  \textit{numpy}, \textit{matplotlib} and \textit{pandas}. The package is written in Python3 and open-sourced with a MIT License\footnote{\url{https://opensource.org/licenses/MIT}}. This is an open-source package and community can contribute by including new algorithms. 
\section{Conclusion and Future Work}
\spear\ is a unified package for semi-supervised data programming that  quickly annotates training data and train machine learning models. 
It eases the use of developing LFs and label aggregation approaches. This allows for better reproducibility, benchmarking and easier ML development in low-resource settings such as textual post-editing. %cold-start scenarios.%on several tasks. 
Presently, we are integrating automatic LF induction approaches such as Snuba \cite{varma2018snuba} that uses a small labeled set to induce LFs automatically. This will significantly increase the scope of datasets without needing human intervention in designing LFs. The package is written in Python3 and open-sourced with a MIT License\footnote{\url{https://opensource.org/licenses/MIT}} open for community contribution.


\section{Acknowledgements}
We thank anonymous reviewers for providing constructive feedback. Ayush Maheshwari is supported by a Fellowship from Ekal Foundation (www.ekal.org). Ganesh Ramakrishnan is grateful to IBM Research, India (specifically the IBM AI Horizon Networks - IIT Bombay initiative) as well
as the IIT Bombay Institute Chair Professorship for
their support and sponsorship.
% \newpage
% While Grasp \cite{shnarch2017grasp} requires a large labeled set for pattern induction, we are using a small labeled set to induce labels on large unlabeled set that is further fed to Grasp for automatic LF induction.

% Acknowledgements should go at the end, before appendices and references

% \acks{}

% Manual newpage inserted to improve layout of sample file - not
% needed in general before appendices/bibliography.

% \newpage

% \vskip {0.2in}
% \newpage
\bibliographystyle{acl_natbib}
\bibliography{refs}

\end{document}