\section{Related Work} \label{Sec:rw}

In the robotic community, LfD techniques are widely used to teach the robots new skills based on human demonstrations. Koenemann et al.~\cite{Koenemann2014} introduced a real-time method to allow a humanoid robot to imitate human whole-body motions. Recently, Welschehold et al.~\cite{Welschehold2016} proposed to transform human demonstrations to different hand-object trajectories in order to adapt to robotic manipulation tasks. The advantage of LfD methods is their abilities to let the robots accurately repeat human motions, however, it is difficult to expand LfD techniques to a large number of tasks since the training process is usually designed for a specific task or needs training data from real robotic systems~\cite{Akgun2012}.


From a computer vision viewpoint, Aksoy et al.~\cite{Aksoy2016} introduced a framework that represents the continuous human actions as ``semantic event chains" and solved the problem as an activity detection task. In~\cite{Yang2015}, Yang et al. proposed to learn manipulation actions from unconstrained videos using CNN and grammar based parser. However, this method needs an explicit representation of both the objects and grasping types to generate command sentences. Recently, the authors in~\cite{Aksoy2017} introduced an unsupervised method to link visual features to textual descriptions in long manipulation tasks. In this paper, we propose to directly learn the output command sentences from the input videos without any prior knowledge. Our method takes advantage of CNN to learn robust features, and RNN to model the sequences, while being easily adapted to any human activity.


Although commands, or in general natural languages, are widely used to control robotic systems. They are usually carefully programmed for each task. This limitation means programming is tedious if there are many tasks. To automatically understand the commands, the authors in~\cite{Tellex2011} formed this problem as a probabilistic graphical model based on the semantic structure of the input command. Similarly, Guadarrama et at.~\cite{Guadarrama13} introduced a semantic parser that used both natural commands and visual concepts to let the robot execute the task. While we retain the concepts of~\cite{Tellex2011} and ~\cite{Guadarrama13}, the main difference in our approach is that we directly use the grammar-free commands from the translation module. This allows us to use a simple similarity measure to map each word in the generated command to the real command on the robot.


In deep learning, Donahue et al.~\cite{Donahue2014} made a first attempt to tackle the video captioning problem. The features were first extracted from video frames with CRF then fed to a LSTM network to produce the output captions. In~\cite{Venugopalan2016}, the authors proposed a sequence-to-sequence model to generate captions for videos from both RGB and optical flow images. Yu et al.~\cite{Haonan2016} used a hierarchical RNN to generate one or multiple sentences to describe a video. In this work, we cast the problem of translating videos to commands as a video captioning task to build on the strong state  the art in computer vision. Furthermore, we use the output of the deep network as the input command to control a full-size humanoid robot, allowing it to perform different manipulation tasks.


%Lorem ipsum dolor sit amet, consectetuer adipiscing elit.
%Ut purus elit, vestibulum ut, placerat ac, adipiscing vitae,
%felis. Curabitur dictum gravida mauris. Nam arcu libero,
%nonummy eget, consectetuer id, vulputate a, magna. Donec
%vehicula augue eu neque. Pellentesque habitant morbi tristique senectus et netus et malesuada fames ac turpis egestas.
%Mauris ut leo. Cras viverra metus rhoncus sem. Nulla et
%lectus vestibulum urna fringilla ultrices. Phasellus eu tellus
%sit amet tortor gravida placerat. Integer sapien est, iaculis
%in, pretium quis, viverra ac, nunc. Praesent eget sem vel
%leo ultrices bibendum. Aenean faucibus. 






\junk{

Recently, the authors in~\cite{Matthias17} introduced a method to solve the problem of linking human whole-body motion and natural language as a captioning task.


, while avoid using the complicated natural language parser
This approach, however, is not straightforward to apply to real robotic applications since the complicated of human spoken language. 



This work formed the problem as an activity detection task and did not be applied to robotic applications.


A drawback of this approach ...

The action understanding problem has been extensively studied in robotics and computer vision over the last few years. 

Our work differs in that we are concerned with completing a known task, and focus on when to ask questions as opposed to what type of question to ask

}




