\section{Translating Videos to Commands} \label{Sec:rnn}
We start by formulating the problem and briefly describing two popular RNNs use in our method: Long-Short Term Memory (LSTM)~\cite{Hochreiter97_LSTM} and Gated Recurrent Neural network (GRU)~\cite{Cho14_GRU}. Then we present the network architecture that translates the input videos to robotic commands.

\subsection{Problem Formulation}
We cast the problem of translating videos to commands as a video captioning task. In particular, the input video is considered as a list of frames, presented by a sequence of features  $\mathbf{X} = (\mathbf{x}_1, \mathbf{x}_2, ..., \mathbf{x}_n)$ from each frame. The output command is presented as a sequence of word vectors $\mathbf{Y}=(\mathbf{y}_1, \mathbf{y}_2, ..., \mathbf{y}_m)$, in which each vector $\mathbf{y}$ represents one word in the dictionary $D$. The video captioning task is to find for each sequence feature $\mathbf{X}_i$ its most probable command $\mathbf{Y}_i$. In practice, the number of video frames $n$ is usually greater than the number of words $m$. To make the problem become more suitable for robotic applications, we use a dataset that contains mainly human's manipulation actions and assume that the output command $\mathbf{Y}$ is in grammar-free format.




\subsection{Recurrent Neural Networks}

\subsubsection{LSTM}
LSTM is a well-known RNN for effectively modelling the long-term dependencies from the input data. The core of an LSTM network is a memory cell $\mathbf{c}$ which has the gate mechanism to encode the knowledge of the previous inputs at every time step. In particular, the LSTM takes an input $\mathbf{x}_t$ at each time step $t$, and computes the hidden state $\mathbf{h}_t$ and the memory cell state $\mathbf{c}_t$ as follows:

\begin{equation}
\label{Eq_LSTM} 
\begin{aligned} 
{\mathbf{i}_t} &= \sigma ({\mathbf{W}_{xi}}{\mathbf{x}_t} + {\mathbf{W}_{hi}}{\mathbf{h}_{t - 1}} + {\mathbf{b}_i})\\
{\mathbf{f}_t} &= \sigma ({\mathbf{W}_{xf}}{\mathbf{x}_t} + {\mathbf{W}_{hf}}{\mathbf{h}_{t - 1}} + {\mathbf{b}_f})\\
{\mathbf{o}_t} &= \sigma ({\mathbf{W}_{xo}}{\mathbf{x}_t} + {\mathbf{W}_{ho}}{\mathbf{h}_{t - 1}} + {\mathbf{b}_o})\\
{\mathbf{g}_t} &= \phi ({\mathbf{W}_{xg}}{\mathbf{x}_t} + {\mathbf{W}_{hg}}{h_{t - 1}} + {\mathbf{b}_g})\\
{\mathbf{c}_t} &= {\mathbf{f}_t} \odot {\mathbf{c}_{t - 1}} + {\mathbf{i}_t} \odot {\mathbf{g}_t}\\
{\mathbf{h}_t} &= {\mathbf{o}_t} \odot \phi ({\mathbf{c}_t})
\end{aligned}
\end{equation}
where $\odot$ represents element-wise multiplication, the function $\sigma: \mathbb{R} \mapsto [0,1], \sigma (x) = \frac{1}{{1 + {e^{ - x}}}}$ is the sigmod non-linearity, and $\phi: \mathbb{R} \mapsto [ - 1,1], \phi (x) = \frac{{{e^x} - {e^{ - x}}}}{{{e^x} + {e^{ - x}}}}$ is the hyperbolic tangent non-linearity. The weight $W$ and bias $b$ are trained parameters. With this gate mechanism, the LSTM network can remember or forget information for long periods of time, while is still robust against vanishing or exploding gradient problems. In practice, the LSTM network is straightforward to train end-to-end and can handle inputs with different lengths using the padding techniques.

\subsubsection{GRU}
A popular variation of the LSTM network is the GRU proposed by Cho et al.~\cite{Cho14_GRU}. The main advantage of the GRU network is that it requires fewer computations in comparison with the standard LSTM, while the accuracy between these two networks are competitive. Unlike the LSTM network, in a GRU, the update gate controls both the input and forget gates, and the reset gate is applied before the nonlinear transformation as follows:

\begin{figure*}[!htbp] 
    \centering
    %\includegraphics[width=0.95\linewidth, height=0.3\linewidth]{figures/3_acnn/architecture/acnn2.pdf}

	%\includegraphics[scale=0.72]{figures/3_detection/overview2.pdf}    				
	%\includegraphics[width=0.99\linewidth, height=0.20\linewidth]{example-image-a} 
	%\includegraphics[width=0.99\linewidth, height=0.26\linewidth]{figures/3_method/overview.pdf} 
	%\includegraphics[scale=0.31]{figures/3_method/overview.pdf} 
	%\includegraphics[scale=0.31]{figures/3_method/overview2.pdf} 
	\includegraphics[scale=0.31]{figures/3_method/overview3.pdf} 
    \vspace{1.0ex}
    \caption{An overview of our approach. We first extract the deep features from the input frames using CNN. Then the first LSTM/GRU layer is used to encode the visual features. The input words are fed to the second LSTM/GRU layer and this layer sequentially generates the output words.}
    \label{Fig:overview} 
\end{figure*}


\begin{equation}
\label{Eq_GRU} 
\begin{aligned} 
{\mathbf{r}_t} &= \sigma ({\mathbf{W}_{xr}}{\mathbf{x}_t} + {\mathbf{W}_{hr}}{\mathbf{h}_{t - 1}} + {\mathbf{b}_r})\\
{\mathbf{z}_t} &= \sigma ({\mathbf{W}_{xz}}{\mathbf{x}_t} + {\mathbf{W}_{hz}}{\mathbf{h}_{t - 1}} + {\mathbf{b}_z})\\
{\mathbf{{\tilde h}}_t} &= \phi ({\mathbf{W}_{xh}}{\mathbf{x}_t} + {\mathbf{W}_{hh}}({\mathbf{r}_t} \odot {\mathbf{h}_{t - 1}}) + {\mathbf{b}_h}\\
{\mathbf{h}_t} &= {\mathbf{z}_t} \odot {\mathbf{h}_{t - 1}} + (1 - {\mathbf{z}_t}) \odot {\mathbf{{\tilde h}}_t}
\end{aligned}
\end{equation}
where $\mathbf{r}_t$, $\mathbf{z}_t$, $\mathbf{h}_t$ represent the reset, update, and hidden gate respectively.

\subsection{Videos to Commands}

\subsubsection{Command Embedding} Since a command is a list of words, we have to represent each word as a vector for computation. There are two popular techniques for word representation: \textit{one-hot} encoding and \textit{word2vec}~\cite{Mikolov2013} embedding. Although the one-hot vector is high dimensional and sparse since its dimensionality grows linearly with the number of words in the vocabulary, it is straightforward to use this embedding in the video captioning task. In this work, we choose the one-hot encoding technique as our word representation since the number of words in our dictionary is relatively small (i.e., $|D|=128$). The one-hot vector $\mathbf{y} \in {\mathbb{R}^{|D|}}$ is a binary vector with only one non-zero entry indicating the index of the current word in the vocabulary. Formally, each value in the one-hot vector $\mathbf{y}$ is defined by:

 \begin{equation}
    \mathbf{y}^j=
    \begin{cases}
      1, & \text{if}\ j=ind(\mathbf{y}) \\
      0, & \text{otherwise}
    \end{cases}
  \end{equation}
where $ind(\mathbf{y})$ is the index of the current word in the dictionary $D$. In practice, we add an extra word \textsf{EOC} to the dictionary to denote the end of command sentences.

\subsubsection{Visual Features}
We first sample $n$ frames from each input video in order to extract deep features from the images. The frames are selected uniformly with the same interval if the video is too long. In case the video is too short and there are not enough $n$ frames, we create an artificial frame from the mean pixel values of the ImageNet dataset~\cite{Olga2015} and pad this frame at the end of the list until it reaches $n$ frames. We then use the state-of-the-art CNN to extract deep features from these input frames. Since the visual features provide the key information for the learning process, three popular CNN are used in our experiments: VGG16~\cite{SimonyanZ14}, Inception\_v3~\cite{Szegedy16_Inception}, and ResNet50~\cite{He2016}.

Specifically, for the VGG16 network, the features are extracted from its last fully connected $\texttt{fc2}$ layer. For the Inception\_v3 network, we extract the features from its $\texttt{pool\_3:0}$ tensor. Finally, we use the features from $\texttt{pool5}$ layer of the ResNet50 network. The dimension of the extracted features is $4096$, $2048$, $2048$, for the VGG16, Inception\_v3, and ResNet50 network, respectively. All these CNN are pretrained on ImageNet dataset for image classifications. We notice that the names of the layers we mention here are based on the Tensorflow~\cite{TensorFlow2015} implementation of these networks.



\subsubsection{Architecture}
Our architecture is based on the encoder-decoder scheme~\cite{Venugopalan2016}~\cite{venugopalan2014translating}~\cite{Ramanishka2017cvpr}, which is adapted from the popular sequence to sequence model~\cite{Sutskever2014_Seq} in machine translation. Although recent approaches to video captioning problem use attention mechanism~\cite{Ramanishka2017cvpr} or hierarchical RNN~\cite{Haonan2016}, our proposal solely relies on the neural architecture. Based on the input data characteristics, our network smoothly encodes the input visual features and generates the output commands, achieving a fair improvement over the state of the art without using any additional modules.

In particular, given an input video, we first extract visual features from the video frames using the pretrained CNN network. These features are encoded in the first RNN layer to create the encoder hidden state. The input words are then fed to the second RNN layer, and this layer will decode sequentially to generate a list of words as the output command. Fig.~\ref{Fig:overview} shows an overview of our approach. More formally, given an input sequence of features $\mathbf{X} = (\mathbf{x}_1, \mathbf{x}_2, ..., \mathbf{x}_n)$, we want to estimate the conditional probability for an output command  $\mathbf{Y}=(\mathbf{y}_1, \mathbf{y}_2, ..., \mathbf{y}_m)$ as follows:
%\begin{equation} \label{Eq_mainP}
%P(\mathbf{y}_1, \mathbf{y}_2, ..., \mathbf{y}_m | \mathbf{x}_1, \mathbf{x}_2, ..., \mathbf{x}_n)
%\end{equation}
\begin{equation} \label{Eq_mainP1}
P(\mathbf{y}_1, ..., \mathbf{y}_m | \mathbf{x}_1, ..., \mathbf{x}_n) = \prod\limits_{i = 1}^m {P({\mathbf{y}_i}|{\mathbf{y}_{i - 1}}, ...,{\mathbf{y}_1}},\mathbf{X})
\end{equation}

Since we want a generative model that encodes a sequence of features and produces a sequence of words in order as a command, the LSTM/GRU is well suitable for this task. Another advantage of LSTM/GRU is that they can model the long-term dependencies in the input features and the output words. In practice, we conduct experiments with the LSTM and GRU network as our RNN, while the input visual features are extracted from the VGG16, Inception\_v3, and ResNet50 network, respectively.

In the encoding stage, the first LSTM/GRU layer converts the visual features $\mathbf{X} = (\mathbf{x}_1, \mathbf{x}_2, ..., \mathbf{x}_n)$ to a list of hidden state vectors $\mathbf{H}^e = (\mathbf{h}_1^e, \mathbf{h}_2^e, ..., \mathbf{h}_n^e)$ (using Equation~\ref{Eq_LSTM} for LSTM or Equation~\ref{Eq_GRU} for GRU). Unlike~\cite{venugopalan2014translating} which takes the average of all $n$ hidden state vectors to create a fixed-length vector, we directly use each hidden vector $\mathbf{h}_i^e$ as the input $\mathbf{x}_i^d$ for the second decoder layer. This allows the smooth transaction from the visual features to the output commands without worrying about the harsh average pooling operation, which can lead to the loss of temporal structure underlying the input video.

In the decoding stage, the second LSTM/GRU layer converts the list of hidden encoder vectors $\mathbf{H}^e$ into the sequence of hidden decoder vectors $\mathbf{H}^d$. The final list of predicted words $\mathbf{\hat{Y}}$ is achieved by applying a softmax layer on the output $\mathbf{H}^d$ of the LSTM/GRU decoder layer. In particular, at each time step $t$, the output $\mathbf{z}_t$ of each LSTM/GRU cell in the decoder layer is passed though a linear prediction layer $\hat{\mathbf{y}}=\mathbf{W}_{z}\mathbf{z}_t+\mathbf{b}_z$, and the predicted distribution $P(\mathbf{y}_t)$ is computed by taking the softmax of $\hat{\mathbf{y}_t}$ as follows:


\begin{equation}
P(\mathbf{y}_t=\mathbf{w}|\mathbf{z}_t)=\frac{\text{exp}(\hat{\mathbf{y}}_{t,w})}{\sum_{w'\in{D}}\text{exp}(\hat{\mathbf{y}}_{t,w'})}
\end{equation}
where $\mathbf{W}_z$ and $\mathbf{b}_z$ are learned parameters, $\mathbf{w}$ is a word in the dictionary $D$.

In this way, the LSTM/GRU decoder layer sequentially generates a conditional probability distribution for each word of the output command given the encoded features representation and all the previously generated words. In practice, we preprocess the data so that the number of input words $m$ is equal to the number of input frames $n$. For the input video, this is done by uniformly sampling $n$ frames in the long video, or padding the extra frame if the video is too short. Since the number of words $m$ in the input commands is always smaller than $n$, we pad a special empty word to the list until we have $n$ words.
 

\subsubsection{Training}

The network is trained end-to-end with Adam optimizer~\cite{kingma2014adam} using the following objective function:

\begin{equation} \label{Eq_LossFunction}
\mathop {\arg \max }\limits_\theta  \sum\limits_{i = 1}^m {logP({\mathbf{y}_i}|{\mathbf{y}_{i - 1}},...,{\mathbf{y}_1};\theta)} 
\end{equation}
where $\theta$ represents the parameters of the network.




%\begin{table*}[!ht]
%\centering\ra{1.3}
%\caption{Affordance Detection Results}
%\renewcommand\tabcolsep{5.0pt}
%\label{tb_result_v2c}
%\hspace{2ex}
%
%\begin{tabular}{@{}rcccccccc@{}}
%\toprule 					 &
%Bleu\_1  & 
%Bleu\_2  & 
%%\multirow{1}{*}[2.5pt]{\scriptsize DeepLab~\cite{Chen2016_deeplab}} & 
%Bleu\_3  &
%Bleu\_4  & 
%%\mu\ltirow{1}{*}[2.5pt]{\scriptsize BB-CNN} & 
%METEOR  &
%ROUGE\_L &
%CIDEr \\
%
%
%\midrule
%S2VT~\cite{Venugopalan2016} 				& 0.383   & 0.265   & 0.201	& 0.159	& 0.183    & 0.382   & 1.431     \\
%SGC~\cite{Ramanishka2017cvpr}			& 0.370   & 0.256   & 0.198	& 0.161	& 0.179    & 0.371   & 1.422     \\
%\cline{1-8}
%LSTM\_VGG16					& 0.372   & 0.255  			 & 0.193	& 0.159	& 0.180    & 0.375   & 1.395     \\
%GRU\_VGG16 					& 0.350   & 0.233 			 & 0.173	& 0.137	& 0.168    & 0.351   & 1.255     \\
%LSTM\_Inception\_v3				& \textbf{0.400}  			 & \textbf{0.286}   & 0.221	& 0.178	& \textbf{0.194}    & \textbf{0.402}   & \textbf{1.594}     \\
%GRU\_Inception\_v3 				& 0.391   & 0.281  			 & \textbf{0.222}	& \textbf{0.188}	& 0.190    & 0.398   & 1.588     \\
%LSTM\_ResNet50 				& 0.398   & 0.279            & 0.215	& 0.174	& 0.193    & 0.398   & 1.550     \\
%GRU\_ResNet50 				& 0.398   & 0.284   & 0.220	& 0.183	& 0.193    & 0.399   & 1.567     \\
%\bottomrule
%\end{tabular}
%\end{table*}




During the training phase, at each time step $t$, the input feature $\mathbf{x}_t$ is fed to an LSTM/GRU cell in the encoder layer along with the previous hidden state $\mathbf{h}_{t-1}^e$ to produce the current hidden state $\mathbf{h}_t^e$. After all the input features are exhausted, the word embedding and the hidden states of the first LSTM/GRU encoder layer are fed to the second LSTM/GRU decoder layer. This decoder layer converts the inputs into a sequence of words by maximizing the log-likelihood of the predicted word (Equation~\ref{Eq_LossFunction}). This decoding process is performed sequentially for each word until the network generates the end-of-command (\textsf{EOC}) token.















\junk{
This layer models the probability distribution of the next word over the word space as follow:
  
The key different in our architecture in comparision with~\cite{venugopalan2014translating} is we do not do average pooling when feeding frame features to LSTM/GRU, instead each feature is fed directly to LSTM/GRU. If the number of frame is less than the number of LSTM step, we pad the missing frames with the mean image from ImageNet dataset. TODO: Compare more.

In the first several time steps, the top LSTM layer (colored red in Figure 2) receives a se- quence of frames and encodes them while the secondLSTM layer receives the hidden representation (ht) and concate- nates it with null padded input words (zeros), which it then encodes. There is no loss during this stage when the LSTMs are encoding. After all the frames in the video clip are ex- hausted, the second LSTM layer is fed the beginning-of- sentence (<BOS>) tag, which prompts it to start decoding its current hidden representation into a sequence of words. While training in the decoding stage, the model maximizes for the log-likelihood of the predicted output sentence given the hidden representation of the visual frame sequence, and the previouswords it has seen. From Equation 3 for a model with parameters θ and output sequence Y = (y1, . . . , ym), this is formulated as



During training the visual feature, sentence pair (V,S) is provided to the model, which then optimizes the log-likelihood (Equation 1) over the entire training dataset using stochastic gradient descent. At each time step, the input xt is fed to the LSTM along with the previous time step’s hidden state ht−1 and the LSTM emits the next hidden state vector ht (and a word). For the first layer of the LSTM xt is the con- catenation of the visual feature vector and the pre- vious encoded word, the ground truth word during training and the predicted word during test time). For the second layer of the LSTM xt is zt of the first layer. Accordingly, inference must also be performed sequentially in the order h1 = fW(x1, 0), h2 = fW(x2,h1), until the model emits the end- of-sentence (EOS) token at the final step T. In our model the output (ht = zt) of the second layer LSTM unit is used to obtain the emitted word





 It updates its hid- den state every time a new word arrives, and encodes the sentence semantics in a compact form up to the words that have been fed in.
 
 
 decoder is conditioned step by step on the first t words of the caption and on the corresponding video descriptor, and is tr111ained to produce the next word of the caption. The objective function which we optimize is the log-likelihood of correct words over the sequence


In practice, (THEORY) - PRACTICE These features are then fed to a RNN (LSTM or GRU) to encode them to a fixed length vector $\mathbf{z}$. This process is done by extracting the hidden state vector $h_i^e$ from LSTM/GRU. The middle state is computed by taking the average of all $m$ feature descriptors:

\begin{equation}
z = \frac{1}{m}\sum\limits_{i = 1}^m {{x_i}}
\end{equation}

The decoder converts the encoded vector $z$ into output sequence of words $y_t$, $t \in {1, . . . ,n}$. In particular,


Specifically, for the VGG16 net, the feature map is extracted from its last fully connected layer (i.e. $\textit{fc2} \in {\mathbb{R}^{4096}}$). For the Inception\_v3 network, we extract the features from its $\textit{pool\_3:0} \in {\mathbb{R}^{2048}}$ tensor. Finally, we use the features from $\textit{pool5} \in {\mathbb{R}^{2048}}$ layer of ResNet50.

During the training phase, the frame features and  groundtruth one-hot vectors are used to reproduce these vectors. In the test time, the list of one-hot vectors is generated based on just the input features, which represents the output sentence.


We apply our approach to both still image and video description scenarios, adapting a popular encoder-decoder model for video captioning [22] as our base model.



In this section, we first briefly review two popular RNN that we use in our experiments: Long Short-Term Memory (LSTM)~\cite{Hochreiter97_LSTM} and a variation of the LSTM called Gated Recurrent Units (GRU)~\cite{Cho14_GRU}. We then describe our approach to directly translate videos to robot commands. Fig.~\ref{Fig:overview} shows an overview of our approach.



Commands: list of words (in grammar free format)
Video: list of frames represent a short human action. Although we can describe anything, in this paper, we only focus on manipulation actions of human. More formally, let: 

\begin{equation}
 F = ({f_1},{f_2},...,{f_n})
 \end{equation} 
represent a list of features extracted from the video, and:
\begin{equation}
 C = ({c_1},{c_2},...,{c_m})
 \end{equation} 
 
represent a list of words in a command. 



that We first briefly describe the Long-Short Term Memory (LSTM) network, which has shown great success to sequential problems such as machine translation~\cite{s}, image captioning~\cite{•}
 
 
Recently, Recurrent Neural Networks (RNNs) have received a lot of attention and achieved impressive results in various applications, including speech recognition [25], image captioning [26] and video description [27]. RNNs have also been applied to facial landmark detection. Very recently, Peng et al. [28] propose a recurrent encoder-decoder network for video-based sequential facial landmark detection. They use recurrent networks to align a sequence of the same face in video frames, by exploiting recurrent learning in spatial and temporal dimensions.


In this section, we propose an approach to translate the input videos to robot commands. Our approach is based on deep RNN models (LSTM and GRU), which have been successfully used in various problems such as machine translation~\cite{Ilya2014}, image captioning~\cite{Donahue2014}, video description~\cite{Venugopalan2016}. 
 
 
These one-hot vectors are then projected into an embedding space with dimension de by multiplication Weyt with a learned parameter matrix We ∈ Rde×K. The result of a matrix- vector multiplication with a one-hot vector is the column of the matrix corresponding to the index of the single non- zero component of the one-hot vector. We can therefore be thought of as a “lookup table,” mapping each of the K words in the vocabulary to a de-dimensional vector.


plus one additional entry for the <BOS> (beginning of sequence) token which is always taken as y0, the vector $y \int RK$ with a single non-zero component yi = 1 denoting the ith word in the vocabulary. 
a binary vector with exactly one non-zero entry at the position indicating the index of the word in the vocabulary

In addition, the proposed recurrent network in this paper contains multiple Long-Short Term Memory (LSTM) components. For self-containness, we give a brief introduction to LSTM. LSTM is one type of the RNNs, which is attractive because it is explicitly designed to remember information for long periods of time. LSTM takes xt, ht−1 and ct−1 as inputs, ht and ct as outputs

where σ(x) = (1 + e−x)−1 is the sigmoid function. The outputs of the sigmoid functions are in the range of [0, 1], here the smaller values indicate “more probability to forget the previous information” and the larger values indicate “more probability to remember the information”. tanh(x)is the tangent non-linearity function and its outputs are in the range [−1, 1]. [x;h] represents the concatenation of x and h. LSTM has four gates, a forget gate ft, an input gate it, an output gate ot and an input modulation gate gt. ht and ct are the outputs. ht is the hidden unit. ct is the memory cell that can be learned to control whether the previous information should be forgotten or remembered.


At time step t, the input to the bottom-most LSTM is the embedded word from the previous time step yt−1. Input words are encoded as one-hot vectors: vectors $y \int RK$ with a single non-zero component yi = 1 denoting the ith word in the vocabulary, where K is the number of words in the vocabulary, plus one additional entry for the <BOS> (beginning of sequence) token which is always taken as y0, the “previous word” at the first time step (t = 1). These one-hot vectors are then projected into an embedding space with dimension de by multiplication Weyt with a learned parameter matrix We ∈ Rde×K. The result of a matrix- vector multiplication with a one-hot vector is the column of the matrix corresponding to the index of the single non- zero component of the one-hot vector. We can therefore be thought of as a “lookup table,” mapping each of the K words in the vocabulary to a de-dimensional vector.
}


