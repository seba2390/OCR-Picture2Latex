\section{EXPERIMENTS} \label{Sec:exp}
\subsection{Dataset}

Recently, the task of describing video using natural language has gradually received more interest in the computer vision community. Eventually, many video description datasets have been released~\cite{Xu16_MSR_Dataset}. However, these datasets only provide general descriptions of the video and there is no detailed understanding of the action. The captions are also written using natural language sentences which can not be used directly in robotic applications. Motivated by these limitations, we introduce a new \textit{video to command} (IIT-V2C) dataset which focuses on \textit{fine-grained} action understanding~\cite{lea2016learning}. Our goal is to create a new large scale dataset that provides fine-grained understanding of human actions in a grammar-free format. This is more suitable for robotic applications and can be used with deep learning methods.

\textbf{Video annotation} 
Since our main purpose is to develop a framework that can be used by real robots for manipulation tasks, we use only videos that contain human actions. To this end, the raw videos in the Breakfast dataset~\cite{Kuehne14_BF_Dataset} are best suited to our purpose since they were originally designed for activity recognition. We only reuse the raw videos from this dataset and manually segment each video into short clips in a fine granularity level. Each short clip is then annotated with a \textit{command sentence} that describes the current human action.

\textbf{Dataset statistics} 
In particular, we reuse $419$ videos from the Breakfast dataset. The dataset contains $52$ unique participants performing cooking tasks in different kitchens. We segment each video (approximately $2-3$ minutes long) into around $10-50$ short clips (approximately $1-15$ seconds long), resulting in $11,000$ unique short videos. Each short video has a single command sentence that describes human actions. We use $70\%$ of the dataset for training and the remaining $30\%$ for testing. Although our new-form dataset is characterized by its grammar-free property for the convenience in robotic applications, it can easily be adapted to classical video captioning task by adding the full natural sentences as the new groundtruth for each video.


\subsection{Evaluation Metric, Baseline, and Implementation}
\textbf{Evaluation Metric} We report the experimental results using the standard metrics in the captioning task~\cite{Xu16_MSR_Dataset}: BLEU, METEOR, ROUGE-L, and CIDEr. This makes our results directly comparable with the recent state-of-the-art methods in the video captioning field.

\textbf{Baseline} We compare our results with two recent methods in the video captioning field: S2VT~\cite{Venugopalan2016} and SGC~\cite{Ramanishka2017cvpr}. The authors of S2VT used LSTM in the encoder-decoder architecture, while the inputs are from the features of RGB images (extracted by VGG16) and optical flow images (extracted by AlexNet). SGC also used LSTM with encoder-decoder architecture, however, this work integrated a saliency guided method as the attention mechanism, while the features are from Inception\_v3. We use the code provided by the authors of the associated papers for the fair comparison.


\begin{figure*}
\centering
\footnotesize
 %\stackunder[5pt]{\includegraphics[width=0.49\linewidth, height=0.09\linewidth]{figures/4_exp/result.pdf}}
  \stackunder[2pt]{\includegraphics[width=0.49\linewidth, height=0.09\linewidth]{figures/4_exp/P13_pancake_82.jpg}}  				
  				  {\tableCaption {righthand carry spatula} {righthand carry spatula} {lefthand reach stove} {lefthand reach pan}}
  				  \vspace{2ex} 
  \hspace{0.25cm}%
  \stackunder[2pt]{\includegraphics[width=0.49\linewidth, height=0.09\linewidth]{figures/4_exp/P38_salad_2.jpg}}  
  				  {\tableCaption {righthand cut fruit} {righthand cut fruit} {righthand cut fruit} {righthand cut fruit}}
  				  \vspace{2ex}
 %\stackunder[5pt]{\includegraphics[width=0.49\linewidth, height=0.09\linewidth]{figures/4_exp/result.pdf}} 
  \stackunder[2pt]{\includegraphics[width=0.49\linewidth, height=0.09\linewidth]{figures/4_exp/p07_pancake_6.jpg}}  
    			  {\tableCaption {righthand crack egg} {righthand carry egg} {lefthand reach spatula} {righthand carry egg}}
  				  \vspace{0ex}
  \hspace{0.25cm}%
 %\stackunder[5pt]{\includegraphics[width=0.49\linewidth, height=0.09\linewidth]{figures/4_exp/result.pdf}}  
  \stackunder[2pt]{\includegraphics[width=0.49\linewidth, height=0.09\linewidth]{figures/4_exp/P43_milk_15.jpg}}    
				  {\tableCaption {righthand stir milk} {righthand hold teabag} {righthand place kettle} {righthand take cacao}}
  				  \vspace{0ex}
  
  %\hspace{-0.25cm}%
 %\hspace{-0.25cm}%
\vspace{1ex}
\caption{Example of translation results of the S2VT, SGC and our LSTM\_Inception\_v3 network on the IIT-V2C dataset.}

\label{Fig:main_result} 
\end{figure*}


\textbf{Implementation} We use $512$ hidden units in both LSTM and GRU in our implementation. The first hidden state of LSTM/GRU is initialized uniformly in $[-0.1, 0.1]$. We set the number of frames for each input video at $30$. Sequentially, we consider each command has maximum $30$ words. If there are not enough $30$ frames/words in the input video/command, we pad the mean frame (from ImageNet dataset)/empty word at the end of the list until it reaches $30$.  During training, we only accumulate the softmax losses of the real words to the total loss, while the losses from the empty words are ignored. We train all the networks for $150$ epochs using Adam optimizer with a learning rate of $0.0001$. The batch size is empirically set to $16$. The training time for each network is around $3$ hours on a NVIDA Titan X GPU. 


\subsection{Results}

%//////////////////////////////////////////////
\begin{table}[!ht]
\centering\ra{1.4}
\caption{Performance on IIT-V2C Dataset}
\renewcommand\tabcolsep{2.5pt}
\label{tb_result_v2c}
\hspace{2ex}

\begin{tabular}{@{}rcccccccc@{}}
\toprule 					 &
\ssmall Bleu\_1  & 
\ssmall Bleu\_2  & 
%\multirow{1}{*}[2.5pt]{\scriptsize DeepLab~\cite{Chen2016_deeplab}} & 
\ssmall Bleu\_3  &
\ssmall Bleu\_4  & 
%\mu\ltirow{1}{*}[2.5pt]{\scriptsize BB-CNN} & 
\ssmall METEOR  &
\ssmall ROUGE\_L &
\ssmall CIDEr \\


\midrule
S2VT~\cite{Venugopalan2016} 				& 0.383   & 0.265   & 0.201	& 0.159	& 0.183    & 0.382   & 1.431     \\
SGC~\cite{Ramanishka2017cvpr}			& 0.370   & 0.256   & 0.198	& 0.161	& 0.179    & 0.371   & 1.422     \\
\cline{1-8}
LSTM\_VGG16					& 0.372   & 0.255  			 & 0.193	& 0.159	& 0.180    & 0.375   & 1.395     \\
GRU\_VGG16 					& 0.350   & 0.233 			 & 0.173	& 0.137	& 0.168    & 0.351   & 1.255     \\
LSTM\_Inception\_v3				& \textbf{0.400}  			 & \textbf{0.286}   & 0.221	& 0.178	& \textbf{0.194}    & \textbf{0.402}   & \textbf{1.594}     \\
GRU\_Inception\_v3 				& 0.391   & 0.281  			 & \textbf{0.222}	& \textbf{0.188}	& 0.190    & 0.398   & 1.588     \\
LSTM\_ResNet50 				& 0.398   & 0.279            & 0.215	& 0.174	& 0.193    & 0.398   & 1.550     \\
GRU\_ResNet50 				& 0.398   & 0.284   & 0.220	& 0.183	& 0.193    & 0.399   & 1.567     \\
\bottomrule
\end{tabular}
\end{table}

Table~\ref{tb_result_v2c} summarizes the captioning results on the IIT-V2C dataset. Overall, the LSTM network that uses visual features from Inception\_v3 (LSTM\_Inception\_v3) achieves the highest performance, winning on the Blue\_1, Blue\_2, METEOR, ROUGE\_L, and CIDEr metrics. Our LSTM\_Inception\_v3 also outperforms S2VT and SGC in all metrics by a fair margin. We also notice that both the LSTM\_ResNet50 and GRU\_ResNet50 networks give competitive results in comparison with the LSTM\_Inception\_v3 network. Overall, we observe that the architectures that use LSTM give slightly better results than those using GRU. However, this difference is not significant when the ResNet50 features are used to train the models (LSTM\_ResNet50 and GRU\_ResNet50 results are a tie).


From the experiments, we notice that there are two main factors that affect the results of this problem: the network architecture and the input visual features. Since the IIT-V2C dataset contains mainly the fine-grained human actions in a limited environment (i.e., the kitchen), the SGC architecture that used saliency guide as the attention mechanism does not perform well as in the normal video captioning task. On the other hand, the visual features strongly affect the final results. Our experiments show that the ResNet50 and Inception\_v3 features significantly outperform the VGG16 features in both LSTM and GRU networks. Since the visual features are not re-trained in the sequence to sequence model, in practice it is crucial to choose the state-of-the-art CNN as the feature extractor for the best performance.


Fig.~\ref{Fig:main_result} shows some examples of the generated commands by our LSTM\_Inception\_v3, S2VT, and SGC models on the test videos of the IIT-V2C dataset. These qualitative results show that our LSTM\_Inception\_v3 gives good predictions in many cases, while S2VT and SGC results are more variable. In addition to the good predictions that are identical with the groundtruth, we note that many other generated commands are relevant. Due to the nature of the IIT-V2C dataset, most of the videos are short and contain fine-grained human manipulation actions, while the groundtruth commands are also very short. This makes the problem of translating videos to commands is more challenging than the normal video captioning task since the network has to rely on the minimal information to predict the output command.  


\subsection{Robotic Applications}

%
%\begin{figure*}
%\centering
%\footnotesize
% %\stackunder[5pt]{\includegraphics[width=0.49\linewidth, height=0.09\linewidth]{figures/4_exp/result.pdf}}
%  \stackunder[2pt]{\includegraphics[width=0.49\linewidth, height=0.14\linewidth]{figures/5_robot/human_pick_hammer_cut/human_pick_all.jpg}}  				
%  				  {Human instruction}
%  				  \vspace{2ex} 
%  \hspace{0.25cm}%
%  \stackunder[2pt]{\includegraphics[width=0.49\linewidth, height=0.14\linewidth]{figures/5_robot/pick_place_hammer_cut/pick_all.jpg}}  
%  				  {Robot execution}
%  				  \vspace{2ex}
% %\stackunder[5pt]{\includegraphics[width=0.49\linewidth, height=0.09\linewidth]{figures/4_exp/result.pdf}} 
%  \stackunder[2pt]{\includegraphics[width=0.49\linewidth, height=0.14\linewidth]{figures/5_robot/human_pour_bottle_cut/human_pour_all.jpg}}  
%    			  {Human instruction}
%  				  \vspace{0ex}
%  \hspace{0.25cm}%
% %\stackunder[5pt]{\includegraphics[width=0.49\linewidth, height=0.09\linewidth]{figures/4_exp/result.pdf}}  
%  \stackunder[2pt]{\includegraphics[width=0.49\linewidth, height=0.14\linewidth]{figures/5_robot/pour_bottle_cut/pour_all.jpg}}    
%				  {Robot execution}
%  				  \vspace{0ex}
%%  \stackunder[2pt]{\includegraphics[width=0.49\linewidth, height=0.13\linewidth]{figures/5_robot/pick_place_hammer_cut/pick_all.jpg}}    
%%				  {\tableCaption {righthand stir milk} {righthand hold teabag} {righthand place kettle} {righthand take cacao}}
%%  				  \vspace{0ex}
%%  \stackunder[2pt]{\includegraphics[width=0.49\linewidth, height=0.13\linewidth]{figures/5_robot/pick_place_hammer_cut/pick_all.jpg}}    
%%				  {\tableCaption {righthand stir milk} {righthand hold teabag} {righthand place kettle} {righthand take cacao}}
%%  				  \vspace{0ex}
%    
%  %\hspace{0.35cm}%
% %\hspace{-0.25cm}%
%\vspace{3ex}
%\caption{ROBOT Example Results. TBD.}
%\label{Fig:robot_imitation} 
%\end{figure*}


\begin{figure*}[ht]
  \centering
 \subfigure[Pick and place task]{\label{fig_resize_map_a}\includegraphics[width=0.99\linewidth, height=0.16\linewidth]{figures/5_robot/pick_all_in_one.pdf}}
 \subfigure[Pouring task]{\label{fig_resize_map_b}\includegraphics[width=0.99\linewidth, height=0.16\linewidth]{figures/5_robot/pour_all_in_one.pdf}}
    
     
 \vspace{2.0ex}
 \caption{Example of manipulation tasks performed by WALK-MAN using our proposed framework. \textbf{(a)} Pick and place task. \textbf{(b)} Pouring task. The frames from human instruction videos are on the left side, while the robot performs actions on the right side. We notice that there are two sub-tasks (i.e., two commands) in these tasks: grasping the object and manipulating it. More illustrations can be found in the supplemental video.}
 \label{Fig:robot_imitation}
\end{figure*}



Given the proposed translation module, we build a robotic framework that allows the robot to perform various manipulation tasks by just ``\textit{watching}" the input video. Our goal in this work is similar to~\cite{Yang2015}, however, we propose to keep the video understanding separately from the vision system. In this way, the robot can learn to understand the task and execute it independently. This makes the proposed approach more practical since it does not require a dataset that has both the caption and the object (or grasping) location. It is also important to note that our goals differ from LfD since we only focus on finding a general way to let the robot execute different manipulation actions, while the trajectory in each action is assumed to be known.

In particular, for each task presented by a video, the translation module will generate an output command sentence. Based on this command, the robot uses its vision system to find relevant objects and plan the actions. Experiments are conducted using the humanoid WALK-MAN~\cite{Niko2016_full}. The robot is controlled using the XBotCore software architecture~\cite{muratore2017xbotcore}, while the OpenSoT library~\cite{Rocchi15} is used to plan full-body motion. The relevant objects and their affordances are detected using AffordanceNet framework~\cite{AffordanceNet17}. For simplicity, we only use objects in the IIT-Aff dataset~\cite{Nguyen2017_Aff} in the demonstration videos so the robot can recognize them. Using this setup, the robot can successfully perform various manipulation tasks by closing the loop: understanding the human demonstration from the video using the proposed method, finding the relevant objects and grasping poses~\cite{Nguyen2017_Aff}, and planning for each action~\cite{Rocchi15}.


Fig.~\ref{Fig:robot_imitation} shows some manipulation tasks performed by WALK-MAN using our proposed framework. For a simple task such as ``righthand grasp bottle", the robot can effectively repeat the human action through the command. Since the output of our translation module is in grammar-free format, we can directly map each word in the command sentence to the real robot command. In this way, we avoid using other modules as in~\cite{Tellex2011} to parse the natural command into the one that uses in the real robot. The visual system also plays an important role in our framework since it provides the object location and the target frames (e.g., grasping frame, ending frame) for the robot to plan the actions. Using our approach, the robot can also complete long manipulation tasks by stacking a list of demonstration videos in order for the translation module. Note that, for the long manipulation tasks, we assume that the ending state of one task will be the starting state of the next task. Overall, WALK-MAN successfully performs various manipulation tasks such as grasping, pick and place, or pouring. The experimental video and our IIT-V2C dataset can be found at the following link:
\vspace{1ex}
\centerline{\url{https://sites.google.com/site/video2command/}}






\junk{
 Furthermore, we can also  We can also stack several videos to create a chain of commands. 
 
Using our approach, the robot can perform different tasks based on the instructions from human demonstration videos.

  can understand the human instructions from the video via the proposed translation module, while the visual information can be solved effectively with the recent advances in deep learning~\cite{Nguyen2017_Aff}.
  
  
, and we already have many separated datasets for video captioning~\cite{Xu16_MSR_Dataset} and affordance detection~\cite{Nguyen2017_Aff}


 using all standard metrics
 
 We train each LSTM/GRU network using the features from VGG16, Inception\_v3 and ResNet50, respectively.
 
 method for video captioning and the recent method that used encoder-decoder scheme with saliency guided~\cite{Ramanishka2017cvpr} (denoted as SGC) as the attention mechanism.


   \textbf{Grasping}, \textbf{Pick and Place} In this experiment, we address how the robot executes a new scenario, i.e. ``pick up an object and place it into another object". Our experiment involves two affordances \texttt{grasp} and \texttt{contain}. Using our approach, the robot can grasp an object from its \texttt{grasp} affordance and bring it to a new location that belongs to the \texttt{contain} affordance of another object. Furthermore, we note that with the aid of our semantic understanding framework, the robot can recognize the target objects while ignoring the irrelevant ones. \textbf{Pouring} Similar to the pick and place experiment, we use two affordances \texttt{grasp} and \texttt{contain} and the associated objects in our dataset. The goal is to pour the liquid from an object to the \texttt{contain} region of another object. Our semantic perception framework gives the robot detail understanding about the target objects, their affordances as well as the relative coordinates for the movement. We notice that besides the basic actions such as grasp, raise, etc., we predefined the pouring action to help the robot complete the task. 
   
 we use the vision system from~\cite{Nguyen2017_Aff} to provide object location, its affordances and grasping frame for the robot, while trajectory is generated by the OpenSoT library~\cite{Rocchi15}. For simplicity, we only use the objects that the IIT-Aff~\cite{Nguyen2017_Aff} dataset so the robot can recognize them. Since there are a huge gap between the number of words in the IIT-V2C dataset  and the the number of objects that the robot can recoginze in the  in the  . videoFor each To create the command for  n our work, instead of using full natural command sentences, we propose to use human demonstrations from videos as the input. The translation module is then used to interpret the video to commands in grammar-free format that the robot can follow. Combined with the vision and planning system, our approach allows the robot to perform manipulation tasks by just ``watching" the input video. Fig.~\ref{Fig:robot_apps} shows a full description of framework in our robotic applications. 


We validate our framework using the WALK-MAN full-size humanoid~\cite{Niko2016_full}. The robot is controlled in real-time using the XBotCore software architecture~\cite{muratore2017xbotcore}, while the OpenSoT library~\cite{Rocchi15} is used to plan full body motion. The relevant objects and their affordances are detected using the framework in~\cite{Nguyen2017_Aff}. To allow the real-time performance, the control and planning system run a control pc, while the vision system runs on a vision pc with a NVIDIA Titan X GPU.


For the safety of the robot, we define some key action such as "reach", "grasp". Each word in the output command will be compared with these basic action using the similarity in word2vector. 

Fig.~\ref{Fig:robot_apps} shows an overview of our robotic application.


To compare the origi- nality in generation, we compute the Levenshtein distance of the predicted sentences with those in the training set. From Table 3, for the MSVD corpus, 42.9 of the predic- tions are identical to some training sentence, and another 38.3 can be obtained by inserting, deleting or substituting one word from some sentence in the training corpus. We note that many of the descriptions generated are relevant. TODO: challenging, failures case.


 We follow the standard procedure in the captioning tasks to evaluate our results.
 
 
 In particular, we use the code from COCO evaluation server~\cite{Chen_COCO_Evaluation} that implements several metrics:

 the baseline network architecture remains challenging to modify, most of the recent work used different models such as attention mechanism~\cite{•}, saliency 



GoogLeNet [32, 12] to extract the frame-level features in our experiment. All the videos’ lengths are kept to 200 frames. For a video with more than 200 frames, we drop the extra frames. For a video with- out enough frames, we pad zero frames. These are com- mon approaches to ensure all the videos have the same length [38, 43]. Feature Extractor: CNN. We consider the input video as a sequence of frames and encode each frame using a CNN. This process extracts the meaningful features from the input images at every time step. These features are then fed to the LSTM network as inputs. In particular, we use three most popular CNNs: VGG16, GoogleNet, and ResNet-50 as our feature extractors. For the VGG16 and ResNet-50, we remove the last classification layer and TODO: describe net without the last layer.
\\




The MSR-VTT dataset is characterized by the unique properties including the large scale clip-sentence pairs, comprehensive video categories, diverse video content and descriptions, as well as multimodal audio and video stream- s. We



Current datasets for video to text mostly focus on specific fine-grained domains. For example, YouCook [5], TACoS [25, 28] and TACoS Multi-level [26] are mainly de- signed for cooking behavior. MSR-VTT focuses on general videos in our life, while MPII-MD [27] and M-VAD [32] on movie domain. Although MSVD [3] contains general web videos which may cover different categories, the very limited size (1,970) is far from representativeness. To col- lect representative videos, we obtain the top 257 represen- tative queries from a commercial video search engine, cor- responding to 20 categories


Since our goal is to collect short video clips that each can be described with one single sentence in our current version


. TOFIX: Similar to the treatment of frame features, we embed words to a lower 500 dimensional space by applying a linear transformation to the input data and learning its parameters via back propa- gation. The embedded word vector concatenated with the output (ht) of the first LSTM layer forms the input to the second LSTM layer (marked green in Figure 2). When considering the output of the LSTM we apply a softmax over the complete vocabulary as in Equation 5.




}