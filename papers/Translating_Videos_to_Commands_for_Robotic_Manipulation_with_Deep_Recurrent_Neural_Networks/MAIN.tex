%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8

\documentclass[letterpaper, 10 pt, conference]{class/ieeeconf}  % Comment this line out
                                                          % if you need a4paper

\IEEEoverridecommandlockouts                              % This command is only
                                                          % needed if you want to
                                                          % use the \thanks command
\overrideIEEEmargins

% The following packages can be found on http:\\www.ctan.org

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{latexsym}
\usepackage{url}
\usepackage{cite}
\usepackage{relsize}
\usepackage{multirow}
\usepackage{afterpage}
\usepackage{ifthen}
\usepackage{graphicx}
\usepackage{algpseudocode,algorithm,algorithmicx}
\usepackage{subfigure}
\usepackage[keeplastbox]{flushend}
\usepackage{epstopdf}
\usepackage{stackengine}
\usepackage{gensymb}
\usepackage[bookmarks=false, linkcolor=blue, urlcolor=blue, citecolor=blue]{hyperref} 
\usepackage{booktabs}
\usepackage{makecell}
\usepackage{hhline}
\usepackage{lipsum}
\usepackage{mathtools}

\usepackage[10pt]{moresize}

            
%\hypersetup{pdfstartview={XYZ null null 1.00}}
\hypersetup{pdfstartview={FitH}}

\setlength{\floatsep}{0.1in}
\setlength{\dblfloatsep}{0.1in}
\setlength{\textfloatsep}{0.1in}
\setlength{\dbltextfloatsep}{0.1in}
\setlength{\intextsep}{0.1in}
\setlength{\abovecaptionskip}{-0.1in}

\renewcommand{\UrlFont}{\scriptsize}

\newcommand*\rot{\rotatebox{90}}

\newcommand \tableCaption[4]{ %4 is number of cells
	%\vspace{-2ex}
    \begin{tabular}{ll} %2 columns - "c" for center; "l" for left
         	\textbf{GT}: #1 	&	\textbf{SGC}: #3\\     % #1 IS FOR REAL TEXT VALUE
            \textbf{Ours}: #2 	&	\textbf{S2VT}: #4\\    %2 rows
     \end{tabular}
}

%\title{\LARGE \bf Translating Videos to Robot Commands with Deep\\ Recurrent Neural Networks}
\title{\LARGE \bf Translating Videos to Commands for Robotic Manipulation\\ with Deep Recurrent Neural Networks}

%\title{\LARGE \bf TBD}

\author{Anh Nguyen$^1$, Dimitrios Kanoulas$^1$, Luca Muratore$^{1,2}$, Darwin G. Caldwell$^1$, and Nikos G. Tsagarakis$^1$
        %\thanks{The authors are with the Department of Advanced Robotics, Istituto Italiano di Tecnologia (IIT), Via Morego 30, 16163, Genova, Italy. {\tt \{Anh.Nguyen, Dimitrios.Kanoulas, Luca.Muratore, Darwin.Caldwell, Nikos.Tsagarakis\}@iit.it}}}
\thanks{$^1$Advanced Robotics Department, Istituto Italiano di Tecnologia, Italy.} 
\thanks{$^2$School of Electrical and Electronic Engineering, The University of Manchester, UK.}
\thanks{{\tt \{Anh.Nguyen, Dimitrios.Kanoulas, Luca.Muratore, Darwin.Caldwell, Nikos.Tsagarakis\}@iit.it}}}

	
\begin{document}
\input{0_localmacros}

\maketitle
\thispagestyle{empty}
\pagestyle{empty}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
%\lipsum[1]
We present a new method to translate videos to commands for robotic manipulation using Deep Recurrent Neural Networks (RNN). Our framework first extracts deep features from the input video frames with a deep Convolutional Neural Networks (CNN). Two RNN layers with an encoder-decoder architecture are then used to encode the visual features and sequentially generate the output words as the command. We demonstrate that the translation accuracy can be improved by allowing a smooth transaction between two RNN layers and using the state-of-the-art feature extractor. The experimental results on our new challenging dataset show that our approach outperforms recent methods by a fair margin. Furthermore, we combine the proposed translation module with the vision and planning system to let a robot perform various manipulation tasks. Finally, we demonstrate the effectiveness of our framework on a full-size humanoid robot WALK-MAN.


\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{1_intro}
\input{2_rw} 
\input{3_rnn}
\input{4_exp}
%\input{5_discussion}
\input{6_conclusions}

\bibliographystyle{class/IEEEtran}
\bibliography{class/IEEEabrv,class/reference}
   
\end{document}


















\junk{
Unlike[] only test on simple setup, we test on the new dataset with real-world scene



This was made possi- ble by leveraging transfer learning from large scale classi- fication data. We show that the PoseNet localizes from high level features and is robust to difficult lighting, motion blur and different camera intrinsics where point based SIFT reg- istration fails. Furthermore we show how the pose feature that is produced generalizes to other scenes allowing us to regress pose with only a few dozen training examples

, then  Lorem ipsum dolor sit amet, consectetur adipiscing elit. Curabitur lobortis varius pellentesque. Vivamus sit amet tellus porta, bibendum justo ac, vestibulum sapien. Integer vel velit at risus pharetra dignissim id sed lacus. Donec lectus orci, fermentum ac volutpat et, semper a sapien. In fringilla gravida dignissim. Praesent sed mauris id mi venenatis accumsan. Curabitur vulputate massa at velit pharetra tristique nec vel diam.Lorem ipsum dolor sit amet, consectetur adipiscing elit. Curabitur lobortis varius pellentesque. Vivamus sit amet tellus porta, bibendum justo ac, vestibulum sapien. Integer vel velit at risus pharetra dignissim id sed lacus. Donec lectus orci, fermentum ac volutpat et, semper a sapien. In fringilla gravida dignissim. Praesent sed mauris id mi venenatis accumsan. Curabitur vulputate massa at velit pharetra tristique nec vel diam.


}