\section{INTRODUCTION} \label{Sec:Intro}
The ability to perform actions based on observations of human activities is one of the major challenges to increase the capabilities of robotic systems~\cite{Chrystopher09}. Over the past few years, this problem has been of great interest to researchers and remains an active field in robotics~\cite{Ramirez2015}. By understanding human actions, robots may be able to acquire new skills, or perform different tasks, without the need for tedious programming. It is expected that the robots with these abilities will play an increasingly more important role in our society in areas such as assisting or replacing humans in disaster scenarios, taking care of the elderly, or helping people with everyday life tasks.

In this paper, we argue that there are two main capabilities that a robot must develop to be able to replicate human activities: \textit{understanding} human actions, and \textit{imitating} them. The imitation step has been widely investigated in robotics within the framework of learning from demonstration (LfD)~\cite{Brenna2009}. In particular, there are two main approaches in LfD that focus on improving the accuracy of the imitation process: kinesthetic teaching~\cite{Akgun2012} and motion capture~\cite{Koenemann2014}. While the first approach needs the users to physically move the robot through the desired trajectories, the second approach uses a bodysuit or camera system to capture human motions. Although both approaches successfully allow a robot to imitate a human, the number of actions that the robot can learn is quite limited due to the need of using expensively physical systems (i.e., real robot, bodysuit, etc.) to capture the training data~\cite{Akgun2012}~\cite{Koenemann2014}. 

The \textit{understanding} step, on the other hand, receives more attention from the computer vision community. Two popular problems that receive a great deal of interest are video classification~\cite{Karpathy2014} and action recognition~\cite{Simonyan2014}. However, the outputs of these problems are discrete (e.g., the action classes used in~\cite{Simonyan2014} are ``diving", ``biking", ``skiing", etc.), and do not provide further meaningful clues that can be used in robotic applications. Recently, with the rise of deep learning, the video captioning problem~\cite{Venugopalan2016} has become more feasible to tackle. Unlike the classification or detection tasks, the output of the video captioning task is a natural language sentence, which is potentially useful in robotic applications.


\begin{figure}[!t] 
    \centering
 
     %\includegraphics[width=0.99\linewidth, height=0.72\linewidth]{figures/1_intro/test9.jpg}   		
     %\includegraphics[width=0.99\linewidth, height=0.62\linewidth]{figures/1_intro/intro.pdf}  
     %\includegraphics[scale=0.28]{figures/1_intro/intro.pdf}  
     \includegraphics[scale=0.265]{figures/1_intro/intro.pdf}   		
     %\includegraphics[width=0.99\linewidth, height=0.65\linewidth]{figures/1_intro/all_5.jpg}
    %\includegraphics[width=0.99\linewidth, height=0.2\linewidth]{example-image-a} 
    \vspace{1ex}
    \caption{Translating videos to commands for robotic manipulation. }
    \label{Fig:intro} 
\end{figure}

Inspired by the recent advances in computer vision, this paper describes a deep learning framework that translates an input video to a command that can be used in robotic applications. While the field of LfD~\cite{Brenna2009} focuses mainly on the imitation step, we focus on the understanding step, but our method also allows the robot to perform useful tasks via the output commands. Our goal is to bridge the gap between computer vision and robotics, by developing a system that helps the robot understand human actions, and use this knowledge to complete useful tasks. In particular, we first use CNN to extract deep features from video frames, then two RNN layers are used to learn the relationship between the visual features and the output command. Unlike the video captioning problem~\cite{Venugopalan2016} which describes the output sentence in a natural language form, we use a grammar-free form to describe the output command. We show that our solely neural architecture further improves the state of the art, while its output can be applied in real robotic applications. Fig.~\ref{Fig:intro} illustrates the concept of our method.


Next, we review the related work in Section~\ref{Sec:rw}, then describe our network architecture in Section~\ref{Sec:rnn}. In Section~\ref{Sec:exp}, we present the experimental results on the new challenging dataset, and on the full-size humanoid robot. Finally, we discuss the future work and conclude the paper in Section~\ref{Sec:con}.



\junk{
 actions from studying
 
 
The remaining of this paper is organized as follows.
that can be used directly in robotic experiments

directly

the state-of-the-art


discuss the advantages and limitations of our approach then

\begin{itemize}
\item A deep learning based method that can be trained end-to-end to directly interpret input videos to robot commands.

\item A dataset that is sufficient for deep learning methods.

\item An robotic framework that use the proposed method to execute different manipulation tasks.

\end{itemize} 


This paper describes the implementation of the ChainModel [15], [16], a biologically inspired model of the mirror neuron system, on an anthropomorphic robot. The work focuses on an interaction and imitation task between the robot and a human, wheremotor control applied to object manipulation is integrated with visual understanding capabilities.More specifically, the re- search focuses on a fully instantiated system integrating perception and learning, capable of executing goal directed motor se- quences and understanding communicative gestures. In particular, the goal is to develop a robotic system that learns to use its motor repertoire to interpret and reproduce observed actions under the guidance of a human user.



A popular approach to solve this problem is learning from demonstration.

The framework of LfD is divided into two fundamental phases: motion transfer and skill modeling. There are two widely used approaches to transfer a motion: kinesthetic guiding and motion capture [4]. The former physically moving the robot in question. This is an intuitive way for users to teach motions to a robot and there are no correspondence issues since the robot is typically aware of its current configuration, i.e., through joint encoders [4,5]. Resulting behaviors tend to be unnatural, because kinesthetic guiding cannot exactly imitate a skill the same way humans would naturally perform it, and it may be difficult for the human to smoothly manipulate the robot. Alternatively, in motion capture systems, users demonstrate a behavior which is extracted by the system, and then converted to a suitable form after some pre-processing steps to remove noise in order to project it onto the robot. One of the main challenges of this approach is how to project recorded data onto the body of the robot, since its morphology may be different from that of the human â€“ the correspondence problem [4]. Typically a body- suit system [6,7] or a camera-based system [8,9] is employed as a motion capturing system. The latter is preferable for demonstrators because it does not constrain performances, allowing for more natural or complex demonstrations.


Transferring skills to humanoid robots based on observations of human activities is widely considered to be one of the most effective ways of increasing the capabilities of such systems [1,2]. It is expected that semantic representations of human activities will play a key role in advancing these sophisticated systems beyond their current capabilities, which will enable these robots to obtain and determine high level understanding of human behaviors. The ability to automatically recognize a human behavior and react to it by generating the next probable motion or action according to human expectations will substantially enrich humanoid robots.

The ability to learn actions from human demonstrations is one of the major challenges for the development of intelligent systems. Particularly, manipulation actions are very challenging, as there is large variation in the way they can be performed and there are many occlusions



This paper describes the implementation of the ChainModel [15], [16], a biologically inspired model of the mirror neuron system, on an anthropomorphic robot. The work focuses on an interaction and imitation task between the robot and a human, wheremotor control applied to object manipulation is integrated with visual understanding capabilities.More specifically, the re- search focuses on a fully instantiated system integrating perception and learning, capable of executing goal directed motor se- quences and understanding communicative gestures. In particular, the goal is to develop a robotic system that learns to use its motor repertoire to interpret and reproduce observed actions under the guidance of a human user.





In order to perform a manipulation action, the robot also needs to learn what tool to grasp and on what object to perform the action. Our system applies CNN based recognition modules to recognize the objects and tools in the video. Then, given the beliefs of the tool and object (from the output of the recognition), our system predicts the most likely action using language, by mining a large corpus using a technique similar to [14]. Putting everything together, the output from the lower level visual perception system is in the form of (LeftHand GraspType1 Object1 Action RightHand GraspType2 Object2). We will refer to this septet of quantities as visual sentence.

At the higher level of representation, we generate a symbolic command sequence. [13] proposed a context-free grammar and related operations to parse manipulation actions. However, their system only processed RGBD data from a controlled lab environment. Furthermore, they did not consider the grasping type in the grammar. This work extends [13] by modeling manipulation actions using a


}







