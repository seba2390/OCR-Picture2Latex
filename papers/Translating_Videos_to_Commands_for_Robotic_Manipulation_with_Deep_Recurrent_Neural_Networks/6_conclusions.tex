\section{Conclusions and Future Work}\label{Sec:con}
In this paper, we proposed a new method to translate human demonstration videos to commands using deep recurrent neural networks. We conducted experiments with the LSTM and GRU network using different visual feature representations. The experimental results showed that our purely neural sequence to sequence architecture outperformed current state-of-the-art methods by a fair margin. We also introduced a new large-scale videos to commands dataset that is suitable for deep learning methods. Finally, we combined our proposed method with the vision and planning module, and performed various manipulation tests on a real full-size humanoid robot.

Our robotic experiments so far are qualitative. We have focused on demonstrating how our approach can be used in a real robotic system to reduce the (tedious) programming when there are many manipulation tasks. Although using the learning approach to translate the demonstration videos to commands could help the robot understand human actions in a meaningful way, the imitation step is still challenging since it requires a robust vision, planning (and LfD) system. Currently, our framework relies solely on the vision system to plan the actions. This does not allow the robot to perform accurate tasks such as ``hammering" or ``cutting" which require precise skills. Therefore, an interesting problem is to combine our approach with LfD techniques to improve the robot manipulation capabilities.


\section*{Acknowledgment}
\addcontentsline{toc}{section}{Acknowledgment}
This work is supported by the European Union Seventh Framework Programme [FP7-ICT-2013-10] under grant agreement no 611832 (WALK-MAN). 

\junk
{

 . task such as  We can also combine our approach with LfD techniques to allow the robot to perform more   It is clear that we need to improve both three components in our framework (translation, vision, and planning) in order to 
 
 
achieve the human level in manipulation tasks.



the robot can understand the concept of human action using our translation module, however, the imitation step is very challenging since it requires a perfect vision and planning to achieve human level. 



We propose the idea of translating videos to commands and use these commands in robotic manipulation tasks. Our approach potentially can perform useful tasks without the need of tedious programming. We have demonstrated that our framework can be used together with other modules such as vision recognition system to complete complicated task. It is also straightforward to combine our framework with other LfD methods to perform tasks that require precise accuracy.

 Using the proposed method, we introduced  detect object affordances with CNN. We have demonstrated that the affordance detection results can be improved by using an object detector and dense CRF. Moreover, we introduced a challenging dataset that is suitable for real-world robotic applications. From the detected affordances, we presented a grasping method that is robust to noisy data. The effectiveness of our approach was demonstrated by performing different grasping experiments in cluttered scenes on the full-size humanoid robot WALK-MAN. We hope this opens up the door to practical solutions to the current limitations of real-world SLAM applications. It is worth restating that t...
 
 


Currently, our approach needs two separate networks to detect object affordances. This architecture can't be trained end-to-end as a single network. In future work, we aim to overcome this limitation by developing a new architecture that can detect the object identity and its affordances simultaneously. Another interesting problem is to extend our robotics experiments with more complicated scenarios.
}