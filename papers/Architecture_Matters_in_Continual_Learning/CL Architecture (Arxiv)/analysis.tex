\section{Role of Architecture Components}\label{sec:components}
We now study the individual components in various architectures to understand how they impact continual learning performance. We start by generic \emph{structural} properties in all architectures such as \emph{width} and \emph{depth} (\emph{cf.}~\secref{sec:analysis-width-depth}), and show that as the width increases, the forgetting decreases. In \secref{sec:batch-norm}, we study the impact of batch normalization and observe that it can significantly improve the learning accuracy in continual learning. Then, in \secref{sec:analysis-skip-conn}, we see that adding skip connections (or shortcuts) to CNNs does not necessarily improve the CL performance whereas pooling layers (\emph{cf.} \secref{sec:analysis-pool} and ~\secref{sec:analysis-global-pool}) can have significant impact on learning accuracy and forgetting. Moreover, we briefly study the impact of attention heads in ViTs in \secref{sec:analysis-attention-heads}. Finally, based on the observations we make in the aforementioned sections, in \secref{sec:improving_arch}, we provide a summary of modifications that can improve various architectures on both Split CIFAR-100 and ImageNet-1K benchmarks\footnote{In this section, we duplicate some of the results across tables to improve readability.}.


\subsection{Width and Depth}
\label{sec:analysis-width-depth}
Recently, \citet{Mirzadeh2021WideNN} have shown that wide neural networks forget less catastrophically by studying the impact of width and depth in MLP and WideResNet models. We extend their study by confirming their conclusions on more architectures and benchmarks.

\begin{table}[h!]
\centering
\caption{Role of width and depth: increasing the number of parameters (by increasing width) reduces the forgetting and hence increases the average accuracy. However, increasing the depth does not necessarily improve the performance, and thus, it is essential to distinguish between scaling the models by making them deeper and wider.}
\label{tab:width-depth}
\resizebox{0.8\linewidth}{!}{%
\begin{tabular}{@{}llccccc@{}}
\toprule
\textbf{Benchmark} & \textbf{Model} & \textbf{Depth} & \textbf{\begin{tabular}[c]{@{}c@{}}Params\\ (M)\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Average\\ Accuracy\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Average\\ Forgetting\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Learning\\ Accuracy\end{tabular}} \\ \midrule
Rot MNIST & MLP-128 & 2 & 0.1 & 70.8 $\pm$0.68 & 31.5 $\pm$0.92 & 96.0 $\pm$0.90 \\
Rot MNIST & MLP-128 & 8 & 0.2 & 68.9 $\pm$1.07 & 35.4 $\pm$1.34 & 97.3 $\pm$0.76 \\
Rot MNIST & MLP-256 & 2 & 0.3 & 71.1 $\pm$0.43 & 31.4 $\pm$0.48 & 96.1 $\pm$0.82 \\
Rot MNIST & MLP-256 & 8 & 0.7 & 70.4 $\pm$0.61 & 32.1 $\pm$0.75 & 96.3 $\pm$0.77 \\
Rot MNIST & MLP-512 & 2 & 0.7 & 72.6 $\pm$0.27 & 29.6 $\pm$0.36 & 96.4 $\pm$0.73 \\ \midrule
CIFAR-100 & CNN x4 & 3 & 2.3 & 68.1 $\pm$0.5 & 8.7 $\pm$0.21 & 76.4 $\pm$6.92 \\
CIFAR-100 & CNN x4 & 6 & 5.4 & 62.9 $\pm$0.86 & 12.4 $\pm$1.62 & 77.7 $\pm$5.49 \\
CIFAR-100 & CNN x8 & 3 & 7.5 & 69.9 $\pm$0.62 & 8.0 $\pm$0.71 & 77.5 $\pm$6.78 \\
CIFAR-100 & CNN x8 & 6 & 19.9 & 66.5 $\pm$ 1.01 & 10.7 $\pm$1.19 & 76.6 $\pm$4.78 \\ \midrule
CIFAR-100 & ViT 512/1024 & 2 & 4.6 & 56.4 $\pm$1.14 & 15.9 $\pm$0.95 & 68.1 $\pm$7.15 \\
CIFAR-100 & ViT 512/1024 & 4 & 8.8 & 51.7 $\pm$1.4 & 21.9 $\pm$1.3 & 71.4 $\pm$5.52 \\ \bottomrule
\end{tabular}%
}
\negspace{-1mm}
\end{table}


\tabref{tab:width-depth} shows that across all architectures, over-parametrization through increasing width is helpful in improving the continual learning performance as evidenced by lower forgetting and higher average accuracy numbers. For MLP, when the width is increased from 128 to 512, the performance in all measures improves. However, 
for both MLP-128 and MLP-256 when the depth is increased from 2 to 8 the average accuracy is reduced, and the average forgetting is increased with a marginal gain in learning accuracy. Finally, note that MLP-256 with 8 layers has roughly the same number of parameters as the MLP-512 with 2 layers. However, the wider one of these two networks has a better continual learning performance.

A similar analysis for ResNets and WideResNets is demonstrated in \tabref{tab:cifar-comp-all}. ResNet-50 and ResNet-101 are four times wider than ResNet-18 and ResNet-34, and from the table, it can be seen that this width translates into drastic improvements in average accuracy and forgetting. Similarly, ResNet-34 and ResNet-101 are the deeper versions of ResNet-18 and ResNet-50, respectively. We can observe that increasing the depth is not helpful in this case. Finally, wider WRN-10-10, WRN-16-10, and WRN-28-10 outperform the narrower WRN-10-2, WRN-10-10, WRN-28-10, respectively. Whereas if we fix the width, increasing the depth is not helpful. Overall, we conclude that over-parametrization through width is helpful in continual learning, whereas a similar claim cannot be made for the depth. 

A possible explanation for this is through the lazy-training regime, gradient orthogonalization, and sparsification induced by increasing the width~\citep{Mirzadeh2021WideNN}. Motivated by this observation, later, we show why global average pooling layers hurt the continual learning performance and how we can alleviate that.




\subsection{Batch Normalization} 
\label{sec:batch-norm}


Batch Normalization (BN)~\citep{BN-IoffeS15} is a normalization scheme that is shown to increase the convergence speed of the network due to its optimization and generalization benefits~\citep{BN-Santurkar,BN-Understand}. Another advantage of the BN layer is its ability to reduce the covariate shift problem that is specifically relevant for continual learning where the data distribution may change from one task to the next. 

There are relatively few works that have studied the BN in the context of continual learning. ~\citet{mirzadeh_understanding_continual} analyzed the BN in continual learning through the generalization lens. Concurrently to this work,~\citet{ContinualNormalization} study the normalization schemes in continual learning and show that BN enables improved learning of each task. Additionally, the authors showed that in the presence of a replay buffer of previous tasks, BN facilitates a better knowledge transfer compared to other normalization schemes such as Group Normalization~\citep{GroupNorm}.

Intuitively, however, one might think that since the BN statistics are changing across tasks, due to evolving data distribution in continual learning, and one is not keeping the statistics of each task, the BN should contribute to an increased forgetting. This is not the case in some of the experiments that we conducted. Similar to the results in~\citet{mirzadeh_understanding_continual,ContinualNormalization}, we found the BN to facilitate the learning accuracy in Split CIFAR-100 and split ImageNet-1K (\emph{cf.} \tabref{tab:cifar-layers} and \tabref{tab:improving architectures}). We believe that this could be due to relatively unchanging BN statistics across tasks in these datasets. To verify this, in Appendix~\ref{sec:appendix-bn-cifar}, we plot the BN statistics of the first layer of \CNN$\times{4}$ on the Split CIFAR-100 dataset, and we show that the BN statistics are stable throughout the continual learning experience. However, if this hypothesis is true, the converse -- a benchmark where the BN statistics change a lot across tasks, such as Permuted MNIST -- should hurt the continual learning performance. In Appendix~\ref{sec:appendix-bn-mnist}, we plot the BN statistics of the first layer of \MLP-128 on Permuted MNIST. It can be seen from the figure that indeed the BN statistics are changing in this benchmark. As a consequence, adding BN to this benchmark significantly hurt the performance, as evidenced by the increased forgetting in \tabref{tab:perm-mnist-bn}.

Overall, we conclude that the effect of the batchnorm layer is data-dependent. In the setups where the input distribution relatively stays stable, such as Split CIFAR-100 or Split ImageNet-1K, the BN layer improves the continual learning performance by increasing the learning capability of the models. However, for setups where the input distribution changes a lot across tasks, such as Permuted MNIST, the BN layer can hurt the continual learning performance by increasing the forgetting.



\subsection{Skip Connections}
\label{sec:analysis-skip-conn}


\begin{table}[t]
\centering
\caption{Role of various components for the Split CIFAR-100 benchmark: While adding skip connections does not have a significant impact on the performance, batch normalization and max polling can significantly improve the learning accuracy of CNNs.}
\label{tab:cifar-layers}
\resizebox{0.72\linewidth}{!}{%
\begin{tabular}{@{}lccccc@{}}
\toprule
\textbf{Model} & \textbf{\begin{tabular}[c]{@{}c@{}}Params\\ (M)\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Average\\ Accuracy\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Average\\ Forgetting\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Learning\\ Accuracy\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Joint\\ Accuracy\end{tabular}} \\ \midrule
CNN x4 & 2.3 & 68.1 $\pm$0.5 & 8.7 $\pm$0.21 & 76.4 $\pm$6.92 & 73.4 $\pm$0.89 \\
CNN x4 + Skip & 2.4 & 68.2 $\pm$0.56 & 8.9 $\pm$0.72 & 76.6 $\pm$7.07 & 73.8 $\pm$0.47 \\
CNN x4 + BN & 2.3 & 74.0 $\pm$0.56 & 8.1 $\pm$0.35 & 81.7 $\pm$6.68 & 80.2 $\pm$0.16 \\
CNN x4 + AvgPool & 2.3 & 68.5 $\pm$0.6 & 8.3 $\pm$0.57 & 76.3 $\pm$7.63 & 73.6 $\pm$0.83 \\
CNN x4 + MaxPool & 2.3 & 74.4 $\pm$0.34 & 9.3 $\pm$0.47 & 83.3 $\pm$6.1 & 79.9 $\pm$0.53 \\
CNN x4 + All & 2.4 & 77.7 $\pm$0.77 & 6.5 $\pm$0.58 & 83.7 $\pm$6.31 & 81.6 $\pm$0.77 \\ \midrule
CNN x8 & 7.5 & 69.9 $\pm$0.62 & 8.0 $\pm$0.71 & 77.5 $\pm$6.78 & 74.1 $\pm$0.83 \\
CNN x8 + Skip & 7.8 & 70.7 $\pm$0.31 & 6.8 $\pm$0.91 & 77.1 $\pm$6.87 & 74.4 $\pm$0.35 \\
CNN x8 + BN & 7.5 & 76.1 $\pm$0.3 & 5.9 $\pm$0.16 & 81.7 $\pm$6.83 & 80.5 $\pm$0.27 \\
CNN x8 + AvgPool & 7.5 & 71.2 $\pm$0.5 & 8.3 $\pm$0.35 & 79.0 $\pm$7.05 & 74.0 $\pm$1.02 \\
CNN x8 + MaxPool & 7.5 & 77.2 $\pm$0.53 & 7.1 $\pm$0.33 & 84.0 $\pm$5.81 & 80.6 $\pm$0.35 \\
CNN x8 + All & 7.8 & 78.1 $\pm$1.15 & 5.7 $\pm$0.36 & 83.3 $\pm$6.27 & 81.9 $\pm$0.51 \\ \midrule
CNN x16 & 26.9 & 76.8 $\pm$0.76 & 4.7 $\pm$0.84 & 81.0 $\pm$6.97 & 79.1 $\pm$0.86 \\
CNN x16 + All & 27.9 & 78.9 $\pm$0.27 & 4.5 $\pm$0.36 & 82.9 $\pm$6.48 & 82.1 $\pm$0.46 \\ \bottomrule
\end{tabular}%
}
\negspace{-2mm}
\end{table}

Skip connections~\citep{Cho-Skip-DBM}, originally proposed for convolutional models by \citet{he2016deep}, are crucial in the widely used ResNet architecture. They are also used in many other architectures such as transformers~\citep{vaswani2017attention}. Many works have been done to explain why skip connections are useful:~\citet{hardt2016identity} show that skip connection tends to eliminate spurious local optima;~\citet{bartlett2018representing} study the function expressivity of residual architectures;~\citet{bartlett2018gradient} show that gradient descent provably learns linear transforms in ResNets;~\citet{jastrzkebski2017residual} show that skip connections tend to iteratively refine the learned representations. However, these works mainly focus on learning a single task. In continual learning problems, due to the presence of distribution shift, it is unclear whether these benefits of skip connections still have a significant impact on model performance, such as forgetting and average accuracy over tasks. 

Here, we empirically study the impact of skip connection on continual learning problems. Interestingly, as illustrated in \tabref{tab:cifar-layers}, adding skip connections to plain CNNs does not change the performance significantly, and the results are very close (within the standard deviation) of each other. Therefore, we conclude that skip connection may not have a significant positive or negative impact on the model performance in our continual learning benchmarks.



\subsection{Pooling Layers}
\label{sec:analysis-pool}
Pooling layers were the mainstay of the improved performance of CNNs before ResNets. Pooling layers not only add local translation invariances, which help in applications like object classification~\citep{AlexNet,CoAtNet}, but also reduce the spatial resolution of the network features, resulting in the reduction of computational cost. Since one family of the architectures that we study are all-convolutional \CNN s, we revisit the role of pooling layers in these architectures in a continual learning setup. Towards this, we compare the network without pooling, \CNNArch, against those that have pooling layers (`\CNNArch+AvgPool' or `\CNNArch+MaxPool') in \tabref{tab:cifar-layers}. To keep the feature dimensions fixed, we set the convolutional stride from 2 to 1 when pooling is used. We make the following observations from the table. 

First, the average pooling (+AvgPool) does not have any significant impact on the continual learning metrics. Second, max pooling (+MaxPool) improves the learning capability of the network significantly, as measured by the improved learning accuracy. Third, in terms of retention, pooling layers do not have a significant impact as measured by similar forgetting numbers. All in all, we see that max pooling achieves the best performance in terms of average accuracy, owing to its superior learning capability.

The ability of max pooling to achieve better performance in a continual learning setting can be attributed to a well-known empirical observation by \citet{Springenberg2015StrivingFS}, where it is shown that max pooling with stride 1 outperforms a CNN with stride 2 and no pooling. Further, we believe that max pooling might have extracted the low-level features, such as edges, better, resulting in improved learning in a dataset like CIFAR-100 that consists of natural images. There is some evidence in the literature that max pooling provides sparser features and precise localization~\citep{Zhou2016LearningDF}. This could have transferred over to the continual learning setup that we considered. It, however, remains an interesting future direction to further study the gains brought by max pooling in both the standard and continual learning setups. 

\subsection{Global Pooling Layers}
\label{sec:analysis-global-pool}

Global average pooling (GAP) layers are typically used in convolutional networks just before the final classification layer to reduce the number of parameters in the classifier. The consequence of adding a GAP layer is to reduce the width of the final classifier. It is argued in \secref{sec:analysis-width-depth} that wider networks forget less. Consequently, the architectures with a GAP layer can suffer from increased forgetting. 

\tabref{tab:global-pool} empirically demonstrate this intuition. From the table it can be seen that applying the GAP layer significantly increases the forgetting resulting in a lower average accuracy. In the previous section, we already demonstrated that average pooling does not result in a performance decrease as long as the spatial feature dimensions are the same. To demonstrate that there is nothing inherent to the GAP layer, and it is just a consequence of a reduced width of the final classifier, we construct another baseline by multiplying the number of channels in the last convolutional layer by 16 and then apply the GAP. This network is denoted as ``CNN x4 (16x)'' in \tabref{tab:global-pool} and it has the same classifier width as that of the network without GAP. It can be seen this architecture has considerably smaller forgetting showing GAP affects continual learning through the width of the final classifier. 

\begin{table}[h]
\centering
\caption{Role of Global Average Pooling (GAP) for Split CIFAR-100: related to our arguments in \secref{sec:analysis-width-depth}, adding GAP to CNNs significantly increases the forgetting. Later, we show that removing GAP from ResNets can also significantly reduce forgetting as well.}
\label{tab:global-pool}
\resizebox{0.85\linewidth}{!}{%
\begin{tabular}{@{}lcclllc@{}}
\toprule
\textbf{Model} & \textbf{\begin{tabular}[c]{@{}c@{}}Params\\ (M)\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Pre-Classification\\ Width\end{tabular}} & \multicolumn{1}{c}{\textbf{\begin{tabular}[c]{@{}c@{}}Average\\ Accuracy\end{tabular}}} & \multicolumn{1}{c}{\textbf{\begin{tabular}[c]{@{}c@{}}Average\\ Forgetting\end{tabular}}} & \multicolumn{1}{c}{\textbf{\begin{tabular}[c]{@{}c@{}}Learning\\ Accuracy\end{tabular}}} & \begin{tabular}[c]{@{}c@{}}\textbf{Joint}\\ \textbf{Accuracy}\end{tabular} \\ \midrule
CNN x4 & 2.3 & \textbf{8192} & 68.1 $\pm$0.5 & 8.7 $\pm$0.21 & 76.4 $\pm$6.92 & 73.4 $\pm$0.89 \\
CNN x4 + GAP & 1.5 & \textbf{512} & 60.1 $\pm$0.43 & 14.3 $\pm$0.8 & 66.1 $\pm$7.76 & 76.9 $\pm$0.81 \\
CNN x4 (16x) + GAP & 32.3 & \textbf{8192} & 73.6 $\pm$0.39 & 5.2 $\pm$0.66 & 75.6 $\pm$4.77 & 77.9 $\pm$0.37 \\ \\

CNN x8 & 7.5 & \textbf{16384} & 69.9 $\pm$0.62 & 8.0 $\pm$0.71 & 77.5 $\pm$6.78 & 74.1 $\pm$0.83 \\
CNN x8 + GAP & 6.1 & \textbf{1024} & 63.1 $\pm$2.0 & 14.7 $\pm$1.68 & 70.1 $\pm$7.18 & 78.3 $\pm$0.97 \\ \\

CNN x16 & 26.9 & \textbf{32768} & 76.8 $\pm$0.76 & 4.7 $\pm$0.84 & 81.0 $\pm$6.97 & 74.6 $\pm$0.86 \\
CNN x16 + GAP & 23.8 & \textbf{2048} & 66.3 $\pm$0.82 & 12.2 $\pm$0.65 & 72.3 $\pm$6.02 & 78.9 $\pm$0.27 \\ \bottomrule
\end{tabular}%
}
\negspace{-3mm}
\end{table}

Inspired by this observation, in \secref{sec:improving_arch} we show that the performance of ResNets in continual learning can be significantly improved by either removing the GAP layer or using the smaller average pooling layers rather than the GAP.



\subsection{Attention Heads}
\label{sec:analysis-attention-heads}
The attention mechanism is a prominent component in the transformer-based architectures that has had great success in natural language processing and, more recently, computer vision tasks. For the latter, the heads of vision transformers (ViTs) are shown to attend to both local and global features in the image \cite{VisionTransformerOriginal}. We double the number of heads while fixing the width to ensure that any change in results is not due to the increased dimension of representations. In \tabref{tab:vit-heads}, it can be seen that even doubling the number of heads in ViTs only marginally helps in increasing the learning accuracy and lowering the forgetting. This suggests that increasing the number of attention heads may not be an efficient approach for improving continual learning performance in ViTs. We, however, note that consistent with other observations in the literature~\citep{paul2022vision}, ViTs show promising robustness against distributional shifts as evidenced by lower forgetting numbers, for the same amount of parameters, on the Split CIFAR-100 benchmark (\emph{cf.}~\figref{fig:intro-cifar-forgets}).

\begin{table}[h]
\centering
\caption{Role of attention heads: for the Split CIFAR-100 benchmark, increasing the number of attention heads (while fixing the total width), does not impact the performance significantly.}
\label{tab:vit-heads}
\resizebox{0.7\linewidth}{!}{%
\begin{tabular}{@{}lcllll@{}}
\toprule
\textbf{Model} & \textbf{Heads} & \multicolumn{1}{c}{\textbf{\begin{tabular}[c]{@{}c@{}}Params\\ (M)\end{tabular}}} & \multicolumn{1}{c}{\textbf{\begin{tabular}[c]{@{}c@{}}Average\\ Accuracy\end{tabular}}} & \multicolumn{1}{c}{\textbf{\begin{tabular}[c]{@{}c@{}}Average\\ Forgetting\end{tabular}}} & \multicolumn{1}{c}{\textbf{\begin{tabular}[c]{@{}c@{}}Learning\\ Accuracy\end{tabular}}} \\ \midrule
ViT 512/1024 & 4 & 8.8 & 50.9 $\pm$0.73 & 23.8 $\pm$1.3 & 72.8 $\pm$6.13 \\
ViT 512/1024 & 8 & 8.8 & 51.7 $\pm$1.4 & 21.9 $\pm$1.3 & 71.4 $\pm$5.52 \\ \midrule
ViT 1024/1536 & 4 & 30.7 & 57.4 $\pm$1.59 & 14.4 $\pm$1.96 & 66.0 $\pm$5.89 \\
ViT 1024/1536 & 8 & 30.7 & 60.4 $\pm$1.56 & 12.2 $\pm$1.12 & 67.4 $\pm$5.57 \\ \bottomrule
\end{tabular}%
}
\end{table}



\subsection{Improving Architectures} \label{sec:improving_arch}
We now provide practical suggestions to improve the performance of architectures, that are derived from our experiments in the previous sections.

For CNNs, we add BatchNormalization and MaxPooling, as they both are shown to improve the learning ability of the model and skip connections that make the optimization problem easier. \tabref{tab:improving architectures} shows the results on both CIFAR-100 and ImageNet-1K. It can be seen from the table that adding these components significantly improves the CNNs performance, as evidenced by improvement over almost all the measures, including average accuracy.

For ResNets, we either remove the GAP, as is the case with CIFAR-100, or locally average the features in the penultimate layer by a 4x4 filter, as is the case with ImageNet-1K. The reason for not fully removing the average pooling from the ImageNet-1K is the resultant large increase in the number of parameters in the classifier layer. It can be seen from \tabref{tab:improving architectures} that fully or partially removing the GAP, CIFAR-100, and ImageNet-1K, respectively, highly improves the retention capability of ResNets, indicated by their lower forgetting. 

\begin{table}[h]
\centering
\caption{Improving CNNs and ResNets architectures on both Split CIFAR-100 and ImageNet-1K benchmarks.}
\label{tab:improving architectures}
\resizebox{0.9\linewidth}{!}{%
\begin{tabular}{@{}lccccc@{}}
\toprule
\textbf{Model} & \textbf{Benchmark} & \textbf{\begin{tabular}[c]{@{}c@{}}Params\\ (M)\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Average\\ Accuracy\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Average\\ Forgetting\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Learning\\ Accuracy\end{tabular}} \\ \midrule
CNN x4 & CIFAR-100 & 2.3 & 68.1 $\pm$0.5 & 8.7 $\pm$0.21 & 76.4 $\pm$6.92 \\
CNN x4 + BN + MaxPool + Skip & CIFAR-100 & 1.5 & 77.7 $\pm$0.77 & 6.5 $\pm$0.58 & 83.7 $\pm$6.31 \\
CNN x8 & CIFAR-100 & 7.5 & 69.9 $\pm$0.62 & 8.0 $\pm$0.71 & 77.5 $\pm$6.78 \\
CNN x8 + BN + MaxPool + Skip & CIFAR-100 & 6.1 & 78.1 $\pm$1.15 & 5.7 $\pm$0.36 & 83.3 $\pm$6.27 \\ \midrule
CNN x3 & ImageNet-1K & 9.1 & 63.3 $\pm$0.68 & 5.8 $\pm$0.93 & 71.6 $\pm$2.31 \\
CNN x3 + BN + MaxPool + Skip & ImageNet-1K & 9.1 & 66.4 $\pm$0.47 & 5.4 $\pm$0.3 & 74.7 $\pm$2.1 \\
CNN x6 & ImageNet-1K & 24.2 & 66.7 $\pm$0.62 & 3.9 $\pm$0.86 & 70.1 $\pm$3.21 \\
CNN x6 + BN + MaxPool + Skip & ImageNet-1K & 24.3 & 72.1 $\pm$0.41 & 4.0 $\pm$0.22 & 75.7 $\pm$2.57 \\ \midrule \midrule
ResNet-18 & CIFAR-100 & 11.2 & 45.0 $\pm$0.63 & 36.8 $\pm$1.08 & 74.9 $\pm$3.98 \\
ResNet-18 w/o GAP & CIFAR-100 & 11.9 & 67.4 $\pm$0.76 & 11.2 $\pm$1.98 & 74.2 $\pm$4.79 \\
ResNet-50 & CIFAR-100 & 23.6 & 56.2 $\pm$0.88 & 9.5 $\pm$0.38 & 67.8 $\pm$5.09 \\
ResNet-50 w/o GAP& CIFAR-100 & 26.7 & 71.4 $\pm$0.29 & 6.6 $\pm$0.12 & 73.0 $\pm$5.18 \\ \midrule
ResNet-34 & ImageNet-1K & 21.8 & 62.7 $\pm$0.53 & 19.0 $\pm$0.67 & 80.4 $\pm$2.57 \\
ResNet-34 w 4x4 AvgPool & ImageNet-1K & 23.3 & 66.0 $\pm$0.24 & 4.2 $\pm$0.16 & 70.2 $\pm$3.87 \\
ResNet-50 & ImageNet-1K & 25.5 & 66.1 $\pm$0.69 & 17.3 $\pm$0.58 & 83.3 $\pm$1.57 \\
ResNet-50 w 4x4 AvgPool & ImageNet-1K & 31.7 & 67.2 $\pm$0.13 & 3.5 $\pm$0.35 & 72.8 $\pm$3.27 \\ \bottomrule
\end{tabular}%
}
\end{table}

%Based on our previous observations, here, we show how to improve the performance of CNNs and ResNets. For CNNs, we use the BatchNormalization, MaxPooling and Skip-Connections that are shown to be helpful in improving the CL performance and for ResNets on CIFAR-100, we remove or reduce the Global Average Pooling (GAP). For the ImageNet-1K benchmark, however, removing the GAP layer would increase the parameters significantly. Hence, only for this scenario, we replace the GAP with a 4x4 pooling which makes the pre-classification feature dimension wider than ResNets with GAPs and we can confirm our conclusions in Sec.~\ref{sec:analysis-global-pool}.  The results are shown in \tabref{tab:improving architectures}.




