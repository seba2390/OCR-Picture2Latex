\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[margin=8em]{geometry}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{enumitem}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{color}
\usepackage{authblk}
\usepackage{float}
% \usepackage{floatrow, tabularx, makecell, array}

\makeatletter
\renewcommand\AB@affilsepx{, \protect\Affilfont}
\makeatother
\newcounter{daggerfootnote}
\newcommand*{\daggerfootnote}[1]{%
    \setcounter{daggerfootnote}{\value{footnote}}%
    \renewcommand*{\thefootnote}{\fnsymbol{footnote}}%
    \footnote[2]{#1}%
    \setcounter{footnote}{\value{daggerfootnote}}%
    \renewcommand*{\thefootnote}{\arabic{footnote}}%
    }
\usepackage{amssymb}

\setlength{\parindent}{0em}
\setlength{\parskip}{1em}
\usepackage[round]{natbib}
\renewcommand{\bibname}{References}
\renewcommand{\bibsection}{\subsubsection*{\bibname}}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\setlength{\affilsep}{2em}   % set the space between author and affiliation

\usepackage{url}
\title{Architecture Matters in Continual Learning}
\author[1]{Seyed Iman Mirzadeh\footnote{Work done during an internship at DeepMind.}}
\author[2]{Arslan Chaudhry} 
\author[2]{Dong Yin}
\author[2]{\authorcr Timothy Nguyen}
\author[2]{Razvan Pascanu}
\author[2]{Dilan Gorur}
\author[2]{Mehrdad Farajtabar\footnote{Correspondence to farajtabar@google.com}}

\affil[1]{Washington State University}
\affil[2]{DeepMind}
\date{}

\usepackage{pifont}
\newcommand{\tick}{{\ding{51}}}

\newcommand\figref[1]{Fig.~\ref{#1}}
\newcommand\secref[1]{Sec.~\ref{#1}}
\newcommand\tabref[1]{Tab.~\ref{#1}}

% style for architectures (could be texttt or anything)
\newcommand{\archstyle}[1]{#1}
% Architecture Templates: e.g., CNN-D
\newcommand\CNNTemp[1]{\archstyle{CNN$\times${#1}}}
\newcommand\MLPTemp[1]{\archstyle{MLP-{#1}}}
\newcommand\ResNetTemp[1]{\archstyle{ResNet-{#1}}}
\newcommand\WRNTemp[2]{\archstyle{WRN-{#1}-{#2}}}
\newcommand\ViTTemp[2]{\archstyle{ViT {#1}/{#2}}}

% Architectures Shortcuts
\newcommand{\CNNArch}{\CNNTemp{N}~}
\newcommand{\MLPArch}{\MLPTemp{N}~}
\newcommand{\ResNetArch}{\ResNetTemp{D}~}
\newcommand{\WRNArch}{\WRNTemp{D}{N}~}
\newcommand{\ViTArch}{\ViTTemp{N}{M}~}

% Architectures names only
\newcommand{\CNN}{\archstyle{CNN}}
\newcommand{\MLP}{\archstyle{MLP}}
\newcommand{\ResNet}{\archstyle{ResNet}}
\newcommand{\WRN}{\archstyle{WRN}}
\newcommand{\ViT}{\archstyle{ViT}}

\newcommand{\negspace}[1]{}
% \newcommand{\negspace}[1]{}



\usepackage{hyperref}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:

% define custom commands
\newcommand{\SKIP}[1]{}

\begin{document}
\vspace{-1mm}
\maketitle

\vspace{-2mm}
\begin{abstract}

A large body of research in continual learning is devoted to overcoming the catastrophic forgetting of neural networks by designing new algorithms that are robust to the distribution shifts. However, the majority of these works are strictly focused on the ``algorithmic'' part of continual learning for a ``fixed neural network architecture'', and the implications of using different architectures are mostly neglected. Even the few existing continual learning methods that modify the model assume a fixed architecture and aim to develop an algorithm that efficiently uses the model throughout the learning experience. However, in this work, we show that the choice of architecture can significantly impact the continual learning performance, and different architectures lead to different trade-offs between the ability to remember previous tasks and learning new ones. Moreover, we study the impact of various architectural decisions, and our findings entail best practices and recommendations that can improve the continual learning performance.
\end{abstract}

\input{introduction}
\input{setup}
\input{analysis}
\input{related_work}
\input{discussion}

\section*{Acknowledgments}
We would like to thank Huiyi Hu, Alexandre Galashov, and Yee Whye Teh for helpful discussions and feedback on this project. 

\bibliography{refs}
\bibliographystyle{apalike}

\input{appendix}


\end{document}

