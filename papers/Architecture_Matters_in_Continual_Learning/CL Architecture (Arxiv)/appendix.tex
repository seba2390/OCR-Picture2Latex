\newpage
\appendix
\onecolumn

\section*{Overview}
In this document, we provide additional details regarding the main work. More specifically:
\begin{itemize}
    \item First, in \secref{sec:appendix-setup-details} we cover the details regarding our experimental setup, design choices, architecture details, and hyper-parameters.
    \item Second, in \secref{sec:appendix-additional-results} we provide additional experiments and results and a more detailed version of some of the experiments.
\end{itemize}


\section{Experimental Setup Details}
\label{sec:appendix-setup-details}

\subsection{Architectures}
\subsubsection{CNNs}
Unless otherwise stated, the CNN models in this work solely include the convolutional layers, followed by a single feed-forward layer for classification. All CNNs use the ReLU activation function and use 3x3 kernels and a stride of 2. 

For the CIFAR-100 experiments, the \emph{base CNN} contains 3 convolutional layers with 32, 64, and 128 channels respectively, and \emph{CNN $\times N$} refers to the base CNN model where the number of channels in each layer is multiplied by $N$. Hence, \emph{CNN $\times 4$} refers to 3 convolutional layers with 128, 256, and 512 channels, followed by a feed-forward classification layer. In Split ImageNet-1K experiments, the \emph{base CNN} contains 6 convolutional layers with 64 channels for the first four layers and 128 channels for the last two layers. Similar to the setup for the Split CIFAR-100 experiments, all models have a single-layer feed-forward layer for classification and use a kernel size of 3, with a stride of 2. 

When we use pooling layers in CNNs(e.g., \secref{sec:analysis-pool}), to keep the dimension of features the same with the \CNN{s} that don't have pooling layers, we use a stride of 1 for convolutional layers. In other words, every convolutional layer leads to a reduced feature map by a factor of 2: either the convolution has stride 2 (e.g., CNNs in \tabref{tab:cifar-comp-all}) or else it has a stride 1, followed by a pooling layer (e.g., CNN+(Avg/Max)Pool in \tabref{tab:cifar-layers} and \tabref{tab:improving architectures}). Finally, for the experiments with skip-connections added, we add 2 skip-connections (with projections) for the 3-layer CNNs on Split CIFAR100 that adds shortcuts from layer 1 to output of layer 2 and layer 2 to layer 3. For the Split ImageNet-1K benchmark for 6-layer CNNs, we add 3 shortcuts that bypass every other layer.

\subsubsection{ResNets}
The ResNets we use in the ImageNet experiment are the standard models, and we do not modify them. However, the ResNets in the CIFAR-100 experiments use $3 \times 3$ kernels with a stride of 1, rather than the $7 \times 7$ kernels with a stride of 2. This is a common practice for the ResNet models for low-dimensional CIFAR images since it does not reduce the dimension of input significantly. Other than this modification for CIFAR-100 experiments, the rest of the ResNet architecture kept the same.

\subsubsection{WideResNets}
The WideResNets (WRN) models on both CIFAR-100 and ImageNet benchmarks follow the original implementation of WRN models. For all experiments, we use a dropout factor of 0.1 as we empirically observed increasing the dropout does not improve the performance significantly. 

\subsubsection{Vision Transformers}
The Vision Transformer (ViT) models in our ImageNet experiment follow the same architecture as the original vision transformers. Similar to the original ViT models, we use the patch size of 16 for both ViT models in our ImageNet-1K experiments.

However, since the original vision transformer paper does not provide the details for the best practices for the CIFAR benchmark, we used smaller versions of the ViTs to match the training cost of other models. In those experiments, \emph{ViT 512-1024} refers to a ViT model with 4 layers, with a width of 512 and MLP size of 1024. Similarly, \emph{ViT 1024-1536} has a width of 1024 with the MLP hidden size of 1536. All models use the patch size of 4 (i.e., 64 patches for CIFAR images), but we empirically observed increasing the patch size to 8 does not impact the results significantly.


\subsection{Hyperparameters}
In this section, we report the hyper-parameters we used for our experiments. We include the chosen hyper-parameter for each architecture in parentheses.

\subsection{Rotated MNIST}
We follow the setting in~\citep{Mirzadeh2021WideNN} for our MNIST experiments in \secref{sec:analysis-width-depth}.
\begin{scriptsize}
\begin{verbatim}
learning rate: [0.001, 0.01 (MLP), 0.05, 0.1]
momentum: [0.0 (MLP), 0.8]
weight decay: [0.0 (MLP), 0.0001]
batch size: [16, 32, 64 (MLP)]
\end{verbatim}
\end{scriptsize}

\subsection{Split CIFAR-100}
We use the following grid of hyper-parameters for the CIFAR-100 experiments:

\begin{scriptsize}
\begin{verbatim}
learning rate: [0.001, 0.005, 0.01 (ViT 1024, ResNet 50/101), 0.05 (CNNs, ResNet 18/34, ViT 512, WRNs), 0.1]
learning rate decay: [1.0 (ResNet 50/101), 0.8 (CNN, ViTs, ResNet 18/34, WRNs)]
momentum: [0.0 (CNN, ViTs, ResNet 50/101), 0.8 (ResNet 18/34, WRNs)]
weight decay: [0.0 (CNNs, ViTs), 0.0001 (ResNets, WRNs)]
batch size: [8 (CNNs, ResNet18/34), 16 (WRNs), 64 (ResNet 50/101, ViT 512), 128 (ViT 1024)]
\end{verbatim}
\end{scriptsize}

We note that the learning rate decay is applied after each task.

\subsection{Split ImageNet-1K}
Due to computation budget, we use smaller a smaller grid for the ImageNet-1K experiments. However, we make sure that the grid is diverse enough to cover various family of architectures.
\begin{scriptsize}
\begin{verbatim}
learning rate: [0.005, 0.01, 0.05, 0.1 (All models)]
learning rate decay: [1.0, 0.75 (All models)]
momentum: [0.0, 0.8 (All models)]
weight decay: [0.0 (CNNs), 0.0001 (ResNets, WRN, ViTs)]
batch size: [64 (All models), 256]
\end{verbatim}
\end{scriptsize}



\section{Additional Results}
\label{sec:appendix-additional-results}

\subsection{Algorithms vs. Architectures}
In \figref{fig:intro-cifar-algs-vs-archs}, we have provided the results for the ResNet-18 model. Here, we provide the average accuracy and average forgetting for various models and algorithms on split CIFAR-100 with 20 tasks.


\begin{table}[h!]
\centering
\caption{Different Algorithms and Architectures}
\label{tab:algorithms-vs-architectures}
\resizebox{0.8\linewidth}{!}{%
\begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{Algorithm} & \textbf{Parameters (M)} & \textbf{Architecture} & \textbf{Average Accuracy} & \textbf{Average Forgetting} \\ \midrule
Finetune & 11.2 & ResNet-18 & 45.0 $\pm$0.63 & 36.8 $\pm$1.08 \\
EWC & 11.2 & ResNet-18 & 50.6 $\pm$0.70 & 26.6 $\pm$2.53 \\
AGEM (Mem = 1000) & 11.2 & ResNet-18 & 61.8 $\pm$0.45 & 22.9 $\pm$1.59 \\
ER (Mem = 1000) & 11.2 & ResNet-18 & 64.3 $\pm$0.99 & 19.7 $\pm$1.26 \\ \midrule
Finetune & 11.9 & ResNet-18 (w/o GAP) & 67.4 $\pm$0.76 & 11.2 $\pm$1.98 \\
AGEM (Mem = 1000) & 11.9 & ResNet-18 (w/o GAP) & 72.8 $\pm$1.33 & 5.8 $\pm$0.39 \\
ER (Mem = 1000) & 11.9 & ResNet-18 (w/o GAP) & 74.4 $\pm$0.33 & 4.6 $\pm$0.54 \\ \midrule
Finetune & 2.3 & CNN x4 & 68.1 $\pm$0.50 & 8.7 $\pm$0.21 \\
ER (Mem = 1000) & 2.3 & CNN x4 & 74.4 $\pm$0.27 & 2.4 $\pm$0.12 \\ \bottomrule
\end{tabular}%
}
\end{table}

We remind the reader that the aim of our work is not to undermine the importance of continual learning algorithms. On the contrary, we appreciate the recent algorithmic improvements in the continual learning literature. The aim of this work is to show that \emph{the role of architecture is significant, and a good architecture can complement a good algorithm in continual learning}.


% \subsection{Evolution of the Average Accuracy}

% \begin{figure*}[h]
% \centering
% \begin{subfigure}{.45\textwidth}
%       \centering
%       \includegraphics[width=.99\linewidth]{figs/avg_accs_cifar.pdf}
%       \caption{}
%       \label{fig:evolution-accs-cifar}
% \end{subfigure}\hfill
% \begin{subfigure}{.45\textwidth}
%       \centering
%       \includegraphics[width=.99\linewidth]{figs/avg_accs_imagenet.pdf}
%       \caption{}
%       \label{fig:evolution-accs-imagenet}
% \end{subfigure}
% \caption{Evolution of average accuracy for various models on cifar and imagenet}
% \label{fig:evolution-accs}
% \end{figure*}

\subsection{Batchnorm Statistics on Split CIFAR-100}
\label{sec:appendix-bn-cifar}
We show the first layer's BN statistics for \CNN$\times{4}$ in \figref{fig:cifar-bn1} using kernel density estimation with Gaussian kernel. As illustrated, the batch statistics and learned BN parameters do not change significantly across different tasks. Here, for simplicity, we have shown only four tasks from the beginning, middle, and late of the learning experience. Moreover, we focus on the first layer's statistics, which is the first layer of the model that operates on the data.

\begin{figure*}[h]
\centering
\begin{subfigure}{.45\textwidth}
      \centering
      \includegraphics[width=.99\linewidth]{figs/bn/bn1_running_mean.pdf}
      \caption{}
      \label{fig:cifar-bn1-running-mean}
\end{subfigure}\hfill
\begin{subfigure}{.45\textwidth}
      \centering
      \includegraphics[width=.99\linewidth]{figs/bn/bn1_running_var.pdf}
      \caption{}
      \label{fig:cifar-bn1-running-var}
\end{subfigure}\hfill
\begin{subfigure}{.45\textwidth}
      \centering
      \includegraphics[width=.99\linewidth]{figs/bn/bn1_weight.pdf}
      \caption{}
      \label{fig:cifar-bn1-weights}
\end{subfigure}\hfill
\begin{subfigure}{.45\textwidth}
      \centering
      \includegraphics[width=.99\linewidth]{figs/bn/bn1_bias.pdf}
      \caption{}
      \label{fig:cifar-bn1-bias}
\end{subfigure}
\caption{BN statistics for the first layer of \CNN$\times 4$ on Split CIFAR-100: the statistics do not change significantly throughout the continual learning experience.}
\label{fig:cifar-bn1}
\end{figure*}

\subsection{Batchnorm Statistics on Permuted MNIST}
\label{sec:appendix-bn-mnist}
In \secref{sec:batch-norm}, we have discussed that if the batch statistics change significantly across tasks, batch normalization can increase the forgetting. To this end, we design an experiment where we train two \MLP-128 networks (with and without BN) on the Permuted MNIST benchmark\citep{DBLP:journals/corr/GoodfellowMDCB13} with five tasks. While Permuted MNIST is not a very realistic benchmark, it fits our requirements for synthetic distribution shift (i.e., shuffling pixels). 

While \tabref{tab:perm-mnist-bn} demonstrates the benefit of adding BN (i.e., improving learning accuracy), we can observe a significant increase in average forgetting as well. To investigate this more, we visualize the BN statistics in \figref{fig:mlp-bn1} where we can see compared to \figref{fig:cifar-bn1}, the statistics change more, confirming our hypothesis in \secref{sec:batch-norm}. 


\begin{figure*}[h]
\centering
\begin{subfigure}{.45\textwidth}
      \centering
      \includegraphics[width=.99\linewidth]{figs/bn/mlp_bn1_running_mean.pdf}
      \caption{}
      \label{fig:mlp-bn1-running-mean}
\end{subfigure}\hfill
\begin{subfigure}{.45\textwidth}
      \centering
      \includegraphics[width=.99\linewidth]{figs/bn/mlp_bn1_running_var.pdf}
      \caption{}
      \label{fig:mlp-bn1-running-var}
\end{subfigure}\hfill
\begin{subfigure}{.45\textwidth}
      \centering
      \includegraphics[width=.99\linewidth]{figs/bn/mlp_bn1_weight.pdf}
      \caption{}
      \label{fig:mlp-bn1-weights}
\end{subfigure}\hfill
\begin{subfigure}{.45\textwidth}
      \centering
      \includegraphics[width=.99\linewidth]{figs/bn/mlp_bn1_bias.pdf}
      \caption{}
      \label{fig:mlp-bn1-bias}
\end{subfigure}
\caption{BN statistics for the first layer of \MLP-128 on Permuted MNIST: the statistics change more compared to~\figref{fig:cifar-bn1}}
\label{fig:mlp-bn1}
\end{figure*}

\begin{table}[h!]
\centering
\caption{Permuted MNIST: The MLP with BN has slightly higher learning accuracy, but significantly higher forgetting as well.}
\label{tab:perm-mnist-bn}
\resizebox{0.65\linewidth}{!}{%
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Model} & \textbf{Average Accuracy} & \textbf{Average Forgetting} & \textbf{Learning Accuracy} \\ \midrule
MLP-128 & 86.8 $\pm 0.95$ & 10.9 $\pm 0.88$ & 95.5 $\pm 0.33$ \\
MLP-128 + BN & 73.2 $\pm 0.82$ & 32.5 $\pm 0.72$ & 97.8 $\pm 0.45$ \\ \bottomrule
\end{tabular}%
}
\end{table}




\subsection{More number of epochs on Split CIFAR-100}
While in our main text, we have used 10 epochs for Split CIFAR-100, here in \tabref{tab:more-epochs} we show that the main conclusions hold even when we train the models longer.

\begin{table}[h!]
\centering
\caption{Comparing the impact of components in with two different settings: The components that are helpful in the short training time (e.g., removing GAP layers, adding pooling or batch norm layers), are also beneficial when the training time is longer.}
\label{tab:more-epochs}
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}ll|lll|lll|@{}}
\cmidrule(l){3-8}
 &  & \multicolumn{3}{c|}{\textbf{Epochs = 10}} & \multicolumn{3}{c|}{\textbf{Epochs = 50}} \\ \midrule
\textbf{Model} & \multicolumn{1}{c|}{\textbf{\begin{tabular}[c]{@{}c@{}}Params\\ (M)\end{tabular}}} & \multicolumn{1}{c}{\textbf{\begin{tabular}[c]{@{}c@{}}Average\\ Accuracy\end{tabular}}} & \multicolumn{1}{c}{\textbf{\begin{tabular}[c]{@{}c@{}}Average\\ Forgetting\end{tabular}}} & \multicolumn{1}{c|}{\textbf{\begin{tabular}[c]{@{}c@{}}Learning\\ Accuracy\end{tabular}}} & \multicolumn{1}{c}{\textbf{\begin{tabular}[c]{@{}c@{}}Average\\ Accuracy\end{tabular}}} & \multicolumn{1}{c}{\textbf{\begin{tabular}[c]{@{}c@{}}Average\\ Forgetting\end{tabular}}} & \multicolumn{1}{c|}{\textbf{\begin{tabular}[c]{@{}c@{}}Learning\\ Accuracy\end{tabular}}} \\ \midrule
ResNet-18 & 11.2 & 45.0  $\pm$ 0.63 & 36.8  $\pm$ 1.08 & 74.9  $\pm$ 3.98 & 37.1 $\pm$ 0.59 & 48.9 $\pm$ 1.35 & 82.4 $\pm$ 4.83 \\
ResNet-18 w/o GAP & 11.9 & 67.4  $\pm$ 0.76 & 11.2  $\pm$ 1.98 & 74.2  $\pm$ 4.79 & 66.1 $\pm$ 0.44 & 17.3 $\pm$ 1.43 & 80.2 $\pm$ 4.77 \\
ResNet-50 & 23.6 & 56.2  $\pm$ 0.88 & 9.5  $\pm$ 0.38 & 67.8  $\pm$ 5.09 & 53.4 $\pm$ 0.29 & 14.5 $\pm$ 0.64 & 75.3 $\pm$ 5.65 \\
ResNet-50 w/o GAP & 26.7 & 71.4  $\pm$ 0.29 & 6.6  $\pm$ 0.12 & 73.0  $\pm$ 5.18 & 71.2 $\pm$ 0.18 & 7.3 $\pm$ 0.22 & 76.5 $\pm$ 4.87 \\ \midrule
CNN x4 & 2.3 & 68.1  $\pm$ 0.5 & 8.7  $\pm$ 0.21 & 76.4  $\pm$ 6.92 & 62.6 $\pm$ 0.4 & 14.4 $\pm$ 0.62 & 75.2 $\pm$ 6.25 \\
CNN x4 + BN & 2.3 & 74.0  $\pm$ 0.56 & 8.1  $\pm$ 0.35 & 81.7  $\pm$ 6.68 & 68.9 $\pm$ 0.93 & 13.8 $\pm$ 0.68 & 80.7 $\pm$ 5.83 \\
CNN x4 + Maxpool & 2.3 & 74.4  $\pm$ 0.34 & 9.3  $\pm$ 0.47 & 83.3  $\pm$ 6.1 & 69.3 $\pm$ 0.79 & 13.5 $\pm$ 0.85 & 81.9 $\pm$ 5.47 \\ \midrule
CNN x8 & 7.5 & 69.9  $\pm$ 0.62 & 8.0  $\pm$ 0.71 & 77.5  $\pm$ 6.78 & 64.3 $\pm$ 0.82 & 13.2 $\pm$ 1.01 & 78.8 $\pm$ 6.61 \\
CNN x8 + BN & 7.5 & 76.1  $\pm$ 0.3 & 5.9  $\pm$ 0.16 & 81.7  $\pm$ 6.83 & 71.7 $\pm$ 0.79 & 11.5 $\pm$ 0.85 & 82.4 $\pm$ 6.18 \\
CNN x8 + Maxpool & 7.5 & 77.2  $\pm$ 0.53 & 7.1  $\pm$ 0.33 & 84.0  $\pm$ 5.81 & 73.6 $\pm$ 2.25 & 12.9 $\pm$ 1.07 & 84.4 $\pm$ 5.06 \\ \bottomrule
\end{tabular}%
}
\end{table}