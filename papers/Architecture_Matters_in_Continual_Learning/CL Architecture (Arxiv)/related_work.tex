\section{Related Work}

As we have discussed before, our work focuses on \emph{architectures} in continual learning rather than the \emph{algorithmic} side. Hence, we start with a brief overview of continual learning algorithms, and then we focus mainly on the part of the literature that is more related to the architecture.

\subsection{Algorithms}
On the algorithmic side of research, various methods have been proposed to alleviate the forgetting problem. \citep{zenke2017continual,ewc,mirzadeh2021linear,yin2020optimization} identify essential knowledge from the past, often in the form of parameters or features, and modify the training objective of the new task to preserve the learned knowledge. However, in practice, \textbf{replay} methods seem to be much more effective as they keep a small subset of previously seen data and either uses them directly when learning from new data (e.g., experience replay)~\citep{Chaudhry2019OnTE,rolnick2018experience,riemer2018learning} or incorporate them as part of the optimization process (e.g., gradient projection)~\citep{farajtabar2020orthogonal,chaudhry2018efficient,balaji2020effectiveness}, or learn a generative model~\citep{shin2017continual,kirichenko2021task} or kernel~\citep{DBLP:conf/icml/DerakhshaniZ0S21} to do so. 

Perhaps the most related side of algorithms to our work is the \textbf{parameter-isolation} methods where different parts of the model are devoted to a specific task~\citep{yoon2018lifelong,PackNet,SuperMaskInSuperPosition}. Often, these algorithms start with a fixed-size model, and the algorithm aims to allocate a subset of the model for each task such that only a specific part of the model is responsible for the knowledge for that task. For instance, PackNet~\citep{PackNet} uses iterative pruning to free space for future tasks, while in~\citep{SuperMaskInSuperPosition} the authors propose to fix with a randomly initialized and over-parameterized network to find a binary mask for each task. Nevertheless, even though the focus of these methods is the model and its parameters, the main objective is to have an algorithm that can use the model as efficiently as possible. Consequently, the significance of the architecture is often overlooked.

%  However, major challenges for all parameter-isolation algorithms are deciding when to expand the model, how much capacity is needed for different tasks, how much parameter sharing is needed for efficiency, and most importantly, how to operate when there are no task boundaries available.
 
% Another group of algorithms within this family may expand the model for new tasks rather than sharing the previous model. For instance, Reinforced Continual Learning (RCL)~\citep{ReinforcedCL} uses reinforcement learning techniques with an LSTM controller to decide on the expansion of the previous model.

We believe our work is orthogonal to the algorithmic side of continual learning research as any of the methods mentioned above can use a different architecture for their proposed algorithm, as we have shown in \figref{fig:intro-cifar-algs-vs-archs} and \tabref{tab:algorithms-vs-architectures}.

\subsection{Architecture}
There have been some efforts on applying architecture search to continual learning~\citep{ReinforcedCL,NASCIL,ENAS-CL,LearnToGrow,pmlr-v139-lee21a}. However, when it comes to architecture search, the current focus of continual learning literature is mainly on deciding how to efficiently share or expand the model by delegating these decisions to the controller. For instance, both~\citet{pmlr-v139-lee21a} and~\citet{pmlr-v139-lee21a} authors use a ResNet model as the base model and propose a search algorithm that decides whether the parameters should be reused or new parameters are needed for the new task, and the implications of architectural decisions (e.g., the width of the model, impact of normalization, etc.) on continual learning metrics have not been discussed. We believe future continual learning works that focus on the architecture search can benefit from our work by designing better search spaces that include various components that we have shown are important in continual learning. 

Beyond architecture search methods, there are several works that focus on the non-algorithmic side of CL. Related to the architecture, \citet{mirzadeh_understanding_continual} study the impact of width and depth on forgetting and show that as width increases, the forgetting will decrease. Our work extends their analysis for larger-scale ImageNet-1K benchmark and various architectures. Recently, \citet{EffectOfScaleInCL} shows that in the pre-training setups, the scale of model and dataset can improve CL performance. In this work, rather than focusing on the pre-training setups, we study the learning and retention capabilities of various architectures when models are trained from scratch. However, we show that while scaling the model can be helpful, the impact of architectural decisions is more significant. For instance, as shown in \tabref{tab:cifar-comp-all} and \tabref{fig:evolution-accs-imagenet}, the best performing architectures are not the ones with highest number of parameters.

% For instance, comparing the best architecture in~\citep{ReinforcedCL} for CIFAR-100 with the best architectures in \tabref{tab:cifar-comp-all} and \tabref{tab:improving architectures} reveals that it is possible to find architectures that are performing significantly better, even though their experiments includes ten tasks rather than twenty tasks in this work. We believe future architecture search methods in CL can build on the findings of this work to design a better search space that involves the architectural components as well.

Finally, while in \secref{sec:components} we have discussed the related works to each specific architectural component, we believe our work draws a comprehensive picture of various aspects of architectures in continual learning and poses interesting directions for future works.