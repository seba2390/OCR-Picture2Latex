\negspace{-1mm}
\section{Comparing Architectures}\label{sec:architectures}
\subsection{Experimental Setup}
Here, for brevity, we explain our experimental setup but postpone more detailed information (e.g., hyper-parameters, details of architectures, the justification behind our experimental setup choices, etc.) to Appendix~\ref{sec:appendix-setup-details}.



\subsubsection{Continual Learning Setup}
We use three continual learning benchmarks for our experiments. The Split CIFAR-100 includes 20 tasks where each task has the data of 5 classes (disjoint), and we train on each task for 10 epochs. The Split ImageNet-1K includes 10 tasks where each task includes 100 classes of ImageNet-1K and we train on each task for 60 epochs. Finally, for a few experiments, we use the small Rotated MNIST benchmark with 5 tasks where the first task is the standard MNIST dataset, and each of the subsequent tasks adds 22.5 degrees of rotation to the images of the previous task. The rationale behind our MNIST setup is that by increasing the rotation degrees, the shift across tasks increases and makes the benchmark more challenging~\citep{mirzadeh2021linear}. We note that the Split CIFAR-100 and Split ImageNet-1K benchmarks use a multi-head classification layer, while the MNIST benchmark uses a single-head classification layer. Thus, Split CIFAR-100 and Split ImageNet-1K belong to the so-called \emph{task incremental learning} setting, whereas Rotated MNIST belongs to \emph{domain incremental learning}~\citep{hsu2018re}.


We work with the most common architectures in the literature (continual learning or otherwise). We denote each architecture with a descriptor. \MLPArch represents fully connected networks with hidden layers of width \archstyle{N}.
Convolutions neural networks (CNN) are represented by \CNNArch where \archstyle{N} is the multiplier of the number of channels in each layer. Unless otherwise stated, the \CNN s have only convolutional layers (with a stride of 2), followed by a densely feed-forward layer for classification. For the CIFAR-100 experiments, we use three convolutional layers, and for the ImageNet-1K experiments, we use six convolutional layers. Moreover, whenever we add pooling layers, we change the convolutional layer strides to 1 to keep the dimension of features the same. The standard ResNet~\citep{he2016deep} of depth \archstyle{D} is denoted by \ResNetArch and WideResNets (WRN)~\citep{WRN-ZagoruykoK16} are denoted by \WRNArch where \archstyle{D} and \archstyle{N} are the depths and widths, respectively. Finally, we also use the recently proposed Vision Transformers (ViT)~\citep{VisionTransformerOriginal}. For the ImageNet-1K experiments, we follow the naming convention in the original paper~\citep{VisionTransformerOriginal}. However, for the Split CIFAR-100 experiments, we use smaller versions of \ViT s where \ViTArch stands for a 4-layer vision transformer with the hidden size of \archstyle{N} and MLP size of \archstyle{M}.

For \emph{each architecture}, we search over a large grid of hyper-parameters (refer to Appendix~\ref{sec:appendix-setup-details}) and report the best results. Further, we average the results over 5 different random initializations, for the corresponding best hyper-parameters, and report the average and standard deviations. Finally, for Split CIFAR-100 and Split ImageNet-1K benchmarks, we randomly shuffle the labels in each run, for 5 runs, to ensure that the results are not biased towards a specific label ordering.


\subsubsection{Metrics} \label{sec:metrics}
We are interested in comparing different architectures from two aspects: (1) how well an architecture can learn a new task \emph{i.e.} their \emph{learning ability} and (2) how well an architecture can preserve the previous knowledge \emph{i.e.} their \emph{retention ability}. For the former, we record \emph{average accuracy}, \emph{learning accuracy}, and \emph{joint/ multi-task accuracy}, while for the latter we measure the \emph{average forgetting} of the model. We now define these metrics.\\
(1) \emph{Average Accuracy $\in [0, 100]$} (the higher the better): The average validation accuracy after the model has been continually trained for $T$ tasks is defined as:
$
    A_T= \frac{1}{T} \sum_{i=1}^T a_{T,i}
$,
where, $a_{t,i}$ is the validation accuracy on the dataset of task $i$ after the model finished learning task $t$.\\
(2) \emph{Learning Accuracy $\in [0, 100]$} (the higher the better): The accuracy for each task directly after it is learned. The learning accuracy provides a good representation of the plasticity of a model and can be calculated using: $\text{LA}_T = \frac{1}{T} \sum_{i=1}^T {a_{i, i}}$. Note that for both Split CIFAR-100 and ImageNet-1K benchmarks, since tasks include images with disjoint labels, the standard deviation of this metric can be high. Moreover, since all models are trained from scratch, the learning accuracy of the first few tasks is usually smaller compared to later tasks.\\
(3) \emph{Joint Accuracy $\in [0, 100]$} (the higher the better): The accuracy of the model when trained on the data of all tasks together.\\
(4) \emph{Average Forgetting $\in [-100, 100]$} (the lower the better): The average forgetting is calculated as the difference between the peak accuracy and the final accuracy of each task, after the continual learning experience is finished. For a continual learning benchmark with $T$ tasks, it is defined as: $F = \frac{1}{T-1} \sum_{i=1}^{T-1}{\text{max}_{t \in \{1,\dots, T-1\}}~(a_{t,i}-a_{T,i})}$.






\subsection{Results}


We first compare different architectures on the Split CIFAR-100 and Split ImageNet-1K benchmarks. While this section broadly focuses on the learning and retention capabilities of different architectures, the explanations behind the performance gaps across different architectures and the analysis of different architectural components is given in the next section.  

\tabref{tab:cifar-comp-all} lists the performance of different architectures on Split CIFAR-100 benchmark. One can make several observations from the table. First, very simple \CNN s, which are not state-of-the-art in the single image classification tasks, significantly outperform both the \ResNet s, \WRN s, and \ViT s in terms of average accuracy and forgetting. This observation holds true for various sizes of widths and depths in all the architectures. A similar overall trend, where for a given parameter count, simple \CNN s outperform other architectures, can also be seen in Fig.~\ref{fig:intro-cifar-accs} and~\ref{fig:intro-cifar-forgets}.

Second, a mere increase in the parameters count, within or across the architectures, does not necessarily translate into the performance increase in continual learning. For instance, ResNet-18 and ResNet-34 have roughly the same performance despite almost twice the number of parameters in the latter. Similarly, WRN-10-10 outperforms WRN-16-10 and WRN 28-10 despite having significantly less number of parameters. Note that we do not draw a general principle that overparametrization is not helpful in continual learning. In fact, in some cases, it indeed is helpful as can be in the across-the-board performance improvement when ResNet-34 is compared with ResNet-50 or when the WRN-10-2 is compared to the WRN-10-10. In the next section, we analyze when overparametrization can help the performance in continual learning. 


Finally, explicitly comparing the learning and retention capabilities of different architectures, one can see from the table that \ResNet s and \WRN s have a higher learning accuracy suggesting that they are better at learning a new task. This also explains their frequent use in single task settings. However, in terms of retention, \CNN s and \ViT s are much better, as evidenced by their lower forgetting numbers. This is further demonstrated in \figref{fig:evolution-accs-cifar} (CIFAR-100) and \figref{fig:evolution-accs-imagenet} (ImageNet-1K), where it can be seen that \ResNet s and \WRN s learn each individual task much better resulting in a higher average accuracy for the first few tasks. However, as the number of tasks increases, \CNN s outperforms the other architectures due to their smaller forgetting, eventually translating into an overall flatter average accuracy curve.

\begin{center}
\centering
\begin{minipage}{0.47\textwidth}
\centering
\captionsetup{type=table}
\begin{table}[H]
\centering
\caption{Split CIFAR-100: the learning and retention capabilities can vary significantly across different architectures.}
\label{tab:cifar-comp-all}
\resizebox{0.99\linewidth}{!}{%
\begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{Model} & \textbf{\begin{tabular}[c]{@{}c@{}}Params\\ (M)\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Average\\ Accuracy\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Average\\ Forgetting\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Learning\\  Accuracy\end{tabular}} \\ \midrule
CNN x1 & 0.3 & 62.2 $\pm$1.35 & 12.6 $\pm$1.14 & 74.1$\pm$7.72 \\
CNN x2 & 0.8 & 66.3 $\pm$1.12 & 10.1 $\pm$0.98 & 75.8 $\pm$7.2 \\
CNN x4 & 2.3 & 68.1 $\pm$0.5 & 8.7 $\pm$0.21 & 76.4 $\pm$6.92 \\
CNN x8 & 7.5 & 69.9 $\pm$0.62 & 8.0 $\pm$0.71 & 77.5 $\pm$6.78 \\
CNN x16 & 26.9 & 76.8 $\pm$0.76 & 4.7 $\pm$0.84 & 81.0 $\pm$6.97 \\ \midrule
ResNet-18 & 11.2 & 45.0 $\pm$0.63 & 36.8 $\pm$1.08 & 74.9 $\pm$3.98 \\
ResNet-34 & 21.3 & 44.8 $\pm$2.34 & 39.9 $\pm$2.28 & 72.6 $\pm$6.34 \\
ResNet-50 & 23.6 & 56.2 $\pm$0.88 & 9.5 $\pm$0.38 & 67.8 $\pm$5.09 \\
ResNet-101 & 42.6 & 56.8 $\pm$1.62 & 9.2 $\pm$0.89 & 65.7$\pm$5.42 \\ \midrule
WRN-10-2 & 0.3 & 50.5 $\pm$2.65 & 36.5 $\pm$2.74 & 84.5 $\pm$5.04 \\
WRN-10-10 & 7.7 & 56.8 $\pm$2.03 & 31.7 $\pm$1.34 & 86.7 $\pm$4.94 \\
WRN-16-2 & 0.7 & 44.6 $\pm$2.81 & 41.4 $\pm$1.43 & 82.4 $\pm$6.09 \\
WRN-16-10 & 17.3 & 51.3 $\pm$1.47 & 37.6 $\pm$2.22 & 86.9 $\pm$3.96 \\
WRN-28-2 & 5.9 & 46.6 $\pm$2.27 & 39.5 $\pm$2.29 & 82.5 $\pm$6.26 \\
WRN-28-10 & 36.7 & 49.3 $\pm$2.02 & 35.8 $\pm$2.56 & 82.5 $\pm$6.26 \\ \midrule
ViT-512/1024 & 8.8 & 51.7 $\pm$1.4 & 21.9 $\pm$1.3 & 71.4 $\pm$5.52 \\
ViT-1024/1546 & 30.7 & 60.4 $\pm$1.56 & 12.2 $\pm$1.12 & 67.4 $\pm$5.57 \\ \bottomrule
\end{tabular}%
}
\end{table}
\end{minipage}\hfill
\begin{minipage}{0.472\textwidth}
\centering
\captionsetup{type=table}
\begin{table}[H]
\centering
\caption{Split ImageNet-1K: the learning and retention capabilities can vary significantly across different architectures.}
\label{tab:imagenet-comp-all}
\resizebox{0.99\linewidth}{!}{%
\begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{Model} & \textbf{\begin{tabular}[c]{@{}c@{}}Params\\ (M)\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Average\\ Accuracy\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Average\\ Forgetting\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Learning\\ Accuracy\end{tabular}} \\ \midrule
CNN x3 & 9.1 & 63.3 $\pm$0.68 & 5.4 $\pm$0.93 & 71.6 $\pm$2.31 \\
CNN x6 & 24.2 & 66.7 $\pm$0.62 & 3.9 $\pm$0.86 & 70.1 $\pm$3.21 \\
CNN x12 & 72.4 & 67.8 $\pm$1.04 & 2.8 $\pm$0.7 & 70.3 $\pm$2.82 \\ \midrule
ResNet-34 & 21.8 & 62.7 $\pm$0.53 & 17.3 $\pm$0.58 & 78.4 $\pm$2.57 \\
ResNet-50 & 25.5 & 66.1 $\pm$0.69 & 19.0 $\pm$0.67 & 83.3 $\pm$1.57 \\
ResNet-101 & 44.5 & 64.1 $\pm$0.72 & 18.9 $\pm$1.32 & 81.1 $\pm$2.89 \\ \midrule
WRN-50-2 & 68.9 & 63.2 $\pm$1.61 & 21.7 $\pm$1.73 & 85.8 $\pm$1.65 \\\midrule
ViT-Base & 86.1 & 58.3 $\pm$0.65 & 15.9 $\pm$1.11 & 72.8 $\pm$2.25 \\ 
ViT-Large & 307.4 & 60.7 $\pm$1.31 & 10.6 $\pm$1.1 & 73.2 $\pm$2.12 \\ \bottomrule
\end{tabular}%
}
\end{table}
\vspace{17mm}
\end{minipage}
\end{center}


\begin{figure*}[ht]
\centering
\begin{subfigure}{.47\textwidth}
      \centering
      \includegraphics[width=.99\linewidth]{figs/avg_accs_cifar.pdf}
      \caption{Split CIFAR-100}
      \label{fig:evolution-accs-cifar}
\end{subfigure}\hfill
\begin{subfigure}{.47\textwidth}
      \centering
      \includegraphics[width=.99\linewidth]{figs/avg_accs_imagenet.pdf}
      \caption{Split ImageNet-1K}
      \label{fig:evolution-accs-imagenet}
\end{subfigure}
\caption{Evolution of average accuracy for various architectures on (a) Split CIFAR-100: CNNs have smaller forgetting than other architectures while WideResNets have the highest learning accuracy, and (b) Split ImageNet-1K WideResNets and ResNets have higher learning accuracy than CNNs and ViTs. However, the latter has smaller forgetting.}
\end{figure*}

A trend similar to CIFAR-100 can also be seen in the ImageNet-1K benchmark as shown in Table~\ref{tab:imagenet-comp-all}. However, the performance difference, as measured by the average accuracy, between \CNN s and other architectures is smaller compared to that of CIFAR-100. We believe that this is due to the very high learning accuracy of other architectures compared to \CNN s on this benchmark, and hence their frequent use in the single task settings, resulting in an improved final average accuracy. The average forgetting of \CNN s is still much smaller than other architectures. 

Overall, from both tables, we conclude that \ResNet s and \WRN s have better learning abilities whereas \CNN s and \ViT s have better retention abilities. In our experiments, simple \CNN s achieve the best trade-off between learning and retention.