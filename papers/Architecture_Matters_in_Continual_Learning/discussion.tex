\negspace{4mm}
\section{Discussion}\label{sec:discussion}

In this work, we have studied the role of architectures in continual learning. Through extensive experiments on a variety of benchmarks, we have shown that different neural network architectures have different learning and retention capabilities, and a slight change in the architecture can result in a significant change in performance in a continual learning setting. 

Our work can be extended in several directions. We have already empirically demonstrated that different architectural components, such as width, depth, normalization, and pooling layers, have very specific effects in a continual learning setting. One could theoretically explore these components vis-Ã -vis continual learning. Similarly, designing architectural components that encode the inductive biases one hopes to have in a model, such as a measure of or resistance to the distributional shifts, a sense of relatedness of different tasks, or fast adaptation to new tasks, could be a very interesting future direction. We hope that the community takes up these questions and answers them in future works.

