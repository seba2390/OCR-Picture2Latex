\section{Introduction}\label{sec:intro}

Continual learning (CL)~\citep{ring1994continual,thrun1995lifelong} is a branch of machine learning where the model is exposed to a sequence of tasks with the hope of exploiting existing knowledge to adapt quickly to new tasks. The research in continual learning has seen a surge in the past few years with the explicit focus of developing algorithms that can alleviate \emph{catastrophic forgetting}~\citep{McCloskey1989CatastrophicII}---whereby the model abruptly forgets the information of the past when trained on new tasks.

While most of the research in continual learning is focused on developing \emph{learning algorithms}, that can perform better than naive fine-tuning on a stream of data, the role of model architecture, to the best of our knowledge, is not explicitly studied in any of the existing works. Even the class of parameter isolation or expansion-based methods, for example~\citep{rusu2016progressive,yoon2018lifelong}, have a cursory focus on the model architecture insofar that they assume a specific architecture and try to find an algorithm operating on the architecture. 
Orthogonal to this direction for designing algorithms, our motivation is that the inductive biases induced by different architectural components are important for continual learning. We seek to characterize the implication of different architectural choices.

\begin{figure*}[t]
\centering
\begin{subfigure}{.33\textwidth}
      \centering
      \includegraphics[width=.99\linewidth]{figs/intro_methods_vs_architectures.pdf}
      \caption{}
      \label{fig:intro-cifar-algs-vs-archs}
\end{subfigure}\hfill
\begin{subfigure}{.33\textwidth}
      \centering
      \includegraphics[width=.99\linewidth]{figs/intro_cifar_accs.pdf}
      \caption{}
      \label{fig:intro-cifar-accs}
\end{subfigure}\hfill
\begin{subfigure}{.33\textwidth}
      \centering
      \includegraphics[width=.99\linewidth]{figs/intro_cifar_forget.pdf}
      \caption{}
      \label{fig:intro-cifar-forgets}
\end{subfigure}
\vspace{-2mm}
\caption{Split CIFAR-100: (a) While compared to naive fine-tuning, continual learning algorithms such as EWC and ER improve the performance, a simple modification to the architecture (removing global average pooling (GAP) layer) can match the performance of ER with a replay size of 1000 examples. (b) and (c) Different architectures lead to very different continual learning performance levels in terms of accuracy and forgetting. This work will investigate the reasons behind these gaps and provide insights into improving architectures.}
\label{fig:intro-cifar}
\end{figure*}

To motivate, consider a ResNet-18 model~\citep{he2016deep} on Split CIFAR-100, where CIFAR-100 dataset~\citep{krizhevsky2009learning} is split into 20 disjoint sets---a prevalent architecture and benchmark in the existing continual learning works. \figref{fig:intro-cifar-algs-vs-archs} shows that explicitly designed CL algorithms, EWC~\citep{ewc} (a parameter regularization-based method) and experience replay~\citep{riemer2018learning} (a memory-based CL algorithm) indeed improve upon the naive fine-tuning. However, one can see that similar or better performance can be obtained on this benchmark by simply removing the global average pooling layer from ResNet-18 and performing the naive fine-tuning. This clearly demonstrates the need for a better understanding of network architectures in continual learning.

Briefly, in this work, we thoroughly study the implications of architectural decisions in continual learning. Our experiments suggest that different components of modern neural networks have different effects on the relevant continual learning metrics---namely average accuracy, forgetting, and learning accuracy (\emph{cf.} Sec.~\ref{sec:metrics})---to the extent that vanilla fine-tuning with modified components can achieve similar of better performance than specifically designed CL methods on a base architecture without significantly increasing the parameters count.  

We summarize our main contributions as follows:
\begin{itemize}[leftmargin=*]
    \item We compare both the learning and retention capabilities of popular architectures. To the best of our knowledge, the significance of architecture in continual learning has not been explored before.
    
    \item We study the role of individual architectural decisions (e.g., width and depth, batch normalization, skip-connections, and pooling layers) and how they can impact the continual learning performance.
    
    \item We show that, in some cases, simply modifying the architecture can achieve a similar or better performance compared to specifically designed CL algorithms (on top of a base architecture). 
    
    \item In addition to the standard CL benchmarks, Rotated MNIST and Split CIFAR-100, we report results on the large-scale Split ImageNet-1K benchmark, which is rarely used in the CL literature, to make sure our results hold in more complex settings. 
    
    \item Inspired by our findings, we provide practical suggestions that are computationally cheap and can improve the performance of various architectures in continual learning. 
\end{itemize}

We emphasize that the main objective of this work is to illustrate the significance of architectural decisions in continual learning, and this does not imply that the algorithmic side is not essential. In fact, one can enjoy the improvements on both sides, as we will discuss in Appendix~\ref{sec:appendix-additional-results}. Finally, we note that the secondary aim of this work is to be a stepping-stone that encourages further research on the architecture side of continual learning by focusing on the \emph{breadth} rather than \emph{depth} of some topics in this work. We believe our work provides many interesting directions that require deeper analysis beyond the scope of this paper but can significantly improve our understanding of continual learning.