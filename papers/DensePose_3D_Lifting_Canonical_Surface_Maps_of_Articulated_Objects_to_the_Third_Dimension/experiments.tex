\section{Experiments}\label{s:experiments}

\input{qual}

We evaluate the quality of our reconstruction on human and animal data, both synthetic and real, and then ablate various components.
%  learned on synthetic (UP-3D) and real (Human~3.6M) human data, and also show that it transfers well to the more diverse 3DPW dataset.
% % % We also train the model on the bear and zebra categories from the LVIS dataset and on a synthetic dataset of dog shapes.
% We ablate the different components of our method and
We compare our results to C3DPO~\cite{novotny19c3dpo} because it is the best-performing Non-Rigid SFM approach that works under assumptions compatible with ours.

Implementation details of the networks and training are provided in the sup.~mat.
We will share the Pytorch code.
% The results are reported in~\Cref{t:quant,f:h36mres,f:up3dres,f:lvisres,f:dogsres}.

\subsection{Datasets and metrics}

\paragraph{UP-3D and Stanford Dogs.}

First, we evaluate the method on two clean, synthetic datasets: UP-3D (humans) and Stanford Dogs.
UP-3D~\cite{Lassner2017} contains SMPL fits for 8515 photos of people rendered under 30 random viewpoints.
We orthographically project the mesh vertices and input their ground-truth visibility and vertex identity to \method directly (instead of using DensePose for UV extraction).
For Stanford Dogs, we follow UP-3D and fit a dog model to a subset of ImageNet using SMALify~\cite{biggs18creatures} on the mask and 2D keypoint annotations provided in StanfordExtra dataset~\cite{biggs2020wldo}.
We obtain in this way 6511 training and 4673 test instances.
% We remove cases where less than 10\% of the model area is visible because this usually results in poor SMAL fits.
% To model dog breed variations, we also learn a set of 5 blendshapes and apply them with random coefficients to the template mesh before posing it.
Please refer to the sup.~mat.~for further details.

We report the mean per-joint position error (MPJPE) of the reconstructions.
Since we have the ``ground-truth'' SMPL/SMAL model~$\X$ for each test instance, we compute MPJPE of the estimated shape in camera coordinates~$\bar\X = \hat\X R_0 + T_0$ using all keypoints (not only the visible ones):
$
\textrm{MPJPE}(\bar\X, \X) = \frac{1}{K} \sum_{k=1}^K \|\bar{X_k}- X_k \|.
$
Since via orthographic projection depth is only known up to a constant, we normalise depth by subtracting its mean from $\X$ and~$\bar\X$ before computing the loss.
We use the original train/test splits.

\input{qual-dogs}

\paragraph{Human 3.6M} consists of real images of 7 people equipped with motion-capture sensors performing various tasks in the lab environment.
The dataset provides the locations of 3D joints rather than full body surface.
Hence, for evaluation purposes, we compute the mean reconstruction error on $N_J = 14$ joints (RE$_{14}$).
In order to obtain the joints' positions~$\hat\J$ from the posed mesh  $\hat\X$, we run the pre-trained linear joint regressor from the SMPL model~\cite{loper15smpl:}: if the resulting joints are correct, the mesh must have been posed correctly.
We rigidly align the sets of points and find the optimal scale before computing the metric:
$
\textrm{RE}(\hat\J, \J) = \min_{s,R,T} \frac{1}{N_J} \sum_{i=1}^{N_J} \|(s\hat{J_i}R + T) - J_i \|.
$

For training, we sampled the videos at 10 frames per second, resulting in 311,424 images.
For evaluation, we use the scheme known as `Protocol \#1'.
The test set videos are sampled at 25 FPS, resulting in 109,792 images.
We ran the pre-trained DensePose detector from Detectron2~\cite{wu2019detectron2} on all images independently to obtain the input UV annotation,
then converted them to 2D projections of SMPL vertices as described in~\Cref{s:csm}.
We use the standard train/test split, setting out all images of subjects 9 and 11 for testing.

\paragraph{3DPW.}

We evaluate DensePose~3D in a transfer-learning setting, training it on Human~3.6M and evaluating it on 3DPW~\cite{marcard18recovering}.
DP3D takes keypoints as input, so is invariant to appearance changes and generalises well, as can be seen in~\Cref{t:quant,f:h36mres}.
We follow the same evaluation protocol as for Human~3.6M, comparing RE on 14 joints.

\paragraph{LVIS.}

Finally, we fit our model to LVIS dataset~\cite{gupta2019lvis} containing animal images taken ``in the wild''.
This task is more challenging, since each category comprises only about 2000 training instances, many of which have occluded parts.
To get input keypoints and visibilities~$(\Y,\Z)$, we pre-process the images with CSE~\cite{neverova2020continuous} in a similar way to DensePose.
%
% We pre-process the data with CSE~\cite{neverova20continuous} obtain the per-pixel descriptors from the joint embedding space with category-specific template mesh surface.
% For each pixel within the mask, we find the closest template mesh vertex in the embedding space.
% Let the set of pixels~$j$ that were matched to the vertex~$i$ of the template be
% \begin{equation}
% \mathcal{M}_i = \{j : i = \textrm{argmin}_{i'}~  \|\mathbf{e}_j - \mathbf{e}_{i'}\| \},
% \end{equation}
% where $e_j$ and $e_{i'}$ are the embeddings of the $j$-th pixel and $i'$-th vertex, respectively.
% Then, all vertices that have been matched to at least one pixel, i.e.~$\mathcal{M}_i \ne \emptyset$, are considered visible.
% For each visible vertex, we find the corresponding 2D keypoint location as the mean coordinate of the pixels matched to it: $y_i = \mathsf{E}_{j \in \mathcal{M}_i}~ p_j$, where~$p_j$ are the coordinates in the image grid.
% This way, we ensure that all occluded surface points are marked as invisible in the DensePose~3D input.
%
The output of CSE is noisier than the one of DensePose, so we predict heteroscedastic variance for the loss~\eqref{e:rep} and maximise the log-likelihood of the Laplace distribution as done by Novotny~\etal~\cite{novotny18capturing}; see sup.~mat.~for details on pre-processing and the loss.
Since there is no 3D ground truth, we provide only qualitative results in~\Cref{f:lvisres}.


\subsection{Comparison to baselines}

\begin{table}[tb]
\centering
\begin{tabular}{l|rrrr}
\toprule
Method                                    & UP-3D         & H3.6M          & 3DPW  & Dogs   \\ \midrule
HMR \cite{kanazawa18end-to-end}           & ---           & 56.8           & 81.3  & ---    \\
GraphCMR \cite{kolotouros19convolutional} & ---           & 50.1           & 70.2  & ---    \\
SPIN \cite{kolotouros19learning}          & ---           & 41.8           & 59.3  & ---    \\
Multi-bodies \cite{biggs20203d}           & ---           & 46.1           & 59.9  & ---    \\ \midrule
C3DPO~\cite{novotny19c3dpo}                  & 107.0         & 216.6          & 199.9 & 345.1    \\
no canon.\,loss~\eqref{e:canon}           & 183.6          & 135.4 & 120.3 & \textbf{241.4}     \\
no ARAP loss~\eqref{e:arap}               & 242.6          & 154.8 & 126.1 & 371.8  \\
no entropy loss~\eqref{e:entropy}         & 113.8          & 119.4 & 99.1  & 505.2  \\
no parts model                            & 205.9         & 125.0          & 102.3 & 684.3   \\
\method (ours)                            & \textbf{91.2} & \textbf{113.6} & \textbf{95.2} &  247.1   \\ \bottomrule
\end{tabular}
\caption{%
\textbf{Evaluation of mesh reconstruction}
reporting mean per-joint position error (MPJPE) on UP-3D and Dogs datasets, and reconstruction error (RE) on Human 3.6M and 3DPW.
The first half of the table shows the results of methods that use 3D supervision.
\method is then compared to C3DPO~\cite{novotny19c3dpo} applied to dense keypoints
and ablated.%\vspace{-1cm}
% To compute RE$_{14}$, we apply the joint regressor from SMPL to the reconstructed mesh.
}\label{t:quant}
\end{table}

We compare our method to C3DPO~\cite{novotny19c3dpo}, where we use 10-dimensional basis and find the optimal strength of canonicalisation loss in the interval~$[0.1, 1]$.
The results are in~\Cref{t:quant} and supplementary figures.
Note that we train C3DPO on dense keypoints (i.e.~6890 input points for humans), while \cite{novotny19c3dpo} trains on 17 sparse joints, which makes results from \Cref{t:quant} incomparable with the ones in \cite{novotny19c3dpo}.
UP-3D and Dogs are less-challenging datasets with clean 2D keypoints and few extreme poses, so C3DPO's simple linear pose model is only slightly inferior to DP3D.
In contrast, the gap is large on Human~3.6M and 3DPW: C3DPO outputs the mean pose failing to adapt to the data.

\input{qual-h36m3dpw}

\subsection{Ablation study}

\paragraph{Removing the loss functions.}

We report the effect of removing various regularisers in \Cref{t:quant}.
Each of them proves important:
the canonicalisation loss prevents predicting a degenerate, flat shape;
the ARAP loss makes the prediction smooth and helps to learn smooth part segmentations by encouraging local rigidity;
the entropy loss makes the part segmentation sharper, allowing the shape to flex more.

\input{qual-lvis}

\paragraph{Removing the part-based model.}

Two reasons why C3DPO may work poorly on dense point clouds are:
(1) learning a very large linear predictor for thousand of points may lead to overfitting, or
(2) the linear model may be unable to capture surface deformations.
We test these hypotheses by replacing the articulation model in our method with a C3DPO-like linear basis.
To reduce the number of parameters in the basis, we express it as a function of the LBO basis $\mathbf{U}$ (\cref{s:method}) and define the posed mesh as
$\label{e:lboc3dpo}
\X = (\alpha \otimes I_3) \mathbf{W}^b \mathbf{U},
$
where $\mathbf{W}^b \in \mathbb{R}^{D \times N_u}$ are trainable parameters, $\alpha$ is a $D$-dimensional vecror of shape coefficients, $\otimes$ is Kronecker product, and $I_3$ is a 3-dimensional identity matrix.
% the basis, we use the blendshapes in form of projected LBO eigenfunctions computed on the template mesh.
% In particular, using the matrix of harmonics $\mathbf{U} \in \mathbb{R}^{N_u \times K}$ defined in~\Cref{s:model}, we compute the predicted shape~$\X$ as
% $\label{e:lboc3dpo}
% \X = (\alpha \otimes I_3) \mathbf{W}^b \mathbf{U},
% $
We train using~\cref{e:totalloss} but remove the entropy loss~\eqref{e:entropy} (as this model has no parts).
We set the number of blendshapes $D=10$ and find the optimal weight of canonicalisation loss in the $[0.1, 1]$ range.

The penultimate row in~\Cref{t:quant} reveals the correct hypothesis.
The model without parts performs significantly better than C3DPO, proving that overfitting explains in large part C3DPO's poor performance.
However, the no-parts model still cannot reach the performance of \method on real-world data (columns 2 and 3), which means that the latter is more efficient than using vanilla linear blendshapes.
Remarkably, the no-parts model performs decently on the synthetic datasets, where  DensePose  annotations  were  simulated  by projecting 3D locations obtained from a parametric model, probably because, despite of high dimensionality, the ``rank'' of the data is still small.
% more appropriate for articulated shapes than linear combination of blendshapes.
The differences are more pronounced in the visual results in \Cref{f:h36mres,f:lvisres,f:dogsres}.
The linear model in most cases produces symmetrical shapes, which tend to be similar regardless of the input, while DP3D with parts reconstructs the movements of arms more accurately.\vspace{-0.5ex}

\begin{figure}[b!]%\vspace{-2ex}
    \includegraphics[width=0.99\linewidth]{images/varnumparts}%\vspace{-3ex}%
    \caption{%
    Reconstruction quality w.r.t.~the number of parts.%
    }
    \label{f:varnumparts}
\end{figure}

\paragraph{Number of latent parts.}

\Cref{f:varnumparts} measures the reconstruction error as a function of the number of latent parts~$M$ on human datasets.
As expected from human anatomy, the method needs at least 5 parts to model the articulation of the body.
The metrics plateau after 10 parts.\vspace{-0.5ex}% (note that the 14-joint RE on Human 3.6M is a noisy metric).

\paragraph{Limitations and robustness.}
DensePose\,3D can be only as good as training annotations provided by DensePose or CSE.
In supp.~mat., we investigate the sensitivity of training to annotation noise, random sparsity (typical for manual annotation), and missing body parts (caused by occlusions).
