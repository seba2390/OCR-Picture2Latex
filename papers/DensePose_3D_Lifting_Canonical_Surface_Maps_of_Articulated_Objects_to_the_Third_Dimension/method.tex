\section{Method}\label{s:method}

We aim to learn reconstructing the 3D shape of a deformable object such as a human or an animal from 2D images, and to do so without 3D supervision.
Instead, we only use dense 2D object points that can be annotated manually or predicted by means of a method such as DensePose, also known as a canonical surface map (CSMs).

We summarise the necessary CSM background in~\cref{s:csm} and then discuss our method.

% We aim at reconstructing the shape of a 3D object, such as a human, from a single image of it.
% While many single-view 3D reconstruction methods take as input images directly, in our work we take advantage of the large body of research on predicting canonical surface maps (CSMs) for humans~\cite{guler18densepose:,Neverova2019,neverova2020continuous} and animals~\cite{kulkarni19canonical,sanakoyeu2020transferring,Kulkarni2020}.
% In order to make the paper self-contained, we first provide the necessary background on CSMs (\Cref{s:csm}) and then introduce our model.

\subsection{Canonical surface maps}\label{s:csm}

A CSM~\cite{thewlis17dense,guler18densepose:,Neverova2019,neverova2020continuous,kulkarni19canonical,sanakoyeu2020transferring,Kulkarni2020} is defined with respect to a reference 3D template, usually given as a triangular mesh with vertices
$
\V = (V_k)_{k = 1}^{K} \in \mathbb{R}^{K\times 3}.
$
For humans, for example, a common reference mesh is the SMPL rest pose (which was created by a 3D artist).

A CSM such as DensePose takes as input an image $I : \Omega \rightarrow \mathbb{R}$ of the object and assigns to each pixel $y \in \Omega$ a point in the mesh $\V$, producing a map $\Omega \rightarrow \V$.\footnote{In practice, the map is valued in $\V \cup \{\text{bkg}\}$ to allow to mark pixels that do not belong to the object as background.}
% To a first approximation, the output of the CSM is thus index $k$ of the vertex $V_k$ corresponding to each pixel $y$.
% In practice, for better accuracy one endows the mesh with continuous UV coordinates, which allows to specify correspondence with points within each triangular face.
%
While this is useful information, it is not yet a 3D reconstruction of the object in the image because $\V$ is a fixed reference template.
In order to obtain a 3D reconstruction, we need instead to \emph{pose} the template by finding a suitable deformation
$
 \X = (X_k)_{k=1}^K \in \mathbb{R}^{K\times 3}
$
of its vertices.

% with UV coordinates assigned to its vertices, DensePose predicts rasterised UV coordinates for each pixel of an input image $I$, had the perfectly posed model~$\{X_k(I) | X_k(I) \in \mathbb{R}^{3}\}_{k = 1}^{K}$ been projected to the image.
% We then use DensePose output to find the 2D image coordinates of visible mesh vertices by means of linear interpolation in the triangulated UV space.

As the first step in the posing process, we `reverse' the CSM output and, for each vertex $V_k$ of the template, find its corresponding pixel location $y_k$, resulting in a collection of 2D vertex locations
$
\Y = (y_k)_{k = 1}^{K} \in \mathbb{R}^{K\times 2}
$.
Due to occlusions, a vertex may be invisible in the image, which prevents extracting its 2D location~$y_k$ from the CSM\@.
Thus we also define visibility indicators
$
\Z = (z_k)_{k = 1}^{K} \in \{0, 1\}^{K}
$.

Note that $\Y$ can also be obtained from the posed mesh $\X$ and the camera projection function $\pi_I$ as
$
y_k = \pi_I(X_k).
$
This calculation does not involve the CSM at all and, as we show later, can be used to constrain the reconstruction.

%  \emph{another} way of obtaining the values $y_k$ that does not involving the CSM at all.
% Specifically, given  $\X$ of the posed mesh, we can write
% $
% y_k = \pi_I(X_k)
% $
% where $\pi_I$ is the camera projection function.
% As we are going to discuss later, the interplay between these two derivation allows us to learn the reconstruction model.

% As a result, the input to our method is the set of image coordinates $\Y = \{y_k\}_{k = 1}^{K} \in \mathbb{R}^{K\times 2}$.
% Here, $y_k = \pi_I(X_k)$ stands for a camera projection $\pi$ of the $k$-th posed vertex $X_k$ into image $I$.
% A vertex is visible, i.e.~$z_k = 1$, if and only if the above interpolation succeeds, namely when the UV coordinates of~$V_k$ belong to the convex hull of the predicted UV coordinates in the image.

% original paragraph
% \subsection{Input data preprocessing}\label{s:preprocessing}
% While many single-view 3D reconstruction methods act directly on images,
% we take advantage of the large body of research on producing canonical surface maps (DensePose) for humans~\cite{guler18densepose:,Neverova2019,neverova2020continuous} and animals~\cite{sanakoyeu2020transferring}.
% Given the category template mesh $
% \V = \{V_k\}_{k = 1}^{K} \in \mathbb{R}^{K\times 3}
% $
% such as SMPL rest pose, with UV coordinates assigned to its vertices, DensePose predicts rasterised UV coordinates for each pixel, had the perfectly posed model~$\X$ been projected to the image.
% We use DensePose output to find the 2D image coordinates of visible mesh vertices by means of linear interpolation in the triangulated UV space.
% As a result, the input to our method is the set of image coordinates $\Y = \{y_k\}_{k = 1}^{K} \in \mathbb{R}^{K\times 2}$, corresponding to template mesh vertices:~$y_k = \pi(X_k)$.
% Due to occlusions, vertices may be invisible in the image, thus we also define visibility indicators~$\Z = \{z_k\}_{k = 1}^{K} \in \{0, 1\}^{K}$.
% A vertex is visible, i.e.~$z_k = 1$, if and only if the above interpolation succeeds, namely when the UV coordinates of~$V_k$ belong to the convex hull of the predicted UV coordinates in the image.

\subsection{Shape model}\label{s:model}

In order to reconstruct the 3D shape of an object from 2D annotations, we must define a \emph{shape model} that constrains the space of possible reconstructions $\X$.
%given the 2D projections~$\Y$ and visibility indicators~$\Z$ of its vertices. 
% In order to get a robust predictor, instead of predicting~$\X$ directly, we exploit the structure of the human body.
To this end, we assume that the underlying object, which could be a human or another animal, has a skeletal structure.
Under this assumption, the pose of the object is expressed by the rigid transformations of $M$ parts
\begin{equation}
g_m = (R_m,T_m) \in SE(3), \quad m=1,\dots,M.
\end{equation}
We assume that each vertex $\V_k$ in the template belongs to one of the $M$ parts with membership strength
$
P_{km} \in [0,1]
$
such that
$
\sum_{m=1}^M P_{km} = 1.
$
The posed vertices~$\X$ are given by the linear combination of part transformations, as in linear blend skinning (LBS):
\begin{equation}\label{e:skinning}
X_k = \sum_{m=1}^M P_{km} \cdot g_{m}(g_{0m}^{-1}(V_k)).
\end{equation}
Here $g_{0m} \in SE(3)$ stands for the rest pose of the $m$-th part.

While we do not force the parts to have a particular semantic, we expect learning to group together surface points that move rigidly together, e.g all points on a forearm.
Next, we explain how we encourage such a solution to emerge.

\paragraph{Part segmentation.}

Having defined per-vertex deformations, we will now describe the part segmentation model $\mathbf{P} = [P_{km}]\in\mathbb{R}^{K\times M}$.
As mentioned before, unlike other parametric models~\cite{loper15smpl:,anguelov05scape:}, we do not require a pre-segmented template shape.
Instead, we treat the part segmentation~$\mathbf{P}$ as a latent variable and learn it together with the rest of the model parameters.
Note that the part segmentation is independent of a particular input instance --- this means that the part assignments stay constant once training finishes.
Intuitively, limiting the number of parts and constraining deformations within parts to rigid ones should force the model to group the vertices that move according to the same rigid transform into the same part.

\paragraph{Smooth segmentation with LBO.}

While we have reduced the deformation of the template to the rigid motions of a small number of parts ($M=10$), the assignment of the template vertices to the different parts can still be irregular, which may lead to unrealistic body deformations.
We address this issue by enforcing the part assignments $\mathbf{P}$ to be smooth.
Combined with~\cref{e:skinning}, this encourages the deformations of the template to be smooth as well.

%By smoothness of the part assignments we mean that $\mathbf{P}$, regarded as a function of the mesh points, should be smooth.
We formalise this intuition by requiring the part assignment~$\mathbf{P}$ to be a smooth function on the mesh surface.
This can be enforced by making sure that $\mathbf{P}$ only contains `low frequency' components.
Formally, this is achieved by expressing $\mathbf{P}$ as a linear combination of selected eigenfunctions of the Laplace-Beltrami operator (LBO~\cite{rustamov2007laplace}), illustrated in \Cref{fig:lbo-fig}.

In more detail, consider the discrete approximation $\Delta$ of the LBO for the reference template mesh $\V$.
Let $\mathbf{u}_i \in \mathbb{R}^{K}$ be the (orthonormal) eigenvectors of $\Delta$ sorted by increasing eigenvalue magnitude, and let $\mathbf{U} = (\mathbf{u}_i)_{i=1}^{N_u}\in\mathbb{R}^{K\times N_u}$ be the matrix containing the $N_u$ first eigenvectors.
We define the part segmentation as
\begin{equation}\label{e:parts}
\mathbf{P} = \textrm{softmax}(\mathbf{U} \mathbf{W}),
\end{equation}
where $\mathbf{W} \in \mathbb{R}^{N_u \times M}$ is a parameter matrix, and the softmax is taken with respect to the part index $k$.

Smoothness can be further increased by reducing~$N_u$ or by initialising~$\mathbf{W} = [W_{im}]$ with decreasing magnitude.
Specifically, we use a variant of Xavier initialisation and set
$
W_{im} \thicksim \mathcal{N}(0, \frac{\exp(- i / \bar{\sigma})}{M^{1/2}}).
$
This focuses the model on low-frequency harmonics at the beginning of training.

\input{lbo_fig}

\paragraph{Transformations predictor.}

Given input 2D keypoint locations~$\Y$ and visibilities~$\Z$, we train a multi-layer perceptron (MLP) to predict the $(M+1)$ rigid part transformations:
\begin{equation}\label{e:transforms}
  \{h_m\}_{m=0}^M = \Phi(\Y, \Z).
\end{equation}
We express transformations in log-space, meaning that
$
(R_m, T_m) = g_m = \exp(h_m)
$
where~$\exp: \mathbb{R}^6 \to \mathbb{R}^{3 \times 3} \times \mathbb{R}^3$ is the exponential map of $SE(3)$; see~\cite{Blanco2010} for details.

Note that we estimate the additional global transformation $h_0$; this is the camera pose used to re-project the posed shape, which is expressed in the object reference frame, back to the image (see \cref{e:rep}).
Note also that \cref{e:skinning} requires the inverse part transformation at rest $g_{0m}^{-1}$; these are learnt as logarithms of canonical pose angles~$\mathbf{w}^r_0 \in \mathbb{R}^{M \times 6}$ so that~%
$
\forall m: \quad g_{0m}^{-1} = \exp(\mathbf{w}^r_{0m})
$.

\subsection{Training}

We train the MLP~\eqref{e:transforms}, mapping the 2D points to the pose parameters, and the part segmentation model~\eqref{e:parts} by combining a number of losses.

% $\Phi(\Y)$ that maps the observed 2D keypoints $\Y$ to the $M$ transformations $g$ per~\cref{e:transforms}, then the transformations are applied to the template mesh vertices per \cref{e:skinning} to get the 3D reconstruction in the world coordinates~$\X$.

\paragraph{Re-projection loss.}

The first loss ensures that the posed mesh reprojects correctly onto the 2D points:
\begin{equation}\label{e:rep}
\mathcal{L}_\text{rep}
=
\frac{\sum_{k=1}^K 
z_k a_k
\left \|
y_k - \pi\left(X_k R_0 + T_0 \right)
\right \|}
{\sum_{k=1}^K z_k a_k},
\end{equation}
where $X_k$ and $(R_0,T_0)$ are obtained by composing the pose regressor~\eqref{e:transforms} with the skinning function~\eqref{e:skinning}.
We weigh the mesh vertices~$V_k$ with the areas~$a_k$ of the corresponding barycells to make the loss resampling-invariant.

\paragraph{Canonicalisation loss.}

The authors of C3DPO~\cite{novotny19c3dpo} proposed the \emph{canonicalisation} loss to remove the ambiguity in recovering the camera pose and a 3D reconstruction, which also helps with overfitting.
% observed that increasing the number of blendshapes can overfit the re-projection loss~\eqref{e:rep} with poor 3D reconstructions.
% In order to be able to increase the model capacity without incurring these degenerate solutions, they proposed to regularise the solution via a \textit{canonicalisation} loss.
% Although our deformation space less over-parametrised than C3DPO (because we do not need to learn a deformation basis), we found that the canonicalisation loss still helps to learn more realistic shapes.
% The canonicalization loss contains some trainable parameters that are updated in parallel with the ones of the model.
The idea is to learn an auxiliary network $\hat \X \approx \Psi(\hat{\X}\tilde{R})$ tasked with undoing a random rotation $\tilde{R}$ applied to the point cloud $\hat\X$ (defined in the object coordinates).
Novotny~\etal~\cite{novotny19c3dpo} prove that this loss can be minimised only if the predicted shapes~$\hat\X$ are indeed canonical w.r.t.~orientation, meaning that the model cannot predict two different reconstructions $(\hat\X_1, \hat\X_2)$ that only differ by a rigid transformation.
Specifically, the loss is formulated as
\begin{equation}\label{e:canon}
\mathcal{L}_\text{canon}
=
\sum_{k=1}^K 
\left \|
\left[\hat{\X} - \Psi(\hat{\X} \tilde{R}) \right]_k
\right \|,
\end{equation}
where~$\tilde{R} \in \mathbb{R}^{3 \times 3}$ is a random rotation matrix and $[\cdot]_k$ extracts the $k$-th row of its argument.

% \paragraph{Geodesic distortion loss.}
% \rk{TODO: remove, unless we decide to use it again.}
% For regularization, let $\mathcal{E}$ be the set of vertex pairs $(k,q)$ connected by a triangle edge in the 3D mesh.
% Consider their distances in the reference 3D model:
% $$
% D^0_{kq} = \|V_k - V_q\|
% $$
% Then, the distortion loss is:
% \begin{equation}\label{e:geoloss}
% \mathcal{L}_\text{dist}
% =
% \sum_{(k,q)\in\mathcal{E}}
% \|
% D_{kq}
% -
% D^0_{kq}
% \|,
% \quad
% D_{kq} = \|X_k - X_q\|.
% \end{equation}

\paragraph{ARAP loss.}

To further increase the robustness of the reconstruction, we encourage the deformation of the template shape to be as-rigid-as-possible (ARAP)~\cite{Sorkine2007}.
This is particularly useful when, as it is often the case, the input DensePose annotations are noisy and biased.
% \cite{Sorkine2007} proposed ARAP regulariser for shape morphing.
ARAP measures the cost of deforming the template mesh $\V$ into the posed mesh $\X$:
% In particular, the energy term for morphing mesh~$\V$ to~$\X$ has the form
\begin{equation}\label{e:arap}
%\begin{aligned}
\mathcal{L}_\text{arap}(\X;\V)
=
\sum_{k=1}^K
\min_{R \in \textit{SO(3)}}
\sum_{q \in \mathcal{N}(k)} 
w_{kq} 
\left \|
V_{\vec{kq}} - X_{\vec{kq}}R
\right \|,
%\end{aligned}
\end{equation}
where $\mathcal{N}(k)$ denotes indices of adjacent template vertices,
$V_{\vec{kq}} = V_q - V_k$, $X_{\vec{kq}} = X_q - X_k$, 
and weights~$w_{kq}$ are defined proportionally to the area of the faces incident to the edge~$kq$;
see~\cite{Sorkine2007} for details.
We back-propagate the error through estimated coordinates~$X_k$ and~$X_q$ but stop gradients after fitting the rotation~$R$.

\paragraph{Entropy regularisation.}

Sometimes the model tends to assign several part indices to a single vertex, which makes deformations too rigid.
We thus regularise the segmentation model by penalising the entropy of the part distribution for each vertex using the following loss:
\begin{equation}\label{e:entropy}
\mathcal{L}_{\textrm{entropy}} = - \frac{1}{K} \sum_{k=1}^K \sum_{m=1}^M P_{km} \log P_{km}.
\end{equation}

\paragraph{Learning formulation.}

To train the method, we optimise the parameters of the networks $\Phi$, $\Psi$, and the matrix~$\W$ (\cref{e:parts}) minimising a weighed combination of the losses above:
\begin{equation}\label{e:totalloss}
\mathcal{L}
=
\mathcal{L}_{\textrm{rep}} +
w_{\textrm{entropy}} \mathcal{L}_{\textrm{entropy}} +
w_{\textrm{canon}} \mathcal{L}_{\textrm{canon}} +
w_{\textrm{arap}} \mathcal{L}_{\textrm{arap}}.
\end{equation}
Loss weights~$\mathbf{w}$ are treated as hyper-parameters, see supplementary material for the values used for the experiments.