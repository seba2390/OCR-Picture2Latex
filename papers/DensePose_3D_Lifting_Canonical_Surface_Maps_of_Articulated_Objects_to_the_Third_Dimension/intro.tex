\section{Introduction}

Recent advances in deep learning have produced impressive results in monocular 3D reconstruction of articulated and deformable objects, at least for particular object categories such as humans.
Unfortunately, while such techniques are general in principle, their success is rather difficult to replicate in other categories.
Before learning to reconstruct 3D objects from images, one must first learn a model of the possible 3D shapes of the objects.
For humans, examples of such models include SMPL~\cite{loper15smpl:} and GHUM~\cite{xu2020ghum}.
Constructing these requires a large dataset of 3D scans of the objects deforming and articulating over time, which have to be acquired with specialised devices such as domes.
Not only this hardware is uncommon, complex and expensive, but it is also difficult if not impossible to apply to many objects of interest, such as wild animals or even certain types of deformable inanimate objects.
Then, after building a suitable 3D shape model, one still has to train a deep neural network regressor that can predict the shape parameters given a 2D image of the object as input~\cite{kolotouros19convolutional,zanfir2020neural,Kanazawa2018}.
Supervising such a network requires in turn a dataset of images paired with the corresponding ground-truth 3D shape parameters.
Images with paired reconstructions are also very difficult to obtain in practice.
Some images may be available from the same scanners that have been used to construct the 3D model in the first place, but these are limited to `laboratory condition' by definition.
Thus, while there is abundance of `in the wild' images of diverse object categories that can be obtained from the Internet, they are lacking 3D ground-truth and are thus difficult to use for learning 3D shape predictors.

In this paper, we are interested in bootstrapping 3D models and monocular 3D predictors \emph{without} using images with corresponding 3D annotations or even unpaired 3D scans.
Fortunately, other modalities can provide strong cues for reconstruction.
For example, previous work~\cite{Kanazawa2018,novotny19c3dpo,kong2019deep,chen2019learning} leveraged 2D annotations for semantic keypoints to accurately reconstruct various object categories.
While these keypoints provide a supervisory signal at sparse image locations, DensePose~\cite{guler18densepose:,neverova2020continuous,sanakoyeu2020transferring} provides \emph{dense} correspondences between the images of humans or other animals and 3D templates of these categories.
Example of these annotations are shown on the left of~\Cref{f:splash}, where the colours encode the indices of corresponding points on the template mesh.
DensePose annotations can be seen as generalising sparse joint locations, with two important differences:
the density is much higher, and the correspondences are defined on the \emph{surface} of the object rather than in its skeleton joints.
Such dense annotations can be obtained manually or with detectors pre-trained on those manual 2D annotations, with the same degree of flexibility and generality as sparse 2D landmarks, while providing much stronger cues for learning detailed 3D models of the objects.
However, such annotations do not appear to have been used to bootstrap 3D object models before.

The main goal of this work is thus to leverage dense surface annotations, such as the ones provided by DensePose, in order to learn a parametric model of a 3D object category without using any 3D supervision.
As done in~\cite{Kanazawa2018,novotny19c3dpo,kong2019deep,chen2019learning}, we further aim to learn a deep neural network predictor that aligns the model to individual 2D input images containing the object of interest.
Our method assumes only having an initial \textit{rigid} canonical 3D template of the object category generated by a 3D artist.
There is no loss of generality here since knowledge of the template is required to collect DensePose annotations in the first place.%
\footnote{The 3D template is used by the human annotators as a reference to mark correspondences and defines the canonical surface mapping for the object category.}
Thus, pragmatically, we include this template in our model.

Our main contribution is a novel parametric mesh model for articulated object categories, which we call \emph{DensePose~3D} (DP3D).
In a purely data-driven manner, DP3D learns to softly assign the vertices of the initial rigid template to one of a number of latent parts, each of which moving in a rigid manner.
The parametrization of the mesh articulation is then given by a set of per-part rigid transforms expressed in the space of the logarithms of $\textsf{SE}(3)$.
In order to pose the mesh, each vertex of the template shape is deformed with a vertex-specific transformation defined as a convex combination of the part-specific transforms, where the weights are supplied by the soft segmentation of the corresponding vertex.
In order to prevent unrealistic shape deformations, we enforce smoothness of the part segmentation, and consequently of the vertex-specific offsets, by expressing the part assignment as a function of a truncated eigenbasis of the Laplace-Beltrami operator computed on the template mesh, which varies smoothly along the mesh surface.
We further regularise the mesh deformations with the as-rigid-as-possible (ARAP) soft constraint.

DP3D is trained in a weakly supervised manner, in the sense that our pipeline (including DensePose training) does not require 3D annotations for the input images.
In an end-to-end fashion, we train a deep pose regressor that, given a DensePose map extracted from an image, predicts the shape deformation parameters, poses the mesh accordingly, and minimises the distance between the projection of the posed mesh to the image plane and the input 2D DensePose annotations.
We show that our method does not need manual DensePose annotations for the training images; it can learn even from the predictions of a DensePose model trained on a different dataset.
This way, DP3D can learn to infer the shape of humans and animals from an unconstrained dataset containing diverse poses.
% by observing images from an arbitrary dataset of bodies in different poses.
Since DP3D does not use images directly but only the DensePose annotations or predictions,
it is robust to changes in the object appearance statistics, which makes it suitable for transfer learning.

We conduct experiments on a synthetic dataset of human poses, and on the popular Human~3.6M benchmark, showing that the model trained on staged Human~3.6M generalises to a more natural 3DPW dataset.
We also fit the models to animal categories in the LVIS dataset.
Note that learning reconstruction of LVIS animals would be impossible with any method requiring 3D supervision since there are no scans or parametric models available for species like bears or zebras.
DP3D produces more accurate reconstructions than a state-of-the-art Non-rigid Structure-from-Motion (NR-SfM) baseline and compares favourably with fully-supervised approaches.
