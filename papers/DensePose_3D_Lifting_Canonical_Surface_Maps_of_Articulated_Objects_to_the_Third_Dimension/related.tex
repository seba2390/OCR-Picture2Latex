\section{Related work}\label{s:related}

In this section we review the relevant prior art: monocular human mesh reconstruction, canonical surface maps, and non-rigid SfM.

% The proposed method solves a non-rigid structure-from-motion (NRSFM) problem, so we start with reviewing the work addressing it.
% We use as input a category-specific canonical surface map, so we proceed with reviewing the methods to produce those maps, focusing on the category of humans.
% Finally, we review methods for dense reconstruction of articulated human bodies from single view.

\paragraph{Image-based human body reconstruction.}

A popular method for reconstructing 3D humans from 2D images is test-time optimisation, where a parametric human model such as SMPL~\cite{loper15smpl:} or SCAPE~\cite{anguelov05scape:} is fitted to a given test image by minimising various types of energies, including 2D keypoint and mask reprojection losses~\cite{guan09estimating,sigal08combined,bogo16keep,Lassner2017,huang17towards,zanfir18monocular,joo18total,pavlakos19expressive,xiang19monocular}.
Alternatively, one can learn a deep regressor which, given a single image as input, predicts the parameters of the 3D shape model directly.
Most methods~\cite{anguelov05scape,mehta17vnect,rogez18lcr-net,sun18integral,kolotouros19convolutional, martinez17a-simple} reconstruct only a sparse set of 3D points, usually corresponding to 2D body joint detections.
HMR~\cite{kanazawa18end-to-end} and GraphCMR~\cite{kolotouros19convolutional} regress instead full 3D meshes.
Kolotouros~\etal~\cite{kolotouros19learning} combine the test-time optimization and deep regression paradigms.
Biggs~\etal~\cite{biggs20203d} regress multiple mesh hypotheses to deal with the inherent ambiguity of monocular 3D reconstruction.
While such methods achieve state-of-the-art monocular human mesh recovery, they require large dataset with 3D annotations to train the 3D shape model and the regressor.
In contrast, our method is trained only with 2D image annotations.

\paragraph{Self-supervised 3D human pose estimation.}

Other methods aim at reconstructing 3D body skeletons without 3D annotations.
Some works leverages multi-view constraints~\cite{kocabas2019self,pavlakos2017harvesting,rhodin2018learning} while Pavlakos \etal~\cite{pavlakos2018ordinal} assume ordinal depth supervision.
Alternatively, adversarial networks can also be used to learn 3D models from 2D annotations in a monocular setup~\cite{kudo2018unsupervised,drover20183dpose,chen2019unsupervised}.
The idea is to train a discriminator that tells if the 2D reprojection of the reconstructed 3D points from multiple random views is plausible or not.
% attempts to distinguish ground-truth keypoint configurations from projections of 3D reconstructions into new random views.
While these methods work well, their inability to deal with occluded keypoints makes them unsuitable for dense reconstruction.
%  state-of-the-art performance on learning sparse skeleton points, they are unsuitable for our dense shape reconstruction problem with occluded keypoints.

\begin{figure*}[tb]
\begin{center}
\includegraphics[width=\linewidth]{images/overview_figure.pdf}%
\vspace{-0.7em}%
\caption{\textbf{Overview of our method.}
The input 2D keypoints~$\Y$ are passed to the network~$\Phi$ that predicts global and per-part rigid transformations.
LBO harmonics are used to regress the soft part segmentation~$\mathbf{P}$.
The transformations, part segmentation, along with the template mesh~$\V$, are used for Linear Blend Skinning to obtain the shape~$\X$.
During training, this shape enters re-projection, canonicalisation, and ARAP losses, while the entropy loss is defined on part segmentation.%
\vspace{-0.5cm}%
}\label{f:overview}
\end{center}
\end{figure*}

\paragraph{Canonical surface maps.}

DensePose~\cite{guler18densepose:} was perhaps the first method to predict dense assignments from an image to a reference 3D template of the human body, also called a \emph{Canonical Surface Map} (CSM).
It introduced a dataset with manually labelled correspondences as well as a new deep network architecture to regress dense correspondences from images.
Follow-up work introduced semi-supervised learning~\cite{Neverova2019} and transferred human correspondences to quadrupeds~\cite{sanakoyeu2020transferring}.
Most recently, Neverova~\etal~\cite{neverova2020continuous} reformulated DensePose as a non-parametric problem by predicting canonical point embeddings for image pixels,
which facilitates its application to a wider range of deformable object categories.

Other works aimed at learning CSMs with limited or no supervision:
{}\cite{thewlis17dense,thewlis19unsupervised,schmidt17self-supervised} do so by using principles such as transformation equivariance, whereas~\cite{kulkarni19canonical} enforces consistency with an initial 3D model of the object.
Relevant to our work, the articulation-aware variant of it~\cite{Kulkarni2020} produces canonical surface maps for categories such as quadruped animals.
The method requires a segmented template mesh with a predefined skeleton structure; in contrast, we learn the articulated structure automatically without supervision.

\paragraph{Non-rigid structure-from-motion.}

NR-SfM is relevant to our work as its goal is to reconstruct a deformable 3D object from 2D keypoint annotations.
The seminal work of Bregler~\cite{bregler00recovering}, which proposed to express the possible deformations of the 3D shape as a linear combination of a small number of basis shapes, has since inspired many follow-up works~\cite{agudo2018image, fragkiadaki2014grouping, dai2014simple, zhu2014complex, akhter2009nonrigid, akhter2011trajectory, agudo2017dust, gotardo2011non, kumar2018scalable, kumar2017spatial, zhu2014complex, agudo2018deformable, zhou2016sparseness, zhou2016sparse, torresani2008nonrigid}.
Traditionally, such methods posed the problem as matrix factorization, but more recently some alternative that leverage deep learning have emerged.
DeepNRSfM~\cite{kong2019deep, wang2020deep} and, more relevantly to our work, C3DPO~\cite{novotny19c3dpo} train an MLP that maps the vectorised list of 2D keypoints to camera and shape parameters and minimise the distance between the input 2D keypoints and the 3D point reprojections.
While C3DPO works well with sparse keypoints such as the human joints, as we show in the experiments, it fails to handle the dense collections of points required to reconstruct meshes.
We address this issue by utilising the known category-level template mesh to learn deformations compatible with the articulation of a latent skeletal structure.
