
%% bare_conf.tex
%% V1.4b
%% 2015/08/26
%% by Michael Shell
%% See:
%% http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.8b or later) with an IEEE
%% conference paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/pkg/ieeetran
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE! 
%% User assumes all risk.
%% In no event shall the IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%*************************************************************************


% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. The IEEE's font choices and paper sizes can   ***
% *** trigger bugs that do not appear when using other class files.       ***                          ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/



\documentclass[conference]{IEEEtran}
% Some Computer Society conferences also require the compsoc mode option,
% but others use the standard conference format.
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[conference]{../sty/IEEEtran}





% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/pkg/ifpdf
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.






% *** CITATION PACKAGES ***
%
%\usepackage{cite}
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of the IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off
% such as if a citation ever needs to be enclosed in parenthesis.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 5.0 (2009-03-20) and later if using hyperref.sty.
% The latest version can be obtained at:
% http://www.ctan.org/pkg/cite
% The documentation is contained in the cite.sty file itself.






% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation
% can be obtained at: 
% http://www.ctan.org/pkg/graphicx
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found at:
% http://www.ctan.org/pkg/epslatex
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). The IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex





% *** MATH PACKAGES ***
%
\usepackage{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics.
%
% Note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/amsmath





% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as the IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/pkg/algorithms
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/pkg/algorithmicx




% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/array


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.




% *** SUBFIGURE PACKAGES ***
\ifCLASSOPTIONcompsoc
  \usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\else
  \usepackage[caption=false,font=footnotesize]{subfig}
\fi
% subfig.sty, written by Steven Douglas Cochran, is the modern replacement
% for subfigure.sty, the latter of which is no longer maintained and is
% incompatible with some LaTeX packages including fixltx2e. However,
% subfig.sty requires and automatically loads Axel Sommerfeldt's caption.sty
% which will override IEEEtran.cls' handling of captions and this will result
% in non-IEEE style figure/table captions. To prevent this problem, be sure
% and invoke subfig.sty's "caption=false" package option (available since
% subfig.sty version 1.3, 2005/06/28) as this is will preserve IEEEtran.cls
% handling of captions.
% Note that the Computer Society format requires a larger sans serif font
% than the serif footnote size font used in traditional IEEE formatting
% and thus the need to invoke different subfig.sty package options depending
% on whether compsoc mode has been enabled.
%
% The latest version and documentation of subfig.sty can be obtained at:
% http://www.ctan.org/pkg/subfig




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure.
% Be aware that LaTeX2e kernels dated 2015 and later have fixltx2e.sty's
% corrections already built into the system in which case a warning will
% be issued if an attempt is made to load fixltx2e.sty as it is no longer
% needed.
% The latest version and documentation can be found at:
% http://www.ctan.org/pkg/fixltx2e


\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/stfloats
% Do not use the stfloats baselinefloat ability as the IEEE does not allow
% \baselineskip to stretch. Authors submitting work to the IEEE should note
% that the IEEE rarely uses double column equations and that authors should try
% to avoid such use. Do not be tempted to use the cuted.sty or midfloat.sty
% packages (also by Sigitas Tolusis) as the IEEE does not format its papers in
% such ways.
% Do not attempt to use stfloats with fixltx2e as they are incompatible.
% Instead, use Morten Hogholm'a dblfloatfix which combines the features
% of both fixltx2e and stfloats:
%
% \usepackage{dblfloatfix}
% The latest version can be found at:
% http://www.ctan.org/pkg/dblfloatfix




% *** PDF, URL AND HYPERLINK PACKAGES ***
%
\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/url
% Basically, \url{my_url_here}.
\usepackage{color}
\usepackage{gensymb}
\usepackage{comment}
\usepackage{graphicx}
\usepackage{amsmath}
%\usepackage{algorithm}
%\usepackage{algpseudocode}
\usepackage{gensymb}
\usepackage{multirow}
\usepackage{tabularx}
\usepackage{adjustbox}

\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{xcolor}
\usepackage{comment}
% \usepackage{minted}
%\usepackage{float}
\usepackage{soul}


% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
%
% paper title
% Titles are generally capitalized except for words such as a, an, and, as,
% at, but, by, for, in, nor, of, on, or, the, to and up, which are usually
% not capitalized unless they are the first or last word of the title.
% Linebreaks \\ can be used within to get better formatting as desired.
% Do not put math or special symbols in the title.
\title{A Unified Hardware Architecture for Convolutions and Deconvolutions in CNN}
%A FPGA-based LiDAR Processing Platform for Autonomous Driving


% author names and affiliations
% use a multiple column layout for up to three different
% affiliations
\author{
\IEEEauthorblockN{Lin Bai, Yecheng Lyu and Xinming Huang}
\IEEEauthorblockA{Worcester Polytechnic Institute\\
\{lbai2, ylyu, xhuang\}@wpi.edu}
}
%Worcester, MA 01609, USA\\

% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}

% make the title area
\maketitle

% As a general rule, do not put math, special symbols or citations
% in the abstract
\begin{abstract}
Deconvolution plays an important role in the state-of-the-art convolutional neural networks (CNNs) for the tasks like semantic segmentation, image super resolution, etc. 
\begin{comment}
image denoising \cite{ref:denoise_2017}, 
etc. However, its intrinsic property like computation complexity prevents it from real-time applications.
\end{comment} 
In this paper, a scalable neural network hardware architecture for image segmentation is proposed. By sharing the same computing resources, both convolution and deconvolution operations are handled by the same process element array. In addition, access to on-chip and off-chip memories is optimized to alleviate the burden introduced by partial sum. As an example, SegNet-Basic has been implemented using the proposed unified architecture by targeting on Xilinx ZC706 FPGA, which achieves the performance of 151.5 GOPS and 94.3 GOPS for convolution and deconvolution respectively. This unified convolution/deconvolution design is applicable to other CNNs with deconvolution.  
\end{abstract}




% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle



\section{Introduction}
In recent years, CNNs have been widely used in computer vision applications such as classification \cite{ref:vgg_2014}, object detection \cite{ref:fasterrcnn_2015}%\cite{ref:yolo2_2017}
, and semantic segmentation \cite{ref:fcn_2015}. However, a CNN usually requires intensive computations, which limits its applicability on embedded devices. To address this issue, FPGA based accelerators \cite{ref:eyeriss_2016} %\cite{ref:chi_fpga_2015}
have been proposed so that CNNs can be important on real-time embedded systems. As a key operation in many neural networks, deconvolution has been widely used in the state-of-the-art CNNs especially for semantic segmentation\cite{ref:fcn_2015}, image super resolution\cite{ref:super_res_2017}, and image denoising\cite{ref:denoise_2017}. Through a learnable way, deconvolution extrapolates new information from input feature maps, which outperforms other interpolation algorithms such as nearest neighbor and bi-cubic interpolation. However, unlike hardware acceleration for convolution, much less attention has been paid on deconvolution. Due to the fact that 
\begin{comment}
1) there are lots of CNNs, especially for segmentation tasks like FCN\cite{ref:fcn_2015}, U-net\cite{ref:u-net_2015} and SegNet\cite{ref:segnet_2017}, contain deconvolution, and 2) 
\end{comment}
deconvolution may become the bottleneck in speed if only convolution has been accelerated, there is a urgent need to optimize the deconvolution operation on FPGAs.
\begin{comment}
According to the network structure of CNNs mentioned above, the feature extraction part includes convolution only while during upsampling, both convolution and deconvolution are involved in a hybrid way. It's not possible to perform both operation in parallel. So that separating the convolution and deconvolution as two peripherals is not efficient. Therefore, merging them into one computation unit is more practical for real world applications.
\end{comment}

\begin{comment}
The operation numbers of convolution and deconvolution are extremely unbalanced in most neural networks like U-Net, FCN
%(Fig.~\ref{fig:cnn_op_stat})
, where convolution still dominants the computation complexity. Besides, concerning to the network structure, convolution and deconvolution won't work in parallel. This gives a good chance to merge these two operations into one block\cite{ref:shuanglong_2018}.
\end{comment}

%\begin{figure}[htbp]
%	\centering
%	\includegraphics[width=0.9\columnwidth]{fig/cnn_op_stat.jpg}
%	\caption{The percentage of network operation}
%	\label{fig:cnn_op_stat}
%\end{figure}

\section{Related Work}

Lately tremendous research progress has been made on high-performance and low-power CNN accelerators. In \cite{ref:utku_2017}, the authors proposed a novel architecture for process element array, which dramatically reduced the external memory bandwidth requirements by intensive data reuse and outperforms the systolic-like structures\cite{ref:chen_2016}. A high-throughput CNN accelerator design was implemented in \cite{ref:xuechao_2017}, where a comprehensive design space exploration on top of accurate models was deployed to determine the optimal design configuration. 
\begin{comment}
Theoretically, those architectures are also capable of performing deconvolution operations. However, the zero-padding in deconvolution creates a great number of redundant calculations like multiplying by zeros.
\end{comment}

Comparing to the accelerator design for convolution, that of deconvolution has not been thoroughly investigated. 
Liu et al. proposed an CNN architecture where convolution and deconvolution were accelerated on the system separately\cite{ref:shuanglong_2018}. This architecture is not efficient enough because in most CNNs, convolution and deconvolution do not work in parallel. A high performance deconvolution module in \cite{ref:xinyu_2017} used reverse looping and stride hole skipping techniques, but with the penalty of additional hardware resources and latency. An unified systolic accelerator was developed in \cite{ref:fcnelement_2018}, which divided the deconvolution into two steps. Firstly, it multiplied one vector with the kernel and then stored the temporary matrices in on-chip memory. Next, it added the overlaps of temporary matrices. This method increased the on-chip BRAM access and introduced unnecessary data storage. Consequently both power consumption and computation latency grew.

To address the issues mentioned above, we analyze the deconvolution properties and fit it into our proposed process element array so that both convolution and deconvolution can be handled by sharing the same on-chip resources. The contributions of our work are summarized as follows:

\begin{itemize}
	\item A novel process element structure is proposed so that both convolution and deconvolution are supported without extra circuit.
	\begin{comment}
	\textcolor{red}{Memory access is organized to boost the processing rate.}
	\end{comment}
	\item A scalable CNN accelerator architecture is proposed, in which data rate and processing speed are configurable depending on the on-chip resources and bandwidth of the target FPGA. 
	\item SegNet-Basic has been implemented on Xilinx ZC706 FPGA efficiently. Its throughput are 151.5 GOPS and 94.3 GOPS for convolution and deconvolution respectively, which outperforms most of the existing FPGA accelerators.
\end{itemize}

The rest of paper is organized as follows. Section \ref{sec:Backgrounds} introduces the background concept. The hybrid-optimization strategies are described in Section \ref{sec:Optimization}. Hardware architecture and its implementation results of SegNet-Basic are discussed in Section \ref{sec:Architecture}-\ref{sec:Results}. In the end, Section \ref{sec:Conclusions} concludes the paper. 

\section{Backgrounds}\label{sec:Backgrounds}
\subsection{CNN architecture for segmentation}
Typical segmentation neural network architecture consists of an encoder and a decoder as shown in Fig.~\ref{fig:seg_arch}. The encoder is used to extract the features from the input image and the decoder generates the segmented output. In CNNs like SegNet and U-Net, decoder reverses their encoder, while others replace decoder with smaller one. Additional connections between encoder and decoder were introduced to SegNet \cite{ref:segnet_2017} and U-Net \cite{ref:u-net_2015}. Without the extra connection, CNN still works but with a small decrease of precision. Usually, the encoder is comprised of convolutional layers and the decoder consists of deconvolutional layers.

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.6\columnwidth]{fig/seg_arch.png}
	\caption{Typical CNN structure for semantic segmentation}
	\label{fig:seg_arch}
\end{figure}

\begin{comment}
\subsection{Convolution}
\st{As the most important operation of CNNs, the convolution layer extracts the features of input feature maps and pushes them to the latter layers. Convolution (in} Fig.~\ref{fig:conv_op}) \st{first multiplies the corresponding elements in the $K\!\times\!K\!\times\!D_{I}$ patch of input image (marked in green) with the kernel (marked in yellow) and then sum all of them up to generate one pixel of output feature map.}
\end{comment}
\begin{comment}
For each channel, convolution consumes 9 multipliers and 8 adders if fully pipelined.

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.6\columnwidth]{fig/conv_op.png}
	\caption{Convolution operation}
	\label{fig:conv_op}
\end{figure}
\end{comment}

\begin{comment}
Convolution involves three-dimensional multiply and accumulate operations of input feature maps and convolution kernel weights. Convolution is comprised of four levels of loops as shown in the pseudo codes in Figure ? and illustrated in Figure ?.
\end{comment}
%To make the analysis simpler, we assume $D_I=1$. The mathematical formula of convolution is Eq.~\ref{eq:conv}.

%\begin{equation}
%	\centering
%	OFM_{(i,j)}=\sum_{i=-\frac{K-1}{2}}^{\frac{K-1}{2}}\sum_{j=-\frac{K-1}{2}}^{\frac{K-1}{2}}(IFM_{(i,j)}*Kernel_{(i,j)})
%	\label{eq:conv}
%\end{equation}
%In case of zero padding, we have to add $\frac{K-1}{2}$ lines of zeros at top, bottom, left and right sides of input feature maps.
\subsection{Deconvolution}
Deconvolution, also called transposed convolution, is a learnable method to perform upsampling. If a convolution unit is directly reused for deconvolution, it consists of the following two steps: 1) padding the input feature map and 2) applying convolution on the padded feature map, as indicated in Fig.~\ref{fig:deconv_op}. After padding, an input feature map with size $I\!F_W\times I\!F_H$ is expanded into $(2\cdot I\!F_W+1)\times (2\cdot I\!F_H+1)$, and consequently the output feature map size becomes $(2\cdot I\!F_W-1)\times (2\cdot I\!F_H-1)$. In order to get exactly twice the size, extra padding for upper row and left column (highlighted in blue in Fig.~\ref{fig:deconv_op}) is needed. 

\begin{comment}
Fig.~\ref{fig:deconv_op} demonstrates an example of the deconvolution when stride=2 and kernel size=3.
The deconvolution visualization in case of stride=2 and kernel size=3, is demonstrated in Fig.~\ref{fig:deconv_op}. It demonstrates how to upsample a $2\times 2$ feature map into $3\times 3$ or $4\time 4$ feature map.
\end{comment}
\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.7\columnwidth]{fig/deconv_op2.png}
	\caption{Deconvolution operation by padding and convolution}
	\label{fig:deconv_op}
\end{figure}

%\begin{figure}[h]
%	\includegraphics[width=0.6\columnwidth]{fig/padding_deconv.pdf}
%\end{figure}
\section{Optimization}\label{sec:Optimization}
Because of the limited resources on an FPGA, a high performance CNN accelerator has to be deeply optimized on memory access and data transfer while maximizing the resource utilization.
\subsection{Loop optimization}
To efficiently map the convolution loops, three loop optimization techniques, loop unrolling, loop tiling and loop interchange, have been considered to customize the computation and communication patterns of the accelerator. Loop unrolling is the parallelism strategy for certain convolution loops, which demands more multipliers. Loop tiling determines the partition of feature maps, and consequently determines the required size of on-chip memory. Loop interchange decides the computation order of the convolution loops \cite{ref:optmloop_2017}.

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.8\columnwidth]{fig/loop.png}
	\caption{Four levels of convolution loops}
	\label{fig:conv_loop}
\end{figure}

After carefully judging all three optimization methods, our optimization strategy is to unroll loop-1, and partially unroll loop-2, loop-4, and apply loop tiling on depth of input feature maps (Fig.~\ref{fig:conv_loop}). In order to jointly optimize with deconvolution, loop-1 is fully unrolled (more details about this will be explained in the next section). In order to reduce the number of partial sums and data transfer, loop-2 must be unrolled as much as possible. However, as the large amount of multipliers are required, loop-2 is only partially unrolled. Further consideration has been taken for on-chip memory access minimization. Therefore, loop-4 is unrolled because of pixel reuse. As the partial sum in loop-2 are stored in BRAM, no more overhead to the off-chip memory is added.

\begin{comment}
\begin{minted}{matlab}
for no in Nof ----------> Output channel loop
  for (y,x) in (Y-K,X-K) --> Feature map loop
    for ni in Nif -------> input channel loop
      for (kx,ky) in (K,K) -----> Kernel loop
            Fout[no,y,x] += Fin[ni,y-ky,x-kx]
            * K[no,ni,ky,kx]
    Fout[no] += n[no]
\end{minted}
\end{comment}

\subsection{Deconvolution}\label{sec:opt_deconv}
Fig.~\ref{fig:deconv_op} presents 
\begin{comment}
an example of extending on a $2\times 2$ feature map, which is
\end{comment}
the naive way to implement the deconvolution on hardware. As it can be seen, too many operations are wasted in multiplication by zeros.
%If convoluting the extended feature map with a $3\times 3$ kernel, we have the output feature map in Fig.~\ref{fig:deconv_result}.


\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.7\columnwidth]{fig/deconv_result.png}
	\caption{Optimization of deconvolution}
	\label{fig:deconv_result}
\end{figure}


 The mathematical expression of the $2\times 2$ feature map deconvolution is given in Fig.~\ref{fig:deconv_result}. Based on equation (\ref{eq:deconv_1}-\ref{eq:deconv_4}) for the deconvolution, we conclude most of the redundant multiplications can be avoided. The procedure is summarized into three steps: 1) padding the input feature map if size doubling is expected; 2) scanning the padded input feature map by a $2\times 2$ sliding window; 3) applying deconvolution for each patch using kernel as in equation (\ref{eq:deconv_1}-\ref{eq:deconv_4}). Three examples are highlighted by colored squares in padded feature map and output feature map in Fig.~\ref{fig:deconv_result}.

\begin{align}
    O\!F_{11}\!=&I\!F_{11}\!\cdot\!K_{11}\!+\!I\!F_{12}\!\cdot\!K_{13}\!+\!I\!F_{21}\!\cdot\!K_{31}\!+\!I\!F_{22}\!\cdot\!K_{33}\label{eq:deconv_1}\\
    O\!F_{12}\!=&I\!F_{12}\!\cdot\!K_{12}\!+\!I\!F_{22}\!\cdot\!K_{32}\\
    O\!F_{21}\!=&I\!F_{21}\!\cdot\!K_{21}\!+\!I\!F_{22}\!\cdot\!K_{23}\\
    O\!F_{22}\!=&I\!F_{22}\!\cdot\!K_{22}\label{eq:deconv_4}
\end{align}

According to TensorFlow, during deconvolution, the kernel should be rotated by 180\degree. To make the figure easier to understand, we assume that the kernel has been rotated already.

\begin{comment}
    \begin{equation}
        \begin{split}
        OF_{11} = & IF_{11}\cdot K_{11}+IF_{12}\cdot K_{13} \\
        & +IF_{21}\cdot K_{31}+IF_{22}\cdot K_{33}
        \label{eq:deconv_1}
        \end{split}
    \end{equation}
    \begin{equation}
        %\centering
        OF_{12} = IF_{12}\cdot K_{12}+IF_{22}\cdot K_{32}
        \label{eq:deconv_2}
    \end{equation}
    \begin{equation}
        %\centering
        OF_{21} = IF_{21}\cdot K_{21}+IF_{22}\cdot K_{23}
        \label{eq:deconv_3}
    \end{equation}
    \begin{equation}
        %\centering
        OF_{22} = IF_{22}\cdot K_{22}
        \label{eq:deconv_4}
    \end{equation}
\end{comment}

\begin{comment}
From hardware perspective, these four equations require 9 multiplications and 5 additions, which means we can reuse the architecture of convolution process element described in the previous section.
\end{comment}
%\subsection{Accessing memory}

\subsection{Quantization method}
A fine-tuning with quantization constraint method \cite{ref:yecheng_tcas_2019} is employed in our design. It effectively diminishes the negative impact of brute-force quantization while introducing more non-linearity. Different from the ordinary quantization method \cite{ref:google_quant}, we quantize the weights and bias before storage. This quantization method does not require modification of the TensorFlow source code.
\begin{comment}
    As illustrated in Fig.~\ref{fig:quant}, d\textcolor{red}{what is the benefit?}
\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.4\columnwidth]{fig/quant.png}
	\caption{Quantization scheme}
	\label{fig:quant}
\end{figure}
\end{comment}

%Alg.~\ref{alg:quant} describes how the parameter regulator works.
%%%%%%%%%%%%%%%%%%%%%%%% parameter quantization algorithm begin
%\begin{algorithm}[htbp]
%	\caption{Normalize Points}\label{alg:quant}
%	\begin{flushleft}
%		\textbf{Input:} \textbf{W} - weights, \textbf{G} - gradients\\
%		\textbf{Input:} \textbf{N} - total bit-width, \textbf{F} - fraction bit-width\\
%		\textbf{Output:} \textbf{Wq} - quantized weights
%	\end{flushleft}
%	\begin{algorithmic}[1]
%		\State $Fs \gets 2^F$ \Comment{fraction scale}
%		\State $Ub \gets 2^{N-1}-1$ \Comment{upper bound}
%		\State $Bb \gets -2^{N-1}$ \Comment{bottom bound}
%		\State $\hat{W}\gets W\cdot Fs$
%		\State $\hat{W}\gets round(\hat{W})$
%		\State $\hat{W}\gets max(Bb,\hat{W})$
%		\State $\hat{W}\gets min(Ub,\hat{W})$
%		\State $\hat{W}\gets \hat{W}/Fs$
%		\State $Wq\gets W$ + \textbf{StopGradient}($\hat{W}$-$W$)
%		\State \textbf{return} $Wq$
%	\end{algorithmic}
%\end{algorithm}
%%%%%%%%%%%%%%%%%%%%%%%% parameter quantization algorithm end

\section{Hardware Architecture}\label{sec:Architecture}
The overview of hardware architecture is shown in Fig.~\ref{fig:acce_arch}. Line buffer converts the convolution into matrix multiplication by re-organizing the input image. The process element array multiplies input image by the weights. After batch normalization, activation and pooling, the output feature map is stored in Output Featuremap (OF) buffer.

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.9\columnwidth]{fig/acce_arch.png}
	\caption{Overview of hardware architecture}
	\label{fig:acce_arch}
\end{figure}

\subsection{Line buffer}
\begin{comment}
\textcolor{red}{
The input data stream from DMA is firstly buffered by stream FIFO. The padding controller decides sending the feature map element or 0 to line buffer.
}
\textcolor{red}{
It reforms the feature map into $3\times 1$ sliding window and then sends them into dual-port RAM. In order to diminish the time cost for feature map transmission, two clock domains are introduced and linked by the dual-port RAM (Fig.~\ref{fig:zero_pad}). Higher speed clock is assigned to input feature map filling side and lower speed clock is assigned to CNN processing side.
}
\end{comment}

A line buffer is designed to build the expected sliding window and to perform zero-padding for convolution. In the proposed accelerator, line buffer bridges the AXI DMA and Input Featuremap (IF) buffer (Fig.~\ref{fig:zero_pad}). Data and valid signals from AXI Stream interface are inserted into cascaded FIFOs. Extra logic is added to generate the status signals (including empty and full signals) of each FIFO, so that the line buffer is able to fit feature maps with different sizes. Padding controller decides when to push the data into each FIFO and when to output zero padding according to the pre-loaded padding mode.

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.9\columnwidth]{fig/zero_padding.png}
	\caption{Block diagram of line buffer}
	\label{fig:zero_pad}
\end{figure}

We choose not to buffer one entire feature map in on-chip memory, because the buffer size would be restricted by the limited on-chip memory and this could result inefficient buffer usage when feature map size drops. Therefore, only part of the input feature map is loaded into IF buffer and consequently this requires different zero-padding modes. Hence we provide different pre-loaded work modes for this line buffer.

\begin{comment}
demonstrated in Fig.~\ref{fig:pad_scheme}. Each pink square represents one padding mode, where the white part is the content of input feature map and the grey part is the expected zero-padding. If including the no-padding mode, the line buffer supports 13 padding modes totally.
\end{comment}

\begin{comment}
\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.4\columnwidth]{fig/padding_scheme.png}
	\caption{Zero padding modes}
	\label{fig:pad_scheme}
\end{figure}
\end{comment}


\subsection{Process element array}
The input feature map is stored in the IF buffers in form of $3\times 1$ vector, which reduces the BRAM consumption. To rebuild the $3\times 3$ / $2\times 2$ sliding window for convolution/deconvolution, a shift register is placed between IF buffer and process element arrays. Each process element array consists of multiple process elements as in Fig.~\ref{fig:conv}, usually in a power of 2. This number is scalable, depending on the bandwidth of platform. Each process element comprises of a 9-multiplier array and an adder tree to sum up the products.

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.6\columnwidth]{fig/process_engine.png}
	\caption{PE array structure and its connection to buffers}
	\label{fig:conv}
\end{figure}

\subsubsection{Convolution}
During convolution, $3\times 3$ sliding windows and their corresponding weights are transmitted into process element array. In the process element, they are multiplied and summed up.

\begin{comment}
\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.85\columnwidth]{fig/pe_deconv.png}
	\caption{Block diagram of process element}
	\label{fig:deconv}
\end{figure}
\end{comment}

\subsubsection{Deconvolution}
As discussed in Section \ref{sec:opt_deconv}, the deconvolution for each $2\times 2$  patch consumes $9$ multiplications and $5$ additions. This fits well to the proposed process element. During deconvolution, the process element structure is reused but with a different data routing mechanism.

Different from convolution who outputs one pixel after another, in the deconvolution mode, one process element generates $4$ pixels in parallel. Considering the bitwidth of OF buffer, these output pixels are fed into buffer in serial.

\subsection{Pooling}
The pooling operation in CNNs is either max pooling or average pooling for $2\times 2$ sliding windows. Another line buffer and shift register are utilized to generate $2\times 2$ sliding windows. Its work mode can be determined prior to compilation or configured on-the-fly.

\begin{comment}
\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.9\columnwidth]{fig/max_pooling.png}
	\caption{Block diagram of max-pooling}
	\label{fig:maxp}
\end{figure}
\end{comment}

\subsection{Batch normalization and activation}
During inference, the batch normalization is downgraded into a multiplication with an addition. We simply absorb it into the process element. Concerning to the activation function, our design supports both ReLU and LeakyReLU.
\begin{comment}
This fulfills most of the neural networks for segmentation tasks, including U-Net, FCN, and SegNet.
\end{comment}

\subsection{System controller}
The system controller determines: 1) the data flow such as when to trigger process element array, which data to be assigned into buffers, when to start data transfer to DDR, and 2) the setting for each component mentioned above, like the padding mode of line buffer module, work mode of process element arrays, and whether to bypass activation or pooling modules. It is implemented as a FSM. These settings are pre-defined and pre-loaded into register files so that FSM only has to read and execute sequentially.

\begin{table*}[!htb]
    \centering
    \caption{Resource comparison for CNN implementations on FPGA}
    \label{tab:comparison}
    \begin{adjustbox}{width=0.9\textwidth}
    \begin{tabular}{|c|c|c|c|c|c|c|} 
        \hline
        & \cite{ref:ref_13} & \cite{ref:ref_18} &  \cite{ref:shuanglong_2018} & \cite{ref:xinyu_2017} & Ours\\
        \hline
        %\multirow{3}{*}{FPGA}  & Zynq & Zynq & Zynq & Zynq & Zynq \\
        %     & ZC7Z045 & ZC7Z045 & ZC7Z045 & ZC7Z020 & ZC7Z045 \\
        %\hline
        FPGA & ZC7Z045 & ZC7Z045 & ZC7Z045 & ZC7Z020 & ZC7Z045 \\
        \hline
        Clock (MHz) & 150& 100 & 200 & 100 & 220 \\
        \hline
        Precision & 16-bit fixed & 16-bit fixed & 16-bit fixed & 12-bit fixed & 8-bit fixed\\
        \hline
        Network & VGG16-SVD  & VGG19  & U-Net  &  GAN & SegNet-basic \\
        \hline
        Operation & CONV & CONV & CONV+DECONV & DECONV & CONV+DECONV \\
        \hline
        Performance & \multirow{2}{*}{187} & \multirow{2}{*}{229} & 125 (CONV) & \multirow{2}{*}{2.6} & 151.5 (CONV) \\
        (GOPS) &  &  & 29 (DECONV)  &  & 94.3 (DECONV)\\
        \hline
        Resource Efficiency & \multirow{2}{*}{0.207(CONV)} &  \multirow{2}{*}{0.254} & 0.14 (CONV) & \multirow{2}{*}{0.012} & 0.168 (CONV) \\
        (GOPS/DSP)  & & & 0.033 (DECONV) & & 0.104 (DECONV)\\
        \hline
    \end{tabular}
    \end{adjustbox}
\end{table*}

\section{Implementation Considerations of SegNet-Basic}\label{sec:SegNet}
\begin{comment}
SegNet \cite{ref:segnet_2017} is one of the pixel level image segmentation neural networks, which is divided into encoder and decoder. SegNet-Basic shown in Fig.~\ref{fig:segnet} is the lite version of SegNet. It balances the computation complexity and performance, making it a better candidate for embedded platforms. We replace the up-sampling layer into deconvolution layer, which is proved having better performance.
\end{comment}
\begin{comment}
\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.9\columnwidth]{fig/segnet.png}
	\caption{Structure of SegNet-Basic}
	\label{fig:segnet}
\end{figure}
\end{comment}
The encoder part of SegNet-Basic includes $4$ convolution layers and  $3$ max pooling layers. The decoder part has $2$ convolution layers and $3$ deconvolution layers. The total parameter size for inference is about 42Mb, with 8-bit quantization for feature maps and weights. The accelerator is designed using Simulink and the HDL Coder toolbox. Our target platform Xilinx ZC706 contains 900 DSP slices and 19.2Mb Block RAMs.
\begin{comment}
\textcolor{red}{Benefiting from the OF buffer and doubled IF buffers, data transmission and processing can run in parallel.However, the accelerator has to wait until the weights and biases loading completes. } 
\end{comment}
\begin{comment}
Therefore, we spend 11 output 19.2Mb on-chip memory for weight to reduce the number of weight batches loaded from off-chip memory. Each IF buffer is designed to buffer $3\times 1$ sliding vectors from $90\times 120\times 8$ feature maps. Accordingly the OF buffer is capable to cache $90\times 120\times 8$ feature maps as well. In summary, the on-chip memory cost for IF and OF buffers is around 4.8Mb. Correspondingly, 576 DSPs are needed to process 8 channel feature maps simultaneously.
\end{comment}

\section{Results and Discussion}\label{sec:Results}
\begin{comment}
The overview of hardware architecture is shown in Fig.~\ref{fig:sys_arch}. This CNN accelerator is loaded as a peripheral of the ARM A9 processor. So that ARM A9 processor could configure CNN accelerator via AXI4 lite bus. Through the dedicated High-Performance Ports (HPs) on Zynq, intermediate data is transmitted between DDR memory and the CNN accelerator by two Scatter-Gather Direct Memory Access (DMAs). Three buffers, weight buffer, IF buffer and OF buffer, are equipped on CNN accelerator. Among them, double buffering technique is deployed on the IF buffer, to boost the throughput.
\end{comment}

The test setup of SegNet-basic hardware accelerator is demonstrated in Fig.~\ref{fig:sys_arch}. In the Zynq platform, hardware accelerator is loaded as a peripheral of ARM A9 processor. Two Direct Memory Access (DMAs) move the data between accelerator and DDR memory. The input images and parameters are pre-loaded into memory and transferred to PL by DMAs. This CNN accelerator clock frequency is 220MHz. Its total resource consumption is summarized in Tab.~\ref{tab:resource}.
\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.55\columnwidth]{fig/sys_arch.png}
	\caption{The hardware architecture of CNN accelerator}
	\label{fig:sys_arch}
\end{figure}

\begin{comment}
Xilinx ZC706 development kit is chosen to be the target FPGA platform for testing. A Zynq XC7Z045 SoC, two 1GB DDR3 SDRAMs on both programmable logic (PL) and processing system (PS) sides respectively, are mounted on the board. 
Inside Zynq XC7Z045 SoC is consists of a dual-core ARM Cortex-A9 processor on PS side, and 437,200 registers, 218,600 Look-Up Tables (LUTs),  19.2Mb Block RAMs, 900 DSP Slices on PL side.
\end{comment}

\begin{comment}
: 8929(XXX\%) registers, 10831(XXX\%) LUTs, 432(XXX\%) BRAMs and 576(XXX\%) DSP Slices, when running at XXX MHz. The resource and performance comparison are 
\end{comment}

\begin{table}[htbp]
    \centering
    \caption{Resource consumption}
    \label{tab:resource}
    \begin{tabular}{|c|c|c|c|}
        \hline
        LUTs & Registers & BRAMs & DSPs \\
        \hline
        16579 (8\%) & 25390 (6\%) & 537 (99\%) & 576 (64\%) \\
        \hline
    \end{tabular}
\end{table}
Comparing to other implementations (in Tab.~\ref{tab:comparison}), our design achieves better performance in case of deconvolution. Due to the sharing architecture, a better balance on both performance and resource efficiency for convolution and deconvolution is obtained. However, in order to support both operations, the architecture is not deeply optimized for convolution specifically. Therefore, the convolution performance is not as high as that from deeply optimized implementation in \cite{ref:ref_18}.

\subsection{Scalability}
Scalability is represented by the number of process element arrays in the accelerator. It is balance of bandwidth and computation capability. In SegNet-Basic, the number of process element arrays is set to 1. This means the input and output data bitwidth is 64. If higher bandwidth is supported, higher performance is possible.

\subsection{Latency of operations}
In order to compare the latency, we perform convolution and max pooling on a $90\times 120$ feature map (resulting a $45\times 60$ feature map) followed by deconvolution. We find the time for convolution and deconvolution are the same. The padding time difference is about $0.6\mu s$ due to different sizes of input feature maps. Considering pooling and ReLU, another $1.2\mu s$ is needed. Double buffering eliminates the data transfer time difference. Therefore, deconvolution saves about 3.2\% processing time if comparing to convolution plus max-pooling and ReLU.


\begin{comment}
\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.95\columnwidth]{fig/latency.png}
	\caption{The latency comparison for convolution(top) and deconvolution(bottom)}
	\label{fig:latency}
\end{figure}
\end{comment}



\section{Conclusions}\label{sec:Conclusions}
In this paper, a scalable and configurable CNN accelerator architecture has been proposed by combining both convolution and deconvolution into single process element. The deconvolution operation is completed in one step and buffering of intermediate results is not needed. In addition, SegNet-Basic has been successfully implemented on Xilinx Zynq ZC706 FPGA that achieves the performance of 151.5 GOPS for convolution and 94.3 GOPS for deconvolution, which outperforms state-of-the-art segmentation CNN implementations.

\section*{Acknowledgment}
This work was supported by the Mathworks Inc.

\begin{thebibliography}{9}
\footnotesize
\bibitem{ref:vgg_2014}
K. Simonyan and A. Zisserman,
''Very deep convolutional networks for large-scale image recognition,''
{\em arXiv preprint arXiv:1409.1556}, 2014.

\bibitem{ref:fasterrcnn_2015}
S. Ren, K. He, R. Girshick, and J. Sun,
''Faster R-CNN: Towards real-time object detection with region proposal networks,''
{\em In Advances in neural information processing systems (NIPS)}, pp. 91-99. 2015.

\bibitem{ref:fcn_2015}
J. Long, E. Shelhamer, and T. Darrell,
''Fully convolutional networks for semantic segmentation,''
{\em In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition (CVPR)}, pp. 3431-3440. 2015.

\bibitem{ref:eyeriss_2016}
Y.H. Chen, J. Emer, and V. Sze,
''Eyeriss: A spatial architecture for energy-efficient dataflow for convolutional neural networks,''
{\em In Proceedings of the 43rd International Symposium on Computer Architecture (ISCA)}, pp. 367-379, 2016.

\bibitem{ref:super_res_2017}
C. Ledig, et al,
''Photo-realistic single image super-resolution using a generative adversarial network,''
{\em In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition (CVPR)}, pp. 4681-4690. 2017.

\bibitem{ref:denoise_2017}
K. Zhang, W. Zuo, Y. Chen, D. Meng, and L. Zhang,
''Beyond a gaussian denoiser: Residual learning of deep CNN for image denoising,''
{\em IEEE Transactions on Image Processing}, vol. 26, no. 7, pp. 3142-3155, 2017.

%\bibitem{ref:yolo2_2017}
%J. Redmon and A. Farhadi,
%''YOLO9000: better, faster, stronger,''
%{\em In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition (CVPR)}, pp. 7263-7271. 2017.

%\bibitem{ref:chi_fpga_2015}
%C. Zhang, P.. Li, G. Sun, Y. Guan, B. Xiao, and J. Cong,
%''Optimizing fpga-based accelerator design for deep convolutional neural networks,''
%{\em In Proceedings of the 2015 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays (FPGA)}, pp. 161-170, 2015.

\bibitem{ref:utku_2017}
U. Aydonat, S. O'Connell, D. Capalija, A.C. Ling, and G.R. Chiu,
''An opencl™ deep learning accelerator on arria 10,''
{\em In Proceedings of the 2017 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays}, pp. 55-64. 2017.

\bibitem{ref:chen_2016}
C. Zhang, Z. Fang, P. Zhou, P. Pan, J. Cong,
''Caffeine: towards uniformed representation and acceleration for deep convolutional neural networks,''
{\em In Proceedings of the 35th International Conference on Computer-Aided Design}, p. 12, 2016.

\bibitem{ref:xuechao_2017}
X. Wei, C.H. Yu, P. Zhang, Y. Chen, Y. Wang, H. Hu, Y. Liang, and J. Cong,
''Automated systolic array architecture synthesis for high throughput CNN inference on FPGAs,''
{\em In Proceedings of the 54th Annual Design Automation Conference}, p. 29, 2017.

\bibitem{ref:shuanglong_2018}
S. Liu, H. Fan, X. Niu, H. Ng, Y. Chu, and W. Luk,
''Optimizing CNN-based Segmentation with Deeply Customized Convolutional and Deconvolutional Architectures on FPGA,''
{\em ACM Transactions on Reconfigurable Technology and Systems (TRETS)}, vol. 11, no. 3, 2018.

\bibitem{ref:xinyu_2017}
X. Zhang, S. Das, O. Neopane, K. Kreutz-Delgado,
''A Design Methodology for Efficient Implementation of Deconvolutional Neural Networks on an FPGA,''
{\em arXiv preprint arXiv:1705.02583}, 2017.

\bibitem{ref:fcnelement_2018}
D. Xu, K. Tu, Y. Wang, C. Liu, B. He, and H. Li,
''FCN-element: accelerating deconvolutional layers in classic CNN processors,''
{\em In Proceedings of the International Conference on Computer-Aided Design}, pp. 22, 2018.

\bibitem{ref:segnet_2017}
V. Badrinarayanan, A. Kendall, and R. Cipolla,
''Segnet: A deep convolutional encoder-decoder architecture for image segmentation,''
{\em IEEE transactions on Pattern Analysis and Machine Intelligence}, vol. 39, no. 12, pp. 2481-2495, 2017.

\bibitem{ref:u-net_2015}
O. Ronneberger, P. Fischer, and T. Brox,
''U-net: Convolutional networks for biomedical image segmentation,''
{\em In International Conference on Medical Image Computing and Computer Assisted Intervention}, pp. 234-241, 2015.

\bibitem{ref:optmloop_2017}
Y. Ma, Y. Cao, S. Vrudhula, and J. Seo,
''Optimizing loop operation and dataflow in FPGA acceleration of deep convolutional neural networks,''
{\em In Proceedings of the 2017 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays}, pp. 45-54, 2017.

\bibitem{ref:yecheng_tcas_2019}
Y. Lyu, L. Bai, and X. Huang.,
''Chipnet: Real-time LiDAR processing for drivable region segmentation on an FPGA,''
{\em IEEE Transactions on Circuits and Systems I: Regular Papers}, vol. 66, no. 5, pp. 1769 - 1779, 2019.
\bibitem{ref:google_quant}
B. Jacob, S. Kligys, B. Chen, M. Zhu, M.Tang, A. Howard, H. Adam, and D. Kalenichenko,
''Quantization and training of neural networks for efficient integer-arithmetic-only inference,''
{\em In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, pp. 2704-2713, 2018.

\bibitem{ref:ref_13}
J. Qiu, J. Wang, S. Yao et al,
''Going deeper with embedded fpga platform for convolutional neural network,''
{\em In Proceedings of the 2016 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays(FPGA)}, pp. 26-35, 2016.

\bibitem{ref:ref_18}
Q. Xiao, Y. Liang et al,
''Exploring heterogeneous algorithms for accelerating deep convolutional neural networks on FPGAs,''
{\em In 2017 54th ACM/EDAC/IEEE Design Automation Conference (DAC)}, pp. 1-6, 2017.

%\bibitem{key}
%I. M. Author,
%``Some related article I wrote,''
%{\em Some Fine Journal}, vol. 17, pp. 1--100, 1987.
%
%\bibitem{baz}
%A. N. Expert,
%{\em A Book He Wrote,}
%His Publisher, 1989.
%
%\bibitem{unpub}
%M. Smith,
%``Title of paper optional here,''
%unpublished.
%
%\bibitem{inpress}
%K. Rose,
%``Title of paper with only first word capitalized,''	% bug fixed by M. Imai
%in press.
%
%\bibitem{trans}
%T. Murayama,
%``Title of paper published in translation journals,''	% bug fixed by M. Imai
%{\em Some English Journal}, vol. 17, pp. 1--100, 1995.	% bug fixed by M. Imai
%({\em Original Foreign Journal, vol. 1, pp. 100-200, 1993}.)	% ditto

\end{thebibliography}

% that's all folks
\end{document}
