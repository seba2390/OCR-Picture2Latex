\documentclass[11pt,lettersize]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,amssymb,amsthm,bm}
\usepackage{fullpage}
\usepackage{graphicx}
\usepackage{subcaption}

\begin{document}
\title{Interpreting Deep Learning: The Machine Learning Rorschach Test?}
\date{}
\author{Adam S. Charles\footnote{A.S.C. is with the Princeton Neuroscience Department at Princeton University, Princeton, NJ. 08544 (contact: adamsc@princeton.edu)}}
\maketitle

% \section{Introduction}

Theoretical understanding of deep learning is one of the most important tasks facing the statistics and machine learning communities. 
While multilayer, or deep, neural networks (DNNs) originated as models of biological networks in neuroscience~\cite{mcculloch1943logical,hopfield1982neural,hopfield1985neural,grossberg1988nonlinear} and psychology~\cite{levine1983neural,rumelhart1987parallel}, and as engineering methods~\cite{pham1970neural,ersu1984application}, they have become a centerpiece of the machine learning (ML) toolbox.
% Neural networks have moved on from their roots as models of biological networks in neuroscience~\cite{mcculloch1943logical,hopfield1982neural,hopfield1985neural,grossberg1988nonlinear} and in psychology~\cite{levine1983neural,rumelhart1987parallel}, and as engineering methods~\cite{pham1970neural,ersu1984application} to become a centerpiece of the machine learning toolbox. 
%Their success has spurred other fields with complex data analysis needs to adopt these methods. The ease of collecting large datasets and the open sharing of reliable, stable code for training neural networks has resulted in widespread adoption of these methods faster than the statistics and machine learning communities could fully analyze how and when these networks work or fail. For example, a full understanding of when and why so-called adversarial examples~\cite{szegedy2013intriguing,nguyen2015deep,moosavi2016deepfool,brown2017adversarial} exist is still missing, potentially leaving many neural network-based systems open to malicious activity. This gap in understanding has recently drawn attention of a number of researchers from a diverse set of backgrounds, who have begun to take a formal, theoretical approach to understanding artificial neural networks, with an emphasis on the most prevalent of such methods: deep neural networks (DNNs). 
In ML, DNNs are simultaneously one of the simplest and most complex methods. They consist of many interconnected nodes that are grouped into layers (see Figure~1a), whose operations are stunningly simple; the $n^{th}$ node of the network at a given layer $i$, $x_i(n)$ is simply a nonlinear function $f(\cdot)$ (e.g. saturating nonlinearity) applied to an affine function of the previous layer 
\begin{gather}
    x_i(n) = f\left( \bm{w}_i(n)\bm{x}_{i-1} +  b_i(n) \right), \nonumber
\end{gather}
where $\bm{x}_{i-1}\in\mathbb{R}^{N_i}$ is the network node values at the previous layer, $\bm{w}_i(n)\in\mathbb{R}^{N_i}$ is the linear weight matrix that projects the previous layer to the $n^{th}$ node of the current matrix and $b_i(n)$ is the offset for node $n$. Even with such simple functions connecting the nodes between layers, the sheer number of nodes creates an explosion in the number of parameters ($\bm{w}_i(n)$ and $b_i(n)$ for all $i$ and $n$) and amplifies the effects of the nonlinearities. To add to the complexity, the parameters of the network (i.e. the weights and offsets across layers) are learned with respect to a cost function relating the inputs and outputs by gradient descent methods, i.e. various flavors of back-propagation~\cite{rumelhart1986learning}. Despite the resulting complexity, researchers have utilized DNNs to great effect in many important applications. 

The relatively recent success of DNNs in ML, despite their long history, can be attributed to a ``perfect storm'' of large labeled datasets~\cite{deng2012mnist,deng2009imagenet}; improved hardware~\cite{jouppi2017datacenter}; clever parameter constraints~\cite{krizhevsky2012imagenet}; advancements in optimization algorithms~\cite{kingma2014adam,sutskever2013importance,johnson2013accelerating}; and more open sharing of stable, reliable code~\cite{abadi2016tensorflow} leveraging the latest in methods such as automatic differentiation~\cite{rall1981automatic}. Original tasks in which DNNs first provided state-of-the-art results centered around image classification~\cite{krizhevsky2012imagenet,lecun1998gradient}, which powers devices such as ATMs. While DNNs have spread well beyond to many other applications (e.g. audio classification~\cite{hinton2012deep}, probability distribution approximation~\cite{kingma2013auto,makhzani2015adversarial} etc.),
the well publicized success in image classification has encouraged continued work that has provided other amazing technologies such as real-time text translation~\cite{good2015blog}. 

Unfortunately, DNN adoption powered by these successes combined with the open-source nature of the machine learning community, has outpaced our theoretical understanding. We cannot reliably identify when and why DNNs will make mistakes. In some applications like text translation these mistakes may be comical and provide for fun fodder in research talks, a single error can be very costly in tasks like medical imaging~\cite{finlayson2018adversarial}. 
Additionally, DNNs shown susceptibility to so-called adversarial examples, or data specifically designed to fool a DNN~\cite{szegedy2013intriguing,nguyen2015deep,moosavi2016deepfool,brown2017adversarial}. One can generate such examples with imperceptible deviations from an image, causing the system to mis-classify an image that is nearly identical to a correctly classified one. Audio adversarial examples can also exert control over popular systems such as Amazon Alexa or Siri, allowing malicious access to devices containing personal information~\cite{carlini2018audio,zhang2017dolphinattack}. As we utilize DNNs in increasingly sensitive applications, a better understanding of their properties is thus imperative. 

Early theory of DNNs or multi-layered networks (the smooth-nonlinearity versions of the non-smooth multi-layered perceptrons~\cite{minsky1990perceptrons}) were thought of more generally as learning machines and early theory sought to use statistical learning theory~\cite{valiant1984theory,vapnik1998statistical,cucker2002mathematical} or function approximation theory~\cite{hornik1991approximation} to analyze quantities such as the Vapnik-Chervonenkis (VC) dimension of DNNs~\cite{vapnik1994measuring}. While these theories address generally the complexity of neural networks with respect to training data, many important questions pertaining to the expressibility, learning rule efficiency, intuition, susceptibility to adversarial examples etc.\ remain. 

Recently, a number of theories spanning subsets of these questions have been proposed. These analyses mostly fall into three main styles of analysis. First are the methods that aim to show how DNNs perform explicit mathematical tasks by demonstrating how specific combinations of nonlinearities and weights recover exactly a known (and typically general) function on the data. The second method tries instead to describe the theoretical limitations and capabilities of the sequence of functions present in any DNN, again typically with constraints or assumptions about the nonlinearities and weights. These analyses can also be functions of the data, for example analyses that try to quantify and understand the cost-function landscape, which depends intimately on the data used for training (e.g.~\cite{ballard2017energy,mhaskar2018analysis}). The third area of DNN theory that is worthy of note is the literature analyzing the abilities of specific algorithms to efficiently solve the high-dimensional, nonlinear optimization programs required to train DNNs (e.g.~\cite{wilson2017marginal,zhang2018theory}). These analyses focus on the interplay between the training algorithm and the properties of DNNs as a mathematical structure. 

Advances in analyzing DNNs have included many different sources of intuition, drawn on both observations about the connections between the local and global computations DNNs perform to operations from other fields as well as applications of various analysis methods to understand how these operations interact. For example, the iterative linear-then-threshold structure has been related to the steps needed to find sparse representations of images~\cite{borgerding2017amp,xin2016maximal,papyan2016convolutional}. This result draws connections to temporally un-rolled iterative algorithms that explicitly solve penalized optimization programs such as basis pursuit de-noising (BPDN, aka LASSO). Specifically, solving the regularized least-squares optimization
\begin{gather}
\arg\min_{\bm{\beta}} \left[ \left\|\bm{y} - \bm{A}\bm{\beta} \right\| + \lambda R(\bm{\beta})\right],
\end{gather}
via a proximal projection method amounts to iteratively calculating
\begin{gather}
\widehat{\bm{\beta}}_{t+1} = P_{\lambda}\left(\widehat{\bm{\beta}}_{t} + \bm{A}^T\left(\bm{y} - \bm{A}\widehat{\bm{\beta}}_{t}) \right)\right),
\end{gather}
where $P_{\lambda}(\bm{z})$ is the nonlinearity that calculates the proximal projection $\min_{\beta} \|\bm{z} -\bm{\beta}\|_2^2 + \lambda R(\bm{\beta})$. In the case where the regularization function $R(\cdot)$ is separable, the proximal projection is a point-wise nonlinearity, mimicking the form of DNNs. Treating $\widehat{\bm{\beta}}_{t}$ at each algorithmic iteration as a different set of variables, these variables can be considered the node values at different layers of a deep neural network with weights $\bm{A}^T\bm{A} + \bm{I}$ between layers, a bias $\bm{A}^T\bm{y}$ at each layer the nonlinearity defined by the proximal projection. This example gives a flavor of how the network weights and nonlinearity can be mapped to a specific operation to understand the functionality of DNNs. A more global computational interpretation looks at the computation of all layers as a whole, drawing a connection to tensor decompositions and classification based on tensor inner products~\cite{cohen2016convolutional,cohen2017analysis}. A non-exhaustive list of additional interpretations and analysis thus far includes:
\begin{itemize}
    \item Complexity analysis such as Chaitin-Kolmogorov complexity~\cite{pearlmutter1991chaitin}, Vapnik-Chervonenkis (VC) dimension calculations~\cite{bartlett1993lower,bartlett1993vapnik,vapnik1994measuring,bartlett1996vc,bartlett1999almost,bartlett2003vapnik}, sample-complexity analysis~\cite{bartlett1998sample}, Lipshitz-based generalization bounds~\cite{bartlett2017spectrally}
    \item analysis of the ability of deep networks as function approximators~\cite{hornik1991approximation,shaham2016provable,mhaskar2016deep,eldan2016power,long2018representing}
    \item inspirations from physics (including low-dimensional structure~\cite{lin2017does}, renormalization~\cite{mehta2014exact}, tools from high-dimensional statistical mechanics~\cite{poole2016exponential}, quantum entanglement~\cite{levine2017deep,levine2018bridging} and connections to the information bottleneck~\cite{tishby2015deep})
    \item chemistry (in interpreting the energy landscape~\cite{ballard2017energy})
    \item connections to wavelet transforms and invariance~\cite{mallat2016understanding,bruna2013invariant,wiatowski2016discrete}
    \item connections to message passing algorithms that marginalize nuisance variables~\cite{patel2016probabilistic}
    \item generalization analysis via cost-function maxima~\cite{mhaskar2018analysis}
    \item high-dimensional probability theory for analyzing the network Jacobaian~\cite{pennington2017resurrecting,pennington2018emergence} or layer-wise Gramm matrix~\cite{pennington2017nonlinear}
    \item equivalence to Gaussian processes in certain limiting cases~\cite{lee2017deep}
    \item equivalence of DNNs to hierarchical tensor decompositions~\cite{cohen2016convolutional,cohen2017analysis,stock2018learning}
    \item relations to better understood single-layer networks~\cite{veit2016residual,philipp2017gradients} 
    \item analysis of the invertability and information retention through CNN layers~\cite{arora2015deep,bahmani2017anchored}
    \item complexity analysis on the learnability of neural networks~\cite{song2017complexity} 
    \item algebraic topology approaches to understanding the complexity of data and choosing DNN architectures~\cite{guss2018characterizing}
    \item probing DNN functionality using tools from psychology~\cite{ritter2017cognitive}
    \item empirical analysis via influence functions~\cite{koh2017understanding}
    \item analysis of specific counter-examples~\cite{safran2017depth,safran2017spurious}
    \item DNN compression-based generalization analysis~\cite{arora2018stronger}
    \item interpretations as hierarchical kernel machines~\cite{anselmi2015deep}
    \item information theory~\cite{yu2018understanding}
    \item analysis of the role of layered representations on DNN properties via the learning dynamics~\cite{haeffele2015global,sagun2017empirical,shalev2017failures,safran2016quality,saxe2013exact}
    \item speed and accuracy guarantees (or lack thereof) of learning methods for DNNs~\cite{arora2014provable,pascanu2014saddle,dauphin2014identifying,arora2016provable,wilson2017marginal,arora2018optimization,kidambi2018insufficiency}  
\end{itemize}

This list, which still omits specialized analyses of specific optimization tricks such as dropout~\cite{gal2015dropout} and newer architectures such as generative adversarial networks (GANS)~\cite{arora2017generalization,arora2017gans} or deep recurrent networks~\cite{levine2017benefits}, demonstrates just how relentless the search for meaning in DNNs has become. In the breadth of possible interpretations, some interesting points begin to emerge. For one, there seems to be a limitless number of interpretations for DNNs, apparently constrained only by the lens by which the mathematical operations are viewed. Physics interpretations stem from researchers with a physics background. Connections to sparsity and wavelets come from researchers well known for important contributions to those fields. 
Ultimately, the interpretation of DNNs appears to mimic a type of Rorschach test --- a psychological test wherein subjects interpret a series of seemingly ambiguous ink-blots~\cite{rorschach1922psychodiagnostik} (see Figure~1). Rorschach tests depend not only on \emph{what} (the result) a subject sees in the ink-blots but also on the \emph{reasoning} (methods used) behind the subject's perception, thus making the analogy particularly apropos. 

\begin{figure*}[h]
\centering
    \begin{subfigure}[t]{0.5\textwidth}
        \centering
        \includegraphics[width=0.9\textwidth]{DNN.pdf}
    \end{subfigure}%
    ~ 
    \begin{subfigure}[t]{0.5\textwidth}
        \centering
        \includegraphics[width=0.9\textwidth]{Normalized_Rorschach_blot_01.jpg}
    \end{subfigure}
        \caption{What do you see? DNNs can be viewed in many ways. 1a. Stylistic example of a DNN with an input layer (red), output layer (blue) and two hidden layers (green); example ``ink blot'' for DNN theory. 1b. Example (normalized) ink blot from the Rorschach test.}
\end{figure*}

On the one hand, is unsurprising given DNNs status as arbitrary function approximators. Specific network weights and nonlinearities allow DNNs to easily adapt to various narratives. On the other hand, they are not unique in their permitting multiple interpretations. One can likewise view standard, simpler, algorithms through various lenses. For example one can derive the Kalman filter --- a time-tested algorithm for tracking a vector over time --- from at least three interpretations: : the orthogonality principle~\cite{haykin2008adaptive}, Bayesian maximum \emph{a-priori} estimation~\cite{barker1995bayesian,charles2015dynamics}, and low-rank updates for least-squares optimization~\cite{moon2000mathematical}. These three derivations allow people with different mathematical mindsets (i.e., linear algebra versus probability theory) to understand the algorithm.

Yet compared to DNNs, the Kalman filter is simple, consisting of only a handful of linear-algebraic operations. It's function is completely understood, allowing each viewpoint to be validated despite the different underlying philosophies.  
Similar validation for DNN theory requires a convergence of the literature. We must distinguish between universal results that are invariant to the analysis perspective and those that are specific to a particular network configuration. A healthy debate is already underway, with respect to the information bottleneck interpretation of DNNs~\cite{tishby2015deep,saxe2018iclr}. We must also better understand how the functions DNNs perform, their mathematical properties, and the impact of the optimization methods interact. The complexity of DNNs, however, introduces many challenges. For one, many standard tools (e.g.\ for understanding how models generalize from training data~\cite{zhang2016understanding} or empirically assessing important network features~\cite{ghorbani2017interpretation}) are difficult to apply to DNNs.
Luckily, there is no shortage of excitement, and we continue to enhance our understanding of DNNs with time.  
The community is also beginning to coalesce, and dedicated meetings like recent workshops at the Conference on Neural Information Processing Systems (NIPS) and the recent Mathematical Theory of Deep Neural Network symposium at Princeton University, will further accelerate our pace. 

Additionally, it is worth mentioning that a similar branching of neural network analysis --- separating out what functions they can solve and the generic properties they have --- has occurred for recurrent neural networks (RNNs)~\cite{hopfield1982neural} as well. Separate literatures have evolved to analyze the functions (typically as an optimization functions) RNNs can solve~\cite{rozell2008sparse,hu2012network,charles2012common} and what generic properties such networks can have (e.g.\ the low-dimensional dynamics~\cite{sussillo2013opening,rivkind2017local} eigenvalue structure~\cite{rajan2006eigenvalue}, expressiveness~\cite{khrulkov2017expressive} or short-term memory of RNNs~\cite{jaeger2001short,buonomano2009state,charles2014short,maass2002real,charles2017distributed}). These questions are especially pertinent given the current rise in the use of RNNs in fields such as neuroscience (e.g.~\cite{zhu2013visual,sussillo2015neural,depasquale2018full,rajan2016recurrent}). Like the DNN literature, a bridging between these two distinct ways of analyzing the same mathematical object has not yet taken place and will similarly be key to substantial progress in our understanding of these systems. 

%Bridging the gap between the two main approaches to analyzing DNNs (and likewise for RNNs) will possibly be key to overcoming some of the big challenges in finding commonality between all the varying viewpoints, and the neural network literature will have much to gain in making this connection.

\vspace{10pt}

{\bf Acknowledgements:} I would like to thank Justin Romberg, Jonathan Pillow, Michael Shvartsman, Mikio Aoi, Ahmed El Hady Camille Rullan, David Zoltowski, and the editorial staff of \emph{SIAM News} (Lina Sorg and Karthika Swamy Cohen), all of who have kindly provided feedback on this manuscript and helped me refine its ideas and message.


\bibliographystyle{unsrt}
\bibliography{rt.bib}

\end{document}
