The basic structure of this section is similar to Section~\ref{sec:angular}.
However, we now consider radial movement only. We define the relevant set $\dt:=\dt(\kappa)$ depending on a parameter $\kappa\geq 1$ independent of $n$, then we compute $\mu(\dt)$ and afterwards 
separately establish the stated upper and lower bounds. Since the radial movement contains a drift towards the boundary that makes the calculations more involved, we first need to prove basic results on such diffusion processes before actually defining $\dt$.
Let us thus start with the definition of the radial movement of a given particle, initially located at $x_0=(r_0, \theta_0)$. Recall that
a particle which at time~$\ss$ is in position
$x_s=(r_s,\theta_s)$ will stay at an angle $\theta_s=\theta_0$ while its
radial distance from the origin $r_s$ evolves according to a diffusion
process with a reflecting barrier at $R$ and generator 
\[
\Delta_{rad} := \frac{1}{2}\frac{\partial^2}{\partial r^2}+\frac{\alpha}{2}\frac{1}{\tanh(\alpha r)}\frac{\partial}{\partial r}.
\]
We are only concerned with the evolution of the process up until the detection time of the target, which occurs when the particle reaches $B_Q(R)$, and since the particles can only move radially, for any $x_0\not\in B_Q(R)$ we can also impose an absorbing barrier for $r_{s}$ at the radius $\rabs_0$ corresponding to the point in $\partial B_Q(R)$ with angle $\theta_0$. Recall that by Part~\eqref{itm:phiInv} of Lemma~\ref{lem:phi}, the function 
$\phi:[0,R]\to [\phi(R),\frac{\pi}{2}]$ has an inverse $\phi^{-1}$ 
which is also decreasing and continuous, so the absorbing barrier is given by $\rabs_0=\phi^{-1}(|\theta_0|)$.
This means that for values of $|\theta_0|>\frac{\pi}{2}$ we choose as absorbing barrier 
the origin $O$, that is,  $\rabs_0=0$.
Also recall that, whatever the value of $\theta_0$, we 
have that $(\theta_0,\rabs_0)$ always belongs to the boundary of $B_Q(R)$.
Since near the origin~$O$ the drift towards the boundary grows to infinity, for a point 
$x_0$ such that $|\theta_0|>\frac{\pi}{2}$ we have $\P_{x_0}(T_{det}\leq t)=0$ (in other words, at these angles the only way to detect $Q$ would be by reaching the origin, but since the drift there is $-\infty$ this is impossible). For the case where the absorbing barrier is distinct from the origin  we use the following result, which we state as a standalone result since it will be of use in later sections as well (by abuse of notation, since $\Delta_{rad}$ defined above depends only on the radial (one-dimensional) movement we use in the following lemma the operator $\Delta_{rad}$ also to denote the specific operator acting over sufficiently smooth one variable functions over the reals):%\dmc{the macros are good. The names I would change a bit, in this way it looks like $\auxY$ is a random variable, whereas $\yabs_0$ $\auxy$ is not. We'll see. Perhaps $\auxy=r$, $\yabs_0=\rho$, $\auxY=\rho'$?} 
\begin{lemma}\label{lemmaradial}
	Let $\{\auxy_s\}_{s\geq 0}$ be a diffusion process on $(0,\auxY]$ with generator $\Delta_{rad}$, and with a reflecting barrier at $\auxY$, and let $\P_{\auxy}$ denote the law of $\auxy_s$ with initial position $\auxy$. Define also $T_{\yabs_0}$, $T_\auxY$ the hitting times of~$\yabs_0$ and $\auxY$ respectively. We have:
	\begin{enumerate}[(i)]
	\item\label{radial:itm:phi1} For any $\lambda>0$ and any $\auxy\in[\yabs_0,\auxY]$,
	\begin{equation*}%\label{cotachern}
	\EE_{\auxy}(e^{-\lambda T_{\yabs_0}})\,\leq\,\frac{\lambda_1 e^{-\lambda_2 (\auxY-\auxy)}+\lambda_2 e^{\lambda_1(\auxY-\auxy)}}{{\lambda_1 e^{-\lambda_2 (\auxY-\yabs_0)}+\lambda_2 e^{\lambda_1(\auxY-\yabs_0)}}}\end{equation*}
	where $\lambda_1=\sqrt{\frac{\alpha^2}{4}+2\lambda}+\frac{\alpha}{2}$ and $\lambda_2=\sqrt{\frac{\alpha^2}{4}+2\lambda}-\frac{\alpha}{2}$.
	\item\label{radial:itm:phi2} If $\auxy\in[\yabs_0,\auxY]$, then
	\[\EE_{\auxy}(T_{\yabs_0})\,\leq\,\EE_{\auxY}(T_{\yabs_0})\,\leq\,\frac{e^{\alpha \auxY}}{\alpha^2}\log(\cotanh(\tfrac12\alpha \yabs_0)).\]
	In particular, if $\yabs_0>\tfrac{1}{\alpha}\log 2$, then 
	$\displaystyle
	\EE_{\auxy}(T_{\yabs_0})\,\leq\,\frac{4}{\alpha^2} e^{\alpha(\auxY-\yabs_0)}$.
	\item\label{radial:itm:phi3} If $\auxy\in[\yabs_0,\auxY]$, then
	\[G_{\yabs_0}(\auxy):=\PP_{\auxy}(T_{\yabs_0}<T_\auxY) = \frac{\log\big(\tfrac{\tanh(\alpha \auxY/2)}{\tanh(\alpha \auxy/2)}\big)}{\log\big(\tfrac{\tanh(\alpha \auxY/2)}{\tanh(\alpha \yabs_0/2)}\big)}.
	\]
	\item\label{radial:itm:phi4} If  $\auxy\in[\yabs_0,\auxY)$, then $\displaystyle
	\EE_{\auxy}(T_{\yabs_0}\,\big|\,T_{\yabs_0}<T_\auxY)\leq \frac{2}{\alpha}(\auxy-\yabs_0)+\frac{2}{\alpha^2}(1-G_{\yabs_0}(\auxy))$.
	\end{enumerate}
\end{lemma}
\begin{proof}
We begin with the proof of~\eqref{radial:itm:phi1} by observing that $\tanh(x)\leq 1$ for all positive values of~$x$ and hence we can couple the trajectory of a particle~$P$ with that of an auxiliary particle $\widetilde{P}$ starting with the same initial position as $P$, but whose radius $\widetilde{\auxy}_\ss$ evolves according to the diffusion with generator
\[
\widetilde{\Delta}_{rad}(f) := \frac{1}{2}f''+\frac{\alpha}{2}f',
\]
in such a way that $\widetilde{\auxy}_\ss\leq y_\ss$ for all $\ss$. It follows that the detection time $\widetilde{T}_{\yabs_0}$ of this auxiliary particle is smaller than the one of $P$ so in particular $\EE_{\auxy}(e^{-\lambda T_{\yabs_0}})\leq\EE_{\auxy}(e^{-\lambda \widetilde{T}_{\yabs_0}})$, and it suffices to prove the inequality for the auxiliary process. Let now $g$ be the solution of the following O.D.E.,
\begin{equation}\label{ODE1}
\frac{1}{2}g''(\auxy)+\frac{\alpha}{2}g'(\auxy)-\lambda g(\auxy)=0
\end{equation}
on $[\yabs_0,\auxY]$ with boundary conditions $g(\yabs_0)=1$ and $g'(\auxY)=0$, which is equal to
\[g(\auxy) = \frac{\lambda_1 e^{-\lambda_2 (\auxY-\auxy)}+\lambda_2 e^{\lambda_1(\auxY-\auxy)}}{{\lambda_1 e^{-\lambda_2 (\auxY-\yabs_0)}+\lambda_2 e^{\lambda_1(\auxy-\yabs_0)}}}\]
where $\lambda_1$ and $\lambda_2$ are as in the statement of the lemma. It follows from It么's lemma that $\{e^{-\lambda s}g(\widetilde{\auxy}_s)\}_{s \ge 0}$ is a bounded martingale, and hence we can apply Doob's optional stopping theorem to deduce $g(\auxy)=\EE_{\auxy}(e^{-\lambda\cdot 0}g(\widetilde{\auxy}_0))=\EE_{\auxy}(e^{-\lambda\widetilde{T}_{\yabs_0}})$, giving the result. To obtain the bound in~\eqref{radial:itm:phi2}, we go back to the original process $\{\auxy_s\}_{s \ge 0}$ which evolves according to $\Delta_{rad}$, and define $F(\auxy)$ as the solution of the O.D.E. on $[\yabs_0,\auxY]$
\begin{equation}\label{eq(ii)}
-1 = \frac{1}{2}F''(\auxy)\,+\,\frac{\alpha}{2}\cotanh(\alpha \auxy)F'(\auxy)
\end{equation}
with boundary conditions $F'(\auxY)=0$ and $F(\yabs_0)=0$. We advance that the solution is smooth and bounded and deduce from It么's lemma that $\{F(\auxy_s)+s\}_{s\ge 0}$ is a martingale, so applying Doob's optional stopping theorem we deduce $F(\auxy)=\EE_{\auxy}(F(\auxy_0)+0)=\EE_{\auxy}(F(\auxy_{t\wedge T_{\yabs_0}})+t\wedge T_{\yabs_0})$ for every $t>0$. Choosing any $c>0$, a simple argument obtained by restarting the process at $\auxY$ every $c$ units of time gives
\[\PP_\auxy(T_{\yabs_0}>t)\leq \PP_\auxY(T_{\yabs_0}>t)\leq(\PP_{\auxY}(T_{\yabs_0}>c))^{\lfloor\frac{t}{c}\rfloor},\]
and hence $\lim_{t\to\infty}t\PP_\auxy(T_{\yabs_0}>t)=0$. We deduce then that $\lim_{t\to\infty}\EE_\auxy(t\wedge T_{\yabs_0})=\EE_\auxy(T_{\yabs_0})$ and since $F$ is bounded, $\lim_{t\to\infty}\EE_{\auxy}(F(\auxy_{t\wedge T_{\yabs_0}})= \EE_{\auxy}(F(\auxy_{T_{\yabs_0}}))=0$. Thus, $F(\auxy)=\EE_\auxy(T_{\yabs_0})$, and it remains to solve the O.D.E. To do so, we  multiply~\eqref{eq(ii)} by $2\sinh(\alpha \auxy)$ to obtain
\[-2\sinh(\alpha \auxy) = \sinh(\alpha \auxy)F''(\auxy)+\alpha\cosh(\alpha \auxy)F'(\auxy) = (\sinh(\alpha \auxy)F'(\auxy))'.\]
Thus, integrating from $\auxy$ to $\auxY$ and using that $F'(\auxY)=0$ we have
\[\frac{2}{\alpha}(\cosh(\alpha \auxY)-\cosh(\alpha \auxy)) = \sinh(\alpha \auxy)F'(\auxy),\]
which in particular proves directly that $F(\auxy)$ is an increasing function, so that $\EE_{\auxy}(T_{\yabs_0})\leq \EE_\auxY(T_{\yabs_0})$. Integrating from $\yabs_0$ to $\auxY$, together with the condition $F'(\auxY)=0$ gives
%\cmk{The denominator inside the second $\log$ below should be $\tanh(\frac12\alpha\rabs_0)$, right?}\dmc{I get it in the numerator}
\[
\EE_\auxY(T_{\yabs_0}) = F(\auxY) = \frac{2}{\alpha^2}\Big(\log\Big(\frac{\sinh(\alpha \yabs_0)}{\sinh(\alpha \auxY)}\Big)-\cosh(\alpha \auxY)\log\Big(\frac{\tanh(\tfrac12\alpha\yabs_0)}{\tanh(\tfrac12\alpha \auxY)}\Big)\Big)
\]
and hence the general bound appearing in~\eqref{radial:itm:phi2} follows by noticing that the first term is negative, and by bounding $\cosh(\alpha \auxY)$ by $\frac{1}{2}e^{\alpha \auxY}$. To obtain $\EE_{\auxy}(T_{\yabs_0})\leq \frac{4}{\alpha^2} e^{\alpha(\auxY-\yabs_0)}$ observe that if we assume $\yabs_0>\tfrac{1}{\alpha}\log 2$ then $\cotanh(\tfrac12\alpha\yabs_0)\leq 1+4e^{-\alpha\yabs_0}$ so the result follows from bounding $\log(1+4e^{-\alpha\yabs_0})\leq4e^{-\alpha\yabs_0}$. 

To establish~\eqref{radial:itm:phi3} and~\eqref{radial:itm:phi4} it will be enough to work with a diffusion evolving on $(0,\infty)$ according to the original generator $\Delta_{rad}$, but without any barriers. Abusing notation we still call the process $\{\auxy_s\}_{s \ge 0}$. To deduce~\eqref{radial:itm:phi3}, observe that the solution of the O.D.E.
\[
0 = \frac{1}{2}G_{\rabs_0}''(\auxy) + \frac{\alpha}{2}\cotanh(\alpha \auxy)G_{\yabs_0}'(\auxy)
\]
with conditions $G_{\yabs_0}(\yabs_0)=1$ and $G_{\yabs_0}(\auxY)=0$ is given by the closed expression given in~\eqref{radial:itm:phi3}. It follows from It么's lemma that $\{G_{\yabs_0}(\auxy_s)\}_{s \ge 0}$ is a bounded martingale, so applying Doob's optional stopping theorem we deduce $G_{\yabs_0}(y)=\EE_\auxy(G_{\yabs_0}(\auxy_0))=\EE_\auxy(G_{\yabs_0}(\auxy_{T_{\yabs_0}\wedge T_\auxY}))=\PP_{\auxy}(T_{\yabs_0}<T_\auxY)$.

Finally, to prove~\eqref{radial:itm:phi4} define the function $H(\auxy)$ as the solution of the ordinary differential equation
\[-G_{\yabs_0}(\auxy) = \frac{1}{2}H''(\auxy)\,+\,\frac{\alpha}{2}\cotanh(\alpha \auxy)H'(\auxy)\]
with boundary conditions $H(\yabs_0)=H(\auxY)=0$. It can be checked directly that the last equation is satisfied by
{\footnotesize
\begin{equation}\label{eq(iv)}
H(\auxy)=\frac{2}{\alpha}G_{\yabs_0}(r)\int_{\yabs_0}^{\auxy}\!\sinh(\alpha l)G_{\yabs_0}(l)\log\Big(\frac{\tanh(\frac{\alpha l}{2})}{\tanh(\frac{\alpha \yabs_0}{2})}\Big)dl+\frac{2}{\alpha}(1{-}G_{\yabs_0}(\auxy))\int_{\auxy}^\auxY\!\sinh(\alpha l)G_{\yabs_0}(l)\log\Big(\frac{\tanh(\frac{\alpha \auxY}{2})}{\tanh(\frac{\alpha l}{2})}\Big)dl,
\end{equation}}
which is smooth. It follows once again from It么's lemma that $\{H(\auxy_s)+\int_0^s G_{\yabs_0}(\auxy_u)du\}_{s \ge 0}$ is a martingale. Since in the proof of~\eqref{radial:itm:phi3} we already showed that $\{G_{\yabs_0}(\auxy_s)\}_{s \ge 0}$ is a martingale, it follows that $\{\int_0^s G_{\yabs_0}(\auxy_u)du-s G_{\yabs_0}(\auxy_s)\}_{s \ge 0}$ is also a martingale. We conclude that $\{H(\auxy_s)+s G_{\yabs_0}(\auxy_s)\}_{s \ge 0}$ is a martingale, so applying Doob's optional stopping theorem we deduce
\[H(\auxy)=\EE_{\auxy}(H(\auxy_0)+0\cdot G_{\yabs_0}(y_0))=\EE_{\auxy}(H(\auxy_{t\wedge T_{\yabs_0}\wedge T_{\auxY}})+(t\wedge T_{\yabs_0}\wedge T_{\auxY})\cdot G_{\yabs_0}(\auxy_{t\wedge T_{\yabs_0}\wedge T_{\auxY}}))\]
for every $t>0$. Reasoning as in the proof of~\eqref{radial:itm:phi2} we can take the limit as $t\to\infty$ to obtain
\[H(\auxy)=\EE_{\auxy}(H(\auxy_{T_{\yabs_0}\wedge T_{\auxY}})+(T_{\yabs_0}\wedge T_{\auxY})\cdot G_{\yabs_0}(\auxy_{T_{\yabs_0}\wedge T_{\auxY}}))=\EE_{\auxy}(T_{\yabs_0}{\bf1}_{\{T_{\yabs_0}<T_{\auxY}\}})\]
where we used that $H(\yabs_0)=H(\auxY)=G_{\yabs_0}(\auxY)=0$ and $G_{\yabs_0}(\yabs_0)=1$. Observing that $\EE_{\auxy}(T_{\yabs_0}\,|\,T_{\yabs_0}<T_\auxY)=\frac{H(\auxy)}{G_{\yabs_0}(\auxy)}$, to obtain the inequality in~\eqref{radial:itm:phi4} we only need to bound $H(\auxy)$. To do so notice that for any $\yabs_0\leq l$ gives $\frac{\tanh(\alpha l/2)}{\tanh(\alpha \yabs_0/2)}\leq \frac{1}{\tanh(\alpha l/2)}$, which together with $0\leq G_{\yabs_0}(l)\leq 1$ allows us to bound from above the first integral of~\eqref{eq(iv)} by $\sinh(\alpha l)\log(\cotanh(\frac12\alpha l))=\frac{2\cotanh(\frac12\alpha l)}{\cotanh^2(\frac12\alpha l)-1}\log (\cotanh(\frac12\alpha l))=f(\cotanh(\frac12\alpha l))$, where $f(z)=\frac{2z}{z^2-1}\log z$ is bounded from above by $1$ on $[1,\infty)$. Using the same argument we can bound the term within the second integral of~\eqref{eq(iv)} by $G_{\yabs_0}(l)$, so that
\[\frac{H(\auxy)}{G_{\yabs_0}(\auxy)}\leq\frac{2}{\alpha}(\auxy-\yabs_0)+\frac{2}{\alpha}(1-G_{\yabs_0}(\auxy))\int_{\auxy}^\auxY \frac{G_{\yabs_0}(l)}{G_{\yabs_0}(\auxy)}dl.\]
Using the fact that $\frac{G_{\yabs_0}(l)}{G_{\yabs_0}(\auxy)}=G_{\auxy}(l)$, the second integral becomes $\int_{\auxy}^\auxY G_{\auxy}(l)dl$ which we control by studying the function $\auxy\mapsto \int_{\auxy}^\auxY G_{\auxy}(l)dl$ on $(0,\auxY)$. Notice first that any critical point $\auxy'$ of said function satisfies
\[\int_{\auxy'}^\auxY G_{\auxy'}(l)dl=\frac{1}{\alpha}\log\Big(\frac{\tanh(\frac12\alpha \auxY)}{\tanh(\frac12\alpha \auxy')}\Big)\sinh(\alpha \auxy')\leq\frac{1}{\alpha},\]
so it will be sufficient to control the integral when either $\auxy=\auxY$ or $\auxy=0$. For the first case we have $\int_{\auxY}^\auxY G_{\auxY}(l)dl=0$, and for the second one, by definition we have $\lim_{\auxy\to 0}G_{\auxy}(l)=0$ for any $l>0$. 
Since $G_\auxy(l)$ is monotone increasing in $\auxy$, by the monotone convergence theorem, $\lim_{\auxy\to 0}\int_\auxy^\auxY G_{\auxy}(l)dl=\int \lim_{\auxy \to 0} G_\auxy(l) dl=0$, so putting all these cases together we conclude $\int_\auxy^\auxY G_{\auxy}(l)dl\leq \frac{1}{\alpha}$.
\end{proof}

\medskip
Before we define $\dt(\kappa)$, let
\[
\phi^{(\ss)}:=\Big(\frac{\ss^{\frac{1}{\alpha}}}{e^{R}}\Big)^{\frac{1}{2}}.%+\phi(R).
\] 
Intuitively, one may think of $\phi^{(\ss)}$ as those points that are so close in terms of angle to the target, so that - even though possibly initially at the boundary of $B_O(R)$ - they have a reasonable chance to detect the target by time $\ss$ through the radial movement. %\dmc{If we put the definition of $\phi^{(\ss)}$ already in the angular section, and I think we should indeed as Marcos suggested, then perhaps also we should say the analogous thing there}\cmk{Yes. We should write $\phi^{(\ss)}=\ss^{\frac{1}{2\alpha}}/\ell_R$}\cmk{I looked at this again. Not sure we gain anything here by changing $e^{R/2}$ to $\ell_R=\sinh(R)$, in fact, it makes some calculations a bit harder to follow.}\dmc{Ok, let's switch it back then if you think}
We define~$\dt(\kappa)$ as the collection of points where a particle initially located can detect the target before time $\ss$ with a not too small probability (depending on $\kappa$). From our discussion preceding Lemma~\ref{lemmaradial}, we will always assume that any point $x_0\in\dt$ satisfies $|\theta_0|\leq\frac{\pi}{2}$. Since $\PP_{r}(T_{\rabs_0}<T_R)=G_{\rabs_0}(r)$, with $G_{\rabs_0}(r)$ as defined in Lemma~\ref{lemmaradial} with $y:=r$, $\yabs_0:=\rabs_0$ and $Y:=R$, it follows that $G_{\rabs_0}(r)$ is decreasing as a function of $r$, continuous and takes values in $[0,1]$. In particular, for $\kappa>1$ 
%\sout{The set $\dt$ will only contain points $x_0=(r_0,\theta_0)$
%such that $|\theta_0|\leq\frac{\pi}{2}$ (in particular $\dt$ 
%contains the origin $O$). 
%Unless explicitly said otherwise, for the ensuing discussion assume $x_0$ is such that %$|\theta_0|\leq\frac{\pi}{2}$. 
%Recall that by Part~\eqref{itm:phiInv} of Lemma~\ref{lem:phi}, the function 
%$\phi(\cdot):[0,R]\to [\phi(R),\frac{\pi}{2}]$ has an inverse $\phi^{-1}$ 
%which is also decreasing and continuous.
%Let $f$ be such that at $x_0=(r_0,\theta_0)\in\ndt$ with $\phi(R)< |\theta_0|<\frac{\pi}{2}$ %defined as} 
%\[
%\sout{f(r_0,\theta_0) = \frac{g(r_0)}{g(\phi^{-1}(|\theta_0|))}, \quad \text{ where } \quad
%g(r) =\log\Big(\frac{\tanh(\alpha R/2)}{\tanh(\alpha r/2)}\Big).}
%\]
%\sout{Clearly, when $\phi(r_0)=|\theta_0|$, we get that $f(r_0,\theta_0)=1$.
%%Thus, $f$ evaluates to $1$ at every point on the boundary of $B_{Q}(R)$.
%Also, when $r_0=R$, we have $f(r_0,\theta_0)=0$.
%Moreover, the mapping $r\mapsto g(r)/g(\phi^{-1}(|\theta_0|))$  
%from $[\phi^{-1}(|\theta_0|),R]$ to $[0,1]$ is continuous and decreasing.
%Henceforth, let $\rabs_0=\phi^{-1}(|\theta_0|)$ and, for the sake of convenience, let $\rabs_0=0$ %if $|\theta_0|\geq\frac{\pi}{2}$ (note that in this case $(\theta_0,\rabs_0)$ belongs to the 
%boundary of $B_Q(R)$ whatever the value taken by $\theta_0$).
%Henceforth, we always assume $\kappa>1$.
%From our ongoing discussion we conclude that for every $\ss\geq 0$} 
there is a unique $\widetilde{\rabs}_0\in [\rabs_0,R]$ such that
%\dmc{if we have $\kappa >1$ then we could directly into the formula (without defining $\delta$) put the right hand side of the equation below, and perhaps also put directly there $\Theta(\kappa^{-2\alpha})$}\cml{but this is not necessarily true if $\ss$ is small}
%\dmc{We'll finally have only values of $\ss$ at least being a large constant, in which case $\delta$ is a constant depending on $\kappa$. Maybe add a sentence something like "Note that by our lower bound on $\ss \ge C$ in Theorem~\ref{thm:radialmain}, we always have $\delta=\Theta(1)$, with the constant depending on $\kappa$". It would be good for the link with the main statement}
\begin{equation}\label{eqn:radial-defrtilde}
\PP_{\widetilde{\rabs}_0}(T_{\rabs_0}<T_R)=
\delta(\kappa,\ss), 
\quad\text{ where }\quad
\delta=\delta(\kappa,\ss):= \frac{1+\ss}{(1+\kappa\ss^{\frac{1}{2\alpha}})^{2\alpha}}.
\end{equation}
Note that $0\leq\delta(\kappa,\ss)< 1$ (the latter inequality holds because we assume $\kappa\geq 1$). Also observe that $\delta(\kappa,0)=1$ and $\delta(\kappa,\ss)$ tends to $\kappa^{-2\alpha}$ as $\ss$ tends to infinity.
Furthermore, $\delta(\kappa,\ss)=\Theta(\kappa^{-2\alpha})$ if $\ss=\Omega(1)$.
Now, define (see Figure~\ref{fig:radial}) 
\[
\dt = \dt(\kappa) := \big\{ x_0\in B_O(R) : |\theta_0|\leq\tfrac{\pi}{2} \wedge \big[|\theta_0|\leq\phi(R)+\kappa\phi^{(\ss)} \vee  r_0\leq\widetilde{\rabs}_0\big]\big\}.
\]
which contains $B_Q(R)$ since every $x_0\in B_Q(R)$ satisfies $r_0<\rabs_0\leq\widetilde{\rabs}_0$.
To better understand the motivation for defining $\widetilde{\rabs}_0$ and $\delta$ as above, we consider the most interesting regime, i.e.,~$\ss=\Omega(1)$. Under this condition the effect of the drift has enough time to move the particle far away from its initial position, and towards the boundary, so that the event $\{T_{\rabs_0}<\ss,\,T_{\rabs_0}<T_R\}$ is mostly explained by the particles' initial trajectory. In particular, by Part~\eqref{radial:itm:phi3} of Lemma~\ref{lemmaradial}, we have that $\PP_{r_0}(T_{\rabs_0}<\ss,\,T_{\rabs_0}<T_R)\approx \PP_{r_0}(T_{\rabs_0}<T_R)=G_{\rabs_0}(r_0)$,  so the condition $r_0\leq\widetilde{\rabs_0}$ aims to include in $\dt$ all points whose probability of detecting the target before reaching the boundary of $B_O(R)$ is not too small. To exhaust all possibilities, we must also include in $\dt$ all points which have a sufficiently large probability of detecting the target even after reaching the boundary of $B_O(R)$. Said points are considered through the condition $|\theta_0|\leq\phi(R)+\kappa\phi^{(\ss)}$, which gives a lower bound of order $\delta(\kappa,\ss)$ for the detection probability, thus explaining our choice of said function.
%\added[id=mk]{To better understand the motivation for defining $\widetilde{\rabs}_0$ as above, we consider the most interesting regime, i.e., $\ss=\Omega(1)$, for which as already observed, $\delta(\kappa,\ss)$ in terms of order only depends on $\kappa$ and controls the probability that a particle initially placed at $\widetilde{\rabs}_0$ detects $Q$ without ever having visited the boundary of $B_O(R)$. Giving the drift that the particle is subject to, the probability of detecting without visiting the boundary, i.e.,~$\PP_{\widetilde{\rabs}_0}(T_{\rabs_0}<T_R)$, is mostly explained by the initial trajectory of the particle, thence mostly independent of $\ss$ for all but very small values of $\ss$ \dmc{should we rather say: it is mostly independent of $\ss$, as long as $\ss = \Omega(1)$ ? See the new remark also below}. Moreover, by Part~\eqref{radial:itm:phi3} of Lemma~\ref{lemmaradial}, we know that $\PP_{\widetilde{\rabs}_0}(T_{\rabs_0}<T_R)=G_{\rabs_0}(\widetilde{\rabs}_0)$ and can thus estimate $\widetilde{\rabs}_0$ as a function of $\delta$.\dmc{here I would prefer also to say just as a function of $\kappa$, but I won't insist}}

\medskip
Before moving to the main theorem of this section, we spend some time building some intuition about the geometric shape of $\dt$. 
Since $\rabs_0$ goes to $0$ when $\theta_0$ tends to $\frac{\pi}{2}$, from~\eqref{eqn:radial-defrtilde} which is used to define $\widetilde{\rabs}_0$, it is not hard to see that 
$\widetilde{\rabs}_0$ also goes to $0$
(and hence $\widetilde{\rabs}_0-\rabs_0$ as well) when $\theta_0$ tends to $\frac{\pi}{2}$.
It requires a fair amount of additional work to show that $\widetilde{\rabs}_0-\rabs_0$, as a function of $\theta_0>\phi(R)+\kappa\phi^{(\ss)}$, first increases very slowly, then reaches a maximum value of roughly $\frac{1}{\alpha}\log\frac{1}{\delta}$
and finally decreases rapidly (we omit the details since we will not rely on this observation). In fact, for all practical purposes, one might think of $\widetilde{\rabs}_0-\rabs_0$ as being essentially constant up to the point when $\rabs_0$ is smaller than a constant (equivalently, $\theta_0$ is at least a constant).

%\cmk{Changed statement to make presentation consistent with strategy of proof.}\dmc{Ok for me}
The main goal of this section is to prove the following result from which Theorem~\ref{thm:radialmain} immediately follows (by the proof strategy discussed in Section~\ref{sec:strategy}) once we show that~$\mu(\dt(\kappa))$ is of the right order:
\begin{theorem}\label{thm:rad} 
The following hold:
\begin{enumerate}[(i)]
\item If $\kappa\geq 1$ and $\phi(R)+\kappa\phi^{(\ss)}\leq \frac{\pi}{2}$, then $\sup_{x_0 \in \ndt}\PP_{x_0}(T_{det}\leq\ss)=O(\delta(\kappa,\ss))$ and
\[
\int_{\ndt\!(\kappa)}\P(T_{det}\leq\ss)d\mu(x_0) = O(\mu(\dt(\kappa))). 
\]
\item\label{thm:rad:itm2} For every $c>0$ there is a $\kappa_0 >0$ such that if $\kappa\ge\kappa_0$ and $\ss= \frac{16}{\alpha}\log \kappa+\Omega(1)$ satisfy $\phi(R)+\kappa\phi^{(\ss)}\leq \frac{\pi}{2}-c$, then $\inf_{x_0 \in \dt}\PP_{x_0}(T_{det}\leq\ss)=\Omega(\delta(\kappa,\ss))$.
\end{enumerate}
\end{theorem}


\begin{comment}
\begin{remark}
The upper bound on $\ss$ implicit in the hypothesis $\phi(R)+\phi^{(\ss)}\leq\frac{\pi}{2}$ stems from the fact that for $\ss > \frac{\pi}{2}e^{\alpha R}$ 
the first probability of the above theorem is $e^{-\Omega(n)}$. The same conclusion follows from the fact that having zero points in a Poisson point process of intensity $n$ occurs with probability $e^{-\Theta(n)}$, hence for such values of $\ss$ the upper bound becomes trivial.
\end{remark}
\end{comment} 

\begin{remark}
The extra hypotheses on $\kappa$ and $\ss$ needed for the lower bounds of Part~\eqref{thm:rad:itm2} of Theorem~\ref{thm:rad} %\sout{$\PP_{x_0}(T_{det}\leq\ss)$ and $\dt$}\cmk{It is unclear what we mean by "and $\dt$". Needs to be clarified}\dmc{Just leave out?} 
are key for our methods to work, %since for small times $\ss$ the probability of a particle travelling too far is exponentially \sout{low} \dm{small}. 
since for small times $\ss= o(1)$ the detection probability for a point always tends to $0$ unless it is already in $B_Q(R)$ or very close to it (but the latter set is of smaller measure than $B_Q(R)$). Similarly, if one is very close to the origin (that is, for angles very close to $\frac{\pi}{2}$) the explosion of the drift towards the boundary at the origin also implies an extra penalization for the probability of detection. Observe nevertheless that the extra hypothesis in Part~\eqref{thm:rad:itm2} are automatically satisfied if $\ss=\omega(1)\cap o(e^{\alpha R})$.
Furthermore, the hypothesis $\ss =\Omega(1)$ is natural since in the case of radial movement only we will show that $\EE(T_{det})=\Theta(1)$, and we are interested in tail behaviors of the detection time.
\end{remark}

%\dmc{So the next remark is now removed I guess by adding your previous remark Amitai, I guess}\cml{maybe, although mine focuses on the hypotheses on $\kappa$, maybe we should mention the consequences on $\ss$?}

%\dm{
%\begin{remark}
%The hypothesis of $\ss =\Omega(1)$ is natural \deleted[id=mk]{(to us)} for the following reasons: on the one hand, since in the case of radial movement only we have $\EE(T_{det})=\Theta(1)$, and we are interested in tail behaviors of the detection time. On the other hand, if $\ss=o(1)$, with probability tending to $1$ the radius does not change by more than a $o(1)$, and so detection cannot be significantly higher than in the static case where $\dt=B_Q(R)$ (of course if one is very close to the origin, there is a stronger drift towards the boundary, but in this case either the particle is already in $B_Q(R)$ or it is rather pushed in the opposite direction, and it is again as the static case).
%\end{remark}}



Among all facts regarding the intuition of $\dt$, we will only need to prove rigorously (see the next result) that if $\phi(R)+\kappa\phi^{(\ss)}\leq |\theta_0|\leq\frac{\pi}{2}$, then $\widetilde{\rabs}_0-\rabs_0\leq\frac{1}{\alpha}\log\frac{1}{\delta}+O(1)$.
\begin{fact}\label{fct:radial-varphi2}
For $\ss> 0$ and $\kappa\geq 1$, 
if $\theta_0\in [\phi(R)+\kappa\phi^{(\ss)},\frac{\pi}{2}]$, then
  $e^{\widetilde{\rabs}_0-\rabs_0}=O(1/\delta^{\frac{1}{\alpha}})$
  where $\delta:=\delta(\kappa,\ss)$.
\end{fact}
\begin{proof}
We first handle some simple cases.
If $\widetilde{\rabs}_0\leq\frac{1}{\alpha}\log\frac{2}{\delta}$, then $e^{\widetilde{r}_0-\rabs_0}\leq e^{\widetilde{r}_0}\leq (2/\delta)^{\frac{1}{\alpha}}$ and we are done.
Similarly, if $\rabs_0\geq R-\frac{1}{\alpha}\log\frac{3}{\delta}$, since $\widetilde{\rabs}_0\leq R$, we have that $e^{\widetilde{r}_0-\rabs_0}\leq e^{R-\rabs_0}\leq (3/\delta)^{\frac{1}{\alpha}}$ and we obtain again the claimed bound. Henceforth, assume that $\widetilde{\rabs}_0>\frac{1}{\alpha}\log\frac{2}{\delta}$ and that $\rabs_0< R-\frac{1}{\alpha}\log\frac{3}{\delta}$.

Let $g(r):=\log(\tanh(\frac12\alpha R)/\tanh(\frac12\alpha r))$ and note that since $\tanh(x)\leq 1$, $\cotanh(x)=1+\frac{2e^{-2x}}{1-e^{-2x}}$ and $1+y\leq e^{y}$, 
\[g(\widetilde{\rabs}_0) \leq \log(\cotanh(\tfrac12\alpha\widetilde{\rabs}_0))
\leq\frac{2e^{-\alpha\widetilde{\rabs}_0}}{1-e^{-\alpha\widetilde{\rabs}_0}}\leq4e^{-\alpha\widetilde{\rabs}_0},\]
where in the last inequality we have used that $\delta\leq 1$. Moreover, since $\tanh(x)=1-\frac{2e^{-2x}}{1+e^{-2x}}$, 
$\cotanh(x)=1+\frac{2e^{-2x}}{1-e^{-2x}}$ and
using twice that $\log(1+y)\geq \frac{y}{1+y}$ for $y>-1$, we have
\[
g(\rabs_0) = \log(\coth(\tfrac12\alpha\rabs_0))+\log(\tanh(\tfrac12\alpha R)) \geq \frac{2e^{-\alpha\rabs_0}}{1+e^{-\alpha\rabs_0}} - 
\frac{2e^{-\alpha R}}{1-e^{-\alpha R}}
\geq e^{-\alpha\rabs_0}-(2+o(1))e^{-\alpha R}.
\]
Since $\delta\leq 1$, by our assumption on $\rabs_0$, we conclude that
  $g(\rabs_0)=\Omega(e^{-\alpha\rabs_0})$. Observing that $g(\widetilde{\rabs}_0)/g(\rabs_0)=G_{\rabs_0}(\widetilde{\rabs}_0)=\delta$, it follows from the 
previously derived bounds on $g(\widetilde{\rabs}_0)$ and $g(\rabs_0)$ 
that $e^{\alpha(\widetilde{\rabs}_0-\rabs_0)}=O(\delta^{-1})$ 
and thus $e^{\widetilde{\rabs}_0-\rabs_0}=O(1/\delta^{\frac{1}{\alpha}})$ as desired.
\end{proof}
For practical purposes one may view $\dt$ as the collection of points of $B_O(R)$
which belong either to a sector of angle $2(\phi(R)+\kappa\phi^{(\ss)})$ 
whose bisector contains $Q$, 
or to the ball $B_Q(R)$, or to those points with angular coordinate $|\theta_0|\geq \phi(R)+\kappa\phi^{(\ss)}$ which are 
within radial distance roughly $\frac{1}{\alpha}\log\frac{1}{\delta}$ of $B_Q(R)$. 
This picture would be accurate except for the fact that it places into $\dt$ points with angular coordinate close to $\frac{\pi}{2}$ and fairly close to the origin $O$, say at distance 
$\frac{1}{\alpha}\log\frac{1}{\delta}-\Omega(1)$. 
However, particles initially located at such points are extremely unlikely to reach $B_Q(R)$ and detect $Q$, since the drift towards the boundary tends to infinity close to the origin.
This partly justifies why $\dt$ is defined so that in the case where $\theta_0$ goes to $\frac{\pi}{2}$ the expression $\widetilde{\rabs}_0-\rabs_0$ tends to $0$, thus 
leaving out of $\dt$ the previously mentioned problematic points.


\begin{figure}
\begin{center}
\begin{tikzpicture}[x=1cm,y=1cm,scale=0.65,
     decoration={markings,
       mark=at position 1 with {\arrow[scale=1.5,black]{latex}};
      }]
      
      \def\c{12}
      \def\barr{4}
      \def\radp{9.5}
      \def\phip{240}
      \def\angabs{-87.94}
      \node[inner sep=0] (O) at (\c,\c) {};
      \node[inner sep=0] (P) at (2,4) {};
      \node[inner sep=0] (Q) at (\c,0) {};
  \draw[fill=gray!20] (11.561cm,0.008cm) --(11.686cm,3.423cm) -- (11.656cm,3.656cm) -- (11.624cm,3.891cm) -- (11.589cm,4.126cm) -- (11.552cm,4.362cm) -- (11.511cm,4.599cm) -- (11.468cm,4.837cm) -- (11.421cm,5.076cm) -- (11.371cm,5.316cm) -- (11.318cm,5.558cm) -- (11.261cm,5.800cm) -- (11.201cm,6.045cm) -- (11.137cm,6.291cm) -- (11.070cm,6.540cm) -- (11.000cm,6.792cm) -- (10.928cm,7.047cm) -- (10.854cm,7.305cm) -- (10.778cm,7.568cm) -- (10.702cm,7.835cm) -- (10.627cm,8.108cm) -- (10.555cm,8.387cm) -- (10.487cm,8.672cm) -- (10.426cm,8.963cm) -- (10.376cm,9.261cm) -- (10.344cm,9.513cm) -- (10.339cm,9.563cm) -- (10.335cm,9.614cm) -- (10.331cm,9.665cm) -- (10.328cm,9.717cm) -- (10.325cm,9.768cm) -- (10.322cm,9.819cm) -- (10.321cm,9.870cm) -- (10.320cm,9.922cm) -- (10.319cm,9.973cm) -- (10.319cm,10.025cm) -- (10.320cm,10.076cm) -- (10.322cm,10.128cm) -- (10.324cm,10.179cm) -- (10.327cm,10.231cm) -- (10.331cm,10.282cm) -- (10.335cm,10.333cm) -- (10.341cm,10.384cm) -- (10.347cm,10.435cm) -- (10.354cm,10.486cm) -- (10.362cm,10.537cm) -- (10.371cm,10.587cm) -- (10.381cm,10.638cm) -- (10.392cm,10.688cm) -- (10.403cm,10.737cm) -- (10.416cm,10.787cm) -- (10.430cm,10.836cm) -- (10.444cm,10.884cm) -- (10.460cm,10.932cm) -- (10.477cm,10.980cm) -- (10.494cm,11.027cm) -- (10.513cm,11.074cm) -- (10.533cm,11.120cm) -- (10.554cm,11.166cm) -- (10.577cm,11.211cm) -- (10.600cm,11.255cm) -- (10.625cm,11.299cm) -- (10.651cm,11.341cm) -- (10.678cm,11.383cm) -- (10.706cm,11.424cm) -- (10.736cm,11.465cm) -- (10.767cm,11.504cm) -- (10.799cm,11.542cm) -- (10.832cm,11.579cm) -- (10.867cm,11.615cm) -- (10.904cm,11.650cm) -- (10.941cm,11.684cm) -- (10.981cm,11.717cm) -- (11.022cm,11.748cm) -- (11.065cm,11.778cm) -- (11.110cm,11.806cm) -- (11.156cm,11.834cm) -- (11.205cm,11.859cm) -- (11.257cm,11.883cm) -- (11.311cm,11.905cm) -- (11.369cm,11.926cm) -- (11.431cm,11.944cm) -- (11.499cm,11.961cm) -- (11.574cm,11.975cm) -- (11.660cm,11.987cm) -- (11.769cm,11.995cm) -- (12.000cm,12.000cm) --(12.231cm,11.995cm) -- (12.340cm,11.987cm) -- (12.426cm,11.975cm) -- (12.501cm,11.961cm) -- (12.569cm,11.944cm) -- (12.631cm,11.926cm) -- (12.689cm,11.905cm) -- (12.743cm,11.883cm) -- (12.795cm,11.859cm) -- (12.844cm,11.834cm) -- (12.890cm,11.806cm) -- (12.935cm,11.778cm) -- (12.978cm,11.748cm) -- (13.019cm,11.717cm) -- (13.059cm,11.684cm) -- (13.096cm,11.650cm) -- (13.133cm,11.615cm) -- (13.168cm,11.579cm) -- (13.201cm,11.542cm) -- (13.233cm,11.504cm) -- (13.264cm,11.465cm) -- (13.294cm,11.424cm) -- (13.322cm,11.383cm) -- (13.349cm,11.341cm) -- (13.375cm,11.299cm) -- (13.400cm,11.255cm) -- (13.423cm,11.211cm) -- (13.446cm,11.166cm) -- (13.467cm,11.120cm) -- (13.487cm,11.074cm) -- (13.506cm,11.027cm) -- (13.523cm,10.980cm) -- (13.540cm,10.932cm) -- (13.556cm,10.884cm) -- (13.570cm,10.836cm) -- (13.584cm,10.787cm) -- (13.597cm,10.737cm) -- (13.608cm,10.688cm) -- (13.619cm,10.638cm) -- (13.629cm,10.587cm) -- (13.638cm,10.537cm) -- (13.646cm,10.486cm) -- (13.653cm,10.435cm) -- (13.659cm,10.384cm) -- (13.665cm,10.333cm) -- (13.669cm,10.282cm) -- (13.673cm,10.231cm) -- (13.676cm,10.179cm) -- (13.678cm,10.128cm) -- (13.680cm,10.076cm) -- (13.681cm,10.025cm) -- (13.681cm,9.973cm) -- (13.680cm,9.922cm) -- (13.679cm,9.870cm) -- (13.678cm,9.819cm) -- (13.675cm,9.768cm) -- (13.672cm,9.717cm) -- (13.669cm,9.665cm) -- (13.665cm,9.614cm) -- (13.661cm,9.563cm) -- (13.656cm,9.513cm) -- (13.650cm,9.462cm) -- (13.644cm,9.411cm) -- (13.638cm,9.361cm) -- (13.631cm,9.311cm) -- (13.624cm,9.261cm) -- (13.617cm,9.211cm) -- (13.609cm,9.161cm) -- (13.600cm,9.111cm) -- (13.592cm,9.062cm) -- (13.583cm,9.012cm) -- (13.574cm,8.963cm) -- (13.564cm,8.914cm) -- (13.555cm,8.865cm) -- (13.545cm,8.817cm) -- (13.534cm,8.768cm) -- (13.524cm,8.720cm) -- (13.513cm,8.672cm) -- (13.502cm,8.624cm) -- (13.491cm,8.576cm) -- (13.480cm,8.529cm) -- (13.469cm,8.481cm) -- (13.457cm,8.434cm) -- (13.445cm,8.387cm) -- (13.434cm,8.340cm) -- (13.422cm,8.293cm) -- (13.410cm,8.247cm) -- (13.398cm,8.200cm) -- (13.385cm,8.154cm) -- (13.373cm,8.108cm) -- (13.361cm,8.062cm) -- (13.348cm,8.017cm) -- (13.336cm,7.971cm) -- (13.323cm,7.926cm) -- (13.311cm,7.880cm) -- (13.298cm,7.835cm) -- (13.285cm,7.790cm) -- (13.273cm,7.746cm) -- (13.260cm,7.701cm) -- (13.247cm,7.657cm) -- (13.235cm,7.612cm) -- (13.222cm,7.568cm) -- (13.209cm,7.524cm) -- (13.197cm,7.480cm) -- (13.184cm,7.436cm) -- (13.171cm,7.392cm) -- (13.159cm,7.349cm) -- (13.146cm,7.305cm) -- (13.134cm,7.262cm) -- (13.121cm,7.219cm) -- (13.109cm,7.175cm) -- (13.096cm,7.132cm) -- (13.084cm,7.090cm) -- (13.072cm,7.047cm) -- (13.060cm,7.004cm) -- (13.047cm,6.961cm) -- (13.035cm,6.919cm) -- (13.023cm,6.876cm) -- (13.011cm,6.834cm) -- (13.000cm,6.792cm) -- (12.988cm,6.750cm) -- (12.976cm,6.708cm) -- (12.964cm,6.666cm) -- (12.953cm,6.624cm) -- (12.941cm,6.582cm) -- (12.930cm,6.540cm) -- (12.918cm,6.499cm) -- (12.907cm,6.457cm) -- (12.896cm,6.416cm) -- (12.885cm,6.374cm) -- (12.874cm,6.333cm) -- (12.863cm,6.291cm) -- (12.852cm,6.250cm) -- (12.841cm,6.209cm) -- (12.831cm,6.168cm) -- (12.820cm,6.127cm) -- (12.810cm,6.086cm) -- (12.799cm,6.045cm) -- (12.789cm,6.004cm) -- (12.779cm,5.963cm) -- (12.769cm,5.922cm) -- (12.759cm,5.882cm) -- (12.739cm,5.800cm) -- (12.682cm,5.558cm) -- (12.629cm,5.316cm) -- (12.579cm,5.076cm) -- (12.532cm,4.837cm) -- (12.489cm,4.599cm) -- (12.448cm,4.362cm) -- (12.411cm,4.126cm) -- (12.376cm,3.891cm) -- (12.344cm,3.656cm) -- (12.314cm,3.423cm) -- (12.439cm,0.008cm) --(12.352cm,0.005cm) --(12.264cm,0.003cm) --(12.176cm,0.001cm) --(12.088cm,0.000cm) --(12.000cm,0.000cm) --(11.912cm,0.000cm) --(11.824cm,0.001cm) --(11.736cm,0.003cm) --(11.648cm,0.005cm) --(11.561cm,0.008cm) --(12.439cm,0.008cm) --cycle;

\draw[fill=gray!40] (11.941cm,0.000cm) -- (11.902cm,1.200cm) -- (11.842cm,2.401cm) -- (11.748cm,3.604cm) -- (11.607cm,4.811cm) -- (11.404cm,6.030cm) -- (11.136cm,7.278cm) -- (10.842cm,8.591cm) -- (10.798cm,8.820cm) -- (10.758cm,9.051cm) -- (10.725cm,9.285cm) -- (10.698cm,9.521cm) -- (10.681cm,9.760cm) -- (10.675cm,9.999cm) -- (10.681cm,10.239cm) -- (10.704cm,10.477cm) -- (10.744cm,10.711cm) -- (10.804cm,10.938cm) -- (10.885cm,11.154cm) -- (10.988cm,11.356cm) -- (11.113cm,11.538cm) -- (11.260cm,11.696cm) -- (11.426cm,11.825cm) -- (11.608cm,11.921cm) -- (11.801cm,11.980cm) -- (12.000cm,12.000cm) --(12.199cm,11.980cm) -- (12.392cm,11.921cm) -- (12.574cm,11.825cm) -- (12.740cm,11.696cm) -- (12.887cm,11.538cm) -- (13.012cm,11.356cm) -- (13.115cm,11.154cm) -- (13.196cm,10.938cm) -- (13.256cm,10.711cm) -- (13.296cm,10.477cm) -- (13.319cm,10.239cm) -- (13.325cm,9.999cm) -- (13.319cm,9.760cm) -- (13.302cm,9.521cm) -- (13.275cm,9.285cm) -- (13.242cm,9.051cm) -- (13.202cm,8.820cm) -- (13.158cm,8.591cm) -- (13.112cm,8.366cm) -- (13.063cm,8.144cm) -- (13.013cm,7.924cm) -- (12.963cm,7.707cm) -- (12.913cm,7.492cm) -- (12.864cm,7.278cm) -- (12.815cm,7.067cm) -- (12.768cm,6.857cm) -- (12.723cm,6.649cm) -- (12.679cm,6.441cm) -- (12.636cm,6.235cm) -- (12.596cm,6.030cm) -- (12.557cm,5.825cm) -- (12.521cm,5.621cm) -- (12.486cm,5.418cm) -- (12.453cm,5.215cm) -- (12.422cm,5.013cm) -- (12.393cm,4.811cm) -- (12.366cm,4.609cm) -- (12.340cm,4.408cm) -- (12.316cm,4.206cm) -- (12.293cm,4.005cm) -- (12.272cm,3.805cm) -- (12.252cm,3.604cm) -- (12.158cm,2.401cm) -- (12.098cm,1.200cm) -- (12.059cm,0.000cm) -- (12.059cm,0.000cm) -- (12.045cm,0.000cm) -- (12.030cm,0.000cm) -- (12.015cm,0.000cm) -- (12.000cm,0.000cm) -- (11.985cm,0.000cm) -- (11.970cm,0.000cm) -- (11.955cm,0.000cm) -- (11.941cm,0.000cm) -- cycle;
\draw[fill=black] (O) ++(\phip:\radp) circle (0.07) node[left] {$x_{\ss}$};
\draw[fill=black] (O) ++(\phip:8.0) circle (0.07) node[left] {$x_0$};
\draw[fill=black] (O) ++(\phip:2.62) circle (0.07) node[right] {$_{A_0}$};
\draw[dotted] (O) -- ++(\phip:2.62);
\draw[fill=black] (O) ++(\phip:3.22) circle (0.07) node[left] {$_{B_0}$};
\draw[dashed] (O)  ++(\phip:2.62) -- ++(\phip:9.38);

      \draw[dotted] (0,\c) -- (2*\c,\c);
      \draw[dotted] (O) -- (Q);
      \draw[dotted] (O) -- ++(\angabs:\c+2);
      \draw[dotted] (\c,-0.75) -- (\c,-2);
      \draw (\c,-1.5) arc (-90:\angabs:\c+1.5);
      \draw[postaction={decorate}] ({(\c+1.5)*cos(-90-5)+\c},{(\c+1.5)*sin(-90-5)+\c}) arc (-90-5:-90:\c+1.5);
      \draw[postaction={decorate}] ({(\c+1.5)*cos(-83.1)+\c},{(\c+1.5)*sin(-83.1)+\c}) arc (\angabs+5:\angabs:\c+1.5);
      \draw[postaction={decorate}] (\c,\c-\radp) arc (-90:\phip-360:\radp);
      \node at (\c+0.9,-1.1) {$_{\phi(R)+\kappa\phi^{(\ss)}}$};
      \node at (\c-2.5,\c-8.7) {$_{\theta_{\ss}=\theta_0}$};
      \draw[thick,black] (2*\c,\c) arc (0:-180:\c);
      \draw[fill=black] (Q) circle (0.07);
      \draw[fill=black] (O) circle (0.07);
      \node[below] at (Q) {$_Q$};      
      \node[above] at (O) {$_O$}; 
	\end{tikzpicture}
\end{center}
  \caption{Half of disk $B_O(R)$.  The position $x_{\ss}:=(r_{\ss},\theta_{\ss})$ of particle $P$ at time
    $\ss$ is shown. The lightly shaded area corresponds to
    $\dt(\kappa)\setminus B_{Q}(R)$ and the strongly shaded
    region represents~$B_Q(R)$. The dashed line corresponds to the radial
    segment where a particle $P$ at initial angle~$\theta_0$ is,
    conditional under not having detected the target~$Q$ at time
    $\ss$. The radial coordinates of $A_0$ and $B_0$ are $\rabs_0:=\rabs_0(\theta_0)$
    and $\widetilde{\rabs}_0:=\widetilde{\rabs}_0(\theta_0)$,
    respectively.}\label{fig:radial}
\end{figure}


\medskip
We next determine $\mu(\dt)$, which simply amounts to performing an integral. %\dmc{since in the main theorem we assume $\ss \ge C$ we could remove the $1$ here, no?}
\begin{lemma}\label{lem:radialmeasure}
If  $\kappa\ge 1$ are such that $\phi(R)+\kappa\phis\le\frac{\pi}{2}$, then  %\dmc{Ok with the change. As I said perhaps remove the +1 below}\cml{again, it depends on $\ss$}%\dmc{I am sorry, but this notation $O(\nu)$ is not good. You don't like $O(1)$, so $O_{\nu}(1)$?}\cmk{I re-wrote in the meantime (see below). There are two things I do not like of the $O_{\nu}(1)$ notation. All our hidden constants depend on $\alpha$ and $\beta$, so writing $O_{\nu}(1)$ is a bit odd to stress that the constant depends on $\nu$ while we don't do the same with $\alpha$ and $\beta$. The other thing is that the dependence on $\nu$ is very nice, the hidden constant depends linearly in $\nu$! The notation $O_{\nu}(1)$ could well be interpreted as if the hidden constant was $2^{2^\nu}$ and hides the fact that we can state what the dependency on one of our model parameters is. Anyway, what I left below is weird, because $n\phi(R)=\nu$ by definition. Another alternative is $\nu\cdot O(1+\kappa\ss^{\frac{1}{2\alpha}})$, but I recall you do not like it either. Finally, why is it ok to explicitly state the dependency on $\kappa$ and $\ss$, but not on $\nu$?}
\[
\mu(\dt(\kappa)) = \Theta\big(n(\phi(R)+\kappa\phi^{(\ss)})\big) = \Theta(\mkor{\nu(1+\kappa\ss^{\frac{1}{2\alpha}})}{1+\kappa\ss^{\frac{1}{2\alpha}}}).
\]
\end{lemma}
\begin{proof}
For the lower bound, simply observe that $\dt:=\dt(\kappa)$ contains a sector $\Upsilon_Q$ 
  of angle $2(\phi(R)+\kappa\phi^{(\ss)})$ on whose bisector lies the target $Q$,
  so  $\mu(\dt)\geq\mu(\Upsilon_Q)=\Theta(n(\phi(R)+\kappa\phi^{(\ss)}))$.
Now, we address the upper bound. 
It suffices to show that $\mu(\dt\setminus\Upsilon_Q)=O(\mu(\Upsilon_Q))$.
Indeed, observe that
\[
\mu(\dt\setminus\Upsilon_Q) 
= O(ne^{-\alpha R})\int_{\phi(R)+\kappa\phi^{(\ss)}}^{\frac{\pi}{2}}\int_0^{\widetilde{\rabs}_0}\frac{1}{2\pi}\sinh(\alpha r_0)dr_0d\theta_0
= O(ne^{-\alpha R})\int_{\phi(R)+\kappa\phi^{(\ss)}}^{\frac{\pi}{2}}e^{\alpha\widetilde{\rabs}_0}d\theta_0.
\]
Thus, since $e^{\alpha\widetilde{\rabs}_0}=e^{\alpha\rabs_0}e^{\alpha(\widetilde{\rabs}_0-\rabs_0)}$, by Fact~\ref{fct:radial-varphi2},
\[
\mu(\dt\setminus\Upsilon_Q) = 
  O(ne^{-\alpha R}\delta^{-1})
  \int_{\phi(R)+\kappa\phi^{(\ss)}}^{\frac{\pi}{2}}e^{\alpha\rabs_0}d\theta_0.
\]
Recall that by definition $|\theta_0|=\phi(\rabs_0)$, so that by Part~\eqref{itm:phi4} of Lemma~\ref{lem:phi}, we have  $|\theta_0|=\phi(\rabs_0)=\Theta(e^{-\frac12\rabs_0})$, 
and
\[
 \mu(\dt\setminus\Upsilon_Q) = O(ne^{-\alpha R}\delta^{-1})\int_{\phi(R)+\kappa\phi^{(\ss)}}^{\frac{\pi}{2}}\theta_0^{-2\alpha}d\theta_0
= O(ne^{-\alpha R}\delta^{-1}(\phi(R)+\kappa\phi^{(\ss)})^{-(2\alpha-1)}).
%= O(n\kappa\phi^{(\ss)}\ss^{-1}).
\]
By our choices of $\phi^{(\ss)}$ and $\delta$ 
  we have $e^{-\alpha R}\delta^{-1}=\frac{1}{1+\ss}(\phi(R)+\kappa\phi^{(\ss)})^{2\alpha}
  \leq (\phi(R)+\kappa\phi^{(\ss)})^{2\alpha}$.
Thus, $\mu(\dt\setminus\Upsilon_Q)=O(n(\phi(R)+\kappa\phi^{(\ss)}))=O(\mu(\Upsilon_Q))$ as claimed. 
\end{proof}

\subsection{Uniform upper bound}\label{sec:upper_rad}
%
%\dmc{Put into brackets all the pages that were already before there}
\begin{comment}
Next, we study the detection probability on $\ndt$.
Fix $x_0=(r_0,\theta_0)\in\ndt$ and recall that a
particle which at time~$\ss$ is in position
$x_\ss=(r_\ss,\theta_\ss)$ will stay at an angle $\theta_\ss=\theta_0$ while its
radial distance from the origin $r_\ss$ evolves according to a diffusion
process with a reflecting barrier at $R$ and generator 
\[
\Delta_{rad} := \frac{1}{2}\frac{\partial^2}{\partial r^2}+\frac{\alpha}{2}\frac{1}{\tanh(\alpha r)}\frac{\partial}{\partial r}.
\]
Since we are only concerned with the evolution of the process up until the detection time of the target, which occurs when the particle reaches $B_Q(R)$, we can also impose an absorbing barrier at $\rabs_0$.
By our convention, this means that for values of $|\theta_0|\geq\frac{\pi}{2}$ we choose as absorbing barrier 
the origin $O$, that is, we let $\rabs_0=0$.
Also, recall that whatever the value of $\theta_0$ we 
have that $(\theta_0,\rabs_0)$ belongs to the boundary of $B_Q(R)$.
Since near the origin~$O$ the drift towards the boundary \dm{tends} to infinity, for a point 
$x_0$ such that $|\theta_0|\geq\frac{\pi}{2}$ we have $\P_{x_0}(T_{det}\leq t)=0$ (in other words, at these angles the only way to detect $Q$ would be by reaching the origin, but since the drift there is $-\infty$ this is impossible). For the case where the absorbing barrier is distinct from the origin  we use the following result whose proof is included for the sake of completeness:\dmc{I went over the proof, i left Amitai's colors for Marcos still}
\begin{lemma}\label{lemmaradial}
	Let $r_\ss$ be the diffusion process on $[\rabs,\rho]$ with generator $\Delta_{rad}$, and with a reflecting barrier at $\rho$. Denoting by $\P_{r_0}$ the law of $r_\ss$ with initial position $r_0$ and $T_{\rabs}$ the hitting time of~$\rabs$, we have:
	\begin{enumerate}[(i)]
	\item\label{radial:itm:phi1} For any $\lambda>0$ and any $r_0\in[\rabs,\rho]$,
	\begin{equation*}%\label{cotachern}
	\EE_{r_0}(e^{-\lambda T_{\rabs}})\,\leq\,\frac{\lambda_1 e^{-\lambda_2 (\rho-r_0)}+\lambda_2 e^{\lambda_1(\rho-r_0)}}{{\lambda_1 e^{-\lambda_2 (\rho-\rabs)}+\lambda_2 e^{\lambda_1(\rho-\rabs)}}},\end{equation*}
	where $\lambda_1=\sqrt{\frac{\alpha^2}{4}+2\lambda}+\frac{\alpha}{2}$ and $\lambda_2=\sqrt{\frac{\alpha^2}{4}+2\lambda}-\frac{\alpha}{2}$.
	\item\label{radial:itm:phi2} For any $r_0\in[\rabs,\rho]$,
	\[\EE_{r_0}(T_{\rabs})\,\leq\,\EE_{\rho}(T_{\rabs})\,\leq\,\frac{2e^{\alpha \rho}}{\alpha^2}\log(\cotanh(\tfrac12\alpha \rabs)).\]
	In particular, if $\rabs>\tfrac{1}{\alpha}\log 2$, then 
	$\displaystyle
	\EE_{r_0}(T_{\rabs})\,\leq\,\frac{32}{\alpha^2} e^{\alpha(\rho-\rabs)}$.
	\item\label{radial:itm:phi3} For any $r_0\in[\rabs,\rho]$,
	\[G_{\rabs}(r_0):=\PP_{r_0}(T_{\rabs}<T_\rho) = \frac{\log\big(\tfrac{\tanh(\alpha \rho/2)}{\tanh(\alpha r_0/2)}\big)}{\log\big(\tfrac{\tanh(\alpha\rho/2)}{\tanh(\alpha \rabs/2)}\big)}.
	\]
	\item\label{radial:itm:phi4} For any $r_0\in[\rabs,\rho]$, 
	\[\EE_{r_0}(T_{\rabs}\,\big|\,T_{\rabs}<T_\rho)\leq \frac{1}{\alpha}(r_0-\rabs)+\frac{2}{\alpha^2}\]
	\end{enumerate}
\end{lemma}
\begin{proof}
Let $\dt:=\dt(\kappa)$.
We begin with the proof of~\eqref{radial:itm:phi1} by observing that $\cotanh(x)\geq 1$ for all positive values of~$x$ and hence we can couple the trajectory of a particle $P$ with that of an auxiliary particle $\widetilde{P}$ starting with the same initial position as $P$, but whose radius $\widetilde{r}_\ss$ evolves according to the diffusion with generator
\[
\widetilde{\Delta}_r(f) := \frac{1}{2}f''+\frac{\alpha}{2}f',
\]
in such a way that $\widetilde{r}_\ss\leq r_\ss$ for all $\ss$. It follows that the detection time $\widetilde{T}_{\rabs}$ of this virtual particle is smaller than the one of $P_i$ so in particular $\EE_{r_0}(e^{-\lambda T_\rabs})\leq\EE_{r_0}(e^{-\lambda \widetilde{T}_{\rabs}})$, and it suffices to prove the inequality for the auxiliary process. Let now $g$ be the solution of the ordinary differential equation
\begin{equation}\label{ODE1}
\frac{1}{2}g''(r)+\frac{\alpha}{2}g'(r)-\lambda g(r)=0
\end{equation}
on $(\rabs,\rho)$ with boundary conditions $g(\rabs)=1$ and $g'(\rho)=0$. Since $\widetilde{r}_\ss$ can be written as $B_\ss+\frac{\alpha}{2}\ss+U_\ss$, where $B_\ss$ is a standard Brownian motion and $U_\ss$ is the process defined at the local time around $\rho$ giving the reflection effect, it follows that $e^{-\lambda\ss}g(\widetilde{r}_\ss)$ is a martingale (\textcolor{red}{see ref.}) and hence using Doob's optional stopping theorem we get $g(r_0)=\EE_{r_0}(e^{-\lambda\widetilde{T}_{\rabs}})$. Now, being a linear ordinary differential equation with constant coefficients, the solution of~\eqref{ODE1} has the form
\[g(r) = C_1 e^{-\lambda_1 r}+C_2 e^{\lambda_2 r}\]
where $\lambda_1$ and $\lambda_2$ are as in the statement of the lemma. The expression  on the right hand side of the inequality in~\eqref{radial:itm:phi1} follows then from the boundary conditions for $g$. 
To obtain the bound in~\eqref{radial:itm:phi2}, we go back to the original process $r_\ss$ which evolves according to $\Delta_{rad}$ and define $w(r,\ss):=\PP_{r}(T_{\rabs}>\ss)$. It follows from (\textcolor{red}{ref.}) that the function $w(r,\ss)$ is smooth and satisfies the partial differential equation
\[\frac{\partial}{\partial t}w(r,\ss) = \frac{1}{2}\frac{\partial^2}{\partial r^2}w(r,\ss)\,+\,\frac{\alpha}{2}\cotanh(\alpha r)\frac{\partial}{\partial r}w(r,\ss)\]
with boundary conditions $w(\cdot,0)= 1$, $w(\rabs,\cdot)= 0$, and $\frac{\partial}{\partial r}w(\rho,\cdot)= 0$. By coupling $r_\ss$ with an analogous process with constant drift $\frac{\alpha}{2}\cotanh(\alpha\rabs)$ it can be seen that as long as $\rabs>0$ we have $\PP_r(T_{\rabs}<\infty)=1$, and hence we obtain the additional boundary condition $\lim_{\ss\to\infty}w(r,\ss)=0$ for all $r\in[\rabs,\rho]$. Define $F(r)=\EE_r(T_{\rabs})=\int_0^\infty w(r,\ss)d\ss$ and use Leibniz's integral rule to deduce that $F$ satisfies the ordinary differential equation
\[-1 = \int_0^\infty\frac{\partial}{\partial \ss}w(r,\ss)d\ss = \frac{1}{2}F''(r)\,+\,\frac{\alpha}{2}\cotanh(\alpha r)F'(r)\]
with boundary condition $F'(\rho)=0$ and $F(\rabs)=0$. Multiplying by $2\sinh(\alpha r)$ we get
\[-2\sinh(\alpha r) = \sinh(\alpha r)F''(r)+\alpha\cosh(\alpha r)F'(r) = (\sinh(\alpha r)F'(r))'.\]
Thus, integrating from $r$ to $\rho$ and using that $F'(\rho)=0$ we have
\[\frac{2}{\alpha}(\cosh(\alpha\rho)-\cosh(\alpha r)) = \sinh(\alpha r)F'(r)\]
which in particular proves directly that $F(r)$ is an increasing function, so that $\EE_{r_0}(T_{\rabs})\leq \EE_\rho(T_{\rabs})$. Integrating from $\rabs$ to $\rho$, together with the condition $F(\rabs)=0$ \dmc{isn't the condition $F'(\rabs)=0$?} gives
\[
\EE_\rho(T_{\rabs}) = F(\rho) = \frac{2}{\alpha^2}\Big(\log\Big(\frac{\sinh(\alpha \rabs)}{\sinh(\alpha\rho)}\Big)-\cosh(\alpha\rho)\log\Big(\frac{\tanh(\tfrac12\alpha \rabs)}{\tanh(\tfrac12\alpha\rho)}\Big)\Big)
\]
and hence the general bound appearing in~\eqref{radial:itm:phi2} follows by noticing that the first term is negative, and by bounding $\cosh(\alpha\rho)$ by $e^{\alpha\rho}$. To obtain $\EE_{r_0}(T_{\rabs})\leq \frac{32}{\alpha^2} e^{\alpha(\rho-\rabs)}$ observe that if we assume $\rabs>\tfrac{1}{\alpha}\log 2$ then $\cotanh(\tfrac12\alpha\rabs)\leq 1+4e^{-\alpha\rabs}$ so the result follows from bounding $\log(1+4e^{-\alpha\rabs})\leq4e^{-\alpha\rabs}$\cmk{So where does the 32 come from? I get only 8.}. Nex, to obtain the equality appearing in~\eqref{radial:itm:phi3} observe that $G_{\rabs}(r_0)$ is smooth (see \textcolor{red}{ref.}) and satisfies the ordinary differential equation
\[
0 = \frac{1}{2}G_{\rabs}''(r) + \frac{\alpha}{2}\cotanh(\alpha r)G_{\rabs}'(r)
\]
with boundary conditions $G_{\rabs}(\rabs)=1$ and $G_{\rabs}(\rho)=0$. The solution to this O.D.E. is given by the closed expression given in~\eqref{radial:itm:phi3}. Finally, to prove~\eqref{radial:itm:phi4} define the function $h_{\rabs}(r,\ss)=\PP_{r_0}(T_{\rabs}\geq \ss,\,T_{\rabs}<T_\rho)$ which, just like $w(r,\ss)$\cmk{This means that the same result referenced before applies in this case? If so, the reference should be included here also.} from the proof of~\eqref{radial:itm:phi2}, is smooth and satisfies the partial differential equation
\[\frac{\partial}{\partial t}h(r,\ss) = \frac{1}{2}\frac{\partial^2}{\partial r^2}h(r,\ss)\,+\,\frac{\alpha}{2}\cotanh(\alpha r)\frac{\partial}{\partial r}h(r,\ss)\]
but this time the boundary conditions have the form
\[h(\rabs,\cdot)= 0,\qquad h(\rho,\cdot)= 0,\qquad\text{ and }\quad h(\cdot,0)=G_{\rabs}(\cdot).\]
Treating $h$ as we did with $w$, we obtain that the function $H(r)=\EE_{r_0}(T_{\rabs},\,T_{\rabs}<T_\rho)=\int_0^\infty h(r,t)dt$ \cmk{} satisfies
\[-G_{\rabs}(r) = \frac{1}{2}H''(r)\,+\,\frac{\alpha}{2}\cotanh(\alpha r)H'(r)\]
with boundary conditions $H(\rabs)=H(\rho)=0$. It can be checked directly that the function
{\footnotesize \[H(r_0)=\frac{2}{\alpha}G_{\rabs}(r_0)\int_{\rabs}^{r_0} \sinh(\alpha l)G_{\rabs}(l)\log\left(\frac{\tanh(\frac{\alpha l}{2})}{\tanh(\frac{\alpha \rabs}{2})}\right)dl+\frac{2}{\alpha}(1-G_{\rabs}(r_0))\int_{r_0}^\rho \sinh(\alpha l)G_{\rabs}(l)\log\left(\frac{\tanh(\frac{\alpha\rho}{2})}{\tanh(\frac{\alpha l}{2})}\right)dl\]}
satisfies this equation. Now, using the definition of $G_{\rabs}(l)$, the expression within the first integral can be upper bounded by $\sinh(\alpha l)\log\left(\cotanh(\frac{\alpha l}{2})\right)$ which is increasing and bounded by $1$. \dmc{Checked until here} Using a similar argument we can bound the term within the second integral by $G_{\rabs}(l)$, so that
\[H(r_0)\leq\frac{2}{\alpha}G_{\rabs}(r_0)(r_0-\rabs)+\frac{2}{\alpha}(1-G_{\rabs}(r_0))\int_{r_0}^\rho G_{\rabs}(l)dl.\]
Since $\EE_{r_0}(T_{\rabs}\,|\,T_{\rabs}<T_\rho)=\frac{H(r_0)}{G_{\rabs}(r_0)}$, and using the fact that $\frac{G_{\rabs}(l)}{G_{\rabs}(r_0)}=G_{r_0}(l)$ we deduce \dmc{shouldn't the upper bound of the integral be $R$ instead of $\rho$?}
\[\EE_{r_0}(T_{\rabs}\,|\,T_{\rabs}<T_\rho)\leq\frac{2}{\alpha}(r_0-\rabs)+\frac{2}{\alpha}\int_{r_0}^\rho G_{r_0}(l)dl\]
and hence to conclude the bound in \eqref{radial:itm:phi4} we only need to control the remaining integral term. To do so observe that the function $r_0\to \int_{r_0}^\rho G_{r_0}(l)dl$ has a single maximum at $r_0'$ satisfying
\[\int_{r'_0}^\rho G_{r'_0}(l)dl=\frac{1}{\alpha}\log\left(\frac{\tanh(\frac{\alpha\rho}{2})}{\tanh(\frac{\alpha r_0'}{2})}\right)\sinh(\alpha r_0')\leq\frac{1}{\alpha}\]
and the result then follows directly from this observation.
\end{proof}
\end{comment}

%\cmk{Changed to make it consistent with strategy of proof.}\dmc{also ok with me}
The goal of this subsection is to prove the following result from which 
the upper bounds of Theorem~\ref{thm:rad} immediately follow.
\begin{proposition}\label{prop:rad-lowerBnd}
If  $\kappa\geq 1$, then
\[
\sup_{x_0 \in \ndt\!(\kappa), |\theta_0| < \frac{\pi}{2}}  \P_{x_0}(T_{det}\leq\ss)=O(\delta(\kappa,\ss)) 
\quad\text{ and } \quad 
\sup_{x_0 \in \ndt\!(\kappa), |\theta_0| \geq \frac{\pi}{2}}  \P_{x_0}(T_{det}\leq\ss)=0. 
\]
%\cmk{For consistency with the angular case and our general strategy, shouldn't we change the statement below to just $\int_{\ndt}\PP_{x_0}(T_{det}\leq\ss)=O(\mu(\dt)\delta(\kappa,\ss))$?}\dmc{Ok for me, but is it like this in the angular section? I don't see it there}\cmk{Check the paragraph before the previous to last displayed equation of the proof of Theorem 9. Its there. Its not part of any statement because in the angular section we do not separate the analysis in lower and upper bound. In fact, we should probably eliminate the headers for the subsections Upper and Lower here and add a short discussion reminding the reader that Theorem 15 follows from Lemma 19, Proposition 20 and 21 using the same arguments used to prove Theorem 9, right?}\dmc{Yes, I see the line, so we should put the same below for symmetry}
Furthermore, if $\phi(R)+\kappa\phis\leq\frac{\pi}{2}$, then
\[
\int_{\ndt\!(\kappa)}\P_{x_0}(T_{det}\leq\ss)d\mu(x_0) = O(\mu(\dt(\kappa))).
\]
\end{proposition}
\begin{proof}
By the discussion previous to Lemma~\ref{lemmaradial}, we have $\P_{x_0}(T_{det}\leq\ss)=0$
  when $x_0=(r_0,\theta_0)\in\ndt$ is such that $|\theta_0|\geq\frac{\pi}{2}$, as claimed.
Henceforth, let $x_0=(r_0,\theta_0)\in\ndt$ be such that
$|\theta_0|<\frac{\pi}{2}$.  
%Let $\rabs_0$ and $\widetilde{\rabs}_0$ be as previousl
%as defined by~\eqref{eqn:radial-defrtilde}.
As in Lemma~\ref{lemmaradial}, 
let $\P_{r_0}$ be the law of $r_\ss$ with initial position $r_0$
and $T_{\rabs}$ be the hitting time of $\rabs$.

Observe that a particle initially at $x_0$ reaches the boundary of $B_{Q}(R)$ either without ever visiting the boundary of $B_O(R)$ (which happens with probability at most $\P_{r_0}(T_{\rabs_0}<T_R))$ or after visiting the 
said boundary for the first time (in which case the time to reach the boundary of $B_Q(R)$ is dominated by the time of a particle starting at the boundary of $B_O(R)$ and having 
the same angular coordinate as the point $x_0$).
Thus,
\[
\P_{r_0}(T_{\det}\leq\ss) \leq \P_{r_0}(T_{\rabs_0}<T_R)+\P_{R}(T_{\rabs_0}\leq\ss).
\]
We will show that each of the right hand side terms above is $O(\delta)$
where $\delta:=\delta(\kappa,\ss)$. This immediately implies
the first part of the proposition's statement.

By our choice of $\widetilde{\rabs}_0$
and since $r_0\geq\widetilde{\rabs}_0$, we have 
\[
\P_{r_0}(T_{\rabs_0}<T_R) \leq \P_{\widetilde{\rabs}_0}(T_{\rabs_0}<T_R)
= \delta.
\]
By Part~\eqref{radial:itm:phi1} of Lemma~\ref{lemmaradial} with $\auxy:=r$, $\yabs_0:=\rabs_0$ and $\auxY:=R$, and Markov's inequality,
for $\lambda>0$, $\lambda_1:=\sqrt{\frac{\alpha^2}{4}+2\lambda}+\frac{\alpha}{2}>0$ and $\lambda_2:=\sqrt{\frac{\alpha^2}{4}+2\lambda}-\frac{\alpha}{2}$, we get that
\begin{equation}
\P_{r_0}(T_{\rabs_0}\leq\ss) 
= \P_{r_0}(e^{-\lambda T_{\rabs_0}}\geq e^{-\lambda\ss})\leq \frac{\lambda_1 e^{-\lambda_2(R-r_0)}+\lambda_2 e^{\lambda_1(R-r_0)}}{\lambda_2e^{\lambda_1(R-\rabs_0)}}e^{\lambda \ss}.
\label{eqn:radial-upper-aux}
%\leq \frac{\lambda_1+\lambda_2}{\lambda_2 e^{\alpha(R-\rabs_0)}}e^{\lambda t}.
\end{equation}
Observing that $\lambda_1\lambda_2=2\lambda$, that $\lambda_2>0$ and $\lambda_1>\alpha$,
and taking $\lambda=\frac{1}{1+\ss}<1$ (since by hypothesis $\ss> 0$) it follows that
\begin{equation}\label{eqn:radial-upper-aux2}
\P_{R}(T_{\rabs_0}\leq\ss) 
\leq \frac{\lambda_1+\lambda_2}{\lambda_2 e^{\lambda_1(R-\rabs_0)}}\cdot e^{\lambda\ss}
= O\Big(\frac{e^{\lambda\ss}}{\lambda}e^{-\lambda_1(R-\rabs_0)}\Big)
= O((1+\ss) e^{-\alpha(R-\rabs_0)}).
\end{equation}
By our choice of $\delta$, we have $1+\ss\leq\delta(1+\kappa\ss^{\frac{1}{2\alpha}})^{2\alpha}
=\Theta(\delta e^{\alpha R}(\phi(R)+\kappa\phi^{(\ss)})^{2\alpha})$.
Moreover, by Part~\eqref{itm:phi4} of Lemma~\ref{lem:phi}, we have 
$|\theta_0|=\phi(\rabs_0)=\Theta(e^{-\frac12\rabs_0})$.
Since by hypothesis $x_0\in\ndt$, we know that $|\theta_0|\geq\phi(R)+\kappa\phi^{(\ss)}$,
it follows that $e^{-\frac12\rabs_0}=\Omega(\phi(R)+\kappa\phi^{(\ss)})$.
Thus, 
\[
\P_{R}(T_{\rabs_0}\leq\ss) 
= O\big(\delta (\phi(R)-\kappa\phis)^{2\alpha}e^{\alpha \rabs_0}\big)=O(\delta)
\]
which establishes the first part of our stated result.

Next, we consider the second stated claim (the integral bound).
By~\eqref{eqn:radial-upper-aux}, denoting by~$\mu_{\theta_0}$ the measure induced by $\mu$ for the angle $\theta_0$, recalling that $\lambda_1-\lambda_2=\alpha$, it follows that
\begin{align*}
\int_{\widetilde{\rabs}_0}^R\P_{x_0}(T_{det}\leq\ss)d\mu_{\theta_0}(r_0)
&= O(n)\int_{\widetilde{\rabs}_0}^R\frac{\lambda_1 e^{-\lambda_2 (R-r_0)}+\lambda_2 e^{\lambda_1(R-r_0)}}{{\lambda_2 e^{\lambda_1(R-\rabs_0)}}}\cdot e^{\lambda\ss}\cdot e^{-\alpha (R-r_0)}dr_0\\
&= O(n)\frac{e^{\lambda_2 (R-\widetilde{\rabs}_0)}-e^{-\lambda_1(R-\widetilde{\rabs}_0)}}{{\lambda_2 e^{\lambda_1(R-\rabs_0)}}}\cdot e^{\lambda\ss} \\
&= O(n) \frac{e^{\lambda_2 (R-\widetilde{\rabs}_0)}}{\lambda_2 e^{\lambda_1 (R-\rabs_0)}}\cdot e^{\lambda\ss}. 
%\\
%&= O(n)e^{-(\lambda_2-\lambda_1)(R-\rabs_0)}e^{-\lambda_2(\widetilde{\rabs}_0-\rabs_0)}\cdot\frac{1}{\lambda_2}\cdot e^{\lambda t}.
\end{align*}
Using once more that $\lambda_1-\lambda_2=\alpha$ and $\lambda_1\lambda_2=2\lambda$, taking again $\lambda=\frac{1}{1+\ss}< 1$,
we obtain
\[
\int_{\widetilde{\rabs}_0}^{R}\P_{x_0}(T_{det}\leq\ss)d\mu_{\theta_0}(r_0)
= O(n (1+\ss) e^{-\alpha (R-\rabs_0)}e^{-\lambda_2(\widetilde{\rabs}_0-\rabs_0)}).
\]
Therefore, since $\widetilde{\rabs}_0\geq\rabs_0$ and $\lambda_2>0$ imply that 
$\lambda_2(\widetilde{\rabs}_0-\rabs_0)$ is positive, it follows that
\[
  \int_{\ndt}\P_{x_0}(T_{det}\leq \ss)d\mu(x_0)
  = O(n(1+\ss)e^{-\alpha R})\int_{\phi(R)+\kappa\phi^{(\ss)}}^{\frac{\pi}{2}}e^{\alpha\rabs_0}d\theta_0.
\]
Using that $|\theta_0|=\phi(\rabs_0)=\Theta(e^{-\frac12\rabs_0})$, since
  $\alpha>\frac12$, by definition of $\delta$ we get
\[
\int_{\ndt}\P_{x_0}(T_{det}\leq \ss)d\mu(x_0)
  = O(n(1+\ss)e^{-\alpha R})\int_{\phi(R)+\kappa\phi^{(\ss)}}^{\frac{\pi}{2}}\theta_0^{-2\alpha}d\theta_0 
  = O(n\delta(\phi(R)+\kappa\phi^{(\ss)})).
\]
Since $\delta\le 1$, the claimed integral bound follows from Lemma~\ref{lem:radialmeasure}.
\end{proof}

%
\subsection{Uniform lower bound}\label{sec:lower_rad}
%
The goal of this subsection is to prove the following lower bound matching the upper bound of Proposition~\ref{prop:rad-lowerBnd}  under the assumption that at least a constant amount of time has passed since the process started (as stated in Theorem~\ref{thm:rad}).
\begin{proposition}\label{prop:rad-upperBnd}
For every fixed arbitrarily small constant $c>0$
there are large enough  constants $\kappa_0, C\geq 1$ such that 
if $\kappa\geq\kappa_0$ and $\ss\geq\frac{16}{\alpha}\log\kappa+C$ satisfy $\phi(R)+\kappa\phi^{(\ss)}\leq\frac{\pi}{2}-c$, then 
\[
\inf_{x_0\in\dt(\kappa)}\P_{x_0}(T_{det}\leq \ss) = \Omega(\kappa^{-2\alpha}).
\]
\end{proposition}
\begin{proof}
Let $\dt:=\dt(\kappa)$.
To obtain the lower bound on the detection probability recall that any $x_0\in\dt$ either satisfies $|\theta_0|\leq\phi(R)+\kappa\phi^{(\ss)}$ or $r_0\leq \widetilde{\rabs}_0$ where $\widetilde{\rabs}_0$ by definition is such that $G_{\rabs_0}(\widetilde{\rabs}_0)=\delta(\kappa,\ss)$. We deal first with the case $\rabs_0<r_0\leq \widetilde{\rabs}_0$ (the case $r_0\leq \rabs_0$ is trivial since it implies that $x_0\in B_Q(R)$ and thus $T_{det}=0$) by noticing that %\dmc{a comment. In the long lemma the subindex is just the radial coordinate $r$ or $r_0$, not $x_0$. Maybe use only the first?}\cmk{I recall once we tried this and it sort of created a mess. Also, the long lemma is sort of a stand-alone. If we used as sub-index $r_0$ then in the angular section the analogue would be to use $\theta_0$. I think it is better to leave it as is. Thus creating a more unified treatment for the 3 settings (angular, radial, mixed).}\cml{I agree with Marcos}
\begin{align*}\P_{x_0}(T_{det}\leq \ss)&\geq\,\P_{x_0}(T_{\rabs_0}\leq \ss\,\big|\,T_{\rabs_0}<T_{R})\P_{x_0}(T_{\rabs_0}<T_{R})\\&\geq\,\big(1-\tfrac{1}{\ss}\EE_{x_0}(T_{\rabs_0}\,\big|\,T_{\rabs_0}<T_{R})\big)\P_{x_0}(T_{\rabs_0}<T_{R}).
\end{align*}
Using Part~\eqref{radial:itm:phi4} of Lemma~\ref{lemmaradial} with $\auxy:=r$, $\yabs_0:=\rabs_0$ and $\auxY:=R$, recalling that $G_{\rabs_0}(r)=\P_{r}(T_{\rabs_0}<T_{R})$, since by case assumption $r_0\leq\widetilde{\rabs}_0$ and recalling again that by definition of $\widetilde{\rabs}_0$ it holds that $G_{\rabs_0}(\widetilde{\rabs}_0)=\delta:=\delta(\kappa,\ss)$, we get
\[
\P_{x_0}\big(T_{det}\leq \ss\big)\geq 
  \Big(1-\frac{1}{\ss}\Big[\frac{2}{\alpha}(\widetilde{\rabs}_0-\rabs_0)+\frac{2}{\alpha^2}(1-\delta)\Big]\Big)\delta.
\]
From Fact~\ref{fct:radial-varphi2} it follows that $\widetilde{\rabs}_0-\rabs_0=\frac{1}{\alpha}\log \tfrac{1}{\delta}+O(1)$, and since we can bound from above $1-\delta$ by $\log\tfrac{1}{\delta}$, we get that the term inside the brackets above is at most $\frac{4}{\alpha^2}\log\frac{1}{\delta}+O(1)$. By hypothesis, $\ss\ge C\ge 1$, so $\delta\geq (1+\kappa)^{-2\alpha}=\Omega(\kappa^{-2\alpha})$ %\dmc{This seems true, but not so easy to see, right?}\cmk{I think it is easy, just note that since $\ss\geq 1$ we have $z^{\frac{1}{2\alpha}}(1+\kappa)\geq 1+\kappa z^{\frac{1}{2\alpha}}$ so $\delta=(1+\ss)/(1+\kappa z^{\frac{1}{2\alpha}})^{2\alpha}\geq (1+z)/(z(1+\kappa)^{2\alpha}\geq (1+\kappa)^{-2\alpha}$}\dmc{We need to discuss the need for $\delta$. Why not putting $\KA^{-2\alpha}$ directly?} 
and hence, using again the hypothesis regarding $\ss$, we see that $\ss>\frac{16}{\alpha}\log\kappa+C\geq \frac{8}{\alpha^2}\log\frac{1}{\delta}+\frac12 C$ for $C$ large enough, so taking $C$ even larger if needed we conclude that 
\[%\label{eq:1stbound}
\P_{x_0}\big(T_{det}\leq \ss\big)=\Omega(\delta)=\Omega(\kappa^{-2\alpha}).
\]
We now handle the points $x_0:=(r_0,\theta_0)\in\dt$ for which $|\theta_0|\leq\phi(R)+\kappa\phi^{(\ss)}$. Observe that for any fixed~$\theta_0$ the detection probability is minimized when $r_0=R$, and for points such that $r_0=R$ the probability decreases with $|\theta_0|$, so it will be enough to consider the case where $r_0=R$ and $|\theta_0|=\phi(R)+\kappa\phi^{(\ss)}$. To obtain lower bounds on the detection probability we will couple the radial movement $r_t$ of the particle starting at $x_0$ with a similar one, denoted by $\widetilde{r}_t$, evolving as follows:
\begin{itemize}
    \item $\widetilde{r}_0=R$ and we let $\widetilde{r}_t$ evolve through the generator $\Delta_{rad}$ on $(0,R+1]$.
    \item Every time $\widetilde{r}_t$ hits $R+1$ it is immediately restarted at $R$.
\end{itemize}
It follows that $r_t$ and $\widetilde{r}_t$ can be coupled so that $r_t\leq\widetilde{r}_t$ for every $t\geq0$, and hence the detection time $\widetilde{T}_{\rabs_0}$ of the new process bounds the original $T_{det}=T_{\rabs_0}$ from above. Thus, it will be enough to bound $\PP_{x_0}(\widetilde{T}_{\rabs_0}\leq\ss)$ from below. Observe that the trajectories of $\widetilde{r}_t$ are naturally divided into excursions from $R$ to $R+1$, which we use to define a sequence of Bernoulli random variables $\{E_i\}_{i\geq1}$ where $E_i=1$ if $\widetilde{r_t}$ reaches $\rabs_0$ on the $i$-th excursion. We also use this division to define a sequence $\{\tau_i\}_{i\geq1}$ of random variables, where $\tau_i$ is the time it takes $\widetilde{r}_t$ to reach either $\rabs_0$ or $R+1$ in the $i$-th excursion. It follows from the strong Markov property that all excursions are independent of one another, and so are the $E_i$'s and $\tau_i$'s.

Fix $m:=\lfloor \tfrac{\alpha}{24}\ss\rfloor$ which from our hypothesis on $\ss$ satisfies $m\geq 1$, and observe that, since the $E_i$'s are i.i.d., the event $\mathcal{E}:=\{\exists 1\leq i\leq m,\,E_i=1\}$ has probability
\[%\label{radial:eqn:boundCalE}
\PP(\mathcal{E})=1-(1-\PP(E_1))^m\geq 1-e^{-m\PP(E_1)}.
\]
From Part~\eqref{radial:itm:phi3} of Lemma~\ref{lemmaradial} with $\auxy:=R$, $\yabs_0:=\rabs_0$ and $\auxY:=R+1$, using  $\log(1+x)\geq\frac{x}{2}$ for $|x|<1$
and $\log(\cotanh(\frac{x}{2}))\leq \frac{2e^{-x}}{1-e^{-x}}$, we have
\[%\label{radial:eqn:boundE1}
\PP(E_1)=\frac{\log\big(\tfrac{\tanh(\alpha (R+1)/2)}{\tanh(\alpha R/2)}\big)}{\log\big(\tfrac{\tanh(\alpha R/2)}{\tanh(\alpha \rabs_0/2)}\big)}
\geq\frac{\log\big(1+\frac{2e^{-\alpha R}(1-e^{-\alpha})}{(1+e^{-\alpha(R+1)})(1-e^{-\alpha R})}\big)}{\log\big(\cotanh(\alpha\frac{\rabs_0}{2})\big)}
=\Omega(e^{-\alpha(R-\rabs_0)}(1-e^{-\alpha\rabs_0})).
%\geq\frac{e^{-\alpha (R-\rabs_0)}(1-e^{-\alpha})(1-e^{-\alpha\rabs_0})}{2(1+e^{-\alpha(R+1)})(1-e^{-\alpha R})}.
\]
Since $\phi(R)+\kappa\phi^{(\ss)}\leq\frac{\pi}{2}-c$, we get $\rabs_0=\Omega(1)$ and deduce that $\PP(E_1)=\Omega(e^{-\alpha(R-\rabs_0)})$.
Recalling that $\phi(R)+\kappa\phi^{(\ss)}=|\theta_0|=\phi(\rabs_0)$, by Part~\eqref{itm:phi4} of Lemma~\ref{lem:phi}, we have that
$e^{-\frac12\rabs_0}=\Theta(\phi(R)+\kappa\phi^{(\ss)})$ and $\phi(R)=\Theta(e^{-\frac12R})$.
Since by hypothesis $\ss,\kappa\geq 1$, it follows that
$e^{\frac12(R-\rabs_0)}=\Theta(e^{\frac12 R}(\phi(R)+\kappa\phi^{(s)}))=\Theta(\kappa\ss^{\frac{1}{2\alpha}})$.
Thus, $\PP(E_1)=\Omega(\kappa^{-2\alpha}/\ss)$ so setting $\kappa_0$ sufficiently large we get
\begin{equation*}%\label{radial:eqn:boundCalE}
\PP(\mathcal{E}) \geq %1-\exp\big({-}m\Theta\big(e^{-\alpha R}(\phi(R)+\kappa\phi^{(\ss)})^{-2\alpha}\big)\big) 
1-e^{-\Omega(\kappa^{-2\alpha})}
=\Omega(\kappa^{-2\alpha}).
\end{equation*}
%Since $\ss=\Omega(1)$, the term in the exponent is $\Omega(\delta)$ and hence $\PP(\mathcal{E})=\Omega(\delta)$.
Clearly, if $i_D$ is the first index such that $E_{i_D}=1$, then $\mathcal{E}=\{1\leq i_D\leq m\}$. Hence, 
\[\P_{x_0}(T_{det}\leq \ss)\geq \P_{x_0}(T_{det}\leq \ss\mid\mathcal{E})\P_{x_0}(\mathcal{E})=\P_{x_0}\Big(\sum_{i=1}^{i_D}\tau_i\leq \ss \mid 1\leq i_D\leq m\Big)\PP(\mathcal{E}),\]
where the $\tau_i$ were defined above. Using Markov's inequality we obtain
\[\P_{x_0}\Big(\sum_{i=1}^{i_D}\tau_i> \ss\,\big|\,1\leq i_D\leq m\Big)\leq \frac{1}{\ss}\big(m\EE_{x_0}(\tau_1 \mid E_1=0)+\EE_{x_0}(\tau_1 \mid E_1=1)\big),\]
since there are at most $m$ excursions where the particle fails to reach $\rabs_0$, followed by a single excursion where it succeeds (here we are conditioning on these events through the value of $E_1$). For the first term, we have
\[\EE_{x_0}(\tau_1 \mid E_1=0)=\EE_{R}(\widetilde{T}_{R+1} \mid \widetilde{T}_{R+1}\leq \widetilde{T}_{\rabs_0})\,\leq\, \frac{6}{\alpha},\]
as can be seen in~\cite{Borodin2002} (formulas 3.0.4 and 3.0.6).  by comparing the process to one with constant drift equal to $\tfrac{\alpha}{2}$. For the second term we use Part~\eqref{radial:itm:phi4} of Lemma~\ref{lemmaradial} with $\auxy:=R$, $\yabs_0:=\rabs_0$ and $\auxY:=R+1$ to obtain
\[
\EE_{x_0}(\tau_1 \mid E_1=1)=\EE_{R}(\widetilde{T}_{\rabs_0} \mid \widetilde{T}_{\rabs_0}<\widetilde{T}_{R+1})\leq \frac{2}{\alpha}(R+1-\rabs_0)+\frac{2}{\alpha^2}.
\]
%As already observed, we can bound $1-\delta$ by $\log\frac{1}{\delta}$. Also,
Recalling that
$e^{\frac12(R-\rabs_0)}=\Theta(\kappa\ss^\frac{1}{2\alpha})$, 
we have $R-\rabs_0=\log(\kappa\ss^{\frac{1}{2\alpha}})+\Theta(1)$.
%so $R-\rabs_0=\frac{1}{\alpha}\log(1+\kappa\ss^{\frac{1}{2\alpha}})^{2\alpha}+\Theta(1)=\frac{1}{\alpha}\log\frac{1+\ss}{\delta}+\Theta(1)$.%\dmc{For the second term $\frac{2}{\alpha^2}(1-\delta)$ can we perhaps just put an upper bound of $\frac{2}{\alpha^2}$? But for the first term I can try the following: $\theta_0=2\kappa \phis=2\kappa \ss^{1/(2\alpha)}e^{-R/2}$ and \dmc{need an upper bound of $\ss$ here to get just an $o(1)$ in the exponent coming from Lemma 7(v)} $\rabs_0=\phi^{-1}(|\theta_0|)=\phi^{-1}(2 \kappa \ss^{1/(2\alpha)}e^{-R/2})=\phi^{-1}(2e^{-\frac12(R- 2\log \kappa-\frac{1}{\alpha}\log \ss+o(1)})=R-2\log \kappa - \frac{1}{\alpha}\log \ss$\cmk{The 3rd term before should be $\phi^{-1}((\phi(r)+\kappa\ss^{\frac{1}{2\alpha}})e^{-\frac{R}{2}})$}, and so $R-\rabs_0=2\log \kappa+\frac{1}{\alpha}\log \ss$, and this is much smaller than $\ss$. Then plug in, and we are fine, being much easier than what is done below...}\dmc{For Section 5, I thought first we have $|\theta_0|=3\KR e^{-r_0/2}$, and so $\rabs_0=\phi^{-1}(|\theta_0|)=\phi^{-1}(2e^{-1/2(r_0-2 \log(3 \KR)+o(1))})$, so $R-\rabs_0=\ldots$, but in fact since this corresponds to later excursions starting anyway at $R$, the definition of $\rabs_0$ does not change.}\cmk{Not sure what this last comment is about.}
Summarizing,
\[\P_{x_0}\Big(\sum_{i=1}^{i_D}\tau_i> \ss\mid 1\leq i_D\leq m\Big)= \frac{6m}{\alpha\ss}+
\frac{1}{\ss}\Big(\frac{2}{\alpha}\log(\kappa\ss^{\frac{1}{2\alpha}})+O(1)\Big).\] 
%\[\P_{x_0}\Big(\sum_{i=1}^{i_D}\tau_i> \ss\mid 1\leq i_D\leq m\Big)\leq \frac{6m}{\alpha\ss}+\replaced[id=mk]{\frac{1}{\ss}\Big(\frac{4}{\alpha^2}\log\frac{1}{\delta}+\frac{1}{\alpha}\log(1+\ss)+\Theta(1)\Big)}{O\Big(\frac{1}{\ss}(R+1-\rabs_0)\Big)}.\] 
By our choice of $m$ it is immediate that $\frac{6m}{\alpha\ss}\leq\frac14$. 
To bound the last term of the displayed equation recall that by hypothesis $\ss\geq\frac{16}{\alpha}\log\kappa+C$, 
so that we have $\frac{2}{\alpha}\log\kappa\le\frac18\ss$ and moreover we can choose 
$C$ large enough so that $\frac{1}{\alpha^2}\log\ss\leq\frac{1}{8}\ss$ and finally conclude that
%\deleted[id=mk]{*DM: I had to do a bit of calculation to see the second one (please add as much as you think is needed): I agree with the $O$ notation above, but to get the $1/4$ for there: first recall that $Ce^{-\rabs_0/2}=|\theta_0|$ for some $C > 0$ (I guess we don't have $C=2$ in general), by Lemma~\ref{lem:phi}(v). Then we need to show that $\ss \gg R-\rabs_0$, or equivalently $e^{(\ss-R)/2} \ge e^{-\rabs_0/2}$, or $e^{(\ss-R)/2}/C \ge |\theta_0|$, or equivalently $e^{\ss/2}/C \ge e^{R/2}|\theta_0|$. Then using that $|\theta_0| \le \phi(R)+\kappa ((\ss^{1/\alpha})/e^R)^{1/2}$ it suffices to show that $e^{\ss/2}/C \ge e^{R/2}(\phi(R)+\kappa ((\ss^{1/\alpha})/e^R)^{1/2})$. If $\phi(R)$ dominates, then the result follows since $\ss=\Omega(1)$. If the second term dominates, we need to have $e^{\ss/2} \ge \kappa \ss^{1/(2\alpha)}$, which again follows from $\ss=\Omega(1)$*}
\[%\label{eq:2ndbound}
\P_{x_0}(T_{det}\leq \ss)=\Big(1-\P_{x_0}\Big(\sum_{i=1}^{i_D}\tau_i> \ss\,\big|\,1\leq i_D\leq m\Big)\Big)\PP(\mathcal{E})=\Omega(\kappa^{-2\alpha}).
\]
%Combining with~\eqref{eq:1stbound} we obtain the desired lower bound for every $x_0 \in \dt$, and 
%The proposition follows.
%For the last part of the proposition, it is enough to notice that $\mu(\dt)\geq \mu(\Upsilon_Q(\kappa\phi^{(\ss)}))\geq 2n\kappa\phi^{(\ss)}$.
\end{proof}
Observe that the bounds established in the preceding proposition are exactly those claimed in the first part of  Theorem~\ref{thm:rad}.

\begin{comment}
\subsection{Time spent within an interval containing the origin}
%
In this section we consider a class of one-dimensional diffusion processes\cml{I would rather say continuous process}\dmc{sure, let's see if we need continuity} taking place in $[0, \auxY]$ with $\auxY$ a positive integer  %, and with generator $\Delta_h$ 
	%with a reflecting barrier at a positive integer value $\auxY$ 
and with given stationary  distribution.
	Our study is motivated by the process $\{\auxy_s\}_{s\geq 0}$ in $(0,\auxY]$ with generator~$\Delta_h$
	(see~\eqref{radialgenerator}) and reflecting barrier at $\auxY$. This section's main result
	will later in the paper be applied precisely to this latter process. However, the arguments of this section are applicable to less constrained  diffusion processes and might even be further
	generalized to a larger class of processes, and so we have opted for an exposition which, instead
	of tailoring the proof arguments to the specific 
	process with generator $\Delta_h$, gives the conditions that need to be satisfied so that
	our results apply to a specific process.

    This section's main result establishes a rather intuitive fact. Roughly speaking, we show that under some general hypotheses, during a not too small time period of length $\ss$ a diffusion process $\{\auxy_s\}_{s\geq 0}$ with stationary distribution $\pi(\cdot)$ on $[0,\auxY]$ spends, with constant probability, 
    within an interval $[0,k]$ ($k$ larger than
    a constant) a period of time that is proportional
    to $\ss\pi([0,k])$. \dm{We remark that $k$ only needs to be larger than a sufficiently large constant $C$, which does not depend on $Y$ (otherwise the result would follow directly from Markov's inequality).}\dmc{Ok?}\cml{We need also to say that it is not that dependent of $\pi$ (otherwise it still depends on $Y$). Maybe it depends on the drift alone?} (Note that this latter quantity is the expected time the process spends in $[0,k]$ during a period of time of length $\ss$ when starting according to the stationary distribution.)
	
	We start by establishing two lemmas. The first one states that a diffusion process $\{\auxy_s\}_{s\geq 0}$ in $[0,\auxY]$ with stationary distribution $\pi(\cdot)$,
    henceforth referred to as the \emph{original process}, can be coupled with a process $\mathfrak{D}$ that is continuous in time but discretized in space with values in $(-\infty,\auxY]\cap\NN$ and defined below: in order to specify $\mathfrak{D}$, we need auxiliary definitions and hypotheses: first, let $\pi_{1}=\pi([0,1])$ and $\pi_m:=\pi((m-1,m])$ where $1<m\le \auxY$. 
    Let~$p_m$ be the probability that the process
    %\dmc{I don't think you want to start right at 0 in which case $p_1=1$ even with drift infinity.  I suggest: "starting from the stationary distribution conditioned on starting in the interval $[0,1]$" But in fact this is the same wording  as the general m case (up to the point 0 exactly the same thing). This is a product space? Stationary distribution together with drift}, 
    hits $m-1$ before it hits $m$ when starting from the stationary distribution conditioned on starting in the interval $(m-1,m]$ if $m\neq 1$ and in the interval~$[0,1]$ if~$m=1$.
    It is easily checked \cml{I am having trouble checking this... help?} that the following system is satisfied by the $p_m$'s and~$\pi_m$'s:
    %\dmc{Ok. I think I understand now. Indeed without fixing one value the system was not determined, since I forgot that the stationary values sum up to 1. But it should be determined, and $p_1=0$ seems the right choice (so we adapt the first line). Then we say: "$p_m$ can be interpreted as the probability of exiting the interval $(m-1,m]$ to the left, that is, to end up in the interval $(m-2,m-1]$ after having been in $(m-1,m]$. Observe that we have $p_1=0$ since we cannot have values below $0$."}
    \begin{equation}\label{radial:def:pim}
    \pi_{m} = 
    \begin{cases} 
    p_{m}\pi_{m}+p_{m+1}\pi_{m+1}, & \text{if $m=1$,} \\    
    (1-p_{m-1})\pi_{m-1}+p_{m+1}\pi_{m+1}, & \text{if $1<m<\auxY$,} \\
    (1-p_{m-1})\pi_{m-1}+(1-p_{m})\pi_{m}, & \text{if $m=\auxY$.} 
    \end{cases}
    \end{equation}
    Furthermore, let $p=p(\{\auxy_s\}_{s\geq 0})$ where 
    \[
	p(\{\auxy_s\}_{s\geq 0}) := \sup_{m\in\NN: 1 \le m\le\auxY} p_m.
	%\PP_{m+1}(\text{if $\{\auxy_s\}_{s\geq 0}$ first exits $(m,m+1]$ at time $t$, then $\auxy_t\in (m-1,m]$}).
	\]
	We are exclusively interested in processes for which the following holds:
	\begin{quote}
	    \mycom{hyp:H0}{(H0)} 
	    $p\in [\underline{c},\overline{c}]$, $0<\underline{c}\leq\overline{c}<1/2$, 
	    for constants $\underline{c},\overline{c}>0$ 
	    that do not depend on $\auxY$. 
	\end{quote}
	In other words, we are interested in the case
	where the original process is subject to a drift towards the boundary $\auxY$.
	To establish our main result we will require the following related hypothesis:
	\begin{quote}
	    \mycom{hyp:H1}{(H1)} Let $q:=1-p$.
	    Then, for all $1<m<\auxY$ it holds that
	    $q\pi_{m-1}\leq p \pi_{m+1}$.
	\end{quote}
	%\cmk{I am concerned that (H1) might imply that either $p$ is bounded away from $0$ or from $1/2$ and thus (H0) should be weakened.}\dmc{Not sure. Imagine that the stationary distribution goes up by a huge factor in every layer towards the boundary (properly normalized, it can be anything I think. Then you only have $(1-p)/p \le F$ for some huge factor $F$ depending on $\auxY$. So you only get $p \ge (1-p) /F$ which is not necessarily bounded from $0$ uniformly. This is possible, right? Also, it does not imply $p < 1/2$, you could have a strong drift alternating with zero drift, since here you compare $k'$ with $k'-2$ always, and the condition is satisfied, but (H0) not.}
	\cml{There is no intuition about (H1)?}Note however that conditions~\linktomycom{hyp:H0} and~\linktomycom{hyp:H1} do not preclude that the process $\{\auxy_s\}_{s\geq 0}$ stays forever within an interval $(m-1,m]$ (indeed, the said hypotheses only bound $p$, but there might be an interval so that the process might remain within such interval forever)\cml{If this were true the stationary distribution would have mass $0$ elsewhere... I think that the true meaning of the following hypothesis is that we can bound uniformly the expected time independently of $Y$}. This explains why we focus on processes that satisfy the next stated additional hypothesis concerning the maximal time $T_m$ the process stays within said interval, where the maximum is taken over all starting positions $y_0 \in (m-1, m]$:
	\begin{quote}
	   \mycom{hyp:H2}{(H2)} There is a constant $L>0$  (independent of $\auxY$) such that 
	   for all $1\leq m\leq\auxY$ \AL{and $y_0\in(m-1,m]$} it holds that $\AL{\EE_{y_0}}(T_m)\le L$.
	\end{quote}
	This last hypothesis says that the process stays, in expectation, at most constant time within any giving interval $(m-1,m]$.
	
    A last assumption that we make in order to derive this section's main result is the following:
	\begin{quote}
	   \mycom{hyp:H3}{(H3)} There is a constant
	   $\widetilde{C}$, independent of $\auxY$, such that if $\widetilde{C}\le k\le \auxY$, $\ss\geq 1/\pi([0,k])$ and 
	   $\auxy_s \le k$ at some time moment $s\ge 0$, then with constant probability $\auxy_t \le k$ for all $s \le t \le s+1$.%\dmc{are you sure we wanted the hypothesis the way you wrote it below? I suggest it differently.} the process $\{y_s\}_{s \ge 0}$ stays throughout one unit of time within $[0,k]$ with constant probability. 
	\end{quote}
	This last hypothesis says that only in the vicinity of $0$ there might exist an infinite drift (towards $Y$). Once we reach a point that is a constant away from $0$, we might stay at roughly this value for at least one unit of time with constant probability (this is a technical assumption that allows us to focus only on the probability to reach such a value when considering the expected time spent at such a value).
    %\cmk{Every informal explanation of why we need this last hypothesis seems to already be implied by the previous ones. Is that so?}\dmc{It could be for example that the process would always jump between the two last layers, since (H1) only bounds the sup of the drift. Then it would satisfy all hypothesis before but never reach anything up there, no? I don't think (H3) is implied by previous hypotheses. On the other hand, I thought that having (H3) at hand, might imply (H2), no? You start close to $Y$ with proba $1-o(1)$, but you exit this quickly because of (H3), but theoretically you could stay for a long time in a layer close to $0$ (smaller than $\widetilde{C}$ with probability 1, so perhaps it does not imply it. Neither (H0) nor (H1) help it seems }
	
	Now, we are ready to define the process $\mathfrak{D}$: set the initial position of $\mathfrak{D}$ as $\auxy_0^{\mathfrak{D}} := \lfloor \auxy_0 \rfloor$. \mk{Next,  if there is a time instant $s$ such that the process changes from being in $[0,1]$ to being in $(1,2]$, or changes from being in $(m, m+1]$ to  either $(m-1,m]$ or $(m+1,m+2]$ for some~$m\neq 1$}, then the process~$\mathfrak{D}$ does the following:  letting $\auxy_s^{\mathfrak{D}}=m'$ before time $s$ (with $m'\le\auxY$, and possibly $m'$ being a negative integer), with probability~$p_{}$ jump to $m'-1$, and with probability $1-p_{}$, jump to $m'+1$ (in the case $m'<\auxY$), or jump to $\auxY-1$ with probability~$p_{}$, and stay at $\auxY$ with probability $1-p_{}$ (in the case $m'=\auxY$). 


	We claim in the following lemma that the time $T^{\mathfrak{D}}_{\ss}(m)$ spent by $\mathfrak{D}$ inside $B_O(m)$ during the period of time $[0,\ss]$ stochastically dominates the time $T_{\ss}(m)$ the original process spent inside $B_O(m)$.

	\begin{lemma}\label{auxiliarydomination}
	If $\ss \ge 0$ and $1\le m \le \auxY$, then $T_{\ss}(m) \preccurlyeq T^{\mathfrak{D}}_{\ss}(m)$. 
	\end{lemma}
	\begin{proof}
	It suffices to show that one can couple the two processes such that at each time $0 \le s \le \ss$, the radial coordinate $\auxy_s^{\mathfrak{D}}$ satisfies $\auxy_s^{\mathfrak{D}} \preccurlyeq \lfloor \auxy_s \rfloor$. We prove, by induction, that the statement holds after all time steps $T_1, T_2, \ldots$ corresponding to moments where for some integer $m$, the radius of the original process exits the interval $(m{-}1, m]$. To this end, suppose then that this holds inductively until time $T_{i-1}$, and we want to show that it holds still at time $T_i$. By the inductive hypothesis, at time $s$ right before time $T_i$, assuming $\auxy_s\neq 0$, there are integers $m'\le m$ such that $\auxy_s \in (m, m+1]$ and $m':=\auxy_s^{\mathfrak{D}}$. Note that the probability that~$\mathfrak{D}$ jumps to $m'-1$ is always $p_{}$, independently of $m'$. On the other hand, the probability that the process $\{\auxy_s\}_{s\ge 0}$ after time $T_i$ belongs to $(m-1, m]$ is at most $p_{}$. %\dmc{Since this is now general, need to take out this: "in fact, this probability corresponds to the drift of the real process when being at the boundary $\auxY$; if the real process is closer to the origin, it has even stronger drift towards $\auxY$."} 
	Hence, the probability that the process is in $(m-1, m]$ after time $T_i$ is bounded from above by the probability that~$\mathfrak{D}$ jumps to $m'-1$, and by the obvious coupling of the probabilities also for $s=T_i$, we have $\auxy_s^{\mathfrak{D}} \le \auxy_s$. 
	If $\auxy_s=0$, by the inductive hypothesis $\auxy_s^{\mathfrak{D}}\le 0$. 
	\end{proof}
	
When $p(\{\auxy_s\}_{s\geq 0})<1/2$, the process $\{\auxy_s\}_{s\geq 0}$ is subject to a drift towards the boundary $\auxY$, 
so intuitively the process $\mathfrak{D}$ will also hit $\auxY$ in a number of steps 
	that is proportional to $\auxY-\auxy_0^{\mathfrak{D}}$ (the initial distance of the process $\mathfrak{D}$ to the boundary
	$\auxY$) and dependent on the intensity of the drift. Moreover, one should expect that the probability of the said number of steps exceeding its expected value by much decreases rapidly. The next result is a quantitative version of this intuition.
	
	\begin{lemma}\label{lem:auxdfrac}
	Let $\{\auxy_s\}_{s\geq 0}$ be a diffusion process on $[0,\auxY]$ and
	suppose that $\mathfrak{D}$ is such that $\auxy_0^{\mathfrak{D}} \ge m$ for some $m \in \mathbb{N}$. Let $p:=p(\{\auxy_s\}_{s\ge 0})$ and assume \linktomycom{hyp:H0} holds. Then, for any $0< \delta <\frac{1}{2p_{}}-1$ and all $C\geq (1-2(1+\delta)p_{})^{-1}$, with probability at least $1-\exp(-\frac13\delta^2p_{}\tau)$, the process
	$\mathfrak{D}$ hits $\auxY$ in at most $\tau=\tau(m):=\lfloor C(\auxY-m)\rfloor$ steps.
	\end{lemma}
	\begin{proof}
	Denote by $U$ the random variable counting the number of time steps up to step $\tau$ where $\mathfrak{D}$ decreases its value (that is, jumps from some integer $m'$ to $m'-1$). 
	If the process~$\mathfrak{D}$ does not hit $\auxY$ during $\tau$ steps, then it 
	must have decreased for $U$ steps and increased during $\tau-U$ steps 
	(so $\auxy^{\mathfrak{D}}_\tau=\tau-2U+\auxy^{\mathfrak{D}}_0$), and moreover $\auxy^{\mathfrak{D}}_\tau$
	would have to be smaller than~$\auxY$.
	Thus, it will suffice to bound from above the probability that $\tau-2U=\auxy^{\mathfrak{D}}_\tau-\auxy^{\mathfrak{D}}_0< \auxY-m$.
	Since $\EE(U)=p_{}\tau$ and by Chernoff bounds (see~\cite[Theorem 4.4(2)]{mu17}), for any $0 <\delta < 1$,
    \[%\label{Chernoffbd}
    \PP(U \ge (1+\delta)\EE(U)) \le e^{-\frac13\delta^2\EE(U)},
    \]
    the claim follows observing that $\frac12(C-1)(\auxY-m)\geq (1+\delta)\EE(U)$ (by hypothesis on $\delta$ and~$C$) and $\tau-2U<\auxY-m$ if and only if $U > \tfrac12(C-1)(\auxY-m)$.
	\end{proof}

	Next, we show that, with high probability over the choice of the starting position, the boundary $\auxY$ is hit quickly:

\begin{lemma}\label{lem:coupling}
	Let $\{\auxy_s\}_{s\geq 0}$ be a diffusion process on 
	$[0,\auxY]$ with $\pi(\cdot)$ its stationary distribution.
	Assume \linktomycom{hyp:H0} and \linktomycom{hyp:H2} hold.
	Then, there is a $C'>0$ such that if $0<k\le \auxY$, then
	the original process has not hit $\auxY$ 
	by time $C'\log(1/\pi([0,k]))$ with probability at most $3\pi([0,k])$.
\end{lemma}
\begin{proof}
Define the event $\mathcal{E}_1:=\{y_0\in [0,k]\}$ and observe that
\begin{equation}\label{mixed:lower:E1Bnd}
\PP_{\pi}(\mathcal{E}_1) = \pi([0, k]).
\end{equation}
Next, fix $0 <\delta \leq 1$, $p:=p(\{\auxy_s\}_{s\geq 0})$, and let $C':=C'(\delta)\geq 3/(\delta^2p)$ 
%\dmc{isn't the second term always larger?}
be a constant which is at least as large as the 
constant $C$ of Lemma~\ref{lem:auxdfrac} (this last result is applicable because~\linktomycom{hyp:H0} holds).
Define $\tau:=\lceil C'\log(1/\pi([0,k]))\rceil$ and
let~$\mathcal{E}_2$ be the event that the process $\mathfrak{D}$ defined before Lemma~\ref{auxiliarydomination} does not hit $\auxY$ during $\tau$ steps.
Conditioned on $\overline{\mathcal{E}}_1$ (so $y_0^{\mathfrak{D}}\geq k$ since $y_0^{\mathfrak{D}}>y_0-1>k-1$), by our choice of~$C'$ and Lemma~\ref{lem:auxdfrac} with $m:=k$,
\begin{equation}\label{mixed:lower:E2Bnd}
\PP_{\pi}(\mathcal{E}_2 \mid \overline{\mathcal{E}}_1) 
\leq \exp\Big(-\frac13\delta^2p_{}\tau\Big) 
\leq \pi([0,k]).
\end{equation}
Next, since \linktomycom{hyp:H2} holds, for the random variable $T_m$ and the constant $L$ therein, by Markov's inequality, with probability at most $1/\rho<1$, the original process exits $(m-1,m]$ before time $\rho L$. Formally, for any integer $1\leq m\leq \auxY$ and $y_0\in (m-1,m]$, we have $\PP_{\auxy_0}(T_m\ge \rho L) \leq 1/\rho$.
Furthermore, after time $\rho L$ has passed, independently of the past, 
 the probability to stay an additional amount of time $\rho L$ within $(m-1,m]$, is again at most $1/\rho$.
Hence, \cml{I think talking about the $\tau$ steps here is out of place, we are focusing on showing that the probability of spending too much time within each interval decreases exponentially first, no?} the total time the process $\mathfrak{D}$ takes during the $\tau$ steps  is bounded from above by the sum of $\tau$ positive i.i.d.~random variables $Z_1, \ldots, Z_{\tau}$, all with expectation $L_{}$ (where $Z_i$ describes the worst-case behavior of the original process with respect to $(m_i,m_i-1]$ for some $m_i$). 
Since we may split a time period of length $k$ into $\lfloor k/(\rho L)\rfloor$ intervals of length $L$ (we throw away the remaining non-integer parts), and for each such interval~$\PP_{y_0}(T_{m_i}\geq \rho L)\leq 1/\rho$ holds independently of the initial position,
for each $Z_i$ and any $k$, we have 
\begin{equation}\label{obs:markov2}
  \P(Z_i \ge k) \le e^{-\lfloor k/(\rho L) \rfloor}.  
\end{equation}\cml{I am a bit confused.. from the preceding argument it is clear that $\lfloor Z/\rho L\rfloor$ is upper bounded by a geometric random variable. Wouldn't this simplify what comes next?}
We now apply Lemma~\ref{bentkus} to $Z:=Z_1+\ldots+Z_{\tau}$: first, for each $1 \le i \le \tau$, we have $\EE(Z_i) \le L_{}$, and $Z_i \preccurlyeq Y^{[L_{}]}$, where $Y^{[L_{}]}$ is a random variable with $\EE(Y^{[L_{}]})=L_{}$, defined so that $\P(Y^{[L_{}]} \ge x)=\P(Z_i \ge x)$ for all $x \ge b$ for some value of $b > 0$ depending on $L_{}$ only: this can be achieved by setting $\P(Y^{[L_{}]}=0):=\P(Z_i=0)=0$ (the equality follows since $Z_i$ describes the worst-case behavior of the original process which is continuous), and by setting $b$ in such a way that $\P(Y^{[L_{}]}=b)=1-\int_{x \ge b} d\PP_{Z_i}(x)$ satisfies $b (1-\int_{x \ge b} d\PP_{Z_i}(x)) + \int_{x \ge b}  x d\PP_{Z_i}(x)=L_{}$ (clearly the left hand side is increasing in $b$; if $\EE(Z_i)=L_{}$, then $b=0$ is the only solution to this, otherwise there exists exactly one solution $b \le L_{}$ by continuity of $Z_i$.) Setting $T:=\sum_{i=1}^{\tau} Y^{[L_{}]}_i$, where the $Y^{[L_{}]}_i$ are i.i.d~copies of $Y^{[L_{}]}$, by Lemma~\ref{bentkus},  we get
$$
\P(Z \ge 12 \rho L_{}\tau) \le \inf_{h \le 12 \rho L_{}\tau} e^{-12h\rho L_{}\tau}\EE(e^{hT}).
$$
By independence of the $Y^{[L_{}]}_i$'s we have 
$$
\EE(e^{hT})=\big(\EE(e^{hY^{[L_{}]}})\big)^{\tau} = \Big(\int_{0}^{\infty} e^{hx}d\PP(Y^{[L_{}]})(x)\Big)^{\tau}.
$$
Letting $h:=1/(4\rho L)$
%\cmk{Not clear why we have to choose $h$ this way.}\dmc{A bit arbitrary, you can change it} 
and assuming $12\rho L\tau$ is an integer,
%\cmk{Not so nice. Maybe polish later}\dmc{You want to carry along ceilings all the time? 12 could be replaced by something slightly larger than 12 chosen so that this number is integer, no?}
by~\eqref{obs:markov2} we get that the integral inside the parenthesis above is at most  
\[
 \sum_{i\geq 0}e^{h(i+1)\rho L_{}}\P(i\rho L_{}\le Z \le (i+1) \rho L_{})
 \leq 
 \sum_{i \ge 0} e^{h(i+1)\rho L_{}} e^{-i}
 \leq e^{1/4}+ \sum_{i \ge 1} e^{-i/2}
 \leq 4.
\]
Hence, from the three preceding displayed inequalities it follows that
\begin{align*}
\P(Z \ge 12 \rho L_{}\tau) &
\le e^{-12 h\rho L_{}\tau} 4^{\tau} 
= \big(4/e^{3}\big)^{\tau}
\leq e^{-\tau}.
\end{align*}
Let $\mathcal{E}_3$ be the event that the original process has not hit $\auxY$ by time 
$12\rho L_{}\tau$. Since $\delta,p_{}\leq 1$, by definition of $C'$, we have $C'>1$, so it follows by definition of $\tau$ that $\tau\geq \log(1/\pi([0,k]))$, and thus our preceding discussion establishes that
\[
\P_{\pi}(\mathcal{E}_3 \mid \overline{\mathcal{E}}_1 \cap \overline{\mathcal{E}}_2 ) \le \pi([0,k]).
\]
The lemma follows by a union bound over events $\mathcal{E}_1$, $\mathcal{E}_2$, $\mathcal{E}_3$.
\end{proof}


We now use the previous lemmata to show this section's main result:
\begin{proposition}\label{prop:coupling}
Let $\{\auxy_s\}_{s\geq 0}$ be a diffusion process on $[0,\auxY]$ with stationary distribution $\pi(\cdot)$. Assume~\linktomycom{hyp:H0},\linktomycom{hyp:H1}, \linktomycom{hyp:H2} and~\linktomycom{hyp:H3} hold.
Then, there are constants $\widetilde{C}>0$, $\widetilde{c}\in (0,1)$, $\widetilde{\eta}> 0$ \cml{independent of $Y$, no?} such that for all 
 $\widetilde{C} \le k\leq \auxY$ and $\ss\geq 1/\pi([0,k])$ we have  
\[
\PP_\pi\big(I_k\geq \widetilde{c} \ss\pi([0,k])\big) \geq \widetilde{\eta},
	\qquad\text{where $I_k := \int_0^\ss {\bf 1}_{[0,k]}(\auxy_s)ds$.}
\]
\end{proposition}
\begin{proof}
First, we establish that there is an integer $\Delta>0$ \cml{depending on $p$ alone} such that
\begin{equation}\label{lower:eqn:existDelta}
\pi([0,k-\Delta])\leq\frac{1}{12}\pi([0,k]) \qquad \text{for all $1\leq k\leq\auxY$.}
\end{equation}
The claim is a consequence of~\linktomycom{hyp:H0} and~\linktomycom{hyp:H1}. 
Indeed, let $p, q, k'$\cml{k'?} be as in~\linktomycom{hyp:H1}. Summing 
over $k'$ with $1<k'<k-1$ we get $q\pi([0,k-2])\leq p\pi([0,k])$ and thus
$\pi([0,k-2m])\leq (p/q)^m\pi([0,k])$ for any $m\in\NN$.
By \linktomycom{hyp:H0} we know that $p/q$ is bounded away from $1$ by a constant independent of $\auxY$. 
Taking $m$ large enough so that $(p/q)^m\leq 1/12$ the claim follows setting $\Delta:=2m$. 


Next, we will split the time period $[0,\ss]$ into time intervals of one unit (throwing away the eventual remaining non-integer part): for the $i$-th time interval let $X_i$ be the indicator random variable being~$1$ if at time instant $i-1$ (that is, at the beginning of the $i$-th time interval) the process $\{y_s\}_{s \ge 0}$ is within $[0,k]$, and $0$ otherwise. Since $\pi$ is stationary,  $\P(X_i=1)=\pi([0,k])$ for any $i$. Thus, setting $X:=\sum_{i=1}^{\lfloor \ss \rfloor} X_i$, we have $(\ss-1)\P(X_1=1)<\EE(X)\leq \ss\P(X_1=1) \le 2\EE(X)$.
Since~\linktomycom{hyp:H3} holds, for $\widetilde{C}$ the constant therein, if $X_i=1$ and $k\ge \widetilde{C}$, then with constant probability the process $\{y_s\}_{s \ge 0}$ stays throughout the entire $i$-th time interval within $[0,k]$.
So by our previous discussion, to establish the desired
result, it suffices to show that for some $\xi>0$  the probability that $X$ exceeds\cml{we want it to exceed, no? Maybe fall behind} $\xi\EE(X)$ is at most a constant strictly less than $1$. We rely on Chebyshev's inequality to do so and thus need to bound from above the variance of $X$. This explains why we now undertake the determination of an upper bound for $\EE(X^2)$.
Since there are at most $P_d \le \ss$ pairs of random variables $X_i$ and $X_j$ such that $|i-j|=d$,
$$
\EE(X^2)=\sum_{i,j} \P(X_i=1,X_j=1) \leq \sum_{d=0}^{\lfloor \ss \rfloor} P_d\P(X_1=1, X_{1+d}=1) 
$$
\cml{here you are using stationarity no?} In order to compute $\P(X_1=1, X_{1+d}=1)$ observe that, by Lemma~\ref{auxiliarydomination}, for any (possibly negative) integer $m \le \auxY$, the time $T_{\ss}^{\mathfrak{D}}(m)$ spent by $\mathfrak{D}$ inside $B_O(m)$ stochastically dominates the time $T_{\ss}(m)$ the original process spent inside $B_O(m)$, that is, $T_{\ss}(m) \preccurlyeq T_{\ss}^{\mathfrak{D}}(m)$.\cml{below you talk about trajectories of the $D$ process? or trajectories of the original process?. If it is the original process I would talk about $T_{\ss}^{\mathfrak{D}}(m)$ later}\cml{ Also the construction is confusing...wouldn't the probability be conditioned on hitting the boundary? How do you get rid of this conditioning later?}
If between time instants $0$ and $d-1$ the original process hits the boundary at least once, $\P(X_{1+d}=1 \mid X_{1}=1) \le \P(X_{1+d}=1)$. 
%\cmk{Changed $r_s, \widehat{r}_s$ by $\auxy_s,\widehat{y}_s$ in many places below. Also changed $R$ to $\auxY$.}\dmc{Ok, but in the next paragraph, I changed your $\auxy_t$ by  $\widetilde{\auxy}_t$. It is not the original process of the statement, no?} 
Indeed, let $\{\widehat{\auxy}_t\}_{t \ge 0}$ 
%\dmc{put brackets around the previous} 
be the trajectory yielding $X_{1+d}=1$, without conditioning on $X_1=1$, and let $\{\widetilde{\auxy}_t\}_{t \ge 0}$ be the trajectory conditional under $X_1=1$: if there exists a time instant $s \in [0, d-1]$ with $\widetilde{\auxy}_s=\auxY$ and we had $\widetilde{\auxy}_0 \le \widehat{\auxy}_0$, then there must have been a time $s' \in [0,d-1]$ with $\widetilde{\auxy}_{s'}=\widehat{\auxy}_{s'}$, and the processes \replaced[id=al]{can be}{are} naturally coupled from then on; conditional and unconditional probabilities are the same in this case. If, still under the assumption of $\widetilde{\auxy}_s=\auxY$ for some $s \in [0,d-1]$, we had $\widehat{\auxy}_0 < \widetilde{\auxy}_0$, the condition $X_1=1$ can only decrease the probability to obtain $X_{1+d}=1$ \cml{why?}. Combining both cases we have the desired inequality. 

Next, for some sufficiently large constant $C'$ as needed in the statement of Lemma~\ref{lem:coupling}, set  $\tau:=\lceil C'\log(1/\pi([0,k-\Delta]))\rceil$ where $\Delta$ is a positive integer satisfying~\eqref{lower:eqn:existDelta}. Observe that for $d \ge \tau$, by Lemma~\ref{lem:coupling}, the process $\{y_s\}_{s \ge 0}$ has hit the boundary $\auxY$ by time $d$ with probability at least $1-3\pi([0,k-\Delta])$. Thus, for $d \ge \tau_{}$,%\dmc{the second term on the right hand side below needs an extra $\P(X_1=1)$, no?}
$$
\P(X_{1}=1, X_{1+d}=1) \le 3\pi([0,k-\Delta])\P(X_1=1)+\big(\P(X_1=1)\big)^2.
$$
Assume then $d < \tau_{}$. In this case, if the boundary $\auxY$ is hit before time $d-1$, by the argument above, the joint probability can be bounded from above by the product. Otherwise, by Lemma~\ref{lem:coupling} with $d-1$ playing the role of $C'\log(1/\pi([0,k-\Delta]))$, there exist universal constants $c_1, c_2 > 0$ so that the process $\mathfrak{D}$ performs with probability at least $1-e^{-c_1d}$ at least $c_2 d$ steps during the time interval $[0, d-1]$. In particular, $c_1$ and $c_2$ can be chosen to depend only on $C'$ in Lemma~\ref{lem:coupling}, but not on $\widetilde{C}$.
We first assume that this is indeed the case.
%observe that by~\eqref{obs:Markov}, each step takes  more than $e L_{}$ time with probability at most $1/e$. Splitting the total time into smaller time amounts as in the argument yielding to~\eqref{obs:markov2} and~\eqref{obs:Bentkus}, for $d$ being at least a large constant $C^*$, there exist universal constants $c_3, c_4 > 0$ such that with probability at least $1-e^{-\frac12 c_3 d}$ \dm{the process $\{y_s\}_{s \ge 0}$} performs at least $c_4 d$ steps (i.e., changes at least $c_4 d$ times the layer\cmk{What is a layer? Why not just say $\mathfrak{D}$ performs at least $c_4d$ steps}) during the time interval $[0,d-1]$. 
Now, by the domination of the process $\mathfrak{D}$ explained above, we have
$$
\P(X_1=1, X_{1+d}=1) \le \P(\auxy_0^{\mathfrak{D}} \le k, \auxy_{d}^{\mathfrak{D}} \le k).
$$
Conditional under $\mathfrak{D}$ having made at least $c_4 d$ steps before time $d-1$, and recalling that $p_{} < 1/2$ (by hypothesis) is the probability that $\mathfrak{D}$ makes a step away from the boundary of $(-\infty,\auxY]$, recalling that $q:=1-p$, we have for some large enough constant $C_1 > 0$ 
%\dmc{changed $c_3$ to $c_4$ below several times}
$$
 \P(\auxy_{d}^{\mathfrak{D}} = k \mid \auxy_0^{\mathfrak{D}}=k) \le \sum_{\ell: 2\ell \ge c_2 d} \binom{2\ell}{\ell}(p_{}q)^{\ell}\le \sum_{\ell:2\ell \ge c_2 d} (4p_{}q)^{\ell} \le C_1(4p_{}q)^{c_2 d/2}.
$$
The same upper bound holds for  $\P(\auxy_{d}^{\mathfrak{D}} = k{-}1 \mid \auxy_0^{\mathfrak{D}}=k)$. For  $\P(\auxy_{d}^{\mathfrak{D}} = k{-}2 \mid \auxy_0^{\mathfrak{D}}=k)$ and  $\P(\auxy_{d}^{\mathfrak{D}} = k{-}3 \mid \auxy_0^{\mathfrak{D}}=k)$ note that one has to have one more step away and one less step towards the boundary of $(-\infty,Y]$, yielding an upper bound of 
$C_1(4p_{}q)^{c_2 d/2} \frac{p_{}}{q}$. In the same way, 
$\P(\auxy_{d}^{\mathfrak{D}} = k{-}2i \mid \auxy_0^{\mathfrak{D}}=k) \le  C_1(4p_{}q)^{c_2 d/2} (\frac{p_{}}{q})^i$, so that for $C_2:=C_2(C_1) > 0$ large enough,
$\P(\auxy_{d}^{\mathfrak{D}} \le k \mid \auxy_0^{\mathfrak{D}}=k) \le C_2 (4p_{}q)^{c_2 d/2}$.
We can get the same upper bound also for  $\P(\auxy_{d}^{\mathfrak{D}} \le k \mid \auxy_0^{\mathfrak{D}}=k{-}1)$. 
Moreover, 
%For $\P(\auxy_{d}^{\mathfrak{D}} \le k \mid \auxy_0^{\mathfrak{D}}=k{-}2)$, 
we have
\begin{align*}
\P(\auxy_{d}^{\mathfrak{D}} = k \mid \auxy_0^{\mathfrak{D}}=k{-}2) &\le \sum_{\ell: 2\ell \ge c_2 d} 
\binom{2\ell}{\ell-1}p_{}^{\ell{-}1}q^{\ell+1} 
\le \frac{q}{p} \sum_{\ell: 2\ell \ge c_2 d} \binom{2\ell}{\ell}(p_{}q)^{\ell} 
\le \frac{q}{p}C_1(4p_{}q)^{\frac{c_2 d}{2}}.
\end{align*}
By the argument from before,  we get
 $\P(\auxy_{d}^{\mathfrak{D}} \le k \mid \auxy_0^{\mathfrak{D}}=k{-}2) \le (q/p)C_2 (4p_{}q)^{\frac{c_2 d}{2}}$
 and iterating the argument, also obtain
 $
 \P(\auxy_{d}^{\mathfrak{D}} \le k \mid \auxy_0^{\mathfrak{D}} = k{-}2i) \le (q/p)^i C_2 (4p_{}q)^{\frac{c_2 d}{2}}.
 $
 Hence, if $k$ is even, 
 \begin{align*}
 \P(\auxy_{d}^{\mathfrak{D}} \le k, \auxy_0^{\mathfrak{D}} \le k)
% & \leq \sum_{j=0}^k\PP(\auxy_d^{\mathfrak{D}} \le k \mid \auxy_0^{\mathfrak{D}}=j)\PP(\auxy_0^{\mathfrak{D}}=j) \\
 & \leq \PP(y_0^{\mathfrak{D}}=0)+2\sum_{i=1}^{k/2}\PP(\auxy_d^{\mathfrak{D}} \le k \mid \auxy_0^{\mathfrak{D}}=k-2i)\PP(\auxy_0^{\mathfrak{D}}=k-2i) \\
& \leq \pi([0,1])+ 2\sum_{i=0}^{k/2}\Big(\frac{q}{p}\Big)^i C_2 (4p_{}q)^{\frac{c_2 d}{2}}\pi([k{-}2i{-}1, k{-}2i])  \\
& \le \pi([0,1])+2C_3(4p_{}q)^{\frac{c_2 d}{2}} \pi([k-1,k]) \\
& \le (1+2C_3(4p_{}q)^{\frac{c_2 d}{2}})\pi([0,k]),
 \end{align*}
\cml{so in your computations you only need for $\auxy_0^{\mathfrak{D}}$ to be initially distributed according to $\pi$, no?} where for the secondlast inequality we used that~\linktomycom{hyp:H1} holds by hypothesis, and for the last inequality we used that $\pi([0,1]),\pi([k-1,k])\leq\pi([0,k])$.
If $k$ is odd, then take $\{1,...,\frac12(k-1)\}$ as the range over which the index of the summations above vary and replace $\PP(\auxy_0^{\mathfrak{D}}=0), \pi([0,1])$ by $\PP(\auxy_0^{\mathfrak{D}}=k)$ and $\pi([k-1,k])$, respectively. Using now that $\pi([k-1,k])\leq\pi([0,k])$ 
we obtain the same bound as for $k$ even.
Thus, combining the cases 
of $d < \tau_{}$, and $d \ge \tau_{}$,
%recalling that  $\P(X_1=1)=\pi([0,k])= \frac{\cosh(\alpha k)-1}{\cosh(\alpha\auxY)-1}\ge 2e^{-\alpha\auxY}(\cosh(\alpha k)-1) \ge 0.99e^{-\alpha(\auxY-k)}$ since by assumption $k \ge \widetilde{C}$ with $\widetilde{C}$ large, %\cmk{Why is this needed? I don't see it being used.}\dmc{I think I agree, I don't need $\P(X_1=1)\le e^{-\alpha(\auxY-k)}$}
  and recalling that $P_d\leq\ss$ denotes the number of pairs of random variables $X_i$ and $X_j$ for which $|i-j|=d$, we get %\dmc{added the extra term $e^{-c_3d}$ in the second term} 
%\dmc{removed $\ss$ before the sum $d \ge \tau_{}$ appearing. I think it was wrong}
\begin{align*}
\EE(X^2) &\le  \ss \sum_{d < \tau_{}} \big(C_3(4p_{}q)^{\frac{c_2 d}{2}}+e^{-c_1d}\big)\P(X_1=1)
\\&
\qquad + \sum_{d \geq \tau_{}}P_d \big(3\pi([0,k{-}\Delta])\P(X_1=1) + (\P(X_1=1))^2\big) \\
&\le C^{*}\EE(X) + \sum_{d \ge \tau_{}} P_d \big( \frac{1}{4}(\P(X_1=1))^2+ (\P(X_1=1))^2 \big) \\
& \le C^{*}\EE(X)+\frac54 (\EE(X))^2 \le \frac43 (\EE(X))^2,
\end{align*}
where for the secondlast inequality we used~\eqref{lower:eqn:existDelta} and that $\sum_{d \le \tau_{}} (C_3(4p_{}(1-p_{}))^{c_2 d/2} +e^{-c_1d})\le C^{*}$
for some $C^{*}$ depending on $c_1$ and $c_2$ only, 
and for the last inequality we used that $\EE(X) \ge C_4$ for some constant $C_4$ large by our hypothesis $\ss\ge 1/\pi([0,k])$.
Hence, $\Var(X)\le \frac13 (\EE(X))^2$. Thus, by Chebyshev's inequality,
$$
\P(|X-\EE(X)| \ge \tfrac34 \EE(X)) \le \Var(X)/(\tfrac34\EE(X))^2 \le \tfrac{16}{27},
$$
and the statement follows taking $\widetilde{\eta}:=\frac{16}{27}$.
\end{proof}

%\cmk{If we agree on the previous write-up, then here I would include a Proposition showing that the process $\{\auxy_s\}_{s\ge 0}$ satisfies \linktomycom{hyp:H0}, \linktomycom{hyp:H1}, \linktomycom{hyp:H2} and \linktomycom{hyp:H3}.}\dmc{I agree up to these smaller comments I made. Please add some Corollary here that the hypotheses are satisfied}


To conclude this section, we show that the process we studied
in Sections~\ref{sec:upper_rad} and~\ref{sec:lower_rad}
satisfies~\linktomycom{hyp:H0} through~\linktomycom{hyp:H3} and hence the conclusion of Proposition~\ref{prop:coupling}  holds for the said process. Reaching such conclusion was the motivation for this section, and it provides a result on which the analysis of the combined radial and angular processes relies.
\begin{corollary}\label{radial:cor:coupling}
Let $\{\auxy_s\}_{s\geq 0}$ be the diffusion process on $(0,\auxY]$ with
generator $\Delta_h$ and a reflecting barrier at $\auxY$. 
Then, there are constants $\widetilde{C}>0$, $\widetilde{c},\widetilde{\eta}\in (0,1)$, such that for all 
$\widetilde{C}\le k\le \auxY$ and $\ss\geq 1/\pi((0,k])$ we have  
\[
\PP_\pi\big(I_k\geq \widetilde{c} \ss\pi((0,k])\big) \geq \widetilde{\eta},
	\qquad\text{where $I_k := \int_0^\ss {\bf 1}_{(0,k]}(\auxy_s)ds$.}
\]
\end{corollary}
\begin{proof}
%Since the stationary distribution of the process $\{\auxy_s\}_{s\geq 0}$ is given by $\pi(\auxy):=\frac{\cosh(\alpha\auxy)-1}{\cosh(\alpha \auxY)-1}$ for $\auxy\in [0,\auxY]$, we have  $\pi([0,k])\geq e^{-\alpha(\auxY-k)}(1-2e^{-\alpha k})\ge\frac12 e^{-\alpha(\auxY-k)}$ provided we take $\widetilde{C}\geq \frac{2}{\alpha}\log(2)$. \dmc{do we need this previous calculation on the lower bound of the stationary distribution later for H3, so perhaps leave it. You had this in mind, right? It is not needed at this point.}
It will be enough to verify that~\linktomycom{hyp:H0}-\linktomycom{hyp:H3} hold and apply
Proposition~\ref{prop:coupling} with $\widetilde{C}$ as therein and satisfying our just stated condition.

Let $C_{\alpha,\auxY}:=(\cosh(\alpha\auxY)-1)^{-1}$. By definition $\pi_m:=\pi((m{-}1,m])=C_{\alpha,\auxY}(\cosh(\alpha m)-\cosh(\alpha(m{-}1)))$.
Using $\cosh x-\cosh y=2\sinh(\frac12(x+y))\sinh(\frac12(x-y))$, we get
\begin{equation}\label{radial:eqn:piSinh}
\pi_{m}=2C_{\alpha,\auxY}\sinh(\alpha(m-\tfrac12))\sinh(\tfrac12\alpha) \qquad\text{for all $1\le m\le \auxY$}.
\end{equation}
Next, let $p_1,...,p_{\auxY}$ be defined as in this subsection's introduction and let $q_i:=1-p_i$.
In our case, since the drift of the process goes to infinity when approaching $0$ we have $p_1=0$.
%\dmc{If you opt for more generality above then just say here we choose $p_1=0$}.
We claim that $q_{m-1}\pi_{m-1}=p_m\pi_m$ for $1< m\le \auxY$.
Indeed, by~\eqref{radial:def:pim}, we have that $\pi_1=p_1\pi_1+p_2\pi_2$, so $q_1\pi_1=p_2\pi_2$. Assume the claim holds for $1\le m<\auxY$. By~\eqref{radial:def:pim} and the inductive hypothesis, we have $p_{m+1}\pi_{m+1}=\pi_m-q_{m-1}\pi_{m-1}=\pi_m-p_{m}\pi_{m}=q_{m}\pi_{m}$, which concludes the inductive proof of our claim.
 
We first establish that~\linktomycom{hyp:H1} holds.
By definition of $p$, we have $p_m\le p$ and $q_m\ge q$ for all $1\le m\le\auxY$. Hence, by the preceding paragraph's claim, for $1<m<\auxY$ we have $q\pi_{m-1}\leq q_{m-1}\pi_{m-1}=p_{m}\pi_{m}\le p\pi_{m+1}$ where the last inequality follows because $\pi_m$ is non-decreasing as a function of $m$. 

To show that $p$ is bounded away from $0$ as required by~\linktomycom{hyp:H0}, it suffices
to observe by the claim two paragraphs above, 
$p/q\ge p_2/q_1=\pi_1/\pi_2=\sinh(\frac12\alpha)/\sinh(\frac32\alpha)$. To establish that $p$ is also bounded from above by a constant strictly smaller than $1/2$, note that by definition of $q_i$, we have $q_{i}\pi_{i}=\pi_{i}-p_{i}\pi_{i}$, so from our claim from two paragraphs above, $p_{m}\pi_{m}=q_{m-1}\pi_{m-1}=\pi_{m-1}-p_{m-2}\pi_{m-2}
=\pi_{m-1}-\pi_{m-2}+p_{m-3}\pi_{m-3}$. Hence, $p_m\pi_m\le\pi_{m-1}-\pi_{m-2}+\pi_{m-3}$ for $3<m\le\auxY$. It is easy to see that the right-hand side is, as a function in $\alpha$, maximized for $\alpha=1/2$. Moreover, for every $\alpha$, it is increasing in $m$, we see it is bounded from above by $0.4618$. By direct calculation, $p_2=\pi_1/\pi_2=\sinh(\alpha/2) / \sinh(3\alpha/2) \le 0.3072$ and $p_3=(1-p_2)\pi_2/\pi_3 \le 0.3557$, where we used that both expressions for $p_2$ and $p_3$ are decreasing as functions in $\alpha$.%\cmk{The monotonicity claims are not that straightforward for me, but its ok.}\dmc{Feel free to add more if you like. That $\alpha=1/2$ is the largest value is intuitive clear and easy to check, for the claim wrt to $m$ one needs to compute the derivative}
%\[
%p_{m} \le \frac{1}{\sinh(\alpha(m-\frac12))}\big(\sinh(\alpha(m-\tfrac32))
%-\sinh(\alpha(m-\tfrac52))+\sinh(\alpha(m-\tfrac72)\big).
%\]
%Since $\sinh(x-\delta)\leq e^{-\delta}\sinh(x)$ for all $\delta>0$ and 
%$\sinh(x)\leq e^x/2$ we conclude that for $3<m\le\auxY$,
%\[
%p_m
%\le e^{-\alpha}-(e^{-2\alpha}{-}e^{-\alpha(2m-3)})+e^{-3\alpha}
%\le e^{-\alpha}-e^{-2\alpha}+e^{-3\alpha}+e^{-5\alpha}
%= (1{+}e^{\alpha})^{-1}+e^{-4\alpha}.
%\]
We conclude that $p_m\leq 0.4618$ for all $1<m\le\auxY$ and thus $p\le 0.4618$ as well.
%$p_m \leq e^{-\alpha/2}/(2\cosh(\frac12\alpha))=(1+e^\alpha)^{-1}$. 
%from~\eqref{radial:def:pim}, we have $p_2\pi_2=\pi_1-p_1\pi_1$, $p_3\pi_3=\pi_2-q_1\pi_1=\pi_2-\pi_1+p_1\pi_1$, and in general, for all $1<m\le\auxY$, 
%\[
%p_m\pi_m = \sum_{j=0}^{m-2}(-1)^j\pi_{m-j-1}+(-1)^{m-1}p_1\pi_1
%= \sum_{j=0}^{m-3}(-1)^j\pi_{m-j-1}+(-1)^{m-2}q_1\pi_1.
%\]
%dmc{the following is not needed I think. From the first paragraph we know that $p_i$ is increasing, and independently of whether m is even or odd we get always $p_R \le (\pi_{R-1}-\pi_{R-2}+\pi_{R-3})/\pi_R < 0.4618$} 
%Recalling that $2\sinh(x)=e^x-e^{-x}$ and 
%using the formula for the sum of a geometric sequence, after some manipulation one obtains that for $a,b\in\NN$, $b\ge a\geq 1$,
%\begin{align*}
%\sum_{j=0}^{a-1}(-1)^j\pi_{b-j} 
%& = 2C_{\alpha,\auxY}\sinh(\tfrac12\alpha)\sum_{j=0}^{a-1}(-1)^{j}\sinh(\alpha(b{-}j{-}\tfrac12)) \\
%& = C_{\alpha,\auxY}\frac{\sinh(\tfrac12\alpha)}{\cosh(\tfrac12\alpha)}\big(\sinh(\alpha b)-(-1)^a\sinh(\alpha(b-a))\big).
%\end{align*}
%So, when $m$ is even, from the two preceding displayed equations, taking $a:=m-1$ and $b:=m-1$ in the latter one, we deduce that
%\[
%p_m\sinh(\alpha(m{-}\tfrac12))\leq
%\frac{\sinh(\alpha(m{-}1))}{2\cosh(\tfrac12\alpha)}.
%\]
%Since $\sinh(x-\delta)\leq e^{-\delta}\sinh(x)$ for all $\delta>0$, we conclude that when $m$ is even $p_m \leq e^{-\alpha/2}/(2\cosh(\frac12\alpha))=(1+e^\alpha)^{-1}$. 


Next, we establish~\linktomycom{hyp:H2}.
Let $T(m)$ be the random variable counting the maximal time the process spends in the interval $(m-1,m]$, where the maximum is taken over all starting positions within $(m-1, m]$; note that for $m=\auxY$, by Part~\eqref{radial:itm:phi2} of Lemma~\ref{lemmaradial} applied with $\yabs_0:=\auxY-1$, for any $\auxy_0 \in [\auxY-1,\auxY]$, we get $\EE_{\auxy_0}(T(\auxY)) \le \frac{4}{\alpha^2}e^{\alpha}$,  and for any $1 \le m < \auxY$, this upper bound holds as well (indeed, for an upper bound we may always assume a reflecting barrier at $m$, since this only increases the time spent in $(m-1,m]$: as long as the value of $m$ is not hit, the processes are equal, and if $m$ is hit, the real process has left the interval, whereas the reflected process has not).

Finally, to check that~\linktomycom{hyp:H3} holds, simply observe that 
for all $\widetilde{C}\le k\le \auxY$ (where $\widetilde{C}$ is at least a constant strictly greater than $1$), the process $\{\auxy_s\}_{s\ge 0}$ dominates a process of constant drift towards $\auxY$, which wherever conditioned on $\auxy_s\le k$ has a constant probability of staying within $[0,k]$ during the unit length time interval $[s,s+1]$, thus establishing the claim and concluding the proof of the corollary.
\end{proof}
\end{comment}