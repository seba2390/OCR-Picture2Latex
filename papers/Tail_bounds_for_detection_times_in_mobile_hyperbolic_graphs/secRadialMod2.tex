
\subsection{Time spent within an interval containing the origin}
%
In this section we consider a class of one-dimensional diffusion processes taking place in $[0,\auxY]$ with $\auxY$ a positive integer  %, and with generator $\Delta_h$ 
	%with a reflecting barrier at a positive integer value $\auxY$ 
and with given stationary  distribution.
	Our study is motivated by the process $\{\auxy_s\}_{s\geq 0}$ in $(0,\auxY]$ with generator~$\Delta_h$
	(see~\eqref{radialgenerator}) and reflecting barrier at $\auxY$. This section's main result
	will later in the paper be applied precisely to this latter process. However, the arguments of this section are applicable to less constrained  diffusion processes and might even be further
	generalized to a larger class of processes, and so we have opted for an exposition which, instead
	of tailoring the proof arguments to the specific 
	process with generator $\Delta_h$, gives the conditions that need to be satisfied so that
	our results as a corollary also apply to our specific process.
 
   
    Our goal is to formalize a rather intuitive fact. Specifically, assume
    $\ss$ is not a too small amount of time and that 
    $\{\auxy_s\}_{s\geq 0}$ is a diffusion process with stationary distribution~$\pi(\cdot)$ on $[0,\auxY]$. Roughly speaking, we show that under some general hypotheses, during a time interval of length $\ss$, when starting from the stationary distribution,  with constant probability the process spends within $(0,k]$ ($k$ larger than a constant) a period of time proportional to the expected time to spend there, that is, proportional to $\ss\pi((0,k])$. We remark that $k$ only needs to be larger than a sufficiently large constant $C$, which does not depend on $Y$ (otherwise the result would follow directly from Markov's inequality), and $k$ does not depend on $\pi$ either.%\cml{I think it would be more precise to state that the constant also does not depend on $\pi$ right?}\dmc{ok?}\cml{great!}
    
    
%    Our goal  is to formalize a rather intuitive fact. Roughly speaking, to show that under some general hypotheses, during a not too small time period of length $\ss$, with constant probability, a diffusion process~$\{\auxy_s\}_{s\geq 0}$ with stationary distribution $\pi(\cdot)$ on $[0,\auxY]$ spends  within the interval \replaced[id=mk]{$(0,k]$}{$[0,k]$} ($k$ larger than a constant) a period of time proportional to the expected one the process spends in \replaced[id=mk]{$(0,k]$}{$[0,k]$} during a period of time of length $\ss$ when starting according to the stationary distribution, that is, proportional to $\ss\pi(\replaced[id=mk]{(0,k]}{[0,k]})$. We remark that $k$ only needs to be larger than a sufficiently large constant $C$, which does not depend on $Y$ (otherwise the result would follow directly from Markov's inequality).

	
	We start by establishing two lemmas. The first one states that a diffusion process $\{\auxy_s\}_{s\geq 0}$ in $[0,\auxY]$ with stationary distribution $\pi(\cdot)$,
    henceforth referred to as the \emph{original process}, can be coupled with a process $\mathfrak{D}$ that is continuous in time but discretized in space with values in $\mathbb{Z}$ and defined next.
    First, let $\pi_m:=\pi((m-1,m])$ where $1\le m\le \auxY$. 
    Let $p'_1=0$ (this restriction could be relaxed, but we impose it for the sake of simplicity of presentation), and let $p'_2,\ldots, p'_{\auxY}$ be the unique solution to the following system of equations:
    \begin{equation}\label{radial:def:pim}
    \pi_{m} = 
    \begin{cases} 
    p'_{m}\pi_{m}+p'_{m+1}\pi_{m+1}, & \text{if $m=1$,} \\    
    (1-p'_{m-1})\pi_{m-1}+p'_{m+1}\pi_{m+1}, & \text{if $1<m<\auxY$,} \\
    (1-p'_{m-1})\pi_{m-1}+(1-p'_{m})\pi_{m}, & \text{if $m=\auxY$.} 
    \end{cases}
    \end{equation}
    Intuitively, one can think of the $p'_m$ as the probability that the process
    hits $m-1$ before it hits $m$ when starting from the stationary distribution conditioned on starting in the interval~$(m-1,m]$. %\dmc{I put it like this, is it okay?}\cmk{In fact, I believe $p'_m$'s are exactly what you say, not only "intuitively".}\dmc{Marcos, I could not get it out exactly, so if you can see it exactly then i remove it..}\cmk{Lets see. We can discuss at some point.}\cml{I couldn't do it either, but maybe it is enough to remove the statement}
    %\dmc{as Amitai suggested, $p'$ itself is no more needed}
%    \dm{Furthermore, let $p':=p'(\{\auxy_s\}_{s\geq 0})$ where 
 %   \[
%	p' := \sup_{m\in\NN: 1 \le m\le\auxY} p'_m,
%	\]
	Denoting next, for $0 \le m < \auxY$, by
	$\widehat{p}_m$ the probability that the original process starting at $m$ hits  $m-1$ before hitting $m+1$ (observe that $\widehat{p}_0=0$). %\dmc{no more halflayer}
	%and by $\widehat{p}_{\auxY}$ the probability that the original process starting at $\auxY-\frac{1}{2}$  hits $\auxY-1$ before hitting $\auxY$, \cml{I found some issues and inaccuracies. There are easy fixes but we need to talk Dieter} 
	We then define
	$$
	p:=\sup\{\sup_{m\in \NN: 1 \le m\le\auxY} p'_m, \sup_{m \in \NN: 0 \le m < \auxY} \widehat{p}_m\},
	$$
	and let $q:=1-p$.
	 We are exclusively interested in processes for which the following holds:
	\begin{quote}
	    \mycom{hyp:H1}{(H1)} 
	    $p \in [\underline{c},\overline{c}]$, $0<\underline{c}\leq\overline{c}<1/2$, 
	    for constants $\underline{c},\overline{c}>0$ 
	    that do not depend on $\auxY$. 
	\end{quote}
	In other words, we are interested in the case
	where the original process is subject to a drift towards the boundary $\auxY$.
	
	Note however that condition~\linktomycom{hyp:H1}  does not preclude that the process $\{\auxy_s\}_{s\geq 0}$, being at position $\auxy$ (for some real $0 \le \auxy \le \auxY$), in expectation stays within an interval $(y-1,y+1] \cap [0,\auxY]$ an amount of time that increases with $\auxY$. This explains why we focus on processes that satisfy the next stated additional hypothesis concerning the maximal time $T_{\auxy\pm 1}$   the process stays within said interval:
	\begin{quote}
	   \mycom{hyp:H2}{(H2)} There is a constant $L>0$  (independent of $\auxY$) such that 
	   for all reals $0\leq \auxy \le \auxY$ it holds that 
	   $\EE_{y}(T_{\auxy\pm 1})\le L$.
	\end{quote}
	The preceding hypothesis says that the process stays, when starting at position $y$, in expectation, at most constant time within any giving interval $(y-1,y+1]$.
	A last assumption that we make in order to derive this section's main result is the following: 
	\begin{quote}
	   \mycom{hyp:H3}{(H3)} There is a constant
	   $\widetilde{C}$, independent of $\auxY$, such that if $\widetilde{C}\le k\le \auxY$, $\ss\geq 1/\pi((0,k])$ and 
	   $\auxy_s \le k$ at some time moment $s\ge 0$, then with constant probability $\auxy_t \le k$ for all $s \le t \le s+1$.%\dmc{are you sure we wanted the hypothesis the way you wrote it below? I suggest it differently.} the process $\{y_s\}_{s \ge 0}$ stays throughout one unit of time within $[0,k]$ with constant probability. 
	\end{quote}
	The preceding hypothesis says that only in the vicinity of $0$ there might exist an infinite drift (towards $Y$). Once we reach a point that is a constant away from $0$, we might stay at roughly this value for at least one unit of time with constant probability (this is a technical assumption that allows us to focus only on the probability to reach such a value when considering the expected time spent roughly there.
    %\cmk{Every informal explanation of why we need this last hypothesis seems to already be implied by the previous ones. Is that so?}\dmc{It could be for example that the process would always jump between the two last layers, since (H1) only bounds the sup of the drift. Then it would satisfy all hypothesis before but never reach anything up there, no? I don't think (H3) is implied by previous hypotheses. On the other hand, I thought that having (H3) at hand, might imply (H2), no? You start close to $Y$ with proba $1-o(1)$, but you exit this quickly because of (H3), but theoretically you could stay for a long time in a layer close to $0$ (smaller than $\widetilde{C}$ with probability 1, so perhaps it does not imply it. Neither (H0) nor (H1) help it seems }
	
	Now, we are ready to define the process $\mathfrak{D}$: set the initial position of $\mathfrak{D}$ as $\auxy_0^{\mathfrak{D}} := \lfloor \auxy_0 \rfloor$. Suppose the original process starts in an interval $(m-1,m]$ for some integer $1 \le m \le \auxY$.  If the original process hits $m$ before hitting $m-1$, the original process stays put at $m-1$ and the \emph{coupling phase} defined below starts.  If the original process hits $m-1$ before hitting~$m$, then $\mathfrak{D}$ jumps to $m-2$ and the coupling phase starts as well. In the coupling phase now iteratively the following happens: suppose the original process is initially at integer $m$, for some $1 \le m < \auxY$, and~$\mathfrak{D}$ is at some integer $m'$ (if $m=\auxY$, directly go to the \emph{move-independent phase} defined below). Mark~$m$ as the center of the current phase, and do the following: As long as the original process neither hits $m-1$ nor $m+1$, $\mathfrak{D}$ stays put. If the original process hits $m-1$, then~$\mathfrak{D}$ jumps to $m'-1$ (note that this happens with probability $\widehat{p}_m \le p$. If the original process hits $m+1$, then~$\mathfrak{D}$ jumps to $m'-1$ with probability $\frac{p-\widehat{p}_m}{\widehat{q}_m},$ and with probability $1-\frac{p-\widehat{p}_m}{\widehat{q}_m},$ $\mathfrak{D}$ jumps to $m'+1$. Note that the total probability that $\mathfrak{D}$ jumps to $m'-1$ is exactly $p$. Moreover, if $m+1=\auxY$, the move-independent phase starts. In either case, $m$ is unmarked as the center of the current phase, and the new location of the original process (either $m-1$ or $m+1$, respectively; note that $m-1=0$ could also happen, this is treated in the same way), is marked as the new center, and the next iteration of the coupling phase (move-independent phase, respectively) with this new location playing the role of $m$. 
	
	In the move-independent phase \emph{only the instants} of movements of the original process and $\mathfrak{D}$ are coupled, but not the movements itself: suppose that at the beginning of this phase the original process is at position $m$ for some integer $0 \le m \le \auxY$, whereas $\mathfrak{D}$ is at some integer $m'$: at the instant when the original process hits $m-1$ or $m+1$, $\mathfrak{D}$ jumps to $m'-1$ with probability $p$ and to $m'+1$ with probability $q$. Then, the next iteration of the independent phase starts with the new location of the original process (that is, either $m-1$ or $m+1$) playing the role of $m$.
	
	\noindent\par
	By construction, it is clear that from the beginning of the coupling phase on, $\mathfrak{D}$ jumps from $m'$ to $m'-1$ with probability~$p$ and from $m'$ to $m'+1$ with probability $1-p$. We also have the following observation that also follows immediately by construction:
	\begin{observation}\label{obs:domination}
	    As long as the original process $\{\auxy_s\}_{s \ge 0}$ does not hit $\auxY$, at every time instant $s$, $\auxy_s^{\mathfrak{D}} \le \auxy_s$.
	\end{observation}


	\begin{comment}
Denote by $T_D$ the stopping time $\mathfrak{D}$ hits $\auxY$ (possibly infinity).
	We claim in the following lemma that the time $T^{\mathfrak{D}}_{\ss}(k)$ spent by $\mathfrak{D}$ inside $(0,k]$ during the period of time $[0,\min\{\ss,T_D\}]$ stochastically dominates the time $T_{\ss}(k)$ the original process spent inside $(0,k]$.

	\begin{lemma}\label{auxiliarydomination}
	If $\ss \ge 0$ and $1\le k \le \auxY$, then $T_{\ss}(k) \preccurlyeq T^{\mathfrak{D}}_{\ss}(k)$. 
	\end{lemma}\cml{This Lemma wont be needed in the end}
	\begin{proof}
	It suffices to show that one can couple the two processes such that at each time $0 \le s \le \ss$, the radial coordinate $\auxy_s^{\mathfrak{D}}$ satisfies $\auxy_s^{\mathfrak{D}} \preccurlyeq \lfloor \auxy_s \rfloor$. It clearly holds before the start of the coupling phase. Let $T_0$ be the beginning of the coupling phase, and note, that by construction, $\auxy_{T_0}^{\mathfrak{D}} = \auxy_{T_0}-1$. Let then $T_1, T_2, \ldots$ be time instants corresponding to moments where the following happens at time $T_i$ for some $i \ge 1$: for some integer $1 \le m < \auxY$, the radius of the original process was exactly $m$ at $T_{i-1}$ and then at time $T_i$ it exits the interval $(m-1,m+1]$ for the first time, or $m=\auxY$ at time $T_{i-1}$, and after having hit $\auxY-\frac12$ the auxiliary process then hits $\auxY$ again (for the first time after having hit $\auxY-\frac12$) or it hits $\auxY-1$ at time $T_i$ for the first time after $T_{i-1}$. We will show by induction that  $\auxy_{T_i}^{\mathfrak{D}} \le \auxy_{T_i}-1$ at any time $T_i$ (as long as $\mathfrak{D}$ does not hit $\auxY$, in which case there is equality at $T_i$, and the process ends), which will imply the statement of the lemma, since between instants $T_{i-1}$ and $T_i$ the original process changes its radial coordinate by at most $1$.  To this end, suppose then that this holds inductively until time $T_{i-1}$, and we want to show that it holds still at time $T_i$. By the inductive hypothesis, there are integers $m'\le m-1$ such that $\auxy_{T_{i-1}} = m$ and $\auxy_{T_{i-1}}^{\mathfrak{D}}=m'$. Note that the probability that~$\mathfrak{D}$ jumps to $m'-1$ at time $T_i$ is always $p$, independently of $m'$. On the other hand, the probability that the process $\{\auxy_s\}_{s\ge 0}$ at time $T_i$ is at most $m-1$  is at most $p$: indeed, by definition, the probability that the original process starting from $m$ hits $m-1$ before hitting $m+1$ (in the case $1 \le m < \auxY$) is $\widehat{p}_m$, and the probability that the original process after having been at $\auxY$ at time $T_{i-1}$ hits first $\auxY-1$ before hitting $\auxY$ again, when already having passed through $\auxY-\frac12$, is $\widehat{p}_{\auxY}$, and both values are smaller than $p$ by definition.
	Hence, in either case, the processes can be coupled in such a way that the auxiliary process has more chance to decrease by one unit, and we then have $\auxy_{T_i}^{\mathfrak{D}} \le \auxy_{T_i}-1$.
	\end{proof}
\end{comment}

	We next show that for $T$ being a sufficiently large constant $C_L$ that depends on $L$ only, conditional under not yet having hit the boundary, $\mathfrak{D}$ is likely to have performed already quite a few steps up to time $T$: 
	\begin{lemma}\label{geometricdominationlemma}
Assume~\linktomycom{hyp:H2} holds. Let $C_L$ be a large enough constant depending on $L$ only with $L$ as in~\linktomycom{hyp:H2}. %\dmc{changed the quantifiers. I think I can fix $c_1$ and then choose $c_2$ in the ineq below: check please} 
For every constant $c_1$, there exists a constant $c_2 > 0$, depending only on~$c_1$ and $L$, such that for any integer $T \ge C_L$, with probability at least $1-e^{c_1 T}$, at least $c_2 T$ steps are performed by $\mathfrak{D}$ up to time $T$.
	\end{lemma}
	\begin{proof} %\dmc{changed reals $m$ below to $y$. Even though it is a starting position, I put $y$ instead of $y_0$} 
	Fix any real $0 \le y \le \auxY$, and recall that  $T_{y\pm 1}$ denotes the time the original process remains in the interval $(y-1,y+1] \cap [0,\auxY]$. By~\linktomycom{hyp:H2} and Markov's inequality, with probability at most $1/2$, the original process exits $(y-1,y+1]$ before time $2 L$. Formally, for all $0 \le y \le \auxY$, we have $\PP_{y}(T_{y \pm 1}\ge 2 L) \leq 1/2$. 
Since this bound is uniform over the starting position $y$ we can restart the process every $2 L$ units of time, thus obtaining
\begin{equation}\label{eq.geometric}
    \P(T_{y\pm 1}\geq 2kL)\leq 2^{-k},
\end{equation} for every integer $k\ge 1$, regardless of $y$. Hence, if at the beginning of an iteration of the coupling phase the original process is at some integer $0 \le m \le \auxY$, the time it takes $\mathfrak{D}$ to wait until its next step from that time moment on, is bounded in the same way as $T_{m\pm 1}$, and the time for the first step to enter the coupling phase can be bounded in the same way as in~\eqref{eq.geometric}. Now, for $i\ge 1$, let $T^{(i)}$ denote the time spent between the instants $T_{i-1}$ and $T_i$, that mark the beginning and end of the $i$-th iteration of the coupling phase (the time before the start of the first iteration of the coupling phase, in case $i=1$, respectively). By~\eqref{eq.geometric}, the random variable~$\lfloor\frac{T^{(i)}}{2 L}\rfloor$ is stochastically bounded from above by a geometric random variable with success probability $\frac{1}{2}$ and support $\{1,2,3,\ldots\}$. The random variables $T^{(1)}, T^{(2)}, \ldots$ are independent but not identically distributed, as the time to remain in some intervals might be longer than in others, but since~\eqref{eq.geometric} holds for all $y$, we may bound $\sum_i \lfloor \frac{T^{(i)}}{2L} \rfloor$ by the sum of  i.i.d.\ geometric random variables with success probability $\frac12$. Hence, for any constant~$c_1$, there is a sufficiently small~$c_2>0$ such that
\[\P\Big(\sum_{i=0}^{c_2 T-1}T^{(i)}> T\Big)\leq \P\Big(\sum_{i=0}^{c_2 T-1}\left\lfloor\frac{T^{(i)}}{2L}\right\rfloor> \frac{T}{3L} \Big)\le \sum_{j=\lfloor \frac{T}{3L} \rfloor+1}^\infty \binom{j-1}{c_2 T -1}2^{-j}\leq e^{-c_1 T},\]
where we used that the sum of $c_2 T$ i.i.d.~geometric random variables has a negative binomial distribution, and where $c_2$ is chosen (depending on $c_1$ and $L$ only) so that the last inequality holds.
Hence, with probability at least $1-e^{c_1 T}$, at least $c_2 T$ steps are performed by $\mathfrak{D}$  during time $T$, and the lemma follows.
	\end{proof}
	From the previous lemma we have the following immediate corollary:
	\begin{corollary}\label{cor:geomdomination}
	Assume~\linktomycom{hyp:H2} holds. Let $C_L$ be a large enough constant depending on $L$ only with $L$ as in \linktomycom{hyp:H2}. Let $C'_L$ be a sufficiently large constant depending on~$L$ only and let $s \ge C'_L$ denote the number of steps performed by $\mathfrak{D}$. For every constant $c'_1 > 0$, there exists $C'_1$ depending only on $L$ and on $c'_1$, such that with probability at least $1-e^{c'_1 s}$, the time elapsed for the $s$ steps is at most~$C'_1 s$.
	\end{corollary}
	
When $p(\{\auxy_s\}_{s\geq 0})<1/2$, the process $\{\auxy_s\}_{s\geq 0}$ is subject to a drift towards the boundary $\auxY$, 
so intuitively the process $\mathfrak{D}$ will also hit $\auxY$ in a number of steps 
	that is proportional to $\auxY-\auxy_0^{\mathfrak{D}}$ (the initial distance of the process $\mathfrak{D}$ to the boundary
	$\auxY$) and dependent on the intensity of the drift. Moreover, one should expect that the probability of the said number of steps exceeding its expected value by much decreases rapidly. The next result is a quantitative version of this intuition.
	
	\begin{lemma}\label{lem:auxdfrac}
	Let $\{\auxy_s\}_{s\geq 0}$ be a diffusion process on $[0,\auxY]$ and
	suppose that $\mathfrak{D}$ is such that $\auxy_0^{\mathfrak{D}} \ge m$ for some $m \in \mathbb{N}$. Let $p:=p(\{\auxy_s\}_{s\ge 0})$ and assume \linktomycom{hyp:H1} holds. Then, for any $0< \delta <\frac{1}{2p_{}}-1$ and all $C\geq (1-2(1+\delta)p_{})^{-1}$, with probability at least $1-\exp(-\frac13\delta^2p_{}\tau)$, the process
	$\mathfrak{D}$ hits $\auxY$ in at most $\tau=\tau(m):=\lfloor C(\auxY-m)\rfloor$ steps.
	\end{lemma}
	\begin{proof}
	Denote by $U$ the random variable counting the number of time steps up to step $\tau$ where $\mathfrak{D}$ decreases its value (that is, jumps from some integer $m'$ to $m'-1$). 
	If the process~$\mathfrak{D}$ does not hit $\auxY$ during $\tau$ steps, then it 
	must have decreased for $U$ steps and increased during $\tau-U$ steps 
	(so $\auxy^{\mathfrak{D}}_\tau=\tau-2U+\auxy^{\mathfrak{D}}_0$), and moreover $\auxy^{\mathfrak{D}}_\tau$
	would have to be smaller than~$\auxY$.
	Thus, it will suffice to bound from above the probability that $\tau-2U=\auxy^{\mathfrak{D}}_\tau-\auxy^{\mathfrak{D}}_0< \auxY-m$.
	Since $\EE(U)=p_{}\tau$ and by Chernoff bounds (see~\cite[Theorem 4.4(2)]{mu17}), for any $0 <\delta < 1$,
    \[%\label{Chernoffbd}
    \PP(U \ge (1+\delta)\EE(U)) \le e^{-\frac13\delta^2\EE(U)},
    \]
    the claim follows observing that $\frac12(C-1)(\auxY-m)\geq (1+\delta)\EE(U)$ (by hypothesis on $\delta$ and~$C$) and $\tau-2U<\auxY-m$ if and only if $U > \tfrac12(C-1)(\auxY-m)$.
	\end{proof}

	Next, we show that, with high probability over the choice of the starting position, the boundary $\auxY$ is hit quickly:

\begin{lemma}\label{lem:coupling}
	Let $\{\auxy_s\}_{s\geq 0}$ be a diffusion process on 
	$[0,\auxY]$ with $\pi(\cdot)$ its stationary distribution.
	Assume \linktomycom{hyp:H1} and \linktomycom{hyp:H2} hold.
	Then, there is a %\dmc{changed this constant, it is not $C'$ as below in the definition of $\tau$} 
	$\widehat{C} >0$ such that if $0<k\le \auxY$, then
	the original process has not hit $\auxY$
	by time $\widehat{C}\log(1/\pi((0,k]))$ with probability at most $3\pi([0,k])$.
\end{lemma}
\begin{proof}
Define the event $\mathcal{E}_1:=\{y_0\in (0,k]\}$ and observe that
\begin{equation*}\label{mixed:lower:E1Bnd}
\PP_{\pi}(\mathcal{E}_1) = \pi((0, k]).
\end{equation*}
Next, fix $0 <\delta \leq 1$, $p$ as before, and let $C':=C'(\delta)\geq 3/(\delta^2p)$ 
%\dmc{isn't the second term always larger?}
be a constant which is at least as large as the 
constant $C$ of Lemma~\ref{lem:auxdfrac} (this last result is applicable because~\linktomycom{hyp:H1} holds).
Define $\tau:=\lceil C'\log(1/\pi((0,k]))\rceil$ and
let~$\mathcal{E}_2$ be the event that the process $\mathfrak{D}$ does not hit $\auxY$ during $\tau$ steps.
Conditioned on $\overline{\mathcal{E}}_1$ (so $y_0^{\mathfrak{D}}\geq k$ since $y_0^{\mathfrak{D}}>y_0-1>k-1$), by our choice of~$C'$ and Lemma~\ref{lem:auxdfrac} with $m:=k$,
\begin{equation*}\label{mixed:lower:E2Bnd}
\PP_{\pi}(\mathcal{E}_2 \mid \overline{\mathcal{E}}_1) 
\leq \exp\big({-}\tfrac13\delta^2p_{}\tau\big) 
\leq \pi((0,k]).
\end{equation*}
Choosing $C'$ sufficiently large, by Corollary~\ref{cor:geomdomination}, applied with $c'_1:=1$, there exists $C'_1$ large enough, so that with probability at least $1-e^{-\tau}$, the time elapsed for $\tau$ steps is at most $C'_1 \tau$.

\begin{comment}
Next, since \linktomycom{hyp:H2} holds, for the random variable $T_m$ and the constant $L$ therein, by Markov's inequality, with probability at most $1/2<1$, the original process exits $(m-1,m]$ before time $2 L$. Formally, for any integer $1\leq m\leq \auxY$ and $y_0\in (m-1,m]$, we have $\PP_{\auxy_0}(T_m\ge 2 L) \leq 1/2$\cml{I added here a proposal for the argument I was thinking about, I think it is simpler so see if you like it}\dmc{I agree, it is shorter}.

\dmc{I need to adapt this, because the coupling of the auxiliary process is different now}
\AL{Since the above bound is uniform over the starting position we can restart the process every $2 L$ units of time, thus obtaining
\begin{equation}\label{eq.geometric}
    \P(T_m\geq 2k'L)\leq 2^{-k'}
\end{equation} regardless of $m$. Let $T^{(i)}$ denote the time spent by the original process within the $i$-th interval it visits so that $\{T^{(i)}\}_{i\geq0}$ is a sequence of i.i.d.\ random variables. The previous argument shows that each random variable $\lfloor\frac{T^{(i)}}{2 L}\rfloor$ is stochastically bounded from above by a geometric random variable $Z_i$ with success probability $\frac{1}{2}$, and from the independence of the $T^{(i)}$ we can take these random variables to be independent as well. Now, observe that \dmc{isn't the last term $\binom{j-1}{\tau -1}2^{-j}$?}\cml{There is a term $2^{-\tau}$ appearing since in this distribution the term multiplying the binomial coefficient is $(1-p)^jp^{r}$ (the terminology as in the wikipedia page)}\cmk{The last $=$ should be $\leq$. We only have stochastic domination for the $\lceil T^{(i)}/(2L)\rceil$'s}\cml{done}\dmc{True, but still change of binomial coefficients, I think}
\[\P\Big(\sum_{i=0}^{\tau-1}T^{(i)}>24 L\tau\Big)\leq \P\Big(\sum_{i=0}^{\tau-1}\left\lfloor\frac{T^{(i)}}{2L}\right\rfloor>11\tau\Big)\leq\sum_{j=11\tau+1}^\infty\binom{j+\tau-1}{\tau-1}2^{-j-\tau}\leq e^{-\tau}\]
where we used that the sum of $\tau$ i.i.d.~geometric random variables has a negative binomial distribution, and where the last inequality holds for sufficiently large $\tau$.}

\dmc{I think from here on we can remove. Half a page less, I am convinced}
Furthermore, after time $\rho L$ has passed, independently of the past, 
 the probability to stay an additional amount of time $\rho L$ within $(m-1,m]$, is again at most $1/\rho$.
 \dm{
We may split a time period of length $k$ into $\lfloor k/(\rho L)\rfloor$ intervals of length $L$ (we throw away the remaining non-integer parts), and for each such interval~$\PP_{y_0}(T_{m_i}\geq \rho L)\leq 1/\rho$ holds independently of the initial position. Thus, denoting by $Z_i$ the time the original process spends within an interval $(m-1,m]$ without exiting it (and thus the auxiliary process stays put), satisfies thus

\begin{equation}\label{obs:markov2}
  \P(Z_i \ge k) \le e^{-\lfloor k/(\rho L) \rfloor}.  
\end{equation}

 }
Thus, the total time the process $\mathfrak{D}$ takes during $\tau$ steps is bounded from above by the sum of $\tau$ positive i.i.d.~random variables $Z_1, \ldots, Z_{\tau}$, all satisfying~\eqref{obs:markov2}.
We now apply Lemma~\ref{bentkus} to $Z:=Z_1+\ldots+Z_{\tau}$: first, for each $1 \le i \le \tau$, we have $\EE(Z_i) \le L_{}$, and $Z_i \preccurlyeq Y^{[L_{}]}$, where $Y^{[L_{}]}$ is a random variable with $\EE(Y^{[L_{}]})=L_{}$, defined so that $\P(Y^{[L_{}]} \ge x)=\P(Z_i \ge x)$ for all $x \ge b$ for some value of $b > 0$ depending on $L_{}$ only: this can be achieved by setting $\P(Y^{[L_{}]}=0):=\P(Z_i=0)=0$ (the equality follows since $Z_i$ describes the worst-case behavior of the original process which is continuous), and by setting $b$ in such a way that $\P(Y^{[L_{}]}=b)=1-\int_{x \ge b} d\PP_{Z_i}(x)$ satisfies $b (1-\int_{x \ge b} d\PP_{Z_i}(x)) + \int_{x \ge b}  x d\PP_{Z_i}(x)=L_{}$ (clearly the left hand side is increasing in $b$; if $\EE(Z_i)=L_{}$, then $b=0$ is the only solution to this, otherwise there exists exactly one solution $b \le L_{}$ by continuity of $Z_i$.) Setting $T:=\sum_{i=1}^{\tau} Y^{[L_{}]}_i$, where the $Y^{[L_{}]}_i$ are i.i.d~copies of $Y^{[L_{}]}$, by Lemma~\ref{bentkus},  we get
$$
\P(Z \ge 12 \rho L_{}\tau) \le \inf_{h \le 12 \rho L_{}\tau} e^{-12h\rho L_{}\tau}\EE(e^{hT}).
$$
By independence of the $Y^{[L_{}]}_i$'s we have 
$$
\EE(e^{hT})=\big(\EE(e^{hY^{[L_{}]}})\big)^{\tau} = \Big(\int_{0}^{\infty} e^{hx}d\PP(Y^{[L_{}]})(x)\Big)^{\tau}.
$$
Letting $h:=1/(4\rho L)$
%\cmk{Not clear why we have to choose $h$ this way.}\dmc{A bit arbitrary, you can change it} 
and assuming $12\rho L\tau$ is an integer,
%\cmk{Not so nice. Maybe polish later}\dmc{You want to carry along ceilings all the time? 12 could be replaced by something slightly larger than 12 chosen so that this number is integer, no?}
by~\eqref{obs:markov2} we get that the integral inside the parenthesis above is at most  
\[
 \sum_{i\geq 0}e^{h(i+1)\rho L_{}}\P(i\rho L_{}\le Z \le (i+1) \rho L_{})
 \leq 
 \sum_{i \ge 0} e^{h(i+1)\rho L_{}} e^{-i}
 \leq e^{1/4}+ \sum_{i \ge 1} e^{-i/2}
 \leq 4.
\]
Hence, from the three preceding displayed inequalities it follows that
\begin{align*}
\P(Z \ge 12 \rho L_{}\tau) &
\le e^{-12 h\rho L_{}\tau} 4^{\tau} 
= \big(4/e^{3}\big)^{\tau}
\leq e^{-\tau}.
\end{align*}

\dmc{until here you could then delete I think it is ok}
\end{comment}

Let $\mathcal{E}_3$ be the event that the original process has not hit $\auxY$ by time 
$C'_1\tau$. Since $\delta,p_{}\leq 1$, by definition of $C'$, we have $C'>1$, so it follows by definition of $\tau$ that $\tau\geq \log(1/\pi((0,k]))$, and thus our preceding discussion establishes that 
\[
\P_{\pi}(\mathcal{E}_3 \mid \overline{\mathcal{E}}_1, \overline{\mathcal{E}}_2 ) \le \pi((0,k]).
\]
The lemma follows by a union bound over events $\mathcal{E}_1$, $\mathcal{E}_2$, $\mathcal{E}_3$, and by setting $\widehat{C}:=C'C'_1$.
\end{proof}

	In order to establish our main result we next state the following fact: 
	\begin{fact}\label{fact:H1}
	For all integers $1 < m < \auxY$, $q \pi_{m-1} \le p \pi_{m}$.
	\end{fact}
	\begin{proof}
Let $p'_1,...,p'_{\auxY}$ be defined as in this subsection's introduction and let $q'_i:=1-p'_i$, and recall that $p'_1=0$ by assumption.
%\dmc{If you opt for more generality above then just say here we choose $p_1=0$}.
We claim that $q'_{m-1}\pi_{m-1}=p'_m\pi_m$ for $1< m \le \auxY$.
Indeed, by~\eqref{radial:def:pim}, we have that $\pi_1=p'_1\pi_1+p'_2\pi_2$, so $q'_1\pi_1=p'_2\pi_2$. Assume the claim holds for $1\le m<\auxY$. By~\eqref{radial:def:pim} and the inductive hypothesis, we have $p'_{m+1}\pi_{m+1}=\pi_m-q'_{m-1}\pi_{m-1}=\pi_m-p'_{m}\pi_{m}=q'_{m}\pi_{m}$, which concludes the inductive proof of our claim.

By definition of  $p$, we have $p'_m\le p$ and $q'_m\ge q$ for all $1\le m < \auxY$. Hence, by the preceding paragraph's claim, for $1<m<\auxY$ we get $q \pi_{m-1} \le q'_{m-1}\pi_{m-1}=p'_{m}\pi_{m} \le p \pi_m$, and the fact follows.
	\end{proof}
We now use the previous lemmata to show this section's main result:
\begin{proposition}\label{prop:coupling}
Let $\{\auxy_s\}_{s\geq 0}$ be a diffusion process on $[0,\auxY]$ with stationary distribution~$\pi(\cdot)$. Assume~\linktomycom{hyp:H1}, \linktomycom{hyp:H2} and~\linktomycom{hyp:H3} hold.
Then, there are constants $\widetilde{C}>0$, $\widetilde{c}\in (0,1)$, $\widetilde{\eta}> 0$ all independent of $\auxY$ such that for all 
 $\widetilde{C} \le k\leq \auxY$ and $\ss\geq 1/\pi((0,k])$ we have  
\[
\PP_\pi\big(I_k\geq \widetilde{c} \ss\pi((0,k])\big) \geq \widetilde{\eta},
	\qquad\text{where $I_k := \int_0^\ss {\bf 1}_{(0,k]}(\auxy_s)ds$.}
\]
\end{proposition}
\begin{proof}
First, we establish that there is an integer $\Delta>0$ depending on $p$ alone such that
\begin{equation}\label{lower:eqn:existDelta}
\pi((0,k-\Delta])\leq\frac{1}{12}\pi((0,k]) \qquad \text{for all $1\leq k\leq\auxY$.}
\end{equation}
The claim is a consequence of~\linktomycom{hyp:H1} and Fact~\ref{fact:H1}: 
Indeed, let $p, q$ be as therein. For any~$k$, summing 
over $m$ with $1<m\le k$ we get $q\pi((0,k-1])\leq p\pi((0,k])$ and thus
$\pi((0,k-m])\leq (p/q)^m\pi((0,k])$ for any $m\in\NN$.
By~\linktomycom{hyp:H1} we know that $p/q$ is bounded away from $1$ by a constant independent of $\auxY$. 
Taking $m$ large enough so that $(p/q)^m\leq 1/12$ the claim follows setting $\Delta:=m$. 


Next, we will split the time period $[0,\ss]$ into time intervals of one unit (throwing away the eventual remaining non-integer part): for the $i$-th time interval let $X_i$ be the indicator random variable being~$1$ if at time instant $i-1$ (that is, at the beginning of the $i$-th time interval) the process $\{y_s\}_{s \ge 0}$ is within $(0,k]$, and $0$ otherwise. Since $\pi$ is stationary, $\P_{\pi}(X_i=1)=\pi((0,k])$ for any $i$. Thus, setting $X:=\sum_{i=1}^{\lfloor \ss \rfloor} X_i$, we have $(\ss-1)\P_{\pi}(X_1=1)<\EE_{\pi}(X)\leq \ss\P_{\pi}(X_1=1) \le 2\EE_{\pi}(X)$.
Since~\linktomycom{hyp:H3} holds, for $\widetilde{C}$ the constant therein, if $X_i=1$ and $k\ge \widetilde{C}$, then with constant probability the process $\{y_s\}_{s \ge 0}$ stays throughout the entire $i$-th time interval within $(0,k]$.
So by our previous discussion, to establish the desired
result, it suffices to show that for some $\xi>0$  the probability that $X$ is smaller than $\xi\EE_{\pi}(X)$ is at most a constant strictly less than $1$. We rely on Chebyshev's inequality to do so and thus need to bound from above the variance of $X$. This explains why we now undertake the determination of an upper bound for $\EE_{\pi}(X^2)$.
Since there are at most $P_d \le \ss$ pairs of random variables $X_i$ and~$X_j$ such that $|i-j|=d$,
$$
\EE_{\pi}(X^2)=\sum_{i,j} \P_{\pi}(X_i=1,X_j=1) \leq \sum_{d=0}^{\lfloor \ss \rfloor} P_d\P_{\pi}(X_1=1, X_{1+d}=1). 
$$
 %In order to compute $\P_{\pi}(X_1=1, X_{1+d}=1)$ observe that, by Lemma~\ref{auxiliarydomination},
 
 %for any (possibly negative) integer $m \le \auxY$, the time $T_{\ss}^{\mathfrak{D}}(m)$ spent by $\mathfrak{D}$ inside $B_O(m)$ stochastically dominates the time $T_{\ss}(m)$ the original process spent inside $B_O(m)$, that is, $T_{\ss}(m) \preccurlyeq T_{\ss}^{\mathfrak{D}}(m)$.\cml{below you talk about trajectories of the $\mathfrak{D}$ process? or trajectories of the original process?. If it is the original process I would talk about $T_{\ss}^{\mathfrak{D}}(m)$ later}\cmk{I understand we are talking about the original procss. I don't understand your suggestion about $T_{\ss}^{\mathfrak{D}}(m)$.}\cml{If we talk about the original process, why is it $T_{\ss}(m)$ relevant?... this threw me off}\cml{ Also the construction is confusing...wouldn't the probability be conditioned on hitting the boundary? How do you get rid of this conditioning later?}\cml{We need to rewrite this part to make it a bit more formal and specific... the idea is correct but the way it is written is confusing}

In order to bound $\P_{\pi}(X_1=1, X_{1+d}=1)$ we construct two processes $\{\widetilde{\auxy}_s\}_{s\geq0}$ and $\{\widehat{\auxy}_s\}_{s\geq0}$ as follows: Initially, $\widetilde{\auxy}_0$ is sampled with 
    distribution $\frac{\pi(\cdot\cap(0,k])}{\pi((0,k])}$.
    Also, with probability $\pi((0,k])$ we let $\widehat{\auxy}_0=\widetilde{\auxy}_0$, and otherwise we sample $\widehat{\auxy}_0$ with distribution  $\frac{\pi(\cdot\cap(k,\auxY])}{\pi((k,\auxY])}$.
    Then, both processes evolve independently according to the same law as the original process until they first meet. Afterward, they still move as the original process but coupled together.
It follows directly that $\{\widehat{\auxy}_s\}_{s \ge 0}$ is a copy of the original process, while $\{\widetilde{\auxy}_s\}_{s \ge 0}$ is a version of the original process conditioned to start within $(0,k]$. Let $\widetilde{T}_{\auxY}$ be the first time that $\{\widetilde{\auxy}_s\}_{s \ge 0}$ hits $\auxY$, and define $\widetilde{X}_i$ and~$\widehat{X}_i$ analogously to the variables $X_i$ with $\widetilde{y}_s$ and $\widehat{y}_s$ playing the role of $y_s$, respectively. Since $\P_{\pi}(X_1=1, X_{1+d}=1)=\P(\widetilde{X}_{1+d}=1)\pi((0,k])$,
it follows that 
\[\P_{\pi}(X_1=1, X_{1+d}=1)\,=\,\P(\widetilde{X}_{1+d}=1, \widetilde{T}_Y
\leq d)\pi((0,k])+\P(X_1=1, X_{1+d}=1,\widetilde{T}_Y>d).
%\P(\widetilde{X}_{1+d}=1, \widetilde{T}_Y>d)\pi((0,k]).
\]
For the first term on the right-hand side above observe that $\widetilde{\auxy}_0\leq \widehat{\auxy}_0$, hence on the event~$\widetilde{T}_{\auxY}\leq d$ both processes have met before time $d$, and so by definition $\widetilde{X}_{1+d}=\widehat{X}_{1+d}$, yielding
\[
\P(\widetilde{X}_{1+d}=1, \widetilde{T}_Y\leq d)\pi((0,k])\leq \P(\widehat{X}_{1+d}=1)\pi((0,k])=\big(\P_{\pi}(X_1=1)\big)^2.
\]
It remains then to bound the term $\P(X_1=1, X_{1+d}=1,\widetilde{T}_Y>d)$. To do so recall that~$\{\widetilde{\auxy}_s\}_{s \ge 0}$ is a version conditioned to start at $(0,k]$ so calling $T_Y$ the first time $\{\auxy_s\}_{s\ge0}$ hits $Y$, we have
\[\P(X_1=1, X_{1+d}=1,\widetilde{T}_Y>d)=\P_{\pi}(X_{1}=1, X_{1+d}=1, T_Y
>d).\]
First, recall that by Lemma~\ref{geometricdominationlemma}, if $d\ge C_L$ for some large enough constant depending on~$L$ only, for every $c_1 > 0$ there exists $c_2 > 0$, depending only on $c_1$ and on $L$, so that the process~$\mathfrak{D}$ performs at least $c_2 d$ steps during the time interval $[0, d]$  with probability at least~$1-e^{-c_1d}$. Note that~$c_1$ and $c_2$ do not depend on $\widetilde{C}$. Let $\mathcal{E}$ be the event that $\mathfrak{D}$ performed at least $c_2 d$ steps up to time $d$. Now, 
\begin{align*}
    & \P_{\pi}(X_{1}=1, X_{1+d}=1, T_Y
>d) \\ & \qquad \le  \P_{\pi}(X_{1}=1, X_{1+d}=1, T_Y
>d, \mathcal{E}) + \P_{\pi}(X_1=1, \overline{\mathcal{E}}) \\ \qquad
     & \qquad \le \P_{\pi}(X_{1}=1, X_{1+d}=1, T_Y
>d, \mathcal{E}) + e^{-c_1 d}\P_{\pi}(X_1=1).
\end{align*}
 Next, recall that as long as the original process does not hit the boundary, by Observation~\ref{obs:domination}, the auxiliary process satisfies $y_s^{\mathfrak{D}} \le y_s$ for every instant $s$. Formally,
$$ \P_{\pi}(X_{1}=1, X_{1+d}=1, T_Y
>d) \le \P(\auxy_0^{\mathfrak{D}} \le k, \auxy_{d}^{\mathfrak{D}} \le k)=\P_{\pi}(X_1=1)\P(\auxy_d^{\mathfrak{D}} \le k \mid \auxy_0^{\mathfrak{D}} \le k),   
$$
and also 
\begin{align*}
\P_{\pi}(X_{1}=1, X_{1+d}=1, T_Y
>d, \mathcal{E}) &\le
\P(\auxy_0^{\mathfrak{D}} \le k, \auxy_{d}^{\mathfrak{D}} \le k, \mathcal{E}) \\
& \le 
\P_{\pi}(X_1=1)\P(\auxy_d^{\mathfrak{D}} \le k \mid \mathcal{E}, \auxy_0^{\mathfrak{D}} \le k),
\end{align*}
and our goal is thus to bound $\P(\auxy_d^{\mathfrak{D}} \le k \mid \mathcal{E}, \auxy_0^{\mathfrak{D}} \le k)$.
Recalling that $p_{} < 1/2$ (by hypothesis) is the probability that~$\mathfrak{D}$ makes a decreasing step, and $q:=1-p$, we have for some large enough constant $C_1 > 0$ %\dmc{added $\mathcal{E}$ below always in the condition. Also slight changes since no max value for the auxiliary process now}
%\dmc{changed $c_3$ to $c_4$ below several times}
$$
 \P(\auxy_{d}^{\mathfrak{D}} = k \mid \auxy_0^{\mathfrak{D}}=k, \mathcal{E}) \le \sum_{\ell: 2\ell \ge c_2 d} \binom{2\ell}{\ell}(p_{}q)^{\ell}\le \sum_{\ell:2\ell \ge c_2 d} (4p_{}q)^{\ell} \le C_1(4p_{}q)^{c_2 d/2}.
$$
The same upper bound holds for  $\P(\auxy_{d}^{\mathfrak{D}} = k{-}1 \mid \auxy_0^{\mathfrak{D}}=k, \mathcal{E})$. For  $\P(\auxy_{d}^{\mathfrak{D}} = k{-}2 \mid \auxy_0^{\mathfrak{D}}=k, \mathcal{E})$ and  $\P(\auxy_{d}^{\mathfrak{D}} = k{-}3 \mid \auxy_0^{\mathfrak{D}}=k, \mathcal{E})$ note that $\mathfrak{D}$ has to make one more decreasing step and one less increasing step, yielding an upper bound of 
$C_1(4p_{}q)^{c_2 d/2} \frac{p_{}}{q}$. In the same way, 
$\P(\auxy_{d}^{\mathfrak{D}} = k{-}2i \mid \auxy_0^{\mathfrak{D}}=k, \mathcal{E}) \le  C_1(4p_{}q)^{c_2 d/2} (\frac{p_{}}{q})^i$, so that for $C_2:=C_2(C_1) > 0$ large enough,
$\P(\auxy_{d}^{\mathfrak{D}} \le k \mid \auxy_0^{\mathfrak{D}}=k, \mathcal{E}) \le C_2 (4p_{}q)^{c_2 d/2}$.
We can get the same upper bound also for  $\P(\auxy_{d}^{\mathfrak{D}} \le k \mid \auxy_0^{\mathfrak{D}}=k{-}1, \mathcal{E})$. 
Moreover, 
%For $\P(\auxy_{d}^{\mathfrak{D}} \le k \mid \auxy_0^{\mathfrak{D}}=k{-}2)$, 
we have
\begin{align*}
\P(\auxy_{d}^{\mathfrak{D}} = k \mid \auxy_0^{\mathfrak{D}}=k{-}2, \mathcal{E}) &\le \sum_{\ell: 2\ell \ge c_2 d} 
\binom{2\ell}{\ell-1}p_{}^{\ell{-}1}q^{\ell+1} 
\le \frac{q}{p} \sum_{\ell: 2\ell \ge c_2 d} \binom{2\ell}{\ell}(p_{}q)^{\ell} 
\le \frac{q}{p}C_1(4p_{}q)^{\frac{c_2 d}{2}}.
\end{align*}
By the argument from before,  we get
 $\P(\auxy_{d}^{\mathfrak{D}} \le k \mid \auxy_0^{\mathfrak{D}}=k{-}2, \mathcal{E}) \le (q/p)C_2 (4p_{}q)^{\frac{c_2 d}{2}}$
 and iterating the argument, also obtain
 $
 \P(\auxy_{d}^{\mathfrak{D}} \le k \mid \auxy_0^{\mathfrak{D}} = k{-}2i, \mathcal{E}) \le (q/p)^i C_2 (4p_{}q)^{\frac{c_2 d}{2}}.
 $
 Hence, if~$k$ is even, 
 %\cmk{Isn't there a factor $2$ missing in front of the second summation below?}\dmc{Don't think so since the first probability is a bound for 2 layers, and the latter contains 2 layers}
 \begin{align*}
 & \P(\auxy_{d}^{\mathfrak{D}} \le k, \auxy_0^{\mathfrak{D}} \le k \mid \mathcal{E}) \\
 & \quad = \sum_{j=0}^{k}\PP(\auxy_d^{\mathfrak{D}} \le k \mid \auxy_0^{\mathfrak{D}}=k{-}j, \mathcal{E})\PP(\auxy_0^{\mathfrak{D}}=k{-}j)
 \\
  & \quad \le \PP(\auxy_d^{\mathfrak{D}} \le k \mid \auxy_0^{\mathfrak{D}}=0, \mathcal{E})\PP(\auxy_0^{\mathfrak{D}}=0) 
  +\!\!\sum_{i=0}^{k/2-1}\PP(\auxy_d^{\mathfrak{D}} \le k \mid \auxy_0^{\mathfrak{D}}=k{-}2i, \mathcal{E})\PP(\auxy_0^{\mathfrak{D}} \in \{k{-}2i, k{-}2i{-}1\})
 \\
& \quad \leq (q/p)^{k/2}C_2(4pq)^{\frac{c_2d}{2}}\pi((0,1])+2\sum_{i=0}^{k/2-1}\Big(\frac{q}{p}\Big)^i C_2 (4p_{}q)^{\frac{c_2 d}{2}}\pi((k{-}2i{-}2, k{-}2i])  \\
& \quad \le 4C_3(4p_{}q)^{\frac{c_2 d}{2}}\pi((0,k]),
 \end{align*}
 where for the secondlast inequality we used Fact~\ref{fact:H1}.
If $k$ is odd, then take $\{0,...,\frac12(k-1)\}$ as the range over which the second index of the summation (the one over $i$ above); the remaining bounds are still valid, and 
we obtain the same bound as for $k$ even.
Thus, recalling that $P_d\leq\ss$ denotes the number of pairs of random variables $X_i$ and $X_j$ for which $|i-j|=d$, and recalling that $\overline{\mathcal{E}}$ holds with probability at most $e^{-c_1 d}$ (in which case the joint probability is only bounded by the probability of $X_1=1$), and adding also the joint probability $(\P(X_1=1))^2$ in case $\widetilde{T}_Y \le d$, we get %\dmc{added the extra term $e^{-c_3d}$ in the second term} 
%\dmc{removed $\ss$ before the sum $d \ge \tau_{}$ appearing. I think it was wrong}
\begin{align*}
\EE_{\pi}(X^2) &\le \ss C_L \P_{\pi}(X_1=1)+ \sum_{d \ge C_L}^{\lfloor \ss \rfloor} P_d \Big( \big(4C_3(4p_{}q)^{\frac{c_2 d}{2}}+e^{-c_1d}\big)\P_{\pi}(X_1=1)+\big(\P_{\pi}(X_1=1)\big)^2\Big)
\\
& \le C_L\EE_{\pi}(X)+2C^{*}\EE_{\pi}(X)+(\EE_{\pi}(X))^2.% \\
%& \le \frac43 (\EE_{\pi}(X))^2,
\end{align*}
where for the second inequality we used that $P_d \le \ss$ and then that $\ss \P_{\pi}(X_1=1)\le 2\EE_{\pi}(X)$, and that $\sum_{d\ge 0} (4C_3(4p_{}(1-p_{}))^{c_2 d/2} +e^{-c_1d})\le C^{*}$
for some $C^{*}$ depending on $c_1$ and $c_2$ only. %\dmc{Maybe remove this? "(that do not depend on $\widetilde{C}$)". It is already said}, 
Thus, since $\EE_{\pi}(X) \ge C_4$ for some constant $C_4$ large by our hypothesis $\ss\ge 1/\pi((0,k])$, we conclude that  $\EE_{\pi}(X^2)\le \frac43 (\EE_{\pi}(X))^2$, so
$\Var_{\pi}(X)\le \frac13 (\EE_{\pi}(X))^2$.
Thus, by Chebyshev's inequality,
$$
\P(|X-\EE_{\pi}(X)| \ge \tfrac34 \EE_{\pi}(X)) \le \Var_{\pi}(X)/(\tfrac34\EE_{\pi}(X))^2 \le \tfrac{16}{27},
$$
and the statement follows taking $\widetilde{c}:=\frac14$ and $\widetilde{\eta}:=\frac{11}{27}$.
\end{proof}

%\cmk{If we agree on the previous write-up, then here I would include a Proposition showing that the process $\{\auxy_s\}_{s\ge 0}$ satisfies \linktomycom{hyp:H0}, \linktomycom{hyp:H1}, \linktomycom{hyp:H2} and \linktomycom{hyp:H3}.}\dmc{I agree up to these smaller comments I made. Please add some Corollary here that the hypotheses are satisfied}


To conclude this section, we show that the process we studied
in Sections~\ref{sec:upper_rad} and~\ref{sec:lower_rad}
satisfies~\linktomycom{hyp:H1} through~\linktomycom{hyp:H3} and hence Proposition~\ref{prop:coupling}  holds for the said process. Reaching such conclusion was the motivation for this section, and it provides a result on which the analysis of the combined radial and angular processes, which we will treat in the next section, relies.
\begin{corollary}\label{radial:cor:coupling}
Let $\{\auxy_s\}_{s\geq 0}$ be the diffusion process on $(0,\auxY]$ with
generator $\Delta_h$ and a reflecting barrier at $\auxY$. 
Then, there are constants $\widetilde{C}>0$, $\widetilde{c},\widetilde{\eta}\in (0,1)$, such that for all 
$\widetilde{C}\le k\le \auxY$ and $\ss\geq 1/\pi((0,k])$ we have  
\[
\PP_\pi\big(I_k\geq \widetilde{c} \ss\pi((0,k])\big) \geq \widetilde{\eta},
	\qquad\text{where $I_k := \int_0^\ss {\bf 1}_{(0,k]}(\auxy_s)ds$.}
\]
\end{corollary}
\begin{proof}
%Since the stationary distribution of the process $\{\auxy_s\}_{s\geq 0}$ is given by $\pi(\auxy):=\frac{\cosh(\alpha\auxy)-1}{\cosh(\alpha \auxY)-1}$ for $\auxy\in [0,\auxY]$, we have  $\pi([0,k])\geq e^{-\alpha(\auxY-k)}(1-2e^{-\alpha k})\ge\frac12 e^{-\alpha(\auxY-k)}$ provided we take $\widetilde{C}\geq \frac{2}{\alpha}\log(2)$. \dmc{do we need this previous calculation on the lower bound of the stationary distribution later for H3, so perhaps leave it. You had this in mind, right? It is not needed at this point.}
It will be enough to verify that~\linktomycom{hyp:H1}-\linktomycom{hyp:H3} hold and apply
Proposition~\ref{prop:coupling} with $\widetilde{C}$ as therein and satisfying our just stated condition.

Let $C_{\alpha,\auxY}:=(\cosh(\alpha\auxY)-1)^{-1}$. By definition $\pi_m:=\pi((m{-}1,m])=C_{\alpha,\auxY}(\cosh(\alpha m)-\cosh(\alpha(m{-}1)))$.
Using $\cosh x-\cosh y=2\sinh(\frac12(x+y))\sinh(\frac12(x-y))$, we get
\begin{equation}\label{radial:eqn:piSinh}
\pi_{m}=2C_{\alpha,\auxY}\sinh(\alpha(m-\tfrac12))\sinh(\tfrac12\alpha) \qquad\text{for all $1\le m\le \auxY$}.
\end{equation}
To show that $p$ is bounded away from $0$ as required by~\linktomycom{hyp:H1}, it suffices
to observe that by Fact~\ref{fact:H1}, 
$p/q\ge p_2/q_1=\pi_1/\pi_2=\sinh(\frac12\alpha)/\sinh(\frac32\alpha)$. To establish that $p$ is also bounded from above by a constant strictly smaller than $1/2$, recall the definition of $p'_m$ from this subsection's introduction and let $q'_m:=1-p'_m$. Note that by definition of $q'_i$, we have $q'_{i}\pi_{i}=\pi_{i}-p'_{i}\pi_{i}$, so from the proof of Fact~\ref{fact:H1}, $p'_{m}\pi_{m}=q'_{m-1}\pi_{m-1}=\pi_{m-1}-p'_{m-2}\pi_{m-2}
=\pi_{m-1}-\pi_{m-2}+p'_{m-3}\pi_{m-3}$. Hence, $p'_m\pi_m\le\pi_{m-1}-\pi_{m-2}+\pi_{m-3}$ for $3<m\le\auxY$. It is easy to see that the right-hand side is, as a function in $\alpha$, maximized for $\alpha=1/2$. Moreover, for every $\alpha$, it is increasing in $m$, we see it is bounded from above by $0.4618$. By direct calculation, $p'_2=\pi_1/\pi_2=\sinh(\alpha/2) / \sinh(3\alpha/2) \le 0.3072$ and $p'_3=(1-p'_2)\pi_2/\pi_3 \le 0.3557$, where we used that both expressions for $p'_2$ and $p'_3$ are decreasing as functions in $\alpha$. %\cmk{The monotonicity claims are not that straightforward for me, but its ok.}\dmc{Feel free to add more if you like. That $\alpha=1/2$ is the largest value is intuitive clear and easy to check, for the claim wrt to $m$ one needs to compute the derivative}
%\[
%p_{m} \le \frac{1}{\sinh(\alpha(m-\frac12))}\big(\sinh(\alpha(m-\tfrac32))
%-\sinh(\alpha(m-\tfrac52))+\sinh(\alpha(m-\tfrac72)\big).
%\]
%Since $\sinh(x-\delta)\leq e^{-\delta}\sinh(x)$ for all $\delta>0$ and 
%$\sinh(x)\leq e^x/2$ we conclude that for $3<m\le\auxY$,
%\[
%p_m
%\le e^{-\alpha}-(e^{-2\alpha}{-}e^{-\alpha(2m-3)})+e^{-3\alpha}
%\le e^{-\alpha}-e^{-2\alpha}+e^{-3\alpha}+e^{-5\alpha}
%= (1{+}e^{\alpha})^{-1}+e^{-4\alpha}.
%\]
 We conclude that $\sup_{1\le m \le \auxY} p'_m\leq 0.4618$ for all $1<m\le\auxY$. To conclude that $p < 1/2$, we also need to bound $\widehat{p}_m$, for $1 \le m < \auxY$: in order to do so, observe that the diffusion process defined by~\eqref{truegenerator} can be coupled with a process of constant drift towards $m+1$ so that the real process is always to the right of the process of constant drift; for a process with constant drift towards $m+1$ starting at $m$ it is well known that the probability to reach $m-1$ before $m+1$ is at most $1/2-\varepsilon$ for some $\varepsilon > 0$ (see~\cite{Borodin2002}, formula 3.0.4).
 %The same argument applies for the probability to first hit $\auxY-1$ before hitting $\auxY$ when starting at $\auxY-\frac12$.
 Hence $\widehat{p}_m \le \frac12 - \varepsilon$ for any $1 \le m < \auxY$, and hence $p < 1/2$, as desired.
%$p_m \leq e^{-\alpha/2}/(2\cosh(\frac12\alpha))=(1+e^\alpha)^{-1}$. 
%from~\eqref{radial:def:pim}, we have $p_2\pi_2=\pi_1-p_1\pi_1$, $p_3\pi_3=\pi_2-q_1\pi_1=\pi_2-\pi_1+p_1\pi_1$, and in general, for all $1<m\le\auxY$, 
%\[
%p_m\pi_m = \sum_{j=0}^{m-2}(-1)^j\pi_{m-j-1}+(-1)^{m-1}p_1\pi_1
%= \sum_{j=0}^{m-3}(-1)^j\pi_{m-j-1}+(-1)^{m-2}q_1\pi_1.
%\]
%dmc{the following is not needed I think. From the first paragraph we know that $p_i$ is increasing, and independently of whether m is even or odd we get always $p_R \le (\pi_{R-1}-\pi_{R-2}+\pi_{R-3})/\pi_R < 0.4618$} 
%Recalling that $2\sinh(x)=e^x-e^{-x}$ and 
%using the formula for the sum of a geometric sequence, after some manipulation one obtains that for $a,b\in\NN$, $b\ge a\geq 1$,
%\begin{align*}
%\sum_{j=0}^{a-1}(-1)^j\pi_{b-j} 
%& = 2C_{\alpha,\auxY}\sinh(\tfrac12\alpha)\sum_{j=0}^{a-1}(-1)^{j}\sinh(\alpha(b{-}j{-}\tfrac12)) \\
%& = C_{\alpha,\auxY}\frac{\sinh(\tfrac12\alpha)}{\cosh(\tfrac12\alpha)}\big(\sinh(\alpha b)-(-1)^a\sinh(\alpha(b-a))\big).
%\end{align*}
%So, when $m$ is even, from the two preceding displayed equations, taking $a:=m-1$ and $b:=m-1$ in the latter one, we deduce that
%\[
%p_m\sinh(\alpha(m{-}\tfrac12))\leq
%\frac{\sinh(\alpha(m{-}1))}{2\cosh(\tfrac12\alpha)}.
%\]
%Since $\sinh(x-\delta)\leq e^{-\delta}\sinh(x)$ for all $\delta>0$, we conclude that when $m$ is even $p_m \leq e^{-\alpha/2}/(2\cosh(\frac12\alpha))=(1+e^\alpha)^{-1}$. 


Next, we establish~\linktomycom{hyp:H2}. Recalling that $T_{(\auxY-1)\pm 1}$ is the random variable counting the maximal time the process spends in the interval $(\auxY-2,\auxY]$ starting at $\auxY-1$; by Part~\eqref{radial:itm:phi2} of Lemma~\ref{lemmaradial} applied with $\yabs_0:=\auxY-2$, for any $\auxy \in (\auxY-2,\auxY]$, we get $\EE_{\auxy}(T_{\yabs_0}) \le \frac{4}{\alpha^2}e^{2\alpha}$, and since $\sup_{\auxY-2 \le y \le \auxY} \EE_y (T_{y \pm 1}) \le \sup_{\auxY-2 < \auxy \le \auxY} \EE_{\auxy}(T_{\yabs_0})$, conditon~\linktomycom{hyp:H2} is satisfied for $y > \auxY-2$. In order to bound $\EE_y(T_{y\pm 1})$ for $y<\auxY-1$, note that the time to exit from such an interval can only increase when imposing a reflecting barrier at $y+1$, and we can then apply  Part~\eqref{radial:itm:phi2} of Lemma~\ref{lemmaradial} to the process with this reflecting barrier at $y$, applied with~$\yabs_0:=y-1$. Hence, for any starting position $y$, the expected time remaining in the interval~$(y-1,y+1]$ is at most a constant $L$, and~\linktomycom{hyp:H2} is satisfied.


Finally, to check that~\linktomycom{hyp:H3} holds, simply observe that 
for all $\widetilde{C}\le k\le \auxY$ (where $\widetilde{C}$ is at least a constant strictly greater than $1$), the process $\{\auxy_s\}_{s\ge 0}$ dominates a process of constant drift towards $\auxY$, which wherever conditioned on $\auxy_s\le k$ has a constant probability of staying within $(0,k]$ during the unit length time interval $[s,s+1]$, thus establishing the claim and concluding the proof of the corollary.
\end{proof}
