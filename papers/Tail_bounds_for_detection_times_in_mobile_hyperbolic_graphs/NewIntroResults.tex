\emph{Random Geometric Graphs (RGGs)} are a family of spatial networks that have been intensively studied as models of communication networks, in particular sensor networks, see for example~\cite{Akyildiz}.
In this model, an almost surely finite number of vertices is distributed in a metric space according to some 
probability distribution and two vertices are joined by an edge if the distance between them is at most a given parameter called
radius.
Typically, the metric space is the $d$-dimensional unit cube or torus (often with $d=2$) and points are chosen according to a Poisson point process of given intensity. 
While simple, RGGs do capture relevant characteristics of some real world networks, for instance, non-negligible clustering coefficient.
However, RGGs fail to exhibit other important features such as scale-freeness and non-homogeneous vertex degree distribution, both of which are staple features
of a large class of networks loosely referred to as "social networks" that are meant to encompass networks such as the Internet, citation networks, friendship relation among individuals, etc. 

A network model that naturally exhibits clustering and scale-freeness is the \emph{Random Hypebolic Graph (RHG)} model introduced by Krioukov et al.~\cite{KPKVB10} where vertices of the network are points in a bounded region of the hyperbolic plane, and connections exist if their hyperbolic distance is small. In~\cite{BPK10}, a surprisingly good maximum likelihood fit of the hyperbolic model was shown for the embedding of the network corresponding to the autonomous systems of the Internet,
drawing much attention, interest, and follow-up work on the model (see the section on related work below). 
%Furthermore, there is increasing evidence that hyperbolic geometry emerges spontaneously in the process of formation of networks~\cite{Bianconi}.


%In the last decade the hyperbolic model introduced by Krioukov et al.~\cite{KPKVB10} received quite a bit of attention from both the mathematical community as well as from proposed a model of complex networks 
It has been recognized that in many applications of geometric network models the entities represented by vertices are not fixed in space but mobile. One way in which this has been addressed is to assume that the vertices of the network move according to independent Brownian motions, giving rise to what Peres et al. in~\cite{Peres2010}) call the \emph{mobile geometric graph} model.
Mobility, and more generally dynamic models, are even more relevant in the context of social networks. Thus, it is natural to adapt the mobile geometric graph setting to the hyperbolic graph context and assume the vertices of the latter graphs again move according to independent Brownian motions but in hyperbolic space. This gives rise, paraphrasing Peres et al., to the \emph{mobile hyperbolic graph} model. 
We initiate the study of this new model by focusing on the fundamental
problem of \emph{detection}, that is, the time until a fixed (non-mobile) added target vertex becomes non-isolated in the (evolving) hyperbolic graph.
We will do this; but in fact do much more. In order to discuss our contributions in detail we first need to precisely describe the model we introduce and formalize the main problem we address in our study. 

%Brownian motion in the hyperbolic plane is a diffusion process that evolves according to the following generator, expressed in polar coordinates $(r,\theta)\in\RR_+\times\RR$ \cml{We need to argue existence and that the process satisfies ItÃ´'s formula, I added reference~\cite{Skorokhod1961StochasticEF} but more is needed}:
%\begin{equation}\label{hyperbgenerator}
%\Delta_{h} = \frac{1}{2}\frac{\partial^2}{\partial r^2}+\frac{1}{2}\frac{1}{\tanh r}\frac{\partial}{\partial r}+\frac{1}{2}\frac{1}{\sinh^2 r}\frac{\partial^2}{\partial\theta^2}.
%\end{equation}
%We consider particles evolving in hyperbolic space according to this generator (in fact a slightly generalized version, as explained below). Each particle represents an entity of a network. We are interested in modelling social networks and explain relations between entities as manifestations of proximity in hyperbolic space. However, since unconstrained Brownian motion in unbounded geometric spaces escapes from any fixed bounded region in finite time, given our modelling goals, it is natural to restrict the motion of each particle to some bounded region. As in the model of Krioukov et al.~\cite{KPKVB10}, we restrict the movement of particles to the disk $B_O(R)$ of radius $R>0$ centered at the origin $O$ of the hyperbolic plane and consider a reflective barrier at the boundary of $B_{O}(R)$.



\subsection{The mobile hyperbolic graph model}\label{sec:model}
%
We first introduce the model of Krioukov et al.~\cite{KPKVB10} in its Poissonized version (see also~\cite{GPP12} for the same description in the so called uniform model): for each $n \in \mathbb{N}^+$, consider a Poisson point process $\mathcal{P}$ on the hyperbolic disk of radius $R :=2 \log (n/\nu)$ for some positive constant~$\nu \in \mathbb{R}^+$ ($\log$ denotes here and throughout the paper the natural logarithm).
The intensity function $\mu$ at polar coordinates $(r,\theta)$ for 
  $0\leq r\leq R$ and $-\pi \leq \theta < \pi$ is equal to $n f(r,\theta)$, where $f(r,\theta)$ is given by
\begin{align*}
f(r,\theta) & := \begin{cases}
  \displaystyle
 \frac{\alpha \sinh(\alpha r)}{2\pi(\cosh(\alpha R)-1)}, 
  &\text{if $0\leq r\leq R$}, \\[2ex]
  0, & \text{otherwise.}
  \end{cases}
\end{align*}
In other words, the angle and radius are chosen independently; the former uniformly at random in $(-\pi,\pi]$ and the latter with density proportional to $\sinh(\alpha r)$. 
%Note that this choice of $f(r,\theta)$ corresponds to the uniform distribution inside a disk of radius $R$ around the origin in a hyperbolic plane of curvature $-\alpha^2$. 
%\dm{(in fact, $\alpha \sinh(\alpha r)$ is the perimeter of the circle at radius $r$, and $\cosh(\alpha R)-1$ is the area of the ball of radius $R$)}
%\cmk{Wikipedia says that the circumference of a ball of radius $r$ in a hyperbolic space of curvature $-\alpha^2$ is $\frac{2}{\alpha}\pi\sinh(\alpha r)$. There is something strange here}. \dmc{Indeed, I found this formula elsewhere too. Very strange for me... For the intensity function,it should be this one, no? I checked, when integrating the density it gives exactly $\cosh(\alpha R)-1$, so it should be right though?}\cmk{My understanding is that $-\alpha^2$ is not really the curvature. I believe we have always been working in hyperbolic plane of curvature $-1$. If we considered curvature $-\zeta^2$ then the degree distribution of vertices varies according to an exponent that depends on $\alpha/\zeta$ and then the typical setting we look at (i.e., $\frac12<\alpha<1$) corresponds to $\frac12\zeta<\alpha<\zeta$. Also, the formula (1) for calculating distances is correct only for hyperbolic space of curvature $-1$, otherwise it should read $\cosh(\zeta d_H) := \cosh(\zeta r)\cosh (\zeta r')- \sinh(\zeta r)\sinh(\zeta r')\cos( \theta{-}\theta')$.  However, as I mentioned before, my undestanding is that we have always (here and in previous papers) implicitly fixed the curvature of the hyperbolic space to $-1$.}\dmc{That seems to be an explanation of this... need to discuss this in order for me to be convinced}

%\dmc{I put the next paragraph already here, as you suggested, before "Identify then the points..."}
%On an intuitive level, the parameter $\alpha$ calibrates the radial distribution of the points: the smaller $\alpha$ is, the closer the points are to the origin. 
%%(see also below the related work section for the thresholds of the existence of a giant component and connectivity). 
%The parameter $\nu$ controls the intensity of the points: the bigger $\nu$, the smaller the radius $R$, and therefore the bigger the number of points in a smaller area (without changing the radial distribution). 
%%\cmk{Do we really need/want the following phrase? I would remove it.}
%%Therefore if a monotone increasing graph property holds for certain values of $(\alpha, \nu)$, it holds also for $(\alpha, \nu')$ with $\nu' \ge \nu$, and also for $(\alpha',\nu)$ with $\alpha' \le \alpha$. 
%%We use the notation $\nnu$ to denote $n/\nu$ below.
Next, identify the points of the Poisson process with vertices
%(that is, identify a point with polar coordinates $(r_v,\theta_v)$ with vertex $v\in V_n$) 
and define the following graph $G_n:=(V_n,E_n)$ %\dmc{is $G_n$ a good notation? The graphs don't have exactly $n$ vertices... Not sure}\cmk{Its fine for me, $n$ is the expected number of vertices rather than the exact number of vertices.} 
where $V_n:=\mathcal{P}$. For $P, P'\in V_n$, $P \neq P'$, with polar coordinates $(r,\theta)$ and $(r',\theta')$ respectively, there is an edge in $E_n$ with endpoints 
  $P$ and $P'$ provided the hyperbolic distance $d_H$ between $P$ and $P'$ is such that  $d_H\leq R$, where $d_H$ is obtained by solving 
\begin{equation}\label{eqn:coshLaw}
\cosh d_H := \cosh r\cosh r'-
  \sinh r\sinh r'\cos( \theta{-}\theta').
\end{equation}
In particular, note that $\EE(|V_n|)=n$.


%\cmk{In my opinion, everything in this section up to here does not belong in a Main Contribution section.}\dmc{The title is different now}

Henceforth, we denote the point whose radius is $0$ by $O$ and refer to it as the \emph{origin}. For a point $P$ and $r\geq 0$ we let $B_P(r)$ denote the ball centered at $P$ of radius $r$, that is, the set of all points at hyperbolic distance less than $r$ from $P$. %\dmc{(we extend this notation also to negative $r$ in case an auxiliary process is also defined for such $r$ below? Mention it?)}\cmk{I don't see the need to do such a thing.}. 
Also, we henceforth denote the boundary of $B_P(r)$ by $\partial B_P(r)$.

\medskip
To define a dynamic version of Krioukov et al.'s model we consider an initial configuration~$\mathcal{P}$ of the Poissonized model in $B_O(R)$ and then associate to each $x_0:=(r_0,\theta_0)\in\mathcal{P}$ a particle that evolves independently of other particles following a trajectory given in radial coordinates by $x_t:=(r_t,\theta_t)$ at time $t$. At a microscopic level a natural choice for the movement of a particle is that of a random walk in $B_O(R)$ with $\partial B_O(R)$ acting as a reflecting boundary, and where at each step the particle can move either in the radial or angular direction. Assuming that the movement does not depend on the angle %\dmc{es simetria?} 
and that there is no angular drift, we conclude that at a macroscopic level particles should move according to a generator of the form
\[\Delta_{h} = \frac{1}{2}\frac{\partial^2}{\partial r^2}+\frac{\alpha}{2}\frac{1}{\tanh(\alpha r)}\frac{\partial}{\partial r}+\frac12\sigma^2_\theta(r)\frac{\partial^2}{\partial\theta^2},\]
where the drift term in the radial component is chosen so that $f(r)drd\theta$ remains the stationary distribution of the process (this can be checked using the Fokker-Planck equation). The function $\sigma^2_{\theta}(\cdot)$ is unrestricted and relates to the displacement given by the angular movement at a given position $(r,\theta)$. In this sense a natural choice is to take $\sigma^2_\theta(r)$ proportional to $\sinh^{-2}(r)$ which follows by taking the displacement proportional to the hyperbolic perimeter at that radius. An alternative, however, is to take $\sigma^2_\theta(r)$ proportional to $\sinh^{-2}(\alpha r)$ 
corresponding to the (re-scaled) Brownian motion in hyperbolic space.
%with which we recover the (re-scaled) hyperbolic Brownian motion given by~\eqref{hyperbgenerator}. 
%\AL{Since both choices seem natural to us, we work throughout this paper with the following generalized generator:}
In order to capture both settings, we introduce an additional parameter 
$\beta$ and work throughout this paper with the following generalized generator:
%\cmk{The latter is a very weak justification, lets discuss alternatives.}\dmc{We should rather say: we generalize this notion by introducing an additional parameter $\beta$, allowing for some slight deformation of the space and giving different tail behaviors}
\begin{equation}\label{truegenerator}
	\Delta_{h} := \frac{1}{2}\frac{\partial^2}{\partial r^2}+\frac{\alpha}{2}\frac{1}{\tanh(\alpha r)}\frac{\partial}{\partial r}+\frac{1}{2\sinh^2(\beta r)}\frac{\partial^2}{\partial\theta^2}
	\end{equation}
where $\beta>0$ is a new parameter related to the velocity of the angular movement which can alternatively be understood as a deformation of the underlying space.
We shall see that thus enhancing our model yields a broader range of behavior and exhibits phase transition phenomena  
(for detection times this is explained in detail in our next section where we summarize our main results).


Fix now an initial configuration of particles $\mathcal{P}$ located at points in $B_O(R)$. 
%\dmc{Would leave out since already explained before: "When clear from the context and referring to a specific particle $P$, we denote by $x_t=(r_t,\theta_t)$ its position in polar coordinates at time $t$".}
We denote by $\PP_{x_0}$ the law of a particle initially placed at a given point $x_0:=(r_0, \theta_0)\in B_O(R)$. 
We have one more fixed target $Q$, located at the boundary of $B_O(R)$ (that is, $r_Q=R$), and at angular coordinate $\theta_Q:=0$.
For any $s>0$, let  $\mathcal{P}_s\subseteq\mathcal{P}$ be the set of points that have \emph{detected} the target $Q$ by time $s$: that is, for each $P \in \mathcal{P}_s$ there exists a time instant $0 \le t \le s$, so that $x_t \in B_{Q}(R)$. Note that if $t=0$, then $P$ might be in the interior of $B_Q(R)$, whereas if~$t > 0$, the first instant at which $P$ detects $Q$ is when $P$ is at the boundary of $B_Q(R)$, that is $x_t\in\partial B_Q(R)$. This instant $t$ is also called the \emph{hitting time} of $B_Q(R)$ by particle $P$. %\cmk{$\theta_R(\cdot,\cdot)$ not defined yet.}\deleted[id=mk]{In particular, if $t > 0$, the angular coordinate $\theta_t$ equals either $+\theta_R(R,r_t)$ or $-\theta_R(R,r_t)$.}
The \emph{detection time} of $Q$, denoted by $T_{det}$, is then defined as the minimum hitting time of  $B_{Q}(R)$ among all initially placed particles. We are particularly interested in the tail probability
%\cmk{Do we really need/want to display the following expression? Putting it inline doesn't suffice?}\dmc{Put $\ge$ instead of $>$ here and later}
$
\PP_{x_0}(T_{det}\ge\ss)
$
for different values of $\ss$ (note that $\ss$ is a function on $n$). In words, we are interested in the tail probability that no particle initially located at position $x_0$ detects the target $Q$ by time $\ss$.
%\cml{reorganize this paragraph, in particular explain what $\PP_{x_0}(T_{det}>\ss)$ means}\dmc{I added a bit}

Observe that any given particle $P$ evolving according to the generator $\Delta_h$ specified in~\eqref{truegenerator} will eventually detect the target at some point, so that $\PP(T_{det}\ge\ss)\to 0$ when $\ss\to\infty$ as soon as there is at least one particle. Following what was done in~\cite{Peres2010} for a similar model on Euclidean space, our main result determines the speed at which $\PP(T_{det}>\ss)$ tends to zero as a function of $\ss$.
We consider the same setting of~\cite{Peres2010}, that is $\ss/\EE(T_{det})\to \infty$ but we have to deal with several additional, both qualitatively and quantitatively different, new issues that arise due to the dependency 
of $\ss$ on $n$.

\subsection{Main results}\label{sec:results}
%
In this section, we present the main results we obtain regarding tail probabilities for detection time, both for the mobile hyperbolic graph model we introduced in the previous section and for two restricted instances: one where the radial coordinate of particles does not change over time and another where the angular coordinate  does not change. We also discuss the relation between the main results and delve into the insights they provide concerning the dominant mechanism (either angular movement, radial movement or a combination of both) that explain the different asymptotic regimes, depending on the relation between $\alpha$ and~$\beta$. We point out that we do not present in this section a complete list of other significant results, in particular the ones that provide a detailed idea of the initial location of particles that typically detect the target. These last results will be discussed at the start of Sections~\ref{sec:angular}, \ref{sec:radial}, and~\ref{sec:mix}
where, in fact, we prove slightly stronger results than the ones stated in this section. Neither do we delve here into those results concerning one-dimensional processes with non-constant drift and a reflecting barrier that might be useful in other settings and thus of independent interest (these results are found in Section~\ref{sec:radial}).

%\dm{\textbf{Notation.} We use standard asymptotic notation of $O(\cdot)$, $o(\cdot)$, $\Theta(\cdot)$, $\Omega(\cdot)$, $o(\cdot)$, with all terms inside asymptotic expressions being positive.}

\smallskip
We begin with the statement describing the (precise) behavior of the detection time tail probability depending on how the model parameters $\alpha$ and $\beta$ relate to each other:\footnote{We use the standard Bachmann--Landau asymptotic (in $n$) notation of $O(\cdot)$, $o(\cdot)$, $\Omega(\cdot)$, $\omega(\cdot)$, $\Theta(\cdot)$,  with all terms inside asymptotic expressions being positive.}
\begin{theorem}\label{thm:intro-mixed}
Let $\alpha\in (\frac12,1]$, $\beta >0$,  $\ss:=\ss(n)$, and assume that particles move according to the generator 
$\Delta_h$ in~\eqref{truegenerator}.
Then, the following hold:
\begin{enumerate}[(i)]
    \item\label{thm:mixed-itm-ssmall} 
    For $\beta\leq\frac12$, if $\ss=\Omega((e^{\beta R}/n)^2)\cap O(1)$, then
%    \dmc{would remove "if" here and replace "then" by "we have"} $\ss=\Omega(n^{4\beta-2})$ and $\ss \le C$\deleted[id=mk]{ for some large enough constant $C > 0$}, then 
%    \dmc{As we discussed Amitai, you can change it if you don't like this artificial cut and you prefer asymptotic notation. You sort of convinced me that this is ugly to mix asymptotic with non-asymptotic notation, but we have to be careful: the large $s$ case needs sometimes a constant bigger than $1$, otherwise we don't get $\log(s)$. If you want to adapt the proof to go back to asymptotic notation (dealing with small $s$ separately) then go ahead} \cmk{I agree that this mix-up of notations is ugly. But, I don't yet understand why do we need a hard threshold.} \cml{I think that we don't, maybe use $\Omega(1)$ but remark that the constant is universal?}
$\displaystyle
\PP (T_{det} \ge \ss)=\exp\big({-}\Theta(ne^{-\beta R}\sqrt{\ss})\big).\label{ssmall}
$
\smallskip
\item
For $\beta\leq\frac12$ and $\ss=\Omega(1)$ the tail exponent depends on the relation between $\alpha$ and $2\beta$ as follows: 
\begin{enumerate}
    \item\label{thm:mixed-itm1}
    For $\alpha<2\beta$, if $\ss=O(e^{\alpha R})$, then 
    $\displaystyle \PP (T_{det} \ge \ss)=\exp\big({-}\Theta(n e^{-\beta R}\ss^{\frac{\beta}{\alpha}})\big)$.
    
    \smallskip    
    \item\label{thm:mixed-itm2} 
    For $\alpha=2\beta$, if $\ss=O(e^{\alpha R}/(\alpha R))$, then $\displaystyle\PP (T_{det} \ge \ss)=\exp\big({-}\Theta(ne^{-\beta R}\sqrt{\ss\log\ss})\big)$.

    \smallskip
    \item\label{thm:mixed-itm3}
    For $\alpha>2\beta$, if $\ss= O(e^{2\beta R})$, then
    $\displaystyle
    \PP (T_{det} \ge \ss)=\exp\big({-}\Theta(ne^{-\beta R}\sqrt{\ss})\big).$
\end{enumerate}
%\cmk{I would like to merge the first and last items above as "For $\alpha\neq 2\beta$, if $\ss=O(e^{(\alpha\wedge 2\beta)R})$, then $\PP(T_{det}\ge \ss)=\exp({-}\Theta(ne^{-\beta R}\ss^{\frac{\beta}{\alpha}\vee \frac{1}{2}}))$". But, this will depend on the discussion below concerning this Theorem.}
\item\label{thm:mixed-itm4}
For $\beta>\tfrac{1}{2}$, if $\ss= \Omega(1)\cap O(e^{\alpha R})$, then
    $\displaystyle
    \PP (T_{det} \ge \ss)=\exp\big({-}\Theta(\ss^{\frac{1}{2\alpha}})\big)$.
\end{enumerate}
\end{theorem}

\begin{remark}
Since we are working with the Poissonized model, the probability of not having any particle to begin with is of order $e^{-\Theta(n)}$, and on this event the detection time is infinite. The reader may check that replacing the upper bound of $\ss$ in each of the previous theorem cases gives a probability of this order, which explains the asymptotic upper bounds on $\ss$. 
\end{remark}

%\subsubsection*{Discussion of the theorem.}
%
Observe that by Theorem~\ref{thm:intro-mixed}, for $\ss=\Theta((e^{\beta R}/n)^{2})$  in the case $\beta\le \frac{1}{2}$, and for $\ss=\Theta(1)$ in the case $\beta>\frac{1}{2}$  we recover a tail exponent of order $\Theta(1)$, showing that the expected detection time occurs at values of $\ss$ of said order. It follows that at $\beta=\frac{1}{2}$ there is a phase transition in the qualitative behavior of the  model; for $\beta<\frac{1}{2}$, since $(e^{\beta R}/n)^2=o(1)$, the detection becomes asymptotically ``immediate" whereas for $\beta>\frac{1}{2}$ the target can remain undetected for an amount of time of order $\Theta(1)$. To explain this change in behavior notice that even though for any $\beta>0$ the value of $\sigma^{2}_{\theta}(r):=\sinh^{-2}(\beta r)$ is minuscule near the boundary of $B_O(R)$ (where particles spend most of the time due to the radial drift), decreasing~$\beta$ does increase $\sigma_{\theta}(\cdot)$ dramatically, allowing for a number of particles tending to infinity, to detect the target immediately. Since for small values of $\ss$ all but a few particles remain ``radially still" near the boundary, we deduce that there must be some purely angular movement responsible for the detection of the target, to which we can associate the tail exponent $ne^{-\beta R}\sqrt{\ss}$ seen in~\eqref{ssmall} of Theorem~\ref{thm:intro-mixed}. The same tail exponent appears in Case~\eqref{thm:mixed-itm3} of Theorem~\ref{thm:intro-mixed} for large $\ss$ when $\beta$ is again sufficiently small (smaller than $\frac{\alpha}{2}$), and again the purely angular movement is responsible for the detection of the target. %This becomes more evident when studying the detection time of a simplified model where particles have no radial movement:
To make the understanding of the observed distinct behaviors even more explicit we study two simplified models where particles are restricted to evolve solely through their angular or radial movement, respectively.
\begin{theorem}[angular movement]\label{thm:angularMain}
Let $\alpha\in (\frac12,1]$, $\beta>0$, $\ss:=\ss(n)$, and assume that particles move according to the generator
  \begin{equation*}\label{anggenerator}
	\Delta_{ang} := \frac{1}{2\sinh^2(\beta r)}\frac{\partial^2}{\partial\theta^2}.
	\end{equation*}
If $\sqrt{\ss}\leq\frac{\pi}{2}(1-o(1))e^{\beta R}$, then 
\[
\P(T_{det}\geq \ss) = 
\begin{cases}
\exp\Big({-}\Theta\Big(\mkor{\nu}{1}+n\Big(\frac{\sqrt{\ss}}{e^{\beta R}}\Big)^{1\wedge \frac{\alpha}{\beta}}\Big)\Big), & \text{if $\alpha\neq\beta$,} \\[8pt]
\exp\Big({-}\Theta\Big(\mkor{\nu}{1}+n\Big(\frac{\sqrt{\ss}}{e^{\beta R}}\Big)^{1\wedge \frac{\alpha}{\beta}}\log\big(\frac{e^{\beta R}}{1+\sqrt{\ss}}\big)\Big)\Big), & \text{if $\alpha=\beta$.}
\end{cases}
\]
\end{theorem}

%\begin{theorem}
%Let $\ss:=\ss(n)$. Assuming that particles move according to the generator
%  \begin{equation}\label{anggenerator}
%	\Delta_{h} := \frac{1}{2\sinh^2(\beta r)}\frac{\partial^2}{\partial\theta^2},
%	\end{equation}
%the following hold: \dmc{Here again as before I would suggest a strict upper bound of $s\le c n^{4\beta}$ in order not to have negative things inside logarithms, but then we mix again notation. Amitai you sort of convinced me that this is ugly and we should maybe put a max or something}
%\dmc{To do: adapt as in angular section, incorporate small values of $\ss$ with the extra $+1$. Also check, the expression inside the $\log$ for the case $\alpha=\beta$ changed}
%  \begin{enumerate}[(i)]
%  \item For $\alpha>\beta$, if $\ss\in \Omega(n^{4\beta-2})\cap O(n^{4\beta})$, then $\displaystyle\P\big(T_{det}\geq \ss\big)=e^{-\Theta\left(n^{1-2\beta}\sqrt{\ss}\right)}.$
%  \item For $\alpha=\beta$, if $\ss\in \Omega(\frac{n^{4\beta-2}}{\log^2 n})\cap O(n^{4\beta})$, then
%    $\displaystyle\P\big(T_{det}\geq \ss\big)=e^{-\Theta\big(n^{1-2\beta}\sqrt{\ss}\log(\frac{n^{\dm{2}}}{\dm{\ss^{1/(2\beta)}}})\big)}.$ 
%  \item For $\alpha<\beta$. If $\ss\in \Omega(n^{4\beta-\frac{2\beta}{\alpha}})\cap O(n^{4\beta})$, then
%    $\P\big(T_{det}\geq \ss\big)=e^{-\Theta\big({n^{1-2\alpha}\ss^\frac{\alpha}{2\beta}}\big)}.$
%\end{enumerate}
%\cmk{Again I believe that all the $n^{1-2\beta}$ should probably be $ne^{-\beta R}$, and similarly for other expressions.}
%\end{theorem}
Observe that the tail exponent appearing in the case $\alpha>\beta$ of the previous theorem is the same as the one appearing in Case~\eqref{thm:mixed-itm3} (where $\beta\leq \frac{1}{2}<\alpha$) and Case~\eqref{thm:mixed-itm-ssmall} (where $\beta<\frac{\alpha}{2}<\alpha$) of Theorem~\ref{thm:intro-mixed}: this is no coincidence. Our proof shows that a purely angular movement is responsible for detection in those cases. Note also that the other exponents contemplated in Theorem~\ref{thm:intro-mixed} are not present in Theorem~\ref{thm:angularMain}. 

\smallskip
We consider next the result of our analysis of the second simplified model where particles move only radially:
\begin{theorem}[radial movement]{\label{thm:radialmain}}
Let $\alpha\in(\tfrac{1}{2},1]$, $\beta>0$, $\ss:=\ss(n)$,
and assume that particles move according to the generator 
\begin{equation*}\label{radialgenerator}
	\Delta_{rad} := \frac{1}{2}\frac{\partial^2}{\partial r^2}+\frac{\alpha}{2}\frac{1}{\tanh(\alpha r)}\frac{\partial}{\partial r}.
\end{equation*}
For every $0<c<\frac{\pi}{2}$, there is a $C>0$ such that if  $\ss\ge C$ and $\ss^{\frac{1}{2\alpha}}\le (\frac{\pi}{2}-c)e^{\frac{R}{2}}$, then
\[
\P(T_{det}\geq \ss) = \exp\big({-}\Theta(\ss^{\frac{1}{2\alpha}})\big).
\]
\end{theorem}
%\begin{theorem}[radial movement]{\label{thm:radialmain}}
%Let $\alpha\in(\tfrac{1}{2},1]$  and let $\ss=\ss(n)$ be a positive function of $n$. 
%Assume that particles move according to the generator
%\begin{equation*}\label{radialgenerator}
%	\Delta_{rad} := \frac{1}{2}\frac{\partial^2}{\partial r^2}+\frac{\alpha}{2}\frac{1}{\tanh(\alpha r)}\frac{\partial}{\partial r}.
%	\end{equation*}
%If $\ss\in\Omega(1)\cap O(n^{2\alpha})$, then
%\[
%\P(T_{det}\geq \ss)\,=\,e^{-\Theta(\ss^{\frac{1}{2\alpha}})}.
%\]
%\end{theorem}

Once more observe that the tail exponent in the $\beta>\frac12$ case of Theorem~\ref{thm:intro-mixed} is the same as the one in Theorem~\ref{thm:radialmain}, and once more this is not a coincidence: our proof shows that when $\beta>\frac{1}{2}$ the detection of the target is the result of the radial motion of particles alone.
In contrast, Cases~\eqref{thm:mixed-itm1} and~\eqref{thm:mixed-itm2} in Theorem~\ref{thm:intro-mixed}, when $\frac{1}{2}<\alpha\leq2\beta\leq1$ 
%\dmc{I would put here both cases $\alpha \le 2\beta$?} \dm{and $\ss=\Omega(1)$ (that is, cases~\eqref{case1} and cases~\eqref{case2} in Theorem~\ref{thm:intro-mixed})}\cml{Agree, fixed} 
yield new tail exponents not observed neither in  Theorem~\ref{thm:angularMain} nor in Theorem~\ref{thm:radialmain}, and which is larger than those given in said results. Since a larger tail exponent means a larger probability of an early detection, in this case, neither the angular nor the radial component of the movement dominates the other, but they rather work together to detect the target quicker: in Case~\eqref{thm:mixed-itm1}, the proof reveals that the radial movement is responsible for pulling particles sufficiently far away from the boundary to a radial value, where $\sigma_{\theta}(r):=\sinh^{-2}(\beta r)$ becomes sufficiently large (although still small) so that with a constant probability at least one particle has a chance of detecting at this radial value through its angular motion. Finally, for Case~\eqref{thm:mixed-itm2}  in Theorem~\ref{thm:intro-mixed} we see a tail exponent of the form $ne^{-\beta R}\sqrt{\ss}$ (as expected when taking $\alpha\nearrow 2\beta$ in Case~\eqref{thm:mixed-itm1} or $\alpha\searrow 2\beta$ in Case~\eqref{thm:mixed-itm3}) but accompanied by a logarithmic correction.  This correction becomes clear when inspecting the proof: it is a result of the balance between the effect of the radial drift and the contribution of the angular movement of particles at distinct radii whose effect is that the contributions to the total angular movement coming from the time spent by a particle within a narrow band centered at a given radius are all about the same, adding up and giving the additional logarithmic factor.
 
\medskip 
Our main theorem, that is Theorem~\ref{thm:intro-mixed}, provides insight on \textit{how unlikely} it is for the target~$Q$ to remain undetected until time $\ss$, and as discussed before, the proof strategy as well as our remarks provide additional information about the \textit{dominant detection mechanisms} (that is, either by angular movement only, by radial movement only, or by a combination of the two). Moreover,  through our techniques we can obtain more precise information showing in each case \textit{where} particles must be initially located in order to detect~$Q$ before time $\ss$ with probability bounded away from zero. Specifically, given a parameter $\kappa>0$ (independent of $n$) %\cmk{Not sure about this $\KA$ and $\KR$ business} \dmc{Agree, probably best to leave it as you have it here} 
we can construct a parametric family of sets $\dt(\kappa)$ depending on $\ss:=\ss(n)$ and the parameters of the model, such that for every $x_0\in\dt(\kappa)$ and sufficiently large $n$, the probability $\PP_{x_0}(T_{det}\ge\ss)$ is at least a constant that depends on the parameter $\kappa$. Even further, we will show that $\mu(\dt(\kappa))$ is of the same order as the tail exponents, which implies that asymptotically the best chance for $Q$ to remain undetected by time $\ss$ is to find these sets initially empty of points.
To be precise, we show the following meta-result:
\begin{theorem}{\label{thm:intro-Dt}}
Let $\alpha\in (\frac12,1]$, $\beta > 0$ and $\ss:=\ss(n)$. For every case considered in Theorems~\ref{thm:intro-mixed},~\ref{thm:angularMain} and~\ref{thm:radialmain} with the corresponding hypotheses, and under the additional assumption of $\ss=\omega(1)$ in the case of Theorem~\ref{thm:intro-mixed}, there is an explicit parametric family of sets $\dt:=\dt(\kappa)$ which is increasing in $\kappa$, and which satisfies:
\begin{enumerate}[(i)]
    \item\label{lowerDt} (Uniform lower bound) For every $\kappa$ sufficiently large and $n$ sufficiently large, 
    \[
    \inf_{x_0\in\dt\!(\kappa)}\PP_{x_0}(T_{det}\ge\ss)
    \]
    is bounded from below by a positive expression that depends only on $\kappa$ and the parameters of the model.
    %\dm{and that tends to $0$ as $\kappa \to \infty$}.\dmc{I put this sentence here for symmetry with b), not sure it is good from a sales point of view to have a lower bound tending to 0. perhaps we should remove it here}\cmk{Yes, don't say anything about the limit.}
    
    \item\label{upperDt} (Uniform upper bound) For every $\kappa$ sufficiently large and $n$ 
    sufficiently large
    \[
    \sup_{x_0\in\ndt\!(\kappa)}\PP_{x_0}(T_{det}\ge\ss)
    \]
    is bounded from above by a positive expression that depends only on $\kappa$ and the parameters of the model, and that tends to $0$ as $\kappa \to \infty$.
    
    \item\label{integralDt} (Integral bound) Fix $\kappa=C$ for some sufficiently large constant $C>0$. For $n$ sufficiently large, we have %\dmc{You wanted to have $\Theta$ }\cmk{I take it back. I just want $O(\cdot)$. Should erase my comment in Section 6.}\dmc{ok, I agree. The added value is not so big, let's rather just put $O()$ there (perhaps one could get $\Theta$ with different $\kappa$ and computing the measure of the difference, but a little bit of work}
    \[
    \int_{x_0\in\ndt\!(C)}\PP_{x_0}(T_{det}\le\ss)d\mu(x_0) = O(\mu(\dt(C))).
    \]
\end{enumerate}
\end{theorem}
The added value of this last theorem is that it reveals that the set~$\dt(\kappa)$  
can roughly be interpreted as the regions where particles \emph{typically detecting} the target are initially located. Moreover, each of the 
Theorems~\ref{thm:intro-mixed}, \ref{thm:angularMain}, and~\ref{thm:radialmain} follow from the instances of Theorem~\ref{thm:intro-Dt} regarding angular, radial and unrestricted movement, respectively, thus
revealing the unified approach that we take throughout the paper as explained in detail in Subsection~\ref{sec:strategy}. 
%\dmc{I put the two last paragraphs that were here "establishing lower bounds..." in that section, to which I refer here. Also, I moved this after related work, since we refer to Wiener sausages} 
%\cmk{Hmm, ... I don't like the change of order between related work and structure of proof sections. It makes much more sense how it was before. Also, the last two paragraphs now in the structure of proof section are completely unrelated to the title of that section. Also, in the rephrased paragraphs, it is now explained what Wiener sausages are, so I see no need of moving the two paragraphs as suggested.}\cml{I think we should discuss this in the final meeting (yes, I am THIS positive)}

Establishing the uniform lower bounds for the different cases of Theorem~\ref{thm:intro-Dt} requires, especially in Case~\eqref{thm:mixed-itm2} of Theorem~\ref{thm:intro-mixed}, a delicate coupling with a discretized auxiliary process. For the integral upper bounds a careful analysis has to be performed: we thereby need to calculate hitting times of Brownian motions in the hyperbolic plane with a reflecting barrier where we make use of results on tails of independent sums of Pareto random variables (that differ greatly according to the exponent).
%\dmc{Let's take out:  "and again in Case~\eqref{thm:mixed-itm2} extra care has to be taken by splitting $\ndt(\kappa)$ into two different subregions."}\cml{In the upper bounds I think we do not split it}

\smallskip
Finally, let us point out that from a technical point of view, the additional difficulties we encounter compared to Euclidean space are substantial: in~\cite{Peres2010} the authors address the Euclidean analogue of the problem studied in this paper by relating the detection probability to the expected volume of the $d$-dimensional Wiener sausage (that is, the neighborhood of the trace of Brownian motion up to a certain time moment -- see also the related work section below). To the best of our knowledge, however, the volume of the Wiener sausage of Brownian motion with a reflecting boundary in hyperbolic space is not known. Even further, our proof techniques give significantly more information: with our proof we are able to characterize the subregions of the hyperbolic space such that particles initially located therein are likely to detect the target up to a certain time moment, whereas particles located elsewhere are not - with the information of the pure volume of the Wiener sausage we would not be able to do so. %(see also the related work section for more details on~\cite{Peres2010}). 
The radial drift towards the boundary of $B_O(R)$ entails many additional technical difficulties that do not arise in Euclidean space (see also the related work section below for work being done there). Moreover, our setup contains a reflecting barrier at the boundary, thereby adding further difficulty to the analysis.
\begin{comment}
\begin{theorem}{\label{thm:intro-Dt}}
Let $\ss:=\ss(n)$ be a positive function of $n$. For every case considered in Theorems~\ref{thm:intro-mixed},~\ref{thm:angularMain} and~\ref{thm:radialmain}, there are $\kappa_0>0$ large and an explicit parametric family of sets $\dt:=\dt(\kappa)$ with $\kappa>\kappa_0$, which is increasing in $\kappa$, and which satisfy:
\begin{enumerate}[(i)]
    \item For every $\kappa>\kappa_0$ there is an $\varepsilon>0$ such that
    \[
    \liminf_{n\to\infty}\inf_{x_0\in\dt}\PP_{x_0}(T_{det}\ge\ss)> \varepsilon.\label{lowerDt}
    \]
    
    \item \dmc{I still don't see why this statement is not vacuous. No condition on $\kappa'$ here} \cmk{I assume that the problem he points out is that one could always choose $\kappa'$ so large that $\ndt$ would be empty and thus the claim vacuously true, right? If so, I think I agree. I believe that what we rally want is, rather than "for all $\kappa>\kappa'$" say "for all $\kappa<\kappa'$.} \dmc{Yes, one could always choose $\kappa'$ large enough. But we don't want this. We want rather for all $\kappa$, choosing $\kappa'= C\kappa$ for some function $C$, no? Recheck this later} \cml{But this is no longer a problem when choosing $\kappa$ first and then taking $n$ sufficiently large no? all of our results are asymptotic in $n$ anyways} For every $\varepsilon>0$ there is a $\kappa'>0$ such that for all $\kappa>\kappa',$
    \[
    \limsup_{n\to\infty}\sup_{x_0\in\ndt}\PP_{x_0}(T_{det}\ge\ss)< \varepsilon.\label{upperDt}
    \]
    \item\label{thm:intro-Dt-itm3} For any fixed $\kappa$,
    \[
    \PP(T_{det}\ge\ss)=e^{-\Theta(\mu(\dt))}.\label{massDt}
    \]
\end{enumerate}
\cmk{I find strange that $\kappa_0$ is mentioned before the enumeration but used only in the first item. Also, supposedly $\dt$ might not even be defined for $\kappa<\kappa_0$, but the last item does not require that $\kappa>\kappa_0$!}\dmc{Indeed, need to change this once I checked the proof below}
%that for each $\varepsilon>0$ there are \dm{$\kappa' \ge\kappa'>0$} with
%\begin{equation}\label{dtepsilon}
    %\liminf_{n\to\infty}\inf_{x_0\in\ndt(\kappa',n)}\PP_{x_0}(T_{det}>\ss)< \varepsilon < \limsup_{n\to\infty}\sup_{x_0\in\dt(\kappa,n)}\PP_{x_0}(T_{det}>\ss).
%\end{equation}
%\begin{equation}\liminf_{n\to\infty}\inf_{x_0\in\dt}\PP_{x_0}(T_{det}>\ss)\geq f_1(\kappa)\label{lowerDt}\end{equation}
%and
%\begin{equation}\limsup_{n\to\infty}\sup_{x_0\in\ndt}\PP_{x_0}(T_{det}>\ss)\leq f_2(\kappa),\label{upperDt}\end{equation}
%for some given positive $f_1, f_2\in o(1)$ \dmc{no quieres decir "for some $f_1(\phi) > 0$? We should have $f_1 > 0$, not $f_1=o(1)$, and we will for this set not be able to get $f_2=o(1)$. If we don't have $f_1 > 0$ the first part is meaningless So I would rather have in mind a small set $\dt$ so that there is constant chance to get a detection there. But I see why below you want to have $f_2=o(1)$: perhaps we should say a statement like 
%\begin{equation}\limsup_{n\to\infty} \lim_{c \to \infty}\sup_{x_0\in\cndt}\PP_{x_0}(T_{det}>\ss)\leq f_2(\kappa),\label{upperDt}\end{equation}
%}\cml{no entendi, mejor lo hablamos}.
%Even further, for any fixed $\kappa$,
%\begin{equation}\mu(\dt)=-\Theta\big(\log\PP(T_{det}>\ss)\big).\label{massDt}\end{equation}
\end{theorem}

\cmk{Not sure what to do with the following paragraph. Will depend on what the previous result ends up being. My feeling is that we want to replace the above statement by Theorem 35, correct?}
It follows that as soon as we find appropriate sequences of sets $\dt$, each of the Theorems~\ref{thm:intro-mixed}, \ref{thm:angularMain}, and~\ref{thm:radialmain}, follow directly from Part~\eqref{thm:intro-Dt-itm3} of Theorem~\ref{thm:intro-Dt}, revealing the unified approach that we take throughout the paper. The added value of this theorem, however, lies in~\eqref{dtepsilon} \dmc{The first point of the theorem,no?} which reveals that the sequences of sets $\dt$ that we find are not only useful to pinpoint the tail exponents, but also can roughly be interpreted as the regions where particles \emph{typically detecting} the target are initially located. 
\end{comment}
%\dmc{Maybe comment out: "This interpretation is not exact since for a fixed $\kappa$, the set $\ndt$ might contain regions where detection is more likely than on some regions of $\dt$": Or explain in more detail, but perhaps not yet at this point} \cmk{I agree that it would be better to explain this somewhere else.}

%As already mentioned, the main result follows from the proof of Theorem~\ref{thm:intro-Dt}: having found the parametric family of sets $\dt$ in each case with good chances to detect the target, the lower bound on the tail exponent is then comparably easier by integrating over $\dt$ with respect to the measure of the particles


\begin{comment}

\AL{At an intuitive level, decreasing $\beta$ increases dramatically the value of $\sigma_{\theta}$ near the boundary of $B_O(R)$, and even though this does not change the fact that $\sigma_{\theta}$ is still very small for $r\approx R$ and hence  In particular, it follows that when  can be observed Observe that as soon as $\beta<1/2$ the probabilities in the theorem have the form $\PP(T_{det}>tn^{-\delta})$ for some case-specific value of $\delta>0$, which suggests that as $n\to\infty$ the detection time converges to zero as a result of the increasing number of particles in the network, forcing us to introduce a rescaled time in order to see the tail exponents. Interestingly, in two first two cases there is a phase transition in both the time-scale and the tail exponents whenever the rescaled time becomes of order $\Theta(1)$, revealing that the typical movement of particles changes qualitatively at said times. Even further, the different scalings and tail exponents across cases suggest the existence of diverse detection mechanisms taking place at the same time, being the optimal one given by the parameters of the network.}
\dm{Indeed, the intuition behind this qualitative change of the behavior is the following: if $\beta > \frac12$ (Case 4), then the last term in~\eqref{truegenerator} corresponding to the angular movement is so small such that angular detection is slowed down by so much, and radial detection dominates. If $\beta < \frac12$ and additionally $2\beta < \alpha$ (Case 3), then, on the one hand, the angular movement corresponding to the last term in~\eqref{truegenerator} is much faster, and on the other hand the drift in the radial movement towards the boundary is so strong so that the probability of radial detection is very small. If $\beta \le \frac12$, but $2\beta > \alpha$, then the drift in the radial movement towards the boundary is not so strong anymore:  initially, the distribution of particles is by our assumption of $\alpha > \frac12$ still biased towards to having more particles closer to the boundary, and thus for small times the angular movement still dominates. For larger times, however, the radial component will accelerate the detection, thereby giving rise to two different scaling phenomena in Case 1. The same phenomenon happens in Case 2, with the difference that the different scaling for larger times comes from the fact that in the case $2\beta=\alpha$ there are different combinations of reaching a certain radial coordinate, whereas in Case 1 there is essentially one optimal such combination. In order to make this intuition more precise, we are able to characterize the region $\mathcal{D}^{(t)}$ of particles which is the subregion of $B_O(R)$ such that any particle initially located therein has a constant probability to detect the target by time $t$:}
\dmc{HERE define $D^t$ and RESULTS ON THEOREM ON $D^t$ with uniform bound}

\AL{To make the understanding of said mechanisms even more explicit we study two simplified models when restricting particles to evolve solely through their angular or radial movement:}
\dm{When restricting to angular movement only, we have the following main theorem:}
\begin{theorem}[angular movement]
Fix $\alpha\in(\tfrac{1}{2},1]$ and let $\ss=\ss(n)$ be a positive function of $n$.
  For the model where particles move according to the generator
  \begin{equation}\label{anggenerator}
	\Delta_{ang} = \frac{1}{2\sinh^2(\beta r)}\frac{\partial^2}{\partial\theta^2}
	\end{equation}
  there is a $c>0$ small enough such that the following hold for all $n$ large:
  \begin{enumerate}[(i)]
  \item If $\alpha>\beta$ and $1\leq t\leq cn^2$, then
    $\P\big(T_{det}\geq t\nnu^{4\beta-2}\big)=e^{-\Theta(\sqrt{t})}.$
    
  \item If $\alpha=\beta$ and $1\leq t\leq c\nnu^{2}$, then
    $\P\big(T_{det}\geq t\nnu^{4\beta-2}/(\log \frac{\nnu^2}{t})^2\big)=e^{-\Theta(\sqrt{t})}.$ 
  
  \item If $\alpha<\beta$ and $1\leq t\leq c\nnu^{\frac{2\beta}{\alpha}}$, then
    $\P\big(T_{det}\geq t\nnu^{4\beta-\frac{2\beta}{\alpha}}\big)=e^{-\Theta(t^{\frac{\alpha}{2\beta}})}.$
\end{enumerate}\end{theorem}
\dm{The main intuition behind the scales \dmc{or scalings?} appearing in the angular movement is that the region primarily detecting the target is a widened (hyperbolic) ball around the target, where the width of the ball at each layer takes into account the speed of the angular movement at that layer. As before, we can characterize the region in which particles initially located there have a constant probability to detect the target by time $t$:
}
\dmc{HERE THE THEOREM ON $D^t$ with constant probability chance in angular case only}
\medskip
\dm{When restricting to radial movement only, we have the following main theorem:}
\begin{theorem}[radial movement]
For the model where particles move according to the generator
\begin{equation}\label{radialgenerator}
	\Delta_{rad} = \frac{1}{2}\frac{\partial^2}{\partial r^2}+\frac{\alpha}{2}\frac{1}{\tanh(\alpha r)}\frac{\partial}{\partial r}
	\end{equation}
there exists $c > 0$ sufficiently small, so that for every $1\leq t\leq c n^{2\alpha}$,
\[
\P(T_{det}\geq t)\,=\,e^{-\Theta(t^{\frac{1}{2\alpha}})}.
\]
\end{theorem}
\dm{The main intuition behind the scalings appearing in the radial movement is that the region primarily detecting the target is a certain sector around the target: a particle starting (and remaining) at a small angle does not have to reach a radial position so close to the origin as a particle starting (and remaining) at a larger angle from the target. Once more, the following theorem makes the region of points having constant probability to detect the particle by time $t$ precise:}

\dmc{HERE THE THEOREM ON $D^t$ with constant probability chance in radial case only}

\AL{Notice that both the time scaling and tail exponent of case $(i)$ in Theorem \ref{thm:angularMain} are the same ones that appear in Theorem \ref{thm:intro-mixed} in cases 1a, 1b \dmc{this should be 2a, no?}, 3, as well as case 2b when neglecting the $\log$ factor, revealing that in these cases (up to the $\log$ factor in Case 2b) the effect of angular movement surpasses the effect of radial movement. In contrast, the time scaling and tail exponent of Theorem \ref{thm:radialmain} are the same as the ones appearing in Theorem~\ref{thm:intro-mixed} case 4, revealing that whenever $\beta>1/2$ the effect of the radial movement mainly contributes to the detection of the target.}
\end{comment}

\mbox{\ }



