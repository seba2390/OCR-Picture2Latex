In this section we collect basic facts and a few useful observations together with a bit of additional notation.
First, for a region $\Omega$ of $B_O(R)$, that is, $\Omega\subseteq B_O(R)$, let $\mu(\Omega)$ denote the expected number of points $\mathcal{P}$ in $\Omega$. The first basic fact we state here gives the expected number of points $\mathcal{P}$ at distance at most $r$ from the origin, 
as well as the expected number of points at distance at most $R$ from a given point $Q \in B_{O}(R)$:
\begin{lemma}[{\cite[Lemma~3.2]{GPP12}}]\label{lem:muBall}
If $0\leq r\leq R$, then
  $\mu(B_{O}(r)) = n e^{-\alpha(R-r)}(1+o(1))$.
Moreover, if $Q \in B_O(R)$ is such that $r_Q:=r$, then 
for $C_{\alpha}:=2\alpha/(\pi (\alpha-\frac12))$,
\[
\mu(B_Q(R) \cap B_O(R)) = n C_{\alpha} e^{-\frac{r}{2}}(1 + O(e^{-(\alpha-\frac12)r}+e^{-r})).
\]
\end{lemma}


%For a given $n \in \NN$, we denote this model by $\poimod_{\alpha,\nu}(n)$.
%\cmk{The following seems out of place. Added a comment in Section 1 instead.}\deleted[id=mk]{Note in particular that $\EE|{V_n}|=n$ since }
%\[
%\deleted[id=mk]{\iint f(r,\theta) \, d\theta\, dr 
  %= \nu e^{\frac{R}{2}}=n.}
%\]
We also use the following Chernoff bounds for Poisson random variables:
\begin{theorem}[Theorem A.1.15 of~\cite{AlonSpencer}]
Let $P$ have Poisson distribution with mean $\mu$. Then, for every $\varepsilon > 0$,
$$
\P(P \le \mu(1-\varepsilon)) \le e^{-\varepsilon^2 \mu/2}
$$
and
$$
\P(P \ge \mu(1+\varepsilon)) \le \left(e^{\varepsilon} (1+\varepsilon)^{-(1+\varepsilon)} \right)^{\mu}.
$$
\end{theorem}

%\dmc{For Prop 29, where the union bound was wrong before}

\begin{comment}
\dmc{If you agree on Amitai's geometric variables, this is not needed anymore}
We will also need the following generalization of the previous bound due to Bentkus~\cite[Theorem 1]{Bentkus}, stated in a simplified form here. For two random variables $X$ and $Y$ defined on the same probability space, we write $X \preccurlyeq Y$ if $Y$ stochastically dominates $X$, that is, $\P(X \ge x) \le \P(Y \ge x)$ for all $x \in \mathbb{R}$. Let $\mathcal{L}(X)$ denote the distribution of the random variable $X$. For a positive random variable $Y$ and for $m \ge 0$ we define the random variable~$Y^{[m]}$ so that $\EE Y^{[m]}=m$, $Y^{[m]} \preccurlyeq Y$  and so that for some minimal possible $b \ge 0$ we have $\PP(0 < Y^{[m]}< b)=0$ and $\PP(Y^{[m]} \ge x)=\PP(Y \ge x)$ for all $x \ge b$.  

\begin{lemma}[\cite{Bentkus}, Theorem 1]\label{bentkus}
Let $S=X_1+\ldots+X_{\ell}$ be a sum of $\ell$ positive independent random variables. Assume that for every $k \in \{1, 2, \ldots, \ell\}$ we have $X_k \preccurlyeq Y^{[m]}$ and $\EE(X_k) \le m$ for some positive random variable $Y$ and some non-negative real number $m$. Let $T:=T_1+\ldots+T_{\ell}$ be a sum of $\ell$ independent random variables $T_k$ so that $\mathcal{L}(T_k)=\mathcal{L}(Y^{[m]})$. Then, for all $x \in \mathbb{R}$, 
$$
\P(S \ge x) \le \inf_{h < x} e^{-hx} \EE(e^{hT}).
$$
%In particular, if $\E S \ge 1/c$ for some constant $c > 0$, 
%$$
%\Prob{ \left(S \ge c \, \E S\right)} \le e^{-c \E S} \E e^T.
%$$
\end{lemma}

We also use the following extension of Chernoff bounds to other sums of independent random variables whose values are bounded.
\begin{theorem}[Theorem 2 of~\cite{Hoeffding}]\label{Hoeffding}
%Let $X_1, \ldots, X_n$ be independent random variables with $X_i$ taking values in $[a_i, b_i]$ for some $a_i, b_i \in \mathbb{R}$. Let $S_n=\sum_{i=1}^n X_i$. Then, for any $t \ge 0$,
$$
\P(|S_n - \EE S_n| \ge t) \le 2\exp\Big(-\frac{2t^2}{\sum_{i=1}^n (b_i-a_i)^2}\Big).
$$
\end{theorem}
\end{comment}
%\dm{We will also need the following generalization of Chernoff's bound due to Bentkus~\cite{Bentkus}, stated in a simplified form here. For two random variables $X$ and $Y$ defined on the same probability space, we write $X \preccurlyeq Y$ if $Y$ stochastically dominates $X$, that is, $\Prob(X \ge x) \le \Prob(Y \ge x)$ for all $x \in \mathbb{R}$. Let $\mathcal{L}(X)$ denote the distribution of the random variable $X$. For a positive random variable $Y$ and for $m \ge 0$ we define the random variable $Y^{[m]}$ so that $\E Y^{[m]}=m$, $Y^{[m]} \preccurlyeq Y$  and so that for some $b > 0$ we have $\Prob(0 < Y^{[m]}< b)=0$ and $\Prob(Y^{[m]} \ge x)=\Prob(Y \ge x)$ for all $x \ge b$ (in other words, one may roughly think of $Y^{[m]}$ as the random variable ``shifting mass that is close to $0$ to $0$ itself'').  } \dmc{check if needed finally}
%\begin{lemma}[\cite{Bentkus}]\label{bentkus}
%Let $S=X_1+\ldots+X_{\ell}$ be a sum of $\ell$ positive independent random variables. Assume that for every $k \in \{1, 2, \ldots, \ell\}$ we have $X_k \preccurlyeq Y^{[m]}$ and $\E X_k \le m$ for some positive random variable $Y$ and some non-negative real number $m$. Let $T=\eps_1+\ldots+\eps_{\ell}$ be a sum of $\ell$ independent random variables $\eps_k$ so that $\mathcal{L}(\eps_k)=\mathcal{L}(Y^{[m]})$. Then, for all $x \in \mathbb{R}$, 
%$$
%\Prob(S \ge x) \le \inf_{h \le x} e^{-hx} \E e^{hT}.
%$$
%In particular, if $\E S \ge 1/c$ for some constant $c > 0$, 
%$$
%\Prob{ \left(S \ge c \, \E S\right)} \le e^{-c \E S} \E e^T.
%$$
%\end{lemma}


\medskip
Throughout this paper, we denote by $\theta_R(r,r')$ the maximal angle between two points at (hyperbolic) distance at most $R$ and radial coordinates  $0\leq r,r'\leq R$. By~\eqref{eqn:coshLaw}, for $r+r'\geq R$, we have
\begin{equation}\label{eqn:angle}
\cosh R = \cosh r\cosh r'-\sinh r\sinh r'\cos \theta_R(r,r').
\end{equation}
Henceforth, consider the mapping $\phi:[0,R]\to [\theta_R(R,R),\pi/2]$ such that $\phi(r):=\theta_R(R,r)$.
For the sake of future reference, we next collect some simple as well as some known facts concerning~$\phi(\cdot)$.
\begin{lemma}\label{lem:phi}
The following hold:
\begin{enumerate}[(i)]
\item\label{itm:phi1} $\cos(\phi(r))=\coth R\cdot\tanh(\frac{r}{2})$.
\item\label{itm:phi2} $\phi(\cdot)$ is differentiable (hence, also continuous) and decreasing.
\item\label{itm:phi3} $\phi(0)=\frac{\pi}{2}$.
\item\label{itm:phiInv} $\phi(\cdot)$ is invertible and its inverse is differentiable (hence, continuous) and decreasing.
\item\label{itm:phi4} $\phi(r)=2e^{-\frac{r}{2}}(1\pm O(e^{-r}))$. 
\end{enumerate}
\end{lemma}
\begin{proof}
The first part follows from~\eqref{eqn:coshLaw} taking $d_H:=R$, $r':=R$, and by the hyperbolic tangent half angle formula. 
The second part follows because the derivative with respect to $r$ of $\arccos(\coth R\cdot\tanh(\frac{r}{2}))$
exists and is negative except when $r=R$ (for details see~\cite[Remark 2.1]{km19}).
The third part follows directly from the first part, and the fourth part follows immediately from
the preceding two parts.
The last part is a particular case of a more general result~\cite[Lemma 3.1]{GPP12}.
\end{proof}

\medskip
To conclude this section, we introduce some notation that we will use throughout the article. For a subset $\Omega$ of $B_O(R)$ we let~$\overline{\Omega}$ denote its complement with respect to $B_O(R)$, i.e., $\overline{\Omega}:=B_{O}(R)\setminus\Omega$.
Thus, $\overline{B}_O(r)$ denotes $B_{O}(R)\setminus B_O(r)$. 