\section{Conclusions}
\label{sec:discussion}

This paper first points out that some state-of-the-art gradient inversion attacks have made strong assumptions about knowing BatchNorm statistics and private labels.  Relaxing such assumptions can significantly weaken these attacks.

The paper then reports the performance of a set of proposed defenses against gradient inversion attacks, and estimates the computation cost of an end-to-end recovery of a single image in different dataset sizes. Our evaluation shows that InstaHide without mixing with data from a public dataset combined with gradient pruning can defend the state-of-the-art attack, and the estimated time to recover a single image in a medium-size client dataset (e.g. of 500,000 images) is enormous.


Based on our evaluation of the attack by~\citep{geiping2020inverting} and multiple defenses for {\em plain-text} gradients, we have the following observations:

\begin{itemize}
    \item {\em Using BatchNorm layers in your deep net but don't share BatchNorm statistics of the private batch during Federated learning weakens the attack}. We have demonstrated in Section~\ref{sec:assumption} that exposing BatchNorm statistics to the attacker significantly improves the quality of gradient inversion. So a more secure configuration of Federated Learning would be to use BatchNorm layers, but do not share BatchNorm statistics in training, which has been shown feasible in \citep{andreux2020siloed, li2021fedbn}.
    
    \item {\em Using a large batch size weakens the attack; a batch size smaller than 32 is not safe}. We have shown that a larger batch size hinders the attack by making it harder to guess the private labels (Section~\ref{sec:assumption}) and to recover the private images even with correct private labels (Section~\ref{sec:exp}). Our experiments suggest that even with some weak defenses applied, a batch size smaller than 32 is not safe against the strongest gradient inversion attack. 
    
    \item {\em Combining multiple defenses may achieve a better utility-privacy trade-off}. In our experiment, for a batch size of 32, combining InstaHide ($k=4$) with gradient pruning ($p=0.9$) achieves the best utility-privacy trade-off, by making the reconstruction almost unrecognizable at a cost of \textasciitilde$7\%$ accuracy loss (using InstaHide also makes the end-to-end recovery of a single image more computationally expensive). Best parameters would vary for different deep learning tasks, but we strongly encourage Federated learning participants to explore the possibility of combining multiple defensive mechanisms, instead of only using one of them. 
    
\end{itemize}


We hope to extend our work by including  evaluation of defenses for high-resolution images, the attack by \citep{yin2021see}  (when its implementation becomes available), and more defense mechanisms  including those rely on adding noise to gradients. 
 
