\appendix

\section{Experimental details and more results}
\label{sec:app_exp}
We run all the experiments on Nvidia RTX 2080 Ti GPUs and V100 GPUs. Table~\ref{tab:app_testbed} summarizes the set of images used in each figure or table in the main paper.  

\captionsetup[table]{font=small}
\begin{table}[H]
    \small
    \centering
    \begin{tabular}{|p{2.5cm}|p{10cm}|}
    \toprule
         {\bf Figure/Table} & {\bf Comments}	\\
    \midrule
        Figure~\ref{fig:BN_var}a & We’ve tuned hyperparams for the attack (see Appendix~\ref{sec:app_hyperparam}) and carried out evaluations on the whole CIFAR-subset. The first sampled batch of size 16 from CIFAR-subset was used in Figure~\ref{fig:BN_var}a to demonstrate the quality of recovery for low-resolution images when BatchNorm statistics are not assumed to be known.  \\
        \midrule
        Figure~\ref{fig:BN_var}b & We’ve tuned hyperparams for the attack (see Appendix~\ref{sec:app_hyperparam}) and carried out evaluations on the whole ImageNet-subset. The best-reconstructed image in ImageNet-subset was used in Figure 1b to demonstrate the quality of recovery for high-resolution images when BatchNorm statistics are not assumed to be known.\\
        \midrule
        Figure~\ref{fig:batch_label_dist} & Percentages of class labels per batch were evaluated over the entire CIFAR10 dataset, for a random seed.	\\
        \midrule
        Figure~\ref{fig:reconstructed_labels} & The first sampled batch of size 16 was used in Figure~\ref{fig:reconstructed_labels} to demonstrate the quality of recovery when labels are not assumed to be known.	\\
        \midrule
        Table~\ref{tab:exp_summary} and Figure~\ref{fig:vis_recon} & We’ve tuned hyperparams for the attack and carried out evaluations on the whole CIFAR-subset. Table~\ref{tab:exp_summary} summarizes the performance of the attack on the whole CIFAR-subset and  Figure~\ref{fig:vis_recon} shows example images.\\
    \bottomrule
    \end{tabular}
    \caption{Summary of experimental testbed for each evaluation.}
    \label{tab:app_testbed}
\end{table}


\subsection{Hyper-parameters}
\label{sec:app_hyperparam}



\paragraph{Training.} For all experiments, we train ResNet-18 for 200 epochs, with a batch size of 128. We use SGD with momentum 0.9 as the optimizer. The initial learning rate is set to 0.1 by default, except for gradient pruning with $p=0.99$ and $p=0.999$. where we set the initial learning rate to 0.02. We decay the learning rate by a factor of 0.1 every 50 epochs.

\paragraph{The attack.}  We report the performance under different $\alpha_{\rm TV}$'s (Figure~\ref{fig:BN_tv_tune}) and $\alpha_{\rm BN}$'s (Figure~\ref{fig:BN_reg_tune}).

\begin{figure}[H]
\captionsetup[subfigure]{labelfont=scriptsize, textfont=tiny}
    \centering
    \subfloat[Original]{\includegraphics[width=0.12\textwidth]{imgs/appendix/TV/original.png}}
    \subfloat[$\alpha_{\rm TV}$=0]{\includegraphics[width=0.12\textwidth]{imgs/appendix/TV/tv_0.png}}
    \subfloat[$\alpha_{\rm TV}$=1e-3]{\includegraphics[width=0.12\textwidth]{imgs/appendix/TV/tv_1e-3.png}}
    \subfloat[$\alpha_{\rm TV}$=5e-3]{\includegraphics[width=0.12\textwidth]{imgs/appendix/TV/tv_5e-3.png}}
    \subfloat[$\alpha_{\rm TV}$=1e-2]{\includegraphics[width=0.12\textwidth]{imgs/appendix/TV/tv_1e-2.png}}
    \subfloat[$\alpha_{\rm TV}$=5e-2]{\includegraphics[width=0.12\textwidth]{imgs/appendix/TV/tv_5e-2.png}}
    \subfloat[$\alpha_{\rm TV}$=1e-1]{\includegraphics[width=0.12\textwidth]{imgs/appendix/TV/tv_1e-1.png}}
    \subfloat[$\alpha_{\rm TV}$=5e-1]{\includegraphics[width=0.12\textwidth]{imgs/appendix/TV/tv_5e-1.png}}
    
    \caption{Attacking a single CIFAR-10 images in $\rm BN_{exact}$ setting, with different coefficients for the total variation regularizer ($\alpha_{\rm TV}$'s). $\alpha_{\rm TV}$=1e-2 gives the best reconstruction.}
    \label{fig:BN_tv_tune}
\end{figure}


\begin{figure}[H]
\vspace{-5mm}
\captionsetup[subfigure]{labelfont=scriptsize, textfont=tiny}
    \centering
    \subfloat[Original]{\includegraphics[width=0.16\textwidth]{imgs/assumptions/BN/original.png}}
    \subfloat[$\alpha_{\rm BN}$=0]{\includegraphics[width=0.16\textwidth]{imgs/assumptions/BN/reconstructed_train_train_bn=0.png}}
    \subfloat[$\alpha_{\rm BN}$=5e-4]{\includegraphics[width=0.16\textwidth]{imgs/assumptions/BN/reconstructed_train_train_bn=5e-4.png}}
    \subfloat[$\alpha_{\rm BN}$=1e-3]{\includegraphics[width=0.16\textwidth]{imgs/assumptions/BN/reconstructed_train_train_bn=1e-3.png}}
    \subfloat[$\alpha_{\rm BN}$=5e-3]{\includegraphics[width=0.16\textwidth]{imgs/assumptions/BN/reconstructed_train_train_bn=5e-3.png}}
    \subfloat[$\alpha_{\rm BN}$=1e-2]{\includegraphics[width=0.16\textwidth ]{imgs/assumptions/BN/reconstructed_train_train_bn=1e-2.png}}
    \caption{Attacking a batch of 16 CIFAR-10 images in $\rm BN_{infer}$ setting, with different coefficients for the BatchNorm regularizer ($\alpha_{\rm BN}$'s). $\alpha_{\rm TV}$=1e-3 gives the best reconstruction.}
    \label{fig:BN_reg_tune}
\end{figure}


\subsection{Details and more results for Section~\ref{sec:assumption}}

\paragraph{Attacking a single ImageNet image.} We launched the attack on ImageNet using the objective function in Eq.~\ref{eq:objective}, where $\alpha_{\rm TV}=0.1$, $\alpha_{\rm BN}=0.001$. We run the attack for 24,000 iterations using Adam optimizer, with initial learning rate 0.1, and decay the learning rate by a factor of $0.1$ at 
$3/8,5/8,7/8$ of training. We rerun the attack 5 times and present the best results measured by LPIPS in Figure~\ref{fig:BN_var}.

\paragraph{Qualitative and quantitative results for a more realistic attack.} We also present results of a more realistic attack in Table~\ref{tab:exp_summary_realistic} and Figure~\ref{fig:vis_recon_realistic}, where the attacker does {\em not} know BatchNorm statistics but knows the private labels. We assume the private labels to be known in this evaluation, because for those batches whose distribution of labels is uniform, the restoration of labels should still be quite accurate~\citep{yin2021see}.
As shown, in the evaluated setting, the attack is no longer effective when the batch size is 32 and Intra-InstaHide with $k=4$ is applied. The accuracy loss to stop the realistic attack is only around $3\%$ (compared to around $7\%$ to stop the strongest attack) .


\begin{figure}[H]
\captionsetup[subfigure]{font=small}
  \centering
  \subfloat{\includegraphics[width=\textwidth]{imgs/Compare_16_32.png}}
  \caption{Reconstruction results under different defenses for a more realistic setting (when the attacker knows private labels but does not know BatchNorm statistics). We also present the results for the strongest attack from Figure~\ref{fig:vis_recon} for comparison. Using Intra-InstaHide with $k=4$ and batch size 32 seems to stop the realistic attack.}
  \label{fig:vis_recon_realistic}
\end{figure}

\captionsetup[table]{font=small}
\begin{table}[H] 
  \scriptsize
  \setlength{\tabcolsep}{2.6pt}
  \renewcommand{\arraystretch}{0.95}
  \begin{tabular}{|l|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}
  \toprule
   &  \multirow{2}{*}{\bf None} & \multicolumn{6}{c|}{\multirow{2}{*}{\bf GradPrune ($p$)}} & \multicolumn{2}{c|}{\multirow{2}{*}{\bf MixUp ($k$)}} & \multicolumn{2}{c|}{\multirow{2}{*}{\bf Intra-InstaHide ($k$)}} & \multicolumn{2}{c|}{\bf GradPrune ($p=0.9$)}\\
   & & \multicolumn{6}{c|}{} & \multicolumn{2}{c|}{} & \multicolumn{2}{c|}{} & {\bf  + MixUp } & {\bf  + Intra-InstaHide}\\
  \midrule
   {\bf Parameter}  & - & 0.5 & 0.7 & 0.9 & 0.95 & 0.99 & 0.999 & 4 & 6 & 4 & 6 & $k=4$ & $k=4$ \\
   \midrule
   {\bf Test Acc.} & 93.37 & 93.19 & 93.01 & 90.57 & 89.92 & 88.61 & 83.58 &  92.31 & 90.41 & 90.04 & 88.20 & 91.37 & 86.10 \\
   \midrule
  {\bf Time (train)} & $1\times$ & \multicolumn{6}{c|}{$1.04\times$} & \multicolumn{2}{c|}{$1.06\times$} & \multicolumn{2}{c|}{$1.06\times$} & \multicolumn{2}{c|}{$1.10\times$} \\
  \midrule
  \multicolumn{14}{|c|}{\bf Attack batch size $= 16$, the strongest attack} \\
  \midrule
  {\bf Avg. LPIPS $\downarrow$}  & 0.41  & 0.41  & 0.42  & 0.46  & 0.48  & 0.50  & 0.55         & 0.50  & 0.49  & 0.69  & 0.69  & 0.62  & \best{0.73}\\
  {\bf Best LPIPS $\downarrow$}  & 0.21  & 0.22  & 0.27  & 0.29  & 0.30  & 0.29  & 0.48         & 0.31  & 0.28  & 0.56  & 0.56  & 0.37  & \best{0.65}\\
  {(LPIPS std.)}                 & 0.09  & 0.08  & 0.07  & 0.06  & 0.06  & 0.06  & 0.04         & 0.10  & 0.10  & 0.06  & 0.07  & 0.10  & 0.05\\
  \midrule
   \multicolumn{14}{|c|}{\bf Attack batch size $= 16$, attacker knows private labels but does not know BatchNorm statistics} \\
   \midrule
   {\bf Avg. LPIPS $\downarrow$}  & 0.49 & 0.51 & 0.48 & 0.51 & 0.52 & 0.56 & 0.60 & 0.71 & 0.71 & \best{0.75} & \best{0.75} & 0.74 &  0.74\\
   {\bf Best LPIPS $\downarrow$}  & 0.30 & 0.33 & 0.31 & 0.33 & 0.34 & 0.39 & 0.44 & 0.48 & 0.53 & \best{0.65} & 0.63 & 0.61 &  0.63\\
   {(LPIPS std.)}                 & 0.08 & 0.09 & 0.08 & 0.08 & 0.07 & 0.07 & 0.05 & 0.08 & 0.07 & 0.04 & 0.05 & 0.08 &  0.05\\
   \midrule
   \multicolumn{14}{|c|}{\bf Attack batch size $= 32$, the strongest attack} \\
  \midrule
  {\bf Avg. LPIPS $\downarrow$}  & 0.45  & 0.46  & 0.48  & 0.52  & 0.54  & 0.58  & 0.63         & 0.50  & 0.49  & 0.69  & 0.69  & 0.62  & \best{0.73}\\
   {\bf Best LPIPS $\downarrow$}  & 0.18  & 0.18  & 0.22  & 0.31  & 0.43  & 0.48  & 0.54         & 0.31  & 0.28  & 0.56  & 0.56  & 0.37  & \best{0.65}\\
   {(LPIPS std.)}                 & 0.11  & 0.11  & 0.09  & 0.07  & 0.05  & 0.04  & 0.04         & 0.10  & 0.10  & 0.06  & 0.07  & 0.10  & 0.05\\
    \midrule
   \multicolumn{14}{|c|}{\bf Attack batch size $= 32$, attacker knows private labels but does not know BatchNorm statistics} \\
   \midrule
   {\bf Avg. LPIPS $\downarrow$}  & 0.48 & 0.50 & 0.53 & 0.53 & 0.55 & 0.60 & 0.63 & 0.73 & 0.72 & 0.76 & 0.76 & 0.76 & \best{0.77} \\
   {\bf Best LPIPS $\downarrow$}  & 0.29 & 0.32 & 0.32 & 0.31 & 0.40 & 0.41 & 0.55 & 0.63 & 0.60 & \best{0.68} & 0.63 & 0.66 & 0.65\\
   {(LPIPS std.)}                 & 0.08 & 0.07 & 0.07 & 0.08 & 0.08 & 0.06 & 0.04 & 0.06 & 0.06 & 0.04 & 0.05 & 0.06 & 0.05\\
  \bottomrule
  \end{tabular}
  \vspace{2mm}
%   \subfloat{\includegraphics[width=0.98\textwidth]{imgs/Compare_16_32.png}}
  \caption{\small Utility-security trade-off of different defenses for a more realistic setting (when the attacker knows private labels but does not know BatchNorm statistics). We also present the results for the strongest attack from Table~\ref{tab:exp_summary} for comparison. We evaluate the attack on 50 CIFAR-10 images and report the LPIPS score ($\downarrow$: lower values suggest more privacy leakage).
  We mark the least-leakage defense measured by the metric in \best{green}.} 
  \label{tab:exp_summary_realistic}
\end{table}

\iffalse
\paragraph{Qualitative and quantitative results for private labels unknown.} Apart from the example in Figure~\ref{fig:reconstructed_labels} with batch size being 16, we provide another example for how unknown labels affect reconstruction quality in Figure~\ref{fig:assumption2_app}, with batch size being 32. We also provide quantitative measurements in Figure~\ref{tab:assumption2_app1} and~\ref{tab:assumption2_app2}.

\begin{figure}[H]
    \centering
    \subfloat[Reconstructions with and without private labels]{
    \includegraphics[width=0.95\textwidth]{imgs/assumptions/label_known_unknown_32.png}
    \label{fig:assumption2_app}
    }\\
    \subfloat[Batch size = 16]{
        \setlength{\tabcolsep}{4pt}
        \small
        \begin{tabular}[b]{|c|c|c|}
                \toprule
                  & {\bf Labels known} &  {\bf Labela unknown} \\
                \midrule
                %  {\bf Avg. PSNR $\uparrow$} & 12.45 & 12.01  \\
                %  {\bf Best PSNR $\uparrow$} & 17.42 & 14.85    \\
                 {\bf Avg. LPIPS $\downarrow$} & 0.44 & 0.58 \\
                 {\bf Best LPIPS $\downarrow$} & 0.25 & 0.32    \\
                \bottomrule
            \end{tabular}
        \label{tab:assumption2_app1}
        }
        \subfloat[Batch size = 32]{
        \setlength{\tabcolsep}{4pt}
        \small
        \begin{tabular}[b]{|c|c|c|}
                \toprule
                  & {\bf Labels known} &  {\bf Labels unknown} \\
                \midrule
                %  {\bf Avg. PSNR $\uparrow$} & 13.01 & 12.16 \\
                %  {\bf Best PSNR $\uparrow$} & 17.09 & 14.62    \\
                 {\bf Avg. LPIPS $\downarrow$} & 0.41 & 0.62 \\
                 {\bf Best LPIPS $\downarrow$} & 0.21 & 0.39    \\
                \bottomrule
            \end{tabular}
        \label{tab:assumption2_app2}
        }
    \caption{A reconstructed batch of 32 images with and without private labels known (a). We also provide quantitative measurements of reconstructions with batch size 16 (b) and 32 (c) ($\downarrow$: lower values suggest more leakage). The gradient inversion attack is weakened when private labels are not available.}
\end{figure}
\fi


% \iffalse
\subsection{More results for the strongest attack}

\paragraph{Full version of Figure~\ref{fig:vis_recon}.} Figure~\ref{fig:vis_recon_full} provides more examples for reconstructed images by the strongest attack under different defenses and batch sizes. 



\begin{figure}[H]
\captionsetup[subfigure]{font=small}
  \centering
  \vspace{-12mm}
  \subfloat[Batch size $=1$]{\includegraphics[width=\linewidth]{imgs/recon_vis_bs=1_BN_exact_small.png}}\\
  \vspace{-3mm}
  \subfloat[Batch size $=16$]{\includegraphics[width=\linewidth]{imgs/recon_vis_bs=16_BN_exact_small.png}}\\
  \vspace{-3mm}
  \subfloat[Batch size $=32$]{\includegraphics[width=\linewidth]{imgs/recon_vis_bs=32_BN_exact_small.png}}\\
  \vspace{-2mm}
  \caption{Reconstruction results under different defenses with batch size 1 (a), 16 (b) and 32 (c). Full version of Figure~\ref{fig:vis_recon}.}
  \label{fig:vis_recon_full}
  \vspace{-2mm}
\end{figure}


\paragraph{Results with MNIST dataset.} We’ve repeated our main evaluation of defenses and attacks (Table~\ref{tab:exp_summary}) on MNIST dataset~\citep{deng2012mnist} with a simple 6-layer ConvNet model. Note that the simple ConvNet does not contain BatchNorm layers. We evaluate the following defenses on the MNIST dataset with a 6-layer ConvNet architecture against the strongest attack (private labels known):

\begin{itemize}
    \item GradPrune (gradient pruning): gradient pruning sets gradients of small magnitudes to zero. We vary the pruning ratio $p$ in \{0.5, 0.7, 0.9, 0.95, 0.99, 0.999, 0.9999\}.
    \item MixUp: we vary $k$ in \{4,6\}, and set the upper bound of a single coefficient to 0.65 (coefficients sum to 1).
    \item Intra-InstaHide: we vary $k$ in \{4,6\}, and set the upper bound of a single coefficient to 0.65 (coefficients sum to 1). 
    \item A combination of GradPrune and MixUp/Intra-InstaHide.
\end{itemize}

We run the evaluation against the strongest attack and batch size 1 to estimate the upper bound of privacy leakage. Specifically, we assume the attacker knows private labels, as well as the indices of mixed images and mixing coefficients for MixUp and Intra-InstaHide. 

\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{imgs/appendix/recon_vis_MNIST.png}
    \caption{Reconstruction results of MNIST digits under different defenses with the strongest atttack and batch size 1.}
    \label{fig:vis_recon_MNIST}
    \vspace{-5mm}
\end{figure}

For MNIST with a simple 6-layer ConvNet, defending the strongest attack with gradient pruning may require the pruning ratio $p\geq 0.9999$. MixUp with $k=4$ or $k=6$ are not sufficient to defend the gradient inversion attack. Combining MixUp ($k=4$) with gradient pruning ($p=0.99$) improves the defense, however, the reconstructed digits are still highly recognizable. Intra-InstaHide alone ($k=4$ or $k=6$) gives a bit better defending performance than MixUp and GradPrune. Combining InstaHide ($k=4$) with gradient pruning ($p=0.99$) further improves the defense and makes the reconstruction almost unrecognizable. 




\subsection{More results for encoding-based defenses}
We visualize the whole reconstructed dataset under MixUp and Intra-InstaHide defenses with different batch sizes in Figure~\ref{fig:encode_bs1}, \ref{fig:encode_bs16} and \ref{fig:encode_bs32}.  Sample results of the original and the reconstructed batches are provided in Figure~\ref{fig:mixup_vs_instahide}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{imgs/appendix/mixup_vs_instahide.png}
    \caption{Original and reconstructed batches of 16 images under MixUp and Intra-InstaHide defenses. We visualize both the original and the absolute images for the Intra-InstaHide defense. Intra-InstaHide makes pixel-wise matching harder.}
    \label{fig:mixup_vs_instahide}
    \vspace{-5mm}
\end{figure}

\begin{figure}[H]
\captionsetup[subfigure]{labelfont=scriptsize, textfont=tiny}
    \centering
    \subfloat[Original]{\includegraphics[width=0.23\textwidth]{imgs/decode_res/InstaHide/bs1_k4/originals.png}} \hspace{1mm}
    \subfloat[MixUp, $k$=4]{\includegraphics[width=0.23\textwidth]{imgs/decode_res/Mixup/bs1_k4/grad_decode.png}} \hspace{1mm}
    \subfloat[MixUp, $k$=6]{\includegraphics[width=0.23\textwidth]{imgs/decode_res/Mixup/bs1_k6/grad_decode.png}} \hspace{1mm}
    \subfloat[MixUp+GradPrune, $k$=4, $p$=0.9]{\includegraphics[width=0.23\textwidth]{imgs/decode_res/Mixup/bs1_k4_gradprune/grad_decode.png}}
    
    \subfloat[Original]{\includegraphics[width=0.23\textwidth]{imgs/decode_res/InstaHide/bs1_k4/originals.png}} \hspace{1mm}
    \subfloat[InstaHide, $k$=4]{\includegraphics[width=0.23\textwidth]{imgs/decode_res/InstaHide/bs1_k4/grad_decode.png}} \hspace{1mm}
    \subfloat[InstaHide, $k$=6]{\includegraphics[width=0.23\textwidth]{imgs/decode_res/InstaHide/bs1_k6/grad_decode.png}} \hspace{1mm}
    \subfloat[InstaHide+GradPrune, $k$=4, $p$=0.9]{\includegraphics[width=0.23\textwidth]{imgs/decode_res/InstaHide/bs1_k4_gradprune/grad_decode.png}}
    \caption{Reconstrcuted dataset under MixUp and Intra-InstaHide against the strongest attack (batch size is 1).}
    \label{fig:encode_bs1}
    \vspace{-10mm}
\end{figure}


\begin{figure}[H]
\captionsetup[subfigure]{labelfont=scriptsize, textfont=tiny}
    \centering
    \subfloat[Original]{\includegraphics[width=0.23\textwidth]{imgs/decode_res/InstaHide/bs1_k4/originals.png}} \hspace{1mm}
    \subfloat[MixUp, $k$=4]{\includegraphics[width=0.23\textwidth]{imgs/decode_res/Mixup/bs16_k4/grad_decode.png}} \hspace{1mm}
    \subfloat[MixUp, $k$=6]{\includegraphics[width=0.23\textwidth]{imgs/decode_res/Mixup/bs16_k6/grad_decode.png}} \hspace{1mm}
    \subfloat[MixUp+GradPrune, $k$=4, p=0.9]{\includegraphics[width=0.23\textwidth]{imgs/decode_res/Mixup/bs16_k4_gradprune/grad_decode.png}}

    
    \subfloat[Original]{\includegraphics[width=0.23\textwidth]{imgs/decode_res/InstaHide/bs1_k4/originals.png}} \hspace{1mm}
    \subfloat[InstaHide, $k$=4]{\includegraphics[width=0.23\textwidth]{imgs/decode_res/InstaHide/bs16_k4/grad_decode.png}} \hspace{1mm}
    \subfloat[InstaHide, $k$=6]{\includegraphics[width=0.23\textwidth]{imgs/decode_res/InstaHide/bs16_k6/grad_decode.png}} \hspace{1mm}
    \subfloat[InstaHide+GradPrune, $k$=4, $p$=0.9]{\includegraphics[width=0.23\textwidth]{imgs/decode_res/InstaHide/bs16_k4_gradprune/grad_decode.png}}
    \caption{Reconstrcuted dataset under MixUp and Intra-InstaHide against the strongest attack (batch size is 16).}
    \label{fig:encode_bs16}
\end{figure}



\begin{figure}[H]
\captionsetup[subfigure]{labelfont=scriptsize, textfont=tiny}
    \centering
    \subfloat[Original]{\includegraphics[width=0.23\textwidth]{imgs/decode_res/InstaHide/bs1_k4/originals.png}} \hspace{1mm}
    \subfloat[MixUp, $k$=4]{\includegraphics[width=0.23\textwidth]{imgs/decode_res/Mixup/bs32_k4/grad_decode.png}} \hspace{1mm}
    \subfloat[MixUp, $k$=6]{\includegraphics[width=0.23\textwidth]{imgs/decode_res/Mixup/bs32_k6/grad_decode.png}} \hspace{1mm}
    \subfloat[MixUp+GradPrune, $k$=4, $p$=0.9]{\includegraphics[width=0.23\textwidth]{imgs/decode_res/Mixup/bs32_k4_gradprune/grad_decode.png}}

    
    \subfloat[Original]{\includegraphics[width=0.23\textwidth]{imgs/decode_res/InstaHide/bs1_k4/originals.png}} \hspace{1mm}
    \subfloat[InstaHide, $k$=4]{\includegraphics[width=0.23\textwidth]{imgs/decode_res/InstaHide/bs32_k4/grad_decode.png}} \hspace{1mm}
    \subfloat[InstaHide, $k$=6]{\includegraphics[width=0.23\textwidth]{imgs/decode_res/InstaHide/bs32_k6/grad_decode.png}} \hspace{1mm}
    \subfloat[InstaHide+GradPrune, $k$=4, $p$=0.9]{\includegraphics[width=0.23\textwidth]{imgs/decode_res/InstaHide/bs32_k4_gradprune/grad_decode.png}}
    \caption{Reconstrcuted dataset under MixUp and Intra-InstaHide against the strongest attack (batch size is 32).}
    \label{fig:encode_bs32}
\end{figure}






\input{problem}