\section{Evaluation of defenses}
\label{sec:exp}

The main goal of our experiments is to understand the trade-offs between data utility (accuracy) and securely defending the state-of-the-art gradient inversion attack even in its strongest setting, \textit{without} any relaxation of its implicit assumptions. Specifically, we grant the attacker the knowledge of 1) BatchNorm statistics of the private batch, and 2) labels of the private batch. 

We vary key parameters for each defense, and evaluate their performance in terms of the test accuracy, computation overhead, and privacy risks (Section~\ref{sec:exp_single}). We then investigate the feasibility of combining defenses (Section~\ref{sec:exp_combine}). We also estimate the computation cost of end-to-end recovery of a single image under evaluated defenses (Section~\ref{sec:exp_estimate}).

As the feasibility of the state-of-the-art attack~\citep{geiping2020inverting} on a batch of high-resolution images remain elusive when its implicit assumptions no longer hold (see Figure~\ref{fig:BN_var}), we focus on the evaluation with low-resolution in trying to understand whether current attacks can be mitigated.


\subsection{Experimental setup} 
\label{sec:exp_setup}

\paragraph{Key parameters of defenses.} We evaluate following defenses on CIFAR-10 dataset \citep{cifar10} with ResNet-18 architecture \citep{he2015resnet}. 

\begin{itemize}
  \item {\bf GradPrune} (gradient pruning): gradient pruning set gradients of small magnitudes to zero. We vary the pruning ratio $p$ in $\{0.5, 0.7, 0.9, 0.95, 0.99, 0.999\}$. 
%   \vspace{-1mm}
  \item {\bf MixUp}: MixUp encodes a private image by linearly combining it with $k-1$ other images from the training set. Following~\citep{huang2020instahide}, we vary $k$ in $\{4, 6\}$, and set the upper bound of a single coefficient to $0.65$ (coefficients sum to $1$). 
%   \vspace{-1mm}
  \item {\bf Intra-InstaHide}: InstaHide ~\citep{huang2020instahide} proposes two versions: Inter-InstaHide and Intra-InstaHide. The only difference is that at the mixup step, Inter-Instahide mixes up an image with images from a public dataset, whereas Intra-InstaHide only mixes with private images. Both versions apply a random sign flipping pattern on each mixed image.
  We evaluate Intra-InstaHide in our experiments, which is a weaker version of InstaHide. Similar to the evaluation of MixUp, we vary $k$ in $\{4, 6\}$, and set the upper bound of a single coefficient to $0.65$. Note that InstaHide flips signs of pixels in the image, which destroys the total variation prior. However, the absolute value of adjacent pixels should still be close. Therefore, for the InstaHide defense, we apply the total variation regularizer on $|x|$, i.e. taking absolute value of each pixel in the reconstruction.
\end{itemize}


We train the ResNet-18 architecture on CIFAR-10 using different defenses, and launch the attack. We provide more details of the experiments in Appendix~\ref{sec:app_exp}. 

\paragraph{The attack.} We use a subset of 50 CIFAR-10 images to evaluate the attack performance. Note that attacking MixUp and InstaHide involves another step to decode private images from the encoded images. We apply \citep{carlini2020attack}'s attack here as the decode step, where the attacker needs to eavesdrop $T$ epochs of training, instead of a single training step. We set $T=20$ in our evaluation. We also grant the attacker the strongest power for the decode step to evaluate the upper bound of privacy leakage. Given a MixUp or Intra-InstaHide image which encodes $k$ private images, we assume the attacker knows:
\begin{enumerate}
% \vspace{-2mm}
  \item The indices of $k$ images in the private dataset. In a realistic scenario, the attacker of \citep{carlini2020attack} would need to train a neural network to detect similarity of encodings, and run a combinatorial algorithm to solve {\em an approximation of} this mapping.
%   \vspace{-2mm}
  \item The mixing coefficients for each of the $k$ private image. In real Federated learning, this information is \textit{not available}.
% \vspace{-2mm}
\end{enumerate}


\paragraph{Hyper-parameters of the attack.} The attack minimize the objective function given in Eq.\ref{eq:objective}. 
We search $\alpha_{\rm TV}$ in $\{0, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5\}$ for all defenses, and apply the best choice for each defense: $0.05$ for GradPrune, $0.1$ for MixUp, and $0.01$ for Intra-InstaHide. We apply $\alpha_{\rm BN}=0.001$ for all defenses after searching it in $\{0, 0.0005, 0.001, 0.01, 0.05, 0.01\}$. We optimize the attack for $10,000$ iterations using Adam~\citep{kingma2014adam}, with initial learning rate $0.1$. We decay the learning rate by a factor of $0.1$ at $3/8, 5/8, 7/8$ of the optimization. 

\paragraph{Batch size of the attack.} \citep{zhu2020deep, geiping2020inverting} have shown that a small batch size is important for the success of the attack. We intentionally evaluate the attack with three small batch sizes to test the upper bound of privacy leakage, including the minimum (and unrealistic) batch size 1, and two small but realistic batch sizes, 16 and 32.

\paragraph{Metrics for reconstruction quality.} We visualize reconstructions obtained under different defenses. Following~\citep{yin2021see}, we also use the learned perceptual image patch similarity (LPIPS) score~\citep{zhang2018unreasonable} to measure mismatch between reconstruction and original images: higher values suggest more mismatch (less privacy leakage).




\begin{figure}[ht]
  \centering
  \includegraphics[width=\linewidth]{imgs/recon_vis_combined.png}
  \caption{Reconstruction results under different defenses with batch size being 1, 16 and 32. When batch size is 32, combining gradient pruning and Intra-InstaHide makes the reconstruction almost unrecognizable (the last column). See Figure~\ref{fig:vis_recon_full} in Appendix~\ref{sec:app_exp} for the full version.}
  \label{fig:vis_recon}
%   \vspace{2mm}
\end{figure}

\newcommand{\best}[1]{\textcolor{green}{\textbf{#1}}}
% \captionsetup[table]{font=small}
\begin{table}[t] 
  \scriptsize
  \setlength{\tabcolsep}{2.8pt}
  \renewcommand{\arraystretch}{0.95}
  \begin{tabular}{|l|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}
  \toprule
   &  \multirow{2}{*}{\bf None} & \multicolumn{6}{c|}{\multirow{2}{*}{\bf GradPrune ($p$)}} & \multicolumn{2}{c|}{\multirow{2}{*}{\bf MixUp ($k$)}} & \multicolumn{2}{c|}{\multirow{2}{*}{\bf Intra-InstaHide ($k$)}} & \multicolumn{2}{c|}{\bf GradPrune ($p=0.9$)}\\
   & & \multicolumn{6}{c|}{} & \multicolumn{2}{c|}{} & \multicolumn{2}{c|}{} & {\bf  + MixUp } & {\bf  + Intra-InstaHide}\\
  \midrule
   {\bf Parameter}  & - & 0.5 & 0.7 & 0.9 & 0.95 & 0.99 & 0.999 & 4 & 6 & 4 & 6 & $k=4$ & $k=4$ \\
   \midrule
   {\bf Test Acc.} & 93.37 & 93.19 & 93.01 & 90.57 & 89.92 & 88.61 & 83.58 &  92.31 & 90.41 & 90.04 & 88.20 & 91.37 & 86.10 \\
   \midrule
   {\bf Time (train)} & $1\times$ & \multicolumn{6}{c|}{$1.04\times$} & \multicolumn{2}{c|}{$1.06\times$} & \multicolumn{2}{c|}{$1.06\times$} & \multicolumn{2}{c|}{$1.10\times$} \\
   \midrule
   \multicolumn{14}{|c|}{\bf Attack batch size $= 1$} \\
   \midrule
   {\bf Avg. LPIPS $\downarrow$}  & 0.19  & 0.19  & 0.22  & 0.35  & 0.42  & 0.52  & 0.52  & 0.34  & 0.46  & 0.58  & \best{0.61}  & 0.41  & 0.60\\
   {\bf Best LPIPS $\downarrow$}  & 0.02  & 0.02  & 0.05  & 0.14  & 0.22  & 0.32  & 0.36  & 0.12  & 0.25  & 0.41  & 0.42  & 0.21  & \best{0.43}\\
   {(LPIPS std.)}                 & 0.16  & 0.17  & 0.16  & 0.13  & 0.11  & 0.08  & 0.06  & 0.08  & 0.07  & 0.06  & 0.09  & 0.07  & 0.09\\
   \midrule
   \multicolumn{14}{|c|}{\bf Attack batch size $= 16$} \\
   \midrule
   {\bf Avg. LPIPS $\downarrow$}  & 0.45  & 0.46  & 0.47  & 0.51  & 0.55  & 0.58  & 0.61    & 0.34  & 0.31  & 0.62  & 0.63  & 0.46  & \best{0.68}\\
   {\bf Best LPIPS $\downarrow$}  & 0.18  & 0.19  & 0.19  & 0.31  & 0.43  & 0.47  & 0.51    & 0.11  & 0.13  & 0.41  & 0.44  & 0.22  & \best{0.54}\\
   {(LPIPS std.)}                 & 0.12  & 0.12  & 0.11  & 0.07  & 0.05  & 0.04  & 0.03    & 0.09  & 0.09  & 0.08  & 0.08  & 0.10  & 0.07\\
   \midrule
   \multicolumn{14}{|c|}{\bf Attack batch size $= 32$} \\
   \midrule
   {\bf Avg. LPIPS $\downarrow$}  & 0.45  & 0.46  & 0.48  & 0.52  & 0.54  & 0.58  & 0.63         & 0.50  & 0.49  & 0.69  & 0.69  & 0.62  & \best{0.73}\\
   {\bf Best LPIPS $\downarrow$}  & 0.18  & 0.18  & 0.22  & 0.31  & 0.43  & 0.48  & 0.54         & 0.31  & 0.28  & 0.56  & 0.56  & 0.37  & \best{0.65}\\
   {(LPIPS std.)}                 & 0.11  & 0.11  & 0.09  & 0.07  & 0.05  & 0.04  & 0.04         & 0.10  & 0.10  & 0.06  & 0.07  & 0.10  & 0.05\\
  \bottomrule
  \end{tabular}
  \subfloat{\includegraphics[width=0.45\linewidth]{imgs/defenses_avg.pdf}}
  \hspace{7mm}
  \subfloat{\includegraphics[width=0.45\linewidth]{imgs/defenses_best.pdf}}
  \caption{\small Utility-security trade-off of different defenses. We train the ResNet-18 model on the whole CIFAR-10 dataset, and report the averaged test accuracy and running time of 5 independent runs. We evaluate the attack on a subset of 50 CIFAR-10 images, and report the LPIPS score ($\downarrow$: lower values suggest more privacy leakage). We mark the least-leakage defense measured by the metric in \best{green}.} 
  \label{tab:exp_summary}
  \vspace{-5mm}
\end{table}


\subsection{Performance of defense methods} 
\label{sec:exp_single}

We summarize the performance of each defense in Table~\ref{tab:exp_summary},  and visualize reconstructed images in Figure~\ref{fig:vis_recon}. We report the averaged and the best results for the metric of reconstruction quality, as a proxy for average-case and worst-case privacy leakage.

\paragraph{No defense.} Without any defense, when batch size is 1, the attack can recover images  well from the gradient. Increasing the batch size makes it difficult to recover well, but the recovered images are visually similar to the originals (see Figure~\ref{fig:vis_recon}). 


\paragraph{Gradient pruning (GradPrune).}
Figure~\ref{fig:vis_recon} shows that as the pruning ratio $p$ increases, there are more artifacts in the reconstructions. However, the reconstructions are still recognizable even when the pruning ratio $p=0.9$, thus the previous suggestion of using $p=0.7$ by \citep{zhu2020deep} is no longer safe against the state-of-the-art attack. Our results suggest that, for CIFAR-10, defending the strongest attack with gradient pruning may require the pruning ratio $p\geq 0.999$. As a trade-off, such a high pruning ratio would introduce an accuracy loss of around $10\%$ (see Table~\ref{tab:exp_summary}).  


\paragraph{MixUp.} MixUp introduces a small computational overhead to training. MixUp with $k=4$ only has a minor impact (\textasciitilde$2\%$) on test accuracy, but it is not sufficient to defend the gradient inversion attack (see Figure~\ref{fig:vis_recon}). Increasing $k$ from $4$ to $6$ slightly reduces the leakage, however, the reconstruction is still highly recognizable.  This suggests that MixUp alone may not be a practical defense against the state-of-the-art gradient inversion attack. 


\paragraph{Intra-InstaHide.} Intra-InstaHide with $k=4$ incurs an extra \textasciitilde$2\%$ accuracy loss compared with MixUp, but it achieves better defense performance: when batch size is 32, there are obvious artifacts and color shift in the reconstruction (see Figure~\ref{fig:vis_recon}). However, with batch size 32,  Intra-InstaHide alone also cannot defend the state-of-the-art gradient inversion, as structures of private images are still vaguely identifiable in reconstructions.

Appendix~\ref{sec:app_exp} provides the whole reconstructed dataset under MixUp and Intra-InstaHide.


\subsection{Performance of combined defenses} 
\label{sec:exp_combine}


We notice that two types of defenses (i.e perturbing gradient and encoding inputs) are complementary to each other, which motivates an evaluation of combining gradient pruning with MixUp or  Intra-InstaHide.

As shown in Figure~\ref{fig:vis_recon}, when the batch size is 32, combining  Intra-InstaHide ($k=4$) with gradient pruning ($p=0.9$) makes the reconstruction almost unrecognizable. The combined defense yields a higher LPIPS score than using gradient pruning with $p=0.999$, but introduces a smaller accuracy loss (\textasciitilde$7\%$ compared with a no-defense pipeline). 

Note that our evaluation uses {\em the strongest attack} and {\em relatively small batch sizes}. As shown in Appendix~\ref{sec:app_exp}, invalidating assumptions in Section~\ref{sec:assumption} or increasing the batch size may hinder the attack with an even weaker defense (e.g. with a lower $p$, or smaller $k$), which gives a better accuracy. 


\subsection{Time estimate for end-to-end recovery of a single image}
\label{sec:exp_estimate}

Table~\ref{tab:time_estimate} shows time estimates for the end-to-end recovery of a single image in a Federated learning setting with GradPrune or InstaHide defense.  
We do not estimate for MixUp since it has been shown to be a weak defense (see Section~\ref{sec:exp_single}). %We consider the case where the whole dataset is recovered.

Our time estimates consider three fairly small dataset sizes.  The largest size in our estimate is a small fraction of a dataset of ImageNet scale.  We consider a client holds a dataset of $N$ private images and participates in Federated learning, which trains a ResNet-18 model with batch size $b=128$. Assumes that the resolution of the client's data is $32\times 32 \times 3$. If the attacker uses a single NVIDIA GeForce RTX 2080 Ti GPU as his computation resource, and runs gradient inversion with 10,000 iterations of optimization,  then $t$, the running time for attacking a single batch is \textasciitilde$0.25$ GPU hours (batch size $b$ has little impact on the attack's running time, but a larger $b$ makes the attack less effective).


\paragraph{Non-defended and gradient pruning.} Recovering a single image in a non-defended pipeline (or a pipeline that applies gradient pruning alone as the defense) only requires the attacker to invert gradient of a single step of training, which takes time $t$. 


\paragraph{InstaHide.} When InstaHide is applied, the current attack~\citep{carlini2020attack} suggests that recovering a single image would involve recovering the whole dataset first. As discussed in Section~\ref{sec:defense}, Carlini et al.'s attack consists of two steps: 1) recover InstaHide images from gradient of $T$ epochs. This would take $ (NT/b) \times t$ GPU hours. 2) Run the decode attack \citep{carlini2020attack} on InstaHide images to recover the private dataset, which involves:

\begin{enumerate}
    \item[2a]  Train a neural network to detect similarity in recovered InstaHide images. Assume that training the network requires at least $n$ recovered InstaHide images, then collecting these images by running gradient inversion would take $(n/b) \times t$ GPU hours. The training takes $10$ GPU hours according to \citep{carlini2020attack}, so training the similarity network would take $(n/b) \times t + 10$ GPU hours in total.
    % \vspace{-2mm}
    \item[2b] Run the combinotorial algorithm to recover original images. Running time of this step has been shown to be at least quadratic in $m$, the number of InstaHide encodings~\citep{chen2020instahide}. This step takes $1/6$ GPU hours with $m=5\times 10 ^{3}$. Therefore for $m=NT$, the running time is at least $1/6 \times (\frac{NT}{5\times 10 ^{3}})^2$ GPU hours.
\end{enumerate}

In total, an attack on InstaHide in this real-world setting would take $ (NT/b) \times t + (n/b) \times t + 10 + 1/6 \times (\frac{NT}{5\times 10 ^{3}})^2$ GPU hours. We use $T=50$ (used by \citep{carlini2020attack}), $n=10,000$ and give estimate in Table~\ref{tab:time_estimate}. As shown, when InstaHide is applied on a small dataset ($N=5,000$), the end-to-end recovery of a single image takes $>3,000\times$ longer than in a no-defense pipeline or GradPrune pipeline; when InstaHide is applied on a larger dataset ($N=500,000$), the computation cost for end-to-end recovery is enormous.

\begin{table}[t]
    \centering
    \footnotesize
    \setlength{\tabcolsep}{10pt}
    \renewcommand{\arraystretch}{0.95}
    \begin{tabular}{|c|c|c|c|}
    \toprule
    {\bf Size of client's dataset ($N$)} & {\bf No defense} & {\bf GradPrune} & {\bf InstaHide} \\
    \midrule
    5,000 & \multirow{3}{*}{0.25} & \multirow{3}{*}{0.25} & 934.48 \\
    50,000 & & & 46,579.01 ($\approx$ 5.5 GPU years)\\
    500,000 & & & 4,215,524.32 ($\approx$ 493.4 GPU years)\\
    \bottomrule
    \end{tabular}
    \vspace{2mm}
    \caption{Time estimates (NVIDIA GeForce RTX 2080 Ti GPU hours) of recovering {\em a single image} from the client's dataset using the state-of-the-art gradient inversion attack~\citep{geiping2020inverting} under different defenses. We assume image resolution of the client's data is $32\times 32 \times 3$. }
    \label{tab:time_estimate}
    \vspace{-5mm}
\end{table}