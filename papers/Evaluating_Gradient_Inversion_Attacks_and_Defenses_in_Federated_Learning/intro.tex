
\section{Introduction}

Federated learning~\citep{mcmahan2016communication,kairouz2019advances} is a framework that allows multiple clients in a distributed environment to collaboratively train a neural network model at a central server, without moving their data to the central server. 

At every training step, each client computes a model update ---i.e., gradient--- on its local data using the latest copy of the global model, and then sends the gradient to the central server. The server aggregates these updates (typically by averaging) to construct a global model, and then sends the new model parameters to all clients. 
%avoids the cost of raw data transferring, and also 
By allowing clients to participate in training without directly sharing their data, such protocols align better with data privacy regulations such as Health Insurance Portability and Accountability Act (HIPPA)~\citep{hippa}, California Consumer Privacy Act (CCPA)~\citep{ccpa}, and General Data Protection Regulation~\citep{gdpr}.

While  sharing gradients was thought to leak little information about the client's private data, recent papers~\citep{zhu2020deep, zhao2020idlg, geiping2020inverting, yin2021see}  developed a  ``gradient inversion attack'' by which an attacker eavesdropping on a client's communications with the server can begin to reconstruct the client's private data. 
The attacker can also be a malicious participant in the Federated Learning scheme, including a honest-but-curious server who wishes to reconstruct private data of clients, or a honest-but-curious client who wishes to reconstruct private data of other clients. These attacks have been shown to work with batch sizes only up to $100$ but even so they have created doubts about the level of privacy ensured in Federated Learning. The current paper seeks to evaluate the risks and suggest ways to minimize them. 

Several defenses against gradient inversion attacks have been proposed. These include perturbing gradients~\citep{zhu2020deep, wei2020framework} and using transformation for training data that clients can apply on the fly~\citep{zhang2017mixup, huang2020instahide}. More traditional cryptographic ideas including secure aggregation \citep{bonawitz2016practical} or homomorphic encryption \citep{phong18} for the gradients can also be used and presumably stop any eavesdropping attacks completely. They will not be studied here due to their special setups and overhead. 

We are not aware of a prior systematic evaluation of the level of risk arising from current attacks and the level of security provided by various defenses, as well as the trade-off (if any) between test accuracy, computation overhead, and privacy risks.

The paper makes two main contributions. First, we draw attention to two strong assumptions that a current gradient inversion attack~\citep{geiping2020inverting} implicitly makes. We show that by nullifying these assumptions, the performance of the attack drops significantly and can only work for low-resolution images. 
The findings are explored in  Section~\ref{sec:assumption} and already imply some more secure configurations in Federated Learning (Section~\ref{sec:discussion}).


Second, we summarize various defenses (Section~\ref{sec:defense}) and systematically evaluate (Section~\ref{sec:exp}) some of their performance of defending against a state-of-the-art gradient inversion attack, and present their data utility and privacy leakage trade-offs. We estimate the computation cost of end-to-end recovery of a single image under each evaluated defense. We also experimentally demonstrate the feasibility and effectiveness of combined defenses. Our findings are summarized as strategies to further improve Federated Learning's security against gradient inversion attacks (Section~\ref{sec:discussion}).

In Appendix~\ref{sec:app_theory}, we provide theoretical insights for mechanism of each evaluated defense. 
