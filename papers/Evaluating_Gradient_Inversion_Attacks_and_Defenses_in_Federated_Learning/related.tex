\section{Gradient Inversion Attacks}

Previous studies have shown the feasibility of recovering input from gradient (i.e. gradient inversion) for image classification tasks, by formulating it as an optimization problem: given a neural network with parameters $\theta$, and the gradient $ \nabla_\theta \mathcal{L}_{\theta}(x^*, y^*)$ computed with a private data batch $(x^*, y^*) \in \mathbb{R}^{b \times d} \times \mathbb{R}^{b}$ ($b, d$ being the batch size, image size), the attacker tries to recover $x \in \mathbb{R}^{b \times d}$, an approximation of $x^*$:

\begin{equation}
    \arg \min _{x}  \mathcal{L}_{\rm grad}(x; \theta, \nabla_{\theta} \mathcal{L}_{\theta}(x^{*}, y^{*})) + \alpha \mathcal{R}_{\rm aux}(x)
\end{equation}


The optimization goal consists of two parts: $\mathcal{L}_{\rm grad}(x; \theta, \nabla_{\theta} \mathcal{L}_{\theta}(x^{*}, y^{*}))$ enforces matching of the gradient of recovered batch $x$ with the provided gradients $\mathcal{L}_{\theta}(x^*, y^*)$, and $\mathcal{R}_{\rm aux}(x)$ regularizes the recovered image based on image prior(s). 

\citep{phong_17} brings theoretical insights on this task by proving that such reconstruction is possible with a single-layer neural network. 
\citep{zhu2020deep} is the first to show that accurate pixel-level reconstruction is practical for a maximum batch size of 8. Their formulation uses $\ell_2$-distance as $\mathcal{L}_{\rm grad}(\cdot, \cdot)$ but no regularization term $\mathcal{R}_{\rm aux}(x)$. The approach works for low-resolution CIFAR datasets~\citep{cifar10}, with simple neural networks with sigmoid activations, but cannot scale up to high-resolution images, or larger models with ReLU activations. A follow-up~\citep{zhao2020idlg} proposes a simple approach to extract the ground-truth labels from the gradient, which improves the attack but still cannot overcome its limitations.  

With a careful choice of $\mathcal{L}_{\rm grad}$ and $\mathcal{R}_{\rm aux}(x)$, \citep{geiping2020inverting} substantially improves the attack and succeeds in recovering a single ImageNet~\citep{deng2009imagenet} image from gradient: their approach uses cosine distance as $\mathcal{L}_{\rm grad}$, and the total variation as $\mathcal{R}_{\rm aux}(x)$. Their approach is able to reconstruct low-resolution images with a maximum batch size of 100 or a single high-resolution image. Based on~\citep{geiping2020inverting}, \citep{wei2020framework} analyzes how different configurations in the training may affect attack effectiveness. 

A more recent work~\citep{yin2021see} further improves the attack on high-resolution images, by introducing to $\mathcal{R}_{\rm aux}(x)$ a new image prior term based on batch normalization~\citep{ioffe2015batch} statistics,  and a regularization term which enforces consistency across multiple attack trials. 

An orthogonal line of work~\citep{zhu2020r} proposes to formulate the gradient inversion attack as a closed-form recursive procedure, instead of an optimization problem. However, their implementation can recover only low-resolution images under the setting where batch size = 1. 

\section{Strong Assumptions Made by SOTA Attacks}
\label{sec:assumption}

\subsection{The state-of-the-art attacks} 

Two recent attacks~\citep{geiping2020inverting,yin2021see} achieve best recovery results.  Our analysis focuses on the former as the implementation of the latter is not available at the time of writing this paper.
%Throughout this paper, we refer to as the state-of-the-art attack and focus on its evaluation.  
We plan to include the analysis for the latter attack in the final version of this paper if its implementation becomes available.

\citep{geiping2020inverting}'s attack optimizes the following objective function:
\begingroup
% \small
\begin{equation}
    \arg \min _{x}  1-\frac{\left\langle\nabla_{\theta} \mathcal{L}_{\theta}(x, y), \nabla_{\theta} \mathcal{L}_{\theta}\left(x^{*}, y^{*}\right)\right\rangle}{\left\|\nabla _ { \theta } \mathcal { L } _ { \theta } ( x , y ) \left|\left\|\mid \nabla_{\theta} \mathcal{L}_{\theta}\left(x^{*}, y^{*}\right)\right\|\right.\right.}+ \alpha_{\rm TV} \mathcal{R}_{\rm TV}(x)
\end{equation}
\endgroup

where $\langle \cdot, \cdot \rangle$ is the inner-product between vectors, and $\mathcal{R}_{\rm TV}(\cdot)$ is the total variation of images.

We notice that Geiping et al. has made two strong assumptions (Section~\ref{sec:assumption_explain}). Changing setups to invalidate those assumptions will substantially weaken the attacks (Section~\ref{sec:assumption_invalidate}). We also summarize whether other attacks have made similar assumptions in Table~\ref{tab:assumption_summary}.


\subsection{Strong assumptions} 
\label{sec:assumption_explain}


We find that previous gradient inversion attacks have made different assumptions about whether the attacker knows Batch normalization statistics or private labels, as shown in  Table~\ref{sec:assumption_explain}. 
Note that \citep{geiping2020inverting}'s attack makes both strong assumptions.


\paragraph{Assumption 1: Knowing BatchNorm statistics.} Batch normalization (BatchNorm) \citep{ioffe2015batch} is a technique for training neural networks that normalizes the inputs to a layer for every mini-batch. It behaves differently during training and evaluation. Assume the model has $L$ batch normalization layers. Given $x^*$, a batch of input images, we use $x^*_l$ to denote the input features to the $l$-th BatchNorm layer, where $l \in [L]$. During training, the $l$-th BatchNorm layer normalizes $x^*_l$ based on the batch's mean ${\rm mean}(x^*_l)$ and variance ${\rm var}(x^*_l)$, and keeps a running estimate of mean and variance of all training data points, denoted by $\mu_l$ and $\sigma^2_l$. 
During inference, $\{\mu_l\}_{l=1}^L$ and $\{\sigma^2_l\}_{l=1}^L$ are used to normalize test images. In the following descriptions, we leave out $\{\cdot\}_{l=1}^L$ for simplicity (i.e. use $\mu, \sigma^2$ to denote $\{\mu_l\}_{l=1}^L, \{\sigma^2_l\}_{l=1}^L$, and ${\rm mean}(x^*)$, ${\rm var}(x^*)$ to denote $\{{\rm mean}(x^*_l)\}_{l=1}^L$, $\{{\rm var}(x^*_l)\}_{l=1}^L$).

We notice that \citep{geiping2020inverting}'s implementation\footnote{The official implementation of \citep{geiping2020inverting}: \href{https://github.com/JonasGeiping/invertinggradients}{https://github.com/JonasGeiping/invertinggradients}.} assumes that BatchNorm statistics of the  private batch, i.e., ${\rm mean}(x^*)$, ${\rm var}(x^*)$, are jointly provided with the gradient. Knowing BatchNorm statistics would enable the  attacker to apply the same batch normalization used by the private batch on his recovered batch, to achieve a better reconstruction.  This implicitly increases the power of the attacker, as sharing private BatchNorm statistics are not necessary in Federated learning~\citep{andreux2020siloed, li2021fedbn}.

Note that this assumption may be \textit{realistic} in some settings:  1) the neural network is shallow, thus does not require using BatchNorm layers, or 2) the neural network is deep, but adapts approaches that normalize batch inputs with a fixed mean and variance (as alternative to BatchNorm), e.g. Fixup initialization \citep{zhang2019fixup}.


\paragraph{Assumption 2: Knowing or able to infer private labels.} Private labels are not intended to be shared in Federated learning, but knowing them would improve the attack. \citep{zhao2020idlg} finds that label information of a {\em single} private image can be inferred from the gradient (see Section~\ref{sec:assumption_invalidate} for details). Based on this, \citep{geiping2020inverting} assumes the attacker knows private labels (see remark at the end of Section 4 in their paper).  However, this assumption may not hold true when multiple images in a batch share the same label, as we will show in the next section. 

\begin{table}[t]
    \small
    \centering
    \setlength{\tabcolsep}{3.8pt}
    \begin{tabular}{|l|cccc|}
    \toprule
        {\bf Assumptions} & \citep{zhu2020deep} & \citep{zhao2020idlg} & \citep{geiping2020inverting} & \citep{yin2021see}\\
    \midrule
    {\bf Knowing BN statistics} & N/A$^\dagger$ & N/A$^\dagger$ & Yes & Yes$^*$\\
    {\bf Knowing private labels } &  No & No$^\ddagger$ & Yes & No$^\ddagger$\\
    \bottomrule
    \end{tabular}
    \vspace{2mm}
    \caption{Assumptions of gradient inversion attacks.
    {%\footnotesize
    $^\dagger$: its evaluation uses a simple model without a BatchNorm layer;  $^\ddagger$: it proposes a method to infer private labels, which works when images in a batch have unique labels (see Section~\ref{sec:assumption_invalidate}); $^*$: although the paper discusses a setting where BatchNorm statistics are unknown, its main results assume knowing BatchNorm statistics. 
    }
    }
    \label{tab:assumption_summary}
    \vspace{-3mm}
\end{table}

\subsection{Re-evaluation under relaxed assumptions} 
\label{sec:assumption_invalidate}

We re-evaluate the performance of the gradient inversion attack in settings where two assumptions above are relaxed. For each relaxation, we re-design the attack (if needed) based on the knowledge that the attacker has. 


\paragraph{Relaxation 1: Not knowing BatchNorm statistics.} We refer to the previous threat model as $\rm BN_{exact}$, where the attacker knows exact BatchNorm statistics of the private batch. We consider a more realistic threat model where these statistics are not exposed, and re-design the attack based on it.


\textit{Threat model.} In each training step, the client normalizes its private batch $x^*$ using the batch's mean ${\rm mean}(x^*)$ and variance ${\rm var}(x^*)$, keeps the running estimate of mean and variance \textit{locally} as in \citep{li2021fedbn}, and shares the gradient. The client releases the final aggregated mean $\mu$, and aggregated variance $\sigma^2$ of all training data points at the end of training. Same as before, the attacker has access to the model and the gradient during training. %He uses $\mu$ and $\sigma^2$ to help him launch the gradient inversion attack.


\textit{Re-design A: $\rm BN_{proxy}$, attacker naively uses $\mu$ and $\sigma^2$.} A simple idea is that the attacker uses $(\mu, \sigma^2)$ as the proxy for $({\rm mean}(x^*), {\rm var}(x^*))$, and uses them to normalize $x$, his guesses of the private batch. Other operations of the gradient inversion attack remain the same as before. However, Figure~\ref{fig:BN_var}.d and~\ref{fig:BN_var}.h show poor-quality reconstruction with this re-design. 


\textit{Re-design B: $\rm BN_{infer}$, attacker infers $({\rm mean}(x^*), {\rm var}(x^*))$ based on $(\mu, \sigma^2)$.} %The failure of $\rm BN_{proxy}$ is mainly due to its non-adaptive way of using $(\mu, \sigma^2)$. 
A more reasonable attacker will try to infer $({\rm mean}(x^*), {\rm var}(x^*))$ while updating $x$, his guesses of the private batch, and uses $({\rm mean}(x), {\rm var}(x))$ to normalize the batch. In this case, $(\mu, \sigma^2)$ could be used as a prior of BatchNorm statistics to regularize the recovery, as suggested in \citep{yin2021see}:

\begingroup
% \small
\begin{equation}
\label{eq:objective}
    \arg \min _{x}  1-\frac{\left\langle\nabla_{\theta} \mathcal{L}_{\theta}(x, y), \nabla_{\theta} \mathcal{L}_{\theta}\left(x^{*}, y^{*}\right)\right\rangle}{\left\|\nabla _ { \theta } \mathcal { L } _ { \theta } ( x , y ) \left|\left\|\mid \nabla_{\theta} \mathcal{L}_{\theta}\left(x^{*}, y^{*}\right)\right\|\right.\right.}+ \alpha_{\rm TV} \mathcal{R}_{\rm TV}(x) + \alpha_{\rm BN} \mathcal{R}_{\rm BN}(x)
\end{equation}
% \vspace{-4mm}
\endgroup

where $ \mathcal{R}_{\mathrm{BN}}(x)=  \sum_{l} \| {\rm mean}(x_l)- \mu_l \|_{2} + \sum_{l} \| {\rm var}(x_l)- \sigma_l^2 \|_{2}$.


We tune $\alpha_{\rm BN}$ and present the best result in Figure~\ref{fig:BN_var}.c and~\ref{fig:BN_var}.g (see results of different $\alpha_{\rm BN}$'s in Appendix~\ref{sec:app_exp}). As shown, for a batch of low-resolution images, $\rm BN_{infer}$ gives a much better reconstruction result than $\rm BN_{proxy}$, but still cannot recover some details of the private batch when compared with $\rm BN_{exact}$. The result for a single high-resolution image is worse: the attacker fails to return a recognizable reconstruction with $\rm BN_{infer}$. This suggests not having access to BatchNorm statistics of the private batch already weakens the state-of-the-art gradient inversion attack.


% \captionsetup[figure]{font=small}
\begin{figure}[H]
% \vspace{-3mm}
    \centering
    \subfloat[Original]{\includegraphics[width=0.123\textwidth]{imgs/assumptions/BN/original.png}}
    \subfloat[$\rm BN_{exact}$]{\includegraphics[width=0.123\textwidth]{imgs/assumptions/BN/reconstructed.png}}
    \subfloat[$\rm BN_{infer}$]{\includegraphics[width=0.123\textwidth]{imgs/assumptions/BN/reconstructed_train_train_bn=1e-3.png}}
    \subfloat[$\rm BN_{proxy}$]{\includegraphics[width=0.123\textwidth ]{imgs/assumptions/BN/reconstructed_defender_train_attacker_eval.png}}
    \hspace{1mm}
    \subfloat[Original]{\includegraphics[width=0.123\textwidth]{imgs/assumptions/BN_highres/original.png}}
    \subfloat[$\rm BN_{exact}$]{\includegraphics[width=0.123\textwidth]{imgs/assumptions/BN_highres/BN_exact.png}}
    \subfloat[$\rm BN_{infer}$]{\includegraphics[width=0.123\textwidth]{imgs/assumptions/BN_highres/reconstructed_bn_1e-3.png}}
    \subfloat[$\rm BN_{proxy}$]{\includegraphics[width=0.123\textwidth ]{imgs/assumptions/BN_highres/BN_proxy.png}}
    \caption{ Attacking a batch of 16 low-resolution images from CIFAR-10 (a-d) and a single high-resolution image from ImageNet (e-h) with different knowledge of BatchNorm statistics. 
    Attack is weakened when BatchNorm statistics are not available (c, d versus b, and g, h versus f). See Appendix~\ref{sec:app_exp} for more examples and quantitative results.}
    \label{fig:BN_var}
\end{figure}

\paragraph{Relaxation 2: Not knowing private labels.}  \citep{zhao2020idlg} notes that label information of a {\em single} private image can be computed analytically from the gradients of the layer immediately before the output layer. \citep{yin2021see} further extends this method to support recovery of labels for a batch of images. However, if multiple images in the private batch belong to a same label, neither approach can tell how many images belong to that label, let alone which subset of images belong to that label. Figure~\ref{fig:batch_label_dist} demonstrates that with CIFAR-10, for batches of various sizes it is possible for many of the training samples to be have the same label, and the distribution of labels is not uniform - and hence, inferring labels becomes harder and the attack would be weakened. In Figure~\ref{fig:reconstructed_labels} we evaluate the worst-case for an attacker in this setting by comparing recoveries where the batch labels are simultaneously reconstructed alongside the training samples. 


\begin{figure}[t]
    \centering
    \subfloat[Distribution of labels in a batch]{
        \includegraphics[width=.35\textwidth]{imgs/assumptions/labels_per_batch.png}
        \label{fig:batch_label_dist}
    }
    \subfloat[Reconstructions with and without private labels]{
    \includegraphics[width=.6\textwidth]{imgs/assumptions/label_known_unknown.png}
    \label{fig:reconstructed_labels}
    }
    \caption{ Attack is weakened when private labels are not available. (a) shows that for CIFAR-10, when the batch size is large, many images in the batch belong to the same class, which essentially weakens label restoration~\citep{zhao2020idlg, yin2021see}. (b) visualizes a reconstructed batch of 16 images with and without private labels known. The quality of the reconstruction drops without knowledge of private labels. 
    }
    \vspace{-3mm}
\end{figure}


\section{Defenses Against the Gradient Inversion Attack}
\label{sec:defense}

Several defense ideas have been proposed to mitigate the risks of gradient inversion.


\subsection{Encrypt gradients}

Cryptography-based approaches encrypt gradient to prevent gradient inversion. \citep{bonawitz2016practical} presents a secure aggregation protocol for Federated learning by computing sum of gradient vectors based on secret sharing \citep{shamir1979share}. \citep{phong18} proposes using homomorphic encryption to encrypt the gradients before sending. These approaches require special setup and can be costly to implement. 


Moreover, with secure aggregation protocol, an honest-but-curious server can still launch the gradient inversion attack on the summed gradient vector. Similarly, an honest-but-curious client can launch the gradient inversion attack on the model returned by the server to reconstruct other clients' private data, even with homomorphic encryption.

As alternatives, two other types of defensive mechanisms have been proposed to mitigate the risks of attacks on \textit{plain-text} gradient.


\subsection{Perturbing gradients}

\paragraph{Gradient pruning.} When proposing the first practical gradient inversion attack, \citep{zhu2020deep} also suggests a defense by setting gradients of small magnitudes to zero (i.e. gradient pruning). Based on their attack, they demonstrate that pruning more than $70\%$ of the gradients would make the recovered images no longer visually recognizable. However, the suggested prune ratio is determined based on weaker attacks, and may not remain safe against the state-of-the-art attack.

\paragraph{Adding noise to gradient.} Motivated by DPSGD \citep{abadi2016deep} which adds noise to gradients to achieve differential privacy~\citep{d09, dr14}, \citep{zhu2020deep, wei2020framework} also suggests defending by adding Gaussian or Laplacian noise to gradient. They show that a successful defense requires adding high noise level such that its accuracy drops by more than $30\%$ with CIFAR-10 tasks. Recent works \citep{papernot2020making, tramer2020differentially} suggest using better pre-training techniques and a large batch size (e.g. $4,096$) to achieve a better accuracy for DPSGD training. 


Since most DPSGD implementations for natural image classification tasks~\citep{abadi2016deep, papernot2020making, tramer2020differentially} use a pre-training and fine-tuning pipeline, it is hard to fairly compare with other defense methods that can directly apply when training the model from scratch. Thus, we leave the comparison with DPSGD to future work.



\subsection{Weak encryption of inputs (i.e. encoding inputs)}

\paragraph{MixUp.} MixUp data augmentation \citep{zhang2017mixup} trains neural networks on composite images created via linear combination of image pairs. It has been shown to improve the generalization of the neural network and stabilizes the training. Recent work also suggests that MixUp increases the model's robustness to adversarial examples~\citep{pang2019mixup, lamb2019interpolated}. 

\paragraph{InstaHide.} Inspired by MixUp, \citep{huang2020instahide} proposes InstaHide as a light-weight instance-encoding scheme for private distributed learning. To encode an image $x \in {\mathbb R}^d$ from a private dataset, InstaHide first picks $k-1$ other images $s_2, s_3, \ldots, s_k$ from that private dataset, or a large public dataset, and $k$ random nonnegative coefficients $\{\lambda_i\}_{i=1}^k$ that sum to $1$, and creates a composite image $\lambda_1 x + \sum_{i=2}^k \lambda_i s_i$ ($k$ is typically small, e.g., $4$). A composite label is also created using the same set of coefficients.\footnote{Only the labels of examples from the private dataset will get combined. See \citep{huang2020instahide} for details.} Then it adds another layer of security: pick a random sign-flipping pattern $\sigma \in \{-1, 1\}^d$ and output the encryption $\tilde{x} = \sigma\circ (\lambda_1 x + \sum_{i=2}^k \lambda_i s_i)$, where $\circ$ is coordinate-wise multiplication of vectors.
The neural network is then trained on encoded images, which look like random pixel vectors to the human eye and yet lead to  good classification accuracy ($<6\%$ accuracy loss on CIFAR-10, CIFAR-100, and ImageNet).


Recently, \citep{carlini2020attack} gives an attack to recover private images of a small dataset, when the InstaHide encodings are revealed to the attacker (not in a Federated learning setting). Their first step is to train a neural network on a public dataset for similarity annotation, 
to infer whether a pair of InstaHide encodings contain the same private image. With the inferred similarities of all pairs of encodings, the attacker then runs a combinatorial algorithm (cubic time in size of private dataset) to cluster all encodings based on their original private images, and finally uses a regression algorithm (with the help of composite labels) to recover the private images. 

Neither \citep{huang2020instahide} or \citep{carlini2020attack} has evaluated their defense or attack in the Federated learning setting, where the attacker observes gradients of the encoded images instead of original encoded images. This necessitates the systematic evaluation in our next section. 