\newpage

\section{Theoretical Insights for Defenses' Working Mechanism}
\label{sec:app_theory}
In this section, we provide some theoretical insights for the mechanism of each defense.

\subsection{Gradient pruning}
Gradient pruning is a non-oblivious case of applying sketching techniques~\citep{cls19} to compress the gradient vector. Usually, if we only observe the vector after sketching, it is hard to recover the original vector unless certain assumptions of the vector itself and the sketching technique have been made. Therefore, gradient pruning prevents the attacker from seeing the original gradient and make the inversion harder. 

\subsection{MixUp and InstaHide} Intuitively, MixUp and InstaHide's working mechanism may come from mixing $k$ images in a single encoded image, which appears to be similar to multiplying the batch size by a factor of $k$, thus makes the attack less effective. In Section~\ref{sec:gradient}, we provide theoretical analysis for this intuition, by showing that mixing $k$ images and using a batch size of $k$ are essentially similar, with any neural network that has a fully connected layer as its first layer. 

Another layer of security of InstaHide seems to come from applying random sign-flipping on the mixed images. As mentioned in Section~\ref{sec:exp_setup}, for an InstaHide-encoded image $x \in \R^d$, we apply the total variation regularizer on $|x|$ instead of $x$, which pushes the gap between absolute value of adjacent pixels (i.e., $| |x_j| - |x_{j+1} | |$) to be small. However having $| |x_j| - |x_{j+1}|  | = \delta$ for some small $\delta < 10^{-4}$ does not imply that $|x_j - x_{j+1}| = \delta$; in fact, $|x_j - x_{j+1}|$ can be as large as $1-\delta$.  Therefore, the random sign flipping operation in InstaHide could potentially make the total variation image prior less effective in some sense (see Figure~\ref{fig:mixup_vs_instahide}). 

\iffalse
\paragraph{Mixing up images is (nearly) increasing batch size}
using one image which is generating from mix-up 4 images is somehow equivalent to using batch size that has 4 images.


Say images are $x_1, \cdots, x_4$

Batched gradient looks like this
\begin{align*}
    \sum_{i=1}^4 {\bf 1}_{\langle w, x_i \rangle } x_i
\end{align*}

MixUp gradient looks 
\begin{align*}
    \sum_{i=1}^4 {\bf 1}_{\langle w, \overline{x} \rangle } x_i
\end{align*}
where $\overline{x} = \sum_{i=1}^4 x_i $
\fi

\subsection{Property of gradient in a small batch}\label{sec:gradient}

The goal of this section is to present the following results,
\begin{lemma}\label{lem:main_gradient}
Given a neural network with ReLU activation function, each row of the gradient of first layer weights is a linear combination of images, i.e.
\begin{align*}
    ( \frac{\partial {\cal L}(W)} { \partial W_1 } )_i = \sum_{j=1}^b \alpha_{i,j} x_i^\top 
\end{align*}
where the $b$ is the number of images in a small batch, $\{x_1, \cdots, x_b \} \in \R^d$ are images in that small batch. 
\end{lemma}
In Section~\ref{sec:gradient_1} and \ref{sec:gradient_2}, we show the above observation holds for one/two-hidden layer neural network. In Section~\ref{sec:gradient_l}, we generalize it to multiple layer neural network.

The standard batched $k$-vector sum can be defined as follows:
\begin{definition}
Give a database $X$ list of vectors $x_1, \cdots, x_n$. Given a list of observations $y_1, \cdots, y_m$ where for each $j \in [m]$, there is a set $S_j$ such that $y_j = \sum_{i \in S_j} x_i$ 
and $|S_j|=b$. We can observe $y_1, \cdots y_m$ but has no access to database, the goal is to recover $S_j$ and the vectors $x_i$ being use, for each $j$.
\end{definition}
The above definition is a mathematical abstraction of MixUp recovery/attack. It can be further generalized to InstaHide, if we only observe the $|y_j|$. We also want to remark that in the above definition, we simplify the formulation by using coefficients $1$ for all vectors. It can be easily generalized to 
settings where random coefficients are assigned to vectors in the database for MixUp/InstaHide. 

Using Lemma~\ref{lem:main_gradient}, we notice that
\begin{lemma}
Under the condition of Lemma~\ref{lem:main_gradient}, given a list of observation of gradients, and the problem recovering images is also a batched vector sum problem. 

\end{lemma}
Thus, gradient attack is essentially an variation of MixUp/Instahide attack.

\subsection{One Hidden Layer}\label{sec:gradient_1}
We consider a one-hidden layer ReLU activated neural network with $m$ neurons in the hidden layer:
\begin{align*}
    f(x) = a^\top \phi(W x )
\end{align*}
where $a\in \R^m$ and $W \in \R^{m \times d}$.
We define objective function $L$ as follows:
\begin{align*}
    L(W) = \frac{1}{2} \sum_{i=1}^n (y_i - f(W,x_i,a))^2
\end{align*}
We can compute the gradient of $L$ in terms of $w_r$
\begin{align*}
    \frac{ \partial L(W) }{ \partial w_r } = \sum_{i=1}^n ( f(W,x_i,a) -y_i ) a_r x_i {\bf 1}_{ \langle w_r, x_i \rangle }
\end{align*}
Let $\wt{x} = \frac{1}{n} \sum_{i=1}^n x_i$,
\begin{align*}
    \frac{ \partial L(W) }{ \partial w_r } = ( f(W, \wt{x}, a) - y ) a_r \wt{x} {\bf 1}_{\langle w_r, \wt{x} \rangle}
\end{align*}
Another version
\begin{align*}
    \frac{ \partial L(W) }{ \partial w_r } = \sum_{i=1}^n ( f(W, x_i, a) - y_i ) \cdot \Big( a_r \wt{x} {\bf 1}_{\langle w_r, \wt{x} \rangle} \Big)
\end{align*}


\subsection{Two Hidden Layers}\label{sec:gradient_2}

Suppose $a \in \R^m$, $V \in \R^{m \times d}, W \in \R^{m \times m}$. The neural network is defined as $f : \R^d \rightarrow \R$, here we slightly deviate from the standard setting and assume the input dimension is $m$, in order to capture the general setting.
\begin{align*}
f(x) = a^{\top} \phi( W \phi ( V x ) )
\end{align*}
Consider the mean square loss
\begin{align*}
L(W, V, a) = \frac{1}{2} \sum_{i=1}^n | f(x_i) - y_i |^2
\end{align*}
The gradient with respect to $W$ is
\begin{align*}
\frac{ \partial L(W, V, a)}{ \partial W } = \sum_{i=1}^{n}(f(x_i) - y_i)  \underbrace{ \diag\{\phi'(W \phi(Vx_i))\} }_{ m \times m }\underbrace{ a }_{m \times 1}  \underbrace{ \phi(Vx_i)^{\top} }_{1 \times m}  
\end{align*}
and the gradient with respect to $V$ is
\begin{align*}
\frac{ \partial L(W, V, a)}{ \partial V } = \sum_{i=1}^{n}(f(x_i) - y_i)\underbrace{\diag\{ \phi'(Vx_i)\}}_{m \times m} \underbrace{W^{\top}}_{m \times m}  \underbrace{\diag\{\phi'(W \phi(Vx_i))\}}_{m \times m} \underbrace{a}_{m \times 1}    \underbrace{x_i^{\top}}_{1 \times d} 
\end{align*}

\subsection{The multi-layers case}\label{sec:gradient_l}


The following multiple layer neural network definition is standard in literature. %should be found in \cite{als19_dnn}.

Consider a $L$ layer neural network with one vector $a\in \R^{m_L}$ and $L$ matrices $W_L \in \R^{m_L \times m_{L-1}}$, $\cdots$, $W_2 \in \R^{m_2 \times m_1}$ and $W_1 \in \R^{m_1 \times m_0}$. Let $m_0 = d$.
In order to write gradient in an elegant way, we define some artificial variables as follows
\begin{align}\label{eq:def_g_h}
g_{i,1} = & ~ W_1 x_i, && h_{i,1} = \phi(W_1 x_i), && \in \R^{m_1} && \forall i \in [n]\notag\\
g_{i,\ell} = & ~ W_{\ell} h_{i,\ell-1}, && h_{i,\ell} = \phi( W_{\ell} h_{i,\ell-1} ), && \in \R^{m_{\ell}} && \forall i \in [n], \forall \ell \in \{2,3,\cdots,L\} \\
%f(W,x_i) = & ~ a^\top h_{i,L}, && && \in \R && \forall i \in [n]\notag \\
D_{i, 1} = & ~ \text{diag} \big(\phi'(W_1 x_i )\big), && && \in \R^{m_1 \times m_1} && \forall i \in [n] \notag\\
D_{i, \ell} = & ~ \text{diag} \big(\phi'(W_{\ell} h_{i, \ell-1})\big), && && \in \R^{m_{\ell} \times m_{\ell}} && \forall i \in [n], \forall \ell \in \{2,3,\cdots,L\} \notag
%\label{eq:nn}
\end{align}
where $\phi(\cdot)$ is the activation function and $\phi'(\cdot)$ is the derivative of activation function. 



Let $f : \R^{m_0} \rightarrow \R$ denote neural network function: 
\begin{align*}
    f(W,x) = a^\top \phi( W_L ( \phi ( \cdots \phi(W_1 x ) ) ) )
\end{align*}
Thus using definition of $f$ and $h$, we have
\begin{align*}
    f(W,x_i) =  a^\top h_{i,L}, ~~~ \in \R, ~~~ \forall i \in [n]
\end{align*}

Given $n$ input data points $(x_1 , y_1), (x_2 , y_2) , \cdots (x_n, y_n) \in \R^{d} \times \R$. We define the objective function $\mathcal{L}$ as follows
\begin{align*}
\mathcal{L} (W) = \frac{1}{2} \sum_{i=1}^n ( y_i - f (W,x_i) )^2 .
\end{align*}
 

We can compute the gradient of ${\cal L}$ in terms of $W_{\ell} \in \R^{m_{\ell} \times m_{\ell-1}}$, for all $\ell \geq 2$
\begin{align}\label{eq:gradient}
\frac{\partial\mathcal{L}(W)}{\partial W_{\ell}} = \sum_{i = 1}^{n} ( f(W, x_i) - y_i) \underbrace{D_{i, \ell}}_{ m_{\ell} \times m_{\ell} } \left( \prod_{k = \ell+1}^{L} \underbrace{W_{k}^{\top}}_{m_{k-1} \times m_k}\underbrace{D_{i, k}}_{ m_{k} \times m_k } \right)  \underbrace{a}_{ m_L \times 1} \underbrace{h_{i, \ell -1}^{\top}}_{1 \times m_{\ell-1}}
\end{align}
Note that the gradient for $W_1 \in \R^{m_1 \times m_0}$ (recall that $m_0 = d$) is slightly different and can not be written by general form. Here is the form
\begin{align}
    \frac{\partial\mathcal{L}(W)}{\partial W_{1}} = \sum_{i = 1}^{n} ( f(W, x_i) - y_i) \underbrace{D_{i, 1}}_{ m_1 \times m_1 } \left( \prod_{k = 2}^{L} \underbrace{W_{k}^{\top}}_{m_{k-1} \times m_k}\underbrace{D_{i, k}}_{ m_{k} \times m_k } \right)  \underbrace{a}_{ m_L \times 1} \underbrace{x_{i}^{\top}}_{1 \times m_{0}}
\end{align}