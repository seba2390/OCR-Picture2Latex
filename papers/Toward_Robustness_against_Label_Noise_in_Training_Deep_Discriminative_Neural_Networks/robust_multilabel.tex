\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2016
%
% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2016}

% after submission
%\usepackage{nips_2017}
%\usepackage[final, nonatbib]{nips_2017_modified}
% after submission end

% to compile a camera-ready version, add the [final] option, e.g.:
\usepackage[final, nonatbib]{nips_2017}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\usepackage{multirow}

% my packages
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{verbatim}
\usepackage[multiple]{footmisc}
\usepackage[ruled]{algorithm2e}

%\usepackage{lastpage}
%\usepackage{fancyhdr}
\usepackage{float}
\usepackage[caption = false]{subfig}

\usepackage{tikz}
\usetikzlibrary{automata,calc,backgrounds,arrows}  %arrows} %automata}  %calc} %,backgrounds}
\usepackage{xcolor}
\usepackage{color}
\usepackage{pgfplots}

\newcommand{\vc}[1]{{\pmb{#1}}}
\newcommand{\x}{{\pmb{x}}}
\newcommand{\y}{{\pmb{y}}}
\newcommand{\yh}{{\hat{\pmb{y}}}}
\newcommand{\yb}{{\bar{\pmb{y}}}}
\renewcommand{\t}{{\pmb{t}}}
\newcommand{\btau}{{\pmb{\tau}}}
\newcommand{\z}{{\pmb{z}}}
\renewcommand{\th}{{\hat{\pmb{t}}}}
\newcommand{\h}{{\pmb{h}}}
\renewcommand{\a}{{\pmb{a}}}
\renewcommand{\b}{{\pmb{b}}}
\renewcommand{\c}{{\pmb{c}}}
\renewcommand{\d}{{\pmb{d}}}
\newcommand{\W}{{\pmb{W}}}
\newcommand{\Wp}{{\pmb{W'}}}
\newcommand{\btheta}{{\pmb{\theta}}}
\newcommand{\bphi}{{\pmb{\phi}}}
\newcommand{\arash}[1]{{\color{red}[[#1]]}}
\renewcommand{\L}{{\mathcal{L}}}
\newcommand{\U}{{\mathcal{U}}}
\def\KL{\text{KL}}



\title{Toward Robustness against Label Noise in \\ Training Deep Discriminative Neural Networks}

% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.

\author{
  Arash Vahdat
%   % \thanks{Use footnote for providing further information about author (webpage, alternative address)---\emph{not} for acknowledging funding agencies.} 
  \\
  D-Wave Systems Inc.\\
  Burnaby, BC, Canada \\
  \texttt{avahdat@dwavesys.com} \\
  %% examples of more authors
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \AND
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
}

\begin{document}
% \nipsfinalcopy is no longer used

\maketitle

\begin{abstract}
  Collecting large training datasets, annotated with high-quality labels, is costly and time-consuming. This paper proposes a novel framework
  for training deep convolutional neural networks from noisy labeled datasets that can be obtained cheaply. The problem is formulated using an undirected graphical model that represents
  the relationship between noisy and clean labels, trained in a semi-supervised setting.
  In our formulation, the inference over latent clean labels is tractable and 
  is regularized during training using auxiliary sources of information. The proposed model is applied to the image labeling
  problem and is shown to be effective in labeling unseen images as well as reducing label noise in training on CIFAR-10 and MS COCO datasets.
\end{abstract}


\section{Introduction}
The availability of large annotated data collections such as ImageNet~\cite{deng2009imagenet} is one of the key reasons why deep convolutional neural networks (CNNs) 
have been successful in the image classification problem. However, collecting training data with such high-quality annotation is very 
costly and time consuming. In some applications, annotators are  required to be trained before identifying classes in data, and 
feedback from many annotators is aggregated  to reduce labeling error. 
On the other hand, many inexpensive approaches for collecting labeled data exist, such as data mining on
social media websites, search engines, querying fewer annotators per instance, or the use of amateur annotators instead of experts. 
However, all these low-cost approaches have one common side effect: \textit{label noise}.

This paper tackles the problem of training deep CNNs for the image labeling task from datapoints with noisy labels. Most previous 
work in this area has focused on modeling label noise for multiclass classification\footnote{Each sample is assumed to belong to only one 
class.} using a directed graphical model similar to Fig.~\ref{fig:intro}.a. It is typically assumed that the clean labels are hidden during training, and
they are marginalized by enumerating all possible classes. These techniques cannot be extended to the multilabel classification problem, where exponentially 
many configurations exist for labels, and the explaining-away phenomenon makes inference over latent clean labels difficult.

We propose a conditional random field (CRF)~\cite{lafferty01CRF} model
to represent the relationship between noisy and clean labels, and we show how modern deep CNNs
can gain robustness against label noise using our proposed structure.
We model the clean labels as latent variables during training,
and we design our structure such that the latent variables can be inferred efficiently.


The main challenge in modeling clean labels as latent is the lack of semantics on latent variables. 
In other words, latent variables may not semantically correspond to the clean labels when the joint probability of clean and noisy labels is 
parameterized such that latent clean labels can take any configuration.  To solve this problem, most previous work relies on either carefully initializing
the conditionals~\cite{Xiao2015}, 
fine-tuning the model on the noisy set after pretraining on a clean set ~\cite{Misra2016}, or
regularizing the transition parameters~\cite{sukhbaatar2014}. In contrast, we inject semantics to the latent variables by formulating the training 
problem as a semi-supervised learning problem, in which the model is 
trained using a large set of noisy training examples and a small set of clean training examples.
To overcome the problem of inferring clean labels, we introduce a novel framework equipped with an auxiliary distribution 
that represents the relation between noisy and clean labels while relying on information sources different than the image content.

This paper makes the following contributions: i) A generic CRF model is proposed for training deep neural networks that is robust against label noise.
The model can be applied to both multiclass and multilabel classification problems, and it can be understood as a robust loss layer, which can be plugged into
any existing network.
ii) We propose a novel objective function for training the deep structured model that benefits from sources of information 
representing the relation between clean and noisy labels.
iii) We demonstrate that the model outperforms previous techniques.


\begin{figure}
 \label{fig:intro}
 \centering
 \subfloat[]{
 \begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=1.5cm, ultra thick, scale=0.85]
      \tikzstyle{every state}=[fill=white,draw=black,text=black, transform shape]
      \node[state, fill=lightgray] 	(x)                    {$\x$};
      \node[state] 			(yh)   [right of=x]    {$\yh$};
      \node[state] 			(y)   [right of=yh]    {$\y$};
      \path (x)         edge               (yh);
      \path (yh)        edge               (y);
  \end{tikzpicture}  
  } \hspace{1cm}
  \subfloat[]{
 \begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=1.5cm, ultra thick, scale=0.85]
      \tikzstyle{every state}=[fill=white,draw=black,text=black, transform shape]
      
      \node[state, fill=lightgray] 	(x)                    {$\x$};
      \node[state] 			(yh)   [right of=x]    {$\yh$};
      \node[state] 			(y)   [right of=yh]    {$\y$};
      \node[state] 			(h)   [right of=y]     {$\h$};
      \path (x)         edge[-]               (yh);
      \path (x)         edge[-, out=45, in=135]               (y);
      \path (yh)        edge[-]               (y);
      \path (y)         edge[-]               (h);
  \end{tikzpicture}  
  }
  \caption{a) The general directed graphical model used for modeling noisy labels. $\x$, $\yh$, $\y$
  represent a data instance, its clean label, and its noisy label, respectively. b) We represent the interactions between clean and noisy labels
  using an undirected graphical model with hidden binary random variables ($\h$).}
\end{figure}


\section{Previous Work} 
\textbf{Learning from Noisy Labels:}
Learning discriminative models from noisy-labeled data is an active area of research. A comprehensive overview of previous work in this area can be found in \cite{FrenayV14}.
 Previous research on modeling label noise can be grouped into two main groups: \textit{class-conditional} and \textit{class-and-instance-conditional} label noise models. 
In the former group, the label noise is assumed to be independent of the instance, and the transition probability from clean classes to the noisy classes is modeled. 
For example, class conditional models for binary classification problems are considered in \cite{natarajan2013learning, mnih2012learning} whereas
multiclass counterparts are targeted in ~\cite{patrini2016,sukhbaatar2014}. In the class-and-instance-conditional group, label noise is explicitly conditioned on each instance. 
For example, Xiao et al.~\cite{Xiao2015} developed a model in which the noisy observed annotation is conditioned on binary random variables indicating if an instance's label 
is mistaken. Reed et al.~\cite{reed2014} fixes noisy labels by ``bootstrapping'' on the labels predicted by a neural network. 
These techniques are all applied to either binary or multiclass classification problems in which marginalization over classes is possible.
Among methods proposed for noise-robust training, Misra et al.~\cite{Misra2016} target the image multilabeling problem but model the label noise for 
each label independently. In contrast, our proposed CRF model represents the relation between all noisy and clean labels while the inference over latent clean labels is still tractable.


\iffalse
For example, Natarajan et al.~\cite{natarajan2013learning} proposed an unbiased estimator of 
loss functions for binary classification. Similarly, Mnih and Hinton \cite{mnih2012learning} explored class conditional noise model for binary
classification using CNNs. Patrini et al.~\cite{patrini2016} and Sukhbaatar et al.~\cite{sukhbaatar2014} proposed approaches to estimate class conditional 
transition probabilities in multiclass problems. In the class-and-instance-conditional group, label noise is explicitly conditioned on each instance. 
For example, Xiao et al.~\cite{Xiao2015} developed a probabilistic model in which the noisy observed annotation is conditioned on whether an instance's label 
is mistaken in addition to its clean latent label. Reed et al.~\cite{reed2014} fixes noisy labels by bootstrapping on the labels predicted by a neural network. 
These techniques are all applied to either binary or multiclass classification problems in which marginalization over classes is possible.
Among methods proposed for noise-robust training, Misra et al.~\cite{Misra2016} targets the image multilabeling problem but models the label noise for 
each label independently ignoring the correlations between all the clean labels and noisy observed annotation. 
\fi

Many works have focused on semi-supervised learning using a small clean dataset combined with noisy labeled data, typically obtained from the web. 
Zhu et al.~\cite{ZhuLG03} used a pairwise similarity measure to propagate labels from labeled dataset to unlabeled one. 
Fergus et al.~\cite{fergusWT09} proposed a graph-based label propagation, and Chen and Gupta \cite{chen2015webly} employed the weighted cross entropy loss.
%Positive Learning \cite{elkan2008learning, fung2005parameter} trains model from only positive instances and unlabeled data. 
Recently Veit et al.~\cite{veit2017learning} proposed a multi-task network containing i) a regression model that maps noisy labels and image features to clean labels 
ii) an image classification model that labels input. 
However, the model in this paper is trained using a principled objective function that regularizes the inference model using extra sources of information without 
the requirement for oversampling clean instances.

\textbf{Deep Structured Models:} Conditional random fields (CRFs)~\cite{lafferty01CRF} are discriminative undirected graphical models, originally proposed for modeling sequential and structured data.
Recently, they have shown state-of-the-art results in segmentation~\cite{lin2016efficient, zheng2015conditional} when combined with deep neural 
networks~\cite{peng2009conditional, artieres2010neural, prabhavalkar2010backpropagation}. The main challenge in training
deep CNN-CRFs is how to do inference and back-propagate gradients of the loss function through the inference. Previous approaches have focused on mean-field
approximation~\cite{zheng2015conditional, krahenbuhl2011efficient},
belief propagation~\cite{chen2015learning, schwing2015fully}, unrolled inference~\cite{DengVHM16, ross2011learning}, and sampling~\cite{kirillov2015joint}. The CNN-CRFs used in this work
are extensions of hidden CRFs introduced in \cite{quattoni2007hidden, maaten2011hidden}.


\section{Robust Discriminative Neural Network}
Our goal in this paper is to train deep neural networks given a set of noisy labeled data and a small set of cleaned data.
A datapoint (an image in our case) is represented by $\x$, and its noisy annotation by a binary vector $\y = \{y_1, y_2, \dots, y_N \} \in \mathcal{Y}_N$, 
where $y_i \in \{0, 1\}$ indicates whether the $i^{th}$ label is present in the noisy annotation.
We are interested in inferring a set of clean labels for each datapoint. The clean labels may be defined on a set different than the set of noisy labels. 
This is typically the case in the image annotation problem where noisy labels obtained from user tags are defined over a large set of textual tags 
(e.g., ``cat'', ``kitten, ``kitty'', ``puppy'', ``pup'', etc.), whereas clean labels are defined on a small set of representative labels
(e.g., ``cat'', ``dog'', etc.). In this paper, the clean label is represented by a stochastic binary vector $\yh = \{\hat{y}_1, \hat{y}_2, \dots, \hat{y}_C \} \in \mathcal{Y}_C$.


We use the CRF model shown in Fig.~\ref{fig:intro}.b. 
In our formulation, both $\yh$ and $\y$ may conditionally depend on the image $\x$. The link between $\yh$ and $\y$ captures the correlations between clean and noisy labels.
These correlations help us infer latent clean labels when only the noisy labels are observed. Since noisy labels are defined over 
a large set of overlapping (e.g., ``cat'' and ``pet'') or co-occurring (e.g., ``road'' and ``car'') entities, $p(\y | \yh, \x)$ may have a multimodal form.
To keep the inference simple and still be able to model these correlations, we introduce a set of hidden binary variables represented by $\h \in \mathcal{H}$.
In this case, the correlations between components of $\y$ are modeled through $\h$. These hidden variables are not connected to $\yh$ in order to keep the CRF graph bipartite.

The CRF model shown in Fig.~\ref{fig:intro}.b defines the joint probability distribution of $\y$, $\yh$, and $\h$ conditioned on $\x$ using a parameterized energy function 
$E_\btheta: \mathcal{Y}_N \times \mathcal{Y}_C \times \mathcal{H} \times \mathcal{X} \rightarrow \mathcal{R}$. The energy function assigns a potential score $E_\btheta(\y, \yh, \h, \x)$ to the configuration
of $(\y, \yh, \h, \x)$, and is parameterized by a parameter vector $\btheta$. This conditional probability distribution is defined using a Boltzmann distribution:
\begin{equation} \label{eq:crf_prob}
p_{\btheta}(\y, \yh, \h | \x) = \frac{1}{Z_{\btheta}(\x)} \exp(-E_\btheta(\y, \yh, \h, \x))
\end{equation}
where $Z_{\btheta}(\x)$ is the partition function defined by 
{\small$ \displaystyle Z_{\btheta}(\x) = \sum_{\y \in \mathcal{Y}_N} \sum_{\yh \in \mathcal{Y}_C} \sum_{\h \in \mathcal{H}} \exp(-E_\btheta(\y, \yh, \h, \x))$}.
The energy function in Fig.~\ref{fig:intro}.b is defined by the quadratic function:
\begin{equation} \label{eq:crf_energy}
E_\btheta(\y, \yh, \h, \x) = - \a^T_\bphi(\x) \yh - \b^T_\bphi(\x) \y  - \c^T \h - \yh^T \W \y - \h^T \Wp \y
\end{equation}

where the vectors $\a_\bphi(\x)$, $\b_\bphi(\x)$, $\c$ are the bias terms and the matrices $\W$ and $\Wp$ are the pairwise interactions. In our formulation,
the bias terms on the clean and noisy labels are functions of input $\x$ and are defined using a deep CNN parameterized by $\bphi$.
The deep neural network together with the introduced CRF forms our \textit{CNN-CRF} model, parameterized by $\btheta = \{\bphi, \c, \W, \Wp \}$. Note that in order to 
regularize $\W$ and $\Wp$, these matrices
are not a function of $\x$.

The structure of this graph is designed such that
the conditional distribution $p_\btheta(\yh, \h| \y, \x)$ takes a simple factorial distribution that can be calculated analytically given $\btheta$ using:
{\small $p_\btheta(\yh, \h| \y, \x) = \prod_{i} p_\btheta(\hat{y}_i| \y, \x) \prod_{j} p_\btheta(h_j| \y)$}
where {\small $ p_\btheta(\hat{y}_i=1| \y, \x) = \sigma(\a_\bphi(\x)_{(i)} + \W_{(i,:)}\y)$}, {\small $p_\btheta(h_j| \y)  = \sigma(\c_{(j)} + \W'_{(j,:)}\y)$}, in which
$\sigma(u) = \frac{1}{1+exp(-u)}$ is the logistic function, and
$\a_\bphi(\x)_{(i)}$ or $\W_{(i,:)}$ indicate the \textit{i}$^{th}$ element and row in the corresponding vector or matrix respectively.


\subsection{Semi-Supervised Learning Approach}
The main challenge here is how to train the parameters of the CNN-CRF model defined in Eq.~\ref{eq:crf_prob}. 
To tackle this problem, we define the training problem as a semi-supervised learning problem where clean labels 
are observed in a small subset of a larger training set annotated with noisy labels. In this case, one can form an objective function by combining 
the marginal data likelihood defined on both the fully labeled clean set and noisy 
labeled set, and using the maximum likelihood method to learn the parameters of the model.
Assume that {\small $D_N = \{ (\x^{(n)}, \y^{(n)})\}$ and $D_C = \{ (\x^{(c)}, \yh^{(c)}, \y^{(c)})\}$}
are two disjoint sets representing the noisy labeled and clean labeled training datasets respectively. 
In the maximum likelihood method, the parameters are trained by maximizing the marginal log likelihood:
\begin{equation} \label{eq:training_naive}
\max_{\btheta} \frac{1}{|D_N|} \sum_n \log p_\btheta(\y^{(n)}|\x^{(n)}) + \frac{1}{|D_C|} \sum_c \log p_\btheta(\y^{(c)}, \yh^{(c)}|\x^{(c)})
\end{equation}
where {\small$p_\btheta(\y^{(n)}|\x^{(n)}) = \sum_{\y, \h} p_\btheta(\y^{(n)}, \y, \h|\x^{(n)})$ and $p_\btheta(\y^{(c)}, \yh^{(c)}|\x^{(c)}) = \sum_{\h} p_\btheta(\y^{(c)}, \yh^{(c)}, \h|\x^{(c)})$}.
Due to the marginalization of hidden variables in log terms, the objective function cannot be analytically optimized.
A common approach to optimizing the log marginals is to use the stochastic maximum 
likelihood method which is also known as persistent contrastive divergence (PCD)~\cite{tieleman2008training, younes1989parametric,kirillov2015joint}. 

The stochastic maximum likelihood method, or equivalently PCD, can be fundamentally viewed as an Expectation-Maximization (EM) approach to training. 
The EM algorithm maximizes the variational lower bound that is formed by subtracting the Kullback–Leibler (KL) divergence between
a variational approximating distribution $q$ and the true conditional distribution from the log marginal probability.
For example, consider the bound for the first term in the objective function:
\begin{eqnarray}
\log p_\btheta(\y|\x) &\geq& \log p_\btheta(\y|\x) - \KL[q(\yh, \h| \y, \x) || p_\btheta(\yh, \h| \y, \x)] \label{eq:elbo_1} \\
&=& \mathbb{E}_{q(\yh, \h| \y, \x)} [\log p_\btheta(\y, \yh, \h| \x)] - \mathbb{E}_{q(\yh, \h| \y, \x)} [\log q(\yh, \h| \y, \x)] = \mathcal{U}_\btheta(\x, \y). \label{eq:elbo_2}
\end{eqnarray}
If the incremental EM approach\cite{neal1998view} is taken for training the parameters $\btheta$, the lower bound $\mathcal{U}_\btheta(\x, \y)$
is maximized over the noisy training set by iterating between two steps. 
In the Expectation step (E step), $\btheta$ is fixed and the lower bound
is optimized with respect to the conditional distribution $q(\yh, \h| \y, \x)$. Since this distribution is only present in the KL term in Eq.~\ref{eq:elbo_1}, the lower bound
is maximized simply by setting $q(\yh, \h| \y, \x)$ to the analytic $p_\btheta(\yh, \h| \y, \x)$. 
In the Maximization step (M step), $q$ is fixed, and the bound is maximized 
with respect to the model parameters $\btheta$, which occurs only in the first expectation term in Eq.~\ref{eq:elbo_2}. This expectation can be written as
{\small $\mathbb{E}_{q(\yh, \h| \y, \x)} [-E_\btheta(\y, \yh, \h, \x)] - \log Z_{\btheta}(\x)$},
which is maximized by updating $\btheta$ in the direction of its gradient, computed using
{\small $- \mathbb{E}_{q(\yh, \h|\x,\y)} [\frac{\partial}{\partial \btheta} E_\btheta(\y, \yh, \h, \x)] 
+ \mathbb{E}_{p(\y, \yh, \h | \x)} [\frac{\partial}{\partial \btheta} E_\btheta(\y, \yh, \h, \x)]$}.
Noting that $q(\yh, \h| \y, \x)$ is set to $p_\btheta(\yh, \h| \y, \x)$ in the E step, it becomes clear that the M step is equivalent to the parameter updates in PCD.

\iffalse
Note that the second log probability term in Eq.~\ref{eq:elbo_1} is defined over
log conditional probability of $\yh$ and $\y$ marginalized over $\h$ and it can be optimized similar to the first term with slight difference that the posterior distribution
is defined over $\h$, i.e, $q(\h |\y, \yh, \x)$ will be set to $p_\btheta(\h| \y, \yh, \x) = p_\btheta(\h| \yh)$ in each E step.
\fi

\subsection{Semi-Supervised Learning Regularized by Auxiliary Distributions}
The semi-supervised approach infers the latent variables using the conditional $q(\yh, \h| \y, \x)=p_\btheta(\yh, \h| \y, \x)$. 
However, at the beginning of training when the model's parameters are not trained yet, 
sampling from the conditional distributions $p(\yh, \h| \y, \x)$ does not necessarily generate the clean labels accurately. 
The problem is more severe with the strong representation power of CNN-CRFs, as they can easily fit to poor conditional distributions that occur at the beginning of training. That is why
the impact of the noisy set on training must be reduced by oversampling clean instances \cite{veit2017learning, Xiao2015}. 
% which basically corresponds to putting a larger weight on the log likelihood term corresponding to the labeled clean set in Eq.~\ref{eq:training}.

In contrast, there may exist auxiliary sources of information that can be used to extract the relationship between noisy and clean labels. 
For example, non-image-related sources may be formed from semantic relatedness of labels \cite{rohrbach2010helps}. 
We assume that, in using such sources, we can form an auxiliary distribution $p_{aux}(\y, \yh, \h)$
representing the joint probability of noisy and clean labels and some hidden binary states.
Here, we propose a framework to use this distribution to train parameters in the semi-supervised setting
by guiding the variational distribution to infer the clean labels more accurately. To do so, 
we add a new regularization term in the lower bound that penalizes the variational distribution for being different from
the conditional distribution resulting from the auxiliary distribution as follows:
{\small
\begin{equation*} % \label{eq:elbo_alpha}
\log p_\btheta(\y|\x) \geq  \U^{aux}_{\btheta}(\x, \y) = \log p_\btheta(\y|\x) - \KL[q(\yh, \h| \y, \x) || p_\btheta(\yh, \h| \y, \x)] - \alpha \KL[q(\yh, \h| \y, \x) || p_{aux}(\yh, \h| \y)]
\end{equation*}}
where $\alpha$ is a non-negative scalar hyper-parameter that controls the impact of the added KL term.
Setting $\alpha = 0$ recovers the original variational lower bound
defined in Eq.~\ref{eq:elbo_1} whereas $\alpha \rightarrow \infty$  forces the variational distribution $q$ to ignore the $p_\btheta(\yh, \h| \y, \x)$ term. 
A value between these two extremes makes the inference distribution intermediate between $p_\btheta(\yh, \h| \y, \x)$
and $p_{aux}(\yh, \h| \y)$. Note that this new lower bound is actually looser than the original bound. This may be undesired if
we were actually interested in predicting noisy labels. However, our goal is to predict clean labels, and the proposed framework benefits
from the regularization that is imposed on the variational distribution. Similar ideas have been explored in the posterior regularization approach \cite{ganchev2010posterior}.

Similarly, we also define a new lower bound on the second log marginal in Eq.~\ref{eq:training_naive} by:
{\small \begin{equation*} % \label{eq:elbo_alpha}
\log p_\btheta(\y, \yh|\x) \geq \L^{aux}_{\btheta}(\x, \y, \yh) = \log p_\btheta(\y, \yh|\x) - \KL[q(\h| \y) || p_\btheta(\h| \y)] - \alpha \KL[q(\h| \y) || p_{aux}(\h| \y)].
\end{equation*}}

\textbf{Auxiliary Distribution:} In this paper, the auxiliary joint distribution $p_{aux}(\y, \yh, \h)$ is modeled by an undirected graphical model in a special form of a restricted Boltzmann machine (RBM),
and is trained on the clean training set. The structure of the RBM is similar to the CRF model shown in
Fig.~\ref{fig:intro}.b with the fundamental difference that parameters of the model do not depend on $\x$:
\begin{equation} \label{eq:rbm_prob}
p_{aux}(\y, \yh, \h) = \frac{1}{Z_{aux}} \exp(-E_{aux}(\y, \yh, \h))
\end{equation}
where the energy function is defined by the quadratic function:
\begin{equation} \label{eq:rbm_energy}
E_{aux}(\y, \yh, \h) = - \a^T_{aux} \yh - \b^T_{aux} \y  - \c^T_{aux} \h - \yh^T \W_{aux} \y - \h^T \Wp_{aux} \y
\end{equation}
and $Z_{aux}$ is the partition function, defined similarly to the CRF's partition function. The number of hidden variables is set to 200 and the parameters of 
this generative model are trained using the PCD algorithm~\cite{tieleman2008training}, and are fixed while the CNN-CRF model is being trained.

\subsection{Training Robust CNN-CRF}
In training, we seek $\btheta$ that maximizes the proposed lower bounds on the noisy and clean training sets:
{\small
\begin{equation} \label{eq:training}
\max_{\btheta} \frac{1}{|D_N|} \sum_n  \U^{aux}_{\btheta}(\x^{(n)}, \y^{(n)}) + \frac{1}{|D_C|} \sum_c \L^{aux}_{\btheta}(\x^{(c)}, \y^{(c)}, \yh^{(c)}).
\end{equation}}
The optimization problem is solved in a two-step iterative procedure as follows:

\textbf{E step:} The objective function is optimized with respect to $q(\yh, \h| \y, \x)$ for a fixed $\btheta$. For $\U^{aux}_{\btheta}(\x, \y)$, this is done by solving the following problem:
\begin{equation}
\min_q \ \KL[q(\yh, \h| \y, \x) || p_\btheta(\yh, \h| \y, \x)] + \alpha \KL[q(\yh, \h| \y, \x) || p_{aux}(\yh, \h| \y)].
\end{equation}
The weighted average of KL terms above is minimized with respect to $q$ when:
\begin{equation} \label{eq:geo_mean}
q(\yh, \h| \y, \x) \propto \left[ p_\btheta(\yh, \h| \y, \x) \cdot p^{\alpha}_{aux}(\yh, \h| \y) \right]^{(\frac{1}{\alpha + 1})},
\end{equation}
which is a weighted geometric mean of the true conditional distribution and auxiliary distribution. Given the factorial structure of these distributions, 
$q(\yh, \h| \y, \x)$ is also a factorial distribution:
{\small \begin{eqnarray} \label{eq:q_factors}
q(\hat{y}_i=1| \y, \x) &=& \sigma\left( \frac{1}{\alpha + 1} (\a_\bphi(\x)_{(i)} + \W_{(i,:)}\y + \alpha \a_{aux(i)} + \alpha \W_{aux(i,:)}\y)\right) \nonumber \\
q(h_j=1| \y) &=& \sigma\left( \frac{1}{\alpha + 1} (\c_{(j)} + \W'_{(j,:)}\y + \alpha \c_{aux(j)} + \alpha \W'_{aux(j,:)}\y )\right). \nonumber
\end{eqnarray} }
Optimizing $\L^{aux}_{\btheta}(\x, \y, \yh)$ w.r.t $q(\h|\y)$ gives a similar factorial result:
\begin{equation} \label{eq:geo_mean2}
q(\h| \y) \propto \left[p_\btheta(\h|\y) \cdot p^{\alpha}_{aux}(\h| \y) \right]^{(\frac{1}{\alpha + 1})}.
\end{equation}

\textbf{M step:} Holding $q$ fixed, the objective function is optimized with respect to $\btheta$. This is achieved by updating $\btheta$ in the direction of 
the gradient of $\mathbb{E}_{q(\yh, \h|\x,\y)} [\log p_\btheta(\y, \yh, \h|\x)]$, which is:
 \begin{eqnarray}
\frac{\partial}{\partial \btheta} \U^{aux}_{\btheta}(\x, \y) &=& \frac{\partial}{\partial \btheta} \mathbb{E}_{q(\yh, \h|\x,\y)} [\log p_\btheta(\y, \yh, \h|\x)] \nonumber \\
&=& - \mathbb{E}_{q(\yh, \h|\x,\y)} [\frac{\partial}{\partial \btheta} E_\btheta(\y, \yh, \h, \x)] 
+ \mathbb{E}_{p(\y, \yh, \h | \x)} [\frac{\partial}{\partial \btheta}  E_\btheta(\y, \yh, \h, \x)],  \label{eq:grad_loss_noisy}
\end{eqnarray}
where the first expectation (the \textit{positive phase}) is defined under the variational distribution $q$ and the second expectation (the \textit{negative phase}) is defined under 
the CRF model $p(\y, \yh, \h | \x)$. With the factorial form of $q$, the first expectation is analytically tractable. 
The second expectation is estimated by PCD~\cite{tieleman2008training, younes1989parametric, kirillov2015joint}. 
This approach requires maintaining a set of particles for each training instance that are used for seeding
the Markov chains at each iteration of training.

%With automated gradient computation libraries such as TensorFlow one only needs to compute the difference of average energies 
%over samples drawn from the positive and negative phase. Having passed this quantity as the training objective value to the optimizer, it will compute and back-propagate the gradient 
%of Monte Carlo estimation of the original loss over the whole network.

The gradient of the lower bound on the clean set is defined similarly:
\begin{eqnarray}
\frac{\partial}{\partial \btheta} \L^{aux}_{\btheta}(\x, \y, \yh) &=& \frac{\partial}{\partial \btheta} \mathbb{E}_{q(\h|\y)} [\log p_\btheta(\y, \yh, \h|\x)] \nonumber \\
&=& - \mathbb{E}_{q(\h|\y)} [\frac{\partial}{\partial \btheta} E_\btheta(\y, \yh, \h, \x)] 
+ \mathbb{E}_{p(\y, \yh, \h | \x)} [\frac{\partial}{\partial \btheta}  E_\btheta(\y, \yh, \h, \x)]  \label{eq:grad_loss_clean}
\end{eqnarray}
with the minor difference that in the positive phase the clean label $\yh$ is given for each instance and the variational distribution is defined over only the hidden variables.

\textbf{Scheduling $\pmb{\alpha}$:} Instead of setting $\alpha$ to a fixed value during training, it is set to a very large value at the beginning of training and is slowly decreased to smaller values.
The rationale behind this is that at the beginning of training, when $p_\btheta(\yh, \h| \y, \x)$ cannot predict the clean labels accurately, it is intuitive to rely more on pretrained $p_{aux}(\yh, \h| \y)$ 
when inferring the latent variables. As training proceeds we shift the variational distribution $q$ more toward the true conditional distribution.

Algorithm~\ref{alg:training} summarizes the learning procedure proposed for training our CRF-CNN. The training is done end-to-end for both CNN and CRF parameters together. 
In the test time, samples generated by Gibbs sampling from $p_\btheta(\y, \yh, \h | \x)$ for the test image $\x$ are used to compute the marginal $p_\btheta(\yh| \x)$. 

\begin{algorithm}[htbp]
  \DontPrintSemicolon
  %\SetKwProg{Fn}{def}{}{}
  %\SetKwFunction{Train}{train}
  \SetKwFunction{GetMinibatch}{getMinibatch}
  %\Fn{\Train{}}{
    \SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
    \Input{ Noisy dataset $D_N$ and clean dataset $D_C$, auxiliary distribution $p_{aux}(\y, \yh, \h)$, a learning rate parameter $\varepsilon$
    and a schedule for $\alpha$} 
    \Output{Model parameters: $\btheta = \{\bphi, \c, \W, \Wp \}$ }
    Initialize model parameters\;
    \While{Stopping criteria is not met}{
      \ForEach{minibatch {\small$\{(\x^{(n)}, \y^{(n)}), (\x^{(c)}, \yh^{(c)}, \y^{(c)}) \}= $ \GetMinibatch{$D_N, D_C$}} }
              {
 	        Compute {\small$q(\yh, \h| \y^{(n)}, \x^{(n)})$} by Eq.\ref{eq:geo_mean} for each noisy instance\;
 	        Compute {\small$q(\h| \y^{(c)})$} by Eq.~\ref{eq:geo_mean2} for each clean instance \;
 	        Do Gibbs sweeps to sample from the current {\small$p_\btheta(\y, \yh, \h | \x^{(\cdot)})$} for each clean/noisy instance\; % starting from the persistent chains\;
 	        $(m_n, m_c) \leftarrow$ (\# noisy instances in minibatch, \# clean instances in minibatch) \;
 	        {\small$\btheta \leftarrow \btheta + \varepsilon \big( \frac{1}{m_n} \sum_n \frac{\partial}{\partial \btheta} \U^{aux}_{\btheta}(\x^{(n)}, \y^{(n)}) \big) +
 	        \frac{1}{m_c} \sum_c \frac{\partial}{\partial \btheta} \L^{aux}_{\btheta}(\x^{(c)}, \y^{(c)}, \yh^{(c)})$} 
 	        by Eq.\ref{eq:grad_loss_noisy} and \ref{eq:grad_loss_clean}\;
              }
    }
 % }
  {}
  \caption{Train robust CNN-CRF with simple gradient descent} \label{alg:training}
\end{algorithm}

\section{Experiments}
In this section, we examine the proposed robust CNN-CRF model for the image labeling problem. 

\subsection{Microsoft COCO Dataset} \label{sec:multilabe1} The Microsoft COCO 2014 dataset is one of the largest publicly available datasets that contains both noisy and clean object labels.
Created from challenging Flickr images, it is annotated with 80 object categories as well as captions describing the images. Following \cite{Misra2016},
we use the 1000 most common words in
the captions as the set of noisy labels. We form a binary vector of this length for each image representing the words present in the caption. We use
73 object categories as the set of clean labels, and form binary vectors indicating whether the object categories are present in the image.
We follow the same 87K/20K/20K train/validation/test split as~\cite{Misra2016}, and use mean average precision (mAP) measure over these 73 object categories as the performance assessment.
Finally, we use 20\% of the training data as the clean labeled training set ($D_C$). The rest of data was used as the noisy training set ($D_N$), in which clean labels
were ignored in training.

\textbf{Network Architectures:} We use the implementation of ResNet-50~\cite{he2016deep} and VGG-16~\cite{simonyan2014very} in TensorFlow as the neural networks that compute the bias coefficients in the energy function of our CRF (Eq.~\ref{eq:crf_energy}).
These two networks are applied in a fully convolutional setting to each image. Their features in the final layer are pooled in the spatial domain using an average pooling operation, and these are passed
through a fully connected linear layer to generate the bias terms. VGG-16 is used intentionally in order to compare our method directly with \cite{Misra2016} that uses the same network. ResNet-50
experiments enable us to examine how our model works with other modern architectures. Misra et al.~\cite{Misra2016} have reported results when the images were upsampled to 565 pixels. Using upsampled images
improves the performance significantly, but they make cross validation significantly slower. Here, we report our results for image sizes of both 224 (small) and 565 pixels (large). 

\textbf{Parameters Update:} The parameters of all the networks were initialized from ImageNet-trained models that are provided in TensorFlow. The other terms in the energy function of our CRF 
were all initialized to zero. Our gradient estimates can be high variance as they are based on a Monte Carlo estimate. 
For training, we use Adam~\cite{kingma2014adam} updates that are shown to be robust against noisy gradients. 
The learning rate and epsilon for the optimizer are set to (0.001, 1) and (0.0003, 0.1) respectively in VGG-16 and ResNet-50.
We anneal $\alpha$ from 40 to 5 in 11 epochs.

\textbf{Sampling Overhead:} Fifty Markov chains per datapoint are maintained for PCD. In each iteration of the training,
the chains are retrieved for the instances in the current minibatch, and 100 iterations of Gibbs sampling are applied for negative phase samples. 
After parameter updates, the final state of chains is stored in memory for the next epoch. Note that we are only required to store the state of the 
chains for either $(\yh, \h)$ or $\y$. 
In this experiment, since the size of $\h$ is 200, the former case is more memory efficient. Storing persistent chains in this dataset 
requires only about 1 GB of memory. In ResNet-50, sampling increases the training time only by 16\% and 8\% for small and large images respectively.
The overhead is 9\% and 5\% for small and large images in VGG-16.

\textbf{Baselines:} Our proposed method is compared against several baselines visualized in Fig.~\ref{fig:baselines}:
\begin{itemize}
\item \textbf{Cross entropy loss with clean labels:} The networks are trained using cross entropy loss with the all clean labels. This defines a performance upper bound for each network.
\item \textbf{Cross entropy loss with noisy labels:} The model is trained using only noisy labels. Then, predictions on the noisy labels are mapped to clean labels using the manual mapping in \cite{Misra2016}.
\item \textbf{No pairwise terms:} All the pairwise terms are removed and the model is trained using analytic gradients without any sampling using our proposed objective function in Eq.~\ref{eq:training}.
\item \textbf{CRF without hidden:} $\W$ is trained but $\W'$ is omitted from the model.
\item \textbf{CRF with hidden:} Both $\W$ and $\W'$ are present in the model.
\item \textbf{CRF without $\x-\y$ link:} Same as the previous model but $\b$ is not a function of $\x$.
\item \textbf{CRF without $\x-\y$ link ($\pmb{\alpha=0}$):} Same as the previous model but trained with $\alpha = 0$.
\end{itemize}

The experimental results are reported in Table~\ref{table:coco} under ``Caption Labels.'' A performance increase
is observed after adding each component to the model. However, removing the $\x-\y$ link generally improves the performance
significantly. This may be because removing this link forces the model to rely on $\yh$ and its correlations with $\y$ for predicting $\y$ on the noisy labeled set.
This can translate to better recognition of clean labels.
Last but not least, the CRF model with no $\x-\y$ connection trained using $\alpha=0$ performed very poorly on this dataset.
This demonstrates the importance of the introduced regularization in training.

\begin{figure}[t]
 \centering
    \subfloat[Clean]{ $\ \ $
 \begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=1.1cm, ultra thick, scale=0.8]
      \tikzstyle{every state}=[fill=white,draw=black,text=black, transform shape]
      \node[state, fill=lightgray] 	(x)                    {$\x$};
      \node[state] 			(yh)   [below of=x]    {$\yh$};
      \path (x)         edge[-]               (yh);
  \end{tikzpicture}  $\ \ $
  } \hspace{0.2cm}
    \subfloat[Noisy]{ $\ \ $
 \begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=1.1cm, ultra thick, scale=0.8]
      \tikzstyle{every state}=[fill=white,draw=black,text=black, transform shape]
      \node[state, fill=lightgray] 	(x)                    {$\x$};
      \node[state] 			(y)   [below of=x]    {$\y$};
      \path (x)         edge[-]               (y);
  \end{tikzpicture}  $\ \ $
  } \hspace{0.2cm}
  \subfloat[No link]{
 \begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=1.1cm, ultra thick, scale=0.8]
      \tikzstyle{every state}=[fill=white,draw=black,text=black, transform shape]
      \node[state, fill=lightgray] 	(x)                    {$\x$};
      \node[state] 			(yh)   [below of=x, xshift=-0.6cm]    {$\yh$};
      \node[state] 			(y)   [right of=yh]    {$\y$};
      %\node[state] 			(h)   [right of=y]     {$\h$};
      \path (x)         edge[-]               (yh);
      \path (x)         edge[-]               (y);
      %\path (yh)        edge[-]               (y);
      %\path (y)         edge[-]               (h);
  \end{tikzpicture}  
  } \hspace{0.2cm}
  \subfloat[CRF w/o $\h$]{
 \begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=1.1cm, ultra thick, scale=0.8]
      \tikzstyle{every state}=[fill=white,draw=black,text=black, transform shape]
      \node[state, fill=lightgray] 	(x)                    {$\x$};
      \node[state] 			(yh)   [below of=x, xshift=-0.6cm]    {$\yh$};
      \node[state] 			(y)   [right of=yh]    {$\y$};
      % \node[state] 			(h)   [right of=y]     {$\h$};
      \path (x)         edge[-]               (yh);
      \path (x)         edge[-]               (y);
      \path (yh)        edge[-]               (y);
      %\path (y)         edge[-]               (h);
  \end{tikzpicture}  $\ \ $
  } \hspace{0.2cm}
  \subfloat[CRF w/ $\h$]{
 \begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=1.1cm, ultra thick, scale=0.8]
      \tikzstyle{every state}=[fill=white,draw=black,text=black, transform shape]
      \node[state, fill=lightgray] 	(x)                    {$\x$};
      \node[state] 			(yh)   [below of=x, xshift=-0.6cm]    {$\yh$};
      \node[state] 			(y)   [right of=yh]    {$\y$};
      \node[state] 			(h)   [right of=y]     {$\h$};
      \path (x)         edge[-]               (yh);
      \path (x)         edge[-]               (y);
      \path (yh)        edge[-]               (y);
      \path (y)         edge[-]               (h);
  \end{tikzpicture}  
  } \hspace{0.2cm} 
  \subfloat[CRF w/o $\x-\y$]{
 \begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=1.1cm, ultra thick, scale=0.8]
      \tikzstyle{every state}=[fill=white,draw=black,text=black, transform shape]
      \node[state, fill=lightgray] 	(x)                    {$\x$};
      \node[state] 			(yh)   [below of=x, xshift=-0.6cm]    {$\yh$};
      \node[state] 			(y)   [right of=yh]    {$\y$};
      \node[state] 			(h)   [right of=y]     {$\h$};
      \path (x)         edge[-]               (yh);
      % \path (x)         edge[-]               (y);
      \path (yh)        edge[-]               (y);
      \path (y)         edge[-]               (h);
  \end{tikzpicture}  
  }   
  \caption{Visualization of different variations of the model examined in the experiments.}
  \label{fig:baselines}
\end{figure}


\subsection{Microsoft COCO Dataset with Flickr Tags} \label{sec:multilabe2}
The images in the COCO dataset were originally gathered and annotated from the Flickr website. This means that these image have actual noisy Flickr tags. 
To examine the performance of our model on actual noisy labels, we collected these tags for the COCO images using Flickr's public API. 
Similar to the previous section, we used the 1024 most common tags as the set of noisy labels. We observed that these tags have significantly more
noise compared to the noisy labels in the previous section; therefore, it is more challenging to predict clean labels from them using the auxiliary distribution.
In this section, we only examine the ResNet-50 architecture for both small and large image sizes. The different baselines introduced 
in the previous section are compared against each other in Table~\ref{table:coco} under ``Flickr Tags.''

\textbf{Auxiliary Distribution vs. Variational Distribution:} As the auxiliary distribution $p_{aux}$ is fixed, and the variational distribution $q$ is updated using Eq.~\ref{eq:geo_mean} 
in each iteration, a natural question is how $q$ differs from $p_{aux}$. Since, we have access to the clean labels in the 
COCO dataset, we examine the accuracy of $q$ in terms of predicting clean labels on the noisy training set ($D_N$) using the mAP measurement at the beginning and end of training
the CRF-CNN model (ResNet-50 on large images). We observed that at the beginning of training, when $\alpha$ is big, $q$ is almost equal to $p_{aux}$, which obtains 49.4\% mAP on this set.
As training iterations proceed, the accuracy of $q$ increases to 69.4\% mAP. Note that the 20.0\% gain in terms of mAP is very significant, and it demonstrates 
that combining the auxiliary distribution with our proposed CRF can yield a significant performance gain in inferring latent clean labels. In other words,
our proposed model is capable of cleaning the noisy labels and proposing more accurate labels on the noisy set as training continues.
%Please refer to our supplementary material for a qualitative comparison between $q$ and $p_{aux}$.
Fig.~\ref{fig:coco_res} in the appendix compares $q$ against $p_{aux}$ in terms of its ability to infer clean labels for a few instances in the noisy training set ($D_N$).

\begin{table}[t]
\caption{The performance of different baselines on the COCO dataset in terms of mAP (\%).} \label{table:coco}
\centering
\begin{tabular}{ lcccccccc }
\multicolumn{1}{c}{} & \multicolumn{5}{c}{Caption Labels (Sec.~\ref{sec:multilabe1})} && \multicolumn{2}{c}{Flickr Tags (Sec.~\ref{sec:multilabe2})} \\
\cline{2-6} \cline{8-9}
 \multicolumn{1}{c}{} & \multicolumn{2}{c}{ResNet-50} && \multicolumn{2}{c}{VGG-16} &&\multicolumn{2}{c}{ResNet-50} \\
 \cline{2-3}\cline{5-6} \cline{8-9}
 Baseline  & Small & Large && Small & Large && Small & Large \\
\hline
 Cross entropy loss w/ clean & 68.57 & 78.38 && 71.99 & 75.50  && 68.57 & 78.38\\
 Cross entropy loss w/ noisy & 56.88 & 64.13 && 58.59 & 62.75  && - & - \\
 \hline
No pairwise link & 63.67 & 73.19 && 66.18 & 71.78  && 58.01 & \textbf{67.84}         \\
CRF w/o hidden & 64.26 & 73.23 && 67.73 & 71.78  && 59.04 & 67.22      \\
 CRF w/ hidden & 65.73 & 74.04 && 68.35 & 71.92   && 59.19 & 67.33           \\
CRF w/o $\x-\y$ link & \textbf{66.61} & \textbf{75.00} && \textbf{69.89} & \textbf{73.16} && \textbf{60.97} & 67.57  \\
CRF w/o $\x-\y$ link ($\alpha=0$) & 48.53 & 56.53 && 56.76 & 56.39 && 47.25 & 58.74  \\
 \hline
 Misra et al.~\cite{Misra2016} & - & - && - & 66.8 && - & -  \\
 Fang et al.~\cite{fang2015captions} reported in \cite{Misra2016} & - & - && - & 63.7 && - & -  \\
\end{tabular}
\end{table}

\subsection{CIFAR-10 Dataset} \label{sec:multiclass}
We also apply our proposed learning framework to the object classification problem in the CIFAR-10 dataset. 
This dataset contains images of 10 objects resized to 32x32-pixel images.
We follow the settings in \cite{patrini2016} and we inject synthesized noise to the original labels in training.
Moreover, we implement the \textit{forward} and \textit{backward} losses proposed in \cite{patrini2016} and 
we use them to train ResNet~\cite{he2016deep} of depth 32 with the ground-truth noise transition matrix. 

Here, we only train the variant of our model shown in Fig.~\ref{fig:baselines}.c that can be trained analytically. For the auxiliary
distribution, we trained a simple linear multinomial logistic regression representing the conditional $p_{aux}(\yh | \y)$ with no hidden variables ($\h$) . 
We trained this distribution such that the output probabilities match the ground-truth noise transition matrix.
We trained all models for 200 epochs. For our model, we anneal
$\alpha$ from 8 to 1 in 10 epochs. Similar to the previous section, we empirically observed that it is better to stop annealing $\alpha$ before it reaches zero.
Here, to compare our method with the previous work, we do not work in a semi-supervised setting, and we assume that we have access only to the noisy training dataset. 

Our goal for this experiment is to demonstrate that
a simple variant of our model can be used for training from images with only noisy labels  and to show that our model can
clean the noisy labels. To do so, we report not only the average accuracy on the clean test dataset, but also the \textit{recovery accuracy}. 
The recovery accuracy for our method is defined as the accuracy of $q$ in predicting the clean labels in the noisy training set at the end of learning. For the baselines,
we measure the accuracy of the trained neural network $p(\yh|\x)$ on the same set.
The results are reported in Table~\ref{table:cifar}.  Overall, our method achieves slightly better prediction accuracy on the CIFAR-10 dataset than the baselines.
And, in terms of recovering clean labels on the noisy training set, our model significantly outperforms the baselines.
%Examples of the recovered clean labels are visualized for the CIFAR-10 experiment in the supplementary material.
Examples of the recovered clean labels are visualized for the CIFAR-10 experiment in Fig.~\ref{fig:cifar_res} in the appendix.

\begin{table}[t]
\caption{Prediction and recovery accuracy of different baselines on the CIFAR-10 dataset.} \label{table:cifar}
\centering
\begin{tabular}{ lcccccccccccc}
\multicolumn{1}{c}{} & \multicolumn{5}{c}{Prediction Accuracy (\%)} && \multicolumn{5}{c}{Recovery Accuracy (\%)} \\
\cline{2-6} \cline{8-12}
Noise (\%)  & 10 & 20 & 30 & 40 & 50 && 10 & 20 & 30 & 40 & 50 \\
\hline
Cross entropy loss  & 91.2 & 90.0 & 89.1 & 87.1 & 80.2          && 94.1 & 92.4 & 89.6 & 85.2 & 74.6 \\
Backward~\cite{patrini2016} & 87.4 & 87.4 & 84.6 & 76.5 & 45.6  && 88.0 & 87.4 & 84.0 & 75.3 & 44.0 \\
Forward~\cite{patrini2016} & 90.9 & 90.3 & 89.4 & 88.4 & 80.0   && 94.6 & 93.6 & 92.3 & 91.1 & 83.1 \\
Our model & \textbf{91.6} & \textbf{91.0} & \textbf{90.6} & \textbf{89.4} &  \textbf{84.3} && \textbf{97.7} & \textbf{96.4} & \textbf{95.1} & \textbf{93.5} & \textbf{88.1} \\
\end{tabular}
\end{table}

\section{Conclusion}
We have proposed a general undirected graphical model for modeling label noise in training deep neural networks. 
We formulated the problem as a semi-supervised learning problem, and we proposed
a novel objective function equipped with a regularization term that helps our variational distribution infer latent clean labels
more accurately using auxiliary sources of information. Our model not only predicts clean labels on unseen instances more accurately,
but also recovers clean labels on noisy training sets with a higher precision.
We believe the ability to clean
noisy annotations is a very valuable property of our framework that will be useful in many application domains.

\subsubsection*{Acknowledgments}
The author thanks Jason Rolfe, William Macready, Zhengbing Bian, and Fabian Chudak for their helpful discussions and comments. This work would not be
possible without the excellent technical support provided by Mani Ranjbar and Oren Shklarsky.
% after submission end

\bibliographystyle{unsrt} % abbrvnat
\small
\bibliography{label_noise}

\appendix
\iffalse
\section{Visualization}
As shown in the E step in Sec. 3.3, the variational distribution $q$ infers latent clean labels by combining information from both the image-based CRF-CNN model $p_\btheta(\yh, \h| \y, \x)$ 
and the label-based auxiliary distribution $p^{\alpha}_{aux}(\yh, \h| \y)$. In our experiments, we observe that in general $q$ proposes clean labels more accurately than the auxiliary distribution. 
Fig.~\ref{fig:coco_res} compares $q$ against $p_{aux}$ in terms of its ability to infer clean labels for a few instances in the noisy training set ($D_N$) for the COCO experiment
with actual Flickr tags. In Fig.~\ref{fig:cifar_res}, examples of the recovered clean labels are visualized for the CIFAR-10 experiment.
\fi

\begin{figure}
\centering
\small
\begin{tabular}{c l l}
\multirow{5}{*}{\subfloat{\includegraphics[trim={0cm 0cm 0cm 0cm},clip, height=0.13\textwidth, width=0.25\textwidth]{COCO_train2014_000000204275.jpg}}} 
  & &\\[1.0ex]
  & \textbf{Flickr} & $\varnothing$ \\[0.5ex]
  & $\pmb{p_{aux}}$ & person \\[0.5ex]
  & $\pmb{q}$       & skateboard, person \\[0.6ex]
  & \textbf{clean}  & skateboard, person \\[1.1ex] \hline
\multirow{5}{*}{\subfloat{\includegraphics[trim={0cm 0cm 0cm 0cm},clip, height=0.13\textwidth, width=0.25\textwidth]{COCO_train2014_000000418087.jpg}}}  
  & & \\[1.0ex]
  & \textbf{Flickr} & $\varnothing$ \\[0.5ex]
  & $\pmb{p_{aux}}$ & person \\[0.5ex]
  & $\pmb{q}$       & person, baseball glove, baseball bat \\[0.6ex]
  & \textbf{clean}  & person, baseball glove, baseball bat, sports ball, chair, bench \\[1.5ex] \hline
\multirow{5}{*}{\subfloat{\includegraphics[trim={0cm 1cm 0cm 3cm},clip, height=0.16\textwidth, width=0.25\textwidth]{COCO_train2014_000000167494.jpg}}}  
  & &\\[2.0ex]
  & \textbf{Flickr} & 2009, miami \\[0.5ex]
  & $\pmb{p_{aux}}$ & person \\[0.5ex]
  & $\pmb{q}$       & person, tennis racket \\[0.6ex]
  & \textbf{clean}  & person, tennis racket \\[3.1ex] \hline
\multirow{5}{*}{\subfloat{\includegraphics[trim={0cm 0cm 0cm 0cm},clip, height=0.13\textwidth, width=0.25\textwidth]{COCO_train2014_000000412296.jpg}}}  
  & &\\[1.0ex]
  & \textbf{Flickr} & uploaded:by=flickr\_mobile, flickriosapp:filter=NoFilter \\[0.5ex]
  & $\pmb{p_{aux}}$ & person \\[0.5ex]
  & $\pmb{q}$       & person, surfboard \\[0.6ex]
  & \textbf{clean}  & person, surfboard \\[1.1ex] \hline
\multirow{5}{*}{\subfloat{\includegraphics[trim={0cm 0cm 0cm 0cm},clip, height=0.13\textwidth, width=0.25\textwidth]{COCO_train2014_000000178192.jpg}}}  
  & &\\[1.0ex]
  & \textbf{Flickr} & computer  \\[0.5ex]
  & $\pmb{p_{aux}}$ & $\varnothing$ \\[0.5ex]
  & $\pmb{q}$       & laptop, mouse, tv, keyboard \\[0.6ex]
  & \textbf{clean}  & laptop, mouse, tv, keyboard \\[1.1ex] \hline
\multirow{5}{*}{\subfloat{\includegraphics[trim={0cm 0cm 0cm 0cm},clip, height=0.15\textwidth, width=0.25\textwidth]{COCO_train2014_000000537555.jpg}}}  
  & &\\[0.1ex]
  & \textbf{Flickr} & square, iphoneography, square format, instagram app, \\
  &                 & uploaded:by=instagram \\[0.5ex]
  & $\pmb{p_{aux}}$ & $\varnothing$ \\[0.5ex]
  & $\pmb{q}$       & cup, dining table, bottle, bowl \\[0.6ex]
  & \textbf{clean}  & cup, dining table, bottle, bowl, spoon, hot dog \\[1.1ex] \hline
\multirow{7}{*}{\subfloat{\includegraphics[trim={0cm 2cm 0cm 0cm},clip, height=0.17\textwidth, width=0.25\textwidth]{COCO_train2014_000000174667.jpg}}}  
  & &\\[1.0ex]
  & \textbf{Flickr} & food, square, square format, nikon, white, orange, fruit, color, \\
  &                 & India, photography, table, project365, bowl, colour, wood, 50mm, \\
  &                 & nikkor, bokeh \\[0.5ex]
  & $\pmb{p_{aux}}$ & orange, apple, banana \\[0.5ex]
  & $\pmb{q}$       & orange, bowl, dining table \\[0.6ex]
  & \textbf{clean}  & orange, bowl \\[1.1ex] \hline
\multirow{5}{*}{\subfloat{\includegraphics[trim={0cm 0cm 0cm 0cm},clip, height=0.16\textwidth, width=0.25\textwidth]{COCO_train2014_000000340504.jpg}}}  
  & &  \\[0.1ex]
  & \textbf{Flickr} & home, light, photo, art, chair, room, table, architecture, apartment, \\
  &                 & interior, couch, decor, beauty, design, live, lamp, indoor, furniture, \\
  &                 & relaxed, sofa, flooring, modern \\[0.5ex]
  & $\pmb{p_{aux}}$ & chair, couch, vase, book, dining table, sink, clock, bed, potted plant \\[0.5ex]
  & $\pmb{q}$       & chair, couch, vase, book \\[0.6ex]
  & \textbf{clean}  & chair, dining table, tv  \\[1.1ex] 
\end{tabular}
\caption{Visualization of inferred labels for a few instances in the noisy training set ($D_N$) of the COCO dataset. Flickr labels represent the noisy labels extracted from
Flickr tags, whereas 
clean labels are the true labels ignored during training. $p_{aux}$ and $q$ correspond to the labels that are extracted using 
these distribution by thresholding at 0.5. The auxiliary distribution $p_{aux}$ tends to assign the label ``person'' to the images with no tag while $q$ adds more clean labels. 
In the last two images, $q$ removes a few unrelated labels.
}
\label{fig:coco_res}
\end{figure}
% after submission end

% after submission
\begin{figure}
 \centering
 \subfloat[cat $\rightarrow$ dog]{\includegraphics[trim={0.7cm 0cm 0cm 0cm},clip, width=0.45\textwidth]{4.png}} \hspace{0.5cm}
 \subfloat[dog $\rightarrow$ cat]{\includegraphics[trim={0.7cm 0cm 0cm 0cm},clip, width=0.45\textwidth]{3.png}} \\
 \subfloat[automobile $\rightarrow$ truck]{\includegraphics[trim={0.7cm 0cm 0cm 0cm},clip, width=0.45\textwidth]{1.png}} \hspace{0.5cm}
 \subfloat[horse $\rightarrow$ deer]{\includegraphics[trim={0.7cm 0cm 0cm 0cm},clip, width=0.45\textwidth]{2.png}}
  \caption{Our proposed model can recover clean labels in the noisy training dataset. Here, corrupted instances are visualized for different categories in the CIFAR-10 training dataset.
  Sub-figures (a) through (d), captioned with \textit{annotated label} $\rightarrow$ \textit{inferred label}, represents the instances that are labeled with the annotated label
  but have been assigned to the inferred label by our proposed variational distribution $q$. In this visualization, images are sorted based on the confidence of $q$
  for the inferred label from left to right and top to bottom, and the mistaken instances are marked with the red frame. The probability that $q$ assigns for the inferred
  label is typically very high ($>0.9$) for these images, which indicates that $q$ is confident in changing the noisy labels.}
  \label{fig:cifar_res}
\end{figure}




\end{document}
