The maximum likelihood parameter estimation requires to find a maximizer of a log-likelihood function which for comparison sets of cardinality $k$ has the form of a sum of $\binom{n}{k}$ elements in the worst case. For pair comparisons, this sum consists of at most $n^2$ elements. It is common for parameter estimators to be defined as maximizers of a pseudo log-likelihood function, which is defined as the log-likelihood function of pair comparisons deduced from the input observations under assumption that these pair comparisons are independent (which in general is false under a Thurstone choice model for comparison sets of three or more items). This is commonly referred as \emph{rank breaking}. In what follows, we consider two different rank-breaking methods: (a) one that deduces $k-1$ pair comparisons from a choice from a comparison set of cardinality $k$, we refer to as rank-breaking method $\mathsf{ALL}$, and (b) one that deduces $1$ pair comparison from a choice from a comparison set of cardinality $k$, we refer to as rank-breaking method $\mathsf{ONE}$. 

\label{sec:rankA}
\paragraph{Rank-breaking method $\mathsf{ALL}$} This rank-breaking method deduces $k-1$ pair comparisons from each comparison set of cardinality $k$. Specifically, for every comparison set $S_t$, the method uses pair comparisons between the chosen item $y_t$ and each non-chosen item $v\in S_t\setminus\{ y_t\}$. Notice that for any comparison set of three or more items, the pair comparisons selected from this set by the given rank-breaking method are not independent. 

For the Luce choice model, the pseudo log-likelihood function is given by (\ref{equ:pseudolikk}), which can be written in a more explicit form as follows
$$
\ell_{k-1}(\theta) = \sum_{t=1}^m\sum_{v\in S_t\setminus\{y_t \}} \log\left(\frac{e^{\theta_{y_t}}}{e^{\theta_{y_t}} + e^{\theta_v}}\right).
$$
We consider the maximum pseudo log-likelihood estimator $\widehat{\theta}_{k-1} := \arg \max_{\theta \in \Theta} \ell_{k-1}(\theta)$.
 
\begin{theorem} If $\lambda_2\left(L_\vec{M} \right) \ge 128(k-1)^2e^{2b} n\log (n)/m$, then with probability at least $1-3/n$,
$$
\mse(\widehat{\theta}_{k-1}, \theta^\star) \le D^2 
\frac{n(\log(n)+2)}{\lambda_2(L_\vec{M})^2}\frac{1}{m}. 
$$
where $D = 16\sqrt{2}\sqrt{k(k-1)^3} e^{2b}$.
\label{thm:break-1}
\end{theorem} 

The proof of Theorem~\ref{thm:break-1} is given in Appendix~\ref{sec:proof-break-1}.

The mean squared error upper bound in Theorem~\ref{thm:break-1} implies that the given parameter estimator is consistent. For any fixed size of a comparison set, the bound in Theorem~\ref{thm:break-1} is equal to that in Theorem~\ref{thm:full} up to a constant factor. Both bounds have the same scaling with parameter $k$.

\label{sec:rankB}
\paragraph{Rank-breaking method $\mathsf{ONE}$} This rank-breaking method deduces $1$ pair comparison from a comparison set of cardinality $k$. From each comparison set $S_t$, this rank-breaking methods selects a pair that consists of the chosen item $y_t$ and an item $z_t$ selected uniformly at random from the set of non-chosen items $S_t\setminus \{y_t\}$. 

For the Luce choice model, the pseudo log-likelihood function is given by (\ref{equ:pseudolik1}), which can be written in a more explicit form as follows
$$
\ell_1(\theta) = \sum_{t=1}^m \log\left(\frac{e^{\theta_{y_t}}}{e^{\theta_{y_t}} + e^{\theta_{z_t}}}\right).
$$
We consider the maximum pseudo log-likelihood estimator $\widehat{\theta}_1 := \arg\max_{\theta\in \Theta} \ell_1(\theta)$.

\begin{theorem} If $\lambda_2\left(L_\vec{M} \right) \ge 8k(k-1)e^{2b}n\log (n)/m$, then with probability at least $1-3/n$,
$$
\mse(\widehat{\theta}_1, \theta^\star) 
\le D^2\frac{n(\log(n)+2)}{\lambda_2(L_\vec{M})^2}\frac{1}{m}
$$
where $D = 4k(k-1) e^{2b}$.
\label{thm:break}
\end{theorem}

The proof of Theorem~\ref{thm:break} is given in Appendix~\ref{sec:proof-break}.

It is noteworthy that the mean squared error upper bounds in Theorem~\ref{thm:break-1} and Theorem~\ref{thm:break} are equal up to a constant factor. Intuitively, one would expect that rank-breaking method $\mathsf{ALL}$ would yield a smaller mean squared error than rank-breaking method $\mathsf{ONE}$ because it uses more information from each observed choice. The reason why the two mean squared error upper bounds are equal up to a constant factor is as follows. When applying Lemma~\ref{lem:mle-taylor} we need to find an upper bound $\alpha$ for the norm of the gradient of the negative pseudo log-likelihood function and a lower bound $\beta$ for the second-smallest eigenvalue of the Hessian matrix of the negative pseudo log-likelihood function. In our proofs, for the case of Theorem~\ref{thm:break}, we obtained $\alpha$ and $\beta$ that scale with $k$ as $1$ and $1/k^2$, respectively. On the other hand, for the case of Theorem~\ref{thm:break-1}, we obtained $\alpha$ and $\beta$ that scale with $k$ as $k$ and $1/k$, respectively. For both cases, it follows that the ratio $\alpha/\beta$ scales as $k^2$.


