In this section, we present upper and lower bounds for the mean squared error of the maximum likelihood parameter estimator for the Thurstone choice model. We will first consider the case of pair comparisons. We then consider the more general case when each comparison set consists of two or more items. For this more general case, we first give an upper bound for the Luce choice model, and then present similar characterization for a class of Thurstone choice models. We end this section with a lower bound on the mean squared error of the maximum likelihood parameter estimator, which establishes minimax optimality of our upper bounds.   

\subsection{Pair Comparisons}
\label{sec:pairs}

We consider pair comparisons according to a Thurstone choice model \GT\ with parameter vector $\theta^\star$ that takes value in $\Theta = [-b,b]^n$ and that satisfies the following conditions:
\begin{itemize}
\item[\bf P1] There exists $A > 0$ such that
\begin{equation}
\frac{d^2 \log(p_2(x))}{dx^2} \leq -A \hbox{ for all } x\in [-2b,2b].
\label{equ:Bcond}
\end{equation} 
\item[\bf P2] There exists $B > 0$ such that
\begin{equation}
\frac{d \log(p_2(x))}{dx} \leq B \hbox{ for all } x \in [-2b,2b].
\label{equ:Acond}
\end{equation}
\item [\bf P3] The weighted-adjacency matrix $\vec{M}$ is irreducible, i.e. $\lambda_2(L_{\vec{M}}) > 0$.
\end{itemize}

Condition {\bf P1} means that $p_2$ is a strictly log-concave function on $[-2b,2b]$. Condition {\bf P2} means that $\log(p_2(x))$ has a bounded derivative on $[-2b,2b]$. Notice that this condition is equivalent to $dp_2(x)/dx \leq B p_2(x)$ for all $x\in [-2b,2b]$. Constants $A$ and $B$ are specific for given Thurstone choice model and the value of the parameter $b$. In particular, for the Bradley-Terry model, it can be easily checked that  {\bf P1} and {\bf P2} hold with $A = e^{-2b/\beta}/[\beta^2(1+e^{-2b/\beta})^2]$ and $B = 1/[\beta(1+e^{-2b/\beta})]$. Condition {\bf P3} means that the observations are such that the graph defined by the weighted-adjacency matrix is connected. Equivalently, there exists no partition of the set of items $N$ into two non-empty sets $S$ and $N\setminus S$ such that some pair of items $i\in S$ and $j\in N\setminus S$ is not compared in the input observations.

\begin{theorem} Under conditions {\bf P1}, {\bf P2} and {\bf P3}, with probability at least $1-2/n$, 
\begin{equation}
\mse(\widehat{\theta},\theta^\star) \le D^2\frac{n(\log(n)+2)}{\lambda_{2}(L_{\vec{M}_{1/4}})^2}\frac{1}{m}
\label{eq:hajek}
\end{equation}
where $D = B/A$.
\label{thm:mle}
\end{theorem}

Before we show a proof of the theorem, we note the following remarks. 

First, notice that $D$ is a constant whose value is specific to given Thurstone choice model and the value of parameter $b$. In particular, for the Bradley-Terry model, we have $D = \beta(e^{2b/\beta} + 1)$.

Second, from (\ref{eq:hajek}), for the mean squared error to be smaller than or equal to $\epsilon^2$, for given $\epsilon > 0$, it suffices that the number of observations is such that  
\begin{equation}
m \geq \frac{1}{\epsilon^2} D^2 \frac{1}{\lambda_2(L_{\vec{M}_{1/4}})^2}\, n(\log(n)+2).
\label{equ:suffm}
\end{equation}
Third, and last, if each pair of items is compared the same number of times, then, from (\ref{equ:unbiased}), we have $m_{i,j} = 1/(2(n-1))$ for all $i\neq j$. Hence, in this case $\lambda_2(L_{\vec{M}_{1/4}}) = n/(2(n-1))$, and, from (\ref{equ:suffm}), it suffices that the number of observations $m$ is such that 
$$
m \geq \frac{4}{\epsilon^2} D^2\, n(\log(n)+2).
$$ 

\paragraph{Proof of Theorem~\ref{thm:mle}} We now go on to provide a proof of Theorem~\ref{thm:mle}. For pair comparisons, the log-likelihood function (\ref{equ:loglik0}) can be written as
$$
\ell(\theta) = \sum_{t=1}^m \log(p_2(\theta_{y_t}-\theta_{z_t}))
$$
where $y_t$ denotes the choice from the comparison pair $S_t = \{y_t,z_t\}$ for each observation $t$. The negative log-likelihood function satisfies the relation in Lemma~\ref{lem:mle-taylor}, which combined with the following two lemmas, establishes the statement of the theorem.

\begin{lemma} The following relation holds:
$$
\min_{\theta\in \Theta}\lambda_2(\nabla^2(-\ell(\theta))) \geq 4 A\frac{m}{n}\lambda_2(L_{\vec{M}_{1/4}}).
$$
\label{lem:LT21}
\end{lemma}

\begin{proof} It is easy to verify that for all $\theta \in \reals^n$ and $i,j\in N$, we have the following identities
\begin{eqnarray*}
\frac{d^2 \log(p_2(\theta_i - \theta_j))}{dx^2} &=& \frac{\partial^2 \log(p_2(\theta_i - \theta_j))}{\partial \theta_i^2}\\
& = & \frac{\partial^2 \log(p_2(\theta_i - \theta_j))}{\partial \theta_j^2}\\
& = & \frac{\partial^2 (-\log(p_2(\theta_i - \theta_j)))}{\partial \theta_i\partial \theta_j}.
\end{eqnarray*}

For all $i,j\in N$ such that $i\neq j$, we have
$$
\frac{\partial^2 (-\log(p_2(\theta_{y_t}-\theta_{z_t})))}{\partial \theta_i \partial \theta_j} = \left\{
\begin{array}{ll}
\frac{d^2}{dx^2}\log(p_2(\theta_{y_t}-\theta_{z_t})), & \hbox{ if } \{i,j\} = \{y_t,z_t\}\\
0, & \hbox{ otherwise}
\end{array}
\right .
$$
and 
$$
\frac{\partial^2 (-\log(p_2(\theta_{y_t}-\theta_{z_t})))}{\partial \theta_i^2} = - \sum_{j\neq i} \frac{\partial^2 (-\log(p_2(\theta_{y_t}-\theta_{z_t})))}{\partial \theta_i \partial \theta_j}.
$$

Combining with condition {\bf P1}, we have
$$
\frac{\partial^2 (-\log(p_2(\theta_{y_t}-\theta_{z_t})))}{\partial \theta_i \partial \theta_j} \leq -A 1_{\{y_t,z_t\}=\{i,j\}}.
$$
It follows that 
\begin{equation}
\nabla^2(-\ell(\theta)) \succeq 4A\frac{m}{n}L_{\vec{M}_{1/4}}, \hbox{ for all } \theta \in [-b,b]^n
\label{equ:succtemp}
\end{equation}
where for two matrices $\vec{A}$ and $\vec{B}$, $\vec{A} \succeq \vec{B}$ is equivalent to saying that $\vec{A}-\vec{B}$ is positive definite; see Appendix~\ref{sec:back}. In (\ref{equ:succtemp}), both $\nabla^2(-\ell(\theta))$ and $4A\frac{m}{n}L_{\vec{M}_{1/4}}$ are positive-definite matrices. Hence, by the elementary fact stated in Lemma~\ref{lem:EigAsuccB} (Appendix), we obtain the assertion of the lemma.
\end{proof}

\begin{lemma} With probability at least $1-2/n$,
$$
\|\nabla(-\ell(\theta^\star))\|_2 \leq 2B\sqrt{m(\log(n)+2)}.
$$
\label{lem:LT22}
\end{lemma}

\begin{proof}
$\nabla(-\ell(\theta))$ is a sum of independent random vectors in $\reals^n$ given by
$$
\nabla(-\ell(\theta)) = \sum_{t=1}^m \nabla(-\log(p_2(\theta_{y_t}-\theta_{z_t}))).
$$
The elements of $\nabla(-\log(p_2(\theta_{y_t}-\theta_{z_t})))$ can be expressed as follows
$$
\frac{\partial (-\log(p_2(\theta_{y_t}-\theta_{z_t})))}{\partial \theta_i} = \left\{
\begin{array}{ll}
-\frac{d\log(p_2(\theta_{y_t}- \theta_{z_t}))}{dx}, & \hbox{ if } i = y_t\\
+\frac{d\log(p_2(\theta_{y_t}- \theta_{z_t}))}{dx}, & \hbox{ if } i = z_t\\
0, & \hbox{ otherwise}.
\end{array}
\right .
$$

If $i\notin \{y_t,z_t\}$, then clearly 
$$
\E\left[\frac{\partial (-\log(p_2(\theta_{y_t}^\star-\theta_{z_t}^\star)))}{\partial \theta_i}\right] = 0.
$$
Otherwise, we have
\begin{eqnarray*}
\E\left[\frac{\partial (-\log(p_2(\theta_{y_t}^\star-\theta_{z_t}^\star)))}{\partial \theta_i}\right] & = & -p_2(\theta_{y_t}^\star-\theta_{z_t}^\star)\frac{d\log(p_2(\theta_{y_t}^\star - \theta_{z_t}^\star))}{dx}\\
&& + p_2(\theta_{z_t}^\star-\theta_{y_t}^\star)\frac{d\log(p_2(\theta_{z_t}^\star - \theta_{y_t}^\star))}{dx}\\
&=& - \frac{dp_2(\theta_{y_t}^\star-\theta_{z_t}^\star)}{dx} + \frac{dp_2(\theta_{z_t}^\star-\theta_{y_t}^\star)}{dx}\\
&=& 0
\end{eqnarray*}
where the last equation is by the fact that $d p_2(x)/dx$ is an even function.

By condition {\bf P2}, we have
\begin{eqnarray*}
\|\nabla(-\log(p_2(\theta_{y_t}-\theta_{z_t})))\|_2^2  &=& \left(\frac{d\log(p_2(\theta_{y_t}-\theta_{z_t}))}{dx}\right)^2 + \left(-\frac{d\log(p_2(\theta_{y_t}-\theta_{z_t}))}{dx}\right)^2\\
& \leq & 2B^2.
\end{eqnarray*}
Hence, by the vector Azuma-Hoeffding bound in Lemma~\ref{prop:azuma-hoeffding}, we have
$$
\Pr[\|\nabla(-\ell(\theta^\star)\|_2 \geq 2B\sqrt{m(\log(n)+2)}] \leq \frac{2}{n}.
$$
\end{proof}

\subsection{Comparison Sets of Two or More Items}
\label{sec:kary}

We now consider a more general case were each comparison set consists of two or more items. We first show an upper bound for the mean squared error of the maximum likelihood parameter estimator when the choices are according to the Luce choice model and comparison sets are of identical sizes. We then present a similar characterization under more general assumption that allow for a broader set of Thurstone choice models and non-identical sizes of comparison sets.  

\begin{theorem} Suppose that choices are according to the Luce choice model, all comparison sets are of cardinality $k\geq 2$, and $\lambda_2(L_{\vec{M}})>0$. Then, with probability at least $1-2/n$, 
$$
\mse(\widehat\theta, \theta^\star)
\le D^2\frac{n(\log (n)+2)}{\lambda_2(L_\vec{M})^2}\frac{1}{m}
$$
where $D = 4k^2e^{4b}$.
\label{thm:full}
\end{theorem}

The proof of Theorem~\ref{thm:full} is provided in Appendix~\ref{sec:proof-full}. The mean squared error upper bound in Theorem~\ref{thm:full} corresponds that in Theorem~\ref{thm:mle} up to a constant factor. If the comparisons sets are unbiased, from (\ref{equ:lambda2L}), we have that $\lambda_2(L_{\vec{M}}) = (1-1/n)k(k-1)$. Hence, the mean squared error upper bound in Theorem~\ref{thm:full} depends on $k$ only through the factor $1/(1-1/k)^2$, which decreases to value $1$ with $k$ in a diminishing returns fashion. This suggests that there is a limited dependence of the mean squared error on the size of comparison sets.  

We now go on to establish a mean-squared upper bound for a class of Thurstone choice models. We will allow for comparison sets of different cardinalities taking values in a set $K$. We will admit the following conditions: 
\begin{itemize}
\item[{\bf A1}] There exists $A > 0$ such that for all $S\subseteq N$ with $|S|\in K$, all $y\in S$, all $i,j\in S$ with $i\neq j$, and all $\theta\in [-b,b]^n$,
$$
\frac{\partial^2 (-\log(p_{y,S}(\theta)))}{\partial \theta_i \partial \theta_j} \leq A\frac{\partial^2 (-\log(p_{y,S}(\vec{0})))}{\partial \theta_i \partial \theta_j} \leq 0
$$
and, moreover, the following holds
$$
\E\left[\frac{\partial^2 (-\log(p_{y,S}(\vec{0})))}{\partial \theta_i \partial \theta_j}  \right]<0.
$$

\item[{\bf A2}] There exists $B > 0$ such that for all $S\subseteq N$ with $|S|\in K$, all $y\in S$, and all $\theta \in [-b,b]^n$,
$$
\|\nabla p_{y,S}(\theta)\|_2 \leq B \| \nabla p_{y,S}({\bm 0})\|_2.
$$
\item[{\bf A3}] There exists $C > 0$ such that for all $S\subseteq N$ with $|S|\in K$, all $y\in S$, and all $\theta \in [-b,b]^n$,
$$
p_{y,S}(\theta) \geq C p_{y,S}({\bm 0}). 
$$
\end{itemize}

Condition {\bf A1} ensures that $\nabla^2(-\log(p_{y,S}(\vec{0})))$ is a Laplacian matrix with non-negative weights, and that $\nabla^2(-\log(p_{y,S}(\vec{\theta})))\succeq A \nabla^2(-\log(p_{y,S}(\vec{0})))$ for all $\theta\in [-b,b]^n$. Condition {\bf A1} also ensures that the expected value of $\nabla^2 (-\log(p_{y,S}(\vec{\theta})))$ is a positive definite matrix where $\nabla^2(-\log(p_{y,S}(\vec{0})))$ is a random matrix when $|S| >2$. Condition {\bf A2} requires that $\|\nabla p_{y,S}(\theta)\|_2$ is bounded for all $\theta \in [-b,b]^n$, while condition {\bf A3} ensures that the choice probabilities are not too much imbalanced. Conditions {\bf A1} and {\bf A2} may be seen as generalizations of conditions {\bf P1} and {\bf P2} for the case of pair comparisons.

Conditions {\bf A1}, {\bf A2} and {\bf A3} can be easily shown to hold for the Luce choice model. For the Luce choice model, we have   
$$
\frac{\partial^2(-\log(p_{y,S}(\theta)))}{\partial \theta_i \partial \theta_j} = -\frac{1}{\beta^2}p_{i,S}(\theta)p_{j,S}(\theta)
$$
hence, {\bf A1} holds with $A = e^{-4b/\beta}$ and $\partial^2(-\log(p_{y,S}(\vec{0})))/\partial \theta_i \partial \theta_j = -1/(|S|\beta)^2$. Conditions {\bf A2} and {\bf A3} hold with $B = 4$ and $C = e^{-2b/\beta}$. 

Note that constants $A$, $B$ and $C$ that appear in {\bf A1}, {\bf A2} and {\bf A3}, respectively, may depend on $F$, the cardinalities of comparison sets, and the parameter $b$, but are independent of any other parameters. In particular, these constants are independent of the number of observations. For any Thurstone choice model, the constants $A$, $B$, and $C$ can be taken to have values arbitrarily near to value $1$ by taking $b$ small enough. 

We next show an upper bound for the mean squared error of the maximum likelihood parameter estimator for a class of Thurstone choice models that satisfy the above stated conditions. Before we do that, we need to introduce some new definitions. 

\begin{definition}[weight function] Let $w^*$ be a function defined on positive integers greater than or equal to $2$, we refer to as \emph{a weight function}, which is defined by
\begin{equation}
w^\star(k) = \left(k\frac{\partial p_k(\vec{0})}{\partial x_1}\right)^2, \hbox{ for } k = 2,3,\ldots
\label{equ:theweight}
\end{equation}
where
\begin{equation}
\frac{\partial p_k(\vec{0})}{\partial x_1} = \int_\reals f(x)^2 F(x)^{k-2}dx.
\label{equ:partp}
\end{equation}
\end{definition}

Notice that for the Luce choice model, $\partial p_k(\vec{0})/\partial x_1 = 1/(\beta k)^2$. Hence, in this case, $w^*(k) = 1/(\beta k)^2$. We will see later in Section~\ref{sec:disc} that for a broad class of Thurstone choice models, which includes well-known cases with noise according to either Gaussian or double-exponential distribution, $\partial p_k(\vec{0})/\partial x_1 = \Theta(1/k^2)$ and, hence, $w^*(k) = \Theta(1/k^2)$.  

\begin{definition}[$\gamma_{F,k}$ parameter] Let $\gamma_{F,k}$ be a parameter defined by
\begin{equation}
\frac{1}{\gamma_{F,k}} = k^3(k-1) \left(\frac{\partial p_k(\vec{0})}{\partial x_1}\right)^2.
\label{equ:gamma}
\end{equation}
\end{definition}

We note that for any comparison set $S$ of cardinality $k$ and all $y\in S$, we have
$$
\frac{1}{\gamma_{F,k}} = \left\|\nabla \log(p_{y,S} ({\bm 0})) \right\|_2^2 = k(k-1)w^\star (k)
$$
which is discussed in more detail in the proof of Lemma~\ref{lem:kgrad}. In particular, for the Luce choice model, we have $\gamma_{F,k} = (1-1/k)/\beta^2$. 

\begin{theorem} Assume {\bf A1}, {\bf A2} and {\bf A3}, let $\sigma_{F,K}$ be such that $1/\gamma_{F,k}\leq \sigma_{F,K}$ for all $k\in K$, and $\lambda_2(L_{\overline{\vec{M}}_{w^\star}}) \ge 32 (\sigma_{F,K}/C)n\log(n)/m$. Then, with probability at least $1-3/n$,
$$
\mse(\widehat{\theta},\theta^\star) \le 
32 D^2 \sigma_{F,K} \frac{n(\log(n)+2)}{\lambda_2(L_{\overline{\vec{M}}_{w^\star}})^2}\frac{1}{m}
$$
where $D = B/(A C)$.
\label{thm:karyub}
\end{theorem}

The proof of the theorem is provided in Appendix~\ref{sec:proof-karyub}. The main technical difference of the proof with respect to that of Theorem~\ref{thm:full} is that $\nabla^2 (-\ell (\theta))$ is a sum of random matrices. Every $\nabla^2 (-\log(p_{y_t, S_t} (\theta)))$ is a random matrix for the following two reasons: (a) $\nabla^2 (-\log(p_{y_t, S_t} (\theta)))$ depends on the randomly chosen $y_t$ and (b) $S_t$ is allowed to be a random set of items. We use the matrix Chernoff bound in the proof of Theorem~\ref{thm:karyub}.

We next show two corollaries of Theorem~\ref{thm:karyub}, which cover two interesting special cases.

\begin{corollary} Suppose that all comparison sets are of identical cardinality of value $k\geq 2$, {\bf A1}, {\bf A2}, {\bf A3} hold, and $\lambda_2(L_{\overline{\vec{M}}_{1/k^2}}) \geq 32(k-1)/(C k)$. Then, with probability at least $1-3/n$,
$$
\mse(\widehat{\theta},\theta^\star) \le 
32 D^2 \left(1-\frac{1}{k}\right)^2\gamma_{F,k}\frac{n(\log(n)+2)}{\lambda_2(L_{\overline{\vec{M}}_{1/k^2}})^2}\frac{1}{m}
$$
where $\gamma_{F,k}$ is given by (\ref{equ:gamma}), and $\vec{M}_{1/k^2}$ is the weighted-adjacency matrix with the weight function $w(k) = 1/k^2$.
\end{corollary}

\begin{corollary} Suppose that comparison sets are independent with each comparison set being a sample without replacement from the set of all items, conditions {\bf A1}, {\bf A2}, {\bf A3} hold, and $m \geq 32(1-1/n)/C n \log(n)$. Then, with probability at least $1-3/n$,
$$
\mse(\widehat{\theta},\theta^\star) \le 
32 D^2 \left(1-\frac{1}{n}\right)^2\gamma_{F,k} \frac{n(\log(n)+2)}{m}.
$$
\label{cor:ksize}
\end{corollary}


\subsection{Lower Bound}

In this section, we present a lower bound for the mean squared error of the maximum likelihood parameter estimator, which establishes minimax optimality of the established upper bounds. We define the following conditions:
\begin{itemize}
\item[{\bf A1'}] There exists $\widetilde{A} > 0$ such that for all $S\subseteq N$ with $|S|\in K$, all $y\in S$, all $i,j\in S$ such that $i\neq j$, and all $\theta\in [-b,b]^n$, it holds
$$
\frac{\partial^2 (-\log(p_{y,S}(\theta)))}{\partial \theta_i \partial \theta_j} \geq \widetilde{A}\frac{\partial^2 (-\log(p_{y,S}(\vec{0})))}{\partial \theta_i \partial \theta_j}.
$$

\item[{\bf A3'}] There exists $\widetilde{C} > 0$ such that for all $S\subseteq N$ with $|S|\in K$, all $y\in S$, and all $\theta \in [-b,b]^n$, it holds
$$
p_{y,S}(\theta) \leq \widetilde{C} p_{y,S}({\bm 0}). 
$$
\end{itemize}

Notice that, in particular, for the special case of $F$ being a double-exponential distribution with parameter $\beta$, we have that {\bf A1'} and {\bf A3'} hold with $\widetilde{A} = e^{4b/\beta}$ and $\widetilde{C} = e^{2b/\beta}$.

\begin{theorem} Under conditions {\bf A1'} and {\bf A3'}, for any unbiased estimator $\widehat \theta$, we have
$$
\E[\mse(\widehat{\theta},\theta^\star)] \ge \frac{1}{\widetilde{A}\widetilde{C}} \left(\sum_{i=2}^n\frac{1}{\lambda_i(L_{\overline{\vec{M}}_F})}\right)\, \frac{1}{m}. 
$$ 
\label{thm:cramer-rao-bound}
\end{theorem}

The following two corollaries follow from the last theorem.

\begin{corollary} If all comparison sets are of cardinality $k\geq 2$, then any unbiased estimator $\widehat \theta$ satisfies
$$
\E[\mse(\widehat{\theta},\theta^\star)] \ge \frac{1}{\widetilde{A}\widetilde{C}}\left(1-\frac{1}{k}\right)\gamma_{F,k} \left(\sum_{i=2}^n\frac{1}{\lambda_i(L_{\overline{\vec{M}}})}\right)\, \frac{1}{m}. 
$$ 
\end{corollary}

\begin{corollary} If, in addition, each comparison set is drawn independently, uniformly at random from the set of all items, then any unbiased estimator $\widehat \theta$ satisfies 
$$
\E[\mse(\widehat{\theta},\theta^\star)] \ge \frac{1}{\widetilde{A}\widetilde{C}} \left(1-\frac{1}{n}\right)^2 \gamma_{F,k} \frac{n}{m}. 
$$
\end{corollary}

The last corollary implies that under the given assumptions, for the mean squared error to be smaller than a constant, it is necessary that the number of observed comparisons is $m =\Omega(\gamma_{F,k} n)$.

{\bf Proof of Theorem~\ref{thm:cramer-rao-bound}} We denote by  $\cov[Y]$ the covariance matrix  of a multivariate random variable $Y$ i.e.,
$$
\cov[Y] = \E[(Y-\E[Y])(Y-\E[Y])^\top].
$$

The proof uses the Cram\'{e}r-Rao inequality, which is stated as follows.

\begin{lemma}[Cram\'{e}r-Rao bound] Let $X$ be a multivariate random variable with distribution $p(x;\theta)$, for parameter $\theta \in \Theta$, and let $\psi: \Theta \rightarrow \reals^r$ be a differentiable function. Then, for any unbiased estimator $\vec{T}(X) = (T_1 (X), \dots, T_r (X) )^\top$ of $\vec{\psi}(\theta) = (\psi_1 (\theta), \dots, \psi_r (\theta ) )^\top$, we have
$$
\cov[\vec{T}(X)] \ge \frac{\partial \boldsymbol{\psi}(\theta)}{\partial \theta} F^{-1}(\theta) \frac{\partial \boldsymbol{\psi}(\theta)}{\partial \theta}^\top
$$
where $\frac{\partial \vec{\psi}(\theta)}{\partial \theta}$ is the Jacobian matrix of $\psi$ and $F(\theta)$ is the Fisher information matrix given by
$$
F(\theta) = \E[\nabla^2(-\log(p(X;\theta)))].
$$
\label{prop:cramer}
\end{lemma}


Let us define $\psi_i(\theta) = \theta_i - \frac{1}{n}\sum_{l=1}^n\theta_l$ for all $i = 1,2,\ldots,n$. Since $\sum_{i=1}^n \theta_i =0$, we have $\sum_{i=1}^n \psi_i (\theta) = 0$. Note that we can write 
\begin{equation}
\frac{\partial \boldsymbol{\psi}(\theta)}{\partial \theta} = \vec{I} - \frac{1}{n}\vec{1}\vec{1}^\top.\label{eq:psi}
\end{equation} 

Let $F(\theta)$ be the Fisher information matrix given by
\begin{equation}
F(\theta) = \sum_{t=1}^m \E\left[\nabla^2(-\log(p_{y_t,S_t}(\theta))) \right] .\label{eq:fisher-p1}
\end{equation}

By conditions {\bf A1'} and {\bf A3'}, and Lemma~\ref{lem:explog}, we have
\begin{align}
\E\left[\nabla^2(-\log(p_{y_t,S_t}(\theta)))\right]
& =\sum_{y \in S_t} p_{y,S_t}(\theta) \nabla^2 (-\log(p_{y,S_t}(\theta))) \cr
& \preceq \sum_{y \in S_t} \frac{\widetilde{C}}{|S_t|} \nabla^2 (-\log(p_{y,S_t}(\theta)))\cr
& \preceq \sum_{y \in S_t} \frac{\widetilde{A}\widetilde{C}}{|S_t|} \nabla^2 (-\log(p_{y,S_t}({\bm 0})))\cr
& = \widetilde{A}\widetilde{C} \left(|S_t|\frac{\partial p_{|S_t|}(\vec{0})}{\partial x_1}\right)^2L_{\vec{M}_{S_t}}
\label{eq:hessian-s1}
\end{align}
where $\vec{M}_S$ is a matrix in $\reals^{n\times n}$ that has each element $(i,j)$ such $\{i,j\}\subseteq S$ equal to $1$ and all other elements equal to $0$.

From \eqref{eq:fisher-p1} and \eqref{eq:hessian-s1},
\begin{equation}
F(\theta) \preceq  \widetilde{A}\widetilde{C} \frac{m}{n} L_{\overline{\vec{M}}_F}. 
\label{eq:fisher-up}
\end{equation}

Note that
$$
\E[\|\widehat{\theta}-\theta\|_2^2] = \tr(\cov[\vec{T}(X)])
= \sum_{i=1}^n \lambda_i(\cov[\vec{T}(X)]).
$$

By the Cram\'{e}r-Rao bound and (\ref{eq:fisher-up}), we have 
\begin{eqnarray*}
\frac{1}{n}\E[\|\widehat{\theta}-\theta\|_2^2]  
&\geq& \frac{1}{n}\sum_{i=1}^n \lambda_i \left(\frac{\partial \boldsymbol{\psi}(\theta)}{\partial \theta} F^{-1}(\theta) \frac{\partial \boldsymbol{\psi}(\theta)}{\partial \theta}^\top \right)\\
&=& \frac{1}{n}\sum_{i=1}^{n-1}  \lambda_i(\vec{U}^\top F^{-1}(\vec{0}) \vec{U})\\
&=& \frac{1}{n}\sum_{i=1}^{n-1}  \frac{1}{\lambda_i(\vec{U}^\top F(\vec{0}) \vec{U})}\\
& \geq &\frac{1}{\widetilde{A}\widetilde{C}} \sum_{i=1}^{n-1}  \frac{1}{\lambda_i(\vec{U}^\top L_{\overline{\vec{M}}_F} \vec{U})}\\
& = &\frac{1}{\widetilde{A}\widetilde{C} m} \sum_{i=2}^{n}  \frac{1}{\lambda_i(L_{\overline{\vec{M}}_F})}.
\end{eqnarray*}

