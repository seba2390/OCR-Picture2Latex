\appendix

\section{Background Material}
\label{sec:back}

\paragraph{Location of Eigenvalues} We make note of the well-known Ger\v sgorin circles theorem, which we state as at the following lemma:

\begin{lemma} Let $\vec{A}\in \reals^{n\times n}$, then all eigenvalues of $\vec{A}$ are located in the union of $n$ discs
$$
\cup_{i=1}^n \left\{z\in C: |z-a_{ii}| \leq \sum_{j\neq i} |a_{i,j}|\right\}.
$$
\label{lem:gersgorin}
\end{lemma}

\paragraph{Properties of Positive Definite and Laplacian Matrices} A symmetric matrix $\vec{A}\in \reals^{n\times n}$ is said to be \emph{positive semidefinite} if $
\vec{x}^\top \vec{A} \vec{x} \geq 0$ for all nonzero $\vec{x}\in \reals^n$. If the inequality is replaced with strict inequality, $\vec{A}$ is said to be \emph{positive definite}. 

Each eigenvalue of a positive definite matrix is a positive real number. Each eigenvalue of a positive semidefinite matrix is a nonnegative real number. 

For two matrices $\vec{A},\vec{B}\in \reals^{n\times n}$, we write $\vec{A} \succeq \vec{B}$ for the \emph{positive semidefinite ordering}, which means that $\vec{A} - \vec{B}$ is a positive semidefinite matrix. Similarly, we write $\vec{A} \succ \vec{B}$ for the \emph{positive definite ordering}, which means that $\vec{A}-\vec{B}$ is a positive definite matrix. Note that $\vec{A} \succeq \vec{0}$ means that $\vec{A}$ is a positive semidefinite matrix, and $\vec{A}\succ \vec{0}$ means that $\vec{A}$ is a positive definite matrix.

We note the following ordering relations for eigenvalues of two positive definite matrices that satisfy the positive semidefinite ordering (e.g., see Corollary~7.7.4~\cite{horn}).

\begin{lemma} For any two positive definite matrices $\vec{A},\vec{B}\in \reals^{n\times n}$ such that $\vec{A}\succeq \vec{B}$, $\lambda_i(\vec{A})\geq \lambda_i(\vec{B})$ for all $i = 1,2,\ldots,n$.
\label{lem:EigAsuccB}
\end{lemma}

For any $\vec{A}\in \reals^{n\times n}$ with zero diagonal such that $\vec{A}\geq \vec{0}$, the Laplacian matrix $L_{\vec{A}}$ is positive semidefinite. This follows from the localization of eigenvalues by the Ger\v sgorin circles theorem. 

\begin{lemma} If $\vec{A}\in \reals^n$ is a symmetric matrix with zero diagonal, then
\begin{enumerate}
\item [(i)] $\lambda_1(L_{\vec{A}}) = 0$, and
\item [(ii)] $\lambda_i(L_{\vec{A}}) = \lambda_{i-1}(\vec{U}^\top L_{\vec{A}} \vec{U})$, for $i\in \{2,3,\ldots,n\}$
\end{enumerate}
where $\vec{U}\in \vec{R}^{n\times n-1}$ is any matrix such that $\vec{U}^\top \vec{U} = \vec{I}$.
\label{lem:Leig}
\end{lemma}

We shall use the following property of real symmetric matrices:

\begin{lemma} If $\vec{A},\vec{B}\in \reals^{n\times n}$ are real symmetric matrices with zero diagonals such that $\vec{B} \geq \vec{A}$ where the inequality holds element-wise, then $L_{\vec{B}} \succeq L_{\vec{A}}$. 
\label{lem:laplaceposdef}
\end{lemma}

\paragraph{Chernoff Tail Bounds} The following bounds follow from the Chernoff bound:

\begin{lemma} Suppose that $X$ is a sum of $m$ independent Bernoulli random variables each with mean $p$, then if $q \leq p \leq 2q$,
\begin{equation}
\Pr[X \leq q m] \leq \exp\left(-\frac{(q-p)^2}{4q} m\right)
\label{equ:che1}
\end{equation}
and, if $p \leq q$, 
\begin{equation}
\Pr[X \geq q m] \leq \exp\left(-\frac{(q-p)^2}{4q} m\right).
\label{equ:che2}
\end{equation}
\label{lem:chernoff}
\end{lemma}

\begin{proof} We prove only prove (\ref{equ:che2}) as (\ref{equ:che1}) follows by similar arguments. By the Chernoff's bound, for every $s > 0$,
\begin{eqnarray*}
\Pr[X \geq q m] & \leq & e^{-s qm}\E[e^{sX}]\\
&=& e^{-sqm} (1-p+pe^s)^m\\
&=& e^{-m h(s)}
\end{eqnarray*}
where $h(s) = qs - \log(1-p+pe^s)$.
Since $\log(1-x) \leq -x$ for all $x\in \reals$, we have $h(s) \geq qs +p - p e^s$. Take $s = s^* := \log(q/p)$ to obtain $h(s^*) \geq q \log(q/p) + p - q$.

Now, let $\epsilon = q-p$, and note that $q \log(q/p) + p - q := g(\epsilon)$ where $g(\epsilon) = q \log(q/(q-\epsilon) - \epsilon$. Since $g'(\epsilon) = q/(q-\epsilon) - 1 
= \epsilon/(q-\epsilon) \geq \epsilon/(2q)$, we have $g(\epsilon) = \int_0^\epsilon g'(x)dx \geq \epsilon^2/(4q)$.

Hence, it follows that $h(s^*) \geq (p-q)^2/(4q)$, and, thus
$$
\Pr[X\geq q m] \leq \exp\left(-\frac{1}{4q}(p-q)^2\right).
$$
\end{proof}

\section{Proof of Lemma~\ref{lem:mle-taylor}}

Let $\Delta = \widehat{\vec{x}} - \vec{x}^\star$. By the Taylor expansion, we have
\begin{equation}
g(\widehat{\vec{x}}) \ge g(\vec{x}^\star) + \nabla g (\vec{x}^\star)^\top \Delta  +\frac{1}{2} \min_{\alpha \in [0,1]} \Delta^\top \nabla^2 g (\vec{x}^\star+\alpha \Delta) \Delta.
\label{eq:taylor-pair}
\end{equation}

Since $g(\widehat{\vec{x}}) \leq g(\vec{x}^\star)$, we have
$$
\min_{\alpha \in [0,1]} \Delta^\top \nabla^2 g (\vec{x}^\star+\alpha \Delta) \Delta \leq -\nabla g (\vec{x}^\star)^\top \Delta.
$$
Hence, 
\begin{equation}
\min_{\vec{x} \in \calX} \Delta^\top \nabla^2 g(\vec{x}) \Delta \le 2  \left\|\nabla g(\vec{x}^\star)\right\|_2 \| \Delta \|_2.
\label{eq:taylor-2}
\end{equation}

Fix an arbitrary $\vec{x}\in \calX$. From condition (i) it follows that $\nabla^2 g(\vec{x})$ has eigenvalue $0$ with eigenvector $\vec{1}$. Combining with condition (ii), we have

\begin{equation}
0=\lambda_1(\nabla^2 g(\vec{x})) < \lambda_2(\nabla^2 g(\vec{x})) \le \dots\le \lambda_n(\nabla^2 g(\vec{x})).
\label{equ:lam}
\end{equation}

Let $\vec{U} = [\vec{u}_1,\vec{u}_2,\ldots,\vec{u}_n] \in \reals^{n\times n}$ where $\vec{u}_1,\vec{u}_2,\ldots,\vec{u}_n$ are ortonormal eigenvectors of $\nabla^2 g(\vec{x})$, which correspond to eigenvalues $\lambda_1(\nabla^2 g(\vec{x})), \lambda_2(\nabla^2 g(\vec{x})), \ldots, \lambda_n(\nabla^2 g(\vec{x}))$, respectively. Note that 
\begin{equation}
\vec{u}_1^\top \Delta = 0. 
\label{equ:ud}
\end{equation}

Let $\Lambda = \mathrm{diag}(\lambda_1(\nabla^2 g(\vec{x})), \lambda_2(\nabla^2 g(\vec{x})), \ldots, \lambda_n(\nabla^2 g(\vec{x})))$.

We have the following relations:
\begin{eqnarray*}
\Delta^\top \nabla^2 g(\vec{x})\Delta & = & \Delta^\top \vec{U}\Lambda\vec{U}^\top\Delta\\
&=& \sum_{i=1}^n \lambda_i(\nabla^2 g(\vec{x}))|(\vec{U}^\top\Delta)_i|^2\\
&=& \sum_{i=1}^n \lambda_i(\nabla^2 g(\vec{x}))|\vec{u}_i^\top\Delta|^2\\
&=& \sum_{i=2}^n \lambda_i(\nabla^2 g(\vec{x}))|\vec{u}_i^\top\Delta|^2\\
&\geq & \lambda_2(\nabla^2 g(\vec{x})) \sum_{i=2}^n |\vec{u}_i^\top\Delta|^2\\
&= & \lambda_2(\nabla^2 g(\vec{x})) \sum_{i=1}^n |\vec{u}_i^\top\Delta|^2\\
&= & \lambda_2(\nabla^2 g(\vec{x})) ||\Delta||_2^2
\end{eqnarray*}
where we use the properties in (\ref{equ:lam}) and (\ref{equ:ud}).

Hence, it follows that
\begin{equation}
\min_{\vec{x}\in \calX}\Delta^\top \nabla^2 g(\vec{x})\Delta \geq ||\Delta||_2^2 \min_{\vec{x}\in \calX} \lambda_2(\nabla^2 g(\vec{x})).
\label{eq:delta-hessian}
\end{equation}

By combining \eqref{eq:delta-hessian} and \eqref{eq:taylor-2}, we conclude the proof of the lemma.


%\section{Proof of Theorem~\ref{thm:mle}}

%\subsection{Proof of Lemma~\ref{lem:LT21}}

%\subsection{Proof of Lemma~\ref{lem:LT22}}

\section{Proof of Theorem~\ref{thm:full}} \label{sec:proof-full}

The log-likelihood function in (\ref{equ:loglik0}) can be written in the following more explicit form:
\begin{equation}
\ell(\theta) = \sum_{t=1}^m \log\left(\frac{e^{\theta_{y_t}}}{\sum_{v\in S_t}e^{\theta_v}}\right).
\label{equ:llikluce}
\end{equation}
Since $\nabla^2(-\ell(\theta))\vec{1} = \vec{0}$, for all $\theta\in \reals^n$, under assumption that $\min_{\theta\in \Theta}\lambda_2 \left(-\nabla^2 \ell (\theta) \right)>0$, the upper bound in Lemma~\ref{lem:mle-taylor} holds. This combined with the following two lemmas yields the statement of the theorem. 

%\subsection{Proof of Lemma~\ref{lem:L11}}

\begin{lemma} The following lower bound holds:
\begin{equation}
\min_{\theta \in \Theta}\lambda_2 \left(\nabla^2 (-\ell (\theta)) \right) \ge \frac{1}{k^2 e^{4b}}\frac{m}{n} \lambda_2 (L_\vec{M}). 
\label{eq:w-l}
\end{equation}
\label{lem:L11}
\end{lemma}

\begin{proof}
From (\ref{equ:llikluce}), we have for $i,j\in \{1,2,\ldots,n\}$ such that $j\neq i$,
$$
\frac{\partial^2 \ell (\theta)}{\partial \theta_i \partial \theta_j} = \sum_{t:\{ i,j\}\subseteq S_t} p_{i,S_t}(\theta)p_{j,S_t}(\theta) 
\hbox{ and }
 \frac{\partial^2 \ell(\theta)}{\partial \theta_i^2} = -\sum_{j\neq i} \frac{\partial^2 \ell (\theta)}{\partial \theta_i \partial \theta_j}.
$$

Since for all $i\neq j$,
$$
\frac{\partial^2 \ell (\theta)}{\partial \theta_i \partial \theta_j} \ge \frac{1}{k^2 e^{4b}}m_{i,j}
$$
we have that $\nabla^2(-\ell (\theta)) -  \frac{1}{k^2 e^{4b}}\frac{m}{n} L_\vec{M}$ is a Laplacian matrix of a matrix with nonnegative elements. Every such Laplacian matrix is positive semidefinite, hence 
\begin{equation}
\nabla^2 (-\ell (\theta)) \succeq \frac{1}{k^2 e^{4b}}\frac{m}{n} L_\vec{M}.
\label{eq:pdm-ell}
\end{equation}

From \eqref{eq:pdm-ell}, we conclude \eqref{eq:w-l}.
\end{proof}

%\subsection{Proof of Lemma~\ref{lem:L12}}

\begin{lemma} With probability at least $1-2/n$,
\begin{equation}
\left\| \nabla (-\ell (\theta^\star)) \right\|_2 \le  2\sqrt{m (\log (n)+2)}. 
\label{eq:w-u}
\end{equation}
\label{lem:L12}
\end{lemma}

\begin{proof}
From (\ref{equ:llikluce}), we have
\begin{equation}
\nabla(-\ell(\theta)) = \sum_{t=1}^m \nabla(-\log(p_{y_t,S_t}(\theta)))
\label{equ:nll}
\end{equation}
where $p_{y_t,S_t}(\theta) = e^{\theta^\star_{y_t}}/\sum_{v\in S_t}e^{\theta_v}$. 

It is straightforward to derive that $\nabla(-\log(p_{y_t,S_t}(\theta)))$ has elements given by
\begin{equation}
\frac{\partial (-\log(p_{y_t,S_t}(\theta)))}{\partial \theta_i}  = \left \{
\begin{array}{ll}
-(1-p_{i,S_t}(\theta)), & \hbox{ if } i = y_t\\
p_{i,S_t}(\theta), & \hbox{ if } i\in S_t\setminus \{y_t\}\\
0, & \hbox{ otherwise}.
\end{array}
\right .
\label{equ:nllik}
\end{equation}

From (\ref{equ:nll}), $\nabla \ell(\theta)$ is a sum of independent random vectors in $\reals^n$ that satisfy for all $t\in \{1,2,\ldots,m\}$,
\begin{equation}
\E\left[\nabla (-\log(p_{y_t,S_t}(\theta^\star)))\right] = \vec{0} 
\label{eq:w-az1}
\end{equation}
and
\begin{equation}
\left\|\nabla(-\log(p_{y_t,S_t}(\theta^\star)))\right\|_2 \leq \sqrt{2}.
\label{eq:w-az2}
\end{equation}

The last two relations are easy to establish using (\ref{equ:nllik}) as follows. Equation~(\ref{eq:w-az1}) holds because for every $i\in N$,
\begin{eqnarray*}
\E\left[\frac{\partial (-\log(p_{y_t,S_t}(\theta^\star)))}{\partial \theta_i}\right]
% &=& \Pr[y_t = i] \frac{\partial}{\partial \theta_i} \log\left(p_{i,S_t}(\theta^\star)\right) + \sum_{j\in S_t\setminus \{y_t\}}\Pr[y_t=j] \frac{\partial}{\partial \theta_i} \log\left(p_{j,S_t}(\theta^\star)\right)\\
&=& -p_{i,S_t}(\theta^\star)(1-p_{i,S_t}(\theta^\star)) + (1-p_{i,S_t}(\theta^\star))p_{i,S_t}(\theta^\star) = 0.
\end{eqnarray*}

Equation~(\ref{eq:w-az1}) follows from
$$
\left\|\nabla (-\log(p_{y_t,S_t}(\theta^\star)))\right\|_2^2 = (1-p_{y_t,S_t}(\theta^\star))^2 + \sum_{j\in S_t\setminus \{y_t\}} p_{j,S_t}(\theta^\star)^2 \leq 2.
$$

By the vector Azuma-Hoeffding bound in Lemma~\ref{prop:azuma-hoeffding}, we have
$$
\Pr[\|\nabla(-\ell(\theta^\star))\|_2\geq 2\sqrt{m(\log(n)+2)}]  \leq \frac{2}{n}.
$$
which completes the proof of \eqref{eq:w-u}.
\end{proof}

\section{Proof of Theorem~\ref{thm:karyub}} \label{sec:proof-karyub}

Since $\nabla^2 (-\ell (\theta))$ is a Laplacian matrix, by condition {\bf A1}, 
$$
\nabla^2(-\ell (\theta)) \succeq A\nabla^2(-\ell ({\bm 0})) \hbox{ for all } \theta\in [-b,b]^n.
$$
Hence, in particular, 
\begin{equation}
\min_{\theta \in [-b,b]^n}\lambda_2(-\ell (\theta)) \geq A\lambda_2(\nabla^2(-\ell ({\bm 0}))).
\label{eq:semi}
\end{equation}

We have the following two lemmas.

%\subsection{Proof of Lemma~\ref{lem:khessi}}

\begin{lemma} If $\lambda_2(L_{\overline{\vec{M}}_{w^\star}}) \ge 32 (\sigma_{F,K}/C) (n\log(n))/m$, then with probability at least $1-1/n$,
$$
\lambda_2(\nabla^2(-\ell ({\bm 0}))) \ge \frac{1}{2}C \frac{m}{n} \lambda_2(L_{\overline{\vec{M}}_{w^\star}}).
$$
\label{lem:khessi}
\end{lemma}

\begin{proof}
$\nabla^2(-\ell({\bm 0}))$ is a sum of independent random matrices given by
$$
\nabla^2(-\ell({\bm 0})) = \sum_{t=1}^m \nabla^2 (-\log (p_{y_t,S_t}({\bm 0}))).
$$

We have the following relations:
\begin{eqnarray}
\E\left[\nabla^2 (-\ell({\bm 0}))\right] 
&=& \sum_{t=1}^m \E\left[\nabla^2(-\log (p_{y_t,S_t}({\bm 0})))\right]  \cr
&=& \sum_{t=1}^m \sum_{y\in S_t} p_{y,S_t}(\theta^\star)\nabla^2(-\log (p_{y,S_t}({\bm 0})))\cr
&\succeq & C \sum_{t=1}^m \sum_{y\in S_t} \frac{1}{|S_t|} \nabla^2(-\log (p_{y,S_t}({\bm 0})))\cr
& = & C \sum_{t=1}^m \sum_{y\in S_t} \frac{1}{|S_t|} L_{\vec{M}_{S_t}} \left(|S_t|\frac{\partial p_{|S_t|}(\vec{0})}{\partial x_1}\right)^2\nonumber 
\label{equ:temp}\\
&=& C \frac{m}{n}  L_{\overline{\vec{M}}_F}\nonumber
\label{eq:E-hessian}
\end{eqnarray}
where we use Lemma~\ref{lem:explog}.

Hence,
\begin{equation}
\lambda_2(\E[\nabla^2 (-\ell ({\bm 0}))]) \ge C \frac{m}{n} \lambda_2(L_{\overline{\vec{M}}_F}).
\label{eq:cher1}
\end{equation} 

By Lemma~\ref{lem:hessian-p2}, for all $t\in \{1,2,\ldots,m\}$,
\begin{equation}
\left\| \nabla^2 \log(p_{y_t,S_t} (\vec{0})) \right\|_2 \le \frac{2}{\gamma_{F, |S_t|}} \leq 2 \sigma_{F,K}.
\label{eq:cher2}
\end{equation}

Using the matrix Chernoff bound with $\epsilon = 1/2$, (\ref{eq:cher1}) and (\ref{eq:cher2}, we conclude the statement of the lemma.
\end{proof}

%\subsection{Proof of Lemma~\ref{lem:kgrad}} \label{sec:proof-lem12}

\begin{lemma} With probability at least $1-2/n$,
$$
\|\nabla \ell (\theta^\star) \|_2 \le  B\sqrt{\sigma_{F,K}}\sqrt{2m(\log(n) +2)}.
$$
\label{lem:kgrad}
\end{lemma}

\begin{proof} For every $S\subseteq N$ and $i,j\in S$ such that $j\neq i$,
\begin{equation}
\frac{\partial \log(p_{j,S} (\theta))}{\partial \theta_i} = \frac{1}{p_{j,S} (\theta)}\frac{\partial p_{j,S} (\theta)}{\partial \theta_i}.
\label{eq:id2}
\end{equation}
and
\begin{equation}
\frac{\partial \log(p_{i,S} (\theta))}{\partial \theta_i} = -\frac{1}{p_{i,S} (\theta)} \sum_{v\in S\setminus \{i\}}\frac{\partial p_{v,S} (\theta)}{\partial \theta_i}.
\label{eq:id1}
\end{equation}

From \eqref{eq:id2} and \eqref{eq:id1}, for every $t\in \{1,2,\ldots,m\}$, 
\begin{equation}
\E\left[\nabla \log p_{y_t,S_t} (\theta^\star) \right]  = {\bm 0}.
\label{eq:aver-grad}
\end{equation}

From \eqref{eq:id2} and \eqref{eq:id1}, for every $S\subseteq N$ such that $|S|=k\geq 2$ and $i,j\in S$ such that $i\neq j$, 
$$
\frac{\partial \log(p_{j,S}(\vec{0}))}{\partial \theta_i} = k \frac{\partial p_k(\vec{0})}{\partial x_1}
$$
$$
\frac{\partial \log(p_{i,S}(\vec{0}))}{\partial \theta_i} = k(k-1)\frac{\partial p_k(\vec{0})}{\partial x_1}.
$$

Hence, for every $t\in \{1,2,\ldots,m\}$ such that $|S_t|=k$
\begin{equation}
\left\|\nabla \log(p_{y_t,S_t} ({\bm 0})) \right\|_2^2  = k^3(k-1)\left(\frac{\partial p_k({\bm 0})}{\partial x_1}\right)^2 = \frac{1}{\gamma_{F,k}}.
\label{equ:l2bound}
\end{equation}

By condition {\bf A3} and (\ref{equ:l2bound}), for every $t\in \{1,2,\ldots,m\}$,
\begin{equation}
\|\nabla \log p_{y_t,S_t} (\theta^\star)\|_2^2 
\leq B^2 \|\nabla \log p_{y_t,S_t} ({\bm 0})\|_2^2 
\leq B^2 \sigma_{F,K}.
\label{eq:aver-grad2}
\end{equation}

Using (\ref{eq:aver-grad}) and (\ref{eq:aver-grad2}) and the vector Azuma-Hoeffding bound in Lemma~\ref{prop:azuma-hoeffding}, with probability at least $1-2/n$,
$$
\|\nabla \ell (\theta^\star) \|_2 \le B\sqrt{\sigma_{F,K}}\sqrt{2m(\log(n) +2)}.
$$
\end{proof}

The negative log-likelihood function satisfies the bound in Lemma~\ref{lem:mle-taylor}. This combined with relation (\ref{eq:semi}), Lemma~\ref{lem:khessi} and Lemma~\ref{lem:kgrad} implies that if $\lambda_2(L_{\overline{\vec{M}}_{w^\star}}) \geq 32 (\sigma_{F,K}/C)n\log(n)/m$, then with probability at least $1-3/n$, 
$$
\frac{1}{n}\|\widehat{\theta}-\theta^\star\|_2^2 \leq 32 \left(\frac{B}{A C}\right)^2 \sigma_{F,K} \frac{n(\log(n)+2)}{\lambda_2(L_{\overline{\vec{M}}_{w^\star}})^2}\frac{1}{m}
$$
which proves the theorem.


\section{Proof of Theorem~\ref{thm:break-1}} \label{sec:proof-break-1}

Under condition that $\lambda_2\left(\nabla^2 (-\ell_{k-1}(\theta) \right)>0$, the negative pseudo log-likelihood function satisfies the bound in Lemma~\ref{lem:mle-taylor}. This, combined with the following two lemmas implies the statement of the theorem.

%\subsection{Proof of Lemma~\ref{lem:break1lambda_2}}

\begin{lemma} If $\lambda_2(L_{\vec{M}}) \geq 8 k(k-1)e^{2b} n\log(n)/m$, then with probability at least $1-1/n$,
$$
\min_{\theta \in \Theta}\lambda_2(\nabla^2(-\ell_{k-1}(\theta))) \geq \frac{1}{4ke^{2b}} \frac{m}{n}\lambda_2(L_{\vec{M}}). 
$$
\label{lem:break1lambda_2}
\end{lemma}

\begin{proof} We will establish the lemma by using the matrix Chernoff bound in Lemma~\ref{cor:matrix} as follows. Note that $\nabla^2(-\ell_{k-1}(\theta))$ is a sum independent random matrices given by: 
$$
\nabla^2(-\ell_{k-1}(\theta)) = \sum_{t=1}^m \nabla^2 \left(\sum_{v\in S_t\setminus \{y_t\}}-\log(p_{y_t,v}(\theta))\right). 
$$ 
The nonzero elements of $\nabla^2 \left(\sum_{v\in S_t\setminus \{y_t\}}-\log(p_{y_t,v}(\theta))\right)$ are, for every $i,j \in S_t$ such that $i\neq j$,
\begin{equation}
\frac{\partial^2\left(\sum_{v\in S_t\setminus \{y_t\}} -\log(p_{y_t,v}(\theta))\right)}{\partial \theta_i \partial \theta_j}
= \left\{
\begin{array}{ll}
-p_{i,j}(\theta)(1-p_{i,j}(\theta)), & \hbox{ if } {\{i,j\}\subseteq S_t, y_t \in \{i,j\}}\\ 
0, & \hbox{ otherwise}
\end{array}
\right .
\label{equ:part2}
\end{equation}
and
$$
\frac{\partial^2\left(\sum_{v\in S_t\setminus \{y_t\}} -\log(p_{y_t,v}(\theta))\right)}{\partial \theta_i^2} = -\sum_{j\in S_t\setminus \{i\}} \frac{\partial^2\left(\sum_{v\in S_t\setminus \{y_t\}} -\log(p_{y_t,v}(\theta))\right)}{\partial \theta_i \partial \theta_j}.
$$

From (\ref{equ:part2}), we have
\begin{eqnarray}
\E\left[ \frac{\partial^2 \left(\sum_{v\in S_t\setminus \{y_t\}}-\log(p_{y_t,v}(\theta))\right)}{\partial \theta_i \partial \theta_j}\right] 
&=& -p_{i,j}(\theta)(1-p_{i,j}(\theta))(p_{i,S_t}(\theta) + p_{j,S_t}(\theta))\nonumber\\
&=& -\frac{e^{\theta_i}+e^{\theta_j}}{\sum_{v\in S_t}e^{\theta_v}}\frac{e^{\theta_i}e^{\theta_j}}{(e^{\theta_i}+e^{\theta_j})^2}\nonumber\\
&=& -\frac{1}{\sum_{v\in S_t}e^{\theta_v}}\frac{1}{e^{-\theta_i}+e^{-\theta_j}}\nonumber\\
&\leq & -\frac{1}{2ke^{2b}}.\nonumber 
\label{eq:k-ex} 
\end{eqnarray}

Hence, we have
$$
\E\left[ \nabla^2(-\ell_{k-1}(\theta))\right] \succeq \frac{1}{2ke^{2b}}\frac{m}{n} L_\vec{M}. 
$$
and, in particular,
\begin{equation}
\lambda_2(\E\left[ \nabla^2(-\ell_{k-1}(\theta))\right]) \geq \frac{1}{2ke^{2b}}\frac{m}{n} \lambda_2(L_\vec{M}). 
\label{eq:k-ex2}
\end{equation}

To apply the matrix Chernoff bound in Lemma~\ref{cor:matrix}, we use the following identities that follow by Lemma~\ref{lem:Leig},
\begin{equation}
\lambda_2(\nabla^2((-\ell_{k-1}(\theta)))) = \lambda_1(\vec{U}^\top\nabla^2((-\ell_{k-1}(\theta))))\vec{U})
\label{equ:lambdas1}
\end{equation}
and
\begin{equation}
\lambda_2(\nabla^2(\E[(-\ell_{k-1}(\theta))])) = \lambda_1(\vec{U}^\top\nabla^2(\E[(-\ell_{k-1}(\theta))]))\vec{U})
\label{equ:lambdas2}
\end{equation}
and the following fact:
\begin{eqnarray}
&& \left\|\vec{U}^\top\nabla^2\left(\sum_{v\in S_t\setminus \{y_t\}}-\log(p_{y_t,v}(\theta))\right) \vec{U}\right\|_2 \\
& \leq & \left\|\vec{U}^\top\right\|_2 \left\|\nabla^2\left(\sum_{v\in S_t\setminus \{y_t\}}-\log(p_{y_t,v}(\theta))\right)\right\|_2\left\|\vec{U}\right\|_2\nonumber\\
&=& \left\|\nabla^2\left(\sum_{v\in S_t\setminus \{y_t\}}-\log(p_{y_t,v}(\theta))\right)\right\|_2\nonumber\\
&\leq & \max_{i}\left| \frac{\partial^2 }{\partial \theta_i^2} \left(\sum_{v\in S_t\setminus \{y_t\}}\log(p_{y_t,v}(\theta))\right) \right|+ \sum_{j\neq i} \left| \frac{\partial^2 }{\partial \theta_i \partial \theta_j} \left(\sum_{v\in S_t\setminus \{y_t\}}\log(p_{y_t,v}(\theta))\right) \right| \nonumber\\
&=& 2 \max_{i} \sum_{j\neq i}  p_{i,j}(\theta)(1-p_{i,j}(\theta))1_{\{i,j\}\subseteq S_t, y_t \in \{i,j\}} \\
&\le&  \frac{1}{2}(k-1).
\label{eq:k-l2}
 \end{eqnarray} 
where the first equation follows by $\|\vec{U}^\top\|_2 = \|\vec{U}\|_2 = 1$, and the second inequality is by the Ger\v sgorin circles theorem (Lemma~\ref{lem:gersgorin}).
\begin{eqnarray*}
&& \Pr[\lambda_2(\nabla^2 (-\ell_{k-1}(\theta)))\leq \frac{1}{4ke^{2b}}\frac{n}{m}\lambda_2(L_{\vec{M}})] \\
&\leq & \Pr[\lambda_2(\nabla^2 (-\ell_{k-1}(\theta)))\leq \frac{1}{2}\lambda_2(\nabla^2 \E[(-\ell_{k-1}(\theta))])]\\
&=& \Pr[\lambda_1(\vec{U}^\top\nabla^2 (-\ell_{k-1}(\theta))\vec{U})\leq \frac{1}{2}\lambda_1(\vec{U}^\top \nabla^2 \E[(-\ell_{k-1}(\theta))]\vec{U})]\\
&\leq & n e^{-\frac{\lambda_1(\vec{U}^\top \nabla^2 \E[(-\ell_{k-1}(\theta))]\vec{U})}{4\frac{k-1}{2}}}\\
& \leq & n e^{-\frac{\lambda_2(L_{\vec{M}})m}{4 k(k-1)e^{2b}n}}\\
& \leq & \frac{1}{n} 
\end{eqnarray*}
where the first inequality is by (\ref{eq:k-ex2}), the equality is by (\ref{equ:lambdas1}) and (\ref{equ:lambdas2}), the second inequality is by the matrix Chernoff bound in Lemma~\ref{cor:matrix} and (\ref{eq:k-l2}), the third inequality is by (\ref{eq:k-ex2}), and the last inequality is by the condition $\lambda_2(L_{\vec{M}}) \geq 8k(k-1)e^{2b} n\log(n)/m$.
\end{proof}

%\subsection{Proof of Lemma~\ref{lem:break1nabla}}

\begin{lemma} With probability at least $1-2/n$,
\begin{equation}
\left\| \nabla(-\ell_{k-1}(\theta^\star))\right\|_2  \le  2\sqrt{k(k-1) m (\log (n)+2)}.
\label{eq:s1-grad-u}
\end{equation}
\label{lem:break1nabla}
\end{lemma}

\begin{proof} $\nabla(-\ell_{k-1}(\theta))$ is a sum of independent random vectors given by:
$$
\nabla(-\ell_{k-1}(\theta)) = \sum_{t=1}^m \nabla\left(\sum_{v\in S_t\setminus \{y_t\}} -\log(p_{y_t,v}(\theta))\right).
$$ 

It is straightforward to show that $\nabla\left(\sum_{v\in S_t\setminus \{y_t\}} -\log(p_{y_t,v}(\theta))\right)$ has the elements given by
\begin{equation}
\frac{\partial \left(\sum_{v\in S_t\setminus \{y_t\}} -\log(p_{y_t,v}(\theta))\right)}{\partial \theta_i} = \left\{
\begin{array}{ll}
-\sum_{v\in S_t\setminus \{y_t\}}p_{v,y_t}(\theta), & \hbox{ if } i=y_t\\
p_{i,y_t}(\theta), & \hbox{ if } i\in S_t\setminus \{y_t\}\\
0, & \hbox{ otherwise }
\end{array}
\right .
\label{equ:partlik1}
\end{equation}

For every $t\in \{1,2,\ldots,m\}$ and $i \in \{1,2,\ldots,n\}$, we have
$$
\E\left[\frac{\partial \left(\sum_{v\in S_t\setminus \{y_t\}}-\log(p_{y_t,v}(\theta^\star))\right)}{\partial \theta_i}\right] = 0.
$$
The last equation obviously holds for every $i\notin S_t$, and it holds for $i\in S_t$ by the following derivations 
\begin{eqnarray*}
\E\left[\frac{\partial}{\partial \theta_i}\left(\sum_{v\in S_t\setminus \{y_t\}}-\log(p_{y_t,v}(\theta^\star))\right)\right]
 & = & -p_{i,S_t}(\theta^\star)\sum_{v\in S_t \setminus\{ i\}} p_{v,i}(\theta^\star) 
+ \sum_{v\in S_t \setminus\{ i\}} p_{v,S_t}(\theta^\star)p_{i,v}(\theta^\star)\\
& = & \sum_{v\in S_t \setminus\{ i\}} (-p_{i,S_t}(\theta^\star) p_{v,i}(\theta^\star) 
+ p_{v,S_t}(\theta^\star)p_{i,v}(\theta^\star))=0.
\end{eqnarray*}

From (\ref{equ:partlik1}), we have
\begin{eqnarray}
\left\|\nabla\left(\sum_{v\in S_t\setminus \{y_t\}}-\log(p_{y_t,v}(\theta))\right)\right\|_2^2
& = & \left(\sum_{v\in S_t\setminus \{y_t\}} p_{v,y_t}(\theta)\right)^2 + \sum_{v\in S_t\setminus \{y_t\}} p_{v,y_t}(\theta)^2\nonumber\\ 
& \le &  (k-1)^2 + k-1 = k(k-1).\nonumber
\end{eqnarray}

Hence,
\begin{equation}
\left\|\nabla\left(\sum_{v\in S_t\setminus \{y_t\}}-\log(p_{y_t,v}(\theta))\right)\right\|_2 \leq \sqrt{k(k-1)}.
\label{equ:lk1sigma}
\end{equation}

Therefore, by vector Azuma-Hoeffding bound in Lemma~\ref{prop:azuma-hoeffding}, (\ref{eq:s1-grad-u}) holds with probability at least $1-2/n$.
\end{proof}

\section{Proof of Theorem~\ref{thm:break}} \label{sec:proof-break}

The proof follows by the same steps as that of Theorem~\ref{thm:break-1}, and the following two lemmas.

%\subsection{Proof of Lemma~\ref{lem:breakk1lambda}}

\begin{lemma} If $\lambda_2\left( L_{\vec{M}} \right) \ge 8k(k-1)e^{2b}n\log(n)/m$, then with probability at least $1-1/n$,
\begin{equation}
\lambda_2(\nabla^2(-\ell_1(\theta))) \ge \frac{1}{4k(k-1) e^{2b}}\frac{m}{n}\lambda_2 (L_{\vec{M}}).
\label{eq:2-chernoff}
\end{equation}
\label{lem:breakk1lambda}
\end{lemma}

\begin{proof} $\nabla^2(-\ell_1(\theta))$ is a sum of random matrices given by:
$$
\nabla^2(-\ell_1(\theta)) = \sum_{t=1}^m \nabla^2 \left(-\log(p_{y_t,z_t}(\theta))\right)
$$
where $p_{y_t,z_t}(\theta) = e^{\theta_{y_t}}/(e^{\theta_{y_t}}+e^{\theta_{z_t}})$.

It is easy to establish that for all $t\in \{1,2,\ldots,m\}$ and $i\neq j$,
\begin{equation}
\frac{\partial^2 (-\log(p_{y_t,z_t}(\theta)))}{\partial \theta_i \partial \theta_j} = \left\{
\begin{array}{ll}
-p_{i,j}(\theta)(1-p_{i,j}(\theta)), & \hbox{ if } {\{i,j\} = \{y_t,z_t\}}\\
0, & \hbox{ otherwise}
\end{array}
\right .
\label{equ:n2logp1}
\end{equation}
and
\begin{equation}
\frac{\partial^2 (-\log(p_{y_t,z_t}(\theta)))}{\partial \theta_i^2} = -\sum_{v\in S_t\setminus \{i\}} \frac{\partial^2 (-\log(p_{y_t,z_t}(\theta)))}{\partial \theta_i\partial \theta_v} .
\label{equ:n2logp2}
\end{equation}

Hence, for every $i,j\in \{1,2,\ldots,n\}$ such that $i\neq j$,
\begin{eqnarray*}
  \E\left[ \frac{\partial^2 (-\log(p_{y_t,z_t}(\theta)))}{\partial \theta_i \partial \theta_j}\right] 
	&=& -p_{i,j}(\theta)(1-p_{i,j}(\theta))\Pr[\{y_t,z_t\}=\{i,j\}]\\
	&=& -p_{i,j}(\theta)(1-p_{i,j}(\theta)) \frac{1}{k-1}(p_{i,S_t}(\theta)+p_{j,S_t}(\theta))\\
	&=& -\frac{1}{k-1}\frac{1}{\sum_{v\in S_t}e^{\theta_v}}\frac{1}{e^{-\theta_i}+e^{-\theta_j}}\\
	& \leq & -\frac{1}{2k(k-1)e^{2b}}.
\end{eqnarray*}

Hence,
$$
\nabla^2 \E[(-\ell_1(\theta))] \succeq \frac{1}{2k(k-1)e^{2b}}\frac{m}{n}L_{\vec{M}}
$$
and, in particular,
\begin{equation}
\lambda_2(\nabla^2 \E[(-\ell_1(\theta))]) \geq \frac{1}{2k(k-1)e^{2b}}\frac{m}{n}\lambda_2(L_{\vec{M}}).
\label{equ:en2lik1}
\end{equation}

By (\ref{equ:n2logp1}) and (\ref{equ:n2logp2}) and Gre\v sgorin circle theorem,
\begin{equation}
\left\|\nabla^2\left(-\log(p_{y_t,z_t}(\theta))\right) \right\|_2 \leq 2 p_{y_t,z_t}(\theta)(1-p_{y_t,z_t}(\theta)) \leq \frac{1}{2}.
\label{equ:en2lik12}
\end{equation} 

The rest of the proof follows by the same arguments as in the proof of Lemma~\ref{lem:break1lambda_2}.
\end{proof}

%\subsection{Proof of Lemma~\ref{lem:breakk1nabla}}

\begin{lemma} With probability at least $1-2/n$,
\begin{equation}
\left\| \nabla(-\ell_1(\theta^\star)) \right\|_2  
\le  2\sqrt{m (\log (n)+2)}.
\label{eq:2-azu}
\end{equation}
\label{lem:breakk1nabla}
\end{lemma}

\begin{proof} $\nabla (-\ell_1(\theta))$ is a sum of independent random vectors in $\reals^n$ with elements given by
$$
\frac{\partial (-\log(p_{y_t,z_t}(\theta)))}{\partial \theta_i} = \left\{
\begin{array}{ll}
-p_{z_t,i}(\theta), & \hbox{ if } i = y_t\\
p_{i,y_t}(\theta), & \hbox{ if } i = z_t\\
0, & \hbox{ otherwise}.
\end{array}
\right .
$$

It follows that for all $t\in \{1,2,\ldots,m\}$ and $i\in \{1,2,\ldots,n\}$,
\begin{eqnarray*}
\E\left[\frac{\partial (-\log(p_{y_t,z_t}(\theta^\star)))}{\partial \theta_i}\right] 
&=& -p_{i,S_t}(\theta^\star) \sum_{j\in S_t \setminus\{ i\}} \frac{1}{k-1}p_{j,i}(\theta^\star) + 
\sum_{j\in S_t \setminus\{ i\}} p_{j,S_t}(\theta^\star)\frac{1}{k-1}p_{i,j}(\theta^\star)\\
&=& \frac{1}{k-1}\sum_{j\in S_t\setminus\{i\}} (-p_{i,S_t}(\theta^\star)p_{j,i}(\theta^\star) + p_{j,S_t}(\theta^\star)p_{i,j}(\theta^\star)) = 0
\end{eqnarray*}
and
\begin{equation}
\left\|\nabla (-\log(p_{y_t,z_t}(\theta)))\right\|_2^2 = p_{z_t,y_t}(\theta)^2 + p_{z_t,y_t}(\theta)^2 \le  2.
\end{equation}
The statement of the lemma than follows by vector Azuma-Hoeffding bound in Lemma~\ref{prop:azuma-hoeffding}.
\end{proof}

\section{Proof of Lemma~\ref{cor:matrix}}

From Theorem 5.1.1 in \cite{tropp2015introduction},
\begin{equation}
\Pr\left[ \lambda_{1}\left(S_m\right) \le
  (1-\epsilon) \lambda_1(\E[S_m]) \right]
	\le n
  \left(\frac{e^{-\epsilon}}{(1-\epsilon)^{1-\epsilon}}
  \right)^{\frac{\lambda_1(\E[S_m])}{\sigma}}\quad\mbox{for}~\epsilon\in [0,1).\label{equ:pro2}
\end{equation}
which combined with the fact
$$
\frac{e^{-\epsilon}}{(1-\epsilon)^{1-\epsilon}} \leq e^{-\frac{\epsilon^2}{2}}, \hbox{ for all } \epsilon \in (0,1]
$$
yields the lemma.

\subsection{Proof of Lemma~\ref{lem:Leig}}

Since $L_{\vec{A}} = \hbox{diag}(\vec{A}\vec{1}) - \vec{A}$, we have
$$
L_{\vec{A}} \vec{1} = \hbox{diag}(\vec{A}\vec{1})\vec{1} - \vec{A}\vec{1} = \vec{A}\vec{1} - \vec{A}\vec{1} = \vec{0}.
$$
Hence, $0$ is an eigenvalue of $L_{\vec{A}}$ for eigenvector $\vec{1}$.

Let $\vec{y}\in \reals^{n}$ be an eigenvector with corresponding eigenvalue $\lambda$ of $L_{\vec{A}}$. Since the columns of $\vec{U}$ are independent, $\vec{U}$ is nonsingular and it has inverse $\vec{U}^{-1}$ (e.g., Section~0.5~\cite{horn}. Let $\vec{x} = \vec{U}^{-1}\vec{y}$). 

Note that the following equations hold:
\begin{eqnarray*}
\vec{x}^T \vec{U}^T L_{\vec{A}} \vec{U}\vec{x} &=& (\vec{U}\vec{x})^\top L_{\vec{A}} (\vec{U}\vec{x})\\
&=& \vec{y}^\top L_{\vec{A}} \vec{y}\\
&=&  \lambda \vec{y}^\top\vec{y}\\
&=&  \lambda \vec{x}^\top \vec{U}^\top \vec{U}\vec{x}\\
&=& \lambda \vec{x}^\top \vec{x}.
\end{eqnarray*}

Hence, it follows that $\lambda$ is an eigenvalue of $\vec{U}^\top L_{\vec{A}} \vec{U}$ with corresponding eigenvector $\vec{x}$. 

\section{Proof of Lemma~\ref{lem:laplaceposdef}}

Let $\vec{C} = \vec{B}-\vec{A} \geq \vec{0}$. Note that 
\begin{eqnarray*}
L_{\vec{B}} - L_{\vec{A}} &=& (\hbox{diag}(\vec{B}\vec{1}) - \vec{B}) - (\hbox{diag}(\vec{A}\vec{1}) - \vec{A})\\
&=& \hbox{diag}((\vec{B}-\vec{A})\vec{1}) - (\vec{B}-\vec{A})\\
&=& L_{\vec{C}}. 
\end{eqnarray*}
Since $L_{\vec{C}}$ is positive semidefinite, it follows that $L_{\vec{B}} - L_{\vec{A}}$ is positive semidefinite, i.e. $L_{\vec{B}}\succeq L_{\vec{A}}$.


\begin{lemma} For all $S\subseteq \{1,2,\ldots,n\}$ such that $|S| = k\geq 2$ we have: for all $i,j\in \{1,2,\ldots,n\}$ such that $i\neq j$, for $v \in \{1,2,\ldots,n\} \setminus \{i,j\}$, 
\begin{equation}
\frac{\partial^2 (-\log(p_k(\vec{x}_v(\vec{0}))))}{\partial \theta_i \partial \theta_j} 
= - k\frac{\partial^2 p_k(\vec{0})}{\partial x_1 \partial x_2}
 + k^2 \left(\frac{\partial p_k(\vec{0})}{\partial x_1}\right)^2
\label{equ:case1}
\end{equation}
and
\begin{equation}
\frac{\partial^2 (-\log(p_k(\vec{x}_j(\vec{0}))))}{\partial \theta_i \partial \theta_j} = \frac{k(k-2)}{2}\frac{\partial^2 p_k(\vec{0})}{\partial x_1 \partial x_2} - k^2(k-1)\left(\frac{\partial p_k(\vec{0})}{\partial x_1}\right)^2.
\label{equ:case2}
\end{equation}
Moreover,
\begin{equation}
\frac{\partial^2 (-\log(p_k(\vec{x}_v(\vec{0}))))}{\partial \theta_i^2} = - \sum_{j\in S\setminus \{i\}} \frac{\partial^2 (-\log(p_k(\vec{x}_v(\vec{0}))))}{\partial \theta_i \partial \theta_j}.
\label{equ:diff2i}
\end{equation}
\label{lem:partiallogs}
\end{lemma}

\begin{proof} Let $S\subseteq N$ be such that $|S| = k$, for an integer $2\leq k \leq n$. Without loss of generality, let $S = \{1,2,\ldots,k\}$. Let $\vec{x}_v(\theta) = (\theta_v - \theta_u, u\in S\setminus \{v\})$, for $v\in S$. We first consider $\frac{\partial^2}{\partial \theta_i \partial \theta_j}(-\log(p_k(\vec{x}_v(\theta))))$ for $i\neq j$. It is easy to note that
\begin{eqnarray*}
\frac{\partial^2 (-\log(p_k(\vec{x}_v(\theta))))}{\partial \theta_i \partial \theta_j} 
&=& -\frac{1}{p_k(\vec{x}_v(\theta))}\frac{\partial^2 p_k(\vec{x}_v(\theta))}{\partial\theta_i \partial \theta_j}\nonumber\\
&&  + \frac{1}{p_k(\vec{x}_v(\theta))^2} \frac{\partial p_k(\vec{x}_v(\theta))}{\partial \theta_i}  \frac{\partial p_k(\vec{x}_v(\theta))}{\partial \theta_j}. 
\end{eqnarray*}

We separately consider two different cases. 

Consider first the case when $\{i\}\cap \{j\}\cap \{v\} = \emptyset$. By differentiation, we have 
$$
\frac{\partial^2 (-\log(p_k(\vec{x}_v(\vec{0}))))}{\partial \theta_i \partial \theta_j} 
= - k\frac{\partial^2 p_k(\vec{0})}{\partial x_1 \partial x_2}
 + k^2 \left(\frac{\partial p_k(\vec{0})}{\partial x_1}\right)^2
$$
which establishes (\ref{equ:case1}).

Consider now the case when $i \neq v$ and $j = v$. First, note
\begin{eqnarray}
\frac{\partial^2}{\partial \theta_i \partial \theta_j}(-\log(p_k(\vec{x}_j(\theta))))
&=& -\frac{1}{p_k(\vec{x}_j(\theta))}\frac{\partial^2 p_k(\vec{x}_j(\theta))}{\partial\theta_i \partial \theta_j}\nonumber\\
&&  + \frac{1}{p_k(\vec{x}_j(\theta))^2} \frac{\partial p_k(\vec{x}_j(\theta))}{\partial \theta_i}  \frac{\partial p_k(\vec{x}_j(\theta))}{\partial \theta_j}.
\label{equ:e0}
\end{eqnarray}

For every $u\in S$, $p_k(\vec{x}_u(\theta))$ does not change its value by changing $\theta$ with $\theta + c \vec{1}$, for every constant $c\in \reals$. Hence, by full differentiation, we have
\begin{equation}
\frac{\partial p_k(\vec{x}_j(\theta))}{\partial \theta_j} = -\sum_{v\in S \setminus \{j\}} \frac{\partial p_k(\vec{x}_u(\theta))}{\partial \theta_v}.
\label{equ:fulldiff}
\end{equation}

From (\ref{equ:fulldiff}),
\begin{equation}
\frac{\partial^2 p_k(\vec{x}_j(\theta))}{\partial\theta_i \partial \theta_j} = -\frac{\partial^2 p_k(\vec{x}_j(\theta))}{\partial \theta_i^2} - \sum_{v\in S \setminus \{i,j\}} \frac{\partial^2 p_k(\vec{x}_j(\theta))}{\partial \theta_i \partial \theta_v}.
\label{equ:inter}
\end{equation}

Now, note that
$$
\frac{\partial^2 p_k(\vec{x}_j)}{\partial \theta_i^2} = \int_\reals f(z) f'(x_i+z)\prod_{v\in S\setminus \{i,j\}}F(x_v+z)dz.
$$
Hence, 
\begin{eqnarray*}
\frac{\partial^2 p_k(\vec{x}_j(\vec{0}))}{\partial \theta_i^2}
&=&  \int_\reals f(z) f'(z) F(z)^{k-2}dz\\
&=& f(z)^2F(z)^{k-2}|_{-\infty}^\infty - \int_\reals f(z) (f(z)F(z)^{k-2})'dz\\
&=& - \int_\reals f(z)f'(z)F(z)^{k-2}dz - (k-2) \int_\reals f(z)^2F(z)^{k-3}dz\\
&=& - \frac{\partial^2 p_k(\vec{x}_j(\vec{0}))}{\partial \theta_i^2} - (k-2) \frac{\partial^2 p_k(\vec{0})}{\partial x_1 \partial x_2}.
\end{eqnarray*}

From this it follows
\begin{equation}
\frac{\partial^2 p_k(\vec{x}_j(\vec{0}))}{\partial \theta_i^2} = -\frac{k-2}{2}\frac{\partial^2 p_k(\vec{0})}{\partial x_1 \partial x_2}.
\label{equ:e2}
\end{equation}

From (\ref{equ:inter}) and (\ref{equ:e2}), 
\begin{equation}
\frac{\partial^2 p_k(\vec{x}_j(\vec{0}))}{\partial\theta_i \partial \theta_j} = -\frac{k-2}{2}\frac{\partial^2 p_k(\vec{0})}{\partial x_1\partial x_2}.
\label{equ:e3}
\end{equation}

From (\ref{equ:fulldiff}),
\begin{equation}
\frac{\partial p_k(\vec{x}_j(\vec{0}))}{\partial \theta_j} = (k-1)\frac{\partial p_k(\vec{0})}{\partial x_1}.
\label{equ:e1}
\end{equation}

Combining (\ref{equ:e0}), (\ref{equ:e3}) and (\ref{equ:e1}), we obtain (\ref{equ:case2}).

Since for every $v\in S$,  $\log(p_k(x_v(\theta)))$ does not change its value by changing $\theta$ to $\theta + c\vec{1}$ for all $c\in \reals$, by full differentiation
$$
\frac{\partial (-\log(p_k(x_v(\theta))))}{\partial \theta_i} = -\sum_{j\in S\setminus \{i\}}\frac{\partial (-\log(p_k(x_v(\theta)))}{\partial \theta_j}.
$$
Taking partial derivative with respect to $\theta_i$ on both sides implies (\ref{equ:diff2i}).
\end{proof}

\begin{lemma} Let $S\subseteq N$ be such that $|S| = k\geq 2$ and $Y$ be a random variable according to distribution $p_{y,S}(\theta)$, for $\theta \in \reals^n$. Then, 
$$
\E\left[\nabla^2(-\log(p_{Y,S}(\vec{0})))\right] =  \left(k\frac{\partial p_k(\vec{0})}{\partial x_1}\right)^2 L_{\vec{M}_S}
$$
where $\vec{M}_S = [m_{i,j}^S]\in \reals^{n\times n}$ is such that $m_{i,j}^S = 1$ if $i,j\in S$ and $i\neq j$, and $m_{i,j}^S = 0 $, otherwise.
\label{lem:explog}
\end{lemma}

\begin{proof} By (\ref{equ:case1}) and (\ref{equ:case2}) in Lemma~\ref{lem:partiallogs}, we have for all $i,j\in \{1,2,\ldots,n\}$ such that $i\neq j$,
\begin{eqnarray}
\E\left[\frac{\partial^2 (-\log(p_{Y,S}(\vec{0})))}{\partial\theta_i \partial \theta_j}\right] 
&=&  \sum_{v\in S\setminus \{i,j\}} p_k(\vec{x}_v(\vec{0})) \frac{\partial^2 (-\log(p_k(\vec{x}_v(\vec{0}))))}{\partial \theta_i \partial \theta_j}\nonumber\\
&& + p_k(\vec{x}_i(\vec{0})) \frac{\partial^2 (-\log(p_k(\vec{x}_i(\vec{0}))))}{\partial \theta_i \partial \theta_j} \nonumber\\
&& + p_k(\vec{x}_j(\vec{0})) \frac{\partial^2 (-\log(p_k(\vec{x}_j(\vec{0}))))}{\partial \theta_i \partial \theta_j} \nonumber\\
&=& \frac{k-2}{k}\frac{\partial^2 (-\log(p_k(\vec{x}_v(\vec{0}))))}{\partial \theta_i \partial \theta_j} \nonumber \\
&& + \frac{2}{k}\frac{\partial^2 (-\log(p_k(\vec{x}_i(\vec{0}))))}{\partial \theta_i \partial \theta_j}, \hbox{ for any } v \in S\setminus\{i,j\}\nonumber\\
&=& -k^2 \left(\frac{\partial p_k(\vec{0})}{\partial x_1}\right)^2.\label{equ:gijdiff}
\end{eqnarray}

By (\ref{equ:diff2i}) in Lemma~\ref{lem:partiallogs}, we have
\begin{eqnarray*}
\E\left[\frac{\partial^2 (-\log(p_{Y,S}(\vec{0})))}{\partial\theta_i^2}\right] &=& -\sum_{u\in S\setminus \{i\}} \sum_{v\in S} p_k(\vec{x}_v(\vec{0}))\frac{\partial^2 (-\log(p_k(\vec{x}_v(\vec{0}))))}{\partial \theta_i\partial \theta_u}\\ 
&=& k^2(k-1) \left(\frac{\partial p_k(\vec{0})}{\partial x_1}\right)^2.
\end{eqnarray*}
\end{proof}

\begin{lemma} If for $S\subseteq N$ such that $|S| =k \geq 2$,
$$
\frac{\partial^2 (-\log(p_{v,S}(\vec{0})))}{\partial \theta_i\partial \theta_j} \leq 0 \hbox{ for all } i,j,v\in \{1,2,\ldots,n\} \hbox{ such that } i\neq j
$$
then
$$
\left\| \nabla^2(-\log(p_{y,S} ({\bm 0}))) \right\|_2 \le \frac{1}{\gamma_{F,k}}
$$
where $1/\gamma_{F,k} = \left(k^2 \partial p_k(\vec{0})/\partial x_1\right)^2$.
\label{lem:hessian-p2}
\end{lemma}

\begin{proof} Without loss of generality, let $y=1$ and $S = \{1,2,\dots,k \}$. By Lemma~\ref{lem:partiallogs}, we have
\begin{equation}
\frac{\partial^2(-\log(p_{1,S} ({\bm 0})))}{\partial \theta_1 \partial \theta_2} 
= - k^2(k-1)\left( \frac{\partial p_k({\bm 0})}{\partial x_1}\right)^2
 + \frac{k(k-2)}{2}\frac{\partial^2 p_k({\bm 0})}{\partial x_1 \partial x_2} 
\label{eq:hessi-1}
\end{equation}
and for $i\neq 1$, $j\neq 1$ and $j\neq i$,
\begin{equation}
\frac{\partial^2 (-\log(p_{1,S} ({\bm 0})))}{\partial \theta_i \partial \theta_j} = -k \frac{\partial^2 p_k({\bm 0})}{\partial x_1 \partial x_2}
+ k^2\left(\frac{\partial p_k({\bm 0})}{\partial x_1}\right)^2. 
\label{eq:hessi-2}
\end{equation}

Since by condition of the lemma the left-hand side in (\ref{eq:hessi-1}) is non positive, we have 
\begin{equation}
(k-2)\frac{\partial^2 p_k({\bm 0})}{\partial x_1 \partial x_2} \le 2k(k-1) \left( \frac{\partial p_k({\bm 0})}{\partial x_1}\right)^2
\label{eq:hessi-3}
\end{equation}
and, since by condition of the lemma the left-hand side in (\ref{eq:hessi-2}) is non positive, we have 
\begin{equation}
\frac{\partial^2 p_k({\bm 0})}{\partial x_1 \partial x_2}\ge 0.
\label{eq:hessi-4}
\end{equation} 

From (\ref{eq:hessi-1}) and (\ref{eq:hessi-4}),
$$
\frac{\partial^2(-\log(p_{1,S} ({\bm 0})))}{\partial \theta_1 \partial \theta_2} 
\geq - k^2(k-1)\left( \frac{\partial p_k({\bm 0})}{\partial x_1}\right)^2.
$$
Consider now the case when $i\neq 1$, $j\neq 1$ and $i\neq j$. If $k = 2$, then obviously 
$$
\frac{\partial^2 (-\log(p_{1,S} ({\bm 0})))}{\partial \theta_i \partial \theta_j} = 0.
$$
Otherwise, if $k > 2$, from (\ref{eq:hessi-2}) and (\ref{eq:hessi-3}), we have
\begin{eqnarray*}
\frac{\partial^2 (-\log(p_{1,S} ({\bm 0})))}{\partial \theta_i \partial \theta_j} & \geq & k^2 \left(\frac{\partial p_k(\vec{0})}{\partial x_1}\right)^2 - \frac{2k^2(k-1)}{k-2} \left(\frac{\partial p_k(\vec{0})}{\partial x_1}\right)^2\\
& = & -k^2\left(-1 + \frac{2(k-1)}{k-2}\right)\left(\frac{\partial p_k(\vec{0})}{\partial x_1}\right)^2\\
& = & -k^2 \frac{k}{k-2}\left(\frac{\partial p_k(\vec{0})}{\partial x_1}\right)^2\\
& \geq & -k^3 \left(\frac{\partial p_k(\vec{0})}{\partial x_1}\right)^2.
\end{eqnarray*}


Hence, it follows  
$$
k^3 \left( \frac{\partial p_k({\bm 0})}{\partial x_1}\right)^2 L_{\vec{M}_S} \succeq \nabla^2 (-\log(p_{y,S} ({\bm 0}))).
$$
Therefore, we conclude
\begin{eqnarray*}
\|\nabla^2 (-\log(p_{y,S} ({\bm 0}))) \|_2 &\leq & k^3 \left( \frac{\partial p_k({\bm 0})}{\partial x_1}\right)^2 \|L_{\vec{M}_S}\|_2\\
& = & k^4 \left(\frac{\partial p_k(\vec{0})}{\partial x_1}\right)^2.
\end{eqnarray*}
\end{proof}


\section{Remark for Theorem~\ref{thm:karyub}}

For the special case of noise according to the double-exponential distribution with parameter $\beta$, we have
$$
p_k(\vec{x})=\frac{1}{1+\sum_{i=1}^{k-1}e^{-x_i/\beta}}.
$$

For every $\theta\in \theta_n$ and every $S\subseteq N$ of cardinality $k$ and $i,j,y\in S$, we can easily check that
$$
\frac{\partial^2}{\partial \theta_i \partial \theta_j} (-\log(p_{y,S}(\theta))) = -\frac{1}{\beta^2} p_{i,S}(\theta)p_{j,S}(\theta).
$$
Furthermore, the following two relations hold
$$
\frac{k}{\beta(k-1)}\left(1-p_{y,S}(\theta)\right)^2 \le \left\|\nabla p_{y,S}(\theta)\right\|_2 \le \frac{2}{\beta}\left(1-p_{y,S}(\theta) \right)^2.
$$

Since 
$$
\min_{y\in S,\theta\in [-b,b]^n} p_{y,S}(\theta) 
= \frac{1}{1+(k-1)e^{2b/\beta}}
\ge p_{y,S}({\bm 0}) e^{-2b/\beta}
$$
and
$$
\max_{y\in S,\theta\in [-b,b]^n} p_{y,S}(\theta) 
= \frac{1}{1+(k-1)e^{-2b/\beta}}
\le p_{y,S}({\bm 0}) e^{2b/\beta}
$$
we have that 
$$
\sigma_{F,K} \le \frac{1}{\beta^2}
$$
and
\begin{align}
&e^{-4b/\beta}\le A \le \widetilde{A} \le e^{4b/\beta}, \\
&e^{-4b/\beta}\le \widetilde{B} \le B \le 4,\\
&e^{-2b/\beta}\le C \le \widetilde{C} \le e^{2b/\beta}.
\end{align}



\section{Proof of Theorem~\ref{thm:clustering-example}}
\label{sec:clustering-example}

Let $p^e$ denote the probability that the point score ranking method incorrectly classifies at least one item: 
$$
p^e = \Pr\left[\bigcup_{v\in N_1} \{v\in \widehat N_1\} \cup \bigcup_{v\in N_2} \{v\in \widehat N_2\}\right]
$$

Let $R_i$ denote the point score of item $i\in N$. If the point scores are such that $R_v > m/n$ for every $v\in N_1$ and $R_v < m/n$ for every $v\in N_2$, then this implies a correct classification. Hence, it must be that in the event of a misclassification of an item, $R_v \leq m/n$ for some $v\in N_1$ or $R_v \geq m/n$ for some $v\in N_2$. Combining this with the union bound, we have
\begin{equation}
p^e 
%\leq \Pr\left[\bigcup_{v\in N_1} \left\{R_v \leq \frac{m}{n}\right\} \cup \bigcup_{v\in N_2} \left\{R_v \geq \frac{m}{n}\right\}\right]
\leq \sum_{v\in N_v}\Pr\left[R_v \leq \frac{m}{n}\right] + \sum_{l\in N_2}\Pr\left[R_v \geq \frac{m}{n}\right].
\label{equ:tobound}
\end{equation}

Let $i$ and $j$ be arbitrarily fixed items such that $i\in N_1$ and $j\in N_2$. We will show that for $t\in \{1,2,\ldots,m\}$,
\begin{equation}
\Pr[y_t = i] \geq \frac{1}{n} + \frac{bk^2}{4n}\frac{\partial p_k(\vec{0})}{\partial x_1}
\label{equ:yti}
\end{equation}
and
\begin{equation}
\Pr[y_t = j] \leq \frac{1}{n} - \frac{bk^2}{4n}\frac{\partial p_k(\vec{0})}{\partial x_1}.
\label{equ:ytj}
\end{equation}

By the Chernoff bound (\ref{equ:che1}) we have
\begin{eqnarray*}
\Pr\left[R_i \leq \frac{m}{n}\right] & \leq & \exp\left(-\frac{1}{4}n\left(\frac{1}{n} - \E[y_1 = i]\right)^2 m\right)\\
& \leq & \exp\left(-\frac{1}{4}\left(\frac{bk^2}{4n}\frac{\partial p_k(\vec{0})}{\partial x_1}\right)^2 m\right)\\
& \leq & \exp\left(-\log(n/\delta)\right)\\
&=& \frac{\delta}{n}.
\end{eqnarray*}

Similarly, by the Chernoff bound (\ref{equ:che2}), we have
$$
\Pr\left[R_j \geq \frac{m}{n}\right] \leq \frac{\delta}{n}.
$$

Combining with (\ref{equ:tobound}), we have $p^e \leq \delta$.

In the remainder of the proof we show that (\ref{equ:yti}) and (\ref{equ:ytj}) hold.

Let $\calA$ contain all $A \subseteq N$ such that $|A| = k-1$ and $A\cap\{i,j\}=\emptyset$ and $\calB$ contain all $B\subseteq N$ such that $|B| = k-2$ and $B\cap\{i,j\}=\emptyset$. Then, we have
\begin{equation}
\Pr[ y_t = i ] - \Pr[ y_t = j ] = \sum_{A\in \mathcal{A}} \Pr[S_t = A\cup\{i\}] D_{i,j}(A) +\sum_{B\in \mathcal{B}} \Pr[S_t = B \cup \{i,j\}] D_{i,j}(B) \label{eq:prdifer}
\end{equation}
where
$$
D_{i,j}(A) = \Pr[ y_t = i | S_t= A\cup\{i \} ] - \Pr[ y_t = j| S_t= A\cup\{j \}]
$$
and
$$
D_{i,j}(B) = \Pr[ y_t = i | S_t= B\cup\{i,j \}] -\Pr[ y_t = j | S_t= B\cup\{i,j \}].
$$

Let $\vec{b}$ be a $k-1$-dimensional vector with all elements equal to $b$. Then, note that
$$
D_{i,j}(A) = p_k (\vec{b}-\theta_{A})- p_k (-\vec{b}-\theta_{A}).
$$

By limited Taylor series development, we have
\begin{eqnarray}
p_k (\vec{x}) &\ge& p_k (\vec{0}) + \nabla p_k (\vec{0})^\top \vec{x} - \frac{1}{2} \beta \|\vec{x}\|_2^2 \label{equ:ptaylor1}\\
p_k (\vec{x}) &\le& p_k (\vec{0}) + \nabla p_k (\vec{0})^\top \vec{x} + \frac{1}{2} \beta \|\vec{x}\|_2^2
\label{equ:ptaylor2}
\end{eqnarray}
where 
\begin{equation}
\beta = \max_{\vec{x}\in [-2b,2b]^{k-1}} \|\nabla^2 p_k (\vec{x}) \|_2.
\label{equ:betaparam}
\end{equation}

Hence, it follows that for every $\theta_{A}\in \{-b,b\}^{k-1}$, 
\begin{equation}
D_{i,j}(A) \geq 2(k-1)b \frac{\partial p_k (\vec{0})}{\partial x_1} - 
 4(k-1)b^2\beta.
\label{eq:cb1}
\end{equation}
Under the condition of the theorem, we have
$$
\beta \le \frac{1}{4b}\frac{\partial p_k (\vec{0})}{\partial x_1}.
$$
Hence, combining with \eqref{eq:cb1}, for every $\theta_{A}\in \{-b,b\}^{k-1}$,
\begin{equation}
D_{i,j}(A) \geq (k-1)b \frac{\partial p_k (\vec{0})}{\partial x_1} \geq  \frac{kb}{2} \frac{\partial p_k (\vec{0})}{\partial x_1}. 
\label{eq:thea}
\end{equation}

By the same arguments, we can show that 
\begin{equation}
D_{i,j}(B) = p_k ({\bm b}-\theta^{(-b)}_{B})- p_k (-{\bm b}-\theta^{(b)}_{B}) \geq \frac{kb}{2} \frac{\partial p_k (\vec{0})}{\partial x_1}
\label{eq:theb}
\end{equation}
where $\theta^{(b)}_{B}\in \{-b,b \}^{k-1}$ and $\theta^{(-b)}_{B}\in \{-b,b \}^{k-1}$ are $(k-1)$-dimensional vectors with the first two elements equal to $b$ and $-b$, respectively, and other elements equal to the parameters of items $B$. 

Since comparison sets are sampled uniformly at random without replacement, 
\begin{equation}
\Pr[S_t = A\cup \{i\}] = \frac{\binom{n-1}{k-1}}{\binom{n}{k}}, \hbox{ for all } A \in {\mathcal A}
\label{equ:setA}
\end{equation}
and
\begin{equation}
\Pr[S_t = B\cup \{i,j\}] = \frac{\binom{n-2}{k-2}}{\binom{n}{k}}, \hbox{ for all } B \in {\mathcal B}.
\label{equ:setB}
\end{equation}

From \eqref{eq:prdifer}, \eqref{eq:thea}, \eqref{eq:theb}, \eqref{equ:setA} and \eqref{equ:setB}, we have
$$
\Pr[ y_t = i ] - \Pr[ y_t = j ] \ge  \frac{k^2b}{2n} \frac{\partial p_k (\vec{0})}{\partial x_1}.
$$

Using this inequality together with the following facts (i) $\Pr[y_t = v] = \Pr[y_t = i]$ for every $v\in N_1$, (ii) $\Pr[y_t = v] = \Pr[y_t = j]$ for every $v\in N_2$, (iii) $\sum_{v\in N} \Pr[y_t = v] = 1$, and (iv) $|N_1| = |N_2| = n/2$, it can be readily shown that
$$
\Pr[ y_t = i ] \ge \frac{1}{n} + \frac{k^2b}{4n} \frac{\partial p_k ({\bm 0})}{\partial x_1},
$$
which establishes (\ref{equ:yti}). By the same arguments one can establish (\ref{equ:ytj}).

\section{Proof of Theorem~\ref{thm:clustering-low}}
\label{sec:clustering-low}

Suppose that $n$ is a positive even integer and $\theta$ is the parameter vector such that $\theta_i =b$ for $i\in N_1$ and $\theta_i = -b$ for $i \in N_2$, where $N_1 =\{1,2,\ldots,n/2\}$ and $N_2 = \{n/2+1,\ldots,n\}$. Let $\theta'$ be the parameter vector that is identical to $\theta$ except for swapping the first and the last item, i.e. $\theta'_i = b$ for $i\in N_1'$ and $\theta'_i = -b$ for $i\in N_2'$, where $N_1' = \{n,2,\ldots,n/2\}$ and $N_2'=\{n/2+1,\ldots,n-1,1\}$. 

We denote with $\Pr_\theta[A]$ and $\Pr_{\theta'}[A]$ the probabilities of an event $A$ under hypothesis that the generalized Thurstone model is according to parameter $\theta$ and $\theta'$, respectively. We denote with $\E_\theta$ and $\E_{\theta'}$ the expectations under the two respective distributions. 

Given observed data $(\vec{S},\vec{y}) = (S_1,y_1), \ldots, (S_m,y_m)$, we denote the log-likelihood ratio statistic $L(\vec{S},\vec{y})$ as follows
\begin{equation}
L(\vec{S},\vec{y}) = \sum_{t=1}^{m}\log\left(\frac{p_{y_t,S_t}(\theta')\rho_t(S_t)}{p_{y_t,S_t}(\theta)\rho_t(S_t)}\right),
\end{equation}
where $\rho_t (S)$ is the probability that $S$ is drawn at time $t$.

The proof follows the following two steps:

\paragraph{Step 1:} We show that for given $\delta \in [0,1]$, for the existence of an algorithm that correctly classifies all the items with probability at least $1-\delta$, it is necessary that the following condition holds
\begin{equation}
\Pr_{\theta'}[L(\vec{S},\vec{y}) \ge \log(n/\delta)] \ge \frac{1}{2}.
\label{equ:plrt}
\end{equation}

\paragraph{Step 2:} We show that 
\begin{eqnarray}
\E_{\theta'}[L(\vec{S},\vec{y})] &\le& 36\frac{m}{n}\left(k^2b\frac{\partial p_k ({\bm 0})}{\partial x_1}\right)^2 \label{equ:el}\\
\sigma^2_{\theta'}[L(\vec{S},\vec{y})] &\le& 144\frac{m}{n}\left(k^2b\frac{\partial p_k ({\bm 0})}{\partial x_1}\right)^2\label{equ:varl}
\end{eqnarray}
where $\sigma^2_{\theta'}[L(\vec{S},\vec{y})]$ denotes the variance of random variable $L(\vec{S},\vec{y})$ under a generalized Thurstone model with parameter $\theta'$.

By Chebyshev's inequality, for every $g\in \reals$,
$$
\Pr_{\theta'}[|L(\vec{S},\vec{y})-\E_{\theta'}[L(\vec{S},\vec{y})]| \geq |g|] \leq \frac{\sigma^2_{\theta'}[L(\vec{S},\vec{y})]}{g^2}.
$$

Using this for  $g = \log(n/\delta)-\E_{\theta'}[L(\vec{S},\vec{y})]$, it follows that (\ref{equ:plrt}) implies the following condition:
\begin{eqnarray*}
\log (n/\delta) - \E_{\theta'}[L(\vec{S},\vec{y})]  & \le &  |\log (n/\delta) - \E_{\theta'}[L(\vec{S},\vec{y})]|\\
& \leq & \sqrt{2} \sigma_{\theta'}[L(\vec{S},\vec{y})].
\end{eqnarray*}

Further combining with (\ref{equ:el}) and (\ref{equ:varl}), we obtain
$$
m \geq \frac{1}{62}\frac{1}{b^2 k^4 (\partial p_k(\vec{0})/\partial x_1)^2} n(\log(n) + \log(1/\delta))
$$
which is the condition asserted in the theorem.

\paragraph{Proof of Step 1.} Let us define the following two events 
$$
A = \{|N_1\setminus \widehat N_1| = 1\} \cap \{|N_2 \setminus \widehat N_2| = 1\}
$$
and
$$
B = \{\widehat N_1 = N_1'\} \cap \{\widehat N_2 = N_2'\}.
$$
Let $B^c$ denote the complement of event $B$.

Note that
$$
\Pr_{\theta}[B] = \Pr_{\theta}[B|A] \Pr_\theta[A]
= \left(\frac{2}{n} \right)^2 \Pr_{\theta}[A]
\le \frac{4}{n^2} \delta
$$
where the second equation holds because $B \subseteq A$ and every possible partition in $A$ has the same probability under $\theta$.

For every $g\in \reals$, we have
$$
\Pr_{\theta'}[L(\vec{S},\vec{y}) \le g] =  \Pr_{\theta'}[L(\vec{S},\vec{y}) \le g,B] + \Pr_{\theta'}[L(\vec{S},\vec{y}) \le g, B^c].
$$

Now, note
\begin{eqnarray*}
\Pr_{\theta'} [L(\vec{S},\vec{y}) \le  g , B] &= & \E_{\theta'}[\Ind(L(\vec{S},\vec{y})\leq g,B)]\\
&= & \E_\theta[e^{L(\vec{S},\vec{y})}\Ind(L(\vec{S},\vec{y})\leq g,B)]\\
&\le & \E_\theta[e^{g}\Ind(L(\vec{S},\vec{y})\leq g,B)]\\
&=& e^g \Pr_{\theta}[L(\vec{S},\vec{y})\leq g, B]\\
&\le & e^g \Pr_{\theta}[B]\\
&\le & e^g\frac{4}{n^2}\delta
\label{eq:bdl-1} 
\end{eqnarray*}
where in the second equation we use the standard change of measure argument.

Since the algorithm correctly classifies all the items with probability at least $1-\delta$, we have 
\begin{equation}
\Pr_{\theta'}[L(\vec{S},\vec{y}) \le g,B^c] \le \Pr_{\theta'}[B^c] \le \delta.
\label{eq:bdl-2}
\end{equation}

For $g = \log(n/\delta)$, from \eqref{eq:bdl-1} and \eqref{eq:bdl-2}, it follows that
$$
\Pr_{\theta'}[L(\vec{S},\vec{y}) \le \log(n/\delta)] \le \delta + \frac{4}{n} \leq \frac{1}{2}
$$
where the last inequality is by the conditions of the theorem.

\paragraph{Proof of Step 2.} If the observed comparison sets $S_1,S_2,\ldots,S_m$ are such that $S_t \cap \{1,n \} = \emptyset$, for every observation $t$, then we obviously have
$$
\log\left(\frac{p_{y_t,S_t}(\theta')}{p_{y_t,S_t}(\theta)}\right) = 0, \hbox{ for all } t.
$$
We therefore consider the case when $S_t \cap \{1,n \} \neq \emptyset$. 

Using (\ref{equ:ptaylor1}), (\ref{equ:ptaylor2}), and (\ref{equ:betaparam}), we have for every $S$ and $i\in S$,
\begin{equation}
|p_{i,S}(\theta')-p_{i,S}(\theta)| 
\le 2kb \frac{\partial p_k (\vec{0})}{\partial x_1} + 4 \beta bk
\le 3kb \frac{\partial p_k (\vec{0})}{\partial x_1} \label{eq:pdifer}
\end{equation}
where the last inequality is obtained from the condition of this theorem. 

From \eqref{eq:pdifer}, for every comparison set $S$ such that $S \cap \{1,n\} \neq \emptyset$, we have
\begin{align}
\sum_{i\in S}\left(p_{i,S}(\theta')-p_{i,S}(\theta)\right)^2 
\le& \sum_{i\in \{1,n \}\cap S}\left(p_{i,S}(\theta')-p_{i,S}(\theta)\right)^2 + 
\left(\sum_{i\in S\setminus \{1,n \} }p_{i,S}(\theta')-p_{i,S}(\theta)\right)^2 \cr
\le& 2\left(3kb \frac{\partial p_k (\vec{0})}{\partial x_1} \right)^2,
\label{eq:dsq}
\end{align}
which is because for every comparison set $S$ such that $1 \in S$,
$$
p_{1,S}(\theta') \le \frac{1}{k} \le p_{1,S}(\theta) \hbox{ and } 
p_{i,S}(\theta') \ge p_{i,S}(\theta)\quad \forall i \neq 1;
$$
and, for every comparison set $S$ such that $n\in S$,
$$
p_{n,S}(\theta') \ge \frac{1}{k} \ge p_{n,S}(\theta) \hbox{ and }
p_{i,S}(\theta') \le p_{i,S}(\theta)\quad\forall i \neq n.
$$

From \eqref{eq:pdifer} and the assumption of the theorem, we have
\begin{equation}
\min_{S}\min_{i\in S} p_{i,S}(\theta) = \min_{S: n\in S} p_{n,S}(\theta)
\ge \frac{1}{k} - 3kb \frac{\partial p_k (\vec{0})}{\partial x_1}
\ge \frac{1}{2k}.
\label{eq:pmin}
\end{equation}

For simplicity of notation, let
\begin{equation}
D = 3kb \frac{\partial p_k (\vec{0})}{\partial x_1}.
\end{equation}

Then, for all $S$ such that $S\cap \{1,n\} \neq \emptyset$, we have
\begin{equation}
\sum_{i\in S} p_{i,S}(\theta') \log\left(\frac{p_{i,S}(\theta')}{p_{i,S}(\theta)}\right) \le 2 k D^2
\label{eq:meanL}
\end{equation}
which is obtained from 
\begin{itemize}
\item[(i)] $p_{i,S}(\theta) \ge 1/(2k)$ for all $i\in S$ that holds by \eqref{eq:pmin},
\item[(ii)] $\sum_{i\in S}(p_{i,S}(\theta')-p_{i,S}(\theta))^2 = 2D^2$ from \eqref{eq:dsq}, 
\item[(iii)] 
$a\log\frac{a}{b} \le \frac{(a-b)^2}{2b}+a-b$. 
\end{itemize}
Similarly to \eqref{eq:meanL}, from (i) and (ii) and $a\left(\log\frac{a}{b}\right)^2 \le \frac{(a-b)^2}{a \wedge b}\left(1+\frac{|a-b|}{3(a\wedge b)}\right) $, we have
\begin{equation}
\sum_{i\in S} p_{i,S}(\theta')\left(\log\left(\frac{p_{i,S}(\theta')}{p_{i,S}(\theta)}\right)\right)^2 \le 8k D^2.
\label{eq:varL}
\end{equation}
Since 
$$
\Pr_{\theta'}[\{S_t\cap \{1,n \}\neq \emptyset\}] = 1- \frac{{n-2 \choose k}}{{n \choose k}} \le 2\frac{k}{n}
$$ 
and according to the model, the input observations are independent, from \eqref{eq:meanL} and \eqref{eq:varL}, we have
\begin{eqnarray}
\E_{\theta'}[L(\vec{S},\vec{y})] 
&=&  m \E_{\theta'}\left[\log\left(\frac{p_{y_1,S_1}(\theta')}{p_{y_1,S_1}(\theta)}\right) \right] \cr
&= & m\sum_{S: S\cap\{1,n\}\neq \emptyset} \Pr_{\theta'}[S_1 = S] \sum_{y\in S}p_{y,S}(\theta')\left[\log\left(\frac{p_{y,S}(\theta')}{p_{y,S}(\theta)}\right) \right] \cr
&\le& 4\frac{m}{n}k^2 D^2
\end{eqnarray}
and
\begin{eqnarray}
\sigma^2_{\theta'}[L(\vec{S},\vec{y})]
&= & m \sigma^2_{\theta'}\left[\log\left(\frac{p_{y_1,S_1}(\theta')}{p_{y_1,S_1}(\theta)}\right) \right] \cr
&\le & m \E_{\theta'}\left[\left(\log\left(\frac{p_{y_1,S_1}(\theta')}{p_{y_1,S_1}(\theta)}\right)\right)^2 \right] \cr
&= & m\sum_{S: S\cap\{1,n\}\neq \emptyset} \Pr_{\theta'}[S_1 = S]\sum_{y\in S}p_{y,S}(\theta')\left[\left(\log\left(\frac{p_{y,S}(\theta')}{p_{y,S}(\theta)}\right)\right)^2 \right] \cr
&\le & 16\frac{m}{n}k^2 D^2.
\end{eqnarray}

\section{Characterizations of $\partial p_k(\vec{0})/\partial x_1$}

In this section, we note several different representations of the parameter $\partial p_k(\vec{0})/\partial x_1$. 

First, note that
\begin{equation}
\frac{\partial p_k(\vec{0})}{\partial x_1} = \frac{1}{k-1}\int_\reals f(x)dF(x)^{k-1}.
\label{equ:ch1}
\end{equation} 
The integral corresponds to $\E[f(X)]$ where $X$ is a random variable whose distribution is equal to that of a maximum of $k-1$ independent and identically distributed random variables with cumulative distribution $F$. 

Second, suppose that $F$ is a cumulative distribution function with its support contained in $[-a,a]$, and that has a differentiable density function $f$. Then, we have
\begin{equation}
\frac{\partial p_k(\vec{0})}{\partial x_1} = A_{F,k} + B_{F,k}
\label{equ:ch2}
\end{equation}

where
$$
A_{F,k} = \frac{1}{k-1}f(a)
$$
and
$$
B_{F,k} = \frac{1}{k(k-1)}\int_{-a}^{a} (-f'(x))dF(x)^k.
$$

The identity (\ref{equ:ch2}) is shown to hold as follows. Note that
$$
\frac{d^2}{dx^2} F(x)^k = \frac{d}{dx}(k F(x)^{k-1}f(x))
= k(k-1)F(x)^{k-2}f(x)^2 + kF(x)^{k-1}f'(x).
$$
By integrating over $[-a,a]$, we obtain
$$
\frac{d}{dx}F(x)^k |_{-a}^{a} = k(k-1) \int_{-a}^{a} f(x)^2 F(x)^{k-2}dx + k \int_{-a}^{a} f'(x) F(x)^{k-1}dx. 
$$
Combining with the fact 
$$
\frac{d}{dx}F(x)^k |_{-a}^{a} = k f(x)F^{k-1}(x) |_{-a}^{a} = k f(a), 
$$
we obtain (\ref{equ:ch2}). 

Note that $B_{F,k} = \E[-f'(X)]/(k(k-1))$ where $X$ is a random variable with distribution that corresponds to that of a maximum of $k$ independent samples from the cumulative distribution function $F$. Note also that if, in addition, $f$ is an even function, then (i) $B_{F,k} \geq 0$ and (ii) $B_{F,k}$ is increasing in $k$.

Third, for any cumulative distribution function $F$ with an even density function $f$, we have $F(-x) = 1-F(x)$ for all $x\in \reals$. In this case, we have the identity
\begin{equation}
\frac{\partial p_k(\vec{0})}{\partial x_1} = \int_0^\infty f(x)^2 (F(x)^{k-2} + (1-F(x))^{k-2}) dx.
\label{equ:gammafeven}
\end{equation}

\section{Proof of Lemma~\ref{prop:gamma}}
\label{sec:gamma}

The upper bound follows by noting that that $B_{F,k}$ in (\ref{equ:ch2}) is such that $B_{F,k} = \Omega(1/k^2)$. Hence, it follows that 
$$
\gamma_{F,k} = O(1).
$$

The lower bound follows by noting that for every cumulative distribution function $F$ such that there exists a constant $C > 0$ such that $f(x) \leq C$ for all $x\in \reals$,
$$
\frac{\partial p_k(\vec{0})}{\partial x_1} = \int_\reals f(x)^2 F(x)^{k-2}dx
\leq C \int_\reals f(x) F(x)^{k-2}dx
= C\frac{1}{k-1}.
$$
Hence, $\gamma_{F,k} \geq (1/C)(k-1)/k^3 = \Omega(1/k^2)$.


\section{Derivations of parameter $\gamma_{F,k}$}

We derive explicit expressions for parameter $\gamma_{F,k}$ for our example generalized Thurstone choice models introduced in Section~\ref{sec:defs}

Recall from (\ref{equ:gamma}) that we have that
$$
\gamma_{F,k} = \frac{1}{(k-1)k^3}\frac{1}{(\partial p_k(\vec{0})/\partial x_1)^2}
$$
where
$$
\frac{\partial p_k(\vec{0})}{\partial x_1} = \int_\reals f(x)^2 F(x)^{k-2}dx
$$


\paragraph{Gaussian distribution} A cumulative distribution function $F$ is said to have a type-3 domain of maximum attraction if the maximum of $r$ independent and identically distributed random variables with cumulative distribution function $F$ has as a limit a double-exponential cumulative distribution function:
$$
e^{-e^{-\frac{x-a_r}{b_r}}}
$$
where
$$
a_r = F^{-1}\left(1-\frac{1}{r}\right)
$$
and
$$
b_r = F^{-1}\left(1-\frac{1}{er}\right) - F^{-1}\left(1-\frac{1}{r}\right).
$$

It is a well known fact that any Gaussian cumulative distribution function has a type-$3$ domain of maximum attraction. Let $\Phi$ denote the cumulative distribution function of a standard normal random variable, and let $\phi$ denotes its density. 

Note that 
\begin{eqnarray*}
\int_\reals \phi(x) d\Phi(x)^r
&\sim & \frac{1}{\sqrt{2\pi}}\int_\reals e^{-\frac{x^2}{2}} d(e^{-e^{-\frac{x-a_r}{b_r}}})\\
&=& \frac{1}{\sqrt{2\pi}}\int_0^{\infty} e^{-\frac{1}{2}(a_r + b_r \log(1/z))^2} e^{-z}dz\\
&=& \frac{1}{\sqrt{2\pi}} e^{-\frac{1}{2}a_r^2} \int_0^\infty z^{a_rb_r} e^{-\frac{1}{2}b_r^2 \log(1/z)^2} e^{-z}dz\\
&\leq & \frac{1}{\sqrt{2\pi}} e^{-\frac{1}{2}a_r^2} \int_0^\infty z^{a_rb_r}  e^{-z}dz\\
&=& \frac{1}{\sqrt{2\pi}} e^{-\frac{1}{2}a_r^2} \Gamma(a_r b_r + 1).
\end{eqnarray*}

Now, note that
$$
a_r \sim \sqrt{2\log(r)} \hbox{ and } b_r = \Theta(1), \hbox{ for large } r.
$$

It is readily checked that $e^{-a_r^2/2} \sim 1/r$ and $\Gamma(a_r b_r + 1) = O(r^\epsilon)$ for every constant $\epsilon > 0$. Hence, we have that
$$
\int_\reals \phi(x) d\Phi(x)^r = O(1/r^{1-\epsilon})
$$
and thus, $\partial p_k(\vec{0})/\partial x_1 = O(1/k^{2-\epsilon})$. Hence,
$$
\gamma_{F,k} = \Omega(1/k^{2\epsilon}).
$$

\paragraph{Double-exponential distribution} Note that $f(x) = \frac{1}{\beta} e^{-\frac{x + \beta \gamma}{\beta}} F(x)$. Hence, we have

\begin{eqnarray*}
\frac{\partial p_k(\vec{0})}{\partial x_1} &=& \int_\reals f(x)^2 F(x)^{k-2}dx\\
&=& \frac{1}{\beta^2}\int_\reals e^{-2\frac{x+\beta\gamma}{\beta}} F(x)^k dx\\
&=& \frac{1}{\beta} \int_0^\infty z e^{-k z}dz\\
&=& \frac{1}{\beta k^2}.
\end{eqnarray*}

\paragraph{Laplace distribution} Let $\beta = \sigma / \sqrt{2}$. Note that 
$$
F(x) = 1-\frac{1}{2}e^{-x/\beta} \hbox{ and } f(x) = \frac{1}{2\beta} e^{-x/\beta}, \hbox{ for } x\in \reals_+.
$$
\begin{eqnarray*}
A &=& \int_0^\infty f(x)^2 F(x)^{k-2}dx\\
&=& \int_0^\infty \left(\frac{1}{2\beta}\right)^2 e^{-2x/\beta} \left(1-\frac{1}{2}e^{-x/\beta}\right)^{k-2} dx\\
&=& \frac{1}{2\beta} \int_{1/2}^1 2(1-z) z^{k-2} dz\\
&=& \frac{1}{\beta} \left(\frac{1}{k-1}\left(1-\frac{1}{2^{k-1}}\right) - \frac{1}{k}\left(1-\frac{1}{2^{k}}\right)\right)\\
&=& \frac{1}{\beta k(k-1)}\left(1-\frac{k}{2^{k-1}} + \frac{k-1}{2^k}\right)
\end{eqnarray*}

and

\begin{eqnarray*}
B &=& \int_0^\infty f(x)^2 (1-F(x))^{k-2}dx\\
&=& \int_0^\infty \left(\frac{1}{2\beta}\right)^2 e^{-2x/\beta} \frac{1}{2^{k-2}}e^{-(k-2)x/\beta} dx\\
&=& \frac{1}{\beta^2 2^k} \int_0^\infty e^{-kx/\beta}dx\\
&=& \frac{1}{\beta k 2^k}.
\end{eqnarray*}

Combining with (\ref{equ:gammafeven}), we obtain
$$
\frac{\partial p_k(\vec{0})}{\partial x_1} = A+B = \frac{1}{\beta k(k-1)}\left(1-\frac{1}{2^{k-1}}\right).
$$

\paragraph{Uniform distribution} Note that
\begin{eqnarray*}
\frac{\partial p_k(\vec{0})}{\partial x_1} &=& \int_\reals f(x)^2 F(x)^{k-2}dx\\
&=& \frac{1}{(2a)^2}\int_{-a}^a \left(\frac{x+a}{2a}\right)^{k-2}dx\\
&=& \frac{1}{2a} \int_0^1 z^{k-2}dz\\
&=& \frac{1}{2a (k-1)}.
\end{eqnarray*}



