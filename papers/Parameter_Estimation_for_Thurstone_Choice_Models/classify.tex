We now consider a Thurstone choice model \GT\ where the strength parameter vector $\theta$ takes value in $\Theta = \{-b,b\}^n$, for a parameter $b > 0$. Here each individual strength parameter takes either a low or a high value. We say that each item is either of a low or a high class. We consider a binary classification problem, where the goal is to correctly classify all items with a prescribed probability of classification error. Let $N_1$ be the set of high class items and $N_2$ be the set of low class items. We shall consider the case when the total number of items is even and the number of high class items is equal to the number of low class items. 

We consider a simple classification algorithm that uses \emph{point scores} defined as follows: each item is associated with a point score equal to the number of comparison sets in which the given item is the chosen item. The algorithm outputs a classification of items with $\widehat N_1$ and $\widehat N_2$ denoting the sets of items classified to be high class or low class, respectively. The algorithm follows the following three steps: (a) for each item compute its point score, (b) sort the items in decreasing order of point scores, and (c) let $\widehat{N}_1$ contain $n/2$ items with highest point scores and $\widehat{N}_2$ contain remaining items. We refer to this algorithm as a \emph{point score ranking method}.

\begin{theorem} Suppose that $b \leq 4/(k^2 \partial p_k(\vec{0})/\partial x_1)$ and
\begin{equation}
b \max_{\vec{x}\in [-2b,2b]^{k-1}} \|\nabla^2 p_k (\vec{x}) \|_2 \le \frac{\partial p_k ({\bm 0})}{\partial x_1}.
\label{equ:bcond}
\end{equation}
Then, for every $\delta \in (0,1]$, if
\begin{equation}
m \ge 64 \frac{1}{b^2}\, \left(1-\frac{1}{k}\right)\gamma_{F,k}\, n(\log(n) + \log(1/\delta))
\label{equ:classifyub}
\end{equation}
then, the point score ranking method correctly classifies all items with probability at least $1-\delta$.
\label{thm:clustering-example}
\end{theorem}

The proof of Theorem~\ref{thm:clustering-example} is provided in Appendix~\ref{sec:clustering-example}. 

The sufficient condition in (\ref{equ:classifyub}) for the point score ranking method to correctly classify all the items with probability at least $1-\delta$ is shown to be necessary up to a constant-factor for any classification algorithm, which is given in the following theorem. 

\begin{theorem} Suppose that $b \le 1/(6 k^2\partial p_k(\vec{0})/\partial x_1)$ and that condition (\ref{equ:bcond}) holds. Then, for every even $n \geq 16$ and $\delta \in (0,1/4]$, for any algorithm to correctly classify all the items with probability at least $1-\delta$, it is necessary that the following condition holds
$$
m \geq \frac{1}{62}\frac{1}{b^2}\, \left(1-\frac{1}{k}\right)\gamma_{F,k}\, n(\log(n) + \log(1/\delta)).
$$
\label{thm:clustering-low}
\end{theorem}

The proof of Theorem~\ref{thm:clustering-low} is given in Appendix~\ref{sec:clustering-low}. In the proof, we use the statistical difference between the case when all the items are correctly classified and the case that an item is incorrectly classified. This proof strategy is motivated by that in \cite{yun2014community} where it was used to analyze the classification error of the stochastic block model.
