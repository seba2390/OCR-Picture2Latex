
Let $N=\{1,2,\dots,n\}$ be a set of two or more items. The input data consists of a sequence of one or more observations $(S_1,y_1)$, $(S_2,y_2)$, $\ldots$, $(S_m,y_m)$, where each observation $t$ consists of (a) a \emph{comparison set} $S_t\subseteq N$ and (b) a \emph{choice} of an item $y_t$ from $S_t$. The case of \emph{pair comparisons} is accommodated as a special case when each comparison set consists of two items. Let $w_{i,S}$ denote the number of observations for which the comparison set is $S$ and the chosen item is $i\in S$. With a slight abuse of notation, for pair comparisons, let $w_{i,j}$ be the number of observations for which the comparison set is $\{i,j\}$ and the chosen item is $i$.

A Thurstone choice model, denoted as \GT, is defined by a cumulative distribution function $F$ of a zero mean random variable and parameter vector $\theta = (\theta_1,\theta_2,\ldots,\theta_n)$ that takes value in $\Theta\subseteq \reals^n$. We can interpret $\theta_i$ as the strength of item $i\in N$. The cumulative distribution function $F$ is assumed to have a density function, denoted by $f$.

According to Thurstone choice model \GT, for any given sequence of comparison sets, choices are independent random variables according to the following distribution: conditional on that the comparison set is $S$, with $k$ denoting the cardinality of $S$, the distribution of choice is 
\begin{equation}
p_{i,S}(\theta) = p_{k}(\vec{x}_{i,S}(\theta)), \hbox{ for } i\in S
\label{equ:gtmodel}
\end{equation} 
where $\vec{x}_{i,S}(\theta)$ is a vector in $\reals^{k-1}$ with elements $\theta_i-\theta_j$, for $j\in S\setminus\{i\}$, and
\begin{equation}
p_k(\vec{x}) = \int_{\reals} \left(\prod_{v=1}^{k-1} F(x_v+z)\right) f(z)dz, \hbox{ for } \vec{x} \in \reals^{k-1}.
\label{equ:pkfunc}
\end{equation}

With a slight abuse of notation, for the case of pair comparisons, we let $p_{i,j}(\theta)$ denote the probability that item $i$ is chosen from the comparison set $\{i,j\}$. In this case, we have 
$$
p_{i,j}(\theta)=p_2(\theta_i-\theta_j)
$$ 
where
$$
p_2(x) = \int_{\reals}F(x+z) f(z) dz, \hbox{ for } x\in \reals.
$$

A Thurstone choice model \GT\ corresponds to the following \emph{probabilistic generative model} of choice, also referred to as a \emph{random utility model}. The items of each comparison set are associated with latent \emph{performance random variables}, which are independent across different items and different comparison sets. For any comparison set $S$ and item $i\in S$, the performance random variable $X_i$ is equal to the sum of a strength parameter $\theta_i$ and a noise random variable with distribution $F$. The given probabilistic generative model assumes that for any given comparison set $S$, the chosen item is the one with the largest performance. Hence, the distribution of choice is $p_{i,S}(\theta) = \Pr[X_i \geq \max_{j\in S}X_j]$ for $i\in S$, which can be expressed as asserted in (\ref{equ:gtmodel}).

It is noteworthy that under a Thurstone choice model, the probability distribution of choice depends only on pairwise differences of the strength parameters. This implies that the probability distribution of choice is shift-invariant with respect to the parameter vector. In order to allow for identifiability of the parameter vector, we assume that $\theta$ is such that $\sum_{i=1}^n \theta_i =0$.

We refer to several examples of Thurstone choice models as follows: (i) \emph{Gaussian distribution}  : $f(x) = 1/(\sqrt{2\pi}\sigma)\, \exp(-x^{2/(2\sigma^2}))$ with variance $\sigma^2$; (ii) \emph{Double-exponential distribution}: $F(x) = \exp(-\exp(-(x+\beta\gamma)/\beta))$ with parameter $\beta > 0$ and $\gamma$ denoting the Euler-Mascheroni constant, which has variance $\sigma^2 = \pi^2 \beta^2/6$; (iii) \emph{Laplace distribution}: $F(x) = 1/2\,\exp(x/\beta)$, for $x<0$, and $F(x) = 1-1/2\, \exp(-x/\beta)$, for $x \geq 0$, with parameter $\beta$, which has variance $\sigma^2 = 2 \beta^2$; and (iv) \emph{Uniform distribution}: $f(x) = 1/(2a)$ for $x\in [-a,a]$, which has variance $\sigma^2 = a^2/3$.

For the general case of comparison sets of two or more items, the distribution of choice admits an explicit form only for some special cases. For example, when noise random variables are according to a double-exponential distribution, we have
$$
p_k(\vec{x}) = \frac{1}{1+\sum_{i=1}^{k-1}e^{-x_i/\beta}}, \hbox{ for } \vec{x} \in \reals^{k-1}.
$$
This amounts to the choice probabilities $p_{i,S}(\theta) = \exp(\theta_i/\beta)/\sum_{v\in S} \exp(\theta_v/\beta)$, for $i \in S$ and $S\subseteq N$, which under suitable re-parametrisation corresponds to the well-known Luce choice model. In particular, for pair comparisons, we have $p_2(x) = 1/(1+\exp(-x/\beta))$, which under suitable re-parametrisation corresponds to the well-known Bradley-Terry model. For pair comparisons, the choice probabilities admit an explicit form also for some other Thurstone choice models; for example, when noise has a Gaussian distribution, we have $p_2(x) = \Phi(x/(2\sigma))$ where $\Phi$ is the cumulative distribution function of a standard normal random variable. 

\paragraph{Maximum Pseudo Likelihood Estimation} We consider parameter estimators that are defined as maximizers of a pseudo log-likelihood function $\widetilde{\ell}: \Theta \rightarrow \reals$. We refer to the parameter estimator $\widehat{\theta}\in \arg\max_{\theta \in \Theta}\widetilde{\ell}(\theta)$ as a maximum pseudo likelihood estimator.  

We devote a special attention to maximum likelihood estimator, defined as a maximizer of the log-likelihood function, which for a Thurstone choice model is given by  
\begin{equation}
\ell(\theta) = \sum_{t=1}^m \log(p_{y_t,S_t}(\theta)).
\label{equ:loglik0}
\end{equation}
The log-likelihood function can be written as follows
\begin{equation}
\ell(\theta) = \sum_{S\subseteq N}\sum_{i\in S} w_{i,S}\log(p_{|S|}(\vec{x}_{i,S}(\theta))) + \hbox{const}
\label{equ:loglik}
\end{equation}
where recall $w_{i,S}$ is the number of observations for which the comparison set is $S$ and $i$ is the choice from $S$. In particular, for pair comparisons, we have
\begin{equation}
\ell(\theta) = \sum_{i=1}^n\sum_{j=1}^n w_{i,j} \log \left( p_2(\theta_i - \theta_j) \right)  + \hbox{const}.
\label{eq:MLtheta}
\end{equation}  

Evaluating the value of the log-likelihood function in (\ref{equ:loglik}) for given parameter vector requires evaluating a sum that in the worst-case consists of exponentially many elements in $n$ (all possible combinations of two or more elements from the ground set of $n$ elements). On the other hand, for pair comparisons, the log-likelihood function in (\ref{eq:MLtheta}) is a sum of at most $n^2$ elements; thus, polynomially many elements in $n$. A common approach to reduce computational complexity is to use the so-called \emph{rank breaking}, which amounts to deducing pair comparisons from any given comparison set of two or more items, and assuming that these pair comparisons are independent (if this is not the case). Using these pair comparisons, one then defines a pseudo log-likelihood function as the log-likelihood function under the assumption that the deduced pair comparisons are independent.   

We shall consider two natural rank-breaking methods. The first rank-breaking method deduces $k-1$ pair comparisons from each comparison set of $k$ items, by taking all pairs that consist of the chosen item and each other item in the given comparison set. The pseudo log-likelihood function in this case is given by
\begin{equation}
\ell_{k-1}(\theta) = \sum_{t=1}^m \sum_{v\in S_t\setminus \{y_t\}} \log(p_{y_t,v}(\theta)).
\label{equ:pseudolikk}
\end{equation}
The second rank-breaking method that we consider deduces $1$ pair comparison from each comparison set of $k$ items, by taking a pair that consists of the chosen item and a randomly picked item from the remaining set of items in the given comparison set. The pseudo log-likelihood function in this case is given by
\begin{equation}
\ell_1(\theta) = \sum_{t=1}^m \log(p_{y_t,z_t}(\theta)).
\label{equ:pseudolik1}
\end{equation}

The first rank-breaking method uses maximum amount of information that is contained in a comparison; by observing choice of one item from a comparison set of $k$ items, we can indeed deduce at most $k-1$ pair comparisons (between the chosen item and each other item in the given comparison set). In general, these pair comparisons are not mutually independent. The second rank-breaking method is conservative in deducing only one pair comparison from each comparison set of two or more items.  

\paragraph{Parameter Estimation Accuracy} We study the accuracy of a maximum pseudo log-likelihood estimator $\widehat \theta$ of the true parameter vector $\theta^\star$ by using the \emph{mean squared error} defined as follows:
\begin{equation}
\mse(\widehat \theta, \theta^\star) = \frac{1}{n}\|\widehat \theta - \theta^\star\|_2^2.
\label{equ:msedef}
\end{equation}

We also consider the probability of classification error for the case when the strength parameters belong to one of two classes and the goal is to correctly classify each item.  

\subsection{Eigenvalues, Adjacency, and Laplacian Matrices}

Here we review some basic definitions that are used throughout the paper. We denote eigenvalues of a matrix $\vec{A}\in \reals^{n\times n}$ as $\lambda_1(\vec{A}), \lambda_2(\vec{A}), \ldots, \lambda_n(\vec{A})$. By convention, we assume that $\lambda_1(\vec{A})\leq \lambda_2(\vec{A})\leq \cdots \leq \lambda_n(\vec{A})$. The \emph{spectral norm} $||\vec{A}||_2$ of matrix $\vec{A}\in \reals^{n\times n}$ is defined by $||\vec{A}||_2 = \sqrt{\lambda_n(\vec{A}^\top \vec{A})}$. The spectral norm of $\vec{A}$ is induced by the Euclidean vector norm as follows $||\vec{A}||_2 = \max\{||\vec{A} \vec{x}||_2: \vec{x}\in \reals^n, ||\vec{x}||=1\}$. If $\vec{A}\in \reals^{n\times n}$ is a real symmetric matrix, then $||\vec{A}||_2 = \lambda_n(\vec{A})$. 

For any \emph{weighted-adjacency} matrix $\vec{A}\in \reals^{n\times n}_+$, we consider a \emph{Laplacian} matrix $L_{\vec{A}}$ defined by
$$
L_{\vec{A}} = \mathrm{diag}(\vec{A}\vec{1}) - \vec{A}
$$ 
where for any given vector $\vec{a}$, $\mathrm{diag}(\vec{a})$ denotes the diagonal matrix with diagonal $\vec{a}$. 

For any weighted-adjacency matrix $\vec{A}\in \reals^{n\times n}_+$, we refer to $\lambda_2(L_{\vec{A}})$ as the \emph{Fiedler value} of $\vec{A}$ (\cite{F73,F89}). The Fielder eigenvalue of a weighted-adjacency matrix quantifies its algebraic connectivity. A weighted-adjacency matrix $\vec{A}$ corresponds to a connected graph if and only if it has strictly positive Fielder value, i.e., $\lambda_2(L_{\vec{A}}) > 0$.

For any given observations and given \emph{weight function} $w:\{1,2,\ldots,n\}\rightarrow \reals_+$, we define the weighted-adjacency matrix $\vec{M}_w$ as follows. Let $m_{i,j}(k)$ be the number of comparisons of cardinality $k$ that contain the pair of items $\{i,j\}$. Then, we define $\vec{M}_w$ to be the matrix in $\reals_+^{n\times n}$ with zero diagonal elements and other elements given by 
\begin{equation}
m_{i,j} = \frac{n}{m}\sum_{k\geq 2} w(k) m_{i,j}(k).
\label{equ:weightmatrix}
\end{equation}
With a slight abuse of notation, let $\vec{M}_a$ be the weighted-adjacency matrix defined for the weight function that takes constant value $a > 0$, and let $\vec{M}$ be written in lieu of $\vec{M}_1$. 

If all comparison sets have identical cardinalities, then each element of the weighted-adjacency matrix is equal to the number of comparisons that contain the corresponding pair of items up to a multiplicative factor. The factor $n/m$ can be interpreted as a normalization with the mean number of comparison sets per item. This normalization is admitted so that for the canonical case of pair comparisons when each pair is compared the same number of times, $\lambda_2(L_{\vec{M}_a})$ is a constant independent of the number of observations $m$ and the number of items asymptotically for large $n$. Indeed, in this case, $\lambda_2(L_{\vec{M}_a}) = \cdots = \lambda_n(L_{\vec{M}_a}) = (1-1/n)a$, which is equal to constant $a$, asymptotically for large $n$.

We say that comparison sets are \emph{unbiased} if for any given cardinality, each set of the given cardinality occurs the same number of times. In particular, for pair comparisons, this means that each distinct pair is compared the same number of times. For any unbiased comparison sets, the weighted-adjacency matrix can be expressed as follows. Let $\mu(k)$ be the fraction of comparison sets of cardinality $k$. Then, for every integer $k\geq 2$ and pair of items $\{i,j\}$, $m_{i,j}(k) = \left(\binom{n-2}{k-2}/\binom{n}{k}\right) \mu(k) m = \left(k(k-1)/[n(n-1)]\right) \mu(k) m$. Hence, for every pair of items, we have
\begin{equation}
m_{i,j} = \frac{1}{n-1}\sum_{k\geq 2} w(k) k(k-1)\mu(k).
\label{equ:unbiased}
\end{equation}
It follows that for any unbiased comparison sets, we have
\begin{equation}
\lambda_2(L_{\vec{M}_w}) = \left(1-\frac{1}{n}\right)\sum_{k\geq 2} w(k) k(k-1)\mu(k)
\label{equ:lambda2L}
\end{equation}
which is a constant independent of $n$, asymptotically for large $n$. 

We shall also consider comparison sets that are assumed to be an independent random sequence according to a given distribution. Specifically, we shall consider the case where all comparison sets are of the same cardinality, and are independent samples according to uniform random sampling without replacement from the set of all items. We denote with $\overline{\vec{M}}_w$ the \emph{expected weighted-adjacency} matrix, where the expectation is with respect to the distribution of the sequence of comparison sets. We say that comparison sets are \emph{a priori unbiased} if all non-diagonal elements of $\overline{\vec{M}}_w$ are equal.  

\subsection{A Key Lemma and Probability Tail Bounds}

All upper bounds for the mean squared error of a maximum pseudo log-likelihood estimator in this paper are established by using the following key lemma.

\begin{lemma} Suppose that $g:\reals^n \rightarrow \reals$ satisfies (i) $\nabla^2 g(\vec{\theta})\vec{1} = \vec{0}$ and (ii) $\lambda_2(\nabla^2g(\vec{\theta}))>0$ for all $\vec{\theta} \in \Theta$, where $\Theta = \{\vec{\theta}\in [-b,b]^n: \vec{\theta}^\top \vec{1} = 0\}$ for $b > 0$. Let $\vec{\theta}^\star$ be an arbitrary vector in $\Theta$ and $\widehat{\vec{\theta}}\in \arg\min_{\vec{\theta}\in \Theta} g(\vec{\theta})$. Then, we have
$$
\|\widehat{\vec{\theta}} -\vec{\theta}^\star \|_2 \le 
\frac{2\left\|\nabla g(\vec{\theta}^\star) \right\|_2}{\min_{\vec{\theta}\in \Theta}\lambda_2(\nabla^2 g(\vec{\theta}))}.
$$
\label{lem:mle-taylor}
\end{lemma}

We shall apply this lemma to the case where $g$ is a negative pseudo log-likelihood function, $\widehat{\theta}$ is a maximizer of the pseudo log-likelihood function, and $\theta^\star$ is the true parameter vector. We upper bound the mean squared error by the following two steps: 
\begin{itemize}
\item[{\bf S1}] find $\alpha > 0$ such that $\left\|\nabla g(\vec{\theta}^\star) \right\|_2 \leq \alpha$, and 
\item[{\bf S2}] find $\beta > 0$ such that $\min_{\vec{\theta}\in \Theta}\lambda_2(\nabla^2 g(\vec{\theta})) \geq \beta$
\end{itemize}
which imply the upper bound $\|\widehat{\vec{\theta}} -\vec{\theta}^\star \|_2 \leq 2\alpha / \beta$. 

All our proofs of the mean squared estimation error upper bounds follow the above two-step procedure, including the proof of Theorem~\ref{thm:mle} in Section~\ref{sec:pairs} and other proofs provided in Appendix. 

In step {\bf S1}, $\nabla g(\vec{\theta}^\star)$ is a sum of random vectors. We will make use of the following vector version of Azuma-Hoeffding bound (Theorem~1.8 in~\cite{H03}) for a sum of random vectors.

\begin{lemma}[vector Azuma-Hoeffding bound] Suppose that $S_m = \sum_{t=1}^m X_t$ is a martingale where $X_1, X_2, \ldots, X_m$ are random variables that take values in $\reals^n$ and are such that $\E[X_t] = \vec{0}$ and $\|X_t\|_2 \leq \sigma$ for all $t\in \{1,2,\ldots,m\}$, for $\sigma > 0$. Then, for every $x > 0$,
$$
\Pr\left[\left\|S_m\right\|_2 \geq x\right] \leq 2e^2 e^{-\frac{x^2}{2m \sigma^2}}.
$$
\label{prop:azuma-hoeffding}
\end{lemma}

In step {\bf S2}, we need to find a lower bound for the second-smallest eigenvalue of the Hessian matrix $\nabla^2 g(\vec{\theta})$ for all $\theta \in \Theta$. For pair comparisons according to a Thurstone choice model or comparisons of two or more items according to the Luce choice model, $\nabla^2 g(\vec{\theta})$ is determined by comparison sets and does not depend on the choices. We can find $\beta$ from a Laplacian matrix when the comparison sets are given. In other cases, $\nabla^2 g(\vec{\theta})$ is a sum of random matrices. We will make use the following matrix version of Chernoff's bound along with properties of eigenvalues of a Laplacian matrix (which are given in Appendix~\ref{sec:back}).

\begin{lemma}[matrix Chernoff bound] Let $S_m = \sum_{t=1}^m X_t$ where $X_1, X_2, \ldots, X_m$ are random independent real symmetric matrices in $\reals^{n\times n}$ such that $\lambda_1 (X_t)\ge 0$ and, $\| X_t \|_2 \le \sigma$ for $t \in \{1,2,\ldots,m\}$, for $\sigma > 0$. Then, for $\epsilon \in [0,1)$,
$$
\Pr\left[ \lambda_{1}(S_m) \le (1-\epsilon) \lambda_1(\E[S_m]) \right] \leq n e^{-\frac{\epsilon^2\lambda_1(\E[S_m])}{2\sigma}}.
$$
\label{cor:matrix}
\end{lemma}
