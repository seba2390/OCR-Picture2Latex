\section{Introduction}
Generative Adversarial Networks (GANs \cite{goodfellow2014generative}) are capable of reproducing certain aspects of a given training set. GANs are artificial neural networks that can be trained to generate fake samples based on real examples. Past successes include the generation of fake celebrity faces~\cite{karras:iclr2018} and fingerprints~\cite{bontrager2017deepmasterprint}. In the domain of games, GANs have generated levels for Mario and others~\cite{volz:gecco2018,gutierrez2020zeldagan,giacomello:cog19,torrado:cog20}. 

In this paper, GANs are used to generate Mega Man levels based on levels in the original game. Data from the Video Game Level Corpus (VGLC \cite{summerville:vglc2016}) is used to train GANs. Volz et al.~\cite{volz:gecco2018} generated Mario levels by placing individual segments left-to-right. In contrast to Mario, Mega Man levels have a snaking pattern of horizontal, vertical, and corner segments. 
% \todo{explain what the problem/challenge is before explaining our solution. Move up some of the text currently in the third paragraph.}
Therefore, different GANs are used for different segment types, resulting in levels with better flow and organization, and a more human-like design. 

Levels were optimized using latent variable evolution (LVE~\cite{bontrager2018deep}): one real-valued vector consisting of the concatenation of multiple latent vectors was used to generate several segments. The vector also contained information on the relative placement of each segment. Levels were evolved using multiple objectives by Non-Dominated Sorting Genetic Algorithm II (NSGA-II~\cite{deb:tec02}), the most relevant being
solution path length as determined by A* Search. 

%Levels were assessed using A* Search to determine the solution path length. 



% \todo{This should be talked about in the second paragraph, right after saying that GANs have been used in Mario and others. Will need to rephrase to remove repetition.}  
Our new approach, MultiGAN, trains distinct GANs on different portions of the training data, and queries the appropriate GAN for each segment type as needed.
MultiGAN is compared with the standard approach of training one GAN on all data: OneGAN.
Although segments produced by GANs make no assumptions regarding neighboring segments,
LVE encourages sensible transitions between segments. However, MultiGAN more easily selects appropriate segments, resulting in significantly longer solution paths, and levels that flow in a more human-like fashion. In contrast, OneGAN levels are chaotic and have shorter solutions, since irregular boundaries and undesirable shortcuts emerge despite evolution.

%there are more irregular boundaries between segments.


%Two main approaches were used in this investigation that are distinct from the aforementioned previous works: 
%1) the previous approach, OneGAN, in which one GAN trained on all data generates all segments, and
%2) a new approach, MultiGAN, in which different GANs are trained on different partitions of the training data, and then queried separately as segments of each type are needed.
%MultiGAN generates longer levels with better structure. Using multiple GANs to generate different level segments allowed for the levels to flow in a more human-like fashion and create longer levels after evolution. In contrast, when using only one GAN, levels are chaotic and short, since there is no cohesion in how segments are generated.

%%% This is repetition
%This paper builds on a methods that have been proven effective for generating video game content using GANs. OneGAN levels are still enjoyable Mega Man levels, but levels generated with the new MultiGAN approach are significantly longer because there are fewer irregular boundaries between segments.
% \todo{MOVE: mention VGLC in an earlier paragraph: paragraph 2, after mentioning levels from original game}  

% \todo[inline]{To wrap up section: Say a human subject study was conducted. Say that although human players do not find either type of level significantly more enjoyable or challenging, etc, analysis of their responses and the specific levels they played indicates why MultiGAN sometimes generates more human-like levels, and also when it comes short. Be vague here since this will need to change a lot after we figure out more details.}
A human subject study was also conducted
that indicates MultiGAN levels are significantly more fun and human-like in their design than OneGAN levels, confirming our analysis of the levels, and indicating that MultiGAN is a promising approach for generating better levels for classic platforming games.


%% This is all repetition
%After training, Latent Variable Evolution ~\cite{bontrager2017deepmasterprint} is used to evolve the latent-variable inputs in order to produce meaningful levels. Ultimately, the MultiGAN method is capable of generating longer levels and may be a more effective means of content generation when dealing with data of differing types.

\section{Related Work}
\label{sec:related}


Procedural Content Generation (PCG~\cite{togelius2011procedural}) is an automated way of creating content for video games. PCG via Machine Learning (PCGML~\cite{Liu_2020}) is a way of combining machine learning with PCG.

Generative Adversarial Networks are an increasingly popular PCGML method for video game level generation. After the GAN is properly trained, randomly sampling vectors from the induced latent space generally produces outputs that could pass as real segments of levels. However, some segments are more convincing than others, or have other desirable qualities (e.g.\ enemy count, solution length, tile distribution), so it makes sense to search the latent space for these desirable segments via methods such as evolution.

%%% Can leave this out
%In another paper~\cite{bontrager2018deep}, Bontrager et al.\ present an interactive evolutionary system in which users can evolve the latent vectors for a GAN trained on different classes of objects (e.g.\ faces or shoes). 
%and users were able to guide evolution towards images that closely resembled given target images. 

The first latent variable evolution (LVE) approach was introduced by Bontrager et al.~\cite{bontrager2017deepmasterprint}. In their work a GAN was trained on a set of real fingerprint images and then evolutionary search was used to find latent vectors that matched subjects in the data set. 
Because the GAN is trained on a specific target domain, most generated phenotypes resemble valid domain artifacts.
The first LVE approach to generating video game levels was applied to Mario~\cite{volz:gecco2018}. Work quickly followed in other domains.

%%% This got repetitive
%Volz et al. presented a simple left-to-right generation method was used in order to use several latent vectors to create a seamless, complete level. For the exploration of PCGML presented by Gutierrez et al., a graph grammar was used in conjunction with a GAN, where the GAN is responsible for room layout and the grammar is responsible for dungeon layout.

Giacomello et al.~\cite{giacomello:cog19} used a GAN to generate 3D levels for the First-Person Shooter Doom. 
Gutierrez and Schrum \cite{gutierrez2020zeldagan} used a Graph Grammar in tandem with a GAN to generate dungeons for The Legend of Zelda. Work in the GVG-AI \cite{GVGAI:TCIAIG2016} variant of Zelda was also done by Torrado et al.~\cite{torrado:cog20} using conditional GANs. This work used bootstrapping to help in the generation of more solvable levels. A similar bootstrapping approach was also used by Park et al.~\cite{park:cog19} in an educational puzzle-solving game.
Additional work has also been done in a broader collection of GVG-AI games by Kumaran et al.~\cite{kumaran:aiide2020}, who used one branched generator to create levels for multiple games derived from a shared latent space.

%Kumaran et al.~\cite{kumaran:aiide2020} uses the same latent space across several different video game levels. 

%Park et al.~\cite{park:cog19} used GANs to generate levels for educational games, and Torrado et al. ~\cite{torrado:cog20} used Conditional GANs for video game level generation. Both Torrade et al. ~\cite{torrado:cog20} and Park et al.~\cite{park:cog19} use bootstapping to make mroe solvable levels. 

%%% These papers don't fit the narrative flow very well, and aren't as relevant. Drop
%Work to capture local and global patterns in PCGML was done by Volz et al.~\cite{volz:cog2020}. Awiszus et al.~\cite{awiszus:aiide2020} presents a way to generate coherent level style generation using only one example, and Hald et al.~\cite{hald:fdg20} used GANs to generate puzzle games. 

Additional work has also been done in the original Mario domain.
In particular, Fontaine et al.~\cite{fontaine2020illuminating} and Schrum et al.~\cite{schrum:gecco2020cppn2gan} have both used the quality diversity algorithm MAP-Elites \cite{mouret:arxiv15} to find a diversity of interesting Mario levels. The approach by Schrum et al.\ specifically used Compositional Pattern Producing Networks \cite{stanley:gpem2007} with GANs to make levels with better cohesion and connectivity. This approach was applied to Zelda as well as Mario. Schrum et al.~\cite{schrum2020interactive} also combined GANs with interactive evolution in these domains to search the space of levels according to user preferences.

%Other works in Mario include the presentation of Illuminating Mario Scenes in the Latent Space of a Generative Adversarial Network by Fontaine et al. ~\cite{fontaine2020illuminating}. 

%In most recent work, the CPPN2GAN method ~\cite{schrum:gecco2020cppn2gan} was used as a means of combining Compositional Pattern Producing Networks with GANs in order to generate lare-scale patterns in video game levels. Additionally, Schrum et al. ~\cite{schrum2020interactive} use interactive evolution to manipulate the latent variable in the GAN for human-driven evolution. 

Previous work with GANs in Mario \cite{volz:gecco2018,fontaine2020illuminating,schrum:gecco2020cppn2gan,schrum2020interactive} all learn to generate one level segment at a time, before placing the adjacent segments left-to-right. Mega Man levels are more complicated in that they have a snaking-pattern that can move right, up, down, and even to the left. The only other paper that has addressed this challenge is recent work by Sarkar and Cooper \cite{sarkar:fdg2020} that uses Variational Autoencoders (VAEs). This method was applied to horizontal Mario levels, vertical Kid Icarus levels, and snaking Mega Man levels. Although their results seem promising, they often still have problems with barriers between segments. In their work, A* paths through the training levels were part of training data, and proposed paths are actually part of the VAE output. However, these paths are not always valid, and example levels shown in the paper are not always traversable. Therefore, in our work, levels are specifically optimized to maximize the resulting A* path length, to assure that the Mega Man levels are actually traversable.


%The most similar previous approach using GANs to generate Mario levels~\cite{volz:gecco2018} involved generating segments with the GAN and placing each one directly after the next in left-to-right generation. Similarly, the training data was gathered by sliding a segment-sized window across the original level from left to right. This paper outlines a more complicated approach, since Mega Man levels have a snaking-pattern that can move up, down, and even back to the left. 


\begin{table}[t!]
\caption{\label{tab:tiles}Tile Types Used in Mega Man.}
{\small Tile types come from VGLC, though additional types were added based on observations of the actual game. The original VGLC did not include enemies, level orbs, or water. The ``In VGLC'' column indicates whether the tile was originally represented in VGLC. ``Training'' indicates whether the tile was used in GAN training sets. ``Char'' is the original character code representation in VGLC (or a made up code for tiles not in VGLC), and ``Int'' is a numeric code used in JSON representations of the training data. In VGLC, one tile was associated with a Cannon obstacle. That tile maps to Int 6, but was deemed unnecessary and becomes a solid block in generated levels. Additionally, although no specific enemy was used for training, a single general enemy type with code 11 was used as a placeholder for an enemy, and an algorithm later specified the type based on location.}

\centering
\begin{tabular}{|c|c|c|c|c|c|}
\hline
Tile type & In VGLC & Training & Char& Int & Game \\ 
\hline % Images needed to be vertically centered within row
Air & Yes & Yes & \texttt - & 0 & \raisebox{-.275\height}{\includegraphics[width=.05\columnwidth]{Tile-0.PNG}} \\
\hline
Solid & Yes & Yes & \texttt \# & 1, 6 & \raisebox{-.275\height}{\includegraphics[width=.05\columnwidth]{Tile-1.PNG}} \\
\hline
Ladder & Yes & Yes &\texttt | & 2 & \raisebox{-.275\height}{\includegraphics[width=.05\columnwidth]{Tile-2.PNG}} \\
\hline
Hazard & Yes & Yes &\texttt H & 3 & \raisebox{-.275\height}{\includegraphics[width=.05\columnwidth]{Tile-3.PNG}} \\
\hline
Breakable & Yes & Yes  &\texttt B & 4 & \raisebox{-.275\height}{\includegraphics[width=.05\columnwidth]{Tile-4.PNG}} \\
\hline
Moving  Platform & Yes & Yes &\texttt M & 5 & \raisebox{-.275\height}{\includegraphics[width=.05\columnwidth]{Tile-5.PNG}} \\
\hline
Orb & No & No &\texttt Z & 7 & \raisebox{-.275\height}{\includegraphics[width=.05\columnwidth]{Tile-7.PNG}} \\
\hline
Player & Yes & No &\texttt P & 8 & \raisebox{-.275\height}{\includegraphics[width=.05\columnwidth]{Tile-8.PNG}} \\
\hline
Null & Yes & Yes &\texttt @ & 9 & \raisebox{-.275\height}{\includegraphics[width=.05\columnwidth]{Tile-9.PNG}} \\
\hline
Water & No & Yes &\texttt W & 10 & \raisebox{-.275\height}{\includegraphics[width=.05\columnwidth]{Tile-10.PNG}} \\
\hline
Generic Enemy & No & Yes &\texttt Varies & 11 & N/A \\
\hline
Ground Enemy & No & No &\texttt G & 11 & \raisebox{-.275\height}{\includegraphics[width=.05\columnwidth]{Tile-11.PNG}} \\
\hline
Wall Enemy & No & No &\texttt W & 12 & \raisebox{-.275\height}{\includegraphics[width=.05\columnwidth]{Tile-12.PNG}} \\
\hline
Flying Enemy & No & No &\texttt F & 13 & \raisebox{-.275\height}{\includegraphics[width=.05\columnwidth]{Tile-13.PNG}} \\
\hline

\end{tabular}
\end{table}


\section{Mega Man}

%% Not relevant
%In the original game, the player can choose from most levels to play in any order. Once those levels are beaten, the player goes through a series of harder levels leading to the final boss. 

Mega Man (Rockman in Japan), was released in 1987 on the Nintendo Entertainment System (NES). Gameplay involves jumping puzzles, killing enemies, and a boss at the end of each level. Mega Man's success led to numerous sequels on the NES and other systems, most with the same graphics aesthetic and game mechanics.

%Within those levels, a multitude of challenging jumping puzzles in which deadly enemies make it difficult for the player to make it to the end. To defend them self and reach the end, the player uses various special weapons that can freeze enemies, smash breakable blocks, create floating platforms, and more. When the player reaches a set of metal doors, they are led through a hallway until they are locked in a simple square room with a boss unique to the level.

% FOR DISCUSSION AND FUTURE WORK: With Mega Man Maker, we are no longer limited to the original game's levels, but we are able to use the vast player-created games for data.
The Video Game Level Corpus (VGLC \cite{summerville:vglc2016}) contains data from numerous games, including Mega Man. VGLC is the source of training data for 
much of the research in Section \ref{sec:related} \cite{volz:gecco2018,gutierrez2020zeldagan,giacomello:cog19,sarkar:fdg2020,schrum:gecco2020cppn2gan,schrum2020interactive}. The levels are represented as text files, where each character represents a different tile type from the level, as seen in %, e.g.\ ground, enemy, air, etc.\ 
Table~\ref{tab:tiles}. The original VGLC data is slightly modified and enhanced as described in the table caption. Additionally, a level orb is added to mark the end of each level (bosses not included).
The snaking pattern of some levels presents an interesting challenge for level generation. 

%%% Wasn't sure what this meant. Seemed like filler.
%Additionally, the complexity of the game rules and versatility of the character presents interesting problems for making the levels difficult.

Mega Man Maker\footnote{\url{https://megamanmaker.com/}} is a fan-made game for building and playing user-created levels. The game includes content from each Mega Man game, including a platform-gun that allows the player to traverse otherwise difficult jumping puzzles with more ease. Mega Man Maker is used to visualize and play levels generated by the GANs.

%\todo[inline]{Still seems weird in Table 1 that the Training column value for Int 11 is No. This implies that no enemies were generated by the GAN at all. I thought that one generic enemy was in the training data and also in the GAN output, and that all we changed was the type of the enemy. Shouldn't one generic enemy type be part of the training data?}

\section{Approach}

First, data was collected from VGLC for training the GANs. OneGAN was trained in a manner similar to previous PCGML work with GANs, but MultiGAN required the collected training data to be separated by type. However, levels were evolved for both approaches using the same genome encoding.

%% Too long
%Since each level segment had a distinct structure type, developing an algorithm to sort the original data into sets served complicated. Since each type of level segment had a distinct, inherent structure, finding an efficient way to generate levels served to be similarly complicated. OneGAN was trained on all of the data despite segment structure. Each GAN of the MultiGAN approach was trained based on the distinct level segment types. Both approaches use the same genome encoding for evolution experiments.

\begin{figure}[t]
\centering
\includegraphics[width=1.0\columnwidth]{MegaManLevelWithArrows.png}
\caption{Level Generated by MultiGAN. \normalfont Each color represents a segment of a different type: Yellow:Down, Pink:Lower Right, Red:Horizontal, Blue:Upper Left, Black:Lower Left, White:Up. Upper Right segments can also be generated, but none are shown. Each GAN was trained only on the data of that given type.}
\label{fig:MultiGANmodel}
\end{figure}

%\todo[inline]{In the caption for Figure 1, change the order that you list the colors to correspond to the order they appear in the path through the level.}

\subsection{Collecting Data}

%The new approach developed in this paper is as follows: each segment type is used as training sets for separate GANs such that each GAN generates a specific type. GANs were trained for every possible segment type: 

MultiGAN requires the training data to be categorized by type:
up, down, horizontal, lower left, lower right, upper left, and upper right segments (Fig.~\ref{fig:MultiGANmodel}). Up and down segments are distinct because down segments often involve falling, thus making it impossible to move upward. Up segments require ladders and/or platforms that enable upward movement. In contrast, there is no distinction between left and right segments, which are both horizontal.

%is why they are grouped into a common horizontal segment category.

The VGLC data was missing many details from the original game, some of which were added back to the data before training (Table~\ref{tab:tiles}). 
%\todo{You added the enemies back in, right? The table mentions three enemy types, but claims that none of them are in the training data, which is clearly wrong. Didn't you have one enemy type (Int code 11?) that was mapped to either ground, wall, or flying based on location? We'll talk about this and figure out how to revise this content.<------ I specified this detail in the chart.}
In contrast, player spawn points were replaced with air tiles. Only one is needed per level, which is better placed using simple rules.

%\todo[inline]{Do these rules get explained later?}

%Level 7 was omitted. There were three segments which were impassable (in the lower right hand corner). In the original game, the player was expected to use the platform gun to traverse, but for the purposes of this research, it was best left out.



%% Just say you replace the player with air and leave it at that
%As for collecting the VGLC data, the initial screen is saved with the spawn tile represented as air. Having the spawn tile included in the training set would potentially create levels with multiple spawn points. For this reason, subsequent segments that contain the spawn point also replace the spawn tile with an air tile. 

%%% Too detailed, but I did add some of this information later
%Though there is overlap on the content of each iteration, every segment has at least one column or row that is unique to the last, depending on the direction moved. The exception to this is with corner segments, as the corner segments are also counted as horizontal, up, or down depending on the direction of the screen scrolling. A simpler scrolling approach was used in Mario ~\cite{volz:gecco2018}.

%% Repeats what came earlier
%In order to train the GAN, data must be collected. Information about Mega Man levels was gathered by the open source Video Game Level Corpus (VGLC).



%\todo{The rest of this paragraph is confusing. You say there are 124 segments in level 10, which is not true. Did you mean that was the tile width? However, I'm not even sure this information is relevant.} 
%The minimum number of segments in any given VGLC level is level 10 with 124 segments, whereas the maximum number of segments in any given VGLC level is level 9 with 344 segments (which consists mainly of identical water hallways).




The modified level files were scanned with a sliding window to create training data. The window size corresponds to the area visible on screen, which is 14 tiles high by 16 tiles wide. The window slides one tile at a time in the appropriate direction given where the path of the level led next. %(Fig.~\ref{fig:iterationexample}). 
Although adjacent segments can be orthogonally adjacent in any direction, there is no branching, so there is always exactly one direction to head in toward the end of the level. 
Horizontal, up, and down segments are categorized according to the direction the window slides while collecting the data. A segment is identified as a corner segment whenever the direction of sliding has to change. However, each corner segment is also considered 
%both a horizontal segment and either an up or down segment, depending on which direction the window slides in when leaving the segment \todo{Is my re-wording of this correct? }. No. If a window slides right, it continues to include the segment until the direction changes, then the new segment (say up) is now in up, and the corner is added to the corner segment. In other words, going from right to up, the lower right corner is both in Horizontal and the LR GAN, but that segment is not in the UpGAN.
a horizontal, up, or down segment, depending on which direction the window slides when entering the segment.
The result of this process is a collection of $14 \times 16$ segment training samples. OneGAN is trained with all of the data, whereas MultiGAN had a separate training set for each segment type (Table~\ref{tab:GANs}).

%%% Not sure we need to keep this figure
%\begin{figure}[t]
%\centering
%\includegraphics[width=1.0 \columnwidth]{IterationExample.PNG} \\
%\caption{Scanning Level Text File To Create Training Data. \normalfont This is a portion of a modified text file from VGLC. Data is collected by scanning one window worth of information, checking which direction to move in next, and then shifting the window over by one. The text file also represents empty null space, which is ignored..}
%\label{fig:iterationexample}
%\end{figure}

%%% Unnecessary reperition
%Although the data was collected in the same way for both the OneGAN and MultiGAN approaches, it was stored differently (Table~\ref{tab:GANs}). MultiGAN data was categorized according to segment type. Horizontal segments are identified by having no barrier on either the left or right side. Up segments were characterized by having no barrier on the up or down side, as well as following either an up segment or a lower right or left corner. Down segments were similar to up segments, but only with upper right or left corners. The corners were determined based on the preceding and proceeding sets. If the previous segment was horizontal and the next segment is up, then that segment is a lower right corner segment, and so on.

\begin{table}[t]
\caption{\label{tab:GANs}Characterization of VGLC Mega Man Data.}
{\small Number of segments of each type in the training data. OneGAN
 uses all training data, but MultiGAN uses a separate training set for each of seven distinct GANs, each consisting of samples with the same segment type.}

\centering
\begin{tabular}{|c|c|c|}
\hline
GAN & Type & Segment Count \\ 
\hline % Images needed to be vertically centered within row
OneGAN & All & 2344 \\
\hline
\multirow{7}{*}{MultiGAN} & Horizontal & 1462 \\
\cline{2-3}
& Up & 518  \\
\cline{2-3}
& Down & 364 \\
\cline{2-3}
& Upper Left Corner & 10   \\
\cline{2-3}
& Lower Right Corner & 9  \\
\cline{2-3}
& Upper Right Corner & 8 \\
\cline{2-3}
& Lower Left Corner & 8 \\
\hline
\end{tabular}
\end{table}

\subsection{Training the GANs}

The type of GAN used is a Deep Convolutional GAN, specifically a Wasserstein GAN \cite{arjovsky2017wasserstein}, as used in previous studies \cite{volz:gecco2018,gutierrez2020zeldagan,schrum:gecco2020cppn2gan}. 
The specific architecture is shown in Fig.~\ref{fig:architecture}.

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{gan_architecture}
\caption{GAN architecture.}
\label{fig:architecture}
\end{figure}


There are two key components to training a GAN: a generator and a discriminator. The generator is the GAN itself, and is responsible for generating fake outputs based on an input latent vector. The discriminator is trained to recognize whether a given input is real (from the training set) or fake (from the generator). 

The 2D JSON training segments of size\footnote{The actual size of the generator output and discriminator input is $32 \times 32$ for compatibility with previous research. Inputs are padded with zeros to be the right size.} $14 \times 16$ are expanded into 3D volumes by converting each integer into a one-hot vector across $12$ channels, one per tile type in Mega Man. During training, the discriminator is shown real and fake level segments. 
Fake segments are generated by the GAN by giving it
randomized latent vectors of length $5$.
Discriminator weights are adjusted to be more accurate, and generator weights are adjusted to produce better fakes.
The GAN is trained for 5000 epochs, at which point the discriminator can no longer determine whether an image is real or fake, and is thus discarded.
However, the generator can now produce convincing fake level segments
given arbitrary latent vector inputs.

%arbitrary latent vectors can now be sent to the generator to produce convincing fake level segments.

%%% This text describes how a level is generated from the raw GAN output, but we actually don't do this during training.
%For each tile position in the 3D generator output, the maximum output becomes $1$ and the remaining outputs become $0$ to produce the desired one-hot encoded format before the result is sent as input to the discriminator.






%The length of the generator output and discriminator input are both 12 based on the 12 tile types in Mega Man.
%The latent variable length is $5$ and the number of epochs is $5,000$. Tiles types are listed in \ref{tab:tiles}. 

%After training, the discriminator can be discarded, and



%The standard approach is to train the GAN on all data, but then there is no easy way to retrieve a segment of a particular type during generation.

%\subsection{Training MultiGAN}

The OneGAN is trained on all data, which is the norm, but as a result there is no clear way to retrieve a segment of a desired type.
MultiGAN is trained on the same data, but each individual GAN of the MultiGAN was trained on a different category of data from Table~\ref{tab:GANs}. Each of the seven GANs had the same architecture (Fig.~\ref{fig:architecture}) and therefore same latent input size of 5 as OneGAN. Each was also trained for 5000 epochs.


% \todo{This paragraph has poor grammar. I feel that most of it is not needed, and anything we choose to keep should be in section 4.1. However, I wonder: does this explain the discrepancy in the partition count not adding up to the total sum?} This means that most segments near corner segments are included in different training sets. Due to the nature of a corner segment, including too many segments in another direction would not be beneficial because they are essentially horizontal or vertical segments after a certain point. This leads to the restricted number of corner segments (around 10 segments per corner versus the horizontal training set having 1,462) Table \ref{tab:GANs}.




\subsection{Genome Encoding}
\label{sec:genome}

% \todo{Both MultiGAN and OneGAN use the same placement rules, so any discussion of that should actually happen in the genome encoding section next. However, there is some superfluous information here. Just move what seems relevant (see next section for details)} 

% This approach gave rise to several challenges when it comes to how the next segment can be placed. For example, an up segment cannot be placed after placing a down segment because that would replace the down segment with an up segment, thereby being counter-productive. Additionally, since segments can be placed in any direction, there was a safeguard to check if the level had wrapped in on itself, thereby making the next segment not placeable.


Complete levels are generated from MultiGAN and OneGAN in the same way. 
The genome is a vector of real numbers from $[-1,1]$ divided into sections for each segment. In each section of 9 variables, the first 5 are latent variables, and the last 4 determine the relative placement of the next segment. These 4 values correspond to up, down, left, right placement. Whichever direction has the maximum value is chosen for the next placement, unless that location is occupied, in which case the next highest value is chosen, and so on until an unoccupied neighboring location is found. If there are no unoccupied neighbors, then level generation terminates early.

For each segment generated by OneGAN, the 5 latent variables of the genome section are sent to the GAN to produce the segment. For MultiGAN, the direction of the next placement and the previous placement determine the appropriate type for the current segment, and the specific GAN for the needed type is used to generate the segment. If the new direction is different from the previous one, a corner GAN is used to generate that segment. If a segment is generated to the right, and the new direction will be up, then the lower-right GAN will generate a segment to the right to prepare for the new up segment, and so on.

%The first three variables after the latent variables represent the ability to generate up, down, and right respectively. The last variable determines the ability to generate left as well. The variables corresponding to direction contain a number from $-1$ to $1$. The range of the latent variables are also from $-1$ to $1$. Which direction is generated next is based on the highest preference of the numbers. 

%If the preferred direction is illegal to traverse to (i.e.\ a segment already exists in that direction), then the next highest preferred direction is chosen and evaluated for validity. If no direction is legal, then the generation terminates prematurely.

For simplicity, the player spawn is placed in the upper-left most section of the first segment placed, and the level ending orb is placed in the lower-right most section of the last segment placed.

\section{Experiments}

This section goes into detail regarding how levels were evolved and evaluated based on desirable properties. Both the OneGAN and MultiGAN methods use NSGA-II~\cite{deb:tec02} to evolve suitable levels. The two fitness objectives are solution path length and connectivity, both determined by A* search. Parameters for evolution are also explained in this section. Code for the experiments is available as part of the MM-NEAT software framework\footnote{\url{https://github.com/schrum2/MM-NEAT}}.

\subsection{NSGA-II}
NSGA-II~\cite{deb:tec02} is a Pareto-based multi-objective evolutionary optimization algorithm. NSGA-II uses the concept of Pareto Dominance to sort populations into Pareto layers based on their objective scores. If an individual is at least as good as another in all objectives, and strictly better in at least one objective, then that individual \emph{dominates} the other. The most dominant Pareto layer contains individuals which are not dominated by any other individual in the population, and is known as the nondominated Pareto front. 

NSGA-II uses elitist selection favoring individuals in the Pareto front
over others. The second tier of individuals consists of those that would be nondominated if the Pareto front were absent, and further layers can be defined by peeling off higher layers in this fashion. When ties need to be broken among individuals in the same layer, individuals that are maximally spread out to lower density regions of the current layer are preferred.

Pareto-based algorithms like NSGA-II are useful when objectives can be in conflict with each other, thus causing trade-offs. However, even when objectives correlate fairly well with each other, as in this paper, NSGA-II is useful in that it can provide a fitness signal in regions of the search space when one objective may be flat, without the need to define any kind of weighting between objectives. 

\subsection{Fitness Functions}

Levels are evolved with a combination of connectivity score and solution path length: the connectivity score provides a smooth gradient to improvement, even in regions of the search space filled with unbeatable levels whose solution path length is undefined.

Solution path length is determined by A* search 
%using Manhattan distance as the heuristic function. A* was performed 
on a simplified model of the game to allow for quick execution. If A* could not beat the level, then it was assigned a score of $-1$, and levels that were beatable were assigned a score equal to the length of the A* path. 

%Every OneGAN and MultiGAN level was generated by evolving latent vectors for 300 generations to encourage longer A* distance and more connectivity. Longer A* paths utilize more space in the levels, making them longer and harder to beat. 

When multiple levels are unbeatable, connectivity  provides a way of differentiating them. Connectivity can also differentiate two levels with the same solution length. Connectivity score is formally defined as the proportion of traversable tiles (e.g.\ air, ladders) in the level that are reachable.
%\todo{Check if this is true. I can't remember if this is the definition, or if connectivity is actually just the set of all locations visited during the search, in which case we may need to change the name.}
Higher connectivity implies more places to explore, thus containing less wasted/unused space.

%Connectivity represents the amount of space in the level that was traversable, allowing the human player to traverse more of the level aside from just the solution path. 

The simplified A* model only allows the avatar to move discretely through the space one tile position at a time, and has a simplified jumping model that simulates the playable game. The model recognizes that Mega Man will die from contact with spikes or due to falling off the edge of the screen, but ignores the existence of enemies in the level. Though the model is not perfect, it is sufficient, and faster to calculate than an actual simulation of the full dynamics of Mega Man. To verify that all evolved champion levels were beatable, they were uploaded to the Mega Man Maker servers, which do not allow levels to be uploaded unless a human player successfully beats them first.


%One consistent challenge was getting the A* representation of Mega Man to be appropriate. Though Mega Man is a tile-based game, the player avatar can actually move smoothly through the space, and had the ability to influence jump height by how long the jump button is held. The simplified A* model only allows the avatar to move discretely through the space one tile position at a time, and has a simplified jumping model that simulates the playable game. The jumping model was originally adapted from the model in Mario ~\cite{volz:gecco2018}, but the falling was adapted for longer and farther jumps. There were several unique cases that needed to be accounted for when jumping, such as not being able to jump and duck at the same time, nor duck while on a ladder. The model recognizes that Mega Man will die from contact with spikes or due to falling off the edge of the screen, but ignores the existence of enemies in the level. Though the model is not perfect, it is sufficient, and faster to calculate than an actual simulation of the full dynamics of Mega Man. In order to stress test the validity of the paths, sample levels were assessed and played in accordance with the solution path. The simulation has minor errors when falling from extreme heights, as the falling path in the game is nonlinear. When not jumping from extreme height, the jumping model is accurate insofar as distance and height, though the model cannot represent a smooth jump. %\todo{say something about stress testing to assure that the model did not screw up too much, i.e. how close are projected paths to actual paths?}.

%We struggled to program in all of the so-called corner cases, such as how to handle spikes, sliding, falling, and passing through objects. Each special object had a set of rules for A* to follow, so it was difficult to implement a set of overarching rules because each block had a special exception.

\subsection{Experimental Parameters}

Levels were evolved using both the OneGAN and MultiGAN approach using NSGA-II with a population size of $\mu = \lambda = 100$ for 300 generations. Preliminary experiments indicated no significant improvements after 300 generations. The evolution experiment was repeated with each pre-trained generator 30 times. 

%\todo{I added a lot here. Check that is is right and fill in the blanks} 
Evolved levels consisted of 10 segments each, unless generation terminated early (see Section~\ref{sec:genome}). Since the GAN latent input size was 5, and each segment used 4 auxiliary variables for determining relative placement, the total length of each real-valued genome was $(5+4)\times 10 = 90$ variables in the range $[-1,1]$. Each generation offspring were created using a 50\% chance of single-point crossover. Whether crossover occurred or not, every offspring was mutated: each real-valued number in the vector
had an independent 30\% chance of polynomial mutation \cite{deb1:cs95:polynomial}.

\section{Results}

Levels produced by OneGAN and MultiGAN are compared quantitatively in terms of their solution path lengths and novelty, and qualitatively in terms of the final levels produced by each run. 
%\todo{Probably need to say something about the novelty analysis here too}

\subsection{Quantitative Analysis}
The A* path lengths are significantly longer ($p < 0.05$) with MultiGAN than OneGAN (Fig.~\ref{fig:averageScores}). The separation between methods is established early in evolution and maintained until the end. The difference in averages is approximately 50 tile steps throughout all of evolution. Because segments are $14 \times 16$ tiles, a difference of 50 means that OneGAN champions sometimes skip one or more segments, though it is also possible for paths to be lengthened with additional twists and turns inside individual segments.


\begin{figure}[t]
\centering
\includegraphics[width=1.0\columnwidth]{AVG-megaManLevels0-MultiGAN.pdf}
\caption{Average Champion A* Path Length Across Evolution. \normalfont Plot of averages across 30 runs of OneGAN and MultiGAN of champion A* path lengths by generation. MultiGAN path lengths are approximately 50 steps longer than OneGAN paths throughout evolution. For context, recall that segments are $14 \times 16$ tiles.}
\label{fig:averageScores}
\end{figure}


%%% Connectiviety not worth talking about
%\todo[inline]{we should consider doing a similar comparison of the connectivity scores. At the very least, we should say something about them, even if all we say is that they don't really matter ... we should check first though}

Levels produced by the two approaches were also compared in terms of a novelty metric. Novelty can be defined in terms of an individual segment, a whole level, or an arbitrary collection of segments. Segment distance $d(x,y)$ between two segments $x$ and $y$ is defined as the number of positions in which their respective tile types differ, normalized to $[0,1]$. Then, the novelty $N$ of a segment $x$ is defined as its average distance from all segments in some reference collection $S$:
\begin{equation}
    N(x,S) = \frac{\sum_{y \in S} d(x,y)}{|S|}
\end{equation}
The novelty of a level $M$ is the average novelty of all segments it contains, where each segment's novelty is calculated with respect to the set of other segments in the level (excluding itself). The Level Novelty $LN$ is:
\begin{equation}
    LN(M) = \frac{\sum_{x \in M} N(x, M - \{x\})}{|M|}
\end{equation}

%%%% Explained more formally above
%The novelty of each segment with respect to the other segments in the level is calculated, that is averaged to get the level novelty, and those scores are averaged by type. 

Within VGLC levels, segments are fairly uniform and less novel. Both GAN approaches produce levels where there is greater segment variety in each level compared to VGLC (Table~\ref{tab:avgnovelty}). This suggests that there is more variety in GAN-generated levels than in the original game, but it may also suggest less consistent style. 

Table~\ref{tab:setnovelty} indicates that there are not that many repeated segments within the individual data sets for a given type of level. MultiGAN repeats segments more than OneGAN (fewer unique segments), likely because of the lack of training samples in corner segment GANs. 
In fact, Table~\ref{tab:cornernovelty} analyzes the distinct segments and novelty scores of each corner GAN from MultiGAN, showing that these corner GANs produced less novel results. However, despite some repetition in the corners, the overall novelty scores of OneGAN and MultiGAN are slightly more than VGLC. 

%This data also indicates that despite a lack of uniformity within individual corner GANs, the diversity across the entire corner set is higher.
%indicates that within each corner GAN, there is a great deal of repetition. The total number of segments in the corners were $132$, whereas the number of distinct segments were $99$. 

%This data also indicates that despite uniformity within levels, the diversity across the entire Mega Man game is high, and both GAN approaches replicate that level of diversity in their outputs.


\begin{table}[t!]
\caption{\label{tab:avgnovelty}Average Level Novelty By Type}
{\small For each type of level, the $LN$ of each level is calculated. The
average $LN$ score across $N$ levels of the given type are presented below.}

\centering
\begin{tabular}{|c|c|c|c|c|}
\hline
Type & $N$ & Average $\pm$ StDev & Min & Max \\ 
\hline
%0.08952787396399
VGLC& 10 & 0.34$\pm$0.09 & 0.11 & 0.45\\ 
\hline 
%0.029090656696673
OneGAN & 30 & 0.43$\pm$0.03 & 0.36 & 0.48\\
\hline
%0.023308971277316
MultiGAN & 30 & 0.41$\pm$0.02 & 0.34 & 0.46 \\
\hline

\end{tabular}
\end{table}

%\todo[inline]{swap the rows and columns of Table 4 the way you did Table 5 ... but keep a commented copy of the original in case we want to switch back.}
\begin{table}[t!]
\caption{\label{tab:setnovelty} Distinct Segments By Type}
{\small For each level type, the total number of segments,
number and percentage of unique segments within the collection (removing duplicates to get a set),
average segment novelty with respect to the collection, and average segment novelty with respect to the set (without duplicates) are shown.
MultiGAN has the lowest percentage of
distinct segments, but is between VGLC and OneGAN in terms of novelty, whether sets or complete collections are used, though the comparative novelty scores are all close.}

% . The third row is the average novelty
%across all segments in each collection, defined with respect to their
%particular collection. The fourth row is the same calculation, but
%restricting each collection to a set by removing duplicates.
%In terms of the collections, the different approaches exhibit similar novelty, though VGLC is less novel than MultiGAN, which is less novel than OneGAN.

\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{|c|c|c|c|c|}
\hline
 & Segments & Distinct       & Average          & Average\\ 
 &          & Segments       & Novelty All      & Novelty Set\\
\hline 
VGLC & 178  & 159 (89.3\%) & 0.4390 & 0.4349  \\
\hline
OneGAN & 300 & 287 (95.7\%) & 0.4709  & 0.4637  \\
\hline
MultiGAN & 300 & 254 (84.7\%) & 0.4542 & 0.4483\\
\hline


\end{tabular}
}
\end{table}

% \begin{table}[t!]
% \caption{\label{tab:setnovelty} Distinct Segments By Type}
% {\small For each level type, the total number of segments,
% number and percentage of unique segments within the collection (removing duplicates to get a set),
% average segment novelty with respect to the collection, and average segment novelty with respect to the set (without duplicates) are shown.
% MultiGAN has the lowest percentage of
% distinct segments, but is between VGLC and OneGAN in terms of novelty, whether sets or complete collections are used, though the comparative novelty scores are all close.}

% % . The third row is the average novelty
% %across all segments in each collection, defined with respect to their
% %particular collection. The fourth row is the same calculation, but
% %restricting each collection to a set by removing duplicates.
% %In terms of the collections, the different approaches exhibit similar novelty, though VGLC is less novel than MultiGAN, which is less novel than OneGAN.

% \centering
% \begin{tabular}{|c|c|c|c|}
% \hline
%  & VGLC & OneGAN & MultiGAN \\ 
% \hline 
% Segments & 178 & 300 & 300 \\
% \hline
% Distinct Segments & 159 (89.3\%) & 287 (95.7\%) & 254 (84.7\%) \\
% \hline
% Average Novelty All & 0.4390 & 0.4709 & 0.4542 \\
% \hline
% Average Novelty Set & 0.4349 & 0.4637 & 0.4483 \\
% \hline

% \end{tabular}
% \end{table}


\begin{table}[t!]
\caption{\label{tab:cornernovelty} Distinct Corner Segments in MultiGAN}
{\small Shows number of segments in MultiGAN levels generated by each corner GAN, number that were distinct, average novelty of collections from each GAN, and average novelty across the distinct segments. Corner segments from the same GAN often differ by only a few tiles, which is why novelty scores are low despite the sometimes high percentage of distinct segments.}
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{|c|c|c|c|c|}
\hline
 & Segments & Distinct & Average & Average\\ 
 &          & Segments       & Novelty All      & Novelty Set\\
\hline 
Lower Left & 42 & 28 (66.7\%) & 0.2453 & 0.2689 \\
\hline
Lower Right & 25 & 20 (80.0\%) & 0.3537 & 0.3448 \\
\hline
Upper Right & 40 & 29 (72.5\%) & 0.2635 & 0.2770 \\
\hline
Upper Left & 25 & 22 (88.0\%) & 0.2875 & 0.2857 \\
%\hline
%Total & 132 & 99 (75.0\%) & 0.4113 & 0.4084\\
\hline
\end{tabular}
}
\end{table}


% \begin{table}[t!]
% \caption{\label{tab:cornernovelty} Distinct Corner Segments}
% {\small The first row is the total number of corner segments
% across all levels of the MultiGAN, and the second row is
% the number of distinct corner segments, and the
% associated percentages. Although the Lower Left and Upper
% Right corner GANs generated more corners, fewer of them were distinct
% and they overall had worse novelty than the other two corner GANs.}

% \centering
% \begin{tabular}{|c|c|c|c|c|c|}
% \hline
%  & Lower Left & Lower Right & Upper Right & Upper Left & Total \\ 
% \hline 
% Segments & 42 & 25 & 40 & 25 & 132 \\
% \hline
% Distinct Segments & 28 (66.7\%) & 20 (80.0\%) & 29 (72.5\%) & 22 (88.0\%) & 99 (75.0\%)   \\
% \hline
% Average Novelty All & 0.2453 & 0.3537 & 0.2635 & 0.2875 & 0.4113 \\
% \hline
% Average Novelty Set & 0.2689 & 0.3448 & 0.2770 & 0.2857 & 0.4084 \\
% \hline

% \end{tabular}
% \end{table}



\setlength\tabcolsep{1.0pt}
\begin{table*}[ph!]
\caption{\label{tab:LEVELS} Example Generated Levels.}
{\small Each is discussed in detail in Section~\ref{sec:levelapp}. Names appear under each level.}

\centering
\begin{tabular}{p{0.35\textwidth}p{0.4\textwidth}}
 \multirow{6}{0.3\textwidth}[4.5cm]{
 \includegraphics[width=0.3\textwidth]{AStarDistAndConnectivitySevenGAN1.png} 
 MultiGAN1 
 \includegraphics[width=0.25\textwidth]{AStarDistAndConnectivityOneGAN15.png} 
OneGAN15} & 
\includegraphics[width=0.35\textwidth]{AStarDistAndConnectivityOneGAN0.png} \\
% Empty space for image above
& OneGAN0\\ 
% Empty space for image above
& \includegraphics[width=0.35\textwidth]{AStarDistAndConnectivitySevenGAN25.png}\\
% Empty space for image above
& MultiGAN25\\ 
% Empty space for image above
& \includegraphics[width=0.3\textwidth]{AStarDistAndConnectivitySevenGAN27.png}\\
% Empty space for image above
& MultiGAN27\\ 


\includegraphics[width=0.35\textwidth]{AStarDistAndConnectivitySevenGAN7.png}
MultiGAN7
& \includegraphics[width=0.4\textwidth]{AStarDistAndConnectivityOneGAN8.png} 
OneGAN8\\



\end{tabular}
\end{table*}




%Leads to lengthier levels, which are harder (clarify not significant in results section). No significant differences between the two. . Way to deal with the data: Present as if we were agnostic about the results and just say it as it is. Talk about pros and cons about each.
\subsection{Qualitative Analysis}
\label{sec:levelapp}

The specific levels talked about in this section can be found in Table~\ref{tab:LEVELS}. Evolved champions from all 30 runs with OneGAN and MultiGAN can be viewed online\footnote{\url{https://southwestern.edu/~schrum2/SCOPE/megaman.php}} and played with Mega Man Maker.

 

OneGAN levels were more confusing due to the lack of flow. There are many random pits that Mega Man cannot traverse without the platform gun (see Section 3). MultiGAN levels generally look more natural, meaning adjacent segments connect and flow better. 

%%% I wasn't sure what this meant. "choppy" and "lack continuity" and "lack all uniformity" seem ill-defined. I think it is better to just drop this.
%In addition, the MultiGAN levels look a little more choppy when the level changes from a normal segment to a water segment, and MultiGAN levels occasionally lack continuity when changing directions. This could be a result of having more direct copies of certain segments across all MultiGAN levels, as seen in Table~\ref{tab:setnovelty}. OneGAN levels, however, lack all uniformity and were seemingly random in segment placement. 



Different MultiGAN levels tend to have the same or nearly identical corner segments because of
issues with novelty pointed out in Table \ref{tab:cornernovelty}. However, a typical 10-segment level often only has one or two representatives of each corner type, so repetition of corner segments within the same level is very rare.


%of the low number of training examples for each corner GAN. 





%%% I don't want to claim that we've done enough analysis to definitely prove this. It is also sort of subjective.
%, which do not appear in MultiGAN levels. This could be again that MultiGAN levels have more direct copies of segments and less novelty than OneGAN, which causes MultiGAN levels to be more consistent in level generation and playability (see Table ~\ref{tab:setnovelty} and Table ~\ref{tab:avgnovelty}). 

%\todo{Because I removed some text (commented above) this information is orphaned. Integrate it with the rest of the text somehow} 

%%% Not sure what this means, and not sure that it is true
%With MultiGAN, each GAN has an individual set of data independent from the other GANs to make perfectly smooth transitions from one segment to another. OneGAN levels look more chaotic due to having all data in one GAN.

As seen in \textbf{OneGAN15}, OneGAN levels tend to have entire segments that are unreachable or unnecessary. One water segment in the figure 
leads to a dead end, and the other is blocked off by the other segments. Note that despite being specifically evolved to maximize
solution length and connectivity, OneGAN struggled to connect segments. Similarly, in \textbf{OneGAN0} there are two unused segments at the bottom.
These segments are reachable, but not needed. Examples like these explain why OneGAN's solution path lengths are on average 50 tiles shorter than those of MultiGAN.

%It is interesting about this level is that nearly all of the tiles are traversable, meaning that Mega Man can travel to them. That being said, two level segments and the majority of a third are unnecessary for beating the level. 
%\todo{two whole segments are being skipped in champion levels of OneGAN. Connect this to my earlier comments about the different path lengths being about 50 ... cutting out two whole segments cuts out at least 28 steps if not more}. 
%Since two whole segments are not in use in the OneGAN champion level, it follows how OneGAN path lengths are about 50. The lack of or improper use of segments can cut out at least 28 steps to the solution path, if not more.

However, OneGAN solutions do sometimes successfully traverse all segments, as seen in \textbf{OneGAN8}. However, even this level has large sections in the lower left and upper mid section that are not traversed, which further explains the shorter solution paths compared to MultiGAN.


%Although OneGAN levels tend to struggle more often with cohesive and continuous placement of segments, there are some that are streamlined. In \textbf{OneGAN8}, every single segment is used in the solution path, suggesting that this particular OneGAN level effectively uses the space in the level. However, there are large sections in the lower left and upper mid section that are not traversed, which helps to explain the shorter solution paths compared to the MultiGAN approach.
%\todo{it uses all segments, but there are large sections in the lower left and upper mid section that are not traversed.}. 

%%% Not sure the connection here is strong enough to point out
%Despite using the majority of the level, the greater novelty and fewer distinct segments shows the overall layout lacks continuity and consistency in the levels (see Table~\ref{tab:avgnovelty}).

%generally had structure that was more properly utilized when evaluating the solution path and connectivity. 

In contrast, MultiGAN levels 
made better use of their allotment of 10 segments.
In \textbf{MultiGAN1}, the average portion of each segment occupied by the solution path is roughly $12\%$, whereas in \textbf{OneGAN15} the portion is only $8\%$. Every ladder in the level is part of the solution path, suggesting a more efficient use of space and intelligent placement. In fact, the left-most side of \textbf{MultiGAN27} even presents an example of ladders between distinct segments perfectly aligning.
\textbf{MultiGAN1} also effectively paired moving platforms with hazard spikes, which is an interesting challenge. MultiGAN levels were good at placing blocks in such a way that a simple miscalculated jump could result in death, as in the original Mega Man, which is known for frustrating platforming challenges. 

MultiGAN levels generally follow a snaking pattern not only in segment placement, but also in the solution path, whereas solution paths for OneGAN levels were more direct. Though MultiGAN does occasionally struggle with unnecessary block placement, such as in \textbf{MultiGAN25} with the lower left corner, the solution paths are both longer and more challenging to traverse than in OneGAN. 

%%%% On second thought, I guess there isn't much meaningful to say about this level, but it does still look nice.
%One advantage of the MultiGAN method is that levels can be constructed in virtually any order and still have interesting makeups, such as \textbf{MultiGAN27}, which starts at the top, goes down two segments, and then goes to the left before looping back around to the right side of that third segment. The level, which almost connects the third segment to the last, forces the player to traverse the remainder of the level in order to reach the orb.

%\todo{You need to explain that the level starts at the top, goes down two segments, and then goes to the left before looping back around to the right side of that third segment}.
%  MultiGAN levels often look far more interesting and complex in terms of overall layout\todo{I worry that this subjective claim is too strong to make without some kind of quantitative evidence}. \todo{I think the whole remainder of this paragraph can be dropped, since I think you're just referring to the water hallways (blocks above and below, and water horizontally through the center). Basically, both OneGAN and MultiGAN suffer from this equally, so if you are going to mention it, point out that both approaches have this issue from the training data ... don't just criticize MultiGAN. In fact, this sets up a topic for the discussion section later. Perhaps by using CPPN2GAN or some other method to assure cohesiveness and continuity across the levels, such weird water sections can be avoided. } That being said, MultiGAN levels sometimes place both segments and blocks in odd ways, such as in \textbf{MultiGAN7} as well as the previous figure in that there will be a randomly placed water segment that is entirely discontinuous with the rest of the level and overall has no benefit. The discontinuous water segments can be eliminated by providing the training data with fewer water segments.

Human subjects confirmed some of these observations.

\section{Human Subject Study}

This section briefly describes a human subject study comparing OneGAN and MultiGAN levels, and the results of the study.

\subsection{Human Subject Study Procedure}

The study was advertised globally on social media and forums, but most of the 20 participants were undergraduate students from Southwestern University.
Response was limited because participants seemed unwilling or unable to install and create an account for Mega Man Maker, which was required in order to participate.

%In order to discover how effective the aforementioned methods are for PCGML, a human subject study was conducted. 

Participants played a random evolved champion level of each type. Subjects were not informed how levels were generated, and were led to believe that some levels \emph{might} be made by humans. Participants compared levels based on how difficult, fun, human-like, and interesting the designs were.

%difficulty, interestingness, how fun they were, and how human-like the designs were. 


%%%% We actually don't need to bother saying this anymore, since this was just put in to help explain the seemingly bad results.
% The participants were not able to see the entire zoomed out level, only one 14x16 screen at a time during their play-through. Therefore, participants could not determine any shortcuts by simply looking at the level. 




% \todo[inline]{given the flow up until this point, I think this whole section should be moved to later in the paper. Basically, have the Results section appear after the Experiments section, and make the new Human Subject Study section contain both a subsection explaining what we did, and a subsection explaining the results.}

%In order for the participants to play the levels created by OneGAN and MultiGAN, they were uploaded to Mega Man Maker. The Mega Man Maker level format was converted from the JSON representation into the .mmlv format that allows the level to be played by MMM. 

Mega Man Maker allows for a rich diversity of tiles that vary mainly in their appearance, but the tile types produced by the GAN (Table \ref{tab:tiles}) are relatively plain. As a result, levels look simpler than typical human-designed levels.
One study participant who had experience with Mega Man Maker pointed out that our levels would be impossible to make using the level editor because the editor blends different tiles from a given tile set to distinguish surfaces, corners, etc. As a result, this user could tell that our levels were made through direct manipulation of level text files.

%tiles on the surface did not appear as they do when being built in MMM directly. This was because the .mmlv file contains variable dedicated to the appearance. Since the text file was edited directly, the level placement rules no longer apply. The details of the conversion form JSON to .mmlv are in the code.



%All tiles were converted to a single tile of that type in MMM. This explains why the levels look rather simplistic. The full use of MMM was not explored as the aesthetics of the level were not pertinent to the research. 
%One study participant pointed out that the tiles on the surface did not appear as they do when being built in MMM directly. This was because the .mmlv file contains variable dedicated to the appearance. Since the text file was edited directly, the level placement rules no longer apply. The details of the conversion form JSON to .mmlv are in the code.

% \todo{There needs to be a whole section explaining how we convert from our format to the Mega Man Maker format. Or perhaps more realistically, you can say a tiny bit about the MMM level format, and then use that brief explanation as a reason to justify why a full explanation of the procedure is outside the scope of this paper. We can also reassure them that the details are in the code if they want to look. However, we do want to make a couple of points about why our levels look so simplistic. Basically, we are not taking full advantage of what MMM has to offer. In fact, one study participant even pointed out that our tiles were not following the rules that a level made in MMM would follow in terms of how surface tiles of a given set differ from others ... it was obvious we had edited the text file directly. I'm not sure if this MMM section belongs here or earlier.}. 


Players had access to the platform-gun, allowing them to more easily traverse the more difficult, or otherwise impossible, jumping puzzles. Participants were asked if they thought the platform-gun was required to beat the level. Even with the platform-gun, some players did not beat both levels. They were encouraged, though not required, to make several attempts at each.


%Some of the difficulty of those OneGAN or MultiGAN levels were not evaluated due to the ability to skip past difficult sections. 





%\todo[inline]{TODO: Explain the experiment that was conducted, including details on all experimental parameters.}

\subsection{Human Subject Study Results}


%Each participant was expected to be seated for at least 45 minutes and was asked to complete both levels. 

Quantitative results from the human subject study are in Table~\ref{tab:subjectStudyData}, including a statistical analysis.

Because it was difficult to find expert Mega Man players, several participants struggled to complete the levels.
Only 14 participants completed the OneGAN level, whereas 17 completed the MultiGAN level. Additionally, 11 thought the platform-gun was required to beat the OneGAN level, whereas only 8 believed this of the MultiGAN level. Needing the platform-gun could indicate bad design, as most actual levels can be traversed by normal jumping.


%%%% Don't need to say this anymore either, now that we have realized we were interpreting the results wrong
%Perhaps MultiGAN levels are more like actual Mega Man levels, but that makes novice players not like them as much. Additionally, OneGAN levels are more likely to have cheating shortcuts due to bad design layout, which may make the novice players like them more.

%\todo[inline]{Possibly irrelevant: }In comparing the OneGAN and MultiGAN levels, most participant responses were close to evenly split, which combined with the small size of the study means that no statistically significant conclusions could be made. The quantitative results of the study are summarized in Table ~\ref{tab:subjectStudyData}, but the most insightful results from the study actually come from the open-ended responses of certain users.



\begin{table}[t!]
\caption{\label{tab:subjectStudyData} Human Subject Study Results}
{\small Shows the data associated with the Human Subject Study that was conducted. Under Type, ``y/n'' means participants could answer \emph{yes} or \emph{no} for each individual level, and ``e/o'' means the participant had to pick \emph{either} one level \emph{or} the other. Responses to ``e/o'' questions were compared using one-sided binomial tests whose approximate $p$ values are shown in the last column, where \textbf{bold} values indicate a significant difference ($p < 0.05$)} \\
\centering
\begin{tabular}{|c|c|c|c|c|}
\hline
Question & Type & OneGAN & MultiGAN & $p$ \\ 
\hline 
\hline
Completed? & y/n & 14 & 17 & N/A \\
\hline
Platform-Gun Required? & y/n & 11 & 8 & N/A  \\
\hline
Created by a Human? & y/n & 4 & 13 & N/A \\
\hline
\hline
Harder? & e/o & 14 & 6 & 0.05766 \\
\hline
More Fun? & e/o & 2 & 18 & \textbf{0.0002012} \\
\hline
More Human-Like? & e/o & 5 & 15 & \textbf{0.02069}\\
\hline
More Interesting? & e/o & 7 & 13 & 0.1316\\
\hline

% The details for how this is calculated were a bit weird
%Died More? & y/n & 11 & 5  \\
%\hline

\end{tabular}
\end{table}

%One participant who said that the MultiGAN level was more difficult said that the levels "reminded [them] of the original Mega Man," whereas the OneGAN levels did not. Another participant said that the OneGAN level was more difficult since the level "forced the player to make a leap of faith" and that the OneGAN level was "confusing." Another participant said that they "took far more damage in the [MultiGAN] level," implying that the enemies were placed strategically in order to force the player to take damage. Other participants had a hard time "timing [their] falls" in the MultiGAN, had ladders that "led to nowhere," or that the level seemed "hard in a random and not very well thought-out way."

Most participants found their OneGAN level harder than their MultiGAN level. One respondent who said the OneGAN level was harder noted that the ladder placement ``made no sense/led to nowhere/death screen.'' Another respondent said the OneGAN level ``seemed hard in a random and not very well thought-out way.'' Yet another said they could not see where they were falling and that it was difficult to ``time [their] falls.'' A different participant said that OneGAN was harder because it ``seemed to force the player to make a leap of faith at the start'' which led to confusion.

%As for how fun the levels were, there was an overwhelming majority in favor of MultiGAN. 

A significant number of participants found the MultiGAN level more fun ($p < 0.05$). % ($p \approx 0.0002012$).
One participant said that the level ```knew' what [they were] going to do and could make things deliberately harder for [them] in an intelligent way,'' whereas the OneGAN level was ``hard in a seemingly unintentional way.'' Another participant said that they ``liked how much longer [MultiGAN] was. There were many places [they] could go.'' However, two participants said that neither were particularly enjoyable, and another participant said that the MultiGAN level ``felt more like [they] were supposed to lose, where [the OneGAN level]...wanted to be beaten.'' For that particular user, the OneGAN level had five segments where the player could fall down to quickly and easily traverse a large portion of the level, whereas the SevenGAN level had more enemies, harder jumping puzzles, and a longer solution path. Such comments also explain why a majority of users thought MultiGAN levels were more interesting than OneGAN levels.

%\todo[inline]{Can you check if this OneGAN level was abnormally short or easy because of bad segment placement?}

%As for how human-like the levels were, most participants thought that MultiGAN was more convincing, though some thought that neither were made by a human. 

A significant number of participants ($p < 0.05$) thought the MultiGAN level's design was more human-like than OneGAN's, and 13 thought the MultiGAN level was created by a human in comparison to OneGAN's 4, though some thought correctly that neither was designed by a human.
One participant said the OneGAN level was not created by a human because it spawns the player next to enemies, and it ``feels impossible to not take damage from them.'' Another participant said that the OneGAN level ``seemed like it was entirely random'' and that it made it ``frustrating.'' Yet another said that the MultiGAN level was made by a human because it ``had more repeated building elements'' and that the OneGAN level had no such pattern, and would sometimes have a ``solitary floating block.'' Finally, one participant said that the OneGAN level ``seemed more random'' than MultiGAN, which led to them thinking that it was ``made by a human trying to trick the player.''

%%% Moves mention of this above
%Most participants thought that MultiGAN was more interesting, and the user comments followed a similar narrative as above.
%\todo[inline]{feel that something about "interestingness" has to be in here for completeness, but there is no room}



\section{Discussion and Future Work}
%MAKE FIT
% With the MultiGAN approach, the level segments are generated appropriately relative to its placement in the level and did not have to evolve as much as OneGAN to make the level beatable. Due to the incohesive and chaotic structure of OneGAN, it could not reach the level length that MultiGAN did, even after longer generations.
%MultiGAN transition too jarring and interpolation

%%% Rewrote this below
%With OneGAN, it is much more difficult to prevent one segment from leading into another one given the nature of the training data. Since OneGAN has all of the data from every VGLC level in one set, generating a segment has no inherent properties. The OneGAN has no way of determining how useful a certain segment will be when being placed in the level. OneGAN generates segments that are based on the amalgamation of different segment types, which on their own have distinct properties (ie. horizontal segments have a floor and open space to the right and left), but when all are used to train a single GAN, evolution is needed to generate segments with distinct properties. 

With OneGAN, the barriers between segments are unpredictable. The majority of the training data consists of horizontal segments, making such segments more likely. However, if Mega Man needs to move up into a new segment, he could get stuck banging into the floor of the segment above him. Often, when Mega Man needs to move down to a new segment, the only way to do so is by falling into what looks like a pit trap, because the next segment was generated beneath a segment modelled on horizontal segments. Note that OneGAN does not appear to be suffering from mode collapse~\cite{thanhtung2020catastrophic}; but it does struggle to pick the right type of segment for a given situation. Because there are higher odds of the next segment being unreachable, the only way to traverse some OneGAN levels is by side routes caused when the sequence of segments snakes back into being adjacent to an earlier segment. However, these side routes result in certain segments being skipped in the solution path.

MultiGAN does not have this problem. When an upward segment is needed, it can be generated reliably. Everything generated by the Up GAN will provide ladders and/or platforms that make such movement possible. Similarly, when movement transitions from horizontal to vertical, the MultiGAN will use an appropriate corner GAN.
However, corner GANs are trained on significantly fewer segments than their horizontal and vertical counterparts, causing a lack of diversity in generated corner segments (Table \ref{tab:cornernovelty}). This lack of diversity will generally not be noticed in any individual MultiGAN level, but repetition of specific corner segments can be seen across levels. 
Also, those familiar with the original game may notice the duplication of certain corner segments from Mega Man.

%%% No need to belabor this point
%These corner segments are also familiar to those familiar with the levels in the original Mega Man, because these corner GANs are more likely to make exact copies of corner segments in previous sections.
%Since corner segments are essentially copied from the training data, these corner GANs are repetitive. 

Both OneGAN and MultiGAN sometimes produce plain hallways filled with water.
This segment is a reproduction of a segment that occurs repeatedly in Level 9, which consists of a long boring hallway filled with water. In the context of the original game, this scenario is an interesting departure from the norm, but the appearance of this segment in levels produced by GANs is usually out of place. In fact, the handling of water by both approaches can lead to unusual segments on occasion, indicating that some special handling of segments containing water may be necessary. However, despite the lack of continuity when handling water segments, LVE resulted in other types of segments derived from the diverse training set fitting well together.

%. This is because of level nine in the training data, since the level is entirely comprised of water hallways.

Conditional Generative Adversarial Networks (CGANs \cite{mirza2014conditional}) could serve as an alternative to using multiple GANs. A CGAN is conditioned on some additional input, allowing it to produce output of a desired type on demand, thus eliminating the need for multiple GANs. 
However, the imbalance of training data would be a more serious issue for CGANs than it is for MultiGAN. The data disparity between horizontal/corner segments is nearly 150 to 1.

%so it depends on first solving the data problem. Data for corners is severely disproportionate to the vast data available for horizontal segments, with a disparity of almost $150$ to $1$. 

%%%% Pretty sure I addressed this
%This problem is in large part due to the nature of how those segments are collected. Since the corner segments have to lead into the other segments, there is less freedom to collect them one frame at a time, so they were collected as individual corners, where each corner training data is wholly distinct from the others. However, collecting more data of the corner instead of only using one segment could circumvent this issue. This differs from the horizontal set in that each training screen is only different by any one row or column. In order for the Conditional GAN to work, the discrepancy must be taken into account.

Larger training sets could solve these problems.
There are many possible sources for Mega Man levels, such as the many other games in the Mega Man franchise. %, and those from the Nintendo are similar in their level structure and game dynamics. 
Data for games beyond Mega Man 1 are not in VGLC, but such a data set could be constructed. A more readily available source of levels is on the Mega Man Maker servers. These can be freely downloaded, and if properly simplified, could be converted into a format usable for training.
% \todo{This comment will only make sense if the relative complexity of MMM (relative to VGLC) is discussed earlier} 
With a large enough training set, MultiGAN should be able to produce greater diversity in its corner segments. However, more data in general means there will still be an imbalance with respect to corner segments, so simply using more data still might not make CGANs easier to apply.

In lieu of more training data, better corner diversity could be achieved by being more permissive about what counts as a corner segment. Currently, a sequence of horizontal and vertical window slices lead into and out of every occurrence of a corner segment. However, the three or four window slices surrounding each corner segment could potentially serve as viable corner segments as well. For example, if a corner segment has a floor section that is four tiles deep, then sliding the window up three tiles still leaves a floor at the bottom of the screen. Ladders and platforms for vertical movement would remain. Including such segments in the training sets for corners would increase their sizes without resulting in corner segments that could not be traversed. These new members of the corner sets could also be removed from horizontal and vertical data sets, slightly improving the balance of data. 

%%% We don't need to say this.
%However, this solution still might not solve the diversity problem.

%%% Sort of merged into two paragraphs above
%Training segments based off of online levels would be more effective in granting diversity in all segments, as there is more data to train on. Another approach may be to use segment diversity as a part of objective evolution, which could provide more interesting corner segments without having to change trained GANs.

A larger problem affecting OneGAN and, to a lesser extent, MultiGAN, is the issue of continuity between adjacent segments. The levels in the training set are all stylistically different,
so if one simply stitched together a series of horizontal segments from different levels, the result would likely not be cohesive. 
To some extent, use of proper fitness functions during evolution creates some cohesion, but it would be easier if evolution did not need to make up for shortcomings in the GAN and genotype encoding.

A recently developed approach that could address this issue is CPPN2GAN \cite{schrum:gecco2020cppn2gan}, which uses Compositional Pattern Producing Networks (CPPNs~\cite{stanley:gpem2007}) to generate GAN latent inputs. CPPN outputs vary gradually as a function of segment location within the level, and since similar latent vectors result in similar GAN outputs, adjacent segments should be linked in a more cohesive way. This approach has the added benefit of scale, because levels of arbitrary size can be generated by a compact CPPN.

%%%% Covered CPPN2GAN above
%In the future, experiments with Compositional Pattern Producing Networks (CPPNs) to generate levels segments using the base of different mathematical functions. Interesting results may come of this since CPPNs produce similar latent vectors to send to the GAN, making similar segments somewhat redundant. This may still be interesting to play insofar as having somewhat of a theme across certain segments, switching to a new one when traversing to a new segment, then going back. Combining CPPNs and Direct Vectors could give the themed segments more diversity and novelty, as seen in Schrum et al. ~\cite{schrum:gecco2020cppn2gan}. 

Because MultiGAN produced less variety in its corner segments, its levels were slightly less diverse than OneGAN's. Having better training data could fix this, but another way to increase diversity is to explicitly favor it during evolution. Quality diversity algorithms like MAP-Elites~\cite{mouret:arxiv15} could help in discovering such diverse levels, as has already been done in other GAN-based approaches \cite{schrum:gecco2020cppn2gan,fontaine2020illuminating,steckel2021illuminating}.

%%%% This was my shot at introducing MAP Elites, but I dropped it (See below)
%As quality diversity approaches have proven useful in generating diverse levels with GAN-based PCGML \cite{schrum:gecco2020cppn2ga,fontaine2020illuminating}, it makes sense to apply MAP-Elites to Mega Man as well. The concept of diversity in MAP-Elites evolution is determined by how the space of possible phenotypes is discretized into bins. Cohesion and other properties could be defined as binning dimensions

%%% Although I think MAP Elites is a good idea, it doesn't really fit 
%%% into the discussion. It's a fun idea, but I don't think it directly addresses
%%% The issues that the paper has been focusing on so far.
%Finally, MAP-Elites could be used as a binning scheme to sort the generated levels into types based on additional properties (ie. number of enemies, number of moving platforms, etc). After being binned, additional levels generated that have the same binning specifications but a higher fitness will replace the level currently in the bin, and evolve levels that way. 

\section{Conclusion}

When using GANs to generate levels with various segment types, it helps to use multiple GANs. Doing so preserves the structure of each segment type. In Mega Man, each segment type affects how adjacent segments connect.
If poorly connected segments are generated, as with OneGAN, then Mega Man cannot properly traverse the entire level.
%If segments that do not connect in the proper way are generated, as with OneGAN, then Mega Man cannot properly traverse the entire level.
MultiGAN is proposed to allow generation of the right type of segment whenever needed. 
MultiGAN was effective in producing levels with longer solution paths going through all available segments, in a way reminiscent of human-designed levels from the original game. 
In fact, a significant number of human subjects confirmed that MultiGAN levels had more human-like design and were more fun, in contrast to OneGAN levels which often had unusual barriers, unreachable segments, and overall stranger level design. 
MultiGAN shows promise for the generation of more complex, challenging, and cohesive levels, and future extensions to the approach should result in more diverse level designs as well.

%Using OneGAN offered more uniformity from segment to segment, but only in that both were equally chaotic and random, following no structure or uniformity. Since the data training samples were small with some GANs in the MultiGAN method, those segments were sometimes repetitive. The MultiGAN method shows promise for generation of more complex yet cohesive levels.

\begin{acks}
%%% Will comment from submission, but wanted to have an idea of space

This research was made possible by the donation-funded Summer Collaborative Opportunities and Experiences (SCOPE) program for undergraduate research at Southwestern University

\end{acks}
