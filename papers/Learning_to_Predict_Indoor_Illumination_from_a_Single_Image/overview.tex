\section{Method overview}

Our goal, illustrated in fig.~\ref{f:overview}, is to predict the HDR lighting conditions from a single photograph. If we cast our goal as a learning problem, training data in the form of $\{\textrm{photo}, \textrm{HDR light probe}\}$ pairs would be ideal for the learning task. Given sufficient data, one could try to regress the HDR light probe directly from the photo. However, such data does not exist currently, and gathering it in sufficient quantity is prohibitively expensive. On the other hand, large datasets of LDR panoramas already exist~\cite{xiao-cvpr-12}, and since they capture the entire environment of the scene, they can potentially be used to learn illumination. To generate input data for training, we extract rectified crops from these panoramas at various orientations and focal lengths, and attempt to learn the relationship between the crops and the panoramas (see fig.~\ref{f:overview}). 

%Hence, we must resort to LDR panoramas as they are much easier to gather and large datasets already exist. To generate input data for training, we crop photos from the panoramas. 

Unfortunately, we cannot directly learn indoor lighting from LDR panoramas since their low dynamic range does not capture lighting properly~\cite{debevec-siggraph-97}. In addition, indoor illumination tends to be localized, i.e., the light sources cannot be assumed to be directional (as is the case outdoors~\cite{holdgeoffroy-cvpr-17}). The center of projection of the panorama can be arbitrarily far away from a point in the cropped scene; as a result, the true illumination incident at this point is, at the very least, a \emph{warped} version of the panorama. 

Given these limitations, we propose using the LDR data to train a network to identify the \emph{location} of light sources in the scene. We do so using two novel, practical solutions to deal with the aforementioned issues. First, we introduce in sec.~\ref{sec:lightdetection} a robust method to detect light sources in LDR panoramic images. Second, we introduce a method to warp the panorama such that it better reflects the lighting conditions at the cropped scene (sec.~\ref{sec:warping}). %photo location \KS{use different term?} (sec.~\ref{sec:warping}). 

These solutions allow us to use a large dataset of LDR panoramas~\cite{xiao-cvpr-12} to learn to predict indoor lighting from photographs. To this end, we introduce in sec.~\ref{sec:learning} an end-to-end convolutional neural network which produces a binary light mask indicating the light positions, as well a low-resolution RGB approximation of the entire panorama. Since using a standard loss function to learn a binary light mask heavily penalizes even small shifts of a light source position, we introduce an in-network, differentiable cosine filter which enables efficient learning. 

In addition to estimating the \emph{positions} of light sources, we need to estimate the \emph{intensities} of these lights to recover complete scene illumination. To achieve this, we captured a new dataset of $2,100$ high-quality HDR environment maps spanning a wide range of scenes and illumination conditions. While this dataset is too small to train a network from scratch, we show that it is sufficient to fine-tune our pre-trained light position prediction network to infer light intensities (sec.~\ref{sec:hdr}). The fine-tuned network produces a light intensity map and RGB panorama that can be combined to create a final HDR environment map (sec.~\ref{sec:experiments}), which in turn can be used to relight virtual objects into the input photo.

% Throughout this paper, and following~\cite{xiao-cvpr-12}, we use the term \emph{photo} to refer to a standard limited-field-of-view image as taken with a normal camera, and the term \emph{panorama} to denote a spherical $360^\circ$ panoramic image.

