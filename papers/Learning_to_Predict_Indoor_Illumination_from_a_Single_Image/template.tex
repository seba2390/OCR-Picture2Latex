\documentclass[acmtog]{acmart}

\usepackage{booktabs} % For formal tables


\makeatletter
\def\runningfoot{\def\@runningfoot{}}
\def\firstfoot{\def\@firstfoot{}}
\makeatother
\settopmatter{printacmref=false} % Removes citation information below abstract
\renewcommand\footnotetextcopyrightpermission[1]{} % removes footnote with conference information in first column
\pagestyle{plain} % removes running headers

% TOG prefers author-name bib system with square brackets
\citestyle{acmauthoryear}
\setcitestyle{square}

\usepackage[ruled]{algorithm2e} % For algorithms
\renewcommand{\algorithmcfname}{ALGORITHM}
\SetAlFnt{\small}
\SetAlCapFnt{\small}
\SetAlCapNameFnt{\small}
\SetAlCapHSkip{0pt}
\IncMargin{-\parindent}

% Metadata Information

\setcopyright{none}
%\acmJournal{TOG}
%\acmYear{2017}\acmVolume{36}\acmNumber{6}\acmArticle{176}\acmMonth{11}
%\acmDOI{10.1145/3130800.3130891}



\makeatletter
\newcommand\footnoteref[1]{\protected@xdef\@thefnmark{\ref{#1}}\@footnotemark}
\makeatother

\usepackage{steinmetz}
\usepackage{physics}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{gensymb}
%\usepackage[usenames,dvipsnames]{color}
\usepackage{mathtools}
\usepackage{multirow}
%\usepackage{algorithm}
%\usepackage{algorithmic}
\usepackage[english]{babel} % English language/hyphenation

\newcommand{\eg}{{\em e.g.,~}}
\newcommand{\ie}{{\em i.e.,~}}
\newcommand{\etc}{{\em etc.}}
\newcommand{\etal}{{\em et~al.~}}
\newcommand{\superscript}[1]{\ensuremath{^{\textrm{#1}}}}
\newcommand{\subscript}[1]{\ensuremath{_{\textrm{#1}}}}

\newcommand{\fn}[1]{Figure~\ref{fig:#1}}
\newcommand{\en}[1]{Equation~\ref{eqn:#1}}
\newcommand{\sn}[1]{Section~\ref{sec:#1}}
\newcommand{\tn}[1]{Table~\ref{tab:#1}}
\newcommand{\cn}[1]{Section~\ref{chap:#1}}
\newcommand{\an}[1]{Appendix~\ref{apnd:#1}}
\newcommand{\syn}[1]{{\em #1}}
\newcommand{\iu}{{i\mkern1mu}}

\DeclareMathOperator*{\argmin}{arg\,min}

% \newcommand{\ersin}[1]{\textcolor{red}{[\textsc{ERSIN}: \emph{#1}]}}
\newcommand{\JF}[1]{\textcolor{red}{[\textsc{JF}: \emph{#1}]}}
\newcommand{\MAG}[1]{\textcolor{green}{[\textsc{MAG}: \emph{#1}]}}
\newcommand{\KS}[1]{\textcolor{blue}{[\textsc{KS}: \emph{#1}]}}
% \newcommand{\emiliano}[1]{\textcolor{blue}{[\textsc{Emiliano}: \emph{#1}]}}
% \newcommand{\CG}[1]{\textcolor{red}{[\textsc{CG}: \emph{#1}]}}


% Document starts
\begin{document}

% Title portion
\title{Learning to Predict Indoor Illumination from a Single Image}

\author{Marc-Andr\'e Gardner}
\affiliation{Universit\'e Laval}
\email{marc-andre.gardner.1@ulaval.ca}
\author{Kalyan Sunkavalli}
\author{Ersin Yumer}
\author{Xiaohui Shen}
\author{Emiliano Gambaretto}
\affiliation{Adobe Research}
\author{Christian Gagn\'e}
\author{Jean-Fran\c cois Lalonde}
\affiliation{Universit\'e Laval}

\renewcommand\shortauthors{}

\begin{abstract}
We propose an automatic method to infer high dynamic range illumination from a single, limited field-of-view, low dynamic range photograph of an indoor scene.
%Inferring scene illumination from a single photograph is a challenging problem; the pixel intensities observed in a photograph are a complex function of scene geometry, reflectance properties, and illumination, all of which are unknown in our case. 
In contrast to previous work that relies on specialized image capture, user input, and/or simple scene models, we train an end-to-end deep neural network that directly regresses a limited field-of-view photo to HDR illumination, without strong assumptions on scene geometry, material properties, or lighting. We show that this can be accomplished in a three step process: 1) we train a robust lighting classifier to automatically annotate the location of light sources in a large dataset of LDR environment maps, 2) we use these annotations to train a deep neural network that predicts the location of lights in a scene from a single limited field-of-view photo, and 3) we fine-tune this network using a small dataset of HDR environment maps to predict light intensities. This allows us to automatically recover high-quality HDR illumination estimates that significantly outperform previous state-of-the-art methods. Consequently, using our illumination estimates for applications like 3D object insertion, produces photo-realistic results that we validate via a perceptual user study.

%\MAG{We capitalize on the high number of low-dynamic range panoramas available to train a network and then fine tune it using a limited HDR dataset.} 

 
\end{abstract}

%
% The code below should be generated by the tool at
% http://dl.acm.org/ccs.cfm
% Please copy and paste the code instead of the example below. 

%
% End generated code
%
%
% The code below should be generated by the tool at
% http://dl.acm.org/ccs.cfm
% Please copy and paste the code instead of the example below. 
%
\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10010147.10010178.10010224.10010225.10010227</concept_id>
<concept_desc>Computing methodologies~Scene understanding</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10010147.10010371.10010382</concept_id>
<concept_desc>Computing methodologies~Image manipulation</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10010147.10010371.10010382.10010236</concept_id>
<concept_desc>Computing methodologies~Computational photography</concept_desc>
<concept_significance>300</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Computing methodologies~Scene understanding}
\ccsdesc[500]{Computing methodologies~Image manipulation}
\ccsdesc[300]{Computing methodologies~Computational photography}
%
% End generated code
%
\keywords{indoor illumination, deep learning}

%\thanks{} 

\begin{teaserfigure}
\centering
\setlength{\fboxsep}{0pt}%
\setlength{\fboxrule}{1pt}%
  \includegraphics[width=0.24\textwidth]{images/stock-composites/35-crop.jpg}
	\begin{picture}(0,0)
		\put(-126,63){\fcolorbox{white}{white}{\includegraphics[height=1cm]{images/stock-composites/35-jpeg_envmap.jpg}}}
	\end{picture}
  \includegraphics[width=0.24\textwidth]{images/stock-composites/35-plants.jpg} 
	\hspace{0.2cm}
  \includegraphics[width=0.24\textwidth]{images/stock-composites/10-crop.jpg} 
	\begin{picture}(0,0)
		\put(-126,63){\fcolorbox{white}{white}{\includegraphics[height=1cm]{images/stock-composites/10-jpeg_envmap.jpg}}}
	\end{picture}
  \includegraphics[width=0.24\textwidth]{images/stock-composites/10-bicycle.jpg}
\caption{Given a single LDR image of an indoor scene, our method automatically predicts HDR lighting (insets, tone-mapped for visualization). Our method learns a direct mapping from image appearance to scene lighting from large amounts of real image data; it does not require any additional scene information, and can even recover light sources that are not visible in the photograph, as shown in these examples. Using our lighting estimates, virtual objects can be realistically relit and composited into photographs.}
\label{f:teaser}
\end{teaserfigure}
 
\maketitle


\vspace{15em}
\input{intro.tex}
\input{background.tex}
\input{overview.tex}
\input{lightdetection.tex}
\input{warping.tex}
\input{learning.tex}
\input{hdr.tex}
\input{results.tex}
\input{discussion.tex}

\section*{Acknowledgements}

The authors would like to thank Yannick Hold-Geoffroy for his help in setting up the renders and the user study. We would also like to thank Henrique Weber for his help with EnvyDepth, and Jean-Michel Fortin for his work on HDR data capture.

Parts of this work were done while Marc-Andr\'e Gardner was an intern at Adobe Research. This work was partially supported by the REPARTI Strategic Network and the FRQNT New Researcher Grant 2016NC189939. We gratefully acknowledge the support of Nvidia with the donation of the GPUs used for this research, funding from Adobe to cover the cost of HDR dataset acquisition, as well as a generous gift from Adobe to J.-F. Lalonde. 

\bibliographystyle{ACM-Reference-Format}
\bibliography{template}
\end{document}