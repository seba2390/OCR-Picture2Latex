\section{Experiments}
\label{sec:experiments}

In this section, we evaluate our approach in several different ways. First, we present light prediction results on the SUN360 dataset (LDR data), where the lights found by the detector of sec.~\ref{sec:lightdetection} are treated as ground truth. Then, we show the results of the HDR fine-tuning procedure, comparing our results with actual HDR ground truth. Finally, we compare our technique to previous work and present a user study comparing the performance of each method at relighting virtual objects. In the following analysis, the term ``LDR network'' refers to the network trained on the SUN360 dataset which recovers a binary light mask (sec.~\ref{sec:learning}), and ``HDR network'' refers to the network finetuned on the HDR dataset which recovers the light intensity (sec.~\ref{sec:hdr}). Note that when not otherwise specified, every lighting estimate shown in the paper and the supplementary material has been computed \emph{completely automatically}. The only manual intervention in these results has been, in some cases, to place virtual objects in the scene. Please refer to the supplementary material for more results for each step.

% We close by showing some interesting insights about the network internals, and provide many more qualitative examples. 

\subsection{Evaluation of the LDR network}

We provide a qualitative way of evaluating the LDR network ability to estimate the light direction from single images by rendering a virtual bunny model into the image. To do so, a coarse environment map is obtained by thresholding the light mask $\mathbf{x}_\text{mask}$ (at $t > 0.5$), detecting the connected components, and assigning a weight corresponding to the mean light probability from $\mathbf{x}_\text{mask}$ to each component. This modified $\mathbf{x}^*_\text{mask}$ is combined with the RGB panorama $\mathbf{x}_\text{RGB}$ into a single environment map by:
%
\begin{equation}
\mathbf{x}_\text{combined} = \lambda_\text{mask} \mathbf{x}^*_\text{mask} + \lambda_\text{RGB}(1-\mathbf{x}^*_\text{mask}) \mathbf{x}_\text{RGB} \,,
\label{e:ldr-envmap}
\end{equation}
%
where the parameters $\lambda_\text{mask}$ and $\lambda_\text{RGB}$ are set to 500 and 1, respectively (these choices are arbitrary and used only to visualize the LDR network capabilities at predicting the positions of light sources). As can be seen in fig.~\ref{f:relighting-bunnies}, our LDR network successfully localizes light sources even in images with small field-of-view and few visually obvious illumination cues. 

\begin{figure}[!t]
\centering
\footnotesize
\setlength{\tabcolsep}{1pt}
\begin{tabular}{cc}
\includegraphics[height=2.5cm]{{images/bunnyRenders/pano_abumqtqhptujdn-kitchen-135-1.06244-0.98464_input}.jpg} & 
\includegraphics[height=2.5cm]{{images/bunnyRenders/pano_abumqtqhptujdn-kitchen-135-1.06244-0.98464_mask}.jpg} \\
\end{tabular}
\caption{Effect of occlusions on the LDR network predictions. While our warping operator does not handle light occlusions, our network is able to generalize beyond these rare instances. Here, even though the window causing the shadows on the handle in the image (left) is occluded in the panorama (right), our network places the highest probability of a light in this direction, thus producing results that are ``better'' than the ground truth.}
\label{f:occluded-lights}
\end{figure}

\paragraph{Handling occluded lights} In sec.~\ref{sec:warping}, we noted that our warping operator is an approximation that does not handle occluded light sources. However, this scenario is rare, and as a result our network is able to robustly generalize beyond these erroneous input data. This is demonstrated in Fig.~\ref{f:occluded-lights}, where our network predicts lights that are not in the ground truth annotation because of occlusion, but are consistent with the shading and shadows observed in the cropped photo. 

% \paragraph{Qualitative relighting results on SUN360 images} 

% While this LDR network is only trained to \emph{localize} lights from a large dataset of LDR data, we can convert its output to an HDR environment map suitable for relighting, by combining the predicted light mask $\mathbf{x}_\text{mask}$ and RGB panorama $\mathbf{x}_\text{RGB}$ as:
% %
% \begin{equation}
% \mathbf{x}_\text{combined} = \lambda_\text{mask} \mathbf{x}_\text{mask} + \lambda_\text{RGB}(1-\mathbf{x}_\text{mask}) \mathbf{x}_\text{RGB} \,,
% \label{e:final-envmap}
% \end{equation}
% %
% where $\lambda_\text{mask}$ and $\lambda_\text{RGB}$ are weights that control the relative importance of the light sources with respect to the background color. These weights are estimated with a three-step procedure. 

% While a set of constant weights may be sufficient for some applications, for relighting it may be preferable to \emph{individually} tune the intensity of each light source. This is particularly true in the case of complex scenes, where some lights may contribute differently to the lighting on the scene. 

% We estimate these weights in a three-step procedure. First, we threshold the light mask $\mathbf{x}_\text{mask}$ and cluster it by detecting the connected components. We assume each of these components to be a light source, with a separate weight. This weight is estimated by using the network output value: while the task is to predict a binary mask, the output layer makes use of a sigmoid activation function so that it can generate any value over $[0,1]$. The output value, in addition to the prediction, can be interpreted as the network \textit{confidence} on a pixel prediction. We use the mean value of each cluster as weight, with the rationale that a more intense light source will have more effects on the scene and thus increase the network confidence. We also take into account the special case of the lights inside the photo, which usually have high confidence values since the network may directly observe them, and we decrease their weights accordingly. Overall, the value of a particular weight may be expressed as the following: \MAG{TODO equation} 


% While this is a very coarse approximation, this approach already gives decent results in a fully automated way. Some render examples can be seen in Fig.\MAG{TODO}, and 500 others, randomly sampled from the test set, are provided as supplementary material. It is to be noted that absolutely no human intervention has been made on these renders, including at the compositing step: the same settings are using through all the renders.

\subsection{Evaluation of the HDR network}
\label{ss:hdreval}
In this section, a more thorough evaluation of the HDR network, including quantitative and qualitative results, is provided. 

\paragraph{Quantitative evaluation}

We begin by evaluating the performance of the HDR network on the HDR test set. Fig.~\ref{f:results-loss-histogram} shows the distribution of the HDR losses (eq.~\ref{e:hdrloss}) on our test set of 2,100 images (15\% of 14,000), and fig.~\ref{f:results-loss-qualitative} shows qualitative examples corresponding to various percentiles of this distribution. In this figure, note that the range of ground truth log-intensities is $[0.04, 3.01]$. Since we use a L2 loss (mean squared error), a value of $\mathcal{L} = 0.02$ (as in fig.~\ref{f:results-loss-qualitative}-(a)) thus indicates a global relative error of around $4.7\%$.

Even though the network has trouble in pinpointing the exact location of small, concentrated light sources (e.g. fig.~\ref{f:results-loss-qualitative}-(b)), it succeeds in finding the dominant light sources in the scene, even when they are located outside the field of view of the photo (as is the case in most of the examples of fig.~\ref{f:results-loss-qualitative}). Larger errors typically occur when the scene is lit by a very large area light sources, such as the window in fig.~\ref{f:results-loss-qualitative}-(e). 
% The light masks $\mathbf{x}_\text{RGB}$ of other examples are also shown in fig.~\ref{f:results-renders-sun360}. Note that the light masks are shown \emph{after} being filtered by eq.~(\ref{e:filter}), with $e=70$.


\paragraph{Virtual object relighting} 

The HDR network output can directly be used to generate an HDR environment map suitable for relighting, by combining its two outputs like so: 
%
\begin{equation}
\mathbf{x}_\text{combined} = 10^{\mathbf{x}_\text{mask}} + \mathbf{x}_\text{RGB} \,,
\label{e:hdr-envmap}
\end{equation}
%
since the HDR network is trained to output \emph{log}-intensity. As a post-process we matched the mean RGB value of the RGB prediction and the color of the light source to the mean RGB value of the input image (following the assumption of Gray World surface reflectance, this value captures the color of the illumination).

Fig.~\ref{f:results-comparison} presents virtual objects inserted in crops extracted from our HDR test set, so that they can be compared with relighting with their ground truth lighting. The network predictions yields convincing relighting results that are very close to ground truth.

\begin{figure}[!t]
\centering 
\includegraphics[width=.72\linewidth]{images/results/results-histogram-crop.pdf}
\caption{Histogram (red) and cumulative histogram (blue) of the loss on the log-intensity (\ref{e:hdrloss}) over the HDR test set, after convergence ($e = 70$). See fig.~\ref{f:results-loss-qualitative} for qualitative examples corresponding to different loss percentiles.}
\label{f:results-loss-histogram}
\end{figure}

\begin{figure*}[!t]
\centering
\footnotesize
\setlength{\tabcolsep}{1pt}
\begin{tabular}{ccc}
\includegraphics[width=.33\linewidth]{images/lightIntensity/results-p1.pdf} &
\includegraphics[width=.33\linewidth]{images/lightIntensity/results-p10.pdf} &
\includegraphics[width=.33\linewidth]{images/lightIntensity/results-p25.pdf} \\
(a) $p = 1, \mathcal{L} = 0.02$ & (b) $p = 10, \mathcal{L} = 0.04$ & (c) $p = 25, \mathcal{L} = 0.07$ \\*[.5em]
\includegraphics[width=.33\linewidth]{images/lightIntensity/results-p50.pdf} &
\includegraphics[width=.33\linewidth]{images/lightIntensity/results-p75.pdf} &
\includegraphics[width=.33\linewidth]{images/lightIntensity/results-p100.pdf} \\
(c) $p = 50, \mathcal{L} = 0.13$ & (d) $p = 75, \mathcal{L} = 0.27$ & (e) $p = 100, \mathcal{L} = 3.1$
\end{tabular}
\caption{Qualitative light intensity and RGB predictions on examples from the HDR test set. For each example, we show (middle) the input image, (top) the ground truth log-intensity $\mathbf{t}_\text{int}$ and RGB panoramas $\mathbf{t}_\text{RGB}$, and (bottom) the corresponding predictions from the HDR network $\mathbf{x}_\text{int}$ and $\mathbf{x}_\text{RGB}$. Light intensities are color-coded from yellow (high intensity) to blue (low intensity). The examples are sorted by error percentile $p$ on the loss (\ref{e:hdrloss}) from top-left to bottom-right in reading order. See fig.~\ref{f:results-loss-histogram} for the complete error distribution on the test set.} 
\label{f:results-loss-qualitative}
\end{figure*}

\paragraph{Global intensity scaling}

Recovering the \emph{absolute} illumination intensities from an uncalibrated LDR image is an ill-posed problem, since many combinations of light intensities and camera parameters (shutter speed, ISO, etc.) may result in the exact same image. Thus, an object lit with our network estimate is sometimes too dark or too bright. Given that the network recovers correct \emph{relative} illumination---that is, the ratio between intensities in different parts of the panorama is accurate---fixing this issue boils down to the selection of a \emph{single}, global intensity scaling parameter. This parameter can be easily specified and is often one of the controls most production compositors offer users to ensure that their renders are properly exposed (even when relighting with ground truth IBLs). %Alternatively, this tuning could also be carried out automatically, assuming we have access to the camera parameters.

In addition to the automatically estimated results in fig.~\ref{f:results-comparison}, we also provide a set of results (fig.~\ref{f:results-comparison}-(c)) where this global intensity scaling has been manually specified. Many more results can be found in the supplementary material, including an example of the effect of this scaling parameter on the appearance of a composite. While our automatic estimate is reasonable in many cases, slightly tuning this scale factor can produce compelling results in almost all cases. Note that all other results in this paper, including comparisons with previous methods, use our \emph{automatic} light estimates.

Finally, we also provide qualitative examples of objects inserted in generic stock photos downloaded from the Internet in fig.~\ref{f:results-renders-stock}. The network is able to obtain robust illumination estimates, even for images with varying fields-of-view and viewpoints, uncontrolled capture settings, and unknown post-processing---all factors that lie outside our HDR training set.


\begin{figure*}[!t]
\centering
\footnotesize
\setlength{\tabcolsep}{1pt}
\begin{tabular}{cccc}
%
\includegraphics[width=.244\linewidth]{images/stock-composites/07-crop.jpg} & 
\includegraphics[width=.244\linewidth]{images/stock-composites/07-drafting-desk.jpg} & 
\hspace{.4em}
\includegraphics[width=.244\linewidth]{images/stock-composites/16-crop.jpg} & 
\includegraphics[width=.244\linewidth]{images/stock-composites/16-jar-trophy.jpg}\vspace{.4em} \\
%
\includegraphics[width=.244\linewidth]{images/stock-composites/18-crop.jpg} & 
\includegraphics[width=.244\linewidth]{images/stock-composites/18-clock-new.jpg} & 
\hspace{.4em}
\includegraphics[width=.244\linewidth]{images/stock-composites/29-crop.jpg} & 
\includegraphics[width=.244\linewidth]{images/stock-composites/29-new.jpg}\vspace{.4em} \\
%
\includegraphics[width=.244\linewidth]{images/stock-composites/17-crop.jpg} & 
\includegraphics[width=.244\linewidth]{images/stock-composites/17-ladder-new.jpg} & 
\hspace{.4em}
\includegraphics[width=.244\linewidth]{images/stock-composites/03-crop.jpg} & 
\includegraphics[width=.244\linewidth]{images/stock-composites/03-books.jpg}\vspace{.4em}\\
%
(a) Input photo & 
(b) Relit by our estimate & 
\hspace{.5em}
(c) Input photo & 
(d) Relit by our estimate \\
\end{tabular}
\caption{Object relighting on a variety of generic stock photos downloaded from the Internet. In all cases, light estimation is performed completely automatically by our HDR network, the output of which is directly used by the rendering engine to relight the virtual objects.}
\label{f:results-renders-stock}
\end{figure*}

\begin{figure*}
\centering
\footnotesize
\setlength{\tabcolsep}{1pt}
\begin{tabular}{ccccc}
%
\includegraphics[width=.195\linewidth]{images/renders-new/userstudy/scene67_groundtruth.jpg} & 
\includegraphics[width=.195\linewidth]{images/renders-new/userstudy/scene67_hdrnetwork.jpg} & 
\includegraphics[width=.195\linewidth]{images/renders-new/userstudy/scene67_artist.jpg} & 
\includegraphics[width=.195\linewidth]{images/renders-new/userstudy/scene67_khan.jpg} & 
\includegraphics[width=.195\linewidth]{images/renders-new/userstudy/scene67_karsch.jpg} \\
%
% \includegraphics[width=.25\linewidth]{images/renders-new/userstudy/scene3_groundtruth.jpg} & 
% \includegraphics[width=.25\linewidth]{images/renders-new/userstudy/scene3_hdrnetwork.jpg} & 
% \includegraphics[width=.25\linewidth]{images/renders-new/userstudy/scene3_khan.jpg} & 
% \includegraphics[width=.25\linewidth]{images/renders-new/userstudy/scene3_karsch.jpg} \\
%
\includegraphics[width=.195\linewidth]{images/renders-new/userstudy/scene13_groundtruth.jpg} & 
\includegraphics[width=.195\linewidth]{images/renders-new/userstudy/scene13_hdrnetwork.jpg} & 
\includegraphics[width=.195\linewidth]{images/renders-new/userstudy/scene13_artist.jpg} & 
\includegraphics[width=.195\linewidth]{images/renders-new/userstudy/scene13_khan.jpg} & 
\includegraphics[width=.195\linewidth]{images/renders-new/userstudy/scene13_karsch.jpg} \\
%
\includegraphics[width=.195\linewidth]{images/renders-new/userstudy/scene27_groundtruth.jpg} & 
\includegraphics[width=.195\linewidth]{images/renders-new/userstudy/scene27_hdrnetwork.jpg} & 
\includegraphics[width=.195\linewidth]{images/renders-new/userstudy/scene27_artist.jpg} & 
\includegraphics[width=.195\linewidth]{images/renders-new/userstudy/scene27_khan.jpg} & 
\includegraphics[width=.195\linewidth]{images/renders-new/userstudy/scene27_karsch.jpg} \\
% %
% \includegraphics[width=.195\linewidth]{images/renders-new/userstudy/scene22_groundtruth.jpg} & 
% \includegraphics[width=.195\linewidth]{images/renders-new/userstudy/scene22_hdrnetwork.jpg} & 
% \includegraphics[width=.195\linewidth]{images/renders-new/userstudy/scene22_artist.jpg} & 
% \includegraphics[width=.195\linewidth]{images/renders-new/userstudy/scene22_khan.jpg} & 
% \includegraphics[width=.195\linewidth]{images/renders-new/userstudy/scene22_karsch.jpg} \\
%
(a) Ground truth lighting &
(b) Our HDR network & 
(c) HDR network, intensity tuned & 
(d) \cite{khan-siggraph-06} & 
(e) \cite{karsch-tog-14}
\end{tabular}
\caption[]{Comparison of (b) our method and (c) our method with a single intensity factor humanly tuned with (a) ground truth lighting, (c) \cite{khan-siggraph-06} and (d) \cite{karsch-tog-14} on virtual object relighting. While our results sometimes visually differ from ground truth, they yield realistic object insertion results. In contrast, Khan et al.~\shortcite{khan-siggraph-06} do not estimate HDR lighting, so renders look flat. Since Karsch et al.~\shortcite{karsch-tog-14} rely on intrinsic decomposition, geometry estimation and inverse lighting, we found the method to be quite sensitive to errors in any one of these steps. Therefore, renders are often much too bright or dark. More results available in the supplementary material.}
\label{f:results-comparison}
% \vspace*{-0.9em}
\end{figure*}

\begin{figure}
\centering
\footnotesize
\setlength{\tabcolsep}{1pt}
\begin{tabular}{cc}
\includegraphics[width=.49\linewidth]{images/renders-new/userstudy/scene17_groundtruth.jpg} & 
\includegraphics[width=.49\linewidth]{images/renders-new/userstudy/scene17_hdrnetwork.jpg} \\
\includegraphics[width=.49\linewidth]{images/renders-new/userstudy/scene111_hdrnetwork.jpg} & 
\includegraphics[width=.49\linewidth]{images/renders-new/userstudy/scene111_groundtruth.jpg} \\
\end{tabular}
\caption[]{For each row, the virtual objects in the image are either lit by the ground truth or by the output of our HDR network. Can you guess which is which? Answers below. \\
\rotatebox{180}{First row: left is GT, second row: right is GT. Did you get it right?}}
\label{f:results-userstudy}
% \vspace{-.5em}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=0.9\columnwidth]{images/user_study_results_sep.pdf}
\caption{Each method that participated in the user study is shown as a column, where blue votes indicate that user preferred the method instead of the ground truth, whereas red votes indicate that the user did not have a strong preference w.r.t the ground truth and the method.}
\label{f:results-us-bars}
\end{figure}

\subsection{Comparison with previous work}

We compare our approach with that of Khan et al.~\shortcite{khan-siggraph-06} and Karsch et al.~\shortcite{karsch-tog-14}, and show comparative relighting results in fig.~\ref{f:results-comparison}. Khan et al.~\shortcite{khan-siggraph-06} estimate the illumination conditions by projecting the background image directly on a sphere and flip it to get the whole environment map. This fails to estimate the proper dynamic range and position of light sources. In contrast, our approach produces robust estimates of lighting direction and intensity even when the light is not visible in the image. 

Karsch et al.~\shortcite{karsch-tog-14} use a light classifier to detect in-view lights, estimate out-of-view light locations by matching the background image to a database of panoramas, and estimate light intensities using a rendering-based optimization. We used the authors' original code to estimate both in-view and out-of-view lighting and these are shown in fig.~\ref{f:results-comparison}-(e). Their panorama matching is based on image appearance features that are not necessarily correlated with scene illumination. As a results, while their technique sometimes retrieves good matches, it may retrieve arbitrarily bad matches. In this case, adjusting the light intensities may not converge to satisfying answers since the light sources are not allowed to move in their optimization. In addition, their inverse lighting approach relies on reconstructing the depth and the diffuse albedo of the scene. Both of these are challenging problems, and errors in these estimates lead to errors in their lighting predictions. In contrast, our method learns a direct mapping between image appearance and scene illumination and yields robust, accurate results. 

\subsection{User study}

In addition to inferring scene illumination, we are also interested in using these estimates for graphics applications like 3D object insertion. This begs the questions, \emph{how realistic do synthetic objects lit by our estimates look when they are composited into input images?} We assess this axis of performance via a perceptual user study. We prepared 20 scenes from our HDR test set, and inserted a variety of different virtual objects in each of them. We generated a reference composite by relighting objects into these images using their ground truth illumination (obtained by warping the HDR panorama from which the image was extracted). We compared these results with objects that were relit using light probes estimated from four different methods: 1) our HDR network (\emph{without} artist tuning, see sec.~\ref{ss:hdreval}), 2) our LDR network, 3) \cite{khan-siggraph-06}, and 4) \cite{karsch-tog-14}. For each technique, we showed users a pair of images --- the reference image rendered with the ground truth illumination and the result rendered with one of the methods to be compared -- and asked them to indicate which image looks the most realistic. The realism of these composites is significantly affected by the geometric alignment of the objects and scenes, quality of the object geometries and materials, and rendering settings; our forced choice A/B test allows us to somewhat remove these factors and isolate the effect of the lighting estimation on the realism of the composite. Note that this test is only possible because we have high-quality ground truth illumination from our HDR dataset.

Two examples of the type of comparison asked of users is shown in fig.~\ref{f:results-userstudy}. The users were given the option to choose the more realistic image from the given two choices, and and additional third option to indicate that they are both equally plausible. Aggregated user study results are given in fig.~\ref{f:results-us-bars}. In total, we gathered responses from $105$ unique participants, with $1080$ comparisons with respect to the ground truth for each method. The results indicate that our HDR network rendering was considered as or more realistic than the ground truth result in $41.85\%$ of the responses, which is a significant improvement over Khan et al.~\shortcite{khan-siggraph-06} ($27.78\%$) and Karsch et al.~\shortcite{karsch-tog-14} ($16.76\%$). Note that even our LDR network ($27.32\%$) is comparable to Khan et al. and a significant improvement over Karsch et al.~\shortcite{karsch-tog-14}.

Beyond the significant improvement our automatic methods provides over these previous methods, note that our results can be further improved by simply tuning the global intensity scaling (see fig.~\ref{f:results-comparison}(c)). This is not possible for either Khan et al. (which cannot recover distinct lights to create effects like shadows) and Karsch et al. (which often gets the direction of the lights wrong and requires significant effort to correct).

% From the rebuttal: We ran a user study with 75 users (A/B testing) on 7 scenes comparing: ours (automatic baseline, obtained by setting light intensities according to network probabilities), ours (artist-tuned), Khan2006, and LDR panorama against HDR ground truth (GT). For each technique, we asked users to tell which image, between GT and estimation, looks the most realistic. Our methods (automatic, tuned) were preferred (46, 49%) of the time vs. GT: this suggests our results are indistinguishable from GT most of the time. Khan2006 obtained 29%, and LDR 40%.



% From Marc-André:
% pct0   -> 3.9326929470908318
% pct10  -> 13.921029532465953
% pct25  -> 24.964714554961091
% pct50  -> 44.241511125972764
% pct75  -> 73.33627948686771 
% pct100 -> 319.49644333657585



