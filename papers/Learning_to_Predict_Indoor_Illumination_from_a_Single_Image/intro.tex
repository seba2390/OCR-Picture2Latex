\section{Introduction}
\label{sec:introduction}

Inferring scene illumination from a single photograph is a challenging problem. The pixel intensities observed in an image are a complex function of scene geometry, materials properties, illumination, the imaging device, and subsequent post-processing. Disentangling one of these factors from another is an ill-posed inverse problem. This is especially hard from a \emph{single limited field-of-view image}, since many of the factors that contribute to the scene illumination are not even directly observed in the photo (Fig.~\ref{f:makethepoint}). This problem is typically addressed in two ways: first, by assuming that scene geometry (and/or reflectance properties) is given (either measured using depth sensors, reconstructed using other methods, or annotated by a user), and second, by imposing strong low-dimensional models on the lighting (e.g., low-frequency spherical harmonics). 

While we have made significant progress in single-image geometric reconstruction~\cite{eigen-iccv-15,bansal2016marr} and reflectance estimation~\cite{bell2015minc,zhou2015intrinsic}, state-of-the-art techniques are still significantly error-prone. These errors can then propagate into lighting estimates when they are directly used in a rendering-based optimization. Fundamentally, indoor lighting varies widely in its geometric and photometric properties; for example, the same scene can have large windows, bright spot lights, and diffuse lamps. This wide range of illuminants typically cannot be accurately represented by low-dimensional lighting models. 

This paper proposes a method to infer high dynamic range (HDR) illumination from a single, limited field-of-view, low dynamic range (LDR) photograph of an indoor scene. Our goal is to be able to model the range of typical indoor light sources, and choose a spherical environment map (or IBL) representation that is often used to represent real-world illumination~\cite{debevec-sig-98}. We also want to make this inference robust to errors in geometry, surface reflectance, and scene appearance models. To this end, we introduce an end-to-end learning based approach, that takes images as input and predicts illumination using deep neural networks.

\begin{figure}[!t]
\centering
\includegraphics[height=2.5cm]{images/pano_fig2_crop.jpg}
\includegraphics[height=2.5cm]{images/pano_fig2.jpg}
\caption{Extracting crops from panoramas: (left) normal lens crop image, (right) spherical panorama.}
\label{f:makethepoint}
\vspace{-.5em}
\end{figure}

Deep neural networks have been successfully applied to closely related problems such as depth estimation~\cite{eigen-iccv-15,bansal2016marr}, reflectance map estimation~\cite{rematas-cvpr-16}, and intrinsic images~\cite{zhou2015intrinsic}. In the vein of this previous work, we propose training a deep neural network to learn a representation for image appearance in terms of illumination. However, training such a network would require a large database of image-HDR illumination pairs. Such a dataset does not currently exist and would require a significant amount of time and effort to assemble. Instead, we resort to the large database of 360 LDR panoramas of Xiao et al.~\shortcite{xiao-cvpr-12}. However, using LDR panoramas as training data poses an additional challenge: the light sources are not explicitly available. Hence, we also introduce a method to detect light sources on a given LDR panorama image, which significantly outperforms the state-of-the-art. We use the results of this algorithm as the output for the training pairs, and images cropped from the corresponding panoramas as the input. This allows us to predict the location of light sources in a scene, but not the intensities of the lights since this information is not accurately captured in LDR panoramas. We resolve this by capturing a new dataset consisting of 2100 HDR environment maps. Fine-tuning the network trained on LDR panoramas using this HDR data allows us to train a network that directly regresses an LDR, limited field-of-view photo to the true HDR scene illumination. 

Unlike previous work, our technique does not require special image capture or user input. Nor does it rely on any assumptions on scene appearance, geometry, material properties or lighting. Instead, we can automatically recover illumination estimates from images of indoor scenes truly captured ``in the wild''. Our work significantly outperforms previous state-of-the-art methods. Consequently, using these illumination estimates for applications like 3D object insertion lead to results that are photo-realistic (e.g. Fig.~\ref{f:teaser}). We demonstrate this over a large set of examples and via a perceptual user study that indicates that objects lit by our illumination estimates are almost indistinguishable from those lit by ground truth illumination. We believe that this represents a significant step forward on an important, and challenging scene understanding problem.

\paragraph{Contributions} Our main contributions are:
%\begin{itemize}

1. An end-to-end illumination estimation method that leverages a deep convolutional network to take a limited-field-of-view image as input and produce an estimation of HDR illumination.

2. A state-of-the-art light source detection method for LDR panoramas and a panorama warping method, that help generate training data for the end-to-end illumination estimation network.
%\item A novel panorama warping algorithm to account for recentering of the spherical IBL with respect to the crop image.

3. A new HDR environment map dataset that can be used to train and evaluate illumination estimation or other scene inference tasks.
%\end{itemize}

4. A benchmarking of the state-of-the-art in single image scene illumination estimation by means of a perceptual user study.

%%% This is the ``teaser'' command, which puts an figure, centered, below 
%%% the title and author information, and above the body of the content.


%\ersin{Kalyan, I think two threads for motivation was fine. See my edits above -- we can discuss if you don't like it. I commented the thoughts out, also I moved the other lighting estimation techniques to background.}

%\KS{Thoughts:}

%Two threads running through this intro: 

%1. Current techniques assume certain models of appearance that are often inaccurate and these inaccuracies (in addition to erroneous estimates of geometry/reflectance) can lead to poor lighting estimates. Instead of "`hand-crafting"' these appearance models, we directly learn the mapping from images to illumination.

%2. Most current lighting estimation techniques make strong assumptions about the lighting -- spherical harmonics, sun-sky models, small set of point/directional lights. This is not accurate for indoor scenes and we want to go beyond this while still keeping the inference tractable.

%Both important? Keep only one thought in the intro?

%Lighting is inherently HDR and we need to recover it from LDR images. -- not important, everyone else does this too 


