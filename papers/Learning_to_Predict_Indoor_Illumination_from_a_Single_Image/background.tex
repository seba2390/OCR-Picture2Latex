\section{Related work}

Reconstructing a scene---and all its properties including geometry, surface reflectance, and illumination---from a single image is one of the long-standing goals of computer vision and graphics, and has been extensively studied in the literature. Because these properties are intrinsically tied to each other, estimating each of them often relies on reconstructing the others too~\cite{barron-pami-15}. In this related work, we will specifically focus on techniques for recovering illumination.

The seminal work of Debevec~\shortcite{debevec-sig-98} on image-based lighting demonstrated that capturing several photographs of a mirrored sphere using different exposures can be used to reconstruct a physi\-cally-correct, omnidirectional HDR radiance map, which can then be used to realistically render novel objects into the scene. Follow-up work has demonstrated that the same can be done from a single shot, provided there is also a diffuse sphere in the scene~\cite{reinhard-book-10}, or a metallic/diffuse hybrid sphere~\cite{debevec-sslp-12}.  


\begin{figure*}[!t]
\includegraphics[width=0.94\linewidth]{images/overview-withRGB.jpg}
\caption{Overview of the paper. Our method automatically predicts the HDR lighting conditions from a single photograph (left). To do so, it relies on a deep CNN that is trained in two stages. First, we rely on a database of LDR panoramas~\cite{xiao-cvpr-12}. To compensate for the low dynamic range, light sources are detected and the panoramas are warped to generate target light masks, which, combined with crops extracted from the panoramas, can be used to train the CNN to predict light directions. Second, the network is fine-tuned on a novel dataset of HDR panoramas, which allows it to learn to predict light intensities.}
\label{f:overview}
\end{figure*}

Previous work on illumination estimation typically models scene appearance as a function of the scene geometry, reflectance properties, and illumination, and optimizes for values that best explain the captured image. This appearance model is typically Lambertian shading under a low-dimensional lighting model like low-order spherical harmonics. This is combined with known geometry captured using depth sensors~\cite{barron2013rgbd} or reconstructed using multi-view stereo~\cite{wu-cvpr-11} or model-based fitting~\cite{valgaerts-tog-12} to estimate lighting. Moreno et al.~\shortcite{moreno-cng-10} assume that the lighting consists of a known set of discrete point lights. These techniques recover illumination from isolated objects~\cite{lombardi2016reflectance}, for which a low-dimensional lighting model is sufficient and the geometry and reflectance can be well modeled. However, we are interested in recovering illumination from indoor scenes that exhibit complex cluttered geometry, spatially-varying non-Lambertian appearance, and a wide variety of lighting conditions that cannot be be parameterized accurately with low-dimensional models. Therefore, we use a more general environment map representation and directly regress it from the image without relying on potentially inaccurate appearance models.

Lalonde et al.~\shortcite{lalonde-ijcv-10} focus on recovering outdoor illumination that is well approximated by analytical sun-sky models. Hold-Geoffroy et al.~\shortcite{holdgeoffroy-cvpr-17} propose a deep learning-based method to predict outdoor illumination from a single input image. Like us, they train their network with image/illumination pairs created from a panorama database. However, outdoor illumination is distant and well approximated by low-dimensional analytical models. As a result, they are able to fit the $3$-parameter Ho\v{s}ek-Wilkie model~\shortcite{Hosek2012} to LDR panoramas to recover HDR illumination, and train a deep neural network to predict these $3$ parameters. In contrast, lighting in indoor scenes is complex and cannot be approximated well with simple low-dimensional models. Instead, we use a non-parametric HDR IBL representation and this makes learning indoor illumination challenging in terms of both data generation and network training. In this work, we address these challenges with a number of novel contributions. We propose a two-stage training process that initially learns to predict light locations from LDR data, and is later fine-tuned to predict light intensities on an HDR dataset. To generate data for the light location prediction task, we develop a state-of-the-art light detector that we use to annotate a large LDR panorama dataset. We also propose a panorama warping step that accounts for spatially-localized indoor lighting (vis-a-vis distant outdoor illumination). We also present a new multi-head neural network to recover indoor illumination and train it with a novel rendering loss that progressively improves its light prediction estimates. 

Khan et al.~\shortcite{khan-siggraph-06} propose flipping an HDR input image to approximate the out-of-view illumination. Similar ideas have also been used in other 2D compositing techniques~\cite{bitouk-sig-08,lalonde-sig-07}. While these approximations might work in some cases, they can be completely incorrect in many others (for example, when the dominant light illuminating the scene is behind the camera). In contrast, our light predictions are significantly more accurate and only require an LDR image as input.

Karsch et al.~\shortcite{karsch-sig-11} estimate scene illumination from single images, but rely on user input to annotate geometry and initial lighting, which is then refined using a rendering-based optimization. %This lighting is then refined by minimizing an appearance-based cost. While their system produces impressive results for photo-realistic 3D object insertion, it depends heavily on the user input. 
Zhang et al.~\shortcite{zhang-siga-16} use a similar scheme to recover illumination from a complete RGBD scan of a scene and user-annotated light positions. Karsch et al.~\shortcite{karsch-tog-14} propose an automatic scene inference technique. They detect light sources visible in the image, and leverage the SUN360 panorama database to predict out-of-view lighting. This is done by finding panoramas that are similar in appearance to the input image and using pre-classified light sources in these panoramas as the light sources for the input image. This transforms the illumination estimation problem into one of image matching with the right metric; however, this is a coarse approximation, and in many cases, the matched panorama may have lighting that is arbitrarily different from the actual illumination in the image. In contrast, we propose directly learning the mapping between image appearance and scene illumination, and demonstrate that this leads to better illumination estimates.

% \begin{figure*}[!t]
% \includegraphics[width=\linewidth]{images/diag_preclassifier.eps}
% \caption{Light classification pipeline. The input panorama is first converted to grayscale, and rotated by $90^\circ$ to align the zenith with the horizon line in order to avoid the large distortions caused by the equirectangular projection. HOG descriptors are extracted and used to train a logistic regression classifier. The detections obtained on the panorama and its rotated version are merged, thresholded, and refined using a CRF and morphological operators (please see text for more details). This classifier accurately detects a wide variety of light sources directly from an LDR equirectangular panorama.}
% \label{f:hogflowchart}
% \end{figure*}

More recently, deep neural network-based techniques have been proposed for estimating reflectance maps---the convolution of a surface BRDF with the incident illumination---from a single image~\cite{rematas-cvpr-16}. These reflectance maps can then be separated into reflectance and illumination estimates~\cite{georgoulis-arxiv-16a}. However, these techniques focus on objects of a known class with approximately known shape and spatially constant reflectance. In contrast, the indoor scenes we focus on have significantly more complex shape and reflectance variations. CNN-based methods have also been proposed for estimating scene depth~\cite{eigen-iccv-15}, surface normals~\cite{eigen-iccv-15,bansal2016marr}, and intrinsic decompositions for indoor scenes~\cite{zhou2015intrinsic}. While challenging, these problems involve recovering properties that are directly observed in the image, unlike lighting which can lie out of the field-of-view and only indirectly effects scene appearance. By recovering illumination from a single image, our work can likely benefit these other scene inference tasks.

