
\section{LDR panorama light source detection}
\label{sec:lightdetection}

In order to use LDR panoramas for training our CNN to detect light sources, we must first detect areas in the panoramas which correspond to bright light sources. To do so, we propose a novel light source detector, and show that it significantly outperforms the approach of Karsch et al.~\shortcite{karsch-tog-14}. 
%An overview of the light detection pipeline is shown in Fig.~\ref{f:hogflowchart}. 

\subsection{Light classification} 

After converting to grayscale, the panorama $P$ is rotated by $90^\circ$ about the pitch angle to yield $P_\text{rot}$, so that the zenith is aligned with the horizon line. This rotation is needed to account for the large distortions caused by the equirectangular projection, which severely stretches regions around the poles. Features are then computed over $P$ and $P_\text{rot}$ separately on square patches at five different scales\footnote{We use $30\times30$ squares at the lowest scale, multiplying their size by 1.5 at each scale.}. In particular, we use HOG~\cite{dalal-cvpr-05}, the mean patch elevation, as well as its mean, standard deviation, and 99th percentile intensity values. These features are used to train two logistic regression classifiers for small (e.g. spotlights and lamps) and large (e.g. windows, reflections) light sources. We found that training classifiers for these two types of classes separately yielded better performance, as these types of light sources significantly differ from one another. 

The resulting logistic regression classifiers are then applied in a sliding-window fashion over $P$ and $P_\text{rot}$ to yield a score at each pixel. 
%(fig.~\ref{f:hogflowchart}). 
Scores from both classifiers are added, then merged on a per-pixel manner according to their elevation angles $S_\text{merged} = S\cos(\theta) + S_\text{rot}^*\sin(\theta)$, where $S$ indicates the regression scores, $\theta$ the pixel elevation, and $S_\text{rot}^*$ is $S_\text{rot}$ rotated back to the original orientation. The resulting scores are then thresholded to obtain a binary mask, refined with a dense CRF~\cite{krahenbuhl-nips-12}, and adjusted with opening and closing morphological operations. The optimal threshold is obtained by maximizing the intersection-over-union (IoU) score between the resulting binary mask and the ground truth labels on the training set. 

\begin{figure}
    \centering
    \includegraphics[width=0.64\columnwidth]{images/globalPrCurves.pdf}
    \caption[]{Precision-recall curves for the light detector on the test set for our detectors and the one of Karsch et al.~\shortcite{karsch-tog-14}. In blue, the curve for the spotlights and lamps detection, and in green, the curve for the windows and light reflections. In red, the result for \cite{karsch-tog-14}. In cyan, the curve for a baseline detector relying solely on the intensity of a pixel. Note that because of the inherent uncertainty of the importance of a light (including reflections) relative to the others (even for a human annotator), a perfect match between human and algorithm predictions is highly unlikely.}
    \label{fig:prcurves}
\end{figure}

\begin{figure}[t]
\centering
\setlength{\tabcolsep}{1pt}
\begin{tabular}{cc}
\includegraphics[width=0.48\linewidth]{images/samplesPreclassifier/pano1.jpg} & 
\includegraphics[width=0.48\linewidth]{images/samplesPreclassifier/pano1_mask.jpg} \\
\includegraphics[width=0.48\linewidth]{images/samplesPreclassifier/pano2.jpg} & 
\includegraphics[width=0.48\linewidth]{images/samplesPreclassifier/pano2_mask.jpg} \\
\includegraphics[width=0.48\linewidth]{images/samplesPreclassifier/pano3.jpg} & 
\includegraphics[width=0.48\linewidth]{images/samplesPreclassifier/pano3_mask.jpg} \\
\includegraphics[width=0.48\linewidth]{images/samplesPreclassifier/pano4.jpg} & 
\includegraphics[width=0.48\linewidth]{images/samplesPreclassifier/pano4_mask.jpg} \\
\includegraphics[width=0.48\linewidth]{images/samplesPreclassifier/pano5.jpg} & 
\includegraphics[width=0.48\linewidth]{images/samplesPreclassifier/pano5_mask.jpg} \\
\end{tabular}
\caption{Light detection results on SUN360 panoramas. (left) the input LDR panoramas; (right) light detection results, shown in cyan and overlaid on the original panorama for reference. The detector is able to handle a wide range of lighting arrangements, including large light patches and spotlights.}
\label{f:lightdetection-results}
\end{figure}

\subsection{Training details and evaluation} 

To train the classifiers, we manually annotate a set of 400 panoramas from the SUN360 database. Four types of light sources are labelled: spotlights, lamps, windows, and (bounce) reflections. We use 80\% of the panoramas for training, and 20\% for testing. The classifier is first trained using labeled lights as positive samples and random negative samples. Subsequently, hard negative mining~\cite{felzenszwalb-pami-10} is used over the entire training set. We discard the bottom 15\% of the panoramas because this region often contain watermarks and light sources are seldom located below the camera. 

Fig.~\ref{fig:prcurves} reports a comparison of precision-recall curves for our two detectors, a baseline method which directly maps the intensity of a pixel to its probability of belonging to a light source, and the approach of Karsch et al.~\shortcite{karsch-tog-14}. As expected, the baseline performs poorly on LDR data like SUN360. The detector from \cite{karsch-tog-14} offers better performance, but our performs significantly better at any level of recall. Fig.~\ref{f:lightdetection-results} shows light detection results on example panoramas from the SUN360 dataset. 





%\begin{table}
%\centering
%\begin{tabular}{ccccc}
%\toprule
%\multirow{2}{*}{Method} & \multicolumn{2}{c}{All lights} & %\multicolumn{2}{c}{Spotlights only} \\
% & mean & median & mean & median \\
%\midrule
%\cite{karsch-tog-14} 	& 0.268 & 0.222 & 0.121 & 0.045 \\
%Ours 					& 0.326 & 0.298 & 0.305 & 0.299 \\
%\bottomrule
%\end{tabular}
%\caption[]{Comparison between our light detector and that of Karsch et al.~\shortcite{karsch-tog-14} on a test set of 80 hand-labeled LDR panoramas from the SUN360 database. We report the mean and median intersection-over-union (IoU) score for all light sources, and for spotlights only. Our approach significantly outperforms that of \cite{karsch-tog-14}, especially for detecting bright (but small) spotlights.}
%\label{t:lightdetection-iou}
%\end{table}

% From Marc-Andr√©:
% OUR DETECTOR :
% MEAN / MED IoU :  0.326159233623 0.298747417355
% MIN / MAX IoU :  0.0145741504337 0.79900990099
% MEAN / MED spots intersect :  0.305134993465 0.284664612467
% #############################
% KARSCH'S DETECTOR
% MEAN / MED IoU :  0.268475126288 0.22152084079
% MIN / MAX IoU :  0.0 0.854946427713
% MEAN / MED spots intersect :  0.12067937909 0.0453275380317



% \begin{figure}
% \centering
% \includegraphics[width=0.75\linewidth]{images/prCurves_pass2.png}
% \caption{Precision-recall curves for the light detector on the test set. In red, the curve for the spotlights and lamps detection, and in blue, the curve for the windows and light reflections. Note that because of the inherent uncertainty of the importance of a light (including reflections) relative to the others (even for a human annotator), a perfect match between human and algorithm predictions is highly unlikely. 
% \JF{Remove gray background, make axes black, make vectorized version}}
% \label{f:4prcurves}
% \end{figure}


%% NOTES FROM MARC-ANDRE
% General outline :
% \begin{enumerate}
%     \item We need to find a way to find the light sources in an LDR image. We cannot just use a pixel value because of the quick saturation (so a white pixel might not be contributing at all to the scene illumination).
%     \item Since we have the panoramas, most of the time we actually see these lights (except in the cases of occlusions). So we can train a light detector.
%     \item We use a well-known approach with a combination of HOG and logistic regression (by all means, a logistic regression is similar to a Linear SVM). We could have use more complicated approaches, but we show here that even this simpler approach works well and, in any way, we just need the light labeled to \textit{train} our network. Also, we append some information to the HOG descriptors vectors, like the elevation of the center of the patch, the mean color, etc.) Finally, we train two different classifiers, of which we combine the output, one for the 
%     spotlights and lamps, and another for windows and reflections, since their descriptors might be quite different.


%     \item To train the classifier, we manually annotated 400 panoramas. Each source have been labeled as one of these categories: spotlight, lamp, window, and reflection. Of these 400 panoramas, we retain 20\% (80 panoramas) as test set. The classifier is first trained using labeled lights as positive samples and random negative samples. Subsequently, we do hard negative mining over all the training set and train again the classifier using these hard negative samples. When evaluating the classifier, we drop the bottom 15\% of the panoramas because of the watermarks and also since the probability of a light being almost under the camera is quite low.


%     \item The first issue is that while HOG descriptors deal reasonably well with rotation and minor scaling, they do not work well with general deformation. Unluckily, a latlong projection distorts a lot the panorama, especially near the zenith, where it is not unreasonable to find a light... To alleviate this issue, we actually extract HOG features from two environment maps. The first one is the one passed as input (gray level), and the second one is rotated such as the zenith becomes part of the horizon. The ceiling lights are then less distorted and the same classifier may be used to find them.

%     \item As usual with HOGs, we use a sliding window at different scales. However, the lights are not perforce rectangular yet we would like to be able to roughly segment them. Each time a detection is fired, we add a constant value to its corresponding zone. This value is decreasing along with the scale. In the end, we obtain an heatmap of the most probable spots for light sources.

%     \item We combine the heatmap produced with the rotated and non-rotated panoramas, weighting them with a simple cosine rule, based on the fact that at the horizon, each detector has the maximum precision; on the contrary, at the zenith, the detector accuracy is quite poor.
%     \item Once we have merged the heatmap, we threshold it so to get a binary mask.

%     \item We apply a CRF on it to add some locality to the detections. Finally, we use standard morphological operators to remove outliers and inliers.
% \end{enumerate}