\section{Panorama recentering warp}
\label{sec:warping}

\begin{figure}[!t]
\centering
\includegraphics[width=0.99\linewidth]{images/warping/warping.pdf}
\caption{The importance of light locality for indoor scenes. Left, a photo for which we want to estimate the lighting conditions. The photo was cropped from the ``original'' panorama (top row, middle). Treating this panorama as the light source for the photo is wrong; its center of projection is in front of the scene in the photo, and relighting a virtual bunny (top row, right) makes it appear to be backlit. The correct HDR panorama, captured with a light probe at the position of the cropped photo, is shown in the middle row, and captures the location of the lights on top of the scene. We introduce a warping operator that can be estimated with no scene information, and distorts the original panorama to approximate the location of the light sources on the top (bottom row). Relighting an object with the warped panorama yields results that are much closer to the ground truth.}
\label{f:warping-problem}
\end{figure}

Detecting the light sources in LDR panoramas is not sufficient for training the CNN to learn lighting from a single photo. The fundamental problem is that the panorama does not represent the lighting conditions in the \emph{cropped scene}, since the panorama center of projection can be arbitrarily far away from the location of the scene points in the cropped photo. Fig.~\ref{f:warping-problem} illustrates this issue. The photo shown on the left was cropped from the ``original'' panorama in the middle column. Treating this original panorama as a light source is incorrect, and results in a backlit bunny. We captured the \emph{actual} lighting conditions by placing a light probe at the scene (middle column of fig.~\ref{f:warping-problem}). Notice how the lighting conditions at the scene differ from those in the original panorama. To allow the use of the SUN360 database (from which we can crop photos but do not have access to the scenes to capture ground truth lighting) for training, we present a novel method that warps the original panorama to approximate the lighting in the cropped photo (bottom row). 
%While our warping function is an approximation, the lighting conditions and relit object obtained with the warped panorama are much closer to the ground truth than the original version, and does not require physical access to the scene (bottom row of fig.~\ref{f:warping-problem}). We now detail this warping function and provide several qualitative examples that demonstrate the need for such an approach. 

\subsection{Warping operator}

The aim of the warping operator is to generate the panorama that would be captured by a virtual camera placed at a point in the cropped photo. This is a challenging problem that is made especially harder by the fact that we do not know the scene geometry, and we make two assumptions to make this task feasible. First, we assume that the scene lies on a sphere, i.e., all scene points are equidistant from the original center of projection. Second, we assume that an image warping suffices to model the effect of moving the camera, i.e., occlusions are not an important factor. These assumptions may not hold for all scene points; however, note that our goal is to model light sources, which are typically located at scene extremities (ceiling, walls, etc.) and are better approximated by these assumptions.

Let us assume that the panorama is placed on the unit sphere, i.e. $x^2 + y^2 + z^2 = 1$, with the camera that captured this panorama at the origin of this sphere. The outgoing rays emanating from a virtual camera placed at $(x_0,y_0,z_0)$, can be parameterized as:
%
\begin{equation}
x(t) = v_x t + x_0  \quad
y(t) = v_y t + y_0  \quad
z(t) = v_z t + z_0  \,.
\label{eq:warp2}
\end{equation}
%
Intersecting these rays with the panorama sphere yields:
%
\begin{equation}
    (v_x t + x_0)^2 + (v_y t + y_0)^2 + (v_z t + z_0)^2 = 1 \,.
    \label{eq:warp3}
\end{equation}
% 

As illustrated in fig.~\ref{f:warp-basics}, we want to model the effect of using a virtual camera whose nadir is at $\beta$. The angle $\beta$ corresponds to the point in the panorama where the photo is extracted, and we will discuss how this is computed shortly. For the case of translating along the $z$-axis, this results in a new camera center, $\{x_0, y_0, z_0\}$ = $\{0, 0, \sin \beta\}$. Warping in arbitrary directions can trivially be achieved by rotating the environment map before and after the warp. Substituting this in eq.~\ref{eq:warp3}, results in the following second degree equation:
%
\begin{equation}
(v_x^2 + v_y^2 + v_z^2)t^2 + 2v_z t \sin\beta  + \sin^2\beta - 1 = 0 \,.
\label{eq:warp4}
\end{equation}
%
Solving (\ref{eq:warp4}) for $t$ (keeping only positive solutions, as negative roots represent the intersection on the other side of the sphere), maps the coordinates from the original environment map to the ones in the warped camera coordinate system. 
%An illustrated example of the effect of the warp operator on an equirectangular panorama is shown in fig.~\ref{f:warp-explanations}. 

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.375\linewidth]{images/diag_explanations_warp.eps}
    \caption{Overview of the warping problem, illustrated in 2D for simplicity. The circle represents a slice of the spherical panorama along the $y$--$z$ plane, with the center of projection (illustrated by a camera) at its center. The aim of the warp operator is create a virtual center of projection with a nadir at an angular distance of $\beta$ with respect to the original nadir. The angle $\beta$ corresponds to the point in the panorama where the photo is extracted.}
    \label{f:warp-basics}
\end{figure}

\begin{figure}[!t]
\centering
\footnotesize
\setlength{\tabcolsep}{1pt}
\begin{tabular}{cc}
\includegraphics[width=0.493\linewidth]{images/warping/findbeta_input_withx.png} &
\includegraphics[width=0.493\linewidth]{images/warping/findbeta_normals.png} \\
(a) Input image & (b) Normals \\
\includegraphics[width=0.493\linewidth]{images/warping/findbeta_output_withx.png} &
\includegraphics[width=0.493\linewidth]{images/warping/findbeta_panowarp.png} \\
(c) Original panorama & (d) Warped panorama \\
\end{tabular}
\caption{$\beta$ selection procedure. From a given crop picture (a), we extract the normals using the method of  Bansal et al.~\shortcite{bansal2016marr} (b). We pick the insertion point by looking at the lowest pixel with a horizontal surface (green X in (a)) and backproject it on to the panorama (c). This gives us the point where we would like the nadir to be, from which $\beta$ can be trivially recovered. We then warp the panorama using this $\beta$ (d).}
\label{f:warp-beta-pick}
\end{figure}


The value of $\beta$ in eq.~(\ref{eq:warp4}) represents the point in the panorama where the photo is extracted. We expect that users will want to insert objects on to flat horizontal surfaces in the photo, and we reflect this in the choice of $\beta$ as follows (see fig.~\ref{f:warp-beta-pick}): we first use the approach of Bansal et al.~\shortcite{bansal2016marr} to detect surface normals in the cropped image, and find flat surfaces by thresholding based on the angular distance between surface normal and the up vector. We back-project the $y$-coordinate of the lowest point of the largest flat area (i.e., the lowest point on the flattest horizontal surface) on to the panorama to obtain $\beta$. In cases where no horizontal surfaces are found (e.g., a flat vertical wall), no warp is applied as the panorama is assumed to be sufficiently close to scene. Note that we always assume the insertion point to be x-centered ---that is, we do not ask the network to estimate the light at far-left or far-right of the image.

% \begin{figure}
% \centering
% \footnotesize
% \setlength{\tabcolsep}{1pt}
% \begin{tabular}{cc}
% \includegraphics[height=2.2cm]{images/tilingpattern_0.png} &
% \includegraphics[height=2.2cm]{images/tilingpattern_sphere_0.png} \\
% \includegraphics[height=2.2cm]{images/tilingpattern_30.png} &
% \includegraphics[height=2.2cm]{images/tilingpattern_sphere_30.png} \\
% \includegraphics[height=2.2cm]{images/tilingpattern_60.png} &
% \includegraphics[height=2.2cm]{images/tilingpattern_sphere_60.png} \\
% (a) Equirectangular panorama & (b) Projected on a sphere
% \end{tabular}
% \caption{Effect of the warp operator on panoramas. Top row: the original environment map. Middle row: warp with $\beta=30^\circ$. Bottom row: warp with $\beta=60^\circ$.}
% \label{f:warp-explanations}
% \end{figure}

\begin{figure}[!t]
\centering
\footnotesize
\setlength{\tabcolsep}{1pt}
\begin{tabular}{ccc}
\includegraphics[width=0.325\linewidth]{images/warping/images/good/noWarp/pano0767-others-135.jpg} &
\includegraphics[width=0.325\linewidth]{images/warping/images/good/withWarp/pano0767-others-135.jpg} & 
\includegraphics[width=0.325\linewidth]{images/warping/images/good/envyDepth/composeHenrique767.jpg} \\
\includegraphics[width=0.325\linewidth]{images/warping/images/good/noWarp/pano0460-others-270.jpg} &
\includegraphics[width=0.325\linewidth]{images/warping/images/good/withWarp/pano0460-others-270.jpg} &
\includegraphics[width=0.325\linewidth]{images/warping/images/good/envyDepth/composeHenrique460.jpg} \\
\includegraphics[width=0.325\linewidth]{images/warping/images/good/noWarp/pano0618-others-00.jpg} &
\includegraphics[width=0.325\linewidth]{images/warping/images/good/withWarp/pano0618-others-00.jpg} & 
\includegraphics[width=0.325\linewidth]{images/warping/images/good/envyDepth/composeHenrique618.jpg} \\
\includegraphics[width=0.325\linewidth]{images/warping/images/good/noWarp/pano0634-others-135.jpg} &
\includegraphics[width=0.325\linewidth]{images/warping/images/good/withWarp/pano0634-others-135.jpg} &
\includegraphics[width=0.325\linewidth]{images/warping/images/good/envyDepth/composeHenrique634.jpg} \\
(a) Original panorama & (b) Our warp & (c) \cite{banterle-cgf-13}
\end{tabular}
\caption{Comparison of objects relit with (a) the original panoramas, (b) our warped panoramas, and (c) panoramas warped using EnvyDepth~\cite{banterle-cgf-13}. The objects relit by our panoramas closely approximate those obtained with EnvyDepth, without the lengthy manual annotation required.}
\label{f:warp-results}
\end{figure}

\subsection{Impact on lighting estimation}

Fig.~\ref{f:warping-problem} compares our warped panorama with a ground truth panorama captured in-place for one scene. We also compare our spherical warp with a geometry-based warp obtained with EnvyDepth~\cite{banterle-cgf-13}, a system that extracts spatially-varying lighting from environment maps by projecting them onto proxy geometry estimated from manual annotations. Comparative relighting results using the original, spherical warp, and geometry-based warp panoramas are presented in fig.~\ref{f:warp-results}. While our operator makes several simplifying scene assumptions, these results illustrate that relighting with our approach provides a close approximation to more expensive techniques, while being completely automatic and without requiring access to the scene. In contrast, the manual labeling process required for the geometric warp takes around 10 minutes per panorama. 

The main limitation of our warping operator is that it fails to appropriately model occlusions. Since we treat the panorama as a projection on a sphere, lights that illuminate a scene point, but are not visible from the original camera are not handled by this approach. However, these situations are rare, and as we show in our results, our network filters them out as outliers, and learns a robust scene appearance to illumination mapping.


%objects protruding from the main surfaces (e.g. a table, columns, etc.) appear unrealistically distorted in the warped versions. However, since our goal is to obtain a more accurate representation for lighting, the distortions have minimal impact on the learning.

%\emiliano{note: This seems to be also relevant as trick for rendering scenes lit by a single IBL. You can assume the IBL refers to the origin of the scene and give it a radius. Then use this trick to adjust the shading away from the center. Also the sampling function is trivial to implement in a shader. This seems to be a relevant and basic CG problem, I am surprised nobody else tackled it. }


% Of course, this operator does not solve everything. In particular:
% \begin{itemize}
%     \item It greatly reduces the resolution in the front zone (and increases it behind the camera). SUN360 having 9000x4500 panoramas, this is not a huge issue in our case.
%     \item This assumes horizontal surfaces everywhere, which is clearly not the case for indoor scenes. We thus use the normal information (using a CNN to estimate them) to ensure that pathological cases do not happen.
%     \item This does not solve the issue in case of severe occlusions, but nothing can (the information we want is simply not there). All in all, as our results in fig.~\ref{f:warp-results} clearly show, we still get a consistently better lighting estimation than by using the IBL at the camera position.
% \end{itemize}
