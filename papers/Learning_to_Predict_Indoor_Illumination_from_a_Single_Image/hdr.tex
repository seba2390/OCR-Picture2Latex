\section{Learning high dynamic range illumination}
\label{sec:hdr}

Up to this point we have trained a network that can predict the \emph{position} of the light sources quite accurately (see sec.~\ref{sec:experiments}), but, since it was trained on LDR data, it does not know about the \emph{intensities} of the light sources. In this section, we further train the network on a novel dataset of high dynamic range panoramas which enables it to jointly reason about light source direction and intensity. 

\subsection{A new dataset of HDR indoor panoramas}

We have captured a novel dataset of 2,100 high-resolution ($7768\times3884$), high dynamic range indoor panoramas. To do so, a Canon 5D Mark III camera with a Sigma 8mm fisheye lens was mounted on a tripod equipped with a robotic panoramic tripod head, and programmed to shoot 7 bracketed exposures at $60^\circ$ increments. The photos were shot in RAW mode, and automatically stitched into a 22 f-stop HDR $360^\circ$ panorama using the PTGui Pro commercial software. The dynamic range is sufficient to correctly expose all pixels in the scenes, including the light sources. Panoramas were captured in a wide variety of indoor environments, such as schools, houses, apartments, museums, laboratories, factories, sports facilities, etc. A visual overview of panoramas in our novel HDR dataset is shown in the supplementary material. The size and variety of this dataset is significantly larger than other similar datasets in the literature (which consist of tens of panoramas), making it extremely useful for training and testing a wide range of problems from scene inference, high dynamic range image processing, and rendering\footnote{This dataset is publicly available at \url{http://www.jflalonde.ca/projects/deepIndoorLight}.}. 

\subsection{Adapting the network to HDR data}

Since the light sources are not saturated in the HDR data, the network can be adjusted to directly learn the light source \emph{intensities} $\mathbf{y}_\text{int}$ instead of the binary light mask $\mathbf{y}_\text{mask}$. To do so, the network undergoes the following four simple changes. First, the weights of the last layer of the light mask predictor (``conv5-1'' in table~\ref{t:learning-architecture}) are initialized to random values. Second, training is performed to update only the weights of the decoders---that is, up to the FC-1024 layer in table~\ref{t:learning-architecture}. This is done to avoid overfitting on the encoder. Third, the target intensity $\mathbf{t}_\text{int}$ is defined as the log of the HDR intensity ($\log_{10}$ is used). Low intensities (below the median of the training dataset) are clamped to 0, since we only care about the light sources: in the unusual case where no pixels would be over this threshold, the ambient term given by the RGB recovery should be enough to light the scene. Finally, the loss is modified to:
%
\begin{align}
    \mathcal{L}_\text{HDR}(\mathbf{y}, \mathbf{t}, e) &= w_1 \mathcal{L}_\text{L2}(\mathbf{y}_\text{RGB}, \mathbf{t}_\text{RGB}) \nonumber \\ 
    &+ w_2 \mathcal{L}_\text{cos}(\mathbf{y}_\text{int}, \mathbf{t}_\text{int}, e)
    + w_3 \mathcal{L}_\text{L2}(\mathbf{y}_\text{int}, \mathbf{t}_\text{int}, e)  \,,
\label{e:hdrloss}
\end{align}
%
where $\mathcal{L}_\text{L2}$ and $\mathcal{L}_\text{cos}$ were defined in eq.~(\ref{e:rgbloss}) and (\ref{e:maskloss}) respectively, and $e$ is continued from training on the LDR data (so the HDR intensities are not overblurred). The L2 term on the intensity was added to reduce deconvolution artifacts. Here, $w_1 = 10$, $w_2 = 1$, and $w_3 = 0.1$. Training is otherwise performed with the same parameters as in sec.~\ref{sec:training-details}, and, just as with the LDR data, 85\% of the HDR data was used for training and 15\% for testing. Similar to the LDR data (sec.~\ref{sec:ldr-data-prep}), 8 crops were extracted from each panorama in the HDR dataset, yielding 14,000 input-output pairs. These are tone-mapped to ensure that the input to the network are LDR images. Finally, the panoramas are also warped using the same procedure as their LDR counterparts. Fig.~\ref{f:learning-curves}-(b) shows the loss (from eq.~(\ref{e:hdrloss})) curves on the training and test set during training. 


