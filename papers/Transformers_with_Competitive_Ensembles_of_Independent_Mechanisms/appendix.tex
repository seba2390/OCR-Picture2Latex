
%\section{Visual Analysis}


\section{Experiment Details}

%\begin{figure}[t!]
    %\vspace{-10mm}
%    \centering
%    \includegraphics[width=0.9\linewidth]{images/cifar_specialization/real.png} \\
%    \includegraphics[width=0.9\linewidth]{images/cifar_specialization/early.png}\\
%    \includegraphics[width=0.9\linewidth]{images/cifar_specialization/late.png}
%    \caption{With independent mechanisms, the information is kept well-separated throughout the entire layer, and all of the layer's parameters are specific to a single mechanism.  Competition patterns of an unsupervised image Transformer on CIFAR images (top right) with 2 mechanisms shows that mechanisms learn to specialize over foreground and background patterns on an early layer (center right) and become more confident in a later layer (bottom right).  }
%    \label{fig:headsmechanisms}
%\end{figure}


\subsection{Image Transformer Details}

For the Image Transformer, we used a baseline with 6 Transformer layers.  As in the other experiments, we made the first 2 layers use TIMs as well as the last layer.  We ran each experiment for 30 epochs with a batch size of 24, and otherwise used the same training hyperparameters as the minGPT repository \citep{mingpt2020}.  We used the Adam optimizer with warmup, with Betas = (0.9, 0.95).  Our baseline model had 8 heads (total per-layer) and a layer hidden size of 184.  When using TIMs, we increased this (for all layers) to 10 heads and a hidden size of 200.  This led to the baseline and the TIM model having roughly the same number of total parameters.  



\subsection{Speech Enhancement Details}

\paragraph{Datasets} 
Neural speech enhancement systems are trained using a parallel corpus of noise and clean examples, which are generated by artificially contaminating clean speech with disturbances such as additive noise and reverberation \citep{speech_cont}. The speech enhancement models considered in this work are trained with the DNS dataset (noisy, no reverb) \citep{reddy2020interspeech}, which is a synthetic corpus recently made publicly available by Microsoft. This corpus is extremely suitable for our study because it is quite big (441 hours) and contains a large variety of possible noises (from 600 different categories). To the best of our knowledge, it is the biggest open-source speech enhancement dataset. Moreover, it has been the object of an international challenge on speech enhancement\footnote{\url{https://dns-challenge.azurewebsites.net/Interspeech2020}}. This gave us the possibility to compare our TIM with the best systems submitted to this competition. 

For evaluation, we used the test sets of the DNS and Voicebank datasets \citep{thiemann2013diverse}. The latter has been adopted to study a transfer learning scenario, where different datasets are used for training and evaluation purposes. Voicebank, in fact, is generated with noisy sequences different from the one contained in the DNS corpus. Since Voicebank is released at 48 kHz, the original raw waveforms were downsampled from 48kHz to 16kHz.

\paragraph{Model Architecture}
The proposed TIM is fed with noisy speech and estimates clean speech at the output. More precisely, we estimate the log-spectral magnitude of the clean signal. Mean Squared Error (MSE) between the clean and the corresponding noisy signal is used as cost function. 
The input waveform is transformed with the Short-Term Fourier Transform (STFT) based on 512 frequency points and window length of 32 ms with 16 ms overlap.

Before adding the transformer layers, we employ four 1D convolutional layers that act as a pre-encoder module. This is done to replace positional encoding from the original transformers and inject relative location information to the frames in the sequence \citep{kim2020t, fu2020boosting}.
The four convolutional layers are based on 1024, 512,128, and 256 channels, respectively. The kernel size is 3. After the convolution, each layer applies layernorm followed by LeakyReLU.
The Transformer part is composed of 8 encoder blocks with a hidden size of 512. In order to employ approximately the same number of parameters (i.e, 6 million), the baseline transformers used a hidden size of 256. We used 16 attention heads, a dropout rate of 0.1, and LeakyReLU activations. We kept the number of heads the same as in the baseline model.  To follow the real-time processing restriction in DNS challenge, a causal setting is adopted to all our models with access to 32 ms of future frames. Attention masks are also applied to the self-attention layers to prevent using the future information.

\paragraph{Training}
We followed the exact same training procedure for the baseline model and the TIMs model, with both trained for 50 epochs.   We used the standard variant of the Adam optimizer with a batch size of 16. The initial learning rate was set to 0.0002 and halved when the validation score decreased for 5 epochs.   We reported test set performance at the epoch with the best validation score, which in practice was near the end of training.  Both models train for about 50 hours on a single Nvidia V100 GPU.  



   

\subsection{BERT Pre-Training and Fine-Tuning Details}

BERT \citep{devlin2018bert} is one of the most popularly used methods to learn the representation of natural language. The BERT model uses a multi-layer Transformer encoder and is trained by the masked language modeling task using Web data corpus \citep{liu2019roberta}. The pre-trained contextual sentence representations have been shown to be effective in a large number of downstream tasks. 

To validate our proposed architecture, we conduct experiments to compare TIM with Transformer on the language pre-training task. For our model, we replace all of the transformer layers except for the first two layers and the last layer with TIM layers (we also report a result where all layers are TIM layers, showing that it leads to worse performance).  We scaled the dimensionality of the hidden nodes and the inner-layer of the FFN sub-layer are set to, the number of mechanisms is set to 2 and the number of heads is set to 16. We mainly test two TIM variants,  TIM without competition (TIM-NoComp) and TIM with competition (TIM-Comp).

For a fair comparison, we set one baseline as a 12-layer Transformer with 130M parameters (BERT-130M). The size of hidden nodes and the inner-layer of the FFN sub-layer are set to 768/4096, and the number of heads is set to 12. We also use the standard BERT-Base model (110M parameters) as another baseline.


\paragraph{Dataset} 
Following \cite{devlin2018bert}, we use English Wikipedia corpus\footnote{\url{https://dumps.wikimedia.org/enwiki}} and BookCorpus\footnote{As the dataset BookCorpus \citep{moviebook} is no longer freely distributed, we follow the suggestions from \citet{devlin2018bert} to crawl from \url{smashwords.com} and collect BookCorpus by ourselves. } for pre-training. By concatenating these two datasets, we obtain a corpus with roughly 3400M words in total. We follow a couple of consecutive pre-processing steps: segmenting documents into sentences by Spacy \footnote{\url{https://spacy.io}}, normalizing, lower-casing, and tokenizing the texts by Moses decoder \citep{Koehn2007MosesOS}, and finally, applying \textit{byte pair encoding} (BPE) \citep{BPE} with setting the vocabulary size $|V|$ as 32,678.

\paragraph{Optimization}
Following the standard settings used in many previous works \cite{devlin2018bert,liu2019roberta}, we train the models for 1000$k$ steps with setting the batch size as 256 and the maximum sequence length as 512. For all the models to compare, we set the masked probability $p$ to be 0.15. We follow previous works to replace 80\% of the masked positions by \texttt{[MASK]}, 10\% by randomly sampled words, and keep the remaining positions unchanged.  We choose the most widely used Adam \cite{DBLP:journals/corr/KingmaB14} as the optimizer, and set the hyper-parameter $\beta$ as $(0.9,0.98)$. The learning rate is set as 1e-4 with a 10$k$-step warm-up stage and then decays linearly to zero. We set the dropout probability as 0.1.  All models are run on 8 NVIDIA Tesla V100 GPUs. 

\paragraph{Fine-tuning}

We use the GLUE (\textbf{G}eneral \textbf{L}anguage \textbf{U}nderstanding \textbf{E}valuation) dataset \citep{DBLP:journals/corr/abs-1804-07461} as the downstream tasks to evaluate the performance of the pre-trained models. Reporting large-scale task performance or averaged performance over all tasks depends on our choice.  Same to the pre-training, we use Adam as the optimizer and set the hyper-parameter $\beta$ as $(0.9,0.98)$. Following all previous works, we apply the hyper-parameter search (over $\beta$ and learning rate) during the fine-tuning for each downstream task. Each configuration was run for five times with different random seeds, and the median and standard deviation over these five results on the development set was be used as the performance of one configuration. 

\textbf{Results}
The overall comparison results are shown in Table~\ref{tab:bert}. It is easy to find that both TIM-NoComp and TIM-Comp achieve lower perplexities (masked language modeling loss) on the validation dataset compared to the two BERT baselines. On the downstream tasks, the two TIM variants are also slightly better than the BERTs on all tasks. Those empirical results show that our proposed TIM is a better model architecture in a wide range of natural language applications.

Similar to previous analysis, we further study the competition patterns in the TIM-Comp model to investigate how the competitive module behaves. 

\clearpage



