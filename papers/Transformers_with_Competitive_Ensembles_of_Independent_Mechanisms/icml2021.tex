%%%%%%%% ICML 2021 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\input{math_commands.tex}
%\usepackage{algorithm2e}
\usepackage{graphicx}
%\usepackage{wrapfig}
\usepackage{wrapfig}


\usepackage{amsmath}
\usepackage{todonotes}

\usepackage{hyperref}
\usepackage{url}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage[algo2e]{algorithm2e}
\usepackage{xcolor}        %colour coding my changes


\newcommand{\cA}{{\mathcal A}}
\newcommand{\cB}{{\mathcal B}}
\newcommand{\cC}{{\mathcal C}}
\newcommand{\cD}{{\mathcal D}}
\newcommand{\cF}{{\mathcal F}}
\newcommand{\cG}{{\mathcal G}}
\newcommand{\cH}{{\mathcal H}}
\newcommand{\cI}{{\mathcal I}}
\newcommand{\cJ}{{\mathcal J}}
\newcommand{\cK}{{\mathcal K}}
\newcommand{\cL}{{\mathcal L}}
\newcommand{\cM}{{\mathcal M}}
\newcommand{\cN}{{\mathcal N}}
\newcommand{\cO}{{\mathcal O}}
\newcommand{\cP}{{\mathcal P}}
\newcommand{\cQ}{{\mathcal Q}}
\newcommand{\cR}{{\mathcal R}}
\newcommand{\cS}{{\mathcal S}}
\newcommand{\cT}{{\mathcal T}}
\newcommand{\cV}{{\mathcal V}}
\newcommand{\cW}{{\mathcal W}}
\newcommand{\cX}{{\mathcal X}}
\newcommand{\cZ}{{\mathcal Z}}

%\newcommand{\va}{{\bm{a}}}
%\newcommand{\vs}{{\bm{s}}}
%\newcommand{\vh}{{\bm{h}}}
%\newcommand{\ve}{{\bm{e}}}
%\newcommand{\vo}{{\bm{o}}}
%\newcommand{\vp}{{\bm{p}}}
%\newcommand{\vq}{{\bm{q}}}
%\newcommand{\vu}{{\bm{u}}}
%\newcommand{\vx}{{\bm{x}}}
%\newcommand{\vv}{{\bm{v}}}
%\newcommand{\vz}{{\bm{z}}}
%\newcommand{\vm}{{\bm{m}}}
\newcommand{\vO}{{\bm{O}}}
\newcommand{\vM}{{\bm{M}}}
\newcommand{\vX}{{\bm{X}}}
\newcommand{\vS}{{\bm{S}}}
\newcommand{\vW}{{\bm{W}}}
\newcommand{\vQ}{{\bm{Q}}}
\newcommand{\vkappa}{{\bm{\kappa}}}
\newcommand{\vV}{{\bm{V}}}
\newcommand{\vA}{{\bm{A}}}
\newcommand{\vR}{{\bm{R}}}
\newcommand{\vxaug}{{\bm{x}_\mathrm{aug}}}
%\newcommand{\vtheta}{{\mbox{\boldmath\ensuremath{\theta}}}}
\newcommand{\Ani}[1]{{\color{blue} [AG: #1]}}
\newcommand{\MCM}[1]{{\color{blue} [MCM: #1]}}
\newcommand{\yb}[1]{{\color{red} [YB: #1]}}
\newcommand{\NRK}[1]{{\color{red} [NRK: #1]}}
\newcommand{\AD}[1]{{\color{red} [AD: #1]}}

\newcommand{\T}{{\mathcal T}}
\newcommand{\G}{{\mathcal G}}
%\newcommand{\E}{{\mathbb E}}
\newcommand{\Proba}{{\mathbb P}}
\usepackage{pifont}% http://ctan.org/pkg/pifont

\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%
% Math Bold
%---------
\newcommand{\bR}{{\mathbb R}}
\newcommand{\bZ}{{\mathbb Z}}
\newcommand{\bS}{{\mathbb S}}

\newcommand{\bull}{{\tiny $\bullet~~$ }}

%\newcommand{\Ani}[1]{{\color{red} [Ani: #1]}}


\newcommand{\highlight}[1]{\colorbox{blue!10}{#1}}
\definecolor{mygray}{gray}{0.4}

\newcommand{\g}[2]{#1\textsubscript{\textcolor{mygray}{$\pm$#2}}}
% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2021} with \usepackage[nohyperref]{icml2021} above.
\usepackage{hyperref}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
\usepackage[accepted]{icml2021}

% If accepted, instead use the following line for the camera-ready submission:
%\usepackage[accepted]{icml2021}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Transformers with Competitive Ensembles of Independent Mechanisms}

\begin{document}

\twocolumn[
\icmltitle{Transformers with Competitive Ensembles of Independent Mechanisms}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2021
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Alex Lamb}{mila}
\icmlauthor{Di He}{msr}
\icmlauthor{Anirudh Goyal}{mila}
\icmlauthor{Guolin Ke}{msr}
\icmlauthor{Chien-Feng Liao}{mila,sinica}
\icmlauthor{Mirco Ravanelli}{mila}
\icmlauthor{Yoshua Bengio}{mila}
\end{icmlauthorlist}

\icmlaffiliation{mila}{Mila, University of Montreal}
\icmlaffiliation{msr}{Microsoft Research Asia}
\icmlaffiliation{sinica}{Research Center for Information Technology Innovation, Academia Sinica}

\icmlcorrespondingauthor{Alex Lamb}{lambalex@iro.umontreal.ca}
\icmlcorrespondingauthor{Di He}{dihe@microsoft.com}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{}%\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
An important development in deep learning from the earliest MLPs has been a move towards architectures with structural inductive biases which enable the model to keep distinct sources of information and routes of processing well-separated.  This structure is linked to the notion of independent mechanisms from the causality literature, in which a mechanism is able to retain the same processing as irrelevant aspects of the world are changed.  For example, convnets enable separation over positions, while attention-based architectures (especially Transformers) learn which combination of positions to process dynamically.  In this work we explore a way in which the Transformer architecture is deficient: it represents each position with a large monolithic hidden representation and a single set of parameters which are applied over the entire hidden representation.  This potentially throws unrelated sources of information together, and limits the Transformer's ability to capture independent mechanisms.  To address this, we propose Transformers with Independent Mechanisms (TIM), a new Transformer layer which divides the hidden representation and parameters into multiple mechanisms, which only exchange information through attention.  Additionally, we propose a competition mechanism which encourages these mechanisms to specialize over time steps, and thus be more independent.  We study TIM on a large-scale BERT model, on the Image Transformer, and on speech enhancement and find evidence for semantically meaningful specialization as well as improved performance.  

\end{abstract}

\section{Introduction}

%Ani_structute
% 
%
%
%


%Independent mechanisms



%for introduction, I guess first thing could be to talk about how structure relates to generalization
%then talk some about RIMs

%then to talk about how Transformers do impose some structure
%then discuss why it's still bad for each step to have a monolithic hidden state and apply one big function to everything.  This means same set of parameters used for handling all types of phenomenon, even unrelated phenomenon.  
%heads vs. mechanisms



%Note: there's something important here about the use of parameters, which I have hard time describing.  
A major theme throughout the history of deep learning has been the introduction of inductive biases in neural architectures,
more recently with a focus on the ability to dynamically keep distinct types of information separated.  While an MLP architecture has one large hidden representation at each layer, a convnet keeps different spatial positions' representations separated by default.  This separation enables more appropriate reuse of parameters, improving generalization (e.g. compared with a fully connected MLP)
by ensuring that some parts of the hidden representation capturing some aspects of the data can remain unchanged when other aspects are changed.  Additionally, it is important to be able to reuse parameters in all situations where the parameters are relevant, and not use parameters in positions where they are irrelevant, and this is where attention
mechanisms can be very useful.

While dividing information between different positions (for example time steps or spatial positions) is already very useful, it has been recognized from
the earliest deep learning work on the notion of disentangling~\citep{bengio2009learning,glorot2011domain,rifai2012disentangling,mathieu2016disentangling,achille2018emergence} that other features of the data could advantageously be kept well-separated, even over overlapping sets of positions.  This has suggested the idea that a model can be decomposed into multiple components, which are often called modules, each operating on a different set of features.  Modularity has been identified as an essential ingredient for generalization in machine learning \citep{jacobs1991adaptive, bottou1991framework,ronco1996modular, reed2015neural, andreas2016neural,rosenbaum2017routing, fernando2017pathnet, shazeer2017outrageously,goyal2019recurrent, goyal2020object, rahaman2020s2rms, goyal2020inductive}.  The motivating intuition is that if the relationship between the modules changes between training and evaluation, then a model which keeps these modules sufficiently separate but can adapt how they are combined could be more robust. It can even be robust to changes where the overall data distribution differs between training and evaluation.  This has been studied in the causality literature through the notion of ``Independent Mechanisms'' \citep{peters2018deep} or causal modules, which can be flexibly re-used,  re-purposed and re-combined.




While modularity and independent mechanisms ideas are closely related, the latter has a special focus on the notion that mechanisms should have the ability to remain unchanged when unrelated aspects of the world are changed.  In that sense it is a more specific idea which builds on the more general concept of modularity.  While the study of independent mechanisms in the context of deep architectures is relatively recent \citep{goyal2019recurrent, mittal2020learning, goyal2020object}, a few ideas are considered central.  One is that mechanisms are separately parameterized (or dynamically parameterized, with the possibility of separation), which means that the function computed by a module remains the same even as other mechanisms need to be changed.  Another central idea is specialization between mechanisms, which is the idea that mechanisms should seek to only model some parts of the world.  One way to help accomplish this is by forcing the mechanisms to compete to explain different positions (in time or space), such that some mechanisms would not be used by the model on positions where they are less relevant.  

%As a simple example of this, suppose that an object recognition system is trained on Earth but evaluated on videos of the same objects but taken on the Moon.  The gravity constant has changed but objects otherwise maintain their visual appearance, shape, solidity, etc. If the handling of the effect of gravity has been disentangled from the effect of other causal factors, then a learner can more easily adapt, with less data, and even use inference (i.e. figuring out the right constant of gravity from the observations) to cope with the change rather than requiring to change its parameters.





In this work we explore how the idea of independent mechanisms can be beneficial in the Transformer architecture.  Transformers ~\citep{vaswani2017attention} are based
on information sharing across positions controlled dynamically by a soft-attention mechanism ~\citep{bahdanau2014neural}, while still using a fully-connected MLP to process the
extracted feature vectors (concatenated over a set of attention heads)
at each position. An important way in which this improves over convnets is that if this attention becomes sufficiently sparse, then it gains the ability to keep information well-separated between different positions.  At the same time, at each position, the Transformer stores a single monolithic hidden representation, over which it applies its entire set of parameters.  For example, if we consider a generative model of images of animals in a field, then some of the parameters like those describing how animals have symmetric eyes or a certain number of feet, are only relevant for the positions in the image where the animal is present.  A normal Transformer, however, would apply the same parameters to the entire hidden representation at all spatial positions.  Additionally, if sources of information need to be accessed over multiple positions, it has no way to keep that information well-separated between parts of the hidden  representation, unless a large fraction of the parameters are set to exactly zero.  In practice, models tend not to learn these sorts of highly sparse parameter matrices as it is not necessary in order to fit the training set. Thus different underlying factors tend to be freely blended together
rather than disentangled: we hypothesize and show empirically that this leads to deteriorated generalization when something about some of these factors changes.  

Our newly proposed technique, which we call \emph{Transformers with Competitive Independent Mechanisms (TIM)} seeks to address this limitation of the Transformer by dividing the hidden representation and parameters into multiple distinct mechanisms.  These mechanisms perform self-attention (over input elements) separately, and information is exchanged sparingly between the mechanisms using attention.  The model is naturally compelled to keep multiple information signals well-separated, even within a single position.  The process of selectively activating some mechanisms and not others relies on competition between
mechanisms, just like in recurrent independent mechanism (RIMs) ~\citep{goyal2019recurrent}. We hypothesize and show empirically that this provides an inductive bias encouraging the mechanisms to be more independent and specialized, more robust to changes only affecting other mechanisms.



%\vspace{-2mm}

\section{Transformers with Competitive Independent Mechanisms}

\begin{figure}%[t!]
    \centering
    \includegraphics[width=1.0\linewidth]{images/model_diagram/tims_fig_2.png}
    \vspace{-3mm}
    \caption{We show the TIM model with two positions and three mechanisms.  First, position-wise attention shares information between positions while keeping the different mechanisms well-separated (which can then be weighted by the competition between mechanisms).  Next, attention is used to share information between the mechanisms at each position.  Finally, a position-wise and mechanism-wise feed-forward layer (including layer norm and residual connections) produces the TIM layer's final output.  }
    \label{fig:timsfig}
    \vspace{-3mm}

\end{figure}

\subsection{Preliminaries}

{\bfseries \itshape Multihead Self-attention sub-layer } The attention mechanism can be formulated as querying a dictionary with key-value pairs \citep{bahdanau2014neural,vaswani2017attention}, e.g., 
\
$\text{Attention}(Q,K,V)=\text{softmax}({QK^T}/{\sqrt{d_{\mathit{model}}}})\cdot V, $
where $d_\mathit{model}$ is the dimensionality of the hidden representations and $Q$ (Query), $K$ (Key), $V$ (Value) are specified as the hidden representations of the previous layer in the so-called \emph{self-attention} sub-layers in the Transformer architecture. The multi-head variant of attention allows the model to jointly attend to information from different representation subspaces, and is defined as
$\text{Multihead}(Q,K,V) = \text{Concat} (\text{head}_1,\cdots,\text{head}_H)W^O$, with the heads defined as: $\text{head}_k = \text{Attention}(QW_k^Q, KW_k^K,VW_k^V)$ where $W_k^Q\in \mathbb{R}^{d_\mathit{model}\times d_K}, W_k^K\in \mathbb{R}^{d_\mathit{model}\times d_K}, W_k^V\in \mathbb{R}^{d_\mathit{model}\times d_V},$ and $W^O\in \mathbb{R}^{Hd_V \times d_\mathit{model}}$ are project parameter matrices, $H$ is the number of heads, and $d_K$ and $d_V$ are the dimensionalities of Key and Value. 



{\bfseries \itshape Group Linear Layer: } It takes multiple hidden representations, and applies a separately parameterized linear transformation to each.  This operation can be efficiently implemented using batched-matrix multiplications.  We set the numbers of groups $n_s$ and define a weight tensor $W \in \mathbb{R}^{n_s \times d_\mathit{in} \times d_\mathit{out}}$.  If the input h is shaped as $h \in \mathbb{R}^{n_s \times d_\mathit{in}}$, then we can define the layer as: $\textit{GroupLinear}(h, W, n_s) = { [ h_j W_j ] }_{j=1}^{n_s}$.  
%\Ani{mention $d_{in}$, $d_{out}$}





%A head is just linear function of one big hidden state.  Then the heads get concatenated together.  So to keep information separate, very large number of parameters need to be set to zero, which the model has to inductive bias to do.  Whereas with multiple mechanisms, separation only requires the inter-mechanism attention to look at the same module, which generally only requires changing a few parameters to saturate the attention.  


%steps: 
%Mechanism Competition sub-layer
%Mechanism-wise Self-attention sub-layer.  
%Inter-mechanism attention sub-layer.  
%Mechanism-wise, Position-wise, FFN sub-layer.  

%Note, need to explain that we keep number of total heads the same when we add more mechanisms, generally.  


\subsection{Specifics of TIM}

We first lay out the parts of a TIM layer and then give more detailed steps in Algorithm~\ref{sec:algodetail}. The proposed method is a drop-in replacement for a standard Transformer layer, and converting an existing Transformer layer to TIM layer involves few changes:

\begin{itemize}
    \item A TIM layer first shares information between positions.  TIM is able to easily keep different streams of processing well-separated by splitting the hidden state and parameters into $n_s$ mechanisms.
    \item Competition over which mechanisms can read from other positions is introduced to encourage to specialization over distinct patterns.  
    \item Attention is used to share information between different mechanisms.  
    \item Position-wise, mechanism-wise feedforward layers are used to process the results of these attention layers.  
\end{itemize}


An overall illustration of how the TIM layer is structured given in Figure~\ref{fig:timsfig}. % and an illustration showing how independent mechanisms differ from heads is given in Figure~\ref{fig:headsmechanisms}.  %We then give a high-level detail of how to turn a transformer layer into a TIM layer in a typical implementation (Section~\ref{sec:integrate}).  

\begin{algorithm}[h]
   \caption{A single TIM Encoder-Layer}
   \label{sec:algodetail}
   \begin{algorithmic}%[1]
   \footnotesize
   \STATE {\bfseries \itshape Hyperparameters:} Number of mechanisms $n_s$, key size $d_k$, value size $d_v$, number of heads for self-attention $H$, number of heads for inter-mechanism attention $H_c$.  We set $d_{mech} = d_{model}/n_s$ and $d_{ffn-m} = d_{ffn}/n_s$ 
   \item[]
    \STATE{\bfseries \itshape Input:} An input hidden representation $h$ for a single example. 
   
  %\begin{multicols}{2}
  
  %\STATE Uses Group Linear layer $\textit{GroupLinear}(h,W,n_g)$ and normal multi-head attention layer $\textit{Attention}(Q, K, V, n_H)$, and layer-normalization $\textit{Norm}(h,n_g)$.  
  
   %\item[]
   %\STATE{\bfseries \itshape Group Linear Layer}
   %\STATE Define a general grouped linear layer method.  For simplicity, the bias terms are omitted.  
   %\STATE Input: h of shape $(T, b, d_{in})$, W of shape $(n_g, d_{in}, d_{out})$
   %\STATE $GLL(h,W,n_g)$
   %\STATE Returns h of shape $(T,b,d_{out})$
   %\item[]
  
   %\STATE{\bfseries \itshape Multi-head Attention}
   %\STATE Define a general multi-head self-attention method.  Just does the attention, no layers.  Has no parameters.  
   %\STATE Input: Q,K of shape $(\textit{pos}, b, H d_{k})$, V of shape $(\textit{pos}, b, H d_{v})$
   %\STATE $MHA(Q, K, V, n_H)$
   %\STATE Output: h of shape $(\textit{pos}, b, H d_{v})$
  
   \item[]
   \STATE{\bfseries \itshape Step 1: Compute Competition Between Mechanisms}
    \STATE \quad $W^c \in \mathbb{R}^{n_s \times d_\mathit{mech} \times 1}$
    \STATE \quad $c = \mathrm{softmax}\big(\mathrm{GroupLinear}(h, W^c, n_s)\big)$

   \item[]
   \STATE{\bfseries \itshape Step 2: Sharing Information Between Positions}
   
   \STATE \quad $W_2^Q, W_2^K \in \mathbb{R}^{n_s \times d_\mathit{mech} \times H d_K}$
   \STATE \quad $W_2^V\in \mathbb{R}^{n_s \times d_\mathit{mech} \times H d_V},$
   \STATE \quad $W_2^O\in \mathbb{R}^{n_s \times Hd_V \times d_\mathit{mech}}$  
   \STATE \textit{Compute query, key, and value:}
   \STATE \quad $Q = \mathrm{GroupLinear}(h, W_2^Q, n_s)$ 
   \STATE \quad $K = \mathrm{GroupLinear}(h, W_2^K, n_s)$
   \STATE \quad $V = \mathrm{GroupLinear}(h, W_2^V, n_s)$
   
     \STATE \textit{Update mechanism hidden states using attention result:}
   \STATE \quad $M := \mathrm{PositionAttention}(Q, K, V, n_s H)$
   \STATE \quad $M := \mathrm{GroupLinear}(M, W_2^O, n_s)$

   \STATE \quad $h := \mathrm{norm}(h + c \odot M, n_s)$

 %  \item[]
 %  \item[]
 %  \item[]
 %  \item[]
 %  \item[]
   \item[]
   
   \STATE{\bfseries \itshape Step 3: Sharing Information Between Mechanisms}

    %(T, bs, n_s*d_model).  First compute the Q,K,V as usual
    %(T,bs,H d_v

   \STATE \quad $W_3^Q,W^K\in \mathbb{R}^{n_s \times d_\mathit{mech} \times H_c d_K}$
   \STATE \quad $W_3^V\in \mathbb{R}^{n_s \times d_\mathit{mech} \times H_c d_V}$
   \STATE \quad $W_3^O\in \mathbb{R}^{n_s \times H_c d_V \times d_\mathit{mech}}$
   \STATE \textit{Compute query, key, and value:}
   \STATE \quad $Q = \mathrm{GroupLinear}(h, W_3^Q, n_s)$
   \STATE \quad $K = \mathrm{GroupLinear}(h, W_3^K, n_s)$
   \STATE \quad $V = \mathrm{GroupLinear}(h, W_3^V, n_s)$
   
   %\STATE \quad Reshape Q,K, and V from $(T,bs,n_s*H_c*d)$ to $(n_s,T*bs,H_c*d)$. 
 
   \STATE \textit{Update mechanism hidden states using attention result:}
   \STATE \quad $M := \mathrm{MechanismAttention}(Q, K, V, H_c)$
   \STATE \quad $M := \mathrm{GroupLinear}(M, W_3^O, n_s)$

   \STATE \quad $h := \mathrm{norm}(h + M, n_s)$

   \item[]
   \STATE{\bfseries \itshape Step 4: Mechanism-wise, Position-Wise, FFN Sub-Layer}
   \STATE \quad $W^{(1)}\in \mathbb{R}^{n_s \times d_\mathit{mech} \times d_\mathit{ffn-m}}$
   \STATE \quad $W^{(2)}\in \mathbb{R}^{n_s \times d_\mathit{ffn-m} \times d_\mathit{mech}}$.  
   \STATE \quad ${\text{F} = \mathrm{GroupLinear}(\sigma(\mathrm{GroupLinear}(h,W^{(1)})),W^{(2)})}$
   \STATE \quad $h := \text{norm}(h + \text{F}, n_s)$
   %\RETURN{} $C$
   %\item[]
%\end{multicols}
\end{algorithmic}

\end{algorithm}


%\subsubsection{Competition between different mechanisms}

{\bfseries \itshape Step 1 Competition between different mechanisms.} Aside from having separate parameters and only exchanging information via inter-mechanism attention, we wanted to create a stronger inductive bias to encourage the mechanisms to specialize.  To do this, we created a competition system in which each mechanism has a layer which outputs a single scalar value (as a function of the current layer's representation), and these are passed through a softmax over the different mechanisms (this softmax is applied position-wise and separately for each layer).  The value of this softmax is then used to weight how much each mechanism is allowed to update its representation after the self-attention.  This competition score is computed as $c = \mathrm{softmax}\big(\mathrm{GroupLinear}(h, W^c, n_s)\big)$, where we note that each mechanism has its own parameters for the layer (hence the use of a Group Linear layer instead of a normal linear layer).  Thus the $n_s$ modules have a per-step weighting for how much they are able to read during the later self-attention stage.  Thus if one mechanism wants to perform attention on a given position, it suppresses the other mechanisms on that position.  We found that this often improved results and that these softmax scores are fairly interpretable as a measure of specialization.  Exact equations for this step are given in Step 1 and used in Step 2 in Algorithm~\ref{sec:algodetail}.  

%\subsubsection{Each Mechanism Shares Information Across Time and Processes Information}

{\bfseries \itshape Step 2 Each mechanism shares information across positions.} This step allows each mechanism to have its own independent dynamics, which are themselves similar to a normal transformer layer.  These independent dynamics allow each mechanism to read information from other time steps using attention and process that information using FFN layers.  We modify the self-attention sub-layer and feed-forward  sub-layers (FFN) to be mechanism-wise as well as position-wise, with separate parameters for each mechanism.  Additionally, the layer-normalization is modified to be performed separately for each mechanism.  The projections and FFN sub-layers can be modified by replacing the linear layers with group linear layers.  When performing the self-attention itself, the mechanisms behave the same as heads, and thus we can use the same type of multi-head attention process, so long as the total number of heads is divisible by the number of mechanisms.  One notable property is if mechanisms only consisted of this part of the model (independent dynamics) by itself, then each mechanism in TIM would be a completely independent transformer model with its own forward pass and its own parameters.  Steps 2 and 4 in  Algorithm~\ref{sec:algodetail} give more detail on this step.  

%\subsubsection{Attention is used to Communication Information Between Different Mechanisms} 

{\bfseries \itshape Step 3 Sharing of information between different mechanisms via attention.}  Although we allow each mechanism to remain independent and process information independently, it is also important to allow the different mechanisms in TIMs to share information between each other (in case the mechanisms are not truly fully independent).  To do this we use a standard multi-head attention sub-layer to share information between the mechanisms, which is done in a position-wise fashion.  We made this attention mechanism relatively small, with just 2 heads with 32 units each.  This is because we want the different mechanisms to be as independent as possible, and thus only share small amounts of high level information.  This can be thought of as another attention layer, where we treat the different mechanisms as positions, and perform this attention in parallel over the different steps in the sequence.  This attention could also be allowed to look at previous layer's mechanisms, which has previously been shown to improve the specialization of layers \citep{lamb2020neural}.  More details on this are given in Step 3 in the appendix's Algorithm~\ref{sec:algodetail}.  

\subsection{Implementing and Integrating TIM}

\label{sec:integrate}

The TIM layer is a drop-in replacement for a standard Transformer layer and turning an existing Transformer layer into a TIM layer is surprisingly straightforward.  It is a drop-in replacement for a single layer which can be flexibly used in a variety of models and architectures (both encoders and decoders).  A simple strategy is if a normal hidden representation is of shape $(T,b,d_{model})$, then our TIM hidden representation should be reshape-able to $(T,b,n_s,d_{model}/n_s)$. First, each layer linear layer in the existing Transformer layer should be replaced by a group-wise linear layer implemented using batch matrix multiplication.  
Second, so long as the number of heads is divisible by the number of mechanisms, the self-attention does not need to be changed, since mechanisms behave interchangeably with heads in this part of the model.  
Third, the inter-mechanism communication can be added as a drop-in module into the Transformer layer.  
Finally, the competition layer is just a single layer with a softmax, which can easily be added.  

Although TIM is a drop-in replacement for a normal Transformer layer, there are a few subtleties that must be considered for successful integration.  First, if the total size of the hidden representation is kept the same, integrating TIM drastically reduces the total number of parameters in the model because all of the linear layers are replaced by grouped-linear layers (which can be thought of as having a block-sparse structure).  This step by itself reduces the number of parameters by a factor of $n_s$, but a TIM layer also adds new parameters to the model through the addition of the Inter-mechanism Attention Sub-Layer and Mechanism-Competition Sub-Layers, although both of these are rather small.  If the total number of hidden units is kept the same, a TIM layer usually reduces the number of parameters by about 30-40\%, depending on the exact hyperparameters.  

%To compensate for this, in all of our experiments we increased the total hidden size to match the number of parameters of the original model, usually by about 20\%.  

Additionally, while we initially thought that it would make sense to replace every Transformer layer with a TIM layer, when we analyzed the mechanism-competition, we found that it was almost always completely flat on the first layer, which suggested to us that the first two layers as well as the last layer should be kept as normal Transformer layers.  

\begin{figure}%[t!]
    \centering
    \includegraphics[width=1.06\linewidth]{images/model_diagram/headsvmechs.pdf}
    \vspace{-1mm}
    \caption{We show a simplified version of the model at a single position to illustrate the difference between heads and mechanisms.  Heads allow for parallel attention, but the differentiation between heads is transient: it begins with the projection layer and ends immediately following the attention.  As a result, most of the parameters are not head-specific.  With independent mechanisms, the information is kept well-separated throughout the entire layer, and all of the layer's parameters are specific to a single mechanism.  }
    \label{fig:headsmechanisms}
    \vspace{-4mm}
\end{figure}

\begin{figure*}[htp!]
    \centering
    \includegraphics[width=\linewidth]{images/cifar_specialization/all_specialization.png}
    %\includegraphics[width=0.7\linewidth,trim={0 0 3.03cm 0},clip]{images/cifar_mnist_sep/cifar_mnist_real.png}
    %\includegraphics[width=0.34\linewidth,trim={0 0 3.6cm 0},clip]{images/cifar_mnist_sep/cifar_mnist_act.png}
    %\includegraphics[width=0.8\linewidth,trim={0cm 0cm 0cm 0.0cm},clip]{images/cifar_mnist_sep/var.png}
    \vspace{-7mm}
    \caption{We trained a TIM version of Image Transformer (pixel-by-pixel, raster-order generative model) with $n_s=2$ and show the activation score for the first mechanism on the bottom row.  On CIFAR-10, we see that the activation of mechanisms is correlated with the background and foreground of the object.  To more directly test the property of independent mechanisms, we constructed a dataset in which the left-half of each image is an MNIST digit and the right-half of the image is a random CIFAR example.  These two sides of the image are independent and follow different dynamics.  We found that TIM learns to specialize over the two sides of the image, with one mechanism only activating on the side with the MNIST digit and one mechanism only activating on the side with the CIFAR example. } %make this caption much more self-contained, literal, and direct.  
    \label{fig:cifar_mnist_special}
    \vspace{-3mm}
\end{figure*}

\vspace{-3mm}
\section{Related Work}
%\Ani{I think, here we need to mention how our work is different from these developments in Transformers. Agree(Di)}

{\bfseries \itshape Specialization and Competition over heads in Transformers.} \citet{cui2019mixed} proposed a mixed multi-head attention mechanism which forces some heads to learn specific patterns, such as attending to precedent/local tokens only.  \citet{clark2019does} studied which positions attention heads focus on and found that some heads have specific patterns, such as attending locally. \citet{vig2020bertology} showed that the heads in a model of protein sequences are semantically meaningful.  \citet{an2020repulsive} considered adding a repulsive force to the heads in Transformers to try to make them more specialized.  In our view, this evidence for specialization over heads is complementary with our results. 

%\citet{michel2019sixteen} studies whether the parallel attention mechanisms (heads) are equally important and necessary. The conclusion is a large number of heads can be pruned with little performance drop.   But some of heads are not explainable.  

{\bfseries \itshape Independent Mechanisms and Modularity in Transformers.} We're not aware of any work which breaks a Transformer's hidden representation into multiple mechanisms with separate parameters which interact through attention, though some works hint at this direction.  The Group Transformer \citep{park2020grouptransformer} replaces the fully-connected layers with group-linear layers and uses low-rank layers to pass information between the groups. The universal transformer \citep{dehghani2018universal} shared parameters between layers and updated using gating, and this gating could behave similarly to the competition that we propose but lacks the idea of having multiple mechanisms.  The Switch Transformer \citep{fedus2021switch} selects different experts for each example but doesn't decompose the hidden state per-position into multiple mechanisms (which have both distinct state and distinct parameters) as we do in TIM.  Combining the Switch Transformer with TIM could be a fruitful area for future research, especially since the switching over mixture of experts could be done different for each mechanism, allowing the switching to be specific to specific sub-patterns within the data.  

{\bfseries \itshape Independent Mechanisms in Recurrent Networks. } The idea of independent  mechanisms has seen a significant amount of focus in recurrent networks \citep{goyal2019recurrent}. The idea is to parameterize the model as an ensemble of mechanisms, having their own dynamics, but sparingly interacting with each other using a bottleneck of attention. In the case of recurrent networks, dividing the hidden representation into mechanisms has the advantage that at a particular time-step, only a fraction of mechanisms can be active, and hence computation is sparse in time, where in the case of transformers, imposing the idea of independent mechanisms in some higher layers has the added advantage that computation can be sparse both in space (i.e., position ) as well as time. 


%Based on the general architecture, several works developed different Transformer variants for different purposes. \citet{gulcehre2018hyperbolic} extended the standard attention and non-linear activation onto the hyperbolic space to better encode hierarchical data. \citet{kitaev2020reformer,beltagy2020longformer,child2019generating} proposed several efficient attention mechanisms to handle long sequences. \citet{katharopoulos2020transformers} established a connection between the Transformer and RNNs using kernels and developed a fast auto-regressive Transformer with linear attention modules. \cite{cui2019mixed} further developed a mixed multi-head attention mechanism to force some heads to learn specific patterns, such as attending to precedent/local tokens only.

\vspace{-2mm}
\section{Experiments}

We seek to answer two questions in our experiments. 
\vspace{-2mm}
\begin{itemize}
    \item Do the mechanisms that we learn with TIM specialize in sensible and semantically meaningful ways?  We analyze this both on toy datasets where we have clearly independent mechanisms by construction (Figure~\ref{fig:cifar_mnist_special}) and on large-scale realistic speech and NLP tasks (Figure~\ref{fig:se} and Figure~\ref{fig:bertanalysis}).
    \item  How using a model which learns these independent mechanisms leads to better quantitative performance, both on the original task and on transfer learning, which we demonstrate in Figure~\ref{tab:enh} and Table~\ref{tab:bert}.  We show substantial improvements on speech enhancement, BERT masked language modeling, and the challenging CATER visual-reasoning task.  
\end{itemize}
%First, do the mechanisms that we learn with TIM specialize in sensible and semantically meaningful ways?  We analyze this both on toy datasets where we have clearly independent mechanisms by construction (Figure~\ref{fig:cifar_mnist_special}) and on large-scale realistic speech and NLP tasks (Figure~\ref{fig:se} and Figure~\ref{fig:bertanalysis}).  Our second question is how using a model which learns these independent mechanisms leads to better quantitative performance, both on the original task and on transfer learning, which we demonstrate in Figure~\ref{tab:enh} and Table~\ref{tab:bert}.  



\subsection{Image Transformer: Evidence of Specialization}
\label{sec:imagetransformer}

We integrated TIM into the Image Transformer, which is a generative model which generates an image pixel-by-pixel, with a small-scale variant of the GPT-2 architecture \citep{mingpt2020,radford2019language}.  We first considered a pedagogic task in which the dataset consists of two clearly independent mechanisms.  Our synthetic task uses MNIST digits \citep{lecun1998mnist} and CIFAR images \cite{krizhevsky2009LearningML} of small realistic images of animals and vehicles.  Each example in our constructed dataset consists of an MNIST digit on its left-side and a CIFAR image on its right-side, with these two examples selected randomly. It is clear that two sides of the image are independent and have completely different types of content, and thus it is natural for each mechanism to specialize over a single side.  

When training with TIM on this dataset, {\bf we found that we were able to nearly exactly recover a competition pattern in which the mechanisms specialize over the two sides of the image } (Fig.~\ref{fig:cifar_mnist_special}, right).  Intriguingly, this specialization does not appear at the very beginning of training, in which the mechanisms mostly specialize over the lightness or darkness of the pixels.  However as training progresses, the two sides of the image become increasingly specialized to one mechanism or the other.  We also visualized the competition pattern with TIM on CIFAR-10 and found a {\bf specialization between foreground and background regions in the images} (Fig.~\ref{fig:cifar_mnist_special}, left).  

%\begin{figure}[t!]
%    %\vspace{-10mm}
%    \centering
%    \includegraphics[width=0.9\linewidth]{images/cifar_%specialization/real.png} \\
%    \includegraphics[width=0.9\linewidth]{images/cifar_%specialization/early.png}\\
%    \includegraphics[width=0.9\linewidth]{images/cifar_%specialization/late.png}
%    \caption{With independent mechanisms, the information is kept well-separated throughout the entire layer, and all of the layer's parameters are specific to a single mechanism.  Competition patterns of an unsupervised image Transformer on CIFAR images (top right) with 2 mechanisms shows that mechanisms learn to specialize over foreground and background patterns on an early layer (center right) and become more confident in a later layer (bottom right).  }
%    \label{fig:specialization_cifar}
%\end{figure}




%overnight, running no-comm ablations
%\todo{image Transformer experiments should be more thorough.  No competition ablation, no communication, 4 modules.  }
%\begin{table}
%    \caption{We compare image Transformer test set likelihoods on various datasets (lower is better).  }
%    \label{fig:image_Transformer}
%    \begin{tabular}{c|c c}
%    \toprule
%    \textit{Task} & \textit{Baseline-6L} & \textit{TIM-6L} \\
%    \midrule
%    CIFAR-10 & 2.037 & \highlight{2.036} \\
%    SVHN & 2.251 & \highlight{2.232} \\
%    CM Hybrid & 1.395 & \highlight{1.392} \\ %2.69,2.70
%    \bottomrule
%    \end{tabular}
%\end{table}

%todo: MNIST -> KMNIST adaptation.  
%todo: run with Celeb-A and see which modules predict attributes.  

\begin{table*}[h!]
    %\vspace{-10mm}
    \centering
    \caption{We compare the baseline BERT models to TIM with and without competition, on both validation likelihood (perplexity) and NLP fine-tuning tasks (reported as accuracy, with median and standard deviation over five fine-tuning trials with different seeds).  We also show that it is essential to make the first and last layers normal Transformer layers and not TIM layers (TIM-All-Layers).  }
    %\vspace{-2mm}
    \resizebox{0.88\linewidth}{!}{
    \begin{tabular}{c|c c c c c}
    \toprule
    \textit{Result} & \textit{BERT}  & \textit{BERT-130M} & \textit{TIM-All-Layers} & \textit{TIM-NoComp} & \textit{TIM-Comp} \\
    TIM Layers & 0/12 & 0/12 & 12/12 & 9/12 & 9/12 \\
    Parameters & 110M  & 130M & 110M & 130M & 130M  \\
    Competition? & \xmark & \xmark & \xmark & \xmark & \checkmark  \\
    \midrule
    Valid-NLL & 2.096 & 2.040 & 2.112 & 2.033 & \highlight{2.027}  \\
    \midrule
    MNLI-M & 84.93 $\pm$ 0.15 & 85.37 $\pm$ 0.29 & 84.19 $\pm$ 0.34 & \highlight{85.89 $\pm$ 0.17} & 85.28 $\pm$ 0.22 \\
    MNLI-MM & 84.91 $\pm$ 0.18 & 85.28 $\pm$ 0.27 & 84.55 $\pm$ 0.15 & \highlight{85.80 $\pm$ 0.07} & 85.17 $\pm$ 0.18 \\
    QNLI & 91.34 $\pm$ 0.21 & 91.84 $\pm$ 0.32 & 91.37 $\pm$ 0.59 & 91.78 $\pm$ 0.14 & \highlight{91.97 $\pm$ 0.20}\\
    SST-2 & 92.88 $\pm$ 0.33 & 92.75 $\pm$ 0.26 & 92.52 $\pm$ 0.56 & 92.75 $\pm$ 0.13 & \highlight{92.97 $\pm$ 0.25} \\
    STS-B & 89.43 $\pm$ 0.25 & 89.34 $\pm$ 0.15 & 88.20 $\pm$ 0.32 & 88.52 $\pm$ 0.28 & \highlight{89.63 $\pm$ 0.05} \\
    \bottomrule
    \end{tabular}}
     \label{tab:bert}
\end{table*}

\begin{figure*}[h!]
    \centering
    \includegraphics[width=1.0\linewidth,trim=4.0cm 0 8.5cm 1.2cm, clip]{images/bert/bert_corr_2.png}
    \vspace{-5mm}
    \caption{On a BERT model, we show the minimum, average, and maximum mechanism competition values of some selected common tokens (left).  One mechanism clearly specializes over the period between sentences, yet the high difference between the minimum and maximum values suggest that these differences are contextual and not a static function of the token.  In particular we found that the modular activation for a period depends on whether it is used to mark the end of a sentence or whether it is used as part of a number or a URL.  Moreover, the mechanism activation is highly correlated between layers (scatter-plot in center, correlation matrix on right).  }
    %\vspace{-3mm}
    \label{fig:bertanalysis}
\end{figure*}

%\subsection{Reinforcement Learning}

\subsection{Speech Enhancement}




\begin{table}[htp!]
    \centering
    \caption{To assess zero-shot transfer generalization we evaluate a model trained on the DNS Speech Enhancement train-set on the voicebank test-set. The performance is reported on wideband PESQ (higher is better). TIM shows improved generalization capabilities in mismatch conditions.}
    \vspace{1mm}
    \resizebox{1.0\linewidth}{!}{
    \begin{tabular}{lcc}
    \toprule
    \textbf{Models} & \textbf{VoiceBank} \\
    \textit{(DNS $\rightarrow$ VoiceBank, Zero-Shot Transfer)} & \textit{(PESQ)} \\
    \midrule
    %\textit{Trained on Voicebank}\\
    %\midrule
    Noisy - no reverb & 1.970 \\ %probably save for appendix
    %Conv-TasNet (b) & 2.630 \\
    %\midrule
    %\textit{Trained on DNS (transfer)}\\
    %\midrule
    Transformer Baseline & 2.517 \\
    TIM-NoComp ($n_s$=4) & 2.503  \\ %2.70, 2.71
    TIM-Comp ($n_s$=2) & \highlight{2.575} \\
    TIM-Comp ($n_s$=4) & 2.540 \\

    \bottomrule
    \end{tabular}}
\end{table}

Speech enhancement aims to improve the quality of speech recordings. A speech signal captured in real environments, in fact, is often corrupted by noise and reverberation that might severely affect its intelligibility. Speech enhancement has long been studied in the research community \citep{Benesty_2014}. Traditional approaches were based on signal processing techniques such as spectral-subtraction or Wiener filtering \citep{boll1979suppression, Ephraim1984, scalart1996speech}. The idea behind these methods is to estimate the noise in non-speech segments and remove it from speech regions. End-to-end deep learning-based speech enhancement has turned out to significantly outperform traditional signal processing methods, and recently using Transformers has led to promising performance \citep{kim2020t}. We believe that TIM fits well with this task because the traditional technique of decomposing the signal into speech and noisy parts and then analyzing these two signals separately embodies the desiderata of independent mechanisms.  %Thus TIM could demonstrate the combined strength of these two approaches.  

\begin{table}[H]
\vspace{-5mm}
  \centering
    \caption{We trained TIM on the DNS speech enhancement dataset and evaluate on the DNS test-set (left).  Results with (*) used additional outside datasets for training.  For external baselines (a) is \citep{choi2020phase}, (b) is \citep{koyama2020exploring}, and (c) is \citep{isik2020poconet}.  }
    \vspace{1mm}
    \label{tab:enh}
    \resizebox{0.8\linewidth}{!}{
    \begin{tabular}{lcc}
    \toprule
    \textbf{Models} & \textbf{Param} & \textbf{DNS} \\
    \textit{(Trained on DNS)} & \textit{(M)} & \textit{(PESQ)} \\
    \midrule
    Noisy - no reverb & n/a & 1.582 \\ %probably save for appendix
    %DARCCN \citep{li2020ioa} & 1.0 & - & 3.17 \\ %save for appendix
    U-Net-MultiScale+ (a) & 3.5 & 2.710 \\
    Conv-TasNet (b)  & 5.1  & 2.730 \\
    %DCCRN \citep{hu2020dccrn} & 3.7 & - & 3.266 \\ %maybe save for appendix
    PoCoNet (c) & 50.0 & 2.722 \\ %alex: I put the *in to separate!
    PoCoNet-SSL* (c) & 50.0 & 2.748\\ %alex: I put the *in to separate!
    \midrule
    Transformer Baseline & 6.1 & 2.727  \\
    TIM-NoComp ($n_s$= 2) & 6.0 & \highlight{2.754}  \\ %2.70, 2.71
    TIM-Comp ($n_s$= 2)   & 6.0 & 2.742  \\
    TIM-Comp ($n_s$= 4)   & 6.0 & 2.730  \\
    \bottomrule
    \end{tabular}}
\end{table}

Table \ref{tab:enh} (left) compares the performance achieved by TIM with other recent systems on the widely-studied Deep Noise Suppression (DNS) dataset \citep{reddy2020interspeech}. DNS is a large corpus composed of roughly 441 hours of clean and noisy speech samples. The clean speech is artificially corrupted with noise sequences from the audioset database, which contains two million human-labeled clips drawn from YouTube videos and belong to about 600 audio events. Noise is added to the clean speech signal using a random signal-to-noise-ratio (SNR) ranging from 0 to 40 dB. We replaced all Transformer layers except for the first two and the last layer with TIM layers and we increased the total number of hidden units and heads (by about 20\%) to match the number of parameters of the baseline, and we used two mechanisms (but achieved slightly worse yet better-than-baseline results with $n_s=4$). The systems are evaluated with the Perceptual Evaluation of Speech Quality (PESQ) score \citep{rix2001perceptual}. To assess the generalization capability of TIM, we tested our model on the Voicebank test-set as well (see Table \ref{tab:enh}-right). Voicebank  \citep{thiemann2013diverse}, in fact, is characterized by noisy conditions different from that of the DNS dataset used for training.  

The results, shown in Table~\ref{tab:enh}, highlight that TIM slightly outperforms the recently-proposed PocoNet \citep{hu2020dccrn} model, which uses additional data and has 8 times the parameters of TIM. 
To the best of our knowledge, TIM achieves the best PESQ performance so far published in the literature on the DNS dataset.
Qualitatively, we found that the competition scores matches our intuition. Indeed, the two mechanisms clearly specialize over speech and non-speech parts of the audio sequence, as shown in Figure ~\ref{fig:se}. Moreover, we intriguingly found that this competition between mechanisms is consistent across layers, starts out with low confidence, and becomes increasingly confident in later layers.
Compared to a standard Transformer, TIM shows superior generalization capabilities. This interesting feature can be appreciated in Table \ref{tab:enh} (right), where we tested our model on a different dataset (VoiceBank). In mismatch conditions, the competition mechanism seems to play a crucial role. 
This finding agrees with our intuition, according to which employing specialized and competing modules can make the model less affected by irrelevant changes of the input distribution. 

%Dash  is used if the number is not reported in the original paper. 




\begin{figure*}[ht!]
    \centering
    %\includegraphics[width=0.32\linewidth]{images/speech/test0.png}
    %\includegraphics[width=0.49\linewidth]{images/speech/test1.png}
    \includegraphics[width=0.30\linewidth]{images/speech/test2.png}
    \includegraphics[width=0.33\linewidth,trim=2.0cm 0cm 1.2cm 0.0cm, clip]{images/speech/speech_layer_correlation.png}
    \includegraphics[width=0.34\linewidth,trim=0.5cm 0cm 1.8cm 1.0cm, clip]{images/speech/speech_layer_entropy.png}
    \caption{An examples of a speech signal (left) with their respective competition patterns over five successive TIM layers (ordered from top to bottom).  In the early layers, the competition is uncertain, but becomes more certain in the deeper layers.  This is further quantified in a correlation matrix of competition over layers (middle) and a plot showing that competition entropy drops in later layers, especially at the lowest percentiles (right).}
    %\vspace{-2mm}
    \label{fig:se}
\end{figure*}



\subsection{BERT Pre-training and Fine-Tuning}
BERT \citep{devlin2018bert} is one of the most popularly used methods to learn the representation of natural language. The BERT model uses a multi-layer Transformer encoder and is trained by the masked language modeling task using Web data corpus \citep{liu2019roberta}. The pre-trained contextual sentence representations have been shown to be effective in a large number of downstream tasks. 

For BERT, we replaced all of the transformer layers except for the first two layers and the last layer with TIM layers (we also report a result where all layers are TIM layers, showing that it leads to worse performance). We used two mechanisms and evenly increased the number of hidden units and total number of heads across all layers to match the number of parameters in the baseline model. 

\textbf{Pre-training.} Following \citet{devlin2018bert}, we used English Wikipedia corpus and BookCorpus for pre-training. By concatenating these two datasets, we obtained a corpus with roughly 3.4 billion words in total.  We trained all model variants with the same procedure and hyperparameters which were tuned on the BERT baseline model.  All models were run on 16 NVIDIA Tesla V100 GPUs.  

\textbf{Fine-tuning.} We used MNLI, MNLI-MM, QNLI, SST-2 and STS-B from the GLUE (\textbf{G}eneral \textbf{L}anguage \textbf{U}nderstanding \textbf{E}valuation) dataset \citep{DBLP:journals/corr/abs-1804-07461} as the downstream tasks to evaluate the performance of the pre-trained models.  Ideally the features learned by BERT would remain useful on these distinct tasks which have relatively small training sets.  

\textbf{Results.} The overall comparison results are shown in Table~\ref{tab:bert}. We found that both TIM-NoComp and TIM-Comp achieve lower perplexities (masked language modeling loss) on the validation dataset compared to the two BERT baselines.  We found generally better and more reliable (less variance between seeds) results when fine-tuning experiments with TIM.  These empirical results show that our proposed TIM is a better model architecture in a wide range of natural language applications.


%Can put in a line graph of BERT NLLs.  



%{\color{red}detaled configurations are put in the appendix currently}
%\subsection{Analysis of Hyperparameters}

%\subsection{Discussion of Modularity }

%-On some dataset, do more than 2 modules.  
%-

\subsection{CATER Occluded Object Tracking}
\vspace{-1mm}

The CATER spatio-temporal reasoning video task \citep{Gridhar2019Cater} involves locating an object's position at the end of a video, even as that object is occluded by other moving objects.  For example, a ball can be hidden under a cup (this process could itself be occluded and not directly observed), followed by the movement of the cup.  Tracking the location of the ball requires reasoning about how objects will move even when they are not directly observable.  We focus on the localization task, in which the goal is to predict the location of the target object in the final frame. This target object may be occluded by other objects hence hiding it from direct visual observation. In this case, it necessary to reason about the movement of the objects to determine where the target is at the end of the scene.  

We first sample frames from the video at a sampling rate of 6 images per second. We pass each sampled frame through a resnet block to get a sequence of feature representations: $\{\vf_1, \vf_2, \vf_3, \ldots, \vf_T\}$. We then pass this sequence through a Transformer, an LSTM, or TIM. This task is setup as a classification task where we have to predict which cell in the $6\times6$ grid contains the target image in the final frame.  We present results in Table~\ref{tab:cater_results}.  We achieved substantial improvements over the transformer baseline and also achieved the biggest improvements when using a large number of mechanisms.  

\begin{table}[hp]
    \centering
    \vspace{-4mm}
    \caption{\textbf{Comparison on CATER Object Tracking}. Here, we compare the Top-1 and Top-5 accuracy of Transformers with TIM. }
    \label{tab:cater_results}
    \renewcommand{\arraystretch}{1.0}
%\setlength{\tabcolsep}{0.5pt}
    \begin{tabular}{|l|c|c|}
    \hline
     \textbf{Model}  & \textbf{Top-1} \% & \textbf{Top-5} \% \\
     \hline
     \textsc{LSTM} & 67.4 & 85.8 \\
     \textsc{Transformer}  & 68.7  & 81.7 \\
     \textsc{TIM-Comp ($n_s=2$)} & 68.4 & 85.7  \\
     \textsc{TIM-Comp ($n_s=4$)} & 71.0 & \highlight{87.3} \\
     \textsc{TIM-Comp ($n_s=8$)} & \highlight{71.1} & 87.2 \\
\hline    
    \end{tabular}
\end{table}


%\section{Discussion: RNN Modularity vs. Transformer Modularity}
%\vspace{-2mm}
%A single layer RNN is already a fairly powerful model which can have strong priors to inform how to select mechanisms (or modules more generally).  However, a single layer Transformer is a rather weak model, as it can only base its representations on a single round of attention based upon the individual tokens and their position encoding.  We've consistently found that the quality of the mechanism-competition is poor in the first layer of a Transformer network and that performance is substantially improved by making the early layers use ordinary Transformer layers rather than TIM layers. This is in contrast with what has been observed with RNNs, where improvements can be obtained by using multiple modules or multiple mechanisms even in a single layer model. 



%Theoretical Analysis using RNN Equivalence Theory

%\section{Future Work}

%It would be interesting to explore ways of making the mechanisms equivariant.  

% Our softmax-based competition shows interesting specialization results, but potentially the question of how to do competition is still very open.  

%Computational sparsity over the use of the modules could be an interesting avenue towards more efficient Transformer training or deployment.  While this is a challenging direction, because it requires non-standard implementations, it's potentially very promising, as it seems absurd to think that every single parameter needs to be computed at every single position.  Our specialization results hint towards a way of bypassing this requirement, but it would require the competition to be completely sparse (i.e. some values exactly zero).  Our experiments suggest that if this is possible, it is only possible on the later layers, and with our training set up it does not seem to happen very often on some tasks (for example, BERT).  

%\newpage
\vspace{-5mm}
\section{Conclusion}
\vspace{-1mm}

Scaling to extremely large Transformers with a very large number of hidden units for each position has become one of the dominant paradigms in applied machine learning. This work explores a new direction in the structure of the Transformer architecture which will become increasingly important as models become larger and researchers seek to model more complex phenomena.  Evidence suggests that the Transformer's success is a result of its use of attention to communicate information between positions, which allows for effective and precise transmission of information even over very long sequences \citep{kaplan2020scaling}.  At the same time, each position within a Transformer is still represented with a single monolithic hidden representation, and a set of parameters which is applied over the entire hidden representation.  Our newly proposed technique, TIM, has shown that it is possible to make the Transformer even more dynamic by breaking the hidden representation and layers into multiple mechanisms which interact via attention and have an inductive bias towards specialization.  We show that these mechanisms specialize over distinct parts of the data and improve results across diverse types of data.  These results suggest that there is room to improve the structural inductive biases in the Transformer and point towards an increasingly central area of future research as state-of-the-art Transformers, and the tasks they're trained on, become larger and more diverse.  



\clearpage
% FOR FINAL VERSION
%\section{Acknowledgements}
%We thank David Reitter for comments on an earlier draft of the paper.
%\section{Broader Impact Section}
%\section*{Broader Impact }


%This paper aims at advancing conceptual theories of cognition and AI using novel machine learning architectures and experiments. Like any other advances in AI if successful, it could lead downstream to nefarious applications as well as to beneficial applications, and as such the follow-up of this research deserves the continuous attention to possible future uses with the social impact in mind.


%\bibliographystyle{plainnat}
%\bibliography{main}



\bibliography{references}
\bibliographystyle{icml2021}

\clearpage

\appendix


\input{appendix}

\end{document}
