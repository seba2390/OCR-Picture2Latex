\iffalse %original figure
\begin{figure*}[!ht]
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[scale=0.6]{figures/smoteBeforeSplitting.pdf}
        \label{fig:sampling_leakage1}
        \vspace{1cm}
    \end{minipage}
    \hfill
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[scale=0.6]{figures/smoteAfterSplit.pdf}
        \label{fig:sampling_leakage2}
    \end{minipage}
    \vfill
    \begin{minipage}{0.45\textwidth}
        \centering
        \subcaption{Sampling Leakge}
        \begin{tabular}{lrrrr}
            \toprule
            {} &  precision &  recall &  f1-score &   support \\
            \midrule
            Existing Customer &      0.84 &   0.83 &     0.84 & 2,550.00 \\
            Attrited Customer &      0.83 &   0.85 &     0.84 & 2,550.00 \\
            accuracy          &      0.84 &   0.84 &     0.84 &    0.84 \\
            macro avg         &      0.84 &   0.84 &     0.84 & 5,100.00 \\
            weighted avg      &      0.84 &   0.84 &     0.84 & 5,100.00 \\
        \bottomrule
        \end{tabular}
        \label{tab:samp_leak}
    \end{minipage}
    \hfill
    \begin{minipage}{0.45\textwidth}
        \centering
        \subcaption{No Sampling Leakge}
        \begin{tabular}{lrrrr}
            \toprule
            {} &  precision &  recall &  f1-score &   support \\
            \midrule
            Existing Customer &      0.96 &   0.83 &     0.89 & 2,551.00 \\
            Attrited Customer &      0.48 &   0.81 &     0.60 &   488.00 \\
            accuracy          &      0.83 &   0.83 &     0.83 &   0.83 \\
            macro avg         &      0.72 &   0.82 &     0.75 & 3,039.00 \\
            weighted avg      &      0.88 &   0.83 &     0.84 & 3,039.00 \\
            \bottomrule
        \end{tabular}
        \label{tab:no_samp_leak}
    \end{minipage}
    \caption{Here we characterize the effect of oversampling in the financial application of banking customer churn (i.e., Attrition) detection. (a) Illustrates the pipeline wherein oversampling is carried out before separating the data for training and evaluation. (b) Illustrates the oversampling pipeline wherein the data for training and evaluation is first separated and only the training dataset is over-sampled. We can see that the pipeline in (a) shows better optimistic performance (i.e., F1 score for \emph{Attrited} class in Table~\ref{tab:samp_leak}) than (b) (i.e., F1 score for \emph{Attrited} class in Table~\ref{tab:no_samp_leak}) due to leakage in information from over-sampling before selecting the test set}
    \label{fig:sampling_leakage_plots}
\end{figure*}
\fi 
\begin{figure*}[!ht]
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[scale=0.6]{figures/smoteBeforeSplitting.pdf}
        \label{fig:sampling_leakage1}
        \vspace{1cm}
    \end{minipage}
    \hfill
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[scale=0.6]{figures/smoteAfterSplit.pdf}
        \label{fig:sampling_leakage2}
    \end{minipage}
    \vfill
    \begin{minipage}{0.45\textwidth}
        \centering
        \subcaption{Sampling Leakge}
        \begin{tabular}{lrrrr}
            \toprule
            {} &  Prec. &  Rec. &  F1 &   Sup. \\
            \midrule
            $\neg$ Attrited &      0.84 &   0.83 &     0.84 & 2,550.00 \\
            Attrited &      0.83 &   0.85 &     0.84 & 2,550.00 \\
            acc.          &      0.84 &   0.84 &     0.84 &    5,100.00 \\
            macro avg.         &      0.84 &   0.84 &     0.84 & 5,100.00 \\
            wt. avg.      &      0.84 &   0.84 &     0.84 & 5,100.00 \\
        \bottomrule
        \end{tabular}
        \label{tab:samp_leak}
    \end{minipage}
    \hfill
    \begin{minipage}{0.45\textwidth}
        \centering
        \subcaption{No Sampling Leakge}
        \begin{tabular}{lrrrr}
            \toprule
            {} &  Prec. &  Rec. &  F1 &   Sup. \\
            \midrule
            $\neg$ Attrited &      0.96 &   0.83 &     0.89 & 2,551.00 \\
            Attrited &      0.48 &   0.81 &     0.60 &   488.00 \\
            acc.          &      0.83 &   0.83 &     0.83 &   3,039.00 \\
            macro avg.         &      0.72 &   0.82 &     0.75 & 3,039.00 \\
            wt. avg.      &      0.88 &   0.83 &     0.84 & 3,039.00 \\
            \bottomrule
        \end{tabular}
        \label{tab:no_samp_leak}
    \end{minipage}
    \caption{Here we characterize the effect of oversampling in the financial application of banking customer churn (i.e., Attrition) detection. (a) Illustrates the pipeline wherein oversampling is carried out before separating the data for training and evaluation. (b) Illustrates the oversampling pipeline wherein the data for training and evaluation is first separated and only the training dataset is over-sampled. We can see that the pipeline in (a) shows better optimistic performance (i.e., F1 score for \emph{Attrited} class in Table~\ref{tab:samp_leak}) than (b) (i.e., F1 score for \emph{Attrited} class in Table~\ref{tab:no_samp_leak}) due to leakage in information from over-sampling before selecting the test set}
    \label{fig:sampling_leakage_plots}
\end{figure*}
\subsection{Data Leakage AntiPattern}\label{sec:data_leakage}
The separation of
training and test while
extolled in every ML101 course can sometimes be violated in insidious ways. Data leakage refers broadly to 
scenarios wherein
a model makes use of information that it is not supposed to have or would not have available in production. Data leakage leads to overly optimistic model performance estimates and poses serious downstream problems upon model deployment (specifically in high risk applications).
Leakage can happen sometimes unintentionally
when feature selection is driven by model validation or test performance or due to the presence of (typically unavailable) features highly correlated with the label. 
Samala et. al.~\cite{samala2020hazards} talk more about the hazards of leakage, paying
particular attention to
medical imaging applications. In our domain of financial analytics, 
increasingly complex features are constantly developed such that their complexity masks underlying temporal
dependencies which are
often the primary causes
of leakage. Below are
specific leakage
antipatterns we have encountered.

\subsubsection{\bf Peek-a-Boo AntiPattern}
Many source time-series datasets are based on reporting that lags the
actual measurement. A good example is Jobs data which is reported in the following month.
Modelers who are simply consuming this data may not be cognizant that the date of availability
lags the date of the data, and unwittingly include it in their models inappropriately.

\subsubsection{\bf Temporal Leakage AntiPattern.}
When constructing training and test datasets by sampling, the process
by which such sampling
is conducted can cause
leakage and thus lead to
not truly independent
training and test
sets.
In forecasting
problems, especially,
temporal leakage happens
when the training and
test split is not
carried out sequentially, thereby leading to high correlation (owing to temporal dependence and causality) between the two sets. 
    
\subsubsection{\bf Oversampling Leakage AntiPattern.}
An egregious form of leakage can be
termed \emph{oversampling leakage}, seen in situations involving a minority class. A well known strategy to use in imbalanced classification is to perform minority over-sampling, e.g., via an algorithm such as SMOTE~\cite{smote}. In such situations, if over-sampling is performed {\it before} splitting into training and test sets, then there is a possibility of information leakage. Due to the subtle nature of this type of leakage, we showcase illustrations and performance characterizations of oversampling leakage, in the context of customer churn detection in banking transactions in Fig.~\ref{fig:sampling_leakage_plots}. 

\subsubsection{\bf Metrics-from-Beyond AntiPattern.} 
This type of antipattern
can also be seen as
pre-processing leakage or
hyper-parameter leakage.
Often times, due to carelessness in pre-processing data, both training and test datasets are grouped and standardized together leading to leakage of test data statistics. For example when using standard normalization, if test and train datasets are normalized together, then the  sample mean and variance used for normalization is a function of the test set and thus
leakage has occurred.
