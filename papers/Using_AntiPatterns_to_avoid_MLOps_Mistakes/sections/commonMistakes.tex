The runaway success of machine learning models has given rise to a better understanding of the technical challenges underlying their widespread deployment~\cite{paleyes2020challenges,sculley2015hidden}. There is now a
viewpoint~\cite{isbell2020} encouraging the rethinking of ML as a software
engineering enterprise.
MLOps---Machine Learning Operations---refers to the
body of work that focuses
on the full
lifecycle of ML model deployment, performance
tracking, and ensuring
stability in
production pipelines. 

At the Bank of New-York Mellon (a large-scale investment banking, custodial banking, and asset servicing enterprise) we have developed a range of enterprise-scale ML pipelines, spanning areas such as customer attrition forecasting, predicting
treasury settlement failures, and balance prediction. In deploying
these pipelines,
we have encountered several recurring antipatterns~\cite{antipatterns}
that we wish to document
in this paper.
Just as design patterns codify best software engineering practices, antipatterns provide a vocabulary to describe defective practices and methodologies.
Antipatterns often
turn out to be commonly utilized approaches that are actually bad,
in the sense that the consequences outweigh any benefits. Using antipatterns to desribe what is happening helps ML teams get past any blamestorming and arrive at a  refactored solution more quickly. While we do not provide a completed formal antipattern taxonomy,
our intent here is
to
support better documentation of issues, rapid communication
between stakeholders, and faster resolution of problems.

Our goals are similar to
the work of~\cite{sculley2015hidden} that argues for
the study of MLOps through
the lens of {\it hidden
technical debt.} While many of the lessons from~\cite{sculley2015hidden} dovetail with our
own conclusions, our
perspective here is
complementary, viz. we focus less on software engineering but more on data pipelines, how data is transduced into decisions, and how feedback from decisions can (and should) be
used to adjust and improve the ML pipeline.
In
particular, our study recognizes
the role of multiple
stakeholders (beyond ML
developers) who play crucial
roles in the success of
ML systems.

Our main contributions are:
\begin{enumerate}
    \item We provide a vocabulary of antipatterns that we have encountered in ML pipelines, especially in the financial analytics domain. While many appear obvious in retrospect we believe cataloging them here will contribute to greater understanding and maturity of ML pipelines.
    \item We argue for a new approach that rethinks ML deployment not just in terms of predictive performance but in terms of a multi-stage decision making loop involving humans. This leads to a more nuanced understanding of ML objectives and how evaluation criteria dovetail with deployment considerations.
    %\item We describe approaches to reduce unfairness and bias that we have found relevant in the enterprise.
    \item Finally, similar to Model Cards~\cite{modelcards}, we provide several recommendations for documenting and managing MLOps at an enterprise scale. In particular we describe the crucial role played by model certification authorities in the enterprise.
\end{enumerate}

\section{Case Study: Forecasting Treasury Fails}
\input{sections/treasuryfails}

%%%%%%% AntiPatterns Table 
\begin{table}[!ht]
\centering
\caption{Nine commonly practiced AntiPatterns.}
\begin{tabular}{c|l}
\toprule
\textbf{Stage} & \textbf{Name} \\ \toprule
\multirow{2}{*}{\textbf{\makecell{Design \& \\ Development}}} & Data Leakage\\ \cline{2-2}
\vspace{-0.2cm}
\\&  Tuning-under-the-Carpet \\ \midrule\midrule
\multirow{3}{*}{\hspace{-0.1cm}\textbf{\makecell{Performance Evaluation}}} &  `PEST' \\ \cline{2-2}
& Bad Credit Assignment \\ \cline{2-2}
& Grade-your-own-Exam \\ \midrule\midrule
\multirow{4}{*}{\textbf{\makecell{Deployment \& \\Maintenance}}} & `Act Now, Reflect Never' \\ \cline{2-2}
& Set \& Forget \\ \cline{2-2}
& `Communicate with Ambivalence'\\ \cline{2-2}
&  `Data Crisis as a Service'\\ \bottomrule
\end{tabular}
\label{tab:antipatterns_summary}
\end{table}

\section{AntiPatterns}
For the most part, we present our antipatterns (summarized in Table~\ref{tab:antipatterns_summary}) in a supervised learning or forecasting context. In a production ML context, there is typically a model that has been approved for daily use. Over time, such a model might be replaced by a newer (e.g., more accurate) model, or retrained with more recent data (but keeping existing hyperparameters or ranges constant or fixed), or retrained with new search for hyperparameters in addition to retraining with recent data. %These \emph{hyperparameters} have a significant effect of the representation learned by the model. 
In this process, we encounter a range of methodological issues leading to several antipatterns, which we identify below.

\input{sections/data_leakage}
\input{sections/act_now_reflect_never}
\input{sections/hyperparameter_tuning}
\input{sections/kiss_antipattern}
\input{sections/bad_credit_assignment}
\input{sections/grade_your_own_exam}
\subsection{Set \& Forget AntiPattern}\label{sec:concept_drift}
    A core assumption of machine learning (ML) pipelines is that the data generating process being sampled from (for training and when the model is deployed in production) generates samples that are \textit{independent} and \textit{identically distributed} (i.i.d). ML pipelines predominantly adopt a `set \& forget' mentality to model training and inference. However, it is quite often the case that the statistical properties of the target variable that the learning model is trying to predict change over time (\textit{concept drift}~\cite{widmer1996learning}). Decision support systems governed by data-driven models are required to effectively handle concept drift and yield accurate decisions. The primary technique to handle concept drift is learning rule sets using techniques based on decision trees and other similar interpretable tree-based approaches. Domingos et al.~\cite{domingos2000mining} proposed a model based on Hoeffding trees. Klienberg et al.~\cite{klinkenberg2004learning}, propose sliding window and instance weighting methods to maintain the learning model consistent with the most recent (albeit drifted) data. Various other approaches based on rule sets, Bayesian modeling have been developed for detection and adaptation of concept drift, details can be found in~\cite{gama2014survey,lu2018learning,webb2016characterizing}.
    An example of model drift adaptation can be seen in Chakraborty et al.~\cite{hqcd} for forecasting protest events. This work provides a use case wherein changes in surrogates can be used to detect change points in the target series with lower delay than just using the target's history.

\subsection{`Communicate with Ambivalence' AntiPattern}\label{sec:calibration}
Most ML pipelines are
tuned to generate
predictions but little
attention is paid to ensure that the model can sufficiently communicate information about its own uncertainty. A well-calibrated model is one where the Brier score (or similar) is carefully calibrated in its confidence. 
%See Fig.~\ref{briers} for examples of well- and poorly-calibrated models.
When poorly calibrated models
are placed in production,
it becomes difficult to
introduce compensatory
or remedial pipelines when
something goes wrong. 
Characterizing
model uncertainty is thus
a paramount feature for
large-scale deployment.
Recent work~\cite{bhatt2020uncertainty} shows that
in addition to explainability, conveying uncertainty can be a significant contributor to ensuring trust in ML pipelines.

\subsection{`Data Crisis as a Service' 
AntiPattern}\label{sec:data_crisis_as_a_service}
The development of models using data manually extracted and hygiened without recording the extraction or hygiene steps leads to
a massive 
data preparation challenge
for
later attempts to validate (or even deploy) ML
models.
 This is often the result of `sensitive' data that is selectively sanitized for the modelers by some third-party data steward organization that cannot adequately determine the risk associated with direct data access. The data preparation steps are effectively swept under the carpet and must be completely re-invented later, often with surprising impact on the models because the pipeline ends up producing different data.

The refactored solution here is to: (i) ensure that your enterprise sets up a professional data engineering practice that can quickly build and support new data pipelines that are governed and resilient; (ii) use
assertions to track
data as they move through
the pipeline, and
(iii) track the pedigree and lineage of all
data products, 
including intermediaries.
We've found graph databases to be ideal for maintaining linkages between data objects and the many assertions you must track.
\begin{figure*}[!ht]
    \begin{minipage}{0.45\textwidth}
    \centering
    \includegraphics[scale=0.34]{figures/modelops_stage_1.png}
    \subcaption{Current state of ML deployment pipeline evaluation focuses only on the single stage performance of the ML model.}
    \label{fig:ml_deployment_1}
    \end{minipage}
    \hfill
    \begin{minipage}{0.45\textwidth}
    \centering
    \includegraphics[scale=0.36]{figures/modelops_stage_2.png}
    \subcaption{ML deployment pipelines are actually multi-stage decision systems with a hierarchical setup of learning, notification and intervention layers, each requiring evaluation.}
    \label{fig:ml_deployment_2}
    \end{minipage}
    \vfill
    \begin{minipage}{0.6\textwidth}
    \centering
    \includegraphics[scale=0.4]{figures/modelops_stage_3.png}
    \subcaption{A characterization (with state-specific illustrations) of the various operational states the ML deployment pipeline can assume.}
    \label{fig:ml_deployment_3}
    \end{minipage}
    \caption{Characterization of the updated ML deployment pipeline and the need for monitoring an expanded set of possible operational states.}
\end{figure*}