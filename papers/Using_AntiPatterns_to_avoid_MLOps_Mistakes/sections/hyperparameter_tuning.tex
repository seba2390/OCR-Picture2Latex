\iffalse We can abstract our ML problems as embodying a learning procedure whose goal is to learn a function $f_\theta(x)$ where $x \mathtt{\sim} {D_X}$ represents an instance of input features drawn from (unknown) distribution $D_X$ and $\theta$ represents the set of parameters of the learning model to be tuned.

Given, \textit{n} independent samples $X = \{x_1,..,x_n\}$ and corresponding $Y = \{y_i \mathtt{\sim} T_Y | y_i = g(x_i)\}$, drawn from (unknown) target distribution $T_Y$ the goal of learning pipelines is to minimize a loss $\mathcal{L}(y;f_\theta(x))$ by tuning the parameters $\theta \in \Theta$ (where $\Theta$ is the parameter space) such that a close approximation of the unknown function $g(x_i)$ is learned. 
\iffalse 
\begin{equation}
    \theta^* = \mathrm{arg\,min}_{\theta \in \Theta} \mathbb{E}_{x \mathtt{\sim} D_X}\big[\mathcal{L}(y,f_{\theta}(x)) \big]
    \label{eq:loss_function}
\end{equation}
Eq.~\ref{eq:loss_function} indicates that the function $f(\cdot)$ is minimized with respect to $\theta \in \Theta$ (where $\Theta$ is the parameter space) yielding an approximation of the unknown target distribution $T_Y$ and the corresponding optimal value of parameters $\theta^*$.
\fi 

The function $f(\cdot)$, in addition to learning parameters $\theta$ is also governed by a separate set of \textit{hyper-parameters} $\lambda \in \Lambda$. Learning models often have multiple components of the  pipeline influenced by these user-governed hyper-parameters. Hence the overall learning objective can be stated formally as: 
\begin{equation}
    \lambda^*,\theta^* = \mathrm{arg\,min}_{\lambda \in \Lambda;\theta \in \Theta} \mathbb{E}_{x \mathtt{\sim} D_X}\big[\mathcal{L}(y,f_{\lambda,\theta}(x)) \big]
    \label{eq:generic_loss_function}
\end{equation}

\iffalse In Eq.~\ref{eq:generic_loss_function}, $f(\cdot)$ is a function of both hyper-parameters $\lambda \in \Lambda$ and learning parameters $\theta \in \Theta$ both of which are tuned.\fi Hence Eq.~\ref{eq:generic_loss_function} can be viewed as consisting of two separate optimization procedures, one with respect to hyper-parameters $\lambda$ and the other with respect to parameters $\theta$.

The goal of any learning procedure (governed by Eq.~\ref{eq:generic_loss_function}) is essentially to learn a function $f_{\lambda^*,\theta^*}(\cdot)$ that has low \textit{generalization error} (i.e., the error on unseen data) on $X^{test} = \{x_i | x_i \mathtt{\sim} D_X\}$. 

To achieve this goal while also yielding a model with low generalization error, Eq.~\ref{eq:generic_loss_function} is usually evaluated on a separate (limited) set of data $X^{val} = \{x_i | x_i \mathtt{\sim} D_X\}, Y^{val} = \{y_i | y_i \mathtt{\sim} T_Y \}$. Usually using $X^{val}$ to tune $\lambda$. This approach is termed \textit{hyper-parameter optimization}.
\fi 


\subsection{Tuning-under-the-Carpet AntiPattern}\label{sec:hyperparameter_tuning}
Different values of hyper-parameters often prove to be significant drivers of model performance and are expensive to tune and mostly task specific. Hyper-parameters play such a crucial role in modeling architectures that entire research efforts are devoted to developing efficient hyper-parameter search strategies~\cite{bergstra2013making,nguyen2019bayesian,henderson2018deep,van2018hyperparameter,probst2019tunability}.

The set of hyper-parameters differs for different learning algorithms. For instance, even a simple classification model like the decision tree classifier, has hyper-parameters like the maximum depth of the tree, the minimum number of samples to split an internal node and the criterion to use for estimating either the impurity at a node (gini) or the information gain (entropy) at each node. Ensemble models like random forest classifiers and gradient boosting machines also have additional parameters governing the number of estimators (trees) to include in the model. Another popular classifier, the support-vector machine which is a maximum margin classifier requires the specification of hyper-parameters that govern the type of kernel used (polynomial, radial-basis-function, linear etc.) as well as the penalty for mis-classification which in-turn governs the margin of the decision boundary learned. For an exhaustive analysis of the effect of hyper-parameters, please refer to~\cite{van2018hyperparameter} wherein the authors perform a detailed analysis of the important hyper-parameters (along with appropriate prior distributions for each) for a wide range of learning models.
\begin{figure*}[!ht]
    \begin{minipage}{0.28\textwidth}
        \centering
        \subcaption{Parzen Tree Estimator based\\ Hyperparameter Tuning}
        \begin{tabular}{p{1.3cm}p{0.4cm}p{0.4cm}p{0.4cm}p{0.4cm}}
            \toprule
            {} &  Prec. &  Rec. &  F1 &   Sup. \\
            \midrule
            $\neg$ Attrited &0.98& 0.98& 0.98&2543\\
            Attrited & 0.91& 0.88& 0.89&496\\
            acc.          & & & 0.97&3039\\
            macro avg.         & 0.94& 0.93& 0.94&3039\\
            wt. avg.      & 0.97& 0.97& 0.97&3039\\
        \bottomrule
        \end{tabular}
        \label{tab:hyp_tuning}
    \end{minipage}
    \hfill
    \begin{minipage}{0.28\textwidth}
        \centering
        \subcaption{No Hyperparameter Tuning\\ (Manually Set)}
        \begin{tabular}{p{1.3cm}p{0.4cm}p{0.4cm}p{0.4cm}p{0.4cm}}
            \toprule
            {} &  Prec. &  Rec. &  F1 &   Sup. \\
            \midrule
            $\neg$ Attrited & 0.97& 0.98& 0.97&2543 \\
            Attrited & 0.89& 0.84& 0.86&496 \\
            acc.          & & & 0.96&3039 \\
            macro avg.         & 0.93&0.91 & 0.92&3039 \\
            wt. avg.      & 0.96& 0.96& 0.96&3039 \\
            \bottomrule
        \end{tabular}
    \label{tab:no_hyp_tuning}
    \end{minipage}
    \hfill
    \begin{minipage}{0.42\textwidth}
    \centering
    \subcaption{Parameter Importance Plot}
    \includegraphics[scale=0.333]{figures/hyperparameter_tuning_figures/parameter_importances_new.png}
    \label{fig:hyp_param_importance}
    \end{minipage}
    \caption{(a) Results of Attrition prediction task using XGboost classifier with hyperparameters tuned using tree structured Parzen estimator~\cite{bergstra2011algorithms,bergstra2013making}. (b) Results for the same task with XGBoost classifier trained with manually set hyperaparameters. We notice a drop in both precision (Prec.) and recall (Rec.) of the attrited customer (i.e., minority) class. (c) The parameter importance plot depicts importance of each hyper-parameter in trained classifier; we notice learning-rate is by far the most important hyperparameter to be tuned followed by n-estimators (i.e., number of trees), used in the XGboost ensemble. }
    \label{fig:hyp_optimization_plots}
\end{figure*}

The resurgent and recently popular learning methodology employing deep neural networks also has hyper-parameters like the hidden size of intermediate layers, the types of units to employ in the network architecture (fully connected, recurrent, convolutional), the types of activation functions (TanH, ReLU, Sigmoid), and types of regularizations to employ (Dropout layers, Batch Normalization, Strided-Convolutions, Pooling, $L_k$-norm regularization terms). In the context of deep learning models, this area of research is termed neural architecture search~\cite{elsken2019neural}.
\iffalse 
\begin{figure*}[!ht]
    \begin{minipage}{0.45\textwidth}
    \centering
    \includegraphics[scale=0.35]{figures/hyperparameter_tuning_figures/optimization_history.png}
    \subcaption{Optimization History Plot}
    \label{fig:hyp_opthist}
    \end{minipage}
    \hfill
    \begin{minipage}{0.45\textwidth}
    \centering
    \includegraphics[scale=0.35]{figures/hyperparameter_tuning_figures/parallel_coordinate_plot.png}
    \subcaption{Parallel Coordinate Plot}
    \label{fig:hyp_parallel_coord}
    \end{minipage}
    \vfill
    \begin{minipage}{0.45\textwidth}
    \centering
    \includegraphics[scale=0.35]{figures/hyperparameter_tuning_figures/parameter_importances.png}
    \subcaption{Parameter Importance Plot}
    \label{fig:hyp_param_importance}
    \end{minipage}
    \hfill
    \begin{minipage}{0.45\textwidth}
    \centering
    \includegraphics[scale=0.38]{figures/hyperparameter_tuning_figures/contour_plot.png}
    \subcaption{Hyperparameter Optimization Contour Plots}
    \label{fig:hyp_contour}
    \end{minipage}
    \caption{(a) Here, we showcase the progression of the task loss as a function of the hyper-parameter optimization procedure on the customer churn detection task. (b) The parallel coordinate plot showcases relationships between various hyper-parameters. (c) Shows the importance of each hyper-parameters (d) Optimization contour characterizes the sampling regions in the function space of each hyper-parameter. }
    \label{fig:hyp_optimization_plots}
\end{figure*}
\fi  
\iffalse %%%% Original Figure
\begin{figure*}[!ht]
    \begin{minipage}{0.45\textwidth}
        \centering
        \subcaption{Parzen Tree Estimator based Hyperparameter Tuning}
        \begin{tabular}{lrrrr}
            \toprule
            {} &  precision &  recall &  f1-score &   support \\
            \midrule
            Existing Customer &0.98& 0.98& 0.98&2543\\
            Attrited Customer & 0.91& 0.88& 0.89&496\\
            accuracy          & & & 0.97&3039\\
            macro avg         & 0.94& 0.93& 0.94&3039\\
            weighted avg      & 0.97& 0.97& 0.97&3039\\
        \bottomrule
        \end{tabular}
        \label{tab:hyp_tuning}
    \end{minipage}
    \hfill
    \begin{minipage}{0.45\textwidth}
        \centering
        \subcaption{No Hyperparameter Tuning (Manually Set)}
        \begin{tabular}{lrrrr}
            \toprule
            {} &  precision &  recall &  f1-score &   support \\
            \midrule
            Existing Customer & 0.97& 0.98& 0.97&2543 \\
            Attrited Customer & 0.89& 0.84& 0.86&496 \\
            accuracy          & & & 0.96&3039 \\
            macro avg         & 0.93&0.91 & 0.92&3039 \\
            weighted avg      & 0.96& 0.96& 0.96&3039 \\
            \bottomrule
        \end{tabular}
    \label{tab:no_hyp_tuning}
    \end{minipage}
    \vfill
    \begin{minipage}{0.45\textwidth}
    \centering
    \includegraphics[scale=0.35]{figures/hyperparameter_tuning_figures/parameter_importances_new.png}
    \subcaption{Parameter Importance Plot}
    \label{fig:hyp_param_importance}
    \end{minipage}
    \caption{(a) Results of a churn prediction task using an XGboost classifier with hyperparameters tuned using a tree structured Parzen estimator~\cite{bergstra2011algorithms,bergstra2013making}. (b) Results for the same task with the XGBoost classifier trained with manually set hyperaparameters. We notice a drop in both the precision and recall of the attrited customer (i.e., minority) class. (c) The parameter importance plot depicts the importance of each hyper-parameter in the trained classifier; we notice that the learning-rate is by far the most important hyperparameter to be tuned followed by the n-estimators (i.e., number of trees) to be used in the XGboost ensemble. }
    \label{fig:hyp_optimization_plots}
\end{figure*}
\fi
Hyper-parameter optimization, has been conducted in multiple ways, however thus far a combination of manual tuning of hyper-parameters with grid-search approaches have proven to be the most effective~\cite{lecun2012efficient,larochelle2007empirical,hinton2012practical} in searching over the space of hyper-parameters. In~\cite{bergstra2012random}, the authors propose that random search (within a manually assigned range) of the hyper-parameter space yields a computationally cheap and an equivalent if not superior alternative to grid search based hyper-parameter optimization. Yet other approaches pose the hyper-parameter search as a Bayesian optimization problem~\cite{joy2016hyperparameter,snoek2012practical} over the search space.
Fig.~\ref{fig:hyp_optimization_plots} characterizes the optimization process on the learning task of detecting ``churn" or customer attrition using their activity patterns in the context of banking transactions. The figures therein yield an analysis of the hyper-parameter optimization process characterizing the relative importance of each hyper-parameter employed in the learning pipeline. As hyper-parameters play such a crucial role in learning (e.g., we notice from the statistics in Fig.~\ref{tab:hyp_tuning} that an XGBoost model is able to achieve a 3.5\% improvement in the F1 score of detecting attrited customers relative to an XGBoost variant without hyperparameter tuning i.e.,  Fig.~\ref{tab:no_hyp_tuning}), it is imperative that the part of a learning pipeline concerned with hyper-parameter optimization be explicitly and painstakingly documented so as to be reproducible and easily adaptable.