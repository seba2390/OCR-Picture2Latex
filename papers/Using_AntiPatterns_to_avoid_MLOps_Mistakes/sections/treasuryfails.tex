
\begin{figure*}
\begin{minipage}{0.4\textwidth}
\includegraphics[scale=0.2]{figures/figures_from_blog/blog_fig1_percentage_change_fails.jpeg}
\subcaption{}
\label{fig:percentage_change_fails}
\end{minipage}
\hfill
\begin{minipage}{0.45\textwidth}
\includegraphics[scale=0.28]{figures/figures_from_blog/blog_fig2_crunchtime.jpeg}
\vspace{1.3em}
\subcaption{}
\label{fig:crunchtime}
\end{minipage}
\caption{Forecasting US Treasury Settlement Fails. (a) Heightened market volatility during March and April from COVID-19, when traders switched to working remotely, led to difficulties for firms in making sure everything was running smoothly. The larger volume of securities settlements in that period contributed to a higher number of fails. (b) Settlement instructions submitted between 2am and 7am NY time have a proportionally higher failure rate because the trade instructions are submitted with less visibility into the day’s market conditions. }
\label{fig:blog_fig1_fig2}
\end{figure*}

Capital markets have long suffered from a nagging problem: every day, roughly 2\% of all U.S. Treasuries and mortgage-backed securities set to change hands between buyers and sellers do not end up with their new owners by the time they are supposed to arrive.
Such `fails' happen for
many reasons, e.g.,
unique patterns in trading,
supply and demand imbalances,
speediness of given
securities, operational
hiccups, or credit events.
After the collapse
of Lehman Brothers, which led to an increase in settlement fails, the
Treasury Market Practices Group (TPMG) in our organization recommended daily
penalty charges on fails
to promote better market
functioning. 
The failed-to party
generally requests and
recoups the TPMG fails
charge from the non-delivering counterparty. After broad
industry adoption, according to the Federal
Reserve, the prevailing rate of settlement fails has fallen considerably.


In the middle of the COVID-19 market crisis, demand for cash and cash-like instruments such as Treasuries was drastically higher than normal, compounding the issue of settlement fails. Fig.~\ref{fig:percentage_change_fails} showcases the fallout of COVID-19 on the market in the form of settlement fails during March and April 2020. Liquidity issues in the Treasury market prompted the Fed to step in and buy more of the securities to restore calm.

We \iffalse BNY Mellon has\fi have developed a machine learning service that
uses intraday metrics
and other signals as early indicators of liquidity issues in specific sets of bonds to forecast
settlement
failures by 1:30pm daily
NY time.  The service also takes into account elements like the velocity of trading in a given security across different time horizons, the volume of bonds circulating, a bond’s scarcity, the number of trades settled every hour and any operational issues, such as higher-than-normal cancellation rates. Fig.~\ref{fig:crunchtime} showcases the daily failure rate dynamics (per hour) and characterizes the complexity of the task that the aforementioned machine learning service is modeling.
The resulting predictions help \iffalse BNY Mellon’s\fi our clients, including bond dealers, to monitor their intraday positions much more closely, manage down their liquidity buffers for more effective regulatory capital treatment, and offset their risks of failed settlements.
Through this and other ML services we have gained significant insight into MLOps issues that we aim to showcase here.

In developing and deploying this application, we encounter issues such as:
\begin{enumerate}
    \item Does the data processing pipeline have unintended side-effects on modeling due to data leakage or HARKing~\cite{gencoglu2019hark}? (Sections ~\ref{sec:data_leakage},~\ref{sec:testing_and_evaluation})
   \item What happens when models `misbehave' in production? How is this misbehavior measured? Are there compensatory or remedial pipelines? (Sections~\ref{sec:meta_modeling},~\ref{sec:calibration})
    \item How often are models re-trained and what is the process necessary to tune models? Is the training and model tuning reproducible? (Section ~\ref{sec:hyperparameter_tuning})
    \item How is model performance assessed and tracked to ensure compliance with performance requirements? (Sections ~\ref{sec:baselines},~\ref{sec:bad_credit_assignment})
    \item What constitutes a material change in the MLOps pipeline? How are changes handled? (Section~\ref{sec:concept_drift})
    \item Where does the input data reside and how is it prepared on a regular basis for input to an ML model? (Section~\ref{sec:data_crisis_as_a_service})
\end{enumerate}
\iffalse 
\begin{enumerate}
    \item Where does the input data reside and how is it prepared on a regular basis for input to an ML model? (Section~\ref{sec:data_crisis_as_a_service})
    \item How often are models re-trained and what is the process necessary to tune models? Is the training and model tuning reproducible? (Section ~\ref{sec:hyperparameter_tuning})
    \item How is model performance assessed and tracked to ensure compliance with performance requirements? (Sections ~\ref{sec:baselines},~\ref{sec:bad_credit_assignment})
    \item Does the data processing pipeline have unintended side-effects on modeling due to data leakage or HARKing~\cite{gencoglu2019hark}? (Sections ~\ref{sec:data_leakage},~\ref{sec:testing_and_evaluation})
    \item Who certifies models for production and how often?
    \item What happens when models `misbehave' in production? How is this misbehavior measured? Are there compensatory or remedial pipelines? (Sections~\ref{sec:meta_modeling},~\ref{sec:calibration})
    \item What constitutes a material change in the MLOps pipeline? How are changes handled? (Section~\ref{sec:concept_drift})
\end{enumerate}
\fi 
\noindent
Any organization employing ML in production needs to grapple with (at least) each of the questions above. In the process of doing so, they might encounter several
antipatterns as we document below.