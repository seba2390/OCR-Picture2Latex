\subsection{`PEST' AntiPattern}\label{sec:baselines}
Like many applied scientific disciplines, machine learning (ML) research is driven by the empirical verification and validation of theoretical proposals. Novel contributions to applied machine learning research comprise (i) validation of previously unverified theoretical proposals, (ii) new theoretical proposals coupled with empirical verification, or (iii) effective augmentations to existing learning pipelines to yield improved empirical performance. Sound empirical verification requires a fair evaluation of the proposed approach w.r.t previously proposed approaches to assess
empirical performance. However, it is quite often the case that empirical verification of newly proposed ML methodologies is insufficient, flawed, or found wanting. In such cases, the reported empirical gains are actually just an occurrence of the \emph{Perceived Empirical SuperioriTy} (PEST) antipattern. 

For example, in~\cite{henderson2018deep}, the authors question claimed advances in reinforcement learning research due to the lack of significance metrics and variability of results. In~\cite{melis2017state}, the authors state that many years of claimed superiority in empirical performance in the field of language modeling is actually faulty and showcase that the well-known stacked LSTM architecture (with appropriate hyperparameter tuning) outperforms other more recent and more sophisticated architectures. In~\cite{mukhoti2018importance}, the authors highlight a flaw in many previous research works (in the context of Bayesian deep learning) wherein a well established baseline (Monte Carlo dropout) when run to completion (i.e., when learning is not cut-off preemptively by setting it to terminate after a specified number of iterations), achieves similar or superior results compared to the very same models which showcased superior results when introduced. The authors thereby motivate the need for identical experimental pipelines for comparison and evaluation of ML models. In~\cite{shen2018baseline}, authors conduct an extensive comparative analysis of the supposed state-of-the-art word embedding models with a
`simple-word-embedding-model'
(SWEM) and find that the SWEM model yields performance comparable or superior to previously claimed (and more complicated) state-of-the-art models. In our financial analytics context, we have found the KISS principle to encourage developers to try simple models first and to conduct an exhaustive comparison of models before advocating for specific methodologies. Recent benchmark pipelines like the GLUE and SQuAD benchmarks~\cite{wang2018glue,rajpurkar2016squad} are potential ways to address the PEST antipattern.
\iffalse Fig.~\ref{fig:baseline_eval_plots} showcases two types of unfair model comparisons wherein baselines are ill-suited for the problem being modeled (Fig.~\ref{fig:unfair_baseline_comparison})or  the effect of employing inappropriate architectures (Fig.~\ref{fig:fair_model_architecture}) for the data being modeled.\fi 

\iffalse
\begin{figure*}[!ht]
    \begin{minipage}{0.45\textwidth}
    \centering
    \includegraphics[scale=0.2]{figures/blank_fig.png}
    \subcaption{Table depicting model performance of inappropriately chosen baseline models.}
    \label{fig:unfair_baseline_comparison}
    \end{minipage}
    \hfill
    \begin{minipage}{0.45\textwidth}
    \centering
    \includegraphics[scale=0.2]{figures/blank_fig.png}
    \subcaption{Table depicting model performance of appropriately chosen baseline models.}
    \label{fig:fair_baseline_comparison}
    \end{minipage}
    \vfill
    \begin{minipage}{\textwidth}
    \centering
    \includegraphics[scale=0.35]{figures/baselines_figures/fair_baseline_eval.png}
    \subcaption{In the figures, we notice that the choice of hyperparameters of the SVM (in this case the kernel to be used) has a significant effect on the ability of the model to fit the data.}
    \label{fig:fair_model_architecture}
    \end{minipage}
    \caption{Fairness in evaluation needs to be ensured at multiple levels. (a) and (b) highlight the importance of choosing appropriate baseline models to compare with as a function of the task at hand, to yield a strong baseline comparison experiment. (c) In the figures, we notice that the choice of hyperparameters of the SVM (in this case the kernel to be used) has a significant effect on the ability of the model to fit the data. This highlights the importance of choice of model specifically in the context of the inherent architecture chosen for modeling.}
    \label{fig:baseline_eval_plots}
\end{figure*}
\fi
\iffalse 
\begin{figure*}[!ht]
    \begin{minipage}{\textwidth}
    \centering
    \includegraphics[scale=0.35]{figures/baselines_figures/fair_baseline_eval.png}
    %\label{fig:fair_model_architecture}
    \end{minipage}
    \caption{In the figures, we notice that the choice of hyperparameters of the SVM (in this case the kernel to be used) has a significant effect on the ability of the model to fit the data. This highlights the importance of choice of model specifically in the context of the inherent architecture chosen for modeling.}
    \label{fig:baseline_eval_plots}
\end{figure*}
\fi 