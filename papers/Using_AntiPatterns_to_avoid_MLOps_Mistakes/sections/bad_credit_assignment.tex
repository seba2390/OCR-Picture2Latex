\subsection{Bad Credit Assignment AntiPattern}\label{sec:bad_credit_assignment}
Another frequent troubling trend in ML modeling is the failure to appropriately identify the source of performance gains in a modeling pipeline. As the peer-review process encourages technical novelty, quite often, research work focuses on proposing empirically superior, and complicated model architectures. Such empirical superiority is explained to be a function of the novel architecture while it is most often the case that the performance gains are in fact a function of clever problem formulations, data preprocessing, hyper-parameter tuning, or the application of existing well-established methods to interesting new tasks as detailed by~\cite{lipton2018troubling}. 

Whenever possible, it is imperative that effective ablation studies highlighting the performance gains of each component of a newly proposed learning models be included as part of the empirical evaluation. There must also be a concerted effort to train and evaluate baselines and the proposed model(s) in comparable experimental settings. Finally as noted in~\cite{lipton2018troubling}, if ablation studies are infeasible, quantifying the error behavior~\cite{kwiatkowski2013scaling} and robustness~\cite{cotterell2018all} of the proposed model can also yield significant insights about model behavior.