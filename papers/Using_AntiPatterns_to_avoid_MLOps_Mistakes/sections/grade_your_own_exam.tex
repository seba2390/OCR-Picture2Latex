\subsection{Grade-your-own-Exam AntiPattern}\label{sec:testing_and_evaluation}
Usually modeling projects begin as curiosity-driven iterations to explore for potential traction. The measure of traction is calculated somewhat informally without formal 3rd-party review or validation. While not a problem at first, if the data science team continues this practice long enough, while building confidence in their results, they never validate them and cannot compare unvalidated results against other methods. To avoid this antipattern,
testing and evaluation data should be sampled independently, and for a robust performance analysis, should be kept hidden until model development is complete and must be used only for final valuation. 
In practice, it is not
uncommon for
model developers to have access to the final test set and by repeated testing against this known test set, modify their model accordingly to improve performance on the known test set. This practice called HARKing (Hypothesizing After Results are Known) has been detailed by Gencoglu et al.~\cite{gencoglu2019hark}. This leads to implicit data leakage. Cawley et. al.~\cite{cawley} discusses the potential effects of not having a statistically `pure' test set such as over-fitting and selection bias in performance evaluation.

The refactored solution here is not simple, but is essential and necessary for effective governance and oversight. Data science
teams must establish an independent `Ground Truth system' with APIs to receive and catalog all forecasts and the data that were used to make them. This system can provide a reliable date stamp that accurately reflects when any data object or forecast was actually made available or made and can help
track independent 3rd party metrics that will stand up to audit.

\iffalse We characterize the effect of such unfair evaluation practices on an application in financial fraud detection in Fig.~\ref{fig:bling_testing_and_evaluation}.



\begin{figure*}[!ht]
    \begin{minipage}{0.45\textwidth}
    \centering
    \includegraphics[scale=0.2]{figures/blank_fig.png}
    \subcaption{Pipeline with unfair evaluation with cognizance of test data.}
    \label{fig:nonblind_testing_pipeline}
    \end{minipage}
    \hfill
    \begin{minipage}{0.45\textwidth}
    \centering
    \includegraphics[scale=0.2]{figures/blank_fig.png}
    \subcaption{Pipeline with fair evaluation without cognizance of the test data during training and model development.}
    \label{fig:blind_testing_pipeline}
    \end{minipage}
    \vfill
    \begin{minipage}{0.45\textwidth}
    \centering
    \includegraphics[scale=0.2]{figures/blank_fig.png}
    \subcaption{Performance using unfair evaluation pipeline.}
    \label{fig:nonblind_testing_performance}
    \end{minipage}
    \hfill
    \begin{minipage}{0.45\textwidth}
    \centering
    \includegraphics[scale=0.2]{figures/blank_fig.png}
    \subcaption{Performance using \emph{fair} evaluation pipeline.}
    \label{fig:blind_testing_performance}
    \end{minipage}
    \caption{ (a) and (b) illustrate pipelines with and without access respectively to the test set during model development and training. The pipeline depicted in (a) is hence unfair and leads to a biased model development process. Results in tables (c), (d) showcases the effect of biased (unfair) and unbiased (fair) model evaluation respectively.}
    \label{fig:blind_testing_and_evaluation}
\end{figure*}
\fi 