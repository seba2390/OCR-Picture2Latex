% USE OF PDFTeX IS NOW REQUIRED
\documentclass[letterpaper]{article}
\usepackage{aaai}
\usepackage{times}
\usepackage{helvet}
\usepackage{courier}
\usepackage{url}
\usepackage{graphicx}
%\frenchspacing

\usepackage{caption}
\usepackage{subcaption}
 
\setlength{\pdfpagewidth}{8.5in} 
\setlength{\pdfpageheight}{11in}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% IMPORTANT -- ADDITION OF A PDF MARK WITH YOUR PAPER TITLE
% AND ALL AUTHOR NAMES IS REQUIRED ON ALL AAAI PAPERS
% COMMENT OUT AUTHOR NAMES FOR SUBMISSION
% ENABLE AUTHOR NAMES FOR FINAL CAMERA READY COPY
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% PDFINFO for PDFTeX
% Uncomment and complete the following for metadata if
% your paper is typeset using PDFTeX
\pdfinfo{
/Title (Confirmation detection in human-agent interaction using non-lexical speech cues)
/Author (Mara Brandt, Britta Wrede, Franz Kummert, Lars Schillingmann)
/Keywords (speech, feature extraction, SVM, stacked formants)
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Section Numbers
% Uncomment if you want to use section numbers
% and change the 0 to a 1 or 2
\setcounter{secnumdepth}{2}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title{Confirmation detection in human-agent interaction using non-lexical speech cues}
\author{Mara Brandt, Britta Wrede, Franz Kummert and Lars Schillingmann\\ Cluster of Excellence Cognitive Interaction Technology (CITEC), Bielefeld University, 33615 Bielefeld, Germany \\ \texttt{\{mbrandt, bwrede, franz, lschilli\} at techfak.uni-bielefeld.de}}
 
\begin{document}
\maketitle

\begin{abstract}
Even if only the acoustic channel is considered, human communication is highly multi-modal. Non-lexical cues provide a variety of information such as emotion or agreement.
The ability to process such cues is highly relevant for spoken dialog systems, especially in assistance systems. In this paper, we focus on the recognition of non-lexical confirmations such as "mhm", as they enhance the system's ability to accurately interpret human intent in natural communication.
We implemented and evaluated a system for online detection of non-lexical confirmations. The architecture uses a Support Vector Machine to detect confirmations based on acoustic features. In a systematic comparison, several feature sets were evaluated for their performance on a corpus of human-agent interaction in a setting with naive users including elderly and cognitively impaired people. Our results show that using stacked formants as features yield an accuracy of 84\% outperforming regular formants and MFCC or pitch based features for online classification.
\end{abstract}

\section{Introduction}\label{sec:Introduction}
% Britta: Deutsch ist auch eine Tonsprache, Literatur?


In human-machine interaction it is important to provide an intuitive interface for the users that allows them to make free use of the modality space. 
%Classic intuitive modalities that are being investigated for HCI are, for example, gesture and gaze. 
Among humans, speech is one of the most important modalities to communicate information, although non-verbal modalities such as gaze, gesture or action have been shown to be highly relevant for establishing common ground as well. 

Speech contains discourse particles or interjections which are important markers about the speaker's attitude \cite{Anderson2000}. 
%(Anderson and Fretheim 2000). 
These particles, that can be part of an utterance or also standalone (interjections), help to ground not only propositional meaning but also convey epistemic states \cite{Fischer2007}. 
%(Fetzer and Fischer, 2007). 
Epistemic states pertain to the attitude of a speaker towards the information i.e. whether the speaker believes that the propositional content of an utterance (the own or the interlocutor's) is new and surprising or is already grounded, whether the speaker believes that the information is correct etc.
This information is highly relevant also in HCI which still tends to be quite brittle with respect to grounding. It is important to make use of these rather subtle cues to infer the user's attitude towards the current interaction. 
%In HCI one important information pertains to the dialog itself, i.e. whether or not the interaction is going well and the user has understood what has been said. 
It is therefore important for a human-agent interaction to change the dialog structure according to the perceived internal state of the user by slowing down or repeating if uncertainty is perceived, or by continuing if the user is confirming.

However, discourse particles/markers or interjections have certain characteristics that render them difficult for automatic recognition with standard ASR approaches: 
For one, their use and surface structure is highly variable between different speakers \cite{Bell2001}.
% (Bell and Gustafson).
Second, discourse particles are often characterized by stylized intonations, i.e. significantly different intonation patterns than ''normal'' speech \cite{Gibbon1997}.
%(Gibbon and Sassen, 1997). 
Indeed, it has been shown that extreme values for prosodic features yield a much higher word error rate in ASR systems \cite{Jurafsky2010}
% (Jurafsky...) 
making it difficult to recognize the lexical units of discourse particles. But also, the meaning of discourse particles depends on the underlying intonation \cite{Gibbon1997}.
However, standard approaches to ASR do explicitly not take prosodic information into account.
%(this probably also pertains even to end-to-end deep learning approaches).

In order to understand the meaning of the discourse particle, it is thus necessary to develop new approaches and investigate their acoustic nature.




%Natural communication important - the user can interact naturally
%audiovisual communication with the system
%Speech recognition not enough, doesn't recognize non-verbal utterances, such as filled-pauses/confirmations?
%FPs as confirmations appear often in natural communication. As sign of understanding, while the other one is talking or simply instead of yes
%FPs can transport affect/socio-emotional signals: uncertainty, hesitations
%
%Important for human-agent/robot-interaction : ability to change the dialog structure according to the perceived internal state of the user : slow down/repeat if perceived uncertainty. Continue to speak, if user confirms with FPs

%First step: focus on FPs for confirmation to enable natural interaction in addition to ASR/dialog system
%Backchannel, discourse markers, feedback can transport important information for the dialog.
%Meta-information is important, not only spoken text - discourse particle (Lotz et al, 2016)
%backchannel/feedback/listener response/discourse particle
%
%prosodic features on very short utterances : Hendrik
%
%confirmation used instead of verbal confirmation user-dependent

One important feedback signal in dialogs is positive acknowledgment which indicates that the listener is still hearing and understanding what is being said. These feedback signals are often called ''filled-pauses'' and contain generally non-lexical acoustic units such as ''mhm'' or ''aha''. It has been shown that this feedback can be used by interaction partners to infer the listener's meta-cognitive state \cite{Brennan1995}.

While the phonetic realizations may be variable, it has been shown that their prosodic cues remain very stable with a very slowly and smoothly declining F0 \cite{Tsiaras}.
%Filled pause detection F0 declines very slowly and very smoothly \cite{Tsiaras}
More specifically, fillers have a flat pitch, which lies in the median of pitch of the user across all his utterances \cite{Garg2006}.
%Improve the performance of automatic speech recognition software, predict mental state of user \cite{Audhkhasi2009}
Also, filled-pauses show a very specific articulation in that the articulators do not change their positions, yielding very stable formants and minimal coarticulation effects \cite{Audhkhasi2009}.
This is acoustically reflected in small fundamental frequency transitions and small spectral envelope deformations %under the assumption that speakers do not change articulator parameters during filled pauses 
\cite{Goto1999}.


\section{Dataset}
\subsection{Scenario}
Our research is part of the KOMPASS project \cite{YaghoubzadehBuschmeier2015}. In this project, a virtual agent ``BILLIE'' is developed to help elderly and cognitively impaired people to plan and structure their daily activities, get reminders and suggestions for possible activities. The users interact naturally with the system to enter their appointments, therefore the system needs to understand natural language inputs and react to feedback. In addition to visual cues for understanding and confirmation, e.g. nodding, it is important for the dialog system to detect non-lexical confirmations like ``mhm'', because the automatic speech recognition (ASR) typically doesn't recognize them.
\subsection{User Study}
As part of the KOMPASS project a user study with participants of the intended user groups, elderly and cognitively impaired people, was conducted. The study was performed as a Wizard of Oz experiment \cite{Kelley1984}. 52 participants, consisting of 18 elderly (f: 14, m: 4), 18 cognitively impaired (f: 10, m: 8) and 16 students (f: 10, m: 6) with German as their first language, interacted with ``BILLIE`` and planned their daily activities for one week (Fig. \ref{fig:WOZ1-interaction}). The participants only got instructions to enter their appointments naturally in German without being instructed to use any special commands or phrases, resulting in natural communication with the system.

\begin{figure}[t]
	\centering
	\includegraphics[width=\columnwidth,height=\textheight,keepaspectratio]{images/WOZ1-interaction}
	\caption{Interaction with the virtual agent ''BILLIE''}
	\label{fig:WOZ1-interaction}
\end{figure}
\subsection{Annotations}
The KOMPASS WOZ1 corpus was annotated automatically and manually. Voice activity detection (VAD) was used to divide the speech signal into segments of continuous speech. 
All segments are automatically annotated as regular utterances, unless they contain non-lexical confirmations, which were manually annotated. The distribution of regular utterances and non-lexical confirmations as well as the subsets used for this evaluation can be seen in Tab. \ref{tab:WOZ1-corpus}. 
The regular utterances contain 394 manually annotated filled-pauses, e.g. elongations and fillers, some of them similar to confirmations, e.g. ''hmm``, which can lead to false-positives in the detection of non-lexical confirmations. 
%All filled-pauses and confirmations were manually annotated by our student worker.
\begin{table}[t]
	\resizebox{\columnwidth}{!}{
	\begin{tabular}{*{5}{c}}
		\hline
		set & \#participants (f, m) & \#all segments & \#confirmations \\
		\hline
		WOZ1 data & 52 (f: 34, m: 18) & 5385 & 129 \\
		Training set & 17 (f: 14, m: 3) & 1885 & 87 \\
		Test set & 4 (f: 3, m: 1) & 415 & 42 \\
		\hline
	\end{tabular}
	}
	\caption{WOZ1 corpus segment distribution and used subsets for training with cross-validation and testing}
	\label{tab:WOZ1-corpus}
\end{table}
\section{Non-Lexical Confirmation Detection System}
\subsection{Architecture}
\begin{figure}[t]
	\centering
	\includegraphics[width=\columnwidth,height=\textheight,keepaspectratio]{images/system-diagram-crop.pdf}
	\caption{System architecture}
	\label{fig:system-diagram}
\end{figure}
\begin{figure}[t]
	\centering
	\includegraphics[width=\columnwidth,height=\textheight,keepaspectratio]{images/source_filter_model}
	\caption{Source-filter model: vocal cords and vocal tract based on \cite{Philippsen}}
	\label{fig:Source-filter}
\end{figure}
We developed a system for the recognition of non-lexical confirmations that can handle sound input from files or microphone recording. The architecture of this system is shown in Fig. \ref{fig:system-diagram}.
To support both offline and online processing with mostly the same algorithms, the software consists of modules that can be used in both modes. 
The input, using either sound files or a microphone as source, is chunked into overlapping frames of 25ms with a frame shift of 10ms.
After that, the frames are windowed with a Blackman-Harris window \cite{Harris1978} for the MFCC related feature sets or a Hann window \cite{Blackman1959} for formant and pitch based feature sets and the different selected features are extracted frame by frame.
Principal Component Analysis (PCA) \cite{Pearson1901} is performed to reduce the dimensionality of the feature vectors of feature sets with stacked features or derivatives except for stacked formants, which is necessary because of the high dimensionality of the stacked features and the small amount of data.
In training mode, a Support Vector Machine (SVM) \cite{Vapnik1995} is trained as described in Sec. \ref{sec:SVM-training} and the trained model is serialized for later classification tasks. 
For the classification mode, the same steps are required, but instead of the SVM training, the sound input is classified frame by frame with the deserialized trained SVM model.
The classification results are calculated according to the description of offline and online classification in Sec. \ref{sec:classification}.
\begin{figure*}[t]
	\centering
	\begin{subfigure}{.5\textwidth}
	  \centering
	  \includegraphics[width=\columnwidth,height=\textheight,keepaspectratio]{images/roc_plot.pdf}
	  \caption{ROC plot of the first test set}
	  \label{fig:sub1}
	\end{subfigure}%
	\begin{subfigure}{.5\textwidth}
	  \centering
	  \includegraphics[width=\columnwidth,height=\textheight,keepaspectratio]{images/roc_plot02.pdf}
	  \caption{ROC plot of the second test set}
	  \label{fig:sub2}
	\end{subfigure}
	\caption{ROC curves of classifiers with different feature sets}
	\label{fig:roc_curve}
\end{figure*}

\subsection{Feature Extraction/Selection}\label{sec:feature-extraction}
As argued above, different features may be suited for recognizing filled pauses. On the one hand, the pitch contour appears to be very salient, thus F0 becomes an interesting feature. On the other hand, the vocal tract (Fig. \ref{fig:Source-filter}) remains stable, which would be reflected in the formants and the MFCCs. The different features are described in this section and an overview of the resulting feature vectors with corresponding sizes are shown in Tab. \ref{tab:feature-vectors}.
\paragraph{Mel-Frequency Cepstral Coefficients}
Mel-frequency cepstral coefficients (MFCCs) are features common in speech recognition. They reflect properties of the vocal tract during speech production and mimic human perception of speech. The coefficients are designed to mitigate speaker dependent characteristics. MFCCs are extracted with the Essentia framework \cite{Bogdanov2013}. For this the sound signal is windowed with a Blackman-Harris window and the spectrum of this window is computed. After that, the first 13 MFCCs in the frequency range from 20 Hz to 7800 Hz are calculated with 40 mel-bands in the filter.
\paragraph{Differentiation}
To capture the salient articulation we also calculated the first and second derivatives of the MFCCs ($\Delta$ and $\Delta\Delta$). For this the polynomial filter introduced by Savitzky and Golay \cite{Savitzky1964} is used which combines differentiation and smoothing. The general formula for this filter is shown in Equ. \ref{equ:savitzky-golay-formular}, where $n$ is the filter length, $a_i$ are the coefficients and $h$ is the normalization factor.
\begin{equation}
y_t=\frac{1}{h}\sum_{i=-\frac{n-1}{2}}^{\frac{n-1}{2}}a_ix_{t+i}
\label{equ:savitzky-golay-formular}
\end{equation}
Savitzky and Golay provide coefficients to use for the calculation of the derivatives (Tab. \ref{tab:SG-coeffs}).

\begin{table}[h]
	\resizebox{\columnwidth}{!}{
	\begin{tabular}{*{4}{c}}
		\hline
		derivative & filter length & coefficients & h \\
		& & & (normalization factor) \\
		\hline
		first & 7 & -3, -2, -1, 0, 1, 2, 3 & 28\\
		\hline
		second & 7 & 5, 0, -3, -4, -3, 0, 5 & 42\\
		\hline
	\end{tabular}
	}
	\caption{Savitzky-Golay filter coefficients}
	\label{tab:SG-coeffs}
\end{table}
\paragraph{Stacked MFCCs}
Stacked MFCCs are another way to model the context and dynamics of MFCCs and can outperform MFCC derivatives \cite{Heck2013}. Instead of calculating the derivatives, the 13 MFCCs of adjacent frames are stacked to form a single feature vector. 15 stacked frames, resulting in a 195-dimensional feature vector, yield the best results in terms of true positive and false positive rate for non-lexical confirmation recognition.
\paragraph{Formants}
As formants are directly correlated with movements of the vocal tract, they should be able to provide good features for filled-pause detection (see Sec. \ref{sec:Introduction}). Non-lexical confirmations are very similar to some filled-pauses, so formants can be used for non-lexical confirmation detection. The linear predictive coding (LPC) algorithm of the Essentia framework is used to calculate linear predictive coefficients of the order 12. These coefficients are used to calculate the polynomial roots using the Eigen3 PolynomialSolver algorithm \cite{eigenweb}. Subsequently, the roots are fixed into the unit circle. The first two formant frequencies which show the configuration of the vocal tract are calculated from these fixed roots.
To measure the stability of the formants, the standard deviation of each formant over 15 frames is calculated and added as features.
\paragraph{Stacked Formants}
The idea of stacked MFCCs to model the dynamics of the signal over time can also be applied to formants. The 2 formants calculated per frame are therefore stacked over 15 frames to form one 30-dimensional feature vector.
\paragraph{Pitch}
Pitch is a feature that measures the frequency of the vocal cord vibrations.
It is calculated using the PitchYinFFT algorithm \cite{paul2007a} of the Essentia framework, which is an optimized version of the Yin algorithm calculated in the frequency domain. The input is therefore windowed with a Hann window.
\paragraph{Stacked Pitch}
Corresponding to the approach with MFCCs and Formants, the calculated pitch values over 15 frames are stacked to form one feature vector.
\paragraph{Principal Component Analysis}
Principal Component Analysis (PCA) is applied to the feature vectors of feature sets with stacked features or derivatives, except for stacked formants, to reduce dimensionality and to transform the features into linearly uncorrelated variables that describe the largest possible variances in the data. The algorithm used is the vector\_normalizer\_pca from the dlib library \cite{dlib09}. When the PCA is performed, the feature vectors are normalized automatically and no additional normalization is necessary.
The PCA parameter $\epsilon$ controls the size of the transformed feature vector. A value $0 < \epsilon <= 1$ can be chosen and large values result in larger feature vectors. For our evaluation, we chose a value of 0.95 to maintain most of the information contained in the features.
\begin{table}[h]
	\resizebox{\columnwidth}{!}{
	\begin{tabular}{ l c }
		\hline
		Feature combination & Feature vector size \\
		\hline
		MFCCs & 13 \\
		MFCCs Delta Deltadelta & 39 \\
		Stacked MFCCs (15 frames) & 195 \\
		SD of formants (15 frames) & 2 \\
		Stacked formants (15 frames) & 30 \\
		Pitch & 1 \\
		Stacked pitch (15 frames) & 15\\
		\hline
	\end{tabular}
	}
	\caption{Tested feature combinations with corresponding feature vector sizes (without PCA)}
	\label{tab:feature-vectors}
\end{table}
\subsection{Model estimation}\label{sec:SVM-training}
A Support Vector Machine (SVM) with radial basis function (RBF) kernel is used for the classification of non-lexical confirmations vs. other speech.
Therefore, the svm\_c\_trainer algorithm of the dlib library is used in the implementation.
For the SVM training a feature vector of the selected features for each frame of a segment in the training set is calculated.
All feature vectors are collected and the features are normalized as part of the PCA transformation.
The normalized feature vectors are then used to train the SVM model.
\subsection{Classification}\label{sec:classification}
Classification can be performed in offline and online mode.
In offline mode, the classification results for each frame are stored and finally combined to calculate accuracy and true/false positive rates.
For online classification, each frame is classified and the classification results are added as $1$ for confirmations and $-1$ for other utterances to a rolling mean over 5 frames, which is used to implement a simple majority vote. If a specified majority threshold is reached the current VAD segment is classified as an utterance containing a non-lexical confirmation.
\section{Evaluation}
\begin{table*}[t]
	%\resizebox{\columnwidth}{!}{
	\centering
	\begin{tabular}{l*{6}{c}}
		\hline
		feature set & feature vector size & cross-validation result & TPR (\%) & FPR (\%) & ROC AUC \\
		\hline
		MFCCs & 13 & 76.5 - 81.4 & 82.7 & 20.2 & 0.87\\
		MFCCs Delta Deltadelta & 22 & 80.0 - 85.7 & 85.7 & 21.4 & 0.88 \\
		Stacked MFCCs & 58 & \textbf{91.0 - 96.8} & 82.8 & 10.0 & \textbf{0.92} \\
		Formants & 2 & 73.8 - 78.5 & 77.1 & 27.8 & 0.78\\
		Stacked Formants & 30 & 83.3 - 88.1 & 91.9 & 16.8 & \textbf{0.93} \\
		Pitch & 1 & 11.7 - 17.6 & 99.8 & 92.0 & 0.60\\
		Stacked Pitch & 11 & 37.9 - 43.8 & 99.1 & 65.1 & 0.71 \\
		\hline
	\end{tabular}
	%}
	\caption{Evaluation results: Cross-validation shows the stable performance of stacked MFCCs, but stacked formants achieve the highest ROC AUC for the test set}
	\label{tab:eval-results}
\end{table*}
\subsection{KOMPASS WOZ1 Data Preparation}\label{sec:data-preparation}
The KOMPASS WOZ1 data is prepared for SVM training and offline classification by extracting all VAD segments from the sound files. 
If the interval contains a manual annotation of a non-lexical confirmation, the segment is added as belonging to the non-lexical confirmation class. 
A script is used to split all segments in training and test sets. Users without non-lexical confirmation utterances are excluded.
We made sure to assign all utterances from one speaker to one of the sets only in order to achieve a speaker independent test set-up.
70 percent of the KOMPASS WOZ1 data are used for training, while the remaining 30 percent are used as test data (see Tab. \ref{tab:WOZ1-corpus}).
A leave-one-user-out cross-validation is performed on the training set. For each fold, all utterances of one user are used as the test set, while the remaining users are used for training. 
%The script for splitting ensures that approximately the same amount of non-lexical confirmations and other VAD segments are added to training and test set resulting in a subset of 280 VAD segments because of the small amount of non-lexical confirmations.
Because segments of non-lexical confirmations are usually shorter than other utterances, an additional step to maintain a uniform distribution of frames belonging to each class has to be performed. Feature vectors of other utterances are discarded prior to SVM training to compensate for the small number of frames belonging to non-lexical confirmations.
The test set contains a realistic subset of unevenly distributed utterances of both classes and all frames of these utterances are classified without balancing the uneven distribution of feature vectors of both classes.
\subsection{Parameter Optimization}\label{sec:parameter-optimization}
\begin{table}[h]
	\resizebox{\columnwidth}{!}{
	\begin{tabular}{l*{3}{c}}
		\hline
		Feature combination & $C$ & $\epsilon$ & $\gamma$ \\
		\hline
		MFCCs & 1 & 0.5 & 0.005 \\
		MFCCs Delta Deltadelta & 1 & 0.1 & 0.005 \\
		Stacked MFCCs (15 frames) & 1 & 0.5 & 0.005 \\
		SD of formants (15 frames) & 5 & 0.005 & 0.05 \\
		Stacked formants (15 frames) & 1 & 0.5 & 0.05 \\
		Pitch & 5 & 0.005 & 0.05 \\
		Stacked pitch (15 frames) & 5 & 0.5 & 0.05\\
		\hline
	\end{tabular}
	}
	\caption{Best SVM parameters found with grid search for each feature set}
	\label{tab:grid-search-results}
\end{table}
Grid search was used to optimize the SVM parameters $C$, $\epsilon$ and $\gamma$ for the RBF kernel. The parameters were tested in the ranges $C\in\{1, 5\}$, $\epsilon\in\{0.005, 0.05, 0.1, 0.5\}$ and $\gamma\in\{0.005, 0.05\}$. The best results for each feature set are shown in Tab. \ref{tab:grid-search-results}.
\subsection{Results on the KOMPASS WOZ1 Data}
The system for non-lexical confirmation detection was tested on the KOMPASS WOZ1 data. Seven different feature sets were evaluated: MFCCs, MFCCs + first and second derivative ($\Delta$, $\Delta\Delta$), stacked MFCCs, formants, stacked formants, pitch and stacked pitch. Grid search was performed as described in Sec. \ref{sec:parameter-optimization} for parameter optimization.
Before the SVM was trained, a leave-one-user-out cross-validation was performed (see Sec. \ref{sec:data-preparation}). To evaluate the performance of the trained models, the sum of the accuracy value weighted with the number of non-lexical confirmations for each fold was calculated. 

Fig. \ref{fig:roc_curve} shows the Receiver Operating Characteristic (ROC) curves of the seven classifiers with different feature sets, that were evaluated on different test sets. For the first test set, the stacked formants can outperform all other feature sets with an area under the curve (AUC) of 0.93. 
In comparison, the standard deviation of the formants achieve an AUC of 0.78, which is even below all of the MFCC related feature sets. 
The feature vectors consisting of 13 MFCCs result in classification results with an AUC of 0.87 
and adding first and second derivative only slightly improves the result (AUC of 0.88).
Stacking of the MFCCs raises the AUC to 0.92
, but stays below the value of stacked formants.
Using pitch as a single feature results in a nearly diagonal ROC curve (AUC of 0.60
), which corresponds to classification results near chance level. Stacking the pitch values to feature vectors over 15 frames only slightly improves the results (AUC of 0.71
).
The results with the second test set show, that the performance of the formant related feature sets is not stable, while the results of MFCC related and pitch related feature sets remain similar.

The online classification was evaluated with the two best feature sets, stacked formants and stacked MFCCs, which achieve accuracy values of 84\% and 79\%, respectively.
\section{Discussion}
In this paper, we described a system for non-lexical confirmation detection in speech. 
Our system is capable of both online and offline processing of speech data. Thus, it can easily be integrated into systems interacting with humans. 
We relied on Support Vector Machines with a RBF kernel for classification. A sliding window approach enables the system to spot filled-pauses within utterances without the necessity to explicitly model parts of speech not relevant for filled-pause detection. 
The system's performance was evaluated on seven different feature sets: MFCCs, MFCCs with first and second derivative ($\Delta$, $\Delta\Delta$), stacked MFCCs, formants, stacked formants, pitch and stacked pitch. 
The results show that successfully detecting non-lexical confirmations requires several frames of context. For this the stacking of the features improves the results and outperforms feature sets with derivatives. 
The results with stacked MFCCs, and MFCC related feature sets in general, are more stable within several performed test runs. But stacked formants have the potential to achieve higher classification results depending on the data. The amount of available data for SVM training might also influence the performance of the stacked feature sets and has to be evaluated.

Our approach can be applied to spot other acoustic events in speech data. In further studies, we aim to apply stacked features for the detection of other non-lexical speech events such as filled-pauses and for detecting socio-emotional signals such as uncertainty. Virtual agents like ''BILLIE`` will become more and more natural interaction partners by integrating those cues.
\section*{Acknowledgments}
The authors gratefully acknowledge the German Federal Ministry of Education and Research (BMBF) for providing funding to project KOMPASS (FKZ 16SV7271K),
within the framework of which our research was able to take place. 
This work was supported by the Cluster of Excellence Cognitive Interaction Technology ``CITEC'' (EXC 277) at Bielefeld University, which is funded by the German Research Foundation (DFG). 
Furthermore, the authors would like to thank our student worker Kirsten K\"astel for data annotation.
\bibliographystyle{aaai}
\bibliography{library}
\end{document}

% \pubnote{\em To appear, AAAI-10} % optional, remove for submission
%
% \pubnote is for printing the paper yourself, and should not be used in
% submitted versions!!!!
% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
% Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
% \author{Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% Address line \\ ... \\ Address line}
% For authors from different institutions:
% \author{Author 1 \\ Address line \\ ... \\ Address line
% \And ... \And
% Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\ ... \\ Address line
% \AND
% Author 2 \\ Address line \\ ... \\ Address line \And
% Author 3 \\ Address line \\ ... \\ Address line}
% If the title and author information does not fit in the area allocated,
% place \setlength\titlebox{height}
% after the \documentstyle line
% where {height} is something like 2.5in

