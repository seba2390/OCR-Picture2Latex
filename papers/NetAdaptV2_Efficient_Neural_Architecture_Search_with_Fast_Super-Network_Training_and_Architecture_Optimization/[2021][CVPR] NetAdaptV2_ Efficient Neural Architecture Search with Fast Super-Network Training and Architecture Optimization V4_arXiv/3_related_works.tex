\section{Related Works}
\label{sec:related_works}

Reinforcement-learning-based methods~\cite{zoph2017nasreinforcement, zoph2018nasnet, Tan2018MnasNetPN, Chen2020MnasFPNLL, Tan2019EfficientNet} demonstrate the ability of neural architecture search for designing high-performance DNNs. However, its search time is longer than the following works due to the long time for training samples individually. Gradient-based methods~\cite{gordon2018morphnet, liu2018darts, wu2018fbnet, cai2018proxylessnas, stamoulis2019singlepath, stamoulis2019singlepathautoml, Mei2020AtomNAS, Xu2020PC-DARTS} successfully discover high-performance DNNs with a much shorter search time, but they can only support differentiable search metrics. NetAdaptV1~\cite{eccv2018-netadapt} proposes a single-layer coordinate descent optimizer that can support both differentiable and non-differentiable search metrics and was used to design state-of-the-art MobileNetV3~\cite{Howard_2019_ICCV}. However, shrinking only one layer for generating each sample and the long time for training samples individually become its bottleneck of search time. The idea of super-network training~\cite{autoslim_arxiv, cai2020once, yu2020bignas}, which jointly trains all the sub-networks in the search space, is proposed to reduce the time for training and evaluating samples and training the discovered DNN at the cost of a significant increase in the time for training a super-network. Moreover, network depth and layer width are usually considered separately in related works. The proposed NetAdaptV2 addresses all these problems at the same time by reducing the time for training a super-network, training and evaluating samples, and training the discovered DNN in balance while supporting non-differentiable search metrics.

The algorithm flow of NetAdaptV2 is most similar to NetAdaptV1~\cite{eccv2018-netadapt}, as shown in Fig.~\ref{fig:overview}. Compared with NetAdaptV2, NetAdaptV1 does not train a super-network but train each sample individually. Moreover, NetAdaptV1 considers only one layer per optimization iteration and different layer widths, but NetAdaptV2 considers multiple layers per optimization iteration and different layer widths, network depths, and kernel sizes. Therefore, NetAdaptV2 is both faster and more effective than NetAdaptV1, as shown in Sec.~\ref{subsubsec:ablation_study} and~\ref{subsec:depth_estimation}.

For the methodology, the proposed ordered dropout is most similar to the partial channel connections~\cite{Xu2020PC-DARTS}. However, they are different in the purpose and the ability to expand the search space. Partial channel connections aim to reduce memory consumption while training a DNN with multiple parallel paths by removing some channels. The number of channels removed is constant during training. Moreover, this number is manually chosen. As a result, partial channel connections do not expand the search space. In contrast, the proposed ordered dropout is designed for jointly training multiple sub-networks and expanding the search space. The number of channels removed (i.e., zeroed out) varies from image to image and from one training iteration to another during training to simulate different sub-networks. Moreover, the final number of channels removed (i.e., the discovered architecture) is searched. Therefore, the proposed ordered dropout expands the search space in terms of layer width as well as network depth when the proposed channel-level bypass connections are used.