\section{Experiment Results}

We apply NetAdaptV2 on two applications (image classification and depth estimation) and two search metrics (latency and multiply-accumulate operations (MACs)) to demonstrate the effectiveness and versatility of NetAdaptV2 across different operating conditions. We also perform an ablation study to show the impact of each of the proposed techniques and the associated hyper-parameters.

\subsection{Image Classification}
\label{sec:image_classification}

\subsubsection{Experiment Setup}
\label{subsec:experiment_setup}

We use latency or MACs to guide NetAdaptV2. The latency is measured on a Google Pixel 1 CPU. The search time is reported in GPU-hours and measured on V100 GPUs.

The dataset is ImageNet~\cite{ijcv2015-russakovsky-ilsvrc}. We reserve 10K images in the training set for comparing the accuracy of samples and train the super-network with the rest of the training images. The accuracy of the discovered DNN is reported on the validation set, which was not seen during the search. The initial network is based on MobileNetV3~\cite{Howard_2019_ICCV}. The maximum learning rate is 0.064 decayed by 0.963 every 3 epochs when the batch size is 1024. The learning rate scales linearly with the batch size~\cite{Goyal2017AccurateLM}. The optimizer is RMSProp with an $\ell$2 weight decay of $10^{-5}$. The dropout rate is 0.2. The decay rate of the exponential moving average is 0.9998. The batch size is 1024 for training the super-network, 2048 for training the latency-guided discovered DNN, and 1536 for training the MAC-guided discovered DNN.

The multi-layer coordinate descent (MCD) optimizer generates 200 samples per iteration ($J=200$). For the latency-guided experiment, each sample is obtained by randomly shrinking 10 layers ($L=10$) from the best sample in the previous iteration. We reduce the latency by 3\% in the first iteration (i.e., initial resource reduction) and decay the resource reduction by 0.98 every iteration. For the MAC-guided experiment, each sample is obtained by randomly shrinking 15 layers ($L=15$) from the best sample in the previous iteration. We reduce the MACs by 2.5\% in the first iteration and decay the resource reduction by 0.98 every iteration. More details are included in the appendix.


\subsubsection{Latency-Guided Search Result}
\label{subsec:latency_guided_search_result}

The results of NetAdaptV2 guided by latency and related works are summarized in Table~\ref{tab:nas_result}. Compared with the state-of-the-art (SOTA) NAS algorithms~\cite{cai2020once, yu2020bignas}, NetAdaptV2 reduces the search time by up to 5.8$\times$ and discovers DNNs with better accuracy-latency/accuracy-MAC trade-offs. The reduced search time is the result of the much more balanced time spent per step. Compared with the NAS algorithms in the class of hundreds of GPU-hours, ProxylessNAS~\cite{cai2018proxylessnas} and Single-Path NAS~\cite{stamoulis2019singlepathautoml}, NetAdaptV2 outperforms them without sacrificing the support of non-differentiable search metrics. NetAdaptV2 achieves either 2.4\% higher accuracy with $1.5\times$ lower latency or 1.4\% higher accuracy with $1.6\times$ lower latency. Compared with SOTA NAS-discovered MobileNetV3~\cite{Howard_2019_ICCV}, NetAdaptV2 achieves 1.8\% higher accuracy with the same latency in around 50 hours on eight GPUs. We estimate the $CO_2$ emission of NetAdaptV2 based on~\cite{strubell_2019_energy}. NetAdaptV2 discovers DNNs with better accuracy-latency/accuracy-MAC trade-offs with low $CO_2$ emission.

\begin{table*}[ht]
\centering
\scalebox{0.91}{
\begin{threeparttable}
\begin{tabular}{l|c|c|c|c|c|c}
\toprule
\multicolumn{1}{c|}{\multirow{2}{*}{Method}} & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Top-1 \\ Accuracy (\%)\end{tabular}} & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Latency \\ (ms)\end{tabular}} & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}MAC\\ (M)\end{tabular}} & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Search Time\\ (GPU-Hours)\end{tabular}} & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Non-Diff. \\ Metrics \end{tabular}} & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}$CO_2$ \\ Emission (lbs) \end{tabular}}
\\
\multicolumn{1}{c|}{}                        &                                                                                 &                               &                                                                    &      &                                                                              \\ \toprule
MnasNet~\cite{Tan2018MnasNetPN}                                       & 75.2                                                                        & 78            & 312        &                           -                              & \checkmark & -                                                                                \\ \hline
ProxylessNAS~\cite{cai2018proxylessnas}                                  & 74.6                                                                            & 78                             & 320                                                                & 500& & 142                                                                                \\ \hline
Single-Path NAS~\cite{stamoulis2019singlepathautoml}                                   & 75.6                                                                           & 82                            & -                                                                & \begin{tabular}[c]{@{}c@{}}243\tnote{*} \\ (24 (TPU V3), 0, 219)\end{tabular} &                 & 69                                                             \\ 
\hline
AutoSlim~\cite{autoslim_arxiv}                                      & 74.6                                                                            & 71                            & 315                                                                & -  & \checkmark & -                                                                               \\ \hline
FBNet~\cite{wu2018fbnet}                                         & 74.9                                                                            & -                             & 375                                                                & -     &        & -                                                                     \\ \hline
MobileNetV3~\cite{Howard_2019_ICCV}                                   & 75.2                                                                            & 51                            & 219                                                                & - & \checkmark       & -                                                                           \\ \hline

FairNAS~\cite{chu2019fairnas} & 76.7 & 77 & 325 & - & \checkmark & - \\ \hline


Once-for-All~\cite{cai2020once}                                   & 76.9                                                                           & 58                            & 230                                                                & \begin{tabular}[c]{@{}c@{}}1315 \\ (1200, 40, 75)\end{tabular} & \checkmark                & 374                                                             \\ \hline

BigNAS~\cite{yu2020bignas}                                   & 76.5                                                                           & -                            & 242                                                                & \begin{tabular}[c]{@{}c@{}}2304 (TPU V3) \\ (2304, -, 0)\end{tabular} & \checkmark                & 655                                                             \\ 
\hline                                    
\hline

\textbf{\begin{tabular}[l]{@{}l@{}}NetAdaptV2 \\ (Guided by Latency)\end{tabular}}                                   & \textbf{77.0}                                                                           & \textbf{51}                            & \textbf{225}                                                                & \textbf{\begin{tabular}[c]{@{}c@{}} 397 \\ (167, 24, 206)\end{tabular}} & \checkmark                & \textbf{113}                                                             \\ \toprule
\end{tabular}
\begin{tablenotes}\small
\item[*] 1) We merge the time for training the super-network and that for training and evaluating samples into one. 2) We train the discovered network for 350 epochs as mentioned in~\cite{stamoulis2019singlepathautoml}.
\end{tablenotes}
\end{threeparttable}
}
\caption{The comparison between NetAdaptV2 \emph{guided by latency} and related works on ImageNet. The number of MACs is reported for completeness although NetAdaptV2 is not guided by MACs and achieves sub-optimal accuracy-MAC trade-offs. The numbers between parentheses show the breakdown of the search time in terms of training a super-network, training and evaluating samples, and training the discovered DNN from left to right. \textit{Non-Diff. Metrics} denotes whether the method supports non-differentiable metrics. The last column \textit{$CO_2$ Emission} shows the estimated $CO_2$ emission based on~\cite{strubell_2019_energy}.}
\label{tab:nas_result}
\end{table*}

\subsubsection{MAC-Guided Search Result}
\label{subsec:mac_guided_search_result}

We present the result of NetAdaptV2 guided by MACs and compare it with related works in Table~\ref{tab:nas_result_large}. For a fair comparison, AutoAugment~\cite{autoaugment} and stochastic depth~\cite{stochastic_depth} with a survival probability of 0.8 are used for training the discovered network, which results in a longer time for training the discovered DNN. NetAdaptV2 achieves comparable accuracy-MAC trade-offs to NSGANetV2-m~\cite{lu2020nsganetv2} while the search time is $2.6 \times$ lower. Moreover, the discovered DNN also outperforms EfficientNet-B0~\cite{Tan2019EfficientNet} and MixNet-M~\cite{Tan2019MixConvMD} by up to $1.5\%$ higher top-1 accuracy with fewer MACs.

\begin{table}[t]
\scalebox{0.85}{
\begin{tabular}{l|c|c|c}
\toprule
\multicolumn{1}{c|}{Method} & \begin{tabular}[c]{@{}c@{}}Top-1\\ Accuracy (\%)\end{tabular} & \begin{tabular}[c]{@{}c@{}}MAC\\ (M)\end{tabular} & \begin{tabular}[c]{@{}c@{}}Search Time\\ (GPU-Hours)\end{tabular} \\ 
\toprule
NSGANetV2-m~\cite{lu2020nsganetv2}                  & 78.3                                                          & 312                                               & \begin{tabular}[c]{@{}c@{}}1674\\ (1200, 24, 450)\end{tabular}    \\ \hline
EfficientNet-B0~\cite{Tan2019EfficientNet}              & 77.3                                                          & 390                                               & -                                                                 \\ \hline
MixNet-M~\cite{Tan2019MixConvMD}                     & 77.0                                                          & 360                                               & -                                                                 \\ \hline \hline
\textbf{\begin{tabular}[c]{@{}l@{}}NetAdaptV2\\ (Guided by MAC)\end{tabular}} & \textbf{78.5}                                                 & \textbf{314}                                      & \textbf{\begin{tabular}[c]{@{}c@{}} 656 \\ (204, 35, 417)\end{tabular}}  \\ 
\toprule
\end{tabular}
}
\caption{The comparison between NetAdaptV2 \emph{guided by MACs} and related works. The numbers between parentheses show the breakdown of the search time in terms of training a super-network, training and evaluating samples, and training the discovered DNN from left to right.}
\label{tab:nas_result_large}
\end{table}

\subsubsection{Ablation Study}
\label{subsubsec:ablation_study}

The ablation study employs the experiment setup outlined in Sec.~\ref{subsec:experiment_setup} unless otherwise stated. To speed up training the discovered networks, the distillation model is smaller.

\noindent \textbullet\ \textbf{Impact of Ordered Dropout}

To study the impact of the proposed ordered dropout (OD), we do not use channel-level bypass connections (CBCs) and multi-layer coordinate descent (MCD) optimizer in this experiment. When we further remove the usage of OD, NetAdaptV2 becomes the same as NetAdaptV1~\cite{eccv2018-netadapt}, where each sample needs to be trained for four epochs by following the setting of NetAdaptV1. To speed up the execution of NetAdaptV1, we use a shallower network, MobileNetV1~\cite{Howard2017MobileNetV1}, in this experiment instead. Table~\ref{tab:ablation_ordered_dropout} shows that using OD reduces the search time by $3.3\times$ while achieving the same accuracy-latency trade-off. If we only consider the time for training a super-network and training and evaluating samples, which are affected by OD, the time reduction is $10.4\times$.


\begin{table}[t]
\centering
\scalebox{0.9}{
\begin{tabular}{c||c|c|c}
\toprule
OD & \begin{tabular}[c]{@{}c@{}}Top-1\\ Accuracy (\%)\end{tabular} & \begin{tabular}[c]{@{}c@{}}Latency\\ (ms)\end{tabular} & \begin{tabular}[c]{@{}c@{}}Search Time\\ (GPU-Hours)\end{tabular} \\ \toprule
& 71.0 (+0)                                                     & 43.9 (100\%)                                           & \begin{tabular}[c]{@{}c@{}}721 (100\%) \\ (0, 543, 178)\end{tabular}                                                      \\ \hline
\checkmark  & 71.1 (+0.1)                                                   & 44.4 (101\%)                                           & \begin{tabular}[c]{@{}c@{}}221 (31\%) \\ (50, 2, 169)\end{tabular}                                                         \\ \toprule
\end{tabular}
}
\caption{The ablation study of the proposed ordered dropout (OD) on MobileNetV1~\cite{Howard2017MobileNetV1} and ImageNet. The numbers between parentheses show the breakdown of the search time in terms of training a super-network, training and evaluating samples, and training the discovered DNN from left to right.}
\label{tab:ablation_ordered_dropout}
\end{table}

\noindent \textbullet\ \textbf{Impact of Channel-Level Bypass Connections}

The proposed channel-level bypass connections (CBCs) enable NetAdaptV2 to search for different network depths. Table~\ref{tab:ablation_cbc_optimizer} shows that CBCs can improve the accuracy by 0.3\%. The difference is more significant when we target at lower latency, as shown in the ablation study on MobileNetV1 in the appendix, because the ability to remove layers becomes more critical for maintaining accuracy.

\begin{table}[t]
\centering
\scalebox{0.9}{
\begin{tabular}{c|c||c}
\toprule
\multicolumn{2}{c||}{Methods} & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Top-1\\ Accuracy (\%)\end{tabular}} \\ \cline{1-2}
CBC           & MCD           &                                                                                \\ \hline
              &               & 75.9 (+0)                                                                           \\ \hline
\checkmark             &               & 76.2 (+0.3)                                                                           \\ \hline
\checkmark             & \checkmark             & 76.6 (+0.7)                                                                           \\ \toprule
\end{tabular}
}
\caption{The ablation study of the channel-level bypass connections (CBCs) and the multi-layer coordinate descent optimizer (MCD) on ImageNet. The latency of the discovered networks is around 51ms, and ordered dropout is used.}
\label{tab:ablation_cbc_optimizer}
\end{table}

\noindent \textbullet\ \textbf{Impact of Multi-Layer Coordinate Descent Optimizer}

The proposed multi-layer coordinate descent (MCD) optimizer improves the performance of the discovered DNN by considering the joint effect of multiple layers per optimization iteration. Table~\ref{tab:ablation_cbc_optimizer} shows that using the MCD optimizer further improves the accuracy by 0.4\%.


\noindent \textbullet\ \textbf{Impact of Resource Reduction and Number of Samples}

The two main hyper-parameters of the MCD optimizer are the per-iteration resource reduction, which is defined by an initial resource reduction and a decay rate, and the number of samples per iteration ($J$). They influence the accuracy of the discovered networks and the search time. Table~\ref{tab:influence_latency_reduction} summarizes the accuracy of the 51ms discovered networks when using different initial latency reductions (with a fixed decay of 0.98 per iteration) and different numbers of samples.

The first experiment is fixing the number of samples per iteration and increasing the initial latency reduction from 1.5\% to 6.0\%, which gradually reduces the time for evaluating samples. The result shows that as long as the latency reduction is small enough, specifically below 3\% in this experiment, the accuracy of the discovered networks does not change with the latency reduction.

The second experiment is fixing the time for evaluating samples by scaling both the initial latency reduction and the number of samples per iteration at the same rate. As shown in Table~\ref{tab:influence_latency_reduction}, as long as the latency reduction is small enough, more samples will result in better discovered networks. However, if the initial latency reduction is too large, increasing the number of samples per iteration cannot prevent the accuracy from degrading.

\begin{table}[t]
\centering
\scalebox{0.8}{
\begin{tabular}{c||c|c|c}
\toprule
                                                                                             \multicolumn{1}{c||}{} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Initial Latency\\ Reduction\end{tabular}} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Number of \\ Samples ($J$)\end{tabular}} & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}Top-1\\ Accuracy (\%)\end{tabular}} \\ \toprule
\multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}Fixed Number of\\ Samples per Iteration\end{tabular}}  & 1.5\%                                                               & 100                                                                      & 76.4   \\ \cline{2-4} 
                                                                                             & 3.0\%                                                               & 100                                                                      & 76.4   \\ \cline{2-4} 
                                                                                             & 6.0\%                                                               & 100                                                                      & 75.9   \\ \toprule
\multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}Fixed Time for\\ Evaluating Samples\end{tabular}} & 1.5\%                                                               & 100                                                                      & 76.4   \\ \cline{2-4} 
                                                                                             & 3.0\%                                                               & 200                                                                      & 76.6   \\ \cline{2-4} 
                                                                                             & 6.0\%                                                               & 400                                                                      & 75.7   \\ \toprule
\end{tabular}
}
\caption{The experiments for evaluating the influence of the two main hyper-parameters of the MCD optimizer, which are the initial latency reduction (with a fixed decay of 0.98 per iteration) and the number of samples ($J$). All discovered networks have almost the same latency (51ms).}
\label{tab:influence_latency_reduction}
\end{table}

\noindent \textbullet\ \textbf{Accuracy Variation across Multiple Executions}

To know the accuracy variation of each step in NetAdaptV2~\cite{li2020reproducibility}, we execute different steps three times and summarize the resultant accuracy of the discovered networks in Table~\ref{tab:reproducibility}. The initial latency reduction is 1.5\%, and the number of samples per iteration is 100 ($J=100$). The latency of discovered networks is around 51ms. According to the last row of Table~\ref{tab:reproducibility}, which corresponds to executing the entire algorithm flow of NetAdaptV2 three times, the accuracy variation is 0.3\%. The variation is fairly small because simply training the same discovered network three times results in an accuracy variation of 0.1\% as shown in the first row. Moreover, when we fix the super-network and execute the MCD optimizer three times as shown in the second row, the accuracy variation is the same as that of executing the entire NetAdaptV2 three times. The result suggests that the randomness in training a super-network does not increase the overall accuracy variation, which is preferable since we only need to perform this relatively costly step one time.

\begin{table}[t]
\centering
\scalebox{0.8}{
\begin{tabular}{c|c|c||c|c|c}
\toprule
\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Training\\ Super-Network\end{tabular}} & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Evaluating\\ Samples\end{tabular}} & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Training\\ Discovered DNN\end{tabular}} & \multicolumn{3}{c}{\begin{tabular}[c]{@{}c@{}}Top-1 Accuracy of\\ Executions (\%)\end{tabular}} \\ \cline{4-6}  
                                                                                     &                                                                               &                                                                                    & 1       & 2       & 3      \\ \toprule
                                                                                     &                                                                              & \checkmark                                                                                  & 76.1    & 76.1    & 76.2   \\  \hline
                                                                                     & \checkmark                                                                             & \checkmark                                                                                  & 76.1    & 76.2    & 76.4   \\  \hline
\checkmark                                                                                    & \checkmark                                                                             & \checkmark                                                                                  & 76.1    & 76.2    & 76.4   \\ \toprule
\end{tabular}
}
\caption{The accuracy variation of NetAdaptV2. The $\checkmark$ denotes the step is executed three times, and the others are executed once. For example, the last row corresponds to executing the entire algorithm flow of NetAdaptV2 three times. For the MCD optimizer, the initial latency reduction is 1.5\%, and the number of samples per iteration is 100 ($J=100$). The latency of all discovered networks is around 51ms, and the accuracy values are sorted in ascending order.}
\label{tab:reproducibility}
\end{table}

\subsection{Depth Estimation}
\label{subsec:depth_estimation}
\subsubsection{Experiment Setup}
\label{subsubsec:depth_estimation_experiment_setup}

NYU Depth V2~\cite{nyudepth} is used for depth estimation. We reserve 2K training images for evaluating the performance of samples and train the super-network with the rest of the training images. The initial network is FastDepth~\cite{icra_2019_fastdepth}.
Following FastDepth, we pre-train the encoder of the super-network on ImageNet.
The batch size is 256, and the learning rate is 0.9 decayed by 0.963 every epoch. After pre-training the encoder,  we train the super-network on NYU Depth V2 for 50 epochs with a batch size of 16 and an initial learning rate of 0.025 decayed by 0.9 every epoch. For the MCD optimizer, we generate 150 ($J=150$) samples per iteration. We search with latency measured on a Google Pixel 1 CPU. The latency reduction is 1.5\% in the first iteration and is decayed by 0.98 every iteration. For training the discovered network, we use the same setup as training the super-network, except that the initial learning rate is 0.05. 

\subsubsection{Search Result}

The comparison between the proposed NetAdaptV2 and NetAdaptV1~\cite{eccv2018-netadapt}, which is used in FastDepth~\cite{icra_2019_fastdepth}, is summarized in Table~\ref{tab:fastdepth}. NetAdaptV2 reduces the search time by $2.4\times$ on NYU Depth V2, and the discovered DNN outperforms that of NetAdaptV1 by 0.5\% in delta-1 accuracy with comparable latency. Because NYU Depth V2 is much smaller than ImageNet, the reduction in the total search time is less than that of applying NetAdaptV2 on ImageNet. The search time spent on ImageNet is for pre-training the encoder, which is a common practice and indispensable when training DNNs for depth estimation on NYU Depth V2.

\begin{table}[t]
\centering
\scalebox{0.8}{
\begin{tabular}{l|c|c|c|c|c}
\toprule
\multicolumn{1}{c|}{\multirow{2}{*}{Method}} & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}RMSE \\ (m)\end{tabular}} & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Delta-1\\ Accuracy\\ (\%)\end{tabular}} & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Latency \\ (ms)\end{tabular}} & \multicolumn{2}{c}{\begin{tabular}[c]{@{}c@{}}Search Time\\ (GPU-Hours)\end{tabular}} \\ \cline{5-6} 
\multicolumn{1}{c|}{}                        &                           &                                                                                   &                               & ImageNet                                 & \begin{tabular}[c]{@{}c@{}}NYU \\ Depth\end{tabular}                                   \\ \toprule
NetAdaptV1~\cite{eccv2018-netadapt}                                      & 0.583                     & 77.4                                                                              & 87.6                          & 96                                       & 65                                          \\ \hline
\textbf{NetAdaptV2}                            & \textbf{0.576}            & \textbf{77.9}                                                                     & \textbf{86.7}                 & 96                                       & \textbf{27}                                 \\ \toprule
\end{tabular}
}
\caption{The comparison between NetAdaptV2 and NetAdaptV1 on depth estimation and NYU Depth V2~\cite{nyudepth}.}
\label{tab:fastdepth}
\end{table}