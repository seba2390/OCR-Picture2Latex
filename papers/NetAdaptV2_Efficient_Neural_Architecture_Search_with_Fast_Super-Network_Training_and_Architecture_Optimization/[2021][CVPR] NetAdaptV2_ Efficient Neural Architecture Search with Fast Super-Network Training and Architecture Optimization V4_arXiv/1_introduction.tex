\begin{figure}[t]
\begin{center}
   \includegraphics[width=1.0\linewidth]{figures/nas_comp_v3}
\end{center}
   \vspace{-4mm}
   \caption{The comparison between NetAdaptV2 and related works. The number above a marker is the corresponding total search time measured on NVIDIA V100 GPUs.}
\label{fig:nas_comparison}
\end{figure}

\section{Introduction}
\label{sec:introduction}

Neural architecture search (NAS) applies machine learning to automatically discover deep neural networks (DNNs) with better performance (e.g., better accuracy-latency trade-offs) by sampling the search space, which is the union of all discoverable DNNs. The search time is one key metric for NAS algorithms, which accounts for three steps: 1) training a \emph{super-network}, whose weights are shared by all the DNNs in the search space and trained by minimizing the loss across them, 2) training and evaluating sampled DNNs (referred to as \emph{samples}), and 3) training the discovered DNN. Another important metric for NAS is whether it supports non-differentiable search metrics such as hardware metrics (e.g., latency and energy). Incorporating hardware metrics into NAS is the key to improving the performance of the discovered DNNs~\cite{eccv2018-netadapt, Tan2018MnasNetPN, cai2018proxylessnas, Chen2020MnasFPNLL, chamnet}.


There is usually a trade-off between the time spent for the three steps and the support of non-differentiable search metrics. For example, early reinforcement-learning-based NAS methods~\cite{zoph2017nasreinforcement, zoph2018nasnet, Tan2018MnasNetPN} suffer from the long time for training and evaluating samples. Using a super-network~\cite{yu2018slimmable, Yu_2019_ICCV, autoslim_arxiv, cai2020once, yu2020bignas, Bender2018UnderstandingAS, enas, tunas, Guo2020SPOS} solves this problem, but super-network training is typically time-consuming and becomes the new time bottleneck. The gradient-based methods~\cite{gordon2018morphnet, liu2018darts, wu2018fbnet, fbnetv2, cai2018proxylessnas, stamoulis2019singlepath, stamoulis2019singlepathautoml, Mei2020AtomNAS, Xu2020PC-DARTS} reduce the time for training a super-network and training and evaluating samples at the cost of sacrificing the support of non-differentiable search metrics. In summary, many existing works either have an unbalanced reduction in the time spent per step (i.e., optimizing some steps at the cost of a significant increase in the time for other steps), which still leads to a long \emph{total} search time, or are unable to support non-differentiable search metrics, which limits the performance of the discovered DNNs.

In this paper, we propose an efficient NAS algorithm, NetAdaptV2, to significantly reduce the \emph{total} search time by introducing three innovations to \emph{better balance} the reduction in the time spent per step while supporting non-differentiable search metrics:

\textbf{Channel-level bypass connections (mainly reduce the time for training and evaluating samples, Sec.~\ref{subsec:channel_level_bypass_connections})}: Early NAS works only search for DNNs with different numbers of filters (referred to as \emph{layer widths}). To improve the performance of the discovered DNN, more recent works search for DNNs with different numbers of layers (referred to as \emph{network depths}) in addition to different layer widths at the cost of training and evaluating more samples because network depths and layer widths are usually considered independently. In NetAdaptV2, we propose \emph{channel-level bypass connections} to merge network depth and layer width into a single search dimension, which requires only searching for layer width and hence reduces the number of samples.

\textbf{Ordered dropout (mainly reduces the time for training a super-network, Sec.~\ref{subsec:ordered_droput})}: We adopt the idea of super-network to reduce the time for training and evaluating samples. In previous works, \emph{each} DNN in the search space requires one forward-backward pass to train. As a result, training multiple DNNs in the search space requires multiple forward-backward passes, which results in a long training time. To address the problem, we propose \emph{ordered dropout} to jointly train multiple DNNs in a \emph{single} forward-backward pass, which decreases the required number of forward-backward passes for a given number of DNNs and hence the time for training a super-network.

\textbf{Multi-layer coordinate descent optimizer (mainly reduces the time for training and evaluating samples and supports non-differentiable search metrics, Sec.~\ref{subsec:optimizer}):} NetAdaptV1~\cite{eccv2018-netadapt} and MobileNetV3~\cite{Howard_2019_ICCV}, which utilizes NetAdaptV1, have demonstrated the effectiveness of the single-layer coordinate descent (SCD) optimizer~\cite{book2020sze} in discovering high-performance DNN architectures. The SCD optimizer supports both differentiable and non-differentiable search metrics and has only a few interpretable hyper-parameters that need to be tuned, such as the per-iteration resource reduction. However, there are two shortcomings of the SCD optimizer. First, it only considers one layer per optimization iteration. Failing to consider the joint effect of multiple layers may lead to a worse decision and hence sub-optimal performance. Second, the per-iteration resource reduction (e.g., latency reduction) is limited by the layer with the smallest resource consumption (e.g., latency). It may take a large number of iterations to search for a very deep network because the per-iteration resource reduction is relatively small compared with the network resource consumption. To address these shortcomings,  we propose the \emph{multi-layer coordinate descent (MCD) optimizer} that considers multiple layers per optimization iteration to improve performance while reducing search time and preserving the support of non-differentiable search metrics.

Fig.~\ref{fig:nas_comparison} (and Table~\ref{tab:nas_result}) compares NetAdaptV2 with related works. NetAdaptV2 can reduce the search time by up to $5.8\times$ and $2.4\times$ on ImageNet~\cite{imagenet_cvpr09} and NYU Depth V2~\cite{nyudepth} respectively and discover DNNs with better performance than state-of-the-art NAS works. Moreover, compared to NAS-discovered MobileNetV3~\cite{Howard_2019_ICCV}, the discovered DNN has $1.8\%$ higher accuracy with the same latency.

