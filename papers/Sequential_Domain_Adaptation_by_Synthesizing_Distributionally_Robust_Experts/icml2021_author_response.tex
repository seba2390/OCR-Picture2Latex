\documentclass{article}

% Please use the following line and do not change the style file.
\usepackage{icml2021_author_response}
\usepackage{multirow}
\usepackage{amsmath}
% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{hyperref}       % hyperlinks
\usepackage{booktabs} % for professional tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{graphicx}
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\usepackage{lipsum}
\usepackage{dsfont}
\usepackage{color}
\usepackage{enumitem}
\usepackage{wrapfig}
\renewcommand\thesection{\Alph{section}}
\renewcommand\thetable{\thesection\Alph{table}}

\newcommand{\viet}[1]{{\color{red}{#1}}}
\usepackage[font=small,labelfont=bf]{caption}
\renewcommand{\thefigure}{\Alph{figure}}

\begin{document}
% Uncomment the following line if you prefer a single-column format
\onecolumn

We thank all referees for their efforts and constructive feedback. Below is our detailed reply to the reviewers' comments.
{\bf Application:} A narrative tailored for the Uber\&Lyft dataset can be recounted as follows: 
``Understanding the dynamics of ride-sharing platforms requires insights about the demand and supply from both sides of the market. These insights are signalled through the ride fares, which can be explained by characteristics such as the travel distances and the origin-destination pairs of the trips, the time of the day as well as the weather conditions. The capability to correctly predict ride fares directly translates into improved profit forecasts, and thus it vitally supports the growth of new-coming platforms. In a competitive market, a follower (e.g., Lyft) needs to target a slightly different market segment (either demographically or geographically) than the leader (e.g., Uber) who had entered earlier. Thus, the demand and supply characteristics for the follower may differ from those of the leader. Nevertheless, as both platforms provide on-demand transportation, it is reasonable to assume that their supply and demand dynamics are similar. The follower, who possesses limited data, can query demand on the leader's platform to collect data in order to leap forward in its predictive precision.''
\vspace{-.1cm}

\textbf{Additional Results:} We extend our experiments on the UCI UJIIndoorLoc WiFi Fingerprinting dataset that has $d=313$ predictive features, here we use the same parameter setting as described in the main paper. Table~\ref{table:rebuttal} shows the averaged cumulative loss of each aggregated expert obtained by the BOA algorithm. Due to limited time we do not yet have results for the SI methods. However, we expect them to display a comparable performance as the IR methods. In each row, the minimum loss is normalized to 1, and the remaining entries are represented by a multiplicative factor relative to the minimum value. At $d \!=\! 313$, generating one expert takes~25.78 seconds for IR-KL and only~0.32 second for IR-WASS.  
\vspace{-.1cm}

{\bf Reviewer~1:} Please refer to the Application above for a real-life example. Regarding your questions:
1) One can train a neural network to learn a representation of the input, then apply our methods with $X$ being the output of the last layer of that deep neural network.
% following is just an extra information that we do not really put here. On the other hand, we can also kernelize the feature space $\mathcal X$. Unfortunately, unlike kernel based SVM methods for example, we would require the knowledge of the feature map to obtain the mean and the covariance information. This would imply that we can only kernelize with finite dimensional feature maps, and thus
% using a neural network to learn the map would be a better alternative.}
2) As the Wasserstein-type divergence is symmetric, we manage to discretize and explore the space between the target and the source domain better than for the KL-type divergence based methods. 
3) In the setting of the paper, we do not yet have any theoretical reason to believe that one method dominates another. Therefore we present both methods (IR and SI) as complementary approaches. 
% {\color{red} The proposed IR and SI methods are complementary rather than competitors. One possible reason could be that the SI methods have more tuning parameters ($\rho_{\rm S}, \rho_{\rm T}$) than the IR methods ($\rho$). The additional flexibility can be advantageous if the construction of experts is cheap. However, the SI methods are computationally more challenging than the IR methods, and for a fixed time budget we can construct more IR experts than SI experts. WE HAVE NO THEORETICAL REASON TO BELIEVE THAT WHY ONE METHOD THEORETICALLY DOMINATES OTHER. THAT IS WHY WE PRESENT BOTH OF THEM IN THE PAPER}
\vspace{-.1cm}

{\bf Reviewer~5:}
We truly appreciate your positive comments about our paper.
Our paper focuses on deriving practical strategies without having to impose auxiliary distributional assumptions. If we desire some theoretical guarantees (for example, in the form of~DOI:10.1007/s10994-009-5152-4,  which addresses the unsupervised setting), we need to impose additional assumptions {similar to} covariate shift or concept drift. This, however, goes beyond the scope of this paper with its page limit.

%However, if further restrictions were to be imposed to the setting based on the distributional information across domains, we could provide theoretical guarantees. 
%We will add a discussion on the intuitive explanations what would be the desired setting that our method would perform better than other methods.

\vspace{-.1cm}
{\bf Reviewer~6:} 
% We will add the standard deviation information of the experimental results in the revision.
Thank you for your remarks about the typos, the missing reference for the convex combination strategy and the suggestion about the presentation of Table~1. We will address all of these in the revision.
\vspace{-.1cm}
\begin{enumerate}[label=\arabic*), leftmargin = 6mm, nosep]
    \item $\Sigma + \mu \mu^\top$ is the second-order moment matrix, and the constraint $\Sigma + \mu \mu^\top \succeq \varepsilon I_p$ is imposed for numerical stability.
    \item We ran another experiment with $d=313$; cf.\ Table~\ref{table:rebuttal}. As space is limited, the variance will be reported in the appendix.
    \item We set $N_{\rm T} = d$ so that $\hat \Sigma_{\rm T}$ is positive-definite without the need of regularization.
    \item Linear least squares problems depend only on the first- and the second-moments of the distributions. Thus, multi-modality does not affect the performance of our approaches. 
    % AS LONG AS FIRST TWO MOMENTS ARE ESTIMATED ACCURATELY.
\end{enumerate}
\vspace{-.15cm}
{\bf Reviewer~7:} 1) Please refer to the Application above for a real-life example. The sequential unseen target data is used to learn the weights of the experts at each time step, and they are not used for the domain adaptation procedure. In \emph{semi-supervised} domain adaptation there is scarce labelled and abundant unlabelled target data, while in \emph{supervised} domain adaptation there is only scarce labelled target data. 
Continuous transfer learning as described in [ref1] addresses a setting different from ours because (i) it considers an evolving target domain over time for a (ii) classification problem (iii) with access to unlabelled target data (semi-supervised setting) (iv) with transfer of domains at each time step, while in our case (i) the target domain is stationary, (ii) we focus on regression, (iii) we have access to limited labelled target data, and (iv) we perform domain adaptation only once to generate the experts. Please also see Figure~1 in the paper. More specifically, we study a single-source-single-target adaptation setting. We will clarify this further in the revision to prevent any confusion.
\vspace{-.1cm}
\begin{enumerate}[label=\arabic*), leftmargin = 6mm, nosep]
\setcounter{enumi}{1}
\item Chen et al.~(2016) and Wang et al.~(2020) consider unsupervised domain adaptation with {unlabelled} target data. Our paper addresses {supervised} domain adaptation with scarce, {labelled} target data. 
% They propose a robust min-max domain adaptation formulation for regression problems where the inner maximization is taken over the conditional distributions with predefined empirical statistics. 
% while we consider \emph{supervised} domain adaptation for \emph{regression} problems.
\item We will provide additional references from the transfer learning literature in the revision.
% Thank you for pointing us out relevant references. 
% Our setting is a special case of transfer learning while it is not really
% Transfer learning and domain adaptation are two interrelated areas with subtle differences and they could be used sometimes interchangeably. 
% We will add these references. 
% Once these differences are clarified and the setting of the study is determined, unfortunately in an 8 page paper there is no space to cover the related literature one for another. We would like to emphasize that none of the transfer learning references provided by the reviewer, namely [ref2], [ref3], [ref4] or [ref1] cite references for domain adaptation literature. However, we will provide a discussion on the difference of our setting with transfer learning and provide appropriate references when we have the a dditional page or in the appendix.
\item It is true that the target distribution may be mis-identified given scarce target data. This is why we have to use robust training to improve the performance of the predictor (see, arXiv:1810.08750, arXiv:1706.06083, arXiv:1710.10571). 
% We could only provide a small discussion on the pros and cons of the two methods in concluding remarks section due to page limitations. 
% We will expand this discussion in the revision.
\item All experiments reported in Table~1 use real-world data. 
% The Additional Results above reports the results on the WiFi dataset.
See also the Additional Results above on the Wifi dataset.
\end{enumerate}
% "IR-KL"	"IR-WASS"	"CC-L"	"CC-TL"	"CC-SL"	"CC-TE"	"CC-SE"	"RWS"	"LSE-T"	"LSE-T&S"
% "3.651199"	"1"	"5.366905"	"13.97612"	"2.302078"	"21.76301"	"1.112741"	"17.62441"	"28.41024"	"3.364484"
% "3.225586"	"1"	"4.807671"	"12.75548"	"2.195975"	"20.14511"	"1.126302"	"36.44791"	"25.18632"	"2.252994"
% "2.229021"	"1"	"3.787803"	"12.82733"	"2.087098"	"22.12336"	"1.125536"	"44.52673"	"61.78423"	"1.331157"
% "1.726287"	"1"	"2.899952"	"11.46934"	"1.854446"	"20.72216"	"1.108925"	"563.8894"	"54.50505"	"1.197554"

%%%% STDs

% "IR-KL"	"IR-WASS"	"CC-L"	"CC-TL"	"CC-SL"	"CC-TE"	"CC-SE"	"RWS"	"LSE-T"	"LSE-T&S"
% "3.91322823"	"0.398547485"	"5.96562474"	"17.0668644"	"1.94278566"	"27.0145252"	"0.432291007"	"23.5629119"	"45.8391204"	"7.67448111"

% "4.78986803"	"0.546055891"	"7.42232957"	"21.7478815"	"2.59963852"	"35.2365484"	"0.628064972"	"145.879027"	"59.3643513"	"7.73012409"

% "5.63869826"	"1.14449182"	"10.753483"	"47.6253514"	"4.53756832"	"90.7360724"	"1.25272014"	"579.53258"	"550.453931"	"7.62778892"

% "5.70718321"	"1.51973052"	"10.710389"	"56.1477174"	"4.34322184"	"117.079047"	"1.65912615"	"62607.6286"	"728.544721"	"7.85473327"


\begin{table*}[h!]
    \centering
    \footnotesize
    \vspace{-.3cm}
    \begin{tabular}{||c|c|c|c|c|c|c|c|c|c|c|c|c|c||}
    \hline
    \hline
    Data Set&Time &IR-KL &IR-WASS & CC-L &CC-TL &CC-SL & CC-TE &CC-SE &RWS &LSE-T &LSE-T$\&$S \\
    \hline
    \hline
    \multirow{4}{*}{$\substack{\text{UJIIndoorLoc}\\[.7ex] {\substack{ \text{Source:Predecessor} \\\text{Target:Posterior}}}}$} 
    &5 &3.65  &{1.00} &5.37  &13.98  &2.30  &21.76  &{1.11}  &17.62 &28.41  &3.36 \\
    &10 &3.23 &{1.00} &4.81 &12.76 &2.20 &20.15 &{1.13} &36.45 &25.19 &2.25\\
    &50 &2.23 &{1.00}	&3.79 &12.83 &2.09 &22.12 &{1.13} &44.53 &61.78 &1.33\\
    &100 &1.73 &{1.00}	&2.90 &11.47 &1.86 &20.72 &{1.11} &563.89 &54.51 &1.20 \\
    \hline
    \hline
    \end{tabular} 
    \vspace{-.3cm}
    \caption{Normalized cumulative loss values averaged over~100 independent runs. Source/target is divided based on timestamps.}
    \vspace{-5mm}
    \label{table:rebuttal}
\end{table*}


\end{document}
