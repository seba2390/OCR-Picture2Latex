
\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\input{style}
\usepackage{multirow}

\input{comments}
\usepackage{booktabs} % for professional tables
\usepackage{icml2021}
\usepackage{hyperref}
\usepackage{xr-hyper}
\makeatletter
\newcommand*{\addFileDependency}[1]{% argument=file name and extension
  \typeout{(#1)}
  \@addtofilelist{#1}
  \IfFileExists{#1}{}{\typeout{No file #1.}}
}
\makeatother
\newcommand*{\myexternaldocument}[1]{%
    \externaldocument{#1}%
    \addFileDependency{#1.tex}%
    \addFileDependency{#1.aux}%
}
\myexternaldocument{main}
\listfiles
\begin{document}
\appendix
	\renewcommand\thesection{\Alph{section}}
	\renewcommand{\theequation}{A.\arabic{equation}}
	\renewcommand{\thefigure}{A.\arabic{figure}}
	\renewcommand{\thetable}{A.\arabic{table}}
\onecolumn
\title{Appendix \\
Sequential Domain Adaptation \\ by Synthesizing Distributionally Robust Experts
}
\date{}
\maketitle
\section{Appendix}
% \begin{proposition}[Minimum Radius SI-KL]
%   \be \notag
% \begin{array}{cl}
%     \min & \msa_{\rm T}^\top \covsa_{\rm T} ^{-1} \msa_{\rm T} - 2 \msa_{\rm T}^\top \covsa_{\rm T}^{-1} \m+\Tr{M\covsa_{\rm T}^{-1}} -\log\det (M\covsa_{\rm T}^{-1}) -
%     \log(1- t) - p \\
%     \st & \tau \in \R_+,~\m \in \R^{p},~M\in \PD^{p}, t \in \R_+\\
%     &\msa_{\rm S}^\top \covsa_{\rm S}^{-1} \msa_k - 2 \msa_{\rm S}^\top \covsa_{\rm S}^{-1} \m+\Tr{M\covsa_{\rm S}^{-1}} -\log\det (M\covsa_{\rm S}^{-1}) -
%     \log(1- t) - p \leq \rho_{\rm S} \\[1ex]
% 	&\begin{bmatrix} M & \m \\ \m^\top & t \end{bmatrix} \succeq 0,~\begin{bmatrix}
% 	    M_{XX} & M_{XY} \\ M_{YX} & M_{YY}-\tau
% 	\end{bmatrix} \succeq 0.
% \end{array}
% \ee
% \end{proposition}
\subsection{Proof of Section~\ref{sec:IR}}
\begin{proof}[Proof of Proposition~\ref{prop:kl_interpolation}]
Note that optimization problem~\eqref{eq:mean_cov_interpolation} constitutes an unbounded convex optimization problem when $\psi$ is the Kullback-Leibler-type divergence of Definition~\ref{def:divergence}.
Let $g(\mu, \Sigma) \Let \lambda \mathds{D}((\mu, \Sigma) \parallel (\msa_{\rm S}, \covsa_{\rm S})) + (1-\lambda) \mathds{D}((\mu, \Sigma) \parallel (\msa_{\rm T}, \covsa_{\rm T}))$,
then, the first order optimality condition reads
\begin{align*}
\nabla_{\mu}g(\mu, \Sigma) &= 2\lambda\covsa^{-1}_{\rm S} (\mu-\msa_{\rm S}) + 2(1-\lambda)\covsa^{-1}_{\rm T} (\mu-\msa_{\rm T}) = 0,\\
\nabla_{\Sigma}g(\mu, \Sigma) &= \lambda \covsa^{-1}_{\rm S} - \lambda \Sigma^{-1} + (1-\lambda) \covsa^{-1}_{\rm T} -( 1 - \lambda) \Sigma^{-1} = 0.
\end{align*}
One can then show $(\msa_\lambda, \covsa_\lambda)$ provided in statement of Proposition~\ref{prop:kl_interpolation} solves the system of equalities above.
\end{proof}

Below we prove Proposition~\ref{prop:grad_f_D}. In the proof of Proposition~\ref{prop:grad_f_D} and its auxiliary lemmas, Lemma~\ref{lemma:dual-KL} and Lemma~\ref{lemma:extreme-KL}, we omit the subscripts $\lambda$ and $\rho$ to avoid clutter.

% The proof of Proposition~\ref{prop:grad_f_D} relies on Lemma~\ref{lemma:dual-KL} and Lemma~\ref{lemma:extreme-KL}.

\viet{
\begin{lemma}[Dual problem, backward KL] \label{lemma:dual-KL}
    Fix $(\msa, \covsa) \in \R^p \times \PD^p$ and $\rho \ge 0$. For any symmetric matrix $H \in \Sym^p$, the optimization problem
    \begin{subequations}
    \be \label{eq:KL-subproblem2}
    \left\{
	\begin{array}{cl}
	\Sup{\m, \cov } & \Tr{H (\cov+\m\m^\top)} \\
	\st & \Tr{\covsa \cov^{-1}} - \log\det (\covsa \cov^{-1}) - p + (\m - \msa)^\top \cov^{-1} (\m - \msa) \leq \rho, \\
	& \cov \succ 0
	\end{array}
	\right.
	\ee
	admits the dual formulation
	\[
	    something here
	\]
    \end{subequations}
\end{lemma}
}

\begin{lemma}[Dual problem] \label{lemma:dual-KL}
    Fix $(\msa, \covsa) \in \R^p \times \PD^p$ and $\rho \ge 0$. For any symmetric matrix $H \in \Sym^p$, the optimization problem
    \begin{subequations}
    \be \label{eq:KL-subproblem2}
    \left\{
	\begin{array}{cl}
	\Sup{\m, \cov } & \Tr{H (\cov+\m\m^\top)} \\
	\st & \Tr{\cov \covsa^{-1}} - \log\det (\cov \covsa^{-1}) - p + (\m - \msa)^\top \covsa^{-1} (\m - \msa) \leq \rho, \\
	& \cov \succ 0
	\end{array}
	\right.
	\ee
	admits the dual formulation
	\be \label{eq:KL-subproblem1}
	\left\{
	\begin{array}{cl}
	\inf &\dualvar (\rho - \msa^\top \covsa^{-1} \msa ) + \dualvar^2  \msa^\top \covsa^{-1} [\dualvar \covsa^{-1} - H]^{-1} \covsa^{-1} \msa  - \dualvar \log \det (I - \covsa^{\half} H \covsa^\half / \dualvar) \\
	\st & \dualvar \ge 0, \; \dualvar \covsa^{-1} \succ H.
	\end{array}
	\right.
	\ee
    \end{subequations}
\end{lemma}
\begin{proof}[Proof of Lemma~\ref{lemma:dual-KL}]
    For any $\m \in \R^p$ such that $(\m - \msa)^\top \covsa^{-1} (\m - \msa) \le \rho$, denote the set $\mc S_\m$ as
	\[
	\mc S_\m \Let \left\{
	\cov \in \PD^p : \Tr{\cov \covsa^{-1}} - \log\det \cov\leq \rho_\mu
	\right\},
	\]
	where $\rho_\m \in \R$ is defined as $\rho_{\m} \Let \rho + p - \log\det\covsa - (\m - \msa)^\top \covsa^{-1}(\m - \msa)$. Using these auxiliary notations, problem~\eqref{eq:KL-subproblem2} can be re-expressed as a nested program of the form
	\[
	\begin{array}{cl}
	\Sup{\m} & \m^\top H \m + \Sup{\cov \in \mc S_\m} ~ \Tr{H \cov} \\
	\st & (\m - \msa)^\top \covsa^{-1} (\m - \msa) \leq \rho,
	\end{array}
	\]
	where we emphasize that the constraint on $\m$ is redundant, but it is added to ensure the feasibility of the inner supremum over $\cov$ for every feasible value of $\m$ of the outer problem. We now proceed to reformulate the supremum subproblem over $\cov$.
	
	Assume momentarily that $H \neq 0$ and that $\m$ satisfies $(\m - \msa)^\top \covsa^{-1} (\m - \msa) < \rho$. In this case, one can verify that $\covsa$ is a Slater point of the convex set $\mc S_\m$. Using a duality argument, we find
	\begin{align*}
	\Sup{\cov \in \mc S_\m} ~ \Tr{H \cov} =& \Sup{\cov \succ 0} \Inf{\phi \ge 0} ~\Tr{H \cov} + \phi \big(\rho_\m - \Tr{\covsa^{-1} \cov} + \log\det \cov \big) \notag\\
	=& \Inf{\phi \ge 0} ~\left\{ \phi \rho_\m + \Sup{\cov \succ 0}~ \big\{ \Tr{(H - \phi \covsa^{-1})\cov}  + \phi \log \det \cov \big\} \right\},  \notag
	\end{align*}
	where the last equality follows from strong duality~\citep[Proposition~5.3.1]{ref:bertsekas2009convex}. %\note{the first equality is wrong and should be removed: the inner inf is either the first term or minus infinity}
	If $H - \phi \covsa^{-1} \not\prec 0$, then the inner supremum problem becomes unbounded. To see this, let $\sigma \in \R_+$ be the maximum eigenvalue of $H - \phi \covsa^{-1}$ with the corresponding eigenvector $v$, then the sequence $(\Sigma_k)_{k\in \mbb N}$ with $\Sigma_k = I + k vv^\top$ attains the asymptotic maximum objective value of $+\infty$. If $H - \phi \covsa^{-1} \prec 0$  then the inner supremum problem admits the unique optimal solution
	\be \label{eq:unique-cov}
	\cov\opt(\phi) = \phi (\phi \covsa^{-1} - H)^{-1},
	\ee
	which is obtained by solving the first-order optimality condition. By placing this optimal solution into the objective function and arranging terms, we have
	\be \label{eq:support-inner}
	\Sup{\cov \in \mc S_\m} ~ \Tr{H \cov} = \Inf{\substack{\phi \ge 0 \\ \phi \covsa^{-1} \succ H }}~ \phi \big( \rho - (\m - \msa)^\top \covsa^{-1} (\m - \msa) \big) - \phi \log \det (I - \covsa^\half H \covsa^\half /\phi).
	\ee
	We now argue that the above equality also holds when $\m$ is chosen such that $(\m - \msa)^\top \covsa^{-1} (\m - \msa) = \rho$. In this case, $\mc S_\m$ collapses into a singleton $\{\covsa\}$, and the left-hand side supremum problem attains the value $\Tr{H\covsa}$. The right-hand side infimum problem becomes
	\[
	    \Inf{\substack{\phi \ge 0 \\ \phi \covsa^{-1} \succ H }}~ - \phi \log \det (I - \covsa^\half H \covsa^\half /\phi).
	\]
	One can show using the l'Hopital rule that
	\[
	    \lim_{\phi \uparrow +\infty}~- \phi \log \det (I - \covsa^\half H \covsa^\half /\phi) = \Tr{H\covsa},
	\]
	which implies that the equality holds. Furthermore, when $H = 0$, the left-hand side of~\eqref{eq:support-inner} evaluates to 0, while the infimum problem on the right-hand side of~\eqref{eq:support-inner} also attains the optimal value of 0 asymptotically as $\phi$ decreases to 0. This implies that~\eqref{eq:support-inner} holds for all $H \in \mathbb{S}^p$ and for any $\m$ satisfying $(\m - \msa)^\top \covsa^{-1} (\m - \msa) \le \rho$. %Furthermore, because the objective function of the infimum problem is continuous in $\phi$, the constraint $\phi \ge 0$ can be converted to the strict inequality $\phi > 0$ without any loss of optimality.
	
	The above line of argument shows that problem~\eqref{eq:KL-subproblem2} can now be expressed as the following maximin problem
	\[
	    \Sup{\m: (\m - \msa)^\top \covsa^{-1} (\m - \msa) \leq \rho} ~ \Inf{\substack{\phi \ge 0 \\ \phi \covsa^{-1} \succ H }}~ \m^\top H \m + \phi \big( \rho - (\m - \msa)^\top \covsa^{-1} (\m - \msa) \big) - \phi \log \det (I - \covsa^\half H \covsa^\half /\phi).
	\]
	For any $\phi\ge 0$ such that $\phi \covsa^{-1} \succ H$, the objective function is concave in $\m$. For any $\m$, the objective function is convex in $\phi$. Furthermore, the feasible set of $\mu$ is convex and compact, and the feasible set of $\phi$ is convex. As a consequence, we can apply Sion's minimax theorem~\cite{ref:sion1958minimax} to interchange the supremum and the infimum operators, and problem~\eqref{eq:KL-subproblem2} is equivalent to
	\[
	\Inf{\substack{\phi \ge 0 \\ \phi \covsa^{-1} \succ H }}~\left\{
	\begin{array}{l}
	\phi \rho - \phi \log \det  (I - \covsa^\half H \covsa^\half /\phi)  \\
	\hspace{2cm} + \Sup{\m:(\m - \msa)^\top \covsa^{-1} (\m - \msa) \leq \rho} ~ \m^\top H \m  - \phi (\m - \msa)^\top \covsa^{-1} (\m - \msa)
	\end{array}
	\right\}.
	\]
	For any $\phi$ which is feasible for the outer problem, the inner supremum problem is a convex quadratic optimization problem because $ \phi \covsa^{-1} \succ H$. Using a strong duality argument, the value of the inner supremum equals to the value of
	\begin{align*}
	    &\Inf{\nu \ge 0} ~ \left\{ \nu \rho - (\nu + \phi) \msa^\top \covsa^{-1} \msa + \Sup{\m}~ \m^\top (H - (\phi + \nu) \covsa^{-1}) \m + 2 (\nu + \phi) (\covsa^{-1} \msa)^\top \m \right\}\\
    =& \Inf{\nu \ge 0} ~ \nu \rho - (\nu + \phi) \msa^\top \covsa^{-1} \msa + (\nu + \phi)^2 (\covsa^{-1} \msa)^\top [(\phi + \nu) \covsa^{-1} - H]^{-1}  (\covsa^{-1} \msa ),
	\end{align*}
	where the equality follows from the fact that the unique optimal solution in the variable $\m$ is given by
	\be \label{eq:unique-mu}
	   % \mu\opt(\phi, \nu) \Let 
	    (\phi + \nu) [ (\phi + \nu)\covsa^{-1} - H]^{-1}\covsa^{-1}\msa.
	\ee
	By combining two layers of infimum problem and using a change of variables $\dualvar \leftarrow \phi + \nu$,  problem~\eqref{eq:KL-subproblem2} can now be written as
	\be \label{eq:KL-subproblem3}
	\left\{
	\begin{array}{cl}
	\inf & \dualvar (\rho - \msa^\top \covsa^{-1} \msa ) +  \dualvar^2  \msa^\top \covsa^{-1} [\dualvar \covsa^{-1} - H]^{-1} \covsa^{-1} \msa  - \phi \log \det (I - \covsa^\half H \covsa^\half /\phi) \\
	\st & \phi \ge 0, \; \phi \covsa^{-1} \succ H, \; \dualvar - \phi \ge 0.
	\end{array}
	\right.
	\ee
	We now proceed to eliminate the multiplier $\phi$ from the above problem. To this end, rewrite the above optimization problem as
	\[
	\begin{array}{cl}
	\inf &\dualvar (\rho - \msa^\top \covsa^{-1} \msa ) + \dualvar^2  \msa^\top \covsa^{-1} [\dualvar \covsa^{-1} - H]^{-1} \covsa^{-1} \msa + g(\dualvar)\\
	\st & \dualvar \ge 0, \; \dualvar \covsa^{-1} \succ H,
	\end{array}
	\]
	where $g(\dualvar)$ is defined for every feasible value of $\dualvar$ as
	\be \label{eq:f-def}
	g(\dualvar) \Let \left\{
	\begin{array}{cl}
	\inf & - \phi \log \det (I - \covsa^{\half} H \covsa^\half / \phi) \\
	\st & \phi \ge 0, \; \phi \covsa^{-1} \succ H, \; \phi \le \dualvar.
	\end{array}
	\right.
	\ee
	Let $g_0 (\phi)$ denote the objective function of the above optimization, which is independent of $\dualvar$. Let $\sigma_1, \ldots, \sigma_p$ be the eigenvalues of $\covsa^\half H \covsa^\half$, we can write the function $g$ directly using the eigenvalues $\sigma_1, \ldots, \sigma_p$ as
	\[
	g_0(\phi) = -\phi \sum_{i = 1}^p \log (1 - \sigma_i/\phi).
	\]
	It is easy to verify by basic algebra manipulation that the gradient of $g_0$ satisfies
	\[
	\nabla g_0 (\phi) = \sum_{i=1}^p \left[ \log\left( \frac{\phi}{\phi - \sigma_i} \right) - \frac{\phi}{\phi - \sigma_i} \right] + p \leq 0,
	\]
	which implies that the value of $\phi$ that solves~\eqref{eq:f-def} is $\dualvar$, and thus $g (\dualvar) = - \dualvar \log \det (I - \covsa^{\half} H \covsa^\half / \dualvar)$. Substituting $\phi$ by $\dualvar$ in problem~\eqref{eq:KL-subproblem3} leads to the desired claim.
\end{proof}


\begin{lemma}[Optimal solution attaining $f(\beta)$] \label{lemma:extreme-KL}
    For any $(\msa, \covsa) \in \R^p \times \PD^p$, $\rho \in \R_{++}$ and $w \in \R^p$, $f(\beta)$ equals to the optimal value of the optimization problem
    \begin{subequations}
    \be \label{eq:KL-subproblem4}
    \left\{
	\begin{array}{cl}
	\Sup{\m, \cov \succ 0} & w^\top (\cov+\m\m^\top) w \\
	\st & \Tr{\cov \covsa^{-1}} - \log\det (\cov \covsa^{-1}) - p + (\m - \msa)^\top \covsa^{-1} (\m - \msa) \leq \rho,
	\end{array}
	\right.
	\ee
	which admits the unique optimal solution
    \be \label{eq:KL-mcov}
        \cov\opt =
        \dualvar\opt(\dualvar\opt \covsa^{-1} - ww^\top)^{-1}, \qquad \m\opt =\cov\opt \covsa^{-1} \msa,
    \ee
    with $\dualvar\opt > w^\top \covsa w$ being the unique solution of the nonlinear equation
    \begin{equation}\label{eq:KL-FOC}
        \rho = \frac{(w^\top \msa)^2 w^\top \covsa w}{(\dualvar - w^\top \covsa w)^2} + \frac{w^\top \covsa w}{\dualvar - w^\top \covsa w} + \log\Big( 1 - \frac{w^\top \covsa w}{\dualvar}\Big).
    \end{equation}
    Moreover, we have $\dualvar\opt \le w^\top \covsa w \big(1 + 2\rho + \sqrt{1 + 4 \rho (w^\top \msa)^2} \big)/(2\rho)$.
    \end{subequations}
\end{lemma}
\begin{proof}[Proof of Lemma~\ref{lemma:extreme-KL}]
    First, note that
    \begin{align*}
        f(\beta) & = \Sup{\QQ \in \mbb B} \EE_\QQ\left[(\beta^\top X - Y)^2 \right] = \Sup{\QQ \in \mbb B} \EE_\QQ \left[w^\top \xi \xi^\top w \right] = \Sup{ (\m, \cov) \in {\mbb U} } w^\top \left( \cov + \m\m^\top  \right) w ,
    \end{align*}
    which, by the definition of $\mbb U$ and definition~\eqref{def:KL}, equals to the optimal value of problem~\eqref{eq:KL-subproblem4}.

    From the duality result in Lemma~\ref{lemma:dual-KL}, problem~\eqref{eq:KL-subproblem4} is equivalent to
    \[
	\begin{array}{cl}
	\inf &\dualvar (\rho - \msa^\top \covsa^{-1} \msa ) + (\dualvar \covsa^{-1} \msa)^\top [\dualvar \covsa^{-1} - ww^\top]^{-1} ( \dualvar \covsa^{-1} \msa ) - \dualvar \log \det (I - \covsa^{\half} ww^\top \covsa^\half / \dualvar) \\
	\st & \dualvar \ge 0, \; \dualvar \covsa^{-1} \succ ww^\top.
	\end{array}
	\]
	Applying~\citet[Fact~2.16.3]{ref:bernstein2009matrix}, we have the equalities
	\begin{align*}
	     \det (I - \covsa^{\half} ww^\top \covsa^\half / \dualvar) &= 1 - w^\top \covsa w/\dualvar \\
	     (\dualvar \covsa^{-1} - ww^\top)^{-1} &= \dualvar^{-1} \covsa + \dualvar^{-2} \big( 1 - w^\top \covsa w/\dualvar \big)^{-1} \covsa w w^\top \covsa,
	\end{align*}
	and thus by some algebraic manipulations we can rewrite
	\be \label{eq:KL-extreme1}
	    f (\beta) =  \left\{
	\begin{array}{cl}
	\inf &\dualvar \rho  + \frac{\dualvar (w^\top \msa)^2}{\dualvar - w^\top \covsa w } - \dualvar \log \big( 1 - w^\top \covsa w/\dualvar \big) \\
	\st & \dualvar > w^\top \covsa w.
	\end{array}
	\right.
	\ee
	Let $f_0$ be the objective function of the above optimization problem. The gradient of $f_0$ satisfies
	\[
	    \nabla f_0(\dualvar) = \rho - \frac{(w^\top \msa)^2 w^\top \covsa w}{(\dualvar - w^\top \covsa w)^2} - \frac{w^\top \covsa w}{\dualvar - w^\top \covsa w} - \log\Big( 1 - \frac{w^\top \covsa w}{\dualvar}\Big).
	\]
	By the above expression of $\nabla f_0 (\dualvar)$ and the strict convexity of $f_0 (\dualvar)$, the value $\dualvar\opt$ that solves~\eqref{eq:KL-FOC} is also the unique minimizer of~\eqref{eq:KL-extreme1}. In other words, $f_0 (\kappa) = f(\beta)$.
	
	We now proceed to show that $(\m\opt, \cov\opt)$ defined as in~\eqref{eq:KL-mcov} is feasible and optimal. First, we prove feasibility of $(\m\opt, \cov\opt)$. By direct computation,
	\begin{subequations}
	\be \label{eq:KL-feasiblity1}
	    (\m\opt - \msa)^\top \covsa^{-1} (\m\opt - \msa) = \msa^\top (\covsa^{-1} \cov\opt - I) \covsa^{-1} (\cov\opt \covsa^{-1} - I) \msa = \frac{(\msa^\top w)^2 w^\top \covsa w}{(\dualvar\opt - w^\top \covsa w)^2}.
	\ee
	Moreover, because $\cov\opt \covsa^{-1} = I + (\dualvar\opt - w^\top \covsa w)^{-1} \covsa ww^\top$, we have
	\be \label{eq:KL-feasiblity2}
	    \Tr{\cov\opt \covsa^{-1}} - \log\det (\cov\opt \covsa^{-1}) - p = (\dualvar\opt - w^\top \covsa w)^{-1} w^\top \covsa w + \log \big(1 - \frac{w^\top \covsa w}{\dualvar\opt}\big).
	\ee
	\end{subequations}
	Combining~\eqref{eq:KL-feasiblity1} and~\eqref{eq:KL-feasiblity2}, we have
	\begin{align*}
	     \Tr{\cov\opt \covsa^{-1}} - \log\det (\cov\opt \covsa^{-1}) - p +  (\m\opt - \msa)^\top \covsa^{-1} (\m\opt - \msa) = \rho,
	\end{align*}
	where the first equality follows from the definition of $\mathds D$, and the second equality follows from the fact that $\dualvar\opt$ solves~\eqref{eq:KL-FOC}. This shows the feasibility of $(\m\opt, \cov\opt)$.
	
	Next, we prove the optimality of $(\m\opt, \cov\opt)$.
	Through a tedious computation, one can show that
	\begin{align*}
	    &w^\top (\cov\opt + (\m\opt)(\m\opt)^\top) w = w^\top (\cov\opt + \cov\opt \covsa^{-1} \msa \msa^\top \covsa^{-1} \cov\opt) w\\
	    =& w^\top \covsa w \Big(1 + \frac{w^\top \covsa w}{\dualvar\opt - w^\top \covsa w} \Big) + (\msa^\top w)^2 \Big( 1 + \frac{2 w^\top \covsa w}{\dualvar\opt - w^\top \covsa w} \Big) + \frac{(w^\top \msa)^2 (w^\top \covsa w)^2}{(\dualvar\opt - w^\top \covsa w)^2}\\
	    =& \frac{\dualvar\opt w^\top \covsa w}{\dualvar\opt - w^\top \covsa w} + \frac{(\dualvar\opt)^2 (\msa^\top w)^2}{(\dualvar\opt - w^\top \covsa w)^2} \\
	    =& \frac{\dualvar\opt w^\top \covsa w}{\dualvar\opt - w^\top \covsa w}  + \frac{\dualvar\opt (\msa^\top w)^2 w^\top \covsa w}{(\dualvar\opt - w^\top \covsa w)^2} + \frac{\dualvar\opt (\msa^\top w)^2}{\dualvar\opt - w^\top \covsa w} \\
	    =& \dualvar\opt \rho - \dualvar\opt \log \big( 1- \frac{w^\top \covsa w}{\dualvar\opt} \big)  + \frac{\dualvar\opt (\msa^\top w)^2}{\dualvar\opt - w^\top \covsa w} = f_0(\dualvar\opt) = f(\beta),
	\end{align*}
	where the antepenultimate equality follows from the fact that $\dualvar\opt$ solves~\eqref{eq:KL-FOC}, and the last equality holds because $\dualvar\opt$ is the minimizer of~\eqref{eq:KL-extreme1}. Therefore, $(\m\opt, \cov\opt)$ is optimal to problem~\eqref{eq:KL-subproblem4}. The uniqueness of~$(\m\opt, \cov\opt)$ now follows from the unique solution of $\cov$ and $\mu$ with respect to the dual variables from~\eqref{eq:unique-cov} and~\eqref{eq:unique-mu}, respectively.
	
	It now remains to show the upper bound on $\dualvar\opt$. Towards that end, we note that for any $\dualvar > w^\top \covsa w$, 
	\begin{align*}
	    0 &  = \rho - \frac{(w^\top \msa)^2 w^\top \covsa w}{(\dualvar\opt - w^\top \covsa w)^2} - \frac{w^\top \covsa w}{\dualvar\opt - w^\top \covsa w} - \log\Big( 1 - \frac{w^\top \covsa w}{\dualvar\opt}\Big)  > \rho - \frac{(w^\top \msa)^2 w^\top \covsa w}{(\dualvar\opt - w^\top \covsa w)^2} - \frac{w^\top \covsa w}{\dualvar\opt - w^\top \covsa w}.
	\end{align*}
	Solving the above quadratic inequality in the variable $\dualvar\opt - w^\top \covsa w$ yields the desired bound. This completes the proof.
% 	For any $\dualvar > w^\top \covsa w$, we have $-\log(1 - w^\top \covsa w/\dualvar) > 0$ and thus\
% 	\[
% 	    \nabla f_0(\dualvar) > \rho - \frac{(w^\top \msa)^2 w^\top \covsa w}{(\dualvar - w^\top \covsa w)^2} - \frac{w^\top \covsa w}{\dualvar - w^\top \covsa w}.
% 	\]
% 	If we denote temporarily by $h(\dualvar)$ the right-hand side of the above inequality, \textcolor{red}{then the solution $\dualvar_{\max}$ that satisfies $h(\dualvar_{\max}) = 0$ should be bigger than $\dualvar\opt$ that satisfies $\nabla f_0(\dualvar\opt) = 0$ (why?)}. By definition, $\dualvar_{\max}$ satisfies
% 	\[
% 	    \rho (\dualvar_{\max} - w^\top \covsa w)^2 - w^\top \covsa w (\dualvar_{\max} - w^\top \covsa w) - (w^\top \msa)^2 w^\top \covsa w = 0,
% 	\]
% 	which is a quadratic equation. Solving for $\dualvar_{\max}$ gives the postulated upper bound.
\end{proof}

We are now ready to prove Proposition~\ref{prop:grad_f_D}.

%\viet{please change $f$ to $f_{\lambda, \rho}$. }
\begin{proof}[Proof of Proposition~\ref{prop:grad_f_D}]
    The convexity of $f$ follows immediately by noting that it is the pointwise supremum of the family of convex functions $\EE_\QQ[(\beta^\top X - Y)^2]$ parametrized by $\QQ$.

    To prove the continuously differentiability and the formula for the gradient, recall the expression~\eqref{eq:KL-extreme1} for the function $f(\beta)$:
    \begin{equation}\label{opt:f}
        f (\beta)  = \left\{
	\begin{array}{cl}
	\inf &\dualvar \rho  + \frac{\dualvar (w^\top \msa)^2}{\dualvar - w^\top \covsa w } - \dualvar \log \big( 1 - w^\top \covsa w/\dualvar \big) \\
	\st & \dualvar > w^\top \covsa w.
	\end{array}
	\right.
    \end{equation}
Problem~\eqref{opt:f} has only one constraint. Therefore, LICQ (hence MFCQ) always holds, which implies that the Lagrange multiplier $\zeta_\beta$ of problem~\eqref{opt:f} is unique for any $\beta$. Also, it is easy to see that the constraint of problem~\eqref{opt:f} is never binding. So, $\zeta_\beta = 0$ for any $\beta$. The Lagrangian function $L_{\beta}: \R \times \R \rightarrow \R $ is given by
\begin{equation*}
L_{\beta} (\dualvar, \zeta) = \rho \dualvar + \frac{\omega_2 \dualvar}{\dualvar - \omega_1} - \dualvar \log\left(1- \frac{\omega_1}{\dualvar} \right) + \zeta (\omega_1 - \dualvar),
\end{equation*}
where $\omega_1 =  w^\top \covsa w$ and $\omega_2 =  (w^\top \msa)^2$.
The first derivative with respect to $\dualvar$ is 
\begin{equation*}
\frac{\mathrm d L_\beta}{\mathrm d \dualvar}(\dualvar , \zeta) = \rho - \frac{\omega_1 \omega_2}{(\dualvar- \omega_1)^2 } - \log\left( 1 - \frac{\omega_1}{\dualvar} \right) - \frac{\omega_1}{\dualvar - \omega_1} - \zeta.
\end{equation*}
The second derivative with respect to $\dualvar$ is
\begin{equation*}
\frac{\mathrm d^2 L_\beta}{\mathrm d\dualvar^2}(\dualvar , \zeta) = \frac{\omega_1}{(\dualvar - \omega_1)^3} \left( 2\omega_2 + \frac{\omega_1}{\dualvar}(\dualvar - \omega_1) \right) .
\end{equation*}
From the proof of Lemma~\ref{lemma:extreme-KL}, we have that the minimizer $\dualvar_\beta$ of problem~\eqref{opt:f} is precisely the $ \dualvar\opt$ defined by equation~\eqref{eq:KL-FOC} (below we write $\dualvar_\beta$ instead of $ \dualvar\opt$ to emphasize and keep track of the dependence on $\beta$). Therefore, for any $\beta$, the minimizer $\dualvar_\beta$ exists and is unique.
So, there exists some constant $\eta_{\beta} > 0$ such that
\begin{equation*}
\frac{\mathrm d^2 L_{\beta}}{\mathrm d\dualvar^2}(\dualvar_{\beta} , \zeta_{\beta}) \ge \eta_{\beta} >0.
\end{equation*}
Therefore, for any $\beta$, the strong second order condition at $\dualvar_{\beta}$ holds (see~\citet[Definition 6.2]{still2018lectures}). By \citet[Theorem 6.7]{still2018lectures},
\begin{equation}\label{eq:f_grad}
\nabla f (\beta) = \nabla_\beta L_\beta (\dualvar_\beta , \zeta_\beta) = \nabla_\beta L_\beta (\dualvar_\beta , 0)\quad \forall \beta\in \R^d.
\end{equation}
Then we compute
\begin{align*}
\nabla_w L_{\beta} (\dualvar, \zeta) & = \nabla_w \left[ \frac{\dualvar (w^\top \msa)^2}{\dualvar - w^\top \covsa w } - \dualvar \log \left( 1 - \frac{w^\top \covsa w}{\dualvar}  \right) + \zeta (w^\top \covsa w - \dualvar) \right] \\
& = \frac{2\dualvar \omega_2}{(\dualvar - \omega_1)^2} \covsa w + \frac{2\dualvar}{(\dualvar - \omega_1)} \msa \msa^\top w + \frac{2\dualvar}{(\dualvar - \omega_1)} \covsa w + 2\zeta \covsa w.
\end{align*}
Hence,
\begin{align*}
&\, \nabla_\beta L_\beta (\dualvar, \zeta) = \frac{d w}{d\beta}^\top \cdot \nabla_w L_{\beta} (\dualvar, \zeta) = [I_d \ \mathbf{0}_d ] \cdot \nabla_w L_{\beta} (\dualvar, \zeta),
%= &\, \frac{2\dualvar \omega_2}{(\dualvar - \omega_1)^2} \left(\covsa_{XX} \beta - \covsa_{XY} \right) + \frac{2\dualvar}{(\dualvar - \omega_1)} \left(\msa_X \msa_X^\top \beta - \msa_X \msa_Y \right)  + \frac{2\dualvar}{(\dualvar - \omega_1)} \left(\covsa_{XX} \beta - \covsa_{XY} \right) + 2\zeta \left(\covsa_{XX} \beta - \covsa_{XY} \right).
\end{align*}
which, when combined with \eqref{eq:f_grad}, yields the desired gradient formula
\begin{equation*}
\nabla f (\beta) = \frac{2\dualvar_\beta \left(  \omega_2  \covsa w \!+\! (\dualvar_\beta \! - \!\omega_1 ) (\covsa \! +\! \msa \msa^\top) w \right)_{1:d}}{(\dualvar_\beta  - \omega_1 )^2}.
% \nabla f (\beta) = \frac{2\dualvar_\beta  \omega_2 }{(\dualvar_\beta  - \omega_1 )^2} \left(\covsa_{XX} \beta - \covsa_{XY} \right) + \frac{2\dualvar_\beta }{(\dualvar_\beta  - \omega_1 )} \left(\msa_X \msa_X^\top \beta - \msa_X \msa_Y \right)  + \frac{2\dualvar_\beta }{(\dualvar_\beta  - \omega_1 )} \left(\covsa_{XX} \beta - \covsa_{XY} \right).
\end{equation*}
By \citet[Theorem 6.5]{still2018lectures}, the function $\beta \mapsto \dualvar_\beta$ is locally Lipschitz continuous, \ie, for any $\beta\in \R^d$, there exists $c_\beta,\epsilon_\beta > 0$ such that if $\norm{\beta' - \beta}_2 \le \epsilon_\beta$, then
\begin{equation*}\label{ineq:gamma_Lip}
|\dualvar_{\beta'} - \dualvar_{\beta}| \le c_\beta \norm{ \beta' - \beta }_2.
\end{equation*}
Note that $\omega_1$ and $\omega_2 $ are both locally Lipschitz continuous in $\beta$. Also, it is easy to see that $\dualvar_\beta > \omega_1 $ for any $\beta$. Thus, $\nabla f (\beta)$ is locally Lipschitz continuous in $\beta$.
\end{proof}


\begin{proof}[Proof of~\ref{prop:wass_interpolation}]
Noting that problem~\eqref{eq:mean_cov_interpolation} is the barycenter problem between two Gaussian distributions with respect to the Wasserstein distance, the proof then directly follows from \citet[\S6.2]{agueh2011barycenters} and \citet[Example~1.7]{ref:mccann1997convexity}.
\end{proof}

\begin{proof}[Proof of Proposition~\ref{prop:IR_W}]
    Again we omit the subscripts $\lambda$ and $\rho$.
    Reminding that $\xi = (X, Y)$, we find
    \begin{equation}\label{eq:wass_f}
        \begin{split}
        &\Sup{\QQ \in \mbb B} \EE_\QQ[(\beta^\top X - Y)^2] = \Sup{\QQ \in \mbb B} \EE_\QQ[(w^\top \xi)^2] \\
        =& \left\{
	\begin{array}{cl}
	\inf & \dualvar \big(\rho - \|\msa\|_2^2 -  \Tr{\covsa} \big) + z + \Tr{Z} \\
	\st & \dualvar \in \R_+, \; z \in \R_+, \; Z \in \PSD^p \\
	& \begin{bmatrix} \dualvar I - ww^\top & \dualvar \covsa^\half \\ \dualvar \covsa^\half & Z \end{bmatrix} \succeq 0, \; \begin{bmatrix} \dualvar I - ww^\top & \dualvar \msa \\ \dualvar \msa^\top & z \end{bmatrix} \succeq 0
	\end{array}
	\right. \\
	=&\left\{
	    \begin{array}{cl}
	        \inf & \dualvar \big(\rho - \|\msa\|_2^2 -  \Tr{\covsa} \big) + \dualvar^2 \msa^\top (\dualvar I - ww^\top)^{-1} \msa + \dualvar^2 \Tr{\covsa (\dualvar I - ww^\top)^{-1}} \\
	        \st & \dualvar \ge \| w \|_2^2 ,
	    \end{array}
	\right.
    \end{split}
    \end{equation}
    where the second equality follows from \citet[Lemma 2]{ref:kuhn2019wasserstein}. By applying~\citet[Fact~2.16.3]{ref:bernstein2009matrix}, we find
	\begin{equation}\label{eq:sherman_morrison}
	     (\dualvar I - ww^\top)^{-1} = \dualvar^{-1} I + \dualvar^{-2} \big( 1 - \|w\|_2^2/\dualvar \big)^{-1}  w w^\top .
	\end{equation}
	Combining \eqref{eq:wass_f} and \eqref{eq:sherman_morrison}, we get
	\[
	    \Sup{\QQ \in \mbb B} \EE_\QQ[(\beta^\top X - Y)^2] =
	    \left\{
	        \begin{array}{cl}
	            \inf & \dualvar \rho + \dualvar  w^\top (\covsa + \msa \msa^\top) w / (\dualvar - \| w \|_2^2 )\\
	            \st & \dualvar \ge \| w\|_2^2.
	        \end{array}
	    \right.
	\]
	One can verify through the first-order optimality condition that the optimal solution $\dualvar\opt$ is
	\[
	    \dualvar\opt = \| w \|_2 \left( \| w \|_2 + \sqrt{\frac{w^\top (\covsa + \msa \msa^\top) w }{\rho}} \right),
	\]
	and by replacing this value $\dualvar\opt$ into the objective function, we find
	\[
	    \Sup{\QQ \in \mbb B} \EE_\QQ[(\beta^\top X - Y)^2] = \big( \sqrt{w^\top (\covsa + \msa\msa^\top) w} + \sqrt{\rho}\|w\|_2 \big)^2,
	\]
	which then completes the proof.
\end{proof}
\newpage
\subsection{Proof of Section~\ref{sec:SI}}

\begin{lemma}[Compactness] \label{lemma:D-set}
For $k\in \{\rm S, \rm T\}$, the set
\[\mbb V_k = \{(\mu, M) \in \R^p\times \mbb S^p_{++} : M- \mu \mu^\top \in \mbb S_{++}^p, \mathds{D}((\mu, M-\mu\mu^\top) \parallel (\msa_k, \covsa_k)) \leq \rho_{k} \} \] 
is convex and compact. Furthermore, the set
\[\mbb V \Let \{(\m, M) \in \R^p\times \mbb S^p_{++} :(\m, M- \m\m^\top) \in \mbb U_{\rho_{\rm S}, \rho_{\rm T}}\} \]
is also convex and compact.
\end{lemma}

\begin{proof}[Proof of Lemma~\ref{lemma:D-set}]
	For any $(\mu, M) \in \R^p\times \mbb S^p_{++} $ such that $M - \m\m^\top \in \PD^p$, we find
	\begin{align}
		&\mathds D\big((\m, M-\m\m^\top) \parallel (\msa_k, \covsa_k)\big) \notag\\
		=& (\m - \msa_k)^\top\covsa^{-1}_k (\m - \msa_k) +\Tr{(M - \m\m^\top) \covsa^{-1}} - \log\det ((M - \m\m^\top) \covsa_k^{-1}) - p \notag\\
		%&=\msa^\top \covsa^{-1} \msa - 2 \msa^\top \covsa^{-1} \m+\Tr{M\covsa^{-1}}  + \log\det ((M - \m\m^\top)^{-1} \covsa) - p\\
		=& \msa_k^\top \covsa_k^{-1} \msa_k - 2 \msa_k^\top \covsa_k^{-1} \m+\Tr{M\covsa_k^{-1}} - \log\det (M\covsa_k^{-1}) - \log(1- \m^\top M^{-1} \m) - p \label{eq:divergence_mu_M},
	\end{align}
	where in the last expression, we have used the determinant formula~\citep[Fact~2.16.3]{ref:bernstein2009matrix} to rewrite
	\[
	    \det(M - \m\m^\top) = (1 - \m^\top M^{-1} \m) \det M.
	\]
% 	The (joint) convexity of all the terms in \eqref{eq:divergence_mu_M} is clear but the term $- \log(1- \m^\top M^{-1} \m)$. To see that the term $- \log(1- \m^\top M^{-1} \m)$ is also (jointly) convex in $(\m, M)$, we first note that $\m^\top M^{-1} \m $ is jointly convex in $(\m, M)$. Since the function $t\mapsto -\log(1-t)$ is a monotone convex function, the term $- \log(1- \m^\top M^{-1} \m)$ is jointly convex in $(\m, M)$. Since $\mbb V_k$ is the level set of a 
	
	
	
	Because $M - \m\m^\top \in \PD^p$, one can show that $1 - \m^\top M^{-1} \m > 0$ by invoking the Schur complement, and as such, the logarithm term in the last expression is well-defined. Moreover, we can write 
	\begin{align}
	\mbb V_k = \left\{(\m, M) :
	\begin{array}{l}
	(\m, M) \in \R^p \times \PD^p,~M - \m\m^\top \in \PD^p,~\exists t \in \R_+: \\
	\msa_k^\top \covsa_k^{-1} \msa_k - 2 \msa_k^\top \covsa_k^{-1} \m+\Tr{M\covsa_k^{-1}} - \log\det (M\covsa_k^{-1}) - \log(1- t) - p \leq \rho \\
	\begin{bmatrix} M & \m \\ \m^\top & t \end{bmatrix} \succeq 0	\end{array}
	\right\}, \label{eq:D-refor}
	\end{align}
	which is a convex set. Notice that by Schur complement, the semidefinite constraint is equivalent to $t \ge \m^\top M^{-1} \m$.
	
	Next, we show that $\mbb V_k$ is compact. Denote by $\mbb U_k =  \{ (\m, \cov)\in \R^p\times \PSD^p:  \mathds{D}( (\m, \cov)\! \parallel\! (\msa_k, \covsa_k) )\le \rho_k \}$. Then, it is easy to see that $\mbb V_k$ is the image of $\mbb U_k$ under the continuous mapping $(\m, \cov) \mapsto (\m, \cov + \m\m^\top)$. Therefore, it suffices to prove the compactness of $\mbb U_k$. Towards that end, we note that 
	\[ {\mathds D} \big( (\m, \cov) \parallel (\msa_k, \covsa_k) \big) =(\msa_k - \m)^\top\covsa_k^{-1} (\msa_k - \m) +  \Tr{\cov \covsa_k^{-1}} - \log\det (\cov \covsa_k^{-1}) - p  \]
	is a continuous and coercive function in $(\m, \cov)$. Thus, as a level set of ${\mathds D} \big( (\m, \cov) \parallel (\msa_k, \covsa_k) \big)$, $\mbb U_k$ is closed and bounded, and hence compact.
	
	To prove the last claim, by the definitions of $\mbb V$ and $\mbb U_{\rho_{\rm S}, \rho_{\rm T}}$ we write
	\begin{align}
	    &\mbb V = \{(\m, M) \in \R^p\times \mbb S^p_{++} :(\m, M- \m\m^\top) \in \mbb U_{\rho_{\rm S}, \rho_{\rm T}}\}\notag\\
	    = &\{(\m, M) \in \R^p\times \mbb S^p_{++} : (\m, M) \in \mbb V_{\rm S}\} \cap \{(\m, M) \in \R^p\times \mbb S^p_{++}: (\m, M) \in \mbb V_{\rm T} \}\cap \{(\m, M) \in \R^p\times \mbb S^p_{++}: M\succeq \eps I\} \label{eq:V_intersect}.
	\end{align}
	The convexity of $\{(\m, M) \in \R^p\times \mbb S^p_{++} :(\m, M- \m\m^\top) \in \mbb U_{\rho_{\rm S}, \rho_{\rm T}}\}$ then follows from the convexity of the three sets in~\eqref{eq:V_intersect}.
	Furthermore, from the first part of the proof, we know that both $\{(\m, M) \in \R^p\times \mbb S^p_{++} : (\m, M) \in \mbb V_{\rm S}\}$ and $\{(\m, M) \in \R^p\times \mbb S^p_{++}: (\m, M) \in \mbb V_{\rm T} \}$ are compact sets, so is their intersection. Also, the last set $\{(\m, M) \in \R^p\times \mbb S^p_{++}: M\succeq \eps I\}$ in \eqref{eq:V_intersect} is closed. Since any closed subset of a compact set is again compact, we conclude that $\mbb V$ is compact. This completes the proof.
	
	
% 	It now remains to show that $\mbb V_{\mathds D}(\msa, \covsa)$ is compact, or equivalently that $\mbb U_{\mathds D}(\msa, \covsa)$ is compact. If $\rho = 0$ then $\mbb U_{\mathds D}(\msa, \covsa)$ is a singleton $\{(\msa, \covsa%
% )\}$ and the claim holds trivially. For the rest of the proof, we consider
% when $\rho > 0$. Pick an arbitrary $(\m, \cov) \in \mbb U_{\mathds D}(\msa, \covsa)$,
% it is obvious that $\cov$ should satisfy
% \begin{equation*}
% \Tr{\cov^\half \covsa^{-1} \cov^\half} - \log\det (\cov^\half \covsa^{-1} %
% \cov^\half) - p \leq \rho,
% \end{equation*}
% which implies that $\cov$ is bounded. To see this, suppose that $\{\cov%
% _{k}\}_{k \in \mbb N}$ is a sequence of positive definite matrices and $%
% \{\sigma_k\}_{k \in \mbb N}$ is the corresponding sequence of the maximum
% eigenvalues of $\{\cov_k^{\half} \covsa^{-1} \cov_k^{\half}\}_{k \in \mbb %
% N}$. Because the function $\sigma \mapsto \sigma - \log \sigma - 1$ is
% non-negative for every $\sigma > 0$, we find
% \begin{equation*}
% \Tr{\cov_k^\half \covsa^{-1} \cov_k^\half} - \log\det (\cov_k^\half \covsa^{-1} \cov_k^\half) - p \geq \sigma_k - \log \sigma_k -1.
% \end{equation*}
% If $\cov_k$ tends to infinity, then $\sigma_k$ tends to infinity. This implies
% that $\cov$ should be bounded in the sense that $\cov \preceq \bar \sigma
% I_p $ for some finite positive constant $\bar \sigma$. Using an analogous
% argument, we can show that $\cov$ is lower bounded in the sense that $\cov %
% \succeq \underline{\sigma} I_p$ for some finite positive constant $%
% \underline{\sigma}$. Moreover, $\m$ is also bounded because $\m$
% should satisfy $(\m-\msa)^{\top}\covsa^{-1}(\m-\msa) \leq \rho$. We now can
% rewrite $\mbb U_{\mathds D}(\msa, \covsa)$ as
% \begin{equation*}
% \mbb U_{\mathds D}(\msa, \covsa) = \left\{ (\m, \cov) \in \R^p \times \PD^p: \begin{array}{l} (\m-\msa)^{\top}\covsa^{-1}(\m-\msa) \leq \rho,~\underline{\sigma} I_p \preceq \cov %
% \preceq \bar{\sigma} I_p \\
% \mathds D \big( (\m, \cov) \parallel (\msa, \covsa%
% ) \big) \leq \rho
% \end{array}
% \right\},
% \end{equation*}
% which implies that $\mbb U_{\mathds D}(\msa, \covsa)$ is a closed set because $%
% \mathds D \big( (\cdot, \cdot) \parallel (\msa, \covsa) \big)$ is a
% continuous function in $(\m, \cov)$ when $\cov$ ranges over $\underline{%
% \sigma} I_p \preceq \cov \preceq \bar{\sigma} I_p$. This observation coupled
% with the boundedness of $(\m, \cov)$ established the compactness of $\mbb U_{\mathds D}(\msa, \covsa)$ and $\mbb V_{\mathds D}(\msa, \covsa)$.
\end{proof}




\begin{proof}[Proof of Theorem~\ref{thm:ls-kl}]
    As $\xi = (X, Y)$, we can rewrite
    \begin{subequations}
    \begin{align}
        & \Min{\beta \in \R^d} \Sup{\QQ \in \mbb B_{\rho_{\rm S}, \rho_{\rm T}}} \EE_\QQ[(\beta^\top X - Y)^2] \\
        =&\Min{\beta \in \R^d} 
        \Sup{\QQ \in \mbb B_{\rho_{\rm S}, \rho_{\rm T}}} \!\begin{bmatrix} \beta \\ -1 \end{bmatrix}^\top \EE_\QQ[\xi \xi^\top] \begin{bmatrix} \beta \\ -1 \end{bmatrix}
        \\
        = &\Min{\beta \in \R^d} \Sup{(\m, M- \m\m^\top) \in \mbb U_{\rho_{\rm S}, \rho_{\rm T}}} \begin{bmatrix} \beta \\ -1 \end{bmatrix}^\top M \begin{bmatrix} \beta \\ -1 \end{bmatrix} \notag\\
        = &\Min{\beta \in \R^d} \Sup{(\m, M) \in \mbb V} \begin{bmatrix} \beta \\ -1 \end{bmatrix}^\top M \begin{bmatrix} \beta \\ -1 \end{bmatrix} \notag\\
        = &\Sup{(\m, M) \in \mbb V}\Min{\beta \in \R^d}  \begin{bmatrix} \beta \\ -1 \end{bmatrix}^\top M \begin{bmatrix} \beta \\ -1 \end{bmatrix} \label{eq:aux-1}\\
        =& \Sup{(\m, M) \in \mbb V}~M_{YY} - M_{XY}^\top M_{XX}^{-1} M_{XY} \label{eq:aux-2}
\end{align}
\end{subequations}
where~\eqref{eq:aux-1} follows from the Sion's minimax theorem, which holds because the objective function is convex in $\beta$, concave in $M$, and Lemma~\ref{lemma:D-set}. Equation~\eqref{eq:aux-2} exploits the unique optimal solution in $\beta$ as $\beta\opt = M_{XX}^{-1} M_{XY}$, in which the matrix inverse is well defined because $M \succ 0$ for any feasible $M$.

Finally, after an application of the Schur complement reformulation to~\eqref{eq:aux-2}, the nonlinear semidefinite program in the theorem statement follows from representations~\eqref{eq:D-refor} and \eqref{eq:V_intersect}. This completes the proof.
\end{proof}

\begin{proof}[Proof of Proposition~\ref{prop:minimum_radius}]
It is well-known that the space of probability measures equipped with the Wasserstein distance $W_2 $ is a geodesic metric space (see \citet[Section 7]{ref:villani2008optimal} for example), meaning that for any two probability distributions $\mc N_0$ and $\mc N_1$, there exists a constant-speed geodesic curve $[0,1] \ni a\mapsto \mc N_a$ satisfying 
\[ W_2 ( \mc N_a, \mc N_{a'} ) = |a - a'| W_2 ( \mc N_0, \mc N_1 ) \quad\forall a,a'\in [0,1].\]

The claim follows trivially if $W_2 ( \mc N_{\rm S}, \mc N_{\rm T} ) \le \sqrt{\rho_{\rm S}}$. Therefore, we assume $W_2 ( \mc N_{\rm S}, \mc N_{\rm T} ) >\sqrt{\rho_{\rm S}}$.

Consider the the geodesic $\mc N_t$ from $\mc N_0 = \mc N_{\rm S}$ to $\mc N_1 = \mc N_{\rm T}$. Also, denote by $\mbb U_k =  \{ (\m, \cov)\in \R^p\times \PSD^p:  \mathds{D}( (\m, \cov)\! \parallel\! (\msa_k, \covsa_k) )\le \rho_k \}$ for $k\in \{\rm S, \rm T\} $. Then, $\mbb U_{\rm S}$ and $\mbb U_{\rm T} $ has empty intersection if and only if 
\[ W_2 ( \mc N_a , \mc N_{\rm S} ) \le \sqrt{\rho_{\rm S}} \Longrightarrow W_2 ( \mc N_a , \mc N_{\rm T} ) > \sqrt{\rho_{\rm T}} \quad\forall a\in [0,1],\]
which is in turn equivalent to 
\[ a W_2 ( \mc N_{\rm T} , \mc N_{\rm S} ) \le \sqrt{\rho_{\rm S}} \Longrightarrow (1-a) W_2 ( \mc N_{\rm T} , \mc N_{\rm S} ) \le \sqrt{\rho_{\rm T}} \quad \forall a\in[0,1].\]
Picking $a = \frac{\sqrt{\rho_{\rm S}}}{W_2 ( \mc N_{\rm T} , \mc N_{\rm S} )} \in (0,1)$, then we have
\begin{align*}
    \left( 1 - \frac{\sqrt{\rho_{\rm S}}}{W_2 ( \mc N_{\rm T} , \mc N_{\rm S} )} \right) W_2 ( \mc N_{\rm T} , \mc N_{\rm S} ) \le \sqrt{\rho_{\rm T}}.
\end{align*}
The above inequality can be rewritten as
\[ W_2 ( \mc N_{\rm T} , \mc N_{\rm S} ) \le \sqrt{\rho_{\rm S}} + \sqrt{\rho_{\rm T}}, \]
which contradicts with our supposition
\[\rho_{\rm T}\geq \left(\sqrt{\mathds  W((\msa_{\rm S}, \covsa_{\rm S}) \parallel (\msa_{\rm T}, \covsa_{\rm T}))} - \sqrt{\rho_{\rm S}}\right)^{2}.\]
Thus, $\mbb U_{\rm S}$ and $\mbb U_{\rm T}$ has non-empty intersection.
\end{proof}

\begin{proof}[Proof of Theorem~\ref{thm:ls-w}]
    As $\xi = (X, Y)$, we can rewrite
    \begin{subequations}
    \begin{align}
    &\Min{\beta \in \R^d} \Sup{\QQ \in \mbb B_{\rho_{\rm S}, \rho_{\rm T}}(\Pnom)} \EE_\QQ[(\beta^\top X - Y)^2] \\
        = & \Min{\beta \in \R^d} \Sup{(\m, M - \mu \mu^\top) \in \mbb U_{\rho_{\rm S}, \rho_{\rm T}}} \begin{bmatrix} \beta \\ -1 \end{bmatrix}^\top M \begin{bmatrix} \beta \\ -1 \end{bmatrix} \notag\\
        = & \Sup{(\m, M - \m\m^\top) \in \mbb U_{\rho_{\rm S}, \rho_{\rm T}}}\Min{\beta \in \R^d}  \begin{bmatrix} \beta \\ -1 \end{bmatrix}^\top M \begin{bmatrix} \beta \\ -1 \end{bmatrix} \label{eq:auxg-1}\\
        = & \Sup{(\m, M - \m\m^\top) \in \mbb U_{\rho_{\rm S}, \rho_{\rm T}}}~M_{YY} - M_{XY}^\top M_{XX}^{-1} M_{XY} 
        \label{eq:auxg-3}
\end{align}
\end{subequations}
where~\eqref{eq:auxg-1} follows from the Sion's minimax theorem, which holds because the objective function is convex in $\beta$, concave in $M$, and the set $\mbb U_{\rho_{\rm S}, \rho_{\rm T}}$ is compact~~\citep[Lemma~A.6]{ref:abadeh2018wasserstein}. Equation~\eqref{eq:auxg-3} exploits the unique optimal solution in $\beta$ as $\beta\opt = M_{XX}^{-1} M_{XY}$, in which the matrix inverse is well defined because $M - \m\m^\top \succeq \eps I$ for any feasible $M$.
\end{proof}

\section{Additional Numerical Results}
The corresponding codes are available anonymously at~\url{https://github.com/RAO-EPFL/DR-DA.git}.
In the following the details of the datasets used in Section~\ref{sec:numerical} are presented.
    \begin{itemize}[leftmargin = 3mm]
      \item \textbf{Uber$\&$Lyft\footnote{Available publicly at~\url{https://www.kaggle.com/brllrb/uber-and-lyft-dataset-boston-ma}}} has~$N_{\rm S} = 5000$ instances in the source domain and~5000 available samples in the target domain. 
      \item \textbf{US Births~(2018)\footnote{Available publicly at~\url{https://www.kaggle.com/des137/us-births-2018}}} has~$N_{\rm S} = 5172$ samples in the source domain and~4828 available samples in the target domain.
       \item \textbf{Life Expectancy{\footnote{Available publicly at~\url{https://www.kaggle.com/kumarajarshi/life-expectancy-who}}}} has~$N_{\rm S} = 1407$ instances in the source domain and~242 available samples in the target domain.
     \item \textbf{House Prices in King County\footnote{Available publicly at~\url{https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data}}} has~$N_{\rm S} = 543$ instances in the source domain and~334 available samples in the target domain.
         \item \textbf{California Housing Prices\footnote{The modified version that we use is available publicly at~\url{https://www.kaggle.com/camnugent/california-housing-prices} and the original dataset is available publicly at~\url{https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html}}} has $N_{\rm S} = 9034$ instances in the source domain, and~6496 available instances in the target domain.
    \end{itemize}
    \begin{figure}
     \centering
     \begin{subfigure}[b]{0.45\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Figures/birth_USAcumloss.pdf}
         \caption{US Births (2018)}
         \label{fig:us_births}
     \end{subfigure}\hfill
     \begin{subfigure}[b]{0.45\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Figures/life_expectancycumloss.pdf}
         \caption{Life Expectancy}
         \label{fig:life_exp}
     \end{subfigure}\hfill
     \begin{subfigure}[b]{0.45\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Figures/housescumloss.pdf}
         \caption{House Prices in KC}
         \label{fig:houses}
     \end{subfigure}\hfill
     \begin{subfigure}[b]{0.45\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Figures/california_housingcumloss.pdf}
         \caption{California Housing}
         \label{fig:california_housing}
     \end{subfigure}\hfill
        \caption{Cumulative loss averaged over 100 runs on logarithmic scale}
        \label{fig:cum_loss_all}
\end{figure}
Figure~\ref{fig:cum_loss_all} demonstrates how the average cumulative loss in~\eqref{eq:cumulative-loss} grows over time for the US Births (2018), Life Expectancy, House Prices in KC and California Housing datasets. 
The results suggest that the IR-WASS and SI-WASS experts perform favorably over the competitors in that their cumulative loss at each time step is lower than that of most other competitors.  
    \newpage
\bibliography{bibliography}
\bibliographystyle{icml2021}
\end{document}

\newpage
\section{Old Appendix}
\subsection{Proof of Section~\ref{sect:dro}}
\begin{proof}[Proof of Theorem~\ref{thm:general}]
    We denote by $\mc M(\m, \cov)$ the space of all probability measures supported on $\R^p$ with fixed mean $\m \in \R^p$ and fixed covariance matrix $\cov \in \PD^p$. Using this notation, for any $\beta \in \R^d$,  we can rewrite the worst-case expected loss under the ambiguity set $\mbb B_{\psi}(\Pnom)$ as a two-layer optimization problem as follows
    \[
        \Sup{\QQ \in \mbb B_{\psi}(\Pnom)}~\EE_\QQ[ \ell(\beta, \xi)] = \Sup{(\m, \cov) \in \mbb U_{\psi}(\msa, \covsa)} \Sup{\QQ \in \mc M(\m, \cov)} \EE_{\QQ}[ \ell(\beta, \xi)].
    \]
    For any $(\m, \cov) \in \mbb U_{\psi}(\msa, \covsa)$, we can apply \cite[Lemma~A.1]{ref:zymler2013distributionally} to form the dual problem of the inner supremum problem as
	\[
	\Sup{\QQ \in \mc M(\m, \cov)} \EE_{\QQ} \left[ \ell(\beta, \xi) \right] \le \left\{
	\begin{array}{cll}
		\inf & h_0 + 2 h^\top \m + \Tr{H(\cov + \m \m^\top)} \\
		\st & h_0 \in \R, \; h \in \R^p, \; H \in \Sym^p \\
		& h_0 + 2h^\top \xi +  \xi^\top H \xi \geq \ell(\beta, \xi) &\forall \xi \in \R^p.
	\end{array}
	\right.
	\]
% 	Notice that the above inequality holds thanks to weak duality of moment problems~\cite{ref:isii1960extrema}. Moreover, strong duality holds whenever $\cov \in \PD^p$, in this case the above inequality becomes an equality. Because the loss function $\ell$ is nonnegative, we can add the semi-infinite constraint
% 	\[
% 	    h_0 + 2t^\top \xi +  \xi^\top T \xi \geq 0 \qquad \forall \xi \in \R^p
% 	\]
% 	into the dual problem without altering its optimal value. This semi-infinite constraint can be reformulated using the S-lemma~\cite{ref:polik2007survey} as a semidefinite constraint
% 	\[
% 	    \begin{bmatrix}
% 	        T & t \\ t^\top & h_0
% 	    \end{bmatrix}
% 	    \succeq 0.
% 	\]
	By letting $\mc T$ denote the effective feasible set of the above infimum problem, that is,
	\begin{align*}
		\mc T &\Let \left\{ h_0 \in \R, \; h \in \R^p, \; H \in \mathbb{S}^p: ~h_0 + 2h^\top \xi +  \xi^\top H \xi \geq \ell(\beta, \xi) ~~\forall \xi \in \R^p \right\}.
	\end{align*}
	Notice that the set $\mc T$ is dependent on $\beta$, however this dependency is omitted for clarity of exposition. For every $\beta \in \R^d$, we find
	\begin{subequations}
	\begin{align}
		\Sup{\QQ \in \mbb B_\psi (\Pnom)} \EE_{\QQ} \left[ \ell(\beta,\xi) \right] &= \Sup{(\m, \cov) \in \mbb U_{\psi}(\msa, \covsa)}~\Sup{\QQ \in \mc M(\m, \cov)}~\EE_{\QQ}[ \ell(\beta, \xi)] \label{eq:KL-refor-1} \\
				&\le \Sup{(\m, \cov) \in \mbb U_{\psi}(\msa, \covsa)}~\Inf{(h_0, h, H) \in \mc T}~h_0 + 2 h^\top \m + \Tr{H (\cov + \m \m^\top)} \label{eq:KL-refor0} \\
				&= \Sup{(\m, M) \in \mbb V_{\psi}(\msa, \covsa)}~\Inf{(h_0, h, H) \in \mc T}~h_0 + 2 h^\top \m + \Tr{HM} \label{eq:KL-refor1}\\
				&=\Inf{(h_0, h, H) \in \mc T}~\Sup{(\m, M) \in \mbb V_{\psi}(\msa, \covsa)}~h_0 + 2 h^\top \m + \Tr{HM} \label{eq:KL-refor2} \\
				&= \Inf{(h_0, h, H) \in \mc T}~\left\{ h_0 + \delta^*_\psi(2h, H) \right\}, \label{eq:KL-refor3}
	\end{align}
	\end{subequations}
	where in~\eqref{eq:KL-refor1} we reparametrize the optimization in the space of first- and second-moment statistics. Equality~\eqref{eq:KL-refor2} due to Sion's minimax theorem~\cite{ref:sion1958minimax} which holds because $\mbb V_{\psi}(\msa, \covsa)$ is convex and compact, $\mc T$ is convex, and the objective function of~\eqref{eq:KL-refor1} is concave-convex. In equality~\eqref{eq:KL-refor3}, we use the definition of the support function $\delta^*_\psi$ of the set $\mbb V_{\psi}(\msa, \covsa)$.
	
	In the next step, we show that the inequality~\eqref{eq:KL-refor0} can actually be converted to an equality, and~\eqref{eq:KL-refor3} is a tight upper bound to the worst-case expected loss. The argument follows from that of~	\cite[Theorem~3.41]{ref:nguyen2019adversarial} with necessary modifications as follows. Denote temporarily by $g$ the optimal value of the inner infimum in~\eqref{eq:KL-refor1}, that is,
	\[
		g(\m, M) = \Inf{(h_0, h, H) \in \mc T} \; h_0 + 2 t^\top \m + \Tr{T M}.
	\]	
	Because $g$ is the pointwise infimum of continuous functions, $g$ is upper semicontinuous~\cite[Lemma~2.41]{ref:aliprantis06hitchhiker}. Moreover, because $\mbb V_\psi(\msa, \covsa)$ is a compact set, the set of optimizers of the supremum problem~\eqref{eq:KL-refor1} is non-empty \cite[Theorem~2.43]{ref:aliprantis06hitchhiker} and \eqref{eq:KL-refor1} can be written with the maximum operator as
	\be \label{eq:auxi}
		\Max{(\m, M) \in \mbb V_\psi(\msa, \covsa)}~g(\m, M).
	\ee
	Denote by $(\m\opt, M\opt)$ the optimal solution of~\eqref{eq:auxi} and let $\cov\opt = M\opt - \m\opt (\m\opt)^\top$. If $\cov\opt \succ 0$, then~$(\m\opt, \cov\opt )$ is feasible for problem~\eqref{eq:KL-refor-1}, and ~\eqref{eq:KL-refor0} holds as an equality as a direct consequence of strong duality~\cite{ref:isii1960extrema}. It now remains to prove the claim for the case when $\cov\opt$ is singular. For any $\rho \in \R_{++}$, because $\psi$ is continuous, there exists a neighborhood $\mc B$ around $(\m\opt, \cov\opt)$ such that $\mc B \cap \mbb U_\psi(\msa, \covsa)$ is non-empty and that there exists $(\bar \m, \bar \cov) \in \mc B \cap \mbb U_\psi(\msa, \covsa)$ such that $\bar\cov \succ 0$. Denote $\bar M = \bar\cov + \bar \m \bar \m^\top$, it is trivial from the construction of $\mbb V_\psi(\msa, \covsa)$ that $(\bar \m, \bar M) \in \mbb V_\psi(\msa, \covsa)$. Consider the sequence $(\m_k, M_k)_{k \in \mbb N}$ defined as
	\[
	    \m_k = \frac{1}{k} \bar \m + \frac{k-1}{k} \m\opt, \quad M_k = \frac{1}{k} \bar M + \frac{k-1}{k} M\opt.
	\]
	Because~$\mbb V_\psi(\msa, \covsa)$ is convex, we have $(\m_k, M_k) \in \mbb V_\psi(\msa, \covsa)$ for any $k \in \mbb N$.
	Notice that the covariance matrix $\cov_k$ associated with $(\m_k, M_k)$ is positive semidefinite for any $k \in \mbb N$ because
	\begin{align*}
	   \cov_k = M_k - \m_k \m_k^\top &= \frac{1}{k} (\bar \Sigma + \bar \m \bar \m^\top) + \frac{k-1}{k} (\cov\opt + \m\opt (\m\opt)^\top) - \left(\frac{1}{k} \bar \m + \frac{k-1}{k} \m\opt \right) \left(\frac{1}{k} \bar \m + \frac{k-1}{k} \m\opt \right)^\top \\
	   &= \frac{1}{k} \bar \cov + \frac{k-1}{k} \cov\opt + \frac{k-1}{k^2} (\bar \m - \m\opt)(\bar \m - \m\opt)^\top \succ 0.
	\end{align*}
	The function $g$ is concave because it is the pointwise infimum of linear, and thus concave, function. Hence we find
	\[
		g(\m_k, M_k) \geq \frac{1}{k} g(\bar \m, \bar M) + \frac{k-1}{k} g(\m\opt, M\opt).
	\]
	As a result, we have
	\begin{align*}
		 g(\m\opt, M\opt) = \lim\limits_{k \to \infty} \frac{1}{k} g(\bar \m, \bar M) + \frac{k-1}{k} g(\m\opt, M\opt) &\leq \lim\limits_{k \to \infty} g(\m_k, M_k) \\
		 &= \lim\limits_{k \to \infty} \Sup{\QQ \in \mc M(\m_k, \cov_k)}~\EE_{\QQ}[ \ell(\beta, \xi)] \\
		 &\le \Sup{(\m,M) \in \mbb V_\psi(\msa, \covsa)} \Sup{\QQ \in \mc M(\m, \cov)} \EE_\QQ[\ell(\beta, \xi)],
	\end{align*}
	where the first inequality is from the concavity of $g$, the second equality is from the strong duality because $\cov_k \succ 0$~\cite{ref:isii1960extrema}, and the last inequality is from the definition of the supremum. This result implies that inequality~\eqref{eq:KL-refor0} is actually an equality.
	
	Exploiting~\eqref{eq:KL-refor3} and the definition of the set $\mc T$, we have
	\[
	    \Inf{\beta \in \R^d}~\Sup{\QQ \in \mbb B_\psi (\Pnom)} \EE_{\QQ} \left[ \ell(\beta,\xi) \right] =
	    \left\{
	        \begin{array}{ccl}
	            \Inf{\beta \in \R^d} & \inf & h_0 + \delta^*_\psi(2h, H) \\
	            &\st & h_0 \in \R, \; h \in \R^p, \; T \in \mathbb{S}^p \\
	            &&h_0 + 2t^\top \xi +  \xi^\top T \xi \geq \ell(\beta, \xi) ~~\forall \xi \in \R^p.
	        \end{array}
	    \right.
	\]
	Combining two infimum problems and utilizing the definition of the set $\mc S$ completes the proof.
	\end{proof}


\begin{proof}[Proof of Proposition~\ref{prop:semi-refor}]
    The proof heavily relies on the fact that for any $\beta \in \R^d$, we have $\beta^\top X - Y = w^\top \xi$ with $w \in \R^p$ being defined as $w \Let [\beta^\top, -1]^\top$.

	Consider now the case when $\ell$ is a $\|\cdot \|_2^2$ regression loss function. In this case, one can rewrite $\mc S$ as
    \begin{align*}
        \mc S &= \left\{ (\beta, h_0, h, H): \exists w \in \R^p,\, w = [\beta^\top, -1]^\top,\, h_0 + 2t^\top \xi +  \xi^\top T \xi \geq (w^\top \xi - \alpha)^2 \qquad \forall \xi \in \R^p \right\} \\
        &= \left\{ (\beta, h_0, h, H):  \exists w \in \R^p,\, w = [\beta^\top, -1]^\top,\, \begin{bmatrix} T - ww^\top & t + \alpha w \\ (t + \alpha w)^\top & h_0 - \alpha^2 \end{bmatrix} \succeq 0 \right\},
    \end{align*}
    where the second equality follows from the S-lemma. The semidefinite constraint involves a quadratic term $ww^\top$, which can be linearized by adding an auxiliary variables $W \in \PSD^p$ along with the constraint $W \succeq ww^\top$. Linearizing this constraint $W \succeq ww^\top$ by the Schur complement completes the reformulation of $\mc S$ for the $\|\cdot\|_2^2$ loss function.

	Consider when $\ell$ is a Huber loss function. In this case, we can write $\ell$ using the inf-convolution formulation as
	\begin{align*}
		\ell(\beta, x, y) &= \Inf{\theta_1 \in \R} \; \half \theta_1^2 + \delta |  \beta^\top x - y - \alpha - \theta_1 | \\
		&= \Inf{\theta_1 \in \R} \; \max\left\{ \half \theta_1^2 + \delta (\beta^\top x - y - \alpha - \theta_1), \; \half \theta_1^2 + \delta (- \beta^\top x + y + \alpha + \theta_1) \right\}.
	\end{align*}
	The semi-infinite constraint defining the feasible set $\mc S$ can be reformulated as
	\[
	\mc S = \left\{ (\beta, h_0, h, H):
	\begin{array}{ll}
	\exists (w, \theta_1) \in \R^p\times\R, \, w = [\beta^\top, -1]^\top \\
	h_0 + 2 t^\top \xi + \xi^\top T \xi \ge \half \theta_1^2 + \delta (w^\top \xi - \alpha - \theta_1) & \forall \xi \in \R^p \\
	h_0  + 2t^\top \xi + \xi^\top T \xi \geq \half \theta_1^2 + \delta (-w^\top \xi + \alpha + \theta_1) & \forall \xi \in \R^p
	\end{array}
	\right\},
	\]
	and hence $\mc S$ admits a conic representation
	\be \notag
	\mc S = \left\{ (\beta, h_0, h, H):
	\begin{array}{l}
		\exists (w, \theta_1) \in \R^p\times \R, \, w = [\beta^\top, -1]^\top \\
		\begin{bmatrix}
			T & t - \frac{\delta}{2} w \\ t^\top - \frac{\delta}{2} w^\top &h_0 - \half \theta_1^2 + \delta(\alpha + \theta_1)
		\end{bmatrix} \succeq 0,
		\\
		\begin{bmatrix}
			T & t + \frac{\delta}{2} w \\ t^\top + \frac{\delta}{2} w^\top &h_0 - \half \theta_1^2 - \delta(\alpha + \theta_1)
		\end{bmatrix} \succeq 0,
	\end{array}
	\right\}.
	\ee
	which is nonlinear because of the quadratic terms in $\theta_1$. 	In the last step, we replace the term $\half \theta_1^2$ by an auxiliary variable $\theta_2 \in \R_+$ with an additional constraint $\theta_2 \geq \half \theta_1^2$. Formulating this additional constraint as a semidefinite constraint completes the proof for the Huber loss.
	
	The other loss functions can be trivially re-expressed as a pointwise maximium of quadratic functions. The detailed proof is thus omitted.
\end{proof}


%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Proof of Section~\ref{sect:KL}}



The uncertainty set on the mean-covariance matrix space
\[ \mbb U_{\mathds D}(\msa, \covsa) = \{(\m, \cov) \in \R^p \times \PD^p: \mathds D\big((\m, \cov) \parallel (\msa, \covsa)\big) \leq \rho\}
\]
induces an uncertainty set on the first- and second-moment matrix space
\[ \mbb V_{\mathds D}(\msa, \covsa) \Let \{(\m, M) \in \R^p \times \PD^p: M - \m\m^\top \in \PD^p, \mathds D\big((\m, M-\m\m^\top) \parallel (\msa, \covsa) \big) \leq \rho\}.
\]


\begin{proof}[Proof of Proposition~\ref{prop:D-set}]
	For any $M \in \PD^p$ such that $M - \m\m^\top \in \PD^p$, we find
	\begin{align*}
		&\mathds D\big((\m, M-\m\m^\top) \parallel (\msa, \covsa)\big) \\
		=& (\m - \msa)^\top\covsa^{-1} (\m - \msa) +\Tr{(M - \m\m^\top) \covsa^{-1}} - \log\det ((M - \m\m^\top) \covsa^{-1}) - p \\
		%&=\msa^\top \covsa^{-1} \msa - 2 \msa^\top \covsa^{-1} \m+\Tr{M\covsa^{-1}}  + \log\det ((M - \m\m^\top)^{-1} \covsa) - p\\
		=& \msa^\top \covsa^{-1} \msa - 2 \msa^\top \covsa^{-1} \m+\Tr{M\covsa^{-1}} - \log\det (M\covsa^{-1}) - \log(1- \m^\top M^{-1} \m) - p ,
	\end{align*}
	where in the last expression, we have used the determinant formula~\citep[Fact~2.16.3]{ref:bernstein2009matrix} to rewrite
	\[
	    \det(M - \m\m^\top) = (1 - \m^\top M^{-1} \m) \det M.
	\]
	Because $M - \m\m^\top \in \PD^p$, one can show that $1 - \m^\top M^{-1} \m > 0$ by invoking the Schur complement, and as such, the logarithm term in the last expression is well-defined. Moreover, we can rewrite $\mbb V_{\mathds D}(\msa, \covsa)$ as
	\begin{align}
x`	&\mbb V_{\mathds D}(\msa, \covsa) \notag \\
	&= \left\{(\m, M) :
	\begin{array}{l}
	(\m, M) \in \R^p \times \PD^p,~M - \m\m^\top \in \PD^p,~\exists t \in \R_+: \\
	\msa^\top \covsa^{-1} \msa - 2 \msa^\top \covsa^{-1} \m+\Tr{M\covsa^{-1}} - \log\det (M\covsa^{-1}) - \log(1- t) - p \leq \rho \\
	\begin{bmatrix} M & \m \\ \m^\top & t \end{bmatrix} \succeq 0	\end{array}
	\right\}, \label{eq:D-refor}
	\end{align}
	which is a convex set. Notice that by Schur complement, the semidefinite constraint is equivalent to $t \ge \m^\top M^{-1} \m$.
	
	It now remains to show that $\mbb V_{\mathds D}(\msa, \covsa)$ is compact, or equivalently that $\mbb U_{\mathds D}(\msa, \covsa)$ is compact. If $\rho = 0$ then $\mbb U_{\mathds D}(\msa, \covsa)$ is a singleton $\{(\msa, \covsa%
)\}$ and the claim holds trivially. For the rest of the proof, we consider
when $\rho > 0$. Pick an arbitrary $(\m, \cov) \in \mbb U_{\mathds D}(\msa, \covsa)$,
it is obvious that $\cov$ should satisfy
\begin{equation*}
\Tr{\cov^\half \covsa^{-1} \cov^\half} - \log\det (\cov^\half \covsa^{-1} %
\cov^\half) - p \leq \rho,
\end{equation*}
which implies that $\cov$ is bounded. To see this, suppose that $\{\cov%
_{k}\}_{k \in \mbb N}$ is a sequence of positive definite matrices and $%
\{\sigma_k\}_{k \in \mbb N}$ is the corresponding sequence of the maximum
eigenvalues of $\{\cov_k^{\half} \covsa^{-1} \cov_k^{\half}\}_{k \in \mbb %
N}$. Because the function $\sigma \mapsto \sigma - \log \sigma - 1$ is
non-negative for every $\sigma > 0$, we find
\begin{equation*}
\Tr{\cov_k^\half \covsa^{-1} \cov_k^\half} - \log\det (\cov_k^\half \covsa^{-1} \cov_k^\half) - p \geq \sigma_k - \log \sigma_k -1.
\end{equation*}
If $\cov_k$ tends to infinity, then $\sigma_k$ tends to infinity. This implies
that $\cov$ should be bounded in the sense that $\cov \preceq \bar \sigma
I_p $ for some finite positive constant $\bar \sigma$. Using an analogous
argument, we can show that $\cov$ is lower bounded in the sense that $\cov %
\succeq \underline{\sigma} I_p$ for some finite positive constant $%
\underline{\sigma}$. Moreover, $\m$ is also bounded because $\m$
should satisfy $(\m-\msa)^{\top}\covsa^{-1}(\m-\msa) \leq \rho$. We now can
rewrite $\mbb U_{\mathds D}(\msa, \covsa)$ as
\begin{equation*}
\mbb U_{\mathds D}(\msa, \covsa) = \left\{ (\m, \cov) \in \R^p \times \PD^p: \begin{array}{l} (\m-\msa)^{\top}\covsa^{-1}(\m-\msa) \leq \rho,~\underline{\sigma} I_p \preceq \cov %
\preceq \bar{\sigma} I_p \\
\mathds D \big( (\m, \cov) \parallel (\msa, \covsa%
) \big) \leq \rho
\end{array}
\right\},
\end{equation*}
which implies that $\mbb U_{\mathds D}(\msa, \covsa)$ is a closed set because $%
\mathds D \big( (\cdot, \cdot) \parallel (\msa, \covsa) \big)$ is a
continuous function in $(\m, \cov)$ when $\cov$ ranges over $\underline{%
\sigma} I_p \preceq \cov \preceq \bar{\sigma} I_p$. This observation coupled
with the boundedness of $(\m, \cov)$ established the compactness of $\mbb U_{\mathds D}(\msa, \covsa)$ and $\mbb V_{\mathds D}(\msa, \covsa)$.

We now proceed to provide the reformulation for the support function of $\mbb V_{\mathds D}(\msa, \covsa)$. By substituting the expression of $\mathds D$, the support function becomes
\be \label{eq:KL-subproblem}
	\delta^*(2h, H) = \left\{
	\begin{array}{cl}
	\Sup{\m, \cov \succ 0} & 2t^\top \m + \Tr{T (\cov+\m\m^\top)} \\
	\st & \Tr{\cov \covsa^{-1}} - \log\det (\cov \covsa^{-1}) - p + (\m - \msa)^\top \covsa^{-1} (\m - \msa) \leq \rho
	\end{array}
	\right.
	\ee
	whose objective function is nonconvex whenever $T \succeq 0$. For any $\m \in \R^p$ such that $(\m - \msa)^\top \covsa^{-1} (\m - \msa) \le \rho$, denote the set $\mc S_\m$ as
	\[
	\mc S_\m \Let \left\{
	\cov \in \PD^p : \Tr{\cov \covsa^{-1}} - \log\det \cov\leq \rho_\mu
	\right\},
	\]
	where $\rho_\m \in \R$ is defined as $\rho_{\m} \Let \rho + p - \log\det\covsa - (\m - \msa)^\top \covsa^{-1}(\m - \msa)$. Using these auxiliary notations, problem~\eqref{eq:KL-subproblem} can be re-expressed as a nested program of the form
	\[
	\begin{array}{cl}
	\Sup{\m} & 2t^\top \m + \m^\top T \m + \Sup{\cov \in \mc S_\m} ~ \Tr{T \cov} \\
	\st & (\m - \msa)^\top \covsa^{-1} (\m - \msa) \leq \rho,
	\end{array}
	\]
	where we emphasize that the constraint on $\m$ is redundant, but it is added to ensure the feasibility of the inner supremum over $\cov$ for every feasible value of $\m$ of the outer problem. We now proceed to reformulate the supremum subproblem over $\cov$.
	
	Assume momentarily that $T \neq 0$ and that $\m$ satisfies $(\m - \msa)^\top \covsa^{-1} (\m - \msa) < \rho$. In this case, one can verify that $\covsa$ is the Slater point of the convex set $\mc S_\m$. Using a duality argument, we find
	\begin{align*}
	\Sup{\cov \in \mc S_\m} ~ \Tr{T \cov} =& \Sup{\cov \succ 0} \Inf{\lambda \ge 0} ~\Tr{T \cov} + \lambda \big(\rho_\m - \Tr{\covsa^{-1} \cov} + \log\det \cov \big) \notag\\
	=& \Inf{\lambda \ge 0} ~\left\{ \lambda \rho_\m + \Sup{\cov \succ 0}~ \big\{ \Tr{(T - \lambda \covsa^{-1})\cov}  + \lambda \log \det \cov \big\} \right\},  \notag
	\end{align*}
	where the last equality follows from strong duality~\citet[Proposition~5.3.1]{ref:bertsekas2009convex}. %\note{the first equality is wrong and should be removed: the inner inf is either the first term or minus infinity}
	If $T - \lambda \covsa^{-1} \not\prec 0$, then the inner supremum problem becomes unbounded. To see this, let $\sigma \in \R_+$ be the maximum eigenvalue of $T - \lambda \covsa^{-1}$ with the corresponding eigenvector $v$, then the sequence $(\Sigma_k)_{k\in \mbb N}$ with $\Sigma_k = I + k vv^\top$ attains the asymptotic maximum objective value of $+\infty$. If $T - \lambda \covsa^{-1} \prec 0$  then the inner supremum problem admits the unique optimal solution
	\be \label{eq:unique-cov}
	\cov\opt(\lambda) = \lambda (\lambda \covsa^{-1} - T)^{-1},
	\ee
	which is obtained by solving the first-order optimality condition. By placing this optimal solution into the objective function and arranging terms, we have
	\be \label{eq:support-inner}
	\Sup{\cov \in \mc S_\m} ~ \Tr{T \cov} = \Inf{\substack{\lambda \ge 0 \\ \lambda \covsa^{-1} \succ T }}~ \lambda \big( \rho - (\m - \msa)^\top \covsa^{-1} (\m - \msa) \big) - \lambda \log \det (I - \covsa^\half T \covsa^\half /\lambda).
	\ee
	We now argue that the above equality also holds when $\m$ is chosen such that $(\m - \msa)^\top \covsa^{-1} (\m - \msa) = \rho$. In this case, $\mc S_\m$ collapses into a singleton $\{\covsa\}$, and the left-hand side supremum problem attains the value $\Tr{T\covsa}$. The right-hand side infimum problem becomes
	\[
	    \Inf{\substack{\lambda \ge 0 \\ \lambda \covsa^{-1} \succ T }}~ - \lambda \log \det (I - \covsa^\half T \covsa^\half /\lambda).
	\]
	One can show using the l'Hopital rule that
	\[
	    \lim_{\lambda \uparrow +\infty}~- \lambda \log \det (I - \covsa^\half T \covsa^\half /\lambda) = \Tr{T\covsa},
	\]
	which implies that the equality holds. Furthermore, when $T = 0$, the left-hand side of~\eqref{eq:support-inner} evaluates to 0, while the infimum problem on the right-hand side of~\eqref{eq:support-inner} also attains the optimal value of 0 asymptotically as $\lambda$ decreases to 0. This implies that~\eqref{eq:support-inner} holds for all $T \in \mathbb{S}^p$ and for any $\m$ satisfying $(\m - \msa)^\top \covsa^{-1} (\m - \msa) \le \rho$. %Furthermore, because the objective function of the infimum problem is continuous in $\lambda$, the constraint $\lambda \ge 0$ can be converted to the strict inequality $\lambda > 0$ without any loss of optimality.
	
	The above line of argument shows that problem~\eqref{eq:KL-subproblem} can now be expressed as the following maximin problem
	\[
	    \Sup{\m: (\m - \msa)^\top \covsa^{-1} (\m - \msa) \leq \rho} ~ \Inf{\substack{\lambda \ge 0 \\ \lambda \covsa^{-1} \succ T }}~ 2t^\top \m + \m^\top T \m + \lambda \big( \rho - (\m - \msa)^\top \covsa^{-1} (\m - \msa) \big) - \lambda \log \det (I - \covsa^\half T \covsa^\half /\lambda).
	\]
	For any $\lambda\ge 0$ such that $\lambda \covsa^{-1} \succ T$, the objective function is concave in $\m$. For any $\m$, the objective function is convex in $\lambda$. Furthermore, the feasible set of $\mu$ is convex and compact, and the feasible set of $\lambda$ is convex. As a consequence, we can apply Sion's minimax theorem~\cite{ref:sion1958minimax} to interchange the supremum and the infimum operators, and problem~\eqref{eq:KL-subproblem} is equivalent to
	\[
	\Inf{\substack{\lambda \ge 0 \\ \lambda \covsa^{-1} \succ T }}~\left\{
	\begin{array}{l}
	\lambda \rho - \lambda \log \det  (I - \covsa^\half T \covsa^\half /\lambda)  \\
	\hspace{2cm} + \Sup{\m:(\m - \msa)^\top \covsa^{-1} (\m - \msa) \leq \rho} ~ 2t^\top \m + \m^\top T \m  - \lambda (\m - \msa)^\top \covsa^{-1} (\m - \msa)
	\end{array}
	\right\}.
	\]
	For any $\lambda$ which is feasible for the outer problem, the inner supremum problem is a convex quadratic optimization problem because $ \lambda \covsa^{-1} \succ T$. Using a strong duality argument, the value of the inner supremum equals to the value of
	\begin{align*}
	    &\Inf{\nu \ge 0} ~ \left\{ \nu \rho - (\nu + \lambda) \msa^\top \covsa^{-1} \msa + \Sup{\m}~ \m^\top (T + (\lambda + \nu) \covsa^{-1}) \m + 2 (t + (\nu + \lambda) \covsa^{-1} \msa)^\top \m \right\}\\
    =& \Inf{\nu \ge 0} ~ \nu \rho - (\nu + \lambda) \msa^\top \covsa^{-1} \msa + (t + (\nu + \lambda) \covsa^{-1} \msa)^\top [(\lambda + \nu) \covsa^{-1} - T]^{-1} (t + (\lambda + \nu) \covsa^{-1} \msa ),
	\end{align*}
	where the equality follows from the fact that the unique optimal solution in the variable $\m$ is
	\be \label{eq:unique-mu}
	    \mu\opt(\lambda, \nu) = [T + (\lambda + \nu)\covsa^{-1}]^{-1}(t + (\lambda + \nu) \covsa^{-1}\m).
	\ee
	By combining two layers of infimum problem and exploiting a change of variables $\dualvar \leftarrow \lambda + \nu$, the support function of $\mbb V_{\mathds D}(\msa, \covsa)$ can now be written as
	\begin{align*}
	&\delta^*(2h, H) \\
	=& \left\{
	\begin{array}{cl}
	\inf & \dualvar (\rho - \msa^\top \covsa^{-1} \msa ) + (t + \dualvar \covsa^{-1} \msa)^\top [\dualvar \covsa^{-1} - T]^{-1} (t + \dualvar \covsa^{-1} \msa ) - \lambda \log \det (I - \covsa^\half T \covsa^\half /\lambda) \\
	\st & \lambda \ge 0, \; \lambda \covsa^{-1} \succ T, \; \dualvar - \lambda \ge 0.
	\end{array}
	\right.
	\end{align*}
	We now proceed to eliminate the multiplier $\lambda$ from the above problem. To this end, rewrite the above optimization problem as
	\[
	\begin{array}{cl}
	\inf &\dualvar (\rho - \msa^\top \covsa^{-1} \msa ) + (t + \dualvar \covsa^{-1} \msa)^\top [\dualvar \covsa^{-1} - T]^{-1} (t + \dualvar \covsa^{-1} \msa ) + f(\dualvar)\\
	\st & \dualvar \ge 0, \; \dualvar \covsa^{-1} \succ T,
	\end{array}
	\]
	where $f(\dualvar)$ is defined for every feasible value of $\dualvar$ as
	\be \label{eq:f-def}
	f(\dualvar) \Let \left\{
	\begin{array}{cl}
	\inf & - \lambda \log \det (I - \covsa^{\half} T \covsa^\half / \lambda) \\
	\st & \lambda \ge 0, \; \lambda \covsa^{-1} \succ T, \; \lambda \le \dualvar.
	\end{array}
	\right.
	\ee
	Let $g(\lambda)$ denote the objective function of the above optimization, which is independent of $\dualvar$. Let $\sigma_1, \ldots, \sigma_p$ be the eigenvalues of $\covsa^\half T \covsa^\half$, we can write the function $g$ directly using the eigenvalues $\sigma_1, \ldots, \sigma_p$ as
	\[
	g(\lambda) = -\lambda \sum_{i = 1}^p \log (1 - \sigma_i/\lambda).
	\]
	It is easy to verify by basic algebra manipulation that the gradient of $g$ satisfies
	\[
	\nabla g(\lambda) = \sum_{i=1}^p \left[ \log\left( \frac{\lambda}{\lambda - \sigma_i} \right) - \frac{\lambda}{\lambda - \sigma_i} \right] + p \leq 0,
	\]
	which implies that the value of $\lambda$ that solves~\eqref{eq:f-def} is $\lambda\opt(\dualvar) = \dualvar$, and thus $f(\dualvar) = - \dualvar \log \det (I - \covsa^{\half} T \covsa^\half / \dualvar)$. We thus conclude that $\delta^*(2h, H)$ is equal to the optimal value of
	\be \label{eq:KL-subproblem1}
	\begin{array}{cl}
	\inf &\dualvar (\rho - \msa^\top \covsa^{-1} \msa ) + (t + \dualvar \covsa^{-1} \msa)^\top [\dualvar \covsa^{-1} - T]^{-1} (t + \dualvar \covsa^{-1} \msa ) - \dualvar \log \det (I - \covsa^{\half} T \covsa^\half / \dualvar) \\
	\st & \dualvar \ge 0, \; \dualvar \covsa^{-1} \succ T.
	\end{array}
	\ee
	The objective function of the above problem involves a convex quadratic-over-linear term. In the last step, we notice that for any $\dualvar$ such that $\dualvar \covsa^{-1} \succ T$, we have the equivalence
	\[
	    (t + \dualvar \covsa^{-1} \msa)^\top [\dualvar \covsa^{-1} - T]^{-1} (t + \dualvar \covsa^{-1} \msa ) \leq z \Longleftrightarrow
	    \begin{bmatrix}
	        \dualvar \covsa^{-1} - T & t + \dualvar \covsa^{-1} \msa \\ (t + \dualvar \covsa^{-1} \msa)^\top & z
	    \end{bmatrix} \succeq 0
	\]
	by Schur complement. Thus, the quadratic-over-linear term in the objective function of~\eqref{eq:KL-subproblem1} can be replaced by the epigraph formulation using the auxiliary variable $z$. This completes the proof.
\end{proof}

\viet{below prop to appendix}



\begin{proof}[Proof of Proposition~\ref{prop:D-intersection-set}]
Leveraging \citep[Corollary~16.4.1]{ref:rockafellar1997convex} the support function of the set $\mbb V_{\psi, \rho_1}(\wh \m_1, \wh \Sigma_1) \cap V_{\psi, \rho_2}(\wh \m_2, \wh \Sigma_2) $ is equivalently to
\begin{equation*}
\begin{array}{llll}
  &\forall (h, H) \in \R^p \times \mbb S^p\\
  &\hspace{.5cm}\delta^*(h, H \, |\, \mbb V_{\psi, \rho_1 }(\wh \m_1, \wh \Sigma_1) \cap \mbb V_{\psi, \rho_2}(\wh \m_2, \wh \Sigma_2)) \\
  &\hspace{.5cm}=\! \left\{
  \begin{array}{cll}
  \inf \!
  &\delta^*(t_1, T_1 \, | \, \mbb V_{\psi, \rho_1 }(\wh \m_1, \wh \Sigma_1)) + \\
  & \hspace{.5cm} \delta^*(t_2, T_2 \, | \, \mbb V_{\psi, \rho_2 }(\wh \m_2, \wh \Sigma_2))\\
  \st & t_1, t_2 \in \R^p,~ T_1, T_2 \in \mbb S^p\\
  & t_1 + t_2 = t, ~ T_1 + T_2 = T,
  \end{array}\right.
  \end{array}
\end{equation*}
as long as Assumption~\ref{ass:non-empty-inter} holds.
\end{proof}


\begin{proposition}[Properties of set] \label{prop:D-set} The mean-second moment uncertainty set 
\[
\mbb V = \left\{
    \begin{array}{l}
     (\m, M) \in \R^p \times \PSD^p \text{ such that:}\\    
     M \succeq \m\m^\top,~
    \mathds D( (\m, M - \m \m^\top) \parallel (\m_\lambda, \cov_\lambda) ) \le r
     \end{array}\!\!
     \right\}
\]
is convex and compact. Moreover, its support function satisfies
\begin{equation*}
\begin{array}{lll}
    \forall (h, H) \in \R^p \times \Sym^p: \\[2ex]
    \delta^*(h, H | \mbb V)\! =\! \left\{\!
	\begin{array}{cl}
	\inf & \dualvar (\rho - \msa^\top \covsa^{-1} \msa ) + z -\\ &\hspace{.5cm} \dualvar \log \det (I - \covsa^{\half} T \covsa^\half / \dualvar) \\[2ex]
	\st & \dualvar \in \R_{+}, \; z \in \R_+ \\[2ex]
	& \dualvar \covsa^{-1} \succ T, \\[2ex]
	&\begin{bmatrix} \dualvar \covsa^{-1} - T & \frac{t}{2} + \dualvar \covsa^{-1} \msa \\ (\frac{t}{2} + \dualvar \covsa^{-1} \msa)^\top & z \end{bmatrix}\!\!
	\succeq\!  0.
	\end{array}
	\right.
	\end{array}
\end{equation*}
\end{proposition}

\viet{The intersection of the two uncertainty sets $\mbb V_{\psi, \rho_1}(\wh \mu_1, \wh \Sigma_1)$ and $\mbb V_{\psi, \rho_2}(\wh \mu_2, \wh \Sigma_2)$ is then defined as
\be\begin{array}{llll}
    &  \mbb V_{\psi} \Let \\ 
    &\hspace{0.5cm}\{(\m, M) \in \R^p \times \PSD^p: M - \m\m^\top \in \PSD^p, \\
    &\hspace{1cm} (\m, M-\m\m^\top) \in \mbb V_{\psi, \rho_k} ~ \forall k \in \{1, 2\}\}.
\end{array} 
\ee
}

\viet{For $k \in \{1, 2\}$, given a nominal mean vector $\msa_k \in \R^p$ and a nominal covariance matrix $\covsa_k \in \PSD^p$, we employ the divergence $\psi$ to construct an uncertainty set $\mbb U_{\psi, \rho_k}(\msa_k, \covsa_k)$ in the mean-covariance matrix space as
\be
\label{eq:U-def}
\begin{array}{llll}
    &\mbb U_{\psi, \rho_k}(\msa_k, \covsa_k) \Let \\
    &\left\{ (\m, \cov) \in \R^p \times \PSD^p: \psi\big ( (\m, \cov) \parallel (\msa_k, \covsa_k)\big) \le \rho_k \right\}.
    \end{array}
\ee
The uncertainty set $\mbb U_{\psi, \rho_k}(\msa_k, \covsa_k)$ thus contains all tuples of mean vectors and covariance matrices which lies in the neighborhood of radius $\rho_k$ from the nominal tuple $(\msa_k, \covsa_k)$, where the neighborhood is prescribed using the divergence $\psi$. 
The ambiguity set $\mbb B_\psi$ over all the probability distributions with first and second moments lying in the intersection of the uncertainty sets $\mbb U_{\psi, \rho_k}(\msa_k, \covsa_k)$, is defined as
\be \label{eq:ambiguity-def}
\begin{array}{llll}
    &\mbb B_{\psi} =\\
    &\hspace{0.5cm}\{
		\QQ \in \mc M(\R^{p}): \QQ \sim (\m, \cov), \; \\
	&\hspace{1cm} (\m, \cov) \in \mbb U_{\psi,k}(\msa_k, \covsa_k) ~~\forall k \in \{1, 2\}
	\}
\end{array}
\ee
}
%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Proofs of Section~\ref{sect:Gelbrich}}

% \begin{proof}[Proof of Proposition~\ref{prop:asymptotic-G}]
%  We first collect some basic results by central limit theorem
% as $n\uparrow \infty $
% \begin{equation*}
% \sqrt{n}(\wh{\mu}_{n} - m)\xrightarrow{dist.} g ,\quad \sqrt{n}(\wh{\Sigma}_{n} - S)%
% \xrightarrow{dist.}G,
% \end{equation*}%
% where $g \sim \mc N(0,S) $ and $G$ is  a $p$-by-$p$ random symmetric matrix with the upper triangle
% component $G_{jk}$ $( j \leq k )$ following a Gaussian distribution with mean 0 and the covariance coefficient between $G_{jk}$ and $G_{j'k'}$ is
% \begin{align*}
%     \mathrm{cov}(G_{jk},G_{j' k'})&=\EE_{\PP}[(\xi_j-\mu_j) (\xi_k-\mu_k) (\xi_{j'}-\mu_{j'}) (\xi_{k'}-\mu_{k'})] \\ &\hspace{2cm} -\EE_{\PP}[(\xi_j-\mu_j) (\xi_k-\mu_k)]\,\EE_{\PP}[(\xi_{j'}-\mu_{j'})(\xi_{k'}-\mu_{k'})].
% \end{align*}
%     Furthermore, $g$ and $G$ are jointly Gaussian distributed with the covariance between components $g_i$ and $G_{jk}$ being
% \[
% \mathrm{cov}(g_i,G_{jk}) = \EE_{\PP}[(\xi_i -\mu_i) (\xi_j-\mu_j) (\xi_k -\mu_k)].
% \]
% We follow the proof of Theorem 2.3 in \cite{ref:rippl2016limit}. Let $\Phi^{m,S}: \R^{p} \times \PSD^p \to \R_+$ be given by
% \[
% (\msa,\covsa) \mapsto \|m-\msa\|^2_2 + \Tr{S+\covsa -2(\covsa^{\frac{1}{2}}S\covsa^{\frac{1}{2}})^\frac{1}{2}}.
% \]
% By~\cite[Lemma 2.4]{ref:rippl2016limit}, $\Phi^{m, S}(m,S) = 0$ and $D\Phi^{m,S}(m, S) = 0$, where $D$ is the differential operator. Moreover, by \cite[Theorem~2.6]{ref:rippl2016limit}, the function $\Phi^{m,S}$ is twice Fr\'{e}chet differentiable at the point $(m,S)$ and thus we can apply the second-order delta method \cite[Theorem 20.8]{ref:vaart1998asymptotic}. This gives
% \[
% n \times \big(\Phi^{m,S}(\msa_n,\covsa_n) \big) \xrightarrow{dist.} \frac{1}{2} D^2 \Phi^{m,S}[(g,G),(g,G)],
% \]
% where $g,G$ are joint Gaussian distributed defined above. This completes the proof.
% \end{proof}

% \begin{proof}[Proof of Proposition~\ref{prop:f_G}]
%     Reminding that $\xi = (X, Y)$, we find
%     \begin{align*}
%         &\Sup{\QQ \in \mbb B_{\mathds G}(\Pnom)} \EE_\QQ[(\beta^\top X - Y)^2] = \Sup{\QQ \in \mbb B_{\mathds G}(\Pnom)} \EE_\QQ[(w^\top \xi)^2] =  \delta^*_{\mathds G}(0, ww^\top) \\
%         =& \left\{
% 	\begin{array}{cl}
% 	\inf & \dualvar \big(\rho - \|\msa\|_2^2 -  \Tr{\covsa} \big) + z + \Tr{Z} \\
% 	\st & \dualvar \in \R_+, \; z \in \R_+, \; Z \in \PSD^p \\
% 	& \begin{bmatrix} \dualvar I - ww^\top & \dualvar \covsa^\half \\ \dualvar \covsa^\half & Z \end{bmatrix} \succeq 0, \; \begin{bmatrix} \dualvar I - ww^\top & \dualvar \msa \\ \dualvar \msa^\top & z \end{bmatrix} \succeq 0
% 	\end{array}
% 	\right. \\
% 	=&\left\{
% 	    \begin{array}{cl}
% 	        \inf & \dualvar \big(\rho - \|\msa\|_2^2 -  \Tr{\covsa} \big) + \dualvar^2 \msa^\top (\dualvar I - ww^\top)^{-1} \msa + \dualvar^2 \Tr{\covsa (\dualvar I - ww^\top)^{-1}} \\
% 	        \st & \dualvar \ge \| w \|_2^2.
% 	    \end{array}
% 	\right.
%     \end{align*}
%     By applying~\cite[Fact~2.16.3]{ref:bernstein2009matrix}, we find
% 	\begin{align*}
% 	     (\dualvar \covsa^{-1} - ww^\top)^{-1} &= \dualvar^{-1} \covsa + \dualvar^{-2} \big( 1 - w^\top \covsa w/\dualvar \big)^{-1} \covsa w w^\top \covsa,
% 	\end{align*}
% 	and we can rewrite the support value $\delta^*_{\mathds G}(0, ww^\top) $ as
% 	\[
% 	    \delta^*_{\mathds G}(0, ww^\top) =
% 	    \left\{
% 	        \begin{array}{cl}
% 	            \inf & \dualvar \rho + \dualvar  w^\top (\covsa + \msa \msa^\top) w / (\dualvar - \| w \|_2^2 )\\
% 	            \st & \dualvar \ge \| w\|_2^2.
% 	        \end{array}
% 	    \right.
% 	\]
% 	One can verify through the first-order optimality condition that the optimal solution $\dualvar\opt$ is
% 	\[
% 	    \dualvar\opt = \| w \|_2 \Big( \| w \|_2 + \sqrt{\frac{w^\top (\covsa + \msa \msa^\top) w }{\rho}} \Big),
% 	\]
% 	and by replacing this value $\dualvar\opt$ into the objective function, we find
% 	\[
% 	    \delta^*_{\mathds G}(0, ww^\top) = \big( \sqrt{w^\top (\covsa + \msa\msa^\top) w} + \sqrt{\rho}\|w\|_2 \big)^2,
% 	\]
% 	which then completes the proof.
% \end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \section{Golden Section Search}
% \label{sect:golden}

% In this section, we describe in details the iterative procedure to solve the distributionally robust least square regression~\eqref{eq:KL-refor} with Kullback-Leibler type divergence ambiguity set. For any $\msa \in \R^p$, $\covsa \in \PD^p$ and $\rho \in \R_+$, consider the optimal value function $G: \R_{++} \to \R$,
% \[
% G(\dualvar) \Let \left\{
%     \begin{array}{cl}
% 	\inf & h_0 + z - \dualvar \log \det (I - \covsa^{\half} T \covsa^\half / \dualvar) \\
% 	\st & \beta \in \R^d,\; h_0 \in \R, \; t \in \R^p, \; H \in \PSD^p, \; w \in \R^p, \; W \in \PSD^p\\
% 	&\dualvar \covsa^{-1} \succ T, \; \begin{bmatrix} \dualvar \covsa^{-1} - T & t + \dualvar \covsa^{-1} \msa \\ (t + \dualvar \covsa^{-1} \msa)^\top & z \end{bmatrix} \succeq 0 \\
% 	& w = [\beta^\top, -1]^\top,\,
% 			\begin{bmatrix}
% 				W & t + \alpha w \\ t^\top + \alpha w^\top &h_0 - \alpha^2
% 			\end{bmatrix} \succeq 0, \,
% 			\begin{bmatrix}
% 				T - W & w \\ w^\top & 1
% 			\end{bmatrix} \succeq 0.
% 	\end{array}
% 	\right.
% \]
% and define
% \[
%     g: \R_{++} \ni \dualvar \mapsto G(\dualvar) + \dualvar (\rho - \msa^\top \covsa^{-1} \msa ) \in \R.
% \]

% \begin{algorithm}[h]
% 	\caption{Golden-section Search Algorithm}
% 	\label{alg:golden-section}
% 	\begin{algorithmic}
% 		\STATE {\bfseries Input:} Range $[a, b] \in \R$, tolerance $\vartheta \in \R_{++}$
% 		\STATE {\bfseries Initialization:} Set $r \leftarrow 0.618$, $\gamma_1 \leftarrow a$, $\gamma_4 \leftarrow b$.
% 		\WHILE{ $|\dualvar_4 - \dualvar_1| > \vartheta$}
% 		\STATE Set $\dualvar_2 \leftarrow r \dualvar_1 + (1-r) \dualvar_4$, $\dualvar_3 \leftarrow (1-r) \dualvar_1 + r \dualvar_4$
% 		%\STATE \textbf{for } $j \in \{1, 2, 3, 4\}$ \textbf{ do } compute $F(\beta_j)$ \textbf{ end for}
% 		\IF{$g(\dualvar_2) \le g(\dualvar_3)$}
% 		    \STATE Set $\dualvar_4 \leftarrow \dualvar_3$
% 		\ELSE
% 		    \STATE Set $\dualvar_1 \leftarrow \dualvar_2$
% 		\ENDIF
% 		\ENDWHILE
% 		\STATE Set $\dualvar\opt \leftarrow (\dualvar_1 + \dualvar_4)/2$
% 		\STATE Solve $G(\dualvar\opt)$ to get $\beta\opt$
% 		\STATE{\bfseries Output:} $\beta\opt$
% 	\end{algorithmic}
% \end{algorithm}

% \begin{subequations}
% \begin{align}
% f(\beta) &= \left\{
% 	\begin{array}{cl}
% 	\Max{\m, \cov \succ 0} & [\beta^\top, -1] (\cov+\m\m^\top)[\beta; -1] \\
% 	\st & \Tr{\cov \covsa^{-1}} - \log\det (\cov \covsa^{-1}) - p + (\m - \msa)^\top \covsa^{-1} (\m - \msa) \leq \rho
% 	\end{array}
% 	\right. \\
% 	&=\left\{
% 	\begin{array}{cl}
% 	\min &\dualvar \rho  + \frac{\dualvar (w^\top \msa)^2}{\dualvar - w^\top \covsa w } - \dualvar \log \big( 1 - w^\top \covsa w/\dualvar \big) \\
% 	\st & \dualvar > w^\top \covsa w, \; w = [\beta; -1].
% 	\end{array}
% 	\right.
% \end{align}
% \end{subequations}




\newpage
\end{document}
\newpage
\section{Old Appendix}
\subsection{Proof of Section~\ref{sect:dro}}
\begin{proof}[Proof of Theorem~\ref{thm:general}]
    We denote by $\mc M(\m, \cov)$ the space of all probability measures supported on $\R^p$ with fixed mean $\m \in \R^p$ and fixed covariance matrix $\cov \in \PD^p$. Using this notation, for any $\beta \in \R^d$,  we can rewrite the worst-case expected loss under the ambiguity set $\mbb B_{\psi}(\Pnom)$ as a two-layer optimization problem as follows
    \[
        \Sup{\QQ \in \mbb B_{\psi}(\Pnom)}~\EE_\QQ[ \ell(\beta, \xi)] = \Sup{(\m, \cov) \in \mbb U_{\psi}(\msa, \covsa)} \Sup{\QQ \in \mc M(\m, \cov)} \EE_{\QQ}[ \ell(\beta, \xi)].
    \]
    For any $(\m, \cov) \in \mbb U_{\psi}(\msa, \covsa)$, we can apply \cite[Lemma~A.1]{ref:zymler2013distributionally} to form the dual problem of the inner supremum problem as
	\[
	\Sup{\QQ \in \mc M(\m, \cov)} \EE_{\QQ} \left[ \ell(\beta, \xi) \right] \le \left\{
	\begin{array}{cll}
		\inf & h_0 + 2 h^\top \m + \Tr{H(\cov + \m \m^\top)} \\
		\st & h_0 \in \R, \; h \in \R^p, \; H \in \Sym^p \\
		& h_0 + 2h^\top \xi +  \xi^\top H \xi \geq \ell(\beta, \xi) &\forall \xi \in \R^p.
	\end{array}
	\right.
	\]
% 	Notice that the above inequality holds thanks to weak duality of moment problems~\cite{ref:isii1960extrema}. Moreover, strong duality holds whenever $\cov \in \PD^p$, in this case the above inequality becomes an equality. Because the loss function $\ell$ is nonnegative, we can add the semi-infinite constraint
% 	\[
% 	    h_0 + 2t^\top \xi +  \xi^\top T \xi \geq 0 \qquad \forall \xi \in \R^p
% 	\]
% 	into the dual problem without altering its optimal value. This semi-infinite constraint can be reformulated using the S-lemma~\cite{ref:polik2007survey} as a semidefinite constraint
% 	\[
% 	    \begin{bmatrix}
% 	        T & t \\ t^\top & h_0
% 	    \end{bmatrix}
% 	    \succeq 0.
% 	\]
	By letting $\mc T$ denote the effective feasible set of the above infimum problem, that is,
	\begin{align*}
		\mc T &\Let \left\{ h_0 \in \R, \; h \in \R^p, \; H \in \mathbb{S}^p: ~h_0 + 2h^\top \xi +  \xi^\top H \xi \geq \ell(\beta, \xi) ~~\forall \xi \in \R^p \right\}.
	\end{align*}
	Notice that the set $\mc T$ is dependent on $\beta$, however this dependency is omitted for clarity of exposition. For every $\beta \in \R^d$, we find
	\begin{subequations}
	\begin{align}
		\Sup{\QQ \in \mbb B_\psi (\Pnom)} \EE_{\QQ} \left[ \ell(\beta,\xi) \right] &= \Sup{(\m, \cov) \in \mbb U_{\psi}(\msa, \covsa)}~\Sup{\QQ \in \mc M(\m, \cov)}~\EE_{\QQ}[ \ell(\beta, \xi)] \label{eq:KL-refor-1} \\
				&\le \Sup{(\m, \cov) \in \mbb U_{\psi}(\msa, \covsa)}~\Inf{(h_0, h, H) \in \mc T}~h_0 + 2 h^\top \m + \Tr{H (\cov + \m \m^\top)} \label{eq:KL-refor0} \\
				&= \Sup{(\m, M) \in \mbb V_{\psi}(\msa, \covsa)}~\Inf{(h_0, h, H) \in \mc T}~h_0 + 2 h^\top \m + \Tr{HM} \label{eq:KL-refor1}\\
				&=\Inf{(h_0, h, H) \in \mc T}~\Sup{(\m, M) \in \mbb V_{\psi}(\msa, \covsa)}~h_0 + 2 h^\top \m + \Tr{HM} \label{eq:KL-refor2} \\
				&= \Inf{(h_0, h, H) \in \mc T}~\left\{ h_0 + \delta^*_\psi(2h, H) \right\}, \label{eq:KL-refor3}
	\end{align}
	\end{subequations}
	where in~\eqref{eq:KL-refor1} we reparametrize the optimization in the space of first- and second-moment statistics. Equality~\eqref{eq:KL-refor2} due to Sion's minimax theorem~\cite{ref:sion1958minimax} which holds because $\mbb V_{\psi}(\msa, \covsa)$ is convex and compact, $\mc T$ is convex, and the objective function of~\eqref{eq:KL-refor1} is concave-convex. In equality~\eqref{eq:KL-refor3}, we use the definition of the support function $\delta^*_\psi$ of the set $\mbb V_{\psi}(\msa, \covsa)$.
	
	In the next step, we show that the inequality~\eqref{eq:KL-refor0} can actually be converted to an equality, and~\eqref{eq:KL-refor3} is a tight upper bound to the worst-case expected loss. The argument follows from that of~	\cite[Theorem~3.41]{ref:nguyen2019adversarial} with necessary modifications as follows. Denote temporarily by $g$ the optimal value of the inner infimum in~\eqref{eq:KL-refor1}, that is,
	\[
		g(\m, M) = \Inf{(h_0, h, H) \in \mc T} \; h_0 + 2 t^\top \m + \Tr{T M}.
	\]	
	Because $g$ is the pointwise infimum of continuous functions, $g$ is upper semicontinuous~\cite[Lemma~2.41]{ref:aliprantis06hitchhiker}. Moreover, because $\mbb V_\psi(\msa, \covsa)$ is a compact set, the set of optimizers of the supremum problem~\eqref{eq:KL-refor1} is non-empty \cite[Theorem~2.43]{ref:aliprantis06hitchhiker} and \eqref{eq:KL-refor1} can be written with the maximum operator as
	\be \label{eq:auxi}
		\Max{(\m, M) \in \mbb V_\psi(\msa, \covsa)}~g(\m, M).
	\ee
	Denote by $(\m\opt, M\opt)$ the optimal solution of~\eqref{eq:auxi} and let $\cov\opt = M\opt - \m\opt (\m\opt)^\top$. If $\cov\opt \succ 0$, then~$(\m\opt, \cov\opt )$ is feasible for problem~\eqref{eq:KL-refor-1}, and ~\eqref{eq:KL-refor0} holds as an equality as a direct consequence of strong duality~\cite{ref:isii1960extrema}. It now remains to prove the claim for the case when $\cov\opt$ is singular. For any $\rho \in \R_{++}$, because $\psi$ is continuous, there exists a neighborhood $\mc B$ around $(\m\opt, \cov\opt)$ such that $\mc B \cap \mbb U_\psi(\msa, \covsa)$ is non-empty and that there exists $(\bar \m, \bar \cov) \in \mc B \cap \mbb U_\psi(\msa, \covsa)$ such that $\bar\cov \succ 0$. Denote $\bar M = \bar\cov + \bar \m \bar \m^\top$, it is trivial from the construction of $\mbb V_\psi(\msa, \covsa)$ that $(\bar \m, \bar M) \in \mbb V_\psi(\msa, \covsa)$. Consider the sequence $(\m_k, M_k)_{k \in \mbb N}$ defined as
	\[
	    \m_k = \frac{1}{k} \bar \m + \frac{k-1}{k} \m\opt, \quad M_k = \frac{1}{k} \bar M + \frac{k-1}{k} M\opt.
	\]
	Because~$\mbb V_\psi(\msa, \covsa)$ is convex, we have $(\m_k, M_k) \in \mbb V_\psi(\msa, \covsa)$ for any $k \in \mbb N$.
	Notice that the covariance matrix $\cov_k$ associated with $(\m_k, M_k)$ is positive semidefinite for any $k \in \mbb N$ because
	\begin{align*}
	   \cov_k = M_k - \m_k \m_k^\top &= \frac{1}{k} (\bar \Sigma + \bar \m \bar \m^\top) + \frac{k-1}{k} (\cov\opt + \m\opt (\m\opt)^\top) - \left(\frac{1}{k} \bar \m + \frac{k-1}{k} \m\opt \right) \left(\frac{1}{k} \bar \m + \frac{k-1}{k} \m\opt \right)^\top \\
	   &= \frac{1}{k} \bar \cov + \frac{k-1}{k} \cov\opt + \frac{k-1}{k^2} (\bar \m - \m\opt)(\bar \m - \m\opt)^\top \succ 0.
	\end{align*}
	The function $g$ is concave because it is the pointwise infimum of linear, and thus concave, function. Hence we find
	\[
		g(\m_k, M_k) \geq \frac{1}{k} g(\bar \m, \bar M) + \frac{k-1}{k} g(\m\opt, M\opt).
	\]
	As a result, we have
	\begin{align*}
		 g(\m\opt, M\opt) = \lim\limits_{k \to \infty} \frac{1}{k} g(\bar \m, \bar M) + \frac{k-1}{k} g(\m\opt, M\opt) &\leq \lim\limits_{k \to \infty} g(\m_k, M_k) \\
		 &= \lim\limits_{k \to \infty} \Sup{\QQ \in \mc M(\m_k, \cov_k)}~\EE_{\QQ}[ \ell(\beta, \xi)] \\
		 &\le \Sup{(\m,M) \in \mbb V_\psi(\msa, \covsa)} \Sup{\QQ \in \mc M(\m, \cov)} \EE_\QQ[\ell(\beta, \xi)],
	\end{align*}
	where the first inequality is from the concavity of $g$, the second equality is from the strong duality because $\cov_k \succ 0$~\cite{ref:isii1960extrema}, and the last inequality is from the definition of the supremum. This result implies that inequality~\eqref{eq:KL-refor0} is actually an equality.
	
	Exploiting~\eqref{eq:KL-refor3} and the definition of the set $\mc T$, we have
	\[
	    \Inf{\beta \in \R^d}~\Sup{\QQ \in \mbb B_\psi (\Pnom)} \EE_{\QQ} \left[ \ell(\beta,\xi) \right] =
	    \left\{
	        \begin{array}{ccl}
	            \Inf{\beta \in \R^d} & \inf & h_0 + \delta^*_\psi(2h, H) \\
	            &\st & h_0 \in \R, \; h \in \R^p, \; T \in \mathbb{S}^p \\
	            &&h_0 + 2t^\top \xi +  \xi^\top T \xi \geq \ell(\beta, \xi) ~~\forall \xi \in \R^p.
	        \end{array}
	    \right.
	\]
	Combining two infimum problems and utilizing the definition of the set $\mc S$ completes the proof.
	\end{proof}


\begin{proof}[Proof of Proposition~\ref{prop:semi-refor}]
    The proof heavily relies on the fact that for any $\beta \in \R^d$, we have $\beta^\top X - Y = w^\top \xi$ with $w \in \R^p$ being defined as $w \Let [\beta^\top, -1]^\top$.

	Consider now the case when $\ell$ is a $\|\cdot \|_2^2$ regression loss function. In this case, one can rewrite $\mc S$ as
    \begin{align*}
        \mc S &= \left\{ (\beta, h_0, h, H): \exists w \in \R^p,\, w = [\beta^\top, -1]^\top,\, h_0 + 2t^\top \xi +  \xi^\top T \xi \geq (w^\top \xi - \alpha)^2 \qquad \forall \xi \in \R^p \right\} \\
        &= \left\{ (\beta, h_0, h, H):  \exists w \in \R^p,\, w = [\beta^\top, -1]^\top,\, \begin{bmatrix} T - ww^\top & t + \alpha w \\ (t + \alpha w)^\top & h_0 - \alpha^2 \end{bmatrix} \succeq 0 \right\},
    \end{align*}
    where the second equality follows from the S-lemma. The semidefinite constraint involves a quadratic term $ww^\top$, which can be linearized by adding an auxiliary variables $W \in \PSD^p$ along with the constraint $W \succeq ww^\top$. Linearizing this constraint $W \succeq ww^\top$ by the Schur complement completes the reformulation of $\mc S$ for the $\|\cdot\|_2^2$ loss function.

	Consider when $\ell$ is a Huber loss function. In this case, we can write $\ell$ using the inf-convolution formulation as
	\begin{align*}
		\ell(\beta, x, y) &= \Inf{\theta_1 \in \R} \; \half \theta_1^2 + \delta |  \beta^\top x - y - \alpha - \theta_1 | \\
		&= \Inf{\theta_1 \in \R} \; \max\left\{ \half \theta_1^2 + \delta (\beta^\top x - y - \alpha - \theta_1), \; \half \theta_1^2 + \delta (- \beta^\top x + y + \alpha + \theta_1) \right\}.
	\end{align*}
	The semi-infinite constraint defining the feasible set $\mc S$ can be reformulated as
	\[
	\mc S = \left\{ (\beta, h_0, h, H):
	\begin{array}{ll}
	\exists (w, \theta_1) \in \R^p\times\R, \, w = [\beta^\top, -1]^\top \\
	h_0 + 2 t^\top \xi + \xi^\top T \xi \ge \half \theta_1^2 + \delta (w^\top \xi - \alpha - \theta_1) & \forall \xi \in \R^p \\
	h_0  + 2t^\top \xi + \xi^\top T \xi \geq \half \theta_1^2 + \delta (-w^\top \xi + \alpha + \theta_1) & \forall \xi \in \R^p
	\end{array}
	\right\},
	\]
	and hence $\mc S$ admits a conic representation
	\be \notag
	\mc S = \left\{ (\beta, h_0, h, H):
	\begin{array}{l}
		\exists (w, \theta_1) \in \R^p\times \R, \, w = [\beta^\top, -1]^\top \\
		\begin{bmatrix}
			T & t - \frac{\delta}{2} w \\ t^\top - \frac{\delta}{2} w^\top &h_0 - \half \theta_1^2 + \delta(\alpha + \theta_1)
		\end{bmatrix} \succeq 0,
		\\
		\begin{bmatrix}
			T & t + \frac{\delta}{2} w \\ t^\top + \frac{\delta}{2} w^\top &h_0 - \half \theta_1^2 - \delta(\alpha + \theta_1)
		\end{bmatrix} \succeq 0,
	\end{array}
	\right\}.
	\ee
	which is nonlinear because of the quadratic terms in $\theta_1$. 	In the last step, we replace the term $\half \theta_1^2$ by an auxiliary variable $\theta_2 \in \R_+$ with an additional constraint $\theta_2 \geq \half \theta_1^2$. Formulating this additional constraint as a semidefinite constraint completes the proof for the Huber loss.
	
	The other loss functions can be trivially re-expressed as a pointwise maximium of quadratic functions. The detailed proof is thus omitted.
\end{proof}


%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Proof of Section~\ref{sect:KL}}



The uncertainty set on the mean-covariance matrix space
\[ \mbb U_{\mathds D}(\msa, \covsa) = \{(\m, \cov) \in \R^p \times \PD^p: \mathds D\big((\m, \cov) \parallel (\msa, \covsa)\big) \leq \rho\}
\]
induces an uncertainty set on the first- and second-moment matrix space
\[ \mbb V_{\mathds D}(\msa, \covsa) \Let \{(\m, M) \in \R^p \times \PD^p: M - \m\m^\top \in \PD^p, \mathds D\big((\m, M-\m\m^\top) \parallel (\msa, \covsa) \big) \leq \rho\}.
\]


\begin{proof}[Proof of Proposition~\ref{prop:D-set}]
	For any $M \in \PD^p$ such that $M - \m\m^\top \in \PD^p$, we find
	\begin{align*}
		&\mathds D\big((\m, M-\m\m^\top) \parallel (\msa, \covsa)\big) \\
		=& (\m - \msa)^\top\covsa^{-1} (\m - \msa) +\Tr{(M - \m\m^\top) \covsa^{-1}} - \log\det ((M - \m\m^\top) \covsa^{-1}) - p \\
		%&=\msa^\top \covsa^{-1} \msa - 2 \msa^\top \covsa^{-1} \m+\Tr{M\covsa^{-1}}  + \log\det ((M - \m\m^\top)^{-1} \covsa) - p\\
		=& \msa^\top \covsa^{-1} \msa - 2 \msa^\top \covsa^{-1} \m+\Tr{M\covsa^{-1}} - \log\det (M\covsa^{-1}) - \log(1- \m^\top M^{-1} \m) - p ,
	\end{align*}
	where in the last expression, we have used the determinant formula~\citep[Fact~2.16.3]{ref:bernstein2009matrix} to rewrite
	\[
	    \det(M - \m\m^\top) = (1 - \m^\top M^{-1} \m) \det M.
	\]
	Because $M - \m\m^\top \in \PD^p$, one can show that $1 - \m^\top M^{-1} \m > 0$ by invoking the Schur complement, and as such, the logarithm term in the last expression is well-defined. Moreover, we can rewrite $\mbb V_{\mathds D}(\msa, \covsa)$ as
	\begin{align}
x`	&\mbb V_{\mathds D}(\msa, \covsa) \notag \\
	&= \left\{(\m, M) :
	\begin{array}{l}
	(\m, M) \in \R^p \times \PD^p,~M - \m\m^\top \in \PD^p,~\exists t \in \R_+: \\
	\msa^\top \covsa^{-1} \msa - 2 \msa^\top \covsa^{-1} \m+\Tr{M\covsa^{-1}} - \log\det (M\covsa^{-1}) - \log(1- t) - p \leq \rho \\
	\begin{bmatrix} M & \m \\ \m^\top & t \end{bmatrix} \succeq 0	\end{array}
	\right\}, \label{eq:D-refor}
	\end{align}
	which is a convex set. Notice that by Schur complement, the semidefinite constraint is equivalent to $t \ge \m^\top M^{-1} \m$.
	
	It now remains to show that $\mbb V_{\mathds D}(\msa, \covsa)$ is compact, or equivalently that $\mbb U_{\mathds D}(\msa, \covsa)$ is compact. If $\rho = 0$ then $\mbb U_{\mathds D}(\msa, \covsa)$ is a singleton $\{(\msa, \covsa%
)\}$ and the claim holds trivially. For the rest of the proof, we consider
when $\rho > 0$. Pick an arbitrary $(\m, \cov) \in \mbb U_{\mathds D}(\msa, \covsa)$,
it is obvious that $\cov$ should satisfy
\begin{equation*}
\Tr{\cov^\half \covsa^{-1} \cov^\half} - \log\det (\cov^\half \covsa^{-1} %
\cov^\half) - p \leq \rho,
\end{equation*}
which implies that $\cov$ is bounded. To see this, suppose that $\{\cov%
_{k}\}_{k \in \mbb N}$ is a sequence of positive definite matrices and $%
\{\sigma_k\}_{k \in \mbb N}$ is the corresponding sequence of the maximum
eigenvalues of $\{\cov_k^{\half} \covsa^{-1} \cov_k^{\half}\}_{k \in \mbb %
N}$. Because the function $\sigma \mapsto \sigma - \log \sigma - 1$ is
non-negative for every $\sigma > 0$, we find
\begin{equation*}
\Tr{\cov_k^\half \covsa^{-1} \cov_k^\half} - \log\det (\cov_k^\half \covsa^{-1} \cov_k^\half) - p \geq \sigma_k - \log \sigma_k -1.
\end{equation*}
If $\cov_k$ tends to infinity, then $\sigma_k$ tends to infinity. This implies
that $\cov$ should be bounded in the sense that $\cov \preceq \bar \sigma
I_p $ for some finite positive constant $\bar \sigma$. Using an analogous
argument, we can show that $\cov$ is lower bounded in the sense that $\cov %
\succeq \underline{\sigma} I_p$ for some finite positive constant $%
\underline{\sigma}$. Moreover, $\m$ is also bounded because $\m$
should satisfy $(\m-\msa)^{\top}\covsa^{-1}(\m-\msa) \leq \rho$. We now can
rewrite $\mbb U_{\mathds D}(\msa, \covsa)$ as
\begin{equation*}
\mbb U_{\mathds D}(\msa, \covsa) = \left\{ (\m, \cov) \in \R^p \times \PD^p: \begin{array}{l} (\m-\msa)^{\top}\covsa^{-1}(\m-\msa) \leq \rho,~\underline{\sigma} I_p \preceq \cov %
\preceq \bar{\sigma} I_p \\
\mathds D \big( (\m, \cov) \parallel (\msa, \covsa%
) \big) \leq \rho
\end{array}
\right\},
\end{equation*}
which implies that $\mbb U_{\mathds D}(\msa, \covsa)$ is a closed set because $%
\mathds D \big( (\cdot, \cdot) \parallel (\msa, \covsa) \big)$ is a
continuous function in $(\m, \cov)$ when $\cov$ ranges over $\underline{%
\sigma} I_p \preceq \cov \preceq \bar{\sigma} I_p$. This observation coupled
with the boundedness of $(\m, \cov)$ established the compactness of $\mbb U_{\mathds D}(\msa, \covsa)$ and $\mbb V_{\mathds D}(\msa, \covsa)$.

We now proceed to provide the reformulation for the support function of $\mbb V_{\mathds D}(\msa, \covsa)$. By substituting the expression of $\mathds D$, the support function becomes
\be \label{eq:KL-subproblem}
	\delta^*(2h, H) = \left\{
	\begin{array}{cl}
	\Sup{\m, \cov \succ 0} & 2t^\top \m + \Tr{T (\cov+\m\m^\top)} \\
	\st & \Tr{\cov \covsa^{-1}} - \log\det (\cov \covsa^{-1}) - p + (\m - \msa)^\top \covsa^{-1} (\m - \msa) \leq \rho
	\end{array}
	\right.
	\ee
	whose objective function is nonconvex whenever $T \succeq 0$. For any $\m \in \R^p$ such that $(\m - \msa)^\top \covsa^{-1} (\m - \msa) \le \rho$, denote the set $\mc S_\m$ as
	\[
	\mc S_\m \Let \left\{
	\cov \in \PD^p : \Tr{\cov \covsa^{-1}} - \log\det \cov\leq \rho_\mu
	\right\},
	\]
	where $\rho_\m \in \R$ is defined as $\rho_{\m} \Let \rho + p - \log\det\covsa - (\m - \msa)^\top \covsa^{-1}(\m - \msa)$. Using these auxiliary notations, problem~\eqref{eq:KL-subproblem} can be re-expressed as a nested program of the form
	\[
	\begin{array}{cl}
	\Sup{\m} & 2t^\top \m + \m^\top T \m + \Sup{\cov \in \mc S_\m} ~ \Tr{T \cov} \\
	\st & (\m - \msa)^\top \covsa^{-1} (\m - \msa) \leq \rho,
	\end{array}
	\]
	where we emphasize that the constraint on $\m$ is redundant, but it is added to ensure the feasibility of the inner supremum over $\cov$ for every feasible value of $\m$ of the outer problem. We now proceed to reformulate the supremum subproblem over $\cov$.
	
	Assume momentarily that $T \neq 0$ and that $\m$ satisfies $(\m - \msa)^\top \covsa^{-1} (\m - \msa) < \rho$. In this case, one can verify that $\covsa$ is the Slater point of the convex set $\mc S_\m$. Using a duality argument, we find
	\begin{align*}
	\Sup{\cov \in \mc S_\m} ~ \Tr{T \cov} =& \Sup{\cov \succ 0} \Inf{\lambda \ge 0} ~\Tr{T \cov} + \lambda \big(\rho_\m - \Tr{\covsa^{-1} \cov} + \log\det \cov \big) \notag\\
	=& \Inf{\lambda \ge 0} ~\left\{ \lambda \rho_\m + \Sup{\cov \succ 0}~ \big\{ \Tr{(T - \lambda \covsa^{-1})\cov}  + \lambda \log \det \cov \big\} \right\},  \notag
	\end{align*}
	where the last equality follows from strong duality~\citet[Proposition~5.3.1]{ref:bertsekas2009convex}. %\note{the first equality is wrong and should be removed: the inner inf is either the first term or minus infinity}
	If $T - \lambda \covsa^{-1} \not\prec 0$, then the inner supremum problem becomes unbounded. To see this, let $\sigma \in \R_+$ be the maximum eigenvalue of $T - \lambda \covsa^{-1}$ with the corresponding eigenvector $v$, then the sequence $(\Sigma_k)_{k\in \mbb N}$ with $\Sigma_k = I + k vv^\top$ attains the asymptotic maximum objective value of $+\infty$. If $T - \lambda \covsa^{-1} \prec 0$  then the inner supremum problem admits the unique optimal solution
	\be \label{eq:unique-cov}
	\cov\opt(\lambda) = \lambda (\lambda \covsa^{-1} - T)^{-1},
	\ee
	which is obtained by solving the first-order optimality condition. By placing this optimal solution into the objective function and arranging terms, we have
	\be \label{eq:support-inner}
	\Sup{\cov \in \mc S_\m} ~ \Tr{T \cov} = \Inf{\substack{\lambda \ge 0 \\ \lambda \covsa^{-1} \succ T }}~ \lambda \big( \rho - (\m - \msa)^\top \covsa^{-1} (\m - \msa) \big) - \lambda \log \det (I - \covsa^\half T \covsa^\half /\lambda).
	\ee
	We now argue that the above equality also holds when $\m$ is chosen such that $(\m - \msa)^\top \covsa^{-1} (\m - \msa) = \rho$. In this case, $\mc S_\m$ collapses into a singleton $\{\covsa\}$, and the left-hand side supremum problem attains the value $\Tr{T\covsa}$. The right-hand side infimum problem becomes
	\[
	    \Inf{\substack{\lambda \ge 0 \\ \lambda \covsa^{-1} \succ T }}~ - \lambda \log \det (I - \covsa^\half T \covsa^\half /\lambda).
	\]
	One can show using the l'Hopital rule that
	\[
	    \lim_{\lambda \uparrow +\infty}~- \lambda \log \det (I - \covsa^\half T \covsa^\half /\lambda) = \Tr{T\covsa},
	\]
	which implies that the equality holds. Furthermore, when $T = 0$, the left-hand side of~\eqref{eq:support-inner} evaluates to 0, while the infimum problem on the right-hand side of~\eqref{eq:support-inner} also attains the optimal value of 0 asymptotically as $\lambda$ decreases to 0. This implies that~\eqref{eq:support-inner} holds for all $T \in \mathbb{S}^p$ and for any $\m$ satisfying $(\m - \msa)^\top \covsa^{-1} (\m - \msa) \le \rho$. %Furthermore, because the objective function of the infimum problem is continuous in $\lambda$, the constraint $\lambda \ge 0$ can be converted to the strict inequality $\lambda > 0$ without any loss of optimality.
	
	The above line of argument shows that problem~\eqref{eq:KL-subproblem} can now be expressed as the following maximin problem
	\[
	    \Sup{\m: (\m - \msa)^\top \covsa^{-1} (\m - \msa) \leq \rho} ~ \Inf{\substack{\lambda \ge 0 \\ \lambda \covsa^{-1} \succ T }}~ 2t^\top \m + \m^\top T \m + \lambda \big( \rho - (\m - \msa)^\top \covsa^{-1} (\m - \msa) \big) - \lambda \log \det (I - \covsa^\half T \covsa^\half /\lambda).
	\]
	For any $\lambda\ge 0$ such that $\lambda \covsa^{-1} \succ T$, the objective function is concave in $\m$. For any $\m$, the objective function is convex in $\lambda$. Furthermore, the feasible set of $\mu$ is convex and compact, and the feasible set of $\lambda$ is convex. As a consequence, we can apply Sion's minimax theorem~\cite{ref:sion1958minimax} to interchange the supremum and the infimum operators, and problem~\eqref{eq:KL-subproblem} is equivalent to
	\[
	\Inf{\substack{\lambda \ge 0 \\ \lambda \covsa^{-1} \succ T }}~\left\{
	\begin{array}{l}
	\lambda \rho - \lambda \log \det  (I - \covsa^\half T \covsa^\half /\lambda)  \\
	\hspace{2cm} + \Sup{\m:(\m - \msa)^\top \covsa^{-1} (\m - \msa) \leq \rho} ~ 2t^\top \m + \m^\top T \m  - \lambda (\m - \msa)^\top \covsa^{-1} (\m - \msa)
	\end{array}
	\right\}.
	\]
	For any $\lambda$ which is feasible for the outer problem, the inner supremum problem is a convex quadratic optimization problem because $ \lambda \covsa^{-1} \succ T$. Using a strong duality argument, the value of the inner supremum equals to the value of
	\begin{align*}
	    &\Inf{\nu \ge 0} ~ \left\{ \nu \rho - (\nu + \lambda) \msa^\top \covsa^{-1} \msa + \Sup{\m}~ \m^\top (T + (\lambda + \nu) \covsa^{-1}) \m + 2 (t + (\nu + \lambda) \covsa^{-1} \msa)^\top \m \right\}\\
    =& \Inf{\nu \ge 0} ~ \nu \rho - (\nu + \lambda) \msa^\top \covsa^{-1} \msa + (t + (\nu + \lambda) \covsa^{-1} \msa)^\top [(\lambda + \nu) \covsa^{-1} - T]^{-1} (t + (\lambda + \nu) \covsa^{-1} \msa ),
	\end{align*}
	where the equality follows from the fact that the unique optimal solution in the variable $\m$ is
	\be \label{eq:unique-mu}
	    \mu\opt(\lambda, \nu) = [T + (\lambda + \nu)\covsa^{-1}]^{-1}(t + (\lambda + \nu) \covsa^{-1}\m).
	\ee
	By combining two layers of infimum problem and exploiting a change of variables $\dualvar \leftarrow \lambda + \nu$, the support function of $\mbb V_{\mathds D}(\msa, \covsa)$ can now be written as
	\begin{align*}
	&\delta^*(2h, H) \\
	=& \left\{
	\begin{array}{cl}
	\inf & \dualvar (\rho - \msa^\top \covsa^{-1} \msa ) + (t + \dualvar \covsa^{-1} \msa)^\top [\dualvar \covsa^{-1} - T]^{-1} (t + \dualvar \covsa^{-1} \msa ) - \lambda \log \det (I - \covsa^\half T \covsa^\half /\lambda) \\
	\st & \lambda \ge 0, \; \lambda \covsa^{-1} \succ T, \; \dualvar - \lambda \ge 0.
	\end{array}
	\right.
	\end{align*}
	We now proceed to eliminate the multiplier $\lambda$ from the above problem. To this end, rewrite the above optimization problem as
	\[
	\begin{array}{cl}
	\inf &\dualvar (\rho - \msa^\top \covsa^{-1} \msa ) + (t + \dualvar \covsa^{-1} \msa)^\top [\dualvar \covsa^{-1} - T]^{-1} (t + \dualvar \covsa^{-1} \msa ) + f(\dualvar)\\
	\st & \dualvar \ge 0, \; \dualvar \covsa^{-1} \succ T,
	\end{array}
	\]
	where $f(\dualvar)$ is defined for every feasible value of $\dualvar$ as
	\be \label{eq:f-def}
	f(\dualvar) \Let \left\{
	\begin{array}{cl}
	\inf & - \lambda \log \det (I - \covsa^{\half} T \covsa^\half / \lambda) \\
	\st & \lambda \ge 0, \; \lambda \covsa^{-1} \succ T, \; \lambda \le \dualvar.
	\end{array}
	\right.
	\ee
	Let $g(\lambda)$ denote the objective function of the above optimization, which is independent of $\dualvar$. Let $\sigma_1, \ldots, \sigma_p$ be the eigenvalues of $\covsa^\half T \covsa^\half$, we can write the function $g$ directly using the eigenvalues $\sigma_1, \ldots, \sigma_p$ as
	\[
	g(\lambda) = -\lambda \sum_{i = 1}^p \log (1 - \sigma_i/\lambda).
	\]
	It is easy to verify by basic algebra manipulation that the gradient of $g$ satisfies
	\[
	\nabla g(\lambda) = \sum_{i=1}^p \left[ \log\left( \frac{\lambda}{\lambda - \sigma_i} \right) - \frac{\lambda}{\lambda - \sigma_i} \right] + p \leq 0,
	\]
	which implies that the value of $\lambda$ that solves~\eqref{eq:f-def} is $\lambda\opt(\dualvar) = \dualvar$, and thus $f(\dualvar) = - \dualvar \log \det (I - \covsa^{\half} T \covsa^\half / \dualvar)$. We thus conclude that $\delta^*(2h, H)$ is equal to the optimal value of
	\be \label{eq:KL-subproblem1}
	\begin{array}{cl}
	\inf &\dualvar (\rho - \msa^\top \covsa^{-1} \msa ) + (t + \dualvar \covsa^{-1} \msa)^\top [\dualvar \covsa^{-1} - T]^{-1} (t + \dualvar \covsa^{-1} \msa ) - \dualvar \log \det (I - \covsa^{\half} T \covsa^\half / \dualvar) \\
	\st & \dualvar \ge 0, \; \dualvar \covsa^{-1} \succ T.
	\end{array}
	\ee
	The objective function of the above problem involves a convex quadratic-over-linear term. In the last step, we notice that for any $\dualvar$ such that $\dualvar \covsa^{-1} \succ T$, we have the equivalence
	\[
	    (t + \dualvar \covsa^{-1} \msa)^\top [\dualvar \covsa^{-1} - T]^{-1} (t + \dualvar \covsa^{-1} \msa ) \leq z \Longleftrightarrow
	    \begin{bmatrix}
	        \dualvar \covsa^{-1} - T & t + \dualvar \covsa^{-1} \msa \\ (t + \dualvar \covsa^{-1} \msa)^\top & z
	    \end{bmatrix} \succeq 0
	\]
	by Schur complement. Thus, the quadratic-over-linear term in the objective function of~\eqref{eq:KL-subproblem1} can be replaced by the epigraph formulation using the auxiliary variable $z$. This completes the proof.
\end{proof}

\viet{below prop to appendix}



\begin{proof}[Proof of Proposition~\ref{prop:D-intersection-set}]
Leveraging \citep[Corollary~16.4.1]{ref:rockafellar1997convex} the support function of the set $\mbb V_{\psi, \rho_1}(\wh \m_1, \wh \Sigma_1) \cap V_{\psi, \rho_2}(\wh \m_2, \wh \Sigma_2) $ is equivalently to
\begin{equation*}
\begin{array}{llll}
  &\forall (h, H) \in \R^p \times \mbb S^p\\
  &\hspace{.5cm}\delta^*(h, H \, |\, \mbb V_{\psi, \rho_1 }(\wh \m_1, \wh \Sigma_1) \cap \mbb V_{\psi, \rho_2}(\wh \m_2, \wh \Sigma_2)) \\
  &\hspace{.5cm}=\! \left\{
  \begin{array}{cll}
  \inf \!
  &\delta^*(t_1, T_1 \, | \, \mbb V_{\psi, \rho_1 }(\wh \m_1, \wh \Sigma_1)) + \\
  & \hspace{.5cm} \delta^*(t_2, T_2 \, | \, \mbb V_{\psi, \rho_2 }(\wh \m_2, \wh \Sigma_2))\\
  \st & t_1, t_2 \in \R^p,~ T_1, T_2 \in \mbb S^p\\
  & t_1 + t_2 = t, ~ T_1 + T_2 = T,
  \end{array}\right.
  \end{array}
\end{equation*}
as long as Assumption~\ref{ass:non-empty-inter} holds.
\end{proof}


\begin{proposition}[Properties of set] \label{prop:D-set} The mean-second moment uncertainty set 
\[
\mbb V = \left\{
    \begin{array}{l}
     (\m, M) \in \R^p \times \PSD^p \text{ such that:}\\    
     M \succeq \m\m^\top,~
    \mathds D( (\m, M - \m \m^\top) \parallel (\m_\lambda, \cov_\lambda) ) \le r
     \end{array}\!\!
     \right\}
\]
is convex and compact. Moreover, its support function satisfies
\begin{equation*}
\begin{array}{lll}
    \forall (h, H) \in \R^p \times \Sym^p: \\[2ex]
    \delta^*(h, H | \mbb V)\! =\! \left\{\!
	\begin{array}{cl}
	\inf & \dualvar (\rho - \msa^\top \covsa^{-1} \msa ) + z -\\ &\hspace{.5cm} \dualvar \log \det (I - \covsa^{\half} T \covsa^\half / \dualvar) \\[2ex]
	\st & \dualvar \in \R_{+}, \; z \in \R_+ \\[2ex]
	& \dualvar \covsa^{-1} \succ T, \\[2ex]
	&\begin{bmatrix} \dualvar \covsa^{-1} - T & \frac{t}{2} + \dualvar \covsa^{-1} \msa \\ (\frac{t}{2} + \dualvar \covsa^{-1} \msa)^\top & z \end{bmatrix}\!\!
	\succeq\!  0.
	\end{array}
	\right.
	\end{array}
\end{equation*}
\end{proposition}

\viet{The intersection of the two uncertainty sets $\mbb V_{\psi, \rho_1}(\wh \mu_1, \wh \Sigma_1)$ and $\mbb V_{\psi, \rho_2}(\wh \mu_2, \wh \Sigma_2)$ is then defined as
\be\begin{array}{llll}
    &  \mbb V_{\psi} \Let \\ 
    &\hspace{0.5cm}\{(\m, M) \in \R^p \times \PSD^p: M - \m\m^\top \in \PSD^p, \\
    &\hspace{1cm} (\m, M-\m\m^\top) \in \mbb V_{\psi, \rho_k} ~ \forall k \in \{1, 2\}\}.
\end{array} 
\ee
}

\viet{For $k \in \{1, 2\}$, given a nominal mean vector $\msa_k \in \R^p$ and a nominal covariance matrix $\covsa_k \in \PSD^p$, we employ the divergence $\psi$ to construct an uncertainty set $\mbb U_{\psi, \rho_k}(\msa_k, \covsa_k)$ in the mean-covariance matrix space as
\be
\label{eq:U-def}
\begin{array}{llll}
    &\mbb U_{\psi, \rho_k}(\msa_k, \covsa_k) \Let \\
    &\left\{ (\m, \cov) \in \R^p \times \PSD^p: \psi\big ( (\m, \cov) \parallel (\msa_k, \covsa_k)\big) \le \rho_k \right\}.
    \end{array}
\ee
The uncertainty set $\mbb U_{\psi, \rho_k}(\msa_k, \covsa_k)$ thus contains all tuples of mean vectors and covariance matrices which lies in the neighborhood of radius $\rho_k$ from the nominal tuple $(\msa_k, \covsa_k)$, where the neighborhood is prescribed using the divergence $\psi$. 
The ambiguity set $\mbb B_\psi$ over all the probability distributions with first and second moments lying in the intersection of the uncertainty sets $\mbb U_{\psi, \rho_k}(\msa_k, \covsa_k)$, is defined as
\be \label{eq:ambiguity-def}
\begin{array}{llll}
    &\mbb B_{\psi} =\\
    &\hspace{0.5cm}\{
		\QQ \in \mc M(\R^{p}): \QQ \sim (\m, \cov), \; \\
	&\hspace{1cm} (\m, \cov) \in \mbb U_{\psi,k}(\msa_k, \covsa_k) ~~\forall k \in \{1, 2\}
	\}
\end{array}
\ee
}
%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Proofs of Section~\ref{sect:Gelbrich}}

% \begin{proof}[Proof of Proposition~\ref{prop:asymptotic-G}]
%  We first collect some basic results by central limit theorem
% as $n\uparrow \infty $
% \begin{equation*}
% \sqrt{n}(\wh{\mu}_{n} - m)\xrightarrow{dist.} g ,\quad \sqrt{n}(\wh{\Sigma}_{n} - S)%
% \xrightarrow{dist.}G,
% \end{equation*}%
% where $g \sim \mc N(0,S) $ and $G$ is  a $p$-by-$p$ random symmetric matrix with the upper triangle
% component $G_{jk}$ $( j \leq k )$ following a Gaussian distribution with mean 0 and the covariance coefficient between $G_{jk}$ and $G_{j'k'}$ is
% \begin{align*}
%     \mathrm{cov}(G_{jk},G_{j' k'})&=\EE_{\PP}[(\xi_j-\mu_j) (\xi_k-\mu_k) (\xi_{j'}-\mu_{j'}) (\xi_{k'}-\mu_{k'})] \\ &\hspace{2cm} -\EE_{\PP}[(\xi_j-\mu_j) (\xi_k-\mu_k)]\,\EE_{\PP}[(\xi_{j'}-\mu_{j'})(\xi_{k'}-\mu_{k'})].
% \end{align*}
%     Furthermore, $g$ and $G$ are jointly Gaussian distributed with the covariance between components $g_i$ and $G_{jk}$ being
% \[
% \mathrm{cov}(g_i,G_{jk}) = \EE_{\PP}[(\xi_i -\mu_i) (\xi_j-\mu_j) (\xi_k -\mu_k)].
% \]
% We follow the proof of Theorem 2.3 in \cite{ref:rippl2016limit}. Let $\Phi^{m,S}: \R^{p} \times \PSD^p \to \R_+$ be given by
% \[
% (\msa,\covsa) \mapsto \|m-\msa\|^2_2 + \Tr{S+\covsa -2(\covsa^{\frac{1}{2}}S\covsa^{\frac{1}{2}})^\frac{1}{2}}.
% \]
% By~\cite[Lemma 2.4]{ref:rippl2016limit}, $\Phi^{m, S}(m,S) = 0$ and $D\Phi^{m,S}(m, S) = 0$, where $D$ is the differential operator. Moreover, by \cite[Theorem~2.6]{ref:rippl2016limit}, the function $\Phi^{m,S}$ is twice Fr\'{e}chet differentiable at the point $(m,S)$ and thus we can apply the second-order delta method \cite[Theorem 20.8]{ref:vaart1998asymptotic}. This gives
% \[
% n \times \big(\Phi^{m,S}(\msa_n,\covsa_n) \big) \xrightarrow{dist.} \frac{1}{2} D^2 \Phi^{m,S}[(g,G),(g,G)],
% \]
% where $g,G$ are joint Gaussian distributed defined above. This completes the proof.
% \end{proof}

% \begin{proof}[Proof of Proposition~\ref{prop:f_G}]
%     Reminding that $\xi = (X, Y)$, we find
%     \begin{align*}
%         &\Sup{\QQ \in \mbb B_{\mathds G}(\Pnom)} \EE_\QQ[(\beta^\top X - Y)^2] = \Sup{\QQ \in \mbb B_{\mathds G}(\Pnom)} \EE_\QQ[(w^\top \xi)^2] =  \delta^*_{\mathds G}(0, ww^\top) \\
%         =& \left\{
% 	\begin{array}{cl}
% 	\inf & \dualvar \big(\rho - \|\msa\|_2^2 -  \Tr{\covsa} \big) + z + \Tr{Z} \\
% 	\st & \dualvar \in \R_+, \; z \in \R_+, \; Z \in \PSD^p \\
% 	& \begin{bmatrix} \dualvar I - ww^\top & \dualvar \covsa^\half \\ \dualvar \covsa^\half & Z \end{bmatrix} \succeq 0, \; \begin{bmatrix} \dualvar I - ww^\top & \dualvar \msa \\ \dualvar \msa^\top & z \end{bmatrix} \succeq 0
% 	\end{array}
% 	\right. \\
% 	=&\left\{
% 	    \begin{array}{cl}
% 	        \inf & \dualvar \big(\rho - \|\msa\|_2^2 -  \Tr{\covsa} \big) + \dualvar^2 \msa^\top (\dualvar I - ww^\top)^{-1} \msa + \dualvar^2 \Tr{\covsa (\dualvar I - ww^\top)^{-1}} \\
% 	        \st & \dualvar \ge \| w \|_2^2.
% 	    \end{array}
% 	\right.
%     \end{align*}
%     By applying~\cite[Fact~2.16.3]{ref:bernstein2009matrix}, we find
% 	\begin{align*}
% 	     (\dualvar \covsa^{-1} - ww^\top)^{-1} &= \dualvar^{-1} \covsa + \dualvar^{-2} \big( 1 - w^\top \covsa w/\dualvar \big)^{-1} \covsa w w^\top \covsa,
% 	\end{align*}
% 	and we can rewrite the support value $\delta^*_{\mathds G}(0, ww^\top) $ as
% 	\[
% 	    \delta^*_{\mathds G}(0, ww^\top) =
% 	    \left\{
% 	        \begin{array}{cl}
% 	            \inf & \dualvar \rho + \dualvar  w^\top (\covsa + \msa \msa^\top) w / (\dualvar - \| w \|_2^2 )\\
% 	            \st & \dualvar \ge \| w\|_2^2.
% 	        \end{array}
% 	    \right.
% 	\]
% 	One can verify through the first-order optimality condition that the optimal solution $\dualvar\opt$ is
% 	\[
% 	    \dualvar\opt = \| w \|_2 \Big( \| w \|_2 + \sqrt{\frac{w^\top (\covsa + \msa \msa^\top) w }{\rho}} \Big),
% 	\]
% 	and by replacing this value $\dualvar\opt$ into the objective function, we find
% 	\[
% 	    \delta^*_{\mathds G}(0, ww^\top) = \big( \sqrt{w^\top (\covsa + \msa\msa^\top) w} + \sqrt{\rho}\|w\|_2 \big)^2,
% 	\]
% 	which then completes the proof.
% \end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \section{Golden Section Search}
% \label{sect:golden}

% In this section, we describe in details the iterative procedure to solve the distributionally robust least square regression~\eqref{eq:KL-refor} with Kullback-Leibler type divergence ambiguity set. For any $\msa \in \R^p$, $\covsa \in \PD^p$ and $\rho \in \R_+$, consider the optimal value function $G: \R_{++} \to \R$,
% \[
% G(\dualvar) \Let \left\{
%     \begin{array}{cl}
% 	\inf & h_0 + z - \dualvar \log \det (I - \covsa^{\half} T \covsa^\half / \dualvar) \\
% 	\st & \beta \in \R^d,\; h_0 \in \R, \; t \in \R^p, \; H \in \PSD^p, \; w \in \R^p, \; W \in \PSD^p\\
% 	&\dualvar \covsa^{-1} \succ T, \; \begin{bmatrix} \dualvar \covsa^{-1} - T & t + \dualvar \covsa^{-1} \msa \\ (t + \dualvar \covsa^{-1} \msa)^\top & z \end{bmatrix} \succeq 0 \\
% 	& w = [\beta^\top, -1]^\top,\,
% 			\begin{bmatrix}
% 				W & t + \alpha w \\ t^\top + \alpha w^\top &h_0 - \alpha^2
% 			\end{bmatrix} \succeq 0, \,
% 			\begin{bmatrix}
% 				T - W & w \\ w^\top & 1
% 			\end{bmatrix} \succeq 0.
% 	\end{array}
% 	\right.
% \]
% and define
% \[
%     g: \R_{++} \ni \dualvar \mapsto G(\dualvar) + \dualvar (\rho - \msa^\top \covsa^{-1} \msa ) \in \R.
% \]

% \begin{algorithm}[h]
% 	\caption{Golden-section Search Algorithm}
% 	\label{alg:golden-section}
% 	\begin{algorithmic}
% 		\STATE {\bfseries Input:} Range $[a, b] \in \R$, tolerance $\vartheta \in \R_{++}$
% 		\STATE {\bfseries Initialization:} Set $r \leftarrow 0.618$, $\gamma_1 \leftarrow a$, $\gamma_4 \leftarrow b$.
% 		\WHILE{ $|\dualvar_4 - \dualvar_1| > \vartheta$}
% 		\STATE Set $\dualvar_2 \leftarrow r \dualvar_1 + (1-r) \dualvar_4$, $\dualvar_3 \leftarrow (1-r) \dualvar_1 + r \dualvar_4$
% 		%\STATE \textbf{for } $j \in \{1, 2, 3, 4\}$ \textbf{ do } compute $F(\beta_j)$ \textbf{ end for}
% 		\IF{$g(\dualvar_2) \le g(\dualvar_3)$}
% 		    \STATE Set $\dualvar_4 \leftarrow \dualvar_3$
% 		\ELSE
% 		    \STATE Set $\dualvar_1 \leftarrow \dualvar_2$
% 		\ENDIF
% 		\ENDWHILE
% 		\STATE Set $\dualvar\opt \leftarrow (\dualvar_1 + \dualvar_4)/2$
% 		\STATE Solve $G(\dualvar\opt)$ to get $\beta\opt$
% 		\STATE{\bfseries Output:} $\beta\opt$
% 	\end{algorithmic}
% \end{algorithm}

% \begin{subequations}
% \begin{align}
% f(\beta) &= \left\{
% 	\begin{array}{cl}
% 	\Max{\m, \cov \succ 0} & [\beta^\top, -1] (\cov+\m\m^\top)[\beta; -1] \\
% 	\st & \Tr{\cov \covsa^{-1}} - \log\det (\cov \covsa^{-1}) - p + (\m - \msa)^\top \covsa^{-1} (\m - \msa) \leq \rho
% 	\end{array}
% 	\right. \\
% 	&=\left\{
% 	\begin{array}{cl}
% 	\min &\dualvar \rho  + \frac{\dualvar (w^\top \msa)^2}{\dualvar - w^\top \covsa w } - \dualvar \log \big( 1 - w^\top \covsa w/\dualvar \big) \\
% 	\st & \dualvar > w^\top \covsa w, \; w = [\beta; -1].
% 	\end{array}
% 	\right.
% \end{align}
% \end{subequations}




\newpage
\section{Random stuff}

For the empirical problem, we should solve the Tikhonov regularizer
\[
     \Min{\beta \in \R^d}~\frac{1}{n} \ell(\beta, \wh x_i, \wh y_i) + \eta \| \beta\|_2^2.
\]

Equivalently, we should also regularize $\covsa$ by adding 
\[
    \covsa \leftarrow \covsa + \begin{bmatrix}
        \eta I_d & 0 \\ 0 & 0
    \end{bmatrix}
\]
notice that we only add in the x-component.

\begin{example}[Robust regression] \label{ex:2}
The Huber loss with target $\alpha \in \R$ and parameter $\delta \in \R_{++}$ is
		\be \label{eq:loss-huber}
		\ell(\beta, x, y) \! =\!
		\begin{cases}
		\half \left( \beta^\top x\! - \! y \! -\!  \alpha \right)^2 & \text{if}~|\beta^\top x \! -\!  y\! - \! \alpha| \!\leq\! \delta , \\
		\delta \left( | \beta^\top x \! - \! y \! -\!  \alpha | \!-\! \half \delta \right) &\text{otherwise.}
		\end{cases}
		\ee
The Huber loss is considered to be more robust against outliers than squared 2-norm loss, since the Huber estimators behave more like the median for small value of $\delta$.
\end{example}

\begin{example}[Pinball regression]  \label{ex:3}
The pinball loss with target $\alpha \in \R$ and parameter $\delta \in [0, 1]$ is
\be \label{eq:loss-pinball}
\ell(\beta, x, y) \! = \! \max \!\left\{ \! -\delta (\beta^\top x \! -\! y \! -\! \alpha), (1-\delta) (\beta^\top x \! - \! y \! - \!\alpha) \! \right\}\! .
\ee
If $\delta = \half$, then the pinball regression coincides with the $\| \cdot \|_1$ regression, or also known as median regression.
\end{example}

\begin{example}[Support vector regression] \label{ex:4}
The $\delta$-insensitive loss with target $\alpha \in \R$ is
 \be \label{eq:loss-svr}
 \ell(\beta, x, y) = \max \left\{0, | \beta^\top x - y - \alpha| - \delta \right\}.
 \ee
\end{example}

Let $\mbb V$ be the set of mean vector and second moment matrices associated with $\mbb U$
\[
    \mbb V = \!\left\{\!\!
     \begin{array}{l}
     (\m, M) \in \R^p \times \PSD^p \text{ such that:}\\    
     M \succeq \m\m^\top,~
     (\m, M - \m \m^\top) \in \mbb U
     \end{array}\!\!
     \right\}\!
\]


\begin{assumption}[Non-empty interior]
\label{ass:non-empty-inter}
We assume that $\mbb V$ has a non-empty interior.
\end{assumption}




\begin{theorem}[Finite dimensional reformulation]
\label{thm:general}
	Suppose that $\psi$ is a continuous divergence, and that $\mbb V$ is a convex, compact set. Let the finite dimensional set $\mc S$ be defined as
\begin{subequations}
    \be \label{eq:S-def}
        \mc S \Let \left\{ 
        \begin{array}{l}
        \beta \in \R^d,\; h_0 \in \R, \; h \in \R^p, \; H \in \PSD^p :\\
        h_0 + 2h^\top \xi +  \xi^\top H \xi \geq \ell(\beta, \xi) \quad \forall \xi \in \R^p 
        \end{array} 
        \right\},
    \ee
    then the distributionally robust regression problem~\eqref{eq:dro} is equivalent to the following finite dimensional optimization problem
	\be \label{eq:refor}
	\begin{array}{cl}
	\inf & h_0 + \delta^*(2h, H \, | \, \mbb V) \\
	\st & \beta \in \R^d,\; h_0 \in \R, \; h \in \R^p, \; H \in \PSD^p\\
	& (\beta, h_0, h, H) \in \mc S.
	\end{array}
	\ee
	\end{subequations}
\end{theorem}

It now suffices to provide the reformulation of the feasible set prescribed through the last constraint. The next proposition shows that $\mc S$ is conic representable for the loss functions in Examples~\ref{ex:1}-\ref{ex:4}.

\begin{proposition}[Reformulation of semi-infinite constraint]
\label{prop:semi-refor}
For common loss functions in Examples~\ref{ex:1}-\ref{ex:4}, the set $\mc S$ is representable using semidefinite constraints as follows.
\begin{enumerate}[leftmargin = 5mm]
    \item  \textbf{Least square regression.} If $\ell$ is the $\|\cdot\|_2^2$-loss function~\eqref{eq:loss-l2}, then $\mc S$ can be expressed as
		\be \notag
	\mc S \!= \!\!
		\left\{ \!\!\!	
		\begin{array}{lll}
		&(\beta, h_0, h, H):\\[2ex]
		&\begin{array}{lll}
		\exists (w, W) \in \R^p \times \PSD^p, \, w = [\beta^\top, -1]^\top, \\[2ex]
			\begin{bmatrix}
				W & t + \alpha w \\ t^\top + \alpha w^\top &h_0 - \alpha^2
			\end{bmatrix} \succeq 0, \\[2ex]
			\begin{bmatrix}
				H - W & w \\ w^\top & 1
			\end{bmatrix} \succeq 0.
			\end{array}
				\end{array}
	\!	\right\}\!.
		\ee
	\item \textbf{Robust regression.} If $\ell$ is the Huber loss function~\eqref{eq:loss-huber}, then $\mc S$ can be expressed as
	\be \notag
		\mc S \!=\! \left\{\!\hspace{-.5cm}\begin{array}{llll}
		&(\beta, h_0, h, H):\\[2ex]
		&\begin{array}{ll}
			\exists (w, \theta_1, \theta_2) \in \R^p \times \R \times \R_+, \, w = [\beta^\top, -1]^\top, \\[2ex]
			\begin{bmatrix}
				H & h - \frac{\delta}{2} w \\ h^\top - \frac{\delta}{2} w^\top & h_0 - \theta_2 + \delta(\alpha + \theta_1)
			\end{bmatrix} \succeq 0, \\[2ex]
			
			\begin{bmatrix}
				\theta_2 & \theta_1 \\ \theta_1 & 2
			\end{bmatrix} \succeq 0,
			\\[2ex]
			\begin{bmatrix}
				H & h + \frac{\delta}{2} w \\ h^\top + \frac{\delta}{2} w^\top & h_0 - \theta_2 - \delta(\alpha + \theta_1)
			\end{bmatrix} \succeq 0
		\end{array}
		\end{array}\!\!\!\!\!\!
		\right\}\!\!.
		\ee
	\item \textbf{Quantile regression.} If $\ell$ is the pinball loss function~\eqref{eq:loss-pinball}, then $\mc S$ can be expressed as
	\be \notag
	\mc S = \left\{ \begin{array}{llll}
	&(\beta, h_0, h, H):\\[2ex]
	&\begin{array}{l}
	    \exists w \in \R^p,\, w = [\beta^\top, -1]^\top\\[2ex]
		\begin{bmatrix}
			H & h - \frac{\delta}{2} w \\ h^\top - \frac{\delta}{2} w^\top & h_0 - \delta \alpha
		\end{bmatrix} \succeq 0, \\[2ex]
		\begin{bmatrix}
			H & h + \frac{(1-\delta)}{2} w \\ h^\top + \frac{(1-\delta)}{2} w^\top & h_0 + (1-\delta)\alpha
		\end{bmatrix} \succeq 0
	\end{array}
	\end{array}
	\right\}.
	\ee
	\item \textbf{Support vector regression.} If $\ell$ is the support vector loss function~\eqref{eq:loss-svr}, then $\mc S$ can be expressed as
		\be \notag
		\mc S = \left\{ \begin{array}{llll}
		    &(\beta, h_0, h, H):\\[2ex]
		&\begin{array}{l}
		    \exists w \in \R^p,\; w = [\beta, -1]^\top\\[2ex]
			\begin{bmatrix}
				H & h - \frac{1}{2} w \\ h^\top - \frac{1}{2} w^\top & h_0 + \alpha + \delta
			\end{bmatrix} \succeq 0,\\[2ex]
			\begin{bmatrix}
				H & h + \frac{1}{2} w \\ h^\top + \frac{1}{2} w^\top & h_0 - \alpha + \delta
			\end{bmatrix} \succeq 0
		\end{array}
		\end{array}
		\right\}.
		\ee
\end{enumerate}
\end{proposition}
\end{document}