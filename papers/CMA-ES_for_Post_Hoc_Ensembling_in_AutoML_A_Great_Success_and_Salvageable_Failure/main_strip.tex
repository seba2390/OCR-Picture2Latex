\documentclass[11pt]{article}
\pdfoutput=1
\usepackage{etex}


\newcommand{\hide}[1]{}

\usepackage[dvipsnames]{xcolor}
\usepackage{amssymb}


\colorlet{lpcolor}{orange}
\definecolor{lccollor}{rgb}{0,0,128}

\colorlet{todocolor}{RedOrange!100}
\colorlet{changedcolor}{ForestGreen}
\newcommand{\nbc}[3]{
		{\colorbox{#3}{\bfseries\sffamily\scriptsize\textcolor{white}{#1}}}
		{\textcolor{#3}{\sf\small$\blacktriangleright$\textit{#2}$\blacktriangleleft$}}
}

\newcommand{\lp}[1]{\nbc{LP}{#1}{lpcolor}}
\newcommand{\todo}[1]{\nbc{TODO}{#1}{todocolor}}
\newcommand{\changed}[1]{\nbc{CHANGED}{#1}{changedcolor}}
\newcommand{\lc}[1]{\nbc{LC}{#1}{lccollor}}


\renewcommand{\changed}[1]{{\color{changedcolor}#1}}
\renewcommand{\changed}[1]{{#1}}


\usepackage{algorithm}
\usepackage{amsmath}
\usepackage[noend]{algpseudocode}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\makeatletter
\newcommand*{\inlineequation}[2][]{%
  \begingroup
    \refstepcounter{equation}%
    \ifx\\#1\\%
    \else
      \label{#1}%
    \fi
    \relpenalty=10000 %
    \binoppenalty=10000 %
    \ensuremath{%
      #2%
    }%
    ~\@eqnnum
  \endgroup
}
\makeatother

\usepackage{microtype} %
\usepackage{booktabs}  %
\usepackage{url}  %
\usepackage{multirow}

\usepackage{amsmath}
\usepackage{amsthm}

\usepackage{graphicx}


\usepackage[final]{automl}


\usepackage{natbib}
\bibliographystyle{apalike}



\title{CMA-ES for Post Hoc Ensembling in AutoML:\\ A Great Success and Salvageable Failure
}


\author[1]{\nameemail{Lennart Purucker}{lennart.purucker@uni-siegen.de}}
\author[1]{\nameemail{Joeran Beel}{joeran.beel@uni-siegen.de}}
\affil[1]{University of Siegen}






\hypersetup{%
  pdfauthor={}, %
  pdftitle={},
  pdfsubject={},
  pdfkeywords={}
}

\begin{document}

\maketitle

\begin{abstract}
Many state-of-the-art automated machine learning (AutoML) systems use greedy ensemble selection (GES) by Caruana et al. (2004) to ensemble models found during model selection post hoc. 
Thereby, boosting predictive performance and likely following Auto-Sklearn 1's insight that alternatives, like stacking or gradient-free numerical optimization, overfit. 
Overfitting in Auto-Sklearn 1 is much more likely than in other AutoML systems because it uses only low-quality validation data for post hoc ensembling.
Therefore, we were motivated to analyze whether Auto-Sklearn 1's insight holds true for systems with higher-quality validation data. 
Consequently, we compared the performance of covariance matrix adaptation evolution strategy (CMA-ES), state-of-the-art gradient-free numerical optimization, to GES on the 71 classification datasets from the AutoML benchmark for AutoGluon. 
We found that Auto-Sklearn's insight depends on the chosen metric. 
For the metric ROC AUC, CMA-ES overfits drastically and is outperformed by GES -- statistically significantly for multi-class classification. 
For the metric balanced accuracy, CMA-ES does not overfit and outperforms GES significantly. 
Motivated by the successful application of CMA-ES for balanced accuracy, we explored methods to stop CMA-ES from overfitting for ROC AUC. 
We propose a method to normalize the weights produced by CMA-ES, inspired by GES, that avoids overfitting for CMA-ES and makes CMA-ES perform better than or similar to GES for ROC AUC. 
\end{abstract}

\changed{\section{Introduction}}
Auto-Sklearn \citep{ask1} was the first automated machine learning (AutoML) system to discover that building an ensemble of models found during model selection is possible in an efficient manner and superior in predictive performance to the single best model. 
Afterwards, several other AutoML systems also build an ensemble \emph{post hoc}:
AutoGluon \citep{ag1}, Auto-Pytorch \citep{apt1, apt2}, MLJAR \citep{mljar}, and H2O AutoML \citep{H2OAutoML20} all implemented \emph{post hoc ensembling}. 

Besides H2O AutoML, all of these systems implemented \emph{greedy ensemble selection} (GES) \citep{caruana2004, caruana2006}, a greedy search for a weight vector to aggregate the predictions of base models. 
In AutoML systems, GES is trained using the base models' predictions on the \emph{validation data}, which are computed while evaluating a base model during model selection.
The frequent usage of GES likely follows Auto-Sklearn's reported insight that alternatives like \emph{stacking} \citep{stakcingWolpert} or gradient-free numerical optimization overfit and are more costly than GES. 

Auto-Sklearn 1, by default, only has limited validation data for post hoc ensembling, that is, a $33\%$ hold-out split of the training data. 
We deem this to be low-quality validation data because, depending on the dataset, $33\%$ are not enough instances to avoid overfitting while training GES. %
Hence, we were motivated to analyze if Auto-Sklearn's insight also holds true for an AutoML system with higher-quality validation data, \emph{e.g.}, AutoGluon with $n$-repeated $k$-fold cross-validation. %
Moreover, we were motivated to focus on gradient-free numerical optimization instead of stacking.
Stacking is generally well-known in ensembling for machine learning and is used by H2O AutoML for post hoc ensembling. 
In contrast, gradient-free numerical optimization has not been used so far. 
Thus, we compare the performance of GES to \emph{covariance matrix adaptation evolution strategy} (CMA-ES) \citep{cma/Hansen14, cma/HansenTutorial2016}, state-of-the-art gradient-free numerical optimization \changed{\citep{{DBLP:conf/gecco/HansenARFP10, szynkiewicz2018comparative, li2020evolution}}}.
\changed{We chose CMA-ES due to its widespread usage in numerical optimization \citep{li2020evolution}.
Moreover, CMA-ES's update is efficient and therefore enables fast training in post hoc ensembling; similar to GES's training.
Furthermore, the function evaluation in post hoc ensembling, i.e., calculating the score of aggregated predictions, takes seconds \citep{ask1}. Thus, we disregarded Bayesian optimization, which is appropriate for tasks with expensive function evaluation such as hyperparameter optimization \citep{DBLP:journals/swevo/LanTRE22}.}

In this study, we aim to boost the predictive performance as much as possible with post hoc ensembling. 
Note that GES selects a small ensemble, while methods like gradient-free numerical optimization or stacking produce an ensemble that includes all base models.
Thus, the inference time and size of the final model are larger for the latter two than for GES. 

Our first contribution is an application of CMA-ES for AutoGluon on the 71 classification datasets from the AutoML Benchmark \citep{automl_benchmark_2022}.
Thereby, we show that Auto-Sklearn's insight w.r.t.{} overfitting of gradient-free numerical optimization depends on the chosen metric. 
We contradict the insight for the metric \emph{balanced accuracy} by showing that CMA-ES statistically significantly outperforms GES.
And we confirm the insight for the metric \emph{ROC AUC} by showing that GES outperforms CMA-ES due to overfitting.  

As a follow-up, our second contribution is a method to avoid overfitting for CMA-ES. 
Motivated by the successful application of CMA-ES for balanced accuracy, we explored methods to stop CMA-ES from overfitting to \emph{salvage} CMA-ES for ROC AUC.  
We identified the chosen method to normalize the ensemble's prediction probabilities as the key to avoiding overfitting. 
With this knowledge, we propose a novel normalization method, inspired by GES's implicit constraints during optimization, that makes CMA-ES perform as well as GES and avoids overfitting for ROC AUC. 
Interestingly, our normalization method also enables us to keep the size of the ensemble small. 


Our code and data are publicly available: see Appendix \ref{apdx/public_links} for details. 
\section{Related Work}
Besides Auto-Sklearn 1's \citep{ask1} statement related to post hoc ensembling, only H2O AutoML names theoretical guarantees \citep{vanderLaanPolleyHubbard+2007} as the reason for using stacking, but does not comment on GES.
In general, details about post hoc ensembling in publications about AutoML systems were only a short comment without experiments or a reference to Auto-Sklearn 1 \citep{ask1, apt1, ag1, H2OAutoML20}. 
We are only aware of the work by \cite{purucker2022assembledopenml}, which proposed a first benchmark and framework for post hoc ensembling. 
The results in their Appendix also showed that GES can outperform stacking. 
To the best of our knowledge, no other work on post hoc ensembling for AutoML exists. 

CMA-ES was previously applied to machine learning problems like hyperparameter optimization \citep{DBLP:conf/aaai/NomuraWAOO21, DBLP:journals/corr/LoshchilovH16} or feature weighting \citep{DBLP:conf/siu/TasciGU18}\footnote{To the best of our knowledge, this work is not available in English. We read a machine-translated version.}
.
However, we found no work that used CMA-ES to directly optimizes the weights of an ensemble. 
Likewise, we have found no work that applies normalization to the solutions produced by CMA-ES nor comparable machine learning methods that apply normalization in this way to combat overfitting.  
\section{Application of CMA-ES for Post Hoc Ensembling}
\label{sec/cma-es_app}
In our application of CMA-ES for post hoc ensembling, we search for an optimal weight vector $W = (w_1, ..., w_{m})$ to aggregate pool $P$ of $m$ base models that minimizes a user-defined loss $L(P, W)$. Thereby, $L$ aggregates the predictions of models in $P$ by taking the $W$-weighted arithmetic mean.  

Hence, we employ CMA-ES, as implemented in pycma \citep{hansen2019pycma}, with default values to find $W$ by minimizing $L$. 
Following GES's first iteration, we set the initial solution $x_0$ to be the weight vector representing the single best model, that is, the weight for the single best model is one while all other models are weighted zero. 
The initial standard deviation is $0.2$ following the intuition that a good weight vector might be close to the initial solution and that the granularity of weights can be small, e.g., between 0 and 1, like in GES. 



\subsection{Experiments: CMA-ES \emph{vs.} GES}
\label{sec/cma-es-compare}
We compared CMA-ES to GES w.r.t.{} ROC AUC following the AutoML Benchmark \citep{automl_benchmark_2022}. 
ROC AUC requires prediction probabilities and is independent of a decision threshold that would transform prediction probabilities into labels.  We use macro average one-vs-rest ROC AUC for multiclass. 
We complemented the comparison by also evaluating w.r.t.{} balanced accuracy, which requires predicted labels and, thus, depends on a decision threshold.

For a threshold-dependent metric, the prediction of CMA-ES is, in our application, the class with the highest value after aggregating the prediction probabilities with the $W$-weighted mean. 
For a threshold-independent metric, we transform the aggregated probabilities for each instance using the softmax function, \emph{i.e.}, we treat the aggregated probabilities of each class as decision functions and take their softmax.
Otherwise, the aggregated probabilities would not represent prediction probabilities, as $W$ can have negative or positive values of any granularity. 

To compare the ensembling methods, we obtained base models and their validation data with AutoGluon \citep{ag1} for each fold of the 71 classification datasets from the AutoML benchmark (AMLB) \citep{automl_benchmark_2022} -- for both metrics.
Then, per fold, we trained the ensemble methods on the validation data, i.e., search for $W$, and scored them on validation and test. The final validation/test score of a method for a dataset is the average over the $10$ folds.

Following the AMLB, we ran AutoGluon for 4 hours with 8 cores (AMD EPYC 7452 CPU) and 32 GB of memory. We increased the memory for several datasets to 64 or 128 GB to avoid that insufficient memory made it impossible to produce multiple base models. In the end, AutoGluon produced between 2 and 24 base models, see Appendix \ref{apdx/data_overview} for details per dataset and metric.

We used the same resources and hardware to train and evaluate the ensemble methods.
However, instead of training ensemble methods for $4$ hours, we follow Auto-Sklearn's default and stop training GES after $50$ iterations. 
This results in $m*50$ total evaluations of $L$ by GES. 
Therefore, we terminated CMA-ES after $m*50$ evaluations of $L$. 

We included the single best base model (SingleBest) in the comparison as a baseline. 
To evaluate the statistical difference between the methods, we perform a Friedman test with a Nemenyi post hoc test ($\alpha = 0.05$), following the AMLB.
\changed{See Appendix \ref{apdx/cd_plots} for more details on the statistical tests.}

\subsection{Results: CMA-ES \emph{vs.} GES}
\label{sec/cma-es-compare-results}
We split the results for binary and multi-class classification in all our evaluations following the AutoML Benchmark \citep{automl_benchmark_2022}.
Figure \ref{fig/cd_plot} shows the mean rank and results of the statistical test with critical difference (CD) plots. \changed{The Friedman tests were significant in all our experiments.}
We observe that CMA-ES is statistically significantly better than GES for balanced accuracy but fails to perform similarly well for ROC AUC. 

To analyze the impact of overfitting on this outcome, we inspect the change of the mean rank of CMA-ES when switching from validation to test data for both metrics, see Table \ref{table/rank_change}. 
\changed{A detailed overview for all methods can be found in Appendix \ref{apdx/mean_rank_change}.}
While the single best is always ranked last, GES overtakes CMA-ES when switching from validation to test data for ROC AUC. 
Notably, CMA-ES has a mean rank of almost $1$ for validation data in 3 out of 4 cases.

On validation data, GES is only competitive for multi-class ROC AUC, where it has a mean rank of $1.6$.  
Nevertheless, GES has a larger distance to the single best on validation for balanced accuracy than it has for test data with a mean rank of ${\sim}2$ against the single best's ${\sim}3$.

In summary, we conclude that Auto-Sklearn's insight w.r.t. overfitting does not generalize to an AutoML system with higher-quality validation data, \emph{i.e.}, AutoGluon, for \emph{balanced accuracy}. 
In contrast, \emph{the insight holds for ROC AUC}. 
Furthermore, we observe that CMA-ES is able to achieve peak performance for ROC AUC on validation data. 

\begin{figure}
    \begin{subfigure}[t]{0.49\linewidth}
        \centering 
        \includegraphics[width=\linewidth, trim={0cm 2cm 0cm 1.8cm},clip]{f14.pdf}
        \caption{Balanced Accuracy - Binary (41 Datasets)}
        \label{fig/cd_plot/bacc_b}
    \end{subfigure}
    \begin{subfigure}[t]{0.49\linewidth}
        \centering
        \includegraphics[width=\linewidth, trim={0cm 2cm 0cm 1.8cm},clip]{f9.pdf}
        \caption{Balanced Accuracy - Multi-class (30 Datasets)}
        \label{fig/cd_plot/bacc_m}
    \end{subfigure}
    \begin{subfigure}[t]{0.49\linewidth}
        \centering
        \includegraphics[width=\linewidth, trim={0cm 2cm 0cm 1.5cm},clip]{f12.pdf}
        \caption{ROC AUC - Binary (41 \changed{Datasets})}
        \label{fig/cd_plot/roc_b}
    \end{subfigure}
    \begin{subfigure}[t]{0.49\linewidth}
        \centering
        \includegraphics[width=\linewidth, trim={0cm 2cm 0cm 1.5cm},clip]{f7.pdf}
        \caption{ROC AUC - Multi-class (30 \changed{Datasets})}
    \label{fig/cd_plot/roc_m}
    \end{subfigure}

  
    \caption{\textbf{CD Plots Comparing GES and CMA-ES:}
    {\normalfont The mean rank (lower is better) of a method is its line's position on the axis. Methods connected by a bar are not significantly different.}
    }
    \label{fig/cd_plot}
\end{figure}

\begin{table}[b]
\caption{Mean rank change from validation to test data for CMA-ES compared to GES and SingleBest.}\label{table/rank_change}
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}llccc@{}}
\toprule
Metric & Task Type & \multicolumn{1}{l}{Mean $Rank_{Validation}$} & \multicolumn{1}{l}{Mean $Rank_{Test}$} & \multicolumn{1}{l}{Absolute Rank (Val $\rightarrow$ Test) } \\ \midrule
Balanced Accuracy & Binary      & 1.00 & 1.12 & 1.0 $\rightarrow$ 1.0 \\
Balanced Accuracy & Multi-class & 1.03 & 1.25 & 1.0 $\rightarrow$ 1.0 \\
ROC AUC           & Binary      & 1.02 & 1.83 & 1.0 $\rightarrow$ 2.0 \\
ROC AUC           & Multi-class & 1.42 & 2.12 & 1.0 $\rightarrow$ 2.0 \\ \bottomrule
\end{tabular}%
}
\end{table}

\section{Normalization to Combat Overfitting}
The results we just presented motivated us to salvage CMA-ES for ROC AUC.
\changed{Due to its good performance for ROC AUC and its wide adaptation by AutoML systems, we decided to analyze GES to determine how to avoid overfitting.
As a result, we found two properties that inspired our approach to salvage CMA-ES for ROC AUC.
This} section describes why and how we use normalization to combat overfitting for a threshold-independent metric like ROC AUC. 
Since our approach is inspired by GES, we start with preliminaries regarding GES and its properties. 
 
\subsection{Preliminaries}
\label{sec/ges_prem}
Greedy ensemble selection with replacement \citep{caruana2004, caruana2006} performs an iterative greedy search to build a list of (repeated) base models, the ensemble $E$, that minimizes a user-defined loss function. 
In each iteration, the base model minimizing the loss, when added to $E$, is \emph{selected} to be part of $E$. 
To produce predictions and evaluate any $E$, the (repeated) predictions of all base models in $E$ are aggregated with the arithmetic mean.
Taking the arithmetic mean of $E$ weights base models that exit multiple times higher.
Hence, given $E$, we can compute a weight vector. 
Assuming we run GES for $N$ iterations\footnote{
We always denote $N$ as the number of the iteration the final $E$ was found in. Depending on the implementation of GES, the final $E$ does not need to be from the final iteration. 
}, then $|E| = N$ and we compute the weight vector using:

\begin{equation}
    W^{pDisc} = \left[\frac{countIn(p_i, E)}{N} \; \bigg| \; p_i \in P \right].
    \label{eq/ges_w}
\end{equation}

While analysing GES, we found two constraints of the weight vector $W^{pDisc}$ that we believe to be essential for its performance. 
That is, $W^{pDisc}$ is \emph{pseudo-discrete} and \emph{sparse}. 
Both properties are only \emph{implicitly} respected by GES and were, to the best of our knowledge, never formally defined.  

\paragraph{Pseudo-Discrete}
We call $W^{pDisc}$ pseudo-discrete because one can transform every weight vector produced by GES into a discrete count of how often a base model has been selected.
This can be done by multiplying $W^{pDisc}$ with $N$,
reversing Equation \ref{eq/ges_w}.
In fact, every weight vector produced by GES is in the set $\mathcal{G} = \{W' \; | \; W' \in H(N) \text{ and } \sum_{i=1}^{m} w_i = 1\}$ with $H(N)$ the $m$-fold Cartesian product of $\{0, 1/N, 2/N, . . . , 1\}$:

\begin{equation}
    H(N) = \{0, 1/N, 2/N, \dots , 1\} \times \cdots \times \{0, 1/N, 2/N, \dots, 1\}.
    \label{eq/h_cat}
\end{equation}

In other words, every weight $w_i \in W^{pDisc}$ can be expressed as a positive fraction with denominator $N$, and the weight vector sums to $1$. 
This follows from GES iteratively building a list of base models $E$ and calculating the final weight vector with Equation \ref{eq/ges_w}. 

We would like to remark that this formulation of GES is very similar to mallows' model average (MMA) \citep{ hansen2007least, hansen2008least, DBLP:journals/jmlr/LeC22} and that GES might share MMA's asymptotic guarantees for regression if $L$ is the squared error \citep{DBLP:journals/jmlr/LeC22}.  

\paragraph{Sparse}
$W^{pDisc}$ is sparse, that is, a weight vector where many models are assigned zero weight -- as intended for an \emph{ensemble selection} approach \citep{ens/es_primer}.
To the best of our knowledge, a guarantee for sparseness was never formally introduced or proven for (greedy) ensemble selection, cf. \citep{caruana2004, caruana2006, ens/es_primer}. 
Here, we shortly provide an argument for why it is likely that GES produces a sparse weight vector:

GES only adds new base models to $E$ if they reduce the loss. 
Hence, it would require at least $m$ iterations where adding a new base model would reduce the loss more than adding an existing base model again (increasing its weight).
As a result, for appropriate values for $m$ and $N$, it is unlikely that enough iterations happened such that each model was able to reduce the loss once.
Auto-Sklearn, for example, uses $m = 50$ and $N = 50$ by default. 
Moreover, once $E$ becomes large, the changes to the aggregated prediction that are induced by adding a new base model are minimal. 
Thus, it also becomes less likely that the changes result in a different loss.
Additonally, the larger $E$ is, the more likely GES has reached a (local) optimum, which can not be improved upon by adding new models. 
In short, the iterative greedy approach to add models to $E$ likely makes $W^{pDisc}$ sparse. 

\subsection{Motivation}
Since all solutions produced by GES are pseudo-discrete and (likely) sparse, and since GES does not seem to overfit, we hypothesized that both properties might help to avoid overfitting. 

Note, the properties can be seen as constraints. 
They constrain the weight vector to be sparse, sum to $1$, and contain only values such that $0 \leq w_i \leq 1$. 
In contrast, our application of CMA-ES uses no such constraints. 
By default, CMA-ES produces a continuous and dense vector which does not need to sum to 1 and may contain negative or positive values of any granularity. 

Thus, our first idea was to constrain the optimization process of CMA-ES such that it would produce results that match the constraints of GES.
However, we found that once the same constraints are introduced, CMA-ES often violates the constraints; making CMA-ES inefficient and often leading to an endless loop due to rejection sampling. 
In other words, we were not able to make CMA-ES produces solution vectors that fulfill all constraints of GES. 
In general, constraining CMA-ES is also not trivial \citep{DBLP:journals/swevo/Biedrzycki20}, and we leave more sophisticated approaches to constrain CMA-ES for post hoc ensembling, like methods based on repair-and-inject or penalization \citep{cma/HansenTutorial2016} or with relaxed constraints, to future work. 

Instead of constraining the optimization process of CMA-ES, we moved to adding the constraints directly to the weight vector when they are evaluated, following a concept observed from GES.
That is, we observed that while the constraints of GES are an implicit result of the algorithm as defined by \cite{caruana2004}, they manifested explicitly only when one computes the weight vector with Equation \ref{eq/ges_w}.
The optimization loop of GES, \emph{i.e.}, iteratively building $E$, does not explicitly consider these constraints, but only greedily minimizes a user-defined loss.
In other words, the optimizer is only implicitly constrained \emph{by applying constraints during the computation of the weight vector; before evaluating the vector's performance}.

In detail, every time GES computes the loss for an ensemble $E$, it first transforms $E$ into $W^{pDisc}$ using Equation \ref{eq/ges_w}. 
Thereby, applying the constraints that the resulting weight vector must sum to 1, is sparse, and $0 \leq w_i \leq 1$. 
Then, the $L(P, W^{pDisc})$ is returned as the loss of $E$.
At this point, it becomes clear that changing Equation \ref{eq/ges_w} leads to different constraints; the loss of $E$ could change without touching the optimization loop of GES. 

As a result, we were motivated to apply the same concept to CMA-ES by normalizing the weight vector before we aggregate the predictions of the base models. 
Thus, changing the loss associated with a weight vector proposed by CMA-ES outside of its optimization process. 
In contrast, our application in Section \ref{sec/cma-es_app} normalized the aggregated predictions for ROC AUC using softmax -- we normalized \emph{after aggregation}. 
Now, however, we propose to normalize \emph{before aggregation} as in GES. 
In turn, this also changes the optimization process of CMA-ES, \emph{e.g.}, the parameter update, because a weight vector might have a different loss depending on normalizing before or after aggregation.


\subsection{Normalization Methods}

We propose three distinct normalization methods. Two of the methods we propose are based on the concept of GES such that the last proposed method tries to simulate Equation \ref{eq/ges_w} fully. 

\paragraph{1) Softmax (CMA-ES-Softmax)} Initially, we propose a simple alternative to our previous usage of CMA-ES by moving the (non-linear) softmax before the aggregation. That is, we normalize the weight vector $W$ by taking its softmax. \changed{That is, for a weight $w_i \in W$, we calculate:
\inlineequation[eq/softmax]{w_i^{s} = \frac{\exp(w_i)}{\sum^m_{j=1} \exp(w_j)}},
resulting in $W^{s} = (w_1^{s},..., w_m^{s})$ } with $\sum_{j=1}^m w^{s}_j = 1$ and $0 \leq w_j \leq 1$ for $w_j \in W^{s}$.  

\paragraph{2) Softmax \& Implict GES Normalization (CMA-ES-ImplictGES)} Next, we propose to re-normalize $W^{s}$ with the aim of producing an equivalent to a pseudo-discrete weight vector $W^{pDisc}$; simulating GES's $\mathcal{G}$ (see Equation \ref{eq/h_cat}). 
Therefore, we round each value of $W^{s}$ to the nearest fraction with denominator $N_{hyp}$ producing a \emph{rounding-discrete} weight vector $W^{rDisc}$.
Then, $N_{hyp}$ represents the number of \emph{hypothetical iterations} for a simulated $\mathcal{G}$. We set $N_{hyp} = 50$, similar to GES.

We produce \changed{$W^{rDisc} = (w_0^{rDisc}, ..., w_m^{rDisc})$} by multiplying each $w_i^{s}$ with $N_{hyp}$ and rounding each element to the nearest integer afterwards; rounding up for values larger than 0.5. \changed{Therefore, we first compute the integer vector $R = (r_1, ..., r_m)$ using $r_i = \lfloor w_i^{s} * N_{hyp}\rceil$.
Note, $R$ can be thought of as a vector of repetitions where $r_i$ denotes} how often a model has been repeated in a hypothetical list of repeated base models $E_{hyp}$.
That is, $E_{hyp}$ is connected to $W^{rDisc}$ like an $E$ to its $W^{pDisc}$. 
Hence, we can compute $W^{rDisc}$ using $R$, paralleling Equation \ref{eq/ges_w}: 

\begin{equation}
    W^{rDisc} = \left[\frac{r_i}{\sum_{j=1}^m r_j} \; | \; r_i \in R \right].
    \label{eq/w_disc}
\end{equation}

$W^{rDisc}$ sums to 1, and each element is between $0$ and $1$. 
Interestingly, we found that this approach also \emph{implicitly trims} base models, as the nearest fraction can be $\frac{0}{N_{hyp}}$ such that the method assigns zero weight to base models in these cases. 

\paragraph{3) Softmax \& Explicit GES Normalization (CMA-ES-ExplicitGES)} 
Finally, we propose to explicitly trim base models and perfect the simulation of Equation \ref{eq/ges_w}.
We can explicitly trim base models based on $N_{hyp}$.
We found that a weight $w_j^s$ is set to zero by rounding if $w_j^s*N_{hyp} \leq 0.5$.
If we reformulate the inequality to $w_j^s \leq 0.5*\frac{1}{N_{hyp}}$, we see that this parallels GES, where the number of iterations determines the minimal weight a model can be assigned, \emph{i.e.}, $\frac{1}{N}$. 

Furthermore, we found that CMA-ES-ImplictGES does not simulate GES sufficiently.
We observed that rounding may result in $\sum_{j=1}^m r_j \neq N_{hyp}$. 
That is, the total number of repetitions in $R$ did not match the number of simulated iterations nor the (hypothetical) length of $E_{hyp}$. 
$R$ was supposed to relate to $E_{hyp}$ for $W^{rDisc}$ like an $E$ to its $W^{pDisc}$. 
Yet for GES, it holds that $|E| = N$ while $|E_{hyp}| \neq N_{hyp}$ can happen in CMA-ES-ImplictGES. 

Considering both, we implemented the third method, \changed{shown in Algorithm \ref{alg/cma_exp_ges}}.
First, we compute $W^{s}$ and trim any base model smaller than $\frac{0.5}{N_{hyp}}$ \changed{(Line \ref{alg/c/trim})}.
If we set all weights to zero, we fall back to an unweighted average \changed{(Line \ref{alg/c/fallback})}.
Second, we round to the nearest integer, producing $R'$ \changed{(Line \ref{alg/c/round})}. 

Next, we set $R'' = R'$ and modify $R''$ to achieve $\sum_{j=1}^m r''_j = N_{hyp}$.
We want to keep the distribution of $R''$ as close as possible to the distribution of $R'$.
Hence, we keep the relative distances between the individual elements in $R'$ and $R''$ similar.  

If $\sum_{j=1}^m r'_j > N_{hyp}$, we decrement elements in $R''$ by $1$ until $\sum_{j=1}^m r''_j = N_{hyp}$ \changed{(Line \ref{alg/c/a})}. We decrement in order from lowest to highest valued element in $R'$, that is, lowest to highest weighted base model in the resulting weight vector. Thus, first trimming base models with only one repetition.
Finally, if $\sum_{j=1}^m r'_j - N_{hyp}$ is large enough, we decrement the most repeated elements. Note, due to rounding, we must decrement each element once in the worst case. 
If $\sum_{j=1}^m r'_j < N_{hyp}$, we have to increase the value of elements in $R''$.
To keep the relative distances similar, we equally distributed $N_{hyp} - \sum_{j=1}^m r'_j$ increments between all non-zero elements in $R''$ \changed{(Line \ref{alg/c/b})}. 
Finally, the $R''$ is transformed into a weight vector with Equation \ref{eq/w_disc}.

\begin{algorithm}
\changed{
\scriptsize
\caption{The Procedure for CMA-ES-ExplicitGES}\label{alg/cma_exp_ges}
\begin{algorithmic}[1]
    \Require Weight vector $W'$ of length $m$, the number of hypothetical iterations $N_{hyp}$ 
    \Ensure Weight vector $W$

    \State $W \gets W^{s} \text{ computed with Equation}$ \ref{eq/softmax} \text{ using } $W'$ \Comment{Apply softmax.}
    
    \For{$i=1$ \textbf{to} $m$} \Comment{Trim base models.} \label{alg/c/trim}
       \If{$w_i \leq \frac{0.5}{N_{hyp}}$} 
            \State $w_i \gets 0$
        \EndIf
    \EndFor

    \If{$\sum_{i=1}^m w_i = 0$} \Comment{Fallback to unweighted average.} \label{alg/c/fallback}
        \State \Return $(\frac{1}{m}, ..., \frac{1}{m})$
    \EndIf

    \State $R' \gets \left[0 \cdots 0\right]$ \Comment{Initialize an empty vector of repetitions.}

    \For{$i=1$ \textbf{to} $m$} \Comment{Round to nearest integer.} \label{alg/c/round}
        \State $r'_i \gets \lfloor w_i^{s} * N_{hyp}\rceil$
    \EndFor

    \State $R'' \gets R'$ 

    \If{$\sum_{j=1}^m r'_j > N_{hyp}$} \label{alg/c/a}
        \State $R'' \gets$ Decrement elements from lowest to highest valued element in $R''$ by $1$ until $\sum_{j=1}^m r_j'' = N_{hyp}$ 
    \EndIf
    
    \If{$\sum_{j=1}^m r'_j < N_{hyp}$}  \label{alg/c/b}
        \State $R'' \gets$ Equally distributed $N_{hyp} - \sum_{j=1}^m r'_j$ increments between all non-zero elements in $R''$
    \EndIf
    \State \Return $W \text{ computed with Equation}$ \ref{eq/w_disc} \text{ using } $R''$.

\end{algorithmic}
}
\end{algorithm}


\subsection{Comparing Normalization Methods}
\label{sec/compare_norm}
We use CMA-ES-ExplicitGES for the final evaluation below because it is the only approach that is in line with GES's concepts.
Nevertheless, here, we \changed{provide an additional comparison of} the three normalization methods on the same data as used in Section \ref{sec/cma-es-compare}. We run CMA-ES, as described above, with the three different methods for normalization on the data from AutoGluon for ROC AUC. 
We ignore the threshold-dependent balanced accuracy because CMA-ES is not affected by overfitting for balanced accuracy. 
Besides normalization, the main difference to the application from Section \ref{sec/cma-es_app} is that we do not apply softmax after aggregation anymore when we apply normalization. 

First, a note regarding sparseness.
On average, across all datasets for ROC AUC, ${\sim}13.2$ base models exist\changed{, see Appendix \ref{apdx/data_overview} for each dataset's number}.  
\changed{For comparison, we computed the average number of non-zero weighted base models for the ensemble methods, see Appendix \ref{apdx/norm_method_comprae}.
This shows} that CMA-ES without normalization has an average ensemble size, that is, the number of non-zero weighted base models, of ${\sim}12.9$. 
In contrast, CMA-ES-ExplicitGES has an average ensemble size of ${\sim}6.3$, CMA-ES-ImplicitGES of ${\sim}5.4$. For context, GES has an average ensemble size of ${\sim}5.8$
Hence, we conclude that CMA-ES produces dense weight vectors.
While our normalization approaches are able to produce sparse vectors like GES. 

Next, we repeat the statistical test performed in Section \ref{sec/cma-es-compare} for all normalization methods, CMA-ES, and the SingleBest, see Figure \ref{fig/cd_plot2} in the Appendix \ref{apdx/norm_method_comprae}.
We observe that all normalization methods outperform CMA-ES and that CMA-ES-ExplicitGES ranks highest.
Furthermore, the different normalization methods are not statistically significantly different from each other. 
Only CMA-ES-ExplicitGES is significantly different from CMA-ES for multi-class.  



\section{Overall Experiments}
\label{sec/exp}
In our final evaluation, we mirror the experiments from Section \ref{sec/cma-es-compare} and compare the SingleBest, GES, CMA-ES, and CMA-ES with normalization (CMA-ES-ExplicitGES). 
\changed{We additionally include stacking in our comparison because it is part of Auto-Sklearnâ€™s insight and used by H2O AutoML.}
For our implementation of stacking \citep{stakcingWolpert}, we use a default Logistic Regression classifier from scikit-learn \citep{scikit-learn} as a stacking model.
We adjusted the code such that we terminate after $m*50$ evaluations to make the method comparable to GES and CMA-ES.
For CMA-ES we stick to the implementation and default hyperparameters as described in Section \ref{sec/cma-es_app}.

Besides the statistical tests, we also inspect the difference in the distributions of relative performance.
Therefore, we follow the AutoML benchmark \citep{automl_benchmark_2022} and use \emph{normalized improvement} to make the scores of methods comparable across different datasets. 
We scale the scores for a dataset such that $-1$ is equal to the score of a baseline, here the SingleBest, and $0$ is equal to the score of the best method on the dataset.
We employ a variant of normalized improvement as we ran into an edge case where the normalized improvement is undefined if the difference between the single best model and the best method is $0$. 
In our variant, for this edge case, we set everything as good as the SingleBest to $-1$ and penalize all methods worse than the baseline with $-10$\changed{; following a penalization approach like PAR10 from Algorithm Selection \citep{DBLP:journals/ai/LindauerRK19}. 
We provide a formalized definition of normalized improvement in Appendix \ref{apdx/norm_improvement}. 
}

\section{Overall Results}
\label{sec/results}

Figure \ref{fig/cd_plot_f} shows the results of the statistical tests and mean rankings for the compared methods. 
The distribution of the relative performance is shown in Figure \ref{fig/ni_plot}. \changed{Additionally, the performance per dataset is provided in Appendix \ref{apdx/overview_of_performance_per_dataset}.}

\begin{figure}
    \begin{subfigure}[t]{0.49\linewidth}
        \centering 
        \includegraphics[width=\linewidth, trim={0cm 2cm 0cm 1.8cm},clip]{f13.pdf}
        \caption{Balanced Accuracy - Binary (41 Datasets)}
    \end{subfigure}
    \begin{subfigure}[t]{0.49\linewidth}
        \centering
        \includegraphics[width=\linewidth, trim={0cm 2cm 0cm 1.8cm},clip]{f8.pdf}
        \caption{Balanced Accuracy - Multi-class (30 Datasets)}
    \end{subfigure}
    \begin{subfigure}[t]{0.49\linewidth}
        \centering
        \includegraphics[width=\linewidth, trim={0cm 2cm 0cm 1.5cm},clip]{f11.pdf}
        \caption{ROC AUC - Binary (41 \changed{Datasets})}
    \end{subfigure}
    \begin{subfigure}[t]{0.49\linewidth}
        \centering
        \includegraphics[width=\linewidth, trim={0cm 2cm 0cm 1.5cm},clip]{f6.pdf}
        \caption{ROC AUC - Multi-class (30 \changed{Datasets})}
    \end{subfigure}

  
    \caption{\textbf{CD Plots for all Methods:}
    {\normalfont Methods connected by a bar are not significantly different.}
    }
    \label{fig/cd_plot_f}
\end{figure}

\begin{figure}
    \begin{subfigure}[t]{0.49\linewidth}
        \centering 
        \includegraphics[width=\linewidth]{f2.pdf}
        \caption{Balanced Accuracy - Binary (41 Datasets)}
        \label{fig/ni_plot/bacc_b}
    \end{subfigure}
    \begin{subfigure}[t]{0.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{f4.pdf}
        \caption{Balanced Accuracy - Multi-class (30 Datasets)} 
        \label{fig/ni_plot/bacc_m}
    \end{subfigure}
    \begin{subfigure}[t]{0.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{f3.pdf}
        \caption{ROC AUC - Binary (41 \changed{Datasets})}
        \label{fig/ni_plot/roc_b}
    \end{subfigure}
    \begin{subfigure}[t]{0.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{f1.pdf}
        \caption{ROC AUC - Multi-class (30 \changed{Datasets})}
    \label{fig/ni_plot/roc_m}
    \end{subfigure}

  
    \caption{\textbf{Normalized Improvement Boxplots: }{\normalfont 
    Higher normalized improvement is better. Each black point represents the improvement for one dataset. A value smaller than $-1$ is worse than the single best model (red vertical line), while $0$ is the best observed value.
    The number in square brackets counts the outliers of a method left of the plot's boundary.}
    }
    \label{fig/ni_plot}
\end{figure}

\paragraph{Overall Predictive Performance}
All post hoc ensembling methods always outperform the SingleBest on average, although not always statistically significant -- see Figure \ref{fig/cd_plot_f}. 
Yet, post hoc ensembling can overfit and become worse for specific datasets, as indicated by the black dots left of the red bar and the number of outliers in square brackets in Figure \ref{fig/ni_plot}.

\emph{For balanced accuracy}, we observe that CMA-ES significantly beats all methods. 
Likewise, we observe that stacking and CMA-ES-ExplicitGES outperform GES  by a small non-significant margin.

\emph{For ROC AUC}, we see that GES and CMA-ES-ExplicitGES outperform all other methods and differ only by a small non-significant margin. 
Both are also significantly different from the SingleBest; unlike stacking. 
Moreover, Figure \ref{fig/ni_plot} shows us that CMA-ES-ExplicitGES has similar or better relative performance distributions than GES (see the medians and whiskers).  

\paragraph{Normalization to Combat Overfitting}
See Table \ref{table/rank_change_2} to inspect overfitting for CMA-ES-ExplictGES. \changed{See Appendix \ref{apdx/mean_rank_change_2} for an overview of the rank change for all compared methods.} 
In general, CMA-ES-ExplictGES's mean rank, compared to GES and the SingleBest, changes only minimally between validation and test data. 
Showing us that it overfits less than CMA-ES (compare to Table \ref{table/rank_change}, Section \ref{sec/cma-es-compare-results}).  
As before, the SingleBest is always the worst-ranked method. 
GES is worse than CMA-ES-ExplictGES on test data for all but ROC AUC Binary. 
On validation data, however, GES is better than CMA-ES-ExplictGES in all cases except for ROC AUC multi-class, where it is tied. 
Now, GES is \emph{more affected by overfitting} than CMA-ES with normalization.

\changed{
\paragraph{No Free Lunch} CMA-ES-ExplictGES for balanced accuracy ranks worse than CMA-ES but better than GES. In contrast, CMA-ES-ExplictGES ranks better than CMA-ES for ROC AUC.   
A decrease in performance for balanced accuracy was to be expected as the normalization method constrained the solutions of CMA-ES to be sparse and pseudo-discrete to combat overfitting, but CMA-ES did not overfit for balanced accuracy.
Moreover, it indicates that satisfying these properties of GES for balanced accuracy is suboptimal. 
Hence, our results also indicate the need to select the best method per task and metric instead of always using the same method; in line with the \emph{no free lunch theorem}. 
Likewise, the drastic differences in performance of the methods between metrics suggest that the optimization landscapes, and the impact of overfitting on them, differ drastically. 
}

\begin{table}[h]
\caption{Mean rank change for CMA-ES-ExplictGES compared to GES and SingleBest. \changed{In the case of a tie for the absolute rank, we assign all tied values the average of their tie-broken ranks.}}\label{table/rank_change_2}
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}llccc@{}}
\toprule
Metric & Task Type & \multicolumn{1}{l}{Mean $Rank_{Validation}$} & \multicolumn{1}{l}{Mean $Rank_{Test}$} & \multicolumn{1}{l}{Absolute Rank (Val $\rightarrow$ Test) } \\ \midrule
Balanced Accuracy & Binary      & 1.74 & 1.78 & 2.0 $\rightarrow$ 1.0 \\
Balanced Accuracy & Multi-class & 1.73 & 1.78 & 2.0 $\rightarrow$ 1.0 \\
ROC AUC           & Binary      & 1.63 & 1.70 & 2.0 $\rightarrow$ 2.0 \\
ROC AUC           & Multi-class & 1.50 & 1.57 & 1.5 $\rightarrow$ 1.0 \\ \bottomrule
\end{tabular}%
}
\end{table}

\section{Conclusion}
Greedy ensemble selection (GES) \citep{caruana2004} is often used for post hoc ensembling in AutoML; likely as a result of Auto-Sklearn 1's \citep{ask1} reported insight that GES is superior to potential alternatives, like gradient-free numerical optimization, for post hoc ensembling.

In this paper, we have shown that Auto-Sklearn's insight w.r.t.{} overfitting depends on the metric when tested for an AutoML system with higher-quality validation data than Auto-Sklearn, e.g., AutoGluon \citep{ag1}.   
Indeed, for the metric ROC AUC, GES does not overfit meaningfully, while gradient-free numerical optimization, e.g., CMA-ES \citep{cma/Hansen14, cma/HansenTutorial2016}, overfits drastically. 
However, for balanced accuracy, CMA-ES does not overfit and outperforms GES.

As a direct consequence, we were motivated to find a method that combats the overfitting of CMA-ES for ROC AUC.
Therefore, we proposed a novel normalization method, is inspired by GES, which successfully salvages CMA-ES for ROC AUC by making CMA-ES perform better than or similar to GES. 





\newpage
\begin{acknowledgements}
The CPU nodes of the OMNI cluster of the University of Siegen (North Rhine-Westphalia, Germany) were used for all experiments presented in this work.
\end{acknowledgements}
\bibliography{references}
\newpage
\appendix
\section{Submission Checklist}

\begin{enumerate}
\item For all authors\dots
  \begin{enumerate}
  \item Do the main claims made in the abstract and introduction accurately
    reflect the paper's contributions and scope?
    \answerYes{We state in the abstract and introduction that we compare GES to CMA-ES w.r.t.{} overfitting. Moreover, we claim to look at normalization to avoid overfitting. This is exactly what we do in the paper.}
  \item Did you describe the limitations of your work?
    \answerYes{In the Appendix, see \ref{apdx/limitations}.}
  \item Did you discuss any potential negative societal impacts of your work?
    \answerYes{In the Appendix, see \ref{apdx/borader_impact}.}
  \item Have you read the ethics author's and review guidelines and ensured that
    your paper conforms to them? \url{https://2023.automl.cc/ethics/}
    \answerYes{We believe our paper confirms to them.}
  \end{enumerate}
\item If you are including theoretical results\dots
  \begin{enumerate}
  \item Did you state the full set of assumptions of all theoretical results?
    \answerNA{We included no theoretical results; only theoretical arguments for our proposed normalization method.}
  \item Did you include complete proofs of all theoretical results?
    \answerNA{We included no theoretical results; only theoretical arguments for our proposed normalization method.}
  \end{enumerate}
\item If you ran experiments\dots
  \begin{enumerate}
  \item Did you include the code, data, and instructions needed to reproduce the
    main experimental results, including all requirements (e.g.,
    \texttt{requirements.txt} with explicit version), an instructive
    \texttt{README} with installation, and execution commands (either in the
    supplemental material or as a \textsc{url})?
    \answerYes{See our code repository (Appendix \ref{apdx/public_links}) for all details.}
  \item Did you include the raw results of running the given instructions on the
    given code and data?
    \answerYes{See our code repository.}
  \item Did you include scripts and commands that can be used to generate the
    figures and tables in your paper based on the raw results of the code, data,
    and instructions given?
    \answerYes{See our code repository.}
  \item Did you ensure sufficient code quality such that your code can be safely
    executed and the code is properly documented?
    \answerYes{We believe that our code quality and documentation are sufficient.}
  \item Did you specify all the training details (e.g., data splits,
    pre-processing, search spaces, fixed hyperparameter settings, and how they
    were chosen)?
    \answerYes{See the Section \ref{sec/cma-es-compare} and \ref{sec/exp}. Additionally, see our code repository.}
  \item Did you ensure that you compared different methods (including your own)
    exactly on the same benchmarks, including the same datasets, search space,
    code for training and hyperparameters for that code?
    \answerYes{We ran all methods on the same data.}
  \item Did you run ablation studies to assess the impact of different
    components of your approach?
    \answerYes{We compared different normalization approaches, see Section \ref{sec/compare_norm}.}
  \item Did you use the same evaluation protocol for the methods being compared?
    \answerYes{We ran all methods on the same data with the same evaluation protocol and code.}
  \item Did you compare performance over time?
    \answerNo{We compared performance for a specific point in time (after $50$ iterations of GES, i.e., after $m*50$ function evaluations of $L$).
    Performance over time was out of scope for our experiments.}
  \item Did you perform multiple runs of your experiments and report random seeds?
    \answerYes{Yes, we used 10-fold cross-validation for all our runs. The used random seeds can be found in our code.}
  \item Did you report error bars (e.g., with respect to the random seed after
    running experiments multiple times)?
    \answerNo{We took the average over the 10 folds as a score following previous work and have not reported variance across folds.}
  \item Did you use tabular or surrogate benchmarks for in-depth evaluations?
    \answerNA{Such benchmarks were not available for our use case.}
  \item Did you include the total amount of compute and the type of resources
    used (e.g., type of \textsc{gpu}s, internal cluster, or cloud provider)?
    \answerYes{See Section \ref{sec/cma-es-compare}.}
  \item Did you report how you tuned hyperparameters, and what time and
    resources this required (if they were not automatically tuned by your AutoML
    method, e.g. in a \textsc{nas} approach; and also hyperparameters of your
    own method)?
    \answerNA{We did not tune hyperparameters. We used a default application of CMA-ES and introduced no meaningful new hyperparameters with our approaches that would require tuning.}
  \end{enumerate}
\item If you are using existing assets (e.g., code, data, models) or
  curating/releasing new assets\dots
  \begin{enumerate}
  \item If your work uses existing assets, did you cite the creators?
    \answerYes{See Section \ref{sec/cma-es-compare} and Appendix \ref{apdx/essential_assets}.}
  \item Did you mention the license of the assets?
    \answerYes{See Appendix \ref{apdx/essential_assets}.}
  \item Did you include any new assets either in the supplemental material or as
    a \textsc{url}?
    \answerYes{See our code repository.}
  \item Did you discuss whether and how consent was obtained from people whose
    data you're using/curating?
    \answerNA{We are only using publicly available data that was used before in benchmarks.}
  \item Did you discuss whether the data you are using/curating contains
    personally identifiable information or offensive content?
    \answerNA{We believe that the data we are using does not contain personally identifiable information or offensive content.}
  \end{enumerate}
\item If you used crowdsourcing or conducted research with human subjects\dots
  \begin{enumerate}
  \item Did you include the full text of instructions given to participants and
    screenshots, if applicable?
    \answerNA{We did not use crowdsourcing or conducted research with human subjects.}
  \item Did you describe any potential participant risks, with links to
    Institutional Review Board (\textsc{irb}) approvals, if applicable?
    \answerNA{We did not use crowdsourcing or conducted research with human subjects.}
  \item Did you include the estimated hourly wage paid to participants and the
    total amount spent on participant compensation?
     \answerNA{We did not use crowdsourcing or conducted research with human subjects.}
  \end{enumerate}
\end{enumerate}

\newpage
\section{Limitations}
\label{apdx/limitations}
We note that our work is limited with respect to the following points:
1) we did not explore variations (w.r.t.{} hyperparameters or implementation) of CMA-ES in our work;
2) we considered overfitting with respect to mean rank change between validation and test data, but did not consider other concepts of overfitting;
3) we only looked at normalization to combat overfitting for CMA-ES and were not able to compare normalization to using constraints during optimization;
4) we only provided a high-level theoretical analysis of GES and were not able to provide more fundamental work or proofs 
\changed{; and 5) we only evaluated our approach for AutoGluon, one AutoML system with its specific approach to AutoML.} 

\section{Broader Impact Statement}
\label{apdx/borader_impact}

After careful reflection, we determine that this work presents \emph{almost} no notable or new negative impacts to society or the environment that are not already present for existing state-of-the-art AutoML systems.
This follows from our work being mostly domain-independent, abstract, and methodical.
We only proposed to replace one component of an AutoML system such that the predictive performance improves. 
Nevertheless, we would like to remark that our work might prompt others to use a default application of CMA-ES instead of GES for a metric like balanced accuracy. 
This might have a negative impact on the environment because this would likely increase the inference time and size of the final ensemble proposed by AutoML systems. 

In contrast -- as a trade-off -- we see the positive impact that higher predictive performance with CMA-ES could  better support decisions made with AutoML systems.
Moreover, we believe that our work might help to understand GES, the currently most used method, better; such that its performance and behaviour becomes more explainable. 


\section{Used Assets: Essential Python Frameworks for the Implementation and Experiments}
\label{apdx/essential_assets}

The following frameworks were essential for our implementation and experiments:
\begin{itemize}
    \item AutoGluon \citep{ag1}, Version: 0.6.2, Apache-2.0 License; We used AutoGluon to generate base models for post hoc ensembling. 
    \item pycma \citep{hansen2019pycma}, Version 3.2.2, BSD 3-Clause License; We used pycma for CMA-ES. 
    \item Assembled \citep{purucker2022assembledopenml}, Version 0.0.4, MIT License; We used Assembled to store the base models generated with AutoGluon and to run our ensemble-related experiments. 
\end{itemize}

\section{DOIs for Data and Code}
\label{apdx/public_links}
The following assets were newly created as part of our experiments:
\begin{itemize}
    \item The code for our experiments: \url{https://doi.org/10.6084/m9.figshare.23609226}.
    \item The prediction data of base models collected by running AutoGluon on the classification datasets from the AutoML benchmark: \url{https://doi.org/10.6084/m9.figshare.23609361}.
\end{itemize}

\newpage
\section{Data Overview}
\label{apdx/data_overview}
See Table \ref{tabel/data_overview} for an overview of the used datasets and their characteristics.  
Additionally, the table shows the mean number of base models and the mean number of distinct algorithms generated by AutoGluon for the dataset for each metric (mean over the 10 folds of a dataset).

\begin{table}[H]
\centering
\caption{Data Overview}
\label{tabel/data_overview}
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}lccccc|cc|cc@{}}
\toprule
                                            &                &             &            &           &             & \multicolumn{2}{c|}{Mean \# Base Models} & \multicolumn{2}{c}{Mean \# Distinct Algorithms} \\ \cmidrule(l){7-10} 
Dataset Name                                & OpenML Task ID & \#instances & \#features & \#classes & Memory (GB) & Balanced Accuracy        & ROC AUC       & Balanced Accuracy           & ROC AUC           \\ \midrule
yeast                                       & 2073           & 1484        & 9          & 10        & 32          & 21.0                     & 21.3          & 12.0                        & 12.1              \\
KDDCup09\_appetency                         & 3945           & 50000       & 231        & 2         & 32          & 11.0                     & 11.0          & 11.0                        & 11.0              \\
covertype                                   & 7593           & 581012      & 55         & 7         & 64          & 13.3                     & 12.9          & 8.3                         & 8.0               \\
amazon-commerce-reviews                     & 10090          & 1500        & 10001      & 50        & 32          & 8.3                      & 8.7           & 5.3                         & 5.7               \\
Australian                                  & 146818         & 690         & 15         & 2         & 32          & 13.0                     & 13.0          & 13.0                        & 13.0              \\
wilt                                        & 146820         & 4839        & 6          & 2         & 32          & 12.0                     & 12.0          & 12.0                        & 12.0              \\
numerai28.6                                 & 167120         & 96320       & 22         & 2         & 32          & 12.0                     & 12.0          & 12.0                        & 12.0              \\
phoneme                                     & 168350         & 5404        & 6          & 2         & 32          & 12.0                     & 11.9          & 12.0                        & 11.9              \\
credit-g                                    & 168757         & 1000        & 21         & 2         & 32          & 13.0                     & 13.0          & 13.0                        & 13.0              \\
steel-plates-fault                          & 168784         & 1941        & 28         & 7         & 32          & 21.0                     & 21.0          & 12.0                        & 12.0              \\
APSFailure                                  & 168868         & 76000       & 171        & 2         & 32          & 12.0                     & 12.0          & 12.0                        & 12.0              \\
dilbert                                     & 168909         & 10000       & 2001       & 5         & 32          & 12.9                     & 12.7          & 7.9                         & 8.3               \\
fabert                                      & 168910         & 8237        & 801        & 7         & 32          & 19.8                     & 19.8          & 11.0                        & 11.0              \\
jasmine                                     & 168911         & 2984        & 145        & 2         & 32          & 12.3                     & 12.9          & 12.3                        & 12.9              \\
airlines                                    & 189354         & 539383      & 8          & 2         & 64          & 9.0                      & 8.9           & 9.0                         & 8.9               \\
dionis                                      & 189355         & 416188      & 61         & 355       & 128         & 4.0                      & 4.2           & 3.0                         & 3.6               \\
albert                                      & 189356         & 425240      & 79         & 2         & 64          & 7.0                      & 7.0           & 7.0                         & 7.0               \\
gina                                        & 189922         & 3153        & 971        & 2         & 32          & 12.0                     & 12.0          & 12.0                        & 12.0              \\
ozone-level-8hr                             & 190137         & 2534        & 73         & 2         & 32          & 13.0                     & 13.0          & 13.0                        & 13.0              \\
vehicle                                     & 190146         & 846         & 19         & 4         & 32          & 24.0                     & 24.0          & 13.0                        & 13.0              \\
madeline                                    & 190392         & 3140        & 260        & 2         & 32          & 12.0                     & 12.3          & 12.0                        & 12.3              \\
philippine                                  & 190410         & 5832        & 309        & 2         & 32          & 12.0                     & 12.0          & 12.0                        & 12.0              \\
ada                                         & 190411         & 4147        & 49         & 2         & 32          & 12.0                     & 12.0          & 12.0                        & 12.0              \\
arcene                                      & 190412         & 100         & 10001      & 2         & 32          & 13.0                     & 13.0          & 13.0                        & 13.0              \\
jannis                                      & 211979         & 83733       & 55         & 4         & 32          & 14.9                     & 15.3          & 9.2                         & 9.3               \\
Diabetes130US                               & 211986         & 101766      & 50         & 3         & 32          & 17.9                     & 18.1          & 10.8                        & 10.9              \\
micro-mass                                  & 359953         & 571         & 1301       & 20        & 32          & 13.0                     & 13.0          & 13.0                        & 13.0              \\
eucalyptus                                  & 359954         & 736         & 20         & 5         & 32          & 13.0                     & 13.0          & 13.0                        & 13.0              \\
blood-transfusion-service-center            & 359955         & 748         & 5          & 2         & 32          & 13.0                     & 13.0          & 13.0                        & 13.0              \\
qsar-biodeg                                 & 359956         & 1055        & 42         & 2         & 32          & 13.0                     & 13.0          & 13.0                        & 13.0              \\
cnae-9                                      & 359957         & 1080        & 857        & 9         & 32          & 20.0                     & 20.0          & 11.0                        & 11.0              \\
pc4                                         & 359958         & 1458        & 38         & 2         & 32          & 13.0                     & 13.0          & 13.0                        & 13.0              \\
cmc                                         & 359959         & 1473        & 10         & 3         & 32          & 21.8                     & 21.6          & 12.0                        & 12.0              \\
car                                         & 359960         & 1728        & 7          & 4         & 32          & 18.4                     & 18.0          & 9.2                         & 9.0               \\
mfeat-factors                               & 359961         & 2000        & 217        & 10        & 32          & 20.0                     & 20.0          & 11.0                        & 11.0              \\
kc1                                         & 359962         & 2109        & 22         & 2         & 32          & 12.8                     & 13.0          & 12.8                        & 13.0              \\
segment                                     & 359963         & 2310        & 20         & 7         & 32          & 21.0                     & 21.0          & 12.0                        & 12.0              \\
dna                                         & 359964         & 3186        & 181        & 3         & 32          & 19.0                     & 19.0          & 10.0                        & 10.0              \\
kr-vs-kp                                    & 359965         & 3196        & 37         & 2         & 32          & 10.0                     & 10.0          & 10.0                        & 10.0              \\
Internet-Advertisements                     & 359966         & 3279        & 1559       & 2         & 32          & 12.0                     & 12.0          & 12.0                        & 12.0              \\
Bioresponse                                 & 359967         & 3751        & 1777       & 2         & 32          & 12.0                     & 12.0          & 12.0                        & 12.0              \\
churn                                       & 359968         & 5000        & 21         & 2         & 32          & 12.0                     & 12.0          & 12.0                        & 12.0              \\
first-order-theorem-proving                 & 359969         & 6118        & 52         & 6         & 32          & 20.1                     & 20.1          & 11.1                        & 11.1              \\
GesturePhaseSegmentationProcessed           & 359970         & 9873        & 33         & 5         & 32          & 20.0                     & 20.4          & 11.0                        & 11.2              \\
PhishingWebsites                            & 359971         & 11055       & 31         & 2         & 32          & 10.0                     & 10.0          & 10.0                        & 10.0              \\
sylvine                                     & 359972         & 5124        & 21         & 2         & 32          & 12.0                     & 12.0          & 12.0                        & 12.0              \\
christine                                   & 359973         & 5418        & 1637       & 2         & 32          & 12.0                     & 12.0          & 12.0                        & 12.0              \\
wine-quality-white                          & 359974         & 4898        & 12         & 7         & 32          & 21.0                     & 21.0          & 12.0                        & 12.0              \\
Satellite                                   & 359975         & 5100        & 37         & 2         & 32          & 12.0                     & 12.0          & 12.0                        & 12.0              \\
Fashion-MNIST                               & 359976         & 70000       & 785        & 10        & 64          & 12.1                     & 13.0          & 8.5                         & 8.2               \\
connect-4                                   & 359977         & 67557       & 43         & 3         & 32          & 16.2                     & 16.3          & 9.0                         & 9.0               \\
Amazon\_employee\_access                    & 359979         & 32769       & 10         & 2         & 32          & 9.1                      & 10.0          & 9.1                         & 10.0              \\
nomao                                       & 359980         & 34465       & 119        & 2         & 32          & 12.0                     & 10.0          & 12.0                        & 10.0              \\
jungle\_chess\_2pcs\_raw\_endgame\_complete & 359981         & 44819       & 7          & 3         & 32          & 19.5                     & 19.9          & 11.0                        & 11.0              \\
bank-marketing                              & 359982         & 45211       & 17         & 2         & 32          & 12.0                     & 12.0          & 12.0                        & 12.0              \\
adult                                       & 359983         & 48842       & 15         & 2         & 32          & 12.0                     & 11.9          & 12.0                        & 11.9              \\
helena                                      & 359984         & 65196       & 28         & 100       & 32          & 7.7                      & 7.9           & 5.0                         & 5.0               \\
volkert                                     & 359985         & 58310       & 181        & 10        & 32          & 13.9                     & 12.5          & 8.9                         & 8.6               \\
robert                                      & 359986         & 10000       & 7201       & 10        & 64          & 9.7                      & 9.3           & 7.6                         & 7.3               \\
shuttle                                     & 359987         & 58000       & 10         & 7         & 32          & 18.9                     & 19.0          & 11.0                        & 11.0              \\
guillermo                                   & 359988         & 20000       & 4297       & 2         & 32          & 9.0                      & 9.0           & 9.0                         & 9.0               \\
riccardo                                    & 359989         & 20000       & 4297       & 2         & 32          & 10.1                     & 9.0           & 10.1                        & 9.0               \\
MiniBooNE                                   & 359990         & 130064      & 51         & 2         & 32          & 10.3                     & 10.2          & 10.3                        & 10.2              \\
kick                                        & 359991         & 72983       & 33         & 2         & 32          & 11.7                     & 12.0          & 11.7                        & 12.0              \\
Click\_prediction\_small                    & 359992         & 39948       & 12         & 2         & 32          & 11.8                     & 12.4          & 11.8                        & 12.4              \\
okcupid-stem                                & 359993         & 50789       & 20         & 3         & 32          & 18.6                     & 19.8          & 11.0                        & 11.4              \\
sf-police-incidents                         & 359994         & 2215023     & 9          & 2         & 64          & 11.0                     & 7.2           & 11.0                        & 7.2               \\
KDDCup99                                    & 360112         & 4898431     & 42         & 23        & 128         & 10.3                     & 7.6           & 8.5                         & 6.9               \\
porto-seguro                                & 360113         & 595212      & 58         & 2         & 64          & 6.2                      & 2.2           & 6.2                         & 2.2               \\
Higgs                                       & 360114         & 1000000     & 29         & 2         & 64          & 3.0                      & 3.0           & 3.0                         & 3.0               \\
KDDCup09-Upselling                          & 360975         & 50000       & 14892      & 2         & 128         & 10.5                     & 9.0           & 10.5                        & 9.0               \\ \bottomrule
\end{tabular}%
}
\end{table}





\changed{

\section{Overview of Rank Change from Validation to Test Data}
This section provides an overview of the rank change from validation to test data for the compared methods to inspect overfitting. Table \ref{tab/rc1} gives the overview for the comparison made in Section \ref{sec/cma-es-compare-results}.
Table \ref{tab/rc2} gives the overview for the comparison made in Section \ref{sec/results}.

\subsection{Supplements for Section \ref{sec/cma-es-compare-results}}
\label{apdx/mean_rank_change}
\begin{table}[h]

    \changed{
    \caption{\changed{Mean rank change from validation to test data for CMA-ES, GES, and SingleBest. We denote the mean rank on validation data with \emph{MRV} and the mean rank on test data with \emph{MRT}.}}
    \label{tab/rc1}
    \begin{subtable}[h]{0.48\textwidth}
        \centering
        \resizebox{\textwidth}{!}{%
        \begin{tabular}{@{}lccc@{}}
        \toprule
        Method & \multicolumn{1}{l}{MRV} & \multicolumn{1}{l}{MRT} & \multicolumn{1}{l}{Absolute Rank (Val $\rightarrow$ Test) } \\ \midrule
        CMA-ES     & 1.00 & 1.12 & 1.00 -\textgreater 1.00 \\
        GES        & 2.06 & 2.44 & 2.00 -\textgreater 2.50 \\
        SingleBest & 2.94 & 2.44 & 3.00 -\textgreater 2.50 \\ \bottomrule
        \end{tabular}%
        }
        \caption{Balanced Accuracy - Binary}

    \end{subtable}
    \hfill
        \begin{subtable}[h]{0.48\textwidth}
        \centering
        \resizebox{\textwidth}{!}{%
        \begin{tabular}{@{}lccc@{}}
        \toprule
        Method & \multicolumn{1}{l}{MRV} & \multicolumn{1}{l}{MRT} & \multicolumn{1}{l}{Absolute Rank (Val $\rightarrow$ Test) } \\ \midrule
        CMA-ES     & 1.03 & 1.25 & 1.00 -\textgreater 1.00 \\
        GES        &  1.97 & 2.33 & 2.00 -\textgreater 2.00 \\
        SingleBest & 3.00    & 2.42 & 3.00 -\textgreater 3.00  \\ \bottomrule
        \end{tabular}%
        }
        \caption{Balanced Accuracy - Multi-class}

    \end{subtable}
        
    \begin{subtable}[h]{0.48\textwidth}
        \centering
        \resizebox{\textwidth}{!}{%
        \begin{tabular}{@{}lccc@{}}
        \toprule
        Method & \multicolumn{1}{l}{MRV} & \multicolumn{1}{l}{MRT} & \multicolumn{1}{l}{Absolute Rank (Val $\rightarrow$ Test) } \\ \midrule
        CMA-ES   & 1.02 & 1.83 & 1.00 -\textgreater 2.00 \\
        GES      & 1.99 & 1.52 & 2.00 -\textgreater 1.00 \\
        SingleBest & 2.99 & 2.65 & 3.00 -\textgreater 3.00 \\ \bottomrule
        \end{tabular}%
        }
        \caption{ROC AUC - Binary}

    \end{subtable}
    \hfill
        \begin{subtable}[h]{0.48\textwidth}
        \centering
        \resizebox{\textwidth}{!}{%
        \begin{tabular}{@{}lccc@{}}
        \toprule
        Method & \multicolumn{1}{l}{MRV} & \multicolumn{1}{l}{MRT} & \multicolumn{1}{l}{Absolute Rank (Val $\rightarrow$ Test) } \\ \midrule
        CMA-ES  & 1.42 & 2.12 & 1.00 -\textgreater 2.00 \\
        GES   & 1.60 & 1.40 & 2.00 -\textgreater 1.00 \\
        SingleBest & 2.98 & 2.48 & 3.00 -\textgreater 3.00 \\ \bottomrule
        \end{tabular}%
        }
        \caption{ROC AUC - Multi-class}

    \end{subtable}
    } %
    
\end{table}


\subsection{Supplements for Section \ref{sec/results}}
\label{apdx/mean_rank_change_2}
\begin{table}[h]
    \changed{
    \caption{\changed{Mean rank change from validation to test data for CMA-ES-ExplicitGES, GES, and SingleBest. We denote the mean rank on validation data with \emph{MRV} and the mean rank on test data with \emph{MRT}. 
    In the case of a tie for the absolute rank, we assign all tied values the average of their randomly tie-broken ranks. As a result, a rank of $1.5$ for validation data for ROC AUC multi-class occurs since CMA-ES-ExplicitGES and GES are tied.}}
    \label{tab/rc2}

    \begin{subtable}[h]{0.48\textwidth}
        \centering
        \resizebox{\textwidth}{!}{%
        \begin{tabular}{@{}lccc@{}}
        \toprule
        Method & \multicolumn{1}{l}{MRV} & \multicolumn{1}{l}{MRT} & \multicolumn{1}{l}{Absolute Rank (Val $\rightarrow$ Test) } \\ \midrule
        CMA-ES-ExplicitGES & 1.74    & 1.78   & 2.00 -\textgreater 1.00 \\
        GES & 1.40 & 2.00 & 1.00 -\textgreater 2.00 \\
        SingleBest & 2.85 & 2.22 & 3.00 -\textgreater 3.00 \\ \bottomrule
        \end{tabular}%
        }
        \caption{Balanced Accuracy - Binary}

    \end{subtable}
    \hfill
        \begin{subtable}[h]{0.48\textwidth}
        \centering
        \resizebox{\textwidth}{!}{%
        \begin{tabular}{@{}lccc@{}}
        \toprule
        Method & \multicolumn{1}{l}{MRV} & \multicolumn{1}{l}{MRT} & \multicolumn{1}{l}{Absolute Rank (Val $\rightarrow$ Test) } \\ \midrule
        CMA-ES-ExplicitGES & 1.73   & 1.78   & 2.00 -\textgreater 1.00 \\
        GES & 1.27 & 2.07 & 1.00 -\textgreater 2.00 \\
        SingleBest & 3.00 & 2.15  & 3.00 -\textgreater 3.00  \\\bottomrule
        \end{tabular}%
        }
        \caption{Balanced Accuracy - Multi-class}

    \end{subtable}
        
    \begin{subtable}[h]{0.48\textwidth}
        \centering
        \resizebox{\textwidth}{!}{%
        \begin{tabular}{@{}lccc@{}}
        \toprule
        Method & \multicolumn{1}{l}{MRV} & \multicolumn{1}{l}{MRT} & \multicolumn{1}{l}{Absolute Rank (Val $\rightarrow$ Test) } \\ \midrule
        CMA-ES-ExplicitGES & 1.63 & 1.70 & 2.00 -\textgreater 2.00 \\
        GES & 1.39 & 1.50 & 1.00 -\textgreater 1.00 \\
        SingleBest & 2.98 & 2.80 & 3.00 -\textgreater 3.00 \\ \bottomrule
        \end{tabular}%
        }
        \caption{ROC AUC - Binary}

    \end{subtable}
    \hfill
        \begin{subtable}[h]{0.48\textwidth}
        \centering
        \resizebox{\textwidth}{!}{%
        \begin{tabular}{@{}lccc@{}}
        \toprule
        Method & \multicolumn{1}{l}{MRV} & \multicolumn{1}{l}{MRT} & \multicolumn{1}{l}{Absolute Rank (Val $\rightarrow$ Test) } \\ \midrule
        CMA-ES-ExplicitGES & 1.50  & 1.57  & 1.50 -\textgreater 1.00 \\
        GES & 1.50 & 1.73 & 1.50 -\textgreater 2.00 \\
        SingleBest & 3.00 & 2.70 & 3.00 -\textgreater 3.00\\ \bottomrule
        \end{tabular}%
        }
        \caption{ROC AUC - Multi-class}

    \end{subtable}
    } %
    
\end{table}
} %

\section{Comparison of Normalization Methods}
\label{apdx/norm_method_comprae}
See Figure \ref{fig/cd_plot2} for a comparison of the three proposed normalization methods following the experiments described in Section \ref{sec/cma-es-compare}.

\changed{
The difference between the presented methods shows a small ablation study of our approaches w.r.t. satisfying the properties of GES, \emph{pseudo-discrete} and \emph{sparse} (specified in Section \ref{sec/ges_prem}).
CMA-ES and CMA-ES-Softmax are versions without either property; CMA-ES-ImplicitGES satisfies only sparseness; and CMA-ES-ExplicitGES satisfies both properties. 
Only the method that satisfies both properties, CMA-ES-ExplicitGES, is significantly different from CMA-ES for multi-class and always has the best mean rank. 
}

\begin{figure}[h]
    \begin{subfigure}[t]{0.49\linewidth}
    \centering
    \includegraphics[width=\linewidth, trim={0cm 2cm 0cm 1.5cm},clip]{f10.pdf}
    \caption{ROC AUC - Binary \changed{(41 Datasets)}}
    \end{subfigure}
    \begin{subfigure}[t]{0.49\linewidth}
        \centering
        \includegraphics[width=\linewidth, trim={0cm 2cm 0cm 1.5cm},clip]{f5.pdf}
        \caption{ROC AUC - Multi-class  \changed{(30 Datasets)}}
    \end{subfigure}

  
    \caption{\textbf{CD Plots Comparing the Normalization Methods for ROC AUC:{} }{\normalfont Mean rank of the methods (lower is better). Methods connected by a bar are not significantly different.}
    }
    \label{fig/cd_plot2}
\end{figure}

\changed{
To analyze the effect of trimming base models on the size of the ensemble, we show the average ensemble size in Table \ref{apd/tab/avg_ens_size}. 

\begin{table}[]
\caption{The average ensemble size (non-zero weighted base models) for  CMA-ES, CMA-ES-ExplicitGES, CMA-ES-ImplicitGES, and GES for binary and multi-class classification with ROC AUC.}
\label{apd/tab/avg_ens_size}
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}l|cccc@{}}
\toprule
Task Type                  & CMA-ES    & CMA-ES-ExplicitGES & CMA-ES-ImplicitGES & GES      \\ \midrule
Binary Classification      & 10.973 & 5.263              & 4.471              & 4.761 \\
Multi-class Classification & 14.827 & 7.300              & 6.310              & 6.850 \\  \midrule
Average                    & ${\sim}12.9$     & ${\sim}6.3$             & ${\sim}5.4$               & ${\sim}5.8$     \\ \bottomrule
\end{tabular}%
}
\end{table}
}


\changed{
\section{Supplements for Experiments Following the AutoML Benchmark \citep{automl_benchmark_2022}}

\subsection{Statistical Test with Critical Difference Plots}
\label{apdx/cd_plots}
Following the AutoML benchmark \citep{automl_benchmark_2022}, we perform a statistical test using a Friedman test with a Nemenyi post hoc test ($\alpha = 0.05$). We implemented the tests re-using code from Autorank \citep{Herbold2020}.

We first calculate the mean rank of each method for each collection of datasets, i.e., the subset of datasets for binary or multi-class classification for both metrics. 
Then, we use the Friedman test as an omnibus test to try to reject the null hypothesis that there is no difference between the methods. 
Only if the Friedman test is significant and rejects the null hypothesis, we perform a Nemenyi post hoc test. 
The test calculates a critical difference (CD). 
Finally, we determine if the difference between methods is significant by verifying that their difference in mean rank is greater than the CD. Otherwise, the difference is not significant. 
We show the results of the Nemenyi post hoc test using CD plots, whereby a horizontal bar connects methods that are not significantly different. 

\subsection{Normalized Improvement}
\label{apdx/norm_improvement}
Our implementation of normalized improvement follows the AutoML benchmark \citep{automl_benchmark_2022}.
That is, we scale the scores for a dataset such that $-1$ is equal to the score of the single best model, and $0$ is equal to the score of the best method on the dataset.

Formally, we normalise the score $s_{D}$ of a method for a dataset $D$ using:  
\begin{equation}
    \frac{s_{D} - s_{D}^{b}}{s_{D}^* - s_{D}^{b}} - 1, 
\end{equation}
with the score of the baseline $s_{D}^{b}$ and the best-observed score for the dataset $s_{D}^*$.
We assume that higher scores are always better. 
 
We extend this definition for the edge cases where no method is better than the baseline, \emph{i.e.}, $s_{D}^* - s_{D}^{b} = 0$.
We suppose that this edge case never happened in the AutoML benchmark. Otherwise, their definition and implementation would have crashed.
In our setting, such an edge case can happen due to overfitting such that the ensemble methods becomes worse than the single best model. 

If the edge case happens, we set the score of all methods worse than the baseline to $-10$, following a penalization-like approach (\emph{e.g.}, PAR10 from Algorithm Selection \citep{DBLP:journals/ai/LindauerRK19}). 
Methods for which $s_{D} - s_{D}^{b} = 0$ holds are assigned a score of $-1$. 



\section{Overview of Performance per Dataset}
\label{apdx/overview_of_performance_per_dataset}

Here we provide the mean and standard deviation over all folds per dataset.
The different combinations of metric and classification tasks are split into separate tables, see Tables \ref{tabo/1},\ref{tabo/2}, \ref{tabo/3}, and \ref{tabo/4}. 
}

\begin{table}[h]
\caption{\changed{\textbf{Balanced Accuracy - Binary:} The mean and standard deviation of the test score over all folds for each method. The best method per dataset is shown in bold.}}
\label{tabo/1}
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}lccccc@{}}
\toprule
Dataset                & CMA-ES                     & CMA-ES-ExplicitGES         & GES               & SingleBest        & Stacking                   \\ \midrule
APSFailure             & \textbf{0.9563 (Â± 0.0118)} & 0.8957 (Â± 0.0207)          & 0.8957 (Â± 0.0205) & 0.8961 (Â± 0.0208) & 0.8977 (Â± 0.0236)          \\
Amazon\_employee\_access & \textbf{0.8262 (Â± 0.0153)} & 0.6883 (Â± 0.0104)          & 0.6884 (Â± 0.0105) & 0.6883 (Â± 0.0104) & 0.6902 (Â± 0.0147)          \\
Australian             & 0.8598 (Â± 0.0293)          & 0.8615 (Â± 0.0237)          & 0.8579 (Â± 0.0357) & 0.8605 (Â± 0.0174) & \textbf{0.871 (Â± 0.0247)}  \\
Bioresponse            & \textbf{0.8085 (Â± 0.0175)} & 0.805 (Â± 0.0218)           & 0.8052 (Â± 0.0228) & 0.8045 (Â± 0.0222) & 0.8066 (Â± 0.02)            \\
Click\_prediction\_small & \textbf{0.6461 (Â± 0.0081)} & 0.5535 (Â± 0.0052)          & 0.5535 (Â± 0.0052) & 0.5535 (Â± 0.0052) & 0.5504 (Â± 0.0057)          \\
Higgs                  & 0.7414 (Â± 0.0012)          & 0.7408 (Â± 0.001)           & 0.7408 (Â± 0.001)  & 0.7407 (Â± 0.001)  & \textbf{0.7417 (Â± 0.0012)} \\
Internet-Advertisements          & \textbf{0.9454 (Â± 0.0251)} & 0.9438 (Â± 0.0247) & 0.9452 (Â± 0.0247) & 0.9365 (Â± 0.0243)          & 0.9445 (Â± 0.0219) \\
KDDCup09-Upselling     & \textbf{0.7906 (Â± 0.0076)} & 0.7818 (Â± 0.0103)          & 0.7823 (Â± 0.0102) & 0.7817 (Â± 0.01)   & 0.7683 (Â± 0.009)           \\
KDDCup09\_appetency     & \textbf{0.6622 (Â± 0.0622)} & 0.5 (Â± 0.0)                & 0.5 (Â± 0.0)       & 0.5 (Â± 0.0)       & 0.5005 (Â± 0.0018)          \\
MiniBooNE              & \textbf{0.9442 (Â± 0.0051)} & 0.9399 (Â± 0.0028)          & 0.94 (Â± 0.0028)   & 0.9399 (Â± 0.0028) & 0.9364 (Â± 0.0031)          \\
PhishingWebsites       & \textbf{0.9723 (Â± 0.0037)} & 0.972 (Â± 0.0046)           & 0.9722 (Â± 0.0049) & 0.9705 (Â± 0.0046) & 0.9715 (Â± 0.0045)          \\
Satellite              & \textbf{0.9281 (Â± 0.0418)} & 0.8317 (Â± 0.0704)          & 0.8248 (Â± 0.0712) & 0.8316 (Â± 0.0703) & 0.832 (Â± 0.078)            \\
ada                    & \textbf{0.8102 (Â± 0.0299)} & 0.7956 (Â± 0.0293)          & 0.7953 (Â± 0.0289) & 0.7943 (Â± 0.0277) & 0.7887 (Â± 0.0282)          \\
adult                  & \textbf{0.842 (Â± 0.0069)}  & 0.8002 (Â± 0.0069)          & 0.7993 (Â± 0.0069) & 0.8002 (Â± 0.0069) & 0.8007 (Â± 0.0067)          \\
airlines               & 0.6422 (Â± 0.0025)          & \textbf{0.6573 (Â± 0.0016)} & 0.6573 (Â± 0.0017) & 0.6569 (Â± 0.0014) & 0.649 (Â± 0.0017)           \\
albert                 & 0.7049 (Â± 0.0021)          & 0.7046 (Â± 0.0023)          & 0.7047 (Â± 0.0023) & 0.7043 (Â± 0.0026) & \textbf{0.7051 (Â± 0.0024)} \\
arcene                 & \textbf{0.8383 (Â± 0.1616)} & 0.7967 (Â± 0.1904)          & 0.8075 (Â± 0.1911) & 0.7808 (Â± 0.1646) & 0.785 (Â± 0.1807)           \\
bank-marketing         & \textbf{0.8722 (Â± 0.0097)} & 0.7447 (Â± 0.0117)          & 0.7448 (Â± 0.012)  & 0.7448 (Â± 0.0116) & 0.7348 (Â± 0.0139)          \\
blood-transfusion-service-center & 0.6315 (Â± 0.0378)          & 0.642 (Â± 0.0476)  & 0.6419 (Â± 0.0437) & \textbf{0.6514 (Â± 0.0398)} & 0.6233 (Â± 0.0403) \\
christine              & \textbf{0.7556 (Â± 0.0133)} & 0.7521 (Â± 0.0173)          & 0.7517 (Â± 0.0163) & 0.7517 (Â± 0.0163) & 0.7523 (Â± 0.0191)          \\
churn                  & \textbf{0.9075 (Â± 0.0248)} & 0.8912 (Â± 0.0211)          & 0.8896 (Â± 0.0188) & 0.8809 (Â± 0.0248) & 0.9001 (Â± 0.024)           \\
credit-g               & \textbf{0.689 (Â± 0.0369)}  & 0.6843 (Â± 0.0494)          & 0.6855 (Â± 0.0474) & 0.684 (Â± 0.0472)  & 0.6843 (Â± 0.0432)          \\
gina                   & \textbf{0.9607 (Â± 0.0153)} & 0.9563 (Â± 0.0179)          & 0.9563 (Â± 0.0179) & 0.9563 (Â± 0.0179) & 0.956 (Â± 0.0187)           \\
guillermo              & \textbf{0.8411 (Â± 0.0102)} & 0.8171 (Â± 0.0094)          & 0.8171 (Â± 0.0094) & 0.8171 (Â± 0.0094) & 0.8307 (Â± 0.0111)          \\
jasmine                & 0.8167 (Â± 0.0169)          & 0.8167 (Â± 0.0186)          & 0.8154 (Â± 0.0183) & 0.8154 (Â± 0.0183) & \textbf{0.8231 (Â± 0.0182)} \\
kc1                    & \textbf{0.7021 (Â± 0.0329)} & 0.6423 (Â± 0.0357)          & 0.6423 (Â± 0.0357) & 0.6423 (Â± 0.0357) & 0.6453 (Â± 0.0415)          \\
kick                   & \textbf{0.6985 (Â± 0.0139)} & 0.6231 (Â± 0.0068)          & 0.6231 (Â± 0.0068) & 0.6231 (Â± 0.0068) & 0.6258 (Â± 0.0086)          \\
kr-vs-kp               & 0.9947 (Â± 0.005)           & 0.9934 (Â± 0.0045)          & 0.9935 (Â± 0.0042) & 0.9932 (Â± 0.005)  & \textbf{0.9956 (Â± 0.0051)} \\
madeline               & \textbf{0.8767 (Â± 0.011)}  & 0.8694 (Â± 0.0192)          & 0.8688 (Â± 0.0185) & 0.8691 (Â± 0.0187) & 0.8716 (Â± 0.0176)          \\
nomao                  & \textbf{0.9733 (Â± 0.0031)} & 0.9668 (Â± 0.0027)          & 0.9666 (Â± 0.0027) & 0.9666 (Â± 0.0027) & 0.9663 (Â± 0.0032)          \\
numerai28.6            & \textbf{0.5209 (Â± 0.0047)} & 0.5192 (Â± 0.0046)          & 0.5194 (Â± 0.0045) & 0.5196 (Â± 0.0042) & 0.5202 (Â± 0.0046)          \\
ozone-level-8hr        & \textbf{0.7963 (Â± 0.0398)} & 0.7203 (Â± 0.0447)          & 0.7203 (Â± 0.0447) & 0.7201 (Â± 0.045)  & 0.6814 (Â± 0.0649)          \\
pc4                    & \textbf{0.8571 (Â± 0.0412)} & 0.8011 (Â± 0.0561)          & 0.8014 (Â± 0.0551) & 0.8019 (Â± 0.0526) & 0.7417 (Â± 0.0598)          \\
philippine             & 0.7819 (Â± 0.019)           & 0.7814 (Â± 0.0149)          & 0.7795 (Â± 0.0161) & 0.7798 (Â± 0.016)  & \textbf{0.7827 (Â± 0.0167)} \\
phoneme                & \textbf{0.9141 (Â± 0.0148)} & 0.8931 (Â± 0.0191)          & 0.8938 (Â± 0.0178) & 0.8905 (Â± 0.0216) & 0.8987 (Â± 0.0206)          \\
porto-seguro           & \textbf{0.5487 (Â± 0.0425)} & 0.5001 (Â± 0.0003)          & 0.5 (Â± 0.0002)    & 0.5001 (Â± 0.0003) & 0.5004 (Â± 0.0006)          \\
qsar-biodeg            & 0.8568 (Â± 0.0335)          & 0.8504 (Â± 0.034)           & 0.8469 (Â± 0.0331) & 0.849 (Â± 0.0342)  & \textbf{0.8568 (Â± 0.0379)} \\
riccardo               & \textbf{0.9986 (Â± 0.0008)} & 0.9986 (Â± 0.0004)          & 0.9985 (Â± 0.0006) & 0.9985 (Â± 0.0007) & 0.9984 (Â± 0.0007)          \\
sf-police-incidents    & \textbf{0.6279 (Â± 0.0117)} & 0.5255 (Â± 0.001)           & 0.5256 (Â± 0.0009) & 0.5248 (Â± 0.0009) & 0.5207 (Â± 0.0005)          \\
sylvine                & \textbf{0.9518 (Â± 0.0073)} & 0.95 (Â± 0.0049)            & 0.9504 (Â± 0.005)  & 0.9506 (Â± 0.0051) & 0.9504 (Â± 0.0068)          \\
wilt                   & \textbf{0.9542 (Â± 0.0312)} & 0.9232 (Â± 0.0482)          & 0.9272 (Â± 0.0401) & 0.9291 (Â± 0.0382) & 0.9057 (Â± 0.0479)          \\ \bottomrule
\end{tabular}%
}
\end{table}

\begin{table}[]
\caption{\changed{\textbf{Balanced Accuracy - Multi-class:} The mean and standard deviation of the test score over all folds for each method. The best method per dataset is shown in bold.}}
\label{tabo/2}
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}lccccc@{}}
\toprule
Dataset       & CMA-ES                     & CMA-ES-ExplicitGES         & GES                        & SingleBest                 & Stacking                   \\ \midrule
Diabetes130US                          & \textbf{0.4946 (Â± 0.0066)} & 0.4498 (Â± 0.0032) & 0.4495 (Â± 0.0038) & 0.4493 (Â± 0.0037) & 0.4526 (Â± 0.0049) \\
Fashion-MNIST & \textbf{0.9112 (Â± 0.0037)} & 0.9098 (Â± 0.0034)          & 0.91 (Â± 0.0033)            & 0.909 (Â± 0.0036)           & 0.9097 (Â± 0.0044)          \\
GesturePhaseSegmentationProcessed      & \textbf{0.737 (Â± 0.0202)}  & 0.7146 (Â± 0.0182) & 0.7141 (Â± 0.0182) & 0.7152 (Â± 0.0171) & 0.7204 (Â± 0.0154) \\
KDDCup99      & 0.7672 (Â± 0.0356)          & \textbf{0.7681 (Â± 0.0358)} & 0.7607 (Â± 0.0392)          & 0.7268 (Â± 0.0421)          & 0.7297 (Â± 0.0387)          \\
amazon-commerce-reviews                & \textbf{0.8547 (Â± 0.0301)} & 0.85 (Â± 0.028)    & 0.8533 (Â± 0.029)  & 0.8307 (Â± 0.0352) & 0.8353 (Â± 0.0314) \\
car           & 0.997 (Â± 0.0057)           & 0.9954 (Â± 0.0112)          & 0.9949 (Â± 0.0115)          & 0.9954 (Â± 0.0112)          & \textbf{0.9989 (Â± 0.0023)} \\
cmc           & 0.5377 (Â± 0.0453)          & \textbf{0.538 (Â± 0.0448)}  & 0.5298 (Â± 0.0441)          & 0.526 (Â± 0.0375)           & 0.5321 (Â± 0.039)           \\
cnae-9        & 0.9611 (Â± 0.0246)          & 0.962 (Â± 0.0245)           & \textbf{0.9657 (Â± 0.0175)} & 0.9556 (Â± 0.0275)          & 0.963 (Â± 0.0214)           \\
connect-4     & \textbf{0.7834 (Â± 0.0074)} & 0.71 (Â± 0.0081)            & 0.7096 (Â± 0.0081)          & 0.7101 (Â± 0.0078)          & 0.7082 (Â± 0.0058)          \\
covertype     & \textbf{0.9707 (Â± 0.0032)} & 0.9579 (Â± 0.0034)          & 0.9577 (Â± 0.0035)          & 0.9579 (Â± 0.0037)          & 0.953 (Â± 0.0029)           \\
dilbert       & \textbf{0.9944 (Â± 0.0012)} & 0.9934 (Â± 0.0015)          & 0.9936 (Â± 0.0027)          & 0.992 (Â± 0.0028)           & 0.994 (Â± 0.002)            \\
dionis        & \textbf{0.8354 (Â± 0.0033)} & 0.8315 (Â± 0.0022)          & 0.8316 (Â± 0.0021)          & 0.826 (Â± 0.0018)           & 0.8332 (Â± 0.0015)          \\
dna           & \textbf{0.9661 (Â± 0.0117)} & 0.9653 (Â± 0.011)           & 0.9635 (Â± 0.0096)          & 0.9612 (Â± 0.0085)          & 0.9645 (Â± 0.0141)          \\
eucalyptus    & 0.691 (Â± 0.0472)           & 0.6806 (Â± 0.0365)          & 0.6799 (Â± 0.0418)          & 0.6901 (Â± 0.0476)          & \textbf{0.6937 (Â± 0.0673)} \\
fabert        & \textbf{0.7164 (Â± 0.0094)} & 0.7094 (Â± 0.0108)          & 0.7102 (Â± 0.0093)          & 0.7104 (Â± 0.0124)          & 0.7123 (Â± 0.0124)          \\
first-order-theorem-proving            & \textbf{0.5121 (Â± 0.0132)} & 0.497 (Â± 0.0282)  & 0.4941 (Â± 0.0205) & 0.4858 (Â± 0.0233) & 0.4913 (Â± 0.0229) \\
helena        & \textbf{0.2491 (Â± 0.0057)} & 0.2411 (Â± 0.0066)          & 0.2414 (Â± 0.0069)          & 0.2283 (Â± 0.0057)          & 0.2092 (Â± 0.004)           \\
jannis        & \textbf{0.6546 (Â± 0.0129)} & 0.5691 (Â± 0.0046)          & 0.5691 (Â± 0.0046)          & 0.5692 (Â± 0.0051)          & 0.5746 (Â± 0.0048)          \\
jungle\_chess\_2pcs\_raw\_endgame\_complete & \textbf{0.9808 (Â± 0.0047)} & 0.9703 (Â± 0.0065) & 0.9707 (Â± 0.0065) & 0.9707 (Â± 0.0063) & 0.9757 (Â± 0.0053) \\
mfeat-factors & 0.9805 (Â± 0.006)           & 0.98 (Â± 0.0062)            & 0.9815 (Â± 0.0047)          & \textbf{0.9825 (Â± 0.0054)} & 0.979 (Â± 0.0088)           \\
micro-mass    & 0.9189 (Â± 0.0403)          & \textbf{0.9227 (Â± 0.0378)} & 0.9152 (Â± 0.0436)          & 0.9072 (Â± 0.0517)          & 0.9017 (Â± 0.0401)          \\
okcupid-stem  & \textbf{0.7 (Â± 0.0088)}    & 0.5638 (Â± 0.0133)          & 0.5635 (Â± 0.0137)          & 0.5636 (Â± 0.0136)          & 0.5505 (Â± 0.0094)          \\
robert        & 0.5136 (Â± 0.0102)          & 0.5134 (Â± 0.0119)          & 0.5124 (Â± 0.0108)          & \textbf{0.5159 (Â± 0.012)}  & 0.51 (Â± 0.0098)            \\
segment       & 0.9442 (Â± 0.0141)          & \textbf{0.9463 (Â± 0.0152)} & 0.9455 (Â± 0.0126)          & 0.9442 (Â± 0.0139)          & 0.9429 (Â± 0.0127)          \\
shuttle       & 0.8543 (Â± 0.009)           & 0.8534 (Â± 0.0117)          & 0.8534 (Â± 0.0117)          & 0.8542 (Â± 0.009)           & \textbf{0.9791 (Â± 0.0476)} \\
steel-plates-fault                     & \textbf{0.8491 (Â± 0.0211)} & 0.828 (Â± 0.0167)  & 0.8234 (Â± 0.0198) & 0.8279 (Â± 0.0194) & 0.8167 (Â± 0.0197) \\
vehicle       & 0.8519 (Â± 0.027)           & 0.8589 (Â± 0.0297)          & 0.8553 (Â± 0.0269)          & 0.8552 (Â± 0.0264)          & \textbf{0.8645 (Â± 0.0298)} \\
volkert       & \textbf{0.7197 (Â± 0.0061)} & 0.6802 (Â± 0.0083)          & 0.6795 (Â± 0.008)           & 0.6805 (Â± 0.0086)          & 0.6767 (Â± 0.0066)          \\
wine-quality-white                     & \textbf{0.4279 (Â± 0.0438)} & 0.3938 (Â± 0.0417) & 0.3938 (Â± 0.0347) & 0.4023 (Â± 0.0333) & 0.3917 (Â± 0.0386) \\
yeast         & \textbf{0.5213 (Â± 0.0644)} & 0.5113 (Â± 0.0556)          & 0.5006 (Â± 0.0752)          & 0.4955 (Â± 0.0732)          & 0.516 (Â± 0.0569)           \\ \bottomrule
\end{tabular}%
}
\end{table}

\begin{table}[]
\caption{\changed{\textbf{ROC AUC - Binary:} The mean and standard deviation of the test score over all folds for each method. The best method per dataset is shown in bold.}}
\label{tabo/3}
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}lccccc@{}}
\toprule
Dataset             & CMA-ES                     & CMA-ES-ExplicitGES         & GES                        & SingleBest                 & Stacking                   \\ \midrule
APSFailure          & 0.9923 (Â± 0.0021)          & 0.9927 (Â± 0.0016)          & \textbf{0.9927 (Â± 0.0016)} & 0.9925 (Â± 0.0017)          & 0.992 (Â± 0.0015)           \\
Amazon\_employee\_access           & 0.901 (Â± 0.0125)  & \textbf{0.9012 (Â± 0.0127)} & 0.9008 (Â± 0.0126) & 0.9003 (Â± 0.0119)          & 0.8967 (Â± 0.0126)          \\
Australian          & 0.9399 (Â± 0.0189)          & 0.9403 (Â± 0.0174)          & 0.9403 (Â± 0.017)           & 0.9402 (Â± 0.0187)          & \textbf{0.9437 (Â± 0.0171)} \\
Bioresponse         & 0.8843 (Â± 0.0178)          & 0.8859 (Â± 0.0163)          & \textbf{0.886 (Â± 0.016)}   & 0.8807 (Â± 0.0166)          & 0.8853 (Â± 0.016)           \\
Click\_prediction\_small           & 0.7098 (Â± 0.0116) & \textbf{0.7102 (Â± 0.0118)} & 0.7101 (Â± 0.0118) & 0.7086 (Â± 0.0122)          & 0.71 (Â± 0.0119)            \\
Higgs               & \textbf{0.8256 (Â± 0.0008)} & 0.8244 (Â± 0.0008)          & 0.8243 (Â± 0.0008)          & 0.8244 (Â± 0.0008)          & 0.8254 (Â± 0.0008)          \\
Internet-Advertisements          & 0.9859 (Â± 0.0106) & 0.9844 (Â± 0.0127)          & 0.9851 (Â± 0.0129) & 0.9845 (Â± 0.0121)          & \textbf{0.9866 (Â± 0.0106)} \\
KDDCup09-Upselling  & 0.9085 (Â± 0.0067)          & \textbf{0.9085 (Â± 0.0066)} & 0.9085 (Â± 0.0068)          & 0.9082 (Â± 0.0067)          & 0.8997 (Â± 0.0074)          \\
KDDCup09\_appetency  & 0.8484 (Â± 0.0128)          & 0.8487 (Â± 0.0131)          & \textbf{0.8487 (Â± 0.0131)} & 0.8462 (Â± 0.0128)          & 0.8373 (Â± 0.0129)          \\
MiniBooNE           & \textbf{0.9874 (Â± 0.0011)} & 0.9873 (Â± 0.001)           & 0.9873 (Â± 0.001)           & 0.9871 (Â± 0.001)           & 0.9866 (Â± 0.0012)          \\
PhishingWebsites    & 0.9967 (Â± 0.0015)          & 0.9969 (Â± 0.001)           & \textbf{0.997 (Â± 0.001)}   & 0.9955 (Â± 0.0015)          & 0.9968 (Â± 0.001)           \\
Satellite           & 0.9682 (Â± 0.0883)          & \textbf{0.9946 (Â± 0.0066)} & 0.9945 (Â± 0.0065)          & 0.9944 (Â± 0.0066)          & 0.9944 (Â± 0.006)           \\
ada                 & 0.9199 (Â± 0.0174)          & \textbf{0.9203 (Â± 0.0177)} & 0.9203 (Â± 0.0176)          & 0.9198 (Â± 0.0179)          & 0.9202 (Â± 0.0179)          \\
adult               & \textbf{0.9318 (Â± 0.004)}  & 0.9316 (Â± 0.0041)          & 0.9316 (Â± 0.0041)          & 0.9312 (Â± 0.0044)          & 0.9303 (Â± 0.0038)          \\
airlines            & 0.7064 (Â± 0.0024)          & 0.7242 (Â± 0.0018)          & \textbf{0.7245 (Â± 0.0019)} & 0.7233 (Â± 0.0019)          & 0.7084 (Â± 0.0022)          \\
albert              & \textbf{0.7782 (Â± 0.0025)} & 0.778 (Â± 0.0025)           & 0.7781 (Â± 0.0025)          & 0.7776 (Â± 0.0026)          & 0.7781 (Â± 0.0025)          \\
arcene              & \textbf{0.913 (Â± 0.1134)}  & 0.8812 (Â± 0.1379)          & 0.8812 (Â± 0.1379)          & 0.8447 (Â± 0.1967)          & 0.873 (Â± 0.1585)           \\
bank-marketing      & 0.9405 (Â± 0.0062)          & 0.9406 (Â± 0.0061)          & \textbf{0.9406 (Â± 0.0062)} & 0.9395 (Â± 0.0064)          & 0.9399 (Â± 0.0062)          \\
blood-transfusion-service-center & 0.7352 (Â± 0.059)  & 0.7383 (Â± 0.0557)          & 0.7394 (Â± 0.0552) & \textbf{0.7487 (Â± 0.0485)} & 0.7437 (Â± 0.0571)          \\
christine           & \textbf{0.8274 (Â± 0.0137)} & 0.8266 (Â± 0.0132)          & 0.8266 (Â± 0.0134)          & 0.8258 (Â± 0.0142)          & 0.8274 (Â± 0.0135)          \\
churn               & \textbf{0.9348 (Â± 0.0164)} & 0.923 (Â± 0.0252)           & 0.924 (Â± 0.0249)           & 0.9221 (Â± 0.0247)          & 0.929 (Â± 0.0195)           \\
credit-g            & 0.7926 (Â± 0.0329)          & 0.797 (Â± 0.0367)           & 0.7984 (Â± 0.0372)          & 0.7894 (Â± 0.0323)          & \textbf{0.7999 (Â± 0.0377)} \\
gina                & \textbf{0.992 (Â± 0.0048)}  & 0.9914 (Â± 0.0052)          & 0.9914 (Â± 0.0052)          & 0.991 (Â± 0.0057)           & 0.9898 (Â± 0.0062)          \\
guillermo           & \textbf{0.9216 (Â± 0.0061)} & 0.9124 (Â± 0.0081)          & 0.9119 (Â± 0.0077)          & 0.9117 (Â± 0.0077)          & 0.9214 (Â± 0.0058)          \\
jasmine             & 0.884 (Â± 0.0157)           & 0.8856 (Â± 0.0165)          & 0.8857 (Â± 0.0166)          & 0.8836 (Â± 0.0178)          & \textbf{0.8858 (Â± 0.0172)} \\
kc1                 & 0.8338 (Â± 0.0409)          & 0.8371 (Â± 0.0382)          & 0.8378 (Â± 0.0367)          & 0.8335 (Â± 0.0426)          & \textbf{0.839 (Â± 0.0354)}  \\
kick                & 0.7913 (Â± 0.0062)          & \textbf{0.7913 (Â± 0.0062)} & 0.7912 (Â± 0.0062)          & 0.7898 (Â± 0.0057)          & 0.7897 (Â± 0.0062)          \\
kr-vs-kp            & 0.9983 (Â± 0.0043)          & \textbf{0.9998 (Â± 0.0002)} & 0.9998 (Â± 0.0002)          & 0.9998 (Â± 0.0002)          & 0.9994 (Â± 0.0012)          \\
madeline            & \textbf{0.9471 (Â± 0.0078)} & 0.9447 (Â± 0.0086)          & 0.9447 (Â± 0.0087)          & 0.9394 (Â± 0.0082)          & 0.9458 (Â± 0.0094)          \\
nomao               & \textbf{0.9964 (Â± 0.0006)} & 0.9964 (Â± 0.0006)          & 0.9964 (Â± 0.0006)          & 0.9963 (Â± 0.0007)          & 0.996 (Â± 0.0005)           \\
numerai28.6         & 0.5301 (Â± 0.0045)          & 0.5305 (Â± 0.0044)          & \textbf{0.5305 (Â± 0.0045)} & 0.5297 (Â± 0.0044)          & 0.5302 (Â± 0.0045)          \\
ozone-level-8hr     & 0.9267 (Â± 0.0287)          & 0.9336 (Â± 0.0184)          & \textbf{0.9338 (Â± 0.0177)} & 0.9329 (Â± 0.0193)          & 0.9328 (Â± 0.0249)          \\
pc4                 & 0.9515 (Â± 0.0191)          & \textbf{0.9526 (Â± 0.0191)} & 0.9524 (Â± 0.0195)          & 0.9513 (Â± 0.0183)          & 0.9519 (Â± 0.0191)          \\
philippine          & 0.877 (Â± 0.0129)           & 0.8756 (Â± 0.013)           & 0.8754 (Â± 0.0131)          & \textbf{0.8772 (Â± 0.0117)} & 0.8754 (Â± 0.0132)          \\
phoneme             & \textbf{0.9717 (Â± 0.0087)} & 0.9684 (Â± 0.0094)          & 0.9684 (Â± 0.0094)          & 0.9678 (Â± 0.0091)          & 0.9705 (Â± 0.0092)          \\
porto-seguro        & 0.5172 (Â± 0.0383)          & \textbf{0.5172 (Â± 0.0382)} & 0.5172 (Â± 0.0382)          & 0.5172 (Â± 0.0382)          & 0.5172 (Â± 0.0381)          \\
qsar-biodeg         & 0.9398 (Â± 0.0313)          & 0.9436 (Â± 0.029)           & \textbf{0.9436 (Â± 0.0289)} & 0.9355 (Â± 0.0337)          & 0.9418 (Â± 0.0307)          \\
riccardo            & \textbf{0.9999 (Â± 0.0001)} & 0.9998 (Â± 0.0001)          & 0.9998 (Â± 0.0001)          & 0.9997 (Â± 0.0002)          & 0.9998 (Â± 0.0001)          \\
sf-police-incidents & 0.6874 (Â± 0.0017)          & 0.6886 (Â± 0.0019)          & \textbf{0.6886 (Â± 0.0019)} & 0.6873 (Â± 0.0017)          & 0.684 (Â± 0.0019)           \\
sylvine             & 0.9863 (Â± 0.0071)          & \textbf{0.9889 (Â± 0.0033)} & 0.9889 (Â± 0.0034)          & 0.988 (Â± 0.0039)           & 0.9884 (Â± 0.0036)          \\
wilt                & 0.994 (Â± 0.0071)           & 0.9946 (Â± 0.0091)          & \textbf{0.9948 (Â± 0.0087)} & 0.9946 (Â± 0.0092)          & 0.9943 (Â± 0.0084)          \\ \bottomrule
\end{tabular}%
}
\end{table}


\begin{table}[]
\caption{\changed{\textbf{ROC AUC - Multi-class:} The mean and standard deviation of the test score over all folds for each method. The best method per dataset is shown in bold.}}
\label{tabo/4}
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}lccccc@{}}
\toprule
Dataset            & CMA-ES                     & CMA-ES-ExplicitGES         & GES                        & SingleBest        & Stacking                   \\ \midrule
Diabetes130US      & 0.7127 (Â± 0.0054)          & \textbf{0.713 (Â± 0.0055)}  & 0.713 (Â± 0.0055)           & 0.7129 (Â± 0.0055) & 0.7115 (Â± 0.0053)          \\
Fashion-MNIST      & 0.9942 (Â± 0.0004)          & \textbf{0.9943 (Â± 0.0004)} & 0.9943 (Â± 0.0004)          & 0.9941 (Â± 0.0004) & 0.9937 (Â± 0.0005)          \\
GesturePhaseSegmentationProcessed      & \textbf{0.94 (Â± 0.0066)}   & 0.9382 (Â± 0.0064) & 0.938 (Â± 0.0064)           & 0.9374 (Â± 0.0063) & 0.937 (Â± 0.0074)  \\
KDDCup99           & \textbf{0.9999 (Â± 0.0001)} & 0.8898 (Â± 0.0233)          & 0.8935 (Â± 0.0148)          & 0.892 (Â± 0.0174)  & 0.9997 (Â± 0.0005)          \\
amazon-commerce-reviews                & \textbf{0.995 (Â± 0.0019)}  & 0.994 (Â± 0.0024)  & 0.9941 (Â± 0.0022)          & 0.9924 (Â± 0.0037) & 0.9919 (Â± 0.0021) \\
car                & 0.9993 (Â± 0.0022)          & 1.0 (Â± 0.0001)             & 1.0 (Â± 0.0001)             & 1.0 (Â± 0.0001)    & \textbf{1.0 (Â± 0.0)}       \\
cmc                & 0.7353 (Â± 0.0338)          & 0.739 (Â± 0.0345)           & \textbf{0.7391 (Â± 0.0344)} & 0.7296 (Â± 0.0332) & 0.7358 (Â± 0.0344)          \\
cnae-9             & 0.9974 (Â± 0.0035)          & \textbf{0.9985 (Â± 0.0018)} & 0.9985 (Â± 0.0017)          & 0.9983 (Â± 0.0018) & 0.9984 (Â± 0.0016)          \\
connect-4          & 0.9494 (Â± 0.0035)          & \textbf{0.9496 (Â± 0.0035)} & 0.9496 (Â± 0.0035)          & 0.9496 (Â± 0.0035) & 0.9472 (Â± 0.0034)          \\
covertype          & 0.9994 (Â± 0.0)             & \textbf{0.9995 (Â± 0.0)}    & 0.9995 (Â± 0.0)             & 0.9994 (Â± 0.0)    & 0.9993 (Â± 0.0001)          \\
dilbert            & 0.9999 (Â± 0.0001)          & 0.9999 (Â± 0.0)             & 0.9999 (Â± 0.0)             & 0.9999 (Â± 0.0002) & \textbf{1.0 (Â± 0.0)}       \\
dionis             & 0.9937 (Â± 0.0027)          & 0.9941 (Â± 0.0025)          & 0.9941 (Â± 0.0025)          & 0.9891 (Â± 0.0051) & \textbf{0.997 (Â± 0.0004)}  \\
dna                & 0.9943 (Â± 0.0033)          & 0.9952 (Â± 0.0025)          & \textbf{0.9953 (Â± 0.0026)} & 0.9947 (Â± 0.0026) & 0.9947 (Â± 0.003)           \\
eucalyptus         & 0.9293 (Â± 0.0173)          & 0.9321 (Â± 0.0154)          & 0.9314 (Â± 0.017)           & 0.9296 (Â± 0.0142) & \textbf{0.9347 (Â± 0.0156)} \\
fabert             & 0.9457 (Â± 0.0038)          & \textbf{0.946 (Â± 0.0038)}  & 0.9458 (Â± 0.0038)          & 0.9445 (Â± 0.004)  & 0.9434 (Â± 0.0044)          \\
first-order-theorem-proving            & 0.8468 (Â± 0.0114)          & 0.8523 (Â± 0.0094) & \textbf{0.8524 (Â± 0.0095)} & 0.8468 (Â± 0.0109) & 0.8408 (Â± 0.0115) \\
helena             & 0.8992 (Â± 0.0027)          & 0.8999 (Â± 0.0027)          & \textbf{0.9 (Â± 0.0027)}    & 0.8986 (Â± 0.0028) & 0.8795 (Â± 0.0014)          \\
jannis             & 0.8872 (Â± 0.0032)          & \textbf{0.8887 (Â± 0.0031)} & 0.8887 (Â± 0.0031)          & 0.8872 (Â± 0.0032) & 0.8846 (Â± 0.0027)          \\
jungle\_chess\_2pcs\_raw\_endgame\_complete & \textbf{0.9992 (Â± 0.0003)} & 0.999 (Â± 0.0002)  & 0.999 (Â± 0.0002)           & 0.999 (Â± 0.0002)  & 0.9991 (Â± 0.0003) \\
mfeat-factors      & 0.9989 (Â± 0.0017)          & \textbf{0.9996 (Â± 0.0004)} & 0.9995 (Â± 0.0005)          & 0.9994 (Â± 0.0007) & 0.9992 (Â± 0.0007)          \\
micro-mass         & 0.9913 (Â± 0.0167)          & 0.9956 (Â± 0.008)           & 0.9956 (Â± 0.0082)          & 0.9957 (Â± 0.0081) & \textbf{0.9976 (Â± 0.002)}  \\
okcupid-stem       & 0.8321 (Â± 0.0059)          & 0.8329 (Â± 0.0055)          & \textbf{0.8329 (Â± 0.0055)} & 0.832 (Â± 0.0057)  & 0.8277 (Â± 0.005)           \\
robert             & \textbf{0.8866 (Â± 0.0036)} & 0.8853 (Â± 0.0039)          & 0.8855 (Â± 0.004)           & 0.8843 (Â± 0.0041) & 0.8817 (Â± 0.005)           \\
segment            & 0.9964 (Â± 0.0009)          & \textbf{0.9965 (Â± 0.0013)} & 0.9965 (Â± 0.0012)          & 0.9961 (Â± 0.0014) & 0.9962 (Â± 0.0012)          \\
shuttle            & 1.0 (Â± 0.0)                & 0.9286 (Â± 0.0)             & 0.9286 (Â± 0.0)             & 0.9286 (Â± 0.0)    & \textbf{1.0 (Â± 0.0)}       \\
steel-plates-fault & 0.9663 (Â± 0.0086)          & \textbf{0.9696 (Â± 0.0048)} & 0.9694 (Â± 0.0049)          & 0.9678 (Â± 0.0062) & 0.9653 (Â± 0.0088)          \\
vehicle            & 0.9665 (Â± 0.0102)          & \textbf{0.9691 (Â± 0.0091)} & 0.9686 (Â± 0.0092)          & 0.9689 (Â± 0.0086) & 0.9683 (Â± 0.0091)          \\
volkert            & 0.9569 (Â± 0.0011)          & \textbf{0.9581 (Â± 0.001)}  & 0.9581 (Â± 0.001)           & 0.9578 (Â± 0.0011) & 0.9521 (Â± 0.0017)          \\
wine-quality-white & 0.8594 (Â± 0.0306)          & 0.8409 (Â± 0.0319)          & 0.8425 (Â± 0.0311)          & 0.8446 (Â± 0.0311) & \textbf{0.8673 (Â± 0.0339)} \\
yeast              & \textbf{0.8834 (Â± 0.035)}  & 0.8632 (Â± 0.0334)          & 0.8638 (Â± 0.0327)          & 0.8571 (Â± 0.0384) & 0.878 (Â± 0.0325)           \\ \bottomrule
\end{tabular}%
}
\end{table}







\end{document}
