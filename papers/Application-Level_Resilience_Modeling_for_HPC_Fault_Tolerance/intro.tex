\section{Introduction}
\label{sec:intro}
\begin{comment}
The continued growth of large-scale high performance computing (HPC) systems is
fueled by the two trends: continued integration of additional
functionality onto system nodes, and the increased number of
nodes (and components) in the systems.
As a result, these large-scale systems are jeopardized by
potentially increasing faults
%~\footnote{We use the terms fault and fault in their classical meaning. An \textit{fault} is a part of system/application statement that may lead to system/application failure. A \texit{Fault} is the underlying causes of fault.} 
in hardware and software~\cite{fengshui_sc13, sigmetrics09:schroeder, asplos15:vilas, dramerrors_asplos12, sc12:vilas}. %and faults.
Ensuring scientific computing integrity through the correctness of application
outcomes in the presence of faults %and faults
remains one of the grand challenges (also known as the resilience challenge) for large-scale HPC systems~\cite{9-cappello2009toward, 9-cappello2014toward}. 
\end{comment}

The high performance computing (HPC) systems are jeopardized by potentially increasing faults
%~\footnote{We use the terms fault and fault in their classical meaning. An \textit{fault} is a part of system/application statement that may lead to system/application failure. A \texit{Fault} is the underlying causes of fault.} 
in hardware and software~\cite{asplos15:vilas, fengshui_sc13}. %and faults.
Ensuring scientific computing integrity through the correctness of application outcomes in the presence of faults 
remains one of the grand challenges (also known as the resilience challenge) for HPC. 

To address the resilience challenge, we must sufficiently understand the resilience of hardware and applications.
Understanding the resilience of hardware is required to validate the hardware design against a desired failure rate target, 
and improve the efficiency of the protection used to achieve the failure rate~\cite{micro15:steven, micro14:wilkening}.
Understanding the resilience of applications is necessary to determine whether the application execution can
remain correct with fault occurrences and whether we should enforce software-based fault tolerance mechanisms 
(e.g., algorithm-based fault tolerance~\cite{ftcg_ppopp13, tc84_abft} and 
software-based checkpoint/restart~\cite{4-kharbascombining})
to ensure application result fidelity and minimize re-computation cost.
Understanding the resilience of applications is also the key to coordinate software- and hardware-based fault tolerance mechanisms~\cite{CrossLayer_icpp12, abft_ecc:SC13} to improve their efficiency. 
%prevent excessive failures from transient faults

%\textbf{paragraph2: necessity to understand application resilience}
Although researchers in the field have made significant progress on understanding the resilience of hardware based on various methodologies 
(such as (micro)architecture-level simulations~\cite{asplos10:feng, hpca09:li}, beam test on silicon~\cite{aspdac14:cher, sc14:cher}, 
and AVF analysis~\cite{isca05:mukherjee, micro03:mukherjee}),
understanding the resilience of applications largely relies on random fault injection (RFI) at the application level~\cite{europar14:calhoun, mg_ics12, bifit:sc12, sc16_gpu_fault_injection, prdc13:sharma}.  
The random fault injection injects artificial faults into application variables or computation logic,
and obtains statistical comparisons %statistics of the comparison 
of application susceptibility to faults.
%Among $N$ fault injection tests, if $M$ of them have correct application execution and outcome,
%then the application resilience can be calculated as $M/N$ (i.e., the success rate of all fault injection tests).
%%% to obtain statistics of the comparison of program susceptibility to soft errors
%and the application resilience is evaluated based on the application responses. 
%With the introduction of faults, if the application remains correct outcome, %or execution, 
%then the application is resilient. 
To ensure statistical significance and sufficient fault coverage,
this methodology has to perform a large amount of random fault injection
tests. 
%which demands a significant amount of resource and time~\cite{}.
%Sometimes, it can take days to finish evaluating the resilience of a single program element
%(e.g., an MPI communication site~\cite{}). 
%Furthermore,
 
\begin{comment}
However, FI has a fundamental limitation. 
First, because of the random nature of FI, it is often difficult to
bound the accuracy of FI. %As a result, the analysis results are often qualitative, not quantitative,
There is no quantitative guidance on how many fault injection tests should be performed.
Different number of fault injection tests can result in different conclusions about the application resilience (see Section~\ref{sec:bac}).
Second, FI tests are not deterministically repeatable,
because where and when to inject a fault are completely random.
However, having determinability on FI tests
is important for reproducing faults in the fault tolerance implementation
or prune unnecessary FI tests.
\end{comment}
However, RFI has a fundamental limitation. 
First, because of the random nature of RFI, it is often difficult to
bound the accuracy of RFI. %As a result, the analysis results are often qualitative, not quantitative,
It is difficult to know how many fault injection tests should be performed.
% number -> numbers by anzheng
Different numbers of fault injection tests can result in different conclusions on the application resilience (see Section~\ref{sec:bac}).
Although previous work~\cite{date09:leveugle} has shown the possibility of determining the number of fault injection within specific confidence level and error level, it has to estimate the application resilience (e.g., percentage of faults resulting in a crash) before fault injection, which is a priori unknown. 
Also, the fault injection results within the expected error level can still be 
different from the real application resilience~\cite{date09:leveugle}.
% few->little by anzheng
Second, RFI gives us little knowledge on how and where faults are tolerated. 
Having such knowledge is important to determine where to enforce fault tolerance mechanisms.
%not deterministically repeatable,
%because where and when to inject a fault are completely random.
%However, having determinability on FI tests
%is important for reproducing faults in the fault tolerance implementation
%or prune unnecessary FI tests.

%making them less effective to guide protection.
%Furthermore, FI demands a significant amount of resource and time, 
%because it has to repeatedly execute the application and then observe application responses. 
%The fundamental reason for slowness and inaccuracy of FI is its
%nature of execution-drivenness. It has to repeatedly execute application and then observe application responses,
%which is often slow and can be inaccurate.
%Although the community has made progress on understanding the resilience of many algorithms 
%and applications~\cite{mg_ics12, bifit:sc12, isdc13:sharma}.  
%the study on the resilience of applications (especially those mission-critical large scientific applications)
%is still limited, as a result of the above limitations of FI.
The limitation of RFI creates a major obstacle to implement efficient fault tolerance mechanisms.
Many fault tolerance strategies, such as selective protection~\cite{sc14:elliott, snl_tr11:Hoemmen} 
and cross-layer protection~\cite{CrossLayer_icpp12, abft_ecc:SC13}, will be difficult
to be enforced without sufficient information on the application resilience.
Hence, we desire a %fast and accurate 
methodology alternative to RFI to understand the application resilience.

\begin{comment}
\textbf{paragraph4: what is application-level fault masking}
Application-level fault masking refers to an application remaining with the correct outcome
because the fault is hidden by the application. For example, a corrupted bit in a data structure is overwritten by an assignment statement, hence does not cause incorrect application outcomes; a corrupted bit of a molecular representation in the Monte Carlo method-based simulation to study molecular dynamics does not matter to the application outcome, because of the statistical nature of the simulation.


software-defined resilience;
\footnote{We use the terms \textit{resilience} and \textit{fault tolerance} interchangeably in this paper.}

\textbf{paragraph4: our modeling methodology. Why we can address the limitation?}
\\

\textbf{paragraph5: summarization of our contributions}
\end{comment}

In this paper, we introduce a fundamentally new methodology to quantify and model the application resilience. Our methodology is based on an observation that at the application level, the application resilience to faults is due to application-level fault masking.
The application-level fault masking happens because of application-inherent semantics and program constructs. For example, a corrupted bit in a data structure could be overwritten by an assignment operation, hence does not cause
incorrect application outcomes; a corrupted bit of a molecular representation in the Monte Carlo method-based simulation to
study molecular dynamics may not matter to application outcomes, because of the statistical nature of the simulation.
%The application-level fault masking is independent of hardware and system software, and should be distinguished from the architecture-level fault masking.
%The architecture-level fault masking happens because an fault does not corrupt the precise semantics of the architecture. For
%example, branch mis-speculated states due to branch prediction or speculative memory disambiguation, if corrupted, will not
%cause any incorrect outcome; A data prefetch instruction with fault corruption will not cause any incorrect outcome.
%The architecture-level fault masking does not consider application semantics.

Based on the above observation, the quantification of the application resilience at the application level
is equivalent to quantifying fault masking at the application level.
By analyzing the application execution information (e.g., the architecture-independent, LLVM~\cite{llvm_lrm} dynamic instruction trace), we can accurately capture those application-level fault masking events, and provide insightful analysis on whether there is any fault tolerance and how it happens.
%provide a repeatable and deterministic approach to model the application resilience. 

In essence, RFI attempts to opportunistically capture those fault masking events:
% a->an by anzheng
an RFI test without causing incorrect application outcomes 
captures one or more fault masking events, and is counted 
to calculate the success rate (or failure rate) of all fault injection tests.
However, the random nature of RFI can miss or redundantly count fault masking events.
Depending on when and where the RFI tests happen, different RFI tests 
can result in different results when evaluating the application resilience.
%Hence, the random fault injection results may not be repeatable.
Our methodology avoids the randomness, hence avoids the limitation of
the traditional RFI.   
%Furthermore, with our methodology, counting fault masking
%invokes the application execution just for once, 
%instead of using massive repetitive executions of the application as the random FI. 
%Hence, our methodology has potential to save evaluation cost.
%In essence, it is equivalent to AVF at the architecture level. 

To capture the application-level fault masking, we classify common fault masking events
into three classes: operation-level fault masking, fault masking due to fault propagation,
and algorithm-level fault masking. The operation-level fault masking 
takes effect at individual operations (e.g., arithmetic computation and assignment), and broadly includes
value overwriting, logical and comparison operations, and value shadowing.
The fault masking due to fault propagation takes effect across operations,
and requires tracking data flows between operations of the application. %which can be time-consuming when analyzing the application execution information. We propose novel optimization techniques to accelerate the analysis without losing accuracy. 
The algorithm-level fault masking manifests at the end of the application execution.
To identify the algorithm-level fault masking, 
we introduce deterministic fault injection, and
treat the application as a black box without requiring 
%about -> of by anzheng
detailed knowledge of the algorithm/application internal mechanisms and semantics. 

Our application-level resilience modeling opens new opportunities to
examine applications and evaluate the effectiveness of application-level fault 
tolerance mechanisms. It is applicable to a number of use cases to address the resilience challenge, such as code optimization and algorithm choice.
In summary, this paper makes the following contributions:

\begin{itemize}
\item
We introduce a novel methodology to model the application resilience.
Our methodology avoids the randomness inherent in the traditional random fault injection,
and brings deterministic and insightful quantification of the application resilience, which is unprecedented.

%\item 
%Based on our new methodology, we introduce a new resilience metric to
%quantify fault masking.
%This metric enables fair comparison of the resilience across data objects.

\item
We comprehensively investigate application-level fault masking events and classify them.
Our investigation answers a primitive question: why can an application tolerate faults at the application level? 
Answering this question is fundamental for enabling resilient applications for HPC and designing efficient fault tolerance mechanisms.
%These techniques enable the identification practical 

\item
We introduce a set of techniques to identify fault masking events. 
%and reduce the cost associated with the techniques. 
Furthermore, we develop a modeling tool based on our modeling methodology and techniques.
The tool is highly configurable and extensible, making the modeling work 
practical and flexible. We apply our tool to representative, computational algorithms and two scientific applications.
We reveal how fault masking typically happens in HPC applications. 
%Our results provide valuable input for future resilient application designs
%and efficient protection designs.

\item 
Using one case study to determine the deployment of fault tolerance mechanisms, we demonstrate tangible benefits of using a model-driven approach to direct fault tolerance designs for HPC applications.

\end{itemize}
