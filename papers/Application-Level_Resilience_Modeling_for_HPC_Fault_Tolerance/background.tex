\section{Background}
\label{sec:bac}
In this section, we introduce the basic fault model for the application-level resilience modeling.
We also give an introductory description for fault masking at the application level. %and compare it with the architecture-level fault masking. 
 
\subsection{Fault Model}
%System faults that influence application execution correctness
%can be broadly divided into soft faults and hard faults.
%soft faults are inherently transient because they are caused by temporary environmental factors, such as cosmic radiation and voltage fluctuation.
%Hard faults are permanent or intermittent, and they are usually caused by aged devices or inherent manufacturing defects. 

%In this paper, we do not limit our discussion to either soft or hard faults.
In this paper, we model the impact of any fault on applications at the application level, 
and do not consider where the faults are originally generated (register, main memory, cache, etc). 
As long as the faults manifest and are propagated to the application level,
we will model if the application is resilient to those faults.  
In addition, we consider single-bit and spatial multi-bit transient faults, because those fault patterns are the most common
ones and impose a more significant threat than others~\cite{asplos15:vilas, fengshui_sc13, micro14:wilkening}. 
%Once those faults are manifested and propagated to the application level, 
%our model is applicable to study the impact of the faults on the execution correctness.  
%We do not focus on those faults that cause application crashes because of the contamination of computation logic or data objects.
%(\textbf{Dong: pending. More accurate discussion is needed.})
%The faults that do not cause application crashes pose a  major threat to the execution correctness of the application. 

In terms of the impact of faults on applications, we focus on the execution correctness. 
We define the execution correctness at the application level (in contrast to the architecture level in the related work~\cite{isca05:mukherjee, micro03:mukherjee}).
An application's execution is deemed correct as long as the outcome it produces is \textit{acceptable}.
Depending on the notion of the outcome acceptance, the correctness can refer to precise numerical integrity 
%an->a by anzheng
(e.g., the numerical outcome of a multiplication operation must be precise), 
or refer to satisfying a minimum fidelity threshold (e.g., the outcome of an iterative solver must meet certain convergence thresholds).    

\subsection{Randomness of Traditional Fault Injection}
The traditional fault injection injects faults randomly into the application.
The randomness results in uncertainty in the fault injection result. 
To motivate our modeling work, we study the randomness of traditional random fault injection.
We leverage an LLVM-based fault injection tool~\cite{europar14:calhoun} and study several  benchmarks from NPB benchmark suite 3.3.1~\cite{nas}. For each fault injection test,
this tool randomly selects an instruction and then randomly flips a bit in the output operand of the instruction. We use single-bit flip for fault injection tests,
similar to the existing work~\cite{europar14:calhoun, mg_ics12, bifit:sc12,  prdc13:sharma}.
%gpu_resilience13, bifit:sc12,  prdc13:sharma}.
We use the statistical approach in~\cite{date09:leveugle} to
quantify error level with 95\% confidence level.
A priori estimation of the application resilience is 0.5 suggested by~\cite{date09:leveugle}.
We do ten sets of fault injection tests.
The number of fault injection tests in the ten sets ranges
from 1000 to 10,000 with a stride of 1000.

Figure~\ref{fig:fi_randomness} shows the results, and uses the success rate of fault injection to evaluate the application resilience, similar to~\cite{ics08:bronevetsky, 
bifit:sc12,  prdc13:sharma}.
%gpu_resilience13, bifit:sc12,  prdc13:sharma} .
The success rate of fault injection is defined as follows: Among $N$ fault injection tests, if $M$ of them have correct application outcomes, then the
success rate is calculated as $M/N$. 
%In the figure, 
A higher value of the success rate indicates that the application is more resilient to faults.
Within the figure, we also show the margin of error (i.e., error level) for the success rate based on~\cite{date09:leveugle}.

\begin{figure}
	\begin{center}
		\includegraphics[height=0.2\textheight]{random_FI.pdf} 
		\vspace{-10pt}
		\caption{The random fault injection results with margins of error for CG, MG, and LU (CLASS B) with 95\% confidence level.}
		\label{fig:fi_randomness}
		\vspace{-15pt}
	\end{center}
\end{figure}

The figure reveals that the fault injection result is sensitive to the number of fault injection tests even with the consideration of error level and confidence level.
%for some benchmarks. 
For CG, the fault injection results are 62\% for 1000 fault
inject tests and 78\% for 6000 fault injection tests.
Furthermore, when the number of fault injection tests is 1000, we find that
LU is slightly more resilient than MG (0.81 for MG vs.
0.86 for LU). However, when the number of fault injection
tests is 3000, MG is more resilient than LU (0.91 for MG vs. 0.81 for LU). % Anzheng
We make totally different conclusions when comparing MG and LU.
This observation is a clear demonstration of the randomness of
using the traditional fault injection to study the application resilience.
Such randomness comes from the limitation of the statistical approach,
in particular, a priori estimation of the application resilience, 
limited confidence level, and inability to capture some fault masking events. 
%We choose 0.5 suggested by~\cite{date09:leveugle}, but
%using different estimated values, we have the same conclusions when comparing MG and LU. 
We must have a new methodology.

\subsection{Fault Masking}
Fault masking can happen at the application level and hardware level.
The application level fault masking happens because of application inherent semantics and program constructs.
%For example, a corrupted bit in a data structure is overwritten by an assignment statement, hence does not cause incorrect application outcomes; 
%a corrupted bit of a molecular representation in the Monte Carlo method-based simulation to study molecular dynamics may not 
%matter to the application outcome, because of the statistical nature of the simulation.
%The application level fault masking is independent of hardware and system software, and should be distinguished from the hardware-level fault masking. 
The hardware level fault masking happens because a fault does not corrupt the precise semantics of hardware.
For example, branch mis-speculated states due to branch prediction or speculative memory disambiguation, if corrupted, will not cause incorrect outcome.
%A data prefetch instruction with fault corruption may not cause incorrect outcome.
\begin{comment}
Some fault masking happens because of the effects of both application and hardware.
For example, a cache line with random flipped bits can be evicted out of the cache because of a cache interference.
If the cache line is not written back to the lower level memory hierarchy (e.g., the cache line is clean),
the data corruption in the cache line will not affect the correctness of the application outcome,
hence we have fault masking.
The cache interference is affected by both memory access patterns of the application and the cache features 
(e.g., the cache size, associativity and eviction policy).
Hence, the fault masking in this example is caused by both application and hardware. 
\end{comment}

Our resilience modeling focuses on the application-level fault masking, and does not include fault masking at the hardware level. %or both application and architecture.
Furthermore, we use a data-oriented approach, and focus on \textit{fault masking happened in individual data objects}. 
In other words, we consider that when a fault (data corruption) happens in a data object, whether the fault can be masked.
A data object can be, for example, a matrix in matrix multiplication or a tree structure in the Barnes-Hut N-body simulation.
Using the data-oriented approach is beneficial for the resilience research, because HPC applications
%with->by  by anzheng
are often characterized by a large amount of data objects, and the application outcomes are typically stored in data objects.
Furthermore, many popular fault tolerance mechanisms are designed to protect data objects (e.g., application-level checkpoint/restart mechanisms
and many algorithm-based fault tolerance methods~\cite{ftcg_ppopp13, ft_lu_hpdc13, jcs13:wu}). Quantifying the resilience of data objects
can greatly benefit the designs of these fault tolerance mechanisms.
For example, if a data object is resilient, then we do not need to apply these mechanisms to protect it, which will improve performance and energy efficiency. In Section~\ref{sec:case_study}, we have a case study to further demonstrate the benefits of our approach.

%and implement selective fault tolerance to minimize protection overhead. 

%(\textbf{Dong: pending --> We focus on data object. explain why we focus on do})


