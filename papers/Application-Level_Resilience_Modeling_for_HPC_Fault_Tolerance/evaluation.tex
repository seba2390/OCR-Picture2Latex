\section{Evaluation}
\label{sec:evaluation}
\begin{table*}[!t]
\begin{center}
%\small
\caption {Benchmarks and applications for the study of the application-level resilience}
\vspace{-5pt}
\label{tab:benchmark}
\tiny
\begin{tabular}{|p{1.7cm}|p{7.5cm}|p{4cm}|p{2.5cm}|}
\hline
\textbf{Name} 	& \textbf{Benchmark description} 		& \textbf{Execution phase for evaluation}  			& \textbf{Target data objects}             \\ \hline \hline
CG (NPB)             & Conjugate Gradient, irregular memory access (input class S)   & The routine conj\_grad in the main computation loop  & The arrays $r$ and $colidx$     \\\hline
MG (NPB)    	       & Multi-Grid on a sequence of meshes (input class S)             & The routine mg3P in the main computation loop & The arrays $u$ and $r$ 	\\ \hline
FT (NPB)             & Discrete 3D fast Fourier Transform (input class S)            & The routine fftXYZ in the main computation loop  & The arrays $plane$ and $exp1$    \\ \hline
BT (NPB)             & Block Tri-diagonal solver (input class S)         		& The routine x\_solve in the main computation loop & The arrays $grid\_points$ and $u$	\\ \hline
SP (NPB)             & Scalar Penta-diagonal solver (input class S)         		& The routine x\_solve in the main computation loop & The arrays $rhoi$ and $grid\_points$  \\ \hline
LU (NPB)            & Lower-Upper Gauss-Seidel solver (input class S)        	& The routine ssor 	& The arrays $u$ and $rsd$ \\ \hline \hline
LULESH~\cite{IPDPS13:LULESH} & Unstructured Lagrangian explicit shock hydrodynamics (input 5x5x5) & 
The routine CalcMonotonicQRegionForElems 
& The arrays $m\_elemBC$ and $m\_delv\_zeta$ \\ \hline
AMG2013~\cite{anm02:amg} & An algebraic multigrid solver for linear systems arising from problems on unstructured grids (we use  GMRES(10) with AMG preconditioner). We use a compact version from LLNL with input matrix $aniso$. & The routine hypre\_GMRESSolve & The arrays $ipiv$ and $A$   \\ \hline
%$hierarchy.levels[0].R.V$ \\ \hline
\end{tabular}
\end{center}
\vspace{-5pt}
\end{table*}

%We evaluate the effectiveness of ARAT, and 
%We use ARAT to study the application-level resilience.
%The goal is to demonstrate 
%that aDVF can be a very useful metric to quantify the resilience of data objects
%at the application level. 
We study 12 data objects from six benchmarks of the NAS parallel benchmark (NPB) suite (we use SNU\_NPB-1.0.3) and 4 data objects from two scientific applications. 
%which is a c version of NPB 3.3, but ARAT can work for Fortran.
Those data objects are chosen to be representative: they have various data access patterns and participate in various execution phases.  
%For the benchmarks, we use CLASS S as the input problems and use the default compiler options of NPB.
For those benchmarks and applications, we use their default compiler options, and use gcc 4.7.3 and LLVM 3.4.2 for trace generation.
To count the algorithm-level fault masking, we use the default convergence thresholds (or the fault tolerance levels) for those benchmarks.
Table~\ref{tab:benchmark} gives 
%for->on by anzheng
detailed information on the benchmarks and applications.
The maximum fault propagation path for aDVF analysis is set to 10 by default.
%the value shadowing threshold is set as 0.01 (except for BT, we use $1 \times 10^{-6}$).
%These value shadowing thresholds are chosen such that any error corruption
%that results in the operand's value variance less than 1\% (for the threshold 0.01) or 0.0001\% (for the threshold $1 \times 10^{-6}$) during the 
%trace analysis does not impact the outcome correctness of six benchmarks.
%LU: check the newton-iteration residuals against the tolerance levels
%SP: check the newton-iteration residuals against the tolerance levels
%BT: check the newton-iteration residuals against the tolerance levels

\subsection{Resilience Modeling Results}
%We use ARAT to calculate aDVF values of 16 data objects. 
Figure~\ref{fig:aDVF_3tiers_profiling}
shows the aDVF results and breaks them down into the three levels 
(i.e., the operation-level, fault propagation level, and algorithm-level).
Figure~\ref{fig:aDVF_3classes_profiling} shows the 
%for->of by anzheng
results for the analyses at the levels of the operation and fault propagation,
and further breaks down the results into 
the three classes (i.e., the value overwriting, logical and comparison operations,
and value shadowing). %based on the reasons of the fault masking.
We have multiple interesting findings from the results.

\begin{figure*}
	\centering
        \includegraphics[width=0.8\textwidth]{three_tiers_gray.pdf}
% * <azguolu@gmail.com> 2017-03-23T03:20:28.808Z:
%
% ^.
        \vspace{-5pt}
        \caption{The breakdown of aDVF results based on the three level analysis. The $x$ axis is the data object name.}
        \vspace{-8pt}
        \label{fig:aDVF_3tiers_profiling}
\end{figure*}


\begin{figure*}
	\centering
	\includegraphics[width=0.8\textwidth]{three_types_gray.pdf}
	\vspace{-5pt}
	\caption{The breakdown of aDVF results based on the three classes of fault masking. The $x$ axis is the data object name. \textit{zeta} and \textit{elemBC} in LULESH are \textit{m\_delv\_zeta} and \textit{m\_elemBC} respectively.} % Anzheng
	\vspace{-5pt}
	\label{fig:aDVF_3classes_profiling}
    %\vspace{-5pt}
\end{figure*}

(1) Fault masking is common across benchmarks and applications.
Several data objects (e.g., $r$ in CG, and $exp1$ and $plane$ in FT)
have aDVF values close to 1 in Figure~\ref{fig:aDVF_3tiers_profiling}, 
which indicates that most of operations working on these data objects
have fault masking.
However, a couple of data objects have much less intensive fault masking.
For example, the aDVF value of $colidx$ in CG is 0.28 (Figure~\ref{fig:aDVF_3tiers_profiling}). 
Further study reveals that $colidx$ is an array to store column indexes of sparse matrices, and there is few operation-level or fault propagation-level fault masking  (Figure~\ref{fig:aDVF_3classes_profiling}).
The corruption of it can easily cause segmentation fault caught by the
algorithm-level analysis. 
$grid\_points$ in SP and BT also have a relatively small aDVF value (0.14 and 0.38 for SP and BT respectively in Figure~\ref{fig:aDVF_3tiers_profiling}).
Further study reveals that $grid\_points$ defines input problems for SP and BT. 
A small corruption of $grid\_points$ 
%change->changes by anzheng
can easily cause major changes in computation
caught by the fault propagation analysis. 

The data object $u$ in BT also has a relatively small aDVF value (0.82 in Figure~\ref{fig:aDVF_3tiers_profiling}).
Further study reveals that $u$ is read-only in our target code region
for matrix factorization and Jacobian, neither of which is friendly
for fault masking.
Furthermore, the major fault masking for $u$ comes from value shadowing,
and value shadowing only happens in a couple of the least significant bits 
of the operands that reference $u$, which further reduces the value of aDVF.
%also reduces fault masking.

(2) The data type is strongly correlated with fault masking.
Figure~\ref{fig:aDVF_3tiers_profiling} reveals that the integer data objects ($colidx$ in CG, $grid\_points$ in BT and SP, $m\_elemBC$ in LULESH) appear to be 
more sensitive to faults than the floating point data objects 
($u$ and $r$ in MG, $exp1$ and $plane$ in FT, $u$ and $rsd$ in LU, $m\_delv\_zeta$ in LULESH, and $rhoi$ in SP).
In HPC applications, the integer data objects are commonly employed to
define input problems and bound computation boundaries (e.g., $colidx$ in CG and $grid\_points$ in BT), 
or track computation status (e.g., $m\_elemBC$ in LULESH). Their corruption 
%these integer data objects
is very detrimental to the application correctness. 

(3) Operation-level fault masking is very common.
For many data objects, the operation-level fault masking contributes 
more than 70\% of the aDVF values. For $r$ in CG, $exp1$ in FT, and $rhoi$ in SP,
the contribution of the operation-level fault masking is close to 99\% (Figure~\ref{fig:aDVF_3tiers_profiling}).

Furthermore, the value shadowing is a very common operation level fault masking,
especially for floating point data objects (e.g., $u$ and $r$ in BT, $m\_delv\_zeta$ in LULESH, and $rhoi$ in SP in Figure~\ref{fig:aDVF_3classes_profiling}).
This finding has a very important indication for studying the application resilience.
In particular, the values of a data object can be different across different input problems. If the values of the data object are different, 
then the number of fault masking events due to the value shadowing will be different. 
Hence, we deduce that the application resilience
can be correlated with the input problems,
because of the correlation between the value shadowing and input problems. 
We must consider the input problems when studying the application resilience.
This conclusion is consistent with a very recent work~\cite{sc16:guo}.

(4) The contribution of the algorithm-level fault masking to the application resilience can be nontrivial.
For example, the algorithm-level fault masking contributes 19\% of the aDVF value for $u$ in MG and 27\% for $plane$ in FT (Figure~\ref{fig:aDVF_3tiers_profiling}).
The large contribution of algorithm-level fault masking in MG is consistent with
the results of existing work~\cite{mg_ics12}. 
For FT (particularly 3D FFT), the large contribution of algorithm-level fault masking in $plane$ (Figure~\ref{fig:aDVF_3tiers_profiling})
comes from frequent transpose and 1D FFT computations that average out 
or overwrite the data corruption.
CG, as an iterative solver, is known to have the algorithm-level fault masking
because of the iterative nature~\cite{2-shantharam2011characterizing}.
Interestingly, the algorithm-level fault masking in CG contributes most to the resilience of $colidx$ which is a vulnerable integer data object (Figure~\ref{fig:aDVF_3tiers_profiling}).

%Our study reveals the algorithm-level fault masking of CG from
%two perspectives. First, $a$ in CG, which is an array for intermediate results,
%has few algorithm-level fault masking (0.008\%);
%Second, $x$ in CG, which is a result vector, has 5.4\% of the aDVF value coming from the algorithm-level fault masking.
%This result indicates that the effects of the algorithm-level fault masking
%are not uniform across data objects. 

(5) Fault masking at the fault propagation level is small.
For all data objects, the contribution of the fault masking at the level of fault propagation is less than 5\% (Figure~\ref{fig:aDVF_3tiers_profiling}).
For 6 data objects ($r$ and $colidx$ in CG, $grid\_points$ and $u$ in BT, and 
$grid\_points$ and $rhoi$ in SP),  there is no fault masking at the level of fault propagation.
In combination with the finding 4, we conclude that once the fault
is propagated, it is difficult to mask it because of the contamination of
more data objects after fault propagation, and only the algorithm semantics can tolerate  propagated faults well. 
%This finding is consistent with our sensitivity analysis. 

(6) Fault masking by logical and comparison operations is small,
%For all data objects, the fault masking contributions due to logical and comparison operations are very small, 
comparing with the contributions of value shadowing and overwriting (Figure~\ref{fig:aDVF_3classes_profiling}). 
Among all data objects, 
the logical and comparison operations in $grid\_points$ in BT contribute the most (25\% contribution in Figure~\ref{fig:aDVF_fine_profiling}), 
because of intensive ICmp operations (integer comparison). %logical OR and SHL (left shifting).


(7) The resilience varies across data objects. %within the same application.
This fact is especially pronounced in two data objects $colidx$ and $r$ in CG (Figure~\ref{fig:aDVF_3tiers_profiling}).
 $colidx$ has aDVF much smaller than $r$, which means $colidx$ is much less resilient than $r$ (see finding 1 for a detailed analysis on $colidx$). 
Furthermore, $colidx$ and $r$ have different algorithm-level
fault masking (see finding 4 for a detailed analysis).

\begin{comment}
\textbf{Finding 7: The resilience of the same data objects varies across different applications.}
This fact is especially pronounced in BT and SP.
BT and SP address the same numerical problem but with different algorithms.
BT and SP have the same data objects, $qs$ and $rhoi$, but
$qs$ manifests different resilience in BT and SP.
This result is interesting, because it indicates that by using
different algorithms, we have opportunities to
improve the resilience of data objects.
\end{comment}

To further investigate the reasons for fault masking, 
we break down the aDVF results at the granularity of LLVM instructions,
based on the analyses at the levels of operation and fault propagation.
The results are shown in Figure~\ref{fig:aDVF_fine_profiling}.
%Because of the space limitation, 
%we only show one data object per benchmark, but each selected data object has the most diverse fault masking events within the corresponding benchmark.
%Based on Figure~\ref{fig:aDVF_fine_profiling}, we have another interesting finding.

(8) Arithmetic operations make a lot of contributions to fault masking.
%For $r$ in CG, $r$ in MG, $exp1$ in FT, $u$ in BT, $qs$ in SP, and $u$ in LU,
%the arithmetic operations, FMul (100\%), Add (16\%), FMul (85\%), 
%FMul (94\%), FMul (28\%), and FAdd (50\%)
For $r$ in CG, $u$ in BT, $plane$ and $exp1$ in FT, $m\_elemBC$ in LULESH, 
arithmetic operations (addition, multiplication, and division) contribute to almost 100\% of the fault masking (Figure~\ref{fig:aDVF_fine_profiling}).  
%(at the operation level and the fault propagation level).
%For $qs$ in SP and $u$ in LU, the store operation also makes
%important contributions as the arithmetic operations because of value overwriting.

\begin{figure*}
	\centering
	\includegraphics[width=0.77\textheight, height=0.23\textheight]{pie_chart.pdf}
	\vspace{-10pt}
	\caption{Breakdown of the aDVF results based on the analyses at the levels of operation and fault propagation}
    \vspace{-10pt}
	\label{fig:aDVF_fine_profiling}
\end{figure*}


\subsection{Sensitivity Study}
\label{sec:eval_sen}
%\textbf{change the fault propagation threshold and study the sensitivity of analysis to the threshold}
ARAT uses 10 as the default fault propagation analysis threshold. 
The fault propagation analysis will not go beyond 10 operations. Instead,
we will use deterministic fault injection after 10 operations. 
In this section, we study the impact of this threshold on the modeling accuracy. We use a range of threshold values and examine how the aDVF value varies and whether
the identification of fault masking varies. 
Figure~\ref{fig:sensitivity_error_propagation} shows the results for 
%add , after BT by anzheng
multiple data objects in CG, BT, and SP.
We perform the sensitivity study for all 16 data objects.
%in six benchmarks and two applications.
Due to the page space limitation, we only show the results for three data objects,
but we summarize the sensitivity study results for all data objects in this section.
%but other data objects in all benchmarks have the same trend.

Our results reveal that the identification of fault masking by tracking fault propagation is not significantly 
affected by the fault propagation analysis threshold. Even if we use a rather large threshold (50), 
the variation of aDVF values is 4.48\% on average among all data objects,
and the variation at each of the three levels of analysis (the operation level, fault propagation level,  and algorithm level) is less than 5.2\% on average. 
In fact, using a threshold value of 5 is sufficiently accurate in most of the cases (14 out of 16 data objects).
This result is consistent with our finding 5 (i.e., fault masking at the fault propagation level is small). %in most benchmarks).
However, we do find a data object ($m\_elementBC$ in LULESH) %and $exp1$ in FT) 
showing relatively high-sensitive (up to 15\% variation) to the threshold. For this uncommon data object, using 50 as the fault propagation path is sufficient. 

%In other words, even though using a larger threshold value can identify more error masking by tracking error 
%propagation, the implicit error masking induced by the error propagation is very limited.

\begin{figure}
		\begin{center}
		\includegraphics[width=0.48\textwidth,height=0.11\textheight]{sensi_study_gray.pdf}
		\vspace{-15pt}
		\caption{Sensitivity study for fault propagation threshold}
		\label{fig:sensitivity_error_propagation}
		\end{center}
\vspace{-15pt}
\end{figure}


\begin{comment}
\subsection{Comparison with the Traditional Random Fault Injection}
%\textbf{compare with the traditional fault injection to verify accuracy}
To show the effectiveness of our resilience modeling, we compare traditional random fault injection
and our analytical modeling. Figure~\ref{fig:comparison_fi} and Table~\ref{tab:comparison} show the results.
The figure shows the success rate of all random fault injection. The ``success'' means the application
outcome is verified successfully by the benchmarks and the execution does not have any segfault. The success rate is used as a metric
to evaluate the application resilience.

We use a data-oriented approach to perform random fault injection.
In particular, given a data object, for each fault injection test we trigger a bit flip
in an operand of a random instruction, and this operand must be a reference to the
target data object. We develop a tool based on PIN~\cite{pintool} to implement the above fault injection functionality.
For each data object, we conduct five sets of random fault injection tests, 
and each set has 200 tests (in total 1000 tests per data object). 
We show the results for CG and FT in this section, but we find that
the conclusions we draw from CG and FT are also valid for the other four benchmarks.


%\begin{table*}
%\label{tab:success_rate}
%\begin{centering}
%\renewcommand\arraystretch{1.1}
%\begin{tabular}{|c|c|c|c|c|c|c|}
%\hline 
%Success Rate (Difference) & Test set 1 & Test set 2 & Test set 3 & Test set 4 & Test set 5 & Average\tabularnewline
%\hline 
%\hline 
%CG-a & 66.1\% (11.7\%) & 68.5\% (15.7\%) & 56.7\% (4.21\%) & 61.3\% (3.57\%) & 43.3\% (26.8\%) & 59.2\%\tabularnewline
%\hline 
%CG-x & 99.2\% (2.2\%) & 98.6\% (1.5\%) & 96.5\% (0.63\%) & 97.8\% (0.64\%) & 93.6\% (3.7\%) & 97.1\%\tabularnewline
%\hline 
%CG-colidx & 36.8\% (12.7\%) & 49.6\% (17.8\%) & 40.2\% (4.6\%) & 52.6\% (24.9\%) & 31.4\% (25.4\%) & 42.1\%\tabularnewline
%\hline 
%FT-exp1 & 52.7\% (1.4\%) & 22.6\% (56.5\%) & 78.5\% (51.0\%) & 60.7\% (16.7\%) & 45.4\% (12.7\%) & 51.9\%\tabularnewline
%\hline 
%FT-plane & 82.1\% (2.5\%) & 79.3\% (5.6\%) & 99.5\% (18.2\%) & 93.2\% (10.7\%) & 66.8\% (20.6\%) & 84.2\%\tabularnewline
%\hline 
%\end{tabular}
%\par\end{centering}
%\caption{XXXXX}
%\end{table*}


\begin{table*}
\begin{centering}
\caption{\small The results for random fault injection. The numbers in parentheses for each set of tests (200 tests per set) are the success rate difference from the average success rate of 1000 fault injection tests.}
\label{tab:comparison}
\renewcommand\arraystretch{1.1}
\begin{tabular}{|c|p{2.2cm}|p{2.2cm}|p{2.2cm}|p{2.2cm}|p{2.2cm}|p{1.8cm}|}
\hline 
       %& Test set 1 & Test set 2 & Test set 3 & Test set 4 & Test set 5 & Average\tabularnewline
       & \hspace{13pt} Test set 1 \hspace{1pt}/  & \hspace{13pt} Test set 2 \hspace{1pt}/ & \hspace{13pt} Test set 3 \hspace{1pt}/ & \hspace{13pt} Test set 4 \hspace{1pt}/ & \hspace{13pt} Test set 5 \hspace{1pt}/ & Ave. of all test / \\
       & success rate (diff.) & success rate (diff.) & success rate (diff.) & success rate (diff.) & success rate (diff.) & \hspace{5pt} success rate \\
\hline 
\hline 
CG-a & 66.1\% (6.9\%) & 68.5\% (9.3\%) & 56.7\% (-2.5\%) & 61.3\% (2.1\%) & 43.3\% (-15.9\%) & 59.2\%\tabularnewline
\hline 
CG-x & 99.2\% (2.1\%) & 98.6\% (1.5\%) & 96.5\% (-0.6\%) & 97.8\% (0.7\%) & 93.6\% (-3.5\%) & 97.1\%\tabularnewline
\hline 
CG-colidx & 36.8\% (-5.3\%) & 49.6\% (7.5\%) & 40.2\% (-2.0\%) & 52.6\% (10.5\%) & 31.4\% (-10.7\%) & 42.1\%\tabularnewline
\hline 
FT-exp1 & 52.7\% (0.8\%) & 22.6\% (-29.3\%) & 78.5\% (26.6\%) & 60.7\% (8.8\%) & 45.4\% (-6.5\%) & 51.9\%\tabularnewline
\hline 
FT-plane & 82.1\% (-2.1\%) & 79.3\% (-4.9\%) & 99.5\% (15.3\%) & 93.2\% (9.0\%) & 66.8\% (-17.4\%) & 84.2\%\tabularnewline
\hline 
\end{tabular}
\par\end{centering}
\vspace{-0.4cm}
\end{table*}

\begin{figure}
	\begin{center}
		\includegraphics[width=0.48\textwidth,keepaspectratio]{verifi-study.png}
		\caption{The traditional random fault injection vs. ARAT}
		\label{fig:comparison_fi}
	\end{center}
\vspace{-0.7cm}
\end{figure}


We first notice from Table~\ref{tab:comparison} that 
%across 5 sets of random fault injection tests, there are big variances (up to 55.9\% in $exp1$ of FT) in terms of the success rate. 
the results of 5 test sets can be quite different from each other and from 1000 random fault inject tests (up to 29.3\%).
1000 fault injection tests provide better statistical significance than 200 fault injection tests.
We expect 1000 fault injection tests potentially provide higher accuracy to quantify the application resilience.
The above result difference is clearly an indication to the randomness of fault injection, and there
is no guarantee on the random fault injection accuracy.

%In Figure~\ref{fig:comparison_fi}, 
We compare the success rate of 1000 fault inject tests with the aDVF value (Fig.~\ref{fig:comparison_fi}). 
We find that the order of the success rate of the three data objects in CG (i.e., $colidx < a < x$) and the two data objects in FT 
(i.e., $exp1 < plane$) is the same as the order of the aDVF values of these data objects. 
%In fact, 1000 fault injection tests
%account for \textcolor{blue}{\textbf{xxx\%}} of total memory references to the data object,
%and provide better resilience quantification than 200 fault injection tests.
The same order (or the same resilience trend)
%between our approach and the random fault injection based on a large number of tests 
is a demonstration of the effectiveness of our approach.
Note that the values of the aDVF and success rate %for a data object
cannot be exactly the same (even if we have sufficiently large numbers of random fault injection), 
because aDVF and random fault injection quantify
the resilience based on different metrics.
Also, the random fault injection can miss some fault masking events that can be captured by our approach.

\end{comment}