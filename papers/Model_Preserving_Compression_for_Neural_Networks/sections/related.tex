%Recently, there has been substantial research interest in methods for pruning and compressing neural networks, though classical magnitude pruning is still considered an effective approach~\cite{blalock2020state,frankle2018lottery,frankle2020pruning,gale2019state,liu2019rethink}.
%%\paragraph{Structured pruning}
%%Methods for structured neural network pruning fall largely into three categories.
%%which does not suffer the computational inefficiencies of sparse networks.
%One line of work concerns structured pruning of deep convolutional networks.
%\citet{li2017l1} is one of the earliest of these works which prunes channels based on their $L_1$-norm.
%However, pruning with just the magnitudes of a trained network does not account for the possibility of duplicate neurons or channels with high magnitude.
%Other works have aimed to resolve these issues:
%one popular approach is to train with a sparsity inducing regularizer such as the $L_1$ or $L_0$ norm, and then prune small magnitude groups~\cite{huang2018sss,yang2020hoyer,liu2017netslim,zhuang2020polar}.
%Other methods aim to estimate the importance of channels.
%\citet{luo2017thinet} greedily removes channels with the smallest effect on the next layer's activation output.
%\citet{he2017feat} cast channel selection as a LASSO regression problem to determine a feature map with minimal reconstruction error of the next layer. 
%\citet{zhuang2018dcp} introduce a loss which encourages discrimination in the intermediate layers, and prune based on this loss.
%\citet{he2019fpgm} calculate the geometric median of channels in a layer to select the most distinct ones.
%\citet{peng2019ccp} analyze the joint impact of pruned and kept channels on the final loss, and prune as an optimization problem.
%Other proposed pruning methods 
%use reinforcement learning~\cite{he2018amc} or 
%meta-learning~\cite{liu2019meta}.

%%Pruning with coresets~\cite{mussay2020coreset} is the closest in spirit to our own work.
%%The coreset algorithm provides a way to select a subset of neurons in the current layer which can approximate those in the next layer, as well as new weight connections.
%%\citet{mussay2020coreset} provide a sample complexity result,
%%%for the size of the coreset that can achieve epsilon accuracy uniformly over the data,
%%%an approximation error analysis,
%%as well as demonstrate their method on fully connected (but not convolution) layers.
%Coreset pruning is based on probabilistically pruning the neurons in one layer based on a weighted set.  This method comes with some theoretical guarantees, and creates dense matrices by sub-selecting neurons in the layer and updating the next layer weights to compensate.  While this method looks at pruning post-non-linearity, it does not use any data to do the pruning.  Results are available for the VGG-16 Cifar10 task, and pruned and fine tuned networks using this method actually gain accuracy for a compression ratio of 75\%.  However, the networks used before pruning had a lower classification accuracy than our and others, and the final classification accuracy was 91.84\%.  
%\paragraph{Filter decomposition}
%Another line of work close to ours is called the filter decomposition~\cite{denten2014svd,jarderberg2014lowrank,peng2018group,lebedev2015cpdecomp,wang2017fact,zhang20153dfilter}:
%here, convolution weights are approximated with a low-rank matrix decomposition such as the SVD~\cite{denten2014svd} or CP-decomposition~\cite{lebedev2015cpdecomp}.
%The main difference is that filter decomposition does not account for the activation function.


%% Cat A. Sparse, no correction
Magnitude pruning~(Cat A., B.) is still considered an effective approach~\cite{blalock2020state,frankle2018lottery,frankle2020pruning,gale2019state,liu2019rethink,li2017l1}. 
There are also channel pruning methods which either do not incorporate a correction~(Cat. B)~\cite{he2019fpgm,luo2017thinet}, employ a correction without local fine tuning~(Cat. E)~\cite{liebenwein2020provable,he2018amc}, or do~(Cat. H)~\cite{luo2017thinet,he2017feat,zhuang2018dcp,peng2019ccp,liu2017netslim}. 
Pruning with coresets~(Cat. H)~\cite{mussay2020coreset} is the closest in spirit to our own work and provides a way to select a subset of neurons in the current layer that can approximate those in the next layer as well as new weight connections.
\citet{mussay2020coreset} provide a sample complexity result,
%for the size of the coreset that can achieve epsilon accuracy uniformly over the data,
%an approximation error analysis,
and demonstrate their method on fully connected (but not convolution) layers.
There are low-rank decomposition methods which approximate the weights, again either without using fine tuning in a corrective step~(Cat. F)~\cite{denten2014svd,lebedev2015cpdecomp}, or with~(Cat. I)~\cite{zhang20153dfilter,idel2020lrank,liebenwein2021alds,jarderberg2014lowrank,peng2018group}.
%skip wang2017fact bc I think it trains from scratch
%Filter pruning methods impose a structured sparsity with a local fine tuning correction~(Cat. A)~\cite{liebenwein2020provable}.



Because we focus on approximating the original model, there are compression methods that do not fit into our taxonomy.
One large category modifies training to encourage pruning~\cite{huang2018sss,yang2020hoyer,alvarez2017compressaware,wen2017coord,zhuang2020polar}.
Another category trains a modified network that informs how to choose the compressed one~\cite{liu2019meta,chen2021once}.


\citet{liebenwein2021lost} use measures of functional approximation to conclude that pruned networks well approximate the original models. It is interesting to explore how their approximation measures differ from our per-label correlation.



%%%% related work backup 1/26/22
%%\jerry{emphasize methods that use original and pruned model, to fit under our notion of model compression/correlation.}
%%\todo[inline]{Re-org via taxonomy table.
%%Also add additional sources from our experiments.}
%%
%%Recently, there has been substantial research interest in methods for pruning and compressing neural networks, though classical magnitude pruning is still considered an effective approach~\cite{blalock2020state,frankle2018lottery,frankle2020pruning,gale2019state,liu2019rethink}.
%%%\paragraph{Structured pruning}
%%%Methods for structured neural network pruning fall largely into three categories.
%%%which does not suffer the computational inefficiencies of sparse networks.
%%One line of work concerns structured pruning of deep convolutional networks.
%%\citet{li2017l1} is one of the earliest of these works which prunes channels based on their $L_1$-norm.
%%However, pruning with just the magnitudes of a trained network does not account for the possibility of duplicate neurons or channels with high magnitude.
%%Other works have aimed to resolve these issues:
%%one popular approach is to train with a sparsity inducing regularizer such as the $L_1$ or $L_0$ norm, and then prune small magnitude groups~\cite{huang2018sss,yang2020hoyer,liu2017netslim,zhuang2020polar}.
%%Other methods aim to estimate the importance of channels.
%%\citet{luo2017thinet} greedily removes channels with the smallest effect on the next layer's activation output.
%%\citet{he2017feat} cast channel selection as a LASSO regression problem to determine a feature map with minimal reconstruction error of the next layer. 
%%\citet{zhuang2018dcp} introduce a loss which encourages discrimination in the intermediate layers, and prune based on this loss.
%%\citet{he2019fpgm} calculate the geometric median of channels in a layer to select the most distinct ones.
%%\citet{peng2019ccp} analyze the joint impact of pruned and kept channels on the final loss, and prune as an optimization problem.
%%Other proposed pruning methods use reinforcement learning~\cite{he2018amc} or meta-learning~\cite{liu2019meta}.
%%
%%Pruning with coresets~\cite{mussay2020coreset} is the closest in spirit to our own work.
%%The coreset algorithm provides a way to select a subset of neurons in the current layer which can approximate those in the next layer, as well as new weight connections.
%%\citet{mussay2020coreset} provide a sample complexity result,
%%%for the size of the coreset that can achieve epsilon accuracy uniformly over the data,
%%%an approximation error analysis,
%%as well as demonstrate their method on fully connected (but not convolution) layers.
%%%Coreset pruning is based on probabilistically pruning the neurons in one layer based on a weighted set.  This method comes with some theoretical guarantees, and creates dense matrices by sub-selecting neurons in the layer and updating the next layer weights to compensate.  While this method looks at pruning post-non-linearity, it does not use any data to do the pruning.  Results are available for the VGG-16 Cifar10 task, and pruned and fine tuned networks using this method actually gain accuracy for a compression ratio of 75\%.  However, the networks used before pruning had a lower classification accuracy than our and others, and the final classification accuracy was 91.84\%.  
%%%\paragraph{Filter decomposition}
%%Another line of work close to ours is called the filter decomposition~\cite{denten2014svd,jarderberg2014lowrank,peng2018group,lebedev2015cpdecomp,wang2017fact,zhang20153dfilter}:
%%here, convolution weights are approximated with a low-rank matrix decomposition such as the SVD~\cite{denten2014svd} or CP-decomposition~\cite{lebedev2015cpdecomp}.
%%%The main difference is that filter decomposition does not account for the activation function.
