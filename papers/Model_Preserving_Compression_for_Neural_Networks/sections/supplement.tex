\appendix


\section{Notation}

Matrices are denoted with capital letters such as $A$, and vectors with lower case $a$.  In situations where we partition a matrix into pieces, the partitions will be referred to as $A_{ij}$.  Individual entries in a matrix will be referred to as lower case letters with two subscripts, $a_{ij}$.  
$\sigma_k(A)$ denotes the $k$-th leading singular value of $A$, and $\kappa(A)$ the condition number.
For a matrix $A\in\R^{n\times m}$ we let $A_{\mathcal{J},\I}$ denote a sub-selection of the matrix $A$ using sets $\mathcal{J}\subset [n]$ to denote the selected rows and $\I\subset [m]$ to denote the selected columns; $:$ denotes a selection of all rows or columns.

\section{Fixed-rank interpolative decompositions}
%\paragraph{Further applications: choosing important sub-samples}
%Aside from pruning, interpolative decompositions can be used to select an important sub-sample of the %training
%data which can act as a surrogate for the whole set.
%Following our notation in Section~\ref{sec:pruneID} for a one hidden layer network, let $Z \in \R^{m \times n}$ be the first-layer output, i.e. $Z = g(W^\top X)$.
%Now we wish to preserve the network outputs with fewer data points by computing a rank-$k$ interpolative decomposition $Z^\top \approx (X^\top)_{:,\I} H$ where $H$ is an interpolation matrix.
%In particular, we compute a rank-$k$ interpolative decomposition of 
%%$\gamma(W^\top X)$ (i.e., 
%$Z^\top$ denoted $\gamma(W^\top X) \approx \gamma(W^\top X_{:,\mathcal{J}})H$, where $|\mathcal{J}|=k$ is the subset of the data and $H$ the associated interpolation matrix. 

As stated in Section~\ref{sec:ID}, a formal algorithmic statement is given for computing fixed-rank interpolative decompositions.
    
\begin{algorithm}[ht]%t
\begin{algorithmic}
\label{alg:genericID}
%\INPUT
\REQUIRE
%\KwIn{
matrix $A \in \R^{n \times m}$, rank-$k$
%}
%\OUTPUT
\ENSURE
%\KwOut{
interpolative decomposition $A_{:,\I} T$
%}
\STATE Compute column-pivoted QR factorization 
\[
    A
    \begin{bmatrix}
    \Pi_1 & \Pi_2
    \end{bmatrix}
    =
    \begin{bmatrix}
    Q_1 & Q_2
    \end{bmatrix}
    \begin{bmatrix}
    R_{11} & R_{12} \\
    & R_{22}
    \end{bmatrix}.
\]
where
$\Pi_1 \in \R^{m \times k}$, 
%$\Pi_2 \in \R^{m \times (m-k)}$, 
%$Q_1 \in \R^{n \times k}$, 
%$Q_2 \in \R^{n \times (\ell-k)}$, 
$R_{11} \in \R^{k \times k}$, 
$R_{12} \in \R^{k \times (m-k)}$, 
%and $R_{22} \in \R^{(\ell-k)\times(\ell-n)}$
and remaining dimensions as required.
\;
\STATE $A_{:,\I} \gets A \Pi_1$ \;
\STATE $T \gets 
\begin{bmatrix}
I_k & R_{11}^{-1} R_{12}
\end{bmatrix}
\Pi^\top$\;
\end{algorithmic}
%\SetAlgoLined
%\DontPrintSemicolon
\caption{Interpolative Decomposition}
\end{algorithm}

\section{Proofs}

\begin{customthm}{\ref{thm:generalization}}
%\label{thm:generalization}
Consider a model $h_{FC}=u^\top g(W^\top x)$ with m hidden neurons and a pruned model $\widehat{h}_{FC}=\widehat{u}^\top g(\widehat{W}^\top x)$ constructed using an $\epsilon$ accurate ID with $n$ data points drawn i.i.d\ from $\cD.$ The risk of the pruned model $\mathcal{R}_p$ on a data set $(x,y) \sim D$ assuming $\cD$ is compactly supported on $\Omega_x\times\Omega$ is bounded by  
\begin{equation*}
    \mathcal{R}_p \leq \mathcal{R}_{ID} + \mathcal{R}_0+ 2  \sqrt{ \mathcal{R}_{ID}  \mathcal{R}_0},
\end{equation*}
where $\mathcal{R}_{ID}$ is the risk associated with approximating the full model by a pruned one and with probability $1-\delta$ satisfies
\begin{equation*}
    {\mathcal{R}}_{ID} \leq \epsilon^2M+M(1+\|T\|_2)^2n^{-\frac{1}{2}} \left( \sqrt{2\zeta dm \log (dm)\log\frac{en}{\zeta dm \log (dm)}}+ \sqrt{\frac{\log (1/\delta)}{2}}\right).
\end{equation*} 
Here, $M = \sup_{x\in\Omega_x} \|u\|_2^2 \| g(W^T x)\|_2^2$ and $\zeta$ is a universal constant that depends on $g$. %the activation function.  


\end{customthm}


\begin{proof}
We can write the risk for this network as
\begin{equation*}
    \mathcal{R}_p=\mathbb{E}(\|\widehat{u}^\top g(\widehat{W}^\top x)- y\|^2),
\end{equation*} and adding and subtract the original network yields

\begin{equation*}
\begin{aligned}
    \mathbb{E}(\|\widehat{u}^\top g(\widehat{W}^\top x)- y\|^2) &= \mathbb{E}(\|(\widehat{u}^\top g(\widehat{W}^\top x)-u^\top g (w^\top x))+(u^\top g (w^\top x)- y)\|^2)\\
    &\leq \mathbb{E}((\|(\widehat{u}^\top g(\widehat{W}^\top x)-u^\top g (w^\top x))\|+\|(u^\top g (w^\top x)- y)\|)^2)\\
   &\leq \mathbb{E}(\|(\widehat{u}^\top g(\widehat{W}^\top x)-u^\top g (w^\top x))\|^2)\\
   &\phantom{\leq}+ \mathbb{E}(2\|(\widehat{u}^\top g(\widehat{W}^\top x)-u^\top g (w^\top x))\|\|(u^\top g (w^\top x)- y)\|)\\
   &\phantom{\leq}+\mathbb{E}(\|(u^\top g (w^\top x)- y)\|^2)\\
   &\leq  \mathcal{R}_{ID} + 2  \sqrt{ \mathcal{R}_{ID}  \mathcal{R}_0}+ \mathcal{R}_0.
\end{aligned}
\end{equation*}

Now, we bound $\mathcal{R}_{ID}$ by 
considering the interpolative decomposition to be a learning algorithm learning the function $u^\top g(W^\top X)$.
Specifically, we use Lemma \ref{lem:prunedRisk} to bound $\mathcal{R}_{ID}$ as  
\begin{equation*}
    \mathcal{R}_{ID} \leq \widehat{\cR}_{ID} + M(1+\|T\|_2)^2n^{-\frac{1}{2}}\left( \sqrt{2p\log(en/p)}+ 2^{-\frac{1}{2}}\sqrt{\log (1/\delta)}\right).
\end{equation*}
where p is the pseudo-dimension. We can then use Lemma \ref{lem:IDEmperical} to bound the empirical risk of the interpolative decomposition as  
\begin{equation*}
    \widehat{\cR}_{ID} \leq  \epsilon^2 \|u\|_2^2 \| g(W^T X)\|_2^2 / n,
\end{equation*}
and it follows that
\begin{equation*}
    \widehat{\cR}_{ID} \leq \epsilon^2 \sup_{x\in\Omega_x} \|u\|_2^2 \| g(W^T x)\|_2^2.
\end{equation*}

\end{proof}


\begin{customlemma}{\ref{lem:prunedRisk}}
Under the assumptions of Theorem~\ref{thm:generalization}, for any $\delta\in(0,1)$, $\cR_{ID}$ satisfies  
\begin{equation*}
    \mathcal{R}_{ID} \leq \widehat{\cR}_{ID} + M(1+\|T\|_2)^2n^{-\frac{1}{2}}\left( \sqrt{2p\log(en/p)}+ 2^{-\frac{1}{2}}\sqrt{\log (1/\delta)}\right)
\end{equation*}
with probability $1-\delta,$ where $M = \sup_{x\in\Omega_x} \|u\| ^2 \| g(W^T x)\|^2$ and $p=\zeta dm \log (dm)$ for some universal constant $\zeta$ that depends only on the activation function.
\end{customlemma}
\begin{proof} 
Considering the interpolative decomposition as a learning algorithm to learn $u^\top g(W^\top X)$, we can use Theorem 11.8 in~\cite{foundationsML} to bound the risk on the data distribution. Given a maximum on the loss function $\eta$, and the ReLU activation function, 


\begin{equation*}
    \mathcal{R}_{ID} \leq \widehat{\mathcal{R}}_{ID} + \frac{\eta}{n^{1/2}}( \sqrt{2p\log en/p}+ 2^{-1/2} \sqrt{\log (1/\delta)})
\end{equation*}
with probability $(1-\delta)$.\footnote{e is the base of the natural log.} Here, the constant $\eta$ is bounded by Lemma~\ref{lem:eta}. Bartlett et al.~\cite{pmlr-v65-harvey17a}  show that the p-dimension for a ReLU network is $O(\mathcal{W}Llog(\mathcal{W}))$ where $\mathcal{W}$ is the number of weights and L is the number of layers.   Here, that translates to $p=\zeta dm \log(dm)$ for some constant $\zeta$ that depends only on the choice of activation function.  
\end{proof}




\begin{customlemma}{\ref{lem:IDEmperical}}

Following the notation of Theorem~\ref{thm:generalization}, an ID pruning to accuracy $\epsilon$ yields a compressed network that satisfies
\begin{equation*}
    \widehat{\cR}_{ID} \leq  \epsilon^2 \|u\|_2^2 \| g(W^T X)\|_2^2 / n,
\end{equation*}
where $X\in\R^{d\times n}$ is a matrix whose columns are the pruning data.
\end{customlemma}

\begin{proof}
\begin{equation}
    \hat{\mathcal{R}}_{ID}=\frac{1}{n}\sum_{i=1}^n |u^\top g(W^\top x_i) - \widehat{u}^\top g(\widehat{W}^\top x_i) |^2
\end{equation}
Here, we can appeal to our definition of the ID to bound each term in the sum.    

\begin{equation}
    |u^\top g(W^\top x_i) - \widehat{u}^\top g(\widehat{W}^\top x_i) | =|u^\top g(W^\top x_i) - {u}^\top  T^\top g(P^\top {W}^\top x_i) |
\end{equation}
By our definition of an $\epsilon$-accurate interpolative decomposition, 

\begin{equation}
    |u^\top g(W^\top x_i) - \widehat{u}^\top g(\widehat{W}^\top x_i) | \leq  \epsilon \|u\| \| g(W^T x_i)\|
\end{equation}

Therefore, 

\begin{equation}
    \hat{\mathcal{R}}_{ID} \leq \frac{1}{n} \epsilon^2 \|u\| ^2 \| g(W^T X)\|^2
\end{equation}
\end{proof}

\begin{lemma}
\label{lem:eta}
The maximum $\eta$ of the loss function associated with approximating the full network with the pruned on is bounded as   
\[
    \eta \leq \sup_{x\in\Omega_x} \|u\|_2^2 \| g(W^T x)\|_2^2 \|(1+ \|T\|))^2
\]

\end{lemma}
\begin{proof} 
\begin{equation*}
    \eta=\max_{x, W, u} \| u^\top g(W^\top x ) - \widehat{ u}^\top g (\widehat{W}^\top x) \|^2
\end{equation*}
For any $x\in\Omega_x$ we have the bound 
\begin{equation*}
\begin{aligned}
    \| u^\top g(W^\top x ) - \widehat{ u}^\top g (\widehat{W}^\top x) \|^2 &\leq (\|u^\top g(W^\top x) \| + \|\widehat{ u}^\top g (\widehat{W}^\top x) \|)^2\\
     &\leq (\|u^\top g(W^\top x) \| + \|{u}^\top  T^\top g(P^\top {W}^\top x)\|)^2\\
     &\leq (\|u^\top g(W^\top x)\|(1+ \|T\|))^2.\\
\end{aligned}
\end{equation*}
Therefore, 
\begin{equation*}
    \eta \leq \sup_{x\in\Omega_x} \|u\|_2^2 \| g(W^T x)\|_2^2 \|(1+ \|T\|))^2
\end{equation*}

%Here, we appeal to the existence of a matrix T which interpolates $g(W^\top x)$ such that each $t_{ij} \leq 2$ \megan{cite}.  This bounds $\|T\| $ \megan{check what the tightest bound on this is is it 2m?  }
\end{proof}

\begin{remarks}
We can explicitly measure the norm of the interpolation matrix $T$ that appears in the upper bound of Lemma~\ref{lem:prunedRisk}.  Moreover, we expect this to be small because there exists an interpolation matrix such that  $t_{ij} \leq 2 \forall \{i,j\}$~\cite{liberty2007randomized}. The better the interpolation matrix, the better the bound.    
\end{remarks}
%\subsection{A note about sparse regression}
%\jerry{Keep?}
%Instead of a rank-revealing QR factorization, one could also use a sparse regression approach.
%Ultimately we did not proceed with this approach because it empirically did not perform as well.





\section{Correlation between random trials}
\label{app:sec:correlation}
We introduce a metric that we call "model correlation" in order to evaluate different pruning methods.  We define the correlation between two models on a data set as the percent of labels that the two models agree on, irrespective of if those labels agree with the ground truth.  There are situations in which the user of a network may care about more than just the simple accuracy --- it may matter which items a network is most likely to get wrong, and how.  It is also possible for two networks to have the same accuracy but perform very differently on subsets of the dataset.  We include this metric after fine tuning to demonstrate the efficacy of our compression method relative to methods that necessitate extensive fine tuning and, therefore, may not correlate well with the original model.  Here we provide some baseline measurements of what affects model correlation, in order to better understand this metric.  

Our baseline measurement of model correlation is the model correlation between two same-sized but randomly initialized networks trained using the same hyper parameters but with different (random) data orders.  We first test the effects for a fully connected one hidden layer network on FashionMNIST. 

\begin{table}[h!]
\begin{center}
 \begin{tabular}{||c c c c c||} 
 \hline
Size & Same Initialization & Same Data Order & Vary Both & Accuracy\\ [0.5ex] 
 \hline\hline
 500 & 94.6 & 94.0 & 93.3 & 89.21 \\ 
 \hline
 2000 & 96.7 & 94.7 & 94.5& 89.63\\
 \hline
 4000 & 97.3 & 95.4 & 95.1 &89.74\\
 \hline
\end{tabular}
\caption{Correlation data for a single hidden layer fully connected network on the FashionMNIST data set. We keep the same initialization but vary the data order (Same Initialization), use the same data order but vary the initialization (Same Data Order) or vary both the data order and initialization (Vary both). This data is averaged over 9 trials.  We see that starting at the same initialization increases the correlation at the end of training, and, interestingly, that using the same data order during training can increase the correlation as well. }
\end{center}
\end{table}
We continue our experiments on the CIFAR10 dataset using the VGG-16 architecture.  We find that two differently randomly initialized models trained using different data orders to  the same state-of-the-art accuracy (93.6\%) agree on classifications 93.0\% of the time.  The effect of data order is also seen on CIFAR10 VGG-16 networks.  When we prune to 50\% FLOPS and then re-train a VGG-16 network using magnitude pruning, if the data order is the same as the original network, then the correlation is 94.01\%.  However, using a different data order the correlation is 93.15\%. The model correlation breaks down quickly when we use a large learning rate (.1 for 200 epochs).  

\section{Comparison methods}
\label{app:sec:comparison_methods}
Here we provide a key for the various methods we compare to in the main text, along with their classifications within our taxonomy. %(Table~\ref{tab:taxonomy}).  
We give both the paper citation and implementation citation.

\begin{table}[h!]
\label{tab:citations}
%\small
\begin{center}
 %\begin{tabular}{||c c c c c||} 
 \begin{tabular}{c c c c c} 
 %\hline 
 \toprule
 \textbf{Dense} & \textbf{Structure Preserving} & \textbf{Corrects Next Layer} & \textbf{No Local FT} & \\ 
 %\textbf{Dense}&\textbf{Structure Preserving}& \textbf{Corrects Next Layer}&\textbf{No Local FT}&\\ 
 \midrule
 %\textbf{Name} & \textbf{Citation} &\textbf{Implementation} & \textbf{Table} & \textbf{Figures}\\ 
 Name & Citation & Implementation & Table & Figures \\ 
 \midrule
 ID & (Ours) &(Ours) & All & All\\
 PFP & \citet{liebenwein2020provable} &\citet{liebenwein2020provable}& - & \ref{fig:vgg16preft}\\ 
 AMC & \citet{he2018amc} & \citet{he2018amc} & \ref{tab:vggImgNet} & -\\
 \midrule
 \\\\
 \toprule
 \textbf{Dense}&\textbf{Structure Preserving}& \textbf{Corrects Next Layer}&\textbf{Local FT}&\\ 
 \midrule
 Name & Citation & Implementation & Table & Figures \\ 
 \midrule
 Thi & \citet{luo2017thinet} & \citet{liebenwein2020provable} & \ref{tab:vggImgNet} & \ref{fig:vgg16preft}\\ 
 CP & \citet{he2017feat} & \citet{he2017feat} &  \ref{tab:vggImgNet} & -\\ 
 NS & \citet{liu2017netslim} &\citet{zhuang2020polar} & \ref{tab:cifarVgg} & -\\ 
 \midrule
 \\\\
 \toprule
 \textbf{Dense}&\textbf{Structure Preserving}&\textbf{No Correction}&&\\
 \midrule
 Name & Citation & Implementation & Table & Figures \\ 
 \midrule
 Uni & Uniform Random Filter Pruning &\citet{liebenwein2020provable}&  - & \ref{fig:vgg16preft}\\ 
 Soft & \citet{softnetHe} &\citet{liebenwein2020provable}&  - & \ref{fig:vgg16preft}\\ 
 StructMag &\citet{li2017l1} & \citet{liebenwein2020provable} & \ref{tab:cifarVgg} & \ref{fig:vgg16preft}\\ 
 FPGM & \citet{he2019fpgm} &(Ours)&  \ref{tab:cifarVgg} & -\\
 {HRank} & ~\citet{lin2020hrank}  &~\citet{lin2020hrank} &  \ref{tab:cifarVgg} & -\\ 
 \midrule
 \\\\
 \toprule
 \textbf{Dense}&\textbf{Extra Layers}&&&\\
 \midrule
 Name & Citation & Implementation & Table & Figures \\ 
 \midrule
 ALDS & \citet{liebenwein2021alds} &\citet{liebenwein2021alds}&  - & \ref{fig:mobilenet_atom3d},\ref{fig:combiningID} \\ 
 Messi & \citet{Maalouf2021DeepLM} &\citet{liebenwein2021alds}&  - & \ref{fig:mobilenet_atom3d},\ref{fig:vgg16preft}\\ 
 PCA & \citet{zhang20153dfilter} &\citet{liebenwein2021alds}&  - &\ref{fig:mobilenet_atom3d}, \ref{fig:combiningID}\\ 
 SVD & \citet{denten2014svd} &\citet{liebenwein2021alds}&  - & \ref{fig:mobilenet_atom3d}, \ref{fig:vgg16preft}\\ 
 LRank & \citet{idel2020lrank} &\citet{liebenwein2021alds} &  \ref{tab:cifarVgg},\ref{tab:vggImgNet}& \ref{fig:combiningID}\\ 
 Polar & \citet{zhuang2020polar}&\citet{zhuang2020polar}&  \ref{tab:cifarVgg} & -\\
 \midrule
 \\\\
 \toprule
 \textbf{Sparse}&&&&\\
 \midrule
 Name & Citation & Implementation & Table & Figures \\ 
 \midrule
 SiPP & \citet{sippBayal}&\citet{sippBayal} &  - & \ref{fig:vgg16preft}\\ 
 Snip & \citet{lee2019snip} &\citet{sippBayal}& - & \ref{fig:vgg16preft}\\
 Thres & \citet{li2017l1} &\citet{liebenwein2020provable}&  - & \ref{fig:vgg16preft}\\ 

\hline
 \end{tabular}

 \end{center}
       \caption{
    Taxonomy of pruning and methods and look-up table for references.  As you go further down the list, methods tend to become more different from our own.  
    }
 \end{table}
 
% \begin{table}[h!]
%%\small
%\begin{center}
% \begin{tabular}{||c c c c c c c||} \hline
% Name & Full Name & Citation &Impl. & Cat.& Tab. & Figs.\\ \hline
% ID & Interpolative Decomposition & (Ours) &(Ours)& E & All & All\\ 
% ALDS & Automatic Layer-wise Decomposition Selector & \citet{liebenwein2021alds} &\citet{liebenwein2021alds}& I& - & \ref{fig:mobilenetCifar},\ref{fig:CombineCifar}, \ref{fig:compose_vggImgNet} \\ 
% Messi & Multiple Estimated SVDs for Smaller Intralayers & \citet{Maalouf2021DeepLM} &\cite{liebenwein2021alds}& I& - & \ref{fig:mobilenetCifar},\ref{fig:vggCifar}\\ 
% PCA & Accel Very Deep ConvNets for Class. \& Detect. & \citet{zhang20153dfilter} &\citet{liebenwein2021alds}& I& - & \ref{fig:mobilenetCifar},\ref{fig:CombineCifar}, \ref{fig:compose_vggImgNet}\\ 
% SVD & Exploiting Linear Struct. w/in ConvNets & \citet{denten2014svd} &\citet{liebenwein2021alds}& F& - & \ref{fig:mobilenetCifar}, \ref{fig:mobilenetImgNet}, \ref{fig:compressonly_vggImgNet}\\ 
% Uni & Full Name & Citation &\citet{liebenwein2020provable}& Cat.& - & \ref{fig:vggCifar}\\ 
% PFP & Provable Filter Pruning & \citet{liebenwein2020provable} &\citet{liebenwein2020provable}& Cat& - & \ref{fig:vggCifar}\\ 
% SiPP & Sensitivity-informed Provable Pruning & \citet{sippBayal}&\cite{sippBayal} & A& - & \ref{fig:vggCifar}\\ 
% Snip & Single-shot Network Pruning & \citet{} &\citet{sippBayal}&G& - & \ref{fig:vggCifar}\\ 
% Thi & ThiNet: A Filter Level Pruning Method & \citet{luo2017thinet} & \citet{liebenwein2020provable} & Cat& \ref{tab:vggImgNet} & \ref{fig:vggCifar}, \ref{fig:compressonly_vggImgNet}\\ 
% Soft & Soft filter pruning & \citet{softnetHe} &\citet{liebenwein2020provable}& Cat& - & \ref{fig:vggCifar}\\ 
% Thres &  Pruning Filters for Efficient ConvNets &\citet{li2017l1} &\citet{liebenwein2020provable}& A& - & \ref{fig:vggCifar}\\ 
% StructMag & Pruning Filters for Efficient ConvNets & \citet{li2017l1} & Impl. & B& \ref{tab:cifarVgg} & \ref{fig:compressonly_vggImgNet}\\ 
%  LRank & Low-Rank Compressino of NNs & \citet{idel2020lrank} &\citet{liebenwein2021alds} & I& \ref{tab:cifarVgg},\ref{tab:vggImgNet}& \ref{fig:CombineCifar}, \ref{fig:compose_vggImgNet}\\ 
%   CP & Channel Pruning for Accel Very Deep NN & \citet{he2017feat} & \citet{he2017feat} & cat& \ref{tab:vggImgNet} & -\\ 
%   AMC & AUtoML for Model Compression & \citet{he2018amc} & \citet{he2018amc} & cat& \ref{tab:vggImgNet} & -\\ 
%   Polar &Structured Pruning using Polarization Regularizer & \citet{zhuang2020polar}&\cite{zhuang2020polar}& cat& \ref{tab:cifarVgg} & -\\ 
%  NS &Network Slimming & \citet{liu2017netslim} &\citet{zhuang2020polar}& cat& \ref{tab:cifarVgg} & -\\ 
%  FPGM &Filter Pruning via Geometric Median & \citet{he2019fpgm} &(Ours)& cat& \ref{tab:cifarVgg} & -\\ 
%
%\hline
%
% \end{tabular}
% \end{center}
%% \label{tab:citations}
% \end{table}
 
\section{Setting $k(\epsilon)$ per layer in deep networks with iterative pruning}
\label{app:sec:iterativeID}
By Definition~\ref{def:ID}, an $\epsilon$-accurate interpolative decomposition is associated with a number $k$ of selected columns.
We observe that for deep networks it is not straightforward to apply a single accuracy $\epsilon$ to the entire network.
Figure \ref{fig:vggMet} illustrates the representative variety in the layer-wise singular value decay for a trained VGG-16 model.
For our method a sharper singular value decay indicates greater prunability. 
However, not all layers contribute equally to the number of FLOPS and the number of parameters.  Typically convolutional layers contribute disproportionately to the number of FLOPs compared to the number of parameters they contain.
When we prune neurons or channels in a layer, that has an effect on the number of FLOPS performed by that layer, and also in the next layer.  Therefore, we iteratively prune the network by finding the layer that will allow us to prune the most FLOPs compared to the error that we expect from pruning that layer, and pruning that layer.  This is given in Algorithm \ref{alg:deepIDIter}.  

\begin{algorithm}[t]{\small
\caption{Pruning a multilayer network with iterative interpolative decompositions}
\begin{algorithmic}
\label{alg:deepIDIter}
%\INPUT
\REQUIRE
Neural net $h(x; W^{(1)},\ldots,W^{(L)})$,
pruning set $X$,
step size $\lambda$,
FLOPs ratio $\rho$
%\OUTPUT
\ENSURE
Pruned network $h(x; \widehat{W}^{(1)},\ldots,\widehat{W}^{(L)})$
\vspace{0.5em}
\FOR{$l \in \{1, \dots, L\}$}
\STATE $\widehat{W}^{(l)} \gets W^{(l)}$
\ENDFOR
\STATE $F \gets \operatorname{Compute\_FLOPs}(h(x; \widehat{W}^{(1)},\ldots,\widehat{W}^{(L)})$
\STATE $F_\rho \gets F * \rho$
\WHILE{$F > F_\rho$}
\STATE $S, K \gets \{\}$
\FOR{$l \in \{1, \dots, L\}$}
\STATE $Z \gets h_{1:l}(X; W^{(1)}, \dots, W^{(l)})$
\COMMENT{layer l activations}
\STATE $R \gets \operatorname{Pivot\_QR}(\operatorname{Reshape}(Z))$
\COMMENT{reshape if Conv layer}
\STATE $k \gets \operatorname{num\_channel}(\widehat{W}^{(l)} \times \lambda$
\COMMENT{calculates proportion of channels or neurons to potentially remove}
\STATE $Err \gets |R[k+1,k+1]/R[1,1]|$
\COMMENT{error from ID approximation}
\STATE $F_l \gets \operatorname{Compute\_FLOPs}(\widehat{W}^{(l)}, \widehat{W}^{(l+1)})$
\COMMENT{compute FLOPs of current and next layer}
\STATE $S.\operatorname{append}(Err / F_l)$
\COMMENT{weighted layer prunability score}
\STATE $K.\operatorname{append}(k)$
\ENDFOR
\STATE $l \gets \operatorname{argmin}(S)$
\STATE $\widehat{W}^{(l)} \gets \operatorname{ID}$ prune layer $l$ to $K[l]$ neurons or channels
\STATE $F \gets \operatorname{Compute\_FLOPs}(h(x; \widehat{W}^{(1)},\ldots,\widehat{W}^{(L)})$
\ENDWHILE
%\STATE $T^{(0)} \gets I$ \;
%\FOR{$l \in \{1 \dots L\}$}
%\STATE $Z \gets h_{1:l}(X; W^{(1)}, \dots, W^{(l)})$
%\COMMENT{layer l activations}
%%\COMMENT{compute activations of layer l}
%\IF{layer $l$ is a FC layer}
%\STATE $(\I, T^{(l)}) \gets \operatorname{ID}(Z^T; \alpha) \textbf{ if } l \notin S \textbf{ else } (:, I)$ \;
%%prune $\alpha\%$ of neurons with ID of $Z^\top$: $\I, T$\;
%%compute rank-$k$ ID of $Z^\top$: $\I, T$\;
%\STATE $\widehat{W}^{(l)} \gets T^{(l-1)} W^{(l)}_{:,\I}$
%\COMMENT{sub-select neurons, multiply T of prev layer's ID}
%%\tcp{select neurons in current layer}
%%$\widehat{W}^{(l+1)} \gets T \widehat{W}^{(l+1)}$
%%\;
%%\tcp{propagate T to next layer}
%\ELSIF{layer l is a Conv layer (or Conv+Pool)}
%\STATE $(\I, T^{(l)}) \gets \operatorname{ID}(\Reshape(Z); \alpha) \textbf{ if } l \notin S \textbf{ else } (:, I)$ \;
%% prune $\alpha\%$ of channels with ID of $\Reshape(Z)$: $\I, T$\;
%%compute rank-$k$ ID of $\Reshape(Z)$: $\I, T$\;
%\STATE $\widehat{W}^{(l)} \gets \Matmul(T^{(l-1)}, W^{(l)}_{\I,\ldots})$
%%\;
%\COMMENT{select channels; multiply T} %in current layer
%% $\widehat{W}^{(l+1)} \gets \Matmul(T, \widehat{W}^{(l+1)})$
%%\tcp{propagate T to next layer}
%%\tcp{depends if next layer is FC or Conv}
%\ELSIF{layer l is a Flatten layer}
%\STATE $T^{(l)} \gets T^{(l-1)} \otimes I \,\,$ 
%\COMMENT{expand T to have the expected size}
%\ENDIF
%\ENDFOR
\end{algorithmic}
%%Specify direction\;
%%How to compute Z\;
%\caption{ID pruning a multi-layer neural network}
}\end{algorithm}






\begin{figure}[!tbp]
\centering

  \includegraphics[width=.25\linewidth]{figures/layer_1.pdf}
  \includegraphics[width=.25\linewidth]{figures/layer_8.pdf}
  \includegraphics[width=.25\linewidth]{figures/layer_13.pdf}

    \caption{Metrics for different layers in VGG-16 for Cifar-10.  The leftmost layer is the first convolutional layer.  The center figure is one of the middle convolutional layers, and the rightmost figure is the last convolutional layer.  As we can see, the singular value decay varies throughout the network. }
\label{fig:vggMet}
\end{figure}
\section{Sensitivity of parameters}
\label{sec:sens}
When we prune using Iterative ID, we have a choice of hyperparameter in how what percent of channels to cut per iteration (pruning fraction $\alpha$ in Algorithm~\ref{alg:deepIDIter}). In Figure~\ref{fig:sensitivity}, we demonstrate that the choice of pruning fraction does not greatly affect the accuracy of the network after iterative pruning.  However, using a smaller $\alpha$ results in the network taking significantly more time to prune.  

\begin{figure}[h!]
    \centering
    \includegraphics[width=\linewidth]{figures/sensitivity.pdf}
    \caption{Iterative pruning performed at different amounts of channels/neurons cut per iteration in the chosen layer.  We see that the choice of number of channels per iteration does not have a large impact on the outcome, though smaller percents take a longer time.}
    \label{fig:sensitivity}
\end{figure}

%One benefit of maintaining properties of the original model may include maintaining the fairness of an already fair model.  In order to accomplish this, we want our method to be robust to the contents of the pruning set, including when classes are underrepresented.  While we do not recommend 


\section{Additional experimental details and results}
%FLOPs reduction = 1 - pruned FLOPs / orinal FLOPs.

\subsection{Illustrative example}


We created a simple synthetic data set for which we know the form of a relatively minimal model representation.

Draw $n$ points iid. from the unit circle in 2 dimensions.
Next select 2 (normalized) random vectors $v_1$ and $v_2$.
All labels are initialized to zero, and add or subtract 1 to the label for each time it produces a positive inner product with one of the random vectors.
With the ReLU activation function it is possible to correctly label all points with a one hidden layer fully connected network of width 4.
Construct pairs of neurons, with each pair aligned with the center of one of the random vectors.  We can think of the pair as creating a flat function by using one neuron to "cut the top" off of the round part of the function created by the other.

We parameterize this with an angle $\phi$ away from the pair. If each neuron in the pair has the same weight magnitude $w$, coefficient $\pm u$ and a bias $b \pm \delta/2$, then given a particular $w$, $b$ and $\delta$, we can write:

\begin{equation*}
       u ({\ReLU( w \cos \phi -b +\delta/2)- \ReLU(w \cos \phi -(b-\delta/2))})
\end{equation*}

As long as $w>b$, this looks like a step function, where the sides get steeper as $w$ approaches infinity.  This allows us to perfectly label all of the points given twice the number of neurons as we have random vectors $v_1$ and $v_2$, which is much smaller than the number of data points $n$.

We train an over-parameterized single hidden layer network to perform fairly well on this task, using an initialization scale that is common in some machine learning platforms such as TensorFlow~\cite{tensorflow2015-whitepaper}. This is shown in figure \ref{fig:patchesBonus}.  
However, magnitude pruning will not necessarily recognize the structure of the minimum representative network because both of the neurons in the pair construction may not have a large magnitude.
On this example we see that the interpolative decomposition is able to select neurons which resemble a close to minimal representative network.  

\begin{figure}[!tbp]
\centering
  \includegraphics[width=.4\linewidth]{figures/patchesAcc.pdf}
  \includegraphics[width=.4\linewidth]{figures/patchesFuncEval.pdf}
\caption{A plot of loss v.s. number of neurons kept (left) and a plot of the function evaluation for the full network, ID pruning, and magnitude pruned model (right).  The data is drawn from the unit circle, and we parameterize X in terms of an angle $\theta$. To see more detail in the magnitude pruned function, we draw it with a scale of 10x applied uniformly.  We see that it takes very few (12) neurons to completely represent the function approximated by the network. In fact, the ID pruned function is visually indistinguishable from the full model in this case.  }
\label{fig:patchesBonus}
\end{figure}


In Figure~\ref{fig:patches} we see that the ID well represents the original network by ignoring duplicate neurons and taking into account differences in the bias.
The ID even achieves slightly better test loss than the original model.
Magnitude pruning does not approximate the original model well; it keeps duplicate neurons and fails to find important information with the same number of neurons.

\begin{figure}[!tbp]
\centering
  \includegraphics[width=.3\linewidth]{figures/fullPatch.pdf}
  \includegraphics[width=.3\linewidth]{figures/IDpatch.pdf}
  \includegraphics[width=.3\linewidth]{figures/magPatch.pdf}
    \caption{Neuron visualization for a simple 2-d regression task. The axes are the weights of the neurons, and the color is the bias.  We have the weights trained for a full model(left) with 5000 hidden nodes.  The 12 neurons kept by the ID (center) represent the function well. These are re-scaled by the magnitude of their coefficients in the second layer to display the effect of the ID.  Magnitude pruning (right) keeps duplicate neurons that do not represent the function.  The test loss is 0.037786 for the full model, 0.037781 for the model pruned with ID, and 0.33 magnitude-pruned model. }
\label{fig:patches}
\end{figure}
  
%We created a synthetic dataset, with points drawn iid. from the unit sphere in 3 dimensions.  We select 2 random vectors and normalize.  All of the labels are initialized to zero, and we add 1 to the labels of each of the points with a positive inner product with each of the vectors.  If we use the ReLU activation function, it is possible to correctly label all of the points using a one hidden layer neural network with 2n neurons in its hidden layer.   One arrangement that accomplishes this is by pairing the the neurons, with each pair aligned with each of the centers of the patches.  

%\subsection{Data set and code licenses}
%Fashion MNIST~\cite{datafashionmnist} is licensed under the MIT License.
%We are unaware of a license for the CIFAR-10 data set~\cite{datacifar10}.
%We adapted code and hyper-parameters from \textcite{liu2019rethink} (MIT License), \textcite{li2017l1} (license unaware), and \textcite{he2019fpgm} (license unaware).

%\subsection{Estimating compute time}
%The experiments on the illustrative example and Fashion MNIST were performed on a 2019 iMac running an Intel I9. The illustrative example computes in minutes. We trained 15 different model configurations, 8 one hidden layer, and 7 two hidden layer network sizes.  For 5 random seeds, we used $5*(50+10+10)=350$ epochs per model size, and trained 15 different model configurations.   We used 10,000 images for the pruning set, however, the runtime for computing the ID is cheap compared to training, using the scipy QR decomposition function which calls a LAPACK subroutine~\cite{lapack}.

%The experiments on CIFAR-10 were performed on a NVIDIA GeForce GTX 1080Ti. 
%For 5 random seeds and two models a total of $5*2*160=1600$ epochs was used for training the %ResNet-56 and
%VGG-16 models.
%The Table~\ref{tab:cifar10mag} our results required $5*3*40=1200$ epochs of fine-tuning for 5 random seeds and 2 pruning configurations on ResNet-56, 1 configuration on VGG-16.
%Computing the interpolative decomposition were comparatively cheap, only requiring 1000 data points. 
%The Table~\ref{tab:cifar10modern} and~\ref{tab:cifar10beforeFT} ID results required $5 * 2 * 200 = 2000$ total epochs of fine-tuning for 5 random seeds and 2 different models.






%\paragraph{Fashion MNIST}
\subsection{Fashion MNIST}
\label{sec:fashionmnist}

For one and two hidden layer networks on Fashion MNIST~\cite{datafashionmnist} we compare the performance of ID and magnitude pruning to networks of the same size trained from scratch.
Each pruning method is used to prune to half the number of neurons of the original model, and is then fine-tuned for 10 epochs.
%For our first experiment, we train simple fully connected one and two hidden layer networks on the Fashion MNIST~\cite{datafashionmnist} data set and prune each network to one half of the neurons in each layer. 
%This consists of 60000 grey-scale training images and 10000 test images with 10 classes.  
We use a stochastic gradient descent optimizer with a learning rate of 0.3 which decays by a factor of 0.9 for each epoch to train the initial networks.  Each was trained for 50 epochs.  Our pruning set was 10000 images, which did not need to be held out from training for the simple data set. The fine tuning ran for 10 epochs with an initial learning rate of 0.1 with a decay rate of 0.6 for two layer networks, and 0.2 with a learning rate decay of 0.7 for one layer.  Larger ID-pruned models (greater than 512 neurons) can be fine-tuned with a much smaller learning rate of 0.002. These learning rates and number of epochs may not be optimal but were determined through brief empirical tests. We used 5 random seeds for each size of model.  The error bars are reported as the uncertainty in the mean, defined in terms of the standard deviation $\sigma$, and number of independent trials $N$,  $\sigma_{mean}=\sigma/\sqrt{N}$.
%We compare the performance of the networks trained from scratch, and pruned using magnitude pruning and ID pruning.  
Results are shown in Figure~\ref{fig:fmnist}; before fine-tuning the ID achieves a significantly higher test accuracy than magnitude pruning.
ID pruning with fine-tuning outperforms training from scratch. %, and all methods begin to converge at large network sizes (except just magnitude pruning).
%We see that the pruning and then fine tuning technique out performs training from scratch for moderate network sizes, and that all of the methods except magnitude pruning without fine tuning begin to converge at large network sizes,
At sufficiently large network sizes, ID pruning alone performs similarly to training a network from scratch. 
When we start to see diminishing returns from adding more neurons, the performance of the various methods begin to converge.  
%In addition by about $2^9$ hidden neurons we've surpassed the minimal network size and see diminishing returns from more neurons.  


\begin{figure}
\centering
  \centering
  \includegraphics[width=.47\linewidth]{figures/OHL.pdf}
  \hspace{2mm}
  \includegraphics[width=.47\linewidth]{figures/THL.pdf}
  
    \caption{Accuracy for a one hidden layer (left) and two hidden layer (right) fully connected neural networks of varying size on Fashion-MNIST for ID and magnitude pruning, as well as training directly.
    These curves are averaged over 5 trials; error bars report uncertainty in the mean. 
    }
\label{fig:fmnist}
\end{figure}

\begin{figure}[ht]
\centering
  \includegraphics[width=.45\linewidth]{figures/metrics.pdf}
  %\caption{Normalized singular value decay, $||R_{22}||/||R||$, and the magnitude of$r_{kk}/r_{00}$ for a matrix Z from a one hidden layer neural network trained on Fashion MNIST.  We see that the three metrics generally correlate well in a practical setting.}
  \includegraphics[width=.45\linewidth]{figures/accuracyvsrkk.pdf}
  %\caption{Accuracy of a single hidden layer fully connected neural network pruned from 4096 to k neurons.  The horizontal lines are the accuracy of the full sized network  }

    \caption{Left: Normalized singular value decay, $\|R_{22}\|_2/\|R\|_2$, and $\lvert r_{k+1,k+1}/r_{1,1}\rvert$ for a matrix from~\eqref{eq:1hiddenfc}) from a one hidden layer network (i.e., $g(W^\top X)$) trained on Fashion MNIST. We see that the metrics generally correlate well in this setting. Right: Accuracy of a single hidden layer network pruned from width 4096 to $k.$  The horizontal lines are the accuracy of the full sized network. The difference  between pruning and test accuracy is due to a slight class imbalance in the canonical test set.}%Note that the slight difference between the pruning and test set exists for the full network and is due to a slight imbalance in classes for the canonical testing set.}
\label{fig:chosingk}
\end{figure}


%\subsection{Ablation studies for design of Algorithm~\ref{alg:deepID}}
%We found that using a held-out pruning set was important to prevent the interpolative decomposition from over-fitting to the training data, and improve generalization performance to the test set.
%Table~\ref{tab:prunesetAblation} reports our results.
%
%\begin{table}[]
%    \centering
%    \caption{Ablation study on the effect of using a held-out (or not) pruning set for the interpolative decomposition.
%    A ResNet-56 model on CIFAR-10 was pruned to 51\% FLOPs reduction with a pruning set of 1000.
%    The pruning set was either held-out from the test set, or randomly sampled from the training set.
%    Accuracies are reported as mean and standard deviation over 5 independent trials.
%    \\}
%    \label{tab:prunesetAblation}
%    \begin{tabular}{ccc}
%    \toprule
%    Interpolative Decomposition &
%    Baseline  &
%    Pruning \\
%    Pruning Set & 
%    Acc. (\%) &
%    Acc. (\%)\\
%    \midrule
%    Held Out &
%    \multirow{2}{*}{93.04 ($\pm$ 0.25)} & 
%    69.64 ($\pm$ 2.14) \\
%    Not Held Out & &
%    66.34 ($\pm$ 1.30)  \\
%    \bottomrule
%    \end{tabular}
%\end{table}
%
%
%In Algorithm~\ref{alg:deepID} we apply the ID from the beginning to the end of the multi-layer network.
%Doing so one could use the interpolative decomposition to approximate the activation outputs of either the original model or the model.
%Table~\ref{tab:Zablation} gives an ablation study which compares the pruning accuracy for both such cases.
%We observe that using the ID to approximate the original model achieves higher pruning accuracy (before fine-tuning).
%We believe that in deep networks there is greater concern for the ID to propagate errors forward through the matrix.
%Thus by approximating the original model's activation outputs, we can mitigate some of this error propagation.
%
%
%\begin{table}[]
%    \centering
%    \caption{Ablation study on the effect of approximating the activation outputs of the original or pruned model using the interpolative decomposition.
%    A ResNet-56 model on CIFAR-10 was pruned to 51\% FLOPs reduction with a held-out pruning set of size 1000.
%    Accuracies are reported as mean and standard deviation over 5 independent trials.
%    \\}
%    \label{tab:Zablation}
%    \begin{tabular}{ccc}
%    \toprule
%    Interpolative Decomposition &
%    Baseline  &
%    Pruning \\
%    Approximation Target & 
%    Acc. (\%) &
%    Acc. (\%)\\
%    \midrule
%    Original Model &
%    \multirow{2}{*}{93.04 ($\pm$ 0.25)} & 
%    69.64 ($\pm$ 2.14) \\
%    Pruned Model & &
%    63.19 ($\pm$ 4.66)  \\
%    \bottomrule
%    \end{tabular}
%\end{table}


%\subsection{Learning rate and prunability}
%\outline{LR and SVD}

\subsection{Additional ImageNet experiments on MobileNet V1}
\label{app:sec:imgnet_extra}
Here we give additional pruning experiments on Mobilenet V1 for ImageNet.
We see that the ID is competitive against compression methods which do not do any local fine tuning (Figure~\ref{fig:mobilenetImgNet_nolocFT}.
Figure~\ref{fig:mobilenetImgNet_yeslocFT} includes methods which do use local fine tuning; ID performs better than two out of the three.
Interestingly, we see that although the PCA and LRank methods performed well on an overparameterized network such as VGG-16, they do not work well on a much more efficient network.

\begin{figure}
\centering
\begin{subfigure}{.45\textwidth}
\centering
\includegraphics[width=.95\linewidth]{figures/mob_nolocFT_ImgNet.pdf}
\caption{Compare with methods that do not use any fine tuning.
% We see that the ID is superior in this setting.
}
\label{fig:mobilenetImgNet_nolocFT}
\end{subfigure}
\begin{subfigure}{0.45\textwidth}
\centering
\includegraphics[width=.95\linewidth]{figures/mob_yeslocFT_ImgNet.pdf}
\caption{Comparing with methods that use a local fine tuning correction.  
%We see that the ID is superior in this setting.
}
\label{fig:mobilenetImgNet_yeslocFT}
\end{subfigure}
\centering
\caption{MobileNet V1 compression results on ImageNet. Note that the matrix methods often work by changing the depth of convolution in the network, and given that MobileNet uses depth-wise separable convolutions, it is not surprising that matrix methods would perform poorly.  }
\end{figure}


\subsection{Imagenet pre fine tuning correlation}
\label{app:sec:imgnet_preft_corr}

\begin{figure}
    \centering
    \includegraphics[width=0.47\linewidth]{figures/mobCorr_ImgNet.pdf}
    \caption{MobileNet correlation results compared against various compression methods. Note that several other compression methods work by doing decompositions on the weight matrix and changing the number of groups in the convolution.  Due to the MobileNet architecture, this may result in a very low accuracy for methods that work well on other architectures.}
    \label{fig:movCorr_ImgNet}
\end{figure}

\begin{figure}
\centering
\begin{subfigure}{.47\textwidth}
\centering
\includegraphics[width=.95\linewidth]{figures/vggCorr_nolocFT_ImgNet.pdf}
\caption{
Comparing with methods that do not use a local fine tuning correction.
}
\label{fig:vggCorrImgNet_nolocFT}
\end{subfigure}
\begin{subfigure}{0.47\textwidth}
\centering
\includegraphics[width=.95\linewidth]{figures/vggCorr_yeslocFT_ImgNet.pdf}
\caption{
Comparing with methods that use a fine tuning correction.
}
\label{fig:vggCorrImgNet_yeslocFT}
\end{subfigure}
\centering
\caption{VGG-16 correlation results on ImageNet}
\end{figure}

Here we report correlation results om ImageNet with the MobileNet V1 and VGG-16 models.
For methods which do not incorporate a fine tuning correction (Figure~\ref{fig:vggCorrImgNet_nolocFT}), we see that the ID proves as good as any other method we compare to.
Figure~\ref{fig:vggCorrImgNet_yeslocFT} compares to methods which do incorporate a local fine tuning correction. 
Similar to accuracy, we see that composing ID with another compression method improves upon either method.





\subsection{Hyper-parameter details}
\paragraph{CIFAR-10}
The VGG-16 models are trained with 5 random seeds and with hyper-parameter specifications and code provided by \citet{liu2019rethink}.
The test set is randomly partitioned into a prune set and new test set.
The Iterative ID used a held out set of size 1000.
%The held-out pruning set is randomly sampled from the test set.
%This shrinks the test set slightly, from 10000 to 9000.
%For magnitude pruning we use the hyper-parameters specified by \textcite{liu2019rethink} to fine-tune for 40 epochs and learning rate 0.001.
%The interpolative decomposition uses a held out pruning set of 1000 data points and the same fine-tuning hyper-parameters, except the ResNet-56 is retrained with initial learning rate 0.01 and decreased to 0.001 after 10 epochs.
%The VGG-16 uses the same hyper-parameters as~\cite{liu2019rethink}.
%For Table~\ref{tab:cifarVgg}, we implement the method by \textcite{he2019fpgm} with their provided code and hyper-parameter settings on our trained models to fine-tune for 200 epochs.
%The interpolative decomposition uses a held out pruning set of 1000 data points with a hyper-parameter configuration from~\textcite{he2019fpgm}: use a starting learning rate 0.1 decaying to 0.02, 0.004, 0.0008 at epochs 60, 120, 160 to train for 200 epochs total with batch size 128 and weight decay 5e-4.
For the iterative ID on VGG-16, we prune 10\% of a layer per iteration.  %This results in pruning the 
For VGG-16 fine tuning we use SGD with initial learning rate of 5e-3, reduced to 2.5e-3 after 20 epochs and again reduced to 1e-3 after a another 20 epochs have passed.
We use a batch size of 128, momentum of 0.9, and weight decay of 5e-4.

%\paragraph{Mobilenet V1}
The full-size Mobilenet V1 network for Cifar-10 was trained using the ADAM optimizer for 120 epochs, using the default parameters of lr=.001, betas= (.9, .999).  The test set is randomly partitioned into a prune set and new test set.  

\paragraph{ImageNet}
For VGG-16 the iterative ID algorithm uses a randomly held out set of 5000 images with a stepsize parameter of 5\%.
For fine-tuning, we use SGD with a learning rate of 1e-7, batch size of 256, momentum of 0.9, and weight decay of 1e-4.
For ID+PCA, we switched to learning rate 1e-8 at 75 epochs.

For Mobilenet V1 we prune to a constant fraction using the ID with a randomly sampled held out set of 1000 images.


\subsection{Estimating compute}
The experiments on the illustrative example and Fashion MNIST were performed on a 2019 iMac running an Intel I9. The illustrative example computes in minutes. We trained 15 different model configurations, 8 one hidden layer, and 7 two hidden layer network sizes.  For 5 random seeds, we used $5*(50+10+10)=350$ epochs per model size, and trained 15 different model configurations.   We used 10,000 images for the pruning set, however, the runtime for computing the ID is cheap compared to training, using the scipy QR decomposition function which calls a LAPACK subroutine~\cite{lapack}.

The experiments on CIFAR-10 were performed on a NVIDIA GeForce GTX 1080Ti on a university compute cluster. 
Epochs of fine tuning are specified in the tables of the main paper.
Computing the interpolative decomposition were comparatively cheap, only requiring 1000 data points. 
Computing costs for any compression method were negligible compared to fine tuning.

ImageNet experiments were conducted on a university compute cluster.
For fine tuning NVIDIA GeForce RTX 3090, RTX A6000, or TITAN RTX were used.
The compression portion was conducted on a NIVIDIA GeForce GTX 1080Ti or FTX 2080Ti.
Epochs of fine tuning are specified in the tables of the main paper.
The compute cost of the compression methods before fine tuning (including interpolative decomposition) were trivial compared to any amount of global fine tuning.



\section{Sensitivity to pruning set}
\label{sec:sensitivity}
\subsection{Pruning set size}
We compare the pre-fine-tuning accuracy as a function of the pruning set size. 
Here we show that the accuracy is not particularly sensitive to the pruning set size.
Table~\ref{tab:id_sensitivity_imagenet} shows on ImageNet that our ID-based pruning method is robust to the prune set size, and in fact quite efficient.
Figure~\ref{fig:id_sensitivity_cifar10} shows on CIFAR-10 that this trend persists across a wide range of compression levels.
Note that the number of pruning examples must be at least the number of neurons or channels that we prune to for each layer.
%Pruning using the interpolative decomposition only takes a modes pruning set size.  At a minimum, it requires at least as many pruning examples as the number of neurons or channels that we wish to prune to. Here we show that the outcome of pruning is not particularly sensitive to the pruning set size. 

\begin{table}[]
    \centering
    \begin{tabular}{cc}
        Prune set size & Pre-fine-tuning accuracy \\
        \hline
        5k & 68.03 \\
        10k & 68.06 
    \end{tabular}
    \caption{Pre-fine-tuning accuracy compared to prune set size for VGG16 model pruned to 25\% FLOPs reduction on ImageNet.}
    \label{tab:id_sensitivity_imagenet}
\end{table}

\begin{figure}
\centering
%\begin{subfigure}{.87\textwidth}
\centering
\includegraphics[width=.5\linewidth]{figures/setsize.pdf}
\caption{
Comparing the pre-fine-tuning accuracy and FLOP reduction for different pruning set sizes on VGG-16 Cifar-10 model.  We see that above the minimum threshold, pruning set size has a minimial impact on the accuracy of the pruned model. 
}
\label{fig:id_sensitivity_cifar10}
%\end{subfigure}
\end{figure}

\subsection{Pruning set contents}

We want our method to be robust to the data selection method used to generate our pruning set. 
We test our method to this specific sensitivity by removing an entire class from the pruning set.
%This may include situations where an entire class is missing from the pruning set. 

We begin with a full-sized VGG-16 CIFAR-10 model that was trained
%to perform reasonably well
on all 10 classes.  Then we draw a pruning set from only 9 of the classes, completely leaving out images from one of the classes. We prune to 50\% of the original FLOPs, using Iterative ID, and compare the per-class test accuracies for each of the 10 classes. Figure~\ref{fig:id_sensitivity_class} shows that the ID maintains good accuracy even on images from the class that was excluded from the pruning set.  

We expect that other methods which preserve the model's decision boundaries (and therefore correlation) will likely show similar results.  However, methods which require extensive fine tuning and effectively re-train the network will likely not recover accuracy on the missing class.  To demonstrate this, we prune the same full-sized network using magnitude pruning using the same pruning set to 50\% FLOPs reduction, and then fine tune with only 9 classes in the fine tuning set.  As shown in Figure~\ref{fig:mag_sensitivity_class}, the accuracy for the other 9 classes recovers, however, the accuracy for the class that was removed does not.  

This experiment
%is a contrived example, but it 
suggests that pruning methods which maintain properties of the original model may potentially be able to maintain fairness (i.e. per-class accuracy) even when some classes of data are under-represented.  
Our compression method was able to reasonably preserve per-class accuracies (our measure of fairness), even without access to data from one of those classes.

%We began with a ``fair model'' which performed well on all 10 classes of CIFAR-10, and prune it to 50\% of the original FLOPs while maintaining accuracy on all classes, even without access to data from one of those classes.  

\begin{figure}
\centering
\begin{subfigure}{.47\textwidth}
\includegraphics[width=.95\linewidth]{figures/subclass.pdf}
\caption{Per class accuracies while pruning a VGG-16 model using only data from 9 classes.}
\label{fig:id_sensitivity_class}
\end{subfigure}
\begin{subfigure}{.47\textwidth}
\includegraphics[width=.95\linewidth]{figures/IDcorr.pdf}
\caption{Model Correlation when ID pruning using data from only 9 classes}
\label{fig:id_sensitivity_corr}
\end{subfigure}
\caption{
Per-class accuracies as we prune a VGG-16 model on CIFAR-10 with only access to data from 9/10 classes.  No fine tuning was done.  We see that ID maintains reasonable accuracy on all (including the unrepresented) classes, and stays correlated with the original model.  
}

\end{figure}

\begin{figure}
\centering
\begin{subfigure}{.47\textwidth}
\includegraphics[width=.95\linewidth]{figures/magFT.pdf}
\caption{Per class accuracies while fine tuning a magnitude-pruned model with only 9 classes}
\label{fig:mag_sensitivity_class}
\end{subfigure}
\begin{subfigure}{.47\textwidth}
\includegraphics[width=.95\linewidth]{figures/magFTcorr.pdf}
\caption{Model Correlation while fine-tuning a magnitude pruned model using data from only 9 classes.}
\label{fig:mag_sensitivity_corr}
\end{subfigure}
\caption{
Comparing class accuracies for a 50\% FLOPS VGG-16 model pruned using magnitude pruning, and then fine-tuned using only 9 out of 10 classes of CIFAR-10.  We see that the model recovers on the represented classes, but not on the unrepresented class. 
}

\end{figure}
