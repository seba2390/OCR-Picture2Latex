There are a number of different design choices to be made in the compression and pruning process.  Classical pruning involves eliminating either channels (analogously neurons) or individual weights, sometimes in a structured way.  
Magnitude pruning on both weights and neurons is still considered an effective approach~\cite{blalock2020state,frankle2018lottery,frankle2020pruning,gale2019state,liu2019rethink,li2017l1}.  When pruning, the method may incorporate a correction to future layers, though it often does not ~\cite{he2019fpgm,luo2017thinet}. Some methods that correct the network chose to do local fine tuning ~\cite{luo2017thinet,he2017feat,zhuang2018dcp,peng2019ccp,liu2017netslim}, whereas others do not ~\cite{liebenwein2020provable,he2018amc}.  

Of particular note, matrix approximation methods
~\cite{denten2014svd,idel2020lrank,liebenwein2021alds,peng2018group,lebedev2015cpdecomp}
%~\cite{denten2014svd,idel2020lrank,jarderberg2014lowrank,liebenwein2021alds,peng2018group,lebedev2015cpdecomp,zhang20153dfilter}
often satisfy the first criteria we desire for compression methods, but typically not the second as they add additional layers. These methods sometimes incorporate local fine tuning after compression ~\cite{idel2020lrank,jaderberg2014speeding,liebenwein2021alds,peng2018group,zhang20153dfilter} and sometimes do not ~\cite{denten2014svd,lebedev2015cpdecomp}.
In contrast, structured pruning methods
%~\cite{he2019fpgm,he2018amc,liebenwein2020provable,liu2019rethink,luo2017thinet}
~\cite{he2019fpgm,he2018amc,liebenwein2020provable,liu2017netslim,liu2019rethink,luo2017thinet}
can satisfy the second criteria we outlined, but typically not the first as they do a poor job of preserving the model's decisions and often require excessive amounts of fine tuning.


Pruning with coresets~\cite{mussay2020coreset} is the closest in spirit to our own work and provides a way to select a subset of neurons in the current layer that can approximate those in the next layer as well as new weight connections. Of note, \citet{mussay2020coreset} provide a sample complexity result,
and demonstrate their method on fully connected (but not convolution) layers.
The HRank method~\cite{lin2020hrank} is also close in spirit to our own, and works by selectively pruning channels that produce low-rank feature maps.   However, the method does not propagate updates into the next layer and instead relies on excessive amounts of fine tuning (30 epochs for each layer pruned) to fix the network's accuracy.  


Recently the literature has started to consider criteria beyond topline accuracy metrics, and \citet{liebenwein2021lost} use measures of functional approximation to conclude that pruned networks well approximate the original models. 
\citet{marx2020multiplicity} characterize when linear models can achieve similar accuracy but with competing predictions.

% It is interesting to explore how their approximation measures differ from our per-label correlation.
