The core of our approach is a novel use of IDs to 
prune neural networks. Here we illustrate the scheme for a single fully connected layer and we extend the scheme to more complex layers (e.g., convolution layers) and deeper networks in Section~\ref{sec:extendid}.
Consider a simple two layer (one hidden layer) fully connected neural network $h_{FC} : \R^d \to \R^c$ of width $m$ defined as 
\begin{equation*}
\label{eq:1hiddenfc}
    h_{FC}(x; W, U)
    =
    U^\top g(W^\top x)
\end{equation*}
with hidden layer $W \in \R^{d \times m}$, output layer $U \in \R^{m \times c}$, and activation function $g$. 
We omit bias terms: they may be readily incorporated by adding a row to $W$ and suitably augmenting the data.

To prune the model we will use an \emph{unlabeled} pruning data set  $\{x_i\}^{n}_{i=1}$ with $x_i \in \R^d$.
Let $X \in \R^{d \times n}$ be the matrix such that $X_{:,i} = x_i$.
Preserving the action of the two layer network to accuracy $\epsilon > 0$ on the data with fewer neurons is synonymous with finding an $\epsilon$ accurate approximation
$h_{FC}(x;W,U) \approx h_{FC}(x; \widehat{W},\widehat{U})$ 
where $\widehat{W}$ has fewer columns than $W$.
We can do this by computing an ID of the activation output of the first layer.

Concretely, let $Z \in \R^{m \times n}$ be the first-layer output, i.e., $Z = g(W^\top X),$ and let $Z^\top \approx (Z^\top)_{:,\I}  T $
be a rank-$k$ ID of $Z^\top$
with $|\I| = k$ and interpolation matrix $T \in \R^{k \times m}$ that achieves accuracy $\epsilon$ as in Definition~\ref{def:ID} (note that this means $k$ is a function of $\epsilon$).
%Importantly, 
Because the activation function $g$ commutes with the sub-selection operator, if $Z \approx T^\top Z_{\I,:}$ then
\begin{align*}
    g(W^\top X) 
    \approx T^\top \left( g(W^\top X) \right)_{\I,:}
    % \approx
    % T^\top g\left(\left( W^\top X \right)_{\I,:} \right)
    =
    T^\top g\left( W_{:,\I}^\top X \right).
\end{align*}
Multiplying both sides by $U^\top$ now gives an approximation of the original network by a pruned one,
\begin{equation}
    h_{FC}\left(x; W, U \right) 
    =
    U^\top g(W^\top X) \approx \\
    h_{FC}\left(x; W_{:,\I}, T U \right)
    =
    U^\top T^\top g\left( W_{:,\I}^\top X \right).
    \label{eqn:PruneApprox}
\end{equation}
% The activation output $Z$ can then be approximated
% \begin{equation}
%     Z \approx 
%     T^\top g(W_{1(:,\I)}^\top X)
% \end{equation}
% by setting $\widehat{W}_1 = W_{1(:,\I)}$ to select neurons in the current layer.
% In order to encode the interpolation matrix $T$ into the neural network, set $\widehat{W}_2 = T W_2$ 
% %to propagate the interpolation matrix into 
% in the next layer.
That is, the ID has pruned the network of width $m$ into a dense sub-network of width $k$ with $\widehat W \equiv W_{:,\I} \in \R^{d \times k}$ and $\widehat U \equiv T U \in \R^{k \times c}$.
% \begin{equation}
% \label{eq:fcID}
%     h_{FC}(x; W_1, W_2)
%     \approx
%     h_{FC}(x; W_{1(:,\I)} , T W_2) 
% \end{equation}
%\jerry{not sure if we want to say this:}
%Furthermore, if the interpolative decomposition achieves $\epsilon$ accuracy, then the neural network achieves \megan{at least}$\epsilon \|W_2\|$ accuracy.
Importantly, 
%An important point is that 
the SVD of $Z^\top$ cannot be used for this task since
it is not clear how to map the dominant left singular vectors back through the activation function $g$ to either a subset of the existing neurons or a small set of new neurons.
%In contrast the sub-selection operator of the interpolative decomposition commutes with the activation function.
This makes use of the ID essential and we provide additional intuition for this scheme in the appendix, specifically in Figure ~\ref{fig:patches}.  




%\begin{algorithm}[t]
%\SetAlgoLined
%\DontPrintSemicolon
%\KwIn{
%FC layer $f_{FC}(x;W_1)$, 
%next layer $f_{2}(x;W_2)$,
%%intermediate layer $g(x; w_3)$ (batch norm, pooling),
%pruning data $X_{p} \in \R^{d \times n_{p}}$,
%rank-$k(\epsilon)$
%}
%\KwOut{
%pruned layers
%$\widehat{f}_{FC}(x;\widehat{W}_1)$, 
%$\widehat{f}_{2}(x;\widehat{W}_2)$,
%%$\widehat{g}(x;\widehat{w}_3)$
%}
%
%%\tcc{apply batch norm or pool layer if it follows $f_{FC1}$}
%$Z \gets \gamma(X_{p}^\top W_1)$\;
%$Z_{:,\I}, T \gets \ID(Z, k(\epsilon))$\;
%$\widehat{W}_1 \gets (W_1)_{(:,\I)}$
%\tcp{select neurons via ID}
%$\widehat{W}_2 \gets \operatorname{matmul}(T, W_2)$
%\tcp{propagate interpolation matrix to next layer}
%%\tcp{depends on if next layer is FC or Conv}
%\caption{ID Pruning a FC Layer\jerry{might get rid of}}
%\label{alg:fcID}
%\end{algorithm}

\subsection{A generalization bound for the pruned network}
% \paragraph{A generalization bound for the pruned network}
A key feature of our ID based pruning method is that it can be used to dynamically select the width of the pruned network to maintain a desired accuracy when compared with the full model. This allows us to provide generalization guarantees for the compressed network in terms of generalization of the full model and the accuracy of the ID. We state the results for a single hidden fully connected layer with scalar output (i.e., $c=1$) and squared loss. They can be extended to more complex networks (at the expense of more complicated dependence on the accuracy each layer is pruned to), more general Lipschitz continuous loss functions, and vector valued output. We defer all proofs to the supplementary material.

Assume $(x,y)\sim \cD$ where $x\in\R^d,$ $y\in\R,$ and the distribution $\cD$ is supported on a compact domain $\Omega_x\times\Omega_y$. We let $\mathcal{R}_0 = \mathbb{E}_{(x,y) \sim \cD}(\|({u}^\top g({W}^\top x)- y)\|^2)$ denote the true risk of the trained full model and $\mathcal{R}_p = \mathbb{E}_{(x,y) \sim \cD}(\|({\widehat{u}}^\top g({\widehat{W}}^\top x)- y)\|^2)$ be the risk of the pruned model. (Since $c=1$ we let $u\in\R^m$ denote the last layer.)
We also define the empirical risk $\widehat{\cR}_{ID}$ of approximating the full model with our pruned model as
\begin{equation*}
\label{eq:pruneRisk}
    \widehat{\cR}_{ID}=\frac{1}{n}\sum_{i=1}^n \left\lvert{u}^\top g({W}^\top x_i) - {\widehat{u}}^\top g({\widehat{W}}^\top x_i) \right\rvert^2,
\end{equation*}
where $\{x_i\}_{i=1}^n$ are $n$ i.i.d.\ samples from $\cD$ (note that we do not need labels for these samples). Using this notation, Theorem~\ref{thm:generalization} controls the generalization error of the pruned model.


\begin{theorem}[Single hidden layer FC]
\label{thm:generalization}
Consider a model $h_{FC}=u^\top g(W^\top x)$ with m hidden neurons and a pruned model $\widehat{h}_{FC}=\widehat{u}^\top g(\widehat{W}^\top x)$ constructed using an $\epsilon$ accurate ID with $n$ data points drawn i.i.d\ from $\cD.$ The risk of the pruned model $\mathcal{R}_p$ on a data set $(x,y) \sim D$ assuming $\cD$ is compactly supported on $\Omega_x\times\Omega$ is bounded by  
\begin{equation}
\label{eq:RiskDcomp}
    \mathcal{R}_p \leq \mathcal{R}_{ID} + \mathcal{R}_0+ 2  \sqrt{ \mathcal{R}_{ID}  \mathcal{R}_0},
\end{equation}
where $\mathcal{R}_{ID}$ is the risk associated with approximating the full model by a pruned one and with probability $1-\delta$ satisfies
\begin{align*}
    {\mathcal{R}}_{ID} 
    &\leq 
    \epsilon^2M+M(1+\|T\|_2)^2n^{-\frac{1}{2}} 
    &\left( \sqrt{2\zeta dm \log (dm)\log\frac{en}{\zeta dm \log (dm)}}+ \sqrt{\frac{\log (1/\delta)}{2}}\right).
\end{align*} 
Here, $M = \sup_{x\in\Omega_x} \|u\|_2^2 \| g(W^T x)\|_2^2$ and $\zeta$ is a universal constant that depends on $g$. %the activation function.  


\end{theorem}
Theorem~\ref{thm:generalization} is developed by considering the ID as a learning algorithm applied to the output of the full model using unlabeled pruning data. This allows us to control the risk of the pruned model in terms of the risk of the original model and the additional risk introduced by the ID. Importantly, here we can control the additional risk in terms of the empirical risk of the ID and an additive term that decays as additional pruning data is used. Lemma~\ref{lem:prunedRisk} codifies this decomposition.
% \begin{remarks} 
% The statement $
%     \mathcal{R} \leq \mathcal{R}_p + \mathcal{R}_0+ 2 
%     \sqrt{ \mathcal{R}_p  \mathcal{R}_0}
% $ in Theorem~\ref{thm:generalization} follows naturally from the squared loss and the Cauchy-Schwartz inequality and holds for all pruning schemes.
% \end{remarks}


\begin{lemma}
\label{lem:prunedRisk}
Under the assumptions of Theorem~\ref{thm:generalization}, for any $\delta\in(0,1)$, $\cR_{ID}$ satisfies  
\begin{equation*}
    \mathcal{R}_{ID} 
    \leq \widehat{\cR}_{ID} + M(1+\|T\|_2)^2n^{-\frac{1}{2}} 
     \left( \sqrt{2p\log(en/p)}+ 2^{-\frac{1}{2}}\sqrt{\log (1/\delta)}\right)
\end{equation*}
with probability $1-\delta,$ where $M = \sup_{x\in\Omega_x} \|u\| ^2 \| g(W^T x)\|^2$ and $p=\zeta dm \log (dm)$ for some universal constant $\zeta$ that depends only on the activation function.
\end{lemma}
%\begin{lemma}
%The p-dimension of the network is 
%\begin{equation}
%    p \leq \zeta dm \log (dm)
%\end{equation}
%\end{lemma}
\begin{remarks}
We believe that the second part of the bound in Lemma~\ref{lem:prunedRisk} is likely loose since it relies on a pseudo-dimension bound for fully connected neural networks. However, when pruning with an ID we only consider subsets of existent neurons and it is plausible that in this setting the upper bound for the pseudo-dimension could be improved.
\end{remarks}

Crucially, an immediate consequence of using an ID for pruning is that we can explicitly control $\cR_{ID}$ in terms of the accuracy parameter. This relation between the ID accuracy and empirical risk is given in Lemma~\ref{lem:IDEmperical} and is what allows us to express the risk of the pruned network in Theorem~\ref{thm:generalization}. 

% Since the interpolative decomposition preserves the action of the network on the pruning data, (Lemma \ref{lem:IDrisk}), the empirical risk on the pruning data is bounded. 
%We can bound each term, beginning with $\hat{R}_s$.  
\begin{lemma}
\label{lem:IDEmperical} Following the notation of Theorem~\ref{thm:generalization}, an ID pruning to accuracy $\epsilon$ yields a compressed network that satisfies
$\widehat{\cR}_{ID} \leq  \epsilon^2 \|u\|_2^2 \| g(W^T X)\|_2^2 / n,$
where $X\in\R^{d\times n}$ is a matrix whose columns are the pruning data.
\end{lemma}


%; given a fixed $k$ this could be computed to assess the quality of an upper bound. 

%Finally, we bound p.  Since the network we are using has a subset of the hypothesis class of the full network, its p-dimension must be the same or smaller.  From [\megan{cite }], we have that:

%for some constant C.  

%Putting this all together, we get:  

%\begin{equation}
%    {\mathcal{R}}_p \leq \epsilon^2 \|u\| ^2 \| g(W^T x)\|^2 +(\|u^\top g(W^\top x) \| (1+2m))^2 ( \sqrt{\frac{2Cdm \log (dm)\log\frac{eN}{Cdm \log (dm)}}{N}}+ \sqrt{\frac{\log \frac{1}{\delta}}{2N}})
%\end{equation} 






% OLD
%To demonstrate why we expect that this method will be effective, we created a simple synthetic data set for which we know the form of a relatively minimal representation.  More details about the data set and experiment can be found in the supplement. We see that the interpolative decomposition keeps a set of neurons which represents the underlying function slightly better than the original model, ignoring duplicates but taking into account differences in the bias.  Magnitude pruning keeps duplicate neurons and fails to find important information with the same number of neurons.    
%We can train an overparameterized single hidden layer network to perform well on this task, and given a good initialization scale of the parameters, the neurons do not need to move very far[cite].  
%A pruning method based on magnitude pruning will not necessarily recognize the pairs of neurons which can perform well on this task when we prune to very small network sizes, since these neurons may not have a large magnitude in the larger model.    
%
%In practice, we see that ID is able to select neurons which resemble a close to minimal representative network.  
