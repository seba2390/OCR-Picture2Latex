

The Appendix provides a list of all methods we compare with (including references) and a lookup table to find the relevant experiments for each method (i.e., which figures and tables they appear in). 
%\todo[color=blue!40]{Jerry: clean up code and make public github. Get code from Megan for her part of the experiments as well.}
%todo[color=green]{Megan: Add in discussion or mention of new experiments from neurips rebuttal}
%\todo[color=green]{Megan: Add in further discussion on how model correlation implies preservation of for ex robustness, fairness, and why it's a good general choice.}
% \paragraph{More careful evaluation of pruning methods}
% We propose a set of evaluation metrics for pruning methods that goes beyond simple post-fine tuning test accuracy. 
% The first metric we define is correlation between two models, defined as the percent of test example predictions the two models agree on---details of this metric are discussed in Appendix~\ref{app:sec:correlation}.  We begin with a pretrained model, $M$, and compress it to create a model $M_c$.  We define model correlation as an aggregate measure of how well $M_c$ preserves the per-example decisions of $M$ on the test set.  This captures information about both examples which $M$  correctly and incorrectly classified.  Models are now often trained to have properties that go beyond test accuracy---for example robustness to adversarial attacks, sub-class classification accuracy, fairness, etc. On a practical level, no single number could perfectly capture the preservation on every possible metric. Our claim is that by better preserving per-example decisions, we may better preserve other special properties of the model. In order to demonstrate this, we show how our method can be used to prune a network while maintaining sub-class accuracy when neither the pruning nor fine-tuning methods have access to data from one of the classes in Appendix \ref{sec:sensitivity}.

\paragraph{More careful evaluation of pruning methods}
We propose a range of evaluations that goes beyond simple post-fine tuning test accuracy on standard vision benchmarks.
First is our proposed model correlation metric, discussed in detail in Section~\ref{sec:correlation}.
While certain benchmark problems are well studied and the literature has characterized layer sizes for more efficient networks, that cannot be relied upon for realistic problems practitioners want to solve. Therefore, we demonstrate the usability of our method by applying it to a non-standard problem and automatically discover a significantly more efficient allocation of flops per layer.
We also consider extensive pre-fine tuning evaluations.
In many cases, we may produce results before fine tuning that are sufficiently strong to remove the need for it. Moreover, fine tuning often washes out differences between different methods and reduces model correlation (particularly if the learning rate is too high for too many epochs).
We show that by preserving model correlation our method is able to preserve the sub-class accuracy of a class which has been removed from the pruning and fine tuning set.
%We show that our method is able to preserve model correlation even when an entire class is removed from the pruning set, as well as preserve the sub-class accuracy of the removed class.
% In order to demonstrate this, we show how our method can be used to prune a network while maintaining sub-class accuracy when neither the pruning nor fine-tuning methods have access to data from one of the classes in Appendix \ref{sec:sensitiviLastly, we show how maintaining  network structure allows us to compose our methods with other matrix-decomposition based techniques and provide post-fine tuning results.  One hidden layer results can be found in the Appendix.ty}.
Lastly, we show how maintaining  network structure allows us to compose our methods with other matrix-decomposition based techniques and provide post-fine tuning results.  One hidden layer results can be found in the Appendix.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %% Megn'a old version 10-7-22
% \paragraph{More careful evaluation of pruning methods}
% We propose a set of evaluation metrics for pruning methods that goes beyond simple post-fine tuning test accuracy. 
% The first metric we define is correlation between two models, defined as the percent of test example predictions the two models agree on---details of this metric are discussed in Appendix~\ref{app:sec:correlation}.  We begin with a pretrained model, $M$, and compress it to create a model $M_c$.  We define model correlation as an aggregate measure of how well $M_c$ preserves the per-example decisions of $M$ on the test set.  This captures information about both examples which $M$  correctly and incorrectly classified.  Models are now often trained to have properties that go beyond test accuracy---for example robustness to adversarial attacks, sub-class classification accuracy, fairness, etc. On a practical level, no single number could perfectly capture the preservation on every possible metric. Our claim is that by better preserving per-example decisions, we may better preserve other special properties of the model. In order to demonstrate this, we show how our method can be used to prune a network while maintaining sub-class accuracy when neither the pruning nor fine-tuning methods have access to data from one of the classes in Appendix \ref{sec:sensitivity}.
% 
% The second metric we consider is pre-fine tuning results. In many cases, we may produce results before fine tuning that are sufficiently strong to remove the need for it. Moreover, fine tuning often washes out differences between different methods and reduces model correlation (particularly if the learning rate is too high for too many epochs). While certain benchmark problems are well studied and the literature has characterized layer sizes for more efficient networks, that cannot be relied upon for realistic problems practitioners want to solve. Therefore, we demonstrate the usability of our method by applying it to a non-standard problem and automatically discover a significantly more efficient allocation of flops per layer. Lastly, we show how maintaining  network structure allows us to compose our methods with other matrix-decomposition based techniques and provide post-fine tuning results.  One hidden layer results can be found in the Appendix.



% This correlation metric 
% allows us to gauge if the two models are representing the same underlying function, and we demonstrate in the experiments below that our method is successful at doing just that.
% In contrast, many other pruning methods and fine tuning procedures do not produce models that are more similar to the original than a randomly initialized and trained model.

%Prior works have found that retraining a same sized network using randomly initialized weights can achieve the same accuracy results \cite{liu2019rethink}, leading to questions about which part of the train-prune-fine tune pipeline is responsible for various method's success, especially when they use many epochs of fine tuning to recover lost accuracy\cite{he2019fpgm,zhuang2020polar, li2019learnfilter}.
%\subsection{Model Preservation}

%\subsection{Broad Applicability}
%\label{sec:preft}
\paragraph{Broad applicability}
% \paragraph{ATOM3D}
We demonstrate the versatility of our method on atypical architectures and beyond vision tasks using the Atom3D Ligand Binding Affinity (LBA) \cite{atom3d} benchmark. This simulate a more typical use case for pruning than benchmark vision models. The LBA task predicts the binding strength between proteins and small molecules which is useful for drug discovery. We train and prune the 3DConv network from \cite{atom3d}, which uses 3-D convolutional layers, and achieves a RMSE of 
1.42.
%1.416. 
We use the same method to achieve a baseline RMSE of 1.43, and prune the network using our Iterative ID. The baseline network uses 12.8G FLOPs per example. For comparison VGG-16 on Imagenet uses 30G FLOPs. Figure~\ref{fig:mobilenet_atom3d} shows that Iterative ID is able to prune 95\% of the total FLOPS without any significant change in the accuracy of the network and does not require any fine tuning.  %This extended ``free flops" regime highlights the advantage of our method.  
% Note that we were unable to find any compression method with a 3DConv implementation in PyTorch.

% \begin{wrapfigure}{r}{0.5\linewidth}
% \includegraphics[width=\linewidth]{figures/Atom3d.pdf}
% \caption{Compressing a 3DConv network trained on the Atom3d  LBA task.  We see that Iterative ID can prune to roughly 5\% of the original FLOPs before losing significant accuracy.}
%     \label{fig:atom3d}
% \end{wrapfigure}

%\begin{figure}[!]
%    \centering
%\includegraphics[width=.8\linewidth]{figures/Atom3d.pdf}
%    \caption{Compressing a 3DConv network trained on the Atom3d  LBA task.  We see that Iterative ID can prune to roughly 5\% of the original FLOPs before losing significant accuracy.}
%    \label{fig:atom3d}
%\end{figure}


\begin{figure}[h]%[h!]
\centering
\begin{subfigure}{.49\textwidth}
  \includegraphics[width=\linewidth]{figures/Atom3d.pdf} % prev width=0.95
  %\label{fig:metrics}
\end{subfigure}
\begin{subfigure}{.49\textwidth}
  \includegraphics[width=\linewidth]{figures/mobilenet.pdf} % prev width=0.95
  %\label{fig:vggpreft}
\end{subfigure}
\caption{Pre-fine tuning pruning results on a 3D-Conv network for Atom3D (left) and MobileNet V1 for CIFAR-10 (right). 
We see that ID either matches or outperforms other methods.}
\label{fig:mobilenet_atom3d}
\end{figure}






\paragraph{Pre-fine tuning results}
We analyze the performance of ID compression against other pruning methods using the CIFAR-10~\cite{datacifar10} and Imagenet~\cite{deng2009imagenet} data sets with VGG16~\cite{simoyan2015vgg} and Mobilenet V1~\cite{MobilenetMainPaper}.  
The ID uses a held-out pruning set of 1000 data points on CIFAR-10 and 5000 data points on Imagenet.
Full hyper-parameter details can be found in the Appendix and code. 
Figures~\ref{fig:mobilenet_atom3d} and~\ref{fig:vgg16preft} illustrate results before fine tuning for structured and unstructured methods.
We see that on CIFAR-10 and ImageNet, Iterative ID matches or outperforms other methods which do not use any global fine tuning.
We even dominate unstructured methods which are typically more parameter efficient.
Many matrix decomposition based methods struggle to compress the depth-wise-separable convolution layers in the Mobilenet V1 network.
Moreover, Iterative ID is able to choose better per-layer sizes than the default VGG16 configuration on CIFAR-10. 
In the appendix, we show that the choice of how much to prune per iteration is not particularly sensitive, and demonstrate that 
%Figure~\ref{fig:vgg16preft}(left) shows 
pruning up to 35\% of the FLOPs results in no degradation to the pre-fine tuning accuracy.
Results for MobileNet V1 on ImageNet can be found in the Appendix.

\begin{figure}[h!]
\centering
\begin{subfigure}{.49\textwidth}
  \includegraphics[width=0.95\linewidth]{figures/vggPreFT.pdf} %prev width=0.95
  \label{fig:vggpreft}
\end{subfigure}
\begin{subfigure}{.49\textwidth}
  \includegraphics[width=0.95\linewidth]{figures/vgg_nolocFT_ImgNet.pdf} % prev width=0.95
  \label{fig:metrics}
\end{subfigure}%
    \caption{
    Pre-fine tuning pruning results on VGG-16 for CIFAR-10 (left) and ImageNet (right).
    ID either matches or outperforms other methods, including unstructured methods (dotted lines) which are typically more parameter efficient.
    %Dotted lines are unstructured methods which are more parameter efficient, 
    Solid lines are structured.
%    Methods shown in dotted lines are unstructured, and therefore remove more parameters per flop than structured methods.  
    Note the ``SVD'' method 
    %does not include any fine tuning, but does 
    changes the structure of the network. 
    Model correlation results can be found in the Appendix.}
\label{fig:vgg16preft}
\end{figure}

\paragraph{Model correlation as a proxy for preserving fairness}

We conduct an experiment to demonstrate the connection between model correlation and a simple fairness metric of per-class accuracy.  We begin with a VGG16 model $M$ trained on all 10 classes of Cifar10.  It performs reasonably well on all 10 classes, so we consider it ``fair''.  We then remove a class from the pruning set to simulate an under-represented class (but leave it in the train and test sets). We then prune $M$ using our ID-based method, creating a compressed model $M_{id}$---we do not perform any fine tuning. Sub-class accuracy for each class can be seen in figure \ref{fig:id_sensitivity_class} as well as the correlation of $M_{id}$ with respect to $M$. Despite not having access to an entire class of data, we can prune the model to 50\% FLOPs while only losing a few percent accuracy on the missing class and retaining reasonably high model correlation. As a comparison we prune the model $M$ with magnitude pruning to create a compressed model $M_m$. The magnitude pruned model loses almost all accuracy on the missing class when we prune to 50\%, and its model correlation is initially only 10\%. We fine tune with the abbreviated data set in an attempt to recover accuracy (figure \ref{fig:mag_sensitivity_class} shows the correlation and sub-class accuracies). While $M_m$ recovers on the represented classes, it fails to recover any accuracy for the unrepresented class during fine tuning.  
 

\paragraph{Model architecture preservation and composition of methods }

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/2plots.pdf} %prev width=0.9
    \caption{
    Composing ID with other compression methods for VGG-16 on CIFAR-10~(left) and ImageNet~(right).  
    For each ID+compression method, the original network was pruned using ID until the correlation degraded on the pruning set. A second method was then applied to the compressed network. 
    This composition produced
    %Starting with an ID compressed network produced 
    smaller and more accurate networks than either method alone.}
    \label{fig:combiningID}
\end{figure}
% \begin{figure}[!]
%     \centering
% \includegraphics[width=.9\linewidth]{figures/vggCombine.pdf} 
% \caption{Results from composing methods on the CIFAR-10 data. set for VGG-16.  We prune a trained model using ID to 30\% FLOPs reduction, and then prune further using low-rank methods which include fine tuning.}
%     \label{fig:CombineCifar}
% \end{figure}

Because our method preserves network structure and only reduces layer sizes it can be easily combined with other methods, see Figure~\ref{fig:combiningID}.  
We first apply Iterative ID
%to eliminate any unnecessary variation in the original network
, pruning until the the model correlation begins to drop on the pruning set.
Then we apply a secondary compression method.
%For each of the combination of methods, we apply iterative ID to the original network until the model correlation starts to drop on the pruning set.  Then we apply a secondary compression method to the pruned network.  
This composition is easy because we are simply replacing the original model with an ID-pruned model that has the same architecture.  The secondary algorithms we use insert extra layers with different computational structure into the network, so composing them with each other is non-trivial. In each example we see that pruning first with ID and then applying a secondary algorithm boosts the performance of the secondary algorithm.  


%%\subsection{Combination of Methods}
%Some low-rank methods perform various kinds of factorizations of the weight matrix to achieve compression and speedups.  However, these methods insert extra layers into the network, which can limit opportunities for further compression, and we do not consider them to be in direct competition with our own.  Here we present evidence that our method can be used to prune neurons without loss of accuracy before applying low-rank methods to the pruned network.  Figure \ref{fig:CombineCifar} shows the pre-fine tuning accuracy of ID followed by various low-rank methods.  In each case, we prune the network with ID to just within the edge of the "free-flops" regime, and then compress the pruned network with a low-rank method. We find that a combination of ID+low rank methods outperforms either method alone.


%\subsection{Post-Global-Fine-Tuning Results}
%\label{sec:postft}
\paragraph{Post-global-fine tuning results}
In Tables \ref{tab:cifarVgg}, and~\ref{tab:vggImgNet} we demonstrate the effectiveness of our method for VGG-16 on CIFAR-10 and ImageNet after moderate amounts of global fine tuning, both alone and when combined with other methods. 
Hyperparameter details can be found in the Appendix. 
Networks pruned with ID attain competitive accuracies while having a much higher prediction correlation with their original models.
This evidence suggests that we have truly compressed the network rather than retraining a new one.
Other methods often do not correlate much more than an independently trained model.
Surprisingly, we find that in some settings combining ID with other matrix decomposition methods like LRank\cite{idel2020lrank} can be competitive without any global fine tuning (Tables~\ref{tab:cifarVgg},~\ref{tab:vggImgNet}).

We also find that ID pruning and then globally fine-tuning a MobileNet V1 architecture results in an accuracy of 89.4\% on Cifar-10 and a correlation of 92.6\% with the original model after 60 epochs at a 42\% FLOPs reduction, as opposed to a model trained from scratch which achieves an accuracy of 88.9\% and a correlation of 88.6\% after 120 epochs.  
%ID+LRank on CIFAR-10 achieves very competitive accuracy with no global fine tuning
%This pre-fine tuning advantage persists into fine tuning, see Appendix. 
%for example pruning 50\% FLOPs on VGG-16 using CIFAR-10.  We see a similar effect on VGG-16 ImageNet, and also provide results using moderate amounts of global fine tuning. Fine tuning curves for LRank and ID+LRank can be found in the Appendix.  



% In addition to global fine tuning, we compose the our ID method with compression methods that incorporate fine tuning.
% This composition is trivial because the ID preserves the network computational structure.
% Because the ID preserves the network computational structure, it is trivial to compose it with any other compression method.
% We compress with ID to the edge of the ``free-flops'' regime, and then compose.
%with another method that incorporates fine tuning.
%Such composition is greatly beneficial, and is also important for understanding of compression method.


% \paragraph{CIFAR-10}
% We present results for Mobilenet and VGG-16 after fine tuning.  We compare to several methods that do only local fine tuning in Figure~\ref{fig:CombineCifar}, and some methods that do global fine tuning in Tables~\ref{tab:cifarMobile} and~\ref{tab:cifarVgg}.  Interestingly, we find that composition of our method with other low-rank methods (from Cat. I) that do local fine tuning outperforms either method alone, and is comparable to methods which perform 200 epochs of global fine tuning while also retaining much higher model correlation. The total time for local fine tuning done by any of these methods is less than the time needed for a single epoch of global fine tuning.  
% \begin{figure}
%     \centering
% \includegraphics[width=.9\linewidth]{figures/mobilenet.pdf}
%     \caption{Results pruning Mobilenet V1 on the CIFAR-10 dataset.  The constant-fraction ID uses no fine tuning, however, the methods that we compare to use local fine tuning.
%     For citation and implementation details on comparison methods, see Appendix~\ref{app:sec:comparison_methods}.
%     }
%     \label{fig:mobilenetCifar}
% \end{subfigure}

% \begin{table}[h!]
%     \centering
%     \begin{tabular}{cccccc}
%         Method & Flops Reduction (\%) & Accuracy & $\Delta$ & Correlation& Epochs  \\ \hline 
%         Scratch&42 & 88.9 & --- & 88.6& 120\\
%         ID & 42 & 89.4&.6 &92.9&60\\
%         %Depth&42.5& 86.15&.15 & - & -\\
%     \end{tabular}
%     \caption{Post-fine tuning MobileNet V1 results on CIFAR-10.
%     %FR refers to the percentage of FLOPs removed and 
%     $\Delta = original - pruned$. Scratch represents a smaller model trained from scratch.}
%     \label{tab:cifarMobile}
% \end{table}


\begin{table}[ht!]
    \centering
    \begin{tabular}{cccccc}
        Method & FLOPs Reduction (\%) & Accuracy & $\Delta$ & Correlation& Epochs  \\ 
        \midrule

        ID & 50 & 93.31 &0.30 & {97.29} & 60\\
        ID+LRank &50&93.43&0.31&95.9&{0}\\
        \hline
        %FPGM & 34 & 93.8 & -.2 & 93.89 & 200 \\
        HRank & 54 & 93.43 & -0.50 & --- & 480 \\
        Polar & 54&93.92&0.04 & --- & 200\\
        NS &51& 93.62&{-0.26}&---&200\\
        Mag &50&93.56&-0.04&93.15&200\\
        \hline
        Independent & 0 & 93.60& --- & 93.02&160 \\
    \end{tabular}
    \vspace{0.2em}
    \caption{Post-fine tuning VGG-16 results on CIFAR-10. ID results are averaged over 5 trials, with a standard deviation of $\pm 0.36$. ``Independent'' is an independently trained model. 
    In the 50\% FLOPs region, ID produces the most correlated model, while ID+LRank produces a model with reasonable accuracy without any global fine tuning.
    $\Delta\equiv$ original accuracy - pruned accuracy.
    }
    \label{tab:cifarVgg}
\end{table}
\vspace{-1em}



%\paragraph{ImageNet}
%However interleaving local fine tuning into the compression method can improve results. 
%A benefit of our method is that the network computational structure is preserved.
%Therefore, an ID pruned model can be composed with any other compression technique.
%Note, this is not possible with low-rank methods.
% Figure~\ref{fig:compose_vggImgNet} shows that for VGG the composition of ID with other compression methods that incorporate local fine tuning performs better than either method alone.
% For a comparison on compression methods which use local finetuning on MobileNet, see Appendix~\ref{app:sec:imgnet_extra}.
% Finally, we globally fine tune our compressed model, see Table~\ref{tab:vggImgNet}.
% We compare against methods which provided their original and pruned models so that we could compute the model correlation.
% By composing ID with methods that incorporate local fine tuning, we are able to achieve near-comparable performance to methods which use global fine tuning.
% Interestingly, our model correlation is the highest in spite of our slightly lower accuracy---highlighting that the ID is better preserving the underlying model.

% % \begin{figure}
%     \centering
% \includegraphics[width=.9\linewidth]{figures/vgg_yeslocFT_ImgNet.pdf}
%     \caption{VGG-16 compression results on ImageNet by compsing ID with other methods.
%     We see that composing ID with other methods that incorporate local fine tuning performs better than the original methods.}
%     \label{fig:compose_vggImgNet}
% \end{figure}

\begin{table}[h!]
    \centering
    \begin{tabular}{cccccc}
        Method & FLOPs Reduction (\%) & Accuracy & $\Delta$ & Correlation & Epochs  \\
        \midrule
        %ID & 45 & 65.81 &5.73 & 78.88 &70\\
        %ID+LRank & 43 & 70.62 & 0.89 & 91.72 & 30 \\
        %ID+LRank & 43 & 70.66 & 0.85 & 91.92 & 40 \\
        ID+LRank & 43 & 68.84 & 2.67 & 85.66 & 0\\
        ID+LRank & 43 & 70.50 & 1.01 & 90.90 & 10 \\
        ID+PCA & 54 & 69.18 & 2.33 & 87.13 & 200 \\
        %ID+PCA & 54 & 68.87 & 2.64 & 86.31 & 65 \\
        %ID+PCA & 54 & 69.20 & 2.31 & 87.32 & 250 \\
        \hline
        LRank(\cite{liebenwein2021alds}-impl.) & 43 & 70.30 & 1.21 & 90.38 & 10 \\
        ThiNet & 46 & 66.59 & 0.89 & 70.15 & 30\\
        AMC & 48 & 70.49 & 0.65 &76.03&120\\
        CP & 53 & 68.20 & 2.66 & 78.47 & 10\\
    \end{tabular}
    \vspace{0.2em}
    \caption{
    %\jerry{Check the Epochs, a bit confusing}
    Post-fine tuning VGG-16 results on ImageNet.  
    We list methods which release both the original and compressed model, so that we can compute the correlation.
    ID composed methods produce competitive accuracies at a range of FLOPs reductions, and produce better correlated models, even without any fine tuning.
    }
    \label{tab:vggImgNet}
\end{table} 