%\outline{
%\begin{itemize}
%    \item Magnitude approach baseline.
%    Offers no guarantees, cannot yield minimal representation because of potential duplicate entries of similar magnitude.
%    \item often state goal to remove redundancy, but magnitude pruning rather crude bc possible to have duplicate neurons at similar magnitude.
%    \item along both of these goals (efficiency, understand minimal model size), we argue that interpolative decompositions provide a better solution than magnitude pruning.
%\end{itemize}
%}

%\jerry{intro a bit rough, please wait until this message disappears before reading intro.}



%The over-parameterization of modern deep networks has driven interest in compression methods to speedup training and inference.


%Modern vastly over-parameterized deep networks have been shown to be effective models for a wide range of machine learning tasks. However, the same over-parameterization that seemingly helps drive their efficacy makes them computationally expensive to train and evaluate at inference time. 
There has recently been a significant theoretical and empirical effort to understand the necessity of over-parameterization and develop algorithmic techniques that can effectively compress (or prune) deep learning models while retaining their performance. This work is largely driven by results that show over-parameterization may aid in training via stochastic algorithms, but that the solutions found are often highly redundant~\cite{frankle2018lottery,frankle2020linear,jaderberg2014speeding,luo2017thinet} and therefore can be compressed. We develop a novel compression methodology that uses unlabeled data to automatically exploit redundancy in trained models and perform structured pruning (i.e., removing redundant neurons/channels) that simultaneously updates the remaining network parameters. By construction, our method provides theoretical guarantees even without any additional training, though performance can typically be further improved by fine tuning.

% Modern over-parameterized deep networks are effective.  
% They are also computationally expensive.
% This has driven interest in compression methods like network pruning to speed up training and inference. 
%In addition to increasing efficiency, compression reveals information about the representational capacity of neural networks.
%In addition to increasing efficiency, compression allows us to reason about the expressive capacity of neural networks.
%Though there is work that shows these over-parameterized models more easily train to ``good'' solutions~\cite{allen2019learning,weinan2020comparative}, it has also been observed
%bartlett2020benign,belkin2018overfitting,hastie2019surprises,
%in parallel work
%that the model fitting process often finds highly redundant (i.e. compressible) solutions via network pruning~\cite{frankle2018lottery,frankle2020linear,luo2017thinet,jaderberg2014speeding}.
% Though there is work that shows that over-parameterized models more easily train to ``good'' solutions, it has been observed that these solutions are often highly redundant ~\cite{frankle2018lottery,frankle2020linear,jaderberg2014speeding,luo2017thinet}.  
% This redundancy can be discovered with compression via network pruning, as we would expect redundant networks to be more compressible.  
%One important class of network pruning is called structured pruning.
%The resulting pruned networks are dense, and do not sacrifice the computational efficiency that other unstructured (sparse) pruning methods do.
%Compression and pruning could also tell us something about the representational capacity of neural networks. 
% We perform structured pruning which removes neurons to create a dense network that does not sacrifice computational efficiency like unstructured sparse pruning does.
%, whereas unstructured pruning creates sparse networks.  
%Our method acts to approximate the output of a layer after the activation function, in such a way that is interpretable as selecting neurons in one layer and updating the coefficients in the next to compensate.  

The core technical development in our pruning methodology is the novel use of a so-called interpolative decomposition (see Section~\ref{sec:ID}) to produce low-rank approximations of layers that directly incorporate the action of the activation function. 
%While it leverages a low-rank approximation, our method differs substantially from methods that directly approximate weight matrices by directly incorporating the activation function. 
By incorporating the activation function, our method differs substantially from existing low-rank approximation methods that directly approximate the weight matrices.  
Unlike other low-rank methods, we produce a dense, smaller network that preserves the original structure.
%In fact, incorporation of the activation function renders the SVD unsuitable for our task. 
%Incorporation of the activation function renders the singular value decomposition (SVD) unsuitable for our task.
Furthermore, we are able to ``propagate'' low-rank factorizations through activation functions by forcing the approximation to be built using columns of the appropriate matrices directly---a process that allows for explicit sub-selection of neurons/channels.
In addition, the factorization naturally provides interpolation matrices that allow us to update the weights of layers adjacent to the one currently being pruned. This allows us to directly produce a compressed network that well approximates (to any desired accuracy) the original network without any additional training---a feature we show theoretically for simple fully connected networks in Section~\ref{sec:pruneID}.  Notably, our method does not require any labeled data.
Extensions to different layer types and more complex architectures can be found in Section~\ref{sec:extendid}. A key feature of our method is its ability to choose per-layer sizes based on the trade-off between FLOPs and the estimated error induced by pruning. We place our work in context within the larger field in Section \ref{sec:context}, and compare it more directly to related works in Section \ref{sec:related}.  

To complement our algorithmic developments and theoretical contributions, in Section~\ref{sec:experiments} we demonstrate that our method scales to large and complex networks of practical interest.  Importantly, we show that we can effectively prune large networks for Atom3D~\cite{atom3d}
%Fashion-MNIST~\cite{datafashionmnist}
, CIFAR-10~\cite{datacifar10}, and ImageNet~\cite{deng2009imagenet}. 
We also introduce a metric of model similarity based on prediction correlation and demonstrate that our method produces compressed models that are highly correlated with the original.  
Relative to competing methods we show better accuracy before fine tuning, and our methods are competitive with other state-of-the-art pruning methods after additional training is performed. Notably, our results are produced using relatively few points for the pruning process (1000 additional unlabeled data points for CIFAR-10, and 5000 for ImageNet). Our methodology suggests several key avenues for future research: we discuss those, along with limitations of our method in Section~\ref{sec:dis}.

%\jerrytwo{Note that unlike other low-rank compression methods, we produce a dense, smaller network that preserves its structure.}\meg{i.e. does not add layers}

%\jerrytwo{
%%Relative to existing techniques, our use of the interpolative decomposition provides several key advantages. 
%To facilitate this discussion,
%%of placing our method within the context of the varied literature, 
%in Table~\ref{tab:taxonomy} we provide a taxonomy of compression methods which operate in the parameter space.
%Our taxonomy consists of two axes: (1) how parameters are removed and the resulting network computational structure, and (2) how the parameters are corrected post removal.
%%We use our taxonomy to structure our related works discussion.
%%Given the breadth of work in this space, we Table X clarifies the discussion.
%Our method combines advantages of both channel pruning and low-rank compression methods.
%}




To summarize, our contributions in this paper are:
%\textbf{Contributions}
\begin{itemize}[leftmargin=*]
    %\item[] \textbf{A principled pruning methodology.} We use interpolative decomposition to develop a principled approach to structured pruning of neural networks that requires only unlabeled data and allows us to directly incorporate the action of non-linear activation functions. Our methodology is applicable to a variety of layer structures and network architectures. 
    \item[] \textbf{A principled pruning method.} We introduce an algorithm for structured pruning based around the use of an interpolative decomposition that requires no labeled data and automatically determines per-layer sizes.   
    %\item[] \textbf{Theoretical guarantees.} We provide theoretical guarantees for the output of our pruning methodology even in the absence of additional training. This takes the form of a generalization guarantee for the pruned network (with respect to the generalization properties of the model we are pruning).
    \item[] \textbf{Theoretical guarantees.} We provide theoretical guarantees for the output of our pruned model before any pre-fine-tuning is performed. 
    % output of our pruning method with respect to the original model. 
    %\item[] \textbf{Practical efficacy.} We demonstrate the efficacy of our pruning methodology for networks of varying complexity. Specifically, we use Fashion MNIST and synthetic data to clearly illustrate the effectiveness of our method relative to magnitude pruning. We then provide comparisons against structured pruning methods on CIFAR-10, Atom3D and Imagenet and show that our method performs better before fine-tuning and provides comparable results after additional training.
    \item[] \textbf{Practical efficacy.}  We demonstrate the efficacy of our method across the ATOM3D, CIFAR-10 and ImageNet data sets and using a variety of models.  Because our method only reduces layer sizes, it can be easily composed with other
    compression 
    %low-rank 
    methods. This allows us to preserve models (with respect to accuracy and predictions) for Cifar-10 and Imagenet at significant compression ratios without the use of any global fine tuning.
    
    %preserves model structure (i.e., it only reduces layer sizes), it can be easily composed with other other low-rank methods. This allows us to preserve models (with respect to accuracy and predictions) for Cifar-10 and Imagenet at significant compression ratios without the use of any global fine tuning.
    
    % to achieve local-only finetuning accuracies and model preservation on Cifar-10 and Imagenet that are comparable to methods that use extensive finetuning.  
    %\item{} 
    %\jerry{We could add our better evaluation methodology as a contribution: correlation, amount FT, data usage, etc}
\end{itemize}
 
% Our pruning method acts to approximate the output of a layer with a 
% classical
% % classical, structured low-rank
% matrix approximation technique called the interpolative decomposition.
% Importantly, this matrix factorization technique allows us to approximate the activation output and trace back through the non-linearity to select neurons to prune.
% For a generic matrix $A \in \R^{n \times m}$ the interpolative decomposition uses a subset $\I \subset [m]$ of the columns of A
% %$\operatorname{col}(A)$
% as the basis for the approximation. An interpolation matrix $T$ approximates the remaining columns as a linear combination of $A_{:,\I}$.
% %$\operatorname{col}(A)$
% This gives $A \approx A_{:,\I} T$.  
% %The interpolative decomposition can be computed using a rank-revealing QR decomposition.
% We compute $\I$ and $T$ using a rank-revealing QR decomposition.  
% %For a simple one hidden layer network $U^\top g(W^\top X)$
% %We use the interpolative decomposition to approximate the activation output of a layer.
% %We use the interpolative decomposition to approximate the activation output of a layer. 
% Once we have an approximation of the activation output,   
% we can trace back through the non-linearity to select neurons (or channels), because the sub-selection operator commutes with the non-linear activation function.
% The interpolation matrix is then propagated into the next layer to encode it into the network.
% %The pruning level can be can be dynamically set for a desired accuracy in a principled manner, due to properties of the rank-revealing QR decomposition.
% This technique naturally leads us to bounds on the error induced by the pruning, and therefore to generalization bounds on the pruned network. %as a whole.  
% %Given our bounds, we can select the amount of pruning based on the degradation in the model.   
% Our pruning method only relies on an in-distribution unlabeled data set.
% Empirically, we have observed the necessary size for this pruning set is reasonable (1000 data points on Fashion MNIST and CIFAR-10).

%However, much of this work relies on classical, albeit oft rediscovered, pruning techniques based on either weight/neuron magnitude~\cite{janowsky1989pruning} or expensive repeated assessment of network accuracy to determine each unit's relevance~\cite{mozer1989skeletonization,mozer1989using}.

% =====================================
%\textbf{Contributions}
%\begin{itemize}
%    \item[] \textbf{A principled pruning methodology.} We develop a principled approach to structured pruning of neural networks that requires only unlabeled data and allows us to directly incorporate the action of non-linear activation functions. Our methodology is applicable to a variety of layer structures and network architectures. 
    % We propose a principled approach to structured pruning via the interpolative decomposition  
    % which 
    % %This matrix approximation technique 
    % allows us to operate ``around'' non-linear activations: we can approximate the activation output and then 
    % %trace back through the non-linearity to
    % select 
    % neurons or channels
    % %structured subsets of the weights 
    % to prune.
    % We show how to prune fully connected and convolution layers, leading to more general multi-layer networks.
%    \item[] \textbf{Theoretical guarantees.} We provide theoretical guarantees for the output of our pruning methodology even in the absence of additional training. This takes the form of a generalization guarantee for the pruned network (with respect to the generalization properties of the model we are pruning).
    % We provide theoretical guarantees on the accuracy of the interpolative decomposition, and a generalization bound for a single hidden layer fully connected network.
%    \item[] \textbf{Practical efficacy.} We demonstrate the efficacy of our pruning methodology for networks of varying complexity. Specifically, we use Fashion MNIST and synthetic data to clearly illustrate the effectiveness of our method relative to magnitude pruning. We then provide comparisons against structured pruning methods on CIFAR-10 and show that our method performs better before fine-tuning and provides comparable results after additional training.
    % We demonstrate the efficacy of our pruning method at various levels of network complexity.
    % On Fashion MNIST and a synthetic data set we observe that interpolative decomposition outperforms structured magnitude pruning.
    % Against other modern structured pruning methods on CIFAR-10 the interpolative decomposition performs comparable to state-of-the-art, and performs especially well before fine-tuning.
% WANT TO INCLUDE SOME OF THIS
%Empirically we demonstrate that pruning with interpolative decompositions performs competitively with SOTA pruning methods, and does especially well before fine-tuning.
%Beginning with simple two layer (one hidden layer) neural nets on a synthetic toy model, we demonstrate the strengths of our method in comparison to magnitude pruning. 
%Our method is effective on more complicated data sets such as Fashion MNIST with multiple hiddden layer networks.
%, and show how to extend our method to multiple layers.  
%Finally, we extend our method to benchmark models of modern deep convolutional networks. 
%Empirically we demonstrate that pruning with interpolative decompositions performs competitively with SOTA pruning methods, and does especially well before fine-tuning.
%show that it is \todo{adjective} to state of the art pruning methods.     
%\end{itemize}


%With the interpolative decomposition, we provide a principled approach to pruning, which accounts for similarity between neurons or channels and mitigates the accuracy loss by propagating an interpolation matrix into the next layer.  




%Pruning~\cite{blalock2020state,gale2019state} is one such approach to model compression that aims to find a sub-network which preserves most of the representative capacity of the original network.
%Network pruning can be \emph{unstructured} where individual parameters are removed, or \emph{structured} where neurons or channels are removed.
%Unstructured pruning has been shown to produce better sub-networks for the same amount of parameters removed, but are hampered by the need for sparse BLAS libraries and even specialized hardware.\todo{cite}.
%Structured pruning, while losing more accuracy per parameter removed, creates a dense sub-network which can readily take advantage of dense BLAS libraries and accelerator hardware\todo{cite}.


% =======================================
% =======================================

% \paragraph{Advantages of the interpolative decomposition}
% %Pruning via interpolative decompositions has several advantages over other structured pruning methods.
% %While other structured pruning methods attempt to improve upon magnitude pruning in various ways, pruning with interpolative decompositions incorporates these benefits different benefits together into one method.

% Interpolative decompositions provide a principled approach to pruning with several advantages over other methods.
% Our method accounts for similarity between neurons or channels, and 
% %And unlike many of the unstructured pruning methods our method
% mitigates accuracy loss by propagating an interpolation matrix into the next layer.
% We have theoretical guarantees on the accuracy of the interpolative decomposition, as well as a generalization bound.
% Magnitude pruning does not account for duplicate neurons of similar magnitude.
% Many structured pruning methods do not preserve a reasonable amount of accuracy without fine-tuning.
% In regards to the work with coresets, we provide a generalization bound and demonstrate that our method works on convolution layers and a wider range of experiments.
% The interpolative decomposition approximates the full output of a layer, not just the weights as with filter decompositions.


%While potentially effective in certain settings, magnitude based pruning yields no theoretical guarantees on the approximation properties of a neural network.
%Moreover, such a scheme is highly unlikely to yield a minimal representation, since for example,  it does not account for the possible duplication of neurons with a similar magnitude.  

%\jerry{putting some stuff here to be reworked \\
%\begin{itemize}
%    The interpolative decopmosition accounts for similarity between neurons
%    \item Notably, this approach is significantly different from direct low-rank approximation of weight matrices~\cite{neyshabur2014search} since it fundamentally allows for principled reduction of the number of neurons and does not simply seek to accelerate the relevant matrix products.
%    \item It is also more practical than weight pruning~\cite{blalock2020state} since the dense matrix product computational structure is retained.
%    \item low-rank matrix approximation requires more work (see notes)
%\end{itemize}
%}


% ====
%The interpolative decomposition can be computed via a rank-revealing QR factorization, and its accuracy can be directly computed as a function of $R$ from the factorization.
%We provide a generalization bound for pruning a one hidden layer fully connected network, as well as theoretical guarantees for the existence of interpolative decompositions.
%Our proof approach interprets pruning as learning algorithm for the original network.
%and use recently derived psuedo-dimension bounds
%Though our proof approach could be used for any pruning method, but it requires that we be able to control the empirical risk of our pruning method (which we are able to do).
%We provide
%Since the interpolative decomposition preserves the action of the layer on data, a generalization bound follows naturally (before fine-tuning).  We can accomplish this despite using unlabeled data for the pruning.  

%In comparison to classical pruning methods interpolative decompositions have several advantages.



%We can provide theoretical guarantees for the existence of such interpolative decompositions, and bounds on the change in the loss for that data set that is used to prune.  
%Given some assumptions about this data set, we can generalize to the ...? 
%Interpolative decompositions are hard to compute, but we give a widely used method based on the QR factorization which is slightly sub-optimal, but is empirically well-behaved.  

%In terms of efficiency and better understanding minimal neural network representations, we argue that interpolative decompositions serve as a strong tool.

%In contrast compression schemes based on interpolatvie decompositions provide provable guarantees for neural networks.
%In addition, we provide empirical results 








% Backup
%Model compression is an attractive remedy to the growing computational requirements of modern deep networks.
%Compression makes training or inference more efficient, benefiting both large scale cloud computing and resource constrained edge devices.
%Beyond efficiency, compression reasons about what are minimum model representations.
%Modern deep networks are though to be overparameterized, which again drives the interest in compressing these models.
%However, there is also the idea that such models are easier to train to find ``good'' solutions.
%This concept can be made precise in the random features regime through its connections to so-called ``ridgeless'' regression~\cite{bartlett2020benign,belkin2018overfitting,hastie2019surprises}, and recent work has explored similar concepts in the more general setting~\cite{allen2019learning,weinan2020comparative}.
%However, in parallel work it has been observed that, even if over-parameterization aids in optimization, the model fitting process often finds highly redundant (i.e., compressible) solutions via network pruning~\cite{frankle2018lottery,frankle2020linear,luo2017thinet,jaderberg2014speeding}.
%
%Pruning~\cite{blalock2020state,gale2019state} is one such approach to model compression that aims to find a sub-network which preserves most of the representative capacity of the original network.
%Network pruning can be \emph{unstructured} where individual parameters are removed, or \emph{structured} where neurons or channels are removed.
%Unstructured pruning has been shown to produce better sub-networks for the same amount of parameters removed,  but are hampered by the need for sparse BLAS libraries and even specialized hardware\todo{cite}.
%Structured pruning, while losing more accuracy per parameter removed, creates a dense sub-network which can readily take advantage of dense BLAS libraries and accelerator hardware\todo{cite}.
%
%Recently, there has been substantial research interest in pruning neural networks either by neurons or weights~\cite{frankle2018lottery,frankle2020pruning,blalock2020state}.
%However, much of this work relies on classical, albeit oft rediscovered, pruning techniques based on either weight/neuron magnitude~\cite{janowsky1989pruning} or expensive repeated assessment of network accuracy to determine each unit's relevance~\cite{mozer1989skeletonization,mozer1989using}.
%While potentially effective in certain settings, magnitude based pruning yields no theoretical guarantees on the approximation properties of a neural network.
%Moreover, such a scheme is highly unlikely to yield a minimal representation, since for example,  it does not account for the possible duplication of neurons with a similar magnitude.  



% In Section~\ref{sec:ID} we describe the interpolative decomposition for generic matrices, as well as the relevant linear algebra background.
% We use this in Section~\ref{sec:pruneID} to build primitives that prune single fully connected layers, and show the generalization bound.  
% Section ~\ref{sec:extendid} extends our method to more complicated networks.  
% In Section~\ref{sec:experiments} we provide extensive empirical results to both better understand our method and demonstrate that it performs comparably to other state of the art pruning methods. We discuss limitations of our method and possible avenues for future work in Section \ref{sec:dis}.



