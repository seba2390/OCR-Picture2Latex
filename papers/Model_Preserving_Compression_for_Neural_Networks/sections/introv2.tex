There has recently been a significant theoretical and empirical effort to understand the necessity of over-parameterization for deep learning models and to develop algorithmic techniques that can effectively compress (or prune) them while retaining performance. 
This work is largely driven by results that show over-parameterization may aid in training via stochastic algorithms, but that the solutions found are often highly redundant~\cite{frankle2018lottery,frankle2020linear,jaderberg2014speeding,luo2017thinet} and therefore can be compressed. 
Typically the focus has been on reducing parameter counts or FLOPs while maintaining accuracy---a task that often necessitates significant fine tuning of compressed models. However, we argue that further criteria are warranted to enable the adoption of compression methods amongst practitioners: 
%We argue that further design guidelines on model preservation and usability are warranted to enable the adoption of compression methods amongst practitioners.
%No current class of compression methods are able to satisfy our criteria. 
(1) preserving the model's per-example decisions ensures the preservation of more fine-grain properties, e.g. 
%such as
fairness~\cite{barocas-hardt-narayanan,rothfair,goel2018fair,roth2019fair,aram2021fair} or adversarial robustness~\cite{carlini2019evaluating,liang2019adversarial,kolter2019adversarial,hein2020adversarial,silva2020adversarial};
(2) compressing networks while retaining their computational structure ensures compatibility with the rest of the machine learning pipeline; and 
%Two of the greatest challenges when compressing are determining the per-layer sizes and fine tuning.
(3) automatically determining per-layer sizing and minimizing the need for fine tuning alleviates key challenges when using compression methods in practice.
%make compression methods very usable.

%Unfortunately, matrix approximation methods change the network's structure by adding additional layers, creating additional dependencies to the ML pipeline that a practitioner must resolve.
%Structured pruning methods do preserve the network structure, but do a poor job at preserving the model's decisions.
%Our method combines advantages of matrix approximation and structured pruning methods to satisfy our criteria.

% Matrix approximation methods~\cite{denten2014svd,idel2020lrank,liebenwein2021alds,peng2018group,lebedev2015cpdecomp}
% %\cite{denten2014svd,idel2020lrank,jarderberg2014lowrank,liebenwein2021alds,peng2018group,lebedev2015cpdecomp,zhang20153dfilter}
% can satisfy the first criteria, but not the second as they add additional layers.
% Structured pruning methods~\cite{he2019fpgm,he2018amc,liebenwein2020provable,liu2019rethink,luo2017thinet}
% %\cite{liu2019rethink,liebenwein2020provable,he2018amc,liu2017netslim,luo2017thinet,he2019fpgm}
% can satisfy the second criteria, but not the first as they do a poor job of preserving the model's decisions.

We develop a novel compression method that satisfies these three criteria. In particular, it uses unlabeled data to remove redundant neurons/channels while simultaneously updating the remaining network parameters to explicitly correct for the removed neurons/channels. 
The core technical development in our pruning method is the novel use of an
%so-called 
interpolative decomposition (see Section~\ref{sec:ID}) to approximate the post-activation output of layers.
The interpolative decomposition~(ID)
explicitly represents 
%represents
%approximates 
the pruned neurons/channels as a linear combination of those kept---allowing us to preserve the model with little to no fine tuning.
%produce low-rank approximations of layers that directly incorporate the action of the activation function. 
% We retain and even improve upon the model preservation properties of matrix approximations by constructing approximations of the activation outputs, something that other matrix methods cannot do.
%(see Section~\ref{sec:ID}).
%Moreover, the specific structure of the ID allows us to ``propogate'' our approximations through activation functions.
Moreover the specific structure of the ID allows us to sub-select channels backward through the activation function, and propagate information about the pruned channels forward, fusing it into the adjacent layer and preserving the structure of the network (albeit with smaller layer sizes).
%This allows us to fuse information about the pruned neurons/channels into adjacent layers and, therefore, preserve the structure of the network (albeit with smaller layer sizes).
Collectively, these properties allow our ID based compression approach to produce a structurally similar compressed network that well approximates (to any desired accuracy) the original network without any additional training---a feature we show theoretically for simple fully connected networks in Section~\ref{sec:pruneID}. We then extend the results to different layer types and more complex architectures in Section~\ref{sec:extendid}, and show how our theoretical results allow us to automatically determine per-layer sizes for a compressed model based on the trade-off between FLOPs and the estimated error induced by pruning.

% Our approximations are ``propagated'' through the activation functions 
% %by forcing the 
% because the ID is 
% %approximation to be 
% built using columns of the appropriate matrices directly---a process that allows for explicit sub-selection of neurons/channels.
% We retain the structure preservation properties of structured pruning; 
% our method allows us to 
% %update
% fuse the information of the pruned neurons/channels into the weights of the adjacent layers.
%our method naturally provides interpolation matrices that allow us to fuse the information of the pruned neurons/channels into the weights of the adjacent layers.
% For full details on the ID, see Section~\ref{sec:ID}.
%% previous version
%The core technical development in our pruning methodology is the novel use of a so-called interpolative decomposition (see Section~\ref{sec:ID}) to produce low-rank approximations of layers that directly incorporate the action of the activation function. 
%We retain and even improve upon the model preservation properties of matrix approximations by constructing approximations of the activation outputs, something that other matrix approximation methods cannot do.
%We ``propagate'' low-rank factorizations through activation functions by forcing the approximation to be built using columns of the appropriate matrices directly---a process that allows for explicit sub-selection of neurons/channels.
%We retain the structure preservation properties of structured pruning; the factorization naturally provides interpolation matrices that allow us to fuse an update into the weights of the adjacent layers.
% adjacent to the one currently being pruned. 
%This allows us to directly 

We ensure that our method truly compresses a network and preserves the decision boundaries, rather than effectively re-training a smaller network through extensive fine tuning through a new metric we develop in Section~\ref{sec:correlation}.  
Consider pruning a pretrained VGG16 model on CIFAR-10 with $93.6\%$ accuracy to $50\%$ of its FLOPs.
A variety of pruning methods achieve a reasonable accuracy of 93.2-93.6\%, including structured magnitude pruning, random channel pruning, and our ID-based method.  
However, methods which rely extensively on fine-tuning (magnitude and random pruning) do not preserve the decision boundaries of the original network.  
The predictions of these pruned models agree with the original model only~($93.2\%, 93.2\%$ respectively) of the time on the test set.
This level of agreement is barely better than a completely independently trained\footnote{
Consider a model $A$.
Another model $B$ of the same architectures is \emph{independently trained} if during training no information from $A$ is passed to $B$.
Essentially this means retraining the same architecture from scratch twice to obtain models $A$ and $B$.
}
(fullsize) VGG16 model~($93.0\%$).
%At the very least, we would expect a compressed model to have more agreement to the original model than an independently trained fullsize model. 
In contrast, our method produces a compressed model whose predictions are highly correlated with the original model, agreeing $97.3\%$ of the time.

% We highlight the importance of considering criteria beyond accuracy to evaluate compression methods. 
% See Section~\ref{sec:correlation} for further discussion.
% A compressed model which matches the accuracy of its original model does not necessarily preserve the decision boundary of the original model.
% % To briefly highlight the importance of considering criteria beyond accuracy for the evaluation of compression methods we illustrate that accuracy alone is not sufficient to distinguish between a compressed and retrained model. 
%  In principle, a variety of pruning method pipelines can result in reasonable accuracy: structured magnitude pruning, our ID-based method, and even random neuron/channel pruning achieve within standard deviation\footnote{We consistently observed a standard deviation of $\approx 0.3\%$ across 5 trials.} accuracies of $93.6\%, 93.3\%, 93.2\%$ respectively.
% To achieve this accuracy, structured magnitude pruning and random pruning require excessive fine tuning. Consequently, the predictions of these pruned models agree with the original model only~($93.2\%, 93.2\%$ respectively) of the time on the test set.
% This level of agreement is barely better than an independently trained
% \footnote{
% Consider a model $A$.
% Another model $B$ of the same architectures is \emph{independently trained} if during training no information from $A$ is passed to $B$.
% Essentially this means retraining the same architecture from scratch twice to obtain models $A$ and $B$.
% }
% (fullsize) VGG16 model~($93.0\%$).
% At the very least, we would expect a compressed model to have more agreement to the original model than an independently trained fullsize model. 
% In contrast, our method produces a compressed model whose predictions are highly correlated with the original model, agreeing $97.3\%$ of the time.
% However, to accomplish this, structured magnitude pruning and random pruning require excessive fine tuning. Consequently, the predictions of these pruned models correlate with the original model~($93.2\%, 93.2\%$ respectively) barely better than an independently trained VGG16 model~($93.0\%$).
% In contrast, in this setting our method produces a compressed model whose predictions are highly correlated with the original model ($97.3\%$).


To complement our algorithmic developments and theoretical contributions, in Section~\ref{sec:experiments} we demonstrate the efficacy of our method on Atom3D~\cite{atom3d}, CIFAR-10~\cite{datacifar10}, and ImageNet~\cite{deng2009imagenet}. The experiments are built to highlight how our ID-based compression method satisfies the desired criteria while competing methods do not. To accomplish this, we first introduce a metric of model similarity based on prediction correlation---this metric allows us to systematically determine how well compressed models are preserving the original model. To highlight the practical efficacy of our method (particularly its ability to automatically chose the size of the compressed network) we show our method successfully extends to Atom3D~\cite{atom3d}, simulating a real-world scenario with little prior compression work and baseline models with a different architecture (based on 3D convolutions). More generally, on all datasets we show our method has superior accuracy before fine tuning and is competitive with other state-of-the-art pruning methods after additional training is performed all while producing compressed models which better correlate with the original. Moreover, because our method only reduces layer sizes but otherwise preserves network structure, it can be easily composed with other compression 
methods. 
This allows us to substantially compress models for CIFAR-10 and ImageNet  without the use of any global fine tuning.




In summary, the key contributions of this paper\footnote{Our code is available at \href{https://github.com/jerry-chee/ModelPreserveCompressionNN}{https://github.com/jerry-chee/ModelPreserveCompressionNN}} are:
\begin{itemize}[leftmargin=*]
    \item[] \textbf{A model preserving compression method}
    We introduce an ID-based algorithm for compressing neural networks that preserves the model's decisions and maintains the network's layer structure without the need for fine tuning.
    %\item[] \textbf{A principled pruning method.} We introduce an algorithm for structured pruning based around the use of an interpolative decomposition that requires no labeled data and automatically determines per-layer sizes.   
    \item[] \textbf{Theoretical guarantees.} 
    We provide theoretical guarantees for the output of our pruned model.
    \item[] \textbf{Practical efficacy.}  We demonstrate the efficacy of our method across the ATOM3D, CIFAR-10 and ImageNet data sets and using a variety of models.
    Our method requires no labeled data, automatically determines per-layer sizes, and often requires little to no fine tuning.
    %We preserve both accuracy and model correlation to achieve compression, not retraining.
    %Because our method only reduces layer sizes, it can be easily composed with other
    %compression 
    %%low-rank 
    %methods. This allows us to preserve models (with respect to accuracy and predictions) for Cifar-10 and Imagenet at significant compression ratios without the use of any global fine tuning.
\end{itemize}



%Recent work suggests that modern deep neural networks are vastly overparameterized, but that such overparameterization can aid in training the network.  Therefore, there is significant interest and a plethora of methods to compress or prune neural networks. These methods come with a series of design choices based on the priorities of the user: to maintain the overall structure of the network or to add new layers, to eliminate single weights (creating sparse matrices) or entire neurons and channels (creating dense networks), to fine tune locally or globally, and many others.  If the goal is to reduce the number of parameters, pruning individual weights may be more effective, whereas eliminating entire neurons or channels is more practical for reducing flops.  
%If the goal is to produce a particular test accuracy with a smaller model regardless of fine tuning costs, pruning is an effective strategy, whereas matrix decomposition based methods appear more effective at reproducing the same per-example outputs between the original and pruned model, with less global fine tuning. Per-example output matching may be understudied in the literature, but may have important implications for fairness, and for preserving other properties such as robustness. This often comes at the cost of adding new layers into the network.   Our method attempts to combine the strengths of these various strategies.  In short, our goal is to produce a narrower network with the same pattern of of dense layers that produces the same per-example outputs on data as the original network, with either no or minimal fine tuning. Because our method produces the exact same architecture of network that is simply narrower, it can trivially be composed with other methods to improve their performance, by first applying our method to generate a pruned network, and then applying another method to the pruned model to further compress the network.  

%Our method is based on a matrix approximation called the interpolative decomposition (ID) (section \ref{sec:ID}), which approximates a matrix as a sub-selection of columns and an interpolation matrix. Typically matrix decomposition based methods use various decompositions to approximate the weight matrix, which involves inserting extra layers into the network.  ID allows us to approximate the output matrix of a layer on unlabeled data using a subset of the neurons or channels, and due to its structured nature, it can do so while taking into account the action of the neuron's activation functions (section \ref{sec:pruneID}). The ID provides an interpolation matrix that propagates updates to the weights in the next layer, compensating for the neurons that were eliminated. This simply makes the network narrower, without inserting extra layers. Since the approximation preserves the overall output of the layer to within a measurable accuracy, we can give theoretical generalization bounds for a single hidden layer network.  This error for a single hidden layer naturally becomes a heuristic to choose per-layer sizes when we extend to deep convolutional networks in section \ref{sec:deepid}.   
%%In section \ref{sec:context} we provide more context for our method within the larger field. 
%We demonstrate empirically that our method produces a narrower network that approximates the per-example outputs both with and without fine tuning in section \ref{sec:experiments}.  