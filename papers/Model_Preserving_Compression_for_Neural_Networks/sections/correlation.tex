An important benefit of our approach is that we are actually able to preserve the original model's predictions better than other methods.
Traditional pruning methods typically do a poor job at preserving the original predictions, due to their heavy reliance on fine tuning that effectively retrains the model.
Here we explain why we might care about preserving per-example predictions beyond top-line accuracy.
We argue that in many situations compression methods must well-approximate the original model, and that accuracy is a poor metric for this use case.

% What does it mean to ``compress'' a model?
% There are two criteria. 
% First, the compressed model must be smaller than the original, e.g. in memory of in inference runtime.
% %We have no qualm with how the reduction in size is measured.
% Second, the compressed model must well-approximate the original.
% We argue that accuracy is a poor metric for measuring if one model well-approximates another.

Consider a pretrained model $M$, a resulting ``compressed'' model $M_C$, and an evaluation set $(X,Y)$.
Accuracy measures the similarity between the true labels $Y$ and the predicted labels from the compressed model $M_C(X)$.
This metric does not directly compare the original model $M$ and compressed model $M_C$.
As we have seen in Section~\ref{sec:intro}, a compressed model can recover the accuracy of the original model, but still differ widely on the predictions.
Instead our model correlation measures the similarity between the original model's predictions $M(X)$ and the compressed model's predictions $M_C(X)$.
One can think of this metric as an ``accuracy to the original predictions'', instead of an accuracy to the true labels.

We propose model correlation as the percent of test example predictions two models agree on---details of this metric are discussed in Appendix~\ref{app:sec:correlation}.
Model correlation is a general metric to measure the similarity between the learned functions 
%decision boundaries
of two models.
% However we propose model correlation as a generic metric to measure preservation between a pre-trained and compressed model.
Our claim is that by better preserving a model's per-example decisions, we can better preserve special properties of the model.
In the following section we provide experimental evidence for this claim.
Models are now often trained to have properties that go beyond test accuracy---for example robustness to adversarial attacks, sub-class classification accuracy, fairness, etc., and this measure of model correlation is likely to correlate with many of these criteria.
% Fairness and robustness properties are two such specific examples.
% Better preserving specialize fairness or adversarial robustness properties is a corollary of better preserving the original model's learned function.
Note that we do not believe preserving per-example decisions ``boosts'' any of these properties, we are simply preserving properties of the baseline model.







% We propose a set of evaluation metrics for pruning methods that goes beyond simple post-fine tuning test accuracy. The first metric we define is correlation between two models, defined as the percent of test example predictions the two models agree on---details of this metric are discussed in Appendix~\ref{app:sec:correlation}.  We begin with a pretrained model, $M$, and compress it to create a model $M_c$.  We define model correlation as an aggregate measure of how well $M_c$ preserves the per-example decisions of $M$ on the test set.  This captures information about both examples which $M$  correctly and incorrectly classified.  Models are now often trained to have properties that go beyond test accuracy---for example robustness to adversarial attacks, sub-class classification accuracy, fairness, etc. On a practical level, no single number could perfectly capture the preservation on every possible metric. Our claim is that by better preserving per-example decisions, we may better preserve other special properties of the model. In order to demonstrate this, we show how our method can be used to prune a network while maintaining sub-class accuracy when neither the pruning nor fine-tuning methods have access to data from one of the classes in Appendix \ref{sec:sensitivity}.

