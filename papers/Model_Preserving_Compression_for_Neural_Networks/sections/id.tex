%\jerry{maybe emphasize that we're not a low-rank pruning method, even though we're using a low-rank matrix approxiation method}

Our pruning strategy relies on a structured low-rank approximation known as an interpolative decomposition (ID). 
% jerry 1.25.22: we talk more about preserving structure in intro now, don't think we need it here
%\jerrytwo{Note that a major benefit of the ID is that it preserves the structure of the network.
%Unlike other low-rank approximation methods, we do not need to store additional matrix factorizations, and we do not need to pay the additional reconstruction cost during inference.
%}
Classically, the Singular Value Decomposition (SVD) (see, e.g.,~\cite{GVL}) provides an optimal low-rank approximation. However, because we consider matrices that include the non-linear activation function the SVD cannot be directly used to either subselect neurons or generate new ones (since it is unclear how to propagate singular vectors ``backwards'' through the non-linearity). In contrast, an ID constructs a structured low-rank approximation of a matrix $A$ where the basis used for the approximation is constrained to be a subset of the columns of $A$. For a matrix $A\in\R^{n\times m}$ we let $A_{\mathcal{J},\I}$ denote a sub-selection of the matrix $A$ using sets $\mathcal{J}\subset [n]$ to denote the selected rows and $\I\subset [m]$ to denote the selected columns; $:$ denotes a selection of all rows or columns.
% The optimal basis is the leading left singular vectors of $A$, but generically these will not be columns of $A$.
%Shortly, we will see how the rank-revealing QR factorization is used to compute the interpolative decomposition.
%\begin{definition}[Interpolative decomposition]
%Given a matrix $A \in \R^{n \times m}$ and some $k \leq \ell$ (as before, $\ell = \min(n,m)$) a \emph{rank-k interpolative decomposition} is an index subset $\I \subset [m]$ with $|\I| = k$ and an interpolation matrix $T \in \R^{k \times m}$ with $T_{:,\I} = I_k$ such that
%\begin{equation}
%\label{eq:ID}
 %   A \approx A_{:,\I} T.
%\end{equation}
%\end{definition}

\begin{definition}[Interpolative Decomposition]
\label{def:ID}
Let $A \in \R^{n \times m} $and $\epsilon \geq 0$. 
An $\epsilon$-accurate \emph{interpolative decomposition} 
$A \approx A_{:,\I} T$
is a subset of columns of A, denoted with the index subset $\I \subset [m],$ and an associated interpolation matrix $T$ such that $\|A - A_{:,\I} T \|_2 \leq \epsilon\|A\|_2.$
\end{definition}






\begin{remarks}
\hspace{-0.8em}
When computing an ID, we would like to find the smallest possible $k\equiv\lvert\I\rvert$ such that the accuracy requirement is satisfied. Moreover, we would like $T$ to have entries of reasonable magnitude and approximation error not much larger than the best possible for a given $k$ (i.e., $\|A - A_{:,\I}T\|_2 = t \sigma_{k+1}(A)$ for some small $t \geq 1$). While necessarily sub-optimal, the advantage is that we explicitly use a subset of the columns of $A$ to build the approximation.
\end{remarks}

IDs are well studied~\cite{cheng2005compression,martinsson2011randomized}, widely used in the domain of rank-structured matrices~\cite{ho2012fast,ho2015hierarchical,ho2016hierarchical,martinsson2005fast,martinsson2019fast,minden2017recursive}, and are closely related to CUR decompositions~\cite{mahoney2009cur,voronin2017efficient} and subset selection problems~\cite{boutsidis2009improved,civril2009selecting,tropp2009column}. While these decompositions always exist, finding them optimally is a difficult task and in this work we appeal to what are known as (strong) rank-revealing QR factorizations~\cite{businger1965linear,chan1992some,chandrasekaran1994rank,gu1996efficient,hong1992rank}.


%\subsection{Rank-Revealing QR factorization}
% \paragraph{Rank-revealing QR factorizations}
\begin{definition}[Rank-revealing QR factorization]
Let $A \in \R^{n \times m}$, $\ell = \min(n,m)$, and take any $k \leq \ell$.
A \emph{rank-revealing QR factorization} of $A$ computes a permutation matrix $\Pi \in \R^{m \times m}$, an upper-trapezoidal matrix $R \in \R^{\ell \times m}$ (i.e. $R_{i,j}=0 \text{ if } i > j$), and a matrix $Q \in \R^{n \times \ell}$ with orthonormal columns (i.e., $Q^\top Q = I$) such that $A \Pi = Q R$ and $Q$ and $R$ satisfy certain properties.
Splitting $\Pi, Q$, and $R$ into $\Pi_1 \in \R^{m \times k}$, $\Pi_2 \in \R^{m \times (m-k)}$, $Q_1 \in \R^{n \times k}$, $Q_2 \in \R^{n \times (\ell-k)}$, $R_{11} \in \R^{k \times k}$, 
$R_{12} \in \R^{k \times (m-k)}$, and $R_{22} \in \R^{(\ell-k)\times(m-k)}$ we can write
%$R_{12} \in \R^{k \times (\ell-m)}$, and $R_{22} \in \R^{(\ell-k)\times(\ell-n)}$ we can write
\begin{equation}
\label{eq:rankQR}
    A
    \begin{bmatrix}
    \Pi_1 & \Pi_2
    \end{bmatrix}
    =
    \begin{bmatrix}
    Q_1 & Q_2
    \end{bmatrix}
    \begin{bmatrix}
    R_{11} & R_{12} \\
    & R_{22}
    \end{bmatrix}.
\end{equation}
\end{definition}

\begin{remarks}
What makes~\eqref{eq:rankQR} a rank-revealing QR factorization is that the permutation $\Pi$ is computed to ensure that $R_{11}$ is as well-conditioned as possible and $R_{22}$ is as small as possible. While more formal statements of these conditions exist, we omit them here as they do not factor into our work.
\end{remarks}

Critically, any rank-revealing QR factorization yields a natural rank-$k$ approximation of $A$ with error
\begin{equation*}
    \| A - Q_1 
    \begin{bmatrix}
    R_{11} & R_{22}
    \end{bmatrix}
    \Pi^\top \|_2
    =
    \| R_{22} \|_2.
\end{equation*}
While finding the optimal rank-revealing QR factorization (\emph{i.e.,} minimizing the error for a given $k$) is closely related to a provably hard problem~\cite{civril2009selecting}, we find the original algorithm of Businger and Golub~\cite{businger1965linear} works well in practice. This routine is available in LAPACK~\cite{lapack,blas3QRCP}, can be easily incorporated into existing code, and has computational complexity $\mathcal{O}(nmk)$ when run for $k$ steps.  
%While optimally finding a rank-revealing QR is closely related to the provably hard problem of finding maximum volume subsets~\cite{civril2009selecting}, many good algorithms exist~\cite{businger1965linear,blas3QRCP,chandrasekaran1994rank,gu1996efficient,golub1976rank} that are effective in practice.
%More specifically, these algorithms seek to ensure that (where $\sigma$ is used to denote the appropriate singular values)
%\begin{equation*}
 %   \sigma_{\min}(R_{11})
%    \geq 
%    \frac{\sigma_k(A)}{f_1(n,k)}
%    \quad\text{and}\quad
%    \sigma_{\max}(R_{22}) 
%    \leq
%    f_2(n,k) \sigma_{k+1}(A),
%\end{equation*}
%for functions $f_1$ and $f_2$ that grow mildly in $n$ and $k$.
%(For many of the specific algorithms referenced explicit expressions of $f_1$ and $f_2$ are known.)
%For our purposes, the original algorithm of Businger and Golub~\cite{businger1965linear} suffices and has runtime $\mathcal{O}(nm\ell)$; readily available implementations exist in LAPACK~\cite{lapack,blas3QRCP}, distributed memory implementations are available (e.g., in ScaLAPAK~\cite{Scalapack}), and randomization can be used to provide efficiency gains in practice~~\cite{liberty2007randomized,duersch2017randomized,martinsson2011randomized,martinsson2017householder,randomreview}.



\paragraph{Computing interpolative decompositions}
%\subsection{Computing interpolative decompositions}
Given a rank-revealing QR factorization, we can immediately construct an ID (a formal algorithmic statement is given in the appendix).
Let $\I\subset [m]$ be such that $A_{:,\I} = A \Pi_1$ and define the interpolation matrix
\begin{equation*}
    T = 
    \begin{bmatrix}
    I_k & R_{11}^{-1} R_{12}
    \end{bmatrix}
    \Pi^\top.
\end{equation*}
With the choice $A_{:,\I} = Q_1 R_{11}$ it follows that the error of the ID as defined by $\I$ and $T$ is 
$%$\begin{align*}
    \|A - A_{:,\I}T\|_2
    %&=
    %\|Q\begin{bmatrix}
    %0 & R_{22}
    %\end{bmatrix}
    %\Pi^\top \|_2 \\
    = \|R_{22}\|_2 
$. %$\end{align*}  
Picking $k$ such that $\|R_{22}\|_2\leq \epsilon \|A\|_2$ yields the desired relative error. Notably, since $\kappa(A_{:,\I}) = \kappa(R_{11})$ and $T = 
\begin{bmatrix}
I_k & R_{11}^{-1} R_{12}
\end{bmatrix}
\Pi^\top$ 
the desired criteria for an ID map back to those of a rank-revealing QR factorization---if $R_{11}$ is well conditioned then the  basis we use for approximation is as well and entries of $T$ are not too large.
If $\sigma_{\max}(R_{22})$ is not much larger than $\sigma_{k+1}(A)$ we get near optimal approximation accuracy.







\paragraph{Accuracy of the matrix approximation}
%\subsection{Accuracy of the matrix approximation}
%\label{sec:accuracyID}
A key feature of using a column-pivoted QR factorization to compute an ID is that it allows us to dynamically determine the approximation rank $k$ as a function of $\epsilon$. 
This can be accomplished by monitoring $\|R_{22}\|_2$ at each step of the column-pivoted QR algorithm. 
However, repeatedly computing $\|R_{22}\|_2$ is expensive and often unnecessary in practice. 
When using the algorithm by Businger and Golub~\cite{businger1965linear} the magnitude of the diagonal entries of $R$ are non-increasing and it is common to use $\lvert r_{k+1,k+1} / r_{1,1}\rvert$ as a proxy for 
%$\|R_{22}\|_2/\|R\|_2$.
$\|R_{22}\|_2/\|A\|_2$.
While formal bounds indicate the approximation may be loose in the worst case, it is effective in practice (see appendix Figure~\ref{fig:chosingk}) and once a candidate $k$ has been identified $\|R_{22}\|_2$ can be computed if desired to certify the result---if the accuracy is unacceptable $k$ can be increased until it is. 
In some settings it may be preferable to fix $k$ and simply accept whatever accuracy is achieved.


% The matrix R gives us useful information about the potential accuracy vs. complexity of the decomposition. 
% In the section above, we showed that the error from our decomposition is $||R_{22}||$. 
% However, computing the value of $||R_{22}||$ for each k can be expensive. 
% In this situation, we can use properties of the column-pivoted QR factorization to speed our algorithm. 
% The largest magnitude entry in $R_{22}$ is  in the upper left-hand corner of the matrix.  
% While this does not give us a useful hard bound on $||R_{22}||/||R||$, in practice we can use this as a proxy for the relative size of $||R_{22}||/||R||$.  
% \megan{cite places this is done in practice/ well established?}.  
% Let $r_{11} = R_{1,1}$ denote the matrix $R$ indexed at $(0,0)$, the top left, and $r_{kk} = R_{k,k}$ denote the matrix $R$ indexed at the $k$-th diagonal.
% In figure {\ref{fig:chosingk}}, we see that the value of $r_{kk}/r_{11}$ tracks well with the optimal bound as determined by the singular value decay, as well as with the actual error $||R_{22}||$. 
% This allows us to control the increase in error that we create for the network locally.  

%In figure (\megan{number}), we see that the degradation in accuracy matches what we would expect given the ratio $r_{kk}/r_{00}$.  




%\jerry{Maybe some simple simulations comparing RRQR to SVD?
%Similar to Fig2 in grant.}


%Given a matrix $A$ an interpolative decomposition may be computed using any rank-revealing QR factorization, though most commonly the column-pivoted QR factorization of Golub and Businger is used \todo{cite}.
%The procedure is given in Algorithm~\ref{alg:genericID}, which we elaborate on in further detail.
%For simplicity let $l = \min(m,n)$ and assume $k < l$.
%
%Concretely, we first compute the column-pivoted QR factorization
%\begin{equation*}
%    A \Pi = Q R
%\end{equation*}
%where $\Pi \in \R^{n \times n}$ is a permutation, $Q \in \R^{m \times l}$ has orthonormal columns and $R \in \R^{l \times n}$ is upper triangular.
%We now partition $\Pi$ and $R$ as 
%\begin{equation*}
%    \Pi = 
%    \begin{bmatrix}
%    \Pi_1 & \Pi_2
%    \end{bmatrix}
%    \quad\text{and}\quad
%    R =
%    \begin{bmatrix}
%    R_{11} & R_{12} \\
%    0    & R_{22}
%    \end{bmatrix},
%\end{equation*}
%where $\Pi_1 \in \R^{n \times k}, R_{11} \in \R^{k \times k}, R_{12} \in \R^{k \times n - k}$, and the remaining dimensions are as required.
%Let $\I$ denote the columns selected by the subselection matrix $\Pi$ such that
%\begin{equation*}
%    A_{:,\I} = A \Pi_1.
%\end{equation*}
%Letting the interpolation matrix
%\begin{equation*}
%    T = 
%    \begin{bmatrix}
%    I_k & R_{11}^{-1} R_{12}
%    \end{bmatrix}
%    \Pi^\top
%\end{equation*}
%we have the interpolative decomposition.
%Note that the error is 
%\begin{align*}
%    \|A - A_{:,\I}T\|_2
%    &=
%    \|Q\begin{bmatrix}
%    0 & R_{22}
%    \end{bmatrix}
%    \Pi^\top \|_2 \\
%    &= \|R_{22}\|_2.
%\end{align*}

