\subsection{Convolution layers}
% \paragraph{Convolution layers}
%\label{sec:convid}


To prune convolution layers with the ID at the channel level we reshape the %order-4 
output tensor into a matrix where each column represents a single output channel.
After this transformation, the key idea is the same as in Section~\ref{sec:pruneID}.
%The weights of convolution layers are an order-4 tensor ($\operatorname{input channels} \times \operatorname{output channels} \times \operatorname{filter width} \times \operatorname{filter height}$).
%and their action on the activations is via a convolution.
%We first reshape the activation tensor matrix of shape $( \operatorname{filter width} \cdot \operatorname{filter height} \cdot \operatorname{input channels} ) \times \operatorname{output channels}$, and then use an interpolative decomposition to prune the output channels.
Consider a simple two layer convolution neural network defined as 
$%\begin{equation*}
    h_{\Conv}(x; W, U)
    =
    \Conv(U, g( \Conv(W, x))))
$, %\end{equation*}
%$h_{Conv}: \R^{q \times r \times s} \to \R^{q' \times r' \times s'}$
with the convolution-layer operator $\Conv$, weight tensors $W$ and $U$,
%such that the output channels of $W$ and the input channels of $U$ match,
and activation function $g$.
%Again we omit bias terms, though they may be readily incorporated.
%We omit specifying the kernel dimension, size, stride, padding, and dilation because they have no effect on the interpolative decomposition at the output channel level.
The kernel dimension, size, stride, padding, and dilation do not change the form of the ID at the output channel level.
Let $Z = g(\Conv(W, X))$ be the activation output of the first layer with unlabeled pruning data $X$, and define $\Reshape$ as the operator which reshapes a tensor into a matrix with the output channels as columns, i.e., $\Reshape(Z)\in\R^{n_i\times m_c}$ where $m_c$ is the number of channels and $n_i$ is the product of all other dimensions (e.g. in the case of a 2d convolution, $n_i$ would be $\text{width} \times \text{height} \times \text{number of examples}$). 

We now compute\footnote{When $n_i$ is large we can appeal to randomized ID algorithms~\cite{martinsson2011randomized}, or the TSQR~\cite{ballard2014tsqr}.
%if needed.
} a rank-$k$ ID
$\Reshape(Z) \approx \Reshape(Z)_{:,\I} T.$
%the reshaped activation output.
%Because the activation function $g$ and reshaping operator $\Reshape$ commute with the sub-selection operator
%\begin{equation*}
%    Z 
%    \approx
%\end{equation*}
%Define $\Reshape(Z)$ as the operator to reshape the order-4 tensor into an order-2 tensor with columns $\operatorname{output channels}$.
%The steps to prune this two layer convolution network are similar to the fully connected case.
%Compute a rank-$k(\epsilon)$ interpolative decomposition on the reshaped activation output
%$
%    \Reshape(Z) 
%    \approx
%    \Reshape(Z)_{:,\I} T
%$.
The activation function $g$ and reshape operator both commute with the sub-selection operator, so
\begin{align*}
    \Reshape(g(\Conv(W, X)))
    &\approx
    \Reshape(g(\Conv(W, X)))_{:,\I} T \\
    &=
    \Reshape(g(\Conv(W_{\I,\ldots}, X))) T,
\end{align*}
where $W_{\I,\ldots}$ denotes an indexing sub-selection of $W$ along its output-channel dimension. 
%$\I$.
Next, we need to propagate this $T$ into the next layer, which we can do with a ``matrix multiply'' by the next-layer's weights along its input channel dimension: we call this operation $\Matmul$.\footnote{If $T \in \R^{m \times n}$ and $U$ is a weight-tensor with $n$ input channels, then to compute $\Matmul(T,U)$ we: (1) reshape $U$ to be an $n \times p$ matrix for some $p$, (2) multiply the reshaped matrix by $T$, producing a $m \times p$ matrix, and (3) reshape the result back to a tensor with $m$ input channels and all other dimensions the same as $U$.}
With this, a little algebraic manipulation of our approximate equality above gives
\begin{equation}
    \Conv(U, g( \Conv(W, X)))) \\
    \approx 
    \Conv(\Matmul(T,U), g( \Conv(W_{\I,\ldots}, X)))),
\end{equation}
and so if we set $\widehat U = \Matmul(T,U)$ and $\widehat W = W_{\I,\ldots}$,
% Thus by selecting channels $\widehat{W} = W_{\I},$ where $W_{\I}$ is the reshaping of $\Reshape(Z)_{:,\I}$ back into an order 4 tensor, and propagating $T$ into the next layer as $\widehat{U} = \Matmul(T,U)$ 
we can preserve the action of the two layer network with fewer channels as
$%\begin{equation*}
%\label{eq:convID}
    h_{\Conv}(x; W, U)
    \approx
    h_{\Conv}(x; \widehat{W}, \widehat{U}).
$ %\end{equation*}
This gives us a recipe for pruning convolution layers analogous to (\ref{eqn:PruneApprox}).
This recipe can be directly applied to a composition of a convolution layer followed by a pooling layer~\cite{goodfellow2016deep} (or any other layer that acts independently by channel) by treating the conv layer/ pooling layer pair as a single convolution layer with a ``fancy'' activation function $g$: we just run the ID on the output post-pooling, and use that to sub-select the convolution layer's weights.
Flatten layers, for connecting to FC layers, are equally straightforward.

%It is straightforward to extend this to arbitrary compositions of linear and convolutional layers: for more details, see Algorithm~\ref{alg:deepID}.

% Here, $\Matmul(\ )$ applies a batch matrix multiplication between $T$ and $\Reshape(U)$ followed by reshaping to the original order-4 tensor dimensions.\footnote{
% %So far we have discussed how to prune two fully connected layers (FC-FC), and two convolution layers (Conv-Conv).
% For a convolution layer followed by a fully connected layer, % (Conv-FC),
% the details of the ID and channel selection remain the same.
% To propagate $T$, 
% %define the propagation function
% let $\Matmul(T, W_2) = (T \otimes I_a) W_2$ with $a = \text{input channels} * \text{filter width} * \text{filter height}$ and $\otimes$ the Kronecker product.
% }
%\begin{equation}
%    Z 
%    \approx
%    T^\top g(\Conv(W_{1(:,\I,:,:)}, X))
%\end{equation}

%\jerry{How much detail for Conv algebra: full details a bit messy.}


%\begin{algorithm}[t]
%\SetAlgoLined
%\DontPrintSemicolon
%\KwIn{
%Conv layer $f_{Conv}(x;W_1)$,
%next layer $f_{2}(x;W_2)$,
%%intermediate layer $g(x; w_3)$ (batch norm, pooling),
%pruning data $X_{p} \in \R^{d \times n_{p}}$,
%rank-$k(\epsilon)$
%}
%\KwOut{
%pruned layers
%$\widehat{f}_{Conv}(x;\widehat{W}_1)$,
%$\widehat{f}_{2}(x;\widehat{W}_2)$,
%%$\widehat{g}(x;\widehat{w}_3)$
%}
%
%%\tcc{apply batch norm or pool layer if it follows $f_{Conv1}$}
%$Z \gets f_{Conv1}(X_{p},W_1)$\;
%$Z \gets Z.\operatorname{reshape}(\operatorname{output channels}, \operatorname{filter width} \cdot \operatorname{filter height} \cdot \operatorname{input channels})$\;
%$Z_{:,\I}, T \gets \ID(Z, k(\epsilon))$\;
%$\widehat{W}_1 \gets (W_1)_{(:,\I,:,:)}$
%\tcp{select output channels via ID}
%$\widehat{W}_2 \gets \operatorname{matmul}(T, W_2)$
%\tcp{propagate interpolation matrix to next layer}
%%\tcp{depends on if next layer is FC or Conv}
%%\todo{presentation} \;
%%$\widehat{W}_2 \gets W_2.\operatorname{reshape}(\operatorname{output channels}, \operatorname{filter width}, \operatorname{filter height},  \operatorname{input channels})$\;
%%$\widehat{W}_2 \gets \operatorname{batch broadcast matmul}(T, \widehat{W}_2)$\;
%%$\widehat{W}_2 \gets W_2.\operatorname{reshape}(\operatorname{input channels}, \operatorname{output channels}, \operatorname{filter width}, \operatorname{filter height})$\;
%\caption{ID Pruning a Conv Layer\jerry{might get rid of}}
%\label{alg:convID}
%\end{algorithm}




\subsection{Deep networks}
% \paragraph{Deep networks}
\label{sec:deepid}

\begin{algorithm}[t]{\small
%\SetAlgoLined
%\DontPrintSemicolon
\caption{Pruning a multilayer network with interpolative decompositions}
\begin{algorithmic}[1]
\label{alg:deepID}
%\INPUT
\REQUIRE
Neural net $h(x; W^{(1)},\ldots,W^{(L)})$,
layers to not prune $S \subset [L]$,
pruning set $X$,
%prune mode $\in \{\operatorname{acc},\operatorname{frac}\}$,
pruning fraction $\alpha$
%Ordered list 
%index list $L$ 
%of layers to be pruned $\{(l_{i}, l_{i+1})\}_{i \in L}$ and activation outputs $\{Z_i\}_{i \in L}$, \\
%prune mode $\in \{\operatorname{frac}, \operatorname{acc}\}$, accuracy $\epsilon$
%\OUTPUT
\ENSURE
Pruned network $h(x; \widehat{W}^{(1)},\ldots,\widehat{W}^{(L)})$
%Pruned layers $\{(\hat{l}_{i}, \hat{l}_{i+1})\}_{i \in L}$
\vspace{0.5em}
%Absorb batch norm layers into their preceding layer\;
%$\operatorname{acc}$: set $k$ to achieve desired accuracy $\epsilon$ or $\operatorname{frac}$: as $\epsilon$ fraction of neurons \;
\STATE $T^{(0)} \gets I$ \;
\FOR{$l \in \{1 \dots L\}$}
\STATE $Z \gets h_{1:l}(X; W^{(1)}, \dots, W^{(l)})$
\COMMENT{layer l activations}
%\COMMENT{compute activations of layer l}
\IF{layer $l$ is a FC layer}
\STATE $(\I, T^{(l)}) \gets \operatorname{ID}(Z^T; \alpha) \textbf{ if } l \notin S \textbf{ else } (:, I)$ \;
%prune $\alpha\%$ of neurons with ID of $Z^\top$: $\I, T$\;
%compute rank-$k$ ID of $Z^\top$: $\I, T$\;
\STATE $\widehat{W}^{(l)} \gets T^{(l-1)} W^{(l)}_{:,\I}$
\COMMENT{sub-select neurons, multiply T of prev layer's ID}
%\tcp{select neurons in current layer}
%$\widehat{W}^{(l+1)} \gets T \widehat{W}^{(l+1)}$
%\;
%\tcp{propagate T to next layer}
\ELSIF{layer l is a Conv layer (or Conv+Pool)}
\STATE $(\I, T^{(l)}) \gets \operatorname{ID}(\Reshape(Z); \alpha) \textbf{ if } l \notin S \textbf{ else } (:, I)$ \;
% prune $\alpha\%$ of channels with ID of $\Reshape(Z)$: $\I, T$\;
%compute rank-$k$ ID of $\Reshape(Z)$: $\I, T$\;
\STATE $\widehat{W}^{(l)} \gets \Matmul(T^{(l-1)}, W^{(l)}_{\I,\ldots})$
%\;
\COMMENT{select channels; multiply T} %in current layer
% $\widehat{W}^{(l+1)} \gets \Matmul(T, \widehat{W}^{(l+1)})$
%\tcp{propagate T to next layer}
%\tcp{depends if next layer is FC or Conv}
\ELSIF{layer l is a Flatten layer}
\STATE $T^{(l)} \gets T^{(l-1)} \otimes I \,\,$ 
\COMMENT{expand T to have the expected size}
\ENDIF
\ENDFOR
\end{algorithmic}
%%Specify direction\;
%%How to compute Z\;
%\caption{ID pruning a multi-layer neural network}
}\end{algorithm}


%% OLD Version
%%\begin{algorithm}[t]
%%\caption{ID pruning a multi-layer neural network}
%%%\caption{Pruning a multilayer network with interpolative decompositions}
%%\label{alg:deepID_acc}
%%\begin{algorithmic}
%%\INPUT
%%Neural network $h(x; W^{(1)},W^{(2)},\dots,W^{(L)})$,
%%layers to not prune $S \subset [L]$,\\
%%%layers to skip during pruning $S \subset [L]$,\\
%%pruning set $X$,
%%%prune mode $\in \{\operatorname{acc},\operatorname{frac}\}$,
%%pruning proportion $\alpha$
%%%Ordered list 
%%%index list $L$ 
%%%of layers to be pruned $\{(l_{i}, l_{i+1})\}_{i \in L}$ and activation outputs $\{Z_i\}_{i \in L}$, \\
%%%prune mode $\in \{\operatorname{frac}, \operatorname{acc}\}$, accuracy $\epsilon$
%%\OUTPUT
%%Pruned network $h(x; \widehat{W}^{(1)},\widehat{W}^{(2)},\dots,\widehat{W}^{(L)})$
%%%Pruned layers $\{(\hat{l}_{i}, \hat{l}_{i+1})\}_{i \in L}$
%%\vspace{0.5em}
%%\hrule
%%\vspace{0.5em}
%%%Absorb batch norm layers into their preceding layer\;
%%%$\operatorname{acc}$: set $k$ to achieve desired accuracy $\epsilon$ or $\operatorname{frac}$: as $\epsilon$ fraction of neurons \;
%%\FOR{$l \in \{1,\dots L\}$}
%%\STATE $\widehat{W}^{(l)} \gets W^{(l)}$
%%\FOR{$l \in \{1 \dots L\} \setminus S$}
%%\IF{layer l is a FC layer}
%%\STATE $Z \gets h_{1:l}(X; W^{(1)}, \dots, W^{(l)})$
%%\COMMENT{output of layer l}
%%\STATE prune $\alpha\%$ of neurons with ID of $Z^\top$: $\I, T$\;
%%%compute rank-$k$ ID of $Z^\top$: $\I, T$\;
%%\STATE $\widehat{W}^{(l)} \gets \widehat{W}^{(l)}_{:,\I}$
%%\;
%%%\tcp{select neurons in current layer}
%%\STATE $\widehat{W}^{(l+1)} \gets T \widehat{W}^{(l+1)}$
%%%\tcp{propagate T to next layer}
%%\ELSIF{layer l is a Conv layer}
%%\STATE $Z \gets h_{1:l}(X; W^{(1)}, \dots, W^{(l)})$\;
%%\STATE prune $\alpha\%$ of channels with ID of $\Reshape(Z)$: $\I, T$\;
%%%compute rank-$k$ ID of $\Reshape(Z)$: $\I, T$\;
%%$\widehat{W}^{(l)} \gets \widehat{W}^{(l)}_{(:,\I,:,:)}$
%%%\;
%%\COMMENT{select channels} %in current layer
%%\STATE $\widehat{W}^{(l+1)} \gets \Matmul(T, \widehat{W}^{(l+1)})$
%%\;
%%\ENDIF
%%%\tcp{propagate T to next layer}
%%%\tcp{depends if next layer is FC or Conv}
%%\ENDFOR
%%\ENDFOR
%%\end{algorithmic}
%%%\SetAlgoLined
%%%\DontPrintSemicolon
%%%%Specify direction\;
%%%%How to compute Z\;
%%\end{algorithm}



The ID pruning primitives for fully connected and convolution layers can now be composed together to prune deep networks.
Algorithm~\ref{alg:deepID} specifies how we chain together the fully connected and convolution primitives to prune feedforward networks, for simplicity we assume for the moment we know the desired layer sizes.
%of arbitrary composition.
%In contrast most pruning methods act on a single layer at a time, and
%only modify the weights in the current layer, and 
%do not make any corrective changes to the next layers.
A multi-layer neural network is pruned from the beginning to the end, where the ID is used to approximate the outputs of the original network.
The ID pruning primitives sub-select neurons (or channels) in the current layer and propagate the corrective interpolation matrix to the next layer.
There are many ways one could prune a multi-layer network with these ID pruning primitives;
we selected the approach in Algorithm~\ref{alg:deepID} through empirical observations (though we do not assert that it is optimal).
% Discussion on the experiments which guided our design choices can be found in the supplement.
%The activation outputs are computed using a held out pruning set with either the original network, or the pruned network.
%For deep networks we found that it was crucial to use the activation outputs from the original network to mitigate harmful cascading approximation errors across the layers.
Note that as a pre-processing step before running Algorithm~\ref{alg:deepID}, batch normalization layers~\cite{batchnorm} should be absorbed into their preceding fully-connected or convolution layers, and dropout~\cite{srivastava2014dropout} layers should be removed.
% Pooling layers~\cite{goodfellow2016deep} operate at the neuron or channel level, and can be incorporated into computing $Z$ without affecting the interpolative decomposition.
% We also consider residual networks~\cite{resnet} which consist of blocks with two or more convolution layers and a skip connection: for such residual blocks we prune all but the last layer in the block.


\paragraph{Iterative Pruning}
While Algorithm~\ref{alg:deepID} is illustrative, in practice we would often like to be able to either specify a desired accuracy or choose layer sizes optimally for a desired compression ratio. Our approach allows us to accomplish this by iteratively selecting layers to compress. We introduce a score function for layers that is the ratio of the estimated relative error $\lvert r_{k+1,k+1} / r_{1,1}\rvert$ introduced by compressing a layer to the number of flops $f_l$ that would be cut if we pruned a layer $l$ to size $k$. We call this score $s_l(k)= \lvert r_{k+1,k+1} / r_{1,1}\rvert /f_l$ and it is heuristic for the compressability of each layer---lower scores imply a layer is easier to compress. However, different layers of the network are connected, and compressing a layer early in the network can effect how well later layers can be compressed.  
Therefore, we prune the network iteratively, measuring the score for each layer at a given pruning percentage (or step size), choosing the layer with the lowest score, pruning it, and then re-calculating the scores for the later layers. 
We repeat the process until the network reaches a desired compression, or until the network performance degrades unacceptably. 
% This method is particularly useful for sequential networks where the layer sizes were not chosen efficiently.
We refer to this method as Iterative ID, and refer to cutting a constant fraction of all neurons in each layer as Constant Fraction ID.  
For full details see Appendix~\ref{app:sec:iterativeID} and Algorithm~\ref{alg:deepIDIter}.
%\todo[inline]{Put IDIter algorithm psuedo code somewhere}

%\begin{algorithm}[t]{\small
%%\SetAlgoLined
%%\DontPrintSemicolon
%\KwIn{
%Neural net $h(x; W^{(1)},\ldots,W^{(L)})$,
%layers to not prune $S \subset [L]$,
%pruning set $X$,
%%prune mode $\in \{\operatorname{acc},\operatorname{frac}\}$,
%pruning fraction $\alpha$
%%Ordered list 
%%index list $L$ 
%%of layers to be pruned $\{(l_{i}, l_{i+1})\}_{i \in L}$ and activation outputs $\{Z_i\}_{i \in L}$, \\
%%prune mode $\in \{\operatorname{frac}, \operatorname{acc}\}$, accuracy $\epsilon$
%}
%\KwOut{
%Pruned network $h(x; \widehat{W}^{(1)},\ldots,\widehat{W}^{(L)})$
%%Pruned layers $\{(\hat{l}_{i}, \hat{l}_{i+1})\}_{i \in L}$
%}
%\vspace{0.5em}
%%Absorb batch norm layers into their preceding layer\;
%%$\operatorname{acc}$: set $k$ to achieve desired accuracy $\epsilon$ or $\operatorname{frac}$: as $\epsilon$ fraction of neurons \;
%\textbf{set} $T^{(0)} \gets I$ \;
%\For{$l \in \{1 \dots L\}$}{
%$Z \gets h_{1:l}(X; W^{(1)}, \dots, W^{(l)})$
%\hfill\tcp{compute activations of layer l}
%\uIf{layer $l$ is a FC layer}{
%$(\I, T^{(l)}) \gets \operatorname{InterpolativeDecomposition}(Z^T; \alpha) \textbf{ if } l \notin S \textbf{ else } (:, I)$ \;
%%prune $\alpha\%$ of neurons with ID of $Z^\top$: $\I, T$\;
%%compute rank-$k$ ID of $Z^\top$: $\I, T$\;
%$\widehat{W}^{(l)} \gets T^{(l-1)} W^{(l)}_{:,\I}$
%\hfill\tcp{sub-select neurons, multiply T of prev layer's ID}
%%\tcp{select neurons in current layer}
%%$\widehat{W}^{(l+1)} \gets T \widehat{W}^{(l+1)}$
%%\;
%%\tcp{propagate T to next layer}
%}
%\uElseIf{layer l is a Conv layer (or Conv+Pool)}{
%$(\I, T^{(l)}) \gets \operatorname{InterpolativeDecomposition}(\Reshape(Z); \alpha) \textbf{ if } l \notin S \textbf{ else } (:, I)$ \;
%% prune $\alpha\%$ of channels with ID of $\Reshape(Z)$: $\I, T$\;
%%compute rank-$k$ ID of $\Reshape(Z)$: $\I, T$\;
%$\widehat{W}^{(l)} \gets \Matmul(T^{(l-1)}, W^{(l)}_{\I,\ldots})$
%%\;
%\hfill\tcp{select channels; multiply T} %in current layer
%% $\widehat{W}^{(l+1)} \gets \Matmul(T, \widehat{W}^{(l+1)})$
%%\tcp{propagate T to next layer}
%%\tcp{depends if next layer is FC or Conv}
%}
%\uElseIf{layer l is a Flatten layer}{
%    $T^{(l)} \gets T^{(l-1)} \otimes I \,\,$ \hfill\tcp{expand T to have the expected size}
%}
%}
%%Specify direction\;
%%How to compute Z\;
%\caption{ID pruning a multi-layer neural network}
%%\caption{Pruning a multilayer network with interpolative decompositions}
%\label{alg:deepID}
%}\end{algorithm}
%
%
%\begin{algorithm}[t]
%\SetAlgoLined
%\DontPrintSemicolon
%\KwIn{
%Neural network $h(x; W^{(1)},W^{(2)},\dots,W^{(L)})$,
%layers to skip during pruning $S \subset [L]$,\\
%pruning set $X$,
%%prune mode $\in \{\operatorname{acc},\operatorname{frac}\}$,
%pruning proportion $\alpha$
%%Ordered list 
%%index list $L$ 
%%of layers to be pruned $\{(l_{i}, l_{i+1})\}_{i \in L}$ and activation outputs $\{Z_i\}_{i \in L}$, \\
%%prune mode $\in \{\operatorname{frac}, \operatorname{acc}\}$, accuracy $\epsilon$
%}
%\KwOut{
%Pruned network $h(x; \widehat{W}^{(1)},\widehat{W}^{(2)},\dots,\widehat{W}^{(L)})$
%%Pruned layers $\{(\hat{l}_{i}, \hat{l}_{i+1})\}_{i \in L}$
%}
%\vspace{0.5em}
%\hrule
%\vspace{0.5em}
%%Absorb batch norm layers into their preceding layer\;
%%$\operatorname{acc}$: set $k$ to achieve desired accuracy $\epsilon$ or $\operatorname{frac}$: as $\epsilon$ fraction of neurons \;
%\lFor{$l \in \{1,\dots L\}$}{
%$\widehat{W}^{(l)} \gets W^{(l)}$
%}
%\For{$l \in \{1 \dots L\} \setminus S$}{
%\uIf{layer l is a FC layer}{
%$Z \gets h_{1:l}(X; W^{(1)}, \dots, W^{(l)})$
%\tcp{output of layer l}
%prune $\alpha\%$ of neurons with ID of $Z^\top$: $\I, T$\;
%%compute rank-$k$ ID of $Z^\top$: $\I, T$\;
%$\widehat{W}^{(l)} \gets \widehat{W}^{(l)}_{:,\I}$
%\;
%%\tcp{select neurons in current layer}
%$\widehat{W}^{(l+1)} \gets T \widehat{W}^{(l+1)}$
%\;
%%\tcp{propagate T to next layer}
%}
%\uElseIf{layer l is a Conv layer}{
%$Z \gets h_{1:l}(X; W^{(1)}, \dots, W^{(l)})$\;
%prune $\alpha\%$ of channels with ID of $\Reshape(Z)$: $\I, T$\;
%%compute rank-$k$ ID of $\Reshape(Z)$: $\I, T$\;
%$\widehat{W}^{(l)} \gets \widehat{W}^{(l)}_{(:,\I,:,:)}$
%%\;
%\tcp{select channels} %in current layer
%$\widehat{W}^{(l+1)} \gets \Matmul(T, \widehat{W}^{(l+1)})$
%\;
%%\tcp{propagate T to next layer}
%%\tcp{depends if next layer is FC or Conv}
%}
%}
%%Specify direction\;
%%How to compute Z\;
%\caption{ID pruning a multi-layer neural network}
%%\caption{Pruning a multilayer network with interpolative decompositions}
%\label{alg:deepID}
%\end{algorithm}