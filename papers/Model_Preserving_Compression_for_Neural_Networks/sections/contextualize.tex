% \begin{table}[h!]
% \centering
% \begin{tabular}{|c c c c| c c c|} 
%  \hline
%  & & & & \multicolumn{3}{c|}{Compression type} \\ [0.5ex]
%  & & & & \multirow{3}{*}{Sparse} & \multicolumn{2}{|c|}{Dense} \\
%  & & & & & \multicolumn{2}{|c|}{Preserves net structure?} \\ 
%  & & & & & \multicolumn{1}{|c}{Yes} & \multicolumn{1}{|c|}{No} \\ [0.5ex] %extra space to next row
%  \hline%\hline
%  \multirow{3}{*}{\rotatebox[origin=c]{90}{Correction}} & & & 
%  None & Cat. A & Cat. B & Cat. C \\ [1ex]
%  \cline{2-4}
%  & \multicolumn{2}{c}{Local}
%  %& \multirow{2}{*}{\rotatebox[origin=c]{90}{Local}} 
%  %& \multirow{2}{*}{\rotatebox[origin=c]{90}{FT?}}
%  & No & Cat. D & Cat. E & Cat. F \\ [1ex]
%  \cline{4-4}
%  & \multicolumn{2}{c}{FT?}
%  & Yes & Cat. G & Cat. H & Cat. I\\ [1ex]
%  \hline
% % \multirow{6}{*}{\rotatebox[origin=c]{90}{Correction}} & 
% % \multirow{2}{*}{None} & weight & channel & low- \\
% % & & prune & prune & rank \\ [1ex]
% % \cline{2-5}
% % & Yes, w/out &\multirow{2}{*}{N/A} & \multirow{2}{*}{ID} & \multirow{2}{*}{N/A}\\
% % & locFT &  & & \\ [1ex]
% % \cline{2-5}
% % & Yes, w/ & \multirow{2}{*}{WP+} & \multirow{2}{*}{CP+} & \multirow{2}{*}{LR+}\\
% % & locFT &  & & \\ [1ex]
% % \hline
% \end{tabular}
% \caption{Taxonomy of parameter compressing methods via 2 axes: the type of compression (how parameters are removed), and what type of correction is taken to improve the network after removing parameters.
% Often, local fine tuning is interleaved into compression methods.
% }
% \label{tab:taxonomy}
% \end{table}


%Relative to existing techniques, our use of the interpolative decomposition provides several key advantages. 
To facilitate a careful discussion of how our method fits within the current literature, Table~\ref{tab:taxonomy} provides a taxonomy of parameter space compression methods.
%Our formulation of compression as preserving the per-example labels necessitates us to focus on the paradigm of compressing a pre-trained model.
We focus on preserving the per-example labels of a pre-trained model. 
Thus we ignore methods which do not take a pre-trained model as input.
The ID~(Cat. E) combines benefits of both low-rank and channel pruning methods, while also incorporating a parameter correction done without additional local fine tuning.
Unlike low-rank methods~(Cat. F), the ID preserves the computational structure of the network.
This allows us to trivially compose the ID with other compression methods, achieving model recovery comparable to that of global fine tuning.
And unlike channel pruning methods~(Cat. B), the ID can fully recover the original model at minimal FLOPs reduction, without any fine tuning. 
%Low-rank methods modify the network structure by decomposing the weight matrices, and do not directly incorporate a corrective step.
%Channel pruning methods retain the network structure, but often poorly preserve model performance without  fine tuning (whether local or global).
%A key feature of our method is that it retains the network structure---just reducing the width---and, therefore, is able to leverage the computational benefits of specific network architectures.




%We can also prune to a fixed compression level and accept the resulting degradation in accuracy.  
%While this process requires additional data, it can be unlabeled data as our process does not require ``ground truth'' labels; in fact, it effectively uses pseudo-labels generated by the trained model. 
% Jerry removed 1/25/22
%Another key feature of our method is that it retains the network structure---just reducing the width---and, therefore, is able to leverage the computational benefits of specific network architectures.

In our taxonomy, we choose to separate any fine tuning (i.e., optimization) steps from the parameter reduction step.
What we call ``fine tuning'' can take two primary forms.
%: either locally interleaved into the parameter reduction, or performed end-to-end globally on the reduced model.
We define ``global'' fine tuning as optimizing the end-to-end loss.
``Local'' fine tuning is where per-layer structures are optimized.
It is valuable to compare compression methods that use similar a type and amount of fine tuning, as well as to compare before and after its application.
%This allows us to better evaluate whether accuracies are achieved due to a fine-tuning technique or due to the compression method itself.
%We do not interleave local fine tuning with the ID in order to better understand the contribution of our compression method.
%However, we recognize that local fine tuning is a powerful method. 
%Because the interpolative decomposition preserves the model structure, we can compose other methods that use local fine tuning on top of it.  
%We demonstrate the efficacy of this approach in section \ref{sec:experiments}.  
