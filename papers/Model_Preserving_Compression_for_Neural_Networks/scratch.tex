%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%% V1 Abstract
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Abstract v1}
%Both matrix decompositions and pruning have been used to compress neural networks, and we seek to combine the strengths of each in a novel method. Matrix methods typically preserve the model output, but insert new layers into the network.  Neuron and channel pruning usually keeps the same structure, but requires fine tuning that may be more consonant with retraining a new network.  
Our goal is to compress deep networks into narrower but identically structured models that closely mirror the output of the original model for valid data points (beyond simple test accuracy), with minimal fine tuning.   We introduce a principled approach to model compression that leverages structured low-rank matrix approximations known as interpolative decompositions to accomplish this task. By explicitly building an approximation to the activation output of each layer, we simultaneously select and eliminate channels (analogously, neurons) and construct an interpolation matrix that propagates a correction into the next layer.  Consequently, our method achieves good performance even without fine tuning and admits theoretical analysis.  Our theoretical generalization bound for a one layer network lends itself naturally to a heuristic that allows our method to automatically choose per-layer sizes for deep networks. Since our method simply makes networks narrower, it can easily be combined with other matrix decomposition techniques. We demonstrate the efficacy of our approach with strong empirical performance on a variety of tasks, models, and datasets--- from simple one hidden layer networks to deep networks on ImageNet. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%% V2 Abstract
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Abstract v2}
To be an effective method for practitioners, neural network compression methods must display the following qualities.
The per-example decisions must be preserved to ensure that properties beyond top-1 accuracy, such as sub-class accuracy or adversarial robustness, are retained.
The computational structure must be preserved to ensure trivial plug-and-play with the rest of the machine learning pipeline.
There must be minimal tuning.

We introduce a principled approach to model compression that satisfies these practical requirements, leveraging a matrix approximation from numerical linear algebra.
The interpolative decomposition is a structured low-rank matrix approximation which sub-selects columns and reconstructs those removed with an interpolation matrix.
By explicitly building an approximation to the activation output of each layer, we simultaneously select and eliminate channels (analogously, neurons) and construct an interpolation matrix that propagates a correction into the next layer.  
The form of the interpolative decomposition allows for preservation of the per-example decisions, as well as the computational structure. 
% maybe more detail?

%By explicitly building an approximation to the activation output of each layer, we simultaneously select and eliminate channels (analogously, neurons) and construct an interpolation matrix that propagates a correction into the next layer.  
%Consequently, 
Our method achieves good performance even without fine tuning and admits theoretical analysis.  
Our theoretical generalization bound for a one layer network lends itself naturally to a heuristic that allows our method to automatically choose per-layer sizes for deep networks. 
Since our method simply makes networks narrower, it can easily be combined with other matrix decomposition techniques. 
We demonstrate the efficacy of our approach with strong empirical performance on a variety of tasks, models, and datasets--- from simple one hidden layer networks to deep networks on ImageNet. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% New Abstract
A neural network compression method that is effective for practitioners must preserve the model's decisions as well as be practically usable.
Preserving the per-example decisions ensures that properties beyond top-1 accuracy, such as sub-class accuracy or adversarial robustness, are retained.
To ensure ease of use, the structure of the network must be preserved to enable trivial plug-and-play with the rest of the machine learning pipeline, and hyper-parameter tuning must be kept to a realistic minimum.
%Our goal is to compress deep networks into narrower but identically structured models that closely mirror the per-example decisions of the original model, 
%with minimal hyper-parameter tuning.
To satisfy these criteria our method combines the advantages of two types of well known compression methods: matrix approximation preserves the model's decisions, and structured pruning preserves the network's structure.
%Matrix approximations typically preserve the model well, but change the network structure by adding additional layers.
%Structured pruning retains the network's structure, but does not typically preserve the model's decisions.


%megan v 3
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% New Abstract
A neural network compression method that is effective for practitioners must preserve the model's decisions as well as be practically usable.Preserving the per-example decisions ensures that properties beyond top-1 accuracy, such as sub-class accuracy or adversarial robustness, are retained.To ensure ease of use, the structure of the network must be preserved to enable trivial plug-and-play with the rest of the machine learning pipeline, and hyper-parameter tuning must be kept to a realistic minimum.
To satisfy these criteria our method combines the advantages of two types of well known compression methods: matrix approximation preserves the model's decisions, and structured pruning preserves the network's structure.


It is practical for neural network compression methods to have several qualities:  First to preserve the per-example decisions to maintain properties beyond top-1 accuracy (like sub-class accuracy or adversarial robustness), and second to ensure ease of use by preserving the structure of the network and minimizing fine tuning and hyperparameter search to ensure trivial plug-and-play within a machine learning pipeline.  