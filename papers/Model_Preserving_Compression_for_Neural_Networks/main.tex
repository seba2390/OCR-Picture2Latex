%abstract rewriting to get better reviewers.  signal for do we want numerical linear algebra stuff at neurips.  classical empirical compression people 
%change the title to automated model preserving neural network compression  %emphasise the model correlation idea, frame the goals we want model preservation structure preservation and not a lot of fine tuning.  preserving per example decisions.  
%in the appendix talk a little bit more about the problem with pruning and how we demonstrate the 
%take the intro that is there and re-write it.  
% use some sort of "chemical notation" 
%add the two references that people noted.  fix our own references that were missing years.  
%re-do some of the plots, don't make it look like the background.  re-do the visuals to look like that.  make id dashed, and make the composed methods more obvious.  hit hte reader over the head with structure preserving. 
%add the checklist back in.  



\documentclass{article}
% if you need to pass options to natbib, use, e.g.:
\PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2022

% ready for submission
% \usepackage{neurips_2022}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
\usepackage[preprint]{neurips_2022}

% to compile a camera-ready version, add the [final] option, e.g.:
%\usepackage[final]{neurips_2022}

% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2022}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors

%%%% User defined macros %%%%%
\input{header}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\title{
Model Preserving Compression for Neural Networks
%Compressing Neural Networks while Preserving the Original Model
}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{%
  %David S.~Hippocampus\thanks{Use footnote for providing further information
  %  about author (webpage, alternative address)---\emph{not} for acknowledging
  %  funding agencies.} \\
  %Department of Computer Science\\
  %Cranberry-Lemon University\\
  %Pittsburgh, PA 15213 \\
  %\texttt{hippo@cs.cranberry-lemon.edu} \\
  % examples of more authors
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \AND
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}




\author{
  Jerry Chee\thanks{equal contribution} \\
  Department of Computer Science\\
  Cornell University\\
  \texttt{jerrychee@cs.cornell.edu}\\
  %Pittsburgh, PA 15213 \\
  %\texttt{hippo@cs.cranberry-lemon.edu} \\
  % examples of more authors
   \And
   Megan Renz\footnotemark[1]\\
   Department of Physics\\
   Cornell University \\
   \texttt{mr2268@cornell.edu}\\
  % Address \\
  % \texttt{email} \\
   \AND
   Anil Damle \\
   Department of Computer Science\\
   Cornell University \\
   \texttt{damle@cornell.edu}\\
  % Address \\
  % \texttt{email} \\
   \And
   Christopher De Sa \\
   Department of Computer Science\\
   Cornell University \\
   \texttt{cdesa@cs.cornell.edu}
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}



\begin{document}


\maketitle


\begin{abstract}
After training complex deep learning models, a common task is to compress the model to reduce compute and storage demands. When compressing, it is desirable to preserve the original model's per-example decisions (e.g., to go beyond top-1 accuracy or preserve robustness), maintain the network's structure, automatically determine per-layer compression levels, and eliminate the need for fine tuning. No existing compression methods simultaneously satisfy these criteria---we introduce a principled approach that does by leveraging interpolative decompositions. Our approach simultaneously selects and eliminates channels 
%a structured low-rank matrix approximation known as the interpolative decomposition.
%By explicitly building an approximation to the activation output of each layer, we simultaneously select and eliminate channels
(analogously, neurons), then constructs an interpolation matrix that propagates a correction into the next layer, preserving the network's structure. 
Consequently, our method achieves good performance even without fine tuning and admits theoretical analysis.
Our theoretical generalization bound for a one layer network lends itself naturally to a heuristic that allows our method to automatically choose per-layer sizes for deep networks.
% Since our method simply makes networks narrower, it can easily be combined with other matrix decomposition techniques.
We demonstrate the efficacy of our approach with strong empirical performance on a variety of tasks, models, and datasets---from simple one-hidden-layer networks to deep networks on ImageNet.

% Preserving the original model's per-example decisions beyond top-1 accuracy and enabling effortless plug-and-play within machine learning pipelines by preserving network structure and minimizing fine tuning are both desirable properties for network compression methods. 
%by preserving the structure of the network while minimizing 
%Practical neural network compression methods should have the following qualities:  First to preserve the model's per-example decisions to maintain properties beyond top-1 accuracy (like
%sub-class accuracy
%fairness criteria
%or adversarial robustness), and second to enable
%effortless plug-and-play within a machine learning pipeline 
%by preserving the structure of the network while minimizing fine tuning and hyperparameter search.
%to ensure trivial plug-and-play within a machine learning pipeline.  
% A neural network compression method that is effective for practitioners must preserve the model's decisions as well as be practically usable.
% Preserving the per-example decisions ensures that properties beyond top-1 accuracy, such as sub-class accuracy or adversarial robustness, are retained.
% To ensure ease of use, the structure of the network must be preserved to enable trivial plug-and-play with the rest of the machine learning pipeline, and hyper-parameter tuning must be kept to a realistic minimum.
%Our goal is to compress deep networks into narrower but identically structured models that closely mirror the per-example decisions of the original model, 
%with minimal hyper-parameter tuning.
%To satisfy these criteria 
%Our method (uniquely?) combines the advantages of two types of well known compression methods: matrix approximation preserves the model's decisions, and structured pruning preserves the network's structure.
%Matrix approximation methods typically only satisfy the first criteria, while structured pruning methods typically only satisfy the second.
% Matrix approximation methods preserve the output, while structured pruning methods preserve the computational structure. 
% Our method combines advantages of both these types of methods.
%Matrix approximations typically preserve the model well, but change the network structure by adding additional layers.
%Structured pruning retains the network's structure, but does not typically preserve the model's decisions.

% We introduce a principled approach that leverages the interpolative decomposition to build a structured low-rank approximation of the activation output of each layer. 
% By doing so, we simultaneously select and eliminate channels 
% %a structured low-rank matrix approximation known as the interpolative decomposition.
% %By explicitly building an approximation to the activation output of each layer, we simultaneously select and eliminate channels
% (analogously, neurons) and construct an interpolation matrix that propagates a correction into the next layer, preserving the network's structure.
% Consequently, our method achieves good performance even without fine tuning and admits theoretical analysis.
% Our theoretical generalization bound for a one layer network lends itself naturally to a heuristic that allows our method to automatically choose per-layer sizes for deep networks.
% % Since our method simply makes networks narrower, it can easily be combined with other matrix decomposition techniques.
% We demonstrate the efficacy of our approach with strong empirical performance on a variety of tasks, models, and datasets---from simple one hidden layer networks to deep networks on ImageNet.
\end{abstract}


% \listoftodos

\section{Introduction}
\label{sec:intro}
\input{sections/introv2}

%\section{Context and Related Work}
\section{Related Work}
\label{sec:related}
\input{sections/relatedv2}

\section{Interpolative decompositions}
\label{sec:ID}
\input{sections/id}

\section{Pruning with interpolative decompositions}
\label{sec:pruneID}
\input{sections/pruneid}

\section{Convolutional and deep networks}
\label{sec:extendid}
\input{sections/extendid}

%\section{Contextualizing our work}
%\label{sec:context}
%\input{sections/contextualize}

\section{Evaluating compression beyond accuracy}
\label{sec:correlation}
\input{sections/correlation}


\section{Experiments}
\label{sec:experiments}
\input{sections/experiments}

\vspace{-2em}
\section{Limitations and future work}
\label{sec:dis}
%% theory does not specifically encode for fine grain properties like fairness or robustness
Our method comes with several limitations and possible extensions for future work.  
While preserving model correlation suggests that we are likely to preserve sub-class loss, our theory does not currently extend to that regime and requires that the unlabeled pruning set be from the same distribution as the test set. More broadly, we do not yet fully understand how trainable the models produced by ID are in different conditions and we cannot make claims about how compressable a given model will be. In future work we will explore modifying training process to improve prunability, which is a common approach~\cite{huang2018sss,yang2020hoyer,liu2017netslim,zhuang2020polar}.  
We will also explore ways to refine our iterative pruning approach to work with
%residual layers and 
more complicated architectures. Of particular note, our method typically exposes a sizable ``free FLOPs" regime and we explore how this can be leveraged more broadly.


\vspace{-1em}
\begin{ack}
\vspace{-1em}
This work was partially funded by the National Science Foundation under awards
DMS-1830274, DGE-1922551, and NSF CAREER Award 2046760.
\end{ack}


{
\small
\bibliographystyle{plainnat}
\bibliography{id_for_nn.bib}
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%% BEGIN INSTRUCTIONS %%%
%The checklist follows the references.  Please
%read the checklist guidelines carefully for information on how to answer these
%questions.  For each question, change the default \answerTODO{} to \answerYes{},
%\answerNo{}, or \answerNA{}.  You are strongly encouraged to include a {\bf
%justification to your answer}, either by referencing the appropriate section of
%your paper or providing a brief inline description.  For example:
%\begin{itemize}
%  \item Did you include the license to the code and datasets? \answerTODO{}
%  \item Did you include the license to the code and datasets? \answerTODO{}
%  \item Did you include the license to the code and datasets? \answerNA{}
%\end{itemize}
%Please do not modify the questions and only use the provided macros for your
%answers.  Note that the Checklist section does not count towards the page
%limit.  In your paper, please delete this instructions block and only keep the
%Checklist section heading above along with the questions/answers below.
%%% END INSTRUCTIONS %%%

% \section*{Checklist}
% \begin{enumerate}


% \item For all authors...
% \begin{enumerate}
%   \item Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?
%     \answerYes{}
%   \item Did you describe the limitations of your work?
%     \answerYes{} Section 7
%   \item Did you discuss any potential negative societal impacts of your work?
%     \answerNA{} 
%   \item Have you read the ethics review guidelines and ensured that your paper conforms to them?
%     \answerYes{}
% \end{enumerate}


% \item If you are including theoretical results...
% \begin{enumerate}
%   \item Did you state the full set of assumptions of all theoretical results?
%     \answerYes{}
%         \item Did you include complete proofs of all theoretical results?
%     \answerYes{} - In the Appendix.  
% \end{enumerate}


% \item If you ran experiments...
% \begin{enumerate}
%   \item Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)?
%     \answerYes{} % - Supplemental material
%     % \href{https://github.com/jerry-chee/ModelPreserveCompressionNN}{https://github.com/jerry-chee/ModelPreserveCompressionNN}
% %   \item Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)?
%     \answerYes{} Appendix
%         \item Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)?
%     \answerYes{} Where including error bars was computationally feasible we did so.  This was not computationally feasible on ImageNet.  
%         \item Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? 
%     \answerYes{} Appendix
% \end{enumerate}


% \item If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...
% \begin{enumerate}
%   \item If your work uses existing assets, did you cite the creators?
%     \answerYes{}
%   \item Did you mention the license of the assets?
%     \answerYes{}{}
%   \item Did you include any new assets either in the supplemental material or as a URL?
%     \answerNo{}
%   \item Did you discuss whether and how consent was obtained from people whose data you're using/curating?
%     \answerNA{}
%   \item Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content?
%     \answerNA{}
% \end{enumerate}


% \item If you used crowdsourcing or conducted research with human subjects...
% \begin{enumerate}
%   \item Did you include the full text of instructions given to participants and screenshots, if applicable?
%     \answerNA{}
%   \item Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable?
%     \answerNA{}
%   \item Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation?
%     \answerNA{}
% \end{enumerate}


% \end{enumerate}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage
\appendix
\section*{Appendix}
\input{sections/supplement}



\end{document}