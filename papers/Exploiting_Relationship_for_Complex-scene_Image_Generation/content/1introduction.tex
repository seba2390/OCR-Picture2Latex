\section{Introduction}

%Generating images from natural language inputs has achieved promising progress owing to the success of Generative Adversarial Networks (GANs) and word embedding. 
In the past few years, text-to-image generation has drawn extensive research attention for its potential applications in art generation, computer-aided design, image manipulation, etc. However, such success is only restricted to simple image generation, which only contains a single object in a small domain, such as flowers, birds, and faces \cite{reed2016generative,bao2017cvae}. Complex-scene generation, on the other hand, targets for synthesizing realistic scene images out of complex sentences depicting multiple objects as well as their interactions. Nevertheless, generating complex-scenes on demand is still far from mature based on recent studies~\cite{johnson2018image, xu2018attngan, li2019object,hinz2019generating}.

\begin{figure}[!t]
\centering
\includegraphics[width=0.9\linewidth,page=1]{figures/new_fig1_v2.pdf}
\caption{Relationship matters for complex-scene image generation. The same object pair (e.g., \textit{man and board}) could show different object shapes, scene layouts and appearances under different relationships.} 
\label{fig:intro_cases}
\end{figure}

Scene graph, a structured language representation, captures objects and their relationships described in the sentence~\cite{xu2017scene}. Such representation is proven effective for image-text cross-modal tasks, such as structural image retrieval~\cite{johnson2015image,schuster2015generating,johnson2018image}, image captioning~\cite{yang2019auto,li2019know,li2018jointly} and visual question answering~\cite{teney2017graph,norcliffe2018learning}. %In the context of complex-scene generation, scene-graph also bridges language and vision, i.e., language $\rightarrow$ scene graph $\rightarrow$ image. On the one hand, complex sentences can be readily converted to scene graphs with off-the-shelf tools \cite{msdn,schuster2015generating}. On the other hand, scene-graph to image generation is much more challenging \cite{johnson2018image}, in terms of visual fidelity and language-image coherence.
In this work, we focus on complex-scene image generation from scene graphs. Although extensive works have been done in scene graph generation from image~\cite{xu2017scene,zellers2018neural,msdn,vtrans} (i.e. image$\rightarrow$scene graph), reversely generating a complex-scene image from a scene graph remains challenging, due to the polymorphism nature of one-to-many mapping from a given scene graph to multiple reasonable images with different scene layouts.
% standard pipeline
% \textcolor{red}{add}

A general pipeline for scene graph based image generation usually consists of two stages~\cite{johnson2018image}. The first one learns to synthesize a rough layout prediction from the scene-graph input. Usually, the object features are encoded with a graph module~\cite{johnson2018image, ashual2019specifying}, followed by a direct regression of bounding-box locations. At the second stage, a position-aware feature tensor, that combines object features and layout generated in the first stage, is fed into an image decoder for generating the final image. For enhancing the object appearances in generated images, Ashual~\textit{et al.} separates appearance embedding from layout embedding. 
% problem statement

However, previous works on complex-scene generation heavily suffer from two fundamental problems: messy layout, and object distortion.
%
1) \textit{Messy layouts}. Image generation models are expected to figure out the reasonable layout from scene-graph inputs. However, there exist an infinite number of reasonable layouts for a given scene-graph. Directly fitting a specific layout introduces huge confusion, and limits the generalization ability. As a result, existing methods are still struggling with messy layouts in practice. 
2) \textit{Distortion in object appearance}. Due to the high diversity in object categories, layouts, and relationship dependencies, objects are often distorted during generation. For each object, the texture and local appearances should be inferred, respecting both its category and allocated spatial arrangement. Moreover, complex and various relations among different objects in the scene-graph can further increase the diversity of shape appearances. As shown in Fig.~\ref{fig:intro_cases}, even with the same object pairs, equipping different relationships could lead to totally different scene layouts and appearances. 

Some works~\cite{ashual2019specifying} simplify the task by only taking a few simple spatial relationships among objects (such as ``left'', ``right'' or ``above'') but ignoring other complex relationships (such as verbs). Meanwhile, to reduce the complexity, some works only consider one specific stage of this task, such as layout generation from scene-graph~\cite{jyothi2019layoutvae}, image generation from layout~\cite{zhao2018image, sun2019image}. All these works did not take account of the semantics and complex relationships among objects, which limits their application prospects.

In this work, we explore relationships to mitigate the above issues. We consider both simple spatial relationships and complex semantic relationships into consideration. We observed that, in different realistic images, relative scale and distance ratios between two interrelated objects from the same ``subject-relation-object'' triplet usually conform to a norm distribution with low variance, as in Fig.~\ref{fig:scale_dist_example}. Even though the ``human'' have various poses, and the skateboard can be oriented to different directions, the scale ratio between the two bounding boxes is naturally clustered with very low variance. Thus, we first introduce relative scale ratios and distance for measuring the rationality of layouts generated from the scene graph. It means that all \textit{various reasonable layouts} relevance to one specific scene graph can be measured under a common standard and result in very similar results. After that, we proposed a \textit{Pair-wise Spatial Constraint Module} for assisting layout generation. Our Spatial Constraint Module is influenced by object pairs and their corresponding relation jointly. Meanwhile, the objective of this novel module is to correct the layout by fitting the relative scale ratio and relative distance ratio between interrelated object pair beside the absolute position coordinates of each object. In this way, the spatial commonsense of complex scene with multiple objects can be modeled.

\begin{figure}[!t]
\centering
\includegraphics[width=0.99\linewidth]{figures/figures2.pdf}
\caption{Distributions of relative scale and distance for ``man riding  skateboard'' and ``man sitting~on bench''.}
\label{fig:scale_dist_example}
\end{figure}

% C2: discriminator
Moreover, for enhancing the influence of relation for object appearance generation, we proposed a \textit{Relation-guided Appearance Generator} and a novel \textit{Scene Graph Discriminator} for guiding image generation. Unlike the traditional discriminator that only judges whether the image is fake or not, our proposed new discriminator has two main functions. One is to determine whether the objects in the generated image are relevant to the objects described in the text scene graph or not, and the other is to discriminate the relation predictions among objects from the generated image are correspondence with the relationship described in the input scene graph. By feeding back these strong discriminant signals, our Scene Graph Discriminator guarantees the generated object patches align with not only single object fine-grained information but also the relation discrepancy among objects. 

The main contributions can be summarized as follows:
\begin{itemize}%\setlength\itemsep{-0.2em}
%\setlength\itemsep{1em}
\item A novel pair-wise spatial constraint module with supervisions of relative scale and distance between objects for learning relationship-aware spatial perceptions. 
\item A relation-guided appearance generator module followed by a scene graph discriminator for generating reasonable object patches respecting object fine-grained information and relation discrepancy.
\item A general framework for synthesizing scene layout and images from scene graphs. The experimental results on Visual Genome~\cite{krishna2017visual} and human-objects interactions dataset HICO-DET~\cite{chao2018learning} demonstrate the complex-scene images generated by our proposed method follow the common sense.% The relation prediction constraints also ensure the appearances of the objects in the final generated images are more reasonable.
\end{itemize}

