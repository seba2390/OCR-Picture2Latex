\section{Experimental Results}
%Experiments of our method are conducted on complex scene graphs dataset Visual Genome\cite{krishna2017visual} and human-objects interactions dataset HICO-DET\cite{chao2018learning}. The COCO dataset\cite{caesar2018coco-stuff} is not used in this paper because the relationship types in COCO are too simple, consisting mainly of naive spatial arrangement relations.

%Quantitative results, qualitative results, and user studies show our method is capable of not only generating complex and reasonable layouts but also synthesizing realistic images with multiple objects from scene graphs.

%\subsection{Datasets and Experimental Settings}
We evaluate our proposed method for generating images at three different resolutions 64$\times$64, 128$\times$128, and 256$\times$256 in below two datasets:
%\begin{itemize}

\noindent\textbf{Visual Genome} \cite{krishna2017visual} was constructed with cognitive tasks that provide crowd-sourced dense annotations of both scene graphs and images. Following the settings of \cite{johnson2018image}, we experiment on Visual Genome version 1.4. %The dataset is split into $80\%$ train, $10\%$ val, and $10\%$ test. 
We keep 178 objects and 45 relations in the dataset by removing images with object and relationship categories less than 2000 and 500, respectively. 

\noindent\textbf{HICO-DET}~\cite{chao2018learning} was built for modeling humans-object interactions. Compared with Visual Genome, the scene graphs provided in the dataset are human-centered. %Object classes, interaction categories as well as their respective bounding boxes are included in the dataset. 
%HICO-DET provides more than 150K annotated instances of human-objects pairs and 600 human-interaction-object categories with an average of 250 instances per human-interaction-object category. 
We keep images that have object categories higher than 1000 and discard images with interaction types that repeat less than 250 times, leaving 19 objects and 22 relationship types in total. Images with Object size below 32$\times$32 and images with objects less than 2 or more than 10 are ignored. Finally, we get 15963 train images and 4034 test images.
%\end{itemize}

%\subsection{Implementation Details}
The COCO dataset~\cite{caesar2018coco-stuff} is not used in this paper because the relationship types in COCO are too simple, consisting mainly of naive spatial arrangement relations. %To ensure all scene graphs are connected, each scene graph adds a virtual ``\_image\_" object and a ``in\_image" relationship that links all objects with ``\_image\_".
We trained models using Adam with an initial \textit{lr}=$10^{-4}$ and batch size of 32 for 200 epochs. %The training takes about four days on 4 Tesla P40 GPUs.

Several previous works target at multi-object image generation. Most of these works are about image synthesis from the ground-truth layout or pixel-level instance segmentation annotation~\cite{sun2019image,hong2018inferring,li2019object}. The work of \citeauthor{ashual2019specifying} aims to generated images from an input scene graph. However, the scene graph used in their work is simplified by only equipping six spatial relationships (right-of, left-of, above, below, surrounding and inside). Moreover, location attributes are assisted by additional information for each node in scene graph. Moreover, location attributes are assisted by additional information for each node in scene graph. \citeauthor{luo2020end} only focus on spatial relationships instead of semantic relationships. Besides, objects used in their paper are mostly rigid bodies. Our paper learns from not just spatial relationships, but semantic relationships (e.g. looking at) as well. We use datasets that involve a large number of non-rigid objects that have various shapes and appearances and be sensitive to their relevant semantic relationships, which drastically increase the difficulty of our task. %Complex and semantic relationships are not covered in these frameworks, which resulted in their methods being evaluated on COCO-stuff and could not be extended to datasets that contain complex relationships. This makes fair comparisons with them very difficult.
%For simple relationship datasets, layouts can be learned generated on simple physical rules. One of our innovations is the relation guided generation of object layouts which applies directly to complex datasets. 
%Luo~\textit{et al.} only focus on spatial relationships instead of semantic relationships. Besides, objects used in their paper are mostly "rigid" bodies e.g. closets, tables. Our paper learns from not just spatial relationships, but semantic relationships (e.g. looking at, sitting on) as well. 
PasteGAN~\cite{yikang2019pastegan} applies both of the scene graph and ground-truth image crops as the inputs for complex-scence image generation. According to our best knowledge, sg2im~\cite{johnson2018image} is the only related work about complex-scene image generation images from scene graphs that contain semantic and complex relationships among objects. 

\noindent\textbf{Compared Methods} In this paper, we compare our proposed method with \textbf{sg2im} and \textbf{PasteGAN} for complex-scene image generation. Moreover, to demonstrate the effectiveness of our relation-guided appearance generator and scene graph discriminator, we also compare our method with \textbf{LostGAN}~\cite{sun2019image} which is designed for generating images by given ground-truth layout.
%We use the code and the pre-trained model provided by the authors at \url{https://github.com/google/sg2im} to generate images of 64$\times$64 and 128$\times$128 resolutions for sg2im, and then we compare these results with the images generated by our proposed method.

%We set the weight for graph loss $\lambda_{5}$ to be 1 and the weight for image adversarial loss $\lambda_{3}$ to be 0.1. In our layout constraint module, the loss for both box and scale-distance $\lambda_{1}$ and $\lambda_{2}$ are set to 1.  
% 256 px models batch size 32
% 64 px models batch size 256

\begin{table}[!t]
\small
\centering
\setlength{\tabcolsep}{1.4mm}{
\begin{tabular}{l|c|cc|cc}
\hline
\multirow{2}{*}{Resolution}& \multirow{2}{*}{Method} &\multicolumn{2}{c|}{Visual Genome} & \multicolumn{2}{c}{HICO-DET}\\\cline{3-6}
% & &\multicolumn{4}{c}{HICO}\\
  &  & IS & FID & IS & FID \\
\hline\hline

\multirow{7}{*}{64$\times$64} & $I$  & 13.9 $\pm$ 1 & 0.0 &  9.8 $\pm$ 0.5 & 0.0 \\
% \cline{2-6}
& sg2im$\dagger$ & 6.3 $\pm$ 0.2 &47.6 & 4.4 $\pm$ 0.1 & 99.9\\
& LostGAN$\dagger$ & 6.9$\pm$0.1 & 38.7 & 4.5$\pm$0.3 & 86.4 \\
& Ours$\dagger$ &  \textbf{7.5} $\pm$ \textbf{0.4}  & \textbf{29.0} &  \textbf{5.5} $\pm$ \textbf{0.1} & \textbf{41.7}\\ 
& sg2im & 5.5 $\pm$ 0.1 &47.5 & 4.4 $\pm$ 0.1 & 94.3\\
& PasteGAN & 6.9$\pm$0.2 & 58.5 & - & -\\
& Ours & \textbf{7.0} $\pm$ \textbf{0.2} & \textbf{37.7} & \textbf{5.3} $\pm$ \textbf{0.7} & \textbf{47.4}\\

\hline

\multirow{6}{*}{128$\times$128} & $I$  & 22.5 $\pm$ 1.9 & 0.0 &  13.7 $\pm$ 0.7 & 0.0 \\
% \cline{2-6}
& sg2im$\dagger$ & 6.3 $\pm$ 0.2 & 83.9 & 4.6 $\pm$ 0.1 & 83.7\\
& LostGAN$\dagger$ & 7.4$\pm$0.3 & 53.4 & 4.8$\pm$0.1 & 79.9 \\
& Ours$\dagger$ &  \textbf{9.4} $\pm$ \textbf{0.4}  & \textbf{41.0} &  \textbf{6.5} $\pm$ \textbf{0.1} & \textbf{60.6}\\ 
& sg2im & 6.2 $\pm$ 0.2 & 83.8 & 4.6 $\pm$ 0.1 & 123.0\\
& Ours & \textbf{9.2} $\pm$ \textbf{0.8} & \textbf{53.0} & \textbf{5.0} $\pm$ \textbf{0.3} & \textbf{61.4}\\
\hline

\multirow{3}{*}{256$\times$256} & $I$  &  30.1 $\pm$  2.3 & 0.0 & 16.3 $\pm$ 0.5 & 0.0 \\
& Ours$\dagger$ & 12.6 $\pm$ 0.5  & 68.3 &7.5 $\pm$ 0.1& 78.3\\ 
& Ours  & 10.8 $\pm$ 0.9 & 85.7 & 6.9 $\pm$ 0.3 & 80.5 \\
\hline
\end{tabular}}
\caption{The comparison of IS and FID among different methods. On each dataset, the test set samples are randomly split into 5 groups. The mean and standard deviation across splits are reported in the above table. $\dagger$ indicates that the images are generated based on the ground-truth layouts instead of the generated layouts. $I$ denotes the real image.}\label{tab:IS}
\end{table}


\begin{figure*}[!t]
\centering
\includegraphics[width=1\linewidth,page=1]{figures/figure256.pdf}
\caption{Examples of layouts and images generated from scene graphs in Visual Genome and HICO-DET for our method and sg2im. In the layout examples, we use red color patches to denote bounding boxes that fail to reflect the distance between object pairs. The bounding boxes with blue background have an unnatural scale configuration. Best viewed in color version.}
\label{fig:cmp}
\end{figure*}

\subsection{Quantitative Results}
% As far as we know, there are no metrics for layout evaluation in a direct way. In \cite{johnson2018image}, layout serves as an intermediate representation influencing the final quality of synthetic images. Therefore, we adopt an indirect way by measuring the quality of generated images.

% To evaluate the generated image quality as well as the 
We adopt two metrics to evaluate the generated images. \\\textbf{Inception Score} (\textit{IS}) \cite{salimans2016improved} measures the diversity of generated images and their quality. A pre-trained InceptionV3 model is adapted to predict the class probabilities for given image. Larger inception scores are better.\\\textbf{Fréchet Inception Distance} (\textit{FID}) \cite{heusel2017gans}\footnote{https://github.com/mseitzer/pytorch-fid} measures the Fréchet distance between the multivariate Gaussian distribution of real images and generated ones. Lower \textit{FID} scores are better.\\
These two metrics are widely used evaluation metrics for generative models. \textit{IS} aims to evaluate the reality of a single object, while \textit{FID} is more suitable to reflect the quality of the generated image contains multiple objects.

Table~\ref{tab:IS} summarizes the performances on the two aforementioned datasets in terms of Inception Score and \textit{FID} score. Our model outperforms sg2im on both VG and HICO-DET datasets. 
Moreover, even without the external information like image crops, our method can still achieve better results reported in PasteGAN. 
In addition, we conduct experiments of GT Layout versions using ground-truth bounding boxes during both training and testing. This method gives an upper bound to the model's performance in the case of perfect layout construction. As shown in Table~\ref{tab:IS}, our method has more potential than sg2im and LostGAN.

We also conducted ablation studies in Table~\ref{tab:ablation}. The relative importance of the \textit{Pair-wise Spatial Constraint Module}, and \textit{Scene Graph Discriminator} are measured by removing $\mathcal{L}_{scm}$ and $\mathcal{L}_{sg}$ from the overall objective function respectively. The ablation studies of \textit{Relation-guided Appearance Generator} is measured by erasing relation embeddings during computing object shape and texture features. 
It can be found that the layout constraint module predicts reasonable spatial layout arrangements that improve the generated image qualities. The relation-guided generator introduces more reasonable appearance information. The scene graph discriminator can advance the correspondence between textual scene graph inputs and generated images. %The main motivation of these three modules is to highlight the influence of relationships during multi-object image generation. 
%Our ablation studies can further demonstrate that the significance of resorting relationships for complex-scene generation, without which performance on \textit{IS} and \textit{FID} drops noticeably.

\begin{table}[!t]
\centering
\small
\setlength{\tabcolsep}{1.2mm}{
\begin{tabular}{c|c|c}
\hline
Method & IS & FID \\
\hline\hline
Ours & 9.2 $\pm$ 0.8 & 53.0\\
%\midrule
w/o Pair-wise Spatial Constrain Module ($\mathcal{L}_{scm}$) & 8.6 $\pm$ 1.2 & 59.8\\
%\midrule
w/o Relation-guided Appearance Generator & 8.7 $\pm$ 0.9 & 57.4\\
w/o Scene Graph Discriminator ($\mathcal{L}_{sg}$) & 7.4 $\pm$ 0.2 & 73.3\\
\hline
\end{tabular}}
\caption{Ablation studies conducted on our proposed method. The experimental results are reported on 128$\times$128 resolution image generation task in Visual Genome.}\label{tab:ablation}
\end{table}

\begin{figure}[!t]
\centering
\includegraphics[width=1\linewidth,page=1]{figures/figure6.pdf}
\caption{Generated samples from ground truth layouts on Visual Genome by sg2im, LostGAN and our method.}
\label{fig:cmp_gt}
\end{figure}
\iffalse
\begin{table}[!t]
% \begin{table}[]
\scriptsize
\centering
\begin{tabular}{p{6cm}|l|l}
\hline
User study                                                              & sg2im & Ours \\ \hline\hline
Generate images more relevant to scene graphs                           & 39\% & 61\% \\ \hline
Generate more realistic images                                          & 35\% & 65\% \\ \hline
Generate images better reflect relationships in scene graphs   & 44\% & 56\% \\ \hline
Generate layouts with reasonable placement                              & 42\% & 58\% \\ \hline
Generate layouts better reflect relationships in scene graphs & 40\% & 60\% \\ \hline
\end{tabular}
\caption{}
\label{tab:user_study}
\end{table}
\fi

\begin{table}[!t]
  % \begin{table}[]
  \small
  \centering
  \setlength{\tabcolsep}{1.2mm}{
  \begin{tabular}{p{5.3cm}|l|l|l}
  \hline
  User study                                                              & sg2im & Same & Ours \\ \hline\hline
  Image is more realistic                                          &  9\% & 26\% & 65\%  \\ \hline
  Image has reasonable object arrangement                          & 12\% & 27\% & 61\%  \\ \hline
  Image reflects relationships in scene graph                  &  9\% & 19\% & 72\%  \\ \hline
  Layout is more reasonable                                        & 11\% & 30\% & 59\%  \\ \hline
  % Generate layouts better reflect relationships in scene graphs & 40\% & 60\% \\ \hline
  \end{tabular}}
  \caption{We performed a user study to compare the quality of generated layouts and images of our method against sg2im.}
  \label{tab:user_study}
  \end{table}

\subsection{Qualitative Results}
Fig.~\ref{fig:cmp} shows the capability of our method compared with that of sg2im on VG and HICO-DET. 
In the 1st column, sg2im predicts the human layout is above the motorcycle, which is not a normal position arrangement. Similarly, in the 5th column, sg2im predicts that ``pant'' is not vertically in line with the ``shirt''. Moreover, in the last column sg2im predicts that the scale of ``sky'' is too small.
%In the 1st column, given the triple ``human ride motorcycle'', sg2im predicts the human layout is above the motorcycle, which is not a normal position arrangement for this interaction. Similarly, in the 5th column, sg2im predicts that ``pant'' is not vertically in line with the ``shirt''. Moreover, in the last column sg2im predicts that the scale of ``sky'' is too small. 
It leads to the chaotic color fill in the generated image. Similarly, in the 7th column sg2im predicts the scale of ``snow'' is much bigger than ``mountain'', which conflicts with the triplet ``snow on mountain''. These displacements occur when the training process is not enhanced with relative distance and scales. 

Fig. \ref{fig:cmp_gt} shows the generated images conditioned on ground truth layouts. We compared our model grounded on the same position layouts compared with sg2im and LostGAN. It can be found that our method is more likely to generate realistic images from rich layouts and with natural objects. 

\noindent\textbf{User Study}\quad We also conduct a user study to measure human preference between images generated by out method and sg2im in Table~\ref{tab:user_study}. We choose the 128$\times$128 resolution models for both cases. %We didn't use the 64 $\times$ 64 model because the generated results tend to be too blurry to recognize. 
Our user study involves 40 students having a background in computer science. We generate 500 test cases from the VG test set for user study. % by removing images that have relationships less than 10. The test cases are randomly chosen from these 500 samples. %We ask each student 20 questions from the 5 questions. %Each question has two or three images, depending on whether a scene graph is referred.
%\begin{itemize}
%\item Which of the two images better reflects the scene graph?
%\item Which of the two images are more realistic?
%\item Which of the two images better reflects the relationships presented in the scene graph?
%\item Given the scene graph, which of the two layouts are more reasonable?
%\item Given the scene graph, which of the two layouts better reflects the relationships presented in the scene graph?
%\end{itemize}
%The results are shown in Table.~\ref{tab:user_study}, 
A majority of users preferred the generated layouts and images from our method in 65\% of image pairs.%, due to the more reasonable layouts and realistic images.

%\fi

