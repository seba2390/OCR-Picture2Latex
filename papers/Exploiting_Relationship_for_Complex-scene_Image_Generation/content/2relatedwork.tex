\section{Related Work}

%\textbf{Conditional Image Generation} tasks aim to synthesize images given meaningful inputs, such as one-hot encoded object category information, images from pixel space, or text descriptions. Generative Adversarial Networks (GANs)~\cite{goodfellow2014generative}, Variational Autoencoders~\cite{kingma2014auto-encoding} and Autoregressive Models~\cite{den-oord2016pixel,den-oord2016conditional} are three most widely used methods for these tasks. In particular, GAN based methods~\cite{miyato2018spectral,zhang2017stackgan,hua2019modeling,huang2017beyond} have risen to prominence in difference conditional generation tasks mentioned above. The earliest conditional image generation task focus on class (one label) based image generation~\cite{mirza2014conditional}. In some other cases, the conditioning signal is a source image. This kind of conditional image generation task is often referred to as image translation~\cite{isola2017image,yang2018crossing}. 

\textbf{Image Synthesis from Sentence} is a conditional image generation task whose conditional signal is natural language. Textual descriptions are traditionally fed directly to a recurrent model for semantic information extractions. After that, a generative model will produce the results conditioned on this vectorized sentence representation. Most of these tasks specialize in single object image generation~\cite{reed2016generative,zhang2017stackgan,xu2018attngan}, whose layout is simple and the object usually centered with a large area in the image. However, generating realistic multi-object images conditioned on text descriptions is still a difficult task, since it addresses very complex sense layout generation and various object appearances mapping, and both of scene layout and object appearances are heavily influenced by the spatial and semantic relationships cross objects. Furthermore, encoding all information, including multiple object categories and the interactions among them into one vector, usually leads to critical details lost. Meanwhile, directly decoding images from such an encoded vector hurts the interpretability of the model.
%For generating realistic multi-object images conditioned on text descriptions, a more natural way is to construct a standard layout first. A layout generator decides the relational dependencies described in the unstructured sentence and then anchor the possible locations of objects to the generated images~\cite{hong2018inferring}. However, it is still a difficult task for generating realistic images containing multiple objects from text descriptions. Since it addresses very complex sense layout generation and various object appearances mapping, and both of scene layout and object appearances are heavily influenced by the spatial and semantic relationships cross objects. Furthermore, encoding all information, including multiple object categories and the interactions among them into one vector, usually leads to critical details lost. Meanwhile, directly decoding images from such an encoded vector hurts the interpretability of the model.

\textbf{Scene Graph}~\cite{xu2017scene} is a directed graph that represents the structured relationships among objects in an explicit manner. Scene graphs have been widely used in many tasks such as image retrieval~\cite{johnson2015image}, image captioning~\cite{anderson2016spice}, which serves as a medium that bridges the gap between language and vision. %Notably, the nodes of a scene graph are nouns that denote objects in the scene, and the edges are most likely verbs and prepositions that represent relationships between objects~\cite{xu2017scene}. 

%\textbf{Scene Graph} is a directed graph that represents the structured relationships among objects in an explicit manner. Notably, the nodes of a scene graph are nouns that denote objects in the scene, and the edges are most likely verbs and prepositions that represent relationships between objects~\cite{xu2017scene}. Scene graphs have been widely used in many tasks such as image retrieval~\cite{johnson2015image}, image captioning~\cite{anderson2016spice}, which serves as a medium that bridges the gap between language and vision.

\textbf{Image Synthesis from Scene Graph}~\cite{johnson2018image} is a derivative of sentence based multiple-object image generation. Since the conditional signals are scene graphs, graphic models are usually applied for extracting essential information from the textual scene graph. After that, these extracted features are directly used for regressing the scene layouts and then treated as input to an image decoder for generating the final image~\cite{ashual2019specifying}. Such a framework is applicable to generation image contains multiple objects with simple spatial interactions. However, it is still suffering from modeling the reasonable scene layouts and appearances following commonsense due to the implication of semantic relationships in the scene graph.

%In this paper, we focus on image generation from the textual scene graph. Different from previous methods, we highlight the impact of relationships among objects by calculating the relative scales and distances among objects and generating object patches respecting relationship discrepancy. The global layout features and local object features are combined to synthesize the images with fine-grained details. Moreover, a novel scene graph discriminator is proposed for ensuring the generated image to correspond to the given textual scene graph.

In this paper, we focus on image generation from the textual scene graph. Different from previous methods, we highlight the impact of relationships among objects for dealing with the messy layout and various object appearance.% by introducing a spatial constraint module. Moreover, the semantic information about relationship is also considered for influencing the appearance of objects. After all, a novel scene graph discriminator is proposed for ensuring the generated image to correspond to the given textual scene graph.


%calculating the relative scales and distances among objects and generating object patches respecting relationship discrepancy. The global layout features and local object features are combined to synthesize the images with fine-grained details. Moreover, a novel scene graph discriminator is proposed for ensuring the generated image to correspond to the given textual scene graph.
