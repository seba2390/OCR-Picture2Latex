\section{Method}
% scanned with grammarly 2020-04-21-10:00
A scene graph is denoted as $\mathcal{G}=\{ \mathcal{C}, \mathcal{R}, \mathcal{E} \}$, where $\mathcal{C}=\{c_1, c_2, ..., c_n\}$ indicate the nodes in the graph, each $c_i\in \mathcal{C}$ denotes the category embedding of an object or instance. Note that we use words like "object" or "instance" in reference to a broad range of categories from "human", "tree" to "sky", "water" etc. The edges of the graph are extracted as a relationship embedding set $\mathcal{R}$. Two related objects $c_j$ and $c_k$ associate with one relationship $r_{jk}\in \mathcal{R}$, which leads to a triplet $\left \langle c_j, r_{jk}, c_k \right \rangle$ in the directed edge set $\mathcal{E}$.

Given a scene graph $\mathcal{G}$ and its corresponding image $I$, scene graph-based image generation model aims to generate an image $\widehat{I}$ according to $\mathcal{G}$ by minimizing $D(I, \widehat{I})$, where $D(I, \widehat{I})$ measures differences between $I$ and $\widehat{I}$. A standard scene graph to image generation task can be formulated as two separate tasks: a scene graph to layout generation task which extracts object features with spatial constraints from scene graphs, and an image generation task, which generates images conditioned on the predicted object features and learned layout constraints, as shown in Fig.~\ref{fig:intro_pipeline} (left).

\begin{figure*}[!ht]
\centering
\includegraphics[width=0.95\linewidth,page=1]{figures/main_figure.pdf}
\caption{Illustrations of standard (left) and our (right) framework for scene graph to image generation. Left: Directly generating layout and image based on object features extracted from scene graph. Right: Our proposed framework with object pair-wise spatial constraints and appearance supervision respecting relationships among objects.}
\label{fig:intro_pipeline}
\end{figure*}

In this paper, we extend the traditional framework by emphasizing the influence of relationship $\mathcal{R}$ for both object layouts and object appearances generation. As shown in Fig.~\ref{fig:intro_pipeline} (right), three novel modules are proposed:
\begin{itemize}
    \item \textbf{Pair-wise Spatial Constraint Module}: a module for constraining layout generation according to the semantic information extracted from $\mathcal{E}$. 
    \item \textbf{Relation-guided Appearance Generator}: for each object $c_i$, we introduce the semantic information of its connected relationships $\{r_{j}|\left \langle c_i, r_{j}, * \right \rangle\in\mathcal{E}\}$ to influence the shape and appearance of the generated image of $c_i$.
    \item \textbf{Scene Graph Discriminator} ($D_{sg}$): a novel discriminator for strengthening the generated image $\widehat{I}$ to be relevant to the appearances of object $\mathcal{C}$, and the relationships $\mathcal{R}$ in the edge set $\mathcal{E}$.
\end{itemize}
%Moreover, we also developed a new image generator by fusing global context features, local object fine-grained features, and corresponding relation information as conditions for image generation.

\subsection{Layout Generator}
Layout generator aims to predict bounding boxes $b_i=(x_i,y_i,w_i,h_i)$ for each object $o_i$ in given scene graph $\mathcal{G}$, where $x_i,y_i,w_i,h_i$ specifies normalized coordinates of the center, width and height in ground-truth image $I$.

In previous work, the object representations are usually extracted from scene graph inputs, and then be passed to a box regression network to get the bounding box predictions $\widehat{b}_{i} = (\widehat{x}_i, \widehat{y}_i, \widehat{w}_i, \widehat{h}_i)$. The box regression network is trained by maximizing the objective:
\begin{equation}
    \mathcal{L}_{box} = -\sum_{i=1}^n \parallel b_i - \widehat{b}_i \parallel_2,
\end{equation}
which penalize the $L_2$ difference between ground-truth and predicted boxes. $n$ indicates the amount of objects.

Since there are various reasonable layouts existing, as previously stated, a scene graph to layout task requires addressing challenging one-to-many mapping. Directly regressing layout to offsets of one specific bounding box would hurt the generalization ability of the layout generator, and make the layout generator to be difficult to convergence. In order to generate reasonable layouts, we relax the constraint of bounding box offsets regression and proposed a novel spatial constraint module for ensuring the rationality of layout.

Our \textbf{Pair-wise Spatial Constraint Module} introduces two novel metrics for measuring the realistic of layouts.

\noindent\textit{1. Relative Scale Invariance}. The scale of an object is represented by the diagonal length of its bounding box. For any given $\left \langle c_j, r_{jk}, c_k\right \rangle$ triplet, the ratio between the scale of the subject and the scale of the object in different images are often roughly the same, as shown in Fig~\ref{fig:scale_dist_variance} (Left). We formulate the relative scale between the layout $b_j$ and $b_k$ as
    \begin{equation}
        s_{jk} = {\sqrt{w_j^2+h_j^2}}\Big/{\sqrt{w_k^2+h_k^2}}.
    \end{equation}
\noindent\textit{2. Relative Distance Invariance}. Similar to relative scale, relative distance target at calculating the distance between two objects in triplet normalized by the scales of two objects. The relative distance of related object pair $c_j$ and $c_k$ in realistic images is also naturally clustered to one specific value, and the distributions of relative distance for different triplets are usually with low variance, as shown in Fig~\ref{fig:scale_dist_variance} (Right). Normally, horizontal flips of images rarely alter spatial relationship distributions, we relax this constraint by using the absolute value of the horizontal coordinate difference. Most importantly, we normalize distance by the summed scales of object pairs so that the zooming effect of object depth can be canceled out. Therefore, the relative distance between the layout $b_j$ and $b_k$ can be formulated as
    \begin{equation}
    \Vec{d_{jk}} = {[|x_j-x_k|, y_j-y_k]^T}\Big/{\left(\sqrt{w_j^2+h_j^2}+\sqrt{w_k^2+h_k^2}\right)}.
    \end{equation}

%Normally, the distance between a subject and an object can be represented by the euclidean distance between their respective central points. However, since turning images upside down disturbs the common occurrence of relationships, euclidean distance is no longer reflective of such placement. For instance, ``human, holding, umbrella'' hints that the umbrella is above the person's head. However, the opposite layout arrangement that an umbrella is beneath a person almost never occurs in natural image settings. Thus, we use vector distance, the difference between object center coordinates to define the spatial distance between paired objects. Since horizontal flips of images rarely alter spatial relationship distributions, we relax this constraint by using the absolute value of the horizontal coordinate difference. Most importantly, we normalize distance by the summed scales of object pairs so that the zooming effect of object depth can be canceled out. Therefore, the relative distance between the layout $b_j$ and $b_k$ can be formulated as
%\begin{equation}
%    \Vec{d_{jk}} = {[|x_j-x_k|, y_j-y_k]^T}\Big/{\left(\sqrt{w_j^2+h_j^2}+\sqrt{w_k^2+h_k^2}\right)}.
%\end{equation}

\begin{figure}[!t]
\centering
\includegraphics[width=0.98\linewidth]{figures/figure4_v2.pdf}
\caption{Distributions of relative scale and distance variance among top-100 triplets in VG and HICO-DET datasets. Low diversity of relative scale and distance is observed, following the property of commonsense knowledge.}
\label{fig:scale_dist_variance}
\end{figure}

We have keenly observed that relationship in a semantic form comes with it an inherent spatial constraint that has not been fully explored by others. For example, the relationship ``holding'' implies that the object should be within arm's reach of the subject instead of miles away. The relationship ``walking'' indicates the relative vertical arrangement between subject and object heavily, whether it's ``man walking-on street'' or ``dog walking-on floor''. It means the relative scale and relative distance between two objects heavily depend on the relationship or interaction between these two objects. Therefore, we devise a training scheme that explicitly leverages this constraint. 

In this work, the scene graph $\mathcal{G}$ is first converted to object feature vectors $\mathcal{C}$ and relation embeddings $\mathcal{R}$, and then fed into a Graph Convolutional Network (GCN). The GCN outputs updated object level feature vector $o_i = T(c_i,\mathcal{C}_i, \mathcal{R}_i)$ aggregated with relation information, where $T$ is the graph convolutional operation, $\mathcal{C}_i$ is the set of object embeddings relevant to $c_i$, $\mathcal{R}_i$ is the set of embeddings for relations among $c_i$ and $\mathcal{C}_i$. It means the output vector $o_i$ for an object $c_i$ should depend on representations of relationships and all objects connected via graph edged jointly. After that, we apply the updated object representations for generating the layout for object $c_i$ by $\widehat{b}_i = B(o_i)$, where $B$ is an bounding box offset regression network. We construct a scale-distance objective for our proposed spatial constraint module to assist the training progress of $B$:
\begin{equation}
    \mathcal{L}_{scm}\!=\!-\sum_{\substack{0<j,k<n \\ \left \langle c_j, r_{jk}, c_k \right \rangle\in\mathcal{E}}}\parallel s_{jk} - \widehat{s}_{jk} \parallel_2\!+\!\parallel \Vec{d}_{jk} - \Vec{\widehat{d}_{jk}}\parallel_2,
\end{equation}
where $\widehat{s}_{jk}$ and $\Vec{\widehat{d}_{jk}}$ is the relative scale and relative distance between generated layouts for related object pair $c_j$ and $c_k$ respectively. $\mathcal{L}_{scm}$ is only computed on the connected object pairs in scene graph, since the relative scale and distance of two objects depend on the relationship between them, as we shown in Fig.~\ref{fig:intro_cases}. 

%During training, the relative scale and distance metrics computed from ground-truth bounding boxes are viewed as regression labels for our spatial constraint module to approximate. 
With the supervision of relative scale and distance, the box regression network learns to arrange object boxes properly for reasonable layout generation. 


\subsection{Image Generator}
Starting from the original object representations $\mathcal{C}\in\mathbb{R}^{n\times d_1}$ and initial relation embeddings $\mathcal{R}\in\mathbb{R}^{m\times d_2}$, we can compute a combined ``object-relation'' vector $v_i$ for each object $c_i$ in scene graph:
\newcommand{\concat}{\ensuremath{+\!\!\!\!+\,}}
\begin{equation}
    v_i = \Big(c_i\concat\frac{1}{|\mathcal{E}_i|}\sum_{\left \langle c_i, r_j,*\right\rangle \in \mathcal{E}_i} r_j\Big) + z_i,
\label{rel_guided}
\end{equation}
where $\concat$ indicates a vector concatenation operation, $\mathcal{E}_i\in\mathcal{E}$ is the collection of all triplet relevant to object $c_i$, $z_i$ is a $d_1+d_2$ dimensional noise vector randomly sampled from a Gaussian distribution, which aims to generate non-deterministic object features. The object and averaged relation embeddings are eventually be concatenated as inputs of our \textbf{Relation-guided Appearance Generator}, which consists of an object mask predictor $g_m$, an object appearnce feature predictor $g_o$ and a full image generator.%In this way, the semantic information about interactions between object $c_i$ and any other objects are also considered as a contributory factor for single object appearance. 

The combined vector $\{v_i\}_{i=1}^n$ will be sent simultaneously to $g_m$ and $g_o$, both of which are four-layer conv nets normalized with spectral normalization techniques~\cite{miyato2018spectral}. Through an STN block~\cite{spatial2015Jaderberg}, the two outputs for different objects will first be filled into their respective bounding box layouts. Then we obtain a set of object shape tensor and appearance tensor. By multiplying these two tensor, we can generate the final \textit{relation-guided} appearance feature tensor for all objects in scene graph as 
\begin{equation}
    a(\mathcal{G}) = \{S(\widehat{b}_i, g_m(v_i))\circ S(\widehat{b}_i, g_o(v_i))\}_{i=1}^{n},
\end{equation}
where $S$ is the STN block.

After that, our full image generator generate the image conditioned on all object appearance feature tensors $a(\mathcal{G})$ and an additional noise vector $z_{I}$. In detail, our image generator utilizes the ResNet architecture~\cite{he2016deep} consists of six ResBlocks as backbone. Consider generating a 256$\times$256 image for scene graph, a randomly generated global latent vector $z_{I}$ is a vector sampled from normal distribution. The vector is then mapped and reshaped to a 1024$\times$4$\times$4 (channels, width, and height) tensor through a fully-connected layer. Then, the tensor will be sent to the first ResBlock. Each of the six ResBlocks will upsample it's inputs bilinearly with a ratio of two. In the meantime, the channel number drops by a factor of 2 except for the third block. Block by block, we fuse object appearance tensor $\{f_{i} = G_{obj}(v_i)\}_{i=1}^n$ with the outputs of each ResBlock (global appearance tensor) using the ISLA-Norm method proposed by \cite{sun2019image}. The final generated image $\widehat{I} = \widehat{I}_t$ comes from the outputs of the last ResBlock, 
\begin{equation}
\begin{split}
    \widehat{I}_t &= R_t(\widehat{I}_{t-1},a(\mathcal{G})) \\
    \widehat{I}_1 &= R_1(z_I,a(\mathcal{G}))
\end{split}    
\end{equation}
where $R$ indicate a ResBlock equipped with ISLA-Norm module, $t$ is the amount of ResBlocks in our image generator, $\widehat{I}_i$ is output of the $i$-th ResBlock. 
%\subsubsection{Global Generator}

%As we know, both the global coarse features and local object fine-grained features are crucial as conditions for image generator. 
Our image generator takes the object appearance features with relational information and global random noise as condition, adapts scene composition, and finally generates the realistic image $\widehat{I}$ for scene graph $S$.

\subsection{Image Discriminator}

Similar to the image generator, we adapt ResNet with downsampling blocks for image discriminator. The ResNet backbone consists of a different number of downsampling ResBlocks with respect to the input image sizes. %Take our 128$\times$128 model as an example, six ResBlocks are utilized for downsampling the images. 
The image downsampled by ResBlocks goes through a linear layer, and the outputs of the linear layer are further summed channel-wise to form a scalar output as the global discriminator score $D_{img}$ to measure whether the input image is real or not, which is similar to traditional GAN based methods.

Since different relationships result in diverse appearance in the same object, we argue that the learned object feature representation reflects not just class-related object styles but also the relationship-aware appearances. Thus, we proposed a novel \textbf{Scene Graph Discriminator} $D_{sg}$ to measure whether the scene graph extracted from the generated image is associated with the given textual scene graph or not. In detail, we first extract object-level feature patches $\{p_i\}_{i=1}^n$ rerouted from the second layer of ResNet backbone, then resize these feature patches to the same size by an RoI align layer~\cite{he2017mask}. Then we introduce an object classifier $F_{obj}$, which attempts to classify the feature patches into categories. By pairing object feature tensors according to the edges of the scene graph, we send the paired object feature $p_j$ and $p_k$ to the relationship classifier $F_{rel}$, which aims to predict the type of relationship of given object feature pair. Our proposed $D_{sg}$ aims to encourage the image generator to be aware of the object categories and relationships exists in the scene graph:
\begin{equation}
    D_{sg}(I)\!=\!\frac{1}{n}\sum_{i = 0}^{n}F_{obj}(c_i|p_i)+\frac{1}{|\mathcal{E}|}\!\sum_{\substack{0<j,k<n \\ \left \langle c_j, r_{jk}, c_k \right \rangle\in\mathcal{E}}}\!F_{rel}(r_{jk}|p_j,p_k).
\end{equation}
Moreover, we introduce an object discriminator $D_{obj}$ to measure whether each object in image appears realistic based on $\{p_i\}_{i=1}^n$.

The overall objective function for training layout generator, image generator and discriminators is defined as:
\begin{equation}
    \mathcal{L} = \lambda_{1}\mathcal{L}_{box} + \lambda_{2}\mathcal{L}_{scm} +  \lambda_{3}\mathcal{L}_{obj} + \lambda_{4}\mathcal{L}_{sg} + \lambda_{5}\mathcal{L}_{img},
\end{equation}
where $\mathcal{L}_{img}$ is image adversarial loss from $D_{img}$,  $\mathcal{L}_{obj}$ is object adversarial loss from $D_{obj}$, $\mathcal{L}_{sg}$ is scene graph relevant loss from $D_{sg}$. In our experiments, we set the loss weight parameters $\lambda_{1},\lambda_{2},\lambda_{3},\lambda_{4}=1$, $\lambda_{5} = 0.1$.

