\section{Convergence Guarantees for DFO-LS} \label{sec_convergence}
In this section, we provide details of the convergence theory for the DFO-LS algorithm.
These results largely follow the arguments in \cite[Section 3]{Cartis2017a}.
% Because of the similarity of DFO-LS to DFO-GN, the arguments from \cite[Section 3]{Cartis2017a} can also be used establish the global convergence of DFO-LS; that is, the convergence of the algorithm to a stationary point from any starting point $\bx_0$.

\subsection{Accuracy of Regression Models} \label{sec_fully_linear}
In \secref{sec_main_algo}, we introduced $\Lambda$-poisedness as the key measure of the quality of the geometry of $Y_k$.
The $\Lambda$-poisedness of $Y_k$ guarantees accuracy of the regression models $\bem_k$ \eqref{eq_linear_models} and $m_k$ \eqref{eq_gn_full_model_dfo}, in the following sense \cite{Conn2009,Grapiglia2016}:

\begin{definition}[Fully linear, scalar model]
	A model $m_k\in C^1$ for a scalar function $f\in C^1$ is fully linear in $B(\bx_k,\Delta_k)$ if there exist positive constants $\kappa_{ef}$ and $\kappa_{eg}$, independent of $\bx_k$ and $\Delta_k$ such that
	\begin{align}
		|m_k(\bs) - f(\bx_k+\bs)| &\leq \kappa_{ef}\Delta_k^2, \\
		\|\grad m_k(\bs) - \grad f(\bx_k+\bs)\| &\leq \kappa_{eg}\Delta_k,
	\end{align}
	for all $\|\bs\|\leq\Delta_k$.
\end{definition}

\begin{definition}[Fully linear, vector model]
	A model $\bem_k\in C^1$ for a vector function $\br\in C^1$ is fully linear in $B(\bx_k,\Delta_k)$ if there exist positive constants $\kappa_{ef}^r$ and $\kappa_{eg}^r$, independent of $\bx_k$ and $\Delta_k$ such that
	\begin{align}
		\|\bem_k(\bs) - \br(\bx_k+\bs)\| &\leq \kappa_{ef}^r\Delta_k^2, \\
		\|J^m(\bs) - J(\bx_k+\bs)\| &\leq \kappa_{eg}^r\Delta_k,
	\end{align}
	for all $\|\bs\|\leq\Delta_k$, where $J^m$ and $J$ are the Jacobians of $\bem_k$ and $\br$ respectively.
\end{definition}

To establish the connection between $\Lambda$-poisedness of $Y_k$ and full linearity of our regression interpolation models $\bem_k$ and $m_k$, we require extra assumptions on the smoothness of the objective.

\begin{assumption} \label{ass_smoothness}
	The function $\br$ is $C^1$ and its Jacobian $J(\bx)$ is Lipschitz continuous in $\mathcal{B}$, the convex hull of $\cup_k B(\bx_k,\Delta_{max})$, with constant $L_J$. 
	We also assume that $\br(\bx)$ and $J(\bx)$ are uniformly bounded in the same region; i.e.~$\|\br(\bx)\|\leq r_{max}$ and $\|J(\bx)\| \leq J_{max}$ for all $\bx\in\mathcal{B}$.
\end{assumption}

If \assref{ass_smoothness} holds, then $\grad f$ is Lipschitz continuous in $\mathcal{B}$ with constant $L_{\grad f} \defeq r_{max}L_J + J_{max}^2$ \cite[Lemma 3.3]{Cartis2017a}.
The main result, analogous to \cite[Lemma 3.4]{Cartis2017a}, is the following.

\begin{lemma} \label{lem_fully_linear}
	Suppose \assref{ass_smoothness} holds, and $Y_k$ with $|Y_k|=p+1$ is $\Lambda$-poised in $B(\bx_k,\Delta_k)$ in the regression sense.
	Then $\bem_k$ and $m_k$ are fully linear models in $B(\bx_k,\Delta_k)$ for $\br$ and $f$ respectively, with constants
	\begin{align}
		\kappa_{ef}^r &= 2\kappa_{eg}^r, \\
		\kappa_{eg}^r &= \frac{1}{2}L_J\left(\sqrt{p}\:C+2\right), \\
		\kappa_{ef} &= \kappa_{eg} + L_{\grad f}/2 + \left(2r_{max}+\kappa_{eg}^r\Delta_{max}\right)\kappa_{eg}^r + (\kappa_{eg}^r\Delta_{max} + J_{max})^2, \\
		\kappa_{eg} &= L_{\grad f} + 2 J_{max}\kappa_{eg}^r \Delta_{max} + 2\kappa_{eg}^r r_{max} + 2(\kappa_{eg}^r\Delta_{max}+J_{max})^2,
	\end{align}
	where $C=\bigO(\Lambda)$.
\end{lemma}
\begin{proof}
	%See \appref{sec_fully_linear_proof}.
This result extends the proof\footnote{\:This argument is not in the original text, but given in the errata (\url{http://www.mat.uc.pt/~lnv/idfo/}).} of \cite[Theorem 2.13]{Conn2009} to vector-valued functions, and their composition in a least-squares objective, in the style of \cite[Lemma 3.4]{Cartis2017a}.

Using $\by_0=\bx_k$, we may write
\be W_k \defeq \begin{bmatrix}1 & (\by_0-\bx_k)^{\top} \\ \vdots & \vdots \\ 1 & (\by_p-\bx_k)^{\top}\end{bmatrix} = \begin{bmatrix}1 & \b{0}^{\top} \\ \bee & L_k \end{bmatrix},\ee
where $\bee\in\R^p$ is the vector of ones and $L_k\in\R^{p\times n}$ has rows $(\by_t-\bx_k)^{\top}$ for $t=1,\ldots,p$.

We will also use scaled versions of these matrices:
\be \hat{W}_k \defeq \begin{bmatrix}1 & \b{0}^{\top} \\ \bee & \hat{L}_k \end{bmatrix}, \quad \text{where} \quad \hat{L}_k \defeq \begin{bmatrix}(\by_1-\bx_k)^{\top}/\Delta_k \\ \vdots \\ (\by_p-\bx_k)^{\top}/\Delta_k \end{bmatrix} = L/\Delta_k. \ee
Equivalently, we have
\be \hat{W}_k = W_k D_k, \quad \text{where} \quad D_k \defeq \begin{bmatrix}1 & \b{0}^{\top} \\ \b{0} & \frac{1}{\Delta_k}I_{n\times n}\end{bmatrix}. \label{eq_W_scaling} \ee
To begin, we recall that if $J(\bx)$ is continuous with Lipschitz constant $L_J$, then \cite[Appendix A]{Nocedal2006}
\be \|\br(\by) - \br(\bx_k) - J(\bx_k)(\by-\bx_k)\| \leq \frac{1}{2}L_J \|\by-\bx_k\|^2. \label{eq_lipschitz_bd} \ee
Our overdetermined interpolation system \eqref{eq_linear_interp_system} can be rewritten in matrix form as
\be W_k \begin{bmatrix}\br_k^{\top} \\ J_k^{\top}\end{bmatrix} = \begin{bmatrix}\br(\bx_k)^{\top} \\ \br(\by_1)^{\top} \\ \vdots \\ \br(\by_p)^{\top} \end{bmatrix}, \quad \text{and so} \quad \begin{bmatrix}\br_k^{\top} \\ J_k^{\top}\end{bmatrix} = W_k^{\dagger}\begin{bmatrix}\br(\bx_k)^{\top} \\ \br(\by_1)^{\top} \\ \vdots \\ \br(\by_p)^{\top} \end{bmatrix}. \label{eq_interp_system_matrix} \ee
Separately, we compute
\be W_k \begin{bmatrix}\br(\bx_k)^{\top} \\ J(\bx_k)^{\top}\end{bmatrix} - \begin{bmatrix}\br(\bx_k)^{\top} \\ \br(\by_1)^{\top} \\ \vdots \\ \br(\by_p)^{\top} \end{bmatrix} = \begin{bmatrix}\br(\bx_k)^{\top} \\ [\br(\bx_k)+J(\bx_k)(\by_1-\bx_k)]^{\top} \\ \vdots \\ [\br(\bx_k) + J(\bx_k)(\by_p-\bx_k)]^{\top} \end{bmatrix} - \begin{bmatrix}\br(\bx_k)^{\top} \\ \br(\by_1)^{\top} \\ \vdots \\ \br(\by_p)^{\top} \end{bmatrix} =: E. \label{eq_E_defn} \ee
Combining \eqref{eq_interp_system_matrix} and \eqref{eq_E_defn}, we get
\be \begin{bmatrix}[\br(\bx_k)-\br_k]^{\top} \\ [J(\bx_k)-J_k]^{\top}\end{bmatrix} = W_k^{\dagger} E, \ee
and so from \eqref{eq_W_scaling}, since $D_k$ is invertible, we have $\hat{W}_k^{\dagger}=D_k ^{-1} W_k^{\dagger}$ and hence conclude
\be \begin{bmatrix}[\br(\bx_k)-\br_k]^{\top} \\ \Delta_k [J(\bx_k)-J_k]^{\top}\end{bmatrix} = D_k^{-1} \begin{bmatrix}[\br(\bx_k)-\br_k]^{\top} \\ [J(\bx_k)-J_k]^{\top}\end{bmatrix} = \hat{W}_k^{\dagger} E. \ee
Since the first row of $E\in \R^{(p+1)\times m}$ is zero, and the norms of all other rows are bounded by \eqref{eq_lipschitz_bd}, we have
\be \|E\| \leq \|E\|_F = \left(0 + \sum_{t=1}^{p}\|\br(\bx_k)+J(\bx_k)(\by_t-\bx_k) - \br(\by_t)\|^2\right)^{1/2} \leq \frac{1}{2}L_J \sqrt{p}\Delta_k^2. \ee
This gives us the error bounds
\begin{align}
	\|\br(\bx_k)-\br_k\| &\leq \left\|\begin{bmatrix}[\br(\bx_k)-\br_k]^{\top} \\ \Delta_k [J(\bx_k)-J_k]^{\top}\end{bmatrix}\right\| \leq \frac{1}{2}L_J\sqrt{p}\|\hat{W}_k^{\dagger}\|\Delta_k^2, \label{eq_rk_error} \\
	\|J(\bx_k)-J_k\| &\leq \Delta_k^{-1} \left\|\begin{bmatrix}[\br(\bx_k)-\br_k]^{\top} \\ \Delta_k [J(\bx_k)-J_k]^{\top}\end{bmatrix}\right\| \leq \frac{1}{2}L_J\sqrt{p}\|\hat{W}_k^{\dagger}\|\Delta_k.
\end{align}
Thus we conclude that for any $\by\in B(\bx_k,\Delta_k)$
\be \|J_k-J(\by)\| \leq \|J_k-J(\bx_k)\| + \|J(\by)-J(\bx_k)\| \leq L_J \left(1 + \frac{1}{2}\sqrt{p}\|\hat{W}_k^{\dagger}\|\right)\Delta_k.  \label{eq_fully_linear_vector_g} \ee
For convenience, define $\kappa_{eg}^r \defeq L_J \left(1 + \frac{1}{2}\sqrt{p}\|\hat{W}_k^{\dagger}\|\right)$.
Next, we compute
\begin{align}
	\|\bem_k(\by-\bx_k) - \br(\by)\| &= \|\br(\by) - \br_k - J_k(\by-\bx_k)\|, \\
	&\leq \|\br(\bx_k)-\br_k\| + \|\br(\by) - \br(\bx_k) - J(\bx_k)(\by-\bx_k)\| \nonumber \\
	&\qquad\qquad + \|J(\bx_k)-J_k\|\cdot\|\by-\bx_k\|, \\
	&\leq \left(\frac{1}{2}L_J\sqrt{p}\|\hat{W}_k^{\dagger}\| + \frac{L_J}{2} + \kappa_{eg}^r\right)\Delta_k^2,
\end{align}
where we use \eqref{eq_fully_linear_vector_g} and \eqref{eq_lipschitz_bd}.
Hence $\bem_k$ is a fully linear model for $\br$ with constants $\kappa_{eg}^r$ defined above and $\kappa_{ef}^r = 2\kappa_{eg}^r$.

Since $\bem_k$ is fully linear, \eqref{eq_fully_linear_vector_g} gives us
$\|J_k\| \leq \|J(\bx_k) - J_k\| + \|J(\bx_k)\| \leq \kappa_{eg}^r\Delta_{max} + J_{max}$, 
so $\|J_k\|$ is uniformly bounded for all $k$. Since $H_k = 2 J_k^{\top}J_k$, we get that $\|H_k\| = 2 \|J_k\|^2$ is uniformly bounded for all $k$. 
Similarly, using \eqref{eq_rk_error}, we have the bounds $\|\br(\bx_k)-\br_k\| \leq \kappa_{eg}^r\Delta_k^2$ and $\|\br_k\| \leq r_{max}+\kappa_{eg}^r\Delta_{max}^2$.

To prove full linearity of $m_k$, we begin by computing
\begin{align}
	\|\grad m_k(\by-\bx_k) - \grad f(\by)\| &= \|\grad f(\by) - 2 J_k^{\top}\br_k - 2 J_k^{\top}J_k(\by-\bx_k)\|, \\
	&\leq \|\grad f(\by) - \grad f(\bx_k)\| + \|2J(\bx_k)^{\top}(\br(\bx_k)-\br_k)\| \nonumber \\ 
	&\qquad\qquad + \|2(J(\bx_k)-J_k)^{\top}\br_k\| + \|2 J_k^{\top}J_k\|\cdot\|\by-\bx_k\|, \\
	&\leq L_{\grad f} \Delta_k + 2J_{max}\kappa_{eg}^r\Delta_k^2 \nonumber \\
	&\qquad\qquad + 2\kappa_{eg}^r r_{max}\Delta_k + 2\left(\kappa_{eg}^r\Delta_{max}+J_{max}\right)^2\Delta_k, \\
	&\leq \kappa_{eg}\Delta_k,
\end{align}
where $\kappa_{eg}\defeq L_{\grad f} + 2 J_{max}\kappa_{eg}^r \Delta_{max} + 2\kappa_{eg}^r r_{max} + 2(\kappa_{eg}^r\Delta_{max}+J_{max})^2$, as required.
Then, we use \eqref{eq_lipschitz_bd} and the above to get
\begin{align}
	|m_k(\by-\bx_k) - f(\by)| &= \left|f(\by) - \|\br_k\|^2 - \bg_k^{\top}(\by-\bx_k) - \frac{1}{2}(\by-\bx_k)^{\top}H_k(\by-\bx_k)\right|, \\
	&\leq \left|f(\by) - f(\bx_k) - \grad f(\bx_k)^{\top}(\by-\bx_k)\right| + \|\br(\bx_k)-\br_k\|\left(\|\br(\bx_k)\|+\|\br_k\|\right) \nonumber \\ 
	&\qquad\qquad + \left\|\grad f(\bx_k) - \bg_k - \frac{1}{2}H_k(\by-\bx_k)\right\|\cdot\|\by-\bx_k\|, \\
	&\leq \frac{1}{2}L_{\grad f} \Delta_k^2 + \kappa_{eg}^r \Delta_k^2 \left(2r_{max}+\kappa_{eg}^r\Delta_{max}\right) \nonumber \\
	&\qquad\qquad + \left[\|\grad f(\bx_k) - \grad m_k(\bx_k)\| + \frac{1}{2}\|H_k\|\cdot\|\by-\bx_k\|\right]\cdot \Delta_k, \\
	&\leq \frac{1}{2}L_{\grad f} \Delta_k^2 + \kappa_{eg}^r \left(2r_{max}+\kappa_{eg}^r\Delta_{max}\right) \Delta_k^2 \nonumber \\
	&\qquad\qquad + \left[\kappa_{eg}\Delta_k + (\kappa_{eg}^r\Delta_{max}+J_{max})^2\Delta_k\right]\Delta_k, \\
	&\leq \kappa_{ef}\Delta_k^2,
\end{align}
where $\kappa_{ef} \defeq \kappa_{eg} + L_{\grad f}/2 + \kappa_{eg}^r \left(2r_{max}+\kappa_{eg}^r\Delta_{max}\right) + (\kappa_{eg}^r\Delta_{max} + J_{max})^2$.
Lastly, we have $C\defeq\|\hat{W}_k^{\dagger}\|\leq\sqrt{p+1}\:\Lambda=\bigO(\Lambda)$ from \cite[Theorem 2.9]{Conn2008}.%\smartqed
\end{proof}

For our convergence theory to hold, we need to be more specific about the geometry of $Y_k$ being `good' in \algref{alg_dfols}; for the purposes of convergence we take `good' to mean `$Y_k$ is $\Lambda$-poised'.
Note that in the $p=n$ case of exact interpolation, there are algorithms for changing $Y_k$ to make it $\Lambda$-poised.
For the regression case of $p>n$, it suffices to make a subset of $n+1$ points in $Y_k$ $\Lambda$-poised --- see \cite[Chapter 6]{Conn2009} for a discussion of these issues. The case of reduced initialization ($p<n$) is addressed at the end of next section.

\subsection{Global Convergence and Complexity}
To ensure global convergence of DFO-LS, we need to add one more phase in \algref{alg_dfols}.
This phase, known as the `criticality phase', is called when the interpolated model constructed in line \ref{ln_loop} has $\|\bg_k\|\leq\epsilon_C$.
In this situation, our model gradient is small, so we impose two requirements: shrink $\Delta_k$ to be of the same magnitude as $\|\bg_k\|$ (specifically, we achieve $\Delta_k\leq \mu\|\bg_k\|$), and ensure $m_k$ is fully linear.
Details of this phase can be found in \cite[Appendix B]{Cartis2017a}.
A version of DFO-LS including the criticality phase is given in \algref{alg_dfols_theory}.
We consider this version of DFO-LS only for case of noiseless objectives (so \texttt{NOISY=FALSE}), and where we do not use the reduced initialization cost (i.e.~$p_{init}=p$).

\begin{algorithm}
	\small{
	\begin{algorithmic}[1]
		\Require Starting point $\bx_0\in\R^n$, initial trust region radius $\Delta_0^{init}>0$ and interpolation set size $p\geq n$. 
		\Statex Parameters from \algref{alg_dfols} are the same, except $p_{init}=p$ and \texttt{NOISY=FALSE}.
		Additional parameters are criticality threshold $\epsilon_C>0$, criticality scaling $\mu>0$ and poisedness constant $\Lambda\geq1$. We also require $\gamma_S < 2c_1/(1+\sqrt{1+2c_1})$, with $c_1$ from \assref{ass_cauchy_decrease}.
		\State Build an initial interpolation set $Y_0\subset B(\bx_0,\Delta_0^{init})$ of size $p_{init}+1=p+1$, with $\bx_0\in Y_0$. Set $\rho_0^{init}=\Delta_0^{init}$.
		\For{$k=0,1,2,\ldots$} 
			\State Given $\bx_k$ and $Y_k$, solve the interpolation problem \eqref{eq_linear_interp_system} and form $m_k^{init}$ \eqref{eq_gn_full_model_dfo}.
			\If{$\|\bg_k^{init}\| \leq \epsilon_C$}
				\State \underline{Criticality Phase}: using \cite[Algorithm 2]{Cartis2017a}, modify $Y_k$ and find $\Delta_k\leq\Delta_k^{init}$ such that $Y_k$ is $\Lambda$-poised in $B(\bx_k,\Delta_k)$ and $\Delta_k\leq\mu\|\bg_k\|$, where $\bg_k$ is the gradient of the new $m_k$. Set $\rho_k= \min(\rho_k^{init}, \Delta_k)$.
			\Else
				\State Set $m_k=m_k^{init}$, $\Delta_k=\Delta_k^{init}$ and $\rho_k=\rho_k^{init}$.
			\EndIf
			\State Follow lines \ref{ln_trs} to \ref{ln_loop_end} of \algref{alg_dfols} to determine $\bx_{k+1}$, $\Delta_{k+1}^{init}$ and $\rho_{k+1}^{init}$, updating $Y_k$ as needed. 
			All references to `improving the geometry of $Y_k$' must be changed to `make $Y_k$ $\Lambda$-poised in $B(\bx_{k+1},\Delta_{k+1}^{init})$'.
			Similarly, checking `geometry of $Y_k$ is good' must be changed to `$Y_k$ is $\Lambda$-poised in $B(\bx_k,\Delta_k)$'.
			We do not terminate if $\rho_k \leq \rho_{end}$, or if objective decrease is too slow.
% 			The calculation of $\bs_k$ in line \ref{ln_trs} must satisfy \assref{ass_cauchy_decrease}.
		\EndFor
	\end{algorithmic}
	} % end font size
	\caption{DFO-LS with criticality phase.}
	\label{alg_dfols_theory}
\end{algorithm}

To guarantee convergence of our algorithm, we want our approximate solution to the trust region subproblem \eqref{eq_tr_subproblem} to provide a reasonable decrease in $m_k$, and so we require the following minimal assumption.

\begin{assumption} \label{ass_cauchy_decrease}
	The calculated step $\bs_k$ \eqref{eq_tr_subproblem} in line \ref{ln_trs} of \algref{alg_dfols} satisfies the `Cauchy decrease' condition
	\be m_k(\b{0}) - m_k(\bs_k) \geq c_1 \|\bg_k\| \min\left(\Delta_k, \frac{\|\bg_k\|}{\max(\|H_k\|, 1)}\right), \ee
	for some $c_1\in[1/2, 1]$ independent of $k$.
\end{assumption}

This assumption is easy to achieve, for instance by one iteration of steepest descent with exact linesearch (achieving $c_1=1/2$).
Lastly, we require one more assumption, which is very common for trust region methods.

\begin{assumption} \label{ass_bdd_hess}
	We assume that $\|H_k\|\leq\kappa_H$ for all $k$, for some $\kappa_H\geq1$.
\end{assumption}

We can now state the convergence result for DFO-LS; aside from the details in \secref{sec_fully_linear} the details of the proof are identical to \cite{Cartis2017a}.

\begin{theorem} \label{thm_lim}
	Suppose Assumptions \ref{ass_cauchy_decrease}, \ref{ass_smoothness} and \ref{ass_bdd_hess} hold.
	Then \algref{alg_dfols_theory} produces iterates $\bx_k$ such that $\lim_{k\to\infty}\Delta_k=0$ and
	$\lim_{k\to\infty}\|\grad f(\bx_k)\|=0$.
\end{theorem}

Again following the details from \cite{Cartis2017a}, we can also bound the number of iterations and objective evaluations required to achieve $\|\grad f(\bx_k)\|\leq \epsilon$. 

\begin{theorem} \label{thm_iters_bound}
	Suppose Assumptions \ref{ass_cauchy_decrease}, \ref{ass_smoothness} and \ref{ass_bdd_hess} hold, and that the criticality threshold $\epsilon_C \geq c_3\epsilon$ for some constant $c_3>0$.
	Then the number of iterations  $i_{\epsilon}$ (i.e.~the number of times a model $m_k$ \eqref{eq_gn_full_model_dfo} is built) needed by \algref{alg_dfols_theory} until $\|\grad f(\bx_{i_{\epsilon}+1})\|< \epsilon$ is at most
	\begin{align}
		\left\lfloor\frac{4f(\bx_0)}{\eta_1 c_1}\left(1 + \frac{\log \overline{\gamma}_{inc}}{|\log \alpha_3|}\right)\max\left(\kappa_H c_4^{-2}\epsilon^{-2}, c_4^{-1}c_5^{-1}\epsilon^{-2}, c_4^{-1}\Delta_0^{-1}\epsilon^{-1}\right)\right. \nonumber \\
\left.\quad + \frac{4}{|\log \alpha_3|}\max\left(0,\log \left(\Delta_0 c_5^{-1} \epsilon^{-1}\right)\right)\right\rfloor \label{eq_num_iters}
	\end{align}
	where $c_4 \defeq \min\left(c_3, (1 + \kappa_{eg}\mu)^{-1}\right)$, where $\mu$ is the criticality phase threshold on $\|\bg_k\|$, and
	\be c_5 \defeq \min\left(\frac{\omega_C}{\kappa_{eg}+1/\mu}, \frac{\alpha_1 c_4}{\kappa_H}, \alpha_1\left(\kappa_{eg} + \frac{2\kappa_{ef}}{c_1(1-\eta_2)}\right)^{-1}\right). \ee
\end{theorem}

For succinctness, we can look at the complexity bounds to leading order in $\epsilon$. 

\begin{corollary} \label{cor_complexity}
	Suppose the assumptions of \thmref{thm_iters_bound} hold.
	Then for $\epsilon\in(0,1]$, the number of iterations  $i_{\epsilon}$ needed by \algref{alg_dfols_theory} until $\|\grad f(\bx_{i_{\epsilon}+1})\|< \epsilon$ is at most $\bigO(\kappa_H \kappa_d^2 \epsilon^{-2})$, and the number of objective evaluations until $i_{\epsilon}$ is at most $\bigO(\kappa_H \kappa_d^2 p \epsilon^{-2})$, where $\kappa_d\defeq\max(\kappa_{ef},\kappa_{eg})=\bigO(p L_J^2)$.
\end{corollary}

If the reduced initialization phase with $p<n$ is appended at the start of \algref{alg_dfols_theory}, Theorem \ref{thm_lim} continues to hold, and the complexity bounds in Theorem \ref{thm_iters_bound} 
and Corollary \ref{cor_complexity} for the resulting algorithm increase by $n$ iterations and function evaluations. This is due to the growing set of directions until full-dimensionality that is being generated in the early phase, when no points get removed; the geometry of this set is automatically adjusted by the algorithm, if needed. 



%\section{Proof of \lemref{lem_fully_linear}} \label{sec_fully_linear_proof}


\section{General Features of DFO-LS} \label{sec_general_features_appendix}
In this section, we provide more details on the general features of DFO-LS summarized in \secref{sec_implementation_general}.
% The majority of these general features are inherited from DFO-GN \cite{Cartis2017a}, but DFO-LS includes variable scaling, two new termination criteria, different default trust region parameters for noisy problems, and a slightly different approach for determining the initial set $Y_0$.

\paragraph{Geometry-Improving Steps}
The goal of the geometry-improving steps in \algref{alg_dfols} is to improve the quality of the model $m_k$; specifically, we wish to make $Y_k$ $\Lambda$-poised in $B(\bx_k,\Delta_k)$, so $m_k$ is a fully linear model for $f$ in the trust region.
However, as mentioned above, guaranteeing the $\Lambda$-poisedness of $Y_k$ if $|Y_k|>n+1$ is not straightforward.
If $|Y_k|=n+1$, then we can achieve $\Lambda$-poised via the iteration \cite[Algorithm 6.3]{Conn2009}
\begin{enumerate}
	\item Select the point $\by_t\in Y_k$ ($\by_t\neq\bx_k$) for which $\max_{\by\in B(\bx_k,\Delta_k)} |\Lambda_t(\by)|$ is maximized;
	\item Replace $\by_t$ in $Y_k$ with $\by^+$, where 
	\be \by^+ = \argmax_{\by\in B(\bx_k,\Delta_k)} |\Lambda_t(\by)|, \label{eq_geom_improvement_2} \ee
	and repeat until $Y_k$ is $\Lambda$-poised.
\end{enumerate}
As in DFO-GN, in practice we perform a simplified geometry-improving phase: we simply choose $\by_t\in Y_k$ to be the point furthest from $\bx_k$, and replace it with $\by^+$ as defined by \eqref{eq_geom_improvement_2}.
We do not repeat this process; only one point is moved per call of the geometry-improving phase.

Similarly, we use a simplified test to determine if the geometry of $Y_k$ needs improving at all.
In theory, we need to check if $Y_k$ is $\Lambda$-poised.
Instead, we say that the geometry of $Y_k$ needs improving if $\max_t \|\by_t-\bx_k\| > \epsilon$, for some threshold $\epsilon$, usually a constant multiple of $\Delta_k$ or $\rho_k$.

\paragraph{Model Updating}
In \algref{alg_dfols}, we only add $\bx_k+\bs_k$ to $Y_k$ in successful steps.
However in practice, like in DFO-GN, we always incorporate new information when it becomes available, and so we update $Y_{k+1}=Y_k\cup\{\bx_k+\bs_k\}\setminus\{\by_t\}$ for some $\by_t\neq\bx_{k+1}$ at every iteration, successful or otherwise.
Similarly, we always choose to centre our trust region at the best value found so far, so we ensure $\bx_k=\argmin_{\by_t\in Y_k} f(\by_t)$ at every iteration --- this optimal point (so far) can come from a trust region step, or even from a geometry-improving phase.

Given a point to add to $Y_k$ (to form $Y_{k+1}$), we use the method from DFO-GN for determining which point it should replace.
This method uses a criterion which chooses to remove points which are far from $\bx_k$ and for which the replacement would most improve the geometry of $Y_{k+1}$.

\paragraph{Inclusion of Bound Constraints and Variable Scaling}
The implementation of DFO-LS solves problems with optional bound constraints.
That is, it solves \eqref{eq_ls_definition} subject to $\b{a} \leq \bx \leq \b{b}$.
The only changes to \algref{alg_dfols} required for this are in the calculation of the trust region step \eqref{eq_tr_subproblem} and geometry-improving step \eqref{eq_geom_improvement_2}, which now also have bound constraints.

For the calculation of a trust region step subject to bound constraints, we use the routine \texttt{TRSBOX} from BOBYQA \cite{Powell2009}, as modified by Zhang et al.~in DFBOLS \cite{Zhang2010}.
Geometry-improving steps with bound constraints are calculated using \cite[Algorithm 3]{Cartis2017a}.

Because bound constraints can often provide information about the natural scaling of a problem, DFO-LS allows the optional internal scaling of variables based on the bound constraints, to reduce the likelihood of ill-conditioning.
If this is used, we internally shift and scale the inputs so that the new feasible region is $\bx\in[0,1]^n$.

\paragraph{Termination Criteria}
There are four ways in which DFO-LS can terminate.
The first three are inherited from DFO-GN:
\begin{itemize}
	\item Small objective value: since we have the lower bound $f\geq 0$ always, we allow termination when $f(\bx_k) \leq \max\{\epsilon_{abs}, \epsilon_{rel}f(\bx_0)\}$, for user-specified parameters $\epsilon_{abs}$ and $\epsilon_{rel}$.
	Having this feature is especially useful for DFO solvers, when often just achieving some desired decrease in the objective is the goal, rather than solving to full optimality (e.g.~when function evaluations are expensive);
	\item Small trust region: we know that $\rho_k\to 0$ as $k\to\infty$ \cite[Lemma 3.11]{Cartis2017a}, so we terminate when $\rho_k \leq \rho_{end}$; and
	\item Computational budget: we terminate after a given number of evaluations of the objective.
\end{itemize}
The last two of these are designed to cause termination after a sufficient number of unsuccessful steps.
The first criteria is triggered by successful steps, but is likely to be triggered only for zero-residual problems.

To ensure a timely termination based on successful steps, we introduce an extra criterion, similar to the ``$f_i^{*\prime}$ test'' of Larson and Wild \cite{Larson2013}.
We define a successful iteration as `slow' if the last $K$ successful iterations have produced an average reduction in $\log(f(\bx_k))$ below a given threshold.
That is, if $\{k_i : i\in\N\}$ are the successful iterations, then iteration $k_i$ is `slow' if
\be \frac{\log(f(\bx_{k_{(i-K)}})) - \log(f(\bx_{k_i}))}{K} < \epsilon, \label{eq_slow_termination_defn} \ee
for some value $\epsilon>0$.
Note that since we are only considering successful iterations, $f(\bx_{k_i})$ will be the best objective value found up to iteration $k_i$.
Our termination condition is then: quit after successful iteration $k_i$ if $\{k_{i-N+1},\ldots,k_{i-1},k_i\}$ were all `slow', for some $N\in\N$.

Lastly, DFO-LS also includes an optional noise-aware termination condition.
Specifically, we terminate if all function values $f(\by_t)$ are within some user-provided `noise level' of $f(\bx_k)$. That is, for all $t=1,\ldots,p$, either
\be |f(\by_t) - f(\bx_k)| \leq \mathrm{const}\cdot \frac{\epsilon}{\sqrt{N_t}} \qquad \text{or} \qquad \left|\frac{f(\by_t)}{f(\bx_k)}\right| \leq \mathrm{const}\cdot \frac{\epsilon}{\sqrt{N_t}}, \label{eq_termination_noise} \ee
where $N_t$ is the number of samples used to estimate the value $f(\by_t)$; see \secref{sec_restarts_description} for details.
Which of these criteria is used depends on whether the user has specified $\epsilon$ as an additive or multiplicative noise level in the evaluation of $f$, and the value of `const' is also user-provided (default is 1).
% In the numerical results in the following sections, we only terminate on this condition when we are using multiple restarts \alert{[check wording]}.


\paragraph{Default Parameters for Noisy Problems}
One of the main problem types that DFO-LS is designed to solve is where objective evaluations are noisy.
In this situation, the set of default parameters --- which are designed for smooth objectives --- are not necessarily good choices.

The most notable examples of this are the parameters which govern decreases of $\Delta_k$ and $\rho_k$, namely $\gamma_{dec}$, $\alpha_1$ and $\alpha_2$ (default values $0.5$, $0.1$ and $0.5$ respectively).
When we have noisy evaluations, it is common to get unsuccessful iterations even when a step $\bs_k$ is useful, because the noise in the objective evaluation leads to inaccuracies in the calculated $r_k$ \eqref{eq_tr_ratio}.
This then leads to unnecessary reductions in the trust region radius, causing the algorithm to progress more slowly, and potentially terminate too early.

In DFO-LS, we allow the user to specify if their objective evaluation is noisy, and consequently modify the default values for several algorithm parameters.
Note that the user can choose to override any parameter value by specifying it directly, even if the default has been modified.
For example, the `noisy problem' default values of $\gamma_{dec}$, $\alpha_1$ and $\alpha_2$ are $0.98$, $0.9$ and $0.95$ respectively.

\paragraph{Other Differences}
There are other small differences between the implementation of DFO-LS and \algref{alg_dfols}, which are inherited from DFO-GN; a list of these may be found in \cite[Section 4.4]{Cartis2017a}.

We also change the default method for constructing the initial set $Y_0$.
In DFO-GN, like DFBOLS \cite{Zhang2010} and BOBYQA \cite{Powell2009}, the initial set is typically taken to be $\bx_0\pm \Delta_0\bee_t$ for coordinate vectors $\bee_t$ (adjusted when for bound constraints and for more than $2n+1$ interpolation points).
In DFO-LS, the default mechanism is to use $\bx_0\pm\Delta_0\b{q}_t$ for random orthonormal vectors $\b{q}_t$ (again, adjusted in the case of bound constraints or $p>2n$).



% \section{Comparison of Performance Measures} \label{sec_performance_measures}
% In \secref{sec_testing_methodology}, we defined two performance measures, `noisy $\t{f}$' and `true $f$', based on the criteria \eqref{eq_solved_threshold} for marking a problem as `solved'.
% Here, we compare these measures.
% 
% As a simple model for noise\footnote{\:This model applies, for instance, to all the noise models in \secref{sec_test_problems}.}, we assume the noisy objective $\t{f}$ is a monotone affine transformation of the smooth objective plus unbiased additive noise:
% \be \t{f}(\bx) \defeq \alpha f(\bx) + \beta + \sigma(\bx)\epsilon, \ee
% for constants $\alpha>0$ and $\beta\in\R$.
% The stochastic noise $\epsilon$ has zero mean, and $\sigma(\bx)$ is chosen so that $\epsilon$ usually takes values of approximately unit size; that is, $|\epsilon|\sim\bigO(1)$.
% By Chebyshev's inequality, we can achieve this scaling of $\epsilon$ for a wide class of noise distributions by choosing $\sigma(\bx)$ to be the standard deviation of the noise.
% 
% Now suppose a solver has reached at point $\bx_k$, which corresponds to `true $f$' accuracy of (exactly) $\tau$ and `noisy $\t{f}$' accuracy $\t{\tau}$.
% That is,
% \be f(\bx_k) = f(\bx^*) + \tau(f(\bx_0)-f(\bx^*)), \quad \text{and} \quad \t{f}(\bx_k) = \mathbb{E}[\t{f}(\bx^*) + \t{\tau}(\t{f}(\bx_0)-\t{f}(\bx^*))], \ee
% Letting $\epsilon_k$ be the realization of $\epsilon$ for the given value of $\t{f}(\bx_k)$, we combine these two expressions to get
% % \be \t{\tau} = \tau + \frac{\sigma(\bx_k)}{\alpha(f(\bx_0)-f(\bx^*))}\epsilon_k = \tau + \frac{\sigma(\bx_k)}{\mathbb{E}\left[\t{f}(\bx_0)-\t{f}(\bx^*)\right]}\epsilon_k. \ee
% \be \t{\tau} = \tau + \frac{\sigma(\bx_k)}{\mathbb{E}\left[\t{f}(\bx_0)-\t{f}(\bx^*)\right]}\epsilon_k. \ee
% Since $|\epsilon_k|\sim\bigO(1)$, we can expect $\t{\tau}$ and $\tau$ to be similar whenever
% \be \tau, \: \t{\tau} \:\gg\: \frac{\sigma(\bx_k)}{\mathbb{E}\left[\t{f}(\bx_0)-\t{f}(\bx^*)\right]}. \label{eq_tau_thresh} \ee
% To approximate this threshold, we replace $\sigma(\bx_k)$ with $\sigma(\bx^*)$, so it depends only on the problem and noise model.
% 
% As noted in \secref{sec_testing_methodology}, we typically find that for a given solver, its performance measured using `noisy $\t{f}$' is better than its performance measured using `true $f$'.
% Although both measures provide useful information, this difference is because there are a number of problems where \eqref{eq_tau_thresh} is not satisfied at the accuracy levels we are using.
% Hence, in the main text, we show numerical results for a given $\tau$, but if \eqref{eq_tau_thresh} is not satisfied for a given problem (and noise model), we use a larger value; i.e.~look for less accurate solutions.
% Specifically, we take the larger value to be
% \be  \tau_{crit}(p) \defeq 10^{\lceil \log_{10}\hat{\tau}(p)\rceil}, \qquad \text{where} \qquad \hat{\tau}(p) \defeq \frac{\mathrm{StDev}\left[\t{f}(\bx^*)\right]}{\mathbb{E}\left[\t{f}(\bx_0)-\t{f}(\bx^*)\right]}. \label{eq_tau_thresh_used} \ee
% That is, we calculate $\hat{\tau}(p)$ and round up to the next largest integer power of 10.

\section{Comparison of Sample Averaging and Regression} \label{sec_regression_v_avg}
There are two places in \algref{alg_dfols} where noise in objective evaluations can have an impact: the construction of the model \eqref{eq_linear_models}, and the measurement of objective decrease \eqref{eq_tr_ratio}. 
We show below that the errors in model construction due to noise are likely comparable when using either sample averaging or regression.
However, for a fixed level of noise, sample averaging will produce a better estimate of objective decrease (compare \cite[Lemma 9.1]{Nocedal2006}, for instance).
Thus, overall, we would expect sample averaging to perform somewhat better than regression, when considering overall robustness. 
Since sample averaging may use more objective evaluations per iteration, this may not be the case when the computational budget is limited.

\paragraph{Error Bounds on Model Estimation}
Here, we give a short argument that sample averaging and regression models produce comparably good models.
For simplicity, suppose we wish to construct a model for a linear function $f(\bx)=c+\b{g}^{\top}\bx$ from noisy evaluations $\t{f}(\bx)=f(\bx)+\epsilon$, where $\epsilon\sim N(0,\sigma^2)$ is i.i.d.~stochastic noise.

If we perform sample averaging using $N$ samples at a given interpolation point $\by_t$, we get an unbiased estimate for $f$ with smaller variance:
\be \t{f}_N(\by_t) = \frac{1}{N}\sum_{i=1}^{N}(f(\by_t) + \epsilon_i) \sim N\left(f(\by_t), \frac{\sigma^2}{N}\right). \ee
Now suppose we construct a regression model as per \eqref{eq_interp_conditions} using points $Y\defeq \{\by_0,\ldots,\by_p\}$ for some $p\geq n$.
We assume that $Y$ is \emph{strongly} $\Lambda$-poised \cite[Definition 4.10]{Conn2009} in $B(\by_0,\Delta)$, which is a stronger condition\footnote{\:It can be achieved if, for instance, $Y$ is formed by concatenating several sets of size $n+1$ which are all $\Lambda$-poised in the \emph{interpolation} sense \cite[Definition 3.6]{Conn2009}.} than \defref{def_poised}, but better suited to comparing the geometry of sets with different sizes $p$.

Under these conditions, the Gauss-Markov Theorem implies that the regression model from \eqref{eq_interp_conditions} gives an optimal unbiased estimator $(\t{c}, \t{\bg})$ for $(c, \b{g})$ with error (co)variance 
\be \mathbb{E}\left[\left(\begin{bmatrix}\t{c} \\ \t{\bg}\end{bmatrix} - \begin{bmatrix}c \\ \bg\end{bmatrix}\right) \left(\begin{bmatrix}\t{c} \\ \t{\bg}\end{bmatrix} - \begin{bmatrix}c \\ \bg\end{bmatrix}\right)^{\top}\right] = \frac{\sigma^2}{N}(W_k^{\top}W_k)^{-1}. \ee
By shifting $Y$ to $\hat{Y}\defeq\{(\by_t-\by_0)/\Delta : t=0,\ldots,p\}$ as in the proof of Lemma \ref{lem_fully_linear},
% per \appref{sec_fully_linear_proof}, 
 we have \eqref{eq_W_scaling}, and so the variance satisfies
\be \left\|(W_k^{\top}W_k)^{-1}\right\| = \left\|D_k\left(\hat{W}_k^{\top}\hat{W}_k\right)^{-1}D_k^{\top}\right\| \leq \max(1, \Delta^{-2})\left\|(\hat{W}_k^{\top}\hat{W}_k)^{-1}\right\| = \frac{\max(1, \Delta^{-2})}{\sigma_{min}(\hat{W}_k)^2}, \ee
where $\sigma_{min}(\hat{W}_k)$ is the smallest singular value of $\hat{W}_k$, since $\|D_k\|=\max(1,\Delta^{-1})$.
However, from \cite[Theorem 4.12]{Conn2009}, the strong $\Lambda$-poisedness of $Y$ gives
\be \frac{1}{\sigma_{min}(\hat{W}_k)} \leq \frac{\theta(n+1)}{\sqrt{p+1}}\Lambda, \ee
for some constant $\theta>0$.
All together, we get
\be \mathbb{E}\left[\left(\begin{bmatrix}\t{c} \\ \t{\bg}\end{bmatrix} - \begin{bmatrix}c \\ \bg\end{bmatrix}\right) \left(\begin{bmatrix}\t{c} \\ \t{\bg}\end{bmatrix} - \begin{bmatrix}c \\ \bg\end{bmatrix}\right)^{\top}\right] \leq \frac{\sigma^2 \max(1, \Delta^{-2}) \theta^2 (n+1)^2 \Lambda^2}{N(p+1)}, \ee
so the square error in the regression model $(\t{c},\t{\bg})$ is inversely proportional to $N(p+1)$, the total number of evaluations of $\t{f}$ used in building the right-hand side of \eqref{eq_linear_interp_system}.
We conclude that, all else being equal (including the strong $\Lambda$-poisedness of $Y$), we would get the same model error from using $|Y|=c(n+1)$ points with no sample averaging, or $|Y|=n+1$ points and using $c$ samples per point.
This provides further support for the similar results for sample averaging and regression models observed in \secref{sec_regression_results}.

\section{General Objective Test Problems} \label{sec_genobj_problems}
\begin{table}[H]
	\centering
	\small{
	\begin{tabular}{rlccccc} % convention: show f0 and f* to 7 sig figs
% 		\toprule
		\hline\noalign{\smallskip}
		\# & Problem & $n$ & $f(\bx_0)$ & $f(\bx^*)$ & Parameters \\ \noalign{\smallskip}\hline\noalign{\smallskip} %\midrule
		1 & ARWHEAD & 100 & 297 & 0 & $N = 100$ \\
		2 & BDEXP* & 100 & 26.52572 & 0 & $N = 100$ \\
		3 & BOX & 100 & 0 & $-11.24044$ & $N = 100$ \\
		4 & BOXPOWER & 100 & 866.2462 & 0 & $N = 100$ \\
		5 & BROYDN7D & 100 & 350.9842 & 40.12284 & $N/2 = 50$ \\
		6 & CHARDIS1 & 98 & 830.9353 & 0 & $NP1 = 50$ \\
		7 & COSINE & 100 & 86.88067 & $-99$ & $N = 100$ \\
		8 & CURLY10 & 100 & $-6.237221\times 10^{-3}$ & $-1.003163\times 10^{4}$ & $N = 100$ \\
		9 & CURLY20 & 100 & $-1.296535\times 10^{-2}$ & $-1.003163\times 10^{4}$ & $N = 100$ \\
		10 & DIXMAANA & 90 & 856 & 1 & $M = 30$ \\
		11 & DIXMAANF & 90 & $1.225292\times 10^{3}$ & 1 & $M = 30$ \\
		12 & DIXMAANP & 90 & $2.128648\times 10^{3}$ & 1 & $M = 30$ \\
		13 & ENGVAL1 & 100 & 5841 & 109.0881 & $N = 100$ \\
		14 & FMINSRF2 & 64 & 23.461408 & 1 & $P = 8$ \\
		15 & FMINSURF & 64 & 32.84031 & 1 & $P = 8$ \\
		16 & NCB20 & 110 & 202.002 & 179.7358 & $N = 100$ \\
		17 & NCB20B & 100 & 200 & 196.6801 & $N = 100$ \\
		18 & NONCVXU2 & 100 & $2.639748\times 10^{6}$ & 231.8274 & $N = 100$ \\
		19 & NONCVXUN & 100 & $2.727010\times 10^{6}$ & 231.6808 & $N = 100$ \\
		20 & NONDQUAR & 100 & 106 & 0 & $N = 100$ \\
		21 & ODC & 100 & 0 & $-1.098018\times 10^{-2}$ & $(NX, NY) = (10, 10)$ \\
		22 & PENALTY3 & 100 & $9.801798\times 10^{7}$ & 0.001 & $N/2 = 50$ \\
		23 & POWER & 100 & $2.550250\times 10^{7}$ & 0 & $N = 100$ \\
		24 & RAYBENDL & 62 & 98.03445 & 96.25168 & $NKNOTS = 32$ \\
		25 & SCHMVETT & 100 & $-280.2864$ & $-294$ & $N = 100$ \\
		26 & SINEALI* & 100 & $-0.8414710$ & $-9.900962\times 10^{3}$ & $N = 100$ \\
		27 & SINQUAD & 100 & 0.6561 & $-4.005585\times 10^{3}$ & $N = 100$ \\
		28 & TOINTGOR & 50 & $5.073786\times 10^{3}$ & $1.373905\times 10^{3}$ & --- \\
		29 & TOINTGSS & 100 & 892 & 10.10204 & $N = 100$ \\
		30 & TOINTPSP & 50 & $1.827709\times 10^{3}$ & 225.5604 & --- \\
% 		\bottomrule
		\noalign{\smallskip}\hline
	\end{tabular}}
	\caption{Details of medium-scale general objective test problems from the CUTEst test set, including the value of $f(\bx^*)$ used in \eqref{eq_solved_threshold} for each problem. Some problems are variable-dimensional; the relevant parameters yielding the given $n$ are provided. Problems marked * have bound constraints. The value of $n$ shown excludes fixed variables. Some of the problems were taken from \cite{Luksan2010}.}
	\label{tab_genobj_problems}
\end{table}
