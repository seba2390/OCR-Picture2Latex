\section{Filter Description}
\label{sec: filter description}
In the description of the filter setup, we follow the convention in~\cite{mourikis2007multi}. The IMU state is defined as,
\begin{equation*}
\mathbf{x}_{I} = 
\left(
{}^I_G \mathbf{q}^\top \quad 
\mathbf{b}_g^\top \quad 
{}^G\mathbf{v}^\top_I \quad 
\mathbf{b}_a^\top \quad
{}^G\mathbf{p}^\top_I \quad
{}^I_C \mathbf{q}^\top \quad
{}^I\mathbf{p}^\top_C
\right)^\top
\end{equation*}
where the quaternion ${}^I_G \mathbf{q}$ represents the rotation from the inertial frame to the body frame. In our configuration, the body frame is set to be the IMU frame. The vectors ${}^G\mathbf{v}_I \in \RR^3$ and ${}^G\mathbf{p}_I \in \RR^3$ represent the velocity and position of the body frame in the inertial frame. The vectors $\mathbf{b}_g \in \RR^3$ and $\mathbf{b}_a \in \RR^3$ are the biases of the measured angular velocity and linear acceleration from the IMU. Finally the quaternion, ${}^I_C \mathbf{q}$ and ${}^I\mathbf{p}_C \in \RR^3$ represent the relative transformation between the camera frame and the body frame. Without loss of generality, the left camera frame is used assuming the extrinsic parameters relating the left and right cameras are known. Using the true IMU state would cause singularities in the resulting covariance matrices because of the additional unit constraint on the quaternions in the state vector. Instead, the error IMU state, defined as,
\begin{equation*}
\tilde{\mathbf{x}}_{I} = 
\left(
{}^I_G \tilde{\bm{\theta}}^\top \quad 
\tilde{\mathbf{b}}_g^\top \quad 
{}^G\tilde{\mathbf{v}}^\top_I \quad 
\tilde{\mathbf{b}}_a^\top \quad
{}^G\tilde{\mathbf{p}}^\top_I \quad
{}^I_C \tilde{\bm{\theta}}^\top \quad
{}^I\tilde{\mathbf{p}}^\top_C
\right)^\top
\end{equation*}
is used with standard additive error used for position, velocity, and biases (e.g. ${}^G\tilde{\mathbf{p}}_I = {}^G\mathbf{p}_I-{}^G\hat{\mathbf{p}}_I$). For the quaternions, the error quaternion $\delta\mathbf{q} = \mathbf{q}\otimes\hat{\mathbf{q}}^{-1}$ is related to the error state as,
\begin{equation*}
\delta\mathbf{q} \approx
\left(
\frac{1}{2} {}^G_I\tilde{\bm{\theta}}^\top \quad 1
\right)^\top
\end{equation*}
where ${}^G_I\tilde{\bm{\theta}} \in \RR^3$ represents a
small angle rotation. With such a representation, the dimension of orientation error is reduced to $3$ enabling proper presentation of its uncertainty. Ultimately $N$ camera states are considered together in the state vector, so the entire error state vector would be,
\begin{equation*}
\tilde{\mathbf{x}} = 
\left(
\tilde{\mathbf{x}}_I^\top \quad
\tilde{\mathbf{x}}_{C_1}^\top \quad
\cdots \quad 
\tilde{\mathbf{x}}_{C_N}^\top
\right)^\top
\end{equation*}
where each camera error state is defined as,
\begin{equation*}
\tilde{\mathbf{x}}_{C_i} = 
\left(
{}^{C_i}_G\tilde{\bm{\theta}}^\top \quad
{}^G\tilde{\mathbf{p}}_{C_i}^\top
\right)^\top
\end{equation*}
In order to maintain bounded computational complexity, some camera states have to be marginalized once the number of camera states reaches a preset limit. Discussions of how to choose camera states to marginalize can be found in Section \ref{subsec: filter update mechanism}. 
\input{tex/ProcessModel.tex}
\input{tex/MeasurementModel.tex}
\input{tex/ObservabilityConstraint.tex}
\input{tex/FilterUpdateMechanism.tex}
\input{tex/ImageProcessingFrontend.tex}









