\PassOptionsToPackage{table, dvipsnames}{xcolor}
\documentclass{article} % For LaTeX2e
\usepackage{iclr2023_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{graphicx,adjustbox}
\usepackage{hyperref}
\usepackage{url}
\usepackage[ruled,linesnumbered]{algorithm2e}
\SetKwComment{Comment}{/* }{ */}
\SetKwInput{kwInput}{Input}
\SetKwInput{kwOutput}{Output}
\RestyleAlgo{ruled}
\usepackage{amsmath}
\usepackage{array}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage[table]{xcolor}
\usepackage[normalem]{ulem}
\useunder{\uline}{\ul}{}
\usepackage{float, wrapfig}


% \usepackage[table,dvipsnames]{xcolor}
\usepackage{amssymb}
\newcommand\ccw[1]{\cellcolor{white}{#1}}
\definecolor{purple}{HTML}{9903F0}
\newcommand{\nbc}[3]{
        {\colorbox{#3}{\scriptsize\textcolor{white}{#1}}}
            {\textcolor{#3}{\small$\blacktriangleright$\textit{#2}$\blacktriangleleft$}}
}

\newcommand{\MTH}[1]{\nbc{Mehrtash}{#1}{purple}}

\setlength{\textfloatsep}{5pt plus 1.0pt minus 2.0pt}
\setlength{\floatsep}{5pt plus 1.0pt minus 2.0pt}
\setlength{\intextsep}{5pt plus 1.0pt minus 2.0pt}

% \title{Faster and Better Data-Free Knowledge \\Distillation via Noisy Layer}
%\title{Efficient Data Generation from Label-Text Embedding for Data-free Knowledge Distillation}

\title{Unleash Data Generation for Efficient and Effective Data-free Knowledge Distillation}


% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Minh-Tuan Tran$^1$, Trung Le$^1$, Xuan-May Le$^2$, Mehrtash Harandi$^1$, Quan Hung Tran$^3$,\\ \textbf{Dinh Phung$^{1,4}$} \\
$^1$Monash University,  $^2$University of Melbourne, $^3$Adobe Research , $^4$VinAI Research
% \texttt{\{tuan.tran7,trunglm,mehrtash.harandi,dinh.phung\}@monash.edu} \\
% \texttt{xuanmay.le@student.unimelb.edu.au, qtran@adobe.com} \\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
\usepackage{caption}
\captionsetup[figure]{skip=2pt}
\captionsetup[table]{skip=2pt}
\captionsetup[algorithm]{skip=2pt}
\vspace{-\baselineskip}

%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\iclrfinalcopy
\begin{document}


\maketitle

\begin{abstract}

% Data-Free Knowledge Distillation (DFKD) has recently made remarkable advancements with its core principle of transferring knowledge from a teacher neural network to a student neural network by employing a generator to create synthetic images, all without requiring access to the original data. Nevertheless, existing methods struggle to address the two most challenging problems in data-free data generation, which include achieving fast convergence, high-quality sampling, and the ability to create a wide array of diverse images. In this paper, we propose a novel noisy layer Generation method (NAYER) which utilizes the meaningful label-text embedding (LTE) as the input and relocates the randomness source from the input to a random noisy layer. The significance of LTE lies in its ability to contain substantial meaningful information, demanding only a few steps to generate high-quality samples. Moreover, by adding a random noisy layer between the generator and LTE, our method effectively addresses the problem of overemphasizing label information while neglecting the essential noise component required for producing diverse samples. By reinitializing the noisy layer in each iteration, we aim to facilitate the generation of diverse samples while still retaining the method's efficiency, thanks to the ease of learning provided by LTE. Experiments carried out on CIFAR-10, CIFAR-100, Tiny-ImageNet, and Imagenet datasets demonstrate that our proposed methods not only outperform the state-of-the-art methods but also achieve speeds 5 to 15 times faster. The code is available at \url{https://github.com/fw742211/nayer}.

Data-Free Knowledge Distillation (DFKD) has recently made remarkable advancements with its core principle of transferring knowledge from a teacher neural network to a student neural network without requiring access to the original data. Nonetheless, existing approaches encounter a significant challenge when attempting to generate samples from random noise inputs, which inherently lack meaningful information. Consequently, these models struggle to effectively map this noise to the ground-truth sample distribution, resulting in the production of low-quality data and imposing substantial time requirements for training the generator. In this paper, we propose a novel Noisy Layer Generation method (NAYER) which relocates the randomness source from the input to a noisy layer and utilizes the meaningful label-text embedding (LTE) as the input. The significance of LTE lies in its ability to contain substantial meaningful inter-class information, enabling the generation of high-quality samples with only a few training steps. Simultaneously, the noisy layer plays a key role in addressing the issue of diversity in sample generation by preventing the model from overemphasizing the constrained label information. By reinitializing the noisy layer in each iteration, we aim to facilitate the generation of diverse samples while still retaining the method's efficiency, thanks to the ease of learning provided by LTE. Experiments carried out on multiple datasets demonstrate that our NAYER not only outperforms the state-of-the-art methods but also achieves speeds 5 to 15 times faster than previous approaches. 

% The code is available at \url{https://github.com/fw742211/nayer}.

\end{abstract}

\section{Introduction}

\begin{wrapfigure}{r}{0.48\textwidth}
\includegraphics[width=1\linewidth]{img/time_plot_c100-visual.pdf} 
\caption{Accuracy of student models and GPU hours of training time on CIFAR-100 dataset. All variants of our method NAYER not only attains the highest accuracies across but also accelerates the training process by 5 to 15 times compared to DeepInv \citep{adi}.}
% \MTH{15x wrt what? also, you have used green to show MAD as well (minor issue). YOu could also think of adding a legend with refs to the methods (not sure about this though)}
% }
\label{fig:time_vs_acc}
\raggedbottom
\end{wrapfigure}
Knowledge distillation (KD) aims to train a student model capable of emulating the capabilities of a pre-trained teacher model. Over the past decade, KD has been explored across diverse domains, including image recognition \citep{kd3}, speech recognition \citep{skd}, and natural language processing \citep{nlpkd}. Conventional KD methods generally assume that the student model has access to all or part of the teacher's training data. However, real-world applications often impose constraints on accessing the original training data. This issue becomes particularly relevant in cases involving privacy-sensitive medical data, which may contain personal information or data considered proprietary by vendors. Consequently, in such contexts, conventional KD methods no longer suffice to address the challenges posed.

Data-Free Knowledge Distillation (DFKD) has recently seen significant advancements as an alternative method. Its core principle involves transferring knowledge from a teacher neural network ($\mathcal{T}$) to a student neural network ($\mathcal{S}$) by generating synthetic data instead of accessing the original training data. The synthetic data enable adversarial training of the generator and student \citep{zskd,zskt}. In this setup, the student seeks to match the teacher's predictions on synthetic data, while the generator aims to create samples that maximize the discrepancy between the student's and teacher's predictions (Fig.~\ref{fig:model1}a). 

Due to its reliance on synthetic samples, the need for an effective and efficient data-free generation technique becomes imperative. A major limitation of current DKFD methods is that they merely generate synthetic samples from random noise, neglecting to incorporate supportive and semantic information \citep{predfkd,fastdfkd,spshnet,kakr}. This limitation in turn incurs the generation of low-quality data and excessive time requirements for training the generator, rendering them unsuitable for large-scale tasks. Notably, almost SOTA DFKD methods do not report results on large-scale ImageNet due to the significant training time involved. Even with smaller datasets such as CIFAR-100 (see Fig.~\ref{fig:time_vs_acc}), state-of-the-art DFKD methods such as CMI \citep{cmi}, MAD \citep{mad}, or DeepInv still demand approximately 25 to 30 hours of training while struggling to achieve high accuracy. This emphasizes the pressing need for more efficient and effective DFKD techniques.

\begin{figure}[t]
\begin{center}
\includegraphics[width=0.9\linewidth]{img/model1.pdf}
\end{center}
\caption{Data Generation Strategies: (a) The classic Data-free Generation, which optimizes random noise (z); (b) the 1-to-1 Noisy Layer Generation, which uses one noisy layer for generating one synthetic image from the embedding of label's text ($\ve_{\vy}$) by CLIP \citep{clip}; (c) K-to-1 Noisy Layer Generation, which uses one noisy layer to generate multiple synthetic images.}
\label{fig:model1}
\end{figure}

In this paper, we introduce a simple yet effective DFKD method called  \textbf{N}oisy L\textbf{AYER} Generation (NAYER). Our approach relocates the source of randomness from the input to the noisy layer and utilizes the meaningful label-text embedding (LTE) generated by a pretrained text encoder \citep{clip} as the input. In this context, LTE plays a crucial role in accelerating the training process due to its ability to encapsulate useful interclass information. It is noteworthy that in the field of text embedding, there is a common observation that the text with similar meanings tend to exhibit closer embedding proximity to one another \citep{doc2vec}. As can be seen in Fig.~\ref{fig:tsne}, in Fig.~\ref{fig:tsne}, our empirical observations reveal a close alignment between the proximity pattern in LTE and that of the ground-truth data. Consequently, by using LTE as input, our approach can proficiently generate high-quality samples that closely mimic the distributions of their respective classes with only a few training steps, as illustrated in Fig.~\ref{fig:as_nl_lte}a. 

However, when utilizing LTE as the input,  we empirically observed that the generation process in existing methods suffers from a form of mode collapse, meaning that the generator consistently produces similar data in every iteration. We attribute this phenomenon to an overemphasis on constant label-related information. This can be demonstrated when we concatenate LTE with random noise to study the weight magnitudes of both, indicative of their significance in learning \citep{lottery}. Fig.~\ref{fig:motivation}d illustrates that weight magnitudes for learning LTE notably exceed those for random noise, indicating the model's excessive emphasis on label information compared to random noise. This emphasis might inadvertently overshadow the crucial random noise component necessary for generating a diverse array of samples. 

Our solution addresses this issue by relocating the source of randomness from the input to the layer level by adding a noisy layer to learn the constant label information.  This involves incorporating a random noise layer to function as an intermediary between the generator and LTE, which prevents the generator from relying solely on unchanging label information (Fig.~\ref{fig:model1}b). The source of randomness now comes from the random reinitialization of the noisy layer for each iteration. Through this mechanism, we aim to effectively mitigate the risk of overemphasizing label information, thus enhancing the diversity of synthesized images. Furthermore, thanks to the inherent ease of learning label-text embeddings, regardless of how it is initialized, the noisy layer can consistently generate high-quality samples in just a few steps, thereby maintaining the method's efficiency. Additionally, we propose leveraging a single noisy layer to generate multiple samples (e.g., 100 images across 100 classes of CIFAR-100) (Fig.~\ref{fig:model1}c). This strategy capitalizes on the multiple gradient sources stemming from various classes, enhancing the diversity of the noisy layer's output, reducing model size and expediting the training process.

We conducted comprehensive experiments on several datasets, including ImageNet, demonstrating the superiority of our proposed techniques over state-of-the-art algorithms. Notably, NAYER not only outperforms the existing state-of-the-art approaches in terms of accuracy but also exhibits remarkable speed enhancements. Specifically, as illustrated in Fig.~\ref{fig:time_vs_acc}, our proposed methods achieve speeds that are 5 to even 15 times faster while also attaining higher accuracies compared to previous methods, highlighting their efficiency and effectiveness.

\section{Related Work}
\textbf{Data-Free Knowledge Distillation.} DFKD methods generate synthetic images to transfer knowledge from a pre-trained teacher model to a student model. These data are used to jointly train the generator and the student in an adversarial manner \citep{zskd,zskt}. Under this adversarial learning scheme, the student attempts to make predictions close to the teacher’s on synthetic data, while the generator tries to create samples that maximize the mismatch between the student’s and the teacher’s predictions. This adversarial game enables a rapid exploration of synthetic distributions useful for knowledge transfer between the teacher and the student. 

\textbf{Data-Free Generation.} As the central principle of DFKD revolves around synthetic samples, the data-free generation technique plays a pivotal role. \citep{adi} proposes the image-optimized method which attempts to optimize the random noise images using teacher network batch normalization statistics. Sample-optimized methods \citep{cmi,spshnet} focus on optimizing random noise over numerous training steps to produce synthetic images in case-by-case strategy. In contrast, generator-optimized methods \citep{mad,kakr,predfkd} attempt to ensure that the generator has the capacity to comprehensively encompass the entire distribution of the original data. In the other words, regardless of the input random noise, these methods aim to consistently yield high-quality samples for training the student model. This approach often prolongs the training process and may not consistently produce high-quality samples, particularly when diverse noises are employed during both the sampling and training phases. Furthermore, the main problem in existing data-free generation is the use of random noise input without any meaningful information, leading to generate the low-quality samples and prolonged training times for the generator. \citep{fastdfkd} introduced FM, a method incorporating a meta generator to accelerate the DFKD process significantly. However, this acceleration comes at the cost of a noticeable trade-off in classification accuracy.

\textbf{Synthetizing Samples from Label Information}. Drawing inspiration from the success of incorporating label information in adversarial frameworks like Conditional GAN \citep{cgan}, several methods in DFKD have adopted strategies to generate images guided by labels. In these approaches, a common practice involves fusing random noise ($\vz$) with a learnable embedding ($\ve_\vy$) of the one-hot label vector, which is used as input for the model \citep{cgdfkd1, cgdfkd2, mad}. This combination enhances control over the resulting class-specific synthetic images. However, despite the potential of label information, its application has yielded only minor improvements. This can be attributed to two key factors. Firstly, the one-hot vector introduces sparse information that merely distinguishes labels uniformly, failing to capture the nuanced relationships between different classes. Consequently, the model struggles to generate images that align closely with ground-truth distributions. Secondly, there exists a challenge in balancing the generated images' quality and diversity when incorporating label information. This can inadvertently lead to an overemphasis on label-related details, potentially overshadowing the crucial contribution of random noise, which is necessary for generating a diverse range of samples. 

% Recently, pretrained language models have emerged as highly successful tools for multi-modal applications, effectively comprehending the connections between images and text. A prime example is CLIP \citep{clip}, which concurrently trains an image encoder and a text encoder. This training aims to predict accurate pairings among batches of (image, text) training instances. As a result, the text embeddings produced by CLIP offer a compelling representation of the inter-class relationships present in the associated text descriptions.

% \textbf{Data-Free Knowledge Distillation.} DFKD pertains to transferring knowledge from a large pre-trained teacher model to a smaller student model without using the original training data. On the one hand, several methods employ random noise image optimization to synthesize data \citep{odfkd, zskd, adi}. For example, \citep{zskd} treated teacher network outputs as a Dirichlet distribution constraint during noise image optimization. Similarly, \citep{adi} regularized synthesized image distribution using teacher network batch normalization statistics. Notably, individual image optimization independence leads to computational inefficiency, hindering synthetic image generation speed. On the other hand, certain methods leverage generative networks to synthesize training data from random noise \citep{cmi, dfkd_gr, spshnet, kakr, mad, fastdfkd}. Recent advancements in this domain have yielded notable results. For instance, Momentum Adversarial Distillation (MAD) \citep{mad} addresses distribution shifts during generator and student adversarial training by applying the momentum update. KAKR \citep{kakr} introduces a meta-learning framework that differentiates between Knowledge Acquisition (learning from new samples) and Knowledge Retention (preserving knowledge from past samples), enhancing the student network's learning process by aligning these two tasks effectively. Furthermore, \citep{spshnet} present innovations such as channel-wise feature exchange and multi-scale spatial activation region consistency constraints to enhance diversity. However, existing methods face challenges in generating samples using non-stationary noise, leading to prolonged training times for the Generator. This limitation hinders their applicability to large-scale tasks. \citep{fastdfkd} introduced FM, a method incorporating a meta generator to accelerate the DFKD process significantly—achieving a remarkable 100-fold increase in knowledge transfer speed. However, this acceleration comes at the cost of a noticeable trade-off in classification accuracy.

% \textbf{Synthetizing Samples from Label Information}. Drawing inspiration from the success of incorporating label information in adversarial frameworks like Conditional GAN \citep{cgan}, several methods in DFKD have adopted strategies to generate images guided by labels. In these approaches, a common practice involves fusing random noise ($z$) with a learnable embedding ($e_y$) of the one-hot label vector, which is used as input for the model \citep{cgdfkd1, cgdfkd2, mad}. This amalgamation enhances control over the resulting class-specific synthetic images. However, despite the potential of label information, its application has yielded only minor improvements in their approach. This can be attributed to two key factors. Firstly, the one-hot vector introduces sparse information that merely distinguishes labels uniformly, failing to capture the nuanced relationships between different classes. Consequently, the model struggles to generate images that align closely with ground-truth distributions. Secondly, there exists a challenge in 
% balancing the generated images' quality and diversity when incorporating label information. This can inadvertently lead to an overemphasis on label-related details, potentially overshadowing the crucial contribution of random noise, which is necessary for generating a diverse range of samples. Recently, pretrained language models have emerged as highly successful tools for multi-modal applications, effectively comprehending the connections between images and text. A prime example is CLIP \citep{clip}, which concurrently trains an image encoder and a text encoder. This training aims to predict accurate pairings among batches of (image, text) training instances. As a result, the text embeddings produced by CLIP offer a compelling representation of the inter-class relationships present in the associated text descriptions.
\section{Proposed Method}
\begin{figure}[t]
\begin{center}
\includegraphics[width=\linewidth]{img/motivation2.pdf}
\end{center}
\caption{(a) Random noise for data generation. (b) One-hot labels only uniformly distinguish labels, lacking inter-class relationships. In contrast, (c) LTE captures inter-class connections, bringing similar classes closer in the embedding space. This proximity enhances the similarity between the input and ground-truth sample distributions, thereby allowing the model to more easily mimic the ground-truth distribution and accelerating the learning process. (d) The averaging magnitude of weight used to learn LTE is much larger than those for random noise, highlighting the model's negative focus on label information while ignoring random noise. }
\label{fig:motivation}
\end{figure}
\subsection{Data-free Knowledege Distillation Framework}
%
% \MTH{this definition is unconventional to me, you want to say that $D =\{ \vx_i, \vy_i\}_{i=1}^m $ with
% $\vx_i \in \mathbb{R}^{c \times h \times w}$ and $y_i \in \{1,2,\cdots,K\}$. Also note that we use cdots and also vx and vy  denote vector x and vector y and you have defined them in your math commands files. It would be a good idea to define notations as well (we show vectors by bold ...).}
%
Consider a training dataset $D =\{ (\vx_i, \vy_i)\}_{i=1}^m $ with $\vx_i \in \mathbb{R}^{c \times h \times w}$ and $\vy_i \in \{1,2,\cdots,K\}$, where the pair $(\vx_i, \vy_i)$ represents a training sample and its corresponding label, respectively. Let $\mathcal{T}=\mathcal{T}_{\theta_\mathcal{T}}$ be a pre-trained teacher network on $D$. The objective of DFKD is to train a student network $\mathcal{S}=\mathcal{S}_{\theta_\mathcal{S}}$ to emulate $\mathcal{T}$'s performance, all without needing access to the original dataset $D$. To achieve this, common DFKD techniques utilize a lightweight generator ($\mathcal{G}=\mathcal{G}_{\theta_\mathcal{G}}$) to create synthetic images. These images are then used to train the student network $\mathcal{S}$, employing random noise $\vz$ as a key element in the process. This noise is typically drawn from a standard Gaussian distribution $\mathcal{N}(\boldsymbol{0}, \mathbf{I})$, and it plays a crucial role in generating diverse synthetic samples for training.

\cite{zskd} introduces an adversarial mechanism to concurrently train the generator ($\mathcal{G}$) and the student ($\mathcal{S}$) through the optimization of min-max objective functions. Specifically, for the synthetic images $\hat{\vx} = \mathcal{G}(\vz)$, the student network $\mathcal{S}$ is optimized to minimize the discrepancy between its predictions $\mathcal{S}(\hat{\vx})$ and those of the teacher $\mathcal{T}(\hat{\vx})$ (Eq.~\ref{eq:ls}). This optimization process enables alignment between the student's output and the teacher's expertise, facilitating effective knowledge transfer.
\begin{equation}
    \min\limits_{\theta_\mathcal{S}}\mathcal{L}_\mathcal{S} \triangleq \mathbb{E}_{\hat{\vx} \sim\mathcal{G}(\vz)}\Big[\mathcal{L}_\text{KL}(\mathcal{T}(\hat{\vx}),\mathcal{S}(\hat{\vx}))\Big],
    \label{eq:ls}
\end{equation}
where $\mathcal{L}_\text{KL}$ denotes the Kullback-Leibler divergence. Let $\mathcal{U}(K)$ be a uniform distribution over $\{1,\cdots,K\}$, $\alpha_{cls}, \alpha_{adv}$ and $\alpha_{bn}$ be hyperparameters of the algorithm. The generator ($\mathcal{G}$) then minimizes $\mathcal{L}_\mathcal{G}$  defined as in Eq.~(\ref{eq:lg}).
\begin{align}
    \label{eq:lg}
\resizebox{0.92\textwidth}{!}{%
      $\min\limits_{\theta_\mathcal{G}}\mathcal{L}_\mathcal{G} \triangleq \mathbb{E}_{\vz \sim \mathcal{N}(0,\text{I}),\hat{\vx} \sim\mathcal{G}(\vz), \hat{\vy} \sim \mathcal{U}(K)}\Big[\alpha_{cls}\mathcal{L}_\text{CE}(\mathcal{T}(\hat{\vx}), \hat{\vy})-\alpha_{adv}\mathcal{L}_\text{KL}(\mathcal{T}(\hat{\vx}),\mathcal{S}(\hat{\vx}))+\alpha_{bn}\mathcal{L}_\text{BN}(\mathcal{T}(\hat{\vx}))\Big]$%
}.
\end{align}

Within this context, $\mathcal{L}_\text{CE}$ represents the Cross-Entropy loss term, serving the purpose of training the student on images residing within the high-confidence region of the teacher's knowledge. Conversely, the negative $\mathcal{L}_\text{KL}$ term facilitates the exploration of synthetic distributions, boosting effective knowledge transfer between the teacher and the student. In other words, the student network takes on a role as a discriminator in GANs, ensuring the generator is geared towards producing images that the teacher has mastered, yet the student network has not previously learned. This approach facilitates the focused development of the student's understanding in areas where it lags behind the teacher, enhancing the overall knowledge transfer process. We also use batch norm regularization ($\mathcal{L}_\text{BN}$) \citep{adi,fastdfkd}, a commonly used loss in DFKD, to constrain the mean and variance of the feature at the batch normalization layer to be consistent with the running-mean and running-variance of the same layer.
% \begin{align}
% \text{BN}(\mathcal{T}(\hat{\vx})) \triangleq \sum_l{\lVert\mu_l - \bar{\mu}_l\rVert^2_2 + \lVert\sigma_l - \bar{\sigma}_l\rVert^2_2}
% \end{align}
\subsection{Data-free Generation based on Label-Text Embedding}
To achieve rapid convergence and generate high-quality images, we introduce an innovative strategy that leverages the concept of label-text embeddings (LTE) as an input. Given the batch of $b$ target classes $\hat{\vy} = [\hat{\vy}_1,\cdots,\hat{\vy}_b]$, their label text $Y_{\hat{\vy}} = [Y_{\hat{\vy}_1},\cdots,Y_{\hat{\vy}_b}]$ is embedded using a pretrained text encoder $\mathcal{C} = \mathcal{C}_{\theta_\mathcal{C}}$, followed by batch normalization to facilitate easier discrimination. 
\begin{equation}
    \ve_{\hat{\vy}} = \text{BatchNorm}(\mathcal{C}(Y_{\hat{\vy}}))
    \label{eq:ey}
\end{equation}

In this approach, we employ CLIP \citep{clip} as the encoder and conduct experiments related to this choice, which are detailed in Table \ref{tab:as_multe}. Several studies suggest that text embedding enjoys the tendency for text with similar meanings to exhibit proximity in their embeddings \citep{doc2vec}. Fig.~\ref{fig:motivation}a-c visually represents the LTE, highlighting their superior capacity to depict the relationship between the `Dog' and `Cat' classes. This is evident in their closer proximity (shorter distance) when compared to the `Car' class. This visualization emphasizes the effectiveness of LTE in capturing inherent connections among different classes, in contrast to random noise or one-hot vectors, thereby providing valuable insights into inter-class relationships. This characteristic of LTE contributes to making the input distribution (representing labels)  and ground-truth distribution  (representing actual data) more similar. As a result, it facilitates the model's mapping between these two distributions, accelerating the learning process. Our approach employs LTE as the input for the generator, eliminating the reliance on random noise for synthetic image generation. It is important to note that the embedding $\ve_{\hat{\vy}}$ will remain fixed throughout the entire training process. 
\begin{equation}
    \hat{\vx} = \mathcal{G}(\ve_{\hat{\vy}})
    \label{eq:genxnew}
\end{equation}
Moreover, the synthetic images $\hat{\vx} = \mathcal{G}(\ve_{\hat{\vy}})$ are used in Eq.~(\ref{eq:ls}) and Eq.~(\ref{eq:lg}) to train the student $\mathcal{S}$ and the generator $\mathcal{G}$. Specifically, we regularly reinitialize the generator $\mathcal{G}$ at specified intervals to generate more diverse images. Thanks to the informative content embedded in LTE, our approach can efficiently produce high-quality samples with minimal computational steps. We have also conducted an ablation study to substantiate this claim, as illustrated in Fig.~\ref{fig:as_nl_lte}a-c. The results of this study highlight that LTE significantly accelerates convergence in terms of Cross-Entropy (CE) Loss and yields higher-quality images (as measured by the Inception Score or IS score) in comparison to random noise and one-hot vectors. This acceleration empowers our method to achieve convergence with a considerably smaller number of generator training steps (30 steps for CIFAR10 and 40 steps for CIFAR100), as opposed to the 2,000 steps required by DeepInv or the 500 steps of CMI, all while maintaining superior accuracy (as detailed in Table~\ref{tab:sota}).

\subsection{Generating Diverse Samples with Noisy Layer}

While leveraging label information provides valuable advantages for data generation including \textbf{(i)} accelerating the convergence rate of the CE loss (see Fig.~\ref{fig:as_nl_lte}a) and \textbf{(ii)} improving the quality of the synthetic images with a higher Inception Score (see Fig.~\ref{fig:as_nl_lte}b), the synthetic images are less diverge as shown in Fig.~\ref{fig:as_nl_lte}c. A naive approach is to incorporate the source of randomness $\vz \sim \mathcal{N}(\boldsymbol{0}, \mathbf{I})$ into the LTE $e_{\hat{\vy}}$ (i.e., $\hat{\vx} = \mathcal{G}(\ve_{\hat{\vy}}, \vz)$). Unfortunately, we empirically observed that the generator consistently produces similar data in every iteration (see Fig.~\ref{fig:as_nl_lte}f). We attribute this issues to the risk of overemphasizing label-related information, potentially overshadowing the essential random noise component necessary for generating a diverse range of samples. To illustrate this concern, we examine the magnitudes of the weights involved in learning the concatenation of $(\ve_{\hat{\vy}}, \vz)$, which can be seen as the significance of these weights \citep{lottery}. As depicted in Fig.~\ref{fig:motivation}d, our findings reveal the average weight magnitudes used for learning LTE are significantly larger than those associated with random noise. This observation underscores the model's tendency to overemphasize label information, while neglecting the importance of random noise.

To address this challenge, we propose a solution that involves repositioning the source of randomness from the input level to the layer level. This is achieved by introducing a noisy layer, responsible for learning the constant LTE input. By incorporating the noise layer as an intermediary between the generator and LTE, we ensure that the generator does not solely rely on unchanging label information. The source of randomness now derives from the random reinitialization of the noisy layer during each iteration. Through this mechanism, our goal is to effectively mitigate the risk of an negatively emphasis on LTE, thus enhancing the diversity of the synthesized images. Furthermore, thanks to the ease of training LTE, regardless of their initialization, the noisy layer consistently produces high-quality samples within a few iterations, thereby maintaining the method's efficiency (see Fig.~\ref{fig:as_nl_lte}d-f). Given LTEs $\ve_{\hat{\vy}}$, we feed these $\ve_{\hat{\vy}}$ into the noisy layer $\mathcal{Z}_{\theta_{\mathcal{Z}}}\sim\mathcal{N}(0, \text{I})$. Then, the output of the noisy layer is fed into the generator $\mathcal{G}$ to produce the batch of synthetic images $\hat{\vx}$:
\begin{equation}
    \hat{\vx} = \mathcal{G}(\mathcal{Z}(\ve_{\hat{\vy}}))
    \label{eq:v}
\end{equation}
To achieve the most straightforward architecture, $\mathcal{Z} \in \mathbb{R}^{e\times r}$ is designed as a single linear layer with an input size equal to the embedding size of the text encoder ($e$) and an output size of the noise dimension ($r$). Typically, the output size is set to 1,000, in line with previous works \citep{kakr, mad}. In our method, we employ a single noisy layer designed to learn from all available classes (K-to-1) (i.e., inputting $\ve_{\hat{\vy}}$ with $\hat{\vy}= 1,\dots,K$ to a single noisy layer $\mathcal{Z}$), meaning that we need $M$ noisy layers if we wish to generate $MK$ synthetic images simultaneously. This design allows the noisy layer to generate multiple samples simultaneously, like 100 for CIFAR100 or 10 for CIFAR10, thus efficiently expediting training. This strategy harnesses multiple gradient sources from diverse classes, enriching the diversity of the noisy layer's output. Moreover, it leads to a reduction in model size and accelerates the training process. Our ablation study (Fig.~\ref{fig:as_nl_lte}d-f) demonstrated that using a single noisy layer to synthesize a batch of images (K-to-1) significantly enhances generator diversity, ensuring both fast convergence and high-quality sample generation.

\subsection{Method Summary}
\begin{figure}[t]
\begin{center}
\includegraphics[width=0.9\linewidth]{img/nlg2.pdf}
\end{center}
\caption{General Architecture of Noisy Layer Generation for Data-free Knowledge Distillation.}
\label{fig:arch}
\end{figure}
\begin{figure}[t]
\centering
\begin{minipage}{0.85\textwidth}
\begin{algorithm}[H]
\caption{NAYER}
\label{alg:nldfkd}
\SetAlgoLined
\kwInput{Pretrained teacher $\mathcal{T}_{\theta_\mathcal{T}}$, student $\mathcal{S}_{\theta_\mathcal{S}}$, generator $\mathcal{G}_{\theta_\mathcal{G}}$, text encoder $\mathcal{C}_{\theta_\mathcal{C}}$, list of labels $\hat{\vy}$ and list of text of these labels $Y_{\hat{\vy}}$\; }
\kwOutput{An optimized student  $\mathcal{S}_{\theta_\mathcal{S}}$}
$\ve_{\hat{\vy}} = BatchNorm(\mathcal{C}(Y_{\hat{\vy}}))$\;
% $\vm, \vs \gets 0.5$\;
\For{$\mathcal{E}$ \textit{epochs}}{
    \For{$I$ \textit{iterations}}{
        Randomly reinitializing noisy layers $\mathcal{Z}_{\theta_{\mathcal{Z}}} \sim \mathcal{N}(0, \text{I})$ for each iteration\;
        \For{$g$ \textit{steps}}{
            $\hat{\vx} \gets \mathcal{G}(\mathcal{Z}(\ve_{\hat{\vy}}))$\;
            $\mathcal{L}_{\mathcal{Z}} \gets \alpha_{cls}\mathcal{L}_\text{CE}(\mathcal{T}(\hat{\vx}), \hat{\vy})-\alpha_{adv}\mathcal{L}_\text{KL}(\mathcal{T}(\hat{\vx}),\mathcal{S}(\hat{\vx}))+\alpha_{bn}\mathcal{L}_\text{BN}(\mathcal{T}(\hat{\vx}))$\;
            % $\theta_{\mathcal{G}} \gets \theta_{\mathcal{G}} - \lambda_{\mathcal{G}}\nabla_{\theta_\mathcal{G}}\mathcal{L}_{\mathcal{G}}$\;
            % $\theta_{\mathcal{Z}} \gets \theta_{\mathcal{Z}} - \lambda_{\mathcal{Z}}\nabla_{\theta_\mathcal{Z}}\mathcal{L}_{\mathcal{G}}$\;
            % $\vm \gets \vm - \lambda_\vm\nabla_{\vm}\mathcal{L}_{\mathcal{G}}$\;
            % $\vs \gets\vs -\lambda_\vs\nabla_{\vs}\mathcal{L}_{\mathcal{G}}$\;
            Update $\theta_{\mathcal{G}}, \theta_{\mathcal{Z}}$ by minimizing $\mathcal{L}_{\mathcal{Z}}$;
        }
        $\mathcal{M} \gets \mathcal{M} \cup \hat{\vx}$\;
    }
    \For{$S$ \textit{iterations}}{
    $\hat{\vx} \sim \mathcal{M}$\;
    Update $\theta_{\mathcal{S}}$ by minimizing $\mathcal{L}_\mathcal{S} \gets \mathcal{L}_\text{KL}(\mathcal{T}(\hat{\vx}),\mathcal{S}(\hat{\vx}))$;
    }
}
\end{algorithm}
\end{minipage}
\end{figure}
The comprehensive architecture of our method is visually depicted in Fig.~\ref{fig:arch}, while the detailed pseudo code is presented in Algorithm \ref{alg:nldfkd}. NAYER initially embeds the label text of each class using a text encoder. It is important to note that the embedding will remain fixed throughout the entire training process. Subsequently, our method undergoes training for $\mathcal{E}$ epochs. Within each epoch, NAYER consists of two distinct phases. The first phase involves training the generator. In each iteration $I$, as described in Algorithm \ref{alg:nldfkd}, the noisy layer $\mathcal{Z}$ is reinitialized (line 4) before being utilized to learn the LTE. The generator and the noisy layer are then trained through $g$ steps using Eq.~(\ref{eq:lz}) to optimize their performance (line 8).
\begin{align}
    \label{eq:lz}
\resizebox{0.92\textwidth}{!}{%
      $\min\limits_{\theta_\mathcal{G},\theta_\mathcal{Z}}\mathcal{L}_\mathcal{Z} \triangleq \mathbb{E}_{\hat{\vx} \sim \mathcal{G}(\mathcal{Z}(\ve_{\hat{\vy}})), \hat{\vy} \sim \mathcal{U}(K)}\Big[\alpha_{cls}\mathcal{L}_\text{CE}(\mathcal{T}(\hat{\vx}), \hat{\vy})-\alpha_{adv}\mathcal{L}_\text{KL}(\mathcal{T}(\hat{\vx}),\mathcal{S}(\hat{\vx}))+\alpha_{bn}\mathcal{L}_\text{BN}(\mathcal{T}(\hat{\vx}))\Big]$%
}
\end{align}
The second phase involves training the student networks. During this phase, all the generated samples are stored in the memory module $\mathcal{M}$ to mitigate the risk of forgetting (line 10), following a similar approach as outlined in \cite{fastdfkd}. Ultimately, the student model is trained by Eq.~(\ref{eq:ls_m}) over $S$ iterations, utilizing the samples from $\mathcal{M}$ (lines 13 and 14).
\begin{equation}
    \min\limits_{\theta_\mathcal{S}}\mathcal{L}_\mathcal{S} \triangleq \mathbb{E}_{\hat{\vx} \sim\mathcal{M}}\Big[\mathcal{L}_\text{KL}(\mathcal{T}_{\theta_\mathcal{T}}(\hat{\vx}),\mathcal{S}_{\theta_\mathcal{S}}(\hat{\vx}))\Big]
    \label{eq:ls_m}
\end{equation}
\section{Experiments}
\subsection{Experimental Settings}
We conducted a comprehensive evaluation of our method across various backbone networks, namely ResNet \citep{resnet}, VGG \citep{vgg}, and WideResNet (WRN)\citep{wrn}, spanning three distinct classification datasets: CIFAR10, CIFAR100 \citep{c10}, and Tiny-ImageNet \citep{tin}. The datasets feature varying scales and complexities, offering a well-rounded assessment of our method's capabilities. In detail, CIFAR10 and CIFAR100 encompass a total of 60,000 images, partitioned into 50,000 for training and 10,000 for testing. CIFAR10 comprises 10 categories, while CIFAR100 boasts 100 categories. The images within both datasets are characterized by a resolution of 32×32 pixels. On the other hand, Tiny-ImageNet comprises 100,000 training images and 10,000 validation images, with a higher resolution of 64 × 64 pixels. This dataset encompasses a diverse array of 200 image categories, contributing to the breadth and comprehensiveness of our evaluation.

\subsection{Results and Analysis}

\noindent
\textbf{Comparison with SOTA DFKD Methods.} Table \ref{tab:sota} displays the results of DFKD achieved by our methods and several state-of-the-art (SOTA) approaches. In general, previous methods exhibit limitations when generating images from random noise, impacting both training time and image diversity. By using LTE as the input and relocating the source of randomness from the input to the layer level,  our approach provides highly diverse training images and faster running time. Notably, with 300 epochs, our method achieves SOTA performance in all comparison cases, except for the Resnet32/Resnet18 case in CIFAR10. However, it is essential to note that our method was designed in a straightforward manner, without incorporating innovative techniques found in current SOTA approaches, such as activation region constraints and feature exchange in SpaceshipNet  \citep{spshnet}, knowledge acquisition and retention meta-learning in KAKR \citep{kakr}, and momentum distillation in MAD \citep{mad}.

\begin{table}[t]
\caption{The distillation results of compared methods in CIFAR10 and CIFAR100.  The best-performing method is highlighted in bold, and the runner-up is underlined. Additionally, we use superscripts to indicate the sources of these results: $^a$ for \cite{fastdfkd}, $^b$ for \cite{kakr}, $^c$ for \cite{mad}, $^d$ for \cite{spshnet}, and $^e$ for our experiments. In this table, 'R' represents Resnet, 'W' corresponds to WideResnet, and 'V' stands for VGG.}
\begin{adjustbox}{width=\linewidth}
\begin{tabular}{lcccccccccccc}
\toprule
\textbf{} &
  \multicolumn{5}{c}{\textbf{CIFAR10}} &
  \multicolumn{5}{c}{\textbf{CIFAR100}} &
  \textbf{TinyImageNet} &
  \textbf{ImageNet} \\ \midrule
\multirow{2}{*}{\textbf{Method}} &
  \textbf{R34} &
  \textbf{W402} &
  \textbf{W402} &
  \textbf{W402} &
  \textbf{V11} &
  \textbf{R34} &
  \textbf{W402} &
  \textbf{W402} &
  \textbf{W402} &
  \textbf{V11} &
  \textbf{R34} &
  \textbf{R50} \\
 &
  \textbf{R18} &
  \textbf{W162} &
  \textbf{W161} &
  \textbf{W401} &
  \textbf{R18} &
  \textbf{R18} &
  \textbf{W162} &
  \textbf{W161} &
  \textbf{W401} &
  \textbf{R18} &
  \textbf{R18} &
  \textbf{R50} \\ \midrule
Teacher &
  95.70 &
  94.87 &
  94.87 &
  94.87 &
  92.25 &
  77.94 &
  77.83 &
  75.83 &
  75.83 &
  71.32 &
  66.44 &
  75.45 \\
Student &
  95.20 &
  93.95 &
  91.12 &
  93.94 &
  95.20 &
  77.10 &
  73.56 &
  65.31 &
  72.19 &
  77.10 &
  64.87 &
  75.45 \\
\midrule
DeepInv$^a$ \citep{adi}&
  93.26 &
  89.72 &
  83.04 &
  86.85 &
  90.36 &
  61.32 &
  61.34 &
  53.77 &
  68.58 &
  54.13 &
  - &
  68.00 \\
DFQ$^a$ \citep{dfq}&
  94.61 &
  92.01 &
  86.14 &
  91.69 &
  90.84 &
  77.01 &
  64.79 &
  51.27 &
  54.43 &
  66.21 &
  - &
  - \\
ZSKT$^a$ \citep{zskt}&
  93.32  &
  89.66 &
  83.74 &
  86.07 &
  89.46 &
  67.74 &
  54.59 &
  36.60 &
  53.60 &
  54.31 &
  - &
  - \\
CMI$^a$ \citep{cmi}&
  94.84 &
  92.52 &
  90.01 &
  92.78 &
  91.13 &
  77.04 &
  68.75 &
  57.91 &
  68.88 &
  70.56 &
  64.01 &
  - \\
PREKD$^b$ \citep{predfkd}&
  93.41 &
  - &
  - &
  - &
  - &
  76.93 &
  - &
  - &
  - &
  - &
  49.94 &
  - \\
MBDFKD$^b$ \citep{mbdfkd}&
  93.03 &
  - &
  - &
  - &
  - &
  76.14 &
  - &
  - &
  - &
  - &
  47.96 &
  - \\
FM$^a$ \citep{fastdfkd}&
94.05	&
92.45	&
89.29	&
92.51	&
90.53	&
74.34	&
65.12	&
54.02	&
63.91	&
67.44 &
- &
57.37$^e$ \\
MAD$^c$ \citep{mad}&
  94.90 &
  92.64 &
  - &
  - &
  - &
  77.31 &
  64.05 &
  - &
  - &
  - &
  62.32 &
  - \\
KAKR\_MB$^b$ \citep{kakr}&
  93.73 &
  - &
  - &
  - &
  - &
  77.11 &
  - &
  - &
  - &
  - &
  47.96 &
  - \\
KAKR\_GR$^b$ \citep{kakr}&
  94.02 &
  - &
  - &
  - &
  - &
  77.21 &
  - &
  - &
  - &
  - &
  49.88 &
  - \\
SpaceshipNet$^d$ \citep{spshnet} &
  \textbf{95.39} &
  93.25 &
  90.38 &
  93.56 &
  {\ul 92.27} &
  {\ul 77.41} &
  69.95 &
  58.06 &
  68.78 &
  71.41 &
  {\ul 64.04} &
  - \\ \midrule
\textbf{NAYER ($\mathcal{E} = 100$)} &
  94.03 &
  93.48 &
  91.12 &
  93.57 &
  91.34 &
  76.29 &
  70.20 &
  59.26 &
  69.89 &
  71.10 &
  61.71 &
  - \\
\textbf{NAYER ($\mathcal{E} = 200$)} &
  94.89 &
  {\ul 93.84} &
  {\ul 91.60} &
  {\ul 94.03} &
  91.93 &
  77.07 &
  {\ul 71.22} &
  {\ul 61.90} &
  {\ul 70.68} &
  {\ul 71.53} &
  63.12&
  - \\
\textbf{NAYER ($\mathcal{E} = 300$)} &
  {\ul 95.21} &
  \textbf{94.07} &
  \textbf{91.94} &
  \textbf{94.15} &
  \textbf{92.37} &
  \textbf{77.54} &
  \textbf{71.72} &
  \textbf{62.23} &
  \textbf{71.80} &
  \textbf{71.75} &
  \textbf{64.17} &
  \textbf{68.92} \\ \bottomrule
\end{tabular}
\end{adjustbox}
\label{tab:sota}

\end{table}

\begin{table}[]
\caption{Comparing training times in hours using a single NVIDIA A100 for DFKD methods on CIFAR-10 and CIFAR-100 with the teacher/student models WRN40-2/WRN16-2. FM ($\mathcal{E} = 100, 200,$ and $300$) corresponds to the settings of three variants of our methods. We were unable to replicate the training times of KAKR and SpaceshipNet as they did not provide access to their source code.}
\begin{adjustbox}{width=\linewidth}
\begin{tabular}{@{}lcccccccccccc@{}}
\toprule
 &
  DeepInv &
  CMI &
  DFQ &
  ZSKT &
  MAD &
  \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}FM\\ $\mathcal{E}=100$\end{tabular}} &
  \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}FM\\ $\mathcal{E}=200$\end{tabular}} &
  \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}FM\\ $\mathcal{E}=300$\end{tabular}} &
  \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}\textbf{NAYER}\\ $\mathcal{E}=100$\end{tabular}} &
  \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}\textbf{NAYER}\\ $\mathcal{E}=200$\end{tabular}} &
  \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}\textbf{NAYER}\\ $\mathcal{E}=300$\end{tabular}} \\ \midrule
\textbf{CIFAR10} &
  \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}89.72 \\ (31.23h)\end{tabular}} &
  \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}92.52 \\ (24.01h)\end{tabular}} &
  \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}92.01 \\ (3.31h)\end{tabular}} &
  \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}89.66  \\ (3.44h)\end{tabular}} &
  \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}92.64\\ (13.13h)\end{tabular}} &
  \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}91.63\\ (2.18h)\end{tabular}} &
  \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}92.05\\ (3.98h)\end{tabular}} &
  \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}92.31\\ (7.02h)\end{tabular}} &
  \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}\textbf{93.48}\\ \textbf{(2.05h)}\end{tabular}} &
  \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}93.84\\ (3.85h)\end{tabular}} &
  \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}94.07\\ (6.78h)\end{tabular}} \\
\textbf{CIFAR100 }&
  \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}61.34\\ (31.23h)\end{tabular}} &
  \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}68.75 \\ (24.01h)\end{tabular}} &
  \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}64.79 \\ (3.31h)\end{tabular}} &
  \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}54.59 \\ (3.44h)\end{tabular}} &
  \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}64.05\\ (26.45h)\end{tabular}} &
  \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}67.15\\ (2.23h)\end{tabular}} &
  \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}67.75\\ (4.42h)\end{tabular}} &
  \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}68.25\\ (7.56h)\end{tabular}} &
  \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}\textbf{70.20}\\ \textbf{(2.15h)}\end{tabular}} &
  \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}71.22\\ (4.03h)\end{tabular}} &
  \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}71.72\\ (7.22h)\end{tabular}} \\
\textbf{Avergaing Speed Up} &
  1$\times$ &
  1.3$\times$ &
  9.73$\times$ &
  9.08$\times$ &
  1.78$\times$ &
  14.17$\times$ &
  7.46$\times$ &
  4.29$\times$ &
  \textbf{14.88$\times$} &
  7.93$\times$ &
  4.47$\times$ \\ \bottomrule
\end{tabular}
\end{adjustbox}
\label{tab:time}
\end{table}

\noindent
\textbf{Additional Experiments at Higher Resolution.} To assess the effectiveness of NAYER, we conducted further evaluations on the more challenging ImageNet dataset. ImageNet comprises 1.3 million training images with resolutions of 224×224 pixels, spanning 1,000 categories. ImageNet's complexity surpasses that of CIFAR, making it a significantly more time-consuming task for data-free training. As displayed in Table \ref{tab:sota}, almost all DFKD methods refrain from reporting results on ImageNet due to their prolonged training times. Therefore, our comparison is primarily against DeepInv \citep{adi}, and for the sake of a fair comparison, we re-conducted the experiments of FM \citep{fastdfkd} to align with our settings. The results clearly demonstrate that NAYER outperforms other methods in terms of accuracy, underscoring its efficacy on a large-scale dataset.

\noindent
\textbf{Training Time Comparison.} As shown in Table \ref{tab:time}, the NAYER model trained for 100 epochs (i.e., $\text{NAYER} (\mathcal{E} = 100)$) achieves an average speedup of $15\times$ compared to DeepInv, while also delivering higher accuracies. This substantial speedup is attributed to the significantly fewer steps required for generating samples (30 for CIFAR-10 and 40 for CIFAR-100) compared to DeepInv's 2000 steps. As a result, DeepInv takes over 30 hours to complete training on CIFAR-10/CIFAR-100, whereas our method only requires approximately 2 hours. These results demonstrate that our method not only achieves high accuracy but also significantly accelerates the model training process.

\noindent
\textbf{Additional Experiments in Data-free Quantization.} To demonstrate the use of our data-free generation method in other data-free tasks, we further conduct experiments in Data-free Quantization. We conducted a comparative analysis against ZeroQ \citep{zeroq}, DFQ \citep{dfq}, and ZAQ \citep{zaq}. ZeroQ retrains a quantized model using reconstructed data instead of original data, DFQ is a post-training quantization approach that utilizes a weight equalization scheme to eliminate outliers in both weights and activations, and ZAQ is the pioneering method that employs adversarial learning for data-free quantization. In this comparison, our method consistently demonstrated superior accuracy across all four scenarios.

\begin{table}[t]
\centering
\caption{The results of compared methods in Data-free Quantization.}
\begin{adjustbox}{width=0.65\linewidth}
\begin{tabular}{@{}llcccccc@{}}
\toprule
\textbf{Dataset}                   & \textbf{Model}       & \textbf{Bit} & \textbf{Float32} & \textbf{ZeroQ} & \textbf{DFQ} & \textbf{ZAQ} & \textbf{\textbf{NAYER ($\mathcal{E} = 300$)}} \\ 

\midrule
\multirow{2}{*}{\textbf{CIFAR10}}  & \textbf{MobileNetV2} & W6A6         & 92.39            & 89.9           & 85.43        &  {\ul 92.15}        & \textbf{92.23}    \\
                                   & \textbf{VGG19}       & W4A8         & 93.49            & 92.69          & 92.66        &  {\ul 93.06}       & \textbf{93.15}    \\ \midrule
\multirow{2}{*}{\textbf{CIFAR100}} & \textbf{Resnet20}    & W5A5         & 69.58            & 65.7           & 59.42        &  {\ul 67.94}     & \textbf{68.23}    \\
                                   & \textbf{Resnet18}    & W4A4         & 77.38            & 70.25          & 40.35        &  {\ul 72.67}        & \textbf{73.32}    \\ \bottomrule
\end{tabular}
\end{adjustbox}
\label{tab:extin}
\end{table}

\subsection{Ablation Study}
\noindent
\textbf{Effectiveness of Label-Text Embedding.} We illustrate the impact of using LTE in comparison with random noise (Z) and one-hot vector (OH) as the inputs for the generator across three distinct aspects: the CE Loss of the teacher model with synthetic samples in Eq.~(\ref{eq:lg}), the Inception Score \citep{is} of synthetic images, and the Diversity Score, calculated as the minimum distance in latent space (determined by the teacher model) between new images and existing ones. As depicted in Fig.~\ref{fig:as_nl_lte}a-c, LTE demonstrates significantly accelerated convergence in terms of CE Loss and generates higher-quality images (measured by IS score) compared to the other inputs. This phenomenon can be attributed to the principle that mapping between two distributions is simplified when they share greater similarity. However, the diversity metric for inputting label information (both LTE and OH) is notably lower than that of random noise. This outcome underscores the adverse effects of the generator overly focusing on constant label information.

\begin{figure}[t]
\begin{center}
\includegraphics[width=1\linewidth]{img/as_lte_nl2.pdf}
\end{center}
\caption{(a-c) Comparing LTE, random noise (Z), and one-hot vector (OH) inputs for the generator. While LTE shows rapid convergence in CE Loss and superior image quality, the diversity metric for LTE and OH are far lower than Z. (d-f) Comparison of using Concatenation of Z and LTE vs. 1-to-1 and K-to-1 noisy layer strategies with LTE input for enhancing generator diversity}
\label{fig:as_nl_lte}
\end{figure}

\noindent
\textbf{Effectiveness of Noisy Layer.} We illustrate the impact of utilizing both 1-to-1 and K-to-1 noisy layer strategies in enhancing the generator's diversity when utilizing LTE as the input. As shown in Fig.~\ref{fig:as_nl_lte}d-f, shifting the source of randomness from the input to the layer level and reinitializing the noisy layer with each iteration significantly boosts the generator's diversity while maintaining rapid convergence and high-quality sampling. Moreover, the experiments demonstrate that using a single noisy layer to synthesize a batch of images (K-to-1) results in faster convergence and a higher diversity score when compared to using one noisy layer for each individual image (1-to-1).

\noindent
\textbf{Visualization.} The synthetic results achieved by NAYER within just 100 generator training steps on ImageNet by employing the ResNet-50 as teacher model are presented in Fig.~\ref{fig:visual}a-b. For further comparison, we also visualize synthetic images generated by NAYER, FM, CMI, and DeepInv in Fig.~\ref{fig:visual}c-f. All of these samples are generated using 20 steps with a ResNet-34 teacher model in the CIFAR-10 dataset. While it remains challenging for human recognition, our method visibly demonstrates superior quality and a more diverse range of images when compared to other methods.

\begin{figure}[t]
\begin{center}
\includegraphics[width=1\linewidth]{img/image_visual.pdf}
\end{center}
\caption{(a, b) Display synthetic data generated by our NAYER for ImageNet in just 100 steps. (c, d, e, f) Showcase synthetic data generated for 5 classes (from top to bottom: Car, Bird, Cat, Dog, Ship) in CIFAR10, using only 20 steps of NAYER, FM, CMI, and DeepInv.}
\label{fig:visual}
\end{figure}
% \begin{figure}[t]
% \begin{center}
% \includegraphics[width=\linewidth]{img/visual5.pdf}
% \end{center}
% \caption{Synthetic data generated in only 20 steps by our NAYER compared to FM, CMI and DeepInv.}
% \label{fig:visual}
% \end{figure}

\section{Conclusion and Future Works}

In this paper, we propose a novel Noisy Layer Generation method (NAYER) which utilizes the meaningful label-text embedding (LTE) as the input and relocates the randomness source from the input to the noisy layer. The significance of LTE lies in its ability to contain substantial meaningful information, enabling the fast generating images in only few steps. On the other hand, the use of noisy layer can help the model address the overfocus problem in using constant input information and increase significantly the diversity. Our extensive experiments on different datasets and tasks prove NAYER's superiority over other state-of-the-art methods in data-free knowledge distillation. 

The proposed NAYER does not incorporate the innovative techniques utilized in current SOTA methods, such as feature mixup \citep{spshnet}, knowledge acquisition and retention \citep{kakr}, and momentum updating \citep{mad}. This leaves space for potential improvements through the integration of these techniques in the future. Additionally, NAYER can be applied to various data-free methods, including but not limited to data-free quantization or data-free model stealing.

\clearpage

\bibliography{iclr2023_conference}
\bibliographystyle{iclr2023_conference}
\clearpage
\appendix
\section{Training Details}
\subsection{Teacher Model Training Details}

In this work, we employed pretrained ResNet-34 and WideResnet-40-2 teacher models from \citep{fastdfkd} for CIFAR-10 and CIFAR-100. For Tiny ImageNet, we trained ResNet-34 from scratch using PyTorch, and for ImageNet, we utilized the pretrained ResNet-50 from PyTorch. Teacher models were trained with SGD optimizer, initial learning rate of 0.1, momentum of 0.9, and weight decay of 5e-4, using a batch size of 128 for 200 epochs. Learning rate decay followed a cosine annealing schedule.

\begin{table}[h]
\centering
\caption{Generator Network ($\mathcal{G}$) Architecture for CIFAR10, CIFAR100 and TinyImageNet.}
\begin{adjustbox}{width=0.8\linewidth}
\begin{tabular}{@{}ll@{}}
\toprule
Output          & \textbf{Size   Layers}                              \\ \midrule
1000            & Input                                 \\
$128 \times h/4 \times w/4$ & Linear, BatchNorm1D, Reshape                        \\
$128 \times h/4 \times w/4$ & SpectralNorm (Conv (3 × 3)), BatchNorm2D, LeakyReLU \\
$128 \times h/2 \times w/2$ & UpSample (2×)                                       \\
$64 \times h/2 \times w/2$  & SpectralNorm (Conv (3 × 3)), BatchNorm2D, LeakyReLU \\
$64 \times h \times w$     & UpSample (2×)                                       \\
$3 \times h \times w$      & SpectralNorm (Conv (3 × 3)), Sigmoid, BatchNorm2D      \\ \bottomrule
\end{tabular}
\end{adjustbox}
\label{tab:g_arch}
\end{table}
\begin{table}[h]
\centering
\caption{Generator Network ($\mathcal{G}$) Architecture for ImageNet.}
\begin{adjustbox}{width=0.8\linewidth}
\begin{tabular}{@{}ll@{}}
\toprule
Output          & \textbf{Size   Layers}                              \\ \midrule
1000            & Input                                 \\
$128 \times h/16 \times w/16$ & Linear, BatchNorm1D, Reshape                        \\
$128 \times h/16 \times w/16$ & SpectralNorm (Conv (3 × 3)), BatchNorm2D, LeakyReLU \\
$128 \times h/8 \times w/8$ & UpSample (2×) \\                    
$128 \times h/8 \times w/8$ & SpectralNorm (Conv (3 × 3)), BatchNorm2D, LeakyReLU \\
$128 \times h/4 \times w/4$ & UpSample (2×) \\                      
$64 \times h/4 \times w/4$ & SpectralNorm (Conv (3 × 3)), BatchNorm2D, LeakyReLU \\
$64 \times h/2 \times w/2$ & UpSample (2×) \\
$64 \times h/2 \times w/2$  & SpectralNorm (Conv (3 × 3)), BatchNorm2D, LeakyReLU \\
$64 \times h \times w$     & UpSample (2×)                                       \\
$3 \times h \times w$      & SpectralNorm (Conv (3 × 3)), Sigmoid, BatchNorm2D      \\ \bottomrule
\end{tabular}
\end{adjustbox}
\label{tab:g_arch_in}
\end{table}
\begin{table}[h]
\caption{The hyperparameters for NAYER applied to four different datasets are detailed below. Specifically, $\alpha_{cls}$, $\alpha_{bn}$, and $\alpha_{adv}$ are the hyperparameters associated with Eq.~(\ref{eq:lg}), and their values are consistent with the settings defined in \citep{fastdfkd}. The variables $I$ and $S$ denote the number of iterations for generating and training the student, respectively, while $g$ represents the training steps to optimize the generator $\mathcal{G}_{\theta_{\mathcal{G}}}$ and the noisy layers $\mathcal{Z}$.}
\begin{adjustbox}{width=1\linewidth}
\begin{tabular}{@{}lllllllll@{}}
\toprule
             & batch size (student) & batch size (generator) & $\alpha_{cls}$ & $\alpha_{bn}$ & $\alpha_{adv}$  & $I$  & $g$   & $S$    \\ \midrule
CIFAR10      & 512                  & 400                    & 0.5 & 10 & 1.33 & 2 & 30  & 400     \\
CIFAR100     & 512                  & 400                    & 0.5 & 10 & 1.33 & 2 & 40  & 400   \\
TinyImageNet & 256                  & 200                    & 0.5 & 10 & 1.33 & 4 & 60  & 1000  \\
ImageNet     & 128                  & 50                     & 0.1 & 0.1  & 0.1 & 20 & 100 & 2000 \\ \bottomrule
\end{tabular}
\end{adjustbox}
\label{tab:hyperpara}
\end{table}

\subsection{Student Model Training Details}

To ensure fair comparisons, we adopt the generator architecture outlined in \citep{fastdfkd} for all experiments. Specifically, the generator architecture for CIFAR10, CIFAR100, and TinyImageNet is elaborated upon in Table \ref{tab:g_arch}, while the generator architecture for ImageNet is provided in Table \ref{tab:g_arch_in}. Across all experiments, we maintain a consistent approach for training the student model, employing a batch size of 512. We utilize the SGD optimizer with a momentum of 0.9 and a variable learning rate, following a cosine annealing schedule that starts at 0.1 and ends at 0, to optimize the student parameters ($\theta_\mathcal{S}$). Additionally, we employ the Adam optimizer with a learning rate of 4e-3 for optimizing the generator.We present the results in three distinct variants, each corresponding to a different value of $\mathcal{E}$: 100, 200, and 300, all incorporating a configuration of 20 warm-up epochs, in line with the settings defined in \citep{fastdfkd}. Further details regarding the parameters can be found in Table \ref{tab:hyperpara}.

\section{Extended Results}
\subsection{Comparison with Different Generation Steps}

We compare NAYER and FM, both utilizing random noise as input, while adjusting the training steps for their generators. To ensure a level playing field, we use identical generator architectures, including the additional linear layer (noisy layer for NAYER), and train all models for 300 epochs. This approach allows us to assess their performance under consistent conditions and understand how varying the generator training steps impacts their accuracy.

\begin{table}[h]
\centering
\caption{The accuracies of our NAYER and FM (which uses random noise as the input) with varying training steps for generators. It's important to note that for a fair comparison, we employ the same generator architectures, including the additional linear layer (noisy layer for NAYER) for FM. Furthermore, all models are trained for 300 epochs}
\begin{adjustbox}{width=\linewidth}
\begin{tabular}{@{}lccccccc@{}}
\toprule
\textbf{Generator's training steps  }  & $g = 2$ & $g = 5$ & $g = 10$ & $g = 20$ & $g = 30$ & $g = 40$ & $g = 50$ \\ \midrule
\textbf{FM} & 57.08 & 63.83 & 65.12  & 66.82  & 67.51  & 68.23 & 68.18  \\
\textbf{NAYER} & \textbf{59.23} & \textbf{65.14} &\textbf{ 68.13}  & \textbf{69.31}  & \textbf{70.42}  & \textbf{71.72}  & \textbf{71.70}   \\ \bottomrule
\end{tabular}
\end{adjustbox}
\label{tab:as_mulfm}
\end{table}

\subsection{Comparison with Different Memory Buffer Size}
In this comparison, we evaluate the accuracies of our NAYER (Noisy Label Generator) and MBDFKD models while varying the memory buffer size. It is crucial to highlight that, to ensure a fair and unbiased assessment, we maintain identical generator architectures, including the additional linear layer (noisy layer for NAYER) for both NAYER and MBDFKD. This uniform setup allows us to isolate the impact of memory buffer size on model performance and draw meaningful conclusions about their respective capabilities.

\begin{table}[h]
\centering
\caption{The accuracies of our NAYER and MBDFKD models were evaluated while varying the memory buffer size. It's important to note that for a fair comparison, we employed the same generator architectures, including the additional linear layer (noisy layer for NAYER), for both NAYER and MBDFKD.}
\begin{adjustbox}{width=0.8\linewidth}
\begin{tabular}{@{}llllllll@{}}
\toprule
\textbf{Memory buffer size} & 5k    & 10k   & 20k   & 40k   & 100k  & 200k  & Full  \\ \midrule
\textbf{MBDFKD }           & 73.33 & 74.12 & 73.72 & 72.68 & 71.96 & 71.27 & 70.72 \\
\textbf{NAYER}                & \textbf{90.41} & \textbf{90.76} & \textbf{90.98} & \textbf{91.21} & \textbf{91.64} & \textbf{91.86} & \textbf{91.94} \\ \bottomrule
\end{tabular}
\end{adjustbox}
\label{tab:as_mulmem}
\end{table}

\subsection{Comparison with Different Text Encoder}
We analyze the accuracies of our NAYER (Noisy Label Generator) model across three distinct text encoders: Doc2Vec \citep{doc2vec}, SBERT \citep{sbert}, and CLIP \citep{clip}. This comparative assessment provides valuable insights into how NAYER performs when coupled with different text encoding methodologies.

\begin{table}[h]
\centering
\caption{The accuracies of our NAYER with three different text encoders, including Doc2Vec, SBERT, and CLIP.}
\begin{adjustbox}{width=0.8\linewidth}
\begin{tabular}{@{}lllllll@{}}
\toprule
             & \multicolumn{3}{c}{CIFAR-10} & \multicolumn{3}{c}{CIFAR-100} \\ \midrule
Text Encoder & Doc2Vec    & SBERT    & CLIP   & Doc2Vec   & SBERT   & CLIP    \\
Accuracy           & 93.98     & 93.94   & \textbf{94.07}  & 71.58     & 71.63   & \textbf{71.72}   \\ \bottomrule
\end{tabular}
\end{adjustbox}
\label{tab:as_multe}
\end{table}

\subsection{Further Comparison with Different Generation Strategies}

In this section, we conduct an extensive comparison of various generation strategies. These strategies encompass using only LTE (LTE), random noise (Z), one-hot vectors (OH), the sum of random noise and one-hot vectors \citep{mad} (sum(OH,Z)), the concatenation of Z and one-hot vectors \citep{cgdfkd1, cgdfkd2} (cat(OH,Z)), the concatenation of Z and LTE (cat(LTE,Z)), the sum of Z and LTE (cat(LTE,Z)), utilizing our noisy layer for each LTE (LTE+NL(1to1)), a single noisy layer for all LTE (LTE+NL(1toN)), and the generation methods of DeepInv \citep{adi} and CMI \citep{cmi}. We evaluate these strategies using three metrics: CE Loss to demonstrate convergence speed, IS Score to illustrate image quality, and a diversity metric. It is evident that our proposed method, which leverages the noisy layer for learning LTE, achieves both rapid convergence and high-quality images while maintaining significant diversity.

\begin{figure}[t]
\begin{center}
\includegraphics[width=\linewidth]{img/as_all.pdf}
\end{center}
\caption{When further comparing various generation strategies based on CE Loss, IS Score, and a diversity metric, it becomes evident that our proposed method, which utilizes the noisy layer for learning LTE, achieves both rapid convergence and high-quality images while maintaining remarkable diversity.}
\label{fig:full_as}
\end{figure}

\subsection{t-SNE Visuallization of LTE and Ground-truth Dataset Distribution}

In this section, we aim to illustrate the interclass information captured by LTE (Label-Text Embedding). To achieve this, we provide t-SNE visualizations of the embeddings for labels and ground-truth data distribution pertaining to four distinct classes: Car, Cat, Dog, and Truck. The t-SNE representation of LTE closely aligns with the ground-truth distribution, especially in the proximity between classes like Car and Truck, as well as Cat and Dog, indicating notably smaller distances compared to other class pairings.

\begin{figure}[t]
\begin{center}
\includegraphics[width=\linewidth]{img/tsne.pdf}
\end{center}
\caption{t-SNE Visualization of Label-Text Embedding and Ground-Truth Dataset Distribution for Four Classes: Car, Cat, Dog, and Truck.}
\label{fig:tsne}
\end{figure}

\end{document}
