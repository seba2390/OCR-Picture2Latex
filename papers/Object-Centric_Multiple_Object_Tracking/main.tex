\documentclass[10pt,twocolumn,letterpaper]{article}
\usepackage[toc,title,page]{appendix}
\usepackage[accsupp]{axessibility}
\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{arydshln}
\usepackage{booktabs}
\usepackage{bbding}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{enumitem}
\usepackage{multirow}
\usepackage{capt-of}% or \usepackage{caption}
\usepackage{bbm}
\usepackage{dsfont}
% Include other packages here, before hyperref.
% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\iccvfinalcopy % *** Uncomment this line for the final submission
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ificcvfinal\pagestyle{empty}\fi

\begin{document}

%%%%%%%%% TITLE
\title{Object-Centric Multiple Object Tracking}

\author{Zixu Zhao$^{1}$ \quad Jiaze Wang$^{2}$\thanks{Work completed during internship at AWS Shanghai AI Lab.} \quad Max Horn$^{1}$ \quad  Yizhuo Ding$^{3*}$ \quad Tong He$^{1}$ \quad Zechen Bai$^{1}$ \\
Dominik Zietlow$^{1}$ \quad Carl-Johann Simon-Gabriel$^{1}$ \quad Bing Shuai$^{1}$ \quad Zhuowen Tu$^{1}$ \quad Thomas Brox$^{1}$\\ 
 Bernt Schiele$^{1}$ \quad Yanwei Fu$^{3}$ \quad Francesco Locatello$^{1}$ \quad Zheng Zhang$^{1}$\thanks{Corresponding author.} \quad Tianjun Xiao$^{1}$\\
$^{1}$ Amazon Web Services \quad$^{2}$ The Chinese University of Hong Kong \quad  $^{3}$ Fudan University\\
{\tt\small \{zhaozixu, jiazew, yizhuodi, htong, baizeche, bshuai, ztu, zhaz, tianjux\}@amazon.com}\\
{\tt\small \{hornmax, zietld, cjsg, brox, bschiel, locatelf\}@amazon.de, yanweifu@fudan.edu.cn}\\
}

%\author{First Author\\
%Institution1\\
%Institution1 address\\
%{\tt\small firstauthor@i1.org}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
%\and
%Second Author\\
%Institution2\\
%First line of institution2 address\\
%{\tt\small secondauthor@i2.org}
%}

\maketitle
% Remove page # from the first page of camera-ready.
\ificcvfinal\thispagestyle{empty}\fi

%%%%%%%%% BODY TEXT
\input{sections/abstract}
\input{sections/introduction}
\input{sections/relatedworks}
\input{sections/method}
\input{sections/experiments}
\input{sections/conclusion}

{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\clearpage
\appendix
\renewcommand{\thesection}{\Alph{section}.\arabic{section}}
\setcounter{section}{0}

\begin{appendices}
\section{EM-inspired Loss Formulation}
In this section, we illustrate the formulation of our EM-inspired loss with more details. First, let's start from the formulation of the Expectation-Maximization (EM) algorithm.
Given data $X$, parameters $\theta$ and a set of unobserved latent data $Z$, we intend to estimate the following data likelihood:
\begin{align}
    p(X|\theta) =\int p(X|Z, \theta) p(Z|\theta) dZ =\mathbb{E}_{Z\sim p(\cdot|\theta)}[p(X|Z, \theta)].
\end{align}
However, $p(Z|\theta)$ is usually intractable in practice, therefore the EM algorithm proposes to instead optimize $\theta$ with the following two-step iterative process:

\begin{enumerate}
    \item Expectation step: Given $\theta^{(t)}$ as the estimation value at iteration step $t$, we compute the log-likelihood $Q(\theta|\theta^{(t)})=\mathbb{E}_{Z\sim p(\cdot|\theta^{(t)}, X)}[\log p(X, Z|\theta)]$.
    \item Maximization step: We obtain the next value by $\theta^{(t+1)}=\arg\min_\theta Q(\theta|\theta^{(t)})$.
\end{enumerate}
Suppose $Z$ is discrete, the E-step can be re-written as:
\begin{align}
    Q(\theta|\theta^{(t)}) &= \mathbb{E}_{Z\sim p(\cdot|\theta^{(t)}, X)}[\log p(X, Z|\theta)]\\
    &=\sum_Z p(Z|\theta^{(t)}, X) \log p(X, Z|\theta)
\end{align}
In our work, we see $\mathcal{I}$ as the unobserved data $Z$, and therefore $\mathcal{I}_{ij}$ as a sample from the distribution $p(Z|\theta^{(t)}, X)$. 
Next, we formulate $\log p(X, Z|\theta)$ with three likelihood functions by design. Let $\text{Dec}(\cdot)$ be the mask decoder shared by slots and memory, the memory representation $\mathcal{M}$ and slot representation $\mathcal{S}$ be two $\theta$-parameterized functions of $\mathcal{I}$ (i.e. $Z$) and $X$.

\begin{enumerate}
    \item $p_1(X, Z|\theta) = \text{Dec}(\mathcal{M})^{\text{Dec}(\mathcal{S})}$ 
    \item $p_2(X, Z|\theta)=\mathcal{N}(\text{Dec}(\mathcal{M});\text{Dec}(\mathcal{S}), I)$
    \item $p_3(X, Z|\theta)=\mathcal{N}(\mathcal{M};\mathcal{S}, I)$
\end{enumerate}

\noindent where $I$ is the identity matrix. Next, let

\begin{equation}
    p(X, Z|\theta) \propto p_1(X, Z|\theta)^{\lambda_1} p_2(X, Z|\theta)^{2\lambda_2} p_3(X, Z|\theta)^{2\lambda_3}
\end{equation}

\noindent and we will have:
\begin{align}
    \log p(X, Z|\theta) = & \lambda_1 \text{Dec}(\mathcal{S}) \log \text{Dec}(\mathcal{M}) \\
    & - \lambda_2 ||\text{Dec}(\mathcal{S}) - \text{Dec}(\mathcal{M})||^2 \\
    & - \lambda_3 ||\mathcal{S} - \mathcal{M}||^2 + C,
\end{align}

\noindent with $C$ being negligible constant.

By taking negative and expanding $\mathcal{S}$ to $\mathcal{S}^i$ and $\mathcal{M}$ to $\mathcal{M}^j$, we have the equivalence between $\log p(X, Z|\theta)$ and Equation (5), i.e. $\mathcal{L}_{assign}$, in the main text. As a result, Equation (7) is equivalent to $Q(\theta|\theta^{(t)})$.
To simplify the computation of $\arg\min_\theta Q(\theta|\theta^{(t)})$ for the M-step, we use stochastic gradient descent to approximate $\theta^{(t+1)} = \theta^{(t)} - \alpha \nabla Q(\theta|\theta^{(t)})$.
\section{Hyper-parameter Selection}
Table~\ref{tab:param} summarizes the hyper-parameter selection for OC-MOT on CATER and FISHBOWL datasets. We ablate the selection of those hyper-parameters and show the best choice in bold. Specifically, we analyzed the effect of memory length and MHA block number in the main text.
\begin{table}[!ht]
	\centering
\resizebox{0.46\textwidth}{!}{
\begin{tabular}{ccc}
\toprule[1.5pt]
Method     & CATER       & FISHBOWL  \\\hline
 $\tau_{out}$ &  \textbf{5}, 7, 9 & 5  \\
$\tau_{iou}$ &  \textbf{0.9}, 0.8, 0.7 & 0.9  \\
$\lambda_1, \lambda_2, \lambda_3$  &$1, 0.1, 0$   & $1, 0, 1$  \\  
Memory Length $T_{max}$   & \textbf{6},10,20,32   & 6  \\ 
Slot Number $N$ &  11 & 24  \\
Buffer Number $M$ &  12, \textbf{15}, 20 & 40  \\
 MHA Block Number & 1, \textbf{2}   &  2 \\
\bottomrule[1.5pt]
\end{tabular}
}
\vspace{+0.05in}
\caption{\textbf{Hyper-parameter selection for OC-MOT}. The best selections are marked in bold. }
	\label{tab:param}
\end{table} 

\section{OC Grouping with Partial Labels}
To understand how labels and unsupervised training objectives can be combined in a synergistic fashion, we performed a series of ablations where we decrease the number of annotated frames gradually.  Generally, it is to be expected, that the performance continues to increase as more labels are added, yet this would be linked to higher labeling effort/cost in real-world applications. Thus, in a practical setting, it is desired to find the point of diminishing returns, where the rate of performance increase declines as more data is added. As these experiments are independent from the memory module, they were performed by training a DINOSAUR model on a per-frame basis on the FISHBOWL dataset.
In order to reduce variance in this comparison, all models are fine-tuned based on the same checkpoint of a vanilla trained DINOSAUR model~\cite{seitzer2022bridging} and use the same subset of frames. 
As shown in Figure~\ref{fig:ablation}, both the purely unsupervised model (DINOSAUR) as well as the fully supervised version of the model on a subset of 8 frames (DETR [8]) show comparable results.  Nevertheless, by adding some annotated frames the performance increases significantly as ambiguity about part-whole relationships is resolved by the labels. The relative performance increase of doubling the number of annotated labels is on average $1.92$\%. Further, we see indication of diminishing returns when increasing the number of frames from 4 to 8 as the performance only improves by $1.6$\% in this case. Overall, this shows that adding a few annotated examples to the data significantly improves the performance of the unsupervised DINOSAUR model and greatly outperforms a model that was exclusively trained on the annotated data.
\begin{figure}[t]
	\centering
	\includegraphics[width=0.48\textwidth]{imgs/bar.pdf}
\caption{\textbf{Effect of partial labeling on OC grouping on FISHBOWL.} Comparison of FG-ARI for training fully unsupervised (DINOSAUR), partially supervised (D+DETR [\#annotated frames]) and fully supervised on 8 frames per video (DETR [8]).}
\label{fig:ablation}
\end{figure}

\begin{table}[!ht]
	\centering
\resizebox{0.45\textwidth}{!}{
\begin{tabular}{lccc}
\toprule[1.5pt]
Method   & IDF1 $\uparrow$   & MOTA $\uparrow$       & IDS $\downarrow$  \\\hline
SAVi (RGB Recon.)  & 46.9\% & 32.3\%   & 12504 \\  
SAVi (Optical Flow) & 53.2\% & 34.9\% &  15394  \\  
\textbf{OC-MOT} &  \textbf{77.9\%} & \textbf{70.3\%}  & \textbf{5898}  \\
\bottomrule[1.5pt]
\end{tabular}
}
\caption{\textbf{Comparison with video object-centric models on FISHBOWL}. The SAVi models are trained with RGB reconstruction and optical flow reconstruction, respectively.}
	\label{tab:results}
\end{table}

\section{SAVi Baseline Analysis}
We reported the MOT results of SAVi on FISHBOWL dataset in the main paper. To handle the grouping problems in complicated scenes, such as over-segmentation (part-whole issue),  we trained the SAVi baseline with supervised DETR-style loss (with 6.25\% detection labels) on top of the self-supervised RGB reconstruction loss. 
 Another option to run the SAVi reconstruction loss is to reconstruct the optical flow and this setting is reported to work better on complicated scenes. As shown in Table~\ref{tab:results}, SAVi with optical flow achieves slightly better IDF1 and MOTA, and even more ID-Switches. The performance still has a large gap with our proposed OC-MOT. In the deep dive analysis, we noticed SAVi with optical flow reconstruction improves on the boundary accuracy for the segmentation masks, but shows similar issues on ID Switch and over-segmentation.

\section{Extension to Real-world Videos.}

In the main paper, we do not report metrics on standard benchmarks such as MOT17 that  heavily reflects the detection performance rather than object association performance, as majority of the bounding boxes can be correctly linked with spatial locations. Therefore it's not a good fit for evaluating our method because current un/weakly-supervised OC models are less capable of producing comparable results with SOTA supervised object detectors. We discussed this as limitation in Section 4.1 on KITTI, which is generalizable to MOT17. Instead, \textbf{one highlight of the work is our novel framework to learn object association in a self-supervised manner}. This is \textit{agnostic to the detection module}. We believe this contribution is quite interesting and novel in MOT community. To further apply OC-MOT to real-world videos, we replaced the object-centric model with SEEM~\cite{zou2023segment} that can accurately segment objects in real worlds. We trained OC-MOT with self-supervised loss on TAO (track any object) dataset and observed quite good tracking performance. In Figure~\ref{fig:reald} , we visualize the tracklets of persons and cars. Overall, OC-MOT performs strong object association without ID labels as long as the detection model can provide good object representations that contain enough information for tracking such as appearances and locations.
\begin{figure}[t]
	\centering
	\includegraphics[width=0.48\textwidth]{imgs/real.pdf}
\caption{\textbf{MOT results of OC-MOT on TAO dataset}. Only the tracklets of persons and cars are visualized.}
\label{fig:reald}
\end{figure}

\end{appendices}

\end{document}
