\section{Related Works}

\begin{figure*}[t]
	\centering
	\includegraphics[width=\textwidth]{imgs/method.pdf}
\caption{\textbf{Overview of OC-MOT}. It consists of two main modules. i) An index-merge module that adapts object-centric slots $\mathcal{S}_t$ into detection results $\mathcal{M}_{t}$ via two steps. First, index each slot into memory buffers by a learnable index matrix $\mathcal{I}_{t}$ indicating all the slot-to-memory assignments. Second, merge slots assigned to the same buffer by recalculating the attention weights masked by $\mathcal{I}_{t}$ backwards. ii) A object memory module that improves temporal consistency by rolling historical state forwards for object association. 
For MOT evaluation, we decode $\mathcal{M}_{t}$ to masks or bounding boxes via a frozen decoder in the object-centric grouping module.}
\label{fig:method}
\end{figure*}

\textbf{Unsupervised Object-centric Learning.} 
Unsupervised object-centric learning describes approaches which aim at tackling the binding problem of visual input signals to objects without additional supervision~\cite{greff2020binding}.
This is often accomplished using architectural inductive biases which force the model to encode input data into a set-structured bottleneck where object representations exert competition~\cite{eslami2016attend, locatello2020object, von2020towards} or exclusive binding to features~\cite{greff2017neural, greff2019multi, burgess2019monet, Engelcke2020GENESIS}.
Since their initial development on synthetic image data, these approaches have been extended to more complicated images by adapting the reconstruction objective~\cite{singh2022illiterate, seitzer2022bridging}, to the decomposition of 3D scenes~\cite{chen2021roots, niemeyer2021giraffe, stelzner2021decomposing}, to synthetic videos~\cite{Kosiorek2018SQAIR,Jiang2020SCALOR, crawford2020exploiting,kabra2021simone,kipf2022conditional,singh2022simple} and to real-world videos by exploiting additional modalities and priors~\cite{kipf2022conditional, bao2022discovering, elsayed2022savi++}.
Our work is most closely related to the last group of methods which apply object-centric learning methods to real-world videos, yet in contrast does not focus on the derivation of object-centric representations themselves.
Instead we focus on how object-centric representations can be used to perform multiple object tracking via long-term memory.
Our work presents the first dedicated memory module, which, independent of the origin of the object-centric representation can match occurrences of objects to previously discovered objects and thus track these over time.

% There is a rich literature of unsupervised object-centric representation learning from static images~\cite{chen2021roots,stelzner2021decomposing,seitzer2022bridging} or videos~\cite{kipf2022conditional,kabra2021simone,singh2022simple,crawford2020exploiting}.  These approaches typically rely on ``slot"-structured bottleneck to decompose scenes into a set of object representations. Most closely related to our works are object-centric models for videos such as SAVi~\cite{kipf2022conditional}, a sequential extension of Slot Attention~\cite{locatello2020object}.
% 
% dinosaur~\cite{seitzer2022bridging}: image-based,training object-centric features by reconstructing features that have high-level homogeneity of objects, extent to real-world dataset
% 
% References: chen2021roots (oc based 3d rendering),stelzner2021decomposing (oc based 3d rendering with NERF), seitzer2022bridging (DINOSAUR),  eslami2016attend (attend infer repeat), burgress2019 (MoNet iterative attention scheme), greff2019multi (Iodine), Lin2020SPACE (foreground/background segmentation, background into parts),  Engelcke2020Genesis (spatial GMM for images conditioned on slot structured bottleneck), Niemeyer (scene as composition of nerfs), Singh2020SLATE (slot attention applied to VQ code of image). 



\looseness=-1\textbf{Self-supervised MOT.} Most works study MOT in supervised settings, where the models are trained with object-level bounding box labels and ID labels~\cite{chu2019famnet, zhang2019robust,zhou2020tracking,zeng2022motr,cai2022memot}. Tracktor++~\cite{bergmann2019tracking} uses a ready-made detector\cite{girshick2015fast} to generate object bounding boxes and propagates them to the next frame as region proposals. MOTR~\cite{zeng2022motr} simultaneously performs object detection and association by autoregressively feeding a set of track queries into a Transformer decoder at the next timestep. To reduce the hand-label annotations, several recent approaches leverage the self-supervised signals to learn object associations from widely available unlabeled videos. For example, CRW~\cite{wang2019learning} and JSTG~\cite{zhao2021modelling} learns video correspondences by applying a cycle-consistent loss. Without  fine-tuning, these models track objects at inference time by propagating the annotations from the first frame.

Our work is mostly related to the unsupervised detect-to-track approaches that assume a robust detector is available. SORT~\cite{bewley2016simple} and IOU~\cite{bochinski2017high} associate detections using heuristic cues such as Kalman filters and intersection-of-union of bounding boxes. Such models do not need training but fail to handle scenarios with frequent occlusion and camera motion. A recent related method uses cross-input consistency~\cite{bastani2021self} to train the tracker: given two distinct inputs from the same video sequence, the model is encouraged to produce consistent tracks. Unfortunately, it suffers performance degradation once the detection boxes are not accurate, e.g., the grouping results from the object-centric model. 
For both supervised and unsupervised trackers, large amount of detection labels are required to train a strong detector. Additionally, supervised trackers require ID labels train feature representations. Overall, MOT is a label-heavy pipeline. Our work has the potential reduce the labeling cost. The heavy-lifting part of object localization and feature binding are done in a self-supervised way: on both backbone training and grouping training. 


\looseness=-1\textbf{Memory Models.} Memory models have been widely used in many video analysis and episodic tasks, such as  action recognition~\cite{wu2019long,jin2021temporal}, video object segmentation~\cite{lu2020video,oh2019video,lai2020mast}, video captioning~\cite{pei2019memory}, reinforcement learning~\cite{goyal2022retrieval}, physical reasoning~\cite{alias2021neural}, and code generation~\cite{liu2021retrieval}. These works utilize an external memory to store  features of prolonged sequences, which can be time-indexed for historical information integration. Recently, memory models have also been used to associate objects for video object tracking. MemTrack~\cite{yang2018learning} and SimTrack~\cite{fu2021stmtrack} retrieve useful information from the memory and combine it with the current frame. However,  they ignore the inter-object association and only focus on single object tracking. MeMOT~\cite{cai2022memot} builds a large spatial-temporal memory for MOT, which stores all the identity embeddings of tracked objects and aggregates the historical information as needed. As expected, it requires costly object ID labels for training the memory. In this paper, we propose a self-supervised memory that leverages the memory rollout for object association. In contrast to previous learnable memory modules, our approach does not write global information in the memory via gradient descent~\cite{trauble2022discrete} but rather maintains a running summary of the scene similar to~\cite{Goyal2021RIMs} (but with multi-head attention rollout). Different than~\cite{Goyal2021RIMs}, we explicitly enforce an assignment between objects and memory buffers with subsequent merging steps for MOT. 