%%%%%%%%% ABSTRACT
\begin{abstract}
Unsupervised object-centric learning methods allow the partitioning of scenes into entities without additional localization information and are excellent candidates for reducing the annotation burden of multiple-object tracking (MOT) pipelines. Unfortunately, they lack two key properties: objects are often split into parts and are not consistently tracked over time. In fact, state-of-the-art models achieve pixel-level accuracy and temporal consistency by relying on supervised object detection with additional ID labels for the association through time.
This paper proposes a video object-centric model for MOT. It consists of 
an index-merge module that adapts the object-centric slots into detection outputs and an object memory module that builds complete object prototypes to handle occlusions.  Benefited from object-centric learning, we only require sparse detection labels ($0\%$-$6.25\%$) for object localization and feature binding. Relying on our self-supervised Expectation-Maximization-inspired loss for object association, our approach requires no ID labels.
Our experiments significantly narrow the gap between the existing object-centric model and the fully supervised state-of-the-art and outperform several unsupervised trackers. Code is available at 
\href{https://github.com/amazon-science/object-centric-multiple-object-tracking}{https://github.com/amazon-science/object-centric-multiple-object-tracking}.
%\href{https://gitlab.aws.dev/zhaozixu/memory}
%{https://gitlab.aws.dev/zhaozixu/memory}.


% Unsupervised object-centric learning methods allow to partition scenes into entities without additional localization information. This replicates how humans naturally perceive and abstract the world. Advances in this field have enabled the scaling of the object-centric pipeline from simulated to real-world data, resulting in competitive performance on object discovery tasks. However, in computer vision tasks requiring pixel-level accuracy and temporal consistency, the unsupervised object-centric paradigm still lags behind fully supervised models. One such challenging task is multiple object tracking (MOT), widely used in video applications. The conventional MOT pipeline involves heavily supervised object detection training using bounding box labels, and heavily supervised association training using ID labels. Our proposed MOT pipeline, however, uses very sparse bounding box labels combined with self-supervised object-centric grouping loss, and no ID labels, relying instead on self-supervised Expectation-Maximization loss for association. Compared with existing video object-centric models, our approach includes an index-merge module that adapts the object-centric slots into detection outputs compatible with MOT metrics, as well as an object memory module that builds complete/amodal object prototypes to handle occlusions. Our experiments show that the proposed pipeline, using only $10\,\%$ bbox labels and $0\,\%$ ID labels for model training, can significantly narrow the gap between the existing object-centric model and the fully supervised state-of-the-art MOT model on both synthetic and real-world data, and outperforms several non-learnable trackers that also do not require ID labels.
\end{abstract}
