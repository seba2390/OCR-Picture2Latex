\section{Experiments}
%We compare our method with five baselines on the CATER~\cite{girdhar2019cater} and FISHBOWL~\cite{tangemann2021unsupervised} benchmarks. 
\looseness=-1We show that 1) OC-MOT consolidates ``objects"  in memory and greatly improves the temporal consistency of object-centric representations; 2) the gap between object-centric learning and MOT can be narrowed down by involving partial labels to improve the grouping performance; 3) the ablation studies demonstrate the effectiveness and feasibility of each module in the  framework. Finally, we turn to KITTI~\cite{Geiger2012CVPR} to discuss our limitations.


\begin{table*}[!ht]
	\centering
\resizebox{0.95\textwidth}{!}{
\begin{tabular}{lccccccccc}
\toprule[1.5pt]
Method  & Detection Label & ID Label  & IDF1 $\uparrow$   & MOTA $\uparrow$     & MT $\uparrow$    & ML $\downarrow$   & FP   $\downarrow$   & FN $\downarrow$    & IDS $\downarrow$  \\\hline
\multicolumn{10}{c}{CATER~\cite{girdhar2019cater}} \\ \hline \hline
SAVi~\cite{kipf2022conditional}   &  &    & 73.2\%  & 52.5\%   & 75.2\% & 21.2\%  &  305027 &  130810 &20352  \\        
IOU~\cite{bochinski2017high} &  &  &  83.0\% &  77.4\%  & 73.3\%  & 17.4\%  &  \textbf{35480} & 173595  & 8259  \\
SORT~\cite{bewley2016simple}   &   &   & 84.5\% & 79.2\%  & 71.8\% & 24.1\%  & 43097 & 148068 & 8219    \\     
Visual-Spatial~\cite{bastani2021self}  & & &  85.8\%     &  80.3\%      &   76.6\%     &     20.8\%      & 51348      &    129680  &     7562       \\
\textbf{OC-MOT}   &  &   & \textbf{88.6\%} & \textbf{82.4\%} &\textbf{82.3\%}       & \textbf{13.9\%} & 57792    & \textbf{105054}   &  \textbf{5658}  \\\hdashline
MOTR~\cite{zeng2022motr}   & 100\%   & 100\%    & 89.3\% & 83.3\%   &  84.8\%      &  4.9\%     & 60647  & 96746  & 3366   \\\hline
\multicolumn{10}{c}{FISHBOWL~\cite{tangemann2021unsupervised} } \\ \hline \hline
SAVi~\cite{kipf2022conditional} &6.25\%    &   & 46.9\% & 32.3\%  & 47.3\% & 15.1\%  & 122006 & \textbf{96710} & 12504 \\  
SORT~\cite{bewley2016simple}   &6.25\% & &68.4\% & 64.3\% & 42.6\%  & 31.9\%  & 30912 & 132434 & 15278  \\  
IOU~\cite{bochinski2017high} & 6.25\%  &  &  71.3\% &  66.6\%  & 11.0\%  & 62.7\%  &31672   &  135394 & 10306   \\
Visual-Spatial~\cite{bastani2021self}  &6.25\%   & &74.6\%    &  68.1\%      &  48.2\%       & 19.8\%   & 28845 & 131076 & 8754    \\
\textbf{OC-MOT} &6.25\%   & &  \textbf{77.9\%} & \textbf{70.3\%} &    \textbf{50.2\%}  &  \textbf{ 13.2\%}  & \textbf{14738}  & 136852 & \textbf{5898}  \\\hdashline
MOTR~\cite{zeng2022motr}   & 100\%   &  100\%  & 81.6\% & 79.8\%   &  58.3\%      &  10.1\%  & 9678   &  92862  & 4185   \\
\bottomrule[1.5pt]
\end{tabular}
}
\vspace{+0.05in}
\caption{\textbf{Evaluation results on CATER and FISHBOWL}. For CATER,  the object-centric grouping module is pre-trained without any label. For FISHBOWL, the grouping module is pre-trained with $6.25\%$ mask labels to improve the detection accuracy. The supervised MOTR~\cite{zeng2022motr} is trained with 100\% box labels and ID labels.
The best results of unsupervised trackers are marked in bold.}
	\label{tab:results}
\end{table*}

%\subsection{Datasets and Metrics}
\vspace{+2.5mm}
\noindent\textbf{Datasets.} CATER~\cite{girdhar2019cater} is a widely used synthetic video dataset for object-centric learning. It is rendered using a  library of 3D objects with various movements. Tracking multiple objects requires temporal reasoning about the long-term occlusions, a common issue in this dataset. FISHBOWL~\cite{tangemann2021unsupervised} consists of 20,000 training and 1,000 validation and test videos recorded from a publicly available WebGL demo of an aquarium, each with a resolution of 480Ã—320px and 128 frames. Compared to CATER, FISHBOWL records more complicated scenes and has even more severe object occlusions. Besides, we also work on the real-world driving dataset KITTI~\cite{Geiger2012CVPR} to analyze the limitation of the proposed object-centric framework.


\vspace{+2.5mm}
\noindent\textbf{Metrics.} Following the standard MOT evaluation protocols~\cite{ristani2016performance, milan2016mot16}, we use Identity F1 score (IDF1), Multiple-Object Tracking Accuracy (MOTA), Mostly Tracked (MT), Mostly Lost (ML), and Identity Switches (IDS) as the metrics. Specifically, IDF1 highlights the tracking consistency, and MOTA measures the  object coverage. To weight down the effect of detection accuracy and focus on the association performance, we set the IoU distance threshold as 0.7.
We also introduce Track mAP~\cite{dave2020tao}, which is more sensitive to identity switches by matching the object bounding boxes to ground-truth through the entire video using 3D IoU. 
 
%\subsection{Settings}
\begin{figure}[!t]
    \centering	\includegraphics[width=62mm]{imgs/grouping_figs.pdf}
\caption{Comparisons of different grouping module settings on FISHBOWL: a) self-supervised DINOSAUR has high object recall but over-segments on both background and big fishes, causing trouble to memory buffer initialization. b) Tuning DINOSAUR with supervised DETR loss and partial mask labels resolves the over-segmentation issue and filters out background slots. 
 }
\label{fig:group}
\end{figure}

\vspace{+2.5mm}
\noindent\textbf{Implementation details.} We train OC-MOT using the Adam optimizer~\cite{kingma2014adam} with a learning rate of $2\cdot10^{-4}$ and an exponentially decaying learning rate schedule. The models were trained on 8 NVIDIA GPUs with batchsize of 8. We set $\tau_{out}$ as 5 for buffer termination.
The IoU threshold $\tau_{iou}$ is set as $0.9$. For the experiments on CATER, we pretrain a SAVi model for object grouping without any annotation. We set $N=11$ and $M=15$.
The hyperparameters in the training loss  $\lambda_1, \lambda_2, \lambda_3$ are selected as $1$, $0.1$, $0$. For the experiments on FISHBOWL, we used a pretrained image-level DINOSAUR~\cite{seitzer2022bridging} as the grouping module and selected  $\lambda_1, \lambda_2, \lambda_3$ as $1$, $0$, $1$. We set $\lambda_2$ to $0$ due to GPU memory limitations when combining the EM loss computation with the high dimensional DINOSAUR features. We set $N= 24$ and $M=40$. In complex scenes of FISHBOWL, we noticed a performance drop due to more severe part-whole issues and over-segmentation on the background as illustrated in Figure~\ref{fig:group}. To avoid tracking background objects and reduce over-segmentation on big objects, we suggest further improving object-centric grouping by utilizing temporal sparse labels. To be more specific, we apply supervised DETR~\cite{carion2020end}-style loss on the decoded masks of slots. Since the object grouping loss already takes the heavy-lifting of discovering objects and parts, we only require very few mask labels to inject semantics about which objects are interesting and how to merge parts into a whole object. In practice, we utilized $6.25\,\%$ (randomly label 8 frames in 128-frame videos) mask labels for DINOSAUR pre-training, with both DETR loss and self-supervised reconstruction loss.  



\begin{table}[!ht]
\centering
\resizebox{80mm}{!}{
\begin{tabular}{lcccc}
\toprule[1.5pt]
\multirow{2}{*}{Method} & OC Metric & \multicolumn{3}{c}{MOT Metric} \\ \cmidrule(r){2-2} \cmidrule(r){3-5}
                        & FG-ARI $\uparrow$    & IDF1 $\uparrow$    & MOTA $\uparrow$   & Track mAP $\uparrow$ \\
                        \cmidrule(r){1-1}\cmidrule(r){2-2} \cmidrule(r){3-5}
SAVi                    & 90.2      & 72.3    & 52.5   & 42.8        \\
OC-MOT                  & \textbf{93.8}      & \textbf{88.6}    & \textbf{82.4}   & \textbf{66.2}        \\ \bottomrule[1.5pt]
\end{tabular}
}
\vspace{+0.05in}
\caption{Comparisons with video object-centric models on CATER. Note that FG-ARI~\cite{kipf2022conditional} is a commonly used OC metric.}
	\label{tab:results_cater}
\end{table}

\subsection{Comparison with the State-of-the-art Methods}
\noindent\textbf{Baselines.} We compare OC-MOT with one object-centric method (SAVi~\cite{kipf2022conditional}), three unsupervised MOT methods (IOU~\cite{bochinski2017high}, SORT~\cite{bewley2016simple}, and Visual-Spatial~\cite{bastani2021self}), and one fully supervised MOT method (MOTR~\cite{zeng2022motr}). For the SAVi evaluation, we remove the background slots and treat each slot prediction as a tracklet. When training SAVi on FISHBOWL, we also provide 6.25\% temporal sparse mask labels to be comparable to our own setting. For fair comparisons, we use the same pre-trained object-centric model (DINOSAUR with 6.25\% detection labels) as the detector for IOU, SORT, and Visual-Spatial.
MOTR utilizes a query-based transformer for both object detection and association but requires object-level annotations (both boxes and object ID) for model training. 
This model and its follow-ups have achieved SOTA results on several MOT benchmarks.

\begin{figure*}[!t]
	\centering
	\includegraphics[width=0.9\textwidth]{imgs/vis.pdf}
\caption{\textbf{MOT results on CATER and FISHBOWL.} We highlight the occlusion cases with colored masks. SAVi over-segments the objects (yellow arrows) and has ID switches after occlusions. In contrast, OC-MOT tracks objects more consistently over time.
 }
\label{fig:vis}
\end{figure*}

\vspace{+2.5mm}
\noindent\textbf{Results on CATER.}
As shown in Table \ref{tab:results}, OC-MOT substantially outperforms the video object-centric model and other unsupervised baselines. Our approach is also competitive with supervised MOTR~\cite{zeng2022motr} trained on expensive object-level annotations, yielding only slightly lower IDF1 and MOTA. OC-MOT can keep tracking more objects but produces fewer ID switches. For example, it achieves $82.3\,\%$ Mostly Tracked (MT) and $13.9\,\%$ Mostly Lost (ML), and shows only $5658$ IDS. Moreover, in Table~\ref{tab:results_cater}, SAVi achieves 90.2\% of FG-ARI but performs bad in terms of other MOT metrics such as 42.8\% Track mAP, indicating that the FG-ARI is not a good metric for measuring object-level temporal consistency.




\vspace{+2.5mm}
\noindent\textbf{Results on FISHBOWL.} FISHBOWL is a more challenging benchmark with serious occlusions and complicated backgrounds. 
In this scenario, SAVi~\cite{kipf2022conditional} tends to split the complicated background into multiple slots, causing the high number of false positive (FP).
Table~\ref{tab:results} shows that OC-MOT achieves state-of-the-art performance among the unsupervised tracking methods. By getting a much lower IDS number, our approach shows its advantage in solving the occlusion problem. The non-linear transformation in frequent occlusions cannot be handled by IoU-based association (IOU~\cite{bochinski2017high}) or Kalman filter (SORT~\cite{bewley2016simple}). 
Compared to supervised MOTR, what would like to highlight is the impressive association capability of OC-MOT. We point out that the lower IDF1 and MOTA are mainly caused by the detection limitation of existing OC models (e.g., DINOSAUR decoder predicts masks at a low feature resolution, making it hard to get pixel-level accuracy).

\begin{figure}[!h]
	\centering
	\includegraphics[width=82mm]{imgs/rollout.pdf}
\caption{\textbf{Visualization of memory rollout.} We show the object reconstructions decoded from the rollout representations. Each column denotes a memory buffer. The rollout predictions are consistent and complete, even when objects are partially occluded
%We observe very consistent rollouts, without ID switches, and well-segmented objects even when they are partially occluded.
 }
\label{fig:rollout}
\end{figure}

\subsection{Visualization}
The MOT results on the occlusion cases are visualized in 
Figure~\ref{fig:vis}. OC-MOT associates the slots from the object-centric model and
generates consistent predictions even when objects frequently interact with each other. Due to the severe occlusions, SAVi~\cite{kipf2022conditional} fails to track objects even using the track query as input, thereby causing more ID switches. Moreover, SAVi produces more false positives due to over-segmentation.

In Figure~\ref{fig:rollout}, we visualize the memory rollout results by decoding the representations to object reconstructions. The memory starts to roll out after the first frame, and, at $t=1$, we visualize the existing memory features. We can observe that the rollouts achieve good temporal consistency and, even more interestingly, that the memory can predict a  complete object even when it has been partially occluded.



\subsection{Ablation Studies}
\label{sec:abl_oc}

\noindent\textbf{Component analysis.}
Table~\ref{tab:component} compares different design choices for the key components in OC-MOT. For the index-merge module, a naive solution is to use a parameter-free dot-product to calculate the feature similarity, inspired by~\cite{rahaman2021dynamic}. As expected, it produces the worst association performance. A further option is to train one single MHA (i.e., two MHAs with shared weights) to cluster slots to buffers as in~\cite{Goyal2021RIMs}. To get the discrete index for object in and out logic, we still follow the indexing and merging steps yet only calculating the attention weights once. We observe that this model yields slightly lower IDF1 and MOTA than training two MHA modules. The latter choice is mathematically the similar but with higher module capacity. For the memory module, we compare utilizing the rollout module to only using the last tracks as the index query. Without aggregating the historical memory features, the association performance drops dramatically, indicating the necessity of building a memory to handle the MOT problem.

\begin{table}[!ht]
\centering
\resizebox{80mm}{!}{
\begin{tabular}{cc|ccc}
\toprule[1.5pt]
 \begin{tabular}[c]{@{}c@{}}Index-Merge\\ Module\end{tabular}    & \begin{tabular}[c]{@{}c@{}}Memory\\ Module\end{tabular} & IDF1 $\uparrow$  & MOTA $\uparrow$  & IDS $\downarrow$  \\ \hline
Dot-product & Rollout  & 72.0\% & 61.5\% & 22050 \\
 One MHA & Rollout       &  86.2\%    &80.5\%   &7655 \\
Two MHA & Last Tracks       &  77.2\%    & 68.8\%     & 16582\\\hdashline
Two MHA         & Rollout   & \textbf{88.6\%} & \textbf{82.4\%} & \textbf{5658}  \\ \bottomrule[1.5pt]
\end{tabular}
}
\vspace{+0.3cm}
\caption{Ablation on OC-MOT components on CATER.}
\label{tab:component}
\end{table}

\noindent\textbf{Effect of memory length.} In Table~\ref{tab:length}, we explore the effect of the memory length $T_{max}$ from 6 to 32. Note that $T_{max}$ equals the length of the training sequence. The tracking performance increases as $T_{max}$ grows. However, for longer videos, we should set a max length of memory due to hardware limitations. To make the model more applicable, we propose to reserve a short-term memory trained with sequences sampled by slow-to-fast pace. The various sampling rates produce both short-term and long-term information and, more importantly, include the occlusion cases during training. Quantitatively, this sampling strategy peaks in performance with $T_{max}=6$. Unless noted otherwise, we set $T_{max}$ to 6 as default in other experiments.

\begin{table}[!ht]
\centering
\resizebox{80mm}{!}{
\begin{tabular}{cc|ccc}
\toprule[1.5pt]
$T_{max}$  & Sequence Sampling                       & IDF1 $\uparrow$  & MOTA $\uparrow$  & IDS  $\downarrow$  \\ \hline
6  & Consecutive                    & 82.9\% & 76.3\% & 7601 \\
10  & Consecutive                    & 83.2\% & 76.5\% & 7524 \\
20  & Consecutive                    & 86.9\% & 78.1\% & 6230 \\
32 & Consecutive                    & 88.4\% & 82.1\%     &  5763      \\\hdashline
6  & Slow-Fast & \textbf{88.6\%} & \textbf{82.4\%} & \textbf{5658}   \\ \bottomrule[1.5pt]
\end{tabular}
}
\vspace{+0.3cm}
\caption{Ablation on the memory length on CATER.}
\label{tab:length}
\end{table}







\subsection{Limitations}


There exist some limitations of the proposed OC-MOT.

\vspace{+1mm}
\noindent\textbf{Inductive bias in the grouping architecture.} We apply the same DINOSAUR+DETR grouping module with 6.25\% temporally sparse mask labels to KITTI dataset. Figure \ref{fig:kitti} visualizes the grouping results. The cars can be detected but the predicted masks are not accurate, especially for far-away objects.
One reason is that DINOSAUR predicts masks at a feature resolution that is down-scaled 16 times from the original size. The architecture of the grouping module needs to be further improved considering multi-resolution inductive biases that have already been adopted in  supervised detection and segmentation pipelines. %This is not trivial for object-centric grouping modules in terms of the learning signal and 
We encourage researchers to develop stronger OC models with powerful detection performance but low labeling cost.


\begin{figure}[!h]
	\centering
	\includegraphics[width=83mm]{imgs/kitti_group.pdf}
\caption{DINOSAUR grouping results on KIITI. We observe that the masks are imprecise, espcially for objects that are far away.
 }
 \vspace{-2mm}
\label{fig:kitti}
\end{figure}

\vspace{+2.5mm}
\noindent\textbf{The model is not trained end-to-end.}
In this paper, we use the pre-trained OC model as a plug-n-play detector, which is supposed to handle different data flexibly. Potential future work is to extend OC-MOT into an end-to-end framework. The object prototype built in the memory may be useful as a prior for object discovery.