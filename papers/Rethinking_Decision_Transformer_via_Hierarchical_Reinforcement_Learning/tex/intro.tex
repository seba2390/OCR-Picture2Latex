
\section{Introduction}


One of the most remarkable characteristics observed in large sequence models, especially Transformer models, is the \emph{in-context learning} ability \citep{radford2019language,brown2020language,ramesh2021zero,gao2020making,akyurek2022learning,garg2022can,laskin2022context,lee2023supervised}. 
With an appropriate \emph{prompt}, a pre-trained transformer can learn new tasks without explicit supervision and additional parameter updates. 
\emph{Decision Transformer (DT)} is an innovative method that attempts to explore this idea for sequential decision making \citep{chen2021decision}.  
Unlike traditional \emph{reinforcement learning (RL)} algorithms, which learn a value function by bootstrapping or computing policy gradient, 
DT directly learns an autoregressive generative model from trajectory data using a causal transformer \citep{vaswani2017attention,radford2019language}.  
This approach allows leveraging existing transformer architectures widely employed in language and vision tasks that are easy to scale, and benefitting from a substantial body of research focused on stable training of transformer  \citep{radford2019language,brown2020language,fedus2022switch,chowdhery2022palm}.


DT is trained on trajectory data, 
$(R_0, s_0, a_0, \dots, R_T, s_T, a_T)$, where $R_t$ is the \emph{return-to-go}, the sum of future rewards along the trajectory starting from time step $t$.  
This can be viewed as learning a model that predicts \emph{what action should be taken at a given state in order to make so many returns}.  Following this, we can view the return-to-go prompt as a \emph{switch}, guiding the model in making decisions at test time. 
If such a model can be learned effectively and generalized well even for out-of-distribution return-to-go, it is reasonable to expect that DT can generate a better policy by prompting a higher return.  
Unfortunately, this seems to demand a level of generalization ability that is often too high in practical sequential decision-making problems.   
In fact, the key challenge facing DT is how to improve its robustness to the underlying data distribution, particularly when learning from trajectories collected by policies that are not close to optimal.  
Recent studies have indicated that for problems requiring the \emph{stitching ability}, referring to the capability to integrate suboptimal trajectories from the data, DT cannot provide a significant advantage compared to behavior cloning \citep{fujimoto2021minimalist,emmons2021rvs,kostrikov2022offline,yamagata2023q,badrinath2023waypoint,xiao2023sample}. 
This further confirms that a naive return-to-go prompt is not good enough for solving complex sequential decision-making problems. 

Recent progress on large language models showed that carefully tuned prompts, either human-written or self-discovered by the model, significantly boost the performance of transformer models  \citep{lester2021power,singhal2022large,zhang2022automatic,wei2022chain,wang2022self,yao2023tree,liu2023chain}. %,zhou2022conditional,zhou2022learning
In particular, it has been observed that the ability to perform complex reasoning naturally emerges in sufficiently large language models when they are presented with a few chain of thought demonstrations as exemplars in the prompts \citep{wei2022chain,wang2022self,yao2023tree}. 
Driven by the significance of these works in language models, a question arises: 
\emph{For RL, is it feasible to learn to automatically tune the prompt, such that a transformer-based sequential decision model is able to learn optimal control policies from offline data?}
This paper attempts to address this problem. Our main contributions are:

\begin{itemize}[leftmargin=0.5cm]

\item We present a generalized framework for studying decision-making through sequential modeling by connecting it with \emph{Hierarchical Reinforcement Learning} \citep{nachum2018data}: 
a high-level policy first suggests a prompt for the current state, a low-level policy subsequently generates an action conditioned on the given prompt. 
We show DT can be recovered as a special case of this framework. 

\item We investigate when and why DT fails in terms of stitching sub-optimal trajectories. To overcome this drawback of DT, we investigate how to jointly optimize the high-level and low-level policies to enable the stitching capability. This further leads to the development of two new algorithms for offline RL. %{Value-prompted Autotuned Decision Transformer} and {Goal-prompted Autotuned Decision Transformer}. 
The joint policy optimization framework is our key contribution compared to previous studies on improving transformer-based decision models \citep{yamagata2023q,wu2023elastic,badrinath2023waypoint}. 

\item We provide experiment results on several offline RL benchmarks, including locomotion control, navigation and robotics, to demonstrate the effectiveness of the proposed algorithms. Additionally, we conduct thorough ablation studies on the key components of our algorithms to gain deeper insights into their contributions.   
Through these ablation studies, we assess the impact of specific algorithmic designs on the overall performance.

\end{itemize}

\iffalse
For example, \citet{wei2022chain} explore the idea of using \emph{chain-of-thought prompting}, which consists of a series of intermediate demonstrations that leads to the final answer. 
\citet{wang2022self} and \citet{yao2023tree} further exploit this topic and investigate how to self-construct better chain-of-thought prompts through majority votes or classical planning algorithms.   
Despite all the differences, an important observation is that the ability to decompose a problem into several sub-tasks then solve them one-by-one is the key for transformer models to success in complex reasoning problems. 
\fi



\begin{figure}[!t]
\centering
\includegraphics[scale=0.7]{method_figs/model_structure.pdf}
\caption{ADT architecture. 
The high-level policy generates prompts that inform the low-level policy to make decisions. 
We concatenate prompts with states instead of treating them as separate tokens. 
%The concatenated vectors and actions are fed into linear embeddings with a positional embedding added. 
Embeddings of tokens are fed into a causal transformer that predicts actions auto-regressively.}
\label{fig:model_structure}
% \vspace{-0.5cm}
\end{figure}%

