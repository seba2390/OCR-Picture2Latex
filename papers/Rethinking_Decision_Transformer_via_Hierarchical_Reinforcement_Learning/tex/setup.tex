


\section{Preliminaries}

\subsection{Offline Reinforcement Learning}

%Let $\sR$ be the set of real numbers. 
We consider Markov Decision Process (MDP) determined by $M=\{\cS, \cA, P, r, \gamma\}$ \citep{puterman2014markov}, 
where $\cS$ and $\cA$ represent the state and action spaces. The discount factor
is given by $\gamma\in[0, 1)$, 
$r:\cS\times\cA\rightarrow \sR$ denotes the reward function,  
$P:\cS\times\cA\rightarrow \Delta(\cS)$ defines the transition dynamics\footnote{We use $\Delta(\cX)$ to denote the set of probability distributions over $\cX$ for a finite set $\cX$.}. 
Let $\tau=(s_0, a_0, r_0, \dots, s_T, a_T, r_T)$ be a trajectory. Its \emph{return} is defined as the discounted sum of the rewards along the trajectory: $R = \sum_{t=0}^{T} \gamma^{t} r_{t}$. 
Given a policy $\pi:\cS\rightarrow \Delta(\cA)$, we use $\EE^\pi$ to denote the expectation under the distribution induced by the interconnection of $\pi$ and the environment.  
The \emph{value function} specifies the future discounted total reward obtained by following policy $\pi$, 
\begin{align}
V^\pi(s) = \EE^\pi\left[ \sum_{t=0}^\infty \gamma^t r(s_t, a_t) \Big| s_0 = s\right]\, ,
\end{align}
There exists an \emph{optimal policy} $\pi^*$ that maximizes values for all states $s\in\cS$. 

In this work, we consider learning an optimal control policy from previously collected offline dataset, $\dataset = \{ \tau_i \}^{n-1}_{i=0}$, consisting of $n$ trajectories.  
Each trajectory is generated by the following procedure:
an initial state $s_0\sim\mu_0$ is sampled from the initial state distribution $\mu_0$; for time step $t\geq 0$, $a_t\sim \pi_{\cD}$, $s_{t+1} \sim P(\cdot|s_t, a_t), r_t=r(s_t, a_t)$, this process repeats until it reaches the maximum time step of the environment. Here $\pi_\dataset$ is an \emph{unknown behavior policy}.  
In offline RL, 
the learning algorithm can only take samples from $\dataset$ without collecting new data from the environment \citep{levine2020offline}.  

\iffalse
The \emph{in-sample optimal value function} considers the optimal value constrained to the dataset \citep{fujimoto2018addressing,kostrikov2022offline,xiaosample}, 
\begin{align}
V^*_{\cD}(s) = \max_{a: \pi_{\cD}(a | s) > 0} r(s,a) + \gamma \EE_{s'\sim P(\cdot | s,a)} \left[ V^*_{\cD} (s') \right]\, , 
\label{eq:in-sample-v}
\end{align}
Intuitively, one can view the offline dataset $\cD$ as an {empirical MDP}, which only contains transitions sampled from the behavior policy.  $V^*_{\cD}$ defines the optimal value of this empirical MDP. \citet{kostrikov2022offline} and \citet{xiaosample} propose algorithms for learning this value. 
\fi


\subsection{Decision Transformer}

Decision Transformer (DT) is an extraordinary example that bridges sequence modeling with decision-making \citep{chen2021decision}. 
It shows that a sequential decision-making model can be made through minimal modification to the transformer architecture \citep{vaswani2017attention,radford2019language}. 
It considers the following trajectory representation that enables autoregressive training and generation: 
\begin{align}
\tau = \left( \widehat{R}_0, s_0, a_0, \widehat{R}_1, s_1, a_1, \dots, \widehat{R}_T, s_T, a_T\right)\, .
\label{eq:dt-traj}
\end{align}
Here $\widehat{R_t} = \sum_{i=t}^{T}  r_i$ is the \emph{returns-to-go} starting from time step $t$. 
We denote $\pi_{ \mathrm{DT}}(a_t|s_t, \widehat{R}_t, \tau_t)$ the DT policy, where $\tau_{t}=(s_0, a_0, \widehat{R}_0, \dots, s_{t-1} a_{t-1}, \widehat{R}_{t-1})$\footnote{We define $\tau_0$ the empty sequence for completeness. } is the sub-trajectory before time step $t$.  
As pointed and verified by \citet{lee2023supervised}, 
$\tau_t$ can be viewed as as a \emph{context} input of a policy, which fully takes advantages of the in-context learning ability of transformer model for better generalization \citep{akyurek2022learning,garg2022can,laskin2022context}.   

%For training, all trajectories in $\cD$ are relabelled as  \plaineqref{eq:dt-traj}.  
DT  assigns a desired returns-to-go ${R}^0$, together with an initial state $s_0$ are used as the initialization input of the model. After executing the generated action,  DT decrements the desired return by the achieved reward and continues this process until the episode reaches termination. 
%An important observation is that the Return-to-go can be viewed as a prompt of the model. 
%\citet{furuta2021generalized} generalize this idea by replacing returns-to-go with hindsight information signals as the input of the autoregressive model. 
\citet{chen2021decision} argues that the conditional prediction model is able to perform policy optimization without using dynamic programming. 
However, recent works observe that
DT often shows inferior performance compared to dynamic programming based offline RL algorithms when the offline dataset consists of sub-optimal trajectories \citep{fujimoto2021minimalist,emmons2021rvs,kostrikov2022offline}. 
%In the next section, we discuss why DT might fails in practice. 



\iffalse
\subsubsection{Limitations of Decision Transformer}
\label{sec:dt-limitations}

\citet{chen2021decision} argues that this autoregressive model can also perform policy optimization without using dynamic programming. 
However, recent works have reported contradictory observations, 
DT often demonstrate inferior performance compared to dynamic-learning based offline RL algorithms when the offline dataset consists of sub-optimal trajectories \citep{fujimoto2021minimalist,emmons2021rvs,kostrikov2022offline}. 
To better understand this phenomenon, we re-examine the illustrative example shown in  Figure 2 of \citet{chen2021decision}. 
The dataset comprises random walk trajectories and their associated per-state returns-to-go.  
Let $\pi_{ \mathrm{DT}}(a_t|s_t, \widehat{R}_t, \tau_t)$ be the policy learned by DT, where we denote $\tau_{t}=(s_0, a_0, \widehat{R}_0, \dots, s_{t-1} a_{t-1}, \widehat{R}_{t-1})$\footnote{We define $\tau_0$ the empty sequence for completeness. }. 
Intuitively, this model learns which action to take at time step $t$ given the history $\tau_t$ in order to achieve ${\widehat{R}_t}$.  
Suppose that $\pi_{\mathrm{DT}}$ can perfectly memorize all trajectory information contained in the offline dataset.   
In this case, 
we can show that the return-to-go prompt in fact acts as a \emph{switch} to guide the model to make decisions.  
Let $\cT(s)$ be the set of trajectories starting from $s$ contained in the data, $R(\tau)$ be the return of  trajectory for $\tau\in\cT(s)$.  
Given an observed return-to-go prompt $R'\in\{ R(\tau), \tau\in\cT(s) \}$, $\pi_{\mathrm{DT}}$ decides which action to take leading towards the trajectory $\tau$. 
Accordingly, by providing $R^*(s)=\max_{\tau\in\cT(s)} R(\tau)$, we should expect DT to follow the optimal observed trajectory following the switch. 

Several issues of DT can be observed here. \emph{First}, $R^*$ is not known. When deploying the policy, the initial return-to-go prompt $R^0$ is hand-picking, which might not be consistent with the offline data. This requires the model $\pi_{\mathrm{DT}}$ generalizes extremely well for unobserved return-to-go and decisions. 
\emph{Second}, even though $R^*$ is known for every state in the data, memorizing trajectory information is still not enough for obtaining the stitching ability. 
To understand this, 
consider a simple example with two trajectories $a\rightarrow b\rightarrow c$, and $d\rightarrow b\rightarrow e$. Suppose that the state $e$ leads to a return of 10, while $c$ leads to a return of 0. 
In this case, using 10 as the return-to-go prompt at state $b$, DT should be able to switch to the desired trajectory. 
However, the information that "leaning towards $c$ can achieve a return of 10" has not been received during training at state $a$ as the trajectory $a\rightarrow b\rightarrow e$ does not exist. 
If the offline data contains another trajectory starting from $a$ and leading to a mediocre return (e.g. 1), DT might switch to that trajectory at $a$, missing a more promising path. 
This indicates that some form of information backpropagation is still required  for transformer-based decision models.  

\fi

