\section{Implementation Details}
\label{sec:appendix-exp-details}
\subsection{Environments}
\paragraph{MuJoCo}
 For the MuJoCo framework, we incorporate nine version 2 (v2) datasets. These datasets are generated using three distinct behavior policies: '-medium', '-medium-play', and '-medium-expert', and span across three specific tasks: 'halfcheetah', 'hopper', and 'walker2d'.  
 
\paragraph{AntMaze}
The AntMaze represents a set of intricate, long-horizon navigation challenges. This domain uses the same umaze, medium, and large mazes from the Maze2D domain, but replaces the agent with an 8-DoF Ant robot from the OpenAI Gym MuJoCo benchmark. For the 'umaze' dataset, trajectories are generated with the Ant robot starting and aiming for fixed locations. To introduce complexity, the "diverse" dataset is generated by selecting random goal locations within the maze, necessitating the Ant to navigate from various initial positions. Meanwhile, the "play" dataset is curated by setting specific, hand-selected initial and target positions, adding a layer of specificity to the task. We employ six version 2 (v2) datasets which include ‘-umaze', ‘-umaze-diverse', ‘-medium-play', ‘-medium-diverse', ‘-large-play', and ‘-large-diverse' in our experiments.

\paragraph{Franka Kitchen} In the Franka Kitchen environment, the primary objective is to manipulate a set of distinct objects to achieve a predefined state configuration using a 9-DoF Franka robot. The environment offers multiple interactive entities, such as adjusting the kettle's position, actuating the light switch, and operating the microwave and cabinet doors, inclusive of a sliding mechanism for one of the doors. For the three principal tasks delineated, the ultimate objective comprises the sequential completion of four salient subtasks: (1) opening the microwave, (2) relocating the kettle, (3) toggling the light switch, and (4) initiating the sliding action of the cabinet door. In conjunction, three comprehensive datasets have been provisioned. The '-complete' dataset encompasses demonstrations where all four target subtasks are executed in a sequential manner. The ‘-partial’ dataset features various tasks, but it distinctively includes sub-trajectories wherein the aforementioned four target subtasks are sequentially achieved. The ‘-mixed’ dataset captures an assortment of subtask executions; however, it is noteworthy that the four target subtasks are not completed in an ordered sequence within this dataset.  We utilize these datasets in our experiments.



\subsection{Hyper-parameters and Implementations}




\begin{table*}[!htbp]
\centering
\caption{ADT Actor (Transformer) Hyper-parameters}
\scalebox{0.9}{
\begin{tabular}{c|ll}
\toprule
                              & Hyper-parameter             & Value                        \\
\midrule
\multirow{12}{*}{Architecture} & Hidden layers        & 3                            \\
                              & Hidden dim           & 128                          \\
                              &  Heads num & 1                 \\
                               &  Clip grad & 0.25                 \\
                               &  Embedding dim & 128                 \\
                               &  Embedding dropout & 0.1                 \\
                               &  Attention dropout & 0.1                 \\
                               &  Residual dropout  & 0.1                 \\
                              &   Activation function  & GeLU                                      \\ 
                             
                              & Sequence length             &  20 (V-ADT),  10 (G-ADT)           \\
                              & G-ADT Way Step                  &  20 (kitchen-partial, kitchen-mixed),  30 (Others)           \\

\midrule
\multirow{8}{*}{Learning}    & Optimizer                  & AdamW                         \\
                              & Learning rate        & 1e-4                         \\
                              & Mini-batch size            & 256                          \\
                              & Discount factor            & 0.99                         \\
                              & Target update rate         & 0.005                         \\
                               & Value prompt scale         & 0.001 (Mujoco) 1.0  (Others)   \\
                              &   Warmup steps   & 10000                               \\
                              &   Weight decay   & 0.0001                              \\
                              & Gradient Steps             & 100k (G-ADT, AntMaze), 1000k (Others) \\
\bottomrule                
\end{tabular}
}
\label{tab:ADT-Hyperparameters}
\end{table*}




We provide the lower-level actor's hyper-parameters used in our experiments in Table \ref{tab:ADT-Hyperparameters}. Most hyper-parameters are set following the default configurations in DT. For the inverse temperature used in calculating the AWR loss of the lower-level actor in V-ADT, we set it to {1.0, 3.0, 6.0, 6.0, 6.0, 15.0} for antmaze-{'umaze', 'umaze-diverse',  'medium-diverse', 'medium-play', 'large-diverse', 'large-play'} dataset, respectively; for other datasets, it is set 3.0. As for G-ADT, the inverse temperature is set to 1.0 for all the datasets.  For the critic used in V-ADT and G-ADT, we follow the default architecture and learning settings in IQL \citep{kostrikov2022offline} and HIQL \citep{park2023hiql}, respectively.  

The implementations of ADT is based on CORL repository \citep{tarasov2022corl}. A key different between the implementation of ADT and DT is that we follow the way in \citep{badrinath2023waypoint} that we concatenate the (scaled) prompt and state, then the concatenated information and the action are treated as two tokens per timestep. In practice, we pretrain the critic for ADT, then the critic is used to train the ADT actor. For each time of evaluation, we run the algorithms for 10 episodes for MuJoCo datasets, 50 episodes for Kitchen datasets, and 100 episodes for AntMaze datasets.

\section{IQL and HIQL}

Implicit Q-learning (IQL) \citep{kostrikov2022offline} offers an approach to avoid out-of-sample action queries. This is achieved by transforming the traditional max operator in the Bellman optimality equation to an expectile regression framework. More formally, IQL constructs an action-value function $Q(s, a)$ and a corresponding state-value function $V(s)$. These are governed by the loss functions:


\begin{align}
& \mathcal{L}_V=\mathbb{E}_{(s, a) \sim \mathcal{D}}\left[L_2^\tau\left(\bar{Q}(s, a)-V(s)\right)\right], \\
& \mathcal{L}_Q=\mathbb{E}_{\left(s, a, s^{\prime}\right) \sim \mathcal{D}}\left[\left(r(s, a)+\gamma V\left(s^{\prime}\right)-Q(s, a)\right)^2\right],
\end{align}



Here, $\mathcal{D}$ represents the offline dataset, $\bar{Q}$ symbolizes the target Q network, and $L_2^\tau$ is defined as the expectile loss with a parameter constraint $\tau \in[0.5,1)$ and is mathematically represented as $L_2^\tau(x)=|\tau-\sI(x<0)| x^2$. Then the policy is extracted with a simple advantage-weighted behavioral cloning procedure resembling supervised learning:
\begin{align}
J_{\pi}=\mathbb{E}_{\left(s, a, s'\right) \sim \mathcal{D}}\left[\exp \left(\beta \cdot \tilde{A}\left(s, a\right)\right) \log \pi \left(a \mid s\right)\right],
\end{align}


where $\tilde{A}\left(s, a\right) = \bar{Q}(s,a) - V(s)$.

Building on this foundation, Hierarchical Implicit Q-Learning \citep{park2023hiql} introduces an action-free variant of IQL that facilitates the learning of an offline goal-conditioned value function $V(s, g)$:
\begin{align}
\mathcal{L}_V=\mathbb{E}_{\left(s, s^{\prime}\right) \sim \mathcal{D}, g \sim p(g \mid \tau)}\left[L_2^\tau\left(r(s, g)+\gamma \bar{V}\left(s^{\prime}, g\right)-V(s, g)\right)\right]
\end{align}
where $\bar{V}$ denotes the target Q network. Then a high-level policy $\pi_{h}^h\left(s_{t+k} \mid s_t, g\right)$, which produces optimal k-steps jump, i.e., $k$-step subgoals $s_{t+k}$, is trained via:
\begin{align}
J_{\pi^h}=\mathbb{E}_{\left(s_t, s_{t+k}, g\right)}\left[\exp \left(\beta \cdot \tilde{A}^h\left(s_t, s_{t+k}, g\right)\right) \log \pi^h\left(s_{t+k} \mid s_t, g\right)\right],
\end{align}

where $\beta$ represents the inverse temperature hyper-parameter, and the value $\tilde{A}^h\left(s_t, s_{t+k}, g\right)$ is approximated using $V\left(s_{t+k}, g\right)-V\left(s_t, g\right)$. Similarly, a low-level policy is trained to learn to reach the sub-goal $s_{t+k}$:
\begin{align}
J_{\pi^l}=\mathbb{E}_{\left(s_t, a_t, s_{t+1}, s_{t+k}\right)}\left[\exp \left(\beta \cdot \tilde{A}^l\left(s_t, a_t, s_{t+k}\right)\right) \log \pi^l\left(a_t \mid s_t, s_{t+k}\right)\right],
\end{align}

where the value $\tilde{A}^l\left(s_t, a_t, s_{t+k}\right)$ is approximated using $V\left(s_{t+1}, s_{t+k}\right)-V\left(s_t, s_{t+k}\right)$. 

For a comprehensive exploration of the methodology, readers are encouraged to consult the original paper.







\section{Complete Experimental Results}
Here we provide the learning curves of our methods on all selected datasets.


\begin{figure}[!htbp]
\centering

\begin{minipage}[t]{0.3\textwidth}
\centering
\includegraphics[width=1\textwidth]{app_figs/halfcheetah-medium-v2_VDT_main_results.pdf}
\end{minipage}
\begin{minipage}[t]{0.3\textwidth}
\centering
\includegraphics[width=1\textwidth]{app_figs/halfcheetah-medium-replay-v2_VDT_main_results.pdf}
\end{minipage}
\begin{minipage}[t]{0.3\textwidth}
\centering
\includegraphics[width=1\textwidth]{app_figs/halfcheetah-medium-expert-v2_VDT_main_results.pdf}
\end{minipage}


\begin{minipage}[t]{0.3\textwidth}
\centering
\includegraphics[width=1\textwidth]{app_figs/hopper-medium-v2_VDT_main_results.pdf}
\end{minipage}
\begin{minipage}[t]{0.3\textwidth}
\centering
\includegraphics[width=1\textwidth]{app_figs/hopper-medium-replay-v2_VDT_main_results.pdf}
\end{minipage}
\begin{minipage}[t]{0.3\textwidth}
\centering
\includegraphics[width=1\textwidth]{app_figs/hopper-medium-expert-v2_VDT_main_results.pdf}
\end{minipage}

\begin{minipage}[t]{0.3\textwidth}
\centering
\includegraphics[width=1\textwidth]{app_figs/walker2d-medium-v2_VDT_main_results.pdf}
\end{minipage}
\begin{minipage}[t]{0.3\textwidth}
\centering
\includegraphics[width=1\textwidth]{app_figs/walker2d-medium-replay-v2_VDT_main_results.pdf}
\end{minipage}
\begin{minipage}[t]{0.3\textwidth}
\centering
\includegraphics[width=1\textwidth]{app_figs/walker2d-medium-expert-v2_VDT_main_results.pdf}
\end{minipage}

\begin{minipage}[t]{0.3\textwidth}
\centering
\includegraphics[width=1\textwidth]{app_figs/antmaze-umaze-v2_VDT_main_results.pdf}
\end{minipage}
\begin{minipage}[t]{0.3\textwidth}
\centering
\includegraphics[width=1\textwidth]{app_figs/antmaze-medium-play-v2_VDT_main_results.pdf}
\end{minipage}
\begin{minipage}[t]{0.3\textwidth}
\centering
\includegraphics[width=1\textwidth]{app_figs/antmaze-large-play-v2_VDT_main_results.pdf}
\end{minipage}

\begin{minipage}[t]{0.3\textwidth}
\centering
\includegraphics[width=1\textwidth]{app_figs/antmaze-umaze-diverse-v2_VDT_main_results.pdf}
\end{minipage}
\begin{minipage}[t]{0.3\textwidth}
\centering
\includegraphics[width=1\textwidth]{app_figs/antmaze-medium-diverse-v2_VDT_main_results.pdf}
\end{minipage}
\begin{minipage}[t]{0.3\textwidth}
\centering
\includegraphics[width=1\textwidth]{app_figs/antmaze-large-diverse-v2_VDT_main_results.pdf}
\end{minipage}

\begin{minipage}[t]{0.3\textwidth}
\centering
\includegraphics[width=1\textwidth]{app_figs/kitchen-complete-v0_VDT_main_results.pdf}
\end{minipage}
\begin{minipage}[t]{0.3\textwidth}
\centering
\includegraphics[width=1\textwidth]{app_figs/kitchen-partial-v0_VDT_main_results.pdf}
\end{minipage}
\begin{minipage}[t]{0.3\textwidth}
\centering
\includegraphics[width=1\textwidth]{app_figs/kitchen-mixed-v0_VDT_main_results.pdf}
\end{minipage}



\caption{Learning curves of V-ADT.}
\label{fig:learning_curves}
\end{figure}


\begin{figure}[!htbp]
\centering


\begin{minipage}[t]{0.3\textwidth}
\centering
\includegraphics[width=1\textwidth]{app_figs/antmaze-umaze-v2_GoalDT_main_results.pdf}
\end{minipage}
\begin{minipage}[t]{0.3\textwidth}
\centering
\includegraphics[width=1\textwidth]{app_figs/antmaze-medium-play-v2_GoalDT_main_results.pdf}
\end{minipage}
\begin{minipage}[t]{0.3\textwidth}
\centering
\includegraphics[width=1\textwidth]{app_figs/antmaze-large-play-v2_GoalDT_main_results.pdf}
\end{minipage}

\begin{minipage}[t]{0.3\textwidth}
\centering
\includegraphics[width=1\textwidth]{app_figs/antmaze-umaze-diverse-v2_GoalDT_main_results.pdf}
\end{minipage}
\begin{minipage}[t]{0.3\textwidth}
\centering
\includegraphics[width=1\textwidth]{app_figs/antmaze-medium-diverse-v2_GoalDT_main_results.pdf}
\end{minipage}
\begin{minipage}[t]{0.3\textwidth}
\centering
\includegraphics[width=1\textwidth]{app_figs/antmaze-large-diverse-v2_GoalDT_main_results.pdf}
\end{minipage}

\begin{minipage}[t]{0.3\textwidth}
\centering
\includegraphics[width=1\textwidth]{app_figs/kitchen-complete-v0_GoalDT_main_results.pdf}
\end{minipage}
\begin{minipage}[t]{0.3\textwidth}
\centering
\includegraphics[width=1\textwidth]{app_figs/kitchen-partial-v0_GoalDT_main_results.pdf}
\end{minipage}
\begin{minipage}[t]{0.3\textwidth}
\centering
\includegraphics[width=1\textwidth]{app_figs/kitchen-mixed-v0_GoalDT_main_results.pdf}
\end{minipage}


\caption{Learning curves of G-ADT.}
\label{fig:learning_curves}
\end{figure}





\section{Visualization of decision-making process of G-ADT}
\begin{figure}[!htbp]
\centering
\includegraphics[scale=0.75]{app_figs/visual_30_356.pdf}
\caption{Example of decision-making process of G-ADT in antmaze-large-play-v2 environments. We present some snapshots within an episode. The red circle represents the sub-goal given by the prompt policy. The pentagram indicates the target position to arrive.}
\label{fig:visualization}
\end{figure}%

