
\section{Conclusion}


We propose to rethink transformer-based decision models through a hierarchical decision-making framework.  
Armed with this, we introduce Autotuned Decision Transformer (ADT), 
which jointly optimizes the hierarchical policies for better performance when learning from sub-optimal data. 
On standard offline RL benchmarks, we show ADT significantly outperforms previous transformer-based decision making algorithms. 

Our primary focus for future work is to investigate the following problems. 
\emph{First}, besides employing values and sub-goals as latent actions generated by the high-level policy, other options for latent actions in hierarchical RL encompass skills \citep{ajay2020opal} and options \citep{sutton1999between}. We would like to investigate the potential extensions of ADT by incorporating skills and options. 
\emph{Second}, according to the reward hypothesis, goals can be conceptualized as the maximization of expected value through the cumulative sum of a reward signal \citep{silver2021reward,bowling2023settling}. Can we establish a unified framework that bridges value-prompted ADT and goal-prompted ADT? 
\emph{Finally}, according to our experiments, the advantages of substituting conventional architectures with transformer models in RL remain uncertain. 
Previous studies have indicated that the incorporation of transformers in RL is most advantageous when dealing with extensive and diverse datasets \citep{chebotar2023q}. 
With this in mind, we intend to apply ADT to create foundational decision-making models for learning multi-modal and multi-task policies.


