\section{Experiment}

We investigate three primary questions in our experiments. 
\emph{First}, how well does ADT perform on offline RL tasks compared to prior DT-based methods? 
\emph{Second}, is it essential to auto-tune prompts for transformer-based decision model?  
\emph{Three}, what is the influence of various implementation details within an ADT on its overall performance?
We refer readers to \cref{sec:appendix-exp-details} for additional details and supplementary experiments.

\if0
In this section, we evaluate DTHL and aim to answer these questions: 
% \begin{enumerate}
% 相比于SOTA policy-decoupled data augmentation技术而言，HIPODE可以表现出一致的优势吗？HIPODE中negative sampling and transition selector两个组件起到怎样的作用？相比于传统的policy-dependent data augmentation method，HIPODE这种policy-decoupled data augmentation算法为什么会表现更优？
\begin{enumerate}[leftmargin=*,noitemsep,topsep=0pt]
\item[] \textbf{Q1}: How does the performance of DTHL compare to existing DT-based methods and prevalent offline RL techniques?
\item[] \textbf{Q2}: Is it feasible to enhance the DT performance through manual prompt tuning, enabling it to achieve the benchmarks set by DTHL?
\item[] \textbf{Q3}: What is the relative significance of individual components (i.e., prompts, RL training regime and tokenization strategies) within the DTHL framework?
\item[] \textbf{Q4}: Does DTHL approach the anticipated performance upper bound?
\end{enumerate}
  % \item \label{q4} What is the weakness of policy-dependent data augmentation methods?
% \end{enumerate}
% 
In the following, we answer \textbf{Q1} in Section \ref{sec:main_results}, showing the effectiveness and superiority of DTHL. Then, we compare DTHL and DT's performance with various prompts in Section  \ref{sec:manual_prompt} to answer \textbf{Q2}. We answer \textbf{Q3} in Section \ref{sec:efficacy_of_component} by ablating the efficacy of prompts and RL traing regimes. Finally, we answer \textbf{Q4} in Section \ref{sec:oracle} to further evaluate the performance of DTHL.

\fi


%\subsection{Experimental Setups}

\paragraph{Benchmark Problems}
%We subject both V-ADT and G-ADT to a series of rigorous experimental evaluations. 
We leverage datasets across several domains including Gym-Mujoco, AntMaze, and FrankaKitchen from the offline RL benchmark D4RL \citep{fu2020d4rl}.  For Mujoco, the offline datasets are generated using three distinct behavior policies: `-medium', `-medium-play', and `-medium-expert', and span across three specific tasks: `halfcheetah', `hopper', and `walker2d'. 
The primary objective in long-horizon navigation task AntMaze is to guide an 8-DoF Ant robot from its starting position to a predefined target location. We employ six datasets which include `-umaze', `-umaze-diverse', `-medium-play', `medium-diverse', `-large-play', and `-large-diverse'. The Kitchen domain focuses on  accomplishing four distinct subtasks using a 9-DoF Franka robot. We utilize three datasets that capture a range of behaviors: `-complete', `-partial’, and `-mixed’ for this domain.

\iffalse
For MuJoCo, we incorporate nine version 2 (v2) datasets. These datasets are generated using three distinct behavior policies: `-medium', `-medium-play', and `-medium-expert', and span across three specific tasks: `halfcheetah', `hopper', and `walker2d'. The primary objective in long-horizon navigation task AntMaze is to guide an 8-DoF Ant robot from its starting position to a predefined target location. For this, we employ six version 2 (v2) datasets which include `-umaze', `-umaze-diverse', `-medium-play', `medium-diverse', `-large-play', and `-large-diverse'. The Kitchen domain focuses on  accomplishing four distinct subtasks using a 9-DoF Franka robot. We utilize three version 0 (v0) datasets that capture a range of behaviors: `-complete', `-partial’, and `-mixed’ for this domain.

\fi

\paragraph{Baseline Algorithms}
We compare the performance of ADT with several representative baselines including (1) \emph{offline RL methods}:  TD3+BC \citep{fujimoto2021minimalist}, CQL \citep{kumar2020conservative} and IQL \citep{kostrikov2022offline}; (2) \emph{valued-conditioned methods}: Decision Transformer (DT) \citep{chen2021decision} and Q-Learning Decision Transformer (QLDT) \citep{yamagata2023q}; (3) \emph{goal-conditioned methods}: HIQL \citep{park2023hiql}, RvS \citep{emmons2021rvs} and Waypoint Transformer (WT) \citep{badrinath2023waypoint}. All the baseline results except for QLDT are obtained from \citep{badrinath2023waypoint} and \citep{park2023hiql} or by running the codes of CORL repository \citep{tarasov2022corl}. For HIQL, we present HIQL's performance with the goal representation in Kitchen and that without goal  representation in AntMaze, as per our implementation in ADT, to ensure fair comparison. QLDT and the transformer-based actor of ADT are implemented based on the DT codes in CORL, with similar architecture. Details are given in Appendix. The critics and the policies to generate prompts used in ADT are re-implemented in PyTorch following the official codes of IQL and HIQL. In all conducted experiments, five distinct random seeds are employed. Results are depicted with 95\% confidence intervals, represented by shaded areas in figures and expressed as standard deviations in tables. The reported results of ADT in tables are obtained by evaluating the final models.



\label{sec:main_results}
% TODO: 加粗
\begin{table*}[t]
\centering
\caption{Performance of V-ADT across all datasets.
% (DT and QLDT), and value-based RL methods. 
The methods on the right of the vertical line are transformer-based methods, the top scores among which are highlighted in \textbf{bold}.}
\scalebox{0.75}{
\begin{tabular}{cccc|ccc}
\toprule 
\textbf{Environment}         & \textbf{TD3+BC} & \textbf{CQL}                          & \textbf{IQL}     & \textbf{DT}     & \textbf{QLDT}  & \textbf{V-ADT}  \\ \hline
halfcheetah-medium-v2        & 48.3 $\pm$ 0.3  & 44.0 $\pm$ 5.4                        & 47.4 $\pm$ 0.2   & 42.4 $\pm$ 0.2  &  42.3 $\pm$ 0.4      & \textbf{48.7 $\pm$ 0.2}     \\
hopper-medium-v2             & 59.3 $\pm$ 4.2  & 58.5 $\pm$ 2.1                        & 66.2 $\pm$ 5.7   & 63.5 $\pm$ 5.2  &   \textbf{66.5 $\pm$ 6.3}     & 60.6 $\pm$ 2.8     \\
walker2d-medium-v2           & 83.7 $\pm$ 2.1  & 72.5 $\pm$ 0.8                        & 78.3 $\pm$ 8.7   & 69.2 $\pm$ 4.9  &   67.1 $\pm$ 3.2     & \textbf{80.9 $\pm$ 3.5}     \\
halfcheetah-medium-replay-v2 & 44.6 $\pm$ 0.5  & 45.5 $\pm$ 0.5                        & 44.2 $\pm$ 1.2   & 35.4 $\pm$ 1.6  &   35.6 $\pm$ 0.5     & \textbf{42.8 $\pm$ 0.2}     \\
hopper-medium-replay-v2      & 60.9 $\pm$ 18.8 & 95.0 $\pm$ 6.4                        & 94.7 $\pm$ 8.6   & 43.3 $\pm$ 23.9 &   52.1 $\pm$ 20.3    & \textbf{83.5 $\pm$ 9.5}     \\
walker2d-medium-replay-v2    & 81.8 $\pm$ 5.5  & 77.2 $\pm$ 5.5                        & 73.8 $\pm$ 7.1   & 58.9 $\pm$ 7.1  &   58.2 $\pm$ 5.1     & \textbf{86.3 $\pm$ 1.4}     \\
halfcheetah-medium-expert-v2 & 90.7 $\pm$ 4.3  & 91.6 $\pm$ 2.8                        & 86.7 $\pm$ 5.3   & 84.9 $\pm$ 1.6  &   79.0 $\pm$ 7.2    & \textbf{91.7 $\pm$ 1.5}     \\
hopper-medium-expert-v2      & 98.0 $\pm$ 9.4  & 105.4 $\pm$ 6.8                       & 91.5 $\pm$ 14.3  & 100.6 $\pm$ 8.3 &   94.2 $\pm$ 8.2     & \textbf{101.6 $\pm$ 5.4}    \\
walker2d-medium-expert-v2    & 110.1 $\pm$ 0.5 & 108.8 $\pm$ 0.7                       & 109.6 $\pm$ 1.0  & 89.6 $\pm$ 38.4 &   101.7 $\pm$ 3.4     & \textbf{112.1 $\pm$ 0.4}    \\ \hline
mujoco-avg                      & 75.3 $\pm$ 4.9  & 77.6 $\pm$ 3.4                        & 76.9 $\pm$ 5.8   & 65.3 $\pm$ 10.1  &   66.3 $\pm$ 6.1                   & \textbf{78.7 $\pm$ 2.8}     \\ 
\midrule
antmaze-umaze-v2             & 78.6            & 74.0                                  & 87.5 $\pm$ 2.6   & 53.6 $\pm$ 7.3  & 67.2 $\pm$ 2.3       & \textbf{88.2 $\pm$ 2.5}     \\
antmaze-umaze-diverse-v2     & 71.4            & 84.0                                  & 62.2 $\pm$ 13.8  & 42.2 $\pm$ 5.4  & \textbf{62.1 $\pm$ 1.6}       & 58.6 $\pm$ 4.3     \\
antmaze-medium-play-v2       & 10.6            & 61.2                                  & 71.2 $\pm$ 7.3   & 0.0 $\pm$ 0.0   & 0.0 $\pm$ 0.0        & \textbf{62.2 $\pm$ 2.5}     \\
antmaze-medium-diverse-v2    & 3.0             & 53.7                                  & 70.0 $\pm$ 10.9  & 0.0 $\pm$ 0.0   & 0.0 $\pm$ 0.0        & \textbf{52.6 $\pm$ 1.4}     \\
antmaze-large-play-v2        & 0.2             & 15.8                                  & 39.6 $\pm$ 5.8   & 0.0 $\pm$ 0.0   & 0.0 $\pm$ 0.0        & \textbf{16.6 $\pm$ 2.9}      \\
antmaze-large-diverse-v2     & 0.0             & 14.9                                  & 47.5 $\pm$ 9.5   & 0.0 $\pm$ 0.0   & 0.0 $\pm$ 0.0        & \textbf{36.4 $\pm$ 3.6}   \\ \hline
antmaze-avg                  & 27.3            & 50.6                                  & 63.0 $\pm$ 8.3   & 16.0 $\pm$ 2.1  & 21.6 $\pm$ 0.7       & \textbf{52.4 $\pm$ 2.9}     \\ 
\midrule
kitchen-complete-v0          & 25.0 $\pm$ 8.8               & 43.8                                  & 62.5             & 46.5 $\pm$ 3.0  &       38.8 $\pm$ 15.8               &   \textbf{55.1 $\pm$ 1.4}                   \\
kitchen-partial-v0           & 38.3 $\pm$ 3.1               & 49.8                                  & 46.3             & 31.4 $\pm$ 19.5 &       36.9 $\pm$ 10.7               &      \textbf{46.0 $\pm$ 1.6}              \\
kitchen-mixed-v0             & 45.1 $\pm$ 9.5               & 51.0                                  & 51.0             & 25.8 $\pm$ 5.0  &       17.7 $\pm$ 9.5               &   \textbf{46.8 $\pm$ 6.3}                 \\ \hline
kitchen-avg                  & 36.1 $\pm$ 7.1               & 48.2                                  & 53.3             & 34.6 $\pm$ 9.2  &       30.5 $\pm$ 12.0               &       \textbf{49.3 $\pm$ 3.1}            \\ \hline
average                      &    52.7             & 63.7                                  & 68.3             & 43.8 $\pm$ 7.3  &        45.4 $\pm$ 5.3              &    \textbf{65.0 $\pm$ 2.9}                \\ 
\bottomrule
\end{tabular}
}
\label{exp:main_results_value}
% \vspace{-0.3cm}
\end{table*}


\begin{table*}[!tbp]
\centering
\caption{Performance of G-ADT across all datasets. The methods on the right of the vertical line are transformer-based methods, the top scores among which are highlighted in \textbf{bold}.}
\scalebox{0.75}{
\begin{tabular}{ccc|cc}
\toprule 
\textbf{Environment}      & \textbf{RvS-R/G}  &  \textbf{HIQL}  & \textbf{WT}    & \textbf{G-ADT} \\ \hline
antmaze-umaze-v2          & 65.4 $\pm$ 4.9    &   83.9 $\pm$ 5.3               & 64.9 $\pm$ 6.1 & \textbf{83.8 $\pm$ 2.3}   \\
antmaze-umaze-diverse-v2  & 60.9 $\pm$ 2.5    &  87.6 $\pm$ 4.8              & 71.5 $\pm$ 7.6 & \textbf{83.0 $\pm$ 3.1}   \\
antmaze-medium-play-v2    & 58.1 $\pm$ 12.7   &   89.9 $\pm$ 3.5              & 62.8 $\pm$ 5.8 & \textbf{82.0 $\pm$ 1.7}   \\
antmaze-medium-diverse-v2 & 67.3 $\pm$ 8.0    &   87.0 $\pm$ 8.4              & 66.7 $\pm$ 3.9 & \textbf{83.4 $\pm$ 1.9}   \\
antmaze-large-play-v2     & 32.4 $\pm$ 10.5   &   87.3 $\pm$ 3.7              & \textbf{72.5 $\pm$ 2.8} & 71.0 $\pm$ 1.3   \\
antmaze-large-diverse-v2  & 36.9 $\pm$ 4.8    &   81.2 $\pm$ 6.6              & \textbf{72.0 $\pm$ 3.4} & 65.4 $\pm$ 4.9   \\ \hline
antmaze-avg               & 53.5 $\pm$ 7.2    &    86.2 $\pm$ 5.4             & 68.4 $\pm$ 4.9 & \textbf{78.1 $\pm$ 2.5}   \\ 
\midrule                   
kitchen-complete-v0       & 50.2 $\pm$ 3.6    &    43.8 $\pm$ 19.5             & 49.2 $\pm$ 4.6 &  \textbf{51.4 $\pm$ 1.7}   \\
kitchen-partial-v0        & 51.4 $\pm$ 2.6    &    65.0 $\pm$ 9.2             & 63.8 $\pm$ 3.5 &  \textbf{64.2 $\pm$ 5.1}   \\
kitchen-mixed-v0          & 60.3 $\pm$ 9.4    &    67.7 $\pm$ 6.8             & \textbf{70.9 $\pm$ 2.1} &  69.2 $\pm$ 3.3   \\ \hline
kitchen-avg               & 54.0 $\pm$ 5.2    &     58.8 $\pm$ 11.8            & 61.3 $\pm$ 3.4 &  \textbf{61.6 $\pm$ 3.4}   \\ \hline
average                   & 53.7 $\pm$ 6.5    &       77.1 $\pm$ 7.5         & 66.0 $\pm$ 4.4 &  \textbf{72.6 $\pm$ 2.8}    \\ 
\bottomrule
\end{tabular}
}
\label{exp:main_results_goal}
% \vspace{-0.6cm}
\end{table*}



\subsection{Main Results}

\cref{exp:main_results_value,exp:main_results_goal} present the performance of two variations of ADT evaluated on offline datasets. ADT significantly outperforms prior transformer-based decision making algorithms. 
Compared to DT and QLDT, two transformer-based algorithms for decision making, V-ADT  exhibits significant superiority especially on AntMaze and Kitchen which require the stitching ability to success. 
Meanwhile,   \cref{exp:main_results_goal} shows that G-ADT significantly outperforms WT, an algorithm that uses sub-goal as prompt for a transformer policy.  
We note that ADT enjoys comparable performance with state-of-the-art offline RL methods. 
For example, V-ADT outperforms all offline RL baselines in Mujoco problems. 
In AntMaze and Kitchen, V-ADT matches the performance of IQL, and significantly outperforms TD3+BC and CQL. 
\cref{exp:main_results_goal} concludes with similar findings for G-ADT.  

\iffalse
Finally, we observe a performance gap between ADT and IQL. To better understand this, we learn a transformer policy by {distilling} the IQL algorithm \citep{lee2023supervised,laskin2022context}. 
In particular, given an offline dataset $\cD$, we run IQL on $\cD$ and learn a policy $\pi^\dagger$. 
For each $s\in\cD$ we label it with the action produced by $\pi^\dagger$. This creates a new training dataset which are then used to learn an auto-regressive transformer policy. 
The distilling baseline excludes the effects of the transformer architecture so that the comparison explicitly focuses on the inherit merit of the algorithms. 
Figure \ref{fig:oracle_ablation}

\begin{figure}[!htbp]
\centering
\includegraphics[scale=0.375]{exp_figs/vdt_oracle_results.pdf}
\vspace{-0.6cm} 
\caption{Learning curves of V-ADT and V-ADT Oracle. The performance gap between V-ADT and V-ADT Oracle is limited on some datasets while evident on others, requiring further refinement.}
\label{fig:oracle_ablation}
\end{figure}%

\fi
%To the best of our knowledge, this represents a benchmark that surpasses any prior DT-based methods. 
%Given that V-ADT and G-ADT is trained following the IQL and HIQL paradigm, respectively, the achieved performance nearing or inferior to that of IQL and HIQL is anticipated.

\iffalse
An integration of findings from Table \ref{exp:main_results_value} and \ref{exp:main_results_goal} further suggests that goal prompts might possess a comparative advantage over value prompts. One plausible explanation is that goal prompts assist in simplifying policy training by decomposing intricate tasks into manageable subtasks. Conversely, while value prompt might attempt to stitch sub-optimal trajectories, it primary focus on the overarching task. Anyhow, as both V-ADT and G-ADT outperforms DT, we can conclude that with appropriately crafted prompts and corresponding training regime as does in ADT, the capabilities of DT can be more effectively exploited to achieve better performance. 
\fi





\subsection{Ablation Studies}
\label{sec:ablations}

\subsubsection{Effectiveness of Prompting}
\label{sec:manual_prompt}


\begin{figure}[!htbp]
\centering
\includegraphics[scale=0.45]{exp_figs/vdt_noprompt_results.pdf}
% \vspace{-0.6cm} 
\caption{Learning curves of V-ADT with and without value prompt. The value prompt significantly boosts the performance in harder diverse datasets.}
\label{fig:prompt_ablation}
\end{figure}%

\begin{figure}[!htbp]
\centering
\includegraphics[scale=0.45]{exp_figs/VDT_prompt_engineering_results.pdf}
% \vspace{-0.5cm}
\caption{Average normalized results of DT using different prompt. Incorporating manual prompt engineering could not help DT outperform V-ADT. }
\label{fig:manual_prompt}
%\vspace{-0.5cm}
\end{figure}%


In \cref{sec:obs} we discuss an illustrative example showing how value-based conditional prediction can be leveraged to solve sequential decision making problem. 
However, it is still unclear how much the value prompt contributes to the remarkable empirical performance of V-ADT. 
This is particularly important to understand as by removing the value prompt, our low-level policy optimization objective \plaineqref{eq:low-level-pi} becomes exactly the same as {advantage-weighted regression} \citep{peng2019advantage} with a transformer policy. 
We thus compare the performance of V-ADT with and without using value prompts in Figure \ref{fig:prompt_ablation}.  
Although the value prompt seems to be less useful for the play datasets, it significantly improves the performance of V-ADT for the much harder diverse datasets. This confirms the effectiveness of value prompting for solving complex problems. 

The main hypothesis behind ADT is that it is essential to learn a policy for adaptive prompt generation in order to make transformer-based sequential decision models able to learn optimal control policies.  
Since the initial return-to-go prompt of DT is a tunable hyper-parameter, a nature question follows: is it possible to match the performance of ADT through manual prompt tuning?   
 Figure~\ref{fig:manual_prompt} delineates the results of DT using different target returns on four different walker2d datasets. The x-axis of each subfigure represents the normalized target return input into DT, while the y-axis portrays the corresponding evaluation performance. Empirical results indicate that manual modifications to the target return could not improve the performance of DT, with its performance persistently lagging behind V-ADT. 
 We also note that there is no single prompt that performs universally well across all domains. 
This highlights that the utility of prompt in DT appears constrained, particularly when working with datasets sourced from unimodal behavior policy. 





\iffalse

\begin{figure}[!htbp]
\includegraphics[scale=0.375]{exp_figs/VDT_prompt_engineering_results.pdf}
\\
\includegraphics[scale=0.375]{exp_figs/vdt_noprompt_results.pdf}
% \vspace{-0.5cm}
\caption{Average normalized results of DT using different prompt. Incorporating manual prompt engineering could not help DT outperform V-ADT. }
\label{fig:manual_prompt}
\end{figure}%

\fi



% \begin{figure}[!htbp]
% \centering

% \begin{minipage}[t]{0.24\textwidth}
% \centering
% \includegraphics[width=1\textwidth]{exp_figs/antmaze-medium-play-v2_VDT_noprompt_results.pdf}
% \end{minipage}
% \begin{minipage}[t]{0.24\textwidth}
% \centering
% \includegraphics[width=1\textwidth]{exp_figs/antmaze-medium-diverse-v2_VDT_noprompt_results.pdf}
% \end{minipage}
% \begin{minipage}[t]{0.24\textwidth}
% \centering
% \includegraphics[width=1\textwidth]{exp_figs/antmaze-large-play-v2_VDT_noprompt_results.pdf}
% \end{minipage}
% \begin{minipage}[t]{0.24\textwidth}
% \centering
% \includegraphics[width=1\textwidth]{exp_figs/antmaze-large-diverse-v2_VDT_noprompt_results.pdf}
% \end{minipage}

% \caption{Learning curves of V-ADT with and without using prompt as part of input.}
% \label{fig:prompt_ablation}
% \end{figure}





\subsubsection{Effectiveness of Low-Level Policy Optimization Objective}

\begin{figure}[!htbp]
\centering

\begin{minipage}[t]{1\textwidth}
\centering
\includegraphics[width=1\textwidth]{exp_figs/vdt_noRL_results.pdf}
\end{minipage}
\begin{minipage}[t]{1\textwidth}
\centering
\includegraphics[width=1\textwidth]{exp_figs/gdt_noRL_results.pdf}
\end{minipage}

% \vspace{-0.3cm} 

\caption{Learning curves of V-ADT and G-ADT with and without using \plaineqref{eq:low-level-pi}.  The results demonstrate that \plaineqref{eq:low-level-pi} is essential in empowering DT with stitching ability to achieve superior performance.}
\label{fig:rl_loss_ablation}
\end{figure}

We claim that the sequence prediction loss used by DT does not suit our low-level policy optimization. 
To verify this claim, we implement a variant of ADT which uses the original DT objective to learn the low-level policy while still keeping learning an adaptive high-level policy. 
Figure \ref{fig:rl_loss_ablation} presents a comparison between this baseline and ADT.  
From the results we observe substantial improvement in performance of both V-ADT and G-ADT when \plaineqref{eq:low-level-pi} is leveraged. 
In particular, without using \plaineqref{eq:low-level-pi} to optimize the low-level policy, the effectiveness of auto-tuned prompting is  notably compromised. 
This also strengthens the need of joint policy optimization of high and low level policies. 
%Intuitively, in the context where DT can entirely memorize the trajectories from the dataset and when the value prompt is fully accurate, using behavior cloning loss is expected to give DT the capability to stitch these trajectories seamlessly. However, ensuring absolute accuracy of the value prompt remains a challenge. As a result, even when DT recalls all trajectories through behavioral cloning, it cannot generalize to stitch trajectories under erroneous prompts. Moreover, even the obtained  prompt is exactly accurate, the corresponding well-performed trajectories are probably absent in the dataset, hindering DT from adopting correct actions, as discussed before.  
%Thus incorporating RL loss during the training of DT is necessary to enhance DT's trajectory stitching ability.
%Empirical results of V-ADT further corroborate this observation, meanwhile proving the previous finding that benefits of using prompts in DT might be limited. As for G-ADT, its performance is also notably compromised without the RL loss, yet it retains a performance edge over DT, attributed to the decomposition of the original task.




% For V-ADT w/o RL loss, despite superior prompts (i.e., value) being utilized, the corresponding well-performed trajectories are conspicuously absent in the datasets. This essentially implies that, in the absence of the RL loss, V-ADT reduces to DT that only "recalls"  trajectories in the dataset.

% \begin{figure}[!htbp]
% \centering

% \begin{minipage}[t]{0.24\textwidth}
% \centering
% \includegraphics[width=1\textwidth]{exp_figs/antmaze-umaze-v2_VDT_noRL_results.pdf}
% \end{minipage}
% \begin{minipage}[t]{0.24\textwidth}
% \centering
% \includegraphics[width=1\textwidth]{exp_figs/antmaze-umaze-diverse-v2_VDT_noRL_results.pdf}
% \end{minipage}
% \begin{minipage}[t]{0.24\textwidth}
% \centering
% \includegraphics[width=1\textwidth]{exp_figs/antmaze-medium-play-v2_VDT_noRL_results.pdf}
% \end{minipage}
% \begin{minipage}[t]{0.24\textwidth}
% \centering
% \includegraphics[width=1\textwidth]{exp_figs/antmaze-medium-diverse-v2_VDT_noRL_results.pdf}
% \end{minipage}

% \begin{minipage}[t]{0.24\textwidth}
% \centering
% \includegraphics[width=1\textwidth]{exp_figs/antmaze-umaze-v2_GoalDT_noRL_results.pdf}
% \end{minipage}
% \begin{minipage}[t]{0.24\textwidth}
% \centering
% \includegraphics[width=1\textwidth]{exp_figs/antmaze-umaze-diverse-v2_GoalDT_noRL_results.pdf}
% \end{minipage}
% \begin{minipage}[t]{0.24\textwidth}
% \centering
% \includegraphics[width=1\textwidth]{exp_figs/antmaze-medium-play-v2_GoalDT_noRL_results.pdf}
% \end{minipage}
% \begin{minipage}[t]{0.24\textwidth}
% \centering
% \includegraphics[width=1\textwidth]{exp_figs/antmaze-medium-diverse-v2_GoalDT_noRL_results.pdf}
% \end{minipage}


% \caption{Learning curves of V-ADT and G-ADT with and without using RL loss. When RL loss is not used, the behavior cloning loss is used as does in DT.}
% \label{fig:rl_loss_ablation}
% \end{figure}



\subsubsection{Effectiveness of Tokenization Strategies}  
\label{sec:exp-abl-tokenize}

\begin{figure}[!htbp]
\centering
\begin{minipage}[t]{1\textwidth}
\centering
\includegraphics[width=1\textwidth]{exp_figs/vdt_individual_token_results.pdf}
\end{minipage}
\begin{minipage}[t]{1\textwidth}
\centering
\includegraphics[width=1\textwidth]{exp_figs/gdt_individual_token_results.pdf}
\end{minipage}
% \vspace{-0.2cm} 
\caption{Learning curves of ADT with different tokenization strategies. Our design contributes to superior performance by equally treating the states and related prompts when computing attention.}
\label{fig:token_ablation}
\end{figure}



In ADT, we diverge from the methodology presented in \citep{chen2021decision} where individual tokens are produced for each input component: return-to-go prompt, state, and action. Instead, we opt for a concatenated representation of prompts and states. Figure~\ref{fig:token_ablation} presents a comparative analysis between these two tokenization strategies. 
We observe that our tokenization method contributes to superior performance both for V-ADT and G-ADT. 

