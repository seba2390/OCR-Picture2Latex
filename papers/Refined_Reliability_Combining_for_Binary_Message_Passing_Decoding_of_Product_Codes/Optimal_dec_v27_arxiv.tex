\documentclass[journal]{IEEEtran}
%\documentclass[journal,draftcls,onecolumn,12pt,twoside]{IEEEtranTCOM}
%\documentclass[journal,11pt,onecolumn]{IEEEtran}% To get it two columns
\usepackage{epsfig,amsfonts,amsbsy,bm,mathrsfs}
\usepackage{mathrsfs} 
\usepackage{graphicx,cite,amssymb,amsmath,bm}
\usepackage{mathtools}
\usepackage{amsmath} 
\usepackage{etex}
\usepackage{amsmath,amssymb}
\usepackage{lipsum}
\usepackage{array,verbatim}
\usepackage{makecell}
\usepackage{siunitx}
\usepackage{float}
\usepackage{xcolor}
\usepackage{colortbl}
\usepackage{tikz}
\usepackage{ctable}
\usepackage[english]{babel}
\usepackage{color}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{balance}
\usepackage{balance}
\usepackage{perpage}
%\MakePerPage{footnote}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{subcaption}
\captionsetup{compatibility=false}
\captionsetup{font=footnotesize}
\usepackage{graphbox,graphicx}
\usepackage{xcolor}
\usepackage{bbm}
%\theoremstyle{definition}
\newtheorem{exmp}{Example}
\newtheorem{proposition}{Proposition}

\newtheorem{example}{Example}
\newtheorem{remark}{Remark}

\addto\captionsenglish{\renewcommand{\figurename}{Fig.}}
\makeatletter
% Reinsert missing \algbackskip
\def\algbackskip{\hskip-\ALG@thistlm}
\makeatother
\def\NoNumber#1{{\def\alglinenumber##1{}\State #1}\addtocounter{ALG@line}{-1}}
%\usepackage[usenames, dvipsnames]{color}

%%%%%%%%%%%%%%%%%%%% Gianluigi's commands %%%%%%%%%%%%%%%%%%%%%%
%\newcommand{\GL}{\textcolor{black}}
%\newcommand{\GLC}[1]{\medskip \textcolor{red!80!green}{\texttt{\textbf{#1}}} \medskip}
%\newcommand{\remove}{\textcolor{gray!50}}
\newcommand{\SNR}{\mathsf{SNR}}
\newcommand{\lab}{\mathsf{L}}

\newcommand{\mep}{{x}}
%\newcommand{\mep}{\mathtt{x}}
\newcommand{\row}{\mathsf{r}}
\newcommand{\col}{\mathsf{c}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\rmT}{\mathsf{T}}
\newcommand{\Q}{\mathsf{Q}}
\newcommand{\G}{\mathsf{G}}
\newcommand{\Bmat}{\mathbf{B}}
\newcommand{\Cmat}{\boldsymbol{C}}
\newcommand{\inalpha}{\mathcal{X}}
\newcommand{\pyx}{p_{Y|X}}
\newcommand{\x}{\boldsymbol{x}}
\newcommand{\A}{\boldsymbol{A}}
\renewcommand{\a}{\boldsymbol{a}}
\renewcommand{\b}{\boldsymbol{b}}
\renewcommand{\u}{\boldsymbol{u}}
\newcommand{\s}{\boldsymbol{s}}
\renewcommand{\t}{\boldsymbol{t}}
\newcommand{\p}{\boldsymbol{p}}
\newcommand{\ua}{\boldsymbol{u}^{\mathsf{a}}}
\newcommand{\us}{\boldsymbol{u}^{\mathsf{s}}}
\newcommand{\tus}{\tilde{\boldsymbol{u}}^{\mathsf{s}}}
\newcommand{\bfX}{\boldsymbol{X}}
\newcommand{\bfY}{\boldsymbol{Y}}
\newcommand{\bfZ}{\boldsymbol{Z}}
\newcommand{\bfB}{\boldsymbol{B}}
\newcommand{\bfP}{\boldsymbol{P}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Np}{\mathbb{N}_0}
\newcommand{\rmR}{\mathsf{R}}
\newcommand{\rmZ}{\mathrm{P}}
\newcommand{\rmC}{\mathsf{C}}
\renewcommand{\P}{\mathsf{P}}
\newcommand{\mI}{\mathsf{I}}
\newcommand{\ent}{\mathsf{H}}
\newcommand{\Pe}{P_\mathsf{e}}
\newcommand{\rateHDD}{\mathsf{R}_{\mathsf{HDD}}}
\newcommand{\phxx}{P_{\hat{X}|X}}
\newcommand{\mIHDS}{\mathsf{I}_\mathsf{HSD}}
\newcommand{\Rc}{R_{\mathsf{c}}}
\newcommand{\Rs}{R_{\mathsf{s}}}
%\newcommand{\nc}{n_{\mathsf{c}}}
\newcommand{\nc}{n}
\newcommand{\rc}{r_{\mathsf{c}}}
%\newcommand{\kc}{k_{\mathsf{c}}}
\newcommand{\kc}{k}
\newcommand{\dc}{d_{\mathsf{c}}}
\newcommand{\ncp}{n'_{\mathsf{c}}}
\newcommand{\kcp}{k'_{\mathsf{c}}}
\newcommand{\lrs}{\boldsymbol{l}^{\mathsf{s}}}
\newcommand{\lr}{\boldsymbol{r}}
\newcommand{\hlr}{\hat{\boldsymbol{r}}}
\newcommand{\lrt}{\tilde{\boldsymbol{r}}}
\newcommand{\lc}{\boldsymbol{c}}
\newcommand{\lcp}{\boldsymbol{c'}}
\newcommand{\llr}{\boldsymbol{l}}
\newcommand{\rr}{\boldsymbol{R}}
\newcommand{\rbol}{\boldsymbol{r}}
\newcommand{\yy}{\boldsymbol{Y}}
\newcommand{\cc}{\boldsymbol{C}}
\newcommand{\xx}{\boldsymbol{X}}
\newcommand{\cprim}{\boldsymbol{c'}}
\newcommand{\dgmd}{\mathsf{d}_\mathsf{GD}}
\newcommand{\dmin}{d_\mathsf{min}}
\newcommand{\dGMD}{\dgmd(\lr,\hat{\lr})}
\newcommand{\Ls}{\boldsymbol{L}^\text{s}}
\newcommand{\lalone}{\boldsymbol{L}}
\newcommand{\Lsc}{\boldsymbol{L}^\text{sc}}
\newcommand{\Lsr}{\boldsymbol{L}^\text{sr}}
\newcommand{\Lsce}{{L}^\text{sc}}
\newcommand{\Lsre}{{L}^\text{sr}}
\newcommand{\w}{w}
\newcommand{\ww}{\boldsymbol{w}}
\newcommand{\BB}{\mathsf{B}}
\newcommand{\DD}{\mathsf{D}}

\newcommand{\msv}{\mathsf{v}}
\newcommand{\msc}{\mathsf{c}}

\newcommand{\mut}{\tilde{\mu}}

%\newcommand{\B}{\forcemath{\bm{B}}}
%\newcommand{\proto}{\mathscr{P}}
%\newcommand{\BMA}[2]{\forcemath{{\left(#1 \,\, #2 \right)}}}
%\newcommand{\BMB}[6]{\forcemath{{\left(\begin{array}{c|cc}
%				#1 & #3 & #5 \\
%				#2 & #4 & #6
%			\end{array}
%			\right)}}}
%\newcommand{\thr}{\forcemath{{\delta^\star}}}
%\newcommand{\thrspa}{\forcemath{{\delta^\star_{\mathsf{SPA}}}}}
%\newcommand{\thre}{\forcemath{{\delta^\star_{\mathsf{E}}}}}
%\newcommand{\ensemble}{\forcemath{{\mathscr{C}}}}
%\newcommand{\ens}[1]{\forcemath{{\ensemble_{\mathsf{#1}}}}}

\definecolor{mygreen}{rgb}{0.0, 0.45, 0.0}

\def\forcemath#1{\ifmmode #1 \else $#1$\fi}

\newcommand{\mv}[2]{\forcemath{m_{#1\rightarrow #2}}}
\newcommand{\mc}[2]{\forcemath{m_{#1\rightarrow #2}}}
\newcommand{\neigh}[1]{\forcemath{\mathcal{N}\left(#1\right)}}
\newcommand{\vn}{\forcemath{\mathsf{v}}}
\newcommand{\bn}{\forcemath{\mathsf{b}}}
\newcommand{\cn}{\forcemath{\mathsf{c}}}
\newcommand{\msg}[2]{\forcemath{m_{#1\rightarrow#2}}}
\newcommand{\mch}{\forcemath{m_{\mathrm{ch}}}}
\newcommand{\sign}{\forcemath{\mathrm{sign}}}

\newcommand{\vnt}{\forcemath{\mathsf{V}}}
\newcommand{\cnt}{\forcemath{\mathsf{C}}}
\newcommand{\degVN}{\ensuremath{d_v}}
\newcommand{\degCN}{\ensuremath{d_c}}

\newcommand{\Pue}{P^{\mathsf{e}}}
\newcommand{\fue}{f^{\mathsf{e}}}%f^{e}
\newcommand{\Que}{Q^{\mathsf{e}}}
\newcommand{\Puc}{P^{\mathsf{c}}}
\newcommand{\Quc}{Q^{\mathsf{c}}}
\newcommand{\fuc}{f^{\mathsf{c}}}
\newcommand{\Puep}{P^{\mathsf{\epsilon}}}
\newcommand{\Quep}{Q^{\mathsf{\epsilon}}}


\newcommand{\ham}{\mathsf{d}_\mathsf{H}}
\newcommand{\EC}{\textcolor{blue}}
\newcommand{\R}{\textcolor{mygreen}}
\newcommand{\RR}{\textcolor{orange}}
\newcommand{\RRR}{\textcolor{red}}
%\newcommand{\SH}{\textcolor{red}}
%\newcommand{\GLL}{\textcolor{cyan}}

\newcommand{\opt}{\tilde{l}}

\newcommand{\otoprule}{\midrule[\heavyrulewidth]}
\newcommand{\qedsymbol}{$\blacksquare$}
 
\newcommand{\Eb}{\mathsf{E_b}}
\newcommand{\Es}{\mathsf{E_s}}
\newcommand{\No}{\mathsf{N_0}} 
\definecolor{armygreen}{rgb}{0.29, 0.33, 0.13}
\newcommand{\SH}{\textcolor{black}}
\newcommand{\SHm}{\textcolor{black}}
\newcommand{\GL}{\textcolor{black}}
\newcommand{\AG}{\textcolor{black}}
\newcommand{\AGc}{\textcolor{black}}
\newcommand{\GLC}[1]{\textcolor{black}{\textbf{\textit{#1}}}}

\ifCLASSINFOpdf
\else
\fi

\hyphenation{op-tical net-works semi-conduc-tor}

\newcommand\myeqa{\stackrel{\mathclap{\normalfont\mbox{\scriptsize $(a)$}}}{=}}
\newcommand\myeqb{\stackrel{\mathclap{\normalfont\mbox{\scriptsize $(b)$}}}{=}}
\newcommand\myeqc{\stackrel{\mathclap{\normalfont\mbox{\scriptsize$(c)$}}}{=}}
\newcommand\myeqd{\stackrel{\mathclap{\normalfont\mbox{\scriptsize$(d)$}}}{=}}
\newcommand\myeqe{\stackrel{\mathclap{\normalfont\mbox{\scriptsize$(e)$}}}{=}}

\let\originalleft\left
\let\originalright\right
\renewcommand{\left}{\mathopen{}\mathclose\bgroup\originalleft}
\renewcommand{\right}{\aftergroup\egroup\originalright}


\newcommand\scalemath[2]{\scalebox{#1}{\mbox{\ensuremath{\displaystyle #2}}}}   % This preambel is used for rescaling of the math environment, even for arrays: e.g., \scalemath{0.9}{eq should be there}   

\newtheorem{thm}{Theorem}
\newtheorem{lemma}{Lemma}
%\newtheorem{remark}{Remark}

\begin{document}
	

  


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title{Refined Reliability Combining for Binary Message Passing Decoding of Product Codes}
\author{
	Alireza Sheikh \IEEEmembership{Member, IEEE}, Alexandre Graell i Amat, \IEEEmembership{Senior Member, IEEE}, Gianluigi~Liva,~\IEEEmembership{Senior~Member,~IEEE}, and Alex Alvarado \IEEEmembership{Senior Member, IEEE}. 
	\thanks{A. Sheikh and A. Alvarado are with the Department of Electrical Engineering, Eindhoven University of Technology, PO Box 513
		5600 MB Eindhoven, The Netherlands (emails: \{asheikh, a.alvarado\}@tue.nl). The work of A. Sheikh and A. Alvarado has received funding from the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme (grant agreement No 757791).}	
	\thanks{A. Graell i Amat is with the Department of Electrical Engineering, Chalmers University of Technology, SE-41296 Gothenburg, Sweden (email: alexandre.graell@chalmers.se).}
	\thanks{G. Liva is with the Institute of Communications and
		Navigation of the German Aerospace Center (DLR), M\"unchner Strasse 20, 82234 We{\ss}ling, Germany (email: gianluigi.liva@dlr.de).}}


\maketitle

\begin{abstract}
	
We propose a novel soft-aided iterative decoding algorithm for product codes (PCs). The proposed algorithm, named iterative bounded distance decoding with combined reliability (iBDD-CR), enhances the conventional iterative bounded distance decoding (iBDD) of PCs by exploiting some level of soft information. In particular, iBDD-CR can be seen as a modification of iBDD where the hard decisions of the row and column decoders  are made based on \GL{a reliability estimate} of the BDD outputs. \GL{The reliability estimates} are derived using extrinsic message passing for generalized low-density-parity check (GLDPC) ensembles, \AG{which  encompass PCs}. 
We perform a density evolution analysis of iBDD-CR  for transmission over the additive white Gaussian noise channel for the GLDPC ensemble. We consider both binary transmission and bit-interleaved coded modulation with quadrature amplitude modulation. We show that iBDD-CR achieves performance gains up to $0.51$ dB compared to iBDD with the same internal decoder data flow.
%at the expense of a \SH{limited} increase in memory. 
This makes the algorithm an attractive solution for very high-throughput applications such as fiber-optic communications.  
 
% We also show that iBDD-CR outperforms the ideal iBDD where the miscorrections are disregarded. 
 


\end{abstract}

\begin{IEEEkeywords}
	Binary message passing, bounded distance decoding, complexity, high speed communications, hard decision decoding, product codes, quantization errors.
\end{IEEEkeywords}


\section{Introduction}


The rediscovery of soft-decision (SD) iterative decoding algorithms and graph-based codes in the early 1990s allowed for the first time performance close to the theoretical limits with relatively low complexity. Turbo \GL{\cite{Berrouetal93}}, and low-density parity-check (LDPC) \GL{\cite{Gallager63:LDPC}} codes were soon widely adopted in communications standards. %and coding for moderate-to-large block lengths was declared solved.  
However, the ever more demanding requirements in terms  of  throughput and  power consumption, asks for new coding solutions.
%is questioning this more than ever.
For example, current optical coherent transceivers operate at data rates of 400 Gbps and the next frontier  is $1$~Tbps. Scaling conventional SD iterative decoders to such high throughputs  is very challenging.  This has spurred a great deal of research in the last few years on designing low-complexity coding and decoding schemes that can operate at very high throughputs while yet achieving performance close to that of conventional SD iterative schemes \cite{Cushon2016,Ste19}. 

A key observation is that the main limitation to achieve very high throughputs arises from the high internal decoder data flow of SD decoders \cite{staircase_frank}. For this reason hard-decision (HD) decoders are appealing solutions when  high throughputs are sought, as the internal decoder data flow can be kept reasonably small. For example, HD coding for optical communications is usually
based on product-like codes with high rate  Bose-Chaudhuri-Hocquenghem (BCH) component codes, which can be efficiently decoded via bounded distance
decoding (BDD) \cite{staircase_frank,JianPfister2017}. The decoding is then performed based on BDD of the component
codes and iterating between the row and column decoders, which we refer here to as iterative BDD (iBDD). The very high throughputs of HD product-like coding schemes, however, are achieved at the expense of a significant performance loss compared to their LDPC code counterparts, decoded via SD belief propagation. 

Recently, several works have focused on the improvement of the performance of conventional iBDD of product-like codes  targeting specifically very high throughputs \cite{Hag18,sheikhTCOM19,She18b,She19,YibitTCOM}. The underlying idea in all these works is to generate some level of reliability information \GL{within} the decoder to assist the conventional BDD. The result is an enhanced performance compared to iBDD while keeping the internal data flow low. Among the algorithms in  \cite{Hag18,sheikhTCOM19,YibitTCOM}, iBDD with scaled reliability (iBDD-SR) \cite{sheikhTCOM19} is the one yielding the smallest increase in complexity, yet achieving about $0.3$ dB performance improvement compared to iBDD for binary transmission. 
%iBDD-SR is based on combining the channel reliability with the reliabilities of the output of the BDD decoder%, properly conveyed by a scaling factor applied to the BDD decision
Following the principle introduced in \cite{Lech12}, iBDD-SR is based on binary message passing (BMP) between component decoders and \AG{generates reliability information at the BDD output by scaling the decisions according to a reliability estimate of the decision. The reliability information, in the form of log-likelihood ratios is then added} to the corresponding channel LLRs to form refined bit estimates (see \cite[Fig.~2]{sheikhTCOM19}). The decoding schemes in \cite{She18b,She19,YibitTCOM,GabrieleSABMSR2019}, on the other hand, yield some additional coding gains but require the knowledge of the least reliable bits in the decoding process, and hence entail further complexity. \GL{Alternative constructions for high-throughput applications include coarsely-quantized low-density parity-check decoders \cite{Lech12,Ben18,Ste19}, two-stage decoders \cite{Montorsi2018}, and SD-HD hybrid schemes based on concatenating a relatively weak SD code and an outer HD product-like code \cite{Ksch17,Ksch18}.}




This paper extends our previous work \cite{sheikhTCOM19} in three different directions: 
\GL{\begin{itemize}
\item[i.] We \GL{derive a more accurate estimate of} the reliability of the BDD outputs, which allows us to derive \GL{an improved} combining \GL{rule for} the BDD outputs and the channel LLRs. The resulting decoding algorithm, dubbed iterative bounded distance decoding with combined reliability (iBDD-CR) is shown to outperform iBDD-SR and, interestingly, also an idealized (genie-aided) iBDD that prevents miscorrections. The \GL{combining rule} can be implemented by means of a small lookup table (LUT).
% that entails a minor memory increase with respect to conventional iBDD.
\item[ii.] We ad\GL{a}pt the algorithm to bit interleaved coded modulation (BICM) with \SH{nonbinary} modulation. For both binary and BICM transmission, we derive the density evolution (DE) equations for iBDD-CR, which provides an amenable analysis of the algorithm. 
\item[iii.] We \SH{evaluate} the effect of quantizing the channel \GL{LLRs} and show that iBDD-CR has low sensitivity to quantization.
\end{itemize}}
%The simulation results indicate that iBDD-CR can be implemented with the low-complex max-log soft information approximation without any performance degradation compared to exact soft information computation. Furthermore, iBDD-CR offers a low sensitivity to quantization errors on channel soft information, e.g., the performance degradation induces by quantizing the channel soft information using $3$ bits is less than $0.07$ dB for BICM with $16$-QAM.
\GL{The combining rule derived in this work is based on DE analysis of (regular) generalized LDPC (GLDPC) codes ensembles that contain product codes \SH{(PCs)} as ensemble members, under extrinsic message passing decoding. For the ensembles under consideration, the derived rule is optimal in the limit of large block lengths. Remarkably, the proposed combining rule provides a substantial coding gain with respect to iBDD-SR also when applied to the decoding of \SH{PCs}, with gains that are consistent with the DE analysis findings.}
We show that 
iBDD-CR yields gains up to $0.51$ dB compared to iBDD with \GL{an} identical decoder data flow, 
%and a \SH{limited increase in the required memory with respect to conventional iBDD}, 
making the algorithm particularly appealing for high-throughput applications.


\medskip

\noindent \emph{Notation:} We use boldface letters to denote vectors
and matrices, e.g., $\boldsymbol{x}$  and $\boldsymbol{X}=[x_{i,j}]$, with $x_{i,j}$ representing the element corresponding to the $i$-th row and $j$-th column of $\boldsymbol{X}$. 
Moreover, $\boldsymbol{X}_{i,:}$ denotes the $i$-th row of $\boldsymbol{X}$.   
$|a|$ denotes the absolute value of $a$,
$\left\lfloor a \right\rfloor$ the largest integer smaller than or equal to $a$, and $\left\lceil a \right\rceil$ the smallest integer larger than or equal to $a$. $(\cdot)^{\mathsf{T}}$ denotes the transpose operation. We also define $\hat l \in \{\pm1\}$ as the sign of value $l$.  $\mathbb{R}$ is the set of real numbers and $p(\cdot)$ is the probability distribution or probability mass function of the continuous or discrete random variables (RVs), respectively. 
A Gaussian distribution with mean $\mu$ and variance $\sigma^2$ is denoted by $\mathcal{N}(\mu ,\sigma^2)$. Furthermore, %\GLC{I would not use Phi, since it is typically used in statistic books for the cumulative of a Gaussian - See Feller, and many other books. Also, we use it later for bit mapping} 
$\G (\lambda ;\mu ,{\sigma ^2}) \triangleq \frac{1}{{\sqrt {2\pi } \sigma }}\exp ( - \frac{{{{\left( {\lambda  - \mu } \right)}^2}}}{{2{\sigma ^2}}})$ stands for the Gaussian function with mean and variance $\mu$ and $\sigma^2$. We also denote by $\Q(x)\triangleq\frac{1}{\sqrt{2\pi}}\int_x^\infty \mathrm{e}^{\frac{-\xi^2}{2}}\mathrm{d}\xi$ the tail \AG{probability} of the standard Gaussian distribution. The Hamming distance between vectors $\a$ and $\b$ is denoted by $\ham(\a,\b)$. Finally, \GL{we define}
\begin{align*}
\bar{\mathbbm{U}}(x)=  \left\{ 
\begin{array}{ll}
1 & \text{if } x<0\\
0 & \text{otherwise}.
\end{array}
\right.
\end{align*}

\section{Preliminaries}
\label{sys_mod} 

We consider binary PCs with BCH component codes. Let $\mathcal{C}$ be an $(\nc,\kc)$ BCH code with minimum Hamming distance $\dmin$ built over the Galois field $\text{GF}(2^v)$ with (even) block length $\nc$ and information block length $\kc$ given by
\begin{align}
\nc=2^v-1, \;\;\; \kc=2^v-vt-1, \label{k_formula}
\end{align} 
where $t \buildrel \Delta \over =  \left\lfloor {\frac{{{\dmin} - 1}}{2}} \right\rfloor$ \AG{is} the error correcting capability of the code.

A (two-dimensional) PC with parameters $(\nc^2,\kc^2)$ and code rate  $R=\kc^2/\nc^2$ is defined as the set of all $\nc\times\nc$ arrays such that each row and each column in the array is a codeword of $\mathcal{C}$. Accordingly, a codeword can be defined as a binary matrix $\cc=[c_{i,j}]$.  
For \AG{ease of explanation, assume first} transmission over the binary input additive white Gaussian noise (bi-AWGN) channel \AG{(in Sec.~\ref{QAM_BICM} we also consider a bit-interleaved coded modulation (BICM))}. The output of the bi-AWGN channel corresponding to code bit $c_{i,j}$ is thus given by
\begin{align}\label{channel_inst}
y_{i,j}=x_{i,j}+n_{i,j},
\end{align}  
where $x_{i,j}=(-1)^{c_{i,j}}$ and $n_{i,j}\sim \mathcal{N}(0,\sigma^2)$. For a given $\Eb/\No$ and code rate $R$, the \AG{noise variance is $\sigma^2=(2 R
\Eb/\No)^{-1}$. The signal-to-noise ratio \SH{(SNR)} per symbol is  $\Es/\No=\frac{1}{2\sigma^2}$}. Let $\lalone=[l_{i,j}]$ be the matrix of channel \AG{LLRs} and $\rr=[r_{i,j}]$ the matrix of hard decisions at the output of the channel, i.e., $r_{i,j}$ is obtained taking the sign of $l_{i,j}$ and mapping $- 1 \mapsto 1$ and $+ 1 \mapsto 0$. We denote this mapping by $\BB(\cdot)$, i.e., $r_{i,j}=\BB(l_{i,j})$.  
%We also denote by $\BB^{-1}(\cdot)$ the inverse of this mapping, i.e, $1 \mapsto -1$ and $0 \mapsto +1$. 
With some abuse of notation we will write $\rr=\BB(\lalone)$.
%Consider a BCH component code with information length $k$ and
%codeword length $n$, in short represented as $(n,k)$.  In the BDD of
%the component code, the decoding is successful if the condition given
%as \begin{align}\label{errorcond} 2e \le {\dmin} - 1, \end{align}

%Assume transmission over the bi-AWGN channel using an
%$(\nc,\kc,\dmin)$ linear block code and denote by
%$\lc=(c_1,\ldots,c_{\nc})$, $\lr=(r_1,\ldots,r_{\nc})$, and
%$\hat{\lr}=(\hat{r}_1,\ldots,\hat{r}_{\nc})$ the transmitted codeword,
%the hard-detected received vector, and the decoded vector,
%respectively. 
PCs are conventionally decoded using \AG{BDD} of the component codes. Here we briefly explain BDD. Consider the decoding of an arbitrary row component, which is an $1 \times n$ array. \AG{Specifically}, assume decoding of the transmitted component codeword $\lc=(c_1,\ldots,c_{\nc})$ based on the hard-detected bits at the channel output,
$\lr=(r_1,\ldots,r_{\nc})$. BDD corrects all
error patterns with Hamming weight up to the error-correcting
capability of the code $t$. If the
weight of the error pattern is larger than $t$ and there exists
another codeword $\tilde{\lc} \in \mathcal{C}$ with
$\mathsf{d_H}(\tilde{\lc},\lr)\le t$, then BDD erroneously decodes $\lr$ onto
$\tilde{\lc}$ and a so-called \emph{miscorrection} \AG{occurs}.  Otherwise, if
such a codeword does not exist, BDD fails. \GL{\AG{In this case, the bounded distance (BD) decoder}  may output its input $\lr$, or it may declare a decoding failure (often referred to as an \emph{erasure}).}
%Thus, the decoded vector $\hat{\rbol}$ for BDD
%can be written as
%\begin{equation}
%\label{eq:BDD_VN}
%\scalemath{0.912}{	\hat{\rbol} = \begin{cases}
%	\lc & \text{if } \ham(\lc, \rbol)  \leq t \\
%	\tilde{\lc} \in \mathcal{C} & \text{if } \ham(\lc, \rbol) > t \text{ and
%	} \exists{\tilde{\lc}} \; \text{such that} \;\ham(\tilde{\lc}, \rbol) \leq t \\
%	\rbol & \text{otherwise}
%	\end{cases}.}
%\end{equation}
The decoding of PCs can be accomplished in an iterative fashion based on  BDD of the component codes and iterating between the row and column decoders, \AG{which we refer to as iBDD}. \GL{In the case of iBDD, if a local \AG{BD decoder} fails, it outputs the input vector.}
%Here, we refer to iterative decoding of PCs based on BDD of the component codes as iBDD.  
 
\begin{figure*}[!t] \centering 
	\includegraphics[scale=0.9]{optimal_binarymessage_block_diagramtikzv1.pdf}  
	%\vspace{-2ex}
	\caption{Block diagram \GL{illustrating the} message passing in iBDD-CR corresponding to $i$th row decoding at iteration $\ell$. The diagram shows the decision on code bit $c_{i,j}$. $\bm{\Psi}_{i,:}^{\mathsf{c},(\ell-1)}$ is the input to BDD, $\bar \mu^{\row,(\ell)}_{i,j}$ is the output of BDD, $l_{i,j}$ is the channel LLR, $\hat l_{i,j}$ is sign of channel LLR, $\tilde \mu^{\row,(\ell)}_{i,j}$ is the output of LUT given in Table~\ref{Tabcomp}, $\psi^{\row,(\ell)}_{i,j}$ is the output of iBDD-CR  and $\tilde l^{\row,(\ell)}_{i,j}$ is the LLR of $\psi^{\row,(\ell)}_{i,j}$.}  %\vspace{-2ex}
	\label{SysPCCST} 
\end{figure*} 
\section{\GL{Iterative Bounded Distance Decoding with Combined Reliability}}\label{iBDD-SRPC}  

Recently, a variant of iBDD called iBDD-SR has been proposed, where the BDD  outbound messages are modified based on the channel reliabilities to reduce  miscorrections. A detailed explanation of iBDD-SR can be found in \cite{sheikhTCOM19}. We highlight that iBDD-SR \GL{exploits an estimate of the} reliability of the BDD decisions \GL{to produce}  weighted sums of BDD outputs and channel \GL{LLRs} (see \cite[Fig.~2]{sheikhTCOM19}). \GL{We provide next an improved combining rule, based on an enhanced model of the component code decoder behavior. The new rule will be shown to be superior to the weighted sum approach of \cite{sheikhTCOM19}, in terms of both asymptotic decoding threshold and finite-length performance.} 


\AG{Consider the decoding of an $(n^2,k^2)$ PC and} let $\bm{\Psi}^{\mathsf{c},(\ell-1)}=[\psi_{i,j}^{\mathsf{c},(\ell-1)}]$ be the decoding result of the $\nc$ column codes at iteration $\ell-1$, \AG{where} $\psi_{i,j}^{\mathsf{c},(\ell-1)}$ corresponds to the decision on code bit $c_{i,j}$. The input of the row decoders at iteration $\ell$ is $\bm{\Psi}^{\mathsf{c},(\ell-1)}$. Without loss of generality, we assume the decoding of the $i$th row code at iteration $\ell$, hence, the input of the decoder is $\bm{\Psi}^{\mathsf{c},(\ell-1)}_{i,:}$.\footnote{\SH{Recall our notation where $\boldsymbol{X}_{i,:}$ denotes the $i$-th row of $\boldsymbol{X}$.}} %\AGc{(NOTE: WE NEED TO DEFINE THIS NOTATION)}.
The block diagram of the iBDD-CR algorithm corresponding to the $i$th row decoding at iteration $\ell$ is schematized in Fig.~\ref{SysPCCST}. Let $\bar\mu_{i,j}^{\mathsf r}$ denote the output of \AG{the $i$th row BD decoder}   corresponding to code bit $c_{i,j}$. $\bar\mu_{i,j}^{\mathsf r}$ takes values on a ternary alphabet, $\bar\mu_{i,j}^{\mathsf r} \in \{\pm1, 0 \}$, where the decoded bits are mapped according to $0\mapsto +1$ and $1\mapsto -1$ if BDD is successful, and the output is $0$ in the case of a decoding failure. Let $\tilde{l}_{i,j}^{\mathsf r, (\ell)}$ be the \GL{soft value of} code bit $c_{i,j}$ at iteration $\ell$, which is formed based on the values of the BDD output and the channel LLRs, i.e., $\bar \mu _{i,j}^{\row,(\ell )}$ and $l_{i,j}$, respectively. In Sec.~\ref{sec:DE_PCs}, we derive a closed-form expression for $\tilde{l}_{i,j}^{\mathsf r, (\ell)}$ \AG{via a DE} analysis.\footnote{\GL{Note that the expression is derived under the assumption of an asymptotically-large block length, under an extrinsic message passing variation of the algorithm, as will be discussed in Sec.~\ref{sec:DE_PCs}}.} For now, assume that \GL{\emph{combining}}  $\bar \mu _{i,j}^{\mathsf r,(\ell )}$ and $l_{i,j}$ results in \GL{a soft value} $\tilde{l}_{i,j}^{\mathsf r, (\ell)}$. 
Then, the hard decision on the code bit $c_{i,j}$ produced by the $i$th row decoder is formed as%the $i$th bit of the $j$th column decoder at iteration $\ell$ is formed as
\begin{equation}\label{eq:BDDchrel_VN}
\psi_{i,j}^{\mathsf{r},(\ell)}=
\mathsf{B}(\tilde{l}_{i,j}^{\mathsf r, (\ell)}) , 
\end{equation}
where ties can be broken with any policy (see Sec.~\ref{sys_mod} for the definition of $\BB(\cdot)$).


The hard decision $\psi_{i,j}^{\mathsf{r},(\ell)}$ is the message
passed from the $i$-th row \AG{decoder} to the $j$-th column \AG{decoder}. In
particular, after applying this procedure to all row \AG{decoders}, the matrix
$\boldsymbol{\Psi}^{\mathsf{r},(\ell)}=[\psi_{i,j}^{\mathsf{r},(\ell)}]$
is formed and used as the input for the $n$ column decoders, and column decoding based on
$\boldsymbol{\Psi}^{\mathsf{r},(\ell)}$ is performed. Assume the decoding of the $j$th column code at iteration $\ell$, hence, the input of the decoder is $\bm{\Psi}^{\mathsf{r},(\ell)}_{:,j}$. As before, we assume that the output of the \AG{$j$th column BD decoder} corresponding to code bit $c_{i,j}$, denoted by $\bar\mu_{i,j}^{\mathsf c}$, takes values on $\{\pm1, 0 \}$.  Similar to row decoding, $\tilde{l}_{i,j}^{\mathsf c, (\ell)}$ is formed based on combining  $\bar\mu_{i,j}^{\mathsf c}$ and $l_{i,j}$. Then, the hard decision on code bit $c_{i,j}$ produced by the $j$th column decoder is formed as 
\begin{equation}\label{eq:BDDchrel_VN_scale}
	\psi_{i,j}^{\mathsf{c},(\ell)}=\BB(\tilde{l}_{i,j}^{\mathsf c, (\ell)}).
\end{equation}
The matrix $\bm{\Psi}^{\mathsf{c},(\ell)}=[\psi_{i,j}^{\mathsf{c},(\ell)}]$ is passed to the $\nc$ row decoders for the decoding iteration $\ell+1$.
%After decoding of the $\nc$ column codes at decoding iteration $\ell$, the matrix $\bm{\Psi}^{\mathsf{c},(\ell)}=[\psi_{i,j}^{\mathsf{c},(\ell)}]$ is passed to the $\nc$ row decoders for the next decoding iteration. 
The iterative process continues until a maximum number of iterations is reached. 

\GL{With reference to Fig.~\ref{SysPCCST}, observe that in the proposed algorithm the component code decoders exchange only \SH{(binary)} hard decisions. Hence, the contribution of the messages passed among component  decoders to the overall internal decoder data flow} \GL{is comparable to that of conventional iBDD \cite[Sec.~III.A]{staircase_frank}.} 
%\GLC{I believe we should not be too aggressive on this point: Frank complained about it, and he might be right. In his version of iBDD, I understood that the nodes exchange only local syndromes, which are much shorter than local codewords. This is a point where we need to have a deeper understanding, if we want to move forward with strong claims.}
%as the input and output for row (column) codes are  $\tilde{l}_{i,j}^{\mathsf r, (\ell)}$ and $\psi_{i,j}^{\mathsf{r},(\ell)}$ ($\tilde{l}_{i,j}^{\mathsf c, (\ell)}$ and $\psi_{i,j}^{\mathsf{c},(\ell)}$), respectively.  
%The crucial modification in iBDD-SR with respect to conventional iBDD is that the hard decisions passed between component decoders are not simply the result of the BDD of the component codes, but are made on the sum of a scaled version of the output of the BDD decoder and the channel LLR.  
%%The sum of the output of the BDD decoder, properly scaled by a scaling parameter $w$, and the channel LLR. 
%Therefore, the channel reliabilities are exploited to make the final hard decisions at each row and column decoding stage. 
%Intuitively, since the channel reliability is exploited in the hard decisions at each row and column decoding, in the case that the reliability of the channel is high and conventional iBDD introduces miscorrections, the modified algorithm may combat the possible miscorrections. 

%We highlight that the proposed algorithm exchanges binary (hard) messages, hence the decoder data-flow is the same as that of conventional iBDD. This comes at a minor increase in complexity (a product and a sum for each code bit and iteration) and some memory increase as the channel LLRs need to be stored.

\section{\AG{Density Evolution} Analysis of iBDD-CR for GLDPC Code Ensembles}\label{sec:DE_PCs}

\subsection{\AG{Density Evolution} Analysis for the bi-AWGN Channel}\label{sec:DE_BIAWGN}

%\GLC{I'm re-writing this section to make it more precise: In the previous version, there was confusion between codes and code ensembles.}

\GL{We provide in this section a DE analysis of PCs, under iBDD-CR decoding. More specifically, we analyze PCs as members of regular GLDPC code ensembles \cite{Lentmaier98:GLDPC}. To do so, we first recall the  Tanner graph representation of a PC \cite{Tanner1981}. A two-dimensional PC defined by an $(n,k)$ component code (used for both rows and columns of the codeword array) can be represented by a graph consisting of two sets of nodes: $n^2$ degree-$2$ variable nodes (VNs) and $2n$ degree-$n$ constraint nodes (CNs). Each VN is associated to a codeword bit, and each CN is associated to a row/column code. VNs and CNs are then connected by edges according to the constraints defined by the PC construction. An example is provided in Fig.~\ref{PCgraph} for the case \AG{of $n=3$ component code}.}

\begin{figure*}[t] \centering 
	\includegraphics[scale=1.4]{procutcodefig_optimal_total_newww.pdf}
	\vspace{-1ex}
	\caption{(a) The PC code array with $9$ bits. (b) Tanner graph of the PC. $\mathsf{r_1}$, $\mathsf{r_2}$, and $\mathsf{r_3}$ ($\mathsf{c_1}$, $\mathsf{c_2}$, and $\mathsf{c_3}$) stand for the first, second, and third BCH row (column) \GL{constraint nodes}. \GL{A length-$8$ cycle is highlighted in the Tanner graph. (c) Tanner graph of a generic regular GLDPC code with $n^2$ degree-$2$ VNs and $2n$ degree-$n$ CNs.}}  %\vspace{-2ex}
	\label{PCgraph} 
\end{figure*}


%\begin{figure}[t] \centering 
%	\includegraphics[scale=1.4]{./paper_Figs/procutcodefig_optimal_totalv1_new.pdf}
%	\vspace{-1.5ex}
%	\caption{(a) \GL{Tanner graph of a generic GLDPC code, belonging to the same ensemble that contains the PC of Fig.~\ref{PCgraph}.} 
%		(b) \GL{}}
%	\vspace{-1.5ex}
%	\label{PCgraph1} 
%\end{figure}

\GL{PCs can be seen as a special class of GLDPC codes \AG{for which the connections between VNs and CNs are directly defined by the PC structure}. The deterministic nature of the  Tanner graph of PCs has a major consequence in terms of DE analysis. Indeed, DE can applied to graphs whose nodes have a tree-like neighborhood down to a certain depth (which is directly related to the number of \AG{decoding iterations}). To determine the limiting behavior in terms of iterative decoding threshold, the number of iterations (and, consequently, the depth for which a tree-like neighborhood is required) has to be taken to infinity. This demands for the block length to grow large (i.e., in the limit, to infinity). However, even if one would be able to construct a \emph{sequence} of PCs with increasing block length, the requirement of a tree-like neighborhood down to a growing depth cannot be attained by any PC. The girth of the Tanner graph of a PC is, in fact, $8$ (\SH{a length-$8$} cycle in the Tanner graph of the PC of Fig.~\ref{PCgraph}(b) is highlighted), jeopardizing the possibility to perform a proper DE analysis.\footnote{\GL{A notable exception, where a DE analysis of PCs can be performed, was described in \cite{Haeger2017tit} for the binary erasure channel for the limiting case where the code rate tends to $1$}.} Hence, rather than analyzing a specific PC, we resort to the analysis of the GLDPC ensemble encompassing the PC.} 
	
\GL{\AGc{
%Fig. \ref{PCgraph1}(a) depicts the Tanner graph of a  GLDPC code based on the same set of nodes as the PC in  Fig.~\ref{PCgraph}. 
\SHm{Fig. \ref{PCgraph}(c) depicts the Tanner graph of a generic PC with $n^2$ degree-$2$ VNs and $2n$ degree-$n$ CNs, where the permutation of the edges is represented by the edge interleaver $\pi$. The PC is defined by a particular permutation. Note also that this Tanner graph corresponds to that of a regular GLDPC code for which all CNs are associated the same $(n,k)$ component code.
%The GLDPC code can be defined by an arbitrary permutation of the edges (represented by the edge interleaver $\pi$  in Fig.~\ref{PCgraph1}). 
%For \SH{a} given set of VNs, CNs, and node degrees, an (unstructured) GLDPC code ensemble is the set of codes defined by all possible edge permutations, i.e., all possible choices for $\pi$. %Assuming all CNs to be associated the same $(n,k)$ component code, the Tanner graph defines a regular GLDPC code. 
The set of codes defined by all possible edge permutations yields the GLDPC code ensemble, which contains among its members the two-dimensional \SH{PC} based on the $(n,k)$ component code.}}
%NOTE: I DO NOT SEE THE NEED TO INTRODUCE TWO TANNER GRAPHS IN FIG. 3. WE ARE REPEATING THINGS UNNECESSARILY. FOR EXAMPLE,  WE SAY HOW A GLDPC CODE ENSEMBLE IS OBTAINED TWICE! WE CAN SAFELY SHORTEN THIS PARAGRAPH.} 
To perform a  DE analysis, we  will need to untangle the number of VNs and CNs from the component code block length: The regular GLDPC code ensembles on interest will be hence defined by the CNs component code, the degree of the VNs (which is $2$), and the block length (i.e., the number of VNs).} 
%\GLC{I've had to add this last remark, since Figure 3 carries the problem of linking the block length with the component code length.}

%\medskip

%\GLC{I've explained in detail the two figures. I still feel that this is super-well-known and basic stuff, so I'm open to remove it. But we shall then consider removing the figures as well... Also, we need to be careful in removing definitions that we will need in the next part}

\GL{\begin{remark}
	The Tanner graph of a product/GLDPC code can be used to describe the message-passing schedule between VNs and CNs. Typically, GLDPC ensembles are analyzed under the assumption of a flooding schedule, i.e., at each iteration VNs pass a message to all the neighboring CNs, and all CNs pass a message to their neighboring VNs. While the principle can be applied to PCs, too, in the following we will analyze a message passing schedule that follows from the decoding algorithm described in Sec.~III. To this aim, we divide the CNs into two classes (or types): The class of CNs associated to row decoders and the class of CNs associated to column decoders. The decoding schedule, hence, will involve two half-iterations: In the first half-iteration, only CNs associated to column decoders are active, while in the second half-iteration only CNs associated to row decoders are active.
\end{remark}}

\GL{\begin{remark}
		A further difference in the analysis with respect to the classical analysis of GLDPC code ensembles stems from the DE analysis approach proposed in \cite{JianPfister2017} for GLDPC code ensembles under iBDD. Typically, a VN in a GLDPC code Tanner graph gets as input the corresponding codeword bit channel observation, and it forwards to the neighboring CNs an updated belief. The updated belief accounts for the channel observation and the extrinsic information provided at the previous iteration, by the CNs output. As suggested in \cite{JianPfister2017}, when analyzing the iBDD algorithm it is particularly convenient to provide the channel observations as input to each CN directly, and to let VNs act as simple message routers (i.e., they forward the message received along one edge over the other edge without performing any modification). We adopt this approach in the following. It follows that the combining of the channel soft information with the output of the local BD decoders takes place within the CNs, as emphasized in Fig.~\ref{SysPCCST}.
\end{remark}}

\medskip

\GL{In order to proceed with the DE analysis, a further important aspect needs to be addressed. In particular, the decoding rule specified for the component code decoders in Sec.~III has to be modified in order to render the message passing \emph{extrinsic}.}
%\AGc{In particular, when a CN performs a decision regarding a given bit, the information provided at its input for the given bit has to be depurated from the bit estimate computed by the other CN protecting the bit, at the previous half-iteration. NOTE: IN MY OPINION THIS CAN BE REMOVED. WE CAN SAFELY ASSUME THAT THE CONCEPT OF EXTRINSIC INFORMATION IS KNOWN TO THE READER.}}
From \eqref{eq:BDDchrel_VN}--\eqref{eq:BDDchrel_VN_scale}, one can infer that the standard row (column) decoding of PCs using iBDD-CR does not fall into the extrinsic decision rule category, as the input of row (column) decoder for iteration $\ell$ is the output of column (row) decoder from the iteration $\ell-1$. Therefore, \AG{for analysis purposes, similar to \cite{JianPfister2017,sheikhTCOM19} we modify the algorithm} such that the BDD of the component code is substituted by \GL{an extrinsic rule relying on BDD} \cite[Sec.~II.B]{JianPfister2017}. 
 
 \GL{T}he iBDD-CR decoding \GL{algorithm} with extrinsic BDD of the component codes is explained in the following. \SH{Without loss of generality}, we consider the decoding of the $i$-th row of the PC \SH{at iteration $\ell$}, corresponding to the transmitted component code codeword $\lc=(c_1,\ldots,c_{\nc})$, where the input for the $i$-th row decoder is $\bm{\Psi}_{i,:}^{\mathsf{c},(\ell-1)}=(\psi_{i,1}^{\mathsf{c},(\ell-1)},\ldots,\psi_{i,n}^{\mathsf{c},(\ell-1)})$. For the decision on the code bit $c_{i,j}$, the $j$-th component of $\bm{\Psi}_{i,:}^{\mathsf{c},(\ell-1)}$ is substituted by the channel output $r_{i,j}$, and hence, \SH{$\tilde{\boldsymbol{\psi}}  \buildrel \Delta \over = 
(\psi_{i,1}^{\mathsf{c},(\ell-1)},\ldots,r_{i,j},\ldots,\psi_{i,n}^{\mathsf{c},(\ell-1)})$ is used as the input for BDD. After performing extrinsic BDD, the outbound message on code bit $c_{i,j}$ ($\bar\mu_{i,j}^{\mathsf r}$)  is}
\SH{
\begin{equation}
\label{eq:BDD_VN_extrinsic}
\scalemath{0.912}{	\bar\mu_{i,j}^{\mathsf r} = \begin{cases}
	{(-1)}^{c_i} & \text{if } \ham(\lc, \tilde{\boldsymbol{\psi}})  \leq t \\
	{(-1)}^{\tilde{c}_i} & \text{if } \ham(\lc, \tilde{\boldsymbol{\psi}}) > t \text{ and
	} \exists{\tilde{\lc}} \; \text{such that} \;\ham(\tilde{\lc}, \tilde{\boldsymbol{\psi}}) \leq t \\
	0 & \text{otherwise}
	\end{cases},}
\end{equation}
where $\tilde{\lc}=(\tilde{c}_1,\ldots,\tilde{c}_{\nc})$ is a valid component codeword. The same decoding rule can be employed for iBDD-CR decoding with extrinsic BDD of column codes.}

%After decoding of all row codes, consider the 
%decoding of the $j$-th column of the PC, corresponding to the transmitted component code word $\lcp=({c'}_1,\ldots,{c'}_{\nc})$, where the input for the $j$-th column decoder is $\bm{\Psi}_{:,j}^{\mathsf{r},(\ell)}=(\psi_{1,j}^{\mathsf{r},(\ell)},\cdots,\psi_{n,j}^{\mathsf{c},(\ell)})^\mathsf{T}$. For the decision on the code bit $c_{i,j}$, the $i$-th component of the $\bm{\Psi}_{:,j}^{\mathsf{r},(\ell)}$ is substituted by the channel output $r_{i,j}$, and hence, $\tilde{\boldsymbol{r}}'  \buildrel \Delta \over = 
%(\psi_{1,j}^{\mathsf{r},(\ell)},\cdots,r_{i,j},\cdots,\psi_{n,j}^{\mathsf{r},(\ell)})$ is used as the input for BDD. In iBDD-CR, the decision on code bit $c_{i,j}$, i.e, $\bar\mu_{i,j}^{\mathsf c}$ with extrinsic BDD of component code can be written as
%\begin{equation}
%\label{eq:BDD_VN_extrinsic1}
%\scalemath{0.912}{	\bar\mu_{i,j}^{\mathsf c} = \begin{cases}
%	{(-1)}^{c'_i} & \text{if } \ham(\lc', \tilde{\boldsymbol{r}}')  \leq t \\
%	{(-1)}^{\tilde{c}'_i} & \text{if } \ham(\lc', \tilde{\boldsymbol{r}}') > t \text{ and
%	} \exists{\tilde{\lc}'} \; \text{such that} \;\ham(\tilde{\lc}', \tilde{\boldsymbol{r}}') \leq t \\
%	0 & \text{otherwise}
%	\end{cases}.}
%\end{equation} 

%\AGc{In the context of message passing over the GLDPC graph based on extrinsic BDD, at the $i$-th VN, the input message from the $j$-th CN is forwarded to the $j'$-th CN. At the $j$-th CN, the input corresponding to the $i$-th VN is replaced by the hard-detected bit $r_i$ at the channel output. This notion of extrinsic message passing has been considered for the DE analysis of iBDD and iBDD-SR in \cite{JianPfister2017,sheikhTCOM19}, respectively which is schematized in \cite[Fig.~4]{sheikhTCOM19}. NOTE: I WOULD REMOVE THIS PARAGRAPH. WE ARE BASICALLY REPEATING WHAT SAID IN REMARK 2.}      

\begin{remark}\label{rr0}
The extrinsic decoding rule \AG{requires decoding $n$ times each  component code}, which is  complex for a practical system. We highlight that we only use the extrinsic decoding rule to derive the DE \GL{analysis}, whereas for the performance evaluation via simulations we use the more practical decoder outlined in Sec.~\ref{sys_mod}, i.e., the standard (intrinsic) row/column decoding of the component codes is employed.  
\end{remark}

%\GLC{I moved part of you remark in remark 2, so we concentrate all discussions about the nature of VNs there.}

We consider transmission over the bi-AWGN channel where a length-$n$ BCH component code with error-correcting capability $t$ is assumed at the CNs. 
We denote by $p_\mathsf{ch}$ the channel output error probability yielded by applying hard detection to the bi-AWGN channel output\GL{, i.e.,} $p_\mathsf{ch}=\Q\left(\frac{1}{\sigma}\right)$. We consider two sets of equal size for CNs, where each set defines a CN type. We refer to the two CN types as row and column CN types. Each VN is connected to one row-type CN and to one column-type CN. Each decoding iteration consists of one row CN \GL{processing}, followed by one column CN \GL{processing}. In the following, we denote by $\mep$ the error probability associated to the messages exchanged by the component decoders \GL{(via VNs)}. In particular, we denote by $\mep^{\row,(\ell)}$ and $\mep^{\col,(\ell)}$ the message error probability at the output of the row component decoder (row-type CN) and column component decoder (column-type CN), respectively, at the $\ell$th iteration. The message error probability at the input of a row-type CN at the $\ell$th iteration is given by $\mep^{\col,(\ell-1)}$, whereas the message error probability at the input of a column-type CN during the $\ell$th iteration is $\mep^{\row,(\ell)}$.

Without loss of generality,  \AG{consider the row-type CN operation at iteration $\ell$ of \GL{DE}}.%\footnote{Recall that in the context of iBDD-CR for PCs, this corresponds to row decoding with $\bm{\Psi}^{\mathsf{c},(\ell-1)}_{i,:}$ as the input which yields $\psi_{i,j}^{\mathsf{r},(\ell)}$, as the input to $j$-th column decoder corresponding to code bit $c_{i,j}$, as shown in Fig.~\ref{SysPCCST}.}
The combining yields \GL{a soft estimate for} code bit $c_{i,j}$, i.e., ${\opt}_{i,j}^{\mathsf r,(\ell)}$, given the corresponding BDD output and channel LLR are $\bar{\mu}_{i,j}^{\mathsf r,(\ell)}$ and $l_{i,j}$, respectively. By applying Bayes' rule to the definition of ${\opt}_{i,j}^{\mathsf r,(\ell)}$, \GL{we have}
\GL{\begin{align}
\opt_{i,j}^{\row,(\ell )} & \buildrel \Delta \over = \ln \frac{{p\left( {\bar \mu _{i,j}^{\row,(\ell )},{{l}_{i,j}}|{c_{i,j}} = 0} \right)}}{{p\left( {\bar \mu _{i,j}^{\row,(\ell )},{{l}_{i,j}}|{c_{i,j}} = 1} \right)}} \nonumber \\ & = \ln \frac{{p\left( {\bar \mu _{i,j}^{\row,(\ell )}|{{l}_{i,j}},{c_{i,j}} = 0} \right)}}{{p\left( {\bar \mu _{i,j}^{\row,(\ell )}|{{l}_{i,j}},{c_{i,j}} = 1} \right)}} + \ln \frac{{p\left( {{{l}_{i,j}}|{c_{i,j}} = 0} \right)}}{{p\left( {{{l}_{i,j}}|{c_{i,j}} = 1} \right)}} \nonumber\\
&=\ln \frac{{p\left( {\bar \mu _{i,j}^{\row,(\ell )}|{l_{i,j}},{c_{i,j}} = 0} \right)}}{{p\left( {\bar \mu _{i,j}^{\row,(\ell )}|{l_{i,j}},{c_{i,j}} = 1} \right)}} + l_{i,j}. \label{LLRoutnosimp} 
\end{align} 
where $l_{i,j}=\frac{2}{\sigma^2}y_{i,j}$. \AG{It follows  $l_{i,j}\sim\mathcal{N}(2/\sigma^2,4/\sigma^2)$ if $c_{i,j}=0$ and $l_{i,j}\sim\mathcal{N}(-2/\sigma^2,4/\sigma^2)$ if $c_{i,j}=1$}.} 

In general, computing the first term of \eqref{LLRoutnosimp} is a formidable task, since the extrinsic decoding rule \eqref{eq:BDD_VN_extrinsic} results in a statistical dependence between $\bar \mu _{i,j}^{\row,(\ell )}$ and $l_{i,j}$. \AG{The trick for computing  this term is that} $\bar \mu _{i,j}^{\row,(\ell )}$ only depends on the \GL{hard value} $\hat l_{i,j}$ and not \GL{on the reliability} $|l_{i,j}|$.\footnote{\GL{Recall that, for what concerns the computation of $\opt_{i,j}^{\row,(\ell )}$} in the extrinsic message passing decoding rule, the input of the row decoder corresponding to code bit $c_{i,j}$ is \GL{given} by $r_{i,j}=\BB(l_{i,j})$. As $\BB(\cdot)$ is a mapper operating on the sign of its input, $\bar \mu _{i,j}^{\row,(\ell )}$ only depends on $\hat l_{i,j}$.} In fact, the extrinsic decoding rule imposes that $l_{i,j} \to \hat l_{i,j} \to \bar \mu _{i,j}^{\row,(\ell )}$ forms a Markov chain, i.e., given $\hat l_{i,j}$, \SH{$\bar \mu _{i,j}^{\row,(\ell )}$} and $l_{i,j}$ are statistically independent. Therefore, one can \GL{re-state} \eqref{LLRoutnosimp} as
\begin{align}\label{LLRoutv1} 
\opt_{i,j}^{\row,(\ell )} = \mut_{i,j}^{\row,(\ell )} + l_{i,j}
\end{align}      
where $\mut_{i,j}^{\row,(\ell )}$ is defined as
\begin{align}\label{mutild_r} 
\mut_{i,j}^{\row,(\ell )}\triangleq\ln \frac{{p\left( {\bar \mu _{i,j}^{\row,(\ell )}|{\hat l_{i,j}},{c_{i,j}} = 0} \right)}}{{p\left( {\bar \mu _{i,j}^{\row,(\ell )}|{\hat l_{i,j}},{c_{i,j}} = 1} \right)}}.
\end{align}  
Similarly, for the column-type CN operation the \GL{soft value} of the VN corresponding to code bit $c_{i,j}$ is derived as 
\begin{align}\label{LLRoutv1_col} 
\opt_{i,j}^{\col,(\ell )} = \mut_{i,j}^{\col,(\ell )} + l_{i,j}, 
\end{align}    
where $\mut_{i,j}^{\col,(\ell )}$ is defined as
\begin{align}\label{mutild_c} 
\mut_{i,j}^{\col,(\ell )}\triangleq\ln \frac{{p\left( {\bar \mu _{i,j}^{\col,(\ell )}|{\hat l_{i,j}},{c_{i,j}} = 0} \right)}}{{p\left( {\bar \mu _{i,j}^{\col,(\ell )}|{\hat l_{i,j}},{c_{i,j}} = 1} \right)}}.
\end{align} 

\begin{figure*}[t]
	\vspace*{-20pt}
	\hrulefill		
	\begin{align}\label{xrl}
	& \nonumber \mep^{\row,(\ell)} = \scalemath{0.8}{f^{\Pue}(\mep^{\col,(\ell-1)}){\Q\left( { \frac{\sigma}{2}\text{min}\left(\ln \left( \frac{f^{\Pue}(\mep^{\col,(\ell-1)})}{f^{\Quc}(\mep^{\col,(\ell-1)})}\right),0\right)+\frac{1}{\sigma}} \right) } + f^{\Puc}(\mep^{\col,(\ell-1)}){\Q\left( { \frac{\sigma}{2}\text{min}\left(\ln \left( \frac{f^{\Puc}(\mep^{\col,(\ell-1)})}{f^{\Que}(\mep^{\col,(\ell-1)})}\right),0\right)+\frac{1}{\sigma}} \right) }} + \\ \nonumber &  \scalemath{0.85}{f^{\Puep}(\mep^{\col,(\ell-1)}){\Q\left( { \frac{\sigma}{2}\text{min}\left(\ln \left(\frac{f^{\Puep}(\mep^{\col,(\ell-1)})}{f^{\Quep}(\mep^{\col,(\ell-1)})}\right),0\right)+\frac{1}{\sigma}} \right) } + f^{\Que}(\mep^{\col,(\ell-1)})\left( {\Q\left( { \frac{\sigma}{2}\ln \left(\frac{f^{\Que}(\mep^{\col,(\ell-1)})}{f^{\Puc}(\mep^{\col,(\ell-1)})}\right)+\frac{1}{\sigma}} \right)}-\Q\left(\frac{1}{\sigma}\right) \right) \cdot \bar{\mathbbm{U}} \left( \ln \left(\frac{f^{\Que}(\mep^{\col,(\ell-1)})}{f^{\Puc}(\mep^{\col,(\ell-1)})}\right) \right)} \\ \nonumber & + \scalemath{0.85}{f^{\Quc}(\mep^{\col,(\ell-1)})\left( {\Q\left( { \frac{\sigma}{2}\ln \left(\frac{f^{\Quc}(\mep^{\col,(\ell-1)})}{f^{\Pue}(\mep^{\col,(\ell-1)})}\right)+\frac{1}{\sigma}} \right)}-\Q\left(\frac{1}{\sigma}\right) \right) \cdot \bar{\mathbbm{U}} \left( \ln \left(\frac{f^{\Quc}(\mep^{\col,(\ell-1)})}{f^{\Pue}(\mep^{\col,(\ell-1)})}\right) \right)} \\  & + \scalemath{0.85}{f^{\Quep}(\mep^{\col,(\ell-1)})\left( {\Q\left( { \frac{\sigma}{2}\ln \left(\frac{f^{\Quep}(\mep^{\col,(\ell-1)})}{f^{\Puep}(\mep^{\col,(\ell-1)})}\right)+\frac{1}{\sigma}} \right)}-\Q\left(\frac{1}{\sigma}\right) \right) \cdot \bar{\mathbbm{U}} \left( \ln \left(\frac{f^{\Quep}(\mep^{\col,(\ell-1)})}{f^{\Puep}(\mep^{\col,(\ell-1)})}\right) \right)}
	\end{align}
	\hrulefill
\end{figure*}

\begin{figure*}[t]
	\vspace*{-20pt}
	\hrulefill		
	\begin{align}\label{xcl}
	& \nonumber \mep^{\col,(\ell)}  = \scalemath{0.85}{f^{\Pue}(\mep^{\row,(\ell)}){\Q\left( { \frac{\sigma}{2}\text{min}\left(\ln \left( \frac{f^{\Pue}(\mep^{\row,(\ell)})}{f^{\Quc}(\mep^{\row,(\ell)})}\right),0\right)+\frac{1}{\sigma}} \right) } + f^{\Puc}(\mep^{\row,(\ell)}){\Q\left( { \frac{\sigma}{2}\text{min}\left(\ln \left( \frac{f^{\Puc}(\mep^{\row,(\ell)})}{f^{\Que}(\mep^{\row,(\ell)})}\right),0\right)+\frac{1}{\sigma}} \right) }} + \\ \nonumber &  \scalemath{0.85}{f^{\Puep}(\mep^{\row,(\ell)}){\Q\left( { \frac{\sigma}{2}\text{min}\left(\ln \left(\frac{f^{\Puep}(\mep^{\row,(\ell)})}{f^{\Quep}(\mep^{\row,(\ell)})}\right),0\right)+\frac{1}{\sigma}} \right) } + f^{\Que}(\mep^{\row,(\ell)})\left( {\Q\left( { \frac{\sigma}{2}\ln \left(\frac{f^{\Que}(\mep^{\row,(\ell)})}{f^{\Puc}(\mep^{\row,(\ell)})}\right)+\frac{1}{\sigma}} \right)}-\Q\left(\frac{1}{\sigma}\right) \right) \cdot \bar{\mathbbm{U}} \left( \ln \left(\frac{f^{\Que}(\mep^{\row,(\ell)})}{f^{\Puc}(\mep^{\row,(\ell)})}\right) \right)} \\ \nonumber & + \scalemath{0.85}{f^{\Quc}(\mep^{\row,(\ell)})\left( {\Q\left( { \frac{\sigma}{2}\ln \left(\frac{f^{\Quc}(\mep^{\row,(\ell)})}{f^{\Pue}(\mep^{\row,(\ell)})}\right)+\frac{1}{\sigma}} \right)}-\Q\left(\frac{1}{\sigma}\right) \right) \cdot \bar{\mathbbm{U}} \left( \ln \left(\frac{f^{\Quc}(\mep^{\row,(\ell)})}{f^{\Pue}(\mep^{\row,(\ell)})}\right) \right)} \\  & + \scalemath{0.85}{f^{\Quep}(\mep^{\row,(\ell)})\left( {\Q\left( { \frac{\sigma}{2}\ln \left(\frac{f^{\Quep}(\mep^{\row,(\ell)})}{f^{\Puep}(\mep^{\row,(\ell)})}\right)+\frac{1}{\sigma}} \right)}-\Q\left(\frac{1}{\sigma}\right) \right) \cdot \bar{\mathbbm{U}} \left( \ln \left(\frac{f^{\Quep}(\mep^{\row,(\ell)})}{f^{\Puep}(\mep^{\row,(\ell)})}\right) \right)}
	\end{align}
	\hrulefill
\end{figure*} 
\GL{The derivation of $\mut_{i,j}^{\row,(\ell )}$ and $\mut_{i,j}^{\col,(\ell )}$ under extrinsic iBDD-CR decoding is provided by the next proposition.}
%\footnote{
%	\GLC{I thought that, to avoid confusing the reader, the comment on how iBDD-SR does things would fit better in a footnote... This way we avoid steering the reader's attention away from the core of the section.}
%	In the iBDD-SR algorithm $\opt_{i,j}^{\col,(\ell )}$ and $\opt_{i,j}^{\row,(\ell )}$ are heuristically modeled as $w^{\row}_{\ell}\cdot\bar \mu _{i,j}^{\row,(\ell )} + l_{i,j}$ and $w^{\col}_{\ell}\cdot\bar \mu _{i,j}^{\col,(\ell )} + l_{i,j}$, respectively, where $w^{\row}_{\ell}$ and $w^{\col}_{\ell}$ are two scaling factors optimized through DE. Comparing \eqref{LLRoutv1} and \eqref{LLRoutv1_col} with $\opt_{i,j}^{\col,(\ell )}$ and $\opt_{i,j}^{\row,(\ell )}$ values of iBDD-SR, once can infer that in iBDD-SR, with the notation given in this paper $\mut_{i,j}^{\row,(\ell )}= w^{\row}_{\ell}\cdot\bar \mu _{i,j}^{\row,(\ell )}$ and $\mut_{i,j}^{\row,(\ell )}= w^{\col}_{\ell}\cdot\bar \mu _{i,j}^{\row,(\ell )}$.
%}

\medskip

%\GLC{I rather call the next a proposition: for a Theorem, there should be something more than a plain unrolling of the equations...Also, a Theorem would need a more formal casting, i.e., an hypothesis and a thesis...}

\begin{proposition}\label{p1lab}
\GL{Over the bi-AWGN channel, \SH{the values of}} $\mut_{i,j}^{\row,(\ell )}$ and $\mut_{i,j}^{\col,(\ell )}$ are provided in Table~\ref{Tabcomp} and Table~\ref{Tabcompc}, respectively,  \GL{where the} message error probability at the row-type and column-type CN at the $\ell$th iteration is given by \eqref{xrl} and \eqref{xcl}, respectively, with $\mep^{\col,(0)}=p_\mathsf{ch}$, \GL{and the} values of $f^{\Pue}(\mep)$, $f^{\Puc}(\mep)$, $f^{\Que}(\mep)$, $f^{\Quc}(\mep)$, $f^{\Puep}(\mep)$, $f^{\Quep}(\mep)$ in Tables~\ref{Tabcomp}--\ref{Tabcompc} are derived in \eqref{pe}--\eqref{q_ep}.
\end{proposition}


\GL{The proof of Proposition \ref{p1lab} is given in Appendix~\ref{APP1}. Owing to \eqref{LLRoutnosimp}, we remark that the combining rule is optimal for GLDPC code ensembles in the sense of minimizing the message error probability, under extrinsic message passing decoding, in the limit of infinitely large blocks.}


\begin{table}[t]	
	\caption{$\mut _{i,j}^{\row,(\ell )}$ for row-type CN operation (row decoding for PC) over iteration $\ell$, based on the corresponding BDD output $\mu _{i,j}^{\row,(\ell )}$ and channel \GL{output hard decision} $\hat l_{i,j}$.}
	%\vspace{-2ex}
	\centering
	\scalebox{1.2}{	
		\begin{tabular}{ccc}
			\toprule
			$\bar \mu _{i,j}^{\row,(\ell )}$ &
			$\hat l_{i,j}$ &
			$\mut_{i,j}^{\row,(\ell )}$ \\[0.1cm]
			\midrule
			$-1$ & $-1$ & $\ln \frac{f^{\Pue}(\mep^{\col,(\ell-1)})}{f^{\Quc}(\mep^{\col,(\ell-1)})}$ \\[0.3cm]
			$1$  & $-1$ & $\ln \frac{f^{\Puc}(\mep^{\col,(\ell-1)})}{f^{\Que}(\mep^{\col,(\ell-1)})}$ \\ [0.3cm]
			$0$ & $-1$ & $\ln \frac{f^{\Puep}(\mep^{\col,(\ell-1)})}{f^{\Quep}(\mep^{\col,(\ell-1)})}$  \\ [0.3cm]
			$-1$  & $1$ & $\ln \frac{f^{\Que}(\mep^{\col,(\ell-1)})}{f^{\Puc}(\mep^{\col,(\ell-1)})}$ \\[0.3cm]
			$1$ & $1$ & $\ln \frac{f^{\Quc}(\mep^{\col,(\ell-1)})}{f^{\Pue}(\mep^{\col,(\ell-1)})}$ \\ [0.3cm]
			$0$  & $1$ & $\ln \frac{f^{\Quep}(\mep^{\col,(\ell-1)})}{f^{\Puep}(\mep^{\col,(\ell-1)})}$ \\[0.3cm]			
			\bottomrule
		\end{tabular}
	}
	\label{Tabcomp}
\end{table} 


\begin{table}[t]	
	\caption{$\mut _{i,j}^{\col,(\ell )}$ for column-type CN operation (column decoding for PC) over iteration $\ell$, based on the corresponding BDD output $\mu _{i,j}^{\col,(\ell )}$ and channel \GL{output hard decision} $\hat l_{i,j}$. 
		%The values of $f^{\Pue}(\mep)$, $f^{\Puc}(\mep)$, $f^{\Que}(\mep)$, $f^{\Quc}(\mep)$, $f^{\Puep}(\mep)$, $f^{\Quep}(\mep)$ in are derived in \eqref{pe}--\eqref{q_ep}.
	}
	%\vspace{-2ex}
	\centering
	\scalebox{1.2}{	
		\begin{tabular}{ccc}
			\toprule
			$\bar \mu _{i,j}^{\col,(\ell )}$ &
			$\hat l_{i,j}$ &
			$\mut_{i,j}^{\col,(\ell )}$ \\[0.1cm]
			\midrule
			$-1$ & $-1$ & $\ln \frac{f^{\Pue}(\mep^{\row,(\ell)})}{f^{\Quc}(\mep^{\row,(\ell)})}$ \\[0.3cm]
			$1$  & $-1$ & $\ln \frac{f^{\Puc}(\mep^{\row,(\ell)})}{f^{\Que}(\mep^{\row,(\ell)})}$ \\ [0.3cm]
			$0$ & $-1$ & $\ln \frac{f^{\Puep}(\mep^{\row,(\ell)})}{f^{\Quep}(\mep^{\row,(\ell)})}$  \\ [0.3cm]
			$-1$  & $1$ & $\ln \frac{f^{\Que}(\mep^{\row,(\ell)})}{f^{\Puc}(\mep^{\row,(\ell)})}$ \\[0.3cm]
			$1$ & $1$ & $\ln \frac{f^{\Quc}(\mep^{\row,(\ell)})}{f^{\Pue}(\mep^{\row,(\ell)})}$ \\ [0.3cm]
			$0$  & $1$ & $\ln \frac{f^{\Quep}(\mep^{\row,(\ell)})}{f^{\Puep}(\mep^{\row,(\ell)})}$ \\[0.3cm]			
			\bottomrule
		\end{tabular}
	}
	\label{Tabcompc}
\end{table} 
%We highlight that table.~\ref{Tabcomp} only contains three different absolute values for $\tilde{\mu}_{i,j}^{\row,(\ell )}$, which in turn reduces the memory required for the storage of $\tilde{\mu}_{i,j}^{\row,(\ell )}$ over different iterations.
\begin{figure}[!t] \centering 
	\includegraphics[width=\columnwidth]{BICM_system_model.pdf}  
	%\vspace{-2ex}
	\caption{\GL{Summary of the notation for the BICM scheme}. \SH{The message $\boldsymbol{u}$ is encoded to $\boldsymbol{b}$. After interleaving and mapping, $\boldsymbol{x}$ is sent through a \GL{Gaussian} channel \GL{where the} noise $\boldsymbol{n}$ \GL{is added}, resulting in $\boldsymbol{y}$. \GL{The LLRs corresponding to $\boldsymbol{b}$ are denoted as} $\boldsymbol{l}$, \GL{whereas the decoder output is} $\hat{\boldsymbol{u}}$. $\Phi$ maps $m$ bits to a \GL{modulation} symbol and $\Phi^{-1}$ maps a symbol to $m$ bits.}} 
	\label{BICM} 
	\vspace{-2ex}
\end{figure}







%	\begin{table}[t]
%	%\renewcommand{\tabcolsep}{0.15cm}
%	%\vspace{-0.5cm}
%	\caption {BRGC of the symbols for 8-ASK.} \label{T2}
%	\vspace{-0.25cm}
%	\begin{center}
%		%	\scriptsize
%		\vspace{-2ex}
%		\begin{center}\begin{tabular}{ccccccccc}
%				\arrayrulecolor{black}\hline
%				\toprule
%				
%				
%				symbols & -7 & -5 & -3 & -1 & 1  & 3 & 5 & 7 \\
%				\midrule
%				bit-level $1$  &  1 & 1 & 1 & 1 & 0 & 0 & 0 & 0 \\
%				\midrule
%				bit-level $2$  & 1 & 1 & 0 & 0 & 0 & 0 & 1 & 1
%				\\
%				\midrule
%				bit-level $3$  & 1 & 0 & 0 & 1 & 1 & 0 & 0 & 1 \\
%				\midrule
%				label  &  111 & 110 & 100 & 101 & 001  & 000 & 010 & 011 \\
%				\hline
%				\toprule                			
%		\end{tabular} \end{center}
%	\end{center}
%	\label{labeling}
%	\vspace{-0.5cm}
%\end{table}

\begin{figure*}[t]
	\vspace*{-20pt}
	\hrulefill		
	\begin{align}\label{QAMxrl}
	& \scalemath{0.78}{\nonumber \mep^{\row,(\ell)}}  \scalemath{0.8}{ \buildrel \Delta \over = \mathsf{g}(\mep^{\col,(\ell-1)}, \sigma, M) = f^{\Pue}(\mep^{\col,(\ell-1)}){\sum\limits_{j = 0}^{M/2 - 1} {w_j} {\Q\biggl( \frac{{ \text{min}\left(\ln \left( \frac{f^{\Pue}(\mep^{\col,(\ell-1)})}{f^{\Quc}(\mep^{\col,(\ell-1)})}\right),0\right)+\mu_{j}}}{\sigma_{j}} \biggr) } } + f^{\Puc}(\mep^{\col,(\ell-1)}){\sum\limits_{j = 0}^{M/2 - 1} {w_j} {\Q\biggl( \frac{{ \text{min}\left(\ln \left( \frac{f^{\Puc}(\mep^{\col,(\ell-1)})}{f^{\Que}(\mep^{\col,(\ell-1)})}\right),0\right)+\mu_{j}}}{\sigma_{j}} \biggr) } }} + \\ \nonumber &  \scalemath{0.78}{f^{\Puep}(\mep^{\col,(\ell-1)}){\sum\limits_{j = 0}^{M/2 - 1} {w_j} {\Q\biggl( \frac{{ \text{min}\left(\ln \left( \frac{f^{\Puep}(\mep^{\col,(\ell-1)})}{f^{\Quep}(\mep^{\col,(\ell-1)})}\right),0\right)+\mu_{j}}}{\sigma_{j}} \biggr) } } + f^{\Que}(\mep^{\col,(\ell-1)})\biggl( {\sum\limits_{j = 0}^{M/2 - 1} {w_j} \biggl( {\Q\biggl( \frac{{ \ln \left( \frac{f^{\Que}(\mep^{\col,(\ell-1)})}{f^{\Puc}(\mep^{\col,(\ell-1)})}\right)+\mu_{j}}}{\sigma_{j}} \biggr) } - \Q\biggl(\frac{\mu_{j}}{\sigma_{j}} \biggr) \biggr) } \biggr) \cdot \bar{\mathbbm{U}} \biggl( \ln \left(\frac{f^{\Que}(\mep^{\col,(\ell-1)})}{f^{\Puc}(\mep^{\col,(\ell-1)})}\right) \biggr)} \\ \nonumber & + \scalemath{0.78}{f^{\Quc}(\mep^{\col,(\ell-1)})\biggl( {\sum\limits_{j = 0}^{M/2 - 1} {w_j}  \biggl( {\Q\biggl( \frac{{ \ln \left( \frac{f^{\Quc}(\mep^{\col,(\ell-1)})}{f^{\Pue}(\mep^{\col,(\ell-1)})}\right)+\mu_{j}}}{\sigma_{j}} \biggr) } - \Q\left(\frac{\mu_{j}}{\sigma_{j}} \right) \biggr) } \biggr) \cdot \bar{\mathbbm{U}} \left( \ln \left(\frac{f^{\Quc}(\mep^{\col,(\ell-1)})}{f^{\Pue}(\mep^{\col,(\ell-1)})}\right) \right)} \\  & + \scalemath{0.78}{f^{\Quep}(\mep^{\col,(\ell-1)})\biggl( {\sum\limits_{j = 0}^{M/2 - 1} {w_j}  \biggl({\Q\biggl( \frac{{ \ln \biggl( \frac{f^{\Quep}(\mep^{\col,(\ell-1)})}{f^{\Puep}(\mep^{\col,(\ell-1)})}\biggr)+\mu_{j}}}{\sigma_{j}} \biggr) } - \Q\biggl(\frac{\mu_{j}}{\sigma_{j}}\biggr) \biggr) } \biggr( \cdot \bar{\mathbbm{U}} \biggl( \ln \left(\frac{f^{\Quep}(\mep^{\col,(\ell-1)})}{f^{\Puep}(\mep^{\col,(\ell-1)})}\right) \biggr)}
	\end{align}
	\hrulefill
\end{figure*} 

\begin{figure*}[t]
	\vspace*{-20pt}
	\hrulefill		
	\begin{align}\label{QAMxcl}
	& \nonumber \mep^{\col,(\ell)} \buildrel \Delta \over = \mathsf{g}(\mep^{\row,(\ell)}, \sigma, M) = \scalemath{0.8}{f^{\Pue}(\mep^{\row,(\ell)}){\sum\limits_{j = 0}^{M/2 - 1} {w_j} {\Q\biggl( \frac{{ \text{min}\left(\ln \left( \frac{f^{\Pue}(\mep^{\row,(\ell)})}{f^{\Quc}(\mep^{\row,(\ell)})}\right),0\right)+\mu_{j}}}{\sigma_{j}} \biggr) } } + f^{\Puc}(\mep^{\row,(\ell)}){\sum\limits_{j = 0}^{M/2 - 1} {w_j} {\Q\biggl( \frac{{ \text{min}\left(\ln \left( \frac{f^{\Puc}(\mep^{\row,(\ell)})}{f^{\Que}(\mep^{\row,(\ell)})}\right),0\right)+\mu_{j}}}{\sigma_{j}} \biggr) } }} + \\ \nonumber &  \scalemath{0.8}{f^{\Puep}(\mep^{\row,(\ell)}){\sum\limits_{j = 0}^{M/2 - 1} {w_j} {\Q\biggl( \frac{{ \text{min}\left(\ln \left( \frac{f^{\Puep}(\mep^{\row,(\ell)})}{f^{\Quep}(\mep^{\row,(\ell)})}\right),0\right)+\mu_{j}}}{\sigma_{j}} \biggr) } } + f^{\Que}(\mep^{\row,(\ell)})\biggl( {\sum\limits_{j = 0}^{M/2 - 1} {w_j} \biggl( {\Q\biggl( \frac{{ \ln \left( \frac{f^{\Que}(\mep^{\row,(\ell)})}{f^{\Puc}(\mep^{\row,(\ell)})}\right)+\mu_{j}}}{\sigma_{j}} \biggr) } - \Q\left(\frac{\mu_{j}}{\sigma_{j}} \right) \biggr) } \biggr) \cdot \bar{\mathbbm{U}} \left( \ln \left(\frac{f^{\Que}(\mep^{\row,(\ell)})}{f^{\Puc}(\mep^{\row,(\ell)})}\right) \right)} \\ \nonumber & + \scalemath{0.8}{f^{\Quc}(\mep^{\row,(\ell)})\biggl( {\sum\limits_{j = 0}^{M/2 - 1} {w_j} \biggl( {\Q\biggl( \frac{{ \ln \left( \frac{f^{\Quc}(\mep^{\row,(\ell)})}{f^{\Pue}(\mep^{\row,(\ell)})}\right)+\mu_{j}}}{\sigma_{j}} \biggr) } - \Q\left(\frac{\mu_{j}}{\sigma_{j}} \right) \biggr) } \biggr) \cdot \bar{\mathbbm{U}} \left( \ln \left(\frac{f^{\Quc}(\mep^{\row,(\ell)})}{f^{\Pue}(\mep^{\row,(\ell)})}\right) \right)} \\  & + \scalemath{0.8}{f^{\Quep}(\mep^{\row,(\ell)})\biggl( {\sum\limits_{j = 0}^{M/2 - 1} {w_j} \biggl( {\Q\biggl( \frac{{ \ln \left( \frac{f^{\Quep}(\mep^{\row,(\ell)})}{f^{\Puep}(\mep^{\row,(\ell)})}\right)+\mu_{j}}}{\sigma_{j}} \biggr) } - \Q\left(\frac{\mu_{j}}{\sigma_{j}} \right) \biggr) } \biggr) \cdot \bar{\mathbbm{U}} \left( \ln \left(\frac{f^{\Quep}(\mep^{\row,(\ell)})}{f^{\Puep}(\mep^{\row,(\ell)})}\right) \right)}
	\end{align}
	\hrulefill
\end{figure*} 



\subsection{Density Evolution Analysis for BICM with $M^{2}$-QAM}\label{QAM_BICM}

\GL{BICM \cite{Zehavi1992,Caire1998} (see Fig.~\ref{BICM}) has become a \emph{de-facto} standard in optical communications \cite{georg_tcom,Buchali_2016} due to the inherent flexibility and implementation simplicity.} 
One can see the $M^{2}$-QAM modulation as a Cartesian product of two amplitude shift keying (ASK) modulations, i.e, the real and imaginary parts of each $M^{2}$-QAM symbol \GL{are} chosen form an $M$-ASK constellation. \GL{More specifically}, for $M=2^{m}$, the constellation points for both real and imaginary parts of the $M^{2}$-QAM symbol are chosen from $\mathcal{X}\triangleq\{(-2^m+1)\cdot \Delta ,...,-\Delta,\Delta,...,(2^m-1)\cdot \Delta\}$ where $\Delta=\sqrt{\frac{3}{2(M^2-1)}}$ is a scaling factor which normalizes the resulting $M^{2}$-QAM constellation energy to $1$. In the following, we only consider transmission of the real part of $M^{2}$-QAM, as the real and imaginary parts can be treated independently for the AWGN channel.  
\SH{We consider binary reflected Gray coding (BRGC) mapping \cite{BRGC}.}
%where the corresponding bit labels for $8$-ASK are shown in Table~\ref{labeling}. 
The output of the AWGN channel at time instant $i$ corresponding to ASK symbol $x_{i}$ is given by 
\begin{align}\label{channel_inst1}
y_{i}=x_{i}+n_{i},
\end{align}  
where $x_{i}\in \mathcal{X}$ and $n_{i}\sim \mathcal{N}(0,\sigma^2)$. 
The LLR of the $k$-th bit level of $y_{i}$ is given as
\begin{align} \label{LLR}
l_i^k = \ln \left(\frac{\sum\limits_{a \in \mathcal{S}_k^0} {{e^{ - \frac{{({y_i} - a{)^2}}}{{2\sigma^2}}}}} }{{\sum\limits_{a \in \mathcal{S}_k^1} {{e^{ - \frac{{({y_i} - a{)^2}}}{{2\sigma^2}}}  }} }}\right),\;k=1,\cdots,m
\end{align}  
where $\mathcal{S}_k^0 \subset \mathcal{X}$ and $\mathcal{S}_k^1 \subset \mathcal{X}$ are sets of size \SH{$2^{m}$} ASK symbols with $0$ and $1$ as the $k$th bit of the corresponding BRGC label, respectively. To alleviate the complexity of the LLR computation the well-known max-log approximation is usually used, yielding \cite[Eq.~6]{Viterbi1998}
\begin{align} \label{LLRMaxlog}
l_i^k & \approx \frac{1}{{2{\sigma ^2}}}\left[ {\mathop {\max }\limits_{a \in {\cal S}_l^0} \left\{ {{{-\left( {{y_i} - a} \right)}^2}} \right\} - \mathop {\max }\limits_{a \in {\cal S}_l^1} \left\{ {{{-\left( {{y_i} - a} \right)}^2}} \right\}} \right] \nonumber \\ &= \frac{1}{{2{\sigma ^2}}}\left[ {\mathop {\min }\limits_{a \in {\cal S}_l^1} \left\{ {{{\left( {{y_i} - a} \right)}^2}} \right\} - \mathop {\min }\limits_{a \in {\cal S}_l^0} \left\{ {{{\left( {{y_i} - a} \right)}^2}} \right\}} \right].
\end{align}         

%\GLC{I re-casted the paragraph \textrm{Channel mixing is a technique which simplifies the analysis of BICM \cite{Xiechanmix}. In BICM with channel mixing, the decoder does not treat the LLRs yielded  by bit-level de-mapping and de-intereleaving differently, i.e., the decoder mixes different bit channels as one mixed channel (see \cite[Sec.~IV--A]{Xiechanmix}). Therefore, the bit channels between encoder and decoder can be considered to have the same LLR distribution (see \cite[Sec.~V--B]{Ivanov2016}). We highlight that as we consider BICM with channel mixing, we do not rely on a multi-edge type allocation for bit channels in the DE analysis of GLDPC ensemble.} I believe this is imprecise: The decoder simply ignores the correlation among the bit levels, is not about treating them differently. It is also true only for analysis purposes that the LLRs distribution is the mixture. Of course, the real decoder gets, at a given variable node, only messages according to the corresponing bit level distribution. I've tried to clarify this in the following.}

%\GL{In the following, we assume that the allocation of bit levels to the nodes in the graph is performed randomly. This implies that, for the purpose of DE analysis, the distribution of the LLRs at the decoder input is given \SH{by} a mixture of the LLR distribution of each bit level. We refer to this approach as \emph{BICM channel mixing}.} \SH{The distribution of LLRs in a BICM  \GL{scheme} employing the max-log approximation and channel mixing can be approximated as a Gaussian mixture given as \cite[Eq.~(28)]{Alvarado20091}\footnote{Note that \eqref{LLRMaxlog_approx} is the modified version of \cite[Eq.~28]{Alvarado20091} such that it complies with the mapping considered in this paper.}}, i.e., 
\begin{align} \label{LLRMaxlog_approx}
p\left( {l|b} \right) = \sum\limits_{j = 0}^{\frac{M}{2} - 1} {{w_j}} {\G}({l;\left( { - 1} \right)^b}{\mu _j},\sigma _j^2),
\end{align} 
where $l$ is the LLR corresponding to transmitted bit $b \in \{0,1\}$, ${w_j} = \frac{{2\left( {{2^{m - \left\lceil {{{\log }_2}\left( {j + 1} \right)} \right\rceil }} - 1} \right)}}{{m \cdot M}}$, $\mu_j=\frac{2\Delta^2(j+1)^2}{\sigma^2}$, and $\sigma^2_j=\frac{4\Delta^2(j+1)^2}{\sigma^2}$, respectively. \GL{To proceed with the DE analysis, we assume that the all-zero codeword is transmitted. This leads to the need to symmetrizing the LLR distribution. To achieve this, we resort to the use of channel adapters \cite{Hou2003}.} 
%\GLC{I discussed with Alex (Chalmers), and he also agrees that the technique is known, and especially in T-COM it may fire back to repeat too many known concepts - this may apply also to the discussion about how GLDPC and PCs are related...}
\SH{Denote by $\bar l$ the LLR of the symmetrized BICM channel. The distribution of $\bar l$ is given as \cite[Eq.~(19)]{Ivanov2016}}
\begin{align} \label{LLRMaxlog_approxllr}
p\left( \bar{l} | b\right) =\frac{p\left( {l|b}\right)+p\left( {-l|1-b}\right)}{2}.
\end{align}  

\medskip

\begin{proposition}
\GL{For a BICM scheme} with BRGC \SH{mapping}, the message error probability at the row-type and column-type CN at the $\ell$th iteration is given by \eqref{QAMxrl} and \eqref{QAMxcl}, respectively, with 
\[
\mep^{\col,(0)}=\sum\limits_{j = 0}^{\frac{M}{2} - 1} {{w_j}}\cdot\Q\left(\frac{\mu_j}{\sigma_j}\right).
\]
\end{proposition}
\GL{The proof is given in Appendix~\ref{AB}.}	




%\section{Implementation \GL{C}omplexity}\label{comp_RA_BMP}

%\GLC{I still have strong concerns with the correctness of the analysis below - see also my comments about the serial/parallel access to the use of the LUT: what we do now, assumes that we update each bit LLR serially}

%A detailed complexity analysis of iBDD-CR requires \GL{taking into account details on a possible hardware implementation of the algorithm}, which is beyond the scope of this paper. \GL{To provide a preliminary complexity assessment, we follow the approach of \cite{Hag18,sheikhTCOM19} where} the complexity of \GL{the }AD and \GL{the} iBDD-SR algorithms are compared with the \GL{one of} iBDD \GL{\cite{staircase_frank}, only} at a high level. In a nutshell, the main structural difference between iBDD-CR and iBDD is that iBDD-CR computes the approximation of LLRs for the BDD outbound messages and exchange the corresponding binary message \GL{hard value} in row/column decoding, whilst iBDD \GL{directly} exchange BDD outbound messages.

%\SH{To implement iBDD-CR, the channel LLRs should be combined with BDD outbound messages (see Fig.~\ref{SysPCCST}). In what follows, we show that this combining imposes an extra required memory for iBDD-CR compared to iBDD.} 
%Therefore, in the following, we evaluate the additional memory requirement of iBDD-CR compared to iBDD. 
%We highlight that both iBDD and iBDD-CR requires at least a memory size of $n^2$ bits to store and update codeword bits in the row/column decoding. 
%\GLC{I believe that we should first discuss that the implication is two-fold: First, there is a (slight) increase in the algorithm complexity at the CNs, and then that the memory required to implement XXX has to be considered as well... Can you please restate this in that form?}

%Consider a PC with BCH component code $(n,k,t)$, decoded over $\ell_\text{max}$ iterations. In the row decoding, \eqref{eq:BDDchrel_VN} and \eqref{LLRoutv1} show that the decision on the code bit $c_{i,j}$ at iteration $\ell$ can be one of the following two cases: i) $\psi_{i,j}^{\mathsf{r},(\ell)}=\BB(l_{i,j})$  if $|\mut_{i,j}^{\row,(\ell )}|<|l_{i,j}|$  and ii) $\psi_{i,j}^{\mathsf{r},(\ell)}=\BB({\mut}_{i,j}^{\mathsf r, (\ell)})$ if $|\mut_{i,j}^{\row,(\ell )}|>|l_{i,j}|$. \SH{Therefore, $\psi_{i,j}^{\mathsf{r},(\ell)}$ can be computed if the decoder store both $l_{i,j}$ and ${\mut}_{i,j}^{\mathsf r, (\ell)}$. We assume $n_{\mathsf{q}_{1}}$ and $n_{\mathsf{q}_{2}}$ bits are used for representing the value of $l_{i,j}\in \mathbb{R}$ and ${\mut}_{i,j}^{\mathsf r, (\ell)}\in \mathbb{R}$, respectively. Hence, the additional memory required for iBDD-CR  compared to iBDD is $(n^2n_{\mathsf{q}_{1}}+3\ell_\text{max}n_{\mathsf{q}_{2}})$ bits. Note that the factor $3$ is due to the fact that in each iteration, three different values for $|{\mut}_{i,j}^{\mathsf r, (\ell)}|$ are possible (see Table~\ref{Tabcomp}), and the factor $n^2$ is due to storing channel LLRs for each code bit. Similarly, the required memory for column decoding is also $(n^2n_{\mathsf{q}_{1}}+3\ell_\text{max}n_{\mathsf{q}_{2}})$ bits, as in principle the values of ${\mut}_{i,j}^{\mathsf r, (\ell)}$  differs from ${\mut}_{i,j}^{\mathsf c, (\ell)}$ (c.f. Table~\ref{Tabcomp} and Table~\ref{Tabcompc}). As the channel LLRs are not updated in the row and column iBDD-CR decoding, the overall additional required memory of iBDD-CR with respect to iBDD is $(n^2n_{\mathsf{q}_{1}}+6\ell_\text{max}n_{\mathsf{q}_{2}})$ bits.} We highlight that both iBDD and iBDD-CR requires at least a memory size of $n^2$ bits to store and update codeword bits in the row/column decoding.
%\begin{exmp}\label{ex3}
%	Consider a PC with ($255$,$231$,$3$) BCH component code decoded with $10$ iterations. Also assume $n_{\mathsf{q}_{1}}=n_{\mathsf{q}_{2}}=10$. In this example, the additional required memory of iBDD-CR over iBDD is $650$k bits. Furthermore, both iBDD and iBDD-SR require $65$k bits to store the codeword bits.       
%\end{exmp}
%\SH{The extra required memory of iBDD-CR is dominated by storing the channel LLRs, as $n^2n_{\mathsf{q}_{1}} \gg  6\ell_\text{max}n_{\mathsf{q}_{2}}$. Channel LLRs can be quantized to reduce the memory required for implementing the iBDD-CR.}
% \GLC{I don't get the meaning of the previous sentence: Can you re-cast it? It sounds clear to me that all LLRs within the decoder are quantized, including the channel LLRs...} Again we assume that each quantized LLR value is represented using $n_{\mathsf{q}_{1}}$ bits. We further assume that the LLRs are quantized using $2^{n_{\mathsf{q}_{3}}}$ quantization levels, i.e., ${n_{\mathsf{q}_{3}}}$ bits. \GLC{I'm very confused: You talk of LLRs, that are quantized with different quantization levels: which LLRs are you referring to? It would be much better to first explain, at the beginning of the section, in a very brief way what are your assumptions: I.e., we assume that the channel LLRs are quantized with XXX bits, and that the LLRs computed within the CN decoder are quantized with ZZZ bits. Now, all is very confused...} This means that there are only $2^{n_{\mathsf{q}_{3}}}$ values that should be stored. Therefore, the overall memory requirement of iBDD-CR with quantized LLRs is $(2^{n_{\mathsf{q}_{3}}}n_{\mathsf{q}_{1}}+6\ell_\text{max}n_{\mathsf{q}_{2}})$. As can be seen the additional memory requirement of iBDD-CR with quantized LLRs does not scale with the component block length $n$. \GLC{Why not? It SHOULD be: for each component codeword bit, I introduce an additional LLR within the CN decoder, so the memory has to increase with $n$...}
%\GLC{An additional point: What assumptions do we make about the look-up table for combining? How many to we assume to have within a CN? If we assume to have only one, then the reviewers may complain (correctly) that the calculation of the LLRs within the CN has to be performed serially, to avoid collisions in the access to the table. This would make the latency of the CN operation to explode, killing the throughput.}
%As seen in Example.~\ref{ex3}, comparing the required $n^2$ bits for baseline iBDD to the extra memory requirement of iBDD-CR with quantized LLRs, one can infer that the additional memory is very negligible, i.e., $2^{n_{\mathsf{q}_{3}}}n_{\mathsf{q}_{1}}+6\ell_\text{max}n_{\mathsf{q}_{2}} \ll n^2$.\footnote{We should remark that there is an additional cost in iBDD-CR compared to iBDD which is implicitly neglected. In particular, iBDD-CR requires to compare the computed LLRs with the boundary of the quantized values and reading the corresponding quantized LLRs from the  memory, which is assumed to be negligible.} \GLC{This assumption may be wrong...}

%\begin{figure*}[t]
%	\vspace*{-20pt}
%	\hrulefill	
%	\begin{align}\label{qi_closed}	
%	{\mathsf{q}_i} = 1+ \frac{{\sum\limits_{b = 0}^1 {\sum\limits_{j = 1}^{\frac{M}{2} - 1} {\frac{{{\sigma _j}{w_j}}}{{\sqrt {2\pi } }}\left( {{e^{ - \frac{{{{\left( {{{\left( { - 1} \right)}^b}{\mu _j} - {\mathsf{d}_{i - 1}}} \right)}^2}}}{{2\sigma _j^2}}}} - {e^{ - \frac{{{{\left( {{{\left( { - 1} \right)}^b}{\mu _j} - {\mathsf{d}_i}} \right)}^2}}}{{2\sigma _j^2}}}}} \right)} } }}{{\sum\limits_{b = 0}^1 {\sum\limits_{j = 1}^{\frac{M}{2} - 1} {{{\left( { - 1} \right)}^b}{\mu _j}{w_j}\left( {\Q\left( {\frac{{{\mathsf{d}_{i - 1}} - {{\left( { - 1} \right)}^b}{\mu _j}}}{{{\sigma _j}}}} \right) - \Q\left( {\frac{{{\mathsf{d}_i} - {{\left( { - 1} \right)}^b}{\mu _j}}}{{{\sigma _j}}}} \right)} \right)} } }}	
%	\end{align}
%	\hrulefill
%\end{figure*}
 
%\GLC{Does it always converge to the MMSE solution? \SH{For a given initial value (quantization levels), it converges in a MMSE sense. But as you see, you need to define a good initial point by trials. Good starting points can be selected by looking at the distribution of LLRs }} \GLC{Then, I propose to weaken our statement as I did above...} 
%We highlight that this \GL{may not be} the best quantization scheme in the sense of minimizing BER of the coded system with PCs. 
%\GLC{I would avoid explaining how we use L-M: This is very well know, and, again, it may really strike back in a T-COM...} In the following we summarize the \GL{Lloyd–Max} algorithm. 
%\begin{equation}\label{dclosed}
%\mathsf{d}_i=\frac{\mathsf{q}_{i}+\mathsf{q}_{i+1}}{2},
%\end{equation}
%Let us assume LLR quantization with $n_{\mathsf{q}}$ bits, where $\{\mathsf{q}_1,\cdots,\mathsf{q}_{2^{n_{\mathsf{q}}}}\}$ are the quantization levels and $\{\mathsf{d}_0,\cdots,\mathsf{d}_{2^{n_{\mathsf{q}}}}\}$ are the quantization boundaries, where $\mathsf{d}_0=-\infty$ and $\mathsf{d}_{2^{n_{\mathsf{q}}}}=\infty$. \SH{In \GL{Lloyd–Max} algorithm, given initial quantization values, the quantization boundaries are computed as $\mathsf{d}_i=\frac{\mathsf{q}_{i}+\mathsf{q}_{i+1}}{2}$, for $i \in \{1,\cdots,2^{n_{\mathsf{q}}}-1\}$.}
%Furthermore, given quantization boundaries, the quantization values are given as
%\begin{equation}\label{qi_quant}
%{\mathsf{q}_i} = \frac{{\int_{{\mathsf{d}_{i - 1}}}^{{\mathsf{d}_i}} {\lambda p\left( {{l} = \lambda} \right)d\lambda } }}{{\int_{{\mathsf{d}_{i - 1}}}^{{\mathsf{d}_i}} {p\left( {{l} = \lambda} \right)d\lambda } }},
%\end{equation} 
%where $i \in \{1,\cdots,2^{n_{\mathsf{q}}}\}$ and $l$ is the LLR RV.
%\GLC{I strongly advise not to make a lemma out of this...}
%\begin{lemma}
%\SH{For the BICM channel with the conditional LLR distribution \eqref{LLRMaxlog_approx}, the $i$th quantization value $\mathsf{q}_i$ according to \GLC{Lloyd–Max} algorithm is given by \eqref{qi_closed}. The derivation of \eqref{qi_closed} is given in Appendix~\ref{AC}.} \GLC{I recommend removing (20) and its derivation: LM is very well know, as soon as you say that this is what you use, people know how to do it... We should really avoid giving too many trivial details, and rather focus on the core of the contribution.}
%\end{lemma}
%\begin{IEEEproof}
%	The proof 
%\end{IEEEproof}
%\GLC{We may skip the following example...}

\section{Numerical  Results}\label{Simres}

In this section, in order to have a direct comparison with iBDD-SR, we consider PCs with the same component codes as those considered in \cite{sheikhTCOM19}, i.e., PCs with ($255$,$231$,$3$) and ($511$,$484$,$3$) BCH component codes.\footnote{Component codes with long block length and 
 $t=3$ are interesting for fiber-optic communications as their error floor 
 %\AGc{What do we mean by dominant error floor? It's a strange concept.} 
 is below $10^{-15}$ and the decoder can be efficiently implemented using LUTs \cite[Appendix~I]{staircase_frank}.} The code rates of the PCs are  $R=0.820$ and $0.897$, respectively. \SH{Also, we  consider $12$ decoding iterations.} 

\SH{\begin{remark}\label{remarkapp}
In the case of  errors with high reliability, i.e., LLRs with high magnitude and  wrong sign, the decoding rule in \eqref{eq:BDDchrel_VN}--\eqref{eq:BDDchrel_VN_scale} will be unable to correct such errors, as it is likely that $\BB(\tilde{l}_{i,j}^{\mathsf r, (\ell)})=\BB({l}_{i,j})$. Therefore, similar to  iBDD-SR \cite[Sec.~VI]{sheikhTCOM19}, one should selectively apply iBDD-CR to to increase the chance of correcting such errors. In particular, unless otherwise specified, we consider iBDD-CR and iBDD-SR for some iterations and then we append a few conventional iBDD iterations. The \GL{additional} iBDD iterations increase the chance of correcting transmission errors with high reliability, as the iBDD decoding rule is independent of channel reliabilities. \GL{Specifically}, we consider a maximum of $10$ iBDD-CR (or iBDD-SR) iterations followed by $2$ conventional iBDD iterations. For the sake of fairness, other decoding algorithms are evaluated with $12$ decoding iterations.
\end{remark}}

%To have a fair comparison, we also consider $12$ iterations for conventional iBDD.  

%\begin{figure}[t] \centering 
%	\includegraphics[scale=0.58]{./paper_Figs/coefficientsv1.pdf}  
%	%\vspace{-2ex}
%	\caption{Evolution of scaling factors \GLC{BUT iBDD-CR DOES NOT HAVE SCALING FACTORS!!!???} for the GLDPC code ensemble with B\GL{a  ($255$,$231$,$3$) BCH} component code  over $20$ iterations \GLC{In the text you write half iterations} for both iBDD-CR and iBDD-SR \cite{sheikhTCOM19}.}  \vspace{-2ex}
%	\label{W_evolv} 
%\end{figure} 

\begin{figure}[t] \centering 
	\includegraphics[scale=0.82]{DE_comparisions_sim.pdf}  
	%\vspace{-2ex}
	\caption{Comparison between DE \GL{thresholds (computed for the relevant GLDPC ensembles)} and \GL{BER} performance of iBDD-CR and iBDD-SR algorithms \GL{applied to PCs}. The \GL{BCH} component code\GL{s} for (a) and (b) \GL{have parameters} ($255$,$231$,$3$) and ($511$,$484$,$3$), respectively.}  \vspace{-2ex}
	\label{DEsimt3} 
\end{figure} 


\begin{figure}[t] \centering 
	\includegraphics[scale=0.6]{Simulationbiawgn.pdf}  
	%\vspace{-2ex}
	\caption{Performance of iBDD, ideal iBDD, AD, iBDD-SR, and iBDD-CR for a PC with \GL{a ($255$,$231$,$3$) BCH} component code  and a staircase code with \GL{a ($511$,$484$,$3$) BCH} component code.}  \vspace{-2ex}
	\label{percompt3} 
\end{figure} 

\begin{figure*}[t] \centering 
	\includegraphics[scale=0.67]{QAM_830_allmethods_combinev1new.pdf}  
	%\vspace{-2ex}
	\caption{Performance of iBDD, ideal iBDD, iBDD-SR, and iBDD-CR with \GL{unquantized} LLRs for \GL{a ($255$,$231$,$3$) BCH} component code  in \GL{a BICM scheme} with $16$-QAM, $64$-QAM, and $256$-QAM modulation. %\GLC{I fear we have too many figures: I quote from the submission guidelines "Figures must be used sparingly and repetitive patterns or results omitted."}
	}  %\vspace{-2ex}
	\label{QAM_mod_all_schemes} 
\end{figure*} 



\begin{figure}[t] \centering 
	\includegraphics[scale=0.6]{16QAM_830_allmethods_tradeoff_RABMP_iBDD.pdf}  
	%\vspace{-2ex}
	\caption{Performance of iBDD-CR with different appended iBDD iterations for the BICM channel with $16$-QAM modulation and PC with BCH component code ($255$,$231$,$3$). %\GLC{In all figures, we have to replace RA-BMP with iBDD-CR!}
	}  \vspace{-2ex}
	\label{tradeoff_RA_BMP} 
\end{figure} 

%In order to have an insight about the iBDD-CR algorithm, in Fig.~\ref{W_evolv} we show the evolution of $\mut_{i,j}^{\row,(\ell )}$ and $\mut_{i,j}^{\col,(\ell )}$ yielded from DE for the GLDPC code ensemble with BCH component code ($255$,$231$,$3$), as a function of the number of \emph{half} iterations, where each iteration corresponds to one row or column decoding step. For the sake of comparison, we also show the evolution of $\mut_{i,j}^{\row,(\ell )}$ and $\mut_{i,j}^{\col,(\ell )}$ (see Remark~\ref{rr1}) resulting from DE analysis of iBDD-SR (see \cite[Sec.IV]{Sheikhecoc2019}) for the same GLDPC ensemble. \GLC{This part is a bit unclear, can we re-state it in a clearer manner? Also, very important: How do you plot $\mut_{i,j}^{\row,(\ell )}$ for iBDD-CR? In principle, at each iteration we have a distribution on it, since it is not simply a scaling factor!} The values of $\mut_{i,j}^{\row,(\ell )}$ and $\mut_{i,j}^{\col,(\ell )}$ for both iBDD-CR and iBDD-SR in Fig.~\ref{W_evolv} are obtained at the $E_\mathrm{b}/N_0$ where the DE output of both iBDD-CR and iBDD-SR provide a BER of $10^{-15}$. One can see that the magnitude of $\mut_{i,j}^{\row/\col,(\ell )}$ is increased over decoding iterations for iBDD-CR. This means that the output of the BDD stage in the iBDD-CR become more reliable, as the decoding proceeds. Furthermore, in iBDD-CR $\mut_{i,j}^{\row/\col,(\ell )}\ne 0$ when $\bar \mu_{i,j}^{\row/\col,(\ell )}= 0$, as opposed to iBDD-SR. Moreover, when $\bar \mu_{i,j}^{\row/\col,(\ell )}\ne 0$ depending on the values of  $\hat{l}_{i,j}$ and $\bar \mu_{i,j}^{\row/\col,(\ell )}$, the magnitude of $\mut_{i,j}^{\row/\col,(\ell )}$ varies differently for a given decoding iteration, while for iBDD-SR the magnitude of $\mut_{i,j}^{\row/\col,(\ell )}$ is constant for a given iteration (c.f. dashed blue and green curves with solid blue and green curves in Fig.~\ref{W_evolv}). Fig.~\ref{W_evolv} clearly shows that the heuristic model for updating the LLRs in iBDD-SR (see Remark~\ref{rr1}) is indeed suboptimal, but follows the updated rule given by iBDD-CR which we developed in this paper. \GLC{The conclusion is unclear, what do you mean? That iBDD-SR performs worse than iBDD-CR, but not that worse?} 

%The values of $\mut _{i,j}^{\row,(\ell )}$ and $\mut _{i,j}^{\col,(\ell )}$ in simulations is found via DE analysis for the GLDPC ensemble with the same component codes. 
\SH{We highlight that for the PC performance simulation, we obtain the values of $\mut _{i,j}^{\row,(\ell )}$ and $\mut _{i,j}^{\col,(\ell )}$ from the DE for a single $E_\mathrm{b}/N_0$, corresponding to an SNR point in the waterfall region. We found that changing the operating SNR point in the waterfall region (which in principle results in different values for $\mut _{i,j}^{\row,(\ell )}$ and $\mut _{i,j}^{\col,(\ell )}$) yields a minor performance difference in the simulation results.} Furthermore, in the following simulation results we \GL{restrict to} $\mut _{i,j}^{\row,(\ell )}=\mut _{i,j}^{\col,(\ell )}$, i.e., for a given decoding iteration the values of $\mut _{i,j}^{\row,(\ell )}$ in Table~\ref{Tabcomp} are used for both row and column decoding, as we found that \AG{this induces a negligible performance loss}.

In Fig.~\ref{DEsimt3}, the performance of PCs with iBDD-CR and iBDD-SR over the bi-AWGN channel is shown and compared with the \GL{DE thresholds of the corresponding GLDPC code ensembles}. As \GL{it} can be seen, the performance improvement of iBDD-CR over iBDD-SR is well-predicted by the DE analysis, \GL{confirming that} DE can be used for as a tool for \GL{the} optimization \GL{of the decoding algorithm}. The gap between \GL{the DE thresholds} and \GL{the BER curves} is due to two reasons. \GL{First and foremost,} DE predicts the performance of the GLDPC \GL{code} ensemble with infinite long block length. By increasing the PC component \GL{code} block length from $255$ to $511$, \GL{the} gap is reduced, as \GL{it} can be seen \GL{in} Fig.~\ref{DEsimt3}. \GL{Second,} \GL{DE analysis relies on an} extrinsic message passing \GL{variation of the algorithms}, whereas \GL{the} simulation results \GL{employ} standard (intrinsic) message passing.    

\newcommand{\tablehighlight}{}
\begin{table}[t]	
	\caption{Comparison of iBDD-CR and iBDD for PCs with ($255$,$231$,$3$) and ($511$,$484$,$3$) BCH component codes, with code rates of $0.820$ and $0.897$, respectively. The $E_\mathrm{b}/N_0$ for iBDD and iBDD-CR are measured at $\text{BER} = 10^{-6}$ from the simulations. The corresponding Shannon limits are also shown. 
%The values for component codes ($511$,$484$,$3$) are provided within parenthesis.
%	\GLC{Are these gaps, or are these Shannon limits? Moreover, I would put the two codes on two rows, not in (). Otherwsiem since the paper explodes with Figures and Tables, what about just mentioning the numerical values in the text?}
}
	%\vspace{-2ex}
	\centering
	\renewcommand{\arraystretch}{1.2}
	\scalebox{0.96}{	
		\begin{tabular}{ccccc}
			\toprule
			\makecell{component \\ code} &
			\makecell{decoding \\ algorithm} &
			\makecell{\tablehighlight{$E_\mathrm{b}/N_0$} 
				\tablehighlight{[dB]}} &
			\makecell{Shannon limit [dB]}&\\
			\midrule
			($255$,$231$,$3$) & iBDD & $4.62$ & $3.54$ (HD) &   \\
			($255$,$231$,$3$) & iBDD-CR & $4.29$ & $2.23$ (SD)  \\
			($511$,$484$,$3$) & iBDD & $5.18$ &  $4.36$ (HD) &   \\
            ($511$,$484$,$3$) & iBDD-CR & $4.89$ & $3.15$ (SD)  \\			
			\bottomrule
		\end{tabular}
	}
	%\vspace{0.2cm}
	\label{Tabcomp1}
	\vspace{-4ex}
\end{table}


In Fig.~\ref{percompt3}, we show the performance of iBDD-CR, iBDD, ideal iBDD%\footnote{\SH{Genie-aided decoder which disregards the misscorrections. Ideal iBDD serves as a bound for the performance of (conventional) iBDD.}}
, \AG{anchor decoding (AD) \cite{Hag18}}, and iBDD-SR for transmission \SH{over} the bi-AWGN channel. 
%For a fair comparison, we consider iBDD, ideal iBDD, and AD with $12$ decoding iterations. 
One can see that iBDD-CR outperforms all other decoders. In particular, iBDD-CR performs even better than ideal iBDD. The performance gain of iBDD-CR over iBDD is $0.36$ dB and $0.29$ dB for PCs with component codes ($255$,$231$,$3$) and ($511$,$484$,$3$), respectively. \SH{In Table~\ref{Tabcomp1}, we show the required $E_\mathrm{b}/N_0$ for iBDD-CR and iBDD to achieve a BER of $10^{-6}$. We also show the corresponding hard-decision (HD) and soft-decision (SD) Shannon limits. Note that iBDD should be compared with the HD Shannon limit, while iBDD-CR should be compared with the SD counterpart, as the algorithm exploits the channel  LLRs.} The gap between the performance of iBDD-CR and the SD Shannon limit is \AG{mainly due to the fact that the structure of \SH{iBDD-CR} structure is intentionally kept very similar to that of iBDD}, \SH{in order to constrain the decoder complexity and \AG{the internal decoder data flow}.} By allowing the exchange of soft information between component decoders this gap can be closed further at the cost of significantly \AG{higher data flow and  complexity} (see \cite{She18b,She19,YibitTCOM} for more details). 

In Fig.~\ref{QAM_mod_all_schemes}, we show the performance of \AG{a  BICM system  using a PC with component  code ($255$,$231$,$3$) for  iBDD, ideal iBDD, iBDD-SR, and iBDD-CR} and $16$-QAM, $64$-QAM, and $256$-QAM. \SH{We also show the DE threshold for the corresponding GLDPC ensemble.} iBDD-CR outperforms others decoders and the \SH{DE analysis predicts} the performance of iBDD-CR accurately. Furthermore, the gain of iBDD-CR over iBDD improves by increasing the modulation order\AG{; the gain \SH{is} up to} $0.51$ dB for $256$-QAM. 
 
%Recall that the number of appended iBDD iterations was fixed to $2$ iterations in the previous figures

\SH{In Fig.~\ref{tradeoff_RA_BMP}, we examine the effect of appended iBDD iterations on the performance of iBDD-CR. We split the total of $12$ decoding iterations between iBDD-CR and iBDD. As it can be seen, increasing the number of iBDD-CR iterations yields a performance improvement, where such improvement saturates at $10$ iBDD-CR and $2$ iBDD iterations. This motivates the choice of appending $2$ iBDD iterations for performance evaluation of iBDD-CR (see Remark~\ref{remarkapp}).} %Interestingly, half of the gap between the performance of iBDD and iBDD-CR with $2$ appended iBDD iterations can be closed with only $2$ iBDD-CR decoding iterations appended with $10$ iBDD iterations.} 
 
\begin{figure}[t] \centering 
	\includegraphics[scale=0.68]{PDF_quantized_none_quantized.pdf}  
	%\vspace{-2ex}
	\caption{Comparison between the PDF of the LLRs and the corresponding quantized PMF using $3$ bits for $16$-QAM modulation and $E_\mathrm{s}/N_0=12.92$ dB. The dashed line shows the boundaries of the optimized nonuniform quantization based on the Lloyd-Max algorithm.}  \vspace{-4ex}
	\label{quant_comp} 
\end{figure} 


\begin{figure}[t] \centering 
	\includegraphics[scale=0.6]{16QAM_830_allmethods_maxlog_quantization.pdf}  
	%\vspace{-2ex}
	\caption{Performance of iBDD-CR with exact, max-log approximation, and quantized \SH{channel} LLRs for the BICM channel with $16$-QAM modulation and PC with BCH component code ($255$,$231$,$3$).}  %\vspace{-2ex}
	\label{Max_loyd_alg} 
\end{figure}  

 
\SH{To evaluate the sensitivity of iBDD-CR to channel LLR quantization,
%Finding the best quantization scheme in the sense of minimizing the BER of iBDD-CR is in general not a trivial task. 
we resort to a classical quantization scheme called \GL{Lloyd–Max} algorithm \cite{Max1960,Lloyd1980}, which \GL{aims at optimizing} the quantization levels in the sense of minimizing the mean squared error between LLRs and the corresponding quantized values.\footnote{We highlight that in this paper our approach is to just show the feasibility of iBDD-CR implementation with limited \SH{channel LLR quantization levels}, using a known quantization technique. In general, one can exploit the properties of the quantized channel in decoding rule to reduce the sensitivity to quantization errors. This analysis is beyond the scope of this paper.}
%Let us assume LLR quantization with $n_{\mathsf{q}}$ bits, where $\{\mathsf{q}_1,\cdots,\mathsf{q}_{2^{n_{\mathsf{q}}}}\}$ are the quantization levels and $\{\mathsf{d}_0,\cdots,\mathsf{d}_{2^{n_{\mathsf{q}}}}\}$ are the quantization boundaries, where $\mathsf{d}_0=-\infty$ and $\mathsf{d}_{2^{n_{\mathsf{q}}}}=\infty$. 	
As an example, Fig.~\ref{quant_comp} shows the distribution of LLRs for $16$-QAM at $E_\mathrm{s}/N_0=12.92$ dB,\footnote{We highlight that $E_\mathrm{s}/N_0=12.92$ dB corresponds to $E_\mathrm{b}/N_0=7.76$ dB which is a point selected in the waterfall region of iBDD-CR (see Fig.~\ref{tradeoff_RA_BMP}).} the optimized quantization values, and the corresponding boundaries using $3$ bits based on the \GL{Lloyd–Max} algorithm. 
%For the sake of reproducibility, we remark that the Lloyd-Max is initialized with $\{\mathsf{q}_1,\cdots,\mathsf{q}_{8}\}=\{-70,-30,-6,-0.5,0.5,6,30,70\}$in Fig.~\ref{quant_comp}. 
As it can be seen, Lloyd–Max yields nonuniform quantization values.} 

\SH{In Fig.~\ref{Max_loyd_alg}, we investigate the effect of \SH{max-log channel LLR approximation and channel LLR quantization based on the Lloyd–Max algorithm on the performance of iBDD-CR}.
In particular, we consider a BICM channel with $16$-QAM modulation, PC with BCH component code ($255$,$231$,$3$), and \AG{$3$-bit and $4$-bit quantization}. \SH{One can see that the performance of iBDD-CR with exact channel LLR computation \eqref{LLR} and max-log channel LLR approximation \eqref{LLRMaxlog} is almost identical}, hence, in a practical system max-log approximation can be employed to reduce the complexity of LLR computations. Furthermore, at a BER of $10^{-7}$, the performance loss of iBDD-CR with $3$ and $4$ bits nonuniform \SH{channel LLR} quantization based on %\GLC{Pay attention: it is Lloyd-Max - I fixed it in many places, but we need to make sure that is fine all over...}
the Lloyd-Max algorithm is small, i.e., $0.07$ dB and $0.045$ dB, respectively. This \AG{shows} that iBDD-CR has a low sensitivity to channel LLR quantization.} 
%\SH{Recalling the discussion in Sec.~\ref{comp_RA_BMP}, this shows that the additional required memory of  iBDD-CR compared to iBDD is limited, as the channel LLRs can be quantized using few bits.} 

%\SH{In \GL{Lloyd–Max} algorithm, given initial quantization values, the quantization boundaries are computed as $\mathsf{d}_i=\frac{\mathsf{q}_{i}+\mathsf{q}_{i+1}}{2}$, for $i \in \{1,\cdots,2^{n_{\mathsf{q}}}-1\}$.}
%Furthermore, given quantization boundaries, the quantization values are given as
%\begin{equation}\label{qi_quant}
%{\mathsf{q}_i} = \frac{{\int_{{\mathsf{d}_{i - 1}}}^{{\mathsf{d}_i}} {\lambda p\left( {{l} = \lambda} \right)d\lambda } }}{{\int_{{\mathsf{d}_{i - 1}}}^{{\mathsf{d}_i}} {p\left( {{l} = \lambda} \right)d\lambda } }},
%\end{equation} 
%where $i \in \{1,\cdots,2^{n_{\mathsf{q}}}\}$ and $l$ is the LLR RV. 
%\begin{exmp}\label{ex4}
%	Assume that $3$ bits are used for quantization of the BICM with conditional LLR distribution  given in \eqref{LLRMaxlog_approx}. Furthermore, assume that the quantization values  $\{\mathsf{q}_1,\cdots,\mathsf{q}_{8}\}$ are initialized as $\{-70,-30,-6,-0.5,0.5,6,30,70\}$. 
%\end{exmp}
   

\section{Conclusion}

\AG{We proposed an iterative soft-aided decoding algorithm for PCs, called iBDD-CR}. iBDD-CR exploits the LLRs in the BDD of the component codes and has the same decoder data flow as that of conventional iBDD. \AG{We performed a  DE analysis of  the GLDPC code ensemble containing PCs for both the bi-AWGN and BICM channels under extrinsic message passing}. \AG{From the analysis}, \GL{an accurate estimate of} the reliability of the BDD outbound messages, which is essential for implementing the iBDD-CR was derived. \AG{The proposed algorithm attains gains up to $0.51$ dB over conventional iBDD and outperforms ideal iBDD. We showed} that iBDD-CR has a low sensitivity to quantization errors on the channel LLRs and can be implemented using the low-complexity max-log LLR approximation. Overall, \AG{the low required internal data flow  and  low sensitivity to quantization errors of iBDD-CR} 
%\AGc{NOTE: WHAT DO WE MEAN BY SIMPLICITY HERE? I SUGGEST TO REMOVE IT}  
makes it an attractive solution for optical communication for $400$G and beyond, where an excellent performance along with stringent constraint on latency and power consumption are required. 

%\AGc{NOTE: I WOULD REMOVE THE FUTURE WORK. IT'S UNNECESSARY IN A JOURNAL PAPER. WE CAN KEEP THE PART OF THE MEMORY.} 
\SHm{
%We believe that further performance improvements over iBDD-CR require at least one of the following two cases: (i) operating beyond BDD, e.g., performing error-erasure decoding of the component code, and (ii) operating beyond binary message passing, e.g., exchanging soft information between component codes. Exploring these directions is left as future work. Furthermore, 
We remark that iBDD-CR requires some extra memory compared to iBDD to store the channel LLRs and the LUTs. The  exact required extra memory  depends on the level of parallelism in implementing row/column decoding and the relative required clock cycles of BDD and combining stages. Therefore, the complexity of iBDD-CR should be investigated via hardware implementation, which is left as future work.}
%\balance

\section*{Acknowledgment}

The authors would like to thank Dr. Alexios Balatsoukas-Stimming for fruitful discussions about the complexity of iBDD-CR.

\appendices

\section{Proof of \GL{Proposition} 1} \label{APP1}
%The decoder behavior is analyzed assuming all-zero codeword transmission. This assumption is motivated by the symmetry of AWGN channel, which is also considered in DE analysis of \cite{JianPfister2017} and \cite{sheikhTCOM19}. 
%Without loss of generality, we only consider the decoding of row-type CNs. For the column-type CNs, the same steps can be followed. 
Let us consider the decoding of row-type CNs. In particular, we first compute $\mut_{i,j}^{\row,(\ell )}$ and then we calculate $\mep^{\row,(\ell)}$. At the first iteration, we have $\mep^{\col,(0)}=p_\mathsf{ch}$, i.e., the input of the row-type CN is initialized with the channel error probability.  
\AG{Let $\hat l_{i,j}$ be}  the RV %\AGc{NOTE: HERE WE USE LOWER CASE LETTERS FOR RVS, BUT MAYBE IS OK TO NOT MESS-UP TOO MUCH NOTATION...} 
representing the sign of the LLR corresponding to code bit $c_{i,j}$ (see Fig.~\ref{SysPCCST}). To compute $\tilde \mu _{i,j}^{\row,(\ell )}$ (see \eqref{mutild_r}), we should compute the probabilities of $\scalemath{0.9}{p ({\bar \mu _{i,j}^{\row,(\ell )}|{\hat l_{i,j}},{c_{i,j}} = 0} )}$ and $\scalemath{0.9}{p( {\bar \mu _{i,j}^{\row,(\ell )}|{\hat l_{i,j}},{c_{i,j}} = 1})}$.  
Depending on the values of $\hat l_{i,j} \in \{\pm 1\}$ and $\mu _{i,j}^{\row,(\ell )} \in \{0, \pm 1\}$, six different terms for $\scalemath{0.9}{p( {\bar \mu _{i,j}^{\row,(\ell )}|{\hat l_{i,j}},{c_{i,j}} = 0} )}$ and $\scalemath{0.9}{p( {\bar \mu _{i,j}^{\row,(\ell )}|{\hat l_{i,j}},{c_{i,j}} = 1})}$ are possible ($12$ in total). The term $\scalemath{0.9}{p( {\bar \mu _{i,j}^{r,(\ell )}=-1|{\hat l_{i,j}=-1},{c_{i,j}} = 0} )}$ is the probability of error at the BDD output given that the channel output is also in error. One can check that this is exactly the definition of $\scalemath{0.9}{p( {\bar \mu _{i,j}^{\row,(\ell )}=1|{\hat l_{i,j}=1},{c_{i,j}} = 1} )}$
 as the values for $\bar \mu _{i,j}^{\row,(\ell )}$ and $\hat l_{i,j}$ are changed from $-1$ to $1$ and the value for $c_{i,j}$ is changed from $0$ to $1$. Therefore, $\scalemath{0.9}{p( {\bar \mu _{i,j}^{\row,(\ell )}=-1|{\hat l_{i,j}=-1},{c_{i,j}} = 0} )}=\scalemath{0.9}{p( {\bar \mu _{i,j}^{\row,(\ell )}=1|{\hat l_{i,j}=1},{c_{i,j}} = 1} )}$. Similarly, the following relations also hold: \\
$\scalemath{0.9}{p( {\bar \mu _{i,j}^{\row,(\ell )}=1|{\hat l_{i,j}=1},{c_{i,j}} = 0} )}=\scalemath{0.9}{ p( {\bar \mu _{i,j}^{\row,(\ell )}=-1|{\hat l_{i,j}=-1},{c_{i,j}} = 1} )}$, $\scalemath{0.9}{ p( {\bar \mu _{i,j}^{\row,(\ell )}=1|{\hat l_{i,j}=-1},{c_{i,j}} = 0} )}=\scalemath{0.9}{ p( {\bar \mu _{i,j}^{r,(\ell )}=-1|{\hat l_{i,j}=1},{c_{i,j}} = 1} )}$, $\scalemath{0.9}{p( {\bar \mu _{i,j}^{\row,(\ell )}=-1|{\hat l_{i,j}=1},{c_{i,j}} = 0} )}=\scalemath{0.9}{p( {\bar \mu _{i,j}^{r,(\ell )}=1|{\hat l_{i,j}=-1},{c_{i,j}} = 1} )}$, $\scalemath{0.9}{p( {\bar \mu _{i,j}^{\row,(\ell )}=0|{\hat l_{i,j}=-1},{c_{i,j}} = 0} )}=\scalemath{0.9}{p( {\bar \mu _{i,j}^{\row,(\ell )}=0|{\hat l_{i,j}=1},{c_{i,j}} = 1} )}$, and $\scalemath{0.85}{p( {\bar \mu _{i,j}^{\row,(\ell )}=0|{\hat l_{i,j}=1},{c_{i,j}} = 0} )}=\scalemath{0.85}{p( {\bar \mu _{i,j}^{\row,(\ell )}=0|{\hat l_{i,j}=-1},{c_{i,j}} = 1})}$.

We assume all-zero codeword transmission. Such assumption yields the computation of six different terms corresponding to $\scalemath{0.9}{p( {\bar \mu _{i,j}^{\row,(\ell )}|{\hat l_{i,j}},{c_{i,j}} = 0})}$.
%and also considered in the DE analysis of \cite{JianPfister2017} and \cite{sheikhTCOM19}.

\SHm{Let $\mep$ be  the input error probability to the row-type CNs for the BDD stage and  denote by $f^{\Pue}(\mep)$} the probability that a \emph{randomly selected bit} in the component code's codeword is decoded incorrectly when it was \emph{initially in error}. \SHm{The notion of \emph{randomly selected bit} is motivated due to the fact that we analyze the ensemble given in Fig.~\ref{PCgraph}(c), where the connection between VN and CN is randomly built.  Also, we use the notion of \emph{initially in error} as we analyze the extrinsic message passing where in each iteration the input corresponding to $c_{i,j}$ is substituted by channel input $r_{i,j}$ (see \eqref{eq:BDD_VN_extrinsic}).}
%NOTE: THIS CAN BE SAFELY REMOVED. WE ARE STATING PRETTY OBVIOUS THINGS HERE. E.G., IT SHOULD BE CLEAR TO THE READER WHAT INITIALLY IN ERROR MEANS} 
Furthermore, we denote by $\Pue\left( i \right)$ the probability that a \emph{randomly selected bit} in the component code's codeword is decoded incorrectly when it was \emph{initially} in error and there are $i$ errors in the other $n-1$ positions. \AG{$\Pue\left( i \right)$ was derived   in \cite[Eq. (5)]{sheikhTCOM19}  for iBDD-SR and the same expression holds for iBDD-CR. 
$f^{\Pue}(\mep)$ is then obtained based on the union of (independent) events for  $i \in \{0,\cdots,n-1\}$ random errors  as}
\begin{align}\label{pe} 
\scalemath{0.86}{ f^{\Pue}(\mep) \buildrel \Delta \over =  p(\bar \mu _{i,j}^{\row,(\ell )}=-1|{\hat l_{i,j}}=-1, c_{i,j}=0) =  \sum\limits_{i = 0}^{n - 1}   b^{n}_i(x) {\Pue\left( i \right)}}.
\end{align} 
where $b^{n}_i(x)\buildrel \Delta \over  = {{n-1}\choose{i}} {\mep^i}{\left( {1 - \mep} \right)^{n - i - 1}}$.

We denote by $f^{\Puc}(\mep)$ and $f^{\Puep}(\mep)$ the probability that a randomly selected bit in the component code's codeword is decoded correctly and erased,\footnote{Note the $\bar \mu _{i,j}^{\row,(\ell )}=0$ corresponds to the erasure output of BDD.} respectively, when it was initially in error. Furthermore, we denote by $\Puc\left( i \right)$ and $P^\epsilon(i)$ the probability that a randomly selected bit in the
component code's codeword is decoded correctly and erased, respectively, when it was initially in error and there are $i$  errors in the other $n-1$ positions. $\Puc\left( i \right)$ is \AG{given} in \cite[Eq. (9)]{sheikhTCOM19} and $P^\epsilon(i)=1-\Pue\left( i \right)-\Puc\left( i \right)$. Similar to the derivation of \eqref{pe},  $f^{\Puc}(\mep)$ and $f^{\Puep}(\mep)$ \AG{are obtained as}
\begin{align}\label{pc} 
\scalemath{0.86}{ f^{\Puc}(\mep) \buildrel \Delta \over  =  p(\bar \mu _{i,j}^{\row,(\ell )}=1|{\hat l_{i,j}}=-1, c_{i,j}=0)   =  \sum\limits_{i = 0}^{n - 1}  b^{n}_i(x) {\Puc\left( i \right)}},
\end{align} 
\begin{align}\label{p_ep} 
\scalemath{0.86}{f^{\Puep}(\mep) \buildrel \Delta \over  =  p(\bar \mu _{i,j}^{\row,(\ell )}=0|{\hat l_{i,j}}=-1,c_{i,j}=0)  =  \sum\limits_{i = 0}^{n - 1}  b^{n}_i(x)  {\Puep\left( i \right)}}.
\end{align}
%Furthermore, we denote by $f^{\Que}(\mep)$, $f^{\Quc}(\mep)$, and $f^{\Quep}(\mep)$ 
%the probability that \emph{randomly selected bit} in the component code's codeword is decoded incorrectly, correctly, and erased, respectively, when the bit was initially correct.
Furthermore, we denote by $f^{\Que}(\mep)$, $f^{\Quc}(\mep)$, and $f^{\Quep}(\mep)$ 
the probability that a randomly selected bit in the component code's codeword is decoded incorrectly, correctly, and erased, respectively, when the bit was initially correct. Also, let us denote by $\Que\left( i \right)$, $\Quc\left( i \right)$, and $Q^\epsilon(i)$ 
the probability that a randomly selected bit in the component code's codeword is decoded incorrectly, correctly, and erased, respectively, when the bit was initially correct and there are $i$ errors in the remaining $n-1$ positions. $\Quc\left( i \right)$ is \AG{given in \cite[Eq. (6)]{sheikhTCOM19}, $\Que\left( i \right)$   in \cite[Eq. (10)]{sheikhTCOM19},} and $\Quep\left( i \right)=1-\Que\left( i \right)-\Quc\left( i \right)$. Following the same steps \AG{as for the  derivation of \eqref{pe}, we get}
\begin{align}\label{qe} 
\scalemath{0.86}{f^{\Que}(\mep) \buildrel \Delta \over  =  p(\bar \mu _{i,j}^{\row,(\ell )}=-1|{\hat l_{i,j}}=1,c_{i,j}=0)   =  \sum\limits_{i = 0}^{n - 1}   b^{n}_i(x) {\Que\left( i \right)}},
\end{align}
\begin{align}\label{qc} 
\scalemath{0.86}{f^{\Quc}(\mep)  \buildrel \Delta \over  =  p(\bar \mu _{i,j}^{\row,(\ell )}=1|{\hat l_{i,j}}=1,c_{i,j}=0)   =  \sum\limits_{i = 0}^{n - 1}   b^{n}_i(x)  {\Quc\left( i \right)}},
\end{align}
\begin{align}\label{q_ep} 
\scalemath{0.86}{f^{\Quep}(\mep)  \buildrel \Delta \over  =  p(\bar \mu _{i,j}^{\row,(\ell )}=0|{\hat l_{i,j}}=1,c_{i,j}=0)  =  \sum\limits_{i = 0}^{n - 1}   b^{n}_i(x)  {\Quep\left( i \right)}}.
\end{align}
%For an arbitrary row/column BDD stage, we denote by $\Pue\left( i \right)$, $\Puc\left( i \right)$, and $P^\epsilon(i)$ the probability that a \emph{randomly selected bit} in the component code's codeword is decoded incorrectly, correctly, and erased, respectively, when it was initially in error and there are $i$  errors in the other $n-1$ positions. Similarly,  These probabilities are computed in the DE derivation for iBDD-SR algorithm \cite[Eqs. (5)--(11)]{sheikhTCOM19} for BCH component codes, assuming all-zero code word transmission. 
%
%Considering the input error probability ``$\mep$'' to the row-type CN for the BDD stage, we denote by $f^{\Pue}(\mep)$, $f^{\Puc}(\mep)$, and $f^{\Puep}(\mep)$ the probability that a \emph{randomly selected bit} in the component code's codeword is decoded incorrectly, correctly, and erased, respectively, when it was initially in error.  These probabilities correspond to the six terms described before. In particular, they can be computed based on the union of events for possible range of ``$i \in \{0,\cdots,n-1\}$'' random errors, each event computed based on binomial probability mass function with parameter $\mep$ and the values of $\Pue\left( i \right)$, $\Puc\left( i \right)$, $P^\epsilon(i)$, $\Que\left( i \right)$, $\Quc\left( i \right)$, and $Q^\epsilon(i)$.  Overall, we have
%\begin{align}\label{pc} 
%& f^{\Puc}(\mep) \buildrel \Delta \over  =  p(\bar \mu _{i,j}^{\row,(\ell )}=1|{\hat l_{i,j}}=-1, c_{i,j}=0)  \nonumber \\ & =  \sum\limits_{i = 0}^{n - 1}   {\Big( {\begin{array}{*{20}{c}}
%		{n - 1}\\
%		i
%		\end{array}} \Big)} {\mep^i}{\left( {1 - \mep} \right)^{n - i - 1}} \cdot  {\Puc\left( i \right)}.
%\end{align} 
%\begin{align}\label{p_ep} 
%& f^{\Puep}(\mep) \buildrel \Delta \over  =  p(\bar \mu _{i,j}^{\row,(\ell )}=0|{\hat l_{i,j}}=-1,c_{i,j}=0)  \nonumber \\ & =  \sum\limits_{i = 0}^{n - 1}   {\Big( {\begin{array}{*{20}{c}}
%		{n - 1}\\
%		i
%		\end{array}} \Big)} {\mep^i}{\left( {1 - \mep} \right)^{n - i - 1}} \cdot  {\Puep\left( i \right)}.
%\end{align}
Recalling \eqref{mutild_r} and the discussion given in the begining of this appendix, by employing \eqref{pe}--\eqref{q_ep} and $\mep^{\col,(\ell-1)}$ as the input error probability of row-type CN at iteration $\ell$, the values of $\tilde \mu _{i,j}^{\row,(\ell )}$ given in Table~\ref{Tabcomp} \AG{are obtained}. 

%Based on \eqref{pe}-\eqref{q_ep}, and the Table.~\ref{Tabcomp} summarizes $\lambda _{i,j}^{r,(\ell )}$ for different values of $\mu _{i,j}^{\row,(\ell )}$ and $L_{i,j}$. 
   
Now we focus on computing $\mep^{\row,(\ell)}$. \AG{Assuming transmission of the all-zero codeword, using \eqref{LLRoutv1}, and applying Bayes' rule}, the probability of error at the output of row decoder is obtained as 
\begin{align}\label{p_bays} 
\mep^{\row,(\ell)} &= p(\opt_{i,j}^{\mathsf r, (\ell)}<0) \nonumber \\ &= \sum\limits_{\mathclap{\substack{
			{\bar \mu _{i,j}^{\row,(\ell )}\in \{ 0, \pm 1\} }\\
			{{{\hat L}_{i,j}}\in \{  \pm 1\} }}}} {p(\opt_{i,j}^{\mathsf r, (\ell)} < 0|\bar \mu _{i,j}^{\row,(\ell )},{{\hat l}_{i,j}})} p(\bar \mu _{i,j}^{\row,(\ell )}|{\hat l_{i,j}})p({\hat l_{i,j}})
\end{align}
\AG{where  $p(\bar \mu _{i,j}^{\row,(\ell )}|{\hat l_{i,j}})$ is given in \eqref{pe}--\eqref{q_ep}}. In the following, we compute ${p(\opt_{i,j}^{\mathsf r, (\ell)} < 0|\bar \mu _{i,j}^{\row,(\ell )},{{\hat l}_{i,j}})}$. In particular, let us first calculate the term ${p(\opt_{i,j}^{\mathsf r, (\ell)}< 0|\bar \mu _{i,j}^{\row,(\ell )}=-1,{{\hat l}_{i,j}=-1})}$ in \eqref{p_bays}. This probability can be written as
\begin{align}\label{p_baysd1} 
& \nonumber {p\left(\opt_{i,j}^{\mathsf r, (\ell)} < 0\lvert\bar \mu _{i,j}^{\row,(\ell )}=-1,{{\hat l}_{i,j}=-1}\right)}\myeqa \\ \nonumber & {p\left( l_{i,j} < -\ln \left( \frac{f^{\Pue}(\mep^{\col,(\ell-1)})}{f^{\Quc}(\mep^{\col,(\ell-1)})}\right)\bigg\lvert{\bar\mu _{i,j}^{\row,(\ell )}=-1,{\hat l}_{i,j}}=-1\right)}\myeqb  \\ \nonumber & {p\left( l_{i,j} < -\ln \left( \frac{f^{\Pue}(\mep^{\col,(\ell-1)})}{f^{\Quc}(\mep^{\col,(\ell-1)})}\right)\bigg\lvert{{\hat l}_{i,j}}=-1\right)}\myeqc \\ \nonumber & \frac{p\left(l_{i,j}<-\ln \left( \frac{f^{\Pue}(\mep^{\col,(\ell-1)})}{f^{\Quc}(\mep^{\col,(\ell-1)})}\right),l_{i,j}<0\right)}{p(l_{i,j}<0)} \myeqd \\ \nonumber & \frac{p\left(l_{i,j}<\text{min}\left(-\ln \left( \frac{f^{\Pue}(\mep^{\col,(\ell-1)})}{f^{\Quc}(\mep^{\col,(\ell-1)})}\right),0\right)\right)}{p(l_{i,j}<0)} \myeqe \\ & \frac{{\Q\left( { \frac{\sigma}{2}\text{min}\left(\ln \left( \frac{f^{\Pue}(\mep^{\col,(\ell-1)})}{f^{\Quc}(\mep^{\col,(\ell-1)})}\right),0\right)+\frac{1}{\sigma}} \right) }}{p(l_{i,j}<0)}
\end{align}
where $(a)$ follows from \eqref{LLRoutv1} and  Table~\ref{Tabcomp}, $(b)$ follows from the Markov chain $l_{i,j} \to \hat l_{i,j} \to \bar \mu _{i,j}^{\row,(\ell )}$ (see Sec.~\ref{sec:DE_BIAWGN}), $(c)$ follows from the definition of conditional probability and the fact that $p(\hat l_{i,j}=-1)=p(l_{i,j}<0)$, $(d)$ follows by intersecting the events $\Big\{l_{i,j}< -\ln \left( \frac{f^{\Pue}(\mep^{\col,(\ell-1)})}{f^{\Quc}(\mep^{\col,(\ell-1)})}\right)\Big\}$ and $\{l_{i,j}<0\}$, and $(e)$ follows by recalling that $l_{i,j}\sim\mathcal{N}(2/\sigma^2,4/\sigma^2)$ if $c_{i,j}=0$ and employing the $\Q(\cdot)$ function. 

With the same approach above, one can compute the following probabilities
\begin{align}\label{p_baysd2} 
& \nonumber {p\left(\opt_{i,j}^{\mathsf r, (\ell)} < 0|\bar \mu _{i,j}^{\row,(\ell )}=1,{{\hat l}_{i,j}=-1}\right)}= \\ &  ~~~~~~~~~\frac{{\Q\left( { \frac{\sigma}{2}\text{min}\left(\ln \left( \frac{f^{\Puc}(\mep^{\col,(\ell-1)})}{f^{\Que}(\mep^{\col,(\ell-1)})}\right),0\right)+\frac{1}{\sigma}} \right) }}{p(l_{i,j}<0)},
\end{align}
\begin{align}\label{p_baysd3} 
& \nonumber {p\left(\opt_{i,j}^{\mathsf r, (\ell)} < 0|\bar \mu _{i,j}^{\row,(\ell )}=0,{{\hat l}_{i,j}=-1}\right)}= \\ &  ~~~~~~~~~\frac{{\Q\left( { \frac{\sigma}{2}\text{min}\left(\ln \left(\frac{f^{\Puep}(\mep^{\col,(\ell-1)})}{f^{\Quep}(\mep^{\col,(\ell-1)})}\right),0\right)+\frac{1}{\sigma}} \right) }}{p(l_{i,j}<0)}.
\end{align}
%\begin{align}\label{p_baysd4} 
%& \nonumber {p\left(\lambda_{i,j}^{\mathsf r, (\ell)} < 0|\bar \mu _{i,j}^{r,(\ell )}=-1,{{\hat l}_{i,j}=1}\right)}= \\ &  \frac{{\Q\left( { \frac{\sigma}{2}.\ln \left(\frac{f^{\Que}(\mep^{\col,(\ell-1)})}{f^{\Puc}(\mep^{\col,(\ell-1)})}\right)+\frac{1}{\sigma}} \right)}-\Q\left(\frac{1}{\sigma}\right)}{p(l_{i,j}>0)}
%\end{align}
%\begin{align}\label{p_baysd5} 
%& \nonumber {p\left(\lambda_{i,j}^{\mathsf r, (\ell)} < 0|\bar \mu _{i,j}^{r,(\ell )}=1,{{\hat l}_{i,j}=1}\right)}= \\ &  \frac{{\Q\left({ \frac{\sigma}{2}.\ln \left(\frac{f^{\Quc}(\mep^{\col,(\ell-1)})}{f^{\Pue}(\mep^{\col,(\ell-1)})}\right)+\frac{1}{\sigma}} \right)}-\Q\left(\frac{1}{\sigma}\right)}{p(l_{i,j}>0)}
%\end{align}
%\begin{align}\label{p_baysd6} 
%& \nonumber {p\left(\lambda_{i,j}^{\mathsf r, (\ell)} < 0|\bar \mu _{i,j}^{r,(\ell )}=0,{{\hat l}_{i,j}=1}\right)}= \\ &  \frac{{\Q\left( { \frac{\sigma}{2}.\ln \left(\frac{f^{\Quep}(\mep^{\col,(\ell-1)})}{f^{\Puep}(\mep^{\col,(\ell-1)})}\right)+\frac{1}{\sigma}} \right)}-\Q\left(\frac{1}{\sigma}\right)}{p(l_{i,j}>0)}
%\end{align}
%\begin{figure*}[t]
%	\vspace*{-20pt}
%	\hrulefill	
%	\begin{align}\label{int_f1}	
%	& \scalemath{0.96}{{\frac{1}{2}\sum\limits_{b = 0}^1 {\sum\limits_{j = 1}^{\frac{M}{2} - 1} \left({{{\left( { - 1} \right)}^b}{\mu _j}{w_j}\left( {\Q\left( {\frac{{{\mathsf{d}_{i - 1}} - {{\left( { - 1} \right)}^b}{\mu _j}}}{{{\sigma _j}}}} \right) - \Q\left( {\frac{{{\mathsf{d}_i} - {{\left( { - 1} \right)}^b}{\mu _j}}}{{{\sigma _j}}}} \right)} \right) + \frac{{{\sigma _j}{w_j}}}{{\sqrt {2\pi } }}\left( {{e^{ - \frac{{{{\left( {{{\left( { - 1} \right)}^b}{\mu _j} - {\mathsf{d}_{i - 1}}} \right)}^2}}}{{2\sigma _j^2}}}} - {e^{ - \frac{{{{\left( {{{\left( { - 1} \right)}^b}{\mu _j} - {\mathsf{d}_i}} \right)}^2}}}{{2\sigma _j^2}}}}} \right)}\right) } }} \\ & \label{deriv_qi23} {\frac{1}{2}\sum\limits_{b = 0}^1 {\sum\limits_{j = 1}^{\frac{M}{2} - 1} {{{\left( { - 1} \right)}^b}{\mu _j}{w_j}\left( {\Q\left( {\frac{{{\mathsf{d}_{i - 1}} - {{\left( { - 1} \right)}^b}{\mu _j}}}{{{\sigma _j}}}} \right) - \Q\left( {\frac{{{\mathsf{d}_i} - {{\left( { - 1} \right)}^b}{\mu _j}}}{{{\sigma _j}}}} \right)} \right)} } }
%	\end{align}
%	\hrulefill
%\end{figure*}
The probability ${p\left(\opt_{i,j}^{\mathsf r, (\ell)} < 0|\bar \mu _{i,j}^{\row,(\ell )}=-1,{{\hat l}_{i,j}=1}\right)}$ is 
\begin{align}\label{p_baysd4} 
& \nonumber {p\left(\opt_{i,j}^{\mathsf r, (\ell)} < 0\lvert\bar \mu _{i,j}^{\row,(\ell )}=-1,{{\hat l}_{i,j}=1}\right)}\myeqa \\ \nonumber & {p\left( l_{i,j} < -\ln \left( \frac{f^{\Que}(\mep^{\col,(\ell-1)})}{f^{\Puc}(\mep^{\col,(\ell-1)})}\right)\bigg\lvert\bar \mu _{i,j}^{\row,(\ell )}=-1,{{\hat l}_{i,j}}=1\right)}\myeqb \\ \nonumber & {p\left( l_{i,j} < -\ln \left( \frac{f^{\Que}(\mep^{\col,(\ell-1)})}{f^{\Puc}(\mep^{\col,(\ell-1)})}\right)\bigg\lvert{{\hat l}_{i,j}}=1\right)}\myeqc \\ & \frac{p\left(l_{i,j}<-\ln \left( \frac{f^{\Que}(\mep^{\col,(\ell-1)})}{f^{\Puc}(\mep^{\col,(\ell-1)})}\right), l_{i,j}>0 \right)}{p(l_{i,j}>0)} \myeqd \\ & \frac{p\left(0 < l_{i,j}<-\ln \left( \frac{f^{\Que}(\mep^{\col,(\ell-1)})}{f^{\Puc}(\mep^{\col,(\ell-1)})}\right) \right)}{p(l_{i,j}>0)},
\end{align}
where $(a)$ follows from \eqref{LLRoutv1} and Table~\ref{Tabcomp}, $(b)$ follows from the Markov chain $l_{i,j} \to \hat l_{i,j} \to \bar \mu _{i,j}^{\row,(\ell )}$, $(c)$ follows from the definition of conditional probability and the fact that $p(\hat l_{i,j}=1)=p(l_{i,j}>0)$, and $(d)$ follows by intersecting the events $\Big\{l_{i,j}< -\ln \left( \frac{f^{\Que}(\mep^{\col,(\ell-1)})}{f^{\Puc}(\mep^{\col,(\ell-1)})}\right)\Big\}$ and $\{l_{i,j}>0\}$. Note that we assumed that $\ln \left(\frac{f^{\Que}(\mep^{\col,(\ell-1)})}{f^{\Puc}(\mep^{\col,(\ell-1)})}\right)<0$, as for $\ln \left(\frac{f^{\Que}(\mep^{\col,(\ell-1)})}{f^{\Puc}(\mep^{\col,(\ell-1)})}\right)>0$ the intersection between the events $\Big\{l_{i,j}< -\ln \left( \frac{f^{\Que}(\mep^{\col,(\ell-1)})}{f^{\Puc}(\mep^{\col,(\ell-1)})}\right)\Big\}$ and $\{l_{i,j}>0\}$ is null and \eqref{p_baysd4} boils down to zero. By recalling the distribution of $l_{i,j}$ and employing $\Q(\cdot)$, we have
\begin{align}\label{p_baysd44}
& \nonumber {p\left(\opt_{i,j}^{\mathsf r, (\ell)} < 0|\bar \mu _{i,j}^{\row,(\ell )}=-1,{{\hat l}_{i,j}=1}\right)}= \\ &  
\scalemath{0.85}{\frac{{\Q\left( { \frac{\sigma}{2}\ln \left(\frac{f^{\Que}(\mep^{\col,(\ell-1)})}{f^{\Puc}(\mep^{\col,(\ell-1)})}\right)+\frac{1}{\sigma}} \right)}-\Q\left(\frac{1}{\sigma}\right)}{p(l_{i,j}>0)}\cdot \bar{\mathbbm{U}} \left( \ln \left(\frac{f^{\Que}(\mep^{\col,(\ell-1)})}{f^{\Puc}(\mep^{\col,(\ell-1)})}\right) \right)}
\end{align}
Similarly, one can compute the following probabilities
\begin{align}\label{p_baysd5}
& \nonumber {p\left(\opt_{i,j}^{\mathsf r, (\ell)} < 0|\bar \mu _{i,j}^{\row,(\ell )}=1,{{\hat l}_{i,j}=1}\right)}= \\ & \scalemath{0.83}{\frac{{\Q\left( { \frac{\sigma}{2}\ln \left(\frac{f^{\Quc}(\mep^{\col,(\ell-1)})}{f^{\Pue}(\mep^{\col,(\ell-1)})}\right)+\frac{1}{\sigma}} \right)}-\Q\left(\frac{1}{\sigma}\right)}{p(l_{i,j}>0)}\cdot \bar{\mathbbm{U}} \left( \ln \left(\frac{f^{\Quc}(\mep^{\col,(\ell-1)})}{f^{\Pue}(\mep^{\col,(\ell-1)})}\right) \right)}
\end{align}
\begin{align}\label{p_baysd6}
& \nonumber {p\left(\opt_{i,j}^{\mathsf r, (\ell)} < 0|\bar \mu _{i,j}^{r,(\ell )}=0,{{\hat l}_{i,j}=1}\right)}= \\ &  \scalemath{0.83}{\frac{{\Q\left( { \frac{\sigma}{2}\ln \left(\frac{f^{\Quep}(\mep^{\col,(\ell-1)})}{f^{\Puep}(\mep^{\col,(\ell-1)})}\right)+\frac{1}{\sigma}} \right)}-\Q\left(\frac{1}{\sigma}\right)}{p(l_{i,j}>0)}\cdot \bar{\mathbbm{U}} \left( \ln \left(\frac{f^{\Quep}(\mep^{\col,(\ell-1)})}{f^{\Puep}(\mep^{\col,(\ell-1)})}\right) \right)}
\end{align}

By substituting \eqref{pe}--\eqref{q_ep} and \eqref{p_baysd1}--\eqref{p_baysd6} into \eqref{p_bays}, the closed-form expression for $\mep^{\row,(\ell)}$ in \eqref{xrl} is obtained. We remark that in this substitution, $p({\hat l_{i,j}})$ in \eqref{p_bays} cancels out with the denominator of \eqref{p_baysd1}--\eqref{p_baysd6}.  

%The message error probability for column component code ($\mep^{\col,(\ell)}$) can be computed similarly assuming $\mep^{\row,(\ell)}$ as the input error probability for the column decoding. The closed-form expression for $\mep^{\col,(\ell)}$ is given in \eqref{xcl}. 


By substituting $\mep^{\col,(\ell-1)}$, $\bar \mu _{i,j}^{\row,(\ell )}$, and $\opt_{i,j}^{\mathsf r, (\ell)}$ with $\mep^{\row,(\ell)}$, $\bar \mu _{i,j}^{\col,(\ell )}$, and $\opt_{i,j}^{\mathsf c, (\ell)}$, and following the same derivation steps as above, Table~\ref{Tabcompc} and \eqref{xcl} are obtained. This concludes the proof.

\section{Proof of \GL{Proposition} 2} \label{AB} 
\AG{By considering channel adapters, we can  assume the transmission of the all-zero codeword  in the DE analysis. With this assumption  and employing \eqref{LLRMaxlog_approx} and \eqref{LLRMaxlog_approxllr}, the PDF of the symmetrized LLRs is}
\begin{align} \label{LLRMaxlog_approx1}
p\left( {\bar l_{i,j}|b_{i,j} = 0} \right) = \sum\limits_{j = 0}^{\frac{M}{2} - 1} {{w_j}} {\G}(\bar l;{\mu _j},\sigma _j^2),
\end{align}
where $w_j$, $\mu _j$, and $\sigma _j^2$ are given in Sec.~\ref{QAM_BICM}. 
The \AG{main difference between the DE analysis for BICM and for the bi-AWGN channel  in Appendix~\ref{APP1} is that  the PDF of the LLRs in \eqref{LLRMaxlog_approx1} should be used in the analysis. By employing \eqref{LLRMaxlog_approx1} in \eqref{p_baysd1}--\eqref{p_baysd6} the different terms for \eqref{p_bays} are obtained and the expressions in \eqref{QAMxrl} and \eqref{QAMxcl} result for $\mep^{\row,(\ell)}$ and $\mep^{\col,(\ell)}$, respectively. We remark that to run DE  the error probability of the VNs should be initialized to $p_\mathsf{ch}$}. For the BICM channel $p_\mathsf{ch}$ can be computed by integrating the tail of the LLR distribution \eqref{LLRMaxlog_approx1}, yielding
\begin{align} \label{pch_computation}
p_\mathsf{ch} = p(\bar l_{i,j}<0|b_{i,j} = 0)=\sum\limits_{j = 0}^{\frac{M}{2} - 1} {{w_j}}\cdot\Q\left(\frac{\mu_j}{\sigma_j}\right).
\end{align}  
This concludes the proof.
%\section{\GL{I would really remove this... It is a plain application of LM to a Gaussian mixture...}} \label{AC} 
%Recall the quantization value $\mathsf{q}_{i}$ in \eqref{qi_quant}. By employing the conditional LLR \eqref{LLRMaxlog_approx}, the nominator of \eqref{qi_quant} can be computed as 
%\begin{align} \label{deriv_qi1}
%&\scalemath{0.99}{\int_{{\mathsf{d}_{i-1}}}^{{\mathsf{d}_{i}}} {\lambda \left( {\frac{1}{2}\sum\limits_{b = 0}^1 {\sum\limits_{j = 1}^{\frac{M}{2} - 1} {{{\left( { - 1} \right)}^b}{w_j}\G (\lambda ;{{\left( { - 1} \right)}^b}{\mu _j},\sigma _j^2)} } } \right)} d\lambda  = }\\ & \label{deriv_qi11} \frac{1}{2}\sum\limits_{b = 0}^1 {\sum\limits_{j = 1}^{\frac{M}{2} - 1} {{{\left( { - 1} \right)}^b}{w_j}\int_{{\mathsf{d}_{i-1}}}^{{\mathsf{d}_{i}}} {\frac{\lambda }{{\sqrt {2\pi \sigma _j^2} }}} } } {e^{ - \frac{{{{\left( {\lambda  - {{\left( { - 1} \right)}^b}{\mu _j}} \right)}^2}}}{{2\sigma _j^2}}}}d\lambda.
%\end{align}  
%By manipulating \cite[Eq.~2.4.1]{gradshteyn2007}, one can show that the following holds
%\begin{align} \label{int_f}
%\int {\frac{{x{e^{ - \frac{{{{\left( {x - {a_1}} \right)}^2}}}{{2a_2^2}}}}}}{{\sqrt {2\pi {a^{2}_2}} }}} dx =  {\frac{{a_1}}{2} - \Q\left( {\frac{{x - {a_1}}}{{{a_2}}}} \right)} - \frac{{{a_2}}}{{\sqrt {2\pi } }}{e^{{e^{ - \frac{{{{\left( {x - {a_1}} \right)}^2}}}{{2a_2^2}}}}}},
%\end{align} 
%where $a_1$ and $a_2$ are two constants. Substituting \eqref{int_f} into \eqref{deriv_qi11} with $a_1={\left( { - 1} \right)}^b$ and $a_2=\sigma_j$ and then applying the boundary values of the integral yields \eqref{int_f1}.
%The denominator of \eqref{int_f} can be calculated as
% \begin{align} \label{deriv_qi2}
%&\int_{{\mathsf{d}_{i-1}}}^{{\mathsf{d}_{i}}} {\left( {\frac{1}{2}\sum\limits_{b = 0}^1 {\sum\limits_{j = 1}^{\frac{M}{2} - 1} {{{\left( { - 1} \right)}^b}{w_j}\G (\lambda ;{{\left( { - 1} \right)}^b}{\mu _j},\sigma _j^2)} } } \right)} d\lambda  = \\ & \label{deriv_qi22}\frac{1}{2}\sum\limits_{b = 0}^1 {\sum\limits_{j = 1}^{\frac{M}{2} - 1} {{{\left( { - 1} \right)}^b}{w_j}\int\limits_{{\mathsf{d}_{i-1}}}^{{\mathsf{d}_{i}}} {\frac{1 }{{\sqrt {2\pi \sigma _j^2} }}} } } {e^{ - \frac{{{{\left( {\lambda  - {{\left( { - 1} \right)}^b}{\mu _j}} \right)}^2}}}{{2\sigma _j^2}}}}d\lambda.
%\end{align} 
%Using $\Q(\cdot)$ function definition, \eqref{deriv_qi22} can be simplified by \eqref{deriv_qi23}.
%Dividing \eqref{int_f1} to \eqref{deriv_qi23} results in \eqref{qi_quant}. This concludes the proof.
 
%\balance 
 
%\bibliographystyle{IEEEtran}
%\bibliography{IEEEabrv,book}

% Generated by IEEEtran.bst, version: 1.14 (2015/08/26)
\begin{thebibliography}{10}
	\providecommand{\url}[1]{#1}
	\csname url@samestyle\endcsname
	\providecommand{\newblock}{\relax}
	\providecommand{\bibinfo}[2]{#2}
	\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
	\providecommand{\BIBentryALTinterwordstretchfactor}{4}
	\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
		\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
		\fontdimen4\font\relax}
	\providecommand{\BIBforeignlanguage}[2]{{%
			\expandafter\ifx\csname l@#1\endcsname\relax
			\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
			\typeout{** loaded for the language `#1'. Using the pattern for}%
			\typeout{** the default language instead.}%
			\else
			\language=\csname l@#1\endcsname
			\fi
			#2}}
	\providecommand{\BIBdecl}{\relax}
	\BIBdecl
	
	\bibitem{Berrouetal93}
	C.~Berrou, A.~Glavieux, and P.~Thitimajshima, ``Near shannon limit
	error-correcting coding and decoding: Turbo-codes,'' in \emph{Proc.\ IEEE
		Int.\ Inf.\ Conf. \ Commun.}, vol.~2, Geneva, Switzerland, May 1993, pp.
	1064--1070.
	
	\bibitem{Gallager63:LDPC}
	R.~G. Gallager, \emph{Low-Density Parity-Check Codes}.\hskip 1em plus 0.5em
	minus 0.4em\relax Cambridge, MA, USA: M.I.T. Press, 1963.
	
	\bibitem{Cushon2016}
	K.~{Cushon}, P.~{Larsson-Edefors}, and P.~{Andrekson}, ``Low-power {400-Gbps}
	soft-decision {LDPC FEC} for optical transport networks,'' \emph{IEEE/OSA J.\
		Lightw.\ Technol.}, vol.~34, no.~18, pp. 4304--4311, Sep. 2016.
	
	\bibitem{Ste19}
	F.~{Steiner}, E.~{Ben Yacoub}, B.~{Matuz}, G.~{Liva}, and A.~{Graell i Amat},
	``One and two bit message passing for {SC-LDPC} codes with higher-order
	modulation,'' \emph{J. Lightw. Technol.}, vol.~37, no.~23, pp. 5914--5925,
	Dec. 2019.
	
	\bibitem{staircase_frank}
	B.~P. Smith, A.~Farhood, A.~Hunt, F.~R. Kschischang, and J.~Lodge, ``Staircase
	codes: {FEC} for 100 {G}b/s {OTN},'' \emph{IEEE/OSA J.\ Lightw.\ Technol.},
	vol.~30, no.~1, pp. 110--117, Jan. 2012.
	
	\bibitem{JianPfister2017}
	Y.~Jian, H.~D. Pfister, and K.~R. Narayanan, ``Approaching capacity at high
	rates with iterative hard-decision decoding,'' \emph{IEEE Trans.\ Inf.\
		Theory}, vol.~63, no.~9, pp. 5752--5773, Sep. 2017.
	
	\bibitem{Hag18}
	C.~H{\"{a}}ger and H.~D. Pfister, ``Approaching miscorrection-free performance
	of product codes with anchor decoding,'' \emph{IEEE Trans. Commun.}, vol.~66,
	no.~7, pp. 2797--2808, Jul. 2018.
	
	\bibitem{sheikhTCOM19}
	A.~Sheikh, A.~{Graell i Amat}, and G.~Liva, ``Binary message passing decoding
	of product-like codes,'' \emph{IEEE Trans.\ Commun.}, vol.~67, no.~12, pp.
	8167--8178, Dec. 2019.
	
	\bibitem{She18b}
	A.~Sheikh, A.~{Graell i Amat}, G.~Liva, C.~H\"ager, and H.~D. Pfister, ``On
	low-complexity decoding of product codes for high-throughput fiber-optic
	systems,'' in \emph{Proc. IEEE Int. Symp. on Turbo Codes \& Iterative Inf.
		Proc. (ISTC)}, Hong Kong, Dec. 2018.
	
	\bibitem{She19}
	A.~Sheikh, A.~{Graell i Amat}, and G.~Liva, ``Binary message passing decoding
	of product codes based on generalized minimum distance decoding,'' in
	\emph{Proc. 53rd Annu. Conf. Inf. Sciences and Systems (CISS)}, Baltimore,
	MD, Mar. 2019.
	
	\bibitem{YibitTCOM}
	Y.~Lei, B.~Chen, G.~Liga, X.~Deng, Z.~Cao, J.~Li, K.~Xu, and A.~Alvarado,
	``Improved decoding of staircase codes: The soft-aided bit-marking {(SABM)}
	algorithm,'' \emph{IEEE Trans.\ Commun.}, vol.~67, no.~12, pp. 8220--8232,
	Dec. 2019.
	
	\bibitem{Lech12}
	G.~Lechner, T.~Pedersen, and G.~Kramer, ``Analysis and {{Design}} of {{Binary
			Message Passing Decoders}},'' \emph{IEEE Trans. Commun.}, vol.~60, no.~3,
	Mar. 2012.
	
	\bibitem{GabrieleSABMSR2019}
	G.~Liga, A.~Sheikh, and A.~Alvarado, ``A novel soft-aided bit-marking decoder
	for product codes,'' Dublin, Ireland, Sep. 2019.
	
	\bibitem{Ben18}
	E.~{Ben Yacoub}, F.~Steiner, B.~Matuz, and G.~Liva, ``{Protograph-Based LDPC
		Code Design for Ternary Message Passing Decoding},'' in \emph{Proc. Int. ITG
		Conf. Syst., Commun. and Coding (SCC)}, Rostock, Germany, Mar. 2019.
	
	\bibitem{Montorsi2018}
	G.~{Montorsi} and S.~{Benedetto}, ``High throughput two-stage soft/hard codecs
	for optical communications,'' in \emph{Proc. IEEE Int. Symp. on Turbo Codes
		\& Iterative Inf. Proc. (ISTC)}, Hong Kong, Dec. 2018.
	
	\bibitem{Ksch17}
	L.~M. Zhang and F.~R. Kschischang, ``Low-complexity soft-decision concatenated
	{LDGM}-staircase {FEC} for high-bit-rate fiber-optic communication,''
	\emph{IEEE/OSA J.\ Lightw.\ Technol.}, vol.~35, no.~18, pp. 3991--3999, Sep.
	2017.
	
	\bibitem{Ksch18}
	M.~Barakatain and F.~R. Kschischang, ``Low-complexity concatenated
	{LDPC}-staircase codes,'' \emph{IEEE/OSA J.\ Lightw.\ Technol.}, vol.~36,
	no.~12, pp. 2443--2449, Jun. 2018.
	
	\bibitem{Lentmaier98:GLDPC}
	M.~Lentmaier and K.~S. Zigangirov, ``Iterative decoding of generalized
	low-density parity-check codes,'' in \emph{Proc. IEEE International Symp. on
		Inform. Theory (ISIT)}, Boston, USA, Aug. 1998.
	
	\bibitem{Tanner1981}
	R.~{Tanner}, ``A recursive approach to low complexity codes,'' \emph{IEEE
		Trans.\ Inf.\ Theory}, vol.~27, no.~5, pp. 533--547, Sep. 1981.
	
	\bibitem{Haeger2017tit}
	C.~H{\"{a}}ger, H.~D. Pfister, A.~{Graell i Amat}, and
	F.~Br{\"{a}}nnstr{\"{o}}m, ``Density evolution for deterministic generalized
	product codes on the binary erasure channel at high rates,'' \emph{IEEE
		Trans. Inf. Theory}, vol.~63, no.~7, pp. 4357--4378, Jul. 2017.
	
	\bibitem{Zehavi1992}
	E.~{Zehavi}, ``8-{PSK} trellis codes for a rayleigh channel,'' \emph{IEEE
		Trans.\ Commun.}, vol.~40, no.~5, pp. 873--884, May 1992.
	
	\bibitem{Caire1998}
	G.~{Caire}, G.~{Taricco}, and E.~{Biglieri}, ``Bit-interleaved coded
	modulation,'' \emph{IEEE Trans.\ Inf.\ Theory}, vol.~44, no.~3, pp. 927--946,
	May 1998.
	
	\bibitem{georg_tcom}
	G.~B\"ocherer, F.~Steiner, and P.~Schulte, ``Bandwidth efficient and
	rate-matched low-density parity-check coded modulation,'' \emph{IEEE Trans.\
		Commun.}, vol.~63, no.~12, pp. 4651--4665, Dec. 2015.
	
	\bibitem{Buchali_2016}
	{F. Buchali}, {F. Steiner}, {G. B\"ocherer}, {L. Schmalen}, {P. Schulte}, and
	{W. Idler}, ``Rate adaptation and reach increase by probabilistically shaped
	64-{QAM}: An experimental demonstration,'' \emph{IEEE/OSA J.\ Lightw.\
		Technol.}, vol.~34, no.~7, pp. 1599--1609, Apr. 2016.
	
	\bibitem{BRGC}
	F.~{Gray}, ``Pulse code communications,'' U.S. Patent 2632058, Mar. 1953.
	
	\bibitem{Viterbi1998}
	A.~J. {Viterbi}, ``An intuitive justification and a simplified implementation
	of the {MAP} decoder for convolutional codes,'' \emph{IEEE J.\ Select.\ Areas
		in Communications.}, vol.~16, no.~2, pp. 260--264, Feb. 1998.
	
	\bibitem{Hou2003}
	J.~{Hou}, P.~H. {Siegel}, L.~B. {Milstein}, and H.~D. {Pfister},
	``Capacity-approaching bandwidth-efficient coded modulation schemes based on
	low-density parity-check codes,'' \emph{IEEE Trans.\ Inf.\ Theory}, vol.~49,
	no.~9, pp. 2141--2155, Sep. 2003.
	
	\bibitem{Ivanov2016}
	M.~{Ivanov}, C.~{H{\"{a}}ger}, F.~{Br{\"{a}}nnstr{\"{o}}m}, A.~{Graell i Amat},
	A.~{Alvarado}, and E.~{Agrell}, ``On the information loss of the {Max-Log}
	approximation in {BICM} systems,'' \emph{IEEE Trans.\ Inf.\ Theory}, vol.~62,
	no.~6, pp. 3011--3025, Jun. 2016.
	
	\bibitem{Max1960}
	J.~{Max}, ``Quantizing for minimum distortion,'' \emph{IRE Transactions on
		Information Theory}, vol.~6, no.~1, pp. 7--12, Mar. 1960.
	
	\bibitem{Lloyd1980}
	S.~{Lloyd}, ``Least squares quantization in {PCM},'' \emph{IEEE Trans.\ Inf.\
		Theory}, vol.~28, no.~2, pp. 129--137, Mar. 1982.
	
\end{thebibliography}




\end{document}
