%File: formatting-instructions-latex-2024.tex
%release 2024.0
\documentclass[letterpaper]{article} % DO NOT CHANGE THIS
\usepackage{aaai24}  % DO NOT CHANGE THIS
\usepackage{times}  % DO NOT CHANGE THIS
\usepackage{helvet}  % DO NOT CHANGE THIS
\usepackage{courier}  % DO NOT CHANGE THIS
\usepackage[hyphens]{url}  % DO NOT CHANGE THIS
\usepackage{graphicx} % DO NOT CHANGE THIS
\urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm}  % DO NOT CHANGE THIS
\usepackage{natbib}  % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\frenchspacing  % DO NOT CHANGE THIS
\setlength{\pdfpagewidth}{8.5in}  % DO NOT CHANGE THIS
\setlength{\pdfpageheight}{11in}  % DO NOT CHANGE THIS
%
% These are recommended to typeset algorithms but not required. See the subsubsection on algorithms. Remove them if you don't have algorithms in your paper.
\usepackage{algorithm}
\usepackage{algorithmic}

%
% These are are recommended to typeset listings but not required. See the subsubsection on listing. Remove this block if you don't have listings in your paper.
\usepackage{newfloat}
\usepackage{listings}
\DeclareCaptionStyle{ruled}{labelfont=normalfont,labelsep=colon,strut=off} % DO NOT CHANGE THIS
\lstset{%
	basicstyle={\footnotesize\ttfamily},% footnotesize acceptable for monospace
	numbers=left,numberstyle=\footnotesize,xleftmargin=2em,% show line numbers, remove this entire line if you don't want the numbers.
	aboveskip=0pt,belowskip=0pt,%
	showstringspaces=false,tabsize=2,breaklines=true}
\floatstyle{ruled}
\newfloat{listing}{tb}{lst}{}
\floatname{listing}{Listing}

\usepackage{enumitem}
\usepackage{color} 
\usepackage{graphicx}
% \usepackage{stfloats}
\usepackage{booktabs}
% \usepackage[skip=7pt]{caption}
\usepackage{subcaption}
\usepackage{todonotes}
\usepackage{array}
\usepackage{amssymb, amsmath}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{threeparttable}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{microtype}
\usepackage{listings}

\definecolor{mypurple1}{RGB}{143, 94, 255}
\definecolor{myblue1}{RGB}{59, 76, 206}
\definecolor{myred1}{RGB}{190, 90, 75}
\definecolor{myyellow1}{RGB}{255, 192, 0}
\definecolor{mygreen1}{RGB}{160, 194, 128}
% Keep the \pdfinfo as shown here. There's no need
% for you to add the /Title and /Author tags.
\pdfinfo{
/TemplateVersion (2024.1)
}
\nocopyright

% DISALLOWED PACKAGES
% \usepackage{authblk} -- This package is specifically forbidden
% \usepackage{balance} -- This package is specifically forbidden
% \usepackage{color (if used in text)
% \usepackage{CJK} -- This package is specifically forbidden
% \usepackage{float} -- This package is specifically forbidden
% \usepackage{flushend} -- This package is specifically forbidden
% \usepackage{fontenc} -- This package is specifically forbidden
% \usepackage{fullpage} -- This package is specifically forbidden
% \usepackage{geometry} -- This package is specifically forbidden
% \usepackage{grffile} -- This package is specifically forbidden
% \usepackage{hyperref} -- This package is specifically forbidden
% \usepackage{navigator} -- This package is specifically forbidden
% (or any other package that embeds links such as navigator or hyperref)
% \indentfirst} -- This package is specifically forbidden
% \layout} -- This package is specifically forbidden
% \multicol} -- This package is specifically forbidden
% \nameref} -- This package is specifically forbidden
% \usepackage{savetrees} -- This package is specifically forbidden
% \usepackage{setspace} -- This package is specifically forbidden
% \usepackage{stfloats} -- This package is specifically forbidden
% \usepackage{tabu} -- This package is specifically forbidden
% \usepackage{titlesec} -- This package is specifically forbidden
% \usepackage{tocbibind} -- This package is specifically forbidden
% \usepackage{ulem} -- This package is specifically forbidden
% \usepackage{wrapfig} -- This package is specifically forbidden
% DISALLOWED COMMANDS
% \nocopyright -- Your paper will not be published if you use this command
% \addtolength -- This command may not be used
% \balance -- This command may not be used
% \baselinestretch -- Your paper will not be published if you use this command
% \clearpage -- No page breaks of any kind may be used for the final version of your paper
% \columnsep -- This command may not be used
% \newpage -- No page breaks of any kind may be used for the final version of your paper
% \pagebreak -- No page breaks of any kind may be used for the final version of your paperr
% \pagestyle -- This command may not be used
% \tiny -- This is not an acceptable font size.
% \vspace{- -- No negative value may be used in proximity of a caption, figure, table, section, subsection, subsubsection, or reference
% \vskip{- -- No negative value may be used to alter spacing above or below a caption, figure, table, section, subsection, subsubsection, or reference

\setcounter{secnumdepth}{0} %May be changed to 1 or 2 if section numbers are desired.

% The file aaai24.sty is the style file for AAAI Press
% proceedings, working notes, and technical reports.
%

% Title

% Your title must be in mixed case, not sentence case.
% That means all verbs (including short verbs like be, is, using,and go),
% nouns, adverbs, adjectives should be capitalized, including both words in hyphenated terms, while
% articles, conjunctions, and prepositions are lower case unless they
% directly follow a colon or long dash
\title{JoTR: A Joint Transformer and Reinforcement Learning Framework for Dialog Policy Learning}
\author{
    Wai-Chung Kwan\equalcontrib \textsuperscript{\rm 1},
    Huimin Wang\equalcontrib \textsuperscript{\rm 2},
    Hongru Wang \textsuperscript{\rm 1},
    Zezhong Wang \textsuperscript{\rm 1},
    Xian Wu \textsuperscript{\rm 2}, \\
    Yefeng Zheng \textsuperscript{\rm 2},
    Kam-Fai Wong \textsuperscript{\rm 1}
}
\affiliations{
    %Afiliations
    \textsuperscript{\rm 1} The Chinese University of Hong Kong\\
     \textsuperscript{\rm 2} Jarvis Lab, Tencent \\
    % If you have multiple authors and multiple affiliations
    % use superscripts in text and roman font to identify them.
    % For example,

    % Sunil Issar\textsuperscript{\rm 2}, 
    % J. Scott Penberthy\textsuperscript{\rm 3}, 
    % George Ferguson\textsuperscript{\rm 4},
    % Hans Guesgen\textsuperscript{\rm 5}
    % Note that the comma should be placed after the superscript

    % 1900 Embarcadero Road, Suite 101\\
    % Palo Alto, California 94303-3310 USA\\
    % % email address must be in roman text type, not monospace or sans serif
    % proceedings-questions@aaai.org
%
% See more examples next
}

%Example, Single Author, ->> remove \iffalse,\fi and place them surrounding AAAI title to use it
\iffalse
\title{My Publication Title --- Single Author}
\author {
    Author Name
}
\affiliations{
    Affiliation\\
    Affiliation Line 2\\
    name@example.com
}
\fi

\iffalse
%Example, Multiple Authors, ->> remove \iffalse,\fi and place them surrounding AAAI title to use it
\title{My Publication Title --- Multiple Authors}
\author {
    % Authors
    First Author Name\textsuperscript{\rm 1,\rm 2},
    Second Author Name\textsuperscript{\rm 2},
    Third Author Name\textsuperscript{\rm 1}
}
\affiliations {
    % Affiliations
    \textsuperscript{\rm 1}Affiliation 1\\
    \textsuperscript{\rm 2}Affiliation 2\\
    firstAuthor@affiliation1.com, secondAuthor@affilation2.com, thirdAuthor@affiliation1.com
}
\fi


% REMOVE THIS: bibentry
% This is only needed to show inline citations in the guidelines document. You should not need it and can safely delete it.
\usepackage{bibentry}
% END REMOVE bibentry

\begin{document}

\maketitle

\begin{abstract}
Dialogue policy learning (DPL) is a crucial component of dialogue modelling. Its primary role is to determine the appropriate abstract response, commonly referred to as the “dialogue action”. Traditional DPL methodologies have treated this as a sequential decision problem, using pre-defined action candidates extracted from a corpus. However, these incomplete candidates can significantly limit the diversity of responses and pose challenges when dealing with edge cases, which are scenarios that occur only at extreme operating parameters. To address these limitations, we introduce a novel framework, JoTR. This framework is unique as it leverages a text-to-text Transformer-based model to generate flexible dialogue actions. Unlike traditional methods, JoTR formulates a word-level policy that allows for a more dynamic and adaptable dialogue action generation, without the need for any action templates. This setting enhances the diversity of responses and improves the system's ability to handle edge cases effectively. In addition, JoTR employs reinforcement learning with a reward-shaping mechanism to efficiently finetune the word-level dialogue policy, which allows the model to learn from its interactions, improving its performance over time. We conducted an extensive evaluation of JoTR to assess its effectiveness. Our extensive evaluation shows that JoTR achieves state-of-the-art performance on two benchmark dialogue modelling tasks, as assessed by both user simulators and human evaluators \footnote{Our code, models and other related resources are publicly available at https://github.com/KwanWaiChung/JoTR.}
\end{abstract}

\section{Introduction}
\begin{figure}
    \centering
    \includegraphics[width=0.45\textwidth]{figure/figure1.png}
    \caption{This illustrates dialogue policy learning in pipeline dialogue system, where NLU/NLG stands for Natural Language Understanding/Generation, and DST refers to Dialogue State Tracking.}
    \label{fig1:intro}
    \vspace{-2mm}
\end{figure}

Dialogue policy learning (DPL) is a fundamental component of pipeline-based dialogue systems. As illustrated in Figure \ref{fig1:intro}, its primary role is to select the appropriate dialogue action to manage the conversation flow, essentially acting as the decision-making core of the system. Previous research in this field has predominantly focused on treating DPL as a sequential decision problem. The optimization of the policy is typically achieved using reinforcement learning (RL), a machine learning approach that learns from its interactions with the environment, based on predefined dialogue action candidates \cite{lipton2016efficient, li2017end, peng2018adversarial,takanobu-etal-2019-guided, wang2020learning, li2020guided, kwan2023survey}. A dialogue action usually comprises one or more combinations of domain name, intent type, and slot name, collectively referred to as an “atomic action” \cite{li2020rethinking}. Traditional DPL approaches filter action candidates from a corpus, placing emphasis on more frequent actions to enhance the efficiency of the RL agent. However, this approach has its limitations. It may underperform in less common situations and can restrict response diversity and naturalness due to the rigid templates of allowable actions \cite{wang-wong-2021-collaborative}. This limitation stems from the fact that policy optimization is based on incomplete action candidates, which include only a subset of possible atomic actions or their combinations. To address this issue, \citet{li2020rethinking} proposed using a GRU-based decoder to predict atomic actions sequentially in a single turn. This approach allows the DPL model to learn the action structure from data, resulting in more flexible and powerful system responses. However, this method has a significant drawback: the dialogue state and atomic action space grow exponentially with the number of domains, intents, or slots \cite{lee-etal-2019-convlab}, making it challenging to train the model effectively. Alternatively, \citet{wang-wong-2021-collaborative} proposed a multi-agent reinforcement learning framework where atomic actions were represented as the cross product of three spaces, with each part assigned to a different agent. While this approach showed promising results, it had the significant drawback of allowing only one atomic action per turn. This limitation necessitates more turns to achieve user goals and can lead to unnatural utterances. A recent work that used a transformer to generate actions is closely related to our research \cite{geishauser-etal-2022-dynamic}. However, their work primarily focused on continual learning and did not address the issue of action efficiency, which is a critical aspect of DPL. Our research aims to build upon these previous studies, addressing their limitations and enhancing the efficiency and effectiveness of DPL.
%%%
% Dialog policy learning (DPL), a core component of pipeline-based dialog systems, aims to select the appropriate dialog action to manage the conversation flow. Previous research has primarily focused on treating DPL as a sequential decision problem and optimizing the policy using reinforcement learning (RL) based on predefined dialog action candidates \cite{lipton2016efficient, li2017end, peng2018adversarial,takanobu-etal-2019-guided, wang2020learning, li2020guided, kwan2023survey}. A dialog action typically consists of one or more combinations of domain name, intent type, and slot name, known as an “atomic action” \cite{li2020rethinking}. Conventional approaches to DPL filter action candidates from a corpus, emphasizing more frequent actions to improve the RL agent's efficiency. However, this approach may underperform in less common situations and can limit response diversity and naturalness due to rigid templates of allowable actions \cite{wang-wong-2021-collaborative}. This limitation arises because policy optimization is based on incomplete action candidates, including only a subset of possible atomic actions or their combinations. To address this issue, \citet{li2020rethinking} proposed using a GRU-based decoder to predict atomic actions sequentially in a single turn, allowing the DPL model to learn the action structure from data and resulting in more flexible and powerful system responses. However, this approach has a drawback: the dialog state and atomic action space grow exponentially with the number of domains, intents, or slots \cite{lee-etal-2019-convlab}, making it difficult to train the model effectively. Alternatively, \citet{wang-wong-2021-collaborative} proposed a multi-agent reinforcement learning framework in which atomic actions were represented as the cross product of three spaces, with each part assigned to a different agent. This approach showed promising results but had the significant drawback of allowing only one atomic action per turn, necessitating more turns to achieve user goals and leading to unnatural utterances. A recent work using a transformer to generate actions is closely related to our research \cite{geishauser-etal-2022-dynamic}. However, their work focused on continual learning and did not address action efficiency.

In this paper, we introduce a novel approach, the \textbf{Jo}int \textbf{T}ransformer \textbf{R}einforcement Learning Framework (\textbf{JoTR}), designed to address the limitations inherent in dialogue action candidates. The primary innovation of JoTR lies in its ability to generate actions with a word-level policy directly, a significant departure from the traditional reliance on fixed, human-defined templates. This is achieved by leveraging the capabilities of a text-to-text transformer, a model that has shown remarkable success in various natural language processing tasks. Simultaneously, JoTR employs reinforcement learning and reward shaping, two powerful techniques in machine learning, to optimize the word-level policy. This results in more efficient responses that require fewer interaction turns, a critical aspect in enhancing the user experience in dialogue systems. Furthermore, JoTR demonstrates improved learning efficiency compared to traditional methods, making it a promising approach for dialogue policy learning (DPL). We evaluate the performance of JoTR on two benchmark multi-domain dialogue modeling tasks, providing a comprehensive assessment of its effectiveness. Our research is primarily focused on developing a system that generates flexible and efficient responses for DPL, a critical aspect of dialogue systems. 

Our work offers several key contributions to the field. Firstly, we formulate the DPL as a conditional sequence generation problem, a novel approach that allows for more dynamic action generation. Secondly, we introduce a transformer reinforcement learning framework, designed to learn the word-level DPL and optimize it with a reward-shaping mechanism. This approach enhances the model's ability to adapt and improve over time. Thirdly, we validate the effectiveness of our proposed models through multi-domain testing on a simulator. This rigorous testing ensures that our model can handle a variety of dialogue scenarios effectively. Lastly, we implement a rule-based simulator for the benchmark Schema-Guided Dialogue (SGD) dataset \cite{rastogi2020towards}, a widely-used resource in dialogue system research. This implementation provides a practical tool for further research and development in the field.

% In this paper, we propose the the \textbf{Jo}int \textbf{T}ransformer \textbf{R}einforcement Learning Framework (\textbf{JoTR}) to address the limitations in dialog action candidates. The key insight of JoTR is that it generates actions with a word-level policy directly, moving away from fixed, human-defined templates by using a text-to-text transformer. Meanwhile, reinforcement learning and reward shaping are applied to optimize the word-level policy, resulting in more efficient responses with fewer interaction turns. JoTR also demonstrates improved learning efficiency compared to traditional methods. We evaluate its performance on two benchmark multi-domain dialog modeling tasks. In summary, our research focuses on developing a system that generates flexible and efficient responses for dialog policy learning (DPL). Specifically, we propose several key contributions: 1) Formulating the DPL as a conditional sequence generation problem to conduct action generation. 2) Introducing a transformer reinforcement learning framework to learn the word-level DPL and optimize it with a reward-shaping mechanism. 3) Validating the effectiveness of our proposed models through multi-domain testing on a simulator. 4) Implementing a rule-based simulator for the benchmark Schema-Guided Dialog (SGD) dataset \cite{rastogi2020towards}.

\section{Related Work}
\paragraph{Dialogue Policy Learning}
The mainstream method for constructing a task-oriented dialogue (TOD) system is the pipeline approach, which divides the system into four interconnected modules: natural language understanding (to identify user intentions), dialogue state tracking (to monitor the dialogue status), dialogue policy learning (to decide the subsequent system action), and natural language generation (to generate dialogue responses) \citet{kwan2023survey}.
Reinforcement learning has been the main stream approach to optimize the dialogue policy \cite{levinLearningDialogueStrategies1997,singhReinforcementLearningSpoken2000,gasic-etal-2010-gaussian}.
To address the challenges of large state-action spaces and low exploration efficiency when applying reinforcement learning to DPL, hierarchical reinforcement learning has been used to break down the complex task into several subtasks and learn separate policies for these subtasks \cite{budzianowski-etal-2017-sub,peng-etal-2017-composite,kristianto-etal-2018-autonomous,tang-etal-2018-subgoal}. 
Alternatively, Other researchers have adopted reward learning and reward shaping to provide denser rewards and foster faster learning \cite{su-etal-2015-reward,su-etal-2016-line,wang-etal-2020-learning-efficient}.
Our approach aligns with this line of work and incorporates reward shaping to refine the dialogue policy.
Recently, a few studies have started to apply multi-agent reinforcement learning for dialogue policy learning \cite{liuIterativePolicyLearning2017,zhang-etal-2020-learning}. \citet{takanobu-etal-2020-multi} propose a joint learning process for the dialogue system and the user agent, and introduced the Hybrid Value Network to enhance the reward for each role. \citet{wang-wong-2021-collaborative} further expand on this approach by partitioning the action space into three subspaces and training a separate agent for each using multi-agent reinforcement learning. These works frame model DPL as a classification problem where the policy chooses a suitable dialogue action from a predefined action list. Contrasting to these methodologies, our approach obviates the need for a predefined action list, generating the dialogue action instead.

\paragraph{Pre-trained Language Model}
Recent studies have demonstrated significant advancements in TOD by fine-tuning pre-trained language models \cite{budzianowski-vulic-2019-hello,hosseini-asl_simple_2020,lee-2021-improving-end,su-etal-2022-multi,zhao-etal-2022-unids}. These studies have effectively unified the learning process of various modules within the pipeline approach. \citet{peng-etal-2021-soloist} introduces SOLOIST, a model that leverages GPT-2 \cite{radford2019language}, a pre-trained auto-regressive transformer decoder model. This model generates the belief state, dialogue action, and response in a sequential manner. 
SOLOIST is first fine-tuned on a large amount of dialogue corpus with a multi-task objective, then fine-tuned on the target dataset. \citet{he2022galaxy} further extends this approach by incorporating unlabelled dialogue data through semi-supervised learning. A notable drawback of sequential subtask generation is the error accumulation from earlier modules to the later modules in the pipeline. To mitigate this, PPTOD first formulates the belief state, then concurrently generates the dialogue action and response, thus preventing error propagation.  
These approaches require a large dialogue corpus to further pre-training and fine-tuning of the pre-trained language model. In contrast, our work only requires on small amount of data to warm up the model to obtain the state-of-the-art results. Additionally, our work utilizes reinforcement learning to fine-tune the model, which does not require labelled data. 

\section{JOTR}
\begin{figure*}[htbp]
% \vspace{-0.3cm}  %调整图片与上文的垂直距离
% \setlength{\abovecaptionskip}{0.2cm}   %调整图片标题与图距离
\setlength{\belowcaptionskip}{-0.4cm}   %调整图片标题与下文距离
\centering
\includegraphics[width=1.8\columnwidth]{figure/figure2.png}
\caption{The joint transformer and reinforcement learning framework illustration consists of: 1) (Left Part) Text Encoding - The encoder processes user act, system act, belief state, and database query results to form the state. 2) (Right Part) Model Optimization - The state directs action generation, with the Action Interpreter generating structured dialogue actions. The transformer-based policy model undergoes interactive optimization through reinforcement learning from scratch.}
%Illustration of the joint transformer and reinforcement learning framework, including: 1) (Left Part) Text Encoding - Encoder processes user act, system act, belief state, and database query results to create the state. 2) (Right Part) Model Optimization - The state guides action generation, with Action Interpreter producing structured dialogue action. The transformer-based policy model undergoes interactive optimization via reinforcement learning from scratch.}
\label{fig:arch}
\end{figure*}

We utilize a transformer-based model to directly generate dialogue actions, distinguishing it from conventional approaches that rely on selecting dialogue actions from a predefined set of atomic dialogue actions and their combinations. The architecture of our model is illustrated in Figure \ref{fig:arch}. In this section, We first provide a formal definition for DPL, then present the overview of our approach.

\subsection{Problem Definition}
% The objective of Dialogue Policy Learning (DPL) is to formulate a policy enabling interaction with a user through the generation of dialogue actions, contingent upon the belief state and database results, to fulfill a user-defined goal, $G=(C, R)$. Here, $C$ refers to the user's constraints (for instance, a flight ticket to Seattle), while $R$ signifies the information the user seeks (such as the ticket price). As alluded to in the preceding section, the belief state maintains a record of the user's constraints throughout the dialogue. The belief state is encapsulated as a list of domain, slot, and value triplets, for example, \textit{[(flight, destination, Seattle), (flight, day, tomorrow)]}. Dialogue action, on the other hand, is represented as a list of domain, intent, slot, and value quadruplets, such as \textit{[(flight, request, time, ?)]}. An external database supplies pertinent entries to the dialogue policy in accordance with the belief state. Figure \ref{fig1:intro} provides an illustrative example of this process.
The goal of DPL is to learn a policy that interacts with a user through the generation of dialogue actions, given the belief state and the database results, to satisfy the user goal $G=(C, R)$, where $C$ denotes the user constraints (e.g. an air ticket to Seattle) and $R$ represents the information required by the user (e.g. the price of the air ticket).  As mentioned in the previous section, the belief state keep tracks of the user's constraints throughout the dialogue. 
The belief state is defined as a list of domain, slot, value triplets (e.g. \textit{[(flight, destination, Seattle), (flight, day, tomorrow)]}).
The dialogue action is represented as a list of domain, intent, slot, value quadruples (e.g. \textit{[(flight, request, time, ?)]}).
An external database provides relevant entries to the dialogue policy based on the belief state. Figure \ref{fig1:intro} provides an illustrative example of such a process.

\subsection{Dialogue State Text Encoding}
The encoder generates state embeddings $e_u, e_s, e_b, e_d \in \mathbb{R}^d$ by encoding the flattened textual representations of four elements: user action, system action, belief state, and the database result. 
These linearized textual representations are referred to as the dialogue state text.
% The user action is defined as a sequence of tokens represented as $A_1, A_2, \ldots, A_{N^u}$, with $N^u$ denoting the total number of tokens in the user action. This sequence is obtained by concatenating the dialog action triplets, which consists of the domain, intent, and slot. An example of such a sequence would be "hotel inform price hotel inform area". 
The user action is a sequence of tokens derived from atomic dialogue action triplets, each comprising the domain $D$, intent $I$, and slot $S$. This sequence is represented as $D_1, I_1, S_1, \ldots, D_{N^u}, I_{N^u}, S_{N^u}$, where $N^u$ denotes the number of atomic user dialogue actions. For instance, "\textit{hotel inform price hotel request name}" is a valid example.
The system action, similar to user action, is represented as $D_1, I_1, S_1, \ldots, D_{N^s}, I_{N^s}, S_{N^s}$, with $N^s$ indicating the number of atomic system dialogue actions.
The belief state is a sequence formed by concatenating the belief state triplets, where each triplet is composed of the domain $D$, slot $S$, and value $V$. This sequence can be represented as $D_1, S_1, V_1, \dots, D_{N^b}, S_{N^b}, V_{N^b}$. A typical example could be "\textit{hotel price expensive hotel area north}".
The database result is represented as $D_1, Q_1, \dots, D_{N^d}, Q_{N^d}$, with $N^d$ denoting for the number of queried domains and $Q$ denoting the number of matched entities in the database. An illustrative example would be “\textit{hotel 4 restaurant 2}”.

To obtain the state embeddings $e_u, e_s, e_b, e_d$, the [CLS] token, a common sentence representation placeholder, is prefixed to each dialogue state text \cite{devlin2018bert}. The output representation of the [CLS] token of each dialogue state text is used as the state embedding.
% Our preliminary experiments show that the model performs poorly when provided with only the state embeddings. The reason for this performance issue is likely the model's confusion regarding the different types of state information being encoded. 
Initial experiments revealed inferior performance if the model is only fed with the state embeddings, likely due to the model's confusion about the varying types of state information being encoded.
Therefore, a context embedding was constructed for each dialogue state text. The context embeddings are added with the state embeddings individually to produce the state $s \in \mathbb{R}^{4 \times d}$.

%As depicted in Figure \ref{fig:arch}, we begin by converting the dialog state into text and creating input embeddings. These are then passed through a transformer encoder to encode the state and context. The output of the encoder is then used by a transformer decoder to generate the final dialog action.
\subsection{Word-level Dialogue Policy}
%Specifically, we cast DPL as a word-level Markov Decision Process (MDP) problem. In each turn, the system agent observes a dialog state $s$ and then executes the action $a$ generated by the policy $\pi(a|s)$. Then the agent perceives the next user response, a reward $r$, and the updated state $s'$. This loop continues until the conversation terminates. Suppose the action $a$ is textualized as a word sequence $w_{1:N} = w_{1}..w_{N}$, and the encoded textualized state following the rules in the Figure \ref{fig:arch} is $s$, then the policy can be decomposed as the product of a series of conditional probabilities:
We have formulated the problem of dialogue policy learning (DPL) as a Markov Decision Process (MDP) on the word level. In this process, the system agent observes the current dialogue state $s$, executes an action $a$ (generated by the policy function), receives a response, a reward $r$, and the updated dialogue state $s'$. This cycle continues until the conversation ends. The action $a$ is textually represented as a sequence of words $w_{1:N} = w_{1}\ldots w_{N}$. The policy function can be detailed as a series of conditional probabilities:
% \setlength{\abovedisplayskip}{2pt}
% \setlength{\belowdisplayskip}{2pt}
% \setlength{\abovedisplayshortskip}{2pt}
% \setlength{\belowdisplayshortskip}{2pt}
% \begin{small}
\begin{equation}
\begin{aligned}
    % \setlength\abovedisplayskip{3pt}
    % \setlength\belowdisplayskip{3pt}
    \pi_{\theta}(a|s) = \prod_{i = 1}^{N} \mathcal{P}_{\theta}(w_i|w_{1:i-1}, s), 
    % \nonumber
    \label{e:2}
\end{aligned}
\end{equation}
where $\mathcal{P}$ is approximated with a transformer encoder-decoder network parameterized by $\theta$, representing the probability of the word $w_i$ condition on the preceding words and state. As shown in Figure \ref{fig:arch}, the transformer decoder generates the dialogue action text word by word, beginning with the start signal “[start]”, conditioned on the dialogue state, and proceeds until it encounters the stop signal “[end]”. 
Additionally, an action interpreter decodes the dialogue action text into structured format, populating slot values from the database, yielding the final dialogue action. 
This process involves verifying whether the dialogue action text adheres to the domain, intent, slot order, discarding any words that violate these conditions.
The policy $\pi$, parameterized by $\theta$, is optimized using reinforcement learning to minimize the negative expected cumulative future rewards: 
% $\mathcal{L}_{\theta} = -\mathbb{E}_{a_t \sim \pi_{\theta}(\cdot|s_t)}[\sum_{t=1}^T r(s_t, a_t)]$, 
\begin{equation}
\begin{aligned}
    % \setlength\abovedisplayskip{3pt}
    % \setlength\belowdisplayskip{3pt}
    \mathcal{L}_{\theta} = -\mathbb{E}_{a_t \sim \pi_{\theta}(\cdot|s_t)}\left[\sum_{t=1}^T r(s_t, a_t)\right],
    % \nonumber
    \label{e:2}
\end{aligned}
\end{equation}
where $s_t$ and $a_t$ are the state and dialogue action turn $t$, and $T$ is the maximum turn. In practice, the expected gradient for a dialogue session can be approximated by using a Monte Carlo sample from $\mathcal{P}_{\theta}$. For each session example, the gradient is approximated as:

% \begin{small}
\begin{equation}
\begin{aligned}
    % \setlength\abovedisplayskip{3pt}
    % \setlength\belowdisplayskip{3pt}
    &\nabla_{\theta}\mathcal{L}_{\theta} \approx -\sum_{t=1}^T r(s_t, a_t)\nabla_{\theta}\log \pi_{\theta}(a_t|s_t)
    = \\&-\sum_{t=1}^T r(s_t, w^t_{1:N^t}) \sum_{i=1}^{N^t} \nabla_{\theta} \log \mathcal{P}_{\theta}(w_i^t|w_{1:i-1}^t, s_t), 
    % \nonumber
    \label{e:3}
\end{aligned}
\end{equation}
% \end{small}
where $N^t$ is the length of the action text at turn $t$. 

\subsection{JoTR for Efficient Policy Training}
We employ Proximal Policy Optimization (PPO) \cite{schulman2017proximal} to optimize the policy. More specifically, we minimize the objective function for each session example.
% \cy{May add PPO equation here.}
% \begin{footnotesize}

% \resizebox{\hsize}{!}
% {
% \begin{footnotesize}
\begin{equation}
\begin{aligned}
    \mathcal{L}_\theta 
    &= \sum_{t=1}^T -\hat{\mathbb{E}}_{t}\left[\min \left(\frac{\pi_{\theta}\left(a_{t} \mid s_{t}\right)}{\pi_{\theta_{\text {old }}}\left(a_{t} \mid s_{t}\right)}  \hat{A}_{t}^\phi, \right . \right .\\ & \left . \left . \operatorname{clip}\left(\frac{\pi_{\theta}\left(a_{t} \mid s_{t}\right)}{\pi_{\theta_{\text {old }}}\left(a_{t} \mid s_{t}\right)}, 1-\epsilon, 1+\epsilon\right) 
    \hat{A}_{t}^\phi \right)\right]  \\
    &= \sum_{t=1}^T -\hat{\mathbb{E}}_{t}\left[\min \left(\frac{    \prod_{i = 1}^{N}\mathcal{P}_{\theta}(w_i|w_{1:i-1}, s_t)}{\prod_{i = 1}^{N}\mathcal{P}_{\theta_{\text{old}}}(w_i|w_{1:i-1}, s_t)}  \hat{A}_{t}^\phi, \right . \right .\\ & \left . \left . \operatorname{clip}\left(\frac{\prod_{i = 1}^{N}\mathcal{P}_{\theta}(w_i|w_{1:i-1}, s_t)}{\prod_{i = 1}^{N}\mathcal{P}_{\theta_{\text{old}}}(w_i|w_{1:i-1}, s_t)}, 1-\epsilon, 1+\epsilon\right) \hat{A}_{t}^\phi\right)\right], 
    % \nonumber 
\end{aligned}
\end{equation}
% \end{footnotesize}
%}
where $\hat{A}_{t}^\phi = r(s_t, a_t) + \gamma V^\phi(s_{t+1}) - V^\phi(s_t)$ is the advantage estimation and $V^\phi(s_t)$ is the value function estimated by the critic parameterized by $\phi$.
% % insert a PPO loss function

% To improve the efficiency and quality of the dialog response, we incorporate reward-shaping in RL finetuning. 
% The motivation is that if we optimize the policy with PPO using only the environment reward, the policy will often generate a lengthy but mostly useless dialog action.
% Therefore, we design the reward shaping function as:

% To enhance the efficiency and quality of dialogue responses, we incorporate reward shaping during reinforcement learning fine-tuning. The objective is to prevent the policy from generating lengthy yet mostly irrelevant dialogue actions when optimizing with PPO using only the environment reward.

% To achieve this, we design a reward-shaping function that assigns additional rewards or penalties based on the system's actions:

% If the system informs a slot present in the user's request slot list, it receives an additional +3 reward. Conversely, if it informs other slots, it receives an additional -1 reward.
% If the system requests a slot present in the user's inform slot list, it receives an additional +3 reward. However, if it requests other slots, it receives an additional -1 reward.

To improve the efficiency and quality of the dialog response, we integrate reward-shaping into the reinforcement learning fine-tuning process. 
The goal is to prevent the policy from generating protracted yet predominantly irrelevant dialogue actions during optimization with PPO. 
To achieve this, we propose a reward-shaping function assigning supplementary rewards to guide the model to learn.
Formally, we replace the $r(s_t, a_t)$ with $\hat{r}(s_t,a_t, G)$ defined as
\begin{equation}
\begin{aligned}
    \hat{r}(s_t,w_{1:N^t}^t, G) = r(s_t, a_t) + F(G, w_{1:N^t}^t),
\end{aligned}
\end{equation}
where $G$ denotes the user goal and $F$ represents the shaping reward. We design $F$ to provide different reward based on the follow: 
(1) If the system informs a slot present in the user's request slot list, it receives an additional $\lambda$ reward. Conversely, informing other slots receives an additional -1 reward. (2) If the system requests a slot included in the user's inform slot list, it receives an additional $\lambda$ reward. However, requesting other slots results in an additional -1 reward. $\lambda$ is a hyperparameter that controls the aggressiveness of the dialogue agent to inform or request additional slots. We try $\lambda$ with value 3, 4, 5, 6, 7. We find that the range of 3 to 5 yielded favourable results during validation. 
Higher $\lambda$ values encourage the dialogue agent to attempt many actions in a single turn, where one successful inform or request action offsets the negative rewards incurred by other irrelevant actions. We pick $\lambda=3$ for all the experiments.


\section{Experiments and Results}
Experiments are carried out on MultiWOZ 2.0 \cite{budzianowski2018multiwoz}, utilizing a publicly accessible agenda-based user simulator \cite{zhu2020convlab}, and on the SGD dataset with our developed rule-based simulator. Furthermore, we incorporate human evaluations, in which evaluators interact with various models and assess the success of the dialogue upon its completion. While all models, except SimpleTOD, are optimized in the dialogue action space, SimpleTOD takes the utterance dialogue history as input and generates both the dialogue action and the system utterance.


\section{Evaluation Metrics}
\label{sec:appendix_simulator}
We focus on three evaluation metrics to align with previous work \cite{wang-wong-2021-collaborative}: success rate, average number of turns, and average reward. A dialogue session is considered successful if it fulfils all the user requests, and reserves an entity that meets the user's specifications if necessary. 
The average number of turns is calculated by counting the number of interactions between the two parties, with each full interaction counted as two turns. 
The average reward is the total cumulative reward obtained in each dialogue session. 
Additionally, since SimpleTOD uses dialogue utterances as input, an NLG component is needed to convert the user's dialogue actions into utterances. However, the performance of SimpleTOD is significantly dependent on the NLG component. Therefore, we also evaluate it against a testing corpus, which we believe is a superior method than using a user simulator equipped with NLU and NLG modules that could potentially introduce noise.

\begin{figure*}
\small
\centering
\includegraphics[width=0.45\textwidth]{figure/figure_2_1.png}
\centering
\includegraphics[width=0.45\textwidth]{figure/figure2_2.png}
\caption{The learning curve of various models on MultiWOZ and SGD, with the mean and standard deviation illustrated over 5 runs. We didn't reach JOIE's 400k frames, as JoTR attains a 0.93 success rate with only 50k frames, surpassing JOIE, and training 400k frames for its variants is costly.}
\label{fig:learning_curve}
\vspace{-3mm}
\end{figure*}

\section{Training and Implementation Details}
\label{sec:implementation}

% We implemented all models using PyTorch \cite{paszke2019pytorch} and Transformers \cite{wolf-etal-2020-transformers}, with a randomly initialized transformer encoder for JoTR's dialogue state text encoder. JoTR$_\text{pretrained}$ uses DistilBert \cite{sanh2019distilbert} with pretrained Huggingface weights. The encoder-decoder model uses a transformer with identical parameters for both parts. All JoTR variants and MLP$_\text{ppo}$ are pre-trained on a training set, consistent with prior work . We used the same 10,000 dialogue turns for model warm-up and a separate 3,000 turns for validation. Identical hyperparameters were used for MultiwWOZ and SGD in both pretraining and PPO fine-tuning. Training included an early stopping mechanism and was conducted on a single RTX 3090 GPU.

We implement all the models in PyTorch \cite{paszke2019pytorch} and Transformers \cite{wolf-etal-2020-transformers}. 
We use a randomly initialized transformer encoder with 1 hidden layer, 1 head, and hidden size of 256 as the dialogue state text encoder of JoTR. 
In JoTR$_\text{pretrained}$, we use DistilBert \cite{sanh2019distilbert} with pretrained weights from Huggingface as the initial weights for the dialogue state text encoder.
For the encoder-decoder model, we use a transformer with 1 hidden layer, 1 attention head, and hidden size of 256 for both the encoder and decoder. The total number of parameters is 5M. All variants of JoTR and MLP$_\text{ppo}$ are warmed up before reinforcement learning by first performing supervised learning on a training set to be consistent with previous work \cite{wang-wong-2021-collaborative}. We use the same set of 10K dialogue turns sampled randomly from the original training set to warm up all models. A separate, non-overlapping set of 3K dialogue turns is used for validation in the warm-up phase.  The same set of hyperparameters are used on MultiwWOZ and SGD in both pretraining and PPO fine-tuning. For the warm-up phase, the models are trained using a batch size of 32 and a learning rate of $3 \times 10^{-4}$. The models are trained for 80 epochs but we include an early stopping mechanism that halted training when no improvement is observed in the validation set over five consecutive epochs. In PPO training, we use an actor learning rate of $5 \times 10^{-7}$ and critic learning rate of $1 \times 10^{-4}$. The critic is a transformer with identical architecture as the encoder-decoder model.
The models are trained with a total of 50K frames. 
All models are trained on a single RTX 3090 GPU, which take about 10 hours to train.
The maximum interaction turn allowed is 40. The main reward provided by the environment is -1 in every turn, and a reward of 80 or -40 at the end for successful or failed dialogue respectively. 

The prompt for ChatGPT is shown in Table 1. Following previous work \cite{wang-etal-2022-super}, we provide one formatting example in the zero-shot setting. In our preliminary experiments, we find that the task definition and the output specification significantly influence ChatGPT's ability to understand the task and generate a structured output. 

\begin{table}[htbp]
    \centering
    \tiny
    \begin{tabular}{p{.95\linewidth}}
    \toprule
     \textbf{Prompt:}
     \\
     \textcolor{mypurple1}{You are a dialogue agent to assist me with my queries and provide me with relevant information from a database. My questions are formatted as tuples of (domain, intent, slot, slot value) accompanied by the number of matching results that satisfy my constraint from the database, e.g., "4 matches".} 
    \\ \textcolor{myyellow1}{Your responses should be formatted as one or several tuples of (domain, intent, slot) to provide me with the necessary information.} 
    \\ \textcolor{myyellow1}{The domain is selected from attraction, hospital, ....} 
    \\ \textcolor{myyellow1}{The intent is selected from inform, request, ....} 
    \\ \textcolor{myyellow1}{The slot includes addr: the address of the hotel, attraction, restaurant, hospital, or police station....}   
    \\ \textcolor{myblue1}{Example 1:} 
    \\ \textcolor{myblue1}{USER: [(train, inform, depart, london kings cross)] 3 matches.} 
    \\ \textcolor{myblue1}{ASSISTANT: [(train, inform, id)]} 
    \\ ... 
    \\ \textcolor{myred1}{Example 2:} 
    \\ \textcolor{myred1}{USER: [(restaurant, inform, area, centre)]}... 
    \\ \hline
       \textbf{Output:}  \\
       ASSISTANT: [(restaurant, request, day), (restaurant, request, time), (restaurant, request, people)]\\
    \bottomrule
    \end{tabular}
    \caption{A zero-shot ChatGPT prompt example for dialogue act prediction. It consists of \textcolor{mypurple1}{task definition}, \textcolor{myyellow1}{output specification}, \textcolor{myblue1}{formatting example} and \textcolor{myred1}{dialogue history}. }
    \label{tab:chatgpt_prompt}
\end{table}

% \begin{table}[H]
%     \centering
%     \begin{tabular}{c|p{.75\linewidth}}
%     \toprule
%      Prompt    & \textcolor{mypurple1}{You are a dialogue agent to assist me with my queries and provide me with relevant information from a database. My questions are formatted as tuples of (domain, intent, slot, slot value) accompanied by the number of matching results that satisfy my constraint from the database, e.g., "4 matches".} \\
%     & \textcolor{myyellow1}{Your responses should be formatted as one or several tuples of (domain, intent, slot) to provide me with the necessary information.} \\
%     & \textcolor{myyellow1}{The domain is selected from attraction, hospital, ....} \\
%     & \textcolor{myyellow1}{The intent is selected from inform, request, ....} \\
%     & \textcolor{myyellow1}{The slot includes addr: the address of the hotel, attraction, restaurant, hospital, or police station....}   \\
%     & \textcolor{myblue1}{Example 1:} \\
%     & \textcolor{myblue1}{USER: [(train, inform, depart, london kings cross)] 3 matches.} \\
%     & \textcolor{myblue1}{ASSISTANT: [(train, inform, id)]} \\
%     & ... \\
%     & \textcolor{myred1}{Example 2:} \\
%     & \textcolor{myred1}{USER: [(restaurant, inform, area, centre)]}... \\
%      \hline
%        Output  & ASSISTANT: [(restaurant, request, day), (restaurant, request, time), (restaurant, request, people)]\\
%     \bottomrule
%     \end{tabular}
%     \caption{A zero-shot ChatGPT prompt example for dialogue act prediction. It consists of \textcolor{mypurple1}{task definition}, \textcolor{myyellow1}{output specification}, \textcolor{myblue1}{formatting example} and \textcolor{myred1}{dialogue history}. }
%     \label{tab:chatgpt_prompt}
% \end{table}
% The maximum interaction turn allowed is 40. The main reward provided by the environment is -1 in every turn, and a reward of 80 or -40 at the end for successful or failed dialogue respectively. 

\section{Simulator Details}
We implement an agenda-based simulator for SGD, following the prevalent approach for user simulator design as outlined in previous works \cite{schatzmann_agenda-based_2007,wang-wong-2021-collaborative, kwan2023survey}. The agenda-based simulator samples a user goal according to the distribution of slots in the training set of SGD. The user goal is kept unknown to the dialogue agent.
It maintains a stack (i.e. user agenda) that stores all the user actions that the user needs to inform perform to achieve his/her goal during the conversation. It acts to the system's actions according to some pre-fined rules.

\subsection{Baseline Agents}
We compare our model, JoTR, to five other models. 1) \textbf{JOIE} \citep{wang-wong-2021-collaborative}, the current state-of-art(SOTA) on MultiWOZ, is a collaborative multi-agent model that generates a single dialogue action without a predefined action list. 2) \textbf{MLP$_{\text{ppo}}$}, an agent optimized with PPO with fixed action candidates. 3) \textbf{SimpleTOD} \citep{hosseini-asl_simple_2020}, a GPT-2-based agent trained with supervised learning to generate dialogue actions along with belief states and responses based on the dialogue history. 4) \textbf{DASP} \citep{jhunjhunwala-etal-2020-multi}, an LSTM-based agent that trained with human supervision to select among N-best action candidates based on the dialogue history. 5) \textbf{ChatGPT} is a large language model developed on the foundation of InstructGPT \cite{ouyang2022training}. It functions as an efficient conversational agent, capable of interpreting user prompts and producing logically consistent replies. ChatGPT has demonstrated impressive results across a diverse range of natural language processing assignments. We use it to generate dialogue actions based on the dialogue action history using a zero-shot prompt as shown in Table \ref{tab:chatgpt_prompt}. 

% \begin{itemize}
%     \item \textbf{JOIE} \citep{wang-wong-2021-collaborative}, the current state-of-art(SOTA) on MultiWOZ, is a collaborative multi-agent model that generates a single dialogue action without a predefined action list.
%     \item \textbf{MLP$_{\text{ppo}}$}, an agent optimized with PPO with fixed action candidates.
%     \item \textbf{SimpleTOD} \citep{hosseini-asl_simple_2020}, a GPT-2-based agent trained with supervised learning to generate dialogue actions along with belief states and responses based on the dialogue history. 
%     \item \textbf{DASP} \citep{jhunjhunwala-etal-2020-multi}, an LSTM-based agent that trained with human supervision to select among N-best action candidates based on the dialogue history. 
%     \item \textbf{ChatGPT} is a large language model developed on the foundation of InstructGPT \cite{ouyang2022training}. It functions as an efficient conversational agent, capable of interpreting user prompts and producing logically consistent replies. ChatGPT has demonstrated impressive results across a diverse range of natural language processing assignments. We use it to generate dialogue actions based on the dialogue action history using a zero-shot prompt as shown in Table \ref{tab:chatgpt_prompt}. 
% \end{itemize}
\begin{figure*}[htbp]
\centering
\includegraphics[width=0.99\textwidth]{figure/figure_case3.png}
\caption{An illustrative dialogue example featuring responses from various models is provided. The system's dialogue actions (text on the right) highlighted in \textcolor{yellow}{yellow} emphasize JoTR's capability to manage complex and out-of-domain user actions, which other models struggle to handle. The dialogue actions in \textcolor{pink}{pink} showcase JoTR's proficiency in preemptively informing relevant slots, an area where other models fall short.
% A dialog example with responses from different models. The system's dialog actions (text on the right) in \textcolor{yellow}{yellow} highlight JoTR's ability to handle complex and out-of-domain user actions while others can't.  The dialog actions in \textcolor{pink}{pink} demonstrate JoTR is able to inform relevant slots preemptively where other models failed to do so.
\vspace{-0.5em} and}
\label{fig:case}
\end{figure*} 

To further demonstrate the advantages of our model, JoTR, we also compare it to three variants. \textbf{JoTR$_{\text{w/o rs}}$} does not use reward shaping. \textbf{JoTR$_{\text{w/o ppo}}$} is only pretrained with supervised learning and not further fine-tuned with PPO. \textbf{JoTR$_{\text{pretrained}}$} uses a pre-trained BERT as the context encoder but with the weights fixed.

% \begin{itemize}
%     \item \textbf{JoTR$_{\text{w/o rs}}$} does not use reward shaping.
%     \item \textbf{JoTR$_{\text{w/o ppo}}$} is only pretrained with supervised learning and not further fine-tuned with PPO.
%     \item \textbf{JoTR$_{\text{pretrained}}$} uses a pre-trained BERT as the context encoder but with the weights fixed.
% \end{itemize}

\subsection{Main Results}
\begin{table}[]
    \centering
    \renewcommand{\arraystretch}{0.9}
    \small
    \setlength{\tabcolsep}{0.4mm}{
    \begin{tabular}{l lll lll }
        \toprule
         \multirow{2}{*}{Model} &  \multicolumn{3}{c}{MultiWOZ} & \multicolumn{3}{c}{SGD} \\
         \cmidrule(lr){2-4}\cmidrule(lr){5-7}& Succ.$\uparrow$ & Turn$\downarrow$ & Rew.$\uparrow$ & Succ.$\uparrow$ & Turn$\downarrow$ & Rew.$\uparrow$ \\
         \midrule 
         JOIE$\dagger$ & 0.91 & 18.90$^*$ & 40.82 &0.51 &11.10$^{*}$ &15.32 \\
         MLP$_{\text{ppo}}$ & 0.56 & 30.72 & -26.76 & 0.54 & 23.43 & 16.50 \\
         SimpleTOD$\ddagger$ & 0.62 & - & - & 0.50 & - & - \\
         DASP$\dagger,\ddagger$ & 0.85 & - & - &0.70 & - & - \\
         ChatGPT & 0.73 & 13.10 & 41.05 & 0.50 & \textbf{11.04} & 15.48 \\
         \midrule 
         JoTR & \textbf{0.93} & \textbf{9.94} & \textbf{68.46} & \textbf{0.79} & 15.23 & \textbf{49.25} \\
         JoTR$_{\text{w/o rs}}$ & 0.89 & 9.95 & 66.42 & 0.72 & 16.53 & 38.84 \\ 
         JoTR$_{\text{w/o ppo}}$ & 0.67 & 18.44 & 32.18 & 0.55 & 24.76 & 14.62 \\
         JoTR$_{\text{pretrained}}$ &0.76 & 14.19 & 44.87 & 0.64 & 19.25 & 28.18 \\
         \bottomrule
    \end{tabular}
    \caption{
    % The performance of dialog act modelling. Succ. and Rew. denote the success rate and reward respectively. 
    % $\dagger$: The results are cited from the corresponding original papers. We adopt the code of SimpleTOD released by the author to evaluate on MultiWOZ and SGD.\footnotemark DASP is trained only in restaurant and hotel domains on MultiWOZ in the original paper. 
    % $\ddagger$: These models are evaluated against a testing corpus. \textbf{Bold} indicates the best result of each metric.$*$: A pair of user utterance and system response is counted as 2 turns in our settings, while in the JOIE paper, it is 1 turn.
    Dialogue act modeling performance is measured by success rate (Succ.), turn, and reward (Rew.) All the RL-based agents use the same reward values and assignments during testing. Results with $\dagger$ are from original papers. We reproduce JOIE to obtain the performance in SGD. Models with $\ddagger$ are tested against a corpus. The best results are in bold. *In our settings, a user-system utterance pair counts as 2 turns, while in the JOIE paper, it's 1 turn.
    }
    \vspace{-3mm}
    \label{tab:results}
    }
\end{table}

\footnotetext{https://github.com/salesforce/simpletod}
The learning process demonstrated in Figure \ref{fig:learning_curve} showcases the superior performance and efficiency of the proposed JoTR model. 
Notably, JoTR outperforms the previous SOTA model JOIE (0.93 vs 0.91 for MultiWOZ and 0.79 vs 0.51 for SGD) despite only trained with 50K frames, compared to JOIE's 400K frames \cite{wang-wong-2021-collaborative}. Moreover, the ability of JoTR to improve significantly with such short training makes it more suitable for real-world applications. Note that we implement JOIE and obtain the result in SGD with 50K frames while the result in MultiWOZ is from the original paper. 

Table \ref{tab:results} shows that JoTR requires significantly fewer turns than JOIE to satisfy the user goal. 
This efficiency can be attributed to JoTR's capacity to generate multiple atomic actions in one turn, in contrast to JOIE's single action prediction. This characteristic not only reduces the total number of interaction turns but also enhances JoTR's practicality for everyday use. All JoTR variants outperform MLP$_{\text{ppo}}$. Notably, JoTR$_\text{w/o ppo}$ surpasses MLP$_{\text{ppo}}$ without additional RL fine-tuning, indicating that the strong learning capacity of transformer is effectively harnessed for learning the specific structural properties of dialogue actions. In comparison to SimpleTOD, JoTR performs better due to its use of dialogue actions as input, which reduces noise and complexity compared to SimpleTOD's language utterances. 
Furthermore, JoTR's encoder-decoder model structure aids in capturing context information more effectively than SimpleTOD's decoder-only model. Lastly, JoTR performs significantly better than ChatGPT. Most errors made by ChatGPT can be categorized into two categories: 1. hallucination on domain, slot and values. 2. Violations of output format constraints. 

\subsection{Ablation Study}
\subsubsection{The Effectiveness of Reward Shaping}
% \noindent \textbf{The effectiveness of reward shaping} \quad  
The success rate increase from 0.89 to 0.93 in MultiWoz and 0.72 to 0.79 in SGD when reward shaping is applied. It manifests that reward shaping plays a big role in achieving high success rate. Furthermore, Figure \ref{fig:learning_curve} demonstrates that JoTR consistently maintains a higher success rate throughout the fine-tuning process.  
This observation highlights the advantage of using a dense and well-designed reward in RL fine-tuning, corroborating previous findings \cite{wang2022integrating}.

\subsubsection{The Necessity of RL Fine-Tuning}  JoTR$_\text{w/o ppo}$ underperforms significantly without continuously being optimized with RL, exhibiting a reduction of as much as 28\% in the success rate on MultiWOZ. This underlines the critical role of RL fine-tuning in refining the behavior of the policy model through the reward.

% \noindent \textbf{Pretrained Model as a Starting Point} \quad JoTR significantly outperforms JoTR$\text{pretrained}$ (0.93 vs 0.76 in Multiwoz, 0.79 vs 0.64 in SGD). Figure \ref{fig:learning_curve} illustrates that JoTR$\text{pretrained}$ exhibits a notably lower success rate initially. This can be attributed to the distinct structure of the dialog actions' input space, which markedly differs from the natural language space where the model was originally pretrained. This discrepancy cannot be bridged effectively by supervised training during the warm-up phase or reinforcement learning in the fine-tuning phase.
\subsubsection{The Importance of Training from Scratch} JoTR significantly outperforms JoTR$_\text{pretrained}$ (0.93 vs 0.76 in Multiwoz, 0.79 vs 0.64 in SGD). 
As evidenced by Figure \ref{fig:learning_curve}, JoTR$_\text{pretrained}$ exhibits a notably lower success rate initially. This can be attributed to the distinct structure of the dialogue actions' input space, which markedly differs from the natural language space where the model was originally pretrained. 
This discrepancy cannot be bridged effectively by supervised training during the warm-up phase or reinforcement learning in the fine-tuning phase.



\subsection{Case Study} \label{sec:case_study}
% As demonstrated in Figure \ref{fig:case_extract}, we observe that when the user requests a slot combination that is rarely found in the training set, JoTR is able to inform all the requested slots, which proves its ability to generate effective and efficient dialog act. JOIE only reply with one requested slot since only one act is allowed in its setting. Also, MLP$_{\text{ppo}}$ is not able to inform the complete slots since such action is not included in the predefined action set. Besides, ChatGPT also failed in this case with a non proper response. 

\begin{figure}
    \centering
    \includegraphics[width=0.47\textwidth]{figure/figure5.png}
    \caption{An example user utterance accompanied by the language responses corresponding to the dialogue actions predicted by various agents. We used the rule-based NLG component in ConvLab2\footnotemark to generate the language response given the predicted dialogue action.}
    \vspace{-1.5em}
    \label{fig:case_extract}
    
\end{figure}

% The findings depicted in Figure \ref{fig:case_extract} reveal that, in cases where the user requests a slot combination that is either infrequently encountered or entirely absent from the training set, both JoTR and JoTR$\text{w/o rs}$ are able to successfully inform all requested slots. This demonstrates their robust capability to generate effective and efficient dialog acts. In contrast, JoTR$\text{w/o ppo}$, JoTR$\text{w/o pretrained}$, and SimpleTOD were unable to inform all requested slots, potentially a reflection of their lesser performance relative to JoTR. JOIE managed to inform only one requested slot, likely due to its design limitation of generating a single action per turn. Furthermore, MLP${\text{ppo}}$ could not provide full slot information since the dialog action for informing address, postcode, and phone number is not featured within its predefined action set. Lastly, ChatGPT responded inappropriately, for reasons elaborated in the preceding section.

% As depicted in Figure \ref{fig:case_extract}, we note that both JoTR and JoTR$\text{w/o rs}$ can successfully relay all requested slots, even when the user asks for a slot combination not previously seen in the training set. This capability underscores their robust efficacy in generating constructive and efficient dialogue acts. Conversely, models such as JoTR$\text{w/o ppo}$, JoTR$\text{w/o pretrained}$, and SimpleTOD fail to inform all requested slots, suggesting a subpar performance compared to JoTR. JOIE could only inform one requested slot, likely attributable to its design limitation of generating a single action per turn. MLP${\text{ppo}}$ also struggled to relay all slots, as its predefined action set does not include dialogue actions for informing address, postcode, or phone number. Lastly, ChatGPT responded inappropriately, a point expanded upon in the previous section.

% In Figure \ref{fig:case}, we provide a comprehensive dialogue example with various models interacting with a user simulator. In the second turn, the user requested the address, postcode, and phone of a park—a slot combination not found in any training example, which serves to highlight JoTR's strength. In alignment with Figure \ref{fig:case_extract}J, JoTR and JoTR${\text{w/o rs}}$ were successful in informing all three slots. Notably, when the user requested a guesthouse in turn three, JoTR could provide the phone number without explicit prompting, whereas JoTR${\text{w/o rs}}$ and other models could not. This suggests that rewarding shaping can incentivize the model to preemptively provide additional information. We also find that the dialogues of other models are significantly longer than those of JoTR.

As demonstrated in Figure \ref{fig:case_extract}, we observe that when the user requests a slot combination that is never seen in the training set, both JoTR and JoTR$_\text{w/o rs}$ are able to successfully informed all requested slots. This demonstrates their robust ability to generate effective and efficient dialogue actions. 
In contrast, JoTR$_\text{w/o ppo}$, JoTR$_\text{w/o pretrained}$, and SimpleTOD were unable to inform all requested slots, potentially a reflection of their inferior performance relative to JoTR.
JOIE only informed one requested slot, likely due to its design limitation of generating a single action per turn. 
Moreover, MLP$_{\text{ppo}}$ could not inform the complete slots as well, since the dialogue action for informing address, postcode, and phone number is not found within its predefined action set.
Lastly, ChatGPT responded inappropriately, for reasons elaborated in the preceding section.  

We also provide a full dialogue example of various models interacting with the user simulator in Figure \ref{fig:case}. The user requested for the address, postcode and phone of the park in the second turn. There is not a single training example that request these three slots simultaneously, which serves to showcase ability of different models to respond with complex and out-of-domain user actions. In consistent with Figure \ref{fig:case_extract}, JoTR and JoTR$_{\text{w/o rs}}$ were able to inform all three slots while other models can't (highlighted in yellow) Furthermore, when the user requested for a guesthouse in turn three, JoTR is able to provide the phone number without being explicitly asked while JoTR$_{\text{w/o rs}}$ and other models failed to do so as (highlighted in pink). This illustrates the rewarding shaping can incentivize the model to provide additional information preemptively. In this example, we can also see the dialogues of other models are significantly longer than those of JoTR. Therefore, JoTR is able to achieve the user's goal efficiently.



\subsection{Human Evaluation}
\label{sec:human-eval}
\begin{table}[htbp] 
\vspace{-3mm}
% \vspace{-0.3cm}  %调整图片与上文的垂直距离
% \setlength{\abovecaptionskip}{0.2cm}   %调整图片标题与图距离
\setlength{\belowcaptionskip}{-0.4cm}   %调整图片标题与下文距离
\centering
\small
\setlength{\tabcolsep}{1.8mm}{
\begin{tabular}{l c c}
\toprule 
Model&Succ.(MultiWOZ)$\uparrow$ &Succ.(SGD)$\uparrow$ \\
\midrule 
JOIE                        &0.56 &0.53   \\
MLP$_{\text{ppo}}$          &0.52 &0.56 \\
SimpleTOD                   &0.62 &0.50 \\
DASP                        & -   & -  \\
ChatGPT                     &0.66 &0.52 \\
\midrule 
JoTR                        &\textbf{0.92} &\textbf{0.76}\\
JoTR$_{\text{w/o rs}}$      &0.84 &0.70\\
JoTR$_{\text{w/o ppo}}$     &0.66 &0.56\\
JoTR$_{\text{pretrained}}$  &0.68 &0.60\\
\bottomrule
\end{tabular}}
\vspace{-0.5em}
\caption{\label{tab:human-evaluation} Human evaluation results. We use the models trained with 50K frames for all agents. }
\end{table}

\footnotetext{https://github.com/thu-coai/ConvLab-2}
We further conduct a human evaluation to validate the simulation results using the models trained with 50K frames. 
We recruit 3 volunteer student helpers as evaluators to interact with different models.  
For each model, we held 50 dialogue sessions.
In each session, an evaluator is assigned a randomly selected model and user goal.
The evaluators are instructed to interact with the model in accordance with the user goal, with a maximum of 20 turns per session, aligning with the settings used in the experiments in previous sections.
At the end of each session, the evaluators assessed the success or failure of the dialogue. 
The results are illustrated in Table \ref{tab:human-evaluation}, which are consistent with the previous results using a user simulator.


\section{Conclusion}
We introduced JoTR, a versatile framework for dialogue policy learning using joint text-to-text transformer reinforcement learning. It trains word-level policies that can generate dialogue actions without the need for predefined templates. Empirical results from two benchmark datasets show that our model, which does not rely on predefined action templates, outperforms the strongest baseline in terms of both policy learning efficiency and dialogue action quality as determined by simulated and human evaluations.



% \section{Acknowledgments}


\bigskip

\bibliography{aaai24,anthology}

\end{document}