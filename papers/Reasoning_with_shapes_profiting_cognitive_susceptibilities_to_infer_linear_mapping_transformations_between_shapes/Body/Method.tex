\section{Method}
\paragraph*{Basic manipulations vs. complex calculations}
An infant has intuitive understanding of numbers and shapes, and can distinguish numerical and identity invariance of objects~\cite{izard2008distinct} regardless of object domain~\cite{wynn2002enumeration}; an ability that surprisingly extends to non-object entities (e.g., action~\cite{sharon1998individuation}). It lets us argue that an infant has basic understanding of transformations by primary perception of numbers and shapes. This intuitive ability is based on an early development of \textit{approximate number system}. This ability encourages present study to concentrate on basic operations and visual properties of shapes for the linear mapping transformation determination task.

\paragraph*{Abstract vs. detailed representations}
Of the entire environment within our visual range, only the essential information for the action in progress is prominent and the rest of the details are ignored~\cite{intraub1997representation} (aka cognitive inhibition~\cite{macleod2007concept}).  For instance, while crossing a street only the information about the direction and speed of cars on the street are required; details such as plate number of the cars or clothes drivers wore, generally not consciously registered in the visual perception. This highlights the significant role that abstractions play in reducing the amount of information to be considered. Additionally, Ballard~\cite{ballard1997deictic} and Agre~\cite{agre1987pengi} further explained this ability as deictic strategies where eye fixation point is used to guide body movement (modeling the behavior at the embodiment level) while fixation point can rapidly change to different location~\cite{ballard1991animate}. 

In the following, we discuss how the proposed method abstracts images and determines linear mapping transformations between the images. 


\subsection{Shape representation} \label{section: Shapre Representation}
The overall procedure of the presented method is independent from the color model of input shape (i.e., RGB, Cyan Magenta Yellow Key (CMYK), Hue Saturation Value (HSV), B\&W, binary, and etc.).%, except at abstraction procedure (see Section~\ref{section: Shape Segmentation}) where color-model-specific aggregation function abstracts a segment of the source shape.
Present study manipulates binary representation of shapes; while extension to other color models is straightforward and requires the modification of segment aggregation function (discussed in Section~\ref{section: Shape Segmentation}). The motivations of binarizing shapes are threefold. First, simple aggregation functions such as count can be applied on binary shapes. This improves the readability of the presented method, and avoids various color-model-based aggregation functions, which are beyond the scope of this manuscript.

Second, real world objects incorporate spatial parameters and materials, introducing distal-to-proximal stimulus mapping challenges, and motion correspondence problem. The objectives of present study are to methodize principle cognitive susceptibilities for transformation determination, and the fact that binary shapes are not as sensible to aforementioned challenges as colorful shapes are, makes the binary model suitable for present study. The binary color model, is a common model among the motion correspondence problem studies (e.g.,~\cite{girod2013principles,hirschmuller2009evaluation}.

Third, a \textit{distance transform} and topological skeleton extraction from binary shapes is straightforward as opposed to colorful shapes. These transformations are applicable alternatives for the segment aggregation functions discussed in Section~\ref{section: Shape Segmentation}. Despite of the promising methods that exist in literature (e.g., topological volume skeletonization~\cite{takahashi2004topological}, or various methods on distance transform algorithms~\cite{fabbri20082d}), to best of our knowledge, none of the proposed methods are comprehensive in the consideration of full explanatory real world object characteristics such as material, illumination, and surface reflectance. For instance, the pattern in a chro\-mium plated sphere in an image frame is indeed reflecting the surrounding environment and the sphere itself is determined implicitly~\cite{adelson2001seeing}. However, binary shapes mask similar properties encouraging least ambiguity.

Present study manipulates binary shapes. In this regard, first a colored shape is converted to its corresponding gray-scale B\&W frame. The procedure is by estimating the luminance for every pixel $x$, $y$ of an image frame in RGB color model (e.g., panel A on Fig.~\ref{Figure: ShapeRepresentation}) as $L_{xy} = 0.2126R + 0.7152G + 0.0722B$ (see panel B on Fig.~\ref{Figure: ShapeRepresentation}). The resulted B\&W image frame is then binarized by normalizing $L_{xy}$ as $B_{xy} = \lfloor L_{xy} / 128 \rfloor$ for the binary pixel $B_{xy}$ (see panel C on Fig.~\ref{Figure: ShapeRepresentation}). Note that, $L_{xy} \in \{0, 1, \dots 255\}$, therefore, $B_{xy} \in \{0, 1\}$.

\begin{figure}[!t]
	\centering
	\includegraphics[width=0.95\columnwidth]{Figures/ShapeRepresentation.pdf}
	\caption
	{
		\textbf{A:} input shape in RGB color model; \textbf{B:} intermediate gray-scale representation of the input; \textbf{C:} binary representation of the input. The proposed method manipulates the binary representation.
	}
	\label{Figure: ShapeRepresentation}
\end{figure}


\subsection{Shape segmentation} \label{section: Shape Segmentation}
Shape segmentation is a well-studied subject in the field of image processing (e.g.,~\cite{pal1993review}). Segmentation commonly proceeds shape semantic analysis, thus highlighting the necessity of adapting segmentation method to the objectives of the study. Accordingly, the segmentation procedure is defined as it follows, which emphasizes the relative location of the pixels to facilitate liner mapping transformation.

In present study, shapes are considered in two Dimensional (2D) Euclidean space. An image frame is segmented in $N$ \textit{sectors} and $M$ \textit{segments} (see Fig.~\ref{Figure: ShapeSegmentation}). Sectors are divisions of the image frame in \textit{angle} ($\varphi$) direction of polar coordinate system and are denoted by Euclidean unit vector $\vec{V}_n$ for $n\in\{1,2, \dots N\}$. Segments are isometric divisions of sectors in \textit{radius} ($r$) direction of polar coordinate system, denoted by the Euclidean unit vector $\vec{V}_{nm}$ for $m \in \{1,2, \dots M\}$. Accordingly, all sectors have equal number of segments, and segments are the smallest segmentation units. Let $I$ denote \textit{segmentation matrix} defined as it follows.
\[
\mathbf{I} =
\bordermatrix{ & \text{Segment} \; 1             & \dots & \text{Segment} \; M  \cr
	  \text{Sector} \; 1 & \vec{V}_{11} & \dots  & \vec{V}_{1M}       \cr
	     \dots & \vdots       & \ddots & \vdots             \cr
	  \text{Sector} \; N & \vec{V}_{N1} & \dots  & \vec{V}_{NM}}      \qquad
\]
\noindent
Each element $V_{nm}$ is a tuple of $\langle x, y, \gamma \rangle$, where $\gamma$ is an aggregated value of a portion of the image frame which is represented by $V_{nm}$. Note that, the dimension of segmentation matrix is independent from the resolution of input image frame. 

The area represented by a segment $\vec{V}_{nm}$ is characterized by two boundaries environing it, and it is defined in polar coordinate system as it follows.
\begin{align} 
r \in & \left] \frac{m-1}{M}       , \frac{m}{M}    \right] \\ 
\varphi \in & \left] \frac{360 (n-1)}{N} , \frac{360n}{N} \right]
\end{align}

A pixel at $r'$, $\varphi'$ Polar coordinate is a member of $\vec{V}_{nm}$ segment if and only if the coordinates of the pixel fall in the boundaries of the segment. Accordingly, the membership of a pixel at $x$, $y$ Cartesian coordinate to $\vec{V}_{nm}$ segment depends on the following condition.
\begin{align} 
\tan^{-1}(\frac{x}{y}) \in & \left] \frac{360(n-1)}{N}, \frac{360n}{N} \right] \\
\vert \sqrt{x^2 + y^2} \vert \in & \left] \frac{m-1}{M}, \frac{m}{M} \right]
\end{align}



\begin{figure}[!t]
	\centering
	\includegraphics[width=0.7\columnwidth]{Figures/ShapeSegmentation.pdf}
	\caption
	{
		This shape illustrates segmentation with $N=4$ sectors and $M=3$ segments. A segment represents an area of the frame. For instance, $\vec{V}_{1,1}$, $\vec{V}_{1,2}$, and $\vec{V}_{1,3}$ respectively represent the areas shaded in yellow, blue, and cyan. Each area is composed of a set of pixels aggregated in $\gamma_{nm}$. For instance, $\gamma_{1,1}$, $\gamma_{1,2}$, and $\gamma_{1,3}$ denote the aggregation of pixels located at yellow, green, and blue shaded areas respectively.
	}
	\label{Figure: ShapeSegmentation}
\end{figure}

\subsection{Shape abstraction}
A shape is abstracted by aggregating pixels in the area of each segment. The binary representation of a shape enables the use of simple \textit{count} aggregate function that is the number of pixels represented by the segment with the value of $1$. Let $\gamma_{nm}$ denote the aggregated value of segment $\vec{V}_{nm}$ which is defined as $S_{nm} = \vert \{B_{xy} \vert B_{xy} = 1 \}  \vert$ for all $x$ and $y$ of pixels belonging to $\vec{V}_{nm}$ segment.

The proposed method operates upon $\gamma_{nm}$ only, and is independent from $x$ and $y$ components. Therefore, the segmentation matrix, $I$, is modified as it follows; and is called \textit{abstraction matrix} ($\Gamma$).
\[
\Gamma =
\bordermatrix{           & \text{Segment} \; 1  & \dots  & \text{Segment} \; M \cr
	  \text{Sector} \; 1 & \gamma_{11}          & \dots  & \gamma_{1M}         \cr
	  \dots              & \vdots               & \ddots & \vdots              \cr
	  \text{Sector} \; N & \gamma_{N1}          & \dots  & \gamma_{NM}}        \qquad
\]

The matrix is independent from the coordinates of each vector in 2D Euclidean space. However, the coordinates are implicitly approximated in the order of each of the vectors. For instance, $\gamma_{2m}$ refers to $m$-th segment on $2 \times (360/N)$-th sector. Finally the $\Gamma$ matrix is normalized using \textit{coefficient of variation} method. 




\subsection{Translation} \label{section: Translation}
In most previous works such as Kabsch algorithm~\cite{kabsch1976solution}, translating input shapes to a position such that their centroid coincide with the center of coordinate system, or any specific coordinate, is a mandatory preprocessing. In general, translation superimposition is inevitable for both partial and full Procrustes superimpositions. 

The abstraction vectors of $\Gamma$ matrix are independent from $x$ and $y$ parameters, hence translation is not essential for the proposed method. However, an alternative application for translation is defined, which enables partial match determination between shapes. For this application, the translation process between the two input shapes could be interpreted as moving the segmentation center of one shape to coordinates pointed out by the segmentation vectors of the other shape (a process similar to translation superimposition in Procrustes analysis). 
%Since segmentation vectors are unit vectors, the set of possible translations is: $\{ \langle 0, \pm1/m \rangle \vert m = 1, 2, \dots M \}$.
Let $T_x$ and $T_y$ denote translation on $x$ and $y$ direction respectively; and $T=T_x \times T_y$ (Cartesian product of translations on $x$ and $y$ coordinates) be the set of all possible translations. The partial match between two shapes is determined by state-space search performed on the $T$ set, which is by applying all the transformations of $T$ on the second shape, and assessing the similarity between the first and second shape (see section~\ref{section: Similarity Measurement}).



\subsection{Rotation} \label{section: Rotation}
Rotation is a rigid body motion of a space that maintains at least one point at its original location; here we fix the segmentation center and move the segmentation vectors. In other words, given that a frame is partitioned into $360/N$ equal sectors, any $(360/N)j$ degrees of rotation for $j \in \{0, 1, \dots N-1\}$ is implemented as $j$ units of circular shift on $\Gamma$ (see Fig.\ref{Figure: ShapeRotation}).

Following the aforementioned objective of using basic operations, rotation is implemented using circular shift operation on $\Gamma$. Accordingly, given $N$ sectors (given that rotation is a rigid body transformation, this operation is independent from $M$), a set of rotation angles that are implemented using circular shift on $\Gamma$, is defined as it follows.
\begin{equation}
R = \left\{ \frac{360}{N}i \,\middle|\, i= 0, 1, \dots N-1 \right\}
\end{equation}

The set $R$ defines a discreet set of rotation angles which are hierarchically extended to a continuous domain using an iterative procedure discussed in Section~\ref{section: Iteration}.

\begin{figure}[!t]
	\centering
	\includegraphics[width=\columnwidth]{Figures/ShapeRotation.pdf}
	\caption
	{
		Abstractions of two inputs, \textit{Shape A} and \textit{Shape B}, are given with partitioning parameters $N=6$ and $M=2$. There is a $120^\circ$ rotation difference between the two shapes. Given $N$ and $M$, the set of rotation angles based on $(360/60)j$ is $R = \{0^\circ, 60^\circ, 120^\circ, 180^\circ, 240^\circ, 300^\circ, 360^\circ \}$. These rotations are implemented using circular shift on $\Gamma$. Accordingly, if the transformation between inputs is $(360/60)j$ degrees of rotation, it is superimposable by $j$ circular shifts of $\Gamma$. Therefore, four times circular shift on $\Gamma_B$ results highest similarity value (e.g., $J=1$), which yields $120^\circ$ rotation as the best linear mapping transformation between \textit{Shape A} and \textit{Shape B}.
	}
	\label{Figure: ShapeRotation}
\end{figure}





\subsection{Similarity measurement} \label{section: Similarity Measurement}
To determine the best linear mapping transformation, the presented method runs a state-space search on transformations, by transforming the second shape, and assessing it's similarity with the first shape. In general, let $\Delta = T \times R$ be the Cartesian product of $T$ translation and $R$ rotation. Note that, if the alternative application for translation introduced in Section~\ref{section: Translation} is of no interest, thence $\Delta = R$. The linear mapping transformations between \textit{Shape A} and \textit{Shape B}, are ranked based on the similarity coefficients $J(\Gamma_A, \delta\Gamma_B)$ for $\delta \in \Delta$.

The similarity between any two elements $\gamma_{nm}^A \in \Gamma_A$ and $\gamma_{nm}^B \in \Gamma_B$ is measured using \textit{Jaccard similarity coefficient}, denoted $j(\gamma_{nm}^A, \gamma_{nm}^B)$, and is calculated as it follows.
\begin{equation}
j(\gamma_{nm}^A, \gamma_{nm}^B) = \frac{\vert \gamma_{nm}^A - \gamma_{nm}^B \vert}{\gamma_{nm}^A+\gamma_{nm}^B}
\end{equation}
The similarity between two abstracted shapes, denoted $J(\Gamma_A, \Gamma_B)$, is calculated as the sum of all pairwise Jaccard similarity indexes as it follows.
\begin{equation}
J(\Gamma_A, \Gamma_B) = \sum_{nm} j(\gamma_{nm}^A, \gamma_{nm}^B)
\end{equation}

Tanimoto similarity coefficient~\cite{rogers1960computer} is an alternative to Jaccard index; however since both methods yield similar results, Jaccard index is chosen. Additionally, other alternatives to Jaccard index are: S{\o}rensen similarity index~\cite{sorensen1948method}, Bray–Curtis dissimilarity~\cite{bray1957ordination} (also known as Czekanowski similarity index), Pearson product moment correlation, and earth mover's distance~\cite{rubner2000earth}.

The similarity assessment between any two $\gamma_{nm}^A$ and $\gamma_{nm}^B$ is also optionally extended by a neighborhood operation. Let $j(N_k(nm, i))$ denote the Jaccard index of neighbor $N_k$ of element $n$, $m$ at $i$-th distance. The extended similarity coefficient $j'$ is calculated as it follows for $d$ neighbors:
\begin{equation}
\begin{split}
j'(\gamma_{nm}^A, \gamma_{nm}^B) =& J(\gamma_{nm}^A, \gamma_{nm}^B)\\
+& \sum_{i=1}^{d} \left[ \log_{d+2} (d+2-i) \sum_{K_i} j(N_k(nm, i)) \right]
\end{split}
\end{equation}


\noindent
This is an adaptive logarithmic neighborhood operation which assigns heavier weight to closer elements than remotes. 
% Throughout our evaluations conducted experiments it is observed that, although $j'$ scores are relatively higher than $j$s (as expected), however rankings of determined transformations are improved with respect to the shapes. Therefore, with reference to relatively higher computational complexity of $j'$, incorporation of neighbors is left optional in the implementation of the method.






\subsection{Iteration} \label{section: Iteration}
How long does it take us to understand the gist of a shape? Henderson et al.~\cite{henderson1998eye} obtained a typical scene fixation of 304ms with 100\% luminance, and Rayner~\cite{rayner1998eye} estimated $233$ms fixation time for an adult reading normal text (Kowler et al.~\cite{kowler1987reading} measured fixation patterns for reading reversed letters). The conceptual and perceptual information understood from a glance at an image frame is a function of the glance duration. Fei-Fei et al.~\cite{fei2007we} studied the perception depth over time. He resulted that we perceive sensory information (e.g., dark and light) in roughly $50$ms, at $107$ms we determine more semantic aspects (e.g., people, room, urban, and water) with considerable accuracy, it takes $150$ms to determine an object (e.g., dog), and at $500$ms we achieve maximum perception (e.g., identify dog as German shepherd).  Greene et al.~\cite{greene2009briefest} conducted similar study and established a perceptual benchmark to types of information we perceive during early perceptual processing; and they inferred that it takes $63$ms to determine naturalness of an image frame and $78$ms to understand whether its forest or not. Such global-to-local view cognitive abilities inspired the present study to see transformation determination as a multi-step procedure an opposed to the some single-step methods.

The longer we are exposed to a shape, the more we understand from it; in other words, the amount of information we perceive from a shape is the function of the number of image processing iterations performed on the shape. Accordingly, present study defines a converging heuristic iterative method that determines the gist of shapes at initial approximation (highest abstraction that corresponds to sensory information such as light/dark classification~\cite{fei2007we}), and by translation superimposition followed by similarity assessment procedure, the most abstracted transformation is determined. Then, segmentation parameters are iteratively incremented. At each iteration, a new abstraction with more details than its preceding abstraction is made. Also, at each iteration, approximated transformations of the preceding iteration are tuned using more detailed abstraction. This process is analogous to: from light, through animal and dog, to German shepherd~\cite{fei2007we,greene2009briefest}.

In general, the proposed method determines an initial approximation of best mapping transformations, and tunes those through successive iterations. The permutations of transformations at each iteration form a state-space that is traversed in best-first search fashion. This approach follows the traits of \textit{Greedy algorithm}~\cite{cormen2009introduction} that determines local optimal choice. To best of our knowledge, cognitive community descriptions of processing segmentations, well overlaps local optimal search method. However, one may consider updating the procedure to follow traits of global optimal search methods to best adapt the application requirements.

Let $l \in \mathbb{N}$ denote an iteration coefficient which is initialized with a user-defined parameter $\omega$. Let $\Gamma_A^l$ be abstraction of shape $A$ at iteration $l$ with $N_l = 2^l$ sectors and $M_l=2^l$ segments. For the purpose of readability of the method, the number of sectors and segments are chosen to be identical; however, the extension of the method to support divers parameters is straight-forward. According to this generalization, the amount of details represented by each abstraction grows exponentially through the iterations. Also, the growth rate can be update to best adapt the application requirements by changing either the growth function or considering $l\in \mathbb{R}$.

Let $\Delta_l$ be the set of all transformations to be applied on $\Gamma_A^l$ at iteration $l$ for $\Delta_\omega = T \times R$. Let $\Upsilon_l = \{\delta_1 \dots \delta_i \dots \delta_\epsilon \}$ be the set of top-$\epsilon$ transformations (i.e., highest similarity) at iteration $l$ for $\epsilon$ being a user-defined parameter. The iteration $l$ tunes best transformations of iteration $l-1$. Accordingly, $\Delta_l$ consists of all $\Upsilon_l$ tunes which is formally defined as it follows for the user-defined parameter $\lambda$ that specifies the tuning range. 
\begin{equation}
\forall j \in \{0, 1, \dots \lambda\} \colon \Delta_l = \{(2^{l-\omega} \delta_{(l-1)i} ) \pm j \}
\end{equation}

For instance, suppose $\omega=3$ then $N=8$ and $M=8$ and assuming only rotation superimposition, we obtain $\Delta_3 = \{0,1,2, \dots 7\}$ which are the number of circular shifts on $\Gamma_A$ that corresponds to $\{0^\circ, 45^\circ, 90^\circ, \dots 315^\circ\}$. Suppose $\epsilon=1$, $\Upsilon_3={2}$, and $\lambda=2$, accordingly $\Delta_4$ is calculated as it follows.
$\Delta_4=\{(2^{4-3} \times 2) \pm \{0, 1\}\}$

\noindent
which corresponds to $\{67.5^\circ, 90^\circ, 112.5^\circ\}$. The pseudo code of the iteration procedure is given in Algorithm~\ref{Algo: IterationAlgorithm}.


\begin{algorithm}
	\caption{Iteration Algorithm}\label{Algo: IterationAlgorithm}
	\begin{algorithmic}[1]
		\Procedure{Iterate}{}
		\State $l \gets \omega$
		\State $\Delta_l \gets T \times R$
		\State $N_l \gets 2^l$
		\State $M_l \gets 2^l$
		\State \textbf{Build} $\; \Gamma_A^l \;$ and $\; \Gamma_B^l \;$
		\State $\Upsilon \gets \text{apply} \; \Delta_l \; \text{on} \; \Gamma_A^l \; \text{and get top-}\epsilon \; \text{transformations}$
		\If {$l < \max l$}
		\State $l \gets l+1$
		\State $\Delta_l \gets \text{all tunes of} \; \Upsilon_{l-1}$
		\State \textit{Goto} 03
		\Else
		\State report $\;\Upsilon_l\;$ as best mapping transformations
		\EndIf
		\EndProcedure
	\end{algorithmic}
\end{algorithm}



\subsection{Validation and verification} \label{section: Validation}
If the difference between two input image frames is $\theta^\circ$ and $\theta \in \Delta_\omega$, according to Section~\ref{section: Rotation}, then $\theta$ is determined using circular shifts on $\Gamma$. However, if $\theta \notin \Delta_\omega$, then $\theta$ is determined using the iterative process. To prove that, consider the following hypothesis:
\begin{equation}
\exists n \in \{1, 2, \dots N \} \colon \frac{360}{N}(n-1) < \theta \leq \frac{360}{N}n 
\end{equation}

\noindent
By definition of segmentation, these are the boundaries of $n$-th region which is divided into $M$ equal segments. According to the hypothesis, $\theta$ belongs to one and only one region, therefore as much as the range is narrowed-down, we get closer and closer to $\theta$ (the motivation of iteration procedure).
%Note that, it is likely to confront some pairs of shapes which various $\theta$s can map them on each other; since at each level $\epsilon$ rotation angles (along with translation parameters) are passed from previous level, determination of multiple $\theta$s is made possible. Achieving appropriate $n$ depends on parameters $d$, $\epsilon$ and $\lambda$.
At each iteration, the results of former iteration are tuned until a result with a user-defined accuracy ($\rho$) is determined. The algorithm performs maximum $c$ iterations which is calculated as it follows.
\begin{itemize}
	\item The segmentation area should be as narrow as $\rho^\circ$, therefore: 
	\begin{equation}
	\rho = \frac{360}{N}n- \frac{360}{N}(n-1) \rightarrow \rho = \frac{360}{N}
	\end{equation}
	
	\item Segmentation grows exponentially through iterations, hence:
	\begin{equation}
	N_l = 2^l \rightarrow \frac{360}{\rho} = 2^l \rightarrow l = \log_2 \frac{360}{\rho} \rightarrow l = \lceil \log_2 \frac{360}{\rho} \rceil
	\end{equation}
	%\[c \rightarrow l-\omega \]
\end{itemize}
