\section{Results}
The accuracy of proposed method is assessed using $100$+ pairs of image frames with diverse resolutions spanning from $50 \times 50$ to $1000 \times 1000$ pixels, and including different categories (e.g., animals, cars, airplane, people, and abstract images). Additionally, the impact of noisy image frames to the accuracy of the proposed method, is assessed using image frames with up to $70$\% of random noise

The evaluations are designed as it follows
\begin{inlinelist}
	\item \textit{Shape A} is a BMP image, or an abstract shape composed of lines, circles, and random noise drawn using features integrated in the implemented tool
	\item \textit{Shape B} is obtained by $\theta$ degrees rotation of \textit{Shape A} plus a random percentage of noise
	\item \textit{Shape A} and \textit{Shape B} are used as inputs for the proposed method
	\item the central tendency of determined rotations is measured as weighted arithmetic mean among top-3 (WM3) rotations, and it is compared with $\theta$. 
\end{inlinelist}
The assessments are performed with default parameters which are: $\omega = 3$, $\lambda=10$, $\epsilon=10$, and the similarity between two segments is calculated excluding neighbor segments. The results of the experiments are discussed as it follows.

\subsection{Top transformations converge rapidly}
The fundamental argument of iterations is to progressively increase the level of details on the image frame abstraction, and accordingly, iteratively improve the accuracy of the calculated approximated transformations, until a user-defined precision criterion is met. Weighted sample variance among top-3 (WV3) approximated transformations provides a measure of dispersion on top approximations. The WV3 reflects the variability in the top-3 approximated transformations, such that: a small WV3 suggests a very reliable WM3, while a large WV3 reflects an uncertainty about the ``best'' linear mapping transformation. According to the experiments, WV3 gets closer to $1$ in a few iterations which yields (a) rapid convergence among top approximated transformations (this confirms the validation of iteration procedure discussed in Section~\ref{section: Validation}), (b) $\textit{WV3} \approx 1$ in few iterations ($>6$) confirms the accuracy of rapidly converged approximated transformations.


\begin{figure*}[!ht]
	\centering
	\includegraphics[width=0.9\textwidth]{Figures/RotationExample.pdf}
	\caption
	{
		\textit{Shape A} is loaded from a BMP image, and \textit{Shape B} is obtained by $270^\circ$ rotation of \textit{Shape A}. The $\Gamma$ matrices of both shapes at different iterations are presented by circular heatmaps. T: determined transformation, S: standard deviation among top-3 determined transformations, D: difference between actual and determined transformations. The normalized similarity index $J(\Gamma_A, \delta \Gamma_B), \forall\delta \in \Delta$ is plotted using a circular heapmap for all the iterations, see panels A2 and B2.  
	}
	\label{Figure: 270}
\end{figure*}


\subsection{Tuning out the cognitive noise}
Selective and visual attention filter irrelevant stimuli to the subject's task by mechanisms such as habituation and cognitive inhibition. There have been promising efforts to model the ability (e.g.,~\cite{tsotsos1995modeling}) since the \textit{spotlight}~\cite{eriksen1972temporal} and \textit{zoom lens}~\cite{eriksen1986visual} models. Additionally, perceived visual information are function of an observer's distance to an object. This aspect has variety of applications namely is Olivia et al.~\cite{oliva2006hybrid} that incorporates this aspect with hybrid images. A hybrid image is composed of two image frames with low and high spatial frequencies, such that either is perceived as noise as a function of observer's distance to the hybrid image frame. In other words, the image of high spatial frequency is dominant at closer distance, while the image with low frequency is perceived at far distance. Whether the noise is a masked image or it is an irrelevant stimuli, it does not impact the perceived information from an image frame. Therefore, the performance of proposed method in approximating linear mapping transformation using noisy image frames, is assessed by experiments where a percentage of \textit{Shape B} is covered with random noise. 

To this extend, an experiment of four tests, $T1$, $T2$, $T3$, and $T4$ is conducted (see Fig.~\ref{Figure: NoiseImpact}). The tests have \textit{Shape A} in common which is a BMP image of a bee. The \textit{Shape B} is created by $234^\circ$ rotation of \textit{Shape A}, and differs among test in the amount of incorporated random noise. The subject in the \textit{Shape A} (i.e., the bee) is represented by $\approx230$K pixels (of $584$K pixels of the image frame). A portion of $120$K pixels (out of the $\approx230K$ pixels) is subject to random noise. This portion is intentionally chosen to cover the body of the bee which presents the majority of perceptible features of the subject. Given that the pixels are binary and the figure is represented by pixels of value 1 (see Section~\ref{section: Shapre Representation}), the random noise is created by setting the value of a random pixel to $1$ in the subject-to-noise portion of \textit{Shape B}. The random noise is added through an iteration of $0$, $5$K, $50$K, and $500$K random pixel selections (a pixel can be selected multiple times) respectively for $T1$, $T2$, $T3$, and $T4$ (see Fig.~\ref{Figure: NoiseImpact}); such that, the majority of perceptible features on \textit{Shape B} are covered with random noise at $T4$. 

The initial segmentation parameter ($\omega = 3$) provides a limited number of variant initial approximations (see Section~\ref{section: Iteration}). Therefore, the WV3 at first iteration (i.e., $l=3$) of the $T1$, $T2$, $T3$, and $T4$ show relatively high dispersion, which indicate the inconsistency of WM3 (see Fig.~\ref{Figure: NoiseImpact}). The initial approximations are tuned at second iteration (i.e., $l=4$) which improve WV3 tenfold (from $118$ to $18$) for the $T1$, $T2$, and $T3$. Despite of a minor discrepancy, WM3 of the tests $T1$, $T2$, and $T3$ are relatively close to actual transformation (i.e., $234^\circ$). However, the considerable noise of $T4$ prevents its WV3 convergence at the same rate as of $T1$, $T2$, and $T3$ (see Fig.~\ref{Figure: NoiseImpact}). The third iteration (i.e., $l=5$) improves approximations, and it brings WV3 of all the test to a same scale, and accordingly provides reliable WM3. Further iterations squeeze the approximations and reach to $\text{WV3}=1.1$ for all tests at sixth iteration (i.e., $l=8$) which indicates a considerable consistency of WM3. Therefore, the method determines WV3 and WM3 for all test at the same scale, given the considerable amount of noise (specially at $T4$). This confirms that even a low amount of perceptible features of the figures is adequate to tune the initial approximations to reliable approximations. For details of the noise impact on other approximations, refer to Supp. Fig.2.17-20.

\begin{figure}[!t]
	\centering
	\includegraphics[width=\columnwidth]{Figures/NoiseImpact.pdf}
	\caption
	{
		Evaluation of random noise impact on transformation determination.s
	}
	\label{Figure: NoiseImpact}
\end{figure}


\subsection{Image resolution defines maximum number of iterations}
When abstracting an image frame, up until a certain iteration, a segment consists of multiple pixels. However beyond that iteration, a segment might be smaller than a pixel (i.e. one pixel belongs to multiple segments). To determine a segment to which a pixel belongs to, the method rounds the position of the pixel. Therefore, beyond a certain iteration, the rounding procedure could potentially increase the distance between the abstractions of two image frames. In such condition, the WV3 converges up-until a certain iteration, and it is saturated beyond that iteration, and accordingly is the WM3 (see Supp. Fig.2.22-23). Therefore, maximum number of iterations, and accordingly the number of \textit{segments} and \textit{sectors} are the function of shape resolution. 


\subsection{Pin-pointed transformation vs. condensed approximations}
The linear mapping transformation between two shapes is determined either as a single transformation with considerable discrepancy with the rest of the approximations (e.g., panel A on Fig.~\ref{Figure: 270}), or a condensed distribution of approximated transformations around actual transformation (e.g., panel B on Fig.~\ref{Figure: 270}). This behavior originates from the discreet representation of image frames (raster graphics); such that, when drawing a \textit{Shape B} from \textit{Shape A}, a pixel of \textit{Shape A} is mapped to a rounded position on \textit{Shape B}. Therefore, pixels of \textit{Shape A} could overlap as mapped on \textit{Shape B}. For instance, the two pixels at $\langle x_1=4, y_1=4 \rangle$ , $\langle x_2=4, y_2=5 \rangle$ belonging to the segment/sector $V_{nm}$ of \textit{Shape A}, with $70^\circ$ rotation, respectively map to positions $\langle x_1^\prime = -0.562, y_1^\prime = 5.628 \rangle$ and $\langle x_2^\prime = -1.33, y_2^\prime = 6.262 \rangle$. As the coordinates are rounded, the two pixels map to position $\langle -1, 6 \rangle$ belonging to the segment/sector $V_{n'm'}$ of \textit{Shape B}. Therefore, two pixels of \textit{Shape A} map to one pixel on \textit{Shape B} (surjective linear transformation). Accordingly, as abstracting the shapes using aggregation function \textit{count} (see Section~\ref{section: Shape Segmentation}), the abstraction parameters are calculated as it follows: $\gamma_{nm} = 2$ and $\gamma_{n'm'} = 1$ (e.g., see comparison of $\gamma$ value distribution plots on Supp. Fig.2.1-22). Hence, comparing $\gamma_{nm}$ and $\gamma_{n'm'}$ results to $j(\gamma_{nm}, \gamma_{n'm'}) = 0.33$ as opposed to expected $j(\gamma_{nm}, \gamma_{n'm'}) = 1$. Such scenarios prevents ``pin-pointing'' the actual transformation (in this case $70^\circ$) and rather provides a condensed distribution of transformations around actual transformation (e.g., see panel B on Fig.~\ref{Figure: 270}).


\subsection{A small similarity is sufficient to determine a reliable linear mapping approximation}
Ideal scenario for comparing two shapes is when there exist a one-to-one correspondence (injective/surjective) between pixels of two the shapes. However, for variety of reasons discussed as it follows, the rotation function on raster graphics is surjective. For instance, rotation function may map multiple pixels of \textit{Shape A} to one pixel of \textit{Shape B}, causing a percentage of deformation on \textit{Shape B} (e.g., see supp. Fig.2.21), and preventing ``pin-pointing'' actual transformation (as above-discussed). Additionally, shapes are possibly subject to noise, which would prevent one-to-one correspondence between the two shapes (non-surjective). Moreover, \textit{Shape A} may consist of congruent figures (e.g., two side-by-side circles of the same radius), and if \textit{Shape B} is determined by $\theta^\circ$ rotation of \textit{Shape A}, then in addition to $\theta^\circ$, multiple rotation angles may also map the congruent shapes on each other. In such cases, actual transformation is determined using incongruent elements (e.g., saddle area, or pedal of the bicycle on Fig.~\ref{Figure: 270}). Such prominent details not only improve approximations for congruent shapes, but are also advantageous when the majority of the shape is covered by noise (e.g., Fig.\ref{Figure: NoiseImpact}) or is deformed (e.g., Supp. Fig.2.21). 

The method discussed in present study, minimizes the impact of such discrepancies on linear mapping transformation determination, by calculating the similarity of two corresponding segments independently from the rest of the segments (an adaptive neighborhood operation of custom range is optionally enabled). Therefore, a higher similarity between few segments is adequate to determine mapping transformation with considerable accuracy. The experiments on deformed, congruent, and noisy image frames illustrate the accuracy of the proposed method on such scenarios.



