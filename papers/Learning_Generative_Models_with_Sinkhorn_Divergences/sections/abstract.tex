% !TEX root = ../SinkhornDivergences.tex

\begin{abstract}
The ability to compare two degenerate probability distributions, that is two distributions supported on low-dimensional manifolds in much higher-dimensional spaces, is a crucial factor in the estimation of generative models.% computer vision or natural language. To estimate generative models in such settings, the ability to compare two degenerate distributions  is crucial. 
It is therefore no surprise that optimal transport (OT) metrics and their ability to handle measures with non-overlapping supports have emerged as a promising tool. Yet, training generative machines using OT raises formidable computational and statistical challenges, because of \emph{(i)} the computational burden of evaluating OT losses, \emph{(ii)} their instability and lack of smoothness, \emph{(iii)} the difficulty to estimate them, as well as their gradients, in high dimension.
%
This paper presents the first tractable method to train large scale generative models using an OT-based loss called Sinkhorn loss which tackles these three issues by relying on two key ideas: \emph{(a)} entropic smoothing, which turns the original OT loss into a differentiable and more robust quantity that can be computed using Sinkhorn fixed point iterations; \emph{(b)} algorithmic (automatic) differentiation of these iterations with seamless GPU execution.
%
%These two approximations result in a robust and differentiable approximation of the OT loss with streamlined GPU execution. 
%
Additionally, Entropic smoothing generates a family of losses interpolating between Wasserstein (OT) and Energy distance/Maximum Mean Discrepancy (MMD) losses, thus allowing to find a sweet spot leveraging the geometry of OT on the one hand, and the favorable high-dimensional sample complexity of MMD, which comes with unbiased gradient estimates.
%
The resulting computational architecture complements nicely standard deep network generative models by a stack of extra layers implementing the loss function. 
\end{abstract}