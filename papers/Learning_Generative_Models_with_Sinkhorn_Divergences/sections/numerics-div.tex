% !TEX root = ../SinkhornDivergences.tex


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Numerical Exploration of the Sinkhorn Divergence}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Sample Complexity}

To better grasp the statistical tradeoff offered by the entropic regularization, we study numerically the so-called sample complexity of these divergence.
%
We consider 
\eq{
	\hat \mu_N = \frac{1}{N}\sum_{i=1}^N \de_{x_i}
	\qandq
	\hat \nu_N = \frac{1}{N}\sum_{i=1}^N \de_{x_i}
}
which are random measures, where the $(x_i)_i$ and $(y_i)_i$ are ponts independently drawn from the same distribution $\xi$.
%
In the numerical experiments, $\xi$ is the uniform distribution on $[0,1]^d$ where $d \in \NN^*$ is the ambient dimension. 

We recall that 
\eq{
	\bar\Ww_{c,\epsilon}(\mu,\nu) \eqdef 2\Ww_{c,\epsilon}(\mu,\nu)-\Ww_{c,\epsilon}(\mu,\mu)-\Ww_{c,\epsilon}(\nu,\nu)
}
\eq{
	\qwhereq 
	\Ww_{c,\epsilon}(\mu,\nu) \eqdef \int c(x,y) \d\ga_{\epsilon}
}
where $\ga_{\epsilon}$ is the unique solution of the entropy-regularization optimal transport problem between $\mu$ and $\nu$.
%
In the following, we consider $c(x,y)=\norm{x-y}^p$ for $p=3/2$ for $(x,y) \in (\RR^{d})^2$.


\begin{figure*}
\centering
\begin{tabular}{c@{}c@{}c}
\includegraphics[width=.32\linewidth]{sample-complexity/influ-epsilon-d2.pdf}&
\includegraphics[width=.32\linewidth]{sample-complexity/influ-epsilon-d3.pdf}&
\includegraphics[width=.32\linewidth]{sample-complexity/influ-epsilon-d5.pdf}\\
$d=2$& $d=3$ & $d=5$\\
\end{tabular}%
\caption{ Influence of the regularization $\epsilon$on the sample complexity rate. The plot displays $\log_{10}(R_{\epsilon,d}(N))$ as a function of $\log(N)$. 
\label{fig:influ-eps}}
\end{figure*}

One has the convergence
\begin{align*}
	\Ww_{c,\epsilon}(\mu,\nu) &\overset{\epsilon\rightarrow 0}{\longrightarrow} 2 W_p(\mu,\nu)^p \\
	\qandq
	\Ww_{c,\epsilon}(\mu,\nu) &\overset{\epsilon\rightarrow +\infty}{\longrightarrow} \norm{\mu-\nu}_{\text{ED}(p)}^2
\end{align*}
where $W_p$ is the Wasserstein-$p$ distance while $\norm{\xi}_{\text{ED}(p)}^2  = \int -\norm{x-y}^p \d\xi(x)\d\xi(y)$ is the Energy Distance, which is a special case of MMD norm for $0<p<2$.

The goal is to study numerically the decay rate toward zero of 
\eq{
	R_{\epsilon,d}(N) \eqdef \EE(\bar \Ww_{c,\epsilon}(\hat\mu_N,\hat\nu_N))
}
and also analyze the standard deviation
\eq{
	S_{\epsilon,d}^2(N) \eqdef \EE( |\bar \Ww_{c,\epsilon}(\hat\mu_N,\hat\nu_N)-R_{\epsilon,d}(N)|^2 ).
}
In these formula, the expectation $\EE$ with respect to random draws of $(x_i)_i$ and $(y_i)_i$ is estimated numerically by averaging over $10^3$ drawings.
%
For optimal transport, i.e. $\epsilon=0$, it is well-known (we refer to the references given in the paper) that $R_{0,d}(N) = O(\frac{1}{N^{p/d}})$, while for MMD norm, i.e. $\epsilon=+\infty$, one has $R_{+\infty,d}(N) = O(\frac{1}{N})$.


Figure~\ref{fig:influ-d} (resp.~\ref{fig:influ-eps}) display in log-log plot the decay of $R_{\epsilon,d}(N)$ with $N$, and allows to compare on a single plot the influence of $d$ (resp. $\epsilon$) for a fixed $\epsilon$ (resp. $d$) on each plot.

\begin{figure*}
\centering
\begin{tabular}{@{}c@{}c@{}c@{}}
\includegraphics[width=.32\linewidth]{sample-complexity/influ-d-eps001.pdf}&
\includegraphics[width=.32\linewidth]{sample-complexity/influ-d-eps01.pdf}&
\includegraphics[width=.32\linewidth]{sample-complexity/influ-d-eps1.pdf}\\
$\epsilon=0.01$& $\epsilon=.1$ & $\epsilon=1$ 
% \includegraphics[width=.32\linewidth]{sample-complexity/influ-d-eps10.pdf}\\
% & $\epsilon=10$
\end{tabular}%
\caption{ Influence of the dimension $d$  on the sample complexity rate for difference $d$. The plot displays $\log_{10}(R_{\epsilon,d}(N))$ as a function of $\log(N)$. 
% 
The shaded bar display the confidence interval at $\pm S_{\epsilon,d}(N)$.
\label{fig:influ-d}}
\end{figure*}


From these experiments, one can conclude on this distribution $\xi$ that:
\begin{itemize}
	\item $\Ww_{c,\epsilon}(\mu,\nu) \geq 0$ (more on this in the following section).
	\item $R_{\epsilon,d}(N)$ as a polynomial decay of the form $1/N^{\kappa_{\epsilon,d}}$.
	\item One recovers the known rates $\kappa_{0,d}=p/d$ (here for $p=3/2$) and $\kappa_{\infty,d}=1$.	
	\item Small values of $\epsilon < 1$ have rates $\kappa_{\epsilon,d}$ close to the rate of OT $\kappa_{0,d}$.
	\item Large values of $\epsilon > 1$ have rates $\kappa_{\epsilon,d}$ matching almost exactly the rate of MMD $\kappa_{+\infty,d}=1$.
	\item The variance $S_{\epsilon,d}^2(N)$ is significantly smaller for small values of $\epsilon$ (i.e. close to OT). 
\end{itemize}
Note that similar conclusion are obtained when testing on other distributions $\xi$ (e.g. a Gaussian).


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Positivity}

For $\epsilon \in \{0,+\infty\}$, both OT and MMD are distances, so that $\bar\Ww_{\epsilon,c}(\mu,\nu)=0$ if and only if $\mu=\nu$.
%
It not known whether this property is true for $0 < \epsilon < +\infty$, and this seems a very difficult problem to tackle. 
%
We investigate numerically this question by looking at small modification of a discrete input measure $\mu = \frac{1}{\sum_i a_i}\sum_{i=1}^N a_i \de_{x_i}$ where the $x_i$ are i.i.d. points drawn in $[0,1]^2$ and $(a_i)_i$ are i.i.d. number drawn uniformly in $[1/2,1]$, and perform a small modification
\eq{
	\mu_t \eqdef \frac{1}{\sum_i a_{i,t}}\sum_{i=1}^N a_i \de_{x_{i,t}}
	\qwhereq
	\choice{
		a_{i,t}=a_{i,t}+t b_i, \\
		x_{i,t}=x_i+t z_i, \\
	}
}
where $(b_i)_i \subset \RR$ are i.d.d. Gaussian distributed $\Nn(0,1)$ and
where $(z_i)_i \subset \RR^2$ are i.d.d. Gaussian distributed $\Nn(0,\Id_2)$.

Figure~\eqref{fig:positivity} shows, on a single realization of $(a_i,x_i,b_i,z_i)$, that $\bar\Ww_{\epsilon,c}(\mu,\mu_t)>0$ for $t \neq 0$. Testing for $10^4$ other realizations gives the same results, showing that experimentally  $\bar\Ww_{\epsilon,c}$ is locally strictly positive for discrete measures. 

\begin{figure*}
\centering
\begin{tabular}{c@{\hspace{5mm}}c}
\includegraphics[width=.32\linewidth]{derivatives/sinkhorn-loss-n10-mmd.pdf}&
\includegraphics[width=.32\linewidth]{derivatives/sinkhorn-loss-n100-p18-mmd.pdf}\\
$p=1$, $N=10$ & $p=1.8$, $N=100$
\end{tabular}%
\caption{ Test of the positivity of $\bar\Ww_{\epsilon,c}(\mu,\mu_t)$ as a function of the perturbation parameter~$t$. 
\label{fig:positivity}}
\end{figure*}



