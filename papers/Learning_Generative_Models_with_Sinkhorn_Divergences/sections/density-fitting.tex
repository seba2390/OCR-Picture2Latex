% !TEX root = ../SinkhornDivergences.tex

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Minimum Kantorovich Estimation}

%%%%
\paragraph{Density fitting.}

We consider a data set of $\N$ (usually very large) observations $(y_1,\dots,y_\N) \in \Xx^\N$ and we want to learn a generative model that produces samples that are similar to that dataset. Samples $x=\gt(z)$ from the generative model are defined by taking as input a sample $z \in \Zz$ from some reference measure $\muz$ (typically a uniform or a Gaussian measure in a low-dimensional space $\Zz$) and mapping it through a differentiable function $\gt : \Zz \rightarrow \Xx$. Formally, this corresponds to defining the generative model measure $\mut$ from which $x$ is drawn as $\mut = g_{\th\#}\muz$. Our goal is to find $\theta$ which minimizes a certain loss $\Loss$ between $\mut$ and the empirical measure $\nu$ associated with the data
\eql{\label{eq-fitting-generic}
	\theta \in \uargmin{\theta} \Loss(\mut,\nu)  
	\qwhereq 
	\nu \eqdef \frac{1}{\N} \sum_{j=1}^{\N} \delta_{y_j}.
}
While we focus here for simplicity on the case of deterministic encoding functions $g_\th$ between $\muz$ and $\mut$, our method extends to more general probabilistic generative models, such as VAE~\cite{VAE}.


%%%%
\paragraph{Distances between measures.}
%
Maximum likelihood estimation (MLE) is obtained by setting $\Loss(\mut,\nu) = -\sum_j \log \frac{\d\mut}{\d x}(y_j)$, where $\frac{\d\mu}{\d x}$ is the density of $\mut$ with respect to a fixed reference measure (a typical choice is $\d x$ being the Lebesgue measure in $\Xx=\RR^d$). This MLE loss can be seen as a discretized version of the relative entropy (a.k.a. the Kullback-Leibler divergence). A major issue with this approach is that in general generative models defined this way (when $\Zz$ has a much smaller dimensionality than $\Xx$) have singular distributions (\emph{i.e.} supported on a low-dimensional manifold), without density with respect to a fixed measure, and therefore MLE cannot be considered.

The usual workaround is to assume that $\Xx$ is equipped with some distance $d_\Xx$, and consider weak metrics, which take into account spatial displacement of these measures, enabling the comparison of singular measures. A classical construction for such a loss function $\Loss$ is through duality (see \emph{e.g}~\cite{sriperumbudur2012empirical}), namely by considering a dual norm $\Loss(\mu,\nu)=\norm{\mu-\nu}_B^*$ where $\norm{\xi}_B^* = \sup\enscond{ \int_\Xx h(x)\d\xi(x) }{ h \in B }$. Here $B$ is a ``unit ball'' of continuous functions that should contain $0$ in its interior. This ensures that $\norm{\cdot}_B^*$ is well defined even for singular inputs, and it is a norm which metrizes the weak convergence of measures (so that for instance $\Loss(\de_x,\de_{x'}) \rightarrow 0$ as $x \rightarrow x'$), see~\cite[Sec.7.2.1]{santambrogio2015optimal} for more details. Classical instances of such settings include the 1-Wasserstein distance (obtained by setting $B = \enscond{g}{\norm{\nabla g}_\infty \leq 1}$ the set of 1-Lipschitz functions) and reproducing kernel Hilbert spaces (letting $B=\enscond{g}{\norm{k \star g}_{L^2(\Xx)} \leq 1}$ where $k$ is an appropriate convolution kernel). The latter define the class of Maximum Mean Discrepency losses \cite{gretton2007kernel} defined by
\begin{equation}
\begin{split}
	\norm{\mu,\nu}_k &= \mathbb{E}_{\mu\otimes\mu}[k(X,X')] + \mathbb{E}_{\nu\otimes \nu}[k(Y,Y')] \\
	&- 2 \mathbb{E}_{\mu\otimes \nu}[k(X,Y)]
\end{split}
\end{equation}  

% A natural distance between measures is the Wasserstein distance. Contrarily to the popular Kullback-Leibler divergence, it defines an actual distance on the whole space of Radon measures on $X$ and has a better ability to deal with the geometry of the problem. 


%%%%
\paragraph{Optimal transport distances.}


In this article, we advocate for a different approach, which is to consider generic optimal transport (OT) metrics which can be used over general spaces $\Xx$ (not just the Euclidean space $\RR^d$ and not only the 1-Wasserstein distance). 
%
The OT metric between two probability distributions $(\mu,\nu) \in \Mm_+^1(\Xx) \times \Mm_+^1(\Xx)$ supported on two metric spaces $(\Xx,\Xx)$ is defined as the solution of the (possibly infinite dimensional) linear program:
\eql{\label{primal}
	\Ww_c(\mu,\nu) \eqdef \min_{\pi \in\Pi(\mu,\nu) } \int_{\Xx \times \Xx} c(x,y) \d \pi(x,y),
}
where the set of couplings is composed of joint probability distributions over the product space $\Xx \times \Xx$ with imposed marginals $(\mu,\nu)$
\eq{ 
	\Pi(\mu,\nu) \eqdef \enscond{\pi \in \Mm_+^1(\Xx \times \Xx) }{ P_{1\sharp}\pi=\mu, P_{2\sharp}\pi=\nu  },
	% \forall (A,B) \subset \Xx \times \Yy, \pi(A \times \Yy) = \mu(A), \pi(\Xx \times B) = \nu(B)
} 
where $P_1(x,y)=x, P_2(x,y)=y$ are simple projector operators.
%
Formula~\eqref{primal} corresponds to the celebrated Kantorovitch formulation~\cite{Kantorovich42} of OT (see~\cite{santambrogio2015optimal} for a detailed account on the theory). 
%
Here $c(x,y)$ is the ``ground cost'' to move a unit of mass from $x$ to $y$, and we shall make no assumptions (except for regularity) on its form. When $\Xx$ is equipped with a distance $d_\Xx$, a typical choice is to set $c(x,y)=d_\Xx(x,y)^p$ where $p>0$ is some exponent, in which case for $p \geq 1$ $\Ww_c^{1/p}$ is the so-called $p$-Wasserstein distance between probability measures.
 

We introduce the regularized optimal transport problem~\cite{CuturiSinkhorn,2016-genevay-nips} defined by
\begin{equation}\label{OTreg}
\min_{\pi\in\Pi(\mu,\nu)} \int c(x,y) \d\pi(x,y) + \epsilon \int \log(\frac{\pi(x,y)}{\d\mu(x)\d\nu(y)})\d\pi(x,y) \tag{$\Pp_\epsilon$}
\end{equation}


And the associated regularized Wasserstein distance associated with cost $c$ and regularization paremeter $\epsilon$ is defined by:
$$\Ww_{c,\epsilon} (\mu,\nu) = \int c(x,y) \d \pi_\epsilon(x,y)$$ where $\pi_\epsilon$ is the optimal coupling for the regularized OT problem \eqref{OTreg}.
 
\begin{thm}[Sinkhorn Loss]
The Sinkhorn loss between two measure $\mu,\nu$ is defined as: \eql{\label{eq-sinkh-loss}\bar{\Ww}_{c,\epsilon} (\mu,\nu) = 2 \Wce (\mu,\nu) - \Wce (\mu,\mu) -\Wce (\nu,\nu).} 
with the following limiting behavior in $\epsilon$: \begin{enumerate}
 \item as $\epsilon \rightarrow 0$,\quad $ \bar{\Ww}_{c,\epsilon} (\mu,\nu) \rightarrow 2 \Ww_c(\mu,\nu)  $
 \item as $\epsilon \rightarrow +\infty$,\quad $ \bar{\Ww}_{c,\epsilon} (\mu,\nu) \rightarrow MMD_{-c}(\mu,\nu) $
\end{enumerate}
where $MMD_{-c}$ is the MMD distance whose kernel is the cost from the optimal transport problem.
\end{thm}

\begin{rem}This theorem is a generalization of~\cite[\S3.3]{e19020047} for continuous measures. \end{rem}


\begin{proof}
1. The first part of the assumption is well known, see for instance \cite{Carlier2017}.

2. Letting $\epsilon$ go to infinity in the regularized OT problem amounts to finding the coupling with minimum entropy in the constraint set. The problem becomes $\min_{\pi \in \Pi(\mu,\nu)} \int log(\frac{\pi(x,y)}{\d\mu(x)\d\nu(y)})d\pi(x,y)$ where $\Pi(\mu,\nu)$ is the set of couplings with marginals $\mu$ and $\nu$. Introducing Lagrange multipliers $u$ and $v$ for these constraints, the dual problem becomes $\max_{u,v} \int u(x) \d\mu(x) + \int v(y) \d\nu(y) - \int \exp(u(x)+v(y))\d\mu(x) \d\nu(y)$ and the primal-dual relation is given by $d\pi(x,y) = \exp(u(x)+v(y))\d\mu(x) \d\nu(y)$. Solving the dual gives $u = v = 0$ and thus the optimal coupling is simply the product of the marginals i.e. $\pi = \mu \otimes \nu$.
\end{proof}

The density fitting problem can be rewritten using the Sinkhorn divergence~\eqref{eq-sinkh-loss}:
\eq{
\umin{\theta} E_\epsilon(\theta) \qwhereq  E_\epsilon(\theta) \eqdef \bar{\Ww}_{c,\epsilon} (\mut,\nu).
}

\paragraph{A Discussion on OT \emph{vs.} MMD}
As proved in Theroem 1, the Sinkhorn loss interpolates between a pure OT loss for $\epsilon=0$ and MMD losses for $\epsilon=+\infty$. 
As such, when $\epsilon \rightarrow +\infty$, our loss takes advantage of the good properties of MMD losses, and in particular a favorable sample complexity of $O(1/\sqrt{n})$ (decay rate of the approximation of the true loss with a mini-batch of size $n$) and unbiased gradient estimates when using mini-batches. Note that sample complexity estimates have not been proved for the Sinkhorn loss, but empirical evidence (see curves in supplementary material) shows that its behavior is similar to that of MMD when epsilon is not too small.  In contrast, the unregularized OT loss suffers from a sample complexity of $O(1/n^{1/d})$, see~\cite{weed2017sharp} for a recent account on this point. 
%
Using MMD to train generative models has been shown to be successful in~\cite{MMD-GAN,li2015generative}. The improved Wasserstein GAN approach~\cite{gulrajani2017improved} (which penalizes the squared norm of the gradient of the dual potential) is similar to an MMD (in fact a dual Sobolev norm).
%
By tuning the $\epsilon$ parameter, our method is able to take the best of both worlds, to blend the non-flat geometry of OT with the high-dimensional rigidity of MMD losses. Additionally, the Sinkhorn loss, as is the case for the original OT problem, can be defined with any cost $c$, whereas MMD losses are only meaningful when used with positive definite kernels $k$. The postivity of the Sinkhorn loss is yet to be proved but empirical evidence (see supplementary) strongly points in that direction.
%
Eventually, in the specific case where $c = \norm{\cdot}_p$ for $1<p<2$, the associated MMD loss is the energy distance~\cite{szekely2004testing}. It was also used to fit generative models in~\cite{CramerGAN}, while \cite{MMDGAN} uses MMD with a gaussian kernel. Note that contrary to what~\cite{CramerGAN} claims, the energy distance cannot be presented as a cure to solve the bias of OT estimation in high-dimension, since the two distances are fundamentally different. 
