% !TEX root = ../SinkhornDivergences.tex

\section{Applications}

We consider two popular problems in machine learning to illustrate the versatility of our method. The first one relies on fitting labeled data with uniform distribution supported on ellipses (note that this could be any parametric shape but ellipses here were a good fit). The second problem consists in tuning a neural network to generate images, first with a fixed cost (on MNIST dataset) and then with a parametric cost (on CIFAR10 dataset). \
%
In both cases, we used simple initializations (see details below) and the algorithm yielded similar results when rerun, meaning that the results displayed are representative of the performance of the algorithm and that the procedure is quite stable.

\subsection{Data Fitting with Ellipses.}

As mentioned earlier, a strength of the Wasserstein distance is its ability to fit a singular probability distribution to an empirical measure (data). That singular probability may be supported on a subset of the space on a lower dimensional manifold, or simply have a degenerate density that becomes null for some subsets of the original space. To illustrate this principle, we consider in what follows a simple 3D example that can easily be visualized.

We use the Iris dataset (3 classes, 50 observations each in 4 dimensions) projected in 3D using PCA. This defines the empirical measure $\nu$ in $\RR^3$. 
%
If we were to find a probability distribution $\mut$ bound to be itself an empirical measure of $K$ atoms (in that case parameter $\theta$ would contain exactly the locations of those $K$ points in addition to their weight), then minimizing the 2-Wasserstein distance of $\mu_\theta$ to $\nu$ would be \emph{strictly equivalent} to the $K$-means problem~\cite{NIPS2012_4651}. In that sense, quantization can be regarded as the most elementary example of Wasserstein loss minimization of degenerate families of probability distributions.

The model we consider is instead composed of $K$ ellipses with uniform density: Each ellipse is parametrized by a $3\times 3$ matrix $A_k$ (the square root of its covariance matrix) and a center $\alpha_k \in \RR^3$, so that $\th=(A_k,\al_k)_k$. Therefore, our results can't be directly compared to that of clustering algorithms, in the sense that we do automatically recover, within such ellipses, entire areas of interest (and not voronoi cells). We assume in this illustration that each ellipse has equal mass $1/K$.
% which makes sense here as our data is evenly distributed between each class, and that the mass is uniformly distributed over each ellipse. 
To recover these ellipses through a push forward, we use a uniform ground density $\muz$ over $K$ centred unit balls, translated and dilated for each ellipse using the push-forward defined by $g_{\th}(z) = A_k z + \alpha_k$ if $z$ is in the $k$-th ball. 
%However depending on the type of data one wants to fit, the model can be adapted otherwise (density decaying when moving away from the center, mass proportional to the size of the ellipse) with simple modifications in either the ground density $\muz$ or the pushforward $\gt$. 

\begin{table}
\centering
%\begin{minipage}[t]{\linewidth}
%\begin{table}
 \begin{tabular}{|c|c|c|c| } \hline
 MMD & $\epsilon = 1$ & $\epsilon = 0.1$ & $\epsilon = 0.001$ \\ \hline \hline
     $\begin{matrix} 36 & 0 & 0 \\ 0 & 39 & 13 \\
 0 & 11 & 42 \end{matrix}$
     &
     $\begin{matrix} 50 & 0 & 0 \\ 0 & 50 & 38 \\
0 & 36 & 47 \end{matrix}$
     & $\begin{matrix} 44 & 0 & 0 \\ 0 & 38 & 5 \\ 0 & 8 & 40 \end{matrix}$
     & $\begin{matrix}  33 & 0 & 0 \\  0 & 37 & 3 \\ 0 & 12 & 25\end{matrix}$
     \\\hline
    \end{tabular}
   % \end{table}
%\end{minipage}
\caption{Evaluation of the fit after convergence of the algorithm : entry $(i,j)$ corresponds to the number of points from class $j$ that are inside ellipse $i$} \label{tab:clustering}
  \end{table}

  \begin{figure*}[ht]
  \centering
  \begin{subfigure}{0.25\textwidth}
  \includegraphics[width=\textwidth]{images/cluster-mmd.png} 
  \caption{MMD}
  \end{subfigure}%
  \begin{subfigure}{0.25\textwidth}
  \includegraphics[width=\textwidth]{images/cluster-1.png} 
  \caption{$\epsilon = 1$ }
  \end{subfigure}%\begin{subfigure}{0.6\textwidth}
  \begin{subfigure}{0.25\textwidth}
  \includegraphics[width=\textwidth]{images/cluster-2.png} 
  \caption{$\epsilon = 0.1$}
  \end{subfigure}%
  \begin{subfigure}{0.25\textwidth}
  \includegraphics[width=\textwidth]{images/cluster-3.png} 
  \caption{$\epsilon = 0.01$}
  \end{subfigure}%
  \caption{Ellipses after convergence of the stochastic gradient descent with $L=20$, $m=200$
  \label{fig:kballs}}
  \end{figure*}
\paragraph{Numerical Illustration.} The fit is obtained using the cost $c(x,y)=\norm{x-y}^2$, the ellipse matrices $(A_k)_k$ are all initialized with the identity matrix (which corresponds to the unit ball) and centers $(\al_k)_k$ are initialized with the $K$-means algorithm. We fixed a maximal buget of Sinkhorn iterations $L = 20$ to be competitive with MMD time-wise, with a minibatch size $m=200$ for both algorithms. Figure~\ref{fig:kballs} displays the results of our method for different values of $\epsilon$ and for MMD with a gaussian kernel (with manually tuned bandwith ). The influence of the regularization parameter $\epsilon$ is crucial: too much regularization (large $\epsilon$,(b)) leads to a loose fit of the data but not regularizing enough leads to very slow convergence of the Sinkhorn algorithm and also yeilds poor performance (d) or requires more cpu time if we increase the total iteration budget. 
%
Since the Iris data is labeled, we can asses the fit of the model by checking the class repartition in each ellipse, as summarized in table \ref{tab:clustering}. Each entry $(i,j)$ corresponds to the number of points from class $j$ that are inside ellipse $i$ (recall there are 50 points per class). The performance difference between MMD and Sinkhorn here is not obvious, once the bandwidth parameter of the kernel is carefully tuned, but we found out that this parameter was more sensitive than $\epsilon$, as the range of values that yield acceptable results are smaller.



%\paragraph{Outliers detection} This simple ball model can further be used for outliers detection. In a toy example, we consider data thar consists of one class of the Iris dataset, and 20\% of points from the two other classes chosen at random. We fit a single ball to the data, that should encompass the points of the first class and discard the points from the other class. Thus the decision rule is that a data point is considered an oulier if it lies outside the ball of radius $r$ and center $x$. Naturally, a more complex shape could be used as long as it can easily be prametrized. The results are displayed in figure \ref{fig:clustering}. 

\subsection{Tuning a Generative Neural Network}

Image generating models such as GAN~\cite{GAN} or VAE~\cite{VAE} have become popular in recent years. The goal is to train a neural network $g_\th$ which generates images $g_\th(z)$ that resemble a certain data set $(y_j)_j$, given a random input $z$ in a latent space $\Zz$. Both methods require a second network for the training of the generative network (an adversial network in the case of GANs, an encoding network in the case of VAEs). Depending on the complexity of the data, our method can rely on the generative network alone by directly comparing its output with the data in Wasserstein distance.

\paragraph{With a fixed cost $c$} This section fits a generative model where the pushforward $g_\th$ is a multilayer perceptron. We begin with experiments on the MNIST dataset, which is a standard benchmark for this type of networks. Since the dataset is relatively simple, learning the cost is superfluous here and we use the ground cost $c(x,y)=\norm{x-y}^2$, which is sufficient for these low resolution images and also the baseline in~\cite{VAE}. We use as $g_\th$ a multilayer perceptron with a 2D latent space $\Zz=\RR^2$. It is composed of 2 fully connected layers: one hidden layer of 500 units and an output layer which is the same size as the data ($\Xx = \RR^{28 \times 28}$). The parameters $\theta$ are thus the weights of both layers, and are initialized with the Xavier method~\cite{Xavier}. We choose $\zeta$ to be a uniform distribution over the unit square $[0,1]^2$. Learning is performed in mini-batches over the MNIST dataset, with the Adam optimizer~\cite{kingma2014adam}.

Figure~\ref{fig:MNIST} displays the manifold of images $g_\th(z)$ generated by the optimized network (i.e. for equi-spaced $z \in [0,1]^2$) after the learning procedure for different values of the hyperparameters $(\epsilon,m,L)$. This shows that the regularization parameter $\epsilon$ can be chosen quite large, which in turn leads to a fast convergence of Sinkhorn iterations. Indeed, using $\epsilon = 1$ with only $L=10$ Sinkhorn iterations (image~(a)) yields a result similar to using $\epsilon = 0.1$ with $L=100$ iterations (image (b)). Regarding the size $m$ of the mini-batches, a too small $m$ value (e.g. $m=10$) leads to poor results, and we observe that $m=200$ is sufficient to learn accurately the manifold. 

\begin{figure*}
%\centeringtextwidth
\centering
\begin{subfigure}{0.28\linewidth}
\includegraphics[width=\linewidth]{images/manifold_bigeps.png} 
\caption*{\hspace*{5pt}(a) $\epsilon = 1$ \\ \hspace*{5pt}$m = 200 , L = 10$}
\end{subfigure}%
\quad
\begin{subfigure}{0.28\linewidth}
\includegraphics[width=\linewidth]{images/manifold_ref.png} 
\caption*{\hspace*{5pt}(b) $\epsilon = 10^{-1}$ \\ \hspace*{5pt}$m = 200 , L = 100$}
\end{subfigure}%\begin{subfigure}{0.6\linewidth}
\quad
\begin{subfigure}{0.28\linewidth}
\includegraphics[width=\linewidth]{images/manifold_smallm.png} 
\caption*{\hspace*{5pt}(c) $\epsilon = 10^{-1}$\\ \hspace*{5pt}$m = 10, L = 300$}
\end{subfigure}%
\caption{Influence of the hyperparameters on the manifold of generated digits.
\label{fig:MNIST}}
\end{figure*}


\begin{figure*}[ht]
\centering
\begin{subfigure}{0.3\linewidth}
\includegraphics[width=\linewidth]{images/cifar-mmd.png} 
\caption{MMD}
\end{subfigure}%
\hspace*{7pt}
\begin{subfigure}{0.3\linewidth}
\includegraphics[width=\linewidth]{images/cifar-1.png} 
\caption{$\epsilon = 1000$}
\end{subfigure}% 
\hspace*{7pt}
\begin{subfigure}{0.3\linewidth}
\includegraphics[width=\linewidth]{images/cifar-2.png} 
\caption{$\epsilon = 10$}
\end{subfigure}%
\caption{Samples from the generator trained on CIFAR 10 for MMD and Sinkhorn loss (coming from the same samples in the latent space)}
\label{fig:cifar10}
\end{figure*}

\paragraph{Learning the cost} With higher-resolution datasets, such as classical benchmarks CIFAR10 or CelebA, using the $\ell^2$ metric between images yields very poor results. It tends to generate images which are basically a blur of similar images. The alternative, already outlined in Algortithm 1 relies on learning another network wich encodes meaningful feature vectors for the images, between which can take the euclidean distance.

We compare our loss with different values for the regularization parameter $\epsilon$ to the results obtained with an MMD loss with a gaussian kernel. The experimental setting is the same as in~\cite{MMDGAN} and we used the same parameters to carry out a fair comparison. 


\begin{table}
\centering
%\begin{minipage}[t]{\linewidth}
%\begin{table}
 \begin{tabular}{c c c c  }
        MMD  & $\epsilon = 1000$ & $\epsilon = 100$ & $\epsilon = 10$   \\  \hline 
     $4.04 \pm 0.07$ & $4.14 \pm 0.06$  &  $3.09 \pm 0.036$    &  $3.11 \pm 0.031$     
    \end{tabular}
   % \end{table}
%\end{minipage}
\caption{Inception Scores} \label{tab:inception}
  \end{table}


Table~\ref{tab:inception} summarizes the inception scores on CIFAR10 for MMD and Sinkhorn loss with varying $\epsilon$. Generative models are very hard to evaluate and there is no consensus on which metric should be used to assess their quality. We choose the inception score introduced in~\cite{inception} as it is well spread, and also the reference in~\cite{MMD-GAN} agains which we compare our losses. The scores are evalutated on 20000 random images.  Figure~\ref{fig:cifar10} displays a few of the associated samples (generated with the same seed). Although there is no striking difference in visual quality, the model with a Sinkhorn loss and a large regularization is the one with the best score. The poor scores of models which have a loss closer to the true OT loss can be explained by two main factors : (i)~the number of iterations required for the convergence of Sinkhorn with such $\epsilon$ might exceed the total iteration budget that we give the algorithm to compute the loss (to ensure reasonable training time of the model), (ii)~it reflects the fact that sample complexity worsens when we get closer to OT metrics, and increasing the batch size might be beneficial in that case.

  
