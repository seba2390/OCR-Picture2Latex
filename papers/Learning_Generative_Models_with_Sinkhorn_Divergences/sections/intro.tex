% !TEX root = ../SinkhornDivergences.tex

\section{Introduction}

Several important statistical problems boil down to fitting densities, \emph{i.e.} estimating the parameters of a chosen model that \emph{fits} observed data in some meaningful way.
%
While the standard approach is maximum likelihood estimation, this approach is often flawed in machine learning tasks where the sought after distribution is obtained in a generative fashion, \emph{i.e.} described using a sampling mechanism (often a non-linear function mapping a low dimensional latent random vector to a high dimensional space). Indeed, in these settings, the density is singular in the sense that it only has positive probability on a low-dimensional manifold of the observation space and is zero elsewhere. 
% %%%%%%%%%%%% "expensive to evaluate" est  aussi un probleme auquel nous devons faire face}
%
To remedy these issues, and in line with several recent proposals~\cite{bassetti2006minimum,montavon2016wasserstein,bernton2017inference,WassersteinGAN}, we propose to shift away from information divergence based methods (among which the MLE) and consider instead the geometry of optimal transport~\cite{villani2003,santambrogio2015optimal} to define such a fitting criterion.

\begin{figure*}[ht]
\centering
	\includegraphics[width=\linewidth]{images/fig-general_mc}
\caption{\label{fig-workflow} % 
	For a given fixed set of samples $(z_1,\ldots,z_m)$, and input data $(y_1,\ldots,y_n)$, flow diagram for the computation of Sinkhorn loss function $\th \mapsto \hat E_\epsilon^{(L)}(\th)$. This function is the one on which automatic differentiation is applied to perform parameter learning. The display shows a simple 2-layer neural network $g_\th : z \mapsto x$, but this applies to any generative model.
	}
\end{figure*}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Previous works.}

For purely generative models, several likelihood-free workarounds exist. 
%
Major approaches include variational autoencoders (VAE)~\cite{VAE}, generative adversarial networks (GAN)~\cite{GAN} and several more variations including combinations of both~\cite{pmlr-v48-larsen16}. The adversarial GAN approach is implicitly geometric in the sense that it computes the best achievable classification accuracy (taking for granted the training and generated datapoints have opposite labels) for a given class of classifiers as a proxy for the distance between two distributions: If accuracy is high distributions are well separated, if accuracy is low they are difficult to tell apart and lie thus at a very close distance.

Geometry was also explicitly considered when trying to minimize a flexible metric between distributions: the maximal mean discrepancy~\cite{gretton2007kernel}. It was shown in ensuing works that the effectiveness of the MMD in that setting~\cite{li2015generative,MMD-GAN} hinges on the ability to find a relevant RKHS bandwidth parameter, which is a highly nontrivial choice.
%
The Wasserstein or earth mover's distance, long known to be a powerful tool to compare probability distributions with non-overlapping supports, has recently emerged as a serious contender to train generative models.
% 
While it was long disregarded because of its computational burden---in its original form solving OT amounts to solving an expensive network flow problem when comparing discrete measures in metric spaces---recent works have shown that this cost can be largely mitigated by settling for cheaper approximations obtained through strongly convex regularizers, in particular entropy~\cite{CuturiSinkhorn,2016-genevay-nips}. The benefits of this regularization has opened the path to many applications of the Wasserstein distance in relevant learning problems~\cite{courty2014domain,2015-Frogner,NIPS2016_6139,pmlr-v51-rolet16}. 
%
Although the use of Wasserstein metrics for inference in generative models was considered over ten years ago in~\cite{bassetti2006minimum}, that development remained exclusively theoretical until a recent wave of papers managed to implement that idea more or less faithfully using several workarounds: entropic regularization over a discrete space~\cite{montavon2016wasserstein}, approximate Bayesian computations~\cite{bernton2017inference} and a neural network parameterization of the dual potential arising from the dual OT problem when considering the 1-Wasserstein distance~\cite{WassersteinGAN}.
%
As opposed to this dual way to compute gradients of the fitting energy, we advocate for the use of a primal formulation, which is numerically stable, because it does not involve differentiating the (dual) solution of an OT sub-problem, as also pointed out in~\cite{Bousquet2017}.
%
Additionally, introducing entropic regularization in the formulation of optimal transport allows to interpolate between a pure OT loss and a Maximum Mean Discrepency loss, thus bridging the gap between these two approaches often presented as opposed points of view.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Contributions.}

The main contributions of this paper are twofold : (i) a theoretical comtribution regarding a new OT-based loss for generative models, (ii) a simple numerical scheme to learn under this loss. (i) We introduce the Sinkhorn loss, based on regularized optimal transport with an entropy penalty, and we prove that when the smoothing parameter $\epsilon=+0$ we recover pure OT loss whereas letting $\epsilon=+\infty$ leads to MMD. The addition of entropy is important to reduce sample complexity and gradient bias, and thus allows us to take advantage of the good geometrical properties of OT without its drawbacks in high-dimensions. (ii) We propose a computationally tractable and stable approach to learn with that Sinkhorn loss, which enables inference for any differentiable generative model. It operates by adding $L$ additional pooling layers (application of a filtering kernel $K$ and pointwise divisive non-linearities), as illustrated on Figure~\eqref{fig-workflow}. As routinely done in standard deep-learning architecture frameworks, the training is then achieved using stochastic gradient descent and automatic differentiation. This provides accurate and stable approximation of the loss and its gradient, at a reasonable extra computational cost, and streams nicely on GPU hardware. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Notations.}

For a matrix $A$, $A^\top$ denotes its transpose. 
%
For two vectors (or matrices) $\dotp{u}{v} \eqdef \sum_i u_i v_i$ is the canonical inner product (the Frobenius dot-product for matrices).
%
We define $\ones_{\m} \eqdef (1/\m,\ldots,1/\m) \in \RR_+^{\m}$ the uniform histogram, so that for $\P \in \RR^{\n \times \m}$, $\P \ones_{\m} \in \RR^\n$ and $\P^\top \ones_{\n} \in \RR^\m$ stand for the row and column averages of $P$.
%
We denote $\Mm_+^1(\Xx)$ the set of probability distributions (positive Radon measures of unit mass) over a metric space $\Xx$. $\de_x$ stands for the Dirac (unit mass) distribution at point $x \in \Xx$.
%
For some continuous map $g : \Zz \rightarrow \Xx$, we denote $g_\sharp : \Mm_+^1(\Zz) \rightarrow \Mm_+^1(\Xx)$ the associated push-forward operator, which is a linear map between distributions. This corresponds to defining, for $\zeta \in \Mm_+^1(\Zz)$ and $B \subset \Xx$, $(g_\sharp \zeta)(B) = g^{-1}(B)$ ; or equivalently, that $\int_\Xx \phi \d(g_\sharp \zeta) = \int_\Zz \phi \circ g \d \zeta$ for continuous functions $\phi$ on $\Xx$ ; or equivalently that a random sample $x$ from $g_\sharp \zeta$ can be obtained as $x=g(z)$ where $z$ is a random sample from $\zeta$. 
