% !TEX root = ../document.tex
\section{Introduction}

A common goal in uncertainty quantification \cite{LeMaitreKnio2010} is the approximation of response surfaces
\begin{equation*}
\psmi\mapsto \rs(\psmi):=\QoI(\pde_{\psmi})\in\R,
\end{equation*}
which describe how a quantity of interest $\QoI$ of the solution $\pde_{\psmi}$ to some partial differential equation (PDE) depends on parameters $\psmi\in\domPS\subset\R^\dps$ of the PDE. The non-intrusive approach to this problem is to evaluate the response surface for finitely many values of $\psmi$ and then to use an interpolation method, such as (tensor-)spline interpolation \cite{deb2001solution}, kernel-based approximation (kriging) \cite{WolfersNobileTempone2016a,GriebelRieger2015}, or (global) polynomial approximation \cite{LeMaitreKnio2010}.

 In this work, we study a variant of polynomial approximation in which least squares projections onto finite-dimensional polynomial subspaces are computed using values of $\rs$ at finitely many random locations. More specifically, given a probability measure $\measure$ on the parameter space $\domPS$ and a polynomial subspace $\vsp\subset L^2_{\measure}(\domPS)$, the approximating polynomial is determined as
\begin{equation}\label{eq:P0}
\P_\vsp \rs:=\argmin_{\p\in \vsp}\norm{\rs-\p}{\NS},
\end{equation}
where $\norm{\cdot}{\NS}$ is a discrete approximation of the $L^2_{\measure}(\domPS)$ norm that is based on evaluations in finitely many randomly chosen sample locations $\psmi_j\in\domPS$, $j\in\{1,\dots,\NS\}$ and a weight function $\weight\colon\domPS\to\R$. 

The case where equally weighted samples are drawn independently and identically distributed from the underlying probability measure itself, $\psmi_j\sim\measure$, has been popular among practitioners for a long time and has been given a thorough theoretical foundation in the past decade  \cite{ChkifaCohenMiglioratiEtAl2015,Cohen2013,MiglioratiNobileTempone2015}. 
More recently, the use of alternative sampling distributions and non-constant weights was studied in \cite{narayan2014christoffel,cohen2016optimal,hampton2015coherence}. In particular, \cite{hampton2015coherence} presented a sampling distribution $\optimaldistribution_{\vsp}$ and a corresponding weight function for which the number of samples required to determine quasi-optimal approximations within $\vsp$ is bounded by $\dim \vsp$ up to a logarithmic factor. (This result was proved in \cite{hampton2015coherence} for total degree polynomial spaces and generalized in \cite{cohen2016optimal} to more general function spaces.) Since this distribution depends on $\vsp$, it is natural to ask how samples can be efficiently obtained from it and whether there is an alternative that works equally well for all polynomial subspaces $\vsp$. 
To address the first question, we present and analyze an efficient algorithm to generate samples from $\optimaldistribution_{\vsp}$ in the case where $\domPS$ is a product domain and $\measure$ is a product measure. For more general cases, we also study Markov chain methods for sample generation and analyze the effect of small perturbations of the sampling distribution on the convergence estimates of \cite{hampton2015coherence,cohen2016optimal}. To address the second question, we provide upper and lower bounds on $\optimaldistribution_{\vsp}$ in the case where $\domPS$ is a hypercube. The lower bound allows us to make the error estimates obtained in \cite{cohen2016optimal} more explicit. The upper bound shows that the arcsine distribution, which was proposed in \cite{narayan2014christoffel}, performs just as well as $\optimaldistribution_{\vsp}$ up to a constant that is independent of $\vsp$. This is advantageous for adaptive algorithms in which the polynomial subspace and the corresponding optimal sampling distribution vary during the iterations. However, the constant mentioned above does depend exponentially on the dimension $\dps$ of $\domPS$. 
\\

 To motivate the main contribution of this work, namely the multilevel weighted least squares polynomial approximation method, we note that the response surface $\rs$ from the beginning of this introduction cannot be evaluated exactly. Indeed, in most cases, the computation of $Q(\pde_{\psmi})$ requires the numerical solution of a PDE. Thus, we can only compute approximations of $\rs$ whose accuracy and computational work are determined by the PDE discretization. If we simply applied  polynomial least squares approximation using a sufficiently fine discretization of the PDE for all evaluations, then we would quickly face prohibitively long runtimes. For this reason, we introduce a multilevel method that combines lots of cheap samples using coarse discretizations with relatively few more expensive samples using fine discretizations of the PDE. In the recent decade, such multilevel algorithms have been studied intensely for the approximation of expectations \cite{harbrecht2013multilevel,heinrich2001multilevel,KuoScheichlSchwabEtAl2015,Haji-AliNobileTamelliniEtAl2015}. The goal of this paper is to extend this earlier work to the reconstruction of the full response surface, using global polynomial approximation and estimating the resulting error in the $L^2_{\measure}$ norm.
 
To describe the multilevel method, assume that we want to approximate a function $\rs$. Assume furthermore that we can only evaluate functions $\rs_{l}$ with $\rs_{l}{\rightarrow}\rs$ as $l\to\infty$ in a suitable sense and that the cost per evaluation increases as $l\to\infty$. A straightforward approach to this situation is to apply least squares approximation to some $\rs_{L}$ that is sufficiently close to $\rs$. The theory of (weighted) polynomial least squares approximation then provides conditions on the number of samples required to achieve quasi-optimal approximation of $\rs_{L}$ within a given space of polynomials $\vsp_{L}$. However, this approach can be computationally expensive, as each evaluation of $\rs_{L}$ requires the numerical solution of a PDE using a fine discretization. As an alternative, our proposed multilevel algorithm starts out with a least squares approximation of $\rs_{0}$ using a relatively large polynomial subspace $\vsp_0$ and correspondingly many samples. To correct for the committed error $\rs-\rs_{0}$, the algorithm then adds polynomial approximations of $\rs_{l}-\rs_{l-1}$ that lie in subspaces $\vsp_l$, $l\in\{1,\dots,\L\}$. 
%If all the polynomial subspaces $\vsp_l$ coincided with $\vsp_{L}$, then this approach would require at least as much work as the straightforward approximation of $\rs_{L}$ in $\vsp_{L}$. However, 
Since we assume that $\rs_{l}\to\rs$ in an appropriate sense, the differences $\rs_{l}-\rs_{l-1}$ may be approximated using smaller polynomial subspaces for $l\to\infty$. Exploiting this fact, it is possible to obtain approximations with significantly reduced computational work. 
Indeed, we show that under certain conditions the work that the multilevel method requires to attain an accuracy of  $\epsilon>0$ is the same as the work that regular least squares polynomial approximation would require if $\rs$ could be evaluated exactly. It is clear that such a result is not always possible. For example, if $\rs$ were constant, then polynomial least squares approximations in any fixed polynomial subspace would yield the exact solution given a sufficiently large sample size. This means that the work required to achieve an accuracy $\epsilon>0$ would be bounded as $\epsilon\to 0$, which can clearly not be true for an algorithm that uses evaluations from approximate functions $\rs_l$ that become more expensive to evaluate as $l\to\infty$. Instead, the optimal computational work required for an accuracy of $\epsilon>0$ by such an algorithm would be determined by the convergence of $\rs_l\to \rs$ and by the work that is required for evaluations of $\rs_l$. 
Our results show that for many problems, the two cases described above are dichotomous. This means that the total computational work is determined either by the convergence and work associated with $\rs_{l}$ or by the convergence of polynomial least squares approximation using exact evaluations (see \Cref{thm:main} for a more formal statement). \\


The remainder of this paper is structured as follows. In \Cref{sec:dpls}, we review the theoretical analysis of weighted least squares approximation. In \Cref{sec:sampling}, we discuss different sampling strategies. We propose algorithms to sample the optimal distribution and we discuss the consequences of using perturbed distributions. In \Cref{sec:nonadaptive}, we introduce a novel multilevel algorithm and prove our main results concerning the work and convergence of this algorithm. For situations in which the regularity of $\rs$ and the convergence of $\rs_l$ are not known, we propose an adaptive algorithm in \Cref{sec:adaptive}. We discuss the applicability of our method to problems in uncertainty quantification in \Cref{sec:uq}. Finally, we present numerical experiments in \Cref{sec:numerics}.
