\section{Weighted least squares polynomial approximation}

\label{sec:dpls}
In this section, we provide a short summary of the theory of weighted discrete least squares polynomial approximation, closely following \cite{cohen2016optimal}.

Assume that we want to approximate a function $\rs\in L^2_{\measure}(\domPS)$, where $\domPS\subset\R^\dps$ and $\measure$ is a probability measure on $\domPS$. % Here, we denote by $L^2_{\measure}(\domPS)$ the space of Lebesgue measurable functions that are square-integrable functions with respect to $\measure$, which we abbreviate by $L^2(\measure)$ whenever the domain is clear from the context. 
The strategy of weighted discrete least squares polynomial approximation is to 
\begin{itemize}
	\item choose a finite-dimensional space $\vsp\subset L^2_{\measure}(\domPS)$ of polynomials on $\domPS$
	\item choose a function $\density\colon\domPS\to\R$ that satisfies $\int_{\domPS}\density(\psmi) \measure(d\psmi)=1$ and $\density>0$
	\item generate $\NS>0$ independent random samples from the \emph{sampling distribution} $\samplingmeasure$ defined by $\frac{\text{d}\samplingmeasure}{\text{d}\measure}:=\density$, 
	$$
	\psmi_j\sim \samplingmeasure, \quad j\in\{1,\dots,\NS\}.
	$$
	Here, $\frac{\text{d}\samplingmeasure}{\text{d}\measure}$ denotes the density, or Radon-Nikodym derivative, of the probability measure $\samplingmeasure$ with respect to the reference measure $\measure$.
	\item evaluate $\rs$ at $\psmi_j$, $j\in\{1,\dots,\NS\}$
	
	\item define the \emph{weight function} $\weight:=\frac{1}{\density}\colon \domPS\to\R$
	\item  and finally define the \emph{weighted discrete least squares approximation}
	\begin{equation}
	\label{eq:dpls:def}
	\P_{\vsp}\rs:=\argmin_{\p\in \vsp} \norm{\rs-\p}{\NS},
	\end{equation}
	where 
	\begin{equation}
	\label{eq:dpls:discretenorm}
	\norm{\f}{\NS}^2:=\langle \f,\f\rangle_{\NS}:=\frac{1}{\NS}\sum_{j=1}^{\NS} w(\psmi_j)|\f(\psmi_j)|^2\quad \forall \f\colon\domPS\to\R.
	\end{equation}
\end{itemize}
It is straightforward to show that the coefficients $\mathbf{\p}$ of $\P_{\vsp}\rs$ with respect to any basis $(\onb_j)_{j=1}^{\dvsp}$ of $\vsp$ are given by
\begin{equation}
\label{eq:dpls:computation}
	\mathbf{G}\mathbf{\p}=\mathbf{\coeff},
\end{equation}
with $\mathbf{G}_{ij}:=\langle \onb_i,\onb_j\rangle_{\NS}$, and $\coeff_j:=\langle \rs,\onb_j\rangle_{\NS}$, $i,j\in\{1,\dots,\dvsp\}$, assuming that $\mathbf{G}$ is invertible. If $\mathbf{G}$ is not invertible, then \Cref{eq:dpls:def} has multiple solutions and we define $\Pi_{\vsp}\rs$ as the one with the minimal $L^2_{\measure}(\domPS)$ norm. 
\begin{rem}
	\label{rem:matvec}
	Assembling the matrix $\mathbf{G}$ requires $\mathcal{O}(m^2N)$ operations. However, using the fact that $\mathbf{G}=\mathbf{M}^{\top}\mathbf{M}$ for $\mathbf{M}_{ij}:=N^{-1/2}\sqrt{\weight(\psmi_i)}\onb_j(\psmi_i)$, matrix vector products with $\mathbf{G}$ can be computed at the lower cost $\mathcal{O}(mN)$ as $\mathbf{G}\mathbf{x}=\mathbf{M}^{\top}(\mathbf{M}\mathbf{x})$. 
\end{rem}

Since $\weight\density=1$, the semi-norm defined in \Cref{eq:dpls:discretenorm} is a Monte Carlo approximation of the $L^2_{\measure}(\domPS)$ norm. Therefore, we may expect that the error $\norm{\rs-\P_{\vsp}\rs}{L^2_{\measure}(\domPS)}$ is close to the optimal one,
\begin{equation}\label{eq:best2}
e_{\vsp,2}(\rs):=\min_{\p\in\vsp}\norm{\rs-\p}{L^2_{\measure}(\domPS)}.
\end{equation}
Part (iii) of \Cref{thm:dpls} below shows that this is true in expectation, provided that the number of samples $\NS$ is coupled appropriately to the dimension $\dvsp=\dim\vsp$ of the approximating polynomial subspace and provided that we ignore outcomes where $\mathbf{G}$ is ill-conditioned.  For results in probability, we need to replace the best $L^2_{\measure}(\domPS)$ approximation by the best approximation in a weighted supremum norm,
\begin{equation}
\label{eq:bestlinf}
e_{\vsp,\weight,\infty}(\rs):=\inf_{\p\in\vsp}\sup_{\psmi\in\domPS}|\rs(\psmi)-\p(\psmi)|\sqrt{\weight(\psmi)}.
\end{equation}

%Whenever we talk about weighted least squares approximation in the remainder of this work, we imply that the density $\density_{*}$ and the corresponding optimal weight function $\weight_*=1/\density_*$ are being used.
\begin{thm}[\textbf{Convergence of weighted least squares, \cite[Theorem 2]{cohen2016optimal}}]
	\label{thm:dpls}
For arbitrary $r>0$, define 
$$\kappa:=\frac{1/2-1/2\log 2}{1+r}.$$ Assume that for all $\psmi\in\domPS$ there exists $\p\in\vsp$ such that $\p(\psmi)\not =0$ and denote by $(\onb_j)_{j=1}^{\dvsp}$ an $L^2_{\measure}$-orthonormal basis of $\vsp$. Finally, assume that 
\begin{equation}
\label{eq:K}
K_{\vsp,\weight}:=\norm{\weight \sum_{j=1}^{\dvsp}\onb_j^2}{L^\infty(\domPS)}\leq \kappa\frac{\NS}{\log \NS}.
\end{equation}
\begin{enumerate}[(i)]
	\item With probability larger than $1-2\NS^{-r}$, we have
	\begin{equation}
	\|\mathbf{G}-\mathbf{I}\|\leq\frac{1}{2},
	\end{equation}
	where $\mathbf{G}$ is the matrix from \Cref{eq:dpls:computation}, $\mathbf{I}$ is the identity matrix, and $\|{\cdot}\|$ denotes the spectral matrix norm.
	\item If $\|{\mathbf{G}-\mathbf{I}}\|\leq 1/2$, then for all $\rs$ with $\sup_{\psmi\in\domPS}|\rs(\psmi)|\sqrt{\weight(\psmi)}<\infty$, we have
	\begin{equation*}
		\norm{\rs-\P_{\vsp}\rs}{L^2_{\measure}(\domPS)}\leq (1+\sqrt{2})e_{\vsp,\weight,\infty}(\rs).
	\end{equation*}
 %\item if $\|\rs\|_{L^\infty}\leq\tau$, then
 %\begin{equation*}
 %E(\rs-\P^{\tau}_{\vsp}\rs))^2\leq (1+\frac{4\kappa}{\log %\NS})e^2_{\vsp,2}(\rs)+8\tau^2\NS^{-r},
 %\end{equation*}
%where $(\P^{\tau}_{\vsp}\rs)(x):=\sign(\P_{\vsp}\rs(x))\min\{\P_{\vsp}\rs(x),\tau\}$
\item If $\rs\in L^2_{\measure}(\domPS)$, then 
\begin{equation*}
\expect\norm{\rs-\P^{\cond}_{\vsp}\rs}{L^2_{\measure}(\domPS)}^2\leq (1+\frac{4\kappa}{\log \NS})e^2_{\vsp,2}(\rs)+2\norm{\rs}{L^2_{\measure}(\domPS)}^2\NS^{-r},
\end{equation*}
where $\expect$ denotes the expectation with respect to the $\NS$-fold draw from the sampling distribution $\samplingmeasure$ and 
\begin{equation*}
\P^{\cond}_{\vsp}\rs:=\begin{cases}
\P_{\vsp}\rs\quad\text{if }\|{\mathbf{G}-\mathbf{I}}\|\leq\frac{1}{2},\\
0\quad\text{otherwise}.
\end{cases}
\end{equation*}
\end{enumerate}
\end{thm}
\begin{proof}
	It is proved in \cite[Theorem 2]{cohen2016optimal} that the bound in part (ii) holds for a fixed $\rs$ with probability larger than $1-2\NS^{-r}$. A look at the proof reveals that the bound only depends on the event $\|{\mathbf{G}-\mathbf{I}}\|\leq 1/2$ and not on the specific choice of $\rs$. The remaining claims are exactly as in \cite{cohen2016optimal}.
\end{proof}



%\begin{rem}
%	The work \cite{cohen2016optimal} also discusses sampling strategies to obtain samples from $\density$ for general $\measure$.% When $\domPS:=[0,1]^d$ and $\measure$ is the Lebesgue measure, it can be shown that $\frac{c}{\pi\sqrt{x(1-x)}}\leq \density \leq \frac{C}{\pi\sqrt{x(1-x)}}$ for $0<c<C<\infty$ and that the results of the previous theorem hold true if we replace $\density$ by $\frac{1}{\pi\sqrt{x(1-x)}}$ and $\kappa$ by $\frac{C}{c}\kappa$ \cite{cohen2016optimal}.\swerror{only in 1d}
%\end{rem}