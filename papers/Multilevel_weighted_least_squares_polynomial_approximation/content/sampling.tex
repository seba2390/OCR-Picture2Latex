% !TEX root = ../document.tex

\section{Sampling strategies}
\label{sec:sampling}
It was observed in \cite{cohen2016optimal} that the constant $K_{\vsp,\weight}$ in \Cref{eq:K} satisfies
\begin{equation}
\label{eq:optimalK}
\aligned
m  = & \int \sum_{j=1}^{\dvsp} \onb_j(\psmi)^2 \measure(d\psmi)\\
\le & \left( \int  w^{-1}(\psmi) \measure(d\psmi) \right)  \left\| \sum_{j=1}^{\dvsp} w \onb_j^2 \right\|_{L^\infty(\domPS)} \\
 = & K_{\vsp,\weight}
\endaligned
\end{equation}
and that the inequality becomes an equality for the weight $\weight^{*}_{\vsp}={\optimaldensity_{\vsp}}^{-1}$ that is associated with the density
\begin{equation}
\label{eq:optimalmeasure}
\optimaldensity_{\vsp}(\psmi):=\frac{1}{\dvsp}\sum_{j=1}^{\dvsp}|\onb_j(\psmi)|^2.
\end{equation}
For this choice, \Cref{thm:dpls} roughly asserts that the number of samples required to determine a near-optimal approximation of $\rs$ in an  $\dvsp$-dimensional space $\vsp$ is smaller than $C\dvsp \log \dvsp$ for some $C>0$. In the remainder of this work, we refer to $\optimalweight_{\vsp}$, $\optimaldensity_{\vsp}$, and
\begin{equation}
\label{eq:optimaldistribution}
\optimaldistribution_{\vsp}\colon \quad \frac{\mathrm{d}\optimaldistribution_{\vsp}}{\text{d}\measure}:=\optimaldensity_{\vsp}
\end{equation}
as the \emph{optimal} weight, density, and distribution, respectively.
Since the optimal distribution $\optimaldistribution_{\vsp}$ depends on $\vsp$, practical implementations need to address the question how to obtain samples from $\optimaldistribution_{\vsp}$
 for general subspaces $\vsp$. Furthermore, since $\optimaldensity_{\vsp}$ depends on $\vsp$, the weight in $e_{\vsp,\infty}(\rs)$ in part (ii) of \Cref{thm:dpls} does as well. To address these issues, we present two types of results in this section.

First, we discuss how to obtain samples from $\optimaldistribution_{\vsp}$.
For the case where $\domPS$ is a $d$-dimensional hypercube and $\measure=\bigotimes_{j=1}^{d}\measure_{j}$ with $\measure_{j}$ satisfying certain assumptions, we propose a method for the generation of $\NS$ samples whose computational work is bounded in expectation by the product $Kd\NS$ with a constant $K$ that depends only on the measures $\measure_{j}$.
For non-product domains or measures, we briefly discuss how to use Markov chain Monte Carlo (MCMC) sampling for the generation of samples from approximate distributions and how perturbations of the sampling distributions affect the error estimates.

Second, we prove for the case $\domPS=[0,1]^d$ and under a rather permissive assumption on $\measure$ (in particular, $\measure$ must be absolutely continuous with respect to the Lebesgue measure) that for any polynomial subspace $\vsp$ the density of the optimal distribution $\optimaldistribution_{\vsp}$ with respect to the Lebesgue measure $\lambda$ satisfies
\begin{equation}
\label{eq:temparcsine}
C^{-d}<\frac{\text{d}\optimaldistribution_{\vsp}}{\text{d}\lambda}\leq C^d \arcsine_{d},
\end{equation}
where $0<C<\infty$ is independent of $\vsp$, and $\arcsine_{d}$ is the Lebesgue density of the $d$-dimensional arcsine distribution,
\begin{equation}
\label{eq:arcdef}
\arcsine_{d}(\psmi):=\prod_{j=1}^{d}\frac{1}{\pi\sqrt{\psmi_j(1-\psmi_j)}}.
\end{equation}

The lower bound in \Cref{eq:temparcsine} implies that the optimal weight $\optimalweight_{\vsp}$ is bounded above by $C^{d}\frac{d\measure}{d\lambda}$, which can be used to make the error estimate in part (ii) of \Cref{thm:dpls} more explicit.

By the upper bound, we may use samples from the $d$-dimensional arcsine distribution instead of the optimal distribution.
	Indeed, the upper bound implies that the weight function $\weight$ associated with the arcsine distribution satisfies $K_{\vsp,\weight}\leq C^dm$. Thus, the required number of samples is increased by at most a factor that is independent of $\vsp$. The advantages are that samples from the arcsine distribution can be generated efficiently, that we can use samples from the same distribution for all polynomial subspaces, and that the weight $\weight$ is easy to analyze and independent of $\vsp$.



\subsection{Sampling from the optimal distribution}\label{sec:optimal-sampling}
%\subsubsection{Exact sampling}
We now describe an efficient algorithm to obtain samples from $\optimaldistribution_{\vsp}$ in the case when $\domPS$ is a Cartesian product,  $\measure$ is a product measure, and $\vsp$ is downward closed.
\begin{definition}[\textbf{Downward closedness}]
	Let $\N:=\{0,1,\dots\}$.
	A set $\mis\subset\N^d$ is called \emph{downward closed} if $\mip \in\mathcal{I}$ implies $\mipp\in\mathcal{I}$ for any $\mipp\in\N^{\dps}$ with $\mipp\leq \mip$ componentwise.

	A space $\vsp$ of polynomials on a Cartesian product domain $\domPS=\prod_{j=1}^{d}I_j$
	with $I_j\subset\R$ is called \emph{downward closed} if
	it is the span of monomials,
	 $$
	\vsp=\operatorname{span}\{\psmi^{\mip}=\prod_{j=1}^{\dps}\ps_{j}^{\mip_j}:\mip\in\mathcal{I}\},
	$$
	for some downward closed set $\mis\subset \N^d$.
\end{definition}
\begin{rem}
	Observe that any non-trivial downward closed polynomial space $\vsp$ includes the constant functions and thus satisfies the assumption of \Cref{thm:dpls} that for all $\psmi\in\domPS$ there exists $\p\in\vsp$ with $\p(\psmi)\not =0$.
\end{rem}
We first discuss the case $\domPS=[0,1]^d$ and $\measure=\lambda$ the Lebesgue measure. For any downward closed subspace
\begin{equation*}
\vsp=\vspan\{\psmi^{\mip}:\mip\in\mis\}\subset L^2_{\lambda}([0,1]^d)
\end{equation*} with $\mis\subset\N^{d}$ and $|\mis|=\dim \vsp=\dvsp$, 
an orthonormal basis is then given by
\begin{equation*}
(\leg_{\mip})_{\mip\in\mis}
\end{equation*}
where
\begin{equation*}
\leg_{\mip}(\psmi):=\prod_{j=1}^{d}\leg_{\mip_j}(\psmi_j)
\end{equation*}
and $(\leg_n)_{n\in \N}$ are the Legendre polynomials on $[0,1]$, which are orthonormal with respect to the one-dimensional Lebesgue measure. By orthonormality, each $\leg_{\mip}^2$ may be interpreted as a probability density with respect to the Lebesgue measure. Thus,
\begin{equation*}
\frac{\text{d}\optimaldistribution_{\vsp}}{\text{d}\lambda}=\optimaldensity_{\vsp}=\frac{1}{\dvsp}\sum_{\mip\in\mis}\leg_{\mip}^2
\end{equation*} may be interpreted as mixture of $\dvsp$ probability densities. An efficient strategy to obtain samples from $\optimaldistribution_{\vsp}$ is therefore to first choose $\mip\in\mis$ at random and then generate a sample from the distribution with Lebesgue density $\leg^2_{\mip}$. Since $\leg^2_{\mip}=\prod_{j=1}^{d}L^2_{\mip_j}$, samples from this distribution can be generated componentwise. Finally, to obtain samples from the univariate distributions with Lebesgue densities $\leg^2_{n}$, $n\in\N$, we use a rejection sampling method with the arcsine proposal density $\arcsine_{1}$. By \cite[Theorem 1]{nevai1994generalized} the Legendre polynomials satisfy
 \begin{equation}
 \label{eq:rejectionbound}
|\leg_{n}(\psmi)|^2\leq 4e\arcsine_{1}(\psmi)\quad\forall \psmi\in[0,1]\;\forall n\in\N.
 \end{equation}

 Therefore, the theory of rejection sampling \cite[Chapter 4.5]{MR2151519} ensures that if we repeatedly generate $\psmi\sim \arcsine_{1}$ and $U\sim\text{Unif}(0,1)$ until $U\leq |\leg_{n}(\psmi)|^2/(4e\arcsine_{1}(\psmi))$ holds, then the resulting sample is exactly distributed according to $\leg_n^2$ and the required number of iterations until acceptance has a geometric distribution with mean $4e$.
The total expected computational work for the generation of $\NS$ samples from $\optimaldistribution_{\vsp}$ is thus $4e\NS d$, if we assume that the computation of $\leg^2_n(\psmi)$ is $O(1)$. In practice, a 3-term recurrence formula whose work is bounded by $3n$ can be used to compute $\leg_{n}(\psmi)$. This increases  the upper bound for the expected  work to $12e\NS \frac{1}{\dvsp}\sum_{\mip\in\mis}|\mip|_{1}$.%ASSUMING ONB INDEX STARTS AT 1


\Cref{eq:rejectionbound} holds more generally for probability measures on $[0,1]$ with Lebesgue densities of the form $\frac{\text{d}\measure}{\text{d}\lambda}=C(\alpha,\beta)\psmi^{\alpha}(1-\psmi)^{\beta}$, $\alpha,\beta\geq -1/2$  \cite[Theorem 1]{nevai1994generalized}. The bound on the associated orthogonal polynomials $(\leg^{\alpha,\beta}_{n})_{n\in\N}$, which are commonly called Jacobi polynomials, is
\begin{equation*}
|\leg^{\alpha,\beta}_n(\psmi)|^2\frac{\text{d}\measure}{\text{d}\lambda}\leq 2e(2+\sqrt{\alpha^2+\beta^2})\arcsine_{1}(\psmi)\quad\forall\psmi\in[0,1]\;\forall n\in\N.
\end{equation*}


Even more generally, the same inequality holds with a constant $C_{\measure}$ independent of $\psmi$ and $n$ for orthogonal polynomials with respect to a wide class of measures $\mu$ that are absolutely continuous with respect to the Lebesgue measure on $[0,1]$ \cite[Theorem 12.1.4]{MR0372517}.
When $C_{\measure}$ is unknown, however, rejection sampling cannot be applied. As a substitute, we could use MCMC sampling (which we also discuss below as an alternative method to sample directly from $\optimaldistribution$ in cases when no product structure of $\domPS$ or $\measure$ can be exploited). The error resulting from the fact that the resulting samples would not be distributed exactly according to $|\leg_n|^2$ can be controlled using \Cref{pro:stability} below.

For orthonormal polynomials $(\her_n)_{n\in\N}$ with respect to rapidly decaying measures supported on the whole real line, such as Gaussian measures, it is shown in  \cite{levin1992christoffel} that $|\her_n(\psmi)|^2\frac{\text{d}\measure}{\text{d}\lambda}$ is exponentially concentrated in an interval $[-a_n,a_n]$ with $C^{-1}n^{b}\leq a_n\leq Cn^b$ for some $b>0$ and $C>0$ depending on $\measure$, and  that for some $C_{\measure}$
\begin{equation*}
|\her_n(\psmi)|^2\frac{\text{d}\measure}{\text{d}\lambda}\leq C_{\measure}\frac{a_n}{4}|1-\frac{\psmi}{a_n}|^{-1/2}\quad\forall \psmi\in[-a_n,a_n]\;\forall n\in\N.
\end{equation*}

 Together with the stability result in \Cref{pro:stability} below, this shows that the previous results can be transfered to measures on the real line, if we simply ignore the mass outside $[-a_n,a_n]$ and apply rejection sampling or Markov chain methods with the proposal density $\frac{a_n}{4}|1-\frac{\psmi}{a_n}|^{-1/2}$.
 Alternatively, a different result in \cite{levin1992christoffel} shows that on $[-a_n,a_n]$ the density $|\her_n(\psmi)|^2\frac{\text{d}\measure}{\text{d}\lambda}$ is bounded by the uniform probability density up to a factor that grows sublinearly in the polynomial degree $n$.
 \\

For the remainder of this subsection, we assume that a polynomial space $\vsp$ is fixed and use the simplified notation  $\optimaldistribution=\optimaldistribution_{\vsp}$, $\optimaldensity=\optimaldensity_{\vsp}$,  $\optimalweight=\optimalweight_{\vsp}$.

We assume that we cannot use exact samples from the optimal distribution because, for example, $\domPS$ is not a hypercube or $\measure$ not a product measure. We therefore discuss the use of Markov chain methods for the generation of samples. Since the resulting samples are not exactly distributed according to the optimal distribution, we need the following stability result.

\begin{pro}[\textbf{Stability with respect to perturbations of the sampling density}]
	\label{pro:stability}
	All results in \Cref{thm:dpls} that are valid for the optimal choice $\optimaldistribution$ with $\frac{\mathrm{d}\optimaldistribution}{\mathrm{d}\measure}=\optimaldensity$ of the sampling distribution hold true if we instead use samples from $\tilde{\samplingmeasure}$ with $\frac{\mathrm{d}\tilde{\samplingmeasure}}{\mathrm{d}\measure}:=\tilde{\density}$ (but keep the weight function $\optimalweight=1/\optimaldensity$), provided that
	\begin{equation*}
	\norm{1-\tilde{\density}/\optimaldensity}{L^p_{\optimaldistribution}(\domPS)}\leq \frac{1}{6}m^{-1-1/p} \quad\text{for some }p\geq 1,
	\end{equation*}
	where $\dvsp=\dim \vsp$,
	and provided that we replace $\kappa$ by $(5/36-5/6\log (5/6))/(1+r)$. We note that the total variation distance $\norm{\cdot}{\text{TV}}$ satisfies
	\begin{equation*}
	\norm{\tilde{\samplingmeasure}-\optimaldistribution}{\text{TV}}:=\frac{1}{2}\int_{\domPS}|\tilde{\density}(\psmi)-\optimaldensity(\psmi)|\measure(\mathrm{d}\psmi)=\frac{1}{2}\norm{1-\tilde{\density}/\optimaldensity}{L^1_{\optimaldistribution}(\domPS)}.
	\end{equation*}
\end{pro}

\begin{proof}
	The proof of \Cref{thm:dpls} in \cite{cohen2016optimal} is based on large deviation bounds for the matrix $\mathbf{G}$ of \Cref{eq:dpls:computation}. In particular, it is based on the observation that $\mathbf{G}$ is a Monte Carlo average,
	\begin{equation*}
	\mathbf{G}=\frac{1}{\NS}\sum_{i=1}^{\NS}\mathbf{X}_i,
	\end{equation*}
	of independent and identically distributed matrices
	\begin{equation*}
	\mathbf{X}_i:=\left(\optimalweight(\psmi_i)\onb_j(\psmi_i)\onb_k(\psmi_i)\right)_{j,k\in\{1,\dots,\dvsp\}} \quad \text{with }\psmi_i\sim \optimaldistribution
	\end{equation*}
	that satisfy
	\begin{equation*}
	\expect \mathbf{X}_i=\mathbf{I}
	\end{equation*}
	by $L^2_{\measure}(\domPS)$-orthonormality of the basis polynomials $\onb_j$, $j\in\{1,\dots,\dvsp\}$.
	A Chernoff inequality for matrices then provides the bound on $\prob(\|{\mathbf{G}-\mathbf{I}}\|\leq 1/2)$ in part (i) of \Cref{thm:dpls} from which everything else follows. The crucial insight is that this inequality permits small perturbations of the expected value. Indeed, if we replace $\optimaldistribution$ by $\tilde{\samplingmeasure}$ in the definition of $\mathbf{X}_i$, then  \cite[Theorem 1.1]{MR2946459} yields the same bound on $\prob(\|{\mathbf{G}-\mathbf{I}}\|\leq 1/2)$, with the new value of $\kappa$, provided that $\|{\expect\mathbf{X}_i-\mathbf{I}}\|\leq 1/6$. To show that this last estimate holds, we use
	\begin{equation*}
	\begin{split}\|{\expect\mathbf{X}_i-\mathbf{I}}\|&\leq \dvsp\norm{\expect\mathbf{X}_i-\mathbf{I}}{\text{max}}\\
	&=\dvsp\max_{j,k\in \{1,\dots,\dvsp\}}|\int_{\domPS}\optimalweight(\psmi)\onb_j(\psmi)\onb_k(\psmi)(\tilde{\density}(\psmi)-\optimaldensity(\psmi))\measure(d\psmi)|\\
	&\leq \dvsp\max_{j,k\in \{1,\dots,\dvsp\}}\norm{\optimalweight \onb_j\onb_k}{L^q_{\optimaldensity\measure}}\norm{1-\tilde{\density}/\optimaldensity}{L^{p}_{\optimaldensity\measure}},
	\end{split}
	\end{equation*}
	where we used Hölder's inequality with $1/q=1-1/p$ in the last step.

	The claim now follows if we can prove that
	\begin{equation*}
	\norm{\optimalweight \onb_j\onb_k}{L^q_{\optimaldensity\measure}}\leq m^{1-1/q}\quad\forall q\in[1,\infty].
	\end{equation*}
	For this, we first consider the case when $q=1$, $p=\infty$, for which
	\begin{equation*}
	\begin{split}
	\norm{\optimalweight \onb_j\onb_k}{L^q_{\optimaldensity\measure}}&=\int_{\domPS}|\onb_j(\psmi)\onb_k(\psmi)|\measure(d\psmi)\\
	&\leq \norm{\onb_j}{L^2_{\measure}}\norm{\onb_k}{L^2_{\measure}}\\
	&\leq 1
	\end{split}
	\end{equation*}
	by the Cauchy-Schwarz inequality and $L^2_{\measure}(\domPS)$-orthonormality of the functions $\onb_j$.

	Next, we consider the case when $q=\infty$, $p=1$, for which
	\begin{equation*}
	\begin{split}
	\norm{\optimalweight \onb_j\onb_k}{L^q_{\optimaldensity\measure}}&=\sup_{\psmi\in \domPS}|\optimalweight(\psmi)\onb_j(\psmi)\onb_k(\psmi)|\\
	&\leq \frac{m}{2}\frac{\onb_j^2(\psmi)+\onb_k^2(\psmi)}{\sum_{i=1}^{\dvsp}\onb_i^2(\psmi)}\\
	&\leq m,
	\end{split}
	\end{equation*}
	where we used the elementary inequality $ab\leq \frac{1}{2}a^2+\frac{1}{2}b^2$ for the second step.

	Finally the claim for $1<q<\infty$ follows from Littlewood's interpolation inequality for $L^q$ norms.
\end{proof}

So far, we have assumed that $\domPS$ and $\measure$ exhibit product structure, which allowed us to generate samples coordinate-wise, exploiting known bounds on univariate orthogonal polynomials. 
For more general cases, we now briefly discuss Metropolized independent sampling, which is a simple MCMC algorithm, for the generation of samples from the optimal distribution $\optimaldistribution$. For an extensive treatment of the theory of MCMC algorithms we refer to \cite{liu2008monte}.

The general strategy of MCMC algorithms for the generation of samples from $\optimaldistribution$ is to construct a Markov chain for which $\optimaldistribution$ is an invariant distribution. Ergodic theory then shows that under some assumptions the location of this Markov chain after $n\gg1$ steps is approximately distributed according to $\optimaldistribution$.
Metropolis-Hastings algorithms are MCMC algorithms that construct Markov chains based on user-specified \emph{proposal densities} $p(\psmi,\cdot)$, $\psmi\in\domPS$ (with respect to $\measure$) and a rejection step to ensure convergence to the desired limit distribution $\optimaldistribution$. More specifically, the transition kernel of a Metropolis-Hastings algorithm has the form
\begin{equation}
\label{eq:MC}
K(\psmi,\mathrm{d}\psmi'):=\begin{cases}
p(\psmi,\psmi')\min\{1,\frac{\optimaldensity(\psmi')p(\psmi',\psmi)}{\optimaldensity(\psmi)p(\psmi,\psmi')}\}\measure(\mathrm{d}\psmi')\quad&\text{if }\psmi'\not = \psmi\\
1-\int_{z\not = \psmi}p(\psmi,z)\min\{1,\frac{\optimaldensity(z)p(z,\psmi)}{\optimaldensity(\psmi)p(\psmi,z)}\}\measure(\mathrm{d}z)\quad &\text{if }\psmi'=\psmi.
\end{cases}
\end{equation}
This kernel can be interpreted (and implemented) as proposing a transition from the current state $\psmi$ to a new state $\psmi'$ drawn from the density $p(\psmi,\cdot)$, and rejecting this transition with a certain probability determined by the values of $\optimaldensity$ and $p$ at the current state $\psmi$ and the proposed state $\psmi'$. The rejection probability is designed to ensure the detailed balance condition $\optimaldistribution(\mathrm{d}\psmi)K(\psmi,\mathrm{d}\psmi')=\optimaldistribution(\mathrm{d}\psmi')K(\psmi',\mathrm{d}\psmi)$, which in turn guarantees that $\optimaldistribution$ is invariant under $K$.

Metropolized independent sampling is the name of the subset of Metropolis-Hastings algorithms for which the proposal density $p$ is independent of the current state $\psmi$. If we denote the corresponding state-independent proposal density by $p(\psmi')$ %, then it can be shown that the spectral gap of $K$, that is the difference between the largest eigenvalue, $1$, and the second, smaller in absolute value eigenvalue, is at least
and define $\gap:=\inf_{\psmi\in\domPS}p(\psmi)/\optimaldensity(\psmi)$, then it can be shown \cite[Section 3.2.2]{liu1996metropolized} that starting from any distribution $\genericmeasure$ we have the bound
\begin{equation}
\label{eq:TV}
\norm{K^{n}\genericmeasure-\optimaldistribution}{TV}\leq 2 (1-g)^n
\end{equation} for the total variation distance between the $n$th step probability distribution $K^n\genericmeasure$ of the Markov chain and the target distribution $\optimaldistribution$.
This means that if the proposal  density satisfies $\gap:=\inf_{\psmi\in\domPS}p(\psmi)/\optimaldensity(\psmi)>0$, then $n:= \gap^{-1}\log(24\dvsp^2)$ Markov chain steps suffice to ensure that
\begin{equation*}
\norm{K^n\genericmeasure-\optimaldistribution}{TV}\leq 2 (1-\gap)^{\gap^{-1}\log(24m^2)}\leq \frac{1}{12m^2},
\end{equation*}
as required by \Cref{pro:stability}. To generate $\NS>0$ independent samples from $K^{n}\genericmeasure$, we have to run $\NS$ independent copies of the Markov chain, which differs from the more common practice to use $\NS$ successive, thus dependent, steps of a single Markov chain.

\subsection{Sampling from the arcsine distribution}
\label{ssec:arcsine}
We now determine lower and upper bounds for the optimal sampling distributions associated with downward closed polynomial spaces on $[0,1]^d$ in terms of the arcsine distribution. Namely, we show in \Cref{pro:sampling} below, under quite general assumptions on the measure $\measure$, the bound
\begin{equation*}
	C^{-d}\leq \frac{\mathup{d}\optimaldistribution_{\vsp}}{\mathup{d}\lambda}(\psmi)\leq C^d\arcsine_{d}(\psmi),
\end{equation*}
where $\arcsine_d$ is the arcsine density from \Cref{eq:arcdef}.


The lower bound can be used to make the bound in \Cref{thm:dpls} more precise. Indeed, it implies that the weight $\optimalweight_{\vsp}$ appearing in $e_{\vsp,\optimalweight_{\vsp},\infty}$ satisfies
\begin{equation}
\label{eq:weightbound}
\optimalweight_{\vsp}(\psmi)\leq C^{d}\frac{\text{d}\measure}{\text{d}\lambda}(\psmi).
\end{equation}

 The upper bound provides an alternative sampling strategy:  Instead of sampling from the optimal distribution, we may simply sample from the arcsine distribution with Lebesgue density $\arcsine_{d}$ without using Acceptance/Rejection or Markov chain methods. Indeed, since using the arcsine distribution for sample generation corresponds to the choice $\density=\arcsine_{d}\frac{\text{d}\lambda}{\text{d}\measure}$ for the density in \Cref{sec:dpls}, the associated weight function is given by
 \begin{equation}
 \label{eq:arcsineweight}
 \weight:=(\arcsine_{d})^{-1}\frac{\text{d}\measure}{\text{d}\lambda}.
 \end{equation}
 Hence, the upper bound  shows that the constant $K_{\vsp,\weight}$ from \Cref{thm:dpls} satisfies
 \begin{equation}
 \label{eq:arcsineK}
 \begin{split} K_{\vsp,\weight}=&\norm{\weight(\psmi)\sum_{j=1}^{\dvsp}\onb_j(\psmi)^2}{L^\infty(\domPS)}\\
 =&\norm{\weight(\psmi)m\frac{\text{d}\optimaldistribution}{\text{d}\measure}(\psmi)}{L^\infty(\domPS)}\\
 \leq &C^d\dvsp,
 \end{split}
 \end{equation}
 which is larger than the optimal value, $\dvsp$, only by the factor $C^d$, which is independent of $\vsp$.
 The advantages are that exact and independent samples from the univariate arcsine distribution can be generated  efficiently as $(\sin(X)+1)/2$ for a uniform random variable $X$ on $[-\pi/2,\pi/2]$, that we can use samples from the same distribution for all polynomial subspaces, and that the weight in \Cref{eq:arcsineweight}, which enters the error estimate in \Cref{thm:dpls} through $e_{\vsp,\weight,\infty}$, is known explicitly, is independent of $\vsp$, and vanishes at the boundary if $\frac{\text{d}\measure}{\text{d}\lambda}$ is bounded.
 

\begin{definition}[\textbf{Doubling measures}]
	A measure $\measure$ on $[0,1]$ is called doubling measure if it is absolutely continuous with respect to the Lebesgue measure and if there exists $L>0$ such that
	\begin{equation*}
	\measure(2I)\leq L\measure(I)
	\end{equation*}
	for any interval $I\subset [0,1]$. Here, $2I$ is defined as the interval with the same midpoint as $I$ and twice the length, intersected with $[0,1]$. We call $L>0$ the doubling constant of $\measure$.
\end{definition}
\begin{pro}[\textbf{Bounds on the optimal distribution}]
	\label{pro:sampling}
	If $[0,1]^d$, $d\geq 1$ is equipped with the product $\measure=\bigotimes_{j=1}^{d}\measure_j$  of doubling measures $\measure_j$ on $[0,1]$, then the optimal sampling distribution $
	\optimaldistribution_{\vsp}$  associated with a finite-dimensional downward closed space $\vsp$ of polynomials on $[0,1]^d$ satisfies
	\begin{equation}
	\label{eq:samplingbounds}
	C^{-d}\leq \frac{\mathup{d}\optimaldistribution_{\vsp}}{\mathup{d}\lambda}(\psmi)\leq C^d\arcsine_{d}(\psmi)
	\end{equation}
	for some constant $C>0$ depending only on the maximal doubling constant of the measures $\measure_j$, $j\in\{1,\dots,d\}$.
\end{pro}
\begin{proof}
	 \Cref{eq:samplingbounds} was shown to hold for the univariate optimal sampling distributions $\optimaldistribution_{k}(\psmi)$ associated with univariate spaces of polynomials of degree less than or equal to $k\in\N$ in \cite[Equation 7.14]{mastroianni2000weighted}.%note that the equation there needs to be divided by w_n to arrive at the Lebesgue density that we are considering This proves the case $d=1$.
	We prove the case $d>1$ by induction.

	To reduce the complexity of notation, we assume without loss of generality that $\measure_1=\measure_j$ for all $j\in\{1,\dots,d\}$. This, together with the assumption that $\vsp$ is downward closed, implies that
	\begin{equation*}
	\vsp=\vspan_{\mip\in \mis\subset \N^d}\{\onb_{\mip}(\psmi):=\onb_{\mip_1}(\psmi_1)\cdots \onb_{\mip_{d}}(\psmi_d)\}
	\end{equation*} for $(\onb_{j})_{j\in\N}$ the univariate orthonormal polynomials associated with $\measure_{1}$ and a multi-index set $\mis\subset\N^d$. For $j\in\N$, we define the multi-index sets $\mis_{j}:=\{\mip\in\mis: \mip_{1}=j\}$ and the spaces
	\begin{equation*}
	\tilde{\vsp}_{j}:=\vspan_{\mip\in \mis_{j}\subset \N^d}\{\tilde{\onb}_{\mip}(\tilde{\psmi}):=\onb_{\mip_2}(\psmi_2)\cdots \onb_{\mip_{d}}(\psmi_{d})\}
	\end{equation*} of polynomials on $[0,1]^{d-1}$ with associated optimal distributions $\tilde{\samplingmeasure}_{j}$ on $[0,1]^{d-1}$. This allows us to write
	\begin{equation*}
	\begin{split}
	\frac{\text{d}\optimaldistribution_{\vsp}}{\text{d}\lambda}(\psmi)&=\optimaldensity_{\vsp}(\psmi)\frac{\text{d}\measure}{\text{d}\lambda}(\psmi)\\
	&=\frac{1}{|\mis|}\sum_{\mip\in\mis}\onb_{\mip}^2(\psmi)\frac{\text{d}\measure}{\text{d}\lambda}(\psmi)\\
	&=\frac{1}{|\mis|}\sum_{j\in\N}\onb_{j}^2(\psmi_1)\frac{\text{d}\measure_{1}}{\text{d}\lambda}(\psmi_1)\sum_{\mip\in\mis_{j}}\tilde{\onb}_{\mip}^2(\tilde{\psmi})\prod_{j=2}^{d}\frac{\mathup{d}\measure_{1}}{\mathup{d}\lambda}(\psmi_j)\\
	&=\frac{1}{|\mis|}\sum_{j\in\N}\onb_{j}^2(\psmi_1)\frac{\text{d}\measure_{1}}{\text{d}\lambda}(\psmi_{1})|\mis_{j}|\frac{\text{d}\tilde{\samplingmeasure}_{j}}{\text{d}\lambda}(\tilde{\psmi}),
	\end{split}
	\end{equation*}
	which by the induction hypothesis for the case $d-1$ entails
	\begin{equation}
	\label{eq:temp}
	C^{-(d-1)}A(\psmi_1)\leq \frac{\text{d}\optimaldistribution_{\vsp}}{\text{d}\lambda}(\psmi)\leq A(\psmi_1)C^{d-1}\arcsine_{d-1}(\tilde{\psmi})
	\end{equation}
	with
	\begin{equation*}
	A(\psmi_1):=\frac{1}{|\mis|}\sum_{j\in\N}\onb_j^2(\psmi_1)\frac{\text{d}\measure_{1}}{\text{d}\lambda}(\psmi_{1})|\mis_{j}|.
	\end{equation*}
	We now use the fact that $A(\psmi_1)$ can be written as a weighted average of the univariate densities $\frac{\mathup{d}\optimaldistribution_{k}}{\mathup{d}\lambda}(\psmi_1)=\frac{1}{k}\sum_{j=1}^{k}\onb_j^2(\psmi_1)\frac{\text{d}\measure_{1}}{\text{d}\lambda}(\psmi_{1})$:
	\begin{equation*}
	A(\psmi_1)=\frac{1}{\sum_{k=1}^{\infty}{w_k}}\sum_{k=1}^{\infty}w_k\frac{\text{d}\optimaldistribution_{w_k}}{\text{d}\lambda}(\psmi_1)
	\end{equation*}
	with $w_k:=|\{j:|\mis_{j}|\geq k\}|$.
	% To prove this, insert $\optimaldensity_{w_k}(\psmi_1)$ into \Cref{eq:weightedaverage}.
	Together with the case $d=1$ this shows
	\begin{equation*}
	C^{-1}\leq A(\psmi_1)\leq C \arcsine_{1}(\psmi_1),
	\end{equation*}
	which, when inserted into \Cref{eq:temp}, yields
	\begin{equation*}
	C^{-d}=C^{-(d-1)}C^{-1}\leq \frac{\text{d}\optimaldistribution_{\vsp}}{\text{d}\lambda}(\psmi)\leq C^{d-1}C\arcsine_{1}(\psmi_1)\arcsine_{d-1}(\tilde{\psmi})= C^{d}\arcsine_{d}(\psmi).
	\end{equation*}
\end{proof}


