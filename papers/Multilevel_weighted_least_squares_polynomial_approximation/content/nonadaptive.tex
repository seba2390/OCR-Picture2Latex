\section{Multilevel weighted least squares approximation}
\label{sec:nonadaptive}
In this section, we define a multilevel weighted polynomial least squares method and establish convergence rates for the approximation of a function $\rs_{\infty}\colon\domPS\subset\R^d\to\R$, $d\in\N\cup\{\infty\}$ in a normed vector space $(\F,\|\cdot\|_{\F})\hookrightarrow (L^2_{\measure}(\domPS),\norm{\cdot}{L^2_{\measure}(\domPS)})$ of continuous functions on $\domPS$, under the following assumptions.
\begin{itemize}
	\item{\textbf{A1:}} (Convergence of approximations) There exist functions $\rs_\n\in \F$, $\n\geq 1$ such that 
	\begin{equation*}
	\norm{\rs_{\infty}-\rs_\n}{\F}\leq C_0\n^{-\sc}
	\end{equation*} 
 
	\begin{equation*}
	\norm{\rs_{\infty}-\rs_{\n}}{L^2_{\measure}(\domPS)}\leq C_0 \n^{-\wc}
	\end{equation*}
	for some $C_0>0$, $\sc>0$ and $\wc\geq \sc$.
	
\item{\textbf{A2(p):}} (Polynomial approximability) There exist  downward closed spaces of polynomials $\vsp_{\dvsp}$, $\dvsp\geq 1$ on $\domPS$ such that
\begin{equation*}
\dim \vsp_{\dvsp}\leq C_1 \dvsp^{\dimp},
\end{equation*}
	\begin{equation*}
	e_{\dvsp,p}(\F):=\sup_{\rs\in \F} \frac{e_{\vsp_{\dvsp},p}(\rs)}{\norm{\rs}{\F}}\leq C_1\dvsp^{-\alpha}
	\end{equation*}
	for some $\dimp>0, \alpha>0$, $C_1>0$ and $p=2$ or $p=\infty$. (In the latter case, we use the shorthand $e_{\vsp_{\dvsp},\infty}(\rs):=e_{\vsp_{\dvsp},\optimalweight_{\dvsp},\infty}(\rs)$, where $\optimalweight_{\dvsp}$ is the optimal weight associated with $\vsp_{\dvsp}$.)

\item{\textbf{A3:}} (Sample work) The work required for a single evaluation of $\rs_\n$ satisfies $\work(\rs_n)\leq C_2\n^{\gamma}$ for some $\gamma>0$, $C_2>0$. 

\end{itemize}
\begin{rem}
		In Assumption A2(p), we have introduced the exponent $\dimp$, which in contrast to previous sections may be different from $1$, to be able to apply our results with common sequences of polynomial subspaces without the need for reparametrization. 
\end{rem}


\begin{exa}[\textbf{Polynomial approximability}]
	\label{exa:polapp}
	\begin{itemize}\item 
For univariate Sobolev spaces $\F=H^{\alpha}(\domPS)$, $\domPS=(0,1)$ with $\alpha>0$, Theorem 1 in \cite{quarteroni1984some} shows that
\begin{equation*}
e_{\dvsp,2}(H^{\alpha}(\domPS))\leq C \dvsp^{-\alpha}
\end{equation*}
for the space $V_{m}$ of univariate polynomials with degree less than $m$ and for $\measure$ the Lebesgue measure.
Analogous results also hold in higher dimensions. Here, optimal sequences of polynomial approximation spaces depend on the available smoothness. In particular, optimal polynomial approximation spaces for functions in Sobolev spaces $H^\alpha(\domPS)$ with $\domPS\subset\R^d$ and $\alpha>0$ are of total degree type, whereas functions in Sobolev spaces $H^{\alpha}_{\mix}(\domPS)$ of dominating mixed smoothness can be optimally approximated by hyperbolic cross polynomial spaces \cite{DuTeUl2015}.

	Similar results for the best approximation in the supremum norm hold for functions in HÃ¶lder spaces $\F=C^{s,t}(\domPS)$, $s\in\N$, $t\in[0,1]$ \cite[Theorem 2]{BagbyBosLevenberg2002} (and their dominating mixed smoothness analogues). 
%	To turn such estimates on approximability in the supremum norm into bounds on the weighted polynomial approximability $e_{\dvsp,\infty}(C^{s,t}(\domPS))$, we may use \Cref{eq:weightbound}.
	 %shows that
	%\begin{equation*}
	%		e_{\vsp_\dvsp,\infty}\leq C \dvsp^{-s-t},
	%	\end{equation*} where $\vsp_\dvsp$ is the space of polynomials of degree less than or equal to $\dvsp$ on $[0,1]$. Thus, Assumption 1 holds with $\alpha=s+t$. Similarly, we have 

\item 
Alternatively, we may simply define the space $\F$ via polynomial approximability of its elements. Assume that we have a sequence $(\vsp_\dvsp)_{\dvsp=1}^\infty$ of downward closed polynomial spaces on $\domPS\subset\R^\dps$ with $\dps\in\N\cup\{\infty\}$. If for some $\alpha>0$ we define
		\begin{equation*}
		\F:=\left\{ \rs\colon \domPS\to\R: \|\rs\|_{\F}:=\sup_{\dvsp\in\N} e_{\vsp_{\dvsp},p}(\rs)\dvsp^{\alpha}<\infty\right\}
		\end{equation*}
		with the auxiliary definition $\vsp_0:=\{0\}$, then it is easy to show that $\|\cdot\|_{\F}$ is a norm of $\F$ and that Assumption 2(p) holds with the given $\alpha$ and $C_1=1$.  The choice of the sequence of subspaces $\vsp_m$ can be based on truncating a orthogonal decomposition of $L^2_\measure(\Gamma)$ such as to include only basis functions whose contribution is above a given threshold in $\vsp_m$.  For more information on this construction, see \Cref{sec:adaptive} and \cite{Haji-AliNobileTamelliniEtAl2015a,devore1998nonlinear}.
\end{itemize}
\end{exa}

We now define the multilevel least squares method for a fixed number of levels $L\in\N$. We introduce subsequences
\begin{equation}
\label{eq:subseq}
\dvsp_k:=M\exp(k/({\dimp+\alpha})), \quad k\in\{0,\dots,L\}
\end{equation} and 
\begin{equation*}
\n_l:=\exp(l/(\gamma+\sc)), \quad l\in\{0,\dots,L\}
\end{equation*} with $M:=\exp(L\delta)$, $\delta:=\frac{\wc-\sc}{\alpha(\gamma+\sc)}\geq 0$ if $\gamma/\sc>\dimp/\alpha$ and $M:=1$ else. For our analysis we assume that $\dvsp$ and $\n$ can take non-integer values; in practice, rounding up to the nearest integer increases the required work only by a constant factor. By abuse of notation, we keep the simple notation $\vsp_k$, $e_{k,p}$,  and $\rs_{l}$ for the quantities $\vsp_{m_k}$, $e_{\dvsp_k,p}$, and $\rs_{n_l}$, respectively. 

Next, we draw independent, identically distributed, random samples
\begin{equation*}
\domPS_k=\{\psmi_{k,1},\dots,\psmi_{k,|\domPS_k|}\}\subset \domPS, \quad k\in \{0,\dots,L\}
\end{equation*}
with $\psmi_{k,j}\sim \optimaldistribution_{k}$, where $\optimaldistribution_{k}:=\optimaldistribution_{\vsp_k}$ is the optimal sampling distribution of $\vsp_{k}$ from \Cref{eq:optimaldistribution}. To ensure accuracy of our approximations, we couple the numbers of samples to the dimensions of the polynomial spaces via
\begin{equation}
\label{eq:size}
m_k^{\dimp}\leq \kappa \frac{|\domPS_k|}{\log |\domPS_k|}\leq 2 m_k^{\dimp}\;\;\forall k\in\{0,\dots,L\},\;\text{where } \kappa:=\frac{1-\log 2}{2+2L}.
\end{equation}
%in particular $|\domPS_{k}|\geq 2$.
 By \Cref{eq:optimalK}, this guarantees that the assumption of \Cref{thm:dpls} is satisfied with $r=L$. Alternatively, we may replace $\kappa$ by $C^{-d}\kappa$ with $C$ from \Cref{pro:sampling} if $\domPS$ and $\measure\ll \lambda$ are products and if we use the arcsine distribution to generate samples, or we may choose $\kappa$ as in \Cref{pro:stability} if we use samples that are only approximately distributed according to the optimal distribution. 

Finally, we denote by $\P_{k}\colon F\to \vsp_k$ the random weighted least squares approximation using evaluations in $\domPS_k$, $k\in\{0,\dots,L\}$ and define the multilevel method
\begin{equation}
\label{eq:mldef}
\begin{split}
\smol_{L}(\rs_{\infty})&:=\P_{L}\rs_{0}+\sum_{l=1}^{L}\P_{L-l}(\rs_{l}-\rs_{l-1})\\&=\sum_{l=0}^{L}\P_{L-l}(\rs_{l}-\rs_{l-1})
\end{split}
\end{equation}
where we used the auxiliary definition $\rs_{-1}:=0$. 

For the sake of readability, we do not keep track of constants  and denote by $\lesssim$ any inequality that holds up to a factor depending only on $C_0,C_1,C_2,\alpha,\sc,\wc,\gamma$.
The exact values of these constants may be determined from the proof of the next Theorem.
%or on the functions $\rs_l$. 
\begin{thm}[\textbf{Convergence in probability}]
	\label{thm:main}	
Denote by 
\begin{equation}
\label{eq:workdef}
\work(\smol_{L}(\rs_{\infty})):=|\Gamma_L|\work(f_0)+\sum_{l=1}^{L}|\Gamma_{L-l}|\left(\work(f_l)+\work(f_{l-1})\right)
\end{equation}
the work that $\smol_{L}(\rs_{\infty})$ requires for evaluations of the functions $\rs_{l}$, $l\in\{0,\dots,L\}$.
Define
\begin{align*}
\rate&:=\begin{cases}
\dimp/\alpha& \text{if } \gamma/\sc\leq \dimp/\alpha\\
\theta \gamma/\sc+(1-\theta)\dimp/\alpha \text{ with }\theta:=\sc/\wc& \text{if }\gamma/\sc>\dimp/\alpha
\end{cases}\\
\text{and}\\
\lrate&:=\begin{cases}
2& \text{if } \gamma/\sc<\dimp/\alpha\\
3+\dimp/\alpha&\text{if }\gamma/\sc=\dimp/\alpha\\
1& \text{if }\gamma/\sc>\dimp/\alpha \text{ and }\wc=\sc\\
2&\text{if }\gamma/\sc>\dimp/\alpha \text{ and }\wc>\sc
\end{cases}.%1 because we are increasing r. 1 from work (if stoch. dominates). 1+lambda if mixed dominance
\end{align*}
	
Let $0<\epsilon\lesssim 1$. If Assumptions A1, A2($\infty$), and A3 hold, then we may choose $L\in\N$ such that 
	\begin{equation*}
	\begin{split}
	\work(\smol_{L}(\rs_{\infty}))\lesssim \epsilon^{-\rate}|\log \epsilon|^{\lrate}\log|\log \epsilon|,
	\end{split}
	\end{equation*}

	and such that in an event $E$ with $\prob(E^{c})\lesssim \epsilon^{\log|\log \epsilon|}$ the multilevel approximation satisfies
	\begin{equation}
	\norm{\rs_{\infty}-\smol_{L}(\rs_{\infty})}{L^2_{\measure}(\domPS)}\leq \epsilon.
	\end{equation}
	
%Here and in the remainder of this work we denote by $C(\dots)$ factors that depend only on the quantities specified in parentheses, but may change from line to line.
\end{thm}

\begin{proof}
	The strategy of this proof is to establish bounds on $\work(\smol_{L}(\rs_{\infty}))$ and $\norm{\rs_{\infty}-\smol_{L}(\rs_{\infty})}{L^2_{\measure}(\domPS)}$ for arbitrary $L\in\N$ first, and then to show that, for the right choice of $L$, the latter is smaller than $\epsilon$ and the former is bounded by $\epsilon^{-\rate}|\log\epsilon|^{\lrate}\log|\log \epsilon|$.
	
		\textbf{Work bounds.}  We may deduce immediately from \Cref{eq:size} the rough upper bound
		\begin{equation*}
		\sqrt{|\domPS_k|}\leq\frac{|\domPS_k|}{\log|\domPS_k|}\leq \frac{2}{\kappa}M^{\dimp}\exp(k\frac{\dimp}{\dimp+\alpha})\lesssim (L+1)M^{\dimp}\exp(k\frac{\dimp}{\dimp+\alpha})
		\end{equation*}
		on the number of samples at level $k\in\{0,\dots,L\}$. %Doing this with a better exponent instead of 1/2 could at most decrease the constant of the final estimate by 2 (the 2 that comes out of the log in the second equality below). Not worth it
		Using \Cref{eq:size} again and inserting the previous estimate, we obtain the finer estimate
		\begin{equation*}
		\begin{split}
		|\domPS_k|&\leq (L+1)M^{\dimp}\exp(k\frac{\dimp}{\dimp+\alpha})\log {|\domPS_k|}\\
	%	&\leq \frac{4}{\kappa}\dvsp_0M\exp(k/(1+\alpha))\log\left( \frac{2}{\kappa}\dvsp_0M\exp(k/(1+\alpha))\right)\\
	%	&= \frac{4}{\kappa}\dvsp_0M\exp(k/(1+\alpha))(\log \frac{2}{\kappa}+\log \dvsp_0 + k/(1+\alpha))\\
		&\lesssim (L+1)M^{\dimp}(\log (L+1)+\log M^{\dimp})\exp(k\frac{\dimp}{\dimp+\alpha})(k+1).
		\end{split}
		\end{equation*}
	Since 
	$$
	\work(\rs_{l})+\work(\rs_{l-1})\lesssim \exp(l\frac{\gamma}{\gamma+\sc})
	$$ by Assumption {A3}, we may conclude that
	
	\begin{equation}
	\label{eq:workbounds}
	\begin{split}
	\work(\smol_{L}(\rs_{\infty}))%&=|\Gamma_L|\work(f_0)+\sum_{l=1}^{L}|\Gamma_{L-l}|\left(\work(f_l)+\work(f_{l-1})\right)\\
	%&\lesssim \sum_{l=0}^{L}\frac{8}{\kappa}\dvsp_0(\log \frac{2}{\kappa}+\log %\dvsp_0)\exp((L-l)/(1+\alpha))(L-l+1)2K\n_0^{\gamma}\exp(l\gamma/(\gamma+\sc))\\
	&\lesssim  (L+1)M^{\dimp}(\log (L+1)+\log M^{\dimp})\sum_{l=0}^{L}\exp((L-l)\frac{\dimp}{\dimp+\alpha})(L-l+1)\exp(l\frac{\gamma}{\gamma+\sc})\\
	&=(L+1)M^{\dimp}(\log (L+1)+\log M^{\dimp})\exp(L\frac{\dimp}{\dimp+\alpha})\sum_{l=0}^{L}\exp\Big(-l\big(\frac{\dimp}{\dimp+\alpha}-\frac{\gamma}{\gamma+\sc}\big)\Big)(L-l+1).
	\end{split}
	\end{equation}
	
	We now distinguish three cases.
	\begin{enumerate}[(a)]
		\item $\gamma/\sc<\dimp/\alpha$: In this case $\dimp/(\dimp+\alpha)>\gamma/(\gamma+\sc)$. Thus, the sum on the right-hand side of \Cref{eq:workbounds} satisfies
		\begin{equation*}
		\begin{split}
		\sum_{l=0}^{L}\exp\Big(-l\big(\frac{\dimp}{\dimp+\alpha}-\frac{\gamma}{\gamma+\sc}\big)\Big)(L-l+1)&\lesssim (L+1)\sum_{l=0}^{L}\exp\Big(-l\big(\frac{\dimp}{\dimp+\alpha}-\frac{\gamma}{\gamma+\sc}\big)\Big)\\
		&\lesssim L+1.
		\end{split}
		\end{equation*}
		Together with the fact that $M=1$ in the case under consideration, this shows that
		\begin{equation*}
		\work(\smol_{L}(\rs_{\infty}))\lesssim \exp(L\frac{\dimp}{\dimp+\alpha})(L+1)^2\log (L+1).
		\end{equation*}
		
		\item $\gamma/\sc=\dimp/\alpha$:  In this case $\dimp/(\dimp+\alpha)=\gamma/(\gamma+\sc)$. Thus, the sum on the right-hand side of \Cref{eq:workbounds} equals $\sum_{l=0}^{L}(L-l+1)\lesssim (L+1)^2$ and we obtain
		\begin{equation*}
		\work(\smol_{L}(\rs_{\infty}))\lesssim \exp(L\frac{\dimp}{\dimp+\alpha})(L+1)^3\log (L+1).
		\end{equation*}
		since $M=1$.
		
		\item $\gamma/\sc>\dimp/\alpha$:  In this case $\dimp/(\dimp+\alpha)<\gamma/(\gamma+\sc)$. Thus, the sum on the right-hand side of \Cref{eq:workbounds} satisfies 
		\begin{equation*}
		\begin{split}
		\sum_{l=0}^{L}&\exp\Big(-l\big(\frac{\dimp}{\dimp+\alpha}-\frac{\gamma}{\gamma+\sc}\big)\Big)(L-l+1)\\
		&=\exp\Big(L\big(\frac{\gamma}{\gamma+\sc}-\frac{\dimp}{\dimp+\alpha}\big)\Big)\sum_{l=0}^{L}\exp\Big(-l\big(\frac{\gamma}{\gamma+\sc}-\frac{\dimp}{\dimp+\alpha}\big)\Big)(l+1)\\
		&\lesssim\exp\Big(L\big(\frac{\gamma}{\gamma+\sc}-\frac{\dimp}{\dimp+\alpha}\big)\Big).
		\end{split}
		\end{equation*}
		If $\wc=\sc$, then $M=1$ and we obtain
		\begin{equation*}
		\begin{split}
		\work(\smol_{L}(\rs_{\infty}))&\lesssim (L+1)M^{\dimp}(\log(L+1)+\log M^{\dimp})\exp(L\frac{\gamma}{\gamma+\sc}))\\
		&\lesssim \exp\Big(L\big(\frac{\gamma}{\gamma+\sc}\big)\Big) (L+1)\log(L+1).
		\end{split}
		\end{equation*} 
	
	 If instead $\wc>\sc$, then $M=\exp(\delta L)$ and we obtain
		\begin{equation*}
		\begin{split}
		\work(\smol_{L}(\rs_{\infty}))&\lesssim (L+1)M^{\dimp}(\log(L+1)+\log M^{\dimp})\exp(L\frac{\gamma}{\gamma+\sc}))\\
		&\lesssim \exp\Big(L\big(\frac{\gamma}{\gamma+\sc}+{\dimp}\delta\big)\Big)(L+1)^2\log(L+1).
				\end{split}
		\end{equation*}
	\end{enumerate}
\textbf{Residual bounds.} First, we show that with high probability 
	\begin{align}
	\label{eq:conv1}
	\|\Id-\P_k\|_{\F\to L^2_{\measure}(\domPS)}&\lesssim M^{-\alpha}\exp(-k\alpha /(\dimp+\alpha))\quad \forall k\in\{0,\dots,L\}.
	\end{align} 
By part (ii) of \Cref{thm:dpls} together with Assumption A2($\infty$), it suffices to show that the event
\begin{equation*}
E:=\{\|{\mathbf{G}_k-\mathbf{I}_k}\|\leq 1/2 \;\forall k\in\N\}
\end{equation*}
has a high probability, where $\mathbf{G}_k$ is the Gramian matrix from \Cref{eq:dpls:computation}. But by the first part of the same theorem, the complementary probability that $\|{\mathbf{G}_k-\mathbf{I}_k}\|\leq 1/2$ for a fixed $k\in\N$ decays as the number of samples $|\domPS_{k}|$ increases. Since the sets $\domPS_{k}$ grow exponentially in $k$, by \Cref{eq:size},  we may conclude using a crude zeroth moment estimate and a geometric series bound:
		\begin{equation}
		\label{eq:probbound}
		\begin{split}
		\prob(E^c)&=\prob\left(\exists k\in\N:\|{\mathbf{G}_k-\mathbf{I}_k}\|>1/2 \right)\\
		&\leq \sum_{k=0}^{\infty}\prob(\|{\mathbf{G}_k-\mathbf{I}_k}\|>1/2)\\
		&\leq 2\sum_{k=0}^\infty |\domPS_{k}|^{-L}\\
		&\leq 2\kappa^{L}M^{-\dimp L}\sum_{k=0}^{\infty}\exp(-kL\frac{\dimp}{\dimp+\alpha})\\
		&=\frac{2\kappa^{L}M^{-\dimp L}}{1-\exp(-L\frac{\dimp}{\dimp+\alpha})}\\
		&\lesssim L^{-L}.
		\end{split}
		\end{equation}
		
		Assuming now that the samples $\domPS_{k}$, $k\in\N$ are such that \Cref{eq:conv1} holds for the associated operators $\P_k$, we obtain
		\begin{equation}
		\label{eq:convbounds}
		\begin{split}
		\norm{\rs_{\infty}-\smol_{L}(\rs_{\infty})}{L^2_{\measure}(\domPS)}
		&=\norm{\rs_{\infty}-\left(\sum_{l=0}^{L}(\rs_{l}-\rs_{l-1})-\sum_{l=0}^{L}(\Id-\Pi_{L-l})(\rs_{l}-\rs_{l-1})\right)}{L^2_{\measure}(\domPS)}\\
		&\leq \norm{\rs_{\infty}-\rs_{L}}{L^2_{\measure}(\domPS)}+\sum_{l=0}^{L}\norm{\Id-\Pi_{L-l}}{\F\to L^2_{\measure}(\domPS)}\norm{\rs_{l}-\rs_{l-1}}{\F}\\
		&\lesssim \exp(-L\frac{\wc}{\gamma+\sc})+M^{-\alpha}\sum_{l=0}^{L}\exp(-(L-l)\frac{\alpha}{\dimp+\alpha})\exp(-l\frac{\sc}{\gamma+\sc})\\
		&= \exp(-L\frac{\wc}{\gamma+\sc})+M^{-\alpha}\exp(-L\frac{\alpha}{\dimp+\alpha})\sum_{l=0}^{L}\exp\Big(l\big(\frac{\alpha}{\dimp+\alpha}-\frac{\sc}{\gamma+\sc}\big)\Big),\\
		\end{split}
		\end{equation}
		where we used Assumption A1.
			Again, we distinguish the cases (a)-(c).
			\begin{enumerate}[(a)]
				\item $\gamma/\sc<\dimp/\alpha$:  In this case $\alpha/(\dimp+\alpha)<\sc/(\gamma+\sc)$. Thus, the sum on the right-hand side of \Cref{eq:convbounds} is uniformly bounded in $L$ and we obtain 
				\begin{equation*}
			\begin{split}
			\norm{\rs_{\infty}-\smol_{L}(\rs_{\infty})}{L^2(\measure)}&\lesssim \exp(-L\frac{\wc}{\gamma+\sc})+\exp(-L\frac{\alpha}{\dimp+\alpha})\\
			&\lesssim \exp(-L\frac{\alpha}{\dimp+\alpha}),
			\end{split}
				\end{equation*}
				where we used the fact that $\wc\geq \sc$ for the last inequality.
				
				\item $\gamma/\sc=\dimp/\alpha$: In this case $\alpha/(\dimp+\alpha)=\sc/(\gamma+\sc)$. Thus, the sum on the right-hand side of \Cref{eq:workbounds} equals $L+1$ and we obtain
				\begin{equation*}
				\begin{split}
				\norm{\rs_{\infty}-\smol_{L}(\rs_{\infty})}{L^2(\measure)}&\lesssim \exp(-L\frac{\wc}{\gamma+\sc})+\exp(-L\frac{\alpha}{\dimp+\alpha})(L+1)\\
				&\lesssim \exp(-L\frac{\alpha}{\dimp+\alpha})(L+1),
				\end{split}
				\end{equation*}
				where we used the fact that $\wc\geq \sc$ for the last inequality.
				
				\item $\gamma/\sc>\dimp/\alpha$: In this case $\alpha/(\dimp+\alpha)>\sc/(\gamma+\sc)$. Thus, the sum on the right-hand side of \Cref{eq:workbounds} is a divergent geometric series and we obtain 
				\begin{equation*}
				\begin{split}
				\norm{\rs_{\infty}-\smol_{L}(\rs_{\infty})}{L^2(\measure)}&\lesssim \exp(-L\frac{\wc}{\gamma+\sc})+M^{-\alpha}\exp(-L\frac{\sc}{\gamma+\sc})\\
				&\lesssim \exp(-L\frac{\wc}{\gamma+\sc}),
				\end{split}
				\end{equation*}
				where we used the definition of $M=\exp(L\delta)$ and $\delta$ in the case $\gamma/\sc>\dimp/\wc$ in the last inequality.
			\end{enumerate}
			
			\textbf{Conclusion.} It remains to choose $L$ such that the residual bound equals $\epsilon$ and insert this choice of $L$ into the work bound. For simplicity, we assume $L$ can be any real number. In practice, rounding up to the next largest value decreases the residual and increases the work only by a constant factor. One final time, we distinguish the cases (a)-(c).
			
		\begin{enumerate}[(a)]
			\item $\gamma/\sc<\dimp/\alpha$: Defining $L$ as the solution of
			
			\begin{equation*}
			\exp(-L\frac{\alpha}{\dimp+\alpha})=\epsilon,
			\end{equation*}
			
			we obtain the second inequality in the following estimate:
			\begin{equation*}
			\begin{split}
		\work(\smol_{L}(\rs_{\infty}))&\lesssim \exp(L\frac{\dimp}{\dimp+\alpha})(L+1)^2\log (L+1)\\
		&\lesssim	\epsilon^{-\rate}|\log\epsilon|^2\log|\log \epsilon|.
			\end{split}
			\end{equation*}
			
			\item $\gamma/\sc=\dimp/\alpha$: 			Since we assumed that $\epsilon\lesssim 1$ there is a unique positive solution of
			\begin{equation*}
			\exp(-L\frac{\alpha}{\dimp+\alpha})(L+1)=\epsilon.
			\end{equation*}
			
			With this choice of $L$ we obtain the second inequality in the following estimate:
			\begin{equation*}
			\begin{split}
			\work(\smol_{L}(\rs_{\infty}))&\lesssim  \exp(L\frac{\dimp}{\dimp+\alpha})(L+1)^3\log(L+1)\\
			&\lesssim \epsilon^{-\rate}|\log\epsilon|^{3+\lambda}\log|\log \epsilon|.
			\end{split}
			\end{equation*}
			
			\item $\gamma/\sc>\dimp/\alpha$: 
		We assume $\wc>\sc$, the case $\wc=\sc$ can be treated analogously. Defining $L$ as the solution of
			\begin{equation*}
			\exp(-L\frac{\wc}{\gamma+\sc})=\epsilon,
			\end{equation*}	
			we obtain the second inequality in the following estimate:
			\begin{equation*}
			\begin{split}
			\work(\smol_{L}(\rs_{\infty}))&\lesssim \exp\Big(L\big(\frac{\gamma}{\gamma+\sc}+\dimp\delta\big)\Big)(L+1)^2\log(L+1)\\
			&\lesssim \epsilon^{-\lambda}|\log\epsilon|^2\log|\log\epsilon|.
			\end{split}
			\end{equation*}
		\end{enumerate}
		In all cases we chose $L$ such that $L\geq |\log\epsilon|$, thus $\prob(E^c)\lesssim L^{-L}\lesssim \epsilon^{\log|\log\epsilon|}$ by \Cref{eq:probbound}.
\end{proof}
%\begin{rem}[\textbf{Computational work}]
%	In the event $E$ in which the residual estimate of the previous theorem holds the condition numbers of all matrices $\mathbf{G}_k$ are bounded by 3. Therefore, using a suitable iterative solver, the work for the solution of the linear system \Cref{eq:dpls:computation} for all levels $k\in\{0,\dots,L\}$ only grows linearly in the dimensions of the spaces $\vsp_{k}$, thus sublinearly in the total number of evaluations $|\Gamma_{k}|$, by \Cref{eq:size}. It is thus negligible compared to the work required for the computation of the samples themselves.
%\end{rem}
\begin{rem}
	The proof does not exploit independence of samples across different $\domPS_k$, $k\in\{0,\dots,L\}$, but instead relies on a simple union bound (see \Cref{eq:probbound}). Thus, we could alternatively first create $\domPS_{L}$ and then define all $\domPS_l$ with $l<L$ as subsets of it.
\end{rem}
\begin{rem}
	To determine the polynomial coefficients of $\Pi_{L-l}(\rs_{l}-\rs_{l-1})$, $l\in\{0,\dots,L\}$, after the functions $\rs_{l}-\rs_{l-1}$ have been evaluated in all $\psmi\in \domPS_{L-l}$, we need to solve linear systems of the form
	\begin{equation}
	\mathbf{G}_k\mathbf{\p}_k=\mathbf{\coeff}_k,\quad k\in\{0,\dots,L\}
	\end{equation}
		as in \Cref{eq:dpls:computation}. In the event $E$ in which the residual estimate of the previous theorem holds, the condition numbers of all matrices $\mathbf{G}_k$ are bounded by 3. Therefore, using a suitable iterative solver  we can determine all coefficients $\mathbf{\p}_k$ to an accuracy of $\epsilon>0$ with $\mathcal{O}(\log\epsilon)$ iterations. Since matrix vector products with $\mathbf{G}_k$ require $\mathcal{O}(m_k^{2\dimp})$ operations by \Cref{rem:matvec}, the associated computational work is, up to logarithmic factors, given by
		\begin{equation*}
		\sum_{k=0}^{L}m_k^{2\dimp}=\sum_{k=0}^{L}M^{2\dimp}\exp(2k\dimp/(\dimp+\alpha))\lesssim M^{2\dimp}\exp(2L\dimp/(\dimp+\alpha)).
		\end{equation*}
		%which compared to the work bounds in the proof of the previous theorem is negligible only when $\gamma/\sc>1/\alpha$. 
		%Compared to the work bounds in the proof of the previous theorem, we see that this computational work is negligible only in the case (c),  $\gamma/\sc>1/\alpha$, and when 
	%	\begin{equation*}
	%	\frac{\gamma}{\gamma+\sc}-\frac{\wc-\sc}{\alpha(\gamma+\sc)}\geq\frac{2}{1+\alpha}.
	%	\end{equation*}
	Inspection of the proof of the previous theorem shows that, even if we include this cost in the work specification, the conclusion holds true with slightly different logarithmic factors and the exponent
		\begin{align*}
		\tilde{\rate}&:=\begin{cases}
		2\dimp/\alpha& \text{if } \gamma/\sc\leq 2\dimp/\alpha\\
		\gamma/\sc& \text{if }\gamma/\sc>2\dimp/\alpha,
		\end{cases}
		\end{align*}
		instead of $\rate$ (assuming for simplicity that $\sc=\wc$), provided that we change the definition of the subsequence $m_k$ in \Cref{eq:subseq} to 
		$$
		m_k:=\exp(k/(2\dimp+\alpha)).
		$$
	\end{rem}

To obtain mean square convergence, we replace the least squares approximations $\P_{k}$ by the stabilized versions $\P_k^{\cond}$ from part (iii) of \Cref{thm:dpls}, and define
\begin{equation}
\label{eq:ml2}
\smol^{\cond}_{L}(\rs_{\infty}):=
\P^{\cond}_{L}\rs_{0}+\sum_{l=1}^{L}\P^{\cond}_{L-l}(\rs_{l}-\rs_{l-1}).
\end{equation}
%For the sake of readability, we do not keep track of constants  and denote by $\lesssim$ any inequality that holds up to a factor depending only on $C_0,C_1,K,\dvsp_0,\n_0,\alpha,\beta,\gamma$ or $r$. %or on the functions $\rs_l$. 
\begin{thm}[\textbf{Mean square convergence}]
	\label{thm:main2}
	Let $0<\epsilon\lesssim 1$. If Assumptions A1, A2(2), and A3 hold, then we may choose $L\in\N$ such that  
		\begin{equation}
		\expect\norm{\rs_{\infty}-\smol^{\cond}_{L}(\rs_{\infty})}{L^2_{\measure}(\domPS)}^2\leq \epsilon^2
		\end{equation}
		and
	\begin{equation*}
	\begin{split}
	\work(\smol_{L}^{\cond}(\rs_{\infty}))\lesssim \epsilon^{-\rate}|\log \epsilon|^{\lrate}\log|\log\epsilon|,
	\end{split}
	\end{equation*}
	with $\rate$ and $\lrate$ as in \Cref{thm:main}.
\end{thm}
\begin{proof}
	The work bounds from the proof of \Cref{thm:main} hold unchanged.
	
%	For the convergence estimate, we cannnot apply the general theory in \cite{WolfersNobileTempone2016a}, since this would require bounds on $\expect\norm{\Id-\Pi^{\cond}_{l}}{\F\to L^2(\measure)}^2$ and \Cref{thm:dpls} only provides bounds on $\expect\norm{\f-\Pi^{\cond}_{l}\f}{L^2(\measure)}^2$ for single functions $\f\in\F$. Therefore, we need to perform the required computations by hand. Using the notation $\Delta_lf:=\rs_{l}-\rs_{l-1}$, with the auxiliary definition $\rs_{-1}:=0$, we obtain the error representation
We next establish residual bounds for arbitrary $L\in\N$ as before, using the error representation
	\begin{equation*}
	\rs_{\infty}-\smol^{\cond}_{L}(\rs_{\infty})=\rs_{\infty}-\rs_{L}+\sum_{l=0}^{L}(\Id-\P^{\cond}_{L-l})(\rs_{l}-\rs_{l-1}).
	\end{equation*}
	The triangle inequality of the norm $(\expect\norm{\cdot}{L^2_{\measure}(\domPS)}^2)^{1/2}$ implies that
	\begin{equation*}
	\begin{split}
	\left(\expect\norm{\rs_{\infty}-\smol^{\cond}_{L}(\rs_{\infty})}{L^2_{\measure}(\domPS)}^2\right)^{1/2}&\leq \left(\norm{\rs_{\infty}-\rs_{L}}{L^2_{\measure}(\domPS)}^2\right)^{1/2}+\sum_{l=0}^{L}\left(\expect\norm{(\Id-\P^{\cond}_{L-l})(\rs_{l}-\rs_{l-1})}{L^2_{\measure}(\domPS)}^2\right)^{1/2}\\
		&\lesssim \norm{\rs_{\infty}-\rs_{L}}{L^2(\measure)}+\sum_{l=0}^{L}\left(e_{\vsp_{L-l},2}^2(\rs_{l}-\rs_{l-1})+\norm{\rs_{l}-\rs_{l-1}}{L^2_{\measure}(\domPS)}^2|\domPS_{L-l}|^{-2\alpha/\dimp}\right)^{1/2}=:(\star)
	\end{split}
	\end{equation*}
	where we used part (iii) of \Cref{thm:dpls} together with the fact that $L\geq 2\alpha/\dimp$ for small enough $\epsilon$ for the second inequality. We observe that
	\begin{itemize}
		\item by Assumption {A1}, we have
		\begin{equation*}
		\norm{\rs_{\infty}-\rs_{L}}{L^2_{\measure}(\domPS)}\lesssim  \exp(-L\frac{\wc}{\gamma+\sc})
		\end{equation*}
		\item by Assumptions {A1} and {A2(2)}, we have 
		\begin{equation*}
		\begin{split}
		e_{\vsp_{L-l},2}^2(\rs_{l}-\rs_{l-1})\lesssim \left(M^{-\alpha}\exp(-(L-l)\frac{\alpha}{\dimp+\alpha})\exp(-l\frac{\sc}{\gamma+\sc})\right)^{2}
		\end{split}
		\end{equation*}
		
		
		\item by \Cref{eq:size} and Assumption A1, we have 
		\begin{equation*}
		\begin{split}
		\norm{\rs_{l}-\rs_{l-1}}{L^2_{\measure}(\domPS)}^2|\domPS_{L-l}|^{-2\alpha/\dimp}&\lesssim \left(M^{-\alpha}\exp(-l\frac{\wc}{\gamma+\sc})\exp(-(L-l)\frac{\alpha}{\dimp+\alpha})\right)^2.
		%&\leq T\dvsp_0^{-r}\exp(-(L-l)2\alpha/(1+\alpha))
		\end{split}
		\end{equation*}
	\end{itemize}
	Combining these observations we arrive at 
	\begin{equation*}
	\begin{split}
	(\star)&\lesssim \exp(-L\frac{\wc}{\gamma+\sc})+M^{-\alpha}\sum_{l=0}^L\exp\left(-(L-l)\frac{\alpha}{\dimp+\alpha}-l\frac{\sc}{\gamma+\sc}\right)\\
	&\lesssim \exp(-L\frac{\wc}{\gamma+\sc})+M^{-\alpha}\exp(-L\frac{\alpha}{\dimp+\alpha})\sum_{l=0}^{L}\exp\left(l\bigg(\frac{\alpha}{\dimp+\alpha}-\frac{\sc}{\gamma+\sc}\bigg)\right).
	\end{split}
	\end{equation*}
From here, the proof may be concluded exactly as that of \Cref{thm:main}.
\end{proof}