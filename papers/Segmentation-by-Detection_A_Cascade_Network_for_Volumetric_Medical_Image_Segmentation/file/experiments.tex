
\noindent
{\bf Data} 
The proposed method was evaluated on a segmentation task for 3D ultrasound images of infants, collected by a team at the Radiology Department at University of Alberta. The data is part of a local study of developmental dysplasia of the hip (DDH)\cite{hareendranathan2017semiautomatic}. It was shown that  with a correct early diagnosis, DDH can be effectively treated by simple bracing in infancy. This diagnosis is currently done using ultrasound images. The segmentation of the hip is an essential step of this procedure and therefore an important task that we address in this work. An illustration of ultrasound image for the hip anatomy is shown in Figure \ref{fig:hip}.

%Developmental dysplasia of the hip (DDH) leads to premature osteoarthritis, accounting for a third of hip replacement surgeries \cite{hareendranathan2017semiautomatic}. With a correct early diagnosis, DDH can be effectively treated by simple bracing in infancy. Diagnosis and treatment planning for DDH is currently performed using ultrasound images. The segmentation of the hip is an essential step of this procedure and therefore an important task that we address in this work. An illustration of ultrasound image for the hip anatomy is shown in Figure \ref{fig:hip}.

\begin{figure}[tb]
\centering
\centerline{\includegraphics[width=.45\textwidth]{fig/hip.png}}
\caption{An illustration of ultrasound image for the hip anatomy. (a) Hip anatomy; (b) Ultrasound image in 2D view where the red area is annotation while the white area is ground truth object region; (c) Femoral head in 3D view.}
\label{fig:hip}
\end{figure}

A total of 19 three-dimensional ultrasound (3DUS) scans from 19 infants aged between 4 and 111 days are used. 3DUS volumes of the hip are obtained on a Philips iU22 scanner using a 13MHz linear transducer in coronal orientation and exported to Cartesian DICOM. Each 3DUS consists of 256 ultrasound slices of 0.13mm thickness containing $397\times192$ pixels measuring $0.11\times0.20mm$. We ran all our experiments on a down-sampled version of the original resolution by factor of two in each dimension. 
Therefore, the data size used in the experiments is $128\times184\times96$. The training set contains 5 left femoral head and 5 right femoral head randomly selected from 3DUS scans, while the remaining scans are chosen for testing.

\noindent
{\bf Comparative experiments}
To evaluate the effect of the proposed method, we compared it with the 3D U-Net. 
Different from the training process of 3D U-Net, the proposed method is trained with the input volume as well as the attention model obtained from the detection module. 
In addition, to demonstrate the effectiveness of the proposed attention mechanism, we compared our 2D attention model with a 3D mask, which can be easily generated from the annotation in the format of a 3D tight bounding box as illustrated in Figure \ref{fig:seg} (b). The 3D mask method replaces the detection module with the mask while the segmentation module is unchanged. 

We evaluate the three segmentation methods using IoU, which is defined as $TP/(TP + FP + FN)$ where $T/P$ denotes $True/False$ and $P/N$ denotes $Positive/Negative$. The comparative models were tested on 9 scans separately and results were reported as the best performance, the worst as well as the average IoU. 

\noindent
{\bf Results} 
Summary results are presented in Table \ref{tb:unet}.
Compared with the 3D U-Net, our method improves the overall performance (average IoU is increased by 8.7\%). The segmentation results are illustrated in Figure \ref{fig:seg} (a). With the guidance of attention model, our method reduced the disrupt of global context and shows a smoother segmented contour. Compared with the left column, results in the middle are low in precision. This is reasonable because of the low contrast in gray scale which is hard to annotate even for the expert. The training loss is also depicted in Figure \ref{fig:loss}, where our method showed a faster convergence speed and more stable training.

When comparing results with the 3D mask, our attention model showed an obvious advantage. It is mainly because the slice-by-slice formed attention model is more precise in localization than the 3D bounding box. As the performance of the detection module increases, the segmentation gets better. That means, we can improve the performance of segmentation by devoting efforts on detection which is an easier task.


\begin{table}[thb]
\centering
\caption{Segmentation results for femoral head}
\begin{tabular}{l|ccc}
\hline
Femoral head & Best IoU & Worst IoU & Average IoU \\ 
\hline
3D U-Net     & 0.729    & 0.483     & 0.622 \\
3D mask & 0.679    & 0.493     & 0.600 \\
Our method   & {\bf 0.779}    & {\bf 0.512}     & {\bf 0.709} \\
\hline
\end{tabular}
\label{tb:unet}
\end{table}



\begin{figure}[t!]
\centering
\centerline{\includegraphics[width=.46\textwidth]{fig/seg.png}}
% \centerline{(a) Hip anatomy}\medskip
\caption{Segmentation results on femoral head. (a) Example segmentation. Output segmentation is in red while the ground truth is in white. The top and middle rows show the results of 3D U-Net and 3D mask respectively while our method is displayed at the bottom. The first two columns are the segmentation results in image slices. The third column displays the 3D view of segmented femoral head. (b) Attention mechanism. The above is the 2D attention model while the bottom is the 3D mask.}
\label{fig:seg}
\end{figure}

\begin{figure}[t!]
\centering
\centerline{\includegraphics[width=.5\textwidth]{fig/loss.png}}
% \centerline{(a) Hip anatomy}\medskip
\caption{Training loss. The loss of 3D U-Net is denoted by the blue line while ours is denoted by red. The right is a zooming in of the dashed region.}
\label{fig:loss}
\end{figure}

%To demonstrate the effectiveness of the attention model proposed in our method, we use a 3D mask for comparison, which can be easily generated from the annotation in the format of a 3D tight bounding box as illustrated in Figure \ref{fig:seg} (b). We presents the results in the last row of Table \ref{tb:unet} where the attention model showed an obvious advantage over the 3D mask. It is mainly because the slice-by-slice formed attention model is more precise in localization than the 3D bounding box. We also report the accuracy in our detection module which is $0.699$ on average IoU. As the performance of the detection module increases, the segmentation gets better. That means, we can improve the performance of segmentation by devoting efforts on detection which is an easier task.
% While a perfect attention model can increase the segmentation to $0.811$. 

% \begin{figure}[htb]
% \centering
% \centerline{\includegraphics[width=.27\textwidth]{fig/atte.png}}
% % \centerline{(a) Hip anatomy}\medskip
% \caption{A comparison of attention model with contour mask.}
% \label{fig:atte}
% \end{figure}
