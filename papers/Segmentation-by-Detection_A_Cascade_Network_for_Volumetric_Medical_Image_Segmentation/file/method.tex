The proposed segmentation-by-detection framework, as depicted in Figure \ref{fig:framework}, consists of a detection module and a segmentation module.
In detection stage, 2D slices (layered box) from the input volume are fed to the RPN. Based on the region proposals obtained from RPN, an attention model (block in orange) is formed. The input volume as well as the attention model are further processed in segmentation stage to get the refined anatomical segmentation. 
\vspace{1em} 

\begin{figure}[t]
\centering
\includegraphics[width=0.95\linewidth]{fig/framework.pdf}
\caption{Schematic representation of the segmentation-by-detection framework. The left part is the detection module while the segmentation module is followed on the right. The blue block denotes the input volume which is 3D ultrasound scan of femoral head. The output segmentation is in red.}
\label{fig:framework}
\end{figure}
% dana could you improve the figure. we can try to think together of better ways 

\noindent\textbf{Detection Module:} 
% dana : here you have to make the clarification that you have ground truth on the boxes (in implementation part)
The detection module follows an RPN architecture, a fully convolutional network which takes image slice as input and outputs object region candidates. 
We use the VGG-16 model as the backbone \cite{simonyan2014very} to learn convolutional features and an $3 \times 3$ spatial window to generate region proposals. At each sliding-window location, 9 anchors are predicted associated with different scales and aspect ratios. The last layer consists of a box-regression (reg) layer and a box-classification (cls) layer in parallel. The reg layer outputs 4 regression offsets, $ t = (t_x,t_y,t_w,t_h)$, denoting a scale-invariant translation as well as log-space height and width shift, where $x,y,w$ and $h$ specify two coordinates of the box center, width and height. The cls layer outputs two scores by softmax, related to probabilities of object and background for each proposal. We assign a positive label (of being object) to candidate which has an Intersection-over-Union (IoU) ratio higher than 0.7 with ground truth box. Note that an image slice may contain multiple object regions or none. 

The loss function of RPN follows the multi-task loss \cite{ren2015faster} which is defined as $L = L_{reg} + L_{cls}$. The regression loss, $L_{reg} = -\log p_{obj}$ is log loss and the classification loss,
\begin{equation} \label{eq:loss}
L_{cls} = \sum_{i \in \{x,y,w,h\}} smooth_{L_1} (t_i - t_i^*)
\end{equation}
is smooth $L_1$ loss where $t_i^*$ denotes the ground truth box for the target object. 
\vspace{1em}

\noindent\textbf{Segmentation Module:}
3D U-Net \cite{cciccek20163d} is utilized in the segmentation module as its outstanding performance in medical image segmentation. The u-shaped architecture consists of two paths: a contracting path, where each layer contains two $3\times3\times3$ convolutions followed by a rectified linear unit (ReLU) and then a max pooling, provides high resolution features. While, the symmetric expanding path for semantically richer features replaces max pooling with a upconvolution $2\times2\times2$ with stride of 2 in each dimension, and then two $3\times3\times3$ convolutions each followed by a ReLU. Skip connections between layers of equal resolution in the contracting path and the expanding path enables context information as well as precise localization.

Different from 3D U-Net, to incorporate the attention model detected by the RPN, our architecture takes as input both the volumetric image data and the candidate RoIs proposed by the RPN, concatenated as 3D volume. 
% dana not sure what you like to say below
% densely annotated
The attention model makes the network to focus on the potential RoIs and can reduce the interference of the surrounding noise.
The anatomical segmentation is then generated from a $1\times1\times1$ convolution which reduces the number of feature maps to the number of labels.  The energy function is computed by a pixel-wise softmax combined with the cross entropy loss.
% dana equation ??

\subsection{System and implementation Details}
The segmentation-by-detection approach adopts a cascade structure with two stages: detection and segmentation. The two networks are trained separately in an end-to-end manner. All the new layers are randomly initialized from zero-mean Gaussian distribution with standard deviations 0.01. Biases are initialized to 0. We use Caffe \cite{jia2014caffe} for the implementation and an NVIDIA Titan X GPU for training.

In the detection stage, we initialize the VGG-16 model by the pre-trained model for ImageNet classification \cite{russakovsky2015imagenet} and further fine-tune the model for our detection task. The input fed to the network are image slices with a fixed size of $184\times96$ and the corresponding ground truth boxes are generated from the annotation in the format of tight bounding boxes surrounding the segmentation contour (as illustrated in Figure \ref{fig:hip} (b), the boundary of white area). To optimize the energy function, stochastic gradient descent (SGD) is used. The global learning rate is set to 0.001, while a momentum of 0.9 and a weight decay of 0.0005 are used. The batch size is set to 256 and each mini-batch only contains the positive anchors for training. The region proposals are obtained from the reg path for each image slice. The attention model is then formed by concatenating all the detected regions, as binary masks, into a volume.

In the segmentation stage, we use the Adam optimizer \cite{kingma2014adam} to learn the network parameters. A global learning rate is set to 0.001 while the two momentum coefficients are set to 0.9 and 0.999 respectively. A batch size of 1 is used due to the memory constraints of the GPU. The network takes the volume data as well as the attention model as input. We train the network for a maximum of 30K iterations and reserve the learned weights with the best performance from every 1K iterations. 
\vspace{1em}

\noindent\textbf{Inference:}
At test time, the 2D slices from an input volume are first fed to the detection module. The attention model is obtained based on the output. Then the volume data as well as the attention model are fed to the segmentation module to get the pixel-wise prediction.


