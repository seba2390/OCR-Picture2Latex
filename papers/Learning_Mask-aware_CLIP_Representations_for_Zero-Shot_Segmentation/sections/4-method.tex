
\input{figs/tex/fintune}

We introduce Mask-Aware Fine-tuning (MAFT), a method for learning mask-aware CLIP representations. 
Within MAFT, we first propose the Image-Proposal CLIP Encoder (IP-CLIP Encoder) to handle images with any number of mask proposals simultaneously (Sec. \ref{sec:IP-CLIP}). Then, \textit{mask-aware loss}  and \textit{self-distillation loss}  are introduced to fine-tune the IP-CLIP Encoder and make it distinguishable for different mask proposals while maintaining transferability (Sec. \ref{sec:Mask-aware tuning}).
The complete diagram of the MAFT is shown in Fig.~\ref{fig:finetune}, we use the ViT-B/16 CLIP model for illustration.


\subsection{Image-Proposal CLIP Encoder (IP-CLIP Encoder)}
\label{sec:IP-CLIP}
IP-CLIP Encoder aims to process arbitrary numbers of images and mask proposals simultaneously. We draw inspiration from MaskFormer \cite{cheng2021mask2former, cheng2021maskformer}, which uses attention-masks in Multihead Attention and provides the flexibility for accepting any number of queries and features of different masked regions. Accordingly, we apply mask proposals as attention-masks in Multihead Attention and designate independent classification queries for each mask proposal.
 % We draw inspiration from MaskFormer \cite{cheng2021mask2former, cheng2021maskformer}, which uses attention-masks to calculate Multihead Attention between queries and features.  It provides the flexibility for Multihead Attention to accept any number of queries and features of different masked regions.
 
In the IP-CLIP Encoder shown in Fig. \ref{fig:finetune}, we denote the features propagate between Transformer layers as $F^i$, where $i = [1,2...12]$. We can express $F^i$ as $F^i = [F^i_{cls};~ F^i_{feat}], \in \mathbb{R}^{(1 + hw) \times d}$, here $1$ represents a class-embedding vector ($F^i_{cls}$), $hw$ represents the number of the flattened image features ($F^i_{feat}$). 
% The output class-embedding vector $F^{12}_{cls}$ is utilized for classification (equals to $E^I$ in Sec. \ref{sec:Revisiting}). 
To obtain the classifications of all mask proposals simultaneously, we repeat $F^i_{cls}$ at layer $L$ $N$ times, where $N$ is the number of mask proposals, denoting the repeated class-embedding vectors as $F^{i*}_{cls}$. We can express the modified features ($F^{i*}$) as $F^{i*} = [F^{i*}_{cls};~ F^i_{feat}], \in \mathbb{R}^{(N + hw) \times d}$.

% Therefore, in the first $L$ Transformer layers, the propagation of $F^{i}$ keeps same with standard CLIP,
% \begin{equation}
%    F^{i+1} =\mathrm{TLayer}^i(F^i)
% \end{equation}
% $\mathrm{TLayer}^i$ denote i$^{th}$ Transformer layer. We simplify the representation of
% Transformer layer, whereas the start $L$ Transformer layers conduct the same structure with CLIP.

% Thus a standard Multihead Attention process in Transformer layers can be formulated as follows:
% \begin{equation}
%    \mathrm{MHAtten}(F^i) =\mathrm{Softmax}(\frac{Que(F^i)Key(F^i)^T}{\sqrt{d}})Val(F^i)
% \end{equation}
% where $Que(\cdot)$, $Key(\cdot)$, and $Val(\cdot)$ denote linear projections, $d$ is the hidden dimension of $F^i$. 

%  \begin{equation}
%     F^{i*} =[F^{i*}_{cls};~ F^i_{feat}], F^{i*} \in \mathbb{R}^{C \times (N + hw)}
% \end{equation}
\noindent \textbf{Propagation of $F^{i}$, where $i = [1, 2, ...L]$.}
We consider that CLIP's classification significantly relies on context information. In the first $L$ Transformer layers, the propagation of $F^{i}$ is the same as in standard CLIP. Specifically, $F^{i}_{cls}$ utilizes cross-attention with all pixels within $F^{i}_{feat}$, effectively retaining the context information. 

In the subsequent $12-L$ Transformer layers, the propagation of $F^{i*}$ can be partitioned into two parts: the propagation of $F^{i*}_{cls}$ and the propagation of $F^{i}_{feat}$.

\noindent \textbf{Propagation of $F^{i*}_{cls}$.}
We use $F^{i*}_{cls}$[$n$] and $M$[$n$] to represent the position $n$ in $F^{i*}_{cls}$ and $M$, where $n=[1,2...N]$. It is expected $F^{i*}_{cls}$[$n$] computes Multihead Attention for the positions where $M$[$n$]$=1$ and itself. To achieve this, we construct an attention bias $B \in \mathbb{R}^{N \times (N+hw)}$ as follows:
\begin{equation}
B_{(i,j)}=\left\{
\begin{aligned}
0  &, \mathrm{if} ~ {\hat{M}}_{(i,j)} = 1\\
-\infty  &, \mathrm{if} ~ {\hat{M}}_{(i,j)} = 0\\
\end{aligned}
\right.
,~~~ \hat{M} = [\mathrm{I}(N,N);~ \mathrm{Flat}(M)]
\end{equation}
here $\mathrm{I}(N,N)$ denotes $N^{th}$ order identity matrix, $\mathrm{Flat}$($\cdot$) denotes the \textit{flatten} operation. $\hat{M}$ is an intermediate variable for better representation. Therefore, a masked Multihead Attention is used for propagating $F^{i*}_{cls}$ 
% \footnote{We omit the MLP Layer and some Layer Normalizations in Transformer layers to  simplify the representation.}
:
\begin{equation}
   F^{(i+1)*}_{cls} =\mathrm{Softmax}(\frac{\mathrm{Que}(F^{i*}_{cls})\mathrm{Key}(F^{i*})^T}{\sqrt{d}} + B)\mathrm{Val}(F^{i*})
   \label{con:modified tlayers1}
\end{equation}
where $\mathrm{Que}(\cdot)$, $\mathrm{Key}(\cdot)$, and $\mathrm{Val}(\cdot)$ denote linear projections, $d$ is the hidden dimension of $F^{i*}$. Notably, We omit the MLP Layer and Layer Normalizations in Transformer layers to simplify the representation in Eq. \ref{con:modified tlayers1} and Eq. \ref{con:modified tlayers2}.

\noindent \textbf{Propagation of $F^{i}_{feat}$.}
A standard Multihead Attention is used for propagating $F^{i}_{feat}$ 
% \footnote{Similar to Eq. \ref{con:modified tlayers}, MLP Layer and Layer Normalizations are simplified.}
: 
\begin{equation}
   F^{i+1}_{feat} =\mathrm{Softmax}(\frac{\mathrm{Que}(F^{i}_{feat})\mathrm{Key}(F^{i}_{feat})^T}{\sqrt{d}})\mathrm{Val}(F^{i}_{feat})
   \label{con:modified tlayers2}
\end{equation}
Therefore, for any given mask proposal $M$[$n$], the corresponding class-embedding $F^{i*}_{cls}$[$n$] only performs Multihead Attention with $F^{i}_{feat}$ where $M$[$n$]$=1$ and $F^{i*}_{cls}$[$n$]. The propagation of $F^{i}_{feat}$ remains undisturbed by attention-masks. Compared with the frozen CLIP,  IP-CLIP Encoder leverages context information effectively and reduces computational costs.



\subsection{Objective}
\label{sec:Mask-aware tuning}
IP-CLIP Encoder with CLIP pre-trained parameters remains challenging in distinguishing different mask proposals, \textit{e.g.}, when the proposals contain more background regions than foreground objects, IP-CLIP may tend to classify them into the foreground categories. To overcome this limitation, we introduce \textit{mask-aware loss} and \textit{self-distillation loss} to fine-tune the IP-CLIP Encoder to be mask-aware without sacrificing transferability. 

We conduct the \textit{mask-aware} loss function ($\mathcal{L}_{ma}$) on $A^c$.  The goal is to assign high scores to high-quality proposals and low scores to low-quality proposals in $A^c$. Concretely, we use the Intersection over Union (IoU) score obtained from ground-truth and align it with the $A^c$ to prompt CLIP to become mask-aware. Assuming there are $k$ classes in ground-truth, we can generate $k$ binary maps of ground-truth and calculate the IOU score ($S_{IoU}$) with $N$ mask proposals. We identify a discrepancy between the maximum values of $A^c$ and $S_{IoU}$. The maximum value of $A^c$ tends to approach 1, whereas the maximum value of $S_{IoU}$ ranges from 0.75 to 0.99. This inconsistency can hinder the alignment between these two metrics. Therefore, we introduced a min-max normalization technique for $S_{IoU}$ as follows:
\begin{equation}
S_{IoU}^{norm} = \frac{S_{IoU} - min(S_{IoU})}{max(S_{IoU}) - min(S_{IoU})},  S_{IoU}\in \mathbb{R}^{K \times N}
\end{equation}
Meanwhile, we select $k$ pre-existing classes in $A^c$ ($A^c_{select}, A^c_{select}\in \mathbb{R}^{K \times N}$), and employ $SmoothL1$ Loss to align it with $S_{IoU}^{norm}$. Therefore, $\mathcal{L}_{ma}$ can be formulated as follows:
\begin{equation}
\mathcal{L}_{ma}(A^c_{select}, S_{IoU}^{norm}) = \mathrm{SmoothL1} (A^c_{select}, S_{IoU}^{norm})
\end{equation}
\begin{equation}
\mathrm{SmoothL1}(x, y) = \left\{
\begin{aligned}
 0.5\cdot (x - y)^2  &, ~~~ \mathrm{if} ~ |x - y| < 1\\
|x - y| - 0.5  &, ~~~ \mathrm{otherwise} ~ \\
\end{aligned}
\right.
\end{equation}

In addition to $\mathcal{L}_{ma}$, we also introduce a \textit{self-distillation} loss $\mathcal{L}_{dis}$ to maintain CLIP's transferability and alleviate overfitting on $C_{seen}$. 
Within $\mathcal{L}_{dis}$, we use a frozen CLIP as the \textit{teacher} net, the  IP-CLIP as the \textit{student} net for self-distillation.
The predictions of the frozen CLIP and IP-CLIP are expected to be the same when no mask is included. Denoting the output of the frozen CLIP as $A_{T}$, and the output of the fine-tuned IP-CLIP without masks as $A_{S}$. We use $SmoothL1$ Loss to minimize the difference as follows:
\begin{equation}
\mathcal{L}_{dis}(A_{S}, A_{T}) = \mathrm{SmoothL1} (A_{S}, A_{T})
\end{equation}
It is important to note that when processing an image through IP-CLIP without mask proposals, the resulting $A_{S}$ is a matrix with dimensions $\mathbb{R}^{C \times 1}$.
Therefore, the final loss function can be formulated as: $\mathcal{L} = \mathcal{L}_{ma} + {\lambda} {\mathcal{L}_{dis}}$, where we set the constant $\lambda$ to 1 in our experiments. The mask-aware fine-tuning process is efficient as we only perform a few iterations (less than 1 epoch).