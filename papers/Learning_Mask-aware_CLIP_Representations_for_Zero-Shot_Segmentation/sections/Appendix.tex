
Here we introduce technical details of the "frozen CLIP" approaches in Sec. \ref{sec:frozen CLIP}. The dataset settings are shown in Sec. \ref{sec:dataset}.
Moreover, we provide additional experiments in Sec. \ref{sec:add-exp}, and additional qualitative results in Sec. \ref{sec:vis}.


%%%%   A. Dataset
\section{Technical details of the "frozen CLIP" approaches}
\label{sec:frozen CLIP}   

\input{figs/tex/appendix-revision}

Fig. \ref{fig:revision}  presents an overview of the "frozen CLIP" approach. 
\textbf{During training}, a standard MaskFormer or Mask2Former is used as Proposal Generator to generate $N$ mask proposals ($M$, $M  \in \mathbb{R}^{N \times h \times w}$) and classification score ($A^p$, $A^p \in \mathbb{R}^{N \times |C_{seen}|}$).
\textbf{During testing}, the input image is merged with $M$ to obtain $N$ sub-images ($I_{sub}$, $I_{sub} \in \mathbb{R}^{N \times \hat{h} \times \hat{w}}$). These sub-images are fed into a frozen CLIP to get the CLIP classification score ($A^c$, $A^c \in \mathbb{R}^{N \times |C_{seen}\cup C_{unseen}|}$). Here $C_{seen}$ and $C_{unseen}$ represent a set of seen classes and unseen classes. An \textit{ensemble} operation is used to ensemble $A^p$ and $A^c$ for the final prediction. The \textit{merge} and the \textit{ensemble} operations will be introduced in detail in following:

\noindent \textbf{Merge operation.}
To generate appropriate sub-images based on mask proposals, \cite{zegformer} presents three different \textit{merge} operations: 1) mask, 2) crop, 3) mask $\&$ crop. Through experimentation, they demonstrate that the mask $\&$ crop option yields the best results. Figure \ref{fig:merge} provides an example of these operations. It's worth noting that all sub-images are resized to $\hat{h} \times \hat{w}$, here $\hat{h}$ and $\hat{w}$ typically take a value of 224, which is the default input size of CLIP Image Encoder. Although acceptable results can be obtained with the \textit{merge} operation, it involves repeatedly feeding images into CLIP, which leads to significant computational redundancy.

\noindent \textbf{Ensemble operation.}
Comparatively, $A^p$ provides higher confidence classification scores for the seen classes, and $A^c$ provides higher confidence classification scores for the unseen classes. Therefore, an ensemble of $A^p$ and $A^c$ achieves better results. The \textit{ensemble} operation can be formulated as:
\begin{equation}
\hat{A}(c)=\left\{
\begin{aligned}
&A^p(c)^\lambda \cdot A^c(c)^{(1 - \lambda)}~ , ~c \in C^{seen}\\
&A^c(c)^\lambda ~~~~~~~~~~~~~~~~~~~~~~, ~c \in C^{unseen}\\
% A^p(c)^{(1 - \lambda)} \cdot A^c(c)^\lambda   &, c \in C^{unseen}\\
\end{aligned}
\right.
\end{equation}
here a geometry mean of $A^p$ and $A^c$ is calculated (dubbed as $\hat{A}$), and the contribution of both classification scores is balanced by $\lambda$. As per literature \cite{zegformer, zsseg, freeseg}, $\lambda$ usually takes values from 0.6 to 0.8. Therefore, the final output ($O$, $O \in \mathbb{R}^{|C_{seen}\cup C_{unseen}| \times h \times w }$) can be obtained by matrix multiplication: $O = \hat{A}^T\cdot M$.
With the \textit{ensemble} operation, the classification results of seen classes primarily depend on $A^p$, whereas the classification results of unseen classes mainly rely on $A^c$.

\input{figs/tex/appendix-merge}



%%%%   B. Dataset
\section{Dataset}
\label{sec:dataset}   
We follow \cite{zs5, gu2020context, pastore2021closer, zegformer, zsseg} to conduct experiments on three benchmarks of the popular \textit{zero-shot} setting, Pascal-VOC, COCO-Stuff and ADE20K, to evaluate the performance of MAFT. Additionally, we evaluate MAFT on the \textit{open-vocabulary} setting \cite{ovseg, zsseg}, \textit{i.e.}, training on COCO-Stuff and testing on ADE20K, Pascal-Context, and Pascal-VOC. 

\begin{itemize}[itemsep=2pt,topsep=0pt,parsep=0pt]
\item \textbf{COCO-Stuff}: COCO-Stuff is a large-scale semantic segmentation dataset that includes 171 classes.  For the \textit{zero-shot} setting \cite{zegformer, zsseg, freeseg}, it is divided into 156 seen classes for training and 15 unseen classes for testing. For the \textit{open-vocabulary} setting, all 171 classes are used for training.

\item \textbf{Pascal-VOC}: There are 10582 images for training and 1,449 images for testing. For the \textit{zero-shot} setting, Pascal-VOC is split into 15 seen classes and 5 unseen classes. For the \textit{open-vocabulary} setting, all 20 classes are used for evaluation (dubbed as PAS-20).

\item \textbf{ADE20K}: ADE20K contains 25k images for training and 2k images for validation. For the \textit{zero-shot} setting, we follow \cite{zegformer} to choose 847 classes present in both training and validation sets, and split them into 572 seen and 275 unseen classes. For the \textit{open-vocabulary} setting, we use two settings of ADE20K: 150 classes (dubbed as A-150) and 847 classes (dubbed as A-847).

\item \textbf{Pascal-Context} is an extensive dataset of Pascal-VOC 2010. Two versions are used for \textit{open-vocabulary} setting, one with 59 frequently used classes (dubbed as PC-59) and another with the whole 459 classes (dubbed as PC-459).
\end{itemize}



%%%%   C. Additional experiments
\section{Additional experiments}
\label{sec:add-exp}

\subsection{Analysis of the Upper Bound of MAFT}
\input{tables/appendix-upp}

Considering the \textit{mask-aware} loss may be limited if the quality of proposals is too bad, we conducted an evaluation of the Upper Bound while using Mask2Former as the proposal generator. The results are presented in Tab. \ref{tab:appendix-upp}. Specifically, we replace $A^c$ by $S_{IoU}$  (IoU score between binary ground-truth masks and proposals) during inference, and multiply proposals with $S_{IoU}$ to obtain the segmentation result. This result can be regarded as the Upper Bound for the given proposals. Notably, the Upper Bound achieves satisfactory results (77.6 \% mIoU), indicating Mask2Former is capable of providing high-quality proposals in most cases. Additionally, there is still a large gap between the current performance and the Upper Bound ($\approx$ 30\% mIoU), which suggests that our MAFT has enormous potential for improvement, whereas we have achieved state-of-the-art performance.

\subsection{Analysis of the Self-Training (ST) strategy}
\input{tables/appendix-st}

Several previous approaches \cite{STRICT, zsseg, zhou2022extract} adopt the Self-Training (ST) strategy to enhance performance. we conduct experiments to investigate the application of ST into our method. Specifically, we use the existing $\mathrm{FreeSeg+MAFT}$ model to generate pseudo-labels for unseen classes on the training data, and then re-train $\mathrm{FreeSeg}$ with the pseudo-labels.
Results are shown in Tab. \ref{tab:appendix-st}.

The improvement of ST on the unseen category is significant (Pascal: 81.8 \% $\rightarrow$ 86.3\%, COCO: 50.4\% $\rightarrow$ 55.2\%) in terms of mIoU$^u$. However, it's essential to highlight the applicability of ST is limited by a crucial requirement: \textbf{unseen classes need to be obtained during training.} This requirement poses significant limitations on generalizing ST to various scenarios, \textit{e.g.}, open-vocabulary settings, since images of unseen classes may not be obtained during training. 


%%%%   D. Visualization
\section{Visualization}
\label{sec:vis}   
\input{figs/tex/appendix-vis} 


We provide more qualitative results, including typical proposals and top-5 $A^c$ (Fig. \ref{fig:appendix-vis-proposal}), as well as examples of models trained on COCO-Stuff and text on A-847 (Fig. \ref{fig:vis-a847}), A-150 (Fig. \ref{fig:vis-a150}), PC-459 (Fig. \ref{fig:vis-pc459}), PC-59 (Fig. \ref{fig:vis-pc59}), Pascal-VOC (Fig. \ref{fig:vis-p20}), and COCO-Stuff(Fig. \ref{fig:vis-coco}).


\noindent \textbf{Typical Proposals and Top-5 $A^c$.} Fig. \ref{fig:appendix-vis-proposal} shows frozen CLIP and mask-aware CLIP classifications of typical proposals. In the 2$^{nd}$ column, we provide high-quality proposals of \textit{thing} classes. Both the frozen CLIP and mask-aware CLIP provide high classification scores for the correct classes. In the 3$^{rd}$ column, we provide proposals that only contain part of the objects (row 1-3), and proposals containing more than 1 class (row 4). The mask-aware CLIP provides more proper results compared to the frozen CLIP. In the 4$^{th}$ column, we provide some high-quality background proposals. The frozen CLIP typically gives incorrect predictions, but the mask-aware CLIP assigns high scores for the correct classes.

\noindent \textbf{Qualitative Analysis.} Fig. \ref{fig:vis-a847},\ref{fig:vis-a150},\ref{fig:vis-pc459},\ref{fig:vis-pc59},\ref{fig:vis-p20},\ref{fig:vis-coco} show segmentation results on Pascal-VOC, COCO-Stuff, ADE20K. In Pascal-VOC dataset (Fig. \ref{fig:vis-p20}), which only contains 20 \textit{thing} classes, the $\mathrm{FreeSeg+MAFT}$ model tends to assign background regions to the similar \textit{thing} classes, \textit{e.g.},  "train" in row 1, "pottedplant" in row3-4. "boat" in row 8. In A-847, A-150, PC-459, PC-59 and COCO-Stuff datasets, both seen classes and unseen classes exist in the input images, the $\mathrm{FreeSeg+MAFT}$ model generates better segmentation results compared to $\mathrm{FreeSeg}$.

% \newpage