
% \subsection{Problem Setting}
\noindent \textbf{Problem Setting.}
Zero-shot segmentation aims at training a segmentation model capable of segmenting novel objects using text descriptions. Given two category sets $C_{seen}$ and $C_{unseen}$ respectively, where $C_{seen}$ and $C_{unseen}$ are disjoint in terms of object categories ($C_{seen} \cap C_{unseen} = \emptyset$). The model is trained on $C_{seen}$ and directly tested on both $C_{seen}$ and $C_{unseen}$. Typically, $C_{seen}$ and $C_{unseen}$ are described with semantic words (\textit{e.g.} sheep, grass).

% \subsection{Revisiting the "frozen CLIP" paradigm}
\noindent \textbf{Revisiting the "frozen CLIP" paradigm.}
\label{sec:Revisiting}
The "frozen CLIP" approaches \cite{zegformer, zsseg, freeseg, ovseg} execute zero-shot segmentation in two steps: mask proposals generation and mask proposals classification. 
In the first step, these approaches train a Proposal Generator to generate $N$ class-agnostic mask proposals (denoting as $M$, $M \in \mathbb{R}^{N \times H \times W}$) and their corresponding classification scores (denoting as $A^{p}$, $A^{p} \in \mathbb{R}^{N \times |C_{seen}|}$). MaskFormer \cite{cheng2021maskformer} and Mask2Former \cite{cheng2021mask2former} are generally used as the Proposal Generator since the Hungarian matching \cite{kuhn1955hungarian} in the training process makes the mask proposals strongly generalizable.
In the second step, $N$ suitable sub-images ($I_{sub}$) are obtained by \textit{merging} $N$ mask proposals and the input image. $I_{sub}$ is then fed into the CLIP Image Encoder to obtain the image embedding ($E^I$). Meanwhile, text embedding ($E^T$) is generated by a CLIP Text Encoder. The classification score ($A^{c}, A^{c}  \in \mathbb{R}^{N \times C}$) predicted by CLIP is calculated as:
\begin{equation}
\label{eq:prob} 
A^{c}_i = \mathrm{Softmax}(\frac{\exp(\frac{1}{\tau}s_{c} (E^T_{i}, E^I))}{\sum_{i=0}^{C}\exp(\frac{1}{\tau}s_{c}(E^T_{i}, E^I))}), i = [1,2,...C]
\end{equation}
where  $\tau$ is the temperature hyper-parameter. $s_{c}(E^T_{i}, E^I)=\frac{E^T_{i} \cdot E^I }{|E^T_{i}| |E^I|}$ represents the cosine similarity between $E^T_{i}$ and $E^I$. $C$ is the number of classes, with $C = |C_{seen}|$ during training and $C = |C_{seen}\cup C_{unseen}|$ during inference. Noting that CLIP is frozen when training to avoid overfitting.

To further enhance the reliability of $A^{c}$, the classification score of the Proposal Generator ($A^{p}$) is ensembled with $A^{c}$ since $A^{p}$ is more reliable on seen classes. This \textit{ensemble} operation is wildly used in "frozen CLIP" approaches.  The pipeline of "frozen CLIP", as well as the \textit{merge} and \textit{ensemble} operations, are described in detail in the Appendix. 

Although  "frozen CLIP" approaches have achieved promising results, it is clear that directly adopting an image-level pre-trained CLIP for proposal classification can be suboptimal. A frozen CLIP usually produces numerous false positives, and the \textit{merge} operation may destroy the context information of an input image. In view of this, we rethink the paradigm of the frozen CLIP and explore a new solution for proposal classification.