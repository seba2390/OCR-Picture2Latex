Semantic segmentation, one of the most widely researched topics in computer vision, has achieved remarkable success \cite{chen2017deeplab,pspnet,huang2019ccnet,huang2021alignseg} with the development of deep learning techniques \cite{resnet}. However, traditional segmentation models are only capable of segmenting a few predefined categories within a closed vocabulary \cite{pascal, coco, miao2021vspw, miao2022large}, which is much smaller than the number of categories used by humans to describe the real world. Therefore, zero-shot segmentation \cite{spnet, zs5, cagnet, han2023global} is introduced to segment objects using arbitrary categories described by texts.

Recently, large-scale visual-language pre-training models (\textit{e.g.} CLIP \cite{radford2021learning} and ALIGN \cite{jia2021scaling}) have shown impressive transferability in recognizing novel categories, leading to their increased adoption for tackling the challenging zero-shot segmentation task \cite{zegformer,zsseg,ovseg,freeseg}. A mainstream solution follows the "frozen CLIP" paradigm, which executes the zero-shot segmentation with two steps: 1) first employing a Proposal Generator to produce class-agnostic mask proposals and 2) then leveraging a frozen pre-trained CLIP to classify each mask proposal via similarity matching in the aligned image-text feature space. While acceptable results are obtained, we reveal that these approaches overlook a crucial issue, \textit{i.e.} the frozen CLIP is insensitive to different mask proposals and tends to produce similar predictions for various proposals of the same image. 

To better illustrate the above-mentioned issue, we show several examples in Fig. \ref{fig:intro}. We use MaskFormer \cite{cheng2021maskformer} to generate a series of mask proposals and select three typical ones. When using frozen CLIP for classification, we observe that it correctly classifies the high-quality \textit{swan} proposal $p_1$. However, for the other two proposals $p_2$ and $p_3$, which respectively contain only shape information of \textit{swan} and both regions of \textit{swan} and \textit{river}, the frozen CLIP produces similar predictions compared to $p_1$. 
This is reasonable since CLIP is trained by image-text pairs, making it insensitive to pixel-level information (\textit{e.g.} background noise), and resulting in numerous false positives.
Based on the above observations, we consider that an expected CLIP for zero-shot segmentation task should \textbf{1) be sensitive to different mask proposals, 2) not compromise its original transferability on novel classes.}
 
To this end, we introduce a Mask-aware CLIP Fine-tuning method (dubbed MAFT). To make CLIP sensitive to different mask proposals, we devise an Image-Proposals CLIP Encoder (IP-CLIP Encoder), which utilizes mask proposals to perform masked Multihead Attention \cite{cheng2021maskformer, cheng2021mask2former}. This design enables the model to handle arbitrary numbers of images and proposals simultaneously. The \textit{mask-aware loss} is proposed to minimise the distance between the IoU score of mask proposals and the classification score of IP-CLIP Encoder, prompting IP-CLIP Encoder to differentiate various proposals. 
Besides, to preserve CLIP's zero-shot transferability, we utilize a frozen CLIP as a teacher network to facilitate fine-tuning. This is achieved by aligning the outputs of the frozen CLIP and IP-CLIP Encoder through \textit{self-distillation loss}.
By performing MAFT, several advantages are provided: 1) Fine-tuning is efficient since only a few mask proposals need to be classified. 2) Compared to pixel-level fine-tuning, mask-aware fine-tuning hardly alters the structure of CLIP itself, preserving its maximum transferability. 3) Mask-aware fine-tuning of CLIP is released from the segmentation module, making it plug-and-play and applicable to any "frozen CLIP" approaches. As shown in Fig. \ref{fig:intro}, the mask-aware CLIP can well distinguish different proposals and provide proper classification scores for both seen (\textit{river}) and unseen (\textit{swan}) classes.


% We evaluate our MAFT on three commonly used zero-shot segmentation benchmarks: COCO-Stuff, Pascal-VOC, and ADE20K. Extensive experiments show that MAFT works well with various zero-shot segmentation methods. It is significantly better than the freezing CLIP counterpart, leading to new state-of-the-art results on all three datasets.
We evaluate our MAFT on three commonly used zero-shot segmentation benchmarks: COCO-Stuff \cite{coco}, Pascal-VOC \cite{pascal}, and ADE20K \cite{ade20k}. Extensive experiments show that MAFT works well with various zero-shot segmentation methods. In particular, by plugging MAFT, the state-of-the-art approach FreeSeg \cite{freeseg} achieves superior performance on COCO-Stuff (42.2\% $\rightarrow$ 50.4\%), Pascal-VOC (78.6\% $\rightarrow$ 81.8\%) and ADE20K (4.4\% $\rightarrow$ 8.7\%) in terms of mIoU of unseen classes. Furthermore, we conduct experiments in a \textit{open-vocabulary} setting, where MAFT enhances the performance of A-847 \cite{ade20k}, A-150 \cite{ade20k}, PC-459 \cite{pc}, PC-59 \cite{pc} and PAS-20 \cite{pascal} datasets by +3.0\%, +11.2\%, +6.4\%, +19.1\% and +4.4\%, respectively.
Notably, our approach outperforms the freezing CLIP counterpart and establishes new state-of-the-art results on all datasets.


\input{figs/tex/fig_intro}
