%\subsection{Few-Shot Segmentation}
\noindent \textbf{Zero-Shot Segmentation}~\cite{shaban2017one} is established to break the restriction of categories and perform segmentation on unseen classes.
Earlier works SPNet \cite{spnet} learn a joint pixel and vocabulary concept embedding space, ZS5 \cite{zs5} utilizes a generative model to generate pixel-level features based on word embeddings of unseen classes, CaGNet \cite{cagnet} incorporates context information for better feature generation.
Recent approaches take the advent of large-scale visual-language models (\textit{e.g.} CLIP \cite{radford2021learning} and ALIGN \cite{jia2021scaling}) to leverage rich alignment features from image-text pairs.
\cite{zabari2021semantic} uses CLIP to generate pseudo-labels for single-image segmentation. STRICT \cite{STRICT} obtains pixel-level pseudo-labels from CLIP for unlabeled pixels and proposes a self-training strategy to capture latent information on unseen classes. LSeg \cite{ghiasi2021open} trains a CNN model to compute per-pixel image embeddings and use CLIP text embeddings as a classifier. \cite{xu2022groupvit} employs contrastive supervision to learn segmentation masks from text.

Concurrently, recent works \cite{zegformer, zsseg, freeseg, ovseg, ghiasi2022scaling} follow the "frozen CLIP" paradigm for zero-shot segmentation, they first generate a series of mask proposals and then utilize CLIP \cite{radford2021learning} or ALIGN \cite{jia2021scaling} to classify them. ZSSeg and OVSeg \cite{zsseg, ovseg} train CLIP adapters to boost performance. FreeSeg\cite{freeseg} simultaneously uses semantic, instance, and panoptic labels and performs fusion training. OpenSeg\cite{ghiasi2022scaling} takes extra images with image-level supervision (\textit{e.g.} captions) to scale up training data. 
% However, the freezing CLIP reduces the representational ability of the model, and does not fit the distribution to zero-shot segmentation task.

\noindent \textbf{Pre-trained model fine-tuning}
is widely used for transferring pre-trained knowledge to downstream tasks, \textit{e.g.} segmentation. However, this strategy may not work well for data-limited tasks like few-shot learning and zero-shot learning due to the daunting \textit{overfitting} problem.
To address this problem and transfer pre-trained knowledge to data-limited tasks, \cite{zhou2022learning, zhou2022conditional, guo2022texts, zsseg, ovseg, freeseg} propose to learn text prompts or image prompts by using (a few) annotated images from target dataset. SVF \cite{svf}  fine-tunes only a few parameters in the pre-trained image encoder to adapt pre-trained knowledge to few-shot segmentation. \cite{zhangcoinseg, zhang2022mining} use contrastive learning to avoid catastrophic forgetting. Alternatively, many outstanding approaches in data-limited tasks \cite{hsnet, cyctr, mmformer, zegformer, zsseg} choose to freeze the parameters of pre-trained models to maintain the transferability.

Specific to the task of zero-shot/ open-vocabulary segmentation, mainstream approaches use frozen CLIP to avoid overfitting. Recently, MaskCLIP \cite{zhou2022extract} conducts adequate experiments to fine-tune CLIP for open-vocabulary segmentation but has failed. While this attempt is meaningful and appreciated,  it is believed that the failure is due to the large domain gap between pixel-level and image-level tasks. 
This motivates us further research fine-tuning CLIP to be mask-aware (region-level task).

