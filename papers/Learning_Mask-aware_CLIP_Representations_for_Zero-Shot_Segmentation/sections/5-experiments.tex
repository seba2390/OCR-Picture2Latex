\input{tables/zss}
\subsection{Setting}
\noindent \textbf{Dataset.}
We first follow \cite{zs5, gu2020context, pastore2021closer, zegformer, zsseg} to conduct experiments on three popular zero-shot segmentation benchmarks, Pascal-VOC, COCO-Stuff and ADE20K, to evaluate our method. Then, we evaluate MAFT on the \textit{open-vocabulary} setting \cite{ovseg, zsseg}, \textit{i.e.}, training on COCO-Stuff and testing on ADE20K (A-847, A-150), Pascal-Context (PC-459, PC-59), and Pascal-VOC (PAS-20). More details of the dataset settings are provided in the Appendix.

\noindent \textbf{Evaluation Metrics.}
To quantitatively evaluate the performance, we follow standard practice \cite{zs5, spnet, cagnet, STRICT, zegformer, zsseg, freeseg}, adopt mean Intersection over Union (mIoU) to respectively evaluate the performance for seen classes (IoU$^s$) and unseen classes (IoU$^u$). We also employ the harmonic mean IoU (hIoU) among the seen and unseen classes to measure comprehensive performance.

\noindent \textbf{Methods.}
Three representative methods are used to verify the generality of MAFT. We unify the three methods into the same framework, with all methods using ResNet101 as the backbone of Proposal Generator and ViT-B/16 CLIP model for a fair comparison.

\begin{itemize}[itemsep=2pt,topsep=0pt,parsep=0pt]
\item \textbf{ZegFormer} (CVPR 2022) \cite{zegformer} is an early adopter of the "frozen CLIP" paradigm. It uses MaskFormer as Proposal Generator and employs an \textit{ensemble} operation to improve the confidence of the results.
\item \textbf{ZSSeg} (ECCV 2022) \cite{zsseg} uses MaskFormer as Proposal Generator and introduces learnable prompts to improve classification accuracy, which significantly affects the subsequent methods. ZSSeg also adopts a self-training strategy, this strategy is excluded from all methods for a fair comparison.
\item \textbf{FreeSeg} (CVPR 2023) \cite{freeseg} represents the state-of-the-art method, unifies semantic, instance, and panoptic segmentation tasks and uses annotations from all three tasks for fusion training. We retrain FreeSeg with only the semantic annotations to ensure fairness.
\end{itemize}

\noindent \textbf{Implementation details.}
We employ ResNet101 as backbone of the Proposal Generator and ViT-B/16 CLIP model. The training process  consists of two stages.
For the \textbf{first} stage, we follow the official code of ZegFormer, ZSSeg and FreeSeg for model training. 
For the \textbf{second} stage, we fine-tune IP-CLIP Encoder with MAFT. We take the batch size
of 16 and set CLIP input image size to 480$\times$480. The optimizer is AdamW with a learning rate of 0.00001 and weight decay of 0.00001. The number of training iterations is set to 100 for Pascal-VOC, 1000 for COCO-Stuff and 5000 for ADE20K.

\subsection{Comparisons with State-of-the-art Methods}
\input{tables/ovs}
In this section, three representative methods are used  \cite{zegformer, zsseg, freeseg} to evaluate the effectiveness of MAFT. We compare three representative methods with MAFT and frozen CLIP. Additionally, we compare the results with previous state-of-the-art methods  \cite{spnet, zs5, cagnet, STRICT}.

\noindent \textbf{Comparisons in the \textit{zero-shot} setting.}
In Tab. \ref{tab:zss}, MAFT remarkably improves the performance. MAFT promotes the state-of-the-art performance by + 8.2\% on COCO, + 3.2\% on Pascal, and +4.3\% on ADE20K in terms of mIoU for unseen classes. It is important to note that the results for seen classes are mainly based on $A^p$ rather than $A^c$ due to the \textit{ensemble} operation in \cite{zegformer, zsseg, freeseg} (Details in Sec. \ref{sec:prelimiary}). Therefore, the effect of MAFT on the seen classes is relatively insignificant. 

\noindent \textbf{Comparisons without ensemble strategy.}
To better showcase the performance gains from MAFT, we removed the \textit{ensemble} operation in \cite{zegformer, zsseg, freeseg} and presented the results in Tab. \ref{tab:zss-woensem}.  It can be seen that the performance of different methods is significantly improved after applying MAFT. In particular, the state-of-the-art method FreeSeg achieves hIoU improvements of 19.1\%, 7.0\%, and 8.3\% on COCO, VOC2012 and ADE20K datasets. 

\noindent \textbf{Comparisons in the \textit{open-vocabulary} setting.}
We further evaluated the transferability of MAFT in the \textit{open-vocabulary} setting \cite{ovseg, zsseg}, using FreeSeg as a baseline for comparison. Results are shown in Tab. \ref{tab:ovs}.
Compared with OVSeg \cite{ovseg} and OpenSeg \cite{ghiasi2022scaling}, FreeSeg achieves suboptimal performance. However, the proposed MAFT enhances the performance of A-847, A-150, PC-459, PC-59 and PAS-20 by 3.0\%,11.2\%, 6.4\%, 19.1\% and 4.4\%, and outperforms OpenSeg on all five datasets.

\subsection{Ablation Study}
\input{tables/abla}

We conduct ablation studies on various choices of designs of our MAFT to show their contribution to the final results in Tab. \ref{tab:ablations}. FreeSeg is used as the baseline model and \textit{ensemble} operation is removed.

\noindent \textbf{Component-wise ablations.} To understand the effect of each component in the MAFT, including the IP-CLIP Encoder and the fine-tuning strategy ($\mathcal{L}_{ma}$, $\mathcal{L}_{dis}$), we start with standard FreeSeg and progressively add each design. (Tab. \ref{tab:ab_1}). 
FreeSeg uses frozen CLIP and yields inferior performance due to CLIP's mask-unaware property ($1^{st}$ row). Then, IP-CLIP Encoder obtains rich context information and greatly reduces the omputational costs, resulting in an improvement of 7.1\% on seen classes and 6.9\% on unseen classes. However, mask-aware is not accomplished at this point.
Using only $\mathcal{L}_{ma}$ for fine-tuning CLIP produces decent performance  (the $3^{rd}$ result). The introduction of $\mathcal{L}_{dis}$ (the $4^{th}$ result) maintains transferability while learning mask-aware representations, which further enhances the performance on unseen classes by 2.6\%.

\noindent \textbf{Effect of different $\mathcal{L}_{ma}$.} 
\textit{Mask-aware} Loss $\mathcal{L}_{ma}$ is an essential component of MAFT. In Tab. \ref{tab:ab_2}, we investigate how different loss functions ($L1$, $L2$, $SmoothL1$ and $KL$ Loss) impact performance, here we remove $\mathcal{L}_{dis}$ for analysis. Results show $SmoothL1$ Loss boosts performance on $C_{unseen}$ to 47.1\% (+17.8\%), $KL$ Loss provides +12.5\% improvement on $C_{seen}$, but only +11.8\% on $C_{unseen}$, manifesting $KL$ Loss compromises the model of transferability comparing with $SmoothL1$ Loss.

\noindent \textbf{Training iterations.} 
Tab. \ref{tab:ab-iter} examines the impact of training iterations. Increasing the number of iterations leads to gradual improvement of IoU$^s$, but it also results in significant overfitting on unseen classes. Therefore, we choose to fine-tune 1k iterations to maximize the zero-shot ability.

\noindent \textbf{Frozen units in CLIP.} 
We also explore the impact of fine-tuning units within IP-CLIP Encoder. As illustrated in Fig. \ref{fig:finetune}, IP-CLIP Encoder comprises convolution layers (dubbed as $conv.$), class embedding ($cls.$), Transformer layers, final projection ($proj.$) and positional embedding ($pos.$, not shown in Fig. \ref{fig:finetune}). We start with fine-tuning the entire IP-CLIP Encoder, and then freezing each unit sequentially, as specified in Tab. \ref{tab:ab-units}. We only freeze $MLP$ in the Transformer layers (dubbed as $mlp$). Compared with fine-tuning the entire IP-CLIP Encoder, the performance of mIoU$^u$ is improved by 5.0\% when freezing $conv.$, $cls.$, $pos.$ and $mlp$.

\noindent \textbf{Start mask attention layer}.
Tab. \ref{tab:ab-layer} presents the results of the start mask attention layer ($L$). 
We observe a significant improvement in the performance of unseen classes by +3.4\% when the value of $L$ increases from 0 to 8. This could be attributed to the fact that starting masked Multihead Attention later enables $F^{i*}_{cls}$ to gain more context information. However, the performance significantly drops when $L=10$ (from 49.7\% to 45.7\%), which may be due to the loss of mask-aware property.

\subsection{Extending MAFT with SAM}
\input{tables/sam}
We explore using the Segment Anything Model \cite{kirillov2023segment} (SAM) as the proposal generator. We evaluate the performance with SAM-H using an original CLIP (dubbed $\mathrm{SAM}$) or a mask-aware fine-tuned CLIP (dubbed $\mathrm{SAM+MAFT}$). In fact, SAM can be seamlessly integrated into our framework as the proposal generator. The results are shown in Tab. \ref{tab:sam}. Experiments are conducted under both \textit{zero-shot} setting and \textit{open-vocabulary} setting.

It can be observed that $\mathrm{SAM+MAFT}$ obtains significant improvement over $\mathrm{SAM}$ under both settings. Besides, $\mathrm{SAM+MAFT}$ also surpasses $\mathrm{FreeSeg+MAFT}$ on all benchmarks. Particularly, in the zero-shot setting (Pascal-VOC), $\mathrm{SAM+MAFT}$ outperforms $\mathrm{FreeSeg+MAFT}$ by 6.8\% in terms of mIoU$^u$. This enhancement can be attributed to the stronger generalization capabilities of SAM for unseen classes. 

\subsection{Extending MAFT with more Vision-Language Models}
\input{tables/llms}
In order to demonstrate the efficacy and robustness of MAFT, we conduct experiments using stronger (CLIP-ViT-L) and ResNet-based (CLIP-Res50) Vision-Language Models. The open-vocabulary results are shown in Tab. \ref{tab:backbone}, we also include the results of OVSeg with CLIP-ViT-L for comparison.

\noindent \textbf{CLIP-ViT-L.}
According to Tab. \ref{tab:backbone}, FreeSeg with a standard CLIP-ViT-L model (dubbed $\mathrm{FreeSeg}$) still can not achieve satisfactory results. However, by integrating our MAFT (dubbed $\mathrm{FreeSeg+MAFT}$), the segmentation results are remarkably enhanced, thus establishing new state-of-the-art benchmarks.

\noindent \textbf{CLIP-Res50.}
Our MAFT can easily adapted into ResNet-based models. Specifically, we modified the $\mathrm{AttentionPool2d}$ unit within CLIP-R50 Image Encoder. The mask proposals are introduced as attention bias ($B$) in Multihead Attention, with $F_{cls}$ being repeated N times. Notably in CLIP-R50, $F_{cls}$ is obtained via $\mathrm{GlobalAveragePooling}$ performing on $F_{feat}$. The results are presented in Tab. \ref{tab:backbone}. The performance on all 5 datasets is improved by a large margin. $\mathrm{FreeSeg+MAFT}$ with CLIP-R50 achieves competitive results with some CLIP-ViT-B-based methods according to Tab. \ref{tab:ovs}.

\subsection{Qualitative Study}

\noindent \textbf{Visualizations of typical proposals.}
Fig. \ref{fig:vis-proposal} shows frozen CLIP and mask-aware CLIP classifications of typical proposals, 
including high-quality proposals of foreground ($p_1$, $p_4$), high-quality proposals of background ($p_3$, $p_6$), a proposal with background noise ($p_2$), and a proposal containing part of the foreground ($p_5$). The proposal regions are highlighted in green or yellow. \\
Several observations can be obtained: (1) The frozen CLIP provides good predictions for $p_1$ and $p_4$. (2) The frozen CLIP assigns $p_2$ as $cat$ and $p_5$ as $horse$, with scores even higher than $p_1$, $p_4$, indicating the frozen CLIP cannot distinguish proposals containing information on the same objects. (3) The frozen CLIP fails to give correct predictions for $p_3$ and $p_6$, which may be due to the lack of context information. (4) Our mask-aware CLIP gives good predictions for high-quality proposals ($p_1$, $p_3$, $p_4$, $p_6$) and provides suitable predictions for $p_2$ and $p_5$.


\noindent \textbf{Qualitative analysis.}
We show some visual examples in Fig. \ref{fig:vis-final}. Some segmentation results of FreeSeg contain background noise (\textit{e.g.} the $1^{st}$ \& $2^{nd}$ row, $3^{rd}$ column) or contain only part of the objects ($3^{rd}$ row, $3^{rd}$ column). In ADE20K-847 dataset, too many classes may lead to the anticipated results (last row, $3^{rd}$ column) with the frozen CLIP.
Using a mask-aware CLIP to learn mask-aware representations can significantly improve these segmentation results, as evident from the last column.

More visual samples are shown in the Appendix.