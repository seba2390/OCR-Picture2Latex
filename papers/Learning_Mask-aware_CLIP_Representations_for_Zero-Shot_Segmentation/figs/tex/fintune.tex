\begin{figure}
% \vspace{-15mm}
\begin{center}
   \includegraphics[width=0.99\linewidth]{figs/pdf/fintune.pdf}
\end{center}
% \vspace{-2mm}
   \caption{
    Overview of the Mask-Aware Fine-tuning (MAFT). 
    In IP-CLIP Encoder, we modify the CLIP Image Encoder, and apply the mask proposals as attention bias in Multihead Attention from the $L^{th}$ layer. The final projection unit is an MLP module used for reshaping the channels of $F_{cls}$. \textit{w.o.} $M$ denotes IP-CLIP Encoder processes image without utilizing mask proposals ($M$). \textit{Mask-aware} Loss is designed to train CLIP to be mask-aware, while \textit{Self-distillation} Loss is designed to maintain the transferability. Only the IP-CLIP Encoder is trained (\textcolor{orange}{orange} part), the Proposal Generator and the CLIP Text Encoder are frozen (\textcolor{cyan}{blue} part).
   }
\label{fig:finetune}
% \vspace{-5mm}
\end{figure}
