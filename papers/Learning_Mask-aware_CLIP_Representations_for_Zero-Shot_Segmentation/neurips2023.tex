\documentclass{article}
\pdfoutput=1

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2023


% ready for submission
% \usepackage{neurips_2023}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
\usepackage[preprint]{neurips_2023}
\usepackage{natbib}
\setcitestyle{numbers,square}

% to compile a camera-ready version, add the [final] option, e.g.:
% \usepackage[final]{neurips_2023}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2023}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage[colorlinks=true,linkcolor=red,citecolor=green,urlcolor=red,]{hyperref}
\usepackage{hyperref}       % hyperlinks

\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors

% my package
\usepackage{graphicx}
\usepackage{enumitem}
\usepackage{multirow}
% \usepackage[dvipsnames,table]{xcolor} 
\usepackage{colortbl}
\usepackage{arydshln}       % 负责画虚线的包

\usepackage{float}
\usepackage{subfig}
\usepackage{amsmath}
\usepackage{makecell}


\usepackage{appendix}
% \usepackage[table]{xcolor}

%\title{Mask-aware CLIP Fine-tuning for \\ Zero-Shot Segmentation}
\title{Learning Mask-aware CLIP Representations for \\ Zero-Shot Segmentation}

\author{%
  Siyu Jiao$^{1, 2, 3}$\thanks{Work done during an internship at Picsart AI Research (PAIR).},\quad Yunchao Wei$^{1, 2, 3}$, \quad Yaowei Wang$^{3}$, \quad Yao Zhao$^{1, 2, 3}$, \quad \textbf{Humphrey Shi} $^{4}$\\
  \\
  $^1$~Institute of Information Science, Beijing Jiaotong University \\
  $^{2}$~Beijing Key Laboratory of Advanced Information Science and Network \\
  $^3$~Peng Cheng Laboratory \quad  $^4$~Picsart AI Research (PAIR) \\
  \texttt{jiaosiyu99@bjtu.edu.cn} \\
}


\begin{document}


\maketitle


\begin{abstract}

Recently, pre-trained vision-language models have been increasingly used to tackle the challenging zero-shot segmentation task. Typical solutions follow the paradigm of first generating mask proposals and then adopting CLIP to classify them. To maintain the CLIP's zero-shot transferability, previous practices favour to freeze CLIP during training. However, in the paper, we reveal that CLIP is insensitive to different mask proposals and tends to produce similar predictions for various mask proposals of the same image. This insensitivity results in numerous false positives when classifying mask proposals. This issue mainly relates to the fact that CLIP is trained with image-level supervision.
To alleviate this issue, we propose a simple yet effective method, named Mask-aware Fine-tuning (MAFT). Specifically,  Image-Proposals CLIP Encoder (IP-CLIP Encoder) is proposed to handle arbitrary numbers of image and mask proposals simultaneously. Then, \textit{mask-aware loss} and \textit{self-distillation loss} are designed to fine-tune IP-CLIP Encoder, ensuring CLIP is responsive to different mask proposals while not sacrificing transferability.
In this way, mask-aware representations can be easily learned to make the true positives stand out. Notably, our solution can seamlessly plug into most existing methods without introducing any new parameters during the fine-tuning process. 
We conduct extensive experiments on the popular zero-shot benchmarks. With MAFT, the performance of the state-of-the-art methods is promoted by a large margin: 50.4\% (+ 8.2\%) on COCO, 81.8\% (+ 3.2\%) on Pascal-VOC, and 8.7\% (+4.3\%) on ADE20K in terms of mIoU for unseen classes. 
Code is available at 
\href{https://github.com/jiaosiyu1999/MAFT.git}{github.com/jiaosiyu1999/MAFT.git}.

\end{abstract}



\section{Introduction}
\label{sec:intro}
\input{sections/1-introduction.tex}

\section{Related Work}
\label{sec:related}
\input{sections/2-related_work.tex}

\section{Preliminary}
\label{sec:prelimiary}
\input{sections/3-preliminary}

\section{Methodology}
\label{sec:method}
\input{sections/4-method}

\section{Experiments}
\label{sec:exp}
\input{sections/5-experiments}


\section{Conclusion}
In this paper, we rethink the "frozen CLIP" paradigm in zero-shot segmentation and propose Mask-Aware Fine-Tune (MAFT) for fine-tuning CLIP. 
Firstly, IP-CLIP Encoder is proposed to handle images with any number of mask proposals. Then, $\mathcal{L}_{ma}$ and $\mathcal{L}_{dis}$ are designed for fine-tuning CLIP to be mask-aware without sacrificing its transferability. MAFT is plug-and-play and can be applied to any "frozen CLIP" approach. Extensive experiments well demonstrate the performance of various zero-shot segmentation methods is improved by plugging MAFT.

\textbf{Limitations.}
Our MAFT introduces a CLIP fine-tining framework to the research of zero-shot segmentation. However, the classification ability for novel classes is still limited by pre-trained vision-language models. How to further narrow this limitation is our future research focus.
\input{figs/tex/vis-proposal}
\input{figs/tex/vis-final}

\newpage

\appendix
\section*{Appendix}
\input{sections/Appendix}

\newpage

% {\small
\bibliographystyle{plain}
\bibliography{egbib}
% }



\end{document}