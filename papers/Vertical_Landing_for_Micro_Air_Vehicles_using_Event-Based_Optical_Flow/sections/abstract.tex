\begin{abstract}

Small flying robots can perform landing maneuvers using bio-inspired optical flow by maintaining a constant divergence. However, optical flow is typically estimated from frame sequences recorded by standard miniature cameras. This requires processing full images on-board, limiting the update rate of divergence measurements, and thus the speed of the control loop and the robot. Event-based cameras overcome these limitations by only measuring pixel-level brightness changes at microsecond temporal accuracy, hence providing an efficient mechanism for optical flow estimation. This paper presents, to the best of our knowledge, the first work integrating event-based optical flow estimation into the control loop of a flying robot. We extend an existing 'local plane fitting' algorithm to obtain an improved and more computationally efficient optical flow estimation method, valid for a wide range of optical flow velocities. This method is validated for real event sequences. In addition, a method for estimating the divergence from event-based optical flow is introduced, which accounts for the aperture problem. The developed algorithms are implemented in a constant divergence landing controller on-board of a quadrotor. Experiments show that, using event-based optical flow, accurate divergence estimates can be obtained over a wide range of speeds. This enables the quadrotor to perform very fast landing maneuvers.


%Typically, optical flow is estimated from a monocular frame-based camera, which requires processing full frames on-board and complex algorithms for measuring fast motion. These limitations do not hold for event-based cameras, which only convey pixel-level intensity changes at micro-second timing accuracy, hence providing .
%Event-based cameras, which only measure pixel-level brightness changes, 

% However, event-based cameras may greatly boost the performance of optical flow based control systems through highly accurate brightness change measurement, low latency, efficient visual processing, and high dynamic range. 




%In this work, event-based optical flow measurement is integrated into the visual control loop of a quadrotor drone.


% However, optical flow is estimated from frame sequences recorded by on-board miniature cameras. This requires processing full images on-board, limiting the update rate, and is generally limited to small image displacements. Event-based vision sensors provide an alternative mechanism by only measuring pixel-level brightness changes at micro-second time resolution. Recent work demonstrates that event-based vision is appropriate for efficiently estimating optical flow.


\end{abstract}