\section{Relations between Optical Flow, Ego-Motion, and Visual Observables}
\label{sec:model}

This section defines the optical flow model, which relates the ego-motion of an MAV equipped with a downward facing camera to the perceived optical flow, and the visual observables presented in \cref{sec:related_landing}. These relations form the basis for our evaluation methods applied in subsequent sections, in which ground truth values for optical flow and visual observables are computed.

In the derivation, use is made of three reference frames: $\mathcal{B}$, $\mathcal{C}$, and $\mathcal{W}$, which describe the body, camera and inertial world reference frames respectively. Their definitions are illustrated in \cref{fig:referenceframes}. In each reference frame, a position is denoted through the coordinates $\left (X,Y,Z\right)$, with corresponding velocity components $\left (U,V,W\right)$. The body frame is centered at the center of gravity of the MAV. The rotation from $\mathcal{W}$ to $\mathcal{B}$ is described by standard Euler angles $\varphi$, $\theta$, $\psi$, denoting roll, pitch, and yaw respectively. Similarly, $p$, $q$, and $r$ describe the roll, pitch, and yaw rotational rates.

\begin{figure}[!t]
	\centering
	\includegraphics[width=0.5\linewidth]{images/referenceframes}
	\caption{Definitions of the world ($\cal W$), body ($\cal B$), and camera ($\cal C$) reference frames. The Euler angle definitions and their signs are also shown.}
	\label{fig:referenceframes}
\end{figure}

The camera reference frame $\cal C$ is centered at the focal point of the DVS. The camera is assumed to be located directly below the MAV's center of gravity, with $X_\mathcal{C} = Y_\mathcal{B}$ and $Y_\mathcal{C} = -X_\mathcal{B}$. However, we account for an offset $\Delta Z$ between $Z_\mathcal{C}$ and $Z_\mathcal{B}$, i.e. $Z_\mathcal{C} = Z_\mathcal{B}+\Delta Z$.

The relations between optical flow and ego-motion are based on the pinhole camera model formulation in \citet{Longuet-Higgins1980}. In this formulation, pixel locations $\left(x,y\right)$ in the sensor's pixel grid and optical flow components $\left(u,v\right)$ in pixels per second are represented by their metric, real-world equivalents $\left(\hat x, \hat y\right)$ and $\left(\hat u, \hat v\right)$. The mapping between the two representations is composed of two parts: correction for lens distortion and transformation through the camera's intrinsics matrix. A two-parameter version of the commonly used Brown-Conrady model \cite{Brown1966} is applied to model the relation between $\left(x,y\right)$ and their undistorted equivalent $\left(x_u,y_u\right)$: 

\begin{equation}
\label{eq:distortion}
{\left[ {\begin{array}{*{20}{c}}
		{{x-x_p}}\\{{y-y_p}}
		\end{array}} \right]} = {\left[ {\begin{array}{*{20}{c}}
		x_u-x_p\\y_u-y_p
		\end{array}} \right]}\left( {1 + {k_1}{r_u^2} + {k_2}{r_u^4}} \right)
\end{equation}

The point $\left(x_p,y_p\right)$ in \cref{eq:distortion} represents the camera's principal point, and $r_u=\sqrt{\left(x_u-x_p\right)^2+\left(y_u-y_p\right)^2}$ is the radial distance to the principal point. Second, the undistorted pixels and corresponding optical flow estimates relate to their metric equivalent through scaling with the focal length $f$:
\begin{equation}
\hat x = \frac{{x_u - {x_p}}}{f},\quad \hat y = \frac{{y_u - {y_p}}}{f},\quad \hat u = \frac{u}{f},\quad \hat v = \frac{v}{f}
\end{equation}
In order to obtain the parameters $k_1$, $k_2$, $x_p$, $y_p$, and $f$, the DVS is calibrated using the Camera Calibration Toolbox in MATLAB \cite{Bouguet1999}. Since the DVS does not record absolute intensity, artificial images are generated by recording events from a flashing checkerboard pattern on an LCD screen, similar to \citet{Mueggler2014}.

The optical flow components $\left(\hat{u},\hat{v}\right)$ due to an arbitrary point moving in $\cal C$ with motion $\left(U_{\cal C},V_{\cal C}, W_{\cal C}\right)$ at depth $Z_{\cal C}$ are obtained as follows:

\begin{equation}
\label{eq:optical_flow_equations}
\begin{aligned}
\hat u &=  - \frac{{{U_{\cal C}}}}{{{Z_{\cal C}}}} + \hat x\frac{{{W_{\cal C}}}}{{{Z_{\cal C}}}} - p + r\hat y + q\hat x\hat y - p{{\hat x}^2}\\
\hat v &=  - \frac{{{V_{\cal C}}}}{{{Z_{\cal C}}}} + \hat y\frac{{{W_{\cal C}}}}{{{Z_{\cal C}}}} + q - r\hat x + q{{\hat y}^2} - p\hat x\hat y
\end{aligned}
\end{equation}

When separate measurements of the rotational rates $p$, $q$, and $r$ are available (for instance, from rate gyro measurements), the optical flow in \cref{eq:optical_flow_equations} can be corrected for the ego-rotation of the camera. This derotation leaves only the translational optical flows $\hat u_T$, $\hat v_T$. Further, if all visible points are part of a single plane, their coordinates $Z_\mathcal{C}$ are interrelated. In most indoor applications, floor surfaces can be assumed planar and horizontal. However, MAVs perform fast horizontal maneuvers through rolling and pitching, such that the ground plane may have a slight inclination in $\mathcal{C}$. In this case, $Z_\mathcal{C}$ is expressed through three parameters: the distance $Z_0$ to the plane at $\left(\hat{x},\hat{y}\right)=\left(0,0\right)$, and the plane slopes ${Z_X}$, ${Z_Y}$. 

\begin{equation}
\label{eq:plane}
Z_\mathcal{C} = Z_0 + Z_X X_\mathcal{C} + Z_Y Y_\mathcal{C}
\end{equation}

This can be rewritten into:

\begin{equation}
\label{eq:plane2}
\frac{Z_\mathcal{C}-Z_0}{Z_\mathcal{C}}=Z_X\hat{x}+Z_Y\hat{y}
\end{equation}

When the surface is flat, the slopes are the tangents of the roll and pitch angles, and can therefore be obtained from separate sensors in e.g. an IMU:

\begin{equation}
{Z_X} =  - \tan \varphi ,\quad {Z_Y} = \tan \theta
\end{equation}

Now we define the \emph{scaled velocities} $\vartheta_x$, $\vartheta_y$, $\vartheta_z$ as follows:

\begin{equation}
\label{eq:normalized_velocities}
{\vartheta _x} =  \frac{U_\mathcal{C}}{Z_0},\quad {\vartheta _y} =  \frac{V_\mathcal{C}}{Z_0},\quad {\vartheta _z} =  \frac{W_\mathcal{C}}{Z_0}
\end{equation}

Following the derivation in \citet{DeCroon2013}, substituting \cref{eq:plane2} and \cref{eq:normalized_velocities} into \cref{eq:optical_flow_equations} leads to the following expression:

\begin{equation}
\label{eq:planar_flow_field1}
\begin{aligned}
\hat u_T &= \left( { - {\vartheta _x} + \hat x{\vartheta _z}} \right)\left( {1-Z_X\hat{x}-Z_Y\hat{y}} \right) \\
\hat v_T &= \left( { - {\vartheta _y} + \hat y{\vartheta _z}} \right)\left( {1-Z_X\hat{x}-Z_Y\hat{y}} \right)
\end{aligned}
\end{equation}

The scaled velocities represent the visual observables presented in \cref{sec:related_landing}. $\vartheta_x$ and $\vartheta_y$ are the opposites of the ventral flows, i.e. $\omega_x = -\vartheta_x$, $\omega_y = -\vartheta_y$. The vertical component $\vartheta_z$ is proportional to the flow field divergence, since $D=\nabla\cdot\mathbf{V}=2\vartheta_z$. It is also the inverse of time-to-contact $\tau = Z_{\cal C}/W_{\cal C} = 1/\vartheta_z$.

Note that, in the presence of the vertical camera offset $\Delta Z$, the rotational rates $p$ and $q$ induce additional translational velocity components into $U_{\cal C}$ and $V_{\cal C}$. These propagate to $\vartheta_x$ and $\vartheta_y$, such that:

\begin{equation}
\vartheta_x = \frac{V_{\cal B}}{Z_{\cal C}} - p\frac{\Delta Z}{Z_{\cal C}},\quad \vartheta_y = \frac{U_{\cal B}}{Z_{\cal C}} + q\frac{\Delta Z}{Z_{\cal C}}
\end{equation}

These corrections are accounted for in the computation of ground truth optical flow and values of $\vartheta_x$ and $\vartheta_y$.

%This is accounted for in computing ground truth optical flow (see \cref{sec:setup}). However, for simplicity and efficiency in estimation, this inclination is neglected here, such that $Z_{\cal C} \approx Z_{\cal W}$
%
%The \emph{scaled velocities} $\vartheta_x$, $\vartheta_y$, $\vartheta_z$ are now defined as:
%
%\begin{equation}
%\label{eq:normalized_velocities}
%{\vartheta _x} =  \frac{U_\mathcal{C}}{Z_{\cal C}},\quad {\vartheta _y} =  \frac{V_\mathcal{C}}{Z_{\cal C}},\quad {\vartheta _z} =  \frac{W_\mathcal{C}}{Z_{\cal C}}
%\end{equation}
%
%Then, \cref{eq:optical_flow_equations} reduces to a final expression for a planar optical flow field:
%
%\begin{equation}
%\label{eq:planar_flow_field2}
%\begin{aligned}
%\hat u_T &= { - {\vartheta _x} + \hat x{\vartheta _z}} \\
%\hat v_T &= { - {\vartheta _y} + \hat y{\vartheta _z}}
%\end{aligned}
%\end{equation}
%
%These scaled velocities can now be related to the visual observables presented in \cref{sec:related_landing}. $\vartheta_x$ and $\vartheta_y$ are the opposites of the ventral flows, i.e. $\omega_x = -\vartheta_x$, $\omega_y = -\vartheta_y$. The vertical component $\vartheta_z$ is proportional to the flow field divergence, since $D=\nabla\cdot\mathbf{V}=2\vartheta_z$, and is the inverse of time-to-contact $\tau = Z_{\cal C}/W_{\cal C} = 1/\vartheta_z$.


%
%Let $C_s = {1 - {Z_X} - {Z_Y}}$ denote the slope correction factor obtained from the roll and pitch attitude, and $\hat{u}_c = \hat{u}_T/C_s$, $\hat{v}_c = \hat{v}_T/C_s$ denote the slope-corrected optical flow components. Now \cref{eq:planar_flow_field1} reduces to a final expression for a \emph{planar optical flow field}:
%
%\begin{equation}
%\label{eq:planar_flow_field2}
%\begin{aligned}
%\hat u_c &= { - {\vartheta _x} + \hat x{\vartheta _z}} \\
%\hat v_c &= { - {\vartheta _y} + \hat y{\vartheta _z}}
%\end{aligned}
%\end{equation}


%\subsection{Ground Truth Measurements and Evaluation Method}
%The OptiTrack system provides measurements of position, velocity, and attitude of the quadrotor at a rate of 120 Hz. From these measurements, ground truth measurements of $\vartheta_x$, $\vartheta_y$, and $\vartheta_z$ are computed through normalization of the body velocities by $Z_0$. In addition, for each individual flow vector a ground truth equivalent can be computed using \cref{eq:optical_flow_equations}. For this, the ground plane orientation with respect to the camera is taken into account. Therefore, $Z_{\cal C}$ is expressed as follows:
%
%\begin{equation}
%\label{eq:plane}
%Z_\mathcal{C} = Z_0 + Z_X X_\mathcal{C} + Z_Y Y_\mathcal{C}
%\end{equation}
%
%where the slopes $Z_X$, $Z_Y$ are the tangents of the roll and pitch angles measured from OptiTrack:
%
%\begin{equation}
%{Z_X} =  - \tan \varphi ,\quad {Z_Y} = \tan \theta
%\end{equation}
%
%Hence, a two-step evaluation method is applied to facilitate separate evaluation of optical flow estimation and subsequent flow field parameter estimation.



%\begin{itemize}
%	\item Description of hardware used: MavTec/Lisa M, Odroid, DVS, Cyberzoo/OptiTrack
%	\item Quick summary of implementation and software (cAER, Paparazzi)
%	\item Practical issues, e.g. DVS infrared sensitivity, event rate limiting, etc
%	\item Texture
%\end{itemize}


%\subsection{Evaluation methods}
%\begin{itemize}
%	\item Description of performance assessment method
%	\textbf{\textit{	
%			\item Refer to prelim/dataset for more details? I think this section could easily become too long.
%	}}
%	\item Performance metrics (e.g. optical flow endpoint error, projection magnitude error, computation time)
%	\item Algorithms used for comparison - including the final algorithm. \textit{\textbf{I thought to just mention the others and refer to original papers/prelim.}}
%	\item Dataset
%	\item Ground truth computation
%\end{itemize}
