\section{Conclusion}
\label{sec:conclusion}

In this paper we present a successful implementation of event-based optical flow estimation into a constant divergence landing controller for flying robots. Three main contributions lead to this result.

First, a novel algorithm for computing event-based optical flow is derived from an existing local plane fitting technique. The algorithm is capable of estimating normal optical flow with a wide range of magnitudes through timestamp-based clustering of the event cloud. Its performance is evaluated in ground texture scenes recorded by a DVS. Accurate estimates are seen in real event scenes with sparse, high contrast edges, as well as in scenes with densely packed, lower contrast features. Compared to the existing technique, optical flow accuracy is slightly improved for fast motion, while a larger number of successful optical flow estimates is obtained during slow motion. In addition, it is shown that the optical flow detection rate can be capped to limit computational effort for the algorithm, which enables implementation on low-end platforms without sacrificing accuracy.

Second, we introduce an algorithm for estimating optical flow based visual observables from normal optical flow measurements. By grouping flow vectors by their direction, the aperture problem can be limited by estimating the parameters of a planar optical flow field. The estimator assesses the reliability of its output through a confidence metric based on the flow estimation rate, the variance of optical flow positions, and the coefficient of determination of the flow field. When coupled to the optical flow algorithm, it is capable of estimating the visual observables accurately over a wide range of speeds. Also, the influence of fast rotational motion on the visual observables is adequately corrected through separate rotational rate measurements.

Third, using the developed pipeline, fast constant divergence landing maneuvers are demonstrated using a quadrotor equipped with a downward facing DVS. Decent tracking performance is achieved for the majority of the descent using a simple proportional controller. The final touchdown of the landing maneuver is not yet performed due to self-induced oscillations close to the ground. However, stability-based control methods have already been demonstrated that can resolve this issue. A future controller based on these methods can, for example, autonomously detect the oscillations and switch to a final touchdown phase based on constant thrust, or perform a complete landing maneuver using an adaptive gains. In addition, our controller does not yet incorporate the visual observables for horizontal stabilization, but relies on an external position tracking system. However, with the estimate accuracy and rotational motion correction presented in this work, this appears feasible.

In a first-order comparison to recent work on landing using frame-based cameras for estimating optical flow, the presented event-based pipeline demonstrates more accurate measurements at high speed and a higher sampling rate, which enable faster maneuvers than previously shown in literature. However, for a more solid conclusion regarding real-time computational benefits of event-based vision, a comparison should be performed where both frame-based and event-based cameras are incorporated in the same hardware configuration. 

While miniature frame-based bottom cameras are readily embedded into several commercially available quadrotors, the on-board hardware configuration in this work is still relatively bulky and inefficient. The implementation in this work is performed on a relatively large MAV in order to carry the weight of the DVS with separate computers for processing events and estimation of visual observables. However, the availability of smaller and lighter event-based cameras, such as the 2.2 mg meDVS, will enable high-speed optical flow control on very small flying robots. % the full potential of the sensor's low latency


%Main conclusions:
%
%\begin{itemize}
%	\item Successful development of an efficient and accurate algorithm for computing event-based optical flow.
%	\item Flow throughput rate control makes algorithm more scalable for use with less powerful hardware.
%	\item Fully developed visual pipeline, which accounts for camera optics and rotational motion.
%	\item Clear correlations between ground truth and estimates, mainly at high speed. At low speeds, the current approach is a bit noisy, generating peaks when $\rho_F$ is limited.
%	\item To the author's knowledge:
%	\begin{itemize}
%		\item the first incorporation of divergence estimation and event-based vision.
%		\item the first presented event-based visual controller for MAVs.
%	\end{itemize}
%	\item Comparison to frame-based divergence landings is still inconclusive and requires further testing, in particular higher speeds. 
%\end{itemize}
%
%Recommendations at this point:
%
%\begin{itemize}
%	\item More intelligent separation/clustering of events into features for estimating optical flow.
%	\item Embedded implementation of DVS for minimal latency
%	\item Parallel/GPU implementation for even faster processing
%	\item All sensors and systems running on a shared clock for optimal sensor synchronization.
%	\item Extension of this work to stability-based height estimation and control
%\end{itemize}