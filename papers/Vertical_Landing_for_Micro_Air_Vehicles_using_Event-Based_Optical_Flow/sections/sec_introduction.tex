\section{Introduction}
% The very first letter is a 2 line initial drop letter followed
% by the rest of the first word in caps.
% 
% form to use if the first word consists of a single letter:
% \IEEEPARstart{A}{demo} file is ....
% 
% form to use if you need the single drop letter followed by
% normal text (unknown if ever used by the IEEE):
% \IEEEPARstart{A}{}demo file is ....
% 
% Some journals put the first two words in caps:
% \IEEEPARstart{T}{his demo} file is ....
% 
% Here we have the typical use of a "T" for an initial drop letter
% and "HIS" in caps to complete the first word.
%\IEEEPARstart{M}{icro} Air Vehicles (MAVs) are currently subject to extensive research and development. Their size and maneuverability make them excellent for exploration or surveillance in confined areas, especially where it is difficult or dangerous for people to operate. In such situations, MAVs need to be capable of autonomous navigation, such that safe flight is guaranteed if communication with an operator is not possible. In GPS-denied environments where prior knowledge regarding obstacles is unavailable, this is a challenging requirement.
%
%On-board monocular visual sensors have great potential to overcome this challenge \cite{Floreano2015}. While these sensors are lightweight, they provide rich information about ego-motion and the environment. In monocular vision, such information is available through bio-inspired \emph{optical flow}. In navigation strategies observed in biology, optical flow cues were identified as primary sensing input \cite{Lee1993,Baird2013}. This led to the development of optical flow driven control systems for tasks such as terrain following \cite{Expert2015}, landing \cite{DeCroon2016}, and obstacle avoidance \cite{Zufferey2006}. \note{mainly this needs some more work to make optical flow landing relevant and introduce divergence etc}

%\IEEEPARstart{R}{apid} advances in micro-electronics catalyzed the development of small flying robots \cite{Floreano2015}, formally referred to as Micro Air Vehicles (MAVs). These vehicles are becoming increasingly autonomous and already demonstrated excellent exploration capabilities in outdoor scenarios. Indoor environments are, however, more challenging due to limited GPS availability and cluttered environments. In such situations, visual sensors are a common solution due to their rich information output and their low weight and power requirements.

Rapid advances in micro-electronics catalyzed the development of tiny flying robots \cite{Floreano2015}, formally referred to as Micro Air Vehicles (MAVs). Due to their size and agility, MAVs have the potential to perform activities in confined and cluttered environments. However, achieving autonomous flight with very small MAVs (for example, the 20-gram DelFly Explorer \cite{DeWagter2014}) is a significant challenge due to strict weight and power limitations for on-board equipment. The speed of cutting edge autonomous MAV navigation pales in comparison to their natural counterparts, the insect.
%For overcoming this challenge, visual navigation techniques observed in flying insects are highly interesting.

%Since the birth of the smart phone, the reduction in the cost of advanced computing and sensors has fueled a rapid expansion in the development of Micro Air Vehicles (MAVs). Many of these have however been developed with a heavy dependency on the Global Positioning System (GPS) severely limiting their use in indoor environments. The few platforms capable of operating in GPS denied environments do so at a fraction of the speed and efficacy of animals seen in nature. Maybe we can look to nature to try to improve the performance of small and light weight MAVs to enable them to perform high speed flight in real world environments.

%Visual sensors enable motion perception through optical flow, the apparent motion of brightness patterns perceived by an observer due to relative motion with respect to the environment \cite{Gibson1979}. In biological vision systems, optical flow provides vital information for navigation, which translates well to robotic applications. Inspired by vision-based landing strategies observed from flying insects \cite{Chahl2004,Baird2013}, researchers have developed several optical flow based controllers for basic navigation tasks of MAVs \cite{Ruffier2005,Expert2015,Herisse2012,DeCroon2016}. Of particular interest for MAVs with hovering capability are vertical control strategies based on the \emph{divergence} of the perceived optical flow field, which is proportional to vertical velocity scaled by height \cite{McCarthy2008}. By keeping divergence constant, an observer approaches the ground while exponentially decreasing their downward speed \cite{Ho2016}. 

The main sensor system of most insects uses some form of visual light to perceive the environment around them. Visual navigation in flying insects is primarily based on optical flow, the apparent motion of brightness patterns perceived by an observer due to relative motion with respect to the environment \cite{Gibson1979}. In essence, optical flow provides information on the ratio of velocity to distance, such that the actual metric distance to the environment is not directly available. Instead, flying insects navigate based on certain visual observables extracted from the optical flow field that relate to ego-motion. Honeybees were seen to control their descent during landings based on the \emph{divergence} of the optical flow field perceived from the ground \cite{Baird2013}. When looking down, flow field divergence conveys the vertical velocity scaled by the height. By maintaining a constant divergence in downward motion, an observer approaches the ground while exponentially decreasing its downward speed. For flying robots capable of vertical landing, this is an interesting strategy. This application has been explored in several experiments with rotorcraft MAVs \cite{Herisse2008,Herisse2012,Ho2016,DeCroon2016}.

While such optical flow based navigation strategies are bio-inspired, most visual sensors employed for measuring optical flow differ significantly from their natural counterparts. Commonly used miniature cameras operate in a \emph{frame-based} manner: full frames are obtained by periodically measuring brightness at all pixels. This is a relatively inefficient process for motion perception, since the information output rate is independent of the actual dynamics in the scene. Static parts of a frame are recorded as well as changing parts, even though only the latter are relevant for motion perception. Therefore, follow-up processing of a full frame is necessary, which at present still requires significant processing. In addition, fast inter-frame displacements lead to motion blur, which limits the accuracy of resulting optical flow estimates or requires complex algorithms to account for this. These characteristics are undesirable for optical flow measurement on board miniature flying robots, which are subject to strict computational limits and exhibit fast dynamics. 

In contrast, biological vision systems, such as the compound eyes of insects, employ an \emph{event-driven} mechanism; they measure changes in the perceived scene at the moment of detection \cite{Linares-barranco2014}. Several researchers have attempted to minic the sensory system in insects in order to measure optical flow. For example, in \citet{Ruffier2014} a tethered rotorcraft MAV was equipped with a 2-photodetector neuromorphic chip for measuring translational optical flow. In \citet{Floreano2013} a miniature curved compound eye design, inspired by the fruit fly Drosophila, was presented.

In particular, \emph{event-based cameras} are a promising class of sensors for optical flow perception. When a pixel of an event-based camera measures a relative increase or decrease in brightness, it registers an event, mapping its pixel location to the timestamp and sign of the change. This timestamp is obtained with microsecond resolution and latencies in the range of 3 to 15 $\upmu$s. In addition, event-based pixel architectures enable intrascene dynamic ranges exceeding 120 dB \cite{Yang2015}. These characteristics are highly desirable in robotic visual navigation. Experiments using event-based cameras demonstrated high performance of visual control systems through low-latency state updates, efficient data processing, and operation over a wide range of illumination conditions \cite{Conradt2009,Delbruck2013}. 

%To perform landing, one of the visual observables is the \emph{divergence} of the optical flow field, which is proportional to vertical velocity scaled by height \cite{McCarthy2008}. It is also the inverse of time-to-contact to a target. By keeping divergence constant, an observer approaches the ground while exponentially decreasing its downward speed.
%
%Optical flow based visual observables are well applicable for robots equipped with a visual sensor \cite{Ruffier2005,Expert2015,Herisse2012,DeCroon2016}. Of particular interest for MAVs with hovering capability are vertical control strategies based on the  perceived from the ground, . By keeping divergence constant, an observer approaches the ground while exponentially decreasing its downward speed \cite{Ho2016}.
%
%Insect based navigation -> optical flow
%optical flow is ratio of velocity and distance
%visual observables derived from optical flow field
%
%divergence definition
%divergence based landing in robots

%This inspired researchers to develop simple controllers for MAVs based on these visual observables . 


%Until now, estimation of optical flow field divergence has mainly been performed using \emph{frame-based} cameras. In such sensors, full frames are obtained by periodically recording brightness at all pixels. However, this limits the application of such sensors to low-speed maneuvers. An inevitable component of any frame-based optical flow estimation technique is processing full frames, since it is not known in advance which parts of the image contain motion. In a typical recent robotic sensing pipeline, this role is provided by a corner detector (for example, \cite{Rosten2010}), providing regions of interest for follow-op processing by a Lucas-Kanade tracker \cite{Lucas1981}. \todo{a better way to convey this argument is needed.} Even as such, limited sampling rates are obtained, in the order of 15 to 25 Hz \cite{Ho2016,Grabe2015}. A second limitation is that, at high speed, inter-frame displacements are large, violating the local assumption of the Lucas-Kanade technique and requiring more advanced algorithms that typically run for several seconds per frame \cite{Baker2011}. OR: For the Lucas-Kanade tracker, a second limitation is its limited pixel neighborhood; if the image motion is too large, the optical flow estimates become inaccurate.\todo{briefly address insect-inspired sensors here as well}

%Until now, estimation of optical flow field divergence has mainly been performed using \emph{frame-based} cameras. In such sensors, full frames are obtained by periodically measuring brightness at all pixels. However, this approach has several limitations. First, an inevitable component of any frame-based optical flow estimation technique is processing full frames, since it is not known in advance which parts of the image contain motion. Second, high-speed motion 
%
%Alternatively, optical mouse sensors \cite{Zufferey2010} or specialized insect-inspired neuromorphic sensors have been applied \cite{Ruffier2005,Expert2015}, which achieve higher sampling rates. However, such sensors typically measure translational flow, requiring combinations of such sensors for measuring divergence \cite{Zufferey2007}. 


%In real-time optical flow based navigation, sampling rates around 25 Hz and latencies in the order of 0.1 s are obtained \cite{Ho2016,Grabe2015}. As a result, control system stability is a limiting factor in employing the fast dynamics of MAVs in optical flow based navigation. With higher rate sensors such as motion tracking systems, this limitation is insignificant, enabling MAVs to perform aggressive maneuvers \cite{Mellinger2014}.

%\emph{Event-based} visual sensors offer a fundamentally different approach that is much less subject to these limitations. Inspired by activity driven motion sensing mechanisms in biological vision \cite{Delbruck2010}, event-based pixels respond independently and asynchronously to changes in brightness. When a pixel measures a relative increase or decrease in brightness, it registers an event, mapping its pixel location to the timestamp and sign of the change. This timestamp is obtained with \emph{micro-second} resolution and latencies in the range of 3 to 15 $\upmu$s \cite{Lichtsteiner2008,Posch2011,Brandli2014}. In addition, event-based pixel architectures enable intrascene dynamic ranges exceeding 120 dB. These characteristics are highly desirable in robotic visual navigation. Experiments using event-based cameras demonstrated high performance of visual control systems through low-latency state updates, efficient data processing, and operation over a wide range of illumination conditions \cite{Conradt2009,Delbruck2013}. 

This novel approach to visual sensing is in general incompatible with state-of-the-art computer vision algorithms for estimating optical flow, due to the lack of absolute brightness measurements. Therefore, several event-based methods for optical flow estimation \cite{Benosman2012,Benosman2014, Barranco2014, Brosch2015, Bardow2016} as well as benchmarking datasets \cite{Barranco2016, Ruckauer2016} have been developed. Of the existing techniques, the local plane fitting approach by \citet{Benosman2014} is the most promising based on its application to estimating time-to-contact (the reciprocal of flow field divergence) in simple scenes \cite{Clady2014} and good results in \citet{Ruckauer2016}. However, until now, no study has discussed an implementation of event-based optical flow into an optical flow based control loop of an MAV. 
%To maximize the benefits of event-based cameras, new event-driven algorithms are rapidly being developed. In several research topics related to visual navigation, including visual Simultaneous Localization And Mapping (SLAM) and optical flow estimation, significant work has recently been performed \cite{Cho2015}. A solid basis for integrating event-based vision into navigation for MAVs is therefore available. In this work we aim to perform this integration by design, implementation, and experimental evaluation of a novel event-based optical flow sensing and control algorithm. \note{this last part requires more attention}

This paper contains \emph{three main contributions}. First, a novel method for estimating event-based optical flow inspired by \citet{Benosman2014} is introduced. Its applicability is extended to a wider range of velocities, while improving computational efficiency. Second, a method for incorporating event-based optical flow into visual estimation of divergence is proposed, which accounts for the aperture problem that occurs in most existing event-based optical flow methods. Third, the proposed algorithms for event-based optical flow divergence estimation are incorporated in a constant divergence landing controller on-board of a quadrotor. To the best of the authors' knowledge, this paper presents the first functional event-based visual controller on-board of an MAV.
 
We begin by discussing related work concerning landing using optical flow, event-based vision, and optical flow estimation in \cref{sec:related}. Then, \cref{sec:model} introduces the mathematical models describing the relations between the ego-motion of the MAV, optical flow, and visual observables used in this work. In \cref{sec:eof_estimation} the estimation method for event-based optical flow is described and evaluated. Subsequently, \cref{sec:landing_eof} introduces the approach to estimating visual observables from event-based optical flow, followed by an assessment of its performance in combination with the optical flow method. Afterwards, \cref{sec:results} presents flight test results of the full estimation pipeline during constant divergence landing maneuvers. Lastly, the main conclusions and recommendations for future work are presented in \cref{sec:conclusion}.

%\begin{itemize}
%	\item Introduction/relevance of subject
%	\item Review of state-of-the-art on optical flow based maneuvers, event-based vision, and event-based optical flow.
%	\textbf{\textit{I thought it best to do this here briefly, and do a more specific review in further sections where relevant.}}
%	\item Scientific gap
%	\item Contributions of this work
%	\item Paper outline
%\end{itemize}
