\section{Related Work}
\label{sec:related}
%\begin{itemize}
%	\item The concept of event-based vision
%	\item Description of DVS and the AER protocol
%	\item Related sensors, e.g. ATIS, DAVIS, meDVS.
%\end{itemize}

This section introduces the fundamental concepts and previous contributions relevant for this work. First, \cref{sec:related_landing} discusses bio-inspired landing strategies involving optical flow and associated research involving MAVs. Second, the concept of event-based cameras is described in \cref{sec:related_event_cams}. Third, an overview of existing approaches to optical flow estimation is provided in \cref{sec:related_optical_flow}, including both frame-based camera applications and recently developed event-based techniques.

\subsection{Landing Using Optical Flow} 
\label{sec:related_landing}
Although optical flow does not by itself provide metric scale to motion, information from optical flow fields is useful for several navigation tasks, including landing. Simple bio-inspired strategies were proposed in the past decades that utilize the visual observables in the optical flow field perceived from the ground. Such strategies form a lightweight alternative for visual estimation of three-dimensional structure, ego-motion, and relative pose, which can be performed through visual Simultaneous Localization And Mapping (SLAM) \cite{Davison2007a} or visual odometry \cite{Nister2004}. These techniques have become increasingly efficient over the last few years \cite{Forster2014,Engel2014}, yet still require processing and maintaining large amounts of measurement data. This strongly contrasts with optical flow based techniques, in which all information required for navigation is contained in a small number of visual observables.

The visual observables related to horizontal motion above ground are the ventral flows $\omega_x$, $\omega_y$, referring to the average flows along the $x$ and $y$ image axes. In several experiments with a tethered MAV \cite{Ruffier2014,Expert2015}, the authors mimicked navigation strategies seen in insects, which have been observed to follow terrain and land using ventral flow \cite{Srinivasan1996}. By maintaining a constant ratio between forward motion and height and at the same time slowing down, they perform smooth landings. Ventral flows may also be used for hover stabilization to augment visual vertical control \cite{Alkowatly2015}.

In recent aerial robotic applications, mainly visual observables based on vertical motion were applied, allowing control of vertical dynamics independent of horizontal motion. One of these observables is flow field divergence $D$, i.e. the ratio of vertical velocity to height. Its reciprocal is the time-to-contact to the ground $\tau$. Similar to ventral flows, divergence was seen to guide docking and landing motion in biology. In \cite{Baird2013} honeybees were seen to keep $D$ constant. Hence, velocity is decreased exponentially, ensuring a smooth touchdown. Other strategies for vertical landing exist based on $\tau$, such as the constantly decreasing $\tau$ strategy observed in braking human drivers \cite{Lee1976}. This strategy provides more control over the landing trajectory \cite{Alkowatly2015}, though it involves more parameters.

In practical control systems, a constant divergence approach suffers from instability as height decreases, due to self-induced oscillations. In \citet{DeCroon2016} it was shown that a relation exists between the employed divergence controller gain and the height at which oscillations occur. A main insight then was that a drone could detect its own oscillations, and in this way determine its height. This strategy can be employed to trigger a separate final touchdown phase, or to continuously measure height by landing at a near-unstable control gain. The finding in \citet{DeCroon2016} also contains an important key to high-performance optical flow divergence landings. This was used in \citet{Ho2016a} to develop an adaptive gain controller, which detects the height at the start of a landing maneuver, sets initial controller gains based on the height, and lands while exponentially reducing the gains. Although the landings performed were quite fast compared to landings in the literature ($D=0.3$ compared to a typical $D=0.05$ \cite{Herisse2012}), the speed of the landings in \citet{Ho2016a} are still quite limited by the standard cameras available on the used AR drone 2.0.

\subsection{Event-Based Cameras}
\label{sec:related_event_cams}
Inspired by the workings of biological retinas, event-based cameras rely on a sensing mechanism that fundamentally differs from their frame-based counterparts. In frame-based cameras the pixel values are measured at fixed time intervals to produce a sequence of images. In event-based cameras, on the other hand, pixel activity is driven by light intensity changes. Whenever a pixel measures a local change, it produces a \emph{event}. Specifically, this occurs when the pixel's logarithmic intensity measurement $I(x,y,t)$ (at pixel location $(x,y)$ and timestamp $t$) increases or decreases beyond a threshold $C$:

\begin{equation}
\label{eq:event_threshold}
\lvert\Delta\left(\log I\left(x,y,t\right)\right)\rvert > C
\end{equation}

Events are encoded according to an Address-Event Representation (AER) \cite{Lichtsteiner2008}, which consists of event information encoded by an address and the timestamp of detection. Typically, an event encodes the pixel position $(x,y)$, the timestamp $t$, and the polarity $P\in\lbrace-1,1\rbrace$, which indicates the sign of the intensity change. A visualization of a basic stream of events, in comparison to an equivalent set of frames, is shown in \cref{fig:events_frames}.

\begin{figure}[!htpb]
	\centering
	\includegraphics[width=.65\linewidth]{images/events_frames.png}
	\caption{Frame-based and event-based visual output generated from a simple synthetic scene, in which a black horizontal bar moves upward. The events are visualized as points in space-time, hence showing the trajectory of the leading and trailing edges of the black bar. Events with positive polarity are highlighted in green; those with negative polarity are marked in red.}
	\label{fig:events_frames}
\end{figure}

The sensor used in this work is the Dynamic Vision Sensor (DVS) - specifically, the DVS128 - which is the first commercially available event-based camera \cite{Cho2015}. It features a 128x128 pixel grid operating at an intrascene dynamic range of 120 dB, measuring events at 1 $\upmu$s timing resolution with a latency of 15 $\upmu$s \cite{Lichtsteiner2008}. A picture of the DVS is shown in \cref{fig:dvs}. Since the availability of the DVS, other event-based cameras have been developed. Most notable are the Asynchronous Time-based Image Sensor (ATIS) \cite{Posch2011}, which measures absolute intensity as well as polarity for each event, and the Dynamic and Active pixel Vision Sensor (DAVIS) \cite{Brandli2014}, whose pixels record events as well as full frames. Interesting in the context of this work is the 2.2 gram micro embedded DVS (meDVS) \cite{Conradt2015}, which is highly suitable for on-board MAV applications.

\begin{figure}[!t]
	\centering
	\includegraphics[width=0.3\linewidth]{images/dvs.png}
	\caption{Picture of the event-based camera employed in this work, the DVS.}
	\label{fig:dvs}
\end{figure}

Event-based cameras have several interesting applications for robotic navigation. Initial work has been performed on visual SLAM with event-based cameras \cite{Weikersdorfer2013,Weikersdorfer2014}. In \citet{Mueggler2014} a pose estimation algorithm based on line tracking is applied to a quadrotor, enabling it to perform aggressive maneuvers . Some studies demonstrate the ability to simultaneously reconstruct intensity maps and relative pose \cite{Kim2014} and, more recently, three-dimensional structure \cite{Kim2016}. Others aim at combining the benefits of event-based and frame-based vision using the DAVIS. For example, the method presented in \citet{Kueng2016} uses frames to identify visual features and events to track their position in high-speed motion, in order to perform visual odometry.

\subsection{Optical Flow Estimation}
\label{sec:related_optical_flow}
In the following, we discuss available techniques for estimating optical flow. Many recent visual navigation experiments, in particular those with commercially available quadrotors, employ standard frame-based cameras, in combination with follow-up processing algorithms. Others employ off-the-shelf optical mouse sensors or specialized neuromorphic optical flow sensors, which directly yield translational optical flow output. For event-based cameras, several techniques have recently been developed, yet these have seen limited applications in robotic navigation.

\subsubsection{Estimation from Frame Sequences}
At present, a wide range of optical flow estimation techniques is available for frame-based cameras. Most of these algorithms derive from the brightness constancy assumption, which states that, when a pixel flows from one frame to another, its intensity $I$ is conserved \cite{Baker2011}. This assumption leads to the well-known optical flow constraint:

\begin{equation}
\label{eq:optical_flow_constraint}
I_x(x,y) u + I_y(x,y) v = -I_t(x,y)
\end{equation}

where $x$ and $y$ are the pixel position and $u$ and $v$ denote the unknown optical flow components in pixels per second. The partial derivatives $I_x$, $I_y$, and $I_t$ are obtained from two sequential frames. Since this equation provides two unknown components, a second constraint is necessary to obtain optical flow. Many recent methods aim at providing a dense optical flow field estimate, where optical flow is estimated for any pixel for the frame. In this case, a global cost function minimization is performed, in which a second constraint is provided by prior knowledge. An example of such a constraint is the requirement of smoothness in the flow field in the well-known Horn-Schunck technique \cite{Horn1981}. Recent dense optical flow algorithms provide accurate results for complex scenes, but at the cost of high computation times \cite{Baker2011}.

In recent real-time robotic applications, the most popular frame-based algorithm is the Lucas-Kanade algorithm \cite{Lucas1981}. This algorithm is originally developed for estimating optical flow in the local neighborhood of a pixel. In order to solve \eqref{eq:optical_flow_constraint}, the second assumption is that $u$ and $v$ are constant across neighboring pixels. Therefore, a least-squares system can be composed based on \eqref{eq:optical_flow_constraint}, using $I_x$, $I_y$, and $I_t$ from neighboring pixels. This system can be solved for $u$ and $v$. This technique is mainly applied to sparse estimation, where motion is only computed locally at visual features of interest.

Local optical flow estimation techniques are subjected to the aperture problem, which occurs when motion ambiguity is present due to a limited field of view \cite{Beauchemin1995}. This occurs along object contours which lack clearly distinguishable corner points. The result is that only \emph{normal flow} can be estimated, which is the motion component normal to the contour's orientation. At corner locations, this ambiguity is not present. Only at these points, optical flow is estimated using Lucas-Kanade. Therefore, a corner detection algorithm (e.g. \cite{Rosten2008}) can be applied to first obtain points of interest in the frame. This strategy is applied in many recent optical flow based landing experiments \citet{DeCroon2013, Alkowatly2015, Ho2016, DeCroon2016}.

Alternatively, in \citet{Herisse2012} a 'pyramidal' variant of Lucas-Kanade is applied \cite{Bouguet2000} to account for large displacements. This is a coarse-to-fine approach: optical flow is first computed for a highly downsampled frame. Then, the frame is iteratively refined, computing more detailed optical flow at each refinement level, using the estimate at the previous level to initialize the estimate.

In all these approaches, it is necessary to process full frames, either to find features of interest such as corners, or to obtain sufficiently detailed dense optical flow. While it is possible to use low resolution frames for faster processing, this comes at the cost of reduced detail and hence lower accuracy. Event-based cameras are much less subject to this trade-off, since their output directly highlights locations of interest for estimating optical flow.

\subsubsection{Optical Flow Sensors}
Hardware-based solutions for estimating optical flow have also been applied. Some researchers employ off-the-shelf optical mouse sensors for measuring translational optical flow e.g. \cite{Zufferey2010}. In addition, the visual motion processing in insects inspired researchers to develop highly simplified optical flow sensors, such as the 2-photodetector elementary motion detector was developed for the tethered MAV research in \cite{Ruffier2005,Ruffier2014}. 
Optical flow sensors achieve relatively high sampling rates due to their simplicity. However, their operating principle is generally limited to measuring translational flow. For measuring patterns of optical flow, such as divergence, multiple separate sensors need to be applied and integrated.

\subsubsection{Event-Based Methods}
Since the introduction of the DVS and subsequently developed sensors, several different approaches to event-based optical flow estimation have been developed. Most of these techniques operate on each newly detected event and its spatiotemporal neighborhood, providing sparse optical flow estimates. However, in most cases, the algorithms do not distinguish between corner points and other visual features. Thus, they primarily estimate normal flow. In the following, a brief review of recent approaches is presented.

An adaptation of the frame-based Lucas-Kanade tracker is introduced in \citet{Benosman2012}. Similar to the original algorithm, it solves the optical flow constraint by including the local neighborhood of a pixel. Since absolute measurements of $I$ are not available, the authors replaced the intensity $I$ by the sum of event polarities at a pixel location, obtained over a fixed time window. The reconstructed 'relative intensity' is used to numerically estimate $I_x$, $I_y$, and $I_t$. However, the number of events is generally too low for this approach to provide accurate gradient estimates, in particular for the temporal gradient $I_t$. 

In \citet{Benosman2014} an algorithm is presented that operates on the spatiotemporal representation of events as a point cloud (as shown in \cref{fig:events_frames}). When representing a sequence of events by three-dimensional points of $(x,y,t)$, they form surface-like structures. The gradient of such a surface relates to the motion of the object that triggered the events. By computing a local tangent plane to an event and its neighbor events, normal flow for that event is estimated. A follow-up study employs this algorithm for detecting and tracking corners from neighboring normal flow vectors, hence obtaining fully observable optical flow \cite{Clady2015}. However, real-time results are not yet demonstrated with a non-parallelized implementation.
%\cite{Tschechne2014} investigated an alternative approach based on 
%
%Very recently, a GPU-based technique for simultaneous estimation of optical flow and absolute intensity
%
%However, in particular \cite{Benosman2014}

In \cite{Barranco2014} a technique is introduced that estimates optical flow on object contours, based on both events and absolute intensity measurements. Input events are used to locate motion boundaries on contours. Along each boundary, motion is estimated using the width of the contour, which is computed from the local event distribution and the absolute intensity. The latter can be reconstructed from events, but having separate intensity measurements (e.g. from a DAVIS or ATIS sensor) simplifies the process.

A bio-inspired approach is proposed in \citet{Brosch2015}. In this approach, optical flow is estimated using direction- and speed-selective filters based on the first stages of visual processing in humans. A bank of spatiotemporal filters is employed, each of which is maximally selective for a certain direction and speed of optical flow. For each new event, the neighboring event cloud is convolved with the filters to obtain a confidence measure for each filter. Optical flow for that event is then obtained from the sum of the confidence measures weighted by direction.

More complex event-based algorithms have also been developed, which have not demonstrated real-time performance, but show promising results. In \citet{Barranco2015} a phase-based optical flow method is discussed, which is developed for high-frequency textures. The algorithm is compared to other event-based methods \cite{Benosman2012,Benosman2014,Barranco2014}, indeed showing significant accuracy improvements. Also, an approach was presented for simultaneous estimation of dense optical flow and absolute intensity \cite{Bardow2016}. This is the only available approach aimed towards dense optical flow estimation. Visual results of this method are encouraging, yet a quantitative evaluation is not performed.

Recently, several datasets for event-based visual navigation have been published. The set in \citet{Barranco2016} provides both frame and event measurements from a DAVIS sensor accompanied by odometry measurements. This facilitates comparison between frame-based and event-based techniques for optical flow estimation or visual odometry. However, to the best of the authors' knowledge, an actual comparison of existing techniques has not yet been published for this set. In this respect, the work in \citet{Ruckauer2016} is more relevant for this work, as it features both an event-based dataset and a comparison of various optical flow algorithms. These are variants of the techniques in \cite{Benosman2012} and \cite{Benosman2014}, as well as a basic direction selective algorithm.

We select the local plane fitting algorithm in \citet{Benosman2014} as the basis of the approach in our work. It has shown the most promising results in \cite{Ruckauer2016} and has recently been incorporated into follow-up experiments \cite{Clady2014,Clady2015}. In addition, its implementations yielded real-time operation for high event measurement rates.

%\subsection{Estimation of Visual Observables}
%Optical flow
%The second class of methods is based on the structure of the observed optical flow field, such as the planar flow field structure in \eqref{eq:planar_flow_field1}. In this case, it is possible to simultaneously estimate the flow field parameters $\vartheta_x$, $\vartheta_y$, and $\vartheta_z$, for example through least-squares estimation. This technique is seen in recent vertical landing approaches \cite{DeCroon2013,Alkowatly2015,DeCroon2015}, and forms the basis of our approach.

%Limited work has been performed towards obtaining visual observables from event-based optical flow. In \citet{Clady2014} an event-based method for estimating time-to-contact $\tau$ is described. However, the this approach is limited to motion towards a point which lies within the camera's field of view. Hence, it is not robust to fast rotational motion and translational motion, which are inevitable in on-board applications in MAVs.
