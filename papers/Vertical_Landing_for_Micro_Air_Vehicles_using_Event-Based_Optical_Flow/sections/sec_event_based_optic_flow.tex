\section{Event-Based Optical Flow Estimation}
\label{sec:eof_estimation}

This section describes our optical flow estimation approach. Since it is based on the work in \citet{Benosman2014}, this baseline approach is explained first in \cref{sec:eof_plane_fitting}. Then, the proposed modifications for achieving higher efficiency (\cref{sec:eof_efficiency_improvements}) and timestamp-based selection (\cref{sec:eof_timesorting}) are discussed. In \cref{sec:results_optical_flow} the result of our improvements is evaluated in comparison to the baseline algorithm.

\subsection{The Baseline Plane Fitting Algorithm}
\label{sec:eof_plane_fitting}
The main working principle of the baseline algorithm is based on the space-time representation of events as a point cloud. In the following, an event is denoted as a space-time point according to $\mathbf{e}_n=\left(x,y,t\right)$, where $x$ and $y$ represent the undistorted pixel locations. Note that the polarity $P$ is not considered; we group positive and negative polarity events and process them separately.

Let $\Sigma_e (x,y) = t$ be a mapping describing the surface along which events are positioned. The shape of $\Sigma_e$ is a result of the feature geometry and, in particular, its motion. In the case of a locally linear feature (such as an edge) and constant motion, this surface reduces to a plane. This is clearly visible in the example scene in \cref{fig:events_frames}. With these assumptions, $\Sigma_e$ can be approximated by a tangent plane within a limited range of $x$, $y$, and $t$.

For each newly detected event $\mathbf{e}_n$, a plane $\mathbf{\Pi}$ is computed that fits best to all neighboring events $\mathbf{e}_i$ for which $x_i \in \left[x_n-\frac{1}{2}\Delta x,\, x_n+\frac{1}{2}\Delta x\right]$, $y_i \in \left[y_n-\frac{1}{2}\Delta y,\, y_n+\frac{1}{2}\Delta y\right]$, and $t_i \in \left[t_n-\Delta t,\, t_n\right]$, where $\Delta x, \Delta y, \Delta t$ indicate spatial and temporal windows. The spatial windows are generally small and are both set to 5 pixels. The temporal window setting has a large influence on the detectable speed and interference of multiple features, which is discussed further in \cref{sec:eof_timesorting}. 

The plane $\mathbf{\Pi} = \left[p_x,\, p_y,\, p_t,\, p_0\right]^T$ is computed through an iterative process of linear least squares regression and outlier rejection. It is represented in homogeneous coordinates, such that the following hold for any event $\mathbf{e}_i$ that intersects with $\mathbf{\Pi}$:

\begin{equation}
\label{eq:plane_system}
p_x x_i + p_y y_i + p_t t_i + p_0 = 0
\end{equation}

Extending \cref{eq:plane_system} with at least four neighboring events, an overdetermined system of equations is obtained, which is solved through linear least-squares. After an initial fit, the Euclidean distance of each event to the plane is computed. All events for which the distance exceeds a threshold $d_{max}$, are rejected from this fit. Using the remaining events, a new least-squares plane fit is computed. In \citet{Benosman2014}, this process is repeated until the change in $\mathbf{\Pi}$ is no longer significant. This is the case if the norm of the change in all components in $\mathbf{\Pi}$ is smaller than a second threshold $k_d$, i.e. $\left\| {\mathbf{\Pi} (i) - \mathbf{\Pi} (i - 1)} \right\| < k_d$. In practice, the latter often occurs already after one or two iterations. In this work, the values for $d_{max}$ and $k_d$ specified in \citet{Ruckauer2016} are applied, which are 0.01 and 1e-5 respectively.

The final plane is preserved and used to compute the local gradients of $\Sigma_e$:

\begin{equation}
\nabla {\Sigma _e}\left( {x,y} \right) = {\left[ {\frac{{\partial {\Sigma _e}}}{{\partial x}},\,\frac{{\partial {\Sigma _e}}}{{\partial y}}} \right]^T} = \left[-\frac{p_x}{p_t},\, -\frac{p_y}{p_t} \right]^T
\end{equation}

In \citet{Benosman2014} the gradient components of $\Sigma_e$ are assumed to be inversely related to the optical flow components $(u,v)$:
\begin{equation}
\label{eq:eof_slopes_1}
\nabla {\Sigma _e}\left( {x,y} \right)  = {\left[ {\frac{1}{{u\left( {x,y} \right)}},\,\frac{1}{{v\left( {x,y} \right)}}} \right]^T}
\end{equation}

However, as is also noted in \citet{Brosch2015,Ruckauer2016}, \cref{eq:eof_slopes_1} is subject to singularities when computing $u$ and $v$. If either component of $\nabla\Sigma_e$ tends to zero, the corresponding optical flow component grows to infinity, which is incorrect. For example, consider a horizontally moving vertical line. Along the $y$-direction, temporal differences between the resulting events are in this case very small. Therefore, $\frac{{\partial {\Sigma _e}}}{{\partial y}}$ is also small, which leads to a high value of the vertical component $v$, even though the line is moving horizontally.

In recent work, two modifications to the previously discussed methodology have been proposed. First, in \citet{Brosch2015} an approach is presented that is robust to singularities in $u$ and $v$, which led to significant accuracy improvements in the comparison in \citet{Ruckauer2016}. In this approach, an orthogonality constraint is imposed on the plane's normal vector $\left[p_x,\,p_y,\,p_t\right]^t$, the optical flow vector $\left[u,\,v,\,1\right]$ and the orientation $\left[l_x,\,l_y,\,0\right]$ of the edge in homogeneous coordinates. This constraint leads to a new expression of the optical flow components $u$ and $v$ in terms of the plane's normal vector:

\begin{equation}
\label{eq:slopes_to_flow}
\left[ {\begin{array}{*{20}{c}}
	u\\
	v
	\end{array}} \right] = \frac{1}{{{{\left\| {\nabla {\Sigma _e}} \right\|}^2}}}\nabla {\Sigma _e} = -\frac{{{p_t}}}{{{p_x}^2 + {p_y}^2}}\left[ {\begin{array}{*{20}{c}}
	{{p_x}}\\
	{{p_y}}
	\end{array}} \right]
\end{equation}

Second, in the implementation in \cite{Ruckauer2016} not all events within the space-time window are considered. For each pixel location, only the most recent event is used for computing optical flow. However, high contrast edges tend to produce multiple events in quick succession at a single pixel. Hence, the most recent event at a pixel occurs slightly later than the first event caused by such an edge, which leads to over-estimation of its speed. It may also lead to optical flow estimates in the opposite direction of the edge motion. To prevent this, a refractory period $\Delta t_R$ (typically 0.1 s) is applied. Events that occur within $\Delta t_R$ are neither processed nor preserved to support future events.

The discussed algorithm with the previously proposed modifications forms our baseline algorithm. In the following, methods are proposed to increase its efficiency and range of application. 

\subsection{Efficiency Improvements}
\label{sec:eof_efficiency_improvements}
In order to enable faster computation and scale the algorithm towards low-end processing hardware, we propose two modifications. 

The first modification is to reduce the number of parameters of the local plane. We reduce \cref{eq:plane_system} by introducing the new parameters $p_x^\ast$, $p_y^\ast$, $p_0^\ast$:

\begin{equation}
p_x^ *  = \frac{{{p_x}}}{{{p_t}}},\, p_y^ *  = \frac{{{p_y}}}{{{p_t}}},\, p_0^ *  = \frac{{{p_0}}}{{{p_t}}}
\end{equation}

hence obtaining the nonhomogeneous, three-parameter form of \cref{eq:plane_system}:

\begin{equation}
\label{eq:plane_system_2}
p_x^* x_i + p_y^* y_i + p_0^* = -t_i
\end{equation}

A second reduction is performed by assuming that the new event $\mathbf{e}_n$ intersects with the plane, which enables the definition of relative coordinates for neighbor events as follows. Given $\mathbf{e}_n$ and a previously identified neighbor event $\mathbf{e}_i$, the relative coordinates of the neighbor event are defined as $\delta x_i = x_i - x_n,\; \delta y_i = y_i - y_n,\; \delta t_i = t_i - t_n$. Substituting these coordinates into \cref{eq:plane_system_2} and rearranging terms, the following relation is obtained: 

\begin{equation}
\label{eq:plane_system_22}
p_x^*\delta {x_i} + p_y^*\delta {y_i} + \delta {t_i} =  - p_x^*{x_n} - p_y^*{y_n} - p_0^* - {t_n}
\end{equation}

By substituting \cref{eq:plane_system_2} (for which we set $i=n$ to enforce that $\emph{e}_n$ intersects with the plane) into \cref{eq:plane_system_22}, the right-hand side of the latter equation reduces to zero. Thus, the final plane $\mathbf{\Pi}^\ast=\left[p_x^\ast,\, p_y^\ast\right]$ is described by two parameters, the slopes:

\begin{equation}
\label{eq:plane_system_3}
p_x^* \delta x_i + p_y^* \delta y_i =  - \delta t_i
\end{equation}

This reduced approach requires significantly less computational effort than the baseline. While solving a homogeneous least squares system is generally performed using a Singular Value Decomposition (SVD), more efficient solvers such as the commonly used QR-decomposition are applicable to nonhomogeneous problems. With a system of $M$ events and $N$ parameters, the computational complexity of the SVD scales with $O(MN^2+N^3)$. In comparison, the complexity of the QR decomposition scales with $O(MN^2-N^3/3)$ \cite{Heath2002}. Hence, a four-parameter SVD solution has a cost that is approximately proportional to $16M+64$, which compares to $4M-8/3$ for a two-parameter QR-decomposition. Note that this is only a rough indication of the true complexity, but it suffices for illustrating the efficiency gain of the parameter reduction. Only a slight reduction in accuracy is introduced with this simplification.

The second modification consists of capping the rate at which optical flow vectors are identified, denoted as the output rate $\rho_F$. Depending on the computational resources available, input events can be processed at a limited rate to maintain real-time performance. In addition, since the approach assumes that for each individual event, motion needs to be estimated, neighboring events produce highly similar optical flow vectors, making the information increasingly redundant with increasing $\rho_F$. Therefore, capping this value also prevents unnecessary computational load on follow-up processes. To achieve this, we keep track of the timestamp $t_f$ of the event for which the latest optical flow vector was identified. If a new event $\mathbf{e}_n$ occurs, optical flow is only estimated if $t_n - t_f > 1/\rho_{F_{max}}$, where $\rho_{F_{max}}$ denotes the output rate limit. The event is, however, still stored to support future events, taking into account the refractory period. Hence, accuracy of the estimates that are still performed, is unaffected. The resulting effect of the value of $\rho_{F_{max}}$ on computational performance is explored in \cref{sec:results_optical_flow_computation}.

\subsection{Timestamp-Based Clustering of Recent Events}
\label{sec:eof_timesorting}

%The baseline algorithm incorporates a fixed setting for the time window $\Delta t$ to collect events. There are two limitations to setting the value of $\Delta t$. First, a smaller $\Delta t$ increases the lower limit for observing velocity. For slower motion, the time difference between neighboring events increases. Second, a larger time window allows inclusion of unrelated events, which is problematic when multiple fast-moving features generate multiple event layers in quick succession. This can be problematic when features are located closely together. A fixed time window therefore imposes a fundamental trade-off between minimal observable speed and feature density.
%
%To overcome this limitation, we propose a novel approach based on ordering of events by $t^*$, which generalizes to any magnitude of optical flow. Consider the event cloud shown in \cref{fig:intersectingFeatures}, where multiple separate features move through an arbitrarily sized space-time window. The points represent the latest timestamps when the event $\mathbf{e}_n$ (highlighted in red) is detected. Note that the points are clustered in multiple smaller clouds, and that most events actually belong to motion that occurred in the past. Only the most recent events provide information regarding the most recently detected feature. In this situation, including all events in the plane fitting process would not provide a sensible motion estimate. However, the 12 events occurring most recently before $\mathbf{e}_n$ (highlighted in green) provide a clear structure of which $\mathbf{e}_n$ is part.

The baseline algorithm incorporates a fixed setting for the time window $\Delta t$ to collect recent events. There are two main drawbacks of using a fixed time window, which are illustrated in \cref{fig:time_window} for two simple one-dimensional cases. First, $\Delta t$ defines the lower limit for the magnitude of observable optical flow. For slower motion, the time difference between neighboring events increases. If this difference is too large, all neighboring events fall outside the time window (as illustrated in \cref{fig:time_window_1}), such that the motion cannot be observed. Second, a larger time window can result in the inclusion of unrelated events. For example, in \cref{fig:time_window_2} events are shown which clearly belong to separate features. Still, a part of the outdated features falls within the time window, which leads to an inaccurate fit. In some cases outlier rejection may prevent this, but with tightly packed features this may still cause a failed estimate. A fixed time window therefore imposes a fundamental trade-off between minimal observable speed and feature density. Since MAVs tend to move at a wide range of velocities, from hovering to fast maneuvers, the capability of observing both fast and slow motion is desirable.

\begin{figure}[!t]
	\centering
	\setlength{\fwidth}{0.3\linewidth}
	\renewcommand{\ylabeldist}{0.15}
	\subfloat[]{
		\input{images/time_window_1}
		\label{fig:time_window_1}
	}
	\subfloat[]{
		\input{images/time_window_2}
		\label{fig:time_window_2}
	}\\
%	\tikzset{external/force remake=true}
	\subfloat[]{
		\input{images/time_window_3}
		\label{fig:time_window_3}
	}
	\caption{Examples with one-dimensional ($x-t$) event structures representing motion, in which a fixed time window for collecting events leads to failed motion estimates. In \protect\subref{fig:time_window_1}, the time window is too small for being able to perceive the slow motion that triggers the events. On the other hand, in \protect\subref{fig:time_window_2} events from two sequential fast-moving features enter the same time window. The bottom image \protect\subref{fig:time_window_3} illustrates the proposed clustering approach, in which the time difference between the current event and the next most recent event $\delta t_i$ defines the maximum time difference $\Delta t_S=k_S \delta t_i$. From the leftmost event, the time difference to the next most recent event exceeds $\Delta t_S$, such that all bottom events are rejected.}
	\label{fig:time_window}
\end{figure}
%
%We apply this finding by considering a fixed number $N_e$ most recent events for computing an initial fit and evaluating its accuracy. The latter is evaluated through the coefficient of determination $R^2$ of the fit, which provides a relative measure of accuracy with respect to the plane gradient. If this is below a rejection threshold $R^2_{min}$, it is assumed that the selected event cloud still consists of multiple separate clusters. Therefore, the \emph{latest point} is rejected, and the plane is recomputed. This step is repeated at most $N_i$ iterations until $R^2$ is sufficiently high. If even with less events $R^2$ is too low, the estimate is rejected. The parameter values for $N_e$, $R^2_{min}$, and $N_i$ are empirically set at 8, 0.8, and 2 respectively.

To accomplish this, we propose a very simple clustering method based on the time order of events, which is illustrated in \cref{fig:time_window_3} for a one-dimensional motion case. First, the minimum number of most recent events $\mathbf{e}_i$ for observing velocity is found. In the one-dimensional case in \cref{fig:time_window_3}, only one point is necessary for constructing a line; for two-dimensional image motion, two linearly independent points $(\delta x_i,\delta y_i,\delta t_i)$ are required in order to construct a plane. From the point with the largest $\delta t_i$, the relative timestamp defines a maximum time increment $\Delta t_S$ between the timestamps of consecutive events. To provide a margin for noise, $\delta t_i$ is scaled with a factor $k_S$ (which has a value of 3 in our experiments), such that $\Delta t_S=-\delta t_i k_S$ (since $\delta t_i$ should be negative). Second, we iterate through the remaining recent events, ordered by decreasing value of $\delta t_i$. If the time difference between two consecutive events $\mathbf{e}_i$ and $\mathbf{e}_{i-1}$ exceeds $\Delta t_S$, $\mathbf{e}_{i-1}$ and all events that occurred before it are assumed to belong to different features, and are rejected. 

Note that this approach does not take spatial location into account, except for finding the first linearly independent events. Therefore, a variation on the baseline process of outlier rejection is still applied, which is independent of time-scale. Instead of rejection based on a distance threshold, the overall fit quality is assessed through the Normalized Root Mean Square Error ($\mathit{NMRSE}$), defined here as follows:

\begin{equation}
\label{eq:NRMSE}
\mathit{NRMSE} = \frac{n}{{\sum\limits_{i = 1}^n {\delta {t_i}} }}\sqrt {\frac{{\sum\limits_{i = 1}^n {{{\left( {\delta {t_i} - p_x^*\delta {x_i} - p_y^*\delta {y_i}} \right)}^2}} }}{n}} 
\end{equation}

Then, while $\mathit{NRMSE} > \mathit{NRMSE_{max}}$, only the event having the maximum distance to $\mathbf{\Pi}^*$ is rejected. This is repeated until a maximum number of $n_{R}$ events are rejected. If $n_R$ is exceeded, the estimated plane is rejected and no optical flow is computed. Suitable values for attaining a high number of successful estimates, without sacrificing significant quality, are empirically set at $\mathit{NRMSE_{max}}=0.3$ and $n_R=2$. 

Still, incorrect optical flow estimates may be detected, either due to noise in the event stream or due to undesired inclusion of outlier events. Depending on the application, certain optical flow magnitudes can be deemed unrealistic in advance. Optical flow estimates are therefore rejected if their magnitude exceeds a threshold $V_{max}$, which is set to 1000 pixels/s. In addition, a minimum number of events $n_{min}$ must be found through the clustering mechanism in order to have sufficient support for a reliable fit. This number is set to 8 events. Further, a maximal time window of $\Delta t=2$ s is maintained such that unnecessary event checking is prevented. Note, however, that this time window can now be much larger than in the baseline approach. 

%The final algorithm is summarized in 
%
%\begin{figure}
%	
%	\begin{algorithmic}[1]
%		\REQUIRE{map $\cal M$ of latest timestamps for all possible pixel locations}
%		\FOR{each new event $\mathbf{e}_n$}
%			\STATE{Compute $\Delta t = t_n-\mathcal{M}\left(x_n,y_n,p_n\right)$}
%			\IF{$\Delta t > \Delta{t_R}$}
%				\STATE{reject event}
%			\ELSE
%				\STATE{Set $\mathcal{M}\left(x_n,y_n,p_n\right)=t_n$}
%			\ENDIF
%		\ENDFOR{}
%	\end{algorithmic}
%\caption{Time-sorted plane estimation}
%\end{figure}

\subsection{Evaluation}
\label{sec:results_optical_flow}
To evaluate optical flow estimation performance, several datasets were recorded in which the DVS was moved by hand, facing towards a ground surface covered with a textured mat. The recordings are performed indoors, using an Optitrack motion tracking system to measure ground truth position and orientation of the DVS. From these measurements, ground truth optical flow vectors are obtained using the relations derived in \cref{sec:model}. This is performed for each event for which optical flow is identified, hence providing the means for quantitative accuracy evaluation. Although currently datasets are already available \cite{Barranco2016,Ruckauer2016}, the recorded set specifically represents motion above a flat surface in indoor lighting conditions, i.e. the environment in which flight tests are performed in \cref{sec:results}.

Images of the recorded ground surfaces are shown in \cref{fig:textures}. The checkerboard in \cref{fig:checkerboard} provides high contrast and clear edges and is hence relatively simple for optical flow estimation. The roadmap texture in \cref{fig:roadmap} has largely unstructured features and lower contrast. It is used to show that our approach extends to more general situations as well. 
Eight short sequences were selected to evaluate the performance of the proposed method. Each sequence is approximately 1.0 s long and consists of event and pose measurements in which one primary motion type is present. Five sets are selected for the checkerboard; one for vertical translational image motion ($\vartheta_y \approx 1.0$), one for rotational motion ($r\approx-1.3$) rad/s, and three sets with diverging motion of different speeds ($\vartheta_z \approx \lbrace0.2,\,0.5,\,2.0\rbrace$). For the roadmap texture, three sets with diverging motion were selected as well ($\vartheta_z \approx \lbrace0.1,\,0.5,\,1.0\rbrace$). 

In the following analysis, the cap on $\rho_F$ is not applied (i.e. $\rho_{F_{max}}=\infty$), except for the assessment of computational complexity in \cref{sec:results_optical_flow_computation}.


\begin{figure}[!ht]
	\centering
	\subfloat[Checkerboard]{
		\includegraphics[width=0.3\linewidth]{images/texture_checkerboard.png}
		\label{fig:checkerboard}
	}
	\subfloat[Roadmap]{
		\includegraphics[width=0.3\linewidth]{images/texture_roadmap.png}
		\label{fig:roadmap}
	}
	\caption{Ground surface textures used during the experiments.}
	\label{fig:textures}
\end{figure}

%\paragraph{Diverging Checkerboard}
%The checkerboard texture, accompanied with slow motion, provides simple and clear line-based features to track. Primarily, this set contains vertical motion, leading to a diverging pattern. Yaw motion is limited, making the orientation of the lines approximately constant. Hence, this is a relatively simple dataset for motion estimation.
%
%\paragraph{Rotating Checkerboard}
%This second checkerboard-based sequence consists mainly of rotation in yaw. This leads to moving lines along which motion magnitude increases further away from the center of rotation. Therefore, it requires an algorithm that can cope with a wide variety of motion magnitudes. 
%
%\paragraph{Diverging Roadmap}
%The third sequence is more challenging due to the presence of more features, which are also less clearly distinguishable in the event data. Similar to the checkerboard equivalent, the primary motion is vertical and divergence magnitude is comparable.\\

\subsubsection{Qualitative evaluation}
\cref{fig:flow_results} shows optical flow vectors (yellow arrows) estimated using the improved algorithm during three of the selected sequences, along with ground truth flow vectors (blue arrows). Note that, for clarity, the time window for visualizing events is larger than for visualizing optical flow, which is why for some event locations, it appears that no optical flow estimates are found. Accurate normal flow estimates are visible for the checkerboard datasets. In \cref{fig:flow_checkerboard} optical flow is generally constant along the checkerboard edges, matching well to the normal component of the ground truth vectors. The rotating checkerboard sequence (\cref{fig:flow_rotation}) also provides accurate optical flow estimates. Clear variation of the normal flow magnitude along the lines is seen. 

\begin{figure*}[!ht]
	\begin{framed}
		\centering
		\subfloat[Translating checkerboard]{
			\includegraphics[width=0.31\linewidth]{images/flow_checkerboard}
			\label{fig:flow_checkerboard}
		}
		\subfloat[Rotating checkerboard]{
			\includegraphics[width=0.31\linewidth]{images/flow_rotation}
			\label{fig:flow_rotation}
		}
		\subfloat[Diverging roadmap ($\vartheta_z=1.0$)]{
			\includegraphics[width=0.31\linewidth]{images/flow_roadmap}
			\label{fig:flow_roadmap}
		}
		\caption{Optical flow estimated in several sequences, shown as yellow arrows. The accompanying blue arrows show the ground truth optical flow. Events are shown as green dots (positive polarity) or red dots (negative polarity). The time window for displaying optical flow in each sequence is 10 ms. To better visualize the event input, a larger window of 50 ms is applied for the events.}
		\label{fig:flow_results}
	\end{framed}
\end{figure*}

Last, in \cref{fig:flow_roadmap} it is clearly visible that the roadmap texture is more challenging. Event structures are less coherent and the visible features are more noisy. Optical flow vectors are sparsely present, yet the available estimates are sufficient for observing the global motion. At some location with noisy features the motion tends to be underestimated, but the majority of the estimates is very similar to the normal direction of the ground truth motion.

\subsubsection{Quantitative evaluation}
For quantitative evaluation, a comparison was made of the proposed optical flow algorithm and the baseline algorithm by \citet{Benosman2014} (as detailed in \cref{sec:eof_plane_fitting}). In this comparison the fixed time window $\Delta t$ for the original approach is set to 100 ms, and the rejection distance $d_{max}$ is set to 0.001 to obtain a similar event density in both algorithms. For a consistent comparison, the baseline algorithm incorporate the same refractory period $\Delta t_R$, maximum speed limit $V_{max}$, and minimum number of events $n_{min}$ as the proposed algorithm.

For benchmarking optical flow accuracy, several error metrics have been introduced such as the endpoint error and angular error \cite{Baker2011}. These metrics have been incorporated into recent event-based optical flow benchmarks as well \cite{Ruckauer2016}. However, they are defined for optical flow that is fully determinable and is not subject to the aperture problem. The algorithms in this section both estimate normal flow. In \citet{Barranco2014} a version of the endpoint error is applied that indicates the magnitude error of the normal flow with respect to the \emph{projection} of ground truth optical flow along the normal flow vector. This metric is employed here as well. For each optical flow vector, we compute the Projection Endpoint Error (PEE), which is defined as follows:

\begin{equation}
\label{eq:PEE}
\mathrm{PEE} = \left| {\left\| {\bf{V}} \right\| - \frac{{\bf{V}}}{{\left\| {\bf{V}} \right\|}} \cdot {{\bf{V}}_{GT}}} \right|
\end{equation}

where $\mathbf{V}=\left[u,\,v\right]^T$ is the normal flow estimate and $\mathbf{V}_{GT}$ denotes the ground truth optical flow vector.

A comparison of the resulting mean absolute PEE values, and their standard deviations, is presented in \cref{tab:flow_errors}. The optical flow density is also shown (abbreviated here as $\eta$) which indicates the percentage of events for which an optical flow estimate was found. A high value of $\eta$ indicates that more motion information can be obtained with a given event input. 

%Remarkably similar results are obtained for the checkerboard sets, both for the average PEE and density. However, a slight difference is seen with the diverging roadmap scene. Our algorithm scores slightly better on the average PEE, reduces the variation in error, and also obtains a slightly higher density.

\begin{table}[!ht]
	\centering
	\setlength{\tabcolsep}{0.5em}
	\renewcommand{\arraystretch}{1.3}
	\caption{Projection Endpoint Error (mean absolute error and standard deviation) and density results of the baseline plane fitting algorithm, and the new algorithm proposed in this work. Values highlighted in bold are the lowest PEE or the highest density result of both algorithms.}
	\footnotesize
	\input{tables/flow_errors}
	\label{tab:flow_errors}
\end{table} 

Overall, the results are very similar. Both algorithms reach good scores on the checkerboard sets with translation, rotation, and medium divergence ($\vartheta_z=0.5$). However, some differences are observable. The proposed algorithm tends to reach a higher optical flow density in the slow divergence ($\vartheta_z=0.2)$ checkerboard scene and in all roadmap scenes, since the baseline algorithm fails to perceive slowly moving features. Still, this does not degrade the estimate accuracy with respect to the baseline algorithm, or only to a limited extent. Note also that in both fast diverging sequences (Checkerboard, $\vartheta_z=2.0$, and Roadmap, $\vartheta_z=1.0$) a lower mean absolute PEE is achieved with our approach.

\subsubsection{Computational Performance Evaluation}
\label{sec:results_optical_flow_computation}
An assessment of computational complexity is made using two datasets, one for both texture types. Both sets have a duration of 12 s and contain approximately 40k events per second. This enables quantifying the potential of $\rho_{F_{max}}$ to regulate processing time, as well as the effect of texture. The algorithm is implemented in C and interfaced with MATLAB through MEX, running single-threaded on a Windows 10 64bit laptop with an Intel Core i7 Q720 quadcore CPU. Each dataset and setting of $\rho_{F_{max}}$ is processed ten times for consistent results. The CPU usage of MATLAB during the test was around 12\%. 

The resulting computation time per event for several settings of $\rho_{F_{max}}$ is shown in \cref{fig:timing_rate_control}, in comparison with the maximal computation time with no control of $\rho_{F_{max}}$. For both textures it is clearly possible to regulate processing time by $\rho_{F_{max}}$. A lower limit appears to be present, which is due to the remaining overhead related to event timestamp copying. Interestingly, there is a clear influence of texture. This difference is due to higher contrast edges in the checkerboard texture, at which several successive events are generated per pixel. Therefore, the refractory period filter rejects these duplicate events before optical flow computation. 

Without the refractory period, computational effort is similar for both textures. In this case, the maximal computation time per event, i.e. without control of $\rho_F$, is 2.11 $\upmu$s. This is equivalent to processing 470k events per second in real-time, which is easily sufficient for processing realistic scenes on the test machine. Event sequences recorded for post-processing contained event peaks below 150k events per second. Nevertheless, if hardware capabilities are more restricted (e.g. in on-board applications), control of $\rho_F$ can be applied to scale the computational complexity of the algorithm down if necessary.

\begin{figure}[h]
	\centering
	\setlength{\fwidth}{0.5\linewidth}
	\renewcommand{\xlabeldist}{-0.05}
	\input{images/timing_rate_control}
	\caption{Processing time per event for checkerboard and roadmap datasets, for different settings of $\rho_{F_{max}}$. The dashed lines indicate the computation times when no limit is applied to $\rho_{F_{max}}$.}
	\label{fig:timing_rate_control}
\end{figure}



