%!TEX root = mainArchitGNN.tex

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%   S   E   C   T   I   O   N   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
The {\it selection} GNNs of Section \ref{sec:selection} create convolutional features adapted to the structure of the graph with linear shift invariant graph filters. The {\it aggregation} GNNs that we describe here apply the conventional CNN architecture of Section \ref{sec:regular} to a signal with temporal (regular) structure that is generated to incorporate the topology of the graph. To create such a temporal arrangement we consider successive applications of the graph shift operator $\bbS$ to the input graph signal $\bbx^g$ (see first row of Fig.~\ref{fig_aggregation}). This creates a sequence of $N$ graph shifted signals $\bby^g_0, \ldots, \bby^g_{N-1}$. The first signal of the sequence is $\bby^g_0 = \bbx^g$, the second signal is $\bby^g_1 = \bbS\bbx^g$, and subsequent members of the sequence are recursively obtained as $\bby^g_k = \bbS\bby^g_{k-1} = \bbS^{k}\bbx^g$. We observe that each vector $\bby_{k}^{g}$ incorporates the underlying support by means of multiplication by the graph shift operator $\bbS$. We arrange the sequence of signals $\bby^g_k$ into the matrix representation of the graph signal $\bbx^g$ that we define as
% 
\begin{equation} \label{eqn:aggregation_matrix}
   \,\, \bbX^g \, := \, [\bby^g_0,     \bby^g_1, ...,           \bby^g_{N-1}] 
          \, := \, [\bbx^g,   \bbS\bbx^g,   ..., \bbS^{N-1}\bbx^g      ] .\!\!
\end{equation}
%
The matrix $\bbX^g$ is a redundant representation of $\bbx^g$. In fact, for any connected graph any row of $\bbX^g$ is sufficient to recover $\bbx^g$ as each row contains $N$ linear combinations of $\bbx^g$ \cite{marques16-aggregation}. We thus note that any such row has successfully incorporated the graph structure included in the powers of the graph shift operator $\bbS$, without any loss of information. Our goal here is to work at a designated node $p$ with the signal $\bbz_p^g$ that contains the components of the diffusion sequence $\bby^g_k$ that are observed at node $p$ (see second row of Fig.~\ref{fig_aggregation}). This is simply the $p$th row of $\bbX^g$ and leads to the definition
% 
\begin{equation} \label{eqn:agg_representation}
   \bbz_p^g \,:=\ \Big[\bbX^g\Big]_p^{\Tr}
            \, =\ \Big[ \big[          \bbx^g\big]_{p} ; 
                        \big[\bbS      \bbx^g\big]_{p} ; \ldots ; 
                        \big[\bbS^{N-1}\bbx^g\big]_{p} 
                  \Big].
\end{equation}
%
The signal $\bbz_p^g$ is a local representation at node $p$ that accounts for the topology of the graph in a temporally structured manner. Indeed, since the diffusion sequence $\bby^g_k$ is generated from a temporal diffusion process the components of the sequence $\bbz_p^g$ are elements of a time sequence. Yet, the components of this time sequence depend on the topology of the graph. The first element of $\bbz_p^g$ is the value of the input signal $\bbx^g$ at node $p$, which is independent of the graph topology, but the second element $\bbz_p^g$ aggregates information from values of the input $\bbx^g$ within the neighborhood of $p$ as defined by the nodes that are connected to node $p$. The third element of $\bbz_p^g$ is an aggregate of aggregates which results in the aggregation of information from the 2-hop neighborhood of $p$. As we move forward in the sequence $\bbz_p^g$ we incorporate information from nodes that are farther from $p$ as determined by the topology of the graph. In this way, we have successfully generated a regular structured signal that effectively incorporates the underlying structure. We note that two consecutive elements of $\bbz_{p}^{g}$ indeed relate neighboring values according to the topology of the graph.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%   F   I   G   U   R   E   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\begin{figure*}[!t]
\centering
\includegraphics[width=0.2300\textwidth]{figures/MultiNodeCNNGSLayer0Shift0.pdf} \hfill 
\includegraphics[width=0.2300\textwidth]{figures/MultiNodeCNNGSLayer0Shift1.pdf} \hfill
\includegraphics[width=0.2300\textwidth]{figures/MultiNodeCNNGSLayer0Shift2.pdf} \hfill
\includegraphics[width=0.2300\textwidth]{figures/MultiNodeCNNGSLayer0Shift3.pdf} \\ \vspace{2mm}
\includegraphics[width=0.3200\textwidth]{figures/MultiNodeCNNGSLayer1Input.pdf}  \hfill
\includegraphics[width=0.3200\textwidth]{figures/MultiNodeCNNGSLayer1Input.pdf}  \hfill
\includegraphics[width=0.3200\textwidth]{figures/MultiNodeCNNGSLayer1Input.pdf}  \\ \vspace{2mm}
\includegraphics[width=0.3200\textwidth]{figures/MultiNodeCNNGSLayer1Conv.pdf}   \hfill
\includegraphics[width=0.3200\textwidth]{figures/MultiNodeCNNGSLayer1Conv.pdf}   \hfill
\includegraphics[width=0.3200\textwidth]{figures/MultiNodeCNNGSLayer1Conv.pdf}   \\ \vspace{2mm}
\includegraphics[width=0.3200\textwidth]{figures/MultiNodeCNNGSLayer1Pool.pdf}   \hfill
\includegraphics[width=0.3200\textwidth]{figures/MultiNodeCNNGSLayer1Pool.pdf}   \hfill
\includegraphics[width=0.3200\textwidth]{figures/MultiNodeCNNGSLayer1Pool.pdf}   \\ \vspace{2mm}
\includegraphics[width=0.3200\textwidth]{figures/MultiNodeCNNGSLayer2Input.pdf}  \hfill
\includegraphics[width=0.3200\textwidth]{figures/MultiNodeCNNGSLayer2Input.pdf}  \hfill
\includegraphics[width=0.3200\textwidth]{figures/MultiNodeCNNGSLayer2Input.pdf}  \\ \vspace{2mm}
\includegraphics[width=0.2300\textwidth]{figures/MultiNodeCNNGSLayer3Input.pdf} \hfill 
\includegraphics[width=0.2300\textwidth]{figures/MultiNodeCNNGSLayer3Shift0.pdf} \hfill
\includegraphics[width=0.2300\textwidth]{figures/MultiNodeCNNGSLayer3Shift1.pdf} \hfill
\includegraphics[width=0.2300\textwidth]{figures/MultiNodeCNNGSLayer3Shift2.pdf} \\ \vspace{2mm}
\caption{Multinode Aggregation Graph Neural Networks. Start by selecting a subset $\ccalP_{1} \subset \ccalV$ of $P_{1}$ nodes of the graph (row 1, diagram 1). Then, proceed to perform $Q_{1}$ local exchanges with neighbors (row 1, diagrams 2, 3, and 4) in order to build $P_{1}$ regular time-structure signals, one at each node (row 2), see \eqref{eqn:multinode_agg_1}. We note that in row 1, the color disks illustrate the reach of the $Q_{1}$ local exchanges of each of the selected nodes $\ccalP_{1}$. Once the regular structured signals have been constructed on each of the $P_{1}$ nodes, proceed with a regular CNN, applying regular convolution (row 3), and regular pooling (row 4), until $F_{L_{1}}$ features are obtained at each node (row 5), see \eqref{eqn:conv_time}-\eqref{eqn_downsampling}, \eqref{eqn:inner_layer_output}. Now, we view each feature as a graph signal being supported on the selected nodes, see \eqref{eqn:outer_layer_input}, zero-padded to fit the graph (row 6, diagram 1), see \eqref{eqn:outer_layer_pad}. We then select a smaller subset $\ccalP_{2} \subseteq \ccalP_{1}$ of $P_{2} \leq P_{1}$ nodes (row 2, diagram 2) and carry out $Q_{2}$ local exchanges with the neighbors, (row 2, diagrams 2, 3 and 4), illustrated with color disks in the last row. These neighbor exchanges create new regular structured signals at each of the $\ccalP_{2}$ nodes, see cf.~\eqref{eqn:outer_layer_time}. Then, we continue by computing $F_{L_{2}}$ regional features at each node by means of regular CNNs and so on.}
	\label{fig_multinode}
\end{figure*}

If the signal $\bbz_p^g$ is a signal in time, it can be processed with a regular CNN architecture and this is indeed our definition of aggregation GNNs. At the first layer $\ell=1$ we take the locally aggregated signal $\bbz_p^g$ as input and produce features $\bbu_{p1}^{fg}$ by convolving with the $K_{p1}$-tap filter $\bbh_{p1}^{fg}$ [cf. \eqref{eqn:conv_time}],
% 
\begin{align}\label{eqn_conv_agg_layer_1}
   \Big[\bbu_{p1}^{fg}\Big]_n 
       :=   \Big[\bbh_{p1}^{fg} \ast \bbz_p^g\Big]_{n}  
	    =   \sum_{k=0}^{K_{p1}-1}  \Big[ \bbh_{p1}^{fg}  \Big]_{k}  \,
	        \Big[ \bbz_p^g \Big]_{n-k} ,
\end{align}
%
where we use zero padding to account for border effects and assume the size of the output is the same as the input. The convolution in \eqref{eqn_conv_agg_layer_1} is the regular time convolution. In fact, except for minor notational differences to identify the aggregation node $p$, \eqref{eqn_conv_agg_layer_1} is the same as \eqref{eqn:conv_time} with $\ell=1$. The topology of the graph is incorporated in \eqref{eqn_conv_agg_layer_1} not because of the convolution but because of the way in which we construct $\bbz_p^g$. To emphasize the effect of the topology of the graph we use \eqref{eqn:agg_representation} to rewrite \eqref{eqn_conv_agg_layer_1} as
%
\begin{equation} \label{eqn_conv_agg_layer_1_shift_explicit}
   \Big[\bbu_{p1}^{fg}\Big]_n 
       = \sum_{k=0}^{K_{p1}-1} 
              \Big[ \bbh_{p1}^{fg}     \Big]_{k} 
              \Big[ \bbS^{n-k-1}\bbx^g \Big]_{p}
\end{equation}
%
Since the convolution in \eqref{eqn_conv_agg_layer_1_shift_explicit} considers consecutive values of the signal $\bbz_p^{g}$, the features $\bbu_{p1}^{fg}$ have a structure that is convolutional on the graph $\bbS$. Each feature element $[\bbu_{p1}^{fg}]_{n}$ is a linear combination of consecutive $K_{p1}$ neighboring values of the input $\bbx^{g}$ starting with shift $\bbS^{n-1}\bbx^{g}$ and ending at $\bbS^{n-K_{p1}-1}\bbx^{g}$. Alternatively, note that the regular convolution operation linearly relates consecutive elements of a vector; and since consecutive elements in vector $\bbz_{p}^{g}$ reflect nearby neighborhoods according to the graph, we have effectively related neighboring values of the graph signal by means of a regular convolution. Thus, coefficients $\bbh_{p1}^{fg}$ encoding low-pass filters further aggregate information across neighborhoods, while high-pass filters output features quantifying differences between consecutive neighborhood resolutions. Thus, low-pass time filters applied to $\bbz_p^{g}$ detect features that are smooth on the graph $\bbS$, while high-pass time filters applied to $\bbz_p^{g}$ detect sharp transitions between signal values between nearby nodes.

Once the features $\bbu_{p1}^{fg}$ in \eqref{eqn_conv_agg_layer_1}, or their equivalents in \eqref{eqn_conv_agg_layer_1_shift_explicit}, are computed, we sum features $\bbu_{p1}^{fg}$ as per \eqref{eqn:agg_features} obtaining $\bbu_{p1}^{f}$, compute local summaries as per \eqref{eqn_group_nonlinearity} yielding $\bbv_{p1}^{f}$, and subsample according to \eqref{eqn_downsampling} resulting in features $\bbx_{p1}^{f}$. Since in this case the indexes of the feature vector represent (neighborhood) resolution, some applications may benefit from non-equally spaced sampling schemes that put more emphasis on sampling the high-resolution (low-resolution) part of the feature vector. Subsequent layers repeat the computation of convolutional features and pooling steps in \eqref{eqn:conv_time}-\eqref{eqn_downsampling}. Formally, all of the variables in \eqref{eqn:conv_time}-\eqref{eqn_downsampling} need to be marked with a subindex $p$ to identify the aggregation node.

\begin{remark} \normalfont
The aggregation GNN architecture reduces trivially to conventional CNNs when particularized to graph signals defined over a cyclic graph. Since $[\bbA_{\dc}^{k} \bbx^{g}]_{p} = [\bbx^{g}]_{1+(p+k) \mod N}$ is a cyclic shift of the input signal $\bbx^{g}$, then $\bbz_{p}^{g}=\bbx^{g}$ in \eqref{eqn:agg_representation} for all $p$ and a regular CNN follows.
\end{remark}

\begin{remark} \normalfont
The aggregation GNN architecture rests on transforming the data on the graph in such a way that it becomes supported on a regular structure, and thus regular CNN techniques can be applied. Transforming graph data is the main concern of graph embeddings \cite{cai17-embeddings}. Unlike the methods surveyed in \cite{cai17-embeddings}, we consider the underlying graph support $\ccalG$ as given (not learned), we do not attempt to compress the graph data as construction of aggregated vector $\bbz_{p}^{g}$ does not entail any loss of information (if all eigenvalues of $\bbS$ are distinct), and the focus is on data defined on top of the graph (the graph signal), rather than the graph itself (given by $\bbS$).
\end{remark}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%   S   E   C   T   I   O   N   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\subsection{Multinode Aggregation Graph Neural Networks}\label{sec_aggregation_multinode}

Selecting a single node $p \in \ccalV$ to aggregate all the information generally entails $N-1$ local exchanges with neighbors [cf. \eqref{eqn:aggregation_matrix}]. For large networks, carrying out all these exchanges might be infeasible, either due to the associated communication overhead or due to numerical instabilities. This can be overcome by selecting a subset of nodes to aggregate local information, i.e., selecting a submatrix of \eqref{eqn:aggregation_matrix} with a few rows and columns in lieu of a single row with all the columns; see Fig.~\ref{fig_multinode}. The selected nodes will first process their own samples using an aggregation GNN and then exchange the obtained outputs with the other selected nodes. This process is repeated until the information has been propagated through the entire graph. 

To explain such a two-level hierarchical architecture, let us denote as $\ell$ the layer index for the aggregation stage and as $r$ the layer index for the exchange stage. The total number of exchange (outer) layers is $R$ and, for each outer layer $r$, a total number of $L_r$ aggregation (inner) layers is run. We start by describing the procedure for $r=1$, where $\ccalP_{1} \subset \ccalV$ denotes the subset of selected nodes and let $Q_{1}$ denotes the number of times the shift is applied ($\bbS^{q}$, for $q=0,\ldots,Q_{1}-1$). It is observed that this amounts to selecting $P_1=|\ccalP_{1}|$ rows and $Q_{1}$ consecutive columns of \eqref{eqn:aggregation_matrix}. Setting $\ell=0$, the signal aggregating the $(Q_{1}-1)$-hop neighborhood information at each node $p \in \ccalP_{1}$ can be constructed as [cf.~\eqref{eqn:agg_representation}]
\begin{equation} \label{eqn:multinode_agg_1}
	\bbz_{p0}^{g}(1,Q_{1}) 
			\,:=\ \Big[ \big[          \bbx^g\big]_{p} ; 
                        \big[\bbS      \bbx^g\big]_{p} ; \ldots ; 
                        \big[\bbS^{Q_{1}-1}\bbx^g\big]_{p} 
                  \Big].
\end{equation}
Since $\bbz_{p0}^{g}$ exhibits a time structure, the regular CNN steps \eqref{eqn:conv_time}-\eqref{eqn_downsampling} can be applied individually at each node (see Fig.~\ref{fig_multinode}). More specifically, since $L_1$ denotes the number of layers for the aggregation stage when $r=1$, a set of $F_{L_{1}}$ descriptive features of the $(Q_{1}-1)$-hop neighborhood of node $p$ is constructed by concatenating $\ell=0,\ldots,L_{1}-1$ layers of the form \eqref{eqn:conv_time}-\eqref{eqn_downsampling} as is done in the aggregation GNN. Setting $\ell=L_1$, the output of the last layer of the aggregation stage is
\begin{equation} \label{eqn:inner_layer_output}
	\bbz_{pL_{1}}(1,Q_{1}) 
		= \Big[ z_{pL_{1}}^{0}; \ldots; z_{pL_{1}}^{F_{L_{1}}} \Big].
\end{equation}
Different feature vectors $\bbz_{p L_{1}}$ of dimension $F_{L_{1}}$ are obtained at each of the $p$ selected nodes, describing the corresponding $(Q_{1}-1)$-hop neighborhood.

In order to further aggregate these local features (describing local neighborhoods) into more global information, we need to collect each feature $g$ at every selected node $p \in \ccalP_{1}$. More precisely, let $P_{1} = |\ccalP_{1}|$ be the number of selected nodes, then
\begin{equation} \label{eqn:outer_layer_input}
	\bbx_{1}^{g} = \Big[ z_{p_{1}L_{1}}^{g};\ldots;z_{p_{P_{1}}L_{1}}^{g} \Big]
\end{equation}
where each $p_{k} \in \ccalP_{1}$. We now set $r=2$ and select a subset of nodes $\ccalP_{2} \subseteq \ccalP_{1}$ to aggregate features $\bbx_{1}^{g}$ by means of local neighborhood exchanges. However, signal $\bbx_{1}^{g}$ has dimension $P_{1} < N$, so it cannot be directly exchanged through the original graph $\ccalG$. We therefore use zero padding to make $\bbx_{1}^{g}$ fit the graph
\begin{equation} \label{eqn:outer_layer_pad}
	\tbx_{1}^{g} = \bbP_{1}^{\Tr} \bbx_{1}^{g}
\end{equation}
with $\bbP_{1} $ being the ${P_{1} \times N}$ fat binary matrix that selects the subset $\ccalP_{1}$ of rows of \eqref{eqn:aggregation_matrix}. Then, we apply $Q_{2}$ times the original shift $\bbS$ to the signals $\tbx_{1}^{g}$, collecting information at nodes $p \in \ccalP_{2}$,
\begin{equation} \label{eqn:outer_layer_time}
	\bbz_{p0}^{g}(2,Q_{2}) 
			\,:=\ \Big[ \big[          \tbx_{1}^g\big]_{p} , 
                        \big[\bbS      \tbx_{1}^g\big]_{p} , \ldots , 
                        \big[\bbS^{Q_{2}-1}\tbx_{1}^g\big]_{p} 
                  \Big]^{\Tr}.
\end{equation}
Once $\bbz_{p0}^{g}$ is collected at each node $p \in \ccalP_{2}$ the time-structure of the signal is exploited to deploy another regular CNN \eqref{eqn:conv_time}-\eqref{eqn_downsampling} (aggregation GNN stage) in order to obtain $F_{L_{2}}$ features that describe the region.

In general, consider the output of \emph{outer layer} $r-1$ is $\bbx_{r-1}^{g}$, consisting of feature $g$ at a subset $\ccalP_{r-1}$ of $P_{r-1}$ nodes [cf. \eqref{eqn:outer_layer_input}], for $g=1,\ldots,F_{L_{r-1}}$. Then, this signal is zero padded to fit the original graph $\tbx_{r-1}^{g}=\bbP_{r-1}^{\Tr}\bbx_{r-1}^{g}$ [cf.~\eqref{eqn:outer_layer_pad}] and the graph shift $\bbS$ is applied $Q_{r}$ times, collecting the shifted versions at a subset of nodes $\ccalP_{r}$ to construct time-structure signal $\bbz_{p0}^{g}(r,Q_{r})$ [cf. \eqref{eqn:outer_layer_time}]. Each node $p \in \ccalP_{r}$ runs a regular CNN \eqref{eqn:conv_time}-\eqref{eqn_downsampling} with $L_{r}$ \emph{inner layers} to produce $F_{L_{r}}$ features $\bbz_{pL_{r}}(r,Q_{r})$ [cf. \eqref{eqn:inner_layer_output}] that are then collected at each of the nodes $p \in \ccalP_{r}$ to produce $\bbx_{r}^{f}$ [cf.~\eqref{eqn:outer_layer_input}], for $f=1,\ldots,F_{L_{r}}$. See Fig.~\ref{fig_multinode} for an illustration of the architecture.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%   S   E   C   T   I   O   N   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\subsection{Practical Considerations}

\myparagraph{Local architecture.} The single node aggregation GNN architecture is entirely \emph{local}. Only one node $p \in \ccalV$ is selected, and that node gathers all the relevant information about the data by means of local exchanges only. Furthermore, the output at the last layer is also obtained at a single node, so there is no need to have actual physical access to every node in the network.

\myparagraph{Regular CNN design.} Since signal $\bbz_{p}^{g}$ gathered at node $p$ exhibits a regular time structure, the state-of-the-art expertise in designing conventional CNNs can be immediately leveraged to inform the design of convolutional layers of aggregation GNNs.

\myparagraph{Numerical normalization.} For big networks, some of the entries of $\bbS^{k}$ (as well as the components of $\bbz_p^{g}$ associated with those powers) can grow too large, leading to numerical instability. To avoid this, aggregation schemes typically work with a normalized version of the graph shift operator that guarantees that the spectral radius of $\bbS$ is one. 
  
\myparagraph{Choice of aggregating node.} The choice of nodes that aggregate all the information has an impact on the overall performance of the algorithm. This decision can be informed by several criteria such as the degree, the frequency content of the signals of interest \cite{marques16-aggregation} or be determined by different measures of centrality in the network \cite{segarra16-centrality}. In particular, in the experiments carried out in Sec.~\ref{sec:sims}, we select nodes based on the leverage scores obtained by the two sampling schemes described in \cite{anis16-spectralproxies} and \cite{varma15-scores}.

\myparagraph{Filter taps.} For a generic (inner) layer $1<\ell<L_{r}$ the generation of the feature vectors $\bbu_{\ell}^{fg}\in \reals^{N_{\ell-1}}$ and $\bbu_{\ell}^{f}\in \reals^{N_{\ell-1}}$ is as in \eqref{eqn:conv_time} and \eqref{eqn:agg_features}, so that we have that $\bbu_{\ell}^{f}\ =\ \sum_{g=1}^{F_{\ell-1}} \bbu_{\ell}^{fg}\  =\ \sum_{g=1}^{F_{\ell-1}} \bbh_{\ell}^{fg} \ast \bbz_{p(\ell-1)}^{g}$. The main difference in this case is on the type and length of the filter coefficients $\bbh_{\ell}^{fg}\in \reals^{K_{\ell}}$. While in classical CNNs the filter coefficients are critical to aggregate the information at different resolutions, here part of that aggregation has been already taken care of in the first layer when transforming $\bbx^{g}$ into $\bbz_p^{g}$. As a result, the filter taps in the aggregation GNN architecture can have a shorter length and place more emphasis in high frequency features.

\myparagraph{Pooling.} Something similar applies to the pooling schemes. The summarization and downsampled vectors for the aggregation architecture are obtained as $[\bbv^{f}_{\ell}]_n 
= \rho_{\ell}( [\bbu_{\ell}^f]_{\bbn_\ell})$ and $\bbx^{f}_{\ell} =  \sigma_\ell (\bbC_{\ell}\bbv^{f}_{\ell} )$, which coincide with their counterparts for classical CNNs in \eqref{eqn_group_nonlinearity} and \eqref{eqn_downsampling}. The difference is therefore not in the expressions, but on how $\bbn_{\ell}$ and $\bbC_{\ell}$ are selected. 
While in traditional CNNs the signal $\bbx^{g}$ is global in that all the samples have the same resolution, in the aggregation architecture the signal $\bbz_p^{g}$ is local and different samples correspond to different levels of resolution. More specifically, aggregation pooling schemes for $\bbn_{\ell}$ and $\bbC_{\ell}$ that preserve the top samples of the feature vectors $\bbu^f_\ell$ to keep finer resolutions combined with a few bottom samples to account for coarser information are reasonable, while in traditional CNNs regular schemes for $\bbn_{\ell}$ and $\bbC_{\ell}$ that extract information and sample the signal support regularly can be more adequate.

\myparagraph{Design flexibility.} The multinode aggregation GNN acts as a decentralized method for constructing regional features. We note that, for ease of exposition, the number of shifts $Q_{r}$ at each outer layer is the same for all nodes as well as the number of features $F_{L_{r}}$ that are obtained at each node. However, this architecture can be adapted to different node-dependent parameters in a straightforward manner.

\myparagraph{Computational cost.} The computational cost of the multinode aggregation GNN at each outer layer $r$ is that of processing the regular CNN for each node, $O (\sum_{p=1}^{P_{r}} \sum_{\ell=1}^{L_{r}} N_{\ell-1} K_{\ell} F_{\ell-1} F_{\ell})$ which can be easily distributed among the $P_{r}$ involved nodes.

\myparagraph{Number of parameters.} The number of parameters of the multinode aggregation GNN is $O(\sum_{p=1}^{P_{r}} \sum_{\ell=1}^{L_{r}} K_{\ell} F_{\ell} F_{\ell-1})$.  We observe, though, that the regular CNNs employed tend to be very small, since the initial $Q_{r}$ regular CNN at each node) as well as the length of the filters $K_{\ell}$ are very small as well (typically, $K_{\ell} \ll Q_{r}$, cf. Sec.~\ref{sec:regular}).
