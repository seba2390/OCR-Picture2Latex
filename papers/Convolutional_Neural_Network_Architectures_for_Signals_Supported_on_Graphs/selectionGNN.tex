%!TEX root = mainArchitGNN.tex

%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% SECTION : Selection %%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%

Generalizing the first layer of a CNN to signals supported on graphs is straightforward as it follows directly from the definition of a linear shift invariant filter \cite{segarra17-linear}. Going back to the definition of convolutional features in \eqref{eqn:conv_time} we reinterpret the filters $\bbh_{1}^{fg}$ as graph filters that process the features $\bbx_0^g$ through a graph convolution. This results in intermediate features $\bbu_{1}^{fg}$ having components 
% 
\begin{equation} \label{eqn:conv_graph}
   \Big[\bbu_{1}^{fg}\Big]_n
      := \Big[\bbh_{1}^{fg} \ast_\bbS \bbx_0^g \Big]_{n} 
      := \sum_{k=0}^{K_1-1} 
             \Big[ \bbh_{1}^{fg} \Big]_{k} 
             \Big[ \bbS^{k} \bbx_{0}^{f}\Big]_{n},
\end{equation}
where we have used $\ast_\bbS$ to denote the graph convolution operation on $\bbS$. The summations in equations \eqref{eqn:conv_time} and \eqref{eqn:conv_graph} are analogous except for the different interpretations of what it means to shift the input signal $\bbx_{0}^{f}$. In \eqref{eqn:conv_time}, a $k$-unit shift at index $n$ means considering $[\bbx_{0}^{f}]_{n-k}$, the value of the signal $\bbx_{0}^{f}$ at time $n-k$. In \eqref{eqn:conv_graph}, graph shifting at node $n$ entails the operation $[ \bbS^{k} \bbx_{0}^{f}]_{n}$ which composes a multiplication by $\bbS^k$ with the selection of the resulting value at $n$. In fact, particularizing \eqref{eqn:conv_graph} to the cyclic graph by making $\bbS=\bbA_{\dc}$ renders \eqref{eqn:conv_time} and \eqref{eqn:conv_graph} equivalent. From the perspective of utilizing \eqref{eqn:conv_graph} as an extractor of local (graph) convolutional features it is important to note that graph convolutions aggregate information through successive local operations [cf. \eqref{eqn_gso_is_a_shift}]. A filter with $K_1$ taps incorporates information at node $n$ that comes from nodes in its $(K_1-1)$-hop neighborhood.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%   F   I   G   U   R   E   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\begin{figure*}[t]
\begin{center}
\begin{tabular}{lll}
\includegraphics[width=0.3\textwidth]{figures/SelectionCNNGSLayer1Input.pdf} & 
\includegraphics[width=0.3\textwidth]{figures/SelectionCNNGSLayer1Conv.pdf}  &
\includegraphics[width=0.3\textwidth]{figures/SelectionCNNGSLayer1Pool.pdf}  \\ \\ 
\includegraphics[width=0.3\textwidth]{figures/SelectionCNNGSLayer2Input.pdf} &
\includegraphics[width=0.3\textwidth]{figures/SelectionCNNGSLayer2Conv.pdf}  & 
\includegraphics[width=0.3\textwidth]{figures/SelectionCNNGSLayer2Pool.pdf}  \\ \\ 
\includegraphics[width=0.3\textwidth]{figures/SelectionCNNGSLayer3Input.pdf} & 
\includegraphics[width=0.3\textwidth]{figures/SelectionCNNGSLayer3Conv.pdf}  & 
\includegraphics[width=0.3\textwidth]{figures/SelectionCNNGSLayer3Pool.pdf}
\end{tabular}
\end{center}  	
\caption{Selection Graph Neural Networks. Consider the input to be a signal supported by a known $N$-node graph. First, convolutional features are obtained by means of graph filtering in the original graph [cf.~\eqref{eqn:graph_filter}]. The color disks in the second column illustrate the convolution operation on each node. Then, a subset of $N_{1}$ nodes is selected, and summarizing function $\rho_{1}$ and pointwise nonlinearity $\sigma_{1}$ are applied to the neighborhood $\bbn_{1}$ for each of these nodes, obtaining the output $\bbx_{1}^{f}$ for the first layer. The color disks in the third column show the reach of the pooling operation, the size of the neighborhood being pooled (in the first row, the disks include only the one-hop neighborhood; also, only a few disks are shown so as not to clutter the illustration). In order to obtain convolutional features for following layers, we zero pad the signal to \emph{fit} the original graph [cf.~\eqref{eqn_zero_padding}] so as to apply a graph filter and then resample the output at the same set of nodes [cf.~\eqref{eqn_conv_features_unpadded_preliminary}-\eqref{eqn_conv_features_unpadded}]. Then, a new smaller subset of nodes is selected, and the summarizing function and pointwise nonlinearity are applied to a neighborhood of these nodes [cf. \eqref{eqn_graph_neigborhood}]. This process is repeated while selecting fewer and fewer nodes.}
\label{fig:selection_cnn}
\end{figure*}

Although we wrote \eqref{eqn:conv_graph} componentwise to emphasize its similarity with \eqref{eqn:conv_time} we can drop the $n$ subindices to write a vector relationship. For future reference we further define the linear shift invariant filter $\bbH_{1}^{fg} := \sum_{k=0}^{K_1-1} [\bbh_{1}^{fg}]_{k} \bbS^{k}$ to write
% 
\begin{equation} \label{eqn:graph_filter}
   \bbu_{1}^{fg} 
	\ =  \ \sum_{k=0}^{K_1-1} 
	             \Big[ \bbh_{1}^{fg} \Big]_{k} \,
	             \bbS^{k} 
	             \bbx_{0}^{f}
    \ := \ \bbH_{1}^{fg}\bbx_{0}^{f}.
\end{equation}
%
The graph filter \eqref{eqn:graph_filter} is a generalization of the Chebyshev filter in \cite{defferrard17-cnngraphs}. More precisely, if $\ccalG$ is an undirected graph, and we adopt the normalized Laplacian as the graph shift operator $\bbS$, then \eqref{eqn:graph_filter} boils down to a Chebyshev filter. The convolutional stage in \cite{kipf17-classifgcnn} is a Chebyshev filter of $K=2$, and thus is also a special case of \eqref{eqn:graph_filter}. We also note that the use of polynomials on arbitrary graph shift operators for the convolutional stage has been also proposed in \cite{du17-topoadapt, gama18-nvgf}.
Asides from replacing the linear time invariant filter in \eqref{eqn:conv_time} with the graph shift invariant filter in \eqref{eqn:graph_filter}, the remaining components of the conventional CNN architecture can remain more or less unchanged. The feature aggregation in \eqref{eqn:agg_features} to obtain $\bbu_{1}^{f}$ needs no modification as it is a simple summation independent of the graph structure. The summarization operator in \eqref{eqn_group_nonlinearity} requires a redefinition of locality. This is not difficult because it follows from \eqref{eqn:graph_filter} that $\bbu_{1}^{f}$ is another $N$-node graph signal that is defined over the same graph $\bbS$. We can then use $\bbn_1$ to represent a graph neighborhood of node $n$ and apply the same summary operator. We point out that $\bbn_1$ need not be the 1-hop neighborhood of $n$. The sampling and activation operation in \eqref{eqn_downsampling} requires a matrix $\bbC_1$ to sample over the irregular graph domain. Apart from the challenge of selecting sampling matrices for graphs -- see \eqref{eqn:selection_set} and \cite{chen15-selection, marques16-aggregation, anis16-spectralproxies, tsitsvero16-uncertainty} --, this does not require any further modification to \eqref{eqn_downsampling}. The first row of Fig.~\ref{fig:selection_cnn} shows the operations carried out in this first layer.

The challenge in generalizing CNNs to GNNs arises beyond the first layer. After implementing the sampling operation in \eqref{eqn_downsampling} the signal $\bbx^{f}_{1}$ is of lower dimensionality than $\bbu_{1}^{f}$ and can no longer be interpreted as a signal supported on $\bbS$. In regular domains this is not a problem because we use the extraneous geometrical information of the underlying domain to define convolutions in the space of lower dimensionality. To see this in terms of graph signals, let us interpret the signal $\bbx_{0}^{g}$ defined on a regular domain as one defined on a cyclic graph with $N_0=N$ nodes, which is also the same graph that describes  $\bbu_{1}^{f}$. Then, if we consider a downsampling factor of $(N_{1}/N_{0})$,  another \textit{cyclic} graph with $N_1$ nodes describes the signal $\bbx_{1}^{f}$. However, when graph signals are defined in a generic irregular domain, there is no extraneous information to elucidate the form of the graph that describes signals beyond the first layer. Resolving mismatched supports is a well-known problem in signal processing whose simplest and most widely-used solution is zero padding. The following sections illustrate how zero padding can be leveraged to resolve one of the critical challenges in the implementation of GNNs.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%   S   E   C   T   I   O   N   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\subsection{Selection Sampling on Graph Convolutional Features}\label{sec_selection_convolution}

Sampling is an operation that selects components of a signal. To explain the construction of convolutional features on graphs, it is more convenient to think of sampling as the \textit{selection of nodes} of a graph which we call active nodes. This implies that at each layer $\ell$ we place the input features $\bbx^f_{\ell-1}$ of dimension $N_{\ell-1}$ on top of the active nodes of the graph $\bbS$. Selection schemes are further discussed in Sec.~\ref{subsec_selection_practical}. Doing so requires that we keep track of the location of the samples. Thus, at each layer $\ell$ we consider input features $\bbx_{\ell-1}^{g}$ each with $N_{\ell-1}$ components, and zero padded features $\tbx_{\ell-1}^{g}$ each with size $N$ but only $N_{\ell-1}$ nonzero components which replicate the values of $\bbx_{\ell-1}^{g}$. The indexes of the nonzero components of $\tbx_{\ell-1}^{g}$ correspond to the location of the elements of $\bbx_{\ell-1}^{g}$ in the original graph. It is clear that we can move from the unpadded to the padded representation by multiplying with an $N \times N_{\ell-1}$ tall binary sampling matrix $\bbD_{\ell-1}^{\Tr}$. Indeed, if we let $[\bbD_{\ell-1}]_{mn}=1$ represent the $m$th component of the unpadded feature, $[\bbx_{\ell-1}^{g}]_m$, is located in the $n$th node of the graph, we can write the padded feature as
%
\begin{equation} \label{eqn_zero_padding}
   \tbx_{\ell-1}^{g} \ =\ \bbD_{\ell-1}^{\Tr} \bbx_{\ell-1}^{g} .
\end{equation}
%
The advantage of keeping track of the padded signal is that convolutional features can be readily obtained by operating in the original graph. Given the notion of graph convolution in \eqref{eqn:graph_filter} and (re-)defining $\bbh_{\ell}^{fg}$ to be the graph filter coefficients at layer $\ell$ we can define intermediate features as [cf. \eqref{eqn:conv_time}]
% 
\begin{equation} \label{eqn_conv_features_padded}
   \tbu_{\ell}^{fg} 
	\ :=  \ \sum_{k=0}^{K_{\ell}-1} 
	             \Big[ \bbh_{\ell}^{fg} \Big]_{k} \,
	             \bbS^{k} \,
	             \tbx_{\ell-1}^{g} .
\end{equation}
%
Although a technical solution to the construction of convolutional features, \eqref{eqn_conv_features_padded} does not exploit the computational advantages of sampling. These can be recovered by selecting components of $\tbu_{\ell}^{fg}$ at the same set of nodes that support $\bbx_{\ell-1}^{g}$. We then define $\bbu_{\ell}^{fg} := \bbD_{\ell-1} \tbu_{\ell}^{fg}$. If we further use \eqref{eqn_zero_padding} to substitute $\tbx_{\ell-1}^{g}$ into the definition of the convolutional features in \eqref{eqn_conv_features_padded}, we can write
% 
\begin{equation} \label{eqn_conv_features_unpadded_preliminary}
   \bbu_{\ell}^{fg}
     := \, \bbD_{\ell-1} \tbu_{\ell}^{fg} 
	  = \, \bbD_{\ell-1}
	       \sum_{k=0}^{K_{\ell}-1} 
	               \Big[ \bbh_{\ell}^{fg} \Big]_{k} \,
	               \bbS^{k} \,
	               \bbD_{\ell-1}^{\Tr} \,
	               \bbx_{\ell-1}^{g} .
\end{equation}
%
If we further define reduced dimensionality $k$-shift matrices 
% 
\begin{equation} \label{eqn_gsp_unpadded_powers}
   \bbS_{\ell}^{(k)} :=  \bbD_{\ell-1}\, \bbS^{k}\,\bbD_{\ell-1}^{\Tr},
\end{equation}
%
and reorder and regroup terms in \eqref{eqn_conv_features_unpadded_preliminary} we can reduce the definition of convolutional features to 
% 
\begin{equation} \label{eqn_conv_features_unpadded}
   \bbu_{\ell}^{fg}
	\ =  \ \sum_{k=0}^{K_{\ell}-1} 
	             \Big[ \bbh_{\ell}^{fg} \Big]_{k} \,
	             \bbS_{\ell}^{(k)}\,
	             \bbx_{\ell-1}^{g} 
	\:=  \ \bbH_{\ell}^{fg} \bbx_{\ell-1}^{g} ,
\end{equation}
%
where we have also defined the \textit{subsampled} linear shift invariant filter $\bbH_{\ell}^{fg} :=  \sum_{k=0}^{K_{\ell}-1} [\bbh_{\ell}^{fg}]_{k}\bbS_{\ell}^{(k)}$. Implementing \eqref{eqn_conv_features_unpadded_preliminary} entails repeated application of the shift operator to the padded signal, which can be carried out with low cost if the original input graph is sparse. In \eqref{eqn_conv_features_unpadded}, the filter $\bbH_{\ell}^{fg}$ takes advantage of sampling to operate directly on a space of lower dimension $N_{\ell-1}$. The matrices $\bbS_{\ell}^{(k)}$ can be computed beforehand because they depend on the graph shift operator and the sampling matrices only. We emphasize that, save for subsampling, \eqref{eqn_conv_features_unpadded} and \eqref{eqn_conv_features_unpadded_preliminary} are equivalent and that, therefore, the features $\bbu_{\ell}^{fg}$ generated by the subsampled filter $\bbH_{\ell}^{fg}$ are convolutional relative to the original graph (shift) $\bbS$. The middle image in Fig.~\ref{fig:selection_cnn} shows zero pad of input signal, convolution in the original graph, and resampling of the filter output.

Features $\bbu_{\ell}^{f}$ can be obtained from features $\bbu_{\ell}^{fg}$ using the same linear aggregation operation in \eqref{eqn:agg_features} which does not require adaptation to the structure of the graph,
%
\begin{equation} \label{eqn_agggregated_features_graph}
   \bbu_{\ell}^{f}
	   \  =\ \sum_{g=1}^{F_{\ell-1}} \bbH_{\ell}^{fg} \bbx_{\ell-1}^{g}.
\end{equation}
%
This completes the construction of convolutional features and leads to the pooling stage we describe next.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%   S   E   C   T   I   O   N   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\subsection{Selection Sampling and Pooling}\label{sec_selection_pooling}

The pooling stage requires that we redefine the summary and sampling operations in \eqref{eqn_group_nonlinearity} and \eqref{eqn_downsampling}. Generalizing the summary operation requires redefining the aggregation neighborhood. In the first layer, this can be readily accomplished by selecting the $\alpha_{1}$-hop neighborhood of each node for some given $\alpha_{1}$ that defines the reach of the summary operation. This information is actually contained in the powers of the shift operator. The 1-hop neighborhood of $n$ is the set of nodes $m$ such that $[\bbS]_{nm} \neq 0$, the 2-hop neighborhood is the union of this set with those nodes $m$ with $[\bbS^2]_{nm} \neq 0$ and so on. In the case of the sampled features the graph neighborhoods need to be intersected with the set of active nodes. This intersection is already captured by the $k$-shift matrices $\bbS_{\ell}^{(k)}$ [cf. \eqref{eqn_gsp_unpadded_powers}]. Thus, at layer $\ell$ we introduce an integer $\alpha_{\ell}$ to specify the reach of the summary operator and define the $\alpha_{\ell}$-hop neighborhood of $n$ as
% 
\begin{equation} \label{eqn_graph_neigborhood}
   \bbn_\ell = \left[ m: \big[\bbS_{\ell}^{(k)}\big]_{nm} \neq 0,\ 
                       \text{for some}\ k\leq \alpha_{\ell} \right].
\end{equation}
%
Summary features $[\bbv^{f}_{\ell}]_n$ at node $n$ are computed from \eqref{eqn_group_nonlinearity} using the graph neighborhoods in \eqref{eqn_graph_neigborhood}. These neighborhoods follow the node proximity encoded by $\bbS$, see third column of Fig.~\ref{fig:selection_cnn}.

To formally explain the downsampling operation in \eqref{eqn_downsampling} in the context of graph signals, begin by defining sampling matrices adapted to irregular domains. This can be easily defined at the $\ell$th  layer if we let the sampling matrix $\bbC_{\ell}$ be a fat matrix with $N_{\ell}$ rows and $N_{\ell-1}$ columns with the properties
% 
\begin{equation} \label{eqn:selection_set}
   [\bbC_{\ell}]_{mn} \in \{0,1\}       , \quad
   \bbC_{\ell} \bbone = \bbone          , \quad
   \bbC_{\ell}^{\Tr} \bbone \leq \bbone .
\end{equation}
%
When $[\bbC_{\ell}]_{mn}=1$ it means that the $n$th component of $\bbv^{f}_{\ell}$ is selected in the product $\bbC_{\ell}\bbv^{f}_{\ell}$ and stored as the $m$th component of the output. The properties in \eqref{eqn:selection_set} ensure that exactly $N_{\ell}$ components of $\bbv^{f}_{\ell}$ are selected and that no component is selected more than once. They do not, however, enforce a regular sampling pattern. We further define the nested sampling matrix $\bbD_{\ell}$ as the product of all sampling matrices applied up until layer $\ell$
%
\begin{equation}\label{eqn:nested_sampling_matrices}
   \bbD_{\ell}\ =\ \bbC_{\ell} \bbC_{\ell-1}  \ldots  \bbC_{1}
              \ =\  \prod_{\ell'=1}^\ell \bbC_{\ell'} .
\end{equation}
%
Matrix $\bbD_{\ell}$ keeps track of the location of the selected nodes in the original graph, for each layer $\ell$, and is thus used for the zero padding operation in \eqref{eqn_conv_features_unpadded_preliminary}.

Each layer of the selection GNN architecture is determined by \eqref{eqn_conv_features_unpadded}-\eqref{eqn_agggregated_features_graph} for the convolution operation and \eqref{eqn_group_nonlinearity}-\eqref{eqn_downsampling} for pooling and nonlinearity. To summarize, the input to layer $\ell$ is $\bbx_{\ell-1}$ comprised of $F_{\ell-1}$ features $\bbx_{\ell-1}^{f}$ located at a subset of nodes given by $\bbD_{\ell-1}$. Then, we use the reduced dimensionality $k$-shift matrices \eqref{eqn_gsp_unpadded_powers} to process $\bbx_{\ell-1}^{f}$ using a graph filter as in \eqref{eqn_conv_features_unpadded}, and obtain aggregated features $\bbu_{\ell}^{f}$ \eqref{eqn_agggregated_features_graph}. A neighborhood $\bbn_{\ell}$ for each element of $\bbu_{f}$ is determined by \eqref{eqn_graph_neigborhood} for some $\alpha_{\ell}$ and the output $\bbv_{\ell}^{f}$ of the summarizing function $\rho_{\ell}$ is computed as in \eqref{eqn_group_nonlinearity}. Finally, following \eqref{eqn_downsampling}, a smaller subset of nodes is selected by means of $\bbC_{\ell}$ and the pointwise nonlinearity $\sigma_{\ell}$ is applied to obtain the $\ell$th output features $\bbx_{\ell}^{f}$, for $f=1,\ldots,F_{\ell}$. See Algorithm~\ref{algm_selection_gnn} for details.

\begin{algorithm}[t]
 	\caption{Selection Graph Neural Network.}
	\label{algm_selection_gnn}

	\begin{algorithmic}[1]
 \Statex \textbf{Input:} $\{\hbx\}$: testing dataset, $\ccalT$: training dataset
 \Statex $\quad \bbS$: graph shift operator, $L$: Number of layers, 
 \Statex $\quad \{F_{\ell}\}$: number of features, $\{K_{\ell}\}$: degree of filters
 \Statex $\quad \{\rho_{\ell}\}$: neighborhood summarizing function
 \Statex $\quad \texttt{selection}$: selection sampling method
 \Statex $\quad \{N_{\ell}\}$: number of nodes on each layer
 \Statex $\quad \{\sigma_{\ell}\}$: pointwise nonlinearity
 \Statex \textbf{Output:} $\{\hby\}$: estimates of $\{\hbx\}$
 \Statex
 \Procedure{selection\_gnn}{$\{\hbx\}$, $\ccalT$, $\bbS$, $L$, $\{F_{\ell}\}$,$\{K_{\ell}\}$, $\{\rho_{\ell}\}$, $\texttt{selection}$, $\{N_{\ell}\}$, $\{\sigma_{\ell}\}$}
 {\Statex{$\rhd$ \emph{Create architecture:}}}
   \For{$\ell = 1:L-1$}
       \State Compute $\bbD_{\ell-1} = \bbC_{\ell-1} \bbD_{\ell-2}$ \Comment{\emph{See \eqref{eqn:nested_sampling_matrices}}}
       \State Compute $\bbS_{\ell}^{(k)}$ for $k=0,\ldots,K_{\ell}-1$ \Comment{\emph{See \eqref{eqn_gsp_unpadded_powers}}}
       \State Create $[\bbh_{\ell}^{fg}]_{k}$, $f = 1,\ldots,F_{\ell}$, $g=1,\ldots,F_{\ell-1}$
       \State Compute filters $\bbH_{\ell}^{fg} = \sum_{k=0}^{K_{\ell-1}} [\bbh_{\ell}^{fg}]_{k} \bbS_{\ell}^{(k)}$
       \State Aggregate filtered features $\sum_{g=1}^{F_{\ell-1}} (\bbH_{\ell}^{fg} \cdot)$
       \State Apply summarizing function $\rho_{\ell}(\cdot)$
       \State Select $N_{\ell}$ nodes following method $\texttt{selection}$
       $$ \bbC_{\ell} = \texttt{selection}(N_{\ell},\bbC_{\ell-1}) $$
       \State Downsample output of summarizing function $\bbC_{\ell} \rho_{\ell}$
       \State Apply pointwise nonlinearity $\sigma_{\ell}(\cdot)$
   \EndFor
   \State Create fully connected layer $\bbA_{L} \cdot$
 \Statex{$\rhd$ \emph{Train:}}
   \State Learn $\{[\bbh_{\ell}^{fg}]_{k}\}$ and $\bbA_{L}$ from $\ccalT$
 \Statex{$\rhd$ \emph{Evaluate:}}
   \State Obtain $\hby$ applying GNN on $\hbx$ with learned coefficients
 \EndProcedure
 	\end{algorithmic}

\end{algorithm}

\begin{remark}\normalfont
The selection GNN architecture recovers a conventional CNN when particularized to graph signals described by a cyclic graph (conventional discrete time signals). To see this, let $\bbS=\bbA_{\dc}$ for a graph of size $N$, and let $\bbC_{\ell-1}$ be the sampling matrix that takes $N_{\ell-1}$ equally spaced samples out of the previous $N_{\ell-2}$ samples, for every $\ell$. Then, the nested sampling matrix $\bbD_{\ell-1}$ becomes a sampling matrix that takes $N_{\ell-1}$ equally spaced samples out of the $N$ original ones. This implies that $\bbS_{\ell}^{(k)} = \bbD_{\ell-1} \bbA_{\dc}^{k} \bbD_{\ell-1}^{\Tr}$ becomes either the $k$th power of the adjacency matrix of a cyclic graph with $N_{\ell-1}$ nodes for $k$ a multiple of $N/N_{\ell-1}$, or the all-zero matrix otherwise. This results in convolutional features obtained by \eqref{eqn_conv_features_unpadded} being equivalent to those obtained by \eqref{eqn:conv_time}. Likewise, making $\alpha_{\ell}=N_{\ell-1}/N_{\ell}$ for all $\ell$ leads to regular pooling and downsampling. This shows that the selection GNN does indeed boil down to the conventional CNN for discrete time signals.
\end{remark}


\begin{remark}\normalfont
The dimension $N_{\ell}$ is being effectively reduced without the need to use a complex multiscale hierarchical clustering algorithm. More specifically, in each layer, only a new set of nodes is used, but there is no need to recompute edges between these nodes or new weight functions, since the underlying graph on which the operations are actually carried out is the same graph support as the initial input data $\bbx$. This, not only avoids the computational cost of obtaining multiscale hierarchical clusters, but also avoids the need to assess when such clustering scheme is adequate.
\end{remark}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%   S   E   C   T   I   O   N   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\subsection{Practical Considerations} \label{subsec_selection_practical}

\myparagraph{Selection of nodes.} There is a vast GSP literature on sampling by selecting nodes, see, e.g., \cite{chen15-selection, anis16-spectralproxies, tsitsvero16-uncertainty, puy15-scores, varma15-scores}. In this paper, we consider that any one of these methods is adopted throughout the Selection GNN, and at each layer $\ell$ matrix $\bbC_{\ell}$ is determined by following the chosen method. On each layer $\ell$ the subset of nodes selected by $\bbC_{\ell}$ is always a subset of the nodes chosen in the previous layer. This implies that $N_{\ell} \leq N_{\ell-1}$ and that $\bbC_{\ell} \bbC_{\ell-1}$ never yields the zero matrix. In particular, in Sec.~\ref{sec:sims}, we adopt the methods proposed in \cite{anis16-spectralproxies} and \cite{varma15-scores} to study their impact on the overall performance of the Selection GNN.

\myparagraph{Locality of filtering.} The graph convolution remains a local operation with respect to the original input graph. Since each convolutional feature is zero padded to fit the graph, the implementation of the graph filter at each layer can be carried out by means of local exchanges in the original support. This can be a good computational option if the original input graph is sparse, and therefore repeatedly applying the graph shift operator exploits this sparsity. This turns out to be particularly useful when such a support represents a physical network with physical connections.

\myparagraph{Centralized computing.} When regarding the selection pooling architecture as a whole, being executed from a single centralized unit (i.e. when local connectivity is not important for computation purposes, for example, in the training phase), it is observed that the computational cost of carrying out convolutions \eqref{eqn_conv_features_unpadded} is reduced to matrix multiplication in the smaller $N_{\ell}$-dimensional space. It is noted that the reduced dimensionality $k$-shift matrices \eqref{eqn_gsp_unpadded_powers} can be obtained before the training phase, and also, that the statistical properties of learning the filter taps are not affected by it. This observation, coupled with the previous one, shows that the selection pooling architecture adequately addresses the global vs. local duality by efficiently computing convolutions in both settings.

\myparagraph{Computation of nonlinearities.} From an implementation perspective, it is observed that, while the local summarizing function $\rho_{\ell}$ involves the neighborhood of the $N_{\ell-1}$ nodes (which are more than the $N_{\ell}$ nodes that are kept in layer $\ell$), this function only has to be computed for those $N_{\ell}$ nodes that are left after downsampling. That is, it is not needed to compute $\rho_{\ell}$ at each one of the $N_{\ell-1}$ nodes, but only at the $N_{\ell}$ nodes that are actually kept after downsampling. In this sense, this nonlinear operation can be subsumed with the pointwise nonlinearity $\sigma_{\ell}$ that is applied to the $N_{\ell}$ nodes. To further illustrate this point, suppose that max-pooling is used and that the corresponding pointwise nonlinearity is a ReLU, $\sigma_{\ell}(x) = \max\{0,x\}$. Then, both operations can be performed simultaneously at node $n$ by doing $\max\{0,\{x_{m} : (m,n) \in \bbn_{\ell}\}\}$, where $\bbn_{\ell}$ denotes the paths in the neighborhood, and where this operation is computed only for nodes $n$ that are part of the $N_{\ell}\leq N_{\ell-1}$ selected nodes.

\myparagraph{Regularization of filter taps.} As the Selection GNN grows in depth (more layers), the number of filter taps in the convolution stage might increase, in order to access information located at further away neighbors (this happens if the few selected nodes at some deeper layer are far away from each other, as measured by the number of neighborhood exchanges). It is a good idea, then, to structure the filter coefficients $\bbh_{\ell}^{fg}$ in these deeper layers. More specifically, filtering with $N$ taps might be necessary, so it makes sense to choose $[\bbh_{\ell}^{fg}]_k$ constant for a range of $k$, since no new substantial information is going to be included for a wide range of those $k$. This reduces the number of trainable parameters and consequently overfitting.

\myparagraph{Definition of neighborhoods.} Information from the weight function $\ccalW$ of the graph can be included in the pooling stage \eqref{eqn_graph_neigborhood}. More precisely, instead of defining the neighborhood only looking at the edge set $\ccalE$, that is $[\bbS_{\ell}^{(k)}]_{nm} \neq 0$, we can make $[\bbS_{\ell}^{(k)}]_{nm} \geq \delta$ so that we summarize only across edges stronger than $\delta$.

\myparagraph{Frequency interpretation of convolutional features.} One advantage of having convolutional features defined always on the same graph $\ccalG$ at every layer $\ell$ is that these can be easily analyzed from a frequency perspective. Since the graph Fourier transform of a signal depends on the eigenvectors $\bbV$ of the graph shift operator \cite{sandryhaila14-freq}, and since the same $\bbS = \bbV \bbLambda \bbV^{-1}$ is used to define all convolutional features [cf. \eqref {eqn_conv_features_unpadded_preliminary}], then they all share the same frequency basis, allowing for a comprehensive frequency analysis at all layers. In particular, if we focus on normal matrix GSOs, i.e. $\bbV^{-1} = \bbV^{\Hr}$, the zero-padding aliasing effect is evidenced in the fact that $\bbV^{\Hr} \bbD^{\Tr} \bbD \bbV$ need not be the identity matrix for arbitrary eigenvectors $\bbV$ and downsampling matrices $\bbD$, altering the frequency content of the input signal to a filter. However, the filter taps are learned from the training set, taking into account this aliasing effect, and therefore are able to cope with it, extracting useful features.

\myparagraph{Computational cost.} The number of computations at each layer is given by the cost of the convolution operation, which is $O(|\ccalE|K_{\ell} F_{\ell} F_{\ell-1})$ if \eqref{eqn_conv_features_unpadded_preliminary} is used, or $O(N_{\ell-1}^{2} K_{\ell} F_{\ell} F_{\ell-1})$ if \eqref{eqn_conv_features_unpadded} is used, since pooling and downsampling incur in negligible cost. We observe that in \eqref{eqn_conv_features_unpadded} the cost tends to be dominated by $N_{\ell-1}^{2}$ making dimensionality reduction (i.e. pooling) a critical step for scalability.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%   F   I   G   U   R   E   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\begin{figure*}[!t]
\centering
\includegraphics[width=0.2300\textwidth]{figures/AggregationCNNGSLayer0Shift0.pdf} \quad 
\includegraphics[width=0.2300\textwidth]{figures/AggregationCNNGSLayer0Shift1.pdf} \quad
\includegraphics[width=0.2300\textwidth]{figures/AggregationCNNGSLayer0Shift2.pdf} \quad
\includegraphics[width=0.2300\textwidth]{figures/AggregationCNNGSLayer0Shift3.pdf} \\ \vspace{2mm}
\includegraphics[width=1.0000\textwidth]{figures/AggregationCNNGSLayer1Input.pdf}  \\ \vspace{2mm}
\includegraphics[width=1.0000\textwidth]{figures/AggregationCNNGSLayer1Conv.pdf}   \\ \vspace{2mm}
\includegraphics[width=1.0000\textwidth]{figures/AggregationCNNGSLayer1Pool.pdf}   \\ \vspace{2mm}
\includegraphics[width=1.0000\textwidth]{figures/AggregationCNNGSLayer2Input.pdf} 
\caption{Aggregation Graph Neural Networks. Select a node $p \in \ccalV$ and perform successive local exchanges with its neighbors. For each $k$-hop neighborhood (illustrated by the increasing disks in the first row), record $\bbS^{k}\bbx^{g}$ at node $p$ and build signal $\bbz_{p}^{g}$ which exhibits a regular structure [cf.~\eqref{eqn:agg_representation}]. Once a regular time-structure signal is obtained, we proceed to apply regular convolution and pooling to process the data [cf. \eqref{eqn:conv_time}-\eqref{eqn_downsampling}].}
	\label{fig_aggregation}
\end{figure*}

\myparagraph{Number of parameters.} The number of parameters to be learned at each layer are determined by the length of the filters, and the number of input and output features and is given by $O(K_{\ell} F_{\ell} F_{\ell-1})$ independent of $N_{\ell-1}$.
