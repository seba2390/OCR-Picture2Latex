%!TEX root = mainArchitGNN.tex

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%   S   E   C   T   I   O   N   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
Given a training set $\ccalT := \{(\bbx,\bby)\}$ formed by inputs $\bbx$ and their associated outputs $\bby$, a learning algorithm produces a representation (mapping) that can estimate the output $\hby$ that should be assigned to an input $\hbx\notin\ccalT$. NNs produce a representation using a stacked layered architecture in which each layer composes a linear transformation with a pointwise nonlinearity \cite{goodfellow16-deeplearn}. Formally, the first layer of the architecture begins with a linear transformation to produce the intermediate output $\bbu_1 := \bbA_{1}\bbx_{0} = \bbA_{1}\hbx$ followed by a pointwise nonlinearity to produce the first layer output $\bbx_1 := \sigma_1(\bbu_1) = \sigma_1(\bbA_{1}\bbx_0)$. This procedure is applied recursively so that at the $\ell$th layer we compute the transformation
%
\begin{equation}\label{eqn_nn_layers}
    \bbx_\ell := \sigma_\ell(\bbu_\ell) := \sigma_\ell(\bbA_{\ell}\bbx_{\ell-1}).
\end{equation}
%
In an architecture with $L$ layers, the input $\hbx=\bbx_0$ is fed to the first layer and the output $\hby = \bbx_L$ is read from the last layer \cite{kuo17-recos}. Elements of the training set $\ccalT$ are used to find matrices $\bbA_{\ell}$ that optimize a training cost of the form $\sum_{(\bbx,\bby)\in\ccalT} f(\bby, \bbx_L)$, where $f(\bby, \bbx_L)$ is a fitting metric that assess the difference between the NN's output $\bbx_L$ produced by input $\bbx$ and the desired output $\bby$ stored in the training set. Computation of the optimal NN coefficients $\bbA_\ell$ is typically carried out by stochastic gradient descent, which can be efficiently computed using the backpropagation algorithm \cite{rumelhart86-backprop}.

The NN architecture in \eqref{eqn_nn_layers} is a multilayer perceptron composed of fully connected layers \cite{kuo17-recos}. If we denote as $M_\ell$ the number of entries of the output of layer $\ell$, the matrix $\bbA_{\ell}$ contains $M_{\ell}\times M_{\ell-1}$ components. This, likely extremely, large number of parameters not only makes training challenging but empirical evidence suggests that it leads to overfitting \cite{huang17-densecnn}. CNNs resolve this problem with the introduction of two operations: Convolution and pooling.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%   F   I   G   U   R   E   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\begin{figure*}[t]
\centering
	\begin{subfigure}{.3\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/RegularCNNLayer1Input.pdf}
		\caption{input}
		\label{layer1input}
	\end{subfigure}
	\hfill
	\begin{subfigure}{.3\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/RegularCNNLayer1Conv.pdf} 
		\caption{convolution}
		\label{layer1conv}
	\end{subfigure}
	\hfill
	\begin{subfigure}{.3\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/RegularCNNLayer1Pool.pdf} 
		\caption{pooling}
		\label{layer1pool}
	\end{subfigure}
	\\ \vspace{0.5cm}
	\begin{subfigure}{.16\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/RegularCNNLayer2Input.pdf}
		\caption{input}
		\label{layer2input}
	\end{subfigure}
	\hfill
	\begin{subfigure}{.16\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/RegularCNNLayer2Conv.pdf}
		\caption{convolution}
		\label{layer2conv}
	\end{subfigure}
	\hfill
	\begin{subfigure}{.16\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/RegularCNNLayer2Pool.pdf}
		\caption{pooling}
		\label{layer2pool}
	\end{subfigure}
	\hfill
	\begin{subfigure}{.05\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/RegularCNNLayer3Input.pdf}
		\caption{}
		\label{layer3input}
	\end{subfigure}
	\hfill
	\begin{subfigure}{.05\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/RegularCNNLayer3Conv.pdf}  
		\caption{}
		\label{layer3conv}
	\end{subfigure}
	\hfill
	\begin{subfigure}{.05\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/RegularCNNLayer3Pool.pdf}
		\caption{}
		\label{layer3pool}
	\end{subfigure}
\caption{Convolutional Neural Networks. \subref{layer1input} Consider the input to be a discrete time signal, represented by a succession of signal values. \subref{layer1conv} Convolve this signal with a filter to obtain corresponding features [cf. \eqref{eqn:conv_time}]. The color disks centered at each node symbolize the convolution operation. \subref{layer1pool} Apply pooling [cf. \eqref{eqn_group_nonlinearity}]. The color disks symbolize the reach of the pooling operation (the number of samples that are pooled together) \subref{layer2input} Downsample to obtain a discrete time signal of smaller size [cf. \eqref{eqn_downsampling}]. \subref{layer2conv}-\subref{layer3pool} Repeat the application of convolution and pooling, trading off the temporal dimension for more features.}
\label{fig:regular_cnn}
\end{figure*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%   S   E   C   T   I   O   N   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\subsection{Convolutional Features}\label{sec_cnn_convolution}

To describe the creation of convolutional features write the output of the $(\ell-1)$st layer as $\bbx_{\ell-1} := [\bbx^{1}_{\ell-1}; \ldots; \bbx^{F_{\ell-1}}_{\ell-1}]$. This decomposes the $M_{\ell-1}$-dimensional output of the $(\ell-1)$st layer as a stacking of $F_{\ell-1}$ features of dimension $N_{\ell-1}$. This collection of features is the input to the $\ell$th layer. Likewise, the intermediate output $\bbu_{\ell}$ can be written as a collection of $F_{\ell}$ features $\bbu_\ell := [\bbu^{1}_{\ell}; \ldots; \bbu^{F_\ell}_{\ell}] $ where $\bbu^{f}_\ell$ is of length $N_{\ell-1}$ and is obtained through convolution and linear aggregation of features $\bbx^{g}_{\ell-1}$ of the previous layer, $g=1,\ldots,F_{\ell-1}$. Specifically, let $\bbh_{\ell}^{fg} := [\ [\bbh_{\ell}^{fg}]_{0}; \ldots; [\bbh_{\ell}^{fg}]_{K_{\ell}-1} \ ]$ be the coefficients of a $K_\ell$-tap linear time invariant filter that is used to process the $g$th feature of the $(\ell-1)$st layer to produce the intermediate feature $\bbu_{\ell}^{fg}$ at layer $\ell$. Since the filter is defined by a convolution, the components of $\bbu_{\ell}^{fg}$ are explicitly given by
% 
\begin{align}\label{eqn:conv_time}
   \Big[\bbu_{\ell}^{fg}\Big]_n 
       :=   \Big[\bbh_{\ell}^{fg} \ast \bbx_{\ell-1}^{g}\Big]_{n}  
	    =   \sum_{k=0}^{K_{\ell}-1}  \Big[ \bbh_{\ell}^{fg}  \Big]_{k}  \,
	        \Big[ \bbx_{\ell-1}^{g} \Big]_{n-k} ,
\end{align}
%
where we consider that: i) the output has the same size than the input and ii) the convolution \eqref{eqn:conv_time} is circular to account for border effects. After evaluating the convolutions in \eqref{eqn:conv_time}, the $\ell$th layer features $\bbu_{\ell}^{f}$ are computed by aggregating the intermediate features $\bbu_{\ell}^{fg}$ associated with each of the previous layer features $\bbx_{\ell-1}^{g}$ using a simple summation, 
%
\begin{equation} \label{eqn:agg_features}
   \bbu_{\ell}^{f}
       \ :=\ \sum_{g=1}^{F_{\ell-1}} \bbu_{\ell}^{fg}
	   \  =\ \sum_{g=1}^{F_{\ell-1}} \bbh_{\ell}^{fg} \ast \bbx_{\ell-1}^{g} .
\end{equation}
%
The vector $\bbu_\ell := [\bbu^{1}_{\ell}; \ldots; \bbu^{F_\ell}_{\ell}]$ obtained from \eqref{eqn:conv_time} and \eqref{eqn:agg_features} represents the output of the linear operation of the $\ell$th layer of the CNN [cf. \eqref{eqn_nn_layers}]. Although not explicitly required, the number of features $F_{\ell}$ and the number of filter taps $K_{\ell}$ are typically much smaller than the dimensionality $M_{\ell-1}$ of the features $\bbx_{\ell-1}$ that are processed by the $\ell$th layer. This reduces the number of learnable parameters from $M_{\ell}\times M_{\ell-1}$ in \eqref{eqn_nn_layers} to $K_{\ell}\times F_{\ell}\times F_{\ell-1}$ simplifying training and reducing overfitting. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%   S   E   C   T   I   O   N   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\subsection{Pooling}\label{sec_cnn_pooling}

The features $\bbu_{\ell}^{fg}$ in \eqref{eqn:conv_time} and their consolidated counterparts $\bbu_{\ell}^{f}$ in \eqref{eqn:agg_features} have $N_{\ell-1}$ components. This number of components is reduced to $N_{\ell}$ at the pooling stage in which the values of a group of neighboring elements are aggregated to a single scalar using a possibly nonlinear summarization function $\rho_\ell$. To codify the locality of $\rho_\ell$, we define, with a slight abuse of notation, $\bbn_\ell$ as a vector containing the indexes associated with index $n$ -- e.g., use $\bbn_\ell=[n-1; n; n+1]$ to group adjacent components -- and define the signal $\bbv^{f}_{\ell}$ with components
% EQN
\begin{equation}\label{eqn_group_nonlinearity}
   \Big[\bbv^{f}_{\ell}\Big]_n 
            = \rho_{\ell}\left( \Big[\bbu_{\ell}^f\Big]_{\bbn_\ell}\right).
\end{equation}
%
The summarization function $\rho_{\ell}$ in \eqref{eqn_group_nonlinearity} acts as a low-pass operation and the most common choices are the maximum $\rho_{\ell}( [\bbu_{\ell}^f]_{\bbn_\ell}) = \max( [\bbu_{\ell}^f]_{\bbn_\ell})$ and the average  $\rho_{\ell}( [\bbu_{\ell}^f]_{\bbn_\ell}) = \bbone^{\Tr} [\bbu_{\ell}^f]_{\bbn_\ell}/ |\bbn_\ell|$ \cite{wiatowski17-maththeory}.

To complete the pooling stage we follow $\eqref{eqn_group_nonlinearity}$ with a downsampling operation. For that matter, we define the sampling matrix $\bbC_{\ell}$ as a fat binary matrix with $N_{\ell-1}$ columns and $N_{\ell}$ rows, which are selected from the rows of the identity matrix. When the sampling matrix $\bbC_{\ell}$ is \textit{regular}, the nonzero entries follow the pattern $[\bbC_{\ell}]_{mn}=1$ if $n$ can be written as $n=(N_{\ell-1}/N_{\ell})m$ and zero otherwise; hence, the product $\bbC_{\ell}\bbv^{f}_{\ell}$ selects one out of every $(N_{\ell-1}/N_{\ell})$ components of $\bbv^{f}_{\ell}$. Downsampling is composed with a pointwise nonlinearity to produce the $\ell$th layer features
% 
\begin{equation}\label{eqn_downsampling}
   \bbx^{f}_{\ell} =  \sigma_\ell \left(\bbC_{\ell}\bbv^{f}_{\ell} \right).
\end{equation}
%
The compression or downsampling factor $(N_{\ell-1}/N_{\ell})$ is often matched to the local summarization function $\rho_{\ell}$ so that the set $\bbn_\ell$ contains $(N_{\ell-1}/N_{\ell})$ adjacent indexes. We further note that although we defined \eqref{eqn_group_nonlinearity} for all $n$, in practice, we only compute the components of $\bbv^{f}_{\ell}$ that are to be selected by the sampling matrix $\bbC_\ell$. In fact, it is customary to combine \eqref{eqn_group_nonlinearity} and \eqref{eqn_downsampling} to simply write $[\bbx^{f}_{\ell}]_n = \sigma_l (\rho_{\ell}( [\bbu_{\ell}^f]_{\bbn_\ell})$ for $n$ in the selection set. Separating the nonlinearity in \eqref{eqn_group_nonlinearity} from the downsampling operation in \eqref{eqn_downsampling} is convenient to elucidate pooling strategies for signals on graphs.

Equations \eqref{eqn:conv_time}-\eqref{eqn_downsampling} complete the specification of the CNN architecture. We begin at each layer with the input $\bbx_{\ell-1} := [\bbx^{1}_{\ell-1}; \ldots; \bbx^{F_{\ell-1}}_{\ell-1}]$. Features are fed to parallel convolutional channels to produce the features  $\bbu_{\ell}^{fg}$ in \eqref{eqn:conv_time} and consolidated into the features $\bbu_{\ell}^{f}$ in \eqref{eqn:agg_features}. These features are fed to the local summarization function $\rho_{\ell}$ to produce features $\bbv^{f}_{\ell}$ [cf. \eqref{eqn_group_nonlinearity}] which are then downsampled and processed by the pointwise activation nonlinearity $\sigma_{\ell}$ to produce the features $\bbx^{f}_{\ell}$ [cf. \eqref{eqn_downsampling}]. The output of the $\ell$th layer is the vector $\bbx_\ell := [\bbx^{1}_{\ell}; \ldots; \bbx^{F_\ell}_{\ell}]$ that groups the features in \eqref{eqn_downsampling}. We point out for completeness that the $L$th layer is often a fully connected layer in the mold of \eqref{eqn_nn_layers} that does not abide to the convolutional and pooling paradigm of \eqref{eqn:conv_time}-\eqref{eqn_downsampling}. Thus, the $L$th layer produces an arbitrary (non convolutional) linear combination of $F_{L-1}$ features to produce the final $F_{L}$ scalar features $\bbx_L$. The output of this readout layer provides the estimate $\hby = \bbx_L$ that is associated with the input $\hbx=\bbx_0$ fed to the first layer.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%   S   E   C   T   I   O   N   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\subsection{Signals on Graphs}\label{sec_cnn_graph signals}

There is overwhelming empirical evidence that CNNs are superb representations of signals defined in regular domains such as time series and images \cite{lecun15-deeplearning}. Our goal in this paper is to contribute to the extension of these architectures to signals supported in irregular domains described by arbitrary graphs. Consider then a weighted graph with $N$ nodes, edge set $\ccalE$ and weight function $\ccalW: \ccalE \to \reals$. We endow the graph with a shift operator $\bbS$, which is an $N\times N$ square matrix having the same sparsity pattern of the graph; i.e., we can have $[\bbS]_{mn}\neq 0$ if and only if $(n,m) \in \ccalE$ or $m=n$. The shift operator is a stand in for one of the matrix representations of the graph. Commonly used shift operators include the adjacency matrix $\bbA$ with nonzero elements $[\bbA]_{mn}=\ccalW(n,m)$ for all $(n,m)\in \ccalE$, the Laplacian $\bbL:=\diag(\bbA \bbone)-\bbA$ and their normalized counterparts $\barbA$ and $\barbL$ \cite{shuman13-mag}. 

Consider the signal $\bbx=[\bbx^1;\ldots;\bbx^F]$ formed by $F$ feature vectors $\bbx^f$ with $N$ components each. The feature vector $\bbx^f$ is said to be a graph signal when each of its $N$ components is assigned to a different vertex of the graph. The graph describes the underlying support of the data $\bbx^f$ (hence, of $\bbx$) by using the weights $\ccalW$ to encode arbitrary pairwise relationships between data elements. The graph shift enables processing of the graph signal $\bbx^f$ because it defines a local linear operation that can be applied to graph signals. Indeed, if we consider the signal $\bby^f:=\bbS\bbx^f$ it follows from the sparsity of $\bbS$ that the $n$th element of $\bby^f$ depends on the elements of $\bbx^f$ associated with neighbors of the node $n$,
% 
\begin{equation}\label{eqn_gso_is_a_shift}
   [\bby^f]_n =  \sum_{m:(m,n)\in\ccalE} [\bbS]_{nm} [\bbx^f]_m.
\end{equation}
%
It is instructive to consider the cyclic graph adjacency matrix $\bbA_{\dc}$, with nonzero elements $[\bbA_{\dc}]_{1 + n \mod N, n}=1$. Since the cyclic graph describes the structure of discrete (periodic) time, we can say that a discrete time signal $\bbx$ is a graph signal defined on the cyclic graph. When particularized to $\bbS=\bbA_{\dc}$, \eqref{eqn_gso_is_a_shift} yields ${y^f_{1+n\mod N}}=x^f_n$ implying that $\bby^f$ is a circularly time shifted copy of $\bbx^f$. This motivates interpretation of $\bbS$ as the generalization of time shifts to signals supported in the corresponding graph \cite{sandryhaila13-dspg}.

Enabling CNNs to process data modeled as graph signals entails extending the operations of convolution and pooling to handle the irregular nature of the underlying support. Convolution [cf.~\eqref{eqn:conv_time}] can be readily replaced by the use of linear, shift invariant graph filters [cf.~\eqref{eqn:conv_graph}]. The summarizing function [cf.~\eqref{eqn_group_nonlinearity}] can also be readily extended by using the notion of neighborhood defined by the underlying graph support. The pointwise nonlinearity can be kept unmodified [cf.~\eqref{eqn_downsampling}], but there are two general downsampling strategies for graph signals: selection sampling \cite{chen15-selection} and aggregation sampling \cite{marques16-aggregation}. Inspired by these, we propose two architectures: selection GNNs (Section \ref{sec:selection}) and aggregation (Section \ref{sec:aggregation}) GNNs.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%   R   E   M   A   R   K   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\begin{remark} \normalfont 
Although our current theoretical understanding of CNNs is limited, empirical evidence suggests that convolution and pooling work in tandem to act as feature extractors at different levels of resolution. At each layer, the convolution operation linearly relates up to $K_{\ell}$ nearby values of each input feature. Since the same filter taps are used to process the whole signal, the convolution finds patterns that, albeit local, are irrespective of the specific location of the pattern in the signal. The use of several features allows collection of different patterns through learning of different filters thus yielding a more expressive operation. The pooling stage summarizes information into a feature of lower dimensionality. It follows that subsequent convolutions operate on summaries of different regions. As we move into deeper layers we pool summaries of summaries that are progressively growing the region of the signal that affects a certain feature. The conjectured value of composing local convolutions with pooling summaries is adopted prima facie as we seek graph neural architectures that exploit the locality of the shift operator to generalize convolution and pooling operations.
\end{remark}
