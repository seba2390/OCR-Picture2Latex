%!TEX root = mainArchitGNN.tex

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% SECTION : Introduction  %%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We consider signals with irregular structure and describe their underlying topology with a graph whose edge weights capture a notion of expected similarity or proximity between signal components expressed at nodes \cite{sandryhaila13-dspg, sandryhaila14-freq, shuman13-mag, sandryhaila14-mag}. Of particular importance in this paper is the interpretation of matrix representations of the graph as shift operators that can be applied to graph signals. Shift operators represent local (one-hop neighborhood) operations on the graph, and allow for different generalizations of convolution, sampling and reconstruction. These generalizations stem either from representations of graph filters as polynomials in the shift operator \cite{sandryhaila13-dspg, segarra17-linear, shuman18-chebyshev} or from the aggregation of sequences generated through successive application of the shift operator \cite{marques16-aggregation}. They not only capture the intuitive idea of convolution, sampling and reconstruction as local operations but also share some other interesting theoretical properties \cite{segarra17-linear, sandryhaila13-dspg, sandryhaila14-freq}. Our goal here is to build on these definitions to generalize Convolutional (C) neural networks (NNs) to graph signals.

CNNs consist of layers that are sequentially composed, each of which is itself the composition of convolution and pooling operations (Section \ref{sec:regular} and Figure~\ref{fig:regular_cnn}). The input to a layer is a multichannel signal composed of features extracted from the previous layer, or the input signal itself at the first layer. The main step in the convolution stage is the processing of each feature with a bank of linear time invariant filters (Section \ref{sec_cnn_convolution}). To keep complexity under control and avoid the number of intermediate features growing exponentially, the outputs of some filters are merged via simple pointwise summations. In the pooling stage we begin by computing local summaries in which feature components are replaced with a summary of their values at nearby points (Sec. \ref{sec_cnn_pooling}). These summaries can be linear, e.g., a weighted average of adjacent components, or nonlinear, e.g., the maximum value among adjacent components. Pooling also involves a subsampling of the summarized outputs. This subsampling reduces dimensionality with a (small) loss of information because the summarizing function is a low-pass operation. The output of the layer is finally obtained by application of a pointwise nonlinear activation function to produce features that become an input to the next layer. This is an architecture that is both simple to implement \cite{najafabadi15-cnnbigdata}, and simple to train \cite{rumelhart86-backprop}. Most importantly, their performance in regression and classification is remarkable to the extent that CNNs have become the standard tool in machine learning to handle such inference tasks \cite{lecun15-deeplearning, lecun10-vision, greenspan16-medical}. 


As it follows from the above description, a CNN layer involves five operations: (i) Convolution with linear time invariant filters. (ii) Summation of different features. (iii) Computation of local summaries. (iv) Subsampling. (v) Activation with a pointwise nonlinearity. A graph (G)NN is an architecture adapted to graph signals that generalizes these five operations. Operations (ii) and (v) are pointwise, therefore independent of the underlying topology, so that they can be applied without modification to graph signals. Generalizing (iii) is ready because the notion of adjacent components is well defined by graph neighborhoods. Generalization of operation (i) is not difficult in the context of graph signal processing advances whereby linear time invariant filters are particular cases of linear shift invariant graph filters. This has motivated the definition of graph (G) NNs with convolutional features computed from shift invariant graph filters, an idea that was first introduced in \cite{bruna14-deepspectralnetworks} and further explored in \cite{henaff15-deepgraph, atwood16-diffusion, defferrard17-cnngraphs, du17-topoadapt, kipf17-classifgcnn, gama18-mimo}. Architectures based on receptive fields, which are different but conceptually similar to graph filters, have also been proposed \cite{niepert16-learningcnn, pasdeloup17-approxtrans, velickovic18-graphattentionnetworks}. However, generalization of operation (iv) has proven more challenging because once the signal is downsampled, it is not easy to identify a coarsened graph to connect the components of the subsampled signal. The use of multiscale hierarchical clustering has been proposed to produce a collection of smaller graphs \cite{bruna14-deepspectralnetworks, henaff15-deepgraph, defferrard17-cnngraphs} but it is not clear which clustering or coarsening criteria is appropriate for GNN architectures. The difficulty of designing and implementing proper pooling is highlighted by the fact that several works exclude the  pooling stage altogether \cite{du17-topoadapt, niepert16-learningcnn, pasdeloup17-approxtrans, gama18-nvgf}. 

In this paper we propose two different GNN architectures, selection GNNs and aggregation GNNs, that include convolutional and pooling stages but bypass the need to create a coarsened graph. In selection GNNs (Sec. \ref{sec:selection} and Fig. \ref{fig:selection_cnn}) we replace convolutions with linear shift invariant filters and replace regular sampling with graph selection sampling. In the first layer of the selection GNN, linear shift invariant filters are well defined as polynomials on the given graph. At the first pooling stage, however, we sample a smaller number of signal components and face the challenge of computing a graph to describe the topology of the subsampled signal. Our proposed strategy is to bypass the computation of a coarsened graph by using zero padding (Sec. \ref{sec_selection_convolution}). This simple technique permits computation of features that are convolutional on the input graph. The pooling stage is modified to aggregate information in multihop neighborhoods as determined by the structure of the original graph and the sparsity of the subsampled signal (Sec. \ref{sec_selection_pooling}).

In aggregation GNNs we borrow ideas from aggregation sampling \cite{marques16-aggregation} to create a signal with temporal structure that incorporates the topology of the graph (Sec. \ref{sec:aggregation} and Fig. \ref{fig_aggregation}). This can be accomplished by focusing on a designated node and considering the local sequence that is generated by subsequent applications of the graph shift operator. This is a signal with a temporal structure because it reflects the propagation of a diffusion process. Yet, it also captures the topology of the graph because subsequent components correspond to the aggregation of information in nested neighborhoods of increasing reach. Aggregation GNNs apply a regular CNN to the diffusion signal observed at the designated node. 

We finally introduce a multinode version of aggregation GNNs, where several regular CNNs are run at several designated nodes (Sec. \ref{sec_aggregation_multinode} and Fig. \ref{fig_multinode}). The resulting CNN outputs are diffused in the input graph to generate another sequence with temporal structure at a smaller subset of nodes to which regular CNNs are applied in turn. We can think of multinode aggregation GNNs as composed of inner and outer layers. Inner layers are regular CNN layers. Output layers stack CNNs joined together by a linear diffusion process. Multinode aggregation GNNs are consistently the best performing GNN architecture (Sec. \ref{sec:sims}). We remark that aggregation GNNs, as well as selection GNNs are proper generalizations of conventional CNNs because they both reduce to a CNN architecture when particularized to a cyclic graph.

The proposed architectures are applied to the problems of localizing the source of a diffusion process on synthetic networks (Sec.~\ref{subsec_sourceloc}) as well as on real-world social networks (Sec.~\ref{subsec_fb}). Performance is additionally evaluated on problems of authorship attribution (Sec.~\ref{subsec_author}) and classification of articles of the \texttt{20NEWS} dataset (Sec.~\ref{subsec_20news}), involving real datasets. Results are compared to those obtained from a graph coarsening architecture using a multiscale hierarchical clustering scheme \cite{defferrard17-cnngraphs}. The results are encouraging and show that the multinode approach consistently outperforms the other architectures.

\noindent \emph{Notation:} The $n$-th component of a vector $\bbx$ is denoted as $[\bbx]_{n}$. The $(m,n)$ entry of a matrix $\bbX$ is $[\bbX]_{mn}$. The vector $\bbx:=[\bbx_1; \ldots; \bbx_n]$ is a column vector stacking the column vectors $\bbx_n$. When $\bbn$ denotes a set of subindices, $|\bbn|$ is the number of elements in $\bbn$ and  $[\bbx]_{\bbn}$ denotes the column vector formed by the elements of $\bbx$ whose subindices are in $\bbn$. The vector $\bbone$ is the all-ones vector. 
