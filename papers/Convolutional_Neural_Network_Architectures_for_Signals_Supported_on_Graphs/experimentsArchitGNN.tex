%!TEX root = mainArchitGNN.tex

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% SECTION : Simulations   %%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We test the proposed GNN architectures and compare their performance with the graph coarsening (multiscale hierarchical clustering) approach of \cite{defferrard17-cnngraphs}. In the first scenario (Sec.~\ref{subsec_sourceloc}), we address the problem of source localization on synthetic stochastic block model (SBM) networks. Then, we move the source localization problem to a more realistic setting of a Facebook network of $234$ users (Sec.~\ref{subsec_fb}). As a third experiment, we address the problem of authorship attribution (Sec.~\ref{subsec_author}). And finally, we test the proposed architectures in the problem of text categorization on the \texttt{20NEWS} dataset (Sec.~\ref{subsec_20news}).

We test the proposed Selection (Sec.~\ref{sec:selection}), Aggregation (Sec.~\ref{sec:aggregation}) and Multinode (Sec.~\ref{sec_aggregation_multinode}) GNN architectures. For the selection of nodes involved in each of the architectures, we test three different strategies. First, we choose nodes based on their degree; second, we select them following the leverage scores proposed by the experimentally designed sampling (EDS) in \cite{varma15-scores}; and third, we determine the appropriate nodes by using the spectral-proxies approach (SP) in \cite{anis16-spectralproxies}.  In all architectures, the last layer is a fully-connected readout layer, followed by a softmax, to perform classification.

Unless otherwise specified, all GNNs were trained using the ADAM optimizer \cite{kingma17-adam} with learning rate $0.001$ and forgetting factors $\beta_{1}=0.9$ and $\beta_{2}=0.999$. The training phase is carried out for $40$ epochs with batches of $100$ training samples. The loss function considered in all cases is the cross-entropy loss between one-hot target vectors and the output from the last layer of each architecture, interpreted as probabilities of belonging to each class. Also, in all cases, we consider max-pooling summarizing functions and ReLU activation functions for the corresponding GNN layers.

\begin{table}
	\centering
\begin{tabular}{lc} \hline
Architecture 				& Accuracy 				\\ \hline
Selection (S) Degree		& $86.9 (\pm 5.9) \%$	\\
Selection (S) EDS			& $90.0 (\pm 4.6) \%$	\\
Selection (S) SP			& $91.1 (\pm 4.7) \%$	\\
Aggregation	(A)	Degree		& $94.2 (\pm 4.7) \%$	\\
Aggregation	(A)	EDS			& $96.5 (\pm 3.1) \%$	\\
Aggregation	(A)	SP			& $95.2 (\pm 4.4) \%$	\\
Multinode (MN) Degree		& $96.1 (\pm 3.4) \%$	\\
Multinode (MN) EDS			& $96.0 (\pm 3.5) \%$	\\
\textbf{Multinode (MN) SP}			& $\mathbf{97.3 (\pm 2.7) \%}$	\\
Graph Coarsening (C) Clustering		& $87.4 (\pm 3.2)\%$ \\ \hline
\end{tabular}
	\caption{Considering that SBM graphs are random, we generate $10$ different instances of SBM graphs with $N=100$ nodes and $C=5$ communities of $20$ nodes each. For each of these $10$ graphs, we randomly generate $10$ different datasets (training, validation and test set). We compute the classification accuracy of each realization, and average across all $10$ realizations for each graph, obtaining $10$ average classification accuracies. In the table we show the classification accuracy, averaged across the $10$ graph instances. The standard deviation from these $10$ graphs is also shown.}
	\label{table_sourceloc}
\end{table}

%%%%%%%%%%%%%%%%%%%
%%% SUBSECTION
%%%	subsec_source
%%%%%%%%%%%%%%%%%%%

\subsection{Source Localization} \label{subsec_sourceloc}

Consider a connected stochastic block model (SBM) network with $N=100$ nodes and $C=5$ communities of $20$ nodes each \cite{decelle11-sbm}. In SBM graphs, edges are randomly drawn between nodes within the same community, independently, with probability $0.8$; while edges are randomly drawn between nodes of different communities, independently, with probability $0.2$. Denote by $\bbA$ the adjacency matrix of such graph.

%%%%%%%%%%%%%%%%%%%
%%% FIGURE
%%%	fig:aggregation
%%%%%%%%%%%%%%%%%%%
%%
%%
\begin{figure*}
\centering
\begin{subfigure}{.33\textwidth}
  \centering
  \includegraphics[width=0.95\textwidth]{pooling_sourceloc_22_matfiles-20181008-sgnn_sp}
  \caption{Selection GNN SP}
  \label{fig:a}
\end{subfigure}%
\hfill
\begin{subfigure}{.33\textwidth}
  \centering
  \includegraphics[width=0.95\textwidth]{pooling_sourceloc_22_matfiles-20181008-agnn_eds}
  \caption{Aggregation GNN EDS}
  \label{fig:mn_P}
\end{subfigure}%
\hfill
\begin{subfigure}{.33\textwidth}
  \centering
  \includegraphics[width=0.95\textwidth]{pooling_sourceloc_22_matfiles-20181008-mngnn_sp}
  \caption{Multinode GNN SP}
  \label{fig:mn_Q}
\end{subfigure}%
\caption{Validation and training loss during training stage. We observe that the validation loss and the training loss are essentially equal throughout the training stage for all three architectures. This shows that the proposed models are not overfitting the data, since the validation loss keeps decreasing with the training steps The best performing selection method of each architecture is represented.}
\label{fig:validation}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In the problem of source localization, we observe a signal that has been diffused over the graph and estimate the spatial origin of such diffused process. More precisely, consider $\bbdelta_{c}$ a graph signal that has a $1$ at node $c$ and $0$ at every other node. Define $\bbx = \bbA^{t} \bbdelta_{c}$ as the diffused graph signal, for some unknown $t \geq 0$. The objective is to estimate the origin $c$ of the diffusion. In this situation in particular, we are interested in estimating the \emph{community} $c$ (rather than the node) that originated the observed signal $\bbx$. We can thus model this scenario as a classification problem in which we observe graph signal $\bbx$ and have to assign it to one of the $C=5$ communities.

In the simulations, we generate the training and test set by randomly selecting the origin $c$ from a pool of $C=5$ nodes (the largest-degree node of each community; recall that all nodes have, on average, the same degree) and randomly selecting the diffusion time $t<25$, as well. We generate a training set of $10,000$ signals and a test set of $200$ signals. The training set is further split in $2,000$ signals for validation, and the rest for training. We simulate $10$ graphs, and for each graph, we simulate $10$ realizations of training and test sets. For numerical reasons, the adopted graph shift operator is $\bbS = \bbA/\lambda_{\max}$ where $\lambda_{\max}$ is the maximum eigenvalue of $\bbA$.

The architectures tested are as follows. For the selection GNN we consider two layers selecting $10$ nodes in each. The number of output features in each layer are $F_{1} = F_{2} = 32$ and the filters consists of $K_{1} = K_{2} = 5$ taps [cf. \eqref{eqn_conv_features_unpadded}]. For the summarizing functions, we consider neighborhoods of size $\alpha_{1} = 6$ and $\alpha_{2} = 8$, respectively [cf. \eqref{eqn_graph_neigborhood}]. In the aggregation GNN, we select the single node with highest: a) degree, b) EDS leverage score, or d) spectral-proxies (SP) norm, depending on the strategy chosen. Then, we construct the regular-structured signal [cf. \eqref{eqn:agg_representation}] and apply the aggregation GNN with two layers. The number of features on each layer is $F_{1}=16$ and $F_{2} = 32$, with filters of size $K_{1} = 4$ and $K_{2}=8$ [cf. \eqref{eqn_conv_agg_layer_1_shift_explicit}]. Max-pooling is applied to reduce the size of the regular signal by half on each layer, and the nonlinearity used is the ReLU. Finally, for the multinode GNN, we consider two outer layers selecting $P_{1}=10$ and $P_{2}=5$ nodes and shifting the signal $Q_{1}=7$ and $Q_{2}=5$ times to build the regular signal on each node [cf. \eqref{eqn:multinode_agg_1}]. Then, for each outer layer, we apply two inner layers. In the first one, we obtain $16$ features at each inner layer; and in the second outer layer, we get $16$ and $32$ for each inner layer. In all inner layers, the filters are of size $3$, with max-pooling by $2$, and a ReLU nonlinearity. We recall that the selection of nodes depends on the sampling strategy selected (degree, EDS or SP). We compare against a two-layer architecture using graph coarsening \cite{defferrard17-cnngraphs}, reducing the number of nodes to a half on each layer, computing $F_{1}=F_{2}=32$ features with filters consisting of $K_{1}=K_{2}=5$ filter taps. In contrast with the previous cases where $\bbS$ was set to the rescaled adjancency matrix, in the graph coarsening architecture we set $\bbS$ to normalized Laplacian, since that was the specification in \cite{defferrard17-cnngraphs} and, more importantly, yields a better performance.

The plots in Fig.~\ref{fig:validation} show the value of the loss function on the training and validation sets as the training stage progresses. We observe that both drop with training, showing that the model is effectively learning from data. We see that it takes some time for the models to start learning (reaching half of the training stage in the case of aggregation), but then effectively lower the training loss. We also see that the Multinode GNN achieves a lower loss value, which translates in better performance on the test set, and that it also takes the least number of training steps before starting to lower the loss function. Finally, we note that the validation loss and the training loss are essentially the same, showing that there is no overfit in the models.

Accuracy results on the test set are presented in Table \ref{table_sourceloc}. The accuracy results for all $10$ realizations of each graph are averaged, and then all $10$ graph mean accuracies are averaged to obtain the values shown in Table \ref{table_sourceloc}. The error values in the table are the square root of the variance computed across the means obtained for each of the $10$ graphs. We observe that the best performance is achieved by Multinode GNN with nodes chosen following the spectral proxies method. We observe that all multinode and aggregation GNNs outperform the graph coarsening approach, and so do selection GNNs following EDS and spectral proxies sampling.

%%%%%%%%%%%%%%%%%%%
%%% SUBSECTION
%%%	subsec_fb
%%%%%%%%%%%%%%%%%%%

\subsection{Facebook network} \label{subsec_fb}

For this second experiment, we also consider the source localization problem, but in this case, we test it on top of a real-world network. In particular, we built a 234-user Facebook network as the largest connected network within the larger dataset provided in \cite{mcauley12-fb}. We observe that the resulting network exhibits two communities of quite different size. The source localization problem formulation is the same than the one described in the previous section, where the objective is to identify which of the two communities originated the diffusion process. This is analogous to finding the start of a rumor. Again, we set $\bbS = \bbA/\lambda_{\max}$. The datasets are generated in the same fashion as described in the previous section.

The three architectures used are as follows. For the selection GNN we use two layers, choosing $10$ nodes after the first one, and use filters with $K_{1}=K_{2}=5$ taps that generate $F_{1}=F_{2}=32$ features on each layer. For the pooling stage, we use a $\max\{\cdot\}$ summarization with $\alpha_{1}=2$ and $\alpha_{2}=4$. In the aggregation GNN we select the best node based on one of the three sampling strategies (degree, EDS and SP) and the gather the regular-structure data at that node. We then process it with a two-layer CNN that generates $F_{1}=32$ and $F_{2}=64$ features, using $K_{1}=K_{2}=4$. Max-pooling of size $2$ is used on each layer (i.e. half of the samples gathered at the node are kept after each layer). In the case of the multinode GNN we use two-outer layers, selecting $P_{1}=30$ and $P_{2}=10$ nodes on each, and gathering $Q_{1}=Q_{2}=5$ shifted versions of the signal at each node. Then, for the inner layers, we use two-layer architectures that generate $16$ features on each layer in the first outer layer, and $16$ and $32$ features on each layer in the second outer layer. In all cases, we use filters of size $3$ and max-pooling by a factor of $2$. Finally, for the graph coarsening architecture, we adopt the normalized Laplacian as GSO, as described in \cite{defferrard17-cnngraphs}, and use two-layers computing $F_{1}=F_{2}=32$ features using graph filters with $K_{1}=K_{2}=5$ filter taps. After each layer, the number of nodes is reduced by half.

\begin{table}
	\centering
\begin{tabular}{lc} \hline
Architecture 				& Accuracy 				\\ \hline
Selection (S) Degree		& $96.0 (\pm 1.5) \%$	\\
Selection (S) EDS			& $95.6 (\pm 1.0) \%$	\\
Selection (S) SP			& $97.6 (\pm 1.2) \%$	\\
Aggregation	(A)	Degree		& $95.8 (\pm 1.6) \%$	\\
Aggregation	(A)	EDS			& $96.9 (\pm 1.2) \%$	\\
Aggregation	(A)	SP			& $95.8 (\pm 1.4) \%$	\\
Multinode (MN) Degree		& $97.6 (\pm 1.3) \%$	\\
Multinode (MN) EDS			& $96.8 (\pm 1.2) \%$	\\
\textbf{Multinode (MN) SP}			& $\mathbf{99.0 (\pm 0.8) \%}$	\\
Graph Coarsening (C) Clustering		& $95.2 (\pm 1.2)\%$ \\ \hline
\end{tabular}
	\caption{Classification accuracy averaged across $10$ different realizations of the training and test sets for the same underlying graph. In parenthesis, we show the standard deviation of the classification accuracy.}
	\label{table_fb}
\end{table}

For training we use $80$ epochs. We also generate $10$ different random realizations of the dataset to account for random variabilities in the setting. Results for all ten architectures are shown in Table \ref{table_fb}. We observe that all architectures achieve a very high classification accuracy. We note that selection GNN tends to outperform aggregation GNN. The best result is obtained for multinode GNN using spectral proxies and is $99.0\%$ classification accuracy.

%%%%%%%%%%%%%%%%%%%
%%% SUBSECTION
%%%	subsec_author
%%%%%%%%%%%%%%%%%%%

\subsection{Authorship attribution} \label{subsec_author}

As a third experiment, we study the problem of authorship attribution, as detailed in \cite{segarra15-wans}. We consider excerpts of works written by a myriad of contemporary authors from the 19th century. We then build a word adjacency network (WAN) using functional words in these excerpts, and obtain a graph profile for each author, i.e., a graph that represents an author's writing style by the way functional words (who act as nodes) are linked (weighted edges) in the excerpts written; see \cite{segarra15-wans} for a full detail on the authors considered and the specific construction of WANs. Then, we take a new excerpt, of unknown authorship, and by looking at the frequency of the functional words, we want to determine who the author is. In the framework presented in this paper, the signature word adjacency network constitutes the underlying graph support, and the frequency count of functional words becomes the graph signal.

In particular, we focus on texts authored by Emily Bront{\"{e}}. We consider a corpus of $682$ excerpts of around $1000$ words, authored by her; and take into consideration $211$ functional words. Then, we take $546$ of these excerpts as a training set, in order to both, build the signature WAN, and also as training samples. The constructed graph consists of $N=211$ nodes, one for each functional word, the edges and their weights are determined by the precedence relationship between each word, as described in \cite{segarra15-wans}; and each training sample consist of a graph signal, where the value associated to each node is the frequency count of that specific functional word. The remaining $136$ excerpts are used as test samples. Once the signature WAN for Bront\"{e} is built, we construct a training set of $1092$ text excerpts, $546$ corresponding to the author, and $546$ corresponding to other contemporary authors; and a test set of $272$ excerpts, $136$ belonging to Bront\"{e}, and $136$ written by other authors. The excerpts corresponding to the training and test set, written by either Bront\"{e} or other contemporary authors, are chosen uniformly at random. The objective is to decide if the excerpts in the test set were written by Bront\"{e}.

Again, we consider the three GNN architectures proposed in this paper, as well as the graph coarsening GNN of \cite{defferrard17-cnngraphs}. For the selection GNN, we consider a two-layer architecture, where we choose $100$ nodes (functional words) as determined by each of the three sampling strategies (degree, EDS and SP). For each layer we set $F_{1}=F_{2}=32$, $K_{1}=K_{2}=5$ and $\alpha_{1}=2$ and $\alpha_{2}=4$. In the aggregation GNN we consider three layers, after aggregating all the information at the chosen node (the choice depends on each sampling strategy). In the first layer we compute $F_{1}=32$ features with a filter of size $K_{1}=6$, and do max-pooling, reducing the number of samples by $4$. The second and third layers use filters of size $K_{2}=K_{3}=4$ to obtain $F_{2}=64$ and $F_{3}=128$ features respectively. Pooling is applied, reducing the size of the vector by a factor of $2$ in each of the last two aggregation GNN layers. The multinode GNN employed consists of two outer layers, choosing $P_{1}=30$ and $P_{2}=10$ nodes, respectively, and aggregating information up to the $Q_{1}=Q_{2}=5$ hop-neighborhood. For each outer layer, we have two inner layers, having $16$ features on each of those for the first outer layer, and $16$ and $32$ features for the second outer layer. All filters are of size $3$ and pooling reduces the size of the vectors by half. Finally, the graph coarsening GNN consists of two layers obtaining $F_{1}=F_{2}=32$ features in each, with graph filters of size $K_{1}=K_{2}=5$, and pooling reducing the size of the graph by half on each layer.

\begin{table}
	\centering
\begin{tabular}{lc} \hline
Architecture 				& Accuracy 				\\ \hline
Selection (S) Degree		& $69.6 (\pm 5.6) \%$	\\
Selection (S) EDS			& $68.1 (\pm 5.3) \%$	\\
Selection (S) SP			& $73.0 (\pm 4.8) \%$	\\
Aggregation	(A)	Degree		& $69.5 (\pm 2.0) \%$	\\
Aggregation	(A)	EDS			& $71.0 (\pm 2.8) \%$	\\
Aggregation	(A)	SP			& $69.2 (\pm 4.0) \%$	\\
Multinode (MN) Degree		& $80.4 (\pm 2.0) \%$	\\
\textbf{Multinode (MN) EDS}			& $\mathbf{80.5 (\pm 2.6) \%}$	\\
Multinode (MN) SP			& $79.9 (\pm 2.8) \%$	\\
Graph Coarsening (C) Clustering		& $65.2 (\pm 5.0)\%$ \\ \hline
\end{tabular}
	\caption{Classification accuracy averaged across $10$ different realizations of the training and test sets (recall that the training and test sets are chosen at random from the available corpus, and the choice of training set affects the constructed underlying graph). In parenthesis, we show the standard deviation of the classification accuracy.}
	\label{table_author}
\end{table}

The graph shift operator $\bbS$ is set to the adjacency matrix after normalizing the weights of each row (to add up to $1$) and symmetrizing it, except for the case of graph coarsening GNNs, where the GSO is the normalized Laplacian obtained from the aforementioned adjacency matrix. For training we use $80$ epochs. And we run the experiment $10$ times, to account for the randomness in the selection of training and test sets (and thus, for the randomness in the creation of the underlying WAN).

Results can be found in Table \ref{table_author}, where we show the classification accuracy averaged over $10$ different realizations of the training and test sets, as well as the estimated standard deviation. We first observe that, in this case, all proposed GNN architectures outperform the graph coarsening GNN. We note that the multinode GNN is the best performing architecture. We also observe that selecting nodes via the EDS sampling method works best for aggregation and multinode GNNs, but spectral proxies yield better results in the case of selection GNN. The best classification accuracy obtained is $80.5\%$, on average across all realizations, and achieved by the multinode GNN whose nodes are selected by means of EDS sampling.

%%%%%%%%%%%%%%%%%%%
%%% SUBSECTION
%%%	subsec_20news
%%%%%%%%%%%%%%%%%%%

\subsection{\texttt{20NEWS} dataset} \label{subsec_20news}

\begin{table}
	\centering
\begin{tabular}{lc} \hline
Architecture 				& Accuracy 				\\ \hline
Selection (S) Degree		& $55.7 (\pm 0.5) \%$	\\
Selection (S) EDS			& $58.1 (\pm 0.5) \%$	\\
Selection (S) SP			& $59.2 (\pm 0.4) \%$	\\
Aggregation	(A)	Degree		& $49.0 (\pm 0.4) \%$	\\
Aggregation	(A)	EDS			& $51.3 (\pm 0.5) \%$	\\
Aggregation	(A)	SP			& $52.9 (\pm 0.5) \%$	\\
Multinode (MN) Degree		& $65.7 (\pm 0.4) \%$	\\
Multinode (MN) EDS			& $66.5 (\pm 0.5) \%$	\\
\textbf{Multinode (MN) SP}			& $\mathbf{67.0 (\pm 0.5) \%}$	\\
Graph Coarsening (C) Clustering		& $62.8 (\pm 0.5)\%$ \\ \hline
\end{tabular}
	\caption{\texttt{20NEWS} dataset on a \texttt{word2vec} graph embedding of $N=1,000$ nodes. Classification accuracy averaged across $10$ different runs. In parenthesis, we show the standard deviation of the classification accuracy.}
	\label{table_20news}
\end{table}

Finally, we consider the classification of articles in the \texttt{20NEWS} dataset which consists of $16,617$ texts ($9,922$ of which are used for training and $6,695$ for testing) \cite{joachims96-20news}. The graph signals are constructed as in \cite{defferrard17-cnngraphs}: each document $x$ is represented using a normalized bag-of-words model and the underlying graph support is constructed using a $16$-NN graph on the \texttt{word2vec} embedding \cite{mikolov13-word2vec} considering the $1,000$ most common words. The GSO adopted is the normalized Laplacian as in \cite{defferrard17-cnngraphs}.

The selection GNN architecture consists of $2$ convolutional layers, selecting $P_{1}=250$ and $P_{2} = 100$ nodes, according to each of the three different sampling strategies. Each layer uses graph filters of $K_{1}=K_{2}=5$ taps to build $F_{1}=32$ and $F_{2}=64$ features. The pooling neighborhoods correspond to $\alpha_{1}=7$ and $\alpha_{2}=12$. For the aggregation GNN we also consider $2$ layers, and use filters of length $K_{1}=K_{2}=11$ to build $F_{1}=F_{2}=32$ features on each layer. Pooling size is $4$, and the data is aggregated at a single node chosen by each of the sampling strategies. The multinode GNN consists of $2$ outer layers that select $P_{1}=70$ and $P_{2}=30$ nodes, respectively. The number of local exchanges to create a temporally-structured signal are $Q_{1} = 10$ and $Q_{2}=25$. Each outer layer employs a regular CNN with $2$ inner layers. Each inner layer of the first outer layer creates $16$ features, while each inner layer of the second outer layer uses $16$ and $32$ features, respectively. All filters involved are of length $5$ and the pooling size is $4$. Finally, for the graph coarsening architecture, we consider $2$ layers, reducing the number of nodes by half on each layer, and computing $F_{1}=32$ and $F_{2}=64$ features, using filters of length $K_{1}=K_{2}=5$.

Training is done for $80$ epochs. Classification accuracy results, averaged out of $10$ runs, are listed in Table~\ref{table_20news}. We note that the multinode GNN is the best performing architecture, followed by graph coarsening. The comparatively poor performance of the aggregation GNN is most likely due to the numerical instabilities that arise from performing a large number of neighborhood exchanges.
