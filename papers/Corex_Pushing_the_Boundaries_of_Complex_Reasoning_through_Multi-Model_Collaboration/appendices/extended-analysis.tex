\section{Extended Results and Further Analysis}

\subsection{Evaluations on Semi-structured Reasoning}
\label{appendix:tables}

As is mentioned in section~\ref{sec:main-results},
due to the constraint of context length,
we conduct experiments on the TAT-QA dataset with \turbol along with other two tasks.

\input{appendices/appendices-tables/qa-16k-tables}

\subsection{Further Cost-effectiveness Analysis}
\label{appendix:cost-effectiveness}

For all computations related to cost-effectiveness, 
we define cost as the sum of: tokens of few-shot exemplars, query tokens, and output tokens.
Here we conduct additional cost-effectiveness analysis on the ARC-c dataset and Penguins dataset, as shown in Figure~\ref{fig:cost-eff-arc} and Figure~\ref{fig:cost-eff-task}. 
The x-axis represents the computational costs, 
calculated in terms of input/output tokens, 
while the size of each dot is proportional to the avg. number of inferences by each method.

\begin{figure}[ht]
    \centering
    \begin{minipage}[t]{0.48\textwidth}
        \includegraphics[width=\linewidth]{appendices/figures/cost-effective-arc-c.pdf}
        \caption{Cost-effectiveness analysis for ARC-c dataset.}
        \label{fig:cost-eff-arc}
    \end{minipage}
    \hfill
    \begin{minipage}[t]{0.48\textwidth}
        \includegraphics[width=\linewidth]{figures/analysis/cost-effective-penguins.pdf}
        \caption{Cost-effectiveness analysis for Penguins in a Table.}
        \label{fig:cost-eff-task}
    \end{minipage}
\end{figure}

% \subsection{Comparative Study of Retrieve Mode and Self-Consistency Decoding.}
\subsection{Annotation Efficiency Analysis}
\label{appendix:ann-eff}

Due to the scarcity and instability of curated prompts~\citep{ye2022the},
we aim to mitigate reliance on them through multi-model collaboration. 
We conduct experiments in scenarios with varying numbers of demonstrations to assess the effectiveness of our approach.


\begin{figure}[ht]
    \centering
    \begin{minipage}[t]{0.48\textwidth}
        \includegraphics[width=\linewidth]{appendices/figures/review-pal-few-shot.pdf}
        \caption{Few-shot performance of Review mode.}
        \label{fig:review-fewshot-pal}
    \end{minipage}
    \hfill
    \begin{minipage}[t]{0.48\textwidth}
        \includegraphics[width=\linewidth]{appendices/figures/retrieve-few-shot.pdf}
        \caption{Few-shot performance of Retrieve mode.}
        \label{fig:retrieve-fewshot}
    \end{minipage}
\end{figure}


As depicted in Figure~\ref{fig:review-fewshot-pal} and Figure~\ref{fig:retrieve-fewshot},
we demonstrate the performance of the Review mode and Retrieve mode under different shots.
The results reveal a marginal impact of the number of few-shot examples on performance, 
underscoring the annotation efficiency of our approach.


% Self-Consistency decoding currently serves as the prevailing strong baseline.
% Here we analyze the performance gains of the Retrieve Mode in comparison to it,
% under the condition of an equal number of inferences.
% \sqs{drawback indicated in section~\ref{section:retrieve}}

