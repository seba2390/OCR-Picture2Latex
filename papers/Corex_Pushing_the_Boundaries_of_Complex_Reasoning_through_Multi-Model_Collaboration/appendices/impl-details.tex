\section{Implementation Details}
\label{appendix:impl-details}

\paragraph{LLMs Settings and Prompts.} 

We access OpenAI and Anthropic models through their respective paid APIs. 
Specifically, 
we utilize two versions of the OpenAI models: \turbo and \gpt, 
as well as \claude from Anthropic.
Following the settings of previous works,
the temperature is set as 0 during the generation process.

As for the prompts,
we maintain the same number of exemplars per task as established by previous studies.
The details and examples of prompts are listed in Appendix~\ref{appendix:prompts}.
Regarding the complexity-based prompting baseline, 
we directly utilize the prompts provided by \citet{fu2023cothub}.

\paragraph{Evaluation.} 
We use accuracy to evaluate the tasks of arithmetic reasoning, commonsense reasoning, and Symbolic reasoning.
For semi-structured understanding,
we employ the official evaluation scripts released along with the literature for FinQA~\citep{chen2021finqa} and ConvFinQA~\citep{chen2022convfinqa},
as well as employing the same test split as in~\citet{chen2022program}.
For TAT-QA~\citep{zhu2021tatqa},
we utilize the arithmetic part to enable the simultaneous use of both CoT-based methods and PAL.
Among all the evaluations involve \crd,
the upper bound of rounds is set as 5.

In the analysis part,
due to the rate limits imposed and a restricted budget, 
we set an upper limit for our sample size. 
Consequently, 
each analysis is based on a maximum of 500 samples per run. 

We draw upon the design of the self-consistency baseline as delineated by \citet{wu2023openicl},
and partially refer to the benchmarking provided by \citet{wang2023hugnlp}.

\paragraph{PAL Details.}

We use Python 3.9.12 to execute the generated program by PAL in Review mode,
following the similar settings from \citet{gao2022pal} and \citet{chen2022program}.
Unlike \codex~\citep{chen2021evaluating} employed in these works,
\turbon is not optimized for generating programs, 
so we might anticipate a compromise in performance when using \turbon as the backbone model.