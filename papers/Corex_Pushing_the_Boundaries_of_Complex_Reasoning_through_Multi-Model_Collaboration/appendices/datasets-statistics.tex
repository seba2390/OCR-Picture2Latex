\section{Statistics and Details of Datasets}
\label{appendix:datasets}

% We evaluate CoK on 12 publicly available benchmark datasets that cover arithmetic reasoning, commonsense reasoning, symbolic reasoning and natural language understanding tasks. 
% The statistics of the datasets are shown below

% \begin{table}[htb]
% \centering
% \caption{Dataset Descriptions.}
% \setlength{\tabcolsep}{13.6pt}
% \label{tab:dataset_stats}
% \resizebox{\linewidth}{!}{
% \begin{tabular}{lcccccc}
% \toprule
% Dataset & Number of samples &Average words &Answer Format & Licence\\
% \midrule
% {CommonSenseQA} & 1,221 & 27.8 & Multi-choice & Unspecified \\
% {StrategyQA} & 2,290 & 9.6  & Yes or No  & Apache-2.0 \\
% {OpenBookQA} & 500  & 27.6  & Multi-choice & Unspecified  \\
% {ARC-c} & 1,172  & 47.5  & Multi-choice  & CC BY SA-4.0  \\
% {BoolQ} &  3,270 & 8.7 & Yes or No  &  CC BY SA-3.0 \\
% {GSM8K}  & 1,319 & 46.9 & Number & MIT License \\
% {SVAMP}  & 1,000 & 31.8 & Number & MIT License \\
% {AQuA} & 254  & 51.9 & Multi-choice  &  Apache-2.0 \\
% {MultiArith} & 600  & 31.8 & Number & CC BY SA-4.0  \\
% {GSM-Hard} & 1319  & 46.9 & Number & Apache-2.0  \\
% \bottomrule
% \end{tabular}
% }
% \end{table}

The detailed information of each dataset is shown in the follow:

\paragraph{Arithmetic reasoning}
\begin{itemize}
    % \item Last Letters \& Coin Flip
    % \citep{Wei2022Chain} are novel benchmarks to evaluate whether the LLM can solve a simple symbolic reasoning problem.
    % The last letters dataset is from \url{https://huggingface.co/datasets/ChilleD/LastLetterConcat}. The coin flip dataset is from \url{https://huggingface.co/datasets/skrishna/coin_flip}.

    \item Grade School Math (GSM8K; \citealp{cobbe2021gsm8k}): Linguistically diverse grade school math word problems created by human problem writers. The problems take between 2 and 8 steps to solve and involve elementary calculations using basic arithmetic operations.
    % \url{https://github.com/openai/grade-school-math}.
    % MIT license: \url{https://github.com/openai/grade-school-math/blob/master/LICENSE}.

    % \item Math Word Problem Repository 
    % MultiArith \citep{roy2015multiarith}.
    % % license: CC BY 4.0, dataset: \url{https://huggingface.co/datasets/ChilleD/MultiArith}.

    \item AddSub~\citep{hosseini2014addsub}: A set of simple arithmetic word problems.

    \item SVAMP \citep{patel2021svamp}: A challenge set for elementary-level Math Word Problems.
    % MIT license: \url{https://github.com/arkilpatel/SVAMP/blob/main/LICENSE}.
    
    % \item AQuA \citep{ling2017aqua}: \url{https://github.com/deepmind/AQuA}.
    % license: \url{https://github.com/deepmind/AQuA/blob/master/LICENSE}.

    \item SingleOP, SingleEQ and MultiArith~\citep{koncel2016mawps}: Grade-school math dataset that aims at solving multi-sentence algebraic word problems.

    \item GSM-Hard~\citep{gao2022pal}: A harder version of the GSM8K dataset, constructed by replacing the numbers in the questions of GSM8K with larger numbers.
    
\end{itemize}

\input{appendices/appendices-tables/math-table}

\paragraph{Commonsense \& Factual reasoning}
\begin{itemize}
    \item CommonsenseQA (CSQA; \citealp{talmor2019commonsenseqa}):
    CSQA is a multiple-choice question answering task. It requires complex semantic reasoning based on prior commonsense knowledge to answer the questions.

    % The homepage is \url{https://www.tau-nlp.org/commonsenseqa}, and \url{https://github.com/jonathanherzig/commonsenseqa}.
    
    \item StrategyQA \citep{geva2021strategyqa}: It is a commonsense QA task with Yes or No answer format that requires models to perform multi-hop reasoning to answer the questions. We use the open-domain setting (question-only set) from~\citet{srivastava2023bb}.
    % \url{https://github.com/google/BIG-bench/tree/main/bigbench/benchmark_tasks/strategyqa}.
    % The original dataset is from \url{https://github.com/eladsegal/strategyqa}, MIT license: \url{https://github.com/eladsegal/strategyqa/blob/main/LICENSE}.

    \item OpenBookQA
    \citep{mihaylov2018openbookqa}: It is a multi-choice QA task to evaluate commonsense knowledge and promote reasoning over a fixed collection of knowledge. 
    % The original dataset is from \url{https://allenai.org/data/open-book-qa}.

    % \item OpenBookQA (\citealp{mihaylov2018openbookqa}): OpenBookQA is a dataset designed for question answering tasks where the models are required to utilize a set of open-source "science facts" to answer multiple-choice questions, promoting reasoning over a fixed collection of knowledge.

    \item ARC-c
    \citep{clark2018think}: A subset of the AI2 Reasoning Challenge, consisting of challenging science questions that require reasoning and a wide breadth of knowledge to answer the multiple-choice problems correctly.
    The original dataset is from \url{https://allenai.org/data/arc}. 
    % CC BY SA-4.0 license: \url{https://creativecommons.org/licenses/by-sa/4.0/}.

    % ARC-c is a subset of the ARC dataset consisting of challenging science questions that require deep reasoning and a wide breadth of knowledge to answer correctly. It aims to test the limits of machine comprehension and reasoning capabilities in the context of educational material.
    
    \item BoolQ
    \citep{clark2019boolq}: It is a knowledge-intensive task and the format is ``Yes'' or ``No''.
    Problems are extracted from real-world internet queries,
    aiming to foster models capable of contextual understanding to provide binary answers.
    % The original dataset is from \url{https://github.com/google-research-datasets/boolean-questions}. CC BY SA-3.0 license: \url{https://creativecommons.org/licenses/by-sa/3.0/}.

\end{itemize}

\input{appendices/appendices-tables/qa-table}

\paragraph{Symbolic Reasoning}

We select the following tasks from BIG-Bench~\citep{srivastava2023bb} and BIG-Bench Hard (BBH)~\citep{suzgun2023bbh}, with Apache License v.2: \url{https://github.com/google/BIG-bench/blob/main/LICENSE}.

\begin{itemize}
    % \item Sports understanding: Determine whether a factitious sentence related to sports is plausible. The answer format is Yes or No.
    \item Date Understanding: A temporal reasoning task. Given a set of sentences about a particular date,
    answer the provided question in MM/DD/YYYY format.
    \item Object Counting: Given a collection of possessions that a person has along with their quantities (e.g., three pianos, two strawberries, one table, and two watermelons), determine the number of a certain object/item class (e.g., fruits).
    \item Penguins in a Table: Given a unique table of penguins (and sometimes some new information), 
    answer a question about the attributes of the penguins.
    \item Reasoning about Colored Objects: Given a context, 
    answer a simple question about the color of an object on a surface.
    \item Repeat Copy: Evaluate LLMs' capability to follow basic natural-language instructions nested within each example's input.

\end{itemize}

\input{appendices/appendices-tables/bbh-table}

\paragraph{Semi-structured Understanding}

\begin{itemize}
    \item FinQA~\citep{chen2021finqa}: Question-Answering pairs over financial reports written by experts, which includes financial QA pairs.
    \item ConvFinQA~\citep{chen2022convfinqa}: A financial-related dataset designed to study the chain of numerical reasoning in conversational QA.
    \item TAT-QA~\citep{zhu2021tatqa}: A QA dataset aiming to stimulate the progress of research over more complex and realistic tabular and textual data.
\end{itemize}

\input{appendices/appendices-tables/semi-table}

