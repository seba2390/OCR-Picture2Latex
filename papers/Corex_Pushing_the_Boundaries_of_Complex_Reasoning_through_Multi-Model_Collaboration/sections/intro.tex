\section{Introduction}

% \begin{flushright}
% \rightskip=7cm\textit{``A problem shared is a problem halved.''} \\
% \vspace{.3em}   
% \rightskip=1.3cm---\emph{English Proverb}
% \end{flushright}

Large Language Models (LLMs) have succeeded in advancing the state-of-the-arts for a series of Natural Language Processing (NLP) tasks~\citep[][\emph{inter alia}]{brown2020GPT3, chowdhery2022palm, openai2023gpt4, touvron2023llama2, zhao2023survey}.
% Recent advancements
Recent research~\citep{wei2022emergent} indicates that scaling up models~\citep{kaplan2020scaling} can yield improvements in both performance and sample efficiency across a broad spectrum of downstream tasks.
Notwithstanding their remarkable proficiency in language understanding and instruction following~\citep{ouyang2022training}, 
the reasoning abilities of LLMs, 
often seen as a hallmark for assessing their potential, 
still present challenges~\citep{suzgun2023bbh, huang2023towards}.
Concurrently,
there is a prevailing view that merely increasing the size might not adequately address their inherent limitations in solving reasoning tasks~\citep{rae2021scaling}.

In response to this challenge,
\citet{wei2022chain} put forth chain-of-thought (CoT) prompting that an LLM generates a series of intermediate steps toward a final answer,
contrasting the use of ``answer-only'' prompts.
% instead of using ``answer-only'' prompts.
% \lpk{prompts}.
% \lpk{`instead of', or `replacing'}
% as a technique to elicit more systematic deductive processes from language models.
Subsequently, 
various approaches have been put forward,
% various approaches have been put forward, 
such as self-consistency decoding~\citep{wang2023sc} which utilizes a majority voting mechanism to determine the final answer,
% least-to-most~\citep{zhou2023leasttomost} that breaks a complex problem into simpler subproblems for better generalization, 
and program-aided language models (PAL; \citealp{gao2022pal,chen2022program}) 
that leverage code generation to reduce errors in computations.
Besides,
curated prompts necessitate task-specific designs~\citep{zheng2023php} have also been utilized to elicit more accurate predictions.
Nevertheless, 
these approaches are confined within a static black box~\citep{yao2023react}, 
wherein the LLM relies exclusively on its internal representation for generating responses and is prone to generating unreliable answers~\citep{ji2023hallu, yin2023selfaware}.
% lead to unreliable reasoning process~\citep{ji2023hallu, yin2023selfaware}.
% wherein the LLM relies exclusively on its internal representations\lpk{todo..} to formulate thoughts.
% Nevertheless, the limitation to a static black box persists~\citep{yao2023react}, where the LLM is solely governed by its built-in cognitive framework, without the augmentation from external insights, for generating responses.
% Furthermore, 
% utilizing curated prompts~\citep{fu2023complexcot, zheng2023php} to elicit responses from models necessitates task-specific designs. 
These shortcomings underscore that relying solely on crafting decoding strategies and specialized prompts may not serve as a silver bullet for addressing complex reasoning tasks~\citep{qiao2023reasoning}.
Alternatively, 
enabling models to ``think outside the box'' emerges as a promising yet underexplored pathway.

% Despite the progress of CoT 
% and its variants 
% on multi-step reasoning tasks, 
% current LLMs are prone to generating unreliable answers~\citep{ji2023hallu, yin2023selfaware} and providing unfactual or unfaithful reasoning chains that most likely lead to a wrong conclusion.
% \lpk{most likely}


\begin{figure*}[t]
  \centering
  \vspace{-0.5em}
  \includegraphics[width=0.985\linewidth]{figures/wrong-cases.pdf}
  \caption{A depiction of three prevalent errors observed across LLMs when employing CoT and PAL to conduct reasoning tasks.}
  \label{fig:wrong-cases}
  \vspace{-1.45em}
\end{figure*}
\begin{figure*}[t]
  \centering
  \includegraphics[width=0.985\linewidth]{figures/corex.pdf}
  \caption{An intuitive illustration of \ours,
  employs LLMs as agents to collaboratively solve a problem.
  The strategies encompass the Debate, Review, and Retrieve modes,
  leveraging both the reasoning process and code synthesis.
  This framework facilitates interactions between models that 
  foster a collaborative environment for the derivation of a well-reasoned answer.}
  \label{fig:method-overview}
  \vspace{-1.35em}
\end{figure*}


% \lpk{relying solely}
% While a single LLM can handle some typical tasks, 
% Drawing upon well-established concepts in sociology, multiple cognitive processes interact and cooperate, yielding a combined effect that is greater than the sum of their individual contributions.

% Within the realm of well-established sociological concepts, multiple cognitive processes interact and cooperate, engendering a collective impact that surpasses the aggregate of their individual effects.

Within the realm of well-established sociological concepts,
multiple cognitive processes interact and cooperate will produce a combined effect that is greater than the sum of their individual contributions~\citep{luppi2022synergistic}.
This principle is echoed within artificial intelligence~\citep{li2023camel}.
Although the study of intelligent agents has been explored for decades~\citep{minsky1988society, minsky2007emotion}, 
the advent of LLMs
has rejuvenated interest and introduced novel challenges in this domain.
An emerging perspective is that encouraging collaboration and communication between models could potentially pave the way for a new stage for enhancing complex reasoning capabilities.

% characterized by their celebrated capabilities of conversation and instruction-following,
% Recent research based on intelligent agents~\citep{minsky1988society, minsky2007emotion} also suggests that fostering cooperation among multiple LLMs can pave the way for further advancements~\citep{li2023camel}. 
% To navigate through this dilemma, 

In this study, 
we propose \ours, 
a suite of human-inspired strategies that leveraging multi-model \underline{co}llaboration to elicit \underline{re}asoning for comple\underline{x} task-solving. % \lpk{that leveraging}
To facilitate synergies between models, 
we first assign distinct personas to different models, 
% \lpk{i love personas better than roles... although it is indeed a bit pretentious, it also sets us aside from zhiyuan liu's coding group role play, which probably deserves a sentence in the intro somewhere...} 
followed by the design of various collaborative paradigms. 
This collective intelligence-based method aims to conquer prevalent obstacles in the current landscape of reasoning,
as exemplified in Figure~\ref{fig:wrong-cases}.
% such as degeneration-of-thought~\citep{liang2023encouraging}.
It also endeavors to alleviate common issues observed in majority voting-based methods like self-consistency, 
where accurate responses might be overwhelmed by incorrect ones and exorbitant costs.
% 抽象一下 -> sc存在严重的边际效益递减问题？
% 我想在这里再分段一次，除了罗列三种合作范式外再添加一些细节
To be specific, \ours configures LLMs as a group of autonomous agents,
% \lpk{a group of autonomous agents?}, 
adopting the paradigms shown in Figure~\ref{fig:method-overview} for multi-model collaboration:
% we propose the following paradigms of multi-model collaboration: 
(1) Debate, utilizing group-based debates among models to effectively enhance the factuality~\citep{du2023improving} of generated content and minimize fallacies and hallucinations; 
(2) Review, enabling models to scrutinize reasoning chains or generated codes from their counterparts to ensure the correctness of generated contents, 
coupled with potential refinements; 
(3) Retrieve, aiming to enable the model to identify the most faithful option from a pool of candidate chains, facilitates a higher degree of alignment with the final response.
The comparison between \ours and recent works is listed in Table~\ref{tab:maethod_comp},
where our approach is task-agnostic,
requiring no prior knowledge or iterative processes during the reasoning phase,
which makes it broadly applicable to a wide array of scenarios.

We conduct extensive experiments across four types of tasks: mathematical reasoning, symbolic reasoning, commonsense reasoning, 
and semi-structured reasoning. 
The results illustrate that our method achieves substantial performance gains over previous strong baselines. 
% Additionally, 
% each mode reveals its strengths when facing different categories of tasks.
Moreover, each mode distinctly excels in different categories of tasks, 
showcasing its specific strengths.
% we have considered scenarios where program synthesis is utilized to solve complex computational problems. 
Further analysis reveals that, 
compared to existing schemes based on majority voting and curated prompts,
\ours significantly reduces the reasoning overhead of the models, 
achieving cost-effectiveness.

\input{sections/results/method-comparison}
