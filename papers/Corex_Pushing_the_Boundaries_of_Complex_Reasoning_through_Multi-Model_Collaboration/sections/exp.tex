\section{Experiment}

\subsection{Experimental Setup}

% \paragraph{Tasks and Datasets.}
\textbf{Tasks and Datasets.}
We evaluate the effectiveness of \ours across four types of reasoning tasks:
(1) Arithmetic reasoning over eight mathematical problems, 
which includes GSM8K~\citep{cobbe2021gsm8k}, MultiArith~\citep{roy2015multiarith}, 
SingleOP/SingleEQ~\citep{koncel2016mawps}, 
% SingleOP/SingleEQ~\citep{koncel2015parsing}, 
AddSub~\citep{hosseini2014addsub}, 
AQuA~\citep{ling2017aqua}, 
SVAMP~\citep{patel2021svamp} and GSM-Hard~\citep{gao2022pal}.
(2) Commonsense reasoning covering four datasets,
including StrategyQA~\citep{geva2021strategyqa},
CommonsenseQA (CSQA; \citealp{talmor2019commonsenseqa}),
BoolQ~\citep{clark2019boolq} and AI2 Reasoning Challenge (ARC-c)~\citep{clark2018think}.
(3) Symbolic reasoning incorporating four tasks derived from BigBench~\citep{srivastava2023bb, suzgun2023bbh}, including Date Understanding, 
Penguins in a Table, 
Colored Objects, 
and Repeat Copy. 
% Object Counting.
(4) Semi-structured understanding, with a focus on FinQA~\citep{chen2021finqa}, ConvFinQA~\citep{chen2022convfinqa} and TAT-QA~\citep{zhu2021tatqa}.
The detailed description and statistics of tasks are listed in Appendix~\ref{appendix:datasets}.

% \paragraph{Baselines.} 
\textbf{Baselines.}
We compare our method with several widely used strong baselines.
(1) Chain-of-Thought prompting (CoT; \citealp{wei2022chain}).
(2) Self-Consistency (CoT-SC; \citealp{wang2023sc}), which employs a majority voting mechanism to select the most consistent answer from several reasoning chains as the final answer.
(3) Complexity-based consistency (ComplexCoT; \citealp{fu2023complexcot}) that selects the majority answer from the candidates with higher reasoning complexity.
(4) Program-aided language model (PAL; \citealp{gao2022pal,chen2022program}) that uses LLMs to generate programs as intermediate reasoning steps,
while offloading the computation to a Python interpreter.

For simplicity and ease of understanding, 
we denote CoT-SC(x) and ComplexCoT(x) in our experiments and analysis to represent cases utilizing different reasoning paths, 
where ``x'' indicates the number of output chains. 
For all baseline methods, we adhere to the few-shot exemplars to ensure fair comparisons. Details can be found in Appendix~\ref{appendix:impl-details}.

% \paragraph{Implementation Details.}
\textbf{Implementation Details.}
We access OpenAI and Anthropic models through their respective APIs. 
Specifically, we employ \turbo for evaluating both \ours and baseline methods in the main experiments.
Moreover, 
in further experiments and analysis involving different LLMs for collaboration, 
we also incorporate the use of \gpt and \claude.
The details of prompts and hyperparameter settings for both baselines and \ours are in Appendix~\ref{appendix:prompts}.

\subsection{Main Results}
\label{sec:main-results}

We report the results of \ours over four categories of tasks. 
For each kind of task, 
the best results are highlighted in \textbf{bold} and the second best results are marked with \underline{underline}.
For Review mode, 
we use \crnl and \crcode to describe the scenarios that use CoT or PAL respectively.
% To further underscore the advantages derived by collaboration, 
All modes within \ours are configured to operate with 5 LLM-based agents, 
ensuring favorable cost-effectiveness.
For \crd, the upper bound of debate rounds is set to 5.

\paragraph{Mathematical Reasoning.} 
Table~\ref{tab:math-result} shows the results across arithmetic tasks with varying difficulties.
Our method achieves notable performance improvements on most benchmarks.

\input{sections/results/math-results}

Broadly,
we surpass the performance of CoT-SC(10) when only 5 agents are involved.
Moreover, 
given the task-agnostic nature of \ours,
it can tackle highly complex computational challenges like GSM-Hard through code synthesis.
For problems of relatively lower complexity, 
the Retrieve mode can identify answers superior to those from majority voting.

\paragraph{Commonsense Reasoning.} 
Table~\ref{tab:qa-result} showcases the performance of \ours in commonsense and factual reasoning tasks\footnote{Due to the nature of commonsense reasoning tasks, 
the Review mode only utilizes NL reasoning chains.}.
We can observe that various modes contribute to performance enhancements.

\input{sections/results/qa-results}

Notably, our approach surpasses ComplexCoT (over 6\% on StrategyQA),
achieving a significant improvement without resorting to intricate prompt design and example selection.

\paragraph{Symbolic Reasoning.}
We report the results for symbolic reasoning in Table~\ref{tab:bbh-result}.
Empirical evidence substantiates that adopting multi-model collaboration can notably outperform most previous baselines on Big-Bench tasks. 
It is noteworthy that 
(1) CoT-SC struggles to ensure consistent outputs on the Repeat Copy.
Conversely, 
through the integration of PAL-based collaboration, 
we manage to attain a remarkably high level of accuracy.
(2) Compared to majority voting, 
both the Review and Retrieve modes enable more judicious answer selection in counting tasks.


\input{sections/results/bigbench-results}
\paragraph{Semi-structured Reasoning.}
We demonstrate the results on FinQA and ConvFinQA in Table~\ref{tab:table-result}.
It can be observed that for these two challenging tasks which require understanding heterogeneous information and performing calculations simultaneously~\citep{lu2023dynamic}, 
methods such as CoT-SC offer limited gains. 
However, 
through various cooperative paradigms, 
significant performance improvements can be achieved.
% \vspace{-1em}
Due to the context length restriction of \turbon, 
our experiments on TAT-QA utilized \turbol,
with the respective results being detailed in Appendix~\ref{appendix:tables}, 
alongside the evaluations on the other tasks.
\input{sections/results/table-results}
Following our extensive experiments across 18 tasks,
it emerges that the Debate mode is competent for tasks utilizing factual knowledge.
For mathematical and counting tasks,
the Review mode serves to effectively mitigate errors within the reasoning chains and repair flawed code. 
Across various tasks, 
the Retrieve mode consistently facilitates performance improvements to varying degrees.

\section{Analysis}

In this section, 
we first aim to make the collaboration process transparent by delving into models' internal behaviors.
Then,
the influence of different backbones is examined to observe how model capability affects performance.
Further,
we assess the efficiency of \ours.

\subsection{In-Depth Analysis of \ours Strategies}

\paragraph{Analysis of Interaction Rounds in Debate Mode.}
We study the number of rounds of communication in the Debate mode of \ours on five tasks,
as depicted in Figure~\ref{fig:debate-rounds}.
Consensus can be reached swiftly for the majority of problems by each team. 
However, 
\ours enables LLMs to engage in more exhaustive discussions for problems that are challenging to reach a consensus on (e.g., over 10\% of ConvFinQA problems requiring more than 3 rounds),
a small proportion of problems require more interactions.
Through observation,
we also notice that the Debate mode exhibits favorable convergence properties,
wherein the interactive process serves as a basis for the judge's decision-making.

\begin{figure}[ht]
    \centering
    \begin{minipage}[t]{0.48\textwidth}
        \includegraphics[width=\linewidth]{figures/analysis/debate-rounds.pdf}
        \caption{Distribution of the number of debate rounds required to reach consensus.}
        \label{fig:debate-rounds}
    \end{minipage}
    \hfill
    \begin{minipage}[t]{0.48\textwidth}
        \includegraphics[width=\linewidth]{figures/analysis/review-rounds.pdf}
        \caption{Performance gains across multiple rounds of review}
        \label{fig:review-rounds}
    \end{minipage}
    \vspace{-0.75em}
\end{figure}

\paragraph{Performance Enhancement per Review.}

We explore the incremental performance gains achieved in specific tasks with each review cycle in the Review mode.
As is demonstrated in Figure~\ref{fig:review-rounds},
we conduct analyses for Repeat Copy and GSM8K with \rcode, 
as long as BoolQ and Penguin with \rnl.
The findings indicate that each review contributes to performance enhancement in general,
yet occasional deviations leading to performance oscillations are also observed.
\subsection{Synergies between Different LLMs}

\begin{figure}[ht]
    \centering
    \begin{minipage}[t]{0.48\textwidth}
        \includegraphics[width=\linewidth]{figures/analysis/debate-diff-models.pdf}
        \caption{Comparison of using different LLMs as judges in Debate mode.}
        \label{fig:debate-analysis-diff-model}
    \end{minipage}
    \hfill
    \begin{minipage}[t]{0.48\textwidth}
        \includegraphics[width=\linewidth]{figures/analysis/retrieve-diff-models.pdf}
        \caption{Comparison of using different LLMs as retrievers in Retrieve mode.}
        \label{fig:retrieve-analysis-diff-model}
    \end{minipage}
\end{figure}


\paragraph{Performance Variability with Diverse LLMs as Judges.}

The backbone LLMs of our agents can be diverse.
In this part,
we discuss the performance variations when employing different LLMs during the debate process.
As shown in Figure~\ref{fig:debate-analysis-diff-model},
we deploy \turbon as debaters and examine the dynamics when different LLMs take the role of judges. 
The observations indicate that the capability of the judge positively correlates with task performance,
with this relationship being evident as the complexity of tasks escalates.
% This is because the judge's role in the debate process requires understanding both the intent of the question and the reasoning process of both parties.
Empirically, 
This can be attributed to the judge's role in the debate process,
which requires understanding both the question and the reasoning process of both parties.

\paragraph{Utilizing Different LLMs as Retrievers.}

In Retrieve Mode, 
the role of the retriever can be played by various LLMs.
Based on the candidate answers from \turbon agents,
we here explore the impact of model selection on the performance,
as depicted in Figure~\ref{fig:retrieve-analysis-diff-model}.
Unlike the debate mode,
our analysis reveals that the model capabilities exert a modest effect on the performance. 
Given that the performance upper bound is determined by the candidates' capabilities, 
the outcomes using different LLMs as retrievers show minimal variance on tasks like ARC-c. 
Notably, our findings indicate that without the need for especially potent models as retrievers, we can still achieve favorable results.


\subsection{Cost-Effectiveness of Multi-Model Collaborations}
By encouraging collaboration between LLMs,
we manage to reduce the costs associated with reasoning tasks while achieving comparable or even superior performance.
\begin{wrapfigure}{r}{8.05cm}
\vspace{-0.75em}
    \centering
    \includegraphics[width=\linewidth]{figures/analysis/cost-effective.pdf}
    \caption{Cost-effectiveness analysis.
    the x-axis represents the computational costs, 
    calculated in terms of input/output tokens, 
    while the size of each dot is proportional to the avg. number of inferences by each method.}
    \vspace{-1.25em}
    \label{fig:cost-eff-main}
\end{wrapfigure}
Based on our analysis conducted on AddSub illustrated in Figure~\ref{fig:cost-eff-main}, 
it reveals that all three modes of \ours consistently match or surpass the prowess of other strong baselines.
Significantly, 
the computational cost of our approach are substantially diminished in comparison to methods using majority voting.
In achieving equivalent performance,
the resource consumption of \ours is confined to a mere 5-10\% of that expended by other strategies.
To substantiate the generality,
we've provided additional experiments in Appendix~\ref{appendix:cost-effectiveness}, 
which further demonstrate a similar trend.

Beyond the efficiency of computational costs, 
another advantage of \ours is its annotation efficiency, 
which reduces the reliance on curated demonstrations. 
Further experiments with varying numbers of demonstrations on this aspect can be found in Appendix~\ref{appendix:ann-eff}.
