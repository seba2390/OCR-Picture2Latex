\subsection{Retrieve}
\label{section:retrieve}


In the final thread of work, we delve into the Retrieve mode to identify the most faithful answer through collaborations. 
While previous strategies based on majority voting mechanism~\citep{wang2023sc, fu2023complexcot} can mitigate the low-diversity issue of techniques such as beam-search~\citep{li2016mutalia},
they still present the following two significant challenges:
\begin{wrapfigure}{r}{4.575cm}
\vspace{-1em}
    \centering
    \includegraphics[width=\linewidth]{figures/retrieve-illustration.pdf}
    \caption{Illustration of retrieving faithful chains with answers.}
    \vspace{-1.75em}
    \label{fig:retrieve-illustration}
\end{wrapfigure}
(1) Correct answers risk being swayed by incorrect ones.
% (1) Correct answers risk being overshadowed by a larger number of incorrect ones.
(2) Despite facilitating a notable enhancement in performance, 
it exponentially escalates the computational burden and tends to reach a performance ``saturation point'' as the sampled chains increase.
We attribute these drawbacks to the limited scope of majority voting techniques that singularly prioritize the prediction while overlooking the faithfulness of reasoning chains~\citep{li2022on}.
In response, 
we propose the Retrieve mode,
a paradigm specifically engineered to evaluate whether the answer can be expressed by the content (explanation) generated during reasoning~\citep{jacovi2020towards, lanham2023measuring}.

Concretely,
given a query \( q \), 
we randomly select an agent \( A_r \) from the pool of \( n \) agents to act as the retriever. 
The remaining agents \( \{A_1, A_2, \ldots, A_{n-1}\} \) independently perform CoT reasoning about \( q \). 
Each of these agents derives its own reasoning chains \( c_i \) and corresponding predictions \( p_i \). 
Together, 
they form a candidate pool, 
denoted by $\mathcal{P} = \{(c_i, p_i)\}_{i=1}^{n-1}$
% \( P = \{(c_1, p_1), (c_2, p_2), \ldots, (c_{n-1}, p_{n-1})\} \).

The retriever \( A_r \) then scrutinizes the candidates in $\mathcal{P}$. For \( (c_i, p_i) \), 
\( A_r \) evaluates the faithfulness between \( c_i \) and \( p_i \). 
Based on this assessment, the retriever assigns a confidence score \( s_i \) in the range \([0,1]\),
which is denoted as: \( s_i = f_r(c_i, p_i) \)
% This confidence evaluation is mathematically expressed 
where \( f_r \) indicates the retriever's evaluation process.
After that,
the most faithful response to the question \( q \) is then determined by the highest confidence:
\[ (c^*, p^*) = \argmax_{(c_i, p_i) \in \mathcal{P}} s_i \]
Here, \( (c^*, p^*) \) denotes the chain-prediction pair that the retriever considers most faithful,
which will serve as the final answer for the query \( q \).

Retrieve mode enables the selection of the most aligned combination of reasoning chains and answers from a diversified candidate pool.
Distinct from previous text quality assessment methods, 
which rely on the log probability of sequences~\citep{adiwardana2020towards} that is computationally inefficient and often unavailable for commercial LLMs,
our approach is entirely predicated on model-to-model interactions~\citep{chen2023exploring} and is reference-free.