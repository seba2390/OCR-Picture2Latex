\vspace{-5pt}
\section{Related Works} \label{sec:related-work}
\vspace{-3pt}
\subsection{3D CNNs for video recognition}
\vspace{-3pt}
Convolutional Neural Networks with spatio-temporal 3D kernels may be considered the natural extension of 2D CNNs for image recognition to CNNs for video recognition.
Although they did not surpass their 2D CNN + RNN competitors~\cite{donahue2015longterm}, \cite{yuehei2015beyond} initially~\cite{ji20133dconv}, \cite{karpathy2014largescale}, \cite{tran2015learning}, arguably due to a high parameter count and insufficient dataset size, 3D CNNs have achieved state-of-the-art results on Human Action Recognition tasks~\cite{carreira2017quo}, \cite{tran2018closer}, \cite{feichtenhofer2019slowfast} since the Kinetics dataset~\cite{kay2017kinetics} was introduced.
% However, these networks~\cite{ji20133dconv, karpathy2014largescale, tran2015learning} did not surpass their 2D CNN + RNN competitors~\cite{donahue2015longterm, yuehei2015beyond} at inception, arguably due to a high parameter count, which in turn made their optimisation difficult.
% It was not until the Kinetics dataset~\cite{kay2017kinetics} was introduced, that the 3D CNN based models led to the state-of-the-art accuracy~\cite{carreira2017quo, tran2018closer, feichtenhofer2019slowfast} for activity recognition in videos.
While recent large-scale Transformer-based methods~\cite{arnab2021vivit}, \cite{neimark2021video} have become leaders in terms of accuracy, 3D CNNs still achieve state-of-the-art accuracy/complexity trade-offs. 
Nevertheless, competitive accuracy comes with high computational cost, which is prohibitive to many real-life use cases.

In image recognition, efficient architectures such as MobileNet~\cite{howard2017mobilenet}, ShuffleNet~\cite{zhang2018shufflenet}, and EfficientNet~\cite{mingxing2019efficientnet} attained improved accuracy-complexity trade-offs.
These architectures were extended to the 3D-convolutional versions {3D-MobileNet} \cite{kopulku2019resource}, {3D-ShuffleNet}~\cite{kopulku2019resource} and X3D~\cite{feichtenhofer2020x3d} ($\approx${3D-EfficientNet}) with similarly improved pareto-frontier in video-recognition tasks. 
While these efficient 3D CNNs work well for offline processing of videos, they are limited in the context of online processing, where we wish to make updates predictions for each frame; real-time processing rates can only be achieved with the smallest models at severely reduced accuracy.
3D CNNs suffer from the restriction that they must process a whole ``clip'' (spatio-temporal volume) at a time. % rather than frame by frame.
When predictions are needed for each frame, this imposes a significant overhead due to repeated computations. % over frames.
In our work, we overcome this challenge by introducing an alternative computational scheme for spatio-temporal convolutions, -pooling, and -residuals, which lets us compute 3D CNN outputs frame-wise (continually) and dispose of the redundancies produced by regular 3D CNNs.


\vspace{-3pt}
\subsection{Architectures for online video recognition}
\vspace{-3pt}

A well-explored approach to video-recognition~\cite{donahue2015longterm}, \cite{yuehei2015beyond}, \cite{kalogeiton2017action}, \cite{singh2017online} is to let each frame pass through a 2D CNN trained on ImageNet in one stream alongside a second stream of Optical Flow~\cite{farneback2003twoframe} and integrate these using a recurrent network. % to model long-term temporal dependencies~\cite{donahue2015longterm, yuehei2015beyond} or used for spatio-temporal action detection tasks~\cite{kalogeiton2017action, singh2017online}. 
Such architectures requires no network modification for deployment in online-processing scenarios, lends themselves to caching~\cite{xu2018deepcache}, and are free of the computational redundancies experienced in 3D CNNs.
However, the overhead of Optical Flow and costly feature-extractors pose a substantial disadvantage.
% It has the disadvantage that optical flow introduces a large computational overhead. %, and the accuracy has generally not reached those achieved by 3D CNNs.
% It's disadvantages 

Another approach is to utilise 3D CNNs for feature extraction. 
In \cite{molchanov2016online}, spatio-temporal features from non-overlaping clips are used to train a recurrent network for hand gesture recognition.
In \cite{kopuklu2019yowo}, a 3D CNN processes a sliding window of the input to perform spatio-temporal action detection.
These 3D CNN-based methods have the disadvantage of either not producing predictions for each input frame~\cite{molchanov2016online} or suffering from redundant computations from overlapping clips~\cite{kopuklu2019yowo}.

Massively Parallel Video Networks~\cite{carreira2018massively} split a DNN into depth-parallel sub-networks across multiple computational devices to improve online multi-device parallel processing performance. While their approach treats networks layers as atomic operations and doesn't tackle the fundamental redundancy of temporal convolutions, \textit{Continual} 3D CNNs reformulate the network layers, remove redundancy, and accelerate inference on single devices as well.

Exploring modifications of the spatio-temporal 3D convolution operating frame by frame, the Recurrent Convolutional Unit (RCU)~\cite{singh2019recurrent} replaces the 3D convolution by aggregating a spatial 2D convolution over the current input with a 1D convolution over the prior output. 
Dissected 3D CNNs~\cite{kopuklu2020dissected} (D3D) cache the $1 \times n_H \times n_W$ frame-level features in network residual connections and aggregate them with the current frame features via $2 \times 3 \times 3$ convolutions.
% concatenates them with the corresponding features in the next frame. 
% This produces intermediary spatio-temporal features of shape $2 \times n_H \times n_W$, which become inputs to a block of convolutional layers. 
% With kernel sizes $k_T \times k_H \times k_W$ of $2 \times 3 \times 3$ and $1 \times 3 \times 3$, the block produces features of shape $1 \times n_H \times n_W$ to be cached once again.
% Here, $n$ denotes the feature map size and $k$ the kernel size and subscripts $T$, $H$, and $W$ denote the time, height, and width dimensions.
% % Following the temporally cached residual blocks, 
% An LSTM is then used for late spatio-temporal modelling prior to prediction.
Like the our proposed \textit{Continual} 3D CNNs, both RCU and D3D are causal and operate frame-by-frame.
However, they are speciality architectures, which are incompatible with pre-trained 3D CNNs, and must be trained from scratch.
% While the idea of caching features is also central to our work, 
We reformulate spatio-temporal convolutions in a one-to-one compatible manner, allowing us to reuse existing model weights. 
% In fact, all results in this paper were achieved without any training!

% The solutions are complementary and can be combined.
