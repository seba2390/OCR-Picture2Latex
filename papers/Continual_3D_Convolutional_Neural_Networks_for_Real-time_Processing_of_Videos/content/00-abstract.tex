% As a general rule, do not put math, special symbols or citations in the abstract
% ICCV max characters: 4000.
% Current characters: ~1500
\begin{abstract}

We introduce \textit{Continual} 3D Convolutional Neural Networks (\textit{Co}3D CNNs), a new computational formulation of spatio-temporal 3D CNNs, in which videos are processed frame-by-frame rather than by clip.
In online tasks demanding frame-wise predictions, \textit{Co}3D CNNs dispense with the computational redundancies of regular 3D CNNs, namely the repeated convolutions over frames, which appear in overlapping clips.
We show that \textit{Continual} 3D CNNs can reuse preexisting 3D-CNN weights to reduce the per-prediction floating point operations (FLOPs) in proportion to the temporal receptive field while retaining similar memory requirements and accuracy.
bgithuThis is validated with multiple models on \mbox{Kinetics-400} and Charades with remarkable results: \textit{Co}X3D models attain state-of-the-art complexity/accuracy trade-offs on \mbox{Kinetics-400} with 12.1$-$15.3$\times$ reductions of FLOPs and 2.3$-$3.8\% improvements in accuracy compared to regular X3D models while reducing peak memory consumption by up to 48\%. 
Moreover, we investigate the transient response of \textit{Co}3D CNNs at start-up
and perform extensive benchmarks of on-hardware processing characteristics for publicly available 3D CNNs.
%
%While yielding an order of magnitude in computational savings, Co3D CNNs have memory requirements comparable with that of corresponding regular 3D CNNs and are less affected by changes in the size of the temporal receptive field.
%We show that Continual 3D CNNs initialised on the weights from preexisting state-of-the-art video recognition models reduce the floating point operations for frame-wise computations by 10.0$-$12.4$\times$ while improving accuracy on \mbox{Kinetics-400} by 2.3$-$3.8\%.
%Moreover, we investigate the transient start-up response of Co3D CNNs and perform an extensive benchmark of online processing speed as well as accuracy for publicly available state-of-the-art 3D CNNs on modern hardware.
%

\keywords{3D CNN, Human Activity Recognition, Efficient, Stream Processing, Online Inference, Continual Inference Network.}

\end{abstract}
% This paper introduces Continual Convolutions, a new computational model for convolutions with a temporal dimension, in which videos are processed frame by frame rather than clip-wise.
% In online processing tasks demanding frame-wise predictions, Continual 3D Convolutional Neural Networks (Co3D CNNs) dispense with the computational redundancies of regular 3D CNNs, namely the repeated processing of frames, which are part of multiple clips in time.
%% Old sentences:
% By increasing their receptive field, Co3D CNNs achieve a significant increase in prediction accuracy at negligible computational and memory costs.
% Hitherto, the deployment of 3D Convolutional Neural Networks (CNNs) in online real-time systems has been restricted to small models and reduced prediction accuracy. 
% We identify and dispense with the computational redundancies of 3D CNNs operating on a continual input stream, and reduce the floating point operations by more than an order of magnitude in the conversion from regular 3D CNNs to Continual 3D CNNs (Co3D CNNs).
% For networks without zero-padding in the temporal dimension, the outputs produced by regular and continual convolutions are identical.
% While computational costs for regular 3D CNNs increase linearly with the clip size, the cost of Co3D CNNs is largely unaffected by changes in the size of the temporal receptive field.
% This allows an increase in the receptive field at negligible computational cost, while increasing the prediction accuracy.

% We introduce Continual 3D Convolutional Neural Networks (Co3D CNNs), a new computational formulation of spatio-temporal 3D CNNs, in which videos are processed frame-by-frame rather than by clip. In online processing tasks demanding frame-wise predictions, Co3D CNNs dispense with the computational redundancies of regular 3D CNNs, namely the repeated convolutions over frames, which appear in overlapping clips. We show that Continual 3D CNNs can reuse preexisting 3D-CNN weights to reduce the per-prediction floating point operations (FLOPs) in proportion to the temporal receptive field while retaining similar memory requirements and accuracy. This is validated with multiple models on the Kinetics-400 and Charades datasets with remarkable results: Continual X3D models attain state-of-the-art complexity/accuracy trade-offs on Kinetics-400 with 12.1-15.3x reductions of FLOPs and 2.3-3.8% improvements in accuracy compared to regular X3D models while reducing peak memory consumption by up to 48\%. Moreover, we investigate the transient response of Co3D CNNs at start-up and perform an extensive benchmark of on-hardware processing speed and accuracy for publicly available 3D CNNs.