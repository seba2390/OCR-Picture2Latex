\section{Conclusion} \label{sec:conclusion}
% \vspace{-5pt}
We have introduced Continual 3D Convolutional Neural Networks (\textit{Co}3D CNNs), a new computational model for spatio-temporal 3D CNNs, which performs computations frame-wise rather than clip-wise while being weight-compatible with regular 3D CNNs. 
In doing so, we are able dispose of the computational redundancies faced by 3D CNNs in continual online processing, giving up to a 15.1$\times$ reduction of floating point operations,
a 9.2$\times$ real-life inference speed-up on CPU, 48\% peak memory reduction,
and an accuracy improvement of $5.6\%$ on Kinetics-400 through an extension in the global average pooling kernel size.

While this constitutes a substantial leap in the processing efficiency of energy-constrained and real-time video recognition systems, there are still unanswered questions pertaining to the dynamics of \textit{Co}3D CNNs.
% Although \textit{Co}3D can be trained just like any other neural network, it is still an open question how to best train them.
Specifically,
the impact of extended receptive fields on the networks ability to change predictions in response to changing contents in the input video is untested.
We leave these as important directions for future work.


% From the results on extended pooling size (\cref{sec:exp-extended-receptive-fields}), it seems that the extension of the temporal field 
% via extensions in the global average pooling layer comes with solely increase in accuracy.
% However, further extended pooling size may negatively impact how ``dynamic'' the network is, i.e. how fast predictions change in response to changing contents in the video.
% Prediction over longer clips may thus show better dynamic characteristics by putting more weight on recent frames.
% This could come in the form of a weighted summation in the temporal dimension as in \cite{yuehei2015beyond, donahue2015longterm} or a self-attention mechanism~\cite{vaswani2017attention}. % akin to \cite{chen2020transformer, dai2019selfattention}, though an efficient continual formulation of transformer-style attention is yet to be seen.

% Despite the significant computational (and in some cases memory) savings of continual CNNs for online processing, there are cases where the regular CNNs are better suited.
% Regular CNNs are still preferred for offline processing of clips, where frame-wise predictions are not needed.

