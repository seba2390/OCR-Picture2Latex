\section{Introduction} \label{sec:introduction}


% The machine learning and computer vision community has made significant advances during the past decade, approaching or surpassing human-level performance in multiple vision tasks. 
% To a large extend, this can be attributed to the availability of large-scale and open-source datasets such as ImageNet~\cite{russakovsky2015imagenet} and Kinetics~\cite{kay2017kinetics, carreira2018kinetics}.
% This allowed large, over-parameterized Convolutional Neural Networks (CNNs) to learn general visual concepts which can be transferred to other tasks in computer vision. 

Through the availability of large-scale open-source datasets such as ImageNet~\cite{russakovsky2015imagenet} and Kinetics~\cite{kay2017kinetics}, \cite{carreira2018kinetics},
deep, over-parameterized Convolutional Neural Networks (CNNs) have achieved impressive results in the field of computer vision.
In video recognition specifically, 3D CNNs have lead to multiple breakthroughs in the state-of-the-art~\cite{carreira2017quo}, \cite{tran2018closer}, \cite{feichtenhofer2019slowfast}, \cite{feichtenhofer2020x3d}. %, generally outperforming RNN-based approaches~\cite{donahue2015longterm, yuehei2015beyond}.
Despite their success in competitions and benchmarks where only prediction quality is evaluated, computational cost and processing time remains a challenge to the deployment in many real-life use-cases with energy constraints and/or real-time needs.
% e.g. autonomous vehicles
To combat this general issue, multiple approaches have been explored.
These include computationally efficient architectures for image~\cite{howard2017mobilenet}, \cite{zhang2018shufflenet}, \cite{mingxing2019efficientnet} 
and video recognition~\cite{kopulku2019resource}, \cite{feichtenhofer2020x3d}, \cite{zhu2020faster},
pruning of network weights~\cite{chen2015compressing}, \cite{han2015deep}, \cite{he2017channel},
knowledge distillation~\cite{hinton2015distilling}, \cite{yim2017gift}, \cite{passalis2018learning},
and 
network quantisation~\cite{hubara2016binarized}, \cite{cai2017quantisation}, \cite{floropoulos2019complete}.

The contribution in this paper is complementary to all of the above. It exploits the computational redundancies in the application of regular spatio-temporal 3D CNNs to a continual video stream in a sliding window fashion (\cref{fig:conv-redundancy}).
This redundancy was also explored recently~\cite{kopuklu2020dissected}, \cite{singh2019recurrent} using specialised architectures. However, these are not weight-compatible with regular 3D CNNs.
We present a weight-compatible reformulation of the 3D CNN and its components as a \mbox{\textit{Continual}} 3D Convolutional Neural Network (\textit{Co}3D CNN).
\textit{Co}3D CNNs process input videos frame-by-frame rather than clip-wise and can reuse the weights of regular 3D CNNs, producing identical outputs for networks without temporal zero-padding. 
Contrary to most deep learning papers, the work presented here needed no training; our goal was to validate the efficacy of converting regular 3D CNNs to Continual CNNs directly, and to explore their characteristics in the online recognition domain.
% To explore the characteristics of Continual CNNs and validate their efficacy, 
Accordingly, 
we perform conversions from five 3D CNNs, each at different points on the accuracy/speed pareto-frontier, and evaluate their frame-wise performance.
While there is a slight reduction in accuracy after conversion due to zero-padding in the regular 3D CNNs, a simple network modification of extending the temporal receptive field recovers and improves the accuracy significantly \textit{without} any fine-tuning at a negligible increase in computational cost.
Furthermore, we measure the transient network response at start-up, and perform extensive benchmarking on common hardware and embedded devices to gauge the expected inference speeds for real-life scenarios.
Full source code is available at \texttt{\url{https://github.com/lukashedegaard/co3d}}.
% We find that Continual 3D CNNs offer a $10.0$$-$$12.4$$\times$ reduction in floating point operations (FLOPs), a $5.9$$-$$9.2$$\times$ speed increase on CPU and a $2.8$$-$$3.8\%$ accuracy improvement on Kinetics-400 over regular 3D CNNs.
% Large model on CPU has a speed up of 2.3/0.25 = 9.2, while some networks "only" speed up 5x


% For offline processing of videos, these networks work well, but in the context of online processing, where we wish to make predictions many times a second, they are still lacking; real-time processing rates using these 3D convolutional networks can only be achieved at the price of severely reduced accuracy.
% At the heart of their limitation is the restriction that they must process a whole ``clip'' at a time rather than frame by frame.
% When predictions are needed for each frame, this imposes a significant overhead due to repeated computations over the same frames.

% In vision tasks with a temporal dimension, a straight-forward and well explored approach~\cite{donahue2015longterm, yuehei2015beyond} is to let each frame pass through a 2D CNN trained on ImageNet, and have a component such as an LSTM integrate the information over the temporal dimension. 
% For offline clip-wise processing, however, these networks architectures were less successful than their 3D-convolutional competitors~\cite{carreira2017quo, tran2018closer, feichtenhofer2019slowfast}.


% Current trends favour the exploration of 3D CNN models or its approximations/deviates, where the temporal dimension is considered earlier rather than later in the network. The downside to this approach is an explosion in the number of parameters and floating point operations (FLOPs, i.e. the number of multiplications and additions) it takes to classify a video clip. 


\begin{figure}
    \centering
    \includegraphics[width=0.75\linewidth]{figures/acc-vs-flops-log-v3.pdf}
    \caption{
        \textbf{Accuracy/complexity trade-off} for \textit{Continual} 3D CNNs and recent state-of-the-art methods on Kinetics-400 using 1-clip/frame testing. 
        $\ssquare$~FLOPs per \textit{clip} are noted for regular networks, while $\sbullet$~FLOPs per \textit{frame} are shown for the Continual 3D CNNs. 
        % The continual models re-used the weights from regular models without further fine-tuning.
        % The \textit{Co}X3D models used the weights from the X3D models without further fine-tuning.
        Frames per clip / global average pool size is noted in the representative points.
        Diagonal and vertical arrows indicate a direct weight transfer from regular to Continual 3D CNN and an extension of receptive field.
    }
    % \vspace{-25pt}
    \label{fig:test-acc-vs-flops}
\end{figure}