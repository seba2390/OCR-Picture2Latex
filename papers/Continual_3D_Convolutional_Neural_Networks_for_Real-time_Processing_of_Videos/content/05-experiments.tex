\vspace{-2mm}
\section{Experiments} 
\vspace{-2mm}
The experiments in this section aim to show the characteristics and advantages of Continual 3D CNNs as compared with regular 3D CNNs.
One of the main benefits of \textit{Co}3D CNNs is their ability to reuse the network weights of regular 3D CNNs. 
As such, all \textit{Co}3D CNNs in these experiments use publicly available pre-trained network weights of regular 3D CNNs~\cite{feichtenhofer2019slowfast}, \cite{feichtenhofer2020x3d}, \cite{fan2021pytorchvideo} without further fine-tuning.
Data pre-processing follows the respective procedures associated with the originating weights unless stated otherwise.
% 
% In fact, no neural network training is performed in any of the experiments in this paper!
The section is laid out as follows: 
First, we showcase the network performance following weight transfer from regular to Continual 3D on multiple datasets for Human Activity Recognition. % Kinetics-400~\cite{kay2017kinetics}, Charades~\cite{sigurdsson2016charades}, and AVA~\cite{gu2018ava}.
This is followed by a study on the transient response of \textit{Co}3D CNNs at startup.
Subsequently, we show how the computational advantages of \textit{Co}3D CNNs can be exploited to improve accuracy by extending the temporal receptive field.
Finally, we perform an extensive on-hardware benchmark of prior methods and Continual 3D CNNs, measuring the 1-clip/frame accuracy of publicly available models, as well as their inference throughput on various computational devices.


% \subsection{Datasets}
% Kinetics-400~\cite{kay2017kinetics} is a large-scale dataset for supervised activity recognition. 
% It consists of 306,245 ten second video-clips from YouTube, ranging 400 action classes. 
% The Kinetics dataset is collected directly by crawling YouTube, making the dataset size vary depending on the time and locality of the download. 
% For our experiments, we use the Kinetics-400 Dec 2020 version
% (92.5\% of train, %203701/220302
% 88.8\% of validation, %16053/18078
% and 92.4\% of test data was available). %32777/35460
% Unless stated otherwise, our experiments use the same data pre-processing steps as \cite{feichtenhofer2019slowfast, feichtenhofer2020x3d}.

% \textbf{Kinetics-600}~\cite{carreira2018kinetics} is the 2nd generation of the Kinetics dataset, extended to 600 action classes.


% \textbf{Charades}~\cite{sigurdsson2016charades}
% Unstructured activity recognition with longer range activities




\begin{table}[t]
\begin{center}
\resizebox{\textwidth}{!}{
\begin{tabular}{llrrrrrrrr}
    \toprule
    &\multirow{2}{*}{\textbf{Model}} 
    &\textbf{Acc.}
    &\textbf{Par.}
    &\textbf{Mem.}  
    &\textbf{FLOPs}
    &\multicolumn{4}{c}{\textbf{Throughput} (preds/s)}
        \\ \cline{7-10}
        % &&&&&\textbf{CPU} & \textbf{TX2}  &\textbf{Xavier} &\textbf{RTX 2080Ti}
        && (\%) & (M) & (MB) & (G) & CPU & TX2  & Xavier  &2080Ti
    \\
    \midrule
    % \multicolumn{10}{c}{\textit{Clip}} \\
    % \midrule
    \parbox[t]{1mm}{\multirow{9}{*}{\rotatebox[origin=c]{90}{Clip}}}                  % CPU      TX2        XAVIER      RTX 2080 Ti
    & I3D-R50                           & 63.98     & 28.04    & 191.59    & 28.61      & 0.93   & 2.54     & 9.20      & 77.15  \\
    & R(2+1)D-18$_8$                    & 53.52     & 31.51    & 168.87    & 20.35      & 1.75   & 3.19     & 6.82      & 130.88 \\
    & R(2+1)D-18$_{16}$                 & 59.29     & 31.51    & 215.44    & 40.71      & 0.83   & 1.82     & 3.77      & 75.81  \\
    & Slow-8×8-R50                      & 67.42     & 32.45    & 266.04    & 54.87      & 0.38   & 1.34     & 4.31      & 61.92  \\
    & SlowFast-8×8-R50                  & 68.45     & 66.25    & 344.84    & 66.25      & 0.34   & 0.87     & 2.72      & 30.72  \\ 
    & SlowFast-4×16-R50                 & 67.06     & 34.48    & 260.51    & 36.46      & 0.55   & 1.33     & 3.43      & 41.28  \\ %\cline{2-9}
    & X3D-L                             & 69.29     & 6.15     & 240.66    & 19.17      & 0.25   & 0.19     & 4.78      & 36.37  \\
    & X3D-M                             & 67.24     & 3.79     & 126.29    & 4.97       & 0.83   & 1.47     & 17.47     & 116.07 \\
    & X3D-S                             & 64.71     & 3.79     & 61.29     & 2.06       & 2.23   & 2.68     & 42.02     & 276.45 \\
    & X3D-XS                            & 59.37     & 3.79     & 28.79     & 0.64       & 8.26   & 8.20     & 135.39    & 819.87 \\
    \midrule
    \parbox[t]{1mm}{\multirow{11}{*}{\rotatebox[origin=c]{90}{Frame}}} & RCU$_8$~\cite{singh2019recurrent}$^\dagger$     & 53.40     & 12.80     & - & 4.71 & - & - & - & - \\
    & \textit{Co}I3D$_{8}$              & 59.58     & 28.04    & 235.87    & 5.68       & 3.00   & 2.41     & 14.88     & 125.59 \\
    & \textit{Co}I3D$_{64}$             & 56.86     & 28.04    & 236.08    & 5.68       & 3.15   & 2.41     & 14.89     & 126.32 \\
    & \textit{Co}Slow$_{8}$             & 65.90     & 32.45    & 175.98    & 6.90       & 2.80   & 1.60     & 6.18      & 113.77 \\
    & \textbf{\textit{Co}Slow$_{64}$}    & \textbf{73.05}  & \textbf{32.45}  & \textbf{176.41}  & \textbf{6.90}   & \textbf{2.92}   & \textbf{1.60}  & \textbf{6.19} & \textbf{102.00}  \\
    & \textit{Co}X3D-$\text{L}_{16}$    & 63.03     & 6.15     & 184.29    & 1.25       & 2.30   & 0.99     & 25.17     & 206.65 \\
    & \textbf{\textit{Co}X3D-$\text{L}_{64}$}    & \textbf{71.61}     & \textbf{6.15}    & \textbf{184.37}     & \textbf{1.25}      &  \textbf{2.30}   & \textbf{0.99}       & \textbf{27.56}       & \textbf{217.53}  \\
    & \textit{Co}X3D-$\text{M}_{16}$    & 62.80     & 3.79    & 68.88     & 0.33      & 7.57   & 7.26       & 88.79      & 844.73  \\
    & \textbf{\textit{Co}X3D-$\text{M}_{64}$}    & \textbf{71.03}     & \textbf{3.79}    & \textbf{68.96}    & \textbf{0.33}      & \textbf{7.51}   & \textbf{7.04}       & \textbf{86.42}      & \textbf{796.32}  \\
    & \textit{Co}X3D-$\text{S}_{13}$    & 60.18     & 3.79    & 41.91     & 0.17      & 13.16  & 11.06      & 219.64      & 939.72 \\
    & \textbf{\textit{Co}X3D-$\text{S}_{64}$}    & \textbf{67.33}     & \textbf{3.79}    & \textbf{41.99}     & \textbf{0.17}      & \textbf{13.19}  & \textbf{11.13}     & \textbf{213.65}      & \textbf{942.97} \\
    \bottomrule
\end{tabular}
}
\end{center}
\caption{
    \textbf{Kinetics-400 benchmark}. The noted accuracy is the single clip or frame top-1 score using RGB as the only input-modality. 
    The performance was evaluated using publicly available pre-trained models without any further fine-tuning. %for either regular and continual networks.
    For speed comparison, predictions per second denote frames per second for the \textit{Co}X3D models and clips per second for the remaining models. Throughput results are the mean of 100 measurements. 
    Pareto-optimal models are marked with bold.
    Mem. is the maximum allocated memory during inference noted in megabytes.
    $^\dagger$Approximate FLOPs derived from paper (see \cref{apx:rcu}).
}
\label{tab:benchmark-kinetics400}
\vspace{-9mm}
\end{table}


\vspace{-2mm}
\subsection{Transfer from regular to Continual CNNs} \label{exp:transfer-reg-to-co}
\vspace{-1mm}
% There is a one-to-one correspondence between the weights in a regular CNN and the continual version of the same network.
% However, because Continual CNNs do not utilise zero-padding, there is a computational discrepancy between them.
To gauge direct transferability of 3D CNN weights, we implement continual versions of various 3D CNNs and initialise them with their publicly available weights for Kinetics-400~\cite{kay2017kinetics} and Charades~\cite{sigurdsson2016charades}. %, and \hl{Something-Something V2}~\cite{goyal2017ssv2}.
While it is common to use an ensemble prediction from multiple clips to boost video-level accuracy on these benchmarks, we abstain from this, as it doesn't apply to online-scenarios. Instead, we report the single-clip/frame model performance. 

\paragraph{Kinetics-400.}
%To gauge the direct transferability of knowledge to Continual CNNs, we initialise a set of Continual X3D (\textit{Co}X3D) networks with Kinetics-400~\cite{kay2017kinetics} pre-trained X3D network weights~\cite{feichtenhofer2020x3d}. 
We evaluate the X3D network variants XS, S, M, and L on the test set using one temporally centred clip from each video. 
The XS network is omitted in the transfer to \textit{Co}X3D, given that it is architecturally equivalent to S, but with fewer frames per clip. 
In evaluation on Kinetics-400, we faced the challenge that videos were limited to 10 seconds. 
Due to the longer transient response of Continual CNNs (see \cref{sec:exp-transient}) and low frame-rate used for training X3D models ($5.0, 6.0, 6.0$ FPS for S, M, and L), the video-length was insufficient to reach steady-state for some models.
As a practical measure to evaluate near steady-state, we repeated the last video-frame for a padded video length of $\approx80\%$ of the network receptive field as a heuristic choice.
The Continual CNNs were thus tested on the last frame of the padded video and initialised with the prior frames. 
The results of the X3D transfer are shown in \cref{tab:benchmark-kinetics400} and \cref{fig:test-acc-vs-flops}.

% It should be noted, that the \textit{Co}X3D models were all operating within their transient response (discussed in \cref{sec:init} and \cref{sec:exp-transient}), due to the limited number of available frames in the Kinetics dataset ($23, 50, 60, 60$ frames for the XS, S, M, and L models respectively as specified by the training frate-rate hyperparameter of the X3D models, i.e. $2.3, 5.0, 6.0, 6.0$ FPS) and the large temporal receptive field ($60, 69, 72, 130$ frames) of \textit{Co}X3D networks.

% We then finetune each network to assess how well this recovers performance.
% For optimisation we employ Stochastic Gradient Descent with momentum, weight decay, and discriminative learning rate~\cite{howard2018universal} which linearly decreases from the last layer ($L$) to the initial layer by a factor 0.01, i.e. $\eta^{(1)} = 0.01 \eta^{(L)}$. 
% The learning rate is determined for each network using the procedure described in \cite{smith2017cyclical} and scheduled according to a cyclic slanted triangular schedule, with $\eta_\text{base} = \eta_\text{max}/4$. Data augmentation and sampling was conducted as in \cite{feichtenhofer2020x3d}.

For all networks, the transfer from regular to Continual 3D CNN results in significant computational savings. 
For the S, M, and L networks the reduction in FLOPs is 12.1$\times$, 15.1$\times$, and 15.3$\times$ respectively.
The savings do not quite reach the clip sizes since the final pooling and prediction layers are active for each frame.
% These savings are less than the clip size due to the final pooling and prediction layers, which are in use for each frame in the Continual CNN, but only once per clip in regular CNNs.
As a side-effect of the transfer from zero-padded regular CNN to Continual CNN without zero-padding, we see a notable reduction in accuracy. 
% This reduction, however, may be exaggerated by the transient response of the \textit{Co}3D CNNs as discussed in \cref{sec:exp-transient}.
This is easily improved by using an extended pooling size for the network (discussed in \cref{sec:design-considerations} and in \cref{sec:exp-extended-receptive-fields}). 
Using a global average pooling with temporal kernel size 64, we improve the accuracy of X3D by 2.6\%, 3.8\%, and 2.3\% in the Continual S, M, and L network variants.
As noted, Kinetics dataset did not have sufficient frames to fill the temporal receptive field of all models in these tests.
% The results in \cref{tab:benchmark-kinetics400} are thus lower than what may be expected for online scenarios given enough frames. 
We explore this further in Sections \ref{sec:exp-transient} and \ref{sec:exp-extended-receptive-fields}. 



\vspace{-3mm}
\paragraph{Charades.}
To showcase the generality of the approach, we repeat the above described procedure with another 3D CNN, the \textit{Co}Slow network~\cite{feichtenhofer2019slowfast}. 
We report the video-level mean average precision (mAP) of the validation split alongside the FLOPs per prediction in \cref{tab:charades}. Note the accuracy discrepancy between 30 view (10 temporal positions with 3 spatial positions each) and 1 view (spatially and temporally centred) evaluation. As observed on Kinetics, the \textit{Co}Slow network reduces the FLOPs per prediction proportionally with the original clip size (8 frames), and can recover accuracy by extending the global average pool size.

% \paragraph{AVA v2.2.}
% Following standard practice~\cite{gu2018ava}, we report the frame-level mean average precision (mAP) using an IoU threshold of 0.5 for a \textit{Co}Slow network initialised on pre-trained Slow weights. As presented in \cref{tab:ava}, \hl{continue here...}

\begin{table}[t]
	\begin{center}
    % \resizebox{\linewidth}{!}{
	\begin{tabular}{llccc}
		\toprule
% 		& \multirow{2}{*}{\textbf{Model}} & \textbf{FLOPS (G)}        & \textbf{Charades}  \\
% 		                                                            \cline{4-4}
% 		                                & & \textbf{$\times$ views}   & mAP (\%) \\
        & \textbf{Model}                & \textbf{FLOPs (G) × views}        & \textbf{mAP (\%)}  \\
		\midrule
        \parbox[t]{1mm}{\multirow{3}{*}{\rotatebox[origin=c]{90}{Clip}}}
		& Slow-8×8~\cite{feichtenhofer2019slowfast}  
		                                & $54.9 \times 30$          & 39.0      \\
        & Slow-8×8~\cite{feichtenhofer2019slowfast}$^\dagger$     
                                        & $54.9 \times 1$           & 21.4      \\
        & Slow-8×8 (ours)    & $54.9 \times 1$           & 24.1      \\
        % & SlowFast$_{8 \times 8}$~\cite{feichtenhofer2019slowfast}          
        %                                 & $ 42.1 \times 30$         & 42.1      \\ % Our results on Charades: 37.566 % mAP
        % & SlowFast$_{8 \times 8}$~\cite{feichtenhofer2019slowfast}$^*$       
        %                                 & $ 42.1 \times 1$          & 22.4      \\
        % & SlowFast$_{8 \times 8}$ (ours)&                           & -         \\
% 		\noalign{\vskip 2pt} \hdashline \noalign{\vskip 2pt}  

		\midrule
		\parbox[t]{1mm}{\multirow{2}{*}{\rotatebox[origin=c]{90}{Fr.}}}
        & \textit{Co}Slow$_8$             & $6.9 \times 1$          & 21.5      \\
        & \textit{Co}Slow$_{64}$          & $6.9 \times 1$          & 25.2      \\
        % & \textit{Co}SlowFast$_{8}$       &                         & -         \\
        % & \textit{Co}SlowFast$_{64}$      &                         & -         \\
        
		\bottomrule
	\end{tabular}
% 	}
% 	\vspace{-6pt}
	\end{center}
	\caption{
	    \textbf{Charades benchmark.} Noted are the FLOPs $\times$ views and video-level mean average precision (mAP) on the validation set using pre-trained model weights. 
        $^\dagger$Results achieved using the publicly available SlowFast code~\cite{feichtenhofer2019slowfast}.
	}
% 	\ificcvfinal \setlength{\belowcaptionskip}{-25pt} \fi
% 	\vspace{-15pt}
	\label{tab:charades}
	\vspace{-10mm}
\end{table}

% \begin{table}
% 	\begin{center}
%     % \resizebox{\linewidth}{!}{
% 	\begin{tabular}{llccc}
% 		\toprule
% % 		& \multirow{2}{*}{\textbf{Model}} & \textbf{FLOPS (G)}        & \textbf{Charades}  \\
% % 		                                                            \cline{4-4}
% % 		                                & & \textbf{$\times$ views}   & mAP (\%) \\
%         & \textbf{Model}                & \textbf{FLOPs (G)}        & \textbf{mAP (\%)}  \\
% 		\midrule
%         \parbox[t]{1mm}{\multirow{2}{*}{\rotatebox[origin=c]{90}{Clip}}}
%         & Slow$_{4 \times 16}$~\cite{feichtenhofer2019slowfast}\footnotemark[\value{footnote}]           
%                                         & 41.8                     & 19.0      \\
%         & Slow$_{4 \times 16}$ (ours)    & 41.8                         & 18.9       \\
%         % & SlowFast$_{8 \times 8}$~\cite{feichtenhofer2019slowfast}$^*$       
%         %                                 &                           & 14.9      \\
%         % & SlowFast$_{8 \times 8}$ (ours)&                           & -         \\
% % 		\noalign{\vskip 2pt} \hdashline \noalign{\vskip 2pt}  

% 		\midrule
% 		\parbox[t]{1mm}{\multirow{2}{*}{\rotatebox[origin=c]{90}{Fr.}}}
%         & \textit{Co}Slow$_4$             & 10.5                        & 12.1         \\
%         & \textit{Co}Slow$_{64}$          &  10.5                       & 13.5         \\
%         % & \textit{Co}SlowFast$_{8}$       &                         & -         \\
%         % & \textit{Co}SlowFast$_{64}$      &                         & -         \\
        
% 		\bottomrule
% 	\end{tabular}
% % 	}
% 	\vspace{-6pt}
% 	\caption{
% 	    \textbf{AVA v2.2 benchmark.} Noted are the FLOPs (excluding detection) and frame-level mAP @ IoU 0.5 on the validation set using pre-trained Slow weights.
% 	}
% % 	\ificcvfinal \setlength{\belowcaptionskip}{-25pt} \fi
% 	\vspace{-15pt}
% 	\label{tab:ava}
% 	\end{center}
% \end{table}

\renewcommand*{\thefootnote}{\arabic{footnote}}

\vspace{-3mm}
\subsection{Ablation Experiments}
\vspace{-1mm}
As described in \cref{sec:init}, Continual CNNs exhibit a transient response during their up-start.
In order to gauge this response, we perform ablations on the Kinetics-400 validation set, this time sampled at 15 FPS to have a sufficient number of frames available.
This corresponds to a data domain shift~\cite{wang2018deep} relative to the pre-trained weights, where time advances slower.

\vspace{-3mm}
\subsubsection{Transient response of Continual CNNs.} \label{sec:exp-transient}
Our expected upper bound is given by the baseline X3D network 1-clip accuracy at 15 FPS.
The transient response is measured by varying the number of prior frames used for initialisation before evaluating a frame using the \textit{Co}X3D model.
Note that temporal center-crops of size $T_{\text{init}} + 1$, where $T_{\text{init}}$ is the number of \text{initialisation frames}, are used in each evaluation to ensure that the frames seen by the network come from the centre. 
This precaution counters a data-bias, we noticed in Kinetics-400, namely that the start and end of a video are less informative and contribute to worse predictions than the central part. 
We found results to vary up to 8\% for a X3D-S network evaluated at different video positions.
The experiment is repeated for two initialisation schemes, ``zeros'' (used in other experiments) and ``replicate'', and two model sizes, S and M.
The transient responses are shown in \cref{fig:transient_response_results}.

\begin{figure}[b!]
    % \vspace{-4mm}
    \centering
    % \includegraphics[width=1.1\linewidth]{figures/transient_response.pdf}
    \begin{subfigure}[b]{0.7\linewidth}
        \centering
        \includegraphics[width=1.0\linewidth]{figures/transient_response_s.pdf}
        \\[-2ex]
        \caption{\textit{Co}X3D-S}
        \label{fig:transient-s}
    \end{subfigure}
    \begin{subfigure}[b]{0.7\linewidth}
        \centering
        \includegraphics[width=1.0\linewidth]{figures/transient_response_m.pdf}
        \\[-2ex]
        \caption{\textit{Co}X3D-M}
        \label{fig:transient-m}
    \end{subfigure}
    \\[-1ex]
    \setlength{\belowcaptionskip}{-5pt}
	\caption{
	\textbf{Transient response} for Continual X3D-\{S,M\} on the Kinetics-400 val at 15 FPS.
	Dotted horizontal lines denote X3D validation accuracy for 1-clip predictions. 
	Black circles highlight the theoretically required initialisation frames.
	}
    \label{fig:transient_response_results}
    \vspace{-2mm}
\end{figure}

For all responses, the first $\approx$25 frames produce near-random predictions, before rapidly increasing at 25$-$30 frames until a steady-state is reached at $49.2\%$ and $56.2\%$ accuracy for S and M.
Relative to the regular X3D, this constitutes a steady-state error of $-1.7\%$ and $-5.8\%$.
% for the \textit{Co}X3D S and M models respectively as compared with the regular X3D models (which lie at $51.0\%$ and $62.1\%$).
Comparing initialisation schemes, we see that the ``replicate'' scheme results in a slightly earlier rise. %($\approx$2 frames) 
% without overshoot.
The rise sets in later for the ``zeros'' scheme, but exhibits a sharper slope, topping with peaks of $51.6\%$ and $57.6\%$ at 41 and 44 frames seen as discussed in \cref{sec:init}.
This makes sense considering that the original network weights were trained with this exact amount of zero-padding. Adding more frames effectively replaces the padded zeros and causes a slight drop of accuracy in the steady state, where the accuracy settles at the same values as for the ``replication'' scheme. 
% This overshoot response was unexpected, and our best conjecture is that the network benefits from some zero states because the regular network, from which is inherited its parameters, was trained using zero padding.
% While this overshoot could be exploited to inflate the measured accuracy, we abstain from this practice and report only accuracy for steady-state or the latest-possible frame while using ``replicate'' padding in case of insufficient frames in the video.
% Another point of surprise was the relatively quick settling time, which is significantly lower ($\approx$35\%) than the temporal receptive field of the network. 


\vspace{-3mm}
\subsubsection{Extended receptive field.} \label{sec:exp-extended-receptive-fields}
Continual CNNs experience a negligible increase in computational cost when larger temporal receptive field are used (see \cref{sec:design-considerations}). 
For \textit{Co}X3D networks, this extension can be trivially implemented by increasing the temporal kernel size of the last pooling layer.
In this set of experiments, we extend \textit{Co}X3D-$\{\text{S,M,L}\}$ to have temporal pooling sizes 32, 64, and 96, and evaluate them on the Kinetics-400 validation set sampled at 15 FPS.
% Note, once again, that the network weights were trained on a different sampling rate. % than the one used for evaluation. 
The Continual CNNs are evaluated at frames corresponding to the steady state.


\begin{table}[b!]
	\begin{center}
	
    % \resizebox{\linewidth}{!}{
	\begin{tabular}{lllcrr}
		\toprule
		\textbf{Model}  & \textbf{Size} & \textbf{Pool} & \textbf{Acc.} & \textbf{FLOPs (K)} & \textbf{Rec. Field} \\
		\midrule
		\multirow{3}{*}{X3D} 
		                & S             & 13            & 51.0          & 2,061,366 %2.061.365.744  
		                                                                                    & 13                 \\
		                & M             & 16            & 62.1          & 4,970,008 %4.970.008.352  
		                                                                                    & 16                 \\
		                & L             & 16            & 64.1          & 19,166,052 % 19.166.052.038 
		                                                                                    & 16                 \\
		\midrule
% 		\multirow{5}{*}{\textit{Co}X3D} 
		    &\multirow{5}{*}{S}         & 13            & 49.2          & 166,565 %166565305    
		                                                                                    & 69                 \\
		                &               & 16            & 50.1          & 166,567 %166566601    
		                                                                                    & 72                 \\
		                &               & 32            & 54.7          & 166,574 %166573513    
		                                                                                    & 88                 \\
		                &               & 64            & 59.8          & 166,587 %166587337    
		                                                                                    & 120                \\
		                &               & 96            &\textit{61.8}  & 166,601 %166601161    
		                                                                                    & 152                \\
		                \cline{2-6}
 		\multirow{4}{*}{\textit{Co}X3D}
		    &\multirow{4}{*}{M}         & 16            & 56.3          & 325,456 %325456057   
		                                                                                    & 72                 \\
		                &               & 32            & 60.7          & 325,463 %325462969    
		                                                                                    & 88                 \\
		                &               & 64            & 64.9          & 325,477 %325476793    
		                                                                                    & 120                \\
		                &               & 96            &\textit{67.3 } & 325,491 %325490617    
		                                                                                    & 152                \\
		                \cline{2-6}
		    &\multirow{4}{*}{L}         & 16            & 53.0          & 1,245,549 %1245549427  
		                                                                                    & 130                \\
		                &               & 32            & 58.5          & 1,245,556 %1245556339  
		                                                                                    & 146                \\
		                &               & 64            &\textit{64.3}  & 1,245,570 %1245570163  
		                                                                                    & 178                \\
		                &               & 96            &\textit{66.3}  & 1,245,584 %1245583987  
		                                                                                    & 210                \\

		\bottomrule
	\end{tabular}
	\end{center}
% 	}
% 	\vspace{-8pt}
	\caption{\textbf{Effect of extending pool size}. %Top-1 accuracy on the Kinetics-400 validation set at 15 FPS for models using pre-trained X3D weights. 
	Note that the model weights were trained at different sampling rates than evaluated at (15 FPS), resulting in a lower top-1 val. accuracy.
% 	Here, the kilo floating point operations (KFLOPs) are noted alongside the temporal kernel size for the global average pooling, and the resulting receptive field in the temporal dimension.
	\textit{Italic numbers} denote measurement taken within the transient response due to a lack of frames in the video-clip.
	}
% 	\ificcvfinal \setlength{\belowcaptionskip}{-25pt} \fi
% 	\vspace{-15pt}
	\label{tab:extended_window_size}
% 	\vspace{-8mm}
\end{table}

\cref{tab:extended_window_size} shows the measured accuracy and floating point operations per frame (\textit{Co}X3D) / clip (X3D) as well as the pool size for the penultimate network layer (global average pooling) and the total receptive field of the network in the temporal dimension. 
% A corresponding visual depiction is shown in \cref{fig:val-acc-extended-frames}.
As found in \cref{exp:transfer-reg-to-co}, each transfer results in significant computational savings alongside a drop in accuracy.
Extending the kernel size of the global average pooling layer increases the accuracy of the Continual CNNs by 11.0$-$13.3$\%$ for 96 frames relative the original 13$-$16 frames, surpassing that of the regular CNNs.
Lying at 0.017$-$0.009$\%$, the corresponding computational increases can be considered negligible. 


% \begin{figure}
%     \centering
%     \includegraphics[width=0.8\linewidth]{figures/val-acc-vs-flops.pdf}
%     % \vspace{-15pt}
% 	\caption{
% 	\textbf{Accuracy versus FLOPs for extended receptive fields} on the Kinetics-400 validation set at 15 FPS.
% 	The global average pool size along the temporal dimension is indicated in each point, and
% 	the arrows denote a one-to-one transfer from regular to Continual X3D without fine-tuning.
% % 	For regular X3D models, the FLOPs per \textit{clip} $\ssquare$ are noted, while the FLOPs per \textit{frame} $\sbullet$ are shown for the continual version \textit{Co}X3D. 
% 	}
%     % \vspace{-10pt}
%     \label{fig:val-acc-extended-frames}
% \end{figure}



% \begin{table}
% 	\begin{center}
% 	\caption{\textbf{Effect of extending pool size}. Top-1 accuracy on Kinetics-400 validation set at 15 FPS is shown for models using pre-trained X3D weights. 
% 	Note that the X3D models were trained at different sampling rates, resulting in a lower accuracy than was originally reported in \cite{feichtenhofer2020x3d}.
% 	\textit{Italic numbers} for \textit{Co}X3D-L denote measurements taken within the transient response due to a lack of frames in the video-clip.
% 	}
% 	\label{tab:extended_window_size}
% % 	\resizebox{\textwidth}{!}{
% 	\begin{tabular}{cccccc}
% 		\toprule
% 		    \textbf{Model}
%             & \multicolumn{4}{c}{\textbf{\textit{Co}X3D pool size}}
% 		    & \multirow{2}{*}{\textbf{X3D}}
%             \\
%             \cline{2-5}
%             \textbf{size}
%             & $16$
%             & $32$
% 		    & $64$
% 		    & $96$
% 		    \\
% 		\midrule
% 		S
% 		  %  & 49.23 % 13 
%             & 50.1 % 16 
%             & 54.7 % 32 
%             & 59.8 % 64 
%             & 61.8 % 96
% 		    & 51.0
%     		\\
%     	M
%             & 56.3 % 16
%             & 60.7 % 32
%             & 64.9 % 64
%     		& 67.3 % 96
%     	    & 62.1
%     		\\
%     	L
%             & -
%             & 58.46 % 32
%             & \textit{64.27} % 64
%             & \textit{66.32} % 96
%     	    & 64.1
    	    
%     		\\
% 		\bottomrule
% 	\end{tabular}\\
% 	\end{center}
% % 	}
% \end{table}




\subsection{Inference benchmarks} \label{sec:inference-benchmarks}
Despite their high status in activity recognition leader-boards~\cite{paperswithcode2021kinetics400},
it is unclear how recent 3D CNNs methods perform in the online setting, where speed and accuracy constitute a necessary trade-off.
% Specifically, many methods sample multiple clips from different spatial and temporal positions in a video and average the predictions to produce their best results.
% Specifically, many methods make use of multi-clip testing to produce their best results.
% This is done by sampling multiple clips from different spatial and temporal positions in a video, and averaging the predictions over all clips.
% To avoid the high computational cost of this practice, a reasonable tactic in the online setting is to make predictions from only a single clip.
% The number of FLOPs as a proxy for inference speed is occasionally noted, but 
To the best of our knowledge, there has not yet been a systematic evaluation of throughput for these video-recognition models on real-life hardware.
%
In this set of experiments, we benchmark the FLOPs, parameter count, maximum allocated memory and 1-clip/frame accuracy of I3D~\cite{carreira2017quo}, R(2+1)D~\cite{tran2018closer}, SlowFast\cite{sovrasov2020ptflops}, X3D~\cite{feichtenhofer2020x3d}, \textit{Co}I3D, \textit{Co}Slow, and \textit{Co}X3D.
To gauge achievable throughputs at different computational budgets, networks were tested on four hardware platforms as described in Appendix B.
% A CPU core of a MacBook Pro (16-inch 2019 2.6 GHz Intel Core i7); 
% Nvidia Jetson TX2;
% Nvidia Jetson Xavier;
% and a Nvidia RTX 2080 Ti GPU (on server with Intel XEON Gold processors).
% For more details on the benchmarking setup, we refer the reader to Appendix B.

As seen in the benchmark results found in \cref{tab:benchmark-kinetics400},
the limitation to one clip markedly lowers accuracy compared with the multi-clip evaluation published in the respective works~\cite{carreira2017quo}, \cite{tran2018closer}, \cite{feichtenhofer2019slowfast}, \cite{feichtenhofer2020x3d}.
Nontheless, the Continual models with extended receptive fields attain the best accuracy/speed trade-off by a large margin. 
For example, \textit{Co}X3D-$\text{L}_{64}$ on the Nvidia Jetson Xavier achieves an accuracy of 71.3\% at 27.6 predictions per second compared to 67.2\% accuracy at 17.5 predictions per second for X3D-M while reducing maximum allocated memory by 48\%! 
%
Confirming the observation in \cite{ma2018shufflenetv2}, we find that the relation between model FLOPs and throughput varies between models, with better ratios attained for simpler models (e.g., I3D) than for complicated ones (e.g., X3D). 
This relates to different memory access needs and their cost. Tailor-made hardware could plausibly reduce these differences.
% See also Appendix C.
Supplementary visualisation of the results in \cref{tab:benchmark-kinetics400} are found in Appendix C.
% For \textit{Co}X3D, the throughput improvements compared to X3D are 5.9$-$9.2$\times$ on CPU, 4.2$-$5.2$\times$ on TX2, 5.0$-$7.2$\times$ on Xavier and 5.3$-$6.7$\times$ on RTX.
% Here, the lower numbers are for X3D-S versus \textit{Co}X3D-S, since X3D-S has a lower clip size of 13 compared to the other models which take 16 frames per clip.
%
% The speed evaluation results are approximately (inversely log-log) proportional to the measured FLOPs. 
% They are, however skewed to the low side for X3D models, confirming the observation in \cite{ma2018shufflenetv2} that FLOPs are not always an accurate proxy for the inference speed on real-life hardware due to differences in memory access cost.
% Across hardware platforms, the Continual models with extended receptive fields attain the best accuracy/speed trade-off by a large margin. 
% For example, \textit{Co}X3D-$\text{L}_{64}$ achieves an accuracy of 71.3\% at 27.6 frames per second on the Nvidia Jetson Xavier, compared to 67.2\% accuracy at 17.5 clips per second for X3D-M. 






% \subsection{Transfer to new datasets}

% Too short:
% - UCF101: 60% of videos over 5 sec (125 frames)
% - HMDB51: 78% of videos over 5 sec (125 frames)
% - Something-Something-V2
% - Jester (mostly around 35 frames)
% - YoutTube-Cars

% OK
% - Charades: But comparison with other methods is tricky (they show different stuff)

% For real-life use-cases, it may be of limited interest to predict among the 400 categories of the Kinetics dataset.
% A better demonstration of the utility of a neural network is the transfer of knowledge to a new dataset.
% To test this, we perform standard parameter transfer learning from Kinetics-400 to Charades, with all but the prediction layer pretrained on Kinetics-400. 
% The transfer is conducted on the regular X3D networks as well as on \textit{Co}X3D networks that are initilised on the same Kinetics-400 pre-trained weights.
% % Optimisation is conducted as described in \cref{exp:transfer-reg-to-co}.
% For optimisation we employ Stochastic Gradient Descent with momentum, weight decay, and discriminative learning rate~\cite{howard2018universal} which linearly decreases from the last layer ($L$) to the initial layer by a factor 0.01, i.e. $\eta^{(1)} = 0.01 \eta^{(L)}$. 
% The learning rate is determined for each network using the procedure described in \cite{smith2017cyclical} and scheduled according to a cyclic slanted triangular schedule, with $\eta_\text{base} = \eta_\text{max}/4$. Data augmentation and sampling was conducted as in \cite{feichtenhofer2020x3d}.






% \begin{table}[!htbp]
% \label{tab:lwhar:model-resolution}
% \begin{center}
% \begin{tabular}{lccc}
%     \toprule
%     \textbf{Model}  & \textbf{Num. Frames}  & \textbf{Frame Rate}   & \textbf{Num. Pixels} \\
%     \midrule
%     X3D-XS          & 4                     & 12                    & $160^2$         \\
%     X3D-S           & 13                    & 6                     & $160^2$         \\
%     X3D-M           & 16                    & 5                     & $224^2$         \\
%     X3D-L           & 16                    & 5                     & $312^2$         \\
%     \bottomrule
% \end{tabular}
% \end{center}
% \caption{Temporal and spatial resolutions for different X3D model configurations.}
% \end{table}








% \subsection{Training from scratch}
% \hl{Consider removing}

% Is convergence faster?
% Do we experience issues?
% Or is there a speed up because we can use multiple consecutive frames averaged for back-propagation. 
% Here, a batch becomes a batch of batches.
% We expect a speed-up in training because a given input video, which has to be loaded from disk due to the size of the dataset, can be reused multiple times. This is akin to Data Echoing~\cite{choi2019faster}.



% \begin{table*}[!htbp]
% \label{tab:benchmark_x3d}
% \begin{center}
% % \resizebox{\textwidth}{!}{
% \begin{tabular}{lcccccc}
%     \toprule
%     \textbf{Model}  &\textbf{CPU (fps)} & \textbf{TX2 (fps)}    & \textbf{Xavier (fps)} & \textbf{RTX 2080 Ti (fps)}    & \textbf{FLOPS (G)}    & \textbf{Params (M)} \\
%     \midrule
%     ReX3D-L         & 2.29 $\pm$ 0.08   & -                     & -                     & -                             & 1.49                  & 6.15               \\
%     ReX3D-M         & 7.53 $\pm$ 0.23   & -                     & -                     & -                             & 0.39                  & 3.79               \\
%     ReX3D-S         & 12.49 $\pm$ 0.75  & -                     & -                     & -                             & 0.20                  & 3.79               \\
%     ReX3D-XS        & 13.14 $\pm$ 0.71  & -                     & -                     & -                             & 0.20                  & 3.79               \\
%     \bottomrule
% \end{tabular}
% % }
% \end{center}
% \caption{
%     \textbf{Inference speed and model size for ReX3D}. 
%     Frames per second (fps) $\pm$ standard deviation are reported for on-device measurements. 
% }
% \end{table*}





% \begin{table}[!htbp]
% \label{tab:benchmark-kinetics600}
% \begin{center}
% \resizebox{\linewidth}{!}{
% \begin{tabular}{llccc} %{llccccccc}
%     \toprule
%     &\textbf{Model}     %\multirow{2}{*}{\textbf{Model}} 
%     &\textbf{Acc. (\%)}     %\multirow{2}{*}{\textbf{Acc. (\%)}} 
%     &\textbf{Params (M)}        %\multirow{2}{*}{\textbf{Params (M)}}    
%     &\textbf{FLOPs (M)}     %\multirow{2}{*}{\textbf{FLOPs (M)}}  
%     % &\multicolumn{4}{c}{\textbf{Speed (it/s)}}
%     %     \\ \cline{6-9}
%     %     &&&&&\textbf{CPU} & \textbf{TX2}  &\textbf{Xavier} &\textbf{RTX 2080 Ti}
%     \\
%     \midrule
%     \parbox[t]{1mm}{\multirow{5}{*}{\rotatebox[origin=c]{90}{Clip}}} 
%     & I3D~\cite{carreira2018kinetics}           & 71.90                 & 12.90                         & 88202                     \\ %& -               & -                     & -                     & -                 \\
%     & 3D-ShuffleNetV1 2.0x~\cite{kopulku2019resource}  & 56.84          & 4.78                         & 393                   \\ %& -                 & -                     & -                     & -                 \\ %\cline{2-9}
%     & 3D-ShuffleNetV1 1.5x~\cite{kopulku2019resource}   & 52.75          & 2.92                         & 235                   \\ %& -                 & -                     & -                     & -                 \\ 
%     & 3D-ShuffleNetV1 1.0x~\cite{kopulku2019resource}  & 45.31          & 1.52                         & 125                   \\ %& -                 & -                     & -                     & -                 \\ 
%     & 3D-ShuffleNetV1 0.5x~\cite{kopulku2019resource} & 35.51          & 0.55                         & 42                   \\ %& -                 & -                     & -                     & -                 \\ 
%     \midrule
%     \parbox[t]{1mm}{\multirow{7}{*}{\rotatebox[origin=c]{90}{Frame}}} 
%     & D-ResNet-101-LSTM~\cite{kopuklu2020dissected} & 69.17          & 62.12                         & 2760                  \\ %& -                 & -                     & -                     & -                 \\ 
%     & D-ResNet-50-LSTM~\cite{kopuklu2020dissected} & 68.22          & 33.12                         & 1337                   \\ %& -                 & -                     & -                     & -                 \\ 
%     & D-ResNet-18-LSTM~\cite{kopuklu2020dissected} & 62.02          & 15.74                      & 602                  \\ %& -       & -                & -                     & -                 \\ 
%     \cline{2-5}
%     & \textit{Co}3D-ShuffleNetV1 2.0x  & -          & -                         & -                                    \\ %& -        & -             & -                     & -                 \\
%     & \textit{Co}3D-ShuffleNetV1 1.5x   & -          & -                         & -                                    \\ %& -       & -              & -                     & -                 \\ 
%     & \textit{Co}3D-ShuffleNetV1 1.0x  & -          & -                         & -                                    \\ %& -        & -             & -                     & -                 \\ 
%     & \textit{Co}3D-ShuffleNetV1 0.5x & -          & -                         & -                                  \\ %& -         & -              & -                     & -                 \\ 
%     \bottomrule
% \end{tabular}
% }
% \end{center}
% \caption{
%     \textbf{Benchmark of state-of-the-art methods on Kinetics-600}. Noted is the single clip/frame top-1 accuracy on Kinetics-600. 
% }
% \end{table}


