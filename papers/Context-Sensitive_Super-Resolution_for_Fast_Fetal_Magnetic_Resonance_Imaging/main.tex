%%%%%%%%%%%%%%%%%%%%%%% file typeinst.tex %%%%%%%%%%%%%%%%%%%%%%%%%
%
% This is the LaTeX source for the instructions to authors using
% the LaTeX document class 'llncs.cls' for contributions to
% the Lecture Notes in Computer Sciences series.
% http://www.springer.com/lncs       Springer Heidelberg 2006/05/04
%
% It may be used as a template for your own input - copy it
% to a new file with a new name and use it as the basis
% for your article.
%
% NB: the document class 'llncs' has its own and detailed documentation, see
% ftp://ftp.springer.de/data/pubftp/pub/tex/latex/llncs/latex2e/llncsdoc.pdf
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\RequirePackage{amsmath}
\PassOptionsToPackage{table}{xcolor} % to fix conflict with tikz package
\documentclass[runningheads,a4paper]{llncs}
% \usepackage{amsmath}
% \usepackage{subfigure}
\usepackage[caption=false]{subfig}
\usepackage[capitalise]{cleveref}
\usepackage{amssymb}
\setcounter{tocdepth}{3}
\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{xcolor}
\usepackage{color}
\usepackage{tikz}
\usepackage{url}
\usepackage{multirow}
\usepackage{gensymb}
%\usepackage[table]{xcolor} 
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{array}
\usepackage{float}

\newcommand{\Mypm}{\mathbin{\tikz [x=1.4ex,y=1.4ex,line width=.1ex] \draw (0.0,0) -- (1.0,0) (0.5,0.08) -- (0.5,0.92) (0.0,0.5) -- (1.0,0.5);}}%
\newcommand{\todo}[1]{}
\renewcommand{\todo}[1]{{\color{red} TODO: {#1}}}

\urldef{\mailsa}\path|{****|
\urldef{\mailsb}\path|****|
\urldef{\mailsc}\path|****|    

\newcommand{\keywords}[1]{\par\addvspace\baselineskip
\noindent\keywordname\enspace\ignorespaces#1}

% \newlength{\tempdima}
% \newcommand{\rowname}[1]% #1 = text
% {\rotatebox{90}{\makebox[\tempdima][c]{\textbf{#1}}}}

% \newcounter{mysubfigure}[figure]
% \renewcommand{\thesubfigure}{\alph{mysubfigure}}
% \newcommand{\mycaption}[1]% #1 = caption
% {\refstepcounter{mysubfigure}\textbf{(\thesubfigure) }{\ignorespaces #1}}

% highlight text
\usepackage{etoolbox}
\usepackage{color,soul} 
\definecolor{yellow}{rgb}{1.0,1.0,0.5}
\definecolor{green}{rgb}{0.5,1.0,0.5}
\newcommand{\hlc}[2][yellow]{ {\sethlcolor{#1} \hl{#2}} }
\setlength\belowcaptionskip{-15pt}

\graphicspath{{./images/}}

\makeatletter
\let\origsection\section
\renewcommand\section{\@ifstar{\starsection}{\nostarsection}}

\newcommand\nostarsection[1]
{\sectionprelude\origsection{#1}\sectionpostlude}

\newcommand\starsection[1]
{\sectionprelude\origsection*{#1}\sectionpostlude}

\newcommand\sectionprelude{%
  \vspace{-2mm}
}

\newcommand\sectionpostlude{%
  \vspace{0mm}
}
\makeatother

\begin{document}

\mainmatter  % start of an individual contribution

% first the title is needed
\title{Context-Sensitive Super-Resolution for Fast Fetal Magnetic Resonance Imaging}

% a short form should be given in case it is too long for the running head
\titlerunning{Super-Resolution for Under-Sampled MRI}

%submission
%\author{--}
\author{Steven McDonagh$^{1}$ \and Benjamin Hou$^{1}$ \and Amir Alansary$^1$ \and Ozan Oktay$^1$ \and Konstantinos Kamnitsas$^1$ \and Mary Rutherford$^2$ \and Jo V. Hajnal$^2$ \and Bernhard Kainz$^{1,2}$}
%
\authorrunning{S. McDonagh et al.}
%\authorrunning{--}
% (feature abused for this document to repeat the title also on left hand pages)

% the affiliations are given next; don't give your e-mail address
% unless you accept that it will be published
%\institute{--}
\institute{
	$^1$Biomedical Image Analysis Group, Imperial College London \\
	$^2$Division of Imaging Sciences and Biomedical Engineering, King’s College London
	%\mailsa\\
	%\mailsb\\
	%\url{https://biomedia.doc.ic.ac.uk}
}

%
%\authorrunning{Paper ID \#553}
%\authorrunning{S. McDonagh \emph{et al.}}
% (feature abused for this document to repeat the title also on left hand pages)

% the affiliations are given next; don't give your e-mail address
% unless you accept that it will be published
%\institute{***} 
%\institute{Department of Computing, Imperial College London, UK} 
%\{steven.mcdonagh, a.alansary14, b.kainz\d.rueckert}@imperial.ac.uk 
%\toctitle{SR for efficient motion compensation}
%\tocauthor{S. McDonagh \emph{et al.}}
\maketitle

\begin{abstract}
\label{sec:abstract}
3D Magnetic Resonance Imaging (MRI) is often a trade-off between fast but low-resolution image acquisition and highly detailed but slow image acquisition. Fast imaging is required for targets that move to avoid motion artefacts. This is in particular difficult for fetal MRI. Spatially independent upsampling techniques, which are the state-of-the-art to address this problem, are error prone and disregard contextual information.
In this paper we propose a context-sensitive upsampling method based on a residual convolutional neural network model that learns organ specific appearance and adopts semantically to input data allowing for the generation of high resolution images with sharp edges and fine scale detail. 
By making contextual decisions about appearance and shape, present in different parts of an image, we gain a maximum of structural detail at a similar contrast as provided by high-resolution data.% while reducing computation time for the motion compensation process.
We experiment on $145$ fetal scans and show that our approach yields an increased PSNR of $1.25$ $dB$ when applied to under-sampled fetal data \emph{cf.} baseline upsampling. Furthermore, our method yields an increased PSNR of $1.73$ $dB$ when utilizing under-sampled fetal data to perform brain volume reconstruction on motion corrupted captured data. 
% \keywords{Fetal, MRI, Placenta, Segmentation}
\end{abstract}

\if 0 %% reworked abstract
%Magnetic Resonance (MR) image acquisition is often a trade-off between fast acquisition of Low-Resolution (LR) images or slow acquisition of High-Resolution (HR) images. The former is usually performed on actively moving targets to avoid motion artefacts in the image, which is particularly difficult in applications such as Fetal MRI. State-of-the-art spatially independent up-sampling techniques are able to address the problem, however, are more prone to errors as they disregard contextual information.
%In this paper, we propose a context-sensitive up-sampling method that’s based on a residual Convolutional Neuronal Network (CNN) model. We train this model to learn organ specific appearances, which allows it to adopt semantically to input data in order to generate high resolution images with sharp edges and fine scale detail. 
%By allowing the model to make contextual decisions about appearance and shape, that’s present in different parts of an image, we gain a maximum structural detail at a similar contrast as provided by high-resolution data. 
% while reducing computation time for the motion compensation process.
%We test on $145$ fetal scans and show that our approach yields an increased PSNR of $1.25$dB when applied to under-sampled fetal data. Furthermore, we also show that slice-wise comparison of low resolution and CNN up-sampled Slice-to-Volume Reconstruction (SVR) volumes yields an increased PSNR of $1.73$dB.
\fi

%============================================================================
%== INTRO ==================================================================
%============================================================================
\vspace{-2mm}
\section{Introduction}
\vspace{-1mm}
\label{sec:intro}
\parskip0pt
Currently, 3D imaging of moving objects is limited by the time it takes to acquire a single image. The slower an imaging modality is, the more likely motion induced artefacts will occur within and between individual slices of a 3D volume. 
Very fast imaging modalities like Computed Tomography are not always applicable because of harmful ionising radiation, and ultrasound often suffers from poor image quality. Thus, Magnetic Resonance Imaging (MRI) is usually the modality of choice when; large fields of view, high anatomical detail, and non-invasive imaging is required. MRI is often applied to image involuntary moving objects such as the beating heart and examination of the fetus in-utero. Motion compensation for cardiac imagining can be achieved through ECG gating.
% \cite{MRM:MRM22250}. 
However, fetal targets do not provide options for gated or tracked image acquisition to compensate for motion. Thus motion compensation is performed during post-processing of oversampled input spaces, usually involving the acquisition of orthogonally oriented stacks of slices~\cite{kainz2015fast}. Oversampling with high resolution (HR) slices causes long scan times, which is uncomfortable and risky for patients like pregnant women. This limits the possible number of scan sequences during examination. However, improving image resolution is key to improving accuracy, understanding of anatomy and assessment of organ size and morphology. Imaging at lower resolution increases acquisition speed, thus partly mitigating the likelihood for motion between individual slices but at the cost of missing structural detail that could render the scan inappropriate for diagnostic purposes. Due to signal-to-noise ratio (SNR) limitations, the acquired slices are usually also \emph{thick} compared to the in-plane resolution and thus negatively influence the visualization of anatomy in 3D.

Na\"ive up-sampling of fast but low resolution (LR) images is undesirable for the clinical practice, since results lack information. Information content cannot be increased by simply increasing the number of pixels with linear interpolation methods. Therefore, optimization-based super-resolution (SR) methods have been explored to generate rich volumetric information from oversampled input spaces. However, these methods are highly dependant on the quality and amount of input samples and depend on the choice of the objective function. Recent work, \emph{e.g.}~\cite{Dong2016a}, on example-based SR has focused on incorporating additional prior image knowledge, and, in particular, deep neural networks have been employed to solve the single-image SR (SISR) problem. However, the majority of recent contributions typically place strong emphasis on natural images and therefore lack domain specific high-frequency detail prior knowledge~\cite{Borman1998}.
\newline
\newline
\noindent \textbf{Contribution:}
We present a novel approach to SISR in the context of motion compensation when using fast to acquire, low resolution volumes. Taking inspiration from recent investigation of network based SR for MRI modalities~\cite{Oktay2016}, we propose a network architecture with convolutional and transposed-convolutional layers and hypothesize that such a deep network architecture can be tailored to context sensitive applications, such as motion compensation of the fetal brain, and yield volume reconstruction improvements from low resolution input. Our network learns subject specific details from potentially motion corrupted input data and accurately reintroduces the expected fidelity allowing motion compensation and high quality reconstruction from fast low resolution input. 

Our model is in particular data-adaptive since the upsampling is performed by learnable transposed-convolution layers instead of a fixed kernel. By performing the upsampling in the final layers of the network we avoid early redundant computation in a HR space, enabling a computational saving. Additionally by considering entire LR in-plane slice samples at training time, in comparison to image \emph{patches}, we gain a large receptive field to enable the learning of spatial context, organ structure and anatomy.  % and \todo{magic to emphasise the input data through re-training on input data}.

We evaluate our method on 145 healthy fetal scans. The proposed approach shows improved qualitative results when compared visually to linear methods. Quantitative reconstruction performance, peak signal-to-noise-ratio (PSNR) and structural similarity index measure (SSIM) improve, accordingly. In particular, we reach comparable reconstruction quality with half as many data samples, thus half of the currently required scan time, when compared to motion compensated reconstruction from high-resolution image acquisition.
%\todo{We also show that our model can be transferred to other image formation principles and provide proof-of-concept through automatic adoption of a model that has been initially trained on T2 weighted data to Balanced Turbo Field Echo (BTFE) sequences.}
\newline
\newline
\noindent \textbf{Related work:}
\label{sec:rel_work}
% Deep Learning SR
% ----------------
% Dong ICCV 2015 \cite{Dong2015}
% Wang ICCV 2015 \cite{Wang2015deep}
% Dong TPAMI 2016 \cite{Dong2016a}
% Liu TIP 2016 \cite{Liu2016}
% Kim CVPR 2016 \cite{Kim2016a}
% Kim CVPR 2016 \cite{Kim2016b}
% Shi CVPR 2016 \cite{Shi2016}
% Johnson ECCV 2016 \cite{Johnson2016}
% Ledig 2016 SRGAN \cite{Ledig2016}
% Sonderby 2016 \cite{Sonderby2016}
% Sajjadi 2016 \cite{Sajjadi2016}
% Dahl 2017 \cite{Dahl2017pixel}
% Medical imaging SR survey
% ----------------
%~\cite{Greenspan2009super}
%
The topic of SR has received much attention in the literature and a large body of work exists however, historically, algorithms exhibiting good performance in 2D domains such as satellite or facial imagery, are not necessarily ideal for 3D medical imaging. This is partly due to domain specific effects such as loss of spatial information caused by motion during slow target acquisition. Various algorithms have been shown to produce leading results~\cite{Nasrollahi2014} in differing domains. %Varying constraints that are imposed on the problem in disparate applications. %Many competitive natural image, network-based architectures have been proposed recently~\cite{Dong2016a,Ledig2016,Dahl2017pixel}. %~\cite{Dong2015,Wang2015deep,Dong2016a,Liu2016,Kim2016a,Kim2016b,Shi2016,Johnson2016,Ledig2016,Sonderby2016,Sajjadi2016,Dahl2017pixel}. 

%Single image SR accounts for missing image information by using examples observed at training time to learn the LR-HR mapping between patches. Learning based single image SR can be classified into non-parametric and parametric categories. Patch-match based non-parametric approaches, such as~\cite{Shi2013}, approximate global search to find patch-correspondences between the target image and a set of atlases. Such non-parametric approaches are often computationally expensive as training sets must be searched for suitable matching HR patch candidates. Learning generative models from training data provides an alternative route to providing the LR-HR mapping. Parametric generative models have been used to upsample MR brain~\cite{Rueda2013single}, cardiac images~\cite{Bhatia2014super} and random forest approaches have utilized non-linear regression to predict HR patches from LR data~\cite{Alexander2014image}. Recently, CNN models have been proposed to perform the SR inference step, motivated by their now well established ability to learn complex non-linear functions and image regression task performance. With relatively few hidden layers, they offer state-of-the-art performance in natural image domains~\cite{Dong2015,Dong2016a}.

%In the current work we choose to adapt a recent SR-CNN style architecture that has also exhibited good performance when applied to problems concerning MRI cardiac data~\cite{Oktay2016}.
SISR accounts for missing image information by using previously observed examples to optimise the LR-HR mapping between images or patches. In the medical imaging domain, data-adaptive patch-based approaches to SISR reconstruction~\cite{Jia2016single,Manjon2010non} have been shown to prevent the occurrence of well-known blurring effects, often found when using classical interpolation approaches. Interpolation techniques tend to increase the smoothness of images in an isotropic manner, however data-adaptive non-local methods allow for highly anisotropic reconstruction where required. In patch-based methods, the radius of 3D patch used to compute the similarity among voxels is often a free parameter and the choice of receptive field size typically affects computational cost when using iterative optimisation. 

Learning based approaches also allow data-adaptive reconstruction and CNNs in particular have recently been successfully applied to context sensitive SISR for cardiac imaging. The work of \cite{Oktay2016} use a regression architecture based on~\cite{Dong2016a} with a modified $l_1$ objective function. The approach performed SR in the slice-select direction of lowest MRI resolution, \emph{i.e.}, one-dimensionally and utilized transposed convolutional layers at the start of the network architecture to perform the upsampling, \emph{prior} to convolutions, thus learning high level features in latter layers on (spatially) large feature maps. 

Two-dimensional SR is a popular research area in natural image processing due to many applications requiring enhancement of a visual experience while limiting the amount of raw data that needs to be recorded, transferred or stored. Recent network-based approaches such as SRGAN~\cite{Ledig2016} %and Pixel Recursive Super Resolution~\cite{Dahl2017pixel} 
apply Generative Adversarial Networks (GAN) %~\cite{GoodfellowGAN2014} 
%and conditional pixel-wise image generation
%~\cite{OordKVEGK16} 
%respectively 
to achieve large up-sampling factors of up to four. %The work of ~\cite{Ledig2016} %utilizes many ResNet~\cite{He2016} blocks and 
%reports state-of-the-art PSNR and SSIM metrics on standard natural image SR benchmarks. %Additional recent GAN based work~\cite{Yu2016} shows an ability to generate $8\times$ super resolved natural images with qualitatively plausible high frequency detail. 
% GANs learn only enough to fool a non-stationary discriminator -- a common failure is 'mode collapsing' where samples lack diversity.

% SR-CNN shown to work for cardiac data. Future work SR-GAN

%Historically, super-resolution algorithms exhibiting good performance in domains such as satellite or facial imagery are not necessarily best for medical purposes. In differing domains, various algorithms have been shown to produce leading results~\cite{Nasrollahi2014}. This is mainly due to the varying constraints that are imposed on the problem in disparate applications. Many competitive natural image, network-based architectures have been proposed recently ~\cite{Dong2015,Wang2015deep,Dong2016a,Liu2016,Kim2016a,Kim2016b,Shi2016,Johnson2016,Ledig2016,Sonderby2016,Sajjadi2016,Dahl2017pixel} \todo{overkill?}. In the current work we choose to adapt a recent SR-CNN style architecture that has also exhibited good performance when applied to problems concerning MRI cardiac data~\cite{Oktay2016}.

% Generating hallucinated details that appear plausible but are inconsistent with ground-truth is not acceptable in this domain -- need to discuss?
%\todo{High-frequency details hallucinated by the state-of-the-art learning-based methods (including our SR-CNN?) may not be authentic wrt GT. Justify why this is OK.}

Motion compensation for MRI volume reconstruction typically incorporates a SR component. However to the best of our knowledge state-of-the-art network based SR techniques, capable of learning problem and sensor specifics from available data have not been harnessed for the upsampling step found in Slice-to-Volume frameworks for the reconstruction task. In this work we investigate the accuracy advantages that such an approach can contribute to the example of fetal MRI volume reconstruction. 

Contemporary SR components in MRI Slice-to-Volume reconstruction (SVR) tasks perform optimisation based incremental updates to the HR volume estimate. To achieve this, the SR problem for volume reconstruction has been modelled directly by considering minimisation of an error norm function and use of Huber function statistics~\cite{gholipour2010robust} or gradient-weighted averaging~\cite{kim2010intersection}. The ill-posed nature of modelling upsampling requires that the objective be regularised. Gholipour~\emph{et al.}~\cite{gholipour2010robust} add a Tikhonov term to the cost for this purpose while Rousseau~\emph{et al.}~\cite{Rousseau2010,rousseau2006registration,Rousseau2013} select a regularisation term that includes an approximation of Total Variation (TV) to better preserve edges. Tourbier~\emph{et al.}~\cite{tourbier2015efficient} apply fast convex optimization techniques for the SR problem also using an edge-preserving TV regularization. Murgasova~\emph{et al.}~\cite{Kuklisova-Murgasova2012} used intensity matching and complete outlier removal for reconstruction. SR volume intensities are iteratively updated using the error gradients resulting from differences between simulated and observed slice samples. Transforming observed slice information to the upsampled volume space requires accurate yet potentially computationally expensive estimation of the sensor point spread function (PSF) and~\cite{kainz2015fast} developed a fast multi-GPU accelerated implementation for the task. 

%%%%

%============================================================================
%== METHOD ==================================================================
%============================================================================
\vspace{-2mm}
\section{Method} 
\vspace{-1mm}
\label{sec:method}
The proposed approach implements a fully three-dimensional CNN architecture to infer upsampled MRI imagery, enabling HR input to be provided for subsequent SVR and motion compensation tasks. We define an architecture utilising 3D volumetric convolutions that have recently been shown to add value for medical imaging tasks considering 3D imagery~\cite{Kamnitsas2016deepmedic,Cciccek20163d}. Fig.~\ref{fig:network} provides a schematic of our upsampling network and architecture design details are provide in the \textbf{3D MRI CNN} subsection below. Fig.~\ref{fig:pipeline} provides a schematic diagram indicating where the upsampling network component is implemented in a SVR reconstruction framework.

% \cite{AlansaryKRMDLDM16}. 
%\todo{We illustrate how this allows for improved compensation of fetal motion providing clinically useful data. (this sentence has no content)} 

\begin{figure}[!htbp]
\centering
  \includegraphics[width=1.0\linewidth]{network/network_schematic_arch_v01.png}
  \caption{Our proposed CNN network architecture for MRI super-resolution. See text for architecture details.}
  \label{fig:network}
\end{figure}

The architecture differs from recent network based MRI SR models~\cite{Oktay2016} by generating feature maps in the LR image space \emph{cf.} early redundant feature channel upsampling or fixed kernels~\cite{Dong2015}, reducing memory and computation requirements while retaining the flexibility of learnable upsampling layers. As previously reported~\cite{Shi2016}, early upsampling tends to introduce redundant computation in the HR space since no additional information is added into the model by performing transposed convolutions at an early stage of the architecture.
%In the in-plane direction we make use of large receptive fields to learn the context of the anatomy seen at training time.

\begin{figure}[!htbp]
\centering
  \includegraphics[width=1.0\linewidth]{images/network/pipeline_arch_v01.pdf}
  \caption{The proposed framework for providing upsampled, high resolution input for motion correction and volume reconstruction.}
  \label{fig:pipeline}
\end{figure}

\noindent Our approach mitigates the acquisition quality cost of low resolution imagery by considering the problem of estimating a high dimensional $\mathbf{y} \in \mathbb{R}^M$, for a given observation $\mathbf{x} = f(y) \in \mathbb{R}^N$ where $(N << M)$. SR is an underdetermined inverse problem, and as such the function $f$ performs a downsampling and is typically non-invertible. The low-dimensional observation $\mathbf{x}$ is mapped to the high-dimensional $\mathbf{y}$ by recovery through the MR image acquisition model~\cite{Greenspan2009super}, a series of operators such that: $\mathbf{x}=DBSM\mathbf{y}+\eta$ where $M$ defines a spatial displacement, \emph{e.g.} due to motion, $S$ is the slice selection operator, $B$ is the point-spread function (PSF) used to blur the selected slice, $D$ is a decimation operator, and $\eta$ is a Rician noise model. We approximate solutions to this inverse problem by estimating $\phi(\mathbf{x},\Theta)$ from the LR input such that a cost, defined between $\phi(\mathbf{x},\Theta)$ and $\mathbf{y}$, is minimized. We estimate the parameters $\Theta$ using a CNN architecture with parameters $\Theta$ that parametrise network layers to model the distribution $p(y|x)$. Training samples are defined as ($\mathbf{x}_i,\mathbf{y}_i$).
\newline
\newline
\noindent \textbf{3D MRI CNN:}
%%
In-plane, low-resolution MRI stacks are synthetically generated simply by filtering HR images with a Cosine Windowed Sinc blurring kernel followed by a decimation operator to provide LR-HR training pairs as input. Training samples consist of entire LR in-plane imagery with a volume defined by $z>=1$ out-of-plane slices forming 3D volume training samples, providing contextual information from multiple slices. Here we report on experimental upsampling factors of $\times2$,$\times4$ and $z=5$.
%An element-wise summation layer to combine directly upsampled imagery and convolution filters, thus learning upsampled image residuals directly.
%% 

%The architecture consists of six convolutional layers, utilising standard ReLU activations and residual units, followed by two transposed-convolutional layers (with corresponding strides of two or four) to up-sample the resulting imagery to the original resolution of the HR input. 

Our 3D-CNN architecture contains nine layers consisting of six convolutional layers, utilising standard ReLU activations and residual units, followed by two transposed-convolutional layers (with corresponding strides of two or four) and a final single-channel layer to build the full resolution output. %A similar architecture is recently found to provide state-of-the-art performance on the super resolution task when considering 3D cardiac MR data~\cite{Oktay2016}.
%The network architecture defines two transposed convolutional layers that perform the initial upsampling, followed by convolutional layers with rectified linear unit (ReLU) activations, enabling the estimation of non-linear upsample mappings $\phi$. 
The ReLU activation function has exhibited strong performance when upscaling both natural images~\cite{Dong2016a} and MRI 3D volume data~\cite{Oktay2016}. Intermediate feature maps $h^{(n)}_j$ at layer $n$ are computed through convolutional kernels $w^n_{kj}$ as $max(0 , \Sigma^K_{k=1} h_k^{(n-1)} \ast w^n_{kj}) = h^n_j$ where $\ast$ is the convolutional operator. We follow the common frugal strategy \cite{Simonyan2014} of applying small $(3\times3\times3)$ convolution kernels and spending compute-budget alternatively on layer count to increase receptive field size. 

%, ($X, Y$ respectively). 
By introducing two transposed convolution layers we perform the upscaling on in-plane sampling dimensions. In this manner, upscaling weights are learned specifically for the SR task where $(x \uparrow U_x) * w_j = h_j^0$ and $(h_1 \uparrow U_y) * w_j = h_j^1$ where $\uparrow$ is a zero-padding upscaling operator and $\{U_x,U_y\} = M/N$ are the in-plane upscaling factors. This allows for explicit optimization of the upsampling filters and facilitates training in an end-to-end manner for the SR task. By implementing trainable upsampling layers we improve upon the alternative strategy of initial independent linear upsampling, followed only by convolutional layers, as we gain an ability to learn upsampling weights specific to the SR task. In practice this often improves MRI image signal quality in image regions close to boundaries~\cite{Oktay2016}. Residuals learned by the convolution layers and the upscaled transposed-convolutional output are used to reconstruct the final HR image. This allows the regression function to learn non-linearities such as the high frequency components of the signal.%, such as edges and intensity signal variegation.

Training involves evaluating the error function $\Psi_{l_2}(\cdot)$ that calculates the difference between the reconstructed HR images and the ground truth volumes that were down-sampled to provide training data. Model weights are updated using standard back-propagation and adaptive moment estimation. % (Adam).%~\cite{Kingma2014}. 
In comparison to modified $l_1$ losses~\cite{Oktay2016} or recent \emph{perceptual-quality} SR objective functions~\cite{Ledig2016}, we implement a standard voxel-wise $l_2$ loss function to provide gradient information and emphasize voxel-wise difference to the ground-truth. An implementation of our model training strategy is made available online\footnote{https://github.com/DLTK}.
\newline
%For SR and restoration problems, \cite{Zhao2015} show that the $l_1$ norm provides a better metric than the $l_2$ norm due to the metric encouraging sharp edges and features. 

% provide a modified loss function defined as $\Psi_{l_1}(r) = \{0.5r^2 \enskip \text{if} \enskip |r|< t \enskip , \enskip |r| - 0.5 \enskip \text{otherwise}\}$, and $t$ as a tuning parameter. This ensures that weight updates are not dominated by large prediction errors, and any large outlying prediction errors are not being over-penalized. % previously exhibiting good MRI SR performance, to ensure 
\noindent \textbf{Fetal Brain Volume Reconstruction:} We combine our SR network with Slice-to-Volume registration (SVR) \cite{kainz2015fast}. SVR requires multiple orthogonal stacks of 2D slices to provide improved reconstruction quality. By upsampling stacks prior to reconstruction we provide a means to acquire larger sets of low-resolution input. %The reconstruction technique is based on splitting the input data into overlapping square patches or superpixels \cite{kainz2015fast}.
The motion-free 3D image is then reconstructed from the upsampled slices and motion-corrupted and misaligned areas are excluded during the reconstruction using an EM-based outliers rejection model.%~\cite{murgasova2012reconstruction}. %The following section provides our experimental investigation of the proposed methods.
 
\if 0
%OUT_OF_DATE
%\begin{table}[!hp]
%\centering
%\begin{tabular}{cccc}
%\toprule
%                 &  Layer Type                   &  Kernel , Stride , Pad  &  ~~ Parameters ~~ \\
%\midrule
%Data Layer       &  $40\times40\times5$ patches  &  -                      &  -                \\
%\midrule
%\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}~~Deonvolutional~~ \\Layers\end{tabular}}  
%                 &  deconv1-01    OUT_OF_DATE              &  8,2,3                  &  9                \\
%                 &  deconv2-01    OUT_OF_DATE              &  8,2,3                  &  9                \\
%\midrule
%\multirow{16}{*}{\begin{tabular}[c]{@{}c@{}}Convolutional \\Layers\end{tabular}}       
%                 &  conv1-64    OUT_OF_DATE      &  3,1,1                  &  1792             \\
%                 &  batchNorm   OUT_OF_DATE      &  -                      &  -                \\
%                 &  ReLU        OUT_OF_DATE      &  -                      &  -                \\
%                 &  conv2-64                     &  3,1,1                  &  110656           \\
%                 &  batchNorm                    &  -                      &  -                \\
%                 &  ReLU                         &  -                      &  -                \\
%                 &  conv3-32                     &  3,1,1                  &  55328            \\
%                 &  batchNorm                    &  -                      &  -                \\
%                 &  ReLU                         &  -                      &  -                \\
%                 &  conv4-16                     &  3,1,1                  &  13840            \\
%                 &  batchNorm                    &  -                      &  -                \\
%                 &  ReLU                         &  -                      &  -                \\
%                 &  conv5-16                     &  3,1,1                  &  6928             \\
%                 &  batchNorm                    &  -                      &  -                \\
%                 &  ReLU                         &  -                      &  -                \\
%                 &  conv6-01                     &  3,1,1                  &  433              \\
%\midrule
%Utility Layer    &  ElemWise Sum                 &  -                      &  -                \\
%\midrule
%Loss Layer       &  ~~Modified Euclidean loss~~  &  -                      &  -                \\
%\bottomrule
%\end{tabular}
%\caption{Fetal Brain SR-Recon Network Topology}
%\label{tab:network-topo}
%\vspace{-1.0cm}
%\end{table}
\fi

%============================================================================
%== EXPERIMENTATION==========================================================
%============================================================================
\vspace{-2mm}
\section{Experiments}
\vspace{-1mm}
\label{sec:experiments}
\noindent\textbf{Data:} 
% iFind
We test our approach on clinical MR scans with varying gestational age. All scans have been ethically approved. The dataset contains $145$ MR scans of healthy fetuses at gestational age between $20$--$25$ weeks. The data has been acquired on a Philips Achieva $1.5$T, the mother lying $20\degree$ tilt on the left side to avoid pressure on the inferior vena cava. Single-shot fast spin (ssFSE) echo T2-weighted sequences are used to acquire stacks of images that are aligned to the main axes of the fetus. Three to six stacks with a voxel size of $1.25mm \times 1.25mm \times 2.5mm$  per stack are acquired for the whole womb. Imagery is manually masked and cropped to isolate fetal brain regions. %We report evaluation of the proposed SR and SVR reconstruction strategy in the following section.
% Melissa
%Dataset II contains $64$ MR scans of both healthy fetuses and fetuses with intrauterine fetal growth restriction (IUGR) at gestational age between $20$-$38$ weeks. \todo{ $xxx$ } subjects of the dataset are from normal fetuses and \todo{ $yyy$ } subjects are from fetuses with intrauterine fetal growth restriction (IUGR). Acquisition was with a 1.5T Philips MRI system using ssFSE sequences and a voxel size of $1.15 \times 1.15 \times 1.25$ mm. 
\newline
\newline
\noindent\textbf{Experimental details:} We employ our 3D MRI network and separately two baseline SR strategies to upsample image stack inputs that serve as input to the SVR pipeline. SVR then performs motion compensation and volume reconstruction. We assess upsampled image quality directly and, additionally, investigate the effect of the proposed upsampling strategy on reconstruction quality, from the (initially) low resolution fetal data. We report three quantitative metrics: PSNR, structural similarity index  (SSIM) %~\cite{Wang2004}  
and cross-correlation. In the first experiment, the data is randomly split into two subsets and used to train ($100$) and test ($45$) with our SR network. MRI stacks represent $46$ individual patients and all image stacks, belonging to a particular patient, are found uniquely in either the train or test set. Images are cropped, intensity normalised and linearly downsampled by factors of $2$ and $4$ with respect to the in-plane stack axes. This resampling provides LR images to our network resulting in multiple training samples per volume with corresponding ground-truth label (HR source image). The network uses these training pairs to learn the LR to HR mapping. Note that image volume size choices introduce a trade-off between available contextual information and pragmatic memory constraints.

%We defer rigorous exploration of optimal patch-size-choice to future work.   
%============================================================================
%== PHANTOM STACK EXPERIMENT (TODO) =========================================
%============================================================================
%\todo{use Amir's phantom stack generator without artificial motion and down-sample these stacks. run one iteration without registration, so that only SR recon is done. (if you use registration activate 'hybrid-mode', but you don't need registration for a motion-free phantom) -- you can also compare slice-wise with the actual input data of fetal MRI (use location of stack slices to sample from recon)}
%============================================================================
%== RESULTS =================================================================
%============================================================================

%\vspace{-11.5mm}
\section{Evaluation and Results}
%\vspace*{-5mm}
\label{sec:results}
\noindent \textbf{Image Quality Assessments:} We compare HR ground-truth 3D volumes with upsampled LR raw data by measuring PSNR, SSIM and cross-correlation. We report SSIM, in particular, due to the well-understood metric properties that afford assessment of local structure correlation and reduced noise sensitivity. LR test imagery is upsampled in-plane ($X,Y$) by factors of $2$, $4$ to align with target ground-truth resolution. Quality metrics in Fig.~\ref{fig:boxplot} report improvements observed for an image upsampling factor of $2$. This provides initial evidence in support of our hypothesis; \emph{learning problem and sensor specific deconvolutional filters to perform MRI stack upsampling is of benefit for subsequent resolution-sensitive tasks such as motion compensation and HR volume reconstruction}.
\vspace{-9.5mm} 
\begin{figure}[H]
%\begin{figure}[!htbp]
     \centering
     \subfloat{\includegraphics[height=1.7cm]{boxplot/PSNR.pdf}\label{}}\hfill
     \subfloat{\includegraphics[height=1.7cm]{boxplot/SSIM.pdf}\label{}}\hfill
     \subfloat{\includegraphics[height=1.7cm]{boxplot/CrossCorr.pdf}\label{}}
     \caption{PSNR, SSIM and Cross Correlation metrics for $45$ LR image stacks with voxel spacing $(2.50\times2.50\times1.25$)mm that are upsampled $\times2$ in-plane (X,Y) and compared to ground-truth image stacks $(1.25\times 1.25\times1.25)$mm using Linear, B-Spline, 3D MRI CNN methods.}
     \label{fig:boxplot}
\end{figure}

\noindent By learning problem specific HR synthesis models, our 3D MRI CNN strategy outperforms the na\"ive baseline up-sampling, quantitatively improving the quality of the inferred HR imagery. Fig.~\ref{fig:input_upsample} exhibits an example of qualitative improvement in orthogonal fetal MRI test-stack axes. 

\begin{figure}[H]
%\begin{figure}[!htbp]
\centering%
%\vspace{-0.5cm}
\resizebox{0.65\textwidth}{!}{
	\begin{tabular}{>{\centering}m{0.02\textwidth} >{\centering}m{0.15\textwidth} >{\centering}m{0.15\textwidth} >{\centering}m{0.15\textwidth} >{\centering}m{0.15\textwidth} >{\centering\arraybackslash}m{0.15\textwidth}}
		&\textbf{LR input($\times2$)} & \textbf{Linear} & \textbf{B-spline} & \textbf{3D MRI CNN} & \textbf{GT HR} \\
		%\vspace{-5mm}
		\rotatebox[origin=c]{90}{\textbf{Axial}}&
            \includegraphics[width=0.15\columnwidth,angle=00,trim= 40 40 40 40,clip]{801/coronal_downsample} 	&
			\includegraphics[width=0.15\columnwidth,angle=00,trim= 40 40 40 40,clip]{801/coronal_linear}     	&
 			\includegraphics[width=0.15\columnwidth,angle=00,trim= 40 40 40 40,clip]{801/coronal_bspline}    	&
			\includegraphics[width=0.15\columnwidth,angle=00,trim= 40 40 40 40,clip]{801/coronal_cnn}			&
			\includegraphics[width=0.15\columnwidth,angle=00,trim= 40 40 40 40,clip]{801/coronal_GT}
		\\
		%&\mycaption{0.1} & \mycaption{0.2} & \mycaption{0.3} & \mycaption{0.4}\\
		\rotatebox[origin=c]{90}{\textbf{Sagittal}}&
			\includegraphics[height=0.15\columnwidth,angle=90,trim= 40 40 40 40,clip]{701/sagittal_downsample} 	&
			\includegraphics[height=0.15\columnwidth,angle=90,trim= 40 40 40 40,clip]{701/sagittal_linear}     	&
			\includegraphics[height=0.15\columnwidth,angle=90,trim= 40 40 40 40,clip]{701/sagittal_bspline}    	&
			\includegraphics[height=0.15\columnwidth,angle=90,trim= 40 40 40 40,clip]{701/sagittal_cnn}        	&
			\includegraphics[height=0.15\columnwidth,angle=90,trim= 40 40 40 40,clip]{701/sagittal_GT}
		\\
		%&\mycaption{0.1} & \mycaption{0.2} & \mycaption{0.3} & \mycaption{0.4}\\
		\vspace{-8pt}\rotatebox[origin=c]{90}{\textbf{Coronal}}&
			\vspace{-8pt}\includegraphics[width=0.15\columnwidth,angle=180,trim= 40 40 40 40,clip]{601/axial_downsample}	&
			\vspace{-8pt}\includegraphics[width=0.15\columnwidth,angle=180,trim= 40 40 40 40,clip]{601/axial_linear}		&
			\vspace{-8pt}\includegraphics[width=0.15\columnwidth,angle=180,trim= 40 40 40 40,clip]{601/axial_bspline}		&
			\vspace{-8pt}\includegraphics[width=0.15\columnwidth,angle=180,trim= 40 40 40 40,clip]{601/axial_cnn}			&
			\vspace{-8pt}\includegraphics[width=0.15\columnwidth,angle=180,trim= 40 40 40 40,clip]{601/axial_GT} 
		%&\mycaption{0.1} & \mycaption{0.2} & \mycaption{0.3} & \mycaption{0.4}\\
		\end{tabular}
}
\caption{Orthogonal fetal MRI stacks showing in-plane stack axes per row. Low resolution input (left) is upsampled by two baselines (col \emph{Linear},\emph{B-spline}) and our learning based approach (col \emph{3D MRI CNN}) \emph{cf.} ground-truth (GT) HR imagery. The learning based 3D MRI CNN, with modality specific priors, provides improved high frequency signal components \emph{cf.} baselines.}
\label{fig:input_upsample}
\end{figure}

We additionally perform preliminary experiments towards integrating network-based SR components more tightly with an SVR pipeline by investigating the ability of the network to upsample LR voxel intensities that result from an initial volume reconstruction iteration. Successful integration of an iterative (learning-based) SR and volume reconstruction loop will facilitate the well understood mutual benefits of reduced-motion SR input and improved input fidelity for the motion correction task. Qualitative comparison of ($\times4$) LR volume-reconstructed input and resulting upsampled results are found in Fig.\ref{fig:recon_upsample}. The benefit of learning the upsampling with modality specific data can be observed to manifest as sharper edge gradients and improved high frequency signal components. The visual quality gap between the baselines and our method can be seen to widen as the prior information required to successfully upsample at larger factors make the task more challenging.

\begin{figure}[!htbp]
%\vspace{-50mm}
     \centering
     \subfloat{\includegraphics[height=35mm]{images/4x_example/4x_qual_method_comparison_v03.png}\label{}}\hfill
     \caption{SR applied to LR ($\times4$) volume reconstructed input. Benefits of learning the specific non-linearities to recover sharp edge gradients and improved high frequency signal components of the modality become more evident \emph{cf.} baselines as the amount of information required to upsample-successfully increases.}
     \label{fig:recon_upsample}
\end{figure}

\noindent \textbf{Volume Reconstruction Improvement:} 
In our third experiment we evaluate SVR performance using LR input stacks, upsampled by the considered strategies, before initiating the volume reconstruction task. We additionally perform SVR reconstruction with original HR imagery to provide the ``ground-truth'' reference brain volumes. Employing the three quality metrics, introduced previously, we evaluate how well super-resolved LR stack reconstructions correspond to the reconstructions due to original high, in-plane, resolution imagery. Table \ref{tab:psnr_ssim_cc} reports PSNR, SSIM and cross-correlation metrics for volume comparison (SR strategy with respect to ``ground-truth'' volume) for the $13$ patients that define the MRI stack test set. Super-resolving the LR input data with the proposed learning based approach can be observed to facilitate reconstruction improvement, across the investigated metrics. Visual evidence supporting this claim is found in Fig.~\ref{fig:dssim} (best viewed in color). Fig.~\ref{fig:dssim} displays 2D slices of patient fetal brain reconstructions resulting from the original HR input-imagery (far left) and identically spatially-located slices (a) resulting from (b) LR imagery (half the in-plane resolution), (c-d) input using na\"ive up-sampling strategies and (e) our 3D MRI CNN upsampling. Corresponding Structural Dissimilarity (DSSIM) error heatmaps (second row) provide improved visual spatial congruence between HR ground-truth and our method, supporting the claim that utilizing sensor specific priors is of marked benefit for the task of MRI fetal brain reconstruction from LR imagery.
% \todo{//significant//?}

\begin{figure}[!h]
\vspace{-7.5mm}
	\centering
	\resizebox{0.7\textwidth}{!}{
	\begin{tabularx}{\linewidth}{@{}cXX@{}}
		\multirow{-4}[2.5]{*}{
		\subfloat[]{%
			\includegraphics[width=0.17\columnwidth,height=0.17\columnwidth,angle=180, trim= 20 20 20 20,clip]{images/heat_maps/gt_recon.png}}
			}
		&
		\begin{tabular}{cccc}
			\vspace{-2pt}\subfloat[]{%
			\includegraphics[width=0.17\columnwidth,height=0.17\columnwidth,angle=180, trim= 20 20 20 20,clip]{images/heat_maps/down_recon.png}}	
			&	
			\vspace{-2pt}\subfloat[]{%
			\includegraphics[width=0.17\columnwidth,height=0.17\columnwidth,angle=180, trim= 20 20 20 20,clip]{images/heat_maps/lin_recon.png}}
			&
			\vspace{-2pt}\subfloat[]{%
			\includegraphics[width=0.17\columnwidth,height=0.17\columnwidth,angle=180, trim= 20 20 20 20,clip]{images/heat_maps/bsp_recon.png}}	
			&
			\vspace{-2pt}\subfloat[]{%
			\includegraphics[width=0.17\columnwidth,height=0.17\columnwidth,angle=180, trim= 20 20 20 20,clip]{images/heat_maps/cnn_recon.png}}
			\\
			\vspace{-2pt}\subfloat[]{%
			\includegraphics[width=0.17\columnwidth,height=0.17\columnwidth,angle=180, trim= 20 20 20 20,clip]{images/heat_maps/down_dssim.png}}	
			&
			\vspace{-2pt}\subfloat[]{%
			\includegraphics[width=0.17\columnwidth,height=0.17\columnwidth,angle=180, trim= 20 20 20 20,clip]{images/heat_maps/lin_dssim.png}}
			&
			\vspace{-2pt}\subfloat[]{%
			\includegraphics[width=0.17\columnwidth,height=0.17\columnwidth,angle=180, trim= 20 20 20 20,clip]{images/heat_maps/bsp_dssim.png}}	
			&
			\vspace{-2pt}\subfloat[]{%
			\includegraphics[width=0.17\columnwidth,height=0.17\columnwidth,angle=180, trim= 20 20 20 20,clip]{images/heat_maps/cnn_dssim.png}}
			\end{tabular}
	\end{tabularx}
}

\caption{(a) 2D slice through a fetal brain reconstruction, resulting from HR input-imagery. Attempting similar reconstruction from faster to acquire LR imagery, at half the in-plane resolution, results in highly degraded visual reconstruction quality (b) and gross DSSIM disparity (\emph{ie}. red heatmap regions) (f) with respect to the HR reconstruction. Na\"ive up-sampling $(\times2)$ of the LR in-plane input prior to reconstruction, with linear interpolation or B-splines, result in over-smoothed input. Loss of sharp gradient information and input-image fidelity can be seen to propagate to the respective reconstructions (c), (d) and disparity, with regard to the HR reconstruction, remains high (g), (h). Our 3D MRI CNN upsampling affords input closer to the original HR imagery and results in improved reconstructions (e) and reduced DSSIM (i) with visibly cooler heatmap regions (standard \emph{jet} color scale).}
%The proposed approach shows better reconstruction quality (e) and (i) compared to reconstructions from upsampled images using bspline (d) and (h), and linear interpolation (c) and (g). While the reconstructed image from directly from down-sampled data shows the worst quality (b) and (f). Red colored regions in the DSSIM heat maps (f-i) show worse reconstruction quality than the blue regions.}
\label{fig:dssim}
% \vspace{-0.5cm}
\end{figure}

\begin{table}[!h]
%\vspace{-0.5cm}
\begin{center}
\resizebox{0.7\textwidth}{!}{
	\begin{tabular}{*4c}    
		\toprule
		{Upsample} & PSNR\emph{[dB]} & SSIM & Cross-correlation  \\
		\midrule
		\rowcolor{white!50} ~\emph{No upsample}             ~~& $18.466 \; \Mypm 1.88$            ~~&~~ $0.534 \, \Mypm 0.15$          ~~&~~ $0.699 \, \Mypm 0.12$ \\ 
		\rowcolor{white!50} ~Linear              ~~& $19.268 \; \Mypm 1.14$            ~~&~~ $0.665 \, \Mypm 0.08$          ~~&~~ $0.815 \, \Mypm 0.06$ \\
		\rowcolor{white!50} ~B-Spline            ~~& $19.985 \; \Mypm 1.52$            ~~&~~ $0.698 \, \Mypm 0.12$          ~~&~~ $0.836 \, \Mypm 0.08$ \\
		\rowcolor{white!50} ~3D MRI CNN  ~~& $\mathbf{21.715 \, \Mypm 1.84}$   ~~&~~ $\mathbf{0.779 \, \Mypm 0.10}$ ~~&~~ $\mathbf{0.885 \, \Mypm 0.07}$ \\
		\bottomrule
		\hline
	\end{tabular}
}
\end{center} 
\caption{PSNR, SSIM and Cross-correlation evaluating disparity between reconstructed volumes using upsampled LR input (Linear, B-Spline, 3D MRI CNN) and ground-truth volumes.}
\vspace{-10mm}
\label{tab:psnr_ssim_cc}
\end{table}

%============================================================================
%== CONCLUSION ==============================================================
%============================================================================
\label{sec:conclusion}
\vspace{-2mm}
\section{Discussion and Conclusion}
\vspace{-1mm}
We introduce a 3D MRI CNN to upsample low resolution MR data prior to performing volumetric motion compensation and SVR reconstruction.
%By harnessing a state-of-the-art single image SR-CNN to upsample data prior to performing the Slice-to-Volume reconstruction, 
Our method produces upsampled images and uses them to reconstruct volumetric fetal brain representations that quantitatively outperform on reconstruction tasks that utilise conventional upscaling methods. This contribution helps to address the well-understood image resolution challenge in fetal brain MRI. Analysis of accuracy metrics, assessing upsampling quality, exhibit a mean PSNR increase of $1.25$ $dB$. Furthermore, when utilizing the upsampled imagery as SVR input, reconstructed fetal brain volumes show improvements of up to $1.73$ $dB$ over the provided baseline. In addition to quality improvement, 3D MRI CNN upsampling provides a computationally efficient approach affording an ability to initially image at lower resolutions, with a shorter acquisition time, thus provides faster and safer scanning for high-risk patients like pregnant women.

The current work has implicitly provided evidence that the method learns the PSF of the investigated MRI data well. In future it would be valuable to investigate this further, explicitly. Real-world LR/HR samples, acquired from scanners at differing resolutions, would allow quantitative evaluation of the ability to reconstruct physical scanner PSF and would further allow investigation of a model's ability to generalise to the reconstruction of PSFs not explicitly seen at training time. Further to this; the current work only investigates a single problem instance under one image modality. Future work will look to investigate the generalisability of the proposed framework to additional problem domains.

% Avenues of promising future work include \todo{(see comment)}. % <----- Expansion to other modalities? / More exotic nets? (GAN? Pixel conditional?) / End-to-end learning? / Tighter framework implementation?

%not for the double blind submission!
%\textbf{Acknowledgments:} MITK\footnote{Medical Imaging Interaction Toolkit (MITK) http://mitk.org/} was used for some of the figures. This research was supported by the NIHR Biomedical Research Centre at Guy's and St Thomas' NHS Foundation Trust and King's College London. The views expressed are those of the author(s) and not necessarily those of the NHS, the NIHR or the Department of Health. Furthermore, this work was supported by Wellcome Trust and EPSRC IEH award 102431, FP7 ERC 319456, and EPSRC EP/N024494/1

\bibliographystyle{splncs03}
\bibliography{references3}

\end{document}

\if 0
%\begin{figure}[!htb]
%	\vspace{-0.7cm}	
%\centering
%    % trim <left bottom right top>
%    \subfloat[input\label{fig:raw_data:subfig:1}]{%
%	\includegraphics[height=2.0cm,trim=0 0 0 0,clip]{601/axial_downsample}
%	}
%    \subfloat[linear\label{fig:raw_data:subfig:2}]{%
%	\includegraphics[height=2.0cm,trim=0 0 0 0,clip]{601/axial_linear}
%	}
%    \subfloat[SR-CNN\label{fig:raw_data:subfig:3}]{%
%	\includegraphics[height=2.0cm,trim=0 0 0 0,clip]{601/axial_cnn}
%	}
%    \subfloat[GT HR\label{fig:raw_data:subfig:4}]{%
%	\includegraphics[height=2.0cm,trim=0 0 0 0,clip]{601/axial_GT}
%	}\\
%    \subfloat[input\label{fig:raw_data:subfig:5}]{%
%	\includegraphics[height=2.0cm,trim=0 0 0 0,clip]{701/sagittal_downsample}
%	}
%   \subfloat[linear\label{fig:raw_data:subfig:6}]{%
%	\includegraphics[height=2.0cm,trim=0 0 0 0,clip]{701/sagittal_linear}
%	}
%    \subfloat[SR-CNN\label{fig:raw_data:subfig:7}]{%
%	\includegraphics[height=2.0cm,trim=0 0 0 0,clip]{701/sagittal_cnn}
%	}
%    \subfloat[GT HR\label{fig:raw_data:subfig:8}]{%
%	\includegraphics[height=2.0cm,trim=0 0 0 0,clip]{701/sagittal_GT}
%	}\\
%     \subfloat[input\label{fig:raw_data:subfig:9}]{%
%	\includegraphics[height=2.0cm,trim=0 0 0 0,clip]{801/coronal_downsample}
%	}
%    \subfloat[linear\label{fig:raw_data:subfig:10}]{%
%	\includegraphics[height=2.0cm,trim=0 0 0 0,clip]{801/coronal_linear}
%	}
%    \subfloat[SR-CNN\label{fig:raw_data:subfig:11}]{%
%	\includegraphics[height=2.0cm,trim=0 0 0 0,clip]{801/coronal_cnn}
%	}
%    \subfloat[GT HR\label{fig:raw_data:subfig:12}]{%
%	\includegraphics[height=2.0cm,trim=0 0 0 0,clip]{801/coronal_GT}
%	}\\
%	 
%	\caption{Example low resolution input image (a), linear interpolation upsampling (b), SR-CNN (c), and high-resolution ground truth (d). %}
%\label{fig:raw_data}
%\end{figure}
\fi

\if 0
%\begin{figure}[!htb]
%\vspace{-0.7cm}	
%\centering
%    \hfill
%    \subfloat[input\label{fig:raw_data:subfig:1}]{%
%	\includegraphics[height=3.0cm,trim=0 0 0 0,clip]%{images/recon_input_images01/cropped_201509290951_PHILIPS484E467_1301_WIPT2_UTERUSTRASENSE_IN}
%	}
%    \subfloat[linear\label{fig:raw_data:subfig:2}]{%
%	\includegraphics[height=3.0cm,trim=0 0 0 0,clip]%{images/recon_input_images01/cropped_201509290951_PHILIPS484E467_1301_WIPT2_UTERUSTRASENSE_linear}
%	}
%    \subfloat[SR-CNN\label{fig:raw_data:subfig:3}]{%
%	\includegraphics[height=3.0cm,trim=0 0 0 0,clip]%{images/recon_input_images01/cropped_201509290951_PHILIPS484E467_1301_WIPT2_UTERUSTRASENSE_cnn_model024}
%	}
%    \subfloat[GT HR\label{fig:raw_data:subfig:4}]{%
%	\includegraphics[height=3.0cm,trim=0 0 0 0,clip]%{images/recon_input_images01/cropped_201509290951_PHILIPS484E467_1301_WIPT2_UTERUSTRASENSE_GT}
%	}
%	\hfill 
%    \null
%	\caption{Example low resolution input image (a), linear interpolation upsampling (b), SR-CNN (c), and high-resolution %ground truth (d). }
%\label{fig:raw_data}
%\end{figure}
\fi

\if 0
%\begin{tikzpicture}
%[   cnode/.style={draw=black, fill=#1, minimum width=5mm , minimum height=5mm , rectangle},
%]
%    \node[cnode=red,label=0:$\Sigma$] (s) at (6,-3) {};
%    \node at (0,-4) {$\vdots$};
%    \node at (3,-4) {$\vdots$};
%    
%    \foreach \x in {1,...,4}
%    {   
%        \pgfmathparse{\x<4 ? \x : "n"}
%        \node[cnode=blue,label=180:$x_{\pgfmathresult}$] (x-\x) at (0,{-\x-div(\x,4)}) {};
%        \node[cnode=gray,label=90:$\varphi_{\pgfmathresult}$] (p-\x) at (3,{-\x-div(\x,4)}) {};
%        \draw (p-\x) -- node[above,sloped,pos=0.3] {$\omega_{\pgfmathresult}$} (s);
%    }
%    
%    \foreach \x in {1,...,4}
%    {   
%        \foreach \y in {1,...,4}
%          {   
%             \draw (x-\x) -- (p-\y);
%          }
%    }
%\end{tikzpicture}
\fi
