%!TEX root = paper.tex

% Intro
Apart from investigating the distribution of session length, ultimately we would like to
be able to predict the length of a session. To that end we gathered
features about the users and sessions, and treated the problem of predicting the
length of a session as a regression problem.


\subsection{Features}

For each of the users and sessions available in our sample we collected
a number of features.
Some features, which we call ``user-based''
are features that we assume do not change between sessions, for example
the gender of a user. Other features which we call ``contextual'' can change
every time a user starts a new session, for example the type of network or device
that a user was using when they started the session, or the length of their last session.
We provide a summary of some of these
features in Table \ref{tab:prediction-features}, separated into user-based and contextual
features.

% Choice of method

\subsection{Model}

\label{subsec:model}


We selected gradient boosted trees (GBTs) \cite{friedman2001gbt} as our model
for a number of reasons: First, because our dataset contains missing data, the algorithm we chose had to be able to handle them explicitly, which decision trees do.

Second, the method should allow for proper modeling of non-negative data.
For such data it is possible to log-transform the dependent and use a squared error objective,
but using an objective function that is better fitted to the distribution of the dependent is
often desirable, which is a common use-case for Generalized Linear Models.
GBTs provide a flexible
optimization framework that allowed us to do just that.
To test both approaches,
we first log-transformed the dependent
and used a root mean squared error objective,
then ran the same experiments again, this time selecting the log-likelihood objective of a Gamma distribution with a log link function,
which allows for explicit modeling of right-skewed, non-negative data. In Section \ref{sec:experiments} we refer to these models
as \textit{linear} and \textit{Gamma} respectively.

Finally, we tested two versions of each model. One aggregated where a single model was created
using the data of all the users, and one per-user, where we separately trained one model
per user, using only the data originating from that user for the training and testing.
This meant that only contextual and not user-level features could be used
to train the per-user models.
This way we tested the trade-off between
the statistical power that a large dataset provides versus having personalized models.

\begin{table}
	\caption{Example user-based and contextual features used in the models.}
	\label{tab:prediction-features}
	\begin{tabular}{ll}
		\toprule
		Feature & Description\\
		\midrule
		Gender & The gender of the user \\
		Age & The age of the user \\
		Subscription Status & Whether the account is ad-supported \\
		\midrule
		Device & The device used for the session \\
		Network & The type of network used for the session \\
		Previous duration & The duration of the user's last session \\
		Absence time & Time elapsed since the last session \\
		\bottomrule
	\end{tabular}
\end{table}

% Experimental setup

\section{Experiments}

\label{sec:experiments}

The baseline model we tested against is the per-user mean session length;
that is, we calculated the mean session length in the training set for each user, and used that value
to make all predictions for each session that user had in the test set. This gave us a baseline
that is simple, but personalized to account for the differences in listening habits between
users.

Because we are focusing on direct length prediction rather than survival probability
\cite{Barbieri2016RSFclick}, thresholded duration classification \cite{lalmas2015gemini}, or
distribution parameter estimation \cite{Kim2014satisfaction, liu2010weibull}, most of the
related work models used in search and ad click scenarios are not directly applicable. Therefore
we don't include them in our comparison.

We used 10-fold cross validation, and stratified our sample per user to ensure that every user
had data points both in the train and test set of each split.

To ensure that we have enough data points per user, we only retained users
that had at least 20 sessions recorded.
The resulting dataset had 3,563,544 sessions.
Due to the size of the dataset we chose to use
the \textit{xgboost} \cite{Chen2016xgboost} variant of GBTs which is
implemented with scalability in mind, utilizing parallel, cache-aware,
and out-of-core computation to handle massive data sets. The parameters
for xgboost were selected through cross-validation on a separate validation set.

% Choice of metric

\subsection{Metrics}

We chose two evaluation metrics to measure the performance of our algorithms.
The first was the Root Mean Square Error (RMSE), which is a common choice for regression problems.
In particular we used the normalized variant of the measure (nRMSE), which is simply the RMSE scaled
by the mean value of the dependent, $\bar{y}$.


Large errors can be observed more often when the distribution
of the dependent variable is highly skewed as it is in our case (see Figure \ref{fig:duration-hist}).
Therefore, we include the Median Absolute Error (MAE) in our analysis
due to its robustness to outliers. This way
a few very large errors will not affect the metric disproportionately, compared to taking the mean.
For confidentiality we normalize all the MAE measurements by the baseline so that it has has an
error of 1, and lower measurements are better.

\subsection{Results}

We report the performance of the various approaches in Table \ref{tab:prediction-metrics}.  As mentioned before, we refer to the models using the RMSE objective as \textit{linear} to avoid confusion
with the nRMSE metric. The linear aggregated model outperforms all models in terms of Median Absolute Error, but cannot beat the baseline on nRMSE.
The aggregated model using Gamma regression has the best performance in terms of nRMSE, but has worse
MAE than its linear counterpart. We note that it's the only model that beats the baseline in
nRMSE.

The per-user linear models outperform the baseline for MAE but not for nRMSE, similarly to the aggregated linear model.
The per-user models using Gamma regression perform similarly to the linear per-user models,
indicating that the change in objective function becomes less important in small data domains.


\begin{table}
	\caption{Performance metrics for length prediction task. We report the
		mean value across the 10 CV folds, and the standard deviation in parentheses.}
	\label{tab:prediction-metrics}
	\begin{tabular}{lll}
		\toprule
		Method (\textit{Objective}) & Normalized MAE & nRMSE \\
		\midrule
		Baseline & 1 \textit{(0.001)} & 1.16 \textit{(0.005)} \\
		Aggregated (\textit{Linear}) & \textbf{0.71} \textit{(0.008)} & 1.23 \textit{(0.008)} \\
		Aggregated (\textit{Gamma}) & 0.93 \textit{(0.007)} & \textbf{1.10} \textit{(0.005)} \\
		Per-user (\textit{Linear}) & 0.83 \textit{(0.002)} & 1.29 \textit{(0.004)} \\
		Per-user (\textit{Gamma}) & 0.86 \textit{(0.001)} & 1.31 \textit{(0.003)} \\
		\bottomrule
	\end{tabular}
\end{table}

What these results indicate is that
the aggregated model using the Gamma regression objective is able to place the mean of the distribution more accurately because it
places more probability mass in the right tail of the distribution. The linear models place more of their probability mass closer
to the origin, allowing them to better capture the shorter sessions that are over-represented in the data, but as
a result miss many of the longer (outlier) sessions. This causes their mean-based metrics to suffer, while median metrics
benefit.

We also see that the per-user models mostly perform worse than their aggregated counterparts. This can be explained by the
fact that per-user models are mostly trained on few data points, and for a flexible model like GBTs, they are likely to
overfit. In this case the trade-off between having a single model trained with all the data versus
having personalized models trained on each user's data favors the aggregated model.