\documentclass[conference]{IEEEtran}
\input header.tex

\begin{document}

\title{Channel Estimation with One-Bit Transceivers in a Rayleigh Environment}
\author{\IEEEauthorblockN{Kang Gao, J. Nicholas Laneman, N. J. Estes, Jonathan Chisum, Bertrand Hochwald}\\
\IEEEauthorblockA{Department of Electrical Engineering, University of Notre Dame, Notre Dame, IN, 46556\\
Email: \texttt{\{kgao,jnl,nestes,jchisum,bhochwald\}@nd.edu}}}


\maketitle

\blfootnote{This work was generously supported by NSF Grant \#1731056, AT\&T, and gifts from Qualcomm Technologies and Futurewei Technologies.}
\begin{abstract}
One-bit transceivers with strongly nonlinear characteristics are being considered for wireless communication because of their low cost and low power consumption.  Although each such transceiver can support only a low data rate, multiple such transceivers can be used to obtain an aggregate high data rate.  An important part of many communication systems is the process of channel estimation, which is particularly challenging when the estimation process uses these transceivers.  The standard analysis of estimation mean-square error versus training length that is available for linear transceivers does not apply with the nonlinearities inherent in one-bit transceivers.  We analyze the training requirements in a large-scale system and show that the optimal number of training symbols strongly depends on the number of receivers, and the optimal number of training symbols can be significantly smaller than the number of transmitters.  These results contrast sharply with classical results obtained with linear transceivers.
%We also give one example of a communication system with 5 one-bit transmitters that achieves a rate very close to the asymptotic analysis using only 4 training symbols.

\end{abstract}

\IEEEpeerreviewmaketitle

\section{Introduction}
Nonlinearities in transceivers are being explored for high-frequency wireless communications because of their potential low cost and low power consumption. Examples, include systems with low-resolution (especially one-bit) analog-to-digital converters (ADCs) at the receiver \cite{nossek2006capacity,li2017channel,li2016much,wen2016bayes} or digital-to-analog converters (DACs) at the transmitter \cite{saxena2017analysis,li2017downlink,jacobsson2017massive,kong2018nonlinear} or both \cite{gao2017power,gao2018beamforming,kong2018multipair,gao2018capacity}. It is shown in \cite{gao2018capacity} that the channel capacity of a system with one-bit transceivers increases linearly with the number of transceivers in a Rayleigh channel, with perfect channel state information at the receiver (CSIR), which promises high data-rate communication with a low probability of error when the number of transceivers is large.

To obtain channel information, training-based scheme is often used in practice, where part of the coherent channel interval is used for training, while the rest part is used for data communication.
There is a trade-off between the training and data:  more training time provides better channel estimation for higher data rate, but the time for data communication is less. This trade-off has been analyzed in \cite{hassibi2003much} for a linear system, where a lower bound of the capacity in a training-based scheme is provided and the optimal training length is analyzed by maximizing the lower bound. The lower bound is obtained by using a worst-case noise analysis.

In \cite{li2017channel} and \cite{li2016much}, lower bounds on the channel capacity with training-based schemes are provided for systems with linear transmitters and one-bit quantized receivers. The authors formulate the quantized output as the combination of signal, Gaussian noise, and quantization noise uncorrelated with the signal, and provide a lower bound using the worst case noise analysis. Such analysis requires Gaussian input signals and is only limited to low SNR for a closed-form expression. We provide a lower bound of the channel capacity of a system with one-bit transceivers by analyzing the mutual information in a training-based scheme. We directly deal with the discrete input and the nonlinearity at the receivers without trying to approximate or linearize the transceiver architecture, and provide the exact achievable rate in a large-scale system.  The method used for one-bit transceivers can also be generalized and applied to systems with other types of nonlinear transceivers.

With the achievable rate of a system with one-bit transceivers in a training-based scheme, we find that the optimal training length can decrease strongly as the number of receivers increases. We also find that the optimal number of training symbols can be smaller than the number of transmitters when we have more receivers than transmitters. We show an example that a system with 5 one-bit transmitters needs only 4 training symbols for high rate communication.

\section{training-based scheme and capacity lower bound}
\label{sec:lower_bound}
We assume a classical discrete-time block-fading channel\cite{hassibi2003much}, where the channel is constant for some discrete interval $\Tb$, after which it changes to an independent value that holds for another interval $\Tb$, and so on. We divide the interval into two phases: $\Tt$ for training and $\Td$ for data transmission, where $\Tt+\Td=\Tb$. For a system with one-bit transceivers, within one coherent interval, the received signal is given by
\begin{subequations}
\begin{align}
    \rxmt &= \sign\left(\sqrt{\snr/\tn}\chm\txmt+\nmt\right),
    \label{eq:training_phase} \\
    \rxmd &= \sign\left(\sqrt{\snr/\tn}\chm\txmd+\nmd\right),
    \label{eq:data_phase}
\end{align}
\label{eq:training_data_model}
\end{subequations}
where $\txmt\in\{\pm1\}^{\tn\times\Tt}$ and $\txmd\in\{\pm1\}^{\tn\times\Td}$ are matrices of  transmitted training signal and data signal, $\rxmt\in\{\pm1\}^{\rn\times\Tt}$ and $\rxmd\in\{\pm1\}^{\rn\times\Td}$ are the corresponding matrices of received signal, $\tn$ and $\rn$ are the number of transmitters and receivers, $\chm$ is a random channel matrix, which is fixed in one coherent time interval, $\nmt$ and $\nmd$ are additive noise, $\sign(\cdot)$ provides the sign of the input as its output. Elements of $\chm,\nmt,\nmd$ are independent and identically distributed ({\it iid}) Gaussian $\cN(0,1)$, $\snr$ is the expected signal power at the receiver. The signals and the channel are modeled as real-valued since only the in-phase (I) information is used and the quadrature (Q) phase is ignored at the receiver. Rayleigh channel is assumed, which appears to hold for non-line of sight (NLOS) channels in many frequency bands \cite{rappaport2014millimeter}, and also appears in the analysis in \cite{ mollen2016one,mollen2017uplink,choi2016near,li2016channel,saxena2017analysis}. For one-bit transceivers with both I and Q phases, the achievable rate will be doubled in a Rayleigh channel, which is also analyzed in \cite{gao2018capacity} in the case when channel is known at the receiver. 

The capacity per transmitter per channel-use is 
\begin{equation*}
\Ctr = \sup\limits_{\Tt,p_{\txmd}(\cdot),\txmd\in\{\pm1\}^{\tn\times\Td}} \frac{1}{\Tb\tn}\MuI(\txmt,\rxmt,\rxmd;\txmd),
\end{equation*}
where $\MuI(\cdot;\cdot)$ is the mutual information notation.  In general, this optimization is difficult to compute, especially for large $\Tb$, $\tn$, and $\rn$.
We use a series of now-standard inequalities to obtain a tractable lower bound.
Let $\txvd(k)$ and $\rxvd(k)$ be the $k$th column of $\txmd$ and $\rxmd$. A lower bound on $\Ctr$ can be obtained by considering $\txvd(k)$ to be {\it iid}\/ with some distribution $p_{\txvd}(\txv)$. Then, 
\begin{align*}
 &\Tb\tn\Ctr \geq  \Ent(\txmd|\txmt,\rxmt) - \Ent(\txmd|\txmt,\rxmt,\rxmd)\\
&= \sum_{k=1}^{\Td}\Ent(\txvd(k)|\txmt,\rxmt) - \sum_{k=1}^{\Td}\Ent(\txvd(k)|\txmt,\rxmt,\rxmd,\\
&\qquad\qquad\txvd(1),\cdots,\txvd(k-1))\\
&\geq \sum_{k=1}^{\Td}\left(\Ent(\txvd(k)|\txmt,\rxmt) - \Ent(\txvd(k)|\txmt,\rxmt,\rxvd(k))\right) \\
&=(\Tb-\Tt)\MuI(\txvd;\rxvd|\txmt,\rxmt),
\numberthis
\label{eq:lower_bound1}
\end{align*}
where $\txvd, \rxvd$ are the same column of $\txmd, \rxmd$. 

This bound is valid for any $\Tt$, and any distribution of $\txmt$ and $\txvd$. We consider that elements of $\txmt$ and $\txmd$ are \iid uniform in $\{\pm1\}$, and a lower bound on $\Ctr$ is
\begin{equation}
    \Ctr\geq C_{\rm bound} =\max_{\Tt} \frac{\Tb-\Tt}{\Tb}\Reff(\Tt),
    \label{eq:lower_bound_capacity}
\end{equation}
where 
\begin{equation}
    \Reff(\Tt) = \frac{1}{\tn}\MuI(\txvd;\rxvd|\txmt,\rxmt)
\end{equation}
is the effective achievable rate per transmitter in each channel-use.
The corresponding training time that optimizes the lower bound is
\begin{equation}
    \Ttopt = \argmax_{\Tt}\frac{\Tb-\Tt}{\Tb}\Reff(\Tt).
    \label{eq:optimal_Tt_setup}
\end{equation}
The complexity of finding $\Reff(\Tt)$ is lower than $\Ctr$ since we do not need to find the optimal $p_{\txmd}(\cdot)$.  Nevertheless, $\Reff(\Tt)$ is still non-trivial to find since the amount of averaging needed to compute $\MuI(\txvd;\rxvd|\txmt,\rxmt)$ is generally exponential in $\tn$ and $\rn$. However, in a large scale limit when $\tn,\rn,\Tb\to\infty$, we can obtain the limit of $\Reff(\Tt)$ using {\it{replica method}}.

The replica method, a tool used in statistical mechanics  \cite{mezard1988spin}, has been applied in many communication system contexts \cite{tanaka2001analysis,tanaka2002statistical,guo2005randomly,wen2016bayes}, neural networks \cite{engel2001statistical}, and error-correcting codes \cite{montanari2000statistical},  image restoration \cite{nishimori1999statistical}. Although the replica method is used for a large scale limit, \cite{gao2018capacity} and \cite{moustakas2003mimo} also show that results obtained from the replica method provide a good approximation for small $\tn$ and $\rn$ ($\approx 8$). A mathematically rigorous justification of the replica method is elusive, but the success of the method maintains its popularity. 

To consider the large $\tn$ and $\rn$ limit, we define $\ratio=\frac{\rn}{\tn},\beta=\frac{\Tb}{\tn}$. Then, (\ref{eq:lower_bound_capacity}) and (\ref{eq:optimal_Tt_setup}) become
\begin{equation}
\Cavg_{\rm{t}}\geq\Cavg_{\rm{bound}}=\max_{\betat}\frac{\beta-\betat}{\beta}\Ravgeff(\betat),\quad\betat=\frac{\Tt}{\tn},
\label{eq:lower_bound_rate_linear_one_bit}
\end{equation}
\begin{equation}
\betatopt=\argmax_{\betat}\frac{\beta-\betat}{\beta}\Ravgeff(\betat),
\label{eq:optimal_Tt_setup_linear_one_bit}
\end{equation}
with $\Cavg_{\rm{bound}}= \lim_{\tn\to\infty}C_{\rm {bound}}, \Cavg_{\rm{t}}=\lim_{\tn\to\infty}\Ctr$,
\begin{equation}
  \Ravgeff(\betat)=\lim_{\tn\to\infty}\frac{1}{\tn}\MuI(\txvd;\rxvd|\txmt,\rxmt),
  \label{eq:Reff_RS_def}
\end{equation}

Through replica method, we show that the mutual information described in (\ref{eq:Reff_RS_def}) is equivalent to that of a new channel of one-bit transceivers with perfect CSIR, but with a new SNR, called ``effective SNR" $\snreff$, which depends on $\snr$ and $\betat$. Some details of the proof of the equivalence and calculation of $\snreff$ is presented in Section \ref{sec:proof}. For now, we show the equivalence in achievable rate, which is the key step in our analysis:
\begin{equation}
    \Ravgeff(\betat)=\lim_{\tn\to\infty}\frac{1}{\tn}\MuI(\txvd;\rxvdequ|\chmequ),
    \label{eq:Reff_equivalence}
\end{equation}
where $\txvd,\chmequ,\rxvdequ$ are related through the new channel
\begin{equation}
    \rxvdequ = \sign\left(\sqrt{{\snreff}/{\tn}}\chmequ\txvd + \nvd\right), 
    \label{eq:equivalent_noise_model}
\end{equation}
where $\chmequ$ is known at the receiver, the elements of $\chmequ$ and $\nvd$ are \iid $\cN(0,1)$, $\snreff$ is calculated from \eqref{eq:snreff_def}. 

With CSIR, \eqref{eq:Reff_equivalence} has been analyzed in \cite{gao2018capacity} and we have
\begin{align*}
&\Ravgeff(\betat) = \min\Big( \ratio(\Cbsc(\snreff)-\Cbsc(A^2r))+\frac{1}{2\ln 2}(\hat{r} + r\hat{r})\\
&-\frac{1}{\sqrt{2\pi}}\int_{\R}\log_2(\cosh(\hat{r}+\sqrt{\hat{r}}z))e^{-z^2/2} dz,1\Big),
\numberthis
\label{eq:Cavg}
\end{align*}
where $\Cbsc(\snr)$ is the capacity of a single transceiver with SNR $\snr$, which is defined as
\begin{equation}
\Cbsc(\snr) = 1-\E_z\left(\Entfuntwo(Q(\sqrt{\snr} z))\right), z\sim\cN(0,1),
\label{eq:single_transceiver_capacity}
\end{equation}
where $\Entfuntwo(p)=-(p\log_2p + (1-p)\log_2(1-p))$ is the binary entropy function, $Q(\cdot)$ is the classical Q-function, and $r,\hat{r},A$ are the solutions of
\begin{equation}
r = \frac{1}{\sqrt{2\pi}}\int_{\R}\tanh(\sqrt{\hat{r}}z+\hat{r})e^{-z^2/2}dz,
\label{eq:q_solution}
\end{equation}
\begin{equation}
\hat{r} = \frac{\ratio A^2}{\pi\sqrt{2\pi}} \int_{\R}
\frac{\exp\left(-(A^2r+\frac{1}{2})z^2\right)}{Q(A\sqrt{r}z)}dz,
\label{eq:E_solution}
\end{equation}
\begin{equation}
A = \sqrt{{\snreff}/({1+\snreff(1-r)})}.
\label{eq:A_solution}
\end{equation} 

For any $\betat$, we can compute $\Ravgeff(\betat)$ numerically through (\ref{eq:Cavg}). By discretizing $\betat$ in the range $(0,\beta)$ to a target accuracy, we can solve for $\Cavg_{\rm{bound}}$ and $\betatopt$ numerically from (\ref{eq:lower_bound_rate_linear_one_bit}) and (\ref{eq:optimal_Tt_setup_linear_one_bit}).

According to \cite{gao2018capacity}, for any $\snreff>0$, when $\ratio\to\infty$,  $\Ravgeff(\betat)\to 1$ which is the upper bound of one-bit transmitters. This implies that  $\Ravgeff(\betat)$ saturates when $\ratio$ is large enough even for small $\betat$, and therefore $\betatopt$ can be smaller than 1, which is different from the linear case\cite{hassibi2003much}. Some numerical results of $\Cavg_{\rm{bound}}$ and $\betatopt$ is shown in Section. \ref{sec:numerical_result}.

\section{Numerical Results}
\label{sec:numerical_result}
For systems with one-bit transceivers, it is possible to have optimal training time smaller than the number of transmitter ($\betatopt<1$) even when the total time is larger than twice of the number of transmitters ($\beta>2$) for large $\ratio$. In a massive MIMO uplink scenario, we have $\ratio=\frac{\rn}{\tn}>>1$. The relationship between $\ratio$ and $\betatopt$ for different values of SNR and $\beta$ is shown in Fig.   \ref{fig:opt_beta_t_SNR10_0} and the corresponding lower bound $\Cavg_{\rm bound}$ is shown in Fig.  \ref{fig:low_bound_vs_alpha}. We can see that $\betatopt$ strongly depends on $\ratio$ for large $\ratio$ at different SNR and with different block lengths, and the numerical result shows that $\betatopt$ decreases by 37 percent when we double $\ratio$, which is fitted by the green dash-dot line in Fig \ref{fig:opt_beta_t_SNR10_0}. When $\ratio$ is large enough, $\betatopt$ can be smaller than 1, even at 0 dB SNR. As is explained in Section \ref{sec:lower_bound}, the decrease of $\betatopt$ is caused by the the saturation of $\Ravgeff(\betat)$, which can also be seen in Fig.  \ref{fig:low_bound_vs_alpha}.

\begin{figure}
\includegraphics[width=3.4in]{../figs/opt_beta_t_vs_alpha_SNR10_0_inf_globecomm.eps}
\centering
    \caption{The optimal training time  $\betatopt$ (\ref{eq:optimal_Tt_setup_linear_one_bit}) versus $\ratio=\frac{\rn}{\tn}$ in a wide range for a system with one-bit transmitters and one-bit receivers. We can see that it's possible to have $\betatopt<1$. Both $\ratio$ and $\betatopt$ are plotted in a log domain. When $\alpha$ is large, $\betatopt$ decreases by 37 percent when we double $\alpha$, which is fitted by the green dash-dot line.}
    \label{fig:opt_beta_t_SNR10_0}
\end{figure}

\begin{figure}
\includegraphics[width=3.5in]{../figs/low_bound_vs_alpha_SNR_10_0_inf_real.eps}
\centering
    \caption{The lower bound $\Cavg_{\rm bound}$ (\ref{eq:lower_bound_rate_linear_one_bit}) versus  $\ratio=\frac{\rn}{\tn}$ in a wide range for a system with one-bit transmitters and one-bit receivers. Because of the one-bit quantization at the transmitter, the rate per transmitter can not be more than 1. Therefore, we can clearly see the saturation effect when we increase $\ratio$.}
    \label{fig:low_bound_vs_alpha}
\end{figure}

Now, we show one example of a system with $\tn=5,\Tt=4,\Tb=25,\beta=\frac{\Tb}{\tn}=5,\betat=\frac{\Tt}{\tn}=0.8$ that achieves a high rate communication with low probability of error in the data phase. We consider 3 configurations of LDPC code in the data phases: code rate 9/10 for both noise free and 0 dB SNR, and code rate 1/2 for 0 dB SNR. The bits of the code are interleaved and distributed among different transmitter antennas and multiple time blocks with BPSK modulation. During the training phase, we are not using \iid sequences. Instead, we design a training matrix $\txmt$ that satisfies
\begin{equation}
    \txmt^\Tp\txv_1\neq\txmt^\Tp\txv_2,\forall \txv_1,\txv_2\in\{\pm1\}^\tn,\txv_1\neq\txv_2,
    \label{eq:Xt_cond}
\end{equation}
which fails with high probability for \iid training sequences when $\tn$ and $\Tt$ are small. The reason of considering this constraint is that the receiver cannot distinguish between $\txvd^{'}$ and $\txvd^{''}$ through a maximum likelihood detector when $\txmt^\Tp\txvd^{'}=\txmt^{\Tp}\txvd^{''}$ which implies
\begin{equation}
    p(\rxvd|\txmt,\rxmt,\txvd^{'})=p(\rxvd|\txmt,\rxmt,\txvd^{''})
\end{equation}
for any $\rxmt,\rxvd,\rn$ and can cause estimation error, which should be avoided for high rate communication with small bit error rate (BER). For the case when $\tn$ and $\Tt$ are large, our conjecture is that (\ref{eq:Xt_cond}) can be satisfied with very high probability even with \iid training, or only a negligible pairs of vectors violate (\ref{eq:Xt_cond}) when compared with total $2^\tn$ possible vectors even when $\Tt\leq \tn$ because of the binary constraint.

The training matrix with $\tn=5,\Tt=4$ that satisfies (\ref{eq:Xt_cond}) is not unique and one example is
\begin{equation}
    \txmt = \begin{bmatrix*}[r]
            -1 & -1 & 1 & 1 \\
             1 & -1 & 1 & -1 \\
             1 & 1 & 1 & 1 \\
             1 & -1 & -1 & 1 \\
            -1 & -1 & 1 & -1 
            \end{bmatrix*}.
            \label{eq:M5_training_sequence}
\end{equation}
We use the same training matrix $\txmt$ for all the time blocks in our simulation. Here is how we compute the log-likelihood ratio (LLR) of $b_k$ which is the $k$th element of $\txvd$ for decoding based on training $\txmt,\rxmt$ and corresponding received data vector $\rxvd$:
\small
\begin{align*}
    &\ln \frac{P(b_k=1|\txmt,\rxmt,\rxvd)}{P(b_k=-1|\txmt,\rxmt,\rxvd)}=\ln \frac{\sum_{b_k=1}p(\txvd|\txmt,\rxmt,\rxvd)}{\sum_{b_k=-1}p(\txvd|\txmt,\rxmt,\rxvd)}\\
    &= \ln \frac{\sum_{b_k=1}p(\rxmt,\rxvd|\txmt,\txvd)}{\sum_{b_k=-1}p(\rxmt,\rxvd|\txmt,\txvd)},
    \numberthis
    \label{eq:LLR_compute}
\end{align*}
\normalsize
where
\begin{equation}
    p(\rxmt,\rxvd|\txmt,\txvd)=\prod_{k=1}^{\rn}p(\rxv_{{\rm t},k}^{\Tp},\rx_{{\rm d},k}|\txmt,\txvd),
    \label{eq:prod_comp}
\end{equation}
$\rxv_{{\rm t},k}^{\Tp}$ is the $k$th row of $\rxmt$, $\rx_{{\rm d},k}$ is the $k$th element of $\rxvd$. Since the elements of $\chm$ are \iid $\cN(0,1)$, we can drop the index $k$ and by considering $p(\rxvt^\Tp,\rxd|\txmt,\txvd)$ in the case of a single receiver for different possible of $\rxd,\rxvt^\Tp$ and $\txvd$, and store those values in a table which can be applied for all $k$. Based on the system model, we have
\begin{align*}
    p(\rxvt^\Tp,\rxd|\txmt,\txvd) &=\E_\bz\prod_{m=1}^{\Tt+1}Q(-\rx(m)\sqrt{\snr}z(m)),
    \numberthis
    \label{eq:condi_prob_ML}
\end{align*}
where $\rx(m)$ is the $m$-th element of $\rxv$ defined as $\rxv = [\rxvt^\Tp,\rxd]^\Tp$, $\bz=\chv^\Tp[\txmt,\txvd]$ with $\chv^\Tp$ as one row of $\chm$, $z(m)$ is the $m$-th element of $\bz$, and $\bz\sim\cN(0,R_\bz)$ with 
\begin{equation}
R_\bz=\frac{1}{\tn}\begin{bmatrix*}[c]
            \txmt^\Tp\txmt & \txmt^\Tp\txvd^{'} \\
            (\txvd^{'})^\Tp\txmt & \tn
            \end{bmatrix*}.
\end{equation}
Then, $p(\rxvt^\Tp,\rxd|\txmt,\txvd)$ can be computed through (\ref{eq:condi_prob_ML}) numerically using Monte Carlo method and (\ref{eq:LLR_compute}) can then be obtained through \eqref{eq:prod_comp} efficiently even for large $\rn$. 

We consider the BER of our system in 3 cases: rate 9/10 LDPC code in noise free case, rate 9/10 LDPC code  at 0 dB SNR, rate 1/2 LDPC code at 0 dB SNR and the simulation results are shown in Fig. \ref{fig:LDPC_M5_beta5_rate}. We can see that the BER can be smaller than $10^{-6}$ as long as $\rn$ is large enough for all those cases, which are $\rn=55,330,95$, and the corresponding $\ratio$ are 11, 66, and 19. The average rates and the values of $\Cavg_{\rm bound}$ (\ref{eq:lower_bound_rate_linear_one_bit}) and $\betatopt$ (\ref{eq:optimal_Tt_setup_linear_one_bit}) with the same $\beta,\ratio$ and SNR for large $\tn$ are shown in TABLE \ref{tab:compariosn_rate_bound} for comparison. We can see that the average achieved rates in the two cases with rate 9/10 LDPC code are very close to $\Cavg_{\rm bound}$, and the corresponding $\betatopt$ are smaller than 1. In the last case with $\ratio=19$, the gap between the average rate and the lower bound $\Cavg_{\rm bound}$ is larger than the other two cases. One reason is that more training is needed since $\betatopt>1$.


\begin{table}[ht]
\caption{Comparison between achieved average rate  and $\Cavg_{\rm bound}$}
\label{tab:compariosn_rate_bound}
\begin{tabular}{ |c|c|c|c| }
\hline
&noise free & 0 dB & 0 dB \\ 
&$\ratio=11$ & $\ratio = 66$ & $\ratio = 19$ \\
\hline
LDPC code rate ($R_{\rm c}$) & 9/10 & 9/10 & 1/2\\
\hline
average rate: $\frac{\beta-\betat}{\beta}R_{\rm c}$ & \multirow{2}{*}{0.756}  & \multirow{2}{*}{0.756} & \multirow{2}{*}{0.42}
\\ 
$(\beta=5,\betat=0.8)$ & & & \\ \hline
$\Cavg_{\rm bound}$& 0.765 & 0.796& 0.545\\ \hline
$\betatopt$& 0.85 &0.72 & 1.32 \\ \hline
\end{tabular}
\end{table}

\begin{figure}
\includegraphics[width=3.3in]{../figs/LDPC_M5_beta5_rate_ML.eps}
\centering
    \caption{The BER of LDPC code vs $\rn$ in the data phase of a one-bit system with $\tn=5,\Tt=4,\Tb=25$, and the training sequences $\txmt$ is defined in (\ref{eq:M5_training_sequence}). BER smaller than $10^{-6}$ can be achieved with $\rn=55,330,95$, and the corresponding $\ratio$ are $11,66,$ and 19.}
    \label{fig:LDPC_M5_beta5_rate}
\end{figure}

\section{Replica Analysis}
\label{sec:proof}
Replica method is used to prove (\ref{eq:Reff_equivalence}), which shows that the average achievable rate per transmitter per channel-use in the data phase in a training-based scheme without CSIR in a large-scale limit defined in (\ref{eq:Reff_RS_def}) is equivalent to that of a channel defined in (\ref{eq:equivalent_noise_model}) with CSIR. We omit many details, and present only the primary steps.

According to the definition of entropy, we have
\begin{equation*}
    \Ent(\rxvd|\txmt,\rxmt) = \frac{-1}{\ln 2}\E\left(\ln \frac{p(\rxvd,\rxmt|\txmt)}{p(\rxmt|\txmt)}\right).
\end{equation*}
Now, we apply ``replica trick" and obtain
\begin{align*}
    \lim_{\tn\to\infty}\frac{\Ent(\rxvd|\txmt,\rxmt)}{\tn}=\lim_{\tn\to\infty}\frac{-1}{\tn\ln2}\lim_{n\to 0}\frac{\partial}{\partial n}\ln\frac{\Xi_n}{\hat{\Xi}_n},
\end{align*}

where
\small
\begin{equation*}
    \Xi_n = \E_{\rxvd,\rxmt,\txmt}[p^n(\rxvd,\rxmt|\txmt)],\hat{\Xi}_n = \E_{\txmt,\rxmt}[p^n(\rxmt|\txmt)].
\end{equation*}
\normalsize
We assume the limit of $n$ and $\tn$ can commute:
\begin{align*}
    \lim_{\tn\to\infty}\frac{\Ent(\rxvd|\txmt,\rxmt)}{\tn}=\lim_{n\to 0}\frac{\partial}{\partial n}\lim_{\tn\to\infty}\frac{-\ln\frac{\Xi_n}{\hat{\Xi}_n}}{\tn\ln2}.
    \numberthis
    \label{eq:replica_Hy_training}
\end{align*}

Also, we consider $n$ as integer to derive $\Xi_n$ and $\hat{\Xi}_n$ as a function of $n$, and we assume the expression still holds for real number $n$ when $n\to 0$. For integer $n$:
\begin{align*}
    &\Xi_n = \E_{\txmt}\sum_{\rxmt,\rxvd}\left(\E_{\txvd,\chm}p(\rxvd,\rxmt|\txvd,\txmt,\chm)\right)^{n+1}\\
    &=\E_{\txmt,[\chm]_{0}^{n},[\txvd]_{0}^{n}}\prod_{k=1}^{\rn}\sum_{\rx_{\text{d},k}}\prod_{a=0}^nQ\left(-\rx_{\text{d},k}\sqrt{{\snr}/{\tn}}\chv^{(a)\Tp}_k\txvd^{(a)}\right)\\
    &\cdot \prod_{k=1}^{\rn}\prod_{p=1}^{\Tt}\sum_{\rx_{\text{t},kp}}\prod_{a=0}^{n}Q\left(-\rx_{\text{t},kp}\sqrt{{\snr}/{\tn}}\chv^{(a)\Tp}_k\txvta[p]\right)\\
    &=\E_{[\txvd]_{0}^{n}}\Big[\E_{[\chv]_{0}^{n}}\Big[\E_{{\bz_{\rm{d}}}}\sum_{\rx_{\text{d}}}\prod_{a=0}^nQ\left(-\rx_{\text{d}}\sqrt{\snr}z_{\rm{d}}^{(a)}\right) \\
    & \cdot\Big[\E_{\bz_{\rm{t}}}\sum_{\rxt}\prod_{a=0}^{n}Q\left(-\rxt\sqrt{\snr}z_{\rm{t}}^{(a)}\right) \Big]^{\Tt}\Big]\Big]^{\rn},
\end{align*}
where $\rx_{\text{t},kp}$ is the $k$th row and $p$th column of $\rxmt$, $\txv_{\text{t},p}$ is the $p$th column of $\txmt$, $\rx_{\text{d},k}$ is the $k$th element of $\rxvd$, $[\chm]_{0}^{n}=\{\chm^{(0)},\cdots,\chm^{(n)}\}$ is a collection of $(n+1)$ replicas of $\chm$, which are \iid with the same distribution as $\chm$,  $\chv^{(a)\Tp}$ is one row of $\chm^{(a)}$,$[\chv]_0^n$ and $[\txvd]_0^n$ are collections of (n+1) replicas of $\chv$ and $\txvd$. $\bzt=[\zt^{(0)},\cdots,\zt^{(n)}]^\Tp,\bzd=[\zd^{(0)},\cdots,\zd^{(n)}]^\Tp$ with  $z^{(a)}_{{\rm{t}}}=\sqrt{1/\tn}\chv^{(a)\Tp}\txvt$, $z_{\rm{d}}^{(a)}=\sqrt{1/\tn}\chv^{(a)\Tp}\txvd^{(a)}$. Based on the central limit theorem (CLT), when $\tn\to\infty$, $\bzt$ and $\bzd$ are both joint Gaussian vectors. Let $\Qh$ and $\Qx$ be the $(n+1)\times(n+1)$ overlap matrices of $[\chv]_0^n$ and $[\txvd]_0^n$, whose elements are defined as $[\Qh]_{ab}=(\chv^{(a)\Tp}\chv^{(b)})/\tn$ and $[\Qx]_{ab}=(\txvd^{(a)\Tp}\txvd^{(b)})/\tn$, and then we have $\bzt\sim\cN(0,\Qh)$ and $\bzd\sim\cN(0,Q_z)$ with $Q_z=\Qh\circ\Qx$ where $\circ$ is the Hadamard product notation. Now, we let
\begin{align*}
\cJ_{\rm t}(\Qh)=\E_{\bz_{\rm{t}}}\sum_{\rxt}\prod_{a=0}^{n}Q\left(-\rxt\sqrt{\snr}z_{\rm{t}}^{(a)}\right),\\
\cJ_{\rm d}(\Qh,\Qx)=\E_{\bz_{\rm{d}}}\sum_{\rxd}\prod_{a=0}^{n}Q\left(-\rxd\sqrt{\snr}z_{\rm{d}}^{(a)}\right),
\numberthis
\label{eq:Jd_def}
\end{align*}
and we have
\begin{equation}
    \Xi_n=\E_{[\txvd]_{0}^{n}}\Big[\E_{[\chv]_0^n}[[\cJ_{\rm t}(\Qh)]^\Tt\cJ_{\rm d}(\Qh,\Qx)]\Big]^\rn.
\end{equation}
Similarly, we have
\begin{align*}
    &\hat{\Xi}_n =\Big[\E_{[\chv]_{0}^{n}}\Big[\cJ_{\rm t}(\Qh) \Big]^{\Tt}\Big]^{\rn}.
\end{align*}
Therefore,
\begin{equation*}
\frac{\Xi_n}{\hat{\Xi}_n}=\E_{[\txvd]_{0}^{n}}\Bigg[\frac{\E_{[\chv]_{0}^{n}}\Big[[\mathcal{J}_{\rm t}(Q_{\rm \chs})]^{\Tt}\mathcal{J}_{\rm d}(Q_{\rm \chs},Q_{\rm \tx})\Big]}{\E_{[\chv]_{0}^{n}}[\mathcal{J}_{\rm t}(Q_{\rm \chs})]^{\Tt}}\Bigg]^{\rn}.
\end{equation*}
Based on the saddle point method \cite{engel2001statistical}, when $\Tt\to\infty$,
\begin{equation}
\frac{\E_{[\chv]_{0}^{n}}\Big[[\mathcal{J}_{\rm t}(Q_{\rm \chs})]^{\Tt}\mathcal{J}_{\rm d}(Q_{\rm \chs},Q_{\rm \tx})\Big]}{\E_{[\chv]_{0}^{n}}[\mathcal{J}_{\rm d}(Q_{\rm \chs})]^{\Tt}}\to \mathcal{J}_{\rm d}({Q}^*_{\rm \chs},Q_{\rm \tx}),
    \label{eq:saddle_point_qh}
\end{equation}
where ${Q}^*_{\rm \chs}$ is the saddle point of $\E_{[\chv]_{0}^{n}}[\mathcal{J}_1(Q_{\rm \chs})]^{\Tt}$, which is also the saddle point of the limit of $\frac{\Ent(\rxmt|\txmt)}{\tn\rn}$. This is clear by applying replica method:  
\begin{align*}
    &\lim_{\tn\to\infty}\frac{\Ent(\rxmt|\txmt)}{\tn\rn}=-\lim_{n\to 0}\frac{\partial}{\partial n}\lim_{\tn\to\infty}\frac{1}{\tn\rn}\ln\hat{\Xi}_n\\
    &=-\lim_{n\to 0}\frac{\partial}{\partial n}\lim_{\tn\to\infty}\frac{1}{\tn}\ln \E_{[\chv]_{0}^{n}}[\mathcal{J}_1(Q_{\rm \chs})]^{\Tt}.
    \numberthis
    \label{eq:training_RS_saddle_point}
\end{align*}
Based on the RS assumption, the diagonal elements of $\Qh^*$ are 1 and the off-diagonal elements of $\Qh^*$ are the same, denoted as $\qh$. By following similar steps in \cite{wen2016bayes}, we have that $\qh$ is the solution of
\begin{equation}
    \frac{\qh}{1-\qh}=\frac{\betat B^2}{\pi}\E_{u}\left(\frac{\exp(-B^2\qh u^2)}{Q(B\sqrt{\qh}u)}\right),
    \label{eq:qh_solution_one_bit_rx}
\end{equation}
where $B=\sqrt{\frac{\snr}{1+\snr(1-\qh)}},u\sim\cN(0,1)$. Then
\begin{align*}
   \cJ_{\rm d}(\sQh,\Qx)=\E_{{\bz}_{\rm{d}}}\sum_{\rxd}\prod_{a=0}^{n}Q\left(-\rxd\sqrt{\snr}z_{\rm{d}}^{(a)}\right),
\end{align*}
with $\bzd\sim\cN(0,\sQh\circ\Qx)$. Because of the special structure of $\sQh$, we can write $\zd^{(a)} = \sqrt{\qh}\zdequ^{(a)}+\sqrt{1-\qh}w_a$ with $\bzdequ=[\zdequ^{(0)},\cdots,\zdequ^{(n)}]^\Tp\sim\cN(0,\Qx)$ independent of $\bw=[w_0,\cdots,w_n]^\Tp\sim\cN(0,I)$. Therefore, we have
\begin{align*}
   \cJ_{\rm d}(\sQh,\Qx)=\E_{\bzdequ}\sum_{\rxd}\prod_{a=0}^{n}Q\left(-\rxd\sqrt{\snreff}\zdequ^{(a)}\right),
\end{align*}
where $\snreff$ is defined as
\begin{equation}
    \snreff={\snr\qh}/({1+\snr(1-\qh)}),
    \label{eq:snreff_def}
\end{equation}
with $\qh$ as the solution of \eqref{eq:qh_solution_one_bit_rx}. Also (\ref{eq:replica_Hy_training}) becomes
\small 
\begin{equation*}
   \lim_{\tn\to\infty}\frac{\Ent(\rxvd|\txmt,\rxmt)}{\tn}=\lim_{n\to 0}\frac{\partial}{\partial n}\lim_{\tn\to\infty}\frac{\ln\E_{[\txvd]_0^n}\cJ_{\rm d}^\rn(\tQh,\Qx)}{-\tn\ln2}. 
\end{equation*}
\normalsize
Now, we consider $\Ent(\rxvdequ|\chmequ)$ based on channel (\ref{eq:equivalent_noise_model}) with $\snreff$ defined in \eqref{eq:snreff_def}. Through replica method, we have
\begin{align*}
    &\lim_{\tn\to\infty}\frac{1}{\tn}\Ent(\rxvdequ|\chmequ)=-\lim_{n\to 0}\frac{\partial}{\partial n}\lim_{\tn\to\infty}\frac{1}{\tn\ln2}\\
    &\ln\E_{[\txvd]_{0}^{n}}\Big[\E_{\bzdequ}\sum_{\rxd}\prod_{a=0}^{n}\E_{\ns}Q(-\rxd\sqrt{\snreff}\zdequ^{(a)})\Big]^\rn.
\end{align*}
Therefore, 
\begin{equation}
    \lim_{\tn\to\infty}\frac{1}{\tn}\Ent(\rxvd|\txmt,\rxmt)=\lim_{\tn\to\infty}\frac{1}{\tn}\Ent(\rxvdequ|\chmequ).
    \label{eq:equ_Hy}
\end{equation}
Similarly,
\begin{equation}
    \lim_{\tn\to\infty}\frac{\Ent(\rxvd|\txvd,\txmt,\rxmt)}{\tn}=\lim_{\tn\to\infty}\frac{\Ent(\rxvdequ|\txvd,\chmequ)}{\tn}.
    \label{eq:equ_Hy_x}
\end{equation}
Combine (\ref{eq:equ_Hy}), (\ref{eq:equ_Hy_x}) and we have (\ref{eq:Reff_equivalence}).

\section{Conclusion}
We provide a lower bound of the channel capacity of one-bit transceivers with unknown channel by analyzing the mutual information in a training-based scheme. Through replica method, we are able to directly deal with the discrete input and quantized output in the analysis of mutual information in a large-scale system without trying to approximate or linearize the nonlinear transceivers, and we show that the effect of training on achievable rate can be fully described by an equivalent channel with perfect CSIR at a new SNR, called effective SNR. Based on the lower bound analysis, we show that the optimal number of training symbols strongly depends on the number of receivers, and the optimal number of training symbols can be significantly smaller than the number of transmitters in a system with one-bit transceivers, which is also supported by the simulation results.


\bibliographystyle{IEEEtran}
\bibliography{bib/refs.bib}

\end{document}
