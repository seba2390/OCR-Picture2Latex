
\section{Dataset Augmentation Results}
\label{sec:results}
This section presents the prediction setup and results for our proposed human-in-the-loop dataset augmentation.

\input{sections/tab-classifiers.tex}
\input{sections/fig-dataset.tex}

\subsection{Experimental Setups}

\subsubsection{Textual classifier selection.}
We predict the topics and opinions of postings using textual classifiers.
We test four such classifiers.
The first is the state-of-the-art deep learning method, RoBERTa \citep{vaswani2017attention,liu2019roberta}, which achieves the best performance.
The other three are traditional classifiers --- including Random Forest (RF) \citep{breiman2001random}, Support Vector Machine (SVM) \citep{chang2011libsvm} and XGBoost \citep{xgboost} --- which use an n-gram-based vectorial representation, where features are weighted with Term Frequency Inverse Document Frequency (TF-IDF)~\citep{rajaraman2011mining}.
We use the implementation of these algorithms from the Python libraries \textit{scikit-learn} \citep{scikit-learn} and \textit{transformers} \citep{wolf-etal-2020-transformers}.

We compare the prediction performance of these models on the $L_0$ labeled dataset (issued from the qualitative study).
We show in \Cref{tab:classifiers} the macro accuracy and F1 scores obtained via $5$-fold cross-validations.
The hyper-parameters are selected via the nested $5$-fold cross-validation and random search. 
Visibly, RoBERTa outperforms all other models in both macro accuracy and macro F1 scores. 
Therefore, in the rest of this paper, we employ RoBERTa for classifying and sampling unlabeled data.

\subsubsection{Iteration setups.}
The test dataset $X_{test}$ used for evaluation contains $114$ labeled Facebook postings.
$X_{test}$ is only used in the convergence evaluation and is kept fixed between iterations.
To keep bounded the human annotation effort, we limit each iteration to $100$ postings.
For each of the four topic, we sample $|X_i|=|X_i^{A}| + |X_i^{T}| + |X_i^{R}| = 10 + 10 + 5= 25$ posts from $U_{i-1}$.
Note that $X_i^{A}$, $X_i^{T}$ and $X_i^{R}$ are the sets of samples selected at iteration $i$ using the three strategies introduces in \cref{subsec:dataset-augmentation}.
Also note that identical postings may be selected multiple times for different topics.

In total, we conduct $7$ iterations of augmentation until we observe convergence in classification performance on $X_{test}$ (see convergence analysis in \Cref{subsec:augmentation-results}).
The first $4$ iterations sampled only Facebook postings as this is the prominent source in our dataset and most used social media in Australia \citep{newman2020reuters}.
After the $5$th iteration, we introduced the other two data sources, Twitter and Youtube.


\subsection{Augmentation Results}
\label{subsec:augmentation-results}

\subsubsection{Augmented dataset statistics.} 
\Cref{tab:dataset} compares the number of postings and opinions between the dataset constructed by the qualitative analysis ($L_0$) and the final labeled dataset after the seventh iteration ($L_7$).
$L_7$ contains $1,381$ postings and $71$ opinions, which is more than double those of $L_0$ ($614$ postings and $65$ opinions). 
We note that \textit{Climate change} is the most prevalent topic in the dataset ($592$ postings in $L_{7}$) while \textit{Austalian bushfire} is the least ($287$ postings).

\subsubsection{Emergence of new opinions.}
During the data augmentation process, the experts continuously evolved the opinion set in addition to labeling new data.
For example, opinions such as ``Covid-19 is a plague sent by God'' were detected and reinforced by the data sampling strategies. 
Similarly, the data sampled uncovered a longer duration of opinions than the range explored by the experts in the qualitative study.
These provided the qualitative researchers with a long-term perspective about how opinions emerge temporally (see more detailed analysis in \cref{sec:opinion-analysis}).
Overall, \cref{tab:dataset} shows that $6$ new opinions have emerged between $L_0$ and $L_7$. 
We refer to \citep{appendix} for a complete list of opinions and their volumes at $L_0$ and $L_7$.

\input{sections/fig-classifier_convergence.tex}

\subsubsection{Convergence analysis.}
\Cref{fig:classifier_convergence}a shows the prediction performance on the test set $X_{test}$, for each topic (accuracy on the left panel, and F1 score on the right panel), over iterations $0$ to $7$.
The solid lines in \Cref{fig:classifier_convergence}b show the performance indicators macro-averaged over topics, together with the cross-validation generalization error (see the iterations and convergence discussion in \Cref{subsec:dataset-augmentation}).

All indicators show that prediction performance improves over subsequent iterations, with the topic \textit{2019-20 Australian bushfire season} demonstrating the fastest growth.
Both accuracy and F1 scores on the test data converge fast in the first $3-4$ iterations, while improvements from the subsequent iterations are limited.
This suggests a reduced marginal utility of the later iterations.
Notably, the performance gain is null between iterations 6 and 7, suggesting that the procedure has converged.
Consequently, we stopped the data augmentation process after the seventh iteration.

The cross-validation performance is stable across iterations.
This is expected as the classifiers learn from the same data on which the generalization is estimated --- i.e., the classifiers are representative of the data they were trained on.
However, the difference between the test set performance and cross-validation performance is indicative of the representativity over the entire dataset which improves as more iterations are performed.
The cross-validation accuracy is consistently lower than the test set accuracy because the test data is more imbalanced than labeled data.
The cross-validation F1 is more optimistic than the test set F1.
Finally, the difference between the two stabilizes for the later iterations, further suggesting the convergence.

\subsubsection{Baseline comparison.} 
We compare the sampling strategies defined in \cref{subsec:dataset-augmentation} with a baseline scenario where we code an equal amount of postings that were all randomly sampled. 
This results in a sequence of baseline batches of postings which are manually annotated by the experts using the exact same procedure as before.
Next, we train classifiers with these baseline batches in iterations and compute the prediction performance on the same test set.
\Cref{subfig:macro_classifier_convergence} shows the baseline performance as dashed lines.
Visibly, the macro accuracy and F1 scores show increasing gaps between the proposed method and the baseline labeling scenario.
This indicates the advantage of our chosen data augmentation strategies, particularly the active learning strategy which is known to outperform random sampling~\citep{tran2018combining}.









