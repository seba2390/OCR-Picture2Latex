
\section{Conclusion}
This work proposes a solution that fills the gap between qualitative and quantitative analysis of problematic online speech.
We construct an ontology (using Wikibase) which is initially populated through a qualitative study.
The latter emerges from both the vocabulary of annotations (the opinions expressed in topics) and collected labeled data from three online social network platforms (Facebook, Twitter, and Youtube).
Next, we collect a large dataset of social media data using keyword search.
Finally, we augment the labeled dataset using a human-in-the-loop machine learning algorithm. 
We present two in-detail case studies with observations of problematic online speech which evolved on an Australian far-right Facebook group. 
Using our machine-labeled dataset, we analyze how problematic opinions emerge over time and how they co-occur.

\subsubsection{Limitations.}
The present study has several limitations, which we group into data and methodological limitations.

The data limitations are mainly related to the human labeling bias, considered platforms, and posting accessibility.
The initial qualitative study, conducted by the team members, may suffer from human labeling bias.
This is a known limitation of qualitative methods, which we partially alleviate using our data augmentation procedure.
Second, this study concentrates on three platforms (Facebook, Twitter, and Youtube), and Facebook makes most of our data sample.
However, all three are mainstream platforms; problematic speech also occurs outside these platforms, and future work would need to account for platforms like 8chan or gab.
Last, our study only leverages public postings --- we do not access the private conversations for technical and ethical reasons. 

We mention four methodological limitations.
First, the quality of the classifier is inferior to any human coder. 
Yet, this is a marginal problem when the goal is not to correctly label each posting but instead to capture patterns across a large number of postings. 
Second, the definition of the set of Internet sources where the data collection occurs remains critical in determining how representative the sample still is; a larger set of Internet places might not address the selection bias (if they are all selected the same way).
Third, the active learning and top confidence sampling strategies that exploit the labeled dataset may further reinforce the initial human sampling bias. 
We mitigate this shortcoming via random sampling strategy.
Last, by design, the classifiers we have deployed cannot identify opinions that were not identified during the qualitative study.
Future research could apply dynamic predictive models designed to capture the label distribution shift and construct an active set of labels.

