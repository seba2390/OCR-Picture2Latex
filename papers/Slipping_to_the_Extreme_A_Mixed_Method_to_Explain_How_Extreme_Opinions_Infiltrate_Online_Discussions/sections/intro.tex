
\section{Introduction}


In 2020, the COVID-19 pandemic alerted the world to complex issues that arise from social media platforms circulating user-generated misinformation, hate speech, and conspiracy theories~\citep{posetti2020disinfodemic}.
Such forms of problematic information \citep{jack2017lexicon} have been studied before, with the influence of disinformation campaigns on elections~\citep{Kim2019}, disaster management~\citep{rajdev2015fake} and other global public health promotions~\citep{bode2018see} being recorded in the literature. 
To date, there exist three primary types of methods for addressing problematic information.
The first type \revK{concentrates} on large-scale monitoring of social media datasets to detect inauthentic accounts (bots and trolls)~\citep{kong2020modeling,kong2020describing,Ram2021a}, coordinated disinformation campaigns~\citep{Rizoiu2018a} and detect the usage of hate speech in social media~\citep{Rizoiu2019}.
The second group aims to understand which platforms, users, and networks contribute to the ``infodemic''~\citep{smith2019mapping,bruns2020covid19,colley2020challenges}.
The third group uses computational modeling to predict future pathways and how the information will spread~\citep{molina2019fake}. 
These studies provide valuable insights into understanding how problematic information spreads and detecting which sources are reshared frequently and by which accounts.
Though the first and third research approaches offer a breadth of knowledge and understanding, there are limitations --- they often have less to say about why certain opinions and views gain traction with vulnerable groups and online communities. Qualitative research methods are well placed to address this gap.

Qualitative methods provide rich, contextual insights into the social beliefs, values, and practices of online communities, which shape how information is shared and how opinions are formed~\citep{boyd2010social,baym2015personal,johns2020will,wu2021cross}.
This is also fundamental to understanding how and why certain opinions and information sources scale to encompass large segments of the online society~\citep{bailo2020online,bruns2020covid19}.
However, a common criticism of qualitative research is that the in-depth knowledge comes at the expense of generating insights of limited representativeness and weak robustness of the findings.
Therefore, there is a gap between the depth of insight gained from ethnographic and qualitative approaches and the breadth of knowledge gained from computational methods from data science.

This paper aims to fill this gap by proposing a mixed-method approach that brings together qualitative insights, large-scale data collection, and human-in-the-loop machine learning approaches.
We apply our method to map both in-depth and in-breadth the problematic information around four topics: \textit{2019-20 Australian bushfire season}, \textit{Climate change}, \textit{COVID-19}, and \textit{Vaccination} on three social media platforms (Facebook, Twitter and Youtube).
Specifically, this work addresses three open questions concerning applying machine learning and qualitative research in analyzing problematic online speech.



The first research question emerges naturally from the gap: \textbf{can we leverage both qualitative and quantitative analysis for studying problematic online speech?}
To address the challenge, we present a complete solution that bridges and facilitates both analyses (shown in \Cref{fig:teaser}). We first build a platform based on an open-source tool, Wikibase, where qualitative and quantitative analysis is conducted. 
It enables constructing an ontology of problematic online speech by performing the qualitative study, which labels data by topics and builds the vocabulary of opinions simultaneously. 
We then collect large-scale raw data using the uncovered vocabulary. 
Next, we employ machine learning algorithms to augment the data labeling process in a human-in-the-loop setting. 
Finally, we show a sample thematic and discourse analysis from the qualitative study focused on two examples of Facebook posts and comments from a far-right public Facebook group, and the quantitative outcome with measurements and statistics of the produced vocabulary.

The second question concerns the scaling of the qualitative approaches.
Such approaches require the team to observe, record and collect online discussions. 
One needs to manually identify online communities where problematic speech occurs and annotate pieces of texts with their underlying opinions.
Therefore, this in-depth exploration faces two challenges --- a significant amount of effort from researchers and the introduction of human bias in the process of collecting information \citep{dixon2016computer}.
While machine learning is known to help data exploration at scale \citep{lin2012large}, a question remains:
\textbf{can we accelerate qualitative research and observations of online behavior with machine learning algorithms?}
We tackle this challenge by adopting the state-of-the-art text classification algorithm, RoBERTa \citep{vaswani2017attention,liu2019roberta}, with a human-in-the-loop learning setting.
We first train the classifiers to identify problematic speech on postings annotated by the qualitative researchers. 
Next, we deploy three strategies to select unlabeled data. 
The active learning~\citep{settles2012active} strategy selects the data for which the classifiers are most uncertain.
The top-confidence strategy selects data that classifiers are most certain about.
The third strategy --- the random strategy --- randomly samples from unlabeled data. 
The qualitative researchers then label the sampled data, introduce the newly labeled data in the ontology, and repeat the procedure iteratively until the predictive performance converges.

The last research question relates to applying the qualitative mapping at scale and analyzing the dynamics of problematic opinions.
The question is \textbf{can we track the dynamics of problematic opinions from online discussions using unlabeled data?}
To answer this question, we leverage the opinion classifiers that we build on the augmented labeled set.
First, we automatically label the opinions in a large set of postings spanning \rev{more than a year, from July 2019 until October 2020.}
This allows us to apply the qualitative-defined coding schema to a significantly larger sample of postings, therefore reducing the unavoidable selection bias of the qualitative study.
\revK{
It also offers \rev{a longitudinal quantitative approach to studying how fringe opinions capture attention via co-occurrence with mainstream opinions.}
We build a network of opinion co-occurrences from the machine-labeled dataset.
We make several observations: first, we investigate the evolution of opinion co-occurrences and highlight three types of dynamics (stable, increasing, and decreasing co-occurrence weight);
next, we examine the conspiracy opinions in the network via centrality measures and identify \rev{their spikes followed by decreasing centrality due to} the efforts of media in debunking them; 
last, we observe that conspiracy opinions are frequently\rev{ rationalized and popularized by} embracing core opinions (e.g., ``Climate change isn't real'').}


The main contributions of this work include:
\begin{itemize}
    \item A mixed-method solution for bridging qualitative and quantitative analysis, including the hosting platform (Wikibase), the initial qualitative study, the unlabeled data collection and the dataset augmentation with machine learning algorithms.
    \item A dataset augmentation procedure that merges qualitative approaches with machine-learning-based human-in-the-loop data augmentation methods.
    \item Case studies of the evolution of problematic online speech in an Australian far-right Facebook group.
    \item Analysis of problematic opinions emergence and co-occurrence by applying quantitative methods on the collected raw data.
\end{itemize}