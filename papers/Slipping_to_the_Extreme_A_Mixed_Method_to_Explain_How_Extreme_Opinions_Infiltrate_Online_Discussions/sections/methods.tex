
\input{sections/fig-teaser.tex}
\section{Methods}

This section details our methodology, which includes three distinct phases implemented sequentially (shown schematically in \Cref{fig:teaser}):
the \emph{qualitative study} (\cref{subsec:qual-study}),
the \emph{unlabeled data collection} (\cref{subsec:unlabeled-data-collection}), and
the \emph{dataset augmentation} using machine learning (\cref{subsec:dataset-augmentation}).


\subsection{Qualitative Study}
\label{subsec:qual-study}

A set of known far-right community pages served as the \emph{data entry point} of the qualitative study, after which we let ourselves guided by users' posting and linking, and recommendation algorithms.
We employed unobtrusive observation approaches to observe internet places where problematic speech occurs, create field notes of rich, qualitative data, construct a vocabulary of opinions to describe it, and gather and label data.

\subsubsection{Choice of qualitative method.}
The team was initially committed to using \emph{digital ethnography} as the \emph{methodological entry point} for studying problematic online content.
Ethnography is a research method that allows the object of the study to ``emerge through fieldwork, as the significant identities and locations unfold''~\citep{hine2015}, rather than predefining a set of users, sites, or keywords to construct the dataset.
When using this method, the researchers are involved hands-on with the participants they study -- i.e., they are visible, participate in discussions and ask questions~\citep{baym2009making}.
However, given the nature of the field and the communities studied in this project, the intrusion or participation of the researcher in community fora may have an undue influence on online discussions.
Therefore, we opted instead for a deep \emph{qualitative study} in which we undertake unobtrusive observations of conversations in public pages, forums, groups, and sites.
However, the rest of the methodology introduced in this paper would work just as well with a proper ethnographic approach.


\subsubsection{Problematic speech.}
Problematic speech is online interactions, speech, and artifacts that are inaccurate, misleading, inappropriately attributed, or altogether fabricated \citep{jack2017lexicon}.
The concept is intentionally broad to encompass concepts like misinformation, disinformation, and hate speech.
\emph{Misinformation} is a type of communication where falsehoods are unintentionally shared by users~\citep[p.~2]{jack2017lexicon}.
\emph{Disinformation} is information that is ``deliberately false and misleading'' \citep[p.~3]{jack2017lexicon} and intended to manipulate users to a particular opinion or worldview, and
\emph{hate speech} refers to ``any form of communication in which others are attacked, denigrated, or intimidated based on religion, ethnicity, gender, national origin, or another group-based trait''~\citep{warner2012detecting,hameleers2021civilized}.
Prior literature suggests an intertwining of these forms of problematic speech as efforts to denigrate outgroups are common to online disinformation campaigns.
\citet{hameleers2021civilized} argue that ``politically motivated, partisan or ideological utterances in false information, such as hate speech and incivility, may be an indicator of disinformation''.

\subsubsection{Study design.}\label{sssec:study_design}
Our qualitative study concentrates on discourses and conversations about four topics manually selected apriori: \textit{2019-20 Australian bushfire season}, \textit{Climate change}, \textit{COVID-19}, and \textit{Vaccination}.
We focus on three major online social media platforms, Facebook, Twitter, and Youtube, selected due to their large volume of discussion around the four chosen topics.
The study unfolded in four steps.
First, from December 2019 through January 2021, one team member undertook unobtrusive observation of discussions, collected field notes and digital artifacts (screenshots, linked data, photos, memes).
Second, the qualitative researcher labeled the data with topics and opinions that she inferred from the content.
Third, the collected data was independently double-coded by a second team member, obtaining an inter-annotator agreement of $81.0\%$.
Forth and last, the two coders reviewed the coded data and resolved disagreements through discussions.

To conduct our digital fieldwork, we first
selected a set of \emph{Internet places} --- Internet place is a generic term denoting where online discussions happen, e.g., Facebook groups or Youtube video comment sections.
In this study, we concentrate solely on publicly accessible places and
identify relevant places using four approaches:
\begin{itemize}
    \item \textbf{News stories identification.} We used the search engines of news content aggregators (e.g., Factiva, Media Cloud, LexisNexis) to identify news stories containing keywords related to chosen topics in the titles.
    Next, we observed the user comments on the articles.
    Finally, we searched social media for postings that mention the news articles. The keyword terms were constantly updated in these early stages of data collection and during iterative processes of coding the data, until a consolidated list was composed (shown in \Cref{tab:keywords}).
    \item \textbf{Page monitoring.} We actively monitored particular users, pages, and Facebook groups found at the previous point.
    We show the analysis of two such groups in \Cref{sec:case_studies}.
    \item \textbf{Cross-page discussion tracking.} We followed links in postings to discussions around the same topics on different Internet places, which we added to the list for tracking.
    \item \textbf{Exploiting recommendations.} We explored social media pages and accounts recommended by the platforms' recommender systems.
    While this introduces algorithmic bias in the sampling, this has been applied in prior literature \citep{woolley2016automation,woolley2018computational} to construct prospective pathways connecting like-minded users.
\end{itemize}

\subsubsection{An ontology to map online problematic speech.}
We collect and store information about four types of entities: \emph{topics}, \emph{postings}, \emph{Internet places} and \emph{opinions}.
The topics are predetermined, while the latter three emerge from the qualitative study.
Note that the postings and Internet places are \emph{data} discovered using the methodology described above, and the opinions are the \emph{vocabulary} describing the data.
Opinions are defined as ideas expressed by a user in a posting.
We construct new opinions during the qualitative study and the data augmentation phase and alter old opinions through merging or splitting.
As a result, we obtain the opinions simultaneously as the data is collected and labeled.%

Both the data (postings and Internet places) and the vocabulary (topics and opinions) are stored in an ontology, in Resource Description Framework (RDF) format~\citep{brickley1999resource}.
Each entry is a triplet linking two entities --- e.g., a posting contains an opinion, or an opinion is linked to a topic.
If, for example, a posting contains more than one opinion, we use multiple triplets, one for each relation.
We use \textit{Wikibase}\footnote{\url{https://wikiba.se/}} as the project's collaborative application for data input and exploration.
Wikibase offers a user-friendly interface to enter new information and connect to existing data (e.g., a new posting expressing an existing opinion);
a navigational tool to explore the links connecting the data; and
an API to search and access the data based on SPARQL queries.







\subsection{Unlabeled Data Collection}
\label{subsec:unlabeled-data-collection}


\input{sections/tab-keywords.tex}
One shortcoming of qualitative studies is the limited representativeness of the gathered data.
This section describes the collection of postings at scale via keyword search.
For each of the four topics, the qualitative study identified a set of keywords (shown in \Cref{tab:keywords}).
The qualitative experts created an initial candidate set of keywords using a mixture of prior knowledge and expertise, as they have been following these topics for years in previous research~\citep{JohnsFlagging2017}.
Next, they fine-tuned the set of keywords based on their frequencies observed during the qualitative study.
Due to the overlap in the messaging between \textit{Australian bushfires} and \emph{Climate change} on one side, and \emph{Covid-19} and \emph{vaccination} on the other side, we present them in two groups.
We use these keywords to search and crawl postings and comments from Facebook (using Crowdtangle\footnote{\url{https://www.crowdtangle.com/}}) and Twitter (using the Twitter commercial APIs).
We further use a customized crawler to gather comments from specific public Facebook pages and groups.
Finally, we use the YouTube API to obtain comments from the Youtube videos mentioned in the Facebook postings.
We obtained a total of $13,321,813$ postings --- $11,437,009$ Facebook postings, $1,793,927$ tweets and $90,877$ Youtube comments.
Our dataset extends from July 2019 until October 2020. \Cref{fig:dataset_profiling} shows the weekly volumes of collected postings.
Note that, for Twitter, we acquired data relating to two time periods: December 2019 -- February 2020 (during the \textit{2019-20 Australian bushfire season}) and March--April 2020 (the starting phase of \textit{Covid-19}).

\input{sections/fig-dataset_profiling.tex}



\subsection{Dataset Augmentation}
\label{subsec:dataset-augmentation}

Here, we describe the process of augmenting the labeled dataset.
The augmentation process has two mandates.
First, we want to leverage the previously collected unlabeled data to create a labeled dataset containing a more encompassing set of opinions and postings compared to the data issued from the qualitative study.
Second, given the size of our unlabeled dataset, we want to maintain the manual labeling effort as limited as possible.
We enrich the dataset iteratively.
At each iteration, we use the machine classifiers to select a batch of previously unlabelled postings which are then annotated by the experts.
We denote the labeled and unlabeled datasets as $L_i$ and $U_i$, respectively, where $i$ indicates the iteration number and $i=0$ is the initial dataset labeled via qualitative analysis.

\input{sections/fig-teaser_classifiers.tex}
\subsubsection{Two levels of classifiers.}
\Cref{fig:teaser-classifiers} shows our classification schema with two levels of connections:
a posting is associated with none, one or more topics and within a topic exist none, one or more opinions.
Given this hierarchy, we deploy two levels of binary classifiers.
\begin{itemize}
    \item At the first level, for a posting
          $\pmb{x}$ we construct the topic classifiers $\hat{y_t}=f_{t,i}(\pmb{x})$ ($\hat{y_t} \in \{0, 1\}$) which determine whether the posting $\pmb{x}$ is about the topic $t$, with the classifier trained on $L_i$.
          Note that we build one classifier for each topic, and a posting can be associated with multiple topics.
          It can also have no topic when $\hat{y_t} = 0, \forall t \in \{1,\dots,4\}$.
          These are off-topic postings.
    \item At the second level, we construct a multi-label opinion classifier for each topic trained with only the opinions associated with a given topic.
    Note that we train the the opinion classifiers solely after the dataset augmentation is complete as we only need the topic classifiers to perform the dataset augmentation.
\end{itemize}
We present a classification example in \Cref{fig:teaser-classifiers} where an unlabeled posting is first determined to be about \textit{Climate change} by the topic classifiers and is then tagged with the opinion ``Climate change is a UN hoax''.
We argue that the proposed scheme with two levels of classifiers is more robust to off-topic postings, as the multi-label opinion classifier is presented only with relevant postings.
Furthermore, a posting can be associated with multiple topics and opinions.

\subsubsection{Unlabeled data sampling.}\label{sssec:sampling_process}
At each iteration, we select a batch of unlabeled postings for manual annotation to augment the labeled dataset.
Within each batch, we aim to balance the \emph{exploitation} of previously labeled data (i.e., the classifiers trained at the previous iteration) and the \emph{exploration} of unlabeled data.
As unlabelled postings require first a topic label (see \Cref{fig:teaser-classifiers}), we only use the output of the topic classifiers.
We employ three strategies to select unlabelled postings at the current iteration, $X_i$
\begin{itemize}
    \item \textbf{Active learning strategy} selects for labeling the postings of which the classifiers are least certain.
    It improves classification performance by selecting unlabeled data around the decision boundary of the learned classifier~\citep{settles2012active}.
    Specifically, we adopt uncertainty sampling in our experiments where uncertainty is defined as~\citep{tran2018combining}:
      \begin{equation}
          u(\pmb{x}) = 1 - p(\hat{y} \mid \pmb{x}; f_{t,i})
      \end{equation}
    where $\hat{y}$ is the predicted label of the candidate $\pmb{x}$ under classifier $f_{t,i}$. We choose candidates with the highest uncertainty values and denote this set as $X_i^{A}$.
    \item \textbf{Top confidence strategy} chooses from unlabeled data where trained classifiers produce the highest confidence scores, i.e., $p(\hat{y} \mid \pmb{x}; f_{t,i})$. This strategy enriches our dataset with data related to the chosen topics, allowing us to deepen the qualitative study. We denote this subset as $X_i^{T}$.
    \item \textbf{Random sampling strategy} favors a completely random exploration by uniformly selecting a set of postings from the unlabeled data.
    Although there is a high likelihood of selecting off-topic postings, the strategy allows uncovering discussions of interest that may lie far from the initial qualitative analysis.
    Similar ideas have been employed in other fields --- e.g., in reinforcement learning, a probability of $\epsilon$ is usually reserved for the Q-learning algorithm to explore random actions \citep{mnih2013playing}.
    Such probability is typically small and in our experiments in \cref{sec:results}, we set the random sampling strategy to account for only $20\%$ of the sampled data.
    We denote this subset as $X_i^{R}$.
\end{itemize}

\subsubsection{Expert annotation.}
At each iteration, the same team members, who performed the qualitative analysis, label the postings returned by the sampling process (\Cref{sssec:sampling_process}).
The predicted labels from the classifiers are hidden during manual labeling.
This ensures that human decisions are not affected by algorithmic predictions.
The human experts inspect both the text and original contexts of given postings --- such as the complete discussions and other metadata (e.g., the videos from Youtube) --- before choosing an existing opinion (or constructing a new opinion) to label a posting as described in \Cref{sssec:study_design}.

\subsubsection{Iterations and convergence.}
We obtain a set of newly annotated postings at the end of each complete iteration that includes data sampling, expert annotation, and retraining the classifiers. %
For each iteration, we compute the expected generalization error via cross-validation, and we evaluate the test error on a dedicated test dataset.
The test dataset was randomly sampled from the unlabeled data and annotated before \rev{performing} the dataset augmentation.
It is kept fixed across iterations and never used in training.
We repeat the dataset augmentation process for several iterations until the convergence of two indicators:
\begin{itemize}
    \item The first indicator is the difference between cross-validation error and test set error.
    An increasingly smaller error indicates that the classifiers generalize better to the larger, unlabeled dataset.
    \item The second indicator is the gain of performance on the test set between two iterations.
    A decreasing gain between iterations shows that the marginal utility of new annotations is increasingly smaller.
\end{itemize}
The iterative process stops when an insignificant gain is made between two consecutive iterations.

