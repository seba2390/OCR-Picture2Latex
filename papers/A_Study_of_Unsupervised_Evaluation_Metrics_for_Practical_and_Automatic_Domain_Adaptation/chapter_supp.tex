\section{Implementations of Unsupervised Domain Adaptation Metrics}

\begin{table*}[h]
    \centering
    \resizebox{1\textwidth}{!}{
        \begin{tabular}{c|c|c|c}
            \toprule
            Metric & w. Source Accuracy & Hard to Attack & Input-level \\
            \hline
            $\mathcal{A}$-distance\cite{BenDavidA} & \checkmark & \checkmark & \\
            $\mathcal{H} \Delta \mathcal{H}$-divergence or MCD~\cite{BenDavidH,MCD} & \checkmark & \checkmark & \\
            MDD~\cite{MDD} & \checkmark & \checkmark & \\
            \hline
            % Importance Weighted Validation (IWV)~\cite{IWV} & \checkmark & & \\ 
            Deep Embedded Validation (DEV)~\cite{DEV} & \checkmark & & \\
            DEVN~\cite{ThreeNV} & \checkmark & & \\
            Entropy ~\cite{SSLEntropy, ADVENT} & & \\ 
            Soft Neighborhood Density (SND)~\cite{SND} & & \\ 
            Mutual Information ~\cite{MI} & & \\ 
            BNM~\cite{ThreeNV} & & \\ 
            ClassAMI~\cite{ThreeNV} & & \\
            \hline
            ISM (ours) & \checkmark & \checkmark &\\
            ACM (ours) & \checkmark & \checkmark& \checkmark\\
            \bottomrule
        \end{tabular}}
    \caption{The metrics of UDA studied in the paper. We implement previous metrics according to their papers and modify them to be positively correlated with target accuracy.}
    \label{tab:metric}
\end{table*}

\subsection{Discrepancy-based Metric:} 

Ben-David's theory~\cite{BenDavidA,BenDavidH} shows that the error rate of a classifier on the target domain can be bounded by the error rate on the source domain and the domain divergence:
\begin{align}
    \epsilon_T(h) \leq \epsilon_S(h)+d_{\mathcal{H}}\left(\mathcal{D}_S, \mathcal{D}_T\right)+\lambda
\end{align}
where $\lambda=\lambda_T+\lambda_S$, and $\lambda_T$ and $\lambda_S$ are the errors of $h^*=\operatorname{argmin}_{h \in H}\left(\epsilon_T(h)+\epsilon_S(h)\right)$ with respect
to $\mathcal{D}_T$ and $\mathcal{D}_S$ respectively. Later works~\cite{DANN} exploits this bound to optimize the domain divergence and source error to minimize the target error. Inspired by this formula, we think that the target error can be approximated by domain divergence and source error. In other words, we can utilize domain divergence and source accuracy as the evaluation metric to measure target accuracy.

We transform these discrepancy-based UDA methods ~\cite{BenDavidA,BenDavidH,MCD,MDD} into UDA metrics. These metrics are composited by source accuracy and domain divergence. We can formalize the UDA metrics as:
\begin{align}
    \mathcal{M}(\mathcal{D}_S, \mathcal{D}_T, \boldsymbol{M}) = 
    A_S(\mathcal{D}_S, \boldsymbol{M}) - d_{\mathcal{M}}(\mathcal{D}_S, \mathcal{D}_T, \boldsymbol{M})
\end{align}
where $\boldsymbol{M}$ is the model to be evaluated, $ A_S$ is source accuracy and $d_{\mathcal{M}}$ is the domain divergence. The model is composed of a feature generator and a classifier: $\boldsymbol{M}=\textbf{f}(\textbf{g}(\cdot))$.
Different metrics for UDA have different $d_{\mathcal{M}}$ terms, we describe each $d_{\mathcal{M}}$ terms in the following.
%and we normalize $d_{\mathcal{M}}$ to [0,1] to balance with the source error $\epsilon_S(h)$. In the following section, we introduce a list of $d_{\mathcal{M}}$ terms that transformed metrics adopt.

\textbf{1) $\mathcal{A}$-distance}~\cite{BenDavidA}:
\begin{align}
    d_{\mathcal{A}}=&2\sup_{h \in \mathcal{H}}|\mathbb{E}_{\mathcal{D}_s} I\left[h=1\right]+\mathbb{E}_{\mathcal{D}_t} I\left[h=0\right]|\nonumber
\end{align}

A domain discriminator $h$ is trained and the accuracy of the domain discriminator is used as the metric. We use one linear layer to model the domain discriminator the same as~\cite{tllib}.
Notably, when evaluating metrics, we only have the validation set of the source and the target domain, but we need to train the domain discriminator on a training set and evaluate it on the other set. So we use 3-fold validation: we split the validation set into three parts, and each time we train the domain discriminator on two parts and evaluate it on the left part. If not specified, for the following metric that needs training additional networks, we use this 3-fold validation to get the metric score.

\textbf{2) $\mathcal{H} \Delta \mathcal{H}$-divergence or MCD}~\cite{BenDavidH,MCD}:
\begin{align}
    d_{\mathcal{H} \Delta \mathcal{H}}&=\sup _{h, h^{\prime} \in \mathcal{H}}\left|\mathbb{E}_{\mathcal{D}_s} I\left[h^{\prime} \neq h\right]-\mathbb{E}_{\mathcal{D}_t} I\left[h^{\prime} \neq h\right]\right|\nonumber
\end{align}

Two additional classifiers are trained on the top of the feature. Apart from supervised training on source features, they also need to agree on the source domain and disagree on the target domain. These two classifiers are modeled by one linear layer. 

\textbf{3) Maximum Mean Discrepancy (MDD)}~\cite{MDD}:
\begin{align}
    d_{f, \mathcal{F}}^{(\rho)}({\mathcal{D}_s}, {\mathcal{D}_t}) = \sup _{f^{\prime}\in\mathcal{F}}\left(\operatorname{disp}_{\mathcal{D}_s}^{(\rho)}\left(f, f^{\prime}\right)-\operatorname{disp}_{\mathcal{D}_t}^{(\rho)}\left(f, f^{\prime}\right)\right)\nonumber
\end{align}
where $f$ is the classifier of the evaluated model and $\operatorname{disp}^{(\rho)}$ is the margin error. An additional classifier $f'$ is trained to agree $f$ on the source domain and disagree $f$ on the target domain. $f'$ is modeled by a linear layer and trained by the algorithm: Eq. (30) in the original paper~\cite{MDD}.

\subsection{Importance Weighted Validation Metric:} 

\textbf{4) Deep Embedded Validation (DEV)}~\cite{DEV}:

DEV is based on the Importance-Weighted cross-validation (IWCV) of the source domain. It needs to train a two-layer domain discriminator $h$ first, and compute IWCV:
\begin{align}
    \ell(\mathbf{x}_i^{s}) &= w_{h}\left(\mathbf{x}_i^{s}\right) I\left(\hat{y}_i^{s}\neq y_i^{s}\right)\nonumber \\
    w_{h}\left(\mathbf{x}_i^{s}\right) &= \frac{n_s}{n_t} \frac{1-h\left(\boldsymbol{z}_i^{s}\right)}{h\left(\boldsymbol{z}_i^{s}\right)}\nonumber
\end{align}
Then DEV adds IWCV and the variance of the risk estimation as follows:
\begin{align}
    \boldsymbol{DEV}&=\operatorname{mean}(\ell)+\eta \operatorname{mean}(W)-\eta \\
    \eta&=-\frac{\widehat{\operatorname{Cov}}\left(\ell, w_h\right)}{\widehat{\operatorname{Var}}\left[w_h\right]}
\end{align}
We use the negative DEV to be positively related to accuracy.

\textbf{5) DEV with normalization (DEVN)}~\cite{ThreeNV}:

In \cite{ThreeNV}, they propose to normalize the weights by either max normalization or standardization to avoid large $\eta$. We implement DEVN with standardization:
\begin{align}
W_{s t}=\frac{W-\bar{W}}{\sigma_W}+1
\end{align}

Then $W_{s t}$ is used in $\boldsymbol{DEV}$.

\subsection{Entropy-based Metric:} 

\textbf{6) Entropy}~\cite{C-Ent,ADVENT}:
\begin{align}
    \boldsymbol{Ent} = -\mathbb{E}_{\mathcal{D}_t}[H(\boldsymbol{p})]=\mathbb{E}_{\mathcal{D}_t}[\sum_k \boldsymbol{p}_k\log \boldsymbol{p}_k]\nonumber
\end{align}

We compute the negative entropy of the predicted probability $\boldsymbol{p}$ of $\boldsymbol{M}$ on target samples.

\textbf{7) Soft Neighborhood Density
(SND)~\cite{SND}}:

In their work, they define the soft neighborhoods as the similarity distribution between target samples and estimate the density by computing the entropy of the distribution. The similarity is defined as $S_{i j}=\left\langle\boldsymbol{p}_i^t, \boldsymbol{p}_j^t\right\rangle$. The similarity distribution is computed as follows:

\begin{align}
P_{i j}&=\frac{\exp \left(S_{i j} / \tau\right)}{\sum_{j^{\prime}} \exp \left(S_{i j^{\prime}} / \tau\right)}
\end{align}
Then SND is defined as:
\begin{align}
\boldsymbol{SND}&=-\frac{1}{N_t} \sum_{i=1}^{N_t} \sum_{j=1}^{N_t} P_{i j} \log P_{i j}
\end{align}

\textbf{8) Mutual Information~\cite{MI}}:

The Mutual Information of the model prediction:
\begin{align}
    \boldsymbol{MI} = H(\mathbb{E}_{\mathcal{D}_t}[\boldsymbol{p}])-\mathbb{E}_{\mathcal{D}_t}[H(\boldsymbol{p})]\nonumber
\end{align}

\subsection{Other Metric:} 

\textbf{9) Batch nuclear-norm maximization (BNM)~\cite{BNM,ThreeNV}}:

BNM is a UDA algorithm that aims to generate diverse and confident predictions. It approaches this via singular value decomposition:
\begin{equation}
\boldsymbol{BNM}=\|P\|_*
\end{equation}
where $P$ is the $\tilde{N}_t \times K$ prediction matrix ($\tilde{N}_t$ is the target validation set size, and $K$ is the number of classes), and $\|P\|_*$ is the nuclear norm (the sum of the singular values) of $P$.

\textbf{10) ClassAMI~\cite{ThreeNV}}:

They propose computing the Adjusted Mutual Information (AMI) between target cluster labels and the predicted labels:
\begin{align}
\boldsymbol{ClassAMI}&=\operatorname{AMI}(P, \operatorname{kmeans}(F)_{\cdot} \text{labels}) \\
P_i &= \underset{k}{\operatorname{argmax}}
[\boldsymbol{p}_i]
\end{align}
where $P$ is the predicted labels for the target data, $\boldsymbol{p}_i$ is the i-th prediction vector, and $F$ is the set of target features.

\subsection{Our Metric:} 

\textbf{11) Inception Score Metric for UDA (ISM)}:

Its formula is presented in the main paper. The MLP classifier $\boldsymbol{h}$ has two layers with a hidden size equal to the feature size (bottleneck dimension). The classifier is trained by the LBFGS optimizer for 200 steps on the source validation set.

\textbf{12) Augmentation Consistency Metric (ACM)}:

Its formula is presented in the main paper. For the data-augmented sample, we use a series of random data augmentation to get it, including Random Resize and Crop, Horizontal Flip, Random Color Jitter, and Random Gaussian Blur.

\begin{table}[t]
    \centering
    \begin{tabular}{ccccc}
        \toprule
        Office31 & A & W & D &     \\
        \hline
        Training & 1,971 & 556 & 498 & \\
        Validation & 846 & 239 & 498 & \\
        \hline
        OfficeHome & Ar & Cl & Pr & Rw     \\
        \hline
        Training & 1,698 & 3,055 & 3,107 & 3,049 \\
        Validation & 729 & 1,310 & 1,332 & 1,308 \\
        \hline
        DomainNet & c & p & r & s     \\
        \hline
        Training & 33,525 & 50,416 & 120,906 & 48,212 \\
        Validation & 14,604 & 21,850 & 52,041 & 20,916 \\
        \hline
        VisDA & Syn & Real & &    \\
        \hline
        Training & 106,677 & 55,388 & & \\
        Validation & 45,720 & 72,372 & & \\
        \bottomrule
    \end{tabular}
    \caption{The statistic of the training set and the validation set of the datasets used in the paper. Following the 70\%/30\% scheme, we split Office31, OfficeHome, and the ``Synthetic'' domain of VisDA into no overlapping training and validation sets.}
    \label{tab:datasets}
\vspace{-2mm}
\end{table}

\section{Details of UDA training}
\subsection{Datasets Splitting}
Generally speaking, the validation set contains different samples with the training set to show generalization. However, Office31 and OfficeHome do not split the training and validation set, so previous works report the accuracy of the target training set. To solve this historical issue, we follow a 70\%/30\% split scheme to split the training and the validation set for Office31 and OfficeHome (except for ``D'' domain of Office31, due to limited samples) and report the target accuracy on the validation set. We also split the training and validation set for the source domain of VisDA2017, as metrics will utilize the source validation set. The training sets and validation sets are listed in Tab~\ref{tab:datasets}

\subsection{Training Implement Details}
Following Transfer-Learning-Library~\cite{tllib}, we train all five methods (Source only, DANN, CDAN, MDD, MCC) through SGD with $0.9$ momentum, and the learning rate of ResNet backbone is scaled by $0.1$. We schedule the learning rate with the commonly used strategy: the learning rate is adjusted by $\eta_p=\frac{\eta_0}{(1+\alpha q)^{\beta}}$,  where $q$ is the training progress linearly changing from $0$ to $1$, $\eta_0=0.01$, $\alpha=10$, $\beta=0.75$. The batch size is set to $32$ for all training. We train each model with one V100 GPU. For the architecture of the model, $\boldsymbol{M}=\textbf{f}(\textbf{g}(\cdot))$, the feature generator $\textbf{g}$ contain a ResNet~\cite{ResNet} backbone and a bottleneck layer, and the classifier $\textbf{f}$ is a linear layer. We use ResNet50 as the backbone for Office31 and OfficeHome, and ResNet101 for VisDA and DomainNet. We train every model for 3000 steps on Office31, OfficeHome, and VisDA, and 6000 steps on DomainNet in total. 

\begin{figure}[htbp]
\begin{minipage}[t]{1\linewidth}
    \centering
    \resizebox{1\textwidth}{!}{  
        \begin{tabular}{c|a|a|a|a|a|a}
            \toprule
            Datasets & \multicolumn{4}{|c}{OfficeHome} & \multicolumn{4}{|c}{VisDA2017} & \multicolumn{4}{|c}{DomainNet} \\
            \hline
            Train Method & \multicolumn{2}{|c}{MI} & \multicolumn{2}{|c}{AC} & \multicolumn{2}{|c}{MI} & \multicolumn{2}{|c}{AC} & \multicolumn{2}{|c}{MI} & \multicolumn{2}{|c}{AC} \\
            \hline
            Metric & corr & dev & corr & dev & corr & dev & corr & dev & corr & dev & corr & dev \\
            \hline
            MI & 0.67 & 7.97 & 0.55 & 5.76 & - 0.21 & 15.3 & 0.87 & 1.99 & -0.75 & 18.9 & 0.95 & 1.62 \\
            ISM & 0.89 & 1.73 & \textcolor{red}{\textbf{0.97}} & 1.43 & 0.87 & 1.77 & 0.89 & 1.57 & 0.88 & \textcolor{red}{\textbf{0.0}} & \textcolor{red}{\textbf{0.97}} & \textcolor{red}{\textbf{0.62}} \\
            ACM & \textcolor{red}{\textbf{0.92}} & \textcolor{red}{\textbf{1.15}} & 0.62 & \textcolor{red}{\textbf{1.37}} & \textcolor{red}{\textbf{0.99}} & \textcolor{red}{\textbf{0.0}} & \textcolor{red}{\textbf{0.89}} & \textcolor{red}{\textbf{0.59}} & \textcolor{red}{\textbf{0.91}} & \textcolor{red}{\textbf{0.0}} & 0.96 & 1.55 \\
            \bottomrule
    \end{tabular}}
    \captionof{table}{A test of whether metrics are attackable on different datasets. These three metrics are transformed into training loss, and then trained models are evaluated by themselves.}
    \label{tab:robustness}
\vspace{2mm}
\end{minipage}

\begin{minipage}[t]{1\linewidth}
    \centering
    \resizebox{1\textwidth}{!}{
    \begin{tabular}{c|a|a|a|a|a|a}
        \toprule
        Training Method & \multicolumn{2}{|c}{Source only} & \multicolumn{2}{|c}{DANN} & \multicolumn{2}{|c}{CDAN} & \multicolumn{2}{|c}{MDD} & \multicolumn{2}{|c}{MCC} & \multicolumn{2}{|c}{ALL} \\
        \hline
        Metric & corr & dev & corr & dev & corr & dev & corr & dev & corr & dev & corr & dev \\
        \hline
        MDD & -0.65 & 7.39 & 0.58 & 6.29 & -0.11 & 4.73 & 0.66 & 0.85 & 0.34 & 4.38 & 0.36 & 5.93 \\
        %IWCV & 0.22 & 7.39 & -0.09 & 6.67 & 0.01 & 5.71 & -0.09 & 47.54 & 0.18 & 5.07 & -0.03 & 32.52 \\
        DEVN & -0.6 & 7.39 & -0.16 & 6.67 & -0.15 & 8.99 & 0.55 & 4.97 & 0.47 & 4.99 & 0.37 & 32.51 \\
        BNM & 0.11 & 6.35 & 0.60 & 3.65 & -0.03 & 3.72 & \textcolor{blue}{0.85} & \textcolor{red}{\textbf{0.00}} & 0.02 & 3.41 & 0.48 & 3.41 \\
        ClassAMI & -0.40 & 7.39 & 0.74 & 6.29 & -0.12 & 9.81 & \textcolor{red}{\textbf{0.94}} & 1.29 & 0.15 & 7.19 & 0.61 & 7.76 \\
        \hline
        ISM & \textcolor{red}{\textbf{0.84}} & \textcolor{red}{\textbf{0.31}} & \textcolor{blue}{0.75} & \textcolor{blue}{3.92} & \textcolor{blue}{0.42} & \textcolor{blue}{1.23} & 0.75 & \textcolor{blue}{0.40} & \textcolor{blue}{0.88} & \textcolor{red}{\textbf{0.66}} & \textcolor{blue}{0.59} & \textcolor{red}{\textbf{1.66}} \\
        ACM & \textcolor{blue}{0.80} & \textcolor{blue}{2.38} & \textcolor{red}{\textbf{0.79}} & \textcolor{red}{\textbf{1.18}} & \textcolor{red}{\textbf{0.61}} & \textcolor{red}{\textbf{0.98}} & \textcolor{blue}{0.85} & \textcolor{red}{\textbf{0.0}} & \textcolor{red}{\textbf{0.93}} & \textcolor{blue}{1.66} & \textcolor{red}{\textbf{0.76}} & \textcolor{red}{\textbf{1.66}} \\
        \bottomrule
    \end{tabular}}
    \caption{Consistency between metrics of UDA and target accuracy on VisDA2017.}
    \label{tab:visda2}
\vspace{2mm}
\end{minipage}

\begin{minipage}[t]{1\linewidth}
    \centering
    \resizebox{1\textwidth}{!}{
    \begin{tabular}{c|a|a|a|a|a|a}
        \toprule
        Training Method & \multicolumn{2}{|c}{Source only} & \multicolumn{2}{|c}{DANN} & \multicolumn{2}{|c}{CDAN} & \multicolumn{2}{|c}{MDD} & \multicolumn{2}{|c}{MCC} & \multicolumn{2}{|c}{ALL} \\
        \hline
        Metric & corr & dev & corr & dev & corr & dev & corr & dev & corr & dev & corr & dev \\
        \hline
        MDD & 0.53 & 4.44 & 0.72 & 1.67 & 0.83 & 1.88 & 0.83 & \textcolor{blue}{1.13} & 0.05 & 5.99 & 0.3 & 6.2 \\
        DEVN & 0.18 & 3.48 & -0.06 & 6.38 & 0.08 & 7.10 & 0.89 & 12.03 & 0.27 & 8.45 & 0.52 & 10.14 \\
        BNM & -0.59 & 6.63 & 0.48 & 4.53 & 0.88 & 1.30 & 0.93 & 1.98 & 0.37 & 17.19 & 0.54 & 17.28 \\
        ClassAMI & \textcolor{red}{\textbf{0.79}} & 3.57 & \textcolor{red}{\textbf{0.77}} & 2.93 & 0.86 & 1.25 & 0.93 & \textcolor{blue}{1.08} & \textcolor{blue}{0.71} & \textcolor{red}{\textbf{1.16}} & 0.81 & \textcolor{red}{\textbf{1.24}} \\
        \hline
        ISM & 0.72 & \textcolor{blue}{1.47} & 0.6 & \textcolor{blue}{1.68} & \textcolor{red}{\textbf{0.91}} & \textcolor{blue}{1.15} & \textcolor{red}{\textbf{0.97}} & 1.45 & 0.70 & 1.96 & \textcolor{blue}{0.88} & 1.96 \\
        ACM & \textcolor{blue}{0.75} & \textcolor{red}{\textbf{1.37}} & \textcolor{red}{\textbf{0.77}} & \textcolor{red}{\textbf{1.16}} & \textcolor{blue}{0.90} & \textcolor{red}{\textbf{1.13}} & \textcolor{blue}{0.95} & \textcolor{red}{\textbf{0.93}} & \textcolor{red}{\textbf{0.94}} & \textcolor{blue}{1.36} & \textcolor{red}{\textbf{0.93}} & \textcolor{blue}{1.73} \\
        \bottomrule
    \end{tabular}}
    \captionof{table}{The ``corr'' and ``dev'' results are averaged over the 12 transfer tasks of OfficeHome. }
    \label{tab:officehome2}
\vspace{2mm}
\end{minipage}

\begin{minipage}[t]{1\linewidth}
    \centering
    \resizebox{1\textwidth}{!}{
    \begin{tabular}{c|a|a|a|a|a|a}
        \toprule
        Training Method & \multicolumn{2}{|c}{Source only} & \multicolumn{2}{|c}{DANN} & \multicolumn{2}{|c}{CDAN} & \multicolumn{2}{|c}{MDD} & \multicolumn{2}{|c}{MCC} & \multicolumn{2}{|c}{ALL} \\
        \hline
        Metric & corr & dev & corr & dev & corr & dev & corr & dev & corr & dev & corr & dev \\
        \hline
        MDD & 0.26 & 2.12 & 0.54 & 3.46 & 0.72 & 1.11 & 0.7 & 12.75 & -0.55 & 10.39 & 0.54 & 4.99 \\
        DEVN & 0.81 & 1.47 & 0.67 & 5.40 & 0.80 & 7.99 & 0.87 & 0.38 & 0.50 & 9.95 & 0.0 & 2.45 \\
        BNM & 0.34 & 3.83 & 0.58 & 6.65 & 0.65 & 4.28 & 0.34 & 14.28 & 0.74 & 1.37 & 0.59 & 2.85 \\
        ClassAMI & 0.57 & \textcolor{red}{\textbf{1.27}} & 0.52 & 3.46 & 0.60 & 5.62 & 0.61 & \textcolor{red}{\textbf{0.04}} & 0.81 & 1.37 & 0.65 & 3.55 \\
        \hline
        ISM & \textcolor{blue}{0.85} & \textcolor{blue}{1.33} & \textcolor{red}{\textbf{0.87}} & \textcolor{blue}{0.7} & \textcolor{blue}{0.96} & \textcolor{blue}{0.41} & \textcolor{red}{\textbf{0.98}} & \textcolor{blue}{0.28} & \textcolor{red}{\textbf{0.92}} & \textcolor{blue}{0.6} & \textcolor{red}{\textbf{0.91}} & \textcolor{blue}{1.13} \\
        ACM & \textcolor{red}{\textbf{0.94}} & 1.41 & \textcolor{blue}{0.8} & \textcolor{red}{\textbf{0.27}} & \textcolor{red}{\textbf{0.98}} & \textcolor{red}{\textbf{0.29}} & \textcolor{blue}{0.93} & \textcolor{red}{\textbf{0.04}} & \textcolor{blue}{0.84} & \textcolor{red}{\textbf{0.15}} & \textcolor{blue}{0.87} & \textcolor{red}{\textbf{0.29}} \\
        \bottomrule
    \end{tabular}}
    \captionof{table}{The ``corr'' and ``dev'' results are averaged over 12 transfer tasks of DomainNet. }
    \label{tab:domainnet2}
\vspace{2mm}
\end{minipage}

\begin{minipage}[t]{1\linewidth}
    \centering
    \resizebox{0.6\textwidth}{!}{  
        \begin{tabular}{c|a|a|a}
            \toprule
            Training Method & \multicolumn{2}{|c}{DANN} & \multicolumn{2}{|c}{CDAN} & \multicolumn{2}{|c}{ALL} \\
            \hline
            Metric & corr & dev & corr & dev & corr & dev \\
            \hline
            $\mathcal{A}$-distance & 0.37 & 1.69 & 0.34 & 1.3 & 0.35 & 3.11 \\
            MCD & 0.46 & \textcolor{blue}{1.48} & 0.32 & 1.71 & 0.48 & 2.20 \\
            MDD & 0.53 & 2.46 & 0.38 & 1.34 & 0.63 & 2.22 \\
            DEV & NaN & - & NaN & -  & NaN & - \\
            DEVN & NaN & - & NaN & -  & NaN & - \\
            Entropy & 0.4 & 2.47 & 0.57 & 2.59 & 0.55 & 2.01 \\
            SND & 0.43 & 6.70 & 0.44 & 3.09 & 0.57 & 6.14 \\
            MI & 0.38 & 2.26 & 0.58 & 1.51 & 0.53 & 2.74 \\
            BNM & 0.29 & 2.99 & 0.54 & 1.75 & 0.32 & 3.59 \\
            ClassAMI & 0.56 & 1.83 & \textcolor{blue}{0.61} & 2.06 & 0.67 & 3.46 \\
            \hline
            ISM & \textcolor{red}{\textbf{0.73}} & 1.52 & \textcolor{red}{\textbf{0.63}} & \textcolor{red}{\textbf{1.04}} & \textcolor{blue}{0.71} &  \textcolor{red}{\textbf{1.41}} \\
            ACM & \textcolor{blue}{0.71} & \textcolor{red}{\textbf{1.46}} & 0.59 & \textcolor{blue}{1.21} & \textcolor{red}{\textbf{0.75}} & \textcolor{blue}{1.84} \\
            \bottomrule
    \end{tabular}}
    \captionof{table}{Consistency between metrics of UDA and target accuracy on Office31. The results are averaged across 6 transfer tasks of Office31.}
    \label{tab:office31}
\vspace{0mm}
\end{minipage}
\end{figure}

\section{Experimental Results}

\subsection{More Consistency Results}

In Section 4.2 of the main paper, we show the results of comparing our ISM and ACM to previous metrics on three datasets. In the Appendix, we add three more previous metrics~\cite{ThreeNV}: DEVN, BNM, and ClassAMI. We show the consistency of each metric with target accuracy on four UDA datasets with five UDA training methods. Tab.~\ref{tab:visda2}, Tab.~\ref{tab:domainnet2}, Tab.~\ref{tab:officehome2} and Tab.~\ref{tab:office31} show the results on VisDA2017, DomainNet, OfficeHome and Office31 respectively.

We find the performance of the metric can vary largely for different training methods. DEVN, BNM, and ClassAMI demonstrate excellent performance in certain cases, yet they may also exhibit significant errors under other conditions. Our ISM and ACM show decent performance for all training methods on all datasets. We find the results of all metrics decrease on Office31, which may be due to small validation sets. DEV and DEVN will collapse on Office31 because source accuracy can be $1$.

\begin{figure}[htbp]
\begin{minipage}[t]{1\linewidth}
	\centering
	\resizebox{1.0\textwidth}{!}{  
		\begin{tabular}{cccccccccccccc}
			\toprule
			Method & c $\to$ p & c $\to$ r & c $\to$ s & p $\to$ c & p $\to$ r & p $\to$ s & r $\to$ c & r $\to$ p & r $\to$ s & s $\to$ c & s $\to$ p & s $\to$ r & Avg    \\
			\hline
			DANN (default) & 35.9 & 54.3 & 43.8 & 38.0 & 54.9 & 35.5 & 49.8 & 50.1 & 38.3 & 54.4 & 43.8 & 53.2 & 46.0 \\
			DANN (searched) & 38.0 & 54.5 & 44.7 & 40.7 & 56.1 & 37.9 & 50.7 & 50.7 & 38.3 & 55.0 & 44.7 & 53.7 & 47.1\\
			Gains ($+\Delta$) & \textcolor{red}{+2.1} & \textcolor{red}{+0.2} & \textcolor{red}{+0.9} & \textcolor{red}{+2.7} & \textcolor{red}{+1.2} & \textcolor{red}{+2.4} & \textcolor{red}{+0.9} & \textcolor{red}{+0.6} & \textcolor{red}{+0.0} & \textcolor{red}{+0.6} & \textcolor{red}{+0.9} & \textcolor{red}{+0.5} & \textcolor{red}{\textbf{+1.1}}\\
			\hline
			CDAN (default) & 40.0 & 55.8 & 44.6 & 44.2 & 57.3 & 39.8 & 55.2 & 53.3 & 41.5 & 56.9 & 46.3 & 55.5 & 49.2 \\
			CDAN (searched) & 40.6 & 56.5 & 45.1 & 45.5 & 58.4 & 40.3 & 55.4 & 53.1 & 42.3  & 57.1 & 46.6 & 56.4 & 49.8 \\
			Gains ($+\Delta$) & \textcolor{red}{+0.6} & \textcolor{red}{+0.7} & \textcolor{red}{+0.5} & \textcolor{red}{+1.3} & \textcolor{red}{+1.1} & \textcolor{red}{+0.5} & \textcolor{red}{+0.2} & \textcolor{green}{-0.2} & \textcolor{red}{+0.8} & \textcolor{red}{+0.2} & \textcolor{red}{+0.3} & \textcolor{red}{+0.9} & \textcolor{red}{\textbf{+0.6}} \\
			\hline
			MDD (default) & 42.3 & 58.4 & 46.6 & 48.5 & 60.1 & 43.6 & 56.8 & 56.3 & 46.3 & 57.2 & 44.8 & 57.2 & 51.5 \\
			MDD (searched) & 42.5 & 58.6 & 47.0 & 48.5 & 60.1 & 43.6 & 57.3 & 55.9 & 46.6 & 57.5 & 45.0 & 57.2 & 51.7 \\
			Gains ($+\Delta$) & \textcolor{red}{+0.2} & \textcolor{red}{+0.2} & \textcolor{red}{+0.4} & \textcolor{red}{+0.0} & \textcolor{red}{+0.0} & \textcolor{red}{+0.0} & \textcolor{red}{+0.5} & \textcolor{green}{-0.4} & \textcolor{red}{+0.3} & \textcolor{red}{+0.3} & \textcolor{red}{+0.2} & \textcolor{red}{+0.0} & \textcolor{red}{\textbf{+0.2}} \\
			\hline
			MCC (default) & 35.1 & 49.2 & 40.6 & 41.0 & 56.0 & 36.2 & 48.3 & 49.0 & 36.3 & 51.9 & 38.9 & 49.9 & 44.4 \\
			MCC (searched) & 41.2 & 53.6 & 44.5 & 51.1 & 59.9 & 40.7 & 58.5 & 54.8 & 38.2 & 61.7 & 47.6 & 55.0 & 50.6 \\
			Gains ($+\Delta$) & \textcolor{red}{+6.1} & \textcolor{red}{+4.4} & \textcolor{red}{+3.9} & \textcolor{red}{+10.1} & \textcolor{red}{+3.9} & \textcolor{red}{+4.5} & \textcolor{red}{+10.2} & \textcolor{red}{+5.8} & \textcolor{red}{+1.9} & \textcolor{red}{+9.8} & \textcolor{red}{+8.7} & \textcolor{red}{+5.1} & \textcolor{red}{\textbf{+6.2}} \\
			\bottomrule
	\end{tabular}}
	\captionof{table}{The hyper-parameters found by our metric v.s. the default hyper-parameters in original papers on DomainNet.}
	\label{tab:domainnet-search}
\vspace{1mm}
\end{minipage}

\begin{minipage}[t]{1\linewidth}
    \centering
    \resizebox{1.0\textwidth}{!}{  
        \begin{tabular}{clc}
            \toprule
            Method & Ar $\to$ Cl Ar $\to$ Pr Ar $\to$ Rw Cl $\to$ Ar Cl $\to$ Pr Cl $\to$ Rw\,Pr $\to$ Ar Pr $\to$ Cl Pr $\to$ Rw Rw $\to$ Ar Rw $\to$ Cl\,Rw $\to$ Pr  & Avg    \\
            \hline
            DANN (default) &\quad 49.0\qquad 61.3\qquad 72.9\qquad53.5\qquad 66.6\qquad 68.6\qquad 55.0\qquad 50.4\qquad 75.2       \,\,\,\qquad 67.1       \,\qquad 56.3      \,\qquad 79.3& 62.9  \\
            DANN (searched) &\quad 51.2\qquad 62.3\qquad 74.2\qquad56.7\qquad 66.0\qquad 70.8\qquad 58.7\qquad 52.7\qquad 76.0       \,\,\,\qquad 67.5      \,\qquad 57.8      \,\qquad 80.8& 64.5  \\
            Gains ($+\Delta$) &\quad \textcolor{red}{+2.2}\qquad \textcolor{red}{+1.0}\qquad \textcolor{red}{+1.3}  \,\,\,\,\quad \textcolor{red}{+3.2}\qquad \textcolor{green}{-0.6}\qquad \textcolor{red}{+2.2}\qquad \textcolor{red}{+3.7}  \,\,\,\,\quad \textcolor{red}{+2.3}  \,\,\,\,\quad \textcolor{red}{+0.8} \,\,\qquad \textcolor{red}{+0.4} \,\qquad \textcolor{red}{+1.5} \,\qquad \textcolor{red}{+1.5}& \textcolor{red}{\textbf{+1.6}}\\
            \hline
            CDAN (default) &\quad 50.4\qquad 69.4\qquad 73.5\qquad56.7\qquad 69.4\qquad 69.1\qquad 57.3\qquad 50.5\qquad 75.5       \,\,\,\qquad 70.6       \,\qquad 55.8      \,\qquad 80.6& 64.9  \\
            CDAN (searched) &\quad 51.1\qquad 69.2\qquad 74.3\qquad58.4\qquad 70.3\qquad 69.7\qquad 61.6\qquad 50.6\qquad 77.5       \,\,\,\qquad 71.4       \,\qquad 56.7      \,\qquad 81.1& 66.0 \\
            Gains ($+\Delta$) &\quad \textcolor{red}{+0.7}\qquad \textcolor{green}{-0.3}\qquad \textcolor{red}{+0.8}  \,\,\,\,\quad \textcolor{red}{+1.7}  \,\,\,\,\quad \textcolor{red}{+0.7}\qquad \textcolor{red}{+0.6}\,\,\,\,\,\quad \textcolor{red}{+4.3}  \,\,\,\,\quad \textcolor{red}{+0.1}  \,\,\,\,\quad \textcolor{red}{+2.0} \,\,\qquad \textcolor{red}{+0.8} \,\qquad \textcolor{red}{+0.9} \,\qquad \textcolor{red}{+0.5}& \textcolor{red}{\textbf{+1.1}} \\
            \hline
            MDD (default) &\quad 51.1\qquad 70.6\qquad 72.1\qquad57.3\qquad 70.6\qquad 76.6\qquad 59.5\qquad 53.9\qquad 74.9       \,\,\,\qquad 70.5       \,\qquad 58.6      \,\qquad 81.7& 66.4  \\
            MDD (searched) &\quad 52.9\qquad 72.2\qquad 75.2\qquad58.8\qquad 71.9\qquad 76.6\qquad 58.7\qquad 52.4\qquad 76.8      \,\,\,\qquad 69.8       \,\qquad 59.2      \,\qquad 81.8& 67.2 \\
            Gains ($+\Delta$) & \quad \textcolor{red}{+1.8}\qquad \textcolor{red}{+1.6}\qquad \textcolor{red}{+3.1}  \,\,\,\,\quad \textcolor{red}{+1.5}\qquad \textcolor{red}{+1.3}\qquad \textcolor{red}{+0.0}\qquad \textcolor{green}{-0.8} \,\,\,\,\,\,\quad \textcolor{green}{-1.5} \,\,\,\,\,\quad \textcolor{red}{+1.9} \,\,\,\qquad \textcolor{green}{-0.7} \,\qquad \textcolor{red}{+0.6} \,\qquad \textcolor{red}{+0.1} & \textcolor{red}{\textbf{+0.8}}\\
            \hline
            MCC (default) &\quad 55.5\qquad 77.7\qquad 80.2\qquad62.8\qquad 75.2\qquad 75.8\qquad 61.7\qquad 50.6\qquad 78.3      \,\,\,\qquad 69.7       \,\qquad 56.3      \,\qquad 83.4& 68.9  \\
            MCC (searched) &\quad 56.1\qquad 78.5\qquad 79.0\qquad63.6\qquad 75.2\qquad 76.6\qquad 64.1\qquad 52.3\qquad 78.3       \,\,\,\qquad 71.6       \,\qquad 56.1      \,\qquad 83.5& 69.5 \\
            Gains ($+\Delta$) &\quad \textcolor{red}{+0.6}\,\qquad \textcolor{red}{+0.8}\qquad \textcolor{green}{-1.2}  \,\,\,\,\quad \textcolor{red}{+0.8}\qquad \textcolor{red}{+0.0}\,\,\,\,\,\quad \textcolor{red}{+0.8}\,\,\,\,\,\quad \textcolor{red}{+2.4}  \,\qquad \textcolor{red}{+1.7}  \,\,\,\,\quad \textcolor{red}{+0.0} \,\,\qquad \textcolor{red}{+1.9} \,\,\qquad \textcolor{green}{-0.2} \,\qquad \textcolor{red}{+0.1}& \textcolor{red}{\textbf{+0.6}} \\
            \bottomrule
    \end{tabular}}
    \captionof{table}{The hyper-parameters found by our metric v.s. the default hyper-parameters in original papers on OfficeHome. The target accuracy of 12 transfer tasks is reported.}
    \label{tab:officehome-search}
\vspace{1mm}
\end{minipage}

\begin{minipage}[t]{1\linewidth}
	\centering
        \resizebox{0.8\textwidth}{!}{  
	\begin{tabular}{cccccccc}  
		\toprule
		Method & A $\to$ W & A $\to$ D & W $\to$ A & W $\to$ D & D $\to$ A & D $\to$ W & Avg    \\
		\hline
		DANN (default) & 90.4 & 81.7 & 69.6 & 97.8 & 72.3 & 93.7 & 84.3 \\
		DANN (searched) & 90.6 & 83.9 & 69.6 & 98.6 & 72.3 & 95.0 & 85.1 \\
		Gains ($+\Delta$) & \textcolor{red}{+0.2} & \textcolor{red}{+2.2} & \textcolor{red}{+0.0} & \textcolor{red}{+0.8} & \textcolor{red}{+0.0} & \textcolor{red}{+1.3} & \textcolor{red}{\textbf{+0.8}} \\
		\hline
		CDAN (default) & 91.2 & 93.0 & 68.2 & 100.0 & 72.1 & 97.1 & 86.9 \\
		CDAN (searched) & 91.6 & 91.5 & 69.6 & 100.0 & 74.1 & 97.5 & 87.4 \\
		Gains ($+\Delta$) & \textcolor{red}{+0.4} & \textcolor{green}{-1.5} & \textcolor{red}{+1.4} & \textcolor{red}{+0.0} & \textcolor{red}{+2.0} & \textcolor{red}{+0.4} & \textcolor{red}{\textbf{+0.5}} \\
		\bottomrule
	\end{tabular}}
	\captionof{table}{The hyper-parameters found by our metric v.s. the default hyper-parameters in original papers on Office31.}
	\label{tab:office31-search}
\end{minipage}
\end{figure}

\subsection{Robustness Results}

For the ``Robustness'' property of metrics, we transform MI and ACM into two training methods. We employ these metrics to select the trade-off $\lambda$ from \{0.1, 0.3, 1.0, 3.0, 10.0\} for these methods. We show the implementation of these methods here. The loss of the ``Mutual Information'' method is as follows:
\begin{align}
 L_{MI} = \mathbb{E}_{(\boldsymbol{x}^s,y^s)}[-\log \boldsymbol{p}_{y^s}]+\lambda (\sum_k \hat{\boldsymbol{p}}_k \log \hat{\boldsymbol{p}}_k-\mathbb{E}_{\boldsymbol{x}^t}[\sum_k \boldsymbol{p}_k \log \boldsymbol{p}_k]),
\end{align}
where $\hat{\boldsymbol{p}}_k$ is the average prediction for class $k$ within a batch. 

The loss of the ``Augment Consist'' method is as follows:
\begin{align}
 L_{AC} = \mathbb{E}_{(\boldsymbol{x}^s,y^s)}[-\log \boldsymbol{p}_{y^s}]- \lambda \mathbb{E}_{\boldsymbol{x}^t} \sum_k I[k=\underset{k}{\operatorname{argmax}}(\boldsymbol{p}^t)] \log \boldsymbol{p}^{t \prime} \nonumber
\end{align}
where $\boldsymbol{p}^{t \prime}$ is the prediction of the model on the sample $\boldsymbol{x}^{t \prime}$, which is the augmented version of $\boldsymbol{x}^t$. We use the same random data augmentation as the ``ACM'' metric.

We show the study of the Robustness property on OfficeHome, VisDA2017, and DomainNet in Tab~\ref{tab:robustness} for ``MI'', ``ISM'' and ``ACM'' metrics. When the models are trained with the ``MI'' method, the ``MI'' metric is inconsistent with the target accuracy. Meanwhile, the ``ISM'' metric is robust to this attack. ``ACM'' is also robust against the attack against it.

\begin{figure}[t]
    \centering
    \subfloat[DANN on OfficeHome]{
        \includegraphics[width=0.24\textwidth]{figures/officehome_dann_visual.png}
    }
    \subfloat[CDAN on OfficeHome]{
        \includegraphics[width=0.24\textwidth]{figures/officehome_cdan_visual.png}
    }
    \subfloat[DANN on Office31]{
        \includegraphics[width=0.24\textwidth]{figures/office31_dann_visual.png}
    }
    \subfloat[CDAN on Office31]{
        \includegraphics[width=0.24\textwidth]{figures/office31_cdan_visual.png}
    }
    \caption{The visualization of the relation between the ACM score and target accuracy. Each sub-figure contains models trained by DANN or CDAN methods on OfficeHome or Office31 datasets.}
    \label{fig:visual_1}
\vspace{-2mm}
\end{figure}

\begin{figure}[t]
    \centering
    \subfloat[DANN on DomainNet]{
        \includegraphics[width=0.24\textwidth]{figures/domainnet_dann_visual.png}
    }
    \subfloat[CDAN on DomainNet]{
        \includegraphics[width=0.24\textwidth]{figures/domainnet_cdan_visual.png}
    }
    \subfloat[DANN on VisDA]{
        \includegraphics[width=0.24\textwidth]{figures/visda_dann_visual.png}
    }
    \subfloat[CDAN on VisDA]{
        \includegraphics[width=0.24\textwidth]{figures/visda_cdan_visual.png}
    }
    \caption{The visualization of the relation between the ACM score and target accuracy. Each sub-figure contains models trained by DANN or CDAN methods on DomainNet or VisDA2017 datasets.}
    \label{fig:visual_2}
\vspace{-2mm}
\end{figure}

\subsection{Hyperparameter Searching Results}

We show the results of hyper-parameter searching on DomainNet, OfficeHome, and Office31 in Tab~\ref{tab:domainnet-search}, Tab~\ref{tab:officehome-search}, and Tab~\ref{tab:office31-search}. The hyper-parameters found by our ACM metric outperform the default hyper-parameters for all four training methods.

\section{Visualizations}

We visualize the consistency between the metric score of our ACM and target accuracy and get some sense of Pearson's correlation between them. In Fig.~\ref{fig:visual_1} and Fig.~\ref{fig:visual_2}, we plot the metric score according to the target accuracy of the models trained by the DANN (CDAN) method on various datasets.

As we can see from the figures, it is clear that the ACM score is positively related to target accuracy. Therefore, when the target accuracy increases, the ACM score tends to increase. This correlation is especially obvious in OfficeHome and DomainNet datasets. %Whereas the results on the VisDA2017 dataset tend to be more complicated and indicate that the target accuracy is more difficult to be predicted by the metric on VisDA2017. This phenomenon is also shown in the results of Tab~\ref{tab:visda2}.


\section{Limitations and Future Works}

Although we have studied various UDA metrics and proposed new metrics for UDA evaluation, the best derivation of the best model (``dev'') remains 1\%-2\%. It is desirable to propose a new metric that better meets the three criteria of robust metrics. One possible direction is combining multiple metrics to evaluate the model. Meanwhile, the time cost of evaluating the metric should also be considered, and metrics in the paper require, at most, to train a simple network. 
In the paper, we use a simple TPE searcher and relatively small search spaces. More advanced searching strategies and neural architecture searching~\cite{NAS} for UDA can be explored. The paper mainly focuses on the single-source UDA for close-set classification. The unsupervised metric for more transfer learning scenarios can be studied, e.g., Partial UDA, Source-Free UDA, UDA for object detection, semantic segmentation, and depth estimation.