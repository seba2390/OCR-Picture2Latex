\documentclass{article} % For LaTeX2e
\usepackage{iclr2024,times}
\iclrfinalcopy 

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
% \usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage[pagebackref=true,breaklinks=true,colorlinks,urlcolor=black,bookmarks=false]{hyperref}
\hypersetup{citecolor=[RGB]{0,139,69}}

% Include other packages here, before hyperref.
\usepackage{subfig}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{enumerate}
\usepackage{multirow}
\usepackage{adjustbox}
\usepackage{epsfig}
\usepackage{ulem}
\usepackage{colortbl}
\definecolor{mygray}{gray}{0.9}
\newcolumntype{a}{>{\columncolor{mygray}}c|c}
% \usepackage[pagebackref,breaklinks,colorlinks]{hyperref}
\usepackage{marvosym}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}

\newcommand{\shuai}[1]{\textcolor{red}{[ #1]}}

\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{A Study of Unsupervised Evaluation Metrics for Practical and Automatic Domain Adaptation\vspace{-3mm}}

\author{
Minghao Chen$^1$\quad
Zepeng Gao$^2$\quad
Shuai Zhao$^{3,5}$\quad
Qibo Qiu$^4$\quad
Wenxiao Wang$^2$\quad \\
\textbf{Binbin Lin$^{2}\textsuperscript{\Letter}$ \quad
Xiaofei He$^1$}
\smallskip
\\
$^1$State Key Lab of CAD\&CG, College of Computer Science, Zhejiang University
\\
$^2$School of Software Technology, Zhejiang University \\
$^3$ReLER Lab, CCAI, Zhejiang University \quad
$^4$Zhejiang Lab \quad $^5$Baidu Inc.
\\
\tt\small \{minghaochen01,zhaoshuaimcc\}@gmail.com
\quad binbinlin@zju.edu.cn  
}

\maketitle

\vspace{-5mm}

\begin{abstract}
  Unsupervised domain adaptation (UDA) methods facilitate the transfer of models to target domains without labels. However, these methods necessitate a labeled target validation set for hyper-parameter tuning and model selection. In this paper, we aim to find an evaluation metric capable of assessing the quality of a transferred model without access to target validation labels. We begin with the metric based on mutual information of the model prediction. Through empirical analysis, we identify three prevalent issues with this metric: 1) It does not account for the source structure. 2) It can be easily attacked. 3) It fails to detect negative transfer caused by the over-alignment of source and target features. To address the first two issues, we incorporate source accuracy into the metric and employ a new MLP classifier that is held out during training, significantly improving the result. To tackle the final issue, we integrate this enhanced metric with data augmentation, resulting in a novel unsupervised UDA metric called the Augmentation Consistency Metric (ACM). Additionally, we empirically demonstrate the shortcomings of previous experiment settings and conduct large-scale experiments to validate the effectiveness of our proposed metric. Furthermore, we leverage our metric to automatically search for the optimal set of hyper-parameters, achieving superior performance comparable to manually tuned sets across four common benchmarks.
\end{abstract}
\vspace{-2mm}

\section{Introduction}

Deep neural networks, when trained on extensive datasets, have demonstrated exceptional performance across various computer vision tasks such as classification~\cite{ConvNeXt, CLIP}, object detection~\cite{DETR, DINO}, and semantic segmentation~\cite{Deeplabv3+, SegFormer}. Some even exhibit remarkable generalization to unseen domains~\cite{ViLD, SEEM}. But performance in specific domains can always be enhanced through fine-tuning with labels. The challenge arises in real-world applications where manually labeling ample data for fine-tuning is both costly and impractical. Consider a household robot equipped with a vision system trained on vast vision datasets~\cite{RobotLifelong}. When introduced to a new home, it's anticipated that the robot would automatically adapt to this new environment by collecting images from the house. Yet, expecting homeowners to label these images is both burdensome and unrealistic.

Unsupervised domain adaptation (UDA) emerged as a solution to this problem. Over recent years, a plethora of UDA methods~\citep{CDAN,MCD,MDD,MCC,Proto} have been developed to facilitate transfer to label-free target domains. While accuracy in these domains has seen improvement, it often hinges on meticulous tuning using a labeled validation set from the target domain, which can be resource-intensive. Referring back to the household robot example, creating validation sets would necessitate precise labels from homeowners, which hinders the fully automatic adaptation. One could pre-test UDA method hyper-parameters in sample homes before actual deployment, selecting parameters that perform well across these homes. However, as UDA methods tend to be hyper-parameter sensitive, different domains might demand distinct hyper-parameter configurations. It is challenging to finalize an optimal set before deployment.

%Is there any way to evaluate target accuracy and choose the optimal UDA model without a labeled validation set? Early studies used domain distance~\cite{DANN} or entropy~\cite{C-Ent} as metrics to choose hyper-parameters for their training methods. However, these evaluation metrics were tightly coupled with the training methods.
DEV~\cite{DEV} first proposed a general UDA evaluation metric, which uses the importance-weighted validation method~\cite{IWV} with a variance control term. Later, SND~\cite{SND} suggested that a good transfer model should have a compact neighborhood for each target feature and introduced the soft neighborhood density metric. However, upon more comprehensive and detailed experiments with UDA evaluation metrics, we discovered previous evaluation metrics often failed to select suitable models in most scenarios, as shown in Tab~\ref{tab:conclusion-dataset}. 
This is because their metrics are based on assumptions that do not always hold true in a wide range of scenarios. More related discussions refer to Related Works.

\begin{table*}[t]
\vspace{-6mm}
    \centering
    \resizebox{1\textwidth}{!}{    
        \begin{tabular}{clc}
            \toprule
            Metrics & Ar $\to$ Cl Ar $\to$ Pr Ar $\to$ Rw Cl $\to$ Ar Cl $\to$ Pr Cl $\to$ Rw\,Pr $\to$ Ar Pr $\to$ Cl Pr $\to$ Rw Rw $\to$ Ar Rw $\to$ Cl\,Rw $\to$ Pr  & Avg    \\
            \hline
            DEV~\cite{DEV} & \quad 4.50 \qquad 2.62\qquad 1.98\qquad1.92\qquad 0.53\qquad 9.25\qquad 1.78\qquad 16.9\qquad 8.10       \,\,\,\qquad 0.82       \,\qquad 0.61      \,\qquad \textbf{0.00} & 4.07\\
            SND~\cite{SND} & \quad 16.6 \qquad 2.62 \qquad 1.91\qquad22.6\qquad 24.4\qquad 19.3\qquad 14.1\qquad 16.9\qquad \textbf{0.00}       \,\,\,\qquad 0.68       \,\qquad 16.9      \,\qquad 0.37 & 11.4\\
            \hline
            ACM (ours) & \quad \textbf{1.53} \qquad \textbf{0.00} \qquad \textbf{0.00}\qquad\textbf{0.68}\qquad \textbf{0.00}\qquad \textbf{1.07}\qquad \textbf{0.00}\qquad \textbf{0.00}\qquad \textbf{0.00}       \,\,\,\qquad \textbf{0.00}       \,\qquad \textbf{0.00}      \,\qquad \textbf{0.00} & \textbf{0.67}\\
            \bottomrule
    \end{tabular}}
    \caption{In all 12 transfer tasks from the OfficeHome dataset, we employ CDAN~\cite{CDAN} as the training method. The hyper-parameter space is defined as {trade-off=\{0.1,0.2,0.3,0.5,1.0,2.0,3.0\}}. We show the deviations between the optimal model determined by metrics and the true best target accuracy. In SND paper~\cite{SND} they only presented results for Ar $\to$ Pr and Rw $\to$ Ar.}
    \label{tab:conclusion-dataset}
    \vspace{-5mm}
\end{table*}

This realization led us to rethink and reassess what a robust UDA evaluation metric should be like. A robust UDA evaluation metric, as we define, should satisfy three principles: 1) Target Unsupervised. 2) Consistency with target accuracy in a wide range of scenarios. 3) Robustness: the metric should not be vulnerable to deliberately designed training methods and hyper-parameter sets. Previous works generally assume the first two principles; we augment this understanding by introducing the "Robustness" principle. This new perspective, inspired by the study of adversarial attacks in neural networks~\cite{Goodfellow2014ExplainingAH}, emphasizes the importance of designing metrics resistant to potential failure cases, thus leading to more robust evaluations.
%While previous works generally assume the first two principles, we add the "Robustness" principle to the definition. Defending against failure cases of a metric can help us create more robust metrics, akin to the study of adversarial attacks in neural networks~\cite{Goodfellow2014ExplainingAH}.

%In this paper, we first study a classic UDA algorithm, mutual information ~\cite{C-Ent,MI}, as an evaluation metric that measures the confidence (entropy) and diversity of the model on target samples. We empirically analyze the metric to determine whether it satisfies the three aforementioned principles. We identify three issues with the metric: 
Building upon this redefined understanding of a robust UDA evaluation metric, we turn our attention to a classic UDA algorithm, mutual information~\cite{C-Ent,MI}, used as an evaluation metric that measures the confidence (entropy) and diversity of the model on target samples. We meticulously dissect the metric to evaluate its adherence to the aforementioned three principles. Our investigation reveals three significant drawbacks with the metric: 1) unaware of the alignment between the prediction and the label, 2) easily attacked by designed training methods, 3) cannot detect negative transfer caused by the over-alignment between the source and target features. 
To address these issues, we first incorporate source accuracy into the metric to retain the source label structure. Then, we 
employ an additional MLP classifier held out during training to defend against attacks. We refer to this new metric as Inception Score Metric for UDA (ISM). Finally, we integrate the metric with data augmentation and propose Augmentation Consistency Metric (ACM) to evaluate models beyond features-level consideration.

We also establish new experimental settings for validating evaluation metrics, which contain sufficient datasets, training methods, and hyperparameter sets. In large-scale validation experiments, our evaluation metrics demonstrate high consistency with target accuracy in most cases.
The study of evaluation metrics also has the potential to advance AutoML~\cite{NAS,ENAS} research in the context of UDA. We employ simple hyperparameter optimization~\cite{Optuna} to illustrate this concept. Experiments show that the hyper-parameters automatically discovered by our metrics outperform manually tuned hyper-parameters for four popular UDA methods.

%The contributions of this work are summarized as follows:
%\begin{itemize}
%\item We investigate an important but often overlooked topic: What unsupervised metrics can be used to evaluate transfer quality during domain adaptation? We define the three practical properties that a robust metric should satisfy and emphasize the importance of the Robustness property of evaluation metrics.

%\item Based on the analysis of the mutual information metric, we propose a new unsupervised evaluation metric: ISM. To detect the negative transfer caused by feature over-alignment, we further propose an input-level evaluation metric: ACM. The proposed metrics are hard to be attacked.

%\item We find that previous works conducted experiments in partial transfer scenarios, which would lead to incorrect conclusions about evaluation metrics. Our experiments include more comprehensive scenarios: 31 transfer tasks in four datasets, five training methods, and up to six hyper-parameters. Our metrics are consistent with target accuracy in this setting.

%\item We use this metric as the search objective for hyper-parameter optimization. Experiments demonstrate that the hyper-parameters unsupervised discovered by our metrics outperform the default hyper-parameters for four UDA methods across four UDA datasets.
%\end{itemize}

\section{Related Works}
\subsection{Unsupervised Domain Adaptation}
Unsupervised domain adaptation (UDA)~\cite{DAN} has been developed to save annotation effort during the transfer from the source domain to the target domain. Most UDA methods aim to reduce the divergence~\cite{BenDavidA,BenDavidH} between the source and target domains, e.g., Discrepancy-based UDA ~\cite{CAN,Deepcoral}, Domain Adversarial UDA ~\cite{DANN,CDAN,MCD,MDD}, self-supervised-based UDA ~\cite{Selfensembling,MCC}.
%These works have shown that optimizing their domain divergences will reduce the error on the target domain. 
However, none has formally studied whether their methods can decide the best model or how to tune hyper-parameters without target labels. 

\subsection{Model Selection for UDA} 

Some previous papers~\cite{DEV,SND} are also interested in the unsupervised evaluation metric for UDA, also known as the model selection for UDA.

\textbf{Importance Weighted Validation:} In~\cite{CDAN}, they tune the trade-off parameter using importance-weighted cross-validation~\cite{IWV}. In the later work~\cite{DEV}, Deep Embedded Validation (DEV) is proposed to select models based on importance weights and control variates. However, their DEV metric requires overlap between the support sets of two domain distributions. In practice, it could easily collapse when no overlap exists between two domain distributions or the source error becomes 0.

%\textbf{Discrepancy-based Metric} In the theory of UDA~\cite{BenDavidA,BenDavidH,MDD}, the target error rate can be bounded by the source error rate and domain discrepancy. Ben-David proposed approximate $\mathcal{A}$-distance~\cite{BenDavidA} and $\mathcal{H} \Delta \mathcal{H}$-divergence~\cite{BenDavidH} as the domain divergence. In its subsequent work, MDD~\cite{MDD} also demonstrated that the target error rate could be bounded by the source error rate and their proposed margin. However, they only show that their domain distance will decrease with the training and the accuracy will increase, but there is no experimental proof that their method can select the hyper-parameters of the training method.

\textbf{Entropy-based Metric:} Morerio et al.~\cite{C-Ent} used the predicted entropy of the target domain samples as the metric to tune the hyper-parameter. Although it is simple and convenient, it was pointed out that it cannot deal with blind confidence~\cite{SND}. SND~\cite{SND} proposes to use the density of the target domain sample domain as an evaluation metric to solve this problem. However, SND cannot solve the ``mode collapse'' problem where all target samples are mapped into one feature point.
%believes that a good transfer model should have a relatively compact neighborhood for each class. %However, in our experiments, we found that SND performed poorly for many hyper-parameter combinations and on a wide range of transfer tasks.

\textbf{Other Metrics:} BPDA~\cite{Balancing} introduces a principle to balance the source supervised error and domain distance in a target error bound. However, their approach is limited to adjusting the trade-off hyper-parameter, leaving other hyper-parameters untouched. More recently, Dinu et al.~\cite{Aggregation} propose linear aggregations of vector-valued models to ensemble various models trained under different hyper-parameters. However, their resultant model aggregates all models across diverse hyper-parameters, demanding significantly more computational resources than a singular model, which is not practical for vision tasks.


\section{Methods}
\subsection{Problem Definition}
During the training of unsupervised domain adaptation, we have a labeled dataset sampled from the source domain, $\left\{\left(\boldsymbol{x}_i^s, y_i^s\right)\right\}_{i=1}^{n_s} \sim \mathcal{D}_s$ and an unlabeled dataset sampled from the target domain, $\left\{\boldsymbol{x}_j^t\right\}_{j=1}^{n_t} \sim \mathcal{D}_t$. We can apply off-the-shelf UDA methods~\cite{DANN,CDAN,MDD,MCC} to train a model $\boldsymbol{M}$ on these two training datasets. 
%During the evaluation of UDA, it is conventional that given a labeled dataset from the target domain, $\left\{(\tilde{\boldsymbol{x}}_j^t, \tilde{y}_j^t)\right\}_{j=1}^{\tilde{n}_t} \sim \mathcal{D}_t$, and compute the accuracy of the model prediction according to the ground truth. However, collecting labels on the target domain can be expensive, and it will cause the leakage of supervisory information if we tune the hyper-parameter on this. To this end, we define 
During the evaluation of UDA, we are given a labeled dataset from the source domain and an unlabeled dataset from the target domain, $\left\{(\tilde{\boldsymbol{x}}_i^s, \tilde{y}_i^s)\right\}_{i=1}^{\tilde{n}_s} \sim \mathcal{D}_s$ and $\left\{\tilde{\boldsymbol{x}}_j^t\right\}_{j=1}^{\tilde{n}_t} \sim \mathcal{D}_t$, which contain different samples from the training sets. Given a trained model $\boldsymbol{M}$
, an unsupervised evaluation metric for UDA should compute a score to reflect the classification accuracy of the model on the target domain $\mathcal{D}_t$, based on the evaluation sets and the model $\boldsymbol{M}$. As a common practice, the model is decomposed into a feature generator $\textbf{g}(\cdot)$ and a linear classifier $\textbf{f}(\cdot)$: $\boldsymbol{M}=\textbf{f}(\textbf{g}(\cdot))$. 

\subsection{Principles of A Robust Metric}
\label{sec:3.2}

We define the principles of a robust unsupervised evaluation metric for UDA as the following:

\textbf{1) Target Unsupervised:} The metric can only access the evaluation sets of UDA, $\left\{(\tilde{\boldsymbol{x}}_i^s, \tilde{y}_i^s)\right\}_{i=1}^{\tilde{n}_s}$ and $\left\{\tilde{\boldsymbol{x}}_j^t\right\}_{j=1}^{\tilde{n}_t}$, and the model $\boldsymbol{M}$. For versatility, the metric should be irrelevant to the training method.

\textbf{2) Consistency:} Given a bunch of models $\{\boldsymbol{M}_l\}_{l=1}^{n_m}$ trained with different UDA methods and different hyper-parameters, the metric score $\{\boldsymbol{S}_l\}_{l=1}^{n_m}$ should be consistent with the target classification accuracy $\{\boldsymbol{A}_l\}_{l=1}^{n_m}$ and this consistency holds for multiple UDA datasets. 

\textbf{3) Robustness:} The metric should maintain consistency when we deliberately design the training method and the hyper-parameter to attack the metric. Typically, if the metric can be transformed to a training loss for UDA, the metric score should still be consistent with the target accuracy when training with this loss. 

Our intuition is that a robust metric should reflect the target domain accuracy under various conditions. At the same time, a robust metric should not be vulnerable to attack, which avoids some methods of deliberately optimizing the metric and finding metric preferences. Just as the robustness of the neural network can be improved through the attack on the neural network~\cite{AdversarialExamples}, the analysis of the attack on the evaluation metric can help us construct a more robust evaluation metric.

We utilize two measurements to measure the degree of consistency between the metric score and target accuracy: 

\textbf{Pearson's correlation coefficient:}
\begin{align}
corr(\{\boldsymbol{S}_l\}_{l=1}^{n_m}, \{\boldsymbol{A}_l\}_{l=1}^{n_m}) = \frac{\mathbb{E}(\boldsymbol{S}\boldsymbol{A})-\mathbb{E}(\boldsymbol{S})\mathbb{E}(\boldsymbol{A})}{\sigma_{\boldsymbol{S}}\sigma_{\boldsymbol{A}}},
\label{eq:corr}
\end{align}
where $\sigma$ is the standard deviation.

\textbf{The deviation of Best Model}
\begin{align}
dev(\{\boldsymbol{S}_l\}_{l=1}^{n_m}, \{\boldsymbol{A}_l\}_{l=1}^{n_m}) = \max_l \boldsymbol{A}_l-\boldsymbol{A}_{l^*},
\label{eq:dev}
\end{align}
where $l^*=\operatorname{argmax}_l \boldsymbol{S}_l$ denotes the best model according to the metric. The metric with a higher correlation and lower deviation is more consistent with target accuracy.

\subsection{Derivation of Our Metrics}

%In this section, we begin with the Entropy and Mutual Information metrics for evaluating UDA. Through a series of experiments and analysis, we find several problems in these metrics: (1) The metrics are unaware of the source structure and alignment between predictions and labels. (2) The metrics can be easily attacked and lead to unfair judgments. (3) These feature-based metrics cannot determine the negative transfer caused by over-alignment between domains. We solve each problem with a series of improvements and finally propose our metrics: ISM and ACM, which are much more robust than previous metrics.

\begin{figure}[t]
\vspace{-6mm}
\begin{minipage}[b]{0.38\textwidth}
    \centering
    \resizebox{0.9\textwidth}{!}{   
    \begin{tabular}{c|c|c}
        \hline
        Metric & CDAN & MCC \\
        \hline
        Source Acc. & 10.7 & 5.2 \\
        Entropy & 3.9 & 26.2 \\
        MI & 4.4 & 14.4 \\
        MI w. source & \textbf{0.3} & \textbf{0.5} \\
        \hline
    \end{tabular}}
    \captionof{table}{Using CDAN~\cite{CDAN} or MCC~\cite{MCC} as the training method, when search trade-off from \{0.1, 0.2, 0.3, 0.5, 1.0, 2.0, 3.0, 5.0, 10.0\}, ``dev'' of metrics. The results are averaged across 12 transfers in OfficeHome.}
    \label{tab:conclusion-source}
\end{minipage}
\hspace{0.02\textwidth}
\begin{minipage}[b]{0.6\textwidth}
	\centering
	\subfloat[CDAN]{
		\includegraphics[width=0.5\textwidth]{figures/visual_cdan_source.png}
    }
	\subfloat[MCC]{
		\includegraphics[width=0.5\textwidth]{figures/visual_mcc_source.png}
	}
    \caption{On OfficeHome, using CDAN~\cite{CDAN} or MCC~\cite{MCC} as the training method, when the trade-off changes, the curves of various metrics. For display convenience, we normalize each metric to [0,1].}
	\label{fig:conclusion-source}
\end{minipage}
\vspace{-6mm}
\end{figure}

\subsubsection{Combine with Source Accuracy}
\label{sec:3.3.1}

Originating from Semi-supervised learning~\cite{SSLEntropy}, the Entropy of the prediction is commonly used in UDA methods~\cite{ADVENT} as a regularizer for unlabeled samples. Entropy can also serve as the evaluation metric for UDA~\cite{C-Ent}. As the Entropy metric is unaware of the ``mode collapse'' phenomenon, Mutual Information~\cite{MI} adds the diversity term. The Mutual Information metric (MI) is defined as: 
$\boldsymbol{MI} = H(\mathbb{E}_{\tilde{\boldsymbol{x}}^t}[\boldsymbol{p}^t]) - \mathbb{E}_{\tilde{\boldsymbol{x}}^t}[H(\boldsymbol{p}^t)]$, where $\boldsymbol{p}^t$ denotes the prediction of the model on $\tilde{\boldsymbol{x}}^t$. MI only considers the quality of the target samples. While 
the source label information and the quality of the source feature are ignored. In some situations, the prediction of the target sample is not aligned with our desired label space. To avoid this problem, SND~\cite{SND} suggests monitoring the source supervising loss or setting a threshold for source accuracy. However, we find this rough approach cannot help to determine the best model. To show the importance of source accuracy during evaluating UDA, we use CDAN~\cite{CDAN} or MCC~\cite{MCC} method to train the models with multiple trade-off hyper-parameters. We use the metric to evaluate the trained models and determine the best trade-off. As shown in Tab.~\ref{tab:conclusion-source} and Fig.~\ref{fig:conclusion-source}, the Entropy metric and MI increase as the trade-off increases, but the target accuracy first increases and then decreases as the trade-off increases. 
%Source accuracy and MI alone cannot determine the optimal trade-off, and setting a threshold for source accuracy is difficult. 
We propose to directly combine MI and source accuracy. We first normalize MI into $[0,1]$, then add it with the source accuracy, as follows:
\begin{align}
\boldsymbol{MI}_{w. source} = \mathbb{E}_{(\tilde{\boldsymbol{x}}^s,\tilde y^s)} I[\underset{k}{\operatorname{argmax}}
[\boldsymbol{p}^s] = \tilde y^s]+\frac{\boldsymbol{MI}}{2\log K}+\frac{1}{2},
\end{align}
where $\boldsymbol{p}^s$ denotes the prediction of the model on $\tilde{\boldsymbol{x}}^s$, and $K$ is the number of classes. 
As shown in Tab.~\ref{tab:conclusion-source} and Fig.~\ref{fig:conclusion-source}, this simple combination can balance MI on the target domain and source accuracy. If we view the MI term as the similarity between two domains, this metric formally follows Ben David's theory~\cite{BenDavidA}, where the source error and the domain discrepancy bound the target error.

\subsubsection{Robustness Property}
\label{sec:3.3.2}

\begin{figure}[t]
\vspace{-6mm}
	\centering
	\includegraphics[width=0.93\textwidth]{figures/MI_vs_ISM.pdf}
	
    \caption{On OfficeHome and VisDA2017, use Mutual Information as the training algorithm. (a)-(b) When the trade-off value changes, the Mutual Information, ISM metric, and target domain accuracy change. (c)-(e) The tSNE~\cite{tSNE} visualization of model features and each metric scores when the trade-offs are equal to 0.3, 1, and 3 on VisDA2017.}
	\label{fig:conclusion-MI}
\vspace{-4mm}
\end{figure}

As mentioned in Section~\ref{sec:3.2}, a robust metric should not be vulnerable when we maliciously increase the metric score. To attack $\boldsymbol{MI}_{w. source}$ metric, we train the model with the following loss:
\begin{align}
 Loss = \mathbb{E}_{(\boldsymbol{x}^s,y^s)}[-\log \boldsymbol{p}_{y^s}]+\lambda (\sum_k \hat{\boldsymbol{p}}_k \log \hat{\boldsymbol{p}}_k-\mathbb{E}_{\boldsymbol{x}^t}[\sum_k \boldsymbol{p}_k \log \boldsymbol{p}_k]),
\end{align}
where $\hat{\boldsymbol{p}}_k$ is the average prediction for class $k$ within a batch. This loss is actually the UDA method used in ~\cite{MI}, which maximizes mutual information. We use this loss to train the models with different trade-off hyper-parameters $\lambda$.
%selected from \{0.01, 0.03, 0.1, 0.3, 1.0, 3.0\}. We use MI and MI w.source metrics to evaluate the models and determine the optimal trade-off. 
As shown in Fig.~\ref{fig:conclusion-MI}, MI and MI w.source would prefer a large trade-off, but target accuracy decreases dramatically as the trade-off becomes larger. To investigate the cause, we use tSNE~\cite{tSNE} to visualize the source and target features. As demonstrated in Fig.~\ref{fig:conclusion-MI} (c)-(e), source features (red) form clear clusters, but target features are pushed away as the trade-off increases. The MI metric is almost unaware of this phenomenon because the training loss is finding the preference of MI.

The MI metric is vulnerable for two reasons: the evaluation metric is fully exposed to the training process, and the linear classifier $\textbf{f}$ cannot detect feature outliers. To solve this problem, we propose to train a new two-layer MLP on top of the source evaluation feature. Then we use the Mutual Information of this MLP classifier on the target evaluation feature as the evaluation metric. The new metric $\boldsymbol{ISM}$ can be formalized as follows:
\begin{align}
    \boldsymbol{IS}
	&= H(\mathbb{E}_{\tilde{\boldsymbol{x}}^t}[\boldsymbol{q}^t]) - \mathbb{E}_{\tilde{\boldsymbol{x}}^t}[H(\boldsymbol{q}^t)], \\
\boldsymbol{ISM} &= \mathbb{E}_{(\tilde{\boldsymbol{x}}^s,\tilde y^s)} I[\underset{k}{\operatorname{argmax}}
[\boldsymbol{p}^s] = \tilde y^s]+\frac{\boldsymbol{IS}}{2\log K}+\frac{1}{2},
\end{align}
where $\boldsymbol{q}^t=\textbf{h}(\textbf{g}(\tilde{\boldsymbol{x}}^t))$ and $\textbf{h}$ is the MLP classifier trained on the source evaluation feature. Noticeably, we still use the classifier of the model to compute the source accuracy, which evaluates whether the classifier $\textbf{f}$ is well-trained. As $\textbf{h}$ is held out during the UDA training and the two-layer MLP is more expressive, $\boldsymbol{ISM}$ is not vulnerable to attack. As shown in Fig.~\ref{fig:conclusion-MI}, when trained with the mutual information loss, $\boldsymbol{ISM}$ can be well consistent with the target accuracy. $\boldsymbol{ISM}$ is short for the Inception Score Metric for UDA because it is formally similar to the Inception Score for generative models~\cite{InceptionScore}. The inception score for the generative model utilizes the mutual information of the ImageNet pretrained inception network ~\cite{Inceptionv3}. Instead, we train an MLP classifier based on the supervision of the source evaluation set and combine it with the original source accuracy.

\subsubsection{Input-level Consistency}
\label{sec:3.3.3}

\begin{figure}[t]
\vspace{-6mm}
	\centering
	\subfloat[Train epochs of CDAN]{
		\includegraphics[width=0.25\textwidth]{figures/visual_visda_cdan_epoch.png}
        }
        \hspace{0.02\textwidth}
	\subfloat[Epoch 1]{
		\includegraphics[width=0.20\textwidth]{figures/TSNE_1.png}
	}
        \hspace{0.02\textwidth}
        \subfloat[Epoch 4]{
		\includegraphics[width=0.20\textwidth]{figures/TSNE_4.png}
        }
        \hspace{0.02\textwidth}
	\subfloat[Epoch 9]{
		\includegraphics[width=0.20\textwidth]{figures/TSNE_9.png}
	}
    \caption{On VisDA2017, using CDAN as the training method. (a) As training epochs grow, the curves of target accuracy, A-distance Metric, and ACM (ours). For display convenience, we normalize each metric to [0,1]. (b)-(d) The tSNE visualization of model features and a target feature (green circle) that gradually misclassified paired with the feature of its augmented view (green cross).}
\label{fig:conclusion-input}
\vspace{-4mm}
\end{figure}

In the experiments, our ISM already shows surprising consistency with target accuracy. However, we find that in some situations where we use feature alignment-based UDA methods, e.g., DANN and CDAN, the target accuracy might decrease with the training process. In these situations, the source and target features are well aligned, and source accuracy does not degrade, but target accuracy is not improving. We interpret this phenomenon as some target features being pulled to the wrong source class to make the target feature distribution the same as the source. We show this phenomenon in Fig~\ref{fig:conclusion-input}: the target accuracy approaches the maximum in the first epoch while the feature distributions of the two domains are not aligned. This phenomenon violates the common UDA assumption that samples embedded nearby are likely to share the labels~\cite{SND}. In these situations, the metrics that consider the features and predictions of two domains are hard to determine the model.

To solve this problem, we propose to use the consistency between the target sample and its augmented view to detect whether target features are over-aligned. This is based on the finding that for misaligned target samples, the features are more unstable to data augmentation. In Fig~\ref{fig:conclusion-input}, a target feature (green dot) is close to its data-augmented feature (green cross) at epoch 1, but the two become farther away as it is over-aligned. We define our Augment Consistency Metric (ACM) as follows:
\begin{align}
    \boldsymbol{AC}
	&= \mathbb{E}_{\tilde{\boldsymbol{x}}^t} I[\underset{k}{\operatorname{argmax}}[\boldsymbol{q}^t]= \underset{k}{\operatorname{argmax}}[\boldsymbol{q}^{t \prime}]] \\
	\boldsymbol{ACM} &= \mathbb{E}_{(\tilde{\boldsymbol{x}}^s,\tilde y^s)} I[\underset{k}{\operatorname{argmax}}[\boldsymbol{p}^s] = \tilde y^s]+\frac{1}{2}(\boldsymbol{AC} + \frac{H(\mathbb{E}_{\tilde{\boldsymbol{x}}^t}[\boldsymbol{q}^t])}{\log K}),
\end{align}
where $\boldsymbol{q}^{t \prime}=\textbf{h}(\textbf{g}(\tilde{\boldsymbol{x}}^{t \prime}))$ denotes the prediction of the MLP classifier on the data-augmented sample $\tilde{\boldsymbol{x}}^{t \prime}$. We use the MLP prediction instead of the original classifier to make it robust and combine it with the diversity term to avoid ``mode collapse''. As shown in Fig~\ref{fig:conclusion-input}, after taking input-level disturbance into consideration, ACM is consistent with target accuracy in the over-alignment situation. While input-level consistency has been utilized as a UDA method ~\cite{Selfensembling}, we are the first to study it as an evaluation metric.

\subsection{Flaws in Previous Metrics}
\label{sec:3.4}
In this section, we reveal the flaws in the experiment settings and metrics of previous works~\cite{DEV, SND}. To verify a robust evaluation metric for UDA, experimenting with sufficient datasets, training methods, and hyper-parameter sets are important. Based on the findings, we will construct our experiment settings in the experiment section~\ref{sec:4.1}.

\textbf{More Datasets and Training Methods are Important.}
In the DEV~\cite{DEV} and SND~\cite{SND} papers, they only used part of UDA datasets and part of UDA training methods, such as in the DEV paper ~\cite{DEV}, only the CDAN training method is used on Office31, and only the MCD training method is used on VisDA. In the SND paper ~\cite{SND}, although they used 4 training algorithms, they only test metrics on two transfer tasks of OfficeHome, Ar $\to$ Pr and Rw $\to$ Ar, and one transfer task of DomainNet, real $\to$ clipart.
However, \textbf{it is easy to draw wrong conclusions by testing evaluation metrics on the part of datasets.} 
%To demonstrate the importance of using a wide range of datasets to validate robust evaluation metrics, we use CDAN as the training method to validate previous evaluation metrics on all 12 transfer tasks on the OfficeHome dataset. We set the hyper-parameter space of CDAN as \{trade-off=\{0.1,0.2,0.3,0.5,1.0,2.0,3.0\}\}, and the evaluation metric needs to find the optimal trade-off. For each metric, we report the derivation of the best model~\ref{eq:dev}.
As shown in the table ~\ref{tab:conclusion-dataset}, although DEV and SND perform well on some transfer tasks, e.g., Ar $\to$ Pr and Rw $\to$ Ar, they perform poorly on most transfer tasks. This is likely because the Ar $\to$ Pr and Rw $\to$ Ar transfer tasks are closer to the assumptions of their metric, e.g., SND assumes that tighter intra-class domains have higher accuracy. Our evaluation metric ACM achieves excellent results on all 12 transfer tasks. 
%In our main experiment in Section \ref{sec:4.1}, we will conduct experiments on as many UDA datasets as possible to validate various UDA evaluation metrics to forbid dataset-biased conclusions.

\iffalse
\begin{figure}[t]
	\centering
        \subfloat[Office31]{
		\includegraphics[width=0.25\textwidth]{figures/visual_office31_method.png}
	}
	\subfloat[OfficeHome]{
		\includegraphics[width=0.25\textwidth]{figures/visual_method.png}
	}
	\subfloat[VisDA2017]{
		\includegraphics[width=0.25\textwidth]{figures/visual_visda_method.png}
	}
	\subfloat[DomainNet]{
		\includegraphics[width=0.25\textwidth]{figures/visual_domainnet_method.png}
	}
	\caption{On four datasets (a) Office31, (b) OfficeHome, (c) VisDA2017, and (d) DomainNet, for various training methods, the average value of each metric. For display convenience, we normalize each metric to [0,1].}
	\label{fig:conclusion-method}
\end{figure}
\fi

It is also important to validate the evaluation metrics using multiple training methods on each dataset. 
%Our goal is to propose a robust evaluation metric that can be used to select hyperparameters for a wide range of UDA training methods rather than only for specific training methods. 
In the experiment, we employ five classic UDA algorithms. 
Additionally, we will investigate an interesting question: Can the metrics be used to select training methods for a transfer scenario? This problem is very important in practice because a large number of UDA algorithms have been proposed, so for a transfer scenario, it is troublesome to choose the most suitable training method. 

%To test this capability of evaluation metrics, we trained models with five UDA algorithms using default hyper-parameters on each dataset. As shown in Fig.~\ref{fig:conclusion-method}, the MCC method is the best on average for the 12 tasks on OfficeHome, while the MDD method is the best on VisDA2017 and DomainNet. However, previous metrics, DEV and SND, cannot select the optimal training method. Our evaluation metric ACM can select the optimal training method for various UDA datasets.


\begin{figure}[t]
\vspace{-6mm}
	\centering
	\subfloat[Learning Rate of MCC]{
		\includegraphics[width=0.24\textwidth]{figures/visual_mcc_lr.png}
        }
	\subfloat[Trade-off of MCC]{
		\includegraphics[width=0.24\textwidth]{figures/visual_mcc_tradeoff.png}
	}
	\subfloat[Temperature of MCC]{
		\includegraphics[width=0.24\textwidth]{figures/visual_mcc_temperature.png}
	}
	\subfloat[Train Epoch of MCC]{
		\includegraphics[width=0.24\textwidth]{figures/visual_mcc_epoch.png}
	}
	\caption{On OfficeHome, when independently changing different hyper-parameters of MCC, curves of various evaluation metrics (averaged across 12 transfer tasks). We normalize each metric to [0,1]. }
	\label{fig:conclusion-hyperparameter}
\vspace{-2mm}
\end{figure}

\linespread{1.1}
\begin{table}[t]
    \centering
    
    \resizebox{1\textwidth}{!}{
    \begin{tabular}{|c|c|c|}
        \hline
        Training Method & Sparse hyper-parameter space & Dense hyper-parameter space \\
        \hline
        \multirow{2}{*}{Source only} & \{lr=\{1e-3,1e-2\}, wd=\{1e-4,1e-3\}, & \multirow{2}{*}{-} \\
        & train-epoch=\{1...10\}\} & \\
        \hline
        \multirow{3}{*}{DANN or CDAN} & 
        \multirow{3}{*}{\shortstack{\{lr=\{1e-3,1e-2\}, wd=\{1e-4,1e-3\}, \\ lr-multi-D=\{0.1 ,1,10\}, trade-off=\{0.1,1,10\} \\
        bottleneck-dim= \{256,512\}, train-epoch=\{1...10\} \}}} & 
        \multirow{3}{*}{\shortstack{\{lr=\{3e-4,1e-3,3e-3,1e-2,3e-2\}, wd=\{1e-4,3e- 4,1e-3\}, \\
        lr-multi-D=\{0.1,0.3,1,3,10\}, trade-off=\{0.1,0.3,1,3,10\}, \\
        bottleneck-dim= \{256,512\}, train-epoch=\{1...10\}\}}} \\
        & & \\
        & & \\
        \hline
        \multirow{3}{*}{MCC} & 
        \multirow{3}{*}{\shortstack{\{lr=\{1e-3,1e-2\}, wd=\{1e-4,1e-3\}, \\ trade-off=\{0.1,1,10\}, train-epoch=\{1...10\}\}}} & 
        \multirow{3}{*}{\shortstack{\{lr=\{3e-4,1e-3, 3e-3,1e-2,3e-2\}, wd=\{1e-4,3e-4,1e-3 \}, \\ temperature=\{1,2,3,4,5\}, trade-off=\{0.1,0.3,1,3,10\}, \\ bottleneck-dim=\{512,1024,2048 \}, train-epoch=\{1...10\}\}}} \\
        & & \\
        & & \\
        \hline
        \multirow{3}{*}{MDD} & 
        \multirow{3}{*}{\shortstack{\{lr=\{4e-4,4e-3\}, wd=\{5e-5,5e-4\}, \\ trade-off=\{0.1,1,10 \}, train-epoch=\{1...10\}\}}} & 
        \multirow{3}{*}{\shortstack{\{lr=\{3e-4,1e-3,3e-3,1e-2,3e-2\}, wd=\{1e-4,3e-4,1e-3 \}, \\ margin=\{1,2,3,4,5\}, trade-off=\{0.1,0.3,1,3,10\}, \\ bottleneck-dim=\{512,1024,2048 \}, train-epoch=\{1...10\}\}}}  \\
        & & \\
        & & \\
        \hline
    \end{tabular}}
    \linespread{1}
    \caption{The hyper-parameter spaces used in the experiments. 
    In the sparse hyper-parameter space, we perform grid search over all combinations of hyper-parameters to analyze the consistency. In the dense hyper-parameter space, we use our metric to search for optimal hyper-parameters.
    }
    \label{tab:hyperparameter}
\vspace{-6mm}
\end{table}
\linespread{1}

\textbf{Larger and Wider Hyper-parameter Sets are Important.}
%For a training method, we need to first determine its hyper-parameter search space and then search for the optimal hyper-parameter.
%We hope that robust evaluation metrics can be applied to as wide a hyper-parameter search space as possible. 
For different datasets, the optimal hyper-parameters may vary by ten times~\cite{tllib}. In addition, it is often necessary to tune multiple hyper-parameters of the method to achieve the optimal result. However, in the previous DEV and SND papers, only one hyper-parameter was adjusted for each training method, and the adjustment range of hyper-parameters was small.
%To demonstrate the importance of tuning multiple and large ranges of hyper-parameters to validate evaluation metrics, we experimented with the impact of various hyper-parameter changes on evaluation metrics. We change the following hyper-parameters for MCC: learning rate, trade-off, temperature, and training epochs, and then observe the consistency of each evaluation metric with target accuracy. 
As shown in Fig.~\ref{fig:conclusion-hyperparameter}, when the hyper-parameter of MCC changes, previous metrics DEV and SND cannot be consistent with target accuracy. In addition, we also found that a larger selection interval will lead to different conclusions from the small ones. For example, when the trade-off values are changed, SND can maintain the same accuracy rate between 0.1 and 1.0, but if the trade-off value increases to 10.0, SND will give the opposite result. Finally, it can be seen from the figure that our ACM always maintains high consistency with target accuracy and can select the optimal value for various hyper-parameters.

%Therefore, in our subsequent main experiment ~\ref{sec:4.4}, we will experiment with various hyperparameters of the training algorithm, and instead of studying each hyperparameter individually, we will study when varying multiple hyperparameters at the same time The consistency of the evaluation index and the accuracy rate (this is more appropriate to the actual scene). For each hyperparameter, we set the maximum and minimum values of its value range to be as wide as possible within a reasonable range.

\section{Experiments}

%In this section, we study unsupervised evaluation metrics on four popular UDA datasets, VisDA2017, DomainNet, Office31, and OfficeHome (results on Office31 are provided in the Appendix). The models to be evaluated are trained with five UDA methods along with different hyper-parameters. We investigate whether the metrics meet the principles in Section ~\ref{sec:3.3}. 

\subsection{Experimental Settings}
\label{sec:4.1}

\textbf{Datasets.} 
UDA datasets studied in the main paper: 
1) \textit{OfficeHome}~\cite{officehome} consists of 15,500 images with 65 classes from four domains: Artistic images (Ar), Clip art (Cl), Product images (Pr), and Real-world (Rw). There are 12 transfer tasks among these domains. 
2) \textit{VisDA2017}~\cite{visda} contains 12 categories and over 280,000 images from the Synthetic source domain and Real-world target domain.  
3) \textit{DomainNet}~\cite{domainnet} is a large-scale dataset for domain adaptation and contains 345 categories from six domains. We select four domains for our experiments: Clipart (c), Painting (p), Real (r), and Sketch (s). We only study single-source domain adaptation of DomainNet. There are 12 transfer tasks among these domains. 
4) \textit{Office31}~\cite{office} is a relatively small dataset containing 4652 images with 31 categories from three domains. Results on Office31 are provided in the Appendix. 

\textbf{Training Methods.} 
We use five popular UDA methods to get trained models. \textbf{1) Source only} \textbf{2) DANN}~\cite{DANN} \textbf{3) CDAN}~\cite{CDAN} \textbf{4) MDD}~\cite{MDD} \textbf{5) MCC}~\cite{MCC}.
The implementations of these methods all follow TL-Lib~\cite{tllib}.
For more implementation details, please refer to the Appendix.

\textbf{Sets of Hyper-parameters.} 
\label{sec:4.1.3}
We find that several hyper-parameters are often manually tuned, and we chose them to check the robustness of metrics. Totally we will change at most six hyper-parameters of the training method: \textbf{1) Early-stopping step} (train-epoch): For the UDA problem, the model at the final step is usually not the best model during training. 
We divide the total training step into ten epochs and evaluate the model after each epoch. 
\textbf{2) Learning rate} (lr): The initial learning rate \textbf{3) Weight decay} (wd) \textbf{4) Trade-off}: The trade-off between the supervised loss on the source domain and the target loss from UDA methods. \textbf{5) Bottleneck dimension}: The feature dimension output by the feature generator. \textbf{6) Hyper-parameter related to training methods}: We choose the margin $\gamma$~\cite{MDD} for MDD and the temperature $T$~\cite{MCC} for MCC. For DANN and CDAN, we tune the learning rate of the domain discriminator as the hyper-parameter to balance the convergence of the discriminator and the generator~\cite{FID}. We define lr-multi-D as the ratio of the learning rate of the discriminator to the generator.

%For a training method, we sample the hyper-parameter from its hyper-parameter space each time we train a model. For the study in Section~\ref{sec:4.2}, we set a rough hyper-parameter space based on the default hyper-parameters of the method. As shown in the table~\ref{tab:hyperparameter}, we use the hyper-parameters in the sparse hyper-parameter space to train models with each method and then analyze the consistency of each metric with the target accuracy. We grid search the hyper-parameter space for each method and collect the models during searching. Noticeably, for searching the hyper-parameter of the Early-stopping step, we split the total training steps into $10$ epochs. At the end of each epoch, we evaluate the model so that we can get models with different early-stopping steps.

\textbf{Unsupervised Evaluation Metrics.} $\mathcal{A}$-distance\cite{BenDavidA}, $\mathcal{H} \Delta \mathcal{H}$-divergence or MCD~\cite{BenDavidH,MCD}, MDD~\cite{MDD}, DEV~\cite{DEV}, Entropy~\cite{SSLEntropy, ADVENT}, SND~\cite{SND}, Mutual Information~\cite{MI}, ISM (ours), ACM (ours). We implement metrics according to the original papers and modify them to be positively correlated with target accuracy. The implementations are listed in the Appendix.

\begin{table*}[t]
\vspace{-6mm}
    \centering
    \resizebox{1\textwidth}{!}{
    \begin{tabular}{c|a|a|a|a|a|a}
        \toprule
        Training Method & \multicolumn{2}{|c}{Source only} & \multicolumn{2}{|c}{DANN} & \multicolumn{2}{|c}{CDAN} & \multicolumn{2}{|c}{MDD} & \multicolumn{2}{|c}{MCC} & \multicolumn{2}{|c}{ALL} \\
        \hline
        Metric & corr & dev & corr & dev & corr & dev & corr & dev & corr & dev & corr & dev \\
        \hline
        $\mathcal{A}$-distance & -0.81 & 6.99 & 0.55 & 6.47 & -0.17 & 6.56 & 0.58 & 2.25 & 0.37 & \textcolor{blue}{1.66} & 0.44 & 7.76 \\
        MCD & -0.58 & 8.03 & \textcolor{blue}{0.77} & 6.29 & -0.26 & 4.73 & \textcolor{red}{\textbf{0.86}} & 0.57 & -0.06 & 66.29 & 0.5 & 8.67 \\
        DEV & 0.12 & 7.39 & -0.08 & 4.37 & -0.08 & 4.71 & -0.09 & 47.54 & -0.11 & 64.27 & -0.03 & 64.27 \\
        Entropy & -0.14 & 8.99 & 0.56 & 4.81 & -0.28 & 9.34 & 0.64 & 1.17 & -0.06 & 66.29 & -0.34 & 66.29 \\
        SND & -0.74 & 8.12 & 0.46 & 8.51 & -0.72 & 9.81 & -0.55 & 50.33 & -0.58 & 67.65 & -0.42 & 52.12 \\
        MI & 0.06 & 6.26 & 0.58 & \textcolor{blue}{3.92} & -0.07 & 3.72 & 0.81 & \textcolor{red}{\textbf{0.0}} & 0.03 & 5.4 & 0.45 & 5.4 \\
        \hline
        ISM & \textcolor{red}{\textbf{0.84}} & \textcolor{red}{\textbf{0.31}} & 0.75 & \textcolor{blue}{3.92} & \textcolor{blue}{0.42} & \textcolor{blue}{1.23} & 0.75 & \textcolor{blue}{0.40} & \textcolor{blue}{0.88} & \textcolor{red}{\textbf{0.66}} & \textcolor{blue}{0.59} & \textcolor{red}{\textbf{1.66}} \\
        ACM & \textcolor{blue}{0.80} & \textcolor{blue}{2.38} & \textcolor{red}{\textbf{0.79}} & \textcolor{red}{\textbf{1.18}} & \textcolor{red}{\textbf{0.61}} & \textcolor{red}{\textbf{0.98}} & \textcolor{blue}{0.85} & \textcolor{red}{\textbf{0.0}} & \textcolor{red}{\textbf{0.93}} & \textcolor{blue}{1.66} & \textcolor{red}{\textbf{0.76}} & \textcolor{red}{\textbf{1.66}} \\
        \bottomrule
    \end{tabular}}
    \caption{Consistency between metrics of UDA and target accuracy on VisDA2017, when models are trained by different UDA methods and hyper-parameters.  The ``ALL'' method denotes assembling models trained by all five methods. The higher the Pearson's correlation (``corr'') and the lower the deviation (``dev''), the better the metric. \textcolor{red}{\textbf{Red score}} is the best and \textcolor{blue}{blue score} is the second best.}
    \label{tab:visda}
\vspace{-4mm}
\end{table*}

\begin{table*}[t]
    \centering
    \resizebox{1\textwidth}{!}{
    \begin{tabular}{c|a|a|a|a|a|a}
        \toprule
        Training Method & \multicolumn{2}{|c}{Source only} & \multicolumn{2}{|c}{DANN} & \multicolumn{2}{|c}{CDAN} & \multicolumn{2}{|c}{MDD} & \multicolumn{2}{|c}{MCC} & \multicolumn{2}{|c}{ALL} \\
        \hline
        Metric & corr & dev & corr & dev & corr & dev & corr & dev & corr & dev & corr & dev \\
        \hline
        $\mathcal{A}$-distance & 0.32 & 5.8 & 0.71 & \textcolor{blue}{1.5} & 0.67 & 1.82 & 0.93 & 1.27 & 0.45 & 8.62 & 0.56 & 6.71 \\
        MCD & 0.57 & \textcolor{red}{\textbf{1.12}} & \textcolor{blue}{0.75} & 2.55 & 0.69 & 1.79 & 0.93 & \textcolor{blue}{1.13} & \textcolor{blue}{0.76} & \textcolor{red}{\textbf{1.03}} & 0.71 & 3.16 \\
        DEV & 0.01 & 4.32 & 0.06 & 8.14 & 0.06 & 3.4 & 0.11 & 9.54 & -0.02 & 11.51 & 0.01 & 12.42 \\
        Entropy & -0.64 & 8.20 & 0.43 & 4.28 & 0.88 & 1.56 & 0.88 & 1.98 & 0.38 & 24.43 & 0.52 & 24.52 \\
        SND & -0.60 & 8.17 & -0.23 & 7.90 & 0.07 & 9.24 & -0.90 & 54.57 & -0.20 & 12.40 & -0.25 & 34.75 \\
        MI & -0.60 & 6.78 & 0.45 & 4.25 & 0.88 & 1.40 & 0.91 & 1.98 & 0.37 & 22.83 & 0.52 & 22.93 \\
        \hline
        ISM & \textcolor{blue}{0.72} & 1.47 & 0.6 & 1.68 & \textcolor{red}{\textbf{0.91}} & \textcolor{blue}{1.15} & \textcolor{red}{\textbf{0.97}} & 1.45 & 0.70 & 1.96 & \textcolor{blue}{0.88} & \textcolor{blue}{1.96} \\
        ACM & \textcolor{red}{\textbf{0.75}} & \textcolor{blue}{1.37} & \textcolor{red}{\textbf{0.77}} & \textcolor{red}{\textbf{1.16}} & \textcolor{blue}{0.90} & \textcolor{red}{\textbf{1.13}} & \textcolor{blue}{0.95} & \textcolor{red}{\textbf{0.93}} & \textcolor{red}{\textbf{0.94}} & \textcolor{blue}{1.36} & \textcolor{red}{\textbf{0.93}} & \textcolor{red}{\textbf{1.73}} \\
        \bottomrule
    \end{tabular}}
    \caption{The ``corr'' and ``dev'' results are averaged over the 12 transfer tasks of OfficeHome. }
    \label{tab:officehome}
\vspace{-4mm}
\end{table*}

\subsection{Main Results}
\label{sec:4.2}

In this section, we investigate whether unsupervised evaluation metrics satisfy the ``Consistency'' principle in Section ~\ref{sec:3.2}. We train the model $\{\boldsymbol{M}_l\}_{l=1}^{n_m}$ using the five UDA methods and the hyper-parameters for the coarse hyper-parameter space in the Tab.~\ref{tab:hyperparameter}. For each metric, we report the Pearson correlation (``corr'') and the deviation of the best model (``dev'').
Tab. ~\ref{tab:visda}, Tab. ~\ref{tab:domainnet}, and Tab. ~\ref{tab:officehome} show the results of UDA metrics for five training methods on VisDA2017, OfficeHome, and DomainNet. As the results show, it is difficult for previous metrics to represent the target accuracy across all training methods. Some metrics can perform well on the transfer task on one of the datasets but did not perform well on all three, which also shows that testing on partial datasets may lead to biased conclusions. 
Notably, our proposed ISM is consistent with the target accuracy for most training methods. Our ACM achieves better performance for training methods that align features of two domains, e.g., DANN and CDAN, as it can detect the over-alignment problem.

\textbf{Comparison of training methods:}
We also investigate the consistency of metrics when comparing different methods. Because in practice, we need to determine the best UDA method for the transfer task. We collected all models trained by all five methods with their metric scores and target accuracy. For each metric, we compute Pearson's correlation and the deviation of the best model, and the results are shown in the "ALL" column.
As shown in Tables ~\ref{tab:visda}, Table ~\ref{tab:domainnet}, and Table ~\ref{tab:officehome}, when comparing all training methods, maintaining consistency has become more difficult for most metrics. It is worth noting that our ISM and ACM perform well on all three datasets, with the deviation of the best model (``dev'') below 2\%. Therefore, we can use the proposed unsupervised metrics to decide the best training method and its hyper-parameters for a dataset.

\textbf{Robustness property} We show the robustness property of ISM and ACM in the Appendix.

\begin{table*}[t]
\vspace{-6mm}
    \centering
    \resizebox{1\textwidth}{!}{
    \begin{tabular}{c|a|a|a|a|a|a}
        \toprule
        Training Method & \multicolumn{2}{|c}{Source only} & \multicolumn{2}{|c}{DANN} & \multicolumn{2}{|c}{CDAN} & \multicolumn{2}{|c}{MDD} & \multicolumn{2}{|c}{MCC} & \multicolumn{2}{|c}{ALL} \\
        \hline
        Metric & corr & dev & corr & dev & corr & dev & corr & dev & corr & dev & corr & dev \\
        \hline
        $\mathcal{A}$-distance & \textcolor{blue}{0.89} & 1.89 & 0.64 & 0.9 & 0.83 & \textcolor{blue}{0.38} & \textcolor{blue}{0.93} & 0.45 & 0.6 & 9.45 & \textcolor{blue}{0.89} & 4.19 \\
        MCD & 0.87 & 1.74 & 0.67 & 6.48 & 0.95 & \textcolor{blue}{0.38} & 0.89 & 0.45 & \textcolor{red}{\textbf{0.94}} & 5.05 & 0.86 & 13.57 \\
        DEV & 0.19 & 1.53 & 0.07 & 3.0 & 0.08 & 1.44 & -0.04 & 12.25 & -0.11 & 5.14 & 0.0 & 2.45 \\
        Entropy & 0.45 & 3.43 & 0.65 & 5.83 & 0.79 & 0.49 & 0.83 & 1.09 & 0.75 & 1.37 & 0.71 & 3.55 \\
        SND & -0.93 & 11.8 & -0.95 & 20.46 & -0.95 & 11.2 & -0.98 & 39.8 & -0.81 & 24.27 & -0.75 & 23.96 \\
        MI & 0.48 & 3.21 & 0.65 & 5.83 & 0.79 & 0.49 & 0.92 & 0.87 & 0.76 & 0.93 & 0.71 & 3.11 \\
        \hline
        ISM & 0.85 & \textcolor{red}{\textbf{1.33}} & \textcolor{red}{\textbf{0.87}} & \textcolor{blue}{0.7} & \textcolor{blue}{0.96} & 0.41 & \textcolor{red}{\textbf{0.98}} & \textcolor{blue}{0.28} & \textcolor{blue}{0.92} & \textcolor{blue}{0.6} & \textcolor{red}{\textbf{0.91}} & \textcolor{blue}{1.13} \\
        ACM & \textcolor{red}{\textbf{0.94}} & \textcolor{blue}{1.41} & \textcolor{blue}{0.8} & \textcolor{red}{\textbf{0.27}} & \textcolor{red}{\textbf{0.98}} & \textcolor{red}{\textbf{0.29}} & \textcolor{blue}{0.93} & \textcolor{red}{\textbf{0.04}} & 0.84 & \textcolor{red}{\textbf{0.15}} & 0.87 & \textcolor{red}{\textbf{0.29}} \\
        \bottomrule
    \end{tabular}}
    \caption{The ``corr'' and ``dev'' results are averaged over 12 transfer tasks of DomainNet. }
    \label{tab:domainnet}
\vspace{-4mm}
\end{table*}

\begin{table*}[t]
	\centering
	\resizebox{0.98\textwidth}{!}{
		\begin{tabular}{cccccccccccccc}  
			\toprule
		Method & plane & bcycl & bus & car & house &  knife & mcycl & person & plant & sktbrd & train & truck & Avg    \\
			\hline
			DANN (default) & 81.7 & 38.7 & 77.8 & 85.8 & 67.2 & 76.7 & 65.5 & 57.9 & 81.3 & 50.4 & 88.5 & 61.0 & 69.4 \\
			DANN (searched) & 84.8 & 45.5 & 86.9 & 86.8 & 74.0 & 91.3 & 75.7 & 59.7 & 89.9 & 51.2 & 82.3 & 62.2 & 74.2\\
			Gains ($+\Delta$) & \textcolor{red}{+3.1} & \textcolor{red}{+6.8} & \textcolor{red}{+9.1} & \textcolor{red}{+1.0} & \textcolor{red}{+6.8} & \textcolor{red}{+14.6} & \textcolor{red}{+10.2} & \textcolor{red}{+1.8} & \textcolor{red}{+8.6} & \textcolor{red}{+0.8} & \textcolor{green}{-6.2} & \textcolor{red}{+1.2} & \textcolor{red}{\textbf{+4.8}} \\
			\hline
			CDAN (default) & 84.8 & 51.5 & 78.8 & 85.1 & 70.0 & 90.8 & 69.7 & 58.6 & 88.2 & 48.5 & 80.2 & 65.3 & 72.6 \\
			CDAN (searched) & 86.6 & 47.0 & 82.6 & 85.9 & 75.6 & 87.0 & 78.0 & 63.5 & 88.2 & 55.0 & 79.6 & 76.2 & 75.4 \\
			Gains ($+\Delta$) & \textcolor{red}{+1.8} & \textcolor{green}{-4.5} & \textcolor{red}{+3.8} & \textcolor{red}{+0.8} & \textcolor{red}{+5.6} & \textcolor{green}{-3.8} & \textcolor{red}{+8.3} & \textcolor{red}{+4.9} & \textcolor{red}{+0.0} & \textcolor{red}{+6.5} & \textcolor{green}{-0.6} & \textcolor{red}{+10.9} & \textcolor{red}{\textbf{+2.8}} \\
			\hline
			MDD (default) & 68.9 & 59.5 & 89.7 & 89.5 & 67.8 & 94.4 & 73.7 & 50.2 & 93.4 & 59.0 & 79.5 & 66.2 & 74.3 \\
			MDD (searched) & 82.0 & 54.6 & 86.9 & 90.7 & 81.4 & 94.6 & 78.2 & 64.4 & 88.4 & 57.2 & 83.1 & 69.4 & 77.6 \\
			Gains ($+\Delta$) & \textcolor{red}{+13.1} & \textcolor{green}{-4.9} & \textcolor{green}{-2.8} & \textcolor{red}{+1.2} & \textcolor{red}{+13.6} & \textcolor{red}{+0.2} & \textcolor{red}{+4.5} & \textcolor{red}{+14.2} & \textcolor{green}{-5.0} & \textcolor{green}{-1.8} & \textcolor{red}{+3.6} & \textcolor{red}{+3.2} & \textcolor{red}{\textbf{+3.3}} \\
			\hline
			MCC (default) & 85.9 & 71.1 & 77.9 & 87.1 & 80.1 & 82.6 & 58.9 & 58.8 & 90.2 & 55.8 & 80.7 & 75.1 & 75.3 \\
			MCC (searched) & 88.5 & 69.3 & 79.2 & 91.0 & 81.7 & 85.0 & 71.0 & 64.3 & 92.8 & 61.1 & 80.0 & 77.2 & 78.4 \\
                Gains ($+\Delta$) & \textcolor{red}{+2.6} & \textcolor{green}{-1.8} & \textcolor{red}{+1.3} & \textcolor{red}{+3.9} & \textcolor{red}{+1.6} & \textcolor{red}{+2.4} & \textcolor{red}{+12.1} & \textcolor{red}{+5.5} & \textcolor{red}{+2.6} & \textcolor{red}{+5.3} & \textcolor{green}{-0.7} & \textcolor{red}{+2.1} & \textcolor{red}{\textbf{+3.1}} \\
			\bottomrule
	\end{tabular}}
        \caption{Hyper-parameters found by our metric vs. those manually tuned on VisDA2017. }
	\label{tab:visda-search}
\vspace{-4mm}
\end{table*}

\subsection{Unsupervised Hyper-parameter Search}

Most UDA methods require manual tuning of hyper-parameters for different datasets. It would be ideal to unsupervised find suitable hyper-parameters automatically. In this section, we show that our ACM can be used for the unsupervised search of hyper-parameters. We will conduct unsupervised hyper-parameter searches for four algorithms: DANN, CDAN, MCC, and MDD. For each UDA training method, we first define its hyper-parameter search space, shown in the dense hyper-parameter space in Tab.~\ref{tab:hyperparameter}. 
We set ACM as the target of the hyper-parameter search. We simply utilized the TPE search algorithm ~\cite{TPE} for 50 trials and Optuna's median pruner ~\cite{Optuna} to speed up the search. For each transfer task in the dataset, we report the target accuracy of the best model found by ACM. We compare this to the performance of the default hyper-parameters for each method used in the TL-Lib~\cite{tllib}.
Tab.~\ref{tab:visda-search} shows the target accuracy of the model found by our metric and the default model on VisDA. For all four training methods, the hyper-parameters found by us outperform those manually tuned by TL-Lib. Unlike previous supervised tuning, our search process requires no label information on the target domain. Results on Office and DomainNet can be found in the Appendix.
%Readers might find the results of the default hyper-parameters are lower than those in TL-Lib~\cite{tllib}. It is because TL-Lib trains the model with more steps, and they split the total steps into more than 30 epochs. It is more likely to select a better model with a larger Early-stopping space.

%We also find the training methods that are sensitive to the hyper-parameter are more likely to find better hyper-parameter, e.g., DANN and MCC.


\section{Conclusion}

This paper studies the principles that a robust UDA evaluation metric satisfies. By analyzing the drawbacks of the mutual information metric, we propose Inception Score Metric for UDA (ISM) and Augmentation Consistency Metric (ACM). By conducting extensive experiments, we validate the effectiveness of our metrics in a variety of scenarios. Additionally, our research highlights the potential of evaluation metrics to further the development of AutoML in the UDA.

%%%%%%%%% REFERENCES
% {\small
	\bibliographystyle{iclr2024}
	\bibliography{egbib}
% }

\clearpage
\appendix

\include{chapter_supp}

\end{document}
