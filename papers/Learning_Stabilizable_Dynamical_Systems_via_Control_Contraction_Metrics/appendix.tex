\section{Justification for Sparsity Assumption for $\hat{B}$}
\label{app:justify_B}

Physically, structural assumption~\eqref{B_simp} is not as mysterious as it appears. Indeed, consider the standard dynamics form for mechanical systems:
\[
	H(q) \ddot{q} + C(q,\dot{q}) \dot{q} + g(q) = B(q) u,
\]
where $q \in \reals^{n_q}$ is the configuration vector, $H \in \Sjpp_{n_q}$ is the inertia matrix, $C(q,\dot{q})\dot{q}$ contains the centrifugal and Coriolis terms, $g$ are the gravitational terms, $B \in \reals^{n_q \times m}$ is the (full rank) input matrix mapping and $u \in \reals^{m}$ is the input. For fully actuated systems, $\mathrm{rank}(B) = m = n_q$. For underactuated systems, $m < n_q$. By rearranging the configuration vector~\cite{Spong1998,Olfati-Saber2001,ReyhanogluSchaftEtAl1999}, one can partition $q$ as $(q_u, q_a)$ where $q_u \in \reals^{n_q - m}$ represents the unactuated degrees of freedom and $q_a \in \reals^{m}$ represents the actuated degrees of freedom. Applying this partitioning to the dynamics equation above yields
\[
	\begin{bmatrix} H_{uu} (q) & H_{ua}(q) \\ H_{ua}(q) & H_{aa} (q)\end{bmatrix} \begin{bmatrix} \ddot{q}_u \\ \ddot{q}_a \end{bmatrix} + \begin{bmatrix} \tau_u(q,\dot{q}) \\ \tau_a (q,\dot{q}) \end{bmatrix} = \begin{bmatrix} O_{(n_q-m)\times m} \\ b(q) \end{bmatrix} u
\]
where $b \in \reals^{m \times m}$ is an invertible square matrix. As observed in~\cite{ReyhanogluSchaftEtAl1999}, a substantial class of underactuated systems can be represented in this manner. Of course, fully actuated systems also take this form ($m = n_q$). Thus, by taking as state $x = (q, p) \in \reals^{n}$ where $p = H(q) \dot{q}$ is momentum (so that $n = 2n_q$), the dynamics can be written as~\eqref{dyn}:
\begin{equation}
	\dot{x} = \begin{bmatrix} \dot{q} \\ \dot{p} \end{bmatrix} = \begin{bmatrix} H^{-1} (q) p \\ \dot{H}(q) \dot{q} - \tau(q,\dot{q}) \end{bmatrix}  + \begin{bmatrix} O_{(n-m) \times m} \\ b(q) \end{bmatrix}u.
\label{under_a}
\end{equation}
Notice that the input matrix takes the desired normal form in~\eqref{B_simp}. To address the apparent difficult of working with the state representation of $(q,p)$ (when usually only measurements of $(q,\dot{q},\ddot{q})$ are typically available \emph{and} the inertia matrix $H(q)$ is unknown), we make use of the following result from~\cite{ManchesterSlotine2017}:

\begin{theorem}[CCM Invariance to Diffeomorphisms]\label{thm:ccm_inv}
Suppose there exists a valid CCM $M_x(x)$ with respect to the state $x$. Then, if $z = \psi(x)$ is a diffeomorphism, then, the CCM conditions also hold with respect to state $z$ with metric $M_z(z) = \Psi(z)^{-T}M_x(x)\Psi(z)^{-1}$, where $\Psi(z) = \partial \psi(x)/\partial x$ evaluated at $x = \psi^{-1}(z)$.
\end{theorem}

Thus, for the (substantial) class of underactuated systems of the form~\eqref{under_a}, one would solve problem~\eqref{prob_gen2} by alternating between fitting $H, \tau, b$ (using the demonstrations $(q,\dot{q},\ddot{q})$) and leveraging Theorem~\ref{thm:ccm_inv} and the previous estimate of $H$ to enforce the matrix inequality constraints using the state representation $(q,p)$. This allows us to borrow several existing results from adaptive control on estimating mechanical system by leveraging the known linearity of $H(q)$ in terms of unknown mass property parameters multiplying known physical basis functions. We leave this extension however, to future work.


\section{Derivation of Problem~\eqref{learn_finite}}\label{sec:deriv}

To go from the general problem definition in~\eqref{prob_gen2} to the finite dimensional problem in~\eqref{learn_finite}, we first must define appropriate function classes for $f$, $b_j$, and $W$. We will do this using the framework of RKHS.

\subsection{Reproducing Kernel Hilbert Spaces}

{\bf Scalar-valued RKHS}: Kernel methods~~\cite{ScholkoepfSmola2001} constitute a broad family of non-parametric modeling techniques for solving a range of problems in machine learning. A scalar-valued positive definite kernel function $k : \X \times \X \mapsto \reals$ generates a Reproducing Kernel Hilbert Space (RKHS) of functions, with the nice property that if two functions are close in the distance derived from the norm (associated with the Hilbert space), then their pointwise evaluations are close at all points. This continuity of evaluation functionals has the far reaching consequence that norm-regularized learning problems over RKHSs admit finite dimensional solutions via Representer Theorems. The kernel $k$ may be (non-uniquely) associated with a higher-dimensional embedding of the input space via a feature map, $\phi: \reals^n \mapsto \reals^D$, such that $k(x, z) = \langle \phi(x), \phi(z)\rangle$, where $D$ is infinite for universal kernels associated with RKHSs that are dense in the space of square integrable functions. Standard regularized linear statistical models in the embedding space implicitly provide non-linear inference with respect to the original input representation.  In a nutshell, kernel methods provide a rigorous algorithmic framework for deriving non-linear counterparts of a whole array of linear statistical techniques, e.g. for classification, regression, dimensionality reduction and unsupervised learning. For details, we point the reader to~\cite{HearstDumaisEtAl1998}.

%\ssmargin{[insert paragraph]}{sumeet}

{\bf Vector-valued RKHS}: Dynamics estimation is a vector-valued learning problem. Such problems can be naturally formulated in terms of  vector-valued generalizations of RKHS conceps. The theory and formalism of vector-valued RKHS can be traced as far back as the work of Laurent Schwarz in 1964, with applications ranging from solving partial differential equations to machine learning. Informally, we say that that ${\cal H}$ is an RKHS of $\reals^n$-valued maps if for any $v\in \reals^n$, the linear functional that maps $f \in {\cal H}$ to $v^T f(x)$ is continuous.

More formally, denote the standard inner product on $\Y$ as $\ip{\cdot}{\cdot}$ and let $\Y(\X)$ be the vector space of all functions $f : \X \rightarrow \Y$ and let $\Lin(\Y)$ be the space of all bounded linear operators on $\Y$, i.e., $n\times n$ matrices. A function $K : \X \times \X \rightarrow \Lin(\Y)$ is an \emph{operator-valued positive definite kernel} if for all $(x,z) \in \X \times \X$, $K(x,z)^T = K(z,x)$ and for all finite set of points $\{x_i\}_{i=1}^{N} \in \X$ and $\{y_i\}_{i=1}^{N} \in \Y$, 
\[
	\sum_{i,j=1}^{N} \ip{y_i}{K(x_i,x_j)y_j} \geq 0.
\]
Given such a $K$, we define the unique $\Y$-valued RKHS $\Hk \subset \Y(\X)$ with reproducing kernel $K$ as follows. For each $x \in \X$ and $y\in \Y$, define the function $K_{x} y = K(\cdot, x) y \in \Y(\X)$. That is, for all $z \in \X$, $K_x y (z) = K(z,x) y$. Then, the Hilbert space $\Hk$ is defined to be the completion of the linear span of functions $\{K_x y \ | \ x \in \X, y \in \Y\}$ with inner product between functions $f  = \sum_{i=1}^{N} K_{x_i} y_i,\ g = \sum_{j=1}^{M} K_{z_j} w_i \in \Hk$, defined as:
\[
	\ip{f}{g}_{\Hk} = \sum_{i=1}^{N}\sum_{j=1}^{M} \ip{y_i}{K(x_i,z_j) w_j}.
\] 
Notably, the kernel $K$ satisfies the following reproducing property:
\begin{equation}
	\ip{f(x)}{y} = \ip{f}{K_x y}_{\Hk} \quad \forall (x,y) \in \X \times \Y \wedge f \in \Hk,
\label{RK_rep}
\end{equation}
and is thereby referred to as the \emph{reproducing kernel} for $\Hk$. As our learning problems involves the Jacobian of $f$, we will also require a representation for the derivative of the kernel matrix. Accordingly, suppose the kernel matrix $K$ lies in $C^{2}(\X \times \X)$. For any $j \in \{1,\ldots, n\}$, define the matrix functions $\frac{\partial}{\partial s^j} K: \X \times \X \rightarrow \Lin(\Y)$ and $\frac{\partial}{\partial r^j} K : \X \times \X \rightarrow \Lin(\Y)$ as
\[
	\dfrac{\partial}{\partial s^j} K(z,x) := \left.\dfrac{\partial}{\partial s^j} K(r , s) \right|_{r = z, s = x}  , \quad \dfrac{\partial}{\partial r^j} K(z,x) := \left. \dfrac{\partial}{\partial r^j} K(r,s) \right|_{r = z,s=x},
\]
where the derivative of the kernel matrix is understood to be element-wise. 
%By the adjoint property of $K$, we have the identity
% \[
% 	\dfrac{\partial}{\partial r^j} K(z,x) = \dfrac{\partial}{\partial s^j} K^T(x,z).
% \]
Define the $C^1$ function $\partial_j K_x y$ on $\Y(\X)$ as
\[
	\partial_j K_x y(z) := \dfrac{\partial}{\partial s^j} K(z, x)y \quad \forall z \in \X.
\]
% Similarly, for any $i,j \in \{1,\ldots,n\}$ define the matrix function $\frac{\partial^2}{\partial r^i \partial s^j} K : \X\times \X \rightarrow \Lin(\Y)$:
% \[
% 	\dfrac{\partial^2}{\partial r^i \partial s^j} K (z,x) := \left.\dfrac{\partial^2}{\partial r^i \partial s^j} K(r,s) \right| _{r=z,s=x},
% \]
% and the $C^0$ function $\partial^2_{ij} K_x y$ on $\Y^\X$:
% \[
% 	\partial^2_{ij} K_x y(z) := \dfrac{\partial} {\partial z^i} (\partial_j K_x y ) (z) = \dfrac{\partial^2}{\partial r^i \partial s^j} K (z ,x) y	\quad \forall z \in \X.
% \]
The following result provides a useful reproducing property for the derivatives of functions in $\Hk$ that will be instrumental in deriving the solution algorithm.

\begin{theorem}[Derivative Properties on $\Hk$ \cite{MicheliGlaunes2014}] \label{thm:RK_der}
Let $\Hk$ be a RKHS in $\Y(\X)$ with reproducing kernel $K \in C^2(\X \times \X)$. Then, for all $(x,y) \in \X \times \Y$:
\begin{enumerate}
	\item[(a)] $\partial_j K_x y \in \Hk$ for all $j = 1,\ldots,n$. 
	\item[(b)] The following derivative reproducing property holds for all $j = 1,\ldots, n$:
	\[
		\ip{\dfrac{\partial f(x)}{\partial x^j}}{y} = \ip{f}{ \partial_j K_x y}_{\Hk}, \quad \forall f \in \Hk.
	\]
\end{enumerate}
\end{theorem}

As mentioned in Section~\ref{sec:soln}, the bi-linearity of constraint~\eqref{nat_contraction_W} forces us to adopt an alternating solution strategy whereby in the ``dynamics" sub-problem, $\{W,\wl,\wu\}$ are held fixed and we minimize $J_d$ with respect to $\{f,B,\lambda\}$. In the ``metric" sub-problem, $\{f,B,\lambda\}$ are held fixed and we minimize $J_m$ with respect to $\{W,\wl,\wu\}$.

In the following we derive several useful representer theorems to characterize the solution of the two sub-problems, under the two simplifying assumptions introduced in Section~\ref{sec:finite}. 

\subsection{Dynamics Sub-Problem}\label{sec:dyn_sub}

Let $K^f$ be the reproducing $\mathcal{C}^2$ kernel for an $\Y$-valued RKHS $\Hk^f$ and let $K^B$ be another reproducing kernel for an $\Y$-valued RKHS $\Hk^B$. Define the finite-dimensional subspaces:
\begin{align}
	\Vf &:= \left\{ \sum_{i=1}^{N_c} K^f_{\xs} a_i + \sum_{i=1}^{N_c} \sum_{p=1}^{n} \partial_p K^f_{\xs} a'_{ip},\quad a_i, a'_{ip} \in \reals^n \right\} \subset \Hk^f. \label{rep} \\
	\Vb &:= \left\{ \sum_{i=1}^{N_c} \Kb_{\xs} c_i + \sum_{i=1}^{N_c} \sum_{p=1}^{n} \partial_p \Kb_{\xs} c'_{ip},\quad c_i, c'_{ip} \in \reals^n \right\} \subset \Hk^B.\label{rep_b}
\end{align}
Note that all $\xs$ taken from the training dataset of $(\xs,\us,\dot{x}_i)$ tuples are a subset of $X_c$.

\begin{theorem}[Representer Theorem for $f,B$]\label{thm:rep_dyn}
Suppose the reproducing kernel $K^B$ is chosen such that all functions $g \in \Hkb$ satisfy the sparsity structure $g^j(x) = 0$ for $j = 1,\ldots,n-m$. Consider then the pointwise-relaxed dynamics sub-problem for problem~\eqref{prob_gen2}:
\begin{align}
&\min_{\substack{\hat{f} \in \Hk^{f}, \ \hat{b}_j \in \Hk^{B}, j =1,\ldots,m \\ \lambda \in \reals_{>0}}} && J_d(\hat{f},\hat{B})  \nonumber \\
&\qquad \text{s.t.} && F(\xs; \hat{f},W,\lambda) \preceq 0, \quad \forall \xs \in X_c. \label{lmi_pw1}
\end{align}
Suppose the feasible set for the LMI constraint is non-empty. Denote $f^*$ and $b_j^*, j = 1,\ldots, m$ as the optimizers for this sub-problem. Then, $f^* \in \Vf$ and $b_j \in \Vb$ for all $j = 1,\ldots, m$. 
\end{theorem}
\begin{proof}
For a fixed $W$, the constraint is convex in $\hat{f}$ by linearity of the matrix $F$ in $\hat{f}$ and $\partial \hat{f}/\partial x$. By assumption~\eqref{ass:B_simp}, and the simplifying form for $B_{\perp}$, the matrix $F$ is additionally independent of $B$. Then, by strict convexity of the objective functional, there exists unique minimizers $f^*, b_j^* \in \Hk$, provided the feasible region is non-empty~\cite{KurdilaZabarankin2006}. 

Since $\Vf$ and $\Vb$ are closed, by the Projection theorem, $\Hk^f = \Vf \oplus \Vf^{\perp}$ and $\Hkb = \Vb \oplus \Vb^{\perp}$. Thus, any $g \in \Hk^f$ may be written as $g^{\Vf} + g^{\perp}$ where $g^{\Vf} \in \Vf$, $g^{\perp} \in \Vf^\perp$, and $\ip{g^{\Vf}}{g^{\perp}}_{\Hk^f} = 0$. Similarly, any $g \in \Hkb$ may be written as $g^{\Vb} + g^{\perp}$ where $g^{\Vb} \in \Vb$, $g^{\perp} \in \Vb^\perp$, and $\ip{g^{\Vb}}{g^{\perp}}_{\Hkb} = 0$.

Let $f = f^{\Vf} + f^{\perp}$ and $b_j = b_{j}^{\Vb} + b_{j}^\perp$, and define 
\[
	\begin{split}
	H^{\V}(x,u) &= f^{\Vf}(x) + \sum_{j=1}^{m} u^j b_j^{\Vb} (x) \\ 
	H^{\perp}(x,u) &= f^{\perp}(x) + \sum_{j=1}^{m} u^j b_j ^\perp (x).
	\end{split}
\]
We can re-write $J_d(f,B)$ as:
\[
\begin{split}
	J_d(f,B) &= \sum_{i=1}^{N} \ip{H^{\V} (x_i, u_i) - \dot{x}_i}{ H{^\V} (x_i,u_i) - \dot{x}_i} + 2\ip{H^{\V} (x_i, u_i) - \dot{x}_i}{H^\perp (x_i,u_i)}  \\
	&\qquad \qquad + \ip{H^\perp(x_i, u_i)}{H^\perp(x_i, u_i)} +  \mu_f \left( \|f^{\Vf}\|_{\Hk^f}^2 + \|f^\perp \|_{\Hk^f}^2 \right)  \\
	&\qquad \qquad + \mu_b \left( \sum_{j=1}^{n} \|b_j^{\Vb} \|_{\Hkb}^2 + \|b_j^\perp \|_{\Hkb}^2 \right). \\
\end{split}
\]
Now, 
\[
	\begin{split}
	&\ip{H^{\V} (\xs, u_i) - \dot{x}_i}{H^\perp (\xs,u_i)}  \\
				&\qquad = \ip{f^\perp (\xs) + \sum_{j=1}^{m} u_i^j b_j^\perp(\xs)}{H^\V (\xs, u_i) - \dot{x}_i} \\
				&\qquad = \underbrace{\ip{f^\perp}{K_{\xs}^f \left(H^\V (\xs, u_i) - \dot{x}_i \right)}_{\Hk}}_{=0} + \underbrace{\ip{\sum_{j=1}^{m} u_i^j b_j^\perp}{\Kb_{\xs} \left(H^\V (\xs, u_i) - \dot{x}_i \right)}_{\Hkb}}_{=0} 
	\end{split}
\]
since $K_{\xs}^f \left(H^\V (\xs, u_i) - \dot{x}_i \right) \in \Vf$, $\Vb^\perp$ is closed under addition, and $\Kb_{\xs} \left(H^\V (\xs, u_i) - \dot{x}_i \right) \in \Vb$. Thus, $J_d(f,B)$ simplifies to
\begin{equation}
\begin{split}
	\sum_{i=1}^{N} \left\| H^{\V} (\xs, u_i) - \dot{x}_i \right\|_2^2 &+ \mu_f  \|f^{\Vf}\|_{\Hk^f}^2 + \mu_b \sum_{j=1}^{n} \|b_j^{\Vb} \|_{\Hk^B}^2  + \\
	&+\left[ \sum_{i=1}^{N}  \left\| H^\perp (\xs, u_i) \right\|_2^2 +  \mu_f \|f^\perp \|_{\Hk^f}^2  + \mu_b\sum_{j=1}^{n}  \|b_j^\perp \|_{\Hk^B}^2 \right].
\end{split}
\label{obj_simp}
\end{equation}
Now, the $(p,q)$ element of $\partial_f W(\xs)$ takes the form 
\[
	\ip{ \frac{ \partial w_{pq} (\xs)}{\partial x}}{f (\xs)} = \ip{f}{K_{\xs}^f\frac{ \partial w_{pq} (\xs)}{\partial x}}_{\Hk^f} =  \ip{f^{\Vf}}{K^f_{\xs}\frac{ \partial w_{pq} (\xs)}{\partial x}}_{\Hk^f}.
\]
Column $p$ of $\frac{\partial f(\xs)}{\partial x} W(\xs)$ takes the form
\[
	\begin{split}
	\sum_{j=1}^{n} w_{j p} (\xs) \dfrac{\partial f(\xs)}{\partial x^j}  = \begin{bmatrix} \sum_{j=1}^{n}  w_{jp}(\xs) \ip{ \frac{\partial f(\xs)}{\partial x^j} }{ e_1} \\ \vdots \\ 	\sum_{j=1}^{n}  w_{jp}(\xs) \ip{ \frac{\partial f(\xs)}{\partial x^j} }{ e_n}	\end{bmatrix} & = 
												       	 \begin{bmatrix} \sum_{j=1}^{n}  w_{jp}(\xs) \ip{ f }{\partial_j K^f_{\xs} e_1}_{\Hk^f} \\ \vdots \\ 	\sum_{j=1}^{n}  w_{jp}(\xs) \ip{ f }{\partial_j K^f_{\xs} e_n}_{\Hk^f}	\end{bmatrix} \\
					&=  \begin{bmatrix} \sum_{j=1}^{n}  w_{jp}(\xs) \ip{ f^{\Vf} }{\partial_j K^f_{\xs} e_1}_{\Hk^f} \\ \vdots \\ 	\sum_{j=1}^{n}  w_{jp}(\xs) \ip{ f^{\Vf} }{\partial_j K^f_{\xs} e_n}_{\Hk^f}	\end{bmatrix},
	\end{split}												       	 
\]
where $e_i$ is the $i^{\text{th}}$ standard basis vector in $\reals^n$. Thus, $f^\perp$ plays no role in pointwise relaxation of constraint~\eqref{nat_contraction_W} and thus does not affect problem feasibility.
Given assumption~\ref{ass:B_simp}, $b^{\perp}$ also has no effect on problem feasibility. Thus, by non-negativity of the term in the square brackets in~\eqref{obj_simp}, we have that the optimal $f$ lies in $\Vf$ and optimal $b_j$ lies in $\Vb$. \qed
\end{proof}

The key consequence of this theorem is the reduction of the infinite-dimensional search problem for the functions $f$ and $b_j, j=1,\ldots,m$ to a finite-dimensional \emph{convex} optimization problem for the constant vectors $a_i,a'_{ip},\{c_i^{(j)}, c_{ip}^{(j)'}\}_{j=1}^{m} \in \reals^n$, by choosing the function classes $\mathcal{H}^f = \Hk^{f}$ and $\mathcal{H}^B = \Hk^{B}$. Next, we characterize the optimal solution to the metric sub-problem.

\subsection{Metric Sub-Problem}\label{sec:met_sub}

By the simplifying assumption in Section~\ref{sec:B_simp}, constraint~\eqref{killing_A} requires that $W_{\perp}$ be only a function of the first $(n-m)$ components of $x$. Thus, define $\kw : \X \times \X \rightarrow \reals$ as a \emph{scalar} reproducing kernel with associated real-valued scalar RKHS $\Hw$. Additionally, define $\kwp : \X \times \X \rightarrow \reals$ as another scalar reproducing kernel with associated real-valued scalar RKHS $\Hwp$. In particular, $\kwp$ is only a function of the first $(n-m)$ components of $x \in \X$ in both arguments.  

For all $(p,q) \in \{1,\ldots,(n-m)\}$, we will take $w_{pq} \in \Hwp$ (corresponding to $W_{\perp}$) and all other entries of $W$ as functions in $\Hw$. Define the kernel derivative functions:
\[
    \partial_j \kw_{x}(z) := \left.\dfrac{\partial \kw}{\partial r^j} \kw(r,s)\right|_{(r=x,s=z)} \quad \partial_j \kwp_{x}(z) := \left.\dfrac{\partial \kwp}{\partial r^j} \kw(r,s)\right|_{(r=x,s=z)} \quad \forall z \in \X.
\]
From~\cite{Zhou2008}, it follows that the kernel derivative functions satisfy the following two properties, similar to Theorem~\ref{thm:RK_der}:
\[
    \begin{split}
            \partial_j \kw_{x} \in \Hw \quad \forall j = 1,\ldots,n,\  x \in \X \\
            \dfrac{\partial f}{\partial x^j}(x) = \ip{f}{\partial_j \kw_x}_{\Hw}, \quad \forall f \in \Hw.
    \end{split}
\]
A similar property holds for $\kwp$ and $\Hwp$. Consider then the following finite-dimensional spaces:
\begin{align}
	\V_{\kw} &:= \left\{ \sum_{i=1}^{N_c}  a_i \kw_{\xs} + \sum_{i=1}^{N_c} \sum_{p=1}^{n}  a_{ip}' \,  \partial_p \kw_{\xs} ,\quad a_i, a_{ip}' \in \reals \right\} \subset \Hw \\
	\V_{\kwp} & := \left\{ \sum_{i=1}^{N_c} c_i \kwp_{\xs} + \sum_{i=1}^{N_c} \sum_{p=1}^{n}  c_{ip}' \,  \partial_p \kwp_{\xs} ,\quad c_i, c_{ip}' \in \reals \right\} \subset \Hwp 
\end{align}
and the proposed representation for $W(x)$:
\begin{equation}
	\begin{split}
	W(x) =  &\sum_{i=1}^{N_c} \hat{\Theta}_i \kwp_{\xs} (x) + \sum_{i=1}^{N_c}\sum_{j=1}^{n} \hat{\Theta}'_{ij} \partial_{j}\kwp_{\xs} (x)  \\
		+  & \sum_{i=1}^{N_c} \Theta_i \kw_{\xs} (x) + \sum_{i=1}^{N_c}\sum_{j=1}^{n} \Theta'_{ij} \partial_{j}\kw_{\xs} (x),
	\end{split}
\label{W_rep}
\end{equation}
where $\hat{\Theta}, \hat{\Theta}' \in \Sj_{n}$ are constant symmetric matrices with non-zero entries only in the top-left $(n-m)\times(n-m)$ block, and $\Theta,\Theta' \in \Sj_{n}$ are constant symmetric matrices with zero entries in the top-left $(n-m)\times(n-m)$ block.

\begin{theorem}[Representer Theorem for $W$]\label{thm:rep_W}
Consider the pointwise-relaxed metric sub-problem for problem~\eqref{prob_gen2}:
\begin{align}
&\min_{\substack{w_{pq} \in \Hwp, (p,q) \in \{1,\ldots,(n-m)\} \\ w_{pq} \in \Hw \text{ else} \\ \wl,\wu \in \reals_{>0}}} && J_m(W,\wl,\wu)  \nonumber \\
&\qquad \text{subject to} && F(\xs; \hat{f}, W, \lambda) \preceq 0, \quad \forall \xs \in X_c, \label{lmi_pw2} \\
&\qquad && \wl I_n \preceq W(\xs) \preceq \wu I_n, \quad \forall \xs \in X_c, \label{lmi_pw3}
\end{align}
Suppose the feasible set of the above LMI constrains is non-empty. Denote $W^*$ as the optimizer for this sub-problem. Then, $W^*$ takes the form given in~\eqref{W_rep}.
\end{theorem}
\begin{proof}
Notice that while the regularizer term is strictly convex, the surrogate loss function for the condition number is affine. However, provided the feasible set is non-empty, there still exists a minimizer (possible non-unique) for the above sub-problem. 

Now, notice that the $(p,q)$ element of $\partial_{f} W_{\perp} (\xs)$ takes the form
\[
	\begin{split} 
		\partial_{f} w_{\perp_{pq}}(\xs) &=\sum_{j=1}^{n} \dfrac{\partial w_{\perp_{pq}} (\xs)}{\partial x^j} f^j (\xs) \\
								&= \sum_{j=1}^{n} \ip{w_{\perp_{pq}}}{\partial_{j}\kwp_{\xs}}_{\Hwp} f^j(\xs) \\
								&= \ip{w_{\perp_{pq}}}{ \sum_{j=1}^{n} f^j(\xs) \partial_{j}\kwp_{\xs}}_{\Hwp} \\
								&= \ip{w^{\V_{\kwp}}_{\perp_{pq}}}{ \sum_{j=1}^{n} f^j(\xs) \partial_{j}\kwp_{\xs}}_{\Hwp} 
	\end{split}
\]
Additionally, the $(p,q)$ element of $W(\xs)$ takes the form
\[
		w_{pq}(\xs) = \begin{cases}  \ip{w^{\V_{\kwp}}_{pq}}{\kwp_{\xs}}_{\Hwp} &\text{ if } (p,q) \in \{1,\ldots,(n-m)\} \\
						 	    \ip{w^{\V_\kw}_{pq}}{\kw_{\xs}}_{\Hw} &\text{ else.}
					\end{cases}
\]
That is, constraints~\eqref{nat_contraction_W} and uniform definiteness at the constraint points in $X_c$ can be written in terms of functions in $\V_\kw$ and $\V_{\kwp}$ alone. By strict convexity of the regularizer, and recognizing that $W(x)$ is symmetric, the result follows. \qed 
\end{proof}


Similar to the previous section, the key property here is the reduction of the infinite-dimensional search over $W(x)$ to the finite-dimensional convex optimization problem over the constant symmetric matrices $\Theta_i,\Theta_{ij}',\hat{\Theta}_i,\hat{\Theta}'_{ij}$ by choosing the function class for the entries of $W$ using the scalar-valued RKHS.

At this point, both sub-problems are finite-dimensional convex optimization problems. Crucially, the only simplifications made are those given in Section~\ref{sec:finite}. However, a final computational challenge here is that the number of parameters scales with the number of training $N$ and constraint $N_c$ points. This is a fundamental challenge in all non-parametric methods. In the next section we present the dimensionality reduction techniques used to alleviate these issues.

\subsection{Approximation via Random Matrix Features}
\label{sec:feat}

The size of the problem using full matrix-valued kernel expansions in grows rapidly in $N_c n$, the number of constraint points times the state dimensionality. This makes training slow for even moderately long demonstrations even in low-dimensional settings. The induced dynamical system is slow to evaluate and integrate at inference time. Random feature approximations to kernel functions have been extensively used to scale up training complexity and inference speed of kernel methods~\cite{RahimiRecht2007, HuangAvronEtAl2014} in a number of applications.  The quality of approximation can be explicitly controlled by the number of random features. In particular, it has been shown~\cite{RahimiRecht2008} that any function in the RKHS associated with the exact kernel can be approximated to arbitrary accuracy by a linear combination of sufficiently large number of random features. 

These approximations have only recently been extended to matrix-valued kernels~\cite{Minh2016,BraultHeinonenEtAl2016}. Given a matrix-valued kernel $K$, one defines an appropriate matrix-valued feature map $\Phi : \X \rightarrow \reals^{d\times n}$ with the property 
\[
    K(x,z) \approx \Phi(x)^T \Phi(z),
\]
where $d$ controls the quality of this approximation. With such an approximation, notice that \emph{any} function $g$ in the associated RKHS $\Hk$ can be re-parameterized as:
\[
    g(x) = \sum_{i=1}^{N_c} K(x,\xs) a_i \approx \sum_{i=1}^{N_c} \Phi(x)^T \Phi(\xs) a_i = \Phi(x)^T \alpha,
\]
where $\alpha = \sum_{i=1}^{N_c} \Phi(\xs)a_i \in \reals^d$. Thus, the function $g$ is now summarized by the $d-$ dimensional vector $\alpha$ instead of $N_c$ vectors in $\Y$ ($N_c n$ parameters). 
Applying a similar trick to:
\[
    \dfrac{\partial}{\partial s^j} K(x,z) \approx \left. \Phi(x)^T \dfrac{\partial}{\partial s^j} \Phi(s) \right|_{s =z},
\]
one can approximate terms of the form
\[
    \sum_{i=1}^{N_c}\sum_{j=1}^{n} \partial_j K_{\xs} c_{ij} (x)
\]
by $\Phi(x)^T v$ for some $v \in \reals^d$. Applying this approximation to the subspaces in~\eqref{rep},~\eqref{rep_b}, and~\eqref{W_rep}, we finally arrive at the function parameterizations\footnote{To apply this decomposition to $W(x)$, we simply leverage vector-valued feature maps for each entry of $W(x)$.} in eqs.~\eqref{param_1}--\eqref{param_2}, and the constraints reformulation in problem~\eqref{learn_finite} in terms of the constant vectors. The regularization terms in the objective follow from the definition of the inner product in $\Hk$:
\small{
\[ 
\begin{split}
    \|g\|_{\Hk}^2 = \sum_{i,j=1}^{N_c} \ip{a_i}{K(x_i,x_j)a_j} \approx \sum_{i,j=1}^{N_c} \ip{\Phi(x_i)a_i}{\Phi(x_j)a_j} &= \ip{\sum_{i=1}^{N_c}\Phi(x_i)a_i}{\sum_{j=1}^{N_c}\Phi(x_j)a_j} \\
    &= \|\alpha\|_2.
\end{split}
\]
}\normalsize
A canonical example is the the Gaussian separable kernel $K_{\sigma}(x,z) := e^{\frac{-\|x-z\|_2^2}{\sigma^2}} I_n$ with feature map:
\[
    \dfrac{1}{\sqrt{s}}\begin{bmatrix} \cos(\omega_1^T x) \\ \sin(\omega_1^T x) \\ \vdots \\ \cos(\omega_s^T x) \\ \sin(\omega_s^T x)    \end{bmatrix} \otimes I_n 
\]
where $\omega_1,\ldots,\omega_s$ are i.i.d. draws from $\mathcal{N}(0,\sigma^{-2}I_n)$. 

\section{Solution Parameters for PVTOL}
\label{app:prob_params}

Notice that the true input matrix for the PVTOL system satisfies Assumption~\ref{ass:B_simp}. Furthermore, it is a constant matrix. Thus, the feature mapping $\Phi_b$ is therefore just a constant matrix with the necessary sparsity structure.

The feature matrix for $f$ was generated using the Random Fourier approximation to the Gaussian separable matrix-valued kernel (see Appendix~\ref{sec:feat}) with $\sigma = 6$ and $s = 8n = 48$ sampled Gaussian directions, yielding a feature mapping matrix $\Phi_f$ with $d_f = 576$ (96 features for each component of $f$). The scalar-valued reproducing kernels for the entries of $W$ were taken to be the Gaussian kernel with $\sigma = 15$. To satisfy condition~\eqref{killing_A}, the kernel for $w_{ij}, \ (i,j) \in \{1,\ldots,(n-m)\}$ was only a function of the first $n-m$ components of $x$. A total of $s = 36$ Gaussian samples were taken to yield feature vectors $\phi_w$ and $\hat{\phi}_w$ of dimension $d_w = 72$.  Furthermore, by symmetry of $W(x)$ only $n(n+1)/2$ functions were actually parameterized. Thus, the learning problem in~\eqref{learn_finite} comprised of $d_f + d_w n(n+1)/2 + 4 = 508$ parameters for the functions, plus the extra scalar constants $\lambda,\wl,\wu$.

The learning parameters used were: model N-R (all $N$): $\mu_f = 0, \mu_b = 10^{-6}$, R-R (all $N$): $\mu_f = 10^{-6}, \mu_b = 10^{-6}$, CCM-R (all $N$): $\mu_f = 10^{-3}, \mu_b = 10^{-6}$; $N \in \{100,250,500\}: \mu_w = 10^{-3}$, $N = 1000: \mu_w = 10^{-4}$. Tolerance parameters: constraints: $\{\delta_{\lambda},\epsilon_{\lambda},\delta_{\wl}, \epsilon_{\wl}\} = \{0.01,0.01,0.01,0.01\}$; discard tolerance $\delta = 0.05$. Note that a small penalization on $\mu_b$ was necessary for all models due to the fact that feature matrix $\Phi_b$ is rank deficient.

\section{CCM Controller Synthesis} \label{ccm_appendix}

Let $\Gamma(p,q)$ be the set of smooth curves $c:[0,1] \rightarrow \X$ satisfying $c(0) = p, c(1) = q$, and define $\delta_c(s) := \partial c(s)/\partial s$. At each time $t$, given the nominal state/control pair $(x^*,u^*)$ and current actual state $x$:
\begin{leftbox}
\begin{itemize}[leftmargin=.4in]
    \item[{\bf Step 1:}] Compute a curve $\gamma \in \Gamma(x^*,x)$ defined by:
\begin{equation}
    \gamma \in \argmin_{c\in \Gamma(x^*, x)} \int_{0}^{1} \delta_c(s)^T M(c(s)) \delta_c (s) ds,
\end{equation}
    and let $\mathcal{E}$ denote the minimal value.
    \item[{\bf Step 2:}] Define: 
    \begin{equation}
    \begin{split}
        \mathcal{E}_d(k):= &2 \delta_{\gamma}(1)^T M(x) (f(x) + B(x) (u^* + k)) \\
        &- 2\delta_{\gamma}(0)^T M(x^*) (f(x^*) + B(x^*)u^*).
    \end{split}
    \label{ccm_ineq}
    \end{equation}
    \item[{\bf Step 3:}] Choose $k(x^*,x)$ to be any element of the set:
    \begin{equation}
        \mathcal{K} := \left\{ k : \mathcal{E}_d(k) \leq -2\lambda \mathcal{E} \right\}.
    \label{ccm_cntrl}
    \end{equation}
\end{itemize}
\end{leftbox}


By existence of the metric/differential controller pair $(M,\delta_u)$, the set $\mathcal{K}$ is always non-empty. The resulting $k(x^*,x)$ then ensures that the solution $x(t)$ indeed converges towards $x^*(t)$ exponentially~\cite{SinghMajumdarEtAl2017}.

From an implementation perspective, note that having obtained the curve $\gamma$, constraint~\eqref{ccm_cntrl} is simply a linear inequality in $k$. Thus, one can analytically compute a feasible feedback control value, e.g., by searching for the smallest (in any norm) element of $\mathcal{K}$; for additional details, we refer the reader to~\cite{ManchesterSlotine2017,SinghMajumdarEtAl2017}.