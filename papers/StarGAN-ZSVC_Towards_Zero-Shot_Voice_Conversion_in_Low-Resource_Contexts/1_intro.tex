\section{Introduction}
Voice conversion is a speech processing task where speech from a source speaker is transformed so that it appears to come from a different target speaker while preserving linguistic content.
A fast, human-level voice conversion
system has significant applications across several industries, from those in privacy and identity protection \cite{privacy} to those of voice mimicry and disguise \cite{vc_vc_spk_id,disguise}.
It can also be essential for addressing downstream speech processing problems in low-resource contexts where training data is limited:
it could be used to augment training data by converting the available utterances to novel speakers---effectively increasing the diversity of training data and improving the quality of the resulting systems.

Recent techniques have improved the quality of voice conversion significantly, in part due to the Voice Conversion Challenge (VCC) and its efforts to concentrate disparate research efforts \cite{vcc2020}. 
Some techniques are beginning to achieve near human-level quality in conversion outputs.
However, much of the advances and improvements in quality are limited in their practical usefulness because they fail to satisfy several requirements that would be necessary for practical use, particularly in low-resource settings.

First, a practical voice conversion system should be trainable on non-parallel data.
That is, training data should not need to contain utterances from multiple speakers saying the same words -- such a setting is known as a parallel data setting.
Non-parallel data is the converse, where the different utterances used to train the model do not contain the same spoken words.
Parallel data is difficult to collect in general, and even more so for low-resource language (those which have limited digitally stored corpora). 
Second, a practical system should be able to convert speech to and from speakers which have not been seen during training.
This is called \textit{zero-shot} voice conversion.
Without this requirement, a  system would need to be retrained whenever speech from a new speaker is desired.
Finally, for a number of practical applications, a voice conversion system needs to run at least in real-time. 
For data augmentation in particular, having the system run as fast as possible is essential for it to be practical in the training of a downstream speech model.

With these requirements in mind, we look to extend existing state-of-the-art voice conversion techniques.
We specifically extend the recent StarGAN-VC2 \cite{stargan-vc2} approach to the zero-shot setting, proposing the new StarGAN-ZSVC model.
StarGAN-ZSVC achieves zero-shot prediction by using a speaker encoding network to generate speaker embeddings for potentially unseen speakers; these embeddings are then used to condition the model at inference time.
  
Through objective and human evaluations, we show that StarGAN-ZSVC performs better than simple baseline models and similar or better than the recent AutoVC zero-shot voice conversion approach~\cite{autovc} across a range of evaluation metrics.
More specifically, it gives similar or better performance in all zero-shot settings considered, and does so more than five times faster than AutoVC.

