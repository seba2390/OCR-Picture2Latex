\section{Related Work}
A typical voice conversion system operates in the frequency domain, first converting an input utterance into a spectrogram and then using some model to map the spectrogram spoken by a source speaker to that of one spoken by a target speaker.
The output spectrogram is then converted to a waveform in the time-domain using
a vocoder \cite{metaanalysis}.
In this paper, we denote spectrogram sequences as $X = [\mathbf{x}_1, \mathbf{x}_2, ..., \mathbf{x}_T]$, where the spectrogram contains $T$ frames, and each frame $\mathbf{x}_i$ consists of some number of frequency bins.
In our case, we use 80-dimensional Mel-scale frequency bins, i.e.\  
$\mathbf{x}_i \in \mathbb{R}^{80}$.

Some models~\cite{vtln-vc,stargan-vc}
use parametric algorithms like the WORLD vocoder \cite{WORLD-vocoder} to convert the output spectrogram 
back to a time-domain waveform.
Others use neural vocoders, which can be 
divided into autoregressive models, such as those of the WaveNet family \cite{wavenet}, and non-autoregressive models, such as MelGAN \cite{MelGAN}. 

Voice conversion models themselves can be classified on several levels.
Older techniques rely on rule-based techniques \cite{rules-vc-1,rules-vc-2} while newer models rely on statistical techniques and often make extensive use of deep neural networks \cite{metaanalysis}.
Models can also be classified into traditional models that can only perform one-to-one voice conversion, such as the recurrent DBLSTM-RNN \cite{BDLSTM} and Gaussian mixture based models \cite{gmm-vc-1,gmm-vc-2}, to newer models like those using variational autoencoders \cite{autovc,autoencoder-vc2} and vector quantized neural networks \cite{vqvae-comparisons} to allow for many-to-many conversion where a single model can convert between several possible source-target speaker pairings.

Finally, the recent AutoVC model~\cite{autovc} (Section~\ref{sec:autovc}) emerged as the first model to be able to perform zero-shot voice conversion where either the source or target speaker is unseen during training.
For our new model, we also take inspiration from recent work on speaker encoding networks trained for speaker verification~\cite{GE2E}, as well as the StarGAN-VC and -VC2 \cite{stargan-vc,stargan-vc2} models (Section~\ref{sec:stargan}). Concretely, we attempt to combine these in a new zero-shot voice conversion~model.

\subsection{StarGAN and Voice Conversion}
\label{sec:stargan}

Generative adversarial networks (GANs) train two separate networks: a generator and a discriminator.
The generator is trained to produce realistic outputs (i.e.\ it should aim to accurately approximate some function), while the discriminator is trained to discern true outputs from ones produced by the generator.
Part of the generator's objective is to fool the discriminator and to essentially maximize its loss metric, while the discriminator is trained to do the opposite.

One set of successful voice conversion techniques relies on re-purposing the StarGAN image-to-image translation technique \cite{choi2018stargan} for voice conversion.
In particular, StarGAN-VC2 \cite{stargan-vc2} extends upon StarGAN-VC \cite{stargan-vc} by training a single generator model to perform many-to-many voice conversion using speaker-dependent modulation factors in so-called conditional instance normalization layers \cite{CIN}.
The model's
generator $G(X_\text{src}, \mathbf{s}_\text{src}, \mathbf{s}_\text{trg})$ converts a spectrogram $X_{\text{src}}$ from a source speaker to a target speaker, producing the converted output $X_{\text{src}\rightarrow \text{trg}}$.
The source and target speaker identities are given as one-hot vectors,  $\mathbf{s}_\text{src}$ and $\mathbf{s}_\text{trg}$, respectively.
The model's discriminator $D(X, \mathbf{s}_\text{src}, \mathbf{s}_\text{trg})$ takes an input spectrogram $X$ and returns a scalar. 
Intuitively, the generator is trained to force the discriminator's output when given converted spectrograms to be high, while the discriminator is trained to make its output low when given converted outputs and high when given original spectrograms.

More formally, the generator $G$ is trained to minimize the loss $\mathcal{L} = \lambda_{\text{id}}\mathcal{L}_{\text{id}} + \lambda_{\text{cyc}}\mathcal{L}_{\text{cyc}} + \mathcal{L}_{G-\text{adv}}$.
The first term, $\mathcal{L}_{\text{id}}$ is an identity loss term.
It aims to minimize the difference between the input and output spectrogram when the model is made to keep the same speaker identity, i.e.\ convert from speaker A to speaker~A. 
It is defined by the $L_2$ loss:
\begin{equation}
    \mathcal{L}_{\text{id}} = ||G(X_\text{src}, \mathbf{s}_{\text{src}}, \mathbf{s}_{\text{src}}) - X_\text{src}||_2
\end{equation}
Next, many-to-many voice conversion systems like StarGAN-VC2 can perform \textit{cyclic mappings}, whereby the model is made to convert an input utterance from a source speaker to a target speaker, and then convert the output utterance back to the source speaker.
The second term of the loss aims to minimize the cyclic reconstruction error between the cyclic mapping and original spectrogram \cite{choi2018stargan}:
\begin{equation}
    \mathcal{L}_{\text{cyc}} = ||X_\text{src} - G(G(X_\text{src}, \mathbf{s}_{\text{src}}, \mathbf{s}_{\text{trg}}), \mathbf{s}_{\text{trg}}, \mathbf{s}_{\text{src}})||_1
\end{equation}
Finally, the adversarial loss term $\mathcal{L}_{G-\text{adv}}$ is added based on the LSGAN \cite{LSGAN} loss.
It defines two constants $a$ and $b$, whereby $G$'s loss tries to push $D$'s output for converted utterances closer to $a$, while $D$'s loss function tries to push $D$'s output for converted utterances closer to $b$ and its output for real outputs closer to $a$. 
Concretely, $G$'s adversarial loss is defined as
\begin{equation}
    \mathcal{L}_{G-\text{adv}} = \left( D(G(X_\text{src}, \mathbf{s}_{\text{src}},\mathbf{s}_{\text{trg}}), \mathbf{s}_{\text{src}}, \mathbf{s}_{\text{trg}}) - a \right)^2.
\end{equation}
while, the discriminator $D$ is trained to minimize the corresponding LSGAN discriminator loss:
\begin{equation}
    \mathcal{L}_{D-\text{adv}} = \left( D(G(X_\text{src}, \mathbf{s}_{\text{src}},\mathbf{s}_{\text{trg}}), \mathbf{s}_{\text{src}}, \mathbf{s}_{\text{trg}}) - b \right)^2 + \left( D(X_\text{src}, \mathbf{s}_{\text{trg}}, \mathbf{s}_{\text{src}}) - a \right)^2
\end{equation}
In~\cite{stargan-vc2}, the authors set the scalar coefficients to be $\lambda_\text{id}=5$, and $\lambda_\text{cyc}=10$.
The original study~\cite{stargan-vc2} does not mention how $a$ and $b$ are set (despite these greatly affecting training); we treat them as hyperparameters.
Note that the true target spectrogram $X_\text{trg}$ does not appear in any of the equations -- this is what allows StarGAN-VC2 to be trained with non-parallel data where the source utterance $X_\text{src}$ has no corresponding utterance from the target speaker.

StarGAN-VC2 uses a specially designed 2-1-2D convolutional architecture for the generator, as well as a projection discriminator \cite{projection_discriminator} which comprises of a convolutional network (to extract features) followed by an inner product with an embedding corresponding to the source/target speaker pair.
For the generator, a new form of modulation-based conditional instance normalization was introduced in \cite{stargan-vc2}. This allows the speaker identity (which is provided as a one-hot vector) to multiplicatively condition the channels of an input feature.
According to \cite{stargan-vc2}, this special layer is a key component in achieving high performance in StarGAN-VC2.

We use these building blocks for our new zero-shot approach. Concretely, the one-hot speaker vectors in StarGAN-VC2 are replaced with continuous embedding vectors obtained from a separate speaker encoding network (which can be applied to arbitrary speakers), as outlined in Section~\ref{sec:starganzsvc}.

\subsection{AutoVC}
\label{sec:autovc}
Zero-shot voice conversion was first introduced in 2019 with the
AutoVC model~\cite{autovc}, which remains one of only a handful of models that can perform zero-shot conversion (see e.g.\ \cite{convoice} for a very recent other example).
For AutoVC, zero-shot conversion is achieved by using an autoencoder with a specially designed bottleneck layer which forces the network's encoder to only retain linguistic content in its encoded latent representation.
The model then uses a separate recurrent speaker encoder model $E(X)$, originally proposed for speaker identification \cite{GE2E}, to extract a speaker embedding $\mathbf{s}$ from an input spectrogram.
These speaker embeddings are then used to supply the missing speaker identity information to the decoder which, together with the linguistic content from the encoder, allows the decoder to synthesize an output spectrogram for an unseen speaker. 

Formally, the full encoder-decoder model is trained to primarily minimize two terms.
The first term is an $L_2$ reconstruction loss between the decoder output spectrogram $X_{\text{src}\rightarrow\text{src}}$ and input spectrogram $X_{\text{src}}$, with the source speaker's encoding (from the speaker encoder) provided to both the encoder and decoder.
The second term is an $L_1$ loss between the speaker embedding of the decoder output $E(X_{\text{src}\rightarrow\text{src}})$ and the original speaker embedding $\mathbf{s}_\text{src} = E(X_{\text{src}})$.
The encoder and decoder consists of convolutional and Long Short-Term Memory (LSTM) \cite{lstm} recurrent layers which are carefully designed
to ensure that no speaker identity information is present in the encoder output.

As with StarGAN-VC and StarGAN-VC2, a corresponding parallel target utterance $X_\text{trg}$ does not appear in any of the loss terms, allowing AutoVC to be trained without parallel data. 
Zero-shot inference is performed by using the speaker encoder to obtain embeddings for new utterances from unseen speakers, which is then provided to the decoder instead of the embedding corresponding to the source speaker, causing the decoder to return a converted output. 
We use this same idea of using an encoding network to obtain embeddings for unseen speakers in our new GAN-based approach, which we describe next.

