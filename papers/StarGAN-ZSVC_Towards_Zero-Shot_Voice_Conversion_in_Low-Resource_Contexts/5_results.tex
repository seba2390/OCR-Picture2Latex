
\section{Experiments}
We perform two sets of experiments.
First we perform an evaluation on seen speakers, where we compare StarGAN-ZSVC to all other models to obtain an indication of both speed and performance.
We then compare StarGAN-ZSVC with AutoVC for zero-shot voice conversion, 
looking at both the output and cyclic reconstruction error.
We encourage the reader to listen to the demo samples\footnote{\url{https://rf5.github.io/sacair2020/}} for the zero-shot models. 

\subsection{Seen-to-seen Conversion} \label{sec:experiments_seen}

In the first set of comparisons, we evaluate
performance for test utterances where other utterances from both the source and target speaker have been seen during training. I.e., while the models have not been trained on these exact test utterances, they have seen the speakers during training.
There is, however, a problem in directly comparing the one-to-one models (traditional baselines) to the many-to-many models (AutoVC and StarGAN-ZSVC).
The one-to-one models are trained on parallel data, always taking in utterances from one speaker as input (VCC2SF1 in our case) and always producing output from a different target speaker (VCCSM2).

\begin{table}[!b]
    \centering
    \renewcommand{\arraystretch}{1.2}
    \caption{
    Objective evaluation results when converting between speakers where both the source and target speaker are seen during training. 
    For all metrics aside from cosine similarity, lower is better.
    Speed is measured as the time (in milliseconds) required to convert one second of input audio.
    The first StarGAN-ZSVC and AutoVC entries correspond to evaluations on the one-to-one test utterances, while the final two starred entries correspond to metrics computed when using test utterances from all seen training speakers for the many-to-many models.}
    \label{tab:seen-to-seen-results}
    \begin{tabular}{l@{\hspace{0.3cm}}c@{\hspace{0.3cm}}c@{\hspace{0.3cm}}c@{\hspace{0.3cm}}c@{\hspace{0.3cm}}c}
    
    \toprule
    Model & MAE & MSE & $\cos(\theta)$ & $e_{\text{norm}}$ & Speed (ms/s)\\
    \midrule
    Linear & 1.277 & 2.689 & 0.980 & 0.860 & \textbf{0.15}\\
    DBLSTM & 1.329 & 3.102 & 0.982 & 0.496 & 12.52\\
    UNet & 1.370 & 3.347 & 0.980 & 0.545 & 100.5 \\
    AutoVC & \textbf{0.993} & \textbf{1.756} & \textbf{0.987} & \textbf{0.259} & 10.99\\
    StarGAN-ZSVC & 1.092 & 2.101 & 0.977 & 0.513 & 1.88 \\
    \midrule
    AutoVC* & \textbf{1.000} & \textbf{1.783} & \textbf{0.987} & \textbf{0.321} & 10.99\\
    StarGAN-ZSVC* & 1.008 & 1.863 & 0.983 & \textbf{0.321} & 1.88 \\
    \bottomrule
    \end{tabular}
\end{table}

In contrast, the many-to-many models are trained without access to parallel data, taking in input utterances from several speakers (4 speakers, including VCC2SF1 and VCCSM2 in our case, as explained in Section~\ref{sec:dataset}).
This means that the one-to-one and many-to-many models observe very different amounts of data.
Moreover, while the data for both the one-to-one and many-to-many models are divided into a 90\%-10\% train-test split, the same exact splits aren't used in both setups; this is because the former requires parallel utterances, and the split is therefore across utterance \textit{pairs} and not just individual utterances.
To address this, we evaluate the many-to-many models in two settings: on the exact same test utterances as those from the test split of the one-to-one models, as well as on all possible source/target speaker utterance pairs where the source utterance is in the test utterances for the 4 seen training speakers.
In the former case, it could happen that the many-to-many model actually observes one of the test utterances during training.
Nevertheless, reporting scores for both settings allows for a meaningful comparison.

The results of this evaluation on seen speakers are given in Table~\ref{tab:seen-to-seen-results}.
The results indicate that AutoVC appears to be the best in this evaluation on seen speakers.
However, this comes at a computational cost: the linear and StarGAN-ZSVC models are a factor of 5 or more faster than the models relying on recurrent layers like DBLSTM and AutoVC.

\begin{table}[!b]
    \renewcommand{\arraystretch}{1.2}
    \centering
    \caption{Objective evaluation results for zero-shot voice conversion for AutoVC and StarGAN-ZSVC. The \textit{prediction} metrics compare the predicted output to the ground truth target, while the \textit{reconstruction} metrics compare the cyclic reconstruction $X_{\text{src}\rightarrow\text{trg}\rightarrow\text{src}}$ with the original source spectrogram. $e_{\text{norm}}$ indicates the vector norm of the speaker embeddings for the compared spectrograms, with lower values indicating closer speaker identities.}
    \label{tab:zero-shot-results}
    \begin{tabular}{c@{\hspace{0.2cm}}l @{\hspace{0.3cm}} c@{\hspace{0.2cm}}c@{\hspace{0.2cm}}c @{\hspace{0.5cm}} c@{\hspace{0.2cm}}c@{\hspace{0.2cm}}c}
    
    \toprule
    & & \multicolumn{3}{c}{Prediction} & \multicolumn{3}{c}{Reconstruction}\\
    \cmidrule(r){3-5} \cmidrule(r){6-8}
    
    Setting & Model & MAE & $\cos(\theta)$ & $e_{\text{norm}}$ & MAE & $\cos(\theta)$ & $e_{\text{norm}}$ \\
    \midrule
    \multirow{2}{*}[0pt]{Seen-to-unseen} & AutoVC & 1.246 & \textbf{0.982} & 0.742 & 1.178 & 0.976 & 0.392 \\
    & StarGAN-ZSVC & \textbf{1.030} & \textbf{0.982} & \textbf{0.705} & \textbf{0.197} & \textbf{0.997} & \textbf{0.124}\\
    \midrule
    \multirow{2}{*}[0pt]{Unseen-to-seen} & AutoVC & 1.014 & \textbf{0.986} & \textbf{0.328} & 1.201 & 0.975 & \textbf{0.753}\\
    & StarGAN-ZSVC & \textbf{0.974} & 0.985 & 0.380 & \textbf{0.921} & \textbf{0.986} & 0.760 \\
    \midrule
    \multirow{2}{*}[0pt]{Unseen-to-unseen} & AutoVC & 1.238 & 0.981 & 0.746 & 1.340 & 0.968 & 0.827 \\
    & StarGAN-ZSVC & \textbf{1.079} & \textbf{0.982} & \textbf{0.742} & \textbf{0.921} & \textbf{0.986} & \textbf{0.760} \\
    \bottomrule
    \end{tabular}
\end{table}

\subsection{Zero-shot Conversion}
Next we compare StarGAN-ZSVC and AutoVC in zero-shot settings, where either the source, target, or both source and target speaker are unseen during training. 
Many-to-many models can also be used in a cyclic manner; 
we use such cyclic reconstruction as another objective evaluation metric, where we compare how well the original spectrogram is reconstructed when performing this cyclical mapping of converting from the source speaker to the target speaker and back again.
Results for zero-shot conversion are shown in Table~\ref{tab:zero-shot-results}.

The performance for AutoVC and StarGAN-ZSVC are similar on most metrics for the unseen-to-seen case.
But for the seen-to-unseen case and the unseen-to-unseen case (where both the target and source speakers are new) StarGAN-ZSVC achieves both better prediction and reconstruction scores.
This, coupled with its fast inference speed (Section~\ref{sec:experiments_seen}), enables it to be used efficiently and effectively for downstream data augmentation purposes.

The results of the subjective evaluation are given in Figure~\ref{fig:subjective-eval}.
To put the values into context, the MOS for the raw source utterances and vocoded source
utterances included in the analysis are 4.86 and 4.33 respectively -- these serve as an upper bound for the MOS values for both models.
Figure~\ref{fig:subjective-eval} largely supports the objective evaluations, providing further evidence that StarGAN-ZSVC outperforms AutoVC in zero-shot settings.
Interestingly, it would appear that StarGAN-ZSVC also appears more natural in the traditional seen-to-seen case.
This evaluation indicates that, for human listeners, StarGAN-ZSVC appears more natural in the low-resource context considered in this paper.


\begin{figure}[!t]
\includegraphics[width=\textwidth]{figures/subjective-eval.eps}
\caption{Mean opinion score for naturalness for AutoVC and StarGAN-ZSVC in various source/target seen/unseen speaker pairings with 95\% confidence intervals shown.} \label{fig:subjective-eval}
\end{figure}

