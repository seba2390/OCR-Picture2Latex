

\section{Experimental Setup}

We compare StarGAN-ZSVC to other voice conversion models using the voice conversion challenge (VCC) 2018 dataset \cite{vcc2018}, which contains parallel recordings of native English speakers from the United States. 
Importantly, we \textit{do not} train StarGAN-ZSVC or the AutoVC model (to which we compare) using parallel input-output examples.
However, the traditional baseline models (below) do require parallel data.
All training and speed measurements are performed on a 
single {NVIDIA} RTX 2070 SUPER GPU. 

\subsection{Dataset}
\label{sec:dataset}
The VCC 2018 dataset was recorded from 8 speakers, each speaking 81 utterances from the same transcript. 4 speakers are used for training and 4 for testing.
To emulate a low-resource setting, we use a 9-minute subset of the VCC 2018 training dataset for StarGAN-ZSVC and AutoVC.
This corresponds to 90\% of the utterances from two female (F) and two male (M) speakers (VCC2SF1, VCC2SF2, VCC2SM1, and VCC2SM2).
This setup is in line with existing evaluations on VCC 2018 \cite{stargan-vc2}, allows for all combinations of inter- and intra-gender conversions, and allows for zero-shot evaluation on the 4 remaining unseen speakers.

In contrast to StarGAN-ZSVC and AutoVC, some of the baseline models only allow for one-to-one conversions, i.e. they are trained on parallel data and can only convert from seen speaker A to seen speaker B. 
We therefore train the baseline models on a single source-target speaker mapping (from VCC2SF1 to VCC2SM2), using 90\% of the parallel training utterances for this speaker pair.

All utterances are resampled to 22.05~kHz and then converted to log Mel-spectrograms with a window and hop length of 1024 and 256 samples, respectively. 
During training, for each batch we randomly sample a $k$-frame sequence from each spectrogram, where $k$ is randomly sampled from multiples of 32 between 96 to 320 (inclusive).
This is done for all models to make it robust to utterance length, with the exception of StarGAN-ZSVC, which requires fixed-size input for its discriminator.
This leads to slightly worse performance for
StarGAN-ZSVC on long or silence-padded sequences.
For a fair comparison, we therefore only consider non-silent frames of the target utterance.

\subsection{Speaker Encoder and Vocoder Setup}
The same WaveGlow vocoder is used
to produce output waveforms
for all networks
to ensure a fair comparison. 
The WaveGlow model is pretrained on a large external corpus, as provided by the original paper \cite{waveglow}. 
However, since all models use log Mel-spectrogram inputs and produce log Mel-spectrogram outputs, we rather perform all quantitative comparisons on the spectrograms of each utterance (instead of waveforms), in order to minimize the vocoder's confounding effect. 

Our WaveGlow implementation takes approximately 240~ms to produce one second of vocoded audio (taking a spectrogram as input).
For the full voice conversion system to be faster than real-time, this means that the combined inference speed of the remaining sub-networks needs to be well under 700ms/s, or preferably significantly faster if used for data augmentation.

The speaker encoder is trained on 90\% of the utterances from a combined set consisting of the VCTK \cite{vctk}, VCC 2018 \cite{vcc2018}, LibriSpeech \cite{librispeech}, and the English CommonVoice 2020-06 datasets.\footnote{Available under a CC-0 license at \url{https://commonvoice.mozilla.org/en/datasets}.} 
It is trained with the Adam optimizer \cite{adam} for 8 epochs with 8 speakers per batch, and 6 utterances per speaker in each batch.
We start with a learning rate of $4\times10^{-4}$ and adjust it down to $3\times10^{-7}$ in the final epoch.
Embeddings for speakers are precomputed by taking the average embedding over 4 arbitrary utterances for each speaker.

\subsection{Baseline Models}
We train 4 baseline models for comparison, all using the Adam optimizer.
The first three are traditional one-to-one conversion models, consisting of a simple linear model, a DBLSTM model \cite{BDLSTM}, and a UNet model \cite{unet}. 
The final model that we compare to is the AutoVC model, 
which is able to do zero-shot many-to-many conversion (Section~\ref{sec:autovc}). 
We compare AutoVC to StarGAN-ZSVC on all variants of seen/unseen source/target pairings. 
Each network is trained according to the method developed by
Smith \cite{smith:cyclic} by evaluating the decrease in loss every few hundred epochs for different learning rates, and updating the learning rate to correspond to the largest decrease in loss. 
This process is repeated until the validation loss begins to increase, after which training is terminated.

All one-to-one models are trained in the same way, taking the source spectrogram $X_{\text{src}}$ as input and trained with an $L_1$ loss (which we found to produce better results than the $L_2$ loss) to predict the ground-truth target spectrogram $X_{\text{trg}}$. 
The linear model consists of 4 convolutional layers with output channels and kernel sizes of (200, 5), (200, 5), (100, 3), and (1, 3), respectively. 
The DBLSTM model is based on the original 
paper \cite{BDLSTM}, but we do not use any time-alignment techniques (such as dynamic time warping). 
The model consists of 4 stacked bidirectional LSTM layers with a hidden size of 256 and a dropout of 0.3, followed by a final linear projection layer to bring the output dimension back to 80. The network is trained with a batch size of 8.
The UNet model is built based on the structure of XResNet \cite{xresnet} using the method defined in the Fast.ai library \cite{fastai:unet}.

Finally, AutoVC is trained using the same loss function as in the original paper \cite{autovc}. It is trained with a batch size of 4 for 4700 epochs. 
The speaker encoder used is the pretrained encoder $E$ described above. 

\subsection{StarGAN-ZSVC Training}
We train StarGAN-ZSVC using the same Adam optimizer and learning rate scheduling technique of Smith \cite{smith:cyclic}.
Furthermore, we employ several tricks for successfully training GANs: 
(i) gradients in $G$ and $D$ are clipped to have a maximum norm of 1; 
(ii) the discriminator's learning rate is made to always be half of the generators learning rate; 
(iii) the number of iterations training the discriminator versus generator is updated every several hundred epochs to ensure that the discriminator's loss is always roughly a factor of 10 lower than the adversarial term of the generator's loss; and
(iv) dropout with a probability of 0.3 is added to the input of $D$ after the first 3000 epochs (if added earlier it causes artifacts and destabilizes training). 

The loss function used is the same as that of StarGAN-VC2, with the exception that the term $\mathcal{L}_\text{cyc}$ (see Section~\ref{sec:stargan}) is squared in our model, which we found to give superior results. 
We set $a = 1$, and $b=0$ for the LSGAN constants, and $\lambda_{\text{cyc}} = 10$, $\lambda_{\text{id}} = 5$ for loss coefficients, being adjusted downwards during training in the same manner as in \cite{stargan-vc2}. 

\subsection{Evaluation}

We compare converted output spectrograms to their ground-truth target spectrograms on the test dataset using several objective metrics.
To account for different speaking rates, we first use dynamic time warping (DTW) to align the converted spectrogram to the target spectrogram, and then perform comparisons over non-silent regions of the \textit{target} utterance. Non-silent regions are defined as those 80-dimensional spectrogram frames where the mean vector element value is greater than -10dB.

In addition to computing the mean absolute error (MAE) and mean square error (MSE) between spectrograms, we also compute a \textit{cosine similarity} by finding the cosine distance between each 80-dimensional source/target frame pair of the Mel-spectrograms and then computing the mean cosine distance over all non-silent frames (after DTW alignment). 
This metric, denoted as $\cos(\theta)$, gives an additional measure of conversion quality. 

Finally, to quantitatively measure speaker similarity (i.e.\ determining whether the generated spectrogram sounds like the target speaker), we define a new metric using the speaker encoder.
We compute speaker embeddings for the target and output converted spectrograms using the speaker encoder $E$. 
The norm of the difference between these vectors, $||E(X_{\text{src}\rightarrow \text{trg}}) - E(X_{\text{trg}})|| = e_{\text{norm}}$, is then used as a measure of speaker similarity, with a smaller norm corresponding to greater similarity between the converted and target spectrogram.

We also perform a subjective listening test with 12 proficient English speakers to assess how well StarGAN-ZSVC compares to AutoVC across various zero-shot settings.
Each participant rated the naturalness of 144 utterances from 1 (bad) to 5 (excellent) where the utterance order and naming is randomized.
The 144 utterances consist of 8 converted utterances for each seen/unseen source/target speaker pairing, for both AutoVC and StarGAN-ZSVC.
A further 14 utterances are included to find a baseline rating for raw and vocoded audio.
The ratings for each subset are averaged to find a mean opinion score (MOS), which serves as a measure of conversion quality.
