\section{Related work}

We describe related works on pinyin input method and pinyin-enhanced pretrained  models here.

\paragraph{Pinyin Input Method} We describe existing works based on whether the input pinyin is perfect or abbreviated.
% include researches over \textit{perfect} pinyin, \textit{abbreviated} pinyin and pinyin with typos.
A majority of existing works focus on {perfect} pinyin.
Traditional models are typically based on statistical language models~\cite{chen-lee-2000-new} and statistical machine translation~\cite{yang-etal-2012-towards}.
Recent works are usually built with neural network. For example, Moon IME~\cite{huang-etal-2018-moon} integrates  attention-based neural network and an information retrieval module. 
\citet{zhang-etal-2019-open} improves an LSTM-based encoder-decoder model with online vocabulary adaptation.
% Researchers also tried to improve pinyin input method by updating the vocabulary during learning~\cite{zhang-etal-arxiv-tracing,}.
For abbreviated pinyin, 
% Using pinyin abbreviations as input has also been investigated for pinyin input method.
CoCAT~\cite{huang-etal-2015-ijcai-input} uses machine translation technology to reduce the number of the typing letters. 
\citet{huang-zhao-2018-chinese} propose an LSTM-based encoder-decoder approach with the concatenation of context words and abbreviated pinyin as input. Our work differs from existing works in that we are the first one to exploit GPT and verify the pros and cons of GPT in different situations.
In addition, there are some works handling pinyin with typing errors.
% \cite{chen-lee-2000-new,zheng-etal-ijcai-2011-chime,jia-zhao-2014-joint}, we leave this as a future work.
% as one special type of incomplete pinyin.
% However, the evaluation of these methods is not comparable with ours.
% There are works in pinyin input method when a potential pinyin sequence contains typos.
\citet{chen-lee-2000-new} investigate a typing model which handles spelling correction in sentence-based pinyin input method.
% , and a spelling model for English which enables modeless Pinyin input. 
CHIME~\cite{zheng-etal-ijcai-2011-chime} is a error-tolerant Chinese pinyin input method. It finds similar pinyin which will be further ranked with Chinese specific features.
% investigate the problem of converting a pinyin sequence with typing errors to the correct Chinese words.
\citet{jia-zhao-2014-joint} propose a joint graph model to globally optimize the tasks of pinyin input method and typo correction.
We leave error-tolerant pinyin input method as a future work.
% However, in this paper, we will only focus on converting a correct pinyin sequence to Chinese characters.

% \subsection{Specialized Pretrained Language Models}

% Specialized pretrained language models are usually focusing on a specific domain or aiming at solving a well-defined problem.
% These models can be either finetuned from a general-purpose pretrained language model like BERT~\cite{devlin-etal-2019-bert} and GPT~\cite{radford2019language}, or pretrained from scratch by designing new pretraining tasks.
% In recent years, many specialized pretrained language models appeared in public.
% MED-BERT~\cite{medbert} provides pretrained contextualized embeddings for disease prediction.
% CodeBERT~\cite{feng-etal-2020-codebert} learns general-purpose representations that support applications such as natural language codesearch, code documentation generation.
% SentiBERT~\cite{yin-etal-2020-sentibert} can effectively capture compositional sentiment semantics.
% In this paper, we provide an approach constructing a new specialized pretrained language model for pinyin input method.

\paragraph{Pinyin-enhanced Pretrained Models}
Our methodology also relates to pretrained models that use pinyin information.
% Pinyin information has been proven to be useful in pretraining Chinese language models and in Chinese Spelling Correction.
\citet{sun-etal-2021-chinesebert} propose a general-purpose Chinese BERT with new embedding layers to inject pinyin and glyph information of characters.
% pinyin information by constructing a pinyin embedding for each character to handle the heteronym phenomenon in Chinese language.
There are also task-specific BERT models, especially for the task of grammatical error correction since an important type of error is caused by characters pronounced with the same pinyin.
% information is especially important for grammatical error correction, 
% In Chinese Spelling Correction, pinyin is used as extra knowledge when fixing spelling errors. 
\citet{zhang-etal-2021-correcting} add a pinyin embedding layer and learns to predict characters from similarly pronounced candidates.
PLOME~\cite{liu-etal-2021-plome} add two embedding layers implemented with two GRU networks to inject both pinyin and shape of characters, respectively.
% to get phonic embeddings to learn misspelled knowledge on phonic level.
\citet{xu-etal-2021-read} add a hierarchical encoder to inject the pinyin letters at character and sentence levels, and add a ResNet encoder to use graphic features of character image.  
% adopts a phonetic encoder which encodes pinyin~(initial, final, and tone) to address the issue of phonetically similarities in spelling errors. 

% \section{Introduction}

% % Pretrained model whether or not can do the IME?
% Pretrained language models can perform with excellence over a series of natural language tasks, including natural language inference, reading comprehension and natural language generation.


% \begin{figure}[ht]
% \centering
% \includegraphics[width=\linewidth]{pinyin.pdf}
% \caption{Example.} 
% \label{fig:pinyin}
% \end{figure}


% % Challenges in PTC
% Pinyin~(拼音) is considered as an official romanization/latinization representation for Chinese language~\cite{wiki:xxx}.
% Since Chinese is a tonal language, each Chinese character can be mapped to at least one toned Pinyin syllable.
% For Pinyin Input Method, the input sequence usually is a concatenation of untoned syllables.
% Pinyin to Character (PTC) has been an important task for building effective and efficient Chinese Input Method Engines~(IME).
% The task is to convert a Pinyin sequence to Chinese characters.
% There are two main challenges in this task.
% One is ambiguous segmentation resolving when cutting a Pinyin sequence into separate syllables.
% The other is candidate character selection that each syllable can be mapped to.
% However, there are less than 500 syllables but more than 6000 Chinese characters for daily use.
% Whats-more, to save typing cost, users prefer to typing in a hybrid mode combining \textit{pinyin abbreviations}~(first character of each syllable) and syllables.
% All these bring new problems to IME development and evaluation.

% % Limitations in current evaluation approach
% % MIU has its own limitations
% For PTC evaluation, existing explorations use either Character-wise precision and recall or sequence error rate as the evaluation metric.
% In practice, Max Input Unit~(MIU)~\cite{jia-zhao-2013-kyss} has been adopted as the basic sequence unit for the evaluation.
% A MIU means a longest consecutive Chinese character sequence inside a sentence.
% A Pinyin IME is expected to generate a full MIU given its full Pinyin sequence, the generation is considered to be positive if the whole sequence is correct.
% In this case, the Pinyin IME cannot accept either user-confirmed context or abbreviations of Pinyin, which is less practical for newly developed user input habit.

% % We need new benchmarks and evaluation metrics
% The demand for evaluation of variant input style is largely overlooked.

% \textcolor{blue}{Extended MIU // MIU with abbreviation?}

% To explore to what extent that current GPT2-based pretrained language models can perform over PTC, we first adopt a filter-based decoding method over GPT-2 to test the PTC performance.
% Firstly, we compare with existing methods by using MIU-based evaluation over Pinyin IME benchmarks.
% Secondly, we construct a new benchmark supporting user-confirmed context and evaluate GPT2 with different input modes.
% We pass the user-confirmed context to GPT-2 as input, and we filter the predicted token distribution by keeping only those tokens whose Pinyin matches the target Pinyin syllable.
% % Current GPT can not do good enough 
% Through our experiments, we find that GPT-2 performs well using complete syllables, but the performance drops consistently when we switch to the abbreviations of Pinyin syllables.

% % Our approach
% In this paper, we consider a scenario that given a piece of user-confirmed context and a sequence of Pinyin syllables, our model needs to convert these syllables into Chinese characters.
% To fulfill the trend of evolving input habits, we also consider supporting a mixture of different input modes.
% As pointed out earlier, the new input habits may involve usages of abbreviations of Pinyin syllables.
% Our model is trained by randomly switch the input mode from complete Pinyin syllables to full abbreviations of Pinyin syllables.



% % In this paper, we propose a new evaluation metric for PTC and a new benchmark PTCLUE to facilitate general-purpose and well-balanced evaluations for PTC.

% % We don't need to care about how pinyin sequence are segmented at word level, which is critical for word-based conversion algorithms.
% % GPT can make use of syllable-level pinyin to generate words token after token.
