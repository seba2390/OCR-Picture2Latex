\section{Language GPT}
\label{sec:frozen}

In this section, we first introduce the public GPT and then describe our further trained GPT.
% GPT2 further shows its generality in zero-shot settings over a wide range of tasks.

\subsection{GPT in the public}
GPT~\cite{radford2018improving,radford2019language} is a multi-layer transformer decoder pretrained over a large amount of data.
It shows strong ability in natural language inference, question answering, semantic similarity, and text classification. 

For applying GPT to address Chinese NLP tasks, the Chinese Pretrained Language Model (CPM) with 2.6 billion parameters, which is \citep{cpm-v1} trained on 100GB Chinese training data, has demonstrated strong performance. However, CPM is not a character-level model and is not applicable for pinyin input method. We turn to employ a widely adopted GPT2-Chinese~\cite{GPT2-Chinese} as our starting model. CPT2-Chinese is trained on 14GB Chinese training data CLUECorpusSmall~\cite{CLUECorpus2020}, which collected from Chinese news, Wikipedia, on-line forum message, and Consumer comments.
% In this section, we explore how GPT trained over Chinese corpus can be used to the PTC task.
% A straightforward approach is that we pass the user-confirmed context to GPT-2 as input, and we filter the predicted token distribution by keeping only those tokens whose Pinyin matches the target Pinyin syllable.

% To facilitate with evaluation using MIU, we adopt two commonly used corpora The Peopleâ€™s Daily corpus and TouchPal corpus~\cite{zhang-etal-arxiv-tracing}.


\subsection{Our GPT} 
To construct strong baselines, we further trained two kinds of GPT with 6 layers and 12 layers based on 800GB Chinese training data, which consists of news, Wikipedia, and novel texts. The configuration of 12-layer GPT is same as the standard GPT.\footnote{\url{https://huggingface.co/gpt2}} The 6-layer GPT is directly truncated and initialized from the converged 12-layer GPT and is continually trained. We train our 12-layer GPT with a batchsize of 2560 on 32x Tesla V100 GPUs. We adopt the Adam optimizer~\cite{kingma2014adam} and set the learning rate to 1e-5 with a linear warmup scheduler. We run the warmup process for 10k steps and train 100k steps in total. For 6-layer GPT, we train it for 50k steps with the same configuration of 12-layer GPT.

% \paragraph{GPT as a pinyin IME} To customize GPT2 for the task PTC, we need to use the pinyin sequence as a constraint during decoding.
% Given the mapping of pinyin syllables and their candidate characters, we can choose to decode characters from that specific syllable for each step.

% To test to what extent that GPT2 can do over the PTC task, we conduct the following experiments: 
% (1) we use CPM-Distill~\cite{cpm-v1} to do the pinyin-constrained decoding for comparison since it has the similar model size with our pretrained GPT,
% (2) we conduct the pinyin-constrained decoding on our pretrained GPT without finetuning on the PD dataset,
% (3) we finetune our pretrained GPT over PD dataset report evaluation results using differnt beam sizes.

% The detailed comparison of our experiments and related works are listed in Table~\ref{tab:miu} of Section~\ref{sec:pd}.
% The main conclusion we can draw is that a frozen GPT can achieve state of the art performance over the PTC benchmark if perfect pinyin is used as constraints during decoding.