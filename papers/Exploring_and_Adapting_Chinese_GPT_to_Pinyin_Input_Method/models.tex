\section{Models}
\label{sec:models}
In this section, we first introduce standard text-based GPT models adopted in this work (section \ref{sec:gpt}).
Afterwards, we introduce how to extend GPT models for pinyin input method with enriched pinyin context (section \ref{sec:pinyin-context}) and pinyin-constrained training (section \ref{sec:training}), respectively.
% that inje two models we used to incorporate pinyin information.

% \begin{figure*}[ht]
% \centering
% \includegraphics[width=\linewidth]{PinyinGPT.pdf}
% \caption{Model structures for GPT and PinyinGPT. The decoding module is the same for the three models.} 
% \label{fig:pinyingpt}
% \end{figure*}

\subsection{GPT Baselines}
\label{sec:gpt}
% Since the generation of 
% We describe the Chinese GPT models that we use to build baselines for pinyin input method.
% In this work, we generate the output by predicting a sequence of Chinese characters with Character-level Chinese GPT working as the backbone.
In this work, we use character-level Chinese GPT as the backbone.
We describe character-level GPT models in this subsection.
% that work as the backbone of the following models.

We start with a publicly available character-level GPT  \cite{GPT2-Chinese}\footnote{\url{https://github.com/Morizeyao/GPT2-Chinese}}, which we call \textbf{GPT (public)}.
The model has the same configuration as the standard 12-layer GPT\footnote{\url{https://huggingface.co/gpt2}}. 
% Firstly, we use a widely adopted 12-layer character-level GPT2-Chinese~\cite{GPT2-Chinese} as our starting model.
It is trained on the CLUECorpusSmall dataset of 14GB \cite{CLUECorpus2020}, which consists of Chinese news, Wikipedia, online forum message, and consumer comments.
We have tried another well known Chinese pretrained language model called CPM \citep{cpm-v1}, which is
% originally built with 2.6 billion parameters and 
trained on 100GB  data. The vocabulary of CPM contains both Chinese characters and words.\footnote{A Chinese word may consist of multiple Chinese characters. For example, the word ``我们'' (we) includes two characters ``我'' and ``们''. } 
We built a baseline with the CPM model of 12 layers\footnote{\url{https://github.com/TsinghuaAI/CPM-1-Distill}} and forced the generated token to be a Chinese character. 
However, this baseline does not work well on pinyin input method, partly because our character-level decoding is inconsistent with the way how CPM is trained.
It is promising to leverage the advantage of CPM on word-level decoding, and we leave this as a future work. 
% The CPM team also released a distilled version which 
% , 
% However, CPM is a word-level model and our exploration over the 12-layer distilled CPM didn't give us an ideal performance over pinyin input method.

% We 
To build a stronger Chinese GPT baseline, we use GPT (public) as the starting point and further pretrain on a 800GB data crawled by us  that is composed of news, Wikipedia, and novel texts. 
% The configuration of 
The model is trained with a batch size of 2,560 on 32x Tesla V100 GPUs.
% We train our 12-layer GPT with a batch size of 2560 on 32x Tesla V100 GPUs. 
We adopt the Adam optimizer~\cite{kingma2014adam} and set the learning rate to 1e-5 with a linear warmup scheduler. We run the warmup process for 10k steps and train 100k steps in total. 
We call this 12-layer GPT model as \textbf{GPT (ours)}.

% We briefly describe how t
To apply GPT (public) and GPT (ours) to pinyin input method, we use the traditional decoding pipeline of GPT to generate the sequence of Chinese characters in an autoregressive way. 
% The input of the model only contains characters.
After encoding all the context of characters, the model predicts a Chinese character at each time step conditioned on the pinyin.
% of the character to be predicted.
% Considering the characteristic of the task, 
Only Chinese characters pronounced with the same pinyin are legitimate candidates to be predicted.
% , which we call pinyin-constrained decoding in this work.
% add an intuitive constraint to 
% shrink the number of candidate Chinese characters to be predicted through removing the characters whose pinyin is not equal to the input pinyin.
% for each step by only considering characters with the same pinyin syllable. 
Without further clarification, this strategy is used in all the experiments.

% that the character to be predicted should come from a smaller vocabulary in which every candidate character has the same pinyin syllable.

% When we apply these models to pinyin input method, we don't change the model structure but use the pinyin sequence as constraints during decoding.
% Specifically, given the mapping of pinyin syllables and their candidate characters, we choose to decode from the candidate characters for that syllable at each step.
% An overview of the model is shown in Figure~\ref{fig:gpt}.


\begin{figure*}[t]
  \centering
  \includegraphics[width=\textwidth]{pinyinGPT-method.pdf}
  \caption{An illustration of the training process of Pinyin-Concat (top) and Pinyin-Embed (bottom), respectively. The example is same as the instance of s2 from Table \ref{tab:task-definition-example}. }
  \label{fig:method-pinyingpt}
\end{figure*}

\subsection{Incorporating Pinyin Context}
\label{sec:pinyin-context}
% Through our exploration, we empirically find that pinyin information is useful to improve performance of GPT on pinyin input method, especially in the setting that the pinyin sequence contains only abbreviations.
We explore two simple ways to incorporate pinyin information and build two models correspondingly.
The first model uses pinyin information horizontally by concatenating pinyin input to the context of characters.
The second model incorporates pinyin information vertically by adding a pinyin embedding layer at the bottom of GPT. 
% We denote these two methods as \textbf{Pinyin-Embed} model and \textbf{Pinyin-Concat} model, respectively.

\paragraph{PinyinGPT-Concat}
\label{sec:concatenation}
In this model, we append a pinyin sequence to the context of Chinese characters.
In the inference stage, the input has the form of $\mathbf{x} = [w_1,\dots,w_n,\texttt{[SEP]},p_{n+1},\dots,p_{n+k},\allowbreak\texttt{[SEP]} ]$, 
% the 
% ,w_{l+1}, \dots,w_{l+T-1}]$, 
where \texttt{[SEP]} is a special token to separate text and pinyin.
The model largely follows the architecture of the standard GPT.
Since there is one-one relationship between pinyin tokens and generated Chinese characters (i.e., the pronunciation of $w_{n+j}$ is $p_{n+j}$), we adjust the absolute positions of the characters to be generated. We assign the position of $p_{n+j}$ to $w_{n+j}$, expecting the model to learn the alignments between pinyin and target characters.\footnote{On abbreviated pinyin, this strategy could bring 0.3 points in terms of P@5.}
We further expand the vocabulary of the word embedding layer by adding pinyin tokens. 
% have one-to-one,to share the position embedding with its corresponding target character.


In the training stage, given an training instance of $[w_1,\dots,w_n,\texttt{[SEP]},p_{n+1},\dots,p_{n+k},\allowbreak\texttt{[SEP]} ,w_{n+1},\dots,w_{n+k}]$, the model is trained to minimize the following loss function, where $\mathbf{w}_{<n+j}$ stands for the characters before $w_{n+j}$ and $\mathbf{p} = [p_{n+1},\dots,p_{n+k}]$.
% An illustration is given in Figure \ref{fig:method-pinyingpt}. 
\begin{equation}
% \mathcal{L_\text{concat}} = - \sum_{j=1}^k \text{log} \ p(w_{n+j} | h(\tilde{x})_i)
\mathcal{L_\text{concat}} = - \sum_{j=1}^k \text{log} \ p(w_{n+j}|\mathbf{w}_{<n+j}, \mathbf{p})
    % \max_{\theta} \log p_{\theta}(\mathbf{x}) = \sum_{j=1}^{k}\log p_{\theta}(x_t|\mathbf{x}_{<t})
\end{equation}

% 
% We make tiny modifications to the vocabulary  does not change the structure of GPT, but only expand the vocabulary with pinyin syllables. 
% During training stage, the target character sequence will be further appended to the input sequence to calculate the autoregressive language model loss.
% During inference stage, the generation starts conditioned over the context sequence and the pinyin sequence.
% We expand the vocabulary of GPT $\mathcal{V}_w$ with the legitimate pinyin vocabulary $\mathcal{V}_p$.
% We use the special token \texttt{[SEP]} to separate the input text sequence and the pinyin sequence.


% We further use position adjustment to make the pinyin tokens to share the position embedding with its corresponding target character.\footnote{This position alignment strategy brings xxx improvements on ...}
% Our model structure is shown in Figure~\ref{fig:pinyingpt-concat}.
% The model will be trained through optimizing the following equation.
% \begin{equation}
%     \max_{\theta} \log p_{\theta}(\mathbf{x}) = \sum_{t=l+k+1}^{l+k+T}\log p_{\theta}(x_t|\mathbf{x}_{<t})
% \end{equation}


\paragraph{PinyinGPT-Embed}
\label{sec:composition}
The original GPT model includes a word embedding layer and a position embedding layer.
In this model, we add a pinyin embedding layer. 
% To compose pinyin information, we choose to add one pinyin embedding layer to the original GPT.
% It is worth noting that 
The basic idea is to provide the model with the pinyin of the character to be generated next.
% in next time step.
Specifically, the embedding of each character is the sum of the  token embedding of the current character, the position embedding of the current character and the pinyin embedding of the next character. 
When a word~(e.g., numbers, punctuations and symbols) has no corresponding pinyin, we use a special token \texttt{[unk]} to represent it instead.
The training process is similar with the standard GPT, as shown in Figure \ref{fig:method-pinyingpt}.
The loss function is given as follows.
\begin{equation}
% \mathcal{L_\text{concat}} = - \sum_{j=1}^k \text{log} \ p(w_{n+j} | h(\tilde{x})_i)
\mathcal{L_\text{embed}} = - \sum_{j=1}^{n+k} \text{log} \ p(w_j|\mathbf{w}_{<j}, \mathbf{p}_{<j+1})
    % \max_{\theta} \log p_{\theta}(\mathbf{x}) = \sum_{j=1}^{k}\log p_{\theta}(x_t|\mathbf{x}_{<t})
\end{equation}
In the inference stage, we transform the input sequence to the same format.

% \mhcomment{We discussed the tools in Section~\ref{sec:training}, should I move them here?}

% Given a text sequence $\mathbf{x} = [w_0,\dots,w_{T}]$, $w_0$ is the special start token \texttt{[CLS]} used in GPT, and the corresponding pinyin sequence is denoted as $\mathbf{p} = [p_1, \dots, p_{T}]$.
% We perform finetuning over GPT by maximizing the following equation.
% \begin{equation}
%     \max_{\theta} \log p_{\theta}(\mathbf{x}) = \sum_{t=1}^{T}\log p_{\theta}(x_t|\mathbf{x}_{<t}, \mathbf{p}_{\le t})
% \end{equation}

% We also consider one variation of the composition model.
% Rather than change the GPT model, we can first compute the next token's distribution directly from the pinyin embedded information through a feed-forward layer and then add to the logits computed from original GPT, see Figure~\ref{fig:pinyingpt}(b).
% % % (3) \textbf{Stack}: we may also stack more transformer layers over GPT to better fuse the information of both tokens and pinyin.
% The two models are denoted as \textbf{Composition-Bottom} model and \textbf{Composition-Top} model respectively.


\subsection{Pinyin-Constrained Training}
\label{sec:training}
% In standard
We describe training details in this subsection.
% In this section, we describe the loss over pinyin-constrained vocabulary.
In standard GPT, the loss function is computed over the whole vocabulary.
However, this is suboptimal for pinyin input method because the major challenge in the inference stage is how to select the best one from characters pronounced with the same pinyin (as described in the end of section \ref{sec:gpt}).
This leads to inconsistency between training and inference stages.
% As pointed out in Section~\ref{sec:intro}, we use pinyin-constrained decoding during inference.
Therefore, in the training stage, the probability of a character is calculated over characters pronounced with the same pinyin, which is formulated as follows.
% change the loss function to better suit the scenario of pinyin input method.
% We choose to compute.
% The probability for predicting a character $w_i$ with pinyin constraint $p_i$ at step $t$ can be represented by the following equation. 

\begin{equation}
    p(w_i) = \frac{\exp{(g(w_i))}}{\sum_{w_j\in \mathcal{V}_{p_i}}\exp{(g(w_j))}},
\end{equation}
where  $\mathcal{V}_{p_i}$ is the set of Chinese characters whose pinyin is $p_i$ and $g$ is the logit before the softmax layer.
% We call the loss \textbf{Pinyin-Constrained Loss (PC-Loss)}.

