\section{Conclusion}

In this paper, we explore how to adapt pretrained Chinese GPT to pinyin input method.
To begin with, we find that a frozen GPT with decoding conditioned on pinyin can reach state-of-the-art performance on perfect pinyin. 
However, in abbreviated setting, the performance drops by a large gap.
Through our experiments, we find that both context enrichment with pinyin and pinyin-constrained training improve the performance.
% To assist the evaluation of pinyin input method, we also create a dataset with 270k test cases with extensive length configurations covering 15 commonly used news domains.
In the future, we would like to investigate more challenging settings including error-tolerant pinyin input method and mixtures of perfect pinyin and abbreviated pinyin.
