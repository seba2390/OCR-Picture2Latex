


\section{Experiment}


In this section, we show the results 
% how GPT and our models work 
on  pinyin input method over the two settings (i.e., perfect pinyin and abbreviated pinyin).
% We first conduct evaluation for the setting where pinyin inputs are perfect pinyin on a public dataset.
% To further illustrate the strength of different models, we construct a new evaluation dataset using text from 15 domains with each domain consisting of 9 configurations of context length and prediction length.
% We also analyze the trade-off between accuracy and latency for our best model with different transformer layers.


\subsection{Settings}
We describe the two datasets used in the following experiments and the evaluation metric.

\paragraph{PD Dataset} PD dataset~\cite{yang-etal-2012-towards} is a commonly used benchmark dataset for the evaluation of pinyin input method \cite{jia-zhao-2014-joint,zhang-etal-arxiv-tracing,huang-etal-2018-moon,zhang-etal-2019-open}. 
The texts in PD are extracted from the People’s Daily\footnote{\url{http://www.people.com.cn/}} from 1992 to 1998.
It contains 5.04 million segments of consecutive Chinese characters (or Maximum Input Unit in some literature) for training and 2,000 segments for testing.
For each test case, the input pinyin are all perfect pinyin and the context is null.

\paragraph{WD Dataset} Since the PD data includes out-of-date news from 20 years ago and does not support us to study the scenario where the context includes real words, we construct a new dataset called WD.
% from WuDaoCorpora~\cite{YUAN202165}.
% \textcolor{red}{need to say something about some special issues (our target characters don't include number, punc, english chars, etc. but we don't add this constraint to the context, right? Another quesiton is, in our embed model, if a context char is a special token, what is the pinyin embedding for that token?}
% \mhcomment{See below.}
We use the WuDaoCorpora \cite{YUAN202165} that contains 3TB Chinese corpus collected from 822 million Web pages.
Currently, 200GB of the corpus has been made publicly available~\footnote{\url{https://resource.wudaoai.cn/home}}.
We randomly select 15 domains from WuDaoCorpora.
For each domain, we first use an off-the-shelf Chinese segmentation toolkit \cite{texsmart2020} to segment the documents into sentences.
Then we automatically obtain the perfect pinyin and abbreviated pinyin of characters with pinyin converting tools.
For each sentence, we randomly choose a context with a range from 0-3, 4-9 and 10+ words.
Consecutively, we choose the target to be 1-3, 4-9 or 10+ words, respectively.
It's further required that the target should be continuous characters that each has its own pinyin.
We call each context-target length tuple like (4-9, 10+) as an evaluation configuration.
For each configuration, we sample 2,000 test cases.
In total, there are 9 configurations of 18,000 cases for each domain.  The whole dataset consists of 270,000 examples. 
We investigate extremely long target lengths for the purpose of research that these configurations may not appear in real cases.
All the instances in the WD dataset are only used for evaluation. 


\paragraph{Evaluation Metric} 
We use precision at top-$K$ as the evaluation metric, which is widely adopted in the literature
% \textcolor{red}{add some citations here}
~\cite{jia-zhao-2014-joint,zhang-etal-arxiv-tracing,zhang-etal-2019-open}. 
It measures if the ground truth exists in the top-$K$ generated results.
Some existing works also use keystroke-based metrics~\cite{jia-zhao-2013-kyss,huang-etal-2015-ijcai-input} and human evaluation, which we don't use in this work because the evaluation process is more complex and time-consuming.

\paragraph{Other Settings} 

We train both PinyinGPT models with the training data of GPT (ours).
% The corpus for the training will be the same as that used for pretraining our own GPT.
To preprocess the corpus, we use a public library \textit{pypinyin}\footnote{\url{https://github.com/mozillazg/python-pinyin}} to get the pinyin of Chinese characters.\footnote{
% The library is very helpful in labelling pinyin for most commonly used words.
If there are heteronym issues, we further verify them with an online dictionary ZDic (\url{https://www.zdic.net/}).}
We initialize both PinyinGPT models with GPT (ours).
Both models are trained  for 100k steps on 32 GPUs of NVIDIA V100 Tensor Core with a bach size of 25,000.
The learning rate is 5e-5.
% Each training case is sampled from the same corpus used for our pretraining of GPT.
We maintain a maximum of 128 tokens for every training example.
We use a probability of 50\% to sample a target sequence with less than 5 words, otherwise we randomly sample a target sequence with 6 to 25 words. 
During inference stage, we use beam search with a beam size of 16 for text generation.


\subsection{Results on Perfect Pinyin}
 We report results on the PD dataset~\cite{yang-etal-2012-towards}.
%  , which is a commonly used benchmark dataset for the evaluation of pinyin input method \cite{jia-zhao-2014-joint,zhang-etal-arxiv-tracing,huang-etal-2018-moon,zhang-etal-2019-open}. % The texts in PD are extracted from the People’s Daily\footnote{\url{http://www.people.com.cn/}} from 1992 to 1998.
% It contains 5.04 million segments of consecutive Chinese characters (or Maximum Input Unit in some literature) for training and 2,000 segments for testing.
% For each test case, the input pinyin are all perfect pinyin and the context is null.
We use pinyin-constraint training in all configurations and train PinyinGPT models with different pinyin vocabularies for perfect pinyin and abbreviated pinyin, respectively.
We compare with the following baselines. 
\begin{itemize}
    \item Google IME is a commercial Chinese IME which provides a debuggable API.
    \item On-OMWA~\cite{zhang-etal-arxiv-tracing} is an online model for word acquisition which adaptively learns new words for Chinese IME. 
    % \textcolor{red}{rewrite this sentence.}
    \item On-P2C~\cite{zhang-etal-2019-open} is a neural pinyin-to-Chinese character conversion model, which is augmented by an online updated vocabulary to support open vocabulary learning.
\end{itemize}

% \paragraph{Resul}
In Table~\ref{tab:perfect}, the first group (top) shows the results of the aforementioned baselines, which are directly extracted from On-P2C~\cite{zhang-etal-2019-open}.
The bottom group shows the performance of GPT~(public) and GPT~(ours) with 
% 
% In the second part, we show how 
frozen parameters.
% perform through pinyin-constrained decoding.
% At last, we list results of our modified models PinyinGPT-Embed and PinyinGPT-Concat.
% The PD results are reported after finetuning over the PD training data.
We can find that GPT~(public) achieves comparative performance with existing systems in terms of P@5 and P@10.
After being trained with a larger corpus, GPT~(ours) surpasses all the baseline models in terms of all metrics.
It is worth noting that existing baselines are supervised models that are fine-tuned on training instances. 
The results demonstrate the effectiveness of GPT models pretrained on vast amount of texts.
%  us that a frozen GPT with pinyin-constrained decoding is already powerful in pinyin input method task.
% , GPT~(public) achieves comparative performance with current state of the art model.
% After training with a larger corpus, GPT~(ours) surpasses all the baseline models in terms of all Top$K$ metrics.

\begin{table}[t]\centering
% \small
\begin{tabular}{lrrrr}\toprule
Model&P@1 &P@5 &P@10 \\\midrule
Google IME  & 70.90 & 78.30 &82.30 \\
On-OMWA  & 64.40 &72.90 &77.90 \\
On-P2C  & 71.30 &80.50 &81.30\\\midrule
GPT (public)  &  67.35& 79.95& 81.60\\
GPT (ours)  & \textbf{73.15} & \textbf{84.10} & \textbf{85.45}  \\
% GPT (public) &  &22.22 &29.99 &31.48 \\
% GPT (ours) & &26.90 &35.56 &37.03 \\\midrule
% PinyinGPT &  & & & \\
% \makebox[0.5cm][r]{}Compose & Y &26.95 &35.56 &37.06 \\
% \makebox[0.5cm][r]{}Compose & N &23.73 &31.80 &33.33 \\
% \makebox[0.5cm][r]{}Concat & Y &26.91 &35.56 &37.03 \\
% \makebox[0.5cm][r]{}Concat & N &\textbf{27.75} &\textbf{40.66} &\textbf{44.20} \\
\bottomrule
\end{tabular}
\caption{Comparison with different methods over PD using perfect pinyin. Each score is averaged over all the domains and context-target length configurations.}
\label{tab:perfect}
\end{table}



\subsection{Results on Abbreviated Pinyin}


\begin{table*}[t]
\centering
\begin{tabular}{lcrrrrrrrrr}\toprule
\multirow{3}{*}{Model} &\multirow{2}{*}{Fix GPT} & &\multicolumn{3}{c}{Perfect Pinyin} & &\multicolumn{3}{c}{Abbreviated Pinyin} \\\cmidrule{4-6}\cmidrule{8-10}
& Parameters& &P@1 &P@5 &P@10 & &P@1 &P@5 &P@10 \\\midrule
GPT (public) & & &76.55 &87.07 &88.58 & &22.22 &29.99 &31.48 \\
GPT (ours) & & &80.22 &90.20 &91.09 & &26.90 &35.56 &37.03 \\\midrule
PinyinGPT-Embed &Y & &72.41 &83.44 &84.78 & &26.95 &35.56 &37.06 \\
PinyinGPT-Embed &N & &69.34 &81.54 &82.99 & &23.73 &31.80 &33.33 \\
PinyinGPT-Concat &Y & &\textbf{80.24} &90.21 &91.10 & &26.91 &35.56 &37.03 \\
PinyinGPT-Concat &N & &78.12 &\textbf{90.38} &\textbf{92.06} & &\textbf{27.75} &\textbf{40.66} &\textbf{44.20} \\
\bottomrule
\end{tabular}
% \end{small}
% \end{adjustbox}
\caption{Overall results on WD dataset
% \textcolor{red}{用dataset 不用benchmark. 自己建的数据不应该叫benchmark.}
for perfect pinyin and  abbreviated pinyin, respectively.}
\label{tab:wd}
\end{table*}

% \subsection{Model Analysis}



% \subsection{Datasets}

% In this subsection, we describe the two datasets used in the following experiments and the evaluation metric.
% % : \textbf{PD} and \textbf{WD}.

% % \begin{table}[ht]
% % \centering
% % % \begin{adjustbox}{max width=\linewidth}
% % % \begin{small}
% % % \small
% % % \begin{tabular}{llcrr}\toprule
% % % & Domain &Train &Test \\\midrule
% % % PD & - &5.04M &2,000 \\\midrule
% % % \multirow{15}{*}{WD} &Entertainment~(娱乐) &- &18,000 \\
% % % &Automobile~(汽车) &- &18,000 \\
% % % &Technology~(科技) &- &18,000 \\
% % % &Education~(教育) &- &18,000 \\
% % % &Agriculture~(农业) &- &18,000 \\
% % % &Economy~(经济) &- &18,000 \\
% % % &Games~(游戏) &- &18,000 \\
% % % &Culture~(文化) &- &18,000 \\
% % % &Sports~(体育) &- &18,000 \\
% % % &International~(国际) &- &18,000 \\
% % % &Society~(社会) &- &18,000 \\
% % % &Military~(军事) &- &18,000 \\
% % % &Real Estate~(房产) &- &18,000 \\
% % % &Medical~(医疗) &- &18,000 \\
% % % &Finance~(财经) &- &18,000 \\
% % \begin{tabular}{lcc}\toprule
% % &PD &WD \\\midrule
% % Train &5.04M &- \\
% % Test &2,000 &270,000 \\
% % \bottomrule
% % \end{tabular}
% % % \end{small}
% % % \end{adjustbox}
% % \caption{Statistics of train and test set for benchmarks \textbf{PD} and \textbf{WD}.}
% % \label{tab:data}
% % \end{table}

% \paragraph{PD Dataset} The PD dataset~\cite{yang-etal-2012-towards} is a commonly used benchmark dataset for the evaluation of pinyin input method \cite{jia-zhao-2014-joint,zhang-etal-arxiv-tracing,huang-etal-2018-moon,zhang-etal-2019-open}. The texts in PD are extracted from the People’s Daily\footnote{\url{http://www.people.com.cn/}} from 1992 to 1998.
% It contains 5.04 million segments of consecutive Chinese characters (or Maximum Input Unit in some literature) for training and 2,000 segments for testing.
% For each test case, the input pinyin are all perfect pinyin and the context is null.



% Before talking about our new benchmark dataset, we first introduce one of the latest large-scale corpus WuDaoCorpora~\cite{YUAN202165}.
% WuDaoCorpora contains 3TB Chinese corpus collected from 822 million Web pages.
% Currently, 200GB of the corpus has been made publicly available online~\footnote{\url{https://resource.wudaoai.cn/home}}.
% Each record of the corpus has an unique key and a data type field.
% The data type field indicates the source domain of the record.

% \paragraph{Setting}

% \mhadd{It's worth noting that when a word~(numbers, punctuation and symbols) has no corresponding pinyin, we use a special token \texttt{[UNK]} to represent it instead.}
% \mhadd{}


\begin{table*}[t]
\centering
% \begin{minipage}[t]{\linewidth}\centering
% % \vspace{0.5cm}
% % \small
% % \begin{adjustbox}{max width=\linewidth}
% \begin{tabular}{lcrrrrrrrrr}\toprule
% \multirow{3}{*}{Model} &\multirow{2}{*}{Fix GPT} & &\multicolumn{3}{c}{Perfect Pinyin} & &\multicolumn{3}{c}{Abbreviated Pinyin} \\\cmidrule{4-6}\cmidrule{8-10}
% & Parameters& &P@1 &P@5 &P@10 & &P@1 &P@5 &P@10 \\\midrule
% GPT (public) & & &76.55 &87.07 &88.58 & &22.22 &29.99 &31.48 \\
% GPT (ours) & & &80.22 &90.20 &91.09 & &26.90 &35.56 &37.03 \\\midrule
% PinyinGPT-Embed &Y & &72.41 &83.44 &84.78 & &26.95 &35.56 &37.06 \\
% PinyinGPT-Embed &N & &69.34 &81.54 &82.99 & &23.73 &31.80 &33.33 \\
% PinyinGPT-Concat &Y & &\textbf{80.24} &90.21 &91.10 & &26.91 &35.56 &37.03 \\
% PinyinGPT-Concat &N & &78.12 &\textbf{90.38} &\textbf{92.06} & &\textbf{27.75} &\textbf{40.66} &\textbf{44.20} \\
% \bottomrule
% \end{tabular}
% % \end{small}
% % \end{adjustbox}
% \caption{Overall experiment results on WD benchmark using perfect pinyin and  abbreviated pinyin.}
% \label{tab:wd}
% \end{minipage}
% \begin{minipage}[t]{\linewidth}\centering
% \vspace{0.5cm}
\small
\begin{tabular}{llrrrrrrrrrrrrr}\toprule
 &\multirow{3}{*}{Model}& &\multicolumn{3}{c}{1-3} & &\multicolumn{3}{c}{4-9} & &\multicolumn{3}{c}{10+} \\\cmidrule{4-6}\cmidrule{8-10}\cmidrule{12-14}
& & &P@1 &P@5 &P@10 & &P@1 &P@5 &P@10 & &P@1 &P@5 &P@10 \\\midrule
\multirow{2}{*}{0-3} &GPT~(ours) & &30.11 &42.27 &45.25 & &13.33 &18.24 &18.99 & &4.16 &5.86 &6.00 \\
&PinyinGPT-Concat & &\textbf{31.72} &\textbf{48.09} &\textbf{53.94} &\textbf{} &\textbf{15.21} &\textbf{24.39} &\textbf{26.94} &\textbf{} &\textbf{5.58} &\textbf{9.22} &\textbf{10.09} \\\midrule
\multirow{2}{*}{4-9} &GPT~(ours) & &49.83 &65.03 &67.96 & &25.53 &34.48 &35.89 & &9.38 &12.70 &13.03 \\
&PinyinGPT-Concat & &\textbf{50.78} &\textbf{70.11} &\textbf{75.58} &\textbf{} &\textbf{26.44} &\textbf{41.51} &\textbf{45.52} &\textbf{} &\textbf{10.20} &\textbf{17.02} &\textbf{18.80} \\\midrule
\multirow{2}{*}{10+} &GPT~(ours) & &59.39 &75.00 &77.60 & &\textbf{35.42} &46.32 &47.94 & &\textbf{14.96} &20.11 &20.63 \\
&PinyinGPT-Concat & &\textbf{59.89} &\textbf{78.81} &\textbf{83.33} &\textbf{} &34.99 &\textbf{51.99} &\textbf{56.62} &\textbf{} &14.93 &\textbf{24.78} &\textbf{27.03} \\
\bottomrule
\end{tabular}
% \end{adjustbox}
\caption{Results of different context-target configurations over WD for abbreviated pinyin. The first column and top row stand for context length range and  target length range, respectively. }
\label{tab:length}
% \end{minipage}
\end{table*}


% \paragraph{Evaluation Metric} 
% We use 
% % Following others, we report 
% precision at top-$K$ as the evaluation metric, which is widely adopted in literature
% % \textcolor{red}{add some citations here}
% ~\cite{jia-zhao-2014-joint,zhang-etal-arxiv-tracing,zhang-etal-2019-open}. 
% It measures if the ground truth exists in the top-$K$ generated results.
% Some existing works also use
% % Sequence prediction accuracy is one of the most widely used evaluation metric in the pinyin input method task.
% % It means the ratio of fully correctly converted sequence on the whole evaluation set.
% % Considering pinyin input method usually returns a ranking list, we also report top-$K$ accuracy following other pinyin IME methods, where $K\in\{1, 5, 10\}$, denoted as P@1, P@5, P@10 respectively.
% % There are also 
% keystroke-based metrics like KySS~\cite{jia-zhao-2013-kyss} and plain keystrokes~\cite{huang-etal-2015-ijcai-input}
% % \textcolor{red}{can you add one more citation here?}, 
% which we don't use in this work because the evaluation process is more complex.
% % that we don't use in this paper.



% \subsection{Frozen GPT on Perfect Pinyin}
% \label{sec:pd}


% To show how our model works on the task, we also report results over the new domain-aware benchmark of both frozen GPT and our modified GPT-based models.

% Besides the commonly used benchmark dataset, we construct a domain-aware evaluation benchmark which covers 15 domains.

% To facilitate the evaluation of the PTC task in new scenarios like contextualized settings, domain-related settings and context-prediction-ratio settings, we propose to create a new benchmark that contains various domains and different context-prediction ratios.


% \paragraph{TouchPal} The corpus consists of user chat history collected by one of the leading IME provider TouchPal\footnote{\url{http://www.touchpal.com}}.
% There are 689K MIUs in the corpus and 2k MIUs in the test set.
% TouchPal has been used in evaluation for pinyin sequence with complete syllables~\cite{zhang-etal-arxiv-tracing,zhang-etal-2019-open}.

% \paragraph{Douban Conversation} \citet{wu-etal-2017-sequential} constructed the Douban Conversation (DC) corpus using dyadic dialogues longer than 2 turns from Douban group.

% We first compare performance of GPT-based models with methods proposed by others over a classic evaluation dataset using perfect pinyin.


% We list the evaluation results using perfect pinyin on the PD benchmark in Table~\ref{tab:perfect}.
% In the first part of the table, we show performance of several traditional methods on the PD benchmark:

% Our experiments over PD indicate that performance can be further boosted by finetuning on the training data of a similar source with the evaluation dataset.
% From results on WD, we can also see that the fixed-GPT baseline is very competitive.
% Therefore, when applying GPT to the pinyin input method, GPT with constrained decoding is a good candidate for the setting of perfect pinyin.


% \subsection{Results in Different Configurations}
% \label{sec:abbr}

In this section, we report results for both perfect pinyin and abbreviated pinyin on WD. 

In Table~\ref{tab:wd}, we list the overall experiment results of two GPT baselines as well as our PinyinGPT models.
We have several findings based on the results.
First, from each row, we can see that there is a drastic performance drop for all models.
% for GPT.
The reason is that each abbreviated pinyin can be mapped to a large amount of candidate characters, so that the problem is more challenging compared to perfect pinyin.
We also believe that the evaluation metric of P@1 might be too strict for abbreviated pinyin because sometimes the top predictions might be correct (as reflected in Figure \ref{fig:case-pinyingpt}) even though they may be different from the ground truth.
Second, adding pinyin information to GPT obtains limited improvement on perfect pinyin, but boosts the abbreviated setting by 5 points on P@5 and 7 points on P@10, respectively.
Third, concatenating pinyin context horizontally is better than adding pinyin embedding vertically.
% , \textcolor{red}{recommending an easy implementation}.
Last, fine-tuning all the parameters performs better than keeping the parameters of GPT fixed.




\subsection{Model Analysis: Ablation Study}
In this section, we conduct experiments to understand the importance of pinyin context and pinyin-constrained training.
Results are given in Figure~\ref{fig:ablation}.
The baseline model is GPT~(ours).
The model \emph{+ Pinyin Context} means that we concatenate pinyin context (i.e., PinyinGPT-Concat) and learn over the whole vocabulary.
The model \emph{+ Pinyin Context + PC-LOSS} means that we use both pinyin context and  pinyin-constrained training.
The figure shows that taking pinyin as extra context works well to improve results in terms of P@5 and P@10.
When the two components are adopted, the performance is further improved.

\begin{figure}[!ht]
\centering
\includegraphics[width=\linewidth]{ablation.pdf}
\caption{Ablation study for concatenating pinyin context and pinyin-constrained training.}
\label{fig:ablation}
\end{figure}


\begin{figure*}[t]
  \centering
  \includegraphics[width=\textwidth]{pinyingpt-case-study.pdf}
  \caption{Case study for GPT~(ours) and PinyinGPT-Concat in both perfect pinyin and abbreviated pinyin.}
  \label{fig:case-pinyingpt}
\end{figure*}


\subsection{Model Analysis: Context-Target Length}
To analyze how context length and target length affect performance, we aggregate experiment results to form a matrix of accuracy for each configuration in Table~\ref{tab:length}.
Each score is averaged over all the domains. 
From each column, we can see that longer context benefits both GPT and our model in pinyin input method, which verifies the power of context understanding ability of GPT models.
An interesting finding is that, when the context is long enough (e.g., 10+), adding pinyin does not help improve the P@1. 



\subsection{Model Analysis: Case Study}

We list three cases in Figure~\ref{fig:case-pinyingpt} to compare model outputs produced by GPT~(ours) and PinyinGPT-Concat.
The first case shows that, given perfect pinyin as the input, both GPT~(ours) and PinyinGPT-Concat make the correct predictions.
In the second case, abbreviated pinyin is given as the input. 
PinyinGPT-Concat makes the correct prediction while the prediction of GPT~(ours) does not fit to the context well.
In Case 3, even if PinyinGPT-Concat ranks the ground truth as the second best, the top 1 prediction still makes much sense and fit well with the context.
In all cases, GPT~(ours) usually generate predictions which are grammatically sound but semantically inappropriate.

\subsection{Model Analysis:  Domains}
In this subsection, we analyze how performance differs with respect to domains.
We put the full table over all domains in the Appendix and sample six domains for illustration in Table~\ref{tab:domain-sample}. 
The table shows that PinyinGPT-Concat achieves consistent improvement over GPT on all domains.
We also find that the absolute scores vary a lot across domains.
% We suspect this is due to the texts on some domains are less predictable than others.
This reflects different predictability for texts on different domains. 
For example, the P@10 score of the  \emph{Culture}  domain is 16 points lower than the \emph{Medical} domain.
In the Medical domain, the texts contain plenty of descriptions of symptoms and instructions of medicines, which are somehow canonically used.
While in the Culture domain, the texts are less constrained and have more variations.


\begin{table*}[!htp]
\centering
\small
\begin{tabular}{lrrrrrrrrrrrrr}\toprule
\multirow{3}{*}{Model} & &\multicolumn{3}{c}{Games} & &\multicolumn{3}{c}{Culture} & &\multicolumn{3}{c}{Sports} \\\cmidrule{3-5}\cmidrule{7-9}\cmidrule{11-13}
& &P@1 &P@5 &P@10 & &P@1 &P@5 &P@10 & &P@1 &P@5 &P@10 \\\midrule
GPT (ours) & &24.04 &32.78 &34.23 & &21.86 &29.33 &30.94 & &28.54 &37.13 &38.69 \\
PinyinGPT-Concat & &\textbf{25.78} &\textbf{38.26} &\textbf{41.89} &\textbf{} &\textbf{22.10} &\textbf{33.33} &\textbf{36.72} & &\textbf{29.81} &\textbf{43.56} &\textbf{46.95} \\
& & & & & & & & & & & & \\
& &\multicolumn{3}{c}{Real Estate} & &\multicolumn{3}{c}{Medical} & &\multicolumn{3}{c}{Finance} \\\cmidrule{3-5}\cmidrule{7-9}\cmidrule{11-13}
& &P@1 &P@5 &P@10 & &P@1 &P@5 &P@10 & &P@1 &P@5 &P@10 \\\midrule
GPT (ours) & &26.53 &35.27 &36.74 & &33.59 &43.54 &44.93 & &29.00 &37.24 &38.47 \\
PinyinGPT-Concat & &\textbf{27.28} &\textbf{40.16} &\textbf{43.86} &\textbf{} &\textbf{34.76} &\textbf{49.28} &\textbf{52.56} &\textbf{} &\textbf{29.17} &\textbf{42.17} &\textbf{45.52} \\
\bottomrule
\end{tabular}
\caption{Results of 6 sample domains over WD using abbreviated pinyin. Each score is averaged over all the context-target length configurations. The table of all 15 domains is attached in the Appendix.}
\label{tab:domain-sample}
\end{table*} 

% \subsection{Trade-off between Accuracy and Latency}
\subsection{Model Analysis: Accuracy versus Latency}
% pinyin IME, as a user-oriented application, requires not only high accuracy but also instant reaction.
% The inference time of PinyinGPT is another metric that we cannot ignore when using it for pinyin input method.
Considering pinyin input method requires both accuracy and efficiency, we further train a 6-layer GPT to investigate the trade-off.
Our 6-layer GPT is directly truncated and initialized from the 12-layer GPT and is continually trained for 50k steps with the same configuration of 12-layer GPT.


The evaluation is conducted over the 9 configurations of context-target length and averaged across all domains.
Specifically, each configuration is inferred using a data center GPU NVIDIA V100 Tensor Core, and the GPU is fully occupied by one model.
The beam size is set to be 16.
% The inference time is averaged over the evaluations of all 15 domains.
We report the average inference time in millisecond as well as accuracy in terms of P@$K$ of PinyinGPT-Concat. 
%  over WD using abbreviated pinyin, each score is averaged over all the domains.
% The field \emph{Time (ms)} is the average inference time . 
Table~\ref{tab:trade-off-4-9} is the result for the configuration (4-9, 4-9).
The table shows that the inference time of the model with 6-layer transformer is almost 30\% faster than that with 12-layer.
However, the performance of the 6-layer model drops consistently in the abbreviated setting.
We also list the experiment results for all configurations in the Appendix.
We recommend readers to select models in a more cost-effective way based on their requirements.

\begin{table}[!htp]
\centering
% \begin{adjustbox}{max width=\linewidth}
% \small
\begin{tabular}{lcc}\toprule
Model &Time (ms) &P@5 \\\midrule
GPT~(ours, 6L) & 94 &27.45 \\
GPT~(ours, 12L) & 142 &34.48 \\
PinyinGPT-Concat~(6L) &94 &32.70 \\
PinyinGPT-Concat~(12L) &145 &41.51 \\
\bottomrule
\end{tabular}
% \end{adjustbox}
\caption{Average inference time for one instance and the overall P@5 for the configuration of (4-9, 4-9). 
% The PinyinGPT-Concat models are built with 6-layer and 12-layer Transformers, respectively.
}
\label{tab:trade-off-4-9}
\end{table}
