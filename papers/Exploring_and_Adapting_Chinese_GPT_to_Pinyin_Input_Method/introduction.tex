\vspace{0.01cm}
\section{Introduction}
\label{sec:intro}
GPT~\cite{radford2018improving,radford2019language} is a Transformer-based \cite{vaswani2017attention} language model that predicts tokens in an autoregressive manner.
With a generic model architecture and the availability of vast web text data, GPT has been successfully developed for English, Chinese~\cite{GPT2-Chinese,cpm-v1}, and many other languages.
It shows extraordinary ability to generate fluent sentences and has been successfully applied to a wide range of natural language generation tasks.
However, it remains unexplored to what extent GPT handles Chinese pinyin input method\footnote{\url{https://en.wikipedia.org/wiki/Pinyin_input_method}}, 
which is used by hundreds of millions people when they enter Chinese characters on computers and cellphones.

Pinyin input method allows users to enter Chinese characters based on their pronunciations. 
Given a pinyin\footnote{\url{https://en.wikipedia.org/wiki/Pinyin}} as the input, pinyin input method returns a list of Chinese characters pronounced with that pinyin.
Fundamental elements of pinyin include initials (声母) and finals (韵母). In most cases, a Chinese character is spelled with one initial followed by one final.
For example, as shown in Table~\ref{tab:intro-example}, the initial and final for the Chinese character ``我 \ (me)''  are \texttt{w} and \texttt{o}, respectively.  
\begin{table}[t]
\centering
% \small
\begin{tabular}{c|c|c|c}
% \hline
\toprule
\textbf{ Character} & \textbf{Perfect Pinyin} &\textbf{Initial} & \textbf{Final}\\
% \hline
\midrule
我& \texttt{wo} & \texttt{w} & \texttt{o} \\
们& \texttt{men} & \texttt{m} & \texttt{en} \\
% \hline
\bottomrule
\end{tabular}
\caption{Examples of initials and finals for Chinese characters ``我们 \ (we)''.}
\label{tab:intro-example}
\end{table}
People may enter \textbf{perfect pinyin} (e.g., ``\texttt{wo men}'' for ``我们''), where initials and finals of all Chinese characters are entered. 
There are about 420 perfect pinyin in common use.
% \textcolor{red}{how many perfect pinyin in total?}
% \mhcomment{There is no a standard number for the total perfect pinyin. Generally speaking, about 420. In our case, we used 425.}
Sometimes, especially when multiple Chinese characters are entered at once, people may use \textbf{abbreviated pinyin} by only entering the initials of characters (e.g.,  ``\texttt{w m}'' for ``我们'').
% For example, the 
% The problem of pinyin input method is to convert pinyin,
% Pinyin is the standard romanization system for Chinese
% which is 

% Pinyin is a sequence of . We don't consider tones of pinyin in this work because tone is not considered as a part of the input of common Chinese input methods.

\begin{table*}[ht]
\centering
% \small
\begin{tabular}{c|l|c|c|c}
% \hline
\toprule
Id&\textbf{Context of Characters} & \textbf{Input Pinyin} & \textbf{Target} & \textbf{Pinyin Type}\\
% \hline
\midrule
s1&我下周有时间，除了& \texttt{li bai yi you dian shi} & {礼拜一有点事} & Perfect \\
s2&我下周有时间，除了& \texttt{l b y y d s} & {礼拜一有点事} & Abbreviated\\
s3&老板帮我解决了难题，& \texttt{l b y y d s} & {老板永远滴神} & Abbreviated\\
% \hline
\bottomrule
\end{tabular}
\caption{Illustrative examples of the task of pinyin input method with perfect pinyin and abbreviated pinyin. In s3, the input pinyin ``\texttt{l b y y d s}'' is the abbreviation of ``\texttt{lao ban yong yuan di shen}''. The translations of s1 and s3 are ``I am free next week except for the next Monday.'' and ``Boss helps me overcome the obstacle. You are the greatest of all time.'', respectively. }
\label{tab:task-definition-example}
\end{table*}
This work, to the best of our knowledge, is the first one to explore the use of Chinese GPT for pinyin input method. 
We start by testing the performance of a frozen GPT.
In this setting, we fix the parameters of GPT and predict Chinese characters from left to right in an autoregressive manner. 
At each time step, only characters pronounced with the same pinyin are legitimate candidates to be predicted.
% , which we call pinyin-constrained decoding.
% we  use pinyin as a constraint to only predict Chinese characters pronounced with the same pinyin. 
We find that, when the input is perfect pinyin, a frozen GPT performs comparably to state-of-the-art systems on the benchmark dataset \cite{yang-etal-2012-towards}.
However, when the input is abbreviated pinyin with only initials of characters, the performance of GPT has a drastic drop.
A major reason is that an abbreviated pinyin maps to many perfect pinyin. 
For example, the initial ``\texttt{w}'' can be the abbreviation for ``\texttt{wo}'', ``\texttt{wei}'', ``\texttt{wang}'', ``\texttt{wai}'', ``\texttt{wu}'',  etc. 
This would lead to exponentially larger number of legitimate candidates of Chinese characters.
% to be generated.
% of characters 
% for an abbreviated pinyin grows exponentially because it 
% include more characters.
% whose abbreviated pinyin is as same as the input.
% abbreviation.
% is ththe candidates for \texttt{w} include characters of many perfect pinyin 
We mitigate this problem by incorporating pinyin information from two directions.
% as additional contexts. 
% Our intuition is that knowledge in pinyin can reduce the search space and benefit the decoding process.
% incorporate
% To this end, we explore the two kinds of methods, composition method and concatenation method, to incorporate pinyin information into GPT.
% For composition, we directly compose pinyin information with each input character either at the bottom embeddings of GPT or on top of GPT.
% For concatenation, we treat the pinyin sequence as part of the input sequence.
% We explore two directions to incorporate pinyin information. 
One is to enrich the input by adding pinyin as additional context.
% input of the model, 
The other is learning over pinyin-constrained vocabulary, which enhances the model's ability to distinguish between Chinese characters pronounced with the same pinyin. 
% The first model concatenates pinyin as additional context of the input and uses the architecture of GPT with an enlarged vocabulary. 
% The second model slightly changes the model architecture by adding a pinyin embedding layer.
% (1) pinyin without changing GPT's structure and (2) adding a pinyin embedding layer to GPT, (2) 

To further facilitate the research on pinyin input method, we construct a new dataset based on the WuDaoCorpora~\cite{YUAN202165}.
Our dataset includes 270K instances from 15 commonly used news domains.
To evaluate towards multiple facets, the dataset covers instances with different numbers of context characters and pinyin.
% lengths and target length.
% We expect the new benchmark to offer insights on comparing different methods from multiple perspectives.
From our experiment results, we have these key findings:
\begin{enumerate}
    \item On {perfect pinyin}, frozen  GPT achieves state-of-the-art results. 
    % Our model enhanced with pinyin information achieves marginal improvements.
    \item On {abbreviated pinyin}, the performance of frozen GPT drops drastically. 
    Context enrichment with pinyin and  pinyin-constrained training both improve the performance.
    % Our pinyin-enhanced model achieves consistent improvements in all domains.
    \item The performance of GPT-based models increases as the context of Chinese characters becomes longer.
    % \item Both the incorporation of pinyin context and learning over pinyin-constrained vocabulary improve the accuracy.
\end{enumerate}


% To sum up, we make the following contributions:
% (1) We verify that a frozen GPT with pinyin-constrained decoding achieves state-of-the-art results on \textit{perfect pinyin}, 
% (2) We construct a large-scale up-to-date evaluation dataset for pinyin input method covering a wide range of domains, and we further split them into different configurations according to context length and prediction length to assist detailed analysis of pinyin input methods.
% (3) We conduct experiments to compare several simple methods to incorporate pinyin information and prove that concatenation is effective to achieve better results over both \textit{perfect pinyin} and \textit{abbreviated pinyin}.

