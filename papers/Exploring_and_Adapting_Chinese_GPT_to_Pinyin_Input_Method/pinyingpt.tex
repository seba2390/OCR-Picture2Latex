\section{PinyinGPT}

\begin{figure*}[ht]
\centering
\includegraphics[width=\linewidth]{PinyinGPT.pdf}
\caption{Model.} 
\label{fig:pinyingpt}
\end{figure*}

% From Section~\ref{sec:frozen}, we know that plain GPT is already powerful over the PTC task with constrained decoding using perfect pinyin syllables.
% However, there's an increasing trend that users prefer to type abbreviations of pinyin.
% Suppose we use pinyin abbreviations rather than perfect syllables as the pinyin input, can GPT still achieve impressive performance?
% % To make pinyin IME more effective and efficient, typing shorter pinyin is a demand from users' perspective.
% % pinyin abbreviations have been investigated as inperfect input in some research~\cite{huang-etal-2018-moon}.
% % In consistent with them, we also define abbreviations of pinyin as the first character of each syllable.
% The answer is negative as our test shows that there will be a drastic drop in accuracy for all top-$K$, see Section~\ref{sec:abbr}.

In this section, we explore incorporating pinyin information into GPT to further strengthen its ability in the pinyin input method task, especially in the setting that the pinyin sequence contains only abbreviations.
We build two main kinds of models to inject the pinyin information:
(1) \textbf{Composition}: the idea of composition is to compose the token information of a specific token with the pinyin information of the next token.
(2) \textbf{Concatenation}: to make GPT aware of the \textit{whole} pinyin inputs, we concatenate the pinyin input sequence with the context input sequence to form a longer sequence before encoding using GPT.

\subsection{Composition} 

Depending on at which layer we inject the pinyin information, there are several variations of composition schemes for us to choose:
(1) \textbf{Bottom}: we can add the pinyin embedded information directly to the token embeddings at the bottom embedding layer before feeding the embeddings into GPT's encoder,
(2) \textbf{Top}: we can first compute the next token's distribution directly from the pinyin embedded information through a feed-forward layer and then add to the logits computed from original GPT.
% (3) \textbf{Stack}: we may also stack more transformer layers over GPT to better fuse the information of both tokens and pinyin.

For \textbf{Composition} models , we are given a text sequence $\mathbf{x} = [w_0,\dots,w_{T-1}]$, $w_0$ is the special start token \texttt{[CLS]} used in GPT, and a pinyin sequence $\mathbf{y} = [p_1,\dots,p_T]$ and perform finetuning over GPT by maximizing the following:
\begin{equation}
    \max_{\theta} \log p_{\theta}(\mathbf{x}) = \sum_{t=1}^{T-1}\log p_{\theta}(x_t|\mathbf{x}_{<t},\mathbf{y}_{<t})
\end{equation}

\subsection{Concatenation} 
The concatenation model takes the \textit{whole} pinyin sequence as part of its inputs. 
During training stage, the target character sequence will be further appended to the input sequence to calculate the autoregressive language model loss.
During inference stage, the generation starts conditioned over the context sequence and the pinyin sequence.
In practice, we expand the vocabulary of GPT $\mathcal{V}$ with the legitimate pinyin vocabulary $\mathcal{P}$, and use the special token \texttt{[SEP]} to separate the input text sequence and the pinyin sequence.
We are given an input sequence $\mathbf{x} = [w_0,\dots,w_l,\texttt{[SEP]},p_{l+1},\dots,p_{l+k},\texttt{[SEP]},w_{l+1},\allowbreak \dots,w_{l+T-1}]$.
To make the pinyin tokens to be aligned with the target tokens, we will share the positional embeddings of the pinyin token with its target token accordingly.
We perform finetuning over GPT by maximizing the following:
\begin{equation}
    \max_{\theta} \log p_{\theta}(\mathbf{x}) = \sum_{t=l+k}^{l + k+ T}\log p_{\theta}(x_t|\mathbf{x}_{<t})
\end{equation}

\subsection{Training}
\label{sec:training}

Unless specified explicitly, the corpus used for the training will be the same as that used when pretraining our own GPT.
Since the corpus is in pure text, we use a public library \textit{pypinyin}\footnote{\url{https://github.com/mozillazg/python-pinyin}} to get the pinyin of a piece of text.
Although the library is very helpful in labelling pinyin for most commonly used words, there are issues of heteronym for some characters which we further verified using online dictionaries like ZDic~(汉典)\footnote{\url{https://www.zdic.net/}}.

We start the training by initialize our model with a checkpoint from our pretrained Chinese GPT discussed in Section~\ref{sec:frozen}.

During training, we also changed the loss function to better suit the scenario of pinyin input method.
As pointed out in Section~\ref{sec:intro}, we choose to filter the predicted token distribution by keeping only those tokens whose pinyin matches the target pinyin syllable during generation.
Considering that the original loss of GPT is computed over the whole vocabulary, this leads to inconsistency between training and evaluation.
In this paper, we choose to use a modified loss function which focuses on the characters sharing the same pinyin.


Our models are trained using 32 GPUs of NVIDIA V100 Tensor Core with a bach size of 25,000 for 100k steps.
Our starting learning rate is set to be 5e-5 .
Each training case is sampled from the same corpus used for our pretraining of GPT.
We maintain a maximum of 128 tokens for every case.
When training the Concatenation model, we use a probability 0.5 to keep the target sequence to be less than 5 words, otherwise we randomly choose from a range 6 to 25 words.
