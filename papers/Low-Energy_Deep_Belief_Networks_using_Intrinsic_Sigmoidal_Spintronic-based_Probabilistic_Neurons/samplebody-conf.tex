\section{Introduction}
The interrelated fields of machine learning (ML), and artificial neural networks (ANN) have grown significantly in previous decades due to the availability of powerful computing systems to train and simulate large scale ANNs within reasonable time-scales, as well as the abundance of data available to train such networks in recent years. The resulting research has realized a bevy of ANN architectures that have performed incredible feats including a wide range of classification problems, and various recognition tasks.

Most ML techniques in-use today rely on supervised learning, where the systems are trained on patterns with a known desired output, or label. However, intelligent biological systems exhibit unsupervised learning whereby statistically correlated input modalities are associated within an internal model used for probabilistic inference and decision making \cite{buesing2011}. One interesting class of unsupervised learning approaches that has been extensively researched is the Restricted Boltzmann machine (RBM) \cite{hinton2006}. RBMs can be hierarchically organized to realize deep belief networks (DBNs) that have demonstrated unsupervised learning abilities, such as natural language understanding \cite{Sarikaya2014}. Most RBM and DBN research has focused on software implementations, which provides flexibility, but requires significant execution time and energy due to large matrix multiplications that are relatively inefficient when implemented on standard Von-Neumann architectures due to the memory-processor bandwidth bottleneck when compared to hardware-based in-memory computing approaches \cite{Merolla2014}. Thus, research into hardware-based RBM designs has recently sought to alleviate these constraints. 

Previous hardware-based RBM implementations have aimed to overcome software limitations by utilizing FPGAs \cite{Kim2010,Ly2010} and stochastic CMOS \cite{Ardakani2017}. In recent years, emerging technologies such as resistive RAM (RRAM) \cite{SHERI2015,Bojnordi2016} and phase change memory (PCM) \cite{Eryilmaz2016} are proposed to be leveraged within the DBN architecture as weighted connections interconnecting building blocks in RBMs. While most of the previous hybrid Memristor/CMOS designs focus on improving the synapse behaviors, the work presented herein overcomes many of the preceding challenges by utilizing a novel spintronic p-bit device that leverages intrinsic thermal noise within low energy barrier nanomagnets to provide a natural building block for RBMs within a compact and low-energy package. The contribution of this paper is go to beyond using low-energy barrier magnetic tunnel junctions (MTJs), as has been previously introduced for a neuron in spiking neuromorphic systems \cite{sengupta2016magnetic,Sengupta2016prob}. To the best of our knowledge this paper is the first effort to use MTJs with near-zero energy barriers as neurons within an RBM implementation. Additionally, various parameters of a hybrid CMOS/spin weight array structure are investigated for metrics of power dissipation, and error rate using the MNIST digit recognition benchmarks.
 


\section{Fundamentals of RBM}
Boltzmann Machines (BM) are a class of recurrent stochastic ANNs with binary nodes whereby each possible state of the network, \textit{v}, has an energy determined by the undirected connection weights between nodes and the node bias as described by (1), where $s_i^v$ is the state of node \textit{i} in \textit{v}, \textit{b\textsubscript{i}} is the bias, or intrinsic excitability of node \textit{i}, and \textit{w\textsubscript{ij}} is the connection weight between nodes \textit{i} and \textit{j} \cite{ackley1985}.
\begin{equation}
  E(v) = -\sum_{i} s_i^v b_i -\sum_{i<j} s_i^v s_j^v w_{ij} 
\end{equation}

\begin{equation}
  P(s_i = 1) = \sigma (b_i + \sum_{j} w_{ij} s_j)
\end{equation}

Each node in a BM has a probability to be in state 1 according to (2), where $\sigma$ is the logistic sigmoid function. BMs, when given enough time, will reach a Boltzmann distribution where the probability of the system being in state \textit{\textbf{v}} is found by $P(v) = \frac{e^{-E(v)}}{\sum_{u} e^{-E(u)}}$, where \textit{\textbf{u}} could be any possible state of the system. Thus, the system is most likely to be found in states that have the lowest associated energy.
%\begin{equation}
 % P(v) = \frac{e^{-E(v)}}{\sum_{u} e^{-E(u)}}
%\end{equation}
Restricted Boltzmann machines (RBMs) are BMs constrained to two fully-connected non-recurrent layers called the \textit{visible layer}, where salient inputs clamp nodes to output levels of either zero or one, and the \textit{hidden layer}, where associations between input vectors are learned. By enforcing the conditional independence of the visible and hidden layers, unbiased samples from the input can be obtained in one time-step, which enhances the learning process. 

The most widely used method for training RBMs is contrastive divergence (CD), which is an approximate gradient descent procedure using Gibbs sampling \cite{carreira2005}. CD operates in three phases: \textit{(1) Positive Phase:} A training input vector, \textbf{$v$}, is applied to the visible layer by clamping the nodes to either 1 or 0 levels, and the hidden layer is sampled, \textbf{$h$}. \textit{(2) Negative Phase:} by clamping the hidden layer to \textbf{$h$}, the reconstructed input layer is sampled, \textbf{$v'$}. Then, clamp the visible layer to v' and sample the hidden layer to obtain \textbf{$h'$}. \textit{(3)} Update the weights according to $\Delta W = \eta (vh^T-v'h'^T)$, where $\eta$ is the learning rate and \textbf{$W$} is the weight matrix. 

DBNs are realized when additional hidden layers are stacked on top of an RBM, and can be trained in a very similar way to RBMs. Essentially, training a DBN involves performing CD on the visible layer and the first hidden layer for as many steps as desired, then fixing those weights and moving up a hierarchy as follows. The first hidden layer is now viewed as a visible layer, while the second hidden layer acts as a hidden layer with respect to the CD procedure identified above. Next, another set of CD steps are performed, and then the process is repeated for each additional layer of the DBN.


\section{Spin-Based Building Block For RBM}
In this section, we provide a detailed description of the p-bit that provides the building block for our proposed spin-based BM architecture. Individual building blocks are interconnected by networks of memristive devices whose resistances can be programmed to provide the desired weights. For instance, in this paper, we will assume that the memristive devices are implemented using the three terminal spin-orbit torque (SOT)-driven domain wall motion (DWM) device proposed in \cite{Sengupta2016hybrid}.

\begin{figure}
\includegraphics[scale=0.12]{fig1.pdf}
\caption{Structure of a p-bit.}
\end{figure}

\begin{figure}
\includegraphics[scale=0.28]{fig2.pdf}
\caption{Time-averaged results over 100 ns for p-bit.}
\end{figure}

The activation function is achieved by a spintronic building block that has been used in the design of probabilistic spin logic devices (p-bits) for a wide variety of Boolean and non-Boolean problems \cite{Camsari2017,Faria2017,sutton2017,behin2016}. The basic functionality of the p-bit shown in Fig. 1 \cite{Camsari2017} is to produce a stochastic output whose steady-state probability is modulated by an input current to generate a sigmoidal activation function. For instance, a high positive input current produces a stochastic output with a high probability of ``0'', and vice versa. In the absence of any input current, the device generates either 0 or VDD outputs with roughly equal probability of 0.5, as shown in Fig. 2. This device consists of a 3-terminal, spin-Hall driven MTJ \cite{Liu2012} that uses a circular, unstable nanomagnet ($\Delta \ll 40kT$), whereby its output is amplified by CMOS inverters as shown in Fig. 1. This MTJ with an unstable free layer can be fabricated using standard technology such that the surface anisotropy to achieve perpendicular magnetic anisotropy (PMA) that is not strong enough to overcome the demagnetizing field. Thus, the magnetization stochastically rotates in the plane, due to the presence of thermal fluctuations. 
 
The charge current that is injected to the spin-Hall layer creates a spin-current flowing into the circular FM (in the +y direction), which does not have an axis with any preferential geometry. The spin-polarization of this spin-current is in the ($\pm z$) direction, and pins the magnetization in the (+z) or (-z) direction depending on the direction of the charge current, through the spin-torque mechanism \cite{sutton2017}. The inherent physics of the spin-current driven low-barrier nanomagnet provides a natural sigmoidal function when a long time average of magnetization is taken. Through the tunneling magnetoresistance effect, a charge current flowing through the MTJ with a stable fixed layer detects the modulated magnetization as a voltage change. To achieve this, a small read voltage $V_R$ is applied between the $V+$ and $V-$ terminals through a reference resistance $R_0$, adjusted to the average conductance of the MTJ $(R_0^-1=GP+GAP/2)$ where $GP$ and $GAP$ represented conductance in parallel (P) and anti-parallel (AP) states, respectively. This voltage becomes an input to the CMOS inverters that are biased at the middle point of their DC operating point, creating a stochastic output whose probability can be tuned by the input charge current.  

\begin{table}[]
\centering
\small
\caption{Parameters for p-bit Based Activation Function.}
\label{tab:parameter}
\begin{tabular}{ccc}
\hline
Parameter & Description & Value  \\ \hline
\multicolumn{3}{l}{\textbf{Circular FM}} \\ \hline
%$M_S$ & \begin{tabular}[c]{@{}l@{}}Saturation magnetization\end{tabular} & $300 emu/cc$ \\
$\phi$ & Diameter & $100 nm$ \\
$t$  & Thickness  & $2 nm$   \\
$\alpha$ & Damping coefficient & $0.01$  \\ \hline
%$T$  & Temperature & $300 K$ \\
%$\Delta t$  & Timestep for sLLG & $0.25 ps$ \\ \hline
\multicolumn{3}{l}{\textbf{MTJ}} \\ \hline
$G0$ & Conductance & $150e^{-6} S$ \\
$P$  & Spin Polarization & $0.52$ \\ \hline
\multicolumn{3}{l}{\textbf{\begin{tabular}[c]{@{}l@{}}Giant Spin Hall Layer(GSHE)\end{tabular}}}      \\ \hline
$\lambda$ & Spin-diffusion length & $2.1 nm$ \\
$\theta$      & Spin Hall Angle  & $0.5$ \\
$Volume$   & $l \times w \times t$ & $100 nm \times 100 nm \times 3.15 nm$ \\ \hline
\end{tabular}
\vspace{-0.3cm}
\end{table}

\begin{figure*}
\includegraphics[height=2.2in, width=5.2in]{fig3.jpg}
\caption{Proposed $32 \times 32$ hybrid CMOS/spin-based weighted array structure for RBM implementation.}
\end{figure*}

Each component of the device is represented by an independent spin-circuit based on experimentally-benchmarked models that have been established in \cite{camsari2015modular} and simulated as a spin-circuit in a SPICE-like platform. Here, we obtain an analytical approximation to the time-averaged behavior of the output characteristics. We start by relating the charge current flowing in the spin Hall layer to the spin-current absorbed by the magnet, assuming short-circuit conditions for simplicity, i.e. 100\% spin absorption by the FM: 

\begin{equation}
  I_s/I_c = \beta = \frac{L}{t} (\theta) (1 - sech(\frac{t}{\lambda}))
\end{equation}
where $I_s$ is the spin-current, $I_c$ is the charge current, $\theta$ is the spin-Hall angle, $L$, $t$, $\lambda$ are the length, thickness and spin diffusion lengths for the spin-Hall layer. The length and width of the GSHE layer are assumed to be the same as the circular nanomagnet. With a suitable choice of the L and t, the spin-current generated can be greater in magnitude than the charge current generating ``gain.'' For the parameters used in this paper, which are listed in Table~\ref{tab:parameter}, the gain factor $\beta$ is $\sim 10$. Next, we approximate the behavior of the magnetization as a function of an input spin-current, polarized in the ($\pm z$) direction. For a magnet with only a PMA in the $\pm z$ direction, a distribution function at steady state can be written analytically as below, as long as the spin-current is also fully in the $\pm z$ direction:

\begin{equation}
  \rho (m_z) = \frac{1}{Z} exp(\Delta m_z^2 + 2 i_s m_z)
\end{equation}
where $Z$ is a normalization constant, $m_z$ is the magnetization along $+z$,  is the thermal barrier of the nanomagnet, and $i_s$ is a normalization quantity for the spin-current such that $i_s= I_s/(4q/ \hbar \alpha kT)$, $\alpha$ being the damping coefficient of the magnet, $q$ the electron charge and $\hbar$ the reduced Planck constant. It is possible to use (4) to obtain an average magnetization $<m_z> = \int_{-1}^{+1} d m_z m_z \rho (m_z) / \int_{-1}^{+1} d m_z \rho (m_z)$. Assuming $\Delta \ll kT$, $<m_z>$ can be evaluated to give the Langevin function, $<m_z> = L(i_s)$ where $L(x) = \frac{1}{x} - coth \frac{1}{x}$, which is an exact description for the average magnetization in the presence of a z-directed spin-current for a low barrier PMA magnet. 




In the present case, however, the nanomagnet has a circular shape with a strong in-plane anisotropy and no simple analytical formula can be derived, thus We use the Langevin function with a fitting parameter that adjusts the normalization current by a factor $\eta$, so that the modified normalization constant becomes $(4 q/\hbar \alpha kT)(\eta)$. This factor increases with elevating the shape anisotropy $(H_d \sim 4 \pi M_s)$ and becomes exactly one when there is no shape anisotropy. Once the magnetization and charge currents are related, we can approximate the output probability of the CMOS inverters by a phenomenological equation along with fitting parameter $\chi$ as follows, $p = \frac{V_{OUT}}{VDD} \approx \frac{1}{2} [1-tanh(\chi <m_z>)]$,   
%\begin{equation}
 %p = \frac{V_{OUT}}{VDD} \approx \frac{1}{2} [1-tanh(\chi <m_z>)]  
%\end{equation}
which allows us to relate the input charge current to the output probability, with physical parameters.  Fig. 2 shows the comparison of the full SPICE-model with respect to aforementioned equations showing good agreement with two fitting parameters $\eta$ and $\chi$, which fit the magnetization and CMOS components, respectively. 




\section{Proposed Weighted Array Design}
Figure 3 shows the structure of the weighted array proposed herein to implement the RBM architecture including the SOT-DWM based weighted connections and biases, as well as the p-bit based activation functions. Transmission gates (TGs) are utilized in write circuits within the bit cells of the weighted connection to adjust weights by moving the DW position. As investigated in \cite{zand2017}, TGs can provide energy-efficient and symmetric switching operation for SOT-based devices, which are desirable during the training phase. Table~\ref{tab:signaling} lists the required signaling for controlling the training and read operations in the weighted array structure. Herein, a chain of inverters are considered to drive signal lines, in which each successive inverter is twice as large as the previous one.

During the read operation, write word line (WWL) is connected to ground (GND) and the source line (SL) is in high impedance (Hi-Z) state, which disconnects the write path. The read word line (RWL) for each row is connected to VDD, which turns ON the read transistors in the weighted connection bit cell. The bit line (BL) will be connected to the input signal (VIN), which results in producing a current that affects the output probability of the p-bit device. The direction of the generated current relies on the VIN signal. In particular, since V- is supplied by a voltage source equal to VDD/2, if VIN is connected to VDD the injected current to the p-bit based activation function will have positive value, and if VIN is zero the input current will be negative. The amplitude of the generated current depends on the resistance of the weighted connection which is defined by the position of the DW in the SOT-DWM device. 

During the training operation, the RWL is connected to GND, which turns OFF the read transistors and disconnects the read path. The WWL is connected to an input pulse (VPULSE) signal which activates the write path for a short period of time. The duration of the VPULSE should be designed in a manner such that it can provide the desired learning rate, $\eta$, to the training circuit. For instance, a high VPULSE duration results in a significant change in the DW position in each training iteration, which effectively reduces the number of different resistive states that can be realized by the SOT-DWM device. Resistance of the weighted connections can be adjusted by the BL and SL signals, as listed in Table~\ref{tab:signaling}. A higher resistance leads to a smaller current injected to the p-bit device. Therefore, the input signal connected to the weighted connection will have lower impact on the output probability of the p-bit device, which means the input signal exhibits a lower weight. The bias nodes can also be adjusted similar to the weighted connection.  

\begin{table}[]
\centering
\small
\caption{Signaling to Control The Array Operations.}
\vspace{-0.2cm}
\label{tab:signaling}
\begin{tabular}{ccccccc}
\hline
Operation & WWL & RWL & BL  & SL   & V+   & V-    \\ \hline
\begin{tabular}[c]{@{}c@{}}Increase Weight\end{tabular} & VPULSE & GND & VDD & GND  & Hi-Z & Hi-Z  \\
\begin{tabular}[c]{@{}c@{}}Decrease Weight\end{tabular} & VPULSE & GND & GND & GND  & Hi-Z & Hi-Z  \\
Read & GND    & VDD & VIN & Hi-Z & VDD  & VDD/2 \\ \hline
\end{tabular}
\vspace{-0.3cm}
\end{table}


\begin{table}[]
\centering
\small
\caption{Relation between the input currents of activation functions and array size for $R_P = 1 M \Omega$.}
\vspace{-0.2cm}
\label{tab:arraysize}
\begin{tabular}{ccccc}
\hline
\multirow{2}{*}{Features}       & \multicolumn{4}{c}{Array Size}                                  \\ \cline{2-5} 
                                & $8 \times 8$ & $16 \times 16$ & $32 \times 32$ & $64 \times 64$ \\ \hline
Max. Positive Current $(\mu A)$ & 2.71         & 5.14           & 10.79          & 21.46          \\
Max. Negative Current $(\mu A)$ & 3.57         & 7.14           & 14.23          & 28.28          \\
Max. output ``0'' Probability   & 0.77         & 0.88           & 0.95           & 0.97           \\
Min. output ``0'' Probability   & 0.175        & 0.08           & 0.038          & 0.026          \\ \hline
\end{tabular}
\end{table}




\section{Simulation Results And Discussion}
To analyze the RBM implementation using the proposed p-bit device and the weighted array structure, we have utilized a hierarchical simulation framework including circuit-level and application-level simulations. In circuit level simulation, the behavioral models of the p-bit and SOT-DWM devices were leveraged in SPICE circuit simulations using 20nm CMOS technology with 0.9V nominal voltage to validate the functionality of the designed weighted array circuit. In application-level simulation, the results obtained from device-level and circuit-level simulations are used to implement a DBN architecture and analyze its behavior in MATLAB.


\begin{figure}
\includegraphics[scale=0.22]{fig4.pdf}
\caption{Weighted array power consumption versus the resistance of the weighted connections and array size.}
\vspace{-0.3cm}
\end{figure}


\subsection{Circuit-level simulation}
The device-level simulations shown in Fig. 2 verified a sigmoidal relation between the input current of the p-bit based activation function and its output probability. The shape of the activation on function is one of the major factors affecting the performance of the RBM. Therefore, we have provided comprehensive analyses on the impacts of weighted connection resistance and weighted array dimensions on the input currents of the p-bit based activation functions, and the power consumption of the weighted array. 



Table~\ref{tab:arraysize} lists the range of the activation function input currents for various weighted array dimensions, while the resistance of the SOT-DWM device in parallel state (RP) is constant and equals $1 M\Omega$.  The experimental results provided in [19, 28] exhibit that an MTJ resistance in the $M\Omega$ range can be obtained by increasing the oxide thickness in an MTJ structure. The highest positive and negative currents can be achieved while the weighted connections are in parallel state, i.e. lowest resistance, and all of the input voltages (VIN) are equal to VDD and GND, respectively. The difference between the amplitude of positive and negative currents in a given array size with constant RP is caused by the different pull-down and pull-up strengths in NMOS read transistors. The maximum and minimum output-level ``0'' probabilities are listed in Table~\ref{tab:arraysize}, which can be obtained according to the measured input currents and the sigmoidal activation function shown in Fig. 2. 





\begin{table}[]
\centering
\small
\caption{Relation between the input currents of activation functions and $R_P$ in a $32 \times 32$ array.}
\vspace{-0.2cm}
\label{tab:rp}
\begin{tabular}{ccccc}
\hline
\multirow{2}{*}{Features}       & \multicolumn{4}{c}{$R_P (M \Omega)$} \\ \cline{2-5} 
                                & 0.25    & 0.5     & 0.75    & 1      \\ \hline
Max. Positive Current $(\mu A)$ & 36.56   & 20.02   & 13.97   & 10.79  \\
Max. Negative Current $(\mu A)$ & 54.95   & 28.12   & 18.9    & 14.23  \\
Max. output ``0'' Probability   & 0.98    & 0.965   & 0.96    & 0.95   \\
Min. output ``0'' Probability   & 0.01    & 0.026   & 0.032   & 0.038  \\ \hline
\vspace{-0.4cm}
\end{tabular}
\end{table}

\begin{table*}[]
\centering
\small
\caption{Comparison between various RBM implementations with an emphasis on activation function structure.} 
\label{tab:compare}
\begin{tabular}{cccccccc} \hline

\iffalse %%%%%%%%%%%%%%%%%%
\begin{tabular}[c]{@{}c@{}}Design\end{tabular}                    & Kim et al.{[}9{]}  & Ly et al. {[}10{]} & Ardakani et al. {[}11{]} & Sheri et al. {[}12{]} & Bojnordi et al. {[}13{]} & Eryilmaz et al. {[}14{]} & \begin{tabular}[c]{@{}c@{}}Proposed\\ Herein\end{tabular}\\  \hline
\fi    %%%%%%%%%%%%%%%%%%%%%

\begin{tabular}[c]{@{}c@{}}Design\end{tabular}                    & {\cite{Kim2010}}  & {\cite{Ly2010}} & {\cite{Ardakani2017}} & {\cite{SHERI2015}} & {\cite{Bojnordi2016}} & {\cite{Eryilmaz2016}} & \begin{tabular}[c]{@{}c@{}}Proposed\\ Herein\end{tabular}\\  \hline

\begin{tabular}[c]{@{}c@{}}Weighted\\ Connection\end{tabular}        & \begin{tabular}[c]{@{}c@{}}Embedded\\ multipliers \end{tabular}  & \begin{tabular}[c]{@{}c@{}}Embedded\\ multipliers \end{tabular} & \begin{tabular}[c]{@{}c@{}}- LFSR\\  - AND/OR gates\end{tabular} &  \begin{tabular}[c]{@{}c@{}}RRAM\\ memristor \end{tabular} & RRAM & PCM & SOT-DWM  \\ \hline

\begin{tabular}[c]{@{}c@{}}Activation\\ Function\end{tabular}        & \begin{tabular}[c]{@{}c@{}}CMOS-based \\ LUTs\end{tabular} & \begin{tabular}[c]{@{}c@{}}-2-kB BRAM\\ - Picewise Linear \\Interpolator\\ - Random number\\ Generator\end{tabular} & \begin{tabular}[c]{@{}c@{}} - LFSR\\ - Bit-wise AND\\ - tree adder\\ - FSM-based \\tanh unit\end{tabular} & Off-chip & \begin{tabular}[c]{@{}c@{}} - $64 \times 16$ LUTs\\ - Pseudo Random \\Number Generator\\ - Comparator\end{tabular} & Off-chip                 & \begin{tabular}[c]{@{}c@{}} - near-zero \\energy barrier \\probabilistic \\spin logic \\ device\end{tabular} \\ \hline
\begin{tabular}[c]{@{}c@{}}Energy per neuron\end{tabular}          & N/A & $\sim 10-100 nJ$ & $\sim 10-100 pJ$ & N/A & $\sim 1-10 nJ$ & N/A    & $\sim 1-10 fJ$  \\ \hline
\begin{tabular}[c]{@{}c@{}}Normalized area per neuron\end{tabular} & N/A  & $\sim 3000 \times$ & $\sim 90 \times$ & N/A & $\sim 1250 \times$ & N/A    & $\sim 1 \times$ \\                                                               
\hline
\end{tabular} 
                
\end{table*}


Moreover, Table~\ref{tab:rp} illustrates the relation between the $R_P$ values and input currents of the activation functions, and their corresponding output probabilities, for a given $32 \times 32$ weighted array. The lower RP resistance and higher array size provides a wider range of output probabilities which can increase the RBM performance. However, this is achieved at the cost of higher area and power consumption. The trade-offs between the array size, weighted connection resistance, and average power consumption in a single read operation is shown in Fig. 4. The lowest power consumption of 22.6 $\mu W$ is realized by an $8 \times 8$ array with $R_P = 1 M \Omega$. However, this array provides the narrowest range of the output probabilities, which significantly reduces the performance of the DBN.




\subsection{Application-level simulation}
In the application-level simulation, we have leveraged the obtained device and circuit behavioral models to simulate a DBN architecture for digit recognition. In particular, learning rate and the shape of the sigmoid activation function is extracted by the SOT-DWM and p-bit device-level simulations, respectively, while the circuit-level simulations defines the range of the output probabilities.  To evaluate the performance of the system, we have modified a MATLAB implementation of DBN by Tanaka and Okutomi \cite{Tanaka2014} and used the MNIST data set \cite{Lecun1998} including 60,000 and 10,000 sample images with $28 \times 28$ pixels for training and testing operations, respectively. We have used Error rate (ERR) metric to evaluate the performance of the DBN, as expressed by $ERR=N_F/N$, where, $N$ is the number of input data, $N_F$ is the number of false inference \cite{Tanaka2014}. 




The simplest model of the DBN that can be implemented for MNIST digit recognition consists 784 nodes in visible layer to handle $28 \times 28$ pixels of the input images, and 10 nodes in hidden layer representing the output classes. Fig. 5 shows the relation between the performance of various DBN topologies, and the number of input training samples ranging from 100 to 5,000, which is obtained using 1,000 test samples. The ERR and RMSE metrics can be improved by enlarging the DBN structure through increasing the number of hidden layers, as well as the number of nodes in each layer. This improvement is realized at the cost of larger area and power consumptions. Increasing the input training samples can improve the DBN performance as well, however it will quickly converge due to the limited weight values that can be provided by SOT-DWM based weighted connections. As shown in Fig. 5, some random behaviors are observed for networks with smaller sizes that are trained by lower number of training samples, which will be significantly reduced by increasing the number of training samples.

The simulation results exhibit the highest error rate of 36.8\% for a $784 \times 10$ DBN that is trained by 100 training samples. Meanwhile, the lowest error rate of 3.7\% was achieved using a $784 \times 800 \times 800 \times 10$ DBN trained by 5,000 input training samples. This illustrates that the recognition error rate can be decreased by increasing the number of hidden layers, and training samples, which is also realized at the cost of higher area and power overheads.


\begin{figure}
\includegraphics[scale=0.5]{fig5.jpg}
\vspace{-0.2cm}
\caption{ERR for various DBN topologies.}
\vspace{-0.4cm}
\end{figure}



\subsection{Disucussion}
Table~\ref{tab:compare} lists previous hardware-based RBM implementations, which have aimed to overcome software limitations by utilizing FPGAs \cite{Kim2010,Ly2010}, stochastic CMOS \cite{Ardakani2017}, and hybrid memristor-CMOS designs \cite{SHERI2015,Bojnordi2016,Eryilmaz2016}. FPGA implementations demonstrated RBM speedups of 25-145 over software implementations \cite{Kim2010,Ly2010}, but had significant constraints such as only realizing a single $128 \times 128$ RBM per FPGA chip, routing congestion, and clock frequencies limited to 100MHz \cite{Ly2010}. The stochastic CMOS-based RBM implementation proposed in \cite{Ardakani2017} leveraged the low-complexity of stochastic CMOS arithmetic to save area and power. However, the need for extremely long bit-stream lengths negate energy savings and lead to very long latencies. Additionally, a significant amount of Linear Feedback Shift Registers (LFSRs) were required to produce the uncorrelated input and weight bit-streams. In both the FPGA and stochastic CMOS designs, improvements were achieved by implementing parallel Boolean circuits such as multipliers and pseudo-random number generators for probabilistic behavior, which has significant area and energy overheads compared to leveraging the physical behaviors of emerging devices to perform the computation intrinsically. Bojnordi et al. \cite{Bojnordi2016} leveraged resistive RAM (RRAM) devices to implement efficient matrix multiplication for weighted products within Boltzmann machine applications, and demonstrated significant speedup of up to 100-fold over single-threaded cores and energy savings of over 10-fold. Similarly, Sheri et al. \cite{SHERI2015} and Eryilmaz et al. \cite{Eryilmaz2016} utilized RRAM and PCM devices to implement matrix multiplication, while the corresponding activation function circuitry is still based on the CMOS technology, which suffers from the aforementioned area and power consumption overheads.

While most of the previous hybrid Memristor/CMOS designs focus on improving the performance of weighted connections, the work presented herein overcomes many of the preceding challenges of generating sigmoidal probabilistic activation functions by utilizing a novel p-bit device that leverages intrinsic thermal noise within low energy barrier nanomagnets to provide a natural building block for RBMs within a compact and low-energy package. As listed in Table V, the proposed design can achieve approximately three orders of magnitude improvement in term of energy consumption compared to the most energy-efficient designs, while realizing at least ~90X device count reduction for considerable area savings. Note that these calculations do not take into account the weighted connections, since the main focus of this paper is on the activation function. While SOT-DWM devices are utilized herein for the weighted connections, any other memristive devices could be utilized without loss of generality.  

\section{Conclusion}
Herein, we developed a hybrid CMOS/spin-based DBN implementation using p-bit based activation functions modeled to produce a probabilistic output that can be modulated by an input current. The device-level simulations exhibited a sigmoid relation between the input currents and output probability. The SPICE model of the p-bit is used to design a weighted array structure to implement RBM. The circuit simulations showed that the performance of the array can be improved by enlarging the array size, as well as reducing the resistance of the weighted connections. However, these improvements are achieved at the cost of increased area and power consumption. For instance, the lowest power dissipation among the examined designs belongs to an $8 \times 8$ array with the maximum resistance of $1 M \Omega$ for weighted connections. However, this structure can only provide the output probabilities ranging from 0.175 to 0.77, which is the narrowest range among the examined designs resulting in a DBN implementation with lowest accuracy. 

Next, we simulated a DBN for digit recognition application in MATLAB using the device and circuit-level behavioral models. Trade-offs include the relations between the recognition accuracy of the DBN and the number of training samples, which are comparable to conventional hardware implementations. The recognition error rate decreased substantially for the first thousand training samples, regardless of the size of the array, while benefits continue through several thousand inputs. However, at least two hidden layers are desirable to achieve suitable error rates. Finally, we have provided a comparison between previous hardware-based RBM implementations and our design with an emphasis on the probabilistic activation function within the neuron structure. The results exhibited that the p-bit based activation function can achieve roughly three orders of magnitude energy improvement, while realizing at least 90X reduction in terms of device count, compared to the previous most energy-efficient designs. The research directions herein enable several intriguing possibilities for future work, including: (1) implementing the entire network in SPICE to obtain more robust results; 2) investigating the effect of process variation and noise on the accuracy of proposed architecture; 3) studying alternative devices with lower susceptibility to thermal noise; and 4) studying the scalability challenges of DBNs using larger datasets, e.g. CIFAR.  


\section{Acknowledgements}
This work was supported in part by the Center for Probabilistic Spin Logic for Low-Energy Boolean and Non-Boolean Computing  (CAPSL), one of the Nanoelectronic Computing Research (nCORE) Centers as task 2759.006, a Semiconductor Research Corporation (SRC) program sponsored by the NSF through CCF 1739635.

%This material is based upon work partially supported by the National Science Foundation under Grant No. 1739635. This work was also supported in part by the Center for Probabilistic Spin Logic for Low-Energy Boolean and Non-Boolean Computing  (CAPSL), one of the Nanoelectronic Computing Research (nCORE) Centers as task 2759.006, a Semiconductor Research Corporation (SRC) program sponsored by the NSF through CCF 1739635.


\iffalse    %%%%%%%%%%%%%%%%%%%%%%comments
We have already seen several typeface changes in this sample.  You can
indicate italicized words or phrases in your text with the command
\texttt{{\char'134}textit}; emboldening with the command
\texttt{{\char'134}textbf} and typewriter-style (for instance, for
computer code) with \texttt{{\char'134}texttt}.  But remember, you do
not have to indicate typestyle changes when such changes are part of
the \textit{structural} elements of your article; for instance, the
heading of this subsection will be in a sans serif\footnote{Another
  footnote here.  Let's make this a rather long one to see how it
  looks.} typeface, but that is handled by the document class file.
Take care with the use of\footnote{Another footnote.}  the
curly braces in typeface changes; they mark the beginning and end of
the text that is to be in the different typeface.

You can use whatever symbols, accented characters, or non-English
characters you need anywhere in your document; you can find a complete
list of what is available in the \textit{\LaTeX\ User's Guide}
\cite{Lamport:LaTeX}.
\fi %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\iffalse %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Math Equations}
You may want to display math equations in three distinct styles:
inline, numbered or non-numbered display.  Each of
the three are discussed in the next sections.

\subsubsection{Inline (In-text) Equations}
A formula that appears in the running text is called an
inline or in-text formula.  It is produced by the
\textbf{math} environment, which can be
invoked with the usual \texttt{{\char'134}begin\,\ldots{\char'134}end}
construction or with the short form \texttt{\$\,\ldots\$}. You
can use any of the symbols and structures,
from $\alpha$ to $\omega$, available in
\LaTeX~\cite{Lamport:LaTeX}; this section will simply show a
few examples of in-text equations in context. Notice how
this equation:
\begin{math}
  \lim_{n\rightarrow \infty}x=0
\end{math},
set here in in-line math style, looks slightly different when
set in display style.  (See next section).
\fi %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\iffalse%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Display Equations}
A numbered display equation---one set off by vertical space from the
text and centered horizontally---is produced by the \textbf{equation}
environment. An unnumbered display equation is produced by the
\textbf{displaymath} environment.

Again, in either environment, you can use any of the symbols
and structures available in \LaTeX\@; this section will just
give a couple of examples of display equations in context.
First, consider the equation, shown as an inline equation above:
\begin{equation}
  \lim_{n\rightarrow \infty}x=0
\end{equation}
Notice how it is formatted somewhat differently in
the \textbf{displaymath}
environment.  Now, we'll enter an unnumbered equation:
\begin{displaymath}
  \sum_{i=0}^{\infty} x + 1
\end{displaymath}
and follow it with another numbered equation:
\begin{equation}
  \sum_{i=0}^{\infty}x_i=\int_{0}^{\pi+2} f
\end{equation}
just to demonstrate \LaTeX's able handling of numbering.

\subsection{Citations}
Citations to articles~\cite{bowman:reasoning,
clark:pct, braams:babel, herlihy:methodology},
conference proceedings~\cite{clark:pct} or maybe
books \cite{Lamport:LaTeX, salas:calculus} listed
in the Bibliography section of your
article will occur throughout the text of your article.
You should use BibTeX to automatically produce this bibliography;
you simply need to insert one of several citation commands with
a key of the item cited in the proper location in
the \texttt{.tex} file~\cite{Lamport:LaTeX}.
The key is a short reference you invent to uniquely
identify each work; in this sample document, the key is
the first author's surname and a
word from the title.  This identifying key is included
with each item in the \texttt{.bib} file for your article.

The details of the construction of the \texttt{.bib} file
are beyond the scope of this sample document, but more
information can be found in the \textit{Author's Guide},
and exhaustive details in the \textit{\LaTeX\ User's
Guide} by Lamport~\shortcite{Lamport:LaTeX}.

This article shows only the plainest form
of the citation command, using \texttt{{\char'134}cite}.

Some examples.  A paginated journal article \cite{Abril07}, an enumerated
journal article \cite{Cohen07}, a reference to an entire issue \cite{JCohen96},
a monograph (whole book) \cite{Kosiur01}, a monograph/whole book in a series (see 2a in spec. document)
\cite{Harel79}, a divisible-book such as an anthology or compilation \cite{Editor00}
followed by the same example, however we only output the series if the volume number is given
\cite{Editor00a} (so Editor00a's series should NOT be present since it has no vol. no.),
a chapter in a divisible book \cite{Spector90}, a chapter in a divisible book
in a series \cite{Douglass98}, a multi-volume work as book \cite{Knuth97},
an article in a proceedings (of a conference, symposium, workshop for example)
(paginated proceedings article) \cite{Andler79}, a proceedings article
with all possible elements \cite{Smith10}, an example of an enumerated
proceedings article \cite{VanGundy07},
an informally published work \cite{Harel78}, a doctoral dissertation \cite{Clarkson85},
a master's thesis: \cite{anisi03}, an online document / world wide web
resource \cite{Thornburg01, Ablamowicz07, Poker06}, a video game (Case 1) \cite{Obama08} and (Case 2) \cite{Novak03}
and \cite{Lee05} and (Case 3) a patent \cite{JoeScientist001},
work accepted for publication \cite{rous08}, 'YYYYb'-test for prolific author
\cite{SaeediMEJ10} and \cite{SaeediJETC10}. Other cites might contain
'duplicate' DOI and URLs (some SIAM articles) \cite{Kirschmer:2010:AEI:1958016.1958018}.
Boris / Barbara Beeton: multi-volume works as books
\cite{MR781536} and \cite{MR781537}.

A couple of citations with DOIs: \cite{2004:ITE:1009386.1010128,
  Kirschmer:2010:AEI:1958016.1958018}. 

Online citations: \cite{TUGInstmem, Thornburg01, CTANacmart}.  


\subsection{Tables}
Because tables cannot be split across pages, the best
placement for them is typically the top of the page
nearest their initial cite.  To
ensure this proper ``floating'' placement of tables, use the
environment \textbf{table} to enclose the table's contents and
the table caption.  The contents of the table itself must go
in the \textbf{tabular} environment, to
be aligned properly in rows and columns, with the desired
horizontal and vertical rules.  Again, detailed instructions
on \textbf{tabular} material
are found in the \textit{\LaTeX\ User's Guide}.

Immediately following this sentence is the point at which
Table~\ref{tab:freq} is included in the input file; compare the
placement of the table here with the table in the printed
output of this document.

\begin{table}
  \caption{Frequency of Special Characters}
  \label{tab:freq}
  \begin{tabular}{ccl}
    \toprule
    Non-English or Math&Frequency&Comments\\
    \midrule
    \O & 1 in 1,000& For Swedish names\\
    $\pi$ & 1 in 5& Common in math\\
    \$ & 4 in 5 & Used in business\\
    $\Psi^2_1$ & 1 in 40,000& Unexplained usage\\
  \bottomrule
\end{tabular}
\end{table}

To set a wider table, which takes up the whole width of the page's
live area, use the environment \textbf{table*} to enclose the table's
contents and the table caption.  As with a single-column table, this
wide table will ``float'' to a location deemed more desirable.
Immediately following this sentence is the point at which
Table~\ref{tab:commands} is included in the input file; again, it is
instructive to compare the placement of the table here with the table
in the printed output of this document.


\begin{table*}
  \caption{Some Typical Commands}
  \label{tab:commands}
  \begin{tabular}{ccl}
    \toprule
    Command &A Number & Comments\\
    \midrule
    \texttt{{\char'134}author} & 100& Author \\
    \texttt{{\char'134}table}& 300 & For tables\\
    \texttt{{\char'134}table*}& 400& For wider tables\\
    \bottomrule
  \end{tabular}
\end{table*}
% end the environment with {table*}, NOTE not {table}!

It is strongly recommended to use the package booktabs~\cite{Fear05}
and follow its main principles of typography with respect to tables:
\begin{enumerate}
\item Never, ever use vertical rules.
\item Never use double rules.
\end{enumerate}
It is also a good idea not to overuse horizontal rules.


\subsection{Figures}

Like tables, figures cannot be split across pages; the best placement
for them is typically the top or the bottom of the page nearest their
initial cite.  To ensure this proper ``floating'' placement of
figures, use the environment \textbf{figure} to enclose the figure and
its caption.

This sample document contains examples of \texttt{.eps} files to be
displayable with \LaTeX.  If you work with pdf\LaTeX, use files in the
\texttt{.pdf} format.  Note that most modern \TeX\ systems will convert
\texttt{.eps} to \texttt{.pdf} for you on the fly.  More details on
each of these are found in the \textit{Author's Guide}.

\begin{figure}
\includegraphics{fly}
\caption{A sample black and white graphic.}
\end{figure}

\begin{figure}
\includegraphics[height=1in, width=1in]{fly}
\caption{A sample black and white graphic
that has been resized with the \texttt{includegraphics} command.}
\end{figure}


As was the case with tables, you may want a figure that spans two
columns.  To do this, and still to ensure proper ``floating''
placement of tables, use the environment \textbf{figure*} to enclose
the figure and its caption.  And don't forget to end the environment
with \textbf{figure*}, not \textbf{figure}!

\begin{figure*}
\includegraphics{flies}
\caption{A sample black and white graphic
that needs to span two columns of text.}
\end{figure*}


\begin{figure}
\includegraphics[height=1in, width=1in]{rosette}
\caption{A sample black and white graphic that has
been resized with the \texttt{includegraphics} command.}
\end{figure}

\subsection{Theorem-like Constructs}

Other common constructs that may occur in your article are the forms
for logical constructs like theorems, axioms, corollaries and proofs.
ACM uses two types of these constructs:  theorem-like and
definition-like.

Here is a theorem:
\begin{theorem}
  Let $f$ be continuous on $[a,b]$.  If $G$ is
  an antiderivative for $f$ on $[a,b]$, then
  \begin{displaymath}
    \int^b_af(t)\,dt = G(b) - G(a).
  \end{displaymath}
\end{theorem}

Here is a definition:
\begin{definition}
  If $z$ is irrational, then by $e^z$ we mean the
  unique number that has
  logarithm $z$:
  \begin{displaymath}
    \log e^z = z.
  \end{displaymath}
\end{definition}

The pre-defined theorem-like constructs are \textbf{theorem},
\textbf{conjecture}, \textbf{proposition}, \textbf{lemma} and
\textbf{corollary}.  The pre-defined de\-fi\-ni\-ti\-on-like constructs are
\textbf{example} and \textbf{definition}.  You can add your own
constructs using the \textsl{amsthm} interface~\cite{Amsthm15}.  The
styles used in the \verb|\theoremstyle| command are \textbf{acmplain}
and \textbf{acmdefinition}.

Another construct is \textbf{proof}, for example,

\begin{proof}
  Suppose on the contrary there exists a real number $L$ such that
  \begin{displaymath}
    \lim_{x\rightarrow\infty} \frac{f(x)}{g(x)} = L.
  \end{displaymath}
  Then
  \begin{displaymath}
    l=\lim_{x\rightarrow c} f(x)
    = \lim_{x\rightarrow c}
    \left[ g{x} \cdot \frac{f(x)}{g(x)} \right ]
    = \lim_{x\rightarrow c} g(x) \cdot \lim_{x\rightarrow c}
    \frac{f(x)}{g(x)} = 0\cdot L = 0,
  \end{displaymath}
  which contradicts our assumption that $l\neq 0$.
\end{proof}

\section{Conclusions}
This paragraph will end the body of this sample document.
Remember that you might still have Acknowledgments or
Appendices; brief samples of these
follow.  There is still the Bibliography to deal with; and
we will make a disclaimer about that here: with the exception
of the reference to the \LaTeX\ book, the citations in
this paper are to articles which have nothing to
do with the present subject and are used as
examples only.
%\end{document}  % This is where a 'short' article might terminate



\appendix
%Appendix A
\section{Headings in Appendices}
The rules about hierarchical headings discussed above for
the body of the article are different in the appendices.
In the \textbf{appendix} environment, the command
\textbf{section} is used to
indicate the start of each Appendix, with alphabetic order
designation (i.e., the first is A, the second B, etc.) and
a title (if you include one).  So, if you need
hierarchical structure
\textit{within} an Appendix, start with \textbf{subsection} as the
highest level. Here is an outline of the body of this
document in Appendix-appropriate form:
\subsection{Introduction}
\subsection{The Body of the Paper}
\subsubsection{Type Changes and  Special Characters}
\subsubsection{Math Equations}
\paragraph{Inline (In-text) Equations}
\paragraph{Display Equations}
\subsubsection{Citations}
\subsubsection{Tables}
\subsubsection{Figures}
\subsubsection{Theorem-like Constructs}
\subsubsection*{A Caveat for the \TeX\ Expert}
\subsection{Conclusions}
\subsection{References}
Generated by bibtex from your \texttt{.bib} file.  Run latex,
then bibtex, then latex twice (to resolve references)
to create the \texttt{.bbl} file.  Insert that \texttt{.bbl}
file into the \texttt{.tex} source file and comment out
the command \texttt{{\char'134}thebibliography}.
% This next section command marks the start of
% Appendix B, and does not continue the present hierarchy
\section{More Help for the Hardy}

Of course, reading the source code is always useful.  The file
\path{acmart.pdf} contains both the user guide and the commented
code.

\begin{acks}
  The authors would like to thank Dr. Yuhua Li for providing the
  MATLAB code of the \textit{BEPS} method.

  The authors would also like to thank the anonymous referees for
  their valuable comments and helpful suggestions. The work is
  supported by the \grantsponsor{GS501100001809}{National Natural
    Science Foundation of
    China}{http://dx.doi.org/10.13039/501100001809} under Grant
  No.:~\grantnum{GS501100001809}{61273304}
  and~\grantnum[http://www.nnsf.cn/youngscientists]{GS501100001809}{Young
    Scientists' Support Program}.

\end{acks}
\fi%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%