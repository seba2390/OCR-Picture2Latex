\subsection{Data acquisition system}
Typically, photometric stereo datasets~\citep{chen2018ps,ikehata2018cnn,shi2016benchmark} are collected using RGB frames owing to their availability.
Despite the convenience of image collection using an RGB camera, the datasets have some issues related to saturation and a limited dynamic range. To overcome these limitations, we introduce a novel data acquisition system with an event camera for photometric stereo datasets in an ambient light environment. The configuration of RGB and event cameras mitigates the saturation problem by extending the dynamic range.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.475\textwidth]{event_flow.pdf}
    \caption{Examples of event signals. (a) is blob09, (b) is sculpture04, (c) is horseman, and (d) is reading. (a), (b), (c) are parts of the training dataset, and (d) is part of the test dataset.}
    \label{fig:event_flow}
\end{figure}
There are two methods for generating event data: synthetic data creation using grayscale images~\citep{hu2021v2e} and real-world data creation using an event camera. The synthetic data creation method generates event signals based on the concept of how many pixels change in two frame intervals. Even if the change in each pixel is synthesized well, the majority of the event signals are generated around the starting time stamp. Thus, the representation of the change in pixel values decreases as it reaches the end of each time stamp. To avoid this uneven distribution of synthetic data, we used an event camera to fill the gap within the time intervals. Fig.~\ref{fig:event_flow} shows the event signals during a specific period in the two frames. To use the temporal property of event signals, our dataset was generated with lights that moved continuously and, were not fixed. The exact light trajectory cannot be reproduced over time, and the light intensity affecting the object changed every moment. Therefore, we used RGB and event cameras simultaneously. There are two main approaches for gathering datasets. The first involves recording the 3D model with two cameras to obtain RGB frames and event signals and acquire ground-truth normals with a 3D scanner. Another method is to collect a 3D model printed with a 3D printer and acquire ground-truth normals after matching the camera settings in Blender~\citep{blender} with the experimental camera settings. We adopted the latter method to obtain ground-truth normals with bare effort.

Conventional photometric stereo methods collect datasets for all main lights by fixing them, turning on the main lights individually, and then recording them. In contrast, event signals have a continuous property that represents uninterrupted light changes from nearby pixels. Since the main light source trajectories change as the light moves, a high-speed RGB camera is required to capture light movements. We used a Grasshopper 3 USB3~(GS3-U3-51S5C-C) RGB camera operating with a global shutter that exposes all pixels to the light simultaneously and a DAVIS 346 mono event camera. The maximum resolution is $2448 \times 2048$, and the maximum frame rate with this resolution setting is 75~FPS. To increase the camera capturing speed, we slightly reduced the resolution to $1760 \times 1600$ at 90~FPS. By contrast, the event camera resolution is $346 \times 260$, generating event signals and 30~FPS grayscale frames. We turned off all functions that adjust the exposure, white balance, and gain values automatically and manually set them with specific values. In addition, we used KOWA~(LM12JC1MS) lens, of which the focal length is 12~mm for the RGB camera and 16~mm focal lengths KOWA~(LM16JC1MS) lens for the event camera. To ensure both cameras capture the object(s) as closely as possible, we used lenses with different focal lengths and field of view~(FOV). When the image plane is transformed with the settings, the occluded area is reduced significantly. Subsequently, we minimized the effect of the radial distortion created by the non-linearity of the lenses by applying the coefficients of the lens distortion model in \ref{sec:equations}.

% in \ref{sec:equations} -> supplementary material

\begin{figure}[t]
    \centering
    \includegraphics[width=0.475\textwidth]{Pedestal.pdf}
    \caption{Red circle indicates the pedestal that upholds and fixes the object.}
    \label{fig:Pedestal}
    %\vspace{-5mm}
\end{figure}
To obtain the data, we printed 23 pieces of 3D models for the training dataset and 10 pieces of 3D models for the test dataset with a 3D printer. The 3D model was rescaled to make all models normalized to the size of 100~mm among width, length, and height. It is challenging to represent a 3D model with a synthetic world coordinate system because there is no angle information with respect to the world coordinates. Therefore, to fix the 3D model, we printed a square pedestal, cylindrical column, and 3D model simultaneously, as indicated by the red circle in Fig.~\ref{fig:Pedestal}. A pedestal makes it easier to map the 3D model in the real world to a synthetic world coordinate system. Subsequently, we drilled a hole with a square pedestal in a table and fixed the 3D model to the hole. Both cameras were set 650~mm above the experimental table to obtain data from the center point of the hole. Additionally, to obtain the light directions for each 3D model, we used a chrome ball with a radius of 35~mm near the 3D model. Fig.~\ref{fig:setting} shows the experimental setting.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\textwidth]{setting.pdf}
    \caption{Our experiment setting. The set consists of a tripod with the fixed event camera and the RGB camera, an experimental table, a square, and a chrome ball. The red line is the distance between the event camera and the experimental table, and the blue line is the distance between the RGB camera and the experimental table.}
    \label{fig:setting}
\end{figure}
We created a dataset for an indoor environment containing several ceiling lamps. Moreover, we used both RGB and event cameras to record 150 frames of each 3D model as light trajectories, created by hand, moved around the object. The RGB frames and event signals were incorporated into the RGB event pair dataset for one 3D model. To obtain the ground-truth surface normals of this dataset, the 3D model captured by the event camera should be matched with the 3D model from the synthetic environment created by Blender. To match the area of the captured 3D model, we set the elements of the camera, lens, and distance between the camera and the 3D model in Blender, similar to the experimental setting. Additionally, we set the scale, location, and rotation of the 3D model to be the same as those of the 3D model in the captured image. The 3D model was fixed with a pedestal in the real world, mapping easily to a synthetic world coordinate system by quickly determining the scale, location, and rotation ideal values of the 3D model to use in world coordinates. A setting similar to the real world was first set up in Blender, and then we acquired the ground-truth surface normals and segmentation masks with Vision Blender~\citep{cartucho2020visionblender}, which is an add-on in Blender.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\textwidth]{light_direction.pdf}
    \caption{Determination of light direction. Approach to get light direction indicates on 3 Dimensional space in (a) and 2 Dimensional plane in (b).}
    \label{fig:lightdirection}
\end{figure}
Finally, to obtain the light directions, we tracked the main light using a chrome ball recorded from the RGB camera. To explain this process briefly, we use the bisector property. In our case, the direction~$\textbf{N}$ from the center of the chrome ball to the highlight point on the chrome ball is the bisector. It separates the center between light direction~$\textbf{L}$ and viewing direction~$\textbf{R}$. This process is illustrated in Fig.~\ref{fig:lightdirection} (a).

Specifically, we masked all areas of the frame captured by the RGB camera, except for the chrome ball part. The main light recorded on the chrome ball was saturated. Therefore, we masked the saturated parts again, except for the main light. Subsequently, we expressed the main light with a point that averaged the values of the saturated coordinates in a masked RGB frame. The point about the main light coordinate is transformed into a camera-based world coordinate system using the camera parameters. The main light coordinate, as well as the coordinates of the chrome ball diameter endpoints, and chrome ball center were transformed into a camera-based world coordinate system. Assuming the chrome ball radius is $r_{\textbf{s}}$, we indicate each value in Fig.~\ref{fig:lightdirection} (b) using Eq.~\ref{input:figureld}.
\begin{equation}
\begin{aligned}
d_{\textbf{s}} &= r_{\textbf{s}}/\sin{\beta} \\
d_{1} &= d_{\textbf{s}}\sin{\gamma} \\
d_{2} &= d_{\textbf{s}}\cos{\gamma} \\
d_{3} &= \sqrt{{r_{\textbf{s}}}^{2}-{d_{1}}^{2}} \\
d_{\textbf{h}} &= d_{2}-d_{3} \\
\end{aligned}
\label{input:figureld}
\end{equation}
Using this equation, we obtained $d_{\textbf{h}}$ which is the distance between $\textbf{O}_{camera}$ and highlight point position~$\textbf{h}$ that light is illuminated on the chrome ball using $d_{2}$ and $d_{3}$. Using $d_{\textbf{h}}$ and the coordinates of the highlighted point in the image plane, we obtained the coordinate~$\textbf{h}$ in the camera-based coordinate system. Additionally, we acquired $\textbf{R}$ and $\textbf{N}$ with coordinates by using Eq.~\ref{input:centerandhighlight}. In this equation, $\textbf{s}$ is the center of the chrome ball position and $\text{unit}$ is a function making vector to the unit normal vector.
\begin{equation}
\begin{aligned}
\textbf{R} &= (\textbf{h} - \textbf{O}_{camera})/d_{\textbf{h}} \\
\textbf{N} &= \text{unit}(\textbf{h} - \textbf{s})
\end{aligned}
\label{input:centerandhighlight}
\end{equation}
Finally, we obtained the light direction $\textbf{L}$ using Eq.~\ref{input:lightdirection}.
\begin{equation}
\textbf{L} = 2(\sum{\textbf{N} \odot \textbf{R}})\textbf{N} - \textbf{R}
\label{input:lightdirection}
\end{equation}


\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\textwidth]{dataset.pdf}
    \caption{Samples of the datasets. This figure shows one of the 150 frames for each model, image event signals created with voxel grids, and ground truth surface normal map. All datasets are described in \ref{sec:datasets}.}
    \label{fig:dataset}
\end{figure}
\subsection{Train dataset}
Learning-based methods require training datasets to estimate the surface normal. We used 3D models introduced in \citep{chen2018ps} and \citep{ikehata2018cnn} as the training datasets. Among the 3D models introduced in CNN-PS, some 3D models that could not be accessed were replaced with other 3D models that had royalty-free licenses from the internet. Our training datasets are classified into three categories: smooth blobby, complex sculpture, and mixed others.

The Blobby dataset~\citep{johnson2011shape} is smooth overall, which implies that the depth difference of the surface changes smoothly over a wide range. Using only this dataset, it is difficult for the model to learn the detailed parts and exceptional intensity changes. To reduce bias, we used the Sculpture dataset~\citep{zisserman2017silnet} consisting of 307 sculptures with opposite properties. We employed a part of the 3D models that are introduced as the most complicated in \citep{chen2018ps}. Although we used an additional dataset for model learning, each dataset had obvious properties: smooth or complicated. For this reason, we included more 3D models with royalty-free licenses.

In conclusion, we gathered data under various conditions from three training datasets. Our training datasets help train the model to estimate the surface normal accurately, even with intensities impaired by ambient illumination. Some examples of training datasets given in Fig.~\ref{fig:dataset} are as follows; Blob09 in the Blobby dataset, Sculpture04 in the Sculpture dataset, and Horseman in the Others dataset. We applied a CLAHE image processing technique to enhance the clarity of the RGB frames and better show the shapes of the objects in the figure. The details of the datasets are provided in \ref{sec:datasets}. The training dataset excludes Blob05, Sculpture01, and Sculpture06, where large pixel errors occurred because of the mismatch in the translation from the RGB frames to the event camera plane.

\subsection{Test dataset}
The DiLiGenT dataset~\citep{shi2016benchmark} was created as a benchmark dataset for photometric stereo in the real world. The DiLiGenT dataset contains RGB frames, light information, and high-quality ground truth normals. These were acquired from 10 3D models with complex reflectance effects of various shapes to evaluate non-Lambertian photometric stereo. However, we require both RGB frames and event signal data. Our data acquisition system retrieves RGB frames, light information, high-quality ground truth normals, and even event signals using the 10 3D models in the DiLiGenT dataset. The DiLiGenT dataset was created in a darkroom environment, whereas the dataset we used was created in the wild environment where ambient light illumination exists. As part of the example, the reading 3D model is shown in Fig.~\ref{fig:dataset}.