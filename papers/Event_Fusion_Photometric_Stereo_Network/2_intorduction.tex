Photometric stereo~\citep{woodham1980photometric} is an important computer vision task that estimates the surface normals of a real object by observing the reflections of light sources. 
The ideal photometric stereo method adopts a darkroom setting to avoid ambient illumination and assumes a Lambertian reflection of the object's surface. 
However, because Lambertian surface objects are impossible to obtain in real-world scenarios, several works~\citep{ikehata2012robust,ikehata2014photometric,santo2017deep,ikehata2018cnn,chen2018ps,logothetis2021px,zheng2019spline} have exploited non-Lambertain objects to approximate the real world. % 여기

To fully model the real-world environment, recent works~\citep{hung2015photometric, hold2019single} have additionally considered  ambient light environments for surface normal estimation. Unlike the studies that utilized only the main light source, these studies used all light sources illuminated on the object. It is difficult to compute the light source information in an ambient light environment because ambient illumination on the surface causes saturation, which is over the pixel intensity value range of expression on the frame. Moreover, these methods fail to precisely predict the surface normal in the wild because they cannot accurately extract essential light information and use approximation methods or contain an error range.

Generally, photometric stereo uses RGB cameras. However, RGB cameras have a limited dynamic range. A limited dynamic range causes saturation of pixel intensities, and hence RGB cameras cannot represent light source information completely. Additionally, it is difficult to exclusively capture the effect of the main light source because all the pixels in the frame update their intensities each time a camera is triggered. In this study, we aimed to overcome these limitations of RGB cameras and improve the surface normal estimation in an ambient light environment by incorporating an event camera, similar to \citep{lichtsteiner2008128, posch2010qvga, brandli2014240}. The notable advantages of the event camera are its sensitivity to small changes in light intensity, high dynamic range, and high temporal resolution. The event signals are captured asynchronously when the change in light intensity exceeds a certain threshold. Accordingly, an event camera is useful for capturing sensitive changes in light intensities due to the movement of the main light in the photometric stereo task. Unlike an RGB camera, an event camera can acquire the necessary light source information more accurately and precisely, even if only the main light direction is used in the ambient light setting.

Event signals occur from the relative values of the light intensities. Therefore, we need absolute light intensities, which can be the standard, such as RGB frames. So, we incorporated RGB frames and event camera signals. The event camera receives information asynchronously, whereas the RGB camera captures the scene synchronously. More specifically, the event signal consists of pixel coordinates, time stamps, and polarity, which express the events in the scene. Thus, asynchronous event signals cannot be fused directly with RGB camera frames without appropriate fusion methods. To adequately combine the advantages of each camera, there are additional preprocessing approaches~\citep{rebecq2017real,innocenti2021temporal,gehrig2019end} to match the event signals to a voxel grid. A voxel grid consisting of accumulated polarities in each pixel of several channels divided into uniform cycles was used to construct the frame-like arrays. Inspired by existing approaches~\citep{rebecq2017real,innocenti2021temporal,gehrig2019end} to represent event signals with a voxel grid, we introduce a fusion method to fuse the two heterogeneous camera signal features to deal with heterogeneous modalities. Our fusion method represents light source information in a complex manner.

Consequently, we introduce an Event Fusion Photometric Stereo Network~(EFPS-Net) to utilize RGB frames and event signals. Unlike dense RGB frames, event signals that only occur when the light intensity change is over the threshold produce sparse data. To reduce the density gap between event signals and RGB frame, we propose a novel interpolation network that interpolates the sparse observation maps~\citep{ikehata2018cnn} generated with event signals to make them similar to the dense observation map generated by RGB frames. Subsequently, the observation fusion module fuses four observation maps generated with the RGB camera and one interpolated observation map generated by the event camera to complement the light source information of each observation map. The features consisting of observation maps generated from the two cameras achieve better performance in predicting the surface normal than using only one modality feature by complementing each disadvantage.

To show event signals that contain information on light sources, we curate an RGB-event paired dataset, that is recorded in an ambient light environment with an RGB camera and an event camera. Our proposed method outperforms other state-of-the-art methods that use an RGB camera on real datasets. To the best of our knowledge, our proposed method is the first work to fuse RGB and event signals for photometric stereo tasks. From our extensive experimental results, utilizing event signals along with RGB frames is much more effective in photometric stereo tasks under ambient light conditions than using only an RGB camera.

In this paper, our contributions are summarized as follows:
\begin{itemize}
    \item We introduce a novel method called EFPS-Net that can solve the real-world photometric stereo problem in the wild environment with existing ambient lights. 
    \item We propose a technique to interpolate the observation map from event signals that represent dense light information than prior methods.
    \item Our methods perform fusion between the RGB-based observation maps and the event interpolated observation map to predict the surface normal, especially the edge.
    \item We curate the RGB event pair dataset made using an RGB camera and an event camera in a real-world environment where ambient light exists, and we show that our method is the state-of-the-art method when using event signals.
\end{itemize}