\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{flow.pdf}
    \caption{Overview of Event Fusion Photometric Stereo Network~(EFPS-Net). EFPS-Net consists of three parts: (i)~Event Interpolation Network~(EI-Net) which interpolates the sparse event observation maps~$O_{e}$,~(ii) Observation Fusion Module~(OFM) that fuses five observation maps, and~(iii) Surface Normal Estimation Network~(SNE-Net) which outputs surface normal vector given fused observation maps.}
    \label{fig:modelarchitecture}
\end{figure}
In this section, we describe our proposed pipeline, i.e., Event Fusion Photometric Stereo Network~(EFPS-Net) shown in Fig.~\ref{fig:modelarchitecture}, which utilizes fused information from RGB frames and event signals.

\subsection{Obseravation map}
An observation map is a feature map that consists of light intensities from each direction.
More specifically, To compute the observation map~$O_{c}$ of ${m\times m}$ size for channel~$c$, we project the $j$~th intensity~$i_{j_{c}}$ of light direction~$(l^{x}_{j},~l^{y}_{j},~l^{z}_{j})$ from the shaped unit hemisphere onto the 2D plane, as defined in Eq.~\ref{input:observationmap}.

\begin{equation}
O_{c}\left( \left \lfloor m\frac{l^{x}_{j}+1}{2}\right \rfloor,\left \lfloor m\frac{l^{y}_{j}+1}{2}\right \rfloor \right) = i_{j_{c}}
\label{input:observationmap}
\end{equation}
Since, at a time, this approach only deals with the necessary part of pixels in either RGB frames or event signals, it is computationally efficient. To fuse two different modalities effectively with observation maps, we converted RGB frames into observation maps~$O_{r,g,b,n}$ and transformed raw event signals into sparse event observation maps.  
In particular, we utilize the method from PX-Net~\citep{logothetis2021px}, which generates RGB observation maps~$O_{r,g,b}$ from RGB frames and $O_{n}$ which is a normalized map of RGB observation maps as defined in Eq.~\ref{input:normalize} to obtain $O_{r,g,b,n}$.


\begin{equation}
O_{n} = \frac{O_{r} + O_{g} + O_{b}}{\text{max}(O_{r} + O_{g} + O_{b})}
\label{input:normalize}
\end{equation}

\subsection{Event Interpolation Network}
To extract useful information from the event signals for EFPS-Net, we propose a novel event signal representation that can also be transformed into an observation map.
The proposed method is fundamentally different from previous representation methods, which simply accumulate the polarity along the time at each pixel coordinate.
We divided the polarities by $\lambda$ before accumulating generated event signals to ensure a wide distribution and applied a non-linear function such as a hyperbolic tangent.
Subsequently, the processed event signals were located in the voxel grid.
Moreover, we separated polarity into two channels.
One channel represents a positive polarity, which indicates that the light intensity is increased. 
The other channel represents negative polarity, which indicates decreased light intensity.
Consequently, we propose a new separate event voxel grid~$V_{s}$ containing the dimensions of polarity, as defined in Eq.~{\ref{input:separate_voxel_grid}}.
We generated $O_{e}$ with each polarity channel of $V_{s}$ after merging all the time slots.

We note that the event observation maps are sparse because event signals do not compulsorily occur even if the light direction is changed.
Sparse event observation maps are insufficient for estimating the surface normal.
Therefore, we used an EI-Net to interpolate the event observation maps into a dense normalized observation map that contains sufficient information.

$$
V\rightarrow \begin{cases}
V(x,y,\delta,0) = \sum^{k(\delta+1)}_{t_{n}=k\delta}{\frac{1}{\lambda}}, & \text{ if } p(x,y,t_{n})=1. \\
V(x,y,\delta,1) = \sum^{k(\delta+1)}_{t_{n}=k\delta}{\frac{1}{\lambda}}, & \text{ if } p(x,y,t_{n})=-1.
\end{cases}
$$
where $B$ is the channel of voxel grids generated during $\Delta T$, which is one period. One channel of voxel grid~$k$ indicates $\frac{\Delta T}{B}$.


\begin{equation}
V_{s} = \frac{\text{exp}^{V}-\text{exp}^{-V}}{\text{exp}^{V}+\text{exp}^{-V}}
\label{input:separate_voxel_grid}
\end{equation}

\begin{figure}[t]
    \centering
    \subfloat[]{
    \includegraphics[width=0.475\textwidth]{blocks_a.pdf}
    }
    \subfloat[]{
    \includegraphics[width=0.475\textwidth]{blocks_b.pdf}
    }
    \caption{Details of Event Interpolation Network~(EI-Net) blocks and Surface Normal Estimation Network~(SNE-Net) blocks. First, EI-Net blocks, depicted in (a), have two blocks which are Down Block encoding features and Up Block up-sampling observation map. Next, SNE-Net blocks consist of DB~(Dense Block) and TB~(Transition Block) as shown in (b). DB is same as \citep{logothetis2021px}, but TB contains an additional batch normalization layer.}
    \label{fig:blocks}
\end{figure}

EI-Net consists of a head part, two Down blocks, sixteen Residual blocks, two Up blocks, and follow by a pred part.
The head part is composed of a head convolution layer, batch normalization layer, and ReLU activation function.
This part amplifies the sparse event observation map to multiple channels, and the Down Block encodes them while reducing the size by half.
As shown in Fig.~\ref{fig:blocks}~(a), the Down Block comprises $5\times5$ kernel convolution layer, a batch normalization layer, and a leaky ReLU activation function.
After obtaining encoded feature maps from the last Down Block, sixteen residual blocks deal with the output feature maps to interpolate the sparse intensity features.
As expressed in Eq.~\ref{input:residual_block}, the residual block consists of a $3\times3$ kernel convolution layer, a batch normalization layer, and a ReLU activation function.
It derives more fluent-intensity output feature maps~$M_{out}$ from the sparse input feature maps~$M_{in}$.
To decode resized inputs, we used Up Blocks, shown in Fig.~\ref{fig:blocks}~(a), which upscale the last layer output of the residual block's $M_{out}$ to match the EI-Net's input resolution of EI-Net.
Finally, we transformed them into an interpolated event observation map using the pred part, which consists of a pred convolution layer and adjusts values between 0 and 1 using a sigmoid activation function to match the range of input values.

\begin{equation}
M_\text{out} = \text{ReLU}(\text{BN}(f^{3 \times 3}(M_\text{in}))) + M_\text{in}
\label{input:residual_block}
\end{equation}

\subsection{Observation Fusion Module}
In our model architecture, the SNE-Net utilizes fused observation maps~$\hat{O}$ as the input.
To acquire fused observation maps, we gathered five new observation maps~$\tilde{O}$ with point-wise convolution.
This operation compensates for each observation map by using other observation maps.
Subsequently, we performed element-wise multiplication for each channel on the OFM input~$O$ after setting it in the 0 to 1 range with the sigmoid activation function.
It injects interacted observation map features at the input observation maps that were concatenated with $O_{r,g,b,n}$ and $O_{\hat{e}}$.
Consequently, we acquire $\hat{O}$, complemented by each channel of the observation map, as input to SNE-Net.
Overall, the OFM implements the process illustrated in Fig.~\ref{fig:ofm}.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.5\textwidth]{ofm.pdf}
    \caption{Overview of Observation Fusion Module~(OFM), which produces five refined observation maps using convolution layers.}
    \label{fig:ofm}
\end{figure}

\subsection{Surface Normal Estimation Network}
%We aim to confirm how beneficial event signals information is to existing photometric stereo methods.
SNE-Net is inspired by PX-Net~\citep{logothetis2021px}, which uses an observation map. SNE-Net uses a large batch size because the observation map size is small. When the model has a large batch size, appropriate batch normalization layers are especially effective. Thus, we attached the batch normalization layer~(BN) to our network between the convolution layer and the ReLU activation function in the Transition Block~(TB), as shown in Fig.~\ref{fig:blocks}~(b). 
Finally, SNE-Net estimates the surface normal from fused observation maps generated from the OFM.

\subsection{Loss function}
EFPS-Net estimates the surface normal per pixel on an object using numerous light directions and intensities represented by image frames and event signals according to each light direction. We used the scale-invariant loss~$\mathcal{L}_{e}$ Eq.~(\ref{input:scale-invariant})~\citep{eigen2014depth} to interpolate the sparse event observation maps~$O_{e}$ to the dense normalized observation map~$O_{n}$. Since event signals represent relative light information based on the absolute intensity of RGB frames, we interpolated them to $O_{n}$.

\begin{equation}
\mathcal{L}_{e} = \frac{1}{n}\sum_{i}R^{2}_{i}-\frac{1}{n^{2}}(\sum_{i}R_{i})^{2},
\label{input:scale-invariant}
\end{equation}
where $R$ is computed using $R_{i} = O_{\hat{e}(i)} - O_{n(i)}$.\\

Moreover, we applied the Mean Angular Error~(MAE) loss function defined in Eq.~\ref{input:mae}. This loss function optimizes the error between the ground truth surface normal~$\textbf{n}$ and the predicted surface normal~$\hat{\textbf{n}}$.

\begin{equation}
\mathcal{L}_{n} = \cos^{-1}(\textbf{n}\cdot \hat{\textbf{n}})
\label{input:mae}
\end{equation}

In summary, EFPS-Net is optimized by combining loss~$\mathcal{L}_{e}$ and loss~$\mathcal{L}_{n}$, defined as:
\begin{equation}
\mathcal{L}_{total} = \mathcal{L}_{e} + \mathcal{L}_{n}
\label{input:total}
\end{equation}