\subsection{Implementation details}
The output resolution of the event camera is $346 \times 260$ pixels, it is cropped to $208 \times 208$ pixels. However, the number of separate event voxel grids is one less than the number of frames because event signals are data generated while two frames are triggered. For this reason, we used the data made with 145 lights, excluding 5 lights from 150 lights, and the model constructed with PyTorch for the experiments. We trained EFPS-Net with Tesla-V100 GPU for 20 epochs with the adam optimizer, after setting the initial learning rate to 0.001, batch size to 2048, and resolution of the observation map to $32 \times 32$. Moreover, for EFPS-Net training, we used cosine annealing for the learning rate scheduler.

\subsection{Evaluation on test dataset}
\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\textwidth]{result_main.pdf}
    \caption{Qualitative result of learning-based methods on the RGB-event pair test dataset.}
    \label{fig:mainresult}
\end{figure}
\begin{table*}[!h]%[!hb]
\caption{Quantitative result of methods on the RGB-event pair test dataset. The underlined values are the second best-performing values, and the bolded values are the first best-performing values.}
\label{tab:all_result}
\resizebox{\textwidth}{!}{%
\renewcommand*{\arraystretch}{1.5}
\begin{tabular}{lccccccccccc}
\cline{1-12}
\multicolumn{1}{l}{Method} & Ball & Bear & Buddha & Cat  & Cow  & Goblet & Harvest & Pot1  & Pot2  & Reading & \multicolumn{1}{l}{Avg.}   \\

\hline
\multicolumn{1}{l}{N-LS~\citep{woodham1980photometric}} & 30.20 & 33.33 & 37.88 & 29.93 & 30.54 & 39.83 & 37.23 & 38.75 & 28.21 & 36.38 & \multicolumn{1}{l}{34.23} \\
\multicolumn{1}{l}{N-L1~\citep{ikehata2012robust}} & 30.31 & 31.88 & 37.18 & 29.63 & 29.33 & 39.24 & 36.49 & 37.13 & 27.98 & 36.55 & \multicolumn{1}{l}{33.57} \\
\multicolumn{1}{l}{N-SBL~\citep{ikehata2012robust}} & 30.99 & 31.61 & 37.57 & 29.83 & 30.23 & 40.37 & 38.03 & 37.97 & 28.26 & 36.77 & \multicolumn{1}{l}{34.06} \\
\multicolumn{1}{l}{PS-LS~\citep{ikehata2014photometric}} & 32.84 & 30.51 & 39.44 & 30.82 & 32.34 & 46.58 & 39.04 & 39.18 & 29.96 & 37.46 & \multicolumn{1}{l}{35.61} \\
\multicolumn{1}{l}{L-PLSBL~\citep{ikehata2014photometric}} & 36.25 & 31.40 & 39.72 & 31.80 & 31.20 & 45.53 & 38.41 & 38.23 & 31.81 & 37.51 & \multicolumn{1}{l}{36.19} \\
\cline{1-12}
\multicolumn{1}{l}{PS-FCN~\citep{chen2018ps}} & 34.54 & 36.26 & 38.05 & 35.01 & 30.64 & 30.23 & 31.51 & 30.38 & 29.19 & 30.39 & \multicolumn{1}{l}{32.62} \\
\multicolumn{1}{l}{CNN-PS~\citep{ikehata2018cnn}, K=10} & \ul{12.38} & 20.09 & 29.18 & 23.18 & 13.89 & 19.98 & \ul{29.12} & \ul{19.38} & \ul{15.56} & 22.84 & \multicolumn{1}{l}{20.56} \\
\multicolumn{1}{l}{PX-Net~\citep{logothetis2021px}, K=10} & 13.40 & \ul{17.96} & \ul{25.68} & \ul{21.11} & \ul{11.46} & \ul{17.61} & 29.56 & 19.84 & 15.94 & \ul{20.48} & \multicolumn{1}{l}{\ul{19.30}} \\
\cline{1-12}
\multicolumn{1}{l}{Ours, K=10} & \textbf{10.14} & \textbf{14.20} & \textbf{25.11} & \textbf{18.74} & \textbf{10.29} & \textbf{16.79} & \textbf{28.57} & \textbf{19.34} & \textbf{13.71} & \textbf{20.12} & \multicolumn{1}{l}{\textbf{17.71}} \\
\hline

\end{tabular}%
}
\end{table*}

Photometric stereo tasks conventionally use Mean Angular Error~(MAE) to estimate performance. Therefore, we used the average MAE as a metric, with a value indicated as an angular error. The angular error is the difference in the angle between the predicted normal and ground-truth normal corresponding to each pixel coordinate for an object. We performed experiments with RGB event pair datasets to prove that event signal data are effective for detecting even features. In particular, features are interrupted by ambient illumination in the real world. The MAE scores of the experimental results for each model are listed in Table~\ref{tab:all_result}. First, it is separated into three sections. The top section is the result of traditional methods, and the middle section is the result of learning-based methods. The methods in the two sections predict the surface normals using only RGB frames and light directions. By contrast, the bottom section describes our method of predicting surface normals using not only RGB frames and light directions but also event signals. Traditional methods achieved low prediction performance when changes were negligible. As shown in Fig.~\ref{fig:mainresult}, PS-FCN could not represent the surface normal maps when displayed in the tangent space. CNN-PS predicted surface normals appropriately, but the deviation in the normal values is small; therefore, CNN-PS cannot be represented in various ranges. Next, PX-Net predicted surface normals with more wide ranges, but PX-Net showed sharp changes when the color changed. Finally, EFPS-Net achieved similar ground-truth normals with spontaneous surface normal changes and state-of-the-art performance.

\subsection{Ablation studies}
This section discusses the experimental results of the two approaches, which contribute considerably to the performance improvement of EFPS-Net. The first is the rotational pseudo-invariant introduced in CNN-PS~\citep{ikehata2018cnn}, and the second is our model architecture. It is desirable for the MAE to be as close to 0 as possible.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\textwidth]{pseudoinvariance_big.pdf}
    \caption{Mean Angular Error~(MAE) scores for each model with different rotational pseudo-invariances.}
    \label{fig:pseudoinvariance}
\end{figure}
\begin{table*}[!h]%[!hb]
\caption{To show the results on the effect of rotational pseudo-invariance. The bold values are the first best-performing values.}
\label{tab:ab1}
\resizebox{\textwidth}{!}{%
\renewcommand*{\arraystretch}{1.5}
\begin{tabular}{lccccccccccc}
\cline{1-12}
\multicolumn{1}{l}{Method} & Ball & Bear & Buddha & Cat  & Cow  & Goblet & Harvest & Pot1  & Pot2  & Reading & \multicolumn{1}{l}{Avg.}   \\

\hline
\multicolumn{1}{l}{CNN-PS~\citep{ikehata2018cnn}, K=1} & 18.68 & 24.58 & 29.26 & 29.75 & 15.29 & 22.74 & 31.18 & 20.85 & 17.55 & 23.62 & \multicolumn{1}{l}{23.35} \\
\multicolumn{1}{l}{CNN-PS~\citep{ikehata2018cnn}, K=5} & 14.27 & 21.60 & 30.79 & 24.31 & 15.95 & 24.44 & 30.74 & 21.88 & 15.60 & 24.07 & \multicolumn{1}{l}{22.37} \\
\multicolumn{1}{l}{CNN-PS~\citep{ikehata2018cnn}, K=10} & 12.38 & 20.09 & 29.18 & 23.18 & 13.89 & 19.98 & 29.12 & 19.38 & 15.56 & 22.84 & \multicolumn{1}{l}{20.56} \\
\cline{1-12}
\multicolumn{1}{l}{PX-Net~\citep{logothetis2021px}, K=1} & 13.23 & 17.53 & 27.48 & 22.08 & 11.38 & 21.79 & 30.02 & 22.01 & 19.76 & 20.66 & \multicolumn{1}{l}{20.60} \\
\multicolumn{1}{l}{PX-Net~\citep{logothetis2021px}, K=5} & 12.44 & 19.34 & 25.99 & 23.68 & 11.00 & 18.41 & 29.55 & 18.69 & 16.80 & 20.13 & \multicolumn{1}{l}{19.60} \\
\multicolumn{1}{l}{PX-Net~\citep{logothetis2021px}, K=10} & 13.40 & 17.96 & 25.68 & 21.11 & 11.46 & 17.61 & 29.56 & 19.84 & 15.94 & 20.48 & \multicolumn{1}{l}{19.30} \\
\cline{1-12}
\multicolumn{1}{l}{Ours, K=1} & 12.86 & 16.20 & 27.90 & 19.50 & 11.83 & 22.59 & 30.48 & 21.65 & 20.17 & 22.06 & \multicolumn{1}{l}{20.53} \\
\multicolumn{1}{l}{Ours, K=5} & 10.74 & \textbf{14.00} & 26.07 & \textbf{18.03} & 11.56 & 17.54 & 29.43 & 20.74 & 14.71 & 20.22 & \multicolumn{1}{l}{18.30} \\
\multicolumn{1}{l}{Ours, K=10} & \textbf{10.14} & 14.20 & \textbf{25.11} & 18.74 & \textbf{10.29} & \textbf{16.79} & \textbf{28.57} & \textbf{19.34} & \textbf{13.71} & \textbf{20.12} & \multicolumn{1}{l}{\textbf{17.71}} \\
\hline

\end{tabular}
}
\end{table*}
First, rotational pseudo-invariance augments data K times by rotating as much as $360\,^{\circ}$ divided by K, instead of interpolating the observation map. We set K to 1, 5, and 10 to confirm the extent to which this method assists. In Fig.~\ref{fig:pseudoinvariance}, all models indicate a performance improvement with as lower average MAE for all entire objects. As shown in Table~\ref{tab:ab1}, each model performs the best when K is 10. As a result, we verified that rotational pseudo-invariance leads to better performance by augmenting the data for a specific angle that could not be generated.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\textwidth]{result_ab2.pdf}
    \caption{Visualization result of error maps for angular error to compare each of our methods. Each of our methods shows the result with three objects. The closer the error color is to red, the error value reaches an angle of $60\,^{\circ}$, and the closer it is to blue, the error value reaches an angle of $0\,^{\circ}$.}
    \label{fig:ab2result}
\end{figure}
\begin{table*}[!h]%[!hb]
\caption{Results of our methods on the RGB event pair test dataset. The bold values are the best values in terms of performance. The w/o $O_{e}$ indicates that $O_{e}$ are not contained as input, while w $O_{e}$ indicates that $O_{e}$ are included as input.}
\label{tab:ab2}
\resizebox{\textwidth}{!}{
\renewcommand*{\arraystretch}{1.5}
\begin{tabular}{lccccccccccc}
\cline{1-12}
\multicolumn{1}{l}{Method} & Ball & Bear & Buddha & Cat  & Cow  & Goblet & Harvest & Pot1  & Pot2  & Reading & \multicolumn{1}{l}{Avg.}   \\

\hline
\multicolumn{1}{l}{PX-Net~\citep{logothetis2021px} with BN~(w/o $O_{e}$), K=10} & 12.29 & 14.47 & 25.25 & 18.62 & 10.38 & 17.85 & 29.91 & 19.40 & 16.47 & 19.98 & \multicolumn{1}{l}{18.46} \\

\hline
\multicolumn{1}{l}{PX-Net~\citep{logothetis2021px} with BN~(w $O_{e}$), K=10} & 11.04 & 15.06 & 25.15 & 19.01 & 11.37 & 17.98 & \textbf{28.23} & 19.65 & 14.56 & 20.08 & \multicolumn{1}{l}{18.21} \\
\multicolumn{1}{l}{Ours with Recon, K=10} & 10.63 & 14.49 & 25.27 & \textbf{17.63} & 10.79 & \textbf{16.61} & 28.75 & 19.38 & 14.67 & 20.04 & \multicolumn{1}{l}{17.83} \\
\multicolumn{1}{l}{Ours with OFM, K=10} & 11.37 & 14.67 & 25.20 & 18.07 & 10.77 & 16.91 & 28.70 & \textbf{19.09} & 15.10 & \textbf{19.94} & \multicolumn{1}{l}{18.01} \\
\cline{1-12}
\multicolumn{1}{l}{Ours, K=10} & \textbf{10.14} & \textbf{14.20} & \textbf{25.11} & 18.74 & \textbf{10.29} & 16.79 & 28.57 & 19.34 & \textbf{13.71} & 20.12 & \multicolumn{1}{l}{\textbf{17.71}} \\
\hline

\end{tabular}
}
\end{table*}
Next, we conducted an experiment to determine the contribution of the components in our model architecture to improvements in model performance. To briefly introduce Table~\ref{tab:ab2}, PX-Net~\citep{logothetis2021px} with BN~(baseline model) represents Surface Normal Estimation Network~(SNE-Net) as the baseline model. Additionally, we provide qualitative results in Fig.~\ref{fig:ab2result}. This model uses observation maps concatenated with observation maps~$O_{r,g,b,n}$ generated from RGB frames and observation maps~$O_{e}$ generated from event signals as input data. Ours with Recon is an EFPS-Net approach, except Observation Fusion Module~(OFM). The last one, Ours with OFM, is an EFPS-Net approach, except for Event Interpolation Network~(EI-Net). Experimental results indicate that using a baseline model provides better performance, with lower MAE scores than that of PX-Net. Therefore, we confirm that using the batch normalization layer is beneficial in the case of a large batch size when acquiring feature maps from TB~(Transition Block). Moreover, Ours with recon showed greater performance, achieving a lower MAE score $0.38\,^{\circ}$ less than that achieved by using only the baseline model. However, in Ours with OFM, the MAE was $0.2\,^{\circ}$ lower than that of the baseline model. This result verifies that OFM fuses observation maps by compensating for each other. As shown in Fig.~\ref{fig:ab2result}, the errors are less distributed, especially in the angular part and boundary of the object. Finally, EFPS-Net achieved an MAE score $1.5\,^{\circ}$ less than that of the baseline model by appropriately using the advantages of the two components.