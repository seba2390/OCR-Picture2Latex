\subsection{Photometric stereo}
Photometric stereo estimates the surface normals of the local geometry of an object using at least three light sources and frames from each light source. In particular, the object must be a Lambertian reflectance surface to use a diffuse Bidirectional Reflectance Distribution Function~(BRDF). 
Perfectly diffuse BRDF cannot exist in the real world because real-world environments violate these constraints. Especially real-world objects have non-Lambertian reflectance surfaces, which are affected by global illumination effects such as ambient lights and casting shadows. When using only an RGB camera, we cannot know the effects of the main light on an object in an ambient light environment. Thus, existing methods~\citep{hung2015photometric, hold2019single} convert all the light information that affects an object into a feature, such as one light information or learnable parameter. However, it contains unnecessary ambient light information. Moreover, the pixel intensities and reflectance directions in each light direction are quite different depending on the components of the object and sure
face properties. Therefore, there are various BRDFs based on reflectance. Since the introduction of photometric stereo by Woodam~\citep{woodham1980photometric}, various methods have been proposed that are robust to these external constraints. Notably, the former methods are the two main categories of traditional and learning-based methods.

First, traditional methods attempt to solve the non-Lambertian reflectance problem statistically~\citep{ikehata2012robust,ikehata2014photometric} or use a sophisticated reflectance model \citep{oren1995generalization,hertzmann2003shape,alldrin2008photometric,goldman2009shape}. A statistical method~\citep{ikehata2012robust} adopts a hierarchical Bayesian approximation. This method achieves competitive performance in estimating surface normals even with a non-Lambertian reflectance surface. However, such methods require many frames and handling high-frequency materials poses substantial difficulties. To overcome these requirements, another method~\citep{ikehata2014photometric} was proposed that adopts a piece-wise linear diffuse model to get better performance. Despite these efforts to improve the non-Lambertian reflectance problem, these still fail to overcome global illumination effects in large parts of objects. Additionally, these methods require more improvement to estimate surface normals precisely when several light sources are available.

To resolve the constraints not solved by traditional methods, learning-based methods using neural networks have been researched. The Deep Photometric Stereo Network~(DPSN)~\citep{santo2017deep}, which is the first neural network method, has achieved better performance than other traditional methods in real-world photometric stereo. However, the DPSN must be fixed and it must have the same light directions during the training and testing phase. To solve the DPSN problem, PS-FCN~\citep{chen2018ps} trains and tests with random order input, which consists of a pair of RGB frames and light directions reshaped in the RGB frame representation. However, PS-FCN needs a lot of train data and cannot fuse intensities effectively when the frames have many saturated signals because it uses a max pooling layer to fuse intensities. Another method is CNN-PS~\citep{ikehata2018cnn}, which estimates surface normals utilizing an observation map that represents light information with intensities per light direction captured from each pixel. Unlike CNN-PS, which generates an observation map from grayscale frames, another method~\citep{logothetis2021px} uses fluent light information by generating observation maps from each channel of the frames and averaging them. These approaches fail to predict the surface normal of an area in the object under limited light conditions or when ambient illumination environments. Consequently, these problems cannot be resolved using only an RGB camera because of the lack of light information and interruption of ambient illumination. To fundamentally overcome this problem, we used an event camera that can capture a more diverse range of wider dynamic range of lights than that captured by the RGB camera.

\subsection{Event camera}
The mechanism of an event camera is inspired by the human optic nerve system, resulting in low power consumption, high dynamic range, and significantly low latency~\citep{gallego2020event}. A conventional RGB camera captures light signals using a two-dimensional~(2D) plan sensor and saves them as an array.

However, in an event camera, event signals are captured pixel-by-pixel asynchronously, along with the time stamp and polarity representing the on and off terms.
Moreover, the event camera disregards the ambient illumination. It primarily detects the change in intensity of the main light because it captures the light intensity differences of each pixel on a logarithmic scale. However, event signals are not suitable for neural networks without proper post-processing. Event signals are not detected when there are no changes in scene lighting, making the data sparse when the changes are minimal.
The format of the event signal is similar to that of point clouds, which are a list of multiple data points consisting of three components: pixel coordinates, time stamp, and polarity.
To use event signals effectively with neural networks, numerous representation methods have been proposed utilizing event signals such as graphs~\citep{bi2019graph,bi2020graph}, point clouds~\citep{sekikawa2019eventnet,vemprala2021representation}, frames~\citep{rebecq2017real,innocenti2021temporal,gehrig2019end} or patches~\citep{sabater2022event}. 

Several approaches~\citep{messikommer2022multi,gehrig2021combining,messikommer2022bridging} incorporate the two modalities~(i.e., RGB and event cameras) and use event representation methods in various tasks. Consequently, considerable advantages are achieved by compensating for the disadvantages of each modality. The RGB camera expresses absolute light intensities but operates insufficiently in saturated or slightly exposed parts. Event cameras represent relative light intensities, even saturated or underexposed parts, in RGB cameras. Despite providing different outputs, the two modalities compensate for each other since the RGB frames and event signals information overlap substantially because the two cameras capture the same scene irradiance. Thus, the output of each camera can account for the information missing in the other's output. Their combined use can satisfactorily overcome ambient illumination environments in photometric stereo.