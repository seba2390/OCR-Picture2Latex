\section{Defending DNN with EagleEye}
\label{sec:model}



\begin{figure}
%\hspace{-15pt}
%\centering
  \epsfig{file=figures/arch3.eps, width=90mm}
  \caption{Use case of \system. \label{fig:arch}}
\end{figure}


Next we present \system, an attack-agnostic adversarial tampering analysis engine. Its design is motivated by a set of desiderata, which we believe are expected for practical and effective defense mechanisms.

\begin{itemize}

\item Attack-agnostic defense. It should be universally effective against known and {\em a priori} unseen attacks.

\item Intact \dnn models. It should require no modification to \dnn models, as such changes, especially to \dnn architectures or training procedures, often result in unpredictable system behaviors.

% \item Zero accuracy loss. The defense against adversarial inputs should not impact the \dnn's classification accuracy, which is paramount for security-critical domains (e.g., self-driving cars, biometric authentication, web content filtering).

%  Maintain accuracy: defenses against adversarial samples should not decrease the DNN’s classification accuracy. This discards solutions based on weight decay, through L1, L2 regularization, as they will cause underfitting.
% % • Maintain speed of network:  Impact on training should nevertheless remain limited to ensure DNNs can still take advantage of large training datasets to achieve good accuracies. For instance, solutions based on Jacobian regularization, like double backpropagation [31], or using radial based activation functions [9] degrade DNN training performance.
% • Defenses should work for

\item Light-weight execution. It should incur negligible performance overhead to the \dl-powered system.
%, which matters for the system usability.

%\item Modular design. Its dependency on the existing system components should be minimal, so that it can be readily deployed into varied contexts.
\end{itemize}
%
% As empirically verified in~\myref{sec:measure}, existing defense methods fail to provide attack-agnostic protection. Further, most of them require modifications either to \dnn models or to training processes. For example, \cite{Drucker:1992:nn,Goodfellow:2014:arxiv,Gu:2014:arxiv} propose new \dnn architectures robust against adversarial perturbations; \cite{Huang:2015:arxiv,Shaham:2015:arxiv,Nokland:2015:arXiv,Papernot:2016:sp,Szegedy:2013:arxiv} propose to alter objective functions. Moreover, due to their tight coupling with \dnns, existing defense solutions cannot be easily integrated into \dl-powered systems, which significantly impedes their deployment in real  products or services.

\system satisfies all these desiderata. In its overview (\myref{sec:overview}), we show that \system, following a modular design, requires no modification to \dnn models or training methods; in its detailed description (\myref{sec:adefense}), we demonstrate that \system provides universal defense by making minimal assumptions regarding incoming attacks; in~\myref{sec:evaluation}, we further validate that \system meets the requirement of light-weight execution by evaluating its empirical performance within \dl-powered systems.


\subsection{Overview}
\label{sec:overview}

In contrast of existing solutions, \system takes a completely new route: it attempts to discriminate adversarial inputs; moreover, for suspicious cases, it is instrumented to infer their correct classification outputs. Therefore, \system provides much richer diagnosis information than existing defense solutions.

%Moreover, the distinct designs of \system and existing defenses entail their complementary nature.

%er, delivering even stronger protection for \dnns.


Specifically, as depicted in Figure~\ref{fig:arch}, \system is deployed as an auxiliary module within a \dl-powered system. It exerts minimal interference with the classification task (1). Rather, for the given input $\mbx$ (2) and \dnn (3), it offers on-demand adversarial tampering analysis: (i) it first runs {\em integrity checking} to assess the possibility that $\mbx$ has been maliciously tampered; (ii) if suspicious, it further performs {\em truth recovery} to infer $\mbx$'s correct classification. The analysis result is combined with the \dnn's classification to form a comprehensive report for the operator's decision-making (4). Clearly the design of \system satisfies the desiderata of intact \dnn models. Next we focus on realizing attack-agnostic defense.


% \begin{figure*}
% \hspace{-10pt}
% \epsfig{file=figures/sparsity.eps, width=190mm}
% \caption{Cumulative distribution of intra-class and inter-class critical distance in the benchmark datasets. \label{fig:sparse}}
% \end{figure*}


\subsection{Minimality Principle}
\label{sec:adefense}

Despite their apparent variations, different attacks follow similar design principles, which entail invariant properties shared by adversarial inputs, independent of concrete attacks. In specific, we exploit the
{\em minimality principle} underlying most attack models as the foundations for building attack-agnostic defenses.

Intuitively, to maximize the attack's evasiveness, the adversary often seeks the minimum possible distortion to convert genuine inputs to adversarial ones. Formally,
\begin{definition}[Minimality Principle]
Given the target \dnn $f$, genuine input $\mbx$, and adversarial output $\mbyp$, the attack seeks to solve an optimization problem as:
\begin{equation*}
\min_{\mbr} ||\mbr|| \quad \textrm{s.t.}\quad f(\mbx + \mbr) = \mbyp
\end{equation*}
\end{definition}
For example, \cite{Goodfellow:2014:arxiv,Huang:2015:arxiv} instantiate this problem with $||\cdot||$ defined as $l_\infty$-norm, while \cite{Papernot:2016:eurosp,Carlini:2016:arXiv} consider $l_1$-norm.

%
% \begin{itemize}
% \item {\bf Minimality} Property - In adversarial crafting attacks, to ensure its evasiveness, an adversarial input is often created by perturbing
%  an genuine input with the minimum possible distortion amplitude.
% \item {\bf Sparsity} Property - In the input manifold (i.e., the topological space spanned by all the inputs), the genuine inputs tend to exist as a set of ``islands'' distant from each other; meanwhile, they are densely surrounded by the adversarial inputs.
% \end{itemize}

To understand the entailments of this principle, we first introduce several key concepts (see Figure~\ref{fig:concept}).


\begin{definition}[Boundary]
A \dnn $f$ partitions the input space (the topological space spanned by all the inputs) into non-overlapping regions. The inputs in each region are classified by $f$ into the same class. Adjacent regions are separated by their \underline{boundary}.
\end{definition}

%We omit the reference \dnn when the context is clear.

% For given \dnn $f$, let $\mathcal{L}_\mby$ and $\mathcal{A}_\mby$ respectively be the sets of genuine and adversarial inputs classified as $\mby$ by $f$.
%We say that input $\mbx$ belongs to class $\mby$ if $\mbx \in \mathcal{L}_\mby \bigcup \mathcal{A}_\mby$.



\begin{definition}[Path]
For given inputs $\mbx$, $\mbxp$ with $\mbxp = \mbx + \mbr$, the \pv $\mbr$ encodes a \underline{path} from $\mbx$ to $\mbxp$, of which
%\footnote{In the case of linear crafting attacks, the sequential order of perturbations can be arbitrarily defined.}
the length is defined as the magnitude of $\mbr$, $||\mbr||$.
\end{definition}

%Intuitively, the path length of two inputs reflects their distance in the input manifold space. To study the distributional properties of genuine and adversarial inputs, we introduce the concept of radius.

\begin{definition}[Radius] The path length of an input $\mbx$ to its nearest neighbor $\mbxp$ in another class $\mbyp$
%$\min_{\mbxy \in \mathcal{L}_{\mbyz}} |{\mathcal P}(\mbx, \mbxy)|$,
     is referred to as $\mbx$'s \underline{radius} to class $\mbyp$, denoted by $\rho (\mbx, \mbyp)$.
\end{definition}

% In the case of $\mby = \mbyz$, $\mathrm{d}(\mbx, \mbyz)$ is referred to as the intra-class critical distance. Note that this distance is measured between genuine inputs, for an adversarial input is meaningful only if itself and its genuine counterpart are classified into different classes. In the case that $\mby \neq \mbyz$ and $\mbxy \in \mathcal{L}_{\mbyz}$ (corr. $\mbxy \in \mathcal{A}_{\mbyz}$), $\mathrm{d}(\mbx, \mbyz)$ is referred to as the genuine (corr. adversarial) inter-class critical distance. We use $\mathrm{d}_{\mathcal L}(\mbx, \mbyz)$ and $\mathrm{d}_{\mathcal A}(\mbx, \mbyz)$ to differentiate the   two cases. The concepts above are illustrated in Figure~\ref{fig:sps}.



%As illustrated in Figure~\ref{fig:sps}, the minimum critical distance ($\min(\mathrm{d}_{\mathcal L}(\mbx, \mbyz), \mathrm{d}_{\mathcal A}(\mbx, \mbyz))$) essentially captures the distance from $\mbx$ to the boundary between classes $\mby$ and $\mbyz$.

%\subsubsection{Re-Interpreting Minimality Principle}
%
% The minimality property essentially roots at the evasiveness requirement for adversarial inputs: with excessive perturbation amplitudes, malicious tampering can be easily spotted even by human eyes.

%Using crafting path and critical distance, we can translate

%Using the concepts above, we translate the minimality principle as follows:

\begin{figure}
\centering
\epsfig{file=figures/concept.eps, width=85mm}
\caption{Concepts of boundary, path, and radius. \label{fig:concept}}
\end{figure}

We now translate the minimality principle in the language of boundary, path, and radius: given a genuine input $\mbx$, among all the possible (adversarial) inputs in the target class $\mbyp$, the adversary seeks $\mbxp$ with the shortest path length from $\mbx$. Therefore, the minimality principle entails the following important properties:
\begin{itemize}
\item Property 1: the path length of $\mbx$ to $\mbxp$ approximates $\mbx$'s radius to $\mbyp$, $\rho(\mbx, \mbyp)$.
\item Property 2: $\mbxp$ tends to distribute extremely close to the boundary of $\mby$ and $\mbyp$.
\end{itemize}


%The sparsity property can be described as follows: genuine inputs are geometrically distant from each other, while they are densely surrounded by adversarial inputs we
Next we empirically verify these properties, while their analytical treatment is deferred to~\myref{sec:analysis}.

Specifically, given a genuine input $\mbx$ (in class $\mby$) and an adversarial one $\mbxp$ (in class $\mbyp$) generated by an attack ${\mathcal A}$, in the given dataset, we find $\mbx$'s closest genuine counterpart $\mbxy$ in class $\mbyp$, i.e., $||\mbx - \mbxy||$ is minimized. We then compute the ratio of $\mbx$'s distance to $\mbxp$ and $\mbxp$:
$||\mbx - \mbxp||/||\mbx - \mbxy||$.

%
%
% \paragraph*{Property 1.\;} For genuine input $\mbx$ (in class $\mby$), we find its nearest genuine input $\mbxy$ in each class $\mbyp$ ($\mbyp \neq \mby$), such that $\mbx$ and $\mbxy$ differ on the minimum number of dimensions. We count the number of dimensions different in $\mbx$ and $\mbxy$ as their distance, denoted by $\mathrm{d}(\mbx, \mbyp)$. Meanwhile, we apply attack ${\mathcal A}$ to $\mbx$ (with target class $\mbyp$), collect the resulting adversarial input $\mbxp$, and count the number of perturbed dimensions as their distance, denoted by $\mathrm{d}_{\mathcal A}(\mbx, \mbyp)$.
% We then compute the ratio of $\mbx$'s shortest distance to adversarial and genuine inputs:
% $\mathrm{d}_{\mathcal A}(\mbx,
%     \mbyp)/\mathrm{d}(\mbx, \mbyp)$.


%two scenarios: (i) intra-class: two genuine inputs $\mbx$ and $\mbxy$ belong to the same class $\mby$; (ii) inter-class: $\mbx$ and $\mbxy$ belong to different classes $\mby$ and $\mbyz$.

% In the intra-class case, for $\mbx \in \mathcal{L}_\mby$, we find its nearest neighbor $\mbxy \in \mathcal{L}_\mby$ and count the number of different components between $\mbx$ and $\mbxy$ as $\mathrm{d}(\mbx, \mby)$, which is then normalized by the input dimensionality (e.g., 784 in the case of \mnist).
%
% In the inter-class case, for $\mbx \in \mathcal{L}_\mby$, we count the number of different components between $\mbx$ and its nearest neighbor $\mbxy \in \mathcal{L}_{\mbyz}$ as  $\mathrm{d}_{\mathcal L}(\mbx, \mbyz)$ for each class $\mbyz \neq \mby$; meanwhile, we apply a crafting attack to $\mbx$, collect the resulting adversarial input, and count the number of perturbed components as $\mathrm{d}_{\mathcal A}(\mbx, \mbyz)$.
% We then compute the ratio of adversarial and genuine inter-class critical distance: $\frac{\mathrm{d}_{\mathcal A}(\mbx, \mbyz)}{\mathrm{d}_{\mathcal L}(\mbx, \mbyz)}$.


% With the notion of crafting path, the sparsity property can be translated as follows: given genuine input $\mbx$ in class $\mby$, the crafting path from $\mbx$ to its nearest genuine input $\mbxy$ in another class $\mbyz$ tends to be much longer than that from $\mbx$ to its nearest adversarial input $\mbxyp$ in class $\mbyz$, formally,
% \begin{equation*}
% \min_{\mbxy \in \mathcal{L}_{\mbyz}} |{\mathcal P}(\mbx, \mbxy)| > \min_{\mbxyp \in \mathcal{A}_{\mbyz}} |{\mathcal P}(\mbx, \mbxyp)|
% \end{equation*}
%This property is illustrated in Figure~\ref{fig:sparsity}.


%
% Figure~\ref{fig:sparse}\;(a) plots the cumulative distribution of intra-class critical distance. It is observed that even within the same class, genuine inputs are far away from each other. For example, in \mnist, the average critical distance is about 0.2, indicating that a random pair of inputs differ at around 157 dimensions; in \cifar and \svhn, the average critical distance rises to over 0.9! Conceivably, for given genuine inputs $\mbx$ and $\mbxy$, all the inputs that lie on the crafting path from $\mbx$ to $\mbxy$ may potentially be adversarial inputs.

Figure~\ref{fig:sparse}\; shows the cumulative distribution of such ratios with respect to \ttp and \ttca on the \cifar dataset (similar results observed on other datasets and attacks). Observe that most ratios lie in the interval of $[0, 0.01]$, regardless of attacks, suggesting
%Also note that the ratio values are smaller in cases of \ttp- and \ttca, compared with \ttg- and \ttha. The difference may be explained by that compared with the adversarial inputs constructed by nonlinear attacks, those found by linear attacks are suboptimal in terms of perturbation amplitudes. However, the difference is insignificant, implying that the concrete attack ${\mathcal A}$ is inconsequential to measure $\mathrm{d}_{\mathcal A}(\mbx, \mbyp)$.
 that $\mbx$ resides much closer to its nearest adversarial neighbor in $\mbyp$ than to its genuine counterpart. Thus $||\mbx - \mbxp||$ approximates $\mbx$'s radius to $\mbyp$.

Further, by applying ${\mathcal A}$ to the adversarial input $\mbxp$, we generate another adversarial one\footnote{Strictly speaking, $\mbxpp$ is adversarial, given that it is created by perturbing another adversarial input $\mbxp$. Here we broadly refer to all the artificially generated inputs as adversarial inputs.} $\mbxpp$ in class $\mby$; similarly, $||\mbxp - \mbxpp||$ approximates $\mbxp$'s radius to $\mby$, $\rho(\mbxp, \mby)$. We then compute the quantity of $||\mbxp - \mbxpp||/||\mbx - \mbxp||$, i.e., a proxy for $\rho(\mbxp, \mby)/\rho(\mbx, \mbyp)$.

Figure~\ref{fig:sparse}\; shows the cumulative distribution of such ratios. Across both attacks, over 80\% of the ratios concentrate in the interval of $[0, 0.2]$, indicating $\mbxp$ distributes closer to the boundary than its genuine counterpart $\mbx$.


\begin{figure}
\epsfig{file=figures/sparsity.eps, width=85mm}
\caption{Cumulative distribution of the ratio of input $\vec{x}$'s shortest distance to adversarial and genuine inputs (on the \svhn dataset). \label{fig:sparse}}
\end{figure}





%$\mathrm{d}_{\mathcal A}(\mbx, \mbyp)$ to reliably approximate $\mbx$'s adversarial radius to $\mbyp$, $\rho(\mbx, \mbyp)$.

%While the concrete attack ${\mathcal A}$ to measure $\mathrm{d}_{\mathcal A}(\mbx, \mbyp)$ is inconsequential, below we use \ttca by default.


%
%
% \vspace{3pt}
% {\bf [Property 2]} Equipped with the tool to measure an input's adversarial radii to different classes, let us proceed to quantifying the second property: are adversarial inputs in general lying closer to class boundaries than their genuine counterparts?
%
% By applying attack ${\mathcal A}$ to genuine input $\mbx$, we measure its radii to different classes, forming a radius vector $\vv{\rho}$, with the $i$-th element as the radius to the $i^{\mathrm{th}}$ class (the radius to its own class is 0 and that to an unreachable class under the constraint of perturbation amplitude is the amplitude limit).
% Similarly, for adversarial inputs $\mbxp$ (generated from $\mbx$) and $\mbxpp$ (generated from $\mbxp$),\footnote{Strictly speaking, $\mbxpp$ is not an adversarial input, given that it is generated by perturbing another adversarial input $\mbxp$. Here we broadly refer to all the artificially generated inputs as adversarial inputs.} we also obtain their radius vectors, $\vv{\rho_\epsilon}$ and $\vv{\rho_{\epsilon^2}}$. This procedure is illustrated in Figure~\ref{fig:concept}.
%
% In particular, we are interested in the minimum component of the radius vector, which indicates the input's distance to its nearest neighboring class.
%
%
% \begin{definition}[Minimum Adversarial Radius]
% For given input $\mbx$, the minimum component of its radius vector $\vv{\rho}(\mbx)$ is called its \underline{minimum adversarial radius} (\mar), i.e., $\min_{\mbyp \neq \mby} \rho(\mbx, \mbyp)$.
% \end{definition}
%
% In the following, with a little abuse of notations, we use $\rho(\mbx)$ to denote $\mbx$'s \mar. We now compare the \mar of $\mbx$, $\mbxp$, and $\mbxpp$ by computing their ratios: $\rho_\epsilon/\rho$ and $\rho_\epsilon/\rho_{\epsilon^2}$. The results for the \cifar dataset are shown in Figure~\ref{fig:ratio} (similar phenomena are observed in other datasets). It is observed that in all the cases, genuine inputs extend much longer \mar, compared with their adversarial counterparts. For instance, over 95\% of $\rho_\epsilon/\rho$ values are below 0.5 on the \cifar dataset. Meanwhile, the \mar of adversarial inputs are fairly comparable: in all the cases, only less than 0.1 $\rho_\epsilon/\rho_{\epsilon^2}$ values are either small than 0.5 or larger than 2, while over 50\% of $\rho_\epsilon/\rho_{\epsilon^2}$ values are equal to 1. Also note that such phenomena are invariant with respect to concrete attacks, suggesting their universal presence. Since the attack used to measure \mar is inconsequential, in the following discussion, we use \ttca by default.
%

%
%
% \begin{figure*}
% %\hspace{-10pt}
% \epsfig{file=figures/fig7.eps, width=180mm}
% \caption{Cumulative distribution of \mar ratios: $\rho_\epsilon/\rho$ and $\rho_\epsilon/\rho_{\epsilon^2}$, with respect to varied attacks on the \cifar dataset. \label{fig:ratio}}
% \end{figure*}
%


%
% , which is generated from , we apply ${\mathcal A}$ and measure its radius vector $\vv{\rho_\epsilon}$. For comparison purpose, we further measure the radius vector $\vv{\rho_{\epsilon^2}}$ of adversarial input $\mbxpp$, which is generated by applying ${\mathcal A}$ to $\mbxp$.
%
% Combining the empirical evidence from both intra-class and inter-class analysis, we can conclude that the sparsity property exists universally across different settings of datasets and \dnn models, irrespective of adversarial crafting attacks.
%We conclude that in general adversarial inputs tend to reside closer to class boundaries than genuine counterparts, which provides the foundations for building attack-agnostic ``differentiators'' that effectively discriminate genuine and adversarial inputs.

\begin{figure*}
\centering
\epsfig{file=figures/framework2.eps, width=180mm}
\caption{Illustration of \system architecture. It discriminates adversarial inputs through the lens of adversarial radius analysis; for suspicious inputs, it further attempts to uncover their correct classification outputs. \label{fig:framework}}
\end{figure*}


\subsection{Building EagleEye}

These properties provide the premise for building effective differentiators to identify adversarial inputs: for a given input $\mbx$ (classified by $f$ as $\mby$), we measure its radii to all other classes, among which we find the minimum one: $\min_{\mbyp \neq \mby} \rho(\mbx, \mbyp)$, referred to as its {\em adversarial radius} (\ar).\footnote{Similar definitions have also been discussed in~\cite{Fawzi:2015:arxiv,Bastani:2016:arXiv,Feng:2016:arXiv}.} With Property 1 and 2, we can differentiate genuine and adversarial inputs via examining their \ars.

However, to realize this idea, we face two major challenges. First, the radius metrics are attack-specific, e.g., it is measured differently by \ttg and \ttpa. Directly measuring radii is at most effective for specific attacks. Second, even for known attacks, finding an optimal threshold is difficult; even if it exists, it tends to vary with concrete datasets and \dnn models.

To tackle the first challenge, we propose {\em adversarial radius probing} (\arp), an attack-neutral method to indirectly approximate \ar. In specific,
it employs semi-random perturbations and measures an input $\mbx$'s \ar as the minimum distortion amplitude (referred to as its \ar probe or probe) required to change its classification outputs.

To address the second challenge, we apply a bootstrapping method to remove the need for error-prone parameter tuning. In specific, for the given input $\mbx$, we generate a set of {\em shadow inputs} $\{\mbxp\}$ via semi-random perturbations. By comparing the probes of $\mbx$ and $\{\mbxp\}$ (differential analysis), we estimate the likelihood that $\mbx$ has been maliciously tampered. If suspicious, we further infer $\mbx$'s correct classification output by analyzing the consensus of $\{\mbxp\}$ (consensus analysis).

The framework of \system is illustrates in Figure~\ref{fig:framework}. Below we elaborate its key components,
\arp in \myref{sec:arp}, bootstrapping in \myref{sec:boostrap}, and probe analysis in \myref{sec:arpa}.


\subsubsection{Adversarial Radius Probing}
\label{sec:arp}

In this stage, by performing random perturbations on the given input $\mbx$, \system estimates the minimum distortion amplitude (i.e., probe) necessary to change its classification by $f$. Intuitively, $\mbx$'s probe, denoted by $\rho(\mbx)$, reflects its \ar in an attack-neutral manner.

Yet, it is often infeasible to estimate $\rho(\mbx)$ by running random perturbations on all of $\mbx$'s components given its high dimensionality. We use a semi-random perturbation method: (i) {\em magnification} - \system first dynamically identifies a set of {\em saliency regions} in $\mbx$ that maximally impact its classification; (ii) {\em diversification} - it performs random perturbations over such regions to estimate
$\rho(\mbx)$. We detail these two operations below.




%
% entailing two key operations: {\em magnification} and {\em diversification}.
%
% \begin{itemize}
% \item Magnification - In this step, for a given input, \system dynamically identifies a set of {\em saliency input regions} that maximally impact its classification output.
%
% %With the assumption that an attacker attempts to minimize the distortion amplitude to evade detection, such salient regions are frequently the ``hot spots'' where the adversarial perturbations are applied.
% \item
%  Diversification - In this step, \system performs random perturbations over salient regions to expose hidden adversarial perturbations. Due to the irreversability of adversarial entropy (cf. Section~\ref{sec:irr}), by randomly perturbing elements in salient regions and performing classification over resulting perturbed inputs, it is expected to observe highly distinguishable measurements from genuine or adversarial inputs. Moreover, by properly selecting a subset of salient regions to perturb, it is even possible to pinpoint where the adversarial perturbations have been applied.
% \end{itemize}
%
% We detail the design and implmentation of these two key operations in Section~\ref{sec:mag} and~\ref{sec:div}, respectively.

\paragraph*{Magnification.\;}
\label{sec:mag}

The magnification operation is loosely based on attention mechanisms~\cite{gregor:2015:arxiv,Almahairi:2016:icml}, inspired by that human's vision automatically focuses on certain regions of an image with ``high resolution'' while perceiving surrounding regions in ``low resolution''. We use a simple attention mechanism with computational advantages.


%By analogy, an attention mechansim adaptively assigns a neural network's capacity across different regions of input data, with the hope of better capturing more important information.

% However, conventional attention mechanisms are computationally expensive in either training or testing stage. For our purpose, we provide a simple mechanism with computational advantages in both inference and learning.

For the given input $\mbx$, we define a set of $d \times d$ spatial regions ($d=4$ in our implementation). We generate all possible regions of $\mbx$ by applying an identity kernel of size $d\times d$ over $\mbx$, similar to a convolution operation.
%In our implementation, we set the stride of this operation to be $d/2$.

To select the top $n$ saliency regions that maximally impact $\mbx$'s classification, we apply a greedy approach.
%as sketched in Algorithm~\ref{}.
We sort the components of $\mbx$ according to their saliency (e.g., its gradient or Jacobian value) in descending order. Let $\rm{rank}(x)$ be the ranking of the component $x$. The saliency of a region $\mbg$ is the aggregated saliency contributed by all the components contained in $\mbg$:
\begin{displaymath}
{\rm saliency}(\mbg) = \sum_{x \in \mbg} c^{-\rm{rank}(x)}
\end{displaymath}
where $c$ $(c \geq 1)$ is a constant.
%$c = 1.5$ in our implementation).

This definition allows us to control the characteristics of selected regions. With large $c$, we focus on regions that cover the most influential components; while with small $c$ (i.e., close to 1), we find regions that contain less influential components. The rationale behind balancing these two factors is as follows: the saliency landscape shifts as $\mbx$ is perturbed as $\mbxp$; yet, due to the inherent continuity of \dnns, the variation tends to be local. By properly setting $c$, we are able to accommodate such shift and still capture the most influential component in $\mbxp$.

We iteratively select the top $n$ regions. Let $\mathcal{R}_i$ be the selected regions after the $i^{\rm th}$ iteration. We then update the saliency of each remaining region by removing the contributions by components contained in regions in $\mathcal{R}_i$. Formally,
\begin{displaymath}
    {\rm saliency}(\mbg) = \sum_{x \in \mbg \bigcap \not\exists \mbg' \in \mathcal{R}_i, x \in \mbg'} c^{-\rm{rank}(x)}
\end{displaymath}

We then pick the region with the largest saliency among the remaining ones. We will discuss the optimal setting of $k$ and $c$ in \myref{sec:evaluation}.


%which together with $\mathcal{R}_i$ constitute $\mathcal{R}_{i+1}$.



\paragraph*{Diversification.\;}
\label{sec:div}

% We define pedagogical method

%In the diversification phase, by leveraging the irreversability property of adversarial entropy, we perform highly diversified classisfication over the saliency regions of an input to expose potential adversarial perturbations.

At this step, given the saliency regions ${\mathcal R}$ of $\mbx$, we perform random perturbations on ${\mathcal R}$ to estimate $\mbx$'s probe $\rho(\mbx)$.

With a little abuse of notations, let $\mbg$ be the set of components contained in the regions of $\mathcal{R}$. At each run, following a predefined distribution $p$ (with parameter $\theta$), we randomly select a subset of components in $\mbg$ to construct the perturbation vector $\mbr$, denoted by $\mbr \leadsfrom_\theta \mbg$. A successful perturbation $\mbr$ results in $f(\mbx + \mbr) \neq \mby$, where $\mby$ is $\mbx$'s current classification output by $f$.


In our current implementation, we instantiate $\theta$ as a uniform distribution and assume the flipping perturbation which sets an input component to either fully-on (`1') or fully-off (`-1'). The distortion amplitude is therefore measurable by the sampling rate $\theta$ of $p$. While other instantiations are certainly possible (e.g., zipf distribution), we find that this instantiation is both (i) effective in discriminating adversarial inputs (\myref{sec:evaluation}) and (ii) simple to control by the system operator. Moreover, the large entropy of uniform distributions enhances the hardness for the adversary to evade the detection (\myref{sec:analysis} and \myref{sec:evaluation}).

In the following, we consider the minimum sampling rate $\theta^*$ required to cause successful perturbations as $\mbx$'s probe, which indirectly reflects $\mbx$'s \ar.

%i.e., $\rho(\mbx) = \arg\min_\theta \exists \mbr  \leadsfrom_p \mbg \cap  f(\mbx + \mbr) \neq \mby$.



%Let $\mbs^{(i)}$ be the perturbation vector generated in the $i$-th trial. With $n$ trials, the probability that $\mbx$ is randomly transformed to an adversarial input in class $\tilde{y}$ is calculated as:
% \begin{displaymath}
% \mathsf{prob}_{\mathcal{A}}(\mbx, \mbg, \tilde{y}) = \frac{\sum_i  \mathbbm{1}( f(\mbx + \mbs^{(i)}) = \tilde{y})}{n}
% \end{displaymath}
% Here $\mathbbm{1}(\cdot)$ is an indicator function that returns $1$ if the condition holds and $0$ otherwise.
% The quantities of $\{\mathsf{prob}_{\mathcal{A}}(\mbx, \mbg, \tilde{y}) \}_{\tilde{y}}$ are then used to compute the ardversarial entropy $\mathsf{entropy}_{\mathcal{A}}(\mbx)$ defined in Eqn.(\ref{eq:entropy}).
%
% Now, one critical question remains: how to select the set of probes in $\mbg$, such that they can well expose diversified classisifcation outputs. In our current implementation, we select probes following a $\mathsf{zipf}$ distribution. More specifically, we sort input elements in $\mbg$ according to their saliency in ascending order. The probability of selecting the $i$-th element as a probe is defined as:
% \begin{displaymath}
% \mathsf{zipf}(i) = \frac{i^{-s}}{\sum_{j=1}^{|\mbg|}j^{-s}}
% \end{displaymath}
% where $s$ $(s \geq 1)$ is the exponent characterizing this distribution. With $s > 1$, more highly ranked elements are picked with higher probability; meanwhile, as $s$ approaches $1$, it gradually degenerates to a uniform distribution.
%
% In our current implementation, we set $s = 1.2$ and run $n = 2\cdot|\mbg|$ trials to construct the perturbation vector $\mbs$ by flipping the selected probes.
%
% %
% \vspace{5pt}
% \paragraph{Hotspot Detection}
%
% Besides determining whether the given input is genuine or tampered, by properly selecting the probes, we may further pinpoint potential regions where adversarial perturbations are applied (``hotspots'').
%
% Recall that we generate all possible regions by applying an identity kernel of size $d \times d$ and stride $\frac{d}{2}$ over the input. Thus, given the set of saliency regions $\mathcal{R}$, we can decompose them into a set of non-overlapping ``micro-regions'', each of size $\frac{d}{2} \times \frac{d}{2}$, as illustrated in Figure~\ref{}. Next we attempt to detect hotspots at the granularity of micro-regions.
%
% Assume that the hotspots consist of $\kappa$
%



\subsubsection{Bootstrap Operation}
\label{sec:boostrap}

By randomly perturbing the given input $\mbx$, the bootstrap operation produces a set of adversarial inputs $\{\mbxp\}$, which we refer to as $\mbx$'s {\em shadow inputs}. Intuitively, such shadow inputs represent $\mbx$'s near (if not the nearest) adversarial counterparts in other classes.

%Therefore, if $\mbx$ itself is adversarial, $\mbx$ and $\{\mbxp\}$ should demonstrate similar \ars.


Specifically, to generate $\{\mbxp\}$, we adopt the same semi-random perturbation strategy as in \myref{sec:arp}, except for that the sampling rate is now fixed to be $\mbx$'s probe $\theta^*$. This practice ensures that the generated shadow inputs are as close to $\mbx$ as possible. Further, as will be revealed in \myref{sec:arpa}, in consensus analysis, this practice also helps uncover the correct classification of $\mbx$ if it is adversarial. To provide stable estimation, we require the number of shadow inputs to be larger than a threshold $k$ ($k \geq 4$ seems sufficient in practice \myref{sec:evaluation}).

We then estimate the probes of all the shadow inputs, $\{\rho(\mbxp)\}$, which, together with $\rho(\mbx)$, are fed as inputs to the phase of probe analysis. For simplicity of presentation, in the following, let $\rho$ and $\rho_\epsilon$ denote the probes of $\mbx$ and $\mbxp \in \{\mbxp\}$ respectively.



\subsubsection{Probe Analysis}
\label{sec:arpa}

In this phase, by analyzing the probes of the given input $\mbx$ and its shadow inputs $\{\mbxp\}$, \system determines the likelihood that $\mbx$ has been maliciously tampered (differential analysis); if so, it further attempts to recover its correct classification result (consensus analysis).


\paragraph*{Differential Analysis.\;} Recall that $\{\mbxp\}$ represent $\mbx$'s near (if not the nearest) adversarial neighbors in other classes. Thus, if $\mbx$ itself is adversarial, $\mbx$ and $\mbxp$ can be considered as adversarial versions of each other, thereby featuring similar \ars; otherwise, if $\mbx$ is a genuine input, the \ars of $\mbx$ and $\{\mbxp\}$ tend to show significant difference (see Figure~\ref{fig:sparse}).

In differential analysis, we leverage this insight and examine the probes of $\mbx$ and each shadow input $\mbxp$. Intuitively, a larger probe ratio of $\rho/\rho_\epsilon$ indicates that $\mbx$ is more likely to be genuine. Concretely, with a given shadow input $\mbxp$, we estimates the likelihood that $\mbxp$ is genuine as:
\begin{equation*}
{\rm genuine}_{\mbxp}(\mbx) = \frac{1}{1 + \exp(1-\rho/\rho_\epsilon)}
\end{equation*}
Here the sigmoid function converts $\rho/\rho_\epsilon$ to the interval of $(0, 1)$, which we may roughly interpret as the ``probability'' that $\mbx$ is genuine. In specific, this probability is 0.5 if $\rho/\rho_\epsilon = 1$. The overall likelihood that $\mbx$ is genuine is computed by aggregating the results over all the shadow inputs:
${\rm genuine}(\mbx) = \sum_{\mbxp}{\rm genuine}_{\mbxp}(\mbx)/|\{\mbxp\}|$.

In our empirical evaluation in \myref{sec:evaluation}, we find a threshold 0.625 works well across all the known attacks.



\paragraph*{Consensus Analysis.\;}
As shown in Figure~\ref{fig:framework}, if $\mbx$ passes the differential analysis, it is reported as ``genuine''; otherwise, it is considered as a ``suspicious'' case and moved to the phase of consensus analysis, in which we attempt to infer its correct classification output.

To be specific, recall that an adversarial input $\mbx$ (in class $\mbyp$) created from a genuine input (in class $\mby$) resides near to the boundary of $\mby$ and $\mbyp$. Thus, among its close adversarial neighbors in other classes, a majority of them should belong to class $\mby$. By leveraging this observation, we simply pick the most common class associated with the shadow inputs $\{\mbxp\}$ as $\mbx$'s most likely correct classification output.


%We can further improve the reliability of this scheme by applying a set of adversarial attacks (e.g., by changing the number of perturbed components at each iteration to create attack variants) to $\mbx$, collecting their corresponding \mar, and deciding its correct classification using majority voting (i.e., consensus analysis in Figure~\ref{fig:framework}).





%
%
%
%
% Recall the \system framework in Figure~\ref{fig:arch}, in the phase of integrity checking, we intend to decide whether the given input (which can be either genuine or adversarial) has been maliciously tampered, by using the property that genuine inputs extend longer \mar than adversarial ones.
%
% Unfortunately, it is challenging to directly apply this property, because it amounts to finding an optimal threshold on the magnitude of \mar to distinguish genuine and adversarial inputs. However, the existence of such an optimal threshold is questionable; even it exists, it is inclined to vary with concrete datasets, \dnn models, or attack models. Next we introduce a self-tuning method that removes the requirement for tedious, error-prone parameter tuning.
%
%
% As illustrated in Figure~\ref{fig:framework}, at a high level, our solution employs a bootstrap strategy, which is inspired by the empirical study above. More specifically, we first apply adversarial crafting attack ${\mathcal A}$ to given input $\mbx$ (which can be either genuine or adversarial) to generate a set of adversarial inputs $\{\mbxp\}$, and estimate their \mar $\rho$ (of $\mbx$) and $\{\rho_\epsilon\}$ (of $\{\mbxp\}$). We then analyze the relative \mar corresponding to the adversarial inputs, defined as  $\rho_\epsilon/\rho$, which are called ``probes''. We differentiate two cases: (i) if $\mbx$ is a genuine input, the probes tend to concentrate in the interval of $[0, 1)$; (ii) meanwhile, if $\mbx$ is an adversarial input, the probes are almost equally likely to appear on either side of 1. Therefore, by examining the distribution of the probes, we are able to infer whether $\mbx$ has been maliciously tampered. We may use a parameterized method to reduce the required number of probes. We consider $\mbx$ as a genuine input, if $\delta$ of its probes appear in the interval of $[0, \kappa)$. In our implementation, we set $\delta = 0.25$ and $\kappa = 0.5$ (details in~\myref{sec:evaluation}).
%
% %In particular, if among $n$ radius vectors, $m$ of them appear in the interval of $[0, 1]$, we





%This detection method is illustrated in Figure~\ref{fig:framework}.

% \begin{algorithm}{\small
%  \SetKwProg{Event}{event}{}{end}
%   \SetAlgorithmName{Algorithm}{List of operations}
%   \SetAlgoLined
%   \SetSideCommentRight
%   \SetNoFillComment
%   \KwIn{input of interest $\mbx$}
%   \KwOut{tampering analysis results of $\mbx$}
%
%   %\KwData{this text}
%   %\KwResult{how to write algorithm with \LaTeX2e }
%   %\tcc{\scriptsize initialization \{\ldots \}}
%
% \tcc{\small bootstrap step}
% apply attack ${\mathcal A}$ to $\mbx$ to generate a set of adversarial inputs $\{\mbxp\}$\;
% compute the radius vector $\vv{\rho}$ of $\mbx$\;
%
%   \For{each adversarial input $\mbxp \in \{\mbxp\}$}{
%   \tcc{\small preparation and inspiration to connect $i,j$}
%      candidate references $\tilde{{\mathcal Q}} \leftarrow {\mathcal Q}$\;
%      \While{true}{
%      find $x^* = \arg\max_{x \in \tilde{{\mathcal Q}}} \Delta_{x}^{i,j}$\;
%      remove $x^*$ from $\tilde{\mathcal Q}$\;
%      \tcc{\small temporal dynamics-based Bernoulli trial}
%      pick $x^*$ with probability $m(\tau_{x^*,k}^r)$\;
%      \lIf{$x^*$ is picked}{break}
%      }
%      compute $\Delta_{\mathcal Q}^{i,j} \leftarrow \Delta_{x^*}^{i,j}$\;
%   }
%   \tcc{\small aggregate at paper level}
%   $\chi_k$ $\leftarrow$ $\ell(\{\varphi_{i,j} - \Delta^{i,j}_{{\mathcal Q}}\}_{i,j \in {\mathcal C}_k})$\;
%   $\psi_k$ $\leftarrow$ $\ell(\{\varphi_{i,j}\}_{i,j \in {\mathcal C}_k}) -\ell(\{\varphi_{i,j} - \Delta^{i,j}_{{\mathcal Q}}\}_{i,j \in {\mathcal C}_k})$\;
%    \caption{Adversarial Tampering Analysis \label{alg:checking}}}
% \end{algorithm}


%
% The properties of minimality and sparsity entail the distinct traits of adversarial and genuine inputs: (i) due to the minimality property, each adversarial input generated by a certain attack tends to reside close to a class boundary; (ii) due to the sparsity property, each genuine input lies further away from a class boundary than some of its surrounding adversarial inputs.
%
%
%
%








%
%
%
%
%
%
% Our ``differentiator'' is built upon the distinct traits of genuine and adversarial inputs, entailed by the minimality and sparsity properties: that
%
