
\begin{abstract}
%The recent advances in deep learning (\dl) have led to breakthroughs in long-standing artificial intelligence tasks. However,

Deep neural networks (\dnns) are inherently vulnerable to adversarial inputs: such maliciously crafted samples trigger \dnns to misbehave, leading to detrimental consequences for \dnn-powered systems. The fundamental challenges of mitigating adversarial inputs stem from their adaptive and variable nature. Existing solutions attempt to improve \dnn resilience against specific attacks; yet, such static defenses can often be circumvented by adaptively engineered inputs or by new attack variants.

Here, we present \system, an attack-agnostic adversarial tampering analysis engine for \dnn-powered systems. Our design exploits the {\em minimality principle} underlying many attacks: to maximize the attack's evasiveness, the adversary often seeks the minimum possible distortion to convert genuine inputs to adversarial ones. We show that this practice entails the distinct distributional properties of adversarial inputs in the input space. By leveraging such properties in a principled manner, \system effectively discriminates adversarial inputs and even uncovers their correct classification outputs. Through extensive empirical evaluation using a range of benchmark datasets and \dnn models, we validate \system's efficacy. We further investigate the adversary's possible countermeasures, which implies a difficult dilemma for her: to evade \system's detection, excessive distortion is necessary, thereby significantly reducing the attack's evasiveness regarding other detection mechanisms.
\end{abstract}
