
\section*{Appendix}

\subsection*{A. Datasets}


The \mnist dataset~\cite{mnist} constitutes a set of 28$\times$28 greyscale images of handwritten digits (`0'-`9'), with 60K training and 10K testing samples.

The \cifar dataset~\cite{cifar} consists of 32$\times$32 color images from ten classes (e.g., `airplane', `automobile', `bird'), split into 50K training and 10K testing samples.


The \svhn dataset~\cite{svhn} comprises color images of house numbers collected by Google Street View. We consider the format of 32$\times$32 pixel images. The task is to classify the digit around the center of each image. There are 73K and 26K digits in the training and testing sets, respectively.

All the datasets are centered and normalized such that the value of each pixel lies in the interval of $[-1, 1]$.


\subsection*{B. Implementation of DNN Models}
\label{sec:app:dnn}

In the following we present the detailed architectures of \dnn models used in our empirical evaluation.

\paragraph*{Convolutional Neural Network.\;}
The \dnn model applied to classifying the \mnist dataset is a convolutional neural network (\cnn). In particular, we adopt an architecture similar to~\cite{Papernot:2016:sp}, which is summarized in Table~\ref{tab:mnist}. In addition, we apply dropout (rate = 0.5) at both fully connected layers.

\begin{table}[h]{\small
  \centering
\begin{tabular}{c|c}
{\bf Layer} & {\bf Definition} \\
\hline
ReLU Convolutional & \# filters: 32, kernel: 3$\times$3   \\
\hline
ReLU Convolutional & \# filters: 32, kernel: 3$\times$3  \\
\hline
Max Pooling & pool: 2$\times$2, stride: 1\\
\hline
ReLU Convolutional & \# filters: 64, kernel: 3$\times$3  \\
\hline
ReLU Convolutional & \# filters: 64, kernel: 3$\times$3  \\
\hline
Max Pooling & pool: 2$\times$2, stride: 1\\
\hline
ReLU Fully Connected & \# units: 256 \\
\hline
ReLU Fully Connected & \# units: 256\\
\hline
Softmax & \# units: 10\\
\hline
\end{tabular}
\caption{Convolutional network architecture (\mnist) \label{tab:mnist} }}
\end{table}

\paragraph*{Maxout Network.\;}
The maxout network (\mxn) model generalizes conventional \cnn models by employing maxout activation functions, which pool over multiple affine feature maps in addition to pooling over adjacent spatial locations as in convolution operations. Therefore, an maxout convolutional layer is defined as the composition of one convolutional layer, one regular pooling layer, and one maxout pooling layer.

To classify the \cifar dataset, we adopt an architecture similar to that in~\cite{Goodfellow:2013:icml}, which consists of three maxout convolutional layers and one maxout fully connected layer, as detailed in Table~\ref{tab:cifar10}. We apply dropout (rate = 0.5) at each maxout convolutional layer.

\begin{table}[h]{\small
  \centering
\begin{tabular}{c|c}
{\bf Layer} & {\bf Definition} \\
\hline
ReLU Convolutional & \# filters: 128 kernel: 5$\times$5, padding: 4 \\
\hline
Max Pooling & pool: 3$\times$3, stride: 2 \\
\hline
Maxout & \# units: 64 (2 pieces/unit)\\
\hline
ReLU Convolutional & \# filters: 256 kernel: 5$\times$5, padding: 3 \\
\hline
Max Pooling & pool: 3$\times$3, stride: 2\\
\hline
Maxout & \# units: 128 (2 pieces/unit)\\
\hline
ReLU Convolutional &  \# filters: 256 kernel: 5$\times$5, padding: 3 \\
\hline
Max Pooling & pool: 3$\times$3, stride: 2\\
\hline
Maxout & \# units: 128 (2 pieces/unit)\\
\hline
ReLU Fully Connected & \# units: 2,000 \\
\hline
Maxout & \# units: 400 (5 pieces/unit)\\
\hline
Softmax & \# units: 10\\
\hline

\end{tabular}
\caption{Maxout network architecture (\cifar) \label{tab:cifar10} }}
\end{table}

\paragraph*{Network-in-Network.\;}

To classify the \svhn dataset, we apply an network-in-network (\nin) model, which features another distinct architecture. In conventional \cnn, the convolution filter is essentially a generalized linear model (\glm) for the underlying data patch. The \nin architecture replaces  \glm with an ``micro neural network'' structure which is a nonlinear function approximator to enhance the abstraction capability of the local model.

Following~\cite{lin:2014:iclr}, we use multilayer perceptron (\mlp) as the instantiation of micro network. In the resulting mlpconv layer, the \mlp is shared among all local receptive fields, while the feature maps are obtained by sliding the \mlp over the input in a similar manner as \cnn.

Specifically, our \nin model comprises three mlpconv layers, followed by one average pooling layer and one softmax layer, as summarized in Table~\ref{tab:svhn}.
Dropout (rate = 0.5) is applied at each mlpconv layer.

\begin{table}[h]{\small
  \centering
\begin{tabular}{c|c}
{\bf Layer} & {\bf Definition} \\
\hline
ReLU Convolutional & \# filters: 96, kernel: 5$\times$5, padding: 2 \\
\hline
ReLU Convolutional &  \# filters: 96, kernel: 1$\times$1 \\
\hline
Max Pooling & pool: 3$\times$3, stride: 2\\
\hline
\hline
ReLU Convolutional & \# filters: 192, kernel: 5$\times$5, padding: 2\\
\hline
ReLU Convolutional & \# filters: 192, kernel: 1$\times$1 \\
\hline
Max Pooling & pool: 3$\times$3, stride: 2\\
\hline
\hline
ReLU Convolutional & \# filters: 192, kernel: 3$\times$3, padding: 1 \\
\hline
ReLU Convolutional & \# filters: 192, kernel: 1$\times$1 \\
\hline
ReLU Convolutional & \# filters: 10, kernel: 1$\times$1 \\
\hline
Average Pooling & pool: 8$\times$8 \\
\hline
Softmax & \# units: 10\\
\hline

\end{tabular}
\caption{Network-in-network architecture (\svhn) \label{tab:svhn} }}
\end{table}


%
% \begin{figure*}
% \hspace{-10pt}
% \epsfig{file=figures/attack2.eps, width=170mm}
% \caption{A schematic nonlinear attack model, which entails three major stages: input saliency analysis, perturbation selection, and misclassification checking (adapted from~\cite{Papernot:2016:sp}). \label{fig:attack}}
% \end{figure*}


\subsection*{C. Implementation of Defense Methods}

We implement one representative defense mechanism from each category of defense strategies in~\myref{sec:defense}.

\paragraph*{Data Augmentation.\;} Recall that with data augmentation, adversarial inputs are incorporated in training a more robust \dnn $f'$. Our implementation of this defense mechanism proceeds as follows.
\begin{myitemize}
\item We begin with an initialized \dnn $f$ and an attack of interest ${\mathcal A}$;
\item At each iteration, regarding the current $f$, we apply ${\mathcal A}$ to a minibatch $\{(\mbx, \mby)\}$ randomly sampled from the training set, and generate an augmented minibatch $\{(\mbx, \mbxp, \mby)\}$;
%
% let ${\mathcal A}$ be the target attack, which takes as input the current $f$, a legitimate input $\mbx$ and an adversarial output $\mbyp$ (randomly chosen), and generates the perturbation vector $\mbr$.
% \begin{equation*}
%     {\mathcal A}: f, \mbx, \mbyp \rightarrow \mbr \quad \textrm{s.t.}\quad f(\mbx + \mbr) = \mbyp
% \end{equation*}
% % We consider the worst-case adversarial output $\mbyp$, leading to the minimum perturbation vector: $\mathsf{min}_{\mbyp} ||g(f, \mbx, \mbyp)||$.
% \item Thus, for each instance $(\mbx, \mby)$ in the training set, we produce a corresponding adversarial input $\mbxp = \mbx + \mbr$.
We update $f$ using the objective function:
\begin{equation*}
\min_{f} \sum_{(\mbx, \mbxp, \mby)}  (\ell(f(\mbx), \mby) +  \ell(f(\mbxp), \mby))
\end{equation*}
% where $\lambda$ controls the weight of legitimate and adversarial inputs ($\lambda = 0.5$ in our implementation).
\end{myitemize}
We instantiate the attack ${\mathcal A}$ as each of \ttg, \tth, \ttp, \ttca and refer to the resulting models as \ttg, \tth, \ttp, and {\ttct} \dnn respectively.

\paragraph*{Robust Optimization.\;} Recall that with robust optimization, one improves \dnn stability by preparing it for the worst-case inputs, which is often formulated as an minimax optimization framework.

We implement the framework in~\cite{Shaham:2015:arxiv}, wherein the loss function is minimized over adversarial inputs generated at each parameter update. To be specific, it instantiates Eqn.\;(\ref{eq:rof}) with finding the perturbation vector $\mbr$ (with respect to an input instance $(\mbx, o)$) that maximizes its inner product with the gradient of objective function:
\begin{equation}
    \label{eq:rof2}
    \mbr = \arg\max_{\mbr} \langle \triangledown \ell (f(\mbx), o)), \mbr \rangle
\end{equation}
For $l_\infty$-norm, we limit $||\mbr||_\infty \leq 0.2$; for $l_1$-norm, we limit $||\mbr||_1 \leq 2*2$ (no more than 2 pixels).


\paragraph*{Model Transfer.\;} Recall that with model transfer, the knowledge in a teacher \dnn $f$ (trained on legitimate inputs) is extracted and transferred to a student \dnn $f'$, such that $f'$ generalizes better to adversarial inputs. Specifically, we implement the defensive distillation mechanism in~\cite{Papernot:2016:sp} as a representative method of model transfer. We set the temperature $\tau = 40$ in the experiments as suggested by~\cite{Papernot:2016:sp}.



\subsection*{D: Training of DNN Models}

The training process identifies the optimal setting for a \dnn's parameters $\mbw$. Due to complex structures of \dnn models and massive amount of training data,
we apply Stochastic Gradient Descent with Nesterov momentum~\cite{Sutskever:2013:icml} as the optimization algorithm. More specifically, let $\ell(\mbw)$ represent the objective function (e.g., the cross entropy of ground-truth class labels and \dnn models' outputs). At each epoch, the gradient of $\ell$ with respect to $\mbw$ is computed over a ``mini-batch'' (e.g., each of 128 samples) sampled from the training data via a back-propagation procedure. The update rule of $\mbw$ is given by:
\begin{equation*}
\left\{
\begin{array}{l}
%\label{eq:updat\end{equation*}e5}
v = \mu v -\lambda \cdot \triangledown \ell(\mbw + \mu v)\\
\mbw := \mbw + v
\end{array}
\right.
\end{equation*}
where $v$ represents the ``velocity'', $\mu$ denotes the ``momentum'', $\lambda$ is the learning rate, and $\triangledown$ is the gradient operator. The training process repeats until the objective function converges.

In implementation, the default learning rates for \mnist, \cifar, and \svhn datasets are respectively set as 0.1, 0.01, and 0.01; the default momentum $\mu$ is fixed as 0.9; the optimization algorithm is run for up to 240 epochs. In addition, we apply an adaptive learning rate scheme: at each epoch, let $\ell$ and $\ell^*$ respectively be the loss of current epoch and the best loss thus far; the learning rate is adjusted as: $\lambda = \lambda \cdot \exp (\frac{\ell^* - \ell}{s})$, where $s = 2.5$ if $\ell^* \geq \ell$ and $s = 0.75$ if  $\ell_* < \ell$. The \dnn models are trained using the training set of each dataset.
 All the algorithms are implemented on top of Theano\footnote{Theano: http://deeplearning.net/software/theano/}, a Python-based \dl library. All the experiments are performed using an array of 4 Nvidia GTX 1080 GPUs.

\subsection*{E: Additional Experiments}

Here we list the experiment results in addition to those in \myref{sec:evaluation}, including the impact of parameter tuning on \system's performance, and the tradeoff between the attack's evasiveness regarding \system and other defenses.

\begin{figure}
%\hspace{-10pt}
\epsfig{file=figures/cm1_cifar.eps, width=80mm}
\caption{Distribution of ratio of distortion amplitudes (without vs. with minimality principle) on \cifar. \label{fig:pratiocifar}}
\end{figure}


\begin{figure}
%\hspace{-10pt}
\epsfig{file=figures/cm1_mnist.eps, width=80mm}
\caption{Distribution of ratio of distortion amplitudes (without vs. with minimality principle) on \mnist. \label{fig:pratiomnist}}
\end{figure}


\begin{figure}
%\hspace{-10pt}
\epsfig{file=figures/cm2_cifar.eps, width=80mm}
\caption{Distribution of ratio of distortion amplitudes (random perturbation vs. minimality principle) on \cifar. \label{fig:pratiocifar2}}
\end{figure}


\begin{figure}
%\hspace{-10pt}
\epsfig{file=figures/cm2_mnist.eps, width=80mm}
\caption{Distribution of ratio of distortion amplitudes (random perturbation vs. minimality principle)  on \mnist. \label{fig:pratiomnist2}}
\end{figure}





\begin{figure}
%\hspace{-10pt}
\epsfig{file=figures/coef_c.eps, width=80mm}
\caption{Impact of number of patches $c$ on \system's performance. \label{fig:coefc}}
\end{figure}


\begin{figure}
%\hspace{-10pt}
\epsfig{file=figures/coef_n.eps, width=80mm}
\caption{Impact of number of patches $n$ on \system's performance. \label{fig:coefn}}
\end{figure}
