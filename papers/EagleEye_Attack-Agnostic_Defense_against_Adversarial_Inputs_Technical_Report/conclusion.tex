\section{Conclusion}
\label{sec:conclusion}


In this paper, we presented a new approach to defend deep learning (\dl) systems against adversarial input attacks. Our work was motivated by the observations that the fundamental challenges to tackle adversarial inputs stem from their adaptive and variable nature, while static defenses can often be circumvented by adaptively engineered inputs or by new attack variants. We developed a principled approach that, by leveraging the underlying design principles shared by varied attacks, discriminates adversarial inputs in a universal, attack-agnostic manner. We designed and implemented \system, a prototype defense engine which can be readily deployed into {\em any} \dl systems, requiring no modification to existing components. Through comprehensive adversarial tampering analysis, \system enables more informative decision-making for the operators of deep learning systems. Our empirical evaluations showed that \system, when applied to three benchmark datasets, detected nearly 96\% adversarial inputs generated by a range of attacks.
