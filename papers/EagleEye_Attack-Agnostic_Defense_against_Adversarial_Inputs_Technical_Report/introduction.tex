\section{introduction}
\label{sec:introduction}


Recent years have witnessed the abrupt advances in deep learning (\dl) techniques~\cite{LeCun:2015:nature}, which lead to breakthroughs in a number of long-standing artificial intelligence tasks (e.g., image classification, speech recognition, and even playing Go~\cite{go}). Internet giants, such as Google, Facebook and Amazon, all have heavily invested in offering \dl-powered services and products.
%



However, designed to model highly nonlinear, nonconvex functions, deep neural networks (\dnns) are inherently vulnerable to adversarial inputs, which are malicious samples crafted by adversaries to trigger \dnns to misbehave~\cite{Szegedy:2013:arxiv}. Figure~\ref{fig:adv} shows an example: both original images are correctly recognized by a \dnn; with a few pixels altered, the resulting adversarial images are misclassified by the same \dnn, though the difference is barely discernible for human eyes. With the increasing use of \dl-powered systems in security-critical domains, adversaries have strong incentive to manipulate such systems via forcing misclassification of inputs: illegal content can bypass content filters that employ \dl to discriminate inappropriate web
content~\cite{Grosse:arxiv:2016}; biometric authentications that apply \dl to validate human faces can be manipulated to allow improper access~\cite{Sharif:2016:ccs}; in the near future, driverless vehicles that use \dl to detect traffic signs may be misled to crashing.



\begin{figure}
    \centering
    % \begin{subfigure}{0.1\textwidth}
    %     \centering
    %     \epsfig{height=0.65in, file=figures/30.png}
    %     \caption{}
    % \end{subfigure}%
    % \hspace{4pt}
    % \begin{subfigure}{0.1\textwidth}
    %     \centering
    %     \epsfig{height=0.65in, file=figures/37.png}
    %     \caption{}
    % \end{subfigure}
    % \hspace{4pt}
    % \begin{subfigure}{0.1\textwidth}
    %     \centering
    %     \epsfig{height=0.65in, file = figures/70.png}
    %     \caption{}
    % \end{subfigure}%
    % \hspace{4pt}
    % \begin{subfigure}{0.1\textwidth}
    %     \centering
    %     \epsfig{height=0.65in, file=figures/73.png}
    % \caption{}
    % \end{subfigure}
    \epsfig{width=80mm, file=figures/sign.eps}
    \vspace{-5pt}
    \caption{(a) (c) genuine inputs - both are correctly recognized; (b) (d) adversarial inputs - (b) is misclassified as ``70 mph'' and (d) is misclassified as ``30 mph''.\label{fig:adv}}
\end{figure}




The fundamental challenges of defending against adversarial input attacks stem from their adaptive and variable nature: they are created tailored to target \dnns, while crafting strategies vary greatly with concrete attacks. Existing solutions attempt to improve \dnn resilience against specific attacks~\cite{Gu:2014:arxiv,Goodfellow:2014:arxiv,Huang:2015:arxiv,Shaham:2015:arxiv,Papernot:2016:sp}; yet, such static defenses, once deployed, can often be circumvented by adaptively engineered inputs or by new attack variants. For instance, the training data augmentation mechanism~\cite{Goodfellow:2014:arxiv,Nokland:2015:arXiv} suggests to train \dnns on adversarial inputs; as detailed in~\myref{sec:measure}, the resulting models often overfit to known attacks, thus being even more vulnerable to unseen variants. Further, most existing solutions require significant modifications to either \dnn architectures or training procedures, which often negatively impact the classification accuracy of \dnn models. Indeed, recent theoretical exploration~\cite{Fawzi:2015:arxiv} has confirmed the inherent trade-off between \dnn robustness and expressivity, which significantly impedes the adoption of existing defense solutions in accuracy-sensitive domains.




In this paper, we take a completely new route: instead of striving to improve \dnn robustness against specific attacks, we aim at defense mechanisms that make minimal assumptions regarding the attacks and adapt readily to their variable nature. To this end, we design, implement and evaluate \system, an attack-agnostic adversarial tampering analysis engine for \dl-powered systems.

%As it is operated as a modular component within \dl-powered systems, \system incurs no impact to \dnn classification accuracy.
At a high level, \system leverages the {\em minimality principle} underlying many attacks: intuitively, to maximize the attack's evasiveness, the adversary often seeks the minimum possible distortion to convert a genuine input to an adversarial one. We show both empirically and analytically that this practice entails the distinct properties shared by adversarial inputs: compared with their genuine counterparts, adversarial inputs tend to distribute ``closer'' to the classification boundaries induced by \dnns in the input manifold space.
%
% invariant properties shared by adversarial inputs: they tend to distribute fairly close to the classification boundaries induced by \dnns in the input space.
By exploiting such properties in a principled manner, \system effectively discriminates adversarial inputs and even uncovers their correct classification outputs. We also investigate the adversary's possible countermeasures by abandoning the minimality principle, which however implies a difficult dilemma for her: to evade \system's detection, excessive distortion is necessary, thereby significantly reducing the attack's evasiveness with respect to other detection mechanisms (e.g., human vision).



Note that we are not arguing to replace existing defense solutions with \system. Rather, their distinct designs entail their complementary nature. \system exerts minimal interference to existing components of \dl-powered systems and is thus compatible with existing defenses. Moreover, the synergistic integration of \system with other mechanisms (e.g., defensive distillation~\cite{Papernot:2016:sp}) delivers even stronger defenses for \dnns.

Our contributions can be summarized as follows.
\begin{itemize}
    \item We expose the limitations of existing defenses against adversarial input attacks, which motivates the design of \system. To our best knowledge, our empirical evaluation (\myref{sec:measure}) is the most comprehensive study to date on varied attack and defense models.

    \item We identify the minimality principle underlying most attacks, which entails the distinct properties shared by adversarial inputs. We design and implement \system, which effectively exploits such properties in a principled manner (\myref{sec:model}).

    \item We analytically and empirically validate \system's efficacy (\myref{sec:analysis}  and \myref{sec:evaluation}), which achieves promising accuracy in discriminating adversarial inputs and even uncovering their correct classification outputs.

    \item We investigate the adversary's possible countermeasures and their implications. We also empirically explore the synergistic integration of \system with existing defense mechanisms (\myref{sec:evaluation} and \myref{sec:discussion}).

\end{itemize}



All the source code of this paper will be released on {\sf GitHub} after the double-blind review is complete.
