\section{Evaluation}
\label{sec:evaluation}

In this section, we empirically evaluate the efficacy of \system. Specifically, our experiments are designed to answer the following key questions.
\begin{itemize}
\item Q: {\em Does \system effectively distinguish adversarial and genuine inputs in an attack-agnostic manner?}
%and also accurately recover their original classification outputs for adversarial cases?}

A: (\myref{sec:sec1}) \system achieves very high detection accuracy across benchmark datasets and attack models. For instance, its average recall and precision are 99.5\% and 97.0\% on the \mnist dataset. %Most impressively, it achieves
%with success rate of 59.7\% in uncovering the correct classification outputs for adversarial cases.
%Particularly, with the number of probes increases, its accuracy can be further improved.
In particular, this performance is achieved under the same parameter setting without any tuning to datasets or attack models.

\item Q: {\em Does \system cause new attack-defense arm races?}

A: (\myref{sec:sec2}) To evade \system's detection, the adversary has to abandon the minimality principle by significantly increasing the distortion amplitude, which weakens the attack's evasiveness with respect to other detection mechanisms and has fairly low success rate (less than 40\% across benchmark datasets).

\item Q: {\em Does \system complement other defense solutions?}

A: (\myref{sec:sec3}) \system exerts minimal interference to existing system components, and is compatible with any defense mechanisms. Further, defense-enhanced \dnns, with stronger generalization capabilities, provide even better foundations for \system to operate, leading to above 4\% increase in both precision and recall.

\end{itemize}


\subsection{Experimental Settings}

We use the same set of original \dnn models and benchmark datasets as in the empirical study in~\myref{sec:measure}. The details of \dnn models and datasets are referred to Appendix A and B. We also evaluate \system against the set of representative attacks in~\myref{sec:attack}. The default setting of parameters is as follows: \# patches $n = 8$, ranking coefficient $c = 1.25$, and \# shadow inputs $k = 4$.


% In particular, as the required number of minimum adversarial radius (\mar) probes varies, we sequentially apply variants of \ttca which differ in the number of input components perturbed at each iteration. By default,


\subsection{Discriminant Power of EagleEye}
\label{sec:sec1}

In this set of experiments, we show that \system is capable of performing accurate detection of adversarial inputs in an  attack-agnostic manner. We prepare the testing set as follows. We first randomly sample 5,000 inputs from each dataset, which form the pool of genuine inputs. We then apply all the attacks to each genuine input to generate its adversarial versions (with the adversarial target class randomly selected and the perturbation amplitude limited as 112 for \ttp and \ttca and 0.25 for \ttg and \ttha as in~\cite{Goodfellow:2014:arxiv,Papernot:2016:sp}); those successfully crafted instances form the pool of adversarial inputs. Due to the high success rates of adversarial attacks (see Table~\ref{tab:robustness}), the genuine and adversarial pools are quite balanced.


We apply \system to detect adversarial inputs and use the following metrics to measure its performance:
\begin{displaymath}
\textrm{\small Recall} = \frac{tp}{tp + fn}  \quad \textrm{\small Precision} =  \frac{tp}{tp + fp}
\end{displaymath}
where $tp$, $fp$, and $fn$ represent the number of true positive, false positive, and false negative cases (adversarial: +, genuine: -). Intuitively, recall and precision measure the sensitivity and specificity of \system.



\begin{table}{\small
  %  \hspace{-5pt}
\begin{tabular}{|c|c|c|c|c|c|}
  \hline
  \multirow{2}{*}{\bf Dataset} & \multirow{2}{*}{\bf Metric} & \multicolumn{4}{c|}{\bf Attack Model}\\
   \cline{3-6}
&  &  {\bf G-} & {\bf H-} & {\bf P-} & {\bf C-} \\
    \hline
    \hline
 \multirow{2}{*}{\mnist}  & Precision &  95.0\% & 96.5\% & 98.4\% & 98.0\% \\
 & Recall &   99.2\% & 100.0\% & 99.2\% & 99.6\% \\
% & Recovery &   25.6\% & 85.6\% & 71.2\% & 56.0\% \\
\hline
\hline
\multirow{2}{*}{\cifar}  & Precision &   88.8\% & 90.8\% & 89.9\% & 91.2\% \\
& Recall &   98.4\% & 94.4\% & 96.4\% & 99.6\% \\
% & Recovery &   36.0\% & 44.8\% & 66.0\% & 60.8\% \\
\hline
\hline
\multirow{2}{*}{\svhn}  & Precision &   88.3\% & 88.6\% & 87.3\ & 88.2\% \\
& Recall &   99.2\% & 96.4\% & 99.2\% & 98.8\% \\
% & Recovery &   28.0\% & 46.0\% & 58.0\% & 62.8\% \\
\hline
% \hline
% CIFAR & 85.2\% & 67.8\% & 78.2\% & 81.7\% & 81.0\% & 88.5\% & 80.5\% \\
% SVHN & 95.2\% & 94.1\% & 93.3\% & 94.5\% & 93.8\% & 94.9\% & 86.0\% \\
%     \hline
\end{tabular}
\caption{\system detection accuracy with respect to different benchmark datasets and attack models. \label{tab:dacc}}}
\end{table}

%
%
%
% \begin{figure}
% %\hspace{-10pt}
% \epsfig{file=figures/reachable2.eps, width=90mm}
% \caption{Distribution of genuine-adversarial input pairs with respect to the number of reachable classes. \label{fig:reachable}}
% \end{figure}


Table~\ref{tab:dacc} summarizes \system's performance against varied attacks on the benchmark datasets. Observe that \system provides universal, attack-agnostic protection for \dnns: across all the cases, \system achieves high precision (above 87\%) and recall (above 94\%), indicating its strong discriminant power against adversarial inputs. Note that in all these cases, \system has slightly better recall than precision.
We hypothesize that those false positive cases are genuine inputs with low classification confidence (see \myref{sec:analysis}), while in the simplest \mnist dataset, most inputs have fairly high confidence scores, resulting in its lowest false positive rates.

We also examine \system's impact on genuine cases misclassified by \dnns. For those cases, \system detects them as genuine inputs with accuracy of 96.4\%, 94.4\%, and 95.6\% on the \mnist, \cifar, and \svhn dataset respectively, implying that \system's performance is relatively independent of the \dnn's accuracy.

% We confirm this hypothesis by computing the average confidence of false positive cases: on the \cifar dataset, it is equal to 0.34, 0.37, 0.55, and 0.47 under \ttg, \tth, \ttp, and \ttha respectively. Meanwhile, in the case of \mnist, most inputs have confidence close to 1.44, resulting in much less false positive cases. In the case of \mnist, most inputs have confidence close to 1.44, resulting in much less false positive cases.

%

We then evaluate \system's effectiveness in uncovering the correct classification of adversarial inputs. Among the adversarial cases detected by \system, we calculate the recovery rate as the proportion of inputs whose classes are correctly inferred. We find \system's recovery is effective against both linear and nonlinear attacks. For example, against \ttha, it achieves 85.6\% recovery on the \mnist dataset; against \ttpa, it achieves 66.0\% recovery on the \cifar dataset. Note that this performance is achieved under the same parameter setting without any adjustment towards datasets or attacks; thus, we believe \system's performance can be further improved by fine parameter tuning. The experiments on parameter tuning are referred to Appendix E.



%
% Such recovery rates are also listed in Table~\ref{tab:dacc}. It is observed that on average \system is able to uncover over 73\% of cases. The recovery error is explained as follows: due to the high nonlinear and nonconvex nature of \dnn, in the input manifold space, one class region is often adjacent to multiple other regions; thus, adversarial inputs tend to be proximate to multiple class boundaries. We validate our explanation by investigating the number of classes reachable from each genuine and adversarial input pair. Figure~\ref{fig:reachable} shows the heatmap of the distribution for the \svhn dataset (adversarial inputs are generated by \ttg- and \ttpa). Observe that a majority of genuine and adversarial inputs can reach the boundaries of all other classes.
%

%
% \begin{figure}
% %\hspace{-10pt}
% \epsfig{file=figures/length.eps, width=90mm}
% \caption{Performance of \system with respect to the number of \mar probes on the \cifar dataset. \label{fig:length}}
% \end{figure}
%
% We now investigate the impact of the ranking coefficient $c$ on \system's performance (). Recall that $c$ balances the
%
% using multiple \mar probes enhances \system's statistical reliability to discriminate adversarial inputs.
% Figure~\ref{fig:length} shows how the performance of \system as a function of the number of \mar probes, on the \cifar dataset (under \ttp- and \ttca). Observe that in general, both precision and recall scores tend to grow with the probe number; however, the growth margin gradually decreases. Also note more probes imply higher computational costs. Thus the optimal setting of the probe number needs to be empirically determined, based on the required accuracy and efficiency.

To summarize, in an attack-agnostic manner, \system effectively discriminates maliciously tampered inputs and even uncovers their original classification outputs. It can be seamlessly deployed into any existing \dl-powered systems. The analysis results of \system can be combined with the \dnn classification results to form comprehensive reports to enable more informative decision-making for the system operators.



\begin{table}{\small
    \centering
\begin{tabular}{|c|c|c|c|c|}
  \hline
  \multirow{2}{*}{\bf Datasets} & \multicolumn{4}{c|}{\bf Attack Models}\\
   \cline{2-5}
&  {\bf G-Attack} & {\bf H-Attack} & {\bf P-Attack} & {\bf C-Attack} \\
    \hline
    \hline
\mnist  & 62.2\% &  94.6\% & 61.2\% & 61.6\% \\
\hline
\cifar  & 70.0\% &  91.2\% & 42.8\% & 79.4\%\\
\hline
\svhn  & 74.8\% & 91.6\% & 27.2\% & 47.6\% \\
\hline
% \hline
% CIFAR & 85.2\% & 67.8\% & 78.2\% & 81.7\% & 81.0\% & 88.5\% & 80.5\% \\
% SVHN & 95.2\% & 94.1\% & 93.3\% & 94.5\% & 93.8\% & 94.9\% & 86.0\% \\
%     \hline
\end{tabular}
\caption{Failure rates of adversarial inputs to reach desirable \ars with respect to benchmark datasets and attacks. \label{tab:success}}}
\end{table}

\subsection{Adversary's Countermeasures}
\label{sec:sec2}

\begin{figure}
\hspace{-15pt}
\epsfig{file=figures/cm1_svhn.eps, width=85mm}
\caption{Distribution of ratio of distortion amplitudes (without vs. with minimality principle) on \svhn. \label{fig:pratio}}
\end{figure}

Next we discuss adversary's possible countermeasures to evade \system's detection and their implications. Remember that the fundamental cornerstone of \system is the minimality principle underlying varied adversarial input attacks. Therefore, to evade \system's detection, one natural option for the adversary is to abandon this principle. Rather than applying the minimum possible distortion, she attempts to find a suboptimal perturbation vector leading to larger \ars.
%
% astute readers may point out that one possible way to evade the detection of \system is to give up this principle; that is, rather than perturbing the most salient input elements in order to minimize the perturbation amplitude, an attacker may choose to perturb more non-salient elements.
%
%
% if we continue to add perturbations, even after the misclassification has been achieved. we intend to measure the trade-off between \system's distinguishing power and the number of perturbed components.

Specifically, after the adversarial input $\mbxp$ achieves the misclassification $\mbyp$, the adversary continues the perturbation process, in an attempt to increase $\mbxp$'s \ar. Following the analysis in~\myref{sec:analysis}, here we empirically evaluate the implications of the adversary's countermeasure. Assume that for a given genuine input $\mbx$, the adversary desires to make $\mbxp$ with \ar comparable with that of $\mbx$.

We first investigate the cases that fail to achieve the desired \ar under the given perturbation amplitude, which is increased to 448 for \ttp and \ttca, and 1 for \ttg and \ttha. Table~\ref{tab:success} lists the failure rates of adversarial inputs on each dataset. For example, even by quadrupling the distortion amplitude, over 40\% of the inputs cannot achieve \ars comparable with their genuine counterparts on \cifar. This is explained by that due to the highly nonlinear, nonconvex nature of \dnns, the \ar of an adversarial input is a nonlinear function of the distortion amplitude as well. Thus, solely increasing the amplitude does not necessarily lead to the desired \ar.





Moreover, we examine those cases that indeed reach the desired \ars. Let $\mbxp$ and $\mbxp'$ represent the adversarial input generated with and without the minimality principle. We measure the ratio of their distortion amplitudes. Figure~\ref{fig:pratio} plots the distribution of such ratios on the \svhn dataset (experiments on other datasets in Appendix E). Note that regardless of the concrete attacks, for a majority of adversarial inputs, in order to make them evasive against \system's detection, the adversary has to amplify the distortion amplitude by more than 2.5 times than that guided by the minimality principle. Such large distortion would be detectable by potential anomaly detection systems or even human vision~\cite{Papernot:2016:eurosp}. This empirical evidence also validates our analysis in~\myref{sec:analysis}.


Besides performing delicate perturbations to increase the \ars of adversarial inputs, the adversary may also try random perturbations in hope of finding adversarial inputs with satisfying \ars. We simulate this countermeasure by generating adversarial inputs with the minimum principle and then applying random perturbation to them. Figure~\ref{fig:pratio2} shows the ratio of distortion amplitudes after and before random perturbations (experiments on other datasets in Appendix E). Again, this countermeasure leads to significantly increased distortion amplitudes.

\begin{figure}
\hspace{-15pt}
\epsfig{file=figures/cm2_svhn.eps, width=85mm}
\caption{Distribution of ratio of distortion amplitudes (random perturbation vs. minimality principle) on \svhn. \label{fig:pratio2}}
\end{figure}


To summarize, \system creates a difficult dilemma for the adversary: she has to balance the attack's evasiveness against \system and other potential detection systems. In many cases, this balance is difficult to find, due to the highly nonlinear, nonconvex nature of \dnns.

%
%
% \begin{figure}
% %\hspace{-10pt}
% \epsfig{file=figures/dtl.eps, width=90mm}
% \caption{Performance of \system on top of defensive distilled \dnns on the \svhn dataset. \label{fig:dtl}}
% \end{figure}
%

\begin{table}{\small
    \centering
\begin{tabular}{|c|c|c|c|}
  \hline
  \multirow{2}{*}{\bf Cases} & \multicolumn{3}{c|}{\bf Dataset}\\
   \cline{2-4}
&  \mnist & \cifar & \svhn \\
    \hline
    \hline
defended by DD & 100.0\% &  98.4\% & 90.8\%\\
\hline
\hline
uncaptured by DD  & 100.0\% &  100.0\% &  100.0\% \\
\hline
\end{tabular}
\caption{Accuracy of \system against adversarial inputs defended/uncaptured by defensive distillation (DD).\label{tab:compat}}}
\end{table}

\subsection{EagleEye and Other Defenses}
\label{sec:sec3}

One advantage of \system is that it exerts minimal interference with existing system components. This feature makes it complementary with other defense mechanisms, while their integration often leads to synergistic effects. In this set of experiments, we validate this hypothesis by investigating the effects of applying \system on top of defensive distillation (\dd)~\cite{Papernot:2016:sp}.
%Recall that \dd improves \dnn robustness by amplifying its classification confidence, which is beneficial for \system as genuine inputs with larger confidence tend to have larger \ars. Meanwhile,
As shown in Table~\ref{tab:robustness}, while effective against \ttg, \tth, and \ttpa, \dd is vulnerable to adaptively designed attacks (e.g., \ttca). For example, \ttca achieves near-perfect success rates on benchmark datasets.

First we consider adversarial inputs that are successfully defended by \dd. In particular, we apply \ttpa over original \dnns and collect all the adversarial inputs which pass original \dnns but are defended by \dd. For each adversarial input $\mbxp$ in this category, \dd recognizes its correct class but is unaware that $\mbxp$ is adversarial.
The first row of Table~\ref{tab:compat} lists \system's accuracy of detecting such cases as adversarial. It is clear that for cases successfully defended by \dd, \system can provide additional diagnosis information for the system operator.

Second we consider adversarial inputs that penetrate the protection of \dd. In particular, we apply \ttca over defensive distilled \dnns and collect all the successfully generated adversarial inputs. Table~\ref{tab:compat} lists \system's accuracy of detecting such cases as adversarial. \system achieves perfect detection rate in this category. It is clear for cases that penetrate the protection of \dd, \system provides another safe net.

%
% protection of \system for defensive-distilled \dnns against \ttg- and \ttca. Figure~\ref{fig:dtl} shows the performance of \system as a function of the \mar probe number on the \cifar dataset (similar performance is observed on other datasets). It is first observed that \system effectively detects adversarial inputs that pass the protection of defensive distillation. Across all the cases, its precision and recall are above 79\% and 81\% even when only one \mar probe is used. Meanwhile, due to the confidence amplification by defensive distillation, \system achieves even higher precision and recall, compared with original \dnn cases. For example, with the \mar probe number set as 4 (the same setting in Table~\ref{tab:dacc}), one observes that precision and recall increases by 4\% and 5\% against \ttga. Moreover, the performance of \system is now less sensitive to the \mar probe number, compared with Figure~\ref{fig:length}. This indicates that with defensive distillation in deployment, \system is able to perform more accurate detection with lower computational cost.

We are not arguing to replace exiting defenses with \system. Rather, we believe it is beneficial to integrate complementary defense mechanisms, which significantly sharpens the edge of vulnerability to adaptive attacks.
