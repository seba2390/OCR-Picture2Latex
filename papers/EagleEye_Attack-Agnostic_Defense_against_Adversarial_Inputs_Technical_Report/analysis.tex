\section{Analysis of EagleEye}
\label{sec:analysis}

In the preceding sections, we present \system that applies adversarial radius analysis (\mar) to distinguish genuine and tampered inputs. Next, we analytically explore the effectiveness of \mar. Note that we do not intend to provide a definitive argument about using \system (or \mar) to mitigate adversarial input attacks, but rather we view it as an initial step towards building universal, attack-agnostic defenses against adversarial inputs for \dl and machine learning systems in general. In specific, our analysis attempts to draw the connection between \mar, \dnn generalizability, and learning theory.


Furthermore, we discuss the adversary's possible countermeasures to evade \system's detection. Recall that \system is built on the premise that the attacks follow the minimality principle: the adversary attempts to minimize the distortion amplitude to maximize the attack's evasiveness. We explore attack variants that (partially) abandon this principle. This amounts to investigating the adversary's design spectrum: the tradeoff between the evasiveness with respect to \system and the distortion amplitude (i.e., the evasiveness with respect to other detection mechanisms), which provides insights for the best practice of \system.


% new work on explaining the abundance of adversarial inputs~\cite{Szegedy:2013:arxiv,Tanay:2016:arxiv}


\subsection{Effectiveness of Radius Analysis}
\label{sec:mara}

The fundamental premise of \mar is that genuine inputs are inclined to have larger adversarial radii than their adversarial counterparts. Below we provide the theoretical underpinnings of this property. For simplicity of exposition, we exemplify $l_1$-norm as the measure of distortion amplitude, while our discussion generalizes to other metrics as well.
%In other words, $\mbx$ extends longer radii to surrounding class boundaries than $\mbxp$.

From the view of an adversarial input $\mbxp$, recall that attack ${\mathcal A}$ produces $\mbxp$ by carefully perturbing a ÃŸgenuine input $\mbx$. Regardless of its concrete implementation, ${\mathcal A}$ is essentially designed to solve the optimization problem:
\begin{equation*}
    \min_{\mbxp} ||\mbxp - \mbx|| \quad \mathrm{s.t.} \quad f(\mbxp) = \mbyp
\end{equation*}
where $||\mbxp - \mbx||$ reflects the perturbation amplitude. Assume that ${\mathcal A}$ operates on $\mbx$ with a sequence of $t$ perturbations and $\mbr^{(i)}$ represent the perturbation vector at the end of the $i^{\mathrm{th}}$ iteration ($i = 1, 2, \ldots, t$), with $\mbxp =  \mbx + \mbr^{(t)}$. The minimality principle implies that ${\mathcal A}$ achieves the desired misclassification only after the $t^{\mathrm{th}}$ iteration, i.e.,
\begin{equation*}
\begin{array}{l}
f(\mbx + \mbr^{(t-1)}) = \mby\\
f(\mbx + \mbr^{(t)}) = \mbyp
\end{array}
\end{equation*}

Therefore, $(\mbx + \mbr^{(t-1)})$ and $(\mbx + \mbr^{(t)})$ represent two inputs lying between the class boundary of $\mby$ and $\mbyp$. It is also noted that $\mbr^{(t-1)}$ and $\mbr^{(t)}$ differ only by a few components, depending on the concrete implementation of ${\mathcal A}$. For example, the attacks in~\cite{Papernot:2016:eurosp,Carlini:2016:arXiv} perturb two input components at each round, and $\mbr^{(t-1)}$ and $\mbr^{(t-1)}$ differ by two components. We thus conclude that $\mbxp$ resides extremely close to the class boundary of $\mby$ and $\mbyp$.

% Furthermore, let $\vec{\sigma}(\mbxp) = [\sigma_1, \sigma_2, \ldots]$ be the output of the softmax function (see Figure~\ref{fig:network}). Given that $\mbyp$ is the predicted class of $\mbxp$ by $f$, $\mbyp = \max_{i}\sigma_i(\mbxp)$.

Next, we move on to explain the effectiveness of \mar from the perspective of a genuine input $\mbx$. Our intuition is that
if $\mbx$ is classified as class $\mby$ by \dnn $f$ with high confidence, its radii to the class boundaries induced by $f$ must be reasonably large. To make this intuition more precise, we resort to statistical learning theory on the connection of classification confidence and \mar.

% \begin{definition}[Classification Margin]
% The classification margin of an input-output pair $(\mbx, \mby)$ is defined as:
% \begin{equation*}
%   \phi(\mbx)  = \mathrm{sup}\{  r : ||\mbx - \mbxp|| \leq r \Rightarrow f(\mbxp) = \mby,\; \forall \mbxp \}
% \end{equation*}
% \end{definition}

\begin{definition}[Confidence]
The classification \underline{confidence} of an input-output pair $(\mbx, \mby)$ (i.e., $f$ classifies $\mbx$ as $\mby$) is measured by the difference of the largest and second largest probabilities in $f(\mbx)$ (e.g., the softmax output). Formally,
$\phi(\mbx)  =  \min_{\mbyp \neq \mby} \sqrt{2}(\delta_{\mby} - \delta_{\mbyp}) \cdot f(\mbx)
$, where $\delta_i$ is the Kronecker delta vector with the $i^{\mathrm{th}}$ element being 1 and 0 otherwise and $\cdot$ denotes inner product.
\end{definition}

We now introduce an interesting result that connects the concept of classification confidence with \mar (adapted from~\cite{Sokolic:2016:arXiv} , pp. 14):

\begin{theorem*}
Given a \dnn $f$ and a genuine input $\mbx$, with $W^{(l)}$ being the weight matrix of the $l^{\mathrm{th}}$ layer of $f$, we have the following bound for the \ar of $\mbx$:
\begin{equation*}
\rho(\mbx) \geq  \frac{  \phi(\mbx) }{  \prod_{W^{(l)}} ||W^{(l)}||_F }
\end{equation*}
where $||\cdot||_F$ represents Frobenius norm.
\end{theorem*}

Intuitively, if a \dnn $f$ makes confident classification of $\mbx$, $\mbx$ tends to have a large \ar. Astute readers may point to the possibility of using confidence instead of \mar to adversarial inputs. However, note that high confidence is only one sufficient condition for large \ars; as observed in our empirical evaluation in~\myref{sec:evaluation} and previous work~\cite{Goodfellow:2014:arxiv}, there are adversarial inputs with high classification confidence but show small \ars.

Another implication of the above theorem is that increasing the classification confidence of \dnns is beneficial for discriminating adversarial inputs. Along this direction, defensive distillation~\cite{Papernot:2016:sp} is designed exactly for this purpose: by increasing the temperature $\tau$, it amplifies the probability difference in the outputs of \dnns.
In~\myref{sec:evaluation}, we empirically show the synergistic effects of integrating defensive distillation and \system.


\subsection{Adversary's Dilemma}

We now explore possible attack variants that attempt to evade \system's detection  or \mar in specific. Since \system is built on top of the minimality principle underlying varied attack models, one possible way to evade its detection is to (partially) abandon this principle in preforming adversarial perturbations.

Specifically, following \myref{sec:mara}, let $\mbr^{(i)}$ be the perturbation vector at the end of the $i^{\mathrm{th}}$ iteration and assume the adversary achieves the desired misclassification right after the $t^{\mathrm{th}}$ iteration:
$f(\mbx + \mbr^{(t)}) = \mbyp$. Now, instead of stopping here, the adversary keeps on perturbing $\mbx$, attempting to increase its \ar $\rho(\mbxp)$. Even in the ideal case, the adversary needs to perturb at least $\rho(\mbx)$ input components, resulting in an approximate perturbation amplitude of $2\cdot\rho(\mbx)$. Intuitively, if $\mbx$ is selected at random, then the quantity of $2\cdot\rho(\mbx)$ represents the distance of two genuine inputs, while in real datasets, the difference of distinct inputs is fairly discernible even to human eyes. Furthermore, due to the high nonlinearity of \dnns, the extra perturbation amplitude is often much larger this lower bound. The empirical validation of this hypothesis is given in~\myref{sec:sec2}, in which we also consider the adversary's another countermeasure of random perturbations.

This analysis above reveals a difficult dilemma for the adversary: she desires to preserve the \ar of an adversarial input to evade \system's detection; yet, to do so, she is forced to introduce extra perturbations sufficient to transform one genuine input to another, thereby significantly reducing the attack's evasiveness regarding other detection mechanisms (e.g., human vision).
% We empirically explore this dilemma faced by the adversary in~\myref{sec:evaluation}.


%the recent breakthrough in computational learning theory on the connection between generalization error and classification margin.
%
% In statistical learning theory, generalization error (\gge) is one key measure of how reliably an \ml algorithm is able to classify previously unseen inputs~\cite{Vapnik:1995:book}. Specifically, in training \dnn model $f$ that predicts output $\mby$ for input $\mbx$, the goal is to minimize the following expected error over all possible values of $(\mbx, \mby)$:
% \begin{equation*}
%   \mathrm{I}[f] = \int_{\mbx \times \mby }\ell(f(\mbx), \mby) \mathrm{Pr}(\mbx, \mby) \mathrm{d}\mbx \mathrm{d} \mby
% \end{equation*}
% where $\mathrm{Pr}(\mbx, \mby)$ is the unknown joint probability distribution of $\mbx$ and $\mby$. While it is impossible to compute $\mathrm{I}[f]$ without $\mathrm{Pr}(\mbx, \mby)$, one resorts to computing the empirical error over the training data ${\mathcal D}$:
% \begin{equation*}
%   \hat{\mathrm{I}}[f] = \frac{1}{|{\mathcal D}|}\sum_{\mbx \times \mby \;  \in {\mathcal D}}\ell(f(\mbx), \mby)
% \end{equation*}
%
% The generalization error is the difference of expected and empirical error: $\mathrm{G}[f] =   \mathrm{I}[f] -  \hat{\mathrm{I}}[f]$, and $f$ is said to generalize if $\lim_{|{\mathcal D}| \rightarrow \infty} \mathrm{G}[f] = 0$. Note that the generalization error of $f$ is an inherent property of its \dnn architecture, independent of training datasets or algorithms. For a \dnn model to be useful in practice, its \gge needs to be reasonably low.
