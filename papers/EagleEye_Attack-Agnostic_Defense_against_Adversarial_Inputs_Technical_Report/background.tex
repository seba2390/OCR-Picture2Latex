


\begin{figure}
\centering
\epsfig{file=figures/network.eps, width=80mm}
\caption{Illustration of \dnn models. \label{fig:network}}
\end{figure}



\section{Attacks and Defenses}
\label{sec:background}

In this section, we introduce a set of fundamental concepts and assumptions, and survey representative attack and defense models in literature.

\subsection{Deep Learning}

\dl represents a class of machine learning algorithms designed to learn high-level abstraction of complex data using multiple processing layers and nonlinear transformations. Figure~\ref{fig:network} shows a typical \dnn architecture.

In this paper, we primarily focus on image classification tasks, while our discussion generalizes to other settings (see \myref{sec:discussion}). In these tasks, the \dnn encodes a mapping $f: \mathcal{X} \rightarrow \mathcal{O}$,  which assigns a given image $\mbx$ (represented as a vector) in the input space ${\mathcal X}$ to one of a set of classes ${\mathcal O}$. For example, with the \mnist dataset as inputs, $f$ classifies each image as one of ten digits `0'-`9'. As shown in Figure~\ref{fig:network}, the last layer of $f$ often employs a softmax function. Specifically, let
\begin{displaymath}
 \vec{z}  \triangleq  \left[z_1, z_2, \ldots \right]\qquad \mbs  =  \frac{\exp(\mbz)}{\sum_i \exp(z_i)} \triangleq \left[\sigma_1, \sigma_2, \ldots \right]
\end{displaymath}
 respectively be the input and output of this layer. Then $\sigma_i$ is the probability that $\mbx$ belongs to the $i^{\rm th}$ class. The predicted class of $\mbx$ is given by $f(\mbx) = \arg\max_i \sigma_i$.

We consider \dnns obtained via supervised learning. Specifically, to train a \dnn $f$, the algorithm takes a training set, of which each instance $(\mbx, o) \in {\mathcal X} \times {\mathcal O}$ constitutes an input and its ground-truth class, and determines the parameter setting of $f$ via minimizing a loss function $\ell(f(\mbx), o)$ (e.g., the cross entropy of ground-truth classes and $f$'s outputs).

\subsection{Attack Methodologies}
\label{sec:attack}

Targeting a \dnn $f$ deployed in use, the adversary attempts to trigger $f$ to misbehave by feeding it with carefully crafted inputs. Given a genuine input $\mbx$ correctly classified by $f$, the adversary generates an adversarial one $\mbxp$ by perturbing $\mbx$ with an insignificant amplitude (e.g., a few pixels). The difference of $\mbx$ and $\mbxp$ ($\mbr = \mbxp - \mbx$) is referred to as the {\em perturbation vector} (\pv).

We differentiate two attack scenarios. In an untargeted attack, the adversary is interested in simply forcing $f$ to misclassify, i.e., $f(\mbxp)\neq f(\mbx)$. In a targeted attack, she further desires for a particular target output $\mbyp$, i.e., $f(\mbxp) = \mbyp$. In the following we focus our discussion on targeted attacks, while the extension to untargeted attacks is straightforward.
%
% For simplicity of exposition, we assume:
% \begin{itemize}
% \item All the components of $\mbx$ are normalized to the interval of $[-1, 1]$. For example, in the case of \rgb images, this can be achieved by dividing the value of each pixel by 127.5 and subtracting it by 1.
% %\item Following existing work~\cite{Papernot:2016:eurosp,Papernot:2016:sp,Carlini:2016:arXiv}, we only consider the ``flipping'' perturbation, which sets an input component to either fully-on (`$1$') or fully-off (`$-1$'). In other words, each component of $\mbr$ is assigned a value from the set of $\{-2, 0, +2\}$.
% \item To ensure the validity of $\mbxp$, after each perturbation, a clipping step follows:
% $\mbxp \leftarrow \max(\min(\mbxp, \vv{1}), -\vv{1})$,
%  where $\vv{1}$ represents an all-one vector.
%\item The distortion amplitude is measured by the number of perturbed components in $\mbx$, i.e., the $l_1$ norm of $\mbr$.\footnote{
%Note that there are attacks that only partially follow these assumptions. For example, the original attacks reported in~\cite{Goodfellow:2014:arxiv,Huang:2015:arxiv} perturb all the input components by a small quantity (e.g., $.007$), instead of flipping a small number of pixels. In such cases, the distortion amplitude is measured by the $l_\infty$ norm of $\mbr$. We refer the readers to the discussion on other norms in Appendix C.}
%\end{itemize}

A variety of attack models have been proposed in literature~\cite{Goodfellow:2014:arxiv,Huang:2015:arxiv,Papernot:2016:eurosp,Carlini:2016:arXiv}. Despite their variations in concrete crafting strategies, they all roughly follow a two-step procedure: (i) {\em saliency estimation:} the adversary assesses the impact of changing each input component on the classification output; (ii) {\em perturbation selection:} the adversary uses the input saliency information to select and perturb a subset of input components. Based on the concrete implementation of the two steps,  we classify existing attacks in two major categories.

\subsubsection{Linear Crafting Attacks}
The class of linear attacks estimate the impact of distorting different input components on $f$'s output via linear approximations and find the \pv $\mbr$ that maximizes the probability of the target output $\mbyp$. Next we detail two representative attack models.

\paragraph{Goodfellow's Attack.\;} Goodfellow {\em et al.}~\cite{Goodfellow:2014:arxiv} proposed the first linear attack model, which computes the gradient of the loss function $\ell$ with respect to the input $\mbx$ and determines $\mbr$ as a gradient sign step in the direction that increases the probability of the target output $\mbyp$.


Specifically, let $\textrm{sign} (\triangledown_{\mbx} \ell (f(\mbx), \mbyp) )$ be the gradient sign of $\ell$ with respect $\mbx$ for given $\mbyp$. Then the \pv is defined as:
$\mbr = - \delta  \cdot \textrm{sign} (\triangledown_{\mbx} \ell (f(\mbx), \mbyp) )$,
where $\delta$ is a parameter controlling the distortion amplitude (i.e., $l_\infty$-norm of $\mbr$). Often, the adversary seeks the minimum $\delta$ to achieve misclassification:
$\min_\delta f( \mbx + \mbr) = \mbyp$.



\paragraph*{Huang's Attack.\;} Huang {\em et al.}~\cite{Huang:2015:arxiv} introduced another linear attack model, which is constructed upon a linear approximation of the output of the softmax layer, i.e., the last layer of a \dnn model (see  Figure~\ref{fig:network}).

Let $\mbsp$ be the softmax output of $\mbxp = \mbx + \mbr$. This attack approximates $\mbsp$ using a linear form:
$\mbsp \approx \mbs + \mbJ \mbr$,
 where $\mbJ = \frac{{\rm d} \mbs}{{\rm d} \mbx}$ is the Jacobian matrix.
Assume the original output $\mby$ and target output $\mbyp$ respectively correspond to the $j^{\rm th}$ and $j_\epsilon^{\rm th}$ row of $J$, denoted by $J_j$ and $J_{j_\epsilon}$. Let $\Delta_J \triangleq J_{j_\epsilon}  - J_j$. To trigger $f(\mbxp) = \mbyp$, the adversary seeks $\mbr$ that maximizes the difference of the $j_\epsilon^{\rm th}$ and $j^{\rm th}$ component of $\mbsp$, i.e.,
$\max_{\mbr}  \Delta_J  \cdot \mbr$.

Similar to~\cite{Goodfellow:2014:arxiv}, this attack determines $\mbr$ as a step in the sign direction of $\Delta_J$, i.e.,
$ \mbr = \delta \cdot  \textrm{sign}(\Delta_J)$,
where $\delta$ controls the distortion amplitude\footnote{In~\cite{Huang:2015:arxiv} Huang {\em et al.} also give the definition of optimal $\mbr$ when the distortion amplitude is measured by $l_1$- or $l_2$-norm of $\mbr$.}.


\subsubsection{Nonlinear Crafting Attacks}

In linear attacks, the \pvs are found in a single attempt. In comparison, nonlinear attacks construct the \pvs iteratively. At each round, the adversary estimates the impact of each input component on the classification output, then selects several components to perturb, and checks whether the updated input causes the target misclassification.
Next we detail two representative nonlinear attacks.

\paragraph*{Papernot's Attack.\;} Papernot {\em et al.}~\cite{Papernot:2016:eurosp} proposed a saliency map to guide the crafting process. Intuitively, this map describes the impact of each input component on the output. Given the current input $\mbx$ (with previously selected components perturbed) and target output $\mbyp$ (corresponding to the $j_\epsilon^{\rm th}$ component of $\mbs$), the $i^{\rm th}$ component is associated with a pair of measures:
\begin{displaymath}
\alpha_i  =  \frac{\partial \sigma_{j_\epsilon}}{\partial x_i} \qquad
\beta_i  =   \sum_{ j \neq j_\epsilon }\frac{\partial \sigma_j}{\partial x_i}
\end{displaymath}
where $\alpha_i$ is its impact on the probability of $\mbyp$, while $\beta_i$ is its impact on all the other classes.


The attack consists of multiple iterations of a greedy procedure.
At each round, two components with the largest value of $(-\alpha \cdot \beta)$ are selected and flipped (to either `$1$' or `$-1$'), and the saliency map is updated accordingly. This process continues until the resulting input is misclassified as $\mbyp$. The distortion amplitude is defined by the number of distorted components, i.e., $l_1$-norm of $\mbr$.

% To increase the attack's success rate, a pair of components $(i, i')$ may be perturbed at each iteration, based on an alternative definition of saliency map~\cite{Papernot:2016:eurosp}:
% \begin{displaymath}
% \alpha_{i, i'}  =  \frac{\partial \sigma_{j_\epsilon}}{\partial x_i} + \frac{\partial \sigma_{j_\epsilon}}{\partial x_{i'}}
% \qquad \beta_{i,i'}  =   \sum_{ j \neq j_\epsilon }\left(\frac{\partial \sigma_j}{\partial x_i} +  \frac{\partial \sigma_j}{\partial x_{i'}}\right)
% \end{displaymath}


\paragraph*{Carlini's Attack.\;} In response to the defensive distillation method~\cite{Papernot:2016:sp}, Carlini and Wagner introduced a nonlinear attack~\cite{Carlini:2016:arXiv}, which differs from~\cite{Papernot:2016:eurosp} in two  aspects:

\begin{itemize}
\item To compensate for the gradient vanishing due to defensive distillation, the input to the softmax layer is artificially amplified by $\tau$ times, where $\tau$ is the ``temperature'' used by defensive distillation. The output of the softmax layer is thus:
$\mbs = \exp(\frac{\mbz}{\tau})/\sum_j \exp(\frac{z_j}{\tau})$.

\item The saliency values of input components are defined as $|\alpha - \beta|$ rather than $(-\alpha \cdot  \beta)$. This modification reduces the complexity of perturbation selection from $O(n^2)$ to $O(n)$, where $n$ is the number of input components. At each iteration, a pair of input components with the largest saliency values are selected and flipped.
\end{itemize}



\subsubsection{Linear vs. Nonlinear Attacks}

Linear attacks require computing gradient or Jacobian only once, while nonlinear attacks often involve multiple rounds of gradient or Jacobian computation. Given their efficiency advantage, linear attacks can be exploited to craft a large number of adversarial inputs.


Meanwhile, existing linear attacks often measure the distortion amplitude by $l_\infty$-norm of the \pv, while existing nonlinear attacks attempt to minimize $l_1$-norm or $l_2$-norm of the \pv.

The empirical comparison of the characteristics of different attacks is detailed in~\myref{sec:measure}.


\subsection{Defense Methodologies}
\label{sec:defense}

A \dnn's resilience against adversarial inputs is inherently related to its stability~\cite{Bastani:2016:arXiv}. Intuitively, a \dnn $f$ is stable, if for any ``proximate'' inputs $\mbx$ and $\mbxp$ (i.e., $||\mbxp - \mbx||$ is small), $f(\mbx)$ and $f(\mbxp)$ are similar. Motivated by this rationale, a plethora of solutions~\cite{Gu:2014:arxiv,Goodfellow:2014:arxiv,Huang:2015:arxiv,Shaham:2015:arxiv,Papernot:2016:sp} have been proposed to improve \dnn stability, which can be roughly classified in three major categories.

\paragraph*{Data Augmentation.\;}
This class of methods improve \dnn stability by proactively generating a set of adversarial inputs and incorporating them in the training process. Formally, given a \dnn $f$ and a known attack $g$, via applying $g$ over $f$, one generates an adversarial input $\mbxp$ for each genuine instance $(\mbx, \mby)$. A new \dnn $f'$ is trained using an augmented objective function:
\begin{equation*}
\min_f \sum_{(\mbx, \mbxp, \mby)}  (\alpha \cdot \ell(f(\mbx), \mby) + (1-\alpha) \cdot \ell(f(\mbxp), \mby))
\end{equation*}
where the parameter $\alpha$ $(0 \leq \alpha \leq 1)$ balances the relative weight of genuine and adversarial inputs.
For instance, Goodfellow {\em et al.}~\cite{Goodfellow:2014:arxiv} suggested equal importance of genuine and adversarial inputs ($\lambda = 0.5$),


Nevertheless, these methods are inherently heuristic, without theoretical guarantee on the robustness or accuracy of the trained \dnn models.

\paragraph*{Robust Optimization.\;}

Another line of work proposed to improve \dnn stability via directly altering its objective function. To be specific, one prepares a \dnn $f$ for the worst possible inputs by training it with an minimax objective function:
\begin{equation}
    \label{eq:rof}
  \min_f  \max_{ ||\mbr || \leq \delta} \ell(f(\mbx + \mbr), o)
\end{equation}

The training algorithm first searches for the ``worst'' \pv (constrained by $\delta$) that maximizes the loss function $\ell$ under the current setting of $f$; it then optimizes $f$ with respect to this \pv. This objective function essentially captures the misclassification error under adversarial perturbations.

Due to the complexity of \dnn models, it is intractable to search for exact worst adversarial inputs. Certain simplifications are often made. Szegedy {\em et al.}~\cite{Szegedy:2013:arxiv} and Gu and Rigazio~\cite{Gu:2014:arxiv} proposed to search for $\mbr$ along the gradient direction of loss function, while Shaham {\em et al.}~\cite{Shaham:2015:arxiv} and Miyato {\em et al.}~\cite{Miyato:2015:arXiv} reformulated this framework for Kullback-Leibler divergence like loss functions.



\paragraph*{Model Transfer.\;} In this method, one transfers the knowledge of a teacher \dnn $f$ to a student \dnn $f'$ such that the model stability is improved.


For instance, Papernot {\em et al.}~\cite{Papernot:2016:sp} proposed to employ distillation~\cite{Ba:2014:nips,Hinton:2015:arxiv}, a technique previously used to transfer the knowledge of an ensemble model into a single model, to improve \dnn stability. Specifically,
\begin{itemize}
\item The teacher \dnn $f$ is trained on genuine inputs; in particular, the input to the softmax layer (see Figure~\ref{fig:network}) is modified as $z/\tau$  for given ``temperature'' $\tau (\tau > 1)$.
\item One evaluates $f$ on the training set and produces a new training set $\{(\mbx, \mbs)\}$, where $\mbs$ encodes the predicted probability distribution (``soft label'') of $\mbx$.
\item By training the student \dnn $f'$ on the new training set under temperature $\tau$, $f'$ is expected to generalize better to adversarial inputs than $f$.
\end{itemize}



Additionally, there is recent work attempting to design new \dnn architectures~\cite{Gu:2014:arxiv} or learning procedures~\cite{Chalupka:2014:arXiv} to improve \dnn stability. Yet, the resulting models fail to achieve satisfying accuracy on genuine inputs. Due to space limitations, we focus our discussion on the above three classes of defense mechanisms.
