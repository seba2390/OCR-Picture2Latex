\section{Additional Related Work}
\label{sec:literature}


Next we review three categories of related work: adversarial machine learning, deep learning-specific attacks and defenses, and robustness of deep neural networks.


Lying at the core of many security-critical domains, machine learning systems are increasingly becoming the targets of malicious attacks~\cite{Barreno:2006:asiaccs,Huang:2011:aisec,Barreno:2010:SML}. Two primary threat models are considered in literature: (i) poisoning attacks, in which the attackers pollute the training data to eventually compromise the learning systems~\cite{Biggio:2012:icml,Xiao:2015:SVM,Rubinstein:2009:imc}, and (ii) evasion attacks, in which the attackers modify the input data at test time to trigger the learning systems to misbehave~\cite{Dalvi:2004:kdd,Lowd:2005:kdd,Nelson:2012:QSE}. Yet, for ease of analysis, most of the work assumes simple learning models (e.g., linear classifier, support vector machine, logistic regression) deployed in adversarial settings.

Addressing the vulnerabilities of deep learning systems to adversarial inputs is more challenging for they are designed to model highly nonlinear, nonconvex functions~\cite{Nguyen:2015:cvpr,Sabour:2016:iclr}. One line of work focuses on developing new attacks against \dnns~\cite{Goodfellow:2014:arxiv,Huang:2015:arxiv,Tabacof:2015:arXiv,Papernot:2016:eurosp,Carlini:2016:arXiv}, most of which attempt to find the minimum possible modifications to the input data to trigger the systems to misclassify.
The detailed discussion of representative attack models is given in~\myref{sec:attack}. Another line of work attempts to improve \dnns resilience against such adversarial attacks~\cite{Goodfellow:2014:arxiv,Gu:2014:arxiv,Huang:2015:arxiv,Papernot:2016:sp}. However, these defense mechanisms often require significant modifications to either \dnn architectures or training processes, which may negatively impact the classification accuracy of \dnns. Moreover, as shown in~\myref{sec:measure}, the defense-enhanced models, once deployed, can often be fooled by adaptively engineered inputs or by new attack variants. To our best knowledge, this work represents an initial step to attack-agnostic defenses against adversarial attacks.

Finally, another active line of research explores the theoretical underpinnings of \dnn robustness. For example,
Fawzi {\em et al.}~\cite{Fawzi:2015:arxiv} explore the inherent trade-off between \dnn capacity and robustness;
Feng {\em et al.}~\cite{Feng:2016:arXiv} seek to explain why neural nets may generalize well despite poor robustness properties;
and Tanay and Griffin~\cite{Tanay:2016:arxiv} offer a theoretical explanation for the abundance of adversarial inputs in the input manifold space.
