
\begin{table*}{\small
    \centering
\begin{tabular}{|c|c|c|c|c|c|c|c|c|}
  \hline
  \multirow{3}{*}{\bf Data}    & \multirow{3}{*}{\bf Original Model} & \multicolumn{7}{c|}{\bf Defense-Enhanced Models}\\
   \cline{3-9}
& &   \multicolumn{4}{c|}{\bf Data Augmentation ($\alpha = 0.5$)} & \multicolumn{2}{c|}{\bf Robust Optimization} & {\bf Model Transfer} \\ \cline{3-8}
    &    & {\bf G-trained} & {\bf H-trained} & {\bf P-trained} & {\bf C-trained} & {\bf $l_1$-norm} & {\bf $l_\infty$-norm} &  {\bf ($\tau = 40$)}\\
    \hline
    \hline
\mnist  &   99.5\% & 98.8\% & 99.0\% & 99.0\% & 98.8\% & 98.1\%& 98.5\%  & 98.9\% \\
\cifar & 85.2\% & 64.4\% & 57.6\% & 75.9\% & 76.7\% & 72.1\%& 71.3\% & 80.5\% \\
\svhn & 95.2\% & 91.3\% & 85.0\% & 91.2\% & 92.4\% & 90.2\%& 81.5\% & 86.0\% \\
    \hline
\end{tabular}
\caption{Classification accuracy of original and defense-enhanced \dnn models with respect to benchmark datasets.  \label{tab:accuracy}}}
\end{table*}

% \begin{table*}{\footnotesize
%    \centering
% \begin{tabular}{|c|c|c|c|c|c|c|c|c|}
%  \hline
%  \multirow{3}{*}{\bf Data}    & \multirow{3}{*}{\bf Original Model} & \multicolumn{7}{c|}{\bf Defense-Enhanced Models}\\
%   \cline{3-9}
%& &   \multicolumn{4}{c|}{\bf Data Augmentation ($\alpha = 0.5$)} & \multicolumn{2}{c|}{\bf Robust Optimization} & {\bf Model Transfer} \\ \cline{3-8}
%    &    & {\bf G-trained} & {\bf H-trained} & {\bf P-trained} & {\bf C-trained} & {\bf $l_1$-norm} & {\bf $l_\infty$-norm} &  {\bf ($\tau = 40$)}\\
%    \hline
%    \hline
%\mnist  &   99.5\% & 99.1\% & 99.0\% & 99.2\% & 99.2\% & 99.4\%& 99.4\%  & 98.9\% \\
%\cifar & 85.2\% & 67.8\% & 78.2\% & 81.7\% & 81.0\% & 88.5\%& 88.5\% & 80.5\% \\
%\svhn & 95.2\% & 94.1\% & 93.3\% & 94.5\% & 93.8\% & 94.9\%& 94.9\% & 86.0\% \\
%    \hline
%\end{tabular}
%\caption{Classification accuracy of original and defense-enhanced \dnn models with respect to benchmark datasets.  \label{tab:accuracy}}}
%\end{table*}


\begin{table*}
    \centering
    {\small
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
  \hline
  \multirow{3}{*}{\bf Data}  & \multirow{3}{*}{\bf Attack}  & \multirow{3}{*}{\bf Original Model} & \multicolumn{7}{c|}{\bf Defense-Enhanced Models}\\
   \cline{4-10}
& & &  \multicolumn{4}{c|}{\bf Data Augmentation ($\alpha = 0.5$)} &  \multicolumn{2}{c|}{\bf Robust Optimization} & {\bf Model Transfer} \\ \cline{4-9}
    &  &  & {\bf G-trained} & {\bf H-trained} & {\bf P-trained} & {\bf C-trained} &  {\bf  $l_1$-norm} &  {\bf  $l_\infty$-norm} &  {\bf ($\tau = 40$)}\\
    \hline
    \hline
   \multirow{4}{*}{\rotatebox{90}{\mnist}}
   & {\bf G-} &15.3\%  & 5.9\%  & 82.4\%  & 40.0\%  & 70.6\%  & 21.2\% & 0.0\%  & 1.18\%  \\
   & {\bf H-} &22.4\%  & 7.1\%  & 87.1\%  & 84.7\%  & 91.8\%  & 22.4\% & 0.0\% & 1.18\%  \\
   & {\bf P-} &{\bf 100.0\%}  & {\bf 100.0\%}  & {\bf 100.0\% } & {\bf 100.0\% } & {\bf 100.0\% } & {\bf 100.0\%}  & {\bf 100.0\%}  & 1.0\%  \\
   & {\bf C-} & 100.0\%  & 100.0\%  & 100.0\%  & 100.0\%  & 100.0\%  &  100.0\% & 100.0\%  & {\bf 100.0\% } \\
    \hline
    \hline
    \multirow{4}{*}{\rotatebox{90}{\cifar}}
    & {\bf G-} &96.5\%  & {\bf 100.0\%}  & {\bf 100.0\%}  & 93.3\%  & {\bf 100.0\%}  & {\bf 98.1\%} & 79.7\%  & 39.8\%  \\
    & {\bf H-} &91.9\%  & 100.0\%  & 100.0\%  & {\bf 100.0\%}  & 93.8\%  & 98.1\% & 71.2\%  & 38.6\%  \\
    & {\bf P-} & {\bf 100.0\%}  &  100.0\%  &  100.0\%  &  100.0\%  &  100.0\%  & 94.3\%  & {\bf 100.0\% } & 21.7\%  \\
    & {\bf C-} & 100.0\%  & 100.0\%  & 100.0\% & 100.0\%  & 100.0\%  & 64.2\% & 98.31\%   & {\bf 100.0\% } \\
     \hline
     \hline
     \multirow{4}{*}{\rotatebox{90}{\svhn}}
     & {\bf G-} &99.5\%  & 36.9\%  & 4.39\%  & 99.5\% & 99.5\%  & 100.0\% & 0.0\%  & 7.5\%  \\
     & {\bf H-} &94.6\%  & 35.4\%  & 5.37\%  & 93.4\%  & 94.9\%  & 96.7\%  & 0.0\%  & 7.5\%  \\
     & {\bf P-} & {\bf 100.0\% } & {\bf 100.0\% } & {\bf 100.0\% } & {\bf 100.0\% } & {\bf 100.0\% } & {\bf 100.0\%} & {\bf 99.7\% } & 3.0\% \\
     & {\bf C-} &100.0\%  & 98.0\%  & 98.1\%  & 100.0\% & 99.5\%  & 95.6\%   & 90.8\% & {\bf 100.0\% } \\
      \hline


\end{tabular}}
\caption{Resilience of original and defense-enhanced \dnn models against adversarial input attacks.  \label{tab:robustness}}
\end{table*}


\section{Empirical Study}
\label{sec:measure}


Next we empirically evaluate the effectiveness of existing defense solutions against varied attacks. To our best knowledge, this evaluation represents the most comprehensive study to date on a range of attack and defense models, and is thus interesting in its own right.




In a nutshell, we show that it is fundamentally challenging to defend against adversarial inputs, which are tailored to target \dnns and crafted with varying strategies.
Unfortunately, existing defenses are inherently static. Although they improve \dnn resilience against specific attacks, the resulting models, once trained and deployed, are unable to adapt to {\em a priori} unknown attacks. The adversary can thus circumvent such defenses by creating inputs exploiting new vulnerability of target \dnns.

% Additionally, the training data augmentation methods use adversarial samples generated by specific attacks in training, entailing unknow effectiveness agaisnt other variants of attacks. Meanwhile, the robust objective function methods, though applicable for a range of attacks, result in inevitable loss in model accuracy.


%We first introduce the setting of our empirical study.

\subsection{Setting of Study}


\paragraph*{Datasets and DNN Models.\;}
To show the prevalence of attack vulnerabilities across different tasks, in our study, we use three benchmark datasets, \mnist~\cite{mnist},  \cifar~\cite{cifar}, and \svhn~\cite{svhn}, which have been widely used to evaluate image classification algorithms. The details of datasets can be found in Appendix A.

We also consider three distinct \dnn architectures and apply each to one of the datasets above. To be specific, we apply the convolutional neural network (\cnn)~\cite{LeCun:1998:cnn}, maxout network (\mxn)~\cite{lin:2014:iclr}, and network-in-network (\nin)~\cite{lin:2014:iclr} models to classifying the \mnist, \cifar, and \svhn datasets, respectively. The implementation details of these \dnn models are referred to Appendix B.



%for the \mnist dataset, we apply a convolutional neural network (\cnn) model~\cite{LeCun:1998:cnn}; For the \cifar dataset, we use an maxout network (\mxn); For the \svhn dataset, we employ an network-in-network (\nin) model~\cite{lin:2014:iclr}.  model~\cite{Goodfellow:2013:icml}.

%
%
% which consists of multiple convolutional layers (with rectifier activation functions) and fully connected layers. The specific \cnn model used in our study adopts an architecture similar to~\cite{Papernot:2016:sp}.
%
%  For the \cifar dataset, we use an maxout network (\mxn) model~\cite{Goodfellow:2013:icml}. \mxn enhances conventional \cnn by introducing {\em maxout} activation functions, which compute maxima across multiple affine feature maps (i.e., pooling across multiple channels, in addition to spatial positions), thereby leading to stronger model averaging capability.
%
% %The architecture of the specific \mxn model used in our study is summarized in Table~\ref{tab:cifar10}.
%

% For the \svhn dataset, we employ an network-in-network (\nin) model~\cite{lin:2014:iclr}. In contrast of conventional \cnn, \nin builds ``micro neural networks'' within more complicated structures to abstract data within receptive fields, which offers enhanced local modeling capability.
% % The concrete \nin model is summarized in Table~\ref{tab:svhn}.

\paragraph*{Attacks and Defenses.\;} We implement all the attack models in~\myref{sec:attack}, which we refer to as \ttg, \tth, \ttp, and \ttca for brevity. In particular, following~\cite{Goodfellow:2014:arxiv,Huang:2015:arxiv}, we set the limit of distortion amplitude for \ttg and \ttha as 0.25 (i.e., $\delta \leq 0.25$ in~\myref{sec:attack}); for \ttp and \ttca, as in~\cite{Papernot:2016:sp}, we fix this limit to be 112, i.e., the adversary is allowed to perturb no more than 112 pixels.

We implement one representative solution from each defense category in~\myref{sec:defense}. In particular, as data augmentation is attack-specific, we refer to the resulting models as \ttg, \tth, \ttp, and {\ttct} \dnn; as robust optimization is norm-specific, we train robust \dnns under both $l_\infty$- and $l_1$-norm criteria. The implementation details of defense methods are referred to Appendix C.



\subsection{``No Free Lunch''}

Table~\ref{tab:accuracy} summarizes the classification accuracy of the original \dnn models (\cnn, \mxn, \nin) trained over legitimate inputs and their defense-enhanced variants on the benchmark datasets (\mnist, \cifar, \svhn).

%All these architectures have achieved accuracy comparable with the state-of-the-art accuracy in varied image recognition tasks~\cite{classification:result}.


 Observe that the original models achieve accuracy (i.e., 99.5\%, 85.2\%, 95.2\%) close to the state of the art~\cite{classification:result}. In comparison, most of their defense-enhanced variants observe non-trivial accuracy drop. For example, the accuracy decreases by 4.7\% from the original \mxn model to its defensive distilled variant (model transfer), while this drop is as significant as 20.8\% in the case of the \ttgt variant (data augmentation).

We thus conclude that the improvement of attack resilience is not ``free lunch'', often at the expense of classification accuracy. This observation is consistent with the theoretical investigation on the trade-off between \dnn expressivity and robustness~\cite{Fawzi:2015:arxiv}.

\subsection{``No Silver Bullet''}

Next we evaluate different \dnns' attack resilience. Under the limit of distortion amplitude, we measure the percentage of legitimate inputs in each testing set which can be converted to adversarial inputs by varied attacks. Table~\ref{tab:robustness} summarizes the results. The most successful attack under each setting is highlighted.

For the original models, most attacks, especially \ttp and \ttca, achieve near-perfect success rates, implying the prevalence of vulnerabilities across \dnn models.

The data augmentation method significantly improves \dnn resilience against linear attacks. The success rate of \ttga drops below  6\% when facing {\ttgt} \cnn.
However, it is much less effective for more complicated \dnns or against nonlinear attacks. Both \ttp and \ttca achieve near-perfect success rates against data augmented \mxn and \nin. This is because data augmentation is only capable of capturing simple, linear perturbations, while the space of \pvs for nonlinear attacks and complex \dnn models is much larger.

By considering worst-case inputs at every step of training, the robust optimization method leads to stronger resilience against linear attacks. For example, in the cases of \mnist and \svhn, the enhanced \dnn models completely block \ttg and \ttha.
 However, similar to data augmentation, robust optimization is ineffective against nonlinear attacks. This is partially explained by that the adversarial perturbations considered in training are essentially still linear (see Eq.(\ref{eq:rof2})).


Model transfer is the only defense effective against nonlinear attacks. The success rate of \ttpa drops to 1\% against defensive distilled \cnn, which is consistent with the results in~\cite{Papernot:2016:sp}. However, this effectiveness is not universal. %Observe that defensive distilled \dnns are highly vulnerable to \ttga, a simple linear attack. This may be explained as follows: \ttga relies only on gradient signs, thereby being largely immune to the gradient vanishing effects of defensive distillation. Moreover,
\ttca, which is engineered to negate the gradient vanishing effects, is able to penetrate the protection of defensive distillation completely.
%, with success rates approaching 100\% across different benchmark datasets. These results are consistent with the findings by Carlini and Wagner~\cite{Carlini:2016:arXiv,Carlini:2016:arxiv2}.

From the study above, we conclude that none of the existing defense solutions is a ``silver bullet''. While they improve \dnn resilience against specific attacks, the resulting models are unable to adapt to new attack variants. There is thus an imperative need for attack-agnostic defense mechanisms.
%In the following, we present \system, the first one in this category.
