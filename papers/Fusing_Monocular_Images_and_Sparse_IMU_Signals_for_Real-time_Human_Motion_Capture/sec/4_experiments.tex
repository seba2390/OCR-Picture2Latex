
%
\begin{figure}
\includegraphics[width=\linewidth]{pic/qualitative-out.jpg}
  \centering
  \caption{
   Qualitative results on the TotalCapture dataset that the subjects are out of the camera view. Visual signals are not available in the gray regions. From left to right: ROMP/PARE results, TIP results, PIP results, our method without the dual coordinate strategy, and our results.}
  %    \setlength{\abovecaptionskip}{-0.3cm}
  % \setlength{\belowcaptionskip}{-1.0cm}
  \label{fig:comparison2}
\end{figure}
\section{Experiments}
%
In this section, we first provide more details of our implementation and the datasets (Sec.~\ref{sec:implementation}).
%
Then we compare our method with previous works (Sec.~\ref{sec:comparisons}).
%
Next, we evaluate our key components (Sec.~\ref{sec:evaluations}).
Finally, we show more results and live demos in our accompanying video (Sec.~\ref{sec:moreResults}) and discuss our limitations (Sec.~\ref{sec:limitations}).
%
\subsection{Implementation Details}\label{sec:implementation}
All training and evaluation processes run on a computer with an Intel(R) Core(TM) i7-8700 CPU and an NVIDIA GTX2080Ti graphics card.
%
We train each recurrent neural network separately with its own loss (without using previous results from other RNNs) and connect them to form our entire method.
%
Following Transpose \cite{TransPose}, we add a gravity velocity to the translation output.
%
%Live demos and more results are shown in our accompanying video.

\paragraph{Datasets.}
%
We use the AIST++ dataset \cite{AIST++} and the AMASS dataset \cite{AMASS} for training.
%
Our model is trained on AIST++ using detected 2D keypoints and synthesized IMUs, and on AMASS using synthesized 2D keypoints and IMUs.
%
We perform our evaluations on TotalCapture \cite{TotalCapture}, AIST++ test split, 3DPW test split \cite{VIP}, and 3DPW-OCC \cite{VIP, ooh2020}.
%
Since the AIST++, 3DPW, and 3DPW-OCC datasets do not have IMU data, we utilize synthesized IMUs.
%
Please refer to our supplementary for the network architecture and more details about datasets.

\paragraph{Metrics.}
%
We use the following metrics to evaluate our method.
%
1) MPJPE is the mean per joint position error in mm.
%
2) PA-MPJPE is the Procrustes-aligned mean per joint position error in mm.
%
3) PVE is a per-vertex error in mm.
%
4) TE is a translation error that measures the absolute position error in cm.
%
Among these metrics, 1), 2), and 3) measure the pose accuracy, and 4) measures the global translation accuracy.
%
\subsection{Comparisons}\label{sec:comparisons}
\paragraph{Quantitative.}
We perform the comparisons with the state-of-the-art vision-based methods ROMP \cite{ROMP}, PARE \cite{PARE}, IMU-based methods TIP \cite{TIP}, PIP \cite{PIP} and combined method HybridCap \cite{HybridCap}, VIP \cite{VIP}.
%
PARE and VIP are both offline solutions.
%
Notice that PARE does not evaluate the global translation; it only estimates camera parameters that can project the subject onto the image.
%
For HybridCap and VIP, since they do not provide their codes, we compare them in the AIST++, 3DPW, and TotalCapture datasets with the numbers in the paper.
%
For all the previous methods, we use their default settings.
%
\par
%
The quantitative results are shown in Tab. ~\ref{tab:allcmp}.
%
Our method demonstrates significant improvements in pose and translation accuracy compared to previous works.
%
The TotalCapture dataset contains 8 camera views, and there are instances where the subject experiences severe occlusion or is out of the camera view.
%
Pure vision-based method ROMP and PARE fail in these cases.
%
With the help of our dual coordinate strategy and feedback mechanism, we achieve better results than ROMP and PARE.
%
We also reduce the pose and translation errors compared to pure IMU-based methods such as TIP, PIP, and the combined method VIP.
%
Besides, the AIST++ dataset comprises 9 camera views, and the subjects perform numerous challenging dancing motions, all captured within the camera view.
%
We significantly reduce pose and translation errors compared to ROMP, PARE, TIP, and PIP.
%
Despite HybridCap being designed explicitly for challenging poses, we consistently achieve superior results compared to it.
%
Finally, even without training on in-the-wild datasets, we still perform better on 3DPW and 3DPW-OCC, representing in-the-wild and occlusion scenarios.
%
\paragraph{Qualitative.}
%
In Fig. ~\ref{fig:comparison}, we present the qualitative results of our methods compared to ROMP, PARE, TIP, and PIP when the subject is within the camera view.
%
Vision-based method ROMP and PARE can better estimate the positions, but the reconstructed poses of end joints sometimes need to be corrected as these regions are small and thus have limited pixels to guide the vision-based methods.
%
On the other hand, IMU-based methods TIP and PIP are suffering drifts indicated by the slightly worse overlay.
%
%All the previous works need help with accurately estimating challenging poses.
%
Our approach leverages vision and IMU inputs more effectively than previous works through our dual coordinate strategy and hidden state feedback mechanism.
%
As a result, we can achieve better pose and translation estimation in challenging motion, occluded, and in-the-wild scenarios.
%
In Fig. ~\ref{fig:comparison2}, we show the qualitative comparison results when the subject is out of the camera view.
%
Each image's left part (denoted by the gray color) is regarded as out of the camera view in this experiment, and we show this part as a reference to evaluate different methods.
%
ROMP and PARE fail as the visual signal is not available.
%
PIP and TIP estimate plausible poses but suffer the drift problem.
%
Due to the hidden state feedback, our IMU mocap branch is corrected from time to time to reduce the drift, so it estimates accurate pose and much better translation when subjects are out of view.
%
% \begin{table}[t]
% \caption{Quantitative ablation study on the dual coordinate strategy and feedback mechanism. We performed it on the TotalCapture dataset as it contains the sequences with subject out-of-view scenarios.}
% \label{tab:ablitioncmp}
% \resizebox{\columnwidth}{!}{
% \begin{tabular}{c|cc|cc}
% \hline
% Method                  & \multicolumn{1}{c}{MPJPE (mm)}      & \multicolumn{1}{c}{PA-MPJPE (mm)}      & \multicolumn{1}{c}{PVE (mm)}      & TE (cm) \\ \hline
% w/o dual coordinate       & \multicolumn{1}{c}{55.2}            & \multicolumn{1}{c}{37.9}               & \multicolumn{1}{c}{69.9}          & 37.09                 \\ 
% w/o feedback            & \multicolumn{1}{c}{49.5}            & \multicolumn{1}{c}{34.0}               & \multicolumn{1}{c}{64.2}          & 23.82            \\ 
% Ours                    & \multicolumn{1}{c}{\textbf{48.7}}   & \multicolumn{1}{c}{\textbf{33.5}}      & \multicolumn{1}{c}{\textbf{63.4}} & \textbf{23.52}   \\ \hline
% \end{tabular}}
% \end{table}
%
\begin{table}[t]
\caption{Quantitative ablation study on the dual coordinate strategy, hidden state feedback mechanism, and optimization. We performed it on TotalCapture and 3DPW-OCC datasets as these two datasets contain cases where humans are occluded or out of the camera view, which are challenging and best demonstrate our key algorithms.}
\label{tab:ablitioncmp}
\resizebox{\columnwidth}{!}{
\begin{tabular}{c|cccc|ccc}
\hline
\multirow{2}{*}{Method} & \multicolumn{4}{c|}{TotalCapture}                       & \multicolumn{3}{c}{3DPW-OCC}                             \\ \cline{2-8} 
                        & \multicolumn{1}{c}{MPJPE} & \multicolumn{1}{c}{PA-MPJPE} & \multicolumn{1}{c}{PVE}      & TE      & \multicolumn{1}{c}{MPJPE} & \multicolumn{1}{c}{PA-MPJPE} & PVE \\ \hline
w/o dual coordinate     & \multicolumn{1}{c}{55.2}  & \multicolumn{1}{c}{37.9}     & \multicolumn{1}{c}{69.9}     & 37.09   & \multicolumn{1}{c}{81.1}  & \multicolumn{1}{c}{55.9}     & 102.0            \\ 
w/o feedback            & \multicolumn{1}{c}{49.5}  & \multicolumn{1}{c}{34.0}     & \multicolumn{1}{c}{64.2}     & 23.82   & \multicolumn{1}{c}{78.1}  & \multicolumn{1}{c}{53.4}     & 98.0   \\
w/o optimization   & \multicolumn{1}{c}{48.8} & \multicolumn{1}{c}{33.6} & \multicolumn{1}{c}{\textbf{63.1}} & 25.28 & \multicolumn{1}{c}{\textbf{75.7}}  & \multicolumn{1}{c}{\textbf{52.2}}     & \textbf{97.1}   \\
Ours            & \multicolumn{1}{c}{\textbf{48.7}}& \multicolumn{1}{c}{\textbf{33.5}}  & \multicolumn{1}{c}{{63.4}}& \textbf{23.52}   & \multicolumn{1}{c}{77.9}& \multicolumn{1}{c}{53.1}  & 97.5   \\ \hline
\end{tabular}}
\end{table}
%
\begin{figure}
  \includegraphics[width=\linewidth]{pic/evaluation.jpg}
  \centering
  \caption{
  Qualitative ablation study on the hidden state feedback mechanism. Visual signals are not available in the gray figure.}
  \label{fig:evaluation}
\end{figure}
%
\subsection{Evaluations}\label{sec:evaluations}
We evaluate three critical components of our method: the dual coordinate strategy, the hidden state feedback and the optimization.
\paragraph{Dual Coordinate Strategy.}
%
We compare our solution with a setting where all the estimations are in the camera coordinate.
%
Quantitative results are shown in Tab. ~\ref{tab:ablitioncmp}.
%
We can clearly see that the dual coordinate strategy makes significant performance improvement.
%
Qualitative results are also shown in Fig. ~\ref{fig:comparison2}.
%
Without our dual coordinate strategy, in the camera coordinate system, the method struggles to estimate the translation accurately, and thus it becomes more challenging to estimate the pose from the IMUs.  
%
\paragraph{Hidden State Feedback.}
%
To examine the effect of the feedback mechanism, we train two alternatives that do not update the hidden states of RNN-T3 and RNN-P1, respectively.
%
As discussed in the Method section, the feedback for RNN-T3 is most effective in determining translations when the subject is moving in again after moving out of the camera view.
%
Since TotalCapture and 3DPW-OCC contain this kind of motion, the translation error is increasing on TotalCapture and 3DPW-OCC in Tab. ~\ref{tab:ablitioncmp} for the former alternative.
%
To better demonstrate this, we pick a walking sequence in the TotalCapture dataset, and the result is shown in the left side of Fig. ~\ref{fig:evaluation}, where the method without feedback fails when the subject moves in the camera view at a different place from where the subject moved out.
%
The error is because the networks learn to avoid sudden translation changes.
%
So, without the feedback mechanism, the vision branch remembers where the subject moves out of the camera view. When the branch is activated again (the subject comes in again), it leans to estimate the translation near the remembered place.
%
However, with the feedback, the synthesized input $\boldsymbol{p}_{\mathrm{2d}}$ can update the hidden state of RNN-T3. On the condition that the IMU mocap works well when the subject is out of the camera view, RNN-T3 can be correctly restarted when the subject comes in again at a new place. 
%
\par
%
The feedback for RNN-P1 is most helpful in reconstructing the ambiguous poses whose IMU measurements are identical.
%
We additionally select a sequence from the TotalCapture dataset, and the corresponding results are presented on the right side of Fig. ~\ref{fig:evaluation}.
%
The gray-colored region in this figure is regarded as out of the camera view in this experiment (the subject is in the camera view at the beginning), and we show this region as a reference for better evaluation.
%
In the beginning, the subject is visible to the camera, so the combined branch updates the hidden states of the IMU branch, thus making it known that the subject is bending rather than sitting.
%
Then when the subject moved out of view, the method with the feedback still reconstructs a bending pose while the one without feedback solves a sitting pose as these two poses share similar IMU signals and sitting data is much more than the bending data in the training dataset.
%
%Due to the feedback from RNN-P1, our complete method demonstrates better capability in estimating ambiguous poses compared to our method without feedback.
%
\paragraph{Optimization.}
%
As shown in Tab. ~\ref{tab:ablitioncmp}, the final 1-iteration optimization primarily aims to improve the 2D overlay, and it has a slight effect on the pose result while reducing the translation error.
%
\hl{In the case of the 3DPW-OCC dataset, the pose error is larger when using optimization due to occlusion affecting the 2D detector.}
%
\subsection{More Results}\label{sec:moreResults}
We test our method on various motions, including moving out and re-entry the camera view, extreme lighting, severe occlusion (e.g., clothes, umbrellas, and boards), and challenging poses (e.g., rapid jumps and large body movements). These results are shown in our accompanying video.

\subsection{Limitations}\label{sec:limitations}
In our method, two branches can help each other when at least one of the branches works well.
%
So, it is easy to imagine that if the two branches both face their inherent difficulties, our system will also suffer.
%   
For example, when the subject moves out of the camera view for a long time, the visual branch has no input, and the drift of the IMU branch becomes large.
%
In such circumstances, IMUs are susceptible to error accumulation caused by magnetic disturbances.
%
We could consider modeling magnetic disturbance from combined modalities in the future.
%Our final result also has a large drift with no information to reduce it.
%
Our method neglects the shape of different people and uses a mean shape for common adults.
%
Since we have visual signals, we could consider body shape in the future to get a more complete reconstruction.
%