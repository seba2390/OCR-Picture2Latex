\section{Introduction}
%
Human motion capture (mocap) is an important research topic in computer vision and graphics and has many applications in virtual reality, medical care, games, and animation.
%
Traditional multi-view techniques achieve high accuracy and robustness, but they are heavy and expensive and thus difficult to be used by end users.
%
Recently, with the development of mobile sensing techniques, RGB cameras and Inertial Measurement Units (IMUs) have been integrated into daily devices like phones, smartwatches, and eyeglasses. 
%
They can be easily accessed, and mocap with these sensors will have much broader applications.
%
%At present, the two main categories of motion capture methods are vision-based \cite{VIBE,ROMP,spin,PARE,PhysAware,hmr,minimal,Baseline,CrossVF,LearnableTO} and sensor-based \cite{SIP,DIP,TransPose,PIP,TIP}.
%
%Inertial Measurement Unit(IMU), which can measure human motion inertia, is a popular sensor used in motion capture.
%
%The RGB camera and the IMUs are heavily used in phones, smartwatches, and smart glasses.
%
%Therefore, It is possible for us to use these common digital devices to capture human motion because of the convenience of the vision-based methods and the IMU-based methods.
%
\par
%Using a monocular camera or sparse IMUs are two lightweight settings to achieve mocap. 
Human motion can be captured from a monocular camera or, alternatively, a sparse set of IMUs.
%
%Among the vision-based methods and IMU-based methods, A large number of motion capture methods are based on visual signals.
%
The former~\cite{ROMP,spin,PARE,VIBE} has become increasingly effective with the development of deep learning techniques.
%
However, they still fail in extreme lighting conditions, severe occlusions, or humans moving out of the camera view.
%
%Though some methods \cite{PARE,ROMP} try to improve the robustness by solving the occlusion problem, they still fail in cases of severe occlusion or people moving out of the camera.
%
%Compared to vision-based methods, IMU-based methods can get rid of these limitations. 
%
While IMU-based methods~\cite{DIP,TransPose,PIP,TIP} can get rid of these limitations, they cannot accurately estimate human translation due to the substantial drifts caused by sensor error accumulation.
%\cite{DIP,TransPose,PIP,TIP} use sparse IMUs to estimate human poses and translations in real time.
%
%However, these techniques cannot accurately estimate human translation due to the substantial drifts caused by sensor error accumulation. 
%
%So, in general, mocap techniques with lightweight settings are still facing difficulties.
%
\par
We propose to fuse monocular images with sparse IMUs to achieve real-time mocap with high accuracy and robustness for estimating both body pose and translation.
%
Our method fuses visual and inertial information to estimate better human motion when the performer is in the camera view.
%
While for some extreme cases where the performer is severely occluded, poorly lighted, or moving out of the camera view, our technique majorly uses IMUs to track the human motion.
%
Compared with the pure visual solutions, our technique outputs plausible mocap results robustly in the aforementioned extreme cases. %, as the inertial signal is still available.  
%
On the other hand, when the performer is visible to the camera, the visual information is used as an online calibration to solve the drifting problem caused by cumulative errors in the inertial signals.
%
So the inertial and visual information leverage each other in our system to achieve a real-time lightweight mocap with high accuracy and robustness.
%
%This out-of-camera consideration makes our method can get rid of the limited range of camera view.
%
%In contrast to previous IMU-based methods \cite{TransPose, TIP, PIP}, our method uses vision fusing IMUs which can get better pose and translation by avoiding drifts and IMU measurements error.
%
%Even out of camera view, we can make it get equal results with the IMU-based methods.
%
\par
% Our technique is based on a dual coordinate strategy. 
% It transforms the IMU signals to the camera coordinate system when combined with visual signals but represents them in the human root coordinate system when using them only.
% We observe that one key feature of visual signals is that they record the absolute position information of the human body, including the body joints. 
% To fully explore this information, directly transforming the IMU signals to the camera coordinate system, which is an absolute coordinate system, is efficient and effective to combine with the visual information.
% However, when the visual signals are not confident, and IMUs only are used to perform mocap, we observe that it is no longer a good choice to still use the camera coordinate system. 
% The reason is that inertial drifting majorly blocks the global motion estimation, but as the human global and local motions are coupled in the camera coordinate system, the drifting will also pollute the local motion estimation.
% On the other hand, learning from the IMU signals in the body root coordinate system protects the local pose estimation from drifting. 
% As a consequence, the delicate design of the dual coordinate strategy explores the inertial signals as much as possible in different cases.

Our technique is based on a dual coordinate strategy, which makes the model training adaptive to the two input modalities.
%
Specifically, the IMU signals are transformed into the camera coordinate system when integrated with visual signals but are processed in the human root coordinate system when used solely.
%
This design fully utilizes the characteristics of the two input modalities: visual signals capture both global position and local pose information of humans, while inertial measurements majorly track the human's local movements with global motion drift caused by inertial error accumulation. 
%
When the visual signal is available and confident, we align the IMU signal to the camera coordinate system as a complement, in which the human's absolute pose and motion dynamics are both well represented, and thus learning in this coordinate system makes full use of the visual signal.
%
%As a result, the complementary signal information is effectively fused to achieve drift-free human motion capture.
%
Differently, when the inertial measurements are used solely (visual signals not available nor confident), the camera coordinate system becomes a suboptimal choice as the global part of the input modalities (\textit{i.e.}, visual signals) does not exist.
%
We thus seek a change from a global "third-person perspective" (\textit{i.e.}, camera coordinate system) to a local "first-person perspective" (\textit{i.e.}, root coordinate system) to capture the human motion.
%
The key is that human motions may be seen differently from the camera's view but are exactly the same from a local view, \textit{e.g.,} walking straight in different directions.
%
To this end, the problem is simplified, and we achieve better results in the root coordinated system for local pose estimation.
%
In summary, the delicate design of the dual coordinate strategy for the two branches explores the inertial signals as much as possible in different use cases.

%
\par
%
Within the dual coordinate strategy, hidden state feedback is further proposed to enable information exchange between the two coordinate systems.
%
In either coordinate system, human motion is estimated independently based on the historical input signals through deep temporal modeling.
%
As a result, the internal state in the coordinate system may deviate from the optimal state during motion tracking. 
%
For example, due to error accumulation, human global motion cannot be faithfully estimated
in the IMU-only branch.
%become unreliable due to severe occlusions, the tracking under the camera coordinate system will become inaccurate (\textit{i.e.,} bad visual signals pollute the internal state).
%
On the other hand, the global motion information is directly measured by the visual signals in the other branch.
%, the tracking suffers error accumulation in the root coordinate system.
%
By our hidden state feedback technique, this error accumulation problem is solved as the hidden state in the IMU-only branch is calibrated by the other branch.
%
Specifically, instead of frequently updating the states or assuming a fixed time interval, our algorithm dynamically detects the need for hidden state updates and performs the strategy only when required.
%
This guarantees the high efficiency of our algorithm.
%
Furthermore, instead of calibrating one branch using the other, we use the final fused results, which is better than either of the two branches.
%
This effectively forms a feedback loop where the final combined result is used for hidden state updates in the two coordinate systems.
%
In summary, the hidden state feedback mechanism effectively and efficiently achieves information interchange between dual coordinate systems.


%\hl{\textbf{[two "In summary" may be deleted for conciseness]}}

%The key idea is that since the two kinds of signals are complementary, their combination should outperform either of them, and the combined results should not be the results only but also used to improve the mocap itself. 
%
%The hidden state feedback mechanism achieves this by using the combined results to update the hidden state of some RNNs to improve the performance of the corresponding individual components of our system, which will also contribute to improving the final quality of our results.





%Our system contains three main steps.
%
%Since the inertial signal is always available while the visual signal is not, we first propose two mocap branches: an IMU branch and an IMU-aided vision branch.
%
%Then, we propose a confidence-based fusion method to smoothly combine the outputs of the two branches to get our final result.  
%
%In the IMU branch, we use 5 IMUs to estimate the root-relative joint positions and velocity by recurrent neural networks following \cite{TransPose}, while in the IMU-aided vision branch, we combine the keypoints detected by \cite{minimal} with the IMUs to estimate the root-relative joint positions again as well as the absolute translation.
%
%In the two branches, we propose a learning-based transformation technique that uses a coordinate transformation to better disentangle global human motions and local human poses with neural networks.
%
%In the final confidence-based fusion scheme, we first get confidence values from the 2D human keypoint detector in \cite{minimal}.
%
%With confidence, we fuse the results of the two branches.
%
%To perform effective feedback, we further use the final fused result to update the two branches.
%
%For the IMU branch, the feedback can help to better distinguish ambiguous poses. For the vision branch, the feedback can give reasonable translation initialization when the user comes back to the camera view.

%
%This is achieved by a branch feedback mechanism between the two branches.
%
%This results in a significant accuracy improvement, especially for ambiguous motions. 
% such as stretching.
%
%It also estimates better translation in the condition when people move out of the camera and move in again at two different places.
%

In this paper, our main contributions are:
\begin{itemize}
    \item An accurate and robust approach that fuses monocular images with sparse IMU signals for real-time human motion capture.
    \item A dual coordinate strategy that makes the neural network better learn from the inertial signals in different cases.
    \item A hidden state feedback mechanism that leverages the combined results in the loop by using them to improve the performance of individual components.
\end{itemize}
%
% Our experiments demonstrate that our method significantly outperforms previous IMU-based and vision-based methods in conditions of people are in or out of camera view.
%