\section{Related Work}
% We combine two input modalities to perform motion capture, so 
% We categorize the related works by the input modality: the vision-based, inertial sensor-based, and combined methods.
%three categories of works: methods based on vision, methods based on pure inertial sensors, and methods fusing vision and inertial information. 
%

\subsection{Vision-based Mocap Methods}
In vision-based mocap, the approaches using multi-view input have achieved remarkable results \cite{harvesting17, CrossVF, LearnableTO, remelli2020lightweight, chun2023learnable, reddy2021tessetrack}, but the system is heavy, and the recording space is limited. 
%
Some works use a single monocular camera, and they represent 3D human poses by skeletons \cite{pavllo20193d,sharma2019monocular,xu2020deep,zhen2020smap}, i.e., the 3D positions of all the body joints, which cannot represent the body shape and the exact 3D motion between two recorded image frames.
%
So some other works use human body parametric models, such as SMPL \cite{SMPL}, and thus the shape and pose parameters of the models are used to represent the body shape and the rotations of the body joints.
%
% omit the "and STAR \cite{STAR}"
%
To obtain the parameters, the optimization-based methods \cite{Lassner, Bogo:ECCV:2016,xiang2019monocular,pavlakos2018learning} fit the parametric body models to 2D observations, while the regression-based methods \cite{kolotouros2019convolutional, hmr, kanazawa2019learning, VIBE, zanfir2021neural, minimal, zhang2022mixste, PhysAware, li2022cliff} train neural networks to estimate the model parameters directly from the input or some low-level features. 
%
% SPIN \cite{spin} proposes a method to combine these two approaches, i.e., using a neural network to initialize an optimization routine and supervising the neural network with the optimization results recurrently.
%
% SPIN \cite{spin} proposes to combine these two approaches by helping each other initialize or supervise.
%
In general, vision-based methods are severely affected by the invisibility issue caused by challenging lighting or occlusions.
%
Recent works, such as ROMP\cite{ROMP}, PARE\cite{PARE} 
have been devoted to better handling this problem, using a collision-aware representation or part-guided attention mechanism.
%
Other works \cite{yuan2022glamr,BEV,huang2022occluded} also focus on solving occlusions, and they design unique mechanisms, such as a deep generative motion infiller or a hypothetical bird's eye view.
%
% ROMP \cite{ROMP} designs a collision-aware representation to overcome the severe overlap between multiple people. 
% %
% PARE \cite{PARE} uses a part-guided attention mechanism to predict occluded body parts from their neighboring parts.
% %
% GLAMR \cite{yuan2022glamr} proposes a deep generative motion infiller to cope with occlusions.
% %
% BEV \cite{BEV} uses a hypothetical bird's eye view to infer depth and resolve occlusions.
% %
% CHOMP \cite{huang2022occluded} proposes to learn joint-level spatial-temporal motion prior to non-occluded data.
%
Although these methods have handled occlusions to a certain extent, they still have difficulties handling extreme occlusions or cases of people out of the camera view.
%
\subsection{IMU-based Mocap Methods}
Unlike vision-based methods, inertial sensor-based methods are unaffected by challenging lighting, occlusions, and camera view limitations.  
%
Commercial inertial mocap solutions such as Xsens \cite{schepers2018xsens} have achieved high accuracy but require dense sensors (17 IMUs), making them inconvenient and expensive.  
%
Recently, mocap with sparse sensors has drawn much more attention.
%, even though reducing the sensor number leads to serious ambiguity.  
%
%Early works were limited by the sensor hardware technology at the time and could only measure acceleration \cite{riaz2015motion,tautges2011motion,slyper2008action} or orientation\cite{schwarz2009discriminative} alone.
%
%These acceleration-based methods \cite{riaz2015motion,tautges2011motion,slyper2008action} achieved sparse accelerators motion capture by integrating with database search methods.
%
%But their performance is limited by database size and the instability of the accelerometers.
% 
%Thanks to the development of IMU sensor technology, later work could obtain both acceleration and orientation.
%
SIP \cite{SIP} successfully reduces the number of sensors to 6, but is limited by the speed of the optimization-based approach and thus could not support real-time motion capture.
%
\cite{DIP, RNN-Ensemble} use recurrent neural networks to achieve real-time pose estimation with sparse IMUs for the first time, but they cannot estimate the global translation of people.  
%
TransPose \cite{TransPose} combines supporting-foot deduction and network prediction, and achieves real-time pose and global translation estimation with only 6 IMUs.  
%
\cite{schreiner2021global} focuses on the real-time estimation of precise position based on pose and orientation data.
%
Later, PIP \cite{PIP} introduces physical constraints \cite{physCap} to achieve higher accuracy, and a novel RNN initialization method to resolve the action ambiguity of long time sitting or standing still.
%
Meanwhile, TIP \cite{TIP} proposes stationary body points to mitigate the effects of drift.  
%
Additionally, \cite{jiang2022avatarposer,winkler2022questsim,ye2022neural3points,aliakbarian2022flag} use VR devices on the user's head and hands to estimate full-body poses.
%
However, due to the pose ambiguity of the sparse IMU setting and the sensor error accumulation, these methods suffer from jittery or drift in motion estimation.
%
%\vspace{-0.05cm}
\subsection{Vision-inertia Fusion Methods}
%
Depending on the type of visual input, the vision-inertia fusion methods can be divided into three categories: fusing multi-view video with IMUs \cite{pons2010multisensor,vonmarcardponsmollPAMI16,TotalCapture,malleson2020real,RealTimeFM,gilbert2019fusing,FusingWI,moniruzzaman2021wearable,bao2022fusepose,huang2020deepfuse}, fusing monocular RGBD video with IMUs \cite{helten2013real,zheng2018hybridfusion}, and fusing monocular RGB video with IMUs \cite{VIP,HybridCap,kaichi2020resolving,henschel2020accurate,henschel2019simultaneous}.
%
%The key point of these methods is how vision-inertial fusion is performed.
%
In the former two categories, some methods use optimization techniques by defining and minimizing energy functions that are correlated with both visual and IMU features \cite{vonmarcardponsmollPAMI16, RealTimeFM,pons2010multisensor,malleson2020real}.  
%
Other methods use dual-stream networks to estimate the pose from IMU and vision separately and then combine them \cite{TotalCapture,gilbert2019fusing}.  
%
However, the combination sometimes is not sufficient and may involve the errors of the two results together.
%
To solve this problem, \cite{FusingWI} proposes to fuse information at an earlier stage.  
%
% To solve this problem, \cite{FusingWI} designs the Orientation Regularized Network (ORN) and fuses the information at an earlier stage.  
%
FusePose \cite{bao2022fusepose} combines the IMU and visual information in the training stage so that they can adaptively compensate for each other.
% To solve this problem, \cite{FusingWI} proposes fusing information earlier, and FusePose \cite{bao2022fusepose} combines them in the training stage.
%
% To solve this problem, \cite{FusingWI} designs the Orientation Regularized Network (ORN) and fuses the information at an earlier stage.  
%
% FusePose \cite{bao2022fusepose} combines the IMU and visual information in the training stage so that they can adaptively compensate for each other.
%
For the methods combining monocular RGB with IMUs, the task is more challenging as monocular RGB contains less information than RGBD or multi-view RGB.
%
\cite{henschel2020accurate,henschel2019simultaneous} only estimate the global root translation.
%
\hl{Both \cite{kaichi2020resolving} and \cite{VIP} are involved in estimating body poses, but while \cite{kaichi2020resolving} necessitates dense IMUs, \cite{VIP} functions as an offline technique.}
%
Recently, one concurrent work HybridCap \cite{HybridCap} combines 4 IMUs and the 2D keypoint positions in the image domain to estimate the pose and translation.
%
They consider the situation that the two kinds of inputs are both available and they fail when the visual information is unavailable, as it is difficult for 4 IMUs to perform mocap.
%
We consider the visual and IMU fusion task in a different manner that we do not want the fused system to suffer the inherent limitation of visual motion capture, i.e., we handle the situation that the visual input is bad (challenging lighting, severe occlusion, or human out of viewing field).
%We explore this problem with different domains with a set of different techniques and with more consideration of the complex motion out of the camera view while producing comparable or better reconstruction results.
%
By this design, we achieve the first real-time system that combines monocular RGB and sparse IMUs to achieve robust and accurate motion capture even when the visual signal is unavailable or severely polluted.
%Although these methods have achieved impressive results in overcoming occlusion, however, the combination of a monocular camera and IMUs has not been able to estimate both pose and global translation in real-time while overcoming severe occlusion and the complex motion out of the camera view.
%