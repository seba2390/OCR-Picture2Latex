\begin{figure}
  \includegraphics[width=\linewidth]{pic/method.jpg}
  %\vspace{-0.7cm}
  % \setlength{\abovecaptionskip}{-0.1cm}
  % \setlength{\belowcaptionskip}{-0.4cm}
  \caption{Overview of our method. The inputs are inertial measurements of 6 IMUs and image domain 2D keypoints obtained by an off-the-shelf 2D human pose estimator. The outputs are real-time human pose and translation. We adopt a dual coordinate strategy, where we estimate joint positions and global movements in both the human's root and camera coordinate systems and we fuse the results from two coordinate systems to get the human pose and translation. To enable the information interchange between the two coordinate systems and make them mutually help each other, we leverage a hidden state feedback mechanism to update the hidden state of the two branches using the final fused results. %With the feedback loop, our method estimates human motion robustly and consistently in challenging cases such as server human occlusion or extreme lighting conditions.
  }
  \label{fig:pipeline}
\end{figure}
\section{Method}
%
%\subsection{Overview}\label{Overview}
As shown in Fig. \ref{fig:pipeline}, the inputs of our method consist of sequential 2D joint detections of a moving subject from monocular images, as well as the accelerations and orientations of 6 IMUs mounted on the left and right forearms, left and right lower legs, head, and pelvis of the subject.
%
The output of our system is the 3D motion of the subject, represented on the kinematic model of SMPL \cite{SMPL}.
%
Note that a calibration step is performed to calibrate the relative orientation between the IMUs' coordinate system and the camera coordinate system, detailed in the supplementary materials.
%
Given this calibration, we can align the two input modalities by transforming either one to the coordinate system of the other.
%
In the following, we introduce our dual coordinate strategy which estimates both the global motion and local pose of the performer (Sec.~\ref{subsubsec:dual-coordinate}) as well as the hidden state feedback to make the estimation more robust and accurate (Sec.~\ref{sec:feedback}).
%
\subsection{Dual Coordinate Strategy}\label{subsubsec:dual-coordinate}
%
Deep learning has shown its great power in vision and graphic tasks.
%
How to represent its input and output significantly influence its performance in real applications.
%
In our deep learning-based technique, we propose to use two coordinate systems to represent our input in different situations and fuse them together to pursue a better estimation.
%
Our key observation is that the visual signals contain both the global and local motion of the performer in the camera coordinate system while the inertial signals majorly contain the local body movement information but the global motion information always drifts due to the inertial error accumulation.
So when the two modalities are both available, the camera coordinate system is more adequate to represent the input.
%
On the other hand, when the visual signals are not available (due to occlusions, extreme lighting, or out-of-camera view) and only the inertial signals are available, the root coordinate system is adopted because the same poses may differ significantly from different camera views but are precisely the same from a local view.
%
Moreover, the IMU-estimated root velocity in the root coordinate system is important to estimate the absolute position of the tracking subject when visual clues are unavailable.
%As described in Sec. ~\ref{subsubsec:Pose} and Sec. ~\ref{subsubsec:Tran}, we show the design of our dual coordinate strategy, which is adaptive to the input modality.
%
%Please refer to our supplementary materials for a detailed description of transforming inertial measurements into different coordinate systems.
%
%As mentioned earlier, we seek a transformation from a global "third-person perspective" (\textit{i.e.}, camera coordinate system) to a local "first-person perspective" (\textit{i.e.}, root coordinate system) to estimate the pose and translation accurately.
%
%The motivation is that different poses may differ significantly from the camera view but are precisely the same from a local view.
%
%Moreover, without the IMU estimation for the velocity in the root coordinate system, it is hard to estimate the absolute position $\boldsymbol{t}_\mathrm{c}^\mathrm{e}$ of the tracking subject when the visual clues are polluted.
%
%Furthermore, by designing the IMU estimation for the velocity, we can smoothly adapt to the velocity estimation in the root coordinate from the absolute position.
As we have disentangled the local pose and global translation estimation, we introduce our dual coordinate strategy individually for the two tasks.%We divide our system into two parts: pose estimation (Sec. ~\ref{subsubsec:Pose}) and global translation estimation (Sec. ~\ref{subsubsec:Tran}).
%
%Then we discuss the design of our dual coordinate strategy (Sec. ~\ref{subsec:dual-coordinate}).
%
\subsubsection{Local Pose Estimation}\label{subsubsec:Pose}
We first describe local pose estimation.
%
To thoroughly learn the human motion prior, our algorithm first estimates joint 3D positions and then estimates joint rotations (i.e., local pose).
%
The joint position estimation task is solved under dual coordinates adaptive for different use cases (demonstrated by RNN-P1 for the root coordinate and RNN-P2 for the camera coordinate in Fig.~\ref{fig:pipeline}).
%
Then, we use linear interpolation to fuse the two results from the two coordinates and estimate the joint rotations under the root frame by inverse kinematics (demonstrated by RNN-P3).
%原来的这个感觉不是很有概括性
% Specifically, as shown in Fig. ~\ref{fig:pipeline}, we use RNN-P1 to estimate the root-relative joint positions in the root coordinate system from the IMU input.
% %
% Besides, we utilize RNN-P2 to estimate the root-relative joint positions in the camera coordinate system and subsequently transform them into the root coordinate system.
% %
% After a linear interpolation to fuse the two joint positions, we leverage RNN-P3 to perform IK, i.e., estimating joint rotations based on the joint positions.
%
\par
Specifically, when estimating joint positions in the root coordinate system, we transform the inertial measurements into the root coordinate by the IMU mounted on the pelvis, obtaining a concatenated input vector as the input of RNN-P1.
%
The input vector is denoted as $\boldsymbol{x}_{\mathrm{r}}=\left[\boldsymbol{a}_{\mathrm{larm}},\cdot\cdot\cdot,\boldsymbol{a}_{\mathrm{root}},\boldsymbol{R}_{\mathrm{larm}},\cdot\cdot\cdot,\boldsymbol{R}_{\mathrm{root}}\right]\in\mathbb{R}^{(3+9)n}$ where $\boldsymbol{a}\in\mathbb{R}^3$ represents the acceleration, $\boldsymbol{R}\in\mathbb{R}^{3\times3}$ represents the rotation, the subscript $\mathrm{r}$ denotes the root coordinate system, and $n=6$ represents the number of IMUs.
%
RNN-P1 outputs the root-relative coordinates of all joints as $\boldsymbol{p}_{\mathrm{r}}^{\mathrm{e}}\in\mathbb{R}^{3J}$, where $J$ represents the number of joints, and the superscript $\mathrm{e}$ indicates the estimation.
%
The loss function used to train  RNN-P1 is defined as:
\begin{equation}
    \mathcal{L}_{P1}=\Vert \boldsymbol{p}_{\mathrm{r}}^{\mathrm{e}}-\boldsymbol{p}_\mathrm{r}^{\mathrm{GT}} \Vert_2^{2},
\end{equation}
where superscript $^{\mathrm{GT}}$ indicates the ground truth.
%
\par
When estimating joint positions in the camera coordinate system, we first transform the inertial measurements into the camera coordinate system.
%
We then reproject the 2D keypoints, detected by MediaPipe \cite{mediapipe}, onto the $Z$ = 1 plane using camera intrinsics to generalize to various camera settings.
%
After this, we get $\boldsymbol{p}_{\mathrm{2d}}\in\mathbb{R}^{2J^{'}}$ where $J'$ represents the number of detected keypoints from MediaPipe.
%
Next, we perform root normalization to represent the keypoints as root-relative 2D key points and the absolute position of the root in the camera coordinate system.
%
After the root normalization, we obtain $\boldsymbol{p}_{\mathrm{2d}}^{'}$.
%
The input of RNN-P2 can be represented as $[\boldsymbol{x}_{\mathrm{c}},\boldsymbol{p}_{\mathrm{2d}}^{'},\boldsymbol{\sigma}]\in\mathbb{R}^{(3+9)n+3J^{'}}$, where $\boldsymbol{x}_{\mathrm{c}}\in\mathbb{R}^{(3+9)n}$ denotes the IMU measurements in the camera coordinate system, and $\boldsymbol{\sigma}\in\mathbb{R}^{J^{'}}$ represents the keypoints' corresponding confidence scores given by MediaPipe.
%
The output of RNN-P2 is root-relative joint positions in the camera coordinate system, denoted as $\boldsymbol{p}_{\mathrm{c}}^{\mathrm{e}}\in\mathbb{R}^{3J}$.
%
The loss function for RNN-P2 is the same as the loss function for RNN-P1.
%
\par
To fuse the results in the two coordinate system, we first transform $\boldsymbol{p}_{\mathrm{c}}^{\mathrm{e}}\in\mathbb{R}^{3J}$ into the root coordinate system.
%
Then, we can fuse them to get the final root-relative coordinates of all joints denoted as $\boldsymbol{p}_{\mathrm{r}}$.
%
We use the average visual confidence of all keypoints $\sigma_{\mathrm{mean}}$ to determine the fusing process.
%
Specifically, if the average confidence $\sigma_{\mathrm{mean}}$ is sufficiently high (i.e., visual information is of good quality), we take the results from the camera coordinate frame;
%
if $\sigma_{\mathrm{mean}}$ is significantly low (i.e., visual information is unreliable or absent), we take the joints estimated from solely IMUs.
%
Otherwise, a linear interpolation is performed between the two results to smooth the process.
%
We experimentally set the lower bound and upper bound to 0.7 and 0.8, respectively.
%
Finally, RNN-P3 takes the concatenated vector $[\boldsymbol{p}_{\mathrm{r}},\boldsymbol{x}_{\mathrm{r}}]$ as input to estimate the joint rotations in 6D representation \cite{zhou2019continuity}, denoted as $\boldsymbol{\varphi}\in\mathbb{R}^{6J}$.
%
We can convert $\boldsymbol{\varphi}$ to axis-angle representation and obtain $\boldsymbol{\theta}_\mathrm{r}$.
%
The loss of RNN-P3 is defined as:
\begin{equation}
\mathcal{L}_{P3}=\lambda_{\mathrm{rot}}\mathcal{L}_{\mathrm{rot}}+\lambda_{\mathrm{pos}}\mathcal{L}_{\mathrm{pos}}.
\end{equation}
%
$\mathcal{L}_{\mathrm{rot}}$, which constrains the estimated joint rotations, is defined as:
\begin{equation}  
\mathcal{L}_{\mathrm{rot}}=\Vert \boldsymbol{\varphi}-\boldsymbol{\varphi}^{\mathrm{GT}} \Vert_2^{2}.
\end{equation}
%
$\mathcal{L}_{pos}$, which constrains the joint positions after forward kinematics, is defined as: 
\begin{equation}  
\mathcal{L}_{pos}=\Vert \mathrm{FK}\left(\boldsymbol{\theta}_{\mathrm{r}}\right)-\mathrm{FK}\left(\boldsymbol{\theta}_\mathrm{r}^{\mathrm{GT}}\right) \Vert_2^{2},
\end{equation}
%
where $\mathrm{FK}(\cdot)$ is the forward kinematics function that calculates the position of the joints.
%
We experimentally set the parameters $\lambda_{\mathrm{rot}}=1$ and $\lambda_{\mathrm{pos}}=100$.
%
\hl{By multiplying the orientation measurement of the root IMU with the calibrated camera extrinsic, joint rotations $\boldsymbol{\theta}\mathrm{r}$ can be transformed into the camera coordinate system, resulting in $\boldsymbol{\theta}\mathrm{c}$.}
%
\begin{table*}[t]
\caption{Quantitative comparisons with state-of-the-art methods ROMP, PARE, TIP, PIP, HybridCap, and VIP. We show results on 3DPW (in the wild pose), 3DPW-OCC (in the wild occluded pose), AIST++ (challenging motion), and TotalCapture (with subject out-of-view scenarios) datasets.}\label{tab:allcmp}
\resizebox{\textwidth}{!}{
\begin{tabular}{c|ccc|ccc|cccc|cccc}
\hline
\multicolumn{1}{c|}{\multirow{2}{*}{Method}} & \multicolumn{3}{c|}{3DPW}                                                                  & \multicolumn{3}{c|}{3DPW-OCC}  
                                             & \multicolumn{4}{c|}{AIST++}                                                                 & \multicolumn{4}{c}{TotalCapture}                                    \\ 
                                             \cline{2-15} 
\multicolumn{1}{l|}{}                        & \multicolumn{1}{c}{MPJPE} & \multicolumn{1}{c}{PA-MPJPE} & PVE              & \multicolumn{1}{c}{MPJPE} & \multicolumn{1}{c}{PA-MPJPE} & PVE
                                             & \multicolumn{1}{c}{MPJPE} & \multicolumn{1}{c}{PA-MPJPE} & \multicolumn{1}{c}{PVE} &TE & \multicolumn{1}{c}{MPJPE} & \multicolumn{1}{c}{PA-MPJPE} & \multicolumn{1}{c}{PVE} &TE \\ 
\hline
ROMP                                         & \multicolumn{1}{c}{91.3}       & \multicolumn{1}{c}{54.9}    & \multicolumn{1}{c|}{108.3}  & \multicolumn{1}{c}{-}          & \multicolumn{1}{c}{-}             & \multicolumn{1}{c|}{-}   
                                             & \multicolumn{1}{c}{90.3}       & \multicolumn{1}{c}{60.0}          & \multicolumn{1}{c}{128.1}    &16.91                    & \multicolumn{1}{c}{145.4}      & \multicolumn{1}{c}{64.6}          & \multicolumn{1}{c}{184.5}   &60.31 \\ 
PARE                                         & \multicolumn{1}{c}{82.0}       & \multicolumn{1}{c}{50.9}    & \multicolumn{1}{c|}{97.9}   & \multicolumn{1}{c}{90.5}       & \multicolumn{1}{c}{56.6}          & \multicolumn{1}{c|}{107.9}   
                                             & \multicolumn{1}{c}{83.7}      & \multicolumn{1}{c}{50.9}     & \multicolumn{1}{c}{116.5}   & -                   & \multicolumn{1}{c}{143.5}     & \multicolumn{1}{c}{60.9}           & \multicolumn{1}{c}{196.8}   & -       \\ 
TIP                                          & \multicolumn{1}{c}{82.5}       & \multicolumn{1}{c}{58.2}    & \multicolumn{1}{c|}{109.9}  & \multicolumn{1}{c}{100.5}      & \multicolumn{1}{c}{68.7}          & \multicolumn{1}{c|}{131.6}  
                                             & \multicolumn{1}{c}{85.1}      & \multicolumn{1}{c}{62.1}     & \multicolumn{1}{c}{115.5}         &69.52                     & \multicolumn{1}{c}{69.3}      & \multicolumn{1}{c}{35.7}            & \multicolumn{1}{c}{88.9}   &56.18   \\ 
PIP                                          & \multicolumn{1}{c}{78.0}       & \multicolumn{1}{c}{49.8}    & \multicolumn{1}{c|}{100.0}  & \multicolumn{1}{c}{97.8}       & \multicolumn{1}{c}{66.0}          & \multicolumn{1}{c|}{126.1}    
                                             & \multicolumn{1}{c}{87.1}      & \multicolumn{1}{c}{62.0}     & \multicolumn{1}{c}{116.5}         &45.17                     & \multicolumn{1}{c}{49.1}      & \multicolumn{1}{c}{34.6}            & \multicolumn{1}{c}{66.0}   &43.77   \\
Hybridcap                                    & \multicolumn{1}{c}{72.1}       & \multicolumn{1}{c}{-}       & \multicolumn{1}{c|}{-}      & \multicolumn{1}{c}{-}          & \multicolumn{1}{c}{-}             & \multicolumn{1}{c|}{-}    
                                             & \multicolumn{1}{c}{33.3}  & \multicolumn{1}{c}{-}         & \multicolumn{1}{c}{-}    & -                   & \multicolumn{1}{c}{-}         & \multicolumn{1}{c}{-}               & \multicolumn{1}{c}{-}      & -\\
VIP                                          & \multicolumn{1}{c}{-}       & \multicolumn{1}{c}{-}       & \multicolumn{1}{c|}{-}      & \multicolumn{1}{c}{-}          & \multicolumn{1}{c}{-}             & \multicolumn{1}{c|}{-}    
                                             & \multicolumn{1}{c}{-}  & \multicolumn{1}{c}{-}         & \multicolumn{1}{c}{-}    & -                   & \multicolumn{1}{c}{-}         & \multicolumn{1}{c}{39.6}               & \multicolumn{1}{c}{-}      & -\\ 
Ours                             & \multicolumn{1}{c}{\textbf{55.0}}& \multicolumn{1}{c}{\textbf{38.9}}& \multicolumn{1}{c|}{\textbf{71.8}} & \multicolumn{1}{c}{\textbf{77.9}}& \multicolumn{1}{c}{\textbf{53.1}}& \multicolumn{1}{c|}{\textbf{97.5}}         
                    & \multicolumn{1}{c}{\textbf{33.1}}& \multicolumn{1}{c}{\textbf{24.0}}& \multicolumn{1}{c}{\textbf{43.2}}   & \textbf{9.92}       & \multicolumn{1}{c}{\textbf{48.7}}& \multicolumn{1}{c}{\textbf{33.5}} & \multicolumn{1}{c}{\textbf{63.4}}     & \textbf{23.52} \\
\hline
\end{tabular}}
\end{table*}
%
\subsubsection{Global Translation Estimation}\label{subsubsec:Tran}
%
The translation estimation differs from pose estimation because pure inertial-based methods suffer from large drifts due to accumulated errors, as demonstrated in~\cite{EgoLocate}.
%
Thus, we adopt a slightly different method for translation estimation.
%
When the visual information is not reliable, we follow TransPose~\cite{TransPose} to use IMU only to estimate the translation. 
%In the cases where visual information is unreliable and thus we only use IMu to estimate the translation, we take an approach similar to TransPose~\cite{TransPose}, where we regress foot-ground contact probabilities and root velocities in the root coordinate system by RNNs (as shown by RNN-T1 and RNN-T2).
%
%Based on the contact rules, we can compute relative translation between frames.
%
%With the orientation measurement of the IMU on the pelvis, 
% The root position can be obtained by accumulating the relative translation, which, though, is prone to low-frequency errors.
%The root position is obtained by accumulating relative translation between two frames, which is prone to low-frequency errors as the accumulation makes the result drift.
%
On the other hand, when visual information is reliable, we directly estimate the absolute position of the human in the camera coordinate system (as shown by RNN-T3).
Then we use the complementary filter algorithm to fuse the two results depending on the reliability of the visual signals.
%On the other hand, visual information is able to correct the accumulated errors.
%
%To achieve this, we directly estimate the absolute position of the human in the camera coordinate system (as shown by RNN-T3).
%
%Due to the noise in the off-the-shelf 2D keypoint detector and depth ambiguity, this position estimation somehow contains high-frequency noise.
%
%Inspired by the complementary filter algorithm, we can apply a high-pass filter on the IMU-based velocity estimation and a low-pass filter on the absolute position estimation, which combine to make a reasonable estimation of human positions.
%
%The cut-off frequency is dynamically determined by the averaged visual confidence.
%
% 跟上面一样，直接就开始讲网络，应该先有个outline
% As illustrated in Fig. ~\ref{fig:pipeline}, inspired by TransPose \cite{TransPose}, We utilize a network RNN-T1 to estimate the probability of foot-ground contact.
% %
% Furthermore, we utilize the foot-ground contact probability to fuse the results of the straightforward foot velocity, calculated from joint rotations, with the velocity estimated by RNN-T2.
% %
% After the fusion, we obtain a fused velocity in the root coordinate system.
% %
% We then transform this fused velocity into the camera coordinate system.
% %
% We fuse the velocity and the estimated absolute position by RNN-T3, obtaining the final translation in the camera coordinate system.
%
\par
To be more specific, RNN-T1 takes inertial measurements $\boldsymbol{x}_{\mathrm{r}}$ as input to estimate foot-ground contact probability $s$.
%
On the other hand, RNN-T2 takes $\boldsymbol{x}_{\mathrm{r}}$ as input and estimates the root velocity $\boldsymbol{{v}_{\mathrm{r}}^{\mathrm{e}}}\in\mathbb{R}^{3}$ in the root coordinate system.
%
To train the models (RNN-T1 and RNN-T2), we utilize the same loss function as TransPose~\cite{TransPose}.
%
We can calculate the foot velocity $\boldsymbol{v}_{\mathrm{r}}^\mathrm{f}\in\mathbb{R}^{3}$ using the estimated pose $\boldsymbol{\theta}_{\mathrm{r}}$ as:
\begin{equation}
\boldsymbol{v}_{\mathrm{r}}^\mathrm{f}=\frac{1}{\Delta t}(\mathrm{GF}(\mathrm{FK}(\boldsymbol{\theta}_{\mathrm{r}}^{(k)}-\mathrm{GF}(\mathrm{FK}(\boldsymbol{\theta}_{\mathrm{r}}^{(k-1)})),
\end{equation}
where $\mathrm{GF}(\cdot)$ represents the function that retrieves the supporting foot position from all joint positions, $\boldsymbol{\theta}_{\mathrm{r}}^{(k)}$ represents the joint rotations in the root coordinate system at frame $k$, and $\Delta t$ is the time interval between frames.
%
%We employ a linear interpolation to fuse the foot velocity $\boldsymbol{v}_{\mathrm{r}}^\mathrm{f}$ and the estimated velocity $\boldsymbol{{v}_{\mathrm{r}}^{\mathrm{e}}}$.
%
If the foot-ground contact probability $s$ is smaller than a threshold $0.7$, we take the estimated root velocity $\boldsymbol{{v}_{\mathrm{r}}^{\mathrm{e}}}$.
%
Otherwise, we derive the root velocity from the foot velocity $\boldsymbol{v}_{\mathrm{r}}^\mathrm{f}$ following TransPose.
%
We denote the final root velocity as  $\boldsymbol{{v}_{\mathrm{r}}}$ in the root coordinate system and transform it into the camera coordinate system as $\boldsymbol{{v}_{\mathrm{c}}}$.
Notice that if we accumulate $\boldsymbol{{v}_{\mathrm{c}}}$ to estimate the root position, the result is prone to low-frequency drift due to the error accumulation.
%
\par
For RNN-T3, we concatenate the inertial measurements in the camera coordinate $\boldsymbol{x}_{\mathrm{c}}$, the canonicalized keypoints $\boldsymbol{p}_{\mathrm{2d}}$, and the corresponding confidence $\boldsymbol{\sigma}$ to estimate the global root position $\boldsymbol{t}_{\mathrm{c}}^{\mathrm{e}}$ of the subject in the camera coordinate system. 
%
The loss function used to train this model is denoted as:
\begin{equation}
\mathcal{L}_{T3}=\Vert \boldsymbol{t}_{\mathrm{c}}^{\mathrm{e}}-\boldsymbol{t}_\mathrm{c}^{\mathrm{GT}} \Vert_2^{2}.
\end{equation}
Also note that due to the noise in the off-the-shelf 2D keypoint detector and depth ambiguity, this position estimation somehow contains high-frequency noise.
%
\par
%Assuming that the subject is in the camera view in the first frame,
%
Next, we fuse the estimated root position $\boldsymbol{t}_{\mathrm{c}}^{\mathrm{e}}$ and the global velocity $\boldsymbol{{v}_{\mathrm{c}}}$ to obtain the final position $\boldsymbol{{t}_{\mathrm{c}}}$ in the camera coordinate system.
%which is represented as a vector in $\mathbb{R}^{3}$.
%
For the fusion process, we utilize the average keypoint confidence $\sigma_{\mathrm{mean}}$ as a reference to fuse the results using the complementary filter algorithm.
%
%To be specific, considering the characteristics of $\boldsymbol{t}_{\mathrm{c}}^{\mathrm{e}}$ and $\boldsymbol{{v}_{\mathrm{c}}}$, we apply a high-pass filter on $\boldsymbol{{v}_{\mathrm{c}}}$ and a low-pass filter on $\boldsymbol{t}_{\mathrm{c}}^{\mathrm{e}}$ to make a reasonable estimation of human positions.
%
%The cut-off frequency is dynamically determined by the averaged visual confidence.
%
The fusion algorithm can be mathematically defined as:
%
\begin{equation}
    \boldsymbol{t}_{\mathrm{c}}^{(k)} = (1-\alpha_k)\boldsymbol{t}_{\mathrm{c}}^{(k-1)} + (1-\alpha_k)\boldsymbol{v}_{\mathrm{c}}^{(k)}\Delta t + \alpha_k\boldsymbol{t}_{\mathrm{c}}^{\mathrm{e}(k)},
\end{equation}
%
where $k$ represents the $k$th frame, $\Delta t$ is the time interval between frames, and $\alpha_k=0.05\sigma_{\mathrm{mean},k}$ is the coefficient dynamically determined by the average keypoint confidence at the $k$th frame $\sigma_{\mathrm{mean},k}$.
%
Intuitively, when visual confidence is low, we directly add the relative translation estimated solely by IMUs to the previous root position.
%
When visual confidence is high, after updating the root position with the IMU-based relative translation, we perform an additional correction step leveraging the estimated root position in the camera frame.
%}
\par
%
Finally, we optimize the joint rotations $\boldsymbol{\theta}_{\mathrm{c}}$ and the global position $\boldsymbol{t}_{\mathrm{c}}$ to minimize the reprojection error and get the final motion output.
%
%The fusion and optimization process is detailed in the supplementary materials.
%
Our energy function is defined as:
\begin{equation}
\boldsymbol{E}(\boldsymbol{\theta}_{\mathrm{c}},\boldsymbol{t}_{\mathrm{c}})=\lambda_{\mathrm{2D}}\boldsymbol{E}_{\mathrm{2D}}+\lambda_{\mathrm{3D}}\boldsymbol{E}_{\mathrm{3D}}+\lambda_{\mathrm{prior}}\boldsymbol{E}_{\mathrm{prior}}+\lambda_{\mathrm{angle}}\boldsymbol{E}_{\mathrm{angle}}+\lambda_{\mathrm{ori}}\boldsymbol{E}_{\mathrm{ori}}.
\end{equation}
%
$\boldsymbol{E}_{\mathrm{2D}}$ enforces the final joint positions reprojects onto 2D close to the 2D keypoints detected by MediaPipe. % \cite{mediapipe}.
%
$\boldsymbol{E}_{\mathrm{3D}}$ enforces the final joint positions close to the predicted one.
%
Following \cite{smplify}, $\boldsymbol{E}_{\mathrm{prior}}$ penalizes the unrealistic human pose using a Geman-McClure \cite{geman-mcclure} error function.
%
Following \cite{smplify}, $\boldsymbol{E}_{\mathrm{angle}}$ penalizes unnatural bending of the knees and elbows.
%
And $\boldsymbol{E}_{\mathrm{ori}}$ enforces the rotations of joints with IMU mounted close to the inertial measurements.
%
We experimentally set $\lambda_{\mathrm{2D}}=1$, $\lambda_{\mathrm{3D}}=1$, $\lambda_{\mathrm{prior}}=0.1$, $\lambda_{\mathrm{prior}}=15.2$, and $\lambda_{\mathrm{ori}}=0.5$.
%
We use the L-BFGS \cite{lbfgs} algorithm to solve this optimization with 1 iteration.
%

%
\subsection{Hidden State Feedback Mechanism}\label{sec:feedback}
The aforementioned dual coordinate strategy can preserve the final result by utilizing the benefits of both branches.
%
However, the two branches do not mutually benefit from each other.
%
For RNN-P1, inspired by \cite{PIP},  utilizing a constant initial hidden state for the RNN training is incorrect.
%
This is because the subject may initiate from different poses, and if the initial state is incorrect, the network can not learn how to change its hidden state according to the changing input signals.
%
%Therefore, this model may incorrectly estimate the joint positions of ambiguous poses.
%
In the case of RNN-T3, when the subject is occluded for a while and reappears in the camera view, the network cannot immediately generate accurate root positions as its hidden states have not been correctly updated during the occlusion.
%
For RNN-P2, RNN-T1, and RNN-T2, we consider them to be less temporally dependent, i.e., these models primarily rely on the input of the current frame rather than the historical information.
%
This consideration motivates us to develop a hidden state feedback mechanism that can enhance the two branches.
%
\par
Considering the difference between RNN-P1 and RNN-T3, we propose different feedback schemes for each.
%
For RNN-P1, as mentioned earlier, a constant initialization of the hidden state in the RNN leads to inaccuracies.
%
PIP utilizes the ground-truth pose to initialize the hidden state of the RNN.
%
This is possible for the first frame but not for the following frames, so after tracking a lot of frames, the initialization has no contribution to the tracking anymore.
%However, assuming the initial pose of the subject is known, the estimation process must either start from a constant pose (such as the pose used during calibration in PIP) or obtain the ground-truth starting pose.
%
%This is limited in certain situations.
%
On the other hand, we have the visual input, and thus we could use the joint positions $\boldsymbol{p}_{\mathrm{c}}^{\mathrm{e}}$ obtained from RNN-P2 to initialize RNN-P1 in the middle of the tracking in case the visual signals are good enough to estimate an accurate pose.
%
%This pose can then be used to initialize the hidden state of RNN-P1.
%
By leveraging the visual information, this mechanism provides a more flexible and adaptive initialization scheme for RNN-P1 during the tracking.
%
This mechanism significantly better distinguishes ambiguous motions, such as standing still and sitting still, because they share identical inertial measurements and the correct historical information is the key to solving the ambiguity.
%
\par
For RNN-T3, since its visual input may be meaningless (e.g. when the performer is out of the camera view), we use the final fused result to update its hidden state if the mean visual confidence of all joints $\sigma_\mathrm{mean}$ is less than the predefined lower bound.
%
To be specific, we use synthesized 2D keypoints $\boldsymbol{p}_\mathrm{2d}$ instead of the unreliable MediaPipe-detected keypoints under these conditions.
%
We synthesize $\boldsymbol{p}_\mathrm{2d}$ as:
\begin{equation}
    \boldsymbol{p}_\mathrm{2d} = \mathrm{\Pi}\left(\mathrm{FK}\left(\boldsymbol{\theta}_\mathrm{c}\right)+\boldsymbol{t}_\mathrm{c}\right),
\end{equation}
where $\mathrm{\Pi}$(·) represents the projection onto $Z=1$ plane from 3D space.
%
Then, we utilize $[\boldsymbol{x}_{\mathrm{c}},\boldsymbol{p}_{\mathrm{2d}},\boldsymbol{\sigma}]$ to run RNN-T3 to update its hidden state.
%
%As $\sigma_\mathrm{mean}$ is low, the output of this RNN-T3 will not affect the final result, but this branch still works fine. 
Due to this hidden state feedback, when $\sigma_\mathrm{mean}$ increases, RNN-T3 will recover immediately to give a result that is both reasonable and consistent with the previous output, avoiding sudden changes.
%
The effectiveness of this feedback mechanism is noticeable when the subject comes back after going outside of the camera view, especially when the re-entry position is different from the going-out position. We will show this in the following section.
%