\section{Introduction}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/teaser.png}
    \caption{
        InstructEval allows the assessment of instruction selection methods for ICL across a range of models and tasks along five metrics.
     %\mz{Make the ranking box horizontal to save space? Now there is too much white space here. Should we add a reference to this figure?}
     }
    \label{fig:teaser}
\end{figure}


One of the most effective insights in NLP research in recent years has been that large language models trained to perform next-token prediction show emergent in-context learning (ICL) abilities~\cite{brown2020language, Scao2022BLOOMA1, Zhang2022OPTOP}.
While the bulk of research interest has shifted away from task-specific models and towards creating ``foundation models" to perform a variety of tasks using appropriately constructed prompts, the performance of ICL remains sensitive to the precise details of prompt construction. Prompt engineering remains critical for achieving optimal ICL performance~\cite{perez2021true, zhao2021calibrate, webson-pavlick-2022-prompt}.
% and will likely remain so as long as textual prompts are the primary means of providing input to foundation models.

In practice, ICL typically involves prompting a language model using a concatenation of a task-specific \textit{instruction}, a short sequence of annotated in-context examples known as \textit{demonstrations}, and a \textit{test example} (Figure \ref{fig:prompt_example}).
Much of the research interest surrounding in-context learning has focused on understanding the optimal selection, ordering of demonstrations, and label-space choices~\cite{Liu2021WhatMG, Su2022SelectiveAM, rubin2022learning, Wang2023LargeLM, lu2021fantastically, Wei2023LargerLM, Pan2023WhatIL}. However, \emph{instruction choice} remains a relatively underexplored aspect of prompt engineering despite its established significance~\cite{mishra-etal-2022-cross} on downstream performance.

Even among recent works exploring automatic instruction selection~\cite{honovich2022instruction, lowperplexityprompts, rlprompt, ape}, the use of different evaluation protocols makes the comparison of their relative performances difficult. Existing studies typically limit their analyses to specific models or tasks; for example, \citet{ape} focus on a single model, and while \citet{rlprompt} consider multiple model scales, they all belong to a single model family. Moreover, evaluations often span disparate task selections with minimal overlap and are primarily dominated by classification tasks, neglecting other task types like multiple-choice QA or generation. Lastly, most previous works tend to emphasize zero-shot accuracy, overlooking other pertinent ICL metrics such as few-shot accuracy and robustness measures.

To address these issues, we build InstructEval, an evaluation suite for the comprehensive evaluation of instruction selection methods. The suite covers a diverse collection of 13 open-sourced autoregressive LLMs from four model families and nine tasks spanning three task types. Additionally, it also incorporates three accuracy metrics and two sensitivity metrics that are of interest to ICL.
We perform evaluations of seven popular instruction selection methods including trivial instruction baselines, manually curated instructions, and sophisticated automatic methods using our suite. 

Overall, we find that the relative effectiveness of these approaches varies significantly across different models and task types. We discover that curated manually-written instructions and task-agnostic instructions can elicit better aggregated performance (over models) than automatically induced ones, highlighting the lack of generalizability of the latter. We also find that including instructions in few-shot prompts usually tends to hurt ICL performance at the model scales we consider. Our findings suggest that it may be optimal for ICL practitioners to omit instructions in few-shot settings and use curated manually-written instructions in zero-shot settings, rather than contemporary automatic induction techniques that require substantial computation and hyperparameter tuning to achieve competitive performance.
We release the evaluation suite we develop to aid the systematic study of even more questions regarding prompt engineering that we do not explicitly address in our work.
%There are more questions regarding the use of instructions in in-context learning settings that our work does not explicitly address (since we attempt to remain faithful to the settings proposed by the authors of the methods we evaluate): Can instructions generated for a task by one model be applied to distinct models and families? Does prepending the instruction to every demonstration affect performance? What is the effect of ensembling instructions? What is the trade-off between compute and performance when using automatic prompt generation methods? As another contribution, we release our evaluation suite to aid future work that systematically addresses such questions.
