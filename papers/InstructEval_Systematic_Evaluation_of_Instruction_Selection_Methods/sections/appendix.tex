\appendix

\section{Variation across model families}
\input{tables/accuracy_family_separated}
\input{tables/templates}
\input{tables/instruction_examples}

We also tabulate the mean relative gain values for zero-shot and few-shot accuracies computed separately for each model family in Table~\ref{table:accuracy_family_separated}, to understand the effect that model family has on instruction performance. Although the trends we discuss in Section~\ref{sec:results} regarding task-agnostic instructions and PromptSource instructions tending to dominate few-shot and zero-shot settings persist, we note that the instruction selection method that emerges the best-performing alternative often changes on varying the choice of model family and task-type. For instance, the automatic instruction induction methods APE and RLPrompt do show above-average performance for certain model families and task-types, but this behavior does not consistently extend to other families and types as well. This indicates a lack of generalizability in these methods.

\section{Implementation details}
\subsection{Evaluation}

%To ameliorate the effect of statistical noise, we include instructions corresponding to 5 random seeds for each method we consider in the instruction pool $\mathcal{I}$, and report results for each method by averaging across these 5 seeds. For consistency, we use $K=6$ demonstrations randomly sampled from the task's training set for every experiment we perform in the few-shot setting and use a fixed task-specific prompt template with every instruction we consider.

We ameliorate the effect of statistical noise by rerunning each instruction selection/induction method we study using 5 random seeds independently for every task (and for every model, where applicable) and report results for each instruction selection/induction method by averaging the aggregated scores associated with all 5 instructions. 

We use $K=6$ demonstrations randomly sampled from the task's training set for every experiment we perform in the few-shot setting.

To maintain consistency, we perform all our experiments using fixed task-specific prompt templates. Each prompt begins with the instruction being tested, and continues into a sequence annotated demonstrations and a test example, each of which follow the templates listed in Table~\ref{table:templates}.




\subsection{Instruction selection methods}
\label{sec:baseline-implementation-details}

\paragraph{PromptSource} We sample and evaluate a random subset of instructions from those included in the public PromptSource repository for each task in our evaluation suite.

\paragraph{Ad hoc} We obtain the set of ad hoc instructions we evaluate for a task by tasking ChatGPT with generating 40 paraphrases of instructions for the task that we obtain from PromptSource. We then select a random sample of instructions from this 40-instruction pool and perform evaluations using each sampled instruction.

\paragraph{Low Perplexity} For each task, we rerank a pool of ChatGPT paraphrases of PromptSource instructions using the SPELL algorithm described by \cite{lowperplexityprompts}. When prompting a specific model, we choose the instruction with the lowest perplexity as measured by that model. 

\paragraph{APE} We use the official repository released by \cite{ape} to generate instructions for each of the tasks we consider. To remain consistent with the original methodology, we use the OpenAI DaVinci to induce and evaluate instructions during the induction phase. We opt to use the simpler version of the methodology proposed by the authors since they report that the computationally intensive Monte-Carlo search strategy only provides marginal improvements in accuracy.

\paragraph{RLPrompt} We use the public repository released by \citet{rlprompt} to induce instructions for the RLPrompt baseline in our evaluations. Although the original work only performs evaluations over classification datasets with a fixed label-space, we augment the codebase to allow instruction induction for MCQ tasks as well by formulating these as cloze-style completion tasks. We create instructions for all tasks  using the default settings of hyperparameters included with the codebase.

\paragraph{Task-agnostic} We completely omit instructions from the prompt when evaluating null instructions. We list the set of generic instructions we evaluate in \tableref{table:generics}.
\input{tables/generics}

We include examples of the instructions we obtain for each method in Table~\ref{table:instruction_examples}.

\section{Drawbacks of aggregation techniques used in previous work}
\label{app:scoring}
Some previous works like the HELM \cite{helm} benchmark also face similar challenges when attempting to compare high-dimensional vectors -- each representing a model evaluated over a variety of tasks -- against each other. HELM resorts to scoring models using head-to-head win rates. The win rate associated with a model indicates the fraction of head-to-head comparisons between the given model and all other models, across all scenarios, where the given model performs better along a specific metric. A notable disadvantage of this scoring technique is that it obscures the magnitude of variation in the metric associated with each test model and only conveys ordinal information about the relative performances of each model. This characteristic of head-to-head win rates makes them unsuitable for spotting broad trends across families of prompting methods.

In other works like BIG-bench \cite{bigbench}, raw metric scores representing task performance are normalized to vary from a range of 0-100 such that a normalized score of 0 corresponds to poor performance, while a normalized score of 100 corresponds to excellent performance on the task. This is done in an attempt to be able to compare the performance of a model across a variety of tasks of varying difficulty such that the normalization proves more forgiving on difficult tasks. While this score does capture cardinal information associated with the underlying variable, it relies on the knowledge of human experts to determine raw score thresholds that constitute poor or excellent performance along a given metric. To apply such a normalization scheme in our case, one would need access to a large array of such threshold scores corresponding to each model scale, task, and metric we consider. Obtaining such threshold scores across all our settings is challenging given the number of tests we perform and the variety of metrics we consider. Hence, this type of normalization proves infeasible in our case.


\iffalse
\section{Full results}

We present our entire array of unaggregated results in Table \ref{table:all_results}.
\input{tables/all_results}
\fi