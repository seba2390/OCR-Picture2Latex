\section{Experimental setup}
\label{sec:experimental_setup}
\label{sec:results}
We perform experiments evaluating 3 families of instruction selection methods (listed in \tableref{table:methods}). 

\paragraph{Task-agnostic instructions}
In practical ICL settings, it is straightforward to use instructions that contain no task-specific information.
\begin{itemize}[leftmargin=*]
    \item \textbf{Null instruction:} We assess the impact of omitting instructions from the prompt. This amounts to constructing prompts that consist of demonstrations and a test example in few-shot, and only an unanswered test-example in zero-shot settings.
    
    \item \textbf{Generic instructions:} We assess the impact of using generic task-agnostic instructions such as \texttt{Complete the following task:}. These instructions require minimal effort to write since they do not demand knowledge of the task. We list the set of generic instructions we evaluate in Table~\ref{table:generics}.
\end{itemize}

\paragraph{Manual task-specific instructions} 

We evaluate manually-written task-specific instructions that ICL practitioners may use in practice.

\begin{itemize}[leftmargin=*]
    \item \textbf{PromptSource:} PromptSource \cite{promptsource} is a public collection of manually-curated prompt templates pertaining to 170+ datasets which are often used off-the-shelf for ICL and are generally considered high-quality.
    
    \item \textbf{Ad hoc:} 
    ICL practitioners often create task-specific instructions ad hoc, based on the semantics of the given task. We simulate this mode of instruction selection by asking ChatGPT to generate several paraphrases of task-specific seed instructions we obtain from PromptSource and randomly sampling from the generated set.
    
\end{itemize}

\paragraph{Automatically synthesized task-specific instructions} We evaluate 3 popular automated instruction selection and induction methods that are representative of previous work.

\begin{itemize}[leftmargin=*]
    \item \textbf{Low Perplexity:} \cite{lowperplexityprompts} find that the perplexity a model associates with an instruction is negatively correlated with its ICL performance when using that instruction. We use the SPELL algorithm proposed by \citet{lowperplexityprompts} to select the least perplexity instructions (for each model) from a large pool of ChatGPT paraphrased instructions.

    \item \textbf{APE}: \cite{ape} is an automatic few-shot method for inducing instructions by prompting a language model to describe the given task, and refining the set of generated prompts using accuracy on a small held-out validation set. While \citet{ape} limit their evaluation to GPT-3~\cite{brown2020language} and InstructGPT~\cite{instructgpt}, we assess APE's applicability to a significantly larger set of models and tasks.

    \item \textbf{RLPrompt}~\cite{rlprompt} is a reinforcement-learning-based approach for few-shot prompt induction. While the original authors only evaluate their method using GPT-2 on a few classification tasks, we expand this assessment to many more models and tasks. Notably, we assess the extensibility of RLPrompt to MCQ tasks, but do not test RLPrompt performance on GQA tasks since the algorithm is not directly applicable to generation tasks. 
\end{itemize}


