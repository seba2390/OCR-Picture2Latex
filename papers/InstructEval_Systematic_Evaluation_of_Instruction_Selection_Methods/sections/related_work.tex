\section{Related Work}
%\mz{We lack a comparison to this paper: https://arxiv.org/abs/2305.14877}
%\anirudh{refine to be more on-topic?}
\iffalse
\paragraph{In-Context Learning} As language models have scaled, the ability to learn in-context, without any weight updates, has emerged. \cite{brown2020language}. While other families of large language models have emerged, in-context learning remains ubiquitous \cite{llama, bloom, gptneo, opt}. Although such as HELM \cite{helm} have arisen for systematic evaluation of \emph{models}, there is no systematic framework to our knowledge for evaluating \emph{prompting methods}, and validating prompt engineering heuristics. The test-suite we propose will ensure that progress in the field of prompt-engineering is structured and objectively evaluated. 

\paragraph{Prompt Engineering Methods} Researchers are interested in the automatic design of high performing instructions for downstream tasks. Some focus on simple heuristics, such as selecting instructions that have the lowest perplexity \cite{lowperplexityprompts}. Other methods try to use large language models to induce an instruction when provided with a few input-output pairs \cite{ape}. Researchers have also used RL objectives to create discrete token sequences that can serve as instructions \cite{rlprompt}. Since the datasets and models used in these works have very little intersection, it is impossible to compare these methods objectively and glean insights. In our work, we evaluate these three methods on a diverse set of tasks and models, and analyze their relative performance. Additionally, we recognize that there are many other interesting angles of prompting that are not covered by instruction engineering \cite{weichain, react, selfconsistency}, but we leave these to future work.

\paragraph{Analysis of Prompting Methods} While most prompt engineering methods focus on accuracy, there are many other interesting dimensions of performance as well. For instance, researchers have found that for most tasks, the selection of demonstrations plays a large role in few-shot accuracy \cite{whatmakesgoodicexamples, selectionmachinetranslation, knnprompting}. Additionally, many researchers have found that even permuting the ordering of a fixed set of demonstrations has a significant effect on downstream accuracy \cite{fantasticallyorderedprompts}. Prompts that are sensitive to the permutation of demonstrations have been shown to also have lower accuracies \cite{relationsensitivityaccuracy}. Especially in low-resource domains, which includes the large public usage of in-context learning, these large swings in accuracy make prompting less dependable. In our test-suite we include sensitivity metrics that go beyond accuracy and allow us to find methods that are not only performant but reliable.

\paragraph{Existing Benchmarks} We recognize that other holistic in-context learning benchmarks exist. BigBench is a large benchmark of 204 tasks that are beyond the capabilities of current LLMs. BigBench seeks to evaluate the few-shot abilities of state of the art large language models, focusing on performance metrics such as accuracy \cite{bigbench}. Similarly, HELM is another benchmark for language model in-context learning ability. Rather than only focusing on performance, HELM branches out and considers many other metrics such as robustness and bias \cite{helm}. Both BigBench and HELM focus on ranking different language model, while fix a generic instruction and prompt format. We instead choose to evaluate instruction induction / selection methods over a fixed set of models. We are the first ever evaluation script that compares different prompt-engineering methods head to head. 
\fi

\paragraph{In-Context Learning and Existing Benchmarks} As language models have scaled, in-context learning has emerged as a popular paradigm and remains ubiquitous among several autoregressive LLM families \cite{brown2020language, llama, bloom, gptneo, opt}. Benchmarks like BigBench \cite{bigbench} and HELM \cite{helm} have been created for the holistic evaluation of these models. BigBench focuses on few-shot abilities of state-of-the-art large language models, while HELM extends to consider metrics like robustness and bias. However, these benchmarks focus on evaluating and ranking \emph{language models}, and do not address the systematic evaluation of \emph{prompting methods}. Although contemporary work by \citet{yang2023improving} also aims to perform a similar systematic analysis of prompting methods, they focus on simple probability-based prompt selection while we evaluate a broader range of methods including trivial instruction baselines, curated manually selected instructions, and sophisticated automated instruction selection.

\paragraph{Automated Prompt Engineering Methods} There has been interest in performing automated prompt-engineering for target downstream tasks within ICL. This has led to the exploration of various prompting methods, ranging from simple heuristics such as selecting instructions with the lowest perplexity \cite{lowperplexityprompts}, inducing instructions from large language models using a few annotated input-output pairs \cite{ape}, to utilizing RL objectives to create discrete token sequences as prompts \cite{rlprompt}. However, these works restrict their evaluation to small sets of models and tasks with little intersection, hindering their objective comparison. %\mz{For paragraphs that only have one work in the last line, try to shorten the paragraph to squeeze in context.}

\paragraph{Understanding in-context learning} There has been much recent work attempting to understand the mechanisms that drive in-context learning. Studies have found that the selection of demonstrations included in prompts significantly impacts few-shot accuracy across most tasks \cite{whatmakesgoodicexamples, selectionmachinetranslation, knnprompting}. Works like \cite{fantasticallyorderedprompts} also show that altering the ordering of a fixed set of demonstrations can affect downstream accuracy. Prompts sensitive to demonstration permutation often exhibit lower accuracies \cite{relationsensitivityaccuracy}, making them less reliable, particularly in low-resource domains.

Our work aims to bridge these gaps by systematically evaluating the efficacy of popular instruction selection approaches over a diverse set of tasks and models, facilitating objective comparison. We evaluate these methods not only on accuracy metrics, but also on sensitivity metrics to glean additional insights. We recognize that other facets of prompting not covered by instruction engineering exist \cite{weichain, react, selfconsistency}, and defer these explorations to future work. 