\section{Evaluation Suite}
%We design a comprehensive evaluation suite to assess the performance of instructions along 5 metrics relevant to practical ICL. To ensure a fair analysis, we test each instruction selection method across 9 tasks and 13 autoregressive models from 4 model families.

\subsection{Prompt format}

\begin{figure}[t]
    \centering
    %\input{figures/prompt_example_tikz}
    \includegraphics[width=\linewidth]{figures/prompt_example.png}
    \caption{
        An example of a prompt following the template we use for IMDB. By  `prompt' we refer to the concatenation of the \textcolor{darkyellow}{instruction}, solved \textcolor{blue}{demonstrations} and an unsolved \textcolor{red}{test example}.
    }
    \label{fig:prompt_example}
\end{figure}
We define a `prompt' as the full textual input provided to an LLM. Our evaluation suite supports the use of any number of demonstrations, arbitrary demonstration templates and the inclusion of custom strings anywhere within the prompt. Since the instructions used can be set to any arbitrary strings, users are free to use any external means to select instructions and have them evaluated by our suite.

For consistency, we conduct all experiments in this work using prompts that begin with an instruction, continue with a sequence of annotated training demonstrations, and conclude with an unsolved test example\footnote{Instructions are omitted during `Null instruction' evaluations. Demonstrations are omitted in zero-shot evaluations.} (Figure~\ref{fig:prompt_example}), and express each example in a minimal, task-specific key-value format (Table~\ref{table:templates}) that reflects task semantics.

\subsection{Metrics}
\begin{figure*}
  \begin{subfigure}[b]{0.32\textwidth}
    \centering
    \resizebox{\linewidth}{!}{
    \input{figures/perturbation_example_tikz}
    }
    \caption{Perturbation accuracy}
  \end{subfigure}\hfill
  \begin{subfigure}[b]{0.32\textwidth}
    \centering
    \resizebox{\linewidth}{!}{
    \input{figures/selection_example_tikz}
    }
    \caption{Selectional sensitivity}
  \end{subfigure}\hfill
  \begin{subfigure}[b]{0.32\textwidth}
    \centering
    \resizebox{\linewidth}{!}{
    \input{figures/permutation_example_tikz}
    }
    \caption{Permutational sensitivity}
  \end{subfigure}
  \caption{
        We provide schematic diagrams that show prompts are modified to measure \textit{perturbation accuracy}, \textit{selectional sensitivity} and \textit{permutational sensitivity}. We perturb the test input to measure perturbation accuracy, and demonstration selection and permutation respectively while measuring selectional and permutational sensitivity.
    }
    \label{fig:sensitivity-metrics}
\end{figure*}

\label{sec:metrics}
\paragraph{Accuracy metrics} Accuracy is typically the primary metric of interest in ICL. While ICL is most commonly performed in few-shot settings where a handful of annotated demonstrations are included in the prompt, models are also prompted zero-shot without the use of such demonstrations. Since real-world scenarios can often contain grammatical errors and misspellings in the test input, it is desirable to find prompts robust to these perturbations. Hence, we measure \textit{zero-shot accuracy}, \textit{few-shot accuracy}, and \textit{perturbation accuracy}\footnote{We choose to treat this as an accuracy metric rather than a sensitivity metric since it is not meaningful to measure sensitivity to such perturbations in situations where a prompt only elicits near random-chance task performance from a model.} in our evaluations. Following \citet{helm}, we measure perturbation accuracy by introducing random capitalization, spacing, contractions and common misspellings in the test input.

\paragraph{Sensitivity metrics} Previous work has shown that the accuracy obtained using a prompt template can fluctuate significantly as a function of the set of demonstrations included in the prompt ~\cite{Liu2021WhatMG, Su2022SelectiveAM, rubin2022learning, Wang2023LargeLM} and the order they are presented in \cite{fantasticallyorderedprompts}. It may be desirable in practice to identify prompt templates and instructions that offer consistent performance regardless of the choice of demonstrations and their arrangement. Hence, we introduce \textit{selectional sensitivity} and \textit{permutational sensitivity} metrics to measure the sensitivity of chosen instructions respectively to selected demonstrations, and the order in which they are arranged. We quantify the sensitivity of an instruction (given a model and task) using the standard deviation of accuracies obtained on varying  the selection or permutation of the demonstrations used, each across 16 random choices.

\subsection{Aggregating metrics across Models}

Each instruction selection method being tested across $N$ models and $M$ datasets yields $NM$ values per metric. Comparing these $NM$-dimensional vectors directly is complex. It can be challenging to reduce them to a single representative scalar. Simple approaches such as computing the mean of these $NM$ values can prove inadequate since the resulting scores would tend to be heavily influenced by metric values that exhibit a high variance across different inspected methods. 

We opt against using aggregation techniques used by previous works \cite{helm, bigbench} due to their drawbacks (Section~\ref{app:scoring}) and instead adopt `mean relative gain' as a means to aggregate accuracy metrics across multiple models. We rely on simple averaging for sensitivity metrics, partly because we observe that these quantities do not show much variation across methods.

\subsubsection{Accuracy metrics}
Considering the range of models and datasets in our evaluation suite, we unsurprisingly observe substantial variation in accuracy magnitudes across model scales and tasks. However, we notice that the degree of variation in accuracy due to instruction choice is usually considerably smaller than the degree of variation due to model and task choice.

To meaningfully compare and aggregate the relative performance of different instruction selection methods across models, we use a measure called \textit{mean relative gain}. First, we define the \textit{relative gain} for a value $x$ from a population $P$ as the percentage by which $x$ exceeds the mean value of $P$:

$$\text{r-gain}_P(x) = 100 \times \dfrac{x-\mu_P}{\mu_P}$$

Consider a collection of models $\mathcal{M}$ and instructions $\mathcal{I}$ for a task $t$. Given a model $m$, we calculate the raw accuracy scores $s_{tmi}$ for each instruction $i \in \mathcal{I}$. Taking this set $S_{tm}$ to be the population, we compare the performances of the instructions against each other by computing their corresponding relative gains $r_{tmi} = \text{r-gain}_{S_{tm}}(s_{tmi})$. Each $r_{tmi}$ represents the degree by which method $i$ outperforms the average performance along the metric on task $t$ for model $m$.

We now define the mean relative gain as 
$$\overline{r}_{ti} = \dfrac{1}{|\mathcal{M}|} \sum_{m \in \mathcal{M}} r_{tmi}$$

These $\overline{r}_{ti}$ values, tabulated and analyzed in \secref{sec:results}, capture not only the ordinal information about each method's performance on a given task but also provide an intuitive sense of the magnitude by which these methods outperform others. Specifically, if an induction method $i$ has a mean relative gain $\overline{r}_{ti}$ on task $t$, this means that method $i$ exceeds average performance (across $\mathcal{I}$) on task $t$ by $\overline{r}_{ti}$ percent when averaged across models $\mathcal{M}$. 

\subsubsection{Sensitivity metrics}
To aggregate the sensitivity of an instruction selection/induction method $i$ over all models for a task $t$, we simply compute the average of the raw sensitivity scores (described in \secref{sec:metrics}). Specifically, if $\sigma_{tmi}$ is the raw sensitivity score obtained for model $m$ and task $t$ when using instruction $i$, then the aggregated sensitivity score $\overline{\sigma}_{ti}$ is given by 

$$\overline{\sigma}_{ti} = \dfrac{1}{|\mathcal{M}|} \sum_{m \in \mathcal{M}} \sigma_{tmi}$$

We choose to avoid more sophisticated aggregation strategies like relative gain for sensitivity metrics since standard deviations are already secondary metrics making it unintuitive to discuss the relative gain of the standard deviation obtained using a method over the average. %\mz{Do we still average the stds? It seems that in table 5 we didn't do it?}

\iffalse
To fairly compare instruction induction methods across various models and tasks, we adopt mean z-scores as a measure of relative performance along a specific metric. This choice allows us to capture the magnitude of variation in the underlying metric without requiring expert knowledge about its specific range of variation.~\footnote{Please refer to Appendix \ref{app:scoring} for more detailed information on why we opted not to use head-to-head win rates from HELM~\cite{helm} and normalized accuracy from BigBench~\cite{bigbench} as alternative scoring methods.}

A z-score measures the relationship between a value and the population it is drawn from. It quantifies how many standard deviations a value $x$ is away from the mean of the population $P$:
$$\text{z-score}_{P}(x) = \dfrac{x - \mu_P}{\sigma_P},$$
Let $s_{pmt}$ represent the raw scores of each instruction induction method $p$ for model $m$ and task $t$. We denote the subset of these metric scores associated with each pair $(m,t)$ as $$S(m,t) = \{s_{pmt}: p \in \mathcal{P}\}.$$ We compute the z-score for metric scores within each $S(m, t)$ and group them by a method $p$:
$$Z_p = \{\text{z-score}_{S(m,t)}(s_{pmt}) : (m,t) \in \mathcal{M} \times \mathcal{T}\}.$$
We then take an average and derive the mean z-score for $p$ across tasks and models:
$$\overline{z}_p = \sum_{z_p \in Z_p} z_p / |Z_p|$$

These mean z-score values are indicative of the degree to which each instruction induction method outperforms all other method under consideration along a certain metric, aggregated over all possible choices of models in $\mathcal{M}$ and tasks in $\mathcal{T}$.  Although they do communicate the magnitude by which certain methods outperform others, they are indeed bound by the limitation that they have to be recomputed for all $p$ whenever a new method $p'$ is inserted into $\mathcal{P}$. However, this limitation is shown by scoring systems used in previous works \cite{helm} as well and is not a cause for concern in practice when evaluating the relative performances of instruction induction methods.
\fi


\subsection{Tasks}
\input{tables/tasks} 
While previous instruction induction \cite{ape, rlprompt} work has tended to focus mostly on classification tasks, we include 9 tasks (\tableref{table:tasks}) in our evaluation suite spanning classification (CLS), multiple-choice question-answering (MCQ) and generative question-answering (GQA) to assess the applicability of instruction selection and induction methods to other task-types as well. We concentrate on tasks that are challenging to contemporary language models, and yet are not so demanding that the performance of these models does not exceed random chance. We exclude certain generative tasks, like summarization, which are challenging to assess objectively.~\footnote{Standard summarization metrics correlate poorly with human preferences~\cite{helm, goyal2023news}.}

\input{tables/models}

\subsection{Models}
\input{tables/methods}
We include a diverse range of 13 autoregressive LLMs (\tableref{table:models}) from 4 model families of sizes ranging from 1.1 billion to 20 billion parameters in our evaluation suite. We choose contemporary models that span different architectures and training paradigms which are known to show good ICL performance. This diversity bolsters the generalizability of insights obtained using our evaluation suite while mitigating potential bias towards any specific model family. Moreover, we select open-source models which are large enough to show non-trivial ICL performance while still being small enough to run on reasonable consumer hardware to ensure the practical significance of our findings.
