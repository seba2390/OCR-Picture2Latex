\section{Results}
\input{tables/accuracy_perc}

We tabulate the mean relative gain values over accuracy metrics in Table~\ref{table:accuracy_perc}, and the mean standard deviations corresponding to selectional and permutational sensitivity metrics in Table~\ref{table:std_mean}.

\subsection{Less sophisticated instruction selection methods tend to show higher accuracy}

We find that \textbf{task-agnostic instructions dominate in few-shot settings} with Null instructions and Generic instructions achieving the highest aggregated performance in 5/9 tasks for few-shot accuracy and 6/9 tasks for perturbation accuracy. Although both these methods show above-average performance in few-shot settings, Null instructions tend to perform better among the two.

Although PromptSource instructions only show an average performance in few-shot settings, their \textbf{manually curated task-specific instructions prove most effective in zero-shot settings}, achieving the highest aggregated performance in 6/9 tasks and usually achieving markedly higher mean relative gain values than even the runner-up method for the task. %For instance, PromptSource instructions outperform the average by $>$17\% in GQA tasks.
This is especially true of GQA tasks where PromptSource instructions outperform the average by $>$17\%.

\textbf{Automatic task-specific instructions are usually outperformed by simple baselines.} They fail to achieve the best zero-shot performance on any task we consider. While they do sometimes perform competitively with simpler baselines in the few-shot setting, emerging as the best-performing instructions in 2/9 tasks, this behavior is inconsistent. Although Low Perplexity instructions and APE instructions seldom show above-average performance in either setting, RLPrompt instructions show above-average performance in 5/7 tasks in both settings. They are still usually outperformed by instructions obtained through simpler means such as Null and PromptSource instructions.

\subsection{Ranges of variation of aggregated scores}
We notice that instructions have a more significant impact in zero-shot settings as compared to few-shot settings. For most tasks, we find that the highest mean relative gain values achieved in the zero-shot setting are markedly greater than those in the few-shot setting. Accordingly, the minimum values for each task are also relatively lower in zero-shot settings. This finding suggests that instructions play a significant role in informing models of semantics in zero-shot settings whereas in few-shot settings, most of a model's understanding of task-semantics comes from the demonstrations.

The degree of variation in accuracy due to instruction choice varies considerably across tasks. AG News and Emotion show the highest variability in few-shot performance while GQA tasks show the most variability in zero-shot settings.

\input{tables/std_mean}
Table~\ref{table:std_mean} shows that selectional and permutational sensitivities vary dramatically across tasks even though they are  roughly consistent across all methods for a given task implying that all the methods we evaluate are comparable in sensitivity, which is unsurprising since none of them explicitly optimize for it. We also find that most methods show comparable, but usually lower permutational sensitivity than selectional sensitivity across all tasks.

\subsection{Analysis}
\input{tables/accuracy_perc_scale_separated}
We tabulate the mean relative gain values for zero-shot and few-shot accuracies computed separately for ``small" models with $<6$ billion parameters and ``large"
 models with $\ge 6$ billion parameters in Table~\ref{table:accuracy_perc_scale_separated}. For ease of comparison, we average the mean relative gain values thus obtained by task-type. 
Although the observations that PromptSource and task-agnostic instructions tend to perform the best across zero- and few-shot settings persist across model scales, we find that the ranges of variation in the few-shot mean relative gain values for large models are consistently smaller than those for small models for every task-type. This suggests that large models are able to grasp task semantics from demonstrations (when provided) while small models are more sensitive to the instruction used. 

\subsection{Discussion}
Our findings reveal that in practical in-context learning settings, simpler prompting methods, such as task-agnostic or expert manually written instructions, often outperform automatically synthesized ones at the model scales we consider. Task-agnostic methods show strong performance in few-shot settings, whereas expert manual instructions appear crucial for achieving good zero-shot accuracy. The superiority of these straightforward methods over automatically induced instructions, which are often not competitive even with simple baselines, suggests a lack of transferability and generalizability among automatic induction methods. The competitive performance of automatic induction methods like APE and RLPrompt as reported by their authors implies either a limitation in their generalizability to a broader range of models and tasks, or the need for substantial hyperparameter tuning to get them to work well across models and tasks.

Our findings suggest that ICL practitioners may often be better off forgoing computationally expensive instruction induction or selection methods in favor of task-agnostic or manually written instructions, which seem to generalize better. Interestingly, we also find that methods that excel for one model and task do not necessarily also perform well for other tasks and models. Consequently, ICL practitioners may be forced to experiment with various instruction selection methods on a model- and task-specific basis in a manner reminiscent of hyperparameter tuning to find the best choice.

On the other hand, since few-shot ICL performance remains largely consistent regardless of the choice of instruction, practitioners could perhaps benefit from simply providing a few in-context demonstrations when available. The fact that null instructions tend to outperform all other methods in our study in few-shot settings suggests that it can be challenging to find instructions that reliably inform diverse models about task semantics. When models fail to grasp the semantics signaled by instructions, these may simply serve as a source of noise, hence impairing ICL performance.

Our findings underscore a broader issue regarding the inconsistent and often insufficient evaluation of instruction selection and induction techniques. We call for more comprehensive evaluations in this space and encourage the use of our evaluation suite to facilitate this process. 