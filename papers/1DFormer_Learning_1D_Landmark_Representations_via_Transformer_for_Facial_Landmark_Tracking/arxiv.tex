\documentclass[review]{elsarticle}

\usepackage{lineno,hyperref}
\usepackage{booktabs}
\usepackage{rotating}
\usepackage{adjustbox}
\usepackage{diagbox}
\usepackage{multirow}
\usepackage[flushleft]{threeparttable} % tablenotes
\usepackage{graphicx}
\usepackage{epstopdf}
\graphicspath{{figures/}}
\usepackage{subfigure}
\usepackage{stfloats}
\usepackage{amsmath}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{listings}
\usepackage{array}
\usepackage{amssymb}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{subcaption}
\usepackage{dsfont}
\usepackage{multirow}
\usepackage{color}
\renewcommand{\thefootnote}{*}
\makeatletter
\newcommand*{\centerfloat}{%
	\parindent \z@
	\leftskip \z@ \@plus 1fil \@minus \marginparwidth
	\rightskip \leftskip
	\parfillskip \z@skip}
\makeatother


\modulolinenumbers[5]

\journal{Journal of \LaTeX\ Templates}

%%%%%%%%%%%%%%%%%%%%%%%
%% Elsevier bibliography styles
%%%%%%%%%%%%%%%%%%%%%%%
%% To change the style, put a % in front of the second line of the current style and
%% remove the % from the second line of the style you would like to use.
%%%%%%%%%%%%%%%%%%%%%%%

%% Numbered
%\bibliographystyle{model1-num-names}

%% Numbered without titles
%\bibliographystyle{model1a-num-names}

%% Harvard
%\bibliographystyle{model2-names.bst}\biboptions{authoryear}

%% Vancouver numbered
%\usepackage{numcompress}\bibliographystyle{model3-num-names}

%% Vancouver name/year
%\usepackage{numcompress}\bibliographystyle{model4-names}\biboptions{authoryear}

%% APA style
%\bibliographystyle{model5-names}\biboptions{authoryear}

%% AMA style
%\usepackage{numcompress}\bibliographystyle{model6-num-names}

%% `Elsevier LaTeX' style
\bibliographystyle{elsarticle-num}
%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\begin{frontmatter}
\title{1DFormer: Learning 1D Landmark Representations via Transformer for Facial Landmark Tracking}
\author{Shi Yin$^1$, Shijie Huang$^1$, Defu Lian$^2$, Shangfei Wang$^3$, Jinshui Hu\thanks{Corresponding Author}$^1$, Tao Guo$^1$, Bing Yin$^1$, Baocai Yin$^1$ and Cong Liu\thanks{Corresponding Author}$^1$} 	
\address{$^1$iFLYTEK Research, Hefei City, Anhui Province, China, 230001\\
$^2$School of Data Science, University of Science and Technology of China\\
$^3$School of Computer Science and Technology, University of Science and Technology of China}

\begin{abstract}
	Recently, heatmap regression methods based on 1D landmark representations have shown prominent performance on locating facial landmarks.  However, previous  methods ignored to make deep explorations on the good potentials of 1D landmark representations for sequential and structural modeling of multiple landmarks to track facial landmarks. To address this limitation, we propose a Transformer architecture, namely 1DFormer, which learns informative 1D landmark representations by capturing the dynamic and the geometric patterns of landmarks via token communications in both temporal and spatial dimensions for facial landmark tracking. For temporal modeling, we propose a recurrent token mixing mechanism, an axis-landmark-positional embedding mechanism, as well as a confidence-enhanced multi-head attention mechanism to adaptively and robustly embed long-term landmark dynamics into their 1D representations; for structure modeling, we design intra-group and inter-group structure modeling mechanisms to encode the component-level as well as global-level facial structure patterns as a refinement for the 1D representations of landmarks through token communications in the spatial dimension via 1D convolutional layers. Experimental results on the 300VW and the TF databases show that 1DFormer successfully models the long-range sequential patterns as well as the inherent facial structures to learn informative 1D representations of landmark sequences, and achieves state-of-the-art performance on facial landmark tracking.
\end{abstract}
\begin{keyword}
Facial landmark tracking, 1D heatmap regression, Transformer
\end{keyword}
\end{frontmatter}

%\linenumbers

\section{Introduction}

Tracking facial landmarks from a video stream \cite{survey,survey2} is a fundamental task for several intelligent multi-media applications, such as emotion analysis and human-computer interaction, while remaining unsolved for the challenging ``in-the-wild" scenarios. Recently, 1D heatmap regression methods, which are built based on 1D landmark representations, i.e., the light-weight 1D feature vectors and heatmaps representing the marginal distribution of every landmark on each axis, have achieved prominent performance for landmark localization of human faces \cite{AOHR, DBLP:conf/accv/XiongZDS20, HybridMatch}, human bodies \cite{I2L, SimCC, chi2022human} and satellites \cite{DBLP:journals/remotesensing/LiuZCW22}.  Compared to the 2D landmark representations  \cite{hourglass,how_far,HRnet} which had quadratic spatial complexity, the 1D representations, with a linear spatial complexity, could achieve higher resolutions and enough channels under the limited machine memory. Thus, these methods based on 1D representations could capture more details of the spatial patterns of landmarks and bring a lower quantization error as well as a better accuracy on locating facial landmarks under the same hardware condition. However, previous methods have not fully explored the good potentials of 1D landmark representations for temporal sequence modeling, which is critical for the facial landmark tracking task. Most of these methods \cite{DBLP:conf/accv/XiongZDS20, I2L, SimCC, chi2022human, DBLP:journals/remotesensing/LiuZCW22} were designed to detect landmarks from static images, without an exploration of temporal modeling. To the best of our knowledge, only Yin \textit{et al.} \cite{AOHR} applied 1D representations to model temporal sequences of landmarks. However, their sequential encoder was quite simple with fixed preset weights for fusing features from the past frames, without a dynamic weight assignment mechanism to adaptively model the temporal correlations and confidences of landmarks from different frames.  These preset weights also decreased dramatically with the exponential decay of time, causing a sub-optimal performance on capturing long-term temporal dependencies from historical frames. What's more, all of these methods predicted the 1D representation of each landmark in a loosely coupled manner, without consideration of the structure modeling upon multiple landmarks. These limitations caused the accuracy bottleneck of these methods.

\begin{figure}[t]
	\centering
	\includegraphics[scale=0.45]{figure1c.pdf}
	\caption{An illustration of the 1D heatmap regression methods, which are built upon 1D landmark representations, including 1D feature vectors and heatmaps. Although current methods achieved remarkable performance on detecting landmarks, they ignored a deep exploration on temporal and structural modeling, which is critical for landmark tracking. See Section \ref{related_work} for detailed analysis.}
	%\vspace{-4mm}
	\label{DRL}
\end{figure}


To address these, as shown in Figure \ref{framework}, we propose a Transformer-based 1D representation learning method, i.e., 1DFormer, to effectively capture the temporal and structural patterns of multiple landmarks for facial landmark tracking.  The clusters of 1D landmark representations, including the hidden features as well as the output heatmaps, are in the form of a sequence of 1D light-weight vectors, which are the ideal input and output patterns of Transformer. Thus, we build our method based on the paradigm of Transformer and develop a Transformer architecture with both temporal modeling and structural modeling techniques. 

For temporal modeling, first, we present a recurrent token mixing mechanism that integrates long-range temporal information from past frames to enhance the features of the current time step with an efficient window sliding and information delivery strategy. Second, to fit our Transformer on learning 1D representations for the temporal sequences of multiple landmarks, we design an axis-landmark-positional embedding mechanism to make the Transformer aware of the semantic distinctions among different axes, different landmarks, as well as different positions in the temporal window. Third, we design a confidence-enhanced multi-head attention mechanism, which combines the temporal correlations learned from the key-query mechanism as well as the positional confidence of each landmark as the integrated attention weights, effectively and robustly guiding the fusion of features from different time steps even under challenging imaging conditions.

For structural modeling, to make full use of the intrinsic geometry patterns of faces for landmark tracking, we design an intra-group and an inter-group structure modeling mechanisms to encode component-level as well as global-level facial structure patterns as a refinement for the 1D representations of the landmark sequence. It is expected that these mechanisms can effectively extract the structural patterns of multiple landmarks, and meanwhile, keep the clear semantic of each landmark. For that purpose, we adopt 1D convolutional layers instead of the vanilla attention layers for token communications in the spatial dimension. From our structure modeling mechanisms, the learned facial structures may help to correct the texture disturbances caused by occlusions or uneven illumination conditions, and further improve the landmark tracking performance for ``in-the-wild" scenarios.

The main contributions of this paper can be summarized as the following. First, as far as we know, we are the first to fit the prominent Transformer paradigm on learning informative 1D representations of facial landmark sequences for facial landmark tracking. Second, we propose a new Transformer architecture, namely 1DFormer, with both temporal modeling and structural modeling mechanisms, to explore the good potentials of 1D representation learning on modeling the long-term sequential as well as the geometric patterns of facial landmarks. Third, as demonstrated by experiments on the 300VW database and the TF database, the tracker based on 1DFormer achieves state-of-the-art performance for facial landmark tracking with a significant improvement of accuracy and stability performance compared to the related works. 



\section{Related Work}
\label{related_work}
%Regression methods for facial landmark detection \cite{survey} and tracking \cite{survey2} can be divided as two categories, i.e., coordinate regression methods and heatmap regression methods.

\subsection{Coordinate Regression Methods}
The coordinate regression methods for facial landmark detection \cite{ESR,Xavier,Ren,CascadedTransformers, RePFormer} and tracking \cite{SDM,TSTN,TSCN,GAN_Tracking,DBN,RBM,Qtracker}  directly
mapped facial appearances to the vectors of landmark coordinates or their increments using classic machine learning models, e.g., Supervised Descent Method (SDM) \cite{SDM}, fern regressors \cite{ESR,Xavier}, random forests \cite{Ren}, probabilistic graphical models \cite{DBN,RBM}, or deep learning models, e.g., CNN \cite{wing,TSCN,DBLP:journals/access/ColacoH22}, RNN \cite{TSTN}, DQN \cite{Qtracker}, GAN\cite{GAN_Tracking}, and Transformer \cite{CascadedTransformers, RePFormer}. These methods were useful in their contexts, however, they ignored to explicitly model the spatial distributions of facial landmarks in the representation level, and consequently,  as reported by Sun ~\emph{et al.} \cite{better_heatmap1} and Yin ~\emph{et al.} \cite{AOHR}, they showed sub-optimal spatial generalization performance on locating facial landmarks.


%Resnet \cite{wing},  an Efficient-net \cite{DBLP:journals/access/ColacoH22}, or a Transformer \cite{}.
%%After that, a coordinate regressor decodes this feature vector as the coordinate vector of facial landmarks. 
%Based on the feature vector of each individual video frame, CNN \cite{}, RNN \cite{TSTN}, or DQN \cite{} was adopted to learn sequence-level landmark representations from multiple video frames. After that, a coordinate regressor decoded  the learned representations as the vectors of landmark coordinates or their increments for each frame. 



\subsection{Heatmap Regression Methods}
\label{relate_heat}
Different from the coordinate regression methods, heatmap regression methods explicitly modeled the spatial distributions of landmarks by learning to predict the probability intensity that a landmark appears in each position of the heatmap. With a good understanding of landmark distributions, these methods achieved a better spatial generalization performance on facial landmark detection \cite{hourglass, how_far, HRnet, CascadedTransformers, RePFormer,AOHR, HybridMatch} and tracking \cite{encoder_decoder,AOHR,tai2018towards} compared to coordinate regression methods. Heatmap regression methods can be further divided into two categories.



\subsubsection{2D Heatmap Regression Methods} 
2D heatmap regression methods \cite{hourglass, encoder_decoder, how_far, HRnet, CascadedTransformers, RePFormer} modeled the joint distribution on the x and the y axis for each landmark with 2D feature maps in the hidden stages and 2D heatmaps in the output stage. These methods assumed that for each landmark, the joint distribution on the x and the y axis obeys a 2D Gaussian distribution, which was taken as the supervised signal to regress 2D heatmaps.
As a representative work, Wang \textit{et al.} \cite{HRnet} proposed a high-resolution representation learning method with abundant encoding and interaction strategies of high-resolutional 2D features and  low-resolutional 2D features for 2D heatmap regression. The 2D heatmap regression approaches were more adept at capturing the positional distributions of landmarks, and achieved better spatial generalization performance than the coordinate regression approaches. However, under limited machine space, these methods could only maintain a small number of channels for the feature maps, and also had to down-sample the feature and heatmap resolutions to a lower value than the input image, due to the high spatial complexity, i.e., $\mathcal{O}(L^2)$ where $L$ is the resolution of 2D features and heatmaps. Those compromises might lead to some loss of the precise positional information of facial landmarks and cause a high quantization error. What's more, the complex feature tensors of each frame also brought about some difficulties for sequential modeling among multiple video frames. These weaknesses restricted the performance of 2D heatmap regression for tracking landmarks.

%
%2D Gaussian distributions around the labeled position of each facial landmark were assumed as  of a landmark and used to supervise the predicted 2D heatmaps.
%
%
%2D or 1D Gaussian heatmaps.
%
%These approaches assume that a landmark coordinate obeys Gaussian distribution around its ground truth label. These approaches predict the , and transform points on the heatmaps to the predicted landmark coordinates.
%
%
%To keep aware of the local details of landmark patterns, joint representation (JR) based methods \cite{hourglass, encoder_decoder, how_far, HRnet, CascadedTransformers, RePFormer} maintained uncompressed or slightly compressed high-dimensional 2D feature maps and heatmaps to represent the joint distribution of landmarks on the x and the y axis. 



\subsubsection{1D Heatmap Regression Methods.}
\label{related_1D}
To alleviate the high-spatial complexity of 2D landmark representations, the 1D heatmap regression methods \cite{AOHR, DBLP:conf/accv/XiongZDS20,I2L, SimCC, chi2022human,DBLP:journals/remotesensing/LiuZCW22} learned 1D features and heatmaps representing the marginal distribution of every landmark on each axis as an alternative, as depicted in Figure \ref{DRL}. These methods assumed that the marginal distribution of each landmark on every axis obeys an 1D Gaussian distribution, and took this probability distribution to supervise the regression of 1D heatmaps. To produce 1D features on one axis, these methods adopted strided CNN \cite{AOHR}, MLP \cite{SimCC}, or pooling operations \cite{I2L} to compress image representations along other axes, while keeping or boosting the feature resolution along the interested axis. For a feature resolution of $L$, the 1D representations only have a spatial complexity of $\mathcal{O}(L)$, thus can achieve higher resolutions and enough channels to thoroughly model the landmark patterns in concerned with each interested axis. Therefore, these methods showed superiority on landmark localization performance compared to the coordinate regression methods and the 2D heatmap regression methods.

However, previous 1D heatmap regression methods have not dived deeply into time series modeling, which is fundamental for tracking landmarks. Most of them \cite{DBLP:conf/accv/XiongZDS20,I2L, SimCC, chi2022human,DBLP:journals/remotesensing/LiuZCW22, HybridMatch} were detection methods, which only processed static images. As far as we know, only Yin \textit{et al.} \cite{AOHR} proposed a tracker based on 1D features and heatmaps for the facial landmark tracking task. Unfortunately, it simply took the preset constants as the weights to fuse features from the historical frames, failing to dynamically adapt to the temporal correlations as well as the image qualities of different video frames. These preset weights also decayed dramatically according to an exponential function, leading to a bottleneck on the modeling capability of long-term patterns of the temporal sequence.  What's more, these methods ignored to develop an effective interaction mechanism to capture the structural patterns of multiple landmarks. These weaknesses led to the accuracy bottleneck of current 1D heatmap regression methods. To solve these, we propose a new facial landmark tracking method that dynamically captures long-term temporal patterns as well as the inherent geometries of facial landmarks to learn informative 1D representations for the landmark sequence by developing a new Transformer architecture, successfully releasing the good potentials of the 1D representation learning paradigm for the facial landmark tracking task.

\section{Methodology}
The proposed facial landmark tracker consists of an 1D feature encoder $f_{en}(\cdot)$ as well as two 1D heatmap decoders $f^x_{de}(\cdot)$ and $f^y_{de}(\cdot)$. The 
video to be extracted facial landmarks is denoted as $\textbf{I}_{1:T}=[\textbf{I}_{1};\textbf{I}_{2},...;\textbf{I}_{T}]$, where $\textbf{I}_{t}\in\mathbb{R}^{ H\times W\times 3} (1\leq t \leq T)$ is the image of the $t$ th frame. $f_{en}(\cdot)$ encodes $\textbf{I}_{1:T}$ as a cluster of features representing the marginal distributions of landmarks on the $x$ and $y$ axes for each time step:
\begin{equation}
	\label{en1}
	\textbf{F}^{x}_{1},\textbf{F}^{y}_{1},\textbf{F}^{x}_{2},\textbf{F}^{y}_{2},...,\textbf{F}^{x}_{T},\textbf{F}^{y}_{T} = f_{en}(\textbf{I}_{1:T};\theta_{en})
	%	%\vspace{-4mm}
\end{equation}
where $\theta_{en}$ is the parameters of $f_{en}(\cdot)$, $\textbf{F}^{x}_{t}=[ \textbf{f}^x_{1t};$ $\textbf{f}^x_{2t}; ...; \textbf{f}^x_{Nt}] \in\mathbb{R}^{N \times L}$ and $\textbf{F}^{y}_{t}=[ \textbf{f}^y_{1t};$ $\textbf{f}^y_{2t}; ...; \textbf{f}^y_{Nt}]\in\mathbb{R}^{N \times L}$ respectively denote the 1D representations on $x$ axis and the $y$ axis at the $t$ th time step. Here, $N$ is the number of the pre-defined landmarks, $\textbf{f}^x_{nt} (1\leq n \leq N)\in\mathbb{R}^{1 \times L}$ and $\textbf{f}^y_{nt}\in\mathbb{R}^{1 \times L}$ respectively denote the 1D features for the $n$ th facial landmark at the $t$ th time step. Based on $\textbf{F}^{x}_{t}$ and $\textbf{F}^{y}_{t}$, the heatmap decoders $f^x_{de}(\cdot)$ and  $f^y_{de}(\cdot)$ predicts two groups of 1D heatmaps \cite{AOHR,I2L}, i.e., $\textbf{h}^x_{nt}\in\mathbb{R}^{1 \times D}$ and $\textbf{h}^y_{nt}\in\mathbb{R}^{1 \times D}$, respectively.  The $x$ and $y$ coordinates of the landmark can be obtained from the peak positions of $\textbf{h}^x_{nt}$ and $\textbf{h}^y_{nt}$, respectively. 

In this paper, we propose a new design of $f_{en}(\cdot)$, which is composed of a backbone encoder extracting 1D representations from each individual frame image, as well as a Transformer, which we called as 1DFormer, to extract sequential patterns and geometric structures of facial landmarks to refine their 1D representations. The main focus of this paper lies in the proposed 1DFormer, whose basic block is the stacking of temporal modeling mechanisms which embed sequential patterns into 1D representations, and structure modeling mechanisms which take advantage of the structure information of facial components and the global shapes to refine the 1D representations of each landmark. Through our 1DFormer, the spatial and temporal patterns of facial landmarks are fully captured from $\textbf{I}_{1:T}$, and informative 1D features are provided to facial landmark localization, especially for challenging imaging conditions.

\begin{figure}
	\centering
	\includegraphics[scale=0.45]{framework_details.pdf}
	\caption{Upper part: an architectural overview of the proposed facial landmark tracking method, i.e., 1DFormer. Lower part
		: the internal architecture of a basic block of 1DFormer on the x axis. The architecture of basic block on the y axis is the same as the x axis.
	}
	\label{framework}
	%%\vspace{-2mm}
\end{figure}


%is supposed to  In our method, $f_{mr}(\cdot)$ 
\subsection{Backbone Encoder}
Before processed by 1DFormer, we first extract 1D representations for every landmark from each individual frame image through a backbone 1D representation encoder denoted as $f_{bk}(\cdot)$. Formally, we have $\textbf{s}^x_{nt},\textbf{s}^y_{nt} = f_{bk}(\textbf{I}_{t}, n;\theta_{bk})$, where $\textbf{s}^x_{nt} (1\leq n \leq N, 1\leq t \leq T)\in\mathbb{R}^{1 \times L}$ and $\textbf{s}^y_{nt}$ respectively denote the $x$-wise and $y$-wise 1D representations for the $n$ th landmark extracted from $I_t$.
Following AOHR \cite{AOHR}, $f_{bk}(\cdot)$ is composed of a FAN network \cite{how_far} cascaded with two groups of strided CNNs to respectively encode 1D representations on the two axes. It is worth noting that, AOHR took the output of strided CNNs as the final outputs of its detector and supervised them with the 1D heatmap labels, while in our method, $\textbf{s}^x_{nt}$ and $\textbf{s}^y_{nt}$ are the dense hidden states of the tracker optimized in an end-to-end manner. 

\subsection{Temporal Modeling Mechanisms of 1DFormer}
\label{temporal}
\subsubsection{Recurrent Token Mixing Mechanism}
Two temporal modeling modules, denoted as $f^x_{te}(\cdot)$ and $f^y_{te}(\cdot)$, are presented to embed the dynamic patterns of the landmark sequence into $\textbf{s}^x_{nt} (1\leq n \leq N, 1 \leq t \leq T)$ and $\textbf{s}^y_{nt}$, respectively, by integrating the 1D landmark representations from different time steps. The structures of $f^x_{te}(\cdot)$ and $f^y_{te}(\cdot)$ are the same as each other and will be discussed in Section \ref{conf}. Here we firstly present a recurrent token mixing mechanism for $f^x_{te}(\cdot)$ and $f^y_{te}(\cdot)$ to capture the long-range sequential patterns of facial landmarks from the video stream, as shown in Eq. \eqref{transxy}:
\begin{equation}
	\begin{split}
		\label{transxy}
		& \textbf{s}^{x'}_{nt} = f^x_{te}(\textbf{s}^{x}_{nt}, \textbf{s}^{x'}_{n(t-1)},...,\textbf{s}^{x'}_{n(t-W+1)})\\
		& \textbf{s}^{y'}_{nt} = f^y_{te}(\textbf{s}^{y}_{nt}, \textbf{s}^{y'}_{n(t-1)},...,\textbf{s}^{y'}_{n(t-W+1)})\\
	\end{split}
\end{equation}
where $\textbf{s}^{x'}_{nt}$ and $\textbf{s}^{y'}_{nt} (1\leq n \leq N, 1 \leq t \leq T)$ are the features integrated with temporal information of the video through mixing temporal tokens, i.e., $\textbf{s}^{x}_{nt}, \textbf{s}^{x'}_{n(t-1)},...,$ $\textbf{s}^{x'}_{n(t-W+1)}$ or $\textbf{s}^{y}_{nt}, \textbf{s}^{y'}_{n(t-1)},...,$ $\textbf{s}^{y'}_{n(t-W+1)}$. First, as a computationally acceptable way, instead of taking the features from all of the past frames as the input, we just send the features from a temporal neighborhood window of $W$ frames into $f^x_{te}(\cdot)$ and $f^y_{te}(\cdot)$ to refine the 1D features of landmarks with sequential patterns. When the chunk of $W$ frames is processed, $f^x_{te}(\cdot)$ and  $f^y_{te}(\cdot)$ slide to process the next chunk with one temporal stride. Second, to model the dynamic patterns of facial landmarks not only within the window of $W$ frames but also from a wide time scale, we deliver the output features of $f^x_{te}(\cdot)$ and $f^y_{te}(\cdot)$ on the $t-1$ th, $t-2$ th, ..., and the $t-W+1$ th frame as the input for calculating $\textbf{s}^{x'}_{nt}$ and $\textbf{s}^{y'}_{nt}$. Since the memories of landmark patterns are delivered with the sliding of the window from the past windows to the current window, $\textbf{s}^{x'}_{nt}$ and $\textbf{s}^{y'}_{nt}$ can still keep a long memory of the landmark dynamics.

\subsubsection{Axis-Landmark-Positional Embedding Mechanism}
To fit the Transformer architecture on learning 1D representations of the temporal sequences of multiple landmarks, we extend the conventional positional embedding mechanism \cite{transformer} of Transformer as axis-landmark-positional (alp) embeddings which distinguish not only different position indexes but also different axes and landmarks, to make the Transformer aware of the discrepancy of marginal distributions on different axes, the semantic distinctions among different facial landmarks, as well as the relative position of each feature vector from a window. These embeddings are denoted as $\textbf{e}^{x}_{nw}$ and $\textbf{e}^{y}_{nw} (1 \leq n \leq N, 1 \leq w \leq W)$ for $f^x_{te}(\cdot)$ and $f^y_{te}(\cdot)$, respectively, where the superscript ($x$ or $y$) denotes the axis the 1D feature belongs to, subscript $n$ is the landmark index, $w$ is the relative position index of an input feature to the whole input window with $W$ frames.  The input feature sequences for $f^x_{te}(\cdot)$ and $f^y_{te}(\cdot)$ are added with the axis-landmark-positional embeddings: $\textbf{s}^{x}_{nt}:=\textbf{s}^{x}_{nt}+\textbf{e}^{x}_{n1}$, $\textbf{s}^{x'}_{n(t-w+1))}:=\textbf{s}^{x'}_{n(t-w+1)}+\textbf{e}^{x}_{nw} (1< w \leq W)$, $\textbf{s}^{y}_{nt}:=\textbf{s}^{y}_{nt}+\textbf{e}^{y}_{n1}$, $\textbf{s}^{y'}_{n(t-w+1))}:=\textbf{s}^{y'}_{n(t-w+1)}+\textbf{e}^{y}_{nw} (1< w \leq W)$. Here $\textbf{e}^{x}_{nw}$ and $\textbf{e}^{y}_{nw}$ are randomly initialized, then optimized with the tracker.
%Through Eq. \eqref{transxy}, 
%Overall, only if $t\leq W$ when no temporal information is needed to deliver from the past, Eq. \eqref{transxy1} is adopted, for most cases, i.e.,  $t \textgreater W$, Eq. \eqref{transxy} is adopted.

%In the original Transformer , the input feature sequences are added with the positional encodings  marking the relative position index  of the current feature vector to the whole input sequence before fed into the multi-head attention module. To make our method aware of the semantic distinctions among different facial landmarks as well as the discrepancy of marginal distributions on different axes,

\subsubsection{Confidence-Enhanced Multi-Head Attention Mechanism}
\label{conf}
As the core of $f^x_{te}(\cdot)$ and $f^y_{te}(\cdot)$, a multi-head attention mechanism is adopted to dynamically determine the attention weights of features from different time steps according to their correlations. The attention weights can be calculated by the attentive scaled dot-product operations between the query and the key matrices:
\begin{equation}
	\label{eq:bs1}
	\begin{aligned}
		\textbf{q}^{xh}_{nt} & = \textbf{s}^x_{nt}{\textbf{W}^{xh}_{q}}^T,\,\,\,	\textbf{q}^{yh}_{nt} = \textbf{s}^y_{nt}{\textbf{W}^{yh}_{q}}^T \\
		\textbf{K}^{xh}_{nt} & = \textbf{S}^x_{nt}{\textbf{W}^{xh}_{k}}^T,\,\,\,
		\textbf{K}^{yh}_{nt} = \textbf{S}^y_{nt}{\textbf{W}^{yh}_{k}}^T \\
		\textbf{a}^{xh}_{nt} & = sf(\frac{\textbf{q}^{xh}_{nt} {\textbf{K}^{xh}_{nt}}^T}{\sqrt{d_h}}),\,\,\,\textbf{a}^{yh}_{nt} = sf(\frac{\textbf{q}^{yh}_{nt} {\textbf{K}^{yh}_{nt}}^T}{\sqrt{d_h}})\\
		% &= softmax(\frac{\textbf{S}^x_{n}{\textbf{W}^x_q}^T {\textbf{W}^x_k}{\textbf{S}^x_{n}}^T}{\sqrt{d_k}})
	\end{aligned}
\end{equation}
where $sf(\cdot)$ denotes the $Softmax$ function; $\textbf{S}^{x}_{nt} = [\textbf{s}^{x}_{nt}; \textbf{s}^{x'}_{n(t-1)};...;$ $\textbf{s}^{x'}_{n(t-W+1)}]$ and $\textbf{S}^{y}_{nt} \in \mathbb{R}^{W \times L}$ denote the feature sequence for the $t$ th input window chunk of the attention module; $h$ denotes the $h$ th head of the multi-head attention mechanism; $\textbf{q}^{xh}_{nt}$ and $\textbf{q}^{xh}_{nt}$ are the $h (1\leq h \leq H)$ th query vector pair to get the attention weight vectors, i.e., $\textbf{a}^{xh}_{nt} \in \mathbb{R}^{1 \times W}$ and $\textbf{a}^{yh}_{nt} \in \mathbb{R}^{1 \times W}$, for the $t$ the time step; $\textbf{K}^{xh}_{n}$ and $\textbf{K}^{yh}_{n} \in \mathbb{R}^{W \times d_h}$ are the $h$ th key matrix pair; $\textbf{W}^{xh}_{q}$, $\textbf{W}^{yh}_{q} $, $\textbf{W}^{xh}_{k}$, and $\textbf{W}^{yh}_{k} \in \mathbb{R}^{d_h \times L}$ are the projection matrices. Through the optimization of the attention mechanism, $\textbf{a}^{xh}_{nt}$ and $\textbf{a}^{yh}_{nt}$ can implicitly capture the correlations among features of the temporal sequence: the intensity of the $w (1 \leq w \leq W)$ th element of $\textbf{a}^{xh}_{nt}$ may reflect the correlation between $\textbf{s}^{x}_{nt}$ and $\textbf{s}^{x'}_{n(t-w+1)}$. 

To avoid low-quality texture features caused by occlusions or extreme head poses propagating misleading information through the attention mechanism, we propose two confidence prediction branches, i.e.,  $f^x_{c}(\cdot)$ and $f^y_{c}(\cdot)$, which consist of a linear projection operation followed by a Sigmoid function, to enhance affinity decision. Here $c^x_{nt} = f^x_{c}(\textbf{s}^x_{nt})$ and $c^y_{nt} = f^y_{c}(\textbf{s}^y_{nt})$ are the predicted confidence for the positional information provided by
$\textbf{s}^x_{nt}$ and $\textbf{s}^y_{nt}$, respectively. Note that we take $\textbf{s}^x_{nt}$ and $\textbf{s}^y_{nt}$ instead of $\textbf{s}^{x'}_{nt}$ and $\textbf{s}^{y'}_{nt}$ as the input for the confidence predictors to objectively reflect the image quality of each individual video frame. It is expected that, if the texture features around the $n$ th landmark at the $t$ th frame are of good quality with a clear positional discrimination for the landmark, $c^x_{nt}$ and $c^y_{nt}$ may be assigned as a high value and the effects of $\textbf{s}^x_{nt}$ and $\textbf{s}^y_{nt}$ on temporal token communications will be encouraged; otherwise, if the image textures of the $n$ th landmark are polluted at the current time step by occlusions, $c^x_{nt}$ and $c^y_{nt}$ may be low, and $\textbf{s}^x_{nt}$ as well as $\textbf{s}^y_{nt}$ will be mitigated to alleviate the effect of feature disturbances. 

We combine the attention weights predicted by the key-query mechanism with the landmark confidences, i.e., $\textbf{c}^x_{nt} = [c^x_{nt}, c^x_{n(t-1)}, $ $...,  c^x_{n(t-W+1)}]$ and $\textbf{c}^y_{nt} \in \mathbb{R}^{1\times W}$, as the integrated affinity vectors, which can model the temporal correlations among a landmark sequence and maintain strong robustness to texture disturbances, guaranteeing the quality of the learned sequential representations of facial landmarks. The integrated affinity vectors are calculated as Eq. \eqref{eq:bs2}:
\begin{equation}
	\label{eq:bs2}
	\begin{aligned}
		\textbf{a}^{xh}_{nt} & = sf(\frac{\textbf{q}^{xh}_{nt} {\textbf{K}^{xh}_{nt}}^T}{\sqrt{d_h}}\odot\textbf{c}^{xT}_{nt}),\,\,\,\textbf{a}^{yh}_{nt} = sf(\frac{\textbf{q}^{yh}_{nt} {\textbf{K}^{yh}_{nt}}^T}{\sqrt{d_h}}\odot\textbf{c}^{yT}_{nt})
		% &= softmax(\frac{\textbf{S}^x_{n}{\textbf{W}^x_q}^T {\textbf{W}^x_k}{\textbf{S}^x_{n}}^T}{\sqrt{d_k}})
	\end{aligned}
\end{equation}
Based on $\textbf{a}^{xh}_{nt}$ and $\textbf{a}^{yh}_{nt}$, the features from different time steps are fused as the temporally refined 1D landmark representations, i.e., 
$\textbf{s}^{x'}_{nt}$ and $\textbf{s}^{y'}_{nt}$, for the $n$ landmark at the $t$ th time step, as shown in the following equation:

\begin{equation}
	\label{eq:bs3}
	\centering
	\begin{aligned}
		\textbf{V}_{nt}^{xh} &= \textbf{S}_{nt}^{x}{\textbf{W}_v^{xh}}^T, \,\,\, \textbf{V}_{nt}^{yh} = \textbf{S}_{nt}^y{\textbf{W}_v^{yh}}^T\\
		\textbf{s}_{nt}^{xh'} &= \textbf{a}_{nt}^{xh} \textbf{V}_{nt}^{xh}, \,\,\, 
		\textbf{s}_{nt}^{yh'} = \textbf{a}_{nt}^{yh} \textbf{V}_{nt}^{yh} \\
		\textbf{o}_{nt}^{x} &= \textrm{Concat}(\textbf{s}_{nt}^{x1'}  , \textbf{s}_{nt}^{x2'}  ,..., \textbf{s}_{nt}^{xH'}){W_{o}^x}^T\\
		\textbf{o}_{nt}^{y} &= \textrm{Concat}(\textbf{s}_{nt}^{y1'}, \textbf{s}_{nt}^{y2'}  ,..., \textbf{s}_{nt}^{yH'}){W_{o}^y}^T\\
		\textbf{o}_{nt}^{x'} &= \textrm{LN}(\textbf{o}_{nt}^{x} + \textbf{s}_{nt}^{x}) , \,\,\, \textbf{o}_{nt}^{y'} = LN(\textbf{o}_{nt}^{y} +\textbf{s}_{nt}^{y})\\
		\textbf{s}_{nt}^{x'} &= \textrm{LN}(\textrm{FFN}(\textbf{o}_{nt}^{x'}) + \textbf{o}_{nt}^{x'}) , \,\,\,
		\textbf{s}_{nt}^{y'} = \textrm{LN}(\textrm{FFN}(\textbf{o}_{nt}^{y'}) + \textbf{o}_{nt}^{y'})
	\end{aligned}
\end{equation} 
where $\textbf{W}^{xh}_{v} $ and $\textbf{W}^{yh}_{v} \in \mathbb{R}^{d_h \times L}$, as well as $\textbf{W}^{x}_{o} $ and $\textbf{W}^{y}_{o} \in \mathbb{R}^{d \times L}$ are the projection matrices; $\textbf{V}^{xh}_{n}$ and $\textbf{V}^{yh}_{n} \in \mathbb{R}^{W \times d_h}$ are the value matrices; LN is the layer normalization operation; FFN denotes a two-layer feed forward network.
\subsection{Structural Modeling Mechanisms of 1DFormer}
\label{structure}
We embed facial structural patterns to enhance the 1D representation of each landmark, as shown in the right-most part of Figure \ref{framework}. According to the inherent structure of a human face, its landmarks can be divided into seven groups, i.e., left eyebrow, right eyebrow, left eye, right eye, nose, mouth, and contour. 
%Considering landmarks of the same group are highly correlated in their positions since they constitute the intrinsic shape of a facial component, 
To encode component-level structural patterns as a refinement for marginal landmark representations, we propose an intra-group structural modeling mechanism. Specifically, at the $t (1\leq t \leq T)$ th time step, we stack the features outputted by $f^x_{te}(\cdot)$ and $f^y_{te}(\cdot)$ for landmarks of the $k  (1 \leq k \leq 7)$ th group as $\textbf{S}^{x'}_{kt}=[\textbf{s}^{x'}_{k(1)t};\textbf{s}^{x'}_{k(2)t};..;\textbf{s}^{x'}_{k(N_k)t}] \in \mathbb{R}^{N_k \times L}$ and $\textbf{S}^{y'}_{kt}=[\textbf{s}^{y'}_{k(1)t};\textbf{s}^{y'}_{k(2)t};...;$ $\textbf{s}^{y'}_{k(N_k)t}]$, respectively, where $N_k$ is the total number of landmarks of the $k$ th group, $\textbf{s}^{x'}_{k(j)t} (1 \leq j \leq N_k)$ and $\textbf{s}^{y'}_{k(j)t}$  denote the 1D representations produced by Eq. \eqref{transxy} for the $j$ th landmark of the $k$ th group. Here we replace the subscript $n (1\leq n \leq N)$ of $\textbf{s}^{x'}_{nt}$  and $\textbf{s}^{y'}_{nt}$ in Eq. \eqref{transxy} with the subscript $k(j)$ just to mark the switch between the universal index of all landmarks to the relative index within a landmark group. We integrate the component-level structural information into the features of the $k$ th landmark group via two residual 1D CNN networks, i.e., $f^{xk}_{ir}(\cdot)$ and $f^{yk}_{ir}(\cdot)$, as shown in Eq. \eqref{ps_sffm_1}:
\begin{equation}
	\label{ps_sffm_1}
	\textbf{P}^{x}_{kt} = LN( f^{xk}_{ir}(\textbf{S}^{x'}_{kt})+\textbf{S}^{x'}_{kt}),\,\,\, \textbf{P}^{y}_{kt} = LN(f^{yk}_{ir}(\textbf{S}^{y'}_{kt})+\textbf{S}^{y'}_{kt}), \,\,\, 1 \leq k \leq 7
\end{equation}
where LN denotes the layer normalization operation \cite{transformer}; the superscript $k$ of $f^{xk}_{ir}(\cdot)$ and $f^{xk}_{ir}(\cdot)$ means that we apply independent 1D CNN parameters for different facial groups; the strides of $f^{xk}_{ir}(\cdot)$ and $f^{xk}_{ir}(\cdot)$ are set as $1$ to guarantee that the shape of  $\textbf{P}^{x}_{kt}$ and $\textbf{P}^{y}_{kt}$ are the same as $\textbf{S}^{x'}_{kt}$ and $\textbf{S}^{y'}_{kt}$; the rows of $\textbf{S}^{x'}_{kt}$ and $\textbf{S}^{y'}_{kt}$ are taken as the channel dimensions for convolutional operations, and the convolutional filters are move along the columns of $\textbf{S}^{x'}_{kt}$ and $\textbf{S}^{y'}_{kt}$. The $j$ th row vectors of $\textbf{P}^{x}_{kt}$ and $\textbf{P}^{y}_{kt}$, denoted $\textbf{p}^{x}_{k(j)t}$ and $\textbf{p}^{y}_{k(j)t}$ respectively, are the features for the $j$ th landmark of the $k$ th group refined by component-level structural patterns.

To embed global facial structural patterns as a refinement for 1D landmark representations,
%Besides the strong structural dependencies of landmarks within a facial component, different facial components are also subject to some global spatial relationships, e.g., their intrinsic relative positions and sizes. Thus, it is also reasonable to consider global structure modeling for facial landmarks from different components.
For that purpose, we adopt an inter-group global structure modeling mechanism based on the output features of the intra-group structure modeling mechanism. Let $\textbf{P}^{x}_{t}=\textbf{P}^{x}_{1t} \textcircled{c}\textbf{P}^{x}_{2t}\textcircled{c}...\textbf{P}^{x}_{7t}$, and $\textbf{P}^{y}_{t}=\textbf{P}^{y}_{1t} \textcircled{c}\textbf{P}^{y}_{2t}\textcircled{c}...\textbf{P}^{y}_{7t}$, where $\textcircled{c}$ denotes the concatenation operation along the first dimension. We introduce two residual 1D CNNs, i.e., $f^{x}_{it}(\cdot)$ and $f^{y}_{it}(\cdot)$, to embed global facial structures as a refinement of the 1D representations for each landmark, as shown in Eq. \eqref{ps_sffm_2}:
%%%\vspace{-1.5mm}
\begin{equation}
	\label{ps_sffm_2}
	\textbf{F}^{x}_{t} = LN(f^{x}_{it}(\textbf{P}^{x}_{t})+\textbf{P}^{x}_{t}),\,\,\,\textbf{F}^{y}_{t} = LN(f^{y}_{it}(\textbf{P}^{y}_{t})+\textbf{P}^{y}_{t})
\end{equation}
where the rows of $\textbf{P}^{x}_{t}$ and $\textbf{P}^{y}_{t}$ are taken as the channel dimensions for the convolutional operations, and the convolutional filters are moving along the columns of $\textbf{P}^{x}_{t}$ and $\textbf{P}^{y}_{t}$.  The $n$ th row vectors of $\textbf{F}^{x}_{kt}$ and $\textbf{F}^{y}_{kt}$, denoted $\textbf{f}^{x}_{nt}$ and $\textbf{f}^{y}_{nt}$ respectively, are the features for the $n$ th landmark refined by global structural patterns. From the proposed structure modeling modules, facial geometric patterns are captured through feature communications among different facial landmarks, and the learned facial structures may help to correct the disturbances on facial appearances caused by occlusions or uneven illumination conditions, and further improve the landmark tracking performance for ``in-the-wild" scenarios.

It is worth noting that, we take 1D convolutional layers instead of attention layers for token communications in the spatial dimension. The reason lies in that, unlike the temporal tokens which are the features of one certain facial landmark from different time steps, the spatial tokens are features from different landmarks which constitute a structure,  and we find that the 1D convolutional layers show a better capability on modeling the general structural patterns among different landmarks while keeping their clear semantic distinguishments. See Section \ref{tcs} for an experimental discussion.



\subsection{1D Heatmap Decoding}
The 1DFormer's output features, i.e., $\textbf{f}^{x}_{nt}$ and $\textbf{f}^{y}_{nt}(1\leq n \leq N, 1\leq t \leq T)$, are fed into the decoders $f^x_{de}(\cdot)$ and $f^y_{de}(\cdot)$,  i.e., two MLP networks, to predict the 1D heatmaps respectively, i.e., $\textbf{h}^{x}_{nt}$ and $\textbf{h}^{y}_{nt}$, for the $n$ th landmark at the $t$ th time step. The landmark coordinates $(x_{nt}, y_{nt})$ can be obtained from the maximum activation position of the 1D heatmaps \cite{AOHR,I2L}.

\subsection{Overall Loss Function}
\label{sec_alg}
Our method is trained with the following loss fucntions:
\begin{small}
	\begin{equation}
		\label{all}
		\begin{split}
			L_o =&  \lambda_h L_h +\lambda_c L_c,  \\
			L_h=&\sum_{t=1}^T\sum_{n=1}^N(||\textbf{h}^x_{nt}-\textbf{h}^{x*}_{nt}||_2^2+||\textbf{h}^y_{nt}-\textbf{h}^{y*}_{nt}||_2^2)\\ 
			L_c =& \sum_{t=1}^T\sum_{n=1}^N((||c^x_{nt} - c^{x*}_{nt}||_2^2)+(||c^y_{nt} - c^{y*}_{nt}||_2^2)) \\
		\end{split}
	\end{equation}
\end{small}
where $\lambda_h$ and $\lambda_c$ are the hyper-parameters determining the weights of each loss in the overall loss function; $L_h$ is the training loss for 1D heatmap regression; $\textbf{h}^{x*}_{nt}$ and $\textbf{h}^{y*}_{nt}$ are the heatmap labels, which are 1D Gaussian distributions around the ground truth landmark positions; $L_c$ is the training loss for confidence regression; $c^{x*}_{nt}$ and $c^{y*}_{nt}$ are the confidence labels. 

Due to the difficulty of manual annotations for landmark confidences, current benchmark databases for facial landmark tracking do not provide confidence labels.  As an alternative, we have to infer confidence labels through a flow-based model \cite{RLE}, and take its inference results as the pseudo labels of confidences. For a better generalization, we just take these pseudo labels as early guidance of our confidence branches, then fine-tune the confidence branches through the supervised signals from heatmap regression in an end-to-end manner. Formally, denote the total training epochs as $E$, when $1\leq e \leq E/2$ we train the tracker with $L_o$; when $E/2 \le e \leq E$ we train the tracker with $L_h$.

\section{Experiments}
In this Section, we first describe the experimental conditions (\ref{ec}). After that, we give the results and analyses of the ablation studies (\ref{as}) as well as the empirical study on the length of temporal window (\ref{esl}). Then, we make empirical study on  different token communication strategies (\ref{tcs}). Then, we make a discussion on the computational complexity of our method (\ref{com_comp}). Then we compare our method with state-of-the-art works (\ref{cp}) by quantitative metrics and visualizations.


\subsection{Experimental Conditions}
\label{ec}
Two video databases, i.e., the 300VW \cite{shen2015first} database and the TF \cite{FGNET} database, are used to evaluate performance of the proposed method. The 300VW database, i.e., the most widely-used database in the field of facial landmark tracking, contains  $114$ ``in-the-wild" videos, each frame is annotated with $68$ facial landmarks. $50$ videos are divided as the training set and $64$ videos for testing. The testing set can be further categorized as three subsets, i.e., S1, S2  and S3, according to their challenging levels.  

To be consistent with the experimental conditions of previous works \cite{TSTN, AOHR}, for experiments on the 300VW database, both the 300VW training set and the 300W \cite{300w} training set is used for training. The 300W database is an image database with 3,148  training images, without any temporal information. We just make multiple copies of an image to be a pseudo static video to pre-train our tracker, then fine-tuning it on the 300VW training set. We adopt all of the $68$ landmarks when testing on the 300VW database, and adopt $7$ landmarks common to the 300VW and the TF databases for testing on the TF database.  All faces are cropped from the facial bounding boxes and resized to $256\times256$ pixels, then fed into the network for training. The training batch size is 10.

%Considering jittering of detected bounding boxes among adjacent frames may be harmful to the tracking performance, we track facial landmarks with two steps. First, we predict landmarks based on the detected bounding box. Second, we extend a more stable bounding box from the left, right, top and bottom bounds of the landmarks predicted in the first step, then use the new bounding box to re-locate the landmarks as the final prediction.

We evaluate the tracking performance on both accuracy and stability. Accuracy reflects the closeness of the predicted landmark coordinates to the ground truths. We use \textit{N}ormalized \textit{R}oot \textit{M}ean \textit{S}quared \textit{E}rror (NRMSE) \cite{300w} as the accuracy metric. A lower value of NRMSE corresponds to a better accuracy performance.
Stability reflects the consistency of movement between predicted landmarks and ground truths. The stability error \cite{tai2018towards} is defined as the error of landmark displacement between the tracking results and the ground truths. A lower value of the stability error corresponds to a better stability performance.

To determine the values of all hyper-parameters, i.e.,  the resolution ($L$) of the 1D features in the hidden stage, the resolution ($D$) of output heatmaps, the input window size ($W$) of Transformers, the block number ($M$) of 1Dformer, the head number ($A$) of attention mechanisms, training epochs ($E$), as well as the training weights ($\lambda_c$ and $\lambda_h$) of loss functions. We first split the $50$ videos of the 300VW training set as $10$ folds, and conduct cross-validation for parameter search. To offset the randomness of the experiment, we repeat the $10$-folds cross-validation for $20$ times independently. The search ranges are: $L,D \in \{128, 256, 512, 768, 1024\}$; $W \in \{2, 4, 6, 8, 10, 12,14,16\}$; $M, A \in \{1, 2, 3, 4, 5\}$; $\lambda_c, \lambda_h \in \{0.1, 0.2, ...,$ $ 0.9\}$; $50\leq E \leq 150$. For $E$, $\lambda_c$ and $\lambda_h$, we assign them the average optimal values from the $10$-folds cross-validation; while for $L$, $D$, $W$, $M$, and $A$, considering both the tracking accuracy and the inference efficiency, we assign them the minimum values 
that the tracking accuracy shows no statistically significant boost (whether $p<0.05$) by value increments. The searched values for these parameters are: $L^{*}=256$, $D^*=768$, $W^*=10$, $M^*=2$, $A^*=4$, $E^*=64$, $\lambda^*_h=0.9$, $\lambda^*_c=0.1$. After cross-validation, we adopt all of the hyper-parameters as their searched optimal values and re-train the proposed method on the whole training set. 

\begin{table}[h]
	\centering
	\renewcommand\tabcolsep{1.8pt}
	\begin{tabular}{*{9}{c}}
		\toprule
		\multirow{2}{*}{Settings} &\multicolumn{2}{c}{300VW S1}&\multicolumn{2}{c}{300VW S2}&\multicolumn{2}{c}{300VW S3}
		&\multicolumn{2}{c}{TF} \\ \cline{2-9}
		& N & S &N &S & N & S & N & S \\
		\midrule
		$BL$&  3.31  &  0.90  &   3.45 &  0.93 &   4.42  &  1.85  &    2.02  &  0.66 \\ 
		\midrule
		%\midrule
		$BL$+$TE^{-r}$& 3.06   & 0.80 & 3.01  & 0.76 &  4.10  &  1.61 & 1.98   & 0.57 \\ 
		$BL$+$TE^{-e}$& 3.03   & 0.78  &  2.98  & 0.74 &  4.08  & 1.60  &  1.97  & 0.55 \\  
		$BL$+$TE^{-a}$& 3.26   &  0.87 &  3.40   &  0.91 &   4.39  &  1.84 &   2.02  & 0.65  \\  
		$BL$+$TE^{-c}$&  3.10  & 0.80 & 3.03  & 0.77 & 4.14   & 1.65 & 2.00  &  0.61 \\ 
		$BL$+$TE$& 2.91   & 0.74  &  2.93  & 0.73  &  3.98  & 1.52  &  1.92   & 0.47 \\  
		\midrule
		%\midrule
		$BL$+$IT$&  3.13  & 0.84  & 3.06  & 0.79 & 4.20   &  1.70 &  2.00 & 0.62 \\ 
		$BL$+$IR$&  3.16   & 0.85  & 3.12  &  0.83 & 4.25   & 1.73  &  2.01  & 0.64 \\ 
		$BL$+$IR$+$IT$&  3.09  & 0.82  & 2.99  &  0.76 &  4.12  & 1.64 &  1.98   & 0.58 \\ 
		%\midrule
		\midrule
		$BL$+$TE$+$IR$&  2.82  & 0.72  &  2.85 & 0.70 &  3.93  & 1.47  &  1.90   & 0.41  \\ 
		$BL$+$TE$+$IT$&  2.80  &  0.72 &  2.81 & 0.69 &  3.88  & 1.44  & 1.88  &  0.38 \\ 
		$BL$+$TE$+$IR$+$IT$ & \textbf{2.74} & \textbf{0.70} & \textbf{2.74}  & \textbf{0.67} & \textbf{3.80}  & \textbf{1.42}  &  \textbf{1.86}  & \textbf{0.35} \\
		\bottomrule
	\end{tabular}
	\caption{NRMSE (N$/\%$) and stability error (S$/\%$)  of each experimental group.}

	\label{abla}
\end{table}

\subsection{Abalation Study}
\label{as}
We conduct multiple groups of experiments for ablation studies. The NRMSE and the stability errors of these experiments are recorded in Table \ref{abla}. The experimental settings are as the following:  $BL$ denotes the baseline method, which only preserves the backbone 1D representation encoder $f_{bk}(\cdot)$ as well as the 1D heatmap decoders, i.e.,  $f^x_{de}(\cdot)$ and $f^y_{de}(\cdot)$. This makes a simple facial landmark detection method without any temporal modeling and structure modeling modules; $TE$ denotes a complete preservation of the temporal modeling mechanisms of 1DFormer; $TE^{-r}$, $TE^{-e}$, $TE^{-a}$, and $TE^{-c}$ are four ablation settings from $TE$: the meaning of the superscript $-r$ is to remove the recurrent token mixing mechanism and simply takes the output features of $f_{bk}(\cdot)$ from one temporal window as the inputs for the temporal modeling modules, instead of delivering long-term historical information recurrently as Eq. \eqref{transxy} does; the meaning of the  superscript $-e$ is to replace the axis-landmark-positional embeddings with the conventional positional embeddings;
the meaning of the superscript $-a$ is to replace the attention mechanism with a simple token mixing communication strategy, i.e., directly adding the feature from each time step with the average feature of the time window; the meaning of the  superscript $-c$ is to keep the attention mechanism but remove its confidence branch in both training and testing phase. $IR$ and $IT$ respectively denote the proposed intra-group and inter-group structure modeling mechanisms. Here, $BL$+$TE$+$IR$+$IT$ corresponds to the full method of our work.

\begin{figure}
	\centering
	\includegraphics[scale=1.2]{visualization_4.pdf}
	\caption{Visualization of the tracking results on a challenging video clip from the 300VW S3. The \textcolor[RGB]{192,0,0}{red points} and \textcolor[RGB]{0,192,0}{green points} are respectively the tracking results without / with the recurrent token mixing mechanism.}
	\label{vis_4}
	%%\vspace{-3mm}
\end{figure}
\begin{figure}
	\centering
	\includegraphics[scale=0.8]{binary_source_affinity_weights_figure.pdf}
	\caption{Visualization of the attention weights as well as the tracked results for an occluded landmark, i.e., the left corner of outer-ocular, from a challenging movie clip. The \textcolor[RGB]{192,0,0}{red point} and \textcolor[RGB]{0,192,0}{green point} are respectively the tracking results without / with the help of the confidence branch.}
	%		 Here we could find that the confidence branch in $TE$  helps to assign plausible attention weights according to the texture quality around the interested landmark in each time step and bringing a better tracking result.}
\label{vis_3}
\end{figure}
\begin{figure}[h]
\centering
\includegraphics[scale=0.9]{visualization_3.pdf}
\caption{Visualization of the tracking results on two challenging video frames, the former from a movie while the latter from the 30VW S3. The \textcolor[RGB]{192,0,0}{red points} and \textcolor[RGB]{0,192,0}{green points} are respectively the results of the tracker without / with structural modeling mechanisms.}
\label{vis_2}
\end{figure}


%\begin{figure*}[h]
%	\centering
%	\includegraphics[scale=0.56]{visualization_2.pdf}
%	\caption{Visiualization of tracking results on some challenging video frames.}
%	\label{vis_1}
%\end{figure*}
\begin{figure}[h]
\centering	
\begin{minipage}{0.48\linewidth}
	\centering
	\includegraphics[scale=0.7]{w_s1.pdf}
	%\subcaption{}
	%\label{300VW_S1}
\end{minipage}%
\begin{minipage}{0.48\linewidth}
	\centering
	\includegraphics[scale=0.7]{stability_s1.pdf}
	%\subcaption{}
	%\label{300VW_S1}
\end{minipage}%

\begin{minipage}{0.48\linewidth}
	\centering
	\includegraphics[scale=0.7]{w_s2.pdf}
	%\subcaption{}
	%\label{300VW_S2}
\end{minipage}
\begin{minipage}{0.48\linewidth}
	\centering
	\includegraphics[scale=0.7]{stability_s2.pdf}
	%\subcaption{}
	%\label{300VW_S2}
\end{minipage}

\begin{minipage}{0.48\linewidth}
	\centering
	\includegraphics[scale=0.7]{w_s3.pdf}
	%\subcaption{}
	%\label{300VW_S3}
\end{minipage}
\begin{minipage}{0.48\linewidth}
	\centering
	\includegraphics[scale=0.7]{stability_s3.pdf}
	%\subcaption{}
	%\label{300VW_S3}
\end{minipage}

\begin{minipage}{0.48\linewidth}
	\centering
	\includegraphics[scale=0.7]{w_tf.pdf}
	%\subcaption{}
	%\label{TF}
\end{minipage}
\begin{minipage}{0.48\linewidth}
	\centering
	\includegraphics[scale=0.7]{stability_tf.pdf}
	%\subcaption{}
	%\label{TF}
\end{minipage}
\caption{NRMSE (\%) and stability errors (\%) on 300VW S1, S2, S3, and the TF database with different length ($W$) of the temporal window.}
%\vspace{-1mm} 		
\label{fig:w}
\end{figure}
%%%\vspace{-2.5mm}

From Table \ref{abla}, we have the following observations and analyses:

First, comparing between the results of $BL$+ $TE$ and $BL$, we find that the temporal modeling mechanism $TE$ can significantly improve the accuracy and stability performance for facial landmark tracking. This results demonstrate that the proposed temporal modeling mechanisms can effectively integrate the complementary information among multiple video frames to enhance the tracking performance under ``in-the-wild" scenarios. 

Second, we find the recurrent token mixing mechanism contributes to the overall tracking performance by the result that  $BL+TE$ outperforms $BL+TE^{-r}$. To further validate the effectiveness of this mechanism, we illustrate the tracking results of $BL+TE$ (green points) and $BL+TE^{-r}$ (red points) on a challenging video from 300VW S3 in Figure \ref{vis_4}. From Figure \ref{vis_4}, we can find that, without the recurrent token mixing mechanism, the tracking performance at the $t$ th frame is possible to be very poor if all of the frames in the temporal window, i.e., from $t-9$ to $t$ for a window length of $10$, suffer from severe occlusions; in contrast, if we add this mechanism, we can deliver informative features out of the current window. In this case, information delivered from historical frames, e.g., the $t-12$ th frame without heavy occlusions, may help to improve the robustness of the tracking performance. 

Third, $BL+TE$ is better than $BL+TE^{-e}$ on the tracking performance, demonstrating the necessity of making distinction between different axes as well as different landmark semantics in the embeddings. 

Fourth, the tracking performance of $BL+TE$ shows significant superiority than $BL+TE^{-a}$ and $BL+TE^{-c}$, respectively demonstrating the effectiveness of the attention mechanism and the contribution of the confidence branch to the attention mechanism. To further validate the effectiveness of the confidence branch, in Figure \ref{vis_3}, we visualize the attention weights given by $BL+TE$ and $BL+TE^{-c}$  for an occluded facial landmark, namely the left corner of outer-ocular in this case, from a challenging video clip of 300VW S3. We can find that the confidence branch helps the attention mechanism to assign high attention weights if the landmark is visible and assign low weights if it is occluded, providing a plausible guidance to fuse features from different time steps, and thus promoting the localization performance for the occluded frames. 

Fifth, we find that both of the intra-group ($IR$) and inter-group ($IE$) structural modeling mechanisms promote the tracking performance. The reason lies that, both of them enhance the 1D representation of landmarks by communicating features among different facial landmarks in the spatial dimension. The former captures the structure patterns of a local component while the latter models the global face patterns. To further validate the effectiveness of the structural modeling mechanisms, we visualize the results on two challenging video frames in Figure \ref{vis_2}. The right are the results of $BL+TE+IT+IR$ while the left are the results of $BL+TE$. The comparison illustrates the significant improvement brought by structure modeling.


\subsection{Empirical Study on the Length of Temporal Window}
\label{esl}
We make an empirical study on the length of the temporal window ($W$) in the recurrent token mixing mechanism. From Figure \ref{fig:w}, we have the following observations. First, when increasing $W$ from 2 to 10, the tracking accuracy and stability boost significantly. The reason may be that, when $W$ is very small, increasing the value of $W$ can provide necessary temporal information for the Transformer to capture. Second, the tracking performance quickly goes to a converge with the increasing of $W$. For example, when $W$ is $10$, the results have no significant gap with $W=16$. The reason may be that, as $W$ reaches an appropriate value, e.g., $10$ in our experiment, the recurrent information delivery strategy from window to window can play an effective role in capturing the long-term temporal patterns of landmark sequences. Thus, there is no need to continuously increase the value of $W$ and a moderate value of $W$ may also bring a good tracking performance comparable to a large $W$. This is an advantage which allows for an acceptable computational complexity of our method.



\begin{figure}
	\centering
	\includegraphics[scale=0.3]{compare.PNG}
	\caption{Visualization of the tracking results by different token communication stategies on a challenging movie frame. In the left image, we show the results of $T\_Att\&S\_Att$, while in the right image, we show the resuls of $T\_Att\&S\_Conv$. Please refer to the texts in Part \ref{tcs} for their definitions. In the left image, the tracker is somewhat confused about the semantics of two landmarks in the occluded eye area and the predicted landmark positions are wrongly squeezed to be close to each other; while in the right image, the landmarks in the occluded area can be accurately inferred with clear semantic distinguishments.}
	\label{vis_0}
\end{figure}

\subsection{Empirical Study on Different Token Communication Strategies}
\label{tcs}
We take attention layers for temporal token communication to capture the dynamic patterns of each landmark, while take 1D convolutional layers for token communication in the spatial dimension to embed the structural patterns of multiple landmarks into their 1D representations. Here we give the experimental results for different token communication strategies on the temporal and spatial dimensions in Table \ref{abla_token} and make analyses. In Table \ref{abla_token},  $T\_Att\&S\_Att$ means taking the attention layers for token communications in both the temporal and the spatial dimensions; $T\_Conv\&S\_Conv$ means taking the 1D convolutional layers for token communications in both the temporal and the spatial dimensions; $T\_Conv\&S\_Att$ means taking the 1D convolution layers for temporal token communication and taking the attention layers for spatial token communication; $T\_Att\&S\_Conv$ means taking the attention layers for temporal token communication and taking the 1D convolution layers for spatial token communication.

From Table \ref{abla_token}, we find that $T\_Att\&S\_Conv$, i.e., the adopted setting in our method, achieves the best results. These results demonstrate that, under the 1D representation learning framework of facial landmarks, the attention layers are more adept at modeling the sequential correlations among temporal tokens, which are the features of one certain facial landmark from different time steps; and 1D convolutional layers show a better capability on capturing the geometric patterns among different landmarks while keeping their clear semantics. As visualized in Figure \ref{vis_0}, compared to the attention layers, when we take 1D convolutional layers for spatial token communications,  the landmarks in the occluded area can be plausibly inferred by the structural patterns of the face and the semantics of different landmarks are clearly distinguished.

\begin{table}
	\centering
	\renewcommand\tabcolsep{1.8pt}
	\caption{NRMSE (N$/\%$) and stability error (S$/\%$)  on different token communication strategies.}
	\begin{tabular}{*{9}{c}}
		\toprule
		\multirow{2}{*}{Settings} &\multicolumn{2}{c}{300VW S1}&\multicolumn{2}{c}{300VW S2}&\multicolumn{2}{c}{300VW S3}
		&\multicolumn{2}{c}{TF} \\ \cline{2-9}
		& N & S &N &S & N & S & N & S \\
		\midrule
		$T\_Att\&S\_Att$ &  2.87  &  0.74 & 2.86 & 0.72 &  3.91  & 1.49  & 1.90  &  0.44 \\ 
		$T\_Conv\&S\_Conv$ & 2.92  &  0.78 &  2.94 & 0.75 &  4.02  & 1.58  & 1.97  &  0.56 \\
		$T\_Conv\&S\_Att$ &   2.91 & 0.76 & 2.92  & 0.74 & 3.99  & 1.53  &  1.95  & 0.51 \\ 
		$T\_Att\&S\_Conv$ (Ours) & \textbf{2.74} & \textbf{0.70} & \textbf{2.74}  & \textbf{0.67} & \textbf{3.80}  & \textbf{1.42}  &  \textbf{1.86}  & \textbf{0.35} \\
		\bottomrule
	\end{tabular}
	\label{abla_token}
\end{table}

\subsection{Discussion on Computational Complexity}
\label{com_comp}
Table \ref{tab:params_and_flops_cp} shows the numbers of parameters and GMACs\footnote{Evaluated with the \href{https://github.com/Lyken17/pytorch-OpCounter}{thop} tool library.} of each sub-moudle of our tracker. From Table \ref{tab:params_and_flops_cp} we could find that the 1DFormer blocks are very slight compared to the backbone encoder \cite{AOHR} and only accounts for $18.02\%$ and $2.76\%$ of the total parameters and GMACs, respectively. In the main body of this paper, experimental results of abation study demonstrate the effectiveness of 1DFormer, and here the statistics from Table \ref{tab:params_and_flops_cp} also show its efficiency. 

\begin{table*}
	\centering
	\caption{Numbers of parameters and GMACs for each sub-moudle of our facial landmark tracker. We can find that the proposed 1DFormer is slight and only accounts for a small part of parameters and GMACs of the whole tracker.}
	\begin{tabular}{*{5}{c}}
		\toprule
		\multirow{2}{*}{Sub-moudles} & Backbone 1D  & 1DFormer & \multirow{2}{*}{Decoder}  & \multirow{2}{*}{All}  \\ 
		& representation encoder \cite{AOHR}&blocks&  &  \\
		\midrule
		Params & 29.73M & 6.88M & 1.58M & 38.19M \\
		GMACs & 28.05 & 0.80 & 0.11 & 28.96 \\
		\bottomrule
	\end{tabular}
	\label{tab:params_and_flops_cp}
\end{table*}

\subsection{Comparison with State-of-the-art Methods}
\label{cp}
We compare our method with the state-of-the-art methods, which include coordinate regression methods, e.g., SDM \cite{SDM}, IFA \cite{IFA},  TSCN \cite{TSCN},  TSTN \cite{TSTN},  GAN \cite{GAN_Tracking}, and MSKI \cite{MSKI}; 2D heatmap regression methods, e.g., FHR \cite{tai2018towards}, FHR+STA \cite{tai2018towards}, ADC \cite{ADC}, SCPAN \cite{SCPAN}, and SAAT \cite{SAAT}; 1D heatmap regression methods, e.g., the Tracker based on Attentive One-dimensional Heatmap Regression (T-AOHR) \cite{AOHR} and SimCC \cite{SimCC}; as well as the hybrid method of 2D and 1D heatmap regression, i.e., HybridMatch (HM) \cite{HybridMatch}. From these methods, FHR, ADC, SCPAN, SAAT, SimCC, MSKI, and HM are detection methods, which only consider spatial modeling from a static image or video frame; SDM, IFA, TSCN, TSTN, FHR+STA, GAN, and T-AOHR are tracking methods which consider both spatial and temporal modeling of a video clip.
\begin{table}[h]
\centering
\renewcommand\tabcolsep{2.0pt}
\begin{tabular}{*{10}{c}}
	\toprule
	\multirow{2}{*}{Method}&\multirow{2}{*}{year}  &\multicolumn{2}{c}{300VW S1}&\multicolumn{2}{c}{300VW S2}&\multicolumn{2}{c}{300VW S3}
	&\multicolumn{2}{c}{TF}\\\cline{3-10}
	& & N & S &N &S & N & S & N & S \\
	\midrule
	SDM \cite{SDM}&	2013 &7.41&	-&	6.18&	-	&13.04&	-&	4.01&	- \\
	TSCN \cite{TSCN}&2014 &	12.54&	-&	7.25&	-&	13.13&	-&	-&	-\\
	IFA \cite{IFA} & 2014 &	-&	-	&-&	-	&-	&-	&3.45&	-\\
	TSTN \cite{TSTN} & 2018 &	5.36&	-&	4.51&	-	&12.84	&-&	2.13&	-\\
	FHR \cite{tai2018towards}& 2019 &4.82&	2.67&	4.23&	1.77&	7.09&	4.43&	2.07&	0.97\\
	FHR+STA \cite{tai2018towards}& 2019 & 4.21&	1.58&	4.02&	1.09&	5.64&	2.62&	2.10&	0.69\\
	GAN \cite{GAN_Tracking}&	2019 &3.50&	0.89&	3.67&	0.84&	4.43&	1.82&	2.03&	0.59\\
	T-AOHR \cite{AOHR}& 2020 &3.06&	0.84	&3.17&	0.87&	4.12&	1.78	&1.97&	0.64 \\
	%		GV\cite{DBLP:conf/accv/XiongZDS20} $\diamondsuit$ & 2020 & & & & & & & & \\
	ADC \cite{ADC}&2020 &4.17&	-	&3.89&	-&	7.28&	-&	-&	-\\   
	SCPAN \cite{SCPAN}& 2021 & 4.49 & -& 4.23 & -&5.87 & -& -&- \\
	SAAT \cite{SAAT}& 2021 &3.46&	-&	3.41&	-&	5.23&	-&	-&	-\\
	SimCC \cite{SimCC} & 2022 & 4.07&  1.22 & 4.12& 1.09  & 5.13 & 2.01  & 2.09 &  0.73 \\
	MSKI \cite{MSKI} & 2022 & 3.89 & 1.49 & 3.94 & 1.23 & 5.07 &  2.07 & 2.19 & - \\
	HM \cite{HybridMatch} & 2023 & 3.15 &  0.89 & 3.23 &  0.92 & 4.28 &  1.80 & 1.99 &  0.67 \\
	\midrule
	\midrule
	Ours & 2023 &\textbf{2.74}   &  \textbf{0.70} & \textbf{2.74}  & \textbf{0.67}  &  \textbf{3.80}  & \textbf{1.42} & \textbf{1.86}  & \textbf{0.35}\\
	\bottomrule
\end{tabular}
\caption{NRSME (N /\%) and stability error (S /\%) of the proposed tracker and the compared methods on the 300VW and the TF databases.}
\label{tab:cp}
%\vspace{-5mm}
\end{table}

\begin{figure*}
	\centering
	\includegraphics[scale=0.9]{visualization_1_2.pdf}
	\caption{Visualization of the tracking results of different tracking methods, i.e., T-AOHR, SimCC, and our method. The three images in the left are video frames from 300VW S3, while the three images in the right are challenging video frames from movies.}
	\label{vis_1}
\end{figure*}

Table \ref{tab:cp} lists the NRMSE performance and stability errors of the proposed tracker and the compared methods on the 300VW and the TF database, respectively. The evaluation results of the compared methods are directly copied from literatures, except for SimCC and HM. Due to the lack of published results of SimCC and HM methods on the respective databases, we just re-implement them under the same experimental condition as our method.  For other methods lacking published results or evaluated under different metrics or normalization standards, we just left their results blank. Experimental results from Table \ref{tab:cp} show that our method outperforms the compared methods on both tracking accuracy and stability. Our method outperforms the NRMSE of T-AOHR, which achieves the best overall performance among the compared methods, by 10.46\%, 13.56\%, 7.77\%, and 5.58\% on the 300VW scenarios 1, 2, 3, and TF databases, respectively; we also outperforms T-AOHR at the stability performance by 16.67\%, 22.99\%, 20.22\%, and 45.31\% on the respective databases. Although T-AOHR applied 1D representations on the facial landmark tracking task and achieved remarkable performance, it had difficulty to adaptively model the long-term temporal dependencies from a video and ignored to model the structure patterns inherent in multiple landmarks, as analyzed in Section \ref{related_1D}. We address these weaknesses by developing a Transformer architecture to release the good potentials of 1D representations on modeling the long-range sequential  patterns as well as the local and global geometric patterns of facial landmarks, thus achieves significant performance boosts compared to T-AOHR and other related works. 

In Figure \ref{vis_1}, we visualize the tracking results of our method and two representative 1D heatmap regression methods, i.e., T-AOHR\cite{AOHR} and SimCC\cite{SimCC}, for a further comparison. From Figure \ref{vis_1}, we can observe that compared to these T-AOHR and SimCC, our method can perform well under challenging image conditions, such as non-frontal head poses and extreme occlusions that commonly encountered in real-world scenarios.

%\subsection{Experimental Results and Analyses}
%\label{ablation}
%\subsubsection{Ablation Studies}
%We conduct multi-group experiments for ablation studies. The NRMSE and the stavility errors of these experiments are recorded in Table \ref{abla}. The experimental settings are as the following:  $BL$ denotes the baseline method, which only preserves the backbone marginal encoder $f_{bk}(\cdot)$ as well as the 1D heatmap decoders, i.e.,  $f^x_{de}(\cdot)$ and $f^y_{de}(\cdot)$. This makes a simple facial landmark detection method without any temporal modeling and structure modeling modules. $TE$ denotes the proposed Transformer-based sequential modeling module; $TE^{-r}$, $TE^{-a}$, and $TE^{-c}$ are three additional ablation settings from $TE$: the superscript $-d$ removes the temporal recurrent mechanism and simply takes the output features of $f_{bk}(\cdot)$ as the input of Transformers, instead of deliverying long-term historical information in a recurrent way as Eq. \eqref{transxy} does; the superscript $-a$ replaces the axis-landmark-positional encodings with the conventional positional encodings; the superscript $-c$ removes the confidence branch from affinity decision. $IR$ and $f_{ie}$ respectively denote the proposed intra-group and inter-group structure modeling mechanisms. Here, $BL$+$TE$+$IR$+$f_{ie}$ corresponds to the full method of our work.
\section{Conclusions}
On addressing the weaknesses of current 1D heatmap regression methods to fully explore the good potentials of 1D landmark representations on temporal and structure modeling of multiple facial landmarks, we propose a new facial landmark tracking method based on a new Transformer architecture, namely 1DFormer, to model long-range temporal  patterns as well as the local and global facial structures via token communications in the temporal and spatial dimensions, respectively. Specifically, a recurrent token mixing mechanism, an axis-landmark-positional embedding mechanism, and a confidence-enhanced multi-head attention mechanism are proposed for temporal modeling; an intra-group and an inter-group structure modeling mechanism are presented for structure modeling. Experimental results on the 300VW and the TF databases demonstrate that our method achieves state-of-the-art performance for facial landmark tracking with a good modeling capability on the temporal dynamics as well as the geometric patterns of facial landmarks. 
%This research also offers a promising direction for future research on leveraging 1D representations for complex sequential and structural modeling tasks in the field of computer vision.

\section*{Acknowledgements}
This work has been supported by the National Key R\&D Program of China  (Grant No. 2022ZD0117900).

\bibliography{egbib}

\end{document}