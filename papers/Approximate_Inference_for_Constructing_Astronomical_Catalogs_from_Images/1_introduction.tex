\section{Introduction}
\label{sec:intro}
Astronomical surveys are the primary source of information about the universe beyond our solar system. They are essential for addressing key open questions in astronomy and cosmology about topics such as the life cycles of stars and galaxies, the nature of dark energy, and the origin and evolution of the universe.

\begin{figure}[hb]
\centering
\includegraphics[width=2in]{sdss_fields}
\caption{Sample data from the Sloan Digital Sky Survey (SDSS). Image boundaries appear as grey lines. All images have the same rectangular size; there is overlap.}
\label{fig:sdss_fields}
\end{figure}

The principal products of astronomical imaging surveys are catalogs of light sources, such as stars and galaxies.
These catalogs are generated by identifying light sources in survey images (e.g., Figure~\ref{fig:sdss_fields}) and characterizing each according to physical parameters such as flux\footnote{Flux is the amount of energy transferred from the light source per unit area (directly facing the light source) per second. ``Apparent brightness'' is another term for flux.}, color, and morphology.

Astronomical catalogs are the starting point for many scientific analyses.
First, catalogs enable demographic inference, which can address fundamental cosmological questions.
For example, researchers may need to know the spatial and luminosity distributions for specific classes (subpopulations) of stars or galaxies.
For these subpopulation-level analyses, accurate quantification of uncertainty in the point estimates of parameters is as important as the accuracy of the point estimates themselves.
It is an open question how to infer a full-sky catalog using Bayesian inference that is also well-calibrated enough for these subpopulation analyses: modeling assumptions that are reasonable for full-sky cataloging are typically too inaccurate for final scientific analysis of subpopulations.
Our work is a step toward creating such a catalog, though this application is not our primary focus.

Second, catalogs inform the design of follow-on surveys using more advanced or specialized instrumentation.
For example, a primary use of the Sloan Digital Sky Survey (SDSS) catalog was to select galaxies to target for follow-up study with a spectrograph~\citep{york2000sloan}.
Whereas image data provides only a rough approximation of the colors of galaxies, spectographs can measure galaxy fluxes for each of hundreds of wavelength bins.
Typically a ``portfolio'' of galaxies to target is selected for each of several galaxy types, e.g., main~\citep{strauss2002spectroscopic}, luminous red galaxies~\citep{eisenstein2001spectroscopic}, and quasars~\citep{richards2002spectroscopic}.
Selecting each portfolio of galaxies amounts to decision making under uncertainty. At present this task is not handled in a statistically coherent way.
Using traditional catalogs, incorporation of uncertainty is not straightforward; astronomers resort to heuristics, typically implemented through cuts based on the raw point estimates appearing in the catalogs. In one case, a portfolio of galaxies was chosen to maximize the sum of the z-values implied by point estimates (and ignoring uncertainties).
In the framework of Bayesian decision theory~\citep{berger2013statistical}, given an approximate posterior distribution, it is straightforward conceptually to select a portfolio of light sources that minimizes a particular cost function.
This second task is our main concern in this work.


\subsection*{Software pipelines for cataloging}

Catalog construction today is based on software pipelines.
For concreteness, we describe the Hyper Suprime-Cam (HSC) software pipeline~\citep{bosch2018hyper}.
The contrasts we subsequently draw between our proposed approach and HSC, however, apply to catalog pipelines in general.
We focus on HSC because it is the state of the art in nearly all respects.
Its code has been merged into the cataloging pipeline for the Large Synoptic Survey Telescope~\citep{lsst}---one of the most important upcoming sky surveys.
HSC draws upon the algorithmic development of both the SDSS software pipeline~\citep{lupton2001sdss}
and SExtractor~\citep{bertin1996sextractor}.

The HSC software pipeline comprises a sequence of steps, including 1) detrending, 2) cosmic ray masking, 3) ``repair'' of saturated and bad pixels through interpolation,
4) estimating the sky background, 5) detecting candidate sources, i.e., localized ``peaks'', 6) estimating the centroids of light sources, 7) estimating the shape of light sources,
8) estimating the flux of light sources, 9) matching light sources to external catalogs,
10) estimating the point-spread function, and 11) performing star/galaxy separation.
Most of these steps depend on estimates from other steps, and many have circular dependencies.
Steps with circular dependencies are repeated multiple times.
For example, at first a circular Gaussian serves as a crude approximation of a star for masking cosmic rays.
Later the cosmic ray detector is rerun with a refined star model.

For the initial sequence of steps (i.e., a ``stage''), the semi-iterative sequence steps are executed on all images independently,
regardless of any overlap.
During later stages, constraints are added that require the algorithm to use a shared estimate for a light source in an overlapping region.
The matching itself depends on aligning the images correctly, which in turn depends on correctly detecting light sources---an additional circular dependency.
Ultimately, aligned, calibrated, and deblended images are ``co-added'' (superimposed) to create one image for each light source.
The final estimate of a light source's properties is based on
the co-added images and accompanying per-pixel variance estimates.

The uncertainty estimates for a light source's flux include only this pixel-level variability.
They do not account for all the other sources of uncertainty that cannot reasonably be modeled as independent across pixels: uncertainty about the light source's centroid, the number of light sources, the image alignments, cosmic ray detection, light sources' shapes, and nearby light sources' fluxes and shapes. The reported uncertainties are based on a Gaussian statistical model of pixels, but one that conditions on the previous stages' estimates of all these quantities.
Effectively, the reported uncertainties are for a conditional distribution rather than a marginal distribution.

Modern cataloging pipelines have struck a balance between algorithmic efficiency and statistical rigor
that has enabled much of the progress in astronomy to date.
Upcoming surveys, however, will probe deeper into the visible universe, creating new challenges.
In particular, whereas blending currently affects just a small number of light sources,
in LSST it is estimated that 68\% of light sources will be blended, requiring new approaches to deblending~\citep{bosch2018hyper}.
In addition, new approaches may let us better interpret existing survey data.
Our aim in this work is to put catalog construction on sounder statistical footing.



\subsection*{Bayesian inference for cataloging}

Our first contribution is a statistical model (Section~\ref{sec:model}) that can simultaneously find centroids,
determine photometry (flux, color, and galaxy morphology), deblend overlapping light sources, perform star/galaxy separation, and adjust estimates of all quantities based on prior information.
Our procedure for all these tasks is based on a single probabilistic model.
The properties of cataloged light sources are modeled as unobserved random variables.
The number of photons recorded by each pixel is modeled by a Poisson distribution with a rate parameter unique to the pixel.
The posterior distribution induced over the unobserved physical properties of the light sources encapsulates knowledge about the catalog's entries, combining prior knowledge of astrophysics with survey imaging data in a statistically efficient manner.
With the model, we can reason about uncertainty
for any quantity in our catalog without conditioning on other estimates being known exactly.

Unfortunately, exact Bayesian posterior inference is intractable for most probabilistic models of interest~\citep{bishop2006pattern}, including this one.
Approximate Bayesian inference is an area of active research. Markov chain Monte Carlo (MCMC) is the most common approach.
Two recent studies demonstrate that Bayesian modeling is the gold standard for astronomical inference, while casting doubt on whether MCMC is viable for constructing a whole astronomical catalog.
~\citet{brewer2013probabilistic} use a single 10,000-pixel image as the dataset for an MCMC procedure.
Obtaining samples from the posterior distribution takes one day using a modern multi-core computer.
\citet{portillo2017improved} run
twelve Intel Xeon cores for an entire day to yield useful results on a similar dataset.
The Sloan Digital Sky Survey---a modern astronomical survey---contains over a billion times as many pixels as these test images.
The upcoming Large Synoptic Survey Telescope (LSST) will collect at least ten terabytes nightly---hundreds of petabytes in total \citep{lsst}. Even basic management of these data requires substantial engineering effort.

Before our work, Tractor~\citep{tractor} was the only program for Bayesian posterior inference that had been applied to a complete modern astronomical imaging survey. Tractor is unpublished work.
It relies on the Laplace approximation: the posterior is approximated by a multivariate Gaussian distribution centered at the mode, having a covariance matrix equal to the negative Hessian of the log-likelihood function at that mode. This approximation is not suitable for either categorical random variables or random variables with multi-modal posteriors---no Gaussian distribution approximates them well. Additionally, because Laplace approximation centers the Gaussian at the mode of the target, rather than the mean, the solution depends on the problem parameterization~\citep{bishop2006pattern}.

Variational inference (VI) is an alternative to MCMC and the Laplace approximation. Like the latter, it uses numerical optimization, not sampling, to find a distribution that approximates the posterior~\citep{blei2017variational}. In practice, the resulting optimization problem is often orders of magnitude faster to solve compared to MCMC approaches. It can be simpler, too. Whereas MCMC transition operators must satisfy strict constraints for validity, the variational optimization problem can in principle be solved using any off-the-shelf technique for numerical optimization. Scaling VI to large datasets is nonetheless challenging.

Our second contribution is to develop and compare two approximate posterior inference procedures for our model: one based on MCMC (Section~\ref{sec:mcmc}) and the other based on VI (Section~\ref{sec:vi}).
Neither is a routine application of Bayesian machinery.
The MCMC procedure combines annealed importance sampling and slice sampling \citep{neal2001annealed, neal2003slice}.
The VI procedure breaks with tradition by optimizing with a variant of Newton's method instead of closed-form coordinate ascent.
For synthetic images drawn from our model, MCMC better quantifies uncertainty, whereas for real astronomical images taken from the Sloan Digital Sky Survey (SDSS), model misspecification may be a more significant limitation than the choice of posterior approximation~(Section~\ref{sec:experiments}).

For either type of data, our VI procedure is orders of magnitude faster than our MCMC procedure.
We scale our VI procedure to the entire Sloan Digital Sky Survey (SDSS) using a supercomputer (Section~\ref{sec:at-scale}).
To our knowledge, this is the largest-scale reported application of VI by at least one order of magnitude.

While our statistical model and inference procedures are accurate on average, the final scientific analysis of a subpopulation of stars or galaxies typically requires priors that are accurate for that particular subpopulation. Several strategies are available for downstream tasks requiring priors specific to the subpopulation, both with and without reprocessing the image data (Section~\ref{sec:discussion}).