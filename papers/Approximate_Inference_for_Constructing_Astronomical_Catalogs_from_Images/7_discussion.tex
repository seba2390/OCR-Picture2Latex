\section{Discussion}
\label{sec:discussion}

We introduced our work by identifying a limitation of existing cataloging pipelines: centroiding, deblending, photometry, star/galaxy separation, and incorporation of priors happen in distinct stages.
Uncertainty is typically not propagated between stages.
Any uncertainty estimates these pipelines produce are based on conditional distributions---that is, they are conditional on the output of the previous stages.

We developed a joint model of light sources' centers, colors, fluxes, shapes, and types (star/galaxy).
Whereas previous approaches to cataloging have been framed in algorithmic terms,
statistical formalisms let us characterize our inferences without ambiguity.
Statistical formalisms also make modeling assumptions transparent---whether the assumptions are appropriate ultimately depends on the downstream application.
We highlighted limitations of the model to guide further development.

A model is only useful when it can be applied to data.
We proposed two procedures: one based on MCMC and the other on VI.
Neither MCMC nor VI could be applied to our model without customization.
The need for problem-specific adjustments is a barrier to the broader adoption of both techniques.
With MCMC, for example, we went through several iterations before settling on slice sampling and AIS, including Metropolis-Hastings (MH) and reversible jump~\citep{green1995reversible}.
Compared to slice sampling, we found MH difficult to tune.
We found that reversible-jump MCMC required carefully constructed proposals to jump often enough between the star and galaxy models and was also difficult to tune.

VI required even more problem-specific customization.
Our VI techniques include the following: 1) approximating an integrand with its second-order Taylor expansion; 2) approximating the point-spread function with a mixture of Gaussians; 3) upper bounding the KL divergence between the color and a GMM prior; 4) limiting the variational distribution to a structured mean-field form; 5) limiting the variational distribution to point masses for some parameters; and 6) optimizing the variational lower bound with a variant of Newton's method rather than coordinate ascent. This final technique was particularly laborious, as it involved manually deriving and implementing both gradients and Hessians for a complicated function.

On synthetic data, MCMC was better at quantifying uncertainty, which is likely due to the restrictive form
of the variational distribution.
Additionally, MCMC provided uncertainty estimates for all latent random variables, whereas VI modeled some random variables as point masses---in effect recovering maximum a posteriori (MAP) estimates for them.
However, MCMC was approximately $1000\times$ slower than VI.

On real data, point estimates from VI were not always worse than point estimates from MCMC. Neither procedures' uncertainty estimates were perfectly calibrated for galaxies, suggesting some degree of model misspecification.
Imperfectly calibrated uncertainties can nonetheless be useful, e.g., for flagging particularly unreliable point estimates. Additionally, even if the uncertainties are ignored by downstream analyses, point estimates typically improve when uncertainty is modeled.
For questions requiring calibrated uncertainties, enhancing the galaxy model may help to reduce model misspecification.
Though the galaxy model we use---one with elliptical contours---is standard in astronomy, a more flexible galaxy model shows promise~\citep{regier2015deep}.

For spectrographic targeting, our current catalog should nonetheless
be an improvement over what came before: previously, uncertainty estimates and prior information were ignored. For analysis of subpopulations, however, we stress a key difference between our catalog and traditional astronomical catalogs: our catalog is based on prior information, whereas traditional catalogs are not.
Moreover, though our prior is accurate enough for large-scale cataloging and deblending, it likely is not accurate enough for a final scientific analysis of a particular subpopulation of light sources (e.g. the galaxies with an ``active galactic nucleus'').
For this use case, which is beyond the scope of our work, we suggest two approaches. First, a user can form a Laplace approximation, to ``remove'' our priors from the catalog and replace them with priors that are more suitable for their subpopulation. To facilitate, any catalog generated with our method should also contain parameters of the priors used to generate it.
Catalog users can then apply new priors directly to the catalog, without revisiting the image data; astronomers typically prefer to work with catalogs rather than images because catalogs are so much smaller.

We would prefer that users deal differently, however, with model misspecification that affects their analysis:
instead of trying to work around model misspecification, enhance our model.
Then, rerun our cataloging software, with the new model, on the images. This approach encourages users to adapt the statistical model and the priors to their needs and to treat the catalog as an intermediate data product~\citep{turon2010telescopes}.
While some work would be required to modify our model, the techniques we illustrate in this paper could still be followed to perform inference.
The MCMC procedure makes it particularly straightforward to make changes.

Because astronomical surveys are large (comprising terabytes of data now, and petabytes in the near future), scalability is of paramount concern.
We approximated the posterior for a large image dataset and demonstrated the scaling characteristics necessary to apply approximate Bayesian inference to hundreds of petabytes of images from the next generation of astronomical surveys.
Our optimization procedure found a stationary point, even though doing so required treating the full dataset as a single optimization problem.

Because of the relative ease of deriving and implementing MCMC, it could be a useful tool for trying different models and testing for misspecification prior to implementing VI. In some cases, it may be simpler to expend more computational resources to scale up the MCMC procedure than to implement VI. For the most computationally intensive problems, however, only VI can currently perform approximate inference.
