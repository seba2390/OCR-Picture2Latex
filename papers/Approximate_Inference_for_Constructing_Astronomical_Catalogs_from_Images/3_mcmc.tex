\section{Markov chain Monte Carlo}
\label{sec:mcmc}

Markov chain Monte Carlo (MCMC) is a common approach for approximating posterior distributions in computationally challenging Bayesian models.
MCMC draws samples from a stochastic process on the parameter space whose stationary distribution is the posterior distribution of interest.
The stochastic process is specified by a transition kernel, denoted $\mathcal T$.
The empirical distribution of these samples approximates the posterior distribution.
Statistics of this empirical distribution, such as its mean and its quantiles, approximate the same
statistics of the posterior distribution.

Our problem presents two challenges for MCMC.
First, the state space is extremely high-dimensional---there are multiple random variables for each of millions of light sources.
We cannot consider transition kernels that require hand-tuning of dimension-specific parameters, such as step size, proposal variance, or temperature schedule.
Second, the state space is trans-dimensional. %\citep{green1995reversible}.
Galaxies have more parameters than stars, and light-source types (star/galaxy) are themselves random.

We propose a multi-level sampling procedure.
In an outer loop based on (block) Gibbs sampling \citep{robert2013monte}, %\todo{Maybe cite Robert and Casella or something instead?}
light sources are treated sequentially. Each light source's latent variables are sampled with any overlapping light sources' latent variables, denoted $z_{-s}$, held fixed.
Formally, in Gibbs iteration $k=1,\ldots,K$, we draw
\begin{align}
z_s^{(k)} \sim p(z_s | x, z_{-s}^{(k-1)}) 
\label{eqzs}
\end{align}
%\todo{In this equation should the stuff on the right-hand side have $k-1$ superscripts?}
for light sources $s=1,\ldots,S$ in sequence.
To speed up convergence,
we initialize $z_1^{(0)},\ldots,z_S^{(0)}$ with approximately correct values determined by a preprocessing routine.
%This is a valid procedure for sampling from the posterior. \todo{What is a valid procedure being referred to here?}

Our strategy for generating samples from the distribution in Equation~\ref{eqzs} is to first draw a sample from the marginal posterior over the source's type~$a_s$ (star or galaxy) and then draw samples from the conditional posterior over the remaining source parameters, $w_s \triangleq (r_s, c_s, e_s, u_s)$:
\begin{align}
    a_s &\sim p(a_s | x, z_{-s}) && \text{ marginal source type; } \label{eq:source-type-marginal} \\
    w_s ~\vert~ a_s &\sim  p(w_s | a_s, x, z_{-s})) && \text{ conditional source parameters.} \label{eq:source-param-conditional}
\end{align}
To generate a sample from Equation~\ref{eq:source-type-marginal} we use annealed importance sampling (AIS) \citep{neal2001annealed}, initialized with outputs of the AIS step.
To generate a conditional sample from Equation~\ref{eq:source-param-conditional} we use slice sampling~\citep{neal2003slice}. 
We will explain each sampler in turn in Sections~\ref{mcmc1} and \ref{mcmc2}.

Recall that $a_s$ is the Bernoulli random variable that indexes the source type (star/galaxy), and thus the dimension of our state space.
This two-step sampling strategy allows us to avoid using a trans-dimensional sampler like reversible-jump MCMC \citep{green1995reversible}, a technique that requires constructing a potentially complex trans-dimensional proposal function~\citep{fan2011reversible}.

\subsection{Sampling the posterior over $a_s$}
\label{mcmc1}
To generate a sample from the marginal posterior over $a_s$, we estimate the marginal posterior probabilities of $a_s=1$ and $0$ (which together sum to one).
By Bayes's rule, we can write the marginal posterior
\begin{align}
p(a_s = 1 | x, z_{-s}) &\propto p(x | a_s = 1, z_{-s}) p(a_s=1 | z_{-s}) \label{eq:source-type-marginal-bayes}
\end{align}
The term $p(x | a_s=1, z_{-s})$ is the marginal likelihood of the observation $x$ given the source is of type $a_s=1$, which is the type of estimand AIS is designed to estimate.  The term $p(a_s=1 | z_{-s}) = p(a_s=1)$ is the prior over source type.

AIS is an iterative procedure to estimate the normalizing constant (i.e., the integral) of an unnormalized probability density $\pi$.  In order to estimate the marginal likelihood $p(x | a_s, z_{-s})$, we estimate the normalizing constant of the distribution 
\begin{align}
\pi(w_s) \coloneqq p(x | w_s, a_s, z_{-s}) p(w_s | a_s, z_{-s})
\end{align}
for both source types, $a_s = 0$ and $a_s=1$.  
This normalizing constant is $p(x | a_s, z_{-s})$.
Given an estimate of $p(x | a_s, z_{-s})$ (for both settings of $a_s$) and a prior over $a_s$, we can construct an estimate of $p(a_s = 1 | x, z_{-s})$ using Bayes' rule as in Equation~\ref{eq:source-type-marginal-bayes}.
Then we can sample from $p(a_s | x, z_{-s})$.

In addition to the target $\pi$, AIS takes as input a sequence of $T$ distributions
$\pi_0, \pi_1, \dots, \pi_T$ that approach the target.
The statistical efficiency of AIS depends on the similarity of intermediate distributions $\pi_{t-1}(z_s) / \pi_{t}(z_s)$.
We set $\pi_0(z_s) \coloneqq p(w_s | a_s, z_{-s})$---a normalized density.
For $t=1,\ldots,T$, we set
\begin{align}
  \pi_t(w_s) &= \pi_0(w_s)^{1-\gamma_t} \pi(w_s)^{\gamma_t}
\end{align}
for a sequence of temperatures $0 = \gamma_0 < \gamma_1 < \dots < \gamma_T = 1$.
These (unnormalized) distributions interpolate between the prior and the posterior.

For $t=1, \dots, T$, let $\mathcal T_t$ be a Markov chain transition that leaves (the normalized version of) $\pi_t$ invariant.
To implement each transition kernel, $\mathcal{T}_t$, we use slice sampling, a Markov chain Monte Carlo method that requires little tuning and automatically adapts to the local scale for each variable \citep{neal2003slice}.
We iterate over each variable in $z_s$, forming a slice-sampling-within-Gibbs transition kernel.

We begin by sampling $w_s^{(0)} \sim \pi_0$.
Then, for $t=1, \dots, T$, we draw
\begin{align}
w_s^{(t)} | w_s^{(t-1)} \sim \mathcal{T}_t(w_s^{(t-1)}, w_s^{(t)}).
\end{align}
After $T$ iterations, $w_s^{(T)}$ is approximately distributed according to (the normalized version of) $\pi_T = \pi$, and
\begin{align}
	\mathcal Z_s \coloneqq \exp \sum_{t=1}^T \log \frac{\pi_{t}(w_s^{(t-1)})} { \pi_{t-1}(w_s^{(t-1)}) }
\end{align}
is a consistent estimator of $p(x | a_s, z_{-s})$ \citep{neal2001annealed}.
AIS can be viewed as importance sampling over an augmented state space where the expanded dimensions begin with the prior distribution and gradually anneal to the targeted posterior according to $T$ temperatures.
Thus, the ratio of these weights is a consistent estimator of the marginal likelihood.

Estimating the marginal likelihood (also referred to as the model evidence) is a rich area of methodological development. \cite{skilling2004nested} presents another popular approach for computing marginal likelihood estimates, known as nested sampling.  However, \cite{friel2012estimating} show cases where nested sampling is less efficient statistically and computationally than AIS, motivating our use of AIS in this work.

\subsection{Sampling source parameters conditioned on $a_s$}
\label{mcmc2}
The final step of our AIS procedure draws samples from $p(w_s | a_s, x, z_{-s})$.
For each source type (star/galaxy), we run $N'$ independent repetitions of our AIS procedure.
We use the resulting samples as independent starting positions for $N'$ Markov chains.
We run these $N'$ chains for $B'$ more steps, monitoring convergence and mixing criteria \citep{gelman1992inference}.
This process yields $N'$ estimates of the marginal likelihood, and $N' \times B'$ (correlated) samples drawn from the Markov chain.

To summarize, the overall AIS-MCMC sampling procedure corresponding to Equation~\ref{eqzs} is as follows:
\begin{itemize}
\item For each source type $a_s = a \in \{0, 1\}$ (e.g., ~star or galaxy)
\begin{itemize}
  \item Run $N'$ independent marginal likelihood estimators, each with $T$ annealing steps. This results in $N'$ independent estimates of $\log p(x | a_s=a, z_{-s})$ and $N'$ approximate posterior samples from $p(w_s | a_s = a, x, z_{-s})$.
  \item For each of the $N'$ approximate posterior samples, run an MCMC chain of length $B'$, using slice-sampling-within-Gibbs transitions.
\end{itemize}
\item Use the $\log p(x | a_s = 0, z_{-s})$ and $\log p(x | a_s=1, z_{-s})$ estimates to approximate $p(a_s = 1 | x, z_{-s})$.
\item Use the estimate of $p(a_s = 1 | x, z_{-s})$ to sample a source type $a_s^{(k)}$, approximating the distribution in Equation~\ref{eq:source-type-marginal}.
\item Randomly choose one of the $N' \times B'$ posterior samples corresponding to the realized $a_s^{(k)}$, which approximates the distribution $w_s^{(k)} \sim p(w_s | x, z_{-s}, a_s)$ from Equation~\ref{eq:source-param-conditional}; or collect all $N' \times B'$ samples to approximate posterior functionals.
\end{itemize}

The AIS-MCMC procedure described above requires us to choose a number of samples and iterations.  For the experiments we describe in Section \ref{sec:experiments}, we use $T = 200$ annealing steps and $N'=25$ independent samples of the marginal likelihood.  For each of the $N'$ samples, we run an additional slice-sampling MCMC chain for $B'=25$ iterations, producing a total of $N'\times B' = 625$ correlated posterior samples of $z_s$.

%We find that the two methods, AIS and SS, are complementary---while AIS computes a marginal likelihood estimator (using SS), it simultaneously acts as a tempering ``burn-in'' (or ``warm-up'') that yields a sample from a distribution that is close to the exact posterior. Starting from this close sample obviates the need to throw away samples from a warm-up phase, as would have been necessary had we initialized our MCMC chain from the prior distribution or a fixed starting point.
