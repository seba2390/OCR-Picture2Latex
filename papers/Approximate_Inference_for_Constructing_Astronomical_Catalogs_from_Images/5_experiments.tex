\section{Experimental results}
\label{sec:experiments}
Our experiments aim to assess
1)~how MCMC and VI compare, statistically and computationally; and
2)~how well our procedures quantify uncertainty.

We base our experiments both on synthetic images drawn from our model (Section~\ref{sec:synth}) and images from the Sloan Digital Sky Survey (Section~\ref{sec:real}).
For both datasets, we run both
the MCMC procedure from Section~\ref{sec:mcmc} (henceforth, MCMC)
and the variational inference procedure from Section~\ref{sec:vi} (henceforth, VI),
and compare their posterior approximations.%
\footnote{Open-source software implementing our inference procedures is available from \url{https://github.com/jeff-regier/Celeste.jl}. Jupyter notebooks demonstrating how to replicate all reported results are stored in the \texttt{experiments} directory.}

We assess the accuracy of point estimates (e.g., posterior means/modes) and uncertainties (e.g., posterior variances), as well as star/galaxy classification accuracy.
Our accuracy measures are averaged over a population of light sources.
While no single metric of quality suffices for all downstream uses of catalogs,
good performance on the metrics we report is necessary (though not sufficient) for good performance on most downstream tasks.
These error metrics, which are an unweighted average across light sources, are likely more representative of performance at spectrograph targeting than for demographic inference. (Demographic inference and spectrograph targeting are the two downstream applications we introduced in Section~\ref{sec:intro}.)


\subsection{Synthetic images}
\label{sec:synth}

\begin{figure}[!hb]
\begin{subfigure}{.49\textwidth}
\includegraphics[width=0.95\textwidth]{figures/real.jpg}
\end{subfigure}
\begin{subfigure}{.49\textwidth}
\includegraphics[width=0.95\textwidth]{figures/synthetic.jpg}
\end{subfigure}
\caption{\textit{Left:} An image from SDSS containing approximately 1000 detectable light sources.
Pixels in error are ``masked'' (black strips).
\textit{Right:} A synthetic image for the same region, generated from our model by conditioning on an SDSS catalog for that region. (Several of the light sources with extremely high flux are excluded---the CCDs cannot record such high flux.)}
\label{real-synthetic}
\end{figure}


\begin{figure}
\begin{floatrow}
\ffigbox{%
  \includegraphics[width=\linewidth]{plots-synthetic/pstar_roc_comparison.png}
}{%
  \caption{ROC curve for star/galaxy classification on synthetic data.}
  \label{roc-synth}
}
\capbtabbox{%
\scalebox{.85}{
\input{plots-synthetic/error_vb_mc_comparison-pair.tex}
}
\vspace{0.5em}
}{%
\caption{%
Left columns: Mean absolute error on synthetic data.
Right column:~Pairwise error differences (and standard error).
Statistically significant differences appear in bold font.}
\label{tab:err-synth}
}
\end{floatrow}
\end{figure}

%---- qq plots on synthetic data -------
\begin{figure}
\centering
\begin{subfigure}{.49\textwidth}
  \centering
  \includegraphics[width=\linewidth]{plots-synthetic/error-scatter-log_flux_r-star.png}
  \caption{log flux (stars)}
\end{subfigure}
\begin{subfigure}{.49\textwidth}
  \centering
  \includegraphics[width=\linewidth]{plots-synthetic/error-scatter-color_ri-star.png}
  \caption{color r-i (stars)}
\end{subfigure}
\\
\begin{subfigure}{.49\textwidth}
  \centering
  \includegraphics[width=\linewidth]{plots-synthetic/error-scatter-log_flux_r-gal.png}
  \caption{log flux (galaxies)}
\end{subfigure}
\begin{subfigure}{.49\textwidth}
  \centering
  \includegraphics[width=\linewidth]{plots-synthetic/error-scatter-color_ri-gal.png}
  \caption{color r-i (galaxies)}
\end{subfigure}
\caption{VI and MCMC performance on synthetic data.  Each pair depicts VI (left, blue) and MCMC (right, orange) with the ground truth along the $x$-axis and the posterior distribution (showing equal-tailed 95.4\% credible intervals) along the $y$-axis.}
\label{qq-synth}
\end{figure}

\begin{table}[b]
\scalebox{.75}{
\input{plots-synthetic/uscore_vb.tex}
}~
\scalebox{.75}{
\input{plots-synthetic/uscore_per.tex}
}
\vspace{.5em}
\caption{
Proportion of light sources having posterior means found by VI (left) and MCMC (right) near the ground truth for synthetic images.
The VI credible intervals correspond to the estimated posterior standard deviation.
For MCMC, we match these with equal-tailed credible intervals derived from samples, where one-half standard deviation (sd) covers 38.2\% of probability mass, 1 sd covers 68.3\%, 2 sds covers 95.4\% and 3 sds covers 99.7\%.}
\label{tab:calibration-synth}
\end{table}

\begin{figure}[b]
\begin{subfigure}{.32\textwidth}
  \centering
  \includegraphics[width=\linewidth]{plots-synthetic/posterior-comparison-gal-log_flux_r-src-302.png}
  \caption{}
\end{subfigure}
\begin{subfigure}{.32\textwidth}
  \centering
  \includegraphics[width=\linewidth]{plots-synthetic/posterior-comparison-star-log_flux_r-src-43.png}
  \caption{}
\end{subfigure}
\begin{subfigure}{.32\textwidth}
  \centering
  \includegraphics[width=\linewidth]{plots-synthetic/posterior-comparison-star-log_flux_r-src-83.png}
  \caption{}
\end{subfigure}
  \caption{Comparison of posterior uncertainty for the flux of three synthetic light sources where the posterior mean is a poor prediction of the true parameter value. VI underestimates posterior uncertainty. MCMC assigns much greater posterior density to the true values.}
\label{uq-synth}
\end{figure}

Synthetic images let us compare inference methods without model misspecification.
On synthetic images, ``ground truth'' for the latent random variables is known.
Synthetic images also let us validate our model by visually checking their similarity to real images.
To generate realistic synthetic images, we take the non-inferred parameter values from real SDSS images, including the point-spread function $\psi_n$, the sky background $\sigma_n$, and structural constants like the dimensions of the images.
To illustrate that synthetic data resemble real images, Figure~\ref{real-synthetic} depicts a synthetic image generated using parameters from an existing catalog.
In our experiments, the light sources in synthetic images are instead drawn from the prior.
Our synthetic study set comprises five overlapping $2048 \times 1489$-pixel images.
Each image is for a different filter band.
The images contain approximately 500 detectable light sources.

Empirically, MCMC performs better for star/galaxy classification than VI for all thresholds of a receiver operating characteristic (ROC) curve (Figure~\ref{roc-synth}).
Both methods have a high area under the curve (AUC).
For MCMC, the AUC is 0.994. For VI, the AUC is 0.981.

Both methods estimate means well for all continuous latent random variables (Table~\ref{tab:err-synth}).
MCMC outperforms VI significantly for some point estimates.
``Direction'' is error, in arcseconds (0.396 pixels), for the directions of the light sources'
centers.
``Flux'' measures the reference band (r-band) flux.
``Colors'' are ratios of fluxes in consecutive bands.
``Galaxy profile'' is a proportion indicating whether a galaxy is de Vaucouleurs
or exponential.
``Galaxy axis'' is the ratio between the lengths of a galaxy's minor and
major axes.
``Galaxy radius'' is the half-light radius of a galaxy in arcseconds.
``Galaxy angle'' is the orientation of a galaxy in degrees.

For color and flux, MCMC often has larger posterior uncertainty.
MCMC assigns substantial probability density to the truth more often than VI (Figure~\ref{qq-synth}).
For light sources where posterior means are particularly poor predictors of the truth, VI severely underestimates the uncertainty, whereas MCMC assigns much greater posterior density to the true values (Figure~\ref{uq-synth}).
For color and log flux---both normally distributed quantities in this synthetic data---errors from MCMC are more nearly normally distributed than those of VI.
Table~\ref{tab:calibration-synth} reports the fraction of sources covered by equal-tailed posterior credible intervals of increasing width. The MCMC uncertainty estimates are more accurately calibrated.
The typical range of effectively independent samples generated MCMC is between 100 and 150 per source.
For a single source, 140 samples is sufficient to approximate a 60\% credible interval with high probability \citep{booth1998monte}.
However, we note that we are averaging over 500 sources, each with independent samples, allowing us to resolve population posterior coverage with higher fidelity.

These empirical results are anticipated by theory: VI underestimates the posterior uncertainty because independence assumptions in the variational distribution do not hold in the posterior~\citep{bishop2006pattern}. Additionally, differences between the candidate variational distributions' marginals and the posteriors' marginals are a source of bias. For the marginals we approximate with point masses (those of $u_s$ and $e_s$), that may be a particularly important source of bias.


\subsection{Real images from SDSS}
\label{sec:real}

Absolute truth is not currently knowable for astronomical catalogs. Fortunately, one area of the sky, called ``Stripe 82,'' has been imaged many times in SDSS.
This region provides a convenient validation strategy: combine exposures from all Stripe-82 runs to produce a high signal-to-noise image, then use parameters estimated from the combined exposure as a surrogate ground truth.

Photo~\citep{lupton2005sdss} is the primary software pipeline for cataloging SDSS.
We use Photo's estimated parameters from the combined Stripe 82 imagery as ground truth.
We then run Photo and our method on just one of the 80 image sets, comparing the results from each to the ground truth.
%Although this ``ground truth'' is still prone to errors, such errors typically favor Photo, since any systematic errors will be consistent in Photo's output.

To reduce the runtime of our algorithms, we test them on only a subset of Stripe 82.
Our Stripe 82 study set comprises five overlapping $2048 \times 1489$-pixel images for a typical region of sky.
Each of these images is captured through a different filter.
The images contain approximately 500 detectable light sources.

For star/galaxy classification in SDSS data, MCMC outperforms VI at some thresholds and performs slightly worse than VI at others (Figure~\ref{roc-real}). In addition to point estimates, our inference procedures approximate posterior uncertainty for source type (star or galaxy), flux, and colors. This is a novel feature of a Bayesian approach, offering astronomers a principled measure of the quality of inference for each light source; Photo gives only conditional uncertainty estimates.

The MCMC procedure is certain ($> 99\%$ certainty) about the classification (star vs. galaxy) for 321 out of 385 light sources.
Of these classifications, 319 (99.4\%) are correct. Of the remaining classifications (>1\% uncertainty), 50 (78.1\%) are correct.
The VI procedure is certain ($> 99\%$ certainty) about the classification for 322 out of 385 light sources.
Of these classifications, 318 (98.8\%) are correct. Of the remaining classifications (>1\% uncertainty), 53 (84.1\%) are correct.

Table~\ref{tab:err-s82} quantifies point-estimate error from MCMC and VI for the real-valued latent random variables, as well as providing a paired error comparison between each method.
Point-estimate errors for MCMC and VI differed significantly only for galaxy profile and galaxy axis ratio.
For galaxy axis, MCMC outperformed VI, repeating our experience with synthetic data.
For galaxy profile, however, VI outperformed MCMC---the opposite of how the methods compared on synthetic data.
Sampler diagnostics, though not conclusive, suggest that insufficient mixing was not to blame.
Model misfit, though an obvious explanation for any result not shared by synthetic data, seems inadequate because MCMC recovered the other galaxy shape parameters at least as well as VI.

Our leading explanation is that ``ground truth'' is unreliable for galaxy profile, and that VI more accurately recreates the ground-truth mistakes.
Recall ground truth is determined by additively combining many overlapping images.
These images were taken through a variety of atmospheric conditions.
Errors in the point-spread function (PSF) are likely compounded by the addition of more data.
Galaxy profile may be particularly susceptible to errors in the PSF because it has the capacity to model image blur that should have been attributed to the PSF.

For SDSS images, MCMC had better calibrated uncertainty estimates, particularly for log flux (Figure~\ref{qq-real}, Figure~\ref{uq-real}, and Table~\ref{tab:calibration-s82}).
Recall that on the synthetic data, MCMC substantially outperformed VI at modeling uncertainty, producing empirical uncertainties that followed their theoretical distribution almost exactly (Table~\ref{tab:calibration-synth}).
On real data, uncertainty estimates for both MCMC and VI are worse than on synthetic data.
Model misspecification appears to have an effect on MCMC that is comparable to the effect of the bias introduced by the independence assumptions of the variational distribution.

Table~\ref{tab:err-s82} also shows that both MCMC and VI have lower error than Photo (previous work) on many metrics. It should be noted, however, that Photo does not make use of prior information, whereas both MCMC and VI do. For many downstream applications, something like Bayesian shrinkage (e.g., via corrections for Eddington bias, or use of default or empirical priors in a Bayesian setting) would first be applied to Photo's estimates---our comparison is not directly applicable for these applications. For the downstream application of selecting spectrograph targets, Photo's estimates are typically used without adjusting for prior information. For this application our results suggest that either our VI or our MCMC procedure may work better than Photo.
Hence, these results, though suggestive, do not conclusively establish that our method outperforms Photo.


\begin{figure}
  \centering
  \begin{subfigure}{.45\textwidth}
    \centering
    \includegraphics[width=\linewidth]{plots-s82/pstar_roc_comparison.png}
  \end{subfigure}
  \caption{The receiver operating characteristic (ROC) curve for star/galaxy classification on Stripe 82 data. The area under the curve (AUC) for MCMC is 0.991 and for VI is 0.985.
  }
\label{roc-real}
\end{figure}

\begin{table}
  \centering
  \scalebox{.85}{
  \input{plots-s82/error_vb_mc_comparison-pair.tex}
  }
  \vspace{0.5em}
  \caption{%
\textit{Left columns:} Mean absolute error on Stripe 82 data.
\textit{Right columns:}~Pairwise error differences for each pair of methods (and standard error).
Statistically significant differences appear in bold font.}
\label{tab:err-s82}
\end{table}

\begin{figure}
\centering
\begin{subfigure}{.49\textwidth}
  \centering
  \includegraphics[width=\linewidth]{plots-s82/error-scatter-log_flux_r-star.png}
  \caption{log flux (stars)}
\end{subfigure}
\begin{subfigure}{.49\textwidth}
  \centering
  \includegraphics[width=\linewidth]{plots-s82/error-scatter-color_ri-star.png}
  \caption{color r-i (stars)}
\end{subfigure}
\\
\begin{subfigure}{.49\textwidth}
  \centering
  \includegraphics[width=\linewidth]{plots-s82/error-scatter-log_flux_r-gal.png}
  \caption{log flux (galaxies)}
\end{subfigure}
\begin{subfigure}{.49\textwidth}
  \centering
  \includegraphics[width=\linewidth]{plots-s82/error-scatter-color_ri-gal.png}
  \caption{color r-i (galaxies)}
\end{subfigure}
\caption{VI and MCMC performance on real data from Stripe 82.  Each pair depicts VI (left, blue) and MCMC (right, orange), with the ground truth along the $x$-axis and the posterior distribution (showing equal-tailed 95.4\% credible intervals) along the $y$-axis.}
\label{qq-real}
\end{figure}

\begin{table}
\scalebox{.75}{
\input{plots-s82/uscore_vb.tex}
}~
\scalebox{.75}{
\input{plots-s82/uscore_mc.tex}
}
\vspace{.5em}
\caption{%
Proportion of light sources having posterior means found by VI (left) and MCMC (right) near the ground truth for SDSS images.
Credible interval widths match standard deviations as described in Table~\ref{tab:calibration-synth}.}
\label{tab:calibration-s82}
\end{table}

\begin{figure}[b]
\begin{subfigure}{.32\textwidth}
  \centering
  \includegraphics[width=\linewidth]{plots-s82/posterior-comparison-star-log_flux_r-src-28.png}
  \caption{}
\end{subfigure}
\begin{subfigure}{.32\textwidth}
  \centering
  \includegraphics[width=\linewidth]{plots-s82/posterior-comparison-star-log_flux_r-src-38.png}
  \caption{}
\end{subfigure}
\begin{subfigure}{.32\textwidth}
  \centering
  \includegraphics[width=\linewidth]{plots-s82/posterior-comparison-gal-log_flux_r-src-249.png}
  \caption{}
\end{subfigure}
  \caption{Comparison of posterior uncertainty for the flux of three light sources from Stripe 82 where the posterior mean is a poor prediction of the true parameter value. VI underestimates posterior uncertainty. MCMC assigns much greater posterior density to the true values.}
\label{uq-real}
\end{figure}


\subsection{Runtime comparison}
MCMC took approximately $1000\times$ longer in wall-clock time than VI to attain good results.
The implementations for MCMC and VI were both carefully optimized for speed, to make their runtimes comparable. In fact, the majority of runtime for MCMC was spent in code also used by VI, since the most computationally intensive calculations (across pixels) are shared by both the variational lower bound and the log likelihood function. This largely rules out ``implementation differences'' as an explanation for the disparity in runtime.

The same hardware was used for all timing experiments: a single core of an Intel Xeon E5-2698 v3 clocked at 2.30GHz.

Our MCMC experiments use a temperature schedule of length 200 for annealed importance sampling (AIS).
We repeated AIS 25 times to generate 25 independent estimates of the normalizing constant for each model.
We then ran each of these 25 independent posterior samples for 25 more slice sampling steps, generating 625 correlated samples.
For MCMC, the number of samples drawn scales linearly with runtime, presenting a speed/accuracy trade-off.
However, the quality of an MCMC posterior approximation is a function of the number of effectively independent samples~\citep{gelman2014bayesian}.
We measure the rate at which slice sampling is able to compute effectively independent samples for a single source ($52\times 52$ image patch).
For stars, we compute 0.225 effectively independent samples per second.
For galaxies, we compute 0.138 effectively independent samples per second.
%
VI is able to compute an approximate posterior distribution for one light source in 9 seconds, on average, for a region of sky imaged once in each of five filter bands. This runtime holds for either synthetic or SDSS data; runtime is largely determined by the number of pixels.


\subsection{Deblending}
\begin{figure}
	\begin{subfigure}{0.45\textwidth}
		\includegraphics[width=1.8in]{figures/three_sources_in_a_row}
		\subcaption{Two galaxies and one star. Their centers are on a line.}
		\label{three_sources_in_a_row}
	\end{subfigure}
	\hfill
	\begin{subfigure}{0.45\textwidth}
		\includegraphics[width=1.8in]{figures/three_sources_two_overlap}
		\subcaption{Two stars and one galaxy, all having 10-nanomaggy flux density.}
		\label{three_sources_two_overlap}
	\end{subfigure}
	\caption{Simulated astronomical images from GalSim.}
	\label{galsim_images}
\end{figure}

For the proposed model, overlapping light sources are not a special case requiring special processing logic.
Existing cataloging pipelines, on the other hand, invoke specialized ``deblending'' routines to deal with overlapping light sources, to avoid, for example, double counting photons.
In this section, we evaluate our procedure using simulated astronomical images from GalSim~\citep{rowe2015galsim}.
Using simulated rather than real data is particularly important for deblending experiments, because ground truth is particularly difficult to establish for overlapping light sources.
In contrast to our synthetic data (Section~\ref{sec:synth}), the simulated data is not drawn from our model, so there is the potential for model misfit.%
%\footnote{A Jupyter notebook showing our experiments with GalSim is posted at \url{https://github.com/jeff-regier/Celeste.jl/blob/master/experiments/galsim.ipynb}}

First, we consider images where three or more peaks in a blend appear in a straight line,
because this case was the ``single biggest failure mode'' for the deblending algorithm used by the Hyper Suprime-Cam (HSC) software pipeline~\citep{bosch2018hyper}.
To verify that this represents no special challenge to our model, we generated the astronomical image in Figure~\ref{three_sources_in_a_row}.
The correct r-band fluxes of the light sources, ordered from bottom to top, are 10 nanomaggies, 3 nanomaggies, and 3 nanomaggies.
Our VI procedure correctly classifies all three and determines that their respective flux densities are 9.98 nanomaggies, 2.90 nanomaggies, and 3.01 nanomaggies. The classifications are correct (assigning greater than 99\% probability to the truth), and mean galaxy angles are both within a few degrees of the truth.
We do not report the HSC pipline's estimation on this image because we could not get it to run without errors.

Second, we consider images with more severe blending and compare our algorithm to SExtractor~\citep{bertin1996sextractor}.
Unlike the SDSS and HSC pipelines, SExtractor is relatively straightforward to run on new data.
Recently released Python bindings make using it particularly straightforward~\citep{barbary2016sep}.
SExtractor is among the most used cataloging software today.

Figure~\ref{three_sources_two_overlap} shows a second simulated image we used for testing.
These light sources all have high flux density---10 nanomaggies each.
The approximate posterior mean recovered by our VI procedure assigns  9.87 nanomaggies, 9.95 nanomaggies, and 10.12 nanomaggies to these light sources.
SExtractor, on the other hand, estimates their flux densities to be 10.85 nanomaggies, 12.81 nanomaggies, and 14.91 nanomaggies.

\cite{melchior2018scarlet} propose a new deblending algorithm, called SCARLET, and report improvements over the HSC approach to deblending.
SCARLET appears at first glance to be quite different from our approach:
it is based on non-negative matrix factorization (NMF) rather than Bayesian statistics.
However, NMF algorithms can be cast as computing a maximum a posteriori (MAP) estimate under some assumptions on the distribution of the data and the factors~\citep{schmidt2009bayesian}, so SCARLET may have some similarity to what we propose.

