\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{enumitem}
\usepackage{multirow}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
%\ifcvprfinal\pagestyle{empty}\fi
% \setcounter{page}{4321}
\pagenumbering{gobble}
\begin{document}

%%%%%%%%% TITLE
%\title{Single Image Dehazing Using Perceptual Losses}

\title{Improved Techniques for Learning to Dehaze and Beyond: A Collective Study}
\author{Yu Liu\textsuperscript{1}, Guanlong Zhao\textsuperscript{2}, Boyuan Gong\textsuperscript{2}, Yang Li\textsuperscript{1}, Ritu Raj\textsuperscript{2}, Niraj Goel\textsuperscript{2}, Satya Kesav\textsuperscript{2}, \\ Sandeep Gottimukkala\textsuperscript{2}, Zhangyang Wang\textsuperscript{2}, Wenqi Ren\textsuperscript{3}, Dacheng Tao\textsuperscript{4}\\
\textsuperscript{1}Department of Electrical and Computer Engineering, Texas A\&M University\\
\textsuperscript{2}Department of Computer Science and Engineering, Texas A\&M University \\
\textsuperscript{3}Chinese Academy of Sciences \qquad
\textsuperscript{4} 
University of Sydney
%{\tt\small \{yliu129, gzhao\}@tamu.edu}
%
}

\maketitle
%\thispagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}
\vspace{-1em}
Here we explore two related but important tasks based on the recently released REalistic Single Image DEhazing (RESIDE) benchmark dataset: (i) single image dehazing as a low-level image restoration problem; and (ii) high-level visual understanding (e.g., object detection) of hazy images. For the first task, we investigated a variety of loss functions and show that perception-driven loss significantly improves dehazing performance. In the second task, we provide multiple solutions including using advanced modules in the dehazing-detection cascade and domain-adaptive object detectors. In both tasks, our proposed solutions significantly improve performance. GitHub repository URL: \url{https://github.com/guanlongzhao/dehaze}. 

%the challenge on improving hazy image processing in computer vision tasks, for example, object detection, with focus on proposed solutions and results. The challenge is addressed from two different angles by four proposed solutions. The first approach is to improving single image dehazing result with perceptual-motivated loss functions. The second approach takes the hazy image directly as input and boosts the object detection in haze. In the study to improving the results of the second approach, the introduced works exploring different combinations of dehazing models with object detection models and also propose some innovative cascaded dehazing models. Besides, domain adaptation is applied by one work to improve the object detection training results on hazy images. In all mentioned works, the REalistic Single Image DEhazing (RESIDE) dataset is employed. Overall, the approaches gauge the state-of-the-art in learning to dehaze and its application in object detection.
\end{abstract}
\vspace{-1em}
%%%%%%%%% BODY TEXT
\section{Introduction}
\label{Intro}
Images taken in outdoor environments affected by air pollution, dust, mist, and fumes often contain complicated, non-linear, and data-dependent noise, also known as haze. Haze complicates many high-level computer vision tasks such as object detection and recognition. 
%Due to the existence of air pollution, dust, mist, and fumes, images taken in an outdoor environment often contain complicated, non-linear and data-dependent noises, known as haze, which challenges many high-level computer vision tasks such as object detection and recognition. 
%Taking autonomous-driving as an example, hazy or foggy weather will obscure the vision of on-board cameras and create a loss of contrast in the subject with light scattering through the haze particles, adding particular challenges for self-driving navigation. Thus, 
Therefore, dehazing has been widely studied in the fields of computational photography and computer vision. Early dehazing approaches often required additional information such as the provision or capture of scene depth by comparing several different images of the same scene~\cite{tan2000enhancement,schechner2001instant,kopf2008deep}. Many approaches have since been proposed to exploit natural image priors and to perform statistical analyses~\cite{he2011single,tang2014investigating,zhu2015fast,berman2016non}. Most recently, dehazing algorithms based on neural networks \cite{cai2016dehazenet,ren2016single,li2017aod} have delivered state-of-the-art performance. For example, AOD-Net \cite{li2017aod} trains an end-to-end system and shows superior performance according to multiple evaluation metrics, improving object detection in the haze using end-to-end training of dehazing and detection modules.
%both computational photography and computer vision fields. Early approaches of dehazing often require additional information such as scene depth to be given or captured from comparing multiple different images of the same scene~\cite{tan2000enhancement,schechner2001instant,kopf2008deep}. Later on, many approaches are proposed to exploit natural image priors and perform statistical analysis~\cite{he2011single,tang2014investigating,zhu2015fast,berman2016non}. Most recently, dehazing algorithms based on neural networks \cite{cai2016dehazenet,ren2016single,li2017aod} have shown state-of-the-art performance. For example, AOD-Net \cite{li2017aod} shows superior performance under multiple evaluation metrics by training an end-to-end system. Furthermore, the authors have shown to improve object detection in the haze, using end-to-end training of dehazing and detection modules.

%AOD-Net minimizes the $\ell_2$ norm of the difference between the haze and clean images.

%While these methods can effectively enhance the visibility of hazy images, their tractability is limited since the required additional information or multiple images are not always available in practice. 

%To address this problem, a single-image dehazing system, which aims at restoring the underlying clean image from a observed hazy image, is more feasible for real application and has received an increased interests in recent years. Traditional single-image dehazing methods exploit natural image prior and perform statical analysis~\cite{he2011single,tang2014investigating,zhu2015fast,berman2016non}. More recently, dehazing algorithms based on neural networks \cite{cai2016dehazenet,ren2016single,li2017aod} have shown state-of-the-art performance, among which the AOD-Net \cite{li2017aod} has the ability to train an end-to-end system while outperforming the others on multiple evaluation metrics. AOD-Net minimizes the $\ell_2$ norm of the difference between the haze and clean images. %However, the $\ell_2$ norm suffers from a few known issues that may harness the performance for a maximum optimized and perceptual clean image. First, $\ell_2$ norm assumes a white Gaussian noise, which is not suitable for the dehazing task. Second, $\ell_2$ norm does not correlate well with the human perception of image quality \cite{zhang2012comprehensive}. Last, $\ell_2$ treats the impact of noise on the local characteristics of an image independently. However, according to \cite{wang2004image}, the sensitivity of the Human Visual System to noise depends on the local structure of a vision. 



%------------------------------------------------------------------------
\section{Review and Task Description}

Here we study two haze-related tasks: 1) boosting single image dehazing performance as an image restoration problem; and 2) improving object detection accuracy in the presence of haze. As noted by \cite{wang2016studying,li2017aod,li2017reside}, the second task is related to, but is often unaligned with, the first.

While the first task has been well studied in recent works, we propose that \textbf{the second task is more relevant in practice and deserves greater attention}. Haze does not affect human visual perceptual quality as much as resolution, noise, and blur; indeed, some hazy photos may even have better aesthetics. However, haze in unconstrained outdoor environments could be detrimental to machine vision systems, since most of them only work well for haze-free scenes. Taking autonomous driving as an example, hazy and foggy weather will obscure the vision of on-board cameras and create confusing reflections and glare, creating problems even for state-of-the-art self-driving cars \cite{li2017reside}.


%We work on two tasks related to haze: 1) boosting the performance of single image dehazing, as an image restoration problem; 2) improving object detection accuracy in the presence of haze. As pointed out by \cite{wang2016studying,li2017aod,li2017reside}, the second task is related to, but is often unaligned with the first task.

%While the first task has been widely studied in recent work, we would like to advocate that \textbf{the second task demands more attentions in practice since our final goal is often to ensure that machine vision systems understand hazy scenes better.}. Unlike low-resolution, noise, or blur, the existence of haze does not hurt the human visual perception quality as much (some hazy photos may even gain extra aesthetic values); however, haze as part of unconstrained outdoor environments could be detrimental to machine vision systems, most of which only work well with the haze-free scene radiance. Taking autonomous driving for example, hazy and foggy weather will obscure the vision of on-board cameras and create confusing reflections and glare, leaving state-of-the-art self-driving cars in struggle \cite{li2017reside}. 

%The objectives of the works introduced in this paper can be divided into two tracks, which benefit the potential hazy image involved computer vision tasks from two aspects: 1) boosting the performance of single image dehazing, and 2) improving the object detection accuracy in the hazy environment. In this section, we briefly introduce the background knowledge, dataset, and evaluation of the mentioned works.

%\subsection{Haze Model and Dehazing Approaches}
\subsection{Haze Modeling and Dehazing Approaches}
The atmospheric scattering model has been widely used to represent hazy images in haze removal works~\cite{mccartney1976optics,narasimhan2000chromatic,narasimhan2002vision}:


\begin{equation}
\label{haze model definition}
I(x) = J(x)t(x) + A(1-t(x)),
\end{equation}

\noindent where $x$ indexes pixels in the observed hazy image, $I(x)$ is the observed hazy image, and $J(x)$ is the clean image to be recovered. The parameter $A$ denotes the global atmospheric light, and $t(x)$ is the transmission matrix defined as:
\begin{equation}
\label{trasmission matrix}
t(x) = e^{-\beta d(x)},
\end{equation}
\noindent where $\beta$ is the scattering coefficient, and $d(x)$ represents the distance between the object and camera.

Conventional single image dehazing methods commonly exploit natural image priors (for example, the dark channel prior (DCP)~\cite{he2011single,tang2014investigating}, the color attenuation prior~\cite{zhu2015fast}, and the non-local color cluster prior~\cite{berman2016non}) and perform statistical analysis to recover the transmission matrix $t(x)$. More recently, convolutional neural networks(CNNs) have been applied for haze removal after demonstrating success in many other computer vision tasks. Some of the most effective models include the multi-scale CNN (MSCNN) which predicts a coarse-scale holistic transmission map of the entire image and refines it locally~\cite{ren2016single}; DehazeNet, a trainable transmission matrix estimator that recovers the clean image combined with estimated global atmospheric light~\cite{cai2016dehazenet}; and the end-to-end dehazing network, AOD-Net \cite{li2017aod,li2017all}, which takes a hazy image as input and directly generates a clean image output. AOD-Net has also been extended to video \cite{li2017end}.

\subsection{RESIDE Dataset}
We benchmark against the REalistic Single Image DEhazing (RESIDE) dataset~\cite{li2017reside}. RESIDE was the first large-scale dataset for benchmarking single image dehazing algorithms and includes both indoor and outdoor hazy images%\footnote{RESIDE dataset was updated once in March 2018, with some changes on the dataset organization. Experiments conducted in this paper were all on the original RESIDE version, now called \href{https://sites.google.com/view/reside-dehaze-datasets}{RESIDE-v0}.}.
\footnote{The RESIDE dataset was updated in March 2018, with some changes made to dataset organization. Our experiments were all conducted on the original RESIDE version, now called \href{https://sites.google.com/view/reside-dehaze-datasets}{RESIDE-v0}.}.
Further, RESIDE contains both synthetic and real-world hazy images, thereby highlighting diverse data sources and image contents. It is divided into five subsets, each serving different training or evaluation purposes. RESIDE contains $110,500$ synthetic indoor hazy images (ITS) and $313,950$ synthetic outdoor hazy images (OTS) in the training set, with an option to split them for validation. The RESIDE test set is uniquely composed of the synthetic objective testing set (SOTS), the annotated real-world task-driven testing set (RTTS), and the hybrid subjective testing set (HSTS) containing $1,000$, $4,332$, and $20$ hazy images, respectively. The three test sets address different evaluation viewpoints including restoration quality (PSNR, SSIM and no-reference metrics), subjective quality (rated by humans), and task-driven utility (using object detection, for example). 

Most notably, RTTS is the only existing public dataset that can be used to evaluate object detection in hazy images, representing mostly real-world traffic and driving scenarios. Each image is annotated with object bounding boxes and categories (person, bicycle, bus, car, or motorbike). $4,807$ unannotated real-world hazy images are also included in the dataset for potential domain adaptation.
 
For Task 1, we used the training and validation sets from ITS + OTS, and the evaluation is based on PSNR and SSIM. For Task 2, we used the RTTS set for testing and evaluated using mean average precision (MAP) scores.

%All our works are benchmarked with the REalistic Single Image DEhazing (RESIDE) dataset~\cite{li2017reside}. RESIDE is the first large-scale dataset for benchmarking single image dehazing algorithms, with both indoor and outdoor hazy images\footnote{RESIDE dataset was updated once in March 2018, with some changes on the dataset organization. Experiments conducted in this paper were all on the original RESIDE version, now called \href{https://sites.google.com/view/reside-dehaze-datasets}{RESIDE-v0}.}. Consisting of both synthetic and real-world hazy images, the RESIDE dataset highlights diverse data sources and image contents and is divided into five subsets, each serving different training or evaluation purposes. It has $110,500$ synthetic indoor hazy images (ITS) and $313,950$ synthetic outdoor hazy images (OTS) in the training set, with an option to be split for validation as well. The RESIDE testing set is uniquely composed of Synthetic Objective Testing Set (SOTS), annotated Real-world Task-driven Testing Set (RTTS), and Hybrid Subjective Testing Set (HSTS) which has $1,000$, $4,332$, and $20$ hazy images correspondingly. The three testing sets address different evaluation viewpoints, ranging from restoration quality (PSNR, SSIM and no-reference metrics), subject quality (rated by human), and task-driven utility (using object detection for example). 

%Most notably, RTTS is the only existing public set that can be used towards evaluating object detection in hazy images, covering mostly real-world traffic and driving scenarios. Each image is annotated with object bounding boxes, with categories from person, bicycle, bus, car, and motorbike. In addition, $4,807$ unannotated real-world hazy images are included in RESIDE for potential domain adaption.
 
%For Task 1, we use the training and validation sets from ITS + OTS, and adopt PSNR and SSIM as the evaluation metrics.. For Task 2, we use the RTTS set for testing, and evaluate with Mean Average Precision (MAP) scores.


%\subsection{Challenge Tracks and Evaluation Strategies}

% \subsubsection{Track 1:Improve Image Dehazing}

% The goal for this track is to as perfectly recover the clean images from the hazy ones as possible. The training and the validation sets are from the RESIDE synthetic subset (ITS + OTS) and the evaluation is based on the synthetic testing set (SOTS). Widely adopted PSNR and SSIM objective metrics are used to measure the task performance.

% \subsubsection{Track 2:Improve Object Detection in Haze}

% In this track, the goal is to boost the visual understanding performance by directly utilizing visual data captured in hazy environments. For demonstration purpose, object detection is chosen as the representative task example. Given different objectives, the testing dataset and performance evaluation strategy is different from that of track 1. The Real-world Task-driven Testing Set (RTTS) in RESIDE, which has $4,332$ outdoor images, is the testing set and the models are evaluated in terms of mAP (MMean Average Precision) scores. Specifically, RTTS covering mostly real-world traffic and driving scenarios. Each image is annotated with object categories chosen from person, bicycle, bus, car, motorbike, and the objects are bounded with boundary boxes.


%---------------------------------------------------
\section{Task 1: Dehazing as Restoration}

Most CNN dehazing models \cite{cai2016dehazenet,ren2016single,li2017aod} refer to the mean-squares error (MSE) or $\ell_2$ norm-based loss functions. However, MSE is well-known to be imperfectly correlated with human perception of image quality \cite{zhang2012comprehensive,zhao2017loss}. Specifically, for dehazing, 
% However, the $\ell_2$ norm suffers from a handful of known limitations that may leave the dehazed image output of the AOD-Net away from the optimal quality, especially considering about its correlation with human perception of image quality\cite{zhang2012comprehensive}. 
% on the one hand, 
the $\ell_2$ norm implicitly assumes that the degradation is additive white Gaussian noise, which is oversimplified and invalid for haze. %On the other hand, 
Conversely, $\ell_2$ treats the impact of noise independently of the local image characteristics such as structural information, luminance and contrast. However, according to \cite{wang2004image}, the sensitivity of the Human Visual System (HVS) to noise depends on the local properties and structure of a vision. 
 
%add justification/motivations of the idea in the first paragraph
%This work proposed the \underline{P}erception-\underline{A}ided Single Image \underline{D}ehazing Network: \textit{PAD-Net} to 
%In this study, 
Here we aimed to identify loss functions that better match human perception to train a dehazing neural network. We used AOD-Net~\cite{li2017aod} (originally optimized using MSE loss) as the backbone but replaced its loss function with the following options: 

%This is based on the know limitations of the $\ell_2$ norm that may leave the dehazed image output of the AOD-Net away from the optimal quality, especially considering about its correlation with human perception of image quality\cite{zhang2012comprehensive}. On the one hand, $\ell_2$ norm implicitly assumes a white Gaussian noise, which is an oversimplified case that is not valid in general dehazing cases. On the other hand, $\ell_2$ treats the impact of noise independently to the local characteristics, such as structural information, luminance and contrast, of an image. However, according to \cite{wang2004image}, the sensitivity of the Human Visual System (HVS) to noise depends on the local properties and structure of a vision. 



% The implemented deep neural network adopts from the AOD-Net~\cite{li2017aod}, which is composed of two major parts: a K-estimation module to estimate $K(x)$ with five convolutional layers, and a clean image generation modules that follows to produce the recovery clean image via element-wise calculation. The entire network diagram of the PAD-Net is visualized in Fig.~\ref{network_architecture}. 

% \begin{figure*}
% \centering
% \includegraphics[width=7.2in,height=1.7in]{figures/network_architecture.eps}
% \caption{The network diagram of PAD-Net}
% \label{network_architecture}
% \end{figure*}

% Output images from the network is compared with the ground truth clean image at the loss layer to compute the error function for back propagation. Different error functions inspired from~\cite{zhao2017loss} are investigated to optimize the image dehazing results and the results are compared to select the best approach to do fine-tuning. These adopted loss functions are briefly introduced in the following:

\begin{itemize}[noitemsep,topsep=2pt,parsep=2pt,partopsep=2pt]

% % $\ell_2$ norm
% \item \textbf{Baseline}: The $\ell_2$ loss.
% The loss function for a patch $P$ can be written as:

% \begin{equation}
% \label{l2 error function}
% \mathcal{L}^{\ell_2}(P) = \frac{1}{N}\sum_{p \in P}^{} (x(p)-y(p))^2,
% \end{equation}

% where $N$ is the number of pixels in the patch, $p$ is the index of the pixel, and $x(p)$ and $y(p)$ are the pixel values of the generated image and the ground truth image respectively.

% % $\ell_1$ norm
\item \textbf{$\ell_1$ loss}: The $\ell_1$ loss for a patch $P$ can be written as:

\begin{equation}
\label{l1 error function}
\mathcal{L}^{\ell_1}(P) = \frac{1}{N}\sum_{p \in P}^{} |x(p)-y(p)|.
\end{equation}
where $N$ is the number of pixels in the patch, $p$ is the index of the pixel, and $x(p)$ and $y(p)$ are the pixel values of the generated image and the ground truth image respectively.
% SSIM
\item \textbf{SSIM loss}: Following \cite{zhao2017loss}, we write the SSIM for pixel $p$ as:
\begin{equation}
\label{SSIM}
\begin{split}
SSIM(p) & = \frac{2\mu_x\mu_y + C_1}{\mu_x^2+\mu_y^2+C_1}\cdot \frac{2\sigma_{xy}+C_2}{\sigma_x^2 + \sigma_y^2+C_2} \\
& =l(p)\cdot cs(p).
\end{split}
\end{equation}
The means and standard deviations are computed using a Gaussian filter with standard deviation $\sigma_G$. The loss function for SSIM can then be defined as:
\begin{equation}
\label{SSIM error function}
\mathcal{L}^{SSIM}(P) = \frac{1}{N}\sum_{p \in P}^{} 1 - SSIM(p).
\end{equation}

% MS-SSIM
\item \textbf{MS-SSIM loss}: 
The choice of $\sigma_G$ would impact the training performance of SSIM. Here we adopt the idea of multi-scale SSIM \cite{zhao2017loss}, where $M$ different values of $\sigma_G$ are pre-chosen and fused:
%used rather than directly fine tune its value:
\begin{equation}
\label{MS-SSIM}
\mathcal{L}^{MS-SSIM}(P) = l_M^\alpha(p) \cdot \prod_{j=1}^{M} cs_j^{\beta_j} (P).
\end{equation}

\item \textbf{MS-SSIM+$\ell_2$ Loss}: using a weighted sum of MS-SSIM and $\ell_2$ as the loss function:

\begin{equation}
\label{MSSSIM-L2}
\mathcal{L}^{MS-SSIM-\ell_2} = \alpha \cdot \mathcal{L}^{MSSSIM} + (1-\alpha)\cdot G_{\sigma_{G}^M} \cdot \mathcal{L}^{\ell_2},
\end{equation}

\noindent a point-wise multiplication between $G_{\sigma_{G}^M}$ and $\mathcal{L}^{\ell_2}$ is added for the $\ell_2$ loss function term, because MS-SSIM propagates the error at pixel $q$ based on its contribution to MS-SSIM of the central pixel $\widetilde q$, as determined by the Gaussian weights.

\item \textbf{MS-SSIM+$\ell_1$ loss}: using a weighted sum of MS-SSIM and $\ell_1$ as the loss function:
\begin{equation}
\label{MSSSIM-L1}
\mathcal{L}^{MSSSIM-\ell_1} = \alpha \cdot \mathcal{L}^{MSSSIM} + (1-\alpha)\cdot G_{\sigma_{G}^M} \cdot \mathcal{L}^{\ell_1},
\end{equation}
\noindent the $\ell_1$ loss is similarly weighted by $G_{\sigma_{G}^M}$.

\end{itemize}

We selected 1,000 images from ITS + OTS as the validation set and the remaining images for training. The initial learning rate and mini-batch size of the systems were set to $0.01$ and $8$, respectively, for all methods. All weights were initialized as Gaussian random variables, unless otherwise specified. We used a momentum of $0.9$ and a weight decay of $0.0001$. We also clipped the $\ell_2$ norm of the gradient to be within [-0.1, 0.1] to stabilize network training. All models were trained on an Nvidia GTX 1070 GPU for around 14 epochs, which empirically led to convergence. For SSIM loss, $\sigma_G$ was set to 5. $C_1$ and $C_2$ in (\ref{SSIM}) were 0.01 and 0.03, respectively. For MS-SSIM losses, the multiple Gaussian filters were constructed by setting $\sigma_G^i=\{0.5, 1, 2, 4, 8\}$. $\alpha$ was set as 0.025 for MS-SSIM+$\ell_1$, and 0.1 for MS-SSIM+$\ell_2$, following \cite{zhao2017loss}. 

As shown in Tables \ref{psnr-res-overall} and \ref{ssim-res-overall}, simply replacing the loss functions resulted in noticeable differences in performance. While the original AOD-Net with MSE loss performed well on indoor images, it was less effective on outdoor images, which are usually the images needing to be dehazed in practice. Of all the options, MS-SSIM-$\ell_2$ achieved both the highest overall PSNR and SSIM results, resulting in 0.88 dB PSNR and 0.182 SSIM improvements over the state-of-the-art AOD-Net. We further fine-tuned the MS-SSIM-$\ell_2$ model, including using a pre-trained AOD-Net as a warm initialization, adopting a smaller learning rate (0.002) and a larger minibatch size (16). Finally, the best achievable PSNR and SSIM were 23.43 dB and 0.8747, respectively. Note that the best SSIM represented a nearly 0.02 improvement over AOD-Net. 
%As shown in Tables \ref{psnr-res-overall} and \ref{ssim-res-overall}, simply replacing loss functions will lead to noticeable performance variations. While the original AOD-Net with MSE loss performs well on indoor images, it appears less effective on outdoor images, which are more practical subjects for dehazing. Among all options, MS-SSIM-$\ell_2$ achieves both the highest overall PSNR and SSIM results, resulting in 0.88 dB PSNR and 0.182 SSIM improvements over the state-of-the-art AOD-Net. We further fine-tuned the MS-SSIM-$\ell_2$ model, including using a pre-trained AOD-Net as warm initialization, adopting a smaller learning rate (0.002) and a larger mini-batch size (16). Finally, the best achievable PSNR and SSIM results are 23.43 dB and 0.8747, respectively. Note that the best SSIM improves nearly 0.02 over AOD-Net.

% %%%%%implementation details: include dataset and tuning tricks
% To train the network, $2,790$ IST and $7,210$ OTS images in RESIDE dataset are randomly selected and another 1,000 non-overlapping set of images are randomly sampled as the validation data. The systems were evaluated on the assigned SOTS subset, which contains $1,000$ synthetic haze images ($500$ indoor and $500$ outdoor images). The initial learning rate and mini-batch size of the systems were set to $0.01$ and $8$ respectively. According to the initial results, the model that gives the best performance is the one that implements the loss function of MS-SSIM with $\ell_2$. Then, this model is finely tuned to further improve the dehazing result, in which a pre-trained AOD-Net model \cite{boyiliee} is used as the initial weights of the PAD-Net. A learning rate of $0.002$ and a mini-batch size of $16$ is used to get the final result, which will be reported in Sec.~\ref{results}. Also, the $\alpha$ in Eqn.~\ref{MSSSIM-L2} is set to $0.1$.

\begin{table}[t]
\begin{center}
\begin{tabular}{|l|c|c||c|}
\hline
\multirow{2}{*}{Models}&\multicolumn{3}{c|}{PSNR}\\ \cline{2-4} 
&Indoor&Outdoor&All\\ \hline
AOD-Net Baseline&\textbf{21.01}&24.08&22.55\\ \hline
$\ell_1$&20.27&25.83&23.05\\ \hline
SSIM&19.64&26.65&23.15\\ \hline
MS-SSIM&19.54&\textbf{26.87}&23.20\\ \hline
MS-SSIM+$\ell_1$&20.16&26.20&23.18\\ \hline
MS-SSIM+$\ell_2$&20.45&26.38&\textbf{23.41}\\ \hline
\hline
%20.68 26.18 23.43
MS-SSIM+$\ell_2$ (fine-tuned)&20.68&26.18&\textbf{23.43}\\ \hline
\end{tabular}
\end{center}
\caption{Comparison of PSNR results (dB) for Task 1.}
\label{psnr-res-overall}
\end{table}

\begin{table}[t]
\begin{center}
\begin{tabular}{|l|c|c||c|}
\hline
\multirow{2}{*}{Models}&\multicolumn{3}{c|}{SSIM}\\ \cline{2-4} 
&Indoor&Outdoor&All\\ \hline
AOD-Net Baseline &\textbf{0.8372}&0.8726&0.8549\\ \hline
$\ell_1$&0.8045&0.9111&0.8578\\ \hline
SSIM&0.7940&0.8999&0.8469\\ \hline
MS-SSIM&0.8038&0.8989&0.8513\\ \hline
MS-SSIM+$\ell_1$&0.8138&\textbf{0.9184}&0.8661\\ \hline
MS-SSIM+$\ell_2$&0.8285&0.9177&\textbf{0.8731}\\ \hline
\hline
% 0.8229 0.9266 0.8747
MS-SSIM+$\ell_2$ (fine-tuned)&0.8229 & \textbf{0.9266} &\textbf{0.8747}\\ \hline
\end{tabular}
\end{center}
\caption{Comparison of SSIM results for Task 1.}
\label{ssim-res-overall}
\end{table}

%Table.~\ref{dehaze result table} compares the dehaze results of different approaches on SOTS using two full-reference metrics, PSNR and SSIM. The results of other models are reported in \cite{li2017reside}. It can be found that the PAD-Net outperforms both PSNR and SSIM among all single image dehazing models with an improvement on the PSNR and SSIM by $10.83\%$ and $2.86\%$ respectively compared to the corresponding best results among the counterpart models.



\section{Task 2: Dehazing for Detection}
% In the task of object detection in haze, there are two additional challenges compared to the general object detection tasks which works on the clean images. On the one hand, the presence of haze degrade the image quality and may obscure the boundary of the object to be detected, which makes the decision making more difficult. On the other hand, most of the object detection models are trained based on the clean image input samples and there might exist the distribution gap between the dehazed images and the clean images even though the dehazing process is targeted at restore the clean image as much as possible. Both issues are tackled by different groups that are introduced in this section, and the results show that they have made some progress on solving these problems. The results of all three teams will be presented in Sec.~\ref{results}

\subsection{Solution Set 1: Enhancing Dehazing and/or Detection Modules in the Cascade}  %% Satya & Sandeep

In \cite{li2017aod}, the authors proposed a cascade of AOD-Net dehazing and Faster-RCNN \cite{ren2015faster} detection modules to detect objects in hazy images. We therefore considered it intuitive to try different combinations of more powerful dehazing/detection modules in the cascade. Note that such a cascade could be subject to further joint optimization, as many previous works \cite{liu2017image,cheng2017robust,li2017aod}. However, \textbf{to be consistent with the results in} \cite{li2017reside}, all detection models used in this section were the original pre-trained versions, \textit{without any re-training or adaptation}.
%In \cite{li2017aod}, the authors proposed a cascade of AOD-Net dehazing and Faster-RCNN \cite{ren2015faster} detection modules, for detecting objects in hazy images. A natural idea would thus be to try different combinations of more powerful dehazing/detection modules in the cascade. Notice that such a cascade could have been subject to further joint optimization as many previous works \cite{liu2017image,cheng2017robust,li2017aod} indicated. However, \textbf{to be consistent with the comparison results in} \cite{li2017reside}, all detection models used in this section are original pre-trained versions, \textit{without any re-training or adaptation}. 

Our solution set 1 considered several popular dehazing modules including DCP \cite{he2011single}, DehazeNet \cite{cai2016dehazenet}, AOD-Net \cite{li2017aod}, and the recently proposed densely connected pyramid dehazing network (DCPDN) \cite{zhang2018densely}. Since hazy images tend to have lower contrast, we also included a contrast enhancement method called contrast limited Adaptive histogram equalization (CLAHE). Regarding the choice of detection modules, we included Faster R-CNN \cite{ren2015faster}\footnote{We replace the backbone of Faster R-CNN from VGG 16 as used by \cite{li2017reside} with the ResNet101 model \cite{he2016deep} to enhance performance.}, SSD \cite{liu2016ssd}, RetinaNet \cite{lin2017focal}, and Mask-RCNN \cite{he2017mask}. 

The compared pipelines are shown in Table \ref{detection1}. In each pipeline, ``X+Y'' by default means applying Y directly on the output of X in a sequential manner. The most important observation is that simply applying more sophisticated detection modules is unlikely to boost the performance of the dehazing-detection cascade, due to the domain gap between hazy/dehazed and clean images (on which typical detectors are trained). The more sophisticated pre-trained detectors (RetinaNet, Mask-RCNN) may have overfitted the clean image domain, again highlighting the demand of handling domain shifts in real-world detection problems. Moreover, a better dehazing model in terms of restoration performance does not imply better detection results on its pre-processed images (e.g., DPDCN). Further, adding dehazing pre-processing does not always guarantee better detection (e.g, comparing RetinaNet versus AOD-Net + RetinaNet), consistent with the conclusion made in \cite{li2017reside}. In addition, AOD-Net tended to generate smoother results but with lower contrast than the others, potentially compromising detection. Therefore, we created two three-stage cascades as in the last two rows of Table \ref{detection1}, and found that using DCP to process AOD-Net dehazed results (with greater contrast) further marginally improved results. 
%which concurs with the conclusion in \cite{li2017reside}. In addition, we find in experiments that AOD-Net tends to generate smoother, but less-contrast results compared to others, that might potentially hurt detection. To this end, we create two three-stage cascades as in last two rows of Table \ref{detection1}, and found using DCP to process AOD-Net dehazed results (whose contrasts will become stronger) to achieve further marginally improved results. 

\begin{table}[t]
\begin{center}
\begin{tabular}{|l|c|}
\hline
\textbf{Pipelines} & \textbf{mAP} \\ 
 \hline
Faster R-CNN & 0.541 \\ \hline
SSD & 0.556 \\ \hline
RetinaNet & 0.531 \\ \hline
Mask-RCNN & 0.457 \\ \hline
\hline
DehazeNet + Faster R-CNN & 0.557 \\ \hline
AOD-Net + Faster R-CNN & \textcolor{blue}{0.563} \\ \hline
DCP + Faster R-CNN & \textcolor{green}{0.567} \\ \hline
DehazeNet + SSD & 0.554 \\ \hline
AOD-Net + SSD & 0.553\\ \hline
DCP + SSD & 0.557 \\ \hline
AOD-Net + RetinaNet & 0.419 \\ \hline
DPDCN + RetinaNet & 0.543 \\ \hline
DPDCN + Mask-RCNN & 0.477 \\ \hline
\hline
%DCP + AOD-Net + Faster R-CNN & 0.522 \\ \hline
AOD-Net + DCP + Faster R-CNN & \textcolor{red}{0.568} \\ \hline
CLACHE + DCP + Mask-RCNN & 0.551 \\ \hline
\end{tabular}
\end{center}
\caption{Solution set 1 mAP results on RTTS. Top 3 results are colored in red, green, and blue, respectively.}
\label{detection1}
\end{table}



% directly applying pre-trained detection models give an unexpected order of mAP: due to overfitting the clean image domain

% stronger detection helps in the presence of dehazing pre-processing

% %0.5717 by BCCR \cite{meng2013efficient} + SSD-512 \cite{liu2016ssd}


% %In this work, different combinations of representative single image dehazing methods and object detection models are explored and the results are compared. Based on the initial result, the team also propose to cascade dehazing models to provide a better-dehazed input for object detection network thus improves the final results.

% The dehazing algorithms that are implemented in this work include dark channel priority (DCP)~\cite{he2011single}, the AOD-Net~\cite{li2017aod} and the DehazeNet~\cite{cai2016dehazenet}, all of which are representative and out-performed single image dehazing methods. The DCP algorithm identified the unique feature which is commonly applied to the outdoor images and exploit it to directly estimate the haze thickness. image. It is based on the observation that most local patches in outdoor haze free images contain some pixels whose intensity is very low in at least one color channel. In hazy images, the intensity of these dark pixels in that channel is mainly contributed by airlight. The DehazeNet introduced CNNs, which have been already demonstrated success in many other computer vision tasks, into the dehaze work and is designed to embody the established assumptions or priors in image dehazing. It explicitly learns and estimates the mapping relations between hazy image patches and their medium transmissions and recover the haze-free image with separately calculated atmospheric light. While the AOD-Net formulated the atmospheric scattering model in a more end-to-end fashion, thus minimizes the reconstruction distortion and bridges the ultimate target gap between the hazy image and the clean image by training the network to directly generate the dehazed image result.

% Object detection using CNNs try to predict bounding boxes and class probabilities directly from full images~(\cite{ren2015faster,redmon2016you,liu2016ssd}). In this work, the team implemented two efficient models that can perform the object detection at a fast speed. The first model chosen in this work is the Faster R-CNN~\cite{ren2015faster}. The main insight of Faster R-CNN is that it simultaneously predicts object bounds and classification scores at each position and the full image convolutional features are shared by the downstream detection networks. In this way, it largely speeds up the slow selective search algorithms in the region proposal step. On the other hand, the single shot multibox detector (SSD)~\cite{liu2016ssd} speed up the detection process by skipping the region proposal step. Instead, it discretizes the output shape of bounding boxes into a set of default boxes at every single location of the image and considers them simultaneously with its classification. 

% In this work, first, different combinations of dehazing methods and object detection methods are compared to explore the optimal results in the design space of this two-step model for solving the object detection in haze. Then, the team further improve the model by 
% cascading the two dehazing models, AOD-Net and DCP, based on the idea that by increasing the feature complexity of the dehazed image, the information restored from different approaches can supplement each other and perform even better when working together. The final results indicate that, by letting the hazy image process via AOD-Net followed by the DCP model and then use Faster R-CNN to do the object detection, the best detection performance can be achieved.

% To train the network, first, the synthetic hazy images from RESIDE are dehazed by DCP. Then, the resulting dehazed images along with VOC2007~\cite{everingham2008pascal} and VOC2012~\cite{everingham2011pascal} dataset are used so that the network has large enough input images to learn both dehazing and object detection tasks. The Faster R-CNN model is built based on ResNet101~\cite{he2016deep}.


% \subsection{Team 2} %Niraj & Ritu

% %overall motivation
% Similar to team one, the team two also explored different methods of dehazing and object detection and select the best combination which gives the highest mAP. However, instead of considering the object detection on hazy images as a specific integral task and train the entire model together, this team look at the two steps in this task, i.e., dehazing and object detection, in a more separate way and fine-tune each step individually.  After that, the test data is evaluated on the trained models built from the best-performed models from each task.


% %introduce the methods using
% In this team's work, the dehazing model that are exploited are DCP~\cite{he2011single} and AOD-Net~\cite{li2017aod}, which are the two representative dehazing models in conventional approaches that leverage the physical priors and the CNN-based approach which are more data-driven and take the advantage of the powerful learning abilities of deep neural networks.

% As for the object detection, the team tried three state-of-the-art object detection networks, which are Faster R-CNN-ResNet101\textbf{citation missing}, Faster R-CNN-VGG16~\cite{simonyan2014very}, and SSD-VGG16. First, all three models are trained by the entire dataset of VOC2007~\cite{everingham2008pascal} and VOC2012~\cite{everingham2011pascal} and evaluated by the RTTS subset of RESIDE~\cite{li2017reside}. Then, the model which gives the best original object detection result, Faster R-CNN-ResNet101, is further fine-tuned by being trained with the dataset in which synthetic haze are added onto the VOC2007 dataset. In this work, three different artificial hazy training sets are generated by adding different haze noises to the original VOS2007 images. In the first hazy set, light whitening noise is added. In the second set, heavy whitening noise is used. And in the third set, the level of added whitening noise is random. Besides, in the third set, additional random noise is added on top of the whitening noise to exaggerate the diversity of the input hazy images. The type of the random noise is selected from the Gaussian noise, the Poisson noise, and the Speckle noise with a variance of $0.01$ in all cases. Besides, the images are also blurred. 

% %implementation detail.
% In this work, the AOD-Net is trained by the first artificial hazy VOC2007 dataset, in which light whitening noise is added to the image. As for the fine-tuning of the Faster R-CNN-ResNet101, three aforementioned artificial hazy datasets are used separately and the testing performance over RTTS is compared on the fine-tuned object detection network. To train the object detection network, $110,000$ iterations are running with first $80,000$ at a learning rate of $0.001$ and the learning rate of the remaining $30,000$ iterations is reduced to $0.0001$. The batch size is $256$. According to the result, after trained with VOC2007 with heavy white lightning, the model can give the best performance and is selected to combine with dehazing models for the final hazy image detection performance assessment. More result figures will be included in the Sec~\ref{results}.

\subsection{Solution Set 2: Domain-Adaptive Mask-RCNN} %boyuan & yang

Motivated by the observations made on solution set 1, we next aimed to more explicitly tackle the domain gap between hazy/dehazed images and clean images for object detection. Inspired by the recently proposed domain adaptive Faster-RCNN \cite{chen2018domain}, we applied a similar approach to design a domain-adaptive mask-RCNN (DMask-RCNN). 

% The third team tackles the distribution gap between the dehazed images and the clean images and works on solving the challenge of object detection in haze from a different angle than the previous two team. In this work, a domain adaptation classifier is designed to minimize the domain discrepancy at the feature map level. Then, the domain adaption classifier is integrated with the state-of-the-art Mask-RCNN object detection network to build an end-to-end trainable model for hazy image object detection network.

% The object detection model implemented in this work is Mask-RCNN~\cite{he2017mask}. It is a flexible and general framework extended from Faster R-CNN\cite{ren2015faster} by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition and classification. Mask-RCNN is a powerful a structure as it efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance.


%domain adaption
%The main contribution of this work is applying the domain adaption to the object detection in the hazy images. 

%Domain adaptation has been widely used~\cite{ganin2014unsupervised,tzeng2017adversarial,liu2016coupled} that allows people to train a model on the source data distribution with a large amount of labeled data while performing well on a different target data which is trained by a large amount of unlabeled data by extracting the domain-invariant features from both distributions. 

%\paragraph{Model} 
In the model shown in Figure \ref{fig:DMask}, the primary goal of DMask-RCNN is to mask the features generated by feature extraction network to be as domain invariant as possible, between the source domain (clean input images) and the target domain (hazy images). Specifically, DMask-RCNN places a domain-adaptive component branch after the base feature extraction convolution layers of Mask-RCNN. The loss of the domain classifier is a binary cross entropy loss:

\begin{figure}[h]
\centering
\includegraphics[width=3.5in,height=1.5in]{DMask_RCNN.pdf}
\caption{DMask-RCNN structure.}
\label{fig:DMask}
\end{figure}

%As depicted in the figure, the DMask-RCNN is built from a standard Mask-RCNN network along with a domain adaption model after the base feature extraction layer, ResNet101, of the Mask-RCNN. The primary target of Domain adaptive Mask-RCNN is to mask the features generated by feature extraction network as domain invariant as possible.

%Mathematically, 

\begin{equation}
-\sum_{i} (y_{i}log(p_i)+(1-y_i)log(1-p_i)),
\end{equation}
\noindent where $y_i$ is the domain label of the $i_{th}$ image, and $p_i$ is the prediction probability from the domain classifier. The overall loss of DMask-RCNN can therefore be written as:

\begin{equation}
\label{total loss}
\begin{split}
L(\theta_{res}, \theta_{head}, \theta_{domain}) & = L_{C,B}(C,B| \theta_{res}, \theta_{head}, x \in D_s )\\
& - \lambda L_d(G_d| \theta_{res}, x\in D_s, D_t) \\
& + \lambda L_d(G_d| \theta_{domain}, x\in D_s, D_t),
\end{split}
\end{equation}

\noindent where $x$ is the input image, and $D_s$ and $D_t$ represents the source and target domain, respectively. $\theta$ denotes the corresponding weights of each network component. $G$ represents the mapping function of the feature extractor; $I$ is the feature map distribution; $B$ is the bounding box of an object and $C $ is the object class. Note that when calculating the $L_{C, B}$, only source domain inputs will be counted in since the target domain has no labels. 
%half batch size of the feature maps which are from the source domain are used since the inputs from the target domain do not have the ground truth label to calculate $L_{C, B}$.

As seen from Eqn. (\ref{total loss}), the negative gradient of the domain classifier loss needs to be propagated back to ResNet, whose implementation relies on the gradient reverse layer~\cite{ganin2014unsupervised} (GRL, Fig.~\ref{fig:DMask}). The GRL is added after the feature maps generated by the ResNet and feeds its output to the domain classifier. This GRL has no parameters except for the hyper-parameter $\lambda$, which, during forward propagation, acts as an identity transform. However, during back propagation, it takes the gradient from the upper level and multiplies it by $-\lambda$ before passing it to the preceding layers. 

% Therefore, with the gradient reverse layer, the total lost function of the DMask-RCNN can be rewritten as:

% \begin{equation}
% \begin{split}
% L(\theta_{res}, \theta_{head}, \theta_{domain}) & = L_{C,B}(C,B| \theta_{res}, \theta_{head}, x \in D_s )\\
% & - \lambda L_d(G_d| \theta_{res}, x\in D_s, D_t)\\
% & + \lambda L_d(G_d| \theta_{domain}, x\in D_s, D_t) \\
% & + ||w||_2, 
% \end{split}
% \end{equation}

% \noindent where $||w||_2$ is the weight regularization.

% Correspondingly, the weight update are as follows:
% \begin{align} 
% \theta_{head} &= \theta_{head} - \partial L_{C,B} / \partial \theta_{head}\\
% \theta_{domain} &= \theta_{domain} - \partial L_d / \partial \theta_{domain}\\
% \theta_{res} &= \theta_{res} - \partial L(\theta_{res}, \theta_{head}, \theta_{domain}) / \partial \theta_{res}.
% \end{align}



% Besides, $\mathcal{H}$-divergence~\cite{ben2010theory} is used to measure the difference between to distribution by the certain classifier and the DMask-RCNN minimizes the distance between two distributions.

\paragraph{Experiments} To train DMask-RCNN, MS COCO (clean images) were always used as the source domain, while \textbf{two target domain options} were designed to consider two types of domain gap: (1) all unannotated realistic haze images from RESIDE; and (2) dehazed results of those unannotated  images, using MSCNN \cite{ren2016single}. The corresponding DMask-RCNNs are called DMask-RCNN1 and DMask-RCNN2, respectively. 

We initialized the Mask-RCNN component of DMask-RCNN with a pre-trained model on MS COCO. All models were trained for 50, 000 iterations with learning rate 0.001, then another 20, 000 iterations with learning rate 0.0001. We used a naive batch size of 2, including one image randomly selected from the source domain and the other from the target domain, noting that larger batches may further benefit performance. We also tried to concatenate dehazing pre-processing (AOD-Net and MSCNN) with DMask-RCNN models to form new dehazing-detection cascades.

%\footnote{We did not try larger batches, which might benefit performance more.}. In addition, we concatenate dehazing pre-processing (AOD-Net and MSCNN) with DMask-RCNN models to form new dehazing-detection cascades.

\begin{table}[t]
\begin{center}
\begin{tabular}{|l|c|}
\hline
\textbf{Pipelines} & \textbf{mAP} \\ 
 \hline
DMask-RCNN1 & 0.612 \\ \hline
DMask-RCNN2 & 0.617 \\ \hline
\hline
AOD-Net + DMask-RCNN1 & 0.602 \\ \hline
AOD-Net + DMask-RCNN2 & 0.605 \\ \hline
\hline
MSCNN + Mask-RCNN & \textcolor{blue}{0.626} \\ \hline
MSCNN + DMask-RCNN1 & \textcolor{green}{0.627} \\ \hline
MSCNN + DMask-RCNN2 & \textcolor{red}{0.634} \\ \hline
\end{tabular}
\end{center}
\caption{Solution set 2 mAP results on RTTS. Top 3 results are colored in red, green, and blue, respectively.}
\label{detection2}
\end{table}

Table \ref{detection2} shows the results of solution set 2 (the naming convention is the same as in Table \ref{detection1}), from which we can conclude that:

\begin{itemize}
\item the domain-adaptive detector presents a very promising approach, and its performance significantly outperforms the best results in Table \ref{detection1};\footnote{By saying that, we also emphasize that Table \ref{detection1} results have not undergone joint tuning as in \cite{liu2017enhance,li2017aod}, so there is potential for further improvements.} 

\item the power of strong detection models (Mask-RCNN) is fully exploited, given the proper domain adaptation, in contrast to the poor performance of vanilla Mask RCNN in Table \ref{detection1};
%Besides, without any joint tuning, MSCNN seems to benefit detection more than AOD-Net, which is consistent as \cite{li2017reside} observed.

\item DMask-RCNN2 is always superior to DMask-RCNN1, showing that the choice of dehazed images as the target domain matters. We make the reasonable hypothesis that the domain discrepancy between dehazed and clean images is smaller than that between hazy and clean images, so DMask-RCNN performs better when the existing domain gap is narrower; and

\item the best result in solution set 2 is from a dehazing + detection cascade, with MSCNN as the dehazing module and DMask-RCNN as the detection module and highlighting: \textbf{the joint value of dehazing pre-processing and domain adaption}.
\end{itemize}
 




% use a MS COCO-pretrained Mask-RCNN model as initialization We train two types of
% DMask-RCNN. Both are trained with one source dataset
% and one target dataset for 50k iteration with learning rate
% 0.001 and 20k iteration with learning rate 0.0001. The
% batch size is 2 which includes one random image selected
% from domain dataset and one random selected from target
% dataset.Then we measure the mAP on the RTTS (Realworld
% Task-Driven Testing Set) dataset from RESIDE.


% To implement the work, two CNN-based dehazing networks, AOD-Net~\cite{li2017aod} and MSCNN~\cite{ren2016single} models are concatenated with the object detection network respectively and the results are compared. In DMask-RCNN, the weights of the Mask-RCNN branch are pretrained by the Microsoft COCO~\cite{lin2014microsoft} dataset, which is also the source dataset to train the entire DMask-RCNN. As for the target dataset, two different hazy image dataset are exploited. The first target dataset is the original unannotated realistic hazy images from RESIDE. The second target dataset is the corresponding dehazed images of the original ones. The model used to dehaze here is the MSCNN~\cite{ren2016single}. The results show that the model that is trained with the second dataset gives a better performance, which will be reported in Sec.~\ref{results}. The number of training iterations in both scenarios is $70,000$, with the learning rate $0.001$ in the first $50,000$ iterations and $0.0001$ in the remaining $20,000$ iterations. The batch size is $2$, which includes one image randomly selected from the source domain dataset and one randomly selected from target domain dataset.


% \section{Results}
% \label{results}

%In this section, we present the best performance from each team introduced in this report and compare them with some other works on the same problem in the community.

%\subsection{Single Image Dehazing}

% \begin{table*}[t]
% \centering
% \caption{Dehaze Result on SOTS}
% \label{dehaze result table}
% \begin{tabular}{l|l|l|l|l|l|l}
% \hline
%               & \textbf{DCP~\cite{he2011single}} & \textbf{CAP} & \textbf{MSCNN~\cite{ren2016single}} & \textbf{DehazeNet~\cite{cai2016dehazenet}} & \textbf{AOD-Net~\cite{li2017aod}} & \textbf{PAD-Net} \\ \hline
% \textbf{PSNR} & 16.62        & 19.05        & 17.57          & 21.14              & 19.06            & 23.43            \\ \hline
% \textbf{SSIM} & 0.8179       & 0.8364       & 0.8102         & 0.8472             & 0.8504           & 0.8747  \\ \hline        
% \end{tabular}
% \end{table*}

%Table.~\ref{dehaze result table} compares the dehaze results of different approaches on SOTS using two full-reference metrics, PSNR and SSIM. The results of other models are reported in \cite{li2017reside}. It can be found that the PAD-Net outperforms both PSNR and SSIM among all single image dehazing models with an improvement on the PSNR and SSIM by $10.83\%$ and $2.86\%$ respectively compared to the corresponding best results among the counterpart models.




% \subsection{Object Detection on Hazy Images}
% Table \ref{detect result} compares the object detection results of different approaches on RTTS measure by mAP. Except for the three teams introduced in this report, three additional results are taken from ~\cite{li2017reside} and the SSD~\cite{liu2016ssd} in these works are trained by VOC2007~\cite{everingham2008pascal} and VOC2012~\cite{everingham2011pascal} dataset with an input size of $512 x 512$. Note that for the works introduced in this report, only the best results among all implemented settings are selected for comparison purpose. The results show that, in general, the introduced teams improve the existing works on detecting the objects in the hazy images and that application of domain adaptation, which is the work from team $3$, outperform others by more than $11.08\%$ improvement.

% \begin{table*}[h]
% \centering
% \caption{Detect Results on RTTS. F R-CNN represents the Faster R-CNN model.}
% \label{detect result}
% \begin{tabular}{l|c|c|c|c|c|c}
% \hline
%              & \textbf{\begin{tabular}[c]{@{}c@{}}DCP~\cite{he2011single} \\ + SSD~\cite{liu2016ssd}\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}DehazeNet~\cite{cai2016dehazenet} \\ + SSD~\cite{liu2016ssd}\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}AOD-Net~\cite{li2017aod} \\ + SSD~\cite{liu2016ssd}\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Team 1:\\ AOD+DCP+F R-CNN\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Team 2:\\ DCP + F R-CNN\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Team 3:\\ MSCNN + DMask-RCNN\end{tabular}} \\ \hline
% \textbf{mAP} & 55.71                                                         & 55.4                                                                & 55.27                                                             & 56.75                                                                       & 57.04                                                                     & 63.36                                                                           \\ \hline
% \end{tabular}
% \end{table*}




\section{Conclusion}
This paper tackles the challenge of single image dehazing and its extension to object detection in haze. The solutions are proposed from diverse perspectives ranging from novel loss functions (Task 1) to enhanced dehazing-detection cascades as well as domain-adaptive detectors (Task 2). By way of careful experiments, we significantly improve the performance of both tasks, as verified on the RESIDE dataset. We expect further improvements as we continue to study this important dataset and tasks.
%With careful experimental efforts, we are able to advance the performance levels of both tasks significantly, as verified on the RESIDE dataset. We expect more future updates to come as we continue to dig into the important dataset and tasks.


% from $4$ groups divided into $2$ tracks. For the team working on improving the single image dehazing, perceptual-motivated loss functions are explored. For the teams working on improving the object detection on hazy images, different combinations of dehazing models and object detection models are explored to generate a better-dehazed feature map for detection, and the domain adaption method is employed to bridge the gap between dehazed image and real clean image to improve the detection results. The REalistic Single Image DEhazing (RESIDE) dataset is employed as the dataset for both tracks. The results show that the groups in both tracks have made major improvements on dehazing or object detection results. Further improvements are expected if the exploration can be continued in the future.

\section*{Acknowledgements}
The study was initially performed as a team project effort in the Machine Learning course (\href{http://people.tamu.edu/~atlaswang/18CSCE633.html}{Spring 2018, CSCE 633}) of CSE@TAMU, taught by Dr. Zhangyang Wang. We acknowledge Texas A\&M High Performance Research Computing (HPRC) for providing some of the computing resources used in this research.


\bibliographystyle{ieeetr}
\bibliography{main}


\end{document}
