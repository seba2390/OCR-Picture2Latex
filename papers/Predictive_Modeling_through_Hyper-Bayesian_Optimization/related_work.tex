In the space of automating model design, \cite{Duvenaud2013} proposes a generative kernel grammar with the ability of defining the space of kernel structures which are built compositionally from a small number of base kernels. \textcolor{blue}{In a similar vein, \cite{gonen2013localized} proposed learning multiple kernels that are weighted and summed in a manner akin to mixture of expert. Work by \cite{pmlr-v70-wang17h} learns a structured kernel to support Bayesian Optimisation in high dimensional search spaces.} \cite{malkomes2016bayesian} proposes the BOMS method, an automatic framework for exploring the potential space of models using Bayesian Optimization. Building on this work to implement model searching within the optimization search of a black-box function \cite{Malkomes:2018:ABO:3327345.3327498} presents ABOMS. This work utilised Bayesian optimization in the model search space to determine the best model hyperparameters, whilst at the same time optimizing the black box function. The fit of the model is measured by the normalized marginal likelihood, allowing comparison of models across iterations. However, this is an indirect measure of the goodness and may not fully align with the goal of the optimization.