The Predictive Modeling framework with HyperBO can be proven to converge under the Theorem below.

\begin{theorem}
	Let $\delta \in (0,1)$ and $\eta \in (0,1)$. Assuming the kernel functions used in both the BO in the function space and the HyperBO, $k(.,.)$ provides a high probability guarantee on the sample paths of GP derivative to be L'-Lipschitz continuous, and the function $f(A^{t}_{p}(\boldsymbol{\theta}))$ is L-Lipschitz continuous. There exists a $t_O = T_{S} \leq T_{O}$ beyond which $\left\|\boldsymbol{\theta}^{*}-\boldsymbol{\theta}_{t_{O}}\right\| < \epsilon$ is satisfied with probability $1-\delta$. Furthermore, the average cumulative regret of the Predictive Modeling framework will converge to $\textcolor{red}{\lim_{T_{I} \rightarrow \infty}} R_{T}/T = \epsilon L$.
\end{theorem}

\begin{proof}
If BO with Thompson Sampling is used \cite{Basu2017}, then we know that
\begin{align*}
Prob(\left\|\boldsymbol{\theta}^{*}-\boldsymbol{\theta}_{t}\right\|>\epsilon) \leq C^{0}_{\epsilon}\text{exp}(-C^{1}_{\epsilon}t)
\end{align*}
where $C^{0}_{\epsilon}$, $C^{1}_{\epsilon}$ are $\epsilon$ dependent constants.

This implies we can set any arbitrary $\epsilon \ll 1$ and an arbitrary low probability $\delta \ll 1$. Then there exists a $t_{O} = T_{S}$ beyond which $\left\|\boldsymbol{\theta}^{*}-\boldsymbol{\theta}_{t_{O}}\right\| < \epsilon$ happens with high probability ($1 - \delta$).
\begin{align*}
\delta = & C^{0}_{\epsilon}\text{exp}(-C^{1}_{\epsilon}T_{S})\\
\implies T_{S} = & C^{1}_{\epsilon}\text{log}(\frac{C^{0}_{\epsilon}}{\delta})
\end{align*}

Using regret as defined in \cite{Srinivas:2010:GPO:3104322.3104451}, and recalling that $A_{p}^{T}(\boldsymbol{\theta}_{t})=\boldsymbol{x}_{T}$, we can write the cumulative regret as:

\begin{align*}
R_{T} = & \sum_{t=1}^{T_{O}}\sum_{t'=1}^{K}|f(\boldsymbol{x}^{*})-f(A_{p}^{T}(\boldsymbol{\theta}_{t}))|
\end{align*}
where $T=(t-1)\times K+t'$ is the actual iterations that the BO in the function space has gone through. Next, we break down $R_{T}$ by introducing $f(A_{p}^{T}(\boldsymbol{\theta}^{*}))$, where $\boldsymbol{\theta}^{*}=\text{argmax }g(\boldsymbol{\theta})$.
\begin{align*}
R_T= & \sum_{t=1}^{T_{O}}\sum_{t'=1}^{K} |f(\boldsymbol{x}^{*})-f(A_{p}^{T}(\boldsymbol{\theta}^{*})) +f(A_{p}^{T}(\boldsymbol{\theta}^{*}))\\
&-f(A_{p}^{T}(\boldsymbol{\theta}_t))|\\ 
\leq & \underbrace{\sum_{t=1}^{T}|f(\boldsymbol{x}^{*})-f(A_{p}^{T}(\boldsymbol{\theta}^{*}))|}_{O(\sqrt{T\text{log}T})}\\
&+\sum_{t=1}^{T_{O}}\sum_{t'=1}^{K} |f(A_{p}^{T}(\boldsymbol{\theta}^{*}))-f(A_{p}^{T}(\boldsymbol{\theta}_t))|\\ 
\leq & O(\sqrt{T\text{log}T})+\\
&T_{0} \operatorname*{max}_{t= [1,T_{0}]}(K \operatorname*{max}_{t'= [1,K]}\underbrace{|f(A_{p}^{T}(\boldsymbol{\theta}^{*}))-f(A_{p}^{T}(\boldsymbol{\theta}_t))|}_{L\left\|\boldsymbol{\theta}^{*}-\boldsymbol{\theta}_{t}\right\|})
\end{align*}
Because the $GP$ predictive posterior is smooth w.r.t $\boldsymbol{\theta}$ it makes the acquisition function to be smooth as well w.r.t. $\boldsymbol{\theta}$. So we can assume that $f(A_{p}^{T}(\boldsymbol{\theta}))$ is L-Lipschitz. Hence,
\begin{align*}
R_T\leq & O(\sqrt{T\text{log}T})+\underbrace{T_{0} K}_{T} L\underbrace{\left\|\boldsymbol{\theta}^{*}-\boldsymbol{\theta}_{t}\right\|}_{\epsilon} \\ 
\leq & O(\sqrt{T\text{log}T})+T \epsilon L \\ 
& \text{Taking the limit }\lim_{T \rightarrow \infty} \frac{R_{T}}{T} = \epsilon L
\end{align*}
\end{proof}
Although the regret does not vanish in our case, it can be made arbitrary small by setting $\epsilon$ very small. We must also note that the existing convergence analysis assumes that the best model is being used throughout the BO, ignoring the effect of estimation of the model on the convergence. In fact in some analysis it is shown that running model selection would fail the convergence \cite{bull2011convergence}. In contrast, we provide the convergence guarantee of our whole approach, including the model selection part, thus making it more useful to look at.