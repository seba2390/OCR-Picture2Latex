Our overarching objective is to search for the optima of the black-box function. Whilst conducting the Bayesian optimization search for the function optima, the model search for this BO is  guided by another BO in the model space, we call it HyperBO. After every $K$ iterations of the inner BO in the function space, a new model, described by a vector of its hyperparameters ($\boldsymbol{\theta}$), is applied. After $K$ iterations with this new model, its performance in progressing the search to the optima is assessed using a scoring function ($g(\boldsymbol{\theta})$). This score is then fed back to the GP within HyperBO ($GP^{\theta}$). The iterations of the BO in the model space are denoted by $t_O$. We use Thompson Sampling in order to select the next best model hyperparameter values for the HyperBO. The choice of Thompson sampling as an acquisition function is guided by the need to analyse the convergence of our method, where we use the analysis similar to \cite{Basu2017}. However, we use GP-UCB as an acquisition function for the BO in the function space, as it provides a stricter convergence guarantee. Over time, the HyperBO will converge to the ideal model hyperparameters, accelerating the convergence of the BO in the function space. We note that by using HyperBO for model selection, we use a more active approach of model selection using prediction, rather than the usual passive way of using estimation. Algorithms \ref{alg:AMMSF} - \ref{alg:HyperBO} describe this Predictive modeling framework.

Mathematically, our optimization problem is defined as:
\begin{align*}
\boldsymbol{\theta}^{*} = & \operatorname*{max}_{\boldsymbol{\theta}\in \boldsymbol{\Theta}} g(\boldsymbol{\theta})
\end{align*}

where $g(\boldsymbol{\theta}) \triangleq$ score function
\begin{align*}
A_{p}^{T} : & p, \boldsymbol{\theta}, T \rightarrow \boldsymbol{x} \text{ (inner optimization)}\\
p \triangleq & \operatorname*{max}_{\boldsymbol{x}\in \boldsymbol{\chi}} f(\boldsymbol{x})
\end{align*}
Where $T$ is the iteration of the BO in the function space, $g(\boldsymbol{\theta})$ is the scoring function, $A_{p}^{T}$ is the BO algorithm for problem $p$, at $T$'th iteration, and $p$ is the optimization problem in the function space. To maintain the stationarity of the HyperBO optimization problem, i.e. maximizing the scoring function $g(\boldsymbol{\theta})$, we should make the scoring function independent of the iteration ($T$) of the BO in the function space. In the next we show how to use the convergence results of Bayesian optimization to derive such a scoring function.

\begin{algorithm}[t]
	\caption{Predictive Modeling Framework}
	\label{alg:AMMSF}
	\textbf{Input}: black-box function $f$, initial data $\boldsymbol{D}_0$ made of (\textbf{x},\textbf{y})\\
	\textbf{Parameter}: number of initial models $m$, number of iteration per model $K$, number of total iterations $R$\\
	\textbf{Output}: $\boldsymbol{x}^{*}$ and $\boldsymbol{\theta}^{*}$
	\begin{algorithmic}[1] %[1] enables line numbers
		\STATE $M_0 \gets \emptyset$
		\STATE Construct initial model set
		\FOR{$t_{O} = 1, 2, ..., m$}
		\STATE $y^{+} \leftarrow max(\boldsymbol{y})$
		\STATE Generate random hyperparameter vector $\boldsymbol{\theta}_{t_{O}}$
		\STATE Generate score $S_{t_{O}}$ for $\boldsymbol{\theta}_{t_{O}}$ and output $\boldsymbol{D}_{t_{O}}$ from Alg 2: Model Score
		\STATE Update model data $\boldsymbol{M}_{t_{O}}=\boldsymbol{M}_{t_{O}-1} \cup (\boldsymbol{\theta}_{t_{O}},S_{t_{O}})$
		\ENDFOR
		\STATE Begin Bayesian Optimization of black-box function
		\WHILE{$t_{O} \leq R$}
		\STATE $y^{+} \leftarrow \text{max}(\boldsymbol{y})$
		\STATE Generate $\boldsymbol{\theta}_{t_{O}}$ from Alg 3: HyperBO
		\STATE Generate score $S_{t_{O}}$ for $\boldsymbol{\theta}_{t_{O}}$ and output $\boldsymbol{D}_{t_{O}}$ from Alg 2: Model Score
		\STATE Update model data $\boldsymbol{M}_{t_{O}}=\boldsymbol{M}_{t_{O}-1} \cup (\boldsymbol{\theta}_{t_{O}},S_{t_{O}})$
		\ENDWHILE
	\end{algorithmic}
\end{algorithm}
\begin{algorithm}
	\caption{Model Score}
	\label{alg:ScoringModel}
	\textbf{Input}: black-box function $f$, data $\boldsymbol{D}_{t_{O}-1}$ hyperparameter vector $\boldsymbol{\theta}_{t_{O}}$, current optima $y^{+}$\\
	\textbf{Parameter}: number of iteration per model $K$\\
	\textbf{Output}:model score $S_{t_{O}}$, data $\boldsymbol{D}_{t_{O}}$
	\begin{algorithmic}[1]
		\STATE $\boldsymbol{D}_{T} = \boldsymbol{D}_{t_{O}-1}$
		\FOR{$i = 1, 2, ..., K$}
		\STATE $T = t_{O}+i$
		\STATE Fit $GP_{t_{O}}$ with $\boldsymbol{\theta}_{t_{O}}$ and $\boldsymbol{D}_{T-1}$
		\STATE Compute $\mu_{T}$ and $\sigma_{T}$ according to (\ref{eq:mu}) and (\ref{eq:sigma})
		\STATE Construct and maximise acquisition function $a_{t}$
		\STATE Sample function $y_{T} = f(\boldsymbol{x}_{T})$ and update data $\boldsymbol{D}_{T} = \boldsymbol{D}_{{T}-1} \cup (\boldsymbol{x}_{T},y_{T})$
		\ENDFOR
		\STATE $f^{+}(A_p(\boldsymbol{\theta})) \leftarrow \text{max}(\boldsymbol{y})$
		\STATE Score performance $S_{t_{O}}$ of $\boldsymbol{\theta}_{t_{O}}$ with (\ref{eq:score2})
		\STATE $\boldsymbol{D}_{t_O} = \boldsymbol{D}_{T}$
	\end{algorithmic}
\end{algorithm}
\begin{algorithm}
	\caption{HyperBO}
	\label{alg:HyperBO}
	\textbf{Input}: model data $\boldsymbol{M}_{t_{O}}$, model score $S$\\
	\textbf{Output}: hyperparameter $\boldsymbol{\theta}_{t_{O}}$
	\begin{algorithmic}[1]
		\STATE $s^{+} \leftarrow \text{max}(S)$
		\STATE Fit $GP_{t_{O}}^{\theta}$ with $\boldsymbol{M}_{t_{O}}$
		\STATE Compute $\mu_{t_{O}}$ and $\sigma_{t_{O}}$ according to (\ref{eq:mu}) and (\ref{eq:sigma})
		\STATE Select next $\boldsymbol{\theta}_{t_{O}}$ through Thompson Sampling
	\end{algorithmic}
\end{algorithm}