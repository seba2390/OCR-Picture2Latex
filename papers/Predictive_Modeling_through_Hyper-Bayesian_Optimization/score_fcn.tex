As mentioned before, a key innovation in our scoring metric is the ability to offset for the moving nature of the BO search, thereby effectively allowing it to function as a black-box that can be analysed independently. In order to maintain this required stationarity, we describe this reduction in progress towards the optima in later iterations via the regret bound of GP-UCB as detailed in \cite{Srinivas:2010:GPO:3104322.3104451}, utilizing it as a means of describing the average regret. 

Regret Bound $R_{T}$ is of $O(\sqrt{T\gamma_{T}\text{log}(\mid D \mid)})$ where $\gamma_{T}$ is of $O((\text{log} T)^{d+1})$. $T$ is the iteration number.

\begin{align*}
\frac{R_{T}}{T} = & O\left(\sqrt{\frac{\gamma_{T} \text{log}(\mid D \mid)}{T}} \right)\\
\frac{R_{T}}{T} = & O\left(\sqrt{\frac{(\text{log} T)^{d+1} \text{log}(\mid D \mid)}{T}} \right)\\
\frac{R_{T}}{T} \sim & C\sqrt{\frac{\text{log} T^{d+1}}{T}}
\end{align*}

For the score we can ignore $C$ as it is a constant. We use the shape of $\frac{R_{T}}{T}$ to normalize the gain in performance during any interval of the BO iterations in the function space, making the gain independent of the iteration ($T$). This is what we use as our scoring function.

\begin{equation}\label{eq:score1}
g(\boldsymbol{\theta}) = \frac{f^{+}(A_{p}(\boldsymbol{\theta}))-y^{+}}{\sqrt{\frac{\text{log } T^{d+1}}{T}}}
\end{equation}
where $y^{+}$ is the best observation before BO in the function space with the new hyperparameter, $f^{+}(A_{p}(\boldsymbol{\theta}))$ is the best observation after BO with the new hyperparameter.

In practise, it was observed that adding a regularization term to (\ref{eq:score1}) can improve the performance further. This term differs slightly depending on the hyperparameter type being tuned. Some examples will be discussed in the later sections.