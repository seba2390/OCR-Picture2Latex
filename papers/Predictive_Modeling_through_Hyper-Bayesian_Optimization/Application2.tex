If a function is monotonic to certain variables, access to monotonicity information can greatly improve the fit of a GP and increase the sample efficiency and convergence during the BO search \cite{Wang2018,Golchi2015,Li2018}. However it is often the case that this information is not available prior to experimentation. As such, we apply our framework for monotonicity discovery.

\subsubsection{Gaussian Process with monotonicity}

\cite{riihimaki2010gaussian} proposed a method of incorporating monotonicity information into a Gaussian process model. The monotonicity is added at some finite locations of the search space through an observation model, via virtual derivative observations. In this method a parameter $\nu$ is used to control the strictness of the monotonicity information. Further details on the theory and implementation of this method can be found at \cite{riihimaki2010gaussian}.

\subsubsection{Searching monotonic models}

Unlike \cite{riihimaki2010gaussian} which requires monotonicity information be incorporated into the GP prior, we work off the assumption that monotonicity information is unknown. Our proposed framework uses HyperBO to search the space of GP models, and in the case of monotonicity, discover the best monotonic GP model to describe the latent function. 

\cite{riihimaki2010gaussian} specified the strictness parameter $\nu$ as a constant. In our work we utilise this parameter instead as a way of discovering monotonicity in a model, by tuning its value to reflect the strength of monotonicity in a given direction and dimension.
To do this we assume independent $\nu$ values in each dimension thereby describing the strictness parameter as a vector $\boldsymbol{\nu}$. In addition we describe monotonicity in both increasing and decreasing directions for each dimension. The reason for having monotonicity directions be described independently is to have the elements of $\boldsymbol{\nu}$ range between $10^{-6}$ - $10^{0}$ where lower values apply a strong monotonic trend. In order to have an effective search, we conduct the search in the log space, thereby searching for values between -6 to 0, where a lower value (-6) invokes a strong monotonicity. We represent  $\boldsymbol{\nu}$ as the $\boldsymbol{\theta}$ in our algorithm. 

As an example, for a 2 dimensional function, we construct a vector $\boldsymbol{\theta}$ for the monotonicity directions [-1 1 -2 2] with $\boldsymbol{\theta} = [\theta_{1}^{-}, \theta_{1}^{+}, \theta_{2}^{-}, \theta_{2}^{+}]$, where $\theta_{d}^{-}$ is the strictness parameter for decreasing monotonicity and $\theta_{d}^{+}$ is for increasing monotonicity in the d-th dimension. 

\begin{figure}[t]
	\centering
	\subfloat[Concrete]{{\includegraphics[height = 3cm, width=3.5cm]{figures/Concrete_1a.png}}}%
	\qquad
	\subfloat[Concrete]{{\includegraphics[height = 3cm,width=3.5cm]{figures/Concrete_1b.png}}}%
	\qquad
	\subfloat[Power Plant]{{\includegraphics[height = 3cm,width=3.5cm]{figures/PowerPlant_1a.png}}}%
	\qquad
	\subfloat[Power Plant]{{\includegraphics[height = 3cm,width=3.5cm]{figures/PowerPlant_1b.png}}}%
	\qquad
	\subfloat[Fish Toxicity]{{\includegraphics[height = 3cm,width=3.5cm]{figures/ToxicFish_1a.png}}}%
	\qquad
	\subfloat[Fish Toxicity]{{\includegraphics[height = 3cm,width=3.5cm]{figures/ToxicFish_1b.png}}}%
	\caption{Regret vs Iteration results of Length Scale tuning experiments. Case 1: HyperBO vs BO performance: a,c,e) Demonstrates Predictive modeling framework is better able to discover appropriate length scales to reach the function optima.  b,d,f): Case 2: GP with best monotonicity information outperforms Standard BO.}%
	\label{fig:LengthScale_experiments}%
\end{figure}

\subsubsection{Monotonicity Regularization}
Regularization of the form shown in (\ref{eq:score2}) is applied for scoring monotonic models. A low value of $\boldsymbol{\theta}$ indicates the presence of monotonicity, while a high value indicates a lack of monotonicity. As such a weighting is applied to the score in order to prefer large values of $\boldsymbol{\theta}$ and thereby prevent the presence of monotonicity being over emphasised. In our experiments, the values of $\theta \in [-6, 0]$. The weighting is designed to favour values closer to 0. Below is the form of the score function used with $\lambda$ being the regularization weight.
\begin{equation}\label{eq:score2}
g(\boldsymbol{\theta}) = \frac{f^{+}(A_{p}(\boldsymbol{\theta}))-y^{+}}{\sqrt{\frac{\text{log } T^{d+1}}{T}}}\left(1+\lambda\left\|\boldsymbol{\theta}\right\|\right)
\end{equation}