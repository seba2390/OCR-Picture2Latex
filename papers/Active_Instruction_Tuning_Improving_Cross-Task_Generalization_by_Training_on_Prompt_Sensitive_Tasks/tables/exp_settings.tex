\begin{table}[t]
\centering
\small
\setlength{\tabcolsep}{1mm}
\begin{NiceTabular}{|l||c|c|}
\Hline
Statistics / IT Models & NIV2 & Self-Instruct \\
\hline
Dataset & &  \\
\# of training tasks (instructions) & 756 & 52K \\
\# of testing tasks (instructions)& 119 & 252 \\
\# of data per task & $\geq$ 200* & 1 \\
Testing on unseen tasks? & \greencheck & \redcross\\
\hline
Active Instruction Tuning & &  \\
\# of tasks in initial training set & 68 & 500 \\
\# of task to select at $i$th iteration & 68 & $500*2^i$ \\
\hline
Evaluation & &  \\
Evaluation Metrics & Rouge-L & \Block{2-1}{Human Eval\\GPT Eval} \\
& & \\
\Hline
\end{NiceTabular}
% \vspace{-2em}
\caption{\footnotesize
Comparison between NIV2~\cite{Wang2022SuperNaturalInstructionsGV} and Self-Instruct~\cite{wang2022self} datasets. Most tasks in NIV2 have more than 200 instances, while Self-Instruct only has one instance for each task. These two settings differ in terms of the definition of the task and generalization objective (zero-shot cross-task v.s. cross-task), described in \citet{kung2023models}.\looseness=-1}
\vspace{-1em}
\label{table:exp-settings}
\end{table}