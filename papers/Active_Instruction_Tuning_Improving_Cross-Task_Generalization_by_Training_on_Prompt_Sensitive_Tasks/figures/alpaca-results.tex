

\begin{figure*}
    \centering
    % \includegraphics[draft]{}
    \includegraphics[width=\textwidth]{../pdfs/Alpaca-Results}
    \vspace{-1em}
    \caption{\footnotesize Pairwise comparison results between different task selection strategies and random sampling on 252 user-oriented instruction test set~\cite {wang2022self}, evaluated by GPT4, Chat-GPT, and Human Annotators. 
For four types of methods at each active instruction tuning iteration: $[1000, 2000, 4000, 8000, 16000]$, we separately conduct a pairwise comparison with \textit{Random Sampling} and report the net winning tasks (Number of Win Tasks - Number of Lose Tasks). Note that we only conduct the human evaluation to compare our proposed \textit{Prompt Uncertainty} to \textit{Random Sampling} method due to high evaluation cost (\$600 US Dollars). We provide further details in \autoref{subsec:human-eval} \autoref{tab:full-results-alpaca}.}
    \label{fig:alpaca-results}
    \vspace{-1em}
\end{figure*}