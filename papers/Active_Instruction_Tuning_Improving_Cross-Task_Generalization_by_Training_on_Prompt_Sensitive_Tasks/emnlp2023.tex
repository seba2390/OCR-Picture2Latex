% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
\usepackage[]{EMNLP2023}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary and may be commented out.
% However, it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}
\usepackage{duckuments}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{soul}
\usepackage{multirow}
\usepackage{nicematrix}
\usepackage{pifont}
\usepackage{arydshln}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{cleveref}
\usepackage{enumitem}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage[normalem]{ulem}
\setlist{leftmargin=5.5mm}
\crefformat{section}{\S#2#1#3} % see manual of cleveref, section 8.2.1
\crefformat{subsection}{\S#2#1#3}
\crefformat{subsubsection}{\S#2#1#3}

\newcommand*\colourcheck[1]{%
  \expandafter\newcommand\csname #1check\endcsname{\textcolor{#1}{\ding{52}}}%
}
\newcommand*\colourcross[1]{%
  \expandafter\newcommand\csname #1cross\endcsname{\textcolor{#1}{\ding{55}}}%
}
\colourcheck{green}
\colourcross{red}
\usepackage{subfiles}
\usepackage{todonotes}

\newcommand{\hb}[1]{\textcolor{blue}{HB:#1}}
\newcommand{\old}[1]{\textcolor{red}{OLD:#1}}
\newcommand{\new}[1]{\textcolor{green}{NEW:#1}}

\newcommand{\SideNote}[2]{\todo[color=#1,size=\small]{#2}} 
\newcommand{\violet}[1]{\SideNote{purple!40}{#1 --violet}}
% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

% \title{Task selection for efficient instruction tuning. Selecting task by highest prompt uncertainty.}
% \title{Active Instruction Tuning: \\ Extending Training Tasks with Prompt Sensitivity for Better Instruction Generalizability}
% \title{Active Instruction Tuning: Scaling Up Instruction Tuning by Selecting Training Tasks with High Prompt Sensitivity}
\title{
% Active Instruction Tuning: Efficient Instruction Tuning by Training on Prompt Sensitive Tasks
Active Instruction Tuning: Improving Cross-Task Generalization by Training on Prompt Sensitive Tasks
}  %Efficiently Improve Instruction Tuned Models by Training Prompt Sensitive Tasks
% \hb{don't need efficiently i guess}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{Po-Nien Kung, ~~~Fan Yin, ~~~Di Wu, ~~~Kai-Wei Chang, ~~~Nanyun Peng\\
   University of California, Los Angeles \\
   {\tt \{ponienkung,fanyin20,diwu,kwchang,violetpeng\}@cs.ucla.edu } \\}

\begin{document}
\maketitle
\begin{abstract}
% Instruction tuning (IT) models can achieve impressive zero-shot generalization to unseen tasks by training a large language model (LLM) on a wide range of tasks with instructions. While IT models' performance scale with the number of training tasks, how to select most informative training tasks efficiently remains a challenge. Randomly collecting tasks can be computationally and annotatively expensive, and training on imbalanced tasks can negatively impact overall generalizability.
% To address these challenges, we propose active instruction tuning using prompt uncertainty.This approach actively extends the training scale by selecting the most informative tasks based on their prompt uncertainty, which we propose to measure by evaluating the model's prediction disagreement among prompt perturbations, reflecting the model's susceptibility to a given task.
% Our experiments on NIV2 and Alpaca setting demonstrate that instruction tuning the models with prompt-uncertain tasks consistently outperforms random sampling and perplexity baselines, especially in early active learning iterations. Our further analysis shows that by selecting prompt-uncertain tasks, the model can be trained on a diverse task set and enhances the model's robustness towards prompts.


% --------------- Old abstract
% \violet{what does ``scale-up'' mean in the title?} This is the old titile, I will change it.
% Instruction tuning (IT) achieve impressive zero-shot generalization by training a large language model (LLM) on diverse tasks with instructions. 
% While prior work shows that IT models' performance scales with the number of training tasks, selecting the most informative training tasks for effiecient improving IT models remains a challenge.
% Training on massive amount of existing tasks can be effective but inpractical due to overwhelming computing. 
% \violet{first motivate why you need to do it. E.g., fine-tuning on extensive number of new tasks and prompts is computationally expensive (may want to give some concrete numbers here), while training on selected most informative tasks may achieve the same results with much less training time.} \hb{A naive approach like randomly selecting ..}
% Randomly selecting tasks is suboptimal and can potentially select imbalanced training tasks \hb{what do you mean by imbalanced training tasks}, affecting overall generalizability.
% Addressing this challenge, we propose active instruction tuning \hb{i know `active learning' so it is easy for me to understand this but it would be good if you can share what is `active' about your paradigm}\violet{+1. More specifically, there must be some special benefits of being `active' in your mind, what are the benefits? At the minimum, what's different here?} based on prompt uncertainty. 
% We measure tasks' prompt uncertainty\violet{why do you need to measure tasks' prompt uncertainty?} by assessing the discrepancy of generation likelihood between original and perturbed instructions \hb{can you give more information about the perturbations?}. The disagreement scores reflect the model's susceptibility to specific tasks, indicating their usefulness to the model. We then collect and train the IT model with the most prompt-uncertain tasks to enhance the overall cross-task \hb{what is cross-task generalization?} generalization ability.
% Our experiments on NIV2 and Self-Instruct datasets demonstrate that our method consistently outperforms other baseline task-selecting strategies (random sampling, sentence perplexity), achieving better out-of-distribution generalization with fewer training tasks \hb{quantify fewer}.
% Additionally, we introduce a task map that categorizes and diagnoses tasks based on prompt uncertainty and generation perplexity. Our findings demonstrate that training on prompt-uncertain tasks improves generalization, while training on prompt-certain and high-perplexity tasks offers no benefit, underscoring the significance of task selection in instruction tuning.
% \hb{To enhance your motivation, you can mention what are the issues with collecting a lot of instruction data? is it hard/expensive to collect quality data?}
% \hb{i am concerned that a reviewer might think that collecting a lot of instruction tuning data may not be very difficult since gpt4 can give you a lot of data quickly in a cheap way then why do we even need active instruction tuning for subselecting the important tasks? maybe i am wrong but this is my current impression. it would be good to address it somewhere.}
% \hb{The problem being solved not clear here or in the introduction: (a) do you consider that you have a base LM that can be instruction tuned on thousands of tasks but you want to subselect them for data-efficient IT. (b) do you consider that you have a IT model and now you have to update it with new tasks iteratively? More of an active learning paradigm.}

Instruction tuning (IT) achieves impressive zero-shot generalization results by training large language models (LLMs) on a massive amount of diverse tasks with instructions. However, how to select new tasks to improve the performance and generalizability of IT models remains an open question. 
Training on all existing tasks is impractical due to prohibiting computation requirements, and randomly selecting tasks can lead to suboptimal performance.
In this work, we propose \textit{active instruction tuning} based on prompt uncertainty,
a novel framework to identify informative tasks, and then actively tune the models on the selected tasks. We represent the informativeness of new tasks with the disagreement of the current model outputs over perturbed prompts.
Our experiments on NIV2 and Self-Instruct datasets demonstrate that our method consistently outperforms other baseline strategies for task selection, achieving better out-of-distribution generalization with fewer training tasks. 
Additionally, we introduce a task map that categorizes and diagnoses tasks based on prompt uncertainty and prediction probability. We discover that training on ambiguous (prompt-uncertain) tasks improves generalization while training on difficult (prompt-certain and low-probability) tasks offers no benefit, underscoring the importance of task selection for instruction tuning.\footnote{Our code and data can be found at \url{https://github.com/PlusLabNLP/Active-IT}}\looseness=-1

\end{abstract}

\section{Introduction}
\subfile{sections/introduction}
\section{Method}
\label{sec:method}
\subfile{sections/method}
\section{Experiment Setting}
\subfile{figures/niv2-results}
\subfile{sections/experiment}
\section{Results}
\label{sec:results}
\subfile{sections/results}
\section{Task Map}
\label{sec:task-map}
\subfile{sections/task_map}
\section{Discussion}
\subfile{sections/discussion}
\section{Related Work}
\subfile{sections/related_work}
\section{Conclusion}
We propose Active Instruction Tuning with prompt uncertainty, a framework to enhance the generalization ability of the IT model in large-scale instruction tuning. Our experiments on NIV2 and Self-Instruct datasets demonstrate that training on prompt uncertain tasks consistently outperforms random sampling and other uncertainty baselines, highlighting the effectiveness of our approach. We also introduce Task Map, a tool that categorizes tasks based on prompt uncertainty and prediction probability, revealing that while training on ambiguous tasks improves generalization, some difficult tasks offer no benefit.
% Additionally, we perform control experiments to demonstrate the reduction in prompt uncertainty following model training on relevant tasks, offering qualitative examples of how this uncertainty reflects task novelty to the model.
These findings motivate future investigations into prompt uncertainty and task selection strategies for better understanding cross-task generalization and instruction tuning.\looseness=-1






% -------------------------- Main paper above











\section*{Limitations}
While our experiments demonstrate the superiority of our proposed prompt uncertainty method over other baseline task selection methods on the NIV2 and Self-Instruct datasets, there are several limitations to consider. Firstly, our experiments are conducted on open-source instruction tuning models and do not consider the impact of reinforcement learning with human feedback in Instruct-GPT ~\cite{ouyang2022training}.
Secondly, although we conducted our experiments on well-constructed instruction tuning datasets, it is important to note that this setting may not fully capture the challenges posed by noisy or poorly constructed tasks in extreme scenarios, which may require techniques such as noisy task filtering or batch active learning.
Lastly, our current experiment on active instruction tuning focuses on comparing task selection methods and does not incorporate the effect of continual learning, which could be valuable for improving IT models in realistic settings.
In summary, our work primarily focuses on introducing active instruction tuning and comparing task selection methods within a controlled environment. We look forward to future research to conduct further analysis to comprehensively examine the effects of all these factors. \looseness=-1
\section*{Ethics Statement}
We describe the computation resources and models we used to conduct our experiments. We conduct all experiments on 4 to 8 48GB NVIDIA A6000 GPUs or 2 to 4 NVIDIA A100 GPUs, along with 48 TB disk storage and AMD EPYC 7413 24-Core Processor. The experiment takes around 5500 GPU hours for one 48GB NVIDIA A6000 GPU. Our experiments do not need to leverage private data. For the model, we use open-sourced Huggingface T5-large-lm-adapt models and LLaMA-7B, Stanford Alpaca-7B for our experiments, and we will release our code once the paper is accepted.\looseness=-1
\section*{Acknowledgements}
We would like to thank Hritik Bansal and Da Yin for their valuable insights during discussion, paper reviews, and constructive comments.
We thank the anonymous reviewers for their feedback.
This work was partially supported by AFOSR MURI via Grant \#FA9550-22-1-0380, Defense Advanced Research Project Agency (DARPA) grant \#HR00112290103/HR0011260656, CISCO and ONR grant \#N00014-23-1-2780.
% Entries for the entire Anthology, followed by custom entries
\bibliography{emnlp2023}
\bibliographystyle{acl_natbib}

\appendix

\section{Appendix}
\label{sec:appendix}
\subsection{Evaluation details}
\label{subsec:human-eval}
% We provide details of our human evaluation method on 252 User-Oriented Instruction Set.
% For each of the five sample sizes from 1000 to 16000, we collect one sample via Random Sampling and another sample via the proposed method, fine-tune LLaMa on the samples, and compare the two via human evaluation and GPT automatic evaluation. 
\paragraph{Human Evaluation}
For human evaluation in \autoref{sec:results} and \autoref{fig:alpaca-results}, we recruit crowd-source workers on Amazon Mechanical Turk who are native English speakers, score at least 70\% on a qualification test, and pass the attention test. For the annotation task, three annotators are presented with the task instruction, the input, and the expected output, followed by two models' outputs in random order. The annotators are asked to indicate whether the first model wins, loses, or has a tie. An example of the annotation interface is presented \autoref{scoring-instructions}. The final comparison decisions are aggregated from the raw annotations using majority voting. We assign a tie label when all the annotators disagree. To calculate the inter-annotator agreement, we define no-contradiction as agreement with the tie entries removed since an annotator slightly supporting a model may also vote for a tie. Under this definition, the no-contradiction rate is measured as 60.4\%. Among the cases with contradiction, we find two annotators agree for 82\% cases, and all annotators disagree in only 18\% cases. We set the per-item reward to \$0.1 to reach an hourly rate of \$15. We collect 3780 comparison annotations to compare \text{Prompt Uncertainty} with \textit{Random Sampling} at each active instruction tuning iteration. The total annotation cost is approximately 600 US Dollars.
\paragraph{GPT-4/Chat-GPT Evaluation}
We conduct a blind pairwise comparison on GPT-4 and Chat-GPT (GPT-3.5) models using Open-AI API, following a similar template as we used for human evaluation, which is shown in \autoref{tab:gpt-template}. To compare two models on one instance, we will randomly assign "(1)" and "(2)" to the model's predictions and prompt the model to reply with the better choice or "Equal" if two predictions are equally good. Note that when applying GPT evaluation, there are very rare cases (about 0.7\% of the cases) that the GPT models will reply with unrelated output, which we will assign "Equal" to these instances. The total annotation cost is approximately 50 US Dollars for GPT-4 and 2 US Dollars for Chat-GPT. For full evaluation results, please refer to \autoref{tab:full-results-alpaca}\looseness=-1
\subsection{Experiment details}
\label{subsec:experiment-details}
\subfile{sections/experiment_details}

\begin{figure*}[h!]
\centering
\fbox{\includegraphics[width=0.8\textwidth]{figures/sample_annotation_task.png}}
\caption{An example of the annotation interface for the human evaluation in \cref{alpaca-result}.}
\label{scoring-instructions}
\end{figure*}


% GPT Eval Template
\begin{table*}
\centering
\begin{tabular}{p{0.98\linewidth}}
\toprule
\textit{Given the model predictions from \textit{Random Sampling} and \textit{Prompt Uncertainty} which trained with 1000 tasks, we first randomly assign \textbf{"(1)"} and \textbf{"(2)"} tags:}\\
\textbf{(1)}: \textcolor{blue}{Prompt-Uncertainty-1000} \\
\sethlcolor{green}
\textbf{(2)}: \textcolor{orange}{Random-Sampling-1000} \\
\sethlcolor{yellow}
\textit{Now we fill the GPT prompt template with these two model predictions.}
\\
\midrule
\textbf{GPT Prompt:}\\
You're a helpful AI system that is meant to imitate human judgment. \\
Given an \textit{instruction}, \textit{input}, and two \textit{predictions} "(1)" and "(2)", please tell me which prediction is most satisfying and correct.\\
If (1) is better, output "(1)".\\
If (2) is better, output "(2)".\\
If both predictions are equally good or equally bad, you can output "Equal".\\
\\        
\textbf{Instruction:}\\
\uline{
If you could help me write an email to my friends inviting them to dinner on Friday, it would be greatly appreciated.
}
\\
\\
\textbf{Input:}
\\
\\
\textbf{Now given two predictions:}
\\
\\
\textbf{(1)}: \textcolor{blue}{\uline{
Dear Friends, \\
I would like to invite you all to join me for dinner on Friday. I am planning to make some of my favorite dishes, and I would love for you all to join me.
\\
\\
The dinner will be held at my place at 7pm on Friday, and I am looking forward to seeing you all there.
\\
\\
Please let me know if you are able to make it so that I can make sure to have enough food.
\\
\\
See you then!
\\
\\
Sincerely,
\\
\\
[Your Name]
}}
\\
\\
\textbf{(2)}: \textcolor{orange}{\uline{Dear [Name], \\ \\ With sincerity, \\ [Your Name]}}
\\
\\
\textbf{Output:}\\
\midrule
\textbf{GPT Output:} (1)\\
\bottomrule
\end{tabular}
\caption{We show our GPT evaluation template. When comparing two models' predictions, we will randomly assign "(1)" or "(2)" tags and then fill them into the GPT templates with provided \textit{instruction} and \textit{inputs}. In this specific instance, the input is an empty string. All the underlined text are the component we injected into the template.}
\label{tab:gpt-template}
\end{table*}


\subfile{sections/full_results_niv2}
\subfile{sections/full_results_alpaca}
\end{document}

