% In this section, we will discuss current instruction tuning paradigms and the underlying hypothesis of prompt uncertainty.
\subsection{Instruction Tuning Paradigm}
By training large language models (LLMs) with diverse tasks and corresponding instructions, it allows the model to achieve a decent cross-task generalization ability~\cite{wei2021finetuned,sanh2021multitask, Wang2022SuperNaturalInstructionsGV, alpaca, vicuna2023, ouyang2022training}.
Following the observation from prior research~\cite {xu2022zeroprompt, wang2022self} that scaling up the number of tasks can significantly improve zero-shot generalization, there is research on continuously adding knowledge to large language models~\cite{scialom-etal-2022-fine, jang2023exploring}, along with many large-scale IT datasets emerged.
\citet{wang2022self, wei2021finetuned, bach2022promptsource, xu2022zeroprompt, Jiao2023ParroTTD} manually augment existing datasets to form large-scale IT datasets and \citet{gupta2022improving, finlayson2022makes} manually construct new IT datasets in specific domains. There are also automatic approaches to collecting large-scale IT datasets. \citet{wang2022self, honovich2022unnatural} propose generating moderate-quality data from powerful IT models, like GPT-4 and Chat-GPT~\cite{openai2023gpt4}.
Recently, Dynosaur~\cite{yin2023dynosaur} proposes to curate instructions for the continuously growing huggingface dataset~\cite{lhoest-etal-2021-datasets} using GPT-4 to create high-quality IT data with low costs.
% With the rapid growth of existing tasks in IT datasets, there is also work focusing on continuously adding knowledge to large language models without catastrophic forgetting~\cite{scialom-etal-2022-fine, jang2023exploring} by applying continual learning techniques or combining expert models.
Additionally, we would like to highlight that as IT models rapidly scale in performance with larger models and datasets, the concern of whether they adhere to instructions still remains~\cite{yin2023did, kung2023models, min2022rethinking, yang2023lacma, li2023evaluating, xue2023tadis}, and requires further investigation. For recent IT development, see \cite{zhang2023instruction} for a detailed survey.\looseness=-1
% In this work, we aim to select novel tasks to improve models cross-task generalization ability. By training on diverse task type and instructions, we may potentially improve the 


% With the rapid growth of existing tasks in IT datasets, we propose the \textit{Active Instruction Tuning} framework to identify and select the most informative tasks for efficient instruction tuning.\looseness=-1

% In this work, we follow prec
% Following the recent trend in rapid scaling up instruction tuning

% Instruction tuning is a 
% \begin{itemize}
%     \item Recently IT draw much attention, a lot of models and datasets emerged. 
%     \item Following the observation that IT with more tasks lead to better performance, it is important to select useful tasks.
%     \item Motivate by this, we propose active instruction tuning in this work
% \end{itemize}
% Instruction tuning can improve models' cross-task generalization by training massive amount of tasks with instructions. While recent IT models and datasets scales rapidly
% \textcolor{blue}{Will write this part the last, in case we have no space for related-work}
% Instruction tuning, which multi-task learning many tasks with instructions, have drawn much attention in the NLP field. Following the success of in-context learning, many work started to meta-learned the model with insructions, to achieve better instruction following ability ~\cite{}
% Coming with the success of in-context learning, many work tried to meta-learned the model with instructions to achieve better instruction following ability. While how model utilize instructions are still debatable~\cite{yin-2023-did, kung2021efficient}, instruction tuning are definitely useful for better model performance.
% Recently instruction tuning have drawn much attention in the NLP field, many new models~\cite{wei2021finetuned, finlayson2022makes, ouyang2022training, Wang2022SuperNaturalInstructionsGV, alpaca, vicuna2023} and datasets ~\cite{} have been proposed. While there are many instruction tuning models, 
% Discuss the difference in current instruction tuning, and why both of them are important.

\subsection{Uncertainty Estimation for LLMs}
Uncertainty estimation is essential for ensuring safe deployments of neural networks~\citep{abdar2021review}. Prior works have decomposed the total uncertainty into aleatoric (data) uncertainty and epistemic (model) uncertainty, and proposed methods to quantify each of them, represented by Monte-Carlo Dropout~\citep{gal2016dropout} and Deep Ensemble~\citep{lakshminarayanan2017simple}. In particular, data uncertainty measures the intrinsic uncertainty from the data distribution. Model uncertainty measures the uncertainty due to lack of understanding of the task, and can be leveraged to detect adversarial or out-of-distribution data~\citep{feinman2017detecting, Yin2022ADDMUDO}. Recent works have also extended uncertainty quantification to autoregressive language models~\citep{xiao2019quantifying, malinin2020uncertainty}. In this work, we propose a novel epistemic uncertainty measurement for instruction-tuned LLMs by measuring the disagreement of models conditioned on perturbed instructions.\looseness=-1
\subsection{Active Learning and Task Selection}
Our work is also related to active learning, which iteratively annotates informative instances from an unlabeled pool for efficient training~\citep{olsson2009literature, siddhant-lipton-2018-deep, zhang-etal-2022-survey}. Strategies for querying informative instances fall into different categories. See~\citet{zhang-etal-2022-survey} for a detailed survey. Our method is more related to disagreement-based active learning~\citep{houlsby2011bayesian, siddhant-lipton-2018-deep, shen-etal-2017-deep}, which queries for instances where multiple models disagree the most, and is usually combined with model uncertainty measurements~\citep{gal2016dropout}. However, different from active learning which selects informative instances, we consider selections at task-level. We show that simply adopting prior active learning strategies at task-level do not work well and propose our own methods. There are also works doing task selection for specific target tasks~\citep{parvez-chang-2021-evaluating, zhou2023not}. However, we do not assume knowledge of the target task but select tasks solely based on the uncertainty information of the model.
