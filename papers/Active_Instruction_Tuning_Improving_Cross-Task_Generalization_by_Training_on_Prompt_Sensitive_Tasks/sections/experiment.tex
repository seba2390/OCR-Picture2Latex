In this work, we experiment with two well-known IT datasets: NIV2 and Self-Instruct~\cite{Wang2022SuperNaturalInstructionsGV, wang2022self}. NIV2 is the largest IT dataset with 1600+ cross-lingual tasks. It focuses on improving model generalization to unseen tasks, while Self-Instruct is used to train the Alpaca model~\cite{alpaca} and aims to enhance model instruction following ability, following the categorization in prior work~\cite{kung2023models}. For detailed setting and comparison, please see \autoref{table:exp-settings}.\looseness=-1
% NIV2 is the current largest instruction tuning dataset with 800+ English tasks and human-crafted human-readable instructions, which we test . For \textit{generalize to unseen instruction} type, we train an LLaMA~\cite{touvron2023llama} model with 52K self-instruct data~\cite{wang2022self} and evaluate on 252 user-oriented instructions test set following the Alpaca model~\cite{alpaca} setting.

\subfile{../tables/exp_settings}
\subsection{Active Instruction Tuning Setting}
\paragraph{Natural Instruction V2 dataset}
We utilize the NIV2 English tasks split, comprising 756 training tasks and 119 testing tasks, including classification and generative tasks, and run our experiment with \textbf{five random seeds}.\footnote{We provide details of our experiments in \autoref{subsec:experiment-details}.}
For each randomized setting, we first randomly select 68 tasks for the initial training set and select another 68 tasks as the validation set, leaving the remaining 620 tasks as the task pool. Afterward, we iteratively apply different task selection strategies to expand the training set and train new IT models, reporting the performance at each iteration $[136, 204, 272, 340]$.
\paragraph{Self-Instruct dataset}
% We utilize the 52K self-instruct dataset as the task pool. For active learning experiment, w
We first randomly sample 500 tasks as the initial training set from the 52K tasks in the Self-Instruct dataset, leaving the remaining tasks as the remaining task pool. We conduct active instruction tuning and compare model performance at each iteration $[1000, 2000, 4000, 8000, 16000]$.\looseness=-1
\subsection{Task Selection Strategies}
Since we are the first to propose active instruction tuning, we construct several baseline task selection strategies: \textit{Random Sampling}, \textit{High Perplexity} and \textit{Low Perplexity}, to compare with our proposed \textit{Prompt Uncertainty} method.
\textit{Random Sampling} will randomly sample tasks from the remaining task pool. This is usually a strong baseline in task-selection experiments 
since we utilize a well-constructed dataset as the task pool, which has less noisy and duplicate data. 
\textit{High and Low Perplexity} are the baselines inspired by prior active learning work, which aims to select difficult/easy tasks by measuring predicted sentence perplexity for generation tasks or entropy for classification tasks. As these uncertainty measurements are established at the instance level, we aggregate the uncertainty score of multiple (ten for NIV2 and one for Self-Instruct) instances in a task to estimate task-level uncertainty.
For our method, we measure the \textit{Prompt Uncertainty} using $n=10$ random examples and $k=20$ prompt perturbations in NIV2 (refer to \autoref{sec:method}). 
For Self-Instruct, we measure the prompt uncertainty using $n=1$ random examples and $k=20$ prompt perturbations.\looseness=-1
% Noted that each task in Self-Instruct training set only has one corresponding example, so we cannot set a higher $n$ value.\looseness=-1





% focus our experiment to compare \textit{Random Sampling}, which shows superior performance
% measure the prompt uncertainty using $n=1$ random examples and $k=20$ prompt perturbations. Noted that each task(instructions) in Self-Instruct training set only have one corresponding example, so we cannot set a higher $n$ value. Also, we do not compare 
% For Self-Instruct, we only compare \textit{Prompt Uncertainty} to \textit{Random Sampling} due to overwhelming cost for human evaluation. Also, 
\subsection{Training and Evaluation}
For NIV2, we follow the current SOTA TK-instruct model's setting, to train the T5-770M model~\cite{raffel2020exploring} and report the Rouge-L score of \textit{Classification}, \textit{Generative} and \textit{Overall} tasks, on both validation and testing set. During training and testing, we will provide a task definition and two examples as instruction demonstration.
For Self-Instruct dataset, we train the LLaMA-7B model~\cite{touvron2023llama} follows Alpaca model setting.
For evaluation, we report the blind pairwise comparison of each task selection methods with \textit{Random Sampling} on the 252 user-oriented test set~\cite{wang2022self}. We follow the evaluation in Vicuna~\cite{vicuna2023} to report GPT-4, Chat-GPT (GPT 3.5) and Human evaluation scores, and provide more details in \autoref{subsec:human-eval}.
