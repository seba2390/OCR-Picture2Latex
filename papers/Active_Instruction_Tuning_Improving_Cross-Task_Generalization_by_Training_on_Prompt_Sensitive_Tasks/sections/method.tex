% In this section, we discuss the Active Instruction Tuning framework and our proposed Prompt Uncertainty method. \looseness=-1
% we extend \textit{Active Learning} setting from instance-level to task-level. 
\subsection{Active Instruction Tuning} The Active Instruction Tuning framework is illustrated in \autoref{fig:overall-pipeline}. In reality, when the number of tasks is large and continuously expanding, training on all existing tasks becomes impractical due to the overwhelming computing cost.
To efficiently improve an instruction tuning model, we can apply a task selection method to actively select the tasks that benefit the current model the most.
By repeating this model training and task selection loop, we can continually improve instruction-tuned models' generalization to unseen tasks.

% For the experiment, we assume there is a large training task pool with a fixed size. The whole training procedure is separated into multiple iterations. For the first iteration, we randomly sample a small number of tasks to train a weak instruction-tuned model. 
% For later iterations, based on the model from the previous iteration, we actively select the most useful tasks, and train a new model with all selected tasks. We compare several task selection strategies by evaluating the model on a set of unseen tasks at each iteration. We also propose our own selection strategy, which is introduced in the next section and demonstrated to be the most effective one.
For the experiment, we use a large training task pool of fixed size. The training procedure consists of multiple iterations. In the first iteration, a small number of tasks are randomly sampled to train a weak instruction-tuned model. In subsequent iterations, we actively select the most useful tasks based on the previous model and train a new model with the selected tasks. We evaluate different task selection strategies by testing the model on unseen tasks at each iteration.\looseness=-1
% After having a weak IT model, we will continue the loop of task selection and instruction tuning. 

% and train a new model with new training tasks from the task pool based on  for multiple iterations.
% iteratively selected more tasks by task selection method and use it to perform instruction tuning. 
% separate tasks into 
% to fairly compare tasks selection methods and make it more reproducible, we will conduct experiment on a fixed task pool, following a similar setting as \textit{Active Learning}.  

% most novel and informative 
% inefficient and difficult to annotate and train on all tasks. 
% To improve an IT model's performance, we will dynamically measure the uncertainty of the current model state to annotate and continual learning 

% In realistic scenario, while

% In this work, we introduce Active Instruction Tuning, a framework to continuously improve the IT modelâ€™s generalization ability in large scale instruction tuning. 
% Active learning aims to select and annotate the most "useful" data for model training to improve performance during testing. \hb{cite?}
% In the context of instruction tuning, active instruction tuning select and annotate the most useful tasks at each model state.
% We demonstrate active instruction tuning in realistic and experiment settings in \autoref{fig:overall-pipeline}. 
% In realistic scenarios where the unlabeled task pool is vast and continuously expanding, it is inefficient and difficult to annotate and train on all tasks. To improve an IT model's performance, we will dynamically measure the uncertainty of the current model state to annotate and continual learning \hb{jargon without much explanation. better say "update the model parameters"} the model with the extended most useful tasks. 
% By continuing this training loop, we expect the model to achieve better performance with less annotating/computing cost.
% Maybe we can mention continual learning in the realistic scenario
% For the experiment setting, while it is similar to the realistic scenario, we will focus on a fixed-size unlabeled task pool without continual learning to make the experiment comparable and reproducible \hb{this line is unclear to me}. In the first stage, we will train our model with a few random sampled tasks to get an initial model state, which is a weak instruction tuning model. After the initial training, we will iteratively do three steps: \textbf{Uncertainty Prediction} to measure the \textbf{Task Uncertainty} of all the remaining tasks; \textbf{Task Sampling} to select tasks using the \textbf{Task Uncertainty} score and add into the training task pool; \textbf{Model Training} to train a new model with the extended task pool, to achieve next model state. 
%Noted that we follow previous active learning work to train a new model from scratch at each iteration instead of continual learning in the experiment settings, to eliminate the influence of continual learning and focus more on the selected tasks quality.
% In the experiment, we will further evaluate the model performance at each model state to compare each task selection method. 
% Noted that \textbf{Random Sampling} is usually a strong baseline in active learning experiment settings.
% When we use a well-constructed dataset with less noise/duplication data/task, random sampling data/task from this dataset can already guarantee a certain degree of task diversity and quality, which is not applicable in realistic settings.

\subsection{Prompt Uncertainty}
Inspired by uncertainty-based active learning~\citep{siddhant-lipton-2018-deep}, we aim to select those highly uncertain tasks as the most informative ones at each stage for training.
While prior active learning work has proposed numerous uncertainty measurements at the instance level for a single task, these uncertainty values are usually not comparable across tasks. We propose Prompt Uncertainty, a task-level uncertainty measurement that estimates uncertainty values by assessing the disagreement of the model on the original prediction given complete and perturbed task instructions. 
By selecting those most prompt-uncertain tasks, we can select the tasks to which the current model is susceptible.\looseness=-1

\paragraph{Prompt Uncertainty Measurement}
Our Prompt Uncertainty method is motivated from \textit{Bayesian Active Learning by Disagreement (BALD)} ~\cite{houlsby2011bayesian} in single task Active Learning.
Instead of measuring the disagreement among ensemble models in a single task, we measure the disagreement of generation likelihoods on the original prediction over perturbed prompts and original prompts of a task.
\autoref{fig:prompt-uncertainty} illustrates the process of measuring the prompt uncertainty of a model to a task's instance $x_0$.
To measure the prompt uncertainty $U_{t}$ for task $t$ given model weights $W$, corresponding unlabeled dataset $X_t$ and instruction (prompt) $I^t_{0}$, we calculate the average disagreement of likelihood between perturbed and original instruction on $n$ randomly sampled examples from $X_t$. \looseness=-1
\[
% \label{eq:prompt-uncertainty}
U_t = \frac{1}{n}\sum_{i=1}^{n}\frac{1}{k}\sum_{j=1}^{k} |p^t_{i,0} - p^t_{i,j}|,
\]
\[
% \label{eq:prob}
% p^t_{i,j} = \{ P(y^t_i|x^t_{i}, I^t_{j}, W) \vert i \in [1,n], j \in [0, K]\}.
p^t_{i,j}\!=\! P(y^t_i|x^t_{i}, I^t_{j}, W),
\]
\[
\text{where } i \in [1,n] \text{, } j \in [0, k]. 
\]
$P$ is the likelihood of prediction $y$ given model weights $W$, a task instruction $I$ and corresponding task instance $x$. $k$ is the number of perturbations.
For each example $x^{t}_{i} \in X^t$, we will first get the original output $y^t_i$ and its corresponding likelihood $p^t_{i,0}$. Then, we will perturb the instruction $k$ times and calculate the average absolute difference between the likelihood of $y^t_i$ given original instruction $p^t_{i,0}$ and perturbed instructions $\{p^t_{i,j} \vert j \in (1, k)\}$. \looseness=-1

In order to perturb a task instruction, it is possible to employ paraphrasing techniques, adding extraneous tokens or randomly omitting words, such that the altered instructions can mostly preserve their meaning (A more detailed discussion can be found in \autoref{subsec:prompt-perturb-meaning}).
In our experiment, we assign a 0.2 drop rate for each word in the instruction to create perturbed instructions. After getting the prompt uncertainty for each remaining task, we will select the highly uncertain ones and add them to the training task pool.\looseness=-1

\paragraph{Underlying Hypothesis}
We describe the underlying hypothesis to propose Prompt Uncertainty.
From an uncertainty perspective, when measuring the model's sensitivity toward sampled prompts from a task, we estimate the model's epistemic uncertainty, reflecting the model's lack of knowledge of a particular task. Different from epistemic uncertainty using an ensemble of models~\citep{gal2016dropout}, we consider an ensemble of slightly different conditions, i.e., perturbations of prompts for the model, and use the original likelihood to represent the ensembled prediction. From the robustness of the in-context learning perspective, if a model cannot robustly map task instructions to specific latent concepts, which is reflected by the sensitivity regarding perturbations in instructions, its generalization ability to the corresponding task is limited~\cite{xie2021explanation, pan2023context}. 
To address this, we hypothesize that training the model on prompt-uncertain tasks will improve its ability to associate prompts with specific latent concepts (tasks), leading to a better zero-shot performance on unseen instructions.