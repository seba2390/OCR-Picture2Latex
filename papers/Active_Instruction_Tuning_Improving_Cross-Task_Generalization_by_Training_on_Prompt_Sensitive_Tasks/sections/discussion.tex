\subsection{Prompt Uncertainty Reflects Task Novelty}
To demonstrate how prompt uncertainty reflects the novelty of tasks to a model, we designed a controlled experiment to visualize how the prompt uncertainty scores of tasks change after the model is trained with relevant tasks.
To collect a set of relevant tasks, we first gathered eight \textit{Word Analogy} tasks from the NIV2~\cite{Wang2022SuperNaturalInstructionsGV} testing set, which is held unseen from the NIV2 training set. 
In \autoref{fig:task-map-shift}, we measured the prediction probability and prompt uncertainty for 620 unseen tasks (unrelated to analogy tasks) from the NIV2 training set and four of the unseen analogy tasks using an instruction-tuned model, labeled as \textbf{M0}, and plotted the task map in blue. 
We further trained the \textbf{M0} model with the other four analogy tasks, resulting in a new model called \textbf{M1}, and used it to plot the task map for the 620 irrelevant tasks and four unseen analogy tasks again in orange. 
It is evident that after training the \textbf{M0} model with the four analogy tasks, the overall prompt uncertainty distribution of the 620 irrelevant tasks remains relatively unchanged, while the prompt uncertainty of the four unseen analogy tasks consistently and significantly decreases.\footnotemark 
This demonstrates that prompt uncertainty can effectively indicate the novelty of tasks within the model. When the model is trained with specific tasks, the prompt uncertainty of those relevant tasks notably decreases.
Additionally, please note that the prediction probability does not increase after training with similar tasks for these four analogy tasks. This observation highlights that using prediction probability alone cannot effectively reflect the novelty of tasks.\looseness=-1
\subfile{../figures/task_map_shift}
\subsection{Prompt Perturbation Methods}
\label{subsec:prompt-perturb-meaning}
While prompt perturbation methods are meant to slightly perturb the prompt without changing its meanings, it is difficult to 100\% guarantee the preservation of instruction meaning after automatic paraphrasing methods. To ensure the prompt uncertainty is not measured using an extreme perturbation case, we perturbed all instructions 20 times in our experiments. We also tried several instruction perturbation methods at our early experiment stage, such as randomly repeating tokens or adding extraneous tokens, which achieved similar prompt uncertainty scores as randomly dropping words. Additionally, for the NIV2 and Self-Instruct datasets we used, which have detailed instructions with many redundant tokens (average 56 words per instruction), randomly dropping 20\% of tokens will mostly preserve the meaning of the instructions. For other datasets with concise instructions, a higher dropping rate is needed to perturb the instructions, leading to a higher probability of changing instructions meaning entirely.\looseness=-1
\footnotetext{From M0 to M1, the average decrease in prompt uncertainty scores is 0.0018 for the 620 irrelevant tasks and 0.039 for the four analogy tasks. The prompt uncertainty of analogy tasks decreases 21 times more than that of the irrelevant tasks.}
