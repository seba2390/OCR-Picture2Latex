\subfile{../figures/overall-pipeline}
% \hb{Teaser Figure: i am confused if the tasks have the labels or not? In active learning, the tasks are unannotated and then get labeled. Here, it seems that you are calculating p(label | prompt) to decide which task to label? here output[0] seems like a label from the dataset. Mention that Output[0] is an output from the model not the Output of the input from the dataset itself.}

% \begin{itemize}
%   \item (LM success, trianing more is better) Recent LLM with instruction tuning have achieve great success. While there can be difference in current IT method, we know that training performance scale with more tasks/instructions.
%   \item (Recent proposed new IT datasets --> scale going larger) To further improvce model's performance, there're many large new datasets proposed from academic and industry, including academic benchmark and GPT-distilelled data.
%   \item While the dataset size is growing larger and larger, it become impossible to train all the tasks and instrucitons. From multi-task learning perspective, training on disproportionate tasks can also leads to negative transfer, hurting the performance. A strategic task selection method is urgently needed.
%   \item Existing work have discuss how to select data or tasks wisely in large data scenario. (Two type, diagnos or active learning type) ... Howerver, there isn't a work discussing how to select tasks for general instruction tuning scenario, which aim to find a balanced 
%   \item In this work, we propose task-level active learning by prompt uncertainty, which can
% \end{itemize}
% Success of instruction tuning
% 1. IT show great success and the performance can be boost by training more diverse task
% 2. Recent work also try to collect more diverse tasks types and instructions by ..., leading to a rapid growth of IT scales
% 3. While the rapid growth of IT task can provide abundent tasks for IT, it also become impractical to train on all tasks. How to select most informative tasks from this gigantic set and to achieve better IT performance becomes a challenge.
% Next
% Prior work have discuss data and tasks selection in multiple directions
Recently, instruction tuning has shown great success in improving large language models' cross-task generalizability.
When training large language models (LLM) with a wide range of tasks with instructions, models like T0~\cite{sanh2021multitask}, FLAN~\cite{wei2021finetuned}, TK-Instruct~\cite{Wang2022SuperNaturalInstructionsGV}, Instruct-GPT~\cite{ouyang2022training}, Alpaca~\cite{alpaca} and Vicuna~\cite{vicuna2023} can perform well on unseen task.
% \hb{earlier in the sentence you say tasks with instructions, now it is task or instructions -- be consistent}
The performance can be further boosted by increasing the number of diverse training tasks~\cite{xu2022zeroprompt,Wang2022SuperNaturalInstructionsGV, longpre2023flan, chung2022scaling}.
% Recent work also try to collect more IT tasks and data, construting a massive amount of existing tasks.
% Following the rapid growth of IT dataset, we aim to train IT model with these gigantic tasks
Based on this observation, many recent studies scale up instruction-tuning datasets by manually or automatically curating more tasks with instructions. 
For example, T0 and FLAN have 60 tasks. The NIV2~\cite{Wang2022SuperNaturalInstructionsGV} benchmark extends its dataset to over 800 English training tasks. Self-Instruct ~\cite{wang2022self} and Unnatural Instructions ~\cite{honovich2022unnatural} prompt LLMs to generate over 50K instruction tuning data, and recently, Dynosaur ~\cite{yin2023dynosaur} dynamically curates over 80K instruction tuning data from Huggingface datasets~\cite{lhoest-etal-2021-datasets}, which is still continuously expanding.\looseness=-1
% While these works provide a massive amount of tasks with instructions and rapidly scale up instruction tuning, it also presents concurrent challenges. 
% To achieve better generalization for instruction tuning, it becomes impractical to train on the gigantic amount of existing tasks due to overwhelming computing cost. Furthermore, randomly selecting a subset of training tasks can potentially lead to a disproportionate task set, resulting in negative transfer~\cite{zhang2022survey} and harms the overall generalizability~\cite{zhou2023not}. In regard to these challenges, it is crucial to employ an efficient task selection strategy that identifies the most informative and valuable tasks for instruction tuning, to achieve better zero-shot cross-task generalization.

% While these works provide a massive amount of tasks with instructions and rapidly scale up instruction tuning, it also presents concurrent challenges. 
% % To achieve better generalization for instruction tuning
However, as the scale of datasets grows rapidly, it becomes impractical to train on all existing tasks due to overwhelming computing costs. 
%Sometimes, one can only select a subset of the whole dataset to train the model given computational constraints.
% \old{Furthermore, randomly selecting a subset of training tasks can
% lead to a disproportionate task set, resulting in negative transfer~\cite{zhang2022survey} and harms the overall generalizability~\cite{zhou2023not}.}
% \new{
One naive solution is to randomly sample tasks for training, but it can potentially select less informative tasks, leading to suboptimal results~\cite{wang2023far}.
% }
Therefore, it is crucial to employ an efficient task selection strategy that identifies the most novel and informative tasks for instruction tuning.\looseness=-1
%to achieve better zero-shot cross-task generalization.
% \hb{i like this motivation -- "It is unclear to what extent should we train our models -- (a) do we train on everything or (b) do we train on tasks whose capabilities are missing from the existing models. (a) is definitely possible to do but seems impractical due to compute cost associated with the large data. this line of thought seemed missing from the abstract"} \violet{+1} 


% While scaling improve performance, it is inefficient and infeasible to traing on all tasks.
% "Dataset diagnosing" refers to the process of analyzing and understanding the characteristics, quality, and potential issues within a dataset. It involves examining the dataset to identify any problems, inconsistencies, biases, or errors that may exist, as well as assessing the overall suitability of the dataset for a particular task or analysis.

% It often overlooks the efficiency of analyzing methods and is impractical for data selection in real-world scenarios.
% \textit{Dataset Diagnosing} aims to understand a dataset's characteristics and potential issues to provide valuable insights, which often overlooks the efficiency of analyzing methods and require annotated labels in datasets, making it impractical for large-scale, real-world data selection.
% \violet{I will frame it as ``data/task selection has been explored under active learning and multi-task learning frameworks, however, they are not applicable for tasks/instruction selection in instruction tuning''.}
Data selection has been explored under active learning and multi-task learning frameworks. Despite its prevalence, we argue that they are not applicable to task selection for instruction tuning.
% Existing work has addressed the selection of informative data/tasks in both single-task and multi-task scenarios. In single-task settings, 
Specifically, active learning methods have focused on selecting the most useful \textit{instances} for a single task, using either uncertainty-based intuitions such as entropy~\cite{settles2009active}, Monte Carlo dropout~\cite{gal2016dropout}, or ensemble disagreement~\cite{houlsby2011bayesian, siddhant-lipton-2018-deep}. However, these uncertainty measurements can only measure uncertainty at \textit{instance-level}, 
% \old{make it not directly applicable to task selection.}
and will become less effective when applied to \textit{task-level} selections as the scales of uncertainty values are not comparable across tasks.
In multi-task learning, previous research~\cite{ivison2022data, poth2021pre, kung2021efficient} has explored measuring task usefulness by assessing its similarity to the target task. While these methods can enhance performance when aware of the target tasks, they are not suitable for instruction tuning, which aims to improve \textit{overall generalization} to arbitrary \textit{unseen} tasks. 
% \hb{do we need to talk about these works here since it feels more like a related work section than an introduction. if you really want you can mention in a few lines that prior works are not directly transferable since their settings and objectives varied a lot from your setup.}
% \hb{is their a reason why MC dropout, entropy based methods cannot be applied to your setup -- such questions will arise if you mention them here explicitly.}
% Existing work has explored data usefulness and selection in the field of \textit{Dataset Diagnosing} and \textit{Active Learning}.
% The former aims to understand a dataset's characteristics and potential issues. Datamap~\cite{swayamdipta-etal-2020-dataset, park2022calibration} proposed to categorize data in task finetuning scenario using training dynamics.
% Recent work~\cite{zhou2023not} discusses how each task can have different usefulness in instruction tuning scenarios by observing pairwise generalization results from the training task pool and resampling for instruction tuning.
% While these \textit{Dataset Diagnosing} work try to identified data/task usefulness, their analyzing methods overlooks the efficiency and is more computing expensive than training through all data/tasks, making it infeasible for large-scale, real-world data selection.
% For \textit{Active Learning}, it aims to identify data usefulness efficiently for a specific target task. In single-task finetuning, previous work utilize uncertainty methods, such as entropy~\cite{settles2009active}, Monte Carlo dropout~\cite{gal2016dropout} and disagreement~\cite{houlsby2011bayesian, siddhant-lipton-2018-deep} to identify most useful data instance. For multi-task learning scenarios, previous research~\cite{ivison2022data, poth2021pre, kung2021efficient} identify task usefulness by measuring its similarity to the target task. While these \textit{Active Learning} work can efficiently select useful data/task, all of them are focusing on improving a specific task performance, which can be fundemantally different from instruction tuning scenario, which aim to achieve better zero-shot cross-task generalization, without a specific target task. 
% Despite the existing research on data/task usefulness across various scenarios, our goal to efficient select tasks for better instruction tuning remain unresloved. 

% \violet{I'll separately introduce the setting -- active instruction tuning, and the approach -- using prompt uncertainty. This will both be clearer and allow follow-up works to improve on either. As a result, you should spend more space introducing/motivating the setting. For now, you directly get into the approach without explaining what's active IT and why it's important/distinctive.}
% "It is unclear to what extent should we train our models -- (a) do we train on everything or (b) do we train on tasks whose capabilities are missing from the existing models. (a) is definitely possible to do but seems impractical due to compute cost associated with the large data. this line of thought seemed missing from the abstract"
% Active instruction tuning
% Challenges: (1) Because IT scales is continuously growing, training on all tasks in impractical. We need a task select method to efficiently and actively select the most informative / novel tasks for IT. (2) While previous 
% What is this? --> Active instruction tuning is a task, which aims to select most...
% Why do we need this? --> In realistic scenario when the number of tasks and the scale of instruction tuning is rapidly growing, it's computing expensive to train on all tasks and random sampling is suboptimal, we need a tasks selection  

% 
% \old{
% In this work, we introduce \textbf{Active Instruction Tuning}, the first framework to continuously improve the IT model's generalization ability in large-scale instruction tuning, demonstrated in \autoref{fig:overall-pipeline}. 
% Given an IT model, we need to apply a task selection strategy to actively select novel and informative tasks for instruction tuning to efficiently improve models' zero-shot generalization to unseen tasks.
% }
% \new{
In this work, we introduce \textbf{Active Instruction Tuning}, a framework that aims to actively identify informative new tasks for an IT model to continuously improve its cross-task generalization ability  (refer to \autoref{fig:overall-pipeline}).
% }
While being related to active learning, our task is more challenging.
Unlike active learning, which focuses on improving \textbf{performance on a single target task} by identifying useful \textit{instances}, our goal is to identify tasks that enhance \textbf{overall generalization}, a novel concept not explored in previous AL research. 
%Also, prior AL work focuses on measuring instance-level uncertainty, which cannot measure the uncertainty of a task, posing a challenge in understanding task-level uncertainty.

% \new{
To identify informative new tasks for Active Instruction Tuning, we propose \textit{prompt uncertainty} (refer to \autoref{fig:prompt-uncertainty}), a novel task-level uncertainty metric that %method for instruction tuning.
measures the sensitivity of an IT model against instruction perturbations for a task. 
Specifically, with a task instruction and a few unlabeled instances, we assess the disagreement of model predictions against original and perturbed prompts on multiple instances to obtain an average disagreement score.
We then select and train the model with the most prompt-uncertain tasks to enhance the overall cross-task generalization ability. 
Since this uncertainty method does not require labeled instances for a task, we can also apply prompt uncertainty to determine novel tasks to manually annotate if needed. %\looseness=-1
% In a realistic scenario when a large corporation can train all existing tasks, they 
% , we can apply prompt uncertainty to determine and manually annotate novel tasks if needed since this uncertainty method does not require labeled instances within the task.
% }
% In this work, we propose Active instruction tuning using prompt uncertainty, which can actively determine the most beneficial training task set during instruction tuning and improve models zero-shot cross-task generalization. 
% For an instruction-tuned model, we will \hb{remove `will'} measure the models' sensitivity towards prompts for each different \hb{each different sounds wrong} task and select the tasks with the highest sensitivity. By choosing the most prompt-sensitive tasks, we can train the model with more diverse tasks and better generalize to new instructions. 
% Specifically, to select mosfor a model and a large pool of unannotated training tasks \hb{complete this by saying that the task is to select a subset to be labeled.}, we will measure tasks' prompt uncertainty by assessing the discrepancy of generation likelihood \hb{generation of what? p(label | prompt)? or the likelihood of the prompt itself p(prompt)?} between original and perturbed instructions \hb{are the perturbed instructions sensical or non-sensical}. The disagreement scores reflect the model's susceptibility to specific tasks, indicating their usefulness to the model. We then collect and train the model with the most prompt-uncertain tasks to enhance the overall cross-task generalization ability.
% In a realistic scenario, we can further determine which new tasks to annotate and train to improve current instruction tuning model, since we can measure prompt uncertainty for unlabeled task data. \hb{this is the first time you have mentioned something related to active learning. no prior mention till now in the introduction}

% \violet{this feels a little loose. Why do you want to introduce task map?}

% \new{
%Besides the effectiveness of our proposed \textit{prompt uncertainty} in \textbf{Active Instruction Tuning}, w
We further explore using \textit{prompt uncertainty} to understand task characteristics and diagnose potential issues. Motivated by Data Map~\cite{swayamdipta-etal-2020-dataset}, which utilizes instance-level training dynamics to categorize and diagnose data quality, we propose \textbf{Task Map}, the first task diagnosing method that categorizes tasks based on Prompt Uncertainty and Prediction Probability. Based on the Task Map, we 
% discuss how Task Map can disentangle \textit{Task Recognition} and \textit{Task Performing} in in-context learning~\cite{pan2023context, xie2021explanation} and 
categorize tasks into \textbf{Ambiguous}, \textbf{Easy} and \textbf{Difficult}, inspired by prior in-context learning research~\cite{xie2021explanation, pan2023context} to facilitate analysis. 

We conduct experiments on two instruction tuning setting: TK-Instruct models with NIV2 dataset, which generalize to unseen tasks, and Alpaca models with Self-Instruct dataset, which generalize to unseen instructions, following our categorization in~\newcite{kung2023models}. Results %in \autoref{fig:niv2-results} and \autoref{fig:alpaca-results} 
show that our active instruction tuning method consistently outperforms baseline methods (random sampling, generation perplexity) for both instruction tuning setting, demonstrating the effectiveness of our approach.
% \hb{can add some numbers..}.
%As shown in \autoref{sec:task-map}, e 
Moreover, we discover that while instruction tuning with \textbf{Ambiguous} tasks can improve generalization effectively,  \textbf{Difficult} tasks offers no benefit, underscoring the importance of task selection in instruction tuning.
Our contributions can be summarized as follows:
% }
\begin{itemize}
% \itemsep0em 
    \item We introduce Active Instruction Tuning, a framework to efficiently improve the IT model's generalization ability in large-scale instruction tuning. \looseness=-1
    \item We propose Prompt Uncertainty, a task-level uncertainty measurement for IT, which can identify novel/informative tasks to improve IT models' zero-shot generalization.\looseness=-1
    \item We further propose Task Map, a task diagnosing tool that categorizes tasks based on their prompt uncertainty and prediction probability, providing insights into task characteristics and quality.\looseness=-1
% \hb{where is the `scaling up' part of the title in abstract and introduction?}
\end{itemize}


\subfile{../figures/teaser_figure}