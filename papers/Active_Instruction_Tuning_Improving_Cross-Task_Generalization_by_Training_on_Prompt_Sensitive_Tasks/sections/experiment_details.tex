We provide the details of our experiments on two well-known instruction tuning datasets: NIV2~\cite{Wang2022SuperNaturalInstructionsGV} and Self-Instruct dataset~\cite{wang2022self}. 
\paragraph{NIV2 - Active Instruction Tuning Details}
We utilize the NIV2 English tasks split, comprising 756 training tasks and 119 testing tasks, including classification and generative tasks.
We employ five random seeds without selection in our active instruction tuning experiment. Each seed involves randomly sampling 68 tasks as initial training tasks and 68 tasks as validation tasks. The remaining 620 training tasks form the remaining task pool. 
In each active learning iteration, we maintain a fixed classification and generative task ratio and select 24 classification tasks and 44 generative tasks using different task selection strategies. This fixed ratio allows a more meaningful comparison of our results as we evaluate overall, classification, and generative task scores separately. After the new tasks are sampled, we add them to the previously selected training tasks and form a new training task set. We further train a new instruction tuning model with the updated training task set.
\paragraph{Self-Instruct - Active Instruction Tuning Details}
We utilize the 52K self-instruct dataset as the task pool. For the active instruction tuning experiment, we will randomly sample 500 tasks as the initial training set and further compare model performance at $[1000, 2000, 4000, 8000, 16000]$ training tasks. For task selection, we will first divide all tasks into 13 chunks by output sequence length $[[1,10], [11,20], ..., [121, 130]]$, and then apply the task selection methods on each chunk of tasks, following the ratio of the number of tasks in all chunks. We conduct this extra step to normalize the output sequence length of the selected task for each task selection method. This ensures there is no imbalance in output sequence length during task selection.
\paragraph{Training Details}
For experiments on NIV2 dataset~\cite{Wang2022SuperNaturalInstructionsGV}, we follow the TK-instruct setting, the SOTA model on the NIV2 dataset to train the T5-770M model~\cite{raffel2020exploring} with learning rate 2e-5, batch size 128 and 200 instances per task for eight epochs. We evaluate the model's zero-shot performance on the validation set at each epoch and select the model checkpoint with the best validation score. For evaluation, we follow ~\cite{kung2023models} setting to report the Rouge-L score of \textit{Overall}, \textit{Classification}, and \textit{Generative} tasks on both validation and testing sets. For experiments on Self-Instruct dataset~\cite{wang2022self}, We follow Alpaca's settings to train the LLaMA-7B model with learning rate 2e-5, batch size 128 for four epochs.

\paragraph{Computing Resources}
For the experiment on NIV2 dataset~\cite{Wang2022SuperNaturalInstructionsGV}, we conduct our experiments using 4 to 8 Nvidia 48GB A6000 GPUs. For each uncertainty method, it takes around 1200 GPU hours, a total of 5000 GPU hours(for a single GPU), to run all experiments for \autoref{fig:niv2-results}. For the experiment on Self-Instruct dataset~\cite{wang2022self}, we run with 2 Nvidia 80GB A100 GPUs. Each uncertainty method takes around 40 GPU hours, which sums to 160 GPU hours for all experiments in \autoref{fig:alpaca-results}.
