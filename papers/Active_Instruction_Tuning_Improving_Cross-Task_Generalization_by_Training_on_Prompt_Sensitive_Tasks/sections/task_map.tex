Prior work tries to understand a dataset's characteristics in the field of \textit{Dataset Diagnosing}. 
% When Prompt Uncertainty can measure task-level uncertainty, we explore using Prompt Uncertainty for task diagnosing.
Motivated by Data Map~\cite{swayamdipta-etal-2020-dataset}, we
propose \textbf{Task Map}, a model-based diagnosing tool that understands the contributions of different groups of tasks towards instruction tuning. Different from previous work using data correctness and variability to construct data map, we propose to map tasks with the dimensions of Prediction Probability and Prompt Uncertainty, as in \autoref{fig:task-map}. This follows a hypothesis from recent work in explaining in-context learning  (ICL)~\cite{xie2021explanation}:
when the model performs a task in-context during test time (ICL), it might implicitly map the prompt to a corresponding latent concept and perform the task under the concept.
Prediction Probability represents the model's confidence to perform a task, indicating task difficulties. In comparison, Prompt Uncertainty represents the consistency of a model to map a prompt to a certain concept, indicating the task's ambiguity to the model.
We further follow the above intuition to categorize the tasks into three types: \textbf{Ambiguous} tasks, where models fail to recognize them and have high prompt uncertainty; \textbf{Easy} and \textbf{Difficult} tasks, where models can map the prompts to a certain latent task knowledge (low prompt uncertainty) and perform the task with high/low confidence (sentence probability), respectively. We then use the tasks from these three categories for instruction tuning on NIV2~\cite{wang2022self} to understand the contributions of different groups of tasks.

\begin{figure}
    \centering
    % \includegraphics[draft]{}
    \includegraphics[width=0.482\textwidth]{pdfs/Task-Map-Results}
    \vspace{-2em}
    \caption{\footnotesize
    The performance comparison between training on \textbf{Ambiguous}, \textbf{Easy}, and \textbf{Difficult} tasks on the NIV2 dataset. The setting is similar to \autoref{fig:niv2-results}, but we run all methods with three runs for the early active instruction tuning iterations.\looseness=-1}
    \vspace{-1.5em}
    \label{fig:task-map-results}
\end{figure}

We show the results in \autoref{fig:task-map-results}. It is seen that while training on \textbf{Ambiguous} tasks can effectively improve IT generalization ability and outperform random baseline, training on \textbf{Easy} tasks and \textbf{Difficult} tasks is generally worse than randomly selecting tasks. Furthermore, when selecting more \textbf{Easy} tasks can still slightly boost the IT model's performance, \textbf{Difficult} tasks can be useless, showing no benefit to the IT model's performance with more training tasks. We hypothesize that \textbf{Difficult} tasks can be too specific and hard to learn, therefore useless for improving the IT model's cross-task generalization.
While our proposed Task Map can already help diagnose task quality for IT, we look forward to future work conducting a more comprehensive analysis to discuss the role of these task categories to bring a comprehensive understanding of instruction tuning and in-context learning.\looseness=-1
 % Our findings discover that when instruction tuning with \textbf{Ambiguous} can improve generalization effectively,  \textbf{Unlearned Tasks} offers no benefit, underscoring the significance of task selection in instruction tuning.