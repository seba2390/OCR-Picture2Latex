\subfile{../figures/alpaca-results}
\begin{figure}
    \centering
    % \includegraphics[draft]{}
    \includegraphics[width=0.482\textwidth]{pdfs/Task-Selected-Uncertainty}
    \vspace{-1.5em}
    \caption{\footnotesize
    The average uncertainty scores of the selected tasks at each active instruction tuning iteration.
    % For \textit{Prompt Uncertainty} at \textbf{Number of Training Tasks = 68}, the reported score is the average prompt uncertainty of the 68 tasks selected by the previous model (in this case, the initial model, which is trained with the 68 initial tasks). This also applied to subsequent iterations and "High/Low Perplexity". 
    For \textit{All Tasks}, we report the average uncertainty score of the 620 training tasks predicted by the initial model. \looseness=-1
    % As more active instruction tuning iterations take place, it is seen that the average uncertainty of the selected scores approaches the \textit{All Tasks} score, which indicates a shortage of desired uncertainty tasks in the task pool.
    }
    \vspace{-1em}
    \label{fig:niv2-uncertainty}
\end{figure}
\subsection{NIV2 Results}
\autoref{fig:niv2-results} displays our experimental results on the NIV2 dataset. For each task selection method, we iteratively select a batch (68) of tasks from the task pool (620 tasks) to train a new model, and compare model performance at each iteration. A better task selection method should achieve consistent superior performance at early iterations, when there are still plenty of tasks to select from.
\autoref{fig:niv2-results} demonstrates that when selecting less than 340 tasks (half of the task pool), our proposed \textit{Prompt Uncertainty} method consistently outperforms other baselines in terms of \textbf{Overall} scores for both the validation and testing sets.
This shows that training on prompt-uncertain tasks is indeed the most effective way for better zero-shot cross-task generalization ability. 
On closer examination, our method is highly effective for \textbf{Classification} tasks, surpassing all other baselines. 
For \textbf{Generative} tasks, the \textit{Low Perplexity} method performs well on testing tasks at early iterations but poorly on the validation set. This inconsistency suggests that the model's overall generalizability is not enhanced, but rather the low perplexity tasks in the training set coincidentally benefit the generative tasks in the testing set. Conversely, our proposed method achieves consistently good performance on both testing and validation tasks, outperforming \textit{Random Sampling} on testing tasks and exhibiting similar performance on validation tasks.
% When choosing more than 408 tasks, performance becomes inconsistent. The \textbf{High Perplexity} method shows a sudden improvement in performance for \textit{Classification} tasks, resulting in the best \textit{Overall} performance. This performance boost is due to a limited number of training tasks in experiment setting, causing the \textbf{High Perplexity} method to select tasks with low perplexity. 
% In Figure \ref{fig:niv2-uncertainty}, we illustrate the average uncertainty scores of the selected tasks using different task selection strategies at each iteration. It is seen that when selecting over 340 tasks (half of the tasks in the training pool), all task selection strategies start deviating and choose tasks with unfavorable uncertainty scores. For \textbf{High Perplexity} method, it started to select tasks with low perplexity score, compare to early iterations. This inconsistency is attributed to the lack of training tasks, making the results erratic and incomparable.
% Note that the lack of task will occur exclusively in an experimental setting. In a practical scenario with a rapidly growing number of tasks, it is expected that there will be a persistent presence of high-uncertainty tasks available for selection.

% We choose to report to 340 tasks (half of the training task pool) due to the lack of desired high/low uncertainty tasks.
We further investigate the trend of uncertainty scores during active instruction tuning.
In \autoref{fig:niv2-uncertainty}, we illustrate the average uncertainty scores of the selected tasks using different task selection strategies at each iteration. It is shown that when selecting for more than half of the tasks in the training pool, all task selection strategies start deviating and choose tasks with unfavorable uncertainty scores. For example, \textit{High Perplexity} method start selecting tasks with low perplexity scores due to the lack of high perplexity tasks. Specifically, when extending the training tasks from 340 to 408 using \textit{prompt uncertainty}, the average uncertainty score of selected tasks is already slightly lower than that of all tasks at the first iteration, indicating there are no high-uncertainty tasks to select from.
Note that the lack of uncertain tasks would occur exclusively in an experimental setting. In practical scenarios where the number of tasks grows rapidly, the exhaustion of uncertain tasks is less likely to happen. \looseness=-1

% there is expected to be a persistent presence of high-uncertainty tasks available for selection.\looseness=-1

\subsection{Self-Instruct Results}
\label{alpaca-result}
% \textcolor{blue}{For each of the five sample sizes from 1000 to 16000, we collect one sample via Random Sampling and another sample via the proposed method, fine-tune LLaMa on the samples, and compare the two via human evaluation and GPT automatic evaluation. For human evaluation, we recruit crowd-source workers on Amazon Mechanical Turk who are native English speakers, score at least 70\% on a qualification test, and pass the attention test. For the annotation task, three annotators are presented with the task instruction, the input, and the expected output, followed by two models' outputs in random order. The annotators are asked to indicate whether the first model wins, loses, or there is a tie. An example of the annotation interface is presented in appendix [sectionid]. The final comparison decisions are aggregated from the raw annotations using majority voting. We assign a tie label when all the annotators disagree. [discuss IAA here] [we define no-contradiction as agreement with the tie entries removed, since an annotator slightly supporting a model may also vote for a tie] We set the per-item reward to \$0.1 to reach an hourly rate of \$15. In total, we collect 3780 comparison annotations and the total annotation cost is approximately \$600.}
We show the pairwise preference comparison of all task selection methods against \textit{Random Sampling} in \autoref{fig:alpaca-results}.
First for \textit{Fully Trained}, we use the official Alpaca release~\cite{alpaca}, which was trained on all 52K tasks. We compare it to \textit{Random Sampling} at each active instruction tuning iteration. It is shown that for both GPT-4 and Chat-GPT evaluation, the \textit{Fully Trained} model outperforms \textit{Random Sampling} with a great margin. However, as more training tasks are randomly sampled, the difference in preferred tasks is diminishing, indicating that IT performance of the Alpaca setting scales with an increasing number of training tasks.

Secondly, for high/low perplexity and our proposed task selection method, we first fine-tune an LLaMA~\cite{touvron2023llama} model with 500 tasks and then iteratively extend the number of training tasks to $[1000, 2000, 4000, 8000, 16000]$. We then report the pairwise comparison results against \textit{Random Sampling} at each iteration.
\autoref{fig:alpaca-results} shows that \textit{Low Perplexity} and \textit{High Perplexity} are generally subpar with \textit{Random Sampling}, indicating that applying inadequate task selection strategies can hurt the model's performance.
In contrast, our \textit{prompt uncertainty} method is almost consistently more preferable by all GPT4, Chat-GPT, and human assessors when selecting less or equal than 8000 tasks, showing that training with prompt-uncertain tasks can lead to better generalization to the user-oriented test tasks. When the number of training tasks increases to 16000, the performance improvement diminishes along with a smaller remaining task pool, which aligns with our results on the NIV2 dataset.
Additionally, we discuss our observations regarding applying GPT4, Chat-GPT, and human assessors for pairwise comparisons.
It is seen that while the number of net winning tasks (Win task - Lose Tasks) varies a lot across each evaluation method, the overall trend is similar, showing a certain alignment of preference across these automatic or human assessors.

In conclusion, our experiments on NIV2 and Self-Instruct demonstrate that our prompt uncertainty method consistently improves cross-task generalization in two different instruction tuning scenarios, surpassing random sampling and other uncertainty baselines.\looseness=-1
% \autoref{fig:alpaca-results} shows the pairwise preference comparison between models. In the top sub-figure, we demonstrate that the instruction tuning performance also scale with more training tasks/instructions in \textbf{Alpaca} setting. We compare the performance of \textbf{Fully trained Alpaca} and \textbf{Random Sampling} using Chat-GPT evaluation. As more training tasks are randomly sampled, the difference in model preference diminishes, indicating that the performance of the \textbf{Alpaca} setting scales with an increased number of training tasks

% It is seen that when random sampling more training tasks, the model preference difference started to get smaller, proving that model performance in \textbf{Alpaca} setting do scale with more training tasks. 
% In the bottom sub-figure, we further show the pairwise comparison score between our proposed \textit{Prompt Uncertainty} method and \textit{Random Sampling} by both human and GPT evaluation. It is seen that our method can consistently outperform \textit{Random Sampling} with more winning tasks, and this performance improvement also marginalizes with more selected tasks and a smaller task pool, which align with our results on the NIV2 dataset. 

\subfile{../figures/task_map}