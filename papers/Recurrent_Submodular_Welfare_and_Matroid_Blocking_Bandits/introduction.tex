\section{Introduction}
The {\em multi-armed bandits} (MAB) model has proven itself to be a successful mathematical framework for studying sequential decision making problems in environments that are initially unexplored by the decision maker. Since its first introduction (see \cite{T33} and later \cite{LR85}), the framework has been extensively studied as a simple yet powerful abstraction of the trade-off between {\em exploration} and {\em exploitation}, ubiquitous in a plethora of applications ranging from online advertising/recommendation systems to resource allocation and dynamic pricing (see \cite{BB12,LS18} and references therein). In the {\em stochastic} MAB setting \cite{LR85} the decision maker sequentially chooses among a set of available {\em actions} (or {\em arms}), each associated with an i.i.d. stochastic reward, while in the {\em combinatorial} MAB setting \cite{CBL12}, a subset of the arms can be selected at each round, subject to feasibility constraints.

Despite the large number of variants of the MAB model that have been introduced, the majority of the results comply with the common assumption that playing an action does not alter the environment, namely, the reward distributions of the subsequent rounds (with notable exceptions discussed below). Only recently, researchers have focused their attention on settings where temporal dependencies of specific structure are imposed between the player's actions and the reward distributions~\cite{KI18, CCB19,BSSS19, PBG19,BPCS20}. In \cite{KI18}, Kleinberg and Immorlica consider the setting of {\em recharging bandits}, where the expected reward of each arm is a concave and weakly increasing function of the time passed since its last play, modeling in that way scenarios of local performance loss. 
%In the same spirit, Cella and Cesa-Bianchi \cite{CCB19} consider a related setting, assuming a parametric form for the reward functions which are non-decreasing and bounded by $1$. 
In a similar spirit, Basu et al.\ \cite{BSSS19} consider the problem of {\em blocking bandits}, in which case once an arm is played at some round, it cannot be played again (i.e., it becomes blocked) for a fixed number of consecutive rounds. Notice that all the aforementioned examples are variations of the stochastic MAB setting, where the decision maker plays (at most) one arm per time step.

When combinatorial constraints and time dynamics come together, the result is a much richer and more challenging setting, precisely because their interplay creates a complex dynamical structure. Indeed, in the standard combinatorial bandits setting, the optimal solution in hindsight is to consistently play the feasible subset of arms of maximum expected reward. However, in the presence of local temporal constraints on the arms, an optimal (or even suboptimal) solution cannot be trivially characterized-- a fact that significantly complicates the analysis, both from the algorithmic as well as from the learning perspective. In this work, we study %action-reward temporal dependencies in 
the following bandit setting-- a common generalization of {\em matroid bandits}, introduced by Kveton et al.\
\cite{KWAEE14}, and blocking bandits~\cite{BSSS19}:

\begin{problem}
[{Matroid Blocking Bandits (MBB)}] 
We consider a set $\A$ of $k$ {\em arms}, a {\em matroid} $\M=(\A,\I)$, and an unknown time horizon of $T$ rounds. Each arm $i \in \A$ is associated with an unknown bounded {\em reward} distribution of mean $\mu_i$, and with a known deterministic {\em delay} $d_i$, such that whenever an action $i$ is played at some round, it cannot be played again for the next $d_i-1$ rounds. At each time step, the {\em player} pulls a subset of the available (i.e., not blocked) arms restricted to be an independent set of $\M$. Subsequently, she observes the reward realization of each arm played ({\em semi-bandit} feedback) and collects their sum. The goal of the player is to maximize her expected cumulative reward over $T$ rounds.
\end{problem}
 
The above model captures a number of applications, varying from team formation to ad placement, when arms represent actions that cannot be played repeatedly without restriction. As a concrete example, consider a recommendation system that repeatedly suggests a variety of products (e.g., songs, movies, books) to a user. The need for diversity on the collection of suggested products (arms), to capture different aspects of user's preferences, can be modeled as a linear matroid. Further, the blocking constraints preclude the incessant recommendation of the same product (which can be detrimental, as the product might be perceived as a ``spam''), while the maximum rate of recommendation (controlled by the delay) might depend on factors such as popularity, promotion and more. Finally, the expected reward of each product is the probability of purchasing (or clicking).

From a technical viewpoint, the \mbb problem is already NP-hard for the simple case of a uniform rank-1 matroid (see Theorem 2.1 in \cite{SST09}), even in the {\em full-information} setting, where the reward distributions are known to the player a priori. The natural common generalization of the algorithms in \cite{BSSS19,KWAEE14}, computes and plays, at each time step, an independent set of maximum mean reward consisting of the available elements. While the above strategy is a $\left(1-\frac{1}{e}\right)$-approximation asymptotically (that is, for $T \to \infty$) for partition matroids, unfortunately, it only guarantees a $\frac{1}{2}$-approximation for general matroids and this guarantee is tight (see Appendix \ref{appendix:tightexample} for an example). A natural question that arises is whether a $\left(1-\frac{1}{e}\right)$-approximation is possible for any matroid. 

The main result of this paper shows that this is indeed possible. Along the way, we identify that the key insight (and also the weak point of the naive $\frac{1}{2}$-approximation) is the underlying {\em diminishing returns} property hidden in the matroid structure. In particular, we discover an interesting connection of \mbb to the following problem of interest in its own right:

\begin{problem}[Recurrent Submodular Welfare (RSW)]
We consider a monotone (non-decreasing) submodular function $f:2^{\A} \rightarrow \mathbb{R}_{\geq 0}$ over a universe $\A$ and a time horizon $T$. At each round $t \in [T]$ we choose a subset $\A_t \subseteq \A$ and collect a reward $f(\A_t)$. However, using an element $i \in \A$ at some round $t \in [T]$ makes it unavailable (i.e., blocked) for a fixed and known number of $d_i-1$ subsequent rounds, namely, during the interval $[t, t + d_i -1]$. The objective is to maximize $\sum_{t \in [T]} f(\A_t)$, subject to the blocking constraints, within a (potentially unknown) time horizon $T$.
\end{problem}

For the above model, which can be thought of as a variant of {\em Submodular Welfare Maximization}~\cite{Von08}, we provide an efficient randomized $\left(1 - \frac{1}{e}\right)$-approximation (asymptotically), accompanied by a matching hardness result. Note that the \rsm problem is a very natural model, capturing applications of submodular maximization in repeating scenarios, where the elements cannot be constantly used without restriction. As an example, consider the process of renting goods to a stream of customers with identical submodular utility functions modeling their satisfaction.

As we show, our approach for the \rsm problem immediately implies an algorithm of the same approximation guarantee for the full-information case of \mbb and, additionally, it has important implications for the {\em bandit} setting, where the reward distributions are initially unknown. The standard goal in this case is to provide a (sublinear in the time horizon) upper bound on the {\em regret}, namely, the difference between the expected reward of a bandit algorithm and a (near-)optimal algorithm, due to the initial lack of knowledge of the former\footnote{
In fact, we upper bound the $\left(1 - \frac{1}{e}\right)${-(approximate) regret}, defined as the difference between $\left(1 - \frac{1}{e}\right) \OPT(T)$ and the expected reward collected by a bandit algorithm. The notion of $\alpha$-regret is widely used in the combinatorial bandits literature~\cite{CWYW16,WC17} for combinatorial problems where an efficient algorithm does not exist, and, thus, any efficient algorithm would inevitably suffer linear regret in standard definition (where $\alpha = 1$).
}.



\subsection{Related Work}
The \mbb model belongs to the family of stochastic {\em non-stationary} bandits, given that the reward distributions of the arms can change over time. Significant members of this family are {\em restless bandits} \cite{Whittle88, GMS10}, where the reward distribution of each arm changes at each time step, and {\em rested bandits} \cite{Gittins79, TL12}, where the distribution changes only when the arm is played. For the setting of restless bandits and without further assumptions on the transition functions, it is PSPACE-hard to even approximate the optimal solution \cite{PT99}. Our model differs from the above cases as we consider a transition function of special form and the transitions can occur both during playing and not playing an arm. In addition, the \mbb model falls into the category of Markov Decision Processes (MDPs) with deterministic transitions and stochastic rewards, but requires an exponential (in the size of the arms) state space, which makes this approach inefficient in practice.

A recent line of research focuses on non-stationary models in the case where each reward distribution is a special function of the player's actions \cite{ CCB19, PBG19,BPCS20}. In \cite{BSSS19}, Basu et al. provide a greedy $\left(1-\frac{1}{e}\right)$-approximation for the full-information case of the blocking bandits problem (a special case of the \mbb model for a uniform rank-1 matroid). As we have already mentioned, generalizing their strategy to the \mbb problem fails to provide the same guarantee for general matroids. In the bandit setting, where the reward distributions are initially unknown, the authors have to overcome the burden of characterizing a (sub)optimal solution, where the rate of mean collected reward exhibits significant fluctuations over time. The key insight is to observe that every time the full-information algorithm plays an arm, its bandit variant, which relies on estimations of the mean rewards, has at least one chance of playing the same arm. However, this key coupling argument, that enables sublinear regret bounds, becomes significantly more involved in the presence of matroid constraints. 

In \cite{KI18}, Kleinberg and Immorlica study the case of recharging bandits. Their approach first computes the ``optimal'' playing frequency ${1/x_i}$ of each arm $i$ via a mathematical formulation. In order to play each arm with this frequency, they develop the technique of {\em interleaved rounding}, where they associate each arm $i$ with a sequence of real numbers $\{\frac{\alpha_i +k}{x_i}\}_{k \in \mathbb{N}}$, with $\alpha_i \sim U[0,1]$. Then, the arms are played sequentially in the same order they appear on the real line. This novel rounding technique exhibits reduced variance and, thus, an improved approximation guarantee comparing to other natural approaches such as independent randomized rounding.

A rich body of research on combinatorial bandits \cite{CTPL15, CWYW16,WWFLLL16,KWAS15,KWAS15b, WC17} focuses on bandit optimization problems over general combinatorial structures.
In \cite{KWAEE14}, Kveton et al.\ consider the problem of stochastic combinatorial bandits where the underlying feasible set is a matroid defined over the ground set of arms. At each round, the player pulls an independent subset of arms and collects their realized rewards, assuming {\em semi-bandit} feedback (as opposed to the pure exploration {\em full-feedback} variant studied in \cite{CGL16}). The authors develop a greedy algorithm based on the Upper Confidence Bound (UCB) method \cite{ACBF02}, while they exploit well-known exchange properties of matroids for achieving optimal regret bounds. Their approach relies on the fact that the optimal solution in hindsight is fixed throughout the time horizon-- a fact that is no longer true in the presence of blocking constraints. Additional lines of research that are related to, yet incompatible with, our problem are {\em bandits with knapsacks} \cite{BKS18,SA18} or {\em with budgets} \cite{CJS15, Sliv13}, and {\em sleeping bandits}~\cite{KNMS10}. 


The \mbb model is also related to the literature on {\em periodic scheduling} \cite{BNLT07, BNBNS02}. In \cite{SST09}, Sgall et al.\ consider the problem of periodically scheduling jobs on a set of machines. Each job is associated with a {\em processing time}, during which it occupies the machine it is executed on, a {\em vacation time}, namely, a minimum time required after its completion in order to be rescheduled, and a {\em reward}. It is not hard to see that the case of unit processing times is a special case of \mbb with a uniform matroid of rank equal to the number of machines, under the objective of maximizing the total reward.
Further, it is known~\cite{BSSS19} that the rank-1 case of \mbb generalizes the {\em Pinwheel Scheduling} problem~\cite{HMRTV89}: Given $k$ colors associated a set of integers $\{d_i\}_{i \in [k]}$, such that $\sum_{i \in [k]} \frac{1}{d_i} = 1$, decide whether there is a coloring of the natural numbers $\nu:\mathbb{N} \to [k]$ such that every color $i \in [k]$ appears at least once every $d_i$ numbers. As it is proved in~\cite{JL14}, the above problem does not admit a pseudopolynomial time algorithm unless SAT can be solved by a randomized algorithm in expected quasi-polynomial
time. 



Finally, the \rsm problem is closely related to the problem of {\em Submodular Welfare Maximization} (SWM) \cite{Von08,MSV08,KLMM05,FV10}: Given $k$ items and $m$ players, each associated with a monotone submodular utility function $u_i:2^{[k]}\rightarrow \mathbb{R}_{\geq 0}$, the goal is to partition the elements into $m$ sets $S_1, \dots, S_m$, one for each player, such that to maximize $\sum_{i \in [m]} u_i(S_i)$. Specifically, \rsm can be thought of as a version of the SWM problem, when the items are distributed to a (possibly infinite) stream of players with identical utilities, and each item can be reused after some fixed time period (note that this is different than the online setting in~\cite{KMZ15}). Interestingly, as noted in \cite{Von08}, the SWM problem with identical utilities is {\em approximation resistant} in the sense that allocating the items to the players uniformly at random achieves the optimal approximation guarantee of $\left(1 - \frac{1}{e}\right)$ for this setting.



\subsection{Our Contributions}

We first focus on the full-information variant of \mbb, where the mean rewards of the arms are known to the player a priori. We assume that the player has access to the matroid $\M$ via an independence oracle and knowledge of the arms' fixed delays, yet she is oblivious to the time horizon $T$. In this sense, she plays {\em online}. An interesting aspect of dynamics, as illustrated in \cite{KI18, BSSS19, BPCS20}, is that one needs to guarantee, via {\em scheduling}, that each arm is roughly played at a frequency close to its ``optimal'' rate. This is particularly important in the presence of ``hard'' blocking constraints, where no reward can be obtained by a blocked arm. 

In order to address the above scheduling problem, we propose a particular  ``decoupled'' {\em two-phase strategy}. We refer to each phase as Player A and Player B. Initially, Player A decides on a schedule that determines arm availability, namely, a subset of rounds where each arm is allowed to be played. Subsequently, Player B chooses a subset of available arms that maximizes the total expected reward, subject to the matroid constraints. In order to completely decouple the two phases, the availability schedule produced by Player A is never affected by which arms are eventually chosen by Player B (that is, it is impossible for Player B to violate the blocking constraints). 

In the case where Player B knows the expected rewards of the arms and due to the above decoupling property, his optimal strategy (given any availability schedule) can be easily characterized: Since the arms of each round are subject to matroid constraints, Player B achieves his goal by playing a maximum expected reward independent set among the available arms of each round, which can be computed efficiently using the greedy algorithm for matroids. Thus, the role of Player A becomes to choose an availability schedule that maximizes the total reward, knowing that Player B will behave exactly as described above. The key observation is that the solution computed by Player B at each round, corresponds to the {\em weighted rank function} of the matroid evaluated on the set of available arms of the round. More importantly, it can be proved that this function is monotone submodular and, hence, Player A's task is a special case of the \rsm problem. 

Focusing our attention on the \rsm problem, any ``good'' solution should guarantee that each element $i \in \A$ is selected a fraction of the time close to $\frac{1}{d_i}$ (the maximum possible), where $d_i$ is the delay. However, a naive randomized approach that selects (if available) each element $i$ with probability $\frac{1}{d_i}$ independently at each round, can be as bad as a $(1 - e^{-\frac{1}{2}}) \approx 0.393$-approximation. Instead, motivated by the rounding technique of Kleinberg and Immorlica \cite{KI18}, we develop a (time-)correlated sampling strategy, which we call {\em interleaved scheduling}. While our technique is based on the same principle of transforming (randomly interleaved) sequences of real numbers into a feasible schedule, our implementation is, to the best of our knowledge, novel. Indeed, as opposed to \cite{KI18}, we additionally face the issue of scheduling more than one arms per round and the fact that our ``hard'' blocking constraints are particularly sensitive to the variance of the produced schedule. Using our technique, we construct a polynomial-time randomized algorithm, named \is (\IS), that achieves the following guarantee for \rsm:

\begin{restatable}{theorem}{restateinterleavedSubmodular}\label{thm:interleavedSubmodular}
The expected reward collected by \is over $T$ rounds, $\Rew^{IS}(T)$, is at least
$
\left(1 - \frac{1}{e} \right) \OPT(T) - \mathcal{O}(d_{\max} f(\A))
$, where $\OPT(T)$ is the optimal reward of \rsm for $T$ rounds and $d_{\max} = \max_{i \in \A} d_i$ is the maximum delay of the instance.
\end{restatable}

The proof of the above guarantee relies on the construction of a {\em convex program} (CP), based on the {\em concave closure} of $f$ (see below), that yields an (approximate up to an additive term) upper bound on the optimal reward. Although our algorithm never computes an optimal solution to this convex program, it allows us to compare its expected collected reward with the optimal solution of CP, leveraging known results on the {\em correlation gap} of submodular functions. As we show via a reduction from the SWM problem with identical utilities, the $\left(1 - \frac{1}{e}\right)$ term in the above guarantee is asymptotically the best possible, unless $\text{P} = \text{NP}$; further, the additive term results from the fact that our algorithm is oblivious to the time horizon $T$. 


We now turn our attention to the {\em bandit setting} of \mbb, where the mean rewards are initially unknown. Our interleaved scheduling method exhibits an additional property: {\em It does not rely on the monotone submodular function itself}, a fact that is particularly important for the bandit setting. Indeed, in the full-information setting Player B computes a maximum expected reward independent set at each round, for any availability schedule provided by Player A. In the bandit setting, however, the reward distributions are not a priori known and, thus, must be learned. Nevertheless, we do not need to wait to learn these distributions to find a good availability schedule. This allows us to make a natural coupling between the strategy of Player B in the bandit and in the full-information case and, thus, to compare the expected reward collected ``pointwise'', assuming a fixed common availability schedule. We remark that the above coupling is very different than the one in \cite{BSSS19}, as ours is independent of the trajectory of the observed rewards.

The above analysis allows us to develop a bandit algorithm for \mbb based on the UCB method, called \ucb (\UCB). Specifically, given any availability schedule provided by Player A (independently of the rewards) and in increasing order of rounds, Player B greedily computes a maximal independent set consisting the available arms of each round, based on estimates (known as UCB indices) of the mean rewards. In order to analyze the regret, we use the independence of the availability schedule in combination with the {\em strong basis exchange} property of matroids. This allows us to decompose the overall regret of our algorithm into contributions from each individual arm. Once we have established this regret decomposition, we can bound the individual regret attributed to each arm using more standard UCB type arguments \cite{KWAEE14}, leading to the following guarantee: 

\begin{restatable}{theorem}{restateApproxRegret}\label{thm:approxregret}
The expected reward collected by \ucb in $T$ rounds, $\Rew^{\UCB}(T)$, for $k$ arms, a matroid of rank $r = \rk(\M)$ and maximum delay $d_{\max}$ is at least
\begin{align*}
    \left(1-\frac{1}{e}\right)\OPT(T) - \mathcal{O}\left(k \sqrt{T \ln(T)} + k^2 + d_{\max}r \right).
\end{align*}
\end{restatable}

In the above bound, the additive loss corresponds to the regret with respect to $\left(1 - \frac{1}{e}\right)\OPT(T)$. Interestingly, our regret bound is very close (even in constant factors) to the information-theoretically optimal bound provided in \cite{KWAEE14} for the non-blocking setting. In fact, except for the small additive $\mathcal{O}(d_{\max}r)$ term, the regret bound in~\cite{KWAEE14} is the same as ours, if we replace the number of arms $k$ with $\sqrt{k\cdot r}$. Intuitively, this is due to the fact that our algorithm must learn the complete order of mean rewards, as opposed to the non-blocking setting where learning the maximum expected reward independent set in hindsight is sufficient for eliminating the regret.


All the omitted proofs of our results have been moved to the Appendix. We refer the reader to Appendix \ref{appendix:notation} for technical notation.