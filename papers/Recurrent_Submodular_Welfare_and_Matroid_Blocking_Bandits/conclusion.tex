\section*{Conclusion and Further Directions}
We explore the effect of action-reward dependencies in the combinatorial MAB setting by introducing and studying the \mbb problem. After relating the problem to \rsm, we provide a $\left(1-\frac{1}{e}\right)$-approximation for its full-information case, based on the technique of interleaved scheduling. Importantly, our technique is oblivious to the reward distributions of the arms-- a fact that allows us to provide regret bounds of optimal dependence in $T$, when these distributions are initially unknown. 


Our work leaves behind numerous interesting questions. By exhaustive search over $\mathcal{O}(1)$-periodic schedules, one can construct a PTAS for the (asymptotic) \mbb problem, assuming {\em constant} $\rk(\M)$ and $\{d_i\}_{i \in [k]}$. It remains an open question, however, whether the $\left(1-\frac{1}{e}\right)$-approximation is the best possible in general. We remark that the hardness of \mbb cannot solely rely on an argument similar to Theorem \ref{thm:submodular:hardness}, since the welfare maximization problem for the class of {\em gross substitutes}, which includes weighted matroid rank functions, is easy \cite{Leme17}. Another interesting direction would be to study natural extensions of the \rsm problem, when additional constraints (knapsack, matroid etc.) are imposed on top of blocking, or when the submodularity assumption is relaxed (see, e.g., \cite{Feige06}).

%{\color{gray}
%Finally, we believe that the study of (combinatorial) bandits under natural temporal correlations between the reward distributions and the player's actions (other than blocking) remains a wide source of interesting algorithmic problems.
%}
