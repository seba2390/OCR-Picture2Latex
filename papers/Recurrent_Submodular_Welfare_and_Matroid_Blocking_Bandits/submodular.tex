\section{Recurrent Submodular Welfare}
Let $f(S): 2^{\A} \rightarrow \mathbb{R}_{\geq 0}$ be a monotone submodular function over a universe $\A$ of $k$ elements, such that $f(\emptyset) = 0$. In the {\em blocking} setting, each element $i \in \A$ is associated with a known deterministic {\em delay} $d_i \in \mathbb{N}_{>0}$, such that once the arm is played at some round $t$, it becomes unavailable for the next $d_i-1$ rounds, namely, in the interval $\{t, \dots, t+d_i-1\}$. At each round $t \in [T]$, the player chooses a subset $\A_t$ of available (i.e., non-blocked) elements and collects a reward $f(\A_t)$. The goal is to maximize the total reward collected, i.e., $\sum_{t \in [T]} f(\A_t)$, within an unknown time horizon $T$. 

Before we present our algorithm, we provide ``bad'' instances for two natural approaches to \rsm.

\begin{remark} \label{rem:greedy}
The greedy approach of choosing $\A_t$ to be the set of all available elements at round $t \in [T]$ can be as bad as a $\frac{1}{k}$-approximation. In order to see that, consider the monotone (budget-additive) submodular function $f(S) = \min\{|S|, 1\}$. Let $k$ be the number of elements with delay $d_i = k$ for each $i \in \A$. Assuming an infinite time horizon, the optimal strategy collects an average reward of $1$, simply by choosing one element at a time in a round-robin manner. However, the average reward of the greedy approach in this case is $\frac{1}{k}$.
\end{remark}

\begin{remark}
The independent randomized sampling approach of adding each arm $i$ to $\A_t$ independently with probability $\frac{1}{d_i}$, if available, can be as bad as a $(1 - \frac{1}{\sqrt{e}} )$-approximation. Consider the same setting as in Remark \ref{rem:greedy}, where for $T \to \infty$ the optimal average reward is $1$. However, the average expected reward of the independent randomized sampling strategy is $1 - (1 - p)^k$, where $p = \frac{1}{2k-1}$ is the probability that each element is selected at each round (in stationarity). For $k \to \infty$, we have that $1 - (1 - p)^k \to 1- e^{-\frac{1}{2}} \approx 0.393$.

\end{remark}
We provide an efficient randomized $\left(1-\frac{1}{e}\right)$-approximation algorithm for \rsm. Informally, the algorithm starts by considering, for each element $i \in \A$, a sequence of rational numbers of the form $\{t\cdot \frac{1}{d_i}\}_{t \in [T]}$. Then, these sequences are {\em interleaved} by randomly adding an {\em offset} $r_i$, drawn uniformly at random from $[0,1]$, for each $i \in \A$ to the corresponding sequence. At every round $t \in [T]$, the algorithm chooses a set $\A_t$, consisting only of elements for which the (perturbed) interval $L_{i,t} = [t\cdot \frac{1}{d_i}+ r_i, (t+1)\cdot \frac{1}{d_i}+ r_i )$ contains an integer.

\begin{algorithm}[\is (\IS)]
For each element $i \in \A$, let $r_i \sim U[0,1]$ be a random {\em offset} drawn uniformly from $[0,1]$. 
At every round $t = 1, 2, \dots$,  let $\A_t \subseteq \A$ be the subset of elements such that for any $i \in \A_t$, the interval $L_{i,t} = [t\cdot \frac{1}{d_i} + r_i, (t+1) \cdot \frac{1}{d_i} + r_i)$ contains an integer. Choose the elements $\A_t$ and collect the reward $f(\A_t)$.
\end{algorithm}


\subsection{Correctness and approximation guarantee.} 
We first show the algorithm is correct, namely, that the elements chosen at each round respect the blocking constraints. The correctness is established by the following simple observation:

\begin{restatable}{fact}{restatefactalwaysavailable}\label{inter:fact:alwaysavailable}
At any $t \in [T]$, all the elements in $\A_t$ are available (i.e., not blocked).
\end{restatable}

In order to prove the competitive guarantee of our algorithm, we first construct a convex programming (CP)-based (approximate) upper bound on the optimal reward. Although our algorithm never computes an optimal solution to this CP, this step allows us to prove our guarantee, leveraging results on the correlation gap of submodular functions. For $\bm{d}^{-1} \in \mathbb{R}^k$ such that $(\bm{d}^{-1})_i = \frac{1}{d_i}, \forall i \in [k]$, consider the following formulation based on the concave closure $f^+$ of $f$:
\begin{align}
\maximize_{\z \in \mathbb{R}^k}~~ T \cdot f^+(\z)~~\textbf{s.t.}~~ \bm{0} \preceq \z \preceq \bm{d}^{-1}. \tag{\textbf{CP}} \label{cp:CP}
\end{align}

In \eqref{cp:CP}, each variable $z_{i}$ can be thought of as the fraction of rounds where element $i\in \A$ is chosen. Intuitively, the constraints indicate the fact that, due to the blocking, each element $i \in \A$ can be played at most once every $d_i$ steps. 
In order to derive \eqref{cp:CP}, we start from a non-convex integer program (IP) with 0-1 variables $\{x_{i,t}\}_{i \in \A, t \in [T]}$, each indicating whether element $i \in \A$ is used at round $t \in [T]$. The objective is to maximize $\sum_{t \in [T]} \sum_{S \subseteq \A} f(S) \prod_{i \in S} x_{i,t} \prod_{i \notin S}(1 - x_{i,t})$ subject to natural blocking constraints. For integral solutions, the above objective is equivalent to $\sum_{t \in [T]} f^+(\x_t)$ (where $(\x_t)_i = x_{i,t}$) and, thus, the above relaxation is simply the result of averaging over time the variables and constraints of this IP. By using the concavity of $f^+$, we are able to show that \eqref{cp:CP} yields an (approximate) upper bound on the optimal solution of \rsm, while the approximation becomes exact as $T$ increases.

\begin{restatable}{lemma}{restateStructuralCP}\label{lem:structural:CP}
Let $\Rew^{CP}(T)$ be the optimal solution to \eqref{cp:CP} and $\OPT(T)$ be the optimal solution over $T$ rounds. We have
$
\Rew^{CP}(T) \geq \OPT(T) - \mathcal{O}(d_{\max} f(\A)),
$ where $d_{\max} = \max_{i \in \A}\{d_i\}$.
\end{restatable}

\begin{remark}
By replacing $f^+(\z)$ in \eqref{cp:CP} with the multi-linear extension $F(\z)$, the formulation no longer yields an upper bound on the optimal reward (not even asymptotically). Indeed, consider a function $f$ over a ground set $\A=\{1,2\}$ with $d_1 = d_2 = 2$, such that $f(\emptyset) = 0$, $f(\{1\}) = f(\{2\}) = 2$ and $f(\{1,2\}) = 3$. For $T \to \infty$, the optimal average reward is $2$, simply by choosing the two elements interchangeably. However, the formulation based on $F(\z)$ in that case would be to maximize $2z_1(1-z_2) + 2z_2(1-z_1) + 3 z_1 z_2$ subject to $z_1,z_2 \leq \frac{1}{2}$, which has a global maximum of $\frac{7}{4} < 2$.
\end{remark}


Before we complete the proof of our first main result, we first compute the probability that $i \in \A_t$, i.e., an element $i \in \A$ is sampled at round $t \in [T]$:

\begin{restatable}{fact}{restatefactsampling}\label{inter:fact:sampling}
For any $i \in \A$ and $t \in [T]$, we have
$\Pro{i \in \A_t} = \Pro{L_{i,t} \cap \mathbb{N} \neq \emptyset } = \frac{1}{d_i}.
$
\end{restatable}

\noindent{\em Proof of Theorem \ref{thm:interleavedSubmodular}.} 
Let us denote by $S \sim {\bf p}$ with ${\bf p} \in [0,1]^k$ the random set $S \subseteq \A$, where each element $i$ participates in $S$ independently with probability equal to $p_i$. 
By Fact~\ref{inter:fact:sampling} and due to the randomness of the offsets $\{r_i\}_{i \in \A}$, we have that $\A_t \sim {\bf d}^{-1}$ for each $t \in [T]$. Let $\z^*$ be an optimal solution to \eqref{cp:CP}. By monotonicity of $f$ and the fact that $\z^* \preceq \bm{d}^{-1}$, for the expected value of $f(\A_t)$ at any round $t \in [T]$, we know that $\Ex{\A_t \sim \bm{d}^{-1}}{f(\A_t)} \geq \Ex{\A_t \sim \z^*}{f(\A_t)}$. Moreover, by definition of the multi-linear extension, we have that $\Ex{\A_t \sim \z^*}{f(\A_t)} = F(\z^*)$, while by Lemma~\ref{lem:correlationgap} (the correlation gap of submodular functions), we have that, $F(\z) \geq \left(1 - \frac{1}{e}\right)f^+(\z)$ for any vector $\z \in [0,1]^k$. By combining the above facts, we can see that
\begin{align*}
\Rew^{IS}(T) = \sum_{t \in [T]} \Ex{\A_t \sim \bm{d}^{-1}}{f(\A_t)} \geq 
\sum_{t \in [T]} F(\z^*) \geq \left(1 - \frac{1}{e}\right)T\cdot f^+(\z^*) = \left(1 - \frac{1}{e}\right) \Rew^{CP}(T).
\end{align*}
Therefore, by Lemma~\ref{lem:structural:CP}, we can conclude that $\Rew^{IS}(T) \geq \left(1 - \frac{1}{e}\right)\OPT(T) - \mathcal{O}(d_{\max} f(\A))$.
\qed
\newline

In Appendix \ref{appendix:hardness}, we provide a $\left(1-\frac{1}{e}\right)$-hardness result for \rsm, thus proving that the guarantee of Theorem~\ref{thm:interleavedSubmodular} is asymptotically tight. This result, which holds even for the special case where $d_{\max} = o(T)$ (that is when the delays are significantly smaller than the time horizon), is proved via a reduction from the SWM problem with identical utilities, in a way that the constructed \rsm instance accepts w.l.o.g. solutions of a simple periodic structure.

\begin{restatable}{theorem}{restateSubmodularHardness}\label{thm:submodular:hardness}
For any $\epsilon>0$, there exists no polynomial-time $\left(1-\frac{1}{e} + \epsilon \right)$-approximation algorithm for the \rsm problem, unless ${\bf P}={\bf NP}$, even in the special case where $d_{\max} = o(T)$.
\end{restatable}