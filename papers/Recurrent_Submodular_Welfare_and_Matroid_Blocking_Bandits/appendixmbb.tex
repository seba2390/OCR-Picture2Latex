\section{Matroid Blocking Bandits: Omitted Proofs}

\subsection{The full-information setting}
\label{appendix:mbb:fullinformation}
We now provide an alternative analysis for \ig (\IG), which, as opposed to Theorem~\ref{thm:interleavedSubmodular} for the \rsm problem, does not rely on the concave closure of submodular functions. We first note that the correctness of \IG follows directly by Fact~\ref{inter:fact:alwaysavailable}, that is, the set of sampled arms $\G_t$ at each round $t \in [T]$ only contains available (i.e., non-blocked) arms, in combination with the fact that the algorithm always plays an independent set $\A_t \in \I_{\M|\G_t} \subseteq \I$. 

For any set $S \subseteq \A$, let $\z(S) = \sum_{i \in S} z_i$. Consider the following LP, based on the matroid polytope associated with $\M$:
\begin{align}
\textbf{maximize: }& T \cdot \sum_{i \in \A} \mu_{i} z_{i} \tag{\textbf{LP}} \label{lp:LP}\\
\textbf{s.t. }& \z(S) \leq \rk(S), \forall S\subseteq \A \label{flp:rank}\\
\qquad & {\bf 0} \preceq \z \preceq \bm{d}^{-1}. \label{flp:window}
\end{align}

In \eqref{lp:LP}, each variable $z_{i}$ can be thought of as the fraction of rounds where arm $i\in \A$ is played. Intuitively, constraints \eqref{flp:window} of \eqref{lp:LP} indicate the fact that, due to blocking constraints, the fraction of time we can play an arm $i \in \A$ is upper bounded by $\frac{1}{d_i}$, while constraints \eqref{flp:rank} impose the rank restrictions in order to guarantee that the set of arms played at any round $t$ corresponds to an independent set of the matroid $\M$. 

As we show in the following lemma, the formulation \eqref{lp:LP} yields an approximate upper bound on $\OPT(T)$, while the approximation becomes exact as $T$ increases.

\begin{restatable}{lemma}{restateStructuralflp}\label{lem:structural:flp}
Let $\Rew^{LP}(T)$ be the optimal solution to \eqref{lp:LP} and $\OPT(T)$ be the optimal expected reward over $T$ rounds. We have
$
\Rew^{LP}(T) \geq \OPT(T) - \mathcal{O}\left(d_{\max} \rk(\M)\right).
$
\end{restatable}
\begin{proof}
In order to prove the Lemma, we first construct an IP upper bound on the optimal expected reward over $T$ rounds, $\OPT(T)$. Then, we construct \eqref{lp:LP} by averaging over time the 0-1 variables of the IP. For any set $S \subseteq \A$, let $\x_t(S) = \sum_{i \in S} x_{i,t}$. 
\begin{align}
\textbf{maximize:}& \sum_{i \in [T]} \sum_{i \in \A} \mu_{i} x_{i,t} \tag{\textbf{IP}} \label{lp:IP}\\
\textbf{s.t.}& \sum_{t' \in [t,t+d_i-1]} x_{i,t'} \leq 1, \forall i \in \A, \forall t \in [T] \label{lp:window} \\
\qquad & \x_t(S) \leq \rk(S), \forall S\subseteq \A, \forall t \in [T] \label{lp:rank} \\
\qquad & \x_{t} \in \{0,1\}^k, \forall t \in [T]. \notag
\end{align}

In \eqref{lp:IP}, each variable $x_{i,t}$ can be thought of as the 0-1 indicator of playing arm $i\in \A$ at time $t \in [T]$. Intuitively, constraints \eqref{lp:window} of \eqref{lp:IP} indicate the fact that, due to the blocking constraints, each arm $i \in \A$ can be played at most once every $d_i$ steps, while constraints \eqref{lp:rank} impose the rank restrictions due to the matroid $\M$ at any round $t$. Let $\Rew^{IP}(T)$ be the optimal solution to \eqref{lp:IP}.

Fix any (optimal) algorithm and let $\A^{*}_t$ be the set of arms played by the algorithm at round $t$. Notice that the sets $\A^{*}_t$ are deterministic, given that the choices of any full-information algorithm that maximizes the expected cumulative reward are independent of the observed reward realizations. By linearity of expectation, the expected reward collected (over the randomness of the reward realizations) by the optimal algorithm can be expressed as 
\begin{align*}
    \Ex{}{\sum_{t \in [T]} \sum_{i \in \A^{*}_t} X_{i,t}} = \sum_{t \in [T]} \sum_{i \in \A^{*}_t} \Ex{}{X_{i,t}} = \sum_{t \in [T]} \sum_{i \in \A^{*}_t} \mu_i.
\end{align*}
Consider a feasible solution of \eqref{lp:IP} such that for each $i \in \A$ and $t \in [T]$, we set $x_{i,t} = 1$, if $i \in \A^{*}_t$, and $x_{i,t}=0$, otherwise. It is not hard to verify that the objective of \eqref{lp:IP} for this assignment coincides with the expected reward collected by the above optimal algorithm. Moreover, constraints \eqref{lp:window} are satisfied, since for any arm $i \in [T]$ and any window of $d_i$ consecutive time steps, the algorithm can play the arm at most once. Finally, constraints \eqref{lp:rank} are satisfied, since for any time $t$, the set of arms played, $\A^{*}_t$, is an independent set of the matroid $\M$, thus satisfying all the rank constraints. Therefore, by exhibiting a feasible solution of \eqref{lp:IP} that has the same objective value as the expected reward of any optimal algorithm, we conclude that $\Rew^{IP}(T) \geq \OPT(T)$.


Consider any optimal solution $\x^*$ of \eqref{lp:IP} for a time horizon $T$. By constraints \eqref{lp:window}, for any $t \in [T]$ and $i \in \A$, we have $\sum_{t' \in [t,t+d_i-1]} x^*_{i,t'} \leq 1$. By working along the lines of the proof of Lemma~\ref{lem:structural:CP} and averaging constraints \eqref{lp:window} over all $t \in [T]$, we get
\begin{align}
    \frac{1}{T}\sum_{t \in [T]} x^*_{i,t} \leq \frac{1}{d_i} \left(1 + \frac{d_i-1}{T}\right), \forall i \in \A \label{eq:flp:window}. 
\end{align}
Similarly, for any set $S \subseteq \A$, by averaging the inequalities of \eqref{lp:rank} over all rounds $t \in [T]$, we get
\begin{align}
    \frac{1}{T}\sum_{t \in [T]} \x^*_t(S) \leq \rk(S), \forall S \subseteq \A. \label{eq:flp:rank}
\end{align}
Now, consider an assignment of \eqref{lp:LP} such that
\begin{align*}
z_i = \left(1 + \frac{d_{\max}-1}{T}\right)^{-1} \frac{1}{T} \sum_{t \in [T]} x^*_{i,t},~~\forall i \in \A.    
\end{align*}

It is not hard to see that by inequality \eqref{eq:flp:window}, we have $z_{i} \leq \frac{1}{d_i}$ for any $i \in \A$. Moreover, given that $\left(1 + \frac{d_{\max}-1}{T}\right)^{-1} \leq 1$, for any set $S \subseteq \A$, we have that $\sum_{i \in S} z_i \leq \frac{1}{T}\sum_{t \in [T]} \x^*_t(S) \leq \rk(S)$. Therefore, the assignment $\z \in \mathbb{R}_{+}$ with $(\z)_i = z_i$ satisfies constraints \eqref{flp:rank} and \eqref{flp:window} of \eqref{lp:LP}. Considering the objective value of \eqref{lp:LP} for the assignment $\z$, we have that
\begin{align*}
    T \sum_{i \in \A} \mu_i z_i &= T  \sum_{i \in \A} \mu_i \left(1 + \frac{d_{\max}-1}{T}\right)^{-1} \frac{1}{T} \sum_{t \in [T]} x^*_{i,t} \\
    &\geq \left(1 + \frac{d_{\max}-1}{T}\right)^{-1} \sum_{t \in [T]} \sum_{i \in \A} \mu_i x^*_{i,t}\\
    &\geq \left(1 - \frac{d_{\max}-1}{d_{\max}-1 + T}\right) \sum_{t \in [T]} \sum_{i \in \A} \mu_i x^*_{i,t},
\end{align*}
where the last inequality follows by the fact that $\frac{1}{1+\beta} = 1 - \frac{\beta}{1+ \beta}$ for any $\beta \in \mathbb{R}$.
By exhibiting a feasible solution of \eqref{lp:LP} of value greater than $\left(1 - \frac{d_{\max}-1}{d_{\max}-1 + T}\right) \Rew^{IP}(T)$, the lemma follows by the fact that $\Rew^{IP}(T) \geq \OPT(T)$ and that $\OPT(T) \leq T \cdot \rk(\M)$, since the rewards of all arms lie in $[0,1]$.
\end{proof}

We are now ready to complete the proof of the following result.

\restateinterleavedGreedy*

\begin{proof}
Before we proceed with the proof, we first emphasize that the algorithm \IG is not aware of the reward realizations of each round before it plays a subset of arms. Therefore, since the objective it to maximize the cumulative expected reward, we can assume that the reward of each arm $i \in \A$ is deterministic and equal to $\mu_i$.

Let $\z^*$ be an optimal solution to \eqref{lp:LP}. Given the fact that the feasible set of \eqref{lp:LP} is essentially the intersection of the matroid polytope $\mathcal{P}(\M)$ and the (downward-closed) blocking constraints $\z^* \leq \bm{d}^{-1}$, it holds that $\z^* \in \mathcal{P}(\M)$. Therefore, the point $\z^*$ can be expressed as a convex combination of characteristic vectors of $k$ independent sets of $\M$, denoted by $T_1, \dots, T_k $, where $T_j \in \I, \forall j \in [k]$. By Lemma \ref{lem:characteristic}, this in turn induces a probability distribution, $\I(\z^*)$, over $T_1, \dots, T_k$, such that the marginal probability of each element $i \in \A$ being in the sampled set is exactly $z^*_i$.

Conditioned on the random offsets $\{r_i\}_{i \in \A}$, the sequence of sampled sets $\{\G_t\}_{t \in [T]}$ is deterministic and independent of the observed rewards. Let $f_{\M,\mu}(\G_t)$ be the weighted rank function over the subset $\G_t$, that is, the expected reward of a maximum independent set of $\M$ contained in $\G_t$. By denoting as $\G_t \sim {\bf p}$ the random set of elements, where each element $i \in \A$ participates with probability equal to $({\bf p})_i = p_i$, we have that $\G_t \sim {\bf d}^{-1}$ for each $t \in [T]$. The expected reward of \IG can be expressed as
\begin{align*}
    \Rew^{IG}(T) = \sum_{t \in [T]} \E{\mu(\A_t)} = \sum_{t \in [T]} \Ex{\G_t \sim {\bf d}^{-1}}{f_{\M,\mu}(\G_t)} \geq  \sum_{t \in [T]} \Ex{\G_t \sim \z^*}{f_{\M,\mu}(\G_t)},
\end{align*}
where the last inequality follows by Lemma \ref{lem:weightedrank}, namely, the fact that the weighted rank function $f_{\M,\mu}(\G_t)$ is a monotone (non-decreasing) and by the fact that $\z^* \preceq \bm{d}^{-1}$.

Let $F_{\M,\mu}(\z)$ and $f_{\M,\mu}^+(\z)$ be the multi-linear extension and the concave closure of function $f_{\M,\mu}$, respectively. By the correlation gap inequality for submodular functions (see Lemma \ref{lem:correlationgap}), for each vector $\z$, we have that $F_{\M,\mu}(\z) \geq \left(1 - \frac{1}{e}\right)f_{\M,\mu}^+(\z)$. Moreover, by definition of the concave closure, it has to be that $f_{\M,\mu}^+(\z) \geq \Ex{I \sim \I(\z)}{f_{\M, \mu}(I)}$, since $f_{\M,\mu}^+(\z)$ is the maximum valued distribution over independent sets, such that the marginal contribution of each element $i \in \A$ is equal to $z_i$, i.e., $\Prob{I \sim \I(\z)}{i \in I} = z_i$. By combining the above facts, we have that
\begin{align*}
    \sum_{t \in [T]} \Ex{\G_t \sim \z^*}{f_{\M,\mu}(\G_t)} = T \cdot F_{\M,\mu}(\z^*) \geq \left(1 - \frac{1}{e}\right) T \cdot f^+_{\M,\mu}(\z^*) \geq \left(1 - \frac{1}{e}\right) T \cdot \Ex{I \sim \I(\z^*)}{f_{\M, \mu}(I)}.
\end{align*}

Using the fact that the greedy algorithm collects every element in $I$ for any independent set $I \in \I$, we have that $\Ex{I \sim \I(\z^*)}{f_{\M, \mu}(I)} = \Ex{I \sim \I(\z^*)}{\mu(I)}$. Finally, since the marginal probability of each element $i \in \A$ being in $I \sim \I(\z^*)$ equals $z^*_i$, we have
\begin{align*}
T \cdot \Ex{I \sim \I(\z^*)}{f_{\M, \mu}(I)} = T \cdot \Ex{I \sim \I(\z^*)}{\mu(I)} = T \cdot \sum_{i \in \A} \mu_i z^*_i = \Rew^{LP}(T).
\end{align*}
By combining the above relations with Lemma \ref{lem:structural:flp}, we get that
\begin{align*}
    \Rew^{IG}(T) \geq \left(1 - \frac{1}{e}\right) \Rew^{LP}(T) \geq \left(1 - \frac{1}{e}\right) \OPT(T) - \mathcal{O}(d_{\max} \rk(\M)),
\end{align*}
thus, the proof is completed.
\end{proof}





\subsection{The bandit setting and regret analysis}

\restateRegretCoupling*
\begin{proof}
Let $\{\G_t(\off)\}_{t \in [T]}$ be the sequence of sampled arms over $T$ rounds as a function of the sampled offsets $\off \in [0,1]^k$. Moreover, let $X_t(S)$ be the realized rewards of a subset $S \subseteq \A$ of arms at round $t \in [T]$. We denote by $\A^{\pi}_t$ the arms played at round $t\in[T]$ and by $H^{\pi}_t = \{\A^{\pi}_1, X_1(\A^{\pi}_1), \dots, \A^{\pi}_t, X_t(\A^{\pi}_t)\}$ the {\em history} of arm playing and observed realizations up to (and including) time $t$ by algorithm $\pi \in \{\IG, \UCB\}$. Recall that we denote by $\mathcal{R}$ the randomness due to the reward realizations of the arms.

Notice that in the case of \UCB and for fixed offsets, the player's actions only depend on the previous realized rewards of the arms. Thus, for any fixed offset vector $\off^{\UCB}$, we have
\begin{align*}
&\Ex{\mathcal{R}}{\sum_{i \in \A} X_{i,t} \event{i \in \arg\max_{S \subseteq \G_t(\off^{\UCB}), S \in \I}\{\bar{\mu}_t(S)\}} }\\
    &= \Ex{\mathcal{R}}{\sum_{i \in \A} \Ex{\mathcal{R}}{ X_{i,t} \event{i \in \arg\max_{S \subseteq \G_t(\off^{\UCB}), S \in \I}\{\bar{\mu}_t(S)\}}~|~H^{\UCB}_{t-1}}}\\
    &= \Ex{\mathcal{R}}{\sum_{i \in \A} \Ex{\mathcal{R}}{ X_{i,t}~|~H^{\UCB}_{t-1}} \event{i \in \arg\max_{S \subseteq \G_t(\off^{\UCB}), S \in \I}\{\bar{\mu}_t(S)\}}} \\
    &= \Ex{\mathcal{R}}{\sum_{i \in \A} \mu_i \event{i \in \arg\max_{S \subseteq \G_t(\off^{\UCB}), S \in \I}\{\bar{\mu}_t(S)\}}}\\
    &=\Ex{\mathcal{R}}{ \mu\left( \arg\max_{S \subseteq \G_t(\off^{\UCB}), S \in \I}\{\bar{\mu}_t(S)\}\right)}.
\end{align*}

Similarly, notice that the algorithm \IG is oblivious to the realized rewards. Therefore, for any fixed offset vector $\off^{\IG}$ and at any time $t \in [T]$, we get
\begin{align*}
    \Ex{\mathcal{R}}{\sum_{i \in \A} X_{i,t} \event{i \in \arg\max_{S \subseteq \G_t(\off^{\IG}), S \in \I}\{{\mu}(S)\}} }
    &= 
    \Ex{\mathcal{R}}{\sum_{i \in \A} \mu_i \event{i \in \arg\max_{S \subseteq \G_t(\off^{\IG}), S \in \I}\{\mu(S)\}}}\\
    &=\Ex{\mathcal{R}}{ \max_{S \subseteq \G_t(\off^{\IG}), S \in \I}\{\mu(S)\}}.
\end{align*}
The lemma follows by observing that the offsets $\off^{\IG}$ and $\off^{\UCB}$ of the two algorithms follow exactly the same distribution. Therefore, we have

\begin{align*}
&\Rew^{\IG}(T) - \Rew^{\UCB}(T) \\
= & \Ex{\off^{\IG} \sim [0,1]^k, \mathcal{R}}{\sum_{t \in [T]}\max_{S \subseteq \G_t(\off^{\IG}), S \in \I}\{\mu(S)\}} - \Ex{\off^{\UCB} \sim [0,1]^k, \mathcal{R}}{\sum_{t \in [T]}\mu\left( \arg\max_{S \subseteq \G_t(\off^{\UCB}), S \in \I}\{\bar{\mu}_t(S)\}\right)} \\
= & \Ex{\off \sim [0,1]^k, \mathcal{R}}{\sum_{t \in [T]}\left(\max_{S \subseteq \G_t(\off), S \in \I}\{\mu(S)\} - \mu\left( \arg\max_{S \subseteq \G_t(\off), S \in \I}\{\bar{\mu}_t(S)\}\right)\right)}.
\end{align*}
\end{proof}

\restateRegretSub*

\begin{proof}
Recall that under the event $\mathcal{E}_{\off}$, both algorithms \IG and \UCB use the same offset vector $\off$ and, thus, they operate on same sequence of sampled arms over time. Let $\G_t = \G_t(\off)$ be the common set of sampled arms and let $\A^{\IG}_t$ and $\A^{\UCB}_t$ be the maximal independent sets computed by \IG and \UCB, respectively, at any round $t \in [T]$. Notice that for any $t \in [T]$ both $\A^{\IG}_t$ and $\A^{\UCB}_t$ are bases of the restricted matroid $\M|\G_t$ and, thus, correspond to independent sets of $\I$ of equal cardinality. Let $\sigma_t$ be the bijection between $\A^{\IG}_t$ and $\A^{\UCB}_t$ described by Theorem~\ref{cor:basesexchange}. For any $t \in [T]$, we have that
$$
\Delta_{\G_t}(\bar\mu) =  \mu(\A^{\IG}_{t}) - \mu(\A^{\UCB}_{t}) = \sum_{i \in \A_t^{\IG}} \mu_i - \sum_{j \in \A_t^{\UCB}} \mu_j = \sum_{i \in \A^{\IG}_t} \left(\mu_i - \mu_{\sigma^{-1}_t(i)}\right) = \sum_{i \in \A^{\IG}_t} \Delta_{i , \sigma^{-1}_{t}(i)}.
$$
\end{proof}

\restateRegretTechnical*

\begin{proof}
We first focus on proving inequality \eqref{inq:reg:under}, that is, the part of the regret attributed to an arm $j >1$ when not enough samples have been collected. Notice that the algorithm $\UCB$ never accumulates regret when it plays the arm $j=1$ of highest mean reward. Recall that for any fixed $j \in \A$, we have $\Delta_{1,j} > \Delta_{2,j} > \dots > \Delta_{j,j} = 0$, since we assume w.l.o.g. that the arms have distinct mean rewards. By construction of our algorithm, if the number of samples from arm $j \in \A$ is increased at some round $t$, it is because there exists exactly one arm $i \in \A$ with $\Delta_{i,j} > 0$, such that $\sigma_t(j) = i$. The above is implied by Theorem~\ref{cor:basesexchange}, given the fact that each bijection $\sigma_t$ for all $t \in [T]$ maps each arm played by \UCB in $\A^{\UCB}_t$ to a single arm played by \IG in $\A_t^{\IG}$. On the other hand, as the number of obtained samples $T_j(t)$ from arm $j \in \A$ by time $t\in [T]$ increases, the maximum suboptimality gap $\Delta_{i,j}$ that can be charged in the under-sampled part of the regret is that of the maximum reward $i \in \A$ that satisfies $T_j(t) \leq \ell_{i,j}$. By the above analysis, for any $j>1$, we get that 
\begin{align}
\sum_{t \in [T]} \sum^{j-1}_{i = 1} \Delta_{i , j} \event{\sigma_t(j)=i, T_j(t) \leq \ell_{i,j}} \notag
&\leq \sum^{j-1}_{i=1} \left(\Delta_{i,j} -  \Delta_{i+1,j}\right)\ell_{i,j} \notag\\
&\leq \sum^{j-1}_{i=1} \left(\Delta_{i,j} -  \Delta_{i+1,j}\right)\frac{8 \ln(T)}{\Delta^2_{i,j}} \label{eq:regret:technical:1},
\end{align}
where the last inequality follows by definition of $\ell_{i,j}$.

The rest of the claim follows by simple algebra. Indeed,
\begin{align*}
    \eqref{eq:regret:technical:1}&\leq \left(\sum^{j-1}_{i=1}\frac{\Delta_{i,j} - \Delta_{i+1,j}}{\Delta^2_{i,j}}\right)8 \ln(T) \notag\\
    &\leq \left(\frac{1}{\Delta_{j-1,j}} + \sum^{j-2}_{i=1}\frac{\Delta_{i,j} - \Delta_{i+1,j}}{\Delta^2_{i,j}}\right)8 \ln(T) \notag\\
    &\leq \left(\frac{1}{\Delta_{j-1,j}} + \sum^{j-2}_{i=1}\frac{\Delta_{i,j} - \Delta_{i+1,j}}{\Delta_{i,j} \Delta_{i+1,j}}\right)8 \ln(T) \notag\\
    &= \left(\frac{1}{\Delta_{j-1,j}} + \sum^{j-2}_{i=1}\left(\frac{1}{\Delta_{i+1,j}} - \frac{1}{\Delta_{i,j}}\right)\right)8 \ln(T) \notag\\
    &= \left(\frac{2}{\Delta_{j-1,j}} - \frac{1}{\Delta_{1,j}}\right)8 \ln(T) \notag\\
    &\leq \frac{16}{\Delta_{j-1,j}} \ln(T) \notag.
\end{align*}




We now focus on proving inequality~\eqref{inq:reg:sufficiently}, that is, the regret accumulated after a sufficient number of samples has been collected from an arm $j > 1$. Notice, that given the event $\mathcal{E}_{\off}$, the expectation in the LHS of inequality~\eqref{inq:reg:sufficiently} is taken only over the randomness of the realized rewards that are observed by \UCB. 


For proving the upper bound, we fix any arm $j > 1$ and focus on each arm $i \in \A$ such that $i < j$ and, thus, $\Delta_{i,j}>0$. Let us fix any such arm $i \in \A$. For any $t \in [T]$, the event $\{\sigma_t(j) = i\}$ implies that $\{\mu_{i} > \mu_j, \bar{\mu}_{i,t} \leq \bar{\mu}_{j,t}\}$, namely, the order of the UCB-indices at time $t \in [T]$ of $i$ and $j$ is inconsistent with the order of their true mean rewards. In the opposite case, the algorithm \UCB would have chosen the set $\A^{\UCB}_t - j + i$, which, as suggested by Theorem~\ref{cor:basesexchange}, is an independent set of $\M$. Therefore, for any arm $i < j$, we have
\begin{align}
    \{\sigma_t(j)=i, T_j(t) > \ell_{i,j}\} \subseteq \{\bar{\mu}_{i,t} \leq \bar{\mu}_{j,t},\mu_i > \mu_j, T_j(t) > \ell_{i,j}\}. \label{eq:lem:ucb:0}
\end{align}
Note that the inclusion in the above expression is because the inconsistency in the order of UCB-indices does not necessarily imply that $\sigma_t(j)=i$ (i.e., that \UCB actually exchanges $j$ for $i$ at time $t \in [T]$).

By definition of the UCB-indices, the event $\bar{\mu}_{i,t} \leq \bar{\mu}_{j,t}$ at time $t \in [T]$ implies that 
\begin{align}
    \hat{\mu}_{i,T_{i}(t)} + \sqrt{\frac{2 \ln{(t)}}{T_{i}(t)}} \leq \hat{\mu}_{j,T_{j}(t)} + \sqrt{\frac{2 \ln{(t)}}{T_{j}(t)}}. \label{eq:lem:ucb:1}
\end{align}

We fix $s_i = T_i(t)$ and $s_j = T_j(t) > \ell_{i,j}$ to be the number of samples obtained from arm $i$ and $j$, respectively, by time $t \in [T]$. Notice that in order for \eqref{eq:lem:ucb:1} to hold, at least one of the following events must be true:
\begin{align*}
     \textbf{(i) }\bigg\{\hat{\mu}_{i,s_{i}} + \sqrt{\frac{2 \ln{(t)}}{s_i}} \leq \mu_i \bigg\}, \textbf{   (ii)  }\bigg\{\hat{\mu}_{j,s_{j}} \geq \mu_j + \sqrt{\frac{2 \ln{(t)}}{s_{j}}}\bigg\},\textbf{   (iii)  } \bigg\{\mu_i < \mu_j + 2 \sqrt{\frac{2 \ln{(t)}}{s_j}}\bigg\}.
\end{align*}
Indeed, it can be easily verified that the simultaneous negation of the above three events contradicts \eqref{eq:lem:ucb:1} for any fixed number of samples $s_i,s_j$. 

By our choice of $\ell_{i,j} = \bigg\lfloor \frac{8 \ln(T)}{\Delta^2_{i,j}}\bigg\rfloor$ and the fact that $s_j \geq \ell_{i,j} + 1 \geq \frac{8 \ln(T)}{\Delta^2_{i,j}}$, we can see that event $\textbf{(iii)}$ cannot be true, since in that case, we have
$$
 \mu_j + 2 \sqrt{\frac{2 \ln{(t)}}{s_j}}  \leq \mu_j + 2 \sqrt{\frac{2 \Delta^2_{i,j}\ln{(t)}}{8 \ln(T)}} \leq \mu_j + \Delta_{i,j} = \mu_i.
$$
Moreover, by Hoeffding's inequality, for the probabilities of the events $\textbf{(i)}$ and $\textbf{(ii)}$, we have that
$$
\Pro{\hat{\mu}_{i,s_{i}} + \sqrt{\frac{2 \ln{(t)}}{s_i}} \leq \mu_i} \leq e^{-4 \ln(t)} = t^{-4}\text{   and   }\Pro{\hat{\mu}_{j,s_{j}} \geq \mu_j + \sqrt{\frac{2 \ln{(t)}}{s_{j}}}} \leq e^{-4 \ln(t)} = t^{-4},
$$
where the probability is taken over the randomness of the reward realizations.

Therefore, for any numbers of samples $s_i = T_i(t)$ and $s_j = T_j(t) > \ell_{i,j}$, we have
\begin{align}
    \Pro{\bar{\mu}_{i,t} \leq \bar{\mu}_{j,t},\mu_i > \mu_j, T_j(t) = s_j, T_i(t)= s_i} 
    &\leq \Pro{\hat{\mu}_{i,s_{i}} + \sqrt{\frac{2 \ln{(t)}}{s_i}} \leq \mu_i} + \Pro{\hat{\mu}_{j,s_{j}} \geq \mu_j + \sqrt{\frac{2 \ln{(t)}}{s_{j}}}} \notag\\
    &\leq 2\cdot t^{-4}  \label{eq:ucb:union}.
\end{align}
Finally, by union bound over the possible number of samples, $s_i$ and $s_j$, and using the aforementioned results, for any $j > 1$ and time $t \in [T]$, we have
\begin{align}
    &\Ex{\mathcal{R}}{\sum_{t \in [T]} \sum^{j-1}_{i = 1} \Delta_{i , j} \event{\sigma_t(j)=i, T_j(t) > \ell_{i,j}}} \notag\\
    &= \Ex{\mathcal{R}}{\sum_{t \in [T]} \sum^{j-1}_{i = 1} \sum^{t-1}_{s_i = 0} \sum^{t-1}_{s_j = \ell_{i,j}+1} \Delta_{i , j} \event{\sigma_t(j)=i, T_j(t) = s_j, T_i(t)= s_i}}\label{lem:ucb:f2}\\
    &\leq \Ex{\mathcal{R}}{\sum_{t \in [T]} \sum^{j-1}_{i = 1} \sum^{t-1}_{s_i = 0} \sum^{t-1}_{s_j = \ell_{i,j}+1} \Delta_{i , j} \event{\bar{\mu}_{i,t} \leq \bar{\mu}_{j,t},\mu_i > \mu_j, T_j(t) = s_j, T_i(t)= s_i}}\label{lem:ucb:f3}\\
    &= \sum_{t \in [T]} \sum^{j-1}_{i = 1} \sum^{t-1}_{s_i = 0} \sum^{t-1}_{s_j = \ell_{i,j}+1} \Delta_{i , j} \Pro{\bar{\mu}_{i,t} \leq \bar{\mu}_{j,t},\mu_i > \mu_j, T_j(t) = s_j, T_i(t)= s_i}\notag\\
    &\leq  \sum_{t \in [T]} \sum^{j-1}_{i = 1} \Delta_{i , j} 2 t(t-1) t^{-4}  \label{lem:ucb:f4},
\end{align}
where in \eqref{lem:ucb:f2} we consider any possible number of samples by time $t$ for each arm. Moreover, inequality \eqref{lem:ucb:f3} follows by \eqref{eq:lem:ucb:0} and \eqref{lem:ucb:f4} follows by \eqref{eq:ucb:union}.
The proof of inequality \eqref{inq:reg:sufficiently} follows by the fact that 
$$\sum_{t \in [T]} t(t-1)t^{-4} \leq \sum_{t \in [T]}t^{-2} \leq \sum^{+\infty}_{t =1}t^{-2} = \frac{\pi^2}{6}.$$
\end{proof}


\subsection{Proof of Theorem \ref{thm:approxregret}}

\restateApproxRegret*

\begin{proof}
By inequality \eqref{eq:regret:twoalgos}, Lemma~\ref{lem:regret:coupling} and Definition \ref{def:gaps}, we can upper bound the $\alpha$-regret, for $\alpha = 1 - \frac{1}{e}$, as
\begin{align}
\alpha \OPT(T) - \Rew^{\UCB}(T) \leq \Ex{\off \sim [0,1]^k, \mathcal{R}}{\sum_{t \in [T]}\Delta_{\G_t(\off)}(\bar{\mu}_t)} + \mathcal{O}(d_{\max}\cdot \rk(\M)), \label{eq:reg:final:1}
\end{align}
where the expectation is taken over the randomness of the offset vector $\off$ and the reward realizations.

Under the event $\mathcal{E}_{\off}$, that is, where both \IG and \UCB use the same offsets $\off$, let $\{\sigma_t\}_{t \in [T]}$ be the sequence of bijections between $\A_t^{\UCB}$ and $\A_t^{\IG}$ over all rounds $t \in [T]$, as described in Theorem~\ref{cor:basesexchange}. Using Lemma~\ref{lem:regret:suboptdecomp}, we have that
\begin{align}
    \Ex{\off \sim [0,1]^k, \mathcal{R}}{\sum_{t \in [T]}\Delta_{\G_t(\off)}(\mu_t)} 
    &= \Ex{\off \sim [0,1]^k, \mathcal{R}}{\sum_{t \in [T]}\sum_{i \in \A^{\IG}_t} \Delta_{i , \sigma^{-1}_{t}(i)}}\notag \\
    &= \Ex{\off \sim [0,1]^k, \mathcal{R}}{\sum_{t \in [T]}\sum_{i \in \A^{\IG}_t}
    \sum_{j \in \A} \Delta_{i,j} \event{\sigma_t(j) = i}}\notag\\
    &\leq \Ex{\off \sim [0,1]^k, \mathcal{R}}{\sum_{t \in [T]}\sum_{j \in \A}\sum_{i < j}
    \Delta_{i,j} \event{\sigma_t(j) = i}}, \label{eq:reg:final:2}
\end{align}
where in the last inequality we restrict ourselves to arms $i < j$, where $\Delta_{i,j}>0$.

Now using the results of Lemma~\ref{lem:regret:technical}, we can further upper bound \eqref{eq:reg:final:2} as 

\begin{align}
&\Ex{\off \sim [0,1]^k, \mathcal{R}}{\sum_{t \in [T]}\sum_{j \in \A}\sum_{i < j} \Delta_{i,j} \event{\sigma_t(j) = i}} \notag \\
&= \Ex{\off \sim [0,1]^k, \mathcal{R}}{\sum_{t \in [T]}\sum_{j \in \A}\sum_{i < j} \Delta_{i,j} \event{\sigma_t(j) = i, T_j(t) \leq \ell_{i,j}}} \notag\\
&\qquad \qquad+ \Ex{\off \sim [0,1]^k}{ \Ex{\mathcal{R}}{\sum_{t \in [T]}\sum_{j \in \A}\sum_{i < j} \Delta_{i,j} \event{\sigma_t(j) = i, T_j(t) > \ell_{i,j}}}} \notag \\
&\leq \sum_{j > 1} \frac{16}{\Delta_{j-1,j}}\ln(T) + \frac{\pi^2}{3} \sum_{j > 1}\sum_{i = 1}^{j-1} \Delta_{i,j}. \label{eq:reg:final:3}
\end{align}

By combining inequalities \eqref{eq:reg:final:1}, \eqref{eq:reg:final:2} and \eqref{eq:reg:final:3}, we can upper bound the regret as a function of the gaps as follows:
\begin{align*}
    \alpha \OPT(T) - \Rew^{\UCB}(T) \leq \sum_{j > 1} \frac{16}{\Delta_{j-1,j}}\ln(T) + \frac{\pi^2}{3} \sum_{j > 1}\sum_{i = 1}^{j-1} \Delta_{i,j} + \mathcal{O}(d_{\max}\cdot \rk(\M))~~ \text{ (gap-dependent regret)}.
\end{align*}

In order to conclude the proof of the theorem, we would like to construct a regret bound that is independent of the gaps. The standard method is to partition the suboptimality gaps into ``small'' and ``large'' and, then, separately study their contribution to the regret. Specifically, for each $j \in \A$ and fixed $\epsilon > 0$, we define:
\begin{align*}
    S_j = \{i < j~|~\Delta_{i,j} \leq \epsilon\}\text{ and }L_j = \{i < j~|~\Delta_{i,j} > \epsilon\}.
\end{align*}
Starting again from \eqref{eq:reg:final:2} and noticing that the total regret due to small gaps can be at most $\epsilon\cdot T$ per arm, we have
\begin{align}
&\Ex{\off \sim [0,1]^k, \mathcal{R}}{\sum_{t \in [T]}\sum_{j \in \A}\sum_{i < j} \Delta_{i,j} \event{\sigma_t(j) = i}} \notag \\
&= \Ex{\off \sim [0,1]^k, \mathcal{R}}{\sum_{t \in [T]}\sum_{j \in \A}\sum_{i \in S_j} \Delta_{i,j} \event{\sigma_t(j) = i}} + \Ex{\off \sim [0,1]^k, \mathcal{R}}{\sum_{t \in [T]}\sum_{j \in \A}\sum_{i \in L_j} \Delta_{i,j} \event{\sigma_t(j) = i}} \notag\\
&\leq 
\epsilon k  T + \Ex{\off \sim [0,1]^k, \mathcal{R}}{\sum_{t \in [T]}\sum_{j \in \A}\sum_{i \in L_j} \Delta_{i,j} \event{\sigma_t(j) = i}}. \label{eq:reg:final:4}
\end{align}

We now focus only on the regret due to the large gaps, namely, the pairs $i,j$ such that $j \in \A$ and $i \in L_j$, which implies that $\Delta_{i,j} > \epsilon$. By exactly the same analysis as in the gap-dependent case, we can reach inequality \eqref{eq:reg:final:3}, in the restricted case where the summations only include pairs of arms such that $\Delta_{i,j} > \epsilon$ (notice that we can apply Lemma~\ref{lem:regret:technical} considering only the set $L_j$ of arms for each $j>1$). In addition, using the fact that $\Delta_{i,j} \leq 1$ for any $i,j \in \A$, we have 
\begin{align}
    \Ex{\off \sim [0,1]^k, \mathcal{R}}{\sum_{t \in [T]}\sum_{j \in \A}\sum_{i \in L_j} \Delta_{i,j} \event{\sigma_t(j) = i}} \leq 
    \sum_{j >1 } \frac{16}{\epsilon}\ln(T) + \frac{\pi^2}{6} k (k-1). \label{eq:reg:final:5}
\end{align}
By combining inequalities \eqref{eq:reg:final:4} and \eqref{eq:reg:final:5} with \eqref{eq:reg:final:1} and \eqref{eq:reg:final:2}, we have
\begin{align*}
    \alpha \OPT(T) - \Rew^{\UCB}(T) \leq \epsilon k T + \frac{16 k }{\epsilon}\ln(T) + \frac{\pi^2}{6} k(k-1) + \mathcal{O}(d_{\max}\cdot \rk(\M)).
\end{align*}
Finally, by setting $\epsilon = 4 \sqrt{\frac{\ln(T)}{T}}$, we get that 
\begin{align*}
   \alpha \OPT(T) - \Rew^{\UCB}(T) \leq 8 k \sqrt{T \ln(T)} + \frac{\pi^2}{6} k(k-1) + \mathcal{O}(d_{\max}\cdot \rk(\M))\quad \text{ (gap-independent regret)}.
\end{align*}
Therefore, we can conclude that the expected reward collected by \UCB in $T$ rounds is at least
\begin{align*}
    \left(1-\frac{1}{e}\right)\OPT(T) - \mathcal{O}\left(k \sqrt{T \ln(T)} + k^2 + d_{\max}\cdot \rk(\M) \right).
\end{align*}
\end{proof}