\section{Additional Results}

\subsection{Tight example for the naive greedy algorithm} \label{appendix:tightexample}

\begin{restatable}{lemma}{restateTightexample}\label{lem:tightexample}
For any $d \geq 2$, there exists an instance of the full-information variant of the \mbb problem (where the mean rewards are known a priori) such that the greedy strategy that plays a maximum mean reward independent set among the available arms collects a $\left(\frac{1}{2} + \frac{1}{2d}\right)$-fraction of the optimal expected reward.
\end{restatable}


\begin{proof}
We consider an infinite time horizon and a graphic matroid based on the graph $G_d = (V_d, E_d)$, which is recursively defined as follows: Let $G_1 = (V_1, E_1)$ with $V_1 = \{u,v\}$, $E_1 = \{\{u,v\}\}$ and assume that the arm associated with edge $\{u,v\}$ has delay $1$ and mean reward $1-\epsilon$, for some $\epsilon > 0$. For the graph $G_d = (V_d, E_d)$, we have $V_d = V_{d-1} \cup \{u_d\}$ and $E_d = E_{d-1} \cup \{\{u,u_d\}, \forall u \in V_{d-1}\}$ (namely, $G_d$ is essentially the result of the join operation between $G_{d-1}$ and a single vertex graph). The arms that are associated with the edges of $E_{d} \setminus E_{d-1}$ all have delay equal to $d$ and mean reward equal to $1- \frac{\epsilon}{d}$. The above recursive construction is illustrated in Figure \ref{fig:graphicmatroid}.

\begin{figure}[h]
\centering
\input{graphicmatroidLarge.tex}
\caption{Recursive definition of $G_d$.}
\label{fig:graphicmatroid}
\end{figure}

Consider now the arm-pulling schedule constructed by the greedy strategy. Let $T_p = E_p \setminus E_{p-1}$ be the new edges added at each step $p \in [d]$ in the recursive definition of $G_d$ (assuming that $E_0 = \emptyset$). Notice that for any integers $d \geq p_1 > p_2 \geq 1$ the edges of $T_{p_1}$ correspond to arms of higher mean reward than the edges of $T_{p_2}$. Therefore, the algorithm produces a periodic schedule of period $d$ as follows: Initially, the algorithm plays the $d$ arms of group $T_d$, collecting reward $d\left( 1 - \frac{\epsilon}{d}\right) = d - \epsilon$. Notice that, by construction, these edges form a spanning tree in $G_{d}$ and, thus, no additional arm can be played at the same time step. In the second time step of the period, the arms of $T_d$ are blocked and the algorithm plays the arms of $T_{d-1}$ collecting $d-1-\epsilon$ reward. Again, this is the maximum reward independent set of $G_d$ among the available arms. The algorithm proceeds similarly in the following steps and collects an average reward of 
$$
\frac{\sum^d_{p=1}(p-\epsilon)}{d} = \frac{d\cdot(d+1)/2-d\epsilon}{d} = \frac{d+1}{2} - \epsilon.
$$
In the above example, the optimal arm-pulling sequence is to play at each time $t \in [T]$, one arm of each group $T_p$ for $p \in [d]$. Notice that by construction of the delays and at each time step, there always exists at least one arm per group that is available. Moreover, by definition of the graph $G_d$, any such selection of arms never contains a circuit and, thus, it is an independent set of the graphic matroid. The expected reward collected by the optimal algorithm at each step is $d - \epsilon\sum_{p \in [d]}\frac{1}{p} = d - \epsilon H(d)$, were $H(d) = \sum_{p \in [d]}\frac{1}{p}$. 

In the above example, the ratio between the average reward collected by the greedy strategy and the optimal reward for $\epsilon \to 0$ becomes
$$
\lim_{\epsilon \to 0} \frac{\frac{d+1}{2} - \epsilon}{d - \epsilon H(d)} = \frac{1}{2} + \frac{1}{2d}.
$$
Therefore, by choosing large enough $d$, we can bring the approximation ratio of the above example arbitrarily close to $\frac{1}{2}$.
\end{proof}