\section{Recurrent Submodular Welfare: Omitted Proofs}
\subsection{Correctness and approximation guarantee}

\restatefactalwaysavailable*
\begin{proof}
Recall that at any round $t \in [T]$, the algorithm only chooses a subset $\A_t$ of the elements. Consider any element $i \in \A$ such that $i \in \A_t$ for some $t \in [T]$. By definition of $\A_t$, the interval $L_{i,t} = [t\cdot \frac{1}{d_i} + r_i, (t+1) \cdot \frac{1}{d_i} + r_i)$ contains an integer. It is not hard to see that, in that case, none of the intervals $L_{i,t'}$ for $t' \in [t-d_{i}+1, d_i - 1]$ can contain an integer. Therefore, the last time element $i$ has been chosen must be before $t- d_i$, which implies feasibility with respect to the blocking constraints.
\end{proof}



\restatefactsampling*

\begin{proof}
For any fixed $i \in \A$ and $t \in [T]$, because of the fact that $\frac{1}{d_i} \leq 1$ and $r_i \in [0,1]$, the interval $L_{i,t} = [t\cdot \frac{1}{d_i} + r_i, (t+1) \cdot \frac{1}{d_i} + r_i)$ clearly contains at most one integral point. The event that $\{[t\cdot \frac{1}{d_i} + r_i, (t+1) \cdot \frac{1}{d_i} + r_i)\cap \mathbb{N} \neq \emptyset \}$ is equivalent to the event that a continuous window of size equal to $\frac{1}{d_i}$ starting from the (real) point $t\cdot \frac{1}{d_i} + r_i$ contains an integer. For $r_i$ ranging in $[0,1]$, the starting point of the interval lies between $t\cdot \frac{1}{d_i}$ and $t\cdot \frac{1}{d_i} + 1$. It is not hard to see that fraction of possible realizations of $r_i$ such that the window contains an integer equals its size. The fact follows since for any $i \in \A$, the window has size $\frac{1}{d_i}$ and the offset $r_i$ is sampled uniformly at random from $[0,1]$.
\end{proof}



\restateStructuralCP*
\begin{proof}
In order to prove the lemma, we first construct an (non-convex) IP upper bound on the optimal expected reward over $T$ rounds, based on the multi-linear extension of $f$.

\begin{align}
\textbf{maximize:}& \sum_{i \in [T]} \sum_{S \subseteq \A} f(S) \prod_{i \in S} x_{i,t} \prod_{i \notin S} (1-x_{i,t}) \tag{\textbf{MP}} \label{mp:MP}\\
\textbf{s.t.}& \sum_{t' \in [t,t+d_i-1]} x_{i,t'} \leq 1, \forall i \in \A, \forall t \in [T] \label{mp:window} \\
\qquad &\x_{t} \in \{0,1\}^k, \forall t \in [T] \notag
\end{align}

In the formulation \eqref{mp:MP}, each variable $x_{i,t}$ can be thought of as the 0-1 indicator of playing arm $i\in \A$ at time $t \in [T]$. Intuitively, constraints \eqref{mp:window} of \eqref{mp:MP} indicate the fact that, due to blocking constraints, each arm $i \in \A$ can be played at most once every $d_i$ steps. Clearly, any optimal solution to \rsm can be mapped onto the above formulation and, thus, the optimal solution of \eqref{mp:MP} provides an upper bound on $\OPT(T)$.


Let $\x_t \in \{0,1\}^k$ for each $t \in [T]$ be a vector such that $(\x_t)_i = x_{i,t}$. Notice that for any integral $\x \in \{0,1\}^k$, the multi-linear extension is equal to the concave closure of any set function $f$, that is, $f^+(\x) = F(\x)$. Therefore, \eqref{mp:MP} remains an upper bound, even if we replace its objective function with $g(\x_1, \dots, \x_T) = \sum_{t \in [T]} f^+(\x_t)$.

We now fix any optimal solution $\{x^*_{i,t}\}_{i\in \A, t \in [T]}$ to \eqref{mp:MP} under the objective $g(\x_1, \dots, \x_T) = \sum_{t \in [T]} f^+(\x_t)$. Let us define the variables $\{z'_{i}\}_{i \in \A}$, such that
\begin{align*}
    z'_i = \frac{1}{T} \sum_{t \in [T]} x^*_{i,t} \geq 0, \quad \forall i \in \A.
\end{align*}
In the above definition, each $z'_i$ is the fraction of time an element $i \in \A$ is chosen in an optimal solution. Let $\z' \in [0,1]^k$, such that $(\z')_i = z'_i$ $\forall i \in \A$.

By concavity of $f^+$, we have
\begin{align*}
g(\x^*_1, \dots, \x^*_T) = \sum_{t \in [T]} f^+(\x^*_t) = T \sum_{t \in [T]} \frac{1}{T} f^+(\x^*_t) \leq T f^+(\frac{1}{T}\sum_{t \in [T]}\x^*_t) = T f^+(\z'),
\end{align*}
where the inequality follows by the fact that $\z'$ can be thought of as a convex combination of $\{\x^*_1, \dots, \x^*_T\}$.


Moreover, for each $i \in \A$ and by averaging constraints \eqref{mp:window} of \eqref{mp:MP} over all $t \in [T]$, we can see that 
\begin{align*}
\frac{1}{T}\sum_{t \in [1,d_i-1]} t x^*_{i,t} + \frac{1}{T} \sum_{t \in [d_i,T]} d_i x^*_{i,t} \leq 1 \Leftrightarrow \frac{1}{T} \sum_{t \in [T]} d_i x^*_{i,t} \leq 1 + \frac{1}{T}\sum_{t \in [1,d_i-1]} (d_i - t) x^*_{i,t}. 
\end{align*}
Given the fact that $\sum_{t \in [1, d_i-1]} x^*_{i,j} \leq 1$, the above inequality immediately implies that
\begin{align*}
    z'_i \leq \frac{1}{d_i}\left(1 + \frac{d_i-1}{T} \right)\quad \forall i \in \A.
\end{align*}
Consider now the assignment $z_i = \left(1 + \frac{d_{\max}-1}{T} \right)^{-1} z'_i$, $\forall i \in \A$. For this assignment, we can easily verify that the constraints of \eqref{cp:CP} are trivially satisfied, since $0 \leq z_i \leq \frac{1}{d_i}$, $\forall i \in \A$.

Let $\z \in [0,1]^k$, such that $(\z)_i = z_i$ $\forall i \in \A$. By the above analysis, we can see that
\begin{align*}
    \z = \z' - \frac{d_{\max}-1}{T + d_{\max} -1} \z',
\end{align*}
where we use the fact that $\frac{1}{1+\beta} = 1 - \frac{\beta}{1+ \beta}$ for any $\beta \in \mathbb{R}$.
Finally, by concavity of $f^+$ we have
\begin{align*}
    f^+(\z) &= 
    f^+\left(\left(1- \frac{d_{\max}-1}{T + d_{\max} -1} \right)\z' + \frac{d_{\max}-1}{T + d_{\max} -1} \bm{0} \right)\\
    &\geq \left(1- \frac{d_{\max}-1}{T + d_{\max} -1} \right)f^+(\z') + \frac{d_{\max}-1}{T + d_{\max} -1} f^+(\bm{0})\\
    &\geq f^+(\z') - \frac{d_{\max}-1}{T + d_{\max} -1} f(\A),
\end{align*}
where the last inequality follows by the facts that $f^+(\bm{0}) = f(\bm{0}) = 0$ and $f^+(\z') \leq f^+(\bm{1}) = f(\A)$, since $f$ is monotone.


Therefore, by exhibiting a feasible solution $\z$ of \eqref{cp:CP} such that 
$$
T f^+(\z) \geq T f^+(\z') - \mathcal{O}(d_{\max} f(\A)) \geq g(\x^*_1, \dots, \x^*_T) - \mathcal{O}(d_{\max} f(\A)) \geq \OPT(T) - \mathcal{O}(d_{\max} f(\A)),
$$
the proof is completed.
\end{proof}




\subsection{Hardness of approximation}
\label{appendix:hardness}
The goal of this section is to show that the $\left(1-\frac{1}{e}\right)$-multiplicative factor in the approximation guarantee of Theorem~\ref{thm:interleavedSubmodular} cannot be improved, unless $\textbf{P} = \textbf{NP}$. Specifically, we prove the following result:

\restateSubmodularHardness* 

In order show the above hardness result, we study for simplicity the average version of \rsm, where the objective is to maximize the average reward over $T$ time steps, namely, $\frac{1}{T}\left(\sum_{t \in [T]} f(\A_t)\right)$, where $\A_t$ is the set of elements used at time $t \in [T]$. Notice that in the average case, the additive term in the approximation guarantee of \ig, as presented in Theorem~\ref{thm:interleavedSubmodular}, vanishes as $T \to \infty$. Let $\OPT$ be the average reward collected by any optimal algorithm for \rsm. 

Our proof relies on a reduction from the Submodular Welfare (SW) problem~\cite{Von08}, in the special case where the players have identical utility functions. The problem can be formally defined as follows:

\begin{definition}[Submodular Welfare with Identical Utilities (SWIU)]
We consider a set of $k$ items and $m$ players, each associated with the same monotone submodular utility function $u:2^{[k]} \rightarrow \mathbb{R}_{\geq 0}$ over the items. The goal is to partition the $k$ items into $m$ subsets $S_1,\dots, S_m$, such that to maximize $\sum_{i \in [m]}u(S_i)$.
\end{definition}

As noted in~\cite{Von08}, the hardness result presented in~\cite{KLMM05} for the SW problem also holds for SWIU, namely, the special case of SW where all the players have the same utility function. Note, also that the \rsm problem is defined in the {\em value oracle} model, as we are only allowed to make queries of the function value for any input set.

\begin{theorem}[\cite{KLMM05}]\label{thm:hardness:swiu}
For any $\epsilon > 0$, there exists no polynomial-time $\left(1 - \frac{1}{e} + \epsilon\right)$-approximation algorithm for the SWIU problem in the value oracle model, unless ${\bf P} = {\bf NP}$.
\end{theorem}

We start from a simple construction for the non-average case of \rsm in order to show how our problem is directly associated with SWIU: Consider an instance of SWIU of $k$ items and $m$ players. Let $u:2^{[k]} \rightarrow \mathbb{R}_{\geq 0}$ be the monotone submodular utility function which is commonly used by all players. Given the above instance, we can construct in polynomial time an instance of \rsm as follows: Let $\A$ be the set of $k$ elements, each corresponding to an item, and let $f:2^{\A} \rightarrow \mathbb{R}_{\geq 0}$ be our function, chosen such that $f \equiv u$. We set the delay of each element $i \in \A$ as well as the time horizon to be equal to the number of players, namely, $ d_i = T  = m$ for each $i \in \A$.

Clearly, in the above construction where the delays are all equal to the time horizon, each element can be chosen at most once by any algorithm for \rsm. Therefore, the above constructed instance of \rsm exactly corresponds to SWIU, given that any solution to latter immediately translates into a solution of \rsm of the same total reward, and the opposite. 

The above construction immediately relates the two problems in the case where the delays can be of the same order as the time horizon. However, it does not rule out the possibility that the \rsm problem might become easier in the special case where $d_{\max} = o(T)$. Indeed, one could argue that for small enough delays, exploiting the possible periodicity of the \rsm solutions might lead to improved approximation guarantees. Notice, further, that the approximation guarantee we provide in Theorem~\ref{thm:interleavedSubmodular} for \IS becomes meaningless in the above scenario, since the additive loss for $d_{\max} = T$ becomes $\mathcal{O}(T\cdot f(\A))$.


In order to overcome the above technical issue and show that the multiplicative factor of $\left(1 - \frac{1}{e}\right)$ in Theorem~\ref{thm:interleavedSubmodular} cannot be improved, we map any instance of SWIU onto an instance of \rsm such that $T \gg d_{\max}$. Given any instance of SWIU, we can construct in polynomial time an instance of \rsm as follows: We define $\A$ to be the set of $k$ items, $f \equiv u$ to be the monotone submodular function and $d_i = m$ $\forall i \in \A$ to be the delay of all elements. In this case, we consider a time horizon $T = m \cdot \lceil\text{poly}(k,m) \rceil$, where by $\text{poly}(k,m)$ we denote some polynomial function in $k$ and $m$. 

We first show that, without loss of generality, we can focus our attention on solutions to the average case of \rsm that exhibit a periodic structure of period $m$.

\begin{lemma}\label{lem:hardness:periodic}
Let $\nu: [T] \rightarrow 2^{\A}$ be any feasible assignment to the above instance of \rsm of average reward $R$. We can construct in polynomial time a feasible assignment $\nu': [T] \rightarrow 2^{\A}$ of average reward at least $R' \geq R$, such that $\nu'(t) = \nu(t + m)$ $\forall t \in \mathbb{N}$, namely, $\nu'$ is a periodic assignment of period $m$.
\end{lemma}
\begin{proof}
Given that the average reward of the assignment $\nu$ is $R$, there must exist a continuous subsequence of rounds of length $m$, that is, $\{t, \dots, t + m -1\}$ for some $t \in [T-m]$, such that
\begin{align*}
    \frac{1}{m}\sum^{t+m-1}_{\tau = t} f(\nu(t)) \geq R.
\end{align*}
In the opposite case, we immediately get a contradiction to the fact that the average reward is at least $R$.

Let $L$ with $|L| = m$ be such a sequence. We now construct the periodic assignment $\nu'$ by repeating the assignment of the subinterval $L$, as follows:
\begin{align*}
\nu'(t) = \nu(L(t \mod m)) \in 2^{\A} ~\forall t \in [T].  
\end{align*}
It is not hard to verify that since $d_i = m$ for each $i \in \A$ and since $L$ is a subsequence of a feasible assignment of length $m$, the assignment $\nu'$ never violates the blocking constraints. Moreover, the average reward of $\nu'$ equals the average reward of the interval $L$ which is at least $R$. Finally, notice that the subsequence $L$ can be found in polynomial time, given the fact that the time horizon $T$ is defined to be polynomial in $k$ and $m$.
\end{proof}

We can now complete the proof of our hardness result. 
\newline

\noindent{\it Proof of Theorem~\ref{thm:submodular:hardness}.}
We prove the result via a reduction from the SWIU problem to the average version of the \rsm. Clearly, the average and non-average version of \rsm share the same approximability status, as the two problems are essentially identical up to a scaling of the objective function. 

Given an instance $I$ of SWIU, we can construct in polynomial time an instance $I'$ of the average version of \rsm, as described above. Let $\OPT_{SWIU}(I)$ and $\OPT_{\rsm}(I')$ be the optimal solution of SWIU and \rsm on the corresponding instance, respectively. 

We first show that when $\OPT_{SWIU}(I) \geq R$ for some reward $R$, then we necessarily have that $\OPT_{\rsm}(I') \geq \frac{R}{m}$. Indeed, let $L:[m] \rightarrow 2^{[k]}$ be an allocation that achieves a reward $R' = \OPT_{SWIU}(I) \geq R$ for the instance $I$ of SWIU. As indicated in proof of Lemma~\ref{lem:hardness:periodic}, we can construct in polynomial time a periodic assignment for the \rsm problem of average reward exactly $\frac{R'}{m}$, which implies that $\OPT_{\rsm}(I') \geq \frac{R'}{m} \geq \frac{R}{m}$.

Now, we would like to show that if $\OPT_{SWIU}(I) \leq \alpha R$ for some reward $R$ and $\alpha \in (0,1)$, then it has to be that $\OPT_{\rsm}(I') \leq \alpha \frac{R}{m}$. We prove the statement via its contrapositive, assuming that $\OPT_{\rsm}(I') > \alpha \frac{R}{m}$ for some reward $R$ and $\alpha \in (0,1)$. Let $\frac{R'}{m}> \alpha \frac{R}{m}$ be the optimal average reward of \rsm. By Lemma~\ref{lem:hardness:periodic}, we can assume w.l.o.g. that the assignment $\OPT_{\rsm}(I')$, that achieves an average reward of $\frac{R'}{m}$, is a periodic assignment of period $m$. However, given that all the delays are equal to $m$ in the instance $I'$ of \rsm, it is easy to see that in any period of $m$ consecutive rounds, each element is played at most once. Moreover, the average reward of each period is exactly $\frac{R'}{m}$. Therefore, any continuous subsequence of length $m$ in the solution of the \rsm naturally induces a solution to the instance $I$ of SWIU of total reward exactly $R'$. This, in turn, implies that $\OPT_{SWIU}(I) \geq R' \geq \alpha R$.

By the above discussion, we have completed the proof of a reduction from SWIU to \rsm. Therefore, any polynomial-time $\left(1 - \frac{1}{e} + \epsilon\right)$-approximation algorithm for \rsm, for some $\epsilon>0$, would imply a $\left(1 - \frac{1}{e} + \epsilon\right)$-approximation algorithm for SWIU. However, by Theorem~\ref{thm:hardness:swiu} this is not possible, unless $\textbf{P} = \textbf{NP}$.
\qed
\newline

We believe that, through a similar reduction as above, we can prove information-theoretic hardness of the \rsm problem by leveraging the results in~\cite{MSV08}. We leave this as future work.