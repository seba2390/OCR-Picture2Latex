\section{Matroid Blocking Bandits}
\label{section:mbb}
Let $\A$ be a set of $k$ arms and $T$ be an unknown time horizon. At any round $t \in [T]$ and for each $i \in \A$ a reward $X_{i,t}$ is drawn independently from an unknown distribution of mean $\mu_{i}$ and bounded support in $[0,1]$. Let $d_i \in \mathbb{N}_{>0}$ be the known determinisitc delay of each arm $i \in \A$, and $d_{\max} = \max_{i \in \A}\{d_i\}$. At any round $t \in [T]$, the player pulls any subset $\A_t$ of the available (i.e., non-blocked) arms, as long as it forms an {independent} set of a given {matroid} $\M = \left(\A,\I\right)$. The player only observes the realized reward of each arm she plays and collects their sum. The goal is to maximize the {\em expected cumulative reward} collected within $T$ rounds, denoted by
$\Rew^{IG}(T) = \Ex{ }{\sum_{t \in [T]} \sum_{i \in \A} X_{i,t} \event{i \in \A_t}}$. 

\subsection{The full-information setting}

The following algorithm is the implementation of \IS in the special case of the full-information \mbb setting, where the mean rewards $\{\mu_{i}\}_{i \in \A}$ are known to the player a priori:


\begin{algorithm}[\ig (\IG)]
For each arm $i \in \A$, let $r_i \sim U[0,1]$ be a random {\em offset} drawn uniformly from $[0,1]$. 
At every round $t = 1, 2, \dots$,  let $\G_t \subseteq \A$ be the subset of arms $i \in \A$, such that the interval $L_{i,t} = [t\cdot \frac{1}{d_i} + r_i, (t+1) \cdot \frac{1}{d_i} + r_i)$ contains an integer. Greedily compute a maximum independent set $\A_t$ of $\M|\G_t$ with respect to $\{\mu_i\}_{i \in \G_t}$ and play these arms.
\end{algorithm}

The correctness of the above algorithm follows directly by Fact~\ref{inter:fact:alwaysavailable} (that is, the sampled arms are never blocked) and by the fact that \IG always plays an independent set of $\M$, i.e., $\A_t \in \I_{\M|\G_t} \subseteq \I$. The approximation guarantee of \IG follows immediately as a special case of Theorem~\ref{thm:interleavedSubmodular}. Indeed, notice that: (i) The reward realizations do not affect the choices of \IG or any optimal algorithm maximizing the total expected reward. Thus, each realization $X_{i,t}$ can be replaced w.l.o.g. by its expected value $\mu_i$. (ii) The value of the greedily computed maximum independent set in $\M|\G_t$ corresponds to the weighted rank function $f_{\M, \mu}(\G_t)$ which, by Lemma~\ref{lem:weightedrank}, is monotone submodular.

\begin{restatable}{theorem}{restateinterleavedGreedy}\label{thm:interleavedGreedy}
The expected reward collected by \ig for $T$ rounds, $\Rew^{IG}(T)$, is at least
$
\left(1 - \frac{1}{e} \right) \OPT(T) - \mathcal{O}(d_{\max} \rk(\M))
$, where $\OPT(T)$ is the optimal expected reward.
\end{restatable}

As a point of interest, in Appendix \ref{appendix:mbb:fullinformation} we provide an alternative proof of the above theorem, which, instead of the concave closure of the weighted rank, now relies on the following (approximate) LP upper bound, based on the matroid polytope. For any set $S \subseteq \A$, let $\z(S) = \sum_{i \in S} z_i$.
\begin{align}
&\maximize_{\z \in \mathbb{R}^k} ~T \cdot \sum_{i \in \A} \mu_{i} z_{i} \tag{\textbf{LP}} \quad 
\textbf{s.t. } \z(S) \leq \rk(S)~ \forall S\subseteq \A
\textbf{, } \bm{0} \preceq \z \preceq \bm{d}^{-1}.
\end{align}

\begin{remark}
The analysis of $\IG$ is tight for rank-1 matroids. Indeed, consider $k$ arms, each of delay $k$ and deterministic reward equal to $1$. For $T \to \infty$, the optimal average reward is equal to $1$, simply by playing the arms in a round-robin manner. However, the probability that at least one arm is sampled at some round $t$ is equal to $\sum^k_{i = 1} {k \choose i} \left(\frac{1}{k}\right)^i \left(1 - \frac{1}{k}\right)^{k - i} = 1 - \left(1 - \frac{1}{k}\right)^k \to 1 - \frac{1}{e}$ as $k \to \infty$.
\end{remark}

\subsection{The bandit setting and regret analysis}

In the setting where the mean rewards are initially unknown, we develop a UCB-based bandit algorithm, \ucb (\UCB). The algorithm is identical to \IG, except for the greedy computation of the maximum independent set over the sampled arms, which is now performed using estimates. Specifically, the algorithm maintains for every $i \in \A$, $t \in [T]$ the following upper estimate of $\mu_i$:
\vspace{-0.5em}
\begin{align*}
\bar{\mu}_{i,t} = \hat{\mu}_{i,T_{i}(t)} + c_{i,t}\text{  with  } c_{i,t} = \sqrt{\frac{2 \ln{(t)}}{T_{i}(t)}},
\end{align*}
where $T_{i}(t)$ denotes the number of times arm $i$ has been played at the beginning of round $t$ and $\hat{\mu}_{i,T_{i}(t)}$ denotes the empirical average of the $T_{i}(t)$ i.i.d. samples from its reward distribution. The term $c_{i,t}$ is the {\em confidence length} around $\hat{\mu}_{i,T_{i}(t)}$ that guarantees $\bar{\mu}_{i,t}$  lies in $[\mu_i, \mu_i + 2c_{i,t}]$ with high probability. Note that all the above quantities are random variables depending on the random offsets and the observed reward realizations.

We are interested in upper bounding the $\alpha$-regret, for $\alpha = 1- \frac{1}{e}$, namely, the difference between $\alpha \OPT(T)$ and the expected reward collected by \UCB. Due to the complex time dynamics, characterizing the optimal expected reward as a function of the instance is hard. However, using Theorem~\ref{thm:interleavedGreedy} we can upper bound $\alpha \OPT(T)$ by the expected reward collected by \IG, thus giving:
\begin{align} \label{eq:regret:twoalgos}
\alpha \OPT(T) - \Rew^{UCB}(T)  \leq \Rew^{IG}(T) - \Rew^{UCB}(T) + \mathcal{O}(d_{\max} \cdot \rk(\M)).
\end{align}
By the above inequality, it becomes clear that in order to upper bound the regret, it suffices to bound the difference between the expected reward collected by \IG and \UCB. This difference not only depends on the reward realizations (through the UCB estimates), but also on the trajectory of sampled arms in each algorithm, which is itself a function of the random offsets. However, by construction of our interleaved scheduling scheme, these offsets are sampled at the initialization phase of each algorithm and are identically distributed. Thus, the trajectories of sampled arms in the two algorithms exhibit a coupled evolution. This allows us to analyse the regret ``pointwise'', under the assumption that the sequences of sampled arms are identical throughout the time horizon. To make this idea precise, let $\off^{\pi} \in [0,1]^k$ be the random offsets used and let $\{\G^{\pi}_t(\off^{\pi})\}_{t \in [T]}$ be the sequence of sampled arms by algorithm $\pi \in \{\IG, \UCB\}$. Using (henceforth) $\mathcal{R}$ to denote the randomness due to the reward realizations of the arms, the next lemma gives our pointwise regret bound.


\begin{restatable}{lemma}{restateRegretCoupling}
\label{lem:regret:coupling} Let $\bar{\mu}_t(S) = \sum_{i \in S} \bar{\mu}_{i,t}$ and $\mu(S) = \sum_{i \in S} \mu_i$. We have
\begin{align*}
    \Rew^{\IG}(T) - \Rew^{\UCB}(T) =  \Ex{\off \sim U[0,1]^k, \mathcal{R}}{\sum_{t \in [T]}\left( \max_{S \subseteq \G_t(\off), S \in \I} \{ \mu\left(S\right)\} - \mu\left(\arg\max_{S \subseteq \G_t(\off), S \in \I}\{\bar\mu_t(S)\}\right)\right)}.
\end{align*}
\end{restatable}

Thus w.l.o.g., we focus on the case where the sequences of sampled arms are identical. Let  $\mathcal{E}_{\off}$ denote the event that both algorithms, \IG and \UCB, sample the same offset vector $\off$, namely, $\off^{\IG} = \off^{\UCB} = \off$. Assuming that $\mathcal{E}_{\off}$ holds for some $\off \in [0,1]^k$, let $\{\G_t\}_{t \in [T]} = \{\G_t(\off)\}_{t \in [T]}$ be the sequence of sampled arms, common in both algorithms. Clearly, \UCB accumulates regret only when it plays independent sets of arms that are suboptimal w.r.t.\ the true means, i.e., when $\mu(\A^{\UCB}_t) < \mu(\A^{\IG}_t)$ for some $t \in [T]$. We assume w.l.o.g.\ that the arms are indexed in decreasing order of mean rewards and that these mean rewards are distinct. We now formally define the gaps related to our analysis:

\begin{definition}[Gaps]\label{def:gaps}
For any subset $S \subseteq \A$ and reward vector $\nu \in \mathbb{R}^k$, we define 
\begin{align*}
\Delta_S(\nu) = \max_{I \in \I, I \subseteq S}\{\mu\left(I\right)\}  - \mu\left(\arg\max_{B \in \I, B \subseteq S}\{\nu\left(B\right)\}\right).
\end{align*}
Moreover, let $\Delta_{i,j} = \mu_i - \mu_{j}$ be the standard suboptimality gap between two arms $i,j \in \A$.
\end{definition}

By Lemma \ref{lem:regret:coupling} and assuming that the event $\mathcal{E}_{\off}$ holds for some $\off$, we are interested in bounding the expectation of $\sum_{t \in [T]} \Delta_{\G_t(\off)}(\bar\mu_t)$ w.r.t.\ the reward realizations. The next step is to decompose the suboptimality of \UCB by noticing that both algorithms play, at each round $t \in [T]$, a basis of $\M|\G_t$ and thus $|\A^{\IG}_t| = |\A^{\UCB}_t|$. We use the following fundamental property of matroids:

\begin{theorem}[Strong Basis Exchange, Corollary 39.12a in \cite{schrijver03}]\label{cor:basesexchange} Let $\M = (\A,\I)$ be a matroid and $I_1, I_2 \in \I$ be two independent sets such that $|I_1| = |I_2|$. Then, there exists a bijection $\sigma : I_1 \rightarrow I_2$, such that for any $i \in I_1$ the set $I_1 - i + \sigma(i)$ is an independent set of $\M$. 
\end{theorem}

Let $\sigma_t: \A^{\UCB}_t \rightarrow \A_t^{\IG}$ for each $t \in [T]$ be the bijection described in Theorem~\ref{cor:basesexchange} with respect to the sets $\A^{\UCB}_t$ and $\A^{\IG}_t$ and let $\sigma_t^{-1}$ be its inverse mapping. 
Note that in any bijection $\sigma_t$ and any $i \in \A^{\UCB}_t \cap \A^{\IG}_t$ we can assume w.l.o.g. that $\sigma_t(i) = i$. Notice, further, that under the event $\mathcal{E}_{\off}$, the bijections $\{\sigma_t\}_{t \in [T]}$ are still random variables that depend on the observed realizations. 

\begin{restatable}{lemma}{restateRegretSub} \label{lem:regret:suboptdecomp} Under the event $\mathcal{E}_{\off}$ and at any time $t \in [T]$, we have 
$\Delta_{\G_t}(\bar{\mu}_t) = \sum_{i \in \A^{\IG}_t} \Delta_{i , \sigma^{-1}_{t}(i)}$.
\end{restatable}
Conditioned on the fact that both algorithms operate on the same sequence $\{\G_t\}_{t \in [T]}$ of sampled arms, Lemma \ref{lem:regret:suboptdecomp} allows us to decompose the suboptimality gap $\Delta_{\G_t}(\bar{\mu}_t)$ of each round $t \in [T]$, into simpler gaps of the form $\Delta_{i,j}$ between any arms $i \in \A^{\IG}_t$ and $j \in \A^{\UCB}_t$ that are perfectly matched according to the bijection $\sigma_t$, namely, $\sigma_t(j) = i$. Assuming that the event $\{\sigma_t(j) = i\}$ directly implies that $i \in A^{\IG}_t$ and $j \in A^{\UCB}_t$, we can further upper bound the regret as
\begin{align*}
\sum_{t \in [T]} \Delta_{\G_t}(\bar{\mu}_t) = \sum_{t \in [T]} \sum_{i \in \A^{\IG}_t} \Delta_{i , \sigma^{-1}_{t}(i)} \leq \sum_{t \in [T]} \sum_{i \in \A^{\IG}_t} \sum_{j \in \A, \Delta_{i,j} > 0} \Delta_{i,j} \event{\sigma_t(j)=i}.
\end{align*}

The above inequality allows us to study the regret attributed to each arm independently, using more standard arguments for UCB-based algorithms in combination with Theorem~\ref{cor:basesexchange}. Specifically, for every pair of arms $i,j \in \A$ with $i < j$ (thus, $\Delta_{i,j} > 0$), we define a threshold $\ell_{i,j}$ with the following key-property: After \UCB ``exchanges'' arm $j$ for arm $i = \sigma_t(j)$ more than $\ell_{i,j}$ times, due to insufficient exploration, then it has collected enough samples to infer that $\mu_j < \mu_i$ with high probability. 
\begin{restatable}{lemma}{restateRegretTechnical} \label{lem:regret:technical}
Let $\ell_{i,j} = \bigg\lfloor \frac{8 \ln(T)}{\Delta^2_{i,j}}\bigg\rfloor$ for any $i<j$. Under event $\mathcal{E}_{\off}$ and for any arm $j>1$, we have
\vspace{-1em}
\begin{align}
&\sum_{t \in [T]} \sum_{i < j} \Delta_{i , j} \event{\sigma_t(j)=i, T_j(t) \leq \ell_{i,j}} \leq \frac{16}{\Delta_{j-1,j}} \ln( T) &\quad\text{(Under-sampled regret)} \label{inq:reg:under}\\
& \Ex{\mathcal{R}}{\sum_{t \in [T]} \sum_{i < j} \Delta_{i , j} \event{\sigma_t(j)=i, T_j(t) > \ell_{i,j}}} \leq \frac{\pi^2}{3}\sum^{j-1}_{i=1} \Delta_{i,j} &\quad\text{(Sufficiently sampled regret)} \label{inq:reg:sufficiently}
\end{align}
\end{restatable}


\begin{proof}[Proof sketch of Theorem \ref{thm:approxregret}] 
By inequality \eqref{eq:regret:twoalgos} and Lemma \ref{lem:regret:coupling}, in order to bound the regret of \UCB, it suffices to upper bound the difference between $\Rew^{\IG}(T)$ and $\Rew^{\UCB}(T)$, conditioned on the fact that both algorithms use exactly the same offset vector $\off$ and, thus, they operate on the exact same sequence of sampled arms, denoted by $\{\G_t\}_{t \in [T]}$. By construction, \IG plays at any round $t \in [T]$ a basis of $\M|\G_t$ of maximum expected reward, while \UCB plays a basis of $\M|\G_t$ that is maximum with respect to the estimates $\{\bar{\mu}_{i,t}\}_{i \in \A}$. By Theorem~\ref{cor:basesexchange}, we can consider a perfect matching between exchangeable arms of $\A^{\IG}_t$ and $\A^{\UCB}_t$ and, thus, to decompose the regret into suboptimality gaps between individual arms. Then, using Lemma~\ref{lem:regret:technical}, we can upper bound on the expected regret due to the fact that \UCB erroneously plays arm $j$ instead of arm $i$, when $\Delta_{i,j} > 0$. The above analysis culminates in a regret bound that is a function of $\{\Delta_{i,j}\}_{i,j\in \A}$. In order to derive a gap-independent regret bound, we partition the gaps into ``small'' and ``large'' and notice that any pair of arms $i,j \in \A$ with $\Delta_{i,j} < \Theta(\sqrt{\frac{\ln(T)}{T}})$ cannot contribute more than $\sqrt{T\ln(T)}$ loss in the regret.
\end{proof}
