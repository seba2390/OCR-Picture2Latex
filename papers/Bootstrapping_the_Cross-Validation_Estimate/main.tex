\documentclass[12pt]{article}
\usepackage{algorithm}% http://ctan.org/pkg/algorithm
\usepackage{algpseudocode}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{enumerate}
% \usepackage{natbib}
\usepackage{url} % not crucial - just used below for the URL 

%\pdfminorversion=4
% NOTE: To produce blinded version, replace "0" with "1" below.
\newcommand{\blind}{1}

% DON'T change margins - should be 1 inch all around.
\addtolength{\oddsidemargin}{-.5in}%
\addtolength{\evensidemargin}{-1in}%
\addtolength{\textwidth}{1in}%
\addtolength{\textheight}{1.7in}%
\addtolength{\topmargin}{-1in}%




\usepackage{caption}
\usepackage{subcaption}

\begin{document}

%\bibliographystyle{natbib}

\def\spacingset#1{\renewcommand{\baselinestretch}%
{#1}\small\normalsize} \spacingset{1}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\if1\blind
{
  \title{\bf Bootstrapping the Cross-Validation Estimate}
  \author{Bryan Cai$^1$,  Fabio Pellegrini$^2$, Menglan Pang$^2$, Carl de Moor$^2$,\\
  Changyu Shen $^2$, Vivek Charu$^3$, and Lu Tian $^4$ %\thanks{\textit{}}
   \vspace{1 cm}\\
   {\small $^1:$ Department of Computer Science, Stanford University $^2:$ Biogen Inc.} \\
   {\small $^3:$ Department of Medicine,  Stanford University}\\
   {\small $^4:$ Department of Biomedical Data Science, Stanford University}
    }
  \maketitle
} \fi

\if0\blind
{
  \bigskip
  \bigskip
  \bigskip
  \begin{center}
    {\LARGE\bf Bootstrapping the Cross-Validation Estimate}
\end{center}
  \medskip
} \fi

\bigskip
\begin{abstract}
Cross-validation is a widely used technique for evaluating the performance of prediction models. It helps avoid the optimism bias in error estimates, which can be significant for models built using complex statistical learning algorithms. However, since the cross-validation estimate is a random value dependent on observed data, it is essential to accurately quantify the uncertainty associated with the estimate. This is especially important when comparing the performance of two models using cross-validation, as one must determine whether differences in error estimates are a result of chance fluctuations. Although various methods have been developed for making inferences on cross-validation estimates, they often have many limitations, such as stringent model assumptions This paper proposes a fast bootstrap method that quickly estimates the standard error of the cross-validation estimate and produces valid confidence intervals for a population parameter measuring average model performance. Our method overcomes the computational challenge inherent in bootstrapping the cross-validation estimate by estimating the variance component within a random effects model. It is just as flexible as the cross-validation procedure itself. To showcase the effectiveness of our approach, we employ comprehensive simulations and real data analysis across three diverse applications.
\end{abstract}

\noindent%
{\it Keywords:} %bootstrap,  cross-validation,
random effects model, mean absolute prediction error, c-index, individualized treatment response score
\vfill

\newpage
\spacingset{1.9} % DON'T change the spacing!
\section{Introduction}

Predictive modeling has emerged as a prominent tool in biomedical research, encompassing diverse applications such as disease diagnosis, patient risk stratification, and personalized treatment recommendations, as seen in studies such as \cite{sullivan2004presentation, hemann2007framingham, solomon2006renal, krittanawong2017artificial}. A wide range of methods have been employed to create prediction models, from basic linear regression to sophisticated deep learning algorithms. Once the models have been developed, it's crucial to assess their performance for a number of reasons. Firstly, the results from a model cannot be effectively utilized or interpreted without understanding its accuracy. For instance, a positive result from a model with a positive predictive value (PPV) of 20\% will be treated differently from a positive result with a PPV of 1\% by both physicians and patients. Secondly, with a wealth of prediction tools at hand, choosing the best model from a set of models can be a challenge, with multiple factors influencing the decision, including cost, interpretability, and, most importantly, the model's future performance in a population. This performance can be measured in various ways, depending on the intended application of the model. If the model aims to predict clinical outcomes, its accuracy can be measured by a prediction accuracy metric, such as mean squared prediction error for continuous outcomes or a receiver operating characteristics (ROC) curve for classification. In other cases, the performance measure can be more complex. If the model is used to recommend treatment to individual patients, the model performance can be measured by the observed treatment benefit among patients who are recommended to receive a particular treatment according to the model. Lastly, even in the model construction step, evaluating the model performance is often needed for optimal tuning. For example, in applying neural networks, the network structure needs to be specified by the analyst to have best prediction performance. 

Cross-validation is a commonly used technique to assess the performance of a predictive model and overcome the over-optimistic bias that results from using the same data for both training and evaluating the model \cite{efron1997improvements, hastie2009elements, tibshirani2009bias, patil2021uniform}. The approach involves using out-of-sample observations to evaluate model performance, thus avoiding optimism bias. The resulting cross-validation estimator is a random quantity, dependent on both the random splitting of data into training and testing sets, and the observed data itself. To reduce the randomness due to the splitting of data, one can repeat the training and testing process multiple times and average the prediction performance on the testing data. The randomness inherent in the observed data reflects the fact that if a new set of data were randomly sampled from the underlying population, the cross-validation results would be different from those based on the current observed data. In essence, the cross-validation estimator is a statistic, or a function of random data, despite its complex construction.

Realizing this fact, it is important to derive and estimate the distribution of this statistic so that we may (1) understand what population parameter the cross-validation procedure estimates and (2) attach an appropriate uncertainty measure to the cross-validation estimate \cite{bayle2020cross, lei2020cross, yousef2021estimating}.  For the simple case of large sample size and small number of parameters,  the asymptotic distribution of the cross-validation estimator has been studied in depth \cite{dudoit2005asymptotics, tian2007model, jiang2008estimating, ledell2015computationally}.  For example, when the model training and validation are based on the same loss function, the cross-validation estimator is asymptotically Gaussian \cite{dudoit2005asymptotics}.  A computationally efficient variance estimator for the cross-validated area under the ROC curve has also been proposed, if the parameters used in the classifier are estimated at the root $n$ rate  \cite{ledell2015computationally}. More recently,  Bayle et al. have established the asymptotic normality of general $K$-fold cross-validated prediction error estimates and proposed a consistent variance estimator under a set of stability conditions \cite{bayle2020cross}. The learning algorithm can be general and flexible, but the error estimate in the validation set needs to be in the form of sum of identically independent distributed random elements. The validity of their proposed variance estimate requires that the variation from model training is dominated by that in estimating the prediction error in the testing set.  Along a similar line, Austern and Zhou (2020) have also studied the asymptotic distribution of $K$ fold cross-validation estimates allowing $K$ increases with the sample size \cite{austern2020asymptotics}.  The proposed variance estimator is very similar to that in \cite{bayle2020cross}. Asymptotic normality of the cross-validation estimator has been established when the prediction model is trained on the same loss as is used for evaluation, the same case as in \cite{dudoit2005asymptotics}. A nested cross-validation method has been introduced to automatically quantify the uncertainty of the cross-validation estimate and construct confidence intervals for the model performance \cite{bates2021cross}. The key is to use an additional loop of cross-validations to correct the finite-sample bias of the variance estimator proposed in \cite{bayle2020cross, austern2020asymptotics}. However, this method still requires specific forms for the performance measure of interest and may not be applicable in certain applications.

The majority of previous work on cross-validation assumes a simpler form for the performance measure, such as the average of a random variable, and requires that the prediction model is trained using the same loss function. However, there are applications of cross-validation not covered by these conventional approaches, as demonstrated in Example 3 of the paper. Additionally, the validity of the proposed confidence intervals relies on suitable stability conditions and large sample approximations. Although resampling methods are a well-known approach for estimating the variance of a statistic and can provide fairly accurate approximations even with small to moderate sample sizes, the main challenge with using them in this setting is the computational speed. This paper aims to characterize the underlying population parameter estimated by the cross-validation procedure and to propose a general, computationally efficient resampling method for constructing a confidence interval for this population parameter, removing as many restrictions as possible.


\section{Method}
In a very general setup, we use random vector $X$ to denote individual observations, and the observed data consist of $n$ independent identically distributed (i.i.d.) copies of $X$, i.e., $D=\{X_1,\cdots, X_n\}.$ The output of the training procedure is a parameter estimate, which can be a finite dimensional vector or infinite dimensional functions, denoted by $\hat{\psi}(D)$ to emphasize its dependence on observed data $D$ and the fact that it is a random quantity. In evaluating the ``performance'' of $\hat{\psi}(D)$ in a new testing set consisting of $m$ i.i.d.  observations $\widetilde{D}=\{\widetilde{X}_1, \cdots, \widetilde{X}_m\},$ a summary statistic is calculated as a function of testing data and $\hat{\psi}(D),$ which can be written as 
$$L\left\{\widetilde{D}, \hat{\psi}(D)\right\}.$$
It is possible to make inference and derive confidence interval on $L\left\{\widetilde{D}, \hat{\psi}(D)\right\}$ by treating $\widetilde{D}$ or both $D$ and $\widetilde{D}$ as random. In most applications, however, we only have a single dataset, and the cross-validation procedure is needed in order to objectively evaluate the model performance. Specifically,  In cross-validation, we randomly divide the observed data $D$ into two non-overlapping parts denoted by $D_{train}$ and $D_{test}$, and calculate 
$L\left\{D_{test}, \hat{\psi}(D_{train})\right\}.$ In order to reduce the variability of random splits, the aforementioned step is oftentimes repeated many times and the average performance measure is obtained as the final cross-validation estimator of the model performance:
$$ \widehat{Err}^{CV}=\frac{1}{B_{CV}}\sum_{b=1}^{B_{CV}} L\left\{D_{test}^b, \hat{\psi}(D_{train}^b)\right\},$$
where $D=D_{train}^b\cup D_{test}^b$ represents the $b$th split. The number of replications, $B_{CV}$, needs to be relatively large to reduce the Monte-Carlo variation due to random splits. It is often in the range of several hundreds in practice. Note that although we used $Err$ to represent the model performance in consistent with notations used in the literature \cite{bates2021cross}, the performance measure is not necessarily a prediction error. It can be other metric with a higher value indicating a good performance as discussed in the following sections.

\subsection{Applications}
Many cross-validation applications can fit into the very general framework described above.  In this paper, we will focus on several typical examples. 

\subsubsection{Application 1} \label{sec:application1}
In the first example, we are interested in evaluating the performance of predicting continuous outcomes measured by mean absolute prediction error \cite{tian2007model, willmott2005advantages}.  The result can help us to determine, for example, whether a newly developed prediction algorithm significantly outperform an existing model.
The prediction model can be constructed by fitting a standard linear regression model, i.e., calculating a regression coefficient vector $\hat{\beta}(D_{train})$ by minimizing a $L_2$ loss function
$$ \sum_{X_i\in D_{train} } \left(Y_i- \beta'\widetilde{Z}_i \right)^2,$$
based on a training dataset $D_{train}$, where  $Z_i$ is the baseline covariate for the $i$th patient and $\widetilde{Z}_i=(1, Z_i')'$ including an intercept.   If nonlinear prediction models are considered, then one may construct the prediction model via a more flexible machine learning algorithm such as random forest or neural network.   In all cases, the prediction error in a testing set $D_{test}$ is calculated as 
$$\hat{\theta}(D_{train}, D_{test})=\frac{1}{n_{test}}\sum_{X_i \in D_{test}}|Y_i-\hat{\beta}(D_{train})'\tilde{Z}_i|,$$
where $n_{test}$ is the number of observations in the testing set. In cross-validation, we may repeatedly split an observed dataset $D$ into training and testing sets of given sizes, $(D_{train}, D_{test})$ , and obtain the corresponding cross-validated mean absolute prediction error estimator, $\hat{\theta}(D_{train}, D_{test})$.  In the end, the sample average of those resulting cross-validated mean absolute prediction error estimators becomes the final estimator measuring the predictive performance of the linear model. In this application, $X=(Z, Y)$ with $Z$ and $Y$ being the predictor and  outcome of interest, respectively, $\hat{\psi}(D)=\hat{\beta}(D)$, and the summary statistic measuring the prediction performance is:
$$L\left(D_{test}, \psi\right)=\frac{1}{n_{test}}\sum_{X_i\in D_{test}}|Y_i-\psi'\widetilde{Z}_i|.$$


\subsubsection{Application 2} \label{sec:application2}
In the second example, we are interested in evaluating the performance of a prediction model in predicting binary outcomes by its c-index for new patients, which is the area under the ROC curve \cite{faraggi2002estimation, pepe2003statistical}. The result can help us to determine, for example, whether c-index from a new prediction model is significantly higher than 0.5 or a desirable level. The prediction model can be constructed via fitting a logistic regression model, i.e., calculating a regression coefficient vector $\hat{\beta}(D_{train})$ by maximizing the log-likelihood function
$$ \sum_{(
Z_i,Y_i)\in D_{train} } \left[ \beta'Z_i Y_i-\log\left\{1+\exp(\beta'\tilde{Z}_i)  \right\}\right],$$
based on a training dataset $D_{train}$. If the dimension of the predictor $Z_i$ is high, a lasso-regularization can be employed in estimating $\beta$.  In any case, a concordance measure, the c-index in a testing set $D_{test}$ can be calculated as 
$$\hat{\theta}(D_{train}, D_{test})=\frac{1}{\tilde{n}_{test, 0}\tilde{n}_{test, 1}}\sum_{X_i \in D_{test}(0)}\sum_{X_j\in D_{test1}(1)}I\left(\hat{\beta}(D_{train})'\tilde{Z}_i<\hat{\beta}(D_{train})'\tilde{Z}_j \right),$$
where $\tilde{n}_{test,g}$ is the number of observations in the set $D_{test}(g)=\{(Z_i,Y_i)\in D_{test}: Y_i=g\}, g\in \{0, 1\}.$
In cross-validation, we may repeatedly split the observed dataset $D$ into training and testing sets, $(D_{train}, D_{test}),$ and obtain the corresponding cross-validated c-indexes $\hat{\theta}(D_{train}, D_{test})$.  In the end, the sample average of those resulting cross-validated c-index estimator is our final estimator measuring the classification performance of the logistic regression. In this application, $X=(Z, Y)$ with $Z$ and $Y$ being the predictor and a binary outcome of interest, respectively, $\hat{\psi}(D)=\hat{\beta}(D)$, and 
$$L\left(D_{test}, \psi\right)=\frac{1}{\tilde{n}_{test, 0}\tilde{n}_{test, 1}}\sum_{X_i\in D_{test}(0)}\sum_{X_j\in D_{test}(1)} I\left(\psi'\tilde{Z}_i<\psi'\tilde{Z}_j \right).$$

\paragraph{Remark 1}
In evaluating the performance of the logistic regression model for predicting binary outcomes, we may choose to use the entire ROC curve to measure the model performance. Since the construction of a stable ROC curve requires sufficient number of cases, i.e, observations with $Y_i=1,$ and controls, i.e., observations with $Y_i=0,$ one may want to construct the ROC curve with as many observations from testing set as possible.  In particular, one can implement the $K$-fold cross-validation, i.e., randomly splitting the observed dataset $D$ in to $K$ parts of approximately equal sizes: $D=D_1\cup D_2 \cup \cdots D_K.$ Let $\hat{\beta}(D^{(-k)})$ be the regression coefficient estimated based on observations not in the $k$th part of the observed data. We may then construct the predicted risk score for patients in the $k$th part as 
$\hat{\beta}(D^{(-k)})'Z_i.$ Cycling through $k=1,\cdots, K,$ we can obtain a predicted risk score for every patient as
$$ \hat{R}_i({\cal D})=\left\{\sum_{k=1}^K \hat{\beta}(D^{(-k)})I(X_i\in D_k)\right\}'Z_i, i=1, \cdots, n,$$
where ${\cal D}$ represents the particular division of the observed dataset into $K$ parts.  The ROC curve can then be calculated as 
$$ \widehat{ROC}(u \mid {\cal D})=\hat{S}_{1{\cal D}}\left\{\hat{S}_{0{\cal D}}^{-1}(u)\right\},$$
where $\hat{S}_{g{\cal D}}(\cdot)$ is the empirical survival function of $\left\{\hat{R}_i({\cal D})\mid Y_i=g, i=1, \cdots, n\right\}, g\in \{0, 1\},$ depending on the particular division of ${\cal D}$. In cross-validation, we may repeatedly split the observed dataset $D$ into $K$ parts and obtain the corresponding cross-validated ROC curve $\widehat{ROC}(u\mid {\cal D})$, and the sample average of those resulting ROC curves becomes the final estimator of the ROC curve measuring the predictive performance of the logistic regression. In this example, 
$$L\left(D_{test}, \psi\right)(u)=\hat{S}_{1} \left\{\hat{S}_{0}^{-1}(u\mid \psi, D_{test} ) \mid \psi, D_{test} \right\},$$
where $\hat{S}_{g}(\cdot\mid \psi, D_{test})$ is empirical survival function of $\left\{\psi'Z_i \mid Y_i=g,  X_i\in D_{test}\right\}, g\in \{0, 1\}.$ Our proposed method will cover inference for estimator from $K$ fold cross-validation as well.


\subsubsection{Application 3}
In  the third example, we are interested in developing a precision medicine strategy and evaluating its performance in a randomized control setting. Specifically, the precision medicine strategy here is a binary classification rule to recommend a treatment to a patient based on his or her baseline characteristics to maximize the treatment benefit for individual patient as well as in a patient population of interest. Before applying this recommendation to clinical practice, it is important to estimate the uncertainty of the treatment effect in the identified subgroup who would be recommended for the treatment, to make sure the anticipated stronger treatment effect is real. There are many different ways of constructing such a treatment recommendation classifier \cite{chen2017general, tian2014simple}.  For example, one may first construct a individualized treatment response (ITR) score  by minimizing a loss function based on a training dataset $D_{train},$ 
$$\sum_{X_i\in D_{train}} \left\{Y_i-\gamma'\tilde{Z}_i-(G_i-\pi)\beta'\tilde{Z}_i \right\}^2 ,$$
where $X_i=(Z_i,G_i,Y_i),$ $Y_i$ is the response of interest with a higher value being desirable, $G_i \in \{0, 1\}$ is a binary treatment indicator and independent of the baseline covariate $Z_i$ (i.e., the treatment is randomly assigned to patients in the training set), and $\pi=P(G_i=1).$ Let the resulting minimizers of $\gamma$ and $\beta$ be $\hat{\gamma}(D_{train})$ and $\hat{\beta}(D_{train})$, respectively \cite{tian2014simple, yadlowsky2021estimation}.  Note that we have the decomposition that 
\begin{align*}
& E\left\{\left(Y-\gamma'\tilde{Z}-(G-\pi)\beta'\tilde{Z} \right)^2\right\}  \\
=& P(G=1)E\left\{\left(Y^{(1)}-\gamma'\tilde{Z}-(1-\pi)\beta'\tilde{Z} \right)^2\right\}+P(G=0)E\left\{\left(Y^{(0)}-\gamma'\tilde{Z}+\pi\beta'\tilde{Z} \right)^2\right\}\\
=& E\left[ \left(Y-\gamma'\tilde{Z} \right)^2 +\pi(1-\pi) \left(Y^{(1)}-Y^{(0)}-\beta'\tilde{Z}\right)^2-\pi(1-\pi)\left(Y^{(1)}-Y^{(0)}\right)^2  \right],
\end{align*}
where $Y^{(g)}$ is the potential outcome if the patient receives treatment $g \in \{0, 1\},$ and the observed outcome $Y=Y^{(1)}G+Y^{(0)}(1-G).$ 
Therefore, minimizing the original loss function with respect to $\beta$ amounts to approximating the conditional average treatment effect (CATE), 
$$\Delta(z)=E\left(Y^{(1)}-Y^{(0)} \mid Z=z\right),$$ via 
 $(1, z')\hat{\beta}(D_{train})$, a linear function of $Z=z$, because the solution $\widehat{\beta}(D_{train})$ minimizes a loss function, whose population limit is 
$$ E\left\{\left(Y^{(1)}-Y^{(0)}-\beta'\tilde{Z} \right)^2\right\}.$$
 The constructed ITR score is $\widehat{\Delta}(z\mid D_{train})=(1, z')\hat{\beta}(D_{train}),$ which can be used to guide the treatment recommendation for individual patient.  There are other ways of constructing the ITR score approximating the CATE.  Once an estimated ITR score is available, one may recommend treatment $G=1$ to patients whose $\widehat{\Delta}(Z\mid D_{train})>c_0$ and treatment $G=0$ to patients whose $\widehat{\Delta}(Z\mid D_{train})\le c_0,$ where $c_0$ is a constant reflecting the ``cost'' of the treatment. Here, we choose $c_0=0$ for simplicity.  In the testing set, one may evaluate the performance of this recommendation system by estimating the average treatment effect among the subgroup of patients recommended to receive the treatment $G=1,$ i.e, $\widehat{D}_{test}^{(1)}=\left\{X \in D_{test} \mid \widehat{\Delta}(Z\mid D_{train})>0\right\}$ and among the subgroup of patients recommended to receive the treatment $G=0,$ i.e., $\widehat{D}_{test}^{(0)}=\left\{X \in \widehat{D}_{test} \mid \widehat{\Delta}(Z\mid D_{train})\le 0\right\}.$  Specifically, we may consider the observed treatment effects
$$ \widehat{\Delta}_1(D_{train},D_{test})= \frac{\sum_{X_i\in \hat{D}_{test}^{(1)}}Y_iG_i}{\sum_{X_i\in \hat{D}_{test}^{(1)}} G_i}-\frac{\sum_{X_i\in \hat{D}_{test}^{(1)}}Y_i(1-G_i)}{\sum_{X_i\in \hat{D}_{test}^{(1)}} (1-G_i)}$$
and
$$ \widehat{\Delta}_0(D_{train},D_{test})= \frac{\sum_{X_i\in \hat{D}_{test}^{(0)}}Y_iG_i}{\sum_{X_i\in \hat{D}_{test}^{(0)}} G_i}-\frac{\sum_{X_i\in \hat{D}_{test}^{(0)}}Y_i(1-G_i)}{\sum_{X_i\in \hat{D}_{test}^{(0)}} (1-G_i)}$$

If $\widehat{\Delta}_1(D_{train}, D_{test})$ takes a ``large" positive value and $\widehat{\Delta}_0(D_{train}, D_{test})$ takes a ``large'' negative value, (in other words, the treatment effect is indeed estimated to be greater among those who are recommended to receive the treatment based on the constructed ITR score), then  we may conclude that $\widehat{\Delta}(Z\mid D_{train})>0, $ is an effective treatment recommendation system. 
In cross-validation, we may repeatedly divide the observed data set $D$ into training and testing sets, $(D_{train}, D_{test}),$ and obtain the corresponding cross-validated treatment effect difference $\widehat{\Delta}_g(D_{train}, D_{test}), g\in \{0, 1\}$.  In the end, the sample average of those resulting cross-validated treatment effect estimators is our final cross-validation estimator measuring the performance of the treatment recommendation system. In this application $X=(Z,G,Y)$ with $Z$, $G$ and $Y$ being predictors, treatment assignment indicator, and a binary outcome, respectively, $\hat{\psi}(D)=\hat{\beta}(D)$, and 
$$L\left(D_{test}, \psi\right)=\frac{\sum_{X_i\in D_{test}}Y_iG_iI(\psi'Z_i>0)}{\sum_{X_i\in D_{test}} G_i I(\psi'Z_i>0)}-\frac{\sum_{X_i\in D_{test}}Y_i(1-G_i)I(\psi'Z_i>0)}{\sum_{X_i\in D_{test}}(1-G_i)I(\psi'Z_i>0)}$$
or
$$
~~~~~~~~~~~~~~~~~~~\frac{\sum_{X_i\in D_{test}}Y_iG_iI(\psi'Z_i\le 0)}{\sum_{X_i\in D_{test}} G_i I(\psi'Z_i\le 0)}-\frac{\sum_{X_i\in D_{test}}Y_i(1-G_i)I(\psi'Z_i\le 0)}{\sum_{X_i\in D_{test}}(1-G_i)I(\psi'Z_i\le 0)}.
$$


\subsection{The Estimand of Cross-validation}
The first important question is what population parameter the cross-validation procedure estimates. As discussed in \cite{bates2021cross}, there are several possibilities.  The first obvious population parameter is 
$$ Err(D_n)=  \lim_{N \rightarrow \infty} L\left(\tilde{D}_N, \hat{\psi}(D_n)\right),$$
where $D_n$ is the training set of sample size $n$ and $\tilde{D}_N$ is a new independent testing set of sample size $N$ drawing from the same distribution as the training dataset, $D_n$. This parameter depends on the training set $D_n,$ and directly measures the performance of the prediction model obtained from observed data $D_n$ in a future population. The second population parameter of interest is 
$$ Err_n=E\{Err(D_n)\},$$
the average performance of prediction models trained based on ``all possible'' training sets of size $n$ sampled from the same distribution as the observed dataset, $D_n.$ The subscript $n$ emphasizes the fact that this population parameter only depends on the sample size of the training set $D_n.$ While $Err(D_n)$ is the relevant parameter of interest in most applications, where we want to know the future performance of the prediction model at hand,  $Err_n$ is a population parameter reflecting the expected performance of prediction models trained via a given procedure. The prediction performance of the model from the observed dataset $D_n$ can be better or worse than this expected average performance. It is known that the cross-validation targets on evaluating a training procedure rather than the particular prediction model obtained from the training procedure. Specifically, the cross-validation estimator $\widehat{Err}^{CV}_m$ actually estimates $Err_m$ in the sense that 
$$\left(\begin{array}{c} \widehat{Err}_m^{CV} \\ Err(D_n) \end{array}\right)\approx \left(\begin{array}{c} Err_m+\epsilon \\ Err_n+\zeta \end{array}\right),$$
where $m$ is the sample size of the training set used in constructing the cross-validation estimate, i.e., the dataset $D_n$ is repeatedly divided into a training set of size $m$ and a testing set of size $(n-m)$ in cross-validation. Here, $\epsilon$ and $\zeta$ are two mean zero random noises and oftentimes nearly independent.
In many cases $Err_n\approx Err_m$, when $m$ is not substantially smaller than $n.$ If we ignore their differences, then $\widehat{Err}^{CV}_m$ can also be viewed as an approximately unbiased estimator for $Err(D_n)$, because 
$$\widehat{Err}^{CV}_m-Err(D_n)=(Err_m-Err_n)+(\epsilon-\zeta),$$ 
whose mean is approximately zero. On the other hand, the variance of 
$\widehat{Err}^{CV}_m-Err(D_n)$ tends to be substantially larger than the variance of $\widehat{Err}^{CV}_m-Err_m,$ since $\epsilon$ and $\zeta$ are often independent and the variance of $\zeta$ is nontrivial relative to that of $\epsilon$. This is analogous to the phenomenon that the sample mean of observed data is a natural estimator of the population mean. It also can be viewed as an unbiased ``estimator'' of the sample mean of a set of future observations, because the expectation of sample mean of future observations is the same as the population mean, which can be estimated by the sample mean of observed data.  In this paper, we take $Err_m$ as the population parameter of interest, because approximately $Err(D_n)$ is simply $Err_m$ plus a random noise $\zeta$, which may be independent of the cross-validation estimate.  In other words, we take the view that the cross-validation estimate evaluates the average performance of a training procedure rather than the performance of a particular prediction model.  As the training sample size goes to infinity, we write $Err=\lim_{n\rightarrow \infty}Err_n.$ When $n$ is sufficiently large, $Err\approx Err_n \approx Err(D_n).$

In the following, we use a simple example to demonstrate the relationship of aforementioned quantities. 

\subsubsection{A Toy Example} \label{sec:toyexample}
Consider the linear prediction model in section \ref{sec:application1}. Suppose that covariate $Z_i\sim N(0, I_{10})$,  and the response
$Y_i=\alpha_0+\beta_0'Z_i+\epsilon_i, i=1,\cdots, n,$
where $I_{10}$ is 10 by 10 identity matrix, $\alpha_0=0$, $\beta_0=(1, 1, 1, 1, 0, 0, 0, 0, 0, 0)',$  $\epsilon_i\sim N(0, 1)$ and $n=90.$  We were interested in constructing a prediction model via fitting a linear regression model and evaluating its performance in terms of the mean absolute prediction error. To this end, for each simulated dataset $D_n=\{X_i=(Z_i, Y_i), i=1,\cdots, n\},$  we estimated the regression coefficients of the linear model by ordinary least square method and denoted the estimators of $\alpha_0$ and $\beta_0$ by $\hat{\alpha}(D_n)$ and $\hat{\beta}(D_n),$ respectively.  Then we calculated the true mean absolute prediction error as the expectation of $|G|,$ where $G\sim N\left(\hat{\alpha}(D_n), 1+\|\hat{\beta}(D_n)-\beta_0\|_2^2\right)$ is a random variable.   This expectation was $Err(D_n)$, the prediction error of the model trained based on dataset $D_n$ in a future population.  We also constructed the cross-validation estimate of the prediction error by repeatedly splitting $D_n$ into a training set of size $m=80$ and a testing set of size $n-m=10.$  The resulting estimator for the estimation error was $\widehat{Err}_m^{CV}.$  Repeating these steps, we obtained 1000 pairs of $Err(D_n)$ and $\widehat{Err}_m^{CV}$ from simulated datasets. The empirical average of 1000 $Err(D_n)$s was an approximation to $Err_n=E\left\{Err(D_n)\right\}.$  Figure \ref{fig:errscatter} is the scatter plot of $Err(D_n)$ vs. $\widehat{Err}^{CV}_m$ based on 1000 simulated datasets. It was clear that $\widehat{Err}^{CV}_m$ and $Err(D_n)$ were almost independent but shared a similar center. Specifically, the empirical average of $\widehat{Err}^{CV}_m$ was 0.859, and the empirical average of $Err(D_n)$ was 0.851. Therefore, the cross-validated error estimator  $\widehat{Err}^{CV}_m$ can be viewed as a sensible estimator for $Err_n=E\left\{Err(D_n)\right\}\approx 0.851$, and more precisely an unbiased estimator for $Err_m$, whose value was estimated as 0.861 using the same simulation described above. Note that $(m, n)=(80, 90)$ and $n$ and $m$ are fairly close. The distribution of the cross-validated error estimators along with $Err_m$ and $Err_n$ are plotted in Figure \ref{fig:errdist}, suggesting that the small difference between $Err_{80}$ and $Err_{90}$ is negligible relative to the variation of the cross-validation estimator $\widehat{Err}^{CV}_m$ itself. In addition, $\widehat{Err}^{CV}_m$ can also be thought as a ``prediction'' to $Err(D_n),$ since the latter was approximately $Err_n$ plus an independent mean zero ``measurement error''.


\begin{figure}
\centering
\begin{subfigure}[b]{0.45\textwidth}
\centering
\includegraphics[width=\textwidth ]{Figcv1.pdf}
\caption{$Err(D_n)$ vs $\widehat{Err}^{CV}_m$ for estimating the mean absolute prediction error}
\label{fig:errscatter}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.45\textwidth}
\centering
\includegraphics[width=\textwidth ]{Figcv2.pdf}
\caption{The distribution of $\widehat{Err}^{CV}_m$ for estimating the mean absolute prediction error; $Err_n$: dashed line; $Err_m$: solid line}
\label{fig:errdist}
\end{subfigure}


\end{figure}

\subsection{Statistical Inference on $Err_m$}
In this section, we aim to construct a valid confidence interval for $Err_m$ based on the cross-validated estimate.  First, we define the cross-validated estimate with repeated training and testing splits as 
$$ \widehat{Err}^{CV}_m = E\left[L\left\{D_{test}^b, \hat{\psi}(D_{train}^b)\right\}\right]$$
where the expectation is with respect to random division of training and testing sets. One may anticipate that $\widehat{Err}^{CV}$ is a ``smooth" functional with respect to the empirical distribution of observed data $D_n$, because each individual observation's contribution to the final estimator is ``averaged'' across different training and testing divisions.  Therefore, we expect that $\widehat{Err}^{CV}_m$ is a root-$n$ regular estimator of $Err$, i.e.,
$$\sqrt{n}\left(\widehat{Err}^{CV}_m-Err\right) \rightarrow N(0, \sigma_D^2),$$
in distribution as $n$, the sample size of $D_n$, goes to infinity and $\lim_{n\rightarrow \infty} m/n \in (0, 1).$  Indeed, under the following regularity conditions, this weak convergence holds in general.
\begin{enumerate}
\item[C1] $\hat{\psi}-\psi_0$ has a first order expansion, i.e, 
$$\hat{\psi}-\psi_0=\frac{1}{n}\sum_{i=1}^n \xi(X_i)+o_p(n^{-1/2}),$$
where $\hat{\psi}$ is a consistent estimator for a population parameter $\psi_0$ based on $D_n=\{X_1,\cdots, X_n\},$ and $\xi(X_i),i=1,\cdots, n$ are independent mean zero random elements with a finite variance.
\item[C2] The process $L\left(D_n, \psi\right)-E\left\{L(D_n, \psi)\right\}$ is stochastic equal continuous at $\psi$, i.e., 
$$ \sqrt{n}\left[L\left(D_n, \psi_1\right)-E\left\{L(D_n, \psi_1)\right\}\right]-\sqrt{n}\left[ L\left(D_n, \psi_2\right)-E\left\{L(D_n, \psi_2)\right\}\right]=o_p(1)$$
as $\|\psi_2-\psi_1\|=o(1),$ where $\|\cdot\|$ is an appropriate norm measuring the distance between $\psi_1$ and $\psi_2.$ \cite{kosorok2008introduction}
\item[C3] The random sequences 
$$\sqrt{n}\left[L\left(D_n, \psi_0\right)-E\left\{L(D_n, \psi_0)\right\}\right]
=\frac{1}{\sqrt{n}}\sum_{i=1}^n \eta(X_i)+o_p(1),$$ converges to a mean zero Gaussian distribution, where $\eta(X_i)$ are independent mean zero random elements with a finite variance.
\item[C4] There exists a linear functional $l_{\psi}(\cdot)$ indexed by $\psi,$ such that
$$E\left\{L(D_n, \psi_2)\right\}-E\left\{L(D_n, \psi_1)\right\}=l_{\psi_1}(\psi_2-\psi_1)+o(\|\psi_2-\psi_1\|).$$ 
%\item $\sqrt{n}\left[E\left\{L(D, \psi_0)\right\}-E\left\{L(D, \psi_n)\right\}\right]=o(1)$
%\item[C5] $E\left\{L(D_n, \psi_0)\right\}-Err=o_p(n^{-1/2}).$
\end{enumerate}
In the Appendix, we have provided an outline to show that 
\begin{align*}
   &\sqrt{n}\left\{L\left\{D_{test}^b, \hat{\psi}(D_{train}^b)\right\}-Err\right\}\\
  =&\frac{1}{\sqrt{n}}\sum_{i=1}^n \left[\frac{I(X_i\in D_{test})}{1-\hat{\pi}}\eta(X_i)+\frac{I(X_i\in D_{train})}{\hat{\pi}}l_{\psi_0}\{\xi(X_i)\} \right] + o_p(1),
\end{align*}
where $\hat{\pi}=m/n.$   After taking expectation with respect to the random training and testing division, i.e., indicators $\{I(X_i \in D_{train}), i=1, \cdots, n\},$
$$ E\left\{\frac{I(X_i\in D_{test})}{1-\hat{\pi}}\right\}=E\left\{\frac{I(X_i\in D_{train})}{\hat{\pi}}\right\}=1$$
and
$$\sqrt{n}\left\{\widehat{Err}^{CV}_m-Err\right\}=\frac{1}{\sqrt{n}}\sum_{i=1}^n \left[\eta(X_i)+l_{\psi_n}\{\xi(X_i)\} \right]+o_p(1)$$
converges weakly to a mean zero Gaussian random distribution with a variance of 
$$\sigma_D^2= E\left(\left[\eta(X)+l_{\psi_0}\{\xi(X)\} \right]^2\right).$$
For a finite sample $n$, we expect that the difference between $Err_m$ and $Err$ may be negligible. Specifically, the distribution of
$$\sqrt{n}\left(\widehat{Err}^{CV}_m-Err_m\right)$$ 
can be approximated by $N(0, \hat{\sigma}_D^2),$ if the following condition holds
\begin{enumerate}
\item[C5] $E \left[\lim_{N\rightarrow}\left\{L(D_N, \widehat{\psi}_n)\right\}\right]-Err=o_p(n^{-1/2}),$ where $\widehat{\psi}_n$ is based on training set of size $n,$ and the expectation is with respect to $\widehat{\psi}_n.$
\end{enumerate}
This assumption is in general true, since $L(D_N, \widehat{\psi}_n)$ often converges to a smooth functional of $\widehat{\psi}_n$ as $N\rightarrow \infty,$ and the expectation of the limit can be approximated by $Err=\lim_{N\rightarrow}\left\{L(D_N, \psi_0)\right\}$ with the approximation error bounded by $O\left\{|E(\widehat{\psi}_n-\psi_0)|\right\},$ which is in the order of $o(n^{-1/2})$ following condition C1. Since $E\left\{\widehat{Err}^{CV}_m\right\}$ is closer to $Err_m$ than $Err$ in finite sample setting, the Gaussian approximation for $\sqrt{n}\left(\widehat{Err}^{CV}_m-Err_m\right)$ is likely to be more accurate. As a consequence, 
an asymptotical confidence interval for $Err_m$ can be constructed as 
$$ \left[\widehat{Err}^{CV}_m-1.96 \frac{\hat{\sigma}_D}{\sqrt{n}},  \widehat{Err}^{CV}_m+1.96 \frac{\hat{\sigma}_D}{\sqrt{n}} \right],$$
where $\hat{\sigma}_D$ is a consistent estimator of the standard error $\sigma_D.$
However, in general it is difficult to obtain such a consistent variance estimator when complex procedures such as lasso regularized regression or random forest are used for constructing the prediction model as in three examples discussed above. An appealing solution is to use the nonparametric bootstrap to estimate $\sigma_D^2$ \cite{davison1997bootstrap, efron1994introduction}. The rational is that, under the same set of assumptions, 
$$\sqrt{n}\left\{\widehat{Err}^{CV*}_m-Err_m\right\}=\frac{1}{\sqrt{n}}\sum_{i=1}^n \left[\eta(X_i)+l_{\psi_0}\{\xi(X_i)\} \right]W_i+o_{P^*}(1),$$
where $\widehat{Err}^{CV*}_m$ is the cross-validated estimator based on bootstrapped data $D^*_n$, $(W_1, \cdots, W_n)\sim Multn\left(n, (1/n, \cdots, 1/n)\right)$, $W_i$ is the number of observation $X_i$ in $D^*_n,$
and $P^*$ is the product probability measure with respect to random data and the independent random weights $(W_1,\cdots, W_n).$ Therefore, conditional on observed data $D_n,$
$$\sqrt{n}\left\{\widehat{Err}^{CV*}_m-\widehat{Err}^{CV}_m\right\}=\frac{1}{\sqrt{n}}\sum_{i=1}^n \left[\eta(X_i)+l_{\psi_0}\{\xi(X_i)\} \right](W_i-1)+o_{P^*}(1),$$
converges weakly to a mean zero Gaussian distribution with a variance of 
$$\hat{\sigma}^2_D=\frac{1}{n}\sum_{i=1}^n\left[\eta(X_i)+l_{\psi_0}\{\xi(X_i)\} \right]^2=\sigma_D^2+o_P(1).$$
Operationally, the following naive bootstrap procedure is expected to generate a consistent variance estimator of $\sigma_D^2$, $\hat{\sigma}_D^2.$
\begin{algorithm}[h!]
  \caption{Naive Bootstrap}\label{NB}
  {small
  \begin{algorithmic}[1]
      \For{$b \gets 1$ to $B_{BOOT}$} 
        \State Sample original data to form a bootstrapped dataset of the size $n$ denoted by $D_b^*;$
        \State Perform cross-validation based on bootstrapped dataset $D_b^*;$
         \For{$k \gets 1$ to $B_{CV}$} 
        \State Randomly split $D^*_b$ into $D^{*(k)}_{b,train}$ of size $m$ and $D^{*(k)}_{b, test}$ of size $n-m;$
        \State Calculate $L\left\{D_{b, test}^{*(k)}, \hat{\psi}(D_{b, train}^{*(k)})\right\}$
      \EndFor
      \State      Calculate the bootstrapped cross-validation estimate 
$$\widehat{Err}_{b,m}^{CV*}=\frac{1}{B_{CV}}\sum_{k=1}^{B_{CV}}L\left\{D_{b, test}^{*(k)}, \hat{\psi}(D_{b, train}^{*(k)})\right\}.$$
      \EndFor
\end{algorithmic}
}
\end{algorithm}

The variance $\sigma_D^2$ can be estimated by $n$ times the empirical variance of $B$ bootstrapped cross-validation estimates $\left\{\widehat{Err}_{1,m}^{CV*}, \cdots, \widehat{Err}_{B_{BOOT},m}^{CV*}\right\}.$  However, there are several concerns in this naive resampling procedure, which may result in poor performance in practice. 
\begin{itemize}
\item The bootstrap procedure samples observations with replacement and results in potential duplicates of the same observation in the bootstrapped dataset. Naively splitting the bootstrapped dataset into training and testing sets may result in overlap between them, which can induce nontrivial optimistic bias in evaluating the model performance. If we apply the naive bootstrap method to analyze the Toy Example described in section \ref{sec:toyexample}, then the empirical average of bootstrapped cross-validation estimates $\widehat{Err}_{b, m}^{CV*}$ was downward biased in comparison with  $Err_m^{CV}$ by 0.80 standard deviation of cross-validation estimates $\widehat{Err}_m^{CV}.$ The practical influence of overlap on the variance estimation is less clear but can be potentially nontrivial.
\item The training set of size $m$ in the bootstrapped dataset $D^*$ contains substantially fewer than $m$ distinct observations, which reduces the ``effective" sample size for training a prediction model and induces a downward bias in evaluating the average model performance. This downward bias may be smaller or greater than the optimism bias induced by the overlap between training and testing sets depending on specific applications, but is undesirable in general.
\item To obtain a cross-validated estimate for each bootstrap sample,  one needs to perform the cross-validation multiple times to reduce the Monte-Carlo variation due to random training/testing divisions, i.e., $B_{CV}$ needs to be sufficiently big such as $\ge 200.$  In addition, the number of bootstraps also can not be too small. The conventional recommendation for estimating variance of a statistic using bootstrap is to set the number of bootstraps $B_{BOOT}\sim 400-1000.$ In such a case, one needs to train and evaluate the prediction model more than $80,000$ times and the corresponding computational burden can be prohibitive for complex training algorithm.  
\end{itemize}

In this paper, we present a modified bootstrap procedure to overcome aforementioned difficulties.  

First, in implementing cross-validation on a bootstrapped dataset, we view bootstrapped data as a weighted samples of the original data, i.e., observation $X_i$ is weighted by a random weight $W_i$, which is the number of this observation selected in this bootstrap iteration. In cross-validation, we first split the original dataset into training and testing sets, $D_n=D_{train}\cup D_{test},$ and bootstrapped training and testing sets denoted by $D^*_{train}$ and $D^*_{test}$, respectively, are then constructed by collecting all observations in $D_{train}$ and $D_{test},$ respectively, but with their respective bootstrap weights. Since $D_{train}$ and $D_{test}$ have no overlap, $D^*_{train}\cap D^*_{test}=\phi$ as well. Therefore, we don't allow that the same observation used in both training and testing. One consequence is that the sample sizes of $D^*_{train}$ and $D^*_{test}$ are not fixed across different bootstrap samples.  But their average sample sizes remain the same as those for $D_{train}$ and $D_{test}.$  

Second, we note that the effective sample size of the training set based on the bootstrapped data can be substantially smaller than $m$. Specifically, the number of distinct observations in $D_{train}^*$ is $0.632m$ on average \cite{efron1997improvements}.  Therefore, it is desirable to increase the number of distinct observations of $D_{train}^*$ by allocating more observations to $D_{train}$, which is used to generate $D_{train}^*$ in a bootstrapped dataset. Ideally, we may want to increase the sample size of $D_{train}$ such that the number of distinct observations used for training is close to $m$ in bootstrapped cross-validation, i.e., $\# D_{train}^*\approx m,$ which requires to increase the sample size in $D_{train}$ from $m$ to $m/0.632.$ On the other hand, the sample size of $D_{test}$ and thus $D_{test}^*$ would be reduced by using a larger training set in the bootstrapped cross-validation and such a large reduction in testing sample size may increase the variance of the resulting estimate. In summary, while we want to increase the sample size in the training set to reduce the bias of estimating the model performance in bootstrapped cross-validation due to the fact that fewer distinct observations are used to train the prediction model, we also want to limit the reduction in the number of testing samples so that the variance of the cross-validation estimate would not be greatly affected by this adjustment in training and testing sample sizes. A compromise is to find a adjusted sample size $m_{adj}$ by minimizing the loss function 
\begin{equation}
\left(\frac{m_{adj}}{0.632m}-1\right)^2+\lambda_0 \left(\frac{n-m}{n-m_{adj}}-1\right)^2,  \label{eq:sizeadj}
\end{equation}
where the first and second terms control the closeness of the ``effective" sample size in the bootstrapped training set to $m$ and the relative change in the sample size of the testing set after the adjustment, respectively. Here, $\lambda_0$ controls the relative importance of theses two tasks in determining the final adjustment. In our limited experience, we have found that the performance of the resulting resampling procedure was not very sensitive to the selection of this penalty parameter within a wide range, and we recommended to set $\lambda_0=1-0.632=0.368$ in practice. %Lastly, The final bootstrap variance estimator can be further adjusted by multiplying a factor $(n-m_{adj}+0.632m_{adj})/n<1$ considering the reduction in the effective sample size in the training set and thus the effective sample size in bootstrapped cross-validation. 

More importantly, to alleviate the computational demand for the bootstrap procedure, we propose the following algorithm:

\begin{algorithm}[h!]
  \caption{Bootstrap Cross-Validation}\label{ag:mainboot}
  {\small
  \begin{algorithmic}[1]
      \For{$b \gets 1$ to $B_{BOOT}$} 
        \State Obtain a bootstrapped dataset $D^*_b$ and frequencies of all observations $\{W_{1b}, \cdots, W_{nb}\}.$
        \State  Calculate $m_{adj}$ by minimizing the loss function in (\ref{eq:sizeadj}) with $\lambda_0=0.368$;
         \For{$k \gets 1$ to $B_{CV}$} 
        \State Split $D_n$ into training and testing sets: $D^{(k)}_{b, train}$ of size $m_{adj}$ and $D^{(k)}_{b, test}$ of size $n-m_{adj};$
        \State Construct the training and testing sets $D_{b,train}^{*(k)}$ and $D_{b, test}^{*(k)}$ by weighing patients in $D^{(k)}_{b, train}$ and $D^{(k)}_{b, test}$ with their bootstrap weights $\left\{W_{ib}, i=1,\cdots, n\right\}.$
        \State Calculate the cross-validation error $ \theta^*_{bk}=L\left\{D^{*(k)}_{b, train}, D^{*(k)}_{b,test}\right\}$
      \EndFor      
\State  Calculate the bootstrapped cross-validation estimate 
$\widehat{Err}_{b,m}^{CV*}=B_{CV}^{-1}\sum_{k=1}^{B_{CV}}\theta^*_{bk}.$
      \EndFor

\State   Fit a random effects model \cite{%clark2015should, 
% pinheiro2006mixed, 
laird1982random} 
$$\theta^*_{bk}=\theta_0+\epsilon^*_{b}+\epsilon_{bk},  b=1, \cdots, B_{BOOT}; k=1, \cdots, B_{CV}$$
where $\epsilon^*_b\sim N(0, \sigma_{BT}^2)$ and $\epsilon_{bk}$ are independent mean zero noise with a variance of $\tau_0^2$.
\State Let
$$\widehat{\sigma}_{BT}^2=\frac{1}{B_{BOOT}-1}\sum_{b=1}^{B_{BOOT}} (\bar{\theta}^*_{b}-\bar{\theta}^*)^2-\frac{1}{B_{CV}(B_{CV}-1)B_{BOOT}}\sum_{b=1}^{B_{BOOT}} \sum_{k=1}^{B_{CV}}(\theta^*_{bk}-\bar{\theta}^*_{b})^2,$$
where 
$\bar{\theta}_b^*=B_{CV}^{-1}\sum_{k=1}^{B_{CV}} \theta^*_{bk}$ and $\bar{\theta}^*=B_{BOOT}^{-1}\sum_{b=1}^{B_{BOOT}} \bar{\theta}^*_b.$ \label{line1:bootest}
\State $\widehat{\sigma}_m^{CV} \gets \widehat{\sigma}_{BT}$ is our bootstrap standard error estimator of cross-validation estimator $\widehat{Err}^{CV}_m$ and the 95\% CI for $Err_m$ is 
$$\left[\widehat{Err}^{CV}_m-1.96\times \widehat{\sigma}_m^{CV},  \widehat{Err}^{CV}_m+1.96\times \widehat{\sigma}_m^{CV} \right].$$ \label{line2:bootest}
\end{algorithmic}
}
\end{algorithm}

The rational is that the center of $\theta^*_{bk}$, $\theta_0+\epsilon_{b}^*,$ is approximately the cross-validation estimate based on the bootstrapped dataset $D^*_b$ as the number of random training and testing  division increasing to infinity. Under this framework, $\sigma_{BT}^2$ measures the between-bootstrap variance, which is the bootstrap variance estimator we aim to calculate,  and $\tau_0^2$ measures the within-bootstrap variance, i.e., the variance due to random training and testing divisions. The empirical variance of $\left\{\bar{\theta}^*_1, \cdots, \bar{\theta}^*_B\right\}$ based on a very big $B_{CV}$ is approximately unbiased in approximating $\sigma_{BT}^2,$ corresponding to the naive bootstrap procedure.  However, this naive approach is very inefficient and there is no need to choose a very large $B_{CV}$ for eliminating the Monte-Carlo variance in estimating cross-validation prediction error for every bootstrapped dataset. Alternatively, an accurate moment estimate for the variance component in the random effects model can be constructed with a moderate $B_{CV}$, say $10-20,$ and a reasonably large $B_{BOOT},$ say 400. This can substantially reduce the computational burden from 80,000 model training to 8,000 model training. 

\paragraph{Remark 2} The total number of model training is $B_{BOOT}\times B_{CV}.$  A natural question is how to efficiently allocate the number of bootstraps and number of cross-validations per bootstrap given the total number of model training.   The variance estimator, $\hat{\sigma}_{BT}^{2},$ is a random statistic itself with a variance \cite{boardman1974confidence, williams1962confidence}, which can be approximated by 
$$  2\left(\frac{\left(\hat{\sigma}_{BT}^2+B_{CV}^{-1}\hat{\tau}_{0}^2\right)^2}{(B_{BOOT}-1)} \right)+2\left(\frac{\left(B_{CV}^{-1}\hat{\tau}_{0}^2 \right)^2}{B_{BOOT}(B_{CV}-1)} \right),$$
where
$$\hat{\tau}_{0}^2= \frac{1}{B_{BOOT}(B_{CV}-1)}\sum_{b=1}^{B_{BOOT}}\sum_{k=1}^{B_{CV}}(\theta^*_{bk}-\bar{\theta}^*_{b})^2$$
is an estimator for $\tau_0^2.$ It is not difficult to show that fixing $B_{BOOT}\times B_{CV}=N_T$, the variance is minimized when 
$$B_{BOOT}\approx \frac{\hat{\sigma}^2_{BT}}{\hat{\tau}^2_{0}}\times N_T$$
and 
$$B_{CV}\approx \frac{\hat{\tau}^2_{0}}{\hat{\sigma}^2_{BT}}.$$
It suggests that the optimal number of cross-validation per bootstrap should be approximately constant, whose value may depend on the specific problem but doesn't change with the budget for the total number of model training. Normally, $\hat{\tau}^2_{0}$ can be substantially greater than $\hat{\sigma}_{BT}^2$ and $B_{CV}$ should be set to be close to their ratio. In the toy example, this ratio is approximately 20. On the other hand, we always can increase the number of bootstraps to improve the precision in approximating the bootstrap variance estimator.  


\paragraph{Remark 3} 
The number of distinct observations used in training the bootstrapped prediction model is smaller than $m_{adj}$. Specifically, the number of distinct observations in bootstrapped training set is on average only $0.632\times m_{adj}.$ Therefore, there is a tendency that the ``effective total sample size" in bootstrap procedure is smaller than $n,$ which may cause a upward bias in estimating the variance of $\widehat{Err}_m^{CV}$ using the bootstrap variance estimator $\hat{\sigma}_{BT}^2.$ To correct this bias, we can consider an adjusted variance estimator
$$ \left(\hat{\sigma}_{m,adj}^{CV}\right)^2=\hat{\sigma}_{BT}^2\left( \frac{n-m_{adj}+0.632m_{adj}}{n}\right)=\hat{\sigma}_{BT}^2\left(\frac{n-0.368m_{adj}}{n}\right),$$
where the factor $(n-0.368m_{adj})/n$ is introduced to account for the reduced sample size in bootstrapped training set. Note that we do not recommend a similar adjustment of the sample size in the testing set, even though that the number of distinct observations in the testing set is also smaller than $(n-m_{adj})$ in bootstrap, because in general this reduction in the number of distinct observations doesn't affect the variance estimation. 

Sometimes, training the prediction model can be very expensive in terms of computation, and it may not be feasible to conduct even the accelerated bootstrap. For example, one may only can train the prediction model 50-100 times.  In such a case, regardless of the selection of $B_{BOOT}$ and $B_{CV}$, the Monte-Carlo error in estimating the bootstrap variance may not be ignorable. Consequentially, 
$$\frac{\sqrt{n}(\widehat{Err}_m^{CV}-Err_m)}{\widehat{\sigma}_m^{CV}}~~~\mbox{ or }~~~ \frac{\sqrt{n}(\widehat{Err}_m^{CV}-Err_m)}{\widehat{\sigma}_{m, adj}^{CV}}$$
may not follow a standard normal distribution. On the other hand, if we can empirically approximate this distribution, then one still can construct a 95\% confidence interval for $Err_m$ based on $(\widehat{Err}_m^{CV}, \widehat{\sigma}_m^{CV}).$ One analogy is that the confidence interval for the population mean of a normal distribution can be constructed using t-distribution rather than the normal distribution in small sample size setting.  With a slight abuse of notations,  let $\sigma_m^{CV}(\infty)$ be the bootstrap variance estimator, if both $B_{BOOT}$ and $B_{CV} \rightarrow \infty$, i.e., the bootstrap variance estimator without any Monte Carlo error, and we have
\begin{equation}\frac{\sqrt{n}(\widehat{Err}_m^{CV}-Err_m)}{\widehat{\sigma}_m^{CV}}=\frac{\sqrt{n}(\widehat{Err}_m^{CV}-Err_m)}{\sigma_m^{CV}(\infty)}\times \frac{\sigma_m^{CV}(\infty)}{\widehat{\sigma}_m^{CV}}. \label{eq:zscoredecomp}\end{equation}
The first term of the left hand side of (\ref{eq:zscoredecomp}) should be approximated well by a standard Gaussian distribution since the ``ideal" bootstrap variance estimator is used. The second term is independent of the first term and reflects the Monte-Carlo variation of approximating $\sigma_m^{CV}(\infty)$ via a small number of bootstrap and cross-validation iterations. To approximate the distribution of this ratio, we can bootstrap the variance estimator based on fitting the random effects model. This observation motivated the following additional steps presented in algorithm \ref{ag:calboot} after line (\ref{line1:bootest}-\ref{line2:bootest}) of algorithm \ref{ag:mainboot}, when very small $B_{BOOT}$ and $B_{CV}$ are used.

\begin{algorithm}[h!]
\caption{Bootstrap Calibration}\label{ag:calboot}
{\small
\begin{algorithmic}[1]
\setcounter{ALG@line}{12}
\For{ $l\gets 1$ to  $L$} 
\State Construct the bootstrapped dataset
$\Theta_l^*=\left\{\theta^{**}_{lbk}, b=1,\cdots, B_{BOOT}; k=1, \cdots, B_{CV}\right\},$
where the vector  $(\theta^{**}_{lb1}, \cdots, \theta^{**}_{lbK})$ is a random sample from $B_{BOOT}$ vectors 
$\{(\theta^*_{b1}, \cdots, \theta^*_{bK}), b=1, \cdots, B_{BOOT}\}.$
\State Let
$$\hat{\sigma}_{l,BT}^{2*}=\frac{1}{B_{BOOT}-1}\sum_{b=1}^{B_{BOOT}} (\bar{\theta}^{**}_{lb}-\bar{\theta}^{**}_l)^2-\frac{1}{B_{CV}(B_{CV}-1)B_{BOOT}}\sum_{b=1}^{B_{BOOT}} \sum_{k=1}^{B_{CV}}(\theta^{**}_{lbk}-\bar{\theta}^{**}_{lb})^2,$$
where 
$\bar{\theta}_{lb}^{**}=B_{CV}^{-1}\sum_{k=1}^{B_{CV}} \theta^{**}_{lbk},~~\mbox{and}~~\bar{\theta}^{**}_l=B_{BOOT}^{-1}\sum_{b=1}^{B_{BOOT}} \bar{\theta}^{**}_{lb}.$
\EndFor
\State Let $Z_l^*=Z_l\frac{\widehat{\sigma}_{BT}}{\widehat{\sigma}_{l, BT}^{*}} , l=1, \cdots, L,$
where $Z_l\sim N(0, 1),$ and use the distribution of $Z_l^*$ to approximating that of  $\sqrt{n}(\widehat{Err}^{CV}_m-Err_m)/\sigma_m^{CV}(\infty) \times \sigma_{m}^{CV}(\infty)/\widehat{\sigma}_m^{CV}.$
\State Find the cut off value $c_{1-\alpha/2}$ such that $\frac{1}{L}\sum_{l=1}^L I\left(|Z_l^*|<c_{1-\alpha/2} \right)=1-\alpha.$
\State The final $(1-\alpha)100$\% CI for $Err_m$ is 
$$\left[\widehat{Err}^{CV}_m-c_{1-\alpha/2}\times \widehat{\sigma}_m^{CV},  \widehat{Err}^{CV}_m+c_{1-\alpha/2}\times \widehat{\sigma}_m^{CV} \right].$$
\end{algorithmic}
}
\end{algorithm}

This resulting confidence interval is expected to be wider than that generated from the algorithm \ref{ag:mainboot}, since $c_{0.975}>1.96.$  However, this is a necessary cost to pay for using a small number of bootstrap and cross validation iterations. Note that although two bootstraps have been used in the modified algorithm, the increase in computational burden is minimal. These two bootstrap steps are not nested and the second bootstrap only involves repeated estimation of the variance component of a simple random effects model, which can be completed relatively fast especially with small or moderate $B_{BOOT}$ and $B_{CV}.$ The performance of this method depends on the normal approximation to the distribution of $(\widehat{Err}_m^{CV}-Err_m)/\sigma_m^{CV}(\infty)$ and the bootstrap approximation to the distribution of $\sigma_m^{CV}(\infty)/\widehat{\sigma}_m^{CV}.$  The second bootstrap is a calibration step for producing a confidence interval of $Err_m$ with a coverage level comparable to that based on $(\widehat{Err}_m^{CV}-Err_m)/\sigma_m^{CV}(\infty)$. If the latter yields a confidence interval which either too conservative or too liberal, then the new confidence interval based on additional bootstrap calibration would suffer the same limitation. Operationally, one may choose for example $(B_{BOOT}, B_{CV})=(20, 25)$ or $(20, 40).$ A slightly bigger value for $B_{CV}$ can prevent a negative or zero variance component estimator in fitting the random effects model.

%\begin{figure}
%\centering
%\includegraphics[width=0.3\textwidth]{frog.jpg}
%\caption{\label{fig:frog}This frog was uploaded via the file-tree menu.}
%\end{figure}

\section{Applications}
\subsection{Application 1}
\subsubsection{Theoretical Properties}
In the first example, we are interested in estimating the mean absolute prediction error from a liner regression model via cross-validation.  In this case, it is not difficult to verify conditions C1-C5 under reasonable assumptions.  For example, under the condition that the matrix $A_0=E(\tilde{Z}\tilde{Z}')$ is non-singular, the least squared estimator of the regression coefficient in the linear regression model, $\hat{\beta},$ converges to a deterministic limit $\beta_0$ as $n \rightarrow \infty,$ and
$$\sqrt{n}(\hat{\beta}-\beta_0)=\frac{1}{\sqrt{n}}\sum_{i=1}^n A_0^{-1}\left(Y_i-\beta_0'\tilde{Z}_i\right)+o_p(1),$$
and thus C1 is satisfied. Second, the class of functions $\{|y-\beta'\tilde{z}| \mid \beta\in \Omega\}$  is Donsker, where $\Omega \subset R^{p+1}$ is a compact set and $p=\mbox{dim}(Z).$  This fact suggests that the empirical process
$$U(\beta)=\sqrt{n}\left[L\left(D_n, \beta\right)-E\left\{|Y-\beta'\tilde{Z}|\right\}\right]$$
is stochastically continuous in $\beta,$ where 
$$L\left(D_n, \beta\right)=\frac{1}{n}\sum_{i=1}^n |Y_i-\beta_0'\tilde{Z}_i|.
$$ 
As a consequence, condition C2 is satisfied.
It is clear that $E\left(|Y-\beta_0'\tilde{Z}_i|\right)$ is differentiable in $\beta$ in a small neighborhood of $\beta_0,$ if the random variable $\beta_0'\tilde{Z}$ has a differentiable density function, which suffices for condition C3.  The central limit theorem implies that
$$\sqrt{n}\left[\frac{1}{n}\sum_{i=1}^n |Y_i-\beta'\tilde{Z}_i|-E\left\{|Y-\beta_0'\tilde{Z}_i|\right\}\right]$$
converges weakly to a mean zero Gaussian distribution as $n \rightarrow \infty$ under the assumption that $E\left\{(Y_i-\beta_0'\tilde{Z}_i)^2\right\}$ is finite.  Lastly, it is obvious that $ E|Y-\hat{\beta}'\tilde{Z}|-E|Y-\beta_0'\tilde{Z}|=O\left\{|E(\hat{\beta})-\beta_0|\right\}=o_p(n^{-1/2})$ and C5 is also satisfied.  Therefore, we expect that $\left(\widehat{Err}_m^{CV}-Err_m\right)$ can be approximated by a mean zero Gaussian distribution whose variance can be consistently estimated by the proposed bootstrap method. Note that we don't need to assume that the linear regression model is indeed correct for the relationship between $Y_i$ and $Z_i.$ 

\subsubsection{Simulation Study}

In numerical study,  we first considered a simple setting, where $Z_i$ followed a 10 dimensional standard multivariate Gaussian distribution and a continuous outcome $Y_i$ was generated via the linear model 
$$Y_i=\beta_0'\tilde{Z}_i+\epsilon_i,$$
where $\beta_0=(0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0)'$ and $\epsilon_i\sim N(0, 1).$ The regression coefficient was selected such that the proportion of the variation explained by the true regression model was 80\%. We let the sample size $n=90$ and considered the cross-validation estimate $\widehat{Err}_m^{CV}$ for the mean absolute prediction error $Err_m, m\in\{40,45,50, 55, 60, 65, 70, 75, 80\}.$ The true value of $Err_m$ was obtained by averaging the empirical mean absolute prediction error of 5,000 estimated prediction models in an independent testing set consisting of 200,000 generated observations. Each prediction model was trained in  a simulated training set of size $m.$ Both training and testing sets were generated according to the linear regression model specified above.  Next, we generated 1,000 independent datasets, $D_n,$ of the size $n.$ For each of dataset, we constructed the cross-validation estimator of $Err_m$.  To this end, we divided the simulated dataset $D_n$ into training and testing sets $B_{CV}=400$ times and calculated the resulting $\widehat{Err}_m^{CV}$ as the average of $B_{CV}$ obtained mean absolute prediction errors in the testing set.  We also implemented the fast bootstrap method to estimate the variance of the cross-validated estimates with $B_{BOOT}=400$ bootstraps and relatively small number of cross-validations for each bootstrap, i.e., $B_{CV}=20$. Thus, constructing one confidence interval required $8,000$ model fitting.  Based on 1,000 simulated datasets, we calculated the empirical mean and standard deviation of $\widehat{Err}_m^{CV}$, and the empirical coverage level of 95\% confidence intervals based on bootstrap variance estimator with and without the sample size adjustment. The results were reported in Table \ref{tab:simulm}.  Next, we examined the performance of bootstrap calibration in algorithm \ref{ag:calboot} for constructing 95\% confidence intervals with a very small number of bootstraps. In particular, we set $(B_{BOOT}, B_{CV})=(20, 25),$ and the results are summarized in terms of empirical coverage probability of constructed confidence intervals with and without the bootstrap calibration. In this setting, the constructing a confidence interval requires only 500 model training, in contrast to 8,000 model training required by the proposed bootstrap procedure and 80,000 model training required by the regular bootstrap procedure. The results can be found in Table \ref{tab:simulmsmall}.

The empirical bias of $\widehat{Err}_m^{CV}$ in estimating $Err_m$ is almost zero,  relative to either the standard deviation of $\widehat{Err}_m^{CV}$ or the true value of $Err_m.$ The empirical coverage level of the 95\% confidence interval based on $B_{BOOT}=400$ bootstraps was fairly close to its nominal level as expected after the sample size adjustment.  The confidence intervals were slightly conservative without the adjustment for the ``effective'' sample size.  Ignoring the difference between $Err_m$ and $Err_n$ for $(m, n)=(80, 90),$  we also examined the empirical coverage level of the constructed confidence interval with respect to $Err(D_n),$ which was the parameter of interest in \cite{bates2021cross}. The empirical coverage level was 92.5\%.  Therefore, in this case, the constructed confidence interval based on bootstrap method not only covered $Err_m$ with sufficient probability as proposed, but also $Err(D_n)$. In this setting, the standard deviation of $\widehat{Err}_m^{CV}$, i.e., $\epsilon$ was 0.073, while the standard deviation of $Err(D_n)$, i.e., $\zeta,$ was much smaller: 0.025.  See Figure \ref{fig:errscatter} for a graphic representation of this phenomena. Therefore, the coverage levels of constructed confidence intervals for $Err_n$ and $Err(D_n)$ are similar.  Lastly, one may construct the confidence interval for $Err(D_n)$ using the nested cross-validation method proposed in \cite{bates2021cross}, since the estimator of mean absolute prediction error in the testing set was a simple average of random statistics. In the simulation, the empirical coverage level of 95\% confidence intervals based on nested cross-validation was 91\% for $Err(D_n)$. It was interesting to note that its coverage level for $Err_{m}, m=80,$ was 91.8\%, quite close to that for $Err(D_n).$   For confidence intervals constructed with a small number of bootstraps ($B_{BOOT}=20)$, the empirical coverage level was substantially lower than that based on $B_{BOOT}=400,$ if no calibration for the confidence interval was made (Table \ref{tab:simulmsmall}). After the additional bootstrap calibration outlined in algorithm \ref{ag:calboot}, however, the empirical coverage level became similar to or higher than that based on a large number of bootstraps. As a price of recovering the proper coverage level, the median width of calibrated confidence intervals increased  11-37\% depending on the training size $m.$

In the second set of simulations,  we let $p=1000$ corresponding to a high dimensional case.  In order to construct a prediction model in this case, we used the lasso regularization \cite{tibshirani1996regression}, i.e., minimizing the loss function
$$ \frac{1}{n}\sum_{i=1}^n (Y_i-\alpha-\beta_Z'Z_i)^2+\lambda_0|\beta_Z|_1, $$
where the penalty parameter $\lambda_0$ is fixed at 0.20 to save computational time. 
Letting the minimizer of the regularized loss be denoted by $\hat{\alpha}$ and $\hat{\beta}_Z$, the outcome of a future patient with covariate $Z$ was predicted by $\hat{\beta}'\tilde{Z},$ where $\hat{\beta}=(\hat{\alpha}, \hat{\beta}_Z')'.$ For each of 1,000 simulated datasets, we computed  $\widehat{Err}_m^{CV}$, its variance estimator $\hat{\sigma}_m^{CV}$ via proposed bootstrap method, and the true prediction error $Err(D_n)$ as in the low-dimensional case.  We reported the true value of $Err_m,$ empirical mean and standard deviation of $\widehat{Err}_m^{CV}$, and the empirical coverage level of the 95\% confidence interval based on bootstrap variance estimate in Table \ref{tab:simulm}. In addition, we examined the performance of the confidence intervals constructed via a small number of bootstraps, i.e., $(B_{BOOT}, B_{CV})=(20, 25).$ The corresponding results were summarized in Table \ref{tab:simulmsmall}. Similar to the low dimensional case, the empirical bias of $\widehat{Err}_m^{CV}$ in estimating $Err_m$ was almost zero and the empirical coverage level of the 95\% confidence interval was slightly higher than the nominal level of 95\%. The over-coverage of confidence intervals based on $\widehat{\sigma}_{m, adj}^{CV}$ was slightly smaller. The empirical coverage level of those confidence intervals was 98.8\% with respect to $Err(D_n).$ Part of the reason of the high coverage level in this setting was the high correlation between $Err(D_n)$ and $\widehat{Err}_m^{CV}$, which was 0.40 (Figure \ref{fig:errscatterhighp}).  In the low dimensional setting, this correlation was almost zero. Similar findings have been reported in \cite{bates2021cross} as well. Lastly, the empirical coverage level of 95\% confidence intervals based on nested cross-validation was 93.9\% for $Err(D_n)$ and 91.2\% for $Err_m,$ where $(n, m)=(90, 80).$   Without the bootstrap calibration, the empirical coverage level of confidence intervals constructed with a small number of bootstrap iterations was lower than those with $B_{BOOT}=400.$ After the bootstrap calibration, the empirical coverage level became similar to or higher than that based on a large number of bootstraps. The median width of calibrated confidence intervals increased 15-23\% depending on the training size $m.$

\begin{figure}
\centering
\includegraphics[width=0.8\textwidth ]{errscatterhighp.pdf}
\caption{$Err(D)$ vs $\widehat{Err}_m^{CV}$ in estimating the mean absolute prediction error of a high dimensional linear regression}
\label{fig:errscatterhighp}
\end{figure}

 The lasso regularized estimator of the regression coefficient clearly does not follow a Gaussian distribution, and the theoretical justification for the Gaussian approximation of the cross-validated estimates provided for low dimensional setting is not applicable here.  However, the empirical distribution of $\widehat{Err}^{CV}_m$ was quite ``Gaussian" with its variance being approximated reasonably well by the proposed bootstrap method.  This phenomena can be visualized by the QQ plot of $\widehat{Err}_m^{CV}$ with the smallest sample size of the training set, $m=40,$ in Figure \ref{fig:errqq}, where the role of lasso regularization was expected to be the biggest.  It is clear that the Gaussian approximation holds well empirically for both low and high dimensional settings.  One possible explanation was that the mean absolute prediction error is a smooth function of the testing data, and thus the cross-validation estimator $\widehat{Err}^{CV}_m$ is still a regular root $n$ estimator of $Err_m$. This observation suggests a broader application of the proposed method for constructing confidence intervals for $Err_m.$

In summary, the proposed confidence interval has a reasonable coverage level but slightly conservative. Empirically, the interval can be viewed as confidence interval for both $Err(D_n)$ and $Err_m$ when $m\approx n,$ although the procedure was designed for the latter. In this case, the confidence interval based on nested cross-validation also had a proper coverage level for both $Err(D_n)$ and $Err_m$, even though the procedure was designed for the former. Furthermore, the bootstrap calibration can be used to maintain proper coverage level of confidence intervals constructed with a very small number of bootstraps at the cost of enlarging the resulting intervals.


\begin{figure}
\centering
\includegraphics[width=0.8\textwidth ]{qqplot.pdf}
\caption{The QQ plot for the empirical distribution of $\widehat{Err}^{CV}_m$ for estimating the mean absolute prediction error}
\label{fig:errqq}
\end{figure}


\begin{table}[ht]{
\centering
\setlength{\tabcolsep}{3pt}
\renewcommand{\arraystretch}{0.5}
\begin{tabular}{ c ccccc c ccccc }
\hline
$m$ & \multicolumn{5}{c}{$p = 10$} & & \multicolumn{5}{c}{$p = 1000$} \\
    & $Err_m$ & $E(\widehat{Err}_m^{CV})$ & SD  & Cov-adj & Cov && $Err_m$ & $E(\hat{Err}_m^{CV})$ & SD  & Cov-adj & Cov\\
    \cline{2-6} \cline{8-12}\\
40  & 0.941 & 0.938 & 0.077 & 96.7\% & 98.0\% && 1.441 & 1.445 & 0.123 & 97.3\% & 98.9\%\\
45  & 0.920 & 0.918 & 0.076 & 96.9\% & 98.1\% && 1.334 & 1.341 & 0.126 & 97.0\% & 98.9\% \\
50  & 0.906 & 0.904 & 0.075 & 96.5\% & 98.2\% && 1.248 & 1.255 & 0.121 & 97.4\% & 99.2\% \\
55  & 0.894 & 0.892 & 0.074 & 96.4\% & 98.1\% && 1.179 & 1.187 & 0.115 & 97.6\% & 99.4\% \\
60  & 0.885 & 0.883 & 0.074 & 96.0\% & 98.1\% && 1.128 & 1.134 & 0.109 & 98.2\% & 99.4\% \\
65  & 0.877 & 0.875 & 0.074 & 95.5\% & 98.2\% && 1.087 & 1.093 & 0.103 & 98.6\% & 99.3\%\\
70  & 0.870 & 0.869 & 0.073 & 94.9\% & 98.0\% && 1.058 & 1.060 & 0.100 & 98.4\% & 99.4\% \\
75  & 0.865 & 0.864 & 0.073 & 94.6\% & 97.9\% && 1.031 & 1.034 & 0.097 & 98.5\% & 99.5\% \\
80  & 0.861 & 0.859 & 0.073 & 93.3\% & 97.7\% && 1.012 & 1.013 & 0.097 & 97.9\% & 99.4\%
\end{tabular}
\caption{Simulation Results for estimating the mean absolute prediction error.  $Err_m$, the true mean absolute prediction error; $E(\widehat{Err}_m^{CV}),$ the empirical average of the cross-validation estimate $\widehat{Err}_m^{CV}$; SD, the empirical standard deviation of the cross-validation estimate $\widehat{Err}_m^{CV}$  ; COV-adj, the empirical coverage level of 95\% confidence intervals based on $\hat{\sigma}_{m, adj}^{CV}$ from bootstrap; COV, the empirical coverage level of 95\% confidence intervals based on $\hat{\sigma}_{m}^{CV}$ from bootstrap.} \label{tab:simulm}}
\end{table} 


\begin{table}[ht]{
\centering
\setlength{\tabcolsep}{3pt}
\renewcommand{\arraystretch}{0.5}
\begin{tabular}{ cccccccc }
\hline\\
$m$   & \multicolumn{3}{c}{Confidence Intervals Based on $\widehat{\sigma}_{m,adj}^{CV}$} &&  \multicolumn{3}{c}{Confidence Intervals based on $\widehat{\sigma}_m^{CV}$}\\
    & Algorithm \ref{ag:mainboot} & Algorithm \ref{ag:mainboot} & Algorithm \ref{ag:calboot} && Algorithm \ref{ag:mainboot} & Algorithm \ref{ag:mainboot} & Algorithm \ref{ag:calboot}\\  
    & $B_{BOOT}=400$  & $B_{BOOT}=20$  & $B_{BOOT}=20$  && $B_{BOOT}=400$  & $B_{BOOT}=20$ & $B_{BOOT}=20$\\
        \cline{2-4} \cline{6-8}\\
        & \multicolumn{7}{c}{$p=10$}\\
        \cline{2-8}\\
40  & 96.7\% & 95.1\% & 96.8\% && 98.0\% & 97.2\% & 98.4\%\\
45  & 96.9\% & 95.2\% & 96.9\% && 98.1\% & 97.4\% & 98.7\%\\
50  & 96.5\% & 94.6\% & 96.8\% && 98.2\% & 97.1\% & 98.8\%\\
55  & 96.4\% & 94.4\% & 96.7\% && 98.1\% & 97.2\% & 98.9\%\\
60  & 96.0\% & 93.2\% & 96.9\% && 98.1\% & 96.9\% & 98.7\%\\
65  & 95.5\% & 93.1\% & 96.7\% && 98.2\% & 96.7\% & 98.9\%\\
70  & 94.9\% & 93.1\% & 97.3\% && 98.0\% & 96.7\% & 99.1\%\\
75  & 94.6\% & 91.6\% & 97.7\% && 97.9\% & 95.9\% & 98.8\%\\
80  & 93.3\% & 89.8\% & 98.4\% && 97.7\% & 93.4\% & 99.1\%\\
   & \multicolumn{7}{c}{$p = 1000$} \\
   \cline{2-8}\\
40  &  97.3\% & 95.9\% & 98.6\% && 98.9\% & 97.5\% & 99.3\%\\
45  &  97.0\% & 95.3\% & 98.4\% && 98.9\% & 97.4\% & 99.3\%\\
50  &  97.4\% & 95.6\% & 98.5\% && 99.2\% & 98.0\% & 99.4\%\\
55  &  97.6\% & 96.4\% & 98.9\% && 99.4\% & 98.6\% & 99.7\%\\
60  &  98.2\% & 97.2\% & 99.2\% && 99.4\% & 98.6\% & 99.7\%\\
65  &  98.6\% & 97.3\% & 99.4\% && 99.3\% & 98.5\% & 99.9\%\\
70  &  98.4\% & 97.5\% & 99.5\% && 99.4\% & 98.7\% & 99.8\%\\
75  &  98.5\% & 96.5\% & 99.4\% && 99.5\% & 98.6\% & 99.8\%\\
80  &  97.9\% & 96.1\% & 99.4\% && 99.4\% & 98.4\% & 99.7\%
\end{tabular}
\caption{Empirical coverage levels of 95\% confidence intervals of the mean absolute prediction error using different numbers of bootstrap iterations.} \label{tab:simulmsmall}}
\end{table} 

\subsubsection{Real Data Examples}

The example is from the UCI machine learning repository.  The dataset contains per capita violent crime rate and 99 prediction features for their plausible connection to crime in 1,994 communities after dropping communities and features with missing data.  The crimes rate was calculated as the number of violated crimes per 100,000 residents in the community of interest. The violent crimes included murder, rape, robbery, and assault.  The crime rate ranged from 0 to 1.0 with a median of 0.15. The predictors in the analysis mainly involved the community of interest, such as the percent of the urban population, the median family income, and law enforcement, such as per capita number of police officers, and percent of officers assigned to drug units. The objective of the analysis was to use these 99 community features to predict the violent crime rate.  For demonstration purpose, we considered a subset consisting of first 600 observations as the observed dataset.  First, we constructed the prediction model by fitting a linear regression model with lasso regularization, where the penalty parameter $\lambda$ was fixed at 0.005 for convenience. We applied our method for $m\in \{60, 120, 180, 240, 300, 360, 420, 480, 540\},$ i.e., the proportion of the observations used for training varied from 10\%, to 20\%, 30\%, $\cdots$, 90\%.  Based on 500 cross-validations, the cross-validation estimate of the mean absolute prediction error for different training sizes was 0.141, 0.121, 0.115, 0.113, 0.111, 0.110, 0.109, 0.109 and 0.108, respectively.  The 95\% confidence interval for $Err_m^{CV}$ was then constructed based on the proposed bootstrap method. The number of bootstraps was 500 and the number of cross validations per bootstrap was 20.  The results are summarized in Table \ref{tab:example1}.  We also constructed the confidence interval for $Err(D_n)$ using nested cross-validation.  The resulting 95\% confidence interval was [0.100, 0.116], which was fairly close to the bootstrap-based confidence interval for $Err_{540}^{CV}$ where the training size $m$ was the closest to $n=600.$ 


\begin{table}[ht]{
\centering
\setlength{\tabcolsep}{3pt}
\renewcommand{\arraystretch}{0.5}
\begin{tabular}{ ccc c cc c rr}
$m$   &  MAPE$^1$     &  95\% CI    && MAPE & 95\% CI &&  $\Delta$ MAPE & 95\% CI \\
\hline\\
    & \multicolumn{2}{c}{Lasso} && \multicolumn{2}{c}{Random Forest} && \multicolumn{2}{c}{Difference$^2$ $(\times 10^{-2})$}\\
    \cline{2-3} \cline{5-6} \cline{8-9}\\
60  &  0.141  & [0.128, 0.154] && 0.121 & [0.110, 0.132] && 2.13  & [1.50, 2.76]\\
120 &  0.121  & [0.109, 0.133] && 0.116 & [0.105, 0.126] && 0.58  & [0.08, 1.09]\\
180 &  0.115  & [0.105, 0.126] && 0.113 & [0.103, 0.124] && 0.15  & [-0.34, 0.64]\\
240 &  0.113  & [0.102, 0.123] && 0.112 & [0.101, 0.122] && 0.01  & [-0.46, 0.48]\\
300 &  0.111  & [0.101, 0.121] && 0.111 & [0.101, 0.122] && -0.05 & [-0.51, 0.40]\\
360 &  0.110  & [0.100, 0.120] && 0.110 & [0.100, 0.121] && -0.06 & [-0.52, 0.41]\\
420 &  0.109  & [0.100, 0.119] && 0.109 & [0.099, 0.120] && -0.05 & [-0.52, 0.41]\\
480 &  0.109  & [0.100, 0.118] && 0.109 & [0.098, 0.119] && -0.03 & [-0.50, 0.44]\\
540 &  0.108  & [0.099, 0.117] && 0.108 & [0.098, 0.118] &&  0.00 & [-0.48, 0.48]\\
\hline
\multicolumn{9}{l}{$^1:$ mean absolute prediction error (MAPE). }\\
\multicolumn{9}{l}{$^2:$ the difference in MAPE between the lasso regularized linear model and random forest. }
\end{tabular} 
\caption{Results of estimating the mean absolute prediction error (MAPE) in predicting the crime rate based on community crime rate data from UCI repository}\label{tab:example1}}
\end{table} 

We have also considered a different prediction model trained using random forest.   The output the random forest algorithm was an ensemble of 200 regression trees constructed based on bootstrapped samples. Based on 500 cross-validations, the cross-validation estimator of the mean absolute prediction error for different training sizes was 0.121, 0.116, 0.113, 0.112, 0.111, 0.110, 0.109, 0.109, and 0.108, respectively, smaller than those from lasso regularized linear regression model in general. The 95\% confidence intervals using the proposed bootstrap method were reported in Table \ref{tab:example1}.  Furthermore, the confidence interval for $Err(D_{600})$ based on nested cross-validation was [0.100, 0.115], which was also fairly close to the bootstrap-based confidence interval for $Err_{540}^{CV}$ as expected.  

It appears that when sample size of the training data is small, the random forest generates more accurate predictions than the lasso-regularized linear model. When the sample size $m$ of the training set is greater than 200, however, the differences in prediction performance become very small. To account for the uncertainty, we also used the proposed bootstrap method to construct the 95\% confidence interval for the difference in mean absolute prediction error between lasso regularized linear model and random forest.  Indeed, when $m=60,$ the 95\% confidence interval for the difference in $Err_m^{CV}$ was [1.50, 2.76] suggesting a statistically significantly better prediction performance of the random forest. The superiority of random forest also holds for $m=120.$ The results for other $m$ were reported in Table \ref{tab:example1} showing no statistically significant difference between two prediction models.




\subsection{Application 2}
\subsubsection{Theoretical Properties}
In the second application, we are interested in estimating the c-index from a logistic regression model via cross-validation.  In this case, it is not difficult to verify the conditions C1-C5 under conventional assumptions.  For example, under the condition that there is no $\beta$ such that the hyperplane $\beta'z=a_0$ can perfectly separate observations with $Y_i=1$ from those with $Y_i=0$ and the matrix 
$$A_0=E\left[\tilde{Z}'\tilde{Z} \frac{\exp(\beta'\tilde{Z})}{\left\{1+\exp(\beta'\tilde{Z})\right\}^2}  \right]$$
is positive definite for all $\beta$, the maximum likelihood estimator based on the logistic regression, $\hat{\beta},$ converges to a deterministic limit $\beta_0$ in probability as $n \rightarrow \infty$ and
$$\sqrt{n}(\hat{\beta}-\beta_0)=\frac{1}{\sqrt{n}}\sum_{i=1}^n A_0^{-1}\left(Y_i-\frac{\exp(\beta_0'\tilde{Z}_i)}{1+\exp(\beta_0'\tilde{Z}_i)}\right)+o_p(1),$$
and thus $C_1$ is satisfied \cite{tian2007model}.
Second, the class of functions $\{I(\beta'\tilde{z}<0)\mid \beta\in \Omega\}$  is Donsker, where $\Omega$ is a compact set in $R^{p+1}$.  This fact suggests that the U-process
$$U(\beta)=\sqrt{n}\left[L\left(D, \beta\right)-P\left(\beta'(\tilde{Z}_1-\tilde{Z}_2)<0\mid Y_1=0, Y_2=1\right)\right]$$
is stochastically continuous, where 
$$L\left(D, \beta\right)=\frac{1}{m_0m_1}\sum_{Y_i=0}\sum_{Y_j=1}I(\beta'\tilde{Z}_i<\beta'\tilde{Z}_j),
$$ 
$m_g=\sum_{i=1}^n I(Y_i=g).$ As a consequence, condition C2 is satisfied as $n \rightarrow \infty$ and $0<P(Y=1)<1.$ 
It is clear that $P\left(\beta'(\tilde{Z}_1-\tilde{Z}_2)<0\mid Y_1=0, Y_2=1\right)$ is differentiable in $\beta$ in a small neighborhood of $\beta_0,$ if $\beta_0'\tilde{Z}$ has a differentiable density function, which suffices for condition C3.  Next, the central limit theorem for U-statistics implies that
$$\sqrt{n}\left[L\left(D, \beta_0\right)-P\left(\beta_0'(\tilde{Z}_1-\tilde{Z}_2)<0\mid Y_1=0, Y_2=1\right)\right]$$
converges weakly to a mean zero Gaussian distribution as $n \rightarrow \infty,$ if $0<P(Y=1)<1.$ Lastly, $E\left\{P\left(\hat{\beta}'(\tilde{Z}_1-\tilde{Z}_2)<0\mid Y_1=0, Y_2=1\right)\right\}-P\left(\beta_0'(\tilde{Z}_1-\tilde{Z}_2)<0\mid Y_1=0, Y_2=1\right)=O(|E(\hat{\beta})-\beta_0|)=o_p(n^{-1/2})$ and C5 is satisfied. Therefore, we expect that $\left(\widehat{Err}_m^{CV}-Err_m\right)$ can be approximated by a mean zero Gaussian distribution whose variance can be consistently estimated by the proposed bootstrap method. Note that we don't need to assume that the logistic regression model is indeed correct for the conditional probability $P(Y=1|Z).$

\subsubsection{Simulation Study}

In the numerical study,  we considered two settings corresponding to low and high dimensional covariates vector  $Z_i.$ In the first setting, $Z_i$ followed a 10 dimensional standard multivariate normal distribution and the binary outcome $Y_i$ followed a Bernoulli distribution 
$$P(Y_i=1|Z_i)=\frac{\exp(\beta_0'\tilde{Z}_i)}{1+\exp(\beta_0'\tilde{Z}_i)},$$
where $\beta_0=(0, 1.16, 1.16, 1.16, 1.16, 0, \cdots, 0)'.$ This regression coefficient was selected such that the mis-classification error of the optimal Bayesian classification rule was approximately 20\%. We let the sample size $n=90$ and considered the cross-validation estimator $\widehat{Err}_m^{CV}$ for the c-index $Err_m, m\in\{40, 45, 50, 55, 60, 65, 70, 75, 80\}.$ The true value of $Err_m$ was obtained by averaging c-indexes of 5,000 logistic regression models trained in different training sets of size $m$ in an independent testing set consisting of 200,000 observations. We also constructed the cross-validation estimator of $Err_m$ from 1,000 simulated datasets $D_n$ of size $n=90$ each.  For each generated dataset, we divided the dataset into training and testing sets 400 times and calculated the resulting $\widehat{Err}_m^{CV}$ as the average of 400 c-indexes from testing sets.  We first implemented the fast bootstrap method to estimate the variance of the cross-validated estimates with $B_{BOOT}=400$ bootstraps and $B_{CV}=20$ cross-validations per bootstrap. The 95\% confidence interval for $Err_m$ was constructed accordingly.  Based on results from 1,000 datasets, we summarized the empirical average and standard deviation of $\widehat{Err}_m^{CV}$ for c-index and the empirical coverage level of 95\% confidence intervals based on bootstrap variance estimates. The results were reported in Table \ref{tab:simulogistic}. We also examined the performance of the bootstrap calibration in algorithm 2 for constructing 95\% confidence intervals with a very small number of bootstraps. To this end, we set $(B_{BOOT}, B_{CV})=(20, 50),$ and the corresponding results were summarized in Table \ref{tab:simulogisticsmall}. The cross-validation estimator $\widehat{Err}_m^{CV}$ was almost unbiased in estimating $Err_m$ with its empirical bias negligible in comparison with the standard deviation of $\widehat{Err}_m^{CV}.$ The empirical coverage level of 95\% confidence intervals with $B_{BOOT}=400$ was fairly close to the nominal level. The coverage levels of the confidence intervals based on $\widehat{\sigma}_{m, adj}^{CV}$ were approximately 90\%, lower than 95\%, for $m=75, 80.$ We also examined the empirical coverage level of confidence intervals based on $\widehat{\sigma}_{m, adj}^{CV}$ with respect to $Err(D_n)$ for $m=80.$ The coverage level was 89.5\%, similar to that for $Err_m.$  When a small number of bootstraps was used, the coverage level of the confidence interval was markedly lower than those constructed via a large number of bootstraps.  However, with the proposed bootstrap calibration, the coverage level became comparable to those using $B_{BOOT}=400$ bootstraps. For example, when $m=80,$ the empirical coverage level of confidence intervals based on $\widehat{\sigma}_m^{CV}$ was 94.2\% for $B_{BOOT}=400,$  90.5\% for $B_{BOOT}=20$ without calibration, but 98.2\% for $B_{BOOT}=20$ after calibration. The median width the calibrated confidence intervals increased $11 - 40\%$ depending on $m$. In this particular setting, we have also examined the choice of $B_{CV}=25$ as in other simulation studies but observed a nontrivial proportion of zero variance component estimator for large $m$, suggesting insufficient number of bootstraps and cross-validation for differentiating intrinsic bootstrap variance from the Monte-Carlo variance due to cross-validation.


In the second set of numerical experiments, we have considered a high dimensional case where the dimension of $Z$, $p$ is set at 1000. To estimate $\beta_0$, we have employed the lasso-regularized logistic regression analysis, where $\hat{\beta}$ was the maximizer of 
$$ \frac{1}{n}\sum_{i=1}^n \left[ (\alpha+\beta_Z'Z_i) Y_i-\log\left\{1+\exp(\alpha+\beta_Z'Z_i)  \right\}\right]-\lambda_0|\beta_Z|_1.$$
 To save computational time, the penalty parameter $\lambda_0$ was fixed at 0.10 in this simulation study. In this case, the lasso-regularized estimator $\hat{\beta}$ was not a root $n$ ``regular'' estimator and its distribution could not be approximated well by a Gaussian. Therefore, the regularity condition C1 was not satisfied. However, the estimation of the c-index in the testing set was a very ``smooth'' step based on U-statistic and therefore, we still expected that the cross-validation estimator of the c-index to approximately follow a Gaussian distribution, whose variance could be estimated well by the proposed bootstrap method. Letting the maximizer of the regularized objective function be denoted by $\hat{\alpha}$ and $\hat{\beta}_Z$, the c-index in the testing set is estimated by the concordance rate between $Y_i<Y_j$ and $\hat{\beta}_Z'Z_i<\hat{\beta}_Z'Z_j.$
For each of 1,000 simulated datasets, we computed  $\widehat{Err}_m^{CV}$ for c-index, its variance estimator $\hat{\sigma}_m^{CV}$ via bootstrap, and the true prediction error $Err(D_n).$ We calculated and reported the true value of $Err_m,$ empirical mean and standard deviation of $\widehat{Err}_m^{CV}$, and the empirical coverage level of 95\% confidence intervals based on the proposed bootstrap variance estimator in Table \ref{tab:simulogistic}.  Similar to the low-dimensional case, Table \ref{tab:simulogisticsmall} summarized the empirical coverage level of confidence intervals constructed only using a very small number of bootstraps. Similar to the low-dimensional case, the empirical bias of $\widehat{Err}_m^{CV}$ in estimating $Err_m$ was almost zero and the empirical coverage level of the 95\% confidence interval was very close to the nominal level of 95\%. There was a moderate over-coverage of confidence intervals based on bootstrap variance estimates $\widehat{\sigma}_m^{CV}.$  On the other hand, the empirical coverage level of those confidence intervals of $Err_m, m=80$ with respect to $Err(D_n)$ was 96.1\%. The empirical correlation coefficient between $Err(D_n)$ and $\widehat{\sigma}_m^{CV}$ was as high as 0.52 in this setting. When only a small number of bootstraps was used,  the proposed bootstrap calibration recovered the coverage level to a comparable level of those using a large number of bootstraps. For example, when $m=70,$ the empirical coverage level of the confidence interval based on $\widehat{\sigma}_{m, adj}^{CV}$ was 93.9\% for $B_{BOOT}=400,$  91.9\% for $B_{BOOT}=20$ before the bootstrap calibration, and 94.3\% for the bootstrap $B_{BOOT}=20$ after calibration. The median width the calibrated confidence interval increased 10-14\% depending on $m$.


In summary, the proposed confidence intervals have a reasonable coverage. The interval can be viewed as confidence interval for both $Err(D_n)$ and $Err_m$ when $m\approx n,$ although the procedure was designed for the latter. In this case, the nested cross-validation is not directly applicable for c-index or ROC curve, since the parameter estimator in the test set doesn't take a form of sum of independent, identically distributed random elements. Lastly, the bootstrap calibration can be used to effectively account for the variability of bootstrap variance estimator due to small number of bootstrap iterations in constructing confidence intervals for $Err_m.$ 

%\newpage

%\begin{figure}
%\includegraphics[width=3 in ]{images/lasso_p_10.png}
%\includegraphics[width=3 in ]{images/lasso_p_1000.png}
%\includegraphics[width=3 in ]{images/ols_p_10.png}
%\includegraphics[width=3 in ]{images/ols_p_1000.png}
%\end{figure}


\begin{table}[ht]{
\centering
\setlength{\tabcolsep}{3pt}
\renewcommand{\arraystretch}{0.5}
\begin{tabular}{ c ccccc c ccccc }
\hline
$m$ & \multicolumn{5}{c}{$p = 10$} & & \multicolumn{5}{c}{$p = 1000$} \\
    & $Err_m$ & $E(\widehat{Err}_m^{CV})$ & SD   & COV-adj & COV && $Err_m$ & $E(\hat{Err}_m^{CV})$ & SD   & COV-adj & COV\\
    \cline{2-6} \cline{8-12}\\
40  & 0.799 & 0.800 & 0.041 & 94.7\% & 96.6\% && 0.573 & 0.570 & 0.049 & 97.3\% & 98.9\%\\
45  & 0.810 & 0.812 & 0.041 & 94.6\% & 96.4\% && 0.588 & 0.585 & 0.059 & 96.2\% & 98.6\%\\
50  & 0.820 & 0.822 & 0.042 & 94.3\% & 96.4\% && 0.607 & 0.602 & 0.067 & 94.3\% & 98.2\%\\
55  & 0.827 & 0.829 & 0.042 & 93.8\% & 96.0\% && 0.623 & 0.618 & 0.075 & 92.7\% & 97.9\% \\
60  & 0.833 & 0.835 & 0.043 & 93.3\% & 95.4\% && 0.641 & 0.635 & 0.083 & 92.4\% & 97.8\% \\
65  & 0.838 & 0.839 & 0.043 & 92.9\% & 95.3\% && 0.659 & 0.652 & 0.089 & 91.7\% & 97.5\% \\
70  & 0.842 & 0.843 & 0.043 & 92.0\% & 94.9\% && 0.674 & 0.669 & 0.092 & 93.8\% & 98.2\% \\
75  & 0.845 & 0.847 & 0.043 & 91.0\% & 94.3\% && 0.691 & 0.685 & 0.098 & 93.9\% & 98.4\% \\
80  & 0.847 & 0.849 & 0.043 & 89.5\% & 94.1\% && 0.706 & 0.702 & 0.105 & 94.8\% & 98.8\%
\end{tabular}
\caption{Simulation Results for estimating the AUC under the ROC curve in predicting binary outcomes, i.e., the c-index. $Err_m$, the true c-index value; $E(\widehat{Err}_m^{CV}),$ the empirical average of the cross-validation estimate $\widehat{Err}_m^{CV}$; SD, the empirical standard deviation of the cross-validation estimate $\widehat{Err}_m^{CV}$  ; COV-adj, the empirical coverage level of 95\% confidence intervals based on $\hat{\sigma}_{m, adj}^{CV}$ from bootstrap; COV, the empirical coverage level of 95\% confidence intervals based on $\hat{\sigma}_{m}^{CV}$ from bootstrap.}  \label{tab:simulogistic}}
\end{table} 


\begin{table}[ht]{
\centering
\setlength{\tabcolsep}{3pt}
\renewcommand{\arraystretch}{0.5}
\begin{tabular}{ cccccccc }
\hline\\
$m$   & \multicolumn{3}{c}{Confidence Intervals Based on $\widehat{\sigma}_{m,adj}^{CV}$} &&  \multicolumn{3}{c}{Confidence Intervals based on $\widehat{\sigma}_m^{CV}$}\\
    & Algorithm \ref{ag:mainboot} & Algorithm \ref{ag:mainboot} & Algorithm \ref{ag:calboot} && Algorithm \ref{ag:mainboot} & Algorithm \ref{ag:mainboot} & Algorithm \ref{ag:calboot}\\  
    & $B_{BOOT}=400$  & $B_{BOOT}=20$  & $B_{BOOT}=20$  && $B_{BOOT}=400$  & $B_{BOOT}=20$ & $B_{BOOT}=20$\\
        \cline{2-4} \cline{6-8}\\
    &\multicolumn{7}{c}{$p=10$}\\
    \cline{2-8}\\
40  & 94.7\% & 93.7\% & 95.4\% && 96.6\% & 95.5\% & 96.9\%\\
45  & 94.6\% & 93.0\% & 94.9\% && 96.8\% & 95.1\% & 97.2\%\\
50  & 94.3\% & 92.3\% & 94.9\% && 96.5\% & 95.2\% & 97.0\%\\
55  & 94.2\% & 91.6\% & 94.3\% && 96.0\% & 95.0\% & 96.5\%\\
60  & 93.4\% & 91.0\% & 94.0\% && 95.4\% & 94.3\% & 96.8\%\\
65  & 93.1\% & 90.8\% & 94.5\% && 95.4\% & 94.2\% & 96.4\%\\
70  & 92.1\% & 90.2\% & 94.7\% && 94.9\% & 93.4\% & 96.5\%\\
75  & 91.1\% & 89.2\% & 95.7\% && 94.3\% & 93.0\% & 97.7\%\\
80  & 89.3\% & 85.2\% & 97.5\% && 94.2\% & 90.5\% & 98.2\%\\
    & \multicolumn{7}{c}{$p = 1000$} \\
   \cline{2-8}\\
40  &  97.3\% & 95.3\% & 98.7\% && 98.9\% & 97.4\% & 99.3\%\\
45  &  96.2\% & 93.0\% & 97.3\% && 98.6\% & 96.2\% & 98.5\%\\
50  &  94.3\% & 92.2\% & 95.4\% && 98.2\% & 95.7\% & 98.2\%\\
55  &  92.7\% & 91.0\% & 94.9\% && 97.9\% & 95.2\% & 97.8\%\\
60  &  92.4\% & 91.5\% & 94.5\% && 97.8\% & 95.0\% & 97.2\%\\
65  &  91.7\% & 91.3\% & 93.9\% && 97.5\% & 94.9\% & 97.4\%\\
70  &  93.8\% & 91.9\% & 94.2\% && 98.2\% & 95.6\% & 97.5\%\\
75  &  93.9\% & 92.9\% & 94.6\% && 98.4\% & 96.3\% & 98.0\%\\
80  &  94.8\% & 92.8\% & 96.7\% && 98.8\% & 96.0\% & 99.0\%
\end{tabular}
\caption{Empirical coverage levels of 95\% confidence intervals of the AUC under the ROC curve (c-index) using various numbers of bootstrap iterations.} \label{tab:simulogisticsmall}}
\end{table} 

\subsubsection{Real Data Examples}
In the first example, we tested our proposed method on the MI dataset from UCI machine learning repository. The dataset contained 1700 patients and up to 111 predictors collected from the hospital admission up to 3 days after admission for each patient. We were interested in predicting all cause mortality. After removing features with more than 300 missing values, there were 100 prediction features available at the day 3 after admission including 91 features available at the admission. The observed data consisted of 652 patients with complete information on all 100 features.  There were 72 binary, 21 ordinal, and 7 continuous features. Out of 652 patients, there were 62 deaths corresponding to a cumulative mortality of 9.5\%. We considered training size $m\in \{196, 261, 326, 391, 456, 522, 587\},$ which represented 30\% to 90\% of the total sample size.  We considered four prediction models, all trained by fitting a lasso regularized logistic regression.  Model 1 was based on 91 features collected at the time of admission; Model 2 was based on 100 features collected up to day 3 after hospital admission; Model 3 was based on 126 features collected at the time of admission after converting all ordinal features into multiple binary features;  and Model 4 was based on 159 features collected up to day 3 after converting all ordinal features into multiple binary features. For simplicity, we fixed the lasso penalty parameter at 0.01 for fitting Model 1 and Model 3, and 0.0075 for fitting Model 2 and Model 4.

First, we estimated the cross-validated c-index, which was the AUC under the ROC curve based on 500 random cross-validations. We then constructed the 95\% confidence interval based on standard error estimated via the proposed bootstrap method. The number of the bootstraps and the number of cross validations per bootstrap were set at 400 and 20, respectively. The results were reported in Table \ref{tab:example2a1}.  Model 1 and Model 2 had similar predictive performance with a small gain in c-index by including 8 additional features collected after hospital admission. Likewise, Model 3 and Model 4 had similar predictive performance, which, however, was inferior to that of Models 1 and 2, suggesting that converting ordinal predictive features into multiple binary features may had a negative impact on the prediction performance of the regression model.  On the one hand, converting ordinal features into binary features allowed more flexible model fitting. On the other hand, this practice increased the number of features and ignored the intrinsic order across different levels of an ordinal variable. Therefore, it was not a surprise that Models 3 and 4 were not as accurate as Models 1 and 2. We then formally compared the model performance by constructing the 95\% confidence interval for the difference in c-index between Model 1 and Model 2; between Model 1 and Model 3; and between Model 2 and Model 4. The detailed comparison results for all training sizes were reported in Table \ref{tab:example2a2}. It was interesting to note that all confidence intervals include zero, suggesting that none of the observed differences in c-index between different models are statistically significant at the 0.05 level.      

In the second example for binary outcome, we tested our proposal on the red wine data set studied in \cite{cortez2009using}. The data set contained measurements for 1,599 red wine samples and was also available in the UCI Repository. Each of the wine samples was evaluated by wine experts for its quality, which was summarized
on a scale from 0 to 10, with 0 and 10 representing the poorest and highest
quality, respectively. 11 features including fixed acidity, volatile
acidity, citric acid, residual sugar, chlorides, free sulfur dioxide, total sulfur dioxide, density, pH, sulphates and alcohol were also measured for all wine samples. \cite{cortez2009using} compared different data mining methods aiming
to predict the ordinal quality score using these eleven features. Here, we conducted a simpler analysis to identify wine samples with a quality score above 6. To this end, we coded a binary outcome $Y = 1$, if the quality was $\ge 7$ and 0, otherwise.  We selected an observed data set consisting of first 400 wines samples from the ``red wine'' data set.  Although the sample size was not small relative to the number of predictive features, there were only 40 observations with $Y=1$ in this subset.  For the cross-validation, the training size $m\in \{200, 240, 280, 320, 360\}.$ We considered two prediction models: Model 1 was based on a regular logistic regression, and Model 2 was based on a random forest with 200 classification and regression trees.  We estimated the c-index based on 500 random cross-validations for each training size $m$ based on the ``observed'' data set including $n=400$ wine samples. We then constructed 95\% confidence interval for the c-index using the proposed bootstrap method with $(B_{BOOT}, B_{CV})=(400, 20).$ The results were reported in Table \ref{tab:example2b}. It was clear that the random forest generated a substantially better prediction model than the logistic regression across all training sizes considered. The confidence intervals of the the difference in c-index were above zero when $m=320$ and 360, suggesting that the random forest was statistically significantly more accurate than the logistic regression model. It was also interesting to note that the performance of the random forest became better with increased training size, while the c-index of the logistic regression was relatively insensitive to $m.$  This was anticipated considering the fact that the random forest fitted a much more complex model than the simple logistic regression and could make a better use of information provided by more training data for improving the prediction accuracy at local regions of the covariate space. 

We then used the cross-validation to estimate the entire ROC curve based on the random forest with 200 regression and classification trees. Due to the small number of cases in the data set, we used the pre-validation method described in Remark 1 of section \ref{sec:application2}, i.e.,  constructing the ROC curve based on risk score for all $n$ samples estimated via a 10-fold cross-validation.  To remove the Monte-Carlo variability in randomly dividing data into 10 parts, the final ROC curve was obtained by averaging over ROC curves from 500 10-fold cross-validations.  We also plotted the ROC curve without cross-validation, which was clearly overly optimistic, since the AUC of the ROC curve was 1, representing a prefect prediction!  The same bootstrap method could be used to estimate the point-wise confidence intervals of the ROC curve. Specifically, 500 bootstrapped data sets were generated and ROC curves from 20 12-fold cross-validations per bootstrapped data set were calculated. 12-fold instead of 10-fold cross-validation was employed to adjust for the reduced effective sample size in the training set. The bootstrap variance estimators for sensitivities corresponding to specificity level at $5\%, 10\%, \cdots, 95\%$ were then obtained. The resulting confidence intervals representing the underlying predictive performance of the trained random forest were plotted in Figure \ref{fig:roc}, suggesting that the real sensitivity in external data would not reach 100\% even for low specificity levels.


\begin{figure}
\centering
\includegraphics[width=0.8\textwidth ]{roc-randomforest-example.pdf}
\caption{The naive ROC curve (dotted) and the ROC curve (solid) based pre-validation and 95\% confidence intervals at selected specificity levels based on red wine data from UCI repository. }
\label{fig:roc}
\end{figure}


\begin{table}[ht]{
\centering
\setlength{\tabcolsep}{3pt}
\renewcommand{\arraystretch}{0.5}\begin{tabular}{ ccc c ccc }
$m$   &  AUC     &  95\% CI    && AUC & 95\% CI  \\
\hline\\
    & \multicolumn{2}{c}{91 features at Day 0} && \multicolumn{2}{c}{100 features at Day 3}\\

196 &  0.711  & [0.645, 0.778] && 0.711 & [0.643, 0.779] \\
261 &  0.729  & [0.660, 0.798] && 0.731 & [0.659, 0.802] \\
326 &  0.743  & [0.672, 0.814] && 0.747 & [0.674, 0.820] \\
391 &  0.753  & [0.681, 0.825] && 0.760 & [0.687, 0.833] \\
456 &  0.759  & [0.688, 0.831] && 0.768 & [0.695, 0.841] \\
522 &  0.766  & [0.694, 0.837] && 0.777 & [0.705, 0.850] \\
587 &  0.771  & [0.702, 0.840] && 0.785 & [0.718, 0.852] \\
\cline{2-6}\\
 & \multicolumn{2}{c}{126 features at Day 0} && \multicolumn{2}{c}{159 features at Day 3} \\
     \cline{2-3} \cline{5-6}\\
196 &  0.676  & [0.596, 0.755] && 0.664 & [0.583, 0.745] \\
261 &  0.692  & [0.610, 0.773] && 0.678 & [0.594, 0.762]\\
326 &  0.700  & [0.616, 0.784] && 0.688 & [0.600, 0.776]\\
391 &  0.712  & [0.628, 0.796] && 0.702 & [0.614, 0.791]\\
456 &  0.716  & [0.631, 0.801] && 0.709 & [0.620, 0.798]\\
522 &  0.723  & [0.640, 0.806] && 0.718 & [0.630, 0.805]\\
587 &  0.729  & [0.644, 0.814] && 0.727 & [0.635, 0.819]

\end{tabular} 
\caption{Results of estimating the AUC under the ROC curve (c-index) based on MI data from UCI Repository.}\label{tab:example2a1}}
\end{table} 

\begin{table}[ht]{
\centering
\setlength{\tabcolsep}{3pt}
\renewcommand{\arraystretch}{0.5}
\begin{tabular}{ ccc c cc c cc }
\hline
$m$   & $\Delta$ AUC  & 95\% CI &&  $\Delta$ AUC & 95\% CI && $\Delta$ AUC & 95\% CI\\
    & \multicolumn{2}{c}{ Models 2 vs. 1 $(\times 10^{-2})$} && \multicolumn{2}{c}{Models 1 vs. 3 $(\times 10^{-2})$} && \multicolumn{2}{c}{Models 2 vs. 4 $(\times 10^{-2})$}  \\
    \cline{2-3} \cline{5-6} \cline{8-9}\\
196 & -0.042 & [-2.361, 2.276] && 3.562 & [-1.400, 8.524] && 4.714 & [-0.647, 10.08] \\
261 & 0.189  & [-2.469, 2.846] && 3.729 & [-1.571, 9.029] && 5.268 & [-0.564, 11.10] \\
326 & 0.387  & [-2.432, 3.205] && 4.317 & [-1.300, 9.933] && 5.900 & [-0.293, 12.09] \\
391 & 0.711  & [-2.076, 3.497] && 4.158 & [-1.468, 9.783] && 5.795 & [-0.621, 12.21] \\
456 & 0.878  & [-1.963, 3.720] && 4.341 & [-1.560, 10.24] && 5.906 & [-0.690, 12.50] \\
522 & 1.157  & [-1.810, 4.124] && 4.307 & [-1.699, 10.31] && 5.976 & [-0.731, 12.68] \\
587 & 1.370  & [-1.508, 4.247] && 4.222 & [-1.998, 10.44] && 5.804 & [-1.338, 12.95] \\
\end{tabular} 
\caption{Results of comparing AUC under the ROC curve (c-index) between different prediction models based on MI data from UCI repository; Model 1: 91 features at Day 0; Model 2: 100 features at Day 3; Model 3: 126 features at Day 0; Model 4: 159 features at Day 3 }\label{tab:example2a2}}
\end{table} 




\begin{table}[ht]{
\centering
\setlength{\tabcolsep}{3pt}
\renewcommand{\arraystretch}{0.5}
\begin{tabular}{ ccc c cc c cl}
$m$   &  AUC     &  95\% CI    && AUC & 95\% CI &&  $\Delta$ AUC & 95\% CI  \\
\hline\\
    & \multicolumn{2}{c}{Logistic Regression} && \multicolumn{2}{c}{Random Forest} && \multicolumn{2}{c}{Difference} \\
    \cline{2-3} \cline{5-6} \cline{8-9}\\
200 &  0.803  & [0.737, 0.869] && 0.855 & [0.780, 0.929] && -0.052 & [-0.112, 0.009] \\
240 &  0.811  & [0.747, 0.875] && 0.866 & [0.791, 0.940] && -0.055 & [-0.114, 0.004]\\
280 &  0.817  & [0.753, 0.880] && 0.874 & [0.797, 0.951] && -0.057 & [-0.118, 0.004]\\
320 &  0.823  & [0.760, 0.881] && 0.885 & [0.812, 0.957] && -0.062 & [-0.119, -0.005]\\
360 &  0.825  & [0.770, 0.879] && 0.897 & [0.837, 0.958] && -0.073 & [-0.129, -0.016]
\end{tabular} 
\caption{Results of  estimating and comparing AUC under the ROC curve (c-index) based on red wine data from UCI Repository}\label{tab:example2b}}
\end{table} 

%\begin{table}
%\begin{tabular}{ c c c c c}
%\hline
%m & Logistic (p = 10) & Logistic (p = 1000) & OLS (p = 10) & OLS (p = 1000) \\
%\hline
%40 & 0.944 & 0.960 & 0.966 & 0.974 \\
%50 & 0.956 & 0.922 & 0.97 & 0.982 \\
%60 & 0.962 & 0.954 & 0.976 & 0.976 \\
%70 & 0.958 & 0.962 & 0.972 & 0.974 \\
%80 & 0.964 & 0.948 & 0.97 & 0.944
%\end{tabular}
%\caption{Coverage}
%\end{table}

%\begin{table}
%\centering
%\begin{tabular}{l|r}
%Item & Quantity \\\hline
%Widgets & 42 \\
%Gadgets & 13
%\end{tabular}
%\caption{\label{tab:widgets}An example table.}
%\end{table}
\subsection{Application 3}
\subsubsection{Theoretical Properties}
In  the third application, we are interested in evaluating the performance of a precision medicine strategy. In this case, it is not difficult to verify conditions C1-C5 under suitable assumptions. For example, if the matrix $A_0=E(\tilde{Z}\tilde{Z}')$ is nonsingular, $P(G=1)=\pi\in (0, 1)$, and $G\perp Z$, i.e., the treatment assignment is randomized,  then $\hat{\gamma}$ and $\hat{\beta}$ converge to deterministic limits $\gamma_0$ and $\beta_0,$ respectively, as $n \rightarrow \infty$, and especially
$$ \sqrt{n}(\hat{\beta}-\beta_0)=\frac{1}{\sqrt{n}}\sum_{i=1}^n \left\{\pi(1-\pi)A_0\right\}^{-1}\left\{Y_i-\gamma_0'\tilde{Z}_i-(G_i-\pi)\beta_0'\tilde{Z}_i\right\}+o_p(1),$$
where $\beta_0$ is an unique minimizer of the loss function
$$ m(\beta)=E\left\{\left(Y^{(1)}-Y^{(0)}-\beta'\tilde{Z}\right)^2\right\}.$$
Thus condition C1 is satisfied.  Second,  the classes of functions $\{yI(\beta'z>0) \mid \beta \in \Omega\}$ , $\{yI(\beta'z\le 0) \mid \beta \in \Omega\}$, $\{I(\beta'z>0)\mid \beta\in \Omega\}$, and $\{I(\beta'z\le 0)\mid \beta\in \Omega\}$ are Donsker, where $\Omega$ is a compact set in $R^{p+1}$.  This fact suggests that the stochastic processes
\begin{align*}
&\frac{1}{\sqrt{n}}\sum_{i=1} \left[Y_iI(\beta'\tilde{Z}_i>0)-E\left\{YI(\beta'\tilde{Z}>0)\right\} \right]\\
&\frac{1}{\sqrt{n}}\sum_{i=1} \left[Y_iI(\beta'\tilde{Z}_i\le 0)-E\left\{YI(\beta'\tilde{Z}\le 0)\right\} \right]\\
&\frac{1}{\sqrt{n}}\sum_{i=1} \left[I(\beta'\tilde{Z}_i>0)-P(\beta'\tilde{Z}>0) \right]\\
&\frac{1}{\sqrt{n}}\sum_{i=1} \left[I(\beta'\tilde{Z}_i\le 0)-P(\beta'\tilde{Z}\le 0) \right]
\end{align*}
are all stochastically continuous in $\beta.$ Therefore, the processes
$$U_1(\beta)=\sqrt{n}\left\{\frac{\sum_{i=1}^n Y_iG_iI(\beta'\tilde{Z}_i>0)}{\sum_{i=1}^n G_i I(\beta'\tilde{Z}_i>0)}-\frac{\sum_{i=1}^n Y_i(1-G_i)I(\tilde{\beta}'Z_i>0)}{\sum_{i=1}^n (1-G_i)I(\tilde{\beta}'Z_i>0)}-E\left(Y^{(1)}-Y^{(0)} \mid \beta'\tilde{Z}>0 \right)\right\}$$
and 
$$
U_0(\beta)=\sqrt{n}\left\{\frac{\sum_{i=1}^n Y_iG_iI(\beta'\tilde{Z}_i\le 0)}{\sum_{i=1}^n G_i I(\beta'\tilde{Z}_i\le 0)}-\frac{\sum_{i=1}^n Y_i(1-G_i)I(\beta'\tilde{Z}_i\le 0)}{\sum_{i=1}^n (1-G_i)I(\beta'\tilde{Z}_i\le 0)}-E\left(Y^{(1)}-Y^{(0)} \mid \beta'\tilde{Z}\le 0 \right)\right\}$$
are also stochastically continuous in $\beta$, i.e., $U_g(\beta_1)-U_g(\beta_2)=o_p(1)$ for $\|\beta_2-\beta_1\|=o(1), g\in \{0, 1\}.$ As a consequence, condition C2 is satisfied.
It is clear that $l_1(\beta)=E\left(Y^{(1)}-Y^{(0)} \mid \beta'\tilde{Z}>0 \right)$ and $l_0(\beta)=E\left(Y^{(1)}-Y^{(0)} \mid \beta' \tilde{Z}\le 0 \right)$ are differentiable in $\beta$ in a small neighborhood of $\beta_0,$ if the random variable $\beta_0'\tilde{Z}$ has a continuously differentiable bounded density function and $E\left(Y^{(g)}\mid \beta_0'\tilde{Z}=s\right)$ is smooth in $s$.  This suffices for condition C3.  Next, the central limit theorem and delta-method together imply that $U_g(\beta_0)$
converges weakly to a mean zero Gaussian distribution as $n \rightarrow \infty,$ where $g\in \{0, 1\}.$  Lastly, $|E\{P(Y^{(1)}-Y^{(0)}\mid \hat{\beta}'\tilde{Z}>0)\}-P(Y^{(1)}-Y^{(0)}\mid \beta_0'\tilde{Z}>0)|+|E\{P(Y^{(1)}-Y^{(0)}\mid \hat{\beta}'\tilde{Z}\le 0)\}-P(Y^{(1)}-Y^{(0)}\mid \beta_0'\tilde{Z}\le 0)|=o_p(|E(\hat{\beta})-\beta_0|)=o_p(n^{-1/2}),$ and C5 is satisfied. Therefore, the cross-validation estimator $\widehat{Err}^{CV}_m$ is a consistent estimator for $Err_m$ and follows asymptotically a Gaussian distribution, whose variance can be estimated by the proposed bootstrap method.

\subsubsection{Simulation Study}
In the simulation study, the covariate $Z_i$ was generated from a $p$ dimensional standard multivariate Gaussian distribution and the continuous outcome $Y_i^{(g)} $ was generated via two linear regression models:
$$ Y_i^{(1)}=\beta_1'\tilde{Z}_i+\epsilon_i^{(1)},$$
and
$$ Y_i^{(0)}=\beta_0'\tilde{Z}_i+\epsilon_i^{(0)},$$
where $\beta_1=(0, 0.25, 0.25, 0.25, 0.25, 0, \cdots, 0)'$, $\beta_0=(0, 0.25, -0.25, 0.25, -0.25, 0, \cdots, 0)'$ and $\epsilon_i^{(g)}\sim N(0, 1), g\in \{0, 1\}.$ The treatment assignment indicator $\{G_1, \cdots, G_n\}$ was a random permutation of $\{1, \cdots, 1, 0, \cdots, 0\}$ consisting of $n/2$ ones and $n/2$ zeros. The observed outcome $Y_i=Y_i^{(1)}G_i+Y_i^{(0)}(1-G_i), i=1, \cdots, n.$  The generated data  $D_n=\left\{X_i=(Y_i, G_i, Z_i), i=1, \cdots, n\right\}.$

In the first set of simulations, we let $p=10$ and the sample size $n=90\times 2=180$.  We considered the cross-validation estimator $\widehat{Err}_m^{CV}$ for $Err_m, m\in\{80, 90, 100, 110, 120, 130, 140\}.$ Due to symmetry, we only considered the case where $Err_m$ was the average treatment effect among patients recommended to receive treatment $G=1,$ denoted as the high value subgroup consisting of responders to the treatment $G=1$.  The true value of $Err_m$ was computed by averaging the treatment effect among identified responders based on the estimated ITR scores from 5,000 generated training sets of size $m$. The true treatment effect among responders was calculated with an independently generated testing set consisting of 200,000 patients. The true $Err_m$ is 0.37, 0.39, 0.40, 0.42, 0.43, and 0.44 for $m=$ 80, 90, 100, 110, 120, 130, and 140, respectively. The increasing trend in $Err_m$ reflected the improved quality of the estimated ITR score based on a bigger training set. Note that in this setting, the average treatment effect among responders based on true individualized treatment effects was 0.56.

We constructed the cross-validation estimates of the $Err_m$ from 1,000 datasets $D_n$ of size $n=90$ each.  For each simulated data set $D_n$, we divided the data set into a training set of size $m$ and a testing set of size $n-m$. The ITR score $\widehat{\Delta}(z\mid D_{train})$ was estimated based on the training set and responders in the testing set was identified as patients whose $\widehat{\Delta}(z\mid D_{train})> 0$. The average treatment effect estimator among responders in the testing set was simply the average difference in $Y$ between responders, who received the active treatment $G=1,$ and responders, who received the control treatment $G=0.$ This process was repeated 400 times and the resulting $\widehat{Err}_m^{CV}$ was the sample average of 400 average treatment effect estimators among identified responders in the testing set.  In addition, we used the proposed bootstrap method to compute the standard error estimators $\hat{\sigma}_m^{CV}$ and $\hat{\sigma}_{m,adj}^{CV}.$ In computing them, we set the number of bootstraps $B_{BOOT}$ to be 400 and the number of cross-validations per bootstrap to be $B_{CV}=20.$ The 95\% confidence interval for $Err_m$ was also constructed for each simulated data set $D_n.$  We also examined the performance of constructed confidence intervals using only a very small number of bootstrap iterations by choosing $(B_{BOOT}, B_{CV})=(20, 25).$

In the second set of simulations, we let $p=1000.$ In calculating the estimated regression coefficient $\hat{\beta}$ in the ITR score $\widehat{\Delta}(z\mid D_{train}),$  we implemented the lasso regularized method. Specifically, we estimated $\beta$ by minimizing a regularized loss function
$$\sum_{X_i\in D_{train}} \left[Y_i-\gamma_0-\gamma_Z'Z_i-(G_i-\pi)\beta'\tilde{Z}_i\right]^2 +\lambda_1|\gamma_Z|_1+\lambda_2|\beta|_1,$$
where $\lambda_1$ and $\lambda_2$ were appropriate penalty parameters. To save computational time, both penalty parameters were fixed at 0.10 in all simulations instead of being adaptively selected via cross-validation within the training set.  The resulting minimizer of $\beta$ was denoted by $\hat{\beta}(D_{train})$ and $\widehat{\Delta}(z\mid D_{train})=\hat{\beta}(D_{train})'\tilde{z}.$ Similar to the low dimensional case, we simulated 1,000 datasets and for each generated dataset $D_n,$ we calculated $Err(D_n),$ $\widehat{Err}_m^{CV}$, the bootstrap standard error estimators $\widehat{\sigma}_m^{CV}$ and $\widehat{\sigma}_{m,adj}^{CV}$, and the corresponding 95\% confidence intervals.  Similar to the low dimensional case, we investigated the performance of the confidence intervals constructed using both a large number and a small number of bootstraps.


The simulation results including the true value of $Err_m$, the empirical average and standard deviation of cross-validation estimator $\widehat{Err}_m^{CV}$, and the empirical coverage of 95\% confidence intervals based on $B_{BOOT}=400$ were summarized in Table \ref{tab:simupm}.  In addition, the empirical coverage levels of the constructed 95\% confidence intervals based on $B_{BOOT}=20$ were summarized in Table \ref{tab:simupmsmall}.
For both low and high dimensional cases, the cross-validated estimator $\widehat{Err}^{CV}_m$ was almost unbiased in estimating $Err_m$, especially relative to the empirical standard deviation of $\widehat{Err}^{CV}_m$. The empirical coverage level of 1000 constructed 95\% confidence intervals for $Err_m$ based on bootstrap variance estimator from a large number of bootstrap iterations was quite close to its nominal level. After sample size adjustment in variance estimation, the constructed confidence intervals based on $\widehat{\sigma}_{m,adj}^{CV}$ slightly under-covered the true parameter with empirical coverage levels between 90\% and 93\%.  When a small number of bootstrap iterations was used ($B_{BOOT}=20$), the proposed bootstrap calibration can be used to maintain the proper coverage level as Table \ref{tab:simupmsmall} showed. As a price, the median width of calibrated confidence interval increased 14-28\% depending on the training size $m$ and dimension $p$. Note that the theoretical justification for the Gaussian approximation to the cross-validated estimator in high dimensional case was not provided as in the previous two examples. However, the empirical distribution of $\widehat{Err}^{CV}_m$ was quite ``Gaussian" with its variance being approximated well by bootstrap method. This observation ensured the good performance of resulting 95\% confidence intervals. In addition, the empirical coverage levels of the 95\% confidence intervals of $Err_{m}$ with respect to $Err(D_n)$ were 92.9\% and 95.4\% in low- and high- dimensional settings, respectively, where $(m, n)=(140, 180).$ 


In summary, proposed confidence intervals based on bootstrap standard error estimator $\widehat{\sigma}_m^{CV}$ have a good coverage level. The bootstrap calibration can effectively correct the under-coverage of the confidence intervals based on a very small number of bootstraps. The constructed confidence interval can be viewed as a confidence interval for both $Err(D_n)$ and $Err_m,$ when $m\approx n$. In this case, due to the complexity of the evaluation procedure in the testing set, no existing method is readily available for studying the distribution of the cross-validation estimator for the average treatment effect among ``responders''.    

\begin{table}[ht]{
\centering
\setlength{\tabcolsep}{3pt}
\renewcommand{\arraystretch}{0.5}
\begin{tabular}{ c ccccc c ccccc }
\hline
$m$ & \multicolumn{5}{c}{$p = 10$} & & \multicolumn{5}{c}{$p = 1000$} \\
    & $Err_m$ & $E(\widehat{Err}_m^{CV})$ & SD   & COV-adj & COV & & $Err_m$ & $E(\hat{Err}_m^{CV})$ & SD   & COV-adj & COV \\
    \cline{2-6} \cline{8-12}\\
80  & 0.369 & 0.377 & 0.196 & 92.7\% & 95.1\% && 0.079 & 0.082 & 0.190& 92.7\% & 95.9\%\\
90  & 0.384 & 0.395 & 0.197 & 91.9\% & 95.2\% && 0.094 & 0.095 & 0.198& 92.0\% & 96.1\%\\
100 & 0.398 & 0.409 & 0.198 & 92.3\% & 95.1\% && 0.108 & 0.110 & 0.208& 91.1\% & 95.9\% \\
110 & 0.412 & 0.422 & 0.199 & 91.9\% & 95.3\% && 0.119 & 0.123 & 0.218& 91.3\% & 95.6\% \\
120 & 0.421 & 0.433 & 0.199 & 91.4\% & 95.1\% && 0.135 & 0.138 & 0.229& 91.0\% & 95.1\& \\
130 & 0.431 & 0.441 & 0.201 & 90.7\% & 95.2\% && 0.152 & 0.152 & 0.242& 90.3\% & 95.0\% \\
140 & 0.439 & 0.449 & 0.202 & 91.4\% & 95.4\% && 0.166 & 0.166 & 0.256& 89.4\% & 94.9\% \\
\end{tabular}
\caption{Simulation sesults for precision medicine. $Err_m$, the true average treatment effect in identified high value subgroup;  $E(\widehat{Err}_m^{CV}),$ the empirical average of the cross-validation estimate $\widehat{Err}_m^{CV}$; SD, the empirical standard deviation of the cross-validation estimate $\widehat{Err}_m^{CV}$  ; COV-adj, the empirical coverage level of 95\% confidence intervals based on $\hat{\sigma}_{m, adj}^{CV}$ from bootstrap; COV, the empirical coverage level of 95\% confidence intervals based on $\hat{\sigma}_{m}^{CV}$ from bootstrap.}\label{tab:simupm}}
\end{table}



\begin{table}[ht]{
\centering
\setlength{\tabcolsep}{3pt}
\renewcommand{\arraystretch}{0.5}
\begin{tabular}{ cccccccc }
\hline\\
$m$   & \multicolumn{3}{c}{Confidence Intervals Based on $\widehat{\sigma}_{m,adj}^{CV}$} &&  \multicolumn{3}{c}{Confidence Intervals based on $\widehat{\sigma}_m^{CV}$}\\
    & Algorithm 1 & Algorithm 1 & Algorithm 2 && Algorithm 1 & Algorithm 1 & Algorithm 2\\ 
    & $B_{BOOT}=400$  & $B_{BOOT}=20$  & $B_{BOOT}=20$  && $B_{BOOT}=400$  & $B_{BOOT}=20$ & $B_{BOOT}=20$\\
        \cline{2-4} \cline{6-8}\\
         & \multicolumn{7}{c}{$p = 10$} \\
         \cline{2-8}\\
80   & 92.7\% & 90.8\% & 94.8\% && 95.1\% & 93.7\% & 96.6\%\\
90   & 91.9\% & 90.2\% & 94.1\% && 95.2\% & 92.7\% & 96.8\%\\
100  & 92.3\% & 90.3\% & 94.6\% && 95.1\% & 93.0\% & 97.4\%\\
110  & 91.9\% & 89.8\% & 95.3\% && 95.3\% & 93.5\% & 97.6\%\\
120  & 91.4\% & 89.0\% & 96.7\% && 95.1\% & 93.3\% & 98.6\%\\
130  & 90.7\% & 89.2\% & 96.5\% && 95.2\% & 92.4\% & 98.7\%\\
140  & 91.4\% & 86.2\% & 97.1\% && 95.4\% & 90.9\% & 98.3\%\\

   & \multicolumn{7}{c}{$p = 1000$} \\
   \cline{2-8}\\
80  &  92.7\% & 89.9\% & 94.2\% && 95.9\% & 93.1\% & 96.7\%\\
90  &  92.0\% & 90.7\% & 95.4\% && 96.1\% & 93.9\% & 97.4\%\\
100 &  91.1\% & 89.6\% & 95.8\% && 95.9\% & 93.9\% & 97.9\%\\
110 &  91.3\% & 89.0\% & 95.0\% && 95.6\% & 93.7\% & 97.3\%\\
120 &  91.0\% & 89.0\% & 94.5\% && 95.1\% & 93.8\% & 97.9\%\\
130 &  90.3\% & 87.5\% & 95.0\% && 95.0\% & 93.6\% & 97.7\%\\
140 &  89.4\% & 86.5\% & 96.0\% && 94.9\% & 92.6\% & 98.1\%
\end{tabular}
\caption{Empirical coverage levels of 95\% confidence intervals of the average treatment effect among responders using various numbers of bootstrap iterations.} \label{tab:simupmsmall}}
\end{table} 


\subsubsection{Real Data Example}
In this section, we consider a recent clinical trial Prevention of Events with Angiotensin Converting Enzyme Inhibition (PEACE) to study whether the ACE inhibitors (ACEi) are effective in reducing future cardiovascular-related events for patients with stable coronary artery disease and normal or slightly reduced left ventricular function \cite{peace2004angiotensin}. While a total of 8290 patients are enrolled in the study,  we focus on a subgroup of 7865 patients with complete covariate information, in which 3947 and 3603 patients were assigned to receive the ACEi and placebo, respectively.  One main endpoint of the study is the patients survival time.  By the end of the study, there were 315 and 292 deaths observed in the control and treatment groups, respectively. Under a proportional hazards model, the estimated hazard ratio was 0.92 with a 95\% confidence interval of (0.78, 1.08) and a two-sided p-value of 0.30. Based on the results of this study, it was inconclusive whether ACEi therapy would reduce the mortality in the overall patient population. However, with further analysis of the PEACE survival data, Solomon et al. (2006) reported that ACEi might significantly prolong the survival for patients whose baseline kidney functions were  abnormal \cite{solomon2006renal}. This finding could be quite important in guiding the individualized treatment recommendation in practice. The objective of our analysis here was systematic identification of a high value subgroup of patients who may benefited from ACEi, even though the average treatment effect of ACEi in the entire study population was neither clinically nor statistically significant. The outcome of interest was the time to all-cause mortality. To build a candidate ITR scoring system capturing the individualized treatment effect, we first used 4 covariates previously identified as statistically and clinically important predictors of the overall mortality in the literature \cite{solomon2006renal}. These covariates are age, gender, eGFR measuring baseline renal function, and left ventricular ejection fraction measuring hearts ability to pump oxygen-rich blood out to the body.  We considered an ITR score derived from a training set of size $m=6292,$ i.e., 80\% of the entire study population.  The ITR score was the estimated difference in restricted mean survival time given baseline covariates \cite{uno2014moving, zhao2016restricted}. To this end, we fit Cox regression in two treatment arms separately: 
$$P(Y_i > t| Z_i=z, G_i=g)=S_g(t)^{\exp(\beta_g'z)}$$ 
and denote the resulting estimators by $\left\{\hat{S}_g(\cdot \mid D_{train}), \hat{\beta}_g(D_{train})\right\}, $ where $S_g(t)$ is the baseline survival function and $\beta_g$ is the regression coefficient in treatment $g$ group, $g\in \{0, 1\}.$  Specifically, $\beta_g$ can be estimated by maximizing the partial likelihood function and $S_g(\cdot)$ can be estimated by a transformation of the Breslow estimator of the cumulative baseline hazard function. Under the assumed Cox regression model, the ITR score for a patient with covariate $z$ is
$$ \widehat{\Delta}(z \mid D_{train})=\int_0^\tau \left[\hat{S}_1(t\mid D_{train})^{\exp(\hat{\beta}_1(D_{train})'z)}-\hat{S}_0(t \mid D_{train})^{\exp(\hat{\beta}_0(D_{train})'z)}\right]dt,$$
where $\tau=2000$ days is a pre-specific cutoff time point.  The average treatment effect in the testing set can be measured in different ways. First, we considered the average treatment effect as the difference in restricted mean survival time, representing by the area between the two survival curves restricted between time zero and the cutoff time point $\tau$, i.e., 
$$\hat{\Delta}_g(D_{train}, D_{test})= \int_0^\tau \hat{S}_{1}(t \mid \hat{D}_{test}^{(g)})dt- \int_0^\tau \hat{S}_{0}(t \mid \hat{D}_{test}^{(g)})dt,$$
where $g\in \{0, 1\},$ 
$$\hat{D}_{test}^{(1)}=\left\{ X\in D_{test} \mid \hat{\Delta}(Z \mid D_{train})\ge \hat{\delta}_0\right\},$$
$$\hat{D}_{test}^{(0)}=\left\{ X\in D_{test} \mid \hat{\Delta}(Z \mid D_{train})< \hat{\delta}_0\right\},$$ 
$\hat{\delta}_0$ was the sample median of $\left\{\widehat{\Delta}(Z\mid D_{train}) \mid X\in D_{train}\right\},$ $\hat{S}_{1}(t\mid \hat{D}_{test}^{(g)})$ was the Kaplan-Meier estimator of the survival function based on patients in $\hat{D}_{test}^{(g)}$ who received the ACEi, and $\hat{S}_{0}(t \mid \hat{D}_{test}^{(g)})$ was the Kaplan-Meier estimator of the survival function based on patients in $\hat{D}_{test}^{(g)}$ who received the placebo.  In other words, we were interested in estimating the average treatment effect in the high value subgroup and its complement. Furthermore, we are also interested in estimating the difference in average treatment effect between the high value subgroup and its complement, i.e., the interaction between treatment and subgrouping based on ITR scores.  Here, the high value subgroup was identified by whether the estimated ITR score $\hat{\Delta}(z\mid D_{train})>\widehat{\delta}_0.$  Based on 500 cross-validations, the cross-validated estimate $\widehat{Err}^{CV}_m$ was 21.1 days for $\hat{\Delta}_1(D_{train}, D_{test}),$ the average treatment effect in the high value subgroup, and -13.2 days for 
 $\hat{\Delta}_0(D_{train}, D_{test}),$ the average treatment effect in the complement of the high value subgroup. Their difference, i.e., the interaction with treatment, was 34.3 days.  Now, we implemented the proposed bootstrap method to estimate the standard errors of those estimators. Specifically, we choose the number of bootstrap to be 500 and the number of cross-validation per bootstrap to be 20.  The 95\% confidence interval for the average treatment effect in the high value subgroup was [-1.3, 45.5] days, and the associated p value for testing no treatment effect was 0.064, nearly statistically significant at the conventional 0.05 level.  The 95\% confidence interval for the average treatment effect in the complement of the high value subgroup was [-31.5, 5.2] days and the associated p value for testing no treatment effect in that subgroup was 0.161.  The 95\% confidence interval for their difference was [4.3, 64.3] and the corresponding p value for testing no difference was 0.025, suggesting that the ITR score constructed from the training set of 6,292 patients had a statistically significant interaction with the treatment, i.e., the average treatment effect in the high value subgroup was greater than that in the remaining patients. The detailed results were summarized in Table \ref{tab:peace}. 
 
 Next, we also summarized the average treatment effect via the commonly used hazard ratio and the results were also reported in Table \ref{tab:peace}. The 95\% confidence intervals of hazard ratios were transformed from confidence intervals of the log-transformed hazard ratio, whose distribution can be approximated better by a Gaussian distribution.  Similarly, the comparison of two hazard ratios was based on the difference between two log-transformed hazard ratios or equivalently the ratio of two hazard ratios. Based on the hazard ratio, while the interaction between the identified high value subgroup and treatment was only marginally statistically significant at the 0.10 level, the average treatment effect in the high value subgroup was statistically significantly favoring ACEi. Lastly, we repeated the analysis with 7 baseline covariates: age, gender, eGFR, left ventricular ejection fraction, hypertension, diabetes, and history of myocardial infarction. The results were summarized in Table \ref{tab:peace}.  The general directions of the result were similar as those based on 4 baseline covariates: the average treatment effect in the high value subgroup tended to be positive, while the average treatment effect in the complement of the high value subgroup tended to be negative after cross-validations. But no difference was statistically significant, which was expected because the three additional covariates didn't demonstrate strong interactions with the treatment in previous analysis, and might dilute the predictiveness of the constructed ITR scores in capturing the individualized treatment effect \cite{zhao2013effectively}.

 

\begin{table}[ht]{
\centering
\setlength{\tabcolsep}{3pt}
\renewcommand{\arraystretch}{0.5}
\begin{tabular}{ crrc c cccc }
\hline
    \multicolumn{4}{c}{restricted mean survival time} & & \multicolumn{4}{c}{hazard ratio} \\
 $Err_m$  &  $\widehat{Err}_m^{CV}$ & 95\% CI for $Err_m$   & $p$ value & & $Err_m$ & $\widehat{Err}_m^{CV}$ & 95\% CI for $Err_m$   & $p$ value\\
    \cline{1-4} \cline{6-9}\\
 \multicolumn{9}{c}{4 covariates}\\
$\hat{\Delta}_1$ & 21.1 & [-1.3,43.5] & 0.064 && $\hat{\Delta}_1$ & 0.80 & [0.66, 0.98]& 0.028 \\ 
$\hat{\Delta}_0$   & -13.2 & [-31.5, 5.2]& 0.161 && $\hat{\Delta}_0$ & 1.18 & [0.85, 1.65]& 0.326 \\
$\hat{\Delta}_1-\hat{\Delta}_0$ & 34.3 & [4.3, 64.3] & 0.025 && $\hat{\Delta}_1/\hat{\Delta}_0$ & 0.68 & [0.44, 1.04] &  0.076  \\
 \multicolumn{9}{c}{7 covariates}\\
$\hat{\Delta}_1$ & 17.9 & [-3.9,39.7] & 0.108 && $\hat{\Delta}_1$ & 0.81 & [0.66, 1.00]& 0.053 \\ 
$\hat{\Delta}_0$   & -10.3 & [-32.6, 12.1]& 0.367 && $\hat{\Delta}_0$ & 1.09 & [0.79, 1.51]& 0.603 \\
$\hat{\Delta}_1-\hat{\Delta}_0$ & 28.1 & [-5.4, 61.7] & 0.100 && $\hat{\Delta}_1/\hat{\Delta}_0$ & 0.75 & [0.48, 1.15] &  0.188  \\
\end{tabular}
\caption{Results of estimated the average treatment effect among identified responders in PEACE trial} \label{tab:peace}}
\end{table}

There are other measures for the performance of a precision medicine strategy.  For example, one may use the average utility (e.g., the RMST) when the strategy is applied to the entire patient population.  One may also use the AD curve representing the average treatment effect in subgroups of patients with predicted treatment benefits above different cutoffs to gauge the performance of a strategy \cite{zhao2013effectively, pellegrini2020proof}. They are not regular prediction errors, but should also be estimated via cross-validation to avoid overfitting.  The bootstrap method proposed in this paper can be used to make rigorous statistical inference for them as well.

%, history of hypertension, diabetes, and history of myocardial infarction. 
%In addition, we considered the scoring systems built with all  baseline covariates listed in Table 2 of Braunwald et al. (2004) except race, country, and serum creatinine, which are not available in our database from the US National Institutes of Health. Moreover, we omitted four binary variables due to lack of variability (i.e., over 95\% of patients exhibited the same covariate value). These excluded variable are: use of Digitalis, use of antiarrhythmic agent, use of anticoagulant, and use of insulin. On the other hand, an extra variable eGFR, which is a function of age, gender, race, and serum creatinine, was available in our database. To this end, we considered the remaining 20 variables from Table 2 of Braunwald et al. (2004) in addition to eGFR, resulting in a total of 21 covariates. In our analysis, we included all patients ($n = 7460$) who had complete information concerning these 21 covariates. To estimate the score for the treatment differences, we considered separate Cox model for each of the two treatment groups. We used the lasso to select informative predictors. 

\section{Discussion}
In this paper, we propose a bootstrap method for making statistical inferences on summary statistics obtained from cross-validation, which is commonly used to evaluate the performance of statistical procedures. We clarify the population parameter that cross-validation estimates and provide asymptotic justification for the bootstrap procedure under certain regularity conditions. The proposed method substantially reduces the computational demands of a regular bootstrap using results from a random effects model and can be applied to quantify the uncertainty of almost any cross-validation-based estimate. Our approach complements the work of \cite{bates2021cross}, which focuses on constructing confidence intervals for the random quantity $Err(D_n)$.

However, there is a significant gap between the empirical performance of the proposed inference in finite samples and its theoretical justification, which requires large sample approximations and root $n$ regular estimates for all relevant parameters. For example, our simulation study shows that the distribution of the cross-validated estimate of the c-index of the logistic regression model with high-dimensional covariates fitted via lasso regularization is reasonably Gaussian, and the associated bootstrap confidence interval performs well even when the sample size is moderate. However, the current theoretical justification cannot confirm the validity of the proposed procedure in this setting, which is often the most important application of the cross-validation procedure. Therefore, further research in this direction is warranted.

We also note an interesting observation that although the bootstrap-based confidence intervals are constructed to cover the population parameter $Err_m$, they demonstrate a comparable coverage level with respect to $Err(D_n)$ for $m\approx n.$ When the dimension is low, this can be explained by the fact that the variance of $Err(D_n)$ is substantially smaller than that of $\widehat{Err}_m^{CV}$, and the correlation coefficient between $Err(D_n)$ and $\widehat{Err}_m^{CV}$ is close to zero. When the dimension is high, however, the variance of $Err(D_n)$ can be comparable to that of $\widehat{Err}_m^{CV}$, and the correlation between $Err(D_n)$ and $\widehat{Err}_m^{CV}$ can be high. It is not clear when the constructed confidence interval can be interpreted as a confidence interval with respect to $Err(D_n)$.




\section{Appendix: Outline of  the theoretical justification for the asymptotic normality of the cross-validation estimator $\widehat{Err}_m^{CV}$}
First, based on condition $C1,$
\begin{align}
\hat{\psi}(D_{train})-\psi_0&=\frac{1}{m}\sum_{i\in D_{train}} \xi(X_i)+o_p(m^{-1})\\
                            &=\frac{1}{n}\sum_{i=1}^n \frac{I(X_i\in D_{train})}{\hat{\pi}} \xi(X_i)+o_p(n^{-1}),
\label{eq1:appendix} \end{align}
where $\hat{\pi}=m/n\in [l, u]$ and $0<l<u<1.$, Next, it follow from the condition $C2$ and $C3$ that
\begin{align*}
&\sqrt{n-m}\left[L(D_{test}, \hat{\psi}(D_{train}))-E\left\{L(D_{test}, \hat{\psi}(D_{train}))\right\}\right]\\
=&\sqrt{n-m}[L(D_{test}, \psi_0)-E\left\{L(D_{test}, \psi_0)\right\}]+o_p(1)\\
=&\frac{1}{\sqrt{n-m}}\sum_{i \in D_{test}} \eta(X_i)+o_p(1). 
\end{align*}
Note that 
\begin{align*}
&E\left\{L(D_{test}, \hat{\psi}(D_{train})\right\}\\
=&E\left\{L(D_{test}, \psi_0)\right\}+l_{\psi_0}(\hat{\psi}(D_{train})-\psi_0)+o(\|\hat{\psi}(D_{train})-\psi_0\|)\\
=&E\left\{L(D_{test}, \psi_0)\right\}+\frac{1}{n}\sum_{i=1}^n \frac{I(X_i\in D_{train})}{\hat{\pi}} l_{\psi_0}\left\{\xi(X_i)\right\}+o_p(n^{-1})
\end{align*}
based on condition C4 and (\ref{eq1:appendix}). Lastly,
\begin{align*}
&\sqrt{n}\left[E\left\{L(D_{test}, \hat{\psi}(D_{train})\right\}-E\left\{L(D_{test}, \psi_0)\right\}\right]\\
=&\sqrt{n}\left[E\left\{L(D_{test}, \hat{\psi}(D_{train})\right\}-E\left\{L(D_{test}, \hat{\psi}(D_{train}))\right\}\right] \\
&+\sqrt{n}\left[E\left\{L(D_{test}, \hat{\psi}(D_{train}))\right\}-E\left\{L(D_{test}, \psi_0)\right\}\right]\\
=& \frac{\sqrt{n}}{(n-m)}\sum_{i \in D_{test}} \eta(X_i)+ \frac{1}{\sqrt{n}}\sum_{i=1}^n \frac{I(X_i\in D_{train})}{\hat{\pi}} l_{\psi_0}\left\{\xi(X_i)\right\}+    o_p(1)\\
=& \frac{1}{\sqrt{n}}\sum_{i=1}^n \left [ \frac{I(X_i\in D_{test})}{1-\hat{\pi}}\eta(X_i)+\frac{I(X_i\in D_{train})}{\hat{\pi}} l_{\psi_0}\left\{\xi(X_i)\right\}\right]+o_p(1).
\end{align*}
Conditional on the observed data and taking expectation with respect to 
$I(X_i\in D_{train})$ and $I(X_i \in D_{test}), i=1, \cdots, n$
$$ \sqrt{n}\left(\widehat{Err}^{CV}_m-E\left\{L(D_{test}, \psi_0)\right\}\right)=\frac{1}{\sqrt{n}}\sum_{i=1}^n \left [ \eta(X_i)+l_{\psi_0}\left\{\xi(X_i)\right\}\right]+o_p(1).$$
Since $\lim_{n\rightarrow \infty} E\left\{L(D_{test}, \psi_0)\right\}=Err,$ we have 
$$ \sqrt{n}\left(\widehat{Err}^{CV}_m-Err\right) $$ 
converges weakly to a mean zero Gaussian distribution with a variance of 
$$\sigma_D^2=E\left(\left [ \eta(X_i)+l_{\psi_0}\left\{\xi(X_i)\right\}\right]^2\right).$$
Lastly, under condition C5, 
$$ \sqrt{n}\left(\widehat{Err}^{CV}_m-Err_m\right)= \sqrt{n}\left(\widehat{Err}^{CV}_m-Err\right) +o_p(1)$$ 
also converges weakly to $N(0, \sigma_D^2).$




% \bigskip
% \begin{center}
% {\large\bf SUPPLEMENTARY MATERIAL}
% \end{center}

% \begin{description}

% \item[Title:] Brief description. (file type)

% \item[R-package for  MYNEW routine:] R-package MYNEW containing code to perform the diagnostic methods described in the article. The package also contains all datasets used as examples in the article. (GNU zipped tar file)

% \item[HIV data set:] Data set used in the illustration of MYNEW method in Section~ 3.2. (.txt file)

% \end{description}


\bibliographystyle{plain}
\bibliography{main}

\end{document}
