\section{Experimental results}
\label{sec:evaluation}
\begin{figure*}[htb]
\includegraphics[trim=0 0 0 0,clip,width=0.98\linewidth]{figures/Mue1_v2.png}
\caption{Adjusting $\mu_1$ raises the \emph{Accuracy Enhancement Degree} (AED) in scenarios with heterogeneous communication quality.}
\label{fig:Mue1}
\end{figure*}

\begin{figure*}[ht]
\includegraphics[trim=0 0 0 0,clip,width=0.97\linewidth]{figures/Mue2_v2.png}
\caption{Increasing $\mu_2$ stabilizes federated learning in scenarios with heterogeneous communication quality.}
\label{fig:Mue2}
\vspace{-0.3cm}
\end{figure*}

We set up an experiment with a DNN model with a size of $130\,kB$ trained on the MNIST~\cite{726791} dataset, which has $10$ labels as road traffic scenarios and is processed as Non-IID data partitions. Similar to the methods in~\cite{caldas2019leaf}, those partitions are divided into $110$ traffic agents, where the first 10 agents exclude a few labels and are used for pre-training as the addressed application scenarios. The pre-trained DNN model with  $68\,\%$ testing accuracy (ACC) is set as the initial model in all three layers as described in Section~\ref{sec:system}. Each of the other $100$ agents is able to train the DNN model locally and communicate to one RSU if the connection can successfully be established. As formulated in Section~\ref{sec:problem_formulation}, we vary $CSR$ and $SCD$, and consider the heterogeneous communication quality in V2X networks.

To evaluate the enhancement of the pre-trained model specifically by various $\mu_1$, we note the testing accuracy changes at time $t$ compared to the pre-trained DNN model as $\Delta ACC = ACC_t-ACC_{pre}$, and define \emph{ACC Enhancement Degree} (AED) as
\begin{equation}
    \label{Eq:AED}
    AED = (\Delta ACC^{\mu_1>0} - \Delta ACC^{\mu_1=0}) / \Delta ACC^{\mu_1=0}.
\end{equation}
The metric AED indicates the testing accuracy enhancement by increasing $\mu_1$ ($\mu_1\,>\,0$) in the set of parameters $\mathcal{M}$ (Algorithm~\ref{alg:agent}) for agents connecting to the RSUs, which can only provide the data with very limited label types. As the first column of Fig.~\ref{fig:Mue1} shows, when we set $\mu_1=0.001$ the communication quality satisfied is (CSR=100\,\%), the AED is overall positive (under $0.5\,\%$) with $\mu_2\,=\,0$ or $\mu_2\,=\,0.001$ after convergence (from around 10\,seconds). By increasing $\mu_1$, the AED is clearly raised. Each row in Fig.~\ref{fig:Mue1} shows the AED during federated learning with various $\mu_1$ and fixed $\mu_2$ with different qualities of communication. The results indicate that the AED can be obviously increased when the communication quality is rather bad (low CSR), which can oft happen in V2X networks in C-ITS due to the low priority of federated learning-related communication messages. When only 20\,\% traffic agents are successfully connected to the RSUs, $\mu_1\,=\,0.001$ can enhance the testing ACC of the federated trained DNN model up to $20\,\%$ compared to the model trained without setting $\mu_1$. 

\begin{figure*}[ht]
\centering
\begin{subfigure}[b]{0.45\textwidth}
   \includegraphics[trim=0 0 0 0 ,clip,width=1\linewidth]{figures/scenario1.png}
   \caption{\centering Scenario I: Non-IID datasets only in RSU layer}
   \label{fig:scenario1} 
\end{subfigure}
\begin{subfigure}[b]{0.45\textwidth}
   \includegraphics[trim=0 0 0 0,clip,width=1\linewidth]{figures/scenario2.png}
   \caption{\centering Scenario II: Non-IID datasets only in traffic agent layer}
   \label{fig:scenario2}
\end{subfigure}
\vspace{2mm}
\caption{\centering Comparison between our framework {\myHFed} and other baseline methods for heterogeneous communication quality. Note that the baseline methods can also be seen as our framework with dedicated parameter combinations.}
\label{fig:HW}
\vspace{-0.3cm}
\end{figure*}

Next, we evaluate the effect of positive $\mu_2$ on federated learning. We notice in Fig.~\ref{fig:Mue1} that a positive value of $\mu_2$ can reduce the AED to some extent. However, for most federated learning application scenarios, a failure in the learning process is caused by unreliable connections between clients and the server. Thus, learning stability can be more significant for federated learning. The first row of Fig.~\ref{fig:Mue2} shows that the concussion in the learning process due to the low communication quality can be well stabilized through a large $\mu_2$. We also present the \emph{Mean Squared Error} (MSE) of the testing accuracy to centralized learning results in the second row of Fig.~\ref{fig:Mue2}, where the raw data from all traffic agents are collected in the cloud layer and used for centralized model training. When $\mu_2\,=\,0.005$, the jitters of the testing accuracy curve are extraordinarily coped, where the performance of federated learning is almost the same as learning with $CSR\,=\,90\,\%$. 

Finally, we compare our {\myHFed} with other two baseline methods --  \emph{FedProx}~\cite{li2020federated}, \emph{HierFAVG}~\cite{9148862} -- considering Non-IID datasets and heterogeneous communication quality with $CSR\,=\,10\,\%$ and $SCD\,=\,1\,s$. We demonstrate all those federated DNN model enhancement approaches in two different empirical C-ITS application scenarios with $100$ traffic agents and $10$ RSUs. In \emph{Scenario I}, the datasets across RSUs are Non-IID, while the datasets in all traffic agents under one RSU have the same distribution. In contrast, the datasets across traffic agents at one RSU are Non-IID in \emph{Scenario II}, where each RSUs has the same data distribution. 

Fig.~\ref{fig:scenario1} shows our framework {\myHFed} can enhance the pre-trained model stably from beginning to convergence, while the jitters of \emph{HierFAVG} curve are more visibly affected by the bad communication situation. Furthermore, as shown in Fig.~\ref{fig:scenario2}, our framework {\myHFed} in \emph{Scenario II} outperforms \emph{FedProx} remarkably, as all models at each RSU are pre-aggregated multiple times, which accelerates the convergence of federated learning. The comparison results indicate that our framework can address hierarchical heterogeneity by adapting more tunable parameters which can result in the success of federated learning in C-ITS. 

\begin{comment}


\setlength{\tabcolsep}{3pt}
\begin{table}[t]
%\bgroup\footnotesize
\begin{threeparttable}
\caption{\centering Overview of the comparison results \Crs{tbd. depending on time}}
\label{table:dist_est}

\begin{tabular}{C{1.6cm}C{1.3cm}C{1.3cm}C{1.3cm}C{1.3cm}}
\toprule %%%%%%%%%%%%%%%%%%
    
\multicolumn{1}{c}{ \textbf{Approaches}}  & \multicolumn{2}{c}{ \textbf{Scenario I} }& \multicolumn{2}{c}{ \textbf{Scenario II}}\\
 
  &  {Convergence Runtime}  &  {Stability}  &  {Convergence Runtime} &  {Stability} \\
 \midrule %%%%%%%%%%%%%%%%%%
 
{\emph{FedAvg}}  &  {$1$}  &  {$2$}  &  {$5$} &  {$10$} \\
{\emph{FedProx}}  &  {$1$}  &  {$2$}  &  {$5$} &  {$10$} \\
{\emph{HierFAVG}}  &  {$1$}  &  {$2$}  &  {$5$} &  {$10$} \\
{\textbf{{\myHFed}}}  &  {$1$}  &  {$2$}  &  {$5$} &  {$10$} \\

\bottomrule %%%%%%%%%%%%%%%%%%
\end{tabular}
    %\begin{tablenotes}
     %\small
     %\item[1] Draft versions
   %\end{tablenotes}
\end{threeparttable}
\end{table}



\noindent  \textbf{Performance by Tuning Parameters I} 
We first consider the ideal LTE communication environment for up-and downlink for model uploading at each agent in a conventional federated learning system as the baseline. In our framework {\myHFed}, the direct communication between agents and RSUs is achieved by C-V2X, and models are finally uploaded from RSUs to the cloud via Ethernet connection. All connections are stable and we set $\mu_1=0$, $\mu_2=0$. 

Through additional pre-aggregations (i.e. $LAR>1$) in RSUs via C-V2X communication, the Roadside models are able to be trained federally there before aggregation in the cloud. Fig.~\ref{fig:LAR_Varied} shows, the testing accuracy of the global model is increased around $2\%$ when $LAR>1$ and the training time is reduced correspondingly.

Then, we consider additional heterogeneity in our federated training environment. We set $CSR<1$, i.e. not all traffic agents are connected to the RSUs continuously. Due to the unstable connections, the traffic agents can not always join the federated learning process. Each RSU can receive the models from different partitions of agents, which can cause the global aggregation results in cloud unstable. 

As the first column of Fig.~\ref{fig:communication} shows, the concussion of testing accuracy increases, when only $10\%$ ($CSR=0.1$) traffic agents can be connected to their RSUs successfully. We can also find a small stable connection time period can raise the concussion frequency. To reduce this effect, we tune the parameters $\mu_1$ and $\mu_2$ in {\myHFed}. 

Both $\mu_1$ and $\mu_2$ have an impact on the federated training process but to a different extent. The third row of Fig.~\ref{fig:communication} shows clearly that the large $\mu_2$ can extremely cancel the concussion when the training process is converged, though the convergence is slowed down and mean squared errors (MSE) to centralized learning results are raised. Thus, we note that the parameters $\mu_1$ and $\mu_2$ should be tuned smaller with a good communication environment. When the connections are unstable or the communication channel is busy, larger $\mu_1$ and $\mu_2$ can ensure the success of the federated model training.

%todos for Rui
% x axis in seconds
% seond col should be added
% 



\Crs{Experiment II: FedYolo on FedKITTI}

To evaluate the performance in real traffic application scenarios...
FedAvg vs FedProx vs HierFAVG vs HHtFed
approach 1 LCR = 1 with mu=0
approach 2 LCR = 3 with mu1 and mu2

\end{comment}