\section{Federated Learning Methods}
\label{sec:method}

Federated learning can enhance the pre-trained DNN model for intelligent traffic applications since it can further improve the model with broad data collected by public traffic without affecting user data privacy. Considering the V2X communication networks for federated learning, the DNN models from agents can be uploaded via direct communication. Thanks to the high-defined sensors at intelligent infrastructure, the highly accurate perception are achieved and can be set for 3D digital twin services, which can be deduced into sensor raw data from a global coordinate system and taken as labels for supervised federated learning implementation.

As a baseline method, we first employ FedAvg~\cite{pmlr-v54-mcmahan17a} without dropping any clients in the hierarchical federated learning framework. For an RSU $k$, the aim is to minimize: 

\begin{equation}
    \label{Eq:rsu-fedavg}
    \min_{w} f_k(w) = \sum_{i \in \mathcal{P}_k^t} \frac{n_{i,k}}{n_k} F_{i,k}(w), %= \mathbb{E}_k[F_{i,k}(w)],
\end{equation}
where $\mathcal{P}_k^t$ is a partition of traffic agents selected at time point $t$ at RSU $k$ and $n_k$ is the total number of data points in all traffic agents selected by the RSU federated learning orchestrator. The agent $i$ trains with $n_{i,k}$ data points and $F_{i,k}(w)$ is averaged optimized objective in the agent $i$ with respect to non-convex DNN optimization.
If $K$ RSU train a DNN model jointly, we can extend the objective form in (\ref{Eq:rsu-fedavg}) as:

\begin{equation}
    \label{Eq:rsu-fedavg-ext}
    \min_{w} h(w) = \sum_{k=1}^{K} \sum_{i \in \mathcal{P}_k^t} \frac{n_{i,k}}{n_k} F_{i,k}(w) % = \mathbb{E}_{i,k}[h_k(w)],
\end{equation}

To address the individual heterogeneity in each layer of the federated learning system, as analyzed in Sec.~\ref{sec:problem_formulation}, we employ the $\gamma_k^t$-inexact solution~\cite{li2020federated} with multiple proximal terms in the loss function of the hierarchical federated learning. The  aim of the traffic agent $i$ connecting RSU $k$ is then to minimize:

\begin{equation}
    \label{Eq:prox-hfl}
    \min_{w_{i,k}} h_k(\mathord{\cdot}) = F_k(w_{i,k}) + \sum_{l=1}^{L} \frac{\mu_{k,l}}{2} ||w_{i,k}-w_l||^2
\end{equation}
where $L$ is the number of aggregation layers. We note $\mathcal{M} = \{\mu_{k,l}|k=1,2,...,K, l=1,2,...,L\}$ as a set of main parameters in \ref{Eq:prox-hfl}. In vehicular networks, $L\,=\,2$. When $l\,=\,1$, $\mu_1$ indicates the heterogeneity of data from traffic agents and $w_1$ is the current weight of DNN model in the connected RSU. For $l\,=\,2$, $\mu_2$ indicates the heterogeneity of models across RSUs and $w_2$ is the current model weight in the traffic cloud.\\
To simplify the notation, we define 
\begin{equation}
    \label{Eq:wdef}
    \begin{split}
        &\textit{for an RSU $k$: }  w_k \eqdef w_{l\,=\,1}, \\
        &\textit{for the traffic cloud: }  w \eqdef w_{l\,=\,2}.
    \end{split}
\end{equation}
We can rewrite (\ref{Eq:prox-hfl}) as a double-layer aggregation system in accordance with the application of hierarchical federated learning in vehicular networks as
\begin{equation}
    \label{Eq:HtHFed}
    \min_{w_{i,k}} h_k(\mathord{\cdot}) = F_k(w_{i,k}) + {\frac{\mu_{k,1}}{2} ||w_{i,k}-w_k||^2} \\ 
    + {\frac{\mu_{k,2}}{2} ||w_{i,k}-w||^2},
\end{equation}
where for each RSU $k$, the set of main parameters is $\mathcal{M}_k = \{\mu_{k,l}|l=1,2\}$.


The method defined in (\ref{Eq:HtHFed}) reduces the effect of individual heterogeneity of data in different layers by setting the parameters $\mu_{k,1}$ and $\mu_{k,2}$ in $\mathcal{M}_k$, which can stabilize federated optimization through adding the additional trailers in the loss. Though the multiple proximal terms compared with a single term might slow down the convergence of training~\cite{li2020federated}, it provides more tunable parameters to separately consider the heterogeneity among RSUs and across agents in one RSU, which will be evaluated in Section~\ref{sec:evaluation}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{comment}


\begin{algorithm}[ht]%Consider splitting the algo into server and client parts or use another template.
 \caption{\raggedright 2-layer-FedAvg} 
 \label{alg:2-layer-FedAvg}
 \begin{algorithmic}[1]
 \renewcommand{\algorithmicrequire}{\textbf{Input:}}
 \renewcommand{\algorithmicensure}{\textbf{Output:}}
 \newcommand{\algorithmicbreak}{\textbf{break}}
 \newcommand{\BREAK}{\STATE \algorithmicbreak}
 
 \REQUIRE {{$\hat{R}_{c,t-1}$, $\{\mathcal{D}_{i,t}|i \in \mathcal{N}_{c,t}\}$, $\hat{x}_{c,t-1-T^{(\rm II)}:t-1}$}, $P_{c,t-1}$}
 \ENSURE  {$\{\hat{R}_{i,t}|i \in \mathcal{N}_{c,t}\}$}
    \\ \textit{Traffic agent $i$ connecting to RSU $k$}:
    \WHILE{Stopping learning not received}
        \IF{$w_{k}$ is received}
            \STATE $w_{i,k} \gets w^t$
        \ELSE
            \STATE $w_{i,k} \gets \eta\nabla \mathcal{L}(w_{i,k})$
        \ENDIF
    \ENDWHILE
    \STATE Sends $w_{i,k}$ to RSU
    \\ \textit{RSU $k$ connecting to traffic cloud}:
    \WHILE{Stopping learning not received}
        \IF{$w$ is received}
            \STATE $w_{k} \gets w$
        \ENDIF
        \IF{$t > t_0 + \frac{1}{f_1}$}
            \STATE $t_0 \gets t$
            \STATE Sends request and receives the $w_{i,k}^t$ from connected agents %Change into math symbol
            \STATE $w_{k} \gets\sum_{i=1}^{I} w_{i,k}$
            \STATE Sends $w_{k}$ to traffic agents
        \ENDIF
    \ENDWHILE
    \\ \textit{Traffic cloud}:
    \WHILE{$t < t_0 + \frac{1}{f_2}$}
        \STATE $t_0 \gets t$
        \STATE Sends request and receives the $w_{k}$ from RSUs
        \STATE $w \gets\sum_{k=1}^{K} w_{k}$
        \STATE Sends $w$ to RSU
        \IF{$t > t_{end}$}
            \STATE Sends request to RSUs for stopping learning 
            \BREAK
        \ENDIF
    \ENDWHILE
    
 \end{algorithmic}
 \end{algorithm}
 
\end{comment}