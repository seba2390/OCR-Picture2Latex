\section{Heterogeneity Problems}
\label{sec:problem_formulation}



\noindent  \textbf{Heterogeneity in datasets.} 
We assume that RSUs are equipped with homogeneous computational capabilities and benefit from a stable communication quality. Particularly, the traffic cloud reliably receives the aggregated models from all RSUs. However, the models generated in each RSU are based on a dataset that is notably different from other RSUs due to the individual position of each RSU. For instance, the model in an RSU equipped at a highway is trained with data in highway scenarios, where the involved dynamic objects in the datasets are typically passenger cars or trucks that move fast in simple road typologies. In contrast, there are various types of dynamic objects with low speed in complex road networks in the datasets, when an RSU is located in cities. In addition, even urban scenarios in various locations can be marked distinct, e.g., at crossings, roundabouts, etc. Therefore, we believe that the models generated across RSUs are based on Non-IID datasets. Besides, the diverse average traffic flows cause the unbalanced agent number at RSUs, which can finally impact the learning results while aggregating the models at the traffic cloud. 

\noindent  \textbf{Heterogeneity in communication.} 
The direct communication between traffic agents and RSUs achieves local aggregation (or pre-aggregation) in federated learning at each RSU, where traffic agents and the RSU represent the clients and server in non-hierarchical federated learning. Then, before one global aggregation, each RSU can implement several \emph{Local Aggregation Rounds} ($LAR$), which accelerates the learning process by means of pre-aggregation between two global aggregation steps. Through sidelink communication in Cellular-V2X, %5G-NR, 
the models at the RSU can be pre-aggregated up to 50 times, if we aggregate models from RSUs every $1$\,s globally in the cloud. In fact, due to the various other traffic services in V2X communication, we need to set the $LAR$ in a proper range with respect to the current \emph{Quality of Service} (QoS) at each RSU. Additionally, according to the current data traffic in the communication channel, not all traffic agents can continuously upload their models to the RSUs. Some traffic agents might need to allocate the communication resource for other tasks with higher priority. The metric \emph{Connection Success Ratio} ($CSR$) indicates the current connection situation specifically for federated learning at each RSU $j$:
\begin{equation}
    \label{Eq:measurement}
    CSR_j = \frac{N_{j,{connected\: agent}}}{N_{j,{participant}}}, (CSR \in [0,1]),
\end{equation}
where $N_{j,{participant}}$ is the number of all connected agents at RSU $j$ and $N_{j,{connected\:agent}}$ is the number of successfully connected agents, which stably upload the models in a predefined duration, specified as \emph{Stable Connection Duration} (SCD) in seconds.

\noindent \textbf{Heterogeneity in computation.} 
Apart from communication, the available computation resources can also vary in each traffic agent. This leads to the fact that not all traffic agents finish the expected local training epochs $E$. Normally, federated learning requires each agent to finish local training by the same $E$. However, the different computational capabilities and currently occupied computational resources cannot always satisfy the request on $E$ within a specific aggregation frequency, which causes further heterogeneity. % in federated learning. 
\emph{Full-task Success Ratio} (FSR) indicates the ratio of traffic agents that can finish the requested $E$ in time. Similarly, $FSR$ can also be individual in each FSR. Note that if an agent cannot even finish one epoch, its results will be discarded, which has the same effect as a failed connection. Thus, in this paper, we will address heterogeneity especially considering varied $CSR$. All heterogeneity metrics are summarized in Tab.~\ref{table:comparison}
