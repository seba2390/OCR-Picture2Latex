
When grasping an object at a location away from the object's center of gravity, the object undergoes rotation about the gripping point due to the moment applied by gravitational force. We show that it is possible to detect this rotation and measure it in real-time using images from the GelSight sensor. The GelSight sensor outputs tactile images with markers labeled on them. These markers trace the object's movement when the sensor is in quasi-static contact with the object, hence providing high-resolution localized information in the contact region. When rotation occurs, it gives rotational patterns of markers on the GelSight surface. We analyze these patterns to determine the Center of Rotation (COR) and eventually calculate the rotation angle. We then show how this method can help a robot to reach a stable grasp pose.

\subsection{Contact Rotation Measurement Overview}
% \begin{figure}[!htbp]
%     \centering
% `    \includegraphics[width=\linewidth]{imgs/rotation_pipline.png}
%     \caption{rotation detection flow chart. After the tactile sensor gets in touch with the object, our algorithm detects the stable contact, whether the rotation happens and quantifies the rotation.}
%     \label{fig:rotation_pip}
% \end{figure}

% We propose a model based algorithm to detect rotation and measure both its magnitude and orientation by processing the time sequence of the GelSight images. When rotation occurs due to wrong grasp conditions, the markers on the GelSight image exhibit rotational patterns as they trace the motion of the object’s part that is in contact with the GelSight sensor. The algorithm analyzes these rotational patterns to quantify the rotation of markers and also give its orientation(CW/CCW). 
Our algorithm proceeds through three steps: 1) Object Contact Detection: Initially, the system keeps looping to check the occurrence of contact between the sensor surface and the object. 2) Detection of Rotation Onset: It then tracks the markers on the GelSight image and keeps checking if rotational patterns occur on the surface. 3) Rotation Angle Measurement: If rotation starts, it then analyzes the marker motions and calculates the Center of Rotation(COR). The algorithm then gives a measure of the angle of rotation and its orientation about the calculated COR. The following subsections detail each step of the algorithm. 

\subsection{Object Contact Detection}
Precise contact detection is an essential pre-processing step for rotation detection. When a grasp occurs, the markers that contact the object can directly trace the objects' motion. In the subsequent frames, these markers, designated as contact markers, can indicate whether the object undergoes rotation or remains stable. The remaining markers are designated as non-contact markers.

The grasping process takes time to complete and the contact area on the gel is dynamic. We first identify a stable contact state of the grasp and then the contact region in the image corresponding to it. To accommodate different situations, we determine a stable grasp state in two ways: soft stable contact and hard stable contact. After 10 frames from the start, if the change in the frame-frame markers' motion magnitude is less than a set threshold, we say the contact is a soft stable contact. However, in some cases, there is hardly a stable contact stage and we designate the frame after 30 frames($\sim$1sec) as the hard stable contact.
% The contact area is dynamic during the grasp; therefore, we wait until the contact becomes stable to detect the contact region. We set two timestamps, one as soft stable contact and the other as hard stable contact, to accommodate different situations. After 10 frames from the start of the contact, if the change in markers' motion magnitude from the last frame is less than a small threshold, we say the contact is stable, namely, a soft stable contact. However, in some cases, there is hardly a stable contact stage in which we set 30 ($\sim$1sec) frames, a hard time stamp, as stable contact detection after the start of contact. 

By the time the contact becomes stable, the illumination changes across the three color channels as seen in Fig.~\ref{fig:contact}(a) and Fig.~\ref{fig:contact}(b). We take the maximum of illumination change across R, G, and B channels at all pixel locations, as in Fig.~\ref{fig:contact}(c), and then filter the resulting image to get the contact region as in Fig.~\ref{fig:contact}(d).
% Meanwhile, we obtain the contact markers with thresholded radial motion in the contact region.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{imgs/contact.jpg}
    \caption{Contact area detection.
    Comparing (b) with (a), contact happens mostly in the bottom part of the image as the difference shown in (c), and (d) is our detected contact area.
    }
    \label{fig:contact}
    \vspace{-5mm}
\end{figure}

\subsection{Detection of Rotation Onset}
After detecting contact, the algorithm tries to identify the markers' rotational patterns in the subsequent frames. If no such patterns are detected, the grasp is considered stable. Otherwise, we process the patterns to measure rotational displacement.

We calculate the relative motion change and angular change of the marker vectors w.r.t contact markers in any given frame after contact. If either of these changes crosses a fixed threshold, we say that the object is in rotation. The object's translational motion will be considered separately in the following sections.
For instance, in Fig.~\ref{fig:rotation_onset}(a), for a certain marker, we check the value of angle $\theta$ between the motion from the initial frame $M_0$ to the stable contacting frame $M_t$ as $d_1$ and the motion from the initial frame $M_0$ to the current frame $M_c$ as $d_2$. We check the magnitude of the motion from the stable contact frame to the current frame as well. If one of them goes beyond a threshold, we consider that as the onset of rotation, and start to measure the rotation angle.
\begin{figure}[t]
    \begin{subfigure}{0.45\linewidth}
        \includegraphics[width=\linewidth]{imgs/rotation_onset.png}
        \caption{Rotation onset detection}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.45\linewidth}
        \includegraphics[width=\linewidth]{imgs/COR.png}
        \caption{Rotational pattern of marker motions with their COR}
    \end{subfigure}
    \caption{(a) Detection of rotation onset: the angle difference $\theta$ between the motion vector $d_1$ (marker motion from initial frame $M_0$ to the contact frame $M_c$), and the motion vector $d_2$(marker motion from initial frame $M_0$ to the current frame $M_t$) indicates the rotation onset. (b) Detection of center of rotation: the normals of rotating motion vectors intersect at the center of rotation.}
    \label{fig:rotation_onset}
    \vspace{-2mm}
\end{figure}
% \vspace{-5mm}

\subsection{Rotation Angle Measurement}
In this step, we detect the location of the center of rotation (COR) and measure rotational displacement about this center. We take the motion of markers from the initial frame and draw normals to each of them. The intersection point of these normal vectors would ideally be the COR. However, due to noise, they might not intersect at a single location. Hence, we formulate a least-squares solution to obtain the best estimation of COR. 

Illustrated in Fig.~\ref{fig:rotation_onset}(b), assuming the COR is $C = (x_c,y_c)$, and one motion vector is from $M_0 = (x_0,y_0)$ to $M_t = (x_t,y_t)$, then the motion vector is perpendicular to the direction joining the COR and the mid point of the motion vector $X_m = (x_m,y_m) = ((x_0+x_t)/2,(y_0+y_t)/2)$. Under the ideal situation with no sensor noise, the perpendicularity can be satisfied as

\begin{equation}
\begin{split}
    & (x_c - x_m, y_c - y_m) \perp (x_0 - x_t, y_0 - y_t) \\
    \Rightarrow & (x_c - x_m)(x_0 - x_t) + (y_c - y_m)(y_0 - y_t) = 0 \\
    \Rightarrow & x_c + \frac{y_0 - y_t}{x_0 - x_t} y_c = x_m + \frac{y_0 - y_t}{x_0 - x_t} y_m
\end{split}
\end{equation}
Therefore, the best estimation of COR $\hat{C} = (\hat{x_c}, \hat{y_c})$ can be solved with a set of motion vectors by forming in normal equation $A x = b$ as
\begin{equation}
\begin{split}
   & \left[\begin{matrix}
   1 & \frac{y^{(1)}_0 - y^{(1)}_t}{x^{(1)}_0 - x^{(1)}_t} \\
   1 & \frac{y^{(2)}_0 - y^{(2)}_t}{x^{(2)}_0 - x^{(2)}_t} \\
   ... \\
   1 & \frac{y^{(n)}_0 - y^{(n)}_t}{x^{(n)}_0 - x^{(n)}_t}
  \end{matrix} \right] 
  \left[\begin{matrix}
  x_c \\
  y_c
  \end{matrix} \right] 
  = \left[\begin{matrix}
   x^{(1)}_m + \frac{y^{(1)}_0 - y^{(1)}_t}{x^{(1)}_0 - x^{(1)}_t} y^{(1)}_m \\
   x^{(2)}_m + \frac{y^{(2)}_0 - y^{(2)}_t}{x^{(2)}_0 - x^{(2)}_t} y^{(2)}_m \\
   ... \\
   x^{(n)}_m + \frac{y^{(n)}_0 - y^{(n)}_t}{x^{(n)}_0 - x^{(n)}_t} y^{(n)}_m
  \end{matrix} \right] \\
\end{split}
\end{equation}
And the rotation center C is estimated by 
\begin{equation}
    x = (A^{T}A)^{-1}A^{T}b
\end{equation}

The angle of rotation is calculated as the angle $\theta$ of\\
 $<\overrightarrow{X_c M_{0}},\overrightarrow{X_c M_t}>$:
\begin{equation}
    \theta = arccos(\frac{<\overrightarrow{X_c M_{0}},\overrightarrow{X_c M_t}>}{|\overrightarrow{X_c M_{0}}||\overrightarrow{X_c M_t}|})
\end{equation}

However, the angle calculated for each maker is an absolute rotation angle value, which does not have direction. We calculate the orientation of rotation by taking the moment of the motion w.r.t the COR. If the sign of the moment is positive, it is counter-clockwise and clockwise if negative. 

The nature of the gel and its interaction with the objects can result in noise with some markers exhibiting orientation in the opposite direction to the object's rotation direction. Therefore, we calculate the moments about COR for all the markers and do a majority vote to decide the final orientation of rotation. If a rotation occurs, one orientation group should dominate the others. However, if an object is grasped and remains static, it could be recognized as a rotation case due to the markers' radial motion. In such a scenario, the number of markers in both groups are similar, which can distinguish false-positive cases from the real rotation cases with dominant numbers as described above.

Clockwise rotation and counter-clockwise rotation cases can be seen in Fig.~\ref{fig:COR}(a) and (b), where we find the CORs for contact markers. A grasp can be considered stable in two cases: 1) no rotation of the object occurs, 2) low rotation with small rotation angles, set as 5$^{\circ}$ in our experiments.
\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{imgs/rotation_direction.png}
    \caption{Three types of contact rotation cases which can be measured from the rotational patterns of the black markers in the GelSight images.}
    \label{fig:COR}
    \vspace{-2mm}
\end{figure}

\subsection{Special Cases}
\subsubsection{Translational Displacement}
One special case is the translational displacement during grasping. When the gripper lifts the object, the object undergoes translational movement due to the imbalance of friction and gravity if there is insufficient normal contact force. It is natural to increase the normal force~\cite{shear-slip} to avoid this displacement. Markers' motion are mostly similar in direction for translation and dissimilar for rotation. We utilize this fact to distinguish between translational and rotational displacement in our analysis. We apply SVD decomposition on the motion of markers. If there is only one dominating direction, which reflects in only one large singular value, we say the motion is translation.

\subsubsection{Small Contact Area}
Another special case is when the contact happens only in a small area with objects that have irregular shapes. As seen in Fig.~\ref{fig:contour}(b), the contact area is not flat but small with rich geometry, so it is hard to track markers inside the contact area. Alternatively, we measure the rotation by locating and tracking the contact area's contour. Specifically, we track the relative angular change of the contour's principal axis because the rotation of the area can be approximated by the rotation of the principal axis. Based on the construction of the tactile sensor, the lights come from the surrounding of the gel pad. So the contact area with huge gradients' color value is more intense. We convert the RGB format into HSV format, filter and smooth the Value channel to extract the main contact area as shown in Fig.~\ref{fig:contour}(c).


% Another special case is the contact happens only in small area when the objects have irregular shapes. As seen in Fig.~\ref{fig:contour}(b), the contacted area is not flatten but small and with rich geometry, so that it is hard to track the markers inside the contact area. Alternatively we track the rotation by locating and tracking the contact area. Here we extract the contour of the main contact area, and track the motion of the contour to estimate the rotation. Specifically, the angle changing of the direction of the contour's principal axis is the rotational displacement because the rotation of the area can be simplified as the rotation of the principal axis. Based on the construction of tactile sensor, the lights come from the surrounding of the gel pad. So the contact area with huge gradients' color value is more intense. We transfer the RGB color channel to HSV color channel, and threshold and smooth on the Value channel to extract the main contact area as shown in Fig.~\ref{fig:contour}(c).

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{imgs/contour.jpg}
    \caption{Rotation detection on contacting with irregular area. (a) The case of grasping a wrench. (b) Gelsight image for grasping. (c) We track the the contact area's long axis of the contour.}
    \label{fig:contour}
    \vspace{-2mm}
\end{figure}

\subsection{Closed-loop control based on rotation measurement}

We build a regrasp control framework to utilze the GelSight feedback to find a stable grasp position of an object. We setup an RGB-D camera to estimate the object's size as a reference for regrasping adjustment step size.
%which is used as a heuristic parameter to adjust the step size for regrasping. And it is incorporated into a closed-loop control algorithm to search for the stable grasp pose on the object based on the rotation measurement feedback. 
%The following two sections talk about how to estimate the object's size as well as how to control grasp-regrasp to reach a stable grasp at the object's center of gravity.

\subsubsection{Object Size Estimation}

We use the real-scale point cloud generated from the RGB-D data to estimate the object's size, as shown in Fig.~\ref{fig:obj}. Given our experimental setting, where the background is a clean and flat table plane, the plane and the object can be segmented out with RANSAC simultaneously. We only estimate the length of the principal axis along the object because the gripper moves along that axis. To find the principal axis, we project the 3D point cloud of the object to the 2D table plane found from the RANSAC, and apply SVD decomposition to find the principal axis with the largest singular value which is the long axis. We take the 95\% longest distance to the center point as the half of length to tolerate the potential outliers in the point cloud.
\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{imgs/objest1.png}
    \caption{Object length measurement from RGB-D data. The 3D point cloud can be generated from a pair of RGB-D images such as (a) and (b). (c) shows the segmentation of table plane (in blue) and object (in green). We then estimate the object's length from the length of the principal axis of 2D projected point cloud of object on the table plane}
    \label{fig:obj}
    \vspace{-2mm}
\end{figure}
\subsubsection{Closed-loop Control} \label{closedloopcontrol}

% With the knowledge of the length of the object, we are able to find the geometry center of the object, and apply the first grasp, since for even distributed-mass object, the geometry center is usually the center of gravity. However, it is unable to predict the mass distribution purely from the vision. If the first grasp is not on the center of gravity, the rotational displacement will happen and we have to regrasp the object based on the tactile rotation measurement feedback. We fix the movement of the gripper along the long axis of the object. If the grasp is detected as unstable, the gripper goes forward and regrasps the object with clockwise rotation detection, otherwise backward. The step size decreases with a certain factor $\alpha$ with more grasping applied where $\alpha < 1$. We keep regrasping the object with the adjusted step size till it reaches a stable grasp. We also try different initial positions away from the midpoint and test the regrasping framework.

With the object's length $L$ measured from vision, we design the regrasp algorithm as following. First, the robot starts with an initial grasp near the geometric center of object. If that grasp is ascertained as rotation, the robot will release the object and move along the object's principal axis to the updated grasp location. The initial regrasp step size is $0.4L$, and then reduced to $\frac{1}{6}L$ for the later regrasps. The initial and adjusted step sizes are heuristic but this configuration is 
verified to be effective in the experiments where the robot could reach a stable grasp pose in a few steps. If the rotation orientation changes during two regrasps, it means that the robot has passed the Center of Gravity. The robot then takes the updated step in the backward direction, and we know that the Center of Gravity is between the two grasp locations. We will then further reduce the step size by a factor of 0.5 for a finer adjustment. 
With this coarse-to-fine grasp adjustment design, our algorithm can quickly find the Center of Gravity within a few grasp trials.
% We build a regrasping framework to utilize GelSight sensor’s feedback to find a stable grasp pose of an object. For each object we start at one edge of the centroidal axis and grasp the object with incremental steps along the axis. The robot grasps the object at each grasp location, and in parallel, the GelSight sensor analyzes each frame according to the above proposed rotation detection algorithm. If the rotation is detected, the robot will place the object back, go to next grasp location and regrasp the object. If no rotation detected, we consider it as a stable grasp and terminate the grasping.



% The robot grasps the object at each grasp location and attempts to lift the object to a maximum height of 5cm. In parallel, the GelSight sensor analyzes each frame according to the above proposed rotation detection algorithm. We designate a grasp to be stable if it doesn't rotate or if the angle of rotation is within 5 degrees. 
% If the algorithm detects the grasp to be unstable during the lifting phase, it raises a preempt signal to the robot through ROS.
% The robot then places the object back. If the GelSight sensor doesn’t detect rotation until 5 cm, the robot waits for 3s to check for rotation detection. If rotation is not detected at this stage, the robot further lifts the object by 3cm and holds it for 5s to indicate that stable grasp pose has been achieved. The grasping terminates once the stable grasp pose is achieved. 