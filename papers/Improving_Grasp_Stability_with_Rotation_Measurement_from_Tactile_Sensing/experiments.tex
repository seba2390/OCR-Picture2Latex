\begin{figure*}[t]
\includegraphics[scale=.52]{imgs/objects.png}
\caption{Experimental objects with different shapes, mass distributions and materials. For effective evaluation, we include objects of irregular shapes like spatula, wrench, etc. as well as soft objects like sponge bar, tooth paste, etc.} 
\label{fig:objects}
\end{figure*}

We conduct two sets of experiments: offline grasp experiments on contact rotation detection, and closed-loop regrasp experiments based on the rotation measurement results. In the offline experiments, we grasp and lift different objects at different points in a  dataset and then evaluate our rotation measurement algorithm by comparing against the ground truth. In the closed-loop regrasp experiments, we integrate the feedback of the GelSight sensor to update grasp locations until a stable grasp is attained. 

% \subsection{Experiment Setting}
We set up a 6 DOF robot arm (UR5e by Universal Robots) and a parallel jaw gripper (WSG50 by Weiss) to perform grasps. We mount a GelSight sensor on one side and a 3D printed finger(PLA) on the other side. An Azure Kinect camera faces the object from a top view. We then place an external USB camera on the object's side view to obtain ground truth of objects' rotation angles and orientation. The setup is shown in Fig.~\ref{fig:robot} (a).
\begin{figure}[!htbp]
    \centering
    \includegraphics[width=\linewidth]{imgs/setup.jpg}
    \caption{(a): Experimental Setup. A Gelsight tactile sensor is mounted on the gripper, an RGB-D camera is used to estimate the size of object and an external camera on the side measures the ground truth of object rotation from the Aruco Markers attached to the objects. (b) and (c): Ground truth rotation measurement from the external camera. The orientation of Aruco markers in the initial frame and current frame are marked as red and yellow arrows respectively.}
    \label{fig:robot}
     \vspace{-2mm}
\end{figure}

\subsection{Contact Rotation Measurement on Offline Data}
\textbf{Dataset Collection: } 
We collect data about the rotation of objects to test the rotation detection algorithm's performance. Data includes time-synchronized image sequences from both the GelSight sensor and an external USB camera. The USB camera is placed on the object's side view to capture the Aruco markers and further measure the rotation of the objects as the ground truth. As illustrated in the Fig.~\ref{fig:robot} (b) and (c), two Aruco markers are attached parallel to the object's long axis. Under this setting, the rotation is 2D on the plane of the Aruco markers. This is consistent with the rotational displacement detected from the Gelsight, which is attached on a parallel gripper where only 2D rotational displacements take place in between. Given the real size of the Aruco markers and intrinsic parameters of the camera, the pose of Aruco markers can be estimated using the Perspective-n-Point (PnP) algorithm. We set the initial direction from one marker to another in the first frame, and then calculate the direction change on the Aruco markers' plane as the rotation ground truth for the following frames.
If the algorithm detects rotation and the angle measurement is above 5$^{\circ}$, we consider it an unstable grasp.

We perform 142 grasps on the first 14 objects, as seen in Fig.~\ref{fig:objects}, at multiple locations with gripping speeds varying from 30-50 mm/s and a gripping force of $\sim$30 N. The robot goes uniformly over different locations on the objects along their centroidal axis and grasps at each point. At each grasp, the robot lifts the object upwards to a height of 5 cm and places it back. While the robot lifts the objects, the GelSight sensor and the external USB camera record images at 30 fps. 
% The USB camera is placed parallel to the plane of the GelSight sensor's axis to capture planar rotations of objects. Each object is attached with Aruco Markers, generally at two different ends. When an object rotates, these two markers trace the object's motion. We then measure the rotation of the objects in the world frame by tracking the two markers' orientation change. 
In total, we collected 142 grasp data points with 98 of them labeled as rotational cases and 44 labelled as stable grasps, including translational slip cases using Aruco markers' relative positions. 
% Among 140 grasp data points, rotation was detected \wenzhen{`detected' is weird. It sounds like this is the detection results from your algorithm. If you meant the groundtruth label, you should make it clear these are rotation cases}in 99 cases, and in the rest of 41 cases, the objects were either grasped successfully or with translational slip, which we do not consider as rotational failure.    
% \caption{Rotation onset time delay in  offline experiments. The absolute time difference of rotation onset between the GelSight detection and the ground truth is calculated.}\wenzhen{What is this caption part?}


\textbf{Accuracy of rotation detection: }
We compare the measured rotation angles from the GelSight images with the ground truth of objects' rotation measured from the external camera. Some examples are shown in Fig.~\ref{fig:result}. The positive value of angles in the diagrams represent clockwise rotation, and negative values represent counter-clockwise rotation. We also evaluate the rotation onset detection delay which can be seen as vertical lines in the figures. 
% We can tell our algorithm can detect the rotation starting time and the rotation angles closely to the ground truth values from the figures.
\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{imgs/new_result.jpg}
    \caption{
    Examples of rotation angle measurement in the offline grasp experiments. x-axis represents the time (in seconds) of experiments and y-axis describes the rotation angles (in degrees). 
    % The compared results between rotation detection from GelSight sensor and ground truth rotation from external camera. 
    The cases in row 3 are one stable grasp with a small rotation angle and a rotational case with irregular-shaped contact area. 
    % Row 3 left shows a static grasp, so the rotation angles are close to zero; Row 3 right shows a failure example, where the contact area is too small and there are not enough markers in the contact area to detect rotation.
    }
    \vspace{-4mm}
    \label{fig:result}
\end{figure}

To numerically evaluate all rotation measurement results, we estimate the mean of the absolute error between the ground truth and the measured angle for each lifting frame in each grasp trial. The lifting frame starts from the rotation onset time stamp and ends when the object detaches from the tactile sensor. The overall average error in rotation angle measurement is 3.96$^{\circ}$. However, from our observation during experiments, for cases where the rotation angles are large, the angles measured are smaller than the ground truth values. This comes from the rotational slip occurring between the contact of the gel's surface and the object. This can also be attributed to the adhesion between the gel and the object, which impedes the rotation. Unfortunately, due to the design of the sensor, these two scenarios are unavoidable. Therefore, we also evaluate the angle errors for the rotation angles under 10$^{\circ}$ to eliminate these effects. For rotation angles under 10$^{\circ}$, the average angle error is 2.69$^{\circ}$. The statistics of average angle error for all experiments can be seen in Fig.~\ref{fig:angle_all}. When contour tracking is used in angle measurement, the average angle error is 3.33$^{\circ}$ for the general case and 2.33$^{\circ}$ for the case of rotation measurement under 10$^{\circ}$. As the numbers imply, both contour tracking and marker motion tracking give similar performance in measuring the rotation angle.  Moreover, we evaluate the average time delay for rotation onset detection, as in Fig.~\ref{fig:onset_all}. The overall error is 4.22 frames, which is around 0.14 seconds. 


%Comparing the two different methods proposed in this work--tracking the marker's motion patterns versus tracking the contour of the contact area--the numerical results with contour tracking are 3.33$^{\circ}$ for the average error of rotational angle measurement and 2.33$^{\circ}$ for the average error of rotation angle measurement under 10$^{\circ}$. 
%The errors are slightly lower than the overall angle measurement's error, so we would consider these two methods work equally well.
% Since the rate of Gelsight image collection is 30 fps, we set 1 second which is 30 frames as threshold to evaluate how quick our rotation onset detection is. From the diagram, most cases are under 1 second which means the reaction to rotation is quick. 

For classification between stable and rotational cases, 
% the results are shown in Table.~\ref{table:offline_result}. 
in general, 134 cases out of 142 cases are classified correctly, where the success rate is 94.37\%. Among them, 41 successful classifications out of 44 are stable cases with a success rate of 93.18\%, and 93 successful classifications out of 98 are rotational cases with a success rate of 94.90\%.
% \begin{table}[]
% \begin{tabular}{|l|l|l|l|}
% \hline
%                         & Total cases & Rotational cases & Stable cases \\ \hline
% Total \#                & 142         & 98               & 44           \\ \hline
% Corrected classified \# & 134         & 93               & 41           \\ \hline
% Accuracy                & 94.37\%     & 94.90\%          & 93.18\%      \\ \hline
% \end{tabular}
% \caption{Results for offline experiments.}
% \label{table:offline_result}
% \end{table}

Most failures come from the imperfect grasping where the contact area is located at the margin of the image. As a result, the contact is only partially visible, and even disappears after lifting, making it hard to track either the markers or the contours.
% Most failures come from small contact area or irregular object shape, which leads to irregular contact area. Curvatures can result in random marker motions on the gel, confusing the algorithm to classify unstable rotation and stable grasp. In such cases, the tracking of markers is not sufficient for accurate rotation detection. One typical failure case when the cup (object 4 in Fig.~\ref{fig:robot}) is grasped with its handle is shown in the last plot in Fig.~\ref{fig:result}. However, the results do show the algorithm's robustness in dealing with rotations of flatter objects.

% , but it is challenging to implement objects with complex shapes with curvatures. The curvatures can result in random marker motions on the gel, confusing the algorithm to classify the rotational and stable grasp.
\begin{figure}[!htbp]
    \centering
    \includegraphics[width=\linewidth]{imgs/new_angle.jpg}
    \caption{Rotation angle error in offline experiments. The absolute angular difference between the GelSight measurement and the ground truth value is calculated for each frame in the lifting phase, and the mean of them represents the average angle error.}
    \label{fig:angle_all}
    \vspace{-4mm}
\end{figure}
\begin{figure}[!htbp]
    \centering
    \includegraphics[width=\linewidth]{imgs/1210onset.png}
    \caption{Rotation onset time delay in  offline experiments. The absolute time difference of rotation onset between the GelSight detection and the ground truth is calculated.}
    \label{fig:onset_all}
     \vspace{-2mm}
\end{figure}
\subsection{Closed-Loop Regrasping Experiment}
% We show how the proposed rotation detection algorithm can lead the robot to a stable grasp pose by performing closed-loop regrasping experiments using feedback from the GelSight sensor.

We follow the closed-loop regrasp framework using adaptive step size as proposed in \ref{closedloopcontrol}. For each object, the robot starts near the geometric center of the object. The robot grasps the object at each grasp location and attempts to lift the object to a maximum height of 5 cm. Simultaneously, the rotation measurement algorithm analyzes each frame from the GelSight sensor. If it detects rotation greater than 5$^{\circ}$, it raises a preemptive signal to the robot through ROS. The robot then places the object back on the table. If no preemptive signal is sent till the maximum height of 5 cm, the robot then waits for 3s to check for rotation. If rotation is detected in either of the above two ways, the robot's step size is updated according to the control framework proposed in \ref{closedloopcontrol}. However, if rotation is not detected at this stage, the robot further lifts the object by 3 cm and holds it for 5s to indicate that a stable grasp pose has been achieved. The process is illustrated in Fig.~\ref{fig:location}. We manually label whether rotation occurred in a particular grasp. For a single object, we define a grasping loop as the object being grasped at multiple locations until it reaches the stable grasp pose. We conducted a total of 109 closed-loop regrasping experiments on 18 objects, at gripping speeds varying from 40-50 mm/s and maximum gripping force varying from $\sim$30-60 N. The rotation detection algorithm takes $\sim$0.045 s to process each frame while the manipulation happens.
% The robot grasps an object at a location, places it back if it gets significant rotation feedback from the sensor. It then proceeds to the next grasp location along the centroidal axis of the object and repeats the lift operation. It keeps grasping the object with incremental steps until a stable grasp is detected as illustrated in Fig.~\ref{fig:location}. We manually label whether rotation occurred in a particular grasp. For a single object, we define a grasp loop as the object being grasped at multiple locations until it reaches the stable grasp pose.
\begin{figure}[!htbp]
    \centering
    \includegraphics[width=\linewidth]{imgs/closeloop.png}
    \caption{Closed-loop regrasping experiments. The robot detects rotation from GelSight images when lifting the object, and then adjust grasping location along the object's principal axis. The first row shows the external side view of grasping, and the second row shows the detected rotational/stable grasping from Gelsight.}
    \label{fig:location}
     \vspace{-2mm}
\end{figure}

\textbf{Results: }
% We conducted a total of 87 closed-loop regrasping experiments on 20 objects at gripping speeds varying from 30-50 mm/s and maximum gripping force varying from 10-50 N. The rotation detection algorithm takes $\sim$0.045 s to process each frame while the manipulation happens. \wenzhen{It's better to move those setup of speed and force number before the 'result' part}
In 105 out of the 109 experiments, the algorithm could drive the robot towards the object's centroid and avoid rotations above a set threshold of 5$^{\circ}$, with a success rate of 96.3\%. In 3 cases, the algorithm misclassified rotation as a stable grasp pose, and in 1 case a stable grasp pose as rotation. The primary reason for this failure is that the contacted area is small in the GelSight sensor's FOV during the grasp resulting in very few contact markers being detected. In cases like a pills box with uneven distribution of marbles as shown in Fig. \ref{fig:location}, our proposed method can drive the robot to a stable grasp pose in 2 regrasps on an average. Also, for objects like a spatula or vernier callipers(13, 22 in Fig. \ref{fig:objects}) 
% \wenzhen{point out the object number in Figure 7}
with uneven mass distribution, our method can detect the centre of gravity in 1.5 regrasps on average. These objects are difficult to grasp stably using vision information alone due to their uneven mass distributions; however, our method proves to be efficient in detecting center of gravity for them. On average, the robot took one regrasp to reach the center of gravity for all the experimental objects. The average number of grasps for each object in the closed-loop experiments can be seen in Fig.~\ref{fig:regrasp}. It took a maximum of 3 regrasps to reach the stable grasp pose across the set of objects. 


\begin{figure}[!htbp]
    \centering
    \includegraphics[width=\linewidth]{imgs/regrasp_plot.png}
    \caption{The average number of grasps for each object in the closed-loop experiments. The x-axis represents the object number in Fig.~\ref{fig:objects}, including different configurations of mass distributions of some objects. The y-axis represents the average number of grasps.}
    \label{fig:regrasp}
     \vspace{-2mm}
\end{figure}

% We conducted a total of 88 closed-loop regrasping experiments at gripping speeds varying from 30-50 mm/s and maximum gripping force varying from 10-50 N. We chose the gripping speeds and forces to account for different grasping conditions. The rotation detection algorithm takes $\sim$0.045 s to process each frame while the manipulation happens.
% \wenzhen{I'm curious how did you set the speed and force? Are you choosing them randomly?} \raj{Yeah, the choice is random with the intention to grasp objects at different forces.}
% The results are reported in table~\ref{table:close-loop}. In 74 out of the 88 closed-loop grasping experiments, the algorithm could drive the robot towards the object's centroid and avoid rotations above a set threshold of 5 degrees. In 3 of the closed-loop re-grasping experiments, the algorithm gave output as rotation while the translational slip occurs at the centroid location. We consider these cases as successful as we did not include the translational slip in our analysis. During the regrasping experiments, a total of 363 grasps were performed on several objects. In 273 cases, rotation is observed, and the algorithm could detect and counteract rotation. The algorithm could estimate the orientation of rotation as CW/CCW accurately in all of 273 cases.
% The results also include that even though the object is stable w.r.to the rotational motion, translational slip can also be a cause of grasp failure which is observed in 3 cases. 
% This can be attributed to a variety of factors like the object being too heavy, not enough gripping/clamping force, requires multi fingers, etc. 
% In a total of 363 grasps, in 2 cases, the algorithm gave false output as the grasp being stable even though rotation occurs, and in 8 cases, it gave the output of rotation even though the grasp was stable. One of the common reasons for failure is observed to be the detection of contact markers. When the objects are grasped with less force, and in the case of hollow and smooth objects, the algorithm finds it challenging to identify contact markers. This compounds in the subsequent steps of the algorithm, thus leading to false results. 
% \begin{table}[!htbp]
% \centering
% \begin{tabular}{|l|l|l|l|}
% \hline
% % Metric      &  Value \\ \hline
% Total Experiments & 87 \\ \hline
% Stable Grasp after Regrasping     & 79          \\ \hline
% \end{tabular}
% \caption{Results from closed-loop experiments.}
% % \wenzhen{The labels in the table need to be modified}}
% \label{table:close-loop}
% \end{table}
% \vspace{-5mm}
% \wenzhen{I suggest you remove this table, since it's too siple and the numbers are already reported in the text}% \subsection{Experiment Setting}

