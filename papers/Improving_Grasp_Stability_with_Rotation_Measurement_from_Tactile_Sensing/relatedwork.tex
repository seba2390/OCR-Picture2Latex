\subsection{Grasping and Regrasping}
% \textbf{Grasping}- While, the grasping problem seems to be able to sample an appropriate grasp for a given object, the main work involves many elements like- the shape of the object to be grasped, the surrounding of the object, physical properties of the object, and mainly the gripper being used to attempt the grasp. Furthermore, finding a suitable grasp candidate among the infinite sample space is tedious and challenging, therefore.
% \arpit{Maybe you can talk about model-based grasping. In this line of work, everything about the object is known or assumed like mesh model, mass distribution like \cite{ferrari1992planning} \cite{roy2013robust}}
% \wenzhen{When writing the related work, you should explain how your work is different (better than) from those work. Your aim is to show your work is important and innovative, and you can't expect the audience to figure it out by themselves.}
% \textbf{Vision based Grasping} 
% Computer Vision techniques have traditionally been applied to aid robotic grasping operations. Both model-based and learning-based techniques have been deployed to obtain grasp poses and plan eventual grasping. In \cite{primitive}, a simplified object model is first constructed using primitive shapes. Multiple grasp candidates are generated using the simplified object model in the simulator, and the best candidate is selected based on some grasp quality metric. \cite{singlepose} provides a framework for registration and 6D pose estimation of everyday household objects using a single RGB image. In \cite{predict_grasp}, a deep learning architecture is proposed to predict the grasping location of multiple objects in an image scene with RGB-D image input. \cite{novelobjectgrasp} uses supervised learning to train an algorithm on synthetic data and then use it to generate grasp points on unknown data without using 3D models of the objects. It can be observed that the methods described above and similar methods do not take physical properties like the coefficient of friction and local geometry into account, which are essential for obtaining a stable grasp.
Computer vision techniques have been employed in robotic manipulation tasks to synthesize grasp poses using visual data of objects and scenes either with model-based methods~\cite{primitive},~\cite{singlepose} or learning-based methods~\cite{predict_grasp},~\cite{novelobjectgrasp}. However, they do not infer the objects' properties, such as mass distribution, friction coefficient, and rigidity. Lack of information of these properties can cause inaccurate or even incorrect grasping operations, which may further result in post-grasp effects like rotation, slippage, and even detachment from the gripper, which may ultimately lead to grasping failure. We use RGB-D data in our work to estimate the geometric center of the object as the initial grasping location and then use a tactile sensor's feedback to adjust the grasp locations.

% We also use 3D point cloud data from RGB-D camera to estimate the geometric center of the object and conduct the first grasp, where we assume in most cases, the geometric center is close to the center of gravity. However, we build upon this using tactile sensor to deal with failure cases.And with the feedback from the tactile sensing, the grasping poses are adjusted with certain policies to search the stable grasping pose.
% \wenzhen{This paragraph can be greatly shortened, since it's not that relevant. You can list a sequence of papers and use one sentence to summarize them.}


Tactile sensors can complement vision data by providing local information at the grasp location, such as measuring slippage~\cite{shear-slip},~\cite{biomim-slip}, force~\cite{vision-touch}, grasp stability~\cite{grasp-stability},~\cite{objPoseSpecificGrasps}, \cite{graspStabilityCNN} and object's Center of Mass (COM)~\cite{weightdist},~\cite{chris-COM}. \cite{predict_grasp} combined tactile sensors with vision sensors to obtain better predictions of grasp outcomes using deep neural networks.
% ~\cite{deformable-obj} used a tactile sensor to estimate slip by tracking change in contact tangential force and counteracting it with increased normal force, thereby preventing grasp failure in deformable objects.
\cite{graspingwithouteye} obtained grasp poses by probing a given workspace and localizing objects from tactile signals without using vision. \cite{analyticGraspSuccess} used tactile feedback to reduce uncertainty in contact position and orientation. However, grasp failure due to incorrect grasp locations is rarely studied using tactile sensors, and we address this problem through our work.

% Center of Mass (COM) is an important physical property of object to help grasping.~\cite{weightdist} used a pressure based tactile sensor and trained an LSTM network to estimate weight distribution of objects.~\cite{chris-COM} used a force/torque sensor and a Newton-Euler formulation of rigid body dynamics to estimate the center of mass location of an object during grasping. Their method required estimating the objects' mass before determining the location of the COM. ~\cite{Center-of-mass-grasp} calculated the COM of an object by defining the weighted distance of the object from a fixed point where weights were proportional to the object's density at the corresponding location from point cloud data. The authors note that this method cannot accurately locate the COM for objects of non-uniform mass distribution.


Regrasping has been used to improve grasp success rate by correcting grasp characteristics, such as pose and force, using feedback from additional sensors. 
% ~\cite{In-hand_proximity} learned a grasp stability predictor and then trained a regrasping algorithm to improve grasp success rate using reinforcement learning techniques.
In~\cite{tactile_regrasp}, the author proposed a method to simulate tactile images at different locations of the object from an initial grasp point, predict grasp success for each of these images, and choose the corresponding location with the highest predicted score for regrasping.~\cite{Center-of-Mass} used a low-resolution pressure tactile sensor to detect grasp failure due to slip, and implemented an LSTM model trained on both tactile and force/torque data to sample a regrasp location achieving stable grasp.~\cite{Center-of-mass-grasp} solved a similar problem like ours to detect wrist moments when large objects are grasped at locations away from their COM. The aforementioned work used both force and torque information from a force/torque sensor to detect grasp instability and guide the robot towards the object's COM. In our work, instead of using force/torque information, we detect similar instabilities in smaller household objects using only images from a tactile sensor. 
% ~\cite{Center-of-mass-grasp} combined 3D range and force/torque sensing to find the center of mass and reached the efficient handle grasp. We also form a closed-loop regrasp framework to improve the grasp stability.


\subsection{Slip Detection based on Tactile Sensing}
% \wenzhen{This paper is not about 'slip detection'. Then why do you review those many works on slip detection? You need to explain}
One of the primary motivations of our work is the capability of tactile sensors to detect slippage. 
% Slippage while manipulation leads to a classical grasp failure. 
% Detecting slip before or during grasp and counteracting 
% Tactile sensors have been explored to detect, classify and measure slip. 
Several model-based~\cite{review},~\cite{piezoelectret},~\cite{BioTac} and learning-based~\cite{calib_slip},~\cite{learn_slip_detect} methods have used tactile sensors to detect, classify and measure slip. 
% Slip can be classified into two categories: translational slip and rotational slip. 
Rotational slip can also result from an incorrect grasping location.  Some early works such as~\cite{3-axis_rotation_slip} and~\cite{tactile_force} used force/torque tactile sensors to classify translational and rotational slip by analyzing contact area.~\cite{Biomimetic} and~\cite {Tactile_CNN_rotation} trained neural networks to classify linear and rotational slip from force/pressure sensor data and piezo-resistive sensor data. 
% However, they limit mainly to classification tasks instead of giving a measurement such as rotation angle. In our work, even though we don't deal with rotational slippage, we provide 
% Slippage is a classical grasp failure.
% Adapt to different tactile sensing characters, model-based method~\cite{review},~\cite{piezoelectret},~\cite{BioTac} and learning-based method~\cite{calib_slip},~\cite{learn_slip_detect} can be applied to extract tactile sensing features and used to detect slip. 
% Apart from the works detecting slip after it occurs, some works aimed at detecting slip at an early stage, which is typically called incipient slip. Some may require the objects' properties~\cite{pre-sliding}, but others like~\cite{Slip_predixtion_HMM} can learn the slip prediction model only from the tactile data. 
% Moreover, the slip prediction can provide feedback to the grasp system and guide the system to adjust the grasp policy to stabilize the grasp as in  ~\cite{slip_predict} and ~\cite{grip_stabilization}. 
% proposed a tactile force sensor to detect pre-slippage given the physical properties of the object. To relax the objects' prior knowledge and generalize the model, data-driven incipient slip detection methods are developed, such as ~\cite{slip_predict} and ~\cite{grip_stabilization} which predicted the slip and provided feedback to stabilize the grasp. Moreover, ~\cite{Slip_predixtion_HMM} transformed the multi-dimensional tactile sensor data into a symbolic sequence of clusters as features by using probabilistic clustering, and then trained hidden Markov models to predict slip with these feautures.
\cite{ContactCentroid} discussed the concept of a contact centroid and proposed a method to obtain rotational spin about this point using remote force/torque sensor readings.


% For a certain type of slip, rotation slip, if we can detect the rotational displacement in time, it will help avoid the rotational slip happening.

% \textbf{Slip detection} ~\cite{review} reviews the slip detection in the early stages based on different sensor designs and physical characteristics. Most of these works used model based approaches. More recently, ~\cite{piezoelectret} designed a piezoelectric-based sensor to detect contact and slippage. ~\cite{BioTac} compared three different slip detection approaches: model-based, vibration-based and learning-based by conducting experiments with two BioTac sensors. ~\cite{calib_slip} provided a spectral analysis of sensory responses and applied LSTM neural network to robust slip detectors on three commercial tactile sensors. ~\cite{learn_slip_detect} extracted features by using window matching pursuit and applied SVM to classify the slip and stable grasping on infrared sensor.

% Machine learning techniques such as Support Vector Machine (SVM) are applied to classify slip in ~\cite{learn_slip_detect};
% Deep learning methods, specifically LSTM neural networks which incorporate temporal information are heavily used such as in ~\cite{calib_slip}, and ~\cite{tactile_vision}. The latter also combined vision information processed with CNN network.
% \wenzhen{The summarization of papers is long but has very limited information. In most cases it's not that important to just define a paper using `model-based' methods or `learning-based' methods, but you need to tell how it works fundamentally. What characteristic/measurement they used to detect slip? If model-based, how did they do the model? If it's learning based, what feature they used? The name of the ML methods are not that important. }


% \textbf{Slip prediction} Apart from the works detecting slip after it occurs, some works aimed at detecting slip at an early stage, which is typically called incipient slip. ~\cite{pre-sliding} proposed a tactile force sensor to detect pre-slippage given the physical properties of the object. To relax the objects' prior knowledge and generalize the model, data-driven incipient slip detection methods are developed, such as ~\cite{slip_predict} and ~\cite{grip_stabilization} which predicted the slip and provided feedback to stabilize the grasp. Moreover, ~\cite{Slip_predixtion_HMM} transformed the multi-dimentsional tactile sensor data into a symbolic sequence of clusters as features by using probabilistic clustering, and then trained hidden Markov models to predict slip with these feautures.
% \wenzhen{Same as above. The name of the ML algorithm is not that important, but fundamentally how the methods work}
% \wenzhen{It might be better to combine this paragraph with the previous one, and try to make it shorter. }

 


% Most works on slip detection focuses on translational slip detection, while rotational slip, which is also a common cause of grasp failure, is less investigated. To detect rotational slip, early works such as~\cite{3-axis_rotation_slip} and~\cite{tactile_force} showed how force/torque tactile sensing could classify translational and rotational slip by analyzing contact states. ~\cite{Biomimetic} and~\cite {Tactile_CNN_rotation} trained neural networks to classify linear and rotation slip and applied on force/pressure sensor data and piezo-resistive sensor data. 
% ~\cite{Center-of-Mass} proposed a grasp planner with tactile-visual sensor modules. The re-grasp planner depending on learned slip detection feedback can estimate the center of mass of the unknown object and an optimal grasp pose on the object. We also focus on the rotation grasp, but we can evaluate the rotation numerically with tactile sensor and use it to adjust the re-grasping pose.

% \wenzhen{You should have a particular section to review the works of using GelSight sensors to detect slip or grasp success. You should discuss how this work is different from those works.}

The GelSight sensor is a vision-based tactile sensor that provides high-resolution information of the contact region. 
% It consists of a soft elastomer gel illuminated with red, green, and blue lights from different directions. The gel is painted with black markers in a rectangular grid. A camera is planted in the gel that captures the gel surface as a 2D RGB image. 
 When an object comes in contact with the gel, the gel surface deforms, and the markers on the gel undergo radial motion. By tracking these marker motions, users can study local phenomena like slippage and rotation in great detail. 
There have been some related studies~\cite{shear-slip},~\cite{Improved_Gelsight} to detect the occurrence of slip using Gelsight tactile sensors, but they are restricted to numerically identifying slip. 
% Moreover, ~\cite{tactile_vision} presented a slip detection method by fusing vision and tactile sensing. The authors trained a DNN to classify a grasp is stable or not.
% ~\cite{in-hand} obtained in-hand pose estimate of objects using Deep Gated Multi-modal Learning with inputs from the GelSight sensor. 
We use the GelSight sensor in our analysis to track and analyze the rotational patterns of objects at the contact location. To the best of our knowledge, this is the first attempt at detecting rotation of objects and measuring rotation angle and its orientation using only a vision-based tactile sensor. 
