Robotic grasping is a long-studied problem and the foundation of many manipulation tasks~\cite{grasp-review}. Frequently-asked questions in robotic manipulation include how to grasp an arbitrary object stably, detect grasp failure promptly, and take precautions to avoid such failure. Traditional grasping research focused on detecting grasping locations based on the shape of the objects~\cite{model_grasp_sythesis},~\cite{data-driven_grasp_sythesis} which is typically obtained from vision. However, those methods rarely considered objects' physical properties, such as mass, mass distribution, surface friction, and rigidity. Such properties, which can attribute to significant differences in the physical interaction between robots and the target objects, are essential to decide the optimal grasps. 

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=\linewidth]{imgs/overview_pic2.jpg}
    \caption{Grasping an object with unknown mass distribution. 
    %Usually, it is hard to tell the mass distribution from vision,
    The center of gravity of an object depends on its mass distribution (Row 1). Grasping away from the center results in contact rotation (Row 2).
    A robot can use tactile images from a GelSight sensor (Row 3) to measure contact rotation caused by torque when lifting an object, and therefore move to a grasp point close to the center of gravity and conduct a stable grasp. 
    %but tactile sensing can provide rich contact information about the grasp. 
    % If a robot grasps an object improperly, significant rotation would happen at the contact surface; with the feedback of tactile sensing, the robot can adjust the grasp location and reach a stable grasp as shown in the middle case.
    }
    \label{fig:overview}
\end{figure}

Tactile sensing provides promising solutions to the challenge. By detecting the contact area and contact force during grasping, tactile sensing gives effective feedback about the grasp outcome, which subsequently can be used to build a closed-loop grasping framework. The study of tactile-sensing-based grasp focuses on detecting and measuring slip ~\cite{robot-human-grasp},~\cite{shear-slip} to choose a proper grasping force to avoid object dropping. However, rotation is another common cause of grasp failure, that has not been well studied. This kind of failure happens when a robot grasps an object at locations far from the object's center of gravity. As a result, the large torque at the contact can make the object rotate or make the grasp vulnerable to external impact. Increasing the gripping force does little to mitigate this type of failure. Instead, this problem can be corrected by choosing other grasp locations closer to the object's center of gravity.  

In this paper, we improve the grasp stability by proposing a model-based method to detect the rotation caused by torque using a tactile sensor. Our method can detect the rotational failure of grasp at an early stage and use tactile information to guide a robot to find a stable grasping location that is close to the object's center of gravity. We apply our method to a vision-based tactile sensor called GelSight~\cite{gelsight-base}, which uses a piece of soft elastomer as the medium of contact and an embedded camera to track the deformation of the elastomer surface. The torque on the contact surface will cause torsion on the elastomer medium, which can be visualized as a rotational pattern of the markers painted on the elastomer surface. We measure the markers' rotation angles and directions to estimate the torque, and show the stability of the grasp against the torsional load.
% In our experiment on XXX grasp samples on XX objects, our method can measure the rotation angle of objects within a standard error of XXX, and predict torsional failure in XX\% of the cases. Using rotation measurement results in a closed-loop re-grasping experiment, the robot could successfully grasp objects in 77 out of 88 cases by moving to a location close to the center of gravity. Our method is simple, yet effective in detecting existing or potential torsional failure and thereby improving grasp stability. 
Because our method is based on the physical feedback during the contact, it applies to a wide variety of arbitrary objects with unknown physical properties, such as mass distribution. With increased grasping stability, our method can help robots operate more robustly and reliably in real-world environments.





% Robotics comes with a promise to make our lives better by providing a cyber-physical infrastructure to interact and support each other. Manipulation of daily objects by robots comes as a direct consequence of the stated goal. Grasping is an important aspect of manipulation tasks. Computer Vision techniques are generally deployed to synthesize grasp locations. Several model-based~\cite{model_grasp_sythesis} as well as data-driven ~\cite{data-driven_grasp_sythesis} methods have been proposed to synthesize grasp poses from vision data. These methods use visual features of the scene and objects like RGBD data. However, these methods do not use object's physical properties like mass distribution, coefficient of friction, and rigidity during the grasp selection process. Hence, the synthesized grasp poses are prone to failure due to rotation, slippage, and fracture. On the contrary, tactile sensors ~\cite{review-01}, ~\cite{review-02} can give direct estimation of object's properties from contact which compliments vision sensing.

% When objects are grasped at locations away from their center of gravity, they rotate about the grasping point, leading to a grasp failure. If object rotating is not handled, it may lead to more hazardous consequence such as slippage. We attempt to solve this rotational displacement problem by detecting, measuring, and correcting it, thus to improve the grasp success rate. 
% \vspace{-5mm}

% We use a high-resolution visuo-tactile sensor, the GelSight sensor~\cite{gelsight} to capture rotation events and analyze them. There have been some related studies~\cite{shear_slip},~\cite{Improved_Gelsight} to detect the occurrence of slip using Gelsight tactile sensors, but they are restricted to numerically identifying slip. Nonetheless, they give us insights to detect and measure the occurrence of rotation during the grasping. We believe that early detection of rotation and counteracting it can significantly improve the grasp success rate.  
% We analyze the time sequence of images from the GelSight sensor to detect and measure rotation of object due to the incorrect grasp pose. We also propose a closed-loop regrasping framework to show how rotation detection can lead a robot to a stable grasp pose. 
% The GelSight sensor outputs GelSight images with markers labeled on them. These markers trace the object's movement when the sensor is in quasi-static contact with the object being manipulated, hence providing high resolution localized information in the contact region. 
% When rotation occurs, it gives rotational patterns of markers on the GelSight surface. We analyze these patterns to determine the Center of Rotation (COR) and eventually calculate the rotation angle about this center. We propose a model-based approach to detect and measure the rotation of objects. We also propose a closed-loop regrasping framework to prove our algorithm's effectiveness in real-time by driving the robot to a stable grasp pose. 
% Therefore, we present our two major contributions using GelSight Sensor during robotic manipulation tasks as:
% \begin{itemize}
%     \item A model-based rotation detection and measurement algorithm.
%     \item A closed loop regrasping framework using rotation detection feedback.
% \end{itemize}
% Our method also helps deal with non-uniform mass distribution objects to obtain a stable grasp pose, which can be a difficult task using vision modality. We believe that our method can act as a suitable feedback mechanism for robots to check whether the objects are in the desired orientation when being manipulated, thus ensuring safe modes of operation. 