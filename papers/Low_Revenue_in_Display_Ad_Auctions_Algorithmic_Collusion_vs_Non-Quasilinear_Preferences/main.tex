\documentclass{article}

\usepackage{arxiv}

\linespread{1.06}

\usepackage{amsmath,amsfonts,amssymb, amsthm}

\usepackage[usenames,dvipsnames]{xcolor}
\definecolor{niceRed}{RGB}{190,38,38}
\definecolor{niceBlue}{HTML}{0466a7}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\hypersetup{
	colorlinks = true,
	urlcolor = {black},
	linkcolor = {niceBlue},
	citecolor = {niceRed} % was nicePink
}

% Natbib setup for author-year style
\usepackage{natbib}
\bibpunct[, ]{(}{)}{,}{a}{}{,}%
\def\bibfont{\small}%
\def\bibsep{\smallskipamount}%
\def\bibhang{24pt}%
\def\newblock{\ }%
\def\BIBand{and}%

\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{cleveref}       % smart cross-referencing
\usepackage{lipsum}         % Can be removed after putting your text content
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{doi}

\usepackage{subcaption} % for subfigures
\usepackage{booktabs}	% nice tables
\usepackage{multirow}

\usepackage{mathtools}
\renewcommand*\vec[1]{\boldsymbol{#1}}
\DeclareMathOperator{\EXP}{\mathbb{E}}
\DeclarePairedDelimiter\dual{\langle}{\rangle}
\newcommand*{\diff}{\textup{d}}
\renewcommand*\vec[1]{\boldsymbol{#1}}
\newcommand{\sym}{\textbf{sym}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\usepackage[ruled]{algorithm2e} % For algorithms

% theorem-like environments
\newtheorem{definition}{Definition}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{problem}{Problem}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{remark}{Remark}[section]

\DeclareMathOperator{\Acal}{\mathcal{A}}
\DeclareMathOperator{\Gcal}{\mathcal{G}}
\DeclareMathOperator{\Ical}{\mathcal{I}}
\DeclareMathOperator{\Lcal}{\mathcal{L}}
\DeclareMathOperator{\Ncal}{\mathcal{N}}
\DeclareMathOperator{\Pcal}{\mathcal{P}}
\DeclareMathOperator{\Scal}{\mathcal{S}}
\DeclareMathOperator{\Vcal}{\mathcal{V}}
\DeclareMathOperator{\Xcal}{\mathcal{X}}
\DeclareMathOperator{\R}{\mathbb{R}}
\DeclareMathOperator{\E}{\mathbb{E}}
\newtheorem{result}{Result}


\title{Low Revenue in Display Ad Auctions: Algorithmic Collusion vs. Non-Quasilinear Preferences}

% Here you can change the date presented in the paper title
%\date{September 9, 1985}
% Or remove it
%\date{}

\newif\ifuniqueAffiliation
% Uncomment to use multiple affiliations variant of author block 
\uniqueAffiliationtrue

\ifuniqueAffiliation % Standard variant of author block
\author{
	Martin Bichler \\
	\small Department of Computer Science\\
	\small Technical University of Munich\\
	\small \texttt{bichler@cit.tum.de} \\
	%% examples of more authors
	\And
	 Alok Gupta\\
	\small Carlson School of Management\\
	\small University of Minnesota \\
	\small \texttt{alok@umn.ed} \\
	\AND
	Laura Mathews\\
	\small Department of Computer Science\\
	\small Technical University of Munich \\
	\small \texttt{laura.mathews@tum.de} \\
	\And 
	Matthias Oberlechner\\
	\small Department of Computer Science\\
	\small Technical University of Munich \\
	\small \texttt{matthias.oberlechner@tum.de}\\
}
\else
% Multiple affiliations variant of author block
\usepackage{authblk}
\renewcommand\Authfont{\bfseries}
\setlength{\affilsep}{0em}
% box is needed for correct spacing with authblk
\newbox{\orcid}\sbox{\orcid}{\includegraphics[scale=0.06]{orcid.pdf}} 
\author[1]{%
	\href{https://orcid.org/0000-0000-0000-0000}{\usebox{\orcid}\hspace{1mm}David S.~Hippocampus\thanks{\texttt{hippo@cs.cranberry-lemon.edu}}}%
}
\author[1,2]{%
	\href{https://orcid.org/0000-0000-0000-0000}{\usebox{\orcid}\hspace{1mm}Elias D.~Striatum\thanks{\texttt{stariate@ee.mount-sheikh.edu}}}%
}
\affil[1]{Department of Computer Science, Cranberry-Lemon University, Pittsburgh, PA 15213}
\affil[2]{Department of Electrical Engineering, Mount-Sheikh University, Santa Narimana, Levand}
\fi

% Uncomment to override  the `A preprint' in the header
\renewcommand{\headeright}{}
\renewcommand{\undertitle}{}
\renewcommand{\shorttitle}{}

%%% Add PDF metadata to help others organize their library
%%% Once the PDF is generated, you can check the metadata with
%%% $ pdfinfo template.pdf
\hypersetup{
pdftitle={Low Revenue in Display Ad Auctions: Algorithmic Collusion vs. Non-Quasilinear Preferences},
pdfsubject={cs.GT},
pdfauthor={Martin~Bichler, 	Alok~Gupta, Laura~Mathews, Matthias~Oberlechner},
pdfkeywords={algorithmic collusion, display ad auctions, equilibrium learning} ,
}

\begin{document}
\maketitle

\begin{abstract}
	The transition of display ad exchanges from second-price to first-price auctions has raised questions about its impact on revenue. Evaluating this shift empirically proves challenging. 
	One key factor that is often ignored is the behavior of automated bidding agents, who are unlikely to use static game-theoretical equilibrium strategies instead of favoring dynamic realms that continuously adapt and learn independently through the process of exploration and exploitation. Thus revenue equivalence between first- and second-price auctions might not hold. 
	Research on algorithmic collusion in display ad auctions found revenue differences between second-price and first-price auctions. First-price auctions can induce Q-learning agents to tacitly collude below the Nash equilibrium in repeated complete-information auctions with payoff-maximizing agents (i.e., agents maximizing value minus price).  
	Our analysis explores wide-spread online learning algorithms' convergence behavior in both complete and incomplete information models, but does not find a systematic deviance from equilibrium behavior. Convergence for Q-learning depends on hyperparameters and initializations, and algorithmic collusion vanishes when competing against other learning algorithms.  
	Apart from their learning behavior, the objectives reported in the literature extend payoff maximization, often focusing on return-on-investment or return-on-spend.  
	We derive equilibrium bid functions for such utility models, revealing that revenue equivalence doesn't hold. 
	In low-competition scenarios, the first-price auction often yields lower revenue than the second-price counterpart.  
	These insights offer an alternative rationale for the potential revenue decrease in first-price auctions. Understanding the intricate interplay of auction rules, learning algorithms, and utility models is crucial in maximizing revenue in the ever-evolving world of display ad exchanges.
\end{abstract}


% keywords can be removed
\keywords{algorithmic collusion \and display ad auctions \and equilibrium learning} 

\section{Introduction}\label{sec:intro}

Real-time bidding is a means by which advertising inventory is sold on a per-impression basis \citep{choi2020online}. Display ad auctions are a prime application, where advertisers compete for ads. In 2022, more than 90\% of all digital display ad spend were transacted via real-time bidding \citep{yuen22}. Moreover, the real-time bidding market size is expected to further increase by USD 16.52 bn from 2021 to 2026 \citep{tunuguntla2021near}. The Google Ad Manager, OpenX, PubMatic, or Xandr are among the largest ad exchanges operating these auctions. 

In the past, most ad exchanges used the second-price auction. Yet, by the year 2017, several of them began experimenting with a first-price auction, which nowadays is used by most major ad exchanges \citep{despotakis2021first}. Google claimed this switch was to ``help advertisers by simplifying how they buy online ads.''\footnote{\url{https://support.google.com/adsense/answer/10858748?hl=en}} However, the effect on revenue is difficult to evaluate empirically. While some studies have observed decreased bid prices after the switch \citep{alcobendas2021adjustment}, definitively determining the causal effect is challenging due to many confounding factors and the changes in demand and supply over time.

Understanding the impact of this format change has significant implications. Theory can provide guidance in such situations. Bayes-Nash equilibrium (BNE) analysis is the standard approach for finding equilibrium bidding strategies in auction theory  \citep{krishna2009auction}. This line of research led to landmark results and provided a principled framework for how to model and reason about bidding behavior in auctions. Human bidders in lab experiments do not always adhere to their risk-neutral BNE strategy, and there is a substantial body of research aiming to explain human bidding behavior via risk aversion or regret \citep{kagel2020handbook}. Since bidding in display ad auctions has to take place in milliseconds, programmatic bidding agents implementing a certain bidding strategy are required, and one can assume a high level of rationality \citep{choi2020online}. One could think that bidding agents in such repeated auctions play the analytical BNE. 
However, the literature on real-time bidding in display ad auctions suggests that this is not the case. 

Automated bidders (aka. autobidders) in display ad auctions do not have a good prior value distribution and rather learn a strategy via a process of exploration and exploitation. There is significant literature reporting simple learning strategies of these agents, who adapt their bid prices based on the history of their play \citep{cai2017real, jin2018real, zhao2018deep, cheng2019extensible}. Bandit algorithms are widespread in this literature \citep{he2013online, tunuguntla2021near, jauvion2018optimization, tilli2021multi, kolumbus2022auctions}. However, it is not clear that such learning algorithms lead to a Nash equilibrium (NE) in repeated play.\footnote{It is well known that computing equilibria is computationally hard in general \citep{daskalakis2009complexity}} 

Computational experiments are one way to study the interaction of learning agents in repeated first- and second-price auctions. Such simulations can abstract away from changes in supply and demand and various confounding factors present in empirical data. 
\citet{banchio2022artificial} report the results of numerical experiments with bidding agents based on Q-learning algorithms in a complete-information model, where the value of bidders is public knowledge. 
{Real-world bidding agents have to deal with heterogeneous types of impressions. However, for the theoretical question, they reduce attention to the repeated sale of impressions of the same type.} 
Interestingly, they find that first-price auctions with no additional feedback lead to \textit{tacit-collusive outcomes}, while second-price auctions do not. 
Tacit collusion refers to forms of anti-competitive co-ordination which can be achieved without any need for an explicit agreement \citep{OECD.2017}. 
In algorithmic collusion algorithms independently learn to recognize patterns in competitors' behaviors, leading to strategies below those in a Nash equilibrium.
Such algorithmic collusion could explain the lower revenues in first-price auctions.

However, conclusions about tacit collusion and format differences remain mixed. Others argue the auction format itself does not cause collusion without explicit communication \citep{deng2023we}, and that algorithms can achieve tacit collusion through "fingerprinting" even in second-price auctions \citep{Assad.2020}. Different algorithms and environments may also produce different outcomes \citep{brown2023competition}. There is no consensus yet on whether algorithmic collusion is an inevitable consequence of first-price auctions.

Importantly, there is significant evidence in the literature that real-time bidding agents do not optimize payoff, but that return-on-investment (ROI) and return-on-spend (ROS) are the primary objectives (see Section \ref{sec:util_models}). Such deviations of pure payoff maximization have not been explored in the related literature, but have a significant impact on the revenue comparison as we show. 


\subsection{Contributions}

We analyze when learning agents end up with low-price outcomes in repeated first-price auctions as they are used for selling display ads. For this, we study the outcomes of learning dynamics under different learning algorithms, different informational assumptions (complete and incomplete information), different numbers of bidders, and different utility models (maximizing payoff, ROI, and ROS). We want to understand when we can expect first-price auctions to lead to lower revenue compared to their second-price counterpart and when the switch from second-price formats is without loss.  

Apart from Q-learning, we use bandit learning algorithms that can handle complete and incomplete information about competitor valuations. Among widely used bandit algorithms \citep{lattimore2020bandit}, the Exp3 algorithm has good regret bounds in the adversarial bandit setting where the rewards can be chosen by an adversary who is aware of the algorithm's strategy. 
It is a standard example of a no-regret learning algorithm and it naturally comes up in the literature on real-time bidding \citep{heidari2016pricing,rhuggenaath2019pso,tilli2021multi}. 
We used also other bandit algorithms (see Appendix) but will report the results of a variant of Exp3 as a representative bandit algorithm, to keep the presentation concise.  

In addition, we also draw on algorithms based on gradient feedback, which have recently been developed for equilibrium learning in auctions. \textit{Simultaneous Online Dual Averaging} (SODA) \citep{bichler2023soda} learns distributional strategies based on a discretized version of the auction, i.e. discretized values and bids. This algorithm is fast and it offers an ex-post equilibrium guarantee: if the algorithm converges to a pure strategy, it has to be a BNE. The results of SODA will provide a baseline for equilibrium strategies, where we don't know the analytical BNE. Importantly, in our experiments, we go beyond the stylized complete-information model and analyze the incomplete-information model, as is usual in auction theory \citep{krishna2009auction}.  

We find that while the convergence speed and robustness are vastly different, bandit learning algorithms eventually converge to the Nash equilibrium in the first- and second-price auction assuming payoff-maximizing motives of all agents. Q-learning is an exception in that convergence depends on the initialization of the algorithm and the discount factor. However, it becomes more robust again if it competes against Exp3 agents and not Q-learning agents. 
Overall, the insights by \citet{banchio2022artificial} are important observations, but our results suggest that the tendency to collude on low prices might be restricted to specific algorithms, hyperparameters, and complete-information settings. 

In the second step, we use equilibrium learning algorithms to find equilibrium in models with non-quasilinear utility functions (ROI and ROS), where no analytical solutions are available. We don't argue that advertisers should use such models, but there is wide-spread evidence that ROI or ROS are being used rather than payoff maximization. 
SODA quickly converges to a pure-strategy BNE {while} Exp3 takes longer, but also finds an approximate BNE.   
The equilibrium outcomes are qualitatively different from the quasi-linear model where bidders maximize payoff. 
For the ROI model, we find that bidders in second-price auctions act truthfully, as with a quasi-linear utility function, but bid much lower in first-price auctions when compared to quasi-linear bidders. 
For the ROS model with budget constraints, we observe that bidders in second-price auctions no longer act truthfully. Still, they bid significantly higher in second-price auctions than in first-price auctions. 
In summary, our results suggest that revenue equivalence fails with these utility models, and ad exchanges earn significantly less with a first-price auction with such non-quasilinear utility functions. While it might not be surprising that there are revenue differences outside the standard quasilinear model, equilibrium learning allows us to quantify these differences, and it provides a powerful new tool to characterize the equilibria in various auction models for which analytical derivations of a BNE are considered intractable. 

\section{Related Literature}
\label{sec:lit}
We draw on different strands in the literature. First, we discuss utility models, as they have been reported for display ad auctions. Second, we introduce relevant work on collusion in auctions. Finally, we discuss literature in equilibrium learning relevant to this paper.

%\MB{Add a sentence discussing the optimization of a separable utility function per game and a cumulative (discounted) reward function.}

%{https://theoryclass.wordpress.com/2023/04/15/the-irrelevance-of-automated-bidding/}


\subsection{Utility Models of Bidders in Display Ad Auctions} \label{sec:util_models}
% https://hanachoi.github.io/research-papers/choi_mela_optimal_reserve.pdf

There is a large literature on display ad auctions and real-time bidding \citep{despotakis2021first}. The authors assume different utility models to describe the advertisers' objectives, and most likely advertisers are not alike in how they define their objectives in practice. Originally, authors used a standard payoff-maximizing or quasi-linear utility function and considered each auction separately \citep{edelman2007internet, despotakis2021first}. However, most literature does not assume payoff maximization. The ROI became a popular metric because of marketing budget allocation \citep{szymanski2006impact, borgs2007dynamics,jin2018real, wilkens2017gsp}. 
Principal-agent problems within bidding firms can explain such deviations from payoff maximization \citep{bichler2018principal}. 
Thus, apart from payoff maximization, we analyze agents that learn to maximize expected ROI as a measure of the ratio of profit to cost:
\begin{equation*}
	\mathbb{E}[ROI] = \mathbb{E}[\tfrac{v-p}{p}],
\end{equation*}
where $v$ denotes the valuation and $p$ the payment or cost.
Note that this ratio becomes very high if prices are close to zero. {In practice, there are minimum bid prices (aka. floors) and we rule out division by zero as a boundary case for our theoretical analysis in this paper.}

Sometimes, bidders want to maximize total value subject to payments being less than a budget constraint 
$B$ \citep{tunuguntla2021near}. In an offline auction where all $M$ impressions are auctioned off at the same time, the utility of the bidder could be described as
\[\max \sum_{m=1}^M\mathbb{E}(x_m v_m) \quad 
s.t. \sum_{m=1}^M x_m p_m \leq B,
\]
where $x_m$ is a binary variable that equals one upon winning and zero otherwise, $p_m$ is the payment, and $v_m$ the value per impression. \citet{tunuguntla2021near} only require the budget constraint for a campaign to hold in expectation and derive the Lagrangian of the resulting optimization problem. Based on the first-order condition for this Lagrangian, they derive a bidding strategy $b_m = v_m/\lambda^*$, where $b_m$ is the bid, and $\lambda^*$ defines a threshold. In a first-price auction $b_m=p_m$. \citet{tunuguntla2021near} devise an algorithm to learn an optimal $\lambda^*$. With an optimal $\lambda^*$, the advertiser wins the subset of auctions for which the value per expenditure is greatest.
By rearranging terms, the optimal shading factor $\lambda=v_m/p_m$ is the ratio of the value for an impression and its payment. This is reminiscent of wide-spread heuristics for the knapsack problem, which rank-order items by the ratio between value and weight of objects. $\lambda \in [0,1]$ is also described as \textit{multiplicative shading factor} in recent publications on real-time bidding \citep{balseiro2019learning, Tardos2022}, and it corresponds to ROS as it is often used by practitioners.\footnote{\url{https://www.indeed.com/career-advice/career-development/roas-vs-roi}} So, apart from payoff-maximization and ROI-maximization, we also study bidders that aim to maximize expected ROS:
\begin{equation*}
	\mathbb{E}[ROS] = \mathbb{E}[\tfrac{v}{p}]
\end{equation*}
%
There is no consensus in the literature yet, which utility model best describes bidders in display ad auctions. We do not argue that ROI or ROS are the right models for advertisers and do not claim that all bidders implement such utility functions. However, we ask what equilibrium bidding strategies would be, if these strategies can be learned, and which revenue they lead to, if bidders optimize ROI or ROS rather than payoff.

\subsection{Collusion in Auctions and Pricing}

Collusion is a key concern in the literature on auctions, and there is much literature about which auction rules are more susceptible \citep{skrzypacz2004tacit, fabra2003tacit,blume2008modeling}. 
It is often a topic in second-price auctions, less so in first-price auctions (see \citet{krishna2009auction}). 
Here, we deal with tacit collusion, where low off-equilibrium prices arise without explicit communication among bidders. In particular, we want to understand algorithmic collusion, i.e. collusion that arises from the repeated interaction of algorithmic agents without them being programmed for explicit collusion. 

Algorithmic collusion was first discussed in the context of oligopoly pricing. Early treatments go back to \citet{greenwald2000shopbots}. \citet{Calvano.2020} analyzes firms who play an infinitely repeated game, pricing simultaneously in each stage and conditioning their prices on history. They find that Q-learning algorithms consistently learn to charge supra-competitive prices. These prices are sustained by collusive strategies with a finite phase of punishments followed by a gradual return to cooperation. \citet{Klein.2021} shows how Q-learning is able to learn collusive strategies when competing algorithms update their prices sequentially; as opposed to \citet{Calvano.2020}, for collusion to occur in their sequential-move setting, they do not require that algorithms can condition on own and competitor past prices. Other related papers on algorithmic collusion in oligopoly pricing were written by \citet{Hettich.2021} and \citet{Asker.2022}, for example. 

\citet{banchio2022artificial} were the first to analyze collusion in the context of real-time bidding in display ad auctions. The environment is different from the stylized oligopoly pricing models. They find that when bidders use simple AI algorithms to determine their bids, the auction format and other design choices can have a first-order effect on revenues and bidder payoffs. The revenues can be significantly lower in first-price auctions than in second-price auctions. In particular, first-price auctions with no additional feedback lead to tacit-collusive outcomes, while second-price auctions do not. However, a simple practical auction design choice - revealing to bidders the winning bid after the auction - can make the first-price auctions more competitive again.


\subsection{Equilibrium Learning}
\label{sec:learning}
The literature on algorithmic collusion and the recent work on equilibrium learning in auctions are closely related and address the same underlying question. In this paper, we leverage this connection. Equilibrium learning tries to find equilibria, while algorithmic collusion aims to identify situations when learning agents might end up in collusive, off-equilibrium states. 
Almost all of the literature on equilibrium learning deals with finite and complete-information games \citep{fudenbergLearningEquilibrium2009}. 
While earlier work considered mixed strategies over normal-form games \citep{zinkevich2003OnlineConvexProgramming,bowling2002MultiagentLearningUsing,bowling2005ConvergenceNoRegretMultiagent,busoniu2008ComprehensiveSurveyMultiagent}, more recently, motivated by the emergence of GANs, there has been a focus on (complete-information) continuous games \citep{mertikopoulos2019learning,letcher2019DifferentiableGameMechanics,balduzzi2018mechanics,schaefer2019CompetitiveGradientDescent, bailey2018MultiplicativeWeightsUpdate}. A common result for many settings and algorithms is that gradient-based learning rules do not necessarily converge to NE and may exhibit cycling behavior but often achieve no-regret properties and thus converge to weaker Coarse Correlated Equilibria (CCE). An analogous result exists for finite-type Bayesian games, where no-regret learners are guaranteed to converge to a Bayesian CCE \citep{hartline2015NoRegretLearningBayesian}.


Earlier approaches to finding equilibria in auctions were usually setting-specific and relied on reformulating the BNE first-order condition of the utility function as a differential equation and then solving this equation analytically (where possible) \citep{vickrey1961CounterspeculationAuctionsCompetitive,krishna2009auction,ausubel2019CoreselectingAuctionsIncomplete}. \citet{armantier2008ApproximationNashEquilibria} introduced a BNE-computation method based on expressing the Bayesian game as the limit of a sequence of complete-information games. They show that the sequence of NE in the restricted games converges to a BNE of the original game. While this result holds for any Bayesian game, setting-specific information is required to generate and solve the restricted games. \citet{rabinovich2013ComputingPureBayesianNash} study best-response dynamics on mixed strategies in auctions with finite action spaces. 
These articles were focused on single-object auctions. \citet{bosshard2017computing,bosshardComputingBayesNashEquilibria2020} were the first to compute equilibria for combinatorial auctions. The method explicitly computes point-wise best responses in a fine-grained discretization of the strategy space via sophisticated Monte-Carlo integration. 

As described earlier, the paper by \citet{banchio2022artificial} analyzes repeated first-price auctions with Q-learning agents, who both have the same fixed value, which is common knowledge. They show that Q-learning can lead to phases with collusive strategies in the first-price auction. \citet{deng2022nash} analyze a similar model analytically and they find that mean-based algorithms converge to the NE in this model.\footnote{Note that the results by  \citet{deng2022nash} assume that at least two bidders have the same highest value, but convergence with a single highest-value bidder is still open and so is convergence in the incomplete-information model.} 
Mean-based means roughly that the algorithm picks actions with low average rewards with low probability. 
This class contains most of the popular no-regret algorithms, including Multiplicative Weights Update (MWU), Follow the Perturbed Leader (FTPL), {and} Exp3. This is also what we find in our complete-information experiments.  
Q-learning is not mean-based in the early rounds where agents explore random actions with high probability, but we find that with the right hyperparameters, Q-learning also converges empirically. 
Given that real-world bidding agents do not necessarily all implement Q-learning, these results suggest that algorithmic collusion might be less of a concern in a model with complete information. 

%\MB{cite Kolumbus and Nisan on convergence as well as Banchio and Mantegazza on collusion. }

In real-world display ad auctions, bidders have only distributional (incomplete) information and no convergence results are available.%\footnote{\citet{Feng2021Convergence} show convergence of mean-based learning rules in a discrete first- and second-price auction with quasilinear utilities, but require a very specific initial exploration phase.} 
Consequently, we analyze algorithms for learning equilibrium in the incomplete-information model. In addition to Exp3 and Q-learning, we draw on SODA \citep{bichler2023soda}, a gradient-based learning algorithm that was shown to be very versatile and allowed for the computation of BNE in a large variety of different auction models. Whenever these algorithms converge to a strategy, it has to be an equilibrium. We will describe these algorithms in more detail below and use them to compute a BNE in cases where an analytical derivation is not tractable. 

\section{Equilibrium Strategies}\label{sec:equi}
Let us now discuss the different utility functions for display ad auctions introduced in Section \ref{sec:util_models} and study the resulting equilibrium problem. We will introduce the equilibrium problem formally and show that for all but the standard quasi-linear utility function, we end up in non-linear differential equations. In general, we cannot solve them analytically, except for simple cases, e.g., ROI with a uniform prior. The equilibrium learning algorithms introduced later will then allow us to find numerical solutions to these problems and compare the resulting equilibria.

\subsection{Model}
Formally, single-item auctions are modeled as incomplete-information games $ \mathcal G = (\Ical, \Vcal, \Acal, F, u)$ with continuous type and action spaces. Each bidder $ i \in \Ical = \{ 1,\dots,N\}$ observes a private value (type) $v_i \in \Vcal_i \subset \R $ drawn from some prior distribution $F$ over $\Vcal := \Vcal_1 \times \dots \times \Vcal_N$. With a slight abuse of notation, we also denote the marginal distribution over $\Vcal_i$ with $F$, since we only consider symmetric bidders with independent and identically distributed valuations. The corresponding probability density function is denoted by $f$. After observing their private types, bidders submit bids $b_i \in \Acal_i \subset \R$ and receive their payoffs as given by the (ex-post) utility function $u_i: \Acal \times \Vcal_i \rightarrow \R$ with $\Acal = \Acal_1 \times \dots \times \Acal_n$.
A pure strategy is a function $\beta_i: \Vcal_i \rightarrow \Acal_i$, mapping a bidder's value to an action. We say that a strategy profile $\beta = (\beta_1, \dots, \beta_n) $ is a BNE if no agent can increase their expected utility $ \tilde u_i $ by unilaterally deviating, i.e.,
\begin{equation}
	\tilde u_i(\beta_i, \beta_{-i}) \geq  \tilde u_i(\beta_i', \beta_{-i}) \quad \forall \beta_i',\,\, \forall i \in \Ical.
\end{equation}
The expected (ex-ante) utility is defined by $ \tilde u_i(\beta_i, \beta_{-i}) := \E_{v_i \sim F}[\bar u_i(\beta_i(v_i), \beta_{-i}, v_i)]$ with the ex-interim utility 
\begin{equation}
	\bar u_i(b_i, \beta_{-i}, v_i) := \E_{v_{-i}}[u_i(b_i,\beta_{-i}(v_{-i}), v_i)].
\end{equation}
In the remaining part of this section, we analyze this model for different utility models, i.e., different ex-post utility functions $u_i$. 
Let $x: \Acal \rightarrow [0, 1]^N$ be the allocation vector, where $x_i(b) = 1$ if bidder $i$ gets the item, i.e., $b_i > \max_{j \neq i} b_j$ and $x_i(b) = 0$ else, and $p: \Acal \rightarrow \R^N $ the price vector. 
As described in the previous section, we consider payoff-maximizing or quasi-linear (QL)
\begin{equation}
	u_i^{QL}(b, v_i) = x_i(b)(v_i - p_i(b)),
\end{equation}
ROI-maximizing
\begin{equation}
	u_i^{ROI}(b, v_i) = x_i(b) \dfrac{v_i - p_i(b)}{p_i(b)},
\end{equation}
and ROS-maximizing agents
\begin{equation}
	u_i^{ROS}(b, v_i) = x_i(b)\dfrac{v_i}{p_i(b)}.
\end{equation}
Note that, depending on the payment rule, the price vector takes the values $p_i(b) = b_i$ in first-price and $p_i(b) = \max_{j \neq i} b_j $ in second-price auctions for bidder $i$ receiving the item, and $p_j(b) = 0$ for all other bidders. In the following analysis, we often use the shorter notation $p_i := p_i(b)$.

\subsection{Payoff-Maximizing Bidders}

With quasi-linear bidders (QL) that maximize payoff, the equilibrium bidding strategies in a first- and second-price auction are well known. The second-price auction has a dominant-strategy BNE of bidding truthfully ($\beta(v)=v$). For the first-price auction, we get a closed-form solution for the (non-truthful) equilibrium bidding strategy via the first-order condition of the expected utility function \citep{krishna2009auction}. Given that all opponents play according to a symmetric, increasing, and differentiable strategy $\beta$, the utility for bidder $i$ submitting bid $b$ with valuation $v$ is given by
\begin{equation}
	\bar u_i (b,\beta_{-i}, v) = G(\beta^{-1}(b))(v-b),
\end{equation}
where $G(\beta^{-1}(b))$ denotes the probability of winning with bid $b$. 
Using the first-order condition $\frac{d}{db} \bar u(b,\beta_{-i}, v) = 0$, one can derive the following ordinary differential equation (ODE)
\begin{equation}
	\dfrac{d}{dv}(G(v)\beta(v)) = v G'(v).
\end{equation}

%\paragraph{Example - FPSB with uniform prior and reservation price}
If we assume independent uniformly distributed valuations for all $N$ agents, i.e., $G(v) = v^{N-1}$, and further suppose that $\beta(0)=0$, we get the equilibrium strategy $\beta(v) = \frac{N-1}{N} v$.
Since we introduce a reserve price $r>0$ for the ROS and ROI utility models, we will also use this reserve price here for better comparison. 
For valuations $v<r$, no bidder can make a positive profit, and consequently, they would not participate in the auction ($\beta_i(v)=0$). For higher valuations, we can again use the ODE to get a BNE with the additional initial condition $\beta(r) = r$. This leads to
\begin{equation}\label{eq:eq_fpsb_ql}
	\beta(v) =  \dfrac{N-1}{N}v + \dfrac{1}{N} \dfrac{r^N}{v^{N-1}} \quad \text{ for } v \geq r.
\end{equation}
See \citet{krishna2009auction} for the equilibrium strategy with general i.i.d. valuations. Revenue equivalence of the first- and second-price auction in the independent private values model with payoff-maximizing bidders are central results in auction theory discussed in related textbooks. 


\subsection{ROI-Maximizing Bidders}
As discussed earlier, ROI maximization is widely mentioned as the objective of advertisers. 
The first observation is that the second-price auction continues to be strategyproof, as with payoff-maximizing bidders.  

\begin{theorem} \label{thm:roi_sp}
	The second-price sealed-bid single-object auction is strategyproof for bidders with an ex-post utility function of $u_i(b, v_i) = x_i(b) \tfrac{ v_i - p_i(b)}{p_i(b)}$.
\end{theorem}

\begin{proof}
Consider bidder 1 and suppose that $p_1 = \max_{j\ne 1}b_j$ is the highest competing bid. By bidding $b_1$, bidder 1 will win if $b_1 \geq p_1$ and lose if $b_1 < p_1$. Let's assume that $b_1$ equals the value of this bidder. Now, suppose that he bids an amount $b'_1 < b_1$. If $b_1 > b'_1 \geq p_1$, then he still wins, and his ROI is still $(b_1-p_1)/p_1 >0$. If $p_1 > b_1 > b'_1$, he still loses. However, if $b_1 > p_1 > b'_1$, then he loses, whereas if he had bid $b_1$, he would have received a positive ROI. Thus bidding less than $b_1$ can never increase his utility but in some circumstances may actually decrease it. Similarly, it is not profitable to bid more than $b_1$. If he bids $b'_1 > b_1$, then there could be $b'_1 > p_1 > b_1$, making his utility negative. If $b'_1 \geq b_1 > p_1$, then this increase to $b'_1$ would not change his ROI.
\end{proof}

%\subsubsection{First-price auction}
Let us next discuss the first-price auction. Analogous to the model with payoff-maximizing agents, we can try to use the first-order condition to derive equilibrium strategies for ROI-maximizing bidders. The ex-interim utility for bidder $i$ with valuation $v$ and bid $b$, given the opponents strategy $\beta$, is defined by
\begin{equation} \label{eq:foc_roi}
	\bar u_i (b, \beta_{-i}, v) = G(\beta^{-1}(b)) \dfrac{v-b}{b}.
\end{equation}
Similar to the first-price auction with quasi-linear bidders, we can derive a first-order condition assuming symmetric strategies and obtain
%\begin{align*}
% 	\dfrac{d}{dp}\bar u(p,v,\beta) &= 0 \\
%	G'(\beta^{-1}(p)) \beta^{-1 '}(p)\dfrac{v-p}{p} +  G(\beta^{-1}(p)) \dfrac{-v}{p^2} &= 0 \\
%	G'(\beta^{-1}(p)) \beta^{-1 '}(p) p (v-p)  -v \cdot G(\beta^{-1}(p)) &= 0  \\
%	\intertext{Assuming symmetry of all bidders, i.e., $ p = \beta(v) $ and using the inverse function theorem we get}
%	G'(v) \dfrac{1}{\beta'(v)} \beta(v) (v-\beta(v)) &=  v \cdot G(v)
%\end{align*}
%This leads to the following, first-order, homogeneous ODE
\begin{equation}
	\frac{d}{dv} \beta(v) = \left( v \beta(v) - \beta(v)^2 \right) \dfrac{G'(v)}{vG(v)}.
\end{equation}
This first-order, non-linear ODE is in general hard to solve analytically. If we restrict our analysis to independent, uniformly distributed valuations over $[0,1]$, i.e., $F(v) = v$ and $ G(v) = v^{N-1} $, the first-order condition \eqref{eq:foc_roi} reduces to
\begin{equation}
	\dfrac{d}{dv} \beta(v) = (N-1) \left( \dfrac{\beta(v)}{v} - \dfrac{\beta(v)^2}{v^2} \right).	
\end{equation}
For this specific form of the ODE, i.e., $\tfrac{d}{dv}\beta(v) = f \big(\tfrac{\beta(v)}{v}\big)$ with $f(x) = (N-1)(x-x^2)$, we can use substitution and derive an analytical solution of the ODE by separation of variables. 
Similar to the quasi-linear case, one can argue that bidders with a valuation $v < r$ do not participate, and that they bid exactly $ r $ for $v = r$. The resulting initial value problem leads to the equilibrium strategies
\begin{equation}
	\beta(v) = 
	\begin{cases}
		\dfrac{v}{1 + \log(r) + \log(v)} &\text{ for } N = 2 \\[10pt]
		\dfrac{(N-2)v^{N-1}}{(N-1)v^{N-2} - r^{N-2}} &\text{ for } N > 2.
	\end{cases}
\end{equation}
%allows us to derive a closed form solution for the initial value problem with $\beta(r)=r$ (reservation price). 
Note that we bound the bids from below to ensure that the utility is well-defined. For the analytical solution depicted in Figure \ref{fig:analytical_QL_ROI_uniform}, we assume a reservation price of $r=0.05$. 
%Hence, bidders with lower valuations bid the reservation price and for higher valuations we solve the ODE with initial condition $\beta(0.05)=0.05$.
For more general prior distributions, such a non-linear ODE is very difficult to solve with standard analytical or numerical techniques \citep{bichler2023soda}. 

\begin{figure}[h]
	\begin{center}
	\begin{subfigure}{0.45\textwidth}
		\centering
		\includegraphics[width=0.9\textwidth]{figures/bne_roi_ql_2.pdf}
		\caption{$N=2$ Bidders}
	\end{subfigure}
	\begin{subfigure}{0.45\textwidth}
		\centering
		\includegraphics[width=0.9\textwidth]{figures/bne_roi_ql_5.pdf}
		\caption{$N=5$ Bidders}
	\end{subfigure}
	\caption{Analytical BNE for Payoff- and ROI-Maximizing Agents in a First-Price Auction with Uniform Prior}
	\label{fig:analytical_QL_ROI_uniform}
	\end{center}
\end{figure}

\subsection{ROS-Maximizing Bidders}
Finally, we analyze ROS-maximizing agents. As indicated earlier, there is an important difference between ROI and ROS: While the expected utility for ROI becomes negative once the price is higher than the value, ROS never becomes negative, independent of how high the payment is. In other words, with a monotone allocation rule, if a bidder is losing, they can always increase their bid without ever having a negative payoff. This leads to a situation where agents can place the highest possible bid, regardless of how high this value is and which payment rule is being used. 

\begin{proposition}
	In the first- and second-price sealed-bid single-object auction where agents are ROS maximizers, ties are broken randomly, and the action space is bounded from above, submitting the maximal bid is an equilibrium.
\end{proposition}
\begin{proof}
Assume all agents bid according to $\beta(v) = \bar b$, where $\bar b > 0$ is the maximal bid the agents can submit. Then, the ex-interim utility is $\bar u(b,\beta_{-i}, v_i) = \tfrac{1}{N} \tfrac{v_i}{\bar b} > 0$ if $b = \bar b$, and $0$ if $b < \bar b$. Since agents can only deviate from $\beta(v)$ by bidding less, this would strictly decrease their payoff. Therefore, $\beta(v) = \bar b$ is indeed an equilibrium strategy.
\end{proof}

This equilibrium strategy is also found by equilibrium learning algorithms. Obviously, this is unrealistic, because it ignores that bidders always have only a finite budget available.  
In this article, we consider ROS-maximizing bidders with a per-auction budget (ROSB), which we simply model using a log barrier function. This ROSB utility is given by
\begin{equation}
	u_i^{ROSB}(b, v_i) = x_i(b) \left( \dfrac{v_i}{p_i(b)} + \log(B - p_i(b)) \right),
\end{equation}
where $B$ is the budget. It is difficult to derive a BNE analytically for such a utility function. The first-order condition of the ex-interim utility $\bar u_i $, under the assumption of symmetric bidders, leads to
\begin{align}
	\label{eq:foc_rosb}
	\dfrac{d \beta(v)}{dv} &= (B-\beta(v))\dfrac{G'(v)}{G(v)} \dfrac{v\beta(v) + \beta(v)^2\log(B-\beta(v))}{v(B-\beta(v))-\beta(v)^2}.
\end{align}
Similar to the first-order condition for ROI-maximizing bidders, this is a general non-linear ODE. But even for a uniform prior, we cannot solve this ODE analytically. 
As we will discuss, SODA provides a numerical technique to quickly converge to an equilibrium in all of these utility models. 

\section{Learning Algorithms}
In the following section, we focus on the learning algorithms agents employ to increase their individual rewards. 
From an agent's point of view, the repeated interaction in an auction can be interpreted as an online optimization problem. 
At each stage $t = 1,2,\dots$ the agent chooses a strategy $x_t \in \Xcal $ from some set and gets a utility $u_t(x_t)$. 
The utility in each stage is determined by the opponents' current strategy, i.e., $u_t(\,\cdot\,) = u_i(\,\cdot\,, x_{-i,t})$. 
If we assume that $\Xcal$ is a closed convex subset of some vector space and $u_t$ are convex functions, this problem is known as an online convex optimization problem \citep{shalev-shwartzOnlineLearningOnline2011}.

A standard performance measure of algorithms generating a sequence of strategies $x_t$ in such a setting is the notion of (external) regret. It is defined by
\begin{equation}
	\text{Reg}(T) = \max_{x \in \Xcal} \sum_{t=1}^T u_t(x) - u_t(x_t)
\end{equation}
and denotes the difference between the aggregated utilities after $T$ stages and the best fixed action in hindsight. We say that an algorithm has \textit{no regret} if the regret $ \text{Reg}(T)$ grows sublinearly. 
Note that this notion of no regret does not make any assumptions on the distribution of the utilities. 
It is therefore especially well suited in a multi-agent setting. 
It is also well known that if all agents follow a no-regret algorithm in multi-agent settings such as our repeated auction, the time average of the resulting strategies converges to a \textit{coarse correlated equilibrium (CCE)} \citep{blum2007ExternalInternalRegret}. There are many no-regret learners such as follow-the-regularized-leader (FTRL), where the agent plays a regularized best response to the aggregated utility function \citep{shalev-shwartzOnlineLearningOnline2011}. If one considers linear surrogates of the (convex) utility function, methods such as dual averaging \citep{nesterov2009PrimaldualSubgradientMethods} can be derived from FTRL.

While FTRL requires knowledge of the functions $u_t$ \textit{(full information feedback)} to compute an update, gradient-based methods use only the gradient $\nabla u_t(x_t)$ evaluated at the $x_t$ to compute the linear surrogate and update the strategy (\textit{gradient feedback}). 
Algorithms with \textit{bandit feedback} require even less and rely solely on the observed utility $u_t(x_t)$ of the played strategy $x_t$ in each stage $t$.
In our experiments, we focus on algorithms in the bandit model applied to repeated auctions with finitely many actions or bids. 
The bandit model mimics the type of information that real-time bidding agents get in display ad auctions after bidding. SODA is an exception as it uses gradient feedback. We primarily use SODA as a numerical tool to provide us with equilibrium bidding strategies to compare against. 

\subsection{Exp3-Algorithm}

If we consider an auction where agents have fixed valuations (complete information), the online optimization problem can be modeled as a multi-armed bandit where each arm describes one out of the possible discrete bids $b \in \Acal^d = \{b_1,\dots,b_m\}$. In general, multi-armed bandit problems are reinforcement learning problems with a single state, where a learner repeatedly chooses an action from a set of available actions and receives a reward associated with the chosen action  \citep{lattimore2020bandit}. An agent repeatedly pulls an arm of a slot machine and aims to maximize the cumulative reward. 
$\varepsilon$-greedy, UCB (Upper Confidence Bound), or Exp3 (Exponential-weight algorithm for Exploration and Exploitation) are well-known no-regret bandit algorithms. 
We used several algorithms but will report the results of a variant of Exp3 as a representative bandit algorithm. Results for other bandit algorithms can be found in the Appendix \ref{app:bandit}.    

\begin{algorithm}[h]
	\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
	\Input{exploration parameter $\varepsilon$, learning rate $\alpha$, number of rounds $T$}
	Initialize strategy $x_1 = \tfrac{1}{m} (1, \dots, 1)$\; 
	\For{$t=1,2,\dots,T$}{
		Choose action $b_t \sim p_t = (1-\varepsilon)\, x_t + \varepsilon\, \tfrac{1}{m} \textbf{1}_m$\;
		Receive reward $R_t$\;
		Update strategy $x_{t+1}(b) = \dfrac{x_t(b) \exp(\alpha R_t(b)))}{\sum_j x_t(b) \exp(\alpha R_t(b)))}$ for all $b \in \Acal^d$\;
	}
	\caption{Exp3-Algorithm}
	\label{alg:bandit_alg}
\end{algorithm}
One key idea of Exp3 is to use an estimator for the reward of actions not played at time $t$.
We focus on a specific estimator known as an importance-weighted estimator. Let $b_t \in \Acal^d $ be the action played in iteration $t$ and $r_t \in [0,1]$ the respective reward. 
Then we define the importance-weighted estimator $ R_t$ by
\begin{equation}
	R_t(b) = \chi_{b_t}(b) \dfrac{r_t}{p_t(b_t)}, \quad \forall b \in \Acal^d
	%loss based version
	%\quad \text{and} \quad \hat \nabla^\text{loss} l(x_t)_i = \begin{cases} 1 - \dfrac{1 - r_t}{x_{i,t}} &\text{ if } $a_t = a_i$ \\ 1 &\text{ else} \end{cases},
\end{equation}
where $p_t(b_t) $ is the probability of playing action $b$ in stage $t$,  $x_t \in \Xcal := \Delta \Acal^d$ the current mixed strategy, and $\chi$ the indicator function.
Note that $ R_t(b) $ is an unbiased estimate of $r_t$ for action $b$ conditioned on the history. This estimate is then used to update the strategy similar to the multiplicative weights update.
This variant of Exp3 \citet{braverman2017selling} (see Algorithm \ref{alg:bandit_alg}) has an additional exploration probability, which we can also find in the bandit version of the online stochastic mirror descent from \citet[Sec.~31]{lattimore2020bandit}.
It is well-known that the Exp3 algorithm is a no-regret learner for $\varepsilon = \alpha \sim T^{-\beta}$ with $\beta \in (0,1)$ \citep{auer2002bandit}. 

\subsection{Q-Learning} \label{sec:qlearner}

Q-learning is the most well-known reinforcement learning algorithm. This is probably one reason, why it is predominantly used in the literature on algorithmic collusion \citep{Calvano.2020}, and specifically by \citet{banchio2022artificial}. 
In contrast to multi-armed bandit algorithms, Q-learning can handle different states of the world in which other actions might be optimal. Note that in \citet{banchio2022artificial} there is only one state and Q-learning reduces to an algorithm for a multi-armed bandit problem, a simple version of reinforcement learning.

In Algorithm \ref{alg:q_learning_auction}, the input includes the discount factor $\gamma$, the learning rate $\alpha$, and the number of episodes $T$. The output is the learned Q-values for all state-action pairs. The Q-table is initialized with some initial value for all state-action pairs. The Q-learning update rule updates the Q-table based on the observed reward and the new state. The exploration-exploitation strategy used to select actions should be chosen carefully to balance exploration with exploitation. 
Similar to \citet{banchio2022artificial}, we focus on the $\varepsilon$-greedy rule, where the agent takes the action that maximizes the Q-value with probability $1-\varepsilon$ (exploitation) or takes an action at random with probability $\varepsilon$ (exploration). This allows us to compare our results to theirs. 
The exploration probability decays over time, e.g., according to $\varepsilon = \epsilon \exp ( - \beta t)$. 
\begin{algorithm}[ht]
	\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
	\Input{Discount factor $\gamma$, learning rate $\alpha$, number of rounds $T$,\newline exploration parameter ($\epsilon,\, \beta$), initial Q-values $Q_0$}
	Initialize Q-values $Q(b) = Q_0$ for all $b \in \Acal^d$\;
	\For{$t=1,2,\dots,T$}{
		Set $\varepsilon = \epsilon \exp ( - \beta t) $\;
		Choose action $b_t \in \argmax_{b \in \Acal^d} Q(b)$ with probability $1-\varepsilon$ or at random with probability $\varepsilon$\;
		Receive reward $r_t$\;
		Update Q-value $Q(b_t) = (1-\alpha) Q(b_t) + \alpha \left[ r_t + \gamma \max_b Q_t(b) \right] $\;
	}
	\caption{Q-Learning Algorithm without States}
	\label{alg:q_learning_auction}
\end{algorithm}
Note that in general, the continuation value is the maximal Q-value of the following state $s_{t+1}$, i.e., $\gamma \max_b Q_t(b, s_{t+1})$. Instead, similar to \citet{banchio2022artificial}, we consider a simplified version without states which reduces the Q-table to a Q-vector. Our baseline implementation is also \textit{optimistic} Q-learning, where the Q-vector is initialized such that the value of Q is larger than the maximum payoff that could ever be achieved. The purpose of such an initialization is to ensure that the algorithm wonâ€™t stop experimenting until all of the Q-values have sufficiently decreased. \citet{banchio2022artificial} argue that in their multi-agent setting, the advantage offered by optimism is a phase of experimentation at the beginning, which improves convergence. We find that the initialization is crucial and also explore alternative initializations in our experiments. 

\subsection{Extensions to Model Incomplete Information} \label{sec:pop_game}

The algorithms described so far were based on the model of complete-information game where the values of the bidders are publicly known. 
In practice, bidders do not have complete information and algorithms need to take into account incomplete information.
This can be done by discretizing the type or valuation space $\Vcal^d := \{v_1, \dots, v_n \} \subset \Vcal$ of bidders with a corresponding prior distribution $F^d$. 
The discrete spaces can be constructed from the continuous game $\Gcal$ using a subset of equidistant points from the spaces and a numerical integration rule to derive a discretized prior $F^d$.
Overall, this defines a discrete incomplete-information game $\Gcal^d = (\Ical, \Vcal^d, \Acal^d, u, F^d)$ for the standard continuous Bayesian game as it is described in textbooks \citep{krishna2009auction}.

The standard bandit algorithms are not defined for games with incomplete information. However, an interpretation as a population game \citep{hartline2015NoRegretLearningBayesian} allows for an extension that can be readily implemented. 
For this, we consider $N$ populations, where each population $i \in \Ical$ consists of $n = \vert \Vcal^d \vert$ players. All players within a population have different types $v \in \Vcal^d$. We write $v_i$ the player from population $i$ with valuation $v \in \Vcal^d$. In the population game, nature draws a player from each population according to the prior $F^d$, i.e., $v_i \sim F^d$ for all populations $i \in \Ical$, who play against each other. Since this game is a complete-information game with finite actions, we can directly apply the algorithms described previously and learn a strategy for each $v_i$ of some population $i$.
Aggregating the strategies over all $v_i$ gives us a strategy for agent $i$ in the discrete incomplete-information game.


\subsection{SODA} \label{sec:soda}
Let us also describe a gradient-based method, the SODA learning algorithm \citep{bichler2023soda}. 
This method provides us with an equilibrium strategy to compare against in situations where no analytical solution is available.

% Discrete distributional strategies
The SODA learning algorithm also acts on the discretized auction game $G^d$. The main idea is to use distributional strategies in the discretized auction game. Distributional strategies are an extension of mixed strategies in complete-information games to settings with incomplete information \citep{Milgrom1985}. 
They can be represented by matrices $s \in \R^{n \times m}$, where each entry $s_{ij}$ of this matrix represents the probability of a valuation-action pair $(v_i, b_j) \in \Vcal^d \times \Acal^d$. 
To be consistent with the auction game the distributional strategy has to satisfy the marginal condition  $\sum_{j=1}^m s_{ij} = F^d(v_i)$, i.e., the probability of a value over all actions has to be equal to the probability of the value given by the prior. We denote the set of such distributional strategies by $\Scal$.
Given a profile of such discrete distributional strategies, one can compute the expected utility $ \tilde u_i $ as sum over all outcomes weighted by the probabilities induced by $ s = (s_1,\dots, s_N) $.
If we consider an example with $N=2$ agents, we get an expected utility for agent 1 by
\begin{equation} \label{eq:exp_util_distr}
	\tilde u_1(s_1, s_2) = \sum_{i_1,j_1 = 1}^{n,m} (s_1)_{i_1j_1} \sum_{i_2,j_2 = 1}^{n,m} (s_2)_{i_2j_2} \, u_1(b_{j_1}, b_{j_2}, v_{i_1}),
\end{equation}
where $u_1(b_1, b_2, v)$ is the ex-post utility. 

% Approximation Game
The expected utility together with the set of distributional strategies allows us to interpret the auction game as a complete-information game $ \Gamma = (\Ical, \Scal, \tilde u) $. 
Note that the set of discrete distributional strategies is a compact and convex subset of $ \R^{n \times m} $ and the expected utility is differentiable and linear in the bidder's own strategy. This allows us to rely on standard tools from online convex optimization to compute the NE of $ \Gamma $, which corresponds to a BNE of $ \Gcal^d $.

% Algorithms
In our experiments, we use dual averaging \citep{nesterov2009PrimaldualSubgradientMethods} with an entropic regularization term (known as exponentiated gradient ascent) and mirror ascent \citep{nemirovskij1983problem} with an Euclidean mirror map (standard projected gradient ascent).
\begin{algorithm}[h]
	\SetAlgoNoLine
	\KwIn{Initial distributional strategy $s_1$, sequence of step sizes $\{\eta_t\}_{t=1}^T$}
	Initialize $y_{i,t} = 0$ for all $i \in \Ical$
	\For{$t = 1, 2, \dots, T$}
	{
		\For{each agent $i\in \mathcal I$}
		{
			observe gradient $c_{i,t} = \nabla_{s_i} \tilde u_i(s_{i,t},s_{-i,t})$\;
			update dual variable $y_{i,t+1} = y_{i,t} + \eta_t c_{i,t}  $\;
			update strategy $s_{i,t+1} = \argmin_{s \in \Scal^d} \Vert s - y_{i,t+1} \Vert_2^2 $\;
		}
	}
	\caption{Simultaneous Online Dual Averaging (SODA)}
	\label{algo:soda}
\end{algorithm}
Given the respective method, all agents simultaneously compute their individual gradients and perform the corresponding update step, as described in Algorithm \ref{algo:soda} for dual averaging. 
Specifically for dual averaging, one can show that if this procedure converges to a single point $s \in \Scal^d$, then this strategy profile is a NE in the approximation game, and thereby a BNE for the discretized auction game \citep[Corollary~1]{bichler2023soda}. Moreover, for some single-object auction formats, such as first- or second-price sealed-bid and all-pay auctions, it is shown that if SODA finds an approximate equilibrium of the discretized auction game, this is also an approximate equilibrium of the continuous game \citep[Theorem~1]{bichler2023soda}. Therefore, SODA provides an ex-post certificate. 

Note that SODA serves only as a baseline as it allows us to approximate the BNE quickly and with high accuracy. It is not meant as an algorithm to simulate display ad auctions. While bandit algorithms such as Exp3 only require information about the price and whether they won or lost in an auction, in SODA the auctioneer would need all agents' mixed strategies to provide the respective feedback for the agents. 

\section{Results}
In this section we report the results of learning dynamics under different learning algorithms, different informational assumptions, and different utility models, to analyze revenue in repeated first-price auctions compared to their second-price counterparts.

\subsection{Experimental Design} \label{sec:exp_design}
In our experiments we simulate $N \in \{2, 3\}$ independent agents bidding in repeated first- and second-price auctions. Each experiment consists of $T$ iterations and is repeated $10$ times.
We focus on experiments with bandit feedback, where in each iteration $t \in \lbrace 1, \dots, T \rbrace$, agents submit their bid from some discrete action space $b_t \in \Acal^d \subset \Acal = [0,1]$, observe their utility (bandit feedback), and update their bidding strategies according to some learning algorithm. The objective of the agents is to maximize their utility, which depends on the bids of all agents and on their own valuation $v_t \in \Vcal^d \subset \Vcal = [0,1]$ from some discrete type space.

We will first discuss the \textit{complete-information model} where the agents have a fixed valuation $v_t=1$. We are interested in comparing the performance of different learning algorithms in the first- and second-price auction with payoff-maximizing agents, i.e., quasi-linear utility functions. We run each experiment for $T=5\cdot 10^5$ iterations.

In the \textit{incomplete-information model}, the valuations of each agent are drawn uniformly (i.i.d.) in each iteration, i.e., $v_t \sim U(\Vcal^d)$. Using the population game interpretation described in Section \ref{sec:pop_game}, we learn a separate strategy for each of the agent's valuations, which allows us to use the algorithms from the complete-information setting. 
Since each agent-value pair gets less feedback that way, we run these experiments for $T = 10^7$ iterations. The experiments differ not only in the payment rule but also in the utility function of the agents, namely quasi-linear (QL), ROI, and ROSB. For ROI and ROSB to be well defined, we introduce a reserve price $r=0.05$, which we also use in the QL setting for better comparison. The budget parameter for ROSB maximizing agents is equal to $B = 1.01$ in what we report. In addition to the bandit algorithms, we use SODA to get a good approximation of the equilibrium strategies in the incomplete-information models and to serve as a baseline for the algorithms with bandit feedback. 

\subsection{Complete-Information Models} \label{sec:results_compl_info}

\paragraph{Setting} Two agents $i \in \{1,2\}$ bid in a first- and second-price auction. All agents value the item at $v = 1$ and choose bids from $\Acal^d = \{0.05, 0.10, \dots, 0.90, 0.95 \}$. 
For the first-price auction, it can be shown that with two agents and a random tie-breaking rule, the NE is to bid $b_1 = b_2 = 0.95$ or $b_1 = b_2 = 0.90$ for both of them. With more than 2 bidders, the NE is to only bid $b_i =0.95$ for all $i \in \{1, \dots, N\}$. In the second-price auction, the unique NE is $b_1 = b_2 = 0.95$.

\paragraph{Results}
Below, we report on experiments with Exp3 and Q-learning with the parameters as defined in Table \ref{tab:param_compl_info}.
The main results are summarized in Figure \ref{fig:q_learner}.     

\input{tables/table_learner_compl_info}

\begin{result}
	The collusive outcomes by using Q-Learning in the complete-information setting from \citet{banchio2022artificial} relies on the parameters of the Q-values. We find evidence that optimistic initialization enables collusive outcomes while lower initializations or lower discount rates lead to higher prices closer to the equilibrium. 
\end{result}

First, we consider two symmetric agents using variations of the Q-learner considered by \citet{banchio2022artificial}, which serves as a baseline. \citet{banchio2022artificial} used an optimistic initialization for their learners, which we replicate by setting the initial Q-values to $\tfrac{1}{1-\gamma}$. 
We can reproduce their results but observe that parameter changes lead to higher prices compared to this baseline Q-learner (see Figure \ref{fig:q_learner}). 

Without the decreasing exploration rate (fix $\varepsilon$), we see cyclic behavior where the q-learner often returns to low prices but fails to converge to a fixed policy. 
Even more interesting is the observation that initializing the Q-values with zero (and fixed exploration rate) instead of optimistic leads to a completely different outcome where the learner converges to the Nash equilibrium. 
One possible explanation is that collusive prices are more stable for higher Q-values since the rewards are much higher. 
This way, the learning algorithm might get stuck with lower bids. 
If we remove this hidden preference for low actions by initializing differently, higher actions are played. 
It is also interesting to note that if we stick to optimistic initialization but have a small discount factor $\gamma$, the learners also converge to higher prices closer to the equilibrium.

\begin{figure}[h]
	\begin{center}
	\begin{subfigure}{0.45\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/compl_ql_fp_2_bids_sym_qlearner.pdf}
		\caption{Q-Learner vs. Q-Learner (Symmetric Agents)}
	\end{subfigure}
	\begin{subfigure}{0.45\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/compl_ql_fp_2_bids_asym_qlearner.pdf}
		\caption{Q-Learner vs. Exp3 (Asymmetric Agents)}
	\end{subfigure}
	\caption{Median Bids for 2 Agents in the FPSB Auction Using Different Q-Learners and Exp3}
	\label{fig:q_learner}
	\end{center}
	\footnotesize We split the learning process into intervals of two thousand iterations and compute the median bid of an agent for each interval. The shaded area shows the mean $ \pm $ std of these median bids over 10 runs, i.e., repetitions of the experiment, for one agent. In the first plot, we show experiments for different parameters of the Q-learner used by both agents (symmetric). In the second plot, we show the results for agents using different learning algorithms (asymmetric), i.e., Q-learner with different parameters and Exp3, to play against an agent using the Exp3 learning algorithm. Note that we report the median instead of the average bid since the exploration makes the average bid harder to interpret, especially when we converge to high bids.
\end{figure}

\begin{result}
	If we use an Exp3 and a Q-learning algorithms, Q-learning becomes more robust and agents bid significantly higher compared to the symmetric case with identical Q-learners.
\end{result}

The second plot in Figure \ref{fig:q_learner} shows additional experiments where the second agent uses the Exp3 algorithm, which we use as an example for no-regret bandit algorithms. If both agents use Exp3, they converge to a Nash equilibrium quickly. If Q-learners compete against Exp3, we observe significantly higher bids compared to the symmetric settings in the first plot. In most cases, the Q-learner even converges to a Nash equilibrium, even though it takes more iterations.

\subsection{Incomplete-Information Model}
In the following experiments we make the arguably more realistic assumption that valuations of opponents are only known in distribution and study this Bayesian setting under different utility models. 
%This is a realistic assumption and we know little about ex-ante convergence of bandit learning algorithms in auctions with incomplete information.
%The equilibrium strategies in these different settings might explain lower revenue in the first-price auctions as already indicated in Figure \ref{fig:analytical_QL_ROI_uniform}. While the computation of the equilibrium functions, i.e., solving the system of ODEs, might be hard, we show that simple learning algorithms find these equilibrium. 

\subsubsection{Equilibria Learned with SODA} \label{sec:results_soda}
Before we analyze bandit algorithms in this setting, we use SODA on a finer discretization to get a good approximation of the equilibrium strategies since analytical solutions are not available for all settings (see discussion in Section \ref{sec:equi}).

\paragraph{Setting} 
The action and type space $\Acal = \Vcal = [0,1]$ of the $N=2$ agents are discretized using $n=m=64$ equidistant points. We consider the first- and second-price auction under different utility models, namely QL, ROI, and ROSB maximizing agents.

\paragraph{Results}
To measure the quality of computed strategies, we use the \textit{relative utility loss $\ell$} as a metric. Given a distributional strategy $s_i$, $\ell$ measures the relative improvement of the expected utility we can get by best responding to the other agents strategies $s_{-i}$, i.e., 
\begin{equation} \label{eq:discr_util_loss}
	\ell(s_i, s_{-i}) = 1 - \dfrac{\tilde u_i(s_i^{br}, s_{-i})}{\tilde u_i(s_i, s_{-i})}.
\end{equation}
Note that in the discretized game, the best response $s_i^{br}$ is the solution of a simple LP. If the absolute utility loss $\ell \cdot \tilde u_i(s_i, s_{-i}) < \varepsilon$ for all agents, the computed strategy profile is a $\varepsilon$-BNE in the discretized auction game.
In our experiments we observe that on average $\ell \leq 10^{-3}$ in all instances. Moreover, we observe that we closely approximate the known equilibrium strategies for QL and ROI. More details on these experiments and additional results for $N=3$ agents can be found in the Appendix \ref{app:soda}.

\begin{figure}[h]
	\begin{center}
	\begin{subfigure}{0.32\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/revenue_strat_fp_2.pdf}
		\caption{[First-Price}
	\end{subfigure}
	\begin{subfigure}{0.32\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/revenue_strat_sp_2.pdf}
		\caption{[Second-Price}
	\end{subfigure}
	\begin{subfigure}{0.32\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/revenue_2.pdf}
		\caption{Expected Revenue}
	\end{subfigure}
	\caption{Results for SODA with 2 Bidders with Uniform Prior }
	\label{fig:soda}
	\end{center}
	\footnotesize 
	The first two plots show the mean and standard deviation of 100 sampled bids from the computed strategies for each discrete valuation in settings with $n=2$ bidders, uniform prior, and different utility functions for the first- and second-price auction. The black line shows the analytical BNE (only known for QL and ROI). In the third plot we use the computed strategies to approximate the expected revenue by simulating $2^{22}$ auctions for each setting.
\end{figure}

We use the computed equilibrium strategies to simulate auctions to approximate the expected revenue.
An important insight from this analysis (see Figure \ref{fig:soda}) is that revenue equivalence breaks for return-on-invest and return-on-spend (with budget) maximizing agents and that revenue in first-price auctions is lower compared to the second-price auctions in equilibrium.

\subsubsection{Bandit Algorithms} \label{sec:bandit_bayesian}
Real-world bidding agents only get bandit feedback, and the question is now whether the revenue in the first-price auction is also lower when we employ bandit algorithms (e.g., Exp3) in the ROI and the ROSB utility models. Since learning is much slower for learners with bandit feedback compared to SODA, we use a coarser discretization similar to the complete information setting.

\paragraph{Setting} We discretize the type and action space with $\Vcal^d = \Acal^d = \{0, 0.05, \dots, 0.95, 1\}$ which corresponds to $n=m=21$ discretization points. We focus on repeated first- and second-price auctions with $N=3$ agents\footnote{In this coarse discretization, bidding the reserve price for all valuations is the BNE for ROI and ROSB maximizing agents if we only consider $N=2$ agents. Therefore, we choose $N=3$.} and different assumptions on the utility function. As described in Section \ref{sec:pop_game}, each agent is represented by a population, which consists of different valuations. This means each agent (i.e. population) has $n=21$ different instances of the bandit learner, i.e., one for each valuation. To get comparable results to the complete information, we increase the number of iterations approximately by the number of discrete values, which leads to $10^7$ iterations instead of $5 \cdot 10^5$. 
In our experiments we use Exp3 with a fix exploration rate $\varepsilon=0.01$ and learning rate $\alpha=0.01$. For ROI and ROSB we use a smaller learning rage $\alpha=0.0005$ since the magnitude of the utilities is larger. 

\paragraph{Results}
First, we observe that the revenues generated by the agents using Exp3 show a similar pattern to the expected revenues in equilibrium (see previous subsection).
We see that revenue equivalence does not hold for ROI and ROSB maximizing agents and especially for ROI maximizing agents, the revenue for the first-price auction is significantly lower compared to the second-price auction. 
Furthermore, the average revenue matches the values predicted by the Bayes-Nash equilibrium strategies in the discretized setting. 
These are approximated using SODA, where the strategies are updated using mirror ascent with a decreasing stepsize $\eta = 0.025 \cdot t^{-0.05}$. 
The computed strategies have a relative utility loss $\ell < 0.1\%$.

\begin{figure}[h]
	\begin{center}
		\begin{subfigure}{0.32\textwidth}
			\centering
			\includegraphics[width=\textwidth]{figures/revenue_bandit_ql.pdf}
			\caption{Payoff-Maximizing}
		\end{subfigure}
		\begin{subfigure}{0.32\textwidth}
			\centering
			\includegraphics[width=\textwidth]{figures/revenue_bandit_roi.pdf}
			\caption{ROI-Maximizing}
		\end{subfigure}
		\begin{subfigure}{0.32\textwidth}
			\centering
			\includegraphics[width=\textwidth]{figures/revenue_bandit_rosb.pdf}
			\caption{ROSB-Maximizing}
		\end{subfigure}
		\caption{Revenue for Exp3 with 3 Bidders with Uniform Prior}
		\label{fig:exp3_incomplete}
	\end{center}
	\footnotesize
	We run Exp3 for $10$ million iterations and compute the average revenue for all $20\thinspace000$ iteration intervals for payoff, ROI and ROSB maximizing agents. We plot the mean and standard deviations of this average revenue per interval over 10 runs. The black horizontal lines denote the expected revenue we would get with the BNE strategies computed using SODA.
\end{figure}

Not only is the average revenue close to the predictions from the equilibrium analysis, but we can also observe that the agents actually learn to play according to the Bayes-Nash equilibrium using a bandit algorithm such as Exp3. 
To this end, we look at the frequency of the bids within an interval and compare it to the distributional equilibrium strategy computed using SODA. 
\begin{result}
	If we use Exp3 in the incomplete-information auction setting, the agents learn to bid according to the BNE and revenues for the first-price auction are lower compared to the second-price auctions for ROI and ROSB maximizing agents, as predicted by the equilibrium analysis.
\end{result}
As we can see in Figure \ref{fig:exp3_ql} - \ref{fig:exp3_roi}, the empirical frequency of actions played by Exp3 converges to the equilibrium strategies. Similar plots for ROSB-maximizing agents and the second-price counterparts and a Gaussian prior can be found in the Appendix \ref{app:bandit_bayesian}.

\begin{figure}[h]
	\begin{center}
	\includegraphics[width=0.95\textwidth]{figures/learning_ql_fp_3.pdf}
	\caption{Strategies for Payoff-Maximizing Agents using Exp3 in the FPSB Auction with Uniform Prior}
	\label{fig:exp3_ql}
	\end{center}
	\footnotesize
	We run Exp3 for $10$ million iterations and visualize the frequency of the last $20\thinspace000$ bids w.r.t. the values after $0.4$, $1$, and $10$ million iterations. On the last plot, we show the distributional equilibrium strategy computed by SODA. The colored lines denote the BNE in the continuous setting.
\end{figure}

\begin{figure}[h!]
	\begin{center}
		\includegraphics[width=0.95\textwidth]{figures/learning_roi_fp_3.pdf}
		\caption{Strategies for ROI-Maximizing Agents using Exp3 in the FPSB Auction with Uniform Prior}
		\label{fig:exp3_roi}
	\end{center}
	\footnotesize
	We run Exp3 for $10$ million iterations and visualize the frequency of the last $20\thinspace000$ bids w.r.t. the values after $0.4$, 
	$1$, and $10$ million iterations. On the last plot, we show the distributional equilibrium strategy computed by SODA. The colored lines denote the BNE in the continuous setting.
\end{figure}

\section{Conclusions}
%%%%%%%%%%%%%%%%%%%%%%%%%%%
In this paper, we study the impact that a move from second-price to first-price auctions can have on the revenue of an ad exchange that faces learning agents.  
Recent research provided evidence that Q-learning in repeated auctions in a complete-information model with payoff-maximizing agents can lead to tacit collusion and lower revenue in the first-price auction \citep{banchio2022artificial}. 
Our experiments show that with different hyper-parameters, Q-learning does converge to the Nash equilibrium in this model. Importantly, when bidders use other bandit algorithms, we do not find evidence for algorithmic collusion and revenue equivalence holds in this model. Recent theoretical results confirm that a wide range of mean-based learning algorithms converge to equilibrium.
While \citet{banchio2022artificial} show that algorithmic collusion can occur with complete information, these results suggest that the phenomenon might be limited to specific algorithms and parameter settings.  

In real-world display ad auctions, bidders neither have complete information nor do they necessarily maximize payoff. ROI or ROSB maximization is widely described in the literature. First, we show that gradient-based learners and bandit learners converge to an equilibrium in all three utility models. This suggests that tacit collusion is also less of a concern in the standard incomplete-information model. However, revenue equivalence breaks in auctions with ROI- or ROSB-maximizing bidders, and the revenue of the first-price auction is lower in such models.

As in prior research about algorithmic collusion, we analyze models of repeated identical auctions, where agents maximize their separable utility for individual auctions. In the field, the types of impressions on auctions vary over time and so does the competition. Also, the algorithms used by bidders in display ad auctions are likely to differ across bidding firms. 
What we can say is that the phenomenon of algorithmic collusion does not arise with a variety of bandit learning algorithms that have been cited in the literature and that provide natural candidates. This makes algorithmic collusion less likely to appear in real-world environments as well. 
Interestingly, we find that for the very different utility models and incomplete information, bandit as well as gradient-based algorithms converge to an equilibrium. The reasons for such consistent convergence in incomplete-information models are of independent interest but outside the scope of this paper (see \citet{bichler2023soda} for a related discussion). 


%\begin{comment}
\section*{Acknowledgments}
This project was funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) - GRK 2201/2 - Project Number 277991500 and BI 1057/9. The authors would also like to thank Markus Ewert and Fabian R. Pieroth for their support.

%\end{comment}

\bibliographystyle{informs2014} 
%\bibliography{literature}

%%% Uncomment this line and comment out the ``thebibliography'' section below to use the external .bib file (using bibtex) .


%%% Uncomment this section and comment out the \bibliography{references} line above to use inline references.
\begin{thebibliography}{68}
	\providecommand{\natexlab}[1]{#1}
	\providecommand{\url}[1]{\texttt{#1}}
	\providecommand{\urlprefix}{URL }
	
	\bibitem[{Alcobendas \protect\BIBand{}
		Zeithammer(2021)}]{alcobendas2021adjustment}
	Alcobendas M, Zeithammer R (2021) Adjustment of bidding strategies after a
	switch to first-price rules. \emph{Available at SSRN 4036006} .
	
	\bibitem[{Armantier et~al.(2008)Armantier, Florens, \protect\BIBand{}
		Richard}]{armantier2008ApproximationNashEquilibria}
	Armantier O, Florens JP, Richard JF (2008) Approximation of {{Nash}} equilibria
	in {{Bayesian}} games. \emph{Journal of Applied Econometrics} 23(7):965--981,
	ISSN 08837252, 10991255, \urlprefix\url{http://dx.doi.org/10.1002/jae.1040}.
	
	\bibitem[{Asker et~al.(2022)Asker, Fershtman, \protect\BIBand{}
		Pakes}]{Asker.2022}
	Asker J, Fershtman C, Pakes A (2022) {Artificial Intelligence, Algorithm
		Design, and Pricing}. \emph{{AEA Papers and Proceedings}} 112:452--456,
	\urlprefix\url{http://dx.doi.org/10.1257/pandp.20221059}.
	
	\bibitem[{Assad et~al.(2020)Assad, Clark, Ershov, \protect\BIBand{}
		Xu}]{Assad.2020}
	Assad S, Clark R, Ershov D, Xu L (2020) \emph{{Algorithmic Pricing and
			Competition: Empirical Evidence from the German Retail Gasoline Market}}.
	\urlprefix\url{http://dx.doi.org/10.2139/ssrn.3682021}.
	
	\bibitem[{Auer et~al.(2002)Auer, Cesa-Bianchi, Freund, \protect\BIBand{}
		Schapire}]{auer2002bandit}
	Auer P, Cesa-Bianchi N, Freund Y, Schapire RE (2002) The nonstochastic
	multiarmed bandit problem. \emph{SIAM Journal on Computing} 32(1):48--77,
	\urlprefix\url{http://dx.doi.org/10.1137/S0097539701398375}.
	
	\bibitem[{Ausubel \protect\BIBand{}
		Baranov(2020)}]{ausubel2019CoreselectingAuctionsIncomplete}
	Ausubel LM, Baranov O (2020) Core-selecting auctions with incomplete
	information. \emph{International Journal of Game Theory} 49:251--273, ISSN
	1432-1270, \urlprefix\url{http://dx.doi.org/10.1007/s00182-019-00691-3}.
	
	\bibitem[{Bailey \protect\BIBand{}
		Piliouras(2018)}]{bailey2018MultiplicativeWeightsUpdate}
	Bailey JP, Piliouras G (2018) Multiplicative weights update in zero-sum games.
	\emph{Proceedings of the 2018 {{ACM}} Conference on Economics and
		Computation}, 321--338 ({ACM}).
	
	\bibitem[{Balduzzi et~al.(2018)Balduzzi, Racaniere, Martens, Foerster, Tuyls,
		\protect\BIBand{} Graepel}]{balduzzi2018mechanics}
	Balduzzi D, Racaniere S, Martens J, Foerster J, Tuyls K, Graepel T (2018) The
	mechanics of n-player differentiable games. Dy J, Krause A, eds.,
	\emph{Proceedings of the 35th International Conference on Machine Learning},
	volume~80 of \emph{Proceedings of Machine Learning Research}, 354--363
	(PMLR), \urlprefix\url{https://proceedings.mlr.press/v80/balduzzi18a.html}.
	
	\bibitem[{Balseiro \protect\BIBand{} Gur(2019)}]{balseiro2019learning}
	Balseiro SR, Gur Y (2019) Learning in repeated auctions with budgets: Regret
	minimization and equilibrium. \emph{Management Science} 65(9):3952--3968.
	
	\bibitem[{Banchio \protect\BIBand{} Skrzypacz(2022)}]{banchio2022artificial}
	Banchio M, Skrzypacz A (2022) Artificial intelligence and auction design.
	\emph{ACM Conference on Economics and Computation}, 747.
	
	\bibitem[{Bichler et~al.(2021)Bichler, Fichtl, Heidekrueger, Kohring,
		\protect\BIBand{} Sutterer}]{bichler2021npga}
	Bichler M, Fichtl M, Heidekrueger S, Kohring N, Sutterer P (2021) Learning
	equilibria in symmetric auction games using artificial neural networks.
	\emph{Nature Machine Intelligence} 687--695.
	
	\bibitem[{Bichler et~al.(2023)Bichler, Fichtl, \protect\BIBand{}
		Oberlechner}]{bichler2023soda}
	Bichler M, Fichtl M, Oberlechner M (2023) Computing {Bayes} {Nash} equilibrium
	strategies in auction games via simultaneous online dual averaging.
	\emph{Operations Research} to appear.
	
	\bibitem[{Bichler \protect\BIBand{} Paulsen(2018)}]{bichler2018principal}
	Bichler M, Paulsen P (2018) A principal-agent model of bidding firms in
	multi-unit auctions. \emph{Games and Economic Behavior} 111:20--40.
	
	\bibitem[{Blum \protect\BIBand{}
		Mansour(2007)}]{blum2007ExternalInternalRegret}
	Blum A, Mansour Y (2007) From {{External}} to {{Internal Regret}}.
	\emph{Journal of Machine Learning Research} 8:1307--1324, ISSN 1532-4435.
	
	\bibitem[{Blume \protect\BIBand{} Heidhues(2008)}]{blume2008modeling}
	Blume A, Heidhues P (2008) Modeling tacit collusion in auctions. \emph{Journal
		of Institutional and Theoretical Economics (JITE)/Zeitschrift f{\"u}r die
		gesamte Staatswissenschaft} 163--184.
	
	\bibitem[{Borgs et~al.(2007)Borgs, Chayes, Immorlica, Jain, Etesami,
		\protect\BIBand{} Mahdian}]{borgs2007dynamics}
	Borgs C, Chayes J, Immorlica N, Jain K, Etesami O, Mahdian M (2007) Dynamics of
	bid optimization in online advertisement auctions. \emph{Proceedings of the
		16th international conference on World Wide Web}, 531--540.
	
	\bibitem[{Bosshard et~al.(2017)Bosshard, B{\"u}nz, Lubin, \protect\BIBand{}
		Seuken}]{bosshard2017computing}
	Bosshard V, B{\"u}nz B, Lubin B, Seuken S (2017) Computing bayes-nash
	equilibria in combinatorial auctions with continuous value and action spaces.
	\emph{IJCAI}, 119--127.
	
	\bibitem[{Bosshard et~al.(2020)Bosshard, B{\"u}nz, Lubin, \protect\BIBand{}
		Seuken}]{bosshardComputingBayesNashEquilibria2020}
	Bosshard V, B{\"u}nz B, Lubin B, Seuken S (2020) Computing bayes-nash
	equilibria in combinatorial auctions with verification. \emph{Journal of
		Artificial Intelligence Research} 69:531--570.
	
	\bibitem[{Bowling(2005)}]{bowling2005ConvergenceNoRegretMultiagent}
	Bowling M (2005) Convergence and {{No}}-{{Regret}} in {{Multiagent Learning}}.
	Saul LK, Weiss Y, Bottou L, eds., \emph{Advances in {{Neural Information
				Processing Systems}} 17}, 209--216 ({MIT Press}).
	
	\bibitem[{Bowling \protect\BIBand{}
		Veloso(2002)}]{bowling2002MultiagentLearningUsing}
	Bowling M, Veloso M (2002) Multiagent learning using a variable learning rate.
	\emph{Artificial Intelligence} 136(2):215--250, ISSN 0004-3702,
	\urlprefix\url{http://dx.doi.org/10.1016/S0004-3702(02)00121-2}.
	
	\bibitem[{Braverman et~al.(2017)Braverman, Mao, Schneider, \protect\BIBand{}
		Weinberg}]{braverman2017selling}
	Braverman M, Mao J, Schneider J, Weinberg SM (2017) Selling to a no-regret
	buyer.
	
	\bibitem[{Brown \protect\BIBand{} MacKay(2023)}]{brown2023competition}
	Brown ZY, MacKay A (2023) Competition in pricing algorithms. \emph{American
		Economic Journal: Microeconomics} 15(2):109--156.
	
	\bibitem[{Busoniu et~al.(2008)Busoniu, Babuska, \protect\BIBand{}
		De~Schutter}]{busoniu2008ComprehensiveSurveyMultiagent}
	Busoniu L, Babuska R, De~Schutter B (2008) A {{Comprehensive Survey}} of
	{{Multiagent Reinforcement Learning}}. \emph{IEEE Transactions on Systems,
		Man, and Cybernetics, Part C (Applications and Reviews)} 38(2):156--172, ISSN
	1094-6977, \urlprefix\url{http://dx.doi.org/10.1109/TSMCC.2007.913919}.
	
	\bibitem[{Cai et~al.(2017)Cai, Ren, Zhang, Malialis, Wang, Yu,
		\protect\BIBand{} Guo}]{cai2017real}
	Cai H, Ren K, Zhang W, Malialis K, Wang J, Yu Y, Guo D (2017) Real-time bidding
	by reinforcement learning in display advertising. \emph{Proceedings of the
		Tenth ACM International Conference on Web Search and Data Mining}, 661--670.
	
	\bibitem[{Calvano et~al.(2020)Calvano, Calzolari, Denicol{\`o},
		\protect\BIBand{} Pastorello}]{Calvano.2020}
	Calvano E, Calzolari G, Denicol{\`o} V, Pastorello S (2020) {Artificial
		Intelligence, Algorithmic Pricing, and Collusion}. \emph{{American Economic
			Review}} 110(10):3267--3297, ISSN 0002-8282,
	\urlprefix\url{http://dx.doi.org/10.1257/aer.20190623}.
	
	\bibitem[{Cheng et~al.(2019)Cheng, Zou, Zhuang, Liu, Xu, \protect\BIBand{}
		Zhang}]{cheng2019extensible}
	Cheng Y, Zou L, Zhuang Z, Liu J, Xu B, Zhang W (2019) An extensible approach
	for real-time bidding with model-free reinforcement learning.
	\emph{Neurocomputing} 360:97--106.
	
	\bibitem[{Choi et~al.(2020)Choi, Mela, Balseiro, \protect\BIBand{}
		Leary}]{choi2020online}
	Choi H, Mela CF, Balseiro SR, Leary A (2020) Online display advertising
	markets: A literature review and future directions. \emph{Information Systems
		Research} 31(2):556--575.
	
	\bibitem[{Daskalakis et~al.(2009)Daskalakis, Goldberg, \protect\BIBand{}
		Papadimitriou}]{daskalakis2009complexity}
	Daskalakis C, Goldberg PW, Papadimitriou CH (2009) The complexity of computing
	a nash equilibrium. \emph{SIAM Journal on Computing} 39(1):195--259.
	
	\bibitem[{Deng(2023)}]{deng2023we}
	Deng A (2023) What do we know about algorithmic collusion now? new insights
	from the latest academic research. \emph{SSRN,
		https://ssrn.com/abstract=4521959} .
	
	\bibitem[{Deng et~al.(2022)Deng, Hu, Lin, \protect\BIBand{}
		Zheng}]{deng2022nash}
	Deng X, Hu X, Lin T, Zheng W (2022) Nash convergence of mean-based learning
	algorithms in first price auctions. \emph{Proceedings of the ACM Web
		Conference 2022}, 141--150.
	
	\bibitem[{Despotakis et~al.(2021)Despotakis, Ravi, \protect\BIBand{}
		Sayedi}]{despotakis2021first}
	Despotakis S, Ravi R, Sayedi A (2021) First-price auctions in online display
	advertising. \emph{Journal of Marketing Research} 58(5):888--907.
	
	\bibitem[{Edelman et~al.(2007)Edelman, Ostrovsky, \protect\BIBand{}
		Schwarz}]{edelman2007internet}
	Edelman B, Ostrovsky M, Schwarz M (2007) Internet advertising and the
	generalized second-price auction: Selling billions of dollars worth of
	keywords. \emph{American economic review} 97(1):242--259.
	
	\bibitem[{Fabra(2003)}]{fabra2003tacit}
	Fabra N (2003) Tacit collusion in repeated auctions: uniform versus
	discriminatory. \emph{The Journal of Industrial Economics} 51(3):271--293.
	
	\bibitem[{Fikioris \protect\BIBand{} Tardos(2022)}]{Tardos2022}
	Fikioris G, Tardos E (2022) Liquid welfare guarantees for no-regret learning in
	sequential budgeted auctions.
	\urlprefix\url{http://dx.doi.org/10.48550/ARXIV.2210.07502}.
	
	\bibitem[{Fudenberg \protect\BIBand{}
		Levine(2009)}]{fudenbergLearningEquilibrium2009}
	Fudenberg D, Levine DK (2009) Learning and {{Equilibrium}}. \emph{Annual Review
		of Economics} ISSN 1941-1383,
	\urlprefix\url{http://dx.doi.org/10.1146/annurev.economics.050708.142930}.
	
	\bibitem[{Greenwald \protect\BIBand{} Kephart(2000)}]{greenwald2000shopbots}
	Greenwald AR, Kephart JO (2000) Shopbots and pricebots. \emph{Agent Mediated
		Electronic Commerce II: Towards Next-Generation Agent-Based Electronic
		Commerce Systems 2}, 1--23 (Springer).
	
	\bibitem[{Hartline et~al.(2015)Hartline, Syrgkanis, \protect\BIBand{}
		Tardos}]{hartline2015NoRegretLearningBayesian}
	Hartline J, Syrgkanis V, Tardos E (2015) No-{{Regret Learning}} in {{Bayesian
			Games}}. Cortes C, Lawrence ND, Lee DD, Sugiyama M, Garnett R, eds.,
	\emph{Advances in {{Neural Information Processing Systems}} 28}, 3061--3069
	({Curran Associates, Inc.}).
	
	\bibitem[{He et~al.(2013)He, Chen, Wang, \protect\BIBand{} Liu}]{he2013online}
	He D, Chen W, Wang L, Liu TY (2013) Online learning for auction mechanism in
	bandit setting. \emph{Decision Support Systems} 56:379--386.
	
	\bibitem[{Heidari et~al.(2016)Heidari, Mahdian, Syed, Vassilvitskii,
		\protect\BIBand{} Yazdanbod}]{heidari2016pricing}
	Heidari H, Mahdian M, Syed U, Vassilvitskii S, Yazdanbod S (2016) Pricing a
	low-regret seller. \emph{International Conference on Machine Learning},
	2559--2567 (PMLR).
	
	\bibitem[{Hettich(2021)}]{Hettich.2021}
	Hettich M (2021) \emph{{Algorithmic Collusion: Insights from Deep Learning}}.
	\urlprefix\url{http://dx.doi.org/10.2139/ssrn.3785966}.
	
	\bibitem[{Jauvion et~al.(2018)Jauvion, Grislain, Dkengne~Sielenou, Garivier,
		\protect\BIBand{} Gerchinovitz}]{jauvion2018optimization}
	Jauvion G, Grislain N, Dkengne~Sielenou P, Garivier A, Gerchinovitz S (2018)
	Optimization of a ssp's header bidding strategy using thompson sampling.
	\emph{Proceedings of the 24th ACM SIGKDD International Conference on
		Knowledge Discovery \& Data Mining}, 425--432.
	
	\bibitem[{Jin et~al.(2018)Jin, Song, Li, Gai, Wang, \protect\BIBand{}
		Zhang}]{jin2018real}
	Jin J, Song C, Li H, Gai K, Wang J, Zhang W (2018) Real-time bidding with
	multi-agent reinforcement learning in display advertising. \emph{Proceedings
		of the 27th ACM international conference on information and knowledge
		management}, 2193--2201.
	
	\bibitem[{Kagel \protect\BIBand{} Roth(2020)}]{kagel2020handbook}
	Kagel JH, Roth AE (2020) \emph{The Handbook of Experimental Economics, Volume
		2} (Princeton university press).
	
	\bibitem[{Klein(2021)}]{Klein.2021}
	Klein T (2021) {Autonomous algorithmic collusion: Q--learning under sequential
		pricing}. \emph{{The RAND Journal of Economics}} 52(3):538--558, ISSN
	1756-2171, \urlprefix\url{http://dx.doi.org/10.1111/1756-2171.12383}.
	
	\bibitem[{Kolumbus \protect\BIBand{} Nisan(2022)}]{kolumbus2022auctions}
	Kolumbus Y, Nisan N (2022) Auctions between regret-minimizing agents.
	\emph{Proceedings of the ACM Web Conference 2022}, 100--111.
	
	\bibitem[{Krishna(2009)}]{krishna2009auction}
	Krishna V (2009) \emph{Auction theory} (Academic press).
	
	\bibitem[{Lattimore \protect\BIBand{}
		Szepesv{\'a}ri(2020)}]{lattimore2020bandit}
	Lattimore T, Szepesv{\'a}ri C (2020) \emph{Bandit algorithms} (Cambridge
	University Press).
	
	\bibitem[{Letcher et~al.(2019)Letcher, Balduzzi, Racani{{\`e}}re, Martens,
		Foerster, Tuyls, \protect\BIBand{}
		Graepel}]{letcher2019DifferentiableGameMechanics}
	Letcher A, Balduzzi D, Racani{{\`e}}re S, Martens J, Foerster J, Tuyls K,
	Graepel T (2019) Differentiable game mechanics. \emph{Journal of Machine
		Learning Research} 20(84):1--40,
	\urlprefix\url{http://jmlr.org/papers/v20/19-008.html}.
	
	\bibitem[{Mertikopoulos \protect\BIBand{}
		Zhou(2019)}]{mertikopoulos2019learning}
	Mertikopoulos P, Zhou Z (2019) Learning in games with continuous action sets
	and unknown payoff functions. \emph{Mathematical Programming}
	173(1-2):465--507.
	
	\bibitem[{Milgrom \protect\BIBand{} Weber(1985)}]{Milgrom1985}
	Milgrom PR, Weber RJ (1985) Distributional strategies for games with incomplete
	information. \emph{Mathematics of Operations Research} 10(4):619--632, ISSN
	0364765X, 15265471.
	
	\bibitem[{Nemirovskij \protect\BIBand{} Yudin(1983)}]{nemirovskij1983problem}
	Nemirovskij AS, Yudin DB (1983) Problem complexity and method efficiency in
	optimization .
	
	\bibitem[{Nesterov(2009)}]{nesterov2009PrimaldualSubgradientMethods}
	Nesterov Y (2009) Primal-dual subgradient methods for convex problems.
	\emph{Mathematical programming} 120(1):221--259.
	
	\bibitem[{OECD(2017)}]{OECD.2017}
	OECD (2017) Algorithms and collusion: Competition policy in the digital age.
	Technical report, OECD,
	\urlprefix\url{www.oecd.org/competition/algorithms-collusion-competition-policy-in-the-digital-age.htm}.
	
	\bibitem[{Rabinovich et~al.(2013)Rabinovich, Naroditskiy, Gerding,
		\protect\BIBand{} Jennings}]{rabinovich2013ComputingPureBayesianNash}
	Rabinovich Z, Naroditskiy V, Gerding EH, Jennings NR (2013) Computing pure
	{{Bayesian}}-{{Nash}} equilibria in games with finite actions and continuous
	types. \emph{Artificial Intelligence} 195:106--139, ISSN 00043702,
	\urlprefix\url{http://dx.doi.org/10.1016/j.artint.2012.09.007}.
	
	\bibitem[{Rhuggenaath et~al.(2019)Rhuggenaath, Akcay, Zhang, \protect\BIBand{}
		Kaymak}]{rhuggenaath2019pso}
	Rhuggenaath J, Akcay A, Zhang Y, Kaymak U (2019) A pso-based algorithm for
	reserve price optimization in online ad auctions. \emph{2019 IEEE congress on
		evolutionary computation (CEC)}, 2611--2619 (IEEE).
	
	\bibitem[{Schaefer \protect\BIBand{}
		Anandkumar(2019)}]{schaefer2019CompetitiveGradientDescent}
	Schaefer F, Anandkumar A (2019) Competitive gradient descent. Wallach H,
	Larochelle H, Beygelzimer A, d\textquotesingle Alch\'{e}-Buc F, Fox E,
	Garnett R, eds., \emph{Advances in Neural Information Processing Systems},
	volume~32 (Curran Associates, Inc.),
	\urlprefix\url{https://proceedings.neurips.cc/paper/2019/file/56c51a39a7c77d8084838cc920585bd0-Paper.pdf}.
	
	\bibitem[{Shalev-Shwartz(2011)}]{shalev-shwartzOnlineLearningOnline2011}
	Shalev-Shwartz S (2011) Online {Learning} and {Online} {Convex} {Optimization}.
	\emph{Foundations and TrendsÂ® in Machine Learning} 4(2):107--194, ISSN
	1935-8237, 1935-8245, \urlprefix\url{http://dx.doi.org/10.1561/2200000018}.
	
	\bibitem[{Skrzypacz \protect\BIBand{} Hopenhayn(2004)}]{skrzypacz2004tacit}
	Skrzypacz A, Hopenhayn H (2004) Tacit collusion in repeated auctions.
	\emph{Journal of Economic Theory} 114(1):153--169.
	
	\bibitem[{Sutton \protect\BIBand{}
		Barto(2018)}]{sutton2018ReinforcementLearningIntroduction}
	Sutton RS, Barto AG (2018) \emph{Reinforcement Learning: An Introduction}.
	Adaptive Computation and Machine Learning Series ({Cambridge, Massachusetts}:
	{The MIT Press}), second edition edition, ISBN 978-0-262-03924-6.
	
	\bibitem[{Szymanski \protect\BIBand{} Lee(2006)}]{szymanski2006impact}
	Szymanski BK, Lee JS (2006) Impact of roi on bidding and revenue in sponsored
	search advertisement auctions. \emph{Second Workshop on Sponsored Search
		Auctions}.
	
	\bibitem[{Thompson(1933)}]{thompson1933}
	Thompson WR (1933) On the likelihood that one unknown probability exceeds
	another in view of the evidence of two samples. \emph{Biometrika}
	25(3/4):285--294, ISSN 00063444,
	\urlprefix\url{http://www.jstor.org/stable/2332286}.
	
	\bibitem[{Tilli \protect\BIBand{} Espinosa-Leal(2021)}]{tilli2021multi}
	Tilli T, Espinosa-Leal L (2021) Multi-armed bandits for bid shading in
	first-price real-time bidding auctions. \emph{Journal of Intelligent \& Fuzzy
		Systems} 41(6):6111--6125.
	
	\bibitem[{Tunuguntla \protect\BIBand{} Hoban(2021)}]{tunuguntla2021near}
	Tunuguntla S, Hoban PR (2021) A near-optimal bidding strategy for real-time
	display advertising auctions. \emph{Journal of Marketing Research}
	58(1):1--21.
	
	\bibitem[{Vickrey(1961)}]{vickrey1961CounterspeculationAuctionsCompetitive}
	Vickrey W (1961) Counterspeculation, auctions, and competitive sealed tenders.
	\emph{Journal of Finance} 16(1):8--37.
	
	\bibitem[{Wilkens et~al.(2017)Wilkens, Cavallo, \protect\BIBand{}
		Niazadeh}]{wilkens2017gsp}
	Wilkens CA, Cavallo R, Niazadeh R (2017) Gsp: the cinderella of mechanism
	design. \emph{Proceedings of the 26th International Conference on World Wide
		Web}, 25--32.
	
	\bibitem[{Yuen(2022)}]{yuen22}
	Yuen M (2022) Programmatic advertising in 2022: Digital display ads industry.
	Technical report,
	\urlprefix\url{https://www.insiderintelligence.com/insights/programmatic-digital-display-ad-spending/}.
	
	\bibitem[{Zhao et~al.(2018)Zhao, Qiu, Guan, Zhao, \protect\BIBand{}
		He}]{zhao2018deep}
	Zhao J, Qiu G, Guan Z, Zhao W, He X (2018) Deep reinforcement learning for
	sponsored search real-time bidding. \emph{Proceedings of the 24th ACM SIGKDD
		international conference on knowledge discovery \& data mining}, 1021--1030.
	
	\bibitem[{Zinkevich(2003)}]{zinkevich2003OnlineConvexProgramming}
	Zinkevich M (2003) Online convex programming and generalized infinitesimal
	gradient ascent. \emph{Proceedings of the {{Twentieth International
				Conference}} on {{International Conference}} on {{Machine Learning}}},
	928--935 ({Washington, DC, USA}: {AAAI Press}).
	
\end{thebibliography}

	
\newpage
\begin{appendix}
\section{Additional Experiments}
\subsection{Complete-Information Model} \label{app:bandit}
Since we only considered Exp3 as a better alternative to Q-learning, we want to run similar experiments for other bandit algorithms as well, to check if the results are robust with respect to different learning algorithms. 
We consider three well-known bandit learners that handle the exploration-exploitation tradeoff differently (see for instance \cite{sutton2018ReinforcementLearningIntroduction, lattimore2020bandit}).
\begin{description}
	\item $\varepsilon$-Greedy. This policy compares the current average reward of each action $a$, which is defined by
	\begin{equation}
		\hat R(a) = \dfrac{\sum_{s=1}^t \chi_a(a_t) r_s(a_t) }{N(a_t)}
	\end{equation}
	with $\chi_a(a_t) = 1 $ if $a_t = a$ and $0$ else. 
	$N_t(a)$ denotes the number an action $a$ has been played up to iteration $t$.
	The $\varepsilon$-Greedy selects the action with the highest average reward with probability $1-\varepsilon_t$ and some random action with probability $\varepsilon_t$ ( $\varepsilon_t = 0.05$ in our experiments).
	
	\item UCB1. Instead of exploring actions at random, UCB1 explores only actions with enough potential or uncertainty (upper-confidence-bound action selection). This is achieved by taking the action with the maximal value which consists of the average reward and an exploration bonus depending on the number an action as been visited. This value is given by
	\begin{equation}
		\hat R(a) + c \sqrt{\dfrac{\ln t}{N_t(a)}},
	\end{equation}
	where $c$ is some exploration parameter ($c=4$ in our experiments).
	
	\item Thompson-Sampling. Finally, we also use a Bayesian method first introduced in \citep{thompson1933}. Thompson sampling assumes a distribution $\Pcal_r(\theta_a)$ (with some parameter $\theta_a$) over the rewards of each action. In each iteration $t$, the learner sample parameters $\theta_a^t \sim \Pcal_\theta$ from some parameter distribution for each action $a$, play the action $a^*$ that maximizes the expected reward given the distribution $\Pcal_r(\theta_a^t)$, and update the parameter distribution $\Pcal_\theta$ for action $a^*$ given the observed reward $r_t(a^*)$.
	In our implementation, we assume that the rewards for each action follow a Gaussian distribution (with known variance $\sigma$) and thereby use a Gaussian prior for the parameter $\theta_a = \mu_a$ (conjugate prior).
\end{description}
Repeating the experiments with two players in a complete-information first-price sealed bid (Section \ref{sec:results_compl_info}) for different bandit algorithms, we observe that most of them converge to the Nash equilibrium (Figure \ref{fig:bandit}).
\begin{figure}[h]
	\begin{center}
		\begin{subfigure}{0.45\textwidth}
			\centering
			\includegraphics[width=\textwidth]{figures/compl_ql_fp_2_bids_sym_bandit.pdf}
			\caption{Bandit vs. Bandit (Symmetric Agents)}
		\end{subfigure}
		\begin{subfigure}{0.45\textwidth}
			\centering
			\includegraphics[width=\textwidth]{figures/compl_ql_fp_2_bids_asym_bandit.pdf}
			\caption{Bandit vs. Exp3 (Asymmetric Agents)}
		\end{subfigure}
	\caption{Median Bids for 2 Agents in the FPSB Auction Using Different Bandit Algorithms}
	\label{fig:bandit}
	\end{center}
	\footnotesize
	We split the learning process into intervals of two thousand iterations and compute the median bid of an agent for each interval. The shaded area shows the mean $ \pm $ std of these median bids over 10 runs, i.e., repetitions of the experiment, for one agent. In the first plot, we show experiments for different bandit algorithms used by both agents (symmetric). We report the median instead of the average bid since the exploration makes the average bid harder to interpret, especially when we converge to high bids.
	In the second plot, we show the results for agents using different learning algorithms (asymmetric), i.e., bandit learning algorithms against an agent using the Exp3.
\end{figure}

The exception here is UCB1, which shows similar behavior to the Q-learner from \citet{banchio2022artificial}. 
But again, when playing against another bandit algorithm, such as Exp3, the bids are significantly higher.

If we compare the revenue under different payment rules for the Q-learner and different bandit algorithms (assuming symmetric agents), we can confirm the observations by \citet{banchio2022artificial} that the revenue in the first-price auction is significantly lower than in the second-price auction for their Q-Learner with optimistic initialization. 
But, our experiments further indicate that this might be specific to this learning algorithm. 
Overall, we observe that the first-price auctions might be harder to learn for some algorithms (i.e., some instances of Q-learning and UCB1). 
And since a failure to learn the Nash equilibrium automatically leads to lower revenue, these methods might seem to collude. But in fact, other versions of Q-learning (i.e., with zero initialization) show opposite results, and a number of reasonable bandit algorithms converge in both settings. 
Note that the slightly lower revenue (see Figure \ref{fig:compl_info_revenue}) in the first-price auction compared to the second-price auction for $\varepsilon$-Greedy, Thompson-Sampling, and Exp3 can be explained by the additional Nash equilibrium at $0.90$ for the first-price auction.

\begin{figure}[h]
	\begin{center}
		\begin{subfigure}{0.45\textwidth}
			\centering
			\includegraphics[width=\textwidth]{figures/revenue_sym_fp_vs_sp_qlearner.pdf}
			\caption{Q-Learner}
		\end{subfigure}
		\begin{subfigure}{0.45\textwidth}
			\centering
			\includegraphics[width=\textwidth]{figures/revenue_sym_fp_vs_sp_bandit.pdf}
			\caption{Bandit Algorithms}
		\end{subfigure}
		\caption{Revenue of the First- and Second-Price Auction with 2 agents}
		\label{fig:compl_info_revenue}
	\end{center}
	\footnotesize
	We compute the average revenue over the last $100\thinspace000$ iterations (excluding the initial learning phase) and report the mean and standard deviation over 10 runs of the experiment.
\end{figure}
\newpage
\subsection{Incomplete-Information Model}
\subsubsection{SODA} \label{app:soda}
We applied SODA to the first- and second-price auctions with different utility models and quickly converge to a pure-strategy BNE. 
As described in Section \ref{sec:soda}, SODA acts on a discretized version we construct by discretizing the valuation and action space using $n=m=64$ equidistant points. Due to better performance, we use a tie-breaking rule in which both agents lose in ties for this discretization. The strategies are updated in each iteration using dual averaging with the entropic regularization term and a decreasing step size $\eta_t = 10 \cdot t^{-0.05}$. We stop the learning algorithm after $5\thinspace000$ iterations, or whenever the relative utility loss $\ell$ of all agents in the discretized game is less than $10^{-4}$. Each experiment is repeated $10$ times. To verify the computed strategies, we compare them to the analytical solutions in the settings where the BNE is known. To this end, we report the following metrics (similar to \cite{bichler2021npga, bichler2023soda}):
\begin{description}
	\item \textit{Relative Utility Loss} $\Lcal$. We approximate the expected utility using the sample-mean of the ex-post utilities, i.e., $ \tilde u_i (\beta_i, \beta^*_{-i}) \approx  \hat u_i (\beta_i, \beta^*_{-i}) := \tfrac{1}{n_v} \sum_{v} u_i( \beta_i(v_i), \beta_{-i}((v_{-i}))) $. The relative loss in the expected utility the agent receives when playing the computed strategy $s_i$ instead of the analytical equilibrium $\beta_i^*$
	\begin{equation} \label{eq:util_loss}
		\mathcal L_i(\beta_i)\ =\ 1 - \frac{\hat u_i(\beta_i, \beta^*_{-i})}{\hat u_i(\beta^*_i,\beta^*_{-i})},
	\end{equation}
	while all other agents play the equilibrium strategy $\beta^*_{-i}$.
	\item \textit{$L_2$ Distance}. The probability-weighted root mean squared error of $\beta_i$ and $\beta^*_i$ in the action space, which approximates the $L_2$ distance of these two functions:
	\begin{equation}
		L_2(\beta_i)\ =\ \bigg(\frac{1}{n_\text{batch}}\sum_{v_i}\left(\beta_i(v_{i}) - \beta^*_i(v_{i})\right)^2\bigg)^\frac{1}{2}.
	\end{equation}
	This way we ensure that our computed strategy not only performs similarly in terms of utility, but also approximates the actual known BNE.
\end{description}
To compute these metrics, we sample valuations (batch size $n_v = 2^{22}$), identify for each valuation the nearest discrete valuation  $v \in \Vcal^d$, and then sample a discrete bid $b \in \Acal^d$ from the computed strategy. The sampled bids are denoted by $\beta_i(v)$ in the definitions above. The same simulation procedure is used to approximate the expected revenue from Figure \ref{fig:soda}.

\input{tables/table_soda}
	
As Table \ref{tab:results_soda} shows, the computed strategies of the discretized game closely approximate the analytical BNE in the original continuous setting. 
We repeat the experiments for a truncated Gaussian prior ($\mu = 0.5, \, \sigma=0.15$) over the interval $[0,1]$. Again we observe convergence in the discretized game with an average relative utility loss $\ell \leq 0.001 $ over 10 runs in all settings. The results are similar to the uniform case. Again, revenue for the first-price auction is lower compared to the second-price auction.
	
\begin{figure}[h]
	\begin{center}
		\begin{subfigure}{0.32\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/revenue_strat_gaus_fp_2.pdf}
		\caption{First-Price}
		\end{subfigure}
		\begin{subfigure}{0.32\textwidth}
			\centering
			\includegraphics[width=\textwidth]{figures/revenue_strat_gaus_sp_2.pdf}
			\caption{Second-Price}
		\end{subfigure}
		\begin{subfigure}{0.32\textwidth}
			\centering
			\includegraphics[width=\textwidth]{figures/revenue_gaus_2.pdf}
			\caption{Expected Revenue}
		\end{subfigure}
	\caption{Results for SODA with 2 Bidders with (truncated) Gaussian Prior}
	\label{fig:soda_gaus}
	\end{center}
	\footnotesize
	The first two plots show the mean and standard deviation of 100 sampled bids from the computed strategies for each discrete valuation
	in settings with $n=2$ bidders, uniform prior, and different utility functions for the first- and second-price auction. The black line shows the analytical BNE (only known for QL and ROI).
	In the third plot we use the computed strategies to approximate the expected revenue by simulating $2^{22}$ auctions for each setting.
\end{figure}
	
\subsubsection{Exp3} \label{app:bandit_bayesian}
The learning progress of Exp3 in the remaining settings not shown in Section \ref{sec:bandit_bayesian}, i.e., first-price auction of ROSB maximizing agents andsecond-price auction for payoff, ROI, and ROSB maximizing agents are shown in the following figures. 
The plots show the frequency of the last $20\thinspace000$ bids w.r.t. the different values after $0.4$, $1$, and $10$ million iterations.   
In the last plot of each figure, we show the distributional equilibrium strategy computed by SODA. The colored lines denote the analytical BNE in the continuous setting. The key observation is that Exp3, which only uses bandit-feedback is able to learn the BNE in the considered settings.
\begin{figure}[h]
	\begin{center}
	\includegraphics[width=1\textwidth]{figures/learning_rosb_fp_3.pdf}
	\caption{Strategies for ROSB-Maximizing Agents using Exp3 in the FPSB Auction with Uniform Prior}
	\end{center} 
	\label{fig:exp3_rosb_fp}
\end{figure}

\begin{figure}[h]
	\begin{center}
		\includegraphics[width=1\textwidth]{figures/learning_ql_sp_3.pdf}
		\caption{Strategies for Payoff-Maximizing Agents using Exp3 in the SPSB Auction with Uniform Prior}
	\end{center} 
	\label{fig:exp3_ql_sp}
\end{figure}
\clearpage
\begin{figure}[t]
	\begin{center}
		\includegraphics[width=1\textwidth]{figures/learning_roi_sp_3.pdf}
		\caption{Strategies for ROI-Maximizing Agents using Exp3 in the SPSB Auction with Uniform Prior}
	\end{center} 
	\label{fig:exp3_roi_sp}
\end{figure}

\begin{figure}[t]
	\begin{center}
		\includegraphics[width=1\textwidth]{figures/learning_rosb_sp_3.pdf}
		\caption{Strategies for ROSB-Maximizing Agents using Exp3 in the SPSB Auction with Uniform Prior}
	\end{center} 
	\label{fig:exp3_rosb_sp}
\end{figure}
\vspace*{\fill}
\end{appendix}
\end{document}
