\documentclass[runningheads, envcountsame, a4paper]{llncs}

\let\proof\relax
\let\endproof\relax
\let\example\relax
\let\endexample\relax

\usepackage[pdftex]{graphicx}
\usepackage{blindtext}
\usepackage{soul}
\usepackage{xcolor}
\usepackage[hyphens]{url}
%\usepackage{hyperref} % forbidden!
\usepackage{amsmath,amsfonts,amsthm,bm} % Math packages
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{enumerate}
\usepackage[linesnumbered,ruled]{algorithm2e}
\usepackage{pifont}% http://ctan.org/pkg/pifont
\usepackage[acronym]{glossaries}
\usepackage{wrapfig}
\usepackage[misc]{ifsym}
\usepackage{filecontents}

%\renewcommand\UrlFont{\color{blue}\rmfamily}
\newcommand{\rpm}{\raisebox{.2ex}{$\scriptstyle\pm$}}
\newcommand{\floor}[1]{\lfloor #1 \rfloor}
\newcommand{\cmark}{\ding{51}}
\newcommand{\xmark}{\ding{55}}
\newcommand{\m}[1]{\ensuremath{\mathrm{#1}}}
\newcommand{\F}{\ensuremath{\mathcal{F}}}
\newcommand\mycommfont[1]{\footnotesize\textcolor{gray}{\textit{#1}}}

\renewcommand{\arraystretch}{0.7}

\SetCommentSty{mycommfont}
\SetKwInput{KwInput}{Require}                % Set the Input
\SetKwInput{KwOutput}{Output}              % set the Output

\graphicspath{ {./figures/} }

% define acronym for method
\newacronym{method}{SREA}{Self-Re-Labeling with Embedding Analysis}
\newacronym{nilm}{NILM}{Non-Intrusive Load Monitoring}

% Put bibliography in the main.tex file
\begin{filecontents}{bibliography.bib}
@article{zhang2016understanding,
  title={Understanding deep learning requires rethinking generalization},
  author={Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
  journal={arXiv:1611.03530},
  year={2016}
}


@Article{limmerEVCharging2019,
AUTHOR = {Limmer, Steffen},
TITLE = {Evaluation of Optimization-Based EV Charging Scheduling with Load Limit in a Realistic Scenario},
JOURNAL = {Energies},
VOLUME = {12},
YEAR = {2019},
NUMBER = {24},
ARTICLE-NUMBER = {4730},
ISSN = {1996-1073},
}

@inproceedings{arpit2017closer,
  title={A closer look at memorization in deep networks},
  author={Arpit, Devansh and Jastrzebski, Stanislaw and Ballas, Nicolas and Krueger, David and Bengio, Emmanuel and Kanwal, Maxinder S and Maharaj, Tegan and Fischer, Asja and Courville, Aaron and Bengio, Yoshua and others},
  booktitle={International Conference on Machine Learning},
  pages={233--242},
  year={2017}
}

@article{zeghidour2020wavesplit,
  title={Wavesplit: End-to-end speech separation by speaker clustering},
  author={Zeghidour, Neil and Grangier, David},
  journal={arXiv:2002.08933},
  year={2020}
}

@inproceedings{sablayrolles2019spreading,
  title={Spreading vectors for similarity search},
  author={Sablayrolles, Alexandre and Douze, Matthijs and Schmid, Cordelia and J{\'e}gou, Herv{\'e}},
  booktitle={ICLR 2019-7th International Conference on Learning Representations},
  pages={1--13},
  year={2019}
}

@inproceedings{tanaka2018joint,
  title={Joint optimization framework for learning with noisy labels},
  author={Tanaka, Daiki and Ikami, Daiki and Yamasaki, Toshihiko and Aizawa, Kiyoharu},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={5552--5560},
  year={2018}
}

@inproceedings{arazo2019unsupervised,
  title={Unsupervised label noise modeling and loss correction},
  author={Arazo, Eric and Ortego, Diego and Albert, Paul and Oâ€™Connor, Noel and McGuinness, Kevin},
  booktitle={International Conference on Machine Learning},
  pages={312--321},
  year={2019}
}

@inproceedings{nguyen2019self,
  title={SELF: Learning to Filter Noisy Labels with Self-Ensembling},
  author={Nguyen, Duc Tam and Mummadi, Chaithanya Kumar and Ngo, Thi Phuong Nhung and Nguyen, Thi Hoai Phuong and Beggel, Laura and Brox, Thomas},
  booktitle={International Conference on Learning Representations},
  year={2019}
}

@inproceedings{reed2015training,
  title={Training Deep Neural Networks on Noisy Labels with Bootstrapping},
  author={Reed, Scott E and Lee, Honglak and Anguelov, Dragomir and Szegedy, Christian and Erhan, Dumitru and Rabinovich, Andrew},
  booktitle={ICLR},
  year={2015}
}

@inproceedings{sugiyama2018co,
  title={Co-teaching: Robust training of deep neural networks with extremely noisy labels},
  author={Sugiyama, Masashi},
  booktitle={NeurIPS},
  year={2018}
}

@inproceedings{atkinson2020identifying,
  title={Identifying label noise in time-series datasets},
  author={Atkinson, Gentry and Metsis, Vangelis},
  booktitle={Adjunct Proceedings of the 2020 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2020 ACM International Symposium on Wearable Computers},
  pages={238--243},
  year={2020}
}

@article{hendrycks2019using,
  title={Using self-supervised learning can improve model robustness and uncertainty},
  author={Hendrycks, Dan and Mazeika, Mantas and Kadavath, Saurav and Song, Dawn},
  journal={arXiv:1906.12340},
  year={2019}
}

@inproceedings{mandal2020novel,
  title={A novel self-supervised re-labeling approach for training with noisy labels},
  author={Mandal, Devraj and Bharadwaj, Shrisha and Biswas, Soma},
  booktitle={Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
  pages={1381--1390},
  year={2020}
}

@article{zhang2017mixup,
  title={mixup: Beyond empirical risk minimization},
  author={Zhang, Hongyi and Cisse, Moustapha and Dauphin, Yann N and Lopez-Paz, David},
  journal={arXiv:1710.09412},
  year={2017}
}

@article{huang2021self,
  title={Self-Adaptive Training: Bridging the Supervised and Self-Supervised Learning},
  author={Huang, Lang and Zhang, Chao and Zhang, Hongyang},
  journal={arXiv:2101.08732},
  year={2021}
}

@inproceedings{han2020sigua,
  title={Sigua: Forgetting may make learning with noisy labels more robust},
  author={Han, Bo and Niu, Gang and Yu, Xingrui and Yao, Quanming and Xu, Miao and Tsang, Ivor and Sugiyama, Masashi},
  booktitle={International Conference on Machine Learning},
  pages={4006--4016},
  year={2020}
}

@inproceedings{chowdhury2019structured,
  title={Structured noise detection: Application on well test pressure derivative data},
  author={Chowdhury, Farhan Asif and Suzuki, Satomi and Mueen, Abdullah},
  booktitle={Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
  pages={2952--2960},
  year={2019}
}

@article{demsarCD06,
  author  = {Janez Dem{\v{s}}ar},
  title   = {Statistical Comparisons of Classifiers over Multiple Data Sets},
  journal = {Journal of Machine Learning Research},
  year    = {2006},
  volume  = {7},
  number  = {1},
  pages   = {1-30}
}

@inproceedings{fonseca2019learning,
  title={Learning sound event classifiers from web audio with noisy labels},
  author={Fonseca, Eduardo and Plakal, Manoj and Ellis, Daniel PW and Font, Frederic and Favory, Xavier and Serra, Xavier},
  booktitle={ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={21--25},
  year={2019}
}
 
@article{frenay2013classification,
  title={Classification in the presence of label noise: a survey},
  author={Fr{\'e}nay, Beno{\'i}t and Verleysen, Michel},
  journal={IEEE transactions on neural networks and learning systems},
  volume={25},
  number={5},
  pages={845--869},
  year={2013},
  publisher={IEEE}
}


@article{wang2020self,
  title={Self-semi-supervised Learning to Learn from NoisyLabeled Data},
  author={Wang, Jiacheng and Ma, Yue and Gao, Shuang},
  journal={arXiv:2011.01429},
  year={2020}
}

@inproceedings{jia2016identifying,
  title={Identifying dynamic changes with noisy labels in spatial-temporal data: A study on large-scale water monitoring application},
  author={Jia, Xiaowei and Chen, Xi and Karpatne, Anuj and Kumar, Vipin},
  booktitle={2016 IEEE International Conference on Big Data (Big Data)},
  pages={1328--1333},
  year={2016}
}

@inproceedings{jawed2020self,
  title={Self-supervised learning for semi-supervised time series classification},
  author={Jawed, Shayan and Grabocka, Josif and Schmidt-Thieme, Lars},
  booktitle={Pacific-Asia Conference on Knowledge Discovery and Data Mining},
  pages={499--511},
  year={2020}
}

@article{laine2016temporal,
  title={Temporal ensembling for semi-supervised learning},
  author={Laine, Samuli and Aila, Timo},
  journal={arXiv:1610.02242},
  year={2016}
}

@inproceedings{wang2017time,
  title={Time series classification from scratch with deep neural networks: A strong baseline},
  author={Wang, Zhiguang and Yan, Weizhong and Oates, Tim},
  booktitle={2017 International joint conference on neural networks (IJCNN)},
  pages={1578--1585},
  year={2017}
}

@article{rolnick2017deep,
  title={Deep learning is robust to massive label noise},
  author={Rolnick, David and Veit, Andreas and Belongie, Serge and Shavit, Nir},
  journal={arXiv:1705.10694},
  year={2017}
}

@article{goldberger2016training,
  title={Training deep neural-networks using a noise adaptation layer},
  author={Goldberger, Jacob and Ben-Reuven, Ehud},
  year={2016}
}

@inproceedings{patrini2017making,
  title={Making deep neural networks robust to label noise: A loss correction approach},
  author={Patrini, Giorgio and Rozza, Alessandro and Krishna Menon, Aditya and Nock, Richard and Qu, Lizhen},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={1944--1952},
  year={2017}
}

@inproceedings{jiang2018mentornet,
  title={Mentornet: Learning data-driven curriculum for very deep neural networks on corrupted labels},
  author={Jiang, Lu and Zhou, Zhengyuan and Leung, Thomas and Li, Li-Jia and Fei-Fei, Li},
  booktitle={International Conference on Machine Learning},
  pages={2304--2313},
  year={2018}
}

@inproceedings{ren2018learning,
  title={Learning to reweight examples for robust deep learning},
  author={Ren, Mengye and Zeng, Wenyuan and Yang, Bin and Urtasun, Raquel},
  booktitle={International Conference on Machine Learning},
  pages={4334--4343},
  year={2018}
}

@article{han2020survey,
  title={A survey of label-noise representation learning: Past, present and future},
  author={Han, Bo and Yao, Quanming and Liu, Tongliang and Niu, Gang and Tsang, Ivor W and Kwok, James T and Sugiyama, Masashi},
  journal={arXiv:2011.04406},
  year={2020}
}

@article{zhang2018generalized,
  title={Generalized cross entropy loss for training deep neural networks with noisy labels},
  author={Zhang, Zhilu and Sabuncu, Mert R},
  journal={arXiv:1805.07836},
  year={2018}
}

@article{li2020dividemix,
  title={Dividemix: Learning with noisy labels as semi-supervised learning},
  author={Li, Junnan and Socher, Richard and Hoi, Steven CH},
  journal={arXiv:2002.07394},
  year={2020}
}

@article{van2015learning,
  title={Learning with symmetric label noise: The importance of being unhinged},
  author={Van Rooyen, Brendan and Menon, Aditya Krishna and Williamson, Robert C},
  journal={arXiv:1505.07634},
  year={2015}
}

@article{faustine2017survey,
  title={A survey on non-intrusive load monitoring methodies and techniques for energy disaggregation problem},
  author={Faustine, Anthony and Mvungi, Nerey Henry and Kaijage, Shubi and Michael, Kisangiri},
  journal={arXiv:1703.00785},
  year={2017}
}

@article{yang2019semisupervised,
  title={Semisupervised multilabel deep learning based nonintrusive load monitoring in smart grids},
  author={Yang, Yandong and Zhong, Jing and Li, Wei and Gulliver, T Aaron and Li, Shufang},
  journal={IEEE Transactions on Industrial Informatics},
  volume={16},
  number={11},
  pages={6892--6902},
  year={2019},
  publisher={IEEE}
}

@inproceedings{zhang2018sequence,
  title={Sequence-to-point learning with neural networks for non-intrusive load monitoring},
  author={Zhang, Chaoyun and Zhong, Mingjun and Wang, Zongzuo and Goddard, Nigel and Sutton, Charles},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={32},
  number={1},
  year={2018}
}

@inproceedings{martins2018application,
  title={Application of a deep learning generative model to load disaggregation for industrial machinery power consumption monitoring},
  author={Martins, Pedro BM and Gomes, Jos{\'e} GRC and Nascimento, Vagner B and de Freitas, Antonio R},
  booktitle={2018 IEEE International Conference on Communications, Control, and Computing Technologies for Smart Grids (SmartGridComm)},
  pages={1--6},
  year={2018}
}

@misc{UCRArchive2018,
    title = {The UCR Time Series Classification Archive},
    author = {Dau, Hoang Anh and Keogh, Eamonn and Kamgar, Kaveh and Yeh, Chin-Chia Michael and Zhu, Yan 
              and Gharghabi, Shaghayegh and Ratanamahatana, Chotirat Ann and Yanping and Hu, Bing 
              and Begum, Nurjahan and Bagnall, Anthony and Mueen, Abdullah and Batista, Gustavo and Hexagon-ML},
    year = {2018}
}

@inproceedings{ioffe2015batch,
  title={Batch normalization: Accelerating deep network training by reducing internal covariate shift},
  author={Ioffe, Sergey and Szegedy, Christian},
  booktitle={International conference on machine learning},
  pages={448--456},
  year={2015}
}

@incollection{NEURIPS2019_9015,
title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
booktitle = {Advances in Neural Information Processing Systems 32},
editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
pages = {8024--8035},
year = {2019},
publisher = {Curran Associates, Inc.}
}

@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv:1412.6980},
  year={2014}
}

@article{song2020learning,
  title={Learning from noisy labels with deep neural networks: A survey},
  author={Song, Hwanjun and Kim, Minseok and Park, Dongmin and Lee, Jae-Gil},
  journal={arXiv:2007.08199},
  year={2020}
}

@article{mcknight2010mann,
  title={Mann-Whitney U Test},
  author={McKnight, Patrick E and Najab, Julius},
  journal={The Corsini encyclopedia of psychology},
  pages={1},
  year={2010},
  publisher={Wiley Online Library}
}

@article{opitz2019macro,
  title={Macro f1 and macro f1},
  author={Opitz, Juri and Burst, Sebastian},
  journal={arXiv:1911.03347},
  year={2019}
}

@article{massidda2020non,
  title={Non-intrusive load disaggregation by convolutional neural network and multilabel classification},
  author={Massidda, Luca and Marrocu, Marino and Manca, Simone},
  journal={Applied Sciences},
  volume={10},
  number={4},
  pages={1454},
  year={2020},
  publisher={Multidisciplinary Digital Publishing Institute}
}

@article{gopinath2020energy,
  title={Energy management using non-intrusive load monitoring techniques-State-of-the-art and future research directions},
  author={Gopinath, R and Kumar, Mukesh and Joshua, C Prakash Chandra and Srinivas, Kota},
  journal={Sustainable Cities and Society},
  pages={102411},
  year={2020},
  publisher={Elsevier}
}

@INPROCEEDINGS{8587415,
  author={P. B. M. {Martins} and J. G. R. C. {Gomes} and V. B. {Nascimento} and A. R. {de Freitas}},
  booktitle={2018 IEEE International Conference on Communications, Control, and Computing Technologies for Smart Grids (SmartGridComm)}, 
  title={Application of a Deep Learning Generative Model to Load Disaggregation for Industrial Machinery Power Consumption Monitoring}, 
  year={2018},
  volume={},
  number={},
  pages={1-6},
  doi={10.1109/SmartGridComm.2018.8587415}}
  
@inproceedings{barsim2015toward,
  title={Toward a semi-supervised non-intrusive load monitoring system for event-based energy disaggregation},
  author={Barsim, Karim Said and Yang, Bin},
  booktitle={2015 IEEE global conference on signal and information processing (GlobalSIP)},
  pages={58--62},
  year={2015}
}

@inproceedings{humala2018universalnilm,
  title={Universalnilm: A semi-supervised energy disaggregation framework using general appliance models},
  author={Humala, Bontor and Nambi, Akshay SN Uttama and Prasad, Venkatesha R},
  booktitle={Proceedings of the Ninth International Conference on Future Energy Systems},
  pages={223--229},
  year={2018}
}

@article{Bernard2018NonIntrusiveLM,
  title={Non-Intrusive Load Monitoring (NILM): Unsupervised Machine Learning and Feature Fusion : Energy Management for Private and Industrial Applications},
  author={Timo Bernard and Martin H. Verbunt and G. V. B{\"o}gel and Thorsten Wellmann},
  journal={2018 International Conference on Smart Grid and Clean Energy Technologies (ICSGCE)},
  year={2018},
  pages={174-180}
}

@article{Holmegaard2016NILMIA,
  title={NILM in an Industrial Setting: A Load Characterization and Algorithm Evaluation},
  author={Emil Holmegaard and Mikkel Baun Kj{\ae}rgaard},
  journal={2016 IEEE SMARTCOMP},
  year={2016},
  pages={1-8}
}

@article{Wang2019ASD,
  title={A Semi-Supervised Deep Transfer Learning Architecture for Energy Disaggregation},
  author={S. Wang and L. Du and Q. Zhou},
  journal={2019 IEEE Power & Energy Society General Meeting (PESGM)},
  year={2019},
  pages={1-5}
}

@article{Chang2018AnES,
  title={An Empirical Study of Ladder Network and Multitask Learning on Energy Disaggregation in Taiwan},
  author={Fang-Yi Chang and Chun Chen and S. Lin},
  journal={2018 Conference on Technologies and Applications of Artificial Intelligence (TAAI)},
  year={2018},
  pages={86-89}
}

@article{Paresh2020MultiLabelAB,
  title={Multi-Label Auto-Encoder based Electrical Load Disaggregation},
  author={Spoorthy Paresh and N. Thokala and A. Majumdar and M. Chandra},
  journal={2020 International Joint Conference on Neural Networks (IJCNN)},
  year={2020},
  pages={1-6}
}

@article{Chen2020ACA,
  title={A convolutional autoencoder-based approach with batch normalization for energy disaggregation},
  author={H. Chen and Yue-Hsien Wang and Chun-Hung Fan},
  journal={The Journal of Supercomputing},
  year={2020},
  volume={77},
  pages={2961-2978}
}

@article{Wu2020RobustLE,
  title={Robust Learning Enabled Intelligence for the Internet-of-Things: A Survey From the Perspectives of Noisy Data and Adversarial Examples},
  author={Yulei Wu},
  journal={IEEE Internet of Things Journal},
  year={2020},
  pages={1-1}
}

@article{Karimi2020DeepLW,
  title={Deep learning with noisy labels: exploring techniques and remedies in medical image analysis},
  author={D. Karimi and Haoran Dou and S. Warfield and A. Gholipour},
  journal={Medical image analysis},
  year={2020},
  volume={65},
  pages={101759}
}

@article{Klemenjak2020TowardsCI,
  title={Towards Comparability in Non-Intrusive Load Monitoring: On Data and Performance Evaluation},
  author={Christoph Klemenjak and S. Makonin and W. Elmenreich},
  journal={2020 IEEE Power & Energy Society Innovative Smart Grid Technologies Conference (ISGT)},
  year={2020},
  pages={1-5}
}

@inproceedings{Hubauer2013AnalysisOD,
  title={Analysis of data quality issues in real-world industrial data},
  author={Thomas Hubauer and S. Lamparter and M. Roshchin and N. Solomakhina and Stuart Watson},
  year={2013}
}

@article{Ding2019CleanitsAD,
  title={Cleanits: A Data Cleaning System for Industrial Time Series},
  author={Xiaoou Ding and Hongzhi Wang and Jiaxuan Su and Zijue Li and Jianzhong Li and H. Gao},
  journal={Proc. VLDB Endow.},
  year={2019},
  volume={12},
  pages={1786-1789}
}

@article{Wang2020TimeSD,
  title={Time Series Data Cleaning: A Survey},
  author={X. Wang and Chen Wang},
  journal={IEEE Access},
  year={2020},
  volume={8},
  pages={1866-1881}
}

@article{Gavrilut2011DealingWC,
  title={Dealing with Class Noise in Large Training Datasets for Malware Detection},
  author={Dragos Gavrilut and Liviu Ciortuz},
  journal={2011 13th International Symposium on Symbolic and Numeric Algorithms for Scientific Computing},
  year={2011},
  pages={401-407}
}

@article{Wang2019ALN,
  title={A Label Noise Robust Stacked Auto-Encoder Algorithm for Inaccurate Supervised Classification Problems},
  author={Ziyang Wang and Xiao-yi Luo and J. Liang},
  journal={Mathematical Problems in Engineering},
  year={2019},
  volume={2019},
  pages={1-19}
}

@inproceedings{Fredriksson2020DataLA,
  title={Data Labeling: An Empirical Investigation into Industrial Challenges and Mitigation Strategies},
  author={Teodor Fredriksson and D. I. Mattos and J. Bosch and H. H. Olsson},
  booktitle={PROFES},
  year={2020}
}

@article{Gan2018AutomaticLF,
  title={Automatic Labeling For Personalized IoT Wearable Monitoring},
  author={O. P. Gan},
  journal={IECON 2018 - 44th Annual Conference of the IEEE Industrial Electronics Society},
  year={2018},
  pages={2861-2866}
}

@article{McInnes2018UMAPUM,
  title={UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction},
  author={L. McInnes and J. Healy},
  journal={arXiv:1802.03426},
  year={2018}
}

@article{berthelot2019mixmatch,
  title={Mixmatch: A holistic approach to semi-supervised learning},
  author={Berthelot, David and Carlini, Nicholas and Goodfellow, Ian and Papernot, Nicolas and Oliver, Avital and Raffel, Colin},
  journal={arXiv:1905.02249},
  year={2019}
}

@inproceedings{chen2019understanding,
  title={Understanding and utilizing deep neural networks trained with noisy labels},
  author={Chen, Pengfei and Liao, Ben Ben and Chen, Guangyong and Zhang, Shengyu},
  booktitle={International Conference on Machine Learning},
  pages={1062--1070},
  year={2019}
}

@article{fawaz2019deep,
  title={Deep learning for time series classification: a review},
  author={Fawaz, Hassan Ismail and Forestier, Germain and Weber, Jonathan and Idoumghar, Lhassane and Muller, Pierre-Alain},
  journal={Data Mining and Knowledge Discovery},
  volume={33},
  number={4},
  pages={917--963},
  year={2019},
  publisher={Springer}
}

@article{demvsar2006statistical,
  title={Statistical comparisons of classifiers over multiple data sets},
  author={Dem{\v{s}}ar, Janez},
  journal={The Journal of Machine Learning Research},
  volume={7},
  pages={1--30},
  year={2006}
}


@article{Karim2018LSTMFC,
  title={LSTM Fully Convolutional Networks for Time Series Classification},
  author={Fazle Karim and Somshubra Majumdar and H. Darabi and Shun Chen},
  journal={IEEE Access},
  year={2018},
  volume={6},
  pages={1662-1669}
}

@article{Wu2020DeepTM,
  title={Deep Transformer Models for Time Series Forecasting: The Influenza Prevalence Case},
  author={N. Wu and Bradley Green and X. Ben and S. O'Banion},
  journal={arXiv:2001.08317},
  year={2020} 
}


@article{castellaniSuppl2021,
  title={Supplementary material for: Estimating the electrical power output of industrial devices with end-to-end time-series classification in the presence of label noise},
  author={Andrea Castellani and Sebastian Schmitt and  Barbara Hammer},
  journal={arXiv:2105.00349},
  year={2021} 
}
\end{filecontents}


\begin{document}

\title{Estimating the Electrical Power Output of Industrial Devices with End-to-End Time-Series Classification in the Presence of Label Noise }

\titlerunning{Estimating Power Output of Industrial Devices in the Presence of Label Noise}

\toctitle{Estimating the Electrical Power Output of Industrial Devices with End-to-End Time-Series Classification in the Presence of Label Noise}
\tocauthor{Andrea~Castellani, Sebastian~Schmitt, Barbara~Hammer}

% \orcidID{}
\newcommand{\orcidauthorA}{\orcidID{0000-0003-0476-5978}} % Andrea
\newcommand{\orcidauthorB}{\orcidID{0000-0001-7130-5483}} % Sebastian
\newcommand{\orcidauthorC}{\orcidID{0000-0002-0935-5591}} % Barbara

\author{Andrea~Castellani (\Letter) \inst{1} \orcidauthorA  \and
Sebastian~Schmitt \inst{2} \orcidauthorB \and
Barbara~Hammer \inst{1} \orcidauthorC} 
%
\authorrunning{A. Castellani and S. Schmitt and B. Hammer}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
\institute{Bielefeld University, \\
\email{\{acastellani,bhammer\}@techfak.uni-bielefeld.de}\\ 
\and
Honda Research Institute Europe, \\
\email{sebastian.schmitt@honda-ri.de}
}
%
\maketitle              % typeset the header of the contribution
%
\begin{abstract}
In complex industrial settings, it is common practice to monitor the operation of machines in order to detect undesired states, adjust maintenance schedules, optimize system performance or collect usage statistics of individual machines. 
In this work, we focus on estimating the power output of a Combined Heat and Power (CHP) machine of a medium-sized company facility by analyzing the total facility power consumption.
We formulate the problem as a time-series classification problem, where the class label represents the CHP power output. 
As the facility is fully instrumented and sensor measurements from the CHP are available, we generate the training labels in an automated fashion from the CHP sensor readings. 
However, sensor failures result in mislabeled training data samples which are hard to detect and remove from the dataset. 
Therefore, we propose a novel multi-task deep learning approach that jointly trains a classifier and an autoencoder with a shared embedding representation.
The proposed approach targets to gradually correct the mislabelled data samples during training in a self-supervised fashion, without any prior assumption on the amount of label noise.
We benchmark our approach on several time-series classification datasets and find it to be comparable and sometimes better than state-of-the-art methods.
On the real-world use-case of predicting the CHP power output, we thoroughly evaluate the architectural design choices and show that the final architecture considerably increases the robustness of the learning process and consistently beats other recent state-of-the-art algorithms in the presence of unstructured as well as structured label noise.

\keywords{Time-series \and Deep Learning \and Label noise \and Self-supervision \and \acrlong{nilm} \and Time-Series Classification}
\end{abstract}
%
%
%
\section{Introduction}
It is common to monitor multiple machines in complex industrial settings for many diverse reasons, such as to detect undesired operational states, adjust maintenance schedules or optimize system performance.
In situations where the installation of many sensors for individual devices is not feasible due to cost or technical reasons, \acrfull{nilm} \cite{Holmegaard2016NILMIA} is able to identify the utilization of individual machines based on the analysis of cumulative electrical load profiles. 
The problem of the generation of labelled training data sets is a cornerstone of data-driven approaches to \acrshort{nilm}.
In this context,  industry  relies mostly on manually annotated data \cite{Fredriksson2020DataLA} and less often the training labels can be automatically generated from the sensors \cite{Gan2018AutomaticLF}, which is often unreliable because of sensor failure and  human misinterpretation.
Data cleaning techniques are often hard to implement \cite{Wang2020TimeSD} which unavoidably leads to the presence of wrongly annotated instances in automatically generated datasets i.e.\ \textit{label noise} \cite{frenay2013classification}. 
Many machine learning methods, and in particular deep neural networks, are able to overfit  training data with noisy labels \cite{zhang2016understanding}, thus it is challenging to apply data-driven approaches successfully in complex industrial settings.

We consider a medium-sized company facility and
 target the problem of estimating the electrical power output of a Combined Heat and Power (CHP) machine by only analyzing the facility electrical power consumption.
The electrical power output of the CHP is sufficient to supply a substantial share of the total electricity demand of the facility.
Therefore, knowing the CHP's electrical power output is very helpful for distributing the electrical energy in the facility, for example when scheduling the charging of electrical vehicles (EVs) or reducing total peak-load \cite{limmerEVCharging2019}.
We propose a data-driven deep learning-based approach to this problem, which is modelled as time-series classification challenge in the presence of label noise, where the class label of each time series represents the estimated CHP power output level.
As the facility is fully instrumented, and sensor measurements from the CHP are available, we generate the training labels in an automated fashion from the CHP sensor readings. 
However, these sensors fail from time to time which resulting wrong labels.
To tackle this problem, we propose a novel multi-task deep learning approach named \acrfull{method}, which targets the detection and re-labeling of wrongly labeled instances in a self-supervised training fashion.


In the following, after a formal introduction to \acrshort{method}, we empirically validate it with several benchmarks data sets for time-series classification and compare against state-of-the-art (SotA) algorithms.
In order to evaluate the performance, we create a training data set from clean sensor readings 
and we corrupt it by introducing three types of artificial noise in a controlled fashion.
After that, we apply the proposed method to a real-world use-case including real sensor failures and show that those are properly detected and corrected by our algorithm.  
Finally, we perform an extensive ablation study to investigate the sensitivity of the \acrshort{method} to its hyper-parameters.





\section{Related work}
Deep learning based techniques constitute a promising approach to solve the NILM problem \cite{massidda2020non,Paresh2020MultiLabelAB}. 
Since the requirement of large annotated data sets is very challenging, the problem is often addressed as a semi-supervised learning task
\cite{humala2018universalnilm,barsim2015toward,yang2019semisupervised}, where only a part of the data is correctly labeled, and the other is left without any label.
Here we take a different stance to the problem of energy disaggregation: we assume that labels for all data are given, but not all labels are  correct, as is often the case in complex sensor networks \cite{frenay2013classification}.

Learning noisy labels is an important topic in machine learning research  \cite{song2020learning,han2020survey}.
Several approaches are based on the fact that deep neural networks tend to first learn clean data statistics during early stages of training  \cite{rolnick2017deep,zhang2016understanding,arpit2017closer}.
Methods can be based on a loss function which is robust to label noise \cite{zhang2018generalized,van2015learning}, or they can introduce an explicit \cite{berthelot2019mixmatch} or implicit \cite{reed2015training} regularization term.
Another adaptation of the loss function is achieved by using different bootstrapping methods based on the predicted class labels \cite{han2020survey,nguyen2019self,wang2020self}.
Some other common approaches are based on labeling samples with smaller loss as clean ones \cite{arazo2019unsupervised,han2020sigua,jiang2018mentornet,li2020dividemix}, and fitting a two-component mixture model on the loss distribution, or using cross-validation techniques \cite{chen2019understanding} in order to separate clean and mislabeled samples.
Several existing methods \cite{sugiyama2018co,mandal2020novel,han2020sigua} require knowledge about the percentage of noisy samples, or estimate this quantity based on  a given. The suitability of these approaches is unclear for real-world applications.
Since our proposed method, \acrshort{method}, targets the correction of mislabeled training samples based on their embedding representation, we do not rely on specific  assumptions on the  amount of label noise in advance.
Our modeling approach is based on the observation that self-supervision 
has proven to provide an effective representation for downstream tasks without requiring labels \cite{hendrycks2019using,huang2021self}, this leading to an improvement of performance in the main supervised task \cite{jawed2020self}.

Most applications of models which deal with label noise come from the domain of image data, where noise can be induced by e.g.\ crowd-working annotations  \cite{Karimi2020DeepLW,han2020survey}. Some approaches consider label noise in other domains such as human activity detection \cite{atkinson2020identifying}, sound event classification \cite{fonseca2019learning}, or malware detection \cite{Gavrilut2011DealingWC}.
An attempt to analyze the effect of noisy data on applications for real-world data sets is made in \cite{Wang2019ALN}, but the authors do not compare to the SotA.
Up to the authors knowledge, we are the first to report a detailed evaluation of time-series classification in the presence of label noise, which is a crucial data characteristics in the domain of \acrshort{nilm}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{CHP electrical power output estimation}\label{sec:CHP}
\begin{figure}[tbh]
    \centering
    \includegraphics[width=0.75\linewidth]{figures/data_plots/CHP_normal.pdf}\\
    \includegraphics[width=0.75\linewidth]{figures/data_plots/CHP_sensor_fail.pdf}
\caption{Sensory data from the CHP and total electrical power. Upper: normal operation. Lower: an example of $P_{CHP}$ sensor malfunction is highlighted in yellow.}
\label{fig:CHP_data}
\end{figure}


The CHP is a complex industrial machinery which burns natural gas in order to produce heat and electrical power.
It is controlled by an algorithm where only some aspects are known, so its behavior is mostly unclear and not well predictable.  
It is known that important control signals are the ambient outside temperature $T_{amb}$, the  internal water temperature $T_{water}$, the generated electrical power output $P_{CHP}$, and the total electricity demand of the facility $P_{tot}$.
Fig.~\ref{fig:CHP_data} shows examples of recorded data from the CHP as well as  $P_{tot}$ of the facility.
 $T_{amb}$ has a known strong influence as the CHP is off in the summer period and more or less continuously on in winter and cold periods.
In the transition seasons (spring and fall), the CHP sometimes turns on (night of 25$^\m{th}$ Sept.), sometimes just heats up its internal water (nights of 20$^\m{st}$, 21$^\m{st}$, and 23$^\m{rd}$ Sept.), or  exhibits a fast switching behavior (e.g. 20$^\m{st}$, and 27$^\m{th}$ Oct.).
Even though the CHP usually turns on for a couple of hours (e.g.\ 25$^\m{th}$ Sept.) at rare instances it just turns on for a very short time (e.g.\ 28$^\m{th}$ Oct.).

Due to the complicated operational pattern, it is already hard to make a detector for the CHP operational state even with full access to the measurement data. 
Additionally, the sensors measuring the CHP output power are prone to failure which can be observed in the yellow highlighted area in the bottom side of Fig.~\ref{fig:CHP_data}. 
During that 10-hour period, the CHP did produce electrical power, even though the sensor reading does not indicate this. 
The total electrical power drawn from the grid, $P_{tot}$, provides a much more stable measurement signal. 
The signature of the CHP is clearly visible in the total power signal and we propose to estimate  $P_{CHP}$ from this signal,  but many variables also affects the total load, e.g.\ PV system, changing workloads, etc.


We focus on estimating the power output but where the estimate power value should only be accurate within a certain range. 
Thus,  the problem is formulated as time-series classification instead of regression, where each class represents a certain range of output values.
The class labels are calculated directly from the $P_{CHP}$ sensor measurement as the mean power output of a fixed-length sliding window.
Due to frequent sensor malfunctioning, as displayed in Fig.~\ref{fig:CHP_data}, the resulting classification problem is subject to label noise.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{\acrfull{method}}\label{sec:method}
\subsubsection{Architecture and loss function} 
\begin{figure}[tb]
    \centering
    \includegraphics[width=0.55\linewidth]{figures/misc/architecture.pdf}\:\:
    \includegraphics[width=0.4\linewidth]{figures/misc/par_dynam2.pdf}
    \caption{\acrshort{method} processing architecture (a) and the dynamics of the parameters $\alpha$ and $w$ during the training epochs (b). }
    \label{fig:CNN}
\end{figure}

In this work, column vectors are denoted in bold (e.g. $\bm{x}$). As described in Sec.~\ref{sec:CHP}, we treat the challenge to model time series data as
a classification problem to predict  averaged characteristics of the process using windowing techniques.
Hence, we deal with a supervised \textit{k}-class classification problem setting with a dataset of $n$ training examples $\mathcal{D}= \{ (\bm{x}_i, \bm{y}_i), i=1,...,n\}$ with $\bm{y}_i \in \{ 0, 1 \} ^k $  being the one-hot encoding label for sample $\bm{x}_i$. Thereby, {\bf label noise} is present, i.e.\ we expect that $\bm{y}_i$ is wrong for a substantial (and possibly skewed) amount of instances in the training set.

The overall processing architecture of the proposed approach is shown in Fig.~\ref{fig:CNN}(a).
The \textit{autoencoder} ($f_{ae}$), represented by the \textit{encoder} ($e$) and the \textit{decoder}, provides a strong surrogate supervisory signal for feature learning \cite{jawed2020self} that is not affected by the label noise. 
Two additional components, a \textit{classifier} network ($f_c$) and a \textit{constrained clustering} module ($f_{cc}$) are introduced, which independently propose class labels as output. 
Each of the three processing pipelines share the same embedding representation, created by the encoder,  and each output module is associated with one separate contribution to the total loss function.


For the autoencoder, a typical reconstruction loss is utilized:
\begin{equation}
    \mathcal{L}_{ae} = \frac{1}{n} \sum_{i = 1}^{n} \left ( \bm{\hat{x}}_i - \bm{x}_i \right ) ^2 ,
\end{equation}
where $\bm{\hat{x}}_i$ is the output of the autoencoder given the input $\bm{x}_i$.

Cross entropy is used as loss function for the classification network output, 
\begin{equation}\label{eq:CE}
    \mathcal{L}_{c} = -\frac{1}{n} \sum_{i=1}^{n}  \bm{y}_{i}^T\cdot \log (\bm{p}^c_{i}),
\end{equation}
where $\bm{p}^c_i$ are the \textit{k}-class softmax probabilities produced by the model for the training sample $i$, i.e. $\bm{p}^c_i = \text{softmax}(f_c(\bm{x}_i))$.


For the constraint clustering loss, we first initialize the cluster center $\bm{C} \in \mathbb{R}^{d \times k}$ in the \textit{d}-dimensional embedding space, with the $k$-means clustering of the training samples.
Then, inspired by the good results achieved in \cite{zeghidour2020wavesplit}, we constrain the embedding space to have small intra-class and large inter-class distances, by iteratively adapting $\bm{C}$. 
The resulting clustering loss is given by:
\begin{equation}
    \mathcal{L}_{cc} = \frac{1}{n}\sum_{i=1}^{n} \Bigg[ {\underbrace{  \left \|\bm  e(\bm{x}_i) - \bm{C}_{\bm{y}_i} \right \| ^ 2 _2}_{\text{intra-class}}} + {\underbrace{\log \sum_{j=1}^{k} \exp \left ( - \left \|\bm e(\bm{x}_i) - \bm{C}_j \right \| _2 \right ) }_{\text{inter-class}}} \Bigg]  + \ell_{reg}.
\end{equation}
The entropy regularization $\ell_{reg} = - \sum_i^k \min_{i \neq j} \log \left \| \bm{C}_i - \bm{C}_j \right \|_2$ is aimed to create well separated embedding for different classes  \cite{sablayrolles2019spreading}.

The final total loss function is given by the sum of those contributions, 
\begin{equation}\label{eq:L}
    \mathcal{L} =   \mathcal{L}_{ae}  + \alpha \left( \mathcal{L}_{c} +  \mathcal{L}_{cc} +  \mathcal{L}_\rho \right),
\end{equation}
where we introduced the dynamic parameter $0\leq \alpha \leq 1$ which changes during the  training  (explained below).
We also add the regularization loss $\mathcal{L}_\rho$ to prevent the assignment of all labels to a single class, 
$   \mathcal{L}_\rho = \sum_{j=1}^k \bm{h}_j\cdot \log \frac{\bm{h}_j}{\bm{p}^\rho_{j}}
$,
where $\bm{h}_j$ denotes the prior probability distribution for class \textit{j}, which assume to be uniformly distributed to $1/k$. The term $\bm{p}^\rho_{j}$ is the mean softmax probability of the model for class \textit{j} across all samples in dataset which we  approximate using mini-batches as done in previous works \cite{arazo2019unsupervised}.


\subsubsection{Re-Labeling strategy} We do not compute the total loss function in Eq.~\eqref{eq:L} with the given (noisy) label $\bm y_i$. 
But, we estimate the true label for each data sample $\bm{y^*}_i$ by taking the weighted average of the given training label $\bm y_i$, and the pseudo-labels proposed by the classifier $\bm y^{c}_i$ and the constraint clustering $\bm y^{cc}_i$. 

In order to increase robustness of the labels proposed by the classifier ($\bm y^{c}_i$), we take for each data sample the exponentially averaged probabilities during the last five training epochs, with the weighting factor $\tau_t \sim e^{\frac{t-5}{2}}$:
\begin{equation}\label{eq:C}
    \bm y^{c}_{i}=  \sum_{\m{last\:5\:epochs\: } t} \tau_t \; [\bm p_i^c]_t.
\end{equation}

The label from constraint clustering ($\bm{y}^{cc}_{i}$) is determined by the distances of the samples to the cluster centers in the embedding space:
\begin{equation}
    \label{eq:CC}
    \bm y^{cc}_{i} = \text{softmin}_j \big(\| \bm e(\bm{x}_i) -\bm C_j \|_2\big)
\end{equation}


Then, the corrected label $\bm{y^*}_i$ (in one hot-encoding) is produced by selecting the class corresponding to the maximum entry:    
 \begin{equation}\label{eq:y*}
     \bm y^*_{i} = \text{argmax} \big[(1-w) \,\bm y_{i}+ w\,(\bm y^{c}_{i} +\bm y^{cc}_{i}) \big]
\end{equation}
where the dynamic weighting factor $0\leq w \leq 1$ is function of the training epoch $t$ and to be discussed below when explaining the training dynamics. 


\subsubsection{Training Dynamics} A key aspect of our proposed approach is to dynamically change the loss function, as well as the label correction mechanism, during the training. 
This is achieved by changing the parameters $\alpha$ (loss function) and $w$ (label correction mechanism) as depicted in Fig.~\ref{fig:CNN}(b). The training dynamics is completely defined by the three hyper-parameters $\lambda_{init}$, $\Delta_{start}$ and $\Delta_{end}$.

Initially, we start with $\alpha=0$ and  $w=0$, and  only train the autoencoder from epoch $t=0$ to epoch $t=\lambda_{init}$. 
At the training epoch $t=\lambda_{init}$, $\alpha$ is ramped up linearly until it reaches $\alpha=1$ in training epoch $t=\lambda_{start}=\lambda_{init}+\Delta_{start}$. 
The purpose of this first \textit{warm-up} period is an unsupervised initialization of the embedding space with slowly turning on the supervision of the given labels.
The dominant structure of the clean labels is learned, as neural networks tend to learn the true labels, rather than overfit to the noisy ones, at early training stages \cite{zhang2016understanding,arpit2017closer}.
Then, we also increase $w$ linearly from zero to one, between epochs $t=\lambda_{start}$ to $t=\lambda_{start}+\Delta_{end}=\lambda_{end}$, thereby turning on the label correction mechanism (\textit{re-labeling}).
After training epoch $t=\lambda_{end}$ until the rest of the training, we keep $\alpha=1$ and $w=1$ which means we are fully self-supervised  where the given training labels do not enter directly anymore (\textit{fine-tuning}). 
We summarize the \acrshort{method} and display the pseudo-code in Algorithm~\ref{alg:method}.

\begin{algorithm}[t]
\DontPrintSemicolon
\footnotesize
  \KwInput{Data $\{ (\bm{x}_i, \bm{y}_i)\}_n$, autoencoder $f_{ae}$, classifier $f_c$, constraint clustering $f_{cc}$,  hyper-parameters: $\lambda_{init}$,   $\Delta_{start}$,  $ \Delta_{end}$.
  }
  Init hyper-parameter ramp-up functions $w_t$ and $\alpha_t$ \tcp*{see Fig.~\ref{fig:CNN}(b)}
  \For{training epoch $t=0$ \KwTo $t_{end}$ }{
  Fetch mini-batch data $\{ (\bm{x}_i, \bm{y}_i)\}_b$ at current epoch $t$\\
  \For{$i=1$ \KwTo $b$}{
  \If{$t==\lambda_{init}$}{
        $f_{cc}\gets$ $k$-means$(\bm{e}(\bm{x}_i))$ \tcp*{Initialize constraint clustering}
        }
    $\bm{\hat{x}}_i = f_{ae}(\bm{x}_i)$ \tcp*{Auto-encoder forward pass}
    $\bm y^c_i \gets$ Eq.\eqref{eq:C} \tcp*{Classifier forward pass}
    $\bm{y}^{cc}_i\gets$ Eq.\eqref{eq:CC}\tcp*{Constraint clustering output}
    Adjust $w\gets w_t$,  $\alpha\gets \alpha_t$  \tcp*{Label correction and loss parameters}
    %Adjust $\alpha\gets \alpha_t$ \tcp*{Loss function parameter}
    $\bm{y^*}_i \gets$ Eq.\eqref{eq:y*} \tcp*{Re-labeling}
    $\mathcal{L}\gets$ Eq.\eqref{eq:L}  \tcp*{Evaluate loss function}
    Update $f_{ae}, f_c, f_{cc}$ by SGD on $\mathcal{L}$
    }
    }
\caption{\acrshort{method}: \acrlong{method}.}\label{alg:method}
\end{algorithm}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Experimental setup}

\subsubsection{Label Noise}\label{sec:noise}
True labels are corrupted by a \textit{label transition matrix} T \cite{song2020learning}, where $T_{ij}$ %:= p(\tilde{y} = j | y = i)$ 
is the probability of the label $i$ being flipped into label $j$. 
For all the experiments, we corrupt the labels with \textit{symmetric} (unstructured) and \textit{asymmetric} (structured) noise with noise ratio $\epsilon \in [0, 1]$.
For symmetric noise, a true label is randomly assigned to other labels with equal probability, i.e.  $T_{ii} = 1 - \epsilon$ and $ T_{ij}= \frac{\epsilon}{k-1}$ ($i\neq j$), with $k$ the number of classes.
For asymmetric noise,  a true label is mislabelled by shifting it by one, i.e.\ $T_{ii} = 1 - \epsilon$ and $T_{(j+1\m{mod} k )j} = \epsilon$ ($i\neq j$).
For the estimation of the CHP power, we also analyze another kind of structured noise which we call \textit{flip} noise, where a true label is only flipped to zero, i.e.\  $ T_{ii} = 1 - \epsilon$ and $T_{i0} = \epsilon$. 
This mimics % particular type or noise reflects the behaviour of
sensor failures, where a broken sensor produces a constant output regardless of the real value.
Note that learning with structured noise is much harder than with unstructured noise \cite{frenay2013classification}.
%The definition of the transition matrices T under the aforementioned types of labels noise is reported in Appendix~\ref{app:T}.


\subsubsection{Network architecture}
Since \acrshort{method} is model agnostic, we use CNNs in the experiments, as  these are currently the SotA deep learning network topology for time-series classification \cite{wang2017time,fawaz2019deep}.
The encoder and decoder have a symmetric structure with 4 convolutional blocks.
Each block is composed by a 1D-conv layer followed by batch normalization \cite{ioffe2015batch}, a ReLU activation and a dropout layer with probability 0.2.
The dimension of the shared embedding space is 32.
For the classifier we use a fully connected network with 128 hidden units and \textit{\#classes} outputs.
We use the Adam optimizer \cite{kingma2014adam} with an initial learning rate of 0.01 for 100 epochs.
Such high value for the initial learning rate helps to avoid overfitting of noisy data in the early stages of training \cite{zhang2016understanding,arpit2017closer}.
We halve the learning rate every $20\%$ of training (20 epochs).
In the experiments, we assume to not have access to any clean data, thus it is not possible to use a validation set, and the models are trained without early stopping.
The \acrshort{method} hyper-parameters $\lambda_{init} = 0$, $\Delta_{start} = 25$ and $\Delta_{end} = 30$ are used if not specified otherwise.
Further implementation details are reported in the supplementary material~\cite{castellaniSuppl2021}, including references to the availability of code and data sets. 



\subsubsection{Comparative methods}\label{sec:methods}
In order to make a fair comparison, we use the same neural network topology throughout all the experiments.
A baseline method, which does not take in accout any label noise correction criteria, is a CNN classifier \cite{wang2017time} trained with
cross-entropy loss function of Eq.~\eqref{eq:CE}, which we refer to as \textit{CE}. 
We compare to \textit{MixUp} \cite{zhang2017mixup}, which is a data augmentation technique that exhibits strong robustness to label noise.
In \textit{MixUp-BMM} \cite{arazo2019unsupervised} a two-component beta-mixture model is fitted to the loss distribution and training with bootstrapping loss is implemented.
\textit{SIGUA} \cite{han2020sigua} implements stochastic gradient ascent on likely mislabeled data, thereby trying to reduce the effect of noisy labels.
Finally, in \textit{Co-teaching} \cite{sugiyama2018co} two networks are simultaneously trained which inform each other about which training examples to keep.
The algorithms \textit{SIGUA} and \textit{Co-teaching} assume that the noise level $\epsilon$ is known. 
In our experiments, we use the true value of $\epsilon$ for those approaches, in order to create an upper-bound of their performance.
All the hyper-parameters of the investigated algorithm are set to their default and recommended values. 





\subsubsection{Implementation details}
For the problem of estimating CHP power output, the raw data consists of 78 days of measurement with a sampling rate of 1 sample/minute.
As preprocessing, we do a re-sampling to 6 samples/hour.
The CHP should have a minimal on-time of one hour, in order to avoid too rapid switching which would damage the machine. 
However, during normal operation, the CHP is controlled in a way that on- and off-time periods are  around 4 to 8 hours. 
Due to these time-scales, we are interested in the power output on a scale of 6 hours, which means we use a sliding window with a size of 6 hours (36 samples) and a stride of 10 minutes (1 sample).
Therefore, the preprocessing of the three input variables  $P_{tot}$, $T_{water}$, and  $T_{amb}$ lead to $\mathbb{R}^{(36 \times 3)}$-dimensional data samples.
For generating the labels, we use 5 power output levels, linearly spaced from 0 to $P_{CHP,max}$, and correspondingly to a five-dimensional one-hot encoded label vector $\bm{y}_i\in \mathbb{R}^5$.
For every dataset investigated, we normalize the dataset to have zero mean and unit standard deviation, and we randomly divide the total available data in train-set and test-set with a ratio 80:20.


\subsubsection{Evaluation measures}
 
To evaluate the performance, we report the \textit{averaged $\mathcal{F}_1$-score} on the test-set, where the well-known $F_1$-scores are calculated for each class separately and then averaged via arithmetic mean, 
$\mathcal{F}_1 = \frac{1}{k} \sum_{j=1}^{k} F_{1,j} $.
This formulation of the $F_1$-score results in a larger penalization when the model do not perform well in the minority class, in cases with class imbalance. 
Other metrics, such as the accuracy, show qualitatively similar results and are therefore not reported in this manuscript.
In order to get performance statistics of the methods,
all the experiments have been repeated 10 times with different random initialization. The non-parametric statistical  \textit{Mann-Whitney  U  Test} \cite{mcknight2010mann}
is used to compare the \acrshort{method} against the SotA algorithms.



\section{Results and discussion}



\subsubsection{Benchmarks Datasets}
We evaluate the proposed \acrshort{method} on publicly available time-series classification datasets from UCR repository \cite{UCRArchive2018}.
We randomly choose 10 datasets with different size, length, number of classes and dimensions in order to try to avoid bias in the data.
A summary of the datasets is given in Table~\ref{tab:UCR_data}. 
%------------------------------------------
\begin{wraptable}[14]{r}{0.5\textwidth}
    \centering
    \scriptsize
    \caption{UCR Single-variate and Multi-variate dataset description.}
    \label{tab:UCR_data}
    \begin{tabular}{l l l l l}
    \toprule
        \textbf{Dataset} & \textbf{Size} & \textbf{Length} & \textbf{\#classes} & \textbf{\#dim}  \\
    \midrule
         ArrowHead$^\dagger$ & 211 & 251 & 3 & 1 \\
         CBF & 930 & 128 & 3 & 1 \\
         Epilepsy & 275 & 206 & 4 & 3 \\
         FaceFour$^\dagger$ & 112 & 350 & 4 & 1 \\
         MelbourneP. & 3633 & 24 & 10 & 1 \\
         NATOPS & 360 & 51 & 6 & 24 \\
         OSULeaf$^\dagger$ & 442 & 427 & 6 & 1 \\
         Plane & 210 & 144 & 7 & 1 \\
         Symbols$^\dagger$ & 1020 & 398 & 6 & 1 \\
         Trace$^\dagger$ &  200 & 275 & 4 & 1\\
    \bottomrule
    \multicolumn{5}{l}{$\dagger$ results reported in the supplementary material~\cite{castellaniSuppl2021}.}
    \end{tabular}
\end{wraptable}
%------------------------------------------

We show a representative selection of all comparisons of the results in Table~\ref{tab:ucr}. 
Without any label noise, CE is expected to provide very good results. We observe that our \acrshort{method} gives similar or better $\F_1$ scores than the CE method on 9 out of 10 datasets without label noise.
Considering all algorithms and datasets, with symmetric noise we achieve statistically significantly better scores in 62, similar scores in 105, and worse scores in 33 experiments out of a total of 200 experiments\footnote{Number of experiments: 10 datasets $\times$ 4 noise levels $\times$ 5 algorithms = 200. Each experiment consists of 10 independent runs. }. 
For the more challenging case of asymmetric noise, the \acrshort{method}-results are 86 times significantly better, 97 times equal, and 17 times worse than SotA algorithms.


\begin{table*}[th!]
    \centering
    \scriptsize
    \caption{$\F_1$ test scores on UCR datasets.
    The best results per noise level are underlined. In parenthesis the results of a Mannâ€“Whitney U test with $\alpha=0.05$ of \acrshort{method} against the other approaches: SREA $\F_1$ is significantly higher ($+$), lower ($-$) or not significant ($\approx$).}
    \label{tab:ucr}
    \setlength{\tabcolsep}{1pt}
    \renewcommand{\arraystretch}{0.5}
    \begin{tabular*}{\linewidth}{l l @{\extracolsep{\fill}} l l l l l l l}
    \toprule
    \textbf{Dataset}&\textbf{Noise}&\textbf{\%}&\textbf{CE}&\textbf{MixUp}&\textbf{M-BMM}&\textbf{SIGUA}&\textbf{Co-teach}&\textbf{\acrshort{method}}\\
    \midrule

    \multirow{8}{0.13\linewidth}{\textit{CBF}}
    & - & 0     & 1.000 $(+)$ & 0.970 $(+)$ & 0.886 $(+)$ & 1.000 $(+)$ & 0.997 $(+)$ & \underline{1.000}\\
    \cmidrule{2-9}
    & \multirow{2}{*}{Symm}& 15  & 0.943 $(+)$ & 0.923 $(+)$ & 0.941 $(+)$ & 0.976 $(+)$ & 0.923 $(+)$ & \underline{1.000} \\
    & & 30  & 0.780 $(+)$ & 0.799 $(+)$ & 0.932 $(+)$ & 0.923 $(+)$ & 0.833 $(+)$ & \underline{0.998} \\
    \cmidrule{2-9}
    & \multirow{2}{*}{Asymm} & 10  & 0.973 $(+)$ & 0.956 $(+)$ & 0.920 $(+)$ & 0.989 $(+)$ & 0.963 $(+)$ & \underline{1.000} \\
    & & 20  & 0.905 $(+)$ & 0.897 $(+)$ & 0.949 $(+)$ & 0.980 $(+)$ & 0.900 $(+)$ & \underline{1.000} \\
    \midrule
    
    \multirow{8}{0.13\linewidth}{\textit{Epilepsy}}
    & - & 0 & \underline{0.974} $(\approx)$ & 0.955 $(+)$ & 0.926 $(+)$ & \underline{0.978} $(\approx)$ & 0.971 $(+)$ & \underline{0.973} \\
    \cmidrule{2-9}
    & \multirow{2}{*}{Symm}& 15 &  \underline{0.890} $(\approx)$ &  \underline{0.913} $(\approx)$ &  \underline{0.899} $(\approx)$ &  \underline{0.884} $(\approx)$ &  \underline{0.861} $(\approx)$ &  \underline{0.861}\\
    & & 30 & 0.784 $(-)$ & 0.823 $(-)$ & 0.805 $(-)$ &  \underline{0.741} $(\approx)$ &  \underline{0.744} $(\approx)$ &  \underline{0.708} \\
    \cmidrule{2-9}
    & \multirow{2}{*}{Asymm} & 10 & \underline{0.919} $(\approx)$ & \underline{0.930} $(\approx)$ & \underline{0.847} $(\approx)$ & \underline{0.905} $(\approx)$ & \underline{0.919} $(\approx)$ & \underline{0.888}\\
    & & 20 & \underline{0.861} $(\approx)$ & 0.894 $(-)$ & 0.891 $(-)$ & \underline{0.826} $(\approx)$ & \underline{0.863} $(\approx)$ & \underline{0.825} \\
    \midrule

    \multirow{8}{0.13\linewidth}{\textit{Melbourne}}
    & - & 0 & \underline{0.923} $(\approx)$ & 0.879 $(+)$ & 0.773 $(+)$ & \underline{0.918} $(\approx)$ & \underline{0.913} $(\approx)$ & \underline{0.911}\\
    \cmidrule{2-9}
    & \multirow{2}{*}{Symm}& 15 & 0.869 $(+)$ & 0.870 $(+)$ & 0.856 $(+)$ & \underline{0.883} $(\approx)$ & \underline{0.886} $(\approx)$ & \underline{0.883}\\
    & & 30 & 0.826 $(+)$ & \underline{0.858} $(\approx)$ & \underline{0.870} $(\approx)$ & \underline{0.855} $(\approx)$ & 0.876 $(-)$ & \underline{0.862}\\
    \cmidrule{2-9}
    & \multirow{2}{*}{Asymm} & 10 & 0.898 $(+)$ & 0.877 $(+)$ & 0.860 $(+)$ & 0.899 $(+)$ & 0.897 $(+)$ & \underline{0.911}\\
    & & 20 & 0.865 $(+)$ & 0.861 $(+)$ & 0.851 $(+)$ & 0.858 $(+)$ & \underline{0.893} $(\approx)$ & \underline{0.903}\\
    \midrule
    
    \multirow{8}{0.13\linewidth}{\textit{NATOPS}}
    & - & 0 & \underline{0.858} $(\approx)$ & \underline{0.801} $(\approx)$ & 0.711 $(+)$ & \underline{0.848} $(\approx)$ & \underline{0.835} $(\approx)$ & \underline{0.866}\\
    \cmidrule{2-9}
    & \multirow{2}{*}{Symm}& 15 & \underline{0.779} $(\approx)$ & 0.718 $(+)$ & 0.702 $(+)$ & \underline{0.754} $(\approx)$ & \underline{0.761} $(\approx)$ & \underline{0.796}\\
    & & 30 & \underline{0.587} $(\approx)$ & 0.580 $(+)$ & 0.602 $(+)$ & 0.593 $(+)$ & 0.673 $(+)$ & \underline{0.670}\\
    \cmidrule{2-9}
    & \multirow{2}{*}{Asymm} & 10 & 0.798 $(+)$ & \underline{0.822} $(\approx)$ & 0.756 $(+)$ & 0.764 $(+)$ & 0.790 $(+)$ & \underline{0.829}\\
    & & 20 & \underline{0.703} $(\approx)$ & \underline{0.763} $(\approx)$ & \underline{0.762} $(\approx)$ & \underline{0.698} $(\approx)$ & \underline{0.733} $(\approx)$ & \underline{0.762}\\
    \midrule
    
    \multirow{8}{0.13\linewidth}{\textit{Plane}}
    & - & 0 & \underline{0.995} $(\approx)$ & 0.962 $(+)$ & 0.577 $(+)$ & 0.981 $(+)$ & \underline{0.990} $(\approx)$ & \underline{0.998}\\
    \cmidrule{2-9}
    & \multirow{2}{*}{Symm}& 15 & 0.930 $(+)$ & 0.953 $(+)$ & 0.873 $(+)$ & \underline{0.971} $(\approx)$ & \underline{0.981} $(\approx)$ & \underline{0.983}\\
    & & 30 & 0.887 $(+)$ & 0.902 $(+)$ & \underline{0.943} $(\approx)$ & 0.862 $(+)$ & \underline{0.941} $(\approx)$ & \underline{0.944}\\
    \cmidrule{2-9}
    & \multirow{2}{*}{Asymm} & 10 & \underline{0.981} $(\approx)$ & \underline{0.986} $(\approx)$ & 0.648 $(+)$ & \underline{0.986} $(\approx)$ & \underline{0.990} $(\approx)$ & \underline{0.976}\\
    & & 20 & \underline{0.952} $(\approx)$ & \underline{0.923} $(\approx)$ & 0.751 $(+)$ & \underline{0.976} $(\approx)$ & \underline{0.990} $(\approx)$ & \underline{0.966}\\
    \bottomrule
    
    \end{tabular*}

\end{table*}



\subsubsection{CHP power estimation}
\begin{table*}[tbh]
    \centering
    \scriptsize
    \setlength{\tabcolsep}{3pt}
    \caption{ $\F_1$ test scores of the CHP power estimation. Same notation as Table~\ref{tab:ucr}.}
    \label{tab:CHP_results}
    \begin{tabular*}{\textwidth}{ l l @{\extracolsep{\fill}}  l l l l l l}
    \toprule
    \textbf{Noise}&\textbf{\%}&\textbf{CE}&\textbf{MixUp}&\textbf{M-BMM}&\textbf{SIGUA}&\textbf{Co-teach}&\textbf{\acrshort{method}}\\
    \midrule
    \textbf{-} & 0 & \underline{0.980} $(\approx)$ & 0.957 $(+)$ & 0.882 $(+)$ & \underline{0.979} $(\approx)$ & \underline{0.974} $(\approx)$ & \underline{0.979} \\
    \midrule
    \multirow{4}{*}{\textbf{\textit{Symmetric}}} & 15 & 0.931 $(+)$ & 0.934 $(+)$ & 0.903 $(+)$ & \underline{0.954} $(\approx)$ & \underline{0.950} $(\approx)$ & \underline{0.960}\\
    & 30 & 0.856 $(+)$ & 0.910 $(+)$ & 0.897 $(+)$ & 0.912 $(+)$ & 0.920 $(+)$ & \underline{0.938}\\
    & 45 & 0.763 $(+)$ & 0.883 $(+)$ & 0.895 $(+)$ & 0.867 $(+)$ & 0.886 $(+)$ & \underline{0.918}\\
    & 60 & 0.661 $(+)$ & \underline{0.761} $(\approx)$ & 0.692 $(+)$ & \underline{0.817} $(\approx)$ & \underline{0.839} $(\approx)$ & \underline{0.800}\\
    \midrule
    \multirow{4}{*}{\textbf{\textit{Asymmetric}}} & 10 & \underline{0.954} $(\approx)$ & 0.945 $(+)$ & 0.893 $(+)$ & \underline{0.959} $(\approx)$ & \underline{0.964} $(\approx)$ & \underline{0.961}\\
    & 20 & 0.924 $(+)$ & 0.925 $(+)$ & 0.899 $(+)$ & \underline{0.935} $(\approx)$ & \underline{0.938} $(\approx)$ & \underline{0.946}\\
    & 30 & \underline{0.895} $(\approx)$ & \underline{0.909} $(\approx)$ & 0.873 $(+)$ & \underline{0.916} $(\approx)$ & \underline{0.923} $(\approx)$ & \underline{0.919}\\
    & 40 & 0.807 $(-)$ & 0.848 $(-)$ & 0.784 $(-)$ & 0.836 $(-)$ & \underline{0.876} $(-)$ & 0.287\\
    \midrule
    \multirow{4}{*}{\textbf{\textit{Flip}}} & 10 & \underline{0.970} $(\approx)$ & 0.950 $(+)$ & 0.860 $(+)$ & 0.965 $(+)$ & \underline{0.973} $(\approx)$ & \underline{0.971}\\
    & 20 & 0.942 $(+)$ & 0.942 $(+)$ & 0.860 $(+)$ & 0.945 $(+)$ & \underline{0.962} $(\approx)$ & \underline{0.963}\\
    & 30 & 0.908 $(+)$ & 0.919 $(+)$ & 0.880 $(+)$ & 0.923 $(+)$ & 0.792 $(+)$ & \underline{0.956}\\
    & 40 & 0.868 $(+)$ & 0.791 $(+)$ & 0.696 $(+)$ & 0.779 $(+)$ & 0.623 $(+)$ & \underline{0.945}\\
    \bottomrule
    \end{tabular*}
\end{table*}

\begin{figure}[tb]
    \centering
        \includegraphics[width=0.35\textwidth]{figures/CHP/flip/cm_flip_03.pdf}\:
       \centering
        \includegraphics[width=0.35\textwidth]{figures/CHP/flip/embedding.pdf}
    \caption{Confusion matrix of the corrected labels (left) and embedding space of the train-set (right) of the CHP power estimation, corrupted with 30\% flip noise.}
    \label{fig:cm_umap}
\end{figure}


Table~\ref{tab:CHP_results} shows the results of the estimation of the CHP output power level. 
Without labels noise, the proposed approach has comparable performance to CE, with an average  $\F_1$-score of 0.979. 
This implies that we successfully solve the  CHP power estimating  problem  by analyzing the total load with an error rate less than 2\%.
When the training labels are corrupted with low level of symmetric label noise, $\epsilon \leq 0.4$, \acrshort{method} consistently outperforms the other algorithms. With higher level of symmetric noise we achieve a comparable performance to the other algorithms.
Under the presence of asymmetric label noise, \acrshort{method} shows  a performance comparable  to  the other SotA algorithms.
Only for a  highly unrealistic asymmetric noise level of 40\%, the performance is significantly worse than the SotA. This indicates that, during the warm-up and relabelling phase, the network is not able to learn the true labels but also learns the wrong labels induced by the structured noise. During the fine-tuning phase, the feedback of the wrongly labeled instances is amplified and the wrong labels a reinforced. 
%Only with a unrealistically high asymmetric noise level of 40\%, the performance of the proposed method is worse then the SotA. 
For flip noise, which reflects sensors failures, \acrshort{method} retains a high performance and outperforms all other SotA algorithms up to noise levels of 40\%.
For low noise levels up to 20\%, \acrshort{method} has similar performance to  Co-teaching, but without the need to know the amount of noise. 

In Fig.~\ref{fig:cm_umap} (left) we show the label confusion matrix (with in-class percentage in parentheses) of \acrshort{method} for the resulting corrected labels for the case of 30\% of flip label noise.  
The corrected labels have 99\% and 98\% accuracy for the fully off- (0) and on-state (4), respectively. The intermediate power values' accuracies are 90\% for state 1, 86\% for state 2 and 92\% for state 3.
As an example, a visualization of the 32 dimensional embedding using the  UMAP \cite{McInnes2018UMAPUM} dimension reduction technique is shown in Fig.~\ref{fig:cm_umap} for the cases of 30\% flip noise. 
The clusters representing the classes are very well separated, and we can see that the majority of the noisy label samples have been corrected and assigned to their correct class. Similar plots for other noise types as well as critical distance plots can be found in the supplementary material~\cite{castellaniSuppl2021}.

Finally, we run \acrshort{method} on a different real-world test-set which includes a  sensor failure, and the corresponding noisy label, as shown in Fig.\ \ref{fig:CHP_data}.
The method was able to correctly re-label the period of the sensor failure.


\subsection{Ablation studies}

\subsubsection{Hyper-parameter sensitivity}
We investigate the effect of the hyper-parameters of \acrshort{method} on both, benchmarks and CHP datasets. 
The observed trends were similar in all datasets, and therefore we only report the result for the CHP dataset.
The effect of the three hyper-parameters related to the training dynamics ($\lambda_{init}$, $\Delta_{start}$, and $\Delta_{end}$) are reported in Fig.~\ref{fig:ablaton_hyper_symm} for the cases of unstructured symmetric label noise (results for the other noise types can be found in the supplementary material~\cite{castellaniSuppl2021}).
For the variation of $\Delta_{start}$  and $\Delta_{end}$ there is a clear pattern for every noise level as the performance increases with the values of the hyper-parameters, and best performance is achieved by  $\Delta_{start}=25$  and $\Delta_{end}=30$.
This shows that both the \textit{warm-up} and \textit{re-labeling} periods should be rather long and last about 55\% of the training time, before fully self-supervised training.
The effect of  $\lambda_{init}$ is not as clear, but it seems that either a random 
initialization of the cluster centers ($\lambda_{init}$ close to zero)
or an extended period of unsupervised training of the autoencoder ($\lambda_{init}$ between 20 and 40) is beneficial.

\begin{figure}[tb]
        \centering
        \includegraphics[width=0.32\textwidth]{figures/CHP_ablaton_hyper/symmetric/delta_start.pdf}
        \includegraphics[width=0.32\textwidth]{figures/CHP_ablaton_hyper/symmetric/delta_end.pdf}
        \includegraphics[width=0.32\textwidth]{figures/CHP_ablaton_hyper/symmetric/lambda.pdf}
      \caption{
      \acrshort{method} sensitivity to hyper-parameters $\Delta_{start}$ (left), $\Delta_{end}$ (middle), $\lambda_{init}$ (right) for the  CHP data and symmetric noise.}
        \label{fig:ablaton_hyper_symm}
\end{figure}



\subsubsection{Loss function components}
We study what effect each of the major loss function components of  Eq.~\ref{eq:L} has on the performance of \acrshort{method} and report the results in Table~\ref{tab:abation}.
It can be observed, that not including the constrained clustering, i.e.\ using only $\mathcal{L}_c$ or $\mathcal{L}_c+\mathcal{L}_{ae}$, gives rather poor performance for all noise types and levels. This is explicitly observed as the performance decreases again during training in the self-supervision phase without $\mathcal{L}_{cc}$  (not shown). 
This seems understandable as the label correction method repeatedly bootstraps itself by using only the labels provided by the classifier, without any anchor to preserve the information from the training labels.  
This emphasizes the necessity of constraining the data in the embedding space during self-supervision.  


\begin{table*}[tb!]
    \scriptsize
    \centering
    \caption{Ablation studies on loss function components of \acrshort{method}.}
    \label{tab:abation}
    \begin{tabular*}{\textwidth}{l @{\extracolsep{\fill}} c c c c c}
        \toprule
        \textbf{Noise} & \textbf{\%} & $\bm{\mathcal{L}_c}$ & $\bm{\mathcal{L}_c + \mathcal{L}_{ae}}$ & $\bm{\mathcal{L}_c + \mathcal{L}_{cc}}$ & $\bm{\mathcal{L}_c + \mathcal{L}_{ae} + \mathcal{L}_{cc}}$\\
        \midrule
        \textbf{\textit{-}} & 0 & 0.472\rpm0.060 & 0.504\rpm0.012 & 0.974\rpm0.003 & \underline{0.980\rpm0.003} \\
        \midrule
        \multirow{3}{*}{\textbf{\textit{Symmetric}}} & 15 & 0.388\rpm0.027 & 0.429\rpm0.023 & 0.943\rpm0.004 & \underline{0.957\rpm0.007} \\
         & 30 & 0.355\rpm0.038 & 0.366\rpm0.041 & 0.919\rpm0.006 & \underline{0.930\rpm0.008} \\
         & 45 & 0.290\rpm0.014 & 0.318\rpm0.010 & 0.892\rpm0.009 & \underline{0.902\rpm0.008}\\
         \midrule
         \multirow{3}{*}{\textbf{\textit{Asymmetric}}} & 10 & 0.400\rpm0.026 & 0.407\rpm0.012 & 0.949\rpm0.003 & \underline{0.957\rpm0.003} \\
         & 20 & 0.348\rpm0.003 & 0.358\rpm0.003 & 0.930\rpm0.009 & \underline{0.941\rpm0.009}\\
         & 30 & 0.342\rpm0.002 & 0.349\rpm0.004 & 0.901\rpm0.007 & \underline{0.922\rpm0.010}\\
         \midrule
         \multirow{3}{*}{\textbf{\textit{Flip}}} & 10 & 0.460\rpm0.030 & 0.468\rpm0.030 & 0.961\rpm0.009 & \underline{0.973\rpm0.006} \\
         & 20 & 0.415\rpm0.028 & 0.424\rpm0.037 & 0.951\rpm0.008 & \underline{0.962\rpm0.005}\\
         & 30 & 0.405\rpm0.030 & 0.411\rpm0.040 & 0.943\rpm0.007 & \underline{0.957\rpm0.003}\\
         \bottomrule
    \end{tabular*}
\end{table*}



\subsubsection{Input variables}
We investigate the influence of the selection of the input variables on the estimation of the CHP power level. The results for all possible combination of the input signals are reported in Fig~\ref{fig:CHP_input}. 
Unsurprisingly, using only the ambient temperature gives by far the worst results, while utilizing all available inputs results in the highest scores. Without the inclusion of the $P_{tot}$, we  still achieve a $\F_1$-score above 0.96 with only using the $T_{water}$ as input signal.


\begin{figure}[tb!]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/CHP_input/asymm/boxplot.pdf}
    \caption{\acrshort{method} sensitivity to the  input variables for CHP data and asymmetric noise.}
    \label{fig:CHP_input}
\end{figure}



\section{Conclusion and future work}
In this work, we presented the problem of estimating the electrical power output of a Combined Heat and Power (CHP) machine by analyzing the total electrical power consumption of a medium size company facility. We presented an approach to estimate the CHP power output by analyzing the total load, the ambient temperature and the water temperature of the CHP, all of which are known to be control variables of the CHP. 
The training dataset for the deep-learning based approach was automatically derived from sensor measurements of the CHP power output, and sensor failures create noisy samples in the generated class labels. 
The proposed \acrfull{method} incorporates an autoencoder,  a classifier and a constraint clustering which all share and operate on a common low-dimensional embedding representation. 
During the network training, the loss function and the label correction mechanism are adjusted in a way that a robust relabeling of noisy training labels is possible.  
We compare  \acrshort{method} to five SotA label noise correction approaches on ten  time-series classification benchmarks and observe mostly comparable or better performance for various noise levels and types. 
We also observe superior performance on the CHP use-case for a wide range of noise levels and all studied noise types.
We thoroughly analyzed the dependence of the proposed methods on the (hyper-)parameters and architecture choices. 

The proposed approach is straight-forward to realize without any (hyper-)parameter tuning, as there are clear insights on how to set the parameters and the method is not sensitive to details. It also has the strong benefit that the amount of label noise need not be known or guessed. 
We used CNNs as building blocks of the proposed algorithm, but since \acrshort{method} is model agnostic, it is possible to utilize other structures, such as recurrent neural networks~\cite{Karim2018LSTMFC} or transformers~\cite{Wu2020DeepTM}, which would also utilize the time-structure of the problem. The application of such dynamic models is left for future work.

Estimating the CHP output as shown in this work will be used in the future in energy optimization scenarios to arrive at more reliable and robust EV charging schedules. 
But, due to the robustness of the proposed method and the ability to exchange the neural networks with arbitrary other machine learning modules, we see a high potential for this architecture to be used for label noise correction in other application domains.
We also see a high potential for an application in anomaly detection scenarios where sensor failures need to be detected. 
A thorough evaluation in these application areas is left for future work.  


\bibliographystyle{splncs04}
\bibliography{bibliography.bib}

\end{document}
