\documentclass{llncs}

\let\proof\relax
\let\endproof\relax
\let\example\relax
\let\endexample\relax

\usepackage[pdftex]{graphicx}
\usepackage{subcaption}
\captionsetup{compatibility=false}

\usepackage{float}
\usepackage{blindtext}
\usepackage{soul}
\usepackage{xcolor}
\usepackage[hyphens]{url}
%\usepackage{hyperref} % forbidden!
\usepackage{amsmath,amsfonts,amsthm,bm} % Math packages
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{enumerate}
\usepackage[linesnumbered,ruled]{algorithm2e}
\usepackage{pifont}% http://ctan.org/pkg/pifont
\usepackage[acronym]{glossaries}
\usepackage{wrapfig}
\usepackage[misc]{ifsym}
\usepackage{filecontents}
\usepackage{textcomp}


\usepackage[acronym]{glossaries}


% Hyperrref setup
%\hypersetup{hidelinks}
%\renewcommand\UrlFont{\color{blue}\rmfamily}

\newcommand{\comment}[1]{ }
\newcommand{\seb}[1]{{\color{blue}\textbf{Seb}: #1}}
\newcommand{\andrea}[1]{{\color{red}\textbf{Andrea}: #1}}
\newcommand{\new}[1]{\textcolor{black}{#1}}

\newcommand{\rpm}{\raisebox{.2ex}{$\scriptstyle\pm$}}
\newcommand{\floor}[1]{\lfloor #1 \rfloor}
\newcommand{\cmark}{\ding{51}}
\newcommand{\xmark}{\ding{55}}
\newcommand{\m}[1]{\ensuremath{\mathrm{#1}}}
\newcommand{\F}{\ensuremath{\mathcal{F}}}
\newcommand\mycommfont[1]{\footnotesize\textcolor{gray}{\textit{#1}}}


\renewcommand\UrlFont{\color{blue}\rmfamily}


\newcommand{\beginsupplement}{%
        \setcounter{table}{0}
        \renewcommand{\thetable}{S\arabic{table}}%
        \setcounter{figure}{0}
        \renewcommand{\thefigure}{S\arabic{figure}}%
     }

%\hypersetup{hidelinks}


\SetCommentSty{mycommfont}

\SetKwInput{KwInput}{Require}                % Set the Input
\SetKwInput{KwOutput}{Output}              % set the Output

\graphicspath{ {./figures/} }

% define acronym for method
\newacronym{method}{SREA}{Self-Re-Labeling with Embedding Analysis}
\newacronym{nilm}{NILM}{Non-Intrusive Load Monitoring}

\begin{filecontents}{bibliosupplementary.bib}
	@article{zhang2016understanding,
		title={Understanding deep learning requires rethinking generalization},
		author={Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
		journal={arXiv:1611.03530},
		year={2016}
	}
	
	
	@Article{limmerEVCharging2019,
		AUTHOR = {Limmer, Steffen},
		TITLE = {Evaluation of Optimization-Based EV Charging Scheduling with Load Limit in a Realistic Scenario},
		JOURNAL = {Energies},
		VOLUME = {12},
		YEAR = {2019},
		NUMBER = {24},
		ARTICLE-NUMBER = {4730},
		ISSN = {1996-1073},
	}
	
	@inproceedings{arpit2017closer,
		title={A closer look at memorization in deep networks},
		author={Arpit, Devansh and Jastrzebski, Stanislaw and Ballas, Nicolas and Krueger, David and Bengio, Emmanuel and Kanwal, Maxinder S and Maharaj, Tegan and Fischer, Asja and Courville, Aaron and Bengio, Yoshua and others},
		booktitle={International Conference on Machine Learning},
		pages={233--242},
		year={2017}
	}
	
	@article{zeghidour2020wavesplit,
		title={Wavesplit: End-to-end speech separation by speaker clustering},
		author={Zeghidour, Neil and Grangier, David},
		journal={arXiv:2002.08933},
		year={2020}
	}
	
	@inproceedings{sablayrolles2019spreading,
		title={Spreading vectors for similarity search},
		author={Sablayrolles, Alexandre and Douze, Matthijs and Schmid, Cordelia and J{\'e}gou, Herv{\'e}},
		booktitle={ICLR 2019-7th International Conference on Learning Representations},
		pages={1--13},
		year={2019}
	}
	
	@inproceedings{tanaka2018joint,
		title={Joint optimization framework for learning with noisy labels},
		author={Tanaka, Daiki and Ikami, Daiki and Yamasaki, Toshihiko and Aizawa, Kiyoharu},
		booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
		pages={5552--5560},
		year={2018}
	}
	
	@inproceedings{arazo2019unsupervised,
		title={Unsupervised label noise modeling and loss correction},
		author={Arazo, Eric and Ortego, Diego and Albert, Paul and Oâ€™Connor, Noel and McGuinness, Kevin},
		booktitle={International Conference on Machine Learning},
		pages={312--321},
		year={2019}
	}
	
	@inproceedings{nguyen2019self,
		title={SELF: Learning to Filter Noisy Labels with Self-Ensembling},
		author={Nguyen, Duc Tam and Mummadi, Chaithanya Kumar and Ngo, Thi Phuong Nhung and Nguyen, Thi Hoai Phuong and Beggel, Laura and Brox, Thomas},
		booktitle={International Conference on Learning Representations},
		year={2019}
	}
	
	@inproceedings{reed2015training,
		title={Training Deep Neural Networks on Noisy Labels with Bootstrapping},
		author={Reed, Scott E and Lee, Honglak and Anguelov, Dragomir and Szegedy, Christian and Erhan, Dumitru and Rabinovich, Andrew},
		booktitle={ICLR},
		year={2015}
	}
	
	@inproceedings{sugiyama2018co,
		title={Co-teaching: Robust training of deep neural networks with extremely noisy labels},
		author={Sugiyama, Masashi},
		booktitle={NeurIPS},
		year={2018}
	}
	
	@inproceedings{atkinson2020identifying,
		title={Identifying label noise in time-series datasets},
		author={Atkinson, Gentry and Metsis, Vangelis},
		booktitle={Adjunct Proceedings of the 2020 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2020 ACM International Symposium on Wearable Computers},
		pages={238--243},
		year={2020}
	}
	
	@article{hendrycks2019using,
		title={Using self-supervised learning can improve model robustness and uncertainty},
		author={Hendrycks, Dan and Mazeika, Mantas and Kadavath, Saurav and Song, Dawn},
		journal={arXiv:1906.12340},
		year={2019}
	}
	
	@inproceedings{mandal2020novel,
		title={A novel self-supervised re-labeling approach for training with noisy labels},
		author={Mandal, Devraj and Bharadwaj, Shrisha and Biswas, Soma},
		booktitle={Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
		pages={1381--1390},
		year={2020}
	}
	
	@article{zhang2017mixup,
		title={mixup: Beyond empirical risk minimization},
		author={Zhang, Hongyi and Cisse, Moustapha and Dauphin, Yann N and Lopez-Paz, David},
		journal={arXiv:1710.09412},
		year={2017}
	}
	
	@article{huang2021self,
		title={Self-Adaptive Training: Bridging the Supervised and Self-Supervised Learning},
		author={Huang, Lang and Zhang, Chao and Zhang, Hongyang},
		journal={arXiv:2101.08732},
		year={2021}
	}
	
	@inproceedings{han2020sigua,
		title={Sigua: Forgetting may make learning with noisy labels more robust},
		author={Han, Bo and Niu, Gang and Yu, Xingrui and Yao, Quanming and Xu, Miao and Tsang, Ivor and Sugiyama, Masashi},
		booktitle={International Conference on Machine Learning},
		pages={4006--4016},
		year={2020}
	}
	
	@inproceedings{chowdhury2019structured,
		title={Structured noise detection: Application on well test pressure derivative data},
		author={Chowdhury, Farhan Asif and Suzuki, Satomi and Mueen, Abdullah},
		booktitle={Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
		pages={2952--2960},
		year={2019}
	}
	
	@article{demsarCD06,
		author  = {Janez Dem{\v{s}}ar},
		title   = {Statistical Comparisons of Classifiers over Multiple Data Sets},
		journal = {Journal of Machine Learning Research},
		year    = {2006},
		volume  = {7},
		number  = {1},
		pages   = {1-30}
	}
	
	@inproceedings{fonseca2019learning,
		title={Learning sound event classifiers from web audio with noisy labels},
		author={Fonseca, Eduardo and Plakal, Manoj and Ellis, Daniel PW and Font, Frederic and Favory, Xavier and Serra, Xavier},
		booktitle={ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
		pages={21--25},
		year={2019}
	}
	
	@article{frenay2013classification,
		title={Classification in the presence of label noise: a survey},
		author={Fr{\'e}nay, Beno{\'i}t and Verleysen, Michel},
		journal={IEEE transactions on neural networks and learning systems},
		volume={25},
		number={5},
		pages={845--869},
		year={2013},
		publisher={IEEE}
	}
	
	
	@article{wang2020self,
		title={Self-semi-supervised Learning to Learn from NoisyLabeled Data},
		author={Wang, Jiacheng and Ma, Yue and Gao, Shuang},
		journal={arXiv:2011.01429},
		year={2020}
	}
	
	@inproceedings{jia2016identifying,
		title={Identifying dynamic changes with noisy labels in spatial-temporal data: A study on large-scale water monitoring application},
		author={Jia, Xiaowei and Chen, Xi and Karpatne, Anuj and Kumar, Vipin},
		booktitle={2016 IEEE International Conference on Big Data (Big Data)},
		pages={1328--1333},
		year={2016}
	}
	
	@inproceedings{jawed2020self,
		title={Self-supervised learning for semi-supervised time series classification},
		author={Jawed, Shayan and Grabocka, Josif and Schmidt-Thieme, Lars},
		booktitle={Pacific-Asia Conference on Knowledge Discovery and Data Mining},
		pages={499--511},
		year={2020}
	}
	
	@article{laine2016temporal,
		title={Temporal ensembling for semi-supervised learning},
		author={Laine, Samuli and Aila, Timo},
		journal={arXiv:1610.02242},
		year={2016}
	}
	
	@inproceedings{wang2017time,
		title={Time series classification from scratch with deep neural networks: A strong baseline},
		author={Wang, Zhiguang and Yan, Weizhong and Oates, Tim},
		booktitle={2017 International joint conference on neural networks (IJCNN)},
		pages={1578--1585},
		year={2017}
	}
	
	@article{rolnick2017deep,
		title={Deep learning is robust to massive label noise},
		author={Rolnick, David and Veit, Andreas and Belongie, Serge and Shavit, Nir},
		journal={arXiv:1705.10694},
		year={2017}
	}
	
	@article{goldberger2016training,
		title={Training deep neural-networks using a noise adaptation layer},
		author={Goldberger, Jacob and Ben-Reuven, Ehud},
		year={2016}
	}
	
	@inproceedings{patrini2017making,
		title={Making deep neural networks robust to label noise: A loss correction approach},
		author={Patrini, Giorgio and Rozza, Alessandro and Krishna Menon, Aditya and Nock, Richard and Qu, Lizhen},
		booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
		pages={1944--1952},
		year={2017}
	}
	
	@inproceedings{jiang2018mentornet,
		title={Mentornet: Learning data-driven curriculum for very deep neural networks on corrupted labels},
		author={Jiang, Lu and Zhou, Zhengyuan and Leung, Thomas and Li, Li-Jia and Fei-Fei, Li},
		booktitle={International Conference on Machine Learning},
		pages={2304--2313},
		year={2018}
	}
	
	@inproceedings{ren2018learning,
		title={Learning to reweight examples for robust deep learning},
		author={Ren, Mengye and Zeng, Wenyuan and Yang, Bin and Urtasun, Raquel},
		booktitle={International Conference on Machine Learning},
		pages={4334--4343},
		year={2018}
	}
	
	@article{han2020survey,
		title={A survey of label-noise representation learning: Past, present and future},
		author={Han, Bo and Yao, Quanming and Liu, Tongliang and Niu, Gang and Tsang, Ivor W and Kwok, James T and Sugiyama, Masashi},
		journal={arXiv:2011.04406},
		year={2020}
	}
	
	@article{zhang2018generalized,
		title={Generalized cross entropy loss for training deep neural networks with noisy labels},
		author={Zhang, Zhilu and Sabuncu, Mert R},
		journal={arXiv:1805.07836},
		year={2018}
	}
	
	@article{li2020dividemix,
		title={Dividemix: Learning with noisy labels as semi-supervised learning},
		author={Li, Junnan and Socher, Richard and Hoi, Steven CH},
		journal={arXiv:2002.07394},
		year={2020}
	}
	
	@article{van2015learning,
		title={Learning with symmetric label noise: The importance of being unhinged},
		author={Van Rooyen, Brendan and Menon, Aditya Krishna and Williamson, Robert C},
		journal={arXiv:1505.07634},
		year={2015}
	}
	
	@article{faustine2017survey,
		title={A survey on non-intrusive load monitoring methodies and techniques for energy disaggregation problem},
		author={Faustine, Anthony and Mvungi, Nerey Henry and Kaijage, Shubi and Michael, Kisangiri},
		journal={arXiv:1703.00785},
		year={2017}
	}
	
	@article{yang2019semisupervised,
		title={Semisupervised multilabel deep learning based nonintrusive load monitoring in smart grids},
		author={Yang, Yandong and Zhong, Jing and Li, Wei and Gulliver, T Aaron and Li, Shufang},
		journal={IEEE Transactions on Industrial Informatics},
		volume={16},
		number={11},
		pages={6892--6902},
		year={2019},
		publisher={IEEE}
	}
	
	@inproceedings{zhang2018sequence,
		title={Sequence-to-point learning with neural networks for non-intrusive load monitoring},
		author={Zhang, Chaoyun and Zhong, Mingjun and Wang, Zongzuo and Goddard, Nigel and Sutton, Charles},
		booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
		volume={32},
		number={1},
		year={2018}
	}
	
	@inproceedings{martins2018application,
		title={Application of a deep learning generative model to load disaggregation for industrial machinery power consumption monitoring},
		author={Martins, Pedro BM and Gomes, Jos{\'e} GRC and Nascimento, Vagner B and de Freitas, Antonio R},
		booktitle={2018 IEEE International Conference on Communications, Control, and Computing Technologies for Smart Grids (SmartGridComm)},
		pages={1--6},
		year={2018}
	}
	
	@misc{UCRArchive2018,
		title = {The UCR Time Series Classification Archive},
		author = {Dau, Hoang Anh and Keogh, Eamonn and Kamgar, Kaveh and Yeh, Chin-Chia Michael and Zhu, Yan 
			and Gharghabi, Shaghayegh and Ratanamahatana, Chotirat Ann and Yanping and Hu, Bing 
			and Begum, Nurjahan and Bagnall, Anthony and Mueen, Abdullah and Batista, Gustavo and Hexagon-ML},
		year = {2018}
	}
	
	@inproceedings{ioffe2015batch,
		title={Batch normalization: Accelerating deep network training by reducing internal covariate shift},
		author={Ioffe, Sergey and Szegedy, Christian},
		booktitle={International conference on machine learning},
		pages={448--456},
		year={2015}
	}
	
	@incollection{NEURIPS2019_9015,
		title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
		author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
		booktitle = {Advances in Neural Information Processing Systems 32},
		editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
		pages = {8024--8035},
		year = {2019},
		publisher = {Curran Associates, Inc.}
	}
	
	@article{kingma2014adam,
		title={Adam: A method for stochastic optimization},
		author={Kingma, Diederik P and Ba, Jimmy},
		journal={arXiv:1412.6980},
		year={2014}
	}
	
	@article{song2020learning,
		title={Learning from noisy labels with deep neural networks: A survey},
		author={Song, Hwanjun and Kim, Minseok and Park, Dongmin and Lee, Jae-Gil},
		journal={arXiv:2007.08199},
		year={2020}
	}
	
	@article{mcknight2010mann,
		title={Mann-Whitney U Test},
		author={McKnight, Patrick E and Najab, Julius},
		journal={The Corsini encyclopedia of psychology},
		pages={1},
		year={2010},
		publisher={Wiley Online Library}
	}
	
	@article{opitz2019macro,
		title={Macro f1 and macro f1},
		author={Opitz, Juri and Burst, Sebastian},
		journal={arXiv:1911.03347},
		year={2019}
	}
	
	@article{massidda2020non,
		title={Non-intrusive load disaggregation by convolutional neural network and multilabel classification},
		author={Massidda, Luca and Marrocu, Marino and Manca, Simone},
		journal={Applied Sciences},
		volume={10},
		number={4},
		pages={1454},
		year={2020},
		publisher={Multidisciplinary Digital Publishing Institute}
	}
	
	@article{gopinath2020energy,
		title={Energy management using non-intrusive load monitoring techniques-State-of-the-art and future research directions},
		author={Gopinath, R and Kumar, Mukesh and Joshua, C Prakash Chandra and Srinivas, Kota},
		journal={Sustainable Cities and Society},
		pages={102411},
		year={2020},
		publisher={Elsevier}
	}
	
	@INPROCEEDINGS{8587415,
		author={P. B. M. {Martins} and J. G. R. C. {Gomes} and V. B. {Nascimento} and A. R. {de Freitas}},
		booktitle={2018 IEEE International Conference on Communications, Control, and Computing Technologies for Smart Grids (SmartGridComm)}, 
		title={Application of a Deep Learning Generative Model to Load Disaggregation for Industrial Machinery Power Consumption Monitoring}, 
		year={2018},
		volume={},
		number={},
		pages={1-6},
		doi={10.1109/SmartGridComm.2018.8587415}}
	
	@inproceedings{barsim2015toward,
		title={Toward a semi-supervised non-intrusive load monitoring system for event-based energy disaggregation},
		author={Barsim, Karim Said and Yang, Bin},
		booktitle={2015 IEEE global conference on signal and information processing (GlobalSIP)},
		pages={58--62},
		year={2015}
	}
	
	@inproceedings{humala2018universalnilm,
		title={Universalnilm: A semi-supervised energy disaggregation framework using general appliance models},
		author={Humala, Bontor and Nambi, Akshay SN Uttama and Prasad, Venkatesha R},
		booktitle={Proceedings of the Ninth International Conference on Future Energy Systems},
		pages={223--229},
		year={2018}
	}
	
	@article{Bernard2018NonIntrusiveLM,
		title={Non-Intrusive Load Monitoring (NILM): Unsupervised Machine Learning and Feature Fusion : Energy Management for Private and Industrial Applications},
		author={Timo Bernard and Martin H. Verbunt and G. V. B{\"o}gel and Thorsten Wellmann},
		journal={2018 International Conference on Smart Grid and Clean Energy Technologies (ICSGCE)},
		year={2018},
		pages={174-180}
	}
	
	@article{Holmegaard2016NILMIA,
		title={NILM in an Industrial Setting: A Load Characterization and Algorithm Evaluation},
		author={Emil Holmegaard and Mikkel Baun Kj{\ae}rgaard},
		journal={2016 IEEE SMARTCOMP},
		year={2016},
		pages={1-8}
	}
	
	@article{Wang2019ASD,
		title={A Semi-Supervised Deep Transfer Learning Architecture for Energy Disaggregation},
		author={S. Wang and L. Du and Q. Zhou},
		journal={2019 IEEE Power & Energy Society General Meeting (PESGM)},
		year={2019},
		pages={1-5}
	}
	
	@article{Chang2018AnES,
		title={An Empirical Study of Ladder Network and Multitask Learning on Energy Disaggregation in Taiwan},
		author={Fang-Yi Chang and Chun Chen and S. Lin},
		journal={2018 Conference on Technologies and Applications of Artificial Intelligence (TAAI)},
		year={2018},
		pages={86-89}
	}
	
	@article{Paresh2020MultiLabelAB,
		title={Multi-Label Auto-Encoder based Electrical Load Disaggregation},
		author={Spoorthy Paresh and N. Thokala and A. Majumdar and M. Chandra},
		journal={2020 International Joint Conference on Neural Networks (IJCNN)},
		year={2020},
		pages={1-6}
	}
	
	@article{Chen2020ACA,
		title={A convolutional autoencoder-based approach with batch normalization for energy disaggregation},
		author={H. Chen and Yue-Hsien Wang and Chun-Hung Fan},
		journal={The Journal of Supercomputing},
		year={2020},
		volume={77},
		pages={2961-2978}
	}
	
	@article{Wu2020RobustLE,
		title={Robust Learning Enabled Intelligence for the Internet-of-Things: A Survey From the Perspectives of Noisy Data and Adversarial Examples},
		author={Yulei Wu},
		journal={IEEE Internet of Things Journal},
		year={2020},
		pages={1-1}
	}
	
	@article{Karimi2020DeepLW,
		title={Deep learning with noisy labels: exploring techniques and remedies in medical image analysis},
		author={D. Karimi and Haoran Dou and S. Warfield and A. Gholipour},
		journal={Medical image analysis},
		year={2020},
		volume={65},
		pages={101759}
	}
	
	@article{Klemenjak2020TowardsCI,
		title={Towards Comparability in Non-Intrusive Load Monitoring: On Data and Performance Evaluation},
		author={Christoph Klemenjak and S. Makonin and W. Elmenreich},
		journal={2020 IEEE Power & Energy Society Innovative Smart Grid Technologies Conference (ISGT)},
		year={2020},
		pages={1-5}
	}
	
	@inproceedings{Hubauer2013AnalysisOD,
		title={Analysis of data quality issues in real-world industrial data},
		author={Thomas Hubauer and S. Lamparter and M. Roshchin and N. Solomakhina and Stuart Watson},
		year={2013}
	}
	
	@article{Ding2019CleanitsAD,
		title={Cleanits: A Data Cleaning System for Industrial Time Series},
		author={Xiaoou Ding and Hongzhi Wang and Jiaxuan Su and Zijue Li and Jianzhong Li and H. Gao},
		journal={Proc. VLDB Endow.},
		year={2019},
		volume={12},
		pages={1786-1789}
	}
	
	@article{Wang2020TimeSD,
		title={Time Series Data Cleaning: A Survey},
		author={X. Wang and Chen Wang},
		journal={IEEE Access},
		year={2020},
		volume={8},
		pages={1866-1881}
	}
	
	@article{Gavrilut2011DealingWC,
		title={Dealing with Class Noise in Large Training Datasets for Malware Detection},
		author={Dragos Gavrilut and Liviu Ciortuz},
		journal={2011 13th International Symposium on Symbolic and Numeric Algorithms for Scientific Computing},
		year={2011},
		pages={401-407}
	}
	
	@article{Wang2019ALN,
		title={A Label Noise Robust Stacked Auto-Encoder Algorithm for Inaccurate Supervised Classification Problems},
		author={Ziyang Wang and Xiao-yi Luo and J. Liang},
		journal={Mathematical Problems in Engineering},
		year={2019},
		volume={2019},
		pages={1-19}
	}
	
	@inproceedings{Fredriksson2020DataLA,
		title={Data Labeling: An Empirical Investigation into Industrial Challenges and Mitigation Strategies},
		author={Teodor Fredriksson and D. I. Mattos and J. Bosch and H. H. Olsson},
		booktitle={PROFES},
		year={2020}
	}
	
	@article{Gan2018AutomaticLF,
		title={Automatic Labeling For Personalized IoT Wearable Monitoring},
		author={O. P. Gan},
		journal={IECON 2018 - 44th Annual Conference of the IEEE Industrial Electronics Society},
		year={2018},
		pages={2861-2866}
	}
	
	@article{McInnes2018UMAPUM,
		title={UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction},
		author={L. McInnes and J. Healy},
		journal={arXiv:1802.03426},
		year={2018}
	}
	
	@article{berthelot2019mixmatch,
		title={Mixmatch: A holistic approach to semi-supervised learning},
		author={Berthelot, David and Carlini, Nicholas and Goodfellow, Ian and Papernot, Nicolas and Oliver, Avital and Raffel, Colin},
		journal={arXiv:1905.02249},
		year={2019}
	}
	
	@inproceedings{chen2019understanding,
		title={Understanding and utilizing deep neural networks trained with noisy labels},
		author={Chen, Pengfei and Liao, Ben Ben and Chen, Guangyong and Zhang, Shengyu},
		booktitle={International Conference on Machine Learning},
		pages={1062--1070},
		year={2019}
	}
	
	@article{fawaz2019deep,
		title={Deep learning for time series classification: a review},
		author={Fawaz, Hassan Ismail and Forestier, Germain and Weber, Jonathan and Idoumghar, Lhassane and Muller, Pierre-Alain},
		journal={Data Mining and Knowledge Discovery},
		volume={33},
		number={4},
		pages={917--963},
		year={2019},
		publisher={Springer}
	}
	
	@article{demvsar2006statistical,
		title={Statistical comparisons of classifiers over multiple data sets},
		author={Dem{\v{s}}ar, Janez},
		journal={The Journal of Machine Learning Research},
		volume={7},
		pages={1--30},
		year={2006}
	}
	
	
	@article{Karim2018LSTMFC,
		title={LSTM Fully Convolutional Networks for Time Series Classification},
		author={Fazle Karim and Somshubra Majumdar and H. Darabi and Shun Chen},
		journal={IEEE Access},
		year={2018},
		volume={6},
		pages={1662-1669}
	}
	
	@article{Wu2020DeepTM,
		title={Deep Transformer Models for Time Series Forecasting: The Influenza Prevalence Case},
		author={N. Wu and Bradley Green and X. Ben and S. O'Banion},
		journal={arXiv:2001.08317},
		year={2020} 
	}
	
	
	@article{castellaniSuppl2021,
		title={Supplementary material for: Estimating the electrical power output of industrial devices with end-to-end time-series classification in the presence of label noise},
		author={Andrea Castellani and Sebastian Schmitt and  Barbara Hammer},
		journal={arXiv:2105.00349},
		year={2021} 
	}
\end{filecontents}


\begin{document}
\beginsupplement

%
\title{Supplementary Material for: Estimating the Electrical Power Output of Industrial Devices with End-to-End Time-Series Classification in the Presence of Label Noise }
\titlerunning{Supplementary Material}

% \orcidID{}
% OrcidID is removed for space reasons.(maybe)?
\newcommand{\orcidauthorA}{\orcidID{0000-0003-0476-5978}} % Andrea
\newcommand{\orcidauthorB}{\orcidID{0000-0001-7130-5483}} % Sebastian
\newcommand{\orcidauthorC}{\orcidID{0000-0002-0935-5591}} % Barbara
% Barbara
\author{Andrea~Castellani \Letter \orcidauthorA \inst{1} \and
Sebastian~Schmitt \orcidauthorB \inst{2} \and
Barbara~Hammer \orcidauthorC \inst{1}} 
%
\authorrunning{A. Castellani and S. Schmitt and B. Hammer}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
\institute{Bielefeld University, \\
\email{\{acastellani,bhammer\}@techfak.uni-bielefeld.de}\\ 
\and
Honda Research Institute Europe, \\
\email{sebastian.schmitt@honda-ri.de}
}


\maketitle              % typeset the header of the contribution


\section{Implementation details}\label{app:network}
All the experiment have been conducted on a Linux Ubuntu 18.04.5 server with $2 \times $ Intel Xeon 4110 (16 cores and 32 threads), 188 GB of DDR4 RAM and $4 \times $ NVIDIA RTX 2080 Ti. The software used is Python 3.8 and the deep learning library used is PyTorch 1.4 \cite{NEURIPS2019_9015}.
%We will open source our source code to reproduce the experiments with the benchmarks datasets right after the review process.
\new{Our source code is publicy available at \url{https://github.com/Castel44/SREA}.}
However, we are not allowed to share the dataset used for the estimation of the CHP electrical power output because of the company privacy policy.

The network used as encoder and decoder in the experiments is a Convolutional Neural Network (CNN).
Its basic block is a convolutional layer followed by a batch normalization layer \cite{ioffe2015batch}, a non-linear activation function and a dropout layer.
The basic \textit{convolution block} (ConvBlock) is defined as:
\begin{align}
    y = &\; W \ast \bm{x} + \bm{b}  \label{eq:conv}\\
    s = &\; \text{BatchNorm}(y)  \nonumber \\
    h = &\; \text{ReLU}(s)  \nonumber \\
    o = &\; \text{Dropout}(h) \nonumber 
\end{align}

In the decoder, the convolution operation in Eq.~\ref{eq:conv} is replaced by a TransposedConvolution (equivalent to up-sampling followed by a convolution). 
Thus, the ConvBlock is called TConvBlock.

The classifier uses what so called DenseBlock, where the convolution operation in Eq.~\ref{eq:conv} is replaced by the matrix multiplication: 
\begin{equation}
    y = W \cdot \bm{x} + \bm{b}
\end{equation}
A detailed description of the network used is reported in Table~\ref{tab:network}.

In all the experiments, we train for 100 epochs in total with the Adam optimizer \cite{kingma2014adam} with weight decay of $10^{-4}$, momentum parameters set to $\beta_1=0.9$ and $\beta_2=0.999$, and \textit{eps} of $10^{-6}$ , the initial learning rate is set to 0.01. We halve the learning rate every 20 epochs. 
The dropout probability is 0.2 and the non-linear activation used is ReLU.
We use a batch size of: $\min(\frac{1}{10}\times \text{dataset\_size}, 128)$.

We stress the fact that experiments across all the datasets, noise type and ratio share the same network and hyper-parameters configuration and lead to performance on the same level and higher over the state-of-the-art (SotA).
Indeed, we are likely reporting sub-optimal results that could be improved with a noise-free validation dataset, which availability is not assumed in this work.

\new{In order to evaluate multiple classifiers, we perform the Friedman non-parametric test at 0.05 level, as described in  \cite{demvsar2006statistical}, followed by Nemenyi post-hoc test.
To visualize this comparison, we use a critical difference diagram \cite{demvsar2006statistical}, where a thick horizontal line shows the algorithm that are not-signigicantly different in terms of F-score.}

\begin{table*}[bt]
    \centering
    \caption{Neural Network structure used in this work.}
    \begin{tabular*}{\textwidth}{l @{\extracolsep{\fill}}   l l}
        \toprule
        \textit{\textbf{Name}} & \textit{\textbf{Description}} & \textit{\textbf{Dimension}} \\
        \midrule
        \multicolumn{3}{c}{\textbf{Encoder}}\\
        \midrule
        ConvBlock1 & 128 filters, $4\times1$, stride=2 & $(\text{input\_size}\times\text{seq\_len})\to(128\times\floor{\frac{\text{seq\_len}}{2}})$\\
        ConvBlock2 & 128 filters, $4\times1$, stride=2 & $(128\times\floor{\frac{\text{seq\_len}}{2}})\to(128\times\floor{\frac{\text{seq\_len}}{4}})$\\
        ConvBlock3 & 256 filters, $4\times1$, stride=2 & $(128\times\floor{\frac{\text{seq\_len}}{4}})\to(256\times\floor{\frac{\text{seq\_len}}{8}})$\\
        ConvBlock4 & 256 ch, $4\times1$, stride=2 & $(256\times\floor{\frac{\text{seq\_len}}{8}})\to(256\times\floor{\frac{\text{seq\_len}}{16}})$\\
        Pool & GlobalAveragePooling1D & $(256\times\floor{\frac{\text{seq\_len}}{16}})\to(256\times1)$\\
        \textbf{Embedding} & 32 filters, $1\times1$ & $(256\times1)\to(32\times1)$\\
        \midrule
        \multicolumn{3}{c}{\textbf{Decoder}}\\
        \midrule
        Upsample & $\floor{\frac{\text{seq\_len}}{16}}$ neurons & $(32\times1)\to(32\times\floor{\frac{\text{seq\_len}}{16}})$\\
        TConvBlock1 & 256 ch, $4\times1$, stride=2 & $(32\times\floor{\frac{\text{seq\_len}}{16}})\to(256\times\floor{\frac{\text{seq\_len}}{16}})$\\
        TConvBlock2 & 256 filters, $4\times1$, stride=2 & $(256\times\floor{\frac{\text{seq\_len}}{8}})\to(128\times\floor{\frac{\text{seq\_len}}{4}})$\\
        TConvBlock3 & 128 filters, $4\times1$, stride=2 & $(128\times\floor{\frac{\text{seq\_len}}{4}})\to(128\times\floor{\frac{\text{seq\_len}}{2}})$\\
        TConvBlock4 & 128 filters, $4\times1$, stride=2 & $(128\times\floor{\frac{\text{seq\_len}}{2}})\to(\text{input\_size}\times\text{seq\_len})$\\
        \midrule
        \multicolumn{3}{c}{\textbf{Classifier}}\\
        \midrule
        DenseBlock & 128 neurons & $(32\times1)\to(128\times1)$\\
        Output & $\#$class neurons & $(128\times1)\to(\text{\#class}\times1)$\\
         \bottomrule
    \end{tabular*}
    \label{tab:network}
\end{table*}


\clearpage
\section{Definition of noise}\label{app:T}
The definition of the labels \textit{transition matrix} T, being $\epsilon \in [0, 1]$ the noise ratio and $k$ the number of classes, is as follows:

\begin{align*}
    \text{Symmetric noise: } T= & \,
    \begin{bmatrix}
        1 - \epsilon & \frac{\epsilon}{k -1} & \cdots & \frac{\epsilon}{k -1} & \frac{\epsilon}{k -1} \\
        \frac{\epsilon}{k -1} & 1-\epsilon & \frac{\epsilon}{k -1} & \cdots & \frac{\epsilon}{k -1}\\
        \vdots & & \ddots & & \vdots\\
        \frac{\epsilon}{k -1} & \cdots & \frac{\epsilon}{k -1} & 1-\epsilon & \frac{\epsilon}{k -1}\\
        \frac{\epsilon}{k -1} & \frac{\epsilon}{k -1} & \cdots & \frac{\epsilon}{k -1} & 1 - \epsilon
    \end{bmatrix}\\
    \text{Asymmetric noise: } T= &\,
    \begin{bmatrix}
        1-\epsilon & \epsilon & 0 & \cdots & 0 \\
        0 & 1-\epsilon & \epsilon & & 0\\
        \vdots & & \ddots & \ddots & \vdots\\
        0 & & & 1-\epsilon & \epsilon\\
        \epsilon & 0 & \cdots & 0 & 1-\epsilon
    \end{bmatrix}\\
    \text{Pair noise: } T = & \,
    \begin{bmatrix}
        1 & 0 & \cdots & & 0 \\
        \epsilon & 1-\epsilon & 0 & \cdots & 0 \\
        \vdots & & \ddots & & \vdots \\
        \epsilon & 0 & \cdots & 0 & 1-\epsilon
    \end{bmatrix}
\end{align*}
    


\section{Benchmarks Datasets results}
In Table~\ref{tab:ucr_symm} and Table~\ref{tab:ucr_asymm} we report the complete set of results with the 10 selected dataset from the UCR repository \cite{UCRArchive2018}, under presence of symmetric and asymmetric noise respectively.
\new{In the figures \ref{fig:UCR_CD_symm}, \ref{fig:UCR_CD_asymm} and \ref{fig:UCR_CD} is shown the critical difference diagram over the UCR benchmarks dataset under presence of symmetric, asymmetric and both lable noise respectively.}

\begin{table*}[h!]
    \centering
    \scriptsize
    \caption{$\F_1$ test scores on UCR datasets with symmetric noise.
    \new{In parenthesis the results of a Mannâ€“Whitney U test with $\alpha=0.05$ of \acrshort{method} against the other approachs: SREA $\F_1$ is significantly higher ($+$), lower ($-$) or not significant ($\approx$).}}
    \label{tab:ucr_symm}
    \begin{tabular*}{\linewidth}{l @{\extracolsep{\fill}} l l l l l l l}
    \toprule
    \textbf{Dataset}&\textbf{\%}&\textbf{CE}&\textbf{MixUp}&\textbf{M-BMM}&\textbf{SIGUA}&\textbf{Co-teach}&\textbf{\acrshort{method}}\\
    \midrule
    \multirow{5}{0.13\linewidth}{ArrowHead}
    & 0     & 0.899 $(\approx)$  & 0.841 $(\approx)$ & 0.751 $(+)$ & 0.859 $(\approx)$ & 0.819 $(+)$ & 0.855 \\
    & 15  & 0.813 $(\approx)$ & 0.723 $(\approx)$ & 0.721 $(\approx)$ & 0.700 $(\approx)$ & 0.766 $(\approx)$ & 0.751\\
    & 30  & 0.721 $(\approx)$ & 0.679 $(\approx)$ & 0.632 $(\approx)$ & 0.626 $(\approx)$ & 0.734 $(\approx)$ & 0.651\\
    & 45 & 0.569 $(-)$ & 0.509 $(\approx)$ & 0.517 $(\approx)$ & 0.527 $(\approx)$ & 0.533 $(\approx)$ & 0.445\\
    & 60 & 0.394 $(-)$ & 0.435 $(-)$ & 0.386 $(\approx)$ & 0.370 $(-)$ & 0.405 $(-)$ & 0.295 \\
    \midrule
    \multirow{5}{0.13\linewidth}{CBF}
    & 0     & 1.000 $(+)$ & 0.970 $(+)$ & 0.886 $(+)$ & 1.000 $(+)$ & 0.997 $(+)$ & 1.000\\
    & 15  & 0.943 $(+)$ & 0.923 $(+)$ & 0.941 $(+)$ & 0.976 $(+)$ & 0.923 $(+)$ & 1.000 \\
    & 30  & 0.780 $(+)$ & 0.799 $(+)$ & 0.932 $(+)$ & 0.923 $(+)$ & 0.833 $(+)$ & 0.998 \\
    & 45  & 0.570 $(+)$ & 0.635 $(+)$ & 0.846 $(+)$ & 0.730 $(+)$ & 0.617 $(+)$ & 0.981 \\
    & 60   & 0.450 $(\approx)$ & 0.456 $(\approx)$ & 0.552 $(\approx)$ & 0.390 $(\approx)$ & 0.448 $(\approx)$ & 0.347 \\
    \midrule
    \multirow{5}{0.13\linewidth}{Epilepsy}
    & 0 & 0.974 $(\approx)$ & 0.955 $(+)$ & 0.926 $(+)$ & 0.978 $(\approx)$ & 0.971 $(+)$ & 0.973 \\
    & 15 & 0.890 $(\approx)$ & 0.913 $(\approx)$ & 0.899 $(\approx)$ & 0.884 $(\approx)$ & 0.861 $(\approx)$ & 0.861\\
    & 30 & 0.784 $(-)$ & 0.823 $(-)$ & 0.805 $(-)$ &0.741 $(\approx)$ &0.744 $(\approx)$ & 0.708 \\
    & 45 & 0.604 $(-)$ & 0.630 $(-)$ & 0.650 $(-)$ & 0.600 $(-)$ & 0.661 $(-)$ & 0.446 \\
    & 60 & 0.464 $(-)$ & 0.441 $(-)$ & 0.446 $(-)$ & 0.459 $(-)$ & 0.399 $(\approx)$ & 0.340\\
    \midrule
    \multirow{5}{0.13\linewidth}{FaceFour}
    & 0 & 0.991 $(\approx)$ & 0.974 $(+)$ & 0.929 $(+)$ & 0.955 $(+)$ & 0.908 $(+)$ & 1.000 \\
    & 15 & 0.893 $(\approx)$ & 0.836 $(+)$ & 0.834 $(+)$ & 0.845 $(\approx)$ & 0.866 $(\approx)$ & 0.931\\
    & 30 & 0.667 $(\approx)$ & 0.658 $(\approx)$ & 0.699 $(\approx)$ & 0.719 $(-)$ & 0.706 $(\approx)$ & 0.663\\
    & 45 & 0.498 $(\approx)$ & 0.493 $(\approx)$ & 0.470 $(\approx)$ & 0.455 $(\approx)$ & 0.504 $(\approx)$ & 0.556\\
    & 60 & 0.364 $(\approx)$ & 0.314 $(\approx)$ & 0.390 $(\approx)$ & 0.274 $(\approx)$ & 0.411 $(\approx)$ & 0.342\\
    \midrule
    \multirow{5}{0.13\linewidth}{Melbourne}
    & 0 & 0.923 $(\approx)$ & 0.879 $(+)$ & 0.773 $(+)$ & 0.918 $(\approx)$ & 0.913 $(\approx)$ & 0.911\\
    & 15 & 0.869 $(+)$ & 0.870 $(+)$ & 0.856 $(+)$ & 0.883 $(\approx)$ & 0.886 $(\approx)$ & 0.883\\
    & 30 & 0.826 $(+)$ & 0.858 $(\approx)$ & 0.870 $(\approx)$ & 0.855 $(\approx)$ & 0.876 $(-)$ & 0.862\\
    & 45 & 0.767 $(+)$ & 0.817 $(+)$ & 0.818 $(\approx)$ & 0.832 $(\approx)$ & 0.848 $(\approx)$ & 0.841\\
    & 60 & 0.662 $(\approx)$ & 0.716 $(+)$ & 0.734 $(+)$ & 0.778 $(+)$ & 0.805 $(+)$ & 0.812\\
    \midrule
    \multirow{5}{0.13\linewidth}{NATOPS}
    & 0 & 0.858 $(\approx)$ & 0.801 $(\approx)$ & 0.711 $(+)$ & 0.848 $(\approx)$ & 0.835 $(\approx)$ & 0.866\\
    & 15 & 0.779 $(\approx)$ & 0.718 $(+)$ & 0.702 $(+)$ & 0.754 $(\approx)$ & 0.761 $(\approx)$ & 0.796\\
    & 30 & 0.587 $(\approx)$ & 0.580 $(+)$ & 0.602 $(+)$ & 0.593 $(+)$ & 0.673 $(+)$ & 0.670\\
    & 45 & 0.436 $(\approx)$ & 0.466 $(\approx)$ & 0.544 $(\approx)$ & 0.452 $(\approx)$ & 0.516 $(\approx)$ & 0.483\\
    & 60 & 0.320 $(\approx)$ & 0.303 $(\approx)$ & 0.339 $(\approx)$ & 0.328 $(\approx)$ & 0.410 $(-)$ & 0.335\\
    \midrule
    \multirow{5}{0.13\linewidth}{OSULeaf}
    & 0 & 0.852 $(\approx)$ & 0.590 $(\approx)$ & 0.478 $(+)$ & 0.796 $(\approx)$ & 0.828 $(\approx)$ & 0.706\\
    & 15 & 0.781 $(-)$ & 0.558 $(\approx)$ & 0.502 $(\approx)$ & 0.694 $(\approx)$ & 0.741 $(\approx)$ & 0.651\\
    & 30 & 0.692 $(\approx)$ & 0.400 $(+)$ & 0.553 $(\approx)$ & 0.611 $(\approx)$ & 0.673 $(\approx)$ & 0.554\\
    & 45 & 0.534 $(-)$ & 0.384 $(\approx)$ & 0.400 $(\approx)$ & 0.474 $(-)$ & 0.539 $(-)$ & 0.355\\
    & 60 & 0.427 $(-)$ & 0.305 $(-)$ & 0.328 $(-)$ & 0.376 $(-)$ & 0.420 $(-)$ & 0.252\\
    \midrule
    \multirow{5}{0.13\linewidth}{Plane}
    & 0 & 0.995 $(\approx)$ & 0.962 $(+)$ & 0.577 $(+)$ & 0.981 $(+)$ & 0.990 $(\approx)$ & 0.998\\
    & 15 & 0.930 $(+)$ & 0.953 $(+)$ & 0.873 $(+)$ & 0.971 $(\approx)$ & 0.981 $(\approx)$ & 0.983\\
    & 30 & 0.887 $(+)$ & 0.902 $(+)$ & 0.943 $(\approx)$ & 0.862 $(+)$ & 0.941 $(\approx)$ & 0.944\\
    & 45 & 0.671 $(+)$ & 0.783 $(+)$ & 0.761 $(+)$ & 0.797 $(+)$ & 0.808 $(+)$ & 0.911\\
    & 60 & 0.575 $(\approx)$ & 0.593 $(\approx)$ & 0.561 $(\approx)$ & 0.529 $(\approx)$ & 0.659 $(\approx)$ & 0.629\\
    \midrule
    \multirow{5}{0.13\linewidth}{Symbols}
    & 0 & 0.994 $(-)$ & 0.931 $(+)$ & 0.642 $(+)$ & 0.979 $(\approx)$ & 0.979 $(\approx)$ & 0.987\\ 
    & 15 & 0.982 $(\approx)$ & 0.871 $(+)$ & 0.689 $(+)$ & 0.968 $(\approx)$ & 0.979 $(\approx)$ & 0.983\\
    & 30 & 0.974 $(\approx)$ & 0.880 $(+)$ & 0.911 $(+)$ & 0.940 $(+)$ & 0.982 $(\approx)$ & 0.983\\
    & 45 & 0.945 $(\approx)$ & 0.815 $(\approx)$ & 0.839 $(\approx)$ & 0.916 $(\approx)$ & 0.928 $(\approx)$ & 0.891\\
    & 60 & 0.862 $(-)$ & 0.738 $(-)$ & 0.599 $(\approx)$ & 0.790 $(-)$ & 0.945 $(-)$ & 0.326\\
    \midrule
    \multirow{5}{0.13\linewidth}{Trace}
    & 0 & 1.000 $(\approx)$ & 0.995 $(\approx)$ & 0.634 $(+)$ & 1.000 $(\approx)$ & 0.933 $(+)$ & 1.000\\
    & 15 & 0.903 $(+)$ & 0.870 $(+)$ & 0.866 $(+)$ & 0.980 $(+)$ & 0.965 $(+)$ & 0.995\\
    & 30 & 0.702 $(+)$ & 0.773 $(+)$ & 0.629 $(+)$ & 0.892 $(+)$ & 0.887 $(+)$ & 0.952\\
    & 45 & 0.612 $(\approx)$ & 0.518 $(+)$ & 0.643 $(\approx)$ & 0.646 $(\approx)$ & 0.781 $(\approx)$ & 0.667\\
    & 60 & 0.402 $(\approx)$ & 0.347 $(+)$ & 0.435 $(\approx)$ & 0.378 $(\approx)$ & 0.552 $(\approx)$ & 0.559\\
    \bottomrule
    \toprule
    \multirow{6}{0.13\linewidth}{\textit{\textbf{Summary Results}}}
    & \textbf{\%} & \multicolumn{2}{c}{\textit{\textbf{\#Better $(+)$}}} & \multicolumn{2}{c}{\textit{\textbf{\#Equal $(\approx)$}}} & \multicolumn{2}{c}{\textit{\textbf{\#Worse $(-)$}}} \\
    & 0 &\multicolumn{2}{c}{\textit{\textbf{26}}} & \multicolumn{2}{c}{\textit{\textbf{23}}} & \multicolumn{2}{c}{\textit{\textbf{1}}} \\
    & 15 &\multicolumn{2}{c}{\textit{\textbf{22}}} & \multicolumn{2}{c}{\textit{\textbf{27}}} & \multicolumn{2}{c}{\textit{\textbf{1}}} \\
    & 30 &\multicolumn{2}{c}{\textit{\textbf{22}}} & \multicolumn{2}{c}{\textit{\textbf{23}}} & \multicolumn{2}{c}{\textit{\textbf{5}}} \\
    & 45 &\multicolumn{2}{c}{\textit{\textbf{13}}} & \multicolumn{2}{c}{\textit{\textbf{28}}} & \multicolumn{2}{c}{\textit{\textbf{9}}} \\
    & 60 &\multicolumn{2}{c}{\textit{\textbf{5}}} & \multicolumn{2}{c}{\textit{\textbf{27}}} & \multicolumn{2}{c}{\textit{\textbf{18}}} \\
    \bottomrule
    \end{tabular*}

\end{table*}


\begin{table*}[th!]
    \centering
    \scriptsize
    \caption{$\mathcal{F}_1$ test scores on UCR datasets with asymmetric noise.
    \new{In parenthesis the results of a Mannâ€“Whitney U test with $\alpha=0.05$ of \acrshort{method} against the other approachs: SREA $\F_1$ is significantly higher ($+$), lower ($-$) or not significant ($\approx$).}}
    \label{tab:ucr_asymm}
    \begin{tabular*}{\linewidth}{l @{\extracolsep{\fill}} l l l l l l l}
    \toprule
    \textbf{Dataset}&\textbf{\%}&\textbf{CE}&\textbf{MixUp}&\textbf{M-BMM}&\textbf{SIGUA}&\textbf{Co-teach}&\textbf{\acrshort{method}}\\
    \midrule
    \multirow{5}{0.13\linewidth}{ArrowHead}
    & 0   & 0.895 $(\approx)$  & 0.850 $(\approx)$ & 0.761 $(+)$ & 0.867 $(\approx)$ & 0.835 $(+)$ & 0.864 \\
    & 10  & 0.850 $(\approx)$ & 0.779 $(\approx)$ & 0.712 $(\approx)$ & 0.743 $(+)$ & 0.825 $(\approx)$ & 0.821\\
    & 20  & 0.790 $(\approx)$ & 0.729 $(\approx)$ & 0.685 $(\approx)$ & 0.724 $(\approx)$ & 0.748 $(\approx)$ & 0.753\\
    & 30  & 0.741 $(\approx)$ & 0.685 $(\approx)$ & 0.616 $(\approx)$ & 0.609 $(\approx)$ & 0.699 $(\approx)$ & 0.563\\
    & 40 & 0.644 $(\approx)$ & 0.592 $(\approx)$ & 0.573 $(\approx)$ & 0.592 $(\approx)$ & 0.512 $(\approx)$ & 0.498 \\
    \midrule
    \multirow{5}{0.13\linewidth}{CBF}
    & 0     & 1.000 $(+)$ & 0.965 $(+)$ & 0.915 $(+)$ & 1.000 $(+)$ & 1.000 $(+)$ & 1.000\\
    & 10  & 0.973 $(+)$ & 0.956 $(+)$ & 0.920 $(+)$ & 0.989 $(+)$ & 0.963 $(+)$ & 1.000 \\
    & 20  & 0.905 $(+)$ & 0.897 $(+)$ & 0.949 $(+)$ & 0.980 $(+)$ & 0.900 $(+)$ & 1.000 \\
    & 30  & 0.779 $(+)$ & 0.771 $(+)$ & 0.954 $(+)$ & 0.880 $(+)$ & 0.857 $(+)$ & 0.999 \\
    & 40   & 0.663 $(+)$ & 0.644 $(+)$ & 0.816 $(+)$ & 0.681 $(+)$ & 0.665 $(+)$ & 0.989 \\
    \midrule
    \multirow{5}{0.13\linewidth}{Epilepsy}
    & 0 & 0.978 $(\approx)$ & 0.951 $(+)$ & 0.905 $(+)$ & 0.948 $(+)$ & 0.956 $(+)$ & 0.984 \\
    & 10 & 0.919 $(\approx)$ & 0.930 $(\approx)$ & 0.847 $(\approx)$ & 0.905 $(\approx)$ & 0.919 $(\approx)$ & 0.888\\
    & 20 & 0.861 $(\approx)$ & 0.894 $(-)$ & 0.891 $(-)$ &0.826 $(\approx)$ &0.863 $(\approx)$ & 0.825 \\
    & 30 & 0.781 $(-)$ & 0.783 $(-)$ & 0.800 $(-)$ & 0.783 $(\approx)$ & 0.727 $(\approx)$ & 0.712 \\
    & 40 & 0.648 $(-)$ & 0.688 $(-)$ & 0.647 $(-)$ & 0.642 $(\approx)$ & 0.671 $(-)$ & 0.571\\
    \midrule
    \multirow{5}{0.13\linewidth}{FaceFour}
    & 0 & 0.974 $(\approx)$ & 0.956 $(+)$ & 0.965 $(+)$ & 0.945 $(+)$ & 0.887 $(+)$ & 0.974 \\
    & 10 & 0.900 $(\approx)$ & 0.844 $(+)$ & 0.879 $(\approx)$ & 0.864 $(+)$ & 0.855 $(+)$ & 0.939\\
    & 20 & 0.754 $(+)$ & 0.720 $(+)$ & 0.748 $(+)$ & 0.721 $(+)$ & 0.696 $(+)$ & 0.912\\
    & 30 & 0.630 $(+)$ & 0.639 $(+)$ & 0.624 $(+)$ & 0.657 $(+)$ & 0.726 $(\approx)$ & 0.811\\
    & 40 & 0.568 $(\approx)$ & 0.516 $(\approx)$ & 0.546 $(\approx)$ & 0.450 $(\approx)$ & 0.613 $(\approx)$ & 0.686\\
    \midrule
    \multirow{5}{0.13\linewidth}{Melbourne}
    & 0 & 0.921 $(\approx)$ & 0.878 $(+)$ & 0.744 $(+)$ & 0.913 $(\approx)$ & 0.911 $(\approx)$ & 0.923\\
    & 10 & 0.898 $(+)$ & 0.877 $(+)$ & 0.860 $(+)$ & 0.899 $(+)$ & 0.897 $(+)$ & 0.911\\
    & 20 & 0.865 $(+)$ & 0.861 $(+)$ & 0.851 $(+)$ & 0.858 $(+)$ & 0.893 $(\approx)$ & 0.903\\
    & 30 & 0.790 $(+)$ & 0.839 $(+)$ & 0.826 $(+)$ & 0.829 $(+)$ & 0.864 $(+)$ & 0.889\\
    & 40 & 0.701 $(+)$ & 0.757 $(+)$ & 0.689 $(+)$ & 0.792 $(+)$ & 0.831 $(\approx)$ & 0.836\\
    \midrule
    \multirow{5}{0.13\linewidth}{NATOPS}
    & 0 & 0.843 $(\approx)$ & 0.818 $(\approx)$ & 0.724 $(+)$ & 0.842 $(\approx)$ & 0.841 $(\approx)$ & 0.851\\
    & 10 & 0.798 $(+)$ & 0.822 $(\approx)$ & 0.756 $(+)$ & 0.764 $(+)$ & 0.790 $(+)$ & 0.829\\
    & 20 & 0.703 $(\approx)$ & 0.763 $(\approx)$ & 0.762 $(\approx)$ & 0.698 $(\approx)$ & 0.733 $(\approx)$ & 0.762\\
    & 30 & 0.627 $(\approx)$ & 0.678 $(\approx)$ & 0.695 $(\approx)$ & 0.654 $(\approx)$ & 0.644 $(\approx)$ & 0.675\\
    & 40 & 0.503 $(\approx)$ & 0.586 $(\approx)$ & 0.609 $(\approx)$ & 0.443 $(+)$ & 0.562 $(\approx)$ & 0.576\\
    \midrule
    \multirow{5}{0.13\linewidth}{OSULeaf}
    & 0 & 0.862 $(\approx)$ & 0.582 $(\approx)$ & 0.579 $(+)$ & 0.806 $(\approx)$ & 0.865 $(\approx)$ & 0.751\\
    & 10 & 0.796 $(\approx)$ & 0.575 $(\approx)$ & 0.550 $(+)$ & 0.760 $(\approx)$ & 0.805 $(\approx)$ & 0.686\\
    & 20 & 0.730 $(\approx)$ & 0.473 $(\approx)$ & 0.502 $(\approx)$ & 0.680 $(\approx)$ & 0.718 $(\approx)$ & 0.634\\
    & 30 & 0.654 $(-)$ & 0.499 $(\approx)$ & 0.427 $(\approx)$ & 0.587 $(\approx)$ & 0.676 $(-)$ & 0.533\\
    & 40 & 0.520 $(-)$ & 0.354 $(-)$ & 0.327 $(-)$ & 0.450 $(-)$ & 0.467 $(-)$ & 0.355\\
    \midrule
    \multirow{5}{0.13\linewidth}{Plane}
    & 0 & 0.995 $(\approx)$ & 0.962 $(+)$ & 0.619 $(+)$ & 0.986 $(+)$ & 0.995 $(\approx)$ & 0.995\\
    & 10 & 0.981 $(\approx)$ & 0.986 $(\approx)$ & 0.648 $(+)$ & 0.986 $(\approx)$ & 0.990 $(\approx)$ & 0.976\\
    & 20 & 0.952 $(\approx)$ & 0.923 $(\approx)$ & 0.751 $(+)$ & 0.976 $(\approx)$ & 0.990 $(\approx)$ & 0.966\\
    & 30 & 0.886 $(+)$ & 0.877 $(\approx)$ & 0.686 $(+)$ & 0.899 $(\approx)$ & 0.924 $(\approx)$ & 0.941\\
    & 40 & 0.745 $(\approx)$ & 0.741 $(\approx)$ & 0.637 $(\approx)$ & 0.694 $(+)$ & 0.738 $(\approx)$ & 0.824\\
    \midrule
    \multirow{5}{0.13\linewidth}{Symbols}
    & 0 & 0.992 $(-)$ & 0.941 $(+)$ & 0.684 $(+)$ & 0.988 $(\approx)$ & 0.987 $(\approx)$ & 0.980\\ 
    & 10 & 0.989 $(-)$ & 0.902 $(+)$ & 0.676 $(+)$ & 0.981 $(\approx)$ & 0.986 $(-)$ & 0.980\\
    & 20 & 0.990 $(-)$ & 0.907 $(+)$ & 0.677 $(+)$ & 0.985 $(-)$ & 0.958 $(+)$ & 0.977\\
    & 30 & 0.984 $(-)$ & 0.707 $(+)$ & 0.696 $(+)$ & 0.857 $(+)$ & 0.968 $(\approx)$ & 0.955\\
    & 40 & 0.908 $(\approx)$ & 0.594 $(+)$ & 0.573 $(+)$ & 0.791 $(\approx)$ & 0.916 $(\approx)$ & 0.798\\
    \midrule
    \multirow{5}{0.13\linewidth}{Trace}
    & 0 & 1.000 $(\approx)$ & 1.000 $(\approx)$ & 0.704 $(+)$ & 0.984 $(\approx)$ & 0.962 $(+)$ & 1.000\\
    & 10 & 0.934 $(+)$ & 0.758 $(+)$ & 0.781 $(+)$ & 0.953 $(\approx)$ & 0.891 $(+)$ & 0.992\\
    & 20 & 0.899 $(+)$ & 0.769 $(+)$ & 0.588 $(+)$ & 0.912 $(+)$ & 0.919 $(+)$ & 0.990\\
    & 30 & 0.737 $(+)$ & 0.665 $(+)$ & 0.551 $(+)$ & 0.833 $(\approx)$ & 0.849 $(\approx)$ & 0.876\\
    & 40 & 0.629 $(\approx)$ & 0.576 $(\approx)$ & 0.549 $(+)$ & 0.467 $(+)$ & 0.679 $(\approx)$ & 0.726\\
    \bottomrule
    \toprule
    \multirow{6}{0.13\linewidth}{\textit{\textbf{Summary Results}}}
    & \textbf{\%} & \multicolumn{2}{c}{\textit{\textbf{\#Better $(+)$}}} & \multicolumn{2}{c}{\textit{\textbf{\#Equal $(\approx)$}}} & \multicolumn{2}{c}{\textit{\textbf{\#Worse $(-)$}}} \\
    & 0 &\multicolumn{2}{c}{\textit{\textbf{26}}} & \multicolumn{2}{c}{\textit{\textbf{23}}} & \multicolumn{2}{c}{\textit{\textbf{1}}} \\
    & 10 &\multicolumn{2}{c}{\textit{\textbf{26}}} & \multicolumn{2}{c}{\textit{\textbf{22}}} & \multicolumn{2}{c}{\textit{\textbf{2}}} \\
    & 20 &\multicolumn{2}{c}{\textit{\textbf{23}}} & \multicolumn{2}{c}{\textit{\textbf{23}}} & \multicolumn{2}{c}{\textit{\textbf{4}}} \\
    & 30 &\multicolumn{2}{c}{\textit{\textbf{22}}} & \multicolumn{2}{c}{\textit{\textbf{22}}} & \multicolumn{2}{c}{\textit{\textbf{6}}} \\
    & 40 &\multicolumn{2}{c}{\textit{\textbf{15}}} & \multicolumn{2}{c}{\textit{\textbf{30}}} & \multicolumn{2}{c}{\textit{\textbf{5}}} \\
    \bottomrule
    \end{tabular*}

\end{table*}


\begin{figure}[th]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/Critical_Distances/UCR_Symm_CD.pdf}
    \caption{Critical difference diagram showing pairwise statistical difference comparison of \acrshort{method} and SoTA on UCR benchmark datasets corrupted with symmetric label noise.}
    \label{fig:UCR_CD_symm}
\end{figure}

\begin{figure}[th]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/Critical_Distances/UCR_Asymm_CD.pdf}
    \caption{Critical difference diagram showing pairwise statistical difference comparison of \acrshort{method} and SoTA on UCR benchmark datasets corrupted with asymmetric label noise.}
    \label{fig:UCR_CD_asymm}
\end{figure}

\begin{figure}[th]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/Critical_Distances/UCR_All_CD.pdf}
    \caption{Critical difference diagram showing pairwise statistical difference comparison of \acrshort{method} and SoTA on UCR benchmark datasets corrupted with both symmetric and asymmetric label noise.}
    \label{fig:UCR_CD}
\end{figure}



\clearpage
\section{CHP electrical power output estimation results}
\new{In Fig.~\ref{fig:ACF} is shown the plot of the autocorrelation function (ACF) on the CHP dataset. It is clearly notable a positive peak at 140 samples, which means that there is a daily seasonal period.
Please note that the time-structure of the time series data is not utilized in this work. Each sample is considered completely independent from each other, even though they might be derived from two adjacent sliding windows.}

\begin{figure}[tb]
    \centering
    \includegraphics[width=0.80\textwidth]{figures/CHP/ACF.pdf}
    \caption{Autocorrelation plot for the CHP dataset. The autocorrelation function has the second-largest positive peak at 140 samples (1 day).}
    \label{fig:ACF}
\end{figure}


In Fig.\ref{fig:cm_umap_symm} and Fig.~\ref{fig:cm_umap_asymm} we show the confusion matrix of the corrected labels (left) and the embedding space (right) of the \acrshort{method} when training with 30\% of symmetric and asymmetric noise ratio, respectively.
In order to visualize the 32-dimensional embedding space, the UMAP dimensional reduction algorithm \cite{McInnes2018UMAPUM} has been used.
Recall that, for the asymmetric noise type, we circularly shift 30\% of the labels by one i.e $2 \to 3,\; 3 \to 4$. 
This reflects on the errors made on the test set, observed by the confusion matrix in Fig.~\ref{fig:cm_umap_asymm} (left), in particular for the intermediate classes i.e. 1, 2, 3, which are the most challenging for this problem.

\new{In Fig.\ref{fig:CHP_CD_symm}, \ref{fig:CHP_CD_asymm}, \ref{fig:CHP_CD_flip} is depicted the critical difference diagram 
on the CHP dataset, under presence of symmetric, asymmetric and flip noise respectively.
In Fig.~\ref{fig:CHP_CD} we show the critical difference diagram with all the noise types are combined.}

\begin{figure}[th]
    \centering
    \includegraphics[width=0.4\textwidth]{figures/CHP/symm/cm_symm.pdf}\:
    \centering
    \includegraphics[width=0.4\textwidth]{figures/CHP/symm/embedding_symm.pdf}
    \caption{Confusion matrix of the corrected labels (left) and embedding space of the train-set (right) of the CHP power estimation, corrupted with 30\% symmetric noise.}
    \label{fig:cm_umap_symm}
\end{figure}


\begin{figure}[th]
    \centering
    \includegraphics[width=0.4\textwidth]{figures/CHP/asymm/cm_asymm.pdf}\:
    \centering
    \includegraphics[width=0.4\textwidth]{figures/CHP/asymm/embedding_asymm.pdf}
    \caption{Confusion matrix of the corrected labels (left) and embedding space of the train-set (right) of the CHP power estimation, corrupted with 30\% asymmetric noise.}
    \label{fig:cm_umap_asymm}
\end{figure}



\begin{figure}[th]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/Critical_Distances/Symm_CHP_CD.pdf}
    \caption{Critical difference diagram showing pairwise statistical difference comparison of \acrshort{method} and SoTA on the CHP dataset corrupted with symmetric label noise.}
    \label{fig:CHP_CD_symm}
\end{figure}

\begin{figure}[th]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/Critical_Distances/Asymm_1_CHP_CD.pdf}
    \caption{Critical difference diagram showing pairwise statistical difference comparison of \acrshort{method} and SoTA on the CHP dataset corrupted with asymmetric label noise.}
    \label{fig:CHP_CD_asymm}
\end{figure}

\begin{figure}[th]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/Critical_Distances/Asymm_-1_CHP_CD.pdf}
    \caption{Critical difference diagram showing pairwise statistical difference comparison of \acrshort{method} and SoTA on the CHP dataset corrupted with flip label noise.}
    \label{fig:CHP_CD_flip}
\end{figure}

\begin{figure}[th]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/Critical_Distances/ALL_CHP_CD.pdf}
    \caption{Critical difference diagram showing pairwise statistical difference comparison of \acrshort{method} and SoTA on the CHP dataset corrupted with symmetric, asymmetric and flip label noise.}
    \label{fig:CHP_CD}
\end{figure}


Fig~\ref{fig:CHP_results} depicts the box-plot of the $\mathcal{F}_1$ test score under different types and levels of noise, with the comparison with other SotA algorithm.
It is clearly visible that the proposed \acrshort{method} has comparable (and sometimes better) performance than the other algorithms.

\begin{figure}[h!]
\begin{subfigure}{\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figures/CHP/symm.pdf}
    \caption{Symmetric noise.}
    \label{fig:CHP_symm}
\end{subfigure}
\begin{subfigure}{\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figures/CHP/asymm.pdf}
    \caption{Asymmetric noise.}
    \label{fig:CHP_asymm}
\end{subfigure}
\centering
\begin{subfigure}{\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figures/CHP/flip.pdf}
    \caption{Flip noise.}
    \label{fig:CHP_flip}
\end{subfigure}%
\caption{$\mathcal{F}_1$ test scores of the CHP power estimation when training with different noise type. Each experiment consists of 10 independent runs. }
\label{fig:CHP_results}
\end{figure}





In Fig.~\ref{fig:CHP_correct} we show the results of the \acrshort{method} on the data with the real sensor failure.
We highlight that the method was able to correctly re-label the period of the $P_{CHP}$ sensor failure (night of 19$^{th}$ Sept.) and also detect the other segments where the CHP was active (night of 25$^{th}$ Sept.).
The offset between the detection and the real activation of the machine is due to the sliding windowing operation in the creation of the data.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{figures/data_plots/CHP_corrected.pdf}
    \caption{Example of label noise in the CHP dataset corrected with \acrshort{method}.}
    \label{fig:CHP_correct}
\end{figure}



\clearpage
\section{Ablation studies}



\comment{
\subsection{Input output window size.
}
\andrea{Input - output window size.}
\andrea{I think it is better to remove this. }
\begin{table*}[tbh]
    \centering
    \scriptsize
    \setlength{\tabcolsep}{3pt}
    \begin{tabular*}{\textwidth}{l @{\extracolsep{\fill}} l  c c c c | c c}
    \toprule
    \multirow{2}{*}{$\Delta t_{in}$} & \multirow{2}{*}{$\Delta t_{out}$} & \multicolumn{4}{c |}{\textbf{Symmetric}} & \multicolumn{2}{c}{\textbf{Asymmetric}} \\
    \cmidrule(lr){3-8}
    & & 0 & 15 & 30 & 45 & 20 & 30\\
    \midrule
    60 & 60 & 0.905\rpm0.036 & 0.891\rpm0.043 & 0.866\rpm0.038 & 0.833\rpm0.010 & 0.896\rpm0.031 & 0.752\rpm0.269 \\
    360 & 60 & 0.919\rpm0.034 & 0.892\rpm0.032 & 0.875\rpm0.028 & 0.877\rpm0.034 & 0.864\rpm0.038 & 0.737\rpm0.295\\
    \underline{360} & \underline{360} & \underline{0.979\rpm0.002} & \underline{0.960\rpm0.006} & \underline{0.938\rpm0.010} & \underline{0.918\rpm0.008} & \underline{0.946\rpm0.005} & \underline{0.919\rpm0.007}\\
    \bottomrule
    \end{tabular*}
    \caption{Real-world application. Comparison at varying of intput and target windows. Mean values and standard deviation over 10 runs. Values of $\Delta t_{IN}$ and $\Delta t_{OUT}$ in minutes. \andrea{cause space issues, I have just reported values for low-noise settings. Thats the same reason why I used underline instead of bold to highlight the best values. Is this okay? Should note others? }}
    \label{tab:CHP_window}
\end{table*}
}

\subsection{Input signals.}

In Table~\ref{tab:CHP_exog} we report the ablation studies on the input variables for the estimation of the CHP power level with the algorithm \acrshort{method}, with both symmetric and asymmetric noise.

\begin{table*}[htb]
    \centering
    \scriptsize
    \setlength{\tabcolsep}{2pt}
    \caption{Ablation studies on the input variables for the estimation of CHP power level with \acrshort{method}. The best results per noise type and ratio are underlined.}
    \label{tab:CHP_exog}
    \begin{tabular*}{\textwidth}{l l l @{\extracolsep{\fill}}  c | c c c | c c}
    \toprule
    \multirow{2}{*}{$\bm{P_{tot}}$} & \multirow{2}{*}{$\bm{T_{amb}}$} & \multirow{2}{*}{$\bm{T_{wtr}}$} &\textbf{Noise} & \multicolumn{3}{c|}{\textbf{\textbf{Symmetric}}} & \multicolumn{2}{c}{\textbf{Asymmetric}} \\
    \cmidrule(lr){4-9}
    & & & 0 & 15 & 30 & 45 & 20 & 30\\
    \midrule
    \xmark & \cmark & \xmark & 0.625\rpm0.043 & 0.524\rpm0.036 & 0.530\rpm0.017 & 0.521\rpm0.053 & 0.491\rpm0.041 & 0.531\rpm0.021 \\
    \xmark & \xmark & \cmark & 0.944\rpm0.003 & 0.903\rpm0.006 & 0.853\rpm0.025 & 0.799\rpm0.028 & 0.886\rpm0.004 & 0.822\rpm0.028 \\
    \xmark & \cmark & \cmark & 0.962\rpm0.003 & 0.935\rpm0.008 & 0.901\rpm0.021 & 0.863\rpm0.014 & 0.915\rpm0.015 & 0.869\rpm0.009\\
    \cmark & \xmark & \xmark & 0.938\rpm0.004 & 0.887\rpm0.004 & 0.845\rpm0.010 & 0.793\rpm0.012 & 0.868\rpm0.010 & 0.832\rpm0.011 \\
    \cmark & \cmark & \xmark & 0.955\rpm0.007 & 0.919\rpm0.009 & 0.889\rpm0.005 & 0.835\rpm0.017 & 0.896\rpm0.009 & 0.871\rpm0.009 \\
    \cmark & \xmark & \cmark & 0.974\rpm0.003 & 0.947\rpm0.004 & 0.928\rpm0.007 & 0.885\rpm0.015 & 0.934\rpm0.009 & 0.910\rpm0.009 \\
    \cmark & \cmark & \cmark & \underline{0.978\rpm0.001} & \underline{0.963\rpm0.003} & \underline{0.937\rpm0.007} & \underline{0.914\rpm0.004} & \underline{0.941\rpm0.007} & \underline{0.921\rpm0.010}\\
    \bottomrule
    \end{tabular*}
\end{table*}


\subsection{Hyper-parameter sensitivity}

In this section, we show the $\mathcal{F}_1$ test scores on the complete grid search of the \acrshort{method} hyper-parameters analyzed: $\lambda_{init} \in \{0, 10, 20, 40\}$, $\Delta_{start} \in \{0, 5, 10, 15, 25\}$ and $\Delta_{end} \in \{ 10, 20, 30 \}$.

In Fig.~\ref{fig:CHP_symm} with symmetric noise, in Fig.~\ref{fig:CHP_asymm} with asymmetric noise and in Fig.~\ref{fig:CHP_flip} with flip noise.
In those figures, each row of plots correspond to a different noise ratio, eacg column to a different value for $\Delta_{end}$.
For every subplot, in the x-axis are listed the values for $\lambda_{init}$.

\begin{figure}[th!]
    \centering
    \includegraphics[width=\textwidth]{figures/CHP/symm/results.pdf}
    \caption{\acrshort{method} hyper-parameter ablation studies on the CHP dataset when training with symmetric noise.}
    \label{fig:CHP_symm}
\end{figure}

\begin{figure}[th!]
    \centering
    \includegraphics[width=\textwidth]{figures/CHP/asymm/results.pdf}
    \caption{\acrshort{method} hyper-parameter ablation studies on the CHP dataset when training with asymmetric noise.}
    \label{fig:CHP_asymm}
\end{figure}

\begin{figure}[th!]
    \centering
    \includegraphics[width=\textwidth]{figures/CHP/flip/results.pdf}
    \caption{\acrshort{method} hyper-parameter ablation studies on the CHP dataset when training with flip noise.}
    \label{fig:CHP_flip}
\end{figure}



\clearpage
\bibliographystyle{splncs04}
\bibliography{bibliosupplementary.bib}

\end{document}

