\section{Model Evaluation}
\label{sec:evaluation}

We begin by evaluating how well the output probabilities of {\method} follow what happens in reality.  
When a team is given a 70\% probability of winning at a given state of the game, this essentially means that if the game was played from that state onwards 100 times, the team is expected to win approximately 70 of them.  
Of course, it is clear that we cannot have the game played more than once so one way to evaluate the probabilities of our model is to consider all the instances where the model provided a probability for team winning $x\%$ and calculate the fraction of instances that ended up in a win for this team.  
Ideally we would expect this fraction to be $x\%$ as well. 

\begin{figure}[t]
\begin{center}
\includegraphics[scale=0.25]{plots/fullwinprob.pdf}%\vspacecap
 \caption{{\method} is consistent over the whole range of probabilities.}
 \label{fig:fullwp}
\end{center}
\end{figure}

In order to obtain these results we split our data in a training and test set in a 70-30\% proportion respectively.    
Figure \ref{fig:fullwp} presents the results on our test set, where we used bins of a 0.05 probability range. 
In particular, as we can see the predicted probabilities match very well with the actual outcome of these instances.  
The fitted line has a slope of 1.007 (with a 95\% confidence interval of [0.994,1.019]), while the intercept is -0.005 (with a 95\% confidence interval [-0.013, 0.0014]), while the $R^2 = 0.99$.  
Simply put the line is for all practical purposes the $y=x$ line, which translates to a fairly consistent and accurate win probability. 

We have also calculated the accuracy of our binary predictions (i.e., without loss of generality the home team is projected to win if $\Pr(H=1| \mathbf{x})>0.5$) on the test set and is equal with 74\%, that is, our predictions are correct in 74\% of the instances.  
Another metric that has been traditionally used in the literature to evaluate the performance of a probabilistic prediction is the Brier score $\brier$ \cite{brier1950verification}.  
In the case of a binary probabilistic prediction the Brier score is calculated as: 

\begin{equation}
\brier = \dfrac{1}{N}\sum_{i=1}^N (\pi_i-y_i)^2
\label{eq:brier}
\end{equation}
where $N$ is the number of observations, $\pi_i$ is the probability assigned to instance $i$ being equal to 1 and $y_i$ is the actual (binary) value of instance $i$.  
Brier scores takes values between 0 and 1 and does not evaluate the accuracy of the predicted probabilities but rather the calibration of these probabilities, that is, the level of certainty they provide.  % (e.g., a 0.9 probability is more calibrated compared to a 0.55 probability).   
The lower the value of $\brier$ the better the model performs in terms of calibrated predictions. 
Our model exhibits a Brier score $\brier$ of 0.17.  
Typically the Brier score of a model is compared to a baseline value $\brier_{base}$ obtained from a {\em climatology} model \cite{mason2004using}. 
A climatology model assigns the same probability to every observation (that is, home team win in our case), which is equal to the fraction of positive labels in the whole dataset.  
Hence, in our case the climatology model assigns a probability of 0.57 to each observation, since 57\% of the instances in the dataset resulted to a home team win. 
The Brier score for this reference model is $\brier_{base}=0.26$, which is obviously of lower quality as compared to our model. 

Next we examine the performance of {\method} as a function of the time elapsed from the beginning of the game, and in particular we consider the quarter of the predicted instance.  
Table \ref{tab:results} presents the results for the accuracy, the Brier score as well as the probability accuracy of {\method}. 
The latter is reflected on the 95\% confidence interval of the slope and intercept of the corresponding line (see Figure \ref{fig:fullwp}). 
As we can see the performance of {\method} improves as the game progresses. 
For example, during the first quarter the accuracy with which the winner is predicted is close to the state-of-the-art prediction accuracy of pre-game win probability models \cite{kpele-plosone}. 
However, as the game progresses {\method} is able to adjust its predictions. 

\begin{table*}
\centering
\begin{tabular}{c||*{2}{c|}c|c}
 & \multicolumn{2}{|c|}{Probability line} & & \\ \hline \hline
Quarter & Slope & Intercept & Brier score & Accuracy \\ \hline
1 & [0.87,0.95] & [0.041,0.088] & 0.21 & 0.64 \\ \hline
2 & [0.89,0.97] & [0.035,0.071] & 0.18 & 0.71 \\ \hline
3 & [0.97,1.03] & [-0.01,0.05] & 0.14 & 0.77 \\ \hline
4 & [0.99,1.21] & [-0.11,0.04] & 0.11 & 0.84 \\ \hline
\end{tabular}
\caption{The performance of our model improves as the game progresses.}
\label{tab:results}
\end{table*}

\iffalse
Qtr1 --> Intercept: 0.01724886 0.08667382 Slope: 0.85147882 0.97192799 Brier: 0.21 (0.25)
Qtr2 --> Intercept: 0.03572466 0.09115063 Slope: 0.84114141 0.93719396 Brier: 0.18 (0.25)
Qtr3 --> Intercept: -0.01549244 0.05285255 Slope: 0.91602858 1.03443241 Brier: 0.14 (0.25)
Qtr4 --> Intercept: -0.1120115 -0.0448949 Slope:  0.9970946  1.2132215 Brier: 0.11 (0.25)
Accuracy --> 0.64 0.71 0.77 0.84
\fi

{\bf Anecdote {\em Evaluation}: }
As mentioned in the introduction, one of the motivating events for {\method} has been Super Bowl 51. 
Super Bowl 51 has been labeled as the biggest comeback in the history of Super Bowl.  
Late in the third quarter New England Patriots were trailing by 25 points and Pro Football Reference was giving Patriots a 1:1000 chance of winning the Super Bowl, while ESPN a 1:500 chance \cite{statsbylopez}. 
PFR considers this comeback a once in a millennium comeback. 
While this can still be the case\footnote{As mentioned earlier 1:$x$ chances does not mean that we observe $x$ {\em failures} first and then the one {\em success}.}, in retrospect Patriots' win highlights that these models might be too optimistic and confident to the team ahead.  
On the contrary, the lowest probability during the game assigned to the Patriots by {\method} for winning the Super Bowl was 2.1\% or about 1:50.  %, i.e., once every half century, which seems more reasonable.  
We would like to emphasize here that the above does not mean that {\method} is ``{\em better}'' than other win probability models.  
However, it is a simple, open model that assigns win probabilities in a conservative (i.e., avoids ``over-reacting''), yet accurate and well-calibrated way.  


\begin{figure}[t]
\begin{center}
\includegraphics[scale=0.4]{plots/sb51.png}%\vspacecap
 \caption{The lowest in-game win probability assigned to the Patriots by {\method} during Super Bowl 51 was 2.1\%, i.e., 1 in 50 chances. During the OT the model does not perform very accurately due to the sparsity of the relevant data.}
 \label{fig:sb51}
\end{center}
\end{figure}