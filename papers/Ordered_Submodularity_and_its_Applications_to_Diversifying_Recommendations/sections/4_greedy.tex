% \documentclass[../main.tex]{subfiles}

% \begin{document}
% !TEX root = main.tex

\section{Analysis of simple greedy algorithm}\label{sec:greedy_analysis}

The simple greedy algorithm for cardinality-constrained nonnegative ordered-submodular maximization works as follows: It initializes $A_0 = \emptyset$ (the empty sequence), and for $\ell = 1, 2, \dots, k$, it selects $A_\ell$ to be the sequence that maximizes $f(A)$ over all sequences obtained by appending an element to the end of $A_{\ell-1}$. In other words, it iteratively appends elements to the sequence $A$ one by one, each time choosing the element that leads to the greatest marginal increase in the value of $f$.

\begin{prop}\label{prop:2approx}
   The greedy algorithm for nonnegative ordered-submodular function maximization over sets of cardinality $k$ outputs a solution whose value is at least $\frac{1}{2}$ times that of the optimum solution.
\end{prop}
\begin{proof}
    Denote the sequence of length $k$ maximizing $f$ as $S = s_1 s_2 \dots s_k$ and the sequence of length $k$ maximizing the marginal increase at each step as $A = a_1 a_2 \dots a_k$. We write $S^j = s_j s_{j+1} \dots s_k$ to denote the suffix of $S$ starting at element $s_j$.
    
    Let $OPT(k) = f(S)$, $ALG(k) = f(A)$, so that we seek to show that $ALG(k) \ge \frac{1}{2} OPT(k)$ for all $k$. We must bound the performance of the greedy algorithm by comparing it to the optimal solution. The key insight is to ask the following question at each step: if we must remain committed to all the greedily chosen elements so far, but make the same choices as the optimum for the rest of the elements, how much have we lost? 
    
    To answer this question, we show via induction that for all $i$, $$f(A_i || S^{i+1}) \ge OPT(k) - f(A_i).$$ 
    
    The base case of $i=0$ is trivial, as $f(A_0 || S^1) = f(S) = OPT(k) \ge OPT(k) - f(A_0)$. So suppose the claim is true for some $i$, and observe that by ordered submodularity we have 
    \begin{align*}
        f(A_i || s_{i+1}) - f(A_i) &\ge f(A_i || s_{i+1} || S^{i+2}) - f(A_i || a_{i+1} || S^{i+2}) \\
        &= f(A_i || S^{i+1}) - f(A_{i+1} || S^{i+2}), \\
        f(A_{i+1} || S^{i+2}) &\ge f(A_i || S^{i+1}) + f(A_i) - f(A_i || s_{i+1}).
    \end{align*}
    
    Applying first the induction hypothesis, then the fact that $f(A_{i+1}) \ge f(A_i || s_{i+1})$ by definition of the greedy algorithm, yields
    \begin{align*}
        f(A_{i+1} || S^{i+2}) &\ge (OPT(k) - f(A_i)) + f(A_i) - f(A_i || s_{i+1}) \\
        &= OPT(k) - f(A_i || s_{i+1}) \\
        &\ge OPT(k) - f(A_{i+1}),
    \end{align*}
    completing the induction.
    
    Finally, taking $i=k$ in the claim gives 
    $$f(A) \ge OPT(k) - f(A) \implies f(A) = ALG(k) \ge \frac{1}{2} OPT(k).$$
\end{proof}

% \end{document}