% !TEX root = main.tex
% \documentclass[../main.tex]{subfiles}

% \begin{document}
\section{Appendix}

\subsection{Greedy algorithm on variants of the KL divergence} \label{sec:bad_netflix}
A natural hope might be to use the KL divergence as a calibration heuristic, as it is perhaps the most commonly used statistical divergence. Unfortunately, the KL divergence cannot be used directly because it is unbounded; our translation to the distance-based overlap measure is also not well-defined on the KL divergence for the same reason. In~\cite{Steck18} an alternative transformation is proposed, yielding the following calibration heuristic: 
$$f(\mathcal{I}) = \sum_g p(g|u) \log \sum_{i \in \mathcal{I}} w_{r(i)} \tilde{q} (g|i).$$
However, this objective function has inconsistent sign, depending on how the recommendation weights are chosen (and we note that Steck does not set any constraints on the weights), and consequently the greedy choice can be far from optimal. In fact, we show that the greedy solution can be negative, while the optimum is positive. So the KL divergence (and variants of it) are not conducive to multiplicative approximation guarantees for the calibration problem.

Suppose there are $4$ genres ($g_k$ for $k=1,2,3,4$), $2$ movies ($i_\ell$ for $\ell=1,2$), and $1$ user ($u$), and that we seek a recommendation list of length $2$ with weights $w_1 > w_2 = 1$. For simplicity of notation, we denote $p(g_k|u)$ as $p_k$. Suppose further that the movies have the following distributions over genres for some $\varepsilon \in (0,\frac{1}{3})$:

\begin{center}
\begin{tabular}{ p{4cm} p{4cm} }
$\tilde{q}(g_1 | i_1) = \frac{1}{2}(1-\varepsilon)$ & $\tilde{q}(g_1 | i_2) = \frac{1}{2}(1-\varepsilon)$ \\[6pt]
$\tilde{q}(g_2 | i_1) = \frac{1}{4}(1-\varepsilon)$ & $\tilde{q}(g_2 | i_2) = \frac{1}{2}(1-\varepsilon)$ \\[6pt]
$\tilde{q}(g_3 | i_1) = \frac{1}{4}(1-\varepsilon)$ & $\tilde{q}(g_3 | i_2) = \frac{\varepsilon}{2}$ \\[6pt]
$\tilde{q}(g_4 | i_1) = \varepsilon$ & $\tilde{q}(g_4 | i_2) = \frac{\varepsilon}{2}$ 
\end{tabular}
\end{center}

Finally, suppose the parameters are such that $$p_3 \log\left(\frac{1-\varepsilon}{2\varepsilon} \right) = (p_2-p_4) \log \left(\frac{2w_1 + 1}{w_1+2} \right).$$

Then, observe that 
\begin{align*}
    f(i_1 i_2) - f(i_2 i_1) &= p_2 \log \left( \frac{\frac{w_1}{4}(1-\varepsilon) + \frac{1}{2}(1-\varepsilon)}{\frac{w_1}{2}(1-\varepsilon) + \frac{1}{4}(1-\varepsilon)} \right) + p_3 \log\left( \frac{\frac{w_1}{4}(1-\varepsilon) + \frac{\varepsilon}{2}}{\frac{w_1\varepsilon}{2} + \frac{1}{4}(1-\varepsilon)} \right) + p_4 \log \left(\frac{w_1\varepsilon + \frac{\varepsilon}{2}}{\frac{w_1\varepsilon}{2} + \varepsilon} \right) \\
    &= p_2 \log\left(\frac{w_1 + 2}{2w_1 + 1} \right) + p_3 \left( \frac{w_1(1-\varepsilon) + 2\varepsilon}{2w_1\varepsilon + 1-\varepsilon} \right) + p_4 \log \left( \frac{2w_1 + 1}{w_1 + 2} \right) \\
    &= p_3 \left( \frac{w_1(1-\varepsilon) + 2\varepsilon}{2w_1\varepsilon + 1-\varepsilon} \right) + (p_4-p_2) \log \left( \frac{2w_1 + 1}{w_1 + 2} \right).
\end{align*}
We can verify that for $\varepsilon < \frac{1}{3}$, we have $\frac{w_1(1-\varepsilon) + 2\varepsilon}{2w_1\varepsilon + 1-\varepsilon} < \frac{1-\varepsilon}{2\varepsilon}$, thus 
\begin{align*}
    f(i_1 i_2) - f(i_2 i_1) &< p_3 \left( \frac{1-\varepsilon}{2\varepsilon}\right) + (p_4-p_2) \log \left( \frac{2w_1 + 1}{w_1 + 2} \right) = 0\\
    \implies f(i_1 i_2) &< f(i_2 i_1).
\end{align*}
That is, the optimal recommendation list ranks $i_2$ first, then $i_1$ second.

However, we also have
\begin{align*}
    f(i_1) - f(i_2) &= p_2 \log \left( \frac{\frac{w_1}{4}(1-\varepsilon)}{\frac{w_1}{2}(1-\varepsilon) } \right) + p_3 \log\left( \frac{\frac{w_1}{4}(1-\varepsilon)}{\frac{w_1\varepsilon}{2}} \right) + p_4 \log \left(\frac{w_1\varepsilon}{\frac{w_1\varepsilon}{2}} \right) \\
    &= p_2 \log\left(\frac{1}{2} \right) + p_3 \left( \frac{1-\varepsilon}{2\varepsilon} \right) + p_4 \log \left(2\right) \\
    &= p_3 \left( \frac{1-\varepsilon}{2\varepsilon} \right) + (p_4-p_2) \log \left(2 \right).
\end{align*}
Since $w_1 > 1$, we have $\frac{2w_1 + 1}{w_1+2} < 2$, thus
\begin{align*}
    f(i_1) - f(i_2) &> p_3 \left( \frac{1-\varepsilon}{2\varepsilon}\right) + (p_4-p_2) \log \left( \frac{2w_1 + 1}{w_1 + 2} \right) = 0\\
    \implies f(i_1) &> f(i_2).
\end{align*}
That is, the greedy algorithm will first choose $i_1$ instead of $i_2$, thereby constructing a suboptimal list.

Now, we compute $ALG = f(i_1i_2)$ and $OPT = f(i_2i_1)$ for the following set of parameters: $p_1 = 0.05, p_2 = 0.9, p_3=p_4 = 0.025$, $\varepsilon = 10^{-10}$, varying $w_1 > 1$. 
\begin{center}
\begin{tabular}{ c r r }
$w_1$ & $ALG$ & $OPT$ \\[3pt] \hline\\[-6pt]
1.1 & -0.823134 & -0.797737 \\[3pt]
1.5 & -0.691859 & -0.585156 \\[3pt]
2 & -0.549794 & -0.371873 \\[3pt]
3.5 & -0.201250 & 0.114023 \\[3pt]
5 & 0.0311358 & 0.386387 \\[3pt]
10 & 0.580034 & 1.01213 \\[3pt]
100 & 2.73099 & 3.20940 
\end{tabular}
\end{center}

We now observe that the function does not have consistent sign; $ALG$ and $OPT$ are negative for lower values of $w_1$ and positive for higher values of $w_1$. This is because the $\tilde{q}(g|i)$'s represent a probability distribution and are thus less than $1$, so when the weights are small we take the logarithm of a number less than $1$, so the function is negative; when the weights are sufficiently large, then the inner summand exceeds $1$ and the function becomes positive.

It is unclear how we should think about approximation when the value of a function is not always positive or negative --- for instance, the approximation ratio $ALG/OPT$ is meaningless, especially considering that $ALG$ and $OPT$ may have opposite signs (such as when $w_1 = 3.5$). So if the simple greedy algorithm is not always optimal, but we have no consistent way of comparing its performance with the optimal solution, then it becomes very difficult to understand the maximization (or approximate maximization) of this specific form of the calibration heuristic.


\subsection{Varying sequential dependencies in calibration}\label{sec:calibration_seqdep}

In Section~\ref{sec:relatedwork}, we described earlier formalisms of sequential submodularity that rely on postfix monotonicity and argued that many natural ordering problems, including the coverage objective function, are not postfix monotone. A different line of papers encodes sequences using DAGs and hypergraphs. Now, we show that this formalism also does not capture the rank-based sequential dependencies that we desire.

We present a simple instance of the calibration problem which hints at the potential intricacies of sequential dependencies. Suppose there are just $2$ genres ($g_1$ and $g_2$), $4$ movies ($i_1$, $i_2$, $i_3$, $i_4$), and $1$ user ($u$). Say that the target distribution is $p(g_1 | u) = p(g_2 | u) = 0.5$, and the weights of the recommended items are $w_1 = 0.5, w_2 = 0.3, w_3 = 0.2$. Suppose further that the movies have genre distributions as follows:

\begin{center}
\begin{tabular}{ p{3cm} p{3cm} }
$p(g_1 | i_1) = 0.4$, & $p(g_2 | i_1) = 0.6$ \\[6pt]
$p(g_1 | i_2) = 0.8$, & $p(g_2 | i_2) = 0.2$ \\[6pt]
$p(g_1 | i_3) = 1$, & $p(g_2 | i_3) = 0$ \\[6pt]
$p(g_1 | i_4) = 0$, & $p(g_2 | i_4) = 1$.
\end{tabular}
\end{center}

Our heuristic for measuring calibration is the overlap measure $G(p,q) = \sum_g \sqrt{p(g|u) \cdot q(g|u)}$. We now consider a few different recommended lists as input to the overlap measure:
\begin{align*}
    f(i_3 i_1 i_2) = G(p, (0.78,0.22)) \approx 0.956 \\
    f(i_3 i_2 i_1) = G(p, (0.82,0.18)) \approx 0.940 \\
    f(i_4 i_1 i_2) = G(p, (0.28,0.72)) \approx 0.974 \\
    f(i_4 i_2 i_1) = G(p, (0.32,0.68)) \approx 0.983 
\end{align*}

Here, we see that $f(i_3 i_1 i_2) > f(i_3 i_2 i_1)$, but $f(i_4 i_1 i_2) < f(i_4 i_2 i_1)$. So it is not always inherently better to rank $i_1$ before $i_2$ or $i_2$ before $i_1$; the optimal ordering is dependent on the context of the rest of the recommended list. Thus this very natural problem setting cannot be satisfactorily encoded by the DAG or hypergraph models of~\cite{Tschiatschek2017} and~\cite{Mitrovic18}, providing further motivation for our framework of ordered submodularity. 
%\end{document}