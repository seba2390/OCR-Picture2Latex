% \documentclass[../main.tex]{subfiles}

% \begin{document}
% !TEX root = main.tex

\section{Application 2: Calibration in personalized recommendations}\label{sec:calibration}

We now consider the setting of \emph{personalized} recommendations, which generates a tailored list of recommendations for each individual user based on their historical preferences. Much research on personalized recommender systems has worked toward improving prediction accuracy (e.g., how many of the recommended items are indeed relevant to the user), but training solely toward accuracy metrics can actually be detrimental to the performance of the system. For instance, recommendation lists focused only on accuracy may suffer from a lack of diversity or novelty~\cite{McNee2006}. Another important metric in machine learning is \emph{calibration}, the degree to which the predicted proportions of the various classes align with the true proportions of the classes in the existing data. From the user's perspective, a recommendation list is calibrated if it closely reflects their various interests in appropriate proportions. This a desirable additional objective when optimizing the user experience; for instance, a user would likely want the system to preserve their minor interests, rather than entirely ``crowding them out'' in favor of major interests only.

Steck (2018) considers the problem of creating calibrated recommendations using the language of \emph{movies} as the items with which users interact, and \emph{genres} as the classes of items~\cite{Steck18}. Each user has a preference distribution over genres that can be inferred from their previous activity, and the goal is to recommend a list of movies whose genres reflect these preferences (possibly also incorporating a ``quality'' score for each movie, representing its general utility or relevance). In our work, we adopt Steck's formulation of distributions over genres, which we describe below.

Suppose that each movie $i$ has a distribution over genres $g$, given by $p(g|i)$. For a user $u$, we consider two induced distributions: one from the list of movies $\mathcal{H}$ that user $u$ has played in the past, and one from the list of movies $\mathcal{I}$ that the system recommends to user $u$:
\begin{itemize}
    \item $p(g|u)$, the distribution over genres $g$ played by user $u$ in the past: $$p(g|u) = \frac{\sum_{i\in \mathcal{H}} w_{u,i} \cdot p(g|i)}{\sum_{i\in \mathcal{H}} w_{u,i}},$$ where $w_{u,i}$ is the weight of movie $i$ (e.g., how recently it was played by user $u$),
    
    \item $q(g|u)$, the distribution over genres $g$ recommended to user $u$: $$q(g|u) = \frac{\sum_{i\in \mathcal{I}} w_{r(i)} \cdot p(g|i)}{\sum_{i\in \mathcal{I}} w_{r(i)}},$$ where $w_{r(i)}$ is the weight of movie $i$ due to its rank $r(i)$.
\end{itemize}

In general, Steck does not provide much guidance on how the weights are intended to be chosen and interpreted in the context of the greedy algorithm. For our purposes, we suppose the weights are weakly decreasing in rank (i.e., $w_a \ge w_b$ if $a < b$). We also suppose that the desired length of the recommendation list is a fixed constant $k$ (i.e., $|\mathcal{I}| = k$) and $\sum_{j=1}^k w_i = 1$. This assumption is without loss, even with the more typical cardinality constraint that the list may have length \emph{at most} $k$ --- we simply linearly consider each possible length $\ell \in [1,k]$, renormalize so that the first $\ell$ weights sum to $1$, and perform the optimization. We then take the maximally calibrated list over all $k$ length-optimal lists. 

The goal of the \emph{calibrated recommendations} problem is to choose $\mathcal{I}$ such that $q$ is ``close'' to $p$. To quantify this concept of closeness between distributions, we introduce the formalism of \emph{overlap measures}.

\subsection{Overlap measures}
For the discussion that follows, we restrict to finite discrete probability spaces $\Omega$ for simplicity, although the concepts can be generalized to continuous probability measures.

A common tool for quantitatively comparing distributions is statistical divergences, which measure the ``distance'' from one distribution to another. A divergence $D$ has the property that $D(p,q) \ge 0$ for any two distributions $p,q$, with equality attained if and only if $p=q$. This means that divergences cannot directly be used to measure calibration, which we think of as a non-negative metric that is uniquely \emph{maximized} when $p=q$. Instead, we define a new but closely related tool that we call \emph{overlap}, which exactly satisfies the desired properties. 

Since divergences have a number of well-studied properties and applications, it is useful to consider overlap measures derived from divergences. We note that~\cite{Steck18} does a version of this, modifying the KL divergence into a \emph{maximum marginal relevance} objective function. However, this proposed objective function may be either positive or negative (see Appendix~\ref{sec:bad_netflix} for an example), meaning that it cannot be used in our greedy algorithm---and in fact, the concept of approximation guarantees is not even well-specified for functions of variable sign. In contrast, our abstraction of overlap measures satisfies non-negativity for all pairs of distributions $p,q$.

Our definition is also more general in two important ways. First, we do not limit ourselves to the KL divergence, so that other divergences and distances with useful properties may be used (one such example is the Hellinger distance, $H(p,q) = \frac{1}{\sqrt{2}} ||\sqrt{p} - \sqrt{q} ||_2$, which forms a bounded metric and has a convenient geometric interpretation using Euclidean distance). Second, in our definition $q$ may be any \emph{subdistribution}, a term we use to denote a vector of probabilities summing to \emph{at most} $1$. This is useful because our greedy algorithm incrementally constructs $q$ from the $0$ vector by adding a new movie (weighted by its rank), so in each iteration we must compute the overlap between the true distribution $p$ and the partially constructed subdistribution $q$.

With these considerations in mind, we now proceed to define overlap measures.

\begin{defn}[Overlap measure]
An \textbf{overlap measure} $G$ is a function on pairs of distributions and subdistributions $(p,q)$ with the properties that
\begin{enumerate}[(i)]
    \item $G(p,q) \ge 0$ for all distributions $p$ and subdistributions $q$,
    \item For any fixed $p$, $G(p,q)$ is uniquely maximized at $q=p$.
\end{enumerate}
\end{defn}

Further, we observe that overlap measures can be constructed based on distance functions.

\begin{defn}[Distance-based overlap measure]
Let $d(p,q)$ be a bounded distance function on the space of distributions $p$ and subdistributions $q$ with the property that $d(p,q) \ge 0$ with $d(p,q) = 0$ if and only if $p=q$. Denote by $d^*$ the maximum value attained by $d$ over all pairs $(p,q)$.

Then, the \textbf{$d$-overlap measure} $G_d$ is defined as $$G_d(p,q) \coloneqq d^* - d(p,q).$$
\end{defn}

Many classical distances are originally defined on pairs of distributions $(p,q)$, but admit explicit functional forms that can be evaluated using the values of $p(x)$ and $q(x)$ for all $x \in \Omega$. This allows us to compute $d(p,q)$, and consequently $G_d(p,q)$, when $q$ is a subdistribution. Now, it is clear that $G_d$ indeed satisfies both properties of an overlap measure: property (i) follows from the definition of $d^*$, and property (ii) follows from the unique minimization of $d$ at $q=p$.

\subsection{Families of ordered-submodular overlap measures}
Any overlap measure produces a version of the calibrated recommendations problem (and the corresponding approximation problem), since for the user's target distribution $p(g|u)$, we seek to calibrate the recommended genre distribution $q(g|u)$ to equal $p(g|u)$, which maximizes the overlap $G(p,q)$. But to execute our greedy algorithm, we are interested in overlap measures that give rise to an ordered-submodular optimization problem in particular. As discussed in Section~\ref{sec:ordsub_defn}, it suffices to study conditions under which the resulting calibration heuristic exhibits diminishing marginal returns and monotonicity (with respect to filling in any position in the sequence that is currently the formal ``null'' with a new item). 

As before, we are also interested in divergence-based overlap measures. One of the main classes of divergences is the family of $f$-divergences, which are generated from functions $f(t)$ that are convex on $t \ge 0$ with $f(1) = 0$. Given such a function $f$, the $f$-divergence of $p$ from $q$ (alternatively, ``from $q$ to $p$'') is defined as $$D_f(p,q) \coloneqq \sum_{x\in \Omega} f\left( \frac{p(x)}{q(x)}\right) q(x).$$ We show that in general, $f$-divergences yield ordered-submodular problems to which we can apply our greedy algorithm.

Recall that $p$ is a given as fixed, while $q$ is constructed incrementally as $q(g|u) = \sum_{i\in \mathcal{I}} w_{r(i)} \cdot p(g|i)$. For any genre $g$, since the $p(g|i)$ are non-negative probabilities, adding a movie $i$ to the sequence always (weakly) increases $q(g|u)$. Then, in the execution of the greedy algorithm, adding $i$ later in the sequence adds onto a larger accumulated value of $q(g|u)$. If $g(y) \coloneqq f\left(\frac{c}{y}\right) y$ is convex for $y > 0$, then $D_f(p,q) = \sum_{y = q(x)} g(y)$ displays ``increasing marginal returns'' with respect to the sequence of movies, and $G_{D_f}(p,q)$ will display decreasing marginal returns.

Indeed, we can verify that we have 
\begin{align*}
    g'(y) &= -f'\left(\frac{c}{y}\right) \cdot \frac{c}{y^2} \cdot y + f\left(\frac{c}{y}\right) \cdot 1 = f\left(\frac{c}{y}\right) - f'\left(\frac{c}{y}\right) \frac{c}{y}, \\
    g''(y) &= -f''\left(\frac{c}{y}\right) \cdot \frac{c}{y^2} + f''\left(\frac{c}{y}\right) \cdot \frac{c}{y^2} \cdot \frac{c}{y} + f'\left(\frac{c}{y}\right) \cdot \frac{c}{y^2} \\
    &= f''\left(\frac{c}{y}\right) \frac{c}{y^3} \\
    &\ge 0,
\end{align*}
since $c,y, f''\left(\frac{c}{y}\right)  \ge 0$ (by definition of convexity of $f$). Consequently, any \emph{bounded} $f$-divergence results in a $D_f$-overlap measure with the diminishing returns property.

Further, we note that many $D_f$-overlap measures based on commonly used $f$-divergences satisfy the monotonicity property. As a concrete example, consider the squared Hellinger distance (obtained by choosing $f(t) = (\sqrt{t}-1)^2$ or $f(t) = 2(1-\sqrt{t})$), which is of the form $$H^2(p,q) = \frac{1}{2} \sum_{x\in \Omega} (\sqrt{p(x)} - \sqrt{q(x)})^2 = 1 - \sum_{x\in \Omega} \sqrt{p(x) \cdot q(x)}.$$ The resulting $H^2$-overlap measure is $$G_{H^2} (p,q) = \sum_{x\in \Omega} \sqrt{p(x) \cdot q(x)}.$$ Clearly, if subdistribution $q$ coordinate-wise dominates subdistribution $q'$, then $G_{H^2}(p,q) \ge G_{H^2}(p,q')$ for all $p$, which establishes the monotonicity property.

Taking these two desired properties together, we have demonstrated that our notion of overlap measures works well with $f$-divergences to create a general family of ordered-submodular calibration problems.

\begin{thm} \label{thm:overlap_fdiv}
Given any bounded $f$-divergence $D_f$ with maximum value $d^* = \max_{(p,q)} d(p,q)$, the corresponding $D_f$-overlap measure $$G_{D_f}(p,q) = d^* - D_f(p,q)$$ is ordered-submodular. Thus, the greedy algorithm provides a $2$-approximation for calibration heuristics using overlap measures of this form.
\end{thm}

Inspired by the squared Hellinger-based overlap measure, we also consider the construction of another general family of overlap measures of the form $$G(p,q) = \sum_{x\in \Omega} g_1(p(x)) \cdot g_2(q(x)),$$ for nonnegative functions $g_1$ and $g_2$. Here, $G$ is ordered-submodular with respect to the recommendation list as long as $g_2$ is any non-decreasing (corresponding to monotonicity) and concave (corresponding to diminishing returns) function. Given such a $g_2$, we fully specify the overlap measure by choosing $g_1$ such that $G$ is uniquely maximized when $q=p$.

That is, we consider the constrained maximization of $\sum_{i=1}^g g_1(p_i) \cdot g_2 (q_i)$, subject to $\sum_{i=1}^g q_i \le 1$. By placing a Lagrange multiplier of $\lambda$ on the constraint, we see that the maximum occurs when $$g_1 (p_i) \cdot g_2' (q_i) = \lambda$$ for all $i$. Since we would like this to be satisfied if $q_i = p_i$ for all $i$, and we can scale the overlap measure by a multiplicative constant without loss, it suffices to set $g_1$ identically to $\frac{1}{g_2'}$. This gives rise to a second family of ordered-submodular overlap measures that includes a wide range of general forms, and also remains easy to compute. 

\begin{thm} \label{thm:overlap_g1g2}
Given any nonnegative non-decreasing concave function $f$, the overlap measure $$G(p,q) = \sum_{x\in \Omega} \frac{f(q(x))}{f'(p(x))}$$ is ordered-submodular. Thus, the greedy algorithm provides a $2$-approximation for calibration heuristics using overlap measures of this form.
\end{thm}

As a concrete example, taking $f(x) = x^\alpha$ for $\alpha \in (0,1)$ gives $\frac{1}{f'(x)} = \alpha x^{1-\alpha}$, which produces the (scaled) overlap measure $$G(p,q) = \sum_{x\in \Omega} p(x)^{1-\alpha} q(x)^\alpha.$$ 
Observe that the natural special case of $\alpha = \frac{1}{2}$ gives $f(x) = \frac{1}{f'(x)} = \sqrt{x}$, providing an alternate construction that recovers the squared Hellinger-based overlap measure.

Finally, we note that until this point our discussion has focused only on calibration, but in practice the recommendations should also have high quality (utility, relevance, etc.). To address this, we can model the overall quality of a list as the sum of quality scores of the individual movies (as in~\cite{ashkan2015, Steck18}), and then optimize for a weighted sum of quality and calibration. As a straightforward sum of scores, the quality metric is modular (and hence ordered-submodular). Then by Lemma~\ref{lem:weighted_submodular} the combination of quality and calibration remains ordered-submodular, and our approximation results still hold.

\begin{cor}
The calibrated recommendations problem with combined quality and calibration metrics is ordered-submodular, and thus admits a 2-approximation via the greedy algorithm.
\end{cor}

\paragraph{Remark.} Since we have established in Theorems~\ref{thm:overlap_fdiv} and \ref{thm:overlap_g1g2} that the calibration heuristic can be formulated as ordered-submodular, our result in Proposition~\ref{prop:2approx} implies that the greedy algorithm gives us a 2-approximation to the calibrated recommendations problem. But unlike in Section~\ref{sec:calibration}, where we show that this factor of $2$ is tight for the coverage problem, we do not have a corresponding tight instance for calibration. \etcomment{I would have said "we do not have" (not just do not present} \ercomment{changed present -> have} In simulation, the greedy algorithm performs nearly optimally on most instances of the calibration problem; however, we leave stronger theoretical guarantees as an open problem.


% \end{document}