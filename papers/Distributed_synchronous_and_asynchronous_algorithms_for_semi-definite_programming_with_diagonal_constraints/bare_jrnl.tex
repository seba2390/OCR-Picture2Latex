
\documentclass[journal]{IEEEtran}

\pdfoutput=1 
\usepackage{amssymb,epsfig,color,cite,amsmath,amsfonts,mathrsfs,algorithm,algorithmicx}
\usepackage{epstopdf}
\usepackage{datetime,fancyhdr}
\usepackage{algpseudocode}
\usepackage{arydshln}
\usepackage{multirow}
\usepackage{float}
%\renewcommand{\algorithmicrequire}{\textbf{Input:}}  % Use Input in the format of Algorithm
%\renewcommand{\algorithmicensure}{\textbf{Output:}} % Use Output in the format of Algorithm

%\SetKwFor{For}{for}{do}{endfor}

\usepackage{times} % assumes new font selection scheme installed
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{latexsym}
\usepackage{subfigure}

\newtheorem{lemma}{Lemma}[section]
\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{remark}{Remark}[section]
\newtheorem{definition}{Definition}[section]
\newtheorem{example}{Example}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{assumption}{Assumption}[section]

\newcommand{\upcite}[1]{\textsuperscript{\textsuperscript{\cite{#1}}}}
% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/pkg/ifpdf
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.






% *** CITATION PACKAGES ***
%
%\usepackage{cite}
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of the IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off
% such as if a citation ever needs to be enclosed in parenthesis.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 5.0 (2009-03-20) and later if using hyperref.sty.
% The latest version can be obtained at:
% http://www.ctan.org/pkg/cite
% The documentation is contained in the cite.sty file itself.






% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  % \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation
% can be obtained at:
% http://www.ctan.org/pkg/graphicx
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found at:
% http://www.ctan.org/pkg/epslatex
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). The IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex





% *** MATH PACKAGES ***
%
%\usepackage{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics.
%
% Note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/amsmath





% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as the IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/pkg/algorithms
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/pkg/algorithmicx




% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/array


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.




% *** SUBFIGURE PACKAGES ***
%\ifCLASSOPTIONcompsoc
%  \usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
%\else
%  \usepackage[caption=false,font=footnotesize]{subfig}
%\fi
% subfig.sty, written by Steven Douglas Cochran, is the modern replacement
% for subfigure.sty, the latter of which is no longer maintained and is
% incompatible with some LaTeX packages including fixltx2e. However,
% subfig.sty requires and automatically loads Axel Sommerfeldt's caption.sty
% which will override IEEEtran.cls' handling of captions and this will result
% in non-IEEE style figure/table captions. To prevent this problem, be sure
% and invoke subfig.sty's "caption=false" package option (available since
% subfig.sty version 1.3, 2005/06/28) as this is will preserve IEEEtran.cls
% handling of captions.
% Note that the Computer Society format requires a larger sans serif font
% than the serif footnote size font used in traditional IEEE formatting
% and thus the need to invoke different subfig.sty package options depending
% on whether compsoc mode has been enabled.
%
% The latest version and documentation of subfig.sty can be obtained at:
% http://www.ctan.org/pkg/subfig




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure.
% Be aware that LaTeX2e kernels dated 2015 and later have fixltx2e.sty's
% corrections already built into the system in which case a warning will
% be issued if an attempt is made to load fixltx2e.sty as it is no longer
% needed.
% The latest version and documentation can be found at:
% http://www.ctan.org/pkg/fixltx2e


%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/stfloats
% Do not use the stfloats baselinefloat ability as the IEEE does not allow
% \baselineskip to stretch. Authors submitting work to the IEEE should note
% that the IEEE rarely uses double column equations and that authors should try
% to avoid such use. Do not be tempted to use the cuted.sty or midfloat.sty
% packages (also by Sigitas Tolusis) as the IEEE does not format its papers in
% such ways.
% Do not attempt to use stfloats with fixltx2e as they are incompatible.
% Instead, use Morten Hogholm'a dblfloatfix which combines the features
% of both fixltx2e and stfloats:
%
% \usepackage{dblfloatfix}
% The latest version can be found at:
% http://www.ctan.org/pkg/dblfloatfix




%\ifCLASSOPTIONcaptionsoff
%  \usepackage[nomarkers]{endfloat}
% \let\MYoriglatexcaption\caption
% \renewcommand{\caption}[2][\relax]{\MYoriglatexcaption[#2]{#2}}
%\fi
% endfloat.sty was written by James Darrell McCauley, Jeff Goldberg and
% Axel Sommerfeldt. This package may be useful when used in conjunction with
% IEEEtran.cls'  captionsoff option. Some IEEE journals/societies require that
% submissions have lists of figures/tables at the end of the paper and that
% figures/tables without any captions are placed on a page by themselves at
% the end of the document. If needed, the draftcls IEEEtran class option or
% \CLASSINPUTbaselinestretch interface can be used to increase the line
% spacing as well. Be sure and use the nomarkers option of endfloat to
% prevent endfloat from "marking" where the figures would have been placed
% in the text. The two hack lines of code above are a slight modification of
% that suggested by in the endfloat docs (section 8.4.1) to ensure that
% the full captions always appear in the list of figures/tables - even if
% the user used the short optional argument of \caption[]{}.
% IEEE papers do not typically make use of \caption[]'s optional argument,
% so this should not be an issue. A similar trick can be used to disable
% captions of packages such as subfig.sty that lack options to turn off
% the subcaptions:
% For subfig.sty:
% \let\MYorigsubfloat\subfloat
% \renewcommand{\subfloat}[2][\relax]{\MYorigsubfloat[]{#2}}
% However, the above trick will not work if both optional arguments of
% the \subfloat command are used. Furthermore, there needs to be a
% description of each subfigure *somewhere* and endfloat does not add
% subfigure captions to its list of figures. Thus, the best approach is to
% avoid the use of subfigure captions (many IEEE journals avoid them anyway)
% and instead reference/explain all the subfigures within the main caption.
% The latest version of endfloat.sty and its documentation can obtained at:
% http://www.ctan.org/pkg/endfloat
%
% The IEEEtran \ifCLASSOPTIONcaptionsoff conditional can also be used
% later in the document, say, to conditionally put the References on a
% page by themselves.




% *** PDF, URL AND HYPERLINK PACKAGES ***
%
%\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/url
% Basically, \url{my_url_here}.




% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
%
% paper title
% Titles are generally capitalized except for words such as a, an, and, as,
% at, but, by, for, in, nor, of, on, or, the, to and up, which are usually
% not capitalized unless they are the first or last word of the title.
% Linebreaks \\ can be used within to get better formatting as desired.
% Do not put math or special symbols in the title.
\title{Distributed synchronous and asynchronous algorithms for semi-definite programming with diagonal constraints}
%
%
% author names and IEEE memberships
% note positions of commas and nonbreaking spaces ( ~ ) LaTeX will not break
% a structure at a ~ so this keeps an author's name from being broken across
% two lines.
% use \thanks{} to gain access to the first footnote area
% a separate \thanks must be used for each paragraph as LaTeX2e's \thanks
% was not built to handle multiple paragraphs
%

\author{Xia Jiang,
        Xianlin~Zeng,~\IEEEmembership{Member,~IEEE,}
        Jian~Sun,~\IEEEmembership{Member,~IEEE,}
        and~Jie~Chen,~\IEEEmembership{Fellow,~IEEE}% <-this % stops a space
\thanks{This work was supported in part by the National Natural Science Foundation of China (Nos. 61720106011, U1613225, 62073035, 61925303), Program for Changjiang Scholars and Innovative Research Team in University (IRT1208), the Youth ChangJiang Scholars Program, the National Key Research and Development Program of China under Grant 2018YFB1700100 and Beijing Institute of Technology Research Fund Program for Young Scholars. \emph{(Corresponding author: Jian Sun.)}}
\thanks{X. Jiang (jiangxia@bit.edu.cn) and J. Sun (sunjian@bit.edu.cn) are with Key Laboratory of Intelligent Control and Decision of Complex Systems, School of Automation, Beijing Institute of Technology, Beijing, 100081, China, and also with the Beijing Institute of Technology Chongqing Innovation Center, Chongqing  401120, China}
\thanks{X. Zeng (xianlin.zeng@bit.edu.cn) is with Key Laboratory of Intelligent Control and Decision of Complex Systems, School of Automation, Beijing Institute of Technology, Beijing, 100081, China}
\thanks{J. Chen (chenjie@bit.edu.cn) is with Beijing Advanced Innovation Center for Intelligent Robots and Systems (Beijing Institute of Technology), Key Laboratory of Biomimetic Robots and Systems (Beijing Institute of Technology), Ministry of Education, Beijing, 100081, China, and also with the School of Electronic and Information Engineering, Tongji University, Shanghai, 200082, China}}
% note the % following the last \IEEEmembership and also \thanks -
% these prevent an unwanted space from occurring between the last author name
% and the end of the author line. i.e., if you had this:
%
% \author{....lastname \thanks{...} \thanks{...} }
%                     ^------------^------------^----Do not want these spaces!
%
% a space would be appended to the last name and could cause every name on that
% line to be shifted left slightly. This is one of those "LaTeX things". For
% instance, "\textbf{A} \textbf{B}" will typeset as "A B" not "AB". To get
% "AB" then you have to do: "\textbf{A}\textbf{B}"
% \thanks is no different in this regard, so shield the last } of each \thanks
% that ends a line with a % and do not let a space in before the next \thanks.
% Spaces after \IEEEmembership other than the last one are OK (and needed) as
% you are supposed to have spaces between the names. For what it is worth,
% this is a minor point as most people would not even notice if the said evil
% space somehow managed to creep in.



% The paper headers
%\markboth{Journal of \LaTeX\ Class Files,~Vol.~14, No.~8, August~2015}%
%{Shell \MakeLowercase{\textit{et al.}}: Bare Demo of IEEEtran.cls for IEEE Journals}
% The only time the second header will appear is for the odd numbered pages
% after the title page when using the twoside option.
%
% *** Note that you probably will NOT want to include the author's ***
% *** name in the headers of peer review papers.                   ***
% You can use \ifCLASSOPTIONpeerreview for conditional compilation here if
% you desire.




% If you want to put a publisher's ID mark on the page you can do it like
% this:
%\IEEEpubid{0000--0000/00\$00.00~\copyright~2015 IEEE}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.

\markboth{IEEE Transactions on Automatic Control,~\today~\currenttime}
{\MakeLowercase{\textit{et al.}}: Distribute method}

% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}




% make the title area
\maketitle

% As a general rule, do not put math, special symbols or citations
% in the abstract or keywords.
\begin{abstract}
This paper develops distributed synchronous and asynchronous algorithms for the large-scale semi-definite programming with diagonal constraints, which has wide applications in combination optimization, image processing and community detection. The information of the semi-definite programming is allocated to multiple interconnected agents such that  each agent  aims to find a solution  by communicating to its neighbors. Based on low-rank  property of solutions and  the Burer-Monteiro factorization,  we  transform the original  problem into a distributed  optimization problem  over unit spheres to reduce variable dimensions and ensure positive semi-definiteness without involving semi-definite projections, which are computationally expensive. 
For  the distributed optimization problem, we propose distributed synchronous and asynchronous algorithms, both of which  reduce computational burden and storage space compared with existing centralized algorithms. 
Specifically, the  distributed synchronous algorithm almost surely  escapes strict saddle points and converges to the set of optimal solutions  to the optimization problem. In addition, the proposed distributed asynchronous algorithm allows communication delays and converges to the set of  critical points to the optimization problem under mild conditions. By applying proposed algorithms to image segmentation applications, we illustrate the efficiency and convergence performance of the two proposed algorithms.
\end{abstract}

% Note that keywords are not normally used for peerreview papers.
\begin{IEEEkeywords}
 Semi-definite programming with diagonal constraints, synchronous and asynchronous algorithms, low-rank matrices, distributed optimization.
\end{IEEEkeywords}






% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle



\section{Introduction}
% The very first letter is a 2 line initial drop letter followed
% by the rest of the first word in caps.
%
% form to use if the first word consists of a single letter:
% \IEEEPARstart{A}{demo} file is ....
%
% form to use if you need the single drop letter followed by
% normal text (unknown if ever used by the IEEE):
% \IEEEPARstart{A}{}demo file is ....
%
% Some journals put the first two words in caps:
% \IEEEPARstart{T}{his demo} file is ....
%
% Here we have the typical use of a "T" for an initial drop letter
% and "HIS" in caps to complete the first word.
\IEEEPARstart{S}{emi-definite} programming (SDP) is an active subfield of convex optimization and has attracted considerable attention due to its widely applications in diverse fields such as control theory\cite{smart_admm,semi_observer,2003Semidefinite}, combinatorial optimization \cite{center_sdp,combina}, operations research\cite{max_like,eco_dispatch}, and machine learning\cite{matrix_learning,kernel_matrixlearning,distance_matrix}. Formally, it aims to maximize or minimize a linear objective function subject to a constraint that is an affine combination of positive semi-definite matrices. One important class of SDP problem is the  SDP with diagonal constraints, which is a relaxation of the ``maximum cut" problem \cite{1995Improved} and also appears in phase retrieval \cite{Waldspurger2012Phase} and $\mathbb Z/2\mathbb Z$ synchronization \cite{2014Exact}. 

Various algorithms have been developed to solve SDP with diagonal constraints but tend to be computational demanding as variable dimensions scale.
On one hand, the arithmetic cost scales badly as the dimension of matrices increases, especially for high-order algorithms. For example, each iteration  costs $O(n^3)$ arithmetic operations with an interior-point solver, which solves SDP in polynomial time \cite{2007Implementation}, and  the computation may run out of memory and time if $n$ is greater than several thousands \cite{BM_smooth}. On the other hand, the storage cost of each iteration may scale beyond the   memory of single computer if the number of unknowns reaches tens of thousands \cite{2015Phase}. Hence, the design of efficient algorithms for large-scale SDP with diagonal constraints is still a challenging problem.

\par  
To reduce the computational burden of large-scale SDP, there are many explorations of efficient centralized works in recent years\upcite{BM_smooth,2017Solving,wang2017mixing,Zhang2012}. 
One key idea is using the Burer-Monteiro factorization that expoits the low-rank property of matrix solutions by replacing the original large scale positive semi-definite matrix  as the product of two ``tall" matrices with lower dimensions to reduce storage cost and avoid computing the semi-definite projection. 
Using this idea, in \cite{BM_SPLR,BM_smooth,lowrank_sdp,2017Solving}, authors transform general SDP into non-convex optimization problems by making use of the low-rank property of solutions, and propose augmented Lagrangian algorithms and Riemannian manifold methods. In addition, the challenging positive semi-definite constraints are eliminated with the cost of introducing non-convexity to the optimization problems. Surprisingly, this change to  non-convex problems does not cause many difficulties because local solutions tend to recover the optimal solution in practice. Despite these advances, the existing augmented Lagrangian algorithms and Riemannian manifold methods do not guarantee converging to global optima, and suffer from slow convergence and difficulties in selecting step sizes. For SDP with diagonal constraints, some recent works\cite{wang2017mixing,erdogdu2018convergence} developed block-coordinate algorithms with rigorous convergence analysis, which have free parameters and better optimization performance than prior works\upcite{BM_SPLR,BM_smooth,lowrank_sdp,2017Solving}. All these centralized algorithms own fine practical evidences for the transformed non-convex optimizations. However, as the matrix dimension grows too large, the lower dimensional matrices in these algorithms may still take too much storage space for a single computer, such as these in some image processing problems. In addition, some information and data of practical problems may be generated and stored at different  locations and cannot be communicated  due to privacy considerations. Hence,  these centralized algorithms can not be applied directly to large scale problems with distributed information and distributed algorithms are in need for large-scale SDP.


\par Distributed optimization algorithms offer a promising approach to address large scale matrix problems by using the problem setup that the information is allocated over different agents\upcite{Sylvester_matrix,linear_eq,Deng2019NetworkFT,2016Implementing,matrix_equation,Lyapunov_matrix,Hong2016}. In distributed setting, agents have access to local information and communicate with their neighbors to seek for a global optimal solution\upcite{consensus_shi,liang_opti,Wang2020}. For large-scale SDP, many works in \cite{smart_admm,ADMM_CDC,fast_dis,semi_power,asyn_admm}  exploited the sparse structure of SDP and introduced additional consensus constrains to the transformed distributed problems. These works proposed distributed algorithms based on alternating direction method of multipliers (ADMM) with iterative message-passing.  Whereas, in ADMM, agents need to solve sub-semidefinite problems at each iteration and have considerable computational burden. Focusing on SDP with tree structures, \cite{dis_pd} proposed a distributed primal-dual interior-point algorithm for constrained semi-definite programming without introducing consensus constraints. The algorithm in \cite{dis_pd} is a second-order algorithm that conducts a recursion over the tree structure to compute the exact search directions and factorizes a relatively small matrix during each iteration. Recently, \cite{ricatti_zeng} proposed a distributed optimization design for solving continuous-time algebraic Riccati inequalities, which have applications in distributed control of multi-agent systems. This design is a first-order algorithm and has well intuitive interpretations, but it needs computing semi-definite projections, which are expensive for large-scale matrices.  
%%%The proposed algorithm is produced by distributing the computations conducted at each iteration of the primal-dual method, which has low computational burden.

%Inspired by these previous excellent works, our research proposes a different first-order distributed algorithm without exact iterative direction search and further considers the communication delays among different nodes.







% by utilizing the inherent low-rank property and sparsity.

%Although the mixing (block-coordinate minimization) method has an excellent convergence performance and a rigorous theoretical analysis, it is difficult to distribute the variable updating over multi-agent networks when applied to large-scale optimization problems, due to the strict orders in variable element iterations. The computing time consumption and storage space consumption of the centralized mixing method can be huge if the dimension of variable is very large. In addition, in some network applications, computation nodes are geographically separate and each node is only accessible to partial information.
\par In this paper, we  develop distributed first-order algorithms for large-scale semi-definite programs with diagonal constraints by taking advantage of low-rank property of solutions and the inherent sparsity of problems.
 The contributions of this paper are summarized as follows.
\begin{itemize}
\item This paper proposes a study on the distributed algorithms for SDP with diagonal constraints and distributed  coefficient matrices information. This study extends the works in \cite{BM_SPLR,BM_smooth,wang2017mixing,erdogdu2018convergence} to distributed setups, which have wide applications in power flow problems\upcite{ADMM_CDC,smart_admm} and distributed state estimation/control\upcite{semi_power,ricatti_zeng}. In addition, the SDP problem in this paper does not require tree structures as in \cite{dis_pd}.
%By matrix factorization, we transform the considered problem into a distributed non-convex optimization problem with unit spherical constraints. Compared with popular alternating direction method of multipliers\cite{ricatti_zeng,smart_admm,semi_power,ADMM_CDC,fast_dis,asyn_admm}, it reduces the dimension of variables and avoids positive semi-definite constraints.  
%We assume that each agent knows partial information of coefficient matrices and passes messages to its connected neighbors.
\item This paper  designs distributed synchronous and asynchronous algorithms for SDP with diagonal constraints by solving an equivalent nonconvex optimization problem, which is obtained using the Burer-Monteiro factorization. %In particular, we provide  updating rules of agents and define the communicated messages between connected agents. 
In particular, the distributed algorithms reduce the computational burden and storage  cost on single agent compared with the existing centralized algorithms\cite{wang2017mixing,erdogdu2018convergence} for SDP with diagonal constraints and show a superior numerical performance in simulation experiments. With the Burer-Monteiro factorization, the proposed algorithms avoid the computational burden of projection to semi-definite cone\upcite{ADMM_CDC,fast_dis,semi_power,asyn_admm}. Compared with the distributed second-order interior-point algorithm in \cite{dis_pd}, the proposed first-order algorithms have lower complexity and the distributed asynchronous algorithm performs well without a global synchronous clock. 
\item This paper analyzes the convergence of our proposed distributed algorithms. For the distributed synchronous algorithm, we show that the variables converge to the set of global optimal solutions almost surely under random initializations, despite of
the non-convexity of feasible sets. For the distributed asynchronous algorithm, we show that the variables converge to the set of critical points of the nonconvex problem under mild conditions.
\end{itemize}
 \par The remainder of the paper is organized as follows. Mathematical notations are given in section \ref{preliminaries_sec}. The semi-definite programming description and distributed algorithms are proposed in section \ref{solver_design}. The convergence properties of the proposed algorithms are analyzed theoretically in section \ref{proof_sec}. The efficiency of distributed algorithms is verified by simulations in section \ref{simulation} and the conclusion is made in section \ref{conclusion}.

\section{Mathematical Notations} \label{preliminaries_sec}

\par We denote $\mathbb{R}$ as the set of real numbers, $\mathbb{R}^n$ as the set of $n$-dimensional real column vectors, $\mathbb{R}^{n\times m}$ as the set of $n$-by-$m$ real matrices, $\mathbb{N}$ as the set of natural numbers, $\mathbb{S}^n$ as the set of $n$ by $n$ symmetric matrices, $\mathbb{S}_+^n$ as the set positive semi-definite matrices, $\emptyset$  as the empty set, respectively. All vectors in the paper are column vectors, unless otherwise noted. The notation $0_n$ denotes an $n \times 1$ vector with all elements of $0$. For a real vector $v$, $\left\|v\right\|$ is the Euclidean norm and $\left\|v\right\|_1$ is 1-norm defined by the sum of  absolute values of elements. We denote $A'$ as the transpose of matrix $A$, $\lambda_{min}(A)$ as the minimum eigenvalue of the matrix $A$. For a symmetric matrix $A$, $A \succeq 0$ denotes that $A$ is positive semi-definite and $A_{(i,j)}$ is the $(i,j)$th element of matrix $A$. For real matrices $A$ and $B$ with same dimensions, $\left<A, B\right>$ denotes the Frobenius inner product of two real matrices such that $\left< A,B\right>=tr(A'B)=\sum_{i,j}A_{(i,j)}B_{(i,j)}$. In addition, $A\circ B$ denotes Hadamard product of two matrices, whose elements are defined by $[A\circ B]_{(i,j)}=A_{(i,j)}B_{(i,j)}$. For a twice-continuously differentiable function $f(x)$, its gradient and Hessian matrix are denoted as $\nabla f(x)$ and $\nabla^2 f(x)$.
\par For a set $\mathcal{S}$, $\left|\mathcal{S}\right|$ denotes the number of elements in the set $\mathcal{S}$. For sets $\mathcal{S}_1$ and $\mathcal{S}_2$, $\mathcal{S}_1\subset \mathcal{S}_2$ means that $\mathcal{S}_1$ is a subset of $\mathcal{S}_2$, $\mathcal{S}_1\cup \mathcal{S}_2$ is the union of $\mathcal{S}_1$ and $\mathcal{S}_2$, $\mathcal{S}_1 \cap \mathcal{S}_2$ is the intersection of $\mathcal{S}_1$ and $\mathcal{S}_2$, and $\mathcal{S}_1\backslash \mathcal{S}_2=\mathcal{S}_1-(\mathcal{S}_1\cap \mathcal{S}_2)$. For a real number $a$, $\lceil a \rceil$ is the smallest integer greater than $a$. For a non-zero vector $x\in \mathbb{R}^n$, the notation ${\rm normal}(x)$ is $\frac{x}{\left\|x\right\|}$ and $A={\rm diag}(x)\in \mathbb{R}^{n\times n}$ denotes a matrix with diagonal element $A_{(i,i)}=x_i$.
%obtained by stacking the columns of the matrix $A$ on top of one another: $vec(A)=[a_{11},\cdots,a_{m1},a_{12},\cdots,a_{m2},\cdots,a_{1n},\cdots,a_{mn}]'$.
\par Let $\mathcal{X}$ be a smooth manifold. Let $f:\mathcal{X} \to \mathbb{R}$ be a real-valued twice-continuously differentiable function. A point $x^*$ is a critical point of $f$ if $\nabla f(x^*)=0_d$. If, in addition, $\lambda_{min}(\nabla^2 f(x^*))<0$, $x^*$ is a strict saddle point of $f$. 
%The differential of a mapping $g$, denoted as $Dg(x)$, is a linear operator from $\Gamma(x)\to \Gamma(g(x))$, where $\Gamma(x)$ is the tangent space of $\mathcal{X}$ at point $x$. Given a curve $\gamma$ in $\mathcal{X}$ with $\gamma(0)=x$ and $\frac{d\gamma}{dt}(0)=v\in \Gamma(x)$, the linear operator is defined as $Dg(x)v=\frac{d(g\circ\gamma)}{dt}(0)\in\Gamma(g(x))$.
%\par When $\mathcal{X}$ is a manifold, the same definition applies, but with the the gradient and Hessian replaced by Riemannian gradient $\nabla_R f(x)$ and Riemannian Hessian $\nabla_R^2 f(x)$.
%differentiable manifold (also differential manifold) is a type of manifold that is locally similar enough to a linear space to allow one to do calculus
%\par A manifold is a second countable Hausdorff space that is locally homeomorphic to Euclidean space. An isomorphism is a mapping between two structures of the same type that can be reversed by an inverse mapping.
%\par The diffeomorphism is an isomorphism of smooth/differentiable manifolds. It is an invertible function that maps one differentiable manifold to another such that both the function and its inverse are smooth. A diffeomorphism is defined formally as following.
% \begin{definition}(Diffeomorphism)
% Given two manifolds $M$ and $N$, a differentiable map $f:M\to N$ is called a diffeomorphism if it is a bijection and its inverse $f^{-1}:N\to M$ is differentiable as well.
% \end{definition}

%\subsection{Coupled optimization problems}\label{clique_sec}
%\par In this section, we consider a coupling optimization problem whose function is a sum of functions:
%\begin{align}\label{couple_pro}
%minimize_x \quad f_1(x)+\cdots+f_N(x),
%\end{align}
%where $f_i:\mathbb{R}^n \to \mathbb{R}$ for all $i=1,\cdots,N$. We assume that each function $f_i$ depends only on some elements of $x$, and we denote the dependent element indices subset of function $f_i$ by $J_{\tilde{i}}\subset \mathbb{N}_n$.
%\par With the dependency descriptions above, we can rewrite the problem (\ref{couple_pro}) as following.
%\begin{align}\label{trans_coup}
%minimize_x \quad \tilde{f}_1(E_{J_1}x)+\cdots+\tilde{f}_N(E_{J_N}x),
%\end{align}
%where $E_{J_{\tilde{i}}}$ is a $0-1$ matrix that is obtained from an identity matrix of order $n$ by deleting the rows indexed by $N_n\backslash J_{\tilde{i}}$. The functions $\tilde{f}_i:\mathbb{R}^{\left|J_{\tilde{i}}\right|}\to \mathbb{R}$ are lower dimensional descriptions of $f_i$ such that $f_i(x)=\tilde{f}_i(E_{J_{\tilde{i}}}x)$ for all $x$ and $i=1,\cdots,N$. %In addition, we denote the ordered set of indices of functions that depend on $x_i$ by $I_i=\{k|i\in J_k\}\subset \mathbb{N}_N$.
%\par The transformed problem (\ref{trans_coup}) enables us to describe the coupling structure of the problem (\ref{couple_pro}) using an multi-agent network graph $\mathbf{G}_s$, providing a clearer picture of the coupling. The graph has a node set $\mathbf{V}_s={1,\cdots,N}$ and an edge set $\mathbf{E}_s$ with $(i,j)\in \mathbf{E}_s$ if and only if $J_{\tilde{i}} \cap J_j \neq \emptyset$. With different dependency relationships between subproblems and variable elements, there can be different graph topology, such as tree graph, circle graph, star graph and so on.
%\par For clarity and expression convenience, we define child nodes and parent nodes in the multi-agent graph, which helps to reduce the need of bidirectional communications and save communication costs in the network topology. For the problem (\ref{trans_coup}), there is an edge between node $i$ and $j$ if $J_{\tilde{i}} \cap J_j \neq \emptyset$. Then if the indices $i<j$, the node $j$ is called the parent node of node $i$, denoted by ${\rm par}(i)$. And the communication information between child and parent nodes is designed in the following proposed algorithms.
%\begin{remark}
%It should be noticed that for each node, there can be several parent nodes. Here for convenience, we only consider the issue that one node only own one parent in this paper. However, the works in this paper can be easily extended to issues that nodes with multi parents. We also provide examples in section Simulations to demonstrate the case of tree structures with unique parent and involved graphs with multi parents.
%\end{remark}

%This graph is the \emph{sparsity graph} of the problem (\ref{couple_pro}). Note that all set $J_{\tilde{i}}$ of the sparsity graph induce complete subgraphs. A graph $\mathbf{G}(\mathbf{V},\mathbf{E})$ is \emph{chordal} if all its cycles of length at least four has a chord, where a chord is an edge between two non-consecutive vertices in a cycle. Next, we will introduce the clique tree structure.
%\par A \emph{clique} of a graph is a maximal node subset that induces a complete subgraph, hence it is possible to agent the nodes in the sparsity graph. Let us denote the set of cliques of $\mathbf{G}_s$ as $\mathbf{C}_{G_S}=\{C_1,\cdots,C_c\}$. There exists a tree defined on $\mathbf{C}_{G_S}$ such that for every two cliques $C_i$ and $C_j$ with $i \neq j$ in the clique tree, $C_i \cap C_j$ is contained in all cliques in the path connecting the two cliques in the tree. This property is called the \emph{clique intersection property} and trees with this property are referred as \emph{clique trees}. Each node in the tree corresponds to each agent of variables. The problems enjoying this inherent structure are called to be coupling with a clique tree structure.
%\begin{example}
%We consider a coupling optimization example with $N=5$.
%\begin{align}\label{exam_pro}
%minimize_x f_1(x)+f_2(x)+f_3(x)+f_4(x)+f_5(x),
%\end{align}
%and assume that $x\in \mathbb{R}^6$, $J_1=\{1,2,3\}$, $J_2=\{1,4\}$, $J_3=\{2,4,5\}$, $J_4=\{2,6\}$,
%$J_5=\{3,4\}$. By the dependency description, we have $I_1=\{1,2\}$, $I_2=\{1,3,4\}$, $I_3=\{1,5\}$,
%$I_4=\{2,3,5\}$, $I_5=\{3\}$, $I_6=\{4\}$. Then, according to the above mentioned graph description of coupling structure, we can describe the function structure in (\ref{exam_pro}) as the sparsity graph in Fig. \ref{spar_graph}. The sparsity graph is chordal and the tree representation of the coupling problem is obtained by clustering the nodes in sparsity graph. The resulted clique tree has three cliques, $C_1=\{1,2,3,4\}$, $C_2=\{2,4,5\}$, $C_3=\{2,6\}$, respectively, and is shown in Fig. \ref{tree_graph}.
%\end{example}
%\begin{figure}
%  \centering
%  % Requires \usepackage{graphicx}
%  \includegraphics[width=6cm]{sparsity_graph.pdf}\\
%  \caption{Sparsity graph of problem (\ref{exam_pro})}\label{spar_graph}
%\centering
%\end{figure}
%\begin{figure}
%  \centering
%  % Requires \usepackage{graphicx}
%  \includegraphics[width=6cm]{tree_graph.pdf}\\
%  \caption{Clique tree of problem (\ref{exam_pro})}\label{tree_graph}
%\centering
%\end{figure}
%\begin{remark}Extracting the tree structure of a sparsity graph may not be easy. One of the most common approaches is to use well-established methods for automating the extraction of tree representations of chordal graphs, such as greedy search methods. In addition, it is possible to make a non-chordal graph chordal by adding some edges to the graph, resulting a chordal embedding graph. Note that, for the non-chordal graph, the same procedure can be applied to its chordal embedding graph for extracting the tree structure.
%\end{remark}
%\par The tree structure in coupling optimization problems makes it possible to develop a distributed method for solving them, which is one of the inspirations of our work.
\section{Problem Description and Distributed Algorithm Design}\label{solver_design}

In this section, we present the problem of solving semi-definite programming with diagonal constraints in a  distributed way. Then, we reformulate the problem into a distributed non-convex optimization using the low-rank property of solutions, and propose distributed synchronous and asynchronous discrete-time algorithms.
\subsection{Problem description and transformation}\label{intro_pro_set}


Let $\mathcal{G}=(\mathcal{V},\mathcal{E})$ be an arbitrary simple, undirected and connected graph with the node set $\mathcal{V}=\{1,\cdots,m\}$ and the edge set $\mathcal{E}=\mathcal{V}\times \mathcal{V}$. Let $X\in\mathbb S_+^{n}$ be the variable. For each  $\tilde i\in \mathcal{V}$, define $J_{\tilde{i}}\subset \{1,\cdots,n\}$ as an ordered set, where $n>m$.
Throughout this paper, we use $\tilde{(\cdot)}$ to denote the index of agent in $\mathcal G$ and $(\cdot)$ to denote an element of $J_{\tilde{(\cdot)}}$. For example,  $\tilde i\in \mathcal{V}$ denotes agent $\tilde i$ of $\mathcal G$ and $j\in J_{\tilde{i}}$ is element $j$ in $J_{\tilde{i}}$. If $J_{\tilde{i}}\cap J_{\tilde{j}}\neq \emptyset$, then  $(\tilde{i},\tilde{j})\in \mathcal{E}$  such that  agents $\tilde{i}$ and $\tilde{j}$ can communicate with each other.

%Without affecting the solution

The distributed semi-definite programming with diagonal constraints  is
\begin{subequations}\label{sdp_pro}
	\begin{align}
	\min_{X\in\mathbb{S}_+^n} \,\, & \sum_{\tilde{i}=1}^m   \langle\overline{M}^{\tilde{i}},  X\rangle, \label{sdp_func}\\
	{\rm s. \, t.}\, \, & X_{(j,j)}=1, \quad j \in \{1,\cdots,n\}, \label{sdp_constr}
	\end{align}
\end{subequations}
where $\overline{M}^{\tilde{i}}\in \mathbb{S}^n$ is a coefficient matrix such that $\overline{M}^{\tilde{i}}_{(j,k)} = 0$ if $(j,k)\notin J_{\tilde{i}}\times J_{\tilde{i}}$, {and $\sum_{\tilde{i}=1}^m \overline{M}^{\tilde{i}}=M$.} 
Define $E_{J_{\tilde{i}}\times J_{\tilde{i}}}\in \mathbb{R}^{n\times n}$ as the $0-1$ matrix with $E_{(l,k)}=1$ for $(l,k) \in J_{\tilde{i}}\times J_{\tilde{i}}$ and $E_{(l,k)}=0$ otherwise. It is clear that $ \langle\overline{M}^{\tilde{i}},  X\rangle =  \langle\overline{M}^{\tilde{i}},  E_{J_{\tilde{i}}\times J_{\tilde{i}}} \circ X\rangle$ for all $i\in\{1,\ldots,n\}$. Without affecting solutions, the local  variable $\overline{X}^{\tilde{i}}\in\mathbb{S}_{+}^{n}$ to be determined by agent $\tilde{i}\in\mathcal V$ is defined as $\overline{X}^{\tilde{i}}=E_{J_{\tilde{i}}\times J_{\tilde{i}}} \circ X$. 
The objective of this paper is to design a distributed algorithm for solving \eqref{sdp_pro} such that each agent $\tilde{i}\in\mathcal V$ only knows local information $\overline{M}^{\tilde{i}}\in \mathbb{S}^n$.

\begin{example}
	Figure \ref{graph_cut} illustrates the  relationship of global optimization variable $X$ and  local variables $\overline{X}^{\tilde{i}}$ ($i\in \{1,\cdots,4\}$), where elements with same indices in different colored matrices are the same.
	\begin{figure*}
		\centering
		\includegraphics[scale=0.4]{matrix_decom.pdf}
		\caption{Matrix decomposition of global variable $X$ over four agents. }\label{graph_cut}
		%  \centering
	\end{figure*}
\end{example}
%
%The objective of this paper is to design a distributed algorithm for solving \eqref{sdp_pro} such that each agent $\tilde{i}\in\mathcal V$ only knows local information $\overline{M}^{\tilde{i}}\in \mathbb{S}^n$.


\begin{remark}
	The centralized version of problem \eqref{sdp_pro} is 
	\begin{align}\label{cen-sdp}
	\min_{X\in\mathbb{S}_{+}^n} \,\, \langle M, X\rangle,
	\quad {\rm s. \, t.}\, \,  X_{i,i}=1,\quad i\in\{1,\ldots,n\},
	\end{align}
	which is a special case of generic semi-definite programming. It appears as a convex relaxation to many problems, such as the maximum cut (MAXCUT) problems\upcite{1995Improved}, community detection\upcite{2016community} and image segmentation\upcite{mincut_graph}. In practical problems such as roadmaps or social networks, the dimension $n$ may be several millions or even billions, which makes centralized computation hard. Hence, the development of distributed algorithms for \eqref{sdp_pro} is of great importance. 
\end{remark}

\begin{remark} 
	There are two scenarios in which the problem (\ref{sdp_pro}) arises. In the first scenario, an arbitrary sparse SDP problem in the standard centralized form is converted into a distributed SDP with multiple positive semi-definite matrices $X^{\tilde{i}}$ by the idea of chordal decomposition of positive semi-definite cones in \cite{chor_spar}. In the second scenario, it is assumed that the SDP is associated with a multi-agent network and matches the formulation in (\ref{sdp_pro}) exactly, such as large-scale image segmentation by multiple agents in section Simulation.
\end{remark}







Since $\langle\overline{M}^{\tilde{i}},  X\rangle$ only depends on  elements with indices in $J_{\tilde{i}}\times J_{\tilde{i}}$ of variable $X\in\mathbb{S}_{+}^{n}$, define matrix $M^{\tilde{i}}\in \mathbb{S}^{|J_{\tilde{i}}|}$ ($X^{\tilde{i}}\in \mathbb{S}^{|J_{\tilde{i}}|}$) as the remaining matrix by deleting elements of $\overline{M}^{\tilde{i}}\in\mathbb{S}_{+}^{n}$ ($\overline{X}^{\tilde{i}}\in\mathbb{S}_{+}^{n}$), whose indices are not in $J_{\tilde{i}}\times J_{\tilde{i}}$. For ease of notation, we define $X^{\tilde{i}}_{\{j,k\}}\triangleq \overline{X}^{\tilde{i}}_{(j,k)}$ for $(j,k)\in J_{\tilde{i}}\times J_{\tilde{i}}$ and $\tilde{i}\in\mathcal V$. Similarly, define $M^{\tilde{i}}_{\{j,k\}}\triangleq \overline{M}^{\tilde{i}}_{(j,k)}$ for $(j,k)\in J_{\tilde{i}}\times J_{\tilde{i}}$ and $\tilde{i}\in\mathcal V$. Hence, problem \eqref{sdp_pro} is equivalent to
\begin{subequations}\label{sdp_pro2}
	\begin{align}
	\min_{X^{\tilde{i}}\in \mathbb{S}_+^{|J_{\tilde{i}}|},\,\tilde{i}\in\mathcal V} \,\, & \sum_{\tilde{i}=1}^m  f_{\tilde{i}}(X^{\tilde{i}}), \quad   f_{\tilde{i}}(X^{\tilde{i}})= \langle {M}^{\tilde{i}}, X^{\tilde{i}}\rangle, \label{sdp_func}\\
	{\rm s. \, t.}\, \, & X^{\tilde{i}}_{\{j,j\}}=1, \ j \in J_{\tilde{i}}, \label{sdp_constr}\\
	& X^{\tilde{i}}_{\{l,k\}}= X^{\tilde{j}}_{\{l,k\}}, \ \forall l,k \in J_{\tilde{i}}\cap J_{\tilde{j}},\ (\tilde{i},\tilde{j})\in \mathcal{E},
	\end{align}
\end{subequations}
where agent $\tilde{i}\in\mathcal V$ knows ${M}^{\tilde{i}}$ and computes a positive semi-definite matrix variable $X^{\tilde{i}}\in \mathbb{S}_{+}^{|J_{\tilde{i}}|}$, $(\tilde{i},\tilde{j})\in \mathcal{E}$ specifies an overlap between the local variables $X^{\tilde{i}}$ and $X^{\tilde{j}}$ of agents $\tilde{i}$ and $\tilde{j}$.




%\begin{figure}
%  \centering
%  % Requires \usepackage{graphicx}
%  \includegraphics[width=8cm]{netshow.eps}\\
%  \caption{Maximum cut of a multi-node graph. The different color nodes belong to different cut sets. The width of edges represents the weight of edges. One maximum cut problem is to seek for the cut that maximizes the weight of the edges between nodes in different cut sets. }\label{graph_show}
%\centering
%\end{figure}


%Based on the notation introductions for coupling optimization problems in section \ref{clique_sec}, we can transform the non-convex problem (\ref{non-convex_pro}) into a distributed optimization problem whose objective function is a sum of subfunctions as follows
%%\begin{subequations}
%%\begin{align}
%%minimize_{V_{J_1}^1,\cdots, V_{J_c}^c} \quad & \sum_{q=1}^c F_q(V_{J_q}^q), \quad F_q(V_{J_q}^q)=\left<M^q ,V_{J_q}^{q'}V_{J_q}^{q}\right>\\
%%subject \quad to \quad  &\left\|v_j^{q}\right\|=1, \quad \forall q=1,\cdots,c, \quad j\in J_q
%%\end{align}\label{distributed_pro}
%%\end{subequations}
%\begin{subequations}
%\begin{align}
%minimize_{V_{J_1}^1,\cdots, V_{J_A}^A} \quad & \sum_{i=1}^A \left<\mathcal{M}^{\tilde{i}} ,V_{J_{\tilde{i}}}^{i'}V_{J_{\tilde{i}}}^{i}\right>\\
%subject \quad to \quad  &\left\|v_j^{i}\right\|=1 \quad \forall i=1,\cdots,A, \ j\in J_{\tilde{i}}\\
%                         &v_s^{\tilde{i}}=v_s^{{\rm par}(i)} \quad \forall s \in J_{\tilde{i}}\cap J_{{\rm par}(i)},
%\end{align}\label{distri_pro2}
%\end{subequations}
%where $A$ is the number of agents in the network graph, $J_{\tilde{i}}$ is variable element indices set of agent $i$, $V_{J_{\tilde{i}}}^{i}\in \mathbb{R}^{p \times \left|J_{\tilde{i}}\right|}$, ${\rm par}(i)$ is the parent agent of $i$ and $\mathcal{M}^{\tilde{i}}\in\mathbb{R}^{\left|J_{\tilde{i}}\right|\times \left|J_{\tilde{i}}\right|}$ is the coefficient matrix of $V_{J_{\tilde{i}}}^{i}$, which is introduced in detail later. Define $\bar{\mathcal{M}}^{\tilde{i}}$ as an $n\times n$ matrix with $\bar{\mathcal{M}}_{(l,k)}^{\tilde{i}}=\mathcal{M}_{(l,k)}^{\tilde{i}}$ for $l,k \in J_{\tilde{i}}$ and otherwise $\bar{\mathcal{M}}_{(l,k)}^{\tilde{i}}=0$. Hence, $\sum_{i=1}^A \bar{\mathcal{M}}^{\tilde{i}}=M$. It should be noted that for convenience, {\color{red}{the indices element $j$ in the each agent $i$'s indices set $J_{\tilde{i}}$ is the indices in origin matrix variable $V$ in problem (\ref{non-convex_pro})}}. In addition, it is noticed again that the child nodes and parent nodes in the algorithm are introduced to reduce communication cost and state the proposed method clearly, who actually perform equal roles, especially in distributed asynchronous method. What's more, the number of agent in (\ref{distri_pro2}) may not equal to that in (\ref{trans_coup}) in some special cases, such as in simulation example 1, so we use notation $A$ instead of $N$. We follow other notations with little ambiguity for simplicity.
\par In most existing distributed works\upcite{ADMM_CDC,fast_dis} for (\ref{sdp_pro}), the updating of local variable $X^{\tilde{i}}$ often involves a projection operator to the positive semi-definite cone. If the dimension of local $X^{\tilde{i}}$ of large-scale SDP is large, the projection operator is difficult and time-consuming. Hence, based on prior works on the low-rank property of matrix variables, we further reduce the computational and storage burden by representing $X\in\mathbb R^{n\times n}$ by $V'V$ with $V=[v_1,\cdots, v_n]\in\mathbb R^{p\times n}$ to avoid the projection operator. It is well-known that the rank of an optimal solution is at most $\lceil\sqrt{2n}\rceil$ (see \cite{SDP_rank}). Let $v^{\tilde{i}}_s$ be the estimate of $v_s$ by agent $\tilde{i}$ for $s\in J_{\tilde{i}}$ and $\tilde{i}\in\mathcal V$. Without causing confusions, we define
$V^{\tilde{i}}=[v^{\tilde{i}}_{s_1},\cdots,v^{\tilde{i}}_{s_{|J_{\tilde{i}}|}}]\in\mathbb R^{p\times |J_{\tilde{i}}|}$, where  $\{s_1,\ldots,s_{|J_{\tilde{i}}|}\}= J_{\tilde{i}}$, $s_1<\cdots<s_{|J_{\tilde{i}}|}$, and $\tilde{i}\in\mathcal V$.
Hence, the local variable $X^{\tilde{i}}$ is replaced by $X^{\tilde{i}}=V^{\tilde{i}'}V^{\tilde{i}}$, where $V^{\tilde{i}}\in \mathbb{R}^{p\times \left|J_{\tilde{i}}\right|}$, $p>\sqrt{2n}$. Then the semi-definite programming (\ref{sdp_pro2}) is rewritten as the following non-convex optimization problem on unit spheres:
\begin{subequations}\label{non-convex_pro}
	\begin{align}
	\min_{V^{\tilde i},\,\tilde{i}\in\mathcal V} \quad & \sum_{\tilde{i}=1}^m \tilde f_{\tilde{i}}(V^{\tilde{i}}), \ \tilde  f_{\tilde{i}}(V^{\tilde{i}})=\langle M^{\tilde{i}} ,V^{\tilde{i}'}V^{\tilde{i}}\rangle \label{obj}\\
	{\rm s.} \ {\rm t.} \quad  &\|v_j^{\tilde{i}}\|=1, \quad \forall \tilde{i}\in\mathcal V, \ {j} \in J_{\tilde{i}}\label{norm_con}\\
	&v_{{s}}^{\tilde{i}}=v_{{s}}^{\tilde{j}}, \quad \forall {s} \in J_{\tilde{i}}\cap J_{\tilde{j}},\ (\tilde{i},\tilde{j})\in \mathcal{E},\label{share_con}
	\end{align}
\end{subequations}
where $V^{\tilde{i}}\in \mathbb{R}^{p \times \left|J_{\tilde{i}}\right|}$ is local variable and $M^{\tilde{i}}\in\mathbb{S}^{\left|J_{\tilde{i}}\right|}$ is local coefficient matrix known by agent $\tilde{i}\in\mathcal V$.
\par The following assumption is needed.
\begin{assumption}\label{pro_assump}
	\begin{itemize}
		\item[(1)] Graph $\mathcal G$ is undirected and connected.
		\item[(2)] Each element of global coefficient matrix $M=\sum_{\tilde{i}=1}^m \overline{M}^{\tilde{i}}$ is non-negative and  diagonal elements of $M$ are zero.
		\item[(3)] The integer $p$  satisfies $p>\sqrt{2n}$.
	\end{itemize}
\end{assumption}
\begin{remark}
	Assumption \ref{pro_assump} (1) is general in distributed optimization.
	Since the norm of column variables $v_i$ is fixed as one,  $M_{(i,i)}=0$ does not affect the solution of optimization problem. In addition,  because elements of coefficient matrix  in MAXCUT problems and community detection are non-negative, 
	Assumption \ref{pro_assump} (2) is  practical for   problem \eqref{sdp_pro}.  Assumption \ref{pro_assump} (3) is a sufficient condition that optimal solutions for $V^{\tilde{i}}$'s recover optimal solutions for $X^{\tilde{i}}$'s.
\end{remark}
%To make expression clear, we use $\tilde{i}$ to denote the $\tilde{i}$th agent in distributed methods and $j$ to denote the $j$th column variable. Then, $v_j^{\tilde{i}}$ denotes the $j$th column variable of agent $\tilde{i}$. It should be noted that for convenience, {{the indices element ${j}$ in the each agent $\tilde{i}$'s indices set $J_{\tilde{i}}$ follows the indices in origin matrix variable $V$ in problem (\ref{non-convex_pro})}}.
%\par The transformed problem (\ref{distri_pro2}) enables us to describe the coupling structure of the problem (\ref{non-convex_pro}) using a multi-agent network graph $\mathbf{G}$, providing a clearer picture of the coupling. The graph has a node set $\mathcal{V}=\{1,\cdots,m\}$ and an edge set $\mathcal{E}$ with $(\tilde{i},\tilde{j})\in \mathcal{E}$ if and only if $J_{\tilde{i}} \cap J_{\tilde{j}} \neq \emptyset$. With different dependency relationships between subproblems and variable elements, there can be different graph topologies, such as tree graphs, star graphs and so on.
%\par For clarity and expression convenience, we define child and parent in the multi-agent graph. For the problem (\ref{non-convex_pro}), there is an edge between agents $\tilde{i}$ and $\tilde{j}$ if $J_{\tilde{i}}\cap J_{\tilde{j}} \neq \emptyset$. Then if the indices $\tilde{i}<\tilde{j}$, the agent $\tilde{i}$ is called the parent of agent $\tilde{j}$, denoted by ${\rm par}(\tilde{j})$. The communication information between child  and parent is introduced in next subsection.
%\begin{remark} This low-rank reformulation (\ref{non-convex_pro}) removes the positive semi-definite constraint $X^{\tilde{i}}\succeq 0$ because the transformation  $X^{\tilde{i}}=V^{\tilde{i}'}V^{\tilde{i}}$ is guaranteed to be a positive semi-definite matrix and the constraint $X_{(j,j)}=1$ is transformed into the constraint $\left\|{v}^{\tilde{i}}_l\right\|=1$ for all $\tilde{i}\in \{1,\cdots,m\}$ and $l \in J_{\tilde{i}}$. Although the transformed problem is non-convex, it is known that when $p >\sqrt{2n}$, optimal solutions for $V\in \mathbb{R}^{p \times n}$ recover optimal solutions for $X$.
%\end{remark}

%What's more, the number of agent in (\ref{distri_pro2}) may not equal to that in (\ref{trans_coup}) in some special cases, such as in simulation example 1, so we use notation $A$ instead of $N$. We follow other notations with little ambiguity for simplicity.


%\subsection{Mixing method for transformed non-convex problem}
%There are some existing methods for solving the transformed non-convex problem with excellent practical convergence performance. However, most of these feasible methods do not provide rigorous theoretical proof for convergence analysis. Among these methods, block-coordinate (minimization/maximization) methods are simpler to implement and have lower computational complexity. The local and global convergence rate guarantees have also been studied in recent works. Hence, in this paper, we are inspired by the Mixing method to solve the non-convex problem (\ref{non-convex_pro}), which is first proposed in \cite{wang2017mixing}.
%\par Specifically, the terms that depends on $\mathbf{v}_i$ are given by $\mathbf{v}_i'(\sum_{j=1}^n M_{(i,j)} \mathbf{v}_j)$, where $M_{(i,j)}$ is the $(i,j)$th element of matrix $M$. In addition, the diagonal elements $M_{(i,i)}=0$. Hence, this problem has a closed form solution given by
%\begin{align}\label{mixing_first}
%\mathbf{v}_i= {\rm normalize} (-\sum_{j=1}^n M_{(i,j)} \mathbf{v}_j).
%\end{align}
%In the Mixing method, $\mathbf{v}_i$ is initialized on the unit sphere and perform cyclic updates over all column vectors $\mathbf{v}_i$ ($i=1,\cdots, n$) in the closed form. For the case of sparse $M$, the time complexity is $O(k\cdot m)$, where $k$ is the rank of $V$ and $m$ is the number of nonzeros in $M$. Since the variable updates in (\ref{mixing_first}) are likely to jump directly to a non-optimal critical point, a Mixing method with step size $\theta$ is provided, which is always guaranteed to converge to a global optimum \cite{wang2017mixing}. The modified variable updating is
% \begin{align}\label{ori_m}
% \mathbf{v}_i(t+1)={\rm normalize}(\mathbf{v}_i(t)-&\theta (\sum_{j<i} M_{(i,j)} \mathbf{v}_j(t+1)\notag\\
% &+\sum_{j>i} M_{(i,j)} \mathbf{v}_j(t))),
% \end{align}
% where $\theta\in (0,\frac{1}{{\rm max}_i \left\|M_{(i,:)}\right\|_1})$ and $M_{(i,:)}$ is the $i$th row vector of the coefficient matrix $M$.
%
%%\begin{algorithm}
%%\caption{The Mixing method}
%%	\label{hhsa}
%%	\begin{algorithmic}[1]  %1	
%%        \State Initialize $\mathbf{v}_i$ randomly on a unit sphere and $\theta\in (0,\frac{1}{{\rm max}_i \left\|M_{(i,:)}\right\|_1})$
%%		\While {not yet converged}
%%		\For {$i=1, \cdots, n$}
%%		\State $\mathbf{v}_i(t+1)={\rm normalize}(\mathbf{v}_i(t)-\theta (\sum_{j<i} M_{(i,j)} \mathbf{v}_j(t+1)+\sum_{j>i} M_{(i,j)} \mathbf{v}_j(t)))$;
%%		\EndFor
%%        \EndWhile
%%	\end{algorithmic}
%%\end{algorithm}
%\begin{remark}
%Although the mixing (block-coordinate minimization) method has an excellent convergence performance and a rigorous theoretical analysis, it is difficult to distribute the variable updating over multi-agent networks when applied to large-scale optimization problems, due to the strict orders in variable element iterations. The computing time consumption and storage space consumption of the centralized mixing method can be huge if the dimension of variable is very large. In addition, in some network applications, computation nodes are geographically separate and each node is only accessible to partial information. Hence, it is necessary to study one distributed algorithm solving large-scale SDP problems.%%, such as cooperative image segmentation
%\end{remark}

\subsection{Distributed synchronous optimization algorithm}\label{pro_trans}
In this subsection, we propose a distributed synchronous algorithm for the transformed distributed non-convex optimization problem (\ref{non-convex_pro}).%, which is summarized in Algorithm \ref{algo_sum}.

To present the algorithm, we need additional definitions and notations. We define ``children" and ``parents" in graph $\mathcal G$. For the problem (\ref{non-convex_pro}), there is an edge between agents $\tilde{i}$ and $\tilde{j}$ if $J_{\tilde{i}}\cap J_{\tilde{j}} \neq \emptyset$. Without loss of generality,  if indices $\tilde{j}<\tilde{i}$ and $J_{\tilde{i}}\cap J_{\tilde{j}} \neq \emptyset$, the agent $\tilde{j}$ is called the {\em parent} of agent $\tilde{i}$, denoted by ${\rm par}(\tilde{i})$, and the set of {\em children} of agent $\tilde{i}$ is denoted by ${\rm ch}(\tilde{i})$. The message passed from agent $\tilde{r}\in {\rm ch}(\tilde{i})$ to agent $\tilde{i} \in\mathcal V$ is denoted by $\varpi^{\tilde{r},\tilde{i}}$. Define the {\em coupling set} of indices between column variables of agent $\tilde{i}$ and its parent ${\rm par}(\tilde{i})$ as $\mathcal{S}_{\tilde{i},{\rm par}(\tilde{i})}={{J}_{\tilde{i}}}\cap J_{{\rm par}(\tilde{i})}$ and the {\em uncoupling set} of indices as $\mathcal{R}_{\tilde{i},{\rm par}(\tilde{i})}=J_{\tilde{i}}\backslash \mathcal{S}_{\tilde{i},{\rm par}(\tilde{i})}$. In addition,  column variables $v_j^{\tilde{i}}$s with $j\in \mathcal{S}_{\tilde{i},{\rm par}(\tilde{i})}$ are called {\em coupling variables} of agent $\tilde{i}$ and its parent ${\rm par}(\tilde{i})$, and  column variables $v_j^{\tilde{i}}$'s with $j\in \mathcal{R}_{\tilde{i},{\rm par}(\tilde{i})}$ are called {\em uncoupling variables} of agent $\tilde{i}$ and its parent ${\rm par}(\tilde{i})$. Accordingly, we have the similar notations $\mathcal{S}_{{\rm ch}(\tilde{i}),\tilde{i}}$ and $\mathcal{R}_{{\rm ch}(\tilde{i}),\tilde{i}}$.

\begin{example}% agent 1-5 5agent
	Fig. \ref{parent_child_graph} gives  a network  to show the previous definitions of $\mathcal{S}_{\cdot,\cdot}$ and $\mathcal{R}_{\cdot,\cdot}$. Noted that $\mathcal{S}_{{\rm ch}(\tilde{i}),\tilde{i}}$ is an empty set for agent $\tilde{i}$ with no children, such as agents $\tilde{3},\tilde{4},\tilde{5}$. Accordingly, the message $\varpi^{\tilde{r},\tilde{i}}, \ \tilde{r}\in {\rm ch}(\tilde{i})$ passed from children agents is zero for $\tilde{i}=\tilde{3},\tilde{4},\tilde{5}$. The similar case $\mathcal{S}_{\tilde{i},{\rm par}(\tilde{i})}=\emptyset$ holds for the root agent, which have no parents. %Let $v_{j\in \mathcal{R}_{\tilde{i},{\rm par}(\tilde{i})}}$ denote the uncoupling column variable set of agent $\tilde{i}$ and $v_{j\in \mathcal{S}_{\tilde{i},{\rm par}(\tilde{i})}}$ denote the coupling column variable set of agent $\tilde{i}$.
	\begin{figure}[h]
		\centering
		% Requires \usepackage{graphicx}
		\includegraphics[width=8cm]{parent_child.pdf}\\
		\caption{The coupling and uncoupling variables over a sparse graph. The  subscript elements $\tilde{(\cdot)}$ of set $\mathcal{S}$ or $\mathcal{R}$ denote different agents and the elements $(\cdot)$ in the brace denote the indices of coupling variables among different agents. } \label{parent_child_graph}
		\centering
	\end{figure}
\end{example}


\par  For each agent $\tilde{i}\in\mathcal V$ and $j\in \mathcal{R}_{\tilde{i},{\rm par}(\tilde{i})}$, define $$p_j^{\tilde{i}}(t)\triangleq\sum_{l<j,l\in J_{\tilde{i}}} M_{\{j,l\}}^{\tilde{i}} {v}_l^{\tilde{i}}(t+1)+\sum_{l>j,l\in J_{\tilde{i}}} M_{\{j,l\}}^{\tilde{i}} {v}_l^{\tilde{i}}(t).$$ 
Define  the step-size $\theta_j^{\tilde{i}}$ as
\begin{align}\label{theta_de}
\theta_j^{\tilde{i}} \in(0,\frac{1}{\sum_{\tilde{s}\in (\tilde{i}, {\rm ch}(\tilde{i}))}\|M_{\{j,:\}}^{\tilde{s}}\|_1}),\ j\in \mathcal{R}_{\tilde{i},{\rm par}(\tilde{i})}
\end{align}
where $M_{\{j,:\}}^{\tilde{s}}$ denotes the $j$th row of matrix $\overline{M}^{\tilde{s}}$.
\par The massage passed from agent $\tilde{r}\in {\rm ch}(\tilde{i})$ to its parent  $\tilde{i}\in \mathcal{V}$ is defined as
\begin{align}\label{distri_mes_syn}
\varpi_j^{\tilde{r},\tilde{i}}(t+1) \triangleq  \sum_{l\in J_{\tilde{r}}} M_{\{j,l\}}^{\tilde{r}} v_l^{\tilde{r}}(t+1) \quad \forall j \in \mathcal{S}_{\tilde{r},\tilde{i}}.
\end{align}
and $\varpi_j^{\tilde{r},\tilde{i}}(t+1)=0$ for $j\in \mathcal{R}_{\tilde{i},{\rm par}(\tilde{i})}\backslash \mathcal{S}_{\tilde{r},\tilde{i}}$.

For $\tilde i\in\mathcal V$ and $j\in \mathcal{R}_{\tilde{i},{\rm par}(\tilde{i})}$, the variable $v_j^{\tilde{i}}(t+1)$  is updated as
\begin{align}\label{vupdatesm}
&v_j^{\tilde{i}}(t+1)={\rm normal}\Big(v_j^{\tilde{i}}(t)-\theta_j^{\tilde{i}} \big[p_j^{\tilde{i}}(t)+\sum_{\tilde{r}\in {\rm ch}(\tilde{i})}\varpi_j^{\tilde{r},\tilde{i}}(t+1)\big]\Big).
\end{align}


The message sent from agent $\tilde i\in\mathcal V$ to its parent ${\rm par}(\tilde{i})$ is
\begin{align}\label{message2par}
\varpi_j^{\tilde{i},{\rm par}(\tilde{i})} =\sum_{l\in J_{\tilde{i}}} M_{\{j,l\}}^{\tilde{i}} v_l^{\tilde{i}}(t+1)+\sum_{\tilde{r} \in {\rm ch}(\tilde{i})}\varpi_j^{\tilde{r},\tilde{i}}(t+1),
\end{align}		
where $j\in \mathcal{S}_{\tilde{i},{\rm par}(\tilde{i})}$.

Define $\mathbf V = (V^{1},\cdots,V^{m})$. The distributed synchronous algorithm is given in Algorithm \ref{algo_sum}.



%\begin{remark}
%	The proposed distributed synchronous method includes two steps. In the first step, all uncoupling variables are updated by local information and messages passed from children, and variables coupling with children are transmitted to each child respectively. Then, the messages related to variables coupling with parent are passed to parent. The two-step design is to guarantee that the communicated messages in the second step are composed of updated uncoupling variables, which is important in the compact transformation in the next section.
%\end{remark}
%The subscript indices of $M_{\{j,l\}}^{\tilde{i}}$ follows that of the $M$ without causing ambiguity, which benefits expressing the relationships between different agents.
%For example, the information transmission of the proposed synchronous method applied to the network Fig. \ref{net_graph} related to the example problem Fig. \ref{parent_child_graph} is illustrated by Fig. \ref{info_tran}.
%
%\par Then, for variables in the variable sets $v_{j\in \mathcal{R}_{\tilde{i},{\rm par}(\tilde{i})}}$ and $v_{j\in \mathcal{S}_{\tilde{i},{\rm par}(\tilde{i})}}$, the proposed updating is, respectively,
%\begin{subequations}\label{distri_method}
%\begin{align}
%&v_j^{\tilde{i}}(t+1)={\rm normal}\big(v_j^{\tilde{i}}(t)-\theta_j^{\tilde{i}} \left[{l}_{\tilde{i}}(t)+\sum_{\tilde{r}\in {\rm ch}(\tilde{i})}\varpi_j^{\tilde{r},\tilde{i}}\right]\big),\notag\\
% &\qquad \qquad \qquad \qquad \qquad \qquad \forall j \in {\color{blue}\mathcal{R}_{\tilde{i},{\rm par}(\tilde{i})}}, \label{distri_rv}\\
%&v_j^{\tilde{i}}(t+1)=v_j^{{\rm par}(\tilde{i})}(t+1), \ \forall j \in {\color{blue}\mathcal{S}_{\tilde{i},{\rm par}(\tilde{i})}},\label{distri_sv}
%\end{align}
%\end{subequations}
%where $${l}_{\tilde{i}}(t)=\sum_{l\in J_{\tilde{i}}} M_{\{j,l\}}^{\tilde{i}}v_l^{\tilde{i}}(t),$$ and $\theta_j^{\tilde{i}}$ is defined as
%\begin{align}\label{theta_de}
%\theta_j^{\tilde{i}} \in(0,\frac{1}{\sum_{\tilde{s}\in (\tilde{i}, {\rm ch}(\tilde{i}))}\|M_{(j,:)}^{\tilde{s}}\|_1}),
%\end{align}
% where $M_{(j,:)}^{\tilde{s}}$ denotes the $j$th row elements of matrix $M^{\tilde{s}}$. The subscript indices of $M_{\{j,l\}}^{\tilde{i}}$ follows that of the $M$ without causing ambiguity, which benefits expressing the relationships between different agents.

%\par The massage passing from node $\tilde{i}$ to its parent node ${\rm par}(\tilde{i})$ in (\ref{distri_rv}) is defined as
%\begin{align}\label{distri_mes}
%\varpi_j^{\tilde{i},{\rm par}(\tilde{i})} = \sum_{l\in J_{\tilde{i}}} M_{\{j,l\}}^{\tilde{i}} v_{l}^{\tilde{i}}(t+1) \quad \forall j \in \mathcal{S}_{\tilde{i},{\rm par}(\tilde{i})}.
%\end{align}
%In addition, the massage passing from node $\tilde{i}$ to children nodes ${\rm ch}(\tilde{i})$ is
%$$v_j^{{\rm ch}(\tilde{i})}(t+1)=v_j^{\tilde{i}}(t+1), \forall j\in \mathcal{S}_{{\rm ch}(\tilde{i}),\tilde{i}}.$$
%%where $\mathcal{S}_{{\rm ch}(\tilde{i}),\tilde{i}}\subset \mathcal{R}_{\tilde{i},{\rm par}(\tilde{i})}$ by the set definitions.
%\par  A detailed summary of the proposed synchronous distributed approach is given in Algorithm \ref{algo_sum}. The information transmission of the proposed synchronous method applied to the network Fig. \ref{net_graph} related to the example problem Fig. \ref{parent_child_graph} is illustrated by Fig. \ref{info_tran}.

\begin{algorithm}[H]
	\caption{Distributed Synchronous Algorithm (DSA)}
	\label{algo_sum}
	\begin{algorithmic}[1]  %1	
		\State  \textbf{Initialization:}  Initialize  $v_j^{\tilde{i}}=v_0\in\mathbb R^p$ such that $\|v_0\|=1$ for all  $\tilde{i}\in\mathcal V$ and $j\in J_{\tilde{i}}$.
		\While{the stopping criteria is not satisfied}
		\For{$\tilde{i}=m$ to $\tilde{i}=1$}
		\For {$j\in \mathcal{R}_{\tilde{i},{\rm par}(\tilde{i})}$}
		\If {$j\in \mathcal{S}_{{\rm ch}(\tilde{i}),\tilde{i}}$}
		\State agent $\tilde{i}$ receives $\varpi_j^{\tilde{r},\tilde{i}}(t+1)$ (computed by \eqref{distri_mes_syn}) from its child $\tilde{r}\in {\rm ch}(\tilde{i})$.
		\EndIf
		\State   agent $\tilde{i}$ updates variable $v^{\tilde{i}}_j(t+1)$ following \eqref{vupdatesm}.
		\If{$j\in \mathcal{S}_{{\rm ch}(\tilde{i}),\tilde{i}}$}
		\State  agent $\tilde{i}$ sends $v_j^{\tilde{i}}(t+1)$ to its child $\tilde{r}\in{\rm ch}(\tilde{i})$, i.e., 
		\begin{align}\label{syn_sup}
			v_j^{\tilde{r}}(t+1)=v_j^{\tilde{i}}(t+1).
		\end{align}
		\EndIf
		\EndFor
		\For {$j\in \mathcal{S}_{\tilde{i},{\rm par}(\tilde{i})} $}
		%             \State $v^{\tilde{i}}_{\mathcal{R}_{\tilde{i},{\rm par}(\tilde{i})}}(t+1)=v^{\tilde{i}}_{\mathcal{R}_{\tilde{i},{\rm par}(\tilde{i})}}(t),$
		\If {$j\in \mathcal{S}_{{\rm ch}(\tilde{i}),\tilde{i}}$}
		\State agent $\tilde{i}$ receives information $\varpi_j^{\tilde{r},\tilde{i}}$ from its child $\tilde{r}\in {\rm ch}(\tilde{i})$.
		\EndIf
		
		%            \State  agent $\tilde{i}$ updates variable $v_j(t+1)$,
		%            \State
		%            \begin{align}\label{vsun}
		%             v_j^{\tilde{i}}(t+1)=v_j^{\tilde{i}}(t),
		%             \end{align}
		\State agent $\tilde{i}$ sends message $\varpi_j^{\tilde{i},{\rm par}(\tilde{i})}$ (computed by \eqref{message2par}) to its parent ${\rm par}(\tilde{i})$.
		
		%	\State $t=t+1.$		
		\EndFor
		%	\State \textbf{End}
		\EndFor
		\State $t\leftarrow t+1$.
		\EndWhile
	\end{algorithmic}
\end{algorithm}


The convergence performance of Algorithm 1 is provided in the following theorem, whose proof is given in the next section.
%\begin{assumption}\label{nodege_assum}
%	The term $\|\sum_{j=1}^{\left|J_{\tilde{i}}\right|} M_{(i,j)}^{\tilde{i}}v_j^{\tilde{i}}\|$ does not degenerate for all $\tilde{i}=1,\cdots,m$ in the updating. That is, all norms are always no less than a positive constant $\sigma>0$.
%\end{assumption}
\begin{theorem}\label{syn_theo}
	Let Assumption \ref{pro_assump} hold and $\{\mathbf V(t)\}$ be a sequence generated by Algorithm 1. Then $\mathbf V(t)$ converges to global optimal solutions to (\ref{non-convex_pro}) almost surely under random initialization as $t\rightarrow\infty$. 
\end{theorem}
%Equivalently, $V^{\tilde{i}'}(t)V^{\tilde{i}}(t)$, $\tilde i\in\mathcal V$, converges to a solution to problem \eqref{sdp_pro2} almost surely under random initialization as $t\rightarrow\infty$.
\begin{remark}
	Note that there may be several parents for each agent in practice. In this paper, we only consider the case that each agent owns one parent for convenience of analysis. However, the algorithm can be  easily extended to cases where agents have multiple parents. We  provide examples in simulations to demonstrate the cases of tree-structured graphs of which one agent has a unique parent and  graphs of which one agent may have multiple parents.	
	%{\color{blue}In addition, the child nodes and parent nodes in proposed algorithms are introduced to reduce communication cost and state the proposed method clearly, who actually perform equal roles, especially in distributed asynchronous method.}
\end{remark}
%% some remarks
%%%%\State Make use of the sparsity of $M$, transform the problem (\ref{non-convex_pro}) into the format as (\ref{distributed_pro}).\label{step_1}
%\par There are some explanations of the proposed synchronous algorithm that should be outlined.
\begin{remark}\label{syn_mark}
	%\begin{figure}
	%  \centering
	%  % Requires \usepackage{graphicx}
	%  \includegraphics[width=4cm]{syn_net.pdf}\\
	%  \caption{The multi-agent graph and variables sharing between different agents. The sharing variable indices of the optimization example in Fig. \ref{parent_child_graph} are displayed between agents. The coupling indices sets are $\mathcal{S}_{4{\rm par}(4)}=\{3\}, \mathcal{S}_{5{\rm par}{5}}=\{3\}, \mathcal{S}_{2{\rm par}(2)}=\{1,4\}, \mathcal{S}_{3{\rm par}(3)}=\{4\}.$} \label{net_graph}
	%\centering
	%\end{figure}
	%\begin{figure}
	%  \centering
	%  % Requires \usepackage{graphicx}
	%  \includegraphics[width=8cm]{structure.pdf}\\
	%  \caption{Variable transmission and updating of algorithm SM between different nodes. For the multi-agent graph in Fig. \ref{net_graph}, at time $t$, agents $4, 5$ send coupling messages to their parent $2$ in parallel and agents $2, 3$ send coupling messages to their parent $1$ in parallel. Each agent in the graph updates uncoupling local variables by local information in parallel. Agent $2$ updates the coupling variable $v_3(t+1)$ by the messages from $4,5$ and sends it down to them. Agent $1$ updates the coupling variable $v_1(t+1), v_4(t+1)$ by the passing messages and sends them down to $2, 3$. When all agents have updated their local variables by messages from parent node or local computations, one iteration of the method SM has been completed.}\label{info_tran}
	%\centering
	%\end{figure}
	Compared with the existing centralized works \cite{wang2017mixing,saddle_escape}, the proposed algorithm decentralizes the storage space and computational burden of large-scale SDP over different agents at the cost of network communication. In addition, the proposed algorithm is applicable for the scenario where global information is located on geographically separated agents such that centralized algorithms can not handle.% In the future distributed asynchronous method, the influence of communication on convergence performance will be further reduced.
	%\par The implement of the proposed method is a distributed computation, which can reduce the computation burden solving large-scale positive semi-definite problems. Most existing works solving SDPs are centralized, which are efficient for small and medium scale problems, while not scale well with the increase of variable dimensions. The proposed distributed method decentralize the storage space and computation over different nodes at the cost of network communication.
	%\end{itemize}
\end{remark}

\subsection{Distributed asynchronous optimization algorithm}
\par The synchronous algorithm given by Algorithm \ref{algo_sum} needs a global clock and the updating rate of variables is limited by the slowest agent. Whereas, asynchronous algorithms update variables by local clocks and allow communication time-delays, then the variables updating of one agent will not be limited by other agents. Hence, in this subsection, we provide a distributed asynchronous algorithm for solving problem (\ref{non-convex_pro}).
\par Let $T^{\tilde{i}}$ be the set of times at which agent $\tilde{i}$ updates local variable $V^{\tilde{i}}$. Noted that agent $\tilde{i}$ may not have access to the most recent value of other agents' variables.
% Thus, we have that, for all $t\in T^{\tilde{i}}$,
%\begin{equation}
%\left\{
%\begin{array}{rl}
%&\forall j\in \mathcal{R}_{\tilde{i},{\rm par}(\tilde{i})}, \ v^{\tilde{i}}_{j}(t+1)={\rm normal}\big(v_j^{\tilde{i}}(t)-\theta_j^{\tilde{i}} \left[l_{\tilde{i}}(t)+\sum_{\tilde{r}\in {\rm ch}(\tilde{i})}\varpi_j^{\tilde{r},\tilde{i}}(\tau_{\tilde{r}}^{\tilde{i}}(t))\right]\big)\\
%&\forall j\in \mathcal{S}_{\tilde{i},{\rm par}(\tilde{i})}, \ v^{\tilde{i}}_{j}(t+1)=v^{{\rm par}(\tilde{i})}_j(\tau_{{\rm par}(i)}^{\tilde{i}}(t)),
%\end{array}
%\right .
%\end{equation}
%where $l_{\tilde{i}}(t)=\sum_{l\in J_{\tilde{i}}} M_{\{j,l\}}^{\tilde{i}}v_l^{\tilde{i}}(t)$ and
Then, we define that the updating time of variables $v_j$ at parent or child node, $\tau_{j}^{\tilde{i}}(t)$, satisfies
\begin{align}\label{taulim}
0\leq \tau_{j}^{\tilde{i}}(t) \leq t, \quad \forall t\in T^{\tilde{i}},
\end{align}
where $j\in J_{{\rm ch}(\tilde{i})}\cup J_{{\rm par}(\tilde{i})}\cup J_{\tilde{i}}$. If $j\in \mathcal{R}_{\tilde{i},{\rm par}(\tilde{i})}$, then $\tau_{j}^{\tilde{i}}(t)=t$. 
The difference $(t-\tau_{j}^{\tilde{i}}(t))$ between the current time $t$ and $\tau_{j}^{\tilde{i}}(t)$ is viewed as a form of communication delay between agent $\tilde{i}$ and its parents or children.
\par For the communication delay between agents, we assume that the following condition holds.
{\begin{assumption}\label{total_asyn}
		(Partial Asynchronism) There exists a positive integer $B$ such that:
		\begin{itemize}
			\item For each agent $\tilde{i}$ and $t\geq 0$, at least one element of the set $\{t,t+1,\cdots,t+B-1\}$ belongs to $T^{\tilde{i}}$.
			\item There holds $${\rm max}\{0,t-B+1\}\leq \tau_{{j}}^{\tilde{i}}(t)\leq t,$$
			for all agent $\tilde{i}\in \mathcal{V}$ and ${j}\in \{1,\cdots,n\}$ and all $t\geq 0$.
		\end{itemize}
\end{assumption}}

\par In the distributed asynchronous Algorithm \ref{algo_asyn}, local variable $V^{\tilde{i}}$ is updated by the time-delayed messages communicated from neighbors and local information. For each agent $\tilde{i}$, define $$p_j^{\tilde{i}}(t)= \sum_{l\in J_{\tilde{i}}} M_{\{j,l\}}^{\tilde{i}} {v}_l^{\tilde{i}}(t),$$ and the step-size $\theta_j^{\tilde{i}}$ satisfies
\begin{align}\label{asyn_step}
\theta_j^{\tilde{i}}\in (0,\frac{1}{(1+B+nB)L}),
\end{align}
where $L= \max_{\tilde{i}\in \mathcal{V}}\{l_1,\cdots,l_m\}$, $l_{\tilde{i}}$ is the best Lipschitz constant of $\nabla \tilde{f}_{\tilde{i}}(V^{\tilde{i}})$ ($\tilde{f}_{\tilde{i}}(V^{\tilde{i}})$ was defined in \eqref{non-convex_pro}) and $B$ is defined in Assumption \ref{total_asyn}.
\par The massage passed from child $\tilde{r}\in {\rm ch}(\tilde{i})$ to parent  $\tilde{i}$ is defined as
\begin{align*}
\varpi_j^{\tilde{r},\tilde{i}}(\tau^{\tilde{i}}(t)) = \sum_{l\in J_{\tilde{r}}} M_{\{j,l\}}^{\tilde{r}} v_l^{\tilde{r}}(\tau_l^{\tilde{i}}(t)) \quad \forall j \in \mathcal{S}_{\tilde{r},\tilde{i}},
\end{align*}
and for variables with indices $j\in \mathcal{R}_{\tilde{i},{\rm par}(\tilde{i})}\backslash \mathcal{S}_{\tilde{r},\tilde{i}}$,
$\varpi_j^{\tilde{r},\tilde{i}}(\tau^{\tilde{i}}(t))=0$.
\par If $t\in T^{\tilde{i}}$, the variable $v_j^{\tilde{i}}(t+1)$ is updated as
\begin{subequations}\label{v_updateasm}
\begin{align}
&v_j^{\tilde{i}}(t+1)=\!{\rm normal}\Big(v_j^{\tilde{i}}(t)-\theta_j^{\tilde{i}} \big[p_j^{\tilde{i}}(t)+\!\sum_{\tilde{r}\in {\rm ch}(\tilde{i})}\!\varpi_j^{\tilde{r},\tilde{i}}(\tau^{\tilde{i}}(t))\big]\Big),\notag\\ 
& \qquad \qquad \qquad  j\in \mathcal{R}_{\tilde{i},{\rm par}(\tilde{i})}\label{asyn_r},\\
& v_j^{\tilde{i}}(t+1)=v_j^{{\rm par}(\tilde{i})}(\tau_{j}^{\tilde{i}}(t)),\quad j\in \mathcal{S}_{\tilde{i},{\rm par}(\tilde{i})}\label{asyn_s}.
\end{align}
\end{subequations}

For times $t \notin T^{\tilde{i}}$, the variable $v_j^{\tilde{i}}$ is unchanged,

$$v_j^{\tilde{i}}(t+1)=v_j^{\tilde{i}}(t), \ \forall j\in J_{\tilde{i}}.
$$
\par The message sent from agent $\tilde{i}\in\mathcal{V}$ to its parent ${\rm par}(\tilde{i})$ is
\begin{align}\label{mess_asyn}
\varpi_j^{\tilde{i},{\rm par}(\tilde{i})} \!=\! \sum_{l\in J_{\tilde{i}}} M_{\{j,l\}}^{\tilde{i}} v_{l}^{\tilde{i}}(t+1)+\sum_{\tilde{r} \in {\rm ch}(\tilde{i})}\varpi_j^{\tilde{r},\tilde{i}}(\tau^{\tilde{i}}(t)),\ j\in J_{\tilde{i}}.
\end{align}	
\begin{algorithm}[H]
	\caption{Distributed Asynchronous Algorithm (DAA) - from the view of agent $\tilde{i}$}
	\label{algo_asyn}
	\begin{algorithmic}[1]  %1	
		\State \textbf{Initialization:} Initialize  $v_j^{\tilde{i}}=v_0\in\mathbb R^p$ such that $\|v_0\|=1$ for all  $\tilde{i}\in\mathcal V$ and $j\in J_{\tilde{i}}$.
		\While{the stopping criteria is not satisfied}
		\State for each agent $\tilde{i}$, keep receiving information $\varpi_j^{\tilde{r},\tilde{i}}(\tau^{\tilde{i}}(t))$ from children and receiving information  $v_j^{{\rm par}(\tilde{i})}(\tau_{j}^{\tilde{i}}(t))$ from parent.
		\If{$t \in T^{\tilde{i}}$}
		\For{$j\in J_{\tilde{i}}$}		
		\State agent $\tilde{i}$ updates variable $v_j^{\tilde{i}}(t+1)$ following (\ref{v_updateasm}).
		\If{$j\in \mathcal{S}_{\tilde{i},{\rm par}(\tilde{i})}$}
		\State agent $\tilde{i}$ sends message $\varpi_j^{\tilde{i},{\rm par}(\tilde{i})}$ (computed by (\ref{mess_asyn})) to its parent,
		\EndIf
		\If{$j\in \mathcal{S}_{\tilde{l},\tilde{i}}$, $\tilde{l}\in {\rm ch}(\tilde{i})$}
		\State agent $\tilde{i}$ sends local variable $v_j^{\tilde{i}}(t+1)$ to each child $\tilde{l}$ that has coupling variable $v_j^{\tilde{i}},j\in \mathcal{S}_{\tilde{l},\tilde{i}}$.
		\EndIf
		\EndFor
		\EndIf
		\State $t\leftarrow t+1$.
		\EndWhile
	\end{algorithmic}
\end{algorithm}


%The stopping criterion can have many choices. One of most common stopping criterions is when the difference of $V_{J_{\tilde{i}}}^{\tilde{i}}$ along time $t$ is smaller than some given error. For space limitation, we omit the iterative notations $t+1$ and $t$ in the variable updating of following summarization.

%%a predefined stopping criterion is satisfied.
%\begin{figure}
%  \centering
%  % Requires \usepackage{graphicx}
%  \includegraphics[width=7cm]{topo_buf.pdf}\\
%  \caption{The multi-agent graph with {\color{blue}unreliable communication} and variables sharing between different agents of optimization example in Fig. \ref{parent_child_graph}. }\label{tree_buf}
%\centering
%\end{figure}
%\begin{figure}
%  \centering
%  % Requires \usepackage{graphicx}
%  \includegraphics[width=8cm]{asyn_structure.pdf}\\
%  \caption{Variable transmission and updating of algorithm ASM between different nodes. For the multi-agent graph in Fig. \ref{net_graph}, at time $t\in T^2$, agent $2$ reads coupling messages from local buffer, which are transmitted from children agents $4, 5$ and are delayed by unreliable communication, and updates the coupling variable $v_3(t+1)$. Agent $2$ sends coupling message to parent agent $1$ by the unreliable communication. In addition, agent $2$ updates coupling variables $v_1(t+1), v_4(t+1)$ by the messages in local buffer passing from parent agent $1$.  Agent $2$ sends $v_3(t+1)$ down to the data buffer of agents $4,5$.}\label{infoasyn_tran}
%\centering
%\end{figure}
\par Before providing the convergence performance of DAA, we need one additional assumption, which is vital in the transformation of proposed algorithm.
\begin{assumption}\label{asyn_assump}
	Each element of matrix $M$ is only accessible to one agent, which implies that for each agent $\tilde{i}\in \mathcal{V}$, $M^{\tilde{i}}_{\{l,k\}}=M_{(l,k)}$ holds for $(l,k)\in J_{\tilde{i}}\times J_{\tilde{i}}$.
\end{assumption}
%\begin{remark}
%	Recall that $M^{\tilde{i}}_{\{l,k\}}=\overline{M}^{\tilde{i}}_{(l,k)}$ and $\sum_{\tilde{i}=1}^m \overline{M}^{\tilde{i}}=M$. Then, with Assumption \ref{asyn_assump}, we obtain that each element of matrix $M$ is only accessible to one agent.
%\end{remark}
\par Next, the convergence performance of DAA is provided in the following theorem, whose analysis is shown in the section \ref{ASM_sec}.
\begin{theorem}\label{asyn_theo}
	Under Assumptions \ref{pro_assump}-\ref{asyn_assump}, the sequence $\{\mathbf V(t)\}$ generated by DAA converges to critical points to (\ref{non-convex_pro}) as $t\rightarrow\infty$. 
	%Equivalently, $V^{\tilde{i}'}(t)V^{\tilde{i}}(t)$, $\tilde i\in\mathcal V$, converges to a critical solution to problem \eqref{sdp_pro2} as $t\rightarrow\infty$.
\end{theorem}
\begin{remark}
	In the implementation of DAA, each agent receives communication information from its neighbors and stores it in local buffer. The local received data may be out-of-date due to time-delays. Each agent $\tilde{i}\in \mathcal{V}$ updates local variables using data in local buffer at the time $t\in T^{\tilde{i}}$ and does not have to wait for the point when other local communicating messages become available. It allows some agents to compute faster and execute more iterations than others.
	
	%\item The structure of clique tree can influence the tradeoff between computation and communication cost, which can be adjusted by the decentralization of optimization problem and the communication topological structure, such as the choice of roots in agent trees. The detailed discussions are left for future works.
	%\end{itemize}
\end{remark}
\section{Theoretical analysis}\label{proof_sec}
In this section, we present theoretical proofs for the convergence properties of proposed distributed synchronous and asynchronous algorithms, respectively.

%In addition, the coupling vectors between different agents $v_j^{\tilde{i}}=v_j^{{\rm par}(\tilde{i})}$ for index $j\in \mathcal{S}_{\tilde{i},{\rm par}(\tilde{i})}$. 
\subsection{Convergence analysis for synchronous algorithm}
\par Firstly, we  develop a compact form of the proposed synchronous algorithm containing all agent's updates.

For any $j\in\{1,\ldots,n\}$, let $v_j \triangleq v_j^{\tilde i}$, where  $j\in \mathcal{R}_{\tilde{i},{\rm par}(\tilde{i})}$ and $v_j^{\tilde i}$ is a column vector of the variable of agent $\tilde i\in\mathcal V$. Then we define the global variable  
\begin{align}\label{V_def}
V=\left[v_1,\cdots,v_n\right]\in \mathbb{R}^{p \times n}.
\end{align}

%\begin{align}\label{V_de}
%{V=\mathbf{Res}[v_{1\in \mathcal{R}_{m,{\rm par}(m)}},\cdots, v_{n\in \mathcal{R}_{1,{\rm par}(1)}}]},??
%\end{align}
%where the notation $\mathbf{Res}$ means rearranging the uncoupling column variables according to increasing indices in the union set $\cup_{\tilde{i}=1}^m \mathcal{R}_{\tilde{i},{\rm par}(\tilde{i})}$.
%The proposed algorithm applies to the cases when a variable is coupling between more than two agents. However, the proof is tedious.
\begin{remark}
	 For $j\in \mathcal{S}_{\tilde{i},{\rm par}(\tilde{i})}$, by the updating design \eqref{syn_sup}, there is a parent of $\tilde{i}$, $\tilde{j}\in {\rm par}(\tilde{i})$, such that $j\in \mathcal{R}_{\tilde{j},{\rm par}(\tilde{j})}$ and $v_j^{\tilde{i}}(t)=v_j^{\tilde{j}}(t)$. Hence, in the following analysis, for convenience, we consider the global variable $V$ instead of $\mathbf{V}$.
\end{remark}
 \par Without loss of generality,  we make the following assumptions in the analysis.
 \begin{assumption}\label{SR_indice}
 	Take any  $\tilde{i}\in\mathcal V$. Indices
 	$j\in \mathcal{R}_{{\rm ch}(\tilde{i}),\tilde{i}}$, $l\in \mathcal{R}_{\tilde{i},{\rm par}(\tilde{i})}$ and $p\in\mathcal{S}_{\tilde{i},{\rm par}(\tilde{i})}$ satisfy that $p>l>j$.
 \end{assumption}
 
 Notice that for each column vector $v_j$ with index $j\in \mathcal{S}_{\tilde{i},{\rm par}(\tilde{i})}$, there must exist one agent $\tilde{j}$ such that $j\in \mathcal{R}_{\tilde{j},{\rm par}(\tilde{j})}$.  
\begin{assumption}\label{twoshare_assu}
	 We assume that there is no shared variables between agents ${\rm ch}(\tilde{i})$ and ${\rm par}(\tilde{i})$ for any $\tilde{i}\in\mathcal V$. That is,
 $\mathcal{S}_{{\rm ch}(\tilde{i}),\tilde{i}}\subset \mathcal{R}_{\tilde{i},{\rm par}(\tilde{i})}$ for all $\tilde{i}\in\mathcal V$.
\end{assumption}
\par Note that Assumptions \ref{SR_indice} and \ref{twoshare_assu} are not needed in the proposed synchronous algorithm, which is developed for the convenience of proof. 

\par Define $\Theta\in\mathbb{R}^{n\times n}$ as a diagonal matrix with diagonal elements,
\begin{align}\label{dia_theta}
\Theta_{(j,j)}=\theta_j^{\tilde{i}} \quad {\rm if} \ j\in \mathcal{R}_{\tilde{i},{\rm par}(\tilde{i})}.
\end{align}

Then, under the above assumptions, we obtain the following result.
\begin{lemma}\label{cmp_lem}
	Under Assumptions \ref{SR_indice} and \ref{twoshare_assu}, the distributed synchronous algorithm is equivalent to
	\begin{align}\label{distri_mapping}
	v_j(t+1)=&{\rm normal}\big(v_j(t)-\Theta_{(j,j)} \mathbf{g}_j(t)\big), \ \forall \ j\in \{1,\cdots,n\},
	\end{align}
	where $\mathbf{g}_j(t)=\sum_{l\in  \{1,\cdots,j-1\}} M_{(j,l)} v_l(t+1)+\sum_{l\in \{j+1,\cdots,n\}} M_{(j,l)} v_l(t)$.
\end{lemma}
\par\textbf{Proof:}
For variables $v_j^{\tilde{i}}$ of agent $\tilde{i}$ such that $j\in \mathcal{R}_{\tilde{i},{\rm par}(\tilde{i})}$, we substitute (\ref{distri_mes_syn}) to the updating (\ref{vupdatesm}) and get
\begin{align}\label{vr_up}
v_j^{\tilde{i}}(t+1)
={\rm normal}\Big(v_j^{\tilde{i}}(t)-\theta_j^{\tilde{i}} \mathbf{g}^{\tilde{i}}_j(t)\Big),\ j\in \mathcal{R}_{\tilde{i},{\rm par}(\tilde{i})}
\end{align}
where 
\begin{align}\label{PHI}
\mathbf{g}^{\tilde{i}}_j(t)=&\sum_{l\in  \{1,\cdots,j-1\}} M^{\tilde{i}}_{\{j,l\}} {v}_l^{\tilde{i}}(t+1)\notag\\
&+\sum_{l\in \{j+1,\cdots,n\}} M^{\tilde{i}}_{\{j,l\}} {v}_l^{\tilde{i}}(t)\notag\\
&+\sum_{\tilde{r}\in {\rm ch}(\tilde{i})}\sum_{l\in J_{\tilde{r}}} M_{\{j,l\}}^{\tilde{r}} v_{l}^{\tilde{r}}(t+1).
\end{align}
%%%Then $\bar{M}^{\tilde{i}}=\sum_{j\in \phi_i}\bar{M}^j$ and $\sum_q^c \bar{M}^q=M$
\par The condition $j\in \mathcal{R}_{\tilde{i},{\rm par}(\tilde{i})}$ holds in the whole process of proof. For convenience, in the following analysis, we omit this condition.
\par {Since} $M_{\{j,l\}}^{\tilde{r}}=\overline{M}_{\{j,l\}}^{\tilde{r}}$, for $(j,l)\in J_{\tilde{r}}\times J_{\tilde{r}}$ and $\tilde{r}\in \mathcal{V}$, we have
\begin{align}\label{msum}
&\sum_{\tilde{r}\in \{\tilde{i},{\rm ch}(\tilde{i})\}} {M}^{\tilde{r}}_{\{j,l\}}=\sum_{\tilde{r}=1}^m \overline{M}^{\tilde{r}}_{\{j,l\}}=M_{(j,l)},\\
& j\in \mathcal{R}_{\tilde{i},{\rm par}(\tilde{i})}\subset  \cup_{\tilde{r}\in \{\tilde{i},{\rm ch}(\tilde{i})\}} J_{\tilde{r}}, l\in \cup_{\tilde{r}\in \{\tilde{i},{\rm ch}(\tilde{i})\}}J_{\tilde{r}},\notag
\end{align}
where the first equality holds because for agent $\tilde{j}\in \{1,\cdots,m\}\backslash\{\tilde{i},{\rm ch}(\tilde{i})\}$, $M_{\{j,l\}}^{\tilde{j}}=0$, where $j\in \mathcal{R}_{\tilde{i},{\rm par}(\tilde{i})}\subset  \cup_{\tilde{r}\in \{\tilde{i},{\rm ch}(\tilde{i})\}} J_{\tilde{r}}, l\in \cup_{\tilde{r}\in \{\tilde{i},{\rm ch}(\tilde{i})\}}J_{\tilde{r}}$.
\par By \eqref{msum}, the term $\sum_{\tilde{r}\in {\rm ch}(\tilde{i})}\sum_{l\in J_{\tilde{r}}} M_{\{j,l\}}^{\tilde{r}} v_{l}^{\tilde{r}}(t+1)$ in \eqref{PHI} satisfies
\begin{align}\label{lastterm_trans}
&\sum_{\tilde{r}\in {\rm ch}(\tilde{i})}\sum_{l\in J_{\tilde{r}}} M_{\{j,l\}}^{\tilde{r}} v_{l}^{\tilde{r}}(t+1) \notag\\
=&\sum_{\tilde{r}\in {\rm ch}(\tilde{i})}\!\sum_{l\in \mathcal{S}_{\tilde{r},\tilde{i}}}\! M_{\{j,l\}}^{\tilde{r}} v_{l}^{\tilde{r}}(t+1)\!+\!\sum_{\tilde{r}\in {\rm ch}(\tilde{i})}\!\sum_{l\in \mathcal{R}_{\tilde{r},\tilde{i}}}\! M_{\{j,l\}}^{\tilde{r}} v_{l}^{\tilde{r}}(t+1)\notag\\
=&\sum_{\tilde{r}\in {\rm ch}(\tilde{i})}\!\sum_{l\in \mathcal{S}_{\tilde{r},\tilde{i}}}\! M_{\{j,l\}}^{\tilde{r}} v_{l}^{\tilde{r}}(t+1)\!+\!\sum_{\tilde{r}\in {\rm ch}(\tilde{i})}\!\sum_{l\in \mathcal{R}_{\tilde{r},\tilde{i}}}\! M_{\{j,l\}}^{\tilde{r}} v_{l}(t+1)\notag\\
%&=\sum_{\tilde{r}\in {\rm ch}(\tilde{i})}\sum_{l\in \mathcal{S}_{\tilde{r},\tilde{i}}} M_{\{j,l\}}^{\tilde{r}} v_{l}^{\tilde{r}}(t+1)+\sum_{l\in \mathcal{R}_{\tilde{r}\in {\rm ch}(\tilde{i}),\tilde{i}}} M_{(j,l)} v_{l}(t+1)\notag\\
=&\sum_{\tilde{r}\in {\rm ch}(\tilde{i})}\sum_{l\in \mathcal{S}_{\tilde{r},\tilde{i}}\cap \{l<j\}} M_{\{j,l\}}^{\tilde{r}} v_{l}^{\tilde{r}}(t+1)\notag\\
&+\sum_{\tilde{r}\in {\rm ch}(\tilde{i})}\sum_{l\in \mathcal{S}_{\tilde{r},\tilde{i}}\cap \{l>j\}} M_{\{j,l\}}^{\tilde{r}} v_{l}^{\tilde{r}}(t)\notag\\
&+\sum_{l\in \mathcal{R}_{\tilde{r}\in {\rm ch}(\tilde{i}),\tilde{i}}} M_{(j,l)} v_{l}(t+1)
\end{align}
where the last equality holds because variables $v_l^{\tilde{r}}$ with indices satisfying $l>j,j\in \mathcal{R}_{\tilde{i},{\rm par}(\tilde{i})}$ are not updated by the algorithm design and $M^{\tilde{i}}_{\{j,l\}}=0$ for $l\in \mathcal{R}_{\tilde{r}{\tilde{i}}}$.
\par Then, by substituting \eqref{lastterm_trans} to $\mathbf{g}_j^{\tilde{i}}$ in \eqref{PHI}, we obtain
\begin{align}\label{Phi_com}
\mathbf{g}^{\tilde{i}}_j(t)=\zeta_j(t+1)+\iota_j
(t)
\end{align} 
where 
\begin{align*}
\zeta_j(t+1)=&\sum_{l\in  \{1,\cdots,j-1\}} M^{\tilde{i}}_{\{j,l\}} {v}_l^{\tilde{i}}(t+1)\\
&+\sum_{\tilde{r}\in {\rm ch}(\tilde{i})}\sum_{l\in \mathcal{S}_{\tilde{r},\tilde{i}}\cap \{l<j\}} M_{\{j,l\}}^{\tilde{r}} v_{l}^{\tilde{r}}(t+1)\\
&+\sum_{l\in \mathcal{R}_{\tilde{r}\in {\rm ch}(\tilde{i}),\tilde{i}}} M_{(j,l)} v_{l}(t+1),
\end{align*} 
and 
\begin{align*}
\iota_j
(t)=&\sum_{l\in \{j+1,\cdots,n\}} M^{\tilde{i}}_{\{j,l\}} {v}_l^{\tilde{i}}(t)\\
&+\sum_{\tilde{r}\in {\rm ch}(\tilde{i})}\sum_{l\in \mathcal{S}_{\tilde{r},\tilde{i}}\cap \{l>j\}} M_{\{j,l\}}^{\tilde{r}} v_{l}^{\tilde{r}}(t).
\end{align*}
\par  Then, we will discuss variables $\zeta_j(t+1)$ and $\iota_j(t)$ respectively. 
\par (1) Consider $\zeta_j$ composed of variables with indices $l$ in the set $\{l|l<j,j\in \mathcal{R}_{\tilde{i},{\rm par}(\tilde{i})}\}$. Let $MR\triangleq\sum_{l\in \mathcal{R}_{\tilde{i},{\rm par}(\tilde{i})}\cap \{l<j\}} M^{\tilde{i}}_{\{j,l\}} {v}_l^{\tilde{i}}(t+1)$ and $MS\triangleq\sum_{\tilde{r}\in {\rm ch}(\tilde{i})}\sum_{l\in \mathcal{S}_{\tilde{r},\tilde{i}}\cap \{l<j\}} M_{\{j,l\}}^{\tilde{r}} v_{l}^{\tilde{r}}(t+1)$. The sum of $MS$ and $MR$ is
\begin{align}\label{lj2}
&MS+MR\notag\\
=&\sum_{\tilde{r}\in \{\tilde{i},{\rm ch}(\tilde{i})\}}\sum_{l\in  \mathcal{R}_{\tilde{i},{\rm par}(\tilde{i})} \cap \{l<j\}} M_{\{j,l\}}^{\tilde{r}} v_{l}^{\tilde{r}}(t+1)\notag\\
=&\sum_{\tilde{r}\in \{\tilde{i},{\rm ch}(\tilde{i})\}}\sum_{l\in  \mathcal{R}_{\tilde{i},{\rm par}(\tilde{i})} \cap \{l<j\}} M^{\tilde{r}}_{\{j,l\}}v_l(t+1)\notag\\
=&\sum_{l\in  \mathcal{R}_{\tilde{i},{\rm par}(\tilde{i})} \cap \{l<j\}} M_{(j,l)}v_l(t+1).
%&\sum_{l\in \mathcal{R}_{\tilde{i},{\rm par}(\tilde{i})}\cap\{l<j\}}M_{\{j,l\}}^{\tilde{i}}v_l^{\tilde{i}}(t+1)=\sum_{l\in \mathcal{R}_{\tilde{i},{\rm par}(\tilde{i})}\cap\{l<j\}}M_{\{j,l\}}^{\tilde{i}}v_l(t+1)
 \end{align}
where the first equality holds because of the condition $\mathcal{S}_{{\rm ch}(\tilde{i}),\tilde{i}}\subseteq \mathcal{R}_{\tilde{i},{\rm par}(\tilde{i})}$, and the last equality holds due to the relationship (\ref{msum}).
\par By Assumption \ref{SR_indice}, any index $l$ in $\mathcal{R}_{\tilde{r}\in {\rm ch}(\tilde{i}),\tilde{i}}$ satisfies $l<j$ for $j\in \mathcal{R}_{\tilde{i},{\rm par}(\tilde{i})}$. Then, with \eqref{lj2}, 
\begin{align}\label{lleqj}
&\zeta_j(t+1)\notag \\
=&\sum_{l\in \mathcal{R}_{\tilde{r}\in {\rm ch}(\tilde{i}),\tilde{i}}}\! M_{(j,l)} v_{l}(t+1)\!+\!\sum_{l\in \mathcal{R}_{\tilde{i},{\rm par}(\tilde{i})}\cap\{l<j\}} \! M_{(j,l)}v_l(t+1)\notag\\
=&\sum_{l\in \{l<j,j\in \mathcal{R}_{\tilde{i},{\rm par}(\tilde{i})}\}} M_{(j,l)}v_l(t+1).
\end{align}
\par (2) Consider $\iota_j$ composed of variables with indices $l$ in the set $\{l|l>j,j\in \mathcal{R}_{\tilde{i},{\rm par}(\tilde{i})}\}$. The term $\sum_{l>j} M^{\tilde{i}}_{\{j,l\}} {v}_l^{\tilde{i}}(t)$ in $\iota_j(t)$ satisfies
\begin{align}\label{lgeqj1}
&\sum_{l\in \{1,\cdots,n\}\cap\{l>j\}} M^{\tilde{i}}_{\{j,l\}} {v}_l^{\tilde{i}}(t)\notag \\
=&\sum_{l\in \mathcal{R}_{\tilde{i},{\rm par}(\tilde{i})}\cap \{l>j\}} M_{\{j,l\}}^{\tilde{i}} v_{l}^{\tilde{i}}(t)+\!\sum_{l\in \mathcal{S}_{\tilde{i},{\rm par}(\tilde{i})}\cap \{l>j\}} M_{\{j,l\}}^{\tilde{i}} v_{l}^{\tilde{i}}(t).
\end{align}
\par It follows from a similar analysis in \eqref{lj2} that the sum of $\sum_{\tilde{r}\in {\rm ch}(\tilde{i})}\sum_{l\in \mathcal{S}_{\tilde{r},\tilde{i}}\cap \{l>j\}} M_{\{j,l\}}^{\tilde{r}} v_{l}^{\tilde{r}}(t)$ in $\iota_j(t)$ and $\sum_{l\in \mathcal{R}_{\tilde{i},{\rm par}(\tilde{i})}\cap \{l>j\}} M_{\{j,l\}}^{\tilde{i}} v_{l}^{\tilde{i}}(t)$ in (\ref{lgeqj1}) is
\begin{align}\label{Rlgeqj}
&\!\sum_{\tilde{r}\in {\rm ch}(\tilde{i})}\sum_{l\in \mathcal{S}_{\tilde{r},\tilde{i}}\cap \{l>j\}}\! M_{\{j,l\}}^{\tilde{r}}  v_{l}^{\tilde{r}}\!(t)+\!\sum_{l\in \mathcal{R}_{\tilde{i},{\rm par}(\tilde{i})}\cap \{l>j\}}\! M_{\{j,l\}}^{\tilde{i}} \!v_{l}^{\tilde{i}}\!(t)\notag\\
=&\sum_{l\in  \mathcal{R}_{\tilde{i},{\rm par}(\tilde{i})} \cap \{l>j\}} M_{(j,l)}v_l(t).
\end{align}
%&\sum_{\tilde{r}\in (\tilde{i},{\rm ch}(\tilde{i}))}\sum_{l\in  \mathcal{R}_{\tilde{i},{\rm par}(\tilde{i})} \cap \{l>j\}} M_{\{j,l\}}^{\tilde{r}} v_{l}^{\tilde{r}}(t)=\sum_{l\in  \mathcal{R}_{\tilde{i},{\rm par}(\tilde{i})} \cap \{l>j\}} M_{\{j,l\}}v_l(t).
\par Then, consider the term $\sum_{l\in \mathcal{S}_{\tilde{i},{\rm par}(\tilde{i})}\cap \{l>j\}} M_{\{j,l\}}^{\tilde{i}} v_{l}^{\tilde{i}}(t)$ in (\ref{lgeqj1}).
% Because for $l\in \mathcal{S}_{\tilde{i},{\rm par}(\tilde{i})}$, $M_{\{j,l\}}^{\tilde{i}}=0$ by . We have
%\begin{align}\label{lgeqj}
%\sum_{l\in  \mathcal{R}_{\tilde{i},{\rm par}(\tilde{i})} \cap \{l>j\}} M_{\{j,l\}}v_l(t)=\sum_{l>j} M_{\{j,l\}}v_l(t).
%\end{align}
Because all column variables are initialized as a same value and the condition $\mathcal{S}_{{\rm ch}(\tilde{i}),\tilde{i}}\subset \mathcal{R}_{\tilde{i},{\rm par}(\tilde{i})}$ in Assumption \ref{twoshare_assu}, $v_l^{\tilde{i}}(t)=v_l^{{\rm par}(\tilde{i})}(t)=v_l(t), \ \forall  l\in \mathcal{S}_{\tilde{i},{\rm par}(\tilde{i})}$, by Algorithm 1. By (\ref{msum}), we have, for $j\in \mathcal{R}_{\tilde{i},{\rm par}(\tilde{i})}$,
\begin{align}\label{Slgeqj}
&\sum_{l\in \mathcal{S}_{\tilde{i},{\rm par}(\tilde{i})}\cap \{l>j\}}M_{\{j,l\}}^{\tilde{i}}v_l^{\tilde{i}}(t)\notag\\
%=&\sum_{l\in \mathcal{S}_{\tilde{i},{\rm par}(\tilde{i})}\cap \{l>j\}}M_{(j,l)}v^{\tilde{i}}_l(t)\notag\\
=&\sum_{l\in \mathcal{S}_{\tilde{i},{\rm par}(\tilde{i})}\cap \{l>j\}}M_{(j,l)}v_l(t),
\end{align}
where the equality holds because for each child $\tilde{r}\in {\rm ch}(\tilde{i})$, $M^{\tilde{r}}_{j,l}=0$, for $j\in \mathcal{R}_{\tilde{i},{\rm par}(\tilde{i})}$, $l\in \mathcal{S}_{\tilde{i},{\rm par}(\tilde{i})}$.
\par Then, with (\ref{Rlgeqj}) and (\ref{Slgeqj}), 
\begin{align}\label{lgeqj}
&\iota_j(t)\notag\\
=&\sum_{l\in  \mathcal{R}_{\tilde{i},{\rm par}(\tilde{i})} \cap \{l>j\}} M_{(j,l)}v_l(t)+\sum_{l\in \mathcal{S}_{\tilde{i},{\rm par}(\tilde{i})}\cap \{l>j\}}M_{(j,l)}v_l(t)\notag\\
=&\sum_{l\in \{j+1,\cdots,n\}} M_{(j,l)}v_l(t).
\end{align}

%In addition, for $\sum_{\tilde{r}\in {\rm par}(\tilde{i})}\sum_{l\in \mathcal{S}_{\tilde{i}\tilde{r}}} M^{\tilde{r}}_{(i,l)} {v}^{\tilde{r}}_l(t)=\sum_{\tilde{r}\in {\rm par}(\tilde{i})}\sum_{l\in \mathcal{S}_{\tilde{i}\tilde{r}}} M_{(i,l)} v_l(t)$. Since $\mathcal{S}_{\tilde{i}\tilde{r}}\subset\{l>j\}$ and all variables are initialized as a same value, then
%\begin{align}
%\sum_{\tilde{r}\in {\rm par}(\tilde{i})}\sum_{l\in \mathcal{S}_{\tilde{i}\tilde{r}}} M_{(i,l)} v_l(t)+\sum_{l\in  \mathcal{R}_{\tilde{i},{\rm par}(\tilde{i})} \cap \{l>j\}} M_{\{j,l\}}v_l(t)=\sum_{l>j}M_{j,l}v_l(t)
%\end{align}
\par  Hence, by \eqref{Phi_com}, (\ref{lleqj}) and  (\ref{lgeqj}), the updating (\ref{vr_up}) of agent $\tilde{i}$ for any vector variable with index $j\in \mathcal{R}_{\tilde{i},{\rm par}(\tilde{i})}$ is 
\begin{align}\label{v_comp_end}
&v_j^{\tilde{i}}(t+1)={\rm normal}\big(v^{\tilde{i}}_j(t)-\Theta_{(j,j)}\mathbf{g}^{\tilde{i}}_j(t)\big),
\end{align}
where $\mathbf{g}_j^{\tilde{i}}(t)=\sum_{l\in  \{l<j,j\in \mathcal{R}_{\tilde{i},{\rm par}(\tilde{i})}\}} M_{(j,l)} v_l(t+1)+\sum_{l\in \{j+1,\cdots,n\}\cap \{j\in \mathcal{R}_{\tilde{i},{\rm par}(\tilde{i})} \}} M_{(j,l)} v_l(t)$ and $\Theta_{(j,j)}=\theta_j^{\tilde{i}}$, which is defined in (\ref{theta_de}). Since \eqref{v_comp_end} holds for each agent $\tilde{i}$, we obtain the desire result \eqref{distri_mapping} with \eqref{V_def}. 
%Besides, initializing local {\color{red} coupling variables} of all agents as same value is to make (\ref{s_v}) holds for $t=0$.%Since the diagonal element $m_{jj}=0$, the term $\sum_{l=1}^n m_{jl} {v}_l$ is independent of variable $v_j$ and can be considered as some constant in the updating of $v_j^{\tilde{i}}$.
%\begin{align*}
%v_j^{\tilde{i}}(t+1)=&{\rm normalize}\big(v_j^{\tilde{i}}(t)-\theta_j^{\tilde{i}} \quad {\rm constant}\big).
%\end{align*}
%
% For each column vector $v_j$ in $V$, there must exist one node $\tilde{i}$ such that $j\in \mathcal{R}_{\tilde{i},{\rm par}(\tilde{i})}$.
% By collecting uncoupling column variables of all agents, we can get the expected compact updating of the distributed synchronous algorithm.
%\begin{align*}
%V'(t+1)=D_y^{-1}(V'(t)-\Theta M V'(t)),
%\end{align*}
%where $\Theta\in\mathbb{R}^{n\times n}$ is a diagonal matrix with diagonal element $\Theta_{(j,j)}$, $\Theta_{(j,j)}=\theta_j^{\tilde{i}}$ if $j\in \mathcal{R}_{\tilde{i},{\rm par}(\tilde{i})}$.
$\hfill\blacksquare$
%\begin{remark}
%\end{remark}
%%%%the step size limit can also be 1/max_i\|c_i\|2   ???
%%%non-optimal critical points are unstable (no local optima, only saddle points)
%%%algorithm is diffeomorphism
%%%mapping is strictly decreasing and ((objective value is lower bounded) ??????)
\par Next, we will discuss the convergence properties of proposed distributed synchronous algorithm. Because the optimization problem (\ref{non-convex_pro}) is a non-convex minimization problem, there may exist several local minima and saddle points, which are regarded as major obstacles for global minima search over continuous spaces. For the semi-definite programming like (\ref{sdp_pro}), it has been known that the low-rank transformed problem (\ref{non-convex_pro}) has no local optima except the global ones if $p > \sqrt{2n}$ \cite{BM_smooth}. \par Thus, the main work is to discuss whether the proposed algorithm escapes strict saddle points and converges to global optimal solutions. At first, we provide the definition of unstable critical points. 
 Denote the update of $V$ generated by DSA as $V (t+1)= h_{SM}(V(t))$. 
\begin{definition}\label{cri_de2}
	Define unstable critical points as the set of critical points where the Jacobian of variable updating $h_{SM}(V)$ has at least a single eigenvalue with magnitude greater than one\upcite{saddle_escape},
	$$\mathcal{A}_g^*=\{V: h_{SM}(V)=V, \max_i\left|\lambda_i(Dh_{SM}(V))\right|>1\}.$$
%	be the set of critical points where the Jacobian of variable updating $h_{SM}(V)$ has at least a single eigenvalue with magnitude greater than one. These are the unstable critical points.
\end{definition}
%\par Then, the set of unstable critical points is defined as
%$$\mathcal{A}_g^*=\{V: h_{SM}(V)=V, \max_i\left|\lambda_i(Dh_{SM}(V))\right|>1\}.$$
%\par The proposed method can be considered as a type of gradient descent methods. To be specific, the update (\ref{distri_mapping}) can be written as
%\begin{align*}
%V(t+1)&=normalize (V'(t)-\Theta \nabla f(V)),\\
%D_y V(t+1)&=V(t)-\Theta \nabla f(V),
%\end{align*}
%where $D_y\in \mathbb{R}^{n \times n}$ is a diagonal matrix with element $y_i=\left\|v_i-\Theta_{(i,i)}\mathbf{g}_i\right\|$.
%\begin{assumption}
%let $f\in \mathcal{C}^2$, and $\left\|\nabla^2f(x)\right\|\leq L$.
%\end{assumption}
%Under the above assumption,
\par Following the work in \cite{saddle_escape}, we have the following property, which was investigated in \cite{wang2017mixing}.
\begin{lemma}\label{un_stable_fix}
	If $p>\sqrt{2n}$, each strict saddle point $V^*$ of the updating $h_{SM}$ is an unstable critical point, meaning $\mathcal{X}^*\subset \mathcal{A}_g^*$, where $\mathcal{X}^*$ is the set of strict saddle points.
\end{lemma}
\par\textbf{Proof:}
Consider the equivalent form in Lemma \ref{cmp_lem} of the proposed synchronous algorithm. It follows from the proof  in \cite{wang2017mixing} that  the Jacobi of proposed algorithm has eigenvalues containing those of the Jacobi of a standard Gauss-Seidel updating proposed in \cite{saddle_escape}. Based on the discussions of standard Gauss-Seidel updating in \cite{saddle_escape}, we obtain the desirable result.
$\hfill\blacksquare$
\par From Lemma \ref{un_stable_fix}, we deduce that all non-optimal critical pints are unstable fixed points. Next, we will prove that the updating $h_{SM}$ is a diffeomorphism, which is an invertible function that maps one differentiable manifold to another such that both the function and its inverse are smooth.
%Before that, we prove a related Lemma, which states the method $M_d$ never degenerates.
%\begin{lemma}\label{conti_lem}
%The proposed method $M_d$ with the step-size matrix $\Theta$ never degenerates. That is, there is a constant $\sigma \in (0,1)$ such that
%\begin{align*}
% \left\|v_i-\Theta_{(i,i)} M_{(i,:)} V' \right\| \geq \sigma>0,
%\end{align*}
%where $\Theta_{(i,i)}$ is the $i$th diagonal element of matrix $\Theta$.
%\end{lemma}
%\textbf{Proof:}
%In the compact proposed algorithm, the step size actually takes a constant $\Theta_{(i,i)}\in(0,\frac{1}{\left\|M_{(i,:)}\right\|_1})$. It is equivalent to taking $\frac{1-\sigma}{\left\|M_{(i,:)}\right\|_1}$ for a constant $\sigma \in (0,1)$. From the triangular inequality,
%\begin{align*}
%\left\|M_{(i,:)} V'\right\|=\left\|\sum_{j=1}^n M_{(i,j)} v_j'\right\|\leq \sum_{j=1}^n \left|M_{(i,j)}\right|\|v_j\|=\|M_{(i,:)}\|_1,
%\end{align*}
%then $\left\|\Theta_{(i,i)} M_{(i,:)} V'\right\|\leq 1-\sigma<1$. What's more, we have $\left\|v_i-\Theta_{(i,i)} M_{(i,:)} V' \right\|\geq 1- \left\|\Theta_{(i,i)} M_{(i,:)} V'\right\|\geq \sigma>0$ and it completes the proof.
%$\hfill\blacksquare$
\begin{lemma}\label{diffeo_lemma}
	Under Assumptions \ref{SR_indice} and \ref{twoshare_assu}, the distributed synchronous updating $h_{SM}$ is a diffeomorphism.
\end{lemma}
\par\textbf{Proof:}
%The proof is similarly to the work in \cite{wang2017mixing}, and for completion, we also provide the brief proof idea here.
By the designed variable updating in Algorithm \ref{algo_sum} and Lemma 4.1, $h_{SM}$ is equivalent to
$$h_{SM}(V)=\left[\begin{matrix}\psi_n(\psi_{n-1}(\cdots\psi_1(V)))\end{matrix}\right]$$
where each column variable updating is defined as
$$ (\psi_i({V}))_{s=1\cdots n} =\left\{
\begin{array}{rcl}
&\frac{v_i-\Theta_{(i,i)} V M_{(:,i)} }{\left\|v_i-\Theta_{(i,i)}  V M_{(:,i)}\right\|} & \text{if} \quad s=i\\
&v_s' & \text{otherwise.}
\end{array} \right.$$
Because a composition of diffeomorphisms is still a diffeomorphism\upcite{inci2012regularity}, to prove this lemma, we only need to prove
 that $\psi_i(V)$ is a diffeomorphism for $i=1,\cdots,n$.
\par In \eqref{distri_mapping}, the step size takes a constant $\Theta_{(i,i)}\in(0,\frac{1}{\left\|M_{(i,:)}\right\|_1})$. Because $M$ is symmetric, it is equivalent to taking $\frac{1-\sigma}{\left\|M_{(:,i)}\right\|_1}$ for a constant $\sigma \in (0,1)$ and from the triangular inequality,
\begin{align*}
\left\| V M_{(:,i)}\right\|=\left\|\sum_{j=1}^n M_{(i,j)} v_j\right\|\leq \sum_{j=1}^n \left|M_{(i,j)}\right|\|v_j\|=\|M_{(i,:)}\|_1.
\end{align*}
Hence, $\left\|\Theta_{(i,i)} V M_{(:,i)}\right\|\leq 1-\sigma<1$. Thus, we have $\left\|v_i-\Theta_{(i,i)} V M_{(:,i)} \right\|\geq 1- \left\|\Theta_{(i,i)}  V M_{(:,i)}\right\|\geq \sigma>0$. Note that the function $\psi_i$ is only non-smooth at the point where the denominator term $\left\|v_i-\Theta_{(i,i)} V M_{(:,i)} \right\|=0$ and we have proved that the term is greater than $0$. Therefore, the function $\psi_i$ and its inverse function are valid and smooth. By the work in  \cite[Lemma C.2]{wang2017mixing}, $\psi_i$ is a diffeomorphism.  Since $h_{SM}(\cdot)$ is the composition of $\psi_i(\cdot)$s, the mapping of $h_{SM}(V)$ is also a diffeomorphism.
$\hfill\blacksquare$
\par Next, with the definition of global variable $V$ in \eqref{V_def}, we provide one equivalent form of objective function in optimization problem \eqref{non-convex_pro}, which will be used in the analysis of Lemma \ref{f_decrease}.
\begin{lemma}\label{ques_equal}
With the definition \eqref{V_def} and Assumptions \ref{SR_indice}, \ref{twoshare_assu}, the objective function of \eqref{non-convex_pro} at time $t$ $\sum_{\tilde{i}=1}^m \langle M^{\tilde{i}} ,V^{\tilde{i}'}(t)V^{\tilde{i}}(t)\rangle =\langle M,V(t)'V(t)\rangle$.
\end{lemma}
\par\textbf{Proof:}
The objective function of optimization problem \eqref{non-convex_pro} is $\sum_{\tilde{i}=1}^m \langle M^{\tilde{i}} ,V^{\tilde{i}'}V^{\tilde{i}}\rangle$. By the algorithm design, after one iteration $t$, $v_j^{\tilde{i}}(t)=v_j(t)$ for all $j\in J_{\tilde{i}}$, which holds because the coupling variables $v_j^{\tilde{i}}$ with $j\in S_{\tilde{i},{\rm par}(\tilde{i})}$ are equal to the uncoupling variables $v_j^{{\rm par}(\tilde{i})}$. Hence, the constraints \eqref{norm_con} and \eqref{share_con} in \eqref{non-convex_pro} hold. Then, we obtain
\begin{align*}
&\sum_{\tilde{i}=1}^m \langle M^{\tilde{i}} ,V^{\tilde{i}'}(t)V^{\tilde{i}}(t)\rangle\\
=&\sum_{\tilde{i}=1}^m \sum_{l,h\in J_{\tilde{i}}} M^{\tilde{i}}_{\{l,h\}} v^{\tilde{i}}_{l}(t)'v^{\tilde{i}}_{h}(t)\\
=&\sum_{\tilde{i}=1}^m \sum_{l,h\in J_{\tilde{i}}} M^{\tilde{i}}_{\{l,h\}} v_{l}(t)'v_{h}(t)\\
%=&\sum_{l,h\in J_{\tilde{i}}} \sum_{\tilde{i}=1}^m  M^{\tilde{i}}_{\{l,h\}} v_{l}(t)'v_{h}(t)\\
=&\sum_{l,h\in J_{\tilde{i}}} M_{(l,h)} v_{l}(t)'v_{h}(t)\\
=&\langle M,V(t)'V(t)\rangle,
\end{align*} 
where the second to last equation holds because $M^{\tilde{i}}_{\{l,h\}}=\overline{M}^{\tilde{i}}_{(l,h)}$ and $\sum_{\tilde{i}=1}^m \overline{M}^{\tilde{i}}=M$.
$\hfill\blacksquare$
\par With Lemma \ref{ques_equal}, before updating $v_i$, all variable $v_j$ except for $v_i$ are given and fixed, then the global function is rewritten as
\begin{align}\label{trans_pro}
f(V)&=\left<M, V'V\right>\notag\\&=\sum_{i=1}^n \sum_{j=1}^n M_{(i,j)} v_i'v_j \notag\\
&=2v_i'\mathbf{g}_i+{\rm constant},
\end{align}
where the last equation holds since the matrix $M$ is symmetric. Note that $ \mathbf{g}_i$ is defined in Lemma \ref{cmp_lem} and is independent of $v_i$ because $M_{(i,i)}=0$. Then, we have the following Lemma stating the monotonous decreasing property of the global function value generated by the proposed synchronous algorithm.
\begin{lemma}\label{f_decrease}
	For the proposed synchronous algorithm with step size $\Theta$, let ${V}(t+1)=h_{SM}(V(t))$. Under Assumptions \ref{SR_indice} and \ref{twoshare_assu}, we have
	\begin{align}\label{f_f}
	f(V(t))-f(V(t+1))=\sum_{i=1}^n \frac{1+y_i(t)}{\Theta_{(i,i)}} \left\|v_i(t)-v_i(t+1)\right\|^2,
	\end{align}
	where $y_i(t)=\left\|v_i(t)-\Theta_{(i,i)}\mathbf{g}_i(t)\right\|$ and  $\mathbf{g}_i(t)=\sum_{l<i} M_{(i,l)} v_l(t+1)+\sum_{l>i} M_{(i,l)} v_l(t)$.
\end{lemma}
%+\sum_{l\in \mathcal{S}_{\tilde{i},{\rm par}(\tilde{i})}}^n M_{(i,l)} v_l(t)
\par\textbf{Proof:}
By (\ref{trans_pro}), the function difference after updating $v_i(t)$ to $v_i(t+1)$ is $2 \mathbf{g}_i'(v_i(t)-{v}_i(t+1))$. Then, by the updating in (\ref{distri_mapping}), ${v}_i(t+1)=(v_i(t)-\Theta_{(i,i)}\mathbf{g}_i(t))/ y_i(t)$, we have
\begin{align}\label{f_de}
&2 \mathbf{g}_i(t)'(v_i(t)-{v}_i(t+1))\notag\\
=& 2(\mathbf{g}_i(t)+\frac{v_i(t)-\Theta_{(i,i)}\mathbf{g}_i(t)}{\Theta_{(i,i)}})'(v_i(t)-{v}_i(t+1))\notag\\
&-2(\frac{v_i(t)-\Theta_{(i,i)}\mathbf{g}_i(t)}{\Theta_{(i,i)}})'(v_i(t)-{v}_i(t+1))\notag\\
=&2 \frac{1}{\Theta_{(i,i)}}v_i(t)'(v_i(t)-{v}_i(t+1))\notag\\
&-2\frac{y_i(t)}{\Theta_{(i,i)}}{v}_i(t+1)'(v_i(t)-{v}_i(t+1))\notag\\
=&\frac{1+y_i(t)}{\Theta_{(i,i)}} 2 (1-v_i(t)'{v}_i(t+1))\notag\\
=&\frac{1+y_i(t)}{\Theta_{(i,i)}} \left\|v_i(t)-{v}_i(t+1)\right\|^2,
\end{align}
where the third equality holds due to the condition $\|v_i\|=1$.
Then, the result holds from summing the above equation over $i=1,\cdots,n$.
$\hfill\blacksquare$
\par Now, we are ready to prove the result in Theorem \ref{syn_theo}.
\par \textbf{Proof of Theorem \ref{syn_theo}:}
Assume Assumptions \ref{SR_indice} and \ref{twoshare_assu} hold. From Lemma $\ref{un_stable_fix}$ and the nonexistence of local optima, all non-optimal critical pints are unstable fixed points. Recall that  $h_{SM}$ is a diffeomorphism by Lemma \ref{diffeo_lemma} and non-optimal critical points of $h_{SM}(\cdot)$ are unstable fixed points by Lemma \ref{un_stable_fix}. It follows from the center-stable manifold theorem (Theorem III.5 of \cite{shub_book}) that the proposed algorithm escapes all non-optimal critical points almost surely under random initialization. By Lemma \ref{f_decrease} and the fact that  $\frac{1+y_i(t)}{\Theta_{(i,i)}}$ in (\ref{f_f}) is always positive over iterations, the objective function value is strictly decreasing. Because the objective function value generated by the proposed algorithm is strictly decreasing and the objective value is lower bounded, the generated variables converge to the set of  first-order critical points. {Thus, the almost sure divergence from the non-optimal critical points and the convergence to critical points imply that $v_j^{\tilde{i}}, j\in J_{\tilde{i}}$ in the updating $h_{SM}$ converges to corresponding column of global optimal solutions of (\ref{non-convex_pro}) almost surely under random initialization.
\par Next, we show that the result of this theorem holds if Assumptions \ref{SR_indice} and \ref{twoshare_assu} are removed. If Assumption \ref{SR_indice} does not hold, the indices of variables can be rearranged manually such that the indices in uncoupling set $\mathcal{R}_{\tilde{i},{\rm par}(\tilde{i})}$ are smaller than the indices in coupling set $\mathcal{S}_{\tilde{i},{\rm par}(\tilde{i})}$ for each agent $\tilde{i}$. Hence, the above analysis still holds without Assumptions \ref{SR_indice}. If Assumption \ref{twoshare_assu} does not hold, in \eqref{Slgeqj} of Lemma \ref{cmp_lem}, for each variable $v_l^{\tilde{i}}, l\in \mathcal{S}_{{\rm ch}(\tilde{i}), \tilde{i}}\cap \mathcal{S}_{\tilde{i},{\rm par}(\tilde{i})}$, there must be a parent agent $\tilde{j}$ such that $l\in \mathcal{R}_{\tilde{j},{\rm par}(\tilde{j})}$ by algorithm design. Then, $v_l^{\tilde{i}}(t)=v_l^{\tilde{j}}(t)=v_l(t)$. Thus, the analysis in \eqref{Slgeqj} analogously holds and the rest of theoretical deductive is true.}
$\hfill\blacksquare$
%\begin{remark}
% In our work, we provide one idea to decentralize the computation burden over different agents when solving large-scale sparse optimization problems. Compared with the existing centralized works \cite{wang2017mixing,saddle_escape}, it can reduce the computation and storage burden on one centralized node. {\color{red} In addition, it is difficult to distribute the variable updating of centralized algorithms over multi-agent networks directly when applied to large-scale optimization problems, due to the strict orders in variable element iterations. What's more, local information is often geographically separate on different nodes in some network applications.} Next, we will prove the proposed distributed asynchronous algorithm can also guarantee to converge even if with communication delays and chaotic transmissions.
%\end{remark}
\subsection{Convergence analysis for asynchronous algorithm }\label{ASM_sec}
%\par We have proved the convergence performance of the synchronous algorithm and we will discuss the convergence property of the asynchronous algorithm in the following paper.
\par {For distributed asynchronous algorithm \ref{algo_asyn}}, let $T^{\tilde{i}}$ be the set of times at which agent $\tilde{i}$ updates variable $V^{\tilde{i}}$. In addition, agent $\tilde{i}$ may not have access to the most recent value of other agents' variables. To collect communicated information from neigbors, define a set $\mathcal{J}_{\tilde{i}}$ as the union $J_{\tilde{i}}\cup \mathcal{R}_{\tilde{r}\in {\rm ch}(\tilde{i}),\tilde{i}}$. Thus, define one possibly outdated variable of agent $\tilde{i}$, {$\mathbf{V}^{\tilde{i}}(t)\in \mathbb{R}^{p\times |\mathcal{J}_{\tilde{i}}|}$}, as
\begin{align}\label{Vi_def}
\mathbf{V}^{\tilde{i}}(t)=\big[v_{s_1}^{\tilde{i}}(\tau_{s_1}^{\tilde{i}}(t)),\cdots,v_{s_{|\mathcal{J}_{\tilde{i}}|}}^{\tilde{i}}(\tau_{s_{|\mathcal{J}_{\tilde{i}}|}}^{\tilde{i}}(t))\big],
\end{align}
where $\{s_1,\cdots,s_{|\mathcal{J}_{\tilde{i}}|}\}=\mathcal{J}_{\tilde{i}}$, $\tau_{{j}}^{\tilde{i}}(t)$ is assumed to satisfy the condition (\ref{taulim}). Recall that, if $s_j\in \mathcal{R}_{\tilde{i},{\rm par}(\tilde{i})}$, then $\tau_{s_j}^{\tilde{i}}(t)=t$. $\mathbf{V}^{\tilde{i}}\in \mathbb{R}^{p\times |\mathcal{J}_{\tilde{i}}|}$ collects time-delayed transmitted information from children and parents. %%%

\par With additional assumption in Assumption \ref{asyn_assump} that each element of global coefficient matrix $M$ is only accessible to one agent, the relationship $M^{\tilde{i}}_{\{l,k\}}=M_{(l,k)}$ holds for all agent $\tilde{i}$. Then, the updating proposed in DAA is rewritten as a compact form as shown in the following lemma, where each column variable $v_j$ of global variabel $V$ defined in \eqref{V_def} is expressed by outdated transmitted information.
\begin{lemma}\label{asyn_comp}
	Under Assumption \ref{asyn_assump}, each column variable $v_j$ generated by DAA with index $j\in \mathcal{R}_{\tilde{i},{\rm par}(\tilde{i})}$ is equivalent to
	\begin{equation}
	\left\{
	\begin{aligned}
	&v_j(t+1)={\rm normal}(v_j(t)-\Theta_{(j,j)}\mathbf{h}_j(\mathbf{V}^{\tilde{i}}(t)), \quad t\in T^{\tilde{i}},\\
	&v_j(t+1)=v_j(t),\quad t\notin T^{\tilde{i}}
	\end{aligned}
	\right.
	\end{equation}
	where $\mathbf{h}_j(\mathbf{V}^{\tilde{i}}(t))=  \big[\sum_{l\in \mathcal{R}_{\tilde{i},{\rm par}(\tilde{i})}} M_{(j,l)} {v}_l(t)+\sum_{l\in \mathcal{S}_{\tilde{i},{\rm par}(\tilde{i})}} M_{(j,l)} {v}_l(\tau_{l}^{\tilde{i}}(t))+\sum_{\tilde{r}\in {\rm ch}(\tilde{i})}\sum_{l\in \mathcal{R}_{\tilde{r},\tilde{i}}} M_{(j,l)} v_l(\tau_{l}^{\tilde{i}}(t))\big]$, $\Theta_{(j,j)}=\theta_j^{\tilde{i}}$ was defined in (\ref{asyn_step}).
\end{lemma}
\par\textbf{Proof:}
By (\ref{v_updateasm}) in DAA, agent $\tilde{i}$ updates local uncoupling variable $v_j$ with index in $j\in \mathcal{R}_{\tilde{i},{\rm par}(\tilde{i})}$
according to
\begin{align}\label{vrasyn}
&v_j^{\tilde{i}}(t+1)\notag\\=&{\rm normal}\Big(v_j^{\tilde{i}}(t)-\theta_j^{\tilde{i}} \big[p_j^{\tilde{i}}+\sum_{\tilde{r}\in {\rm ch}(\tilde{i})}\varpi_j^{\tilde{r},\tilde{i}}(\tau^{\tilde{i}}(t))\big]\Big)\notag\\
=&{\rm normal}\Big(v_j^{\tilde{i}}(t)-\theta_j^{\tilde{i}} \big[\sum_{l\in J_{\tilde{i}}} M_{\{j,l\}}^{\tilde{i}} {v}_l^{\tilde{i}}(t)\notag\\
&+\sum_{\tilde{r}\in {\rm ch}(\tilde{i})}\sum_{l\in J_{\tilde{r}}} M_{\{j,l\}}^{\tilde{r}} v_l^{\tilde{r}}(\tau_{l}^{\tilde{i}}(t))\big]\Big)\notag\\
=&{\rm normal}\Big(v_j^{\tilde{i}}(t)-\theta_j^{\tilde{i}} \big[\sum_{l\in \mathcal{R}_{\tilde{i},{\rm par}(\tilde{i})}} M_{(j,l)} {v}_l^{\tilde{i}}(t)\notag\\
&+\sum_{l\in \mathcal{S}_{\tilde{i},{\rm par}(\tilde{i})}} M_{(j,l)} {v}_l^{{\rm par}({\tilde{i}})}(\tau_{l}^{\tilde{i}}(t))\notag\\
&+\sum_{\tilde{r}\in {\rm ch}(\tilde{i})}\sum_{l\in \mathcal{R}_{\tilde{r},\tilde{i}}} M_{(j,l)} v_l^{\tilde{r}}(\tau_{l}^{\tilde{i}}(t))\big]\Big)
\end{align}
where the last equality holds because $M_{\{j,l\}}^{\tilde{r}}=0$, for $j\in \mathcal{R}_{\tilde{i},{\rm par}(\tilde{i})}$, $l\in \mathcal{S}_{\tilde{r},\tilde{i}}$, by the assumption that each element of $M$ is only accessible to one agent. 
%because each element of $M$ is only known to one agent, then $M_{(j,l)}^{\tilde{r}}=0$ for $l\in \mathcal{S}_{\tilde{r},\tilde{i}}$. %such that  $M^{\tilde{r}}_{j,l}=0$ for all $l\in S_{{\rm ch}(\tilde{i}),\tilde{i}}$.
\par For the second term $\sum_{l\in \mathcal{S}_{\tilde{i},{\rm par}(\tilde{i})}} M_{(j,l)} {v}_l^{{\rm par}({\tilde{i}})}(\tau_{l}^{\tilde{i}}(t))$ of (\ref{vrasyn}), similarly to the discussions of (\ref{Slgeqj}) in the synchronous case, we obtain
\begin{align}\label{secondterm}
\!\sum_{l\in \mathcal{S}_{\tilde{i},{\rm par}(\tilde{i})}}\! M_{(j,l)} {v}_l^{{\rm par}({\tilde{i}})}\!(\tau_{l}^{\tilde{i}}(t))\!=\!\sum_{l\in \mathcal{S}_{\tilde{i},{\rm par}(\tilde{i})}} \! M_{(j,l)} {v}_l(\tau_{l}^{\tilde{i}}(t)).
\end{align}

\par Then, substituting (\ref{secondterm}) to (\ref{vrasyn}), we have
\begin{align}\label{ljasy}
&v_j(t+1)\notag\\
=&{\rm normal}\Big(v_j(t)-\Theta_{(j,j)} \big[\sum_{l\in \mathcal{R}_{\tilde{i},{\rm par}(\tilde{i})}} M_{(j,l)} {v}_l^{\tilde{i}}(t)\notag\\
&+\sum_{l\in \mathcal{S}_{\tilde{i},{\rm par}(\tilde{i})}} M_{(j,l)} {v}_l(\tau_{l}^{\tilde{i}}(t))\notag\\
&+\sum_{\tilde{r}\in {\rm ch}(\tilde{i})}\sum_{l\in \mathcal{R}_{\tilde{r},\tilde{i}}} M_{(j,l)} v_l^{\tilde{r}}(\tau_{l}^{\tilde{i}}(t))\big]\Big)\notag\\
=&{\rm normal}\Big(v_j(t)-\Theta_{(j,j)}\big[\sum_{l\in \mathcal{R}_{\tilde{i},{\rm par}(\tilde{i})}} M_{(j,l)} {v}_l(t)\notag\\
&+\sum_{l\in \mathcal{S}_{\tilde{i},{\rm par}(\tilde{i})}} M_{(j,l)} {v}_l(\tau_{l}^{\tilde{i}}(t))\notag\\
&+\sum_{\tilde{r}\in {\rm ch}(\tilde{i})}\sum_{l\in \mathcal{R}_{\tilde{r},\tilde{i}}} M_{(j,l)} v_l(\tau_{l}^{\tilde{i}}(t))\big]\Big),\notag\\
&={\rm normal}(v_j(t)-\Theta_{(j,j)} \mathbf{h}_j(\mathbf{V}^{\tilde{i}}(t))), \quad t\in T^{\tilde{i}},
\end{align}
%$$v_j(t+1)={\rm normal}(v_j(t)-\Theta_{(j,j)} \nabla_{j} f(V^{\tilde{i}}(t))), \quad t\in T^{\tilde{i}},$$
where {$\mathbf{h}_j(\mathbf{V}^{\tilde{i}}(t))= \big[\sum_{l\in \mathcal{R}_{\tilde{i},{\rm par}(\tilde{i})}} M_{(j,l)} {v}_l(t)+\sum_{l\in \mathcal{S}_{\tilde{i},{\rm par}(\tilde{i})}} M_{(j,l)} {v}_l(\tau_{l}^{\tilde{i}}(t))+\sum_{\tilde{r}\in {\rm ch}(\tilde{i})}\sum_{l\in \mathcal{R}_{\tilde{r},\tilde{i}}} M_{(j,l)} v_l(\tau_{l}^{\tilde{i}}(t))\big]$}, $\Theta_{(j,j)}=\theta_j^{\tilde{i}}$ for $j\in \mathcal{R}_{\tilde{i},{\rm par}(\tilde{i})}$. For $t\notin T^{\tilde{i}}$, $v_j(t+1)=v_j(t)$ holds naturally.
$\hfill\blacksquare$
\par Define the updating direction $s_j$ ($j\in \mathcal{R}_{\tilde{i},{\rm par}(\tilde{i})}$) as, %%??????M?????????????????
\begin{align}\label{s_de}
s_j(t)&=\frac{1}{\Theta_{(j,j)}}(v_j(t+1)-v_j(t)).
\end{align}
If $t\in T^{\tilde{i}}$, 
\begin{align}
s_j(t)=\frac{1}{\Theta_{(j,j)}}\!\Big(\!{\rm normal}\big(v_j(t)-\Theta_{(j,j)}\mathbf{h}_j(\mathbf{V}^{\tilde{i}}(t))\big)-v_j(t)\!\Big),
\end{align}
 and if $t\notin T^{\tilde{i}}$, $s_j(t)=0$.
\par In the following lemma, we present a vital descent property of local variable iteration, which will be used in the proof of Theorem \ref{asyn_theo}.
\begin{lemma}\label{coor_descent}
	Suppose Assumption \ref{asyn_assump} holds. For any agent $\tilde{i}$ and time $t$, we have
	\begin{align}\label{sineq}
	s_j(t)'\mathbf{h}_j(\mathbf{V}^{\tilde{i}}(t))\leq -\|s_j(t)\|^2, \quad j\in \mathcal{R}_{\tilde{i},{\rm par}(\tilde{i})}.
	\end{align}
\end{lemma}
\textbf{Proof:}
If $t\notin T^{\tilde{i}}$, the inequality (\ref{sineq}) is true since both sides are zero. If $t\in T^{\tilde{i}}$, by the definition of $s_j(t)$ in (\ref{s_de}) and by Lemma \ref{asyn_comp}, $v_j(t+1)=(v_j(t)-\Theta_{(j,j)}\mathbf{h}_j(\mathbf{V}^{\tilde{i}}(t)))/y_j(t)$ for $j\in \mathcal{R}_{\tilde{i},{\rm par}(\tilde{i})}$, where $y_j(t)=\|v_j(t)-\Theta_{(j,j)}\mathbf{h}_j(\mathbf{V}^{\tilde{i}}(t))\|$, we have
\begin{align*}
&s_j(t)'\mathbf{h}_j(\mathbf{V}^{\tilde{i}}(t))\\
=& \frac{-1}{\Theta_{(j,j)}}(v_j(t)-v_j(t+1))'\mathbf{h}_j(\mathbf{V}^{\tilde{i}}(t))\\
=& \frac{-1}{\Theta_{(j,j)}} \bigg[\Big(\mathbf{h}_j(\mathbf{V}^{\tilde{i}}(t))\\
&+\frac{v_j(t)-\Theta_{(j,j)}\mathbf{h}_j(\mathbf{V}^{\tilde{i}}(t))}{\Theta_{(j,j)}}\Big)'(v_j(t)-v_j(t+1))\\
&-\Big(\frac{v_j(t)-\Theta_{(j,j)}\mathbf{h}_j(\mathbf{V}^{\tilde{i}}(t))}{\Theta_{(j,j)}}\Big)' (v_j(t)-v_j(t+1)) \bigg]\\
=&  \frac{-1}{\Theta_{(j,j)}} \bigg[\frac{1}{\Theta_{(j,j)}} v_j(t)'(v_j(t)-v_j(t+1))\\
&-\frac{y_j(t)}{\Theta_{(j,j)}} v_j(t+1)'(v_j(t)-v_j(t+1))\bigg]\\
=& -\frac{1+y_j(t)}{\Theta_{(j,j)}^2}\|v_j(t+1)-v_j(t)\|^2\\
=&-(1+y_j(t))\|s_j(t)\|^2\\
\leq &-\|s_j(t)\|^2,
\end{align*}
where the last inequality holds because $y_j$ is non-negative.
$\hfill\blacksquare$
\par Making use of Lemma \ref{coor_descent}, we discuss the relationship of global variable $V$ and local variables $\mathbf{V}^{\tilde{i}}$, and the gradient of objective function at the point $V(t_k)$ when $k \to \infty$ in Theorem \ref{asyn_theo}. Before discussions, it should be noted that $\mathbf{h}_j(\mathbf{V}^{\tilde{i}}(t))$ defined in Lemma \ref{asyn_comp} is exactly the gradient of global function $f$ with respect to column variable $v_j(t)$, where $j\in \mathcal{R}_{\tilde{i},{\rm par}(\tilde{i})}$, so for convenience, we use $\mathbf{h}_j(\mathbf{V}^{\tilde{i}}(t))$ in the following analysis. The proof follows the studies of gradient-like optimization algorithms in Proposition 5.1, section 7, \cite{paral_distri_book}.
\par \textbf{Proof of Theorem \ref{asyn_theo}:} We follow the proof of Proposition 5.1 in  \cite{paral_distri_book}. By Assumption \ref{pro_assump} (2), the objective function of \eqref{non-convex_pro} satisfies $\sum_{\tilde{i}=1}^m f_{\tilde{i}}\geq 0$. In addition, with the analysis in Lemma \ref{coor_descent}, we have, for $j\in \{1,\cdots,n\}$,
\begin{align*}
s_j(t)'\mathbf{h}_j(\mathbf{V}^{\tilde{i}}(t))&\geq -\|s_j(t)\|\|\mathbf{h}_j(\mathbf{V}^{\tilde{i}}(t))\|\\
-(1+y_j(t))\|s_j(t)\|^2 &\geq -\|s_j(t)\|\|\mathbf{h}_j(\mathbf{V}^{\tilde{i}}(t))\|\\
(1+y_j(t))\|s_j(t)\|^2 &\leq \|s_j(t)\|\|\mathbf{h}_j(\mathbf{V}^{\tilde{i}}(t))\|\\
\|s_j(t)\|&\leq \frac{1}{1+y_j(t)}\|\mathbf{h}_j(\mathbf{V}^{\tilde{i}}(t))\|,
\end{align*}
where the last inequality holds because $y_j$ is non-negative. Then, there is a positive constant $K_3=1$ such that $\|s_j(t)\|\leq K_3 \|\mathbf{h}_j(\mathbf{V}^{\tilde{i}}(t))\|$. 
What's more, {with the block-descent property in Lemma \ref{coor_descent},} the assumptions in Proposition 5.1 \cite{paral_distri_book} hold, where the  product is replaced by inner product of vectors and the term $\left|s_j(t)\right|$ is replaced by $\|s_j(t)\|$. Then, by a similar analysis as Proposition 5.1 in \cite{paral_distri_book}, we obtain that 
\begin{align}\label{s_lim}
\lim_{t \to \infty} s_j(t)=0,
\end{align}
for each $j\in\{1,\cdots,n\}$. In addition, by (\ref{s_de}), we obtain
\begin{align}
&\lim_{t\to \infty}\|V(t+1)-V(t)\|=0.\label{Vgap}
\end{align}
Then, consider the boundeness of $\|v_j^{\tilde{i}}(t)-v_j(t)\|$.
\begin{align}\label{vv}
\|v_j^{\tilde{i}}(t)-v_j(t)\|=&\|v_j^{\tilde{i}}(\tau_j^{\tilde{i}}(t))-v_j(t)\|\notag\\
=&\Theta_{(j,j)}\|\sum_{\tau=\tau_{j}^{\tilde{i}}(t)}^{t-1}s_j(\tau)\|\notag\\
\leq &\Theta_{(j,j)}\sum_{\tau=t-B}^{t-1}\|s_j(\tau)\|.
\end{align}
With \eqref{s_lim} and \eqref{vv}, we also obtain
\begin{align}
&\lim_{t\to \infty}\|v_j^{\tilde{i}}(t)-v_j(t)\|=0, \ \forall j\in \mathcal{R}_{\tilde{i},{\rm par}(\tilde{i})}.\label{ViVgap}
\end{align}
In addition, for each $j\in \{1,\cdots,n\}$, there is a unique agent $\tilde{i}$ such that $j\in \mathcal{R}_{\tilde{i},{\rm par}(\tilde{i})}$ and the equation \eqref{ViVgap} holds.
\par Define $V^*$ as a limit point of $V(t)$ and $\{t_k\}$ as a sequence such that $\lim_{k\to\infty}V(t_k)=V^*$. Let $\tau_k$ be such that $\left|t_k-\tau_k\right|\leq B$ and $\tau_k\in T^{\tilde{i}}$. Then, by equations (\ref{Vgap}) and (\ref{ViVgap}), $v_j(\tau_k)$  converges to $v_j^*$, which is $j$th column of $V^*$, and $v_j^{\tilde{i}}(\tau_k)$ converges to $v_j^*$ such that $\mathbf{V}^{\tilde{i}}(\tau_k)$ converges. Then, we have
\begin{align*}
&\lim_{k\to \infty}({\rm normal}(v_j^{\tilde{i}}(\tau_k)-\Theta_{(j,j)}\mathbf{h}_j(\mathbf{V}^{\tilde{i}}(\tau_k))-v_j^{\tilde{i}}(\tau_k))\\
=&\lim_{k\to \infty}\Theta_{(j,j)}s_j(\tau_k)=0.
\end{align*}%%???????lim_t\to \infty s_i(t)=0??t?????i??????
Since it holds for each $j\in\{1,\cdots,n\}$, we get the desired result.
$\hfill\blacksquare$


\section{Simulation}\label{simulation}
In this section, numerical tests and large-scale image segmentation application are presented to show the efficiency of the proposed distributed algorithms.
\par \textit{Example 1:} We present one special sparse coupling numerical optimization problem, which has been investigated in \cite{dis_pd,distri_sed_tac,fast_dis} and of which the corresponding connected graph is one clique tree. More detailed information of clique trees can be found in \cite{dis_pd}. Here we only introduce some brief concepts and focus on the discussions about the numerical convergence performance of proposed algorithms. For the coupling optimization (\ref{sdp_pro}), we assume that the dimension of global matrix variable $X$ is $n=8$, the number of local functions is $N=6$, the corresponding dependent element indices set are $C_1=\{1,3\}$, $C_2=\{1,2,4\}$, $C_3=\{4,5\}$, $C_4=\{3,4\}$, $C_5=\{3,6,7\}$, $C_6=\{3,8\}$. By the clique tree transformations in \cite{distri_sed_tac}, the corresponding clique tree owns five agents, shown in Fig.\ref{simu_graph}. It shows that the number of agents in corresponding problem (\ref{non-convex_pro}) is $m=5$, which implies that one agent has multiple local functions. The local functions assigned to $i$th agent are denoted by a function set $\phi_{\tilde{i}}$. Then, the function sets of the clique tree are $\phi_{\tilde{1}}=\{f_2\}$, $\phi_{\tilde{2}}=\{f_1,f_4\}$, $\phi_{\tilde{3}}=\{f_3\}$, $\phi_{\tilde{4}}=\{f_5\}$, $\phi_{\tilde{5}}=\{f_6\}$. In addition, the ordered index sets are $J_{\tilde{1}}=\{1,2,4\}$, $J_{\tilde{2}}=\{1,3,4\}$, $J_{\tilde{3}}=\{4,5\}$, $J_{\tilde{4}}=\{3,6,7\}$, $J_{\tilde{5}}=\{3,8\}$. More specifically, we provide the decomposed diagram of sparse coefficient matrix $M$ over five different agents as following. Elements with different colors are assigned to different agents.
$$\left[
\begin{array}{c|c|c|c|c|c|c|c}
W_{11} &{\textcolor{red}{W_{12}}}&{\textcolor{blue}{W_{13}}}&{\textcolor{red}{W_{14}}}&0&0&0&0\\ \cdashline{1-8}[0.8pt/2pt]
{\textcolor{red}{W_{21}}} &W_{22}&0&{\textcolor{red}{W_{24}}}&0&0&0&0\\ \cdashline{1-8}[0.8pt/2pt]
{\textcolor{blue}{W_{31}}} &0&W_{33}&{\textcolor{blue}{W_{34}}}&0&{\textcolor{cyan}{W_{36}}}&{\textcolor{cyan}{W_{37}}}&{\textcolor{magenta}{W_{38}}}\\ \cdashline{1-8}[0.8pt/2pt]
{\textcolor{red}{W_{41}}} &{\textcolor{red}{W_{42}}}&{\textcolor{blue}{W_{43}}}&W_{44}&{\textcolor{green}{W_{45}}}&0&0&0\\ \cdashline{1-8}[0.8pt/2pt]
0 &0&0&{\textcolor{green}{W_{54}}}&W_{55}&0&0&0\\ \cdashline{1-8}[0.8pt/2pt]
0 &0&{\textcolor{cyan}{W_{63}}}&0&0&0&{\textcolor{cyan}{W_{67}}}&0\\ \cdashline{1-8}[0.8pt/2pt]
0 &0&{\textcolor{cyan}{W_{73}}}&0&0&{\textcolor{cyan}{W_{76}}}&W_{77}&0\\ \cdashline{1-8}[0.8pt/2pt]
0 &0&{\textcolor{magenta}{W_{83}}}&0&0&0&0&W_{88}
\end{array}
\right].$$
\begin{figure}
	\centering
	% Requires \usepackage{graphicx}
	\includegraphics[width=6cm]{simu_graph.pdf}\\
	\caption{Clique tree of simulation problem}\label{simu_graph}
	\centering
\end{figure}
\par Since the diagonal elements will not influence the optimization result, they are assigned to any agent without effect. Let $f^*$ be the optimal function value of the optimization problem (\ref{sdp_pro}), which is solved by the solver YALMIP when the dimension $n$ is not too large. %Denote the expected difference between the function value and the optimal function by a small constant $\sigma>0$.
\par $1)$ We use the proposed distributed synchronous, asynchronous algorithms and the centralized algorithm SDPLR \cite{BM_SPLR}, which are all coded by MATLAB, to solve the sparse optimization problem. The simulation results are shown in Figs. \ref{compare_fig}-\ref{gradf}. The original global variable value is $X(k)=V(k)'V(k)$. In Fig. \ref{compare_fig}, the proposed algorithms and SDPLR all converge to the optimal function value $f^*$, which is calculated by the solver YALMIP. It is observed that distributed algorithms converge much faster than the SDPLR algorithm for the MAXCUT problem. In Fig. \ref{gapx}, the trajectories of $\left\|X(k+1)-X(k)\right\|_F$, where the trajectories $\{X(k)\}$ are generated by the proposed algorithms DSA and DAA respectively, are shown to converge to zeros. It shows that varaible $X(k)$ converges to one limiting point.
\begin{figure}
	\centering
	\subfigure[The trajectories of $f$ by DSA, DAA and SDPLR]{
		\includegraphics[width=8cm]{fk2-eps-converted-to.pdf}
		\label{compare_fig}
	}
	
	\subfigure[The trajectories of $\left\|X(k+1)-X(k)\right\|_F$ along time]{
		\includegraphics[width=8cm]{xk2-eps-converted-to.pdf}
		\label{gapx}
	}
	\subfigure[The trajectories of gradient of $f$ along time]{
		\includegraphics[width=8cm]{gradf2-eps-converted-to.pdf}
		\label{gradf}
	} 	
	\caption{Convergent trajectories generated by proposed algorithms and SDPLR}
\end{figure}
The norm of the Riemannian gradient of $f$ is defined as $\left\|{\rm grad}f\right\|_F^2=\sum_{i=1}^n(\left\|{g}_i\right\|^2-\left<v_i,{g}_i\right>^2)$, where $\|\cdot\|_F$ represents the Frobenius norm of a matrix, and $g_i=\sum_{j=1}^n M_{(i,j)}v_j$. The trajectories of $\left\|{\rm grad} f(k)\right\|_F$ are shown in Fig. \ref{gradf} and converge to zeros, which implies that the generated sequences $\{V(k)\}$ converge to critical points. In addition, by the numerical experiments, for most cases, we have observed that the asynchronous algorithm converges faster than the synchronous algorithm. In addition, in the next simulation, we provide quantitative comparisons of convergence rates between DAA and DSA.
\par $2)$ In order to compare the performance of the proposed distributed algorithms with the inspired centralized algorithm, which is proposed in \cite{wang2017mixing}, we make use of MPICH distributed model, which is a high-performance message passing interface, to develop a multi-processers environment on one computer with a Core(TM) I5-8250U CPU, 1.6GHz. Both centralized algorithm and distributed algorithms are coded by C language. For the distributed algorithms, we use five processes to deal with the optimization problem.
\par We provide two experiments with different dimensions and collect the number of iterations and executive time of different algorithms. In each experiment, the stop criterion of iterations reaches an expected error between the function value $f(k)$ and optimal value $f^*$. In addition, we use $sn$ to denote the number of shared variables over the multi-agent network, e.g., the $sn$ of network shown in Fig. \ref{simu_graph} is $5$. The executed time comparisons of centralized algorithm and distributed algorithms are listed in following table \ref{com_table}.
\par By the comparative test, the distributed synchronous and asynchronous algorithms both converge faster than the centralized Mixing algorithm. As the dimension of problem increases, the role of distributed design is more important, especially when communication between different agents is sparse. In addition, by the simulation, we observe that distributed asynchronous algorithm often converges faster than distributed synchronous algorithm. It should be pointed out that although communication time-delay will not make asynchronous algorithm diverge, coordinating the trade-off between communication and computation may further improve the convergence performance of distributed asynchronous algorithm in practice, which is one future research direction of our work.
\begin{table}[!htbp]
	\centering
	\caption{the execution time comparisons}\label{com_table}
	\begin{tabular}{|c|c|c|c|c|c|}
		\hline
		${\rm dimension}$ &${\rm algorithm}$&$sn$&${\rm iterations}$&${\rm time(ms)}$&${\rm error}$\\
		\hline
		\multirow{3}*{8} &$Mixing$&0&291&20&0.00023\\
		\cline{2-6}
		&DSA&5&150&5&0.00023\\
		\cline{2-6}
		&DAA&5&150&4&0.00023\\
		\hline
		\multirow{3}*{18} &$Mixing$ &0&600&41&0.0058\\
		\cline{2-6}
		&DSA&8&390&16&0.0058\\
		\cline{2-6}
		&DAA&8&361&10&0.0058\\
		\hline
	\end{tabular}
\end{table}
%\begin{remark}
%	{\color{red}By the simulation, we observe that distributed asynchronous method often converges faster than distributed synchronous method. From the optimization perspective, the proposed distributed methods are seen as gradient descent methods on unit spheres. Hence, one possible explanation is that the distributed asynchronous method may be considered one analogous stochastic gradient descent method on a manifold.}
%\end{remark}
\begin{figure}
	\centering
	% Requires \usepackage{graphicx}
	\includegraphics[width=6cm]{graphcut_show-eps-converted-to.pdf}\\
	\caption{Distributed image segmentation set-up. Each agent only has access to a subset (colored grids) of the whole image pixels.}\label{image_cut_show}
	\centering
\end{figure}
\begin{figure}
	\centering
	% Requires \usepackage{graphicx}
	\includegraphics[width=6cm]{graph_nodeconn-eps-converted-to.pdf}\\
	\caption{The four-connected neighborhood of image segmentation set-up.}\label{pixel_show}
	\centering
\end{figure}
\par \textit{Example 2:} We apply the proposed distributed asynchronous algorithm to solving MAXCUT problems from image segmentation over a multi-agent system, as shown in Fig. \ref{image_cut_show}. There is one edge between agents if there exists an intersection between image pixels of different local images. Hence, there exist several parents for one agent, which is different from the first example, where each agent has only one parent. In this example, we will show that the proposed distributed algorithm efficiently achieves image segmentation.
\par There have been some works applying general graph cut algorithms to image segmentation \cite{cut_survey,cut_2008,image_seg2013,graph_cut_06}. For image segmentation, we need to create a graph representation of the image. One algorithm is to consider assigning each pixel of the image as a node and using a four-connected neighborhood to create the edges\cite{image_seg2013}, as shown in Fig. \ref{pixel_show}. We here only utilize the intensity components of the RGB of all pixels to provide one simple connected matrix $M$, whose $(i,j)$th element related to nodes $(i,j)$ is defined by the following equation \cite{image_seg2013}
\begin{align*}
&M_{i,j}=\\
&{\rm max}((2[\|rgb(i)-rgb(j)\|_2>t]-1)\|rgb(i)-rgb(j)\|_2,0),
\end{align*}
where $rgb(i)$ is the intensity vector of RGB of the $i$th pixel, $t$ is adjustable threshold value. The output of operator ${\rm max}(a,0)$ is the bigger one of $a$ and $0$. Then, the generated matrix $M$ is a typical large-scale sparse matrix. For some algorithms which add seeds to different regions, the only change is the development of matrix elements. We only use the simplest RGB information between different pixels to segment image. However, it should be noted that the proposed algorithms are applicable for general MAXCUT problems (\ref{sdp_pro}) that include more involved development of coefficient matrix elements. In some intelligent algorithms, the graph cut problem is often used as an important pretreatment\cite{contour_p}. Therefore, the large-scale sparse graph cut problem is vital in image segmentation.
\par We apply the proposed distributed asynchronous algorithm on images of the Berkeley database \cite{database}. We have computed the results for three images (Airplane, Church, Bird) in Figure \ref{image}. It is seen that the proposed distributed algorithm achieves image segmentation efficiently. While the existing centralized algorithms can not deal with image segmentation because of the large dimension of image data.
%Although there are some inaccurate segmentation between different regions, many graph cuts use interaction with users by adding manual scribbles or regions containing the object of interest to help the segmentation procedure. In addition, one could also add some knowledge of brightness and texture channels to improve the results.
\begin{figure}[htbp]
	\centering
	\subfigure[Airplane segmentation]{
		\includegraphics[width=4cm]{plane_b-eps-converted-to.pdf}
		\includegraphics[width=4cm]{plane_b_cut-eps-converted-to.pdf}
	}
	
	\subfigure[Church segmentation]{
		
		\includegraphics[width=4cm]{church_b-eps-converted-to.pdf}
		\includegraphics[width=4cm]{church_big_cut-eps-converted-to.pdf}
	}
	\subfigure[Brid segmentation]{
		\includegraphics[width=4cm]{red_bird_b-eps-converted-to.pdf}
		\includegraphics[width=4cm]{bird_b_cut-eps-converted-to.pdf}
	}
	
	\caption{Segmentation results of images from Berkeley database}
	\label{image}
\end{figure}


\section{Conclusion}\label{conclusion}
This paper has studied distributed synchronous and asynchronous algorithms for solving large-scale SDP with diagonal constraints by making use of the inherent sparsity of programming and low-rank property of solutions. Each agent updates its local variables by local information and communicating messages over the underlying topology. To handle the communication delays in networks, one distributed asynchronous algorithm is proposed without  global clocks. Although the transformed optimization problem is non-convex, variables of distributed synchronous and asynchronous algorithms eventually converge to optimal solutions and critical points of SDP, repectively.  The efficiency of proposed distributed algorithms is verified by the numerical simulations.
\par Future work involves developing and analyzing communication-efficient distributed algorithms, which balance the computational and communication cost of different agents, for SDP with diagonal constraints. The objective SDP problem of this paper has diagonal constraints. In future, we will further attempt to extend the distributed algorithms to more general semi-definite programs with linear constraints. 
\bibliographystyle{ieeetran}
\bibliography{refer}

%\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{Xia_Jiang.jpg}}]{Xia Jiang}
%	received the bachelor's degree in Control Science and Engineering from Shandong University, Ji'nan, China, in 2017. She is currently pursuing the Ph.D. degree in control science and engineering with the School of Automation. Her current research interests include distributed optimization and distributed computation of semi-definite problems.
%\end{IEEEbiography}
%
%\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{Xianlin_Zeng.png}}]{Xianlin Zeng} (S'12-M'15) received the B.S. and M.S. degrees in Control Science and Engineering from the Harbin Institute of Technology,
%	Harbin, China, in 2009 and 2011, respectively, and the Ph.D. degree in Mechanical
%	Engineering from the Texas Tech University in 2015. He is currently an associate professor in the Key Laboratory of Intelligent Control and Decision of Complex Systems, School of Automation, Beijing Institute of
%	Technology, Beijing, China. His current research interests include distributed
%	optimization, distributed control, and distributed computation of network systems.
%\end{IEEEbiography}
%\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{Jian_Sun.png}}]{Jian Sun} received the bachelor's degree from the Department of Automation and Electric Engineering, Jilin Institute of Technology, Changchun, China, in 2001, the master's degree from the Changchun Institute of Optics, Fine Mechanics and Physics, Chinese Academy of Sciences (CAS), Changchun, China, in 2004, and the Ph.D. degree from the Institute of Automation, CAS, Beijing, China, in 2007.
%	
%	He was a Research Fellow with the Faculty of Advanced Technology, University of Glamorgan, Pontypridd, U.K., from 2008 to 2009. He was a Post-Doctoral Research Fellow with the Beijing Institute of Technology, Beijing, from 2007 to 2010. In 2010, he joined the School of Automation, Beijing Institute of Technology, where he has been a Professor since 2013. His current research interests include networked control systems, time-delay systems, and security of Cyber-physical systems.
%	
%	Dr. Sun is an Editorial Board Member of the {\em Journal of Systems Science \& Complexity} and {\em Acta Automatica Sinica}.
%\end{IEEEbiography}
%\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{Jie_Chen.jpg}}]{Jie Chen} (M'09-SM'12-F'19) received the B.S., M.S., and Ph.D. degrees from the Beijing Institute of Technology (BIT), Beijing, China, in 1986, 1996, and 2001, respectively.
%	
%	He is currently a Professor with the School of Automation, BIT, and serves as the Director of the Key Laboratory of Intelligent Control and Decision of Complex Systems, BIT, and the President of Tongji University, Shanghai, China. His research interests include complex systems, multiagent systems, multiobjective optimization and decision, constrained nonlinear control, and optimization methods.
%	
%	Prof. Chen serves as the Managing Editor for the Journal of Systems Science and Complexity, and is editorial board members and associate editors for several journals, including IEEE TRANSACTIONS ON CYBERNETICS, INTERNATIONAL JOURNAL OF ROBUST AND NONLINEAR CONTROL, and SCIENCE CHINA INFORMATION SCIENCES. He is a recipient of the National Natural Science Award of China, and the National Science and Technology Progress Awards of China. He is an Academician of Chinese Academy of Engineering, a Chief Scientist of National 973 Basic Research Program, Principal Investigator of an Innovative-Research-Group Program supported by the Natural Science Foundation of China (NSFC). He serves as the Vice President of the Chinese Association of Automation, and an Executive Director of the Chinese Artificial Intelligence Society.
%\end{IEEEbiography}

\end{document}





