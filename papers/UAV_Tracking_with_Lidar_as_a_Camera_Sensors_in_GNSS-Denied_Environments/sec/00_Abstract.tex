
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%%                ABSTRACT                  %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{abstract}%
    \label{sec:abstract}%
    Light detection and ranging (LiDAR) sensor has become one of the primary sensors in robotics and autonomous system for high-accuracy situational awareness. In recent years, multi-modal LiDAR systems emerged, and among them, LiDAR-as-a-camera sensors provide not only 3D point clouds but also fixed-resolution 360\textdegree panoramic images by encoding either depth, reflectivity, or near-infrared light in the image pixels. This potentially brings computer vision capabilities on top of the potential of LiDAR itself. In this paper, we are specifically interested in utilizing LiDARs and LiDAR-generated images for tracking Unmanned Aerial Vehicles (UAVs) in real-time which can benefit applications including docking, remote identification, or counter-UAV systems, among others. This is, to the best of our knowledge, the first work that explores the possibility of fusing the images and point cloud generated by a single LiDAR sensor to track a UAV without a priori known initialized position. We trained a custom YOLOv5 model for detecting UAVs based on the panoramic images collected in an indoor experiment arena with a motion capture (MOCAP) system. By integrating with the point cloud, we are able to continuously provide the position of the UAV. Our experiment demonstrated the effectiveness of the proposed UAV tracking approach compared with methods based only on point clouds or images. Additionally, we evaluated the real-time performance of our approach on the Nvidia Jetson Nano, a popular mobile computing platform. 
    %
   % LiDAR has become one of the primary sensors in robotics and autonomous system for high-accuracy situational awareness. In recent years, multi-modal LiDAR systems emerged, and among them, %Ouster 
   % LiDAR-as-a-camera sensors
   % %LiDARs 
   % provide not only 3D point clouds but also fixed-resolution 360\textdegree panoramic images by encoding either depth, reflectivity, or near-infrared light in the image pixels. This potentially brings computer vision capabilities on top of the potential of LiDAR itself.
   % %within one LiDAR device by using LiDAR as a camera. 
   % In this paper, we are specifically interested in utilizing %the Ouster 
   % LiDARs and LiDAR-generated images for tracking Unmanned Aerial Vehicles (UAVs) in real-time which can benefit %certain 
   % applications including docking, remote identification, or counter-UAV systems, among others. This is, to the best of our knowledge, the first work that explores the possibility of fusing the images and point cloud generated by %the ouster 
   % a single LiDAR sensor to track a UAV without a priori known initialized position. 
   % % we specifically explore the possibility of fusing the images and point cloud generated by the ouster LiDAR for tracking Unmanned Aerial Vehicles (UAVs) in real-time. 
   % We trained a custom YOLOv5 model for detecting UAVs based on the %ouster 
   % panoramic images collected in an indoor experiment arena with a MOCAP system. By integrating with the point cloud, we are able to continuously provide the position of the UAV. Our experiment demonstrated the effectiveness of the proposed UAV tracking approach compared with methods %mainly 
   % based only on point clouds or images. Additionally, we evaluated the real-time performance of our approach on the Nvidia Jetson Nano, a popular mobile computing platform. 
   %
   % a new method for real-time tracking of Unmanned Aerial Vehicles (UAVs) using LiDAR-as-Camera methods. 
   % Our approach utilizes LiDAR sensors to generate images for computer vision applications and trains YOLOV5 to accurately detect and track UAVs in real-time. The UAV dataset collected in indoor environments is used to train the model, and the LiDAR point cloud data is fused with the LiDAR signal image stream to obtain the UAV's trajectory data. Our experiments demonstrate that the proposed method performs well in terms of both accuracy and speed,
   % and is able to effectively track UAVs even in challenging scenarios such as low-light conditions. The method can be applied in various scenarios, such as UAV landing and special flights, to provide effective external tracking.

\end{abstract}

\begin{IEEEkeywords}

    UAV; LiDAR-as-a-camera; drone tracking; LiDAR;
    LiDAR detection; LiDAR tracking; YOLOV5; 

\end{IEEEkeywords}