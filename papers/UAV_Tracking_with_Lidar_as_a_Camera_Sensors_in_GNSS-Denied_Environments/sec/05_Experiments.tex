%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%%              EXPERIMENTS                 %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.495\textwidth]{fig/UAV_points.pdf}
    % \caption{UAV in point cloud}
    \caption{UAV point cloud of drones at different distances, the bottom line shows UAV point cloud of four consecutive frames at the same long distance.}
    \label{fig:pcd_frame}
\end{figure}

\begin{figure*}[ht]
    \begin{subfigure}{.32\textwidth}
        \centering
        \setlength{\figurewidth}{\textwidth}
        \setlength{\figureheight}{1.05\textwidth}
        \scriptsize{\input{tex/error_seq/error_seq1}}
        \caption{\scriptsize{Seq\,1}}
        \label{fig:err_seq1}
    \end{subfigure}
    \hfill
    \begin{subfigure}{.32\textwidth}
        \centering
        \setlength{\figurewidth}{\textwidth}
        \setlength{\figureheight}{1.03\textwidth}
        \scriptsize{\input{tex/error_seq/error_seq2}}
        \caption{\scriptsize{Seq\,2}}
        \label{fig:err_seq2}     
    \end{subfigure}
    \hfill
    \begin{subfigure}{.32\textwidth}
        \centering
        \setlength{\figurewidth}{\textwidth}
        \setlength{\figureheight}{\textwidth}
        \scriptsize{\input{tex/error_seq/error_seq3}}
        \caption{\scriptsize{Seq\,3}}
        \label{fig:err_seq3}     
    \end{subfigure}
    \caption{Absolute position error (APE) value of three data sequences.}
    \label{fig:ape_error} 
\end{figure*}


\begin{figure*}[t]
    \begin{subfigure}{.32\textwidth}
        \centering
        \setlength{\figurewidth}{\textwidth}
        \setlength{\figureheight}{\textwidth}
        \scriptsize{\input{tex/XYZ_plot/XY_plot_seq3}}
        \caption{\scriptsize{XY trajectory plot}}
        \label{fig:xy_plot}
    \end{subfigure}
    \hfill
    \begin{subfigure}{.32\textwidth}
        \centering
        \setlength{\figurewidth}{\textwidth}
        \setlength{\figureheight}{\textwidth}
        \scriptsize{\input{tex/XYZ_plot/XZ_plot_seq3}}
        \caption{\scriptsize{XZ trajectory plot}}
        \label{fig:xz_plot}     
    \end{subfigure}
    \hfill
    \begin{subfigure}{.32\textwidth}
        \centering
        \setlength{\figurewidth}{\textwidth}
        \setlength{\figureheight}{\textwidth}
        \scriptsize{\input{tex/XYZ_plot/YZ_plot_seq3}}
        \caption{\scriptsize{YZ trajectory plot}}
        \label{fig:yz_plot}     
    \end{subfigure}
    \caption{Comparison of estimated trajectories with the point cloud tracking method and our proposed method from three different projections.}
    \label{fig:full_traj} 
\end{figure*}

\begin{figure*}[t]
    \begin{subfigure}{.32\textwidth}
        \centering
        \setlength{\figurewidth}{\textwidth}
        \setlength{\figureheight}{\textwidth}
        \scriptsize{\input{tex/velo_error_seq/velo_error_seq1}}
        \caption{\scriptsize{Seq\,1}}
        \label{fig:vel_err_seq1}
    \end{subfigure}
    \hfill
    \begin{subfigure}{.32\textwidth}
        \centering
        \setlength{\figurewidth}{\textwidth}
        \setlength{\figureheight}{\textwidth}
        \scriptsize{\input{tex/velo_error_seq/velo_error_seq2}}
        \caption{\scriptsize{Seq\,2}}
        \label{fig:vel_err_seq2}     
    \end{subfigure}
    \hfill
    \begin{subfigure}{.32\textwidth}
        \centering
        \setlength{\figurewidth}{\textwidth}
        \setlength{\figureheight}{\textwidth}
        \scriptsize{\input{tex/velo_error_seq/velo_error_seq3}}
        \caption{\scriptsize{Seq\,3}}
        \label{fig:vel_err_seq3}     
    \end{subfigure}
    \caption{Velocity estimation error for each linear component in the three data sequences.}
    \label{fig:vel_errs} 
\end{figure*}


\section{Experimental Results}

In this section, we report the experimental results, based on the three data sequences gathered in the indoor test environment. 

\subsection{UAV in the Ouster LiDAR point cloud}

The first parameter to analyze is the number of points that reflect from the UAV at different distances. Our analysis, shown in Fig~\ref{fig:pcd_frame}, reveals that the point cloud structure generated by the UAV is significantly influenced by the distance from the target. At short distances, the point cloud produced by LiDAR is abundant and presents comprehensive details. When the distance extends to a medium range, the number of UAV point clouds decreases to less than 100, but the three-dimensional structure of the UAV remains discernible. However, when the distance is at a medium range of 7\,m according to our results, the number of UAV point clouds reduces to single digits, and the point cloud structure becomes highly unpredictable and unstructured. It is worth noting that in a more realistic application, additional elements such as other sensor payloads or a cargo bay would potentially increase significantly the reflective surface of the UAV. 



\subsection{Trajectory Validation}

We also show a quantitative analysis of the APE based on ground truth, with the main results summarized in Fig.~\ref{fig:ape_error}. To ensure the trajectories are compared under the same coordinates, we utilize the coordinates of the ground truth as the reference coordinates and convert all trajectories generated by the three UAV tracking methods to these coordinates. 
% For the primary evaluation metric, we employ the absolute pose error (APE) [29]. To compute the error for each trajectory, we use the open-source EVO toolset. This enables us to conduct a comprehensive evaluation of the accuracy of each tracking method.
Table~\ref{tab:methods_compare} presents a comprehensive comparison of three different UAV tracking methods in terms of detectable distance, average APE, algorithm update frequency, and need for initial conditions. 
\input{tb/perf}

The image-based UAV tracking method shows a relatively small average error; however, its overall error distribution is inconsistent, as the Y-axis error in the \textit{Seq\,1} sequence reaches up to 0.3\,m. Conversely, the point cloud-based UAV tracking method has the largest average error, but its error distribution is more uniform. Our proposed method, on the other hand, achieves the smallest average error and minimal error fluctuation. Additionally, to supplement the quantitative trajectory analysis, we also provide a visualization of the trajectories based on the point cloud tracking method and our proposed method from three different viewpoints, as illustrated in Fig~\ref{fig:full_traj}, with more consistent behavior.


\subsection{Velocity Validation}

In addition to pose estimation, we conducted a quantitative analysis of the UAV velocities based on the ground truth data and compared them with different UAV tracking methods. Fig.~\ref{fig:vel_errs} illustrates the velocity errors of each method along the X, Y, and Z axes. The experimental results reveal that all methods have similar mean values of the velocity errors, but different fluctuations. The image tracking method has a large fluctuation in the Y-axis velocity error, reaching up to \textit{0.75 m/s} in \textit{Seq\,3}. The point cloud tracking method also has relatively large fluctuations in all dimensions. In contrast, our method achieves smaller overall velocity errors in both the mean value and fluctuation range.

\subsection{Resource Consumption}
% To validate the performance of our UAV tracking design and better understand its limitations, we collected three flight trajectory sequences (\textit{Seq 1}, \textit{Seq 2}, and \textit{Seq 3}) in a large open area of 10 x 10 square meters, 0.5 to 8 meters away from the LiDAR scanner, as detailed in Table~\ref{tab:sequence_detail}. \textit{Seq 1} and \textit{Seq 3} represent a helical ascension trajectory, while \textit{Seq 2} represents an elliptical trajectory. We first focused on analyzing \textit{Seq 3} and compared it to the ground truth trajectory obtained from the Mocap system. 

% We conducted the experiment on two different platforms, the Lenovo Legion Y7000P equipped with 16GB RAM, 6-core Intel i5-9300H (2.40GHz) and Nvidia GTX 1660Ti (1536 CUDA cores, 6GB VRAM), as well as the commonly used embedded computing platform Jetson Nano with 4-core ARM A57 64-bit CPU (1.43GHz), 4GB RAM, and 128-core Maxwell GPU. 
Both the Intel laptop and the Jetson Nano run ROS Melodic on Ubuntu 18.04. The CPU and memory utilization is measured with a ROS resource monitor tool~\footnote{\href{https://github.com/alspitz/cpu_monitor}{https://github.com/alspitz/cpu\_monitor}}. Additionally, for minimizing the difference of the operating environment, we unified the dependencies used in each method into same version. The results are summarized in Table~\ref{tab:runtime_src}.

The memory utilization of each selected method was roughly equivalent in both processor architecture platforms. However, the same algorithm showed generally higher CPU utilization and achieved the highest publishing rate when running on the Intel processor. For the Intel processor, the point cloud tracking method had higher CPU utilization than other methods but the lowest publishing rate. The fusion method performed well on the laptop and had the smallest overall error. On the embedded computing platform, the CPU utilization of all methods did not differ significantly, and the point cloud tracking method had the lowest memory utilization but the lowest pose publication rate. The difference in CPU utilization is caused by the use of CUDA GPU acceleration in the Open3D binaries utilized for the Jetson Nano platform, while the Intel computer uses only the CPU for point cloud data processing. The image processing also leverages the embedded GPU in the Jetson Nano board. Because of the small ROI that we extract to process the point cloud, the fused method adds little overhead on top of the vision-only method.

\input{tb/res}


% Fig~\ref{fig:full_traj} compares the trajectory obtained by our tracking method with the ground truth trajectory from three different perspectives of \textit{Seq 3}, while Fig~\ref{fig:ape_error} shows the errors on the X, Y, Z axes, and overall for \textit{Seq 1}, \textit{Seq 2}, and \textit{Seq 3}.

% \begin{table}[htb]
% \centering
% \caption{ Details of sequences that use for our experiment. } 
% \renewcommand{\arraystretch}{1.2}
% \resizebox{\linewidth}{!}{%
% \begin{tabular}{@{}lcccc@{}}
% \hline
% Sequences & Time (s) & Ground Truth & Trajectory & Distance (m) \\ \hline
% \textit{Seq 1}& 35.8 & Mocap  &  elliptical trajectory & 7.0    \\
% \textit{Seq 2} & 26.9 &Mocap  & spiral trajectory & 6.3   \\
% \textit{Seq 3} &32.7 & Mocap  & spiral trajectory & 8.0   \\\hline
% \end{tabular}
% }
%     \label{tab:sequence_detail}   
% \end{table}





% \begin{figure*}[t]
%     \begin{subfigure}{.23\textwidth}
%         \centering
%         \setlength{\figurewidth}{1.2\textwidth} % here
%         \setlength{\figureheight}{\textwidth}
%         \scriptsize{\input{tex/Velo_x}}
%         \label{fig:Velo_x}
%         \caption{\scriptsize{Velo\_x}}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}{.23\textwidth}
%         \centering
%         \setlength{\figurewidth}{1.2\textwidth}
%         \setlength{\figureheight}{\textwidth}
%         \scriptsize{\input{tex/Velo_y}}
%         \caption{\scriptsize{Velo\_y}}
%         \label{fig:Velo_y}     
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}{.23\textwidth}
%         \centering
%         \setlength{\figurewidth}{1.2\textwidth}
%         \setlength{\figureheight}{\textwidth}
%         \scriptsize{\input{tex/Velo_z}}
%         \caption{\scriptsize{Velo\_y}}
%         \label{fig:Velo_y}     
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}{.23\textwidth}
%         \centering
%         \setlength{\figurewidth}{1.2\textwidth}
%         \setlength{\figureheight}{\textwidth}
%         \scriptsize{\input{tex/Velo_a}}
%         \caption{\scriptsize{Velo\_all}}
%         \label{fig:Velo_y}     
%     \end{subfigure}
%     \caption{On the left is a top view of the trajectory comparison, and on the right is a side view of the trajectory comparison.}
%     \label{fig:velo_xyza}
% \end{figure*}



% \subsection{blah blah}
% We also compared our method with a UAV tracking method based on Ouster OS0-128 LiDAR images and point clouds in the environment of \textit{Seq 3}, and the trajectories are shown in Fig~\ref{fig:full_traj}. The point cloud tracking method only uses Ouster OS0-128 LiDAR point cloud data as input, with a frame rate of 10Hz. When tracking the UAV using only point cloud data as input, we need to know the initial position of the UAV, because the point cloud of the UAV is sparser than that of larger objects such as cars or humans, and it is difficult to distinguish the point cloud of the UAV from the environment using features. The image tracking method only uses Ouster OS0-128 LiDAR signal images, with a frame rate of 10Hz. First, the signal image is subjected to target detection processing to obtain the bounding box of the UAV in the signal image, and then the image in the bounding box is converted into point cloud data. After obtaining the point cloud in the ROI, the point cloud clustering algorithm is used to separate the point cloud of the UAV based on the number and distance features of the point cloud clusters, thus obtaining the trajectory of the UAV.
% \begin{figure}[htb] 
%     \centering   
%     \includegraphics[width=0.4\textwidth]{fig/to_do_sign.png}  
%     \caption{traj-fig to fill.}
%     \label{fig:to-do} 
% \end{figure}
% \begin{figure}[h] 
%     \centering 
%     \scriptsize{\input{tex/velo_error_compare}}
%     \caption{compare velo error of different methods. }
%     \label{fig:velo_arr} 
% \end{figure}


% Table~\ref{tab:methods_compare} compares these three methods in terms of detectable range, average ATE error and root mean square error (RMSE), operating speed, and prerequisite conditions, while Fig~\ref{fig:vel_errs} shows the velocity error of different UAV tracking methods.
% Based on the experimental results above, it can be concluded that the point cloud tracking method is superior to the image tracking method in terms of the detectable range, but the method runs slowly, with an average frame rate of only 1.55 Hz, because this method does not limit the number of point clouds to be clustered by selecting ROI. Moreover, due to the slow running speed, the speed error of this method is very large.Although the image tracking method has advantages in accuracy and operating speed, and the speed error of this method is the smallest, the detection distance of this method is only 2.4 m. Our method combines Ouster OS0-128 LiDAR image and point cloud data, which has advantages in both running speed and detection range. Although the error is slightly larger than that of the image tracking method, the average error is only 3.6 cm, and the speed error is not significantly different from that of the image tracking method, only slightly larger, with a total error of about 0.19 m/s.












% \begin{figure}[htb] 
%     \centering   
%     \includegraphics[width=0.45\textwidth]{fig/box_error.png}  
%     \caption{Box plot of the error of X value, Y value, Z value, and trajectory (Euclidean distance). }
%     \label{fig:box_error} 
% \end{figure}

% \begin{figure}[t] 
%     \centering 
%     \scriptsize{\input{tex/velo_error}}
%     \caption{velo_error. }
%     \label{fig:velo_all_seq} 
% \end{figure}