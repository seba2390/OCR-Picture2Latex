
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%%              RELATED WORKS               %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \newpage
\section{Related Work} \label{sec:related_work}

This section reviews the literature in the field of UAV detection and tracking. Due to the limited research on tracking small objects such as UAVs based on LiDAR, we focus on: (i) vision-based and (ii) LiDAR-based UAV tracking; (iii) research into the potential of LiDAR-as-a-camera sensors; and (iv) applications of UAV tracking.

\subsection{UAV tracking with cameras}

Vision-based methods are widely used to track small objects and UAVs~\cite{mueller2016benchmark, 5354489, 8988144}. They can be divided into two categories: those that rely on passive or active visual markers, and those that detect and track objects in general, e.g., with traditional computer vision or deep learning.
For example, \cite{5354489}~introduces a trinocular system with ground-based cameras to control a rotary-wing UAV in real time based on its key features. Alternatively, \cite{6696776} presents an infrared binocular vision system with PTU and exosensors to track drones cheaply under any weather and time conditions based on their infrared spectra. Recent developments in deep convolutional neural networks (CNNs) have boosted adoption in the field of object detection and tracking. Arguably, a large part of the state-of-the-art in tracking is based on deep learning methods. Recent advances in deep CNNs have improved object detection and tracking performance~\cite{li2018deep}. For instance, \cite{8988144}~proposes a CNN-based markerless UAV relative positioning system that allows the stable formation and autonomous interception of multiple UAVs.

Depth cameras can also detect UAVs and help them avoid obstacles using deep learning models that process depth maps ~\cite{carrio2018drone}. While depth cameras can provide accurate position and size measurements, and vision sensors are generally capable of robust tracking and relative localization, our focus in this paper is on the use of Ouster LiDARs because of their flexibility with respect to environmental conditions and their ability to provides more accurate and informative signal images than depth cameras.

\subsection{UAV tracking with LiDARs}

While LiDAR systems are often employed for detecting and tracking objects, they pose unique challenges in detecting and tracking UAVs due to their small size, varied shapes and materials, high speed, and unpredictable movements.

When deployed from a ground robot, a crucial parameter is relative localization between different devices. Li et al.~\cite{qingqing2021adaptive} suggest a new approach for tracking UAVs using LiDAR point clouds. They take into account the UAV speed and distance to adjust the LiDAR frame integration time, which affects the density and size of the point cloud to be processed.

By conducting a probabilistic analysis of detection and ensuring proper setup, as shown in~\cite{dogru2022dronedetection}, it is possible to achieve detection using fewer LiDAR beams, while performing continuous tracking only on a small number of hits. The limitations in the 3D LiDAR technology can be overcome by moving the sensor to increase the field of view and improve the coverage ratio. Additionally, combining a segmentation approach and a simple object model while leveraging temporal information in~\cite{razlaw2019DetectionAT} has been shown to reduce parametrization effort and generalize to different settings.

Another approach, departing from the typical sequence of track-after-detect, is to leverage motion information by searching for minor 3D details in the 360$^{\circ}$ LiDAR scans of the scene. If these clues persist in consecutive scans, the probability of detecting a UAV increases. Furthermore, analyzing the trajectory of the tracked object enables the classification of UAVs and non-UAV objects by identifying typical movement patterns~\cite{hammer2018potentiallidardetection, hammer2018lidarsmalluavs}.

\subsection{LiDAR as a camera}

The Ouster LiDAR blurs the traditional boundaries between cameras and LiDARs by providing 360\textdegree panoramic images, including depth, signal, and ambient images in the near-infrared spectrum, in addition to the point cloud output. These signal and ambient images capture the strength of laser light reflected back to the sensor and ambient light, respectively. Ouster has proven that training models based on these images are effective for various computer vision applications, such as object detection, segmentation, feature extraction, and optical flow~\cite{tampuu2022lidar,angus2018lidar}. Several existing DL models have also been demonstrated to be effective in analyzing these images~\cite{xianjia2022analyzing}. 

\subsection{Applications of UAV tracking}

Recently, researchers have shown interest in tracking and detecting UAVs due to two primary reasons: the rising demand for identifying and detecting foreign objects or drones in areas with controlled airspace, like airports~\cite{guvenc2018detection, hengy2017multimodal}, and the potential for optimizing the utilization of UAVs as versatile mobile sensing platforms through tracking and detection~\cite{queralta2020autosos}.

The ability to track UAVs from unmanned ground vehicles (UGVs) allows for miniaturization and greater flexibility in multi-robot systems, reducing the need for high-accuracy onboard localization. This was demonstrated in the DARPA Subterranean challenge~\cite{rouvcek2019darpa, petrlik2020robust}, where UAVs were dynamically deployed from UGVs in GNSS-denied environments. Localization and collaborative sensing were key challenges, with reports indicating that LiDAR-based tracking was useful in domains where visual-inertial odometry (VIO) has limitations, such as low-visibility situations~\cite{queralta2020vio, qingqing2019offloading}.

Similarly, tracking UAVs is crucial in the landing phase of the aerial system. Different methods using a ground-based stereo camera~\cite{kong2013uavstereolanding} or having the UAV carry an infrared camera to detect signals from the destination~\cite{gui2013uavinfraredlanding} have been proposed. As these works employ cameras as their main sensory system, they can be easily affected by background lighting conditions while in our approach we prefer a LiDAR which is more resilient in these environmental conditions.
