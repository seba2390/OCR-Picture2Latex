%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%%              INTRODUCTION                %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}\label{sec:introduction}
\thispagestyle{FirstPage}

Achieving high-level situational awareness is a vital but %formidable 
not trivial task in the domain of mobile robotics and autonomous systems in general, as it enables effective decision-making, navigation, and control in %heterogeneous 
complex environments~\cite{fan2019key}. In recent years, researchers have exploited diverse sensor modalities to enhance robotics perception systems. Among these, LiDAR and cameras have emerged as the primary perception components~\cite{kato2018autoware}. In contrast to cameras, LiDARs offer key features, including long-range and precise geometry data, and generally greater resilience to adverse weather conditions, such as fog and rain. 
Currently, there are various modalities of LiDARs available~\cite{qingqing2022multi}. Ouster LiDARs~\cite{tampuu2022lidar, angus2018lidar, xianjia2022analyzing} stand out by offering dense point clouds and 360\textdegree field of view with low-resolution images, obtained by encoding depth, reflectivity, or near-infrared light in the image pixels. These are so-called lidar as a camera sensors~\cite{tampuu2022lidar, xianjia2022analyzing}. Other lidar manufacturers are expected to provide similar functionality in the near future.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.48\textwidth]{fig/system_arch_fancy.pdf}
    \caption{Diagram of proposed UAV tracking system based on the image and point cloud generated by an Ouster LiDAR.}
    \label{fig:concept}
\end{figure}

This type of LiDAR as a camera sensor is inherently compatible with deep learning (DL) models for vision sensors, obviating the need for external camera mounting and calibration~\cite{xianjia2022analyzing}. Moreover, the LiDAR-as-camera approach can augment conventional LiDAR-based methods for object detection and tracking, as the algorithms for the latter are more computationally intensive and less mature than vision sensors. Despite the low vertical resolution and lack of color, the key motivation for considering such an approach is simply to do more with already existing data and without additional sensors.

We are interested in studying the potential of these sensors for tracking unmanned aerial vehicles (UAVs), fusing both lidar point cloud data and signal images. Given that the images generated by the LiDAR as a camera sensors have low resolution (see for example in figure~\ref{fig:pcd_frame}), we are particularly interested in short-range tracking of UAVs, which finds potential applications in UAV docking, or collaboration, among others. The deployment of UAVs requires accurate localization, especially in cases where GNSS signals suffer from degradation or are not available~\cite{li2018high}. 

In this paper, we propose a UAV tracking approach based on the integration of images and 3D point clouds generated by an Ouster LiDAR sensor. The diagram of the approach we follow is illustrated in figure~\ref{fig:concept}. The UAV can be detected in signal images instead of manually giving its initial position as it is needed in other point-cloud-only approaches. The detection also yields an approximate region of interest (ROI) in the point cloud, which will be expanded if no detection occurs. This approach reduces computation overhead by avoiding the need for an overall point cloud search. UAV identification is achieved by clustering points within the ROI, followed by continuous position estimation using the Kalman Filter (KF).

The remainder of this document is organized as follows. Section II introduces the current state-of-the-art UAV tracking methods relevant to the presented approach. Section III  introduces the methodology and the experimental setup with results presented in Section IV. Section V concludes the work and outlines future research directions.

% Given the low-resolution images generated by the Ouster LiDAR, we are particularly interested in short-range tracking of unmanned aerial vehicles (UAVs). This capability could potentially benefit applications such as UAV docking and collaboration. Accurate UAV localization is essential, particularly in scenarios where GNSS signals are degraded or not accessible.



% The conceptual figure can be seen in figure~\ref{fig:concept}.

% \red{Multiple industrial use cases benefit from the deployment of unmanned aerial vehicles (UAVs)~\cite{shakhatreh2019unmanned}. When accurate localization is needed, GNSS-RTK is the de-facto standard for gathering aerial data with UAVs~\cite{li2018high}. For example, high-accuracy photogrammetry~\cite{lee2018assessment}, civil infrastructure monitoring~\cite{kim2018structural}, or in urban environments where GNSS signals suffer more degradation~\cite{li2018high}. As UAVs become ubiquitous across different domains and application areas~\cite{queralta2020collaborative}, having access to more flexible and lower-cost solutions to precise UAV navigation can aid in accelerating adoption and widespread use. In this paper, we consider the problem of UAV navigation through relative localization to a companion unmanned ground vehicle (UGV). We consider a ground robot as a more flexible platform from the point of view of deployment, but in simulations, we also consider localization based on fixed beacons in the environment, closer to how GNSS-RTK systems are deployed.}

% Within the different approaches that can be used for cooperative relative localization, from visual sensors~\cite{hui2013autonomous} to cooperative SLAM~\cite{kim2019uav}, wireless ranging technologies offer high performance with low system complexity~\cite{queralta2020uwb}. In particular, ultra-wideband (UWB) wireless ranging offers unparalleled localization performance within the different radio technologies in unlicensed bands~\cite{shule2020uwb}. Other benefits of UWB include resilience to multipath, high time resolution, and low interference with other radio technologies~\cite{yu2021applications}.


