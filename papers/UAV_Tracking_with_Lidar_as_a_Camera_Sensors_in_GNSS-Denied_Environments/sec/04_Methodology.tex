%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%%              METHODOLOGY                 %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \newpage
\section{Methodology}

\subsection{Hardware Information}

The experimental setup consists of an Ouster OS0-128 LiDAR, an Intel computer, and a Holybro X500 V2 drone  shown in Fig.~\ref{fig:track_device}. The Ouster OS0-128 boasts a wide field of view ($360^\circ \times 90^\circ$) and is capable of producing both dense point cloud data and signal images at a frequency of 10\,Hz. The drone is equipped with OptiTrack markers, allowing the acquisition of the drone's actual position at 100\,Hz in the motion capture (MOCAP) system, which is also partially visible in Fig.~\ref{fig:track_device}.

% The Intel computer has 16\,GB RAM, a 6-core Intel i5-9300H (2.40\,GHz) processor, and an Nvidia GTX 1660Ti (1536 CUDA cores, 6\,GB VRAM).

\begin{figure}[t] 
    \centering   
    \includegraphics[width=0.48\textwidth]{fig/hardware_info.pdf}  
    \caption{Experimental hardware and site.}
    \label{fig:track_device} 
\end{figure}

\subsection{Software Information}

The system was implemented based on the ROS Noetic framework on Ubuntu 20.04 operating system. The tracking package, Ouster drivers, and OptiTrack mocap program were executed on the laptop computer connected to the Ouster LiDAR. Our tracking approach requires YOLOv5~\footnote{\href{https://github.com/ultralytics/yolov5/releases}{https://github.com/ultralytics/yolov5/releases}} for UAV detection and Open3D~\footnote{\href{http://www.open3d.org/}{http://www.open3d.org/}} for point cloud data processing. The algorithms and code designed and developed for these experiments are written in Python and are publicly available in a GitHub repository~\footnote{\href{http://github.com}{https://github.com/TIERS/UAV-tracking-based-on-LiDAR-as-a-camera}}.
% he computer ran the OptiTrack receiver node program, the Ouster LiDAR driver, and our MAV tracking package. 

% The Ouster LiDAR driver provides real-time, 1:1 spatially synchronized point cloud data and signal images, ensuring that every pixel in the 2D structured data corresponds to a 3D point in the LiDAR data without any discretization or resampling occurring. 
% The latter was implemented as a ROS node and capable of simultaneously processing point cloud data and signal images in real time. 
% The position of the UAV was extracted from the point cloud and laser radar signal image using a point cloud library (Open3D) and image object detection (YOLOV5), respectively.

\subsection{Data collection}\label{subsec:data}

Regarding the data consisting of UAV detections with the Ouster LiDAR, we collected three different data sequences ($Seq \quad i, i \in (1 \sim 3)$).
% \textit{Seq 1}, \textit{Seq 2}, and \textit{Seq 3}) 
in an indoor area of $10.0 \times 10.0 m^2$, with distances ranging 0.5\,m to 8\,m between the LiDAR and the UAV. The details of the collected data can be seen in Table~\ref{tab:sequence_detail}. \textit{Seq\,1} and \textit{Seq\,3} represent a helical ascension trajectory, while \textit{Seq 2} represents an elliptical trajectory. 
 
\input{tb/data}


\subsection{UAV tracking fusing signal images and point clouds}

This manuscript introduces an approach to tracking a UAV by fusing LiDAR signal images and point cloud data, both generated within the same sensor. The proposed methodology's overarching framework is illustrated in Fig.~\ref{fig:concept}. It is worth noting that the Ouster OS0-128 LiDAR serves as the sole input source for the entire system, providing real-time UAV position outputs. The tracking procedure comprises two primary stages, namely:

% (i) Obtain the initial position of the UAV
\subsubsection{Initialization of UAV position}

Fig~\ref{fig:signal_image} shows the signal image and point cloud data of a UAV at its initial position. Notably, when the UAV approaches the Ouster LiDAR, the signal image of the UAV appears clearer. To detect the position of the UAV in the signal image and obtain the ROI in the image, the state-of-the-art object detection algorithm YOLOV5 is utilized. Given that the Ouster LiDAR signal image and the point cloud data are spatially linked, the corresponding point cloud ROI can be extracted. Subsequently, by employing ground removal and point cloud clustering techniques, the UAV point cloud can be extracted and, as such, the initial position of the UAV can be estimated.

\begin{figure}[b] 
    \centering   
    \includegraphics[width=0.45\textwidth]{fig/signal_img.pdf}  
    \caption{Example of a signal image (top) and its corresponding point cloud with background removed (bottom).}
    \label{fig:signal_image} 
\end{figure}


% (ii) fusing signal images and point clouds
\subsubsection{Fusion of signal image and point cloud data}

Our goal for fusing the signal image and point cloud data is to acquire an accurate ROI. Initially, we perform image detection on each signal image to identify the UAV. This allows us to obtain a more precise ROI. Additionally, we can use the UAV's initial position from the previous step as a reference to extract the UAV point cloud from the environment based on the number of UAV point clouds and their distance. However, if YOLOv5 fails to detect the UAV, we select the ROI predicted by the Kalman filter and separate the UAV point cloud from it. This process is explicated in Algorithm~\ref{alg:fusing_data}.


\input{algs/al_kf}

\subsection{Evaluation}
% In order to validate the accuracy of the estimated poses and velocities of the UAV, we calculated the absolute pose error (APE) and velocity error based on the ground truth from the mocap system. Additionally, as Jetson Nano is a popular mobile computing platform, we  performed our tracking program in it to evaluate the computation consumption as a reference.
To validate the precision of the UAV estimated poses and velocities by our approach, we calculate the absolute pose error (APE) and velocity error based on the ground truth from the MOCAP system. We conducted a comparative analysis of our proposed method with a UAV tracking method that solely relies on either Ouster LiDAR images or point clouds.
The point cloud tracking method uses only Ouster OS0-128 LiDAR point cloud data as input, with a frame rate of\,10Hz. When tracking the UAV using point cloud data, the initial position of the UAV needs to be known as the point cloud of the UAV is sparser than that of larger objects, such as cars or humans, and distinguishing the point cloud of the UAV from the environment using features is challenging. On the other hand, the image tracking method uses only Ouster OS0-128 LiDAR signal images with a frame rate of 10\,Hz. Firstly, the signal image undergoes target detection processing to obtain the UAV's bounding box in the signal image. Subsequently, the image in the bounding box is converted into point cloud data. The point cloud clustering algorithm is then utilized to separate the UAV's point cloud from the environment based on the number and distance features of the point cloud clusters. This approach allows us to obtain the trajectory of the UAV. Both of these methods are estimated by the Kalman filter method to obtain the UAV's trajectory.

We conducted the experiments on two different platforms to assess real-time performance, the Lenovo Legion Y7000P equipped with 16GB RAM, 6-core Intel i5-9300H (2.40GHz) and Nvidia GTX 1660Ti (1536 CUDA cores, 6GB VRAM), as well as the commonly used embedded computing platform Jetson Nano with 4-core ARM A57 64-bit CPU (1.43GHz), 4GB RAM, and 128-core Maxwell GPU. 

% Furthermore, in addition to the Intel laptop, we also implemented our tracking program on the Jetson Nano, a widely used mobile computing platform, to assess its computational efficiency. 
%The Jetson Nano has 4G RAM and Quad-core ARM Cortex-A57 64-bit with 1.43\,GHz. The evaluations were operated with all three collected data sequences described in~\ref{subsec:data}.

