\section{Methods}
\label {Sec:methodology}
Given the under-explored nature of the practices of mobile money users in prior work, we adopted an exploratory approach using qualitative methods. The data collection was completed between July and September 2022 with the help of a contractor identified through a formal bidding process. 
The interview we present here was immediately preceded by another interview during the same session that we report on in a separate paper. 

\subsection {Instrument design and pilots}
All tools, data collection procedures, and recruitment and screening material were developed by two lead researchers, natives of Kenya and Tanzania, with feedback from a team of co-authors with complementary expertise. This was useful in providing  interpretation of contextual nuances while developing the data collection instruments. We opted for structured interviews (Appendix \ref{Appendix_IDI}) to maintain consistency across the contract research assistants working in the two countries. The user-agent interview centered on the following topics: 
\begin{itemize}
    \item practices of MoMo users through the customer journey,
    \item perceptions of and experiences with agents, and
    \item security and privacy concerns in the use of MoMo.
\end{itemize}  

The questions were informed by prior studies that suggest the influence of trust in user behaviour and  practices such as agent selection \cite{chamboko2021role}, information disclosure \cite{yisa2023investigating}, and security delegation \cite{forget2016or}.

We conducted two iterations of cognitive walkthroughs with an independent expert and modified the questions for clarity after each iteration. We then completed three initial pilots with participants similar to  those in the target population, made further modifications, and completed two more pilots. We do not include pilot interview data in the results presented here. The two native researchers worked with native student assistants to translate the interview guides to Kiswahili, a language spoken in both Kenya and Tanzania, albeit with different regional nuances. 

\subsection{Participant sample and recruitment}
 The research assistants recruited 36 participants within each country. The respondent population was MoMo users in Kenya and Tanzania over the age of 18 years. 
We use stratified purposive sampling to achieve heterogeneity of the sample by gender, location (rural and urban), and age (Table \ref{table:demographics}). Although we planned in advance to conduct at least 36 interviews, we stopped there due to reaching saturation.
\begin {table} [htbp]
\caption{Number of participants per category}
\centering
\begin {tabular} {|p{1cm}|p{1.3cm}|p{1.2cm}|p{1.5cm}|}
\hline
& &Kenya & Tanzania \\
\hline
Age& 18-25& 13& 9 \\
& 26-39& 16& 14 \\
& 40-49& 3& 5\\
& 50-65& 4& 8\\
\hline
Gender& Female& 16& 18\\
& Male& 20& 18\\
\hline
Location& Urban& 18& 17\\
& Rural& 18& 19\\
\hline
\end{tabular}
\label{table:demographics}
\end{table}

\subsection {Data collection}
Each user-agent interview lasted approximately 20-40 minutes, with the total interview time for the two interviews averaging 75 minutes. All interviews in Kenya were face-to-face: 12 in English, 10 in Kiswahili, and 14 using a mix.\footnote{In Kenya, people often speak by mixing English and Kiswahili, and sometimes this mix morphs to `sheng' which is a Kiswahili and English-based slang} In Tanzania, all interviews  were over the phone, in Kiswahili, with the exception of one interview that used a mix. The different data collection methods in the two countries was necessitated by complex national protocol modification processes that were beyond the researchers' control. Research has shown that while different modes of interview data collection may present logistical variations, there is little to no impact to validity of interview data \cite{oates2022audio}. We asked the security- and privacy-related questions towards the end of the interview to minimize bias in how users may have perceived their interactions and actions with the agent. 

\subsection{Data analysis}
The contractor translated the Kiswahili transcripts into English, and submitted all transcripts to the researchers. The native lead researchers conducted an inductive thematic analysis of the data using NVivo qualitative data analysis software \cite{braun2006using}. This began with one researcher checking all transcripts to ensure de-identification and renaming them using pseudonyms: KE01-KE36 and TZ01-TZ36 for participants from Kenya and Tanzania respectively. Following this, the two researchers independently read a 20\% sample of the transcripts and jointly developed an initial codebook, which they used to independently code a second  20\% sample of the transcripts. They met to discuss and resolve any conflicts. One of the two researchers then coded the remaining transcripts, with the second researcher spot checking random samples of the work to ensure consistency; the two met to discuss whenever there were coding differences. Coding differences were largely around code definitions and the importance of particular data points, which we resolved by adding clear code descriptions with sample quotes, and revisiting the study questions respectively. When there was a codebook change, the researcher re-coded the full set of excerpts with the new scheme. We required three coding iterations to be sure that the codes had been applied consistently. The codes (Appendix \ref{Appendix_Codes}) were also discussed with the entire  research team as they emerged. We organized the resulting codes into final themes and sub-themes  (Appendix \ref{Appendix_ThemeEmergence} before selecting useful excerpts for reporting purposes.  

\subsection{Ethical considerations}
This study was approved at the national country levels, and at our institution's IRB. See Appendix \ref{appendix:approvals}. We collected participants' phone numbers for the purpose of compensation via mobile money. However, these were stored separately from the interview transcripts. All participants also provided informed consent to be interviewed and audio-recorded, and were compensated in amounts approved by the local ethics committee: US \$3.40 in Kenya, and US \$10 and \$5 in Tanzania for urban and rural participants respectively. These amounts were for both interviews together, and the difference in Tanzania was based on estimates for hourly wages in urban areas, and daily work output in rural areas in Tanzania. Recordings were stored in password-protected files that were shared between the researchers and the country leads who were responsible for overseeing all data collection activities for their respective countries. Transcripts were anonymized and de-identified before analysis. 

\subsection{Limitations}
The study has some methodological limitations. Given the qualitative nature of the study and the small sample size, our results may not be generalizable to entire populations of MoMo users. Some nuances may also have been lost in translation of the interviews between  languages, which was done by native research assistants working with the native researchers.
The cross-sectional design of the study also poses additional limitations. Regulators, governments and policy-makers are regularly introducing or changing aspects of this ecosystem and we expect that user  practices will also continuously adapt to this changing environment. 
While we did our best to minimize bias in the design of the study, our results may suffer from self-reporting and social desirability bias if participants felt the urge to report ``proper'' behaviour. 
Finally, in an effort to ensure consistency across several assistants and within the two countries, the decision to use structured interviews may have limited the richness of the data that we could collect.   