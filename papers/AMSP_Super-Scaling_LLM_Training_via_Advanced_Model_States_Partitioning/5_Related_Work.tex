

\section{Related Work}
\noindent\textbf{Model state sharding techniques.} We summarize several techniques that save GPU memory and support large-scale model training by partitioning the model states. 
DP \cite{DemystifyingParallelandDistributedDeepLearning} serves as the fundamental approach to model training parallelism and has been extensively adopted across various frameworks, such as PyTorch-DDP \cite{PyTorchDistributed}, Horovod \cite{Horovod}, and Tensorflow-DDP \cite{TensorFlow}. In DP, the complete training state is replicated among each rank with distinct input data. The communication overhead arises from the synchronization of gradients among DP ranks.
ZeRO \cite{ZeRO} and Fully Sharded Data Parallel(FSDP \cite{xu2020automatic}), split parameters, gradients, and optimizer state in the scope of DP. ZeRO family serves as a crucial memory optimization technique in LLM training. ZeRO demonstrates limited horizontal scalability as the scale of the DP scope increases, primarily due to frequent and inefficient inter-node communication. Inspired by these sharding techniques, recent work dynamically selects the model state to partition, balancing memory consumption with communication overhead, thus achieving scalability efficiency. For instance, MiCS\cite{MiCS} chooses to partition the training state within subgroups, trading partial redundancy in the training state for improved communication efficiency. ZeRO++\cite{ZeRO++} takes a different approach by redundantly storing an additional set of secondary parameters on each node, in exchange for enhanced communication efficiency through parameter pre-fetching. PyTorch FSDP\cite{PyTorchFSDP} controls the split range of the training state by setting a sharding factor. Setting the sharding factor to 1 results in a fully redundant setup, while aligning it with the DP world size achieves zero redundancy. The aforementioned efforts break the constraint that the split of the training state must be on a global scale, while not attempting to decompose the data dependency of the training state and adopt different partitioning strategies for different training states. 

\noindent\textbf{Model parallelism and 3D parallelism.} Model parallelism is represented by two approaches: tensor parallelism and pipeline parallelism. Tensor parallelism~\cite{Megatron-LM} involves partitioning specific layer weights and introducing additional AllReduce communication. Pipeline parallelism\cite{GPipe,DAPPLE,PipeDream,PipeMare} divides the layers of the model horizontally among each rank. Recent innovations have proposed methods that autonomously discern parallelism approaches by intricately melding both data and model parallelism for distinct operators within the model. To illustrate, solutions like Alpa \cite{Alpa}, OptCNN \cite{OptCNN}, FlexFlow \cite{FlexFlow,Unity}, and TensorOpt \cite{TensorOpt} incorporate both data and tensor parallelism. These leverage a variety of search algorithms to refine and enhance the execution of blueprints. However, while these automated parallelism solutions focus on optimizing the partitioning and placement strategies for the optimal operators within the computational graph, they overlook strategies related to the orthogonal placement of the model states.

\noindent\textbf{Large-scale communication optimization.} 
% With the increase in training scale, the improvement in training efficiency achieved through greater computational investment gradually diminishes. 
Some works\cite{ByteScheduler, P3, PyTorchFSDP} try to overlap communication with computation to mitigate communication costs. 
ZeRO++ and Espresso\cite{Hi-SpeedDNNTrainingwithEspresso} utilize quantization and compression techniques to reduce communication volume, albeit at the expense of precision. 
DEAR\cite{DeAR} aggregates multiple small communications using fixed-size buffers to reduce communication overheads. Hetu\cite{HetuMoE} leverages hierarchical all-to-all to minimize inter-node communication volume under poor inter-node communication. 
% Recognizing that the performance of collective communication primitives significantly degrades with increasing training scope, 
Similarly, Hybrid AllReduce\cite{HighlyScalable} attempts to decompose a single collective communication primitive into a combination of multiple subgroup communications, while targeting on large scales. 
