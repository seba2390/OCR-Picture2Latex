%-------------------------------------------------------------------------------
\section{Introduction}
\label{sec_intro}
%-------------------------------------------------------------------------------
\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{trend.pdf}
    \caption{The token size for cutting-edge language models has been growing at an exponential rate over time, while the model size is reducing. The legend is represented in the format $ModelName-ModelSize$. For example, LLaMA-6.7B means the LLaMA model with 6.7 billion parameters.}
    \label{trend}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{Compared.pdf}
    \caption{Comparison of training a LLaMA-6.7B model with ZeRO-1 using 2T data and micro-batch-size of 4 on 128 GPUs and 1024 GPUs. When training on 1024 GPUs instead of 512 GPUs, the overall throughput is doubled, but per GPU efficiency drops by 77\% due to the increased ratio of communication and computation and suboptimal memory usage.}
    \label{compared}
\end{figure}




% In order to be able to process these large amounts of data quickly, the number of computational resources used to train these models is constantly increasing

LLMs, when trained on extensive text corpora, have demonstrated their prowess in undertaking new tasks \cite{Newtask1,Newtask2,Newtask3} either through textual directives or a handful of examples \cite{GPT1}. 
These few-shot properties first appeared when scaling models to a sufficient size \cite{kapscaling}, followed by a line of works that focus on further scaling these models \cite{GPT1,GPT2,GPT3,OPT,GLM-130B}. These efforts are grounded in the scaling law posted by OpenAI \cite{kapscaling,openaiscalinglaw}. However, DeepMind \cite{trainingcomputeoptimal} presented a contrarian perspective that smaller models supplemented with vast datasets can potentially yield enhanced results. Meta, while considering the inference budgets, trained the LLaMA model \cite{LLaMA}, which has a tenth of the model size of DeepMind's model but uses 1 TB tokens. The performance brought by this methodology outperformed those from larger models. Since the introduction of Chinchilla~\cite{trainingcomputeoptimal} by DeepMind, the token sizes for cutting-edge language models have been growing at an exponential rate. However the parameter sizes of the model have remained around 7B, as depicted by the likes of LLaMA2 \cite{Llama2}, Alpaca \cite{Alpaca}, and InternLM \cite{InternLM} in Figure \ref{trend}. 

Consequently, there is a preference to process this exponentially growing number of tokens using smaller models on larger training scales. For instance, as illustrated in Figure \ref{compared}, by training LLaMA-6.7B using micro-batch-size of 4 on a scale of 1024 GPUs \cite{LLaMA,Llama2} expanded from 128 GPUs \cite{Megatron-LM,ZeRO,PyTorchFSDP}, we can halve the time \cite{LLaMA,trainingcomputeoptimal} required to process data. 
% However, such super-scale training scenarios present distinct challenges and the decreasing computational efficiency and rising communication latency in Figure \ref{compared} demonstrate these two challenges: 
However, the decreasing computational efficiency and rising communication latency in Figure \ref{compared} are attributed to the following challenges:
% {\bfseries (1)} \textbf{the batch size per GPU is limited} by the maximum global batch size that can be employed without compromising convergence efficiency. 
{\bfseries (1)} \textbf{Per-GPU batch size is limited.} Due to the requirements of convergence and reproducibility, the global batch size remains constant. This limitation compromises the computational efficiency achieved on GPUs, especially at large scales.
{\bfseries (2)} \textbf{The latency of communication grows super-linearly} as the number of GPUs increases. 

There are two popular solutions to alleviate these challenges, namely 3D parallelism \cite{Megatron-LM,Megatron-LM1,Alpa} and ZeRO \cite{ZeRO,ZeRO++,ZeRO-Infinity}. However, they have limited scalability and cannot handle such super-scale scenarios.
Therefore, based on \textbf{(1)} and \textbf{(2)} we can trade GPU memory for communication. More specifically, instead of spreading model states across all the GPUs, we maintain copies of them. Besides, within the model state, there are three distinct components: parameters ($P$), gradient ($G$), and optimizer states ($OS$). Each of them exhibits varying degrees of communication overhead and memory cost. This heterogeneity offers the potential to flexibly select the redundancy level for each component individually. 


% Therefore, there is a tendency to deal with this exponential growth of tokens with smaller models on larger training scales. For example, as shown in Figure \ref{compared} we can reduce the time to process the data by a factor of two at a larger training size of 1024 GPUs versus 128 GPUs.
% , and proposed the 70B-Chinchilla training with 1.4 billion tokens. 
% Such a trend is illustrated by the left red arrow in Figure \ref{trend}. 
% These efforts are grounded in an underlying premise posited by OpenAI \cite{kapscaling,openaiscalinglaw}: 
% a power-law relationship intertwining the sizes of model parameters, dataset, and computational resources employed during the training of LLMs.

% However, DeepMind \cite{trainingcomputeoptimal} presented a contrarian perspective that smaller models supplemented with vast datasets can potentially yield enhanced results, and proposed the 70B-Chinchilla training with 1.4 billion tokens. 
% From Figure \ref{trend}, it is evident that compared to the contemporaneous GLM-130B, Chinchilla utilized half parameters but employed $3\times$ data. 

% Training these models that follow the latest trend has the following characteristics: (1) Model size range from 7B to 30B, which is approximately 10 times less than the models at historical scaling area. (2) Token size ranges from 1T to 2T, which is nearly 4 times larger than the previous models. (3) To swiftly process these vast amounts of data, the computational resources dedicated to training these models have surged dramatically.  For example, Figure \ref{compared} compares the training of a LLaMA-7B model allocating 1024 GPUs versus 128 GPUs, respectively. When we adopt the most efficient deep learning training system, 1024 GPUs gain 2 $\times$ time reduction compared to 128 GPUs shown in Figure \ref{compared}. However, per GPU efficiency (TFlops) plummets by 77\%, because higher ratio of communication and computation and inadequate memory usage.

% Therefore, recent DL training systems experience great challenges of scalability.
% (1) communication latency surges by 166\% due to collective limited scalability, (2) efficient computing time shrinks by 62\%, and (3) memory allocation per rank drops by 30\%. (2) and (3) both caused by the batch size per GPU being limited by the maximum global batch size that can be used during the training without sacrificing convergence efficiency. As a result, the throughput of the whole training cluster does not grow linearly with the number of GPUs. 






% 3D parallelism combines data parallelism \cite{ParallelizedSGD,PyTorchDistributed}, pipeline parallelism \cite{PipeDream,DAPPLE,GPipe}, and tensor parallelism \cite{Megatron-LM,Megatron-LM1} to distribute model training workloads across hundreds of GPUs. 



% This approach can achieve excellent per-GPU computing and memory efficiency at the expense of extra activation communication. In contrast, ZeRO could reduce the communication latency by overlapping communication with computation but underperforms by a high ratio of communication and computation. ZeRO is a memory-efficient variation of data parallelism, where model-states are partitioned across all the GPUs (Fig. \ref{GeneralPartition} blue block) and reconstructed using gather-based communication collectives on-the-fly during training. However, these collective communication overhead of ZeRO grows larger as the size of cluster scale up \S\ref{Comm_scaless} and are accompanied by negligible explicit memory overheads since batch size per GPU is limited by the maximum global batch size that can be used during the training without sacrificing convergence efficiency\cite{DemystifyingParallelandDistributedDeepLearning,LargeDP}.






Incorporating the above observations, we design \SysName to promote a more granular and adaptive strategy, allowing for component-specific optimization based on their individual communication and memory constraints. \SysName consists of three key modules, as shown in Figure \ref{SystemOverview}. Specifically, (1) it constructs a unified partition space for $P$, $G$, and $OS$ (grey block in Fig \ref{GeneralPartition}) based on the number of computing resources used by the user and the provided model size. This allows for finer-grained partitioning of $P$, $G$, and $OS$. (2) It introduces a \emph{Scale-Aware partitioner}, which consists of two key components: data dependency rules to avoid high data transfer costs, and a cost model focusing on communication and GPU memory for combination strategies. An established Integer Linear Programming (ILP) algorithm is then used to determine the best partitioning approach. (3) Communication Optimizer constructs a tree-based stratified communication strategy tailored for cross-node exchanges within model states. Furthermore, for specialized scenarios involving data exchanges between $P$ and $OS$, we incorporate a prefetch algorithm to ensure correct data sequencing.



% (1) we build a \emph{partition space} (Fig \ref{GeneralPartition}grey block) for components (P, G, and OS) respectively based on the number of compute resources used and model size provided by users. It realizes a finer-grained partition of the P, G, OS. (2) \emph{Scale-Aware partitioner}
%  (3) Communication Optimizer constructs a tree-based stratified communication strategy tailored for cross-node exchanges within model states. Furthermore, for specialized scenarios involving data exchanges between P and OS, we incorporate a prefetch algorithm to ensure correct data sequencing.


% In the extended partitioner space, we introduce an \emph{scale-aware partitioner}. This partitioner c

% % Unlike previous techniques where these components were treated collectively as a unified entity for optimization, our approach promotes a more granular and adaptive strategy, allowing for component-specific optimization based on their individual communication and memory constraints.
% minimize the collective communication overhead brought by the newest model training characteristic, making it near-linear scaling. 



% However, the batch size per GPU is limited by the maximum global batch size that can be used during the training without sacrificing convergence efficiency, which decreases the computation efficiency. Besides, communication overhead increases by the GPU number due to the limited scalability of collectives shown in Figure \ref{Comm_scaless}.


% The primary cause of this challenge stems from the rigidity in partitioning the Model-State in existing DL frameworks. These techniques process the tripartite components (Parameters-P, Graient-G, Optimizer States-OS) within the model state as an integrated unit although they have different memory and communication costs. Even though ZeRO is divided into three different stages to reduce memory usage, it operates in a holistic manner. For example, training a 7B model in 1024 GPUs, 


% and partitions the model state at the DP-world-size level.

% % the efficiency of ZeRO can be limited by high ratio of communication and computation







% % However, memory allocated at each rank decreases by 30\%, efficient computing time decreases by 62\%, communication latency increases by 166\% and the final accelerator efficiency (Tflops) decreases by 77\%. More generally, each GPU in the training cluster gains lower memory allocation but pays a higher ratio between communication and computation, which leads to a drastic decrease in the performance of GPUs. 

% % The primary cause of this challenge stems from the limited scalability of communication operators, as shown in Figure \ref{Comm_scaless}, when training at 512 GPUs, the both effective bandwidth of AllGather and Reduce-Scatter algorithm will decrease a lot, AllReduce shows a decreasing trend although it has been optimized from several studies comparing to training at 64 GPUs within the different number of parameters.

% The primary cause of this challenge stems from the rigidity in partitioning the Model-State in existing DL frameworks. These techniques like ZeRO \cite{ZeRO,ZeRO-Infinity,ZeRO++,PyTorchFSDP}, Megatron-LM 3D \cite{Megatron-LM,Megatron-LM1}, Alpa \cite{Alpa} process the tripartite components (Parameters-P, Graient-G, Optimizer States-OS) within the model state as an integrated unit.
% Even though ZeRO is divided into three different stages to reduce memory usage, it operates in a holistic manner and partitions the model state at the DP-world-size level. This means a DP-world-size level collective communication will be executed several times, which dramatically increases the communication overhead due to the limited scalability of the communication operator itself. As shown in Figure \ref{Comm_scaless}, when training with 512 GPUs, the effective bandwidth of both AllGather and Reduce-Scatter algorithms will decrease a lot. AllReduce shows a decreasing trend although it has been optimized from several studies compared to training at 64 GPUs within different numbers of parameters. Inspired by MiCS's solution \cite{MiCS} to address the issue of low communication bandwidth in cloud scenarios by reducing the number of communication participants, we observe that scaling down the communication can alleviate the challenges posed by the limited scalability of the communication operator. However, MiCS also treats the model state as a unit and then partitions it at a smaller scale like 1 node, or 2 nodes. etc. In this way, the size of the communication group increases as the model size grows to avoid the OOM error. As the subgroup size extends to DP-world-size, the benefit of MiCS is diminished. It reveals that such coarse-grained partitioning, i.e., treating the model state as an undivided entity, inadvertently bypasses potential optimization avenues. 
% Model-parallel-based techniques like Megatron-TP \cite{Megatron-LM,Megatron-LM1,PipeDream} and Alpa \cite{Alpa}, on the other hand, slices the parameters tensor of modules such as attention and MLP, allocating the G and OS of the model state to devices based on P tensor slicing. Nevertheless, we discern that such coarse-grained partitioning, treating the model state as an undivided entity, inadvertently bypasses potential optimization avenues. 


% Consequently, we can construct a smaller communication subgroup for each of the three elements, P, G, and OS, allowing flexibility in addressing memory and communication challenges across various scenarios. As illustrated in Figure \ref{GeneralPartition}, the previous DL framework's partitioning approach can be visualized as a grey block at the bottom, labeled "predefined group", encompassed by n(4) grey dashed lines. Given varying training inputs, users might opt for an optimal strategy from the top grey block. Nonetheless, by disaggregating the three components of the model state and permitting independent partitioning strategies, we can expand our search space to \(n^3(4^3)\) green lines, a substantial increase from the initial n(4) grey lines. This not only offers users a broader array of partition combinations at the top, but \SysName also automatically determines the most communication-efficient combination on their behalf. While the instances P, G, and OS are interdependent in terms of data, their computations remain autonomous \cite{ZeRO}. Leveraging this computational independence and satisfaction of data dependencies, we can configure distinct sharding scopes for each, which offers an opportunity to minimize communication overhead \cite{OverlapTP, EINNET, Alpa} in linear-scaling training scenarios. 


% The growth rate is proportional to the increase in data volume, typically involving the use of more than 256 GPUs. \cite{Megatron-LM,Megatron-LM1,ZeRO,PyTorchFSDP}.

% for three reasons: 

% This trend showcases three novel characteristics:
% Building upon this perspective, LLama, while taking inference budgets into consideration, trained a 7-billion parameter model using 1 terabyte of data. The results from this approach significantly outstripped those from much larger models. Consequently, the data size for cutting-edge Natural Language Processing (NLP) models has been escalating at an exponential pace, as depicted in Figure \ref{trend}. This trend have three new characteristics :


% \emph{(1) More Resource to train Smaller Model with Bigger DataSet}. To expediently process these vast quantities of data (D), there's been a steady surge in the computational resources (C) allocated for training these models. However, the model size (N) does not increase proportionally.\emph{(2) Memory not the Bottleneck}. With ample computational resources at one's disposal, memory storage is no longer the primary bottleneck in the training process due to constraints in global batch size \cite{DemystifyingParallelandDistributedDeepLearning, OnLarge-BatchTrainingforDeepLearning,LargeBatchOptimizationforDeepLearning}.\emph{(3) High Ratio between Comm $\&$ Comp}. Instead, the substantial overhead of collective communication within thousands of GPUs leads to a high ratio between communication and computation. 


% However, this poses scalability challenges for contemporary deep learning (DL) systems \cite{Megatron-LM,Megatron-LM1,ZeRO,PyTorchFSDP}for three reasons:


% \begin{itemize}[leftmargin=*,topsep=1pt, itemsep=2pt, itemindent=8pt]

%         \item {\bfseries P1}: \textsl{\bfseries Rigidity in Model-State Partitioning}.
%         Data-parallel-based techniques like Zero\cite{ZeRO,ZeRO-Infinity,ZeRO++} and MiCS \cite{MiCS} often process the tripartite components (Parameters-P, Graient-G, Optimizer States-OS) within the model state as an integrated unit. Even as Zero integrates stages to trim memory usage, it operates in a comprehensive manner. Model-parallel-based techniques like Megatron-TP \cite{Megatron-LM,Megatron-LM1,PipeDream,} and Alpa \cite{Alpa}, on the other hand, slices the parameters tensor of modules such as attention and MLP, allocating the G and OS of the model state to devices based on P tensor slicing. Nevertheless, we discern that such coarse-grained partitioning, treating the model state as an undivided entity, inadvertently bypasses potential optimization avenues.
        
%       \item {\bfseries P2}: \textsl{\bfseries Limited scalability of Communication Operators}.
%         Nvidia\cite{NvidiaBlog1} once elucidated the scalability challenges of the all-reduce\cite{Allreduce} algorithm, proposing tree-based or binary tree solutions\cite{TreeAllreduce1, TreeAllreduce2, TreeAllreduce3, Flattenedbutterfly} as potential remedies. However, these didn't offer flawless resolutions \cite{NvidiaBlog1}. Within the training ecosystem, there are other operators, like all-gather\cite{AllGather}, reduce\cite{Reduce}, and reduce-scatter\cite{ReduceScatter}, that grapple with scalability issues. Figure \ref{Comm_scaless} provides a visual representation of this challenge. Yet, the scalability problems of these operators remain largely unaddressed by the contemporary DL system, which will lead to high communication to exacerbate the ratio.

%       \item {\bfseries P3}: \textsl{\bfseries Low Compute Efficiency}.
%         When harnessing thousands of GPUs, the GPU's per-GPU batch size is confined by the maximum global batch size, ensuring no compromise in convergence efficiency. This implies that while global batch sizes cannot be perpetually escalated without decelerating model convergence\cite{DemystifyingParallelandDistributedDeepLearning,OnLarge-BatchTrainingforDeepLearning,LargeBatchOptimizationforDeepLearning}, utilizing thousands of GPUs invariably necessitates diminutive per-GPU batch sizes, memory is not a bottleneck for training at this point.
        
      
        
% \end{itemize}


% Essentially, while the approach of utilizing small models and large amounts of data has clear advantages algorithmically and in terms of inference costs, it makes LLM training less scalable.

% To bridge these gaps, we design \SysName, a near-linear scaling LLM training framework that outperforms previous DL systems tailored for earlier scaling laws. The core design of \SysName derives from the following three insights. First, \emph{the challenge posed by the limited scalability of the communication operator can be effectively tackled by narrowing its operational domain}. By targeting the operator's activity solely to periods when its communication thrives in efficiency, the hurdle of diminished effective bandwidth can be adeptly sidestepped. As shown in Figure\ref{Comm_scaless}, high bandwidth can be guaranteed by scaling the amount of each transmission, however, this can be detrimental to the overlap between communication and computation\cite{PyTorchFSDP}, or by reducing the number of participants in a pooled communication, utilizing memory in exchange for communication, in scenarios where memory is not a bottleneck. Second, \emph{It pays to build a larger partition space by partitioning the three members of the model state at a finer granularity}. While the instances P, G, and OS are interdependent in terms of data, their computations remain autonomous\cite{ZeRO}. Leveraging this computational independence and ensuring data dependencies are met, we can configure distinct sharding scopes for each. This approach offers an opportunity to minimize communication overhead \cite{OverlapTP,EINNET} in linear-scaling training scenarios. Third, \emph{layered communication can further reduce the overhead of across-nodes communication}. In the realm of communication operator optimization, numerous studies \cite{TreeAllreduce1,TreeAllreduce2,TreeAllreduce3}have suggested that hierarchical operations excel in scaling communication. Drawing inspiration from this, we can adapt such a strategy for scaling training. This involves conceptualizing GPUs distributed across multiple nodes as a two-dimensional grid. Within this framework, we first aggregate data across nodes in parallel, followed by consolidating local data on each individual node.



Extensive evaluations show a significant system throughput and scaling efficiency improvement of \SysName on training LLaMA-based models. On A100 (80GB) GPU clusters with 800Gbps network, the throughput of \SysName is 4 $\times$ larger than that of DeepSpeed, which is the state-of-the-art DP framework for large model training. Compared to Megatron-LM-3D, a state-of-the-art system specialized for training Transformer models, \SysName achieves up to 36.9\% larger throughput. \SysName gets near-linear (90.3\%) strong scaling efficiency in 1024 GPU training, which is up to 55\% better than DeepSpeed. 

In summary, we make the following contributions:
% In addition, \SysName successfully copes with the aforementioned deploymenand achieves the following desirable properties:




\begin{itemize}[leftmargin=*]
      \item 
      We build a unified partitioning space for model states, enabling independent and fine-grained partitioning strategies for $P$,$G$, and $OS$ of the model states. 
        
        % the data-dependent and computation-independent properties of the model state to construct a larger partition space than previous DL systems \cite{ZeRO,ZeRO-Infinity,ZeRO++,Megatron-LM,PyTorchFSDP}.


      % \item {\bfseries A2}: \textsl{\bfseries Optimial partition strategy}.
      %    \SysName discerns optimal partitioning strategies tailored to diverse scenarios within the adaptive partitioning framework. These strategies are distinguished by the employment and formulation of multiple communication subgroups that can be flexibly combined, setting them apart from the strategies of prior DL systems. By implementing these innovative approaches, we achieved a superior throughput compared to previous systems.
      
      \item 
      We design a scale-aware partitioner in \SysName to automatically derive the optimal partition strategy. 
      
      % for $P$, $G$, and $OS$ at each partition strategy and also integrates a dedicated communication optimizer to manage the data placement variances resulting from the differential partitioning of $P$, $G$, and $OS$.

      \item 
      We evaluate \SysName on training large models with 1024 GPUs and get near-linear (90.3\%) strong scaling efficiency.
\end{itemize}

Based on our current understanding and research, \SysName aims to address some of the challenges observed in distributed training frameworks given the prevalent training trends. We've explored the possibility of fine-grained partitioning of model states' members as a potential solution.

% Based on our current understanding and research, \SysName is the first framework to identify the limitations of distributed training frameworks under the existing training trend and contemplate fine-grained partitioning of model states' members as a solution.

 % We have systematically outlined the shortcomings of current methodologies ({\bfseries P1}$\sim${\bfseries P3}) and proposed a comprehensive end-to-end solution to address these limitations.