\begin{abstract}
Large Language Models (LLMs) have demonstrated impressive performance across various downstream tasks. When training these models, there is a growing inclination to process more tokens on larger training scales but with relatively smaller model sizes. Zero Redundancy Optimizer (ZeRO), although effective in conventional training environments, grapples with scaling challenges when confronted with this emerging paradigm. To this end, we propose a novel LLM training framework \SysName, which undertakes a granular partitioning of model states, encompassing parameters ($P$), gradient ($G$), and optimizer states ($OS$). Specifically, \SysName (1) builds a unified partitioning space, enabling independent partitioning strategies for $P$, $G$, and $OS$; (2) incorporates a scale-aware partitioner to autonomously search for optimal partitioning strategies: (3) designs a dedicated communication optimizer to ensure proficient management of data placement discrepancies arising from diverse partitioning strategies. Our evaluations show that \SysName achieves up to 90.3\% scaling efficiency across 1024 GPUs. 

% that surpasses ZeRO by 55\%.

% The computational resources allocated for training 7B-30B models have seen a significant increase because the trend in large language model training is increasingly emphasizing training vast amounts of data on smaller models. Zero Redundancy Optimizer (ZeRO) has been used to train a wide range of large language models on massive GPUs clusters due to its efficiency and scalability.  However, as the magnitude of training scales up, ZeRO encounters challenges. Increased collective communication overheads, along with constraints that necessitate a reduced batch size per GPU, have limited ZeRO's scalability. This limitation arises from ZeRO's rigid approach to partitioning the model state, thereby underutilizing the inherent heterogeneity of model states in terms of parameters-$P$, gradients-$G$, and optimizer states -$OS$ with respect to memory and communication. In response, \SysName introduces a unified partitioning space for model states, enabling distinct partitioning strategies for $P$, $G$, and $OS$ components. Furthermore, \SysName incorporates a scale-aware partitioner designed to determine the most efficient partition combinations for $P$, $G$, and $OS$ under each partitioning strategy. Additionally, \SysName integrates a dedicated communication optimizer to manage the data placement variances resulting from the differential partitioning of $P$, $G$, and $OS$. Our evaluation shows that \SysName achieves near-linear (e.g., 90.3\%) strong scaling efficiency in 1024 A100(80GB) GPU clusters with an 800Gbps network, which is up to 55\% better than ZeRO.

% achieves near-linear weak scaling efficiency, specifically 90.3\% in training with 1024 GPUs which is up to 55\% better than DeepSpeed when evaluated o

% 用于训练 7B-30B 模型的计算资源显著增加，因为大型语言模型训练的趋势越来越强调在较小模型上训练大量数据。 然而，随着训练规模扩大，大量的集合通信开销和at scale which forces batch size per GPU to be small，ZeRO's effective scalability is limited because of 他刚性地切分model state 从而失去利用model states 内 parameters，Gradient，Optimizer stats在显存和通信的异质性。Based on it , lins 为 model states 构建了一个统一的切分空间 to specify unique partitioning strategies for P，G，OS。Lins designs a scale-aware partitioner to automatically derive efficient partition combination for P,G,os at each partition strategy.Lins also impelments a specific communication optimizer to orchestrate the data placement difference brought by discrepancy partitioning of P,G,OS. Our evaluation shows that LinS achieve near-linear (e.g., 90.3%) weak scaling efficiency in 1024 GPU training, which is up to 55% better than DeepSpeed on A100(80GB) GPU clusters with
% 800Gbps network

% How to train a large model effectively using unlimited hardware.
    % 现有的框架在对中等size的模型的训练不能很好的扩展到大集群环境中，因为更高的通信计算比，导致更大的通信开销。

    % In this paper，我们提出了7B，
    % (1) it produces larger but fewer kernels, converting a large number of off-core data movements into on-core data exchanges;
    % %Website
    % While recent deep learning workload schedulers exhibit excellent performance, it is arduous to deploy them in practice due to some substantial defects, including inflexible intrusive manner, exorbitant integration and maintenance cost, limited scalability, as well as opaque decision processes. Motivated by these issues, we design and implement Lucid, a non-intrusive deep learning workload scheduler based on interpretable models. It consists of three innovative modules. First, a two-dimensional optimized profiler is introduced for efficient job metric collection and timely debugging job feedback. Second, Lucid utilizes an indolent packing strategy to circumvent interference. Third, Lucid orchestrates resources based on estimated job priority values and sharing scores to achieve efficient scheduling. Additionally, Lucid promotes model performance maintenance and system transparent adjustment via a well-designed system optimizer. Our evaluation shows that Lucid reduces the average job completion time by up to 1.3x compared with state-of-the-art preemptive scheduler Tiresias. Furthermore, it provides explicit system interpretations and excellent scalability for practical deployment.

\end{abstract}
