\vspace{-10pt}
\section{Conclusion}

Large Language Models (LLMs) are increasingly being trained with more tokens but smaller model sizes. The traditional Zero Redundancy Optimizer (ZeRO) struggles to adapt to this new trend. To address this, we introduced \SysName, a novel training framework. This framework efficiently partitions model states and optimally manages data placement, achieving a 90\% scaling efficiency on 1024 GPUs.