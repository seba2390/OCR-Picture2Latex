%----------------------------------------------------
\section{Background}
\label{sec_motivation}
%-----------------------------------------------------



\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{GeneralPartition.pdf}
    \caption{Comparing the search spaces of \SysName and prior work. By decomposing the three components of the model state and independently selecting their partitioning strategies and introducing a different level of redundancy strategy named $Intra{\-}Partition$\&$Inter{\-}Replica$, we can tap into a more expansive search space than prior work (ZeRO).}
    \label{GeneralPartition}
\end{figure}

% However, it primarily aligns with the data parallelism computation approach, albeit with added communications for sharing the model states. \\



% Deep learning model training process mainly consists of three phases, i.e., forward computation, backward computation, and pa- rameter updating. In order to train the model faster, we can harness the computing power of multiple machines. A gradient synchro- nization step is performed before updating the model parameters to ensure all workers will use the same set of parameters to evaluate the incoming new training samples.
% Deep learning model training is memory consuming as it needs to hold the model states including model parameters, gradients from backward computation, and optimizer states for parameter updating. Because of the limited on-device memory resource, activa- tion checkpointing and gradient accumulation are typically enabled. Activation checkpointing discards the activation outputs from the forward phase and requires activation recomputation in the back- ward phase. Gradient accumulation divides one large data batch into multiple small micro-batches to reduce the memory footprint of storing activation outputs. However, for models with billions of parameters, these two techniques alone are not sufficient. Many solutions targeting at gigantic model training are thus proposed.


% \noindent\textbf{}.
% In the realm of data-driven processes, companies prefer to leverage the full potential of their GPU resources to expedite the training of models with specific datasets, thereby reducing time consumption.\\

% Model parallel work well within a single node,where the inter-GPU communication bandwidth is high,but the efficiency degrads quickly beyond a single node\ref{Megatron-LM}
% Data parallel has good compute/communication efficiency but poor memory efficiency.
% Data - and model - parallel maintains all the model states required over the entire training process statically.

% ZeRO eliminates the above memory redundancies by partitioning the model states into multiple disjoint sets and only keeping one set in memory at a time.
% while draw in more communication overhead than basic data parallel when scale model to bigger workload.(low communication volume).
% Zero-Redundancy parallelism [26, 27, 29] shards parameters as well but communicates parameters on-demand to recover the unsharded form and executes the model as if it were replicated on every device. 

% \noindent\textbf{}.
% To ensure that GPU devices remain fully utilized during distributed training, it is essential to minimize downtime caused by non-computational operations.


%\subsection{Backgroud}
%\label{Data-Centric_LLM_training}




\subsection{LLM Training}
% \noindent\textbf{Data-Centric LLM training}
There is a power law relationship between the parameter number of an autoregressive language model and its performance \cite{kapscaling,openaiscalinglaw}. This results in the trend of training larger and larger models \cite{GPT3,NLG,LaMDA,OPT,GLM-130B}, for higher performance. However, \cite{trainingcomputeoptimal,LLaMA} demonstrated that the best performance is not always achieved with the largest models, but can be with smaller models trained on more data. For instance, Chinchilla \cite{trainingcomputeoptimal}, while operating within the same computational budget as Gopher (280B) \cite{ScalingLanguageModels}, employs only 70B parameters but leverages 4 $\times$ more data, surpassing Gopher's performance. Furthermore, LLaMA-6.7B \cite{LLaMA}, maintaining the same computational budget as Chinchilla, trains a 6.7B model on 1T tokens, diverges from the suggested 200B tokens for a 10B model, and achieves a performance improvement over Chinchilla. This shift underscores an emerging trend where researchers are increasingly emphasizing extensive data training on smaller models, 
% while still utilizing the computational resources traditionally allocated to heftier models.
with similar GPU resource demands.


\subsection{3D parallelism}
\label{DistributedDLsystem}
% The process of training a deep learning model predominantly involves three stages: forward propagation, backward propagation, and parameter update \cite{Galvatron}. 
% % When the model or data becomes so extensive that a singular device cannot finish the training within an acceptable timeframe, 
% % we turn to machine learning parallelization strategies, which distribute the computation across multiple devices. 
% When the model or data grows and exceeds the capacity of a single device, deep learning training parallelism becomes essential. 
Training large models across multi-GPU clusters necessitates the use of advanced parallelism techniques, and Data Parallelism (DP) \cite{ParallelizedSGD,PyTorchDistributed}, Pipeline Parallelism (PP) \cite{PipeDream,DAPPLE,GPipe}, and Tensor Parallelism (TP) \cite{Megatron-LM,Megatron-LM1}are the three predominant strategies. Data Parallelism, or DP, is best suited for scenarios where the model's size can comfortably fit within a single GPU's memory. In this approach, each GPU retains a complete set of the model weights and processes distinct batches of input data concurrently, essentially duplicating the model across GPUs while splitting the data \cite{LargeDP, ParallelizedSGD, HeterogeneityDP,CommunicationDP}. On the other hand, when a model's size exceeds an individual GPU's memory capacity, Model Parallelism (MP) comes into play. Rather than dividing the data as in DP, MP divides the model itself, assigning segments of the model to different GPUs. Within MP, there are two primary techniques. The first, Pipeline Parallelism, divides the model into sequential stages. Each stage comprises a continuous sequence of layers, and though each stage relies sequentially on the previous one for a specific micro-batch, multiple micro-batches can be processed simultaneously at different stages, but it introduces empty bubbles that cause inefficiencies. The second technique, Tensor Parallelism, distributes individual layers of the model across several GPUs. For a single input, every GPU manages a unique section of the layer, allowing layer computations to happen concurrently, but this creates additional communication overhead that cannot be overlapped.

\subsection{ZeRO}
\label{ZeRO}
ZeRO is a memory optimization technique specifically tailored for data parallel training. It functions by partitioning and distributing model states, including $P$, $G$, and $OS$, across the GPUs being utilized. Model states are aggregated only when required for computation at a specific layer. The optimization provided by ZeRO is categorized into three stages as shown in Figure \ref{GeneralPartition}). In ZeRO-1, only the $OS$ is partitioned and distributed across the GPUs. In ZeRO-2, both the $OS$ and $G$ undergo partitioning and distribution. ZeRO-3 advances this by partitioning all components of model states. ZeRO-3 offers the highest memory efficiency for large-scale model training, but this comes with the requirement of increased collective communications overhead. 

Besides, when running on thousands of GPUs, the batch size per GPU is limited by the maximum global batch size that can be used during the training without sacrificing convergence efficiency, which will lead to lower computation efficiency. Therefore, ZeRO has a high ratio of communication and computation when training on thousands of GPUs.

Aiming to curtail the costly inter-node communication inherent in collective exchanges, recent refinements to ZeRO-3, exemplified by methods like MiCS \cite{MiCS}, leverage on-device memory to facilitate more efficient communication. Under MiCS, the GPU cluster is segmented into distinct sub-groups. Within these sub-groups, model states are divided, yet consistently duplicated among the different sub-groups. 

\subsection{Mix-Precision Training}
\label{mixprecision}

Mixed precision is a renowned Large Model Training (LLM) technique that can reduce memory consumption and enhance training efficiency. Within this method, the forward and backward passes for $P$ are conducted in the FP16 format, resulting in FP16 $G$. However, the $OS$ and master weights are preserved in the FP32 format. Given this arrangement, there is a noticeable disparity in the memory footprint of $P$, $G$, and $OS$ within the model state. When leveraging optimizers of the Adam \cite{Adam}, they maintain a copy of the master weights as well as corresponding momentum and bias. This translates to a storage requirement of three times the FP32 data in the form of $OS$. As a result, the proportional memory footprint of $P$, $G$, and $OS$ stands at a ratio of 2:2:12, respectively \cite{mixprecision}.



% Distributed DL systems adopt 3D parallelism and Zero Redundancy Optimizer (ZeRO) to improve training efficiency by utilizing multiple GPU devices.
% 3D parallelism combines data parallelism \cite{ParallelizedSGD,PyTorchDistributed}, pipeline parallelism \cite{PipeDream,DAPPLE,GPipe}, and tensor parallelism \cite{Megatron-LM,Megatron-LM1} to distribute model training workloads across hundreds of GPUs. Data parallelism (DP) techniques have gained popularity as a means to amplify distributed training for extensive input datasets. This strategy involves distributing data samples among several workers \cite{LargeDP, ParallelizedSGD, HeterogeneityDP,CommunicationDP} to process and coordinate model updates. 

% ZeRO distributes model states across all devices in a cluster to minimize each device's memory usage. It has three distinct stages, each offering varying degrees of memory savings. Nevertheless, this approach frequently leads to amplified communication costs stemming from collective communications. Besides, when running on thousands of GPUs, the batch size per GPU is limited by the maximum global batch size that can be used during the training without sacrificing convergence efficiency \cite{DemystifyingParallelandDistributedDeepLearning, LargeBatchOptimizationforDeepLearning, OnLarge-BatchTrainingforDeepLearning}. In other words, as global batch size cannot be increased infinitely without slowing down the model convergence, training on thousands of GPUs forces the batch size per GPU to be very small, which reduces the compute-to-communication ratio and thus creates a communication bottleneck. 


% Data parallelism (DP) techniques have gained popularity as a means to amplify distributed training for extensive input datasets. This strategy involves distributing data samples among several workers \cite{LargeDP, ParallelizedSGD, HeterogeneityDP,CommunicationDP} to process and coordinate model updates, such as gradients. Each worker typically holds a copy of the model, to fit within its memory, which can lead to superfluous memory usage. To mitigate this inefficiency, DeepSpeed introduced ZeRO \cite{ZeRO} (also referred to as FSDP in FairScale \cite{PyTorchFSDP}). ZeRO distributes model states across all devices in a cluster to minimize each device's memory usage. It has three distinct stages, each offering varying degrees of memory savings. Nevertheless, this approach frequently leads to amplified communication costs stemming from collective communications. Besides, when running on thousands of GPUs, the batch size per GPU is limited by the maximum global batch size that can be used during the training without sacrificing convergence efficiency \cite{DemystifyingParallelandDistributedDeepLearning, LargeBatchOptimizationforDeepLearning, OnLarge-BatchTrainingforDeepLearning}. In other words, as global batch size cannot be increased infinitely without slowing down the model convergence, training on thousands of GPUs forces the batch size per GPU to be very small, which reduces the compute-to-communication ratio and thus creates a communication bottleneck. 

% Model parallelism (MP) divides the model into multiple parts and each worker is only responsible for the computation of the partial model. There are mainly two kinds of paradigms commonly used for large-scale Transformers training, including distributed tensor parallelism (TP) \cite{Megatron-LM, MPDLRM,Tesseract} and layer-wise pipeline parallelism (PP) \cite{DAPPLE,GPipe,PipeDream,Chimera,PipeMare}. Tensor parallelism segments a tensor along a specific dimension, with individual devices storing distinct segments of the entire tensor. This strategy preserves the computational graph's integrity. The method allocates the operator's parameters over various devices, where each device calculates a local outcome based on its assigned data fragment. Once this operator-based computation concludes, collective communication mechanisms, such as all-gather \cite{allgather1} or all-reduce, are implemented to aggregate the final result. Although tensor parallelism brings certain advantages, it also comes with increased communication overheads. This amplified overhead predominantly arises from the need for synchronization following each segmented tensor operation \cite{Hanayo}. Pipeline parallelism divides the model based on its layers and concurrently segments the mini-batch into smaller units known as micro-batches. Within this framework, each worker operates as a distinct pipeline stage, executing either the forward or backward propagation computation specific to a given micro-batch. 

% Nevertheless, the direct implementation of these techniques to scale Transformers encounters significant obstacles in terms of system efficiency and user convenience. Recent innovations have proposed methods that autonomously discern parallelism approaches by intricately melding both data and model parallelism for distinct operators within the model. To illustrate, solutions like Alpa \cite{Alpa}, OptCNN \cite{OptCNN}, FlexFlow \cite{FlexFlow,Unity}, and TensorOpt \cite{TensorOpt} incorporate both data and tensor parallelism. These leverage a variety of search algorithms to refine and enhance the execution of blueprints. However, while these automated parallelism solutions focus on optimizing the partitioning and placement strategies for the optimal operators within the computational graph, they overlook strategies related to the orthogonal placement of the model states.

% uch as FlexFlow, OptCNN\cite{OptCNN}, TensorOpt\cite{TensorOpt} and Alpa\cite{Alpa}
% Furthermore, numerous MP frameworks are tailored specifically for particular neural network architectures \cite{Megatron-LM, MPDLRM}, such intrusive frameworks are user-unfriendly， making them unsuitable for direct application to arbitrary architectures. On top of that, MP has frequent all-reduce synchronization communication, which is difficult to overlap with the computation. As the number of communicators increases, the overhead of cross-node all-reduce communication is hard to ignore.

% Data parallelism divides the large volume of input data into multiple parts and each device is only responsible for partial data \cite{LargeDP,ParallelizedSGD, HeterogeneityDP}. It requires each device to store a whole model replica, suffering from large model scales\cite{Galvatron}.
% Additionally, with the expansion of the cluster size, the communication burden linked with ZeRO predictably intensifies.

% To expedite the training process, we can leverage the computational capabilities of several machines. Before adjusting the model's parameters, a step to synchronize the gradients is executed to guarantee that all participating machines utilize a consistent set of parameters when assessing new training data.
% \textbf{Strong Interdependence}. In the landscape of deep learning, Data-Centric LLM training presents a shift from traditional task-specific DL training. This emerging paradigm emphasizes self-supervised training across broad data sets, paving the way for adaptation to various downstream tasks, which include data-quality-centric evaluations, and fine-tuning tasks, among others. A pronounced mutual dependency exists between these downstream tasks and the pretraining task. Specifically, the commencement of any downstream task hinges on the completion of the pretraining task. Furthermore, the performance outcomes of these downstream tasks can determine the necessity to revisit or reinitiate the pretraining task.

% \noindent\textbf{Unbalanced resource used}. Figure \ref{figure_treemap_job_types} delves deeper into this context, showcasing the distribution of task counts and the corresponding GPU processing time across diverse workload categories. A striking observation from this data is the dominant presence of evaluation tasks in both clusters. Despite this dominance in task numbers, they have a surprisingly low resource footprint, evidenced by Kcluster's minimal 0.8\% resource consumption. In stark contrast, pretraining jobs, although constituting a meager 0.9\% and 3.2\% of the total job count in the Scluster and Kcluster respectively, lay claim to a significant portion of GPU processing time, consuming 69.5\% and 94.0\% in each cluster respectively. It is worth noting that a limited number of resource-consuming upstream pretraining jobs directly affects the initiation of a large number of less resource-consuming downstream tasks. 

% \noindent\textbf{Super-scale training}. The dependency of these two tasks and the clear unbalance between job distribution and resource consumption lead to the realization that there are clear benefits to scheduling more resources to the maximum, even using as many as 1024 GPUs, in model pretraining tasks, even if the model size is only 1B in size. Adopting this strategy ensures that numerous downstream tasks can be rapidly deployed within the cluster, providing organizations with a more agile environment for model evaluation and fine-tuning, and thus speeding up deployment. However, a challenge arises when considering current model training methodologies. Dominated by task-specific approaches and driven by a sense of cost-effectiveness, current training approaches have not yet fully embraced this super-scale strategy, such as using 1024 GPUs to train 1B models.


% Distinguished from task-specific DL models, Data-Centric LLM training follows an emerging paradigm that performs self-supervised training on broad data and further adapts to a wide range of downstream tasks.
% In the realm of data-driven workflows, there's a growing trend among enterprises to fully exploit all available GPU resources within a cluster, often numbering up to 1024 GPUs. This is pursued with the objective of swiftly pretraining models on a fixed dataset size, even when the pretraining model size is as compact as 7B. This strategic approach empowers them to rapidly kickstart a plethora of downstream tasks within the cluster, which correspond to the more dominant and resource-intensive pretrained models. These downstream tasks primarily encompass evaluations and sft operations.

% Figure 1 presents the distribution of task counts and GPU processing time across various workload types. It is evident from the data that evaluation tasks dominate the overall number of tasks in both clusters. Surprisingly, they consume a minuscule proportion of resources, with Kalos utilizing a mere 0.8\%. On the flip side, even though pretraining jobs account for only 0.9\% and 3.2\% of the total job count in the Seren and Kalos clusters respectively, they occupy a substantial 69.5\% and 94.0\% of the total GPU usage time in each cluster. A noteworthy observation is that the initiation of various evaluation tasks and sft tasks hinges on the pretraining models that have already been completed within the cluster.


% \begin{figure}[t]
%     \centering
%     \includegraphics[width=\linewidth]{treemap_job_dist.pdf}
%     \caption{Distribution of different workload types in Scluster (a, b) and Kcluster (c, d). Note that CPU jobs are excluded. \emph{SFT}: Supervised Fine-Tuning for model alignment. \emph{MLLM}: Multimodal Large Language Model. \textsl{Other}: Unclassified jobs.}
%     \label{figure_treemap_job_types}
% \end{figure}



\section{Challenges and Motivation}

\subsection{Challenges for Linear-Scaling Training}




\label{model_training}

% \noindent\textbf{Super-scale Communication Overhead}.

% \noindent\textbf{High Ratio of Communication-to-Computation.}


\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{Comm_scaless.pdf}
    \caption{Effective bandwidths of three common collective communication algorithms at varying communication volumes, evaluated both at 64 and 512 A100 GPUs with 200GB/s NvLink and 800Gb/s InfiniBand network.}
    \label{Comm_scaless}
\end{figure}

\begin{algorithm}[t]
    \caption{ZeRO-3 algorithm}
    \small
    \label{zero_alg}
    \begin{algorithmic}[1]
        \Input \textbf{model,world size}
        \Output \textbf{model}
        \While {model not converged}
        \State \emph{AllGather(\textbf{P},world size);}
        \State \emph{model.forward();}
        \State \emph{partition(\textbf{P},world size);}
        \State \emph{AllGather(\textbf{P},world size);}
        \State \emph{model.backward();}
        \State \emph{partition(\textbf{P},world size);}
        \State \emph{ReduceScatter(\textbf{G},world size);}
        \State \emph{optimizer.step();}
        \EndWhile
    \end{algorithmic}
\end{algorithm}

\noindent\textbf{Limited scalability of communication operator}.
\label{highRatioOfCommToComp}
Allreduce operations used to sum gradients over multiple GPUs have usually been implemented using rings \cite{Horovod} to achieve full bandwidth. The downside of rings is that latency scales linearly with the number of GPUs, preventing scaling above hundreds of GPUs. NCCL 2.4 adds double binary trees \cite{TreeAllreduce3,Flattenedbutterfly}, which offer full bandwidth and a logarithmic latency even lower than 2D ring \cite{TreeAllreduce1,TreeAllreduce2} latency though not perfect, the performance shown in Figure \ref{Comm_scaless}. However, ZeRO-3 uses extra other two primitive Allgather \cite{allgather1} and ReduceScatter \cite{ReduceScatter} to aggregate partitioned $P$ and $G$ shown in Algorithm \ref{zero_alg} line 2,5,8, which shows a very limited scalability in Figure \ref{Comm_scaless} when scale LLM training from 64 to 512 GPUs.


% The training parallelism techniques mentioned in \S\ref{DistributedDLsystem} necessitate cross-device communication. These communication patterns are commonly orchestrated using collective operations such as all-reduce \cite{Allreduce, TreeAllreduce1}, all-gather \cite{allgather1}, and reduce-scatter \cite{AMD}. Compared to the MP solutions, ZeRO-powered DP solutions are general to various models and do not require model refactoring. Thus, in the following sections, we will primarily analyze the communication overhead associated with DP-based techniques. ZeRO generally involves 3 communication steps within a single training step when using gradient accumulation techniques:

% % In the following sections, I will primarily focus on analyzing the communication overhead associated with DeepSpeed Zero and Megatron-LM.

% \begin{itemize}[leftmargin=*]
% \item \emph{All-Gather}. In both the forward and backward propagation phases, the system executes dual all-gather parameter operations \cite{ZeRO, PyTorchFSDP, MiCS} within a communication cohort characterized by a DP world size.

% \item \emph{Reduce-Scatter}. Upon completing the backward propagation, gradient synchronization becomes imperative, necessitating a reduce-scatter operation within a communication group of a consistent size. The frequency of such reduce-scatter operations aligns with the gradient accumulation \cite{GradAccumulate} magnitude.


% \end{itemize}

% However, these three communication operators have limited scalability, as shown in Figure \ref{Comm_scaless}, when training on 512 GPUs, both effective bandwidth of the All-Gather and Reduce-Scatter algorithm will decrease a lot, All-Reduce shows a decreasing trend although it has been optimized from several studies \cite{NvidiaBlog1, TreeAllreduce1, TreeAllreduce2, TreeAllreduce3, Flattenedbutterfly} comparing to training on 64 GPUs ranging from 4MB to 512MB of parameters number. Furthermore, the default number of parameters set by DeepSpeed \cite{DeepSpeed, PyTorchDistributed, ZeRO} ranges from 10MB to 500MB, which is included in 4MB to 512MB. At the same time, we cannot endlessly raise the number of parameters, for example to 2GB, which has less influence on scalability shown at \ref{Comm_scaless}, to avoid this limitation due to the constraint of global-batch-size and overlap efficiency \cite{ZeRO++}. Therefore, due to the limited scalability of communication operators, communication overhead will increase a lot when we use thousands of GPUs. 

% The main reason behind this challenge is the number of cross-node communications surging, which generally have lower bandwidth (InfiniBand or Ethernet) \cite{InfiniBand} than intra-node (NvLink or NVSwitch) \cite{NVLink,NVSwitch}. 
%  NVIDIA \cite{NvidiaBlog1} attempts to use hierarchical solutions 
%  \cite{TreeAllreduce1, TreeAllreduce2, TreeAllreduce3, Flattenedbutterfly} as potential remedies for All-Reduce though didn't offer flawless resolutions \cite{NvidiaBlog1}. Nevertheless, the idea of hierarchy can be borrowed for All-Gather and Reduce-Scatter. 



% Nvidia\cite{NvidiaBlog1} once elucidated the scalability challenges of the all-reduce\cite{Allreduce} algorithm, proposing tree-based or binary tree solutions\cite{TreeAllreduce1, TreeAllreduce2, TreeAllreduce3, Flattenedbutterfly} as potential remedies. However, these didn't offer flawless resolutions \cite{NvidiaBlog1}. Within the training ecosystem, there are other operators, like all-gather\cite{AllGather}, reduce\cite{Reduce}, and reduce-scatter\cite{ReduceScatter}, that grapple with scalability issues. Figure \ref{Comm_scaless} provides a visual representation of this challenge. Yet, the scalability problems of these operators remain largely unaddressed by the contemporary DL system, which will lead to high communication to exacerbate the ratio.

% On the one hand, communication efficiency in three distinct operations is largely influenced by the user-defined bucket size. By fine-tuning this parameter, communication processes can be substantially enhanced, potentially achieving speed boosts exceeding 2$\times$, as noted by \cite{PyTorchDistributed}. Within the DeepSpeed framework \cite{DeepSpeed}, default values are set as 500MB for the all-gather bucket size and 10MB for the reduce-scatter bucket size. In contrast, the conventional DDP approach \cite{PyTorchDistributed} designates a default bucket size of 25MB. Figure \ref{Comm_scaless} shows the communication performance of three communication operators in the range of 10MB to 500MB in a cluster, where nodes are equipped with high-bandwidth NVLink as an intra-node interconnect, and cross-node links use an 800 Gbps IB network. A noteworthy decline in the effective bandwidth for both reduce-scatter and all-gather operations becomes evident when the system scales from 64 GPUs to 512 GPUs. Considering that the frequent collective communication intrinsic to DDP-based methods operates within this less-linear-scalable communication interval, as the system expands, the latency increasingly undermines the communication's efficiency. At the same time, \cite{ZeRO++} shows that as the compute cluster grows in size, the micro-batch size that each card can compute decreases with a fixed global batch size, thus decreasing the computational efficiency. This leads to very high communication-to-computation ratios due to reduced computational efficiency and increased communication overhead per rank.



% On the other hand, low-bandwidth clusters are common in most cloud environments, and cross-node links typically use less than 100Gbps Ethernet \cite{DeAR}, making them a communication bottleneck. This further increases the communication-to-computation ratios.

\noindent\textbf{Rigid model-states partitioning}. 
ZeRO-1 achieves a fourfold reduction in memory compared to DP but also introduces additional overhead in the form of ReduceScatter and AllGather communications \cite{ZeRO}. ZeRO-2 goes further by achieving an eightfold memory reduction; however, it also introduces more gradient synchronization overhead compared to ZeRO-1, especially when utilizing gradient accumulation techniques \cite{ZeRO++}. ZeRO-3's memory reduction scales linearly with the DP degree, but it also adds approximately 50\% more communication overhead \cite{ZeRO-Infinity, ZeRO++}. Notably, when there's a requirement for a ninefold memory reduction during training, ZeRO-2 becomes infeasible due to OOM. Using ZeRO-3 in such cases would involve excessive costs associated with aggregating parameters on a larger scale. These constraints underscore ZeRO's inherent rigidity in model state partitioning. As a result, user-chosen strategies are frequently sub-optimal due to the limited choices among ZeRO-1, ZeRO-2, and ZeRO-3. Moreover, MiCS \cite{MiCS} is a strategy rooted in ZeRO3, segmenting the model state by forming subgroups. It introduces an alternative partitioning strategy beyond ZeRO's complete partition, giving users an added choice. However, at its core, it remains a ZeRO approach and still faces the rigidity issues associated with the model state.


% ZeRO-1 reduce 4 $\times$ memory to DP,while increase reduce-scatter and allgather communication overhead, ZeRO-2 reduce 8 $\times$ memory with introduce extra graient synchornization overhead than ZeRO-1 when using gradient accumulation techniques. ZeRO-3 memory reduction is linear with DP degree,while bring more about 50\% communication overhead. However, when we need 9 $\times$ memory reduction as training, at that time ZeRO-2 fails to use since OOM, while if we use ZeRO-3, the cost bring by aggregate parameters from large scale is unacceptable. These boundary shows us the feature of ZeRO's rigidity of model state partitioning. Therefore, the strategy selected by users always sub-optimal since the limited selection of ZeRO-1,ZeRO-2,ZeRO-3.


% Aiming to curtail the costly inter-node communication inherent in collective exchanges, recent refinements to ZeRO-3, exemplified by methods like MiCS \cite{MiCS}, leverage on-device memory to facilitate more efficient communication. Under the MiCS strategy, the GPU cluster is segmented into distinct sub-groups. Within these sub-groups, model states are divided, yet consistently duplicated among the different sub-groups. MiCS uses the smallest number of nodes as the partition group size, allowing us to train models with the selected batch size. 

% MiCS's approach of dividing subgroups is an effective strategy for super-scale training, but the size of the subgroups in MiCS tends to increase as the model grows, which may inadvertently lead to out of memory (OOM) errors \cite{ZeRO++}. If subgroup sizes are endlessly expanded to circumvent OOM errors, the benefits of this approach are diminished due to increased cross-node communication. This problem is caused by the inflexible behavior of MiCS to collectively shrink communication subgroups simultaneously for all three model states (parameter, gradient, and optimizer), limiting the potential for performance optimization.
% For example, when MiCS has an OOM problem, we can make the slice of OS, the model state with the highest memory usage, saving more memory compared to making the slice of parameters, gradient, and optimizer states larger at the same time, which would increase the synchronization communication overhead within the subgroup.

% Model-Parallel based techniques like tensor parallel \cite{Megatron-LM,Megatron-LM1} and pipeline parallel \cite{GPipe} partially partition the parameters tensor of the modules, for example, attention $\&$ MLP, \cite{Megatron-LM1}, in which G and OS of the model states will be assigned to different devices according to the way the parameters tensor is sliced, that form a kind of unity. On the one hand, cutting part of the module results in the model states being only partially cut, which is not as effective as ZeRO in saving the memory of the model states. On the other hand,  tensor parallel requires frequent and expensive all-reduce communication, so it is usually used within a node to avoid cross-node communication \cite{OverlapTP}, 
% % and the limitation of the slicing range leads to very inflexible model state slicing.
% which is not that flexible.

% Therefore, the existing DL framework process model states that an integrated unit will inadvertently bypass potential optimization avenues.
% Furthermore, while MiCS achieves a reduction in communication overhead by downsizing communication, the simultaneous collective downsizing of communication groups for the three model states of parameters, gradients, and optimizers behaves INFLEXIBLY, which can limit the potential for performance optimization.

% The process of training a deep learning model predominantly involves three stages: forward propagation, backward propagation, and the updating of parameters. To expedite the training process, we can leverage the computational capabilities of several machines. Before adjusting the model's parameters, a step to synchronize the gradients is executed to guarantee that all participating machines utilize a consistent set of parameters when assessing new training data. When the model or data becomes so extensive that a singular device cannot finish the training within an acceptable timeframe, we turn to machine learning parallelization strategies, which distribute the computation across multiple devices.

% \noindent\textbf{Less-Super-Scalable in Data parallelism}. Data parallelism techniques have gained popularity as a means to amplify distributed training for extensive input datasets. This strategy involves distributing data samples among several workers to process and coordinate model updates, such as gradients. While each worker typically holds a copy of the model, necessitating the model to fit within the device's memory, this can lead to superfluous memory usage. To mitigate this inefficiency, DeepSpeed ZeRO \cite{ZeRO} also referred to as FSDP in FairScale \cite{PyTorchFSDP} was introduced. ZeRO distributes model states across all devices in a cluster to minimize each device's memory usage. It has three distinct stages, each offering varying degrees of memory savings. Nevertheless, this approach frequently leads to amplified communication costs stemming from collective communications. Additionally, with the expansion of the cluster size, the communication burden linked with ZeRO predictably intensifies.

% : ZeRO-1 targets only the optimizer states; ZeRO-2 handles both gradients and optimizer states; while ZeRO-3 equally divides all three states—parameters, gradients, and optimizer states—among all devices in the training cluster.

% \textbf{Super-Scale attempts}. Aiming to curtail the costly inter-node communication inherent in collective exchanges, recent refinements to ZeRO-3, exemplified by methods like MiCS \cite{MiCS}, allocate on-device memory to facilitate more efficient communication. Under the MiCS strategy, the GPU cluster is segmented into distinct sub-groups. Within these sub-groups, model states are divided, yet they are consistently duplicated among the different sub-groups. For the partition group size, MiCS uses the smallest number of nodes allow us to train models with the selected batch size. The MiCS approach of dividing subgroups is an effective strategy for super-scale training, but the size of the subgroups in MiCS tends to increase as the model grows, which may inadvertently lead to OOM (out of memory) errors. If subgroup sizes are endlessly expanded to circumvent OOM errors, the benefits of the MiCS approach are diminished due to increased cross-node communication.

% , i.e., 1 node for BERT 10B, 2 nodes for BERT 15B and 20B, and 8 nodes for BERT 50B.

% \noindent\textbf{Intrusive and Less-Super-Scalable Model parallelism}. Model parallelism (MP) divides the model into multiple parts and each worker is only responsible for the computation of the partial model. Furthermore, numerous MP frameworks are tailored specifically for particular neural network architectures \cite{Megatron-LM, MPDLRM}, such intrusive frameworks are user-unfriendly， making them unsuitable for direct application to arbitrary architectures. On top of that, MP has frequent all-reduce synchronization communication, which is difficult to overlap with the computation. As the number of communicators increases, the overhead of cross-node all-reduce communication is hard to ignore.

% Due to the complexity of DL model architecture, a variety of model parallelism approaches have been proposed with different model partition techniques. There are mainly two kinds of paradigms commonly used for large-scale transformer training, including distributed tensor parallelism (TP) and layer-wise pipeline parallelism (PP). 
% For example, Megatron-LM \cite{Megatron-LM} uses TP, partitions the feed-forward and self-attention modules in Transformers to multiple devices, and inserts communication operations (e.g., All-Reduce) to guarantee consistent results. GPipe \cite{GPipe} first proposes PP, treats each model as a sequence of layers, and partitions the model into multiple composite layers across the devices. The workers are organized as a pipeline and transfer intermediate results at the partition boundaries between neighboring partitions.


% \noindent\textbf{Computation Graph Specific Automatic parallelism} techniques, such as FlexFlow\cite{FlexFlow}, OptCNN\cite{OptCNN}, TensorOpt\cite{TensorOpt} and Alpa\cite{Alpa}, have been developed to seamlessly combine both data and model parallelism, aiming to identify enhanced distributed training methodologies. However, while these automated parallelism solutions focus on optimizing the partitioning and placement strategies for the optimal operators within the computational graph, they overlook strategies related to the orthogonal placement of the model states.

% \subsection{Non-negligible Communication overhead}
% \label{communication_overhead}

% The training parallel techniques mentioned in \S\ref{model_training} necessitate inter-device communication. These communication patterns are commonly orchestrated using collective operations such as all-reduce, all-gather, and reduce-scatter\cite{AMD}. Compared to the MP solutions, ZeRO-powered data-parallel (DP) solutions are general to various models and do not require model refactoring. In the following sections, we will primarily analyze the communication overhead associated with DDP-based techniques. ZeRO generally involves 3 communication steps within a single training step when using gradient accumulation techniques:

% % In the following sections, I will primarily focus on analyzing the communication overhead associated with DeepSpeed Zero and Megatron-LM.

% \textbf{All-Gather}. During both the forward and backward propagation phases, the system executes dual all-gather parameter operations within a communication cohort characterized by a DP world size.

% \textbf{Reduce-Scatter}. Upon completion of the backward propagation, gradient synchronization becomes imperative, necessitating a reduce-scatter operation within a communication group of consistent size. The frequency of such reduce-scatter operations aligns with the gradient accumulation magnitude.

% \textbf{All-Reduce}. Vanilla data parallelism uses all-reduce for gradient reduction across DP world size. 

% \noindent\textbf{Less-Super-Scalable of Communication Operator}. Communication efficiency in three distinct operations is largely influenced by the user-defined bucket size. By fine-tuning this parameter, communication processes can be substantially enhanced, potentially achieving speed boosts exceeding 2X, as noted by \cite{PyTorchDistributed}. Within the DeepSpeed framework\cite{DeepSpeed}, default values are set at 500MB for the all-gather bucket size and 10MB for the reduce-scatter bucket size. In contrast, the conventional DDP approach\cite{PyTorchDistributed} designates a default bucket size of 25MB. Figure 2 graphically illustrates the communication behavior within the 10MB to 500MB range. A noteworthy decline in effective bandwidth for both reduce-scatter and all-gather operations becomes evident when the system scales from 64 GPUs to 512 GPUs. Considering that the frequent collective communication intrinsic to DDP-based methods operates within this less-super-scalable communication interval, as the system expands, the latency increasingly undermines the communication's efficiency.



% Hence, when aiming to scale up to 512 GPUs without compromising effective bandwidth, there are two approaches to consider:

% 1)Increase the bucket size to over 500MB, which may result in additional temporary GPU memory usage and an elevated risk of running out of memory (OOM).

% 2)Implement redundant storage to reduce the number of participants in collective communication.

% The traffic volume for each all-gather and reduce-scatter operation is influenced by the user-defined bucket size. When appropriately configured, this bucket size can significantly enhance communication efficiency, potentially yielding a speedup of more than 2X\cite{PyTorchDistributed}.\\

% \noindent\textbf{Megatron's Tensor Parallelism (TP)} includes specific communication operations in a training step. 

% \textbf{All-Reduce}.This approach splits GEMMs into the MLP and self-attention\cite{Attention} blocks across GPUs while requiring only two all-reduce operations in the forward pass and two all-reduce in the backward pass.

% In tensor parallelism, the communication volume for each all-reduce is correlated with the model size and is represented as \( bsh \), where \( b \) stands for batch size, \( s \) for sequence length, and \( h \) for hidden size. Taking the LLaMA 6.7B model parameters as an illustrative example: with a hidden size of 4096, sequence length of 2048, and a batch size of 8 per GPU, the estimated communication volume for a single allreduce operation is approximately 50MB.


% Owing to the gradient communication overhead, parallel training jobs usually have lower GPU utilization \cite{Philly, MLaaS} and less colocation interference.

% We also evaluate the three-job packing situation and find it typically suffers from acute speed degradation, which is in line with previous work \cite{Gavel}


\subsection{Opportunities for Linear-scale Training}

\noindent\textbf{Unified partition space for model-states}.
\label{flexibleParSpace}
To address the rigidity issue of model state partitioning, we propose constructing a unified partitioning space. This space goes beyond the conventional strategies of Data Distributed Parallelism (DDP), where each GPU redundantly holds a complete set of model states, and the ZeRO approach that assigns each GPU an independent $1/N$ portion of the model states, ensuring the entire training cluster retains just one set of model states. We introduce a partitioning strategy with partial redundancy, inspired by the concept of MiCS. This partitions within $n$ nodes and introduces redundancy across $n/N$ nodes, which we term \emph{intra-partition\&inter-replica} redundancy strategy.

As illustrated in Figure \ref{GeneralPartition}, the partitioning space of ZeRO can be represented by the blue block. However, by decomposing the three components of the model state and independently selecting their partitioning strategies, we can tap into a more expansive gray search space. Notably, ZeRO stages 1, 2, and 3 are encompassed within this unified space.



% To address the challenge of rigidity of model state partitioning, we can build a unified partition space for it. As shown in Figure \ref{GeneralPartition}, the previous DL framework strategy partition space can be represented like a blue block. However, if we can disaggregate the three members of the model state and choose the partition strategy independently, we will gain a larger grey search space.

% Furthermore, the user may have more selection about different combinations of three distinct slicing ways at the top, meanwhile, \SysName will automatically choose the communication optimal combination for users.


% Previously, ZeRO, MiCS, and MP strategies treated the model states as a singular entity for partitioning, as depicted by the grey section labeled "predefined group" in Figure \ref{GeneralPartition}. Based on the parallel training approach, different slicing strategies were then chosen for this unified entity. For instance, the ZeRO3 approach splits the model state within the DP world size, while MiCS slices it based on the memory requirements, allocating it across one-node, dual-node, or quad-node configurations, etc. Typically, TP aims to partition the model state predominantly within a single machine. As a result, the traditional approach of treating the model state as a single unit offers at most \( n(4) \) partitioning options, as shown by the grey lines in the bottom section of Figure \ref{GeneralPartition}. This means that during runtime, 'p', 'g', and 'os' utilize the same partitioning strategy, with each rank receiving equal data chunks of 'p', 'g', and 'os' -- represented by the grey blocks in the upper section of Figure \ref{GeneralPartition}. However, we observe that such coarse-grained partitioning of the model state as a monolithic entity could miss out on certain optimization opportunities. Thus, we decided to disaggregate the three components of the model state and choose partitioning strategies for each independently. With this approach, the \( n(4) \) grey lines shown in the bottom section of Figure \ref{GeneralPartition} expand to \( n^3(4^3) \) green lines. This increase stems from that each component now has \( n(4) \) distinct choices. Consequently, we have constructed a more flexible partitioning space. In this setup, data chunks allocated to each rank will vary, as indicated by the blue blocks in the top section of Figure \ref{GeneralPartition}. 


\noindent\textbf{Scale-aware Partitioner.}
To automatically search for an optimal combination for the user at the partition space, we can construct a generic memory and communication cost model within this larger partitioning space. Considering the data dependencies among $P$, $G$, and $OS$, and based on the principles of minimal memory consumption and communication, we can search for the most cost-efficient partitioning strategy combination for a specific scale of compute resource.

% From Section \ref{flexibleParSpace}, we understand that when we utilize more computational resources for model training, the communication-to-computation ratio will increase, subsequently affecting the training efficiency. Given our expanded partitioning space, there are various combinations of 'p', 'g', and 'os' slicing strategies available. To optimize this, we can construct a generic memory usage model and a communication cost model within this larger partitioning space. Taking into account the data dependencies among 'p', 'g', and 'os', and based on the principles of minimal memory consumption and communication, we can select the most cost-efficient partitioning strategy combination for a specific scale of compute resource.

\noindent\textbf{Extra communication optimization.}
When the three components of the model state are divided in a different way than the DP approach, the data synchronization method differs from the traditional approach. This provides additional opportunities for communication optimization. For example, we can design hierarchical collection and reduction methods for cross-machine partitioning of $P$ and $G$ to circumvent the overhead associated with naive global synchronization

% The three components of the model states, when partitioned differently from the DDP and ZeRO approach, result in data synchronization methods that diverge from traditional ones. This presents additional opportunities for communication optimization. For instance, rather than relying on a global synchronization within the communication group, MiCS has designed hierarchical gather and reduce methods for cross-machine partitioning of parameters and gradients to minimize inter-machine communication. Inspired by MiCS, by constructing a generic stratified communication synchronization method within the expanded partitioning space mentioned in Figure \ref{GeneralPartition}, we can circumvent the overheads associated with naive global synchronization. This approach provides avenues to further reduce the inefficiencies caused by the high communication-to-computation ratio when the computational resource scale is large.
% As discussed in \S\ref{Data-Centric_LLM_training}, for the efficient functioning of a data-centric system, we have determined that training a Large Language Model (LLM) at a super-scale is more effective in data-centric LLM training. However, as pointed out in \S\ref{model_training}, current training approaches are inadequately optimized for super scale. \S\ref{communication_overhead} identifies communication as the primary bottleneck for super-scale training. This insight motivated us to design a system specifically aimed at reducing the communication overhead inherent in super-scale setups. While MiCS achieves a reduction in communication overhead by shrinking the communication scale, it is the act of concurrently and collectively downsizing the communication groups for the three model states — parameters, gradients, and optimizer — that limits the potential for performance optimizations. We've observed that by respecting the data dependencies between model states, individually creating communication groups for each of the three elements of the model state can open up a considerably broader optimization space, which includes the MiCS strategy as a special case.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{example.pdf}
    \caption{An optimization example of \SysName. Different model state partition strategies result in varied memory usage and affect which components participate in communication. \SysName minimizes both the number of participants and the range of communication without stressing the memory.}
    \label{example}
\end{figure}

\subsection{Motivating Example} 
Figure \ref{example} showcases an optimization example identified by \SysName when training a 6.7B LLaMA model on 1024 A100(80GB) GPUs, with each node comprising 8 GPUs. We apply ZeRO1 as a comparison, which only partitions model-states $OS$ into 1024 GPUs. Besides, following the insight from MiCS \cite{MiCS}, we partition the model states into one node, and there are 128 replicas of model states in the training cluster. However, \SysName selects a strategy that remains a full replica of $P$ and $G$ like ZeRO, while sharding $OS$ to one node like MiCS.

% The optimization selected the 'whole replica' strategy for both parameters and gradients, while the optimizer state adopted the 'Intra Partition\&Inter Replica' strategy with shardOS set equal to the number of ranks per node (8). To highlight the superiority of \SysName, we applied the ZeRO1 technique to the 6.7B LLaMA model in line with ZeRO's guidelines concerning model dimensions. Furthermore, drawing insights from MiCS as elaborated in \S\ref{model_training}, we allocated this 6.7B model to a single node, denoting it as MiCS-shard8.

\noindent\textbf{Memory usage analysis}. When utilizing mixed precision in the training of the 6.7B LLaMA model, the memory proportions for $P$, $G$, and $OS$ are 2, 2, and 12 respectively, as detailed in Section \S\ref{mixprecision}. In the case of ZeRO-1, which partitions only $OS$ across 1024 GPUs, the memory footprint becomes $2+2+12/1024$. On the other hand, MiCS, which distributes $P$, $G$, and $OS$ over 8 GPUs, has a memory footprint of $(12+2+2)/8$. Meanwhile, \SysName, which allocates only $OS$ to 8 GPUs, has a memory footprint of $2+2+12/8$. Consequently, the memory consumption for ZeRO-1, MiCS, and \SysName are approximately $4.01GB$, $2GB$, and $14.5GB$ respectively. Although \SysName exhibits a higher memory consumption in this context, 
% it remains sufficiently minimal to allocate the maximum micro-batch size required for convergent training.
it remains sufficient for the maximum micro-batch-size required for convergent training.


% the memory consumption typically comprises two main segments: the model state and activations \cite{ZeRO, MixedPrecisionTraining}. The model state consumes memory generally approximately 16 times the model size if we use Adam optimizer \cite{Adam}. Within this state, the proportions occupied by P, G, and OS are 2, 2, and 12 respectively. Therefore, the memory consumption ratios for these three strategies approximately stand at 2:1:3. For ZeRO1, it seems that it has a more efficient memory footprint for activation to increase the micro batch size. However, there is a limitation to global batch size for training convergence. For example, for convergent, the global-batch size is set to 4096, since we have 1024 GPUs for training, the maximum micro-batch size each GPU can handle is restricted to 4.

 % On the other hand, the memory taken up by activations is contingent on the size of the input data, which is beyond the scope of this paper. it cannot leverage the excess memory to process additional data. This limitation arises from the constraints on the global batch size, set at 4096. Given 1024 participating GPUs, the maximum micro-batch size each GPU can handle is restricted to 4.

 % We observed that even though \SysName consumes 3 $\times$ the GPU memory compared to ZeRO1, memory usage is not the bottleneck. 

\noindent\textbf{Communication overhead analysis}. Figure \ref{example} (b) illustrates the communication of the three approaches. Both ZeRO-1 and \SysName, having partitioned only $OS$, necessitate the communication solely for $OS$. Specifically, Zero-1's $OS$ communication spans the entire world size, encompassing cross-node communication for each instance within the world size. In contrast, \SysName restricts its communication to intra-node, thus eliminating any cross-node communication. However, since MiCS segments $P$, $G$, and $OS$, all three components require communication. Yet, all these communications are confined within the node, resulting in zero cross-node exchanges. Consequently, when solely considering cross-node communications, the overhead for ZeRO-1 is up to the number of world size, while MiCS and \SysName are 0. However, noting that MiCS incurs additional intra-node communication for $P$ and $G$ compared to \SysName. 

As a result, \SysName reduces the number of cross-node and intra-node communications to 0 and 8 without increasing the pressure of GPU memory.
% As shown in Figure \ref{example} (a), consider the 6.7B LLaMA model being trained using mixed precision. The model state of this setup occupies 107GB, distributed as 13.4GB for parameters, 13.4GB for gradient, and a significant 80.2GB for optimizer state. By partitioning the optimizer state across 8 A100 GPUs(80GB), we called it the 'Intra partition \& Inter Replica' strategy and adopted the 'whole replica' strategy for Parameters and Gradients, each GPU is tasked with accommodating a model state memory footprint of 36.8GB, and the remaining 43GB is amply sufficient to manage the memory requirements of activations and temporary variables needed for the model's training. These 8 A100 GPUs handle in-place updates for their respective 1/8 share of the optimizer state. Subsequent to this, they utilize the fast intra-node NVlink communication to perform an all-gather operation, ensuring a synchronized, updated set of parameters across all ranks. Importantly, our strategy eliminates the need for the 1024 cross-node all-gather communications required by zero1, bringing that count down to 0. In addition, our strategy also eliminates the all-gather parameter and gradient synchronization-related communications required by MiCS.

% As illustrative examples, Figure \ref{example} displays three optimizations identified by our method when training LLaMA models of varying sizes in 1024 GPUS. Following the guidelines proposed by Zero regarding model sizes, we apply Zero1 for the LLaMA model with a size of 6.7B, Concurrently, based on the takeaways from MiCS, as mentioned in \S\ref{model_training}, we partition these models across different ranks: 1 node (8 ranks) for the 6.7B model and labeled as MiCS-shard8.
% when training a 6.7B-sized model on 1024 cards, our method searched for a superior model state grouping compared to both zero1 and MiCS-shard8. Specifically, we adopted the 'whole replica' strategy for Parameters and Gradients, while using the 'Intra partition \& Inter Replica' approach for Optimizer states. This distinction allows the optimizer step's parameter update operations to be executed using faster intra-node communications (via NVlink) when compared to zero1. Additionally, in contrast to the MiCS-shard8 approach, our method reduces overheads associated with all-gather parameters and gradient synchronization during each forward and backward computation.

% As shown in Figure 3 (b), for training a 30B-sized model using 1024 ranks, our method search out a strategy where 'Parameters' and 'Gradients' utilize the 'Intra-partition \& inter-Replica' approach, while the 'optimizer state' employs the 'world-size partition' method. This arrangement, in comparison to zero3, can significantly reduce the overhead associated with the global allgather of parameters and gradient accumulation during both forward and subsequent computations. Additionally, when contrasted with the MiCS-shard32 method, our approach curtails the overheads introduced by the 4-node cross-node all-gather parameters and the cross-node gradient synchronizations that result from the sharding across 32 ranks during each forward and backward computation. At the same time, our approach offers additional memory savings compared to MiCS-shard32. This is primarily because we partitioned the optimizer states, which consume the most memory, into 1024 segments rather than just 32.


% for training a 13B-sized model with 1024 cards, our approach determined a specific grouping strategy: using the 'whole replica' strategy for Parameters, the 'Intra-partition \& Inter-Replica' method for Gradients, and the 'world-size partition' approach for Optimizer states. Compared to zero2, this configuration reduces the global gradient synchronization during gradient accumulation. Furthermore, when juxtaposed with the MiCS-shard16 method, our approach curtails the overheads introduced by the 2-node cross-node all-gather parameters and the cross-node gradient synchronizations that result from the sharding across 16 ranks during each forward and backward computation.

