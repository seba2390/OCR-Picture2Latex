We demonstrate our approach on the tasks of 3D super-resolution and surface reconstruction from sparse point clouds, on both objects and scenes as well as synthetic and real-world data.
%
We evaluate on three datasets with increasing complexity: ShapeNet~\cite{chang2015shapenet} (synthetic shapes), 3DFront~\cite{fu20203dfront} (synthetic scenes) and Matterport3D~\cite{chang2017matterport3d} (real-world 3D scans).
%
For ShapeNet, we use the 13 class subset and train/test split from \cite{choy20163d}. 
%
For 3DFront, we use the scenes which have furniture in a train/test split of $15000/2850$ rooms.
%
We use the official train/test split for Matterport3D of 72/18 buildings and 1799/394 rooms.

\smallskip
\noindent \textbf{Metrics.}
We evaluate the reconstructed geometry with $4$ complementary metrics: Volumetric IoU (IoU), Chamfer $\ell_1$ Distance (CD) in meters, Normal Consistency (NC) as cosine distances and F-score (F1) at $1\%$ window size threshold. 
Additional evaluation details can be found in the appendix.

\subsection{3D Super Resolution}
For the task of 3D super resolution, we consider low-resolution geometry as input, and aim to reconstruct high-resolution target geometry.
%
We use input / target voxel sizes of $0.434$m / $0.054$m for 3DFront, $0.15$m / $0.0375$m for Matterport3D, and resolutions $8^3$ / $64^3$ for ShapeNet.
%
For synthetic and real-world 3D scenes of varying sizes, we operate in a sliding window fashion with a stride of $64$ voxels; we similarly run all baselines in the same sliding fashion.

\smallskip
\noindent \textbf{Comparison to state of the art.}
We compare to state-of-the-art 3D generative approaches: SG-NN~\cite{dai2020sg} which operates on sparse volumetric data, IFNet~\cite{chibane2020implicit} which learns implicit reconstruction, and Convolutional Occupancy Networks~\cite{peng2020convolutional} which uses a hybrid of volumetric convolutions coupled with implicit decoders. While these methods encode the entire generative process in the network, we additionally use an explicit database that can assist the reconstruction during inference.
%
In Tab.~\ref{tab:superres}, we show a quantitative comparison; our ability to leverage strong priors from retrieved scene data enables more effective reconstructions of 3D scenes.
%
We additionally show a qualitative comparison in Fig.~\ref{fig:experiments_superresolution}; our approach maintains sharper detail which is more easily propagated by retrieving priors from the train set.
{
\begin{table*}[tp]
\begin{center}
\small
\begin{tabular}{|l|l|c|l|l|l|l|l|l|l|l|l|l|}  
\hline
\multirow{2}{*}{Method} & \multicolumn{4}{c|}{ShapeNet} & \multicolumn{4}{c|}{3DFront} & \multicolumn{4}{c|}{Matterport3D}  \\ 
\cline{2-13}
                        & IoU$\uparrow$ & CD$\times 10^{-2}$$\downarrow$ & F1$\uparrow$ & NC$\uparrow$            & IoU$\uparrow$ & CD$\downarrow$ & F1$\uparrow$ & NC $\uparrow$           & IoU$\uparrow$ & CD$\downarrow$ & F1$\uparrow$ & NC$\uparrow$ \\
\hline
SGNN                    & 0.624 & 0.668 & 0.813 & 0.889         & 0.639 & 0.032 & 0.733 & 0.900              & 0.731 & 0.021 & 0.697 & 0.916                \\ 
ConvOcc                 & 0.648 & 0.726 & 0.838 & \textbf{0.906}            & 0.631 & 0.033 & 0.711 & 0.901             & 0.584 & 0.027 & 0.542 & 0.879                    \\ 
IFNet                   & 0.650 & 0.623 & 0.838 & 0.892              & 0.639 & 0.041 & 0.736 & 0.878             & 0.593 & 0.028 & 0.624 & 0.893              \\ 
\hline
Ours                    & \textbf{0.655} & \textbf{0.590} & \textbf{0.844} & 0.905               & \textbf{0.751} & \textbf{0.027} &\textbf{0.801} & \textbf{0.922}              &  \textbf{0.739} & \textbf{0.020} & \textbf{0.708} & \textbf{0.923}                  \\
\hline
\end{tabular}
\caption{Evaluation of reconstruction performance on 3D super-resolution on ShapeNet, 3DFront, and Matterport3D, with $8\times$ higher target resolution for synthetic data and $4\times$ higher resolution for real data.}
\label{tab:superres}
\end{center}
\vspace{-0.15cm}
\end{table*}
}

\begin{figure*}
	\centering
	\includegraphics[width=\linewidth]{figures/experiments_superresolution.jpg}
    \vspace{-0.6cm}
	\caption{
	    3D super resolution on 3DFront (top) and Matterport3D (bottom) datasets. In contrast to other approaches, our method generates more coherent 3D geometry with sharper details.
    }
	\label{fig:experiments_superresolution}
	\vspace{-0.25cm}
\end{figure*}

\subsection{Surface Reconstruction}
We additionally demonstrate our approach on the task of surface reconstruction from point cloud data.
%
Input point clouds are obtained by randomly sampling $500$ points for each shape in ShapeNet, $1000$ points per $3.45^3\mathrm{m}^3$ for 3DFront, and $1000$ points per $2.4^3\mathrm{m}^3$ for Matterport3D.
%

\smallskip
\noindent \textbf{Comparison to state of the art.}
We compare to the state-of-the-art 3D generative approaches as in the 3D super-resolution task (SG-NN~\cite{dai2020sg}, IFNet~\cite{chibane2020implicit}, Convolutional Occupancy Networks~\cite{peng2020convolutional}), in addition to Screened Poisson Surface Reconstruction (SPSR)~\cite{kazhdan2006poisson,kazhdan2013screened} and Local Implicit Grids (LIG)~\cite{jiang2020local}.
%
All data-driven methods are trained on our data.
%
For our method, SG-NN, and IFNet which take volumetric input, we consider the point cloud as a volumetric occupancy grid (occupancy for voxels containing any points). 
%
Tab.~\ref{tab:pcrecon} shows a quantitative comparison, where our learned use of train scene data through an attention-based refinement provides more accurate geometric reconstruction.
%
Fig.~\ref{fig:experiments_surface_reconstruction} additionally shows that our reconstructions more effectively capture both global structures and local details in the scenes.
%
{
\begin{table*}[tp]
\begin{center}
\small
\begin{tabular}{|l|l|c|l|l|l|l|l|l|l|l|l|l|} 
\hline
\multirow{2}{*}{Method} & \multicolumn{4}{c|}{ShapeNet} & \multicolumn{4}{c|}{3DFront} & \multicolumn{4}{c|}{Matterport3D}  \\ 
\cline{2-13}
                        & IoU$\uparrow$ & CD$\times 10^{-2}$$\downarrow$ & F1$\uparrow$ & NC$\uparrow$     & IoU$\uparrow$ & CD$\downarrow$ & F1$\uparrow$ & NC$\uparrow$    & IoU$\uparrow$ & CD$\downarrow$ & F1$\uparrow$ & NC$\uparrow$ \\ 
\hline
SPSR                   & 0.333 & 3.225 & 0.523 & 0.852              & 0.204 & 0.438 & 0.267  & 0.755            & 0.234 & 0.105 & 0.245 & 0.841                  \\ 
LIG                   & 0.589 & 0.751 & 0.767 & 0.872              & 0.566 & 0.041 & 0.673 &  0.886            & 0.546 & 0.034 & 0.576 & 0.868                   \\ 
SGNN                    & 0.494 & 0.876 & 0.673 & 0.857                & 0.738 & 0.025& 0.804  & 0.919            & 0.441 & 0.029 & 0.471 & 0.867                   \\ 
ConvOcc                 & 0.600 & 0.779 & 0.765 & 0.913              & 0.565 & 0.037 & 0.667  & 0.905             & 0.419 & 0.034 & 0.420  & 0.859                    \\ 
IFNet                   & 0.777 & 0.420 & 0.937 & 0.923             & 0.779 & 0.028 & 0.832 & 0.918              & 0.575 & 0.029 & 0.607 & 0.866              \\ 
\hline
Ours                    & \textbf{0.783} & \textbf{0.377} & \textbf{0.947} & \textbf{0.938}               & \textbf{0.863} & \textbf{0.021} & \textbf{0.875} & \textbf{0.955}             & \textbf{0.710}  & \textbf{0.021} & \textbf{0.702}  & \textbf{0.917}                    \\
\hline
\end{tabular}
\caption{Reconstruction performance on the point cloud to surface reconstruction on ShapeNet, 3DFront, and Matterport3D.}
\label{tab:pcrecon}
\end{center}
\vspace{-0.50cm}
\end{table*}
}

\begin{figure*}
	\centering
	\includegraphics[width=\linewidth]{figures/experiments_surface_reconstruction.jpg}
	\vspace{-0.6cm}
	\caption{
	    Point cloud to surface reconstruction on 3DFront (top) and Matterport3D (bottom) datasets.
	    Our approach captures more coherent structures and object details.
    }
	\label{fig:experiments_surface_reconstruction}
	\vspace{-0.25cm}
\end{figure*}

\subsection{Ablations}
\paragraph{Effect of retrieval and attention-based refinement.}
We evaluate the effect of our retrieval-based priors and attention-based refinement in Tab.~\ref{tab:ablation}.
We consider \emph{Retrieval} as the initial $1^\mathrm{st}$ nearest neighbor estimate provided by the retrieved scene data, \emph{U-Net} as a U-Net backbone styled similar to our refinement (and similar number of parameters to our refinement) but without using retrievals or attention as there are no retrievals to attend to, and \emph{Naive} to be our retrieval and refinement using concatenation of features instead of attention.
A visualization is shown in Fig.~\ref{fig:experiment_components}, with \emph{Retrieval} appearing disjoint between different retrieved chunks, \emph{U-Net} producing over-smoothed results, \emph{Naive} providing more details but still suffering from over-smoothing, and our method (with retrieval priors combined with attention-based refinement) producing the most consistent structure with local details defined.

\begin{figure*}
	\centering
	\includegraphics[width=\linewidth]{figures/experiment_components.jpg}
	\caption{Qualitative evaluation of our method (\textit{Ours}) in comparison to $1^\mathrm{st}$ nearest neighbor retrieval (\textit{1-NN Retrieval}), a U-Net like network which doesn't use retrievals (\textit{U-Net}), and naive fusion of retrieved approximations during refinement (\textit{Naive}).}
	\label{fig:experiment_components}
	\vspace{-0.25cm}
\end{figure*}

{
\setlength{\tabcolsep}{4pt}
\begin{table}
\small
\resizebox{\linewidth}{!}{
\begin{tabular}{|l|l|l|l|l|l|l|l|l|} 
\hline
\multirow{2}{*}{Network}             & \multicolumn{4}{c|}{3D Super Resolution} & \multicolumn{4}{c|}{Surface Reconstruction}  \\ 
\cline{2-9}
                                     & IoU$\uparrow$ & CD$\downarrow$ & F1$\uparrow$ & NC$\uparrow$                    & IoU$\uparrow$ & CD$\downarrow$ & F1$\uparrow$ & NC$\uparrow$                            \\ 
\hline
Retrieval                            & 0.67 & 0.032 & 0.71 & 0.87                       & 0.70 & 0.028 & 0.75 & 0.88                              \\ 
U-Net                             & 0.68 & 0.029 & 0.77 & 0.91                       & 0.83 & 0.024 & 0.85 & 0.94                                \\ 
Naive                                & 0.71 & 0.028 & 0.77 & 0.91                      & 0.84 & 0.023 & 0.86 & \textbf{0.96}                               \\ 
\hline
Ours & \textbf{0.75} & \textbf{0.026} & \textbf{0.80} & \textbf{0.92}                       &  \textbf{0.86} & \textbf{0.021} & \textbf{0.88} & \textbf{0.96}                               \\
\hline
\end{tabular}
}
\vspace{-0.15cm}
\caption{Our attention based refinement performs better in comparison to not using any retrievals (\textit{U-Net}) or naivly fusing of retrievals during refinement (\textit{Naive}) on 3DFront dataset.}
\label{tab:ablation}
\vspace{-0.35cm}
\end{table}
}

\smallskip
\noindent \textbf{Effect of number of nearest neighbor retrievals.}
Tab.~\ref{tab:nn_ablation} shows the effect of increasing number of nearest neighbor retrievals used as a prior for the scene reconstruction.
With more nearest neighbors, the attention-based refinement has more candidates to select geometry from, improving  performance but with decreasing marginal gain.
\begin{table}
\centering
\small
\begin{tabular}{|l|l|l|l|l|c|} 
\hline
k & IoU$\uparrow$ & CD$\downarrow$ & F1$\uparrow$ & NC$\uparrow$ & \makecell{GPU\\memory}  \\ 
\hline
0 & 0.684 & 0.029 & 0.773 & 0.909 & 2.3G  \\ 
1 & 0.733 & 0.028 & 0.794 & 0.920  &  6.4G  \\ 
2 & 0.741 & 0.027 & 0.797 & \textbf{0.923}  &  7.9G \\ 
3 & 0.745 & 0.027 & 0.797 & 0.922  & 9.0G\\ 
\hline
4 & \textbf{0.751} & \textbf{0.027} & \textbf{0.801} & 0.922 & 9.9G \\
\hline
\end{tabular}
\vspace{-0.15cm}
\caption{Additional retrieval approximations help in improving refinement quality, at the cost of higher GPU memory utilization during training. Evaluation performed on 3D super-resolution task on 3DFront dataset.}
\label{tab:nn_ablation}
\vspace{-0.25cm}
\end{table}

\smallskip
\noindent \textbf{Extending the database during test time.}
Our approach can take advantage of new entries in the database without retraining.
Specifically, we conducted an experiment where we train on a subset of $8$ ShapeNet classes and evaluate it on other $5$ classes.
In Tab.~\ref{tab:unseen_classes}, we show that if we augment the database with chunks from the train set of the $5$ classes, our method improves without retraining by leveraging better retrievals.
Note that the other baselines would need to be retrained or refined to take advantage from new data.

\begin{table}
\centering
\small
\begin{tabular}{|l|l|l|l|l|l|} 
\hline
Variant & IoU$\uparrow$ & CD$\downarrow$ & F1$\uparrow$ & NC$\uparrow$ \\
\hline
Ours  & 0.478 & 0.034 & 0.601 & 0.811 \\
Ours (extended DB) & \textbf{0.579} & \textbf{0.029} & \textbf{0.743} & \textbf{0.825} \\
\hline
\end{tabular}
\vspace{-0.15cm}
\caption{Extending the database on ShapeNet subset during test time with additional new chunks leads to better reconstruction quality without the need of retraining.}
\label{tab:unseen_classes}
\vspace{-0.25cm}
\end{table}

\subsection{Implicit Reconstruction with a Database}
We can also apply our approach to implicit networks for 3D reconstruction by leveraging our retrieval estimates as an initial reconstruction for implicit-based refinement.
We thus incorporate our retrieval-based reconstruction with IFNet~\cite{chibane2020implicit}, maintaining our distance field database and incorporating the IFNet encoder (which also takes volumetric input) as well as IFNet decoder.
For additional architecture details, we refer to the appendix.
Tab.~\ref{tab:implicit_variant} shows that our retrieval-based reconstruction on 3DFront super-resolution task also helps to improve upon a learned implicit 3D reconstruction. Qualitative results are shown in Fig.~\ref{fig:experiment_implicit}.
\begin{table}
\centering
\small
\begin{tabular}{|l|l|l|l|l|} 
\hline
Method & IoU$\uparrow$ & CD$\downarrow$ & F1$\uparrow$ & NC$\uparrow$ \\ 
\hline
IFNet & 0.639 & 0.041 & 0.736 & 0.878 \\ 
Ours (Implicit) & {\bf 0.687} & {\bf 0.038} & {\bf 0.766} & {\bf 0.897} \\ 
\hline
\end{tabular}
\vspace{-0.15cm}
\caption{Performance of an implicit variant of method that extends IFNet. Evaluated on 3D super-resolution task on 3DFront dataset.}
\label{tab:implicit_variant}
\vspace{-0.5cm}
\end{table}

\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{figures/experiment_implicit.jpg}
	\vspace{-0.6cm}
	\caption{Qualitative evaluation of the implicit variant of our method on 3D super-resolution task on 3DFront dataset.}
	\label{fig:experiment_implicit}
	\vspace{-0.5cm}
\end{figure}

\paragraph{Limitations.}
Our approach to leverage high-quality train scene data for 3D reconstruction shows notable improvements from state of the art; however, several limitations remain.
Our retrieval estimates and refinement operate in two separate stages without gradient propagation from the final reconstruction to the retrieved scene data, resulting in possible suboptimal retrieval where the refinement must compensate more; developing a differentiable $k$-NN retrieval~\cite{plotz2018neural,tseng2020retrievegan} for refinement could bridge these disconnects.
Additionally, by constructing our retrieval dictionary from chunks of train scenes, our dictionary size can grow quite large; we believe a learned dictionary could help to construct the most informative characteristics to be leveraged for reconstruction.
We refer to the appendix for visualisation and discussion of limitations and failure cases.
