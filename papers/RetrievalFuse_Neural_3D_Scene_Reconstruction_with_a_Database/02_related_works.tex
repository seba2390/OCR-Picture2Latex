
\paragraph{Learned 3D Shape Reconstruction.} 
3D shape reconstruction is a long-standing problem in computer vision. 
We refer readers to Szeliski~\cite{szeliski2010computer} for a more comprehensive review of the classic techniques. 
Recently, inspired by the progress of deep learning for images, many developments have been made in deep generative models for reconstructing 3D shapes, largely focusing on leveraging different geometric representations.

Early generative neural networks focused on voxel grids as a natural extension of pixels, with a regular structure well-suited for convolutions, but can struggle with cubic growth in dimension \cite{maturana2015voxnet,wu20153d,choy20163d,dai2017shape}.
Multi-resolution representations were proposed~\cite{hane2017hierarchical, tatarchenko2017octree} to address the cubic complexity with hierarchical data structures.
%
Rather than operating on a regular grid, point cloud based approaches propose to generate points only on the geometric surface \cite{fan2017point,yang2019pointflow}, but do not encode structural connectivity. 
%
Mesh-based approaches have also been proposed to efficiently capture surface geometry while encoding connectivity, but tend to rely on strong topological assumptions such as a template mesh that is then deformed~\cite{wang2018pixel2mesh}, or a small number of vertices for free-form generation~\cite{dai2019scan2mesh}.
%
Implicit representations encoded directly by the neural network enable modeling of a continuous surface, typically as binary occupancies or signed distance fields \cite{mescheder2019occupancy,deepsdf,chibane2020implicit}; such representations have seen notable success in modeling single objects but can struggle to directly scale to scenes.

\paragraph{Learned 3D Scene Reconstruction.} 
Compared to shape reconstruction, scene-level reconstruction is significantly more challenging due to the scale, variance, and complexity of geometry. 
Several approaches have been proposed to combine local implicit functions with a coarse volumetric basis \cite{jiang2020local,deep_local_shapes,peng2020convolutional} to capture complex, large-scale scene reconstructions.
SG-NN~\cite{dai2020sg} leverages a single, sparse volumetric network for large-scale scene completion in a self-supervised fashion. These approaches rely on encoding the full generative process into network parameters, whereas we leverage a basis of existing scene geometry, that does not need to be fully encoded but rather refined to transfer desired geometric characteristics from the valid scene geometry (e.g., clean structures, local details). 

\paragraph{2D/3D Retrieval.} Our approach is related to 2D image retrieval and completion applications~\cite{datta2008image}, where recent work~\cite{radenovic2018fine, xu2020texture} focuses on developing a CNN to automatically retrieve relevant patches from a large collection of unordered images. 
Note that memorization is also an active area of research in language models~\cite{khandelwal2019generalization}.

For 3D retrieval, the pioneering work of Chen \etal~\cite{chen2003visual} proposed a 3D shape retrieval system based on visual similarity.
More recently, several works have been proposed to leverage 3D CAD model retrieval to represent objects in input images or 3D scans \cite{li2015database,izadinia2017im2cad,avetisyan2019scan2cad,kuo2020mask2cad,izadinia2020licp}, but are limited to the objects in the CAD dataset, while we use our retrieval as a basis for enabling more accurate reconstruction from learned selection and blending of retrieved scene geometry. 
