\documentclass{article}

%
%
%

%
\usepackage[final,nonatbib]{neurips_2021}

\usepackage[]{algorithm2e}

%
%
%

%
%

%
%

\usepackage[utf8]{inputenc} %
\usepackage[T1]{fontenc}    %
\usepackage{hyperref}       %
\usepackage{url}            %
\usepackage{booktabs}       %
\usepackage{amsfonts}       %
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{nicefrac}       %
\usepackage{microtype}      %
\usepackage{xcolor}         %

\usepackage{graphicx}
\usepackage[tight,footnotesize,sf,SF]{subfigure}

\input{macros}


\makeatletter
\newcommand*{\addFileDependency}[1]{%
  \typeout{(#1)}
  \@addtofilelist{#1}
  \IfFileExists{#1}{}{\typeout{No file #1.}}
}
\makeatother

\newcommand*{\myexternaldocument}[1]{%
    \externaldocument{#1}%
    \addFileDependency{#1.tex}%
    \addFileDependency{#1.aux}%
}


\title{
Sparse Quadratic Optimisation over the\\ Stiefel Manifold with Application to\\ Permutation Synchronisation
}



\author{%
  Florian Bernard\\
  TU Munich, University of Bonn
  \And
  Daniel Cremers\\TU Munich
  \And 
  Johan Thunberg\\
  Halmstad University
}

\begin{document}

\maketitle

\begin{abstract}
We address the non-convex optimisation problem of finding a sparse matrix on the Stiefel manifold (matrices with mutually orthogonal columns of unit length) that maximises (or minimises) a quadratic objective function. Optimisation problems on the Stiefel manifold occur for example in spectral relaxations of various combinatorial problems, such as graph matching, clustering, or permutation synchronisation. Although sparsity is a desirable property in such settings, it is mostly neglected in spectral formulations since existing solvers,~e.g.~based on eigenvalue decomposition, are unable to account for sparsity while at the same time maintaining global optimality guarantees. We fill this gap and propose a simple yet effective sparsity-promoting modification of the Orthogonal Iteration algorithm for finding the dominant eigenspace of a matrix. By doing so, we can guarantee that our method finds a Stiefel matrix that is globally optimal with respect to the quadratic objective function, while in addition being sparse. As a motivating application we consider the task of permutation synchronisation, which can be understood as a constrained clustering problem that has particular relevance for matching multiple images or 3D shapes in computer vision, computer graphics, and beyond. We demonstrate that the proposed approach outperforms previous methods in this domain.
\end{abstract}

\section{Introduction}
We are interested in optimisation problems of the form
\begin{align}\label{eq:stiefel}
    %
    \argmax_{U \in \St(m,d)} ~f(U) ~~\text{with}~~ f(U) =\tr(U^T W U) ~~\text{and}~~ \St(m,d) := \{X \in \R^{m \times d}: X^T X = \matI_d\}\,,
\end{align}
where $\matI_d$ is the identity matrix of dimension $d$, $W \in \R^{m \times m}$ and the set $\St(m,d)$ denotes the Stiefel manifold ($m \geq d$). Throughout the paper, w.l.o.g.~we consider a maximisation formulation and we assume that $W$ is a symmetric and positive semidefinite matrix (see Lemma~\ref{ass:sympsd} in Sec.~\ref{sec:bg}). Despite the non-convexity of Problem~\eqref{eq:stiefel}, problems of this form can be solved to global optimality based on the eigenvalue decomposition,~i.e.~by setting $U^* = V_d$, where $V_d \in \R^{m \times d}$ denotes an orthogonal basis of eigenvectors corresponding to the $d$ largest eigenvalues of $W$, see Lemma~\ref{lem:solprob}. The fact that we can efficiently find global optima of Problem~\eqref{eq:stiefel} makes it a popular relaxation formulation of various difficult combinatorial problems. This includes spectral relaxations~\cite{umeyama1988eigendecomposition,Leordeanu:2005ur,Cour:2006un} of the NP-hard quadratic assignment problem (QAP)~\cite{Pardalos:1993uo}, spectral clustering~\cite{ng2001spectral,von2007tutorial}, or spectral permutation synchronisation~\cite{Pachauri:2013wx,shen2016normalized,Maset:YO8y6VRb}. Yet, a major difficulty of such approaches is to discretise the (relaxed) continuous solution in order to obtain a feasible solution of the original (discrete) problem. For example, in the QAP one optimises over the set of permutation matrices, in  clustering one optimises over the set of matrices with rows comprising canonical basis vectors, and in permutation synchronisation one optimises over multiple permutation matrices that are stacked into a large block matrix. Often such combinatorial sets can be characterised by Stiefel matrices that are \emph{sparse} -- for example, the set of (signed) $d {\times} d$ permutation matrices can be characterised by matrices in $\St(d,d)$ that have exactly $d$ non-zero elements, whereas any other element in $\St(d,d)$  that has more than $d$ non-zero elements is not a signed permutation matrix.

The most common approach to obtain sparse solutions in an optimisation problem is to integrate an explicit sparsity-promoting regulariser.
However, a major hurdle when considering quadratic optimisation problems over the Stiefel manifold is that the incorporation of sparsity-promoting terms is not compatible with solutions based on eigenvalue decomposition, so that desirable global optimality guarantees are generally no longer maintained.

Instead, we depart from the common path of integrating explicit regularisers and instead exploit the orthogonal-invariance present in Problem~\eqref{eq:stiefel} to assure sparsity. To be more specific, for any orthogonal matrix $Q \in \OO(d) := \St(d,d)$ it holds that $f(U) = \tr(U^T W U) = \tr(U^T W UQQ^T) = \tr((UQ)^T W (UQ)) = f(UQ)$. Hence, if $\bar{U} \in \St(m,d)$ is a solution to Problem~\eqref{eq:stiefel}, so is $\bar{U}Q \in \St(m,d)$ for any $Q \in \OO(d)$. The subspace $\im(\bar{U}Q)$ that is spanned by $\bar{U}Q$ for a given $\bar{U} \in \St(m,d)$ (and arbitrary $Q \in \OO(d)$) is equal to the subspace $\im(\bar{U})$.
Motivated by this observation  we  utilise orthogonal-invariance in order to find a solution  
%
$U^* \in \{ U \in \St(m,d) : \im(U) = \im(\bar{U})\}$ such that $U^*$ is sparse. To this end, we build upon the additional degrees of freedom due to $Q \in \OO(d)$, which allows to rotate a given solution $\bar{U}\in \St(m,d)$ to a sparser representation $U^* = \bar{U} Q \in \St(m,d)$ -- most notably, while remaining a globally optimal solution to Problem~\eqref{eq:stiefel}.

\textbf{Main contributions.} We summarise our main contributions as follows: (i) For the first time we propose an algorithm that exploits the orthogonal-invariance in quadratic optimisation problems over the Stiefel manifold while simultaneously accounting for sparsity in the solution. (ii) Despite its simplicity, our algorithm is effective as it builds on a modification of the well-established Orthogonal Iteration algorithm for finding the most dominant eigenspace of a given matrix. (iii) Our algorithm is guaranteed to converge to the dominant subspace with the same convergence rate as the original Orthogonal Iteration algorithm, and our solution constitutes a global optimiser of Problem~\eqref{eq:stiefel}. (iv) We experimentally confirm the efficacy of our approach in the context of the permutation synchronisation problem.

\section{Preliminaries \& Related Work}\label{sec:bg}
%
In this section we clarify our assumptions, introduce additional preliminaries, and provide reference to related work.
%
%
Let $\lambda_1, \lambda_2, \ldots, \lambda_m$ be the eigenvalues of $W \in \R^{m \times m}$ ordered decreasingly. We impose the following assumption on $W$:
\begin{assumption}[Separated eigenspace]
We assume that $\lambda_d > \lambda_{d+1}$.
\end{assumption}
Throughout the paper we also assume that $W$ is symmetric and positive semidefinite (p.s.d.), which, however, is not a restriction as the following straightforward result indicates:
\begin{lemma}[Symmetry and positive semidefiniteness]\label{ass:sympsd}
In Problem~\eqref{eq:stiefel}, if  $W$ is not symmetric and not p.s.d.~there is an equivalent optimisation problem (i.e.~with the same optimisers) where $W$ has been replaced by a symmetric p.s.d.~matrix $\tilde W$.
\end{lemma}
\begin{proof}
See Appendix.
\end{proof}

Next, we define the notion of a dominant subspace and convergence of matrix sequences to such a subspace.
\begin{definition}[Dominant invariant subspace]\label{def:domsub}
 The $d$-dimensional \emph{dominant invariant subspace} (or \emph{dominant subspace} in short) of the matrix $W \in \R^{m \times m}$ is defined as the subspace $\im(V_d) \subseteq \R^{m \times m}$, where $V_d \in \St(m,d)$ is the matrix whose columns are formed by the $d$ eigenvectors corresponding to the $d$ largest eigenvalues of $W$. 
\end{definition}

\begin{definition}[Convergence]
We say that a sequence of matrices $\{U_t\}$ converges to the dominant subspace of $W$ if  %
$\lim_{t \rightarrow \infty} \| V_dV_d^TU_t - U_t \| = 0$.
\end{definition}
%

There exists a close relation between the dominant subspace of $W$ and solutions of Problem~\eqref{eq:stiefel}:
%
\begin{lemma}[Solution to Problem~\eqref{eq:stiefel}]\label{lem:solprob}
%
Problem~\eqref{eq:stiefel} is solved for any matrix $U^*$ that forms an orthogonal basis for the $d$-dimensional dominant subspace of $W$,~i.e.~$U^* \in \{ U \in \St(m,d) : \im(U) = \im(V_d)\}$.
\end{lemma}
\begin{proof}
See Appendix.
\end{proof}


\textbf{Basic algorithms for computing eigenvectors.} In order to find the dominant subspace of $W$
%
we can use algorithms for finding eigenvectors. The Power method~\cite{golub2013matrix} is an efficient way for finding the single most dominant eigenvector of a given matrix,~i.e.~it considers the case $d=1$. It proceeds by iteratively updating a given initial $v_0 \in \St(m,1)$ based on the update $v_{t+1} \gets Wv_t/\|W v_t\|$. In order to find the $d$ most dominant eigenvectors, one can consider the Orthogonal Iteration algorithm~\cite{golub2013matrix}, which generalises the Power method to the case $d>1$.
%
The algorithm proceeds by repeatedly computing $V_{t+1}R_{t+1} \gets W V_{t}$ based on the (thin) QR-decomposition of $W V_{t}$, where $V_t \in \St(m,d)$ and $R_t \in \R^{d \times d}$ is upper triangular. For $t \rightarrow \infty$
%
the sequence $\{V_{t}\}$ converges (under mild conditions on the intial $V_0$) to the dominant invariant subspace as long as $|\lambda_d| > |\lambda_{d+1}|$, see Thm.~8.2.2 in~\cite{golub2013matrix}.

%

%

\textbf{Sparse Stiefel optimisation.}
There are numerous approaches for addressing general optimisation problems over the Stiefel manifold, including generic manifold optimisation techniques (e.g.~\cite{absil2009optimization,manopt}), or Stiefel-specific approaches (e.g.~\cite{manton2002optimization,wen2013feasible}). In the following we will focus on works that consider \emph{sparse} optimisation over the Stiefel manifold that are most relevant to our approach.
%
In order to promote sparse solutions, sparsity-inducing regularisers can be utilised, for example via the minimisation of the (non-convex) $\ell_p$-`norm' for $0 \leq p < 1$, or the (convex) $\ell_1$-norm~\cite{qu2016finding}. However, the non-smoothness of such regularisers often constitutes a computational obstacle. 
%
%
 
The optimisation of non-smooth functions over the Stiefel manifold has been considered in~\cite{chen2020proximal}, where the sum of a non-convex smooth and a convex non-smooth function is optimised via a proximal gradient method. In~\cite{li2021weakly}, the authors consider the optimisation of a sum of non-smooth weakly convex functions over the Stiefel manifold using Riemannian subgradient-type methods. Yet, in practice often differentiable surrogates of non-smooth sparsity-promoting terms are considered~\cite{song2015sparse,lu2016convex,qu2020finding,breloy2021majorization}.
%
Instead of minimising $\ell_p$-`norms' with $0 \leq p \leq 1$, on the Stiefel manifold one may instead choose the \emph{maximisation} of $\ell_p$-norms with $p>2$, such as the %
$\ell_3$-norm~\cite{xue2020blind}, or the $\ell_4$-norm~\cite{zhai2020complete}. Further motivation for sparsity promoting higher-order norms in this context can be found in~\cite{qu2019geometric,zhang2019structured,li2018global}. 

%

Optimisation over the Stiefel manifold has a close connection to optimisation over the Grassmannian manifold~\cite{absil2009optimization}. There are numerous approaches for Grassmannian manifold optimisation  (e.g.~\cite{edelman1998geometry,de2004grassmann}), including sparse optimisation via an $\ell_1$-norm regulariser~\cite{wang2017grassmannian} and the optimisation of non-convex and non-smooth objective functions via a projected Riemannian subgradient method~\cite{zhu2019linearly}. In our case, due to the rotation-invariance of the objective $f(U) = f(UQ)$ for any $Q \in \OO(D)$ in Problem~\eqref{eq:stiefel}, finding the dominant subspace of $W$ could also be posed as an optimisation problem over the Grassmannian manifold. However, we are not only interested in identifying this subspace (which can for example be done via the Orthogonal Iterations algorithm~\cite{golub2013matrix}, or via Grassmann-Rayleigh Quotient Iterations~\cite{absil2002grassmann}), but we want to find a specific choice of coordinates for which the representation of the subspace is sparse. 


\section{Proposed Method for Sparse Quadratic Optimisation over the Stiefel}
\begin{algorithm}\label{alg:qrsync}
\SetKwInput{Input}{Input}
\SetKwInput{Output}{Output}
\SetKwInput{Initialise}{Initialise}
\SetKwRepeat{Do}{do}{while}
\DontPrintSemicolon

 \Input{$W \in \R^{m \times m}$, $U_0 \in \R^{m \times d}$, $\epsilon > 0$}
 \Output{$U^*$}
 %
\Initialise{$t \gets 0$}%

  \Repeat{convergence}{ \label{alg:line1} 
    $U_{t+1}R_{t+1} \gets WU_t Z(U_t)$ \tcp{unique QR-decomposition}
   }
    $U^* \gets U_{t+1}$
 \caption{Overview of our proposed algorithm. In terms of convergence properties our algorithm is equivalent to the Orthogonal Iteration algorithm and thus produces a $U^*$ in the dominant subspace of $W$. However, our modification introduces the matrix $Z(U_t)$ with the purpose of promoting sparsity of the solution (see Sec.~\ref{sec:choosingZ} how we choose $Z(U_t)$). }
\end{algorithm}
In this section we introduce our Algorithm~\ref{alg:qrsync} that can be seen as a modification of the popular Orthogonal Iteration algorithm. The main difference is that we iteratively weigh the matrix $U_t \in \St(m,d)$ by a matrix $Z(U_t) \in \R^{d \times d}$, where the matrix function $Z$ maps onto the set of full rank matrices.
%
%
%
%
The purpose of the matrix $Z(U_t)$ (see Sec.~\ref{sec:choosingZ} for our specific choice) is to promote sparsity.
%
Intuitively, we characterise sparsity as having few elements that are large, whereas most elements are close to zero, which is formally expressed in \eqref{eq:sec}.

First, we focus on the overall interpretation of our approach:
%
%
%
%
We want to ensure that we retrieve a $U^*$ that is a global maximiser of Problem~\eqref{eq:stiefel}, which, in some relaxed sense, is also sparse. However, we cannot in general augment the objective function in Problem~\eqref{eq:stiefel} with an explicit sparsity regulariser and expect that the respective solution is still a maximiser of the original objective. Instead we steer the solution towards being more sparse by weighing our  matrix of interest $U_t$ with the matrix $Z(U_t)$. 

\subsection{Convergence}
We start by ensuring that, under the assumption that $Z_t(U_t)$ is full rank, the sequence $\{U_t\}$ generated by Algorithm~\ref{alg:qrsync} converges to the dominant subspace of $W$. This can straightforwardly be shown by using the following result. 

\begin{lemma}\label{lem:101}
Consider the two algorithms:\par
$\qquad$\textbf{Algorithm A: } $V_{t+1}R_{t+1} \gets WV_tX_t$, \par
$\qquad$\textbf{Algorithm B: } $\bar V_{t+1}\bar R_{t+1} \gets  W\bar V_t$, \par
where $X_t \in \mathbb{R}^{d \times d}$ is full rank, and for each of the two left-hand sides above the two matrices in the product are obtained by the unique (thin) QR-factorisation of the corresponding right-hand side, where $R_{t+1}$ and $\bar{R}_{t+1}$  are upper triangular with positive diagonal. Now, if $V_t = \bar V_t \in \St(m,d)$ up to a rotation from the right, then $V_{t'}$ is equal to $\bar V_{t'}$ up to rotation from the right for all $t' > t$.
\end{lemma}

\begin{proof}
Suppose $V_t = \bar V_t \bar Q_t$, where $\bar Q_t \in \OO(d)$. It holds that $V_{t+1}R_{t+1}  = WV_t X_t  = W \bar V_t \bar Q_t X_t = \bar V_{t+1} (\bar R_{t+1} \bar Q_t X_t) = \bar V_{t+1} \tilde{Q}_{t+1}\tilde{R}_{t+1}$,
where $\tilde{Q}_{t+1}$ and $\tilde{R}_{t+1}$ are the two matrices in the QR-factorisation of the matrix $(\bar R_{t+1} \bar Q_t X_t)$. This means that $V_{t+1} = \bar V_{t+1} \tilde{Q}_{t+1}$. This is due to the uniqueness of the QR-factorisation, so that also $R_{t+1} = \tilde{R}_{t+1}$. Now the results follows readily by using induction. 
\end{proof}

%
Thus, if $Z_t(U_t)$ is full rank for each $t$, we can identify our Algorithm~\ref{alg:qrsync} with Algorithm A in Lemma~\ref{lem:101}, and conclude that the sequence $\{U_t\}$ converges to the dominant subspace of $W$ as $t \rightarrow \infty$. This is because Algorithm B in Lemma~\ref{lem:101} corresponds to the Orthogonal Iteration algorithm, which is known to converge to the dominant subspace of $W$~\cite{golub2013matrix}. 
%
%

We continue by investigating the behaviour at the limit where the columns of $U_t$ are in the dominant subspace of $W$. Let us consider the update $U_{t+1}R_{t+1} \gets WU_t Z(U_t)$ of Algorithm~\ref{alg:qrsync},
%
%
%
and further assume that the $d$ largest eigenvalues of $W$ are equal, which is for example the case for synchronisation problems assuming cycle consistency (cf.~Sec.~\ref{sec:permsync}). 
%
%
%
%
\begin{lemma}\label{lem:xxx1}
%
%
Assume that the $d$ largest eigenvalues of the p.s.d. matrix $W$  are all equal to one\footnote{If they are all equal to a value $\lambda_1 \neq 1$, we can w.l.o.g.~consider $\lambda_1^{-1}W$ in place of $W$, since the optimiser of Problem~\eqref{eq:stiefel} is invariant to scaling $W$.} and strictly larger than the other eigenvalues, and assume that the columns of $U_t \in \St(m,d)$ are contained in the dominant subspace of $W$. 
Provided $Z(U_t)$ is full rank, it holds that 
$$WU_tZ(U_t) = U_tZ(U_t) = U_{t+1}R_{t+1} = U_tQ_{t+1}R_{t+1},$$
where $U_{t+1} R_{t+1}$ is the unique QR-factorisation of $U_tZ(U_t)$ and $Q_{t+1} R_{t+1}$ is the unique QR-factorisation of $Z(U_t)$.  
\end{lemma}

\begin{proof}
    Since all the $d$ largest eigenvalues of $W$ are equal to one and the columns of $U_t$ are in the dominant subspace it holds that $WU_t = U_tU_t^TU_t = U_t$, which explains the first equality. While the  second equality follows by definition, the third equality remains to be proven. 
    %
    %
    %
    %
    Let $Q_{t+1}\tilde{R}_{t+1}$ be the unique QR-decomposition of $Z(U_t)$.
    We want to show that $U_{t+1} = U_t Q_{t+1}$ and $R_{t+1} = \tilde{R}_{t+1}$.
    From the QR-decomposition of $Z(U_t)$ it follows that $U_t Z(U_t) = U_t  Q_{t+1}\tilde{R}_{t+1}$, while from the QR-decomposition of $U_t Z(U_t)$ it follows that $U_t Z(U_t) = U_{t+1}R_{t+1}$. The uniqueness of the QR-decomposition implies $U_t  Q_{t+1} = U_{t+1}$ and $\tilde{R}_{t+1} = R_{t+1}$.
    %
    %
    %
    %
\end{proof}
%
%
Hence, under the assumptions in Lemma~\ref{lem:xxx1}, the columns of $U_{t'}$ span the dominant subspace of $W$ for any $t' \geq t$. %
The update in  Algorithm~\ref{alg:qrsync} reduces to the  form
\begin{align}
\label{eq:nisse:1}
Q_{t+1}R_{t+1} & \gets Z(U_t),\\
    U_{t+1} & \gets U_{t}Q_{t+1}, \label{eq:nisse:2}
    %
\end{align}
%
i.e.~updating $U_t$ to obtain $U_{t+1}$ simplifies to multiplication of $U_t$ with the $Q$-matrix from the QR-factorisation of $Z(U_t)$. 


\subsection{Choosing $Z(U_t)$ to Promote Sparsity}\label{sec:choosingZ}

%
%

We now turn our attention to the choice of $Z(U_t)$ in  Algorithm~\ref{alg:qrsync}. We know that, as long as $Z(U_t)$ is full rank in each iteration $t$, the columns of $U_t$ converge to the dominant subspace of $W$. In addition to our objective function $f$ in Problem~\eqref{eq:stiefel}, we now introduce the secondary objective
\begin{align}\label{eq:sec}
    g(U) = \sum_{i=1}^m \sum_{j=1}^d (U_{ij})^p =  \tr(U^T(U.^{\wedge (p-1)})),
\end{align}
where for $p$ being a positive integer the notation $U.^{\wedge p}$ means to raise each element in $U$ to the power $p$. For $U \in \St(m,d)$ and $p$ larger than $2$, the maximisation of $g$ promotes \emph{sparsity in a relaxed sense}. To be specific, if $p$ is odd,  a few larger elements (with value close to 1) lead to a larger value of $g$ compared to many smaller elements (with value close to 0), so that sparsity and non-negativity are simultaneously promoted. Analogously, if $p$ is even, a few elements with value closer to $\pm 1$ lead to a larger value of $g$ compared to many smaller elements close to $0$.




Let us consider the maximisation of 
\begin{align}
    g(UQ) = \tr(h(U,Q))
\end{align}
with respect to $Q$, where 
$h(U,Q) = (UQ)^T((UQ).^{\wedge (p-1)}),$
$Q \in \mathbb{O}^d$, and $U \in \St(m,d)$. This means that we want to rotate $U$ by $Q$ in such a way that the secondary objective $g$ is maximised after the rotation. By applying the rotation from the right, we ensure that the columns $U$ span the same space after the rotation, but a higher objective value is achieved for the secondary objective $g$ in~\eqref{eq:sec}.

%
%
%
%
The extrinsic gradient with respect to $Q$ (for $Q$ relaxed to be in $\R^{d \times d}$) of $g(UQ)$ at $Q = \matI$ is $ p{\cdot}h(U,\matI)$. 
Thus the (manifold) gradient of $g(UQ)$ at $Q = \matI$ is given by the projection of the extrinsic gradient at $Q = \matI$ onto the tangent space, which reads
\begin{align}
    \nabla_Q g(UQ)|_{Q = \matI} = p(h(U,\matI) - h^T(U,\matI)).
\end{align} 
For a small step size $\alpha$, we can perform a first-order approximation of gradient ascent if we choose $Z = \matI + \alpha p(h(U,\matI) - h^T(U,\matI))$, and then update $U$ in terms of the Q-matrix of the QR-factorisation of $Z$. 
Now, let us assume that we investigate the behaviour at the limit under the assumptions in Lemma~\ref{lem:xxx1},~i.e.~that the $d$ largest eigenvalues of $W$ are equal. In this context the Q-matrix of the QR-factorisation is a retraction~\cite{absil2009optimization} 
and serves as a first-order approximation of the exponential map. 
%
%
With that, the updates become
\begin{align}
    Q_{t+1}R_{t+1} &\gets \matI + \alpha_t (h(U_t,\matI) - h^T(U_t,\matI)),\label{eq:mangradasc}\\
     U_{t+1} &\gets U_t Q_{t+1}.
\end{align}
%
%
This choice of the matrix $Z(U_t)$ ensures that it is full rank, since adding the identity matrix and a skew-symmetric matrix results in a full rank matrix. Hence, for sufficiently small step size $\alpha$, we iteratively move in an ascent direction on $\mathbb{O}^d$ by utilising the QR-retraction.


\section{Application to Permutation Synchronisation}\label{sec:permsync}
\subsection{Permutation Synchronisation in a Nutshell} Permutation synchronisation is a procedure to improve matchings between multiple objects~\cite{Huang:2013uk,Pachauri:2013wx}, and related concepts have been utilised to address diverse tasks, such as multi-alignment~\cite{bernard2015solution,arrigoni2016spectral,huang2019learning}, multi-shape matching~\cite{huang2019tensor,huang2020consistent,gao2021isometric}, multi-image matching~\cite{zhou2015multi,tron2017fast,bernard2019hippi,birdal2019probabilistic,birdal2021quantum}, or multi-graph matching~\cite{yan2016short,bernard:2018,swoboda2019convex}, among many others. Permutation synchronisation refers to the process of establishing cycle consistency in the set of pairwise permutation matrices that encode correspondences between points in multiple objects. In computer vision there is a typical application where the points are feature descriptors or key-points, and the objects are images, as shown in Fig.~\ref{fig:house}. 

Let $k$ denote the number of objects, where each object $i$ contains $m_i$ points. For $\onevec_p$ being a $p$-dimensional vector of all ones, and vector inequalities being understood in an element-wise sense, let $P_{ij} \in \perm_{m_im_j} := \{ X \in \{0,1\}^{m_i \times m_j}: X \onevec_{m_j} \leq \onevec_{m_i}, \onevec_{m_i}^T X \leq \onevec_{m_j}^T\}$ be the partial permutation matrix that represents the correspondence between the $m_i$ points in object $i$ and the $m_j$ points in object $j$. In the case of \emph{bijective} permutations, the set of pairwise permutations $\mathcal{P} := \{P_{ij}\}_{i,j=1}^k$ is said to be cycle-consistent if for all $i,j,\ell$ it holds that $P_{i\ell} P_{\ell j} = P_{ij}$. 

We define the set of %
partial permutation matrices with full row-rank as $\overline{\perm}_{m_i d} := \{ X \in \perm_{m_id}: X \onevec_{d} = \onevec_{m_i}\}$, where $d$ denotes the total number of distinct points across all objects. Cycle consistency is known to be equivalent to the existence of so-called object-to-universe matchings $\mathcal{U} := \{P_i \in \overline{\perm}_{m_i d}\}_{i=1}^k$ such that for all $i,j$ we can write $P_{ij} = P_i P_j^T$ (see~\cite{Huang:2013uk,Pachauri:2013wx} for details).
%
The object-to-universe characterisation of cycle consistency is also valid for the case of non-bijective (i.e.~partial) permutations (see~\cite{tron2017fast,bernard2019synchronisation} for details).

Given the noisy (i.e.~not cycle-consistent) set of pairwise permutations $\mathcal{P} = \{P_{ij}\}_{i,j=1}^k$, permutation synchronisation can be phrased as the optimisation problem
\begin{align}\label{eq:permsync}
    \argmax_{\{P_i \in \overline{\perm}_{m_i d}\}}~\sum_{i,j} \tr(P_{ij}^T P_i P_j^T) \quad \Leftrightarrow \quad
    \argmax_{P \in \mathbb{U}}~\tr(P^T W P)\,,
\end{align}
where for $m := \sum_i m_i$  we define the set $\mathbb{U} := \overline{\perm}_{m_1d} \times \ldots \times \overline{\perm}_{m_kd} \subset \R^{m \times d}$, the $(m {\times} d)$-dimensional block matrix $P = [P_1^T,\ldots,P_k^T]^T$, and the block matrix $W := [P_{ij}]_{ij} \in \R^{m \times m}$ in order to allow for a compact matrix representation of the objective.
%

%

\subsection{Proposed Permutation Synchronisation Approach} The core idea of existing spectral permutation synchronisation approaches~\cite{Pachauri:2013wx,shen2016normalized,Maset:YO8y6VRb} is to replace the feasible set $\mathbb{U}$ in Problem~\eqref{eq:permsync} with the Stiefel manifold $\St(m,d)$, so that we obtain an instance of Problem~\eqref{eq:stiefel}. We utilise Algorithm~\ref{alg:qrsync} to obtain a (globally optimal) solution $U^* \in \St(m,d)$ of this spectral formulation. In our algorithm we choose $p=3$ to promote sparsity and non-negativity  in the resulting $U^*$ via the function $g(U) = \tr(U^T(U.^{\wedge 2})) = \sum_{i=1}^m \sum_{j=1}^d U_{ij}^3$. With that, in addition to $U^*$ being an orthogonal matrix, it contains few large elements that are close to 1 and many smaller elements that are close to 0. As such, we can readily project the matrix $U^*$ onto the set $\mathbb{U}$ in terms of a Euclidean projection. The Euclidean projection is given by projecting each of the $k$ blocks of $U^* = [U_1^{*T}, \ldots, U_k^{*T}]^T$ individually onto the set of %
partial permutations,~i.e.~$P_i = \proj_{\overline{\perm}_{m_id}}(U_i^*) = \argmax_{X \in \overline{\perm}_{m_id}} \tr(X^T U_i^*)$, see e.g.~\cite{bernard2019synchronisation},
which amounts to a (partial) linear assignment problem~\cite{Munkres:1957ju,Bertsekas:1998vt} that we solve based on the efficient implementation in~\cite{bernard2016fast}.
%



\subsection{Experimental Results}\label{sec:exp}
We experimentally compare our proposed approach with various methods for permutation synchronisation and perform an evaluation on both real and synthetic datasets. In particular, our comparison includes two existing spectral approaches, namely \textsc{MatchEig}~\cite{Maset:YO8y6VRb} and \textsc{Spectral}~\cite{Pachauri:2013wx}, where for the latter we use the efficient implementation from the authors of~\cite{zhou2015multi}. In addition, we also compare against the alternating minimisation method \textsc{MatchALS}~\cite{zhou2015multi}, and against the non-negative matrix factorisation approach \textsc{NmfSync}~\cite{bernard2019synchronisation}. To emphasise that the methods \textsc{MatchEig} and \textsc{MatchALS} do not guarantee cycle consistency (but instead  aim to improve the initial matchings), in all plots we show  results of respective methods as dashed lines.
We use the fscore to measure the quality of obtained multi-matchings, which is defined as the fraction $f = \frac{2 \cdot p \cdot r}{p + r}$, where $p$ and $r$ denote the precision and recall, respectively. All experiments are run on a Macbook Pro (2.8 GHz quad core i7, 16 GB RAM), where for $\epsilon = 10^{-5}$ we use $f(U_t)/f(U_{t+1})  \geq 1-\epsilon$  as convergence criterion in Algorithm~\ref{alg:qrsync}, and a step size of $\alpha_t = \| h(U_t,\matI) - h^T(U_t,\matI)\|_{\infty}^{-1}$ in~\eqref{eq:mangradasc}.


\textbf{Real data.} In this experiment we use the CMU house image sequence~\cite{cmuHouse} comprising $111$ frames within the  
experimental protocol of~\cite{Pachauri:2013wx}.
We generate a sequence of permutation synchronisation problem instances with a gradually increasing number of objects $k$ by extracting respective pairwise matchings for $k$ objects. To this end, we vary $k$ from $20$ to $111$ and sample the pairwise matchings evenly-spaced from the $111 \times 111$ pairwise matchings.
%
Quantitative results in terms of the fscore, the objective value of Problem~\eqref{eq:permsync}, and the runtimes are shown in Fig.~\ref{fig:housequant}, in which the individual problem instances vary along the horizontal axis. We can see that our proposed method dominates other approaches in terms of the fscore and the objective value (the reported objective values are divided by $k^2$ to normalise the scale in problems of different sizes), while being among the fastest. Note that we do not report the objective value for $\textsc{MatchEig}$ and $\textsc{MatchALS}$, since they do not lead to cycle-consistent matchings so that the obtained solution does not lie in the feasible set  of Problem~\eqref{eq:permsync}.
Qualitative results of the matching between one pair of images for $k=111$ are shown in Fig.~\ref{fig:house}.  As expected, our proposed approach clearly outperforms the \textsc{Spectral} baseline, since our  method is guaranteed to  converge to the same same subspace that is spanned by the spectral solution, while at the same time providing a sparser and less negative solution that is thereby closer to the feasible set $\mathbb{U}$.
%
%
\begin{figure*}[t]
\centering
%
    \centerline{\includegraphics[width=7.5cm]{images/sparseStiefel_synthetic_legend.pdf}}
     \centerline{
    \begin{tabular}{ccc}%
    %
     \includegraphics[width=0.33\linewidth]{images/house_fscore.pdf}  & \includegraphics[width=0.33\linewidth]{images/house_objectives.pdf} & \includegraphics[width=0.33\linewidth]{images/house_times.pdf}%
    %
    \end{tabular}
    }
    \caption{Quantitative results on the CMU house sequence in terms of the fscore ($\uparrow$), objective value of Problem~\eqref{eq:permsync} ($\uparrow$), and runtime ($\downarrow$). The individual instances of permutation synchronisation problems vary along the horizontal axis. Methods that do not guarantee cycle consistency are shown as dashed lines.}
    \label{fig:housequant} 
\end{figure*}


%
\newcommand{\figScale}{0.47\linewidth}
\begin{figure}[ht!] 
\centering
    %
    \begin{tabular}{cc}%
     \textsc{Input} & \textsc{MatchEig}\\
     \includegraphics[width=\figScale]{images/house_in_1_111.png} & \includegraphics[width=\figScale]{images/house_MatchEig_1_111.png}\\
     \textsc{MatchALS}& \textsc{Spectral}\\
     \includegraphics[width=\figScale]{images/house_MatchALS_1_111.png} & \includegraphics[width=\figScale]{images/house_Spectral_1_111.png}\\
     \textsc{NmfSync} & \textsc{Ours}\\
     \includegraphics[width=\figScale]{images/house_NmfSync_1_111.png} & \includegraphics[width=\figScale]{images/house_Ours_1_111.png}%
    \end{tabular}
    %
    %
    %
    %
    %
    %
    \caption{Comparison of  matchings between the first and last image of the CMU house sequence obtained by several methods.
    %
    The colour of the dots indicates the ground truth correspondence, and the lines show the obtained matchings (green:~correct, red:~wrong). Overall, our approach obtains the best matchings, see also Fig.~\ref{fig:housequant}.}
    %
    \label{fig:house}
\end{figure} 



\textbf{Synthetic data.} We reproduce the procedure described  in~\cite{bernard2019synchronisation} for generating synthetic instances for the synchronisation of partial permutations. Four different parameters are considered for generating a problem instance: the universe size $d$, the number of objects $k$ that are to be matched, the observation rate $\rho$, and the error rate $\sigma$ (see~\cite{bernard2019synchronisation} for details).  %
One of these parameters varies in each experimental setting, while the others are kept fixed. Each individual experiment is repeated $5$ times with different random seeds. In Fig.~\ref{fig:synthetic} we compare the performance of \textsc{MatchEig}~\cite{Maset:YO8y6VRb}, \textsc{MatchALS}~\cite{zhou2015multi}, \textsc{Spectral}~\cite{Pachauri:2013wx}, \textsc{NmfSync}~\cite{bernard2019synchronisation} and \textsc{Ours}.
%
The first row shows the fscore, and the second row the respective runtimes. Note that we did not run \textsc{MatchALS} on the larger instances since the runtime is prohibitively long. The methods \textsc{MatchEig} and \textsc{MatchALS} do not guarantee cycle consistency, and are thus shown as dashed lines. It can be seen that in most settings our method obtains superior performance in terms of the fscore, while being almost as fast as the most efficient methods that are also based on a spectral relaxation (\textsc{MatchEig} and \textsc{Spectral}).
%
\newcommand{\figWidth}{3.7cm}
\begin{figure}[ht!] 
    \centerline{\includegraphics[width=7.5cm]{images/sparseStiefel_synthetic_legend.pdf}}
     \centerline{ 
        \subfigure{\includegraphics[width=\figWidth]{images/sparseStiefel_synthetic_nViews_fscore.pdf}}%
        \subfigure{\includegraphics[width=\figWidth]{images/sparseStiefel_synthetic_errorRate_fscore.pdf}}%
        \subfigure{\includegraphics[width=\figWidth]{images/sparseStiefel_synthetic_observationRate_fscore.pdf}}%
        \subfigure{\includegraphics[width=\figWidth]{images/sparseStiefel_synthetic_universeSize_fscore.pdf}}%
      }%
    \centerline{%
        \subfigure{\includegraphics[width=\figWidth]{images/sparseStiefel_synthetic_nViews_time.pdf}}%
        \subfigure{\includegraphics[width=\figWidth]{images/sparseStiefel_synthetic_errorRate_time.pdf}}%
        \subfigure{\includegraphics[width=\figWidth]{images/sparseStiefel_synthetic_observationRate_time.pdf}}%
        \subfigure{\includegraphics[width=\figWidth]{images/sparseStiefel_synthetic_universeSize_time.pdf}}%
      }%
    \caption{Experimental comparison of permutation synchronisation methods on synthetic data. Each column shows a different varying parameter. The first row shows the fscore ($\uparrow$) and the second row the runtime ($\downarrow$). Methods that do not guarantee cycle consistency are shown with dashed lines. Our method guarantees cycle consistency and leads to higher fscores in most settings.} 
    \label{fig:synthetic}
\end{figure} 




\section{Discussion and Limitations}\label{sec:lim}
Our algorithm has several favourable properties: it finds a globally optimal solution to Problem~\eqref{eq:stiefel}, it is computationally efficient, it has the same convergence behaviour as the Orthogonal Iteration algorithm~\cite{golub2013matrix}, and promotes (approximately) sparse solutions. Naturally, since our solution is globally optimal (with respect to Problem~\eqref{eq:stiefel}) and thus converges to $\im(V_d)$, the amount of sparsity that we can achieve is limited by the sparsest orthogonal basis that spans $\im(V_d)$.  In turn, we rather interpret sparsity in some looser sense, meaning that there are few elements that are large, whereas most elements are close to zero. Furthermore, since the secondary sparsity-promoting problem (with objective in~\eqref{eq:sec}) is generally non-convex, we cannot guarantee that we attain its global optimum.

Since permutation synchronisation was our primary motivation it is the main focus of this paper. In the case of permutations, the assumption that the eigenvalues corresponding to the $V_d$ are equal (as we explain in Sec.~\ref{sec:choosingZ}) is reasonable, since this must hold for cycle-consistent bijective matchings. However, broadening the scope of our approach to other problems in which this assumption may not be valid will require further theoretical analysis and possibly a different strategy for choosing the matrix $Z(U_t)$. Studying the universality of the proposed method, as well as analysing different convergence criteria and different matrix functions $Z(U_t)$ are open problems that we leave for future work.

The proposed method comprises a variation of a well-known iterative procedure for computing the dominant subspace of a matrix. The key component in this procedure is the QR-factorisation, which is differentiable almost everywhere and is therefore well-suited to  be applied within a differentiable programming context (e.g.~for end-to-end training of neural networks). 

\section{Conclusion}
%
We propose an efficient algorithm to find a matrix that forms an orthogonal basis of the dominant subspace of a given matrix $W$ while additionally promoting sparsity and non-negativity.
%
%
%
Our procedure inherits favourable properties from the Orthogonal Iteration algorithm, namely it is simple to implement, converges almost everywhere, and is computationally efficient. Moreover, our method is designed to generate sparser solutions compared to the Orthogonal Iteration algorithm. This is achieved by rotating our matrix of interest in each iteration by a matrix corresponding to the gradient of a secondary objective that promotes sparsity (and optionally non-negativity).
%

The considered problem setting is relevant for various spectral formulations of difficult combinatorial problems, such as they occur in applications like multi-matching, permutation synchronisation or assignment problems. Here, the combination of orthogonality, sparsity and non-negativity is desirable, since these are properties that characterise binary matrices such as permutation matrices.
%
%
%
Experimentally we show that our proposed method outperforms existing methods in the context of partial permutation synchronisation, while at the same time having favourable a runtime. 


 
\subsection*{Broader Impact}\label{sec:impact}
The key contribution of this paper is an effective and efficient optimisation algorithm for addressing sparse optimisation problems over the Stiefel manifold. Given the fundamental nature of our contribution, we do not see any direct ethical concerns or negative societal impacts related to our work.

Overall, there are numerous opportunities to use the proposed method for various types of multi-matching problems over networks and graphs.
%
The problem addressed is highly relevant within the fields of machine learning (e.g.~for data canonicalisation to faciliate efficient learning with non-Euclidean data), computer vision (e.g.~for 3D reconstruction in structure from motion, or image alignment), computer graphics (e.g.~for bringing 3D shapes into correspondence) and other related areas. 

The true power of permutation synchronisation methods appears when the number of objects to synchronise is large.
%
With large quantities of data
%
becoming increasingly available,
%
the introduction of our scalable optimisation procedure
that can account for orthogonality -- while promoting sparsity and non-negativity -- is  an important contribution to the synchronisation community on the one hand. Moreover, on the other hand,  it has the potential to have an impact on more general non-convex optimisation problems, particularly in the context of spectral relaxations of difficult and large combinatorial problems.


\subsection*{Acknowledgement}
JT was supported by the Swedish Research Council (2019-04769).
%


{
\small
\bibliographystyle{ieee}
\bibliography{references}
}

\clearpage
\appendix

% Define chapter number
\newcommand{\snum}{A}

% Prepend supplemental chapter to equation list
\renewcommand{\theequation}{\snum\arabic{equation}}

\section{Proofs}
\subsection*{Proof of Lemma 1}
\begin{proof}
If the matrix $W$ is not symmetric, we can split $W$ into the sum of a symmetric part $W_s$ and skew-symmetric part $W_a$. It holds that $\tr(U^T W_a U) = \frac{1}{2}\tr(U^T W_a U)  + \frac{1}{2}\tr(U^T W_a^T U) =  \frac{1}{2}\tr(U^T W_a U)   -\frac{1}{2}\tr(U^T W_a U) = 0$. Further, if $W_s$ is not positive semidefinite, we can shift its eigenvalues via  $W_s {-} \lambda_{m}\matI_m = \tilde{W}$ to make it p.s.d. Since $\alpha \tr(U^T \matI_m U) = \alpha d$ is constant for any scalar $\alpha$, the term $\lambda_{m}\matI_m$ does not affect the optimisers of Problem~\eqref{eq:stiefel}. Thus,  we can replace $W$ in~\eqref{eq:stiefel} by the p.s.d. matrix $\tilde W$ without affecting the optimisers. 
\end{proof}


\subsection*{Proof of Lemma 2}

%
\begin{proof}
Consider the eigenvalue decomposition $(\Lambda, V)$ of $W$, i.e.~$W = V\Lambda V^T$, where $\Lambda = \diag(\lambda_1,\ldots,\lambda_m)$ contains the decreasingly ordered (nonnegative) eigenvalues on its diagonal and $V \in \St(m,m)$. For $U \in \St(m,d)$, this matrix can be written as the product of $V$ and another matrix $R \in \St(m,d)$, i.e., $U = VR$.
%
%
So, instead of optimising over $U$, we can optimise over $R$. Let the $i$-th row of $R$ be denoted as $r_i$ for $i = 1,2, \ldots, m$.  
It holds that $\tr(U^TWU) =  \tr(R^T\Lambda R) = \sum_{i=1}^m\lambda_i\|r_i\|_{2}^2$.
%
%
%
Thus, an equivalent formulation of Problem~\eqref{eq:stiefel} is the  optimisation problem 
\begin{align}
    \label{eq:1}
    & \max_{R = [r_1^T, r_2^T, \ldots, r_m^T]^T \in \St(m,d)} ~\sum_{i=1}^m\lambda_i\|r_i\|_{2}^2.
\end{align}
%
We observe that $0 \leq \|r_i\|_{2}^2 \leq 1$, and that $\sum_{i=1}^m\|r_i\|_{2}^2 = \tr(RR^T) = \tr(R^TR) = d$.  Hence, a relaxation to \eqref{eq:1} is given by
\begin{align}
    \label{eq:2}
    & \max_{p \in \R^{m}} ~\sum_{i=1}^m\lambda_ip_i, \\
   & ~~~\text{s.t.} \quad  0 \leq p_i \leq 1, \\
   \label{eq:4}
    &\quad \quad   \sum_{i=1}^m p_i = d.
\end{align}
This is a linear programming problem for which an optimal solution is given by $p_1 = \ldots = p_d = 1$ and $p_{d+1} = \ldots = p_m = 0$  (since the $\lambda_i$'s are provided in decreasing order).
%
%
Now we choose $R^* = [\matI_d, \mathbf{0}]^T \in \R^{m \times d}$, so that
\begin{equation}
\label{eq:5}
    r_1^* = e_1^T, r_2^* = e_2^T, \ldots, r_d^* = e_d^T \quad \text{and} \quad r_{d+1}^* = \ldots = r_{m}^* = \zerovec^T_d,
\end{equation}
%
where $e_i \in \R^{d}$ is the unit vector with element equal to $1$ at the $i$-th place.
%
We observe that for this choice $R^* \in \St(m,d)$, and that the objective value for \eqref{eq:1} is the same as the optimal value for the problem defined by \eqref{eq:2}-\eqref{eq:4}. Since the latter problem was a relaxation of the former problem, $R^*$ is an optimal solution to Problem~\eqref{eq:1}. 
%
%
%
%
The corresponding optimal $U$ for Problem~\eqref{eq:stiefel} is $U^* = VR^* = V_d$. The observation that for any $U' = V_d Q$ with $Q \in \OO(d)$  we have that $\tr(U'^T W U') = \tr(Q^T V_d^T W V_d Q) = \tr(V_d^T W V_d)$ concludes the proof.
%
%
\end{proof}


\section{Additional Experiments}
In the following we provide further evaluations of our proposed algorithm.

\subsection{Step Size and Comparison to Two-Stage Approaches}
%
In this section, on the one hand we experimentally confirm that our  approach of choosing the step size $\alpha_t$ (see Sec.~\ref{sec:exp}) is valid  and that
%
in practice
it is not necessary to perform line search. On the other hand, 
%
%
we verify that our 
%
proposed algorithm leads to results that are comparable to two-stage approaches derived from Lemma~\ref{lem:xxx1}. Such two-stage approaches
%
first determine the matrix $U_0 \in \im(V_d)$ that spans the $d$-dimensional dominant subspace of $W$, and subsequently utilise the updates in equations~\eqref{eq:nisse:1} and~\eqref{eq:nisse:2} in order to make $U_0$ sparser.
As explained, this corresponds to finding a matrix $Q \in \OO(d)$ 
that maximises our secondary objective 
\begin{align}\label{eq:secapp}
    g(U_0Q) = \sum_{i=1}^m \sum_{j=1}^d (U_0Q)_{ij}^p\,.
\end{align}
%
We compare our proposed algorithm to two different settings of  two-stage approaches:
%
%
\begin{enumerate}
    \item \textbf{Our algorithm as stage two.} Our proposed algorithm forms the second stage of a two-stage approach. To this end, in the first stage we use the Orthogonal Iteration algorithm~\cite{golub2013matrix} to find the matrix $V_d$ that spans the $d$-dimensional dominant subspace of $W$. Subsequently in the second stage, we initialise $U_0 \gets V_d$, and according to Lemma~\ref{lem:xxx1} we make use of the updates in equations~\eqref{eq:nisse:1} and~\eqref{eq:nisse:2} in order to make $U_t$ iteratively sparser. We consider two variants for the second stage: 
    \begin{enumerate}
        \item In the variant  denoted \textsc{Ours/2-stage} we run the second-stage updates exactly for the  number of iterations that the Orthogonal Iteration required in the first stage to find $V_d$ (with convergence threshold $\epsilon=10^{-5}$). We use the step size $\alpha_t$ as described in Sec.~\ref{sec:exp}. 
        \item  In the variant  denoted \textsc{Ours/2-stage/bt} we utilise  backtracking line search (as implemented in the ManOpt toolbox~\cite{manopt}) in order to find a suitable step size $\alpha_t$ in each iteration. Here, we run the algorithm until convergence w.r.t.~to $g$,~i.e.~until $g(U_tQ_t)/g(U_{t+1}Q_{t+1})\geq 1 - \epsilon$ for $\epsilon = 10^{-5}$.
    \end{enumerate}
    
    \item \textbf{Manifold optimisation as stage two.} Further, we consider the trust regions method~\cite{absil2007trust} to find a (local) maximiser of~\eqref{eq:secapp} in the second stage. Here, the optimisation over the Riemannian manifold $\OO(d)$ is performed using the ManOpt toolbox~\cite{manopt}. For the first stage, we consider three different initialisations for finding the matrix $V_d$ that spans the $d$-dimensional dominant subspace of $W$:  the Matlab functions \textit{eig()} and \textit{eigs()}, as well as our  implementation of the Orthogonal Iteration algorithm~\cite{golub2013matrix}. We call these methods \textsc{Eig+ManOpt}, \textsc{Eigs+ManOpt} and \textsc{OrthIt+ManOpt}, respectively.
    
    
    
    
   
\end{enumerate}

Results are shown in Figs.~\ref{fig:housequantapp} and ~\ref{fig:ablationapp} for the CMU house sequence and the synthetic dataset, respectively. We observe the following:
\begin{itemize}
    \item In terms of \textbf{solution quality} (fscore and objective), all considered methods are comparable in most cases. For the real dataset (Fig.~\ref{fig:housequantapp}) \textsc{Eigs+ManOpt} performs worse due to numerical reasons. For the largest considered permutation synchronisation problems (the right-most column in the synthetic data setting shown in Fig.~\ref{fig:ablationapp}) \textsc{Ours} leads to the best results on average.
    %
    %
    %
    \item In terms of \textbf{runtime}, in overall \textsc{Ours} is among the fastest, considering both the real and the synthetic data experiments. In the real dataset, where $d=30$ is relatively small,
    \textsc{Eigs+ManOpt} is the fastest (but with poor solution quality), while \textsc{Eig+ManOpt} is the slowest (with comparable solution quality to \textsc{Ours}). 
    Methods that utilise the Orthogonal Iteration  have  comparable runtimes in the real data experiments. 
    
    Most notably, in the largest considered synthetic data setting (right-most column in Fig.~\ref{fig:ablationapp}) \textsc{Ours} is among the fastest (together with \textsc{Ours/2-stage}), while \textsc{Ours} has the largest fscore on average (as mentioned above) -- this indicates that \textsc{Ours} is particularly well-suited for permutation synchronisation problems with increasing size.
    \item Overall \textsc{Ours} is the simplest method, see Algorithm~\ref{alg:qrsync}: the solution is computed in one single stage rather than in two consecutive stages, and it does not require \textbf{line search}, as can be seen by comparing \textsc{Ours} with \textsc{Ours/2-stage/bt} across all experiments.
    %
\end{itemize}


%

\begin{figure*}[t]
\centering
%
    \centerline{\includegraphics[width=0.9\linewidth]{images_appendix/sparseStiefel_synthetic_ablation_legend.pdf}}%
     \centerline{
    \begin{tabular}{ccc}%
    %
     \includegraphics[width=0.33\linewidth]{images_appendix/house_ablation_fscore.pdf}  & \includegraphics[width=0.33\linewidth]{images_appendix/house_ablation_objectives.pdf} & \includegraphics[width=0.33\linewidth]{images_appendix/house_ablation_times.pdf}%
    %
    \end{tabular}
    }
    \caption{Comparison of \textsc{Ours} to different two-stage approaches on permutation synchronisation problems from the CMU house sequence (see Sec.~\ref{sec:exp} for details). We consider the fscore ($\uparrow$), objective value ($\uparrow$), and runtime ($\downarrow$). The individual instances of permutation synchronisation problems vary along the horizontal axis. }
    \label{fig:housequantapp} 
\end{figure*}


%
\begin{figure}[ht!] 
    \centerline{\includegraphics[width=0.9\linewidth]{images_appendix/sparseStiefel_synthetic_ablation_legend.pdf}}
     \centerline{ 
        \subfigure{\includegraphics[width=\figWidth]{images_appendix/sparseStiefel_synthetic_ablation_nViews_fscore.pdf}}%
        \subfigure{\includegraphics[width=\figWidth]{images_appendix/sparseStiefel_synthetic_ablation_errorRate_fscore.pdf}}%
        \subfigure{\includegraphics[width=\figWidth]{images_appendix/sparseStiefel_synthetic_ablation_observationRate_fscore.pdf}}%
        \subfigure{\includegraphics[width=\figWidth]{images_appendix/sparseStiefel_synthetic_ablation_universeSize_fscore.pdf}}%
      }%
    \centerline{%
        \subfigure{\includegraphics[width=\figWidth]{images_appendix/sparseStiefel_synthetic_ablation_nViews_time.pdf}}%
        \subfigure{\includegraphics[width=\figWidth]{images_appendix/sparseStiefel_synthetic_ablation_errorRate_time.pdf}}%
        \subfigure{\includegraphics[width=\figWidth]{images_appendix/sparseStiefel_synthetic_ablation_observationRate_time.pdf}}%
        \subfigure{\includegraphics[width=\figWidth]{images_appendix/sparseStiefel_synthetic_ablation_universeSize_time.pdf}}%
      }%
    \caption{Comparison of \textsc{Ours} to different two-stage approaches on synthetic permutation synchronisation problems (see Sec.~\ref{sec:exp} for details).
    Each column shows a different varying parameter. The first row shows the fscore ($\uparrow$) and the second row the runtime ($\downarrow$). Note that  the right-most column shows the largest considered permutation synchronisation instances -- for these \textsc{Ours} obtains the best fscore while being among the fastest (together with \textsc{Ours/2-stage}).} 
    \label{fig:ablationapp}
\end{figure} 

%

%



\subsection{Comparison to Riemannian Subgradient and Evaluation of  Different $p$}
In Fig.~\ref{fig:housequant2} we compare \textsc{Ours} with $p=3$ and $p=4$ to the Riemannian subgradient-type method (with QR-retraction) by Li et~al.~\cite{li2021weakly} with $\ell_1$-norm as sparsity-inducing penalty. In the qualitative results (bottom) we can observe that the Riemannian subgradient-type method and Ours ($p=4$) obtain sparse solutions with few elements with large absolute values (both positive and negative), whereas Ours ($p=3$) obtains a sparse and (mostly) nonnegative solution. Since for permutation synchronisation we are interested in nonnegative solutions, \textsc{Ours} with $p=3$ thus outperforms the two alternatives quantitatively (top).
\begin{figure*}[t!]
\centering
%
    \centerline{\includegraphics[width=0.65\linewidth]{images_appendix/house_ablation2_legend.pdf}}%
     \centerline{
    \begin{tabular}{ccc}%
    %
     \includegraphics[width=0.33\linewidth]{images_appendix/house_ablation2_fscore.pdf}  & \includegraphics[width=0.33\linewidth]{images_appendix/house_ablation2_objectives.pdf} & \includegraphics[width=0.33\linewidth]{images_appendix/house_ablation2_times.pdf}%
     \\ &&\\
      \includegraphics[width=0.32\linewidth]{images_appendix/house_ablation2_RiemannianSubgrad.png}  & \includegraphics[width=0.32\linewidth]{images_appendix/house_ablation2_Ours_p_4.png} 
      & \includegraphics[width=0.32\linewidth]{images_appendix/house_ablation2_Ours_p_3.png} 
    %
    \end{tabular}
    }
    \caption{Comparison of \textsc{Ours} (with $p=3$ and $p=4$)  to the Riemannian subgradient-type method by Li et~al.~\cite{li2021weakly}.  Here, permutation synchronisation problems from the CMU house sequence (see Sec.~\ref{sec:exp} for details) are evaluated. \textbf{Top:} we consider the fscore ($\uparrow$), objective value ($\uparrow$), and runtime ($\downarrow$), where the individual instances of permutation synchronisation problems vary along the horizontal axis. \textbf{Bottom:} for each of the three methods we show the obtained $U$-matrix for $k=20$  (before projection). It can be seen that the Riemannian subgradient-type method and Ours ($p=4$) obtain sparse solutions with few elements with large absolute values (both positive and negative), whereas Ours ($p=3$) obtains a sparse and (mostly) nonnegative solution. }
    \label{fig:housequant2} 
\end{figure*}



\subsection{Effect of Sparsity-Promoting Secondary Objective}
In Fig.~\ref{fig:sparsityQual} we illustrate the effect of our sparsity-promoting secondary objective. It can clearly be seen that our method (right) results in a significantly sparser solution compared to the Orthogonal Iteration algorithm (left).
\begin{figure*}[t!]
\centering
     \centerline{
    \begin{tabular}{ccc}%
    %
     \includegraphics[width=0.35\linewidth]{images_appendix/ablation_sparsity_orthIt.png}  & & \includegraphics[width=0.35\linewidth]{images_appendix/ablation_sparsity_our_p_3.png} 
    %
    \end{tabular}
    }
    \caption{Illustration of the effect of our sparsity-promoting secondary objective $g$ for a synthetic permutation synchronisation problem ($k=5, d=30, \rho=0.9, \sigma=0.3$, cf.~Sec.~\ref{sec:exp}). The matrix $U$ obtained by the Orthogonal Iteration algorithm (left) is not sparse. Our method  gives a sparse and mostly nonnegative $U$ (right).}
    \label{fig:sparsityQual} 
\end{figure*}



\end{document}