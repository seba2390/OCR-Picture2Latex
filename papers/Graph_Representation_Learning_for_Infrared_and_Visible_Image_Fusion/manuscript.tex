
%% bare_jrnl.tex
%% V1.4b
%% 2015/08/26
%% by Michael Shell
%% see http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.8b or later) with an IEEE
%% journal paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/pkg/ieeetran
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE! 
%% User assumes all risk.
%% In no event shall the IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%*************************************************************************


% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. The IEEE's font choices and paper sizes can   ***
% *** trigger bugs that do not appear when using other class files.       ***                          ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/



\documentclass[journal]{IEEEtran}
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[journal]{../sty/IEEEtran}





% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/pkg/ifpdf
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.






% *** CITATION PACKAGES ***
%
%\usepackage{cite}
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of the IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off
% such as if a citation ever needs to be enclosed in parenthesis.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 5.0 (2009-03-20) and later if using hyperref.sty.
% The latest version can be obtained at:
% http://www.ctan.org/pkg/cite
% The documentation is contained in the cite.sty file itself.






% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  % \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation
% can be obtained at: 
% http://www.ctan.org/pkg/graphicx
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found at:
% http://www.ctan.org/pkg/epslatex
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). The IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex





% *** MATH PACKAGES ***
%
%\usepackage{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics.
%
% Note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/amsmath





% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as the IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/pkg/algorithms
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/pkg/algorithmicx




% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/array


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.




% *** SUBFIGURE PACKAGES ***
%\ifCLASSOPTIONcompsoc
%  \usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
%\else
%  \usepackage[caption=false,font=footnotesize]{subfig}
%\fi
% subfig.sty, written by Steven Douglas Cochran, is the modern replacement
% for subfigure.sty, the latter of which is no longer maintained and is
% incompatible with some LaTeX packages including fixltx2e. However,
% subfig.sty requires and automatically loads Axel Sommerfeldt's caption.sty
% which will override IEEEtran.cls' handling of captions and this will result
% in non-IEEE style figure/table captions. To prevent this problem, be sure
% and invoke subfig.sty's "caption=false" package option (available since
% subfig.sty version 1.3, 2005/06/28) as this is will preserve IEEEtran.cls
% handling of captions.
% Note that the Computer Society format requires a larger sans serif font
% than the serif footnote size font used in traditional IEEE formatting
% and thus the need to invoke different subfig.sty package options depending
% on whether compsoc mode has been enabled.
%
% The latest version and documentation of subfig.sty can be obtained at:
% http://www.ctan.org/pkg/subfig




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure.
% Be aware that LaTeX2e kernels dated 2015 and later have fixltx2e.sty's
% corrections already built into the system in which case a warning will
% be issued if an attempt is made to load fixltx2e.sty as it is no longer
% needed.
% The latest version and documentation can be found at:
% http://www.ctan.org/pkg/fixltx2e


%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/stfloats
% Do not use the stfloats baselinefloat ability as the IEEE does not allow
% \baselineskip to stretch. Authors submitting work to the IEEE should note
% that the IEEE rarely uses double column equations and that authors should try
% to avoid such use. Do not be tempted to use the cuted.sty or midfloat.sty
% packages (also by Sigitas Tolusis) as the IEEE does not format its papers in
% such ways.
% Do not attempt to use stfloats with fixltx2e as they are incompatible.
% Instead, use Morten Hogholm'a dblfloatfix which combines the features
% of both fixltx2e and stfloats:
%
% \usepackage{dblfloatfix}
% The latest version can be found at:
% http://www.ctan.org/pkg/dblfloatfix




%\ifCLASSOPTIONcaptionsoff
%  \usepackage[nomarkers]{endfloat}
% \let\MYoriglatexcaption\caption
% \renewcommand{\caption}[2][\relax]{\MYoriglatexcaption[#2]{#2}}
%\fi
% endfloat.sty was written by James Darrell McCauley, Jeff Goldberg and 
% Axel Sommerfeldt. This package may be useful when used in conjunction with 
% IEEEtran.cls'  captionsoff option. Some IEEE journals/societies require that
% submissions have lists of figures/tables at the end of the paper and that
% figures/tables without any captions are placed on a page by themselves at
% the end of the document. If needed, the draftcls IEEEtran class option or
% \CLASSINPUTbaselinestretch interface can be used to increase the line
% spacing as well. Be sure and use the nomarkers option of endfloat to
% prevent endfloat from "marking" where the figures would have been placed
% in the text. The two hack lines of code above are a slight modification of
% that suggested by in the endfloat docs (section 8.4.1) to ensure that
% the full captions always appear in the list of figures/tables - even if
% the user used the short optional argument of \caption[]{}.
% IEEE papers do not typically make use of \caption[]'s optional argument,
% so this should not be an issue. A similar trick can be used to disable
% captions of packages such as subfig.sty that lack options to turn off
% the subcaptions:
% For subfig.sty:
% \let\MYorigsubfloat\subfloat
% \renewcommand{\subfloat}[2][\relax]{\MYorigsubfloat[]{#2}}
% However, the above trick will not work if both optional arguments of
% the \subfloat command are used. Furthermore, there needs to be a
% description of each subfigure *somewhere* and endfloat does not add
% subfigure captions to its list of figures. Thus, the best approach is to
% avoid the use of subfigure captions (many IEEE journals avoid them anyway)
% and instead reference/explain all the subfigures within the main caption.
% The latest version of endfloat.sty and its documentation can obtained at:
% http://www.ctan.org/pkg/endfloat
%
% The IEEEtran \ifCLASSOPTIONcaptionsoff conditional can also be used
% later in the document, say, to conditionally put the References on a 
% page by themselves.




% *** PDF, URL AND HYPERLINK PACKAGES ***
%
%\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/url
% Basically, \url{my_url_here}.
\UseRawInputEncoding
\usepackage{cite}
\usepackage{graphicx}
%\usepackage[linesnumbered]{algorithm2e}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{pifont}
\usepackage{booktabs}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{color}
\usepackage{multirow}
\usepackage[table,xcdraw]{xcolor}
\usepackage{stfloats}
% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
%
% paper title
% Titles are generally capitalized except for words such as a, an, and, as,
% at, but, by, for, in, nor, of, on, or, the, to and up, which are usually
% not capitalized unless they are the first or last word of the title.
% Linebreaks \\ can be used within to get better formatting as desired.
% Do not put math or special symbols in the title.


\title{Graph Representation Learning for Infrared and Visible Image Fusion}




% author names and IEEE memberships
% note positions of commas and nonbreaking spaces ( ~ ) LaTeX will not break
% a structure at a ~ so this keeps an author's name from being broken across
% two lines.
% use \thanks{} to gain access to the first footnote area
% a separate \thanks must be used for each paragraph as LaTeX2e's \thanks
% was not built to handle multiple paragraphs
%

\author{Jing~Li, Lu~Bai, Bin~Yang, Chang~Li, Lingfei~Ma, and~Edwin R. Hancock~\IEEEmembership{IEEE~Fellow} % <-this 
}

% note the % following the last \IEEEmembership and also \thanks - 
% these prevent an unwanted space from occurring between the last author name
% and the end of the author line. i.e., if you had this:
% 
% \author{....lastname \thanks{...} \thanks{...} }
%                     ^------------^------------^----Do not want these spaces!
%
% a space would be appended to the last name and could cause every name on that
% line to be shifted left slightly. This is one of those "LaTeX things". For
% instance, "\textbf{A} \textbf{B}" will typeset as "A B" not "AB". To get
% "AB" then you have to do: "\textbf{A}\textbf{B}"
% \thanks is no different in this regard, so shield the last } of each \thanks
% that ends a line with a % and do not let a space in before the next \thanks.
% Spaces after \IEEEmembership other than the last one are OK (and needed) as
% you are supposed to have spaces between the names. For what it is worth,
% this is a minor point as most people would not even notice if the said evil
% space somehow managed to creep in.



% The paper headers
\markboth{Journal of \LaTeX\ Class Files,~Vol.~x, No.~x, x~x}%
{Shell \MakeLowercase{\textit{et al.}}: Bare Demo of IEEEtran.cls for IEEE Journals}
% The only time the second header will appear is for the odd numbered pages
% after the title page when using the twoside option.
% 
% *** Note that you probably will NOT want to include the author's ***
% *** name in the headers of peer review papers.                   ***
% You can use \ifCLASSOPTIONpeerreview for conditional compilation here if
% you desire.




% If you want to put a publisher's ID mark on the page you can do it like
% this:
%\IEEEpubid{0000--0000/00\$00.00~\copyright~2015 IEEE}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.



% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}




% make the title area
\maketitle

% As a general rule, do not put math, special symbols or citations
% in the abstract or keywords.
\begin{abstract}
Infrared and visible image fusion aims to extract complementary features to synthesize a single fused image. Many methods employ convolutional neural networks (CNNs) to extract local features due to its translation invariance and locality. However, CNNs fail to consider the image's non-local self-similarity (NLss), though it can expand the receptive field by pooling operations, it still inevitably leads to information loss. In addition, the transformer structure extracts long-range dependence by considering the correlativity among all image patches, leading to information redundancy of such transformer-based methods. However, graph representation is more flexible than grid (CNN) or sequence (transformer structure) representation to address irregular objects, and graph can also construct the relationships among the spatially repeatable details or texture with far-space distance. Therefore, to address the above issues, it is significant to convert images into the graph space and thus adopt graph convolutional networks (GCNs) to extract NLss. This is because the graph can provide a fine structure to aggregate features and propagate information across the nearest vertices without introducing redundant information. Concretely, we implement a cascaded NLss extraction pattern to extract NLss of intra- and inter-modal by exploring interactions of different image pixels in intra- and inter-image positional distance. We commence by preforming GCNs on each intra-modal to aggregate features and propagate information to extract independent intra-modal NLss. Then, GCNs are performed on the concatenate intra-modal NLss features of infrared and visible images, which can explore the cross-domain NLss of inter-modal to reconstruct the fused image. We progressively expand the kennel sizes and dilations to increase the receptive field of GCNs, when we extract intra- and inter-modal NLss. Ablation studies and extensive experiments illustrates the effectiveness and superiority of the proposed method on three datasets.
\end{abstract}

% Note that keywords are not normally used for peerreview papers.
\begin{IEEEkeywords}
 Infrared image, Visible image, Image fusion, Graph Convolutional Networks.
\end{IEEEkeywords}






% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle

\begin{figure}[!t]
\centering
\includegraphics[width=3.5 in]{1.pdf}
\caption{Fusion results of our method, CNN-based method (IFCNN \cite{ zhang2020ifcnn }) and transformer-based method (SwinFusion \cite{ma2022swinfusion}). IFCNN mainly focuses on local features, which cannot extract the NLss, such as the spatially repeatable detail or texture of similar objects in far-space distance ($i.e.$, the two endpoint regions of the blue lines in visible image). SwinFusion employs transformer structure, which inevitably leads to redundant information. For example, SwinFusion preserve more illumination information from the lights of cars and street lamps, ($i.e.$, the regions existing in the yellow circles). The difference map also demonstrates that our method can preserve more details than IFCNN and SwinFusion.}
\label{FIG:1}
\end{figure}

\section{Introduction}
\IEEEPARstart{I}{mage} fusion can combine the superiority of multi-source images that captured by
different sensors to produce an informative fused image \cite{karim2022current,zhang2023visible}. Infrared and visible image fusion technology has been widely applied to intelligence acquisition or analysis \cite{ma2019infrared,tang2023datfuse,li2017pixel,ma2019locality,liu2018deep}, because infrared sensors are sensitive to the thermal radiation of the heat source, that can enhance the infrared targets. However, the infrared sensor fails to capture the texture due to the limitation of imaging mechanism. In contrast, the visible sensor can preserve abundant texture by capturing reflected light. Therefore, it is necessary to utilize the advantages of infrared and visible sensors to generate an informative fused image \cite{xu2020u2fusion,tang2022superfusion,zhou2021semantic}. Generally speaking, infrared and visible image fusion can be divided into traditional and deep learning (DL) based methods \cite{li2020attentionfgan,li2019coupled}.

\begin{figure*}[!h]
\centering
\includegraphics[width=6in]{2.pdf}
\caption{The overall framework of the proposed method. We design a cascaded NLss extraction pattern, which first extract independent intra-modal NLss of infrared and visible images by independent NLss of intra-modal extraction branch with two separate data flow, and then cross-domain NLss of inter-modal extraction branch extracts inter-modal NLss by concatenating intra-modal NLss features of infrared and visible images to reconstruct fused image. GCB and \textcircled{c} denote graph convolution block and the concatenate operation, respectively.}
\label{FIG:2}
\end{figure*}

The traditional fusion methods first utilize mathematical transformation to compute features, and then the features are combined by well-designed fusion strategies to generate fused image. Traditional methods mainly include multi-scale transformer methods (MST) \cite{shreyamsha2015image,burt1987laplacian}, sparse representation-based methods \cite{zhang2018sparse}, saliency-based methods \cite{bavirisetti2016two}, subspace-based methods \cite{mitianoudis2007pixel} and other hybrid methods \cite{liu2015general}. Although traditional methods can produce satisfactory results, they still suffer from the problem that the single representation neglects the difference among multi-source images. Moreover, traditional methods also need to manually design complicated fusion strategies.

To overcome the shortcoming of traditional methods, the end-to-end DL models are applied to the fusion task, because DL-based methods can fuse infrared and visible images in an end-to-end way with their powerful nonlinear fitting ability \cite{liu2018deep}. DL methods include CNN-based methods, generative adversarial network (GAN)-based methods, transformer-based methods \cite{xu2022infrared} and the other methods \cite{yue2023dif, ren2021infrared}. CNN-based methods extract infrared and visible image features by designing parallel convolution kernels \cite{tian2021depth}. In addition, the GAN structure is also utilized to image fusion task, which models the distribution of source images by designing an adversarial game. CNN- or GAN-based methods all conduct the sliding window on source images, which can introduce shift-invariance and locality.

Unfortunately, the existing CNN- or GAN-based methods mainly employ convolution operations to extract local features, thus they fail to consider the image's non-local self-similarity. In CNNs, though pooling operations can expand the receptive field to extract global features, it still inevitably leads to information loss. Besides, CNN- or GAN-based methods capture features by designing deeper or complex network without considering long-range dependency. Thus, the under-used latent interactions among image patches, regardless of their intra- and inter-image positional distance, leads to an unsatisfied performance.

Therefore, transformer structure is utilized to address the issues of CNN- or GAN-based methods. Li et al. \cite{li2022cgtf} and Vibashan et al. \cite{vs2021image} combined the transformer with CNNs to extract image's local features and long-range dependencies. In addition, Ma et al. \cite{ma2022swinfusion} and Li et al \cite{wang2022swinfuse} introduced Swin-transformer to infrared and visible image fusion tasks. However, transformer can extract long-range dependencies by self-attention mechanism, yet it considers the correlativity among all the image patches, which leads to transformer-based methods preserve redundant information.

To this end, developing a fusion method to effectively extract non-local self-similarity of images without preserving redundant information still remains a significant challenge. In this paper, we consider that infrared or visible images contain the same or similar irregular objects with spatially repeatable details or texture information. Therefore, it is significant to explore the feasibility of introducing graph representation into infrared and visible fusion task, because graph representation is more flexible than grid (CNN) or sequence (i.e., the transformer structure) representation to deal with the irregular objects, and it can also construct the relationships among the spatially repeatable details or texture information. Concretely, Fig. 1 shows that the different lamps with far-space distance in visible image have spatially repeatable texture information, such as the two endpoint regions of the blue lines in visible image. In addition, the different parts of the car with far-space distance in infrared image have similar thermal radiation details, such as the two endpoint regions of the red lines in infrared image. Similarly, the two endpoint regions of the green or orange lines also have spatially repeatable details. Therefore, when we convert image into graph space, these spatially repeatable regions become the nearest neighbors of themselves such as the graph of infrared or visible image in second row of Fig. 1. Based on this, the graph convolution can aggregate features and propagate information among the nearest vertices of the graph, which can not only extract image's non-local self-similarity, but also flexibly address irregular objects without introducing redundant information.

Fig. 2 shows the overall framework of our model. In our method, we treat the source image as a graph structure rather than a grid or sequence structure, which aims to extract spatial non-local self-similarity relationships and long-range dependency by exploring interactions of different image pixels in intra- and inter-image positional distance. Therefore, we convert images into the graph space, which treats the pixel as the vertices to construct graph structure, then we design a cascaded NLss extraction pattern, which first extracts intra-modal NLss of infrared and visible images by independent NLss of intra-modal extraction branch, and then extracts inter-modal NLss to reconstruct fused image by cross-domain NLss of inter-modal extraction branch. Specifically, in independent NLss of intra-modal extraction branch, we perform GCN on each intra-modal to aggregate features and propagate information across the nearest vertices by two separate data flow, because infrared and visible image have different modals. In cross-domain NLss of inter-modal extraction branch, we perform GCN on the concatenate intra-modal NLss features of infrared and visible images, which aims to increase the interactions of multi-modal features and explore the inter-modal NLss to reconstruct the fused image. Besides, to extract the global features of the intra- and inter-modal, we utilize dynamic graph convolutional networks, which is allowed to dynamically compute the vertex neighbors by the $k$-nearest neighbors (KNN) to construct a new graph structure in each layer. Therefore, we progressively expand the $k$ and dilations of GCN to increase the receptive field, which can extract global features and address the over-smoothing problem in GCN \cite{simonovsky2017dynamic, valsesia2018learning}.

To demonstrate the effectiveness of our fusion method, we also compare our method with a CNN-based method (IFCNN \cite{ zhang2020ifcnn } and a transformer-based method (SwinFusion \cite{ma2022swinfusion}). On the one hand, IFCNN extracts features by CNN, which mainly focuses on local features. Thus, compared with our method, IFCNN cannot preserve spatially repeatable detail or texture information of similar objects in far-space distance ($i.e.$, the two endpoint regions of the green lines in infrared image). For example, IFCNN preserves less details in the blue circles than our method, which is also shown in difference map of IFCNN and our result. On the other hand, SwinFusion employs transformer structure, which inevitably leads to information redundancy in their fused result. For example, SwinFusion preserves too much illumination information from the lights of cars and street lamps, which leads to the detail loss such as the regions existing in the yellow circle and red block. In addition, the difference map of SwinFusion and our result also illustrates that our method can preserve more details than SwinFusion.


Therefore, the key contributions of this work are summarized as follows:
\begin{itemize}
\item To simultaneously extract NLss of images with less redundant information and consider the spatially repeatable detail or texture information of images with similar objects in the far-space distance, we explore the feasibility of introducing the graph representation into the infrared and visible fusion task, which constructs a graph to aggregate features and propagate information among the nearest vertices.

\item To extract the independent modal NLss and cross-domain NLss, we propose a cascaded NLss extraction pattern, which includes two stages, i.e., the intra-modal NLss extraction for feature representation and the cross-domain NLss extraction in inter-modal. The former stage aims to extract the independent intra-modal NLss of each image, and the latter stage can increase the interactions of multi-modal features by extracting the cross-domain NLss to reconstruct fused images.

\item Ablation studies show the effectiveness of our fusion strategies, and qualitative and quantitative experiments demonstrate that our method has better performance than other CNN-, GAN- and transformer-based methods on three datasets.

\end{itemize}

The remainder of this paper is arranged as follows. Related works are presented in Section \uppercase\expandafter{\romannumeral2}. The details of the proposed method are shown in Section \uppercase\expandafter{\romannumeral3}. Section \uppercase\expandafter{\romannumeral4} shows the comparison and generalization experiments. General conclusions are presented in Section \uppercase\expandafter{\romannumeral5}.

\begin{figure}[!h]
\centering
\includegraphics[width=3.5 in]{3.pdf}
\caption{The differences of data representation for CNN, transformer and graph convolutional network. We introduce graph representation into infrared and visible fusion task, because graph representation is more flexible than grid (CNN) or sequence (Transformer structure) representation to deal with the irregular objects, and it can also construct the relationships among the spatially repeatable details or texture information with far-space distance.}
\label{FIG:3}
\end{figure}

\section{Related works}
\label{}
This part shows some related works about the CNN-, GAN- and transformer-based methods. In addition, some works of graph convolutional networks for vision tasks are also presented. Fig. 3 shows the differences of data representation for the CNN, the transformer and the graph convolutional network.

\subsection{Image fusion based on CNN and GAN}
CNN fuses images in an end-to-end way to avoid the complicated design of fusion rules that exist in traditional methods. Thus, CNNs were widely applied in infrared and visible image fusion tasks due to  their translation invariance and locality. Zhang et al. \cite{zhang2020ifcnn} utilized CNNs to extract saliency features of source images, and then the features were combined and reconstructed by two convolution layers (IFCNN). Li et al. \cite{li2021rfn} utilized the residual network to address the fusion task with a multi-stage training method (RFN-Nest). Tang et al. \cite{tang2022image} combined the image fusion with the vision task to propose a real-time fusion model via semantic-aware (SeAFusion). Li et al. \cite{li2021different} proposed a meta-learning fusion model to fuse images with different resolutions, and their model can produce fused results with arbitrary resolutions. Jian et al. \cite{jian2021infrared} fused source images by a deep decomposition process and saliency analysis stage to combine local and global saliency regions. In addition, image fusion also can be treated as an unsupervised problem and can be then addressed by GANs. Therefore, Ma et al. \cite{ma2019fusiongan} first utilized GANs to address the image fusion task and proposed FusionGAN, which designs an adversarial game to model the distribution of source images. Then, to better preserve the information of images, many works were proposed by extending GAN to dual discriminators (DDcGAN \cite{ma2020ddcgan} and D2WGAN \cite{li2020infrared}) or multi-classification GAN (GANMcC \cite{ma2020ganmcc}). Moreover, to perceive the discriminative region of source images, an attention mechanism was introduced into the dual discriminators GAN to fuse infrared and visible images (MgANFuse) \cite{li2020multigrained}. However, CNN- or GAN-based methods mainly extracted local features by convolution operations, but ignored the image's non-local self-similarity. Thus, transformer was applied to extract the global features of the image in fusion task.

\subsection{Transformer based fusion methods}
Transformer can capture long-range dependencies via a self-attention mechanism, which has been widely utilized to address sequence data, such as natural language processing \cite{devlin2019bert}. In addition, transformer was also applied in many computer vision tasks by splitting images into patches, and the image patches were embedded to sequence data to extract long-range dependency. Transformer has achieved tremendous success in image classification \cite{zheng2021rethinking}, image segmentation \cite{carion2020end}, and object detection \cite{dosovitskiy2021image}. Therefore, transformer was also applied to infrared and visible image fusion task. Li et al. \cite{li2022cgtf} combined CNN with transformer to extract the local features by CNN and capture long-range dependencies by transformer. Moreover, Wang et al. built a pure transformer network to extract the long-range dependency of images, and they designed a L1-norm based strategy to measure and preserve infrared saliency and visible texture information  \cite{wang2022swinfuse}. Ma et al. \cite{ma2022swinfusion} also proposed a pure transformer based fusion model (SwinFusion), which utilizes the cross-domain global learning to implement intra- and inter-domain fusion based on self-attention and cross-attention, and they introduced Swin transformer to extract long-range dependency of images. However, transformer always considered the correlativity among all the image patches, which leads to the fused result contains redundant information. Therefore, it is necessary to introduce graph representation into infrared and visible fusion task, because graph representation can extract image's non-local self-similarity without introducing redundant information.

\begin{figure}[!t]
\centering
\includegraphics[width=3.5 in]{4.pdf}
\caption{Graph representation of the image. In our method, we embed the image patches into feature vectors $X=\left[ {{\text{x}}_{1}},{{\text{x}}_{2}},\cdots ,{{\text{x}}_{N-1}}\text{,}{{\text{x}}_{N}} \right]$, which can be viewed as the vertices $\mathcal{V}=\left\{ {{v}_{1}},{{v}_{2}},\cdots ,{{v}_{N-1}},{{v}_{N}} \right\}$, and then we calculate each vertice's nearest neighbors and add edges to construct the graph representation of the image.}
\label{FIG:4}
\end{figure}

\subsection{Graph convolutional networks for vision tasks}
GCNs have achieved tremendous success in many applications \cite{bai2020learning}. The early GCN architecture was proposed based on the spatial-based GCNs \cite{micheli2009neural}, and then many variants of spatial-based GCN were proposed and applied to different tasks. In addition, Bruna et al. \cite{bruna2013spectral} proposed a spectral-based GCN by utilizing spectral theory, which was also improved and extended by many variants of GCN to deal with graph data. GCN was suitable to tackle graph data, such as natural language processing \cite{bastings2017graph}, and social networks \cite{tang2009relational}. GCNs were also introduced into point cloud data process methods \cite{simonovsky2017dynamic}, because they can analyze the point cloud image by considering their relationships. In computer vision domain, Han et al. \cite{han2022vision} treated images as a graph structure to propose a Vision GNN (VIG), which split images into sub-image patches and regarded them as the nodes of graphs. Thus, VIG can process irregular objects flexibly. Besides, GCN has also been used for hyperspectral image classification \cite{yu2022edge} and instance segmentation \cite{fan2020correlation}. Although several works have applied GCN to deal with multi-feature fusion, such as multi-view learning \cite{chen2023learnable} and hyperspectral image classification \cite{liu2020cnn}, the former mainly fused the features by a fully-connected neural network in the shared latent space of the original multi-view representations, and the latter combined CNN with GCN to extract the small-scale and large-scale spectral-spatial features in pixel- or super-pixel nodes. However, they all failed to consider the NLss of intra- and inter-modals of multi-modal features. Therefore, we introduce the graph representation into the infrared and visible fusion task by adaptively utilizing the neighborhood structure and latent interactions among intra- and inter-image positional distance.

\section{Methodology}
\label{}
This section first introduces the graph representation of the image, followed by some details of the independent NLss of intra-modal / cross-domain NLss of inter-modal extraction branches and loss functions.
%\subsection{Framework overview}

\subsection{Graph representation of image}
In our method, we first convert images into the graph space. Fig. 4 shows the construction of graph for an image. Given infrared image and visible image with the size of $H\times W\times C$, $H$, $W$ and $C$ denote the height, weight and channel size of the image, respectively. We first spilt source images to $N$ sub-image patches, and then embed the image patches into feature vectors ${{\text{x}}_{i}}\in {{\mathbb{R}}^{D}}$, where $D$ denotes the dimension and $i=1,2,\cdots ,N$ , we can get $X=\left[ {{\text{x}}_{1}},{{\text{x}}_{2}},\cdots ,{{\text{x}}_{N-1}}\text{,}{{\text{x}}_{N}} \right]$, which can be viewed as the vertices $\mathcal{V}=\left\{ {{v}_{1}},{{v}_{2}},\cdots ,{{v}_{N-1}},{{v}_{N}} \right\}$. In addition, given a vertex ${{\nu }_{i}}$, we calculate its nearest neighbors $\mathcal{N}\left( {{\nu }_{i}} \right)$ and design an edge ${{e}_{ji}}$ from ${{\nu }_{j}}$ to ${{\nu }_{i}}$ for each ${{\nu }_{j}}$ in $\mathcal{N}\left( {{\nu }_{i}} \right)$. To this end, we can transfer the image to the graph structure $\mathcal{G}=\left( \nu ,\varepsilon  \right)$ where $\varepsilon$ denotes the edges of the graph.

\begin{figure}[!t]
\centering
\includegraphics[width=3.5in]{5.pdf}
\caption{The structure of the graph convolution block (GCB) and the details of $k$ and $d$. The $k$ of KNN in graph convolution is progressively expanded from 3 to 8, and the dilation rate of GCNs is progressively expanded from 1 to 3. $k$ and $d$ aim to increase the receptive field.}
\label{FIG:5}
\end{figure}

\subsection{Independent NLss of intra-modal / Cross-domain NLss of inter-modal extraction branches}
In our method, the GCN is conducted on intra-modal and inter-modal to capture spatial non-local self-similarity relationships and long-range dependency. For GCN on the intra-modal, we design an independent NLss of intra-modal extraction branch to extract intra-modal NLss of infrared and visible images, respectively. For GCN on inter-modals, we concatenate the intra-modal NLss features of two source images in channel dimension, and then the features are transferred to a graph structure to reconstruct fused image.

\subsubsection{Graph convolutional operation} in this work, we first embed the image patches into the feature vector $X\in {{\mathbb{R}}^{D}}$, which is utilized to construct a graph $\mathcal{G}=\left( \nu ,\varepsilon  \right)$. To this end, GCN can capture informative features at each vertex by aggregating information from their neighborhoods. The graph convolution operation $\mathsf{\mathcal{F}}$ is defined as follows:
\begin{equation}
\begin{split}
 {{\mathcal{G}}_{l+1}}&=\mathcal{F}\left( {{\mathcal{G}}_{l}},{{\mathcal{W}}_{l}} \right) \\ 
 & =\text{Update}\left( \text{Aggregate}\left( {{\mathcal{G}}_{l}},{{\mathcal{W}}_{agg}} \right),{{\mathcal{W}}_{update}} \right), \\
\end{split}
\end{equation}
where ${{\mathsf{\mathcal{G}}}_{l}}$ and ${{\mathsf{\mathcal{G}}}_{l+1}}$ denote the input and output of the $l$-th layer, respectively. Aggregate is the aggregation function ,which aims to combine information from vertices' neighborhoods. Update denotes the update function, which can calculate the vertex representation by aggregated information. ${{\mathcal{W}}_{agg}}$ and ${{\mathcal{W}}_{update}}$ denote the learnable weights of aggregation and update functions, respectively. In addition, the aggregation and update operations of a vertex exchange information among their neighbor vertices can be formulated as follows:
\begin{equation}
\begin{split}
{{X}_{i+1}}=f\left( {{X}_{i}},h\left( \left\{ {{X}_{j}}|j\in \mathsf{\mathcal{N}}\left( {{X}_{i}} \right) \right\},{{X}_{i}},{{\mathsf{\mathcal{W}}}_{agg}} \right),{{\mathsf{\mathcal{W}}}_{update}} \right),
\end{split}
\end{equation}
where $f\left( \cdot  \right)$ and $h\left( \cdot  \right)$ are the update and aggregation functions, respectively, and the ${{X}_{i}}$ and ${{X}_{i+1}}$ denote the vertex features of $i$-th and ($i+1$)-th layers. $\mathsf{\mathcal{N}}\left( {{X}_{i}} \right)$ denotes the neighbor vertices of a vertex $X$ at $i$-th layer. ${{X}_{j}}$ denotes the parametrized features of its neighbor vertices by ${{\mathsf{\mathcal{W}}}_{agg}}$. Moreover, the aggregation function $h\left( \cdot  \right)$ is formulated as follows:
\begin{equation}
\begin{split}
h\left( \cdot  \right)=\max \left( {{X}_{j}}-{{X}_{i}}|j\in \mathsf{\mathcal{N}}\left( {{X}_{i}} \right) \right).
\end{split}
\end{equation}

Thus, the update function is calculated as $f\left( \cdot  \right)=h\left( \cdot  \right)*{{\mathsf{\mathcal{W}}}_{update}}$. To this end, we adopt the above graph convolution operation in our method, which is denoted as $\text{GraphConv}\left( X \right)$.

\subsubsection{Graph convolution block}
Fig. 5 shows the structure of the graph convolution block (GCB), which aims to dynamically compute the vertex neighbors by the $k$-nearest neighbors (KNNs) and adds the new edge set $\varepsilon _{i}^{dy}$to construct a new graph structure in each layer, which can enlarge the receptive field to extract global features. The $\varepsilon _{i}^{dy}$is defined as follows:
\begin{equation}
\begin{split}
\varepsilon _{i}^{dy}=\left\{ {{e}_{ij}}=\left\{ {{v}_{i}},{{v}_{j}} \right\}|{{v}_{j}}\in \mathsf{\mathcal{N}}\left( {{v}_{i}} \right) \right\},
\end{split}
\end{equation}
where ${{e}_{ij}}$ denotes the edge from ${{\nu }_{i}}$ to ${{\nu }_{j}}$, $\mathcal{N}\left( {{\nu }_{i}} \right)$ is the $k$ nearest neighbors of vertex ${{\nu }_{i}}$. Therefore, the dynamic graph convolution $\text{DynGC}\left( X \right)$ is formulated as follows:
\begin{equation}
\begin{split}
\text{DynGC}\left( X \right)=\text{GraphConv}\left( X,\varepsilon _{i}^{dy} \right),
\end{split}
\end{equation}
where the $X$ denotes the vertex features. In addition, to increase the feature diversity, a fully-connected layer (FC) is utilized before and after graph convolution operations, and the activation function is also applied to avoid model collapse. Moreover, we implement residual learning in the dynamic graph convolution to design a deeper network, which can enable our model to capture more deep features and obtain reliable convergence in training:
\begin{equation}
\begin{split}
X'=\sigma \left( \text{Fc}\left( \text{DynGC}\left( \text{Fc}\left( X \right) \right) \right) \right)+X,
\end{split}
\end{equation}
where $\sigma $ denotes the GeLU activation function \cite{hendrycks2016gaussian} that employed in this work. Besides, to enhance the feature representation of our model, we insert a feed-forward network (FFN), which includes a multi-layer perceptron with two FC layers, and the GCB can be formulated as follows:
\begin{equation}
\begin{split}
\text{GCB}\left( X \right)&=\text{FFN}\left( X' \right) \\ 
 & =\text{FFN}\left( \sigma \left( \text{Fc}\left( \text{DynGC}\left( \text{Fc}\left( X \right) \right) \right) \right)+X \right).
\end{split}
\end{equation}

\subsubsection{Independent NLss of intra-modal extraction branch}
Fig. 2 shows the structure of independent NLss of intra-modal extraction branch, which includes six GCBs with progressively expanded $k$ and dilations $d$ of GCNs to enlarge the receptive field, and $k$ denotes the $k$-nearest neighbors, $d$ represents the dilation coefficient. Fig. 5 also shows the details of the GCN with different $k$ and $d$ values. Besides, we design two separate data flow in this branch to extract intra-modal NLss of infrared and visible images due to their modal discrepancy. Therefore, the intra-modal NLss  $fea_{ir}^{intra}$ of infrared image (ir) and $fea_{vis}^{intra}$ visible image (vis) are formulated as follows:
\begin{equation}
\begin{split}
fea_{ir}^{intra}=\text{GCB}\left( {{X}_{ir}} \right),
\end{split}
\end{equation}
\begin{equation}
\begin{split}
fea_{vis}^{intra}=\text{GCB}\left( {{X}_{vis}} \right),
\end{split}
\end{equation}
where ${{X}_{ir}}$ and ${{X}_{vis}}$ denote the vertex features of infrared and visible images, respectively.


\subsubsection{Cross-domain NLss of inter-modal extraction branch}
the cross-domain NLss of inter-modal extraction branch has the same structure with the independent NLss of intra-modal extraction branch, as shown in Fig. 2. However, this  branch performs GCNs on the concatenate intra-modal NLss features of infrared and visible images to explore the cross-domain NLss. Therefore, we first concatenate the intra-modal NLss of source images in channel wise as follows:
\begin{equation}
\begin{split}
fea^{intra}=\text{Concat}\left( fea_{ir}^{intra} ,fea_{vis}^{intra} \right),
\end{split}
\end{equation}
where $\text{Concat}(\cdot )$ denotes the concatenation operation. Accordingly, we transfer the features ($fea^{intra}$) to a graph $\mathcal{G}$. Furthermore, we utilize GCBs to capture the inter-modal NLss among the inter-image positional distance to reconstruct the fused image, and a convolution operation $\rho \left( \cdot  \right)$ is applied to reduce the data dimension by the following equation:
\begin{equation}
\begin{split}
{{I}_{fused}}=\rho \left( \text{GCB}\left( fea^{intra} \right) \right),
\end{split}
\end{equation}
where ${{I}_{fused}}$ denotes fused image.

\subsection{Loss functions}
To model the data distribution of source images and preserve enough information, we design a loss function to supervise the training process of our method. The total loss $\mathcal{L}$ contains two parts: infrared content loss ${\mathcal{L}_{ir}}$ and visible detail loss ${\mathcal{L}_{vi}}$, which is formulated as follows:
\begin{equation}
\begin{split}
\mathcal{L}={\mathcal{L}_{ir}}+\lambda {\mathcal{L}_{vi}},
\end{split}
\end{equation}
where $\lambda$ aims to balance ${\mathcal{L}_{ir}}$ and ${\mathcal{L}_{vi}}$. The fused image should have a similar intensity to infrared image. Therefore, ${\mathcal{L}_{ir}}$ mainly focuses on the content loss to guide our model to preserve enough infrared intensity information using the following equation:
\begin{equation}
\begin{split}
{\mathcal{L}_{ir}}=\frac{1}{HW}\left( \left\| {{I}_{fused}}-I \right\|_{F}^{2} \right),
\end{split}
\end{equation}
where $I$ and ${{I}_{fused}}$ denote the infrared and fused images, respectively. $H$ and $W$ denote the height and width of an image. $\left\| \cdot  \right\|_{F}^{{}}$ is the matrix Forbenius norm. Besides, to preserve more details from visible image, we not only force the fused image to have similar intensity with visible image, but also calculate gradient discrepancies of the fusion result and visible image to preserve more texture information. Therefore, ${\mathcal{L}_{vi}}$ is defined as follows:
\begin{equation}
\begin{split}
{\mathcal{L}_{vi}}=\frac{1}{HW}\left( \left\| {{I}_{fused}}-V \right\|_{F}^{2}+\left\| \nabla {{I}_{fused}}-\nabla V \right\|_{F}^{2} \right),
\end{split}
\end{equation}
where $V$ denotes visible image, $\nabla $ means the gradient operator.


\begin{figure}[!t]
\centering
\includegraphics[width=3in]{6.pdf}
\caption{Fused example of the proposed method and nine fusion methods on the TNO dataset}
\label{FIG:6}
\end{figure}

\begin{figure}[!t]
\centering
\includegraphics[width=3in]{7.pdf}
\caption{Another fused example from the TNO dataset.}
\label{FIG:7}
\end{figure}

\begin{figure}[!t]
\centering
\includegraphics[width=3in]{8.pdf}
\caption{Fused example of the proposed method and nine methods on the Roadscene.}
\label{FIG:8}
\end{figure}


\section{Experiments}
In this section, we first show the experimental configurations and some implementation details, and then demonstrate the superiority of the proposed method. We conduct comparative and generalization experiments on TNO  $\footnote{https://figshare.com/articles/TNO\_Image\_Fusion\_Dataset/1008029}$, Roadscene $\footnote{https://github.com/hanna-xu/RoadScene}$ and M3FD $\footnote{https://github.com/dlut-dimt/TarDAL.}$ public datasets in qualitative and quantitative ways. In addition, we also conduct several ablation experiments to demonstrate the effectiveness of cascaded NLss extraction pattern and changeable kennel size and dilation, and we also evaluate our method in the target detection task. In the end, we show the efficiency comparison results of all the compared methods.

\subsection{Experimental configurations}
To comprehensively evaluate the performance of our fusion model, we compare it with nine methods, which include traditional fusion methods (CBF \cite{shreyamsha2015image}, LP \cite{burt1987laplacian}), CNN-based methods (IFCNN \cite{zhang2020ifcnn}, SeAFusion \cite{tang2022image}, SEDRFuse \cite{jian2020sedrfuse}), GAN-based methods (FusionGAN \cite{ma2019fusiongan}, DDcGAN \cite{ma2020ddcgan}, CrossFuse \cite{wang2023cross}) and transformer-based methods (SwinFusion \cite{ma2022swinfusion}). We first conduct the comparison experiments on the TNO dataset, and both infrared and visible images in TNO are gray images. In addition, with the development of sensor technology, the visible sensor does not limit to gray space, which can capture the scene in color space to improve the human visual effect. Therefore, we also select other two public datasets with colorized visible images (Roadscene dataset and M3FD dataset) to illustrate the superiority of our method. For the infrared and color visible image fusion task, we first transform the RGB image to YCbCr, and the Y channel of YCbCr denotes the luminance, which contains the main detail and texture of the scene, Cb and Cr denote the color information. Then, we fuse infrared image and Y channel of visible image to generate a new Y channel ${{Y}^{f}}$. Finally, we combine  ${{Y}^{f}}$ with Cb and Cr channels to produce a color-fused image in RGB space.

In our experiment, qualitative and quantitative analysis are utilized to evaluate the proposed and other methods simultaneously. For qualitative analysis, we evaluate fused results by human visual inspection, which mainly focuses on illuminance, sharpness, contrast and so on. For quantitative analysis, four evaluation metrics are selected in our work, including structural similarity index measure (SSIM) \cite{wang2004image}, peak signal-to-noise ratio (PSNR) \cite{ma2016infrared}, correlation coefficient (CC) \cite{ma2019infrared} and the rate of noise or artifacts added to the fused result in the fusion process (Nabf) \cite{kumar2013multifocus}. Moreover, SSIM aims to measure the structure information among images, PSNR can calculate distortion in the fusion process by computing the ratio of peak value power and noise power in given images, and CC can measure the linear correlation between fused and source images. For SSIM, PSNR and CC, a larger value reflects better performance. However, a smaller value of Nabf denotes better performance.

\subsection{Implementation details}
 In our method, we train the model on the TNO dataset, and we extend it by cropping source images into sub-image with a size of $64\times 64$, and the stride of cropping is set to 20, thus we can collect 22,252 image pairs. The parameters of our method are updated by the Adam optimizer, while the learning rate is initialized to $1\times {{10}^{\text{-}4}}$ and decayed exponentially. In addition, the batch size and epoch of the proposed method are set to 4 and 100, respectively. The $k$ of KNN in graph convolution is progressively expanded from 3 to 8, and the dilation of GCNs is progressively expanded from 1 to 3. Fig. 5 shows the details of different $k$ and $d$ combinations. The hyper-parameters $\lambda$ is set to 1.5 in this study. Besides, we train our model on the NVIDIA P5000 GPU with 32 GB memory.

\subsection{Comparative experiments on TNO}
To comprehensively illustrate the superiorities of our method, we compare it with nine fusion methods on 20 image pairs of TNO in qualitative and quantitative ways. The comparative methods contain traditional methods, CNN-based methods, GAN-based methods and transformer-based methods.

\begin{table*}[]
\centering
\caption{{Quantitative analysis of four metrics on TNO, Roadscene, M3FD datasets, and the pedestrian detection results on M3FD. The \textbf{bold} values indicate the best model performance, and the \textcolor{red}{red} and \textcolor{blue}{blue} values denote the second and third order.}}
\setlength{\tabcolsep}{0.5 mm}
\renewcommand\arraystretch{1.1}{
\begin{tabular}{ccccccccccc}
\hline
Dataset            & \multicolumn{10}{c}{TNO}                                                                                                                                                                                                                                                                                                               \\ \hline
Methods            & CBF                                 & LP                                 & IFCNN                               & FusionGAN                           & DDcGAN       & SEDRFuse                           & SeAFusion                          & SwinFusion                         & CrossFuse   & Our                                 \\ \hline
SSIM ↑             & 0.957±0.322                         & 1.239±0.148                        & {\color[HTML]{00B0F0} 1.293±0.149}  & 1.156±0.146                         & 1.072±0.155  & 1.171±0.174                        & {\color[HTML]{FF0000} 1.306±0.134} & 1.291±0.137                        & 1.191±0.164 & \textbf{1.344±0.14}                 \\
CC    ↑            & 0.389±0.197                         & 0.432±0.193                        & {\color[HTML]{00B0F0} 0.481±0.184}  & 0.418±0.166                         & 0.476±0.156  & {\color[HTML]{FF0000} 0.512±0.169} & 0.477±0.188                        & 0.465±0.194                        & 0.447±0.177 & \textbf{0.513±0.172}                \\
PSNR↑              & {\color[HTML]{00B0F0} 14.638±4.911} & 12.436±3.775                       & {\color[HTML]{FF0000} 14.845±4.218} & 13.347±3.011                        & 12.334±2.228 & 13.606±3.556                       & 13.545±3.719                       & 13.241±3.755                       & 13.659±4.04 & \textbf{15.416±4.364}               \\
Nabf ↓             & 0.132±0.082                         & {\color[HTML]{FF0000} 0.02±0.009}  & 0.062±0.019                         & {\color[HTML]{00B0F0} 0.026±0.03}   & 0.123±0.106  & 0.083±0.081                        & 0.081±0.11                         & 0.051±0.02                         & 0.074±0.086 & \textbf{0.016±0.013}                \\ \hline
Dataset            & \multicolumn{10}{c}{RoadSence}                                                                                                                                                                                                                                                                                                         \\ \hline
SSIM ↑             & 1.256±0.152                         & 1.417±0.137                        & {\color[HTML]{00B0F0} 1.425±0.131}  & 1.196±0.108                         & 1.157±0.115  & 1.198±0.163                        & 1.338±0.109                        & {\color[HTML]{FF0000} 1.444±0.121} & 1.233±0.131 & \textbf{1.456±0.135}                \\
CC    ↑            & 0.595±0.218                         & 0.69±0.166                         & {\color[HTML]{00B0F0} 0.686±0.173}  & 0.63±0.179                          & 0.641±0.14   & 0.685±0.163                        & 0.678±0.18                         & {\color[HTML]{FF0000} 0.691±0.185} & 0.634±0.21  & \textbf{0.692±0.178}                \\
PSNR↑              & {\color[HTML]{00B0F0} 17.179±2.687} & 16.931±2.583                       & {\color[HTML]{FF0000} 17.258±2.486} & 13.081±1.545                        & 14.171±2.019 & 15.354±1.885                       & 15.883±2.038                       & 15.156±2.14                        & 13.96±1.475 & \textbf{17.353±2.522}               \\
Nabf ↓             & 0.037±0.017                         & 0.034±0.007                        & 0.027±0.004                         & 0.044±0.023                         & 0.064±0.024  & {\color[HTML]{FF0000} 0.017±0.01}  & 0.072±0.024                        & {\color[HTML]{00B0F0} 0.02±0.006}  & 0.049±0.015 & \textbf{0.002±0.001}                \\ \hline
Dataset            & \multicolumn{10}{c}{M3FD}                                                                                                                                                                                                                                                                                                              \\ \hline
SSIM ↑             & 0.853±0.233                         & 1.231±0.133                        & {\color[HTML]{00B0F0} 1.327±0.113}  & 1.252±0.13                          & 1.066±0.086  & {\color[HTML]{FF0000} 1.33±0.104}  & 1.306±0.106                        & 1.31±0.11                          & 1.248±0.106 & \textbf{1.339±0.122}                \\
CC    ↑            & 0.342±0.136                         & 0.39±0.131                         & {\color[HTML]{00B0F0} 0.466±0.127}  & 0.365±0.122                         & 0.392±0.114  & \textbf{0.503±0.123}               & 0.434±0.133                        & 0.433±0.139                        & 0.435±0.134 & {\color[HTML]{FF0000} 0.467±0.127}  \\
PSNR↑              & 13.127±2.276                        & 12.265±1.824                       & \textbf{14.371±1.935}               & {\color[HTML]{FF0000} 14.006±1.794} & 11.368±1.105 & 13.598±1.864                       & 13.521±1.976                       & 13.629±2.03                        & 13.16±1.963 & {\color[HTML]{00B0F0} 13.768±1.959} \\
Nabf ↓             & 0.026±0.015                         & {\color[HTML]{FF0000} 0.006±0.002} & 0.028±0.006                         & {\color[HTML]{00B0F0} 0.006±0.005}  & 0.063±0.041  & 0.012±0.01                         & 0.028±0.009                        & 0.026±0.007                        & 0.045±0.015 & \textbf{0.001±0.001}                \\ \hline
                   & \multicolumn{10}{c}{Object detection on M3FD}                                                                                                                                                                                                                                                                                          \\ \hline
AP@0.5             & 0.297                               & 0.191                              & 0.356                               & 0.232                               & 0.072        & 0.353                              & \textbf{0.364}                     & 0.336                              & 0.300       & 0.359                               \\
AP@0.7             & 0.238                               & 0.162                              & 0.291                               & 0.195                               & 0.057        & 0.289                              & \textbf{0.307}                     & 0.281                              & 0.230       & \textbf{0.307}                      \\
AP@0.9             & 0.027                               & 0.025                              & 0.041                               & 0.020                               & 0.007        & 0.043                              & 0.045                              & 0.039                              & 0.029       & \textbf{0.055}                      \\
mAP@{[}0.5:0.95{]} & 0.182                               & 0.124                              & 0.224                               & 0.146                               & 0.043        & 0.225                              & 0.235                              & 0.214                              & 0.182       & \textbf{0.237}                      \\ \hline
\end{tabular}}
\end{table*}

\subsubsection{Qualitative analysis}
Fig. 6 and Fig. 7 show two experimental results of our method and nine fusion methods on the TNO dataset. We select a typical region from each image and zoom in them on the right bottom corner. Compared with nine fusion methods, our method can preserve clearer texture and details. In Fig. 6, all the methods can produce satisfactory results. However, our fused result can capture sharper edges with abundant texture. For example, the regions of the red block of our result in Fig. 6 is more distinct, and it also preserves more texture than other methods, such as the white part in the sign. In Fig. 7, only our result and CrossFuse can clearly present the shape of the propeller of the helicopter in the red blocks. However, our result preserves more meaningful information than CrossFuse, such as the empennage of the helicopter in green blocks.

We also comprehensively compare our method with the transformer-based method (SwinFusion), because transformer can extract the long-range dependency of the image. However, it considers the correlativity among all the image patches by self-attention mechanism, which leads to transformer-based methods inevitably introducing redundant information. For example, the regions that we zoomed in the right bottom of Fig. 6 and Fig. 7 show that our result can preserve clearer texture and capture more details than SwinFusion. Besides, we also zoom in several typical regions of CrossFuse and our results at the bottom of Fig. 6 and Fig. 7, which shows that our result preserves more textures and infrared thermal features than CrossFuse, because we use graph representation to adaptively utilize the neighborhood structure and latent interactions among the regions with spatially repeatable detail or texture information. For example, the red, yellow and green lines that exist in our result denote two endpoint regions of a line that have same or similar objects with spatially repeatable detail or texture information, and our method can aggregate features and propagate information among the nearest vertices rather than all the image patches.


\subsubsection{Quantitative Analysis}
in quantitative experiments, 20 image pairs are selected from TNO to compare various model performance, and we use multiple metrics to evaluate them objectively, which include SSIM, PSNR, CC and Nabf. Table \uppercase\expandafter{\romannumeral1} shows that the proposed method achieves better performance than others on all four metrics. Specifically, our method has the largest SSIM value, which means it can capture more structure features from source images, and the largest PSNR value of our method illustrates it can introduce less noise. Moreover, the largest CC value and the smallest Nabf of our method demonstrate that our method could preserve more information from source images with fewer artifacts.

\begin{figure}[!t]
\centering
\includegraphics[width=3in]{9.pdf}
\caption{Another fused example from the Roadscene.}
\label{FIG:9}
\end{figure}

\subsection{Generalization experiments on Roadscene}
In our experiments, we also analyze the generalization of our method, thus 150 image pairs are selected from Roadscene to test and compare our method with others in qualitative and quantitative ways. In addition, the generalization experiments on Roadscene can also test the performance of our method in RGB space.

\subsubsection{Qualitative analysis}
the visible images in Roadscene are captured in RGB space, thus we show several examples in Fig. 8 and Fig. 9. We also select and zoom in the typical regions in the blue or red blocks to show the differences of fusion methods. Fig. 8 shows that the house existing in the red blocks are missing or blurry in the results of SwinFusion, SeAFusion, CrossFuse, FusionGAN and DDcGAN. However, our result has clear profiles and sharper edges than them, and our result also contains more textures than the other methods. Moreover, the regions existing in the red blocks of Fig. 9 also suffer overexposure issues in SwinFusion, SeAFusion, CrossFuse and IFCNN, which fail to present the details of the car, and our result has clear textures than the others, such as the man in the red block.

SwinFusion and our method all can extract long-range dependency, however our method utilizes graph representation to extract spatial non-local self-similarity relationship without introducing redundant information, Fig. 8 and Fig. 9 show that our results preserve clearer textures than SwinFusion. Besides, we compare our result with CrossFuse, and two typical regions are enlarged at the bottom of Fig. 8 and Fig. 9, which illustrates that our results have clearer texture than CrossFuse, because the results of CrossFuse are overexposure, which leads to the features of the car in zoomed regions are disappeared. The red and yellow lines existing in our results denote two endpoint regions of a line that have same or similar objects with spatially repeatable detail or texture information.



\subsubsection{ Quantitative Analysis}
in generalization experiments, we use four metrics to quantitatively evaluate all the methods on 150 image pairs from the Roadscene. Table \uppercase\expandafter{\romannumeral1} shows that we achieve the best results on all metrics, which illustrates that our method can obtain better results in the quantitative analysis. Therefore, quantitative analysis indicates that our method has better generalization ability on the Roadscene dataset.

\subsection{Generalization experiments on M3FD}
We also conduct a generalization experiment on the M3FD dataset, and 150 image pairs are selected from M3FD to qualitatively and quantitatively evaluate the performance of all the methods.

\begin{figure}[!t]
\centering
\includegraphics[width=3in]{10.pdf}
\caption{Fused example from the M3FD.}
\label{FIG:10}
\end{figure}

\begin{figure}[!t]
\centering
\includegraphics[width=3in]{11.pdf}
\caption{Another fused example from M3FD.}
\label{FIG:11}
\end{figure}

\begin{figure}[!t]
\centering
\includegraphics[width=3in]{12.pdf}
\caption{Ablation experiments of our method. Fixed\_k\_in\_GCN, W/o\_dilation\_in\_GCN and W/o\_NLss\_of\_inter-modal aim to demonstrate the effectiveness of changeable kennel size, dilation cascaded and NLss extraction pattern.}
\label{FIG:12}
\end{figure}

\subsubsection{Qualitative Analysis}
in Fig. 10, only our method, CrossFuse and DDcGAN can capture the background information that exists in red blocks, such as the buildings. However, our method has more saliency targets than CrossFuse, such as the human in red blocks, and DDcGAN also preserves fewer textures than our method about the human and mountains. Moreover, compared with the other methods of Fig. 10, our result can balance the information of two source images. The qualitative analysis of M3FD illustrates the superiorities of our method.

In addition, we also compare our method with CrossFuse on the M3FD dataset, and several typical regions are selected and presented at the bottom of Fig. 10 and Fig. 11. We can see that our result preserves more details than CrossFuse. For example, the clouds are clear in our result but are disappeared in CrossFuse at the bottom of Fig. 11, and the bottom of Fig. 10 also demonstrates that our method contains more textures and saliency targets than CrossFuse.


\subsubsection{Quantitative Analysis}
four metrics are utilized to quantitatively evaluate all methods on 150 image pairs of M3FD. Table \uppercase\expandafter{\romannumeral1} shows that our method has the largest value on SSIM and also has the minimum performance on Nabf. Besides, it also has acceptable performance on CC (rank second) and PSNR (rank third), which illustrates that we obtain better quantitative results than others in most cases on the M3FD dataset.

\subsection{Ablation analysis}
In this work, we propose a cascaded NLss extraction pattern to extract intra- and inter-modal NLss, and we also progressively expand the kennel size and dilation to increase the receptive field of GCN when we extract intra- and inter-modal NLss. Therefore, to demonstrate the effectiveness of the above operations, we design several ablation experiments.

\begin{figure*}[!h]
\centering
\includegraphics[width=6in]{13.pdf}
\caption{The target detection results of the proposed method with 9 methods on three scenes of different datasets}
\label{FIG:13}
\end{figure*}

\subsubsection{Ablation analysis of cascaded NLss extraction pattern}
we implement a cascaded NLss extraction pattern to extract NLss of intra- and inter-modals by exploring interactions of different image pixels in intra- and inter-image positional distance. Specifically, we design an independent NLss of intra-modal branch and cross-domain NLss of inter-modal extraction branch. Therefore, we conduct an ablation experiment to demonstrate the effectiveness of the cascaded NLss extraction pattern by dropping out the cross-domain NLss of inter-modal extraction branch, and the trained model named W/o\_NLss\_of\_inter-modal.

Fig. 12 shows several results of W/o\_NLss\_of\_inter-modal, and our method can preserve more texture information and details than W/o\_NLss\_of\_inter-modal. For example, in the first column, our method preserves clearer texture of the tree that existing in the red block. In the red block of the middle column, our method captures more infrared thermal details of the hand than W/o\_NLss\_of\_inter-modal, and achieves a better visual effect than W/o\_NLss\_of\_inter-modal in the last column, such as the region of the yellow blocks of our method can present the profile and edge of the helicopter. 

In Fig. 12, our method has better performance than W/o\_NLss\_of\_inter-modal. However, W/o\_NLss\_of\_inter-modal also achieves satisfactory fused results, which can also demonstrate the effectiveness of independent NLss of intra-modal branch. Besides, 20 image pairs of TNO are used to compare our method with W/o\_NLss\_of\_inter-modal in a quantitative way, Table \uppercase\expandafter{\romannumeral2} shows that our method has better performance than W/o\_NLss\_of\_inter-modal on all the four metrics, which can demonstrate the effectiveness of the cascaded NLss extraction pattern.

\subsubsection{Ablation analysis of changeable kennel size and dilation}
in our method, to increase the receptive field of GCN when we extract intra- and inter-modal NLss, we progressively expand the kennel size and dilation in different GCB. Thus, we conduct an ablation experiment to demonstrate the effectiveness of changeable kennel size and dilation. Specifically, we train a fusion model by seting a fixed $k$=3, which is named Fixed\_k\_in\_GCN, and also train a fusion model by dropping out dilation operation, which is named W/o\_dilation\_in\_GCN.

Fig. 12 shows the fused results of Fixed\_k\_in\_GCN, W/o\_dilation\_in\_GCN and our method, which demonstrates that our method preserves more texture and infrared thermal objects. For example, only our method can capture the textures existing in the red blocks of the first column, and it also preserves more infrared saliency information than Fixed\_k\_in\_GCN and W/o\_dilation\_in\_GCN in the red block of the second column. In addition, two ablation experiments all fail to preserve the edges and profile of the helicopter, which are captured by our method in the last column.

We also compare our method with Fixed\_k\_in\_GCN and W/o\_dilation\_in\_GCN in a quantitative way. Table \uppercase\expandafter{\romannumeral2} shows the quantitative analysis results, which illustrate that our method has better performance than Fixed\_k\_in\_GCN on all four metrics, and also has better results than W/o\_dilation\_in\_GCN on SSIM, CC and Nabf. The ablation experiments demonstrate the effectiveness of changeable kennel size and dilation.

\subsection{Extended comparison experiments on target detection}
To demonstrate the effectiveness of our fusion model in specific downstream task, we compare our fusion model with other methods in a target detection vision task, which aims to detect targets from digital images, such as pedestrians. Specifically, we design a target detection experiment based on a trained YOLOv5 \cite{redmon2016you} model to evaluate the performance of the proposed and other methods. In the target detection experiment, we take the fused results of each method as the inputs of the trained YOLOv5 model, respectively, and then we compare the performance of target detection to evaluate the effectiveness of fusion models. Fig. 13 shows the target detection results of all the methods, and the three scenes are selected from three datasets, which are located in blocks with different colors. The first two rows present the detection results of the TNO dataset, only our method and SwinFusion can detect all the targets compared to the other methods, yet we achieve better visual effect than SwinFusion. The detect results of Roadscene are presented in the middle two rows, which shows that SEDRFuse, FusionGAN, SeAFusion all detect the person in their result by a mistake. In addition, CBF also causes a false recognition of the car, both LP and DDcGAN fail to detect all the targets. Besides, our method, CrossFuse, SwinFusion and IFCNN all detect the targets, but our result has a higher confidence coefficient and better visual effect than them. The examples of M3FD are shown in the last two rows. Note that LP, FusionGAN, CrossFuse and DDcGAN cannot detect any objects. CBF, ifcnn, SEDRFuse and SwinFusion only detect part of the targets. SeAFusion and our result can detect all the objects, yet our result obtain higher confidence coefficient than SeAFusion. In addition, compared with TNO and Roadscene, M3FD provides the ground turth of pedestrian detection. Therefore, to demonstrate the superiority of our method on the high-level task, we take M3FD to compare the performance of each method by detecting the pedestrian, and the results are shown in Table {\uppercase\expandafter{\romannumeral1}}, which illustrates that our method is superior to other methods in terms of detection AP@0.7, AP@0.9, and mAP@[0.5:0.95]. Therefore, target detection experiments also illustrate the effectiveness and superiority of the proposed method.


\subsection{Efficiency comparison}
In our work, we calculate the average running time of each method on three datasets to compare their efficiency. The traditional methods are implemented by the CPU, and the other methods are conducted by the GPU. Table \uppercase\expandafter{\romannumeral3} shows that CrossFuse consume less time than other methods. In addition, compared with the deep learning based methods, our method consumes more time but less time than CBF, since we need to dynamically compute the vertex neighbors by the k-nearest neighbors (KNNs) and add the new edge set to construct a new graph structure in each layer, which leads to unsatisfied computing efficiency. However, our method can obtain better performance in qualitative and quantitative analysis than the other compared methods. Therefore, considering both computing efficiency and the performance of fused results, our method is still acceptable in most cases.

\begin{table}[]
\centering
\caption{{Quantitative analysis of ablation experiments on four metrics. The bold values indicate the best model performance.}}
\setlength{\tabcolsep}{0.7 mm}
\renewcommand\arraystretch{1.5}{
\begin{tabular}{ccccc}
\hline
                                                                     & SSIM↑               & CC↑                  & PSNR↑                 & Nabf↓                \\ \hline
Fixed\_k\_in\_GCN                                                    & 1.285±0.155         & 0.502±0.172          & 15.342±4.219          & 0.042±0.024          \\
\begin{tabular}[c]{@{}c@{}}W/o\_dilation\_\\ in\_GCN\end{tabular}    & 1.331±0.152         & 0.507±0.175          & \textbf{15.428±4.278} & 0.017±0.009          \\
\begin{tabular}[c]{@{}c@{}}W/o\_NLss\_of\_\\ inter-modal\end{tabular} & 1.264±0.145         & 0.496±0.172          & 15.336±4.285          & 0.036±0.015          \\
Our                                                                  & \textbf{1.344±0.14} & \textbf{0.513±0.172} & 15.416±4.364          & \textbf{0.016±0.013} \\ \hline
\end{tabular}}
\end{table}


\begin{table}[]
\centering
\caption{Mean of running time of proposed method and other methods on three datasets. The bold values indicate the best model performance. (unit: second)}
\setlength{\tabcolsep}{5.5 mm}
\renewcommand\arraystretch{1.1}{
\begin{tabular}{cccc}
\hline
           & TNO           & Roadscene     & M3FD          \\ \hline
CBF        & 10.95         & 5.97          & 57.06         \\
LP         & 0.16          & 0.08          & 1.3           \\
IFCNN      & 0.65          & 0.28          & 1.53          \\
FusionGAN  & 0.41          & 0.49          & 0.31          \\
DDcGAN     & 1.35          & 1.7           & 0.93          \\
CrossFuse  & \textbf{0.04} & 0.03          & \textbf{0.09} \\
SeAFusion  & 0.05          & \textbf{0.02} & 0.19          \\
SwinFusion & 1.62          & 0.78          & 3.97          \\
SEDRFuse   & 2.27          & 1.47          & 6.04          \\
Our        & 3.26          & 2.65          & 7.84          \\ \hline
\end{tabular}}
\end{table}

\section{Conclusion}
In this work, we introduce graph representation to address the issues that the existing CNN- or GAN-based methods capture features by designing deeper or complex networks without considering spatial non-local self-similarity. Therefore, our method transfers images into graph space and proposes a fusion method based on graph convolutional networks, which can adaptively utilize the neighborhood structure and latent interactions among intra- and inter-image positional distance to extract spatial non-local self-similarity of intra- and inter-modal, because the same or similar objects with far-space distance still have spatially repeatable detail or texture information. The qualitative and quantitative analysis of comparative and generalization experiments demonstrate that the proposed method has better performance than the other methods. In future work, we consider to combine graph representation with high-level vision tasks to fuse infrared and visible images, such as the semantic-driven fusion method.



% An example of a floating figure using the graphicx package.
% Note that \label must occur AFTER (or within) \caption.
% For figures, \caption should occur after the \includegraphics.
% Note that IEEEtran v1.7 and later has special internal code that
% is designed to preserve the operation of \label within \caption
% even when the captionsoff option is in effect. However, because
% of issues like this, it may be the safest practice to put all your
% \label just after \caption rather than within \caption{}.
%
% Reminder: the "draftcls" or "draftclsnofoot", not "draft", class
% option should be used if it is desired that the figures are to be
% displayed while in draft mode.
%
%\begin{figure}[!t]
%\centering
%\includegraphics[width=2.5in]{myfigure}
% where an .eps filename suffix will be assumed under latex, 
% and a .pdf suffix will be assumed for pdflatex; or what has been declared
% via \DeclareGraphicsExtensions.
%\caption{Simulation results for the network.}
%\label{fig_sim}
%\end{figure}

% Note that the IEEE typically puts floats only at the top, even when this
% results in a large percentage of a column being occupied by floats.


% An example of a double column floating figure using two subfigures.
% (The subfig.sty package must be loaded for this to work.)
% The subfigure \label commands are set within each subfloat command,
% and the \label for the overall figure must come after \caption.
% \hfil is used as a separator to get equal spacing.
% Watch out that the combined width of all the subfigures on a 
% line do not exceed the text width or a line break will occur.
%
%\begin{figure*}[!t]
%\centering
%\subfloat[Case I]{\includegraphics[width=2.5in]{box}%
%\label{fig_first_case}}
%\hfil
%\subfloat[Case II]{\includegraphics[width=2.5in]{box}%
%\label{fig_second_case}}
%\caption{Simulation results for the network.}
%\label{fig_sim}
%\end{figure*}
%
% Note that often IEEE papers with subfigures do not employ subfigure
% captions (using the optional argument to \subfloat[]), but instead will
% reference/describe all of them (a), (b), etc., within the main caption.
% Be aware that for subfig.sty to generate the (a), (b), etc., subfigure
% labels, the optional argument to \subfloat must be present. If a
% subcaption is not desired, just leave its contents blank,
% e.g., \subfloat[].


% An example of a floating table. Note that, for IEEE style tables, the
% \caption command should come BEFORE the table and, given that table
% captions serve much like titles, are usually capitalized except for words
% such as a, an, and, as, at, but, by, for, in, nor, of, on, or, the, to
% and up, which are usually not capitalized unless they are the first or
% last word of the caption. Table text will default to \footnotesize as
% the IEEE normally uses this smaller font for tables.
% The \label must come after \caption as always.
%
%\begin{table}[!t]
%% increase table row spacing, adjust to taste
%\renewcommand{\arraystretch}{1.3}
% if using array.sty, it might be a good idea to tweak the value of
% \extrarowheight as needed to properly center the text within the cells
%\caption{An Example of a Table}
%\label{table_example}
%\centering
%% Some packages, such as MDW tools, offer better commands for making tables
%% than the plain LaTeX2e tabular which is used here.
%\begin{tabular}{|c||c|}
%\hline
%One & Two\\
%\hline
%Three & Four\\
%\hline
%\end{tabular}
%\end{table}


% Note that the IEEE does not put floats in the very first column
% - or typically anywhere on the first page for that matter. Also,
% in-text middle ("here") positioning is typically not used, but it
% is allowed and encouraged for Computer Society conferences (but
% not Computer Society journals). Most IEEE journals/conferences use
% top floats exclusively. 
% Note that, LaTeX2e, unlike IEEE journals/conferences, places
% footnotes above bottom floats. This can be corrected via the
% \fnbelowfloat command of the stfloats package.




%\section{Conclusion}
%The conclusion goes here.



% if have a single appendix:
%\appendix[Proof of the Zonklar Equations]
% or
%\appendix  % for no appendix heading
% do not use \section anymore after \appendix, only \section*
% is possibly needed

% use appendices with more than one appendix
% then use \section to start each appendix
% you must declare a \section before using any
% \subsection or using \label (\appendices by itself
% starts a section numbered zero.)
%


%\appendices
%\section{Proof of the First Zonklar Equation}
%Appendix one text goes here.
%
%% you can choose not to have a title for an appendix
%% if you want by leaving the argument blank
%\section{}
%Appendix two text goes here.


% use section* for acknowledgment
%\section*{Acknowledgment}
%
%
%The authors would like to thank...


% Can use something like this to put references on a page
% by themselves when using endfloat and the captionsoff option.
\ifCLASSOPTIONcaptionsoff
  \newpage
\fi



% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://mirror.ctan.org/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
%\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)
%%\begin{thebibliography}{1}
%%
%%\bibitem{IEEEhowto:kopka}
%%H.~Kopka and P.~W. Daly, \emph{A Guide to \LaTeX}, 3rd~ed.\hskip 1em plus
%%  0.5em minus 0.4em\relax Harlow, England: Addison-Wesley, 1999.
%%
%%\end{thebibliography}


\bibliographystyle{IEEEtran} 
%\bibliography{karim2022current,zhang2023visible,gao2022dcdr,ma2019infrared,tang2023datfuse,li2017pixel,ma2019locality,liu2018deep,xu2020u2fusion,yao2023laplacian,tang2022superfusion,zhou2021semantic,Zhao2022eff,li2020attentionfgan,li2019coupled,shreyamsha2015image,burt1987laplacian,zhang2018sparse,zhu2018novel,bavirisetti2016two,han2013fast,mitianoudis2007pixel,cui2015detail,liu2015general,ma2016infrared,xu2022infrared,yue2023dif, ren2021infrared,tian2021depth,li2022cgtf,vs2021image,ma2022swinfusion,wang2022swinfuse,tang2022image,jian2020sedrfuse,li2020multigrained,wang2023cross,zhang2020ifcnn,li2021rfn,li2021different,jian2021infrared,ma2019fusiongan,ma2020ddcgan,li2020infrared,ma2020ganmcc,Attention2017,devlin2019bert,zheng2021rethinking,carion2020end,dosovitskiy2021image,bai2020entropic,bai2020learning,micheli2009neural,atwood2016diffusion,niepert2016learning,bruna2013spectral,bastings2017graph,tang2009relational,wale2008comparison,simonovsky2017dynamic,landrieu2018large,han2022vision,yu2022edge,chen2023learnable,liu2020cnn,bao2022attention,hendrycks2016gaussian, fan2020correlation,valsesia2018learning,wang2004image,kumar2013multifocus,redmon2016you}
% Generated by IEEEtran.bst, version: 1.14 (2015/08/26)
\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{zhang2020ifcnn}
Y.~Zhang, Y.~Liu, P.~Sun, H.~Yan, X.~Zhao, and L.~Zhang, ``Ifcnn: A general
  image fusion framework based on convolutional neural network,''
  \emph{Information Fusion}, vol.~54, pp. 99--118, 2020.

\bibitem{ma2022swinfusion}
J.~Ma, L.~Tang, F.~Fan, J.~Huang, X.~Mei, and Y.~Ma, ``Swinfusion: Cross-domain
  long-range learning for general image fusion via swin transformer,''
  \emph{IEEE/CAA Journal of Automatica Sinica}, vol.~9, no.~7, pp. 1200--1217,
  2022.

\bibitem{karim2022current}
S.~Karim, G.~Tong, J.~Li, A.~Qadir, U.~Farooq, and Y.~Yu, ``Current advances
  and future perspectives of image fusion: A comprehensive review,''
  \emph{Information Fusion}, 2022.

\bibitem{zhang2023visible}
X.~Zhang and Y.~Demiris, ``Visible and infrared image fusion using deep
  learning,'' \emph{IEEE Transactions on Pattern Analysis and Machine
  Intelligence}, 2023.

\bibitem{ma2019infrared}
J.~Ma, Y.~Ma, and C.~Li, ``Infrared and visible image fusion methods and
  applications: A survey,'' \emph{Information Fusion}, vol.~45, pp. 153--178,
  2019.

\bibitem{tang2023datfuse}
W.~Tang, F.~He, Y.~Liu, Y.~Duan, and T.~Si, ``Datfuse: Infrared and visible
  image fusion via dual attention transformer,'' \emph{IEEE Transactions on
  Circuits and Systems for Video Technology}, 2023.

\bibitem{li2017pixel}
S.~Li, X.~Kang, L.~Fang, J.~Hu, and H.~Yin, ``Pixel-level image fusion: A
  survey of the state of the art,'' \emph{information Fusion}, vol.~33, pp.
  100--112, 2017.

\bibitem{ma2019locality}
J.~Ma, J.~Zhao, J.~Jiang, H.~Zhou, and X.~Guo, ``Locality preserving
  matching,'' \emph{International Journal of Computer Vision}, vol. 127, pp.
  512--531, 2019.

\bibitem{liu2018deep}
Y.~Liu, X.~Chen, Z.~Wang, Z.~J. Wang, R.~K. Ward, and X.~Wang, ``Deep learning
  for pixel-level image fusion: Recent advances and future prospects,''
  \emph{Information Fusion}, vol.~42, pp. 158--173, 2018.

\bibitem{xu2020u2fusion}
H.~Xu, J.~Ma, J.~Jiang, X.~Guo, and H.~Ling, ``U2fusion: A unified unsupervised
  image fusion network,'' \emph{IEEE Transactions on Pattern Analysis and
  Machine Intelligence}, vol.~44, no.~1, pp. 502--518, 2020.

\bibitem{tang2022superfusion}
L.~Tang, Y.~Deng, Y.~Ma, J.~Huang, and J.~Ma, ``Superfusion: A versatile image
  registration and fusion network with semantic awareness,'' \emph{IEEE/CAA
  Journal of Automatica Sinica}, vol.~9, no.~12, pp. 2121--2137, 2022.

\bibitem{zhou2021semantic}
H.~Zhou, W.~Wu, Y.~Zhang, J.~Ma, and H.~Ling, ``Semantic-supervised infrared
  and visible image fusion via a dual-discriminator generative adversarial
  network,'' \emph{IEEE Transactions on Multimedia}, 2021.

\bibitem{li2020attentionfgan}
J.~Li, H.~Huo, C.~Li, R.~Wang, and Q.~Feng, ``Attentionfgan: Infrared and
  visible image fusion using attention-based generative adversarial networks,''
  \emph{IEEE Transactions on Multimedia}, vol.~23, pp. 1383--1396, 2020.

\bibitem{li2019coupled}
Q.~{Li}, L.~{Lu}, Z.~{Li}, W.~{Wu}, Z.~{Liu}, G.~{Jeon}, and X.~{Yang},
  ``Coupled gan with relativistic discriminators for infrared and visible
  images fusion,'' \emph{IEEE Sensors Journal}, pp. 1--1, 2019.

\bibitem{shreyamsha2015image}
B.~Shreyamsha~Kumar, ``Image fusion based on pixel significance using cross
  bilateral filter,'' \emph{Signal, image and video processing}, vol.~9, pp.
  1193--1204, 2015.

\bibitem{burt1987laplacian}
P.~J. Burt and E.~H. Adelson, ``The laplacian pyramid as a compact image
  code,'' in \emph{Readings in computer vision}.\hskip 1em plus 0.5em minus
  0.4em\relax Elsevier, 1987, pp. 671--679.

\bibitem{zhang2018sparse}
Q.~Zhang, Y.~Liu, R.~S. Blum, J.~Han, and D.~Tao, ``Sparse representation based
  multi-sensor image fusion for multi-focus and multi-modality images: A
  review,'' \emph{Information Fusion}, vol.~40, pp. 57--75, 2018.

\bibitem{bavirisetti2016two}
D.~P. Bavirisetti and R.~Dhuli, ``Two-scale image fusion of visible and
  infrared images using saliency detection,'' \emph{Infrared Physics \&
  Technology}, vol.~76, pp. 52--64, 2016.

\bibitem{mitianoudis2007pixel}
N.~Mitianoudis and T.~Stathaki, ``Pixel-based and region-based image fusion
  schemes using ica bases,'' \emph{Information Fusion}, vol.~8, no.~2, pp.
  131--142, 2007.

\bibitem{liu2015general}
Y.~Liu, S.~Liu, and Z.~Wang, ``A general framework for image fusion based on
  multi-scale transform and sparse representation,'' \emph{Information Fusion},
  vol.~24, pp. 147--164, 2015.

\bibitem{xu2022infrared}
M.~Xu, L.~Tang, H.~Zhang, and J.~Ma, ``Infrared and visible image fusion via
  parallel scene and texture learning,'' \emph{Pattern Recognition}, vol. 132,
  p. 108929, 2022.

\bibitem{yue2023dif}
J.~Yue, L.~Fang, S.~Xia, Y.~Deng, and J.~Ma, ``Dif-fusion: Towards high color
  fidelity in infrared and visible image fusion with diffusion models,''
  \emph{arXiv preprint arXiv:2301.08072}, 2023.

\bibitem{ren2021infrared}
L.~Ren, Z.~Pan, J.~Cao, and J.~Liao, ``Infrared and visible image fusion based
  on variational auto-encoder and infrared feature compensation,''
  \emph{Infrared Physics \& Technology}, vol. 117, p. 103839, 2021.

\bibitem{tian2021depth}
F.~Tian, Y.~Gao, Z.~Fang, Y.~Fang, J.~Gu, H.~Fujita, and J.-N. Hwang, ``Depth
  estimation using a self-supervised network based on cross-layer feature
  fusion and the quadtree constraint,'' \emph{IEEE transactions on circuits and
  systems for video technology}, vol.~42, pp. 158--173, 2021.

\bibitem{li2022cgtf}
J.~Li, J.~Zhu, C.~Li, X.~Chen, and B.~Yang, ``Cgtf: Convolution-guided
  transformer for infrared and visible image fusion,'' \emph{IEEE Transactions
  on Instrumentation and Measurement}, 2022.

\bibitem{vs2021image}
V.~VS, J.~M.~J. Valanarasu, P.~Oza, and V.~M. Patel, ``Image fusion
  transformer,'' 2021.

\bibitem{wang2022swinfuse}
Z.~Wang, Y.~Chen, W.~Shao, H.~Li, and L.~Zhang, ``Swinfuse: A residual swin
  transformer fusion network for infrared and visible images,'' \emph{arXiv
  preprint arXiv:2204.11436}, 2022.

\bibitem{simonovsky2017dynamic}
M.~Simonovsky and N.~Komodakis, ``Dynamic edge-conditioned filters in
  convolutional neural networks on graphs,'' in \emph{Proceedings of the IEEE
  conference on computer vision and pattern recognition}, 2017, pp. 3693--3702.

\bibitem{valsesia2018learning}
D.~Valsesia, G.~Fracastoro, and E.~Magli, ``Learning localized generative
  models for 3d point clouds via graph convolution,'' in \emph{International
  conference on learning representations}, 2018.

\bibitem{li2021rfn}
H.~Li, X.~Wu, and J.~Kittler, ``Rfn-nest: An end-to-end residual fusion network
  for infrared and visible images,'' \emph{Information Fusion}, vol.~73, pp.
  72--86, 2021.

\bibitem{tang2022image}
L.~Tang, J.~Yuan, and J.~Ma, ``Image fusion in the loop of high-level vision
  tasks: A semantic-aware real-time infrared and visible image fusion
  network,'' \emph{Information Fusion}, vol.~82, pp. 28--42, 2022.

\bibitem{li2021different}
H.~Li, Y.~Cen, Y.~Liu, X.~Chen, and Z.~Yu, ``Different input resolutions and
  arbitrary output resolution: a meta learning-based deep framework for
  infrared and visible image fusion,'' \emph{IEEE Transactions on Image
  Processing}, vol.~30, pp. 4070--4083, 2021.

\bibitem{jian2021infrared}
L.~Jian, R.~Rayhana, L.~Ma, S.~Wu, Z.~Liu, and H.~Jiang, ``Infrared and visible
  image fusion based on deep decomposition network and saliency analysis,''
  \emph{IEEE Transactions on Multimedia}, 2021.

\bibitem{ma2019fusiongan}
J.~Ma, W.~Yu, P.~Liang, C.~Li, and J.~Jiang, ``Fusiongan: A generative
  adversarial network for infrared and visible image fusion,''
  \emph{Information Fusion}, vol.~48, pp. 11--26, 2019.

\bibitem{ma2020ddcgan}
J.~Ma, H.~Xu, J.~Jiang, X.~Mei, and X.-P. Zhang, ``Ddcgan: A dual-discriminator
  conditional generative adversarial network for multi-resolution image
  fusion,'' \emph{IEEE Transactions on Image Processing}, vol.~29, pp.
  4980--4995, 2020.

\bibitem{li2020infrared}
J.~Li, H.~Huo, K.~Liu, and C.~Li, ``Infrared and visible image fusion using
  dual discriminators generative adversarial networks with wasserstein
  distance,'' \emph{Information Sciences}, 2020.

\bibitem{ma2020ganmcc}
J.~Ma, H.~Zhang, Z.~Shao, P.~Liang, and H.~Xu, ``Ganmcc: A generative
  adversarial network with multiclassification constraints for infrared and
  visible image fusion,'' \emph{IEEE Transactions on Instrumentation and
  Measurement}, vol.~70, pp. 1--14, 2020.

\bibitem{li2020multigrained}
J.~Li, H.~Huo, C.~Li, R.~Wang, C.~Sui, and Z.~Liu, ``Multigrained attention
  network for infrared and visible image fusion,'' \emph{IEEE Transactions on
  Instrumentation and Measurement}, vol.~70, pp. 1--12, 2020.

\bibitem{devlin2019bert}
J.~Devlin, M.-W. Chang, K.~Lee, and K.~Toutanova, ``Bert: Pre-training of deep
  bidirectional transformers for language understanding,'' 2019.

\bibitem{zheng2021rethinking}
S.~Zheng, J.~Lu, H.~Zhao, X.~Zhu, Z.~Luo, Y.~Wang, Y.~Fu, J.~Feng, T.~Xiang,
  P.~H. Torr \emph{et~al.}, ``Rethinking semantic segmentation from a
  sequence-to-sequence perspective with transformers,'' in \emph{Proceedings of
  the IEEE/CVF conference on computer vision and pattern recognition}, 2021,
  pp. 6881--6890.

\bibitem{carion2020end}
N.~Carion, F.~Massa, G.~Synnaeve, N.~Usunier, A.~Kirillov, and S.~Zagoruyko,
  ``End-to-end object detection with transformers,'' in \emph{European
  conference on computer vision}.\hskip 1em plus 0.5em minus 0.4em\relax
  Springer, 2020, pp. 213--229.

\bibitem{dosovitskiy2021image}
A.~Dosovitskiy, L.~Beyer, A.~Kolesnikov, D.~Weissenborn, X.~Zhai,
  T.~Unterthiner, M.~Dehghani, M.~Minderer, G.~Heigold, S.~Gelly, J.~Uszkoreit,
  and N.~Houlsby, ``An image is worth 16x16 words: Transformers for image
  recognition at scale,'' 2021.

\bibitem{bai2020learning}
L.~Bai, L.~Cui, Y.~Jiao, L.~Rossi, and E.~R. Hancock, ``Learning backtrackless
  aligned-spatial graph convolutional networks for graph classification,''
  \emph{IEEE Transactions on Pattern Analysis and Machine Intelligence},
  vol.~44, no.~2, pp. 783--798, 2020.

\bibitem{micheli2009neural}
A.~Micheli, ``Neural network for graphs: A contextual constructive approach,''
  \emph{IEEE Transactions on Neural Networks}, vol.~20, no.~3, pp. 498--511,
  2009.

\bibitem{bruna2013spectral}
J.~Bruna, W.~Zaremba, A.~Szlam, and Y.~LeCun, ``Spectral networks and locally
  connected networks on graphs,'' \emph{arXiv preprint arXiv:1312.6203}, 2013.

\bibitem{bastings2017graph}
J.~Bastings, I.~Titov, W.~Aziz, D.~Marcheggiani, and K.~Sima'an, ``Graph
  convolutional encoders for syntax-aware neural machine translation,''
  \emph{arXiv preprint arXiv:1704.04675}, 2017.

\bibitem{tang2009relational}
L.~Tang and H.~Liu, ``Relational learning via latent social dimensions,'' in
  \emph{Proceedings of the 15th ACM SIGKDD international conference on
  Knowledge discovery and data mining}, 2009, pp. 817--826.

\bibitem{han2022vision}
K.~Han, Y.~Wang, J.~Guo, Y.~Tang, and E.~Wu, ``Vision gnn: An image is worth
  graph of nodes,'' \emph{arXiv preprint arXiv:2206.00272}, 2022.

\bibitem{yu2022edge}
C.~Yu, J.~Huang, M.~Song, Y.~Wang, and C.-I. Chang, ``Edge-inferring graph
  neural network with dynamic task-guided self-diagnosis for few-shot
  hyperspectral image classification,'' \emph{IEEE Transactions on Geoscience
  and Remote Sensing}, vol.~60, pp. 1--13, 2022.

\bibitem{fan2020correlation}
H.~Fan, H.-M. Hu, S.~Liu, W.~Lu, and S.~Pu, ``Correlation graph convolutional
  network for pedestrian attribute recognition,'' \emph{IEEE Transactions on
  Multimedia}, 2020.

\bibitem{chen2023learnable}
Z.~Chen, L.~Fu, J.~Yao, W.~Guo, C.~Plant, and S.~Wang, ``Learnable graph
  convolutional network and feature fusion for multi-view learning,''
  \emph{Information Fusion}, vol.~95, pp. 109--119, 2023.

\bibitem{liu2020cnn}
Q.~Liu, L.~Xiao, J.~Yang, and Z.~Wei, ``Cnn-enhanced graph convolutional
  network with pixel-and superpixel-level feature fusion for hyperspectral
  image classification,'' \emph{IEEE Transactions on Geoscience and Remote
  Sensing}, vol.~59, no.~10, pp. 8657--8671, 2020.

\bibitem{hendrycks2016gaussian}
D.~Hendrycks and K.~Gimpel, ``Gaussian error linear units (gelus),''
  \emph{arXiv preprint arXiv:1606.08415}, 2016.

\bibitem{jian2020sedrfuse}
L.~Jian, X.~Yang, Z.~Liu, G.~Jeon, M.~Gao, and D.~Chisholm, ``Sedrfuse: A
  symmetric encoder--decoder with residual block network for infrared and
  visible image fusion,'' \emph{IEEE Transactions on Instrumentation and
  Measurement}, vol.~70, pp. 1--15, 2020.

\bibitem{wang2023cross}
Z.~Wang, W.~Shao, Y.~Chen, J.~Xu, and L.~Zhang, ``A cross-scale iterative
  attentional adversarial fusion network for infrared and visible images,''
  \emph{IEEE Transactions on Circuits and Systems for Video Technology}, 2023.

\bibitem{wang2004image}
Z.~Wang, A.~C. Bovik, H.~R. Sheikh, and E.~P. Simoncelli, ``Image quality
  assessment: from error visibility to structural similarity,'' \emph{IEEE
  transactions on image processing}, vol.~13, no.~4, pp. 600--612, 2004.

\bibitem{ma2016infrared}
J.~Ma, C.~Chen, C.~Li, and J.~Huang, ``Infrared and visible image fusion via
  gradient transfer and total variation minimization,'' \emph{Information
  Fusion}, vol.~31, pp. 100--109, 2016.

\bibitem{kumar2013multifocus}
B.~S. Kumar, ``Multifocus and multispectral image fusion based on pixel
  significance using discrete cosine harmonic wavelet transform,''
  \emph{Signal, Image and Video Processing}, vol.~7, no.~6, pp. 1125--1143,
  2013.

\bibitem{redmon2016you}
J.~Redmon, S.~Divvala, R.~Girshick, and A.~Farhadi, ``You only look once:
  Unified, real-time object detection,'' in \emph{Proceedings of the IEEE
  conference on computer vision and pattern recognition}, 2016, pp. 779--788.

\end{thebibliography}
% biography section
% 
% If you have an EPS/PDF photo (graphicx package needed) extra braces are
% needed around the contents of the optional argument to biography to prevent
% the LaTeX parser from getting confused when it sees the complicated
% \includegraphics command within an optional argument. (You could create
% your own custom macro containing the \includegraphics command to make things
% simpler here.)
%\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{mshell}}]{Michael Shell}
% or if you just want to reserve a space for a photo:

%%%%%%%%%%%%%%%%\begin{IEEEbiography}{Michael Shell}
%%%%%%%%%%%%%%%%Biography text here.
%%%%%%%%%%%%%%%%\end{IEEEbiography}
%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%% if you will not have a photo at all:
%%%%%%%%%%%%%%%%\begin{IEEEbiographynophoto}{John Doe}
%%%%%%%%%%%%%%%%Biography text here.
%%%%%%%%%%%%%%%%\end{IEEEbiographynophoto}
%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%% insert where needed to balance the two columns on the last page with
%%%%%%%%%%%%%%%%% biographies
%%%%%%%%%%%%%%%%%\newpage
%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%\begin{IEEEbiographynophoto}{Jane Doe}
%%%%%%%%%%%%%%%%Biography text here.
%%%%%%%%%%%%%%%%\end{IEEEbiographynophoto}

% You can push biographies down or up by placing
% a \vfill before or after them. The appropriate
% use of \vfill depends on what kind of text is
% on the last page and whether or not the columns
% are being equalized.

%\vfill

% Can be used to pull up biographies so that the bottom of the last one
% is flush with the other column.
%\enlargethispage{-5in}



% that's all folks
\end{document}


