\documentclass[11pt]{article}

%--------- Packages -----------
% \usepackage[margin=1in]{geometry}
\usepackage[margin=1in,footskip=0.25in]{geometry}
\usepackage{fullpage}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{color}
\usepackage{enumitem}
\usepackage{url}
\usepackage{listings}
\usepackage{bbm}
\usepackage{matlab-prettifier}
\usepackage{mathtools}
\usepackage{caption}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{subcaption}
\usepackage{float}
\usepackage{hyperref}

%-------------Notation--------------
%\usepackage{algorithm,algorithmic}
\usepackage{shorthands}
%----------Spacing-------------
\setlength{\topmargin}{-0.6 in}
\setlength{\headsep}{0.75 in}
\setlength{\parindent}{0 in}
\setlength{\parskip}{0.1 in}
\setlength{\hoffset}{0 in}



%----------Header--------------
\title{On a Relation Between the Rate-Distortion Function and \\Optimal Transport}
\author{Eric Lei}


%========= BEGIN DOCUMENT =========

\begin{document}
    \maketitle

    \section{Introduction}
    \subsection{Rate-Distortion}

    Let $X \sim P_X$ be the random variable we wish to compress, supported on $\mathcal{X}$. Let $\mathcal{Y}$ be the reproduction space, and $\dist: \mathcal{X} \times \mathcal{Y} \rightarrow \mathbb{R}_{\geq 0}$ be a distortion measure. The asymptotic limit on the minimum number of bits required to achieve average distortion $D$ is given by the rate-distortion function \cite{shannonRD, CoverThomas},  defined as
 %   \begin{definition} The rate-distortion function $R(D)$ of a source $P_X$ under distortion function $\dist$ is given by 
        \begin{equation}
    	    R(D) := \inf_{\substack{P_{Y|X}:  \EE_{P_{X,Y}}[\dist(X,Y)]  \leq D}} I(X;Y), 
    	    \label{eq:RD}
    	\end{equation} 
   % \end{definition} 
    where $P_{X,Y}=P_X P_{Y|X}$ is a joint distribution. Any rate-distortion pair $(R,D)$ satisfying $R > R(D)$ is achievable by some lossy source code, and no code can achieve a rate-distortion less than $R(D)$. It is important to note that $R(D)$ is achievable only under asymptotic blocklengths.
    

    Let $\DKL(\mu || \nu)$ be the Kullback-Leibler (KL) divergence, defined as $\EE_\mu\bracket{\log \frac{d\mu}{d\nu}}$ when the density $\frac{d\mu}{d\nu}$ exists and $+\infty$ otherwise. $R(D)$ has the following alternate form \cite[Ch.~10]{CoverThomas},
    \begin{equation}
    R(D) = \inf_{Q_Y} \inf_{\substack{P_{Y|X}:  \EE_{P_{X,Y}}[\dist(X,Y)]  \leq D}} \DKL(P_{X,Y}||P_X \otimes Q_Y).
    \label{eq:RD_alt}
    \end{equation}
    Due to the convex and strictly decreasing properties \cite{CoverThomas} of $R(D)$, it suffices to fix some $\lambda > 0$, and solve 
    \begin{equation}
        \inf_{Q_Y} \inf_{P_{Y|X}}\DKL(P_{X,Y}|| P_X \otimes Q_Y) + \lambda \mathop{\EE}_{P_{X,Y}}[\dist(X,Y)].
        \label{eq:RD_alt_regl}
    \end{equation}

    

    % \begin{lemma}[Double-Minimization Form, cf. {\cite[Ch.~10]{CoverThomas}}, {\cite{Yeung2002AFC}}]
    %     The minimizers $P^{(\beta)}_{Y|X}$, $Q_Y^{(\beta)}$ of
    %     \begin{equation}
	   %   \RD(\beta):=\inf_{Q_Y} \inf_{P_{Y|X}   } \DKL(P_{X,Y}|| P_X \otimes Q_Y) + \beta \mathop{\EE}_{P_{X,Y}}[\dist(X,Y)]
	   %   \label{eq:double}
	   %  \end{equation}
	   %  yield a unique point $R_\beta = \DKL(P_XP^{(\beta)}_{Y|X} || P_X \otimes Q_Y^{(\beta)})$ and $D_\beta = \EE_{P_X P^{(\beta)}_{Y|X}} [\dist(X,Y)]$ on the positive-rate regime of the rate-distortion curve, i.e. $R(D_\beta) = R_\beta$, such that $D_\beta < D_{\mathrm{max}}$ where $R(D_{\mathrm{max}}) = 0$.
    %     \begin{proof}
    %     See \cite[Lemma~10.8.1]{CoverThomas}.
    %     \end{proof}
    % \end{lemma}
	
    In discrete settings,  the optimizers $Q_Y, P_{Y|X}$ satisfy
    \begin{align}
        \frac{dP_{Y|X=x}}{dQ_Y}(x,y) &= \frac{e^{-\lambda \dist(x,y)}}{\int_{\mathcal{Y}} e^{-\lambda \dist(x,\tilde{y})}dQ_Y} , \label{eq:BA_1} \\
        Q_Y &= \int_{\mathcal{X}}  dP_{Y|X} dP_X. 
        \label{eq:BA_2}
    \end{align}

    This solution corresponds to a point on $R(D)$ corresponding to $\lambda$. The Blahut-Arimoto (BA) algorithm \cite{blahut, arimoto} solves \eqref{eq:RD_alt} by alternating steps on $P_{Y|X}$ and $Q_Y$ until convergence. Similarly, NERD \cite{NERD} provides a variant for continuous sources when only samples are available. Sweeping over $\lambda$ gives the entire rate-distortion curve.
    

    \subsection{Optimal Transport}
    Optimal transport theory has two formulations; the Monge problem, and the Kantorovich relaxation. Intuitively, suppose we have two sets of points $\mathfrak{X} = \{x_1,\dots,x_n\}$ and $\mathfrak{Y} = \{y_1,\dots,y_n\}$. The Monge problem seeks to find a matching (i.e., a bijection) between $\mathfrak{X}$ and $\mathfrak{Y}$ that minimizes the average pairwise cost. The Kantovorich problem relaxes the bijection requirement by allowing each $x_i$ to be soft-matched to all $y_j$'s, i.e., each $x_i$ can send some mass to each $y_j$ such that the total mass sent sums to 1. In the discrete case, they have the same optimal solution, which is actually given by a hard-matching, i.e., a bijection. In the case for general measures, they are only equivalent under certain conditions. We will only discuss the Kantorovich case since that is most relevant.

    The Kantorovich problem seeks to find the minimum distortion coupling between two probability measures $\mu$ and $\nu$,
    \begin{equation}
        W(\mu, \nu) := \inf_{\substack{\pi \in \Pi(\mu, \nu)}} \EE_{X,Y\sim \pi}[\dist(X,Y)],
    \end{equation}
    where $\pi$ is a joint distribution that marginalizes to $\mu$ and $\nu$. The Kantorovich problem can be regularized with an entropy term, 
    \begin{equation}
        S_\epsilon(\mu, \nu) := \inf_{\substack{\pi \in \Pi(\mu, \nu)}} \EE_{\pi}[\dist(X,Y)] + \epsilon \DKL(\pi||\mu \otimes \nu),
        \label{eq:entropic_OT}
    \end{equation}
    which is known as entropy-regularized optimal transport, with regularization $\epsilon > 0$. For discrete measures $\mu,\nu$, \eqref{eq:entropic_OT} can be solved efficiently using the Sinkhorn-Knopp algorithm \cite{sinkhorn, knopp_sinkhorn}.

    A brief introduction to optimal transport theory can be found in \cite{CourseNotesOT, ComputationalOT}. 

    \section{Optimal Transport and the Rate-Distortion Function}
    \subsection{Equivalence of Extremal Sinkhorn Divergence and Rate Distortion}
    We first show that entropic OT can be used to upper bound $R(D)$. First, observe that the inner minimization problem in \eqref{eq:RD_alt_regl} looks similar to the entropic OT problem. Let us define 
    \begin{align}
        S(D) :=& \inf_{Q_Y} \inf_{\substack{\pi_{XY} \in \Pi(P_X, Q_Y): \\ \EE_{\pi_{X,Y}}[\dist(X,Y)]  \leq D }} \DKL(\pi_{X,Y}||P_X \otimes Q_Y) \label{eq:SD_1}\\ 
        =&\inf_{Q_Y} \inf_{\substack{P_{Y|X}: \\ \EE_{P_{X,Y}}[\dist(X,Y)]  \leq D \\ Q_Y = \int_{\mathcal{X}} dP_{Y|X}P_X}} \DKL(P_{X,Y}||P_X \otimes Q_Y), \label{eq:SD_2}
    \end{align}
    which we call the \emph{Sinkhorn-distortion function}. Similar to $R(D)$, we can trace out $S(D)$ by sweeping over $\lambda > 0$, and solving the inner minimization \eqref{eq:entropic_OT} by setting  $\epsilon = \frac{1}{\lambda}$, and then optimizing over all $Q_Y$ for the outer minimization, which is convex in $Q_Y$ \cite{feydy2019interpolating}. We call this \texttt{SinkhornRD}. A variant of the Sinkhorn-distortion function is often used to solve generative modeling tasks with Sinkhorn divergences \cite{sinkhornGAN, salimans2018improving, shen2020sinkhorn}, where one wishes to find some $Q_Y \approx P_X$ by solving $\min_{Q_Y} S_\epsilon (P_x, Q_Y)$. The neural methods parallel NERD.

	    
	    \begin{proposition}[Sinkhorn Distortion Upper Bound]
	    \label{prop:SD_UB}
	    Given source $P_X$ on $\mathcal{X}$, reconstruction space $\mathcal{Y}$, and distortion measure $\dist$,  
	    \begin{equation}
	        R(D) \leq S(D).
	        \label{eq:RD-Sinkhorn}
	    \end{equation}
	    \end{proposition}
	   \begin{proof}
            The inner minimization problem of $R(D)$ in \eqref{eq:RD_alt} only has a marginal constraint on $P_X$, whereas the inner minimization of $S(D)$ in  \eqref{eq:SD_2} has an additional marginal constraint on $Q_Y$ as well.
    	\end{proof}
        
        

    	
    	% \begin{figure*}[t]
     %         \centering
     %         \begin{subfigure}[b]{0.475\textwidth}
     %             \centering
     %             \includegraphics[width=0.9\textwidth]{figures/RDcurve_discrete_bad.pdf}
     %             \caption{$R(D)$ (via Blahut-Arimoto) and entropic OT upper bound on a discrete source with 6 atoms.}
     %             \label{fig:simple}
     %         \end{subfigure}
     %         \hfill
     %         \begin{subfigure}[b]{0.475\textwidth}
     %             \centering
     %             \includegraphics[width=0.9\textwidth]{figures/RD_minmax_mnist_RD_sinkhorn.pdf}
     %             \caption{$R(D)$ (via NERD) and entropic OT upper bound on MNIST images.}  
     %            \label{fig:mnist_sinkhorn}
     %         \end{subfigure}
     %        \caption{Comparison of the RD-Sinkhorn objective with the true rate-distortion objective. }
     %     \end{figure*}
    	
    	% Under discrete settings, \eqref{eq:entropic_OT} can be solved efficiently via Sinkhorn iterations \cite{lightspeed}. We call the upper bound in \eqref{eq:RD-Sinkhorn} \textit{RD-Sinkhorn}. In order to illustrate the behavior of RD-Sinkhorn, we first examine the rate-distortion curve of a discretized Gaussian source over 6 atoms, shown in Fig.~\ref{fig:simple}. In such a setting, we can compute the true rate-distortion curve and RD-Sinkhorn easily and exactly, via Blahut-Arimoto and Sinkhorn iterations respectively.  Although RD-Sinkhorn traces a curve that is not convex, we see that for large portions of the rate-distortion curve, the gap is small and RD-Sinkhorn empirically provides a good estimate of the true rate-distortion function. 
     
    	% We then apply RD-Sinkhorn to an MNIST source by parametrizing $Q_Y$ with a generator neural network, similar to the work in \cite{sinkhornGAN}. To estimate $R(D)$ in this setting, we use the neural estimator NERD \cite{NERD}, and compare the curves in Fig.~\ref{fig:mnist_sinkhorn}.


        Next, we show that without further assumptions, $R(D)$ and $S(D)$ are equivalent.
        \begin{theorem}[Sinkhorn-Rate-Distortion Equivalence]
            For any source $P_X$ and distortion function $\dist: \mathcal{X} \times \mathcal{Y} \rightarrow \mathbb{R}_{\geq 0}$, it holds that
            \begin{equation}
                R(D) = S(D).
            \end{equation}
        \end{theorem}
        \begin{proof}
            We know that the optimizing distributions for $R(D)$ must satisfy \eqref{eq:BA_1} and \eqref{eq:BA_2} simultaneously. To show that $S(D)$ achieves the same objective as $R(D)$ on the same $P_X$ and distortion measure, it suffices to show that the $R(D)$-optimal $Q_Y^*$ and $P_{Y|X}^*$ are feasible for $S(D)$. From \cite[Ch.~4, Prop.~4.3]{ComputationalOT}, the optimal coupling in entropic OT is unique and has the form 
            \begin{equation}
            \frac{d\pi}{dP_X dQ_Y} (x,y) = u(x) e^{-\lambda \dist(x,y)} v(y)
            \end{equation}
            where $u(x), v(y)$ are dual variables that ensure valid couplings. The $R(D)$-optimal joint distribution $P_XP_{Y|X}^*$, which is guaranteed to be a coupling between $P_X$ and $Q_Y^*$ due to \eqref{eq:BA_2}, indeed has the form
            \begin{equation}
                \frac{dP_XP_{Y|X}^*}{dP_X dQ_Y^*}(x,y) = \frac{1}{\int_{\mathcal{Y}}e^{-\lambda \dist(x,y')}dQ_Y^*} \cdot e^{-\lambda \dist(x,y)} \cdot 1
            \end{equation}
           where the first term only depends on $x$ and the last term only depends on $y$.
            Since from Prop.~\ref{prop:SD_UB}, $R(D)$ is a lower bound of $S(D)$, we are done.
        \end{proof}
        \begin{remark}
            This result implies that (i) Sinkhorn generative models are equivalent to solving $R(D)$, and (ii) the proposed \emph{\texttt{SinkhornRD}} algorithm can also solve $R(D)$ and is an alternative to Blahut-Arimoto. \textcolor{blue}{Perhaps some implications of the rate-distortion optimal codebook from the lens of entropic OT?}
        \end{remark}

        \begin{figure}
            \centering
            \includegraphics[width=0.5\linewidth]{figures/5atoms_comparison.pdf}
            \caption{Equivalence of $S(D)$ and $R(D)$ on a discrete source with $\dist(x,y)=(x-y)^2$. }
            \label{fig:SD-RD}
        \end{figure}

        We numerically verify the equivalence in Fig.~\ref{fig:SD-RD} on a discrete source with 5 atoms under squared-error distortion. For $R(D)$, we use Blahut-Arimoto, and for $S(D)$, we solve the convex problem using sequential quadratic programming \cite{SLSQP, 2020SciPy} with $Q \mapsto S_{\epsilon}(P_X, Q)$ as the objective function, showing that the two different objectives result in the same function.

        \subsection{Entropic OT and the Rate Function}
        \begin{proposition}
            We have that 
            \begin{equation}
                R(D, Q_Y) = S(D, Q_Y)
            \end{equation}
            if and only if 
            \begin{equation}
                \int_{\mathcal{X}} \frac{e^{-\lambda \dist(x,y)}}{\int_{\mathcal{Y}} e^{-\lambda \dist(x,y')} dQ_Y} dP_X = 1, \quad \forall y \in \mathcal{Y}.
                \label{eq:rate_eq_cond}
            \end{equation}
        \end{proposition}
        \begin{proof}
            Note that for a given $Q_Y$, the solution to $R(D,Q_Y)$ is given by \eqref{eq:BA_1}. The optimizing $P_{Y|X}$, however, does not necessarily produce a valid coupling for the joint $P_X P_{Y|X}$ (we only know this holds for the $R(D)$-optimal $Q_Y$). In order for $P_XP_{Y|X}$ to be a valid coupling, the $Q_Y$ marginal constraint must be satisfied, i.e. $\forall y \in \mathcal{Y}$,
            \begin{align}
                \int_{\mathcal{X}} dP_{Y|X}(y) dP_X = dQ_Y(y) &\iff \int_{\mathcal{X}} \frac{e^{-\lambda \dist(x,y)}}{\int_{\mathcal{Y}} e^{-\lambda \dist(x,y')} dQ_Y} dQ_Y(y) dP_X(x) = dQ_Y(y)\\
                & \iff \int_{\mathcal{X}} \frac{e^{-\lambda \dist(x,y)}}{\int_{\mathcal{Y}} e^{-\lambda \dist(x,y')} dQ_Y} dP_X = 1.
            \end{align}
            When a coupling is satisfied, $P_XP_{Y|X}$ is feasible for $S(D, Q_Y)$, and since $R(D, Q_Y) \leq S(D, Q_Y)$, we are done.
        \end{proof}
        \begin{corollary}
            Equivalent when $P_X$ and $Q_Y$ are uniform discrete and the distortion matrix is symmetric (rows and columns are permutations of eachother). Note that \eqref{eq:BA_1} immediately provides the optimal Sinkhorn coupling in this case (matching two sets of points with symmetric cost). Also, note that this induces a weakly symmetric $R(D)$-optimal channel.
        \end{corollary}
        \begin{proof}
            When $P_X = \text{Unif}(\mathcal{X})$, $Q_Y = \text{Unif}(\mathcal{Y})$, \eqref{eq:rate_eq_cond} evaluates to 
            \begin{equation}
                \sum_{x \in \mathcal{X}} 
            \end{equation}
        \end{proof}
        

    \section{Optimal Transport and Channel Capacity}
    Given a channel $p(y|x)$, the channel capacity \cite{shannon48} describes its maximal rate of communication, given by 
    \begin{equation}
        C = \max_{r(x)} I(X;Y).
        \label{eq:capacity}
    \end{equation}
    We show that \eqref{eq:capacity} can also be interpreted as an extremal optimal transport problem. 
    \begin{theorem}
        For any channel $p(y|x)$, we have that 
        \begin{equation}
            C = 2\cdot  \max_{r(x)} S_{\frac{1}{2}}(r(x), r(y)),
        \end{equation}
        where $r(y) := \sum_{x \in \mathcal{X}} p(y|x)r(x)$, and the cost function that induces the Sinkhorn problem in \eqref{eq:entropic_OT} is given by $\dist(x,y) = - \log \frac{p(y|x)}{r(y)}$. 
        \begin{proof}
            Using \cite[Lemma.~10.8.1]{CoverThomas}, we have that 
            \begin{equation}
                C = \max_{r(x)} \min_{q(x|y)} \EE_{r(x)p(y|x)} \bracket*{-\log \frac{q(x|y)}{r(x)}},
                \label{eq:capacity_alt}
            \end{equation}
            with the optimal $q^*(x|y)$ satisfying $q^*(x|y)r(y) = r(x)p(y|x)$. Hence, the inner problem in \eqref{eq:capacity_alt} under $q^*(x|y)$ can be rewritten as 
            \begin{align}
                \EE_{r(x)p(y|x)}\bracket*{-\log \frac{q^*(x|y)}{r(x)}} &= 2\EE_{r(y)q^*(x|y)} \bracket*{-\log \frac{p(y|x)}{r(y)}} + \EE_{r(y)q^*(x|y)} \bracket*{\log \frac{q^*(x|y)}{r(x)}} \\
                &= 2\EE_{r(y)q^*(x|y)} \bracket*{\dist(x,y)} + \DKL(r(y)q^*(x|y)||r(y)r(x)). \label{eq:optimal_sinkhorn}
            \end{align}
            To show that \eqref{eq:optimal_sinkhorn} is equal to $2 \cdot S_{\frac{1}{2}}(r(x), r(y))$, we again use the fact from \cite[Ch.~4]{ComputationalOT} that the optimal Sinkhorn coupling is unique and has the form $\pi(x,y) = u(x) e^{-2 \dist()$
        \end{proof}
    \end{theorem}
    

    

    \section{Applications}
    Potentially:
    \begin{enumerate}
        \item Optimal transport lens of information theory
        \item Operational meaning of codes, coding for optimal transport/matching?
        \item Connecting generalization bounds in learning theory that use optimal transport and rate-distortion theory
    \end{enumerate}


\bibliographystyle{ieeetr} 
\bibliography{ref}



\end{document}


