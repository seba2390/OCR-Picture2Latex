
\section{Comparison With Regular Packet-based Traffic Classification}
\label{sec:policy}

To highlight the benefit of combining flow-based and packet-based traffic classification in FPHTC, we compare it against regular packet-based traffic classification without the help from a flow-based classifier. In this section, we provide a theoretical justification of why FPHTC performs better than regular packet-based traffic classification.

In regular packet-based traffic classification, a decision tree at the routing policy designer is trained with the true labels of the packets. In contrast, in FPHTC only the flow-based classifier requires true labels.  Hence, for fair comparison, we maintain that the training dataset size of the regular packet-based traffic classifier is equal to the training dataset size of the flow-based classifier in FPHTC, when both are measured in terms of the number of flows.

Let $n$ be the number of flows in the training dataset for the routing policy designer in FPHTC. Recall that these flows are labeled by the flow-based classifier. The flow-based classifier is trained with a $\lambda$ fraction of these flows, which have been labeled by DPI. If the cost of labeling each flow using DPI is $c_\text{DPI}$, then the total cost is $n\lambda c_\text{DPI}$. Increasing $\lambda$ will lead to a more accurate flow-based classifier in FPHTC and ultimately a more accurate routing policy. However, this will also result in a greater cost of labeling flows. Thus, there exists a trade-off that should be carefully analyzed.

Suppose $\mathcal{H}$ is the hypothesis set of a classifier with some capacity measure $|\mathcal{H}|_C$. If $\hat{f}\in\mathcal{H}$ is the function learned by the classifier from $n$ training samples, and $f$ is the ground truth, i.e., the target function of interest, then the generalization bound can be expressed as follows \cite{Vapnik16}:
\begin{equation}
R(\hat{f})-R(f)\leq O\left(\frac{|\mathcal{H}|_C}{n^r}\right) + \epsilon,
\label{eq:g}
\end{equation}
where $R(\cdot)$ is the expected loss, the $O(\cdot)$ term is the estimation error, and $\epsilon$ is the approximation error. The rate of learning is given by $O(n^{-r})$. For difficult or \textit{non-separable} problems, $r=\frac{1}{2}$ and this represents a slow rate of learning. In contrast, for easy or \textit{separable} problems, where the trained classifier makes no training error, $r=1$ and this represents a fast rate of learning \cite{Vapnik16}.

In FPHTC, the flow-based classifier and the routing policy designer play the role of the teacher and the student respectively. Let $\mathcal{H}_\text{fl}$ be the hypothesis set for the flow-based classifier, with capacity measure $|\mathcal{H}_\text{fl}|_C$. Let $f_\text{fl}\in\mathcal{H}_\text{fl}$ be the function learned by the flow-based classifier and $f$ be the ground truth. Since only a fraction $\lambda$ of the flows are used for training, the generalization bound of the flow-based classifier is given by
\begin{equation}
R(f_\text{fl})-R(f)\leq O\left(\frac{|\mathcal{H}_\text{fl}|_C}{n\lambda}\right) + \epsilon_\text{fl},
\label{eq:t}
\end{equation}       
where $\epsilon_\text{fl}$ is the approximation error of the flow-based classifier. Here we use a common assumption that the rate of learning for the more sophisticated teacher is inversely proportional to the size of the training dataset, i.e., $O((n\lambda)^{-1})$. 

Similarly, let $\mathcal{H}_\text{rp}$ be the hypothesis set of the routing policy designer in FPHTC with capacity measure $|\mathcal{H}_\text{rp}|_C$, and let $f_\text{rp} \in \mathcal{H}_\text{rp}$ be the function determined by the routing policy designer.  We have   
\begin{equation}
R(f_\text{rp})-R(f_\text{fl})\leq O\left(\frac{|\mathcal{H}_\text{rp}|_C}{n^\alpha}\right) + \epsilon_\text{rp},
\label{eq:st}
\end{equation}
where $\epsilon_\text{rp}$ is the approximation error of the routing policy. As the student learns using the teacher's predictions, the decision boundary of the original classification problem has been translated to a smoother one. Thus, the student, aided by the teacher's predictions, learns at a faster rate than with the true labels, so that its rate of learning is represented by the parameter $0.5\leq\alpha\leq 1$. 

Combining (\ref{eq:t}) and (\ref{eq:st}), we get
\begin{equation}
\begin{split}
R(f_\text{rp})-R(f)&=R(f_\text{rp})-R(f_\text{fl})+R(f_\text{fl})-R(f)\\
&\leq O\left(\frac{|\mathcal{H}_\text{rp}|_C}{n^\alpha}\right) + \epsilon_\text{rp} + O\left(\frac{|\mathcal{H}_\text{fl}|_C}{n\lambda}\right) + \epsilon_\text{fl}\\
&\leq O\left(\frac{\lambda^\alpha|\mathcal{H}_\text{rp}|_C+|\mathcal{H}_\text{fl}|_C}{n^\alpha\lambda^\alpha}\right) + \epsilon_\text{rp}+ \epsilon_\text{fl}.
\end{split}
\label{eq:kd}
\end{equation}
This gives the generalization bound for FPHTC. We note that this bound improves as $\lambda$ increases. 

As a further step for system optimization, we can consider a weighted sum of the generalization bound and the DPI labeling cost as a function of $\lambda$:
\begin{equation}
C(\lambda) = K\cdot\frac{\lambda^\alpha|\mathcal{H}_\text{rp}|_C+|\mathcal{H}_\text{fl}|_C}{n^\alpha\lambda^\alpha} + \epsilon_\text{rp}+ \epsilon_\text{fl} + n\lambda c_\text{DPI},
\label{eq:cost}
\end{equation} 
where $K$ is a constant of proportionality. To minimize $C(\lambda)$, we differentiate (\ref{eq:cost}) wrt $\lambda$ and set the derivative equal to zero. Finally, we obtain the optimal $\lambda$ for FPHTC as
\begin{equation}
\lambda^* =\left(\frac{\alpha K|\mathcal{H}_\text{fl}|_C}{n^{1+\alpha} c_\text{DPI}}\right)^{\frac{1}{1+\alpha}}.
\label{eq:opt}
\end{equation}

In regular packet-based traffic classification, we have the same hypothesis set $\mathcal{H}_\text{rp}$, since it represents the capability of the same router. The function $f_\text{pk}\in\mathcal{H}_\text{rp}$ is chosen to approximate the ground truth $f$ without the help of the teacher. Again, we can bound the regular packet-based traffic classifier as follows:
\begin{equation}
R(f_\text{pk})-R(f)\leq O\left(\frac{|\mathcal{H}_\text{rp}|_C}{\sqrt{n\lambda}}\right) + \epsilon_\text{pk},
\label{eq:s}
\end{equation}
where $\epsilon_\text{pk}$ is the approximation error of the regular packet-based traffic classifier. As the student is trained using true labels in this case, the classification problem is difficult and the rate of learning is slower at $O((n\lambda)^{-1/2})$. Comparing (\ref{eq:s}) with (\ref{eq:kd}), we see that FPHTC outperforms regular packet-based traffic classification if the following inequality holds:
\begin{equation}
O\left(\frac{\lambda^\alpha|\mathcal{H}_\text{rp}|_C+|\mathcal{H}_\text{fl}|_C}{n^\alpha\lambda^\alpha}\right) + \epsilon_\text{rp}+ \epsilon_\text{fl}
\leq
O\left(\frac{|\mathcal{H}_\text{rp}|_C}{\sqrt{n\lambda}}\right) + \epsilon_\text{pk}.
\label{eq: ineq}
\end{equation} 

Let us now explain why it is reasonable for (\ref{eq: ineq}) to hold in our traffic classification problem. As the teacher is a highly complex flow-based classifier and the student is a simple decision tree trained at the routing policy designer, we have $|\mathcal{H}_\text{fl}|_C>>|\mathcal{H}_\text{rp}|_C$. Hence, FPHTC may be viewed as an instance of Hinton's knowledge distillation framework \cite{Hinton15}, except that in our case the feature spaces of the teacher and the student are different. Furthermore, the flow-based classifier is trained with many flow-level statistical features, whereas the routing policy is designed based on a much smaller number of packet-level features. Therefore, the approximation error of the routing policy is much larger than that of the flow-based classifier. Thus, in (\ref{eq: ineq}), $\epsilon_\text{fl}+\epsilon_\text{rp}<<\epsilon_\text{pk}$. Furthermore, as $\alpha\geq 0.5$, we have $n^\alpha\lambda^\alpha\geq \sqrt{n\lambda}$ when $\lambda\leq 1$. Therefore, even though $|\mathcal{H}_\text{fl}|_C>>|\mathcal{H}_\text{rp}|_C$, the large value of $|\mathcal{H}_\text{fl}|_C$ can be balanced by the parameters $\alpha$ and $\lambda$.

Note that $\alpha$ is an intrinsic parameter that we cannot control, but we can control $\lambda$. On the one hand, if $\lambda$ is close to 1, there is no benefit from using $\lambda$. On the other hand, if $\lambda$ is too small, both $n^\alpha\lambda^\alpha$ and $\sqrt{n\lambda}$ approaches zero and the difference between $\alpha$ and 0.5 is lost. However, a moderate $\lambda$ can satisfy (\ref{eq: ineq}) and allow FPHTC to outperform regular packet-based traffic classification.
