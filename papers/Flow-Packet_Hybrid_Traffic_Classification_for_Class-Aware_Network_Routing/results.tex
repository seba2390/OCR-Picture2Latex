
\section{Experimental Evaluation}
\label{sec:results}

In this section, we first discuss our experimental setup. Then, we evaluate the performance of FPHTC and compare it against regular packet-based traffic classification to demonstrate the impacts of the training dataset size, and the online setting.

\subsection{Dataset and Learning Models}

We use the combined real-world traffic traces of ISCX VPN-nonVPN (2016) \cite{Vpn16, Vpn_url} and ISCX Tor-nonTor (2016) \cite{Tor17, Tor_url}. It contains pcap files for 43590 encrypted TCP bidirectional flows from 8 application types. We group these 8 application types into 3 CoS categories as shown in Table \ref{tab:cos}. For flow-based classification, we extract 268 features from each TCP flow. Our current set of features includes source and destination IP addresses in addition to the list of 266 features used in \cite{Chowdhury19}. 

We remove the flows with no payload from our dataset and use 90\% of the rest of the dataset as the full training dataset, plus 10\% for testing. In various experiments that require different training dataset sizes, we use a randomly selected subset of the full training dataset. All training datasets are balanced by applying the \textit{sklearn.utils.resample} function from scikit-learn v0.21.3 \cite{sklearn} so that the number of training samples in each CoS class is the same. All machine learning models are implemented in Python 3.8.3. We use balanced test accuracy as the main performance metric, which is the  average of the proportion of correctly classified samples in each class. This performance metric is not affected by the imbalance in the dataset.

The routing policy designer takes labeled packets as input with 4 features: source IP address, destination IP address, source port number, and destination port number. The 32-bit IPv4 addresses are converted to decimal numbers to be used as feature values. Given a set of labeled flows, we obtain the corresponding set of unique packets having these 4 features. Note that these 4 features guarantee that all packets from a particular TCP flow are mapped to the same routing output.

We note that the traffic dataset is structured. For this type of data, it is known that the gradient boosted tree ensemble is appropriate as a learning model \cite{Kdd15}. We use the state-of-the-art gradient boosted tree ensembles XGBoost \cite{Xgb16} and LightGBM \cite{Lgbm17} as the flow-based classifier. In our experiments, the XGBoost model is trained with 100 trees in the ensemble. The learning rate is 0.3 and the maximum depth of each tree is limited to 6. For LightGBM, we use the gradient-based one-side sampling (GOSS) boosting method. The number of trees in the ensemble is 100 with the number of leaves and maximum depth restricted to 31 and unlimited, respectively. The learning rate remains 0.3.  

The routing policy designer trains a single CART with the predictions from the flow-based classifier as training targets. We use a decision tree classifier with balanced class weights and entropy as the criterion for choosing the best split. The parameters such as maximum depth and maximum leaf nodes, which determine the structure of the tree, are kept unlimited.

\begin{table}[t]
	\centering
	\begin{tabular}{|P{0.4\textwidth}|P{0.4\textwidth}|}
		\hline
		CoS label       & Application type    \\ \hline
		Delay Sensitive & CHAT, VOIP          \\ \hline
		Delay Moderate  & AUDIO, VIDEO        \\ \hline
		Delay Tolerant  & FTP, MAIL, P2P, WEB \\ \hline
	\end{tabular}
	\caption{Application types and CoS labels.}
	\label{tab:cos}
\end{table}


\subsection{Impact of Size of Training Datasets}

\begin{figure}[t]
	\centering
	\includegraphics[width=10cm]{"figures/hybrid1".pdf}
	\caption{Balanced test accuracy vs.~$n$, size of training dataset for the routing policy designer in FPHTC, with 90\% confidence interval.}
	\label{fig:hybrid1}
\end{figure}

\begin{table}[t]
	\centering
	\begin{tabular}{|P{0.15\textwidth}|P{0.15\textwidth}|P{0.15\textwidth}|P{0.15\textwidth}|}
		\hline
		\# Flows in the training dataset & Flow-based classifier& FPHTC & Regular packet-based \\ \hline
		1000                                                         & 79.59\% & 74.99\%  & 65.93\%             \\ \hline
		5000                                                         & 89.59\%  & 84.85\%  & 78.97\%             \\ \hline
		10000                                                       & 92.27\% & 87.13\%  & 83.85\%             \\
		\hline
	\end{tabular}
	\caption{Balanced test accuracy vs.~\# flows in the training dataset for the flow-based classifier and the regular packet-based classifier.}
	\label{tab:flow-based}
\end{table}      

The performance of FPHTC is shown in Fig.~\ref{fig:hybrid1} with 1000 flows in the training dataset of the flow-based classifier. We present the balanced test accuracy of FPHTC with 90\% confidence interval. We observe that it increases as the training dataset size for the routing policy designer is increased. Recall that, since this training dataset is generated by the flow-based classifier, there is no theoretical limit to its size. In contrast, the training dataset size for the regular packet-based traffic classifier remains the same as that of the flow-based classifier. Thus, we observe that FPHTC outperforms regular packet-based traffic classification when the training dataset for the routing policy designer becomes large enough. The performance gain of FPHTC is larger when the flow-based classifier and the regular packet-based traffic classifier are trained using a small training dataset. For example, when the flow-based classifier with LightGBM and the routing policy designer are trained using 1000 flows and the maximum number of available flows, i.e., full training dataset containing 34473 flows, respectively, FPHTC is about 9\% more accurate than regular packet-based traffic classification. Thus, FPHTC significantly improves accuracy in the low data regime.

We note that the performance of FPHTC is strictly ascending in Fig.~\ref{fig:hybrid1}. If we had more than 34473 unique flows in our available dataset for training, we could have further increased the training dataset size for the routing policy designer and observed a larger gain of FPHTC over regular packet-based traffic classification. Also, we see that using LightGBM as the flow-based classifier performs slightly better than using XGBoost. Therefore, we will present the results using LightGBM as the flow-based classifier for the rest of the experiments.

In Table \ref{tab:flow-based}, for various training dataset sizes, we present the balanced test accuracy of the flow-based classifier with LightGBM, regular packet-based traffic classifier, and FPHTC, where $n=34473$. The experiment has been repeated over 10 randomly chosen training and test sets and then the average accuracy is listed. We note that there is a significant gap between the accuracy of the flow-based classifier and the regular packet-based traffic classifier. The gap is especially large when we have a small training dataset. This gap is due to the many more features observed by the flow-based classifier than the regular packet-based traffic classifier. This confirms our rationale for this work, that there is room for improvement for the routing policy if it can utilize the knowledge learned by the flow-based classifier. Furthermore, we observe that FPHTC can substantially reduce that gap, especially when the amount of training data is small.


\subsection{Classification Performance of FPHTC in Online Setting}

Finally, we implement the routing policy update procedure of FPHTC in an online setting. In this experiment, we simply use the test accuracy of the routing policy as a feedback signal. If the accuracy is dropped below some accuracy threshold at the end of a time slot, re-training of the flow-based classifier and the routing policy update begin from the next time slot. Training stops when the accuracy crosses back above the accuracy threshold and the consecutive improvement in accuracy is less than some saturation threshold.


\begin{figure}[t]
	\centering
	\includegraphics[width=10cm]{"figures/online_FPHTC".pdf}
	\caption{Performance of FPHTC in online setting with traffic pattern changing after every 10 time slots.}
	\label{fig:online_FPHTC}
\end{figure}

\begin{table}[t]
	\centering
	\begin{tabular}{|P{0.27\textwidth}|P{0.35\textwidth}|}
		\hline
		& Application types in the dataset    \\ \hline
		At time slot 0 & AUDIO, FTP, VIDEO, VOIP, WEB          \\ \hline
		At time slot 10  & FTP, MAIL, P2P, VIDEO, VOIP      \\ \hline
		At time slot 20  & AUDIO, CHAT, FTP, MAIL, WEB\\ \hline
	\end{tabular}
	\caption{Change of traffic pattern over time.}
	\label{tab:pattern}
\end{table}

To simulate a never-ending stream of traffic, we keep shuffling our training dataset randomly. We run our experiment over 30 time slots, and we change the data pattern after every 10 time slots. To simulate traffic pattern change, we always use only a subset of 5 application types as the incoming traffic. After 10 time slots, we pick another subset of 5 application types and generate the incoming traffic. Thus, the test traffic pattern is changed at time slot 0, at time slot 10, and at time slot 20. The changing of applications over time is shown in Table \ref{tab:pattern}. 

At each time slot where re-training is needed, the first 1000 flows are selected and labeled by DPI to be used for training the flow-based classifier. Then, 10000 flows are labeled by the trained flow-based classifier and used for routing policy design. We use an accuracy threshold of 80\% and a saturation threshold of 1\% in our experiment. Fig.~\ref{fig:online_FPHTC} illustrates how the balanced test accuracy drops after every 10 time slots due to the traffic pattern change. Then re-training begins, the routing policy is updated, and we observe gradual increase of the test accuracy. Once the test accuracy is saturated, the routing policy update stops, and we observe a flat line in the test accuracy. Again, we observe that FPHTC substantially outperforms regular packet-based traffic classification in the online setting.