% \newcommand{\var}[1]{\mathbf{#1}}
\newcommand{\var}[1]{\uppercase{#1}}
\newcommand{\spc}[1]{\mathcal{#1}}
% \DeclareMathOperator{\argmin}{arg\,min}
\makeatletter
\newcommand*{\rsimdots}{%
  \mathrel{%
    \mathpalette\@rsimdots{}% Adopt to math style size via \mathpalette
  }%
}
\newcommand*{\@rsimdots}[2]{%
  % #1: math style
  % #2: unused
  \sbox0{$#1\sim\m@th$}%
  \sbox2{$#1\vcenter{}$}% \ht2 is height of the math axis
  \dimen@=.75\ht2\relax % distance dot to math axis
  \sbox2{$#1\cdot\m@th$}% single vertically centered dot
  \sbox2{% two dots above and below the math axis
    \rlap{\raisebox{\dimen@}{\copy2}}%
    \raisebox{-\dimen@}{\copy2}%
  }%
  % Rotate the two dots.
  \sbox2{$#1\rotatebox[origin=c]{-45}{\copy2}$}%
  % Combine symbol
  \rlap{\hbox to \wd0{\hss\copy2\hss}}%
  \copy0 %
}
\makeatother

\section{Introduction}
Generative modeling, in its unconditional form, refers to the problem of estimating the data generating distribution: given \textit{i.i.d.} samples $\mathbf{X}$ with an unknown distribution $P_{\var{x}}$, a generative model seeks to 
find a parametric distribution that closely resembles $P_{\var{x}}$. In modern deep generative models, 
we often approach this problem via a {\em latent variable} -- i.e., we assume that there is some variable $\var{z} \in \spc{Z}$ 
associated with the observed data $\var{x} \in \spc{X}$ that 

follows a {\em known} distribution $P_{\var{z}}$ (also referred to as the \textit{prior} in generative models). Thus, we can learn a mapping $f: \spc{Z} \to \spc{X}$ such that the distribution after transformation, denoted by $P_{f(\var{z})}$, aligns well with the data generating distribution $P_{\var{x}}$. Therefore, sampling from $P_{\var{X}}$ becomes convenient since $P_{\var{Z}}$ can be efficiently sampled. Frequently, $f$ is parameterized by deep neural networks and optimized with stochastic gradient descent (SGD).
% While the mechanics of how each component in the architecture is implemented varies 
% from one model to the other, if the overall workflow functions as intended, it is quite convenient and effective,  
% because drawing samples from 
% the known density $p_{\theta}$
% is much easier.

Existing generative modeling methods variously optimize the transformation $f$, most commonly modeling it as a Maximum Likelihood Estimation (MLE) or distribution matching problem. For instance, given data $\mathbf{X} = \{ \mathbf{x}_{1}, \dots, \mathbf{x}_{n}\}$, a variational autoencoder (VAE) \citep{kingma2013auto} first constructs $\var{Z}$ through the approximate posterior $q_{\var{z}|\var{X}}$ and maximizes a lower bound of likelihood $p_{f(\var{Z})}(\mathbf{X})$. Generative adversarial networks (GANs) \citep{goodfellow2014gan} relies on a simultaneously learned discriminator such that samples of $P_{f(\var{Z})}$ are indistinguishable from $\mathbf{X}$. Results in \citep{arjovsky2017wasserstein, li2017mmdgan} suggest that GANs minimize the distributional discrepancies between $P_{f{(\var{Z})}}$ and $P_{\var{X}}$. Flow-based generative models optimize $p_{f(\var{Z})}(\mathbf{X})$ explicitly through the {\it change of variable rule} and efficiently calculating the Jacobian determinant of the inverse mapping $f^{-1}$. 

% Consider generative models with an explicit decoder or generator structure, for instance, generative adversarial networks (GANs) \citep{goodfellow2014gan} or variational autoencoders (VAEs) \citep{kingma2013auto}. The parameterized empirical density $p_\tau(\var{X})$ can be written as $\int p_\tau(\var{x}|\var{z})p(\var{z}) dz$ where $\var{z}$ is the latent variable with a suitable prior and the conditional density $p_\tau(\var{x}|\var{z})$ is usually modeled with a multi-layer perceptron or a convolutional neural network. Such a decoder shapes (or non-linearly transforms) a simple prior distribution $p_z$ into the distribution $p_{\var{x}}$. Training a generative model of this form involves numerical optimization of an appropriate loss function, e.g., associated with the likelihood \citep{kingma2013auto}.
% % During training, we use a decoder distribution 
% % $q_{\tau'}(z|x)$ and an encoder distribution 
% % $p_\tau(x|z)$ to train the parameters $\tau$ and $\tau'$ by maximizing the lower bound of likelihood.
% In flow based generative models, we learn a parametric invertible mapping $f_{\tau}$ by directly maximizing the log likelihood of $p_{\tau}(\var{x})$ using the \textit{change of variable} rule. The conditional density, $p_{\tau}(\var{x}|\var{z})$, therefore, is characterized by the inverse mapping $f_{\tau}^{-1}$. 
% The ``flow'' nomenclature is used because of the transformation of a (standard normal) distribution to the empirical distribution 
% of the samples composed by invertible transformations.
%
In all examples above, the architecture or objective notwithstanding, the common goal is to find a suitable function $f$ that reduces the difference between $P_{f(\var{Z})}$ and $P_{\var{X}}$. 
%
% \revise{Eric: take 1 paragraph...use a fig below, describe the mechanics of VAE. Don't yet "call" it a forward operator yet.}
%
% \revise{Eric: take 1 paragraph...copy/paste from previous submission, describe the mechanics of FLOW/GLOW. Don't yet call it a forward operator yet.}
%
% \revise{Now, draw attention to figure and the two previous paragraphs..and say that density approximation is fundamentally dependennt on forward operator....talk about yes, it works..but it is a lot of work. Copy/paste lines from difficulties in GLOW, yadda yadda...}
%
%G a generative learning algorithm 
Thus, a key component in many deep generative models 
is to learn a \textit{forward operator} as defined below.
\begin{definition}[Forward operator]
\label{def:forward}
A forward operator $f^\star \in \mathcal{C}: \spc{Z} \to \spc{X}$ is defined to be a mapping associated with some latent variable ${\var{z}} \sim P_{\var{z}}$ such that $f^\star = \arg \min_{f \in \mathcal{C}} d(P_{f(\var{z})}, P_{\var{x}})$ for some function class $\mathcal{C}$ and a distance measure $d(\cdot,\cdot)$.
\end{definition}



%One benefit is that 
%the log-determinant of the Jacobian for the %sequence of invertible transformations can be %efficiently computed or approximated. This %enables optimization of the maximum likelihood %as well as efficient density estimation and %sampling.


%\begin{comment}
%\revise{Eric: you can use this more %holistic view here..}
%In the generative model setting, $f^*$ %is sometimes referred to as the %\textit{optimal generator}. And in %practice, $p_\var{z}$ is often chosen %to be certain known parametric %distribution (e.g. Gaussian), and %therefore samples of $p_{\var{x}}$ can %be approximately generated by applying %$f^*$ to samples of $p_\var{z}$, which %can be easily produced. 
%\end{comment}



%  The state-of-the-art generative models can be broadly categorized into three sub-classes: \begin{inparaenum}[(a)] \item Generative adversarial networks (GANs) \citeppending{}, \item Variational autoencoders (VAE) \citeppending{}, \item Flow-based generative models \citeppending{}. \end{inparaenum} GANs  adopt a generator-critic scheme and use a surrogate loss to enforce $f(Z)$ to be \textit{in-distribution}. On the other hand, VAEs \citeppending{} pose generation as a variational inference problem and optimize the lower bound of the marginal likelihood $p_f(\var{x})$. Flow-based generative models \citeppending{} matches the posterior to the data distribution through explicitly maximization of $p_f(\var{x})$ using the \textit{change of variables}. 

{\bf Motivation:} The specifics 
of the forward operator may differ from case to case. But its properties and how it is estimated 
numerically greatly influences the empirical performance of the model. 
For instance, mode collapse issues in GANs are well known and solutions continue to emerge \citep{srivastava2017veegan}. To learn the forward operator, VAEs use an approximate posterior $q_{\var{Z}|\var{X}}$ that may 
sometimes fail to align with the prior \citep{Kingma2016improved, dai2018diagnosing}. 
%and therefore creating a discrepancy between the learned operator (from %approximated posterior) and the target operator (from the prior). 
Flow-based generative models enable direct access to the posterior likelihood, yet in order to tractably evaluate the Jacobian of the transformation during training, one must either restrict the expressiveness at each layer \citep{dinh2017RealNVP, Kingma2018Glow} or use more involved solutions \citep{Chen2018neuralode}. 
Of course, solutions 
to mitigate these weaknesses \citep{ho2019flow++} remains an active area of research.


%Based on these known pitfalls, we can summarize that a better learning algorithm %for 
The starting point of our work is to evaluate the extent to which 
we can {\em radically} simplify the forward operator in deep generative models. Consider some desirable properties of a hypothetical forward operator (in Def. \eqref{def:forward}): \begin{inparaenum}[\bfseries (a)]\item Upon  convergence, the learned operator $f^\star$ minimizes the distance between $P_{\var{x}}$ and $P_{f(\var{z})}$ over all possible operators of a certain class. \item The training directly learns the mapping from the prior distribution $P_{\var{Z}}$, rather than a variational approximation. \item The forward operator $f^\star$ can be efficiently learned and sample generation is also efficient.  
\end{inparaenum} 
It would appear that 
these criteria violate the ``no free lunch rule'', and 
some compromise must be involved. Our goal is 
to investigate this trade-off: 
which design 
choices can make 
this approach work? 
Specifically, a well studied construct in dynamical systems, namely the Perron-Frobenius operator \citep{lemmens2012nonlinear}, suggests an alternative \textit{linear} route to model the forward operator. Here, 
we show that 
if we are willing to give up on a few features  
in existing models -- this may be acceptable depending on the downstream use case --  
then, the forward operator in generative models can be efficiently approximated as the estimation of a {\it closed-form linear operator} in the reproducing kernel Hilbert space (RKHS). With simple adjustments of existing results, we identify a novel way to replace the expensive training for generative tasks with a simple principled kernel approach.
% Surprisingly, with only minor adjustments, a direct instantiation of this idea works. 

\textbf{Contributions.} Our results are largely based on results in kernel methods and dynamical systems, but we demonstrate their relevance in generative modeling and complement recent ideas that emphasize links between deep generative models and dynamical systems. Our contributions are 
\begin{inparaenum}[\bfseries (a)] \item We propose a \textit{non-parametric} method for transferring a known prior density {\it linearly} in RKHS to an unknown data density -- equivalent to learning a nonlinear forward operator in the input space. When compared to its functionally-analogous module used in other deep generative methods, our method 
avoids multiple expensive training steps yielding significant efficiency gains; \item We evaluate this idea in multiple scenarios and show competitive generation performance and efficiency benefits with pre-trained autoencoders on popular image datasets including MNIST, CIFAR-10, CelebA and FFHQ; \item As a special use case, we demonstrate the advantages over other methods in limited data settings. 
\end{inparaenum}
