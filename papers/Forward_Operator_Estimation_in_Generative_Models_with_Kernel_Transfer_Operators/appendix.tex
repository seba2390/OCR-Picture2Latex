\clearpage
\appendix
% \section{Appendix}

\section{Choice of Kernel is relevant yet flexible}\label{sec:choice_of_kernel}

In some cases, one would focus on identifying (finite) eigenfuntions and modes of the underlying operator \citep{williams2015extDMD,brunton2016sindy}. But rather than finding certain modes that best characterize the dynamics, we care most about minimizing the error of the transferred density and therefore whether the span of functions is rich/expressive enough. In particular, condition (iii) in Proposition \ref{prop:klus} requires an input RKHS spanned by sufficiently rich bases \citep{fukumizu2013kernel}. For this reason, the choice of kernel here 
cannot be completely ignored since it determines the family of functions contained in the induced RKHS.

To explore the appropriate kernel setup for our application, we empirically evaluate the effect of using several different kernels via a simple experiment on MNIST. We first train an autoencoder to embed MNIST digits on to a hypersphere $S^2$, then generate samples from kPF by the procedure described by Alg. \ref{alg:gen_algo} using the respective kernel function as the input kernel $k$. Subplot (b) and (c) in Fig. \ref{fig:mnist_gen} show the generated samples using Radial Basis Function (RBF) kernel and arc-cosine kernel, respectively. Observe that the choice of kernel has an influence on the sample population, and a kernel function with superior empirical behavior is desirable. 

Motivated by this observation, we evaluated the Neural Tangent Kernel (NTK) \citep{jacot2018NTK}, a well-studied neural kernel in recent works. We use it for a 
few reasons, 
\begin{inparaenum}[\bfseries (a)] \item NTK, in theory, corresponds to a trained infinitely-wide neural network, which spans a rich set of functions that satisfies the assumption. 
  %Therefore, NTK is spanned by a rich set of nonlinear functions
\item For well-conditioned inputs (i.e., no duplicates) on hypersphere,
  the positive-definiteness of NTK is proved in \citep{jacot2018NTK}. Therefore, invertibility of the Gram matrix $K_{\var{z}\var{z}} = \Phi^T\Phi$ is \textit{almost guaranteed} if the prior distribution $p_{\var{z}}$ is restricted on a hypersphere
\item
  NTK can be non-asymptotically approximated \citep{arora2019onexact}.
\item Unlike other parametric kernels such as RBF kernels, NTK is less sensitive to hyperparameters, as long as the number of units used is large enough \citep{arora2019onexact}.
\end{inparaenum} Subplot (d) of Fig. \ref{fig:mnist_gen} shows that kPF learned with NTK as input kernel is able to generate samples that are more consistent with the data distribution. However, we should note that NTK is merely a convenient choice of kernel that requires less tuning, and is not otherwise central to our work. In fact, as shown in our experiment in Tab. \ref{tab:fid_table}, a well-tuned RBF kernel may also achieve a similar performance. Indeed, in practice, any suitable choice of kernel may be conveniently adopted into the proposed framework without major modifications.

\begin{figure*}[h]
%\vspace*{-2em}
    \centering
    \textcolor{red!50!white}{\fboxrule=1pt\fbox{
    \includegraphics[width=0.22\linewidth]{imgs/mnist_data_latent.png}}}
    \textcolor{blue!50!white}{\fboxrule=1pt\fbox{
    \includegraphics[width=0.22\linewidth]{imgs/rbf_sampled.png}
    \includegraphics[width=0.22\linewidth]{imgs/arccos_sampled.png}
    \includegraphics[width=0.22\linewidth]{imgs/ntk_sampled.png}}}
    % \vspace*{-1em}
    \caption{\footnotesize 10k samples from MNIST dataset ({\it left to right}) (a) projected on $\mathbf{S}^2$ shown in $(\theta, \phi)$ using auto-encoder, and 10K generated samples from kPF with input kernel of type (b) RBF (c) arccos (d) NTK. Color of sampled points represents the class of their nearest training set neighbor in the output RKHS.}  \label{fig:mnist_gen}
\end{figure*}

\clearpage
\section{Density Estimation with kPF Operator}

The displayed transferred density with the kPF operator on toy data in Fig. 1 is \textit{approximated} using the empirical kernel conditional density operator (CDO) \citep{schuster2020kcdo}, since there is currently no known methods that can exactly reconstruct density from the transferred mean embeddings. The marginalized transferred density $p_{\mathcal{P}_\mathcal{E}}$ has the following form
\begin{equation}
p_{\mathcal{P}_\mathcal{E}} = C_\rho^{-1}\mathcal{P}_\mathcal{E}\mu_{\var{z}} = C_\rho^{-1}C_{\var{x}\var{z}}C_{\var{z}\var{z}}^{-1}\mu_{\var{z}},  
\end{equation}

where $C_\rho = E_{y \sim \rho}[l(y, \cdot) \otimes l(y, \cdot)]$ is the covariance operator of a independent reference density $\rho$ in $\spc{G}$. The above density function is also an element of RKHS $\mathcal{G}$, and therefore we can evaluate the density at $x^*$ by using the reproducing property $p_{\mathcal{P}_\mathcal{E}}(x^*) = \langle p_{\mathcal{P}_\mathcal{E}}, l(x^*, \cdot) \rangle$. The results in \citet{schuster2020kcdo} show that the empirical estimate of $p_{\mathcal{P}_\mathcal{E}}$ may be constructed from $m$ samples of the reference density $\{y_i\}_{i \in [m]}$ and $n$ training samples $\{x_i\}_{i \in [n]}$ and $\{z_i\}_{i \in [n]}$ as 
\begin{equation}
\label{eq:empirical_kcdo}
    \hat{p}_{\mathcal{P}_\mathcal{E}} = (\hat{C}_\rho + \alpha' I)^{-1}\hat{C}_{\var{x}\var{z}}(\hat{C}_{\var{z}\var{z}} + \alpha I)^{-1}\hat{\mu}_{\var{z}} = \sum_{i = 1}^{m} \beta_i l(y_i, \cdot)
\end{equation}

where 
\begin{equation}\beta = m^{-2}(L_{\var{y}} + \alpha'I)^{-2}L_{\var{y}\var{x}}(K_{\var{z}} + \alpha I)^{-1}\Phi^\top \hat{\mu}_{\var{z}}
\end{equation} and 
\begin{equation}
L_{\var{y}}= [l(y_i, y_j)]_{ij} \in \mathbb{R}^{m \times m} , L_{\var{y}\var{x}}= [l(y_i, x_j)]_{ij} \in \mathbb{R}^{m \times n}, K_{\var{z}}= [k(z_i, z_j)]_{ij} \in \mathbb{R}^{n \times n}
\end{equation}


In Fig. \ref{fig:inaccurate-density}, we use a uniform density in the square $(\pm 4, \pm 4)$ as the reference density $\rho$ and constructed $\hat{p}_{\mathcal{P}_\mathcal{E}}$ using $m = 10000$ samples $\{y_i\}_{i \in [m]}$ from $\rho$. Due to the form of the empirical kernel CDO, where the estimated density function $\hat{p}_{\mathcal{P}_\mathcal{E}}$ is a linear combination of $\{l(y_i, \cdot)\}_{i \in [m]}$ (as in Eq. \ref{eq:empirical_kcdo}), the approximation can be inaccurate if reference samples are relatively sparse and the densities are 
`sharp'. In those cases, to obtain a better density estimate, we may either increase the number of reference samples used to construct the empirical CDO (which can be computationally difficult due to the need to compute $(L_{\var{y}} + \alpha'I)^{-2}$), or, with some prior knowledge to the true distribution, choose a reference density which is localized around the ground truth density.

\begin{figure}[h]
    \centering
    \includegraphics[scale=0.1]{imgs/distribution_2spirals_ntk_rbf.png}
    \includegraphics[scale=0.1]{imgs/distribution_8gaussians_ntk_rbf.png}
    \includegraphics[scale=0.1]{imgs/distribution_checkerboard_ntk_rbf.png}
    \includegraphics[scale=0.1]{imgs/distribution_rings_ntk_rbf.png}
    \caption{Inaccurate density estimation result from kernel CDO using 10k samples from uniform reference density $\rho$}
    \label{fig:inaccurate-density}
\end{figure}

Therefore, to show a more faithful density estimate of the transferred distribution for visualization purpose, we use a composite of the uniform density and the true density with weight $4:1$ as the reference density $\rho$ in Fig. \ref{fig:density}. In this case, approximately 20\% of the reference samples concentrates around the high-density areas of the true density, which helps to form a better basis for $\hat{p}_{\mathcal{P}_\mathcal{E}}$. Note that the choice of $\rho$ does not affect the transferred density embedding $\hat{\mathcal{P}}_\mathcal{E}\hat{\mu}_{\var{z}}$ since it is independent of $p_{\var{x}}$ and $p_{\var{z}}$. After this modification, the reconstructed density more accurately reflects the true density compared to GMM and GLOW, indicating the transferred distribution by kPF in RKHS matches better to the ground truth distribution. This is also reflected in the generated samples in Fig. \ref{fig:samples}, where samples generated by the kPF operator are clearly more aligned with the ground truth distribution.

\begin{figure}[h]
    %\begin{minipage}{0.55\textwidth}
        \centering
        \begin{tabular}{ m{1cm} m{2cm} m{2cm} m{2cm}  m{2cm}  }
            \textrm{GT} &
            \includegraphics[width=\linewidth]{imgs/supplement/data_2spirals.png} &
            \includegraphics[width=\linewidth]{imgs/supplement/data_8gaussians.png} &
            \includegraphics[width=\linewidth]{imgs/supplement/data_checkerboard.png} &
            \includegraphics[width=\linewidth]{imgs/supplement/data_rings.png}\\ 
            \textrm{GMM} &
            \includegraphics[width=\linewidth]{imgs/supplement/samples_2spirals_gmm.png} &
            \includegraphics[width=\linewidth]{imgs/supplement/samples_8gaussians_gmm.png} &
            \includegraphics[width=\linewidth]{imgs/supplement/samples_checkerboard_gmm.png} &
            \includegraphics[width=\linewidth]{imgs/supplement/samples_rings_gmm.png}\\
            \textrm{Glow} &
            \includegraphics[width=\linewidth]{imgs/supplement/samples_2spirals_glow.png} &
            \includegraphics[width=\linewidth]{imgs/supplement/samples_8gaussians_glow.png} &
            \includegraphics[width=\linewidth]{imgs/supplement/samples_checkerboard_glow.png} &
            \includegraphics[width=\linewidth]{imgs/supplement/samples_rings_glow.png}\\
            \textrm{NTK-kPF} &
            \includegraphics[width=\linewidth]{imgs/supplement/samples_2spirals_wm.png} &
            \includegraphics[width=\linewidth]{imgs/supplement/samples_8gaussians_wm.png} &
            \includegraphics[width=\linewidth]{imgs/supplement/samples_checkerboard_wm.png} &
            \includegraphics[width=\linewidth]{imgs/supplement/samples_rings_wm.png}\\
        \end{tabular}
        \caption{\label{fig:samples}Sample comparisons of the distribution matching methods}
\end{figure}

\newcommand{\centered}[1]{\begin{tabular}{l} #1 \end{tabular}}
    %\end{minipage}
    %\hspace{0.02\textwidth}

\clearpage
\section{Effect of $\gamma$ on Sample Quality}

In the sampling stage, our proposed method finds the approximate preimage of the transferred kernel embeddings by taking the weighted Fr\'{e}chet mean of the top $\gamma$ neighbors among the training samples.
The choice of $\gamma$ therefore influences the quality of generation. 

From Figure \ref{fig:fid_v_gamma}, we can observe that,
in general, FID worsens as $\gamma$ increases. This observation aligns with our intuition of preserving only the local similarities represented by the kernel, and similar ideas have 
been previously used in the literature \citep{hastie2001statisticallearning,kwok2004pre}. However, significantly decreasing $\gamma$ leads 
to the undesirable result where the 
generator merely generates the training samples (in the extreme case where $\gamma = 1$, generated samples will just be reconstructions of training samples). Therefore, in our experiments, we choose $\gamma = 5$ to achieve a balance between generation quality and the distance to training samples.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.6\textwidth]{imgs/supplement/fid_curve_gamma_2.png}
    \caption{FID \textit{versus} $\gamma$ on a few computer vision datasets.}
    \label{fig:fid_v_gamma}
\end{figure}

\clearpage
\section{Weighted Fr\'{e}chet Mean on the Hypersphere}
\label{appdx:wfm_sphere}
While the weighted Fr\'{e}chet Mean in Euclidean space can be computed in closed-form as a weighted arithmetic mean (as in Eq. \ref{eq:wfm_euc}), on the hypersphere there is no known closed-form solution. Thus, we adopt the iterative algorithm in \citep{chakraborty2015recursive} for an approximate solution given data points $\mathbf{X} = \{\mathbf{x}_1 \dots \mathbf{x}_\gamma\}$ and weight vector $\mathbf{s}$:
\begin{align*}
    M_1 &= \mathbf{x}_1\\
    M_{i+1} &:= \cos(\|\mathbf{s}_{i+1}\mathbf{v}\|) M_i + \sin(\|\mathbf{s}_{i+1}\mathbf{v}\|)\frac{\mathbf{v}}{\|\mathbf{v}\|}
\end{align*}
where, $\mathbf{v} = \frac{\theta}{\sin(\theta)}\left(X_{i+1} - M_i \cos(\theta)\right), \theta = \arccos(M_i^tX_{i+1})$. This algorithm iterates through the data points once, yielding a complexity of only $O(\gamma d)$, where $d$ is the dimension of $\mathcal{X}$. Under the prescribed iteration, $M_{n}$ converges asymptotically to the true Fr\'{e}chet mean for finite data points. We refer the readers to \citep{chakraborty2015recursive} for further details.

\clearpage
\section{Fast approximation of Moore-Penrose inverse}

When computing the inverted kernel matrix $K_\textrm{inv}$ in Algo. \ref{alg:gen_algo}, conventional approaches typically performs SVD or Cholesky decomposition. Both procedures are hard to parallelize, and therefore, can be slow when $K$ is large. Alternatively, we can utilize an iterative procedure proposed in \citet{Razavi2014} to approximate the Moore-Penrose inverse.

\begin{align}
    Z_{1} &= K / (\Vert K \Vert_1 \Vert K \Vert_\infty)\\
    Z_{i+1} &:= Z_i (13I - KZ_i (15I - KZ_i (7I - KZ_i ))) 
\end{align}
where 
\begin{equation}
    \Vert K \Vert_1 = \max_j \sum_{i = 0}^{n} K_{ij}, \Vert K \Vert_\infty = \max_i \sum_{j = 0}^{n} K_{ij} 
\end{equation}

Since this iterative procedure mostly involves matrix multiplications, it can be efficiently parallelized and implemented on GPU. The same procedure has also seen success in approximating large self-attention matrices in language modeling \citep{xiong2021nystromformer}. For the NVAE experiment, we run this iteration for $10$ steps and use $K_\textrm{inv} = Z_{10}$.

\clearpage
\section{Nystrom Approximation of kPF}
\label{sec:nystrom}

Due to the need to store and compute a kernel matrix inverse $(K + \lambda nI)^{-1}$ or $K^\dagger$, the memory and computational complexity of kPF is at least $O(n^2)$ and $O(n^3)$, respectively. The sup-quadratic complexity hinders the use of kPF on extremely large datasets. In our experiments, we already adopted a simple subsampling strategy which randomly select 10k training samples from each dataset ($\leq$ 50k samples) to fit our hardware configuration which works 
well. But for larger datasets with potentially more modes, a larger set of subsamples must be considered, and in those cases kPF may not be 
suitable for commodity/affordable hardware. 
In order to overcome this problem, we can combine kPF with conventional kernel approximation methods such as the Nystr\"om method \citep{williams2000nystrom}.

Let $(\mathbf{X_\star}, \mathbf{Z_\star})$ be a size $v$ subset of the training set (which we refer to as the \textit{landmark points}) and $(\Psi_\star, \Phi_\star)$ be their corresponding kernel feature maps. The weighting coefficients $\mathbf{s}$ for each prior sample $z^* \sim Z$ derived in Alg. \ref{alg:gen_algo} can be approximated by
\begin{align}
    % \mathbf{s} = \Psi^\top \Psi' (\Psi'^\top \Psi')^\dagger \Psi'^\top \Psi(\Phi^\top \Phi' (\Phi'^\top \Phi')^\dagger \Phi'^\top \Phi)^\dagger
    \nonumber\mathbf{s} &= L (K + \lambda nI)^\dagger \Phi^\top k(z^\star, \cdot) \\\nonumber
    &\approx L_{\Psi_\star} W_{\Psi_\star}^\dagger L_{\Psi_\star}^\top (K_{\Phi_\star} W_{\Phi_\star}^\dagger K_{\Phi_\star}^\top + \lambda nI)^\dagger \Phi^\top k(z^\star, \cdot) \\
    &= L_{\Psi_\star} W_{\Psi_\star}^\dagger L_{\Psi_\star}^\top (\lambda n)^{-1}(I - K_{\Phi_\star}^\top(\lambda n W_{\Phi_\star}^\dagger + K_{\Phi_\star} K_{\Phi_\star}^\top)^\dagger K_{\Phi_\star}) \Phi^\top k(z^\star, \cdot)
\end{align}
where $L_{\Psi_\star} = \Psi^\top \Psi_\star \in \mathbb{R}^{n \times v}$, $W_{\Psi_\star} = \Psi_\star^\top \Psi_\star \in \mathbb{R}^{v \times v}$, $K_{\Phi_\star} = \Phi^\top \Phi_\star \in \mathbb{R}^{n \times v}$, $W_{\Phi_\star} = \Phi_\star ^\top \Phi_\star \in \mathbb{R}^{v \times v}$, and the last identity is due to applying the Woodbury formula on $(K_{\Phi_\star} W_{\Phi_\star}^\dagger K_{\Phi_\star}^\top + \lambda nI)^\dagger$. Assuming $v \ll n$, the memory complexity is reduced to $O(nv)$ and the computation complexity to $O(nv^2)$.

We empirically evaluated the Nystr\"om-approximated kPF on the CelebA experiment and present the result in Tab. \ref{tab:fid_nystrom}. It can be observed that when $v$ is sufficiently large, the performance of Nystr\"om approximated kPFs is as good as 
the ones using the full kernel matrices.

\begin{table}[h]
    \centering
    \begin{tabular}{c || c | c | c| c}
    \toprule
    \diaghead{NVNV}{$n$}{$v$} & 100 & 500 & 1000 & \thead{w/o Approximation}\\
    \midrule\midrule
         10,000 & 45.9 & 41.6 & 42.3 & 41.8\\
         \midrule
         30,000 & 46.3 & 40.5 & 42.4 & -\\
         \midrule
         50,000 & 45.2 & 44.1 & 42.0 & -\\
    \bottomrule
    \end{tabular}
    \caption{FIDs of samples generated by Nystr\"om-approximated NTK-kPF on CelebA. $n$ denotes the size of the training data subset we consider in computing kPF, while $v$ denotes the size of selected landmark points for Nystr\"om approximation. Without approximation, we cannot fit the kernel matrices onto a GPU with 11GB RAM when $n > 10,000$. It is worth noting that the approximated kPFs can perform similarly to the full kPF even with $v < 0.05n$, which indicates that Nystr\"om approximation does not sacrifice much in terms of performance while delivering significant efficiency gain.}
    \label{tab:fid_nystrom}
\end{table}

\clearpage
\section{Does kPF Memorize Training Data?}

Since in kPF, samples are generated by linearly interpolating between training samples, it is natural to wonder whether it `fools' the metrics by simply replicating training samples. For comparison, we consider an alternative scheme that generates data through direct manipulation of the training data, namely Kernel Density Estimation (KDE). 

We fit KDEs by varying noise levels $\sigma$ and compare their FIDs and nearest samples in the latent space to kPF in Fig. \ref{fig:overfitting}. We observe that, although KDE can reach very low FIDs when $\sigma$ is small, almost all new samples closely resemble some instance in the training set, which is a clear indication of memorization. In contrast, kPF can generate diverse samples that do not simply replicate the observed data.

\begin{figure}[ht]
    \begin{subfigure}[b]{0.49\linewidth}
    \centering
    \begin{tikzpicture}
    \node[inner sep=0pt] (img) at (0,0) {\includegraphics[width=\textwidth]{imgs/overfitting/kde_small.png}};
    \draw[draw=red, line width=2pt] (-3.2, -2.7) rectangle (-2.15, 2.7);
    \end{tikzpicture}
    \caption{KDE, $\sigma = 0.005$, FID: 30.9}
    \end{subfigure}
    \begin{subfigure}[b]{.49\linewidth}
    \centering
    \begin{tikzpicture}
    \node[inner sep=0pt] (img) at (0,0) {\includegraphics[width=\textwidth]{imgs/overfitting/kde_mid.png}};
    \draw[draw=red, line width=2pt] (-3.2, -2.7) rectangle (-2.15, 2.7);
    \end{tikzpicture}
    \caption{KDE, $\sigma = 0.01$, FID: 36.3}
    \end{subfigure}\\
    \centering
    \begin{subfigure}[b]{.49\linewidth}
    \centering
    \begin{tikzpicture}
    \node[inner sep=0pt] (img) at (0,0) {\includegraphics[width=\textwidth]{imgs/overfitting/kde_large.png}};
    \draw[draw=red, line width=2pt] (-3.2, -2.7) rectangle (-2.15, 2.7);
    \end{tikzpicture}
    \caption{KDE, $\sigma = 0.02$, FID: 48.0}
    \end{subfigure}
    \begin{subfigure}[b]{.49\linewidth}
    \centering
    \begin{tikzpicture}
    \node[inner sep=0pt] (img) at (0,0) {\includegraphics[width=\textwidth]{imgs/overfitting/kpf.png}};
    \draw[draw=red, line width=2pt] (-3.2, -2.7) rectangle (-2.15, 2.7);
    \end{tikzpicture}
    \caption{kPF, FID: 41.0}
    \end{subfigure}
    \caption{Comparing KDE to kPF. Generated samples are presented in \textcolor{red}{$\msquare$}, followed by their 5 nearest neighbors in the latent space (ordered from closest to furthest)}
    \label{fig:overfitting}
\end{figure}

\clearpage
\section{Assessing Sample Diversity}

Although FID is one of the most frequently used measures for assessing sample quality of generative models, certain diversity considerations, such as mode collapse, may not be conveniently deduced from it \citep{sajjadi2018prd}. To enable explicit examination of generative models with respect to both accuracy (i.e., generating samples within the support of the data distribution) and diversity (i.e., covering the support of the data distribution as much as possible), \citet{sajjadi2018prd} proposed an approach to evaluate generative models with generalized definitions of \textit{precision} and \textit{recall} between distributions. Quality of generation can then be assessed by evaluating the PRD curve, which depicts the trade-offs between accuracy (precision) and diversity (recall). We present the PRD curves in Fig. \ref{fig:prd}. The observations align with our results in Tab. \ref{tab:fid_table} and kPF performs competitively in both accuracy and sample diversity.

\begin{figure}[h]
    \centering
    \begin{tabular}{c c c}
        \quad MNIST & \quad CIFAR-10 & \quad CelebA\\
        \includegraphics[width=0.3\textwidth]{imgs/prd_mnist_model_comp.pdf}&
        \includegraphics[width=0.3\textwidth]{imgs/prd_cifar10_model_comp.pdf}&
        \includegraphics[width=0.3\textwidth]{imgs/prd_celeba64_model_comp.pdf}\\
    \end{tabular}
    \caption{PRD curves on all datasets. kPF is competitive to the other methods in terms of Area Under Curve (AUC)}
    \label{fig:prd}
\end{figure}

\clearpage
\section{Exploring Kernel Configurations}

To investigate the implication of kernel choices on generation quality, we tested 25 different kernel configurations for CelebA generation (results are presented in Tab. \ref{tab:kernel_config}). For RBF kernels used in the CelebA experiments of the main text, we use a bandwidth of $\sigma_{in} = {\sqrt{2 |\spc{Z}|}} / {8} \approx 0.71$ when used as input kernel and $\sigma_{out} \approx 0.34$, and we adopt the same notation here.

\begin{table}[h]
    \centering
    \begin{tabular}{ c | c | c | c | c | c}
        \toprule
         \diaghead{\theadfont Input kernel kernel}{Input \\ kernel}{Output \\ kernel}
         & \thead{RBF \\($\sigma = \sigma_{out} / 4$)} & \thead{RBF \\($\sigma = \sigma_{out} / 2$)}& \thead{RBF \\($\sigma = \sigma_{out}$)}& \thead{RBF \\($\sigma = 2\sigma_{out}$)}& \thead{RBF \\($\sigma = 4\sigma_{out}$)}\\
         \midrule\midrule
         \thead{RBF \\($\sigma = \sigma_{in} / 2$)} & 41.50 & 41.20 & 41.21 & 50.92 & 66.06 \\\midrule
         \thead{RBF \\($\sigma = \sigma_{in}$)} & 41.90 & 42.11 & 41.91 & 45.83 & 50.70 \\\midrule
         \thead{RBF \\($\sigma = 2\sigma_{in}$)} & 42.20 & 42.82 & 42.69 & 65.76 & 70.19 \\\midrule
         \thead{NTK \\ ($L=8$, $w=10,000$)} & 41.90 & 41.56 & 41.73 & \textbf{37.86} & 37.89  \\\midrule
         \thead{Arccos \\ ($L=1$, $\textrm{deg}=1$)} & 41.71 & 42.03 & 42.22 & 52.83 & 63.13\\
         \bottomrule
         
    \end{tabular}
    \caption{FID table for different kernel configurations.}
    \label{tab:kernel_config}
\end{table}

It can be seen that kernel configurations and parameters indeed has a non-trivial impact on the generation quality, with NTK-kPF being the most robust to the choice of parameters. This aligns with our previous observations and offers some support for using NTK as an input kernel despite the additional compute cost.

\clearpage

\section{Experimental Details}
\label{sec:experimental_details}

In this section, we provide the detailed specifications for all of our experiments. We have also provided our code in the supplemental material.
\subsection{Density Estimation on Toy Densities}

We generated $10000$ samples from each of the toy densities to learn the kPF operator. The input kernel $k$ is a ReLU-activated NTK corresponding to a fully-connected network with depth $L = 4$ and width $w = 10000$ at each layer, and the output kernel $l$ is a Gaussian kernel. Unless specified otherwise, we always uses a Gaussian kernel as the output kernel for the remainder of this appendix. The bandwidth of the output kernel was adjusted separately for density estimation and sampling for the purpose of demonstration. For comparison, we also fit/estimate a 10-component GMM and a Glow model with 50 coupling layers, where each of them were trained until convergence.


\subsection{Image Generation with Computer Vision Datasets}

To generate results in Tab. \ref{tab:fid_table} and Tab. \ref{tab:fid_table_limited}, we first trained an autoencoder for each dataset following the model setup in \citep{Ghosh2020From}, which uses a modified Wasserstein autoencoder \citep{tolstikhin2018wasserstein} architecture with batch normalization. Additionally, we applied spectral normalization on both the encoder and the decoder, following \citep{Ghosh2020From}, to obtain a regularized autoencoder. The latent representations were projected onto a hypersphere before decoding to image space. We trained the models on two NVIDIA GTX 1080TI GPUs. A detailed model specification is provided below in Table \ref{tab:vision_architecture}.

We used an NTK with $L = 8$ and $w = 10000$ as the input kernel $k$ (i.e. the embedding kernel of $p_Z$) for NTK-kPF, and a Gaussian kernel with bandwidth $\sigma_{in} = {\sqrt{2 |\spc{Z}|}} / {8}$ for RBF-kPF. The bandwidth for the output Gaussian kernel is selected by grid search over $\{2^{-i} * \sigma_{data}| i \in [8]\}$, where $\sigma_{data}$ is the empirical data standard deviation, based on cross-validation of a degree 3 polynomial kernel MMD between the sampled and the ground-truth latent points. Further, to mitigate the deterioration of performance of kernel methods in a high-dimensional setting due to the curse of dimensionality \citep{Evangelista2006Taming}, in practice, we model $\spc{Z}$ as a space with fewer dimensions than the input space $\spc{X}$. As a rule of the thumb, we choose $\spc{Z}$ such that $|\spc{Z}| = |\spc{X}| / 4$.

To generate images from kPF learned on NVAE latent space, we used the pre-trained checkpoints provided in \citep{vahdat2020NVAE} to obtain the latent embeddings for 2000 FFHQ images. We then construct the kPF from the concatenated latent space of the lowest resolution ($8 \times 8$). During sampling, prior samples at those resolutions are replace by the kPF samples, while for other resolutions samples remain generated from inferred Gaussian distributions. The batchnorm statistics were readjusted for $500$ iterations following \citep{vahdat2020NVAE}. We use rbf kernels as input and output kernels, with bandwidths $\sigma_k$, $\sigma_l$ chosen by the \textit{median heuristic} ($\sim100$ for input and $\sim70$ for output in our experiments).

\newcommand{\conv}[2]{\textrm{Conv}_{#1}^{#2}}
\newcommand{\convt}[2]{\textrm{ConvT}_{#1}^{#2}}
\newcommand{\resblock}[2]{\textrm{ResBlock}_{#1} \times #2}
\renewcommand{\arraystretch}{1.2}
\begin{figure}[h]
    \begin{minipage}{0.55\textwidth}
    \centering
    {\footnotesize    
    \begin{tabular}{c|c|c|c}
            \toprule
          & MNIST & CIFAR-10 & CelebA\\
          \midrule\midrule
          \multirow{4}{*}{Encoder}
                 & $\conv{128}{4\times4}$ & $\conv{128}{4\times4}$ & $\conv{128}{5\times5}$ \\
                 & $\conv{256}{4\times4}$ & $\conv{256}{4\times4}$ & $\conv{256}{5\times5}$ \\
                 & $\conv{512}{4\times4}$ & $\conv{512}{4\times4}$ & $\conv{512}{5\times5}$ \\
                 & $\conv{1024}{4\times4}$ & $\conv{1024}{4\times4}$ & $\conv{1024}{5\times5}$ \\
                 \midrule
          \multirow{4}{*}{Decoder}
                 & $\convt{512}{4\times4}$ & $\convt{512}{4\times4}$ & $\convt{512}{5\times5}$ \\
                 & $\convt{256}{4\times4}$ & $\convt{256}{4\times4}$ & $\convt{256}{5\times5}$ \\
                 & $\convt{1}{4\times4}$ & $\convt{3}{4\times4}$ & $\convt{128}{5\times5}$ \\
                 &                       &                       & $\convt{3}{5\times5}$ \\
                %  &                   &                   & \convt{32}{5\times5} \\\cline{2-4}
                %  &\multicolumn{3}{c|}{5x5 conv, stride 1} \\
                 \bottomrule
    \end{tabular}
    }
    \captionof{table}{Model architecture for computer vision experiments. Subscript denotes the number of output channels and superscript denotes the window size of the convolution kernel. Batch normalization and activation is applied between each pair of convolution layers}
    \label{tab:vision_architecture}
    \end{minipage}
    \hspace{0.02\textwidth}
    \begin{minipage}{0.4\textwidth}
    \centering
    {\footnotesize
    \begin{tabular}{p{0.25\linewidth}>{\centering\arraybackslash}p{0.65\linewidth}}
         \toprule
          \multirow{4}{*}[-0.5em]{Encoder}& 5x5 conv, stride 4 \\\cline{2-2}
                 & $\resblock{64}{2}$ \\
                 & $\resblock{64}{2}$ \\
                 & $\resblock{128}{2}$ \\
                 & $\resblock{256}{2}$ \\
                 \midrule
          \multirow{4}{*}[-0.5em]{Decoder}
                 & $\resblock{256}{2}$ \\
                 & $\resblock{128}{2}$ \\
                 & $\resblock{64}{2}$ \\
                 & $\resblock{64}{2}$ \\\cline{2-2}
                 &5x5 conv, stride 4 \\
                 \bottomrule
    \end{tabular}
    }
    \captionof{table}{Model architecture for experiments for image generation on brain imaging dataset. Subscript denotes the number of output channels. Upsampling and downsampling are performed using strided convolutions.}
    \label{tab:brain_imaging_architecture}
    \end{minipage}
\end{figure}


% \newcommand{\resblock}[2]{\textrm{ResBlock}_{#1} \times #2}
% \begin{wraptable}{r}{0.5\textwidth}
%     \centering
%     \begin{tabular}{p{0.1\textwidth}>{\centering\arraybackslash}p{0.3\textwidth}}
%          \toprule
%           \multirow{4}{*}[-0.5em]{Encoder}& 5x5 conv, stride 4 \\\cline{2-2}
%                  & $\resblock{64}{2}$ \\
%                  & $\resblock{64}{2}$ \\
%                  & $\resblock{128}{2}$ \\
%                  & $\resblock{256}{2}$ \\
%                  \midrule
%           \multirow{4}{*}[-0.5em]{Decoder}
%                  & $\resblock{256}{2}$ \\
%                  & $\resblock{128}{2}$ \\
%                  & $\resblock{64}{2}$ \\
%                  & $\resblock{64}{2}$ \\\cline{2-2}
%                  &5x5 conv, stride 4 \\
%                  \bottomrule
%     \end{tabular}
%     \caption{Model architecture for experiments for image generation on brain imaging dataset. Subscript denotes the number of output channels. Upsampling and downsampling are performed using strided convolutions.}
%     \label{tab:my_label}
%     \vspace*{-1.5em}
% \end{wraptable}

\subsection{Image Generation for Brain Images}

For the high-resolution brain imaging dataset, we used a custom version of ResNet \citep{he2016resnet} with 3D convolutions. The detailed architecture is shown in Fig. \ref{tab:brain_imaging_architecture}. Due to the large size of the data, we trained the model on 4 NVIDIA Tesla V100 GPUs. 

\paragraph{Mandatory ADNI statement regarding data use.} Data used in preparation of this article were obtained from the Alzheimer’s Disease Neuroimaging Initiative
(ADNI) database (adni.loni.usc.edu). As such, the investigators within the ADNI contributed to the design
and implementation of ADNI and/or provided data but did not participate in analysis or writing of this report.
A complete listing of ADNI investigators can be found in the \href{http://adni.loni.usc.edu/wp-content/uploads/how_to_apply/ADNI_Acknowledgement_List.pdf}{\text{ADNI Acknowledgement List}}.

The T1 MR brain dataset we utilize consists of images from $184$ subjects diagnosed with Alzheimers's disease and $292$ healthy controls/ normal subjects. Images were first coregistered to a MNI template and segmented to preserve only the white matter and grey matter. Then, all images were resliced and resized to $160 \times 196 \times 160$ and rescaled to the range of $[-1, 1]$. Voxel-based morphometry (VBM) was used to obtain the $p$-value map of data and generated images.

%\clearpage
% \clearpage


% \begin{figure}[!ht]
%     \centering
%     \scalebox{1}{    \begin{tabular}{c c c}
%             \includegraphics[width=0.32\textwidth]{imgs/mnist_gmm_generations.png} &
%             \includegraphics[width=0.32\textwidth]{images/mnist_glow_generations.png} &
%             \includegraphics[width=0.32\textwidth]{images/mnist_kpf_generations.png}\\
%             \includegraphics[width=0.32\textwidth]{images/cifar_gmm_generations.png} &
%             \includegraphics[width=0.32\textwidth]{images/cifar_glow_generations.png} &
%             \includegraphics[width=0.32\textwidth]{images/cifar_kpf_generations.png}
%             GMM & Glow & NTK-kPF\\
%         \end{tabular}
%     }
  
% \end{figure}

\newpage
\section{More Samples}

In this section we present additional uncurated set of samples on MNIST, CIFAR-10, CelebA based on pre-trained SRAE and FFHQ based on NVAE. From the figures, it can be seen that kPF produces consistent and diverse samples, often better in quality than the alternatives.
\vfill
\begin{figure}[h]
    \centering
    \includegraphics[width=0.495\textwidth]{imgs/supplement/mnist_gmm_generations.png}
    \includegraphics[width=0.495\textwidth]{imgs/supplement/mnist_glow_generations_2.png}\\
    \includegraphics[width=0.495\textwidth]{imgs/supplement/mnist_kpf_generations.png}\\
    \caption{MNIST results from $\textrm{SRAE}_\textrm{GMM}$ (top left), $\textrm{SRAE}_{Glow}$ (top right) and our $\textrm{SRAE}_\textrm{NTK-kPF}$.}
    \label{fig:mnist-more-samples}
\end{figure}
\vfill

\newpage
\vfill
\begin{figure}
    \centering
    \includegraphics[width=0.495\textwidth]{imgs/supplement/cifar_gmm_generations.png}
    \includegraphics[width=0.495\textwidth]{imgs/supplement/cifar_glow_generations_2.png}\\
    \includegraphics[width=0.495\textwidth]{imgs/supplement/cifar_kpf_generations.png}\\
    \caption{CIFAR results from $\textrm{SRAE}_\textrm{GMM}$ (top left), $\textrm{SRAE}_{Glow}$ (top right) and our $\textrm{SRAE}_\textrm{NTK-kPF}$.}
    \label{fig:cifar-more-samples}
\end{figure}
\vfill

\newpage
\vfill
\begin{figure}
    \centering
    \includegraphics[width=0.495\textwidth]{imgs/supplement/celeba_gmm_generations.png}
    \includegraphics[width=0.495\textwidth]{imgs/supplement/celeba_glow_generations_2.png}\\
    \includegraphics[width=0.495\textwidth]{imgs/supplement/celeba_kpf_generations.png}\\
    \caption{CelebA results from $\textrm{SRAE}_\textrm{GMM}$ (top left), $\textrm{SRAE}_{Glow}$ (top right) and our $\textrm{SRAE}_\textrm{NTK-kPF}$ (bottom).}
    \label{fig:celeba-more-samples}
\end{figure}
\vfill

\begin{figure}
    \centering
    \includegraphics[width = 0.95\textwidth]{imgs/supplement/NVAE_kpf_rbf_collage.png}
    \caption{Additional samples from kPF+NVAE pre-trained on FFHQ}
    \label{fig:nvae-more-samples}
\end{figure}

\clearpage