\section{Simplifying the estimation of the forward operator}

\paragraph{Forward operator as a dynamical system:} 
The dynamical system view of generative models has been described by others \citep{Chen2018neuralode, grathwohl2018scalable, behrmann2019invertible}. 
These strategies model the evolution of latent variables in a residual neural network in 
terms of its dynamics over continuous or discrete time $t$, and consider the output function $f$ as the evaluation function at a predetermined boundary condition $t = t_1$. Specifically, given an input (i.e., initial condition) $z(t_0)$, $f$ is defined as
\begin{equation}
    f(z(t_0)) = z(t_0) + \int_{t_0}^{t_1} \Delta_t(z(t)) dt
\end{equation}
where $\Delta_t$ is a time-dependent neural network function and $z(t)$ is the intermediate solution at $t$. This view of generative models is not limited to specific methods or model archetypes, but generally useful, for example, by viewing the outputs of each hidden layer as evaluations in discrete-time dynamics. After applying $f$ on a random variable $Z$, the marginal density of the output over any subspace $\Lambda \subseteq \mathcal{X}$ can be expressed as
\begin{align}
\label{eq0}
\int_\Lambda p_{f(Z)}(x) dx = \int_{z \in {f}^{-1}(\Lambda)} p_{\var{z}}(z)dz
\end{align}
If there exists some neural network instance $\Delta^\star_t$ such that the corresponding output function $f^\star$ satisfies $P_X = P_{f^\star(Z)}$, by Def. \ref{def:forward}, $f^\star$ is a forward operator. Let $\mathbf{X}$ be a set of \textit{i.i.d.} samples drawn from $P_X$. In typical generative learning, either maximizing the likelihood $\tfrac{1}{|\mathbf{X}|}\sum_{x \in \mathbf{X}} p_{f(Z)}(x)$ or minimizing the distributional divergence $d(P_{f(Z)}, P_{\mathbf{X}})$ requires evaluating and differentiating through $f$ or $f^{-1}$ many times.

\paragraph{Towards a one-step estimation of forward operator:} Since $f$ and ${f}^{-1}$ in \eqref{eq0} will be highly nonlinear in practice, evaluating and computing the gradients can be expensive. Nevertheless, the dynamical systems literature suggests a {\em linear} extension of $f^*$, namely the \textit{Perron-Frobenius} operator or transfer operator, that conveniently transfers $p_{\var{z}}$ to $p_{\var{x}}$.
\begin{definition}[Perron-Frobenius operator \citep{mayer1980ruelle}]
\label{def:pf}
Given a dynamical system $f: \spc{X} \to \spc{X}$, the Perron-Frobenius (PF) operator $\mathcal{P}: L^1(\mathcal{X}) \rightarrow L^1(\mathcal{X})$ is an \textit{infinite-dimensional linear} operator defined as $\int_\Lambda (\mathcal{P}p_{\var{z}})(x) dx = \int_{z \in {f^{-1}(\Lambda)}}p_{\var{z}}(z)dz$ for all $\Lambda \subseteq \mathcal{X}.$
\end{definition}

Although in Def. \ref{def:pf}, the PF operator $\mathcal{P}$ is defined for self-maps, it is trivial to extend $\mathcal{P}$ to mappings $f: \mathcal{Z} \to \mathcal{X}$ by restricting the RHS integral $\int_{z \in {f^{-1}(\Lambda)}}p_{\var{z}}(z)dz$ to $\mathcal{Z}$.

It can be seen that, for the forward operator $f^*$,  the corresponding PF operator $\mathcal{P}$ satisfies
\begin{align}
\label{eq1}
p_{\var{x}} = \mathcal{P}p_{\var{z}}. 
\end{align}
If $\mathcal{P}$ can be efficiently computed, transferring the tractable density $p_{\var{z}}$ to the target density $p_{\var{x}}$ can be accomplished simply by applying $\mathcal{P}$. 
%\revise{if we have figs for GLOW/VAE etc, refer to them %here}. 
However, since $\mathcal{P}$ is an infinite-dimensional operator on $L^1(\mathcal{X})$, it is impractical to instantiate it explicitly and exactly. 
Nonetheless, there exist several methods for estimating the Perron-Frobenius operator, including Ulam's method \citep{ulam1960collection} and the Extended Dynamical Mode Decomposition (EDMD) \citep{williams2015data}.
Both strategies project $\mathcal{P}$ onto a finite number of hand-crafted basis functions -- this may suffice 
in many settings but may fall short in modeling highly complex dynamics. 

% Thus, a natural solution is to search for a vector space of infinite dimension to model any complex dynamics. Next, we extend the PF operator in RKHS, denoted by embedded PF operator which will be expressive enough for complex dynamics. 

\paragraph{Kernel-embedded form of PF operator:} A natural 
extension of PF operator 
%over the previously mentioned methods 
is to represent $\mathcal{P}$ by an infinite set of functions \citep{klus2020eigendecompositions}, e.g., projecting it onto the bases of an RKHS via the \textit{kernel trick}. There, for a characteristic kernel $l$, the $\textit{kernel mean embedding}$ uniquely identifies an element $\mu_{\var{x}} = \mathcal{E}_{l}p_{\var{x}} \in \spc{G}$ for any $p_{\var{x}} \in L^1(\spc{X})$. Thus, to approximate $\mathcal{P}$, we may alternatively solve for the dynamics from $p_{\var{z}}$ to $p_{\var{x}}$ in their {\em embedded} form. Using Tab. \ref{tab:notations} notations, we have the following linear operator that defines the dynamics between two embedded densities.

% Let $(k, \mathcal{H}, \mathcal{E}_{k})$ and $(l, \mathcal{G}, \mathcal{E}_{l})$ be two sets of positive definite kernels, their induced RKHS and the corresponding mean embedding operator (as defined in Tab. \ref{tab:notations}). We can apply $\mathcal{E}_{l}$ on both sides on \eqref{eq1} to get
% \begin{align}
% \label{eq2}
% \mathcal{E}_{l}p_{\var{x}} = \mathcal{E}_{l}\mathcal{P}p_{\var{z}}.
% \end{align}

\begin{definition}[Kernel-embedded Perron-Frobenius operator \citep{klus2020eigendecompositions}]
\label{def:kpf}Given $p_{\var{z}} \in L^1(\mathcal{X})$ and $p_{\var{x}} \in L^1(\mathcal{X})$. Denote $k$ as the \textbf{input kernel} and $l$ as the \textbf{output kernel}. Let $\mu_{\var{x}} = \mathcal{E}_l p_{\var{x}}$ and $\mu_{\var{z}} = \mathcal{E}_k p_{\var{z}}$ be their corresponding mean kernel embeddings. The kernel-embedded Perron-Frobenius (kPF) operator, denoted
by $\mathcal{P}_{\mathcal{E}}:\mathcal{H}\rightarrow \mathcal{G}$, is defined as
%\boxed{
\begin{align}
    \mathcal{P}_\mathcal{E} = \mathcal{C}_{\var{x}\var{z}}\mathcal{C}_{\var{z}\var{z}}^{-1}
\end{align}
\end{definition}

\begin{proposition}[\citet{song2013kernel}] \label{prop:kpf}
With the above definition, $\mathcal{P}_\mathcal{E}$ satisfies
\begin{align}
   \mu_{\var{x}} = \mathcal{P}_\mathcal{E}\mu_{\var{z}}
\end{align}
under the conditions:   \begin{inparaenum}[\bfseries (i)] \item $\mathcal{C}_{\var{z}\var{z}}$ is injective \item $\mu_{t} \in \text{range}(\mathcal{C}_{\var{z}\var{z}})$ \item $\mathbb{E}[g(\var{x})|\var{z} = \cdot] \in \mathcal{H}$ for any $g \in G$.\end{inparaenum}
%}
\end{proposition}
The last two assumptions can sometimes be difficult to satisfy for certain RKHS (see Theorem 2 of \citet{fukumizu2013kernel}).  In such cases, a relaxed solution can be constructed by replacing $\mathcal{C}^{-1}_{ZZ}$ by a regularized inverse $(\mathcal{C}_{ZZ} + \lambda I)^{-1}$ or a Moore-Penrose pseudoinverse $\mathcal{C}^\dagger_{ZZ}$.
% \begin{tcolorbox}
% \begin{remark}
% Note that $\mathcal{P}_\mathcal{E}$ here essentially has the same form as the \textit{conditional mean embedding} $\mathcal{U}_{\var{x}|\var{z}}$ in \citep{song2013kernel}.
% \end{remark} 
% \end{tcolorbox}
% While the first assumption in Prop. \ref{prop:kpf} can be easily satisfied with topological $\spc{X}$, continuous $k$, and fully supported $p_{\var{z}}$, the last two assumptions often need to be relaxed by taking the regularized inverse $(C_{\var{z}\var{z}} + \lambda n I)^{-1}$ (see Theorem 2 of \citep{fukumizu2013kernel}).

\begin{figure}[t]
    \centering
    \vspace{-1em}
    \includegraphics[width=0.65\textwidth, clip, trim={0 2cm 5cm 0}]{imgs/kpf_workflow.png}
    \vspace{-8pt}
    \caption{Summary of our framework. The proposed operator is estimated to transfer RKHS embedded densities from one to anothter (blue arrows). Generating new samples (red arrows) involves embedding the prior samples $z^* \sim P_Z$, applying the operator $\Psi^* = \hat{\mathcal{P}}_\mathcal{E}\phi(z^*)$, and finding preimages $\psi^\dagger(\Psi^*)$. The pre-trained autoencoder projects the data onto a \textit{smooth} latent space and is only required when generating high-dimensional data such as images.}
    % \vspace{-1em}
    \label{fig:kpf_workflow}
\end{figure}

The following proposition shows commutativity between the (kernel-embedded) PF operator and the mean embedding operator, showing its equivalence to $\mathcal{P}$ when $l$ is characteristic.

\begin{proposition}[\citep{klus2020eigendecompositions}]
  \label{prop:klus}
   With the above notations, $\mathcal{E}_l \circ \mathcal{P} = \mathcal{P}_{\mathcal{E}} \circ \mathcal{E}_k$.
\end{proposition}

\paragraph{Transferring embedded densities with the kPF operator:} The kPF operator is a powerful tool that allows transferring embedded densities in RKHS. The main steps are:
\begin{tcolorbox}[top=0mm,bottom=0mm]
\begin{compactenum}[\bfseries (1)] \item Use mean embedding operator $\mathcal{E}_{l}$ on $p_{\var{z}}$. Let us denote it by $\mu_{\var{z}}$.  \item  Transfer $\mu_{\var{z}}$ using kPF operator $\mathcal{P}_\mathcal{E}$ to get the mean embedded $p_{\var{x}}$, given by $\mu_{\var{x}}$.\end{compactenum}
\end{tcolorbox}
Of course, in practice with finite data, $\{\mathbf{x}_i\}_{i \in [n]} \sim P_{\var{x}}$ and $\{\mathbf{z}_i\}_{i \in [n]} \sim P_{\var{x}}$, $\mathcal{P}_\mathcal{E}$ must be estimated empirically (see \citet{klus2020eigendecompositions} for an error analysis).
\begin{align}
\hat{\mathcal{P}}_\mathcal{E} = \hat{\mathcal{C}}_{\var{x}\var{z}}(\hat{\mathcal{C}}_{\var{z}\var{z}})^{-1} \nonumber \approx \Psi(\Phi^T\Phi + \lambda n I)^{-1}\Phi^T \approx \Psi(\Phi^T\Phi)^\dagger\Phi^T
\end{align}
where $\Phi = [k(\mathbf{z}_1, \cdot), \cdots, k(\mathbf{z}_n, \cdot)],~
 \Psi = [l(\mathbf{x}_1,\cdot), \cdots, l(\mathbf{x}_n, \cdot)]$ are simply the corresponding feature matrices for samples of $P_{\var{x}}$ and $P_{\var{z}}$, and $\lambda$ is a small penalty term.

\paragraph{Learning kPF for unconditional generative modeling:} Some generative modeling methods such as VAEs and 
flow-based formulations explicitly model the latent variable $Z$ as conditionally dependent on the data variable $X$. This allows deriving/optimizing the likelihood $p_{f(Z)}(X)$. 
This is desirable but may not be essential in all 
applications. 
To learn a kPF, however, $X$ and $Z$ can be independent RVs. While it may not be immediately obvious why we could assume this independence, we can observe the following property for the empirical kPF operator, assuming that the empirical covariance operator $\hat{\mathcal{C}}_{ZZ}$ is non-singular:
%
% In unconditional generative modeling, often we assume a joint distribution of the data variable $X$ and a latent variable $Z$, such that the marginal $p_{Z}$ conforms to a known simple distribution (e.g., Gaussian). However, since the joint distribution $p(X, Z)$ is typically unknown, sampling jointly in general is not possible. Existing generative methods that learn with explicit densities often use additional mechanisms to capture the joint (e.g. approximate posterior in VAE). Nevertheless, to learn a kPF operator, we may simply create an known-distributed latent variable $Z$ that contains no information of $X$, in other words, $X$ and $Z$ are independent RVs. While it may not be immediately obvious why we could assume this independence, we have the following property for the empirical kPF operator assuming that the empirical covariance operator $\hat{\mathcal{C}}_{ZZ}$ is non-singular:
%
\begin{align}\label{eq:emp_mean_emb}
    \hat{\mathcal{P}}_{\mathcal{E}}\hat{\mu}_{Z} &= \hat{\mathcal{C}}_{XZ}\hat{\mathcal{C}}_{ZZ}^{-1}\hat{\mu}_{Z} = \underbrace{\Psi \Phi^\top}_{\hat{\mathcal{C}}_{XZ}} (\underbrace{\Phi \Phi^\top}_{\hat{\mathcal{C}}_{ZZ}})^{-1} \Phi \mathds{1}_{n}
    = {\Psi (\Phi^\top \Phi)^{-1} \Phi^\top \Phi} \mathds{1}_{n} = \Psi \mathds{1}_{n} = \hat{\mu}_{X}
\end{align}
%
Suppose that $\{\mathbf{x}_i\}_{i \in [n]}$ and $\{\mathbf{z}_j\}_{i \in [n]}$ are independently sampled from the marginals $P_X$ and $P_Z$. It is easy to verify that (\ref{eq:emp_mean_emb}) holds for any pairing $\{(\mathbf{x}_i, \mathbf{z}_j)\}_{(i, j) \in [n] \times [n]}$. However, instantiating the RVs in this way rules out the use of kPF for certain downstream tasks such as controlled generation or mode detection, since $\var{Z}$ does not contain information regarding $\var{X}$. Nevertheless, if sampling is our only goal, then this instantiation of kPF will suffice.
% Learning the kPF operator is similar in spirit to other implicit density models such as GAN, where the search criteria is based on sample discrepancy rather than explicit likelihoods.

% Since we cannot make strong assumptions about the joint in this case, our construction of the kPF operator $\mathcal{P}_\mathcal{E}$ does not seek to explicitly establish a joint density model; rather, we are only learning a mapping such that the transferred population using the operator matches with the data population in distribution, a spirit shared by many implicit density modeling methods \citepp{arjovsky2017wasserstein,li2017mmdgan}.

% \begin{wrapfigure}{r}{0.425\textwidth}
%     \vspace{-8pt}
%     \centering
%     \includegraphics[width=0.23\linewidth]{imgs/data_dist_2spirals.png}
%     \includegraphics[width=0.23\linewidth]{imgs/data_dist_8gaussians.png}
%     \includegraphics[width=0.23\linewidth]{imgs/data_dist_checkerboard.png}
%     \includegraphics[width=0.23\linewidth]{imgs/data_dist_rings.png}\\
%     \includegraphics[width=0.23\linewidth]{imgs/distribution_2spirals_gmm.png}
%     \includegraphics[width=0.23\linewidth]{imgs/distribution_8gaussians_gmm.png}
%     \includegraphics[width=0.23\linewidth]{imgs/distribution_checkerboard_gmm.png}
%     \includegraphics[width=0.23\linewidth]{imgs/distribution_rings_gmm.png}\\
%     \includegraphics[width=0.23\linewidth]{imgs/distribution_2spirals_glow.png}
%     \includegraphics[width=0.23\linewidth]{imgs/distribution_8gaussians_glow.png}
%     \includegraphics[width=0.23\linewidth]{imgs/distribution_checkerboard_glow.png}
%     \includegraphics[width=0.23\linewidth]{imgs/distribution_rings_glow.png}\\
%     \includegraphics[width=0.23\linewidth]{imgs/supplement/distribution_2spirals_kpf_density.png}
%     \includegraphics[width=0.23\linewidth]{imgs/supplement/distribution_8gaussians_kpf_density.png}
%     \includegraphics[width=0.23\linewidth]{imgs/supplement/distribution_checkerboard_kpf_density.png}
%     \includegraphics[width=0.23\linewidth]{imgs/supplement/distribution_rings_kpf_density.png}\\
%     \vspace{-8pt}
%     \caption{Density estimation on 2D toy data. \textbf{Top to bottom: (1)} Training data samples, and learned densities of \textbf{(2)} GMM \textbf{(3)} Glow \textbf{(4)} Proposed kPF operator. Details of this experiment can be found in the supplement.}
%     \vspace{-4em}
%     \label{fig:density}
%     % \vspace{-7em}
% \end{wrapfigure}
%
\paragraph{Mapping $Z$ to $\mathcal{G}$:} Now, since  $\mathcal{P_\mathcal{E}}$ is a {\em deterministic} linear operator, we can easily set up a scheme to map samples of $\var{z}$ to elements of $\mathcal{G}$ where the expectation of the mapped samples equals $\mu_{\var{x}}$

% Now, we can easily set up a strategy to map samples of $\var{z}$ to elements of $\mathcal{G}$ by utilizing the fact that $\mathcal{P_\mathcal{E}}$ is a deterministic linear operator and show that the mapped density asymptotically converges to $p_{\var{x}}$. 

Define $\phi(z) = k(z, \cdot)$ and $\psi(x) = l(x, \cdot)$ as feature maps of kernels $k$ and $l$. We can rewrite $\mu_{\var{x}}$  as 
\begin{align}
{{\mu}_{\var{x}}} = \mathcal{P}_\mathcal{E}\mathcal{E}_k {p_{\var{z}}}  
                = \mathcal{P}_\mathcal{E}E_{\var{z}}[\phi(\var{z})]
                = E_{\var{z}}[\mathcal{P}_\mathcal{E}(\phi(\var{Z}))]
                = E_{\var{Z}}[\psi\left(\psi^{-1}\left(\mathcal{P}_\mathcal{E} \phi\left(\var{z}\right)\right)\right)]
\label{eq:approx_sample}
\end{align}
Here $\psi^{-1}$ is the inverse or the \textit{preimage map} of $\psi$. Such an inverse, in general, may not exist \citep{kwok2004pre,honeine2011preimage}. We will discuss a procedure to approximate $\psi^{-1}$ in \S\ref{sec:preimage}. In what follows, we will temporarily assume that an exact preimage map exists and is tractable to compute. 

Define $\Psi^* = \hat{\mathcal{P}}_\mathcal{E} \phi(Z)$ as the \textit{transferred sample} in $\mathcal{G}$ using the empirical embedded PF operator $\hat{\mathcal{P}}_\mathcal{E}$. Then the next result shows that asymptotically the transferred samples converge in distribution to the target distribution. 

\begin{proposition}
  \label{prop:convergence} As $n \to \infty$, $\psi^{-1}\left(\Psi^*\right) \stackrel{\mathcal{D}}{\to} P_{\var{x}}$. That is, the preimage of the transferred sample approximately conforms to  $P_{\var{X}}$ under previous assumptions when $n$ is large.
\end{proposition}
\begin{proof}
Since $\hat{\mathcal{P}}_\mathcal{E} \overset{\text{asymp.}}{\to} \mathcal{P}$, the proof immediately follows from \eqref{eq:approx_sample}.
\end{proof}
%\begin{proof}
% The above statement can be shown by noticing that \eqref{eq:approx_sample} has the same form as the kernel mean embedding $\mu_{\var{x}} = \
% \mathcal{E}_{l}p_{\var{x}} = E_{\var{x}}[l(\var{x}, \cdot)]$. %which finishes the proof.
%\end{proof}
%We observe that \eqref{eq:approx_sample} has the exact form of the empirical estimate of $\mu_{\var{x}}$. Thus, $\Psi^*_i = \mathcal{P}_\mathcal{E} k(z_i, \cdot)$ are the feature mappings of transferred samples in $\mathcal{G}$ and $[\psi^{-1}(\Psi^*_i)]_{i \in [s]}$ can be viewed approximately as samples drawn from $p_{\var{x}}$. Since the empirical mean kernel embedding of the approximate samples converges asymptotically to that of $p_{\var{x}}$, their distribution also asymptotically converges to $p_{\var{x}}$, which gives our main result.

% There are, however, several issues remain to be addressed in order to ensure the validity and practicality of this method. We will continue to discuss our solutions to these issues in the following sections.


% \begin{figure*}[!ht]
% %\vspace*{-2em}
%     \centering
%     \textcolor{red!50!white}{\fboxrule=1pt\fbox{
%     \includegraphics[width=0.22\linewidth]{imgs/mnist_data_latent.png}}}
%     \textcolor{blue!50!white}{\fboxrule=1pt\fbox{
%     \includegraphics[width=0.22\linewidth]{imgs/rbf_sampled.png}
%     \includegraphics[width=0.22\linewidth]{imgs/arccos_sampled.png}
%     \includegraphics[width=0.22\linewidth]{imgs/ntk_sampled.png}}}
%     % \vspace*{-1em}
%     \caption{\footnotesize 10k samples from MNIST dataset ({\it left to right}) (a) projected on $\mathbf{S}^2$ shown in $(\theta, \phi)$ using auto-encoder, and 10K generated samples from kPF with input kernel of type (b) RBF (c) arccos (d) NTK. Color of sampled points represents the class of their 'nearest' point in the output RKHS.}  \label{fig:mnist_gen}
% \end{figure*}

% \subsection{Choice of Kernel is relevant yet flexible}\label{sec:choice_of_kernel}
% In some cases, one would focus on identifying (finite) eigenfuntions and modes of the underlying operator \citep{williams2015extDMD,brunton2016sindy}. But rather than finding certain modes that best characterize the dynamics, we care most about minimizing the error of the transferred density and therefore whether the span of functions is rich/expressive enough. In particular, condition (iii) in Proposition \ref{prop:klus} requires an input RKHS spanned by sufficiently rich bases \citepp{fukumizu2013kernel}. For this reason, the choice of kernel here 
% cannot be completely ignored since it determines the family of functions contained in the induced RKHS.

% To explore the appropriate kernel setup for our application, we empirically evaluate the effect of using several different kernels via a simple experiment on MNIST. \revise{We first train an autoencoder to embed MNIST digits on to a hypersphere $S^2$, then generate samples from kPF by the procedure described later in Sec. \ref{sec:gen_sample} using the respective kernel function as the input kernel $k$}. Subplot (b) and (c) in Fig. \ref{fig:mnist_gen} show the generated samples using Radial Basis Function (RBF) kernel and arc-cosine kernel, respectively. Observe that the choice of kernel has a clear influence on the sample population, and a kernel function with superior empirical behavior is desirable. 

% Motivated by this observation, we evaluated the Neural Tangent Kernel (NTK) \citep{jacot2018NTK}, a well-studied neural kernel in recent work. We use it for a 
% few reasons, 
% \begin{inparaenum}[\bfseries (a)] \item NTK, in theory, corresponds to a trained infinitely-wide neural network, which spans a rich set of functions that satisfies the assumption. 
%   %Therefore, NTK is spanned by a rich set of nonlinear functions
% \item For well-conditioned inputs (i.e., no duplicates) on hypersphere,
%   the positive-definiteness of NTK is proved in \citep{jacot2018NTK}. Therefore, invertibility of the Gram matrix $K_{\var{z}\var{z}} = \Phi^T\Phi$ is \textit{almost guaranteed} if the prior distribution $p_{\var{z}}$ is restricted on a hypersphere
% \item
%   NTK can be non-asymptotically approximated \citep{arora2019onexact}.
% \item Unlike other parametric kernels such as RBF kernels, NTK is less sensitive to hyperparameters, as long as the number of units used is large enough \citep{arora2019onexact}.
% \end{inparaenum} Subplot (c) of Fig. \ref{fig:mnist_gen} shows that kPF learned with NTK as input kernel is able to generate samples that are more consistent with the data distribution. However, we should note that NTK is merely a convenient choice of kernel that requires less tuning, and is not otherwise central to our work. In fact, as later showed in our experiment, a well-tuned RBF kernel may also achieve a similar performance. Indeed, in practice, any suitable choice of kernel may be conveniently adopted into the proposed framework without major modifications.

%  Motivated by this observation, we evaluate the recently proposed Neural Tangent Kernel (NTK) \citep{jacot2018NTK} as the embedding kernel $k$ for $p_{\var{z}}$ due to the following properties:
% \begin{inparaenum}[\bfseries (a)] \item NTK, in theory, corresponds to a trained infinitely-wide neural network, which spans a rich set of functions that satisfies the assumption. 
%   %Therefore, NTK is spanned by a rich set of nonlinear functions
% \item For well-conditioned inputs (i.e., no duplicates) on hypersphere,
%   the positive-definiteness of NTK is proved in \citep{jacot2018NTK}. Therefore, invertibility of the Gram matrix $K_{\var{z}\var{z}} = \Phi^T\Phi$ is \textit{almost guaranteed} if the prior distribution $p_{\var{z}}$ is restricted on a hypersphere
% \item
%   NTK can be non-asymptotically approximated \citep{arora2019onexact}.
% \item Unlike other parametric kernels such as RBF kernels, NTK is less sensitive to hyperparameters, as long
%   as the number of units used is large enough \citep{arora2019onexact}.
% \end{inparaenum}
% The embedding of generated samples for MNIST using NTK is also shown in Fig. \ref{mnist_gen}.
% Observe that the sample distribution more closely resembles the original distribution.
  
  
%using either of these two kernels fails to recover the true distribution accurately.
%Such sensitivity of the choice of kernel necessitates choosing
% and a kernel with superior empirical behavior would be desirable. 



%% Often, the operator of a flow-based generative model is learned by maximizing the likelihood of data. Because the data is usually discrete in input space, given sufficient modeling power, such optimization objective can result in the forwarded densities collapsing around the data. However, assuming the data is projected onto a sufficiently $\textit{smooth}$ low-dimensional feature space, it suffices to learn the mapping from a known distribution to the empirical distribution of projected data directly, avoiding the issue of collapsing densities. Furthermore, the flow-based generative models often suffer from large memory consumption or slow inference. Hence, the need for efficient (invertible) ``transformation'' of tractable density to the target density is eminent and serves as the motivation for this work.

% Flow-based generative model directly learn the mapping from a known
% distribution to the target data distribution in the input space through likelihood maximization.
% During image generation, the fact that data may often lie on or near a low-dimensional manifold is
% usually not explicitly leveraged. 
% If one could estimate this manifold perfectly, we could 
% uniquely identify a distribution in the input space.
% But often, the manifold can only be empirically estimated.
% Our simplifying assumption here is that with some structural or smoothness constraints on
% the distribution on the latent space, 
% we should still be able to approximately identify the true distribution. 
% VAEs use this intuition to make the latent distribution consistent with a standard normal distribution, 
% and several contemporary works also propose estimating flow on latent spaces \citep{Kingma2016improved,Mahajan2020Latent}.
% We will follow this rationale and use an autoencoder -- here, benefiting from the reduced variance in
% the latent space, our approach will effectively learn the mapping from a known simple distribution to the
% empirical latent distribution.

% \textbf{Structure of the latent space.}  We will assume that a suitable auto-encoder is either provided or can
% be successfully trained from scratch for the data at hand. 
% Given measurements from an input data space, $\mathcal{M}$, we seek
% to learn a mapping $E:\mathcal{M} \rightarrow \mathcal{L}\subset \mathbf{R}^d$,
% with $d< m$, where $\mathcal{M}\subset \mathbf{R}^m$ \revise{and $\mathcal{L}$ is the latent space}.
% Additionally, we also learn the inverse of the mapping $E$ which is denoted by
% $D:\mathcal{L}\rightarrow \mathcal{M}$, i.e., we require $D\circ E = \text{id}$ (identity).
% Further, we require $\mathcal{L}$ to be a bounded subset of $\mathbf{R}^d$.
% For simplicity, we model it as a
% unit hypersphere $\mathbf{S}^{d-1}$. This essentially ensures that $\mathcal{L}$ is
% \begin{inparaenum}[\bfseries (a)] \item bounded \item geodesically complete \item geodesically convex
% \end{inparaenum} space. 


% \begin{figure*}[!t]
%   \centering
%   \includegraphics[scale=0.8]{imgs/PF-Flow-workflow_final}%
%   \caption{\label{fig:network} \footnotesize
%     A description of generative procedure in our method. {\it (a) schematic of an auto-encoder (b) kernel representations of latent space and noise from prior (c) operator learning and inferencing (d) solve pre-image and decode the generated latent representation to obtain final generation result in data space}
%   }
% \end{figure*}

% \begin{Remark}
%   A bounded latent space with specified structure
%   provides several benefits over one without any such constraints.
%   First, it guarantees that the Gram matrix constructed with a (positive-definite) kernel function
%   is bounded, which is required for a valid empirical estimation of the mean embedding in Def. \ref{def:me} \citep{Song2009hilbert}.
%   Moreover, the projection onto the hypersphere also helps ensure the positive-definiteness of
%   kernels constructed using the latent representations, which is discussed in \ref{para:NTK}. 
%   At this point, the requirement of geodesic completeness and convexity may not be apparent 
%   but these properties will be useful for sample generation.
% \end{Remark}

% \subsection{Using Transfer Operators to estimate Flow}

% %With a (almost) vanilla
% If training the auto-encoder provides us a suitably well-structured latent space, as desired,
% we can now study how the density transfer operators can help transfer a tractable density
% to the density on the latent space, i.e., {\it mimicking} 
% the mechanics within a flow-based generative model.
% %(in terms of transferring density) on the ``well-structured'' latent space given by the encoder $E$}. 

% \textbf{Transfer operators.}  Let $\left\{X_t\right\}_{t\geq 0}$ be a stationary and ergodic
% Markov process defined on $\mathcal{X}$. Then, we can define the transition density function,
% $p_{\tau}$ (with time lag $\tau$), by
% \begin{align}
%   P[X_{t+\tau}\in \mathcal{A} | X_t = x] = \int_{\mathcal{A}} p_{\tau}(y|x) dy,
%   \text{where $\mathcal{A}$ is a measurable set on $\mathcal{X}$.}
% \end{align}

% Let $L^1(\mathcal{X})$ be the space of probability densities. We wish to emulate 
% the dynamical system of interest by instead utilizing a well-known transfer operator,
% namely the (Ruelle) Perron-Frobenius operator \citep{mayer1980ruelle}, where the ``transfer'' terminology
% originates from statistical mechanics. \revise{ Notice that the notion of $t$ in our work is not literal - it merely serves as a convenient way to describe the transfer operators in the context of evolution of states, and is also analogous to how flow is considered explicitly as continuous dynamical systems in Neural ODE \citep{Chen2018neuralode}}
%some additional lines from https://mathoverflow.net/questions/47726/ruelle-perron-frobenius-operator/47728


% \begin{definition}[Perron-Frobenius operator \citep{mayer1980ruelle}]
% \label{def:pf}
% The Perron-Frobenius (PF) operator $\mathcal{P}: L^1(\mathcal{X}) \rightarrow L^1(\mathcal{X})$
% push-forwards or transfers a probability density $p_t\in L^1(\mathcal{X})$
% given the lag $\tau$ as $\left(\mathcal{P} p_t\right)(y) = \int p_{\tau}(y|x)p_t(x) dx$. 
% \end{definition}

% As mentioned already, we will use the PF operator to transfer the tractable probability density to the latent space (where the data distribution is unknown and possibly intractable). 
% z
% The PF operator will be an efficient alternative to the ``expensive'' flow-based generative model if \begin{inparaenum} \item PF operator is computationally efficient \item we can relax the invertibility of the flow-based model as clearly PF operator is not invertible. \end{inparaenum}.

% \begin{Remark}
% If such an operator can be efficiently estimated, we can use it to transfer the tractable probability density from a known distribution $p_t$ to the target distribution $p_{t + \tau}$ whose density is generally unknown. Since we do not make any distributional assumptions on the target distribution,
% a linear solution to such dynamics in the input space, in general, may not exist.
% But notice that performing these operations {\em after} a mapping to an higher dimensional (linear) space is more sensible. 
% \revise{Nevertheless, in spaces spanned by a sufficiently large set of non-linear functions, or specifically, an RKHS, one can potentially identify a linear operator that is equivalent to a highly non-linear operator in the input space. In  \citep{klus2020eigendecompositions}, the authors showed two important results that define such operators in RKHS in terms of covariance and cross-covariance operators, namely kernel Perron-Frobenius operator (kPF) $\mathcal{P}_{k}$ and embedded Perron-Frebenius operator (ePF) $\mathcal{P}_{\mathcal{E}}$.} \\

% \revise{Let $X \sim p_t$ and $Y \sim p_{t + \tau}$ be observations at time $t$ and $t + \tau$. Let $\mathcal{H}$ and $\mathcal{G}$  be the RKHSs associated with a certain kernel $k$, $\phi_H$ and $\phi_G$ be their respective feature mapping. The cross-covariance operator $C_{YX}:\mathcal{H}\rightarrow \mathcal{G}$
% is defined as $C_{YX} = E_{YX}\left[\phi_G(Y) \otimes \phi_H(X) \right]$, and the covariance operator $C_{XX}:\mathcal{H} \rightarrow \mathcal{H}$ is
% defined analogously. If both densities of interest live in the same RKHS, that is, $p_t \in \mathcal{H}$ and $p_{t + \tau} \in \mathcal{H}$, $\mathcal{P}_{k}$ can be used to push forward the density such that $p_{t + \tau} = \mathcal{P}_{k} p_t$. However, in our setting, $\mathcal{P}_{k}$ is not directly applicable since the densities we are interested are elements of $L^1(\mathcal{X})$. Therefore, the densities need to be first embedded into the RKHS and can then be transferred using the $\mathcal{P}_{\mathcal{E}}$ in the embedded form, namely $\mathcal{E}p_{t + \tau} = (\mathcal{P}_{\mathcal{E}} \circ \mathcal{E})p_t$}. 
%{\color{red} If the dynamical system we wish to emulate involves an action $\Gamma$ (say, a matrix
%with positive entries), in other words, the iterates $\Gamma^t$,
%the behavior of the Perron-Frobenius operator must emulate $\Gamma$. This statement is exact for
%certain Markov chains. Further, since we wish to utilize a mapping to a RKHS, we will need
%to utilize analogous results for kernelized Perron-Frobenius operators \citep{fukumizu2013kernel}.}
% \end{Remark}

%% \paragraph{Kernel transfer operators} We extends the definition of PF operator, which is defined on the input space, to the kernel Perron-Frobenius (kPF) operator on RKHS \citep{}. This will give us an computationally efficient PF operator (or a good estimate).

% \begin{definition}[Embedded Perron-Frobenius operator \citep{klus2020eigendecompositions}]
% \label{def:kpf} \revise{Given $p_t \in L^1(\mathcal{X})$ and $p_{t + \tau} \in L^1(\mathcal{X})$. Let $\mu_{t} = \mathcal{E}p_t$ and $\mu_{t+ \tau} = \mathcal{E}p_{t + \tau}$ be their respective kernel mean embedding. The kernel Perron-Frobenius (kPF) operator, denoted
% by $\mathcal{P}_{\mathcal{E}}:\mathcal{H}\rightarrow \mathcal{G}$, is defined by
% %\boxed{
% $\mu_{t + \tau} = \mathcal{P}_\mathcal{E}\mu_{t} = C_{YX}(C_{XX} + \epsilon I)^{-1}\mu_{t}  ~\refstepcounter{equation}(\theequation)$\label{eq:kpf}, under the condition that \begin{inparaenum}[\bfseries (i)] \item $C_{XX}$ is injective \item $\mu_{t} \in \text{Range}(C_{XX})$ \item $E[Y|X = \cdot] \in \mathcal{H}$.\end{inparaenum}}
% %}
% \end{definition}

% \revise{Notice that the above defined $\mathcal{P}_\mathcal{E}$ essentially has the same form as the kernel conditional embedding in \citep{song2013kernel}. While the first two conditions in the above definition can be satisfied (see Theorem 2 of \citep{fukumizu2013kernel}), the last condition requires $\exists f \in \mathcal{H}$ s.t. $\forall x$, $f(x) = E[Y| X = x]$, which highlights the importance of the kernel choice. With this operator, we can transfer $p_t$ to $p_{t + \tau}$ in their embedded forms. The following proposition demonstrates the implication of using the embedded Perron-Frobenius operator.}

% \begin{proposition}[\citep{klus2020eigendecompositions}]
%   \label{prop:klus}
% With the above notations, $\mathcal{E}\circ \mathcal{P} = \mathcal{P}_{\mathcal{E}} \circ \mathcal{E}$.
% \end{proposition}

% \begin{Remark}
%   \revise{The commutativity in the proposition shows that the \textit{transferred kernel mean embedding} of $p_t$ by the linear operator we constructed in RKHS is equivalent to the kernel mean embedding of \textit{transferred $p_t$} by a highly nonlinear operator in the input space.}
% \end{Remark}

% \revise{This paragraph needs revision}
% The above defined kPF operator can be used to transfer $g\in \mathcal{H}$, denoted by (as given in \eqref{eq:kpf}), $\mathcal{P}_kg = \left(C_{XX}+\epsilon I\right)^{-1}C_{YX}g$. Notice that this equation is well-defined only if $\mathcal{G} = \mathcal{H}$. 
% But in our setting, the probability density we are interested in transferring, $p_t$, is defined on $L^1(\mathcal{X})$ rather than in the RKHS. {\it So, kPF cannot be used
%   directly since $g$ lies on a RKHS and may not be a probability density.}
% As a solution, it turns out that we can utilize mean embedding as defined in Def. \ref{def:me} and use the operator in the embedded form: given a  $p_t \in L^1(\mathcal{X})$, we may
% first use mean embedding to get the corresponding $\mu_{t} = \mathcal{E}p_t \in \mathcal{H}$, where $\mathcal{E}$ is the mean embedding operator. Then,
% the Perron-Frobenius operator for {\em embedded} densities, denoted by {\it embedded Perron-Frobenius operator},
% $\mathcal{P}_{\mathcal{E}}$, can be expressed as $\mathcal{P}_{\mathcal{E}} = C_{YX}\left(C_{XX}+\epsilon I\right)^{-1}$.

% Notice that compared to kPF operator, \revise{we do not need the assumption of
% $\mathcal{G} = \mathcal{H}$ for $\mathcal{P}_{\mathcal{E}}$ to be a valid mapping}, which is due to the reproducing property of kernel (see \citep{klus2020eigendecompositions} section 3.3 for details).


% \revise{Since $p_{t + \tau}$ is generally intractable, the operator can only be empirically estimated. Given samples $X = \{x_i\}_{i =1}^N \sim p^N_t$ and $Y = \{y_i\}_{i =1}^N \sim p^N_{t + \tau}$, let $\Phi_H$ and $\Phi_G$ denote the feature maps of $X$ and $Y$, respectively. The sample estimate of the embedded Perron-Frobenius operator is given by 
% $\widehat{\mathcal{P}}_{\mathcal{E}} = \widehat{\mathcal{C}}_{YX}(\widehat{\mathcal{C}}_{XX} + \epsilon I)^{-1} = \Phi_G  \left(G_{XX}+N\epsilon I\right)^{-1} \Phi_H^T ~\refstepcounter{equation}(\theequation)\label{eq:mkpfe}$
% , where $G_{XX} = \Phi_H^\top \Phi_H$ is the Gram matrix of $\Phi_H$. To generate a sample of the transferred density which is $\textit{approximately}$ $p_{t + \tau}$ using a sample $x^*$ of $p_t$, we can construct $\phi_\mathcal{G}(y^*) = \hat{\mathcal{P}}_{\mathcal{E}}\phi_\mathcal{H}(x_i^*) = \Phi_G(G_{XX} + \epsilon I)^{-1}k(X, x^*)$. The distribution of the resulting samples has the following property,}

% {\color{blue} I DO NOT UNDERSTAND THIS SENTENCE  During inference, given $n$ new samples $\{x_i^*\}_{i = 1}^n \sim p_t^n$, the feature maps of generated samples on the target space $\{y_i^*\}_{i = 1}^n$ can be constructed as $\phi_\mathcal{G}(y_i^*) = \hat{\mathcal{P}}_{\mathcal{E}}\phi_\mathcal{H}(x_i^*) = \langle \hat{\mathcal{P}}_{\mathcal{E}}, k(x_i^*, \cdot) \rangle$.}}

% \begin{theorem}[Proof in \ref{proof:generator}]
% \label{theorem: mean_emb}
% Let $\mu_{t + \tau}$ be the kernel mean embedding of the true distribution $p_{t + \tau}$. The resulting empirical mean embedding $\hat{\mu}^*_{t + \tau} = \tfrac{1}{n}\sum_{i = 1}^n \phi_G(y_i^*)$ satisfies $E[\hat{\mu}^*_{t + \tau}] = \mu_{t + \tau}$
% \end{theorem}
% \revise{Theorem \ref{theorem: mean_emb} simply implies that $\hat{P}_\mathcal{E}\hat{\mu}^*_{t + \tau}$ is an unbiased estimator of the kernel mean embedding of the true distribution on the latent space. If the kernel is characteristic and the exact preimage exists, then $\phi^{-1}_\mathcal{G}(\hat{P}_\mathcal{E}\phi_\mathcal{H}(x^*)) \sim p_{t + \tau}$ asymptotically, which concludes our main result.}

% With these results, we have all the tools in hand to have almost a single step flow based generative model
% if we start from a well-structured encoding latent space.

%{\color{red}We will discuss shortly that unlike a standard flow based generative model,
%  an explicit requirement on invertibility will not be needed in our formulation since
%  we {\it only need to transfer a tractable density to the latent space}.  }


\section{Sample generation using the Kernel transfer operator}\label{sec:gen_sample}
At this point, the transferred sample  $\Psi^*$, obtained by the kPF operator, remains an element of RKHS $\spc{G}$. To translate the samples back to the input space, we must find the  preimage $x^*$ such that $\psi(x^*) = \Psi^*$.

\subsection{Solving for an approximate preimage}\label{sec:preimage}

% \begin{wrapfigure}{r}{0.425\textwidth}
% \vspace{-18pt}
%     \centering
%     \includegraphics[width=0.23\linewidth]{imgs/data_2spirals.png}
%     \includegraphics[width=0.23\linewidth]{imgs/data_8gaussians.png}
%     \includegraphics[width=0.23\linewidth]{imgs/data_checkerboard.png}
%     \includegraphics[width=0.23\linewidth]{imgs/data_rings.png}\\
%     \includegraphics[width=0.23\linewidth]{imgs/samples_2spirals_mds.png}
%     \includegraphics[width=0.23\linewidth]{imgs/samples_8gaussians_mds.png}
%     \includegraphics[width=0.23\linewidth]{imgs/samples_checkerboard_mds.png}
%     \includegraphics[width=0.23\linewidth]{imgs/samples_rings_mds.png}\\
%     \includegraphics[width=0.23\linewidth]{imgs/samples_2spirals_wm.png}
%     \includegraphics[width=0.23\linewidth]{imgs/samples_8gaussians_wm.png}
%     \includegraphics[width=0.23\linewidth]{imgs/samples_checkerboard_wm.png}
%     \includegraphics[width=0.23\linewidth]{imgs/samples_rings_wm.png}\\
%     \caption{Sample generation results. \textbf{Top:} Data samples \textbf{Middle:} Samples by MDS-based preimage \textbf{Bottom:} Samples by weighted Fr\'{e}chet mean.}
%     \label{fig:samples}
%     \vspace{-1em}
% \end{wrapfigure}

Solving the preimage in kernel-based methods is known to be ill-posed \citep{Mika1999kpca} because the mapping $\psi(\cdot)$ is not necessarily surjective, i.e., a unique preimage $\mathbf{x}^* = \psi^{-1}(\Psi^*), \Psi^* \in \mathcal{H}$ may not exist. Often, an approximate preimage $\psi^{\dagger}(\mathbf{X}, \Psi^*) \approx \psi^{-1}(\Psi^*)$ is constructed instead based on relational properties among the training data in the RKHS. We consider two options in our framework \textbf{(1)} MDS-based method \citep{kwok2004pre,honeine2011preimage}, 
\begin{equation}
\psi_{\textrm{MDS}}^{\dagger}(\mathbf{X}, \Psi^*) = \tfrac{1}{2}(\mathbf{X}'\mathbf{X}'^\top)^{-1}\mathbf{X}'(\textrm{diag}(\mathbf{X}'^\top \mathbf{X}') - \mathbf{d}^\top), \textrm{where}\;\forall i \in [\gamma], \mathbf{d}_i = \Vert l(\mathbf{x}'_i, \cdot) - \Psi^* \Vert_{\mathcal{G}}
\end{equation}
which optimally preserves the distances in RKHS to the preimages in the input space, and \textbf{(2)} weighted Fr\'{e}chet mean \citep{friedman2001elements}, which in Euclidean space takes the form
\begin{equation}
\label{eq:wfm_euc}
\psi_{\textrm{wFM}}^{\dagger}(\mathbf{X}, \Psi^*) = \psi_{\textrm{wFM}}^{\dagger}(\mathbf{X}'; \mathbf{s}) = \mathbf{X}'\mathbf{s}/\Vert \mathbf{s} \Vert_1, \textrm{where}\;\forall i \in [\gamma],\mathbf{s}_i = \langle l(\mathbf{x}'_i, \cdot), \Psi^* \rangle
\end{equation}

\begin{wraptable}{r}{0.5\textwidth}
    \vspace{-2em}
    \begin{minipage}{0.5\textwidth}
        \begin{algorithm}[H]
        \footnotesize
        \caption{Sample Generation from kPF}
        \label{alg:gen_algo}
        \begin{algorithmic}[1]
        %   \begin{flushleft}
        
          \STATE {\bfseries Input:} 
          %\begin{compactitem}
            Training data $\mathbf{X} = \{\mathbf{x}_1, \dots, \mathbf{x}_n\}$, Optional autoencoder $(E, D)$, input/output kernels ($k, l$), neighborhood size $\gamma$\\
          \STATE {\bfseries Training}
          \STATE \quad $\mathbf{X} = \left(E\left(\mathbf{x}_1\right), \dots, E\left(\mathbf{x}_n\right)\right)$ if $E$ is provided
          \STATE \quad Sample $\{\mathbf{z}_i\}_{i \in [n]} \sim P_Z$ independently \\
          \STATE \quad Construct $L, K \in R^{n \times n}$ s.t.\\
                 \qquad $L_{ij} = l(\mathbf{x}_i, \mathbf{x}_j), K_{ij} = k\left(\mathbf{z}_i, \mathbf{z}_j\right)$\\
          \STATE \quad $K_{\textrm{inv}} = (K + \lambda n I)^{-1}$ or $K^{\dagger}$\\
          \STATE {\bfseries Inference}
          \STATE \quad Generate new prior sample $\mathbf{z}^* \sim P_Z$\\
          \STATE \quad$\mathbf{s} = L\cdot K_{\textrm{inv}}[k(\mathbf{z}_1, \mathbf{z}^*) \dots k(\mathbf{z}_n, \mathbf{z}^*)]^\top$\\
          \STATE \quad $ind = \textrm{argsort}(\mathbf{s})[-\gamma:]$\\
        
          \STATE \quad$\mathbf{x}^* = \psi_{\textrm{wFM}}^{\dagger}\left(\mathbf{X}[ind]; \mathbf{s}[ind] \right)$.
          \STATE {\bfseries Output} $D(\mathbf{x}^*)$ if $D$ is provided else $\mathbf{x}^*$
        \end{algorithmic}
        \end{algorithm}
    \end{minipage}
    \vspace{-1em}
\end{wraptable}
where $\mathbf{X}'$ a neighborhood of $\gamma$ training samples based on pairwise distance or similarity in RKHS, following \citep{kwok2004pre}. The weighted Fr\'{e}chet mean preimage uses the inner product weights $\langle \Psi^*, \psi(\mathbf{x}_i) \rangle$ as measures of similarities to interpolate training samples. On the toy data (as in Fig. \ref{fig:density}), weighted Fr\'{e}chet mean produces fewer samples that deviate from the true distribution and is easier to compute. Based on this observation, we use the weighted Fr\'{e}chet mean as the preimage module for all experiments that requires samples, while acknowledging that other preimage methods can also be substituted in.

% \begin{wrapfigure}{r}{0.5\textwidth}

% \end{wrapfigure}

With all the ingredients in hand, we now present an algorithm for sample generation using the kPF operator in Alg. \ref{alg:gen_algo}. The idea is simple yet powerful: at training time, we construct the empirical kPF operator $\hat{\mathcal{P}}_{\mathcal{E}}$ using the training data $\{\mathbf{x}_i\}_{i \in [s]}$ and samples of the known prior $\{\mathbf{z}_i\}_{\i \in [n]}$. At test time, we will transfer new points sampled from $P_Z$ to feature maps in $\mathcal{H}$, and construct their preimages as the generated output samples.
% \begin{figure}
% \begin{minipage}{0.55\textwidth}
%     \vspace{-1em}
%     \begin{algorithm}[H]
%     \footnotesize
%     \caption{Sample Generation from kPF}
%     \label{alg:gen_algo}
%     \begin{algorithmic}[1]
%     %   \begin{flushleft}
    
%       \STATE {\bfseries Input:} 
%       %\begin{compactitem}
%         Training data $\mathbf{X} = \{\mathbf{x}_1, \dots, \mathbf{x}_n\}$, Optional autoencoder $(E, D)$, input/output kernels ($k, l$), neighborhood size $\gamma$\\
%       \STATE {\bfseries Training}
%       \STATE \quad $\mathbf{X} = \left(E\left(\mathbf{x}_1\right), \dots, E\left(\mathbf{x}_n\right)\right)$ if $E$ is provided
%       \STATE \quad Sample $\{\mathbf{z}_i\}_{i \in [n]} \sim p_Z$ independently \\
%       \STATE \quad Construct $L, K \in R^{n \times n}$ s.t.\\
%              \qquad $L_{ij} = l(\mathbf{x}_i, \mathbf{x}_j), K_{ij} = k\left(\mathbf{z}_i, \mathbf{z}_j\right)$\\
%       \STATE \quad $K_{\textrm{inv}} = (K + \lambda n I)^{-1}$ or $K^{\dagger}$\\
%       \STATE {\bfseries Inference}
%       \STATE \quad Generate new prior sample $\mathbf{z}^* \sim p_Z$\\
%       \STATE \quad$\mathbf{s} = L\cdot K_{\textrm{inv}}[k(\mathbf{z}_1, \mathbf{z}^*) \dots k(\mathbf{z}_n, \mathbf{z}^*)]^\top$\\
%     %   \STATE \quad $ind = \textrm{arg\,top-}\gamma(\mathbf{s})$
%       \STATE \quad $ind = \textrm{argsort}(\mathbf{s})[-\gamma:]$\\
    
%       \STATE \quad$\mathbf{x}^* = \psi_{\textrm{wFM}}^{\dagger}\left(\left\mathbf{X}[ind]\right.; \left. \mathbf{s}[ind] \right.\right)$.
%       \STATE {\bfseries Output} $D(\mathbf{x}^*)$ if $D$ is provided else $\mathbf{x}^*$
%     \end{algorithmic}
%     \end{algorithm}
% \end{minipage}
% \begin{minipage}{0.4\textwidth}
%     \centering
%     \includegraphics[width=0.23\linewidth]{imgs/data_2spirals.png}
%     \includegraphics[width=0.23\linewidth]{imgs/data_8gaussians.png}
%     \includegraphics[width=0.23\linewidth]{imgs/data_checkerboard.png}
%     \includegraphics[width=0.23\linewidth]{imgs/data_rings.png}\\
%     \includegraphics[width=0.23\linewidth]{imgs/samples_2spirals_mds.png}
%     \includegraphics[width=0.23\linewidth]{imgs/samples_8gaussians_mds.png}
%     \includegraphics[width=0.23\linewidth]{imgs/samples_checkerboard_mds.png}
%     \includegraphics[width=0.23\linewidth]{imgs/samples_rings_mds.png}\\
%     \includegraphics[width=0.23\linewidth]{imgs/samples_2spirals_wm.png}
%     \includegraphics[width=0.23\linewidth]{imgs/samples_8gaussians_wm.png}
%     \includegraphics[width=0.23\linewidth]{imgs/samples_checkerboard_wm.png}
%     \includegraphics[width=0.23\linewidth]{imgs/samples_rings_wm.png}\\
%     \caption{Sample generation results. \textbf{Top:} Data samples \textbf{Middle:} MDS-based preimage samples \textbf{Bottom:} Weighted Fr\'{e}chet mean samples.}
%     \label{fig:samples}
% \end{minipage}
% \end{figure}

\subsection{Image generation} Image generation is a common application for generative models \citep{goodfellow2014gan,dinh2017RealNVP}. 
While our proposal is not image specific, constructing sample preimages in a high dimensional space with limited training samples can be challenging, 
since the space of images is usually not dense in a reasonably sized neighborhood. However, empirically images often lie near a low dimensional manifold in the ambient space \citep{Seung2000manifold}, and one may utilize an autoencoder (AE) $(E, D)$ to embed the images onto a latent space that represents coordinates on a learned manifold. If the learned manifold lies close to the true manifold, we can learn densities on the manifold directly \citep{dai2018diagnosing}. 

Therefore, for image generation tasks, the training data is first projected onto the latent space of a pretrained AE. Then, the operator will be constructed using the projected latent representations, and samples will be mapped back to image space with the decoder of AE. Our setup can be viewed analogously to other generative methods based on so called ``\textit{ex-post}'' density estimation of latent variables \citep{Ghosh2020From}. We also restrict the AE latent space to a hypersphere $\mathbf{S}^{n- 1}$ to ensure that \begin{inparaenum}[\bfseries (a)]
\item $k(\cdot, \cdot)$ and $l(\cdot, \cdot)$ are bounded and \item the space is geodesically convex and complete, which is required by the preimage computation. To compute the weighted Fr\'{e}chet mean on a hypersphere, we adopt the recursive algorithm in \citet{chakraborty2015recursive} (see appendix \ref{appdx:wfm_sphere} for details).
\end{inparaenum}

\begin{figure}[t]
    \centering
    \begin{tabular}{c|c}
        \setlength{\tabcolsep}{1pt}
         \begin{tabular}{m{1cm} m{1cm} m{1cm} m{1cm} m{1cm}}
            Data &
            \includegraphics[width=\linewidth]{imgs/data_dist_2spirals.png}&
            \includegraphics[width=\linewidth]{imgs/data_dist_8gaussians.png}&
            \includegraphics[width=\linewidth]{imgs/data_dist_checkerboard.png}&
            \includegraphics[width=\linewidth]{imgs/data_dist_rings.png}\\
            GMM &
            \includegraphics[width=\linewidth]{imgs/distribution_2spirals_gmm.png}&
            \includegraphics[width=\linewidth]{imgs/distribution_8gaussians_gmm.png}&
            \includegraphics[width=\linewidth]{imgs/distribution_checkerboard_gmm.png}&
            \includegraphics[width=\linewidth]{imgs/distribution_rings_gmm.png}\\
            Glow &
            \includegraphics[width=\linewidth]{imgs/distribution_2spirals_glow.png}&
            \includegraphics[width=\linewidth]{imgs/distribution_8gaussians_glow.png}&
            \includegraphics[width=\linewidth]{imgs/distribution_checkerboard_glow.png}&
            \includegraphics[width=\linewidth]{imgs/distribution_rings_glow.png}\\
            kPF &
            \includegraphics[width=\linewidth]{imgs/supplement/distribution_2spirals_kpf_density.png}&
            \includegraphics[width=\linewidth]{imgs/supplement/distribution_8gaussians_kpf_density.png}&
            \includegraphics[width=\linewidth]{imgs/supplement/distribution_checkerboard_kpf_density.png}&
            \includegraphics[width=\linewidth]{imgs/supplement/distribution_rings_kpf_density.png}\\
            % \caption{Density estimation on 2D toy data. \textbf{Top to bottom: (1)} Training data samples, and learned densities of \textbf{(2)} GMM \textbf{(3)} Glow \textbf{(4)} Proposed kPF operator. Details of this experiment can be found in the supplement.}
            % \vspace{-4em}
            % \label{fig:density}
            % \vspace{-7em}
        \end{tabular}
         & 
         \setlength{\tabcolsep}{1pt}
         \begin{tabular}{m{1cm} m{1.2cm} m{1.2cm} m{1.2cm} m{1.2cm}}
            Data&
            \includegraphics[width=\linewidth]{imgs/data_2spirals.png}&
            \includegraphics[width=\linewidth]{imgs/data_8gaussians.png}&
            \includegraphics[width=\linewidth]{imgs/data_checkerboard.png}&
            \includegraphics[width=\linewidth]{imgs/data_rings.png}\\
            MDS &
            \includegraphics[width=\linewidth]{imgs/samples_2spirals_mds.png}&
            \includegraphics[width=\linewidth]{imgs/samples_8gaussians_mds.png}&
            \includegraphics[width=\linewidth]{imgs/samples_checkerboard_mds.png}&
            \includegraphics[width=\linewidth]{imgs/samples_rings_mds.png}\\
            wFM &
            \includegraphics[width=\linewidth]{imgs/samples_2spirals_wm.png}&
            \includegraphics[width=\linewidth]{imgs/samples_8gaussians_wm.png}&
            \includegraphics[width=\linewidth]{imgs/samples_checkerboard_wm.png}&
            \includegraphics[width=\linewidth]{imgs/samples_rings_wm.png}\\
         \end{tabular}
    \end{tabular}
    \vspace{-8pt}
    \caption{\textit{Left figure:} Density estimation on 2D toy data. \textbf{Top to bottom: (1)} Training data samples, and learned densities of \textbf{(2)} GMM \textbf{(3)} Glow \textbf{(4)} Proposed kPF operator. More details in appendix. \textit{Right figure:} Sample generation results. \textbf{Top:} Data samples \textbf{Middle:} MDS-based preimage samples \textbf{Bottom:} Weighted Fr\'{e}chet mean samples.}
    \label{fig:density}
\end{figure}

% it is not practically possible to compute the preimage directly from the infinite-dimensional feature map, we left multiply with $\Phi_G$ and use the geodesic interpolation (gI) module to construct an approximate preimage. Finally, we decodes the interpolated latent representation to the image space. A visual description is shown in Figure \ref{fig:network}.
 
%  \pichskip{2pt}% Horizontal gap between picture and text
% \parpic[r][b]{%
%   \scalebox{0.7}{
%   \begin{minipage}{80mm}
% \begin{bluebox}\label{algo:gI}
%   \begin{compactenum}[(a)]
%     \item Let the inputs be $\left\{\mathbf{l}_i\right\}_{i=1}^\gamma \subset \mathbf{S}^{d-1}$ and $\left\{w_i\right\}_{i=1}^\gamma$.
%     \item Initialize $\mathbf{m} = \mathbf{l}_1$.
%     \item For $j=2, \cdots, \gamma$
%       \begin{compactenum}
%         \item Set $t = \sfrac{w_j}{\sum_{k=1}^j w_k}$, ~~$\theta = \arccos(\mathbf{m}^\top\mathbf{l}_j)$.
%         \item Update 
%         $$
%         \mathbf{m} = \frac{1}{\sin(\theta)}\left(\mathbf{m}\sin\left((1-t)\theta\right) + \mathbf{l}_j \sin\left(t\theta\right)\right).
%         $$
%     \end{compactenum}
%     \item Return $\mathbf{m}$ as the output.
% \end{compactenum}
% \end{bluebox}
% \captionof{figure}{The geodesic interpolation algorithm}
% \label{algo:gI}
% \end{minipage}
% }
% }

%The detailed algorithm is described in .
% {\bf Properties.}
% If $K_Z$ is indeed invertible, the empirical mean embedding of samples generated using the proposed algorithm (assuming the preimage is exact) is equal to the empirical mean embedding of the latent representations, indicating a match in distribution. Further, we use locally-weighted Fr\'echet mean on sphere to construct approximate preimage of sample in RKHS. 
% \revise{Since a closed-form solution for the weighted Fr\'echet mean does not exist on the sphere}, we propose to use a simple and efficient algorithm, namely the geodesic interpolation (gI) \citep{salehian2015efficient}, that uses the geodesic on a hypersphere to iteratively computes the weighted Fr\'{e}chet mean \citep{frechet1948elements} of top $\gamma$ latent representations (see Fig. \ref{algo:gI}). 
% The algorithm has the following properties: \begin{inparaenum}[\bfseries(i)] \item the geodesically completeness of the latent space guarantees that the geodesic interpolation is well-defined \item the geodesic convexity of the latent space guarantees that the output of ``gI'' algorithm lies on the latent space \item the ``gI'' algorithm converges asymptotically to the Fr\'{e}chet mean \citep{salehian2015efficient}. \end{inparaenum}



% {\bf Choice of Kernels.}
% In order to be able to linearize the dynamics between the prior and the target distribution,
% one must first identify a set of nonlinear basis functions such that the corresponding dynamics lies in its span.
% Known results in dynamical systems guarantee the existence of such a linear operator, such as the Koopman operator \citep{koopman1931hamiltonian},
% given an infinite set of basis functions.
%(often referred to as observables in the literature).
%Studies in Koopman operator usually
%Results in the dynamical systems literature
%focus on
%Often, one is interested in
% But rather than identifying certain modes that best characterize the dynamics \citep{williams2015extDMD,brunton2016sindy}
%and therefore enable finite approximations of the infinite-dimensional operator.
%However, in our case,
% we care most about minimizing the error of the transferred density,
%and are more interested in
% and whether the span of functions is rich/expressive enough and can be efficiently computed.
%which becomes our intuition of working in RKHS.
%{\it Radial Basis Function (RBF)} and {\it arc-cosine} are some popular options. 
%compute the inner product of
%infinite-dimensional feature maps and therefore have been frequently used in various machine learning tasks.
% \revise{ Therefore, the choice of kernel is important since it directly determines the family of functions spanned by the operator.}
% We empirically evaluate the effect of using several different kernels by a simple experiment on MNIST. \revise{The MNIST digits are first trained for 100 epochs using an autoencoder with latent space restricted to $S^2$, then samples are generated using procedure described in Figure \ref{gen_alg} using the respective kernel function}.
% The behavior of Radial Basis Function (RBF) and arc-cosine, two standard options in the literature,
% in our setting can be observed in Fig. \ref{mnist_gen}, where
% an operator is learned in RKHS to push a uniform distribution on $\mathbf{S}^2$ to the empirical distribution of 10,000
% random samples from MNIST wrapped on $\mathbf{S}^2$.
% Subplot (b) and (c) show the generated samples when using Radial Basis Function (RBF) kernel and arc-cosine kernel, respectively. Observe that
% the choice of kernel has a clear influence on the posterior,
% but
%using either of these two kernels fails to recover the true distribution accurately.
%Such sensitivity of the choice of kernel necessitates choosing
% and a kernel with superior empirical behavior would be desirable. 
%yet for modeling complicated dynamics they often fall short of spanning the true or approximately-true basis functions. 


%\pichskip{15pt}% Horizontal gap between picture and text
%\parpic[l][t]{%
  %\begin{minipage}{\linewidth}
    %\includegraphics[scale=0.23]{imgs/mnist_data_latent.png}%
    %\includegraphics[scale=0.23]{imgs/arccos_sampled.png}
    %\includegraphics[scale=0.23]{imgs/rbf_sampled.png}
    %\includegraphics[scale=0.23]{imgs/ntk_sampled.png}
    %\captionof{figure}{\small{10K samples from MNIST dataset ({\it left to right}) (a) projected on $\mathbf{S}^2$ shown in coordinate $(\theta, \phi)$, generated samples using (b) arccos (c) RBF (d) NTK.}} \label{mnist_gen} 
  %\end{minipage}
%}
%\vspace*{-2em}
% {\bf NTK.}\label{para:NTK} We use Neural Tangent Kernel (NTK) \citep{jacot2018NTK} as our embedding kernel due to the following properties:
% \begin{inparaenum}[\bfseries (a)] \item NTK, in theory, corresponds to a trained infinitely-wide neural network, and
%   can be non-asymptotically approximated \citep{arora2019onexact}.
%   %Therefore, NTK is spanned by a rich set of nonlinear functions
% \item For well-conditioned inputs (i.e., no duplicates) on the sphere,
%   the positive-definiteness of NTK is proved in \citep{jacot2018NTK}. Therefore, invertibility of $G_{XX}$ is \textit{almost guaranteed}
%   if the sampling distribution is restricted on a hypersphere, which is true by our modeling choice.
% \item Unlike other parametric kernels such as RBF kernels, NTK is less sensitive to hyperparameters, as long
%   as the number of units used is large enough \citep{arora2019onexact}.
% % \item For a spherical latent space and well-conditioned samples (i.e., no duplicates),
% %   NTK is positive-definite \citep{jacot2018NTK}
% \end{inparaenum}
% The embedding of generated samples for MNIST using NTK is also shown in Fig. \ref{mnist_gen}.
% Observe that the sample distribution more closely resembles the original distribution.

% \paragraph{Analysis of the proposed algorithm:} In this section, we analyze the above algorithm in terms of memory complexity. We store matrices $K_Z$ and $K_L$, both of these matrices can be large as they are of size $N\times N$ (we will fix the storage requirement next). {\it Hence, no additional training is required given an autoencoder, $(E, D)$}. 

% \input{nystrom.tex}


%\paragraph{Learning a manifold} The solution for the second fact is essentially learning the local coordinates for $\mathcal{M}$ as well as preserving the boundedness, convexity and completeness property of the latent space. Given a non-linear data manifold $\mathcal{M}\subset \mathbf{R}^m$, we essentially want to learn local-coordinate or chart $\left(U, \mathfrak{f} \right)$ for $U\subset \mathcal{M}$ and $\mathfrak{f}:U \rightarrow B \subset \mathbf{R}^d$. Here, $B$ is a bounded, geodesically complete and complete subset of $\mathbf{R}^d$.  Thus, $\mathcal{M}$ can be identified with an indexed set of charts (an atlas) \citep{}, $\left\{\left(U_{\alpha}, \mathfrak{f}_{\alpha}\right)\right\}_{\alpha}$. Moreover, $\mathfrak{f}$ is diffeomorphic, i.e., bijective and differentiable.  Learning an atlas of $\mathcal{M}$ is equivalent to learning the manifold $\mathcal{M}$, which can be accomplished using the auto-encoder setup by introducing denoising auto-encoder \citep{}. For the sake of completeness, we briefly discuss denoising auto-encoder \citep{}.

%{\it Denoising auto-encoder:} Given a sample $X \in \mathcal{M}$, we first perturb $X$ using the stochastic function $q_D$ to generate $\widetilde{X}\sim q_D\left(\widetilde{X}|X\right)$. Thus, the purpose of learning an encoder $E$ on the set  $\left\{\left\{\widetilde{X}\right\}\bigcup \left\{q_D\left(\widetilde{X}|X\right)\right\}\right\}$ is learning the local coordinate map. In addition, we learn a decoder, $D$ to map the learned coordinate (latent space) to $X$. Together $E$ and $D$ essentially ensures the diffeomorphic property of the chart map. Observe that, the learned local chart ensures that the latent space is locally flat and additionally, we use the norm constraint on the latent space. Thus, the latent space is  locally flat and bounded.

% In the next section, we present the empirical analysis by choosing a vanilla auto-encoder with a bounded and complete latent space, e.g., a hypersphere.  




