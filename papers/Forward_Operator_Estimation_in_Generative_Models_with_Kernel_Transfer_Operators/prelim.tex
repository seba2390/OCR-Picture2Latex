\section{Preliminaries}


We briefly introduce reproducing kernel Hilbert space (RKHS) and kernel embedding of probability distributions, concepts 
we will use frequently. 

%which are the building blocks of this paper.
%\edit{We can define feature maps earlier here}
\begin{definition}[RKHS \citep{aronszajn1950theory}]
\label{def:rkhs}
For a set $\mathcal{X}$, let $\mathcal{H}$ be a set of functions $g:\mathcal{X}\rightarrow \mathbf{R}$. Then, $\mathcal{H}$ is a reproducing kernel Hilbert space (RKHS) with a product $\langle \cdot, \cdot \rangle_{\mathcal{H}}$ if there exists a function $k:\mathcal{X}\times \mathcal{X}\rightarrow \mathbf{R}$ (called a reproducing kernel) such that \begin{inparaenum}[(i)] \item $\forall x \in \mathcal{X}, g \in \mathcal{H}, g(x) = \langle g, k(x, \cdot)\rangle_{\mathcal{H}}$; \item $\mathcal{H} = cl({\text{span}}(\left\{k(x, \cdot), x \in \mathcal{X}\right\}))$, where $cl(\cdot)$ is the set closure. \end{inparaenum} 
\end{definition}

\begin{wraptable}{r}{0.45\textwidth}
\centering % to have the caption near the table
{\small
\begin{tabular}{p{.45cm} p{.45cm} p{3.65cm} }
\specialrule{1pt}{1pt}{0pt}
\rowcolor{azure!20}
\multicolumn{2}{c}{Notations} &  Meaning \\ \specialrule{1pt}{0pt}{1pt}
$\var{z}$ & $\var{x}$  & Random variable\\
$\mathbf{Z}$ & $\mathbf{X}$ & Data samples\\
$\spc{Z}$ & $\spc{X}$  & Domain \\
$P_{\var{z}}$ & $P_{\var{x}}$ & Distribution\\
$p_{\var{z}}$ & $p_{\var{x}}$  & Density function \\
$k$ & $l$  & Kernel function \\
$\mathcal{H}$ & $\mathcal{G}$  & RKHS\\
$\phi(\cdot)$ & $\psi(\cdot)$  & Feature mapping\\
$\mathcal{E}_k$ & $\mathcal{E}_l$  & Mean embedding operator\\
$\mu_{\var{z}}$ & $\mu_{\var{x}}$  & Kernel mean embedding\\
\bottomrule
\end{tabular}
\vspace{-1em}
\caption{\label{tab:notations} Notations  used in this paper.}
}
\end{wraptable}

The function $\phi(x) = k(x, \cdot) : \mathcal{X} \to \mathcal{H}$ is referred to as the \textit{feature mapping} of the induced RKHS $\mathcal{H}$. A useful identity derived from feature mappings is the {\it kernel mean embedding}: it defines a mapping from a probablity measure in $\mathcal{X}$ to an element in the RKHS.
% The use of kernel mean embeddings has recently regained much attention in the machine learning community for tasks associated with distribution matching.

\begin{definition}[Kernel Mean Embedding \citep{smola2007ahilbertspace}]
\label{def:me}
Given a probability measure $p$ on $\mathcal{X}$ with an associated RKHS $\mathcal{H}$ equipped with
a reproducing kernel $k$ such that $\sup_{x \in \mathcal{X}} k(x, x) < \infty$, the kernel mean embedding of $p$ in RKHS $\mathcal{H}$, denoted by $\mu_{p} \in \mathcal{H}$, is defined as $\mu_{p} = E_p[\phi(x)]= \int k(x, \cdot) p(x)dx$, and the mean embedding operator $\mathcal{E}: L^1(\mathcal{X}) \to \mathcal{H}$ is defined as $\mu_p = \mathcal{E}p$. 
\end{definition}

\begin{tcolorbox}[bottom=0mm,top=0mm]
\begin{remark}
For characteristic kernels, the operator $\mathcal{E}$ is injective. Thus, two distributions $(p, q)$ in $\mathcal{X}$ are identical \textit{iff} $\mathcal{E}p = \mathcal{E}q$.
\end{remark}
\end{tcolorbox}
 This property allows using of  
{\it Maximum Mean Discrepancy (MMD)}
for distribution matching \citep{gretton2012kernel, li2017mmdgan} and is 
common, see \citep{Muandet2017kernel,zhou2018pnas}. For a finite number of
samples $\left\{\mathbf{x}_i\right\}_{i=1}^n$ drawn from the probability measure $p$,
an unbiased empirical estimate of $\mu_{\mathcal{H}}$ is $\hat{\mu}_{\mathcal{H}} = \tfrac{1}{n} \sum_{i=1}^{n}k(\mathbf{x}_i, \cdot)$ such that $\lim_{n \to \infty} \tfrac{1}{n} \sum_{i=1}^{n}k(\mathbf{x}_i, \cdot) = \mu_{\mathcal{H}}$.

Next, we review the covariance/cross covariance operators, two widely-used identities in kernel methods \citep{fukumizu2013kernel,song2013kernel} and building blocks of our approach.
\begin{definition}[Covariance/Cross-covariance Operator]
    Let $X, Z$ be random variables defined on $\mathcal{X} \times \mathcal{Z}$ with joint distribution $P_{X, Z}$ and marginal distributions $P_{X}$, $P_{Z}$. Let $(l, \phi, \mathcal{H})$ and $(k, \psi, \mathcal{G})$ be two sets of (a) bounded kernel, (b) their corresponding feature map, and (c) their induced RKHS, respectively. The (uncentered) covariance operator $\mathcal{C}_{ZZ}: \mathcal{H} \to \mathcal{H}$ and cross-covariance operator $\mathcal{C}_{XZ}: \mathcal{H} \to \mathcal{G}$ are defined as 
    \begin{equation}
        \mathcal{C}_{ZZ} \triangleq \mathbb{E}_{z \sim P_Z}[\phi(z) \otimes \phi(z)]\qquad \mathcal{C}_{XZ} \triangleq \mathbb{E}_{(x, z) \sim P_{X, Z}}[\psi(x) \otimes \phi(z)]
    \end{equation}
    where $\otimes$ is the outer product operator.
\end{definition}

% {\bf Auto-encoders. } Auto-encoders (AEs) \citeppending{} are a popular set of tools in unsupervised representation learning that aims to capture intrinsic patterns (i.e. discovering latent variables) by embedding the data onto a lower dimensional space. The core assumption behind AE is that the data (e.g. images, texts, audio sequences, etc.) often lie close to an (unknown) lower dimensional manifold $\mathcal{M} \subset \mathcal{X}$ such that $\text{dim}(\mathcal{M}) \ll |\mathcal{X}|$. Assuming the maximum distance between the data and the manifold is small enough, the data distribution can be approximated by the distribution of projections onto the manifold \citeppending{}. Practical uses of this property in generative modeling can be seen in VAE, where the prior is modeled as a distribution on the latent space of an AE. Another useful property of AEs is that they allow a continuous traversal of latent variables if trained with proper regularization scheme... \citeppending{}

% {\bf (Regularized) Auto-encoders.} Images often lie close to an (unknown) lower dimensional manifold
% $\mathcal{M} \subset \mathbf{R}^m$ such that $\text{dim}(\mathcal{M}) \ll m$, and operating
% with densities in a lower dimensional setting is often
% much easier. VAEs leverage this explicitly via auto-encoders.
% If the parameterized empirical density is $p_\tau(x)$, in VAEs, 
% we write it as $\int p_\tau(x|z)p(z) dz$ where $z$ is 
% the low dimensional representation with 
% a suitable prior. We then use a decoder distribution 
% $q_{\tau'}(z|x)$ and an encoder distribution 
% $p_\tau(x|z)$ and train the parameters $\tau$ and $\tau'$. 
% In practice, $p_\tau(\cdot)$ and 
% $q_{\tau'}(\cdot)$ are assumed to be 
% Gaussian,
% %-- and 
% %despite various useful properties -- 
% %is consider its key limitation.  
% %Further,
% but it is known that
% jointly fitting the manifold as well as regressing to a particular prior distribution
% can be challenging in VAEs \citep{Kingma2016improved,dai2018diagnosing}. 
% Now, consider the following 
% approach, 
% %that follows the 
% %auto-encoder rationale from VAE described 
% %above but does
% where we do not impose a distributional assumption on $z$.
% If a well-regularized auto-encoder is able to capture
% information about the data generating density \citep{alain2014regularized},
% we can think of $z$ as the input measurements (likely, meaningful representations 
% from the input data) which is subsequently mapped to an
% infinite dimensional space (of observables of these measurements). 
% %% a potential approach may involve using an auto-encoder to learn the manifold. 
% %% We then utilize the idea in flow based 
% %% models to transform the density in the latent 
% %% space to a known prior distribution. 
% %% Notice that even without strict 
% %% assumptions on 
% %% $q_{\tau'}(\cdot)$, 
% Now, if we could push-forward the embedded RKHS distribution to the RKHS mapping of a simpler
% distribution, similar to flow-based generation, one could easily sample 
% from the simple (e.g., standard normal) distribution 
% and transform it via the learned mapping
% to samples on the latent space. 
% In summary, instead of explicitly searching for the eigenfunctions,
% we propose to {\it Step 1:} embed the density from an auto-encoder into a RKHS, {\it Step 2:}
% learn a kernel transfer operator in RKHS in one step. In the remainder
% of this paper, we will provide the details to operationalize this idea and show that this simple
% approach, in fact, performs surprisingly well with highly favorable computational properties.

% Our developments are focused on simplifying flow-based generative models,
%which will be the underlying framework of our proposed method.

% \begin{definition}[Flow-based generative model \citep{dinh2014nice}]
% \label{def:flow}
% %A flow-based generative model is constructed using a sequence of invertible transformations.
% A flow-based generative model explicitly learns the data distribution by trying to bijectively
% map it to a tractable density via invertible transformations.
% Formally, given a random variable $\mathbf{z}$ following a tractable density, i.e., $\mathbf{z} \sim p_{\theta}(\mathbf{z})$, a
% flow-based model learns an invertible mapping $f_{\phi}$ such that the data sample $\mathbf{x} = f_{\phi}(\mathbf{z})$ and the corresponding
% data distribution, $p_{\widetilde{\theta}}(\mathbf{x}) = f_* p_{\theta}(\mathbf{z})$, where, $f_* = |\frac{dz}{dx}|$ is the push-forward operator.  
% \end{definition}


% As argued before, although flow-based generative model is an obvious choice to model the data density (target density) using the learned invertible mapping from the tractable density, in practice, these models often suffer from large memory consumption or slow inference. Hence, the need for efficient (invertible) ``transformation'' of tractable density to the target density is eminent and serves as the motivation for this work.
