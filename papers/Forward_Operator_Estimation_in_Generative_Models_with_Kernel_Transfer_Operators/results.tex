\section{Experimental Results}
\begin{figure*}[b]
    \vspace{0.3em}
    \centering
    % \textcolor{cyan!50!white}{\fboxrule=1pt\fbox{\includegraphics[width=.225\textwidth, valign=t]{imgs/celeba_gmm_collage.png}}}
    % \textcolor{cyan!50!white}{\fboxrule=1pt\fbox{\includegraphics[width=.225\textwidth, valign=t]{imgs/celeba_glow_collage.png}}}
    % \textcolor{cyan!50!white}{\fboxrule=1pt\fbox{\includegraphics[width=.225\textwidth, valign=t]{imgs/celeba_2svae_collage.png}}}
    % \textcolor{cyan!50!white}{\fboxrule=1pt\fbox{\includegraphics[width=.225\textwidth, valign=t]{imgs/celeba_kpf_collage.png}}}
    \setlength{\tabcolsep}{1.5pt}
    \renewcommand{\arraystretch}{1.2}
    \begin{tabular}{cccc}
    \includegraphics[width=.24\textwidth, valign=t]{imgs/mnist_2svae_collage.png} &
    \includegraphics[width=.24\textwidth, valign=t]{imgs/mnist_glow_collage.png} &
    \includegraphics[width=.24\textwidth, valign=t]{imgs/mnist_gmm_collage.png} &
    \includegraphics[width=.24\textwidth, valign=t]{imgs/mnist_kpf_collage.png}\\
    \includegraphics[width=.24\textwidth, valign=t]{imgs/cifar10_2svae_collage.png} &
    \includegraphics[width=.24\textwidth, valign=t]{imgs/cifar10_glow_collage.png} &
    \includegraphics[width=.24\textwidth, valign=t]{imgs/cifar10_gmm_collage.png} &
    \includegraphics[width=.24\textwidth, valign=t]{imgs/cifar10_kpf_collage.png}\\
    \includegraphics[width=.24\textwidth, valign=t]{imgs/celeba_2svae_collage.png} &
    \includegraphics[width=.24\textwidth, valign=t]{imgs/celeba_glow_collage.png} &
    \includegraphics[width=.24\textwidth, valign=t]{imgs/celeba_gmm_collage.png} &
    \includegraphics[width=.24\textwidth, valign=t]{imgs/celeba_kpf_collage.png}\\
     $\textrm{Two-stage VAE}$ & $\textrm{SRAE}_\textrm{Glow}$ & $\textrm{SRAE}_\textrm{GMM}$ & $\textrm{SRAE}_\textrm{NTK-kPF}$
    \end{tabular}
    \vspace{-8pt}
    \caption{Comparison of different sampling techniques using AE trained on CelebA 64x64. Left to right: samples of (1) Two-Stage VAE \citep{dai2018diagnosing} (2) $\textrm{SRAE}_{\rm Glow}$ \citep{Kingma2018Glow} (3) $\textrm{SRAE}_{\rm GMM}$ (4) $\textrm{SRAE}_\textrm{ NTK-kPF}$ using 10k latent points.}
    \label{fig:celeba}
    \vspace{-1em}
\end{figure*}

\textbf{Goals.} In our experiments, we seek to answer three questions: 
\begin{inparaenum}[\bfseries (a)] \label{first}
\item With sufficient data, can our method generate new data with comparable performance with other state-of-the-art generative models?
\item If only limited data samples were given, can our method still estimate the density with reasonable accuracy?
\item What are the runtime benefits, if any?
\end{inparaenum}

\textbf{Datasets/setup.} To answer the first question, we evaluate our method on standard vision datasets, including MNIST, CIFAR10, and CelebA, where the number of data samples is much larger than the latent dimension. We compare our results with other VAE variants (Two-stage VAE \citep{dai2018diagnosing}, WAE \citep{arjovsky2017wasserstein}, CV-VAE \citep{Ghosh2020From}) and flow-based generative models (Glow \citep{Kingma2018Glow}, CAGlow \citep{Liu2019CAflow})
The second question is due to 
the broad use of kernel methods in small sample size settings. For this more challenging case, we randomly choose 100 training samples ($<1$\% of the full dataset) from CelebA and evaluate the quality of generation compared to other density approximation schemes. We also use a dataset of T1 Magnetic Resonance (MR) images from the Alzheimer's Disease Neuromaging Initiative (ADNI) study. 

% The purpose of the first set of experiments is to show that the proposed method yields competitive measures on the task of image generation compared to other non-adversarial generative methods while enjoying the benefit of {\it one step} density estimation.
% Our second setting is motivated by \cite{Arora2020Harnessing}, where kernel methods consistently outperform neural networks in a small data setting. Our goal is to demonstrate that unlike other density approximation methods based on deep neural architectures, our proposed method produces reasonable sample generation both quantitative and qualitatively when only few data were given.

% (on MNIST using kPF with random projection with RBF kernel, we can achieve $19.1$ FID score)



\textbf{Distribution transfer with many data samples.}
We evaluate the quality by calculating the Fr\'{e}chet Inception Distance (FID) \citep{Martin2017ttur} with 10K generated images from each model. Here, we use a pretained regularized autoencoder \citep{Ghosh2020From} with a latent space restricted to the hypersphere (denoted by SRAE) to obtain \textit{smooth} latent representations. We compare our kPF to competitive end-to-end deep generative baselines (i.e. flow and VAE variants) as well as other density estimation models over the same SRAE latent space. For the latent space models, we experimented with Glow \citep{Kingma2018Glow}, VAE, Gaussian mixture model (GMM), and two proposed kPF operators with Gaussian kernel (RBF-kPF) and NTK (NTK-kPF) as the input kernel. The use of NTK is motivated by promising results at the interface of kernel methods and neural networks \citep{jacot2018NTK, Arora2020Harnessing}. Implementation details are included in the appendix.

% \begin{wraptable}{r}{0.55\textwidth}
%       {
%   \footnotesize
%   \centering
%         \begin{tabular}{cccc} 
%         \specialrule{1pt}{1pt}{0pt} \rowcolor{azure!20}
%         &  MNIST  &  CIFAR  &  CelebA \\
%         \specialrule{1pt}{0pt}{1pt}
%             $\textrm{Glow}^\ddag$    & 25.8         & -          & 103.7\\
%             $\textrm{CAGlow}^\ddag$    & 26.3         & -          & 104.9\\
%             Vanilla VAE         & \textbf{13.7} & 111.0      & 52.1\\
%             $\textrm{CV-VAE}^\dagger$ & 33.8          & 94.8       & 48.9\\
%             $\textrm{WAE}^\dagger$    & 20.4          & 117.4      & 53.7\\
%             Two-stage VAE       & 18.3          & 110.3      & 44.7\\
%         \hline 
%         % $\textrm{SAE}_{\textit{rand}}$ & 55.6 & 187.8 & 86.5\\
%             $\textrm{SRAE}_{\textrm{Glow}}$ & 23.7 &  110.7 & 59.8 \\
%             $\textrm{SRAE}_{\textrm{GMM}} $ & 16.7 & 79.2 & 42.0\\
%             $\textrm{SRAE}_{\textrm{RBF-kPF}} (\textit{ours})$ &  21.7 & 77.9 & 41.9\\
%             $\textrm{SRAE}_{\textrm{NTK-kPF}} (\textit{ours})$ & 21.5 & \textbf{77.5} & \textbf{41.0} \\
%         % $\textrm{SRAE}_{\textit{NTK-kPF Nystr\"{o}m}} (\textit{ours})$ &  &  & \\
%             %$\textrm{SAE}_{\textit{NNGP-kPF 10k}} (\textit{ours})$ & $\textbf{19.1}$ & - & - \\
%         \bottomrule
%         \end{tabular}
%     }
    
%   \caption{ Comparative FID values. SRAE indicates an autoencoder with hyperspherical latent space 
%   and spectral regularization following \cite{Ghosh2020From}. Results reported from $\ddag$: \cite{Liu2019CAflow}. $\dagger$: \cite{Ghosh2020From}.}
%     \label{tab:fid_table}
%     \vspace{-1em}
% \end{wraptable}

\begin{figure*}[t]
    \centering
    \includegraphics[scale=0.24]{imgs/nvae/sampled_153.png}
    \includegraphics[scale=0.24]{imgs/nvae/sampled_175.png}
    \includegraphics[scale=0.24]{imgs/nvae/sampled_176.png}
    \includegraphics[scale=0.24]{imgs/nvae/sampled_177.png}
    \includegraphics[scale=0.24]{imgs/nvae/sampled_225.png}
    \includegraphics[scale=0.24]{imgs/nvae/sampled_260.png}
    \vspace{-8pt}
    \caption{Representative samples from learned kPF on pre-trained NVAE latent space}
    \vspace{-1em}
    \label{fig:nvae}
\end{figure*}



Comparative results are shown in Table \ref{tab:fid_table}. We see that for images with structured feature spaces, e.g., MNIST and CelebA, our method matches other non-adversarial generative models, which provides evidence in support of the premise that the forward operator 
can be simplified. 
%
\begin{figure}[b]
    \centering
    \vspace{-1em}
    \begin{minipage}{0.55\textwidth}
               {
       \footnotesize
       \centering
            \begin{tabular}{cccc} 
            \specialrule{1pt}{1pt}{0pt} \rowcolor{azure!20}
            &  MNIST  &  CIFAR  &  CelebA \\
            \specialrule{1pt}{0pt}{1pt}
                $\textrm{Glow}^\ddag$    & 25.8         & -          & 103.7\\
                $\textrm{CAGlow}^\ddag$    & 26.3         & -          & 104.9\\
                Vanilla VAE         & 36.5 & 111.0      & 52.1\\
                $\textrm{CV-VAE}^\dagger$ & 33.8          & 94.8       & 48.9\\
                $\textrm{WAE}^\dagger$    & 20.4          & 117.4      & 53.7\\
                Two-stage VAE       & 16.5          & 110.3      & 44.7\\
            \hline 
            % $\textrm{SAE}_{\textit{rand}}$ & 55.6 & 187.8 & 86.5\\
                $\textrm{SRAE}_{\textrm{Glow}}$ & \textbf{15.5} &  85.9 & \textbf{35.0} \\
                $\textrm{SRAE}_{\textrm{VAE}} $ & 17.2 & 198.0 & 48.9\\
                $\textrm{SRAE}_{\textrm{GMM}} $ & 16.7 & 79.2 & 42.0\\
                $\textrm{SRAE}_{\textrm{RBF-kPF}} (\textit{ours})$ &  19.7 & 77.9 & 41.9\\
                $\textrm{SRAE}_{\textrm{NTK-kPF}} (\textit{ours})$ & 19.5 & \textbf{77.5} & 41.0 \\
            % $\textrm{SRAE}_{\textit{NTK-kPF Nystr\"{o}m}} (\textit{ours})$ &  &  & \\
                %$\textrm{SAE}_{\textit{NNGP-kPF 10k}} (\textit{ours})$ & $\textbf{19.1}$ & - & - \\
            \bottomrule
            \end{tabular}
        }
        
       \captionof{table}{ Comparative FID values. SRAE indicates an autoencoder with hyperspherical latent space and spectral regularization following \cite{Ghosh2020From}. Results reported from $\ddag$: \cite{Liu2019CAflow}. $\dagger$: \cite{Ghosh2020From}.}
        \label{tab:fid_table}
    \end{minipage}
    \hfill
    \begin{minipage}{0.42\textwidth}
        \includegraphics[width=\linewidth, trim=10 0 10 10, clip]{imgs/time_v_fid.pdf}
          \vspace{-14pt}
        \captionof{figure}{FID  \textit{versus} training time for latent space models. All models are learned on the latent representations encoded by the same pre-trained SRAE.} %Our $10k$ and $\textrm{nystr\"om}$ model shows $>40$x and $\sim4$x speedup, respectively, compared to other models trained on latent space. }
        \label{fig:runtime}
    \end{minipage}
\end{figure}
%
% \begin{figure*}[th]
%     \centering
%     \vspace{1em}
%     \setlength{\fboxsep}{0pt}
%     \setlength{\fboxrule}{1.5pt}
%     \textcolor{red}{\fbox{\includegraphics[width=0.045\textwidth, height=0.22\textwidth]{imgs/mnist_nearest_generation.png}}}
%     \includegraphics[width=.20\textwidth, height=0.22\textwidth]{imgs/mnist_nearest_top5.png}
%     \hspace{0.05in}
%     \textcolor{red}{\fbox{\includegraphics[width=0.045\textwidth, height=0.22\textwidth]{imgs/cifar_nearest_generation.png}}}
%     \includegraphics[width=.20\textwidth, height=0.22\textwidth]{imgs/cifar_nearest_top5.png}
%     \hspace{0.05in}
%     \textcolor{red}{\fbox{\includegraphics[width=0.045\textwidth, height=0.22\textwidth]{imgs/celeba_nearest_generation.png}}}
%     \includegraphics[width=.20\textwidth, height=0.22\textwidth]{imgs/celeba_nearest_top5.png}
%     \vspace*{-0.5em}
%     \caption{Generations (in red box) and training samples corresponding to the top-5 latent representations used in geodesic interpolation. It can be observed that the samples with top kernel values indeed share high visual similarity.}
%     \label{fig: generation_topk}
% \end{figure*}
% \vspace{1em}
%
Further, we present qualitative results on all datasets (in Fig. \ref{fig:celeba}), where we compare our kPF operator based model with other density estimation techniques on the latent space. Observe that our model
generates comparable visual results as $\textrm{SRAE}_{\rm Glow}$.

% \begin{wrapfigure}{r}{0.45\textwidth}
%     \centering
%     \vspace{-1em}
%       \includegraphics[width=\linewidth, trim=10 10 10 10, clip]{imgs/training_time_comparison.png}
%       \vspace{-14pt}
%     \caption{ Comparison of additional training time for density estimation on latent space. All models were fitted on the latent representations of CelebA images encoded by an SRAE.} %Our $10k$ and $\textrm{nystr\"om}$ model shows $>40$x and $\sim4$x speedup, respectively, compared to other models trained on latent space. }
%     \label{fig:runtime}
%     \vspace{-2em}
% \end{wrapfigure}

Since kPF learns the distribution on a pre-trained AE latent space for image generation, using a more powerful AE can 
offer improvements in generation quality. In Fig. \ref{fig:nvae}, we present representative images by learning 
our kPF on NVAE \cite{vahdat2020NVAE} latent space, pre-trained on the FFHQ dataset. NVAE builds a hierarchical prior and achieves state-of-the-art generation quality among VAEs. We see that kPF can indeed generate high-quality and diverse samples with the help of NVAE encoder/decoder. In fact, any AE/VAE may be substituted in, assuming that the latent space is \textit{smooth}.\\*[3pt]
%
\noindent {\bf Summary:} When a sufficient number of samples are available, our algorithm performs as well as the alternatives, which is attractive given the efficient training.  In Fig. \ref{fig:runtime}, we present comparative result of FIDs with respect to the training time. Since kPF can be computed in closed-form, it achieves significant training efficiency gain compared to other deep generative methods while delivering competitive generative quality.

\textbf{Distribution transfer with limited data samples.} Next, we present our evaluations when only a limited number of samples are available. Here, each of the density estimators was trained on latent representations of the same set of 100 randomly sampled CelebA images, and 10K images were generated to evaluate FID (see Table \ref{tab:fid_table_limited}).
Our method outperforms Glow and VAE, while offering competitive performance with GMM. Surprisingly, GMM remains a strong baseline for both tasks, which agrees with results in \citet{Ghosh2020From}. However, note that GMM is restricted by its parametric form and is less flexible than our method (as shown in Fig \ref{fig:density}).

\setlength{\intextsep}{10pt}%
\setlength{\columnsep}{7pt}
\begin{wraptable}{r}{0.51\textwidth}
   \centering
%   \vspace{-1em}
   {\small
    \begin{tabular}{ccccc} 
      \specialrule{1pt}{1pt}{0pt} \rowcolor{azure!20}
        VAE &  Glow  &  GMM  &  RBF-kPF & NTK-kPF \\
     \specialrule{2pt}{0pt}{1pt}
        59.3 & 77.0 & 39.6 & 40.6 & 40.9\\
        \bottomrule
        \end{tabular}
    }
    \vspace{-1em}
   \caption{FID values for few samples setting density approximation on CelebA. }
   \vspace{-1em}
    \label{tab:fid_table_limited}
\end{wraptable}

Learning from few samples is  common in biomedical  applications where acquisition is costly. Motivated 
by interest in making synthetic but 
statistic preserving data (rather than the 
real patient records) 
publicly available to researchers 
(see NIH N3C Data Overview), 
we present results on generating high-resolution
$(160\times 196\times 160)$ brain images:  $183$ samples from group AD
(diagnosed as Alzheimer's disease) and $291$ samples from group CN (control normals).
For $n = 474 \ll d = 5017600$, using our kernel operator, we can generate high-quality samples 
that are in-distribution. We present comparative results with VAEs.
The generated samples in Fig. \ref{fig: mr_gen} clearly show that our method generates sharper images.
%which can indicate the generated distribution is more aligned to the data distribution.
%
To check if the results are also scientifically meaningful, 
we test consistency between group difference testing (i.e., cases versus controls differential tests on each voxel) on the real images (groups were AD and CN) and   
the same test was performed on the generated samples (AD and CN groups), 
%indeed represent the original data distribution
using a FWER corrected two-sample $t$-test \cite{ashburner2000voxel}. 
%between samples generated using images from either group CN or group AD.
The results (see Fig \ref{fig: mr_gen}) show that while there is a deterioration in regions
identified to be affected by disease (different across groups), 
many statistically-significant regions from  tests on the real images are preserved in  voxel-wise tests
on the generated images.\\*[3pt]
\noindent {\bf Summary:} We achieve improvements in the small sample size setting compared to other generative methods. This is useful in many data-poor settings. For larger datasets, our method still compares competitively with alternatives, but with a smaller resource footprint.

\begin{figure}[b!]
    \centering
    \begin{minipage}{.63\linewidth}
      \centering
      \includegraphics[width=0.98\textwidth]{imgs/mri_data.png}
      \includegraphics[width=0.98\textwidth]{imgs/mri_sampled_vae.png}
      \includegraphics[width=0.98\textwidth]{imgs/mri_sampled_pf.png}
    \end{minipage}%
    \begin{minipage}{0.25\linewidth}
        \centering
        \includegraphics[width=0.95\textwidth]{imgs/vbm_pval_data.png}
        \includegraphics[width=0.95\textwidth]{imgs/vbm_pval_gen.png}
        \includegraphics[width=\textwidth]{imgs/vbm_colorbar.png}
    \end{minipage}
    \vspace{-8pt}
    \caption{\footnotesize {\bf Left.} {\it Top: data}, generated samples of {\it Middle: VAE , Bottom: kPF with SRAE}.
    {\bf Right.} Statistically significant regions {\it Top: voxel-wise tests on real data, Bottom: voxel-wise tests on generated samples} are shown in negative log $p$-value thresholded at $\alpha=0.01$.}
    \label{fig: mr_gen}
    \vspace{-0.8em}
\end{figure}

%% \begin{figure*}[hbt!]
%%         \centering
%%         \includegraphics[width=0.6\textwidth, valign=t]{imgs/mri_data.png}
%%         \includegraphics[width=0.3\textwidth, valign=t]{imgs/vbm_pval_data.png} \\
%%         \includegraphics[width=0.6 \textwidth, valign=t]{imgs/mri_sampled_vae.png}
%%         \includegraphics[width=0.3\textwidth, valign=t]{imgs/vbm_pval_gen.png} \\
%%         \includegraphics[width=0.6\textwidth, valign=c]{imgs/mri_sampled_pf.png}
%%         \includegraphics[width=0.3\textwidth, valign=c]{imgs/vbm_colorbar.png}
%% \end{figure*}


% \begin{comment}
% \begin{figure*}[hbt!]
%     \begin{subfigure}{.6\textwidth}
%         \centering
%         \includegraphics[width=\textwidth, valign=t]{imgs/mri_data.png}\\
%         \includegraphics[width=\textwidth, valign=t]{imgs/mri_sampled_vae.png}\\
%         \includegraphics[width=\textwidth, valign=t]{imgs/mri_sampled_pf.png}
%         \caption{}
%         \label{fig: mr_gen}
%     \end{subfigure}
%     ~~~~
%     \begin{subfigure}{.3\textwidth}
%         %\vspace{0.2in}
%         \centering
%             \includegraphics[width=\textwidth, valign=t]{imgs/vbm_pval_data.png}
%             \includegraphics[width=\textwidth, valign=t]{imgs/vbm_pval_gen.png}
%             \vspace*{-0.03in}
%             \includegraphics[width=\textwidth, valign=t]{imgs/vbm_colorbar.png}
%             \caption{Statistically significant regions {\it Top: data, Bottom: samples} are shown in negative log $p$-value thresholded at $p<0.01$.}
%             \label{fig: mr_ttest}
%     \end{subfigure}
%     \caption{Generated samples and statistical analysis of ADNI MR data}
%     \end{figure*}
% \end{comment}




% Figures are only placeholders
%\begin{figure*}[hbt!]
 %   \centering
 %   \includegraphics[width=\textwidth, valign=t]{imgs/mnist_sphere_proj_ntk.png}
 %   \caption{MNIST projected on a three-dimensional unit-sphere shown in cartesian coordinate $(\theta, \phi)$. Left to right: (1) $10k$ randomly selected training points (2) kPF-flow estimated directly using all $10k$ training points (3) $2k$ Nystr\"{o}m points (4) kPF-flow apporximated using $2k$ Nystr\"{o}m points}
%\end{figure*}
