\section{Limitations}
\label{sec:limitations}
%When a dataset allows
%a low-dimensional structured latent %space representation,
%a kernel Perron-Frobenius operator %is effective for simplifying
%forward operators in deep %generative models. 
%However, some direction we did not explore in our work is how to
Our proposed simplifications can be variously
useful, but deriving the density of the posterior given a mean embedding or providing an exact preimage for the generated sample in RKHS is unresolved at this time. 
While density estimation from kPF has been partially addressed in \citet{schuster2020kernelconditional}, finding the pre-image is often ill-posed. The weighted Fr\'{e}chet mean preimage only provides an approximate solution and evaluated empirically,and due to the interpolation-based sampling strategy, samples cannot be obtained beyond the convex hull of training examples. Making $Z$ and $X$ independent RVs also limits its use for certain downstream task such as representation learning or semantic clustering. Finally, like other kernel-based methods, the sup-quadratic memory/compute cost can be a bottleneck on large datasets and kernel approximation (e.g. \citep{rahimi2008random}) may have to be applied; we discuss this in appendix \ref{sec:nystrom}.

% \paragraph{Negative Societal Impact.} While the paper is focused on efficiency, we acknowledge that improvements in deep generative models can be used nefariously, including misinformation propagation. 

\section{Conclusions}
We show that using recent developments in regularized autoencoders, a linear kernel transfer operator can potentially be an efficient substitute for the forward operator in some generative models, if 
some compromise in capabilities/performance is acceptable. Our proposal, despite its simplicity, shows comparable empirical results to other generative models, while offering efficiency benefits. Results on brain images also show promise for applications to high-resolution 3D imaging data generation, which is being pursued actively in the community.

% Autoencoder
% Let $\mathcal{X} \subseteq \mathbf{R}^m$ be the set of all possible data points (in our case, images). We start by making the well-acknowledged manifold assumption: there exists a manifold $\mathcal{M} \subset \mathbf{R}^m$ such that $\text{dim}\left(\mathcal{M}\right) \ll m$ and $\forall \mathbf{x} \in \mathcal{X}, \exists \mathbf{m} \in \mathcal{M}$: $\Vert \mathbf{x} - \mathbf{m} \Vert_2 \leq \epsilon$ for some $\epsilon > 0$. Under this assumption, an oracle auto-encoder $A = (E^*, D^*)$ where $E: \mathbf{R}^m \rightarrow \mathbf{R}^d$, $D: \mathbf{R}^d \rightarrow \mathbf{R}^m$ can be formulated as

% $$
% A \in \{(E, D)| \forall \mathbf{x} \in \mathcal{X}, \Vert \mathbf{x} - (D \circ E)(\mathbf{x}) \Vert_2 \leq \epsilon\}
% $$

% In practice, it is usually impractical to observe and optimize for all possible data points. Therefore, a near-optimal auto-encoder can be estimated as

% $$
% \hat{A} = \argmin_{E, D} \sum_{\mathbf{x} \in \mathbb{X}} \Vert \mathbf{x} - (D \circ E)(\mathbf{x}) \Vert_2
% $$

% \noindent through stochastic gradient descent for a sufficiently large discrete set $\mathbb{X} \subset \mathcal{X}$. Here, $E(\mathbf{x})$, or so called latent space, can be interpreted as coordinates on a lower dimensional manifold. 
% Estimating the optimal transfer operator in kernel space lie at the core of our proposed method. 

% The sampling stage of our proposed procedure relies on reconstructing the kernel pre-image in the latent space. Therefore, both the smoothness of the latent space and the proper pre-image method directly affect the generation results. Since Properties of regularized autoencoders have been well studied .

% Our proposed method roughly resembles the procedure of a Two-Stage VAE \cite{dai2018diagnosing}, in the sense that we also try to separate manifold learning and distribution fitting. Several key differences in our method include (1) at the manifold learning stage, we deterministically train the autoencoder by projecting the latent space onto a hypersphere, and (2) we only need to perform a one-step optimization at the distribution fitting stage, hence reduce the cost of training by a large margin. A recent work \cite{Ghosh2020From} also propose to perform ex-post density estimation on the latent space of a deterministically-trained autoencoder using k-Gaussian mixtures. Our method does not rely on constructing a posterior from a fixed family of distributions and ideally is able to approximate arbitrary latent distributions given an appropriate kernel.

% The ability to capture complex non-linear dynamics using our kernel method is also linked the recent advances in neural kernels. To be able to model a complex
