\begin{abstract}
% Flow-based generative models refer to deep generative models with 
% tractable likelihoods, and 
% offer several attractive properties including 
% efficient density estimation and sampling. 
% Despite many advantages, 
% current formulations (e.g., normalizing flow) often have an expensive memory/runtime footprint,
% which hinders their use in a number of applications. 
% In this paper, we consider the setting where we have access to an autoencoder, which is
% suitably effective for the dataset of interest. 
% Under some mild conditions,
% we show that we can calculate a mapping to a RKHS which subsequently enables deploying 
% mature ideas from the kernel methods literature for flow-based generative models. 
% Specifically, we can \textit{explicitly} map the RKHS distribution (i.e., 
% approximate the flow) to match or align with  
% a template/well-characterized distribution, via kernel transfer operators. 
% This leads to a direct and resource efficient approximation avoiding iterative optimization. 
% We empirically show that this simple idea yields competitive results on popular datasets such as CelebA,
% as well as promising results on a public 3D brain imaging dataset where the sample sizes are much smaller. 

Generative models which use explicit density modeling (e.g., variational autoencoders, flow-based generative models) involve finding a mapping from a known distribution, e.g. Gaussian, to the unknown input distribution. This often requires searching over a class of non-linear functions (e.g., representable by a deep neural network). While effective in practice, the associated runtime/memory costs can increase rapidly, usually as a function of the performance desired in an application. We propose a much cheaper (and simpler) strategy 
to estimate this mapping based on adapting known results in kernel transfer operators. We show that our formulation enables highly efficient distribution approximation and sampling, and offers surprisingly good empirical performance that compares favorably with powerful baselines, but with significant runtime savings. We show that the algorithm also performs well in small sample size settings (in brain imaging). 

%of arbitrarily shaped distributions in $\mathbf{R}^n$ and $\mathbf{S}^{n - 1}$. We propose to solve the problem of learning the non-linear density transfer operator in the input space by alternatively computing the {\it closed-form} optimal linear operator in the reproducing-kernel Hilbert space (RKHS). Through a comprehensive empirical evaluation, we further show that the proposed method yields comparable or better image generation samples on several computer vision datasets compared with popular VAE variants and other density estimation methods, while having a significant advantage in learning time. {\color{red} WRITE ONE SENTENCE ABOUT MEDICAL EXPTS}
\end{abstract}
