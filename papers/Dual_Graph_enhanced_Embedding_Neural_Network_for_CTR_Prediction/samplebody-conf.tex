% \setlength{\belowcaptionskip}{-0.55cm} 

% \newcommand{\yy}[1]{{\color{purple} [yy: #1]}}

\section{Introduction}\label{introduction}
% In the era of information explosion, there is a large amount of overloaded data.
The prediction of click-through rate (CTR) plays a crucial role in many information retrieval (IR) tasks, ranging from web search, personalized recommendation and online advertising, which is multi-billion dollars business nowadays \cite{li2019multi}.
% The goal of CTR prediction is to estimate the probability that a user will click on a recommended item based on her preference.
% This probability shows the user's interest in the recommended item, which determines the subsequent item ranking to make personalized recommendation for each user.
% There are two main streams of methods for CTR prediction, one is about modeling feature interactions of multiple category features, the other one is to mine temporal interest from user behaviors.
% \ghf{trainable embedding vectors?}
Most of the existing methods for CTR prediction can be classified into two categories, i.e., feature interaction modeling based methods and user interest mining based methods.
Both feature interaction modeling based methods and user interest mining based methods follow a similar Embedding \& Representation learning paradigm: input features are first transformed into trainable embedding vectors which are randomly initialized, and then transformed into fixed-length vector via feature interaction or interest mining, finally fed into fully connected layers to get the prediction score.
Models in the former class such as Factorization Machines (FM) \cite{rendle2010factorization}, Neural FM (NFM) \cite{he2017neural}, Product based Neural Network (PNN) \cite{qu2018product} and FM based Neural Network (DeepFM) \cite{guo2017deepfm} focus on designing novel structure to capture more useful feature interactions more effectively.
% Factorization machines (FM) learns a low dimensional latent vector for each feature and then models the pairwise feature interactions as the inner product of latent vectors.
% With the success of deep neural networks (DNNs), many works utilize DNN to learn feature interactions \cite{zhang2016deep,he2017neural,guo2017deepfm}.
% However, as the DNNs learns feature interactions in an implicit fashion, it's hard to know whether each order of feature interactions are modeled or not.
% Deep cross network (DCN) and Compressed interaction network (CIN) propose to model each order of feature interactions explicitly by applying feature crossing at each layer \cite{wang2017deep,lian2018xdeepfm}. \ruiming{why only these two models?}
The models in the latter class like Deep Interest Network (DIN) \cite{zhou2018deep}, Deep Interest Evolution Network (DIEN) \cite{zhou2019deep} and Multi-Interest Network with Dynamic routing (MIND) \cite{li2019multi} aim to mine user interest from each user's behavior sequence precisely.
% Deep interest network (DIN) uses an attention network to learn user's interest that is relevant to the target item \cite{zhou2018deep}.
% Deep interest evolution net-work (DIEN) utilizes GRU layers and auxiliary loss to learn user's dynamic interest \cite{zhou2019deep}. 
% Multi-interest network with dynamic routing (MIND) leverages capsule network to learn user's multiple interests \cite{li2019multi}.
% Deep cross network (DCN) solves this problem by using feature crossing explicitly at each layer \cite{wang2017deep}.
% Inspired by DCN, Compressed interaction network (CIN) further applies an vector-wise feature crossing operation to learn feature interactions.
% The former one heavily relies on the co-action of features in each sample while neglecting the item co-occurrence information in user behavior \cite{zhou2020can}.
% The latter one learns diverse interests in each user's behavior sequence.
% However, the feature interaction for other features are not fully exploited, which makes users' and items' representation sub-optimal and restricts the model's performance.

Though these two kinds of methods for CTR prediction have been investigated for years and obtained great progress, several challenges still exist, which limit the performance of existing methods, especially when deployed in large-scale industrial applications. 
% \ruiming{can we have two figures to support this observations?}
\begin{itemize}
\item \textbf{Feature Sparsity.} The performance of feature interaction based models heavily rely on the co-occurrence of different features.
However, as the number of users and items growing continuously in real recommender system, there are numerous sparse features that appear very few times in the training set.
To verify this discovery, we plot the feature frequency distribution of Tmall\footnote{https://tianchi.aliyun.com/dataset/dataDetail?dataId=42} and Alimama\footnote{https://tianchi.aliyun.com/dataset/dataDetail?dataId=56} dataset in Figure \ref{fig:statistics}.
As we can see, frequency of most features are relatively low.
It is hard for these methods to learn a good representation for these sparse features due to the low frequency of occurrence.

% However, there are numerous sparse features that appear very few times in the training set.
% The number of users and items is growing continuously in real recommender system.
% There are numerous sparse features that appear very few times in the training set.
% As the performance of feature interaction modeling based methods heavily rely on the co-occurrence of different features, 

% However, the performance of feature interaction modeling based methods heavily relies on the co-action of features in the training set, which 
% with feature embeddings under-optimized.
% However, the performance of feature interaction modeling based methods heavily relies on the co-action of features in the training set, which 
% may suffer from feature sparsity issue for industrial application where there are numerous long-tail features that appear very few times.
\item \textbf{Behavior Sparsity.} User interest mining based methods need rich user behaviors to obtain user's diverse interests.
However, user behaviors are characterized by heavy-tailed distributions, i.e., a significant proportion of users has very few interactions in the history, as shown in Figure \ref{fig:statistics}. This poses a key challenge  for these models with only limited user behavior information.
% However, there are many long-tail users with a short behavior sequence in the history.
% It's hard for existing methods to extract user interests accurately with such limited behaviors.
% while in some platforms like app stores or ticketing websites, most users are not active which results in a limited historical behavior sequence. 
% \item \textbf{Behavior ambiguity.} 
% In the real world, users' decisions are affected by many factors, and their behaviors are the results of multifaceted reasons, such as interest matching, social influence or just a random click. 
% The above mentioned two kind of methods just utilize user interactions with items (click, purchase, download, etc.) as supervision signal to predict user's preference on items, which overlook the underlying reasons for these actions and can easily lead to sub-optimal prediction results. 
\end{itemize}

% To solve these issues, in this work, we formulate the CTR prediction as embedd argue that, by organizing the user behaviors as a attribute-related collaborative heterogeneous graph and then apply graph convolution on this graph, the rich auxiliary relationships which imply user preferences and item semantics allows each feature to learn a more contextual representation and each user to get a more rich 


% we propose to apply graph convolution on attribute-related collaborative heterogeneous graph to

Recently, graphs have been used to represent relational information in recommendation datasets. Incorporating and exploiting the graph representation has shown to be effective for alleviating data sparsity~\cite{li2019graph,wang2019neural,ying2018graph}. The intuition motivating the involvement of graphs in recommender systems is that we include more relational information and increase the connectivity among users and items, leading to an improvement of recommendation quality.


\begin{figure}
	\centering
	\setlength{\belowcaptionskip}{-0.3cm}
	\setlength{\abovecaptionskip}{0cm}
	\includegraphics[width=0.45\textwidth]{Figure_statis.pdf}
	\caption{Statistics of feature frequency and behavior length distribution in Tmall and Alimama dataset.
% 	\yz{If have time, please change the font size of the xticks, yticks, and all the text in the figure. Way too small}
	}
% 	(\textcolor{blue}{zhirong: Tmall and Alimama two datasets feature/behavior distribution?})}
	\label{fig:statistics}
\end{figure}

We propose a novel 
Dual Graph enhanced Embedding Neural Network (DG-ENN), which is designed with two considerations to address the above two challenges in existing methods.
Specifically, we construct two kinds of graphs (i.e., attribute graphs and collaborative graph) from different perspectives to tackle the two above-mentioned sparsity issues. \emph{On the one hand}, a user (item) attribute graph is constructed by using user (item) features, such as gender, age, city, occupation (category, seller, brand). High-order proximity in the attribute graphs helps to enhance the embedding of sparse features, because learning embedding of a node also helps learning embedding of its neighbors, such that sparse features have more chances to be updated. With such enhanced feature embeddings, feature interactions can be learned more effectively. \emph{On the other hand}, a collaborative graph is built from the collaborative signals between users and items. In this graph, there exist edges between a user and an item, representing the user interacting with this item. Besides such user-item edges, user-user edges are defined based on similarity of user profiles and behaviors while item-item edges are formulated based on their transition relations in user behaviors. Exploiting the proximity of this graph, user behaviors with short length can be enhanced with other users' behaviors, by learning node representations.

Yet, how to learn effective user and item representations from the two aforementioned graphs is still challenging due to the following two reasons. First, in the user (item) attribute graph, the user (item) attributes are of different fields with very different characteristics, which makes aggregating them during the learning process non-trivial. Second, in the collaborative graph, there are various kinds of edges, resulting complex relations such as $u_1 \rightarrow v_1$, $u_1 \rightarrow u_2 \rightarrow v_1$, $u_1 \rightarrow u_2 \rightarrow v_2 \rightarrow v_1$ and $u_1 \rightarrow v_2 \rightarrow u_2 \rightarrow v_1$, which makes the relation modeling between user $u_1$ and item $i_1$ difficult.
% $\mathcal{U}\mathcal{V}$, $\mathcal{U}\mathcal{U}\mathcal{V}$, $\mathcal{U}\mathcal{U}\mathcal{V}\mathcal{V}$ and $\mathcal{U}\mathcal{U}\mathcal{U}\mathcal{V}\mathcal{V}$ that are all possible reasons drive user $\mathcal{U}$ to click on target item $\mathcal{V}$.
% which should be modeled and learned distinguishably.
To handle such two issues, DG-ENN learns the user and item embeddings from these two graphs with two novel strategies. To learn embeddings in the user (item) attribute graph, a divide-and-conquer strategy is proposed to learn the information for each field of attributes individually and perform the information integration at the end, so that the information of different attributes (with different semantics) will not make the learning process chaotic. When learning from the collaborative graph, an organized learning mechanism, inspired by curriculum learning, is introduced to learn the user-user and item-item edges (which are relative easier to train) first, and learn user-item edges after that.
% \yz{The above sentence is a bit abstract and hard to follow}
Furthermore, DG-ENN serves as an embedding learning framework, which works compatibly with the existing deep CTR models, including both feature interaction modeling based and user interest mining based methods.
% we argue that, by organizing the user behaviors as a attribute graph and a collaborative graph (see Fig \ref{fig:example} for an example) and then employ graph neural network (GNN) on these two graphs \cite{kipf2016semi}, the rich auxiliary relationships which imply user preferences and item semantics allows each node to learn a more contextual representation.
% Specially, we propose a novel method named dual graph enhanced embedding neural network (DG-ENN), which is designed with two considerations to correspondingly address the above two challenges in existing methods: 1) we develop an attribute graph neural network and then apply graph convolution on the attribute graph to utilize the high-order feature interaction modeling capacity of GNN to enhance the representation of sparse feature;
% 2) we design a collaborative graph neural network to fully exploit the collaborative signal existing in the user-user relevance graph, item-item transition graph and user-item interaction graph to explore the potential preference of users to learn their real interest. 


% Our DG-ENN is superior to existing methods in that:
% 1) compared with feature interaction modeling based methods, it models the high-order connectivity in attribute graph to enhance the embedding of sparse feature;
% 2) compared with user interest mining based methods, it exploits the user-user relations, item-item relations and user-item interactions to enrich the embedding of user behavior;
% 3) it combine the advantage of feature interaction and behavior sequence modeling by utilizing the high-order feature interaction capacity of GCN on features and behaviors, which is better than only consider part of them.

% to represent user behavior.
% Figure \ref{fig:example} illustrates an example.


% However, we find that all previous works only captured one type of auxiliary information for users and/or one type for items in the model, while ignoring a plenty of additional heterogeneous
% relationships on the graph.

% Several recent works have explored to utilize GNN for recommendation and have demonstrated its effectiveness for node representation learning.
% For instance, graph convolutional matrix completion (GCMC) \cite{berg2017graph}, PinSage employ graph convolutions on a bipartite graph to inject the collaborative signal into the embedding learning process \cite{ying2018graph}.
% GraphRec aggregates information from both user-item bipartite graph and user-user social graph to capture collaborative signal and social influence simultaneously \cite{fan2019graph}.
% However, utilizing GNN-based models to learn graph embedding for CTR prediction task poses two unique challenges:
% \begin{enumerate}
% \item[\textbf{C1:}] 
% Challenge 1 is : \textsl{how to aggregate attribute information with both features and relationships preserved and the characteristics of different attributes considered}.
% Some existing works overlook the attribute information and only using user/item id for representation learning, which are challenging to learn accurate embeddings for cold-start user/items (which do not appear in the training set).
% Some other works using the simple concatenation or linear transformation of attribute embeddings as the initial user/item representation.
% However, they overlook rich relationships between user/item nodes and attributes. 
% Besides, different types of attributes have different characteristics. 
% Traditional GNNs aggregate neighbor embeddings without considering their types, which fail to distinguish the characteristics difference of different types of attribute and will lead to a sub-optimal result. 

% \item[\textbf{C2:}] 
% There are multiple graphs (user-user graph, item-item graph and user-item graph) imply different structure relations between users and items, which can be formulated as a heterogeneous graph.
% Existing methods for heterogeneous graph representation learning can be categorized into two groups, path based and GNN based.
% Path based methods design different semantic meta-paths connecting users and items over heterogeneous graph, and then learn node representation by aggregating information from meta-paths neighbors.
% GNN based methods employ deep neural networks to aggregate neighbor information directly. 
% It is noticed that different types of neighbors are fused independently in these two kind of methods which reduce their ability to model graph correlations or more complex graph structure.
% Considering the correlations between different graphs should have a better node representation.
% Thus challenge 2 is: \textsl{how to aggregate structure information of user-item heterogeneous graph by considering the correlations between different sub-graphs.}
% % % There are mainly two lines of works that are explored to model structure information from heterogeneous graph to learn node representation.
% % One line of works design different semantic meta-paths connecting users and items over heterogeneous graph, and then learn node representation by aggregating information from meta-paths neighbors for each node.
% % The other line of works utilize GNN to aggregate different types of neighbors directly.
% \end{enumerate}

% To tackle the above challenges, we propose a novel end-to-end neural recommendation framework named Heterogeneous Graph enhanced Embedding Neural Network (HG-ENN) for CTR prediction.
% To handle the attribute information modeling issues, we leverage the divide-and-conquer strategy to integrate different attribute information while maintaining their intrinsic characteristics.
% In this work, we propose a novel attribute-related collaborative heterogeneous graph enhanced embedding neural network 
To sum up, our contributions in this paper can be summarized as follows: 
% \ruiming{Guowei check the contributions.}
\begin{itemize}
	\item We propose a novel Dual Graph enhanced Embedding Neural Network named DG-ENN, which enhances the feature embedding in an end-to-end graph neural network framework. To the best of our knowledge, this is the first deep CTR model using graphs for alleviating the feature sparsity and behavior sparsity problems. 
	
% 	\ruiming{graph information (more specifically, XXX and XXX)} an attribute-related collaborative heterogeneous graph and adapt graph convolution on this graph to alleviate the feature sparsity and behavior sparsity problem.

% 	We propose Heterogeneous Graph enhanced Embedding Neural Network (HG-ENN), a novel end-to-end graph neural network recommendation framework for CTR prediction. Besides, the Heterogeneous Graph enhanced Embedding can 
% 	be equipped to any state-of-the-art CTR prediction models to improve the recommendation performance.

	\item More specifically, a user (item) attribute graph and a collaborative graph in DG-ENN are proposed to alleviate the feature sparsity and behavior sparsity problem. To learn these graphs effectively, we propose to perform a divide-and-conquer learning strategy and a curriculum-learning-inspired organized learning strategy for these two kinds of graphs, respectively.
	%a novel Dual Graph enhanced Embedding Neural Network framework named DG-ENN, which enhances the initial embedding in an end-to-end graph neural network.
	
% 	a dual-level graph convolution which can utilize the attribute information for feature learning and the collaborative information for behavior representation.
% 	The learned embedding can be equipped to any state-of-the-art CTR prediction models to improve the recommendation performance.

% We leverage the divide-and-conquer strategy to integrate different attribute information while maintaining their intrinsic characteristics. 
% 	We also design an organized learning mechanism to aggregate structure information.
	
	\item We perform extensive experiments on three public datasets, demonstrating significant improvements of DG-ENN over state-of-the-art methods for CTR prediction. The necessity of the two kinds of graphs is verified empirically. Moreover, the validity of the proposed two learning strategies is also demonstrated.
\end{itemize} 



\section{Related Work}\label{related_work}
We briefly review three kinds of existing methods that are relevant to our work: 1) feature interaction modeling for CTR prediction, 2) user interest mining for CTR prediction, and 3) graph neural network for recommendation.
\newline
\textbf{Feature Interaction Modeling for CTR prediction.}
Using raw features directly for CTR prediction can hardly lead to a good result, thus feature interactions modeling is playing a core role and has been extensively studied in the literature \cite{lian2018xdeepfm}.
% FM is a widely used model since it's easy to implement and  of high-efficiency.
FM utilizes a low dimensional latent vector to represent each feature and learns 2-order feature interactions through the inner product of the related features' vectors \cite{rendle2010factorization}.
Owing to its superior performance in learning feature interactions, many extensions of FM are proposed \cite{juan2016field,pan2018field, xiao2017attentional}.
% Owing to its superior performance in learning feature interactions, many extensions of FM include Field-aware FM (FFM) \cite{juan2016field}, Gradient Boosting FM (GBFM) \cite{cheng2014gradient}, Field-weighted FM (FwFM) \cite{pan2018field} and Attentional FM (AFM)~\cite{xiao2017attentional} are proposed.
% Field-aware Factorization machines (FFM) \cite{juan2016field} learns multiple vectors for each feature to interact with different fields.
% Gradient Boosting Factorization machines (GBFM) \cite{cheng2014gradient} extends FM by selecting ``good" features with gradient boosting and models interactions between selected features only. 
% To explicitly model the importance of different feature interactions, Field-weighted Factorization machines (FwFM) \cite{pan2018field} uses additional parameters to explicitly capture importance of different feature interactions.
Recently, Deep Neural Network (DNN) has achieved great success with its great power of feature representation learning.
It is promising to exploit DNN for CTR prediction.
NFM \cite{he2017neural} enhances FM with DNN to model non-linear and high-order feature interactions simultaneously.
PNN further introduces a product layer between the embedding layer and DNN to model the feature interactions \cite{qu2018product}.
% It is noticed that NFM and PNN focus more on high-order feature interactions while ignore low-order feature interactions, which are also essential for CTR prediction.
% Therefore, the 
Wide \& Deep \cite{cheng2016wide} and DeepFM \cite{guo2017deepfm} introduce an interesting hybrid architecture, which contain a shallow model and a DNN model to learn low-order and high-order feature interactions simultaneously.
% However, as the final function learned by DNN can be arbitrary, there is no theoretical guarantee on whether each order of feature interaction is modeled or not.
Deep \& Cross Network (DCN) \cite{wang2017deep} and  CIN \cite{lian2018xdeepfm} apply feature crossing at each layer explicitly. Thus the orders increase at each layer and are determined by layer depth.
% Inspired by DCN, Compressed Interaction Network (CIN) \cite{lian2018xdeepfm}, a more effective model, is proposed to capture the feature interactions of bounded orders.
\newline
\textbf{User Interest Extraction for CTR prediction.}
Besides feature interactions modeling, user interest extraction is also very important.
Many works are proposed recently that focus on learning user interest representation from user behavior history.
DIN supposes that user interest is diverse, then uses an attention network to assign different scores to different user behaviors for user representation learning \cite{zhou2018deep}.
DIEN observes that user interest is dynamic, thus it utilizes GRU layers and auxiliary loss to capture evolving user interest for user's historical behavior sequence \cite{zhou2019deep}.
DSIN argues that user behavior sequence are composed of different homogeneous sessions \cite{feng2019deep}.
So it employs self-attention layer and Bi-LSTM to model user's inter-session and intra-session interests.
% It's insufficient for a single fixed-length vector to express user's various interests.
% Based on this assumption,
MIND learns multiple vectors for representing user's interests by using capsule network and dynamic routing architecture \cite{li2019multi}.
% Controllable multi-interest framework for the sequential recommendation (ComiRec) further improves MIND by introducing an additional controllable aggregation module to balance the recommendation accuracy and diversity \cite{cen2020controllable}. 
Despite great success has been made by these two kinds of CTR prediction methods, they cannot effectively solve the feature sparsity and behavior sparsity problems.
We are going to solve them in this paper by incorporating and exploiting the graph representation learning.
% \ruiming{limitations of these two categories. we are going to solve them in this paper.}
\newline
\textbf{Graph Neural Network for Recommendation.}
% Almost all the above CTR prediction methods model user preference on items based on the current sample while ignoring the rich auxiliary information from node relations, which easily suffers from the risks of feature sparsity and behavior sparsity.
% It's promising to leverage GNN for recommendation.
Graph Neural Network is widely used in recommender system in recent years.
FiGNN models feature interactions via graph propagation on the fully-connected fields graph \cite{li2019fi}.
GIN utilizes user behaviors to construct a co-occurrence commodity graph to mine user intention \cite{li2019graph}.
GCMC \cite{berg2017graph} treats the recommendation task as a link prediction problem and employs a graph auto-encoder framework on the user-item bipartite graph to learn user and item embeddings.
To better capture the collaborative signal existed in the user-item bipartite graph, many other GNN based works are then be proposed \cite{ying2018graph, wang2019neural, he2020lightgcn}.
% GCMC \cite{berg2017graph} treats the recommendation task as a link prediction problem and employs a graph auto-encoder framework on the user-item bipartite graph to learn user and item embeddings.  
% Neural Graph Collaborative Filtering (NGCF) \cite{wang2019neural} improves the recommendation effect by utilizing multiple GCN layers to explicitly modeling the high-order collaborative signals.
% Pinterest extends Graphsage \cite{hamilton2017inductive} on the pins-boards graph and proposes a large-scale GNN-based recommendation model named Pinsage \cite{ying2018graph}.
% Furthermore, to make better use of collaborative filter signals, LightGCN removes all transformation parameters and activation function of GCN and achieves better performance \cite{he2020lightgcn}.
% In order to identify the importance of neighbors, Graph attention network GAT \citep{velivckovic2017graph} introduces attention mechanism to measure impacts of different neighbors. 
% On the other hand, GraphSage \citep{hamilton2017inductive} performs a non-spectral graph convolution over a fixed size of sampled neighbors to integrate neighbor features for learning accurate node representations. 
To make full use of other information beyond user-item interactions, KGAT \cite{wang2019kgat} constructs a collaborative knowledge by combining user-item graph with knowledge graph and then applies graph convolution to get the final node representations. 
Heterogeneous graph Attention Network (HGAT) \cite{linmei2019heterogeneous} utilizes a semantic-level attention network and a node-level attention network to discriminate the importance of neighbor nodes and node types. 
% Heterogeneous graph neural network (HetGNN) \cite{zhang2019heterogeneous} also uses an attention network to aggregate different type of neighbor. Besides, it uses operations like LSTM, CNN to encode heterogeneous contents like text and image. 
Although these GNN-based models have made progress, applying them directly for CTR prediction is still challenging, as depicted in Section \ref{introduction}.

% is difficult to overcome the challenges mentioned in Section \ref{introduction}, which urges us to propose DG-ENN model. A more comprehensive comparison will be elaborated in Section XXX.

% Enhanced Graph Embedding with Side information (EGES) uses Deep-Walk to generate node sequence and the Skip-Gram model is used for graph embedding.




% \section{Preliminary}
% % In this paper, we organize the user tagging records as an undirected weighted Collaborative Tag Graph (CTG). 
% %The definition of Collaborative Tag Graph (CTG) is presented as:

% \begin{definition}[\textbf{Collaborative Heterogeneous Graph}]
% 	Let $\mathcal{V}$, $\mathcal{E}$ and $\mathcal{W}$ denote the sets of nodes, edges and edge weights respectively, where $\mathcal{V} = \mathcal{U}\cup\mathcal{I}$. 
% 	A collaborative heterogeneous graph is defined as an undirected weighted
% 	graph $\mathcal{G}=(\mathcal{V},\mathcal{E},\mathcal{W})$ with a node type mapping function $\phi(v):\mathcal{V}\rightarrow\mathcal{H}, \forall v \in \mathcal{V}$, where $\mathcal{H}=\{1,2\}$ whose elements represent \textit{user} and \textit{item} node type, respectively. For each edge $e=(v,v',w)\in\mathcal{E}$, it represents that the edge weight between node $v$ and $v'$ is $w\in\mathcal{W}$.
% \end{definition}

% \textbf{Example.} Fig XXX illustrates a CHG example for item recommendation.
% We can see that the graph consists of two types of nodes (user and item) and their semantic relations (purchasing relation between users and items, and social relation between users).

% \begin{definition}[\textbf{Attribute-related Collaborative Heterogeneous Graph}]
%     An attribute-related collaborative heterogeneous graph is defined as a graph $\mathcal{G}=(\mathcal{V},\mathcal{E},\mathcal{W},\mathcal{X})$, where $\mathcal{X}$ is the node attribute matrix and $\mathcal{X} = \mathcal{X_U}\cup\mathcal{X_I}$.
% \end{definition}   

% \textbf{Example.} Fig XXX illustrates a Attribute related CHG example for item recommendation.
% We can see that user node is associated with different characteristics: gender and age, and item node is associated with different attributes: brand, category and seller.

\section{Preliminary}
% To tackle the shortcomings of existing methods, we propose a novel Heterogeneous Graph enhanced Embedding Neural Network (HG-ENN) for CTR prediction, as shown in Figure XXX. From a bottom-up perspective, HG-ENN consists of four components: initial embedding component, heterogeneous graph enhanced embedding component, feature representation component and deep network component. In the following sections, we will present these components in detail.
\subsection{Problem Definition}
In this section, we formulate the CTR prediction task with necessary notations.
There are a set of $M$ users $\mathcal{U} = \left\{u_1,u_2,...,u_M\right\}$, a set of $N$ items $\mathcal{V} = \left\{v_1,v_2,...,v_N\right\}$, a set of $J$ fields of user attributes $\mathcal{A} =  \left\{\mathcal{A}^1,\mathcal{A}^2,...,\mathcal{A}^J\right\}$, a set of $K$ fields of item attributes $\mathcal{B} =  \left\{\mathcal{B}^1,\mathcal{B}^2,...,\mathcal{B}^K\right\}$ and a set of $R$ fields of other features like timestamp, displayed position denoted as $\mathcal{C} =\left\{\mathcal{C}^1,\mathcal{C}^2,\cdots,\mathcal{C}^R\right\}$ to describe the context. 
The user-item interactions are denoted as a matrix $\mathcal{Y}\in\mathbb{R}^{M \times N}$, where $y_{uv} = 1$ denotes user $u$ has interaction with item $v$ before, otherwise $y_{uv}=0$.
Further, each user and item is associated with a list of attributes $\mathbf{A}_u \subset \mathcal{A}$ and $\mathbf{B}_v \subset \mathcal{B}$.
% where $J$, $K$ are the number of user and item-associated attributes.
% \in \mathcal{R}^J  \in \mathcal{R}^K
% In addition to the attributes, user interactions with items $\mathcal{S} = \left\{s_1,s_2,\cdots,s_T\right\}$ \ruiming{universe?} can also utilized as features to predict user's preference on items (where $T$ is the length of user behavior sequence in the past), which can be encoded as a multi-hot feature as below.
% An encoding example of user attribute and user behavior is presented as \ruiming{some item attributes?}
% $$
%  \begin{matrix}
%  \underbrace{ [0,0,\cdots,1,0] } &  \underbrace{ [0,1,\cdots,0] } &  \underbrace{ [0,0,\cdots,1] } &  \underbrace{ [1,1,\cdots,1,0] }.\\
%  Occupation & City & Age & Click
% \end{matrix}
% $$
% $\textbf{H}_u = \{h_1,h_i,\cdots,h_{T_u}\}$
In addition to the attributes, each user also has a behavior sequence denoted as $\textbf{S}_u = \{s_1,s_i,\cdots,s_{T_u}\}$, where $s_i \in \mathcal{V}$ and $T_u$ is the length of user $u$'s behavior sequence in the past.
Besides user and item features, we denote context features as a list of $\mathbf{C} \subset \mathcal{C}$.
% Besides user and item features, other features like timestamp, displayed position to describe the context are also considered, denoted as $\mathcal{C} =\left\{c_1,c_2,\cdots,c_L\right\}$, where $L$ is the number of context features.
Concatenating all these features in a predefined order, one instance can be represented as: 
% \ruiming{not very consistent with the above. can we put this part before giving the example?}
\begin{equation}
\textbf{x}= [u, v, \textbf{A}_u, \textbf{B}_v, \textbf{S}_u, \textbf{C}]
\end{equation}
% An example of an sample is presented as: [Jerry, Phone, Male & London, Electronics & Apple, Book &  ]
An encoding example of user ID, user attribute and user behavior feature is presented as:
$$
 \begin{matrix}
 \underbrace{ [0,0,\cdots,1,0] } &  \underbrace{ [0,1,\cdots,1] } &  \underbrace{ [1,1,\cdots,1,0] } \\
 u: User ID & \textbf{A}_u:Occupation \& City &  \textbf{S}_u: Click
\end{matrix}
$$
The representations of other features are similar, so we omit them for simplicity.
It is noticed that we categorize user id, item id, user attributes, item attributes and context as features except the user behavior.
These features may encounter the feature sparsity problem when appear very few times.  
% \ruiming{relationship between feature and attribute.} 
The goal of CTR prediction is to predict the probability that user $u$ will be interested in the target item $v$ under context $\textbf{C}$.

% \ruiming{can we include user id into user features?}
% \ruiming{the same problem as s}


% As user-item interactions can be regarded as a user-item bipartite graph, we can denote a user-item interaction matrix $\mathbf{Y}\in\mathbb{R}^{M \times N}$ to represent the interaction relations, where $y_{uv} = 1$ means user $u$ has interaction with item $v$ before, otherwise $y_{uv}=0$.


\subsection{Base model}

Most of existing CTR prediction methods follow a similar Embedding \& Representation learning \& Prediction paradigm.
We refer them as the base model in this section.

\subsubsection{Initial Embedding}
The input data in CTR prediction are usually in a high-dimensional sparse binary form. 
It is common to apply an embedding layer upon the input to compress it into a low dimensional, dense real-value vector by looking up from an embedding table \textbf{W}.
% It is common to transform each instance into high-dimensional sparse binary features, which is the concatenation of all fields. Specially,
% \begin{equation}
% \textbf{x}= [\textbf{x}_1, \textbf{x}_2, \textbf{x}_i, \textbf{x}_j, \cdots, \textbf{x}_M]
% \end{equation}
% where $M$ is the number of fields, $\textbf{x}_i$ is a one-hot vector for univalent field $i$, and $\textbf{x}_j$ is a multi-hot vector for multivalent field $j$.
% Then an embedding layer is applied upon the vectors to compress it into a low dimensional, dense real-value vector by looking up from an embedding table \textbf{W}.
For one-hot vector $u,v$, the embedding representation is a single vector.
For multi-hot vector $\textbf{A}_u, \textbf{B}_v, \textbf{S}_u, \textbf{C}$, the embedding representation is a list of vectors.
The embedding vectors of these fields are then concatenated together to get the embedding of the whole input features.
\begin{equation}
 \textbf{E} = [\textbf{e}_u, \textbf{e}_v, \textbf{e}_{\textbf{A}_u}, \textbf{e}_{\textbf{B}_v}, \textbf{e}_{\textbf{S}_u}, \textbf{e}_{\textbf{C}}]
 \label{output_emb}
 \end{equation}
% It is noticed that the length of multi-hot behavior feature are dynamic, we transforms the embedding vectors of user behaviors into a fixed-length vector using the average pooling.
 
\subsubsection{Representation Learning}
% Using raw features directly for CTR prediction can rarely reveal the complex intrinsic reasons of user decision (e.g., male teenagers like shooting games which is the match of app category with user age and gender \cite{guo2017deepfm}), which show limited performance in practice.
% Therefore existing methods spend a lot of time on designing advanced network architecture for extracting useful feature interactions.
% Both feature interaction module and user interest mining module can be used for representation learning.
Many existing works focus on designing advanced network architecture for feature interaction modeling or user interest mining, which can be formulated as:
\begin{align}
\textbf{P} = f(\textbf{E})
\end{align}
% \ruiming{this is only feature interaction based model. what about user behavior based model?} 
For simplicity, we use the inner product module as the base representation learning module:
\begin{align}
\textbf{P} = [\textbf{E}_1, \cdots, \textbf{E}_F, \langle\textbf{E}_1,\textbf{E}_2\rangle,\cdots, \langle\textbf{E}_{F-1},\textbf{E}_{F}\rangle]
\end{align}
where $\langle,\rangle$ denotes the inner product operation, $F$ is the number of fields.
We also evaluate the performance of other representation learning module in the experiment section to validate the effectiveness of our proposed dual graph enhanced embedding.
% To handle the dynamic length of multi-hot behavior feature, we firstly transforms the embedding vectors of user behaviors into a fixed-length vector using the average pooling:
% $\textbf{e}_s = avg(\{\textbf{e}_{st}\}_{t=1}^T)$. \ruiming{I think no need to go into such details.}
% Then we build the inner product layer above the embedding layer to obtain the second order feature interaction explicitly:
% \begin{align}
% \textbf{P} = \sum_{i=1}^{f}\sum_{j=1}^{f}\langle\textbf{e}_i,\textbf{e}_j\rangle
% \end{align}
% where $\langle,\rangle$ denotes the inner product operation, $f = J + K + L + 3$ is the number of fields.
% The final representation is the concatenation of embedding vector and the inner product layer:
% \begin{align}
% \textbf{F} = concat(\textbf{E}, \textbf{P})
% \end{align}

\subsubsection{Fully Connected Layer}
% The fully connected layer is used as a classifier and for implicit feature interaction learning. 
The output of representation learning component is fed into the fully connected layer, which serves as a classifier.
% \begin{equation}
%  \textbf{a}^{(1)} = \sigma(\textbf{W}^{(1)}\textbf{P}+\textbf{b}^{(1)})
%  \end{equation}
% \begin{equation}
% \cdots
% \end{equation}
\begin{equation}
 \textbf{a}^{(l)} = \sigma(\textbf{W}^{(l)}\textbf{a}^{(l-1)}+\textbf{b}^{(l)})
 \end{equation}
 where $\textbf{a}^{(0)} = \textbf{P}$, $l$ is the current layer depth and $\sigma$ is the activation function. $\textbf{a}^{(l-1)}$, $\textbf{W}^{(l)}$ and $\textbf{b}^{(l)}$ are the input, model parameters and bias of the $l$-th layer. The output is a real number as the predicted CTR:
\begin{equation}
\widehat{y} = Sigmoid(\textbf{W}^{(L)}\textbf{a}^{(L-1)}+\textbf{b}^{(L)})
 \end{equation}
 
\subsubsection{Model Training}
The widely-used logloss is adopted as the objective function, which is defined as:
\begin{equation}
L_{logloss} = -\frac{1}{|\mathcal{S}|}\sum_{(\textbf{x}, y) \in \mathcal{S} }{y(\textbf{x})log\widehat{y}(\textbf{x})+(1-y(\textbf{x}))log(1-\widehat{y}(\textbf{x}))}
 \end{equation}
where $|\mathcal{S}|$  is the total number of training instances, $y(\textbf{x})$ is the real value for input vector $\textbf{x}$, and $\widehat{y}(\textbf{x})$ is the predicted value by our model.

% \yz{Maybe mention other than the logloss, regularization terms for all the learnable embeddings are applied?}

\section{Dual Graph enhanced Embedding}

\begin{figure*}
	\centering
	\setlength{\belowcaptionskip}{-0.3cm}
	\setlength{\abovecaptionskip}{0cm}
	\includegraphics[width=1\textwidth]{model.pdf}
	\caption{Overview of our proposed DG-ENN framework. The left part is the attribute graph convolution module, the central part is the collaborative graph convolution module and the right part is the deep network module.} 
	\label{fig:model}
\end{figure*}
% \ruiming{closer connection between this figure and text} \yz{some important terms are still missing in the figure such as divide-and-conquer, curriculum learning. Might worth to add those information. Since those terms constantly show up in different sections, directly mark them on the figure might help people to understand instantly while those terms got mentioned}
As stated earlier, most of existing methods focus on the representation learning layer, while overlook the embedding layer. Whereas, embdding layer with random initialization suffers from the feature sparsity and behavior sparsity issue.
Motivated by this observation, in this paper, we focus on the embedding learning  with a dual graph enhanced embedding network (DG-ENN) based on the base model.

% As shown in Figure \ref{fig:model}, \ruiming{connect text with the figure.} 
Dual graph enhance embedding component contains three modules: \textbf{graph construction}, \textbf{attribute graph convolution} and \textbf{collaborative graph convolution}. In this section, we elaborate each of these three modules in detail. Figure \ref{fig:model} gives a depiction of our proposed dual graph convolution framework.
% The attributes features encoding module is used to explicitly encode attribute information into representations of user nodes and item nodes with both features and relationships preserved and the characteristics of different attributes considered.
% The structure information modeling module propagates the structure associations alone different links using an organized learning mechanism to refine the representations of both user and item.
\subsection{Graph Construction}
\subsubsection{Attribute Graph} 
An attribute can be in multiple users or items, serving as a bridge to improve their representation.
Based on this bridge, we construct two attribute graphs $\mathcal{G}_{ua}=\langle \mathcal{U}\cup\mathcal{A},  \mathcal{E}_{ua}\rangle$ and $\mathcal{G}_{vb}=\langle \mathcal{V}\cup\mathcal{B}, \mathcal{E}_{vb}\rangle$.
Edges $e_{ua}=(u,a)$ and $e_{vb}=(v,b)$ indicate that attribute $a$ belongs to user $u$ and attribute $b$ belongs to item $v$.
The attribution graphs establish attribute connections to alleviate the feature sparsity problem. 
% \ruiming{the definition is strange, only nodes with no edges.}
% \ruiming{"attribute" and "feature"}
% Taking $v_1$->$c_1$, $v_2$->$c_1$ and $v_3$->$c_1$ as an example, category $c_1$ takes users $v_1$, $v_2$ and $v_3$ as inputs through neighbor aggregation to refine its representation, and then in turn contributes to the representation of $v_1$, $v_2$ and $v_3$.
\subsubsection{Collaborative Graph.}

Inspired by the collaborative filtering (CF) that similar users may exhibit similar preference on items \cite{sarwar2001item}, we utilize the collaborative signals to expand user behaviors and therefore alleviate the behavior sparsity problem. 
User-item interactions matrix $\mathcal{Y}$ can be regarded as a user-item bipartite graph $\mathcal{G}_{uv}=\langle \mathcal{U}\cup\mathcal{V}, \mathcal{E}_{uv}\rangle$. 
% \ruiming{graph definition}
There is an edge $e_{uv}=(u,v)$ if $y_{uv} = 1$. 
% \ruiming{$i$ or $v$?}
% where $\mathbf{A}_{ui}$ is the adjacent matrix:
% \begin{align}
%     \mathbf{A}_{ui} = \begin{pmatrix}
%             0 &\mathcal{Y} \\
%             \mathcal{Y}^\intercal & 0
%         \end{pmatrix}.
% \end{align}
However, $\mathcal{G}_{uv}$ only reveals the user-item interaction relation, but ignores the direct connections inside users and inside items. 
% \yz{If you talk about one-layer gcn, yes. But two layer GCN does consider two hop (u-u, i-i) relations. Might need to change a bit of the wording to avoid confusion}
As a result, we construct user-user similarity graph and item-item transition graph to extract such more complex relations.
% \ruiming{co-occurrence} 
The user-user similarity graph is built based on the user preferences and user attributes simultaneously:
\begin{align}
    sim(i,j) = \alpha_1\frac{\langle\textbf{Y}_i,\textbf{Y}_j\rangle}{||\textbf{Y}_i|| \cdot ||\textbf{Y}_j||} + \alpha_2\frac{\langle\textbf{A}_i,\textbf{A}_j\rangle}{||\textbf{A}_i|| \cdot ||\textbf{A}_j||}
\end{align}
where $\textbf{Y}_i$ and $\textbf{Y}_j$ denote the $i$-th and $j$-th row of the user-item interaction matrix $\mathcal{Y}$, $\textbf{A}_i$ and $\textbf{A}_j$ denote the attributes of corresponding user $i$ and user $j$.
We set $\alpha_1 = \alpha_2 = 0.5$ for simplicity. 
% \ruiming{where is $i$ and $j$? this equation is incorrect. }
% \ruiming{maybe we can define feature vectors of users and items first?}
After calculating the overall similarity between each two users, we can build a $k$-NN graph $\mathcal{G}_{uu}=\langle \mathcal{U}\cup\mathcal{U}, \mathcal{E}_{uu}\rangle$ with a pre-defined $k$. 
% \ruiming{graph definition}
% \yz{Might worth to discuss briefly why u-u and i-i graph doesn't construct follow the same rule} 
The item-item transition graph is built based on the sequential information of different users' behavior sequences.
Two items are connected in the item-item transition graph if they are interacted by the same users consecutively.
With all users' behavior sequences considered, we can construct an item-item graph $\mathcal{G}_{vv}=\langle \mathcal{V}\cup\mathcal{V}, \mathcal{E}_{vv}\rangle$. It can reflect user's preferences on group of items, which are ignored by the user-item bipartite graph. 
As a result, we can get the overall user-item collaborative graph $\mathcal{G}_{cf}=\langle \mathcal{U}\cup\mathcal{V}, \mathcal{E}_{uu}\cup\mathcal{E}_{uv}\cup\mathcal{E}_{vv}\rangle$.
By aggregating neighborhood information from $\mathcal{G}_{cf}$ iteratively, user's representation can be enhanced with other usersâ€™ behaviors, thus the behavior sparsity problem can be alleviated. 
% \ruiming{figure out, why it can alleviate behavior sparsity problem?}
% \ruiming{define $\mathcal{G}_{cf}$?} 
% \ruiming{make consistent with $i$ and $v$}

% However, it can only capture the co-occurrence of users and items, but ignores the user-user and item-item transition information.
% Previous CF based models which construct the user-item bipartite graph for relation modeling only capture the co-occurrence of items, but ignores the item-item transition information. 
% As a result, we construct an item-item graph from users' behavior to capture item-item transition information.
% Two items are connected by an undirected edge if they occur consecutively.
% By connecting all pairwise item transitions from all user's behavior sequence, we can construct an item-item graph reveals the global item transition information which can be used to learn a better item representation.

% For users, social relations like classmates, friends, or colleagues imply the explicit similarities between users, and can be directly used to construct a user-user social graph to boost the representation of users.
% However, in some cases, user-user social relations are not accessible.
% Thus we propose to construct a user-user graph based on the user-user similarity in the user-item interaction matrix.

\subsection{Attribute Graph Convolution}\label{Attribute Graph Convolution}
% \ruiming{which part of Figure 2?} 
With the two attribute graphs $\mathcal{G}_{ua}$ and $\mathcal{G}_{vb}$, we enrich the representation of sparse features with graph convolution.
% We use graph convolution with the following considerations:
% (1) feature interaction are applied at vector-wise; (2) high-order feature interactions are modeled explicitly; (3) all feature interactions can be compressed into a single vector, which is efficient. \ruiming{I do not see why we need this sentence.}
The user (item) attributes contain different fields with very different characteristics (for example, item price and item category are very different in semantics as well as distributions), which makes aggregating them during the learning process non-trivial.
% (\ruiming{example})

However, most of existing GNNs mix neighbors information indistinguishably and fail to distinguish different characteristics of neighbor attributes nodes, leading to sub-optimal results \cite{kipf2016semi,wang2019neural,he2020lightgcn}.
To consider different characteristics of attributes, we propose a divide-and-conquer strategy to integrate different attribute information while maintaining their intrinsic characteristics.
More specifically, we learn the information for each field of attributes individually and perform information integration at the end.
%apply multiple GNNs to model feature interactions of different attributes, then we aggregate multiple representations for user/item into a single vector.
\subsubsection{Field-wise Information Propagation}\label{Field-wise Information Propagation}
We first describe the information propagation within a field of attributes.
We adopt the state-of-the-art GCN models for such field-wise information propagation.
We use $h$ to denote the central node and $N_h$ to denote its neighbor set in this graph.
We adopt the following three types of GCN aggregators as potential candidates:
\begin{itemize}
\item \textsl{GCN Aggregator.}
GCN \cite{kipf2016semi} sums up the representation of central node and its neighbors and then applies a nonlinear transformation to generate the new representation:
\begin{align}
f_{GCN}^{(l)}(\textbf{e}_h^{(l)}, \textbf{e}_{N_h}^{(l)}) = \sigma (W^{(l)} \sum_{i\in \{h\} \cup \{N_h\}} d(h,i) \textbf{e}_i^{(l)})
\label{GCN}
\end{align}
where $\mathbf{e}^{(0)}$ is the initial embedding from $\mathbf{E}$, 
$\sigma$ and $W^{(l)}$ are the nonlinear activation function and transformation matrix of layer $l$.
$d(h,i) = 1/\sqrt{|N_h||N_i|}$ is the normalization factor. 
\item \textsl{NGCF Aggregator.} NGCF \cite{wang2019neural} improves GCN by considering additional feature interactions between central node and neighbor nodes. Besides, it aggregates the neighbors first and then add the neighbor representation to central representation, which can be formulated as follows:
\begin{equation}
\begin{aligned}
f_{NGCF}^{(l)}(\textbf{e}_h^{(l)}, \textbf{e}_{N_h}^{(l)}) = \sigma(W_1^{(l)}\mathbf{e}_{h}^{(l)} + \sum_{i\in N_h} d(h,i)(W_1^{(l)}\textbf{e}_i^{(l)} + \\ W_2^l(\textbf{e}_h^{(l)} \odot \textbf{e}_{i}^{(l)})))
\label{NGCF}
\end{aligned}
\end{equation}
where $W_1^{(l)}$, $W_2^{(l)}$ are the trainable weight matrix and $\odot$ denotes the element-wise product. 
\item \textsl{LightGCN Aggregator.} LightGCN \cite{he2020lightgcn} argues that the feature transformations and nonlinear activation function are not necessary and might even degrade the recommendation performance. Therefore, it removes the weight matrix and activation function:
\begin{align}
f_{LightGCN}^{(l)}(\textbf{e}_h^{(l)}, \textbf{e}_{N_h}^{(l)})  = \sum_{i\in N_h} d(h,i) \textbf{e}_i^{(l)} 
\label{lightgcn}
\end{align}
\end{itemize}
% \yz{For the GCN model with transformation function, are those weights shared across field or each field has its own transformation function ($W$)? Since there is no subscript for $W$ I will assume the transformation functions are shared. Might need clarification in the text if not.}

The feature representation with layer $l+1$ information propagation is formulated as:
\begin{align}
\textbf{e}_h^{(l+1)} = f_{\star}^{(l)}(\textbf{e}_h^{(l)}, \textbf{e}_{N_h}^{(l)})
% (\textbf{e}_h^{(l)}, \textbf{e}_{N_h}^{(l)}).
\end{align}
% \ruiming{presentation of $f$}
% \subsubsection{Feature Interaction Combination.}
Noticed that we use separate parameters for different fields when using GCN or NGCF aggregators. As aggregators are very important for the performance of our method, we will evaluate the effectiveness of the three GCN aggregators in experiment section.
After propagation of $L$ layers, we have $L + 1$ embeddings for each node.
Following \cite{he2020lightgcn}, we average these embeddings to get the final embedding for all central nodes:
\begin{align}
\hat{\textbf{e}}_h = \sum_{l=0}^{L}\textbf{e}_h^{(l)}
\end{align}
% we obtain multiple embeddings for each node, which represents different order of feature interactions.
% Since low-order feature interaction and high-order feature interactions can all be useful, we sum embeddings of all layer to form the final embedding of each node. 
% \ruiming{no need for this arguement. Give a reference, then merge it into the previous subsection.}
% \begin{align}
% \hat{\textbf{e}}_h = \sum_{l=0}^{L}\textbf{e}_h^{(l)}
% \end{align}

\subsubsection{Cross-field Information Integration.}

% \yz{Should we use Cross-field Information Integration? Maybe more accurate?}
As we have $J$ fields of user attributes and $K$ fields of item attributes, we generate $J$ user representations and $K$ item representations by field-wise information propagation in the previous section.
% For each attribute ${a_i}$ for user and ${a_j}$ for item, we can get a refined representation $Z_{a_i}$ and $Z_{a_j}$ with multi-layer GCN model.
% As there are no other graphs related with attributes, we use $Z_{a_i}$ and $Z_{a_j}$ as the enhanced embedding for CTR prediction directly.
As different fields of attributes have different importance to the final representation, it's natural to employ an attention mechanism to assign different importance scores for individual representations.
%  (\ruiming{which will be validated empirically in the experiments})
However, as the main contribution of this part of model is introducing the attribute graphs and modeling field-wise information individually (the effectiveness of which will be validated empirically in the experiments), we apply the average operation over multiple embeddings to get the final user and item representations:
\begin{align}
\textbf{z}_u = \sum_{t=1}^{J}\hat{\textbf{e}}_u^{(t)},  
\textbf{z}_v = \sum_{t=1}^{K}\hat{\textbf{e}}_v^{(t)}
\end{align}
where $\hat{\textbf{e}}_u^{(t)}$ denotes the embedding obtained from Section \ref{Field-wise Information Propagation} with respect to user attribute of field $t$ (we omit $t$ in Section \ref{Field-wise Information Propagation} for the sake of clarity).
% \ruiming{$t$ starts from 1? $t$ is not appeared in Eq.15, should be illustrated.}
The embedding of all the features in data instance can be refined as:
% \begin{equation}
%  \textbf{Z} = [\textbf{z}_u, \textbf{z}_v, \textbf{z}_{a_u}, \textbf{z}_{b_v}, \textbf{e}_c, \textbf{z}_s]
%  \end{equation}
\begin{equation}
 \textbf{Z} = [\textbf{z}_u, \textbf{z}_v, \textbf{z}_{\textbf{A}_u}, \textbf{z}_{\textbf{B}_v}, \textbf{z}_{\textbf{S}_u}, \textbf{e}_{\textbf{C}}]
 \end{equation}
Noted that all the features (except contextual features) are enhanced.
The reason why we don't construct graphs for contextual features is the risk of introducing noise as there are no clear relations between users/items and contextual features in most cases.


% \ruiming{state the reasons?}


% Some existing works overlook the attribute information and only using user/item id for representation learning, which are challenging to learn accurate embeddings for cold-start user/items (which do not appear in the training set).
% Some other works using the simple concatenation or linear transformation of attribute embeddings as the initial user/item representation.
% However, they overlook rich relationships between user/item nodes and attributes. 
% Besides, different types of attributes have different characteristics. 

\subsection{Collaborative Graph Convolution}\label{Collaborative Graph Convolution}
% \ruiming{which part of Figure 2?} 
The behavior sparsity issue is a challenge for the model to capture user interests with very limited user behavior information.
%by the lack of interaction records in the history.
% Unlike the methods in \cite{feng2020mtbrn,hu2018leveraging} which introduce additional users and items with the path extraction strategy, 
Using the high-order proximity of the collaborative graph to enrich user behaviors is beneficial to alleviate this issue.
However, the underlying reasons motivating a user to click an item may be various, which might be difficult for existing models to capture such complex relations.
For example, $u_1 \rightarrow v_1$, $u_1 \rightarrow u_2 \rightarrow v_1$, $u_1 \rightarrow u_2 \rightarrow v_2 \rightarrow v_1$ and $u_1 \rightarrow v_2 \rightarrow u_2 \rightarrow v_1$
%($\mathcal{U}\mathcal{V}$, $\mathcal{U}\mathcal{U}\mathcal{V}$, $\mathcal{U}\mathcal{U}\mathcal{V}\mathcal{V}$ and $\mathcal{U}\mathcal{U}\mathcal{U}\mathcal{V}\mathcal{V}$)
are all possible reasons to drive user $u_1$ to click on a target item $v_1$.
Existing meta-path based methods, like \cite{hu2018leveraging,wang2019heterogeneous}, introduce additional information with the path extraction strategy.
However, they need expert knowledge to design meta-paths.
Besides, it's difficult for meta-path methods to exhaustively search all useful meta-paths, which largely limits their performance.
GNN based methods use neighbor aggregation for behavior expanding, which don't need domain knowledge.
However, existing GNN based methods \cite{wang2019kgat,linmei2019heterogeneous,zhang2019heterogeneous} aggregate different types of neighbors at the same time, which overlook the dependency during the process of neighbor aggregation.
This reduce their ability to model graph correlations and more complex graph structure.
% That is, the correlations between neighbors which could constitute a complex relation structure .
% \ruiming{need one sentence to explain "dependency"}
%  reduce their ability to model graph correlations or more complex graph structure
To solve these issues, we design an organized learning mechanism by taking inspiration from the curriculum learning, which introduces different concepts at different time and then uses the previous learned concepts to promote the learning of new concepts \cite{bengio2009curriculum}.
% \ruiming{concept?}
Concretely, we first learn separate representations for users and items using user-user edges and item-item edges, then we use the user-item edges to learn the correlations between users and items. 
By this way, the complex node relation can be modeled well.
% \ruiming{state the reason, "dependency"}
As shown in the right part of Figure \ref{fig:model}, collaborative graph convolution network includes two components: 1) information propagation within Users/Items, 2) behavior expanding across users and items. 
% \yz{Maybe add the two step curriculum learning scheme in the Figure 2 diagram with step 1 and step 2 notation}

% By this way, we can capture the dependency during the process of neighbor aggregation, which is overlooked by existing GNN based methods \cite{wang2019kgat,linmei2019heterogeneous,zhang2019heterogeneous,wang2019knowledge}. 

\subsubsection{Information Propagation within Users/Items}
We first illustrate the information propagation within users/items. The input of this component is the refined embedding from attribute graph convolution module.
Taking user node as an example, we denote the central node as $u$ and its user neighbor set as $N_u$. 
% \ruiming{I change presentation here, check.}
The information propagation is formulated as: 
% \ruiming{why second-order? $f$ is not defined.}
% \begin{align}
% \textbf{z}_u^{(1)} = f^{(0)}(\textbf{z}_u^{(0)}, \textbf{z}_{N_u}^{(0)})
% \end{align}
\begin{align}
\textbf{z}_u^{(l)} = f_{\star}^{(l-1)}(\textbf{z}_u^{(l-1)}, \textbf{z}_{N_u}^{(l-1)})
\end{align}
where $\textbf{z}_0^{(0)}$ and $\textbf{z}_{N_u}^{(0)}$ are the refined embedding from $Z$.
% We can further stack more information propagation layers to explore high-order user-user information propagation.
% More formally, the $l$-order information propagation can be represented as: \ruiming{$l$? do we really need Eq.18?}
We use the average pooling of all layer's output as the final representation, as different layers of information propagation can represent different length of relations:
\begin{align}
\hat{\textbf{z}}_u = \sum_{l=0}^{L}\textbf{z}_u^{(l)}
\end{align}
Similarly, the representation for item nodes is:
\begin{align}
\hat{\textbf{z}}_v = \sum_{l=0}^{L}\textbf{z}_v^{(l)}
\end{align}

% \ruiming{one or two sentences for item node.}

\subsubsection{Behavior Expanding Across Users and Items}
After learning from user-user and item-item edges, 
we use user-item edges to learn the user-item preferences that can be used for user behavior expanding.
Taking user node as an example, the user-item correlations can be modeled as: 
% \ruiming{why second-order? what is $f$?}
\begin{align}
\textbf{q}_u^{(l)} = f_{\star}^{(l-1)}(\textbf{q}_u^{(l-1)}, \textbf{q}_{N_u}^{(l-1)})
\end{align}
% \begin{align}
% \textbf{q}_u^{(1)} = f^{(0)}(\textbf{q}_u^{(0)}, \textbf{q}_{N_u}^{(0)})
% \end{align}
where $\textbf{q}_u^{(0)} = \hat{\textbf{z}}_u$ and $\textbf{q}_{N_u}^{(0)} = \hat{\textbf{z}}_{N_u}$ are the enriched embedding after information propagation within users/items.
% The high-order user-item correlations are: \ruiming{$l$? do we really need Eq.21?}
Then we use the average pooling of all layers' output as the final representation:
\begin{align}
\hat{\textbf{p}}_u = \sum_{l=0}^{L}\textbf{p}_u^{(l)}
\end{align}
Similarly, the embedding of item $v$ is generated by the same process:
\begin{align}
\hat{\textbf{p}}_v = \sum_{l=0}^{L}\textbf{p}_v^{(l)}
\end{align}
Notice that $\textbf{q}_u^{(0)} = \hat{\textbf{z}}_u$ and $\textbf{q}_v^{(0)} = \hat{\textbf{z}}_v$ are also included in the final representation, because user-user relations and item-item relations also contain useful neighbors that can be used to expanding user behaviors.
Comparing with equation~\ref{output_emb}, after the graph enhanced operations,  we can get the final enhanced embdding for all the features:
\begin{equation}
 \textbf{P} = [\hat{\textbf{p}}_u, \hat{\textbf{p}}_v, \textbf{z}_{\textbf{A}_u}, \textbf{z}_{\textbf{B}_v}, \hat{\textbf{p}}_{\textbf{S}_u},
 \textbf{e}_{\textbf{C}}]
 \end{equation}

\subsection{Complexity Analysis}
% \ghf{Why do not compare the complexity with the general CTR prediction models. }
Since scalability is important for graph-based algorithms, we analyze the time complexity of DG-ENN for model training and online inference respectively.
As the enhanced embedding can be used directly for online inference, the time complexity of DG-ENN is \textbf{the same as} base model.
For model training, the layer-wise graph convolution is the main time cost.
Taking LightGCN aggregator as an example, the computational complexity for attribute graph is $\mathcal{O}(L \cdot |\mathcal{G}_{ua}| \cdot d$), where $|\mathcal{G}_{ua}|$ denotes the number of edges existed in $\mathcal{G}_{ua}$, $d$ is the embedding size and $L$ is the number of graph convolution layers.
Similarly, the computational complexity for collaborative graph is $\mathcal{O}(L \cdot |\mathcal{G}_{cf}| \cdot d$).
In real-world industrial application, there may be numerous edges connecting users (items) with attributes and connecting users and items.
To scale up the model training, neighbor sampling is necessary.
% As our model aggregates different types of neighbors individually, all types of neighborhood information need be preserved by using neighbor sampling independently.
% Thus neighbor sampling is very appropriate for our proposed DG-ENN, making it scalable for large-scale task. \ruiming{need?} 

% Existing meta-path based methods like \cite{feng2020mtbrn,hu2018leveraging,wang2019heterogeneous} introduce additional behaviors with the path extraction strategy. However, the definition of meta-path not only requires domain knowledge, but also can only leverage parts of existing relations.
% Existing GNN based methods like  \cite{wang2019kgat,linmei2019heterogeneous,zhang2019heterogeneous,wang2019knowledge} introduce additional behaviors with neighbor aggregation.
% However, these methods aggregate different types of neighbors at the same time, and thus fail to capture the dependency during the process of neighbor aggregation.
% It is noticed that different types of neighbors are fused independently in these two kind of methods which reduce their ability to model graph correlations or more complex graph structure.
% Considering the correlations between different graphs should have a better node representation.
% To cope with the structure information modeling issues, we design a organized learning mechanism by taking inspiration from the curriculum learning which introduces different concepts at different time and then use the previous learned concepts to promotes the learning of new concepts \cite{bengio2009curriculum}.
% Concretely, we first learn a separate representation for users and items use user-user graph and item-item graph, then we use the user-item graph to learn the correlations between users and items.

% \subsubsection{Graph Construction}
% As shown in Figure XXX, the heterogeneous graph enhanced embedding layer is composed of two modules: \textbf{attributes features encoding} and \textbf{structure information modeling}.
% The attributes features encoding module is used to explicitly encode attribute information into representations of user nodes and item nodes with both features and relationships preserved and the characteristics of different attributes considered.
% The structure information modeling module propagates the structure associations alone different links using an organized learning mechanism to refine the representations of both user and item.
% \subsubsection{\textbf{Attribute features Encoding }}
% $\newline$
% In real e-commerce scenery, user and item are associated with a number of attributes (e.g., city, occupation, category and brand), which is capable of refining user's and itemâ€™s property.
% Rather than traditionally using a simple concatenation or linear transformation of attribute embeddings as the initial user/item representation, we utilize GNN to extract both feature and relationship for attribute learning.
% % In order to obtain accurate modeling and representation for attributes and user/item, we employ a multi-channel graph propagation framework.
% Specially, given the initial embeddings $\textbf{e}_u$ and $\textbf{e}_v$ for a single user and item from the input initial embedding vector $\textbf{e}$.
% We denote the corresponding attribute embeddings with regard to $\textbf{e}_u$ and $\textbf{e}_v$ as $\{$$\textbf{e}_{a_1}^u$, $\textbf{e}_{a_i}^u$, $\cdots$, $\textbf{e}_{a_{N_u}}^u$$\}$ and $\{$$\textbf{e}_{a_1}^v$, $\textbf{e}_{a_j}^v$, $\cdots$, $\textbf{e}_{a_{N_v}}^v$$\}$, where $N_u$ and $N_v$ are the number of attributes belongs to $\textbf{e}_u$ and $\textbf{e}_v$.


% \textbf{Single Type Attribute Aggregation} 

% It's noticed that we take user side as an example for the simplicity of description.
% An attribute can be involved in multiple users or items, serving as the bridge to improving their representation.
% Taking $v_1$->$c_1$, $v_2$->$c_1$ and $v_3$->$c_1$ as an example, category $c_1$ takes users $v_1$, $v_2$ and $v_3$ as inputs through neighbor aggregation to refine its representation, and then in turn contributes to the representation of $v_1$, $v_2$ and $v_3$.


% These two categories of methods just utilize user interactions with items (click, purchase, download, etc.) as supervision signal to predict user's preference on items, which overlook the underlying reasons for these actions and can easily lead to sub-optimal prediction results. 
% Nodes in CTR prediction task are users and items that have attributes to describe user demographics (age, gender, occupation, etc.) and item profiles (category, seller, brand, etc.).
% Taking item side as an example, we organize the relationships among items and attributes in the form of heterogeneous graph.
% An item $i$ and an attribute $x$ are connected by a undirected edge if $i$ has attribute $x$.


% \subsubsection{\textbf{Structure Information Modeling}}
% $\newline$
% \textbf{Information Propagation Inside Users/Items}
% % Social relations like classmates, friends, or colleagues imply the explicit similarities between users, and can be directly used to construct a user-user social graph to boost the representation of users.


% Having obtained user-user graph $\mathcal{G}_{uu}$, item-item graph $\mathcal{G}_{ii}$ and user-item graph $\mathcal{G}_{ui}$, we now apply GNNs on these two graphs separately to leverage the homogeneous signal to update their embedding.


% \textbf{Information Propagation Across Users and Items}



\section{EXPERIMENTS}\label{experiment}
% We conduct extensive experiments on three real-world datasets to answer the following research questions:
% \begin{itemize}
% \item \textbf{(RQ1)} How does our proposed DG-ENN framework perform compared with state-of-the-art CTR methods and GNN models? 
% \item \textbf{(RQ2)} How do different components affect the performance of DG-ENN respectively? 
% \item \textbf{(RQ3)} Can our proposed DG-ENN framework really alleviate the feature sparsity and behavior sparsity problem? \ruiming{change the question}
% \end{itemize}
\subsection{Experiment Setup}\label{ExperimentSetup}
\subsubsection{Datasets}
We evaluate the effectiveness of our proposed model on three large-scale datasets: \emph{Alipay}, \emph{Tmall}, and \emph{Alimama}.
\begin{itemize}
	\item \textbf{Alipay}\footnote{https://tianchi.aliyun.com/dataset/dataDetail?dataId=53}: 
	This dataset is provided by Ant Financial Services in IJCAI-16 contest \cite{qin2020user}.
	It contains users' online/on-site behavior logs in 2015.
% 	 from July 1st to November 30th
	Each log contains multiple fields, including user ID, item ID, seller, category, online action type and timestamp.
	\item \textbf{Tmall}\footnote{https://tianchi.aliyun.com/dataset/dataDetail?dataId=42}: 
	This dataset is provided by Tmall.com in IJCAI-15 contest \cite{qin2020user}.
% 	It contains users' shopping records in the past 6 months before and on the ``Double 11'' day.
	The user profile is described by user ID, age range and gender. The item attributes include category and brand. The context features are timestamp and action type.
	\item \textbf{Alimama}\footnote{https://tianchi.aliyun.com/dataset/dataDetail?dataId=56}: 
	This dataset is provided by Alimama \cite{feng2019deep}.
% 	It contains 8 days ad display/click logs from the website of Taobao. 
	Each log in this dataset is composed of 12 feature fields including user ID, item ID, user micro group ID, occupation, shopping level, brand, category and some other information.
\end{itemize}
\subsubsection{Dataset Preprocessing.}
For each user, their clicked items are sorted by the interaction timestamp.
Following \cite{ren2019lifelong,qin2020user}, we split the dataset for evaluation.
Specifically, supposing there are \textbf{T} historical behaviors for a user, behavior [\textsl{1}, \textsl{T-3}] are collected as user behavior feature in the training set to predict the target item \textsl{T-2}. 
Similarly, behavior [\textsl{1}, \textsl{T-2}] are used as user behavior feature in the validation set to predict the target item \textsl{T-1}, behavior [\textsl{1}, \textsl{T-1}] are used as user behavior feature in the testing set to predict the target item \textsl{T}.
For each user, we random sample 10 non-clicked items to replace the target item as the negative samples.
Table \ref{tab:dataset} shows the statistics of the three datasets. 
% \ruiming{train/valid/test?}
 
\begin{table}[t]
 \caption{\small{Dataset statistics.}}
 \centering
 	\vspace{-0.2cm}
 	\setlength{\tabcolsep}{1mm}
 \begin{tabular}{c|c|c|c|c|c}
 \hline
 \textbf{Dataset} 	  & \textbf{\#Users}   & \textbf{\#Items} & \textbf{\#Instances} & \textbf{\#Features} & \textbf{\#Fields}  \\
 \hline
 Alipay   & 438,380 & 800,496 & 4,822,180 & 1,248,930 & 5  \\
 Tmall   & 415,800 & 565,888 & 4,573,800 & 994,771 & 8   \\
 Alimama  & 43,047 & 47,240 & 473,517 & 158,338 & 12 \\ 
 \hline
\end{tabular}
\label{tab:dataset}
	\vspace{-0.5cm}
\end{table}
\subsubsection{Baseline Models}
% In Section \ref{related_work}, we categorize existing methods that is relevant to our work into three types: (1) the feature Interaction model for CTR prediction; (2) the user interest mining model; (3) the GNN-based models for recommendation. We compare our model with the following popular CTR prediction methods of three types.
To verify the effectiveness of our proposed DG-ENN framework, we compare it with three groups of CTR prediction models: (A) feature interaction based models (LR \cite{lee2012estimating}, FM \cite{rendle2010factorization}, DeepFM \cite{guo2017deepfm}, PNN \cite{qu2018product}, AutoINT+ \cite{song2019autoint}); (B) user interest mining based models (DIN \cite{zhou2018deep}, DIEN \cite{zhou2019deep}); (C) GNN based models (GIN \cite{li2019graph}, FiGNN \cite{li2019fi}).


% we compare with two groups of methods: (A) CTR models and (B) GNN models. \ruiming{why do we need GNN models for CTR prediction?}
% We also split the CTR models into three parts: (A1) logistic regression and feature interaction based models, \ruiming{LR also belongs to FI based models} (A2) user interest mining based models and (A3) GNN based models.

% \begin{itemize}
%     \item \textbf{LR} \cite{lee2012estimating}: (A1) Logistic regression (LR) models shallow linear combination of features for CTR prediction task.
%     \item \textbf{FM} \cite{rendle2012factorization}: (A1) Factorization machines (FM) models all interactions between features which is effective even in huge sparsity.
% 	\item \textbf{DeepFM} \cite{guo2017deepfm}: (A1) DeepFM combines FM and DNN which models low-order and high-order feature interactions.
%     \item \textbf{PNN} \cite{qu2018product}: PNN can capture high-order feature interactions with a product layer. \ruiming{why high-order?}
%     \item \textbf{AutoInt+} \cite{song2019autoint}: (A1) AutoInt learns high-order feature interactions automatically by using multi-head self-attention mechanism to effectively improve the accuracy of CTR. In our comparison, we use AutoInt+ which integrates AutoInt and DNN network. \ruiming{why?}
%     \item \textbf{DIN} \cite{zhou2018deep}: (A2) DIN is the first model using the mechanism of attention to capture user's interests.
%     \item \textbf{DIEN} \cite{zhou2019deep}: (A2) DIEN designs an interest evolving layer and an interest extractor layer to capture evolving user interests and temporal interests simultaneously.
% 	\item \textbf{GIN} \cite{li2019graph}: (A3) GIN uses user behaviors to construct a co-occurrence commodity graph to mine user intention. \ruiming{check co-occurrence, maybe I used it somewhere.}
	
% % 	GIN first uses graph expressiveness to construct user interests in CTR prediction tasks. 
% % 	It can solve behavior sparsity problem of recommendation task by co-occurrence commodity graph.

%     \item \textbf{FiGNN} \cite{li2019fi}: (A3) FiGNN treats different fields as graph nodes and interactions between different fields as graph edges to construct a fully connected graph, then models feature interactions via graph propagation.
%     \item \textbf{GCN} \cite{kipf2016semi}: (B) GCN learns central node representations by aggregating message from neighbors. 
%     It's the most classic semi-supervised homogeneous convolutional network.
%     % \item \textbf{LightGCN} \cite{bpr}: Lightgcn is a simplified version of GCN by removing the feature transformation and nonlinear activation parts in GCN. 
%     % It's also a homogeneous graph model.
%     % \item \textbf{HAN} \cite{bpr}: Unlike homogeneous graph models, the heterogeneous graph model HAN considers the differences between different nodes and different relationships, and integrates the attention mechanism into the graph neural network. HAN gets the final node representation through the corresponding aggregation operation by using semantic level attention and node level attention to simultaneously learn the importance between meta-paths and node neighbors.
%     \item \textbf{KGAT} \cite{wang2019kgat}: (B) KGAT contains two parts. The first part is embedding layer which is trained by TransR, and the second part is attentive embedding propagation layer, which recursively mines high-level connected information in the form of graph convolution, and finally outputs the vector to complete the prediction. The training of two parts is relatively independent, and the loss functions are also different.
%     \item \textbf{HGAT} \cite{linmei2019heterogeneous}: (B) HGAT considers the heterogeneity of different types of information through heterogeneous graph convolution. It designs a dual-level attention mechanism which learn the importance of different graph node types and nodes. \ruiming{?}
% \end{itemize}
% We report the experimental results of baseline methods from \cite{ngcf,dgcf,lightgcn} directly.

\subsubsection{Evaluation Metrics}
We adopt two widely-used evaluation metrics, namely \textit{AUC} and \textit{Logloss}~\cite{guo2017deepfm}, to evaluate the performance.
% is a widely used evaluation metric for CTR prediction task.
\textit{AUC} ($\uparrow$)  measures the goodness of assigning positive samples higher scores than randomly chosen negative samples.
A higher AUC value indicates a better performance.
\textit{Logloss} ($\downarrow$) measures the distance between the predicted scores and the true labels.
A lower Logloss value means a better model performance.
% \begin{itemize}[leftmargin=*]
%     \item \textit{AUC} ($\uparrow$) is a widely used evaluation metric for CTR prediction task. 
%     It measures the goodness of assigning positive samples higher scores than randomly chosen negative samples. A higher AUC value indicates a better performance.
%     \item \textit{Logloss} ($\downarrow$) measures the distance between the predicted scores and the true labels.
%     A lower Logloss value means better model performance.
% \end{itemize}

\subsubsection{Parameter Settings}
For fair comparison, we set embedding dimension of all models as 10, and batch size as 2000. We tune learning rate from \{1e-1,1e-2,1e-3,1e-4\}, $L_{2}$ from \{0,1e-1,1e-2,1e-3,1e-4,1e-5\}, and dropout ratio from 0 to 0.9. The deep layers for all models are \{400,400,400,1\}. The models are optimized with Adam optimizer \cite{kingma2014adam}. 
In addition to the above hyper-parameters for all models, we tune the GCN layer size for graph models in the range of \{1,2,3,4\}.
We use the validation set for tuning hyper-parameters, and the performance comparison is conducted on the testing set.
We run each experiments 5 times and report the average results.
% We choose the best hyper-parameters by the
% the models for graph comparison in Section~\ref{tab:graphaccuracy} \ruiming{?} also need to tune the GNN layers which is searched in the range of \{1,2,3,4\}. 
% All models are trained 4 times and reported with average test results. 

\begin{table}[t]
\setlength{\abovecaptionskip}{0.1cm}
\setlength{\belowcaptionskip}{-0.0cm}
\centering
\caption{The overall comparison. 
% Underline indicates the second best model performance. Boldface denotes the best model performance. 
% We conduct Wilcoxon signed rank test.
$\star$ indicates a statistically significant level $p$-value<0.05 comparing DG-ENN with the best baseline (indicated by underlined numbers).}
% \ruiming{p-value?} \ruiming{remove the last three algorithms
\setlength{\abovecaptionskip}{0.2cm}
\setlength{\belowcaptionskip}{-0.0cm}
\setlength{\tabcolsep}{1mm}{
\small
\begin{tabular}{c|c|c|c|c|c|c}
\midrule[0.25ex]
Dataset &
\multicolumn{2}{c|}{Alipay} & 
\multicolumn{2}{c|}{Tmall} &
\multicolumn{2}{c}{Alimama} \\ \hline 
Model & AUC & Logloss &  AUC & Logloss  & AUC & Logloss \\\hline \hline
LR & 0.8196 & 0.2276 & 0.8760 & 0.1991 & 0.7207 & 0.2693  \\
FM  & 0.8498 & 0.2175 & 0.9026 & 0.1831 & 0.7396 & 0.2668  \\\hline
% DNN  & 0.8380 & 0.2353 & 0.9095 & 0.1864 & 0.6392 & 0.3034  \\
AutoInt+ & 0.8631 & 0.2147 & 0.9181 & 0.1730 & 0.7499 & 0.2611  \\
DeepFM  & 0.8648 & 0.2084 & 0.9155 & 0.1774 & 0.7653 & 0.2581  \\
PNN & \underline{0.8756} & \underline{0.2020} & \underline{0.9261} & \underline{0.1650} & \underline{0.7758} & \underline{0.2534}  \\ \hline
DIN & 0.8649 & 0.2081 & 0.9169 & 0.1761 & 0.7644 & 0.2584  \\
DIEN & 0.8731 & 0.2037 & 0.9235 & 0.1684 & 0.7710 & 0.2554  \\
 \hline
 GIN & 0.8645 & 0.2093 &0.9194 & 0.1716 & 0.7621 & 0.2595     \\
FiGNN & 0.8632 & 0.2121 & 0.9180 & 0.1753 & 0.7438 & 0.2635  \\ \hline
DG-ENN & $\textbf{0.9216}^{\star}$ & $\textbf{0.1674}^{\star}$ & $\textbf{0.9501}^{\star}$ & $\textbf{0.1399}^{\star}$ & $\textbf{0.8443}^{\star}$ & $\textbf{0.2254}^{\star}$ \\
\hline \hline
\end{tabular}}
\label{tab:ctraccuracy}
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Performance Comparison}\label{PerformanceComparison}
In this section, we compare the performance of DG-ENN with the state-of-the-art CTR prediction models.
Table \ref{tab:ctraccuracy} shows the experimental results of all compared models on three datasets.
We conduct Wilcoxon signed rank tests \cite{significant-test} to evaluate the statistical significance of DG-ENN with the best baseline algorithm. 
We have the following observations:
\begin{itemize}
    \item DG-ENN consistently yields the best performance for all datasets. 
    More precisely, DG-ENN outperforms the strongest baselines by \textbf{5.25\%}, \textbf{2.59\%} and \textbf{8.83\%} in terms of \textit{AUC} (\textit{17.13\%}, \textit{15.21\%} and \textit{11.05\%} in terms of \textit{Logloss}) on Alipay, Tmall and Alimama, respectively. 
    Possible reasons for the great improvement of DG-ENN over state-of-the-art CTR models may be the field-wise information propagation with attribute graph for alleviating the feature sparsity problem and the organized learning with user-item collaborative graph for behavior expanding across users and items.
    In contrast, most existing CTR methods ignore the rich relations existed in the data.
    We will further validate this observation in later experiments.
%   \ruiming{may not be able to say that.}
    \item LR performs worst among all baselines, which indicates that shallow linear combination of features is insufficient for CTR prediction. 
    FM performs better than LR, proves that the effectiveness of second-order feature interactions. 
    AutoInt+, DeepFM and PNN outperform FM, indicates that the modeling of high-order feature interactions is efficient for improving the performance of CTR prediction.
    DIN and DIEN achieve a comparable performance with DeepFM and PNN, demonstrates that user interest mining is also useful for representation learning. 
    \item GIN applies graph convolution on item-item graph to enrich user behaviors.
    However, it ignores the rich attribute information and the complex relations between users and items, thus it behaves much worse than DG-ENN.
    FiGNN employs graph convolution on field graph to model feature interactions.
    As no other relation information are introduced, it behaves no better than existing feature interaction based models. 
\end{itemize}

% All models use the validation set to find the optimal parameter, and then obtain the model performance on the test set.

\subsection{Ablation Study of DG-ENN}
\label{componentComparison}
\begin{table}
\setlength{\abovecaptionskip}{0.2cm}
\setlength{\belowcaptionskip}{-0.0cm}
\caption{Compatibility of embedding enhancement.}
    \centering
\resizebox{\linewidth}{!}{
\small
\begin{tabular}{c|c|c|c|c|c|c}
\midrule[0.25ex]
Dataset &
\multicolumn{2}{c|}{Alipay} & 
\multicolumn{2}{c|}{Tmall} &
\multicolumn{2}{c}{Alimama} \\ \hline 
%\cline{2-7}
Model & AUC & Logloss &  AUC & Logloss  & AUC & Logloss \\\hline \hline
PNN & 0.8756 & 0.2020 & 0.9261 & 0.1650 & 0.7758 & 0.2534  \\
DG-PNN  & \textbf{0.9216} & \textbf{0.1674} & \textbf{0.9501} & \textbf{0.1399} & \textbf{0.8443} & \textbf{0.2254}  \\ \hline
DIN  & 0.8649 & 0.2081 & 0.9169 & 0.1761 & 0.7644 & 0.2584  \\
DG-DIN  & \textbf{0.9283} & \textbf{0.1608} & \textbf{0.9644} & \textbf{0.1176} & \textbf{0.8331} & \textbf{0.2317}  \\\hline
FiGNN & 0.8632 & 0.2121 & 0.9180 & 0.1753 & 0.7438 & 0.2635  \\
DG-FiGNN & \textbf{0.9115} & \textbf{0.1767} & \textbf{0.9432} & \textbf{0.1501} & \textbf{0.8155} & \textbf{0.2406}  \\
\hline \hline
\end{tabular}}
\label{tab:Effect of embedding enhancement}
\end{table}

\begin{table}
\setlength{\abovecaptionskip}{0.2cm}
\setlength{\belowcaptionskip}{-0.0cm}
\caption{Superiority of dual graph convolution.}
    \centering
\resizebox{\linewidth}{!}{
\small
\begin{tabular}{c|c|c|c|c|c|c}
\midrule[0.25ex]
Dataset &
\multicolumn{2}{c|}{Alipay} & 
\multicolumn{2}{c|}{Tmall} &
\multicolumn{2}{c}{Alimama} \\ \hline 
%\cline{2-7}
Model & AUC & Logloss &  AUC & Logloss  & AUC & Logloss \\\hline \hline
PNN & 0.8756 & 0.2020 & 0.9261 & 0.1650 & 0.7758 & 0.2534  \\\hline
GCN-PNN  & 0.9036 & 0.1842 & 0.9402 & 0.1542 & 0.7953 & 0.2487  \\
KGAT-PNN & 0.9096  & 0.1796 & 0.9426 & 0.1510 & 0.7968 & 0.2467   \\
HGAT-PNN  & \underline{0.9119} & \underline{0.1764} & \underline{0.9433} & \underline{0.1495} & \underline{0.8002} & \underline{0.2454}  \\\hline
DG-PNN  & \textbf{0.9216} & \textbf{0.1674} & \textbf{0.9501} & \textbf{0.1399} & \textbf{0.8443} & \textbf{0.2254}  \\ 
\hline\hline
\end{tabular}}
\label{tab:Superiority of dual graph convolution}
\end{table}

\begin{table}
\setlength{\abovecaptionskip}{0.2cm}
\setlength{\belowcaptionskip}{-0.0cm}
\caption{Effect of dual graph construction.}
    \centering
\resizebox{\linewidth}{!}{
\small
\begin{tabular}{c|c|c|c|c|c|c}
\midrule[0.25ex]
Dataset &
\multicolumn{2}{c|}{Alipay} & 
\multicolumn{2}{c|}{Tmall} &
\multicolumn{2}{c}{Alimama} \\ \hline 
%\cline{2-7}
Model & AUC & Logloss &  AUC & Logloss  & AUC & Logloss \\\hline \hline
PNN & 0.8756 & 0.2020 & 0.9261 & 0.1650 & 0.7758 & 0.2534  \\\hline
attribute graph & 0.9037 & 0.1831 & 0.9365 & 0.1545 & 0.8097 & 0.2428  \\
uu \& vv graph  & \underline{0.9122} & \underline{0.1753} & \underline{0.9438} & \underline{0.1473} & \underline{0.8232} & \underline{0.2353}  \\
uv graph & 0.9109 & 0.1771 & 0.9437 & 0.1477 & 0.8221 & 0.2371  \\\hline
DG-ENN & \textbf{0.9216} & \textbf{0.1674} & \textbf{0.9501} & \textbf{0.1399} & \textbf{0.8443} & \textbf{0.2254}  \\
\hline \hline
\end{tabular}}
\label{tab:Effect of dual graph construction}
\end{table}
In this section, we conduct a series of experiments to better understand the design rationality of our proposed DG-ENN.
% In order to make our experiment more complete and explain the problem more clearly, in this section, we will conduct the experiment in three parts to discuss the effects of different components on the model. All models are based on PNN.
\subsubsection{On the compatibility of embedding enhancement.}
To investigate the compatibility of our proposed dual graph enhanced embedding, we integrate PNN, DIN and FiGNN with the dual graph enhanced embedding, which we named as DG-PNN, DG-DIN and DG-FiGNN.
The experimental results are presented in Table \ref{tab:Effect of embedding enhancement}.
From these results, we can see that DG-PNN, DG-DIN and DG-FiGNN significantly outperform the original PNN, DIN and FiGNN models.
It validates the compatibility of our embedding enhancement approach by demonstrating its effectiveness on working with various popular CTR models.
% effectiveness of enhancing initial embedding in CTR models and the effectiveness of our proposed dual graph enhanced embedding.
This enhanced embedding is more informative with richer field-wise information and expanded user behaviors.
% can provide more contextual feature representation and more rich interest extraction, leading to improved recommendation quality. \ruiming{change "more contextual feature representation and more rich interest extraction" --> "is more informative with richer field-wise information and expanded use behaviors"}

\subsubsection{On the superiority of dual graph convolution.}
To demonstrate the superiority of our proposed dual graph convolution module, we consider the variants of DG-PNN with different graph convolution models on our constructed graphs.
Specially, we compare dual graph convolution with GCN \cite{kipf2016semi}, KGAT \cite{wang2019kgat} and HGAT \cite{linmei2019heterogeneous}.
Noticed that the original GCN, KGAT and HGAT are not designed for CTR prediction.
We remove the prediction layer of these models and then apply them on our constructed graphs for embedding enhancement. We named these variants as GCN-PNN, KGAT-PNN and HGAT-PNN.
Table \ref{tab:Superiority of dual graph convolution} summarizes the results, from which we have the following findings:
\begin{itemize}
\item All these embedding enhanced models outperform the original PNN model, further verifies the effectiveness of embedding enhancement with relational information represented as graph.
\item KGAT-PNN behaves better than GCN-PNN on all three datasets. A possible reason is that GCN models the constructed graphs as a homogeneous graph, which ignores the different chasracteristics of differessnt fields while KGAT considers such differences.
% \ruiming{knowledge graph and user-item graph?}.
\item HGAT-PNN outperforms KGAT-PNN on all three datasets.
This is because that HGAT-PNN utilizes all the graphs and models them in an heterogeneous manner, while KGAT only considers the collaborative graph and item-attribute graph.
% knowledge graph and user-item graph. \ruiming{graph names}
\item DG-PNN consistently outperforms all baselines, which validates the superiority of our proposed dual graph convolution.
\end{itemize}

\subsubsection{On the effect of dual graph construction.}
We conduct experiments on three datasets to validate the effectiveness of the construction of attribute graph and collaborative graph.
We divide the collaborative graph into two parts: (1) user-user edges combined with item-item edges and (2) user-item edges, for detailed comparison.
Specially, we design four comparing variants: (1) DG-ENN only with the attribute graph (named attribute graph), (2) DG-ENN only with the user-user edges and item-item edges in the collaborative graph (named uu $\&$ vv graph), (3) DG-ENN only with the user-item edges in the collaborative graph (named uv graph) and (4) DG-ENN with neither attribute graph nor collaborative graph (that is PNN).
Table 
\ref{tab:Effect of dual graph construction} shows the comparison between different variants. 
We observe that PNN performs the worst in all these models, which proves the effectiveness of attribute graph and collaborative graph.
Moreover, we find that DG-ENN performs better than all the other models. 
It indicates that these attribute graph and collaborative graph are complementary to each other and can be combined together to improve the embedding quality and therefore boost the model performance.

\subsection{In-depth Analysis on Graph Modeling}\label{componentComparison}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Impact of Aggregators.}
To explore the impact of different aggregators, as formulated in Equation \ref{GCN}-\ref{lightgcn}, we compare the performance of our proposed model with different aggregators.
% \ruiming{remind the reviewers in which section or in which equation},
Figure \ref{fig:figureaggregators} summarizes the experimental results.
We can see that GCN aggregator performs better than NGCF aggregator on all datasets.
A possible reason is that additional feature interactions between central node and neighbor nodes introduced by NGCF aggregator makes it easy to overfit.
Moreover, we can see that LightGCN aggregator which removes both the weight matrix and activation function achieves the best performance on all datasets.
% This result further verifies our assumption that a simple aggregator could obtain a better result in CTR prediction scenery. \ruiming{where?}
\begin{figure}[htp]
	\centering
	\setlength{\belowcaptionskip}{-0.3cm}
	\setlength{\abovecaptionskip}{0cm}
	\includegraphics[width=0.45\textwidth]{table6.pdf}
	\caption{Impact of Aggregators.}
	\label{fig:figureaggregators}
\end{figure}

\subsubsection{Impacts of Attribute Information Exploitation}
To verify the effectiveness of our divide-and-conquer strategy to integrate different attribute information, as explained in Section \ref{Attribute Graph Convolution}, 
we replace our proposed attribute graph convolution module with other two alternatives: (1) using the linear transformation of ID embedding and attribute embeddings as the refined user/item representation \cite{kipf2016semi,berg2017graph}, (2) modeling the different fields of attributes without considering their fields.
Figure \ref{fig:figureInformationExploitation} shows the experimental results, we can see that the first alternative gets the worst performance, proving the effectiveness of modeling attributes as graphs.
Besides, modeling the different fields of attributes without considering their fields (i.e., the second alternative) performs worse than our model, verifies the necessary of modeling field-wise information individually.
\begin{figure}[htp]
	\centering
	\setlength{\belowcaptionskip}{-0.3cm}
	\setlength{\abovecaptionskip}{0cm}
	\includegraphics[width=0.45\textwidth]{table7.pdf}
	\caption{Impact of Attribute Information Exploitation.}
	\label{fig:figureInformationExploitation}
\end{figure}

\subsubsection{Impacts of Collaborative Signal Exploiting}
To validate the superiority of our design of organized learning for the collaborative graph, as explained in Section \ref{Collaborative Graph Convolution}.
We conduct three different operations on the aggregated embeddings from multiple types of edges: 
(1) element-sum operation; (2) element-mean operation; (3)  attention operation.
% \ruiming{do not repeat. "user's and item's two embedding" --> "two embeddings of user (item)"}
From the results in Figure \ref{fig:figureCollaborativeSignalExploiting}, we can see that our DG-ENN obtains the best results.
Besides, we find that attention operation achieves the second best results.
\begin{figure}[htp]
	\centering
	\setlength{\belowcaptionskip}{-0.3cm}
	\setlength{\abovecaptionskip}{0cm}
	\includegraphics[width=0.45\textwidth]{table8.pdf}
	\caption{Impacts of Collaborative Signal Exploiting.}
	\label{fig:figureCollaborativeSignalExploiting}
\end{figure}
% \yz{For our approach, is the order of step 1 and step 2 matters? What if we swap the training order, will the performance change?}
% \yz{I have the same question while reading the corresponding methodology section. I don't think I'm fully convinced the motivation of using the curriculum learning (ordered training)}
% \yz{Use consistent terminology (curriculum learning) } \ruiming{what is this?}
\subsection{Case Study}
In this part, we conduct experiments to verify that our model can solve the problem of feature sparsity and behavior sparsity.

\subsubsection{Feature Sparsity Analysis}
In order to prove that our model can solve the feature sparsity well, we select instances in the test set containing one of the four features with low frequency in the training set. The four chosen features are presented in Table \ref{tab:CaseStudy}, where they are represented by feature fields with subscripts of desensitization information. We report the performance (i.e., Logloss) of PNN and DG-PNN on the selected test instances in Table \ref{tab:CaseStudy}. We can find that DG-PNN achieves significant performance improvement on the test samples with sparse features, compared to PNN. This result demonstrates that our proposed  dual graph enhanced embedding alleviates the feature sparsity issue.

% which be choosed randomly from the test set, and then compare PNN and DGE-PNN using Root Mean Square Error (RMSE). RMSE can measure the differences between model prediction values and the
% actual observation values. A lower RMSE value means better model performance. Table \ref{tab:CaseStudy} shows experiment results in Alimama. 

% Frequency represents the number of occurrences in the test set and we choose four low frequency features. We can find that sparse features can be better expressed by DGE-PNN which makes predicted values closer to the actual values.
% the user-item interaction features and
\begin{table}[htp]
\setlength{\abovecaptionskip}{0.1cm}
\setlength{\belowcaptionskip}{-0.0cm}
\caption{Feature Sparsity Analysis in Alimama.}
    \centering
\scalebox{0.9}
{
\small
\begin{tabular}{c|c|c|c}
\midrule[0.25ex]
Feature &  Frequency & PNN (Logloss) & DG-PNN (Logloss) \\ \hline 
Brand\_{1} & 12 & 0.3502 & 0.2868 \\
Brand\_{2} & 5 & 0.3218 & 0.3111  \\ 
Cate\_{1} & 8 & 0.6125 & 0.5645    \\ 
Cate\_{2} & 9 & 0.0851 & 0.0223    \\ 
\hline \hline
\end{tabular}}
\label{tab:CaseStudy}
\end{table}
\subsubsection{Behavior Sparsity Analysis.}
Besides, the behavior sparsity problem can also be solved well by our model. We choose Alipay dataset for experiment because this dataset includes  less attribute information which may make noise for behavior sparsity analysis. Figure \ref{fig:BehaviorSparsityAnalysis} shows the performance comparison between DIN and DG-DIN with respect to different lengths of user behavior sequences. 
The result shows that the relative improvement of DG-DIN over DIN is more significant when length of user behavior sequence is less. That is to say, our proposed dual graph enhanced embedding alleviates the behavior sparsity issue.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% Lastly, we investigate the influence of graph structure. The result also is in Table \ref{tab:aggregationcomparison}. PNN-noGRAPH means all field feature embeddings are directly fed into MLPs, not updated by graph propagating. PNN-homo and PNN-hete both have constructed user-item graph, item-item graph and user\&item-attribute graph. The only distinction is whether the user\&item-attribute graph is a homogeneous graph or a heterogeneous graph. The result is apparent. PNN-hete has a better performance than other models which is also in accordance with our intuition, since heterogeneous graph can make fuller use of information and yet homogeneous graph simultaneously models several different signals and influence each other, adding noise to the model.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{table}
% \setlength{\abovecaptionskip}{0.1cm}
% \setlength{\belowcaptionskip}{-0.0cm}
% \caption{Impact of aggregators.}
%     \centering
% \resizebox{\linewidth}{!}{
% \small
% \begin{tabular}{c|c|c|c|c|c|c}
% \midrule[0.25ex]
% Dataset &
% \multicolumn{2}{c|}{Alipay} & 
% \multicolumn{2}{c|}{Tmall} &
% \multicolumn{2}{c}{Alimama} \\ \hline 
% %\cline{2-7}
% Aggregator & AUC & Logloss &  AUC & Logloss  & AUC & Logloss \\\hline \hline
% GCN & 0.8827 & 0.2055 & 0.9306 & 0.1670 & 0.6734 & 0.2927  \\
% NGCF & 0.8784 & 0.2077 & 0.9304 & 0.1662 & 0.6349 & 0.3013  \\ 
% LightGCN  & 0.8961 & 0.1936 & 0.9377 & 0.1582 & 0.7389 & 0.2747 \\ 
% \hline \hline
% \end{tabular}}
% \label{tab:Impact of aggregators}
% \end{table}


% \begin{table}
% \setlength{\abovecaptionskip}{0.1cm}
% \setlength{\belowcaptionskip}{-0.0cm}
% \caption{Impact of attribute information exploitation.}
%     \centering
% \resizebox{\linewidth}{!}{
% \small
% \begin{tabular}{c|c|c|c|c|c|c}
% \midrule[0.25ex]
% Dataset &
% \multicolumn{2}{c|}{Alipay} & 
% \multicolumn{2}{c|}{Tmall} &
% \multicolumn{2}{c}{Alimama} \\ \hline 
% %\cline{2-7}
% Model & AUC & Logloss &  AUC & Logloss  & AUC & Logloss \\\hline \hline
% Linear transformation & 0.8434 & 0.2304 & 0.9043 & 0.1906 & 0.6287 & 0.3084  \\
% Unified modeling & 0.8897 & 0.1989 & 0.9336 & 0.1617 & 0.7317 & 0.2776 \\ 
% Field-wise modeling & 0.8961 & 0.1936 & 0.9377 & 0.1582 & 0.7389 & 0.2747   \\ 
% \hline \hline
% \end{tabular}}
% \label{tab:Impact of attribute informtion exploiting}
% \end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{table}
% \setlength{\abovecaptionskip}{0.1cm}
% \setlength{\belowcaptionskip}{-0.0cm}
% \caption{Impact of Collaborative information exploiting.}
%     \centering
% \resizebox{\linewidth}{!}{
% \small
% \begin{tabular}{c|c|c|c|c|c|c}
% \midrule[0.25ex]
% Dataset &
% \multicolumn{2}{c|}{Alipay} & 
% \multicolumn{2}{c|}{Tmall} &
% \multicolumn{2}{c}{Alimama} \\ \hline 
% %\cline{2-7}
% Model & AUC & Logloss &  AUC & Logloss  & AUC & Logloss \\\hline \hline
% ReduceSum & 0.8901 & 0.1968 & 0.9329 & 0.1619 & 0.7238 & 0.2791  \\
% ReduceMean & 0.8916 & 0.1961 & 0.9338 & 0.1615 & 0.7231 & 0.2824  \\ 
% Attention  & 0.8582 & 0.2209 & 0.9179 & 0.1761 & 0.6549 & 0.2966    \\ 
% Our  & 0.8961 & 0.1936 & 0.9377 & 0.1582 & 0.7389 & 0.2747    \\ 
% \hline \hline
% \end{tabular}}
% \label{tab:Impact of Collaborative information exploiting}
% \end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{figure}
	\centering
	\setlength{\belowcaptionskip}{-0.3cm}
	\setlength{\abovecaptionskip}{0cm}
	\includegraphics[width=0.4\textwidth]{max_ubs.pdf}
	\caption{Behavior Sparsity Analysis.}
	\label{fig:BehaviorSparsityAnalysis}
\end{figure}
\section{Conclusions}
% \ruiming{need to refine}
In this paper, we focus on exploiting the graph representation learning to alleviate the feature sparsity and behavior sparsity problems for existing CTR models.
We propose a novel dual graph enhanced neural network based on attribute graph and collaborative graph. 
% \ruiming{propose a .....} \ruiming{two graphs}
On the one hand, to learn the feature representation from attribute graph effectively, we propose a divide-and-conquer learning strategy to perform field-wise attribute modeling.
On the other hand, to model the complex user-item relation for behavior expanding, we design a organized learning strategy inspired by curriculum-learning to learn the correlations within users/items and also between users and items. 
% \ruiming{within users/items and also between users and items}
The extensive experiments on three real-world datasets have demonstrated the superiority of our proposed DG-ENN over the state-of-the-art methods. Moreover, the proposed dual graph enhanced embedding is able to work collaboratively with various deep CTR models to boost their performance.
%and our model has achieved a significant improvement against state-of-the-art baselines.

% propose DG-ENN for CTR prediction to alleviate feature sparsity and behavior sparsity problem. The novel dual graph enhanced embedding neural network aggregate user (item) attribute graph information and collaborative graph information. Besides, we propose to perform a divide-and-conquer learning strategy and a curriculum-learning-inspired organized learning strategy
% for two graphs. Our framework overcomes the feature sparsity problems and behavior sparsity problems especially in large-scale industrial applications.
% In future, we will put more efforts on methods of graph constructing. Furthermore, another exciting direction is the landing of large-scale graphs in industrial scene, which can bring higher benefits for industry.

% \yz{I don't think it is necessary to discuss the future work.}
