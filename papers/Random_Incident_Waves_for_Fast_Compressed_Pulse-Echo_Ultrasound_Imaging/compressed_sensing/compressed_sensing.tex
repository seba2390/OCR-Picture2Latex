%---------------------------------------------------------------------------------------------------------------
% 1.) mathematical description of compressed sensing
%---------------------------------------------------------------------------------------------------------------
% a) CS deals with the stable recovery of a high-dimensional vector from a low-dimensional vector of potentially corrupted observations
% article:KutyniokGAMM2013: Theory and Applications of Compressed Sensing
% 1 Introduction
% - Taking a different viewpoint [key idea of CS],
%   it concerns the EXACT RECOVERY OF A HIGH-DIMENSIONAL SPARSE VECTOR AFTER A DIMENSION REDUCTION STEP. (p. 79)
\ac{CS} deals with
the stable%
\footnote{
  % a) adjective "stable" indicates that neither inaccurate observations nor a sparsity defect result in huge recovery errors
  % book:Foucart2013, Chapter 1: An Invitation to Compressive Sensing / Sect. 1.1: What is Compressive Sensing?
  % Stability.
  % - COMPRESSIVE SENSING FEATURES ANOTHER CRUCIAL ASPECT, namely,
  %   ITS RECONSTRUCTION ALGORITHMS ARE STABLE. (p. 7)
  % - This means that THE RECONSTRUCTION ERROR STAYS UNDER CONTROL when
  %   THE VECTORS ARE NOT EXACTLY SPARSE and when
  %   THE MEASUREMENTS y ARE SLIGHTLY INACCURATE. (pp. 7, 8)
  % - Without the stability requirement,
  %   the compressive sensing problem would be swiftly resolved and would not present much interest since
  %   MOST PRACTICAL APPLICATIONS INVOLVE NOISE AND COMPRESSIBILITY RATHER THAN SPARSITY. (p. 8)
  The adjective \adjective{stable} indicates that
  neither inaccurate observations nor
  a sparsity defect result in
  huge recovery errors
  \cite[7, 8]{book:Foucart2013}.
}
recovery of
% 1.) high-dimensional vector to be recovered
a high-dimensional vector
$\vect{x} \in \C^{ N }$ from
% 2.) low-dimensional vector of potentially corrupted observations
a low-dimensional vector of
potentially corrupted observations
$\vect{y}^{(\eta)} \in \C^{ M }$, $M \ll N$.
% b) known nonadaptive linear map \Phi from the high-dimensional space of vectors to be recovered to the low-dimensional space of observations provides the pristine observations
% article:KutyniokGAMM2013: Theory and Applications of Compressed Sensing
% 1 Introduction
% - The key idea of compressed sensing is to recover a sparse signal from VERY FEW NONADAPTIVE, LINEAR MEASUREMENTS by convex optimization. (p. 79)
Both vectors satisfy
the underdetermined linear algebraic system
\begin{equation}
 %--------------------------------------------------------------------------------------------------------------
 % underdetermined linear algebraic system / potentially corrupted observations
 %--------------------------------------------------------------------------------------------------------------
  \vect{y}^{(\eta)}
  =
  \mat{\Phi}
  \vect{x}
  +
  \vectsym{\eta},
 \label{eqn:cs_math_prob_general_obs_error}
\end{equation}
where
% 1.) known matrix representing the observation process
the known matrix
$\mat{\Phi} \in \C^{ M \times N }$ represents
the nonadaptive observation process, and
% 2.) unknown additive errors of bounded l2-norm
\TODO{upper bound is known!}
the unknown vector
$\vectsym{\eta} \in \C^{ M }$ denotes
additive errors of
bounded $\ell_{2}$-norm
$\tnorm{ \vectsym{\eta} }{2} \leq \eta$.

%---------------------------------------------------------------------------------------------------------------
% 2.) CS as a regularization method based on sparsity
%---------------------------------------------------------------------------------------------------------------
% a) basic linear algebra either negates the existence of any solution to the underdetermined linear algebraic system or predicts infinitely many solutions
Since
basic linear algebra either negates
the existence of
any solution to
% 1.) underdetermined linear algebraic system
the underdetermined linear algebraic system
\eqref{eqn:cs_math_prob_general_obs_error} or predicts
infinitely many solutions,
% b) CS replaces the identity by an inequality using the known upper bound on the l2-error and postulates that a known dictionary of structural building blocks represents the high-dimensional vector almost sparsely
% article:TroppPIEEE2010: Computational Methods for Sparse Solution of Linear Inverse Problems
% I. INTRODUCTION
% - In most applications, these problems [linear inverse problems] are
%   ILL-CONDITIONED OR UNDERDETERMINED, so
%   one MUST APPLY ADDITIONAL REGULARIZING CONSTRAINTS in order to obtain interesting or useful solutions. (p. 948)
% - Over the last two decades,
%   SPARSITY CONSTRAINTS HAVE EMERGED AS A FUNDAMENTAL TYPE OF REGULARIZER. (p. 948)
% - THIS APPROACH SEEKS
%   AN APPROXIMATE SOLUTION TO A LINEAR SYSTEM WHILE REQUIRING THAT
%   THE UNKNOWN HAS FEW NONZERO ENTRIES RELATIVE TO ITS DIMENSION
%   [Find sparse \vect{x} such that \mat{\Phi} \vect{x} \approx \vect{u}]
%   where \vect{u} is a target signal and \mat{\Phi} is a known matrix. (p. 948)
% - Generically, THIS FORMULATION IS REFERRED TO AS SPARSE APPROXIMATION [1]. (p. 948)
% - COMPRESSIVE SAMPLING REFERS TO A SPECIFIC TYPE OF SPARSE APPROXIMATION PROBLEM FIRST STUDIED IN [2] and [3]. (p. 948)
%   [2] article:CandesITIT2006_1, [3] article:DonohoITIT2006
\ac{CS} replaces
% 1.) CS replaces the identity by an inequality using the known upper bound on the l2-norm of the additive errors
the identity by
an inequality using
the known upper bound on
the $\ell_{2}$-norm of
the additive errors and postulates that
% 2.) CS postulates that a known dictionary of structural building blocks represents the high-dimensional vector almost sparsely
a known dictionary of
structural building blocks, e.g.
an orthonormal basis or
a frame, represents
the high-dimensional vector almost sparsely
\cite{article:TroppPIEEE2010}.
% c) latter constraint effectively reduces the total number of unknown components to a relatively small number of unknown coefficients associated with the relevant structural building blocks
The latter constraint effectively reduces
the total number of
unknown components to
a relatively small number of
unknown coefficients associated with
the relevant structural building blocks.
% d) identification of these building blocks and the subsequent estimation of their coefficients then enable the approximate recovery of the high-dimensional vector
The identification of
these building blocks and
the subsequent estimation of
their coefficients then enable
the approximate recovery of
the high-dimensional vector.
% e) both measures render CS a unique type of regularization method
% book:Hansen2010, Chapter 1: Introduction and Motivation
% book:Hansen1998, Sect. 1.3: Prelude to Regularization
Both measures render
\ac{CS} a unique type of
regularization method
(cf. e.g.
\cite[Chapt. 1]{book:Hansen2010},
\cite[Sect. 1.3]{book:Hansen1998}%
).

%---------------------------------------------------------------------------------------------------------------
% 3.) nearly-sparse representation
%---------------------------------------------------------------------------------------------------------------
% a) column vectors \vectsym{\psi}_{n} of the unitary matrix \mat{\Psi} define the admissible structural building blocks
Let
the column vectors
$\vectsym{\psi}_{n} \in \C^{ N }$,
$n \in \setcons{ N }$, of
the unitary matrix
$\mat{\Psi} \in \C^{ N \times N }$, which represents
a suitable orthonormal basis of
$\C^{ N }$, e.g.
the \name{Fourier},
a wavelet, or
the canonical basis, define
the admissible structural building blocks.
%, i.e. $\mat{\Psi} \herm{ \mat{\Psi} } = \herm{ \mat{\Psi} } \mat{\Psi} = \mat{I}$
% b) vector of transform coefficients constitutes a nearly-sparse representation of the high-dimensional vector, if the sorted absolute values of its components decay rapidly
The vector of
transform coefficients
\begin{equation}
 %--------------------------------------------------------------------------------------------------------------
 % nearly-sparse representation / vector of transform coefficients
 %--------------------------------------------------------------------------------------------------------------
  \vectsym{\theta}
  =
  \herm{ \mat{\Psi} }
  \vect{x}
 \label{eqn:def_transform_coefficients}
\end{equation}
constitutes
a nearly-sparse representation of
%\footnote{
  % paper avoids the adjective "compressible" to prevent confusion with the acoustic material parameter of "compressibility"
  % article:TroppPIEEE2010: Computational Methods for Sparse Solution of Linear Inverse Problems
  % I. INTRODUCTION / A. Formulations
  % - IN PRACTICE, SIGNALS TEND TO BE COMPRESSIBLE, RATHER THAN SPARSE. (p. 949)
  % - Mathematically,
  %   A COMPRESSIBLE SIGNAL HAS A REPRESENTATION WHOSE ENTRIES DECAY RAPIDLY WHEN SORTED IN ORDER OF DECREASING MAGNITUDE. (p. 949)
%  This paper avoids
%  the adjective \adjective{compressible} to
%  prevent confusion with
%  the acoustic material parameter of
%  \term{compressibility}.
%} of
the high-dimensional vector, if
the sorted absolute values of
its components decay rapidly.
% c) exact indices of these significant components are typically unknown a priori
The exact indices of
the significant components, which exceed
a specified absolute value, however, are
typically unknown \emph{a priori}.
% e) exactly sparse representations constitute excellent approximations of nearly-sparse representations
% article:TroppPIEEE2010: Computational Methods for Sparse Solution of Linear Inverse Problems
% I. INTRODUCTION / A. Formulations
% - COMPRESSIBLE SIGNALS ARE WELL APPROXIMATED BY SPARSE SIGNALS, so
%   the sparse approximation framework applies to this class. (p. 949)
% Exactly-sparse representations constitute
% excellent approximations of
% nearly-sparse representations.

%---------------------------------------------------------------------------------------------------------------
% 4.) CS problem and sparsity-promoting methods for its solution
%---------------------------------------------------------------------------------------------------------------
% a) insertions of the nearly-sparse representation and the sensing matrix into the underdetermined linear algebraic system
The insertions of
% 1.) nearly-sparse representation
the nearly-sparse representation
\eqref{eqn:def_transform_coefficients} and
% 2.) sensing matrix
the sensing matrix
\begin{equation}
 %--------------------------------------------------------------------------------------------------------------
 % sensing matrix
 %--------------------------------------------------------------------------------------------------------------
  \mat{A}
  =
  \mat{\Phi}
  \mat{\Psi},
 \label{eqn:cs_math_prob_general_sensing_matrix}
\end{equation}
which
% 3.) sensing matrix does not include any zero columns
% article:KutyniokGAMM2013: Theory and applications of compressed sensing
% 1 Introduction / 1.1 The Compressed Sensing Problem
% - Throughout we will always assume that
%   m < n and that
%   A DOES NOT POSSESS ANY ZERO COLUMNS, even if not explicitly mentioned. (p. 80)
is assumed not to contain
any zero columns, i.e.
$\vect{a}_{n} \in \C^{ M } \setminus \{ \vect{0} \}$ for
all $n \in \setcons{ N }$, into
% 4.) underdetermined linear algebraic system
the underdetermined linear algebraic system
\eqref{eqn:cs_math_prob_general_obs_error} yield
\begin{equation}
 %--------------------------------------------------------------------------------------------------------------
 % corrupted observations from the nearly-sparse representation
 %--------------------------------------------------------------------------------------------------------------
  \vect{y}^{(\eta)}
  =
  \underbrace{
    \mat{\Phi}
    \mat{\Psi}
  }_{ = \mat{A} }
  \vectsym{\theta}
  +
  \vectsym{\eta}
  =
  \mat{A}
  \vectsym{\theta}
  +
  \vectsym{\eta}.
 \label{eqn:cs_math_prob_general_obs_trans_coef_error}
\end{equation}
% b) CS problem associated with the corrupted observations
The associated \ac{CS} problem reads
\begin{equation}
\begin{alignedat}{2}
 %--------------------------------------------------------------------------------------------------------------
 % CS problem associated with the corrupted observations
 %--------------------------------------------------------------------------------------------------------------
  &
  \text{Recover}
  &
  \text{nearly-sparse }
  \vectsym{\theta}
  \in
  \C^{ N }\\
  &
  \text{subject to}
  \quad
  &
  \dnorm{ \vect{y}^{(\eta)} - \mat{A} \vectsym{\theta} }{2}{1}
  &\leq
  \eta
\end{alignedat}
\label{eqn:cs_math_prob_general}
\end{equation}
and
% c) summary of sparsity-promoting methods for the solution of the CS problem
% book:Foucart2013, Chapter 4: Basis Pursuit / Sect. 4.1: Null Space Property
% Nonconvex Minimization
% - Recall that the NUMBER OF NONZERO ENTRIES OF A VECTOR z \in \C^{ N } IS APPROXIMATED BY
%   THE qth POWER OF ITS lq-QUASINORM, [...]. (p. 81)
% - This observation suggests to REPLACE THE l0-MINIMIZATION PROBLEM (P_{0}) BY THE OPTIMIZATION PROBLEM
%   [minimize \norm{ \vect{z} }{q} subject to \mat{A} \vect{z} = \vect{y}] (P_{q}). (p. 81)
% book:Eldar2012, Chapter 1: Introduction to compressed sensing / Sect. 1.5 Signal recovery via l1 minimization
% - Given measurements y and the knowledge that our original signal x is sparse or compressible,
%   it is natural to attempt to recover x by solving an optimization problem of the form
%   [ \hat{x} = argmin \norm{ z }{0} subject to z \in B(y) ] (1.10), where
%   B(y) ensures that \hat{x} is consistent with the measurements y. (p. 27)
% - For example, in the case where our measurements are exact and noise-free,
%   we can set B(y) = {z: Az = y}. (p. 27)
% - When the measurements have been contaminated with a small amount of bounded noise,
%   we could instead consider B(y) = {z: \norm{ Az − y }{2} \leq \epsilon}. (p. 27)
% - In both cases, (1.10) finds the sparsest x that is consistent with the measurements y. (p. 27)
% - Note that in (1.10) we are inherently assuming that x itself is sparse. (p. 27)
% - In the more common setting where x = \Phi c, we can easily modify the approach and instead consider
%   [ \hat{c} = argmin \norm{ z }{0} subject to z \in B(y) ] (1.11) where
%   where B(y) = {z: A \Phi z = y} or B(y) = {z: \norm{ A \Phi z − y }{2} \leq \epsilon}. (p. 27)
% - By considering \tilde{A} = A \Phi we see that (1.10) and (1.11) are essentially identical. (p. 27)
% - One avenue for translating this problem into something more tractable is to replace
%   \norm{ . }{0} with its convex approximation \norm{ . }{1}. (p. 27)
% - Specifically, we consider
%   [ \hat{x} = argmin \norm{ z }{1} subject to z \in B(y) ] (1.12). (p. 27)
% article:TroppPIEEE2010: Computational Methods for Sparse Solution of Linear Inverse Problems
% III. OPTIMIZATION
% - Another fundamental approach to sparse approximation replaces
%   the combinatorial l0 function in the mathematical programs from Section I-A with
%   the l1-norm, yielding convex optimization problems that admit tractable algorithms. (p. 953)
% - In a concrete sense [48], the l1-norm is the closest convex function to the l0 function,
%   so this ``relaxation'' is quite natural. (p. 953)
%   [48] R. Gribonval and M. Nielsen, "Highly sparse representations from dictionaries are unique and independent of the sparseness measure," Aalborg Univ., Aalborg, Denmark, Tech. Rep., Oct. 2003.
% - Finally, we note another common formulation
%   [min \norm{x}{1} subject to \norm{ \mat{\Phi} \vect{x} - \vect{u} }{2} \leq \epsilon] (11) that
%   explicitly parameterizes the error norm. (p. 954)
% article:TroppPIEEE2010: Computational Methods for Sparse Solution of Linear Inverse Problems
% I. INTRODUCTION / A. Formulations
% - One natural variation [idealized l0-minimization problem] is
%   to RELAX THE EQUALITY CONSTRAINT TO ALLOW SOME ERROR TOLERANCE \epsilon \geq 0,
%   in case the observed signal is contaminated with noise
%   [min \norm{ \vect{x} }{0} subject to \norm{ \mat{\Phi} \vect{x} - \vect{u} }{2} \leq \epsilon] (2). (p. 949)
% - It is most common to measure the prediction–observation discrepancy with the Euclidean norm, but
%   other loss functions may also be appropriate. (p. 949)
% article:FoucartACHA2010: A note on guaranteed sparse recovery via l1-minimization
% - Let us note that THE RESULTS OF [2, 5, 1], EVEN THOUGH STATED FOR \R RATHER THAN \C, ARE VALID IN BOTH SETTINGS. (p. 97)
%   [5] article:FoucartACHA2009
% - Using (2), we can establish our main result in the COMPLEX SETTING, as stated below. (p. 97)
% article:FoucartACHA2009: Sparsest solutions of underdetermined linear systems via lq-minimization for 0 < q \leq 1
% 3. Approximate recovery from imperfect data
% - In order to approximately recover the original vector \vect{x} \in \R^{ N } from the knowledge of \vect{y},
%   we shall solve the minimization
%   [minimize \norm{ \vect{z} }{q} subject to \norm{ \mat{A} \vect{z} - \vect{y} }{2} \leq \beta_{2s} \theta] (P_{q, \theta}). (p. 397)
% - Lemma 3.1. A solution of (P_{q, \theta}) exists for any 0 < q \leq 1 and any \theta \geq 0. (p. 397)
% article:ChartrandISPL2007: Exact Reconstruction of Sparse Signals via Nonconvex Minimization
the methods for
its stable solution coalesce into
the sparsity-promoting $\ell_{q}$-minimization method
\cite[(1.11) and (1.12)]{book:Eldar2012},
\cite[(2) and (11)]{article:TroppPIEEE2010},
\cite[($\text{P}_{q, \theta}$)]{article:FoucartACHA2009}%
\footnote{
  % article:FoucartACHA2010: A note on guaranteed sparse recovery via l1-minimization
  % - Let us note that THE RESULTS OF [2, 5, 1], EVEN THOUGH STATED FOR \R RATHER THAN \C, ARE VALID IN BOTH SETTINGS. (p. 97)
  %   [5] article:FoucartACHA2009
  The first author confirms
  the validity of
  the results, which are exclusively stated for
  $\R^{N}$, for
  $\C^{N}$ in
  \cite{article:FoucartACHA2010}.
}
\begin{equation}
\begin{alignedat}{2}
 %--------------------------------------------------------------------------------------------------------------
 % sparsity-promoting lq-minimization method
 %--------------------------------------------------------------------------------------------------------------
  \hat{\vectsym{\theta}}^{(q, \eta)}
  &\in
  \underset{ \tilde{\vectsym{\theta}} \in \C^{ N } }{ \arg\min }
  \dnorm{ \tilde{\vectsym{\theta}} }{q}{1}\\
  &
  \mspace{24.5mu}
  \text{subject to}
  &
  \dnorm{ \vect{y}^{(\eta)} - \mat{A} \tilde{\vectsym{\theta}} }{2}{1}
  &\leq
  \eta,
\end{alignedat}
\tag{$\text{P}_{q, \eta}$}
\label{eqn:cs_lq_minimization}
\end{equation}
where
% 1.) parameter q of quasinorm
the parameter
$q \in [ 0; 1 ]$ determines
the type of
optimization method.
% d) parameter q = 1 induces the convex l1-minimization method
% book:Eldar2012, Chapter 1: Introduction to compressed sensing / Sect. 1.5 Signal recovery via l1 minimization
% - One avenue for translating this problem into something more tractable is to REPLACE
%   \norm{ . }{0} WITH ITS CONVEX APPROXIMATION \norm{ . }{1}. (p. 27)
% - Specifically, we consider
%   [ \hat{x} = argmin \norm{ z }{1} subject to z \in B(y) ] (1.12). (p. 27)
% - Provided that B(y) is convex, (1.12) IS COMPUTATIONALLY FEASIBLE. (p. 27)
% article:TroppPIEEE2010: Computational Methods for Sparse Solution of Linear Inverse Problems
% III. OPTIMIZATION
% - Another fundamental approach to sparse approximation replaces
%   the combinatorial l0 function in the mathematical programs from Section I-A with
%   the l1-norm, yielding CONVEX OPTIMIZATION PROBLEMS THAT ADMIT TRACTABLE ALGORITHMS. (p. 953)
% - In a concrete sense [48], the l1-NORM IS THE CLOSEST CONVEX FUNCTION TO THE l0 FUNCTION,
%   so this ``relaxation'' is quite natural. (p. 953)
%   [48] R. Gribonval and M. Nielsen, "Highly sparse representations from dictionaries are unique and independent of the sparseness measure," Aalborg Univ., Aalborg, Denmark, Tech. Rep., Oct. 2003.
% article:ChartrandISPL2007: Exact Reconstruction of Sparse Signals via Nonconvex Minimization
% I. INTRODUCTION
% - Importantly, this result [exact recovery w/ very high probability] continues to hold if
%   the l0 norm is replaced by the l1 norm, RESULTING IN A CONVEX PROBLEM as follows:
%   [min_{ \vect{u} } \norm{ \vect{u} }{1}, subject to \mat{Phi} \vect{u} = \vect{y}] (2) (p. 707)
The parameter $q = 1$ induces
% 1.) convex l1-minimization method
the convex $\ell_{1}$-minimization method, whose
implementation permits
computationally efficient algorithms, whereas
% e) half-open parameter interval q \in [ 0; 1 ) induces the nonconvex lq-minimization method
% book:Foucart2013, Chapter 4: Basis Pursuit / Sect. 4.1: Null Space Property
% Nonconvex Minimization
% - For 0 < q < 1, on the other hand, the optimization problem becomes
%   NONCONVEX AND IS EVEN NP-HARD; see Exercise 2.10. (p. 81)
% - Nonetheless, the properties of the lq-minimization for 0 < q < 1 can prove useful on theoretical questions. (p. 81)
% proc:ChartrandICASSP2008: Iteratively reweighted algorithms for compressive sensing
% ABSTRACT
% - In this paper we consider the use of ITERATIVELY REWEIGHTED ALGORITHMS for
%   COMPUTING LOCAL MINIMA OF THE NONCONVEX PROBLEM. (p. 3869)
% 1. INTRODUCTION
% - Specifically, the l1 NORM IS REPLACED WITH THE lp NORM, WHERE 0 < p < 1
%   (in which case \norm{·}{p} isn’t actually a norm, though d(x, y) = \norm{x − y}{p}^{p} is a metric):
%   [ min_{ \vect{u} } \norm{ \vect{u} }{p}^{p}, subject to \mat{\Phi} \vect{u} = \vect{b}. ] (2) (p. 3869)
% 2. ALGORITHMS FOR NONCONVEX COMPRESSIVE SENSING
% - It must be noted that (2) is a NONCONVEX OPTIMIZATION PROBLEM when p < 1, and
%   ALL OF THE ALGORITHMS CONSIDERED HERE ARE ONLY DESIGNED TO PRODUCE LOCAL MINIMA. (p. 3870)
% - Candès, Wakin, and Boyd have proposed
%   an iteratively reweighted l1 minimization algorithm corresponding to the p = 0 case above [3]. (p. 3871)
%   [3] E. J. Candès, M. B.Wakin, and S. P. Boyd, “Enhancing sparsity by reweighted l1 minimization.” Preprint.
% article:ChartrandISPL2007: Exact Reconstruction of Sparse Signals via Nonconvex Minimization
% I. INTRODUCTION
% - It is natural to ask what happens if the l1 NORM IS REPLACED BY THE lp NORM FOR SOME p \in ( 0, 1 ). (p. 707)
% - THE RESULTING OPTIMIZATION PROBLEM [l1 replaced by lp, p \in ( 0, 1 )] WILL NOT BE CONVEX, and
%   IT IS DESCRIBED IN THE LITERATURE AS INTRACTABLE. (p. 707)
% - Be that as it may, in this letter, we demonstrate that
%   THE MUCH SIMPLER TASK OF FINDING A LOCAL MINIMIZER CAN PRODUCE
%   EXACT RECONSTRUCTION OF SPARSE SIGNALS WITH MANY FEWER MEASUREMENTS THAN WHEN p = 1. (p. 707)
% II. RESTRICTED ISOMETRY CONSTANTS
% - On the one hand,
%   the value of these results depends on the ABILITY TO COMPUTE A GLOBAL MINIMIZER OF A NONCONVEX FUNCTIONAL. (p. 708)
% - On the other hand,
%   ANY LOCAL OPTIMIZATION METHOD CAN DO SO IF INITIALIZED BY A POINT SUFFICIENTLY CLOSE TO THE GLOBAL OPTIMUM. (p. 708)
% IV. CONCLUSIONS
% - Moreover, the algorithmic approach in this letter is relatively naive;
%   MORE SOPHISTICATED OPTIMIZATION METHODS SHOULD REDUCE THE RECONSTRUCTION TIME FURTHER. (p. 710)
% - This will increase the number of applications for which the approach is worthwhile. (p. 710)
the half-open parameter interval
$q \in [ 0; 1 )$ induces
% 1.) nonconvex lq-minimization method
the nonconvex $\ell_{q}$-minimization method, whose
global intractability necessitates
local approximations.

%---------------------------------------------------------------------------------------------------------------
% 5.) sufficient conditions on the sensing matrix for the stable recovery and their verifiability
%---------------------------------------------------------------------------------------------------------------
% a) multiple sufficient conditions on the sensing matrix ensure the stable recovery of the nearly-sparse representation in the CS problem by the sparsity-promoting lq-minimization method
Multiple sufficient conditions on
% 1.) sensing matrix
the sensing matrix
\eqref{eqn:cs_math_prob_general_sensing_matrix} ensure
the stable recovery of
% 2.) nearly-sparse representation
the nearly-sparse representation
\eqref{eqn:def_transform_coefficients} in
% 3.) CS problem
the \ac{CS} problem
\eqref{eqn:cs_math_prob_general} by
% 4.) sparsity-promoting lq-minimization method
the sparsity-promoting $\ell_{q}$-minimization method
\eqref{eqn:cs_lq_minimization}.
% b) sufficient conditions specify upper bounds for various characteristic measures quantifying the suitability of the sensing matrix
They specify
% 1.) upper bounds
upper bounds for
% 2.) various characteristic measures
various characteristic measures quantifying
% 3.) suitability
the suitability of
% 4.) sensing matrix
the sensing matrix
\eqref{eqn:cs_math_prob_general_sensing_matrix}, e.g.
% 5.) null space constants in the lq-robust null space property of order s
% book:Foucart2013, Chapter 4: Basis Pursuit / Sect. 4.3: Robustness
% - Definition 4.21.
%	- Given q \geq 1, the matrix A \in \C^{ m × N } is said to satisfy
%	  the lq-ROBUST NULL SPACE PROPERTY OF ORDER s (with respect to \norm{ . }) with
%	  constants 0 < \rho < 1 and \tau > 0 if, for any set S ⊂ [N] with card(S) \leq s,
%	  [ \norm{ \vect{v}_{S} }{q} \leq \frac{ \rho }{ s^{1 - 1/q} } \norm{ \vect{v}_{\bar{S}} }{1} + \tau \norm{ \mat{A} \vect{v} } ] for
%	  all \vect{v} \in \C^{ N }. (p. 87)
% - Theorem 4.22.
%	- Suppose that the matrix A \in \C^{ m × N } satisfies
%	  the l2-ROBUST NULL SPACE PROPERTY OF ORDER s WITH CONSTANTS 0  < \rho < 1 AND \tau > 0.
%	  Then, for any \vect{x} \in \C^{ N }, a solution x^{#} of (P_{1,η}) with
%	  \norm{ . } = \norm{ . }{2}, \vect{y} = \mat{A} \vect{x} + \vect{e}, and \norm{ \vect{e} }{2} \leq \eta approximates
%	  the vector x with lp-error
%	  [ ] for
%	  some constants C, D > 0 depending only on \rho and \tau. (p. 88)
% book:Foucart2013, Chapter 4: Basis Pursuit
% - We investigate CONDITIONS ON THE MATRIX A which
%   ENSURE EXACT OR APPROXIMATE RECONSTRUCTION OF
%   THE ORIGINAL SPARSE OR COMPRESSIBLE VECTOR x. (p. 77)
% - In Sect. 4.1, we start with
%   A NECESSARY AND SUFFICIENT CONDITION FOR
%   THE EXACT RECONSTRUCTION OF EVERY SPARSE VECTOR \vect{x} \in \C^{ N } as a solution of (P_{1}) with
%   the vector \vect{y} \in \C^{ m } obtained as \vect{y} = \mat{A} \vect{x}.
%   This condition is called the NULL SPACE PROPERTY. (p. 77)
% - In Sects. 4.2 and 4.3,
%   WE STRENGTHEN THIS NULL SPACE PROPERTY TO MAKE THE RECONSTRUCTION VIA BASIS PURSUIT
%   STABLE WITH RESPECT TO SPARSITY DEFECT AND
%   ROBUST WITH RESPECT TO MEASUREMENT ERROR. (p. 77)
% book:Eldar2012, Chapter 1: / Sect. 1.
% - Definition 1.2
%	- A matrix A satisfies the null space property (NSP) of order k if there exists a constant C > 0 such that,
%	  [ \norm{ \vect{h}_{\lambda} }{2} \leq C \frac{ \norm{ \vect{h}_{\lambda^{c}} }{1} }{ \sqrt{k} } ] (1.5)
%	  holds for all \vect{h} \in \null{\mat{A}} and for all \lambda such that \abs{ \lambda } \leq k. (p. 17)
% - We will see in Section 1.5 (Theorem 1.8) that the NSP of order 2k is SUFFICIENT to establish
%   a guarantee of the form (1.6) for a practical recovery algorithm (l1 minimization). (p. 18)
the null space constants
\cite[Def. 4.21]{book:Foucart2013},
\cite[Def. 1.2]{book:Eldar2012},
% 6.) restricted isometry ratio
% article:FoucartACHA2009: Sparsest solutions of underdetermined linear systems via lq-minimization for 0 < q \leq 1
% 2. Exact recovery via lq-minimization
% - Our results are to be stated in terms of a quantity invariant under the change A ← cA, namely
%   [ \gamma_{2s} := {alpha_{2s}}^{-2} {\beta_{2s}}^{2} \geq 1 ]. (p. 396)
the restricted isometry ratio
\cite{article:FoucartACHA2009}, and
% 7.) restricted isometry constant (RIC)
% article:FoucartACHA2010: A note on guaranteed sparse recovery via l1-minimization
% - A much favored tool in the analysis of (P_{1}) has been
%   the RESTRICTED ISOMETRY CONSTANTS δ_{k} of the m × N measurement matrix \mat{A}, defined as
%   the SMALLEST POSITIVE CONSTANTS δ such that
%   [(1 - δ) \norm{ \vect{z} }{2}^{2} \leq \norm{ \mat{A} \vect{z} }{2}^{2} \leq (1 + δ) \norm{ \vect{z} }{2}^{2}] for
%   all k-sparse vector \vect{z} \in \C^{ N }. (1) (p. 97)
% - This notion was introduced by Candès and Tao in [3], where
%   it was shown that all s-sparse vectors are recovered as unique solutions of (P_{1}) as soon as δ_{3s} + 3 δ_{4s} < 2. (p. 97)
%   [3] article:CandesITIT2005
% - Candès showed in [2] that s-sparse recovery is guaranteed as soon as δ_{2s} < √2 − 1 ≈ 0.4142. (p. 97)
%   [2] article:CandesCRAS2008
% - The purpose of this note is to show that the threshold on δ_{2s} can be pushed further —
%   we point out that Davies and Gribonval proved that it cannot be pushed further than 1 / √2 ≈ 0.7071 in [4].
%   [4] article:DaviesITIT2009
% article:CandesCRAS2008: The Restricted Isometry Property and Its Implications For Compressed Sensing [May]
% article:CandesSPM2008: An Introduction To Compressive Sampling [Mar.]
the \ac{RIC}
\cite{article:FoucartACHA2010,article:CandesCRAS2008,article:CandesSPM2008}.
% c) evaluation of these measures for a fixed sensing matrix is a computationally-intractable combinatorial problem
% article:TillmannITIT2014: The Computational Complexity of the Restricted Isometry Property, the Nullspace Property, and Related Concepts in Compressed Sensing
% ABSTRACT
% - This paper deals with the computational complexity of conditions which guarantee that
%   the NP-hard problem of finding the sparsest solution to
%   an underdetermined linear system can be solved by efficient algorithms. (p. 1248)
% - The most well-known ones are
%   the MUTUAL COHERENCE,
%   the RESTRICTED ISOMETRY PROPERTY (RIP), and
%   the NULLSPACE PROPERTY (NSP). (p. 1248)
% - While EVALUATING the mutual coherence of a given matrix is easy, it has been suspected for some time that
%   EVALUATING RIP AND NSP IS COMPUTATIONALLY INTRACTABLE IN GENERAL. (p. 1248)
% - We confirm these conjectures by showing that
%   for a given matrix A and positive integer k,
%   COMPUTING THE BEST CONSTANTS FOR WHICH THE RIP OR NSP HOLD IS, IN GENERAL, NP-HARD. (p. 1248)
% - These results are based on the fact that determining the spark of a matrix is NP-hard, which is also established in this paper. (p. 1248)
% I. INTRODUCTION
% - In this paper, we show that
%   it is NP-hard to compute the RIC and NSC of a given matrix with given k; see Sections III and IV, respectively. (p. 1249)
% - Prior to this, in Section II, we prove NP-hardness of computing the spark of a matrix, i.e.,
%   the smallest number of linearly dependent columns. (p. 1249)
% article:JuditskyMP2011: On verifiable sufficient conditions for sparse signal recovery via l1 minimization
% - TOSO: article:JuditskyMP2011
% article:KutyniokGAMM2013: Theory and applications of compressed sensing
% - Ideally, we aim for a matrix which has high spark, low mutual coherence, and a small RIP constant.
% - As our discussion in this section will show, these properties are often quite difficult to achieve, and
%   even computing, for instance, the RIP constant is computationally intractable in general
%   (see article:TillmannITIT2014).
The evaluation of
% 1.) various characteristic measures
these measures for
% 2.) sensing matrix
a fixed sensing matrix
\eqref{eqn:cs_math_prob_general_sensing_matrix}, however, is
% 3.) intractable combinatorial problem
a computationally-intractable combinatorial problem
\cite{article:TillmannITIT2014}.
% d) complexity [evaluation of measures] impedes both the deterministic construction of high-dimensional sensing matrices and the verification of the sufficient conditions
% article:KutyniokGAMM2013: Theory and applications of compressed sensing
% - It is still an OPEN QUESTION (cf. Section 4 for more details) whether
%   DETERMINISTIC MATRICES CAN BE CAREFULLY CONSTRUCTED TO HAVE SIMILAR PROPERTIES with respect to compressed sensing problems.
Its complexity impedes both
% 1.) deterministic construction of high-dimensional sensing matrices
the deterministic construction of
high-dimensional sensing matrices
\eqref{eqn:cs_math_prob_general_sensing_matrix}, whose
characteristic measures meet
the upper bounds, and
% 2.) verification of the sufficient conditions
the verification of
the sufficient conditions.
% e) evaluation of a characteristic measure named worst-case coherence is relatively simple
% book:Foucart2013, Chapter 5: Coherence / Sect. 5.1 Definitions and Basic Properties
% - We start with the definition of the COHERENCE OF A MATRIX. (p. 111)
% - Definition 5.1.
%	- The COHERENCE µ = µ(A) of the matrix A is defined as (5.1). (p. 111)
% book:Foucart2013, Chapter 5: Coherence
% - In compressive sensing, the analysis of recovery algorithms usually involves
%   a QUANTITY THAT MEASURES THE SUITABILITY OF THE MEASUREMENT MATRIX. (p. 111)
% - THE COHERENCE IS A VERY SIMPLE SUCH MEASURE OF QUALITY. (p. 111)
% - In general, THE SMALLER THE COHERENCE, THE BETTER THE RECOVERY ALGORITHMS PERFORM. (p. 111)
% - In Sects. 5.3, 5.4,and 5.5, we give
%   some SUFFICIENT CONDITIONS EXPRESSED IN TERMS OF THE COHERENCE that guarantee
%   the success of orthogonal matching pursuit, basis pursuit, and thresholding algorithms. (p. 111)
% article:KutyniokGAMM2013: Theory and applications of compressed sensing
% Sect. 3: Conditions for Sparse Recovery / Sect. 3.2: Sufficient Conditions / Sect. 3.2.1: Mutual Coherence
% - The MUTUAL COHERENCE OF A MATRIX, initially introduced in [21], measures the smallest angle between each pair of its columns. (p. 88)
%   [21] D.L. Donoho and X. Huo. Uncertainty principles and ideal atomic decomposition. IEEE Trans. Inform. Theory, 47:2845–2862, 2001.
% - The maximal mutual coherence of a matrix certainly equals 1 in the case when two columns are linearly dependent. (p. 88)
In contrast,
% 1.) evaluation
the evaluation of
% 2.) characteristic measure
a characteristic measure named
% 3.) worst-case coherence
worst-case coherence
\cite[Def. 5.1]{book:Foucart2013} is
% 4.) relatively simple
relatively simple.
% f) worst-case coherence loosely bounds from above the RIC and ensures the RIP for s nonzero components, if the number of observations meets M \in \bigomega{ s^{2} }
% article:TillmannITIT2014: The Computational Complexity of the Restricted Isometry Property, the Nullspace Property, and Related Concepts in Compressed Sensing
% - However, the sparsity levels for which the mutual coherence can guarantee recoverability are
%   quite often too small to be of practical use.
% - This emphasizes the importance of other concepts.
% book:Foucart2013, Chapter 6: Restricted Isometry Property / Sect. 6.1 Definitions and Basic Properties
% - As with the coherence, small restricted isometry constants are desired. (p. 133)
% - Proposition 6.2.:
%	- If the matrix A has l2-normalized columns a_{1}, ..., a_{N}, i.e., \norm{ a_{j} }{2} = 1 for all j ∈ [N], then
%	  \delta_{1} = 0, \delta_{2} = \mu, \delta_{s} \leq \mu_{1} (s - 1) \leq \mu (s - 1), s \geq 2. (p. 134)
% book:Foucart2013, Chapter 6: Restricted Isometry Property
% - THE COHERENCE IS A SIMPLE AND USEFUL MEASURE OF THE QUALITY OF A MEASUREMENT MATRIX.
%   However, THE LOWER BOUND ON THE COHERENCE in Theorem 5.7 limits
%   the performance analysis of recovery algorithms to RATHER SMALL SPARSITY LEVELS. (p. 133)
% - A finer measure of the quality of a measurement matrix is needed to overcome this limitation. (p. 133)
% - This is provided by the concept of restricted isometry property, also known as uniform uncertainty principle. (p. 133)
It loosely bounds from above
% 1.) restricted isometry constant (RIC)
the \ac{RIC}
\cite[Prop. 6.2]{book:Foucart2013} and, by
% 2.) Welch lower bound
% article:KutyniokGAMM2013: Theory and applications of compressed sensing
% Sect. 3: Conditions for Sparse Recovery / Sect. 3.2: Sufficient Conditions / Sect. 3.2.1: Mutual Coherence
% - The LOWER BOUND presented in the next result, also KNOWN AS THE WELCH BOUND, is more interesting. (p. 88)
% - Lemma 3.7:
%	- Let A be an m×n matrix. Then we have [\mu(\mat{A}) \in [ \sqrt{ \frac{ n - m }{ m (n - 1) } } ; 1 ]]. (p. 88)
its \name{Welch} lower bound
\cite[Thm. 5.7]{book:Foucart2013},
\cite[Lem. 3.7]{article:KutyniokGAMM2013}, ensures
% 3.) restricted isometry property (RIP)
the \ac{RIP} for
% 4.) s-sparse representations
$s$ nonzero components, if
% 5.) number of observations
the number of
observations meets
% 6.) M \in \bigomega{ s^{2} }
$M \in \bigomega{ s^{2} }$.
% g) certain types of random sensing matrices also obey the RIP with very high probability, if the number of observations is sufficiently large
% book:Foucart2013, Chapter 1: An Invitation to Compressive Sensing / Sect. 1.1: What is Compressive Sensing?
% - A BREAKTHROUGH IS ACHIEVED BY RESORTING TO RANDOM MATRICES - THIS DISCOVERY CAN BE VIEWED AS THE BIRTH OF COMPRESSIVE SENSING. (p. 6)
% article:TroppPIEEE2010: Computational Methods for Sparse Solution of Linear Inverse Problems
% I. INTRODUCTION / C. Verifying Correctness
% - CERTAIN RANDOM MATRICES, HOWEVER, SATISFY MUCH STRONGER RIP BOUNDS WITH HIGH PROBABILITY. (p. 950)
% - This fact explains the BENEFIT OF RANDOMNESS IN COMPRESSIVE SAMPLING. (p. 950)
% - Establishing the RIP for a random matrix requires techniques more sophisticated than
%   the simple coherence arguments; see [14] for discussion. (p. 950)
Fortunately,
certain types of
% 1.) random sensing matrices
random sensing matrices
\eqref{eqn:cs_math_prob_general_sensing_matrix} also obey
the \ac{RIP} with
very high probability, if
the number of
observations is
sufficiently large
\cite[6]{book:Foucart2013},
\cite{article:TroppPIEEE2010}.
% h) realizations of i.i.d. random variables governed by certain distributions as entries and randomly and uniformly chosen scaled rows of a Fourier basis
% coll:Fornasier2015, Sect. 3: Mathematical Modelling and Analysis / RIP for Gaussian and Bernoulli Random Matrices
% - Optimal estimates for the RIP constants in terms of the number m of measurement matrices can be obtained for
%   Gaussian, Bernoulli, or more general subgaussian random matrices. (p. 221)
% book:Foucart2013, Chapter 1: An Invitation to Compressive Sensing / Sect. 1.1: What is Compressive Sensing?
% - Simple examples are
%   GAUSSIAN MATRICES whose entries consist of independent random variables following a standard normal distribution and
%   BERNOULLI MATRICES whose entries are independent random variables taking the values +1 and −1 with equal probability. (p. 6)
% - A key result in compressive sensing states that, with HIGH PROBABILITY on the random draw of an m × N Gaussian or Bernoulli matrix A,
%   all s-sparse vectors x can be reconstructed from y = Ax using a variety of algorithms provided
%   m \geq C s ln( N / s ) (1.3),
%   where C > 0 is a universal constant (independent of s, m, and N). (p. 6)
% - This bound is in fact optimal. (p. 6)
% book:Foucart2013, Chapter 9: Sparse Recovery with Random Matrices / Sect. 9.1: Restricted Isometry Property for Subgaussian Matrices
% - We consider a MATRIX \mat{A} \in \R^{ m \times N } HAVING RANDOM VARIABLES AS THEIR ENTRIES. (p. 272)
% - Such \mat{A} is called a RANDOM MATRIX or random matrix ensemble.
% article:BaraniukCA2008: A Simple Proof of the Restricted Isometry Property for Random Matrices (real-valued CS problem / canonical basis)
% 6 Discussion
% - Furthermore, we prove above that the RIP HOLDS FOR \Phi(ω) WITH HIGH PROBABILITY when
%   the matrix is drawn according to one of the distributions
%   [ \phi_{ i, j } \sim \gaussian{ 0 }{ 1 / n } ] (4.4),
%   [ \phi_{ i, j } := + 1 / \sqrt{n} with probability 0.5; - 1 / \sqrt{n} with probability 0.5 ] (4.5), or
%   [ \phi_{ i, j } := + \sqrt{ 3 / n } with probability 1/6; 0 with probability 2/3; - \sqrt{ 3 / n } with probability 1/6 ] (4.6) []. (p. 261)
% 1.) realizations of i.i.d. random variables governed by certain distributions (Gaussian, Bernoulli) as entries
Realizations of
\ac{IID} random variables governed by
certain distributions, e.g.
Gaussian or
\name{Bernoulli}, as
entries
\cite[Thm. 5.2]{article:BaraniukCA2008}%
\footnote{
  % a) right multiplication of the first reference observation process by any unitary matrix \mat{\Psi} preserved the RIP (universality)
  % coll:Fornasier2015, Sect. 3: Mathematical Modelling and Analysis / Preliminaries and Notation
  % - This EXPOSITION MOSTLY TREATS COMPLEX VECTORS IN \C^{ N } although sometimes
  %   the considerations will be restricted to the real case \R^{ N }. (p. 212)
  % coll:Fornasier2015, Sect. 3: Mathematical Modelling and Analysis / RIP for Gaussian and Bernoulli Random Matrices
  % - It is useful to observe that
  %   the CONCENTRATION INEQUALITY IS INVARIANT UNDER UNITARY TRANSFORMS. (p. 222)
  % - Indeed, suppose that \vect{z} is not sparse with respect to the canonical basis but with respect to
  %   a DIFFERENT ORTHONORMAL BASIS. (p. 222)
  % - Then \vect{z} = \mat{U} \vect{x} for a SPARSE \vect{x} and a UNITARY MATRIX \mat{U} \in \C^{ N \times N }. (p. 222)
  % - Applying the measurement matrix \mat{A} yields [ \mat{A} \vect{z} = \mat{A} \mat{U} \vect{x} ], so that this situation is equivalent to
  %   [1.)] WORKING WITH THE NEW MEASUREMENT MATRIX \mat{A}' = \mat{A} \mat{U} and
  %   [2.)] again SPARSITY WITH RESPECT TO THE CANONICAL BASIS. (p. 222)
  % - The crucial point is that
  %   \mat{A}' SATISFIES AGAIN THE CONCENTRATION INEQUALITY (17) ONCE \mat{A} DOES. (p. 222)
  % - Hence, Theorem 4 also applies to \mat{A}' = \mat{A} \mat{U}. (p. 222)
  % - This fact is sometimes referred to as
  %   THE UNIVERSALITY OF THE GAUSSIAN OR BERNOULLI RANDOM MATRICES. (p. 222)
  % - It does not matter in which basis the signal \vect{x} is actually sparse. (p. 222)
  % - At the coding stage, where one takes random measurements \vect{y} = \mat{A} \vect{z},
  %   knowledge of this basis is not even required. (p. 222)
  % - Only the decoding procedure needs to know \mat{U}. (p. 222)
  % book:Foucart2013, Chapter 9: Sparse Recovery with Random Matrices / Sect. 9.1: Restricted Isometry Property for Subgaussian Matrices / Universality
  % - Often sparsity does not occur with respect to the canonical basis but rather with respect to
  %   SOME OTHER ORTHONORMAL BASIS. (p. 280)
  % - Hence, we consider this model [ \vect{y} = \mat{A} \mat{U} \vect{x}, \vect{x} \in \C^{N} ] with
  %   [1.)] a random m \times N matrix \mat{A} and
  %   [2.)] a fixed (deterministic) ORTHOGONAL MATRIX \mat{U} \in \R^{ N \times N } as
  %   a NEW MEASUREMENT MATRIX OF INTEREST in this context. (p. 280)
  % - Theorem 9.15
  % - In particular,
  %   since the ORTHOGONAL MATRIX \mat{U} IS ARBITRARY in the above theorem,
  %   SPARSE RECOVERY WITH SUBGAUSSIAN MATRICES IS UNIVERSAL WITH RESPECT TO
  %   THE ORTHONORMAL BASIS in which signals are sparse. (p. 281)
  % - It even means that at the encoding stage when measurements \vect{y} = \mat{A} \mat{U} \vect{x} are taken,
  %   the orthogonal matrix \mat{U} does not need to be known. (p. 281)
  % - It is only used at the decoding stage when a recovery algorithm is applied. (p. 281)
  % - The theorem only states that for a FIXED ORTHOGONAL \mat{U},
  %   a random choice of \mat{A} will work well with high probability. (p. 281)
  % article:BaraniukCA2008: A Simple Proof of the Restricted Isometry Property for Random Matrices (real-valued CS problem!)
  % 6 Discussion
  % - However,
  %   we are often interested in signals that
  %   are SPARSE OR COMPRESSIBLE IN SOME ORTHONORMAL BASIS \Psi \neq I, in which case
  %   WE WOULD LIKE THE RIP TO HOLD FOR THE MATRIX \Phi(ω) \Psi. (p. 261)
  % - In this setting it is easy to see that by
  %   choosing our net of points in the k-dimensional subspaces spanned by sets of k columns of \Psi,
  %   THEOREM 5.2 WILL ESTABLISH THE RIP FOR \Phi(ω) \Psi for
  %   EACH OF THE DISTRIBUTIONS (4.4), (4.5), and (4.6). (pp. 261, 262)
  % - This UNIVERSALITY OF \Phi WITH RESPECT TO THE SPARSITY-INDUCING BASIS is an attractive feature that
  %   is known for the Gaussian distribution (4.4) (based on symmetry arguments), but to our knowledge
  %   the UNIVERSALITY of the distributions (4.5) and (4.6) has not been previously shown. (p. 262)
  % - Indeed, we see that
  %   ANY DISTRIBUTION SATISFYING A CONCENTRATION INEQUALITY analogous to that of (4.3) WILL PROVIDE BOTH
  %   [1.)] THE CANONICAL RIP AND
  %   [2.)] THE UNIVERSALITY WITH RESPECT TO \Psi. (p. 262)
  % - More generally, it follows that with high probability such a \Phi will simultaneously satisfy
  %   the RIP with respect to an exponential number of fixed bases. (p. 262)
  The result holds universally under any
  unitary transform, i.e.
  the right multiplication of
  % 1.) random observation process
  a random observation process by
  % 2.) N_{\text{lat}}-dimensional DFT matrix
  any complex-valued unitary
  $N_{\text{lat}} \times N_{\text{lat}}$ matrix preserves
  the \ac{RIP}
  \cite[222]{coll:Fornasier2015},
  %\cite[280, 281]{book:Foucart2013},
  \cite[Sect. 6]{article:BaraniukCA2008}.
} and
% 2.) randomly and uniformly chosen scaled rows of a Fourier basis
% article:TroppPIEEE2010: Computational Methods for Sparse Solution of Linear Inverse Problems
% I. INTRODUCTION / C. Verifying Correctness
% - For GAUSSIAN AND BERNOULLI MATRICES, RIP holds when K \approx m / log( N / m ). (p. 950)
% - For MORE STRUCTURED MATRICES, such as a RANDOM SECTION OF A DISCRETE FOURIER TRANSFORM,
%   RIP often holds when K \approx m / log^{p}( N ) for a small integer p. (p. 950)
% article:RudelsonCPAM2008: On Sparse Reconstruction from Fourier and Gaussian Measurements
%
\TODO{scaled?}
randomly and uniformly chosen scaled rows of
a \name{Fourier} basis
\cite[Thm. 3.3]{article:RudelsonCPAM2008}, for example, require
$M \in \bigomega{ s \ln( N / s ) }$ and
$M \in \bigomega{ s \ln^{4}( N ) }$ observations,
respectively.
% i) orders of growth are significantly better than that guaranteed by the worst-case coherence
These orders of
growth are
almost linear in $s$ and, thus, significantly better than
that guaranteed by
the worst-case coherence.

%---------------------------------------------------------------------------------------------------------------
% 6.) consequences of the underlying physical models for the sensing matrix / random structure
%---------------------------------------------------------------------------------------------------------------
% a) individual entries of the observation process and those of the sensing matrix depend on the underlying physical model
% book:Foucart2013, Chapter 12: Random Sampling in Bounded Orthonormal Systems
% - While this [subgaussian random matrices are optimal] is a very important insight for the theory,
%   THE USE OF SUCH “COMPLETELY RANDOM” MATRICES, WHERE ALL ENTRIES ARE INDEPENDENT,
%   IS LIMITED FOR PRACTICAL PURPOSES. (p. 367)
% - However, structure is important for several reasons:
%	1.) APPLICATIONS MAY IMPOSE CERTAIN STRUCTURE ON THE MEASUREMENT MATRIX DUE TO
%	    PHYSICAL OR OTHER CONSTRAINTS. (p. 367)
% article:KutyniokGAMM2013: Theory and applications of compressed sensing
% - Moreover,
%   MOST APPLICATIONS DO NOT ALLOW FOR A FREE CHOICE OF THE SENSING MATRIX AND
%   ENFORCE A PARTICULARLY STRUCTURED MATRIX.
% - Exemplary situations are
%   the application of data separation, in which
%   the sensing matrix has to consist of two or more orthonormal bases or frames [32, Chapter 11], or
%   HIGH RESOLUTION RADAR, for which
%   THE SENSING MATRIX HAS TO BEAR A PARTICULAR TIME-FREQUENCY STRUCTURE [38].
In
medical imaging,
% 1.) individual entries of the observation process
the individual entries of
the observation process and, in conjunction with
the orthonormal basis,
% 2.) individual entries of the sensing matrix
those of
the sensing matrix
\eqref{eqn:cs_math_prob_general_sensing_matrix} depend on
the underlying physical model.
% b) various imaging parameters controlling the instrumentation enable the systematic manipulation of groups of these entries within technologically and physiologically tolerable limits
% book:Foucart2013, Chapter 12: Random Sampling in Bounded Orthonormal Systems
% - By a STRUCTURED RANDOM MATRIX, we mean a STRUCTURED MATRIX THAT IS GENERATED BY A RANDOM CHOICE OF PARAMETERS. (p. 367)
Various imaging parameters controlling
the instrumentation, however, enable
the systematic manipulation of
groups of
these entries within
technologically and
physiologically tolerable limits.
% c) randomizations of these degrees of freedom [imaging parameters within tolerable limits] generate sensing matrices with random structures
The randomizations of
these degrees of
freedom generate
% 1.) sensing matrices with random structures
sensing matrices
\eqref{eqn:cs_math_prob_general_sensing_matrix} with
random structures that
% 2.) random structures potentially improve the aforementioned characteristic measures
potentially improve
the aforementioned characteristic measures and, consequently, aid in
% 3.) random structures aid in meeting the associated sufficient conditions
meeting the associated sufficient conditions.
% d) degrees of freedom in the physical models underlying MRI and compressed beamforming in UI specify subsets of scaled Fourier coefficients to be processed
In fact,
the degrees of
freedom in
the physical models underlying
% 1.) magnetic resonance imaging (MRI)
% book:Foucart2013, Chapter 1: An Invitation to Compressive Sensing / Sect. 1.2: Applications, Motivations, and Extensions
% Magnetic Resonance Imaging
% - The MEASUREMENT MATRIX A = R_{K} F \in \C^{ m × N } IS A PARTIAL FOURIER MATRIX. (p. 11)
% - In words, the vector y collects the SAMPLES OF THE THREE-DIMENSIONAL FOURIER TRANSFORM OF THE DISCRETIZED IMAGE x ON THE SET K. (p. 11)
% - In the general scenario, the discretized image x will be sparse or compressible only after transforming into a suitable domain,
%   using wavelets, for instance — in mathematical terms, we have x = W x' for some unitary matrix W \in \C^{ N × N } and some sparse vector x' \in \C^{ N }. (p. 11)
% - This leads to the model y = A' x', with
%   the transformed measurement matrix A' = A W = R_{K} F W \in \C^{ m × N } and
%   a sparse or compressible vector x' \in \C^{ N }. (pp. 11, 12)
% - The challenge is to determine good sampling sets K with small size that still ensure recovery of sparse images. (p. 12)
% - The theory currently available predicts that
%   SAMPLING SETS K CHOSEN UNIFORMLY AT RANDOM AMONG ALL POSSIBLE SETS OF CARDINALITY m WORK WELL
%   (at least when W is the identity matrix). (p. 12)
% - Indeed, the results of Chap. 12 guarantee that an s-sparse x' \in C^{ N } can be reconstructed by l1-minimization if m \geq C s ln( N ). (p. 12)
% - Unfortunately, such random sets K are difficult to realize in practice due to the continuity constraints of the trajectories curves k_{1},... ,k_{L}. (p. 12)
% - Therefore, good realizable sets K are investigated empirically. (p. 12)
% - One option that seems to work well takes the trajectories as parallel lines in R^{3} whose intersections with a coordinate plane are chosen uniformly at random. (p. 12)
% - This gives some sort of approximation to the case where K is “completely” random. (p. 12)
\ac{MRI}
\cite[11, 12]{book:Foucart2013},
\cite{article:LustigMRM2007} and
% 2.) compressed beamforming in UI
% proc:SchiffnerIUS2016a: A low-rate parallel Fourier domain beamforming method for ultrafast pulse-echo imaging
% - Equating the Fourier coefficients of (11) with respect to the interval (8) with (9b) yields
%   the N_{ω,rnd} × M linear system (12) for all ν ∈ V_{LP,rnd}. (p. 3)
% article:BurshteinITUFFC2016: Sub-Nyquist Sampling and Fourier Domain Beamforming in Volumetric Ultrasound Imaging
% IV. RECOVERY FROM SUB-NYQUIST SAMPLES
% - In vector–matrix notation, (19) [Fourier coefficients of the beamformed signal] can be rewritten as
%   [c = H D b = A b] (20) where
%   c is a vector of length K with kth entry c[k],
%   H is a K × K diagonal matrix with kth entry h[k],
%   D is a K × N matrix whose rows are taken from the N × N discrete fourier transform (DFT) matrix corresponding to
%   the relevant Fourier indices of Φ( t; θ_{x}, θ_{y} ), and
%   b is a column vector of length N with lth entry b_{l}. (p. 708)
% article:ChernyakovaITUFFC2014: Fourier-Domain Beamforming: The Path to Compressed Ultrasound Imaging
% V. Further Reduction Through Compressed Sensing / A. Parametric Representation
% - We now recast the problem in vector-matrix notation.
% - Defining a length-M measurement vector c with kth entry c[k], k ∈ µ, we can rewrite (17) as
%   [c = H D b = A b] (19), where
%   H is an M × M diagonal matrix with h[k] as its kth entry,
%   D is an M × N matrix formed by taking the set µ of rows from an N × N Fourier matrix, and
%   b is a length-N vector with lth entry bl. (p. 1260)
% - Our goal is to determine b from c. (p. 1260)
% article:ChernyakovaITUFFC2014: Fourier-Domain Beamforming: The Path to Compressed Ultrasound Imaging
% V. Further Reduction Through Compressed Sensing / B. Prior Work
% - IN OUR CASE, A, defined in (19), IS FORMED BY TAKING K SCALED [!] ROWS FROM AN N × N FOURIER MATRIX. (p. 1261)
% - It can be shown that by choosing K ≥ C L (log N)^{4} rows uniformly at random for some positive constant C,
%   THE MEASUREMENT MATRIX A OBEYS THE RIP WITH HIGH PROBABILITY [31] [article:RudelsonCPAM2008]. (p. 1261)
% article:WagnerITSP2012: Compressed Beamforming in Ultrasound Imaging
% VII. SIGNAL RECONSTRUCTION / B. CS Approach for Signal Reconstruction Throughout
% - Then (39) may be expressed in the following matrix form:
%   [c \approx H \hat{V} x / T = A x] (40)
%   where H is the K x K diagonal matrix H( 2 \pi k_{j} / T ) as its jth diagonal element, and
%   x is a length N vector, whose jth element equals b_{l} for j = q_{l}, and 0 otherwise. (p. 4652)
% - Finally, \hat{V} is a K x N matrix, formed by taking the set \kappa of rows from an N x N DFT matrix. (p. 4652)
% - The formulation obtained in (40), is a classic CS problem, where our goal is to reconstruct
%   the N-dimensional vector x, known to be L-sparse, with L << N, based on
%   its projection onto a subset of K orthogonal vectors, represented by the rows of A. (p. 4652)
% - IN OUR CASE, A IS FORMED BY CHOOSING K [sic! -> scaled] ROWS FROM THE FOURIER BASIS. (p. 4652)
% - Selecting these rows uniformly at random it may be shown that if
%   [K \geq C L (log N)^{4}] (41) for some positive constant C, then
%   A obeys the RIP with large probability [22] [article:RudelsonCPAM2008]. (p. 4652)
% 2.) compressed beamforming
compressed beamforming in
\ac{UI}
\cite{article:ChernyakovaITUFFC2018,proc:SchiffnerIUS2016a,article:BurshteinITUFFC2016,article:ChernyakovaITUFFC2014,article:WagnerITSP2012}, for example, specify
subsets of
scaled \name{Fourier} coefficients to
be processed.
% e) random and uniform selection [Fourier coefficients] generates the aforementioned random sensing matrix meeting the RIP with relatively few observations
Their random and uniform selection generates
the aforementioned random sensing matrix meeting
the \ac{RIP} with
relatively few observations.
% f) introduction of diagonal weighting matrices into the underdetermined linear algebraic system always enables the l2-normalization of the sensing matrix's column vectors
In addition,
the introduction of
% 1.) diagonal weighting matrices
diagonal weighting matrices, whose
entries equal
% 2.) l2-norms of the sensing matrix's column vectors
the $\ell_{2}$-norms of
the sensing matrix's column vectors
$\tnorm{ \vect{a}_{n} }{2}$ or
% 3.) reciprocal l2-norms of the sensing matrix's column vectors
their reciprocals, into
% 4.) underdetermined linear algebraic system
the underdetermined linear algebraic system
\eqref{eqn:cs_math_prob_general_obs_trans_coef_error} always enables
% 5.) l2-normalization of the sensing matrix's column vectors
the $\ell_{2}$-normalization of
the sensing matrix's column vectors without violating
% 6.) mathematical equivalence
the mathematical equivalence.
% g) resulting normalized sensing matrix minimizes both the RIR and the RIC and meets the associated sufficient conditions
% book:Foucart2013, Sect. 6.1 Definitions and Basic Properties
% - As with the coherence, small restricted isometry constants are desired. (p. 133)
% - Proposition 6.2.:
%	- If the matrix A has L2-NORMALIZED COLUMNS a_{1}, ..., a_{N}, i.e., \norm{ a_{j} }{2} = 1 for all j ∈ [N], then
%	  \delta_{1} = 0, \delta_{2} = \mu, \delta_{s} \leq \mu_{1} (s - 1) \leq \mu (s - 1), s \geq 2. (p. 134)
% article:KutyniokGAMM2013: Theory and applications of compressed sensing
% - Ideally, we aim for a matrix which has high spark, low mutual coherence, and a small RIP constant.
The resulting normalized sensing matrix minimizes both
% 1.) restricted isometry ratio
the restricted isometry ratio and
% 2.) restricted isometry constant (RIC)
the \ac{RIC} for
% 3.) nearly-sparse representations
$1$-sparse representations
\eqref{eqn:def_transform_coefficients} and better conforms with
% 4.) associated sufficient conditions
%\TODO{really? minimum $2s = 2$?}
the associated sufficient conditions
\cite[Prop. 6.2]{book:Foucart2013}.

%---------------------------------------------------------------------------------------------------------------
% 7.) properties and significance of the transform point spread function (TPSF)
%---------------------------------------------------------------------------------------------------------------
% a) TPSF frequently quantifies the coherence of the sensing matrices in medical imaging
% article:ProvostITMI2009: The Application of Compressed Sensing for Photo-Acoustic Tomography
% VI. COMPRESSED SENSING / B. PA Forward Operator as a CS-Matrix
% - Therefore, columns being linearly independent implies that
%   two different basis vectors output two different sets of measurements. (p. 588)
% - At the same time, linear combinations of columns being noise-like implies that
%   one measurement of a basis vector is NOT CORRELATED to the measurement of another basis vector. (pp. 588, 589)
% - These two conditions will be verified using the POINT SPREAD TRANSFORM (TPSF) introduced in [20]. (p. 589)
%   [20] article:LustigMRM2007
% article:LustigMRM2007: Sparse MRI: The application of compressed sensing for rapid MR imaging
% THEORY / Point Spread Function and Transform Point Spread Function Analysis
% - The POINT SPREAD FUNCTION (PSF) is a NATURAL TOOL TO MEASURE INCOHERENCE. (p. 1185)
% - The MR IMAGES OF INTEREST ARE TYPICALLY SPARSE IN A TRANSFORM DOMAIN rather than the usual image domain. (p. 1185)
% - In such a setting [sparsity in a transform domain],
%   INCOHERENCE IS ANALYZED BY GENERALIZING THE NOTION OF PSF TO TRANSFORM POINT SPREAD FUNCTION (TPSF) which measures how
%   a SINGLE TRANSFORM COEFFICIENT of the underlying object ends up INFLUENCING
%   OTHER TRANSFORM COEFFICIENTS of the measured undersampled object. (p. 1185)
The \ac{TPSF} frequently quantifies
% 1.) coherence
the coherence of
% 2.) sensing matrices
the sensing matrices
\eqref{eqn:cs_math_prob_general_sensing_matrix} in
% 3.) medical imaging
medical imaging
(cf. e.g.
\cite{article:ProvostITMI2009,article:LustigMRM2007}%
).
% b) TPSF equals the mutual correlation coefficient of the column vectors
% article:ProvostITMI2009: The Application of Compressed Sensing for Photo-Acoustic Tomography
% VI. COMPRESSED SENSING / B. PA Forward Operator as a CS-Matrix
% - The TPSF is defined as (23). (p. 589)
% article:LustigMRM2007: Sparse MRI: The application of compressed sensing for rapid MR imaging
% THEORY / Point Spread Function and Transform Point Spread Function Analysis
% - Let \Psi be an orthogonal sparsifying transform (nonorthogonal TPSF analysis is beyond our scope and is not discussed here). (p. 1185)
% - The TPSF(i; j) IS GIVEN BY THE FOLLOWING EQUATION,
%   [ TPSF(i; j) = e_{j}^{∗} \Psi F_{u}^{∗} F_{u} \Psi^{*} e_{i} ]. [2] (p. 1185)
It equals
% 1.) mutual correlation coefficient
the mutual correlation coefficient of
% 2.) column vectors
the column vectors given by
\cite[(23)]{article:ProvostITMI2009},
\cite[(2)]{article:LustigMRM2007}
\begin{equation}
 %--------------------------------------------------------------------------------------------------------------
 % transform point spread function (TPSF)
 %--------------------------------------------------------------------------------------------------------------
  \tpsf{ \mat{A} }{ n_{1} }{ n_{2} }
  =
  \frac{
    \inprod{ \vect{a}_{ n_{1} } }{ \vect{a}_{ n_{2} } }
  }{
    \norm{ \vect{a}_{ n_{1} } }{2}
    \norm{ \vect{a}_{ n_{2} } }{2}
  }
 \label{eqn:cs_math_tpsf}
\end{equation}
for
% 3.) all pairs of indices
all $( n_{1}, n_{2} ) \in \setcons{ N }^{2}$.
% c) TPSF reduces to the PSF and exclusively quantifies the coherence of the observation process
% article:LustigMRM2007: Sparse MRI: The application of compressed sensing for rapid MR imaging
% THEORY / Point Spread Function and Transform Point Spread Function Analysis
% - Let F_{u} be the UNDERSAMPLED FOURIER OPERATOR and let e_{i} be the ith vector of the NATURAL BASIS
%   (i.e, having “1” at the ith location and zeroes elsewhere). (p. 1185)
% - Then PSF(i; j) = e_{j}^{∗} F_{u}^{∗} F_{u} e_{i} measures
%   the CONTRIBUTION OF A UNIT-INTENSITY PIXEL AT THE iTH POSITION TO A PIXEL AT THE jTH POSITION. (p. 1185)
% - Under Nyquist sampling there is no interference between pixels and PSF(i; j) | i \neq j = 0. (p. 1185)
% - UNDERSAMPLING CAUSES PIXELS TO INTERFERE and PSF(i; j) | i \neq j TO ASSUME NONZERO VALUES. (p. 1185)
% - The PSF of pure 2D random sampling, where samples are chosen at random from a Cartesian grid, offers a standard for comparison. (p. 1185)
%	-> Empirically, the real and the imaginary parts separately behave much like zero-mean random white Gaussian noise. (p. 1185)
%	-> The standard deviation of the observed SPR depends on the number, N, of samples taken and the number, D, of grid points defining the underlying image. (p. 1185)
If
% 1.) orthonormal basis
the orthonormal basis is
% 2.) canonical
canonical, i.e.
$\mat{\Psi} = \mat{I}$,
% 3.) transform point spread function (TPSF)
the \ac{TPSF}
\eqref{eqn:cs_math_tpsf} reduces to
% 4.) point spread function (PSF)
the \ac{PSF} and exclusively quantifies
% 5.) coherence
the coherence of
% 6.) observation process
the observation process
\cite{article:LustigMRM2007}.
% d) TPSF trivially attains its maximum absolute value of unity
For
% 1.) n_{1} = n_{2}
$n_{1} = n_{2}$,
% 2.) both column vectors match
both column vectors match, and
% 3.) transform point spread function (TPSF)
the \ac{TPSF}
\eqref{eqn:cs_math_tpsf} trivially attains
% 4.) maximum absolute value
its maximum absolute value of
% 5.) unity
unity.
% e) absolute value of the TPSF ideally approaches zero with noise-like statistics
% article:ProvostITMI2009: The Application of Compressed Sensing for Photo-Acoustic Tomography
% VI. COMPRESSED SENSING / B. PA Forward Operator as a CS-Matrix
% - We would like this TPSF to have the FOLLOWING PROPERTIES for m \neq n.
%	1.) First, TPSF(m, n) SHOULD BE MUCH SMALLER THAN 1 [absolute value].
%	    -> This property characterizes the fact that two different basis vectors output different measurements. (p. 589)
%	2.) Second, TPSF(m, n) SHOULD BE noise-like to characterize the fact that
%	    one measurement of a basis vector is not correlated to the measurement of another basis vector. (p. 589)
% article:LustigMRM2007: Sparse MRI: The application of compressed sensing for rapid MR imaging
% THEORY / Point Spread Function and Transform Point Spread Function Analysis
% - WE WOULD LIKE TPSF(i; j) | i \neq j TO BE AS SMALL AS POSSIBLE, AND HAVE RANDOM NOISE-LIKE STATISTICS. (p. 1185)
For
% 1.) n_{1} \neq n_{2}
$n_{1} \neq n_{2}$, however,
% 2.) both column vectors typically differ
both column vectors typically differ, and
% 3.) absolute value
the absolute value of
% 4.) transform point spread function (TPSF)
the \ac{TPSF}
\eqref{eqn:cs_math_tpsf} ideally approaches
% 5.) zero
zero with
% 6.) noise-like statistics
noise-like statistics
\cite{article:ProvostITMI2009,article:LustigMRM2007}.
% f) properties indicate the reliable discrimination of the admissible structural building blocks by the observation process and guide the sparsity-promoting lq-minimization method
These properties, which are referred to as
% 1.) incoherent aliasing
incoherent aliasing, indicate
% 2.) reliable discrimination
the reliable discrimination of
% 3.) admissible structural building blocks
the admissible structural building blocks by
% 4.) observation process
the observation process and guide
% 5.) sparsity-promoting lq-minimization method
the sparsity-promoting $\ell_{q}$-minimization method
\eqref{eqn:cs_lq_minimization}.
% g) practical evaluations of the TPSF typically fix the second index according to the expected support of the nearly-sparse representation
% article:ProvostITMI2009: The Application of Compressed Sensing for Photo-Acoustic Tomography
% VI. COMPRESSED SENSING / B. PA Forward Operator as a CS-Matrix
% - Fig. 1 shows TPSFs evaluated for ONE BASIS COEFFICIENT m and all n's: TPSF(m, n). (p. 589)
% article:LustigMRM2007: Sparse MRI: The application of compressed sensing for rapid MR imaging
% THEORY / Point Spread Function and Transform Point Spread Function Analysis
% - In this case PSF(i; j) | i \neq j looks random as illustrated in Fig. 4a.
% - An example [TPSF] using an orthogonal wavelet transform is illustrated by Fig. 4b. (p. 1185)
% article:LustigMRM2007: Sparse MRI: The application of compressed sensing for rapid MR imaging
% RESULTS / Multislice Fast Spin-Echo Brain Imaging
% - The reason is that some of the coarse-scale wavelet components in these reconstructions were not recovered correctly because of
%   the large peak interference of coarse-scale components that was documented in the TPSF theoretical analysis (see Fig. 5a). (p. 1192)
% - This is because the theoretical TPSF peak interference in such sampling scheme is significantly smaller (see Fig. 5b), which enables
%   better recovery of these components. (p. 1192)
Owing to
% 1.) high dimensionality
the high dimensionality of
% 2.) sensing matrices
the sensing matrices
\eqref{eqn:cs_math_prob_general_sensing_matrix},
% 3.) practical evaluations
practical evaluations of
% 4.) transform point spread function (TPSF)
the \ac{TPSF}
\eqref{eqn:cs_math_tpsf} usually select
% 5.) one index
one index from
% 6.) expected support
the expected support of
% 7.) nearly-sparse representation
the nearly-sparse representation
\eqref{eqn:def_transform_coefficients}, i.e.
$n_{2} \in \supp( \vectsym{\theta} )$
(cf. e.g.
\cite[Fig. 1]{article:ProvostITMI2009},
\cite[Figs. 4 and 5]{article:LustigMRM2007}%
).

% article:LipworthJOSAA2013: Metamaterial apertures for coherent computational imaging on the physical layer
% 3. MEASUREMENT MATRIX AND FIGURES OF MERIT
% - For this reason [RIP and StRIP cannot be met in practice] we turn to
%   a MORE EMPIRICAL MEASURE of the ability of a matrix to reconstruct sparse signals. (p. 1606)
% - Duarte-Carvajalino and Sapiro [25], inspired by the work of Elad [26], proposed
%   A METRIC SUITABLE FOR DETERMINISTIC MATRICES based on
%   the OFF-DIAGONAL ELEMENTS OF THE GRAM MATRIX
%   [ G = \tilde{H}^{T} \tilde{H} ], (20)
%   where \tilde{H} is the measurement matrix H with normalized columns
%   (H is actually the “effective” sensing/dictionary matrix, defined as the product of the sensing matrix and the sparsifying dictionary [25]). (p. 1606)
%   [25] J. M. Duarte-Carvajalino and G. Sapiro, “Learning to sense sparse signals: simultaneous sensing matrix and sparsifying dictionary optimization,” IEEE Trans. Image Process. 18, 1395–1408 (2009).
%   article:Duarte-CarvajalinoITIP2009
%   [26] M. Elad, “Optimized projections for compressed sensing,” IEEE Trans. Signal Process. 55, 5695–5702 (2007).
% - The matrix reconstruction metric is the AVERAGE MUTUAL COHERENCE
%   [ \mu_{g} = \sum_{ i \neq j } \abs{ G_{i,j} }^{2} / ( M ( M - 1 ) ) ] (21)
%   which was empirically shown to be
%   PROPORTIONAL TO MEAN-SQUARED-ERROR (MSE) VALUES FOR RECONSTRUCTIONS [25], calculated using
%   the rasterized scene and its approximation \hat{f} according to (22). (p. 1606)
