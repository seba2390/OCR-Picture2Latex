%---------------------------------------------------------------------------------------------------------------
% 1.) significance of the left multiplications by the adjoint normalized sensing matrices
%---------------------------------------------------------------------------------------------------------------
% a) left multiplications of the normalized recorded RF voltage signals by the adjoint normalized sensing matrices qualitatively recovered the normalized nearly-sparse representations
% article:Schiffner2018, Sect. VI. Implementation / Sect. VI-C. Sparsity-Promoting lq-Minimization Method
% - \ac{SPGL1} is iterative and LEFT MULTIPLIED a sequence of recursively-generated vectors by
%   the potentially densely-populated NORMALIZED SENSING MATRIX \eqref{eqn:recon_reg_norm_sensing_matrix} OR ITS ADJOINT.
The left multiplications of
% 1.) normalized linear algebraic system (all pulse-echo measurements, multifrequent, all array elements, additive errors)
the normalized recorded \ac{RF} voltage signals
\eqref{eqn:recovery_reg_norm_obs_trans_coef_error} by
% 2.) adjoint normalized sensing matrices (all pulse-echo measurements, multifrequent, all array elements)
the adjoint normalized sensing matrices
\eqref{eqn:recon_reg_norm_sensing_matrix} initialized
% 3.) SPGL1
\ac{SPGL1} and qualitatively recovered
% 4.) normalized nearly-sparse representations
the normalized nearly-sparse representations
\eqref{eqn:recon_reg_norm_trans_coef}
(cf. \cref{%
  fig:sim_study_obj_A_adj_1_images_kap,%
  fig:sim_study_obj_B_adj_1_dft_kap%
}).
% b) left multiplications linearly combined the TPSFs for all n_{1} \in \supp[ \vectsym{\theta}^{(\kappa)} ] and added errors
They linearly combined
% 1.) transform point spread functions (TPSFs)
the \acp{TPSF}
\eqref{eqn:cs_math_tpsf} for
% 2.) all supporting indices
all $n_{1} \in \supp[ \vectsym{\theta}^{(\kappa)} ]$ and added
% 3.) projections of the additive errors
errors, as shown in
\eqref{eqn:app_adjoint_tpsf}
(cf. Appendix \ref{app:adjoint}).
% c) left multiplications assigned linear combinations of the zero-lag cross-correlations to the components and significantly enhanced the popular DAS method
Equivalently,
they assigned
% 1.) linear combinations
linear combinations of
% 2.) zero-lag time-domain cross-correlations
the zero-lag cross-correlations
\eqref{eqn:app_adjoint_xcorr_td} between
% 3.) recorded RF voltage signals
the recorded \ac{RF} voltage signals
\eqref{eqn:recovery_disc_freq_v_rx_Fourier_series} and
% 4.) pulse echoes
the pulse echoes of
% 5.) admissible structural building blocks
the admissible structural building blocks to
% 6.) components
the components and, thus, significantly enhanced
% 7.) popular DAS method
the popular \ac{DAS} method.
% d) popular DAS method combines the canonical basis with the approximation of all pulse echoes by delayed Dirac delta distributions
% article:BessonITUFFC2018: Ultrafast Ultrasound Imaging as an Inverse Problem: Matrix-Free Sparse Image Reconstruction
% II. PARAMETRIC MATRIX-FREE FORMULATIONS OF THE MEASUREMENT MODEL AND ITS ADJOINT / B. Adjoint of the Proposed Measurement Model and Its Relationship With the Delay-and-Sum Algorithm
% - In light of (11), it is demonstrated that
%   DAS ACHIEVES A BACKPROJECTION SOLUTION OF THE INVERSE PROBLEM when
%   [1.)] the EFFECT OF THE PULSE-ECHO WAVEFORM IS NEGLECTED (u considered as a Dirac) and when
%   [2.)] the APODIZATION WEIGHTS a( \vect{r}, x_{t} ) are set to be equal to o_{d}( \vect{r}, x_{t} ). (p. 341)
% article:DavidJASA2015: Time domain compressive beam forming of ultrasound signals
% IV. TIME DOMAIN 2D COMPRESSED BEAM FORMING / A. 2D beam forming matrix / 2. Relationship with DAS
% - Adding that term back in Eq. (24), we find that DAS is equivalent to
%   [ D = G^{T} R ] (25) which involves
%   the same matrix G as we are using in the t-CBF framework. (p. 2779)
% - Subsequently, G can be interpreted as a BEAM FORMING MATRIX. (p. 2779)
% - The final image D is obtained by SUCCESSIVE PROJECTIONS OF THE RAW DATA ON THE COLUMNS OF DICTIONARY G. (p. 2779)
% - Equations (20) and (25) together define
%   A DIRECT RELATIONSHIP BETWEEN THE DAS IMAGE D AND THE SCATTERER DISTRIBUTION I:
%   [ D = G^{T} G I ]. (26) (p. 2779)
% - From this, we can sense the IMPORTANCE OF THE MATRIX G^{T} G:
%   it LINKS THE SCATTERER DISTRIBUTION TO THE FINAL DAS IMAGE and can therefore be seen as a Point Spread Function (PSF) of the acquisition system. (p. 2779)
% - As we will see further in the development
%   the mutual coherence of that matrix, that accounts for the similarity of its column vectors, is of tremendous importance in
%   the resolution achievable in both DAS and t-CBF frameworks. (p. 2779)
% - Equation (26) also shows that we should expect the DAS and the t-CBF images to be different in nature. (p. 2779)
\TODO{no temporal discretization!}
In fact,
% 1.) popular DAS method
the latter combines
% 2.) canonical basis
the canonical basis with
% 3.) approximation
the approximation of
% 4.) pulse echoes
all pulse echoes by
% 5.) delayed Dirac delta distributions
delayed \name{Dirac} delta distributions
\cite[Sect. IV.A.2]{article:DavidJASA2015}.

%---------------------------------------------------------------------------------------------------------------
% 2.) proposed method enabled the quantitative recovery / convex l1-minimization
%---------------------------------------------------------------------------------------------------------------
% a) proposed method enabled the quantitative recovery of the specified compressibility fluctuations via the sparsity-promoting lq-minimization method
% article:Schiffner2018, Sect. V. Image Recovery Based on Compressed Sensing
% - The proposed method circumvents this difficulty [ no direct solution ] by
%   REFORMULATING THE DISCRETIZED LINEAR \ac{ISP} AS AN INSTANCE OF
%   THE \ac{CS} PROBLEM \eqref{eqn:cs_math_prob_general}.
% - Postulating the existence of a nearly-sparse representation of the discretized compressibility fluctuations in
%   a known orthonormal basis \eqref{eqn:def_transform_coefficients},
%   the sparsity-promoting $\ell_{q}$-minimization method \eqref{eqn:cs_lq_minimization} ensures its stable recovery if
%   the sensing matrix \eqref{eqn:cs_math_prob_general_sensing_matrix} meets one of the sufficient conditions
%   (cf. \cref{sec:compressed_sensing}).
The proposed method, in contrast, enabled
% 1.) quantitative recovery
the quantitative recovery of
% 2.) specified vectors stacking the regular samples in the discretized relative spatial fluctuations in the unperturbed compressibility
the specified compressibility fluctuations
\eqref{eqn:recovery_sys_lin_eq_gamma_kappa_bp_vector} via
% 3.) sparsity-promoting lq-minimization method
the sparsity-promoting $\ell_{q}$-minimization method
\eqref{eqn:recovery_reg_norm_lq_minimization}.
% b) coherent sidelobes and the secondary maxima in the TPSFs induced by the QPW caused artifacts
% article:LustigMRM2007: Sparse MRI: The application of compressed sensing for rapid MR imaging
% THEORY / A Simple, Intuitive Example of Compressed Sensing
% - To get intuition for the importance of incoherence and the feasibility of CS in MRI, consider the example in Fig. 2. (p. 1183)
% - An intuitive plausible recovery procedure is illustrated in Fig. 2e – h. (p. 1184)
% - It is based on
%   [1.)] THRESHOLDING,
%   [2.)] RECOVERING THE STRONG COMPONENTS, and
%   [3.)] CALCULATING THE INTERFERENCE CAUSED BY THEM AND SUBTRACTING IT. (p. 1184)
% - Subtracting the interference of the strong components
%   [1.)] reduces the total interference level and
%   [2.)] enables recovery of weaker, previously submerged components. (p. 1184)
% - By iteratively repeating this procedure, one can recover the rest of the signal components. (p. 1184)
% - A recovery procedure along these lines was proposed by Donoho et al.
%   (Sparse Solution of Underdetermined Linear Equations by Stagewise Orthogonal Matching Pursuit, 2006, Stanford University, Statistics Department, technical report #2006-02) as
%   a fast approximate algorithm for CS reconstruction. (p. 1184)
% - A similar approach of recovery of MR images was proposed in Ref. (22) (p. 1184)
Since
% 1.) identification
the identification of
% 2.) significant components
the significant components in
% 3.) normalized nearly-sparse representations
the normalized nearly-sparse representations
\eqref{eqn:recon_reg_norm_trans_coef} essentially thresholded
% 4.) aforementioned products
the aforementioned products
\cite[Fig. 2]{article:LustigMRM2007}, however,
% 5.) coherent sidelobes
the coherent sidelobes and
% 6.) secondary maxima
the secondary maxima in
% 7.) transform point spread functions (TPSFs)
the \acp{TPSF}
\eqref{eqn:cs_math_tpsf} induced by
% 8.) quasi-plane wave (QPW)
the \ac{QPW}
(cf. \cref{%
  fig:sim_study_obj_A_sr_1_tpsf_images_qpw_1,%
  fig:sim_study_obj_B_sr_1_tpsf_images_qpw_1%
}) caused
% 9.) artifacts
artifacts for
% 10.) sufficiently large additive errors
sufficiently large additive errors
(cf. \cref{%
  fig:sim_study_obj_A_sr_1_images_spgl1_l1_qpw,%
  fig:sim_study_obj_B_sr_1_dft_images_spgl1_l1_qpw,%
  fig:sim_study_obj_B_sr_1_dft_images_spgl1_lq_qpw%
}).
% c) FEHMs increased the numbers of components within the illustrated dynamic range
The \acp{FEHM} for
% 1.) wire phantom
the wire phantom, which significantly exceeded
% 2.) size of a volume element
the size of
a volume element
(cf. \cref{tab:sim_study_obj_A_sr_1_tpsf_fehm}), increased
% 3.) numbers of components within the illustrated dynamic range
the numbers of
components within
the illustrated dynamic range and, thus, the relative \acp{RMSE} for
% 4.) convex l1-minimization method
the convex $\ell_{1}$-minimization method
\eqreflqmin{eqn:recovery_reg_norm_lq_minimization}{ 1 }
(cf. \cref{%
  fig:sim_study_obj_A_sr_1_quality_vs_snr_spgl1_l1_qpw,%
  fig:sim_study_obj_A_sr_1_quality_vs_snr_spgl1_l1_rnd_apo,%
  fig:sim_study_obj_A_sr_1_quality_vs_snr_spgl1_l1_rnd_del,%
  fig:sim_study_obj_A_sr_1_quality_vs_snr_spgl1_l1_rnd_apo_del%
}).
\TODO{special case: QPW for 30 dB}
% TODO: perfect recovery of wire phantom by QPW for SNR 30 dB ?
% d) secondary maxima prevented this reduction for the QPW
Although
% 1.) tissue-mimicking phantom
the tissue-mimicking phantom reduced
% 2.) ratio FEHMs vs.normalized spatial frequency element
this ratio
(cf. \cref{tab:sim_study_obj_B_sr_1_tpsf_fehm}) and, thus, the relative \acp{RMSE} for
% 3.) random waves
the random waves
(cf. \cref{%
  fig:sim_study_obj_B_sr_1_quality_vs_snr_spgl1_l1_rnd_apo,%
  fig:sim_study_obj_B_sr_1_quality_vs_snr_spgl1_l1_rnd_del,%
  fig:sim_study_obj_B_sr_1_quality_vs_snr_spgl1_l1_rnd_apo_del%
}),
% 4.) secondary maxima
the secondary maxima prevented
this reduction for
% 5.) quasi-plane wave (QPW)
the \ac{QPW}
(cf. \cref{fig:sim_study_obj_B_sr_1_quality_vs_snr_spgl1_l1_qpw}).
% e) mean SSIM indices confirmed the excellent recovery of the object's structure exemplified in previous figure
The mean \ac{SSIM} indices, however, confirmed
% 1.) excellent structural recovery
the excellent structural recovery of both
% 2.) both phantoms
phantoms by
% 3.) random waves
the random waves.

%---------------------------------------------------------------------------------------------------------------
% 3.) superiority of the nonconvex l0.5-minimization method to the convex l1-minimization method
%---------------------------------------------------------------------------------------------------------------
% a) consistent improvements of all quality metrics by the nonconvex l0.5-minimization method indicate its superiority to the convex l1-minimization method for the specified compressibility fluctuations
The consistent improvements of
% 1.) mean SSIM indices, relative RMSEs, and number of components within the illustrated dynamic range
all quality metrics by
% 2.) nonconvex l0.5-minimization method
the nonconvex $\ell_{0.5}$-minimization method
\eqreflqmin{eqn:recovery_reg_norm_lq_minimization}{ 0.5 }
(cf. \cref{%
  fig:sim_study_obj_A_sr_1_ssim_index_rel_rmse_N_iter_vs_snr_kap,%
  fig:sim_study_obj_B_sr_1_ssim_index_rel_rmse_N_iter_vs_snr_kap%
}) indicate
% 3.) superiority
its superiority to
% 4.) convex l1-minimization method
the convex $\ell_{1}$-minimization method
\eqreflqmin{eqn:recovery_reg_norm_lq_minimization}{ 1 } for
% 5.) specified vectors stacking the regular samples in the discretized relative spatial fluctuations in the unperturbed compressibility
the specified compressibility fluctuations
\eqref{eqn:recovery_sys_lin_eq_gamma_kappa_bp_vector}.
% b) larger numbers of iterations increase the computational costs and the recovery times
% article:Schiffner2018, Sect. VII. Simulation Study / Sect. VII-A. Parameters / Sect. VII-A.8) Regularization (subsubsec:sim_study_params_regularization)
% - Since \ac{SPGL1} provided the initial guess,
%   \name{Foucart}'s ALGORITHM ENTAILED SIX $\ell_{1}$ MINIMIZATIONS.
% article:FoucartACHA2009: Sparsest solutions of underdetermined linear systems via lq-minimization for 0 < q \leq 1
% 5. Numerical experiments
% - This improvement is obtained at a default cost of 4 times a 10-iteration reweighted l1-minimization, i.e.
%   at a cost of 40 times an l1-minimization. (p. 406)
% article:ChartrandISPL2007: Exact Reconstruction of Sparse Signals via Nonconvex Minimization
% IV. CONCLUSIONS
% - The REQUIRED RECONSTRUCTION TIME IS GENERALLY LONGER THAN WITH p = 1 but much less than with p = 0. (p. 710)
The larger numbers of
iterations, which arose from
% 1.) six executions of SPGL1
the six executions of
\ac{SPGL1}, however, increase
% 2.) computational costs
the computational costs and
% 3.) recovery times
the recovery times.
% c) findings agree with those derived from numerical experiments in the literature
% article:FoucartACHA2009: Sparsest solutions of underdetermined linear systems via lq-minimization for 0 < q \leq 1
% ABSTRACT
% - Finally, we display the RESULTS OF SOME EXPERIMENTS which indicate that
%   the lq-METHOD PERFORMS BETTER THAN OTHER AVAILABLE METHODS. (p. 395)
% 1. Introduction
% - Finally, we compare in Section 5 our lq-ALGORITHM with four existing methods:
%   [1.)] the orthogonal greedy algorithm, see e.g. [13],
%   [2.)] the regularized orthogonal matching pursuit, see [12],
%   [3.)] the l1-minimization, and
%   [4.)] the reweighted l1-minimization, see [6]. (p. 396)
% - The last two, as well as our lq-algorithm, use the l1-magic software available on Candès’ web page. (p. 396)
% - It comes as a small surprise that THE lq-METHOD PERFORMS BEST. (p. 396)
% 5. Numerical experiments
% - We compare in this section the algorithm described in Section 4 with four other existing algorithms, namely
%   [1.)] the orthogonal greedy algorithm (OGA, see [13]),
%   [2.)] the regularized orthogonal matching pursuit (ROMP, see [12]),
%   [3.)] the l1-minimization (L1), and
%   [4.)] the reweighted l1-minimization (RWL1, see [6]). (p. 404)
% - In our FIRST EXPERIMENT, we JUSTIFY the values attributed by DEFAULT to
%   [(i)] the number of iterations n, [(ii)] the exponent q, and [(iii)] the sequence (\epsilon_{k}) in our lq-algorithm. (p. 404)
% 	- The choice is based on the computations summarized in Figs. 1, 2, and 3. (p. 404)
%	- signal length: N = 512; numbers of measurements: m = 128
% - Let us point out that our lq-algorithm allows several choices for q, including q = 0, and that
%   the SPARSEST OUTPUT PRODUCED FROM THESE CHOICES IS EVENTUALLY RETAINED. (p. 406)
% proc:SaabICASSP2008: Stable sparse approximations via nonconvex optimization
% 4. NUMERICAL EXPERIMENTS
% proc:ChartrandICASSP2008: Iteratively reweighted algorithms for compressive sensing
% 3. NUMERICAL EXPERIMENTS
% - IRLS w/ and w/o epsilon-regularization vs iteratively reweighted l1-min.
%
% article:ChartrandISPL2007: Exact Reconstruction of Sparse Signals via Nonconvex Minimization
% Abstract
% - We give [...] and MANY NUMERICAL EXAMPLES, both
%   in ONE COMPLEX DIMENSION, and larger-scale examples in TWO REAL DIMENSIONS. (p. 707)
% III. NUMERICAL EXAMPLES
% - DFT MATRIX at randomly chosen discrete frequencies
% - signal length: N = 512; numbers of measurements: M = 8, 16, ..., 128; sparsities: K = round( l * M / 16 ) <= M, l in \Z
% - values of p: p = 1, 0.95, 0.75, and 0.5
%   (Smaller values of p can be used but only with more careful supervision of parameters than that afforded by the automated approach described below.) (p. 708)
% - The experiment was repeated ten times for each value of M and K. (p. 708)
% - spike-train signal x: random selection of indices, randomly choosing the real and imaginary parts of each nonzero entry from the standard normal distribution (p. 708)
% - Conceptually, the matrix consisted of a random selection of rows from the discrete Fourier transform (DFT) matrix. (p. 708)
% - In actual fact, instead of multiplication by a matrix, the fast Fourier transform would be computed and the corresponding elements selected. (p. 708)
% - We adopt a simple computational approach for
%   COMPUTING LOCAL MINIMIZERS OF PROBLEM (3), namely, gradient descent with projection. (p. 708)
% - Similarly, we always BEGIN OUR ITERATION WITH THE EASILY COMPUTED MINIMUM-l2 NORM FIT TO THE DATA, as this is
%   the most sensible from the perspective of signal reconstruction. (p. 708)
% - For p = 0.5, EXACT RECONSTRUCTION is obtained with
%   a SPARSITY-TO-MEASUREMENT RATIO NEARLY DOUBLE THAT REQUIRED WHEN p = 1. (p. 709)
% - It is remarkable that
%   EVEN A VALUE OF p ONLY SLIGHTLY LESS THAN 1 GIVES
%   EXACT RECONSTRUCTION FOR SIGNIFICANTLY FEWER MEASUREMENTS
%   (12 % fewer, in this example). (p. 709)
% - DECREASING p FURTHER DECREASES THE REQUIRED NUMBER OF MEASUREMENTS, but it appears that
%   THE SMALLER THE VALUE OF p, THE LESS IMPROVEMENT IS SEEN FOR A GIVEN DECREASE IN p. (p. 709)
These findings agree with
% 1.) findings
those derived from
% 2.) numerical experiments
numerical experiments in
% 3.) scientific literature
the literature
(cf. e.g.
\cite[Sect. 5]{article:FoucartACHA2009},
\cite[Sect. 3]{proc:ChartrandICASSP2008},
\cite[Sect. III]{article:ChartrandISPL2007}%
).
% d) minimization of the lq-quasinorm, q \in ( 0; 1 ), in the sparsity-promoting method exactly recovered sparse representations from a substantially smaller number of error-free observations
% proc:ChartrandICASSP2008: Iteratively reweighted algorithms for compressive sensing
% ABSTRACT
% - In [1], it was shown EMPIRICALLY that using lp minimization with p < 1 can do so [exact sparse signal recovery] with
%   FEWER MEASUREMENTS than with p = 1. (p. 3869)
%   [1] article:ChartrandISPL2007
% 1. INTRODUCTION
% - In the other direction, it was shown in [1] that
%   A NONCONVEX VARIANT OF BASIS PURSUIT WILL PRODUCE
%   EXACT RECONSTRUCTION WITH FEWER MEASUREMENTS. (p. 3869)
% - Specifically, the l1 NORM IS REPLACED WITH THE lp NORM, WHERE 0 < p < 1
%   (in which case \norm{·}{p} isn’t actually a norm, though d(x, y) = \norm{x − y}{p}^{p} is a metric):
%   [ min_{ \vect{u} } \norm{ \vect{u} }{p}^{p}, subject to \mat{\Phi} \vect{u} = \vect{b}. ] (2) (p. 3869)
% - That
%   FEWER MEASUREMENTS ARE REQUIRED FOR EXACT RECONSTRUCTION THAN WHEN p = 1 WAS DEMONSTRATED BY
%   NUMERICAL EXPERIMENTS in [1], with random and nonrandom Fourier measurements. (p. 3869)
% 2. ALGORITHMS FOR NONCONVEX COMPRESSIVE SENSING
% - EXACT RECOVERY BECOMES POSSIBLE WITH MANY FEWER MEASUREMENTS, or with signals that are much less sparse. (p. 3870)
% article:ChartrandISPL2007: Exact Reconstruction of Sparse Signals via Nonconvex Minimization
% Abstract
% - We show that by replacing the l1 norm with the lp norm with p < 1,
%   EXACT RECONSTRUCTION IS POSSIBLE WITH SUBSTANTIALLY FEWER MEASUREMENTS. (p. 707)
% I. INTRODUCTION
% - It is natural to ask what happens if the l1 NORM IS REPLACED BY THE lp NORM FOR SOME p \in ( 0; 1 ). (p. 707)
% IV. CONCLUSIONS
% - In this letter, we have seen that
%   BY COMPUTING LOCAL lp MINIMIZERS WITH p < 1,
%   FEWER MEASUREMENTS ARE REQUIRED THAN PREVIOUSLY OBSERVED. (p. 710)
In fact,
% 1.) minimization
the minimization of
% 2.) lq-quasinorm, q \in ( 0; 1 )
the $\ell_{q}$-quasinorm, $q \in ( 0; 1 )$, in
% 3.) sparsity-promoting lq-minimization method
the sparsity-promoting method
\eqref{eqn:cs_lq_minimization} exactly recovered
% 4.) nearly-sparse representations
sparse representations
\eqref{eqn:def_transform_coefficients} from
% 5.) significantly smaller number
a significantly smaller number of
% 6.) error-free observations
error-free observations
\cite{proc:ChartrandICASSP2008,article:ChartrandISPL2007}.
% e) sufficient conditions for this [exact] recovery using the restricted isometry ratio or the restricted isometry constant mathematically justify these findings
% article:FoucartACHA2009: Sparsest solutions of underdetermined linear systems via lq-minimization for 0 < q \leq 1
% 2. Exact recovery via lq-minimization
% - The second special instance we are pointing out corresponds to the choice t = s + 1. (p. 397)
% - The RIGHT-HAND SIDE OF THIS INEQUALITY TENDS TO INFINITY AS q APPROACHES ZERO. (p. 397)
% - The following result is then straightforward.
% - Corollary 2.2.
%     Under the assumption that γ_{ 2s + 2 } < ∞,
%     EVERY S-SPARSE VECTOR IS EXACTLY RECOVERED BY SOLVING (P_{q}) FOR SOME q > 0 SMALL ENOUGH. (p. 397)
% proc:ChartrandICASSP2008: Iteratively reweighted algorithms for compressive sensing
% 1. INTRODUCTION
% - A THEOREM was also proven in terms of the RESTRICTED ISOMETRY CONSTANTS of Φ,
%   generalizing a result of [17] to show that
%   a CONDITION SUFFICIENT FOR (2) [idealized lp-minimization] TO RECOVER x EXACTLY IS WEAKER FOR SMALLER p. (p. 3869)
% - Thus, the DEPENDENCE OF THE SUFFICIENT NUMBER OF MEASUREMENTS M on the signal size N DECREASES as p → 0. (pp. 3869, 3870)
% article:ChartrandISPL2007: Exact Reconstruction of Sparse Signals via Nonconvex Minimization
% Abstract
% - We give a THEOREM IN THIS DIRECTION [exact recovery w/ fewer measurements], and [...]. (p. 707)
% I. INTRODUCTION
% - We begin with THEORETICAL RESULTS concerning when
%   a GLOBAL MINIMIZER IS GUARANTEED TO BE AN EXACT RECONSTRUCTION. (p. 707)
% - For a given \mat{\Phi}, the restricted isometry condition (5) will hold for larger values of K [sparsity] when p < 1. (p. 708)
Sufficient conditions, which relax
% 1.) upper bounds
the upper bounds on
% 2.) restricted isometry ratio
% article:FoucartACHA2009: Sparsest solutions of underdetermined linear systems via lq-minimization for 0 < q \leq 1
% 2. Exact recovery via lq-minimization
% - Our results are to be stated in terms of a quantity invariant under the change A ← cA, namely
%   [ \gamma_{2s} := {alpha_{2s}}^{-2} {\beta_{2s}}^{2} \geq 1 ]. (p. 396)
the restricted isometry ratio or
% 3.) restricted isometry constant
% article:ChartrandISPL2007: Exact Reconstruction of Sparse Signals via Nonconvex Minimization
constant for
% 4.) smaller parameters q
smaller parameters $q$, ensure
% 5.) stable recovery
\TODO{really stable?}
the stable recovery and justify
% 6.) exact recovery
these findings
\cite[Thm. 3.1]{article:FoucartACHA2009},
\cite[Thm. 1]{article:ChartrandISPL2007}%
\footnote{
  % article:FoucartACHA2009: Sparsest solutions of underdetermined linear systems via lq-minimization for 0 < q \leq 1
  % 1. Introduction
  % - Chartrand [7] studied it
  %   [minimize_{ \vect{z} \in \R^{N} } \norm{ \vect{z} }{q} subject to \mat{A} \vect{z} = \vect{y}, 0 < q \leq 1, (P_{q})] in terms of
  %   Restricted Isometry Constants. (p. 396)
  %   [7] article:ChartrandISPL2007
  % - He stated that s-sparse vectors can be exactly recovered by solving (P_{q}) under the assumption that
  %   δ_{ as } + b δ_{ ( a + 1 ) s } < b − 1 holds for some b > 1 and a := b^{q / ( 2 − q )}. (p. 396)
  % - He then claimed that EXACT RECOVERY OF s-SPARSE VECTORS CAN BE OBTAINED FROM THE SOLUTION OF (P_{q}) FOR SOME q > 0 small enough,
  %   provided that δ_{2s+1} < 1. (p. 396)
  % - There was a MINOR IMPRECISION IN HIS ARGUMENTS, as he neglected the fact that as must be an integer when he chose
  %   the number a under the requirement 1 < a < 1 + 1 / s. (p. 396)
  % - A correct justification would be to define a := 1 + 1 / s, so that
  %   the sufficient condition δ_{as} + b δ_{( a + 1 ) s} < b − 1, where b := a^{( 2 − q ) / q} > 1, becomes feasible for q > 0 small
  %   enough as long as δ_{2s+1} < 1. (p. 396)
  \name{Foucart} \emph{et al.} \cite{article:FoucartACHA2009} corrected
  a minor imprecision in
  the argument.
}.

%---------------------------------------------------------------------------------------------------------------
% 4.) convergence of the proposed implementation for the nonconvex lq-minimization method
%---------------------------------------------------------------------------------------------------------------
% a) intractability of the nonconvex lq-minimization method necessitates the approximation of the global minima by local minima using suitable initializations
% article:Schiffner2018, Sect. II. Compressed Sensing in a Nutshell
% - The parameter $q = 1$ induces the convex $\ell_{1}$-minimization method, whose
%   implementation permits computationally efficient algorithms, whereas
%   the half-open parameter interval $q \in [ 0; 1 )$ induces
%   the nonconvex $\ell_{q}$-minimization method, whose
%   global intractability necessitates local approximations.
The intractability of
% 1.) nonconvex sparsity-promoting lq-minimization method, q \in ( 0; 1 )
the nonconvex $\ell_{q}$-minimization method
\eqref{eqn:recovery_reg_norm_lq_minimization}, $q \in [ 0; 1 )$, necessitates
% 2.) approximation
the approximation of
% 3.) global minima
the global minima by
% 4.) local minima
local minima using
% 5.) suitable initializations
suitable initializations.
% b) although the results indicate the convergence to the global minimum
% article:FoucartACHA2009: Sparsest solutions of underdetermined linear systems via lq-minimization for 0 < q \leq 1
% 4. Description of the algorithm
% - Unfortunately,
%   the CONVERGENCE OF THE WHOLE SEQUENCE (z_{n}) COULD NOT BE ESTABLISHED RIGOROUSLY. (p. 401)
% - However, several points beside the numerical experiments of Section 5 hint at
%   its CONVERGENCE TO THE ORIGINAL s-SPARSE VECTOR x. (p. 401)
% proc:ChartrandICASSP2008: Iteratively reweighted algorithms for compressive sensing
% 2. ALGORITHMS FOR NONCONVEX COMPRESSIVE SENSING
% - However, the fact that
%   in practice we are able to recover signals exactly, combined with
%   theoretical results [1, 18] that give circumstances in which (2) has a unique, global minimizer that is exactly u∗ = x, strongly suggests that
%   THE COMPUTED LOCAL MINIMIZERS ARE ACTUALLY GLOBAL,
%   at least UNDER A BROAD SET OF CIRCUMSTANCES. (p. 3870)
% article:ChartrandISPL2007: Exact Reconstruction of Sparse Signals via Nonconvex Minimization
% II. RESTRICTED ISOMETRY CONSTANTS
% - While WE PROVIDE NO GUARANTEES, the numerical results below strongly suggest that
%   THE LEAST-SQUARES SOLUTION IS OFTEN OR EVEN ALWAYS SUFFICIENTLY CLOSE, IF THERE ARE ENOUGH MEASUREMENTS. (p. 708)
Although
% 1.) proposed implementation
the proposed implementation does not guarantee
% 2.) recovery
the recovery of
% 3.) global minimum
the global minimum
\cite[Sect. 4]{article:FoucartACHA2009},
% 4.) observed improvements
the observed improvements indicate
% 5.) convergence
its convergence to
% 6.) specified nearly-sparse representations
the specified sparse representations
\eqref{eqn:recovery_reg_sparse_representation}.

%---------------------------------------------------------------------------------------------------------------
% 5.) optimal specification of the parameter q
%---------------------------------------------------------------------------------------------------------------
% a) simulation study focused on the two parameters q \in \{ 0.5; 1 \} in the sparsity-promoting lq-minimization method
The simulation study focused on
the two parameters $q \in \{ 0.5; 1 \}$.
% f) choice of q and number of iterations
% g) specification of the parameter q = 0.5 in the simulation study was arbitrary
% TODO: no significant improvement for q < 0.5
%\TODO{why $q = 0.5$? no improvement for lower}
% b) Foucart et al. demonstrated the benefits of retaining the sparsest result provided by a finite set of discrete parameters at the expense of higher computational costs
% article:FoucartACHA2009: Sparsest solutions of underdetermined linear systems via lq-minimization for 0 < q \leq 1
% 5. Numerical experiments
% - Thus, as the approximation of the original problem (P0),
%   ONE WOULD INTUITIVELY EXPECT THAT, among the approximations of the problems (Pq),
%   THE REWEIGHTED l1-MINIMIZATION IS THE BEST OPTION TO RECOVER SPARSE VECTORS. (p. 404)
% - THIS IS NOT THE CASE, though, and THERE APPEARS TO BE SOME ADVANTAGES IN LETTING THE PARAMETER q VARY,
%   as demonstrated by the numerical experiments below. (p. 404)
% - Fig. 2 also shows that the choice q = 0 is NOT UNEQUIVOCALLY THE BEST CHOICE FOR A SINGLE q. (p. 406)
% - Based on these considerations, our preferred values for the parameters are
%   [...] exponents: q ∈ { 0, 0.05, 0.1, 0.2 }. (p. 406)
% - Let us point out that our lq-algorithm allows several choices for q, including q = 0, and that
%   the SPARSEST OUTPUT PRODUCED FROM THESE CHOICES IS EVENTUALLY RETAINED. (p. 406)
% - In this way, it is no surprise that the lq-method performs at least as well as
%   the reweighted l1-minimization. (p. 406)
% - It is surprising, however, that it does perform better, even by a small margin. (p. 406)
\name{Foucart} \emph{et al.} \cite[Sect. 5]{article:FoucartACHA2009} demonstrated
the benefits of retaining
the sparsest result provided by
a finite set of
discrete parameters
$q \in [ 0; 1 ]$ at the expense of
higher computational costs.
% c) reweighted l1 minimization is a special instance of Foucart's algorithm for q = 0
% article:FoucartACHA2009: Sparsest solutions of underdetermined linear systems via lq-minimization for 0 < q \leq 1
% 5. Numerical experiments
% - We point out that the REWEIGHTED l1-MINIMIZATION discussed in [6], which
%   came to our attention while we were testing this scheme, is
%   the SPECIAL INSTANCE OF THE ALGORITHM (18) with
%   q = 0, \epsilon_{n} = \epsilon, z_{0} = minimizer of (P_{1}). (p. 404)
%   [6] article:CandesJFAA2008: Enhancing Sparsity by Reweighted l1 Minimization
The reweighted $\ell_{1}$ minimization
\cite{article:CandesJFAA2008} is
a special instance of
% 2.) Foucart's algorithm
\name{Foucart}'s algorithm for
$q = 0$ that does not unequivocally achieve
the best results.
% d) Achim et al. proposed a method to infer the optimal parameter q from the characteristic exponent of a symmetric \alpha-stable distribution
% article:AchimITCI2015: Reconstruction of Ultrasound RF Echoes Modeled as Stable Random Variables
% - ultrasound RF echoes are best characterized statistically by alpha-stable distributions.
% - Together, these two facts form the basis of an lp minimization approach that employs
%   the iteratively reweighted least squares (IRLS) algorithm, but in which
%   the parameter p is judiciously chosen, by relating it to
%   the characteristic exponent of the underlying alpha-stable distributed data.
\name{Achim} \emph{et al.} \cite{article:AchimITCI2015} proposed
a method to infer
the optimal parameter $q$ from
the characteristic exponent of
a \acl{SaS} distribution modeling
% 1.) temporal samples of the beamformed RF voltage signals
the temporal samples of
an individual beamformed \ac{RF} voltage signal or
% 2.) DFTs of the temporal samples
its \acp{DFT}.
% c) suitable statistical models for the nearly-sparse representation in the normalized CS problem permit similar methods
The author speculates that
suitable statistical models for
% 1.) normalized nearly-sparse representation / nearly-sparse normalized vector of transform coefficients
the normalized nearly-sparse representation
\eqref{eqn:recon_reg_norm_trans_coef} could enable
% 2.) similar methods
similar methods.
