%---------------------------------------------------------------------------------------------------------------
% 1.) sparsity-promoting lq-minimization method
%---------------------------------------------------------------------------------------------------------------
% a) SPGL1 implemented the convex sparsity-promoting l1-minimization method
% article:VanDenBergSIAMJSC2009: Probing the Pareto Frontier for Basis Pursuit Solutions
Spectral projected gradient for
$\ell_{1}$-minimization (\acs{SPGL1})\acused{SPGL1}
\cite{article:VanDenBergSIAMJSC2009} implemented
% 1.) convex sparsity-promoting l1-minimization method
the convex $\ell_{1}$-minimization method
\eqreflqmin{eqn:recovery_reg_norm_lq_minimization}{ 1 }.
% b) Foucart's algorithm iteratively applied SPGL1 to approximate the intractable nonconvex sparsity-promoting lq-minimization method
% article:FoucartACHA2009: Sparsest solutions of underdetermined linear systems via lq-minimization for 0 < q \leq 1
% ABSTRACT
% - We then INTRODUCE
%   A SIMPLE NUMERICAL SCHEME TO COMPUTE SOLUTIONS WITH MINIMAL l_{q}-QUASINORM, and
%   we study its CONVERGENCE. (p. 395)
% 1. Introduction
% - Next, in Section 4, we propose
%   A NUMERICAL ALGORITHM TO APPROXIMATE THE MINIMIZATION (P_{q}). (p. 396)
% - We then DISCUSS CONVERGENCE ISSUES and prove that
%   the OUTPUT OF THE ALGORITHM is not merely an approximation, but IS IN FACT EXACT. (p. 396)
% 2. Exact recovery via lq-minimization
% - Let us remark that, IN PRACTICE, WE DO NOT SOLVE (P_{q}) but
%   AN APPROXIMATED PROBLEM, WHICH STILL YIELDS EXACT SOLUTIONS. (p. 396)
% 4. Description of the algorithm
% - We assume from now on that x is an s-SPARSE VECTOR. (p. 400)
% - The MINIMIZATION PROBLEM (P_{q}) SUGGESTED TO RECOVER x IS NONCONVEX, SO IT NEEDS TO BE APPROXIMATED. (p. 400)
% - We propose in this section
%   an ALGORITHM TO COMPUTE A MINIMIZER OF THE APPROXIMATED PROBLEM, for which
%   we give an informal but detailed justification. (p. 400)
% - We shall proceed ITERATIVELY,
%   [1.)] STARTING FROM A VECTOR z_{0} SATISFYING A z_{0} = y, which is a REASONABLE GUESS for x, and
%   [2.)] CONSTRUCTING A SEQUENCE (z_{n}) RECURSIVELY by defining z_{n + 1} as a solution of
%         the minimization problem (18). (p. 400)
% - Here, the sequence (\epsilon_{n}) is a NONINCREASING SEQUENCE OF POSITIVE NUMBERS. (p. 400)
% - We point out that THE SCHEME IS EASY TO IMPLEMENT, since
%   EACH STEP REDUCES TO AN l1-MINIMIZATION PROBLEM (P1) RELATIVELY TO THE RENORMALIZED MATRIX
%   A_{n} := A \diag{ ( |z_{n, i}| + \epsilon_{n} )^{ 1 - q } }. (p. 400)
% - Unfortunately, the CONVERGENCE OF THE WHOLE SEQUENCE (z_{n}) COULD NOT BE ESTABLISHED RIGOROUSLY. (p. 401)
% 5. Numerical experiments
% - Let us point out that OUR lq-ALGORITHM ALLOWS SEVERAL CHOICES FOR q, INCLUDING q = 0, and that
%   the SPARSEST OUTPUT PRODUCED FROM THESE CHOICES IS EVENTUALLY RETAINED. (p. 406)
\name{Foucart}'s algorithm
\cite[Sect. 4]{article:FoucartACHA2009} iteratively applied
% 1.) convex l1-minimization method
this method based on
\ac{SPGL1} to
a sequence of
renormalized \ac{CS} problems to approximate
% 2.) nonconvex sparsity-promoting lq-minimization method, q \in [ 0; 1 )
the nonconvex $\ell_{q}$-minimization method
\eqref{eqn:recovery_reg_norm_lq_minimization} for
the half-open parameter interval
$q \in [ 0; 1 )$.
% c) SPGL1 is iterative and left multiplied a sequence of recursively-generated vectors by the normalized sensing matrix or its adjoint
% article:RokhlinJCP1990: Rapid Solution of Integral Equations of Scattering Theory in Two Dimensions
% - most iterative solvers: APPLICATION OF THE MATRIX TO A SEQUENCE OF RECURSIVELY GENERATED VECTORS
% article:RokhlinJCP1985: Rapid solution of integral equations of classical potential theory
% - MOST ITERATIVE SCHEMES FOR SOLUTION OF LINEAR SYSTEMS resulting from classical potential theory
%   REQUIRE APPLICATION OF THE MATRIX OF THE SYSTEM TO A SEQUENCE OF RECURSIVELY GENERATED VECTORS.
\ac{SPGL1} is
iterative and left multiplied
% 1.) sequence of recursively-generated vectors
a sequence of
recursively-generated vectors by
% 2.) normalized sensing matrix
the potentially densely-populated normalized sensing matrix
\eqref{eqn:recon_reg_norm_sensing_matrix} or
% 3.) adjoint of the normalized sensing matrix
its adjoint.
% d) matrix-free implementation interpreted each type of matrix-vector product as a linear map and dedicated a customized auxiliary function to its numerical evaluation
Its matrix-free implementation interpreted
each type of
matrix-vector product as
a linear map and dedicated
a customized auxiliary function to
its numerical evaluation.
% e) auxiliary functions aimed at circumventing the explicit storage of the associated matrix and accelerating the numerical computations
Both functions aimed at
% 1.) circumventing the explicit storage of the associated matrix in the fast but limited RAM
circumventing
the explicit storage of
the associated matrix in
the fast but limited \ac{RAM} and
% 2.) accelerating the numerical computations
accelerating
the numerical computations.
% f) memory consumption of the normalized sensing matrix and the number of multiplications executed by the associated matrix-vector product pose challenges
In fact,
% 1.) memory consumption of the normalized sensing matrix
the memory consumption of
the normalized sensing matrix
\eqref{eqn:recon_reg_norm_sensing_matrix} and
% 2.) number of multiplications executed by the associated matrix-vector product
the number of
multiplications executed by
the associated matrix-vector product pose
challenges for
modern \ac{UI} systems.
% g) memory consumption of the normalized sensing matrix and the number of multiplications executed by the associated matrix-vector product
They equal
% 1.) memory consumption of the normalized sensing matrix
$M_{\text{conv}} = N_{\text{obs}} N_{\text{lat}} w_{\text{c}}$, where
$w_{\text{c}} \in \Rplus$ denotes
the amount of
memory allocated to
a complex-valued variable, and
% 2.) number of multiplications executed by the associated matrix-vector product
$N_{\text{mul},\text{conv}} = N_{\text{obs}} N_{\text{lat}}$,
respectively.
% h) auxiliary functions composed the FMM, a fast basis transform, and the normalization of the column vectors
For this reason,
both functions composed
% 1.) fast multipole method for the observation process
the \ac{FMM},
% 2.) fast basis transform for the nearly-sparse representation
a fast basis transform, and
% 3.) normalization of the column vectors
\TODO{ordentlich, punkt 3}
the normalization of
the column vectors.
% h) FMM efficiently approximated the action of the observation process or its adjoint on a suitable vector
%The \ac{FMM} efficiently approximated
%the action of
% 1.) observation process (all pulse-echo measurements, multifrequent, all transducer elements)
%the observation process
%\eqref{eqn:recovery_sys_lin_eq_v_rx_born_all_f_all_in_mat} or
% 2.) adjoint of the observation process (all pulse-echo measurements, multifrequent, all transducer elements)
%its adjoint on
%a suitable vector, whereas
% i) fast basis transform efficiently inferred the nearly-sparse representation from the discretized relative spatial fluctuations in compressibility or vice versa
%the fast basis transform efficiently inferred
% 1.) nearly-sparse representation
%the nearly-sparse representation
%\eqref{eqn:recovery_reg_sparse_representation} from
% 2.) vector stacking the regular samples in the discretized relative spatial fluctuations in the unperturbed compressibility
%the compressibility fluctuations
%\eqref{eqn:recovery_sys_lin_eq_gamma_kappa_bp_vector} or
%vice versa.
% j) diagonal inverse weighting matrix readily permitted the efficient normalization of the column vectors by numerically evaluating the associated matrix-vector products
The latter corresponded to
a multiplication by
% 1.) diagonal inverse weighting matrix
the diagonal inverse weighting matrix
\eqref{eqn:recovery_reg_norm_weighting_matrix_inv} and readily permitted
an efficient evaluation.
