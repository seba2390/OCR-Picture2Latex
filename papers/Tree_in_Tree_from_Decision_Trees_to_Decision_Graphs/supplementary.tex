\documentclass{article}
\PassOptionsToPackage{numbers,sort&compress}{natbib}
% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2019

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2019}

% to compile a camera-ready version, add the [final] option, e.g.:
     \usepackage[final]{neurips_2021}
     %\usepackage[review]{neurips_2019}

% to avoid loading the natbib package, add option nonatbib:
%     \usepackage[nonatbib]{neurips_2019}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{mathtools}
\usepackage{mathrsfs}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{amsthm}
\usepackage{float} 
\usepackage{chngcntr}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors

\usepackage[noend,linesnumbered,ruled,vlined]{algorithm2e}
\title{Supplementary Materials for\\ ``Tree in Tree: from Decision Trees to Decision Graphs''}
\begin{document}
\maketitle
\appendix
\counterwithin{table}{section}
\numberwithin{algocf}{section}

\section{Pseudocode to fine-tune TnT decision graph}
We propoed the TnT algorithm to construct a decision graph from scratch. The TnT decision graph can be further fine-tuned using alternating optimization \cite{carreira2018alternating}. As opposed to TnT, TnT fine-tuning requires a predefined graph structure as input. A comparison between TnT and TnT(fine-tuned) is presented in Fig. 4, where TnT(fine-tuned) slightly improves both train and test accuracy. Algorithm A.1 shows the pseudocode to fine-tune TnT. Similar to Algorithm 2 in the main text, the TnT fine-tune algorithm also computes the subset $\mathbb{X}_{subset}, \mathbb{Y}_{subset}$ at each node. The hyperparameter $N$ is the number of rounds for TnT fine-tune and we fix $N=5$ for all experiments in Fig. 4.

\begin{algorithm}[H]
  \SetAlgoLined
  \KwData{Training set $\mathcal{X},\mathcal{Y}$}
  \KwIn{TnT decision graph $G$}
  \KwResult{TnT decision graph $G^\prime$ fine-tuned on $\mathcal{X},\mathcal{Y}$}
  \{$infer(n, \mathcal{X})$ denotes the forward inference of data $\mathcal{X}$ starting from node $n$\}\; 
  \{Nodes are visited in the breadth-first order\}\;
    \For{$i\gets1$ \KwTo $N$}{
        \For{$\text{each node ($n_i$)} \in G$}{
            Samples that visit $n_i$: $\mathcal{X}_i,\mathcal{Y}_i \subset \mathcal{X},\mathcal{Y}$\;
            \uIf{$n_i$ is an internal node}{
                $\mathcal{Y}_{i,left} \gets infer(n_i.left\_child, \mathcal{X}_i)$\;
                $\mathcal{Y}_{i,right} \gets infer(n_i.right\_child, \mathcal{X}_i)$\;
                $index\_left \gets (\mathcal{Y}_{i}=\mathcal{Y}_{i,left} \textbf{ and } \mathcal{Y}_{i}\neq \mathcal{Y}_{i,right})$ \;
                $index\_right \gets (\mathcal{Y}_{i}=\mathcal{Y}_{i,right} \textbf{ and } \mathcal{Y}_{i}\neq \mathcal{Y}_{i,left})$ \;
                $\mathcal{X}_{subset}, \mathcal{Y}_{subset} \gets$ copy samples from $\mathcal{X}_{i},\mathcal{Y}_{i}$ at $index\_left \textbf{ or } index\_right$\; 
                $\mathcal{Y}_{subset}[index\_left] \gets 0$,     $\mathcal{Y}_{subset}[index\_right] \gets 1$\; 
                Update the split function of $n_i$ based on $\mathcal{X}_{subset}, \mathcal{Y}_{subset}$ \;
            }
            \uElseIf{$n_i$ is a leaf node}{
                $\mathcal{X}_{subset} \gets \mathcal{X}_i$,    $\mathcal{Y}_{subset} \gets \mathcal{Y}_i$\;
                Label the leaf $n_i$ as the dominant class of $\mathcal{Y}_{subset}$\; 
            }
        }
    }
  \label{algorithm2}
  \caption{Tree in Tree fine-tune}
\end{algorithm}

\section{Hyperparameters of TnT}
The TnT algorithm has three hyperparameters. $N_1$ is the number of merging phases where we merge micro trees into the graph. $N_2$ is the number of rounds to grow and optimize micro trees. The choice of $N_1$ and $N_2$ reflects the trade-off between training time and classification performance. We empirically set $N_1=2, N_2=5$ for all experiments in this work. $C$ is the cost complexity pruning coefficient to tune the complexity of TnT decision graphs \cite{bradford1998pruning, kiran2017cost}. With greater $C$, TnT tends to have fewer splits. For example, Fig. 5 in the main text visualizes various model complexities with 20, 129 and 1046 splits, which is achieved with $C=1e-2$, $C=1e-3$ and $C=1e-4$, respectively. %High-quality visualization figures are uploaded as supplementary materials.

Figure 4 in the main text plots the classification performance as a function of model complexity. We tuned $C$ to change the number of splits. For each dataset, we sampled 30 values of $C$ which are equally spaced on a log scale. The maximum and minimum values of $C$ are summarized in Table B.1.
\begin{table}[h]
    \centering
    \caption{The maximum and minimum values of $C$ on different datasets.}
    \scalebox{0.92}{
    \begin{tabular}{c|cccccccc}
    \toprule
         Dataset& MNIST & Connect-4 & Letter& Optical recognition& Pendigits& Protein & SenseIT & USPS \\ \midrule
         $C_{min}$ & 1e-4 & 6e-5 & 5e-5 & 3e-4 & 5e-4 & 8e-4 & 3e-4 & 8e-4\\
         $C_{max}$ & 5e-2 & 1e-2 & 2e-2 & 6e-2 & 1e-1 & 1e-2 & 1e-2 & 3e-2\\
         \bottomrule
    \end{tabular}
    }
\end{table}

In addition to using TnTs as stand-alone classifiers, we combine TnT decision graphs with ensemble methods and present TnT-bagging and TnT-AdaBoost. Additional hyperparameters are introduced to TnT-bagging and TnT-AdaBoost by the ensemble methods. In this work, we tuned the number of base estimators and the total number of splits to change the ensemble complexity. For the bagging ensemble, we randomly draw samples from the training set with replacement to train each base estimator. We set \textit{max$\_$samples} to 1.0 and \textit{bootstrap$\_$features=False} for both Random Forest and TnT-bagging. For the AdaBoost ensemble, we used the \textit{``SAMME''} algorithm with a learning rate of 1.0 to build both AdaBoost and TnT-AdaBoost. Both ensemble methods were implemented using the scikit-learn library in Python \cite{scikit-learn}.

\section{Comparison of TnT and DT ensembles}
Table C.1 is similar to Table 2 in the main text but includes additional datasets. A summary on model comparison is given in the last two rows. The results show that both bagging and AdaBoost ensembles benefit from using the TnT as a base estimator.  

\begin{table}[!htbp]
  \caption{Comparison of TnT ensembles with random forest and AdaBoost. Mean train and test accuracy ($\pm$ standard deviation) is calculated across 5 independent trials. We tune the ensemble size ($\#$E, the number of base estimators) and split count ($\#$S) to change the complexity of the ensembles. Dataset statistics are given in the format: Dataset name ($\#$ Train/Test samples * $\#$ Features, $\#$ Classes). }
  \label{table1}
  \centering
  \scalebox{0.75}{
  \begin{tabular}{lllllllllll}
    \toprule
    model & & \#E & \#S & train &  test & & \#E & \#S & train &  test\\
    \midrule
    TnT-bagging & \multirow{14}{*}{ \rotatebox{90}{MNIST (60k/10k*784, 10)}} 
      & 5 &  4.8k  & \textbf{97.46$\pm$0.16} & \textbf{93.65$\pm$0.24} &
     \multirow{14}{*}{ \rotatebox{90}{Connect-4 (45.3k/22.3k*126, 3)}} 
      & 5 &  4.6k  & \textbf{84.42$\pm$0.19} & \textbf{80.61$\pm$0.18}\\
     Random Forest & & 5 & 4.8k & 96.55$\pm$0.36 & 92.31$\pm$0.57  &
      & 5 & 4.6k & 83.60$\pm$0.12 & 79.21$\pm$0.19  \\\cmidrule{3-6} \cmidrule{8-11}
     TnT-AdaBoost & & 5 &  640  & \textbf{90.26} & 88.38 &
     & 5 &  450  & \textbf{77.75$\pm$0.16} & \textbf{77.39$\pm$0.19}   \\
     AdaBoost & & 5 &  640 & 89.75 & \textbf{88.61} &
     & 5 &  450 & 77.28 & 76.74   \\ \cmidrule{3-6} \cmidrule{8-11}
     
     TnT-bagging& & 10 &  9.6k  & \textbf{98.28$\pm$0.06} & \textbf{94.92$\pm$0.20} &
     & 10 &  9.2k  & \textbf{85.11$\pm$0.05} & \textbf{81.44$\pm$0.14}  \\
     Random Forest & & 10 & 9.6k & 97.44$\pm$0.18 & 93.64$\pm$0.38 &
     & 10 & 9.2k & 84.21$\pm$0.12 & 79.85$\pm$0.20   \\\cmidrule{3-6} \cmidrule{8-11}
     TnT-AdaBoost & & 10 &  1.4k  & \textbf{95.09$\pm$0.09} & \textbf{92.36$\pm$0.13} &
     & 10 &  940  & \textbf{80.10$\pm$0.23} & \textbf{78.94$\pm$0.29} \\
     AdaBoost & & 10 &  1.4k & 94.28 & 91.49 &
     & 10 &  940 & 79.69 & 78.37  \\\cmidrule{3-6} \cmidrule{8-11}
     
     TnT-bagging & & 20 &  19.2k  & \textbf{98.64$\pm$0.06} & \textbf{95.57$\pm$0.14} &
     & 20 &  18.3k  & \textbf{85.66$\pm$0.12} & \textbf{81.93$\pm$0.13}  \\
     Random Forest & & 20 & 19.2k & 97.90$\pm$0.12 & 94.36$\pm$0.19 &
     & 20 & 18.3k & 84.57$\pm$0.08 & 80.39$\pm$0.09\\\cmidrule{3-6} \cmidrule{8-11}
     TnT-AdaBoost & & 20 &  2.9k  & \textbf{98.03$\pm$0.11} & \textbf{94.49$\pm$0.21} &
     & 20 &  1.8k  & 82.46$\pm$0.41 & 80.53$\pm$0.50 \\
     AdaBoost & & 20 &  2.9k & 97.70 & 94.04 &
     & 20 &  1.8k & \textbf{82.77} & \textbf{81.14}\\\cmidrule{3-6} \cmidrule{8-11}
     
     TnT-bagging & & 100 &  111k  & 99.09$\pm$0.03 & \textbf{96.11$\pm$0.09} &
     & 100 &  143k  & 88.44$\pm$0.07 & \textbf{82.84$\pm$0.02}  \\
     Random Forest & & 100 & 292k & \textbf{100} & 95.72$\pm$0.17 &
     & 100 & 718k & \textbf{100} & 82.33$\pm$0.10\\
     
     
    %% dataset 3,4
    \midrule
    TnT-bagging & \multirow{14}{*}{ \rotatebox{90}{Letter (13.4k/6.6k*16, 26)}} 
      & 5 &  5.3k  & 98.08$\pm$0.12 & \textbf{89.97$\pm$0.37} &
     \multirow{14}{*}{ \rotatebox{90}{Optical recognition (3.8k/1.8k*64, 10)}} 
      & 5 &  890  & \textbf{99.48} & 90.45$\pm$1.24\\
     Random Forest & & 5 & 5.3k & \textbf{98.16$\pm$0.11} & 89.93$\pm$0.25  &
      & 5 & 890 & 99.38$\pm$0.11 & \textbf{90.46$\pm$0.91} \\\cmidrule{3-6} \cmidrule{8-11}
     TnT-AdaBoost & & 5 &  440  & \textbf{74.51$\pm$0.83} & \textbf{73.58$\pm$0.63} &
     & 5 &  200  & \textbf{96.74$\pm$0.29} & \textbf{88.31$\pm$0.61}   \\
     AdaBoost & & 5 &  440 &  73.40 & 71.38 &
     & 5 &  200 & 96.73 & 87.87  \\ \cmidrule{3-6} \cmidrule{8-11}
     
     TnT-bagging& & 10 &  10.6k  & \textbf{99.16$\pm$0.10} & \textbf{92.35$\pm$0.15} &
     & 10 &  1.8k  & \textbf{99.83} & \textbf{92.41$\pm$0.51}  \\
     Random Forest & & 10 & 10.6k & 99.10$\pm$0.08 & 91.92$\pm$0.33 &
     & 10 & 1.8k & 99.79$\pm$0.10 & 92.23$\pm$0.37   \\\cmidrule{3-6} \cmidrule{8-11}
     TnT-AdaBoost & & 10 & 900  & \textbf{82.90$\pm$0.38} & \textbf{80.02$\pm$0.33} &
     & 10 &  420  & \textbf{99.81$\pm$0.06} &92.87$\pm$0.65 \\
     AdaBoost & & 10 & 900 & 81.10 & 78.09 &
     & 10 &  420 & 99.58 & \textbf{92.92$\pm$0.02} \\\cmidrule{3-6} \cmidrule{8-11}
     
     TnT-bagging & & 20 &  21.3k  & \textbf{99.57$\pm$0.04} & \textbf{93.35$\pm$0.19} &
     & 20 &  3.6k  & \textbf{99.91} & \textbf{92.93$\pm$0.41}  \\
     Random Forest & & 20 & 21.3k & 99.33$\pm$0.03 & 92.85$\pm$0.21 &
     & 20 & 3.6k & 99.84$\pm$0.06 & 92.78$\pm$0.23 \\\cmidrule{3-6} \cmidrule{8-11}
     TnT-AdaBoost & & 20 &  1.8k  & \textbf{90.89$\pm$0.67} & \textbf{85.33$\pm$0.56} &
     & 20 &  820  & \textbf{99.99$\pm$0.01} & \textbf{94.52$\pm$0.55} \\
     AdaBoost & & 20 & 1.8k & 89.84 & 84.75 &
     & 20 &  820 & 99.97 &94.50$\pm$0.02 \\\cmidrule{3-6} \cmidrule{8-11}
     
     TnT-bagging & & 100 &  108k  & 99.78$\pm$0.02 & \textbf{94.37$\pm$0.03} &
     & 100 &  18k  & 99.93$\pm$0.03 & \textbf{93.62$\pm$0.17}  \\
     Random Forest & & 100 & 136k & \textbf{100} & 94.29$\pm$0.07 &
     & 100 & 19k & \textbf{100} & 93.37$\pm$0.24\\
     
     
    %% dataset 5,6
    \midrule
    TnT-bagging & \multirow{14}{*}{ \rotatebox{90}{Pendigits (7.5k/3.5k*16, 10)}} 
      & 5 &  570  & \textbf{99.32$\pm$0.11} & \textbf{94.12$\pm$0.27} &
     \multirow{14}{*}{ \rotatebox{90}{Protein (11.9k/5.9k*357, 3)}} 
      & 5 &  1.4k  & \textbf{77.05$\pm$0.58} & \textbf{59.59$\pm$0.62} \\
     Random Forest & & 5 & 570 & 98.86$\pm$0.12 & 92.77$\pm$0.41 &
      & 5 & 1.4k & 77.30$\pm$0.53 & 59.67$\pm$0.33  \\\cmidrule{3-6} \cmidrule{8-11}
     TnT-AdaBoost & & 5 &  200  & \textbf{98.53$\pm$0.14} & \textbf{93.24$\pm$0.62} &
     & 5 &  140  & \textbf{63.99} & \textbf{59.29}  \\
     AdaBoost & & 5 &  200 & 97.66 & 92.31 &
     & 5 &  140 & 62.43 & 58.45   \\ \cmidrule{3-6} \cmidrule{8-11}
     
     TnT-bagging& & 10 &  1.1k  & \textbf{99.54$\pm$0.10} & \textbf{94.81$\pm$0.19} &
     & 10 &  2.7k  & 80.87$\pm$0.40 & \textbf{62.75$\pm$0.25}  \\
     Random Forest & & 10 & 1.1k & 99.01$\pm$0.13 & 93.47$\pm$0.33 &
     & 10 & 2.7k & \textbf{80.88$\pm$0.28} & 62.60$\pm$0.33   \\\cmidrule{3-6} \cmidrule{8-11}
     TnT-AdaBoost & & 10 &  410  & 99.52$\pm$0.22 & \textbf{94.83$\pm$0.21} &
     & 10 &  270  & \textbf{67.47} & \textbf{61.16} \\
     AdaBoost & & 10 &  410 & \textbf{99.65} & 94.75$\pm$0.02 &
     & 10 &  270 & 66.76 & 60.92  \\\cmidrule{3-6} \cmidrule{8-11}
     
     TnT-bagging & & 20 &  2.3k  & \textbf{99.61$\pm$0.05} & \textbf{95.48$\pm$0.16} &
     & 20 &  5.4k  & \textbf{83.20$\pm$0.47} & \textbf{64.44$\pm$0.44}  \\
     Random Forest & & 20 & 2.3k & 99.16$\pm$0.10 & 93.71$\pm$0.24 &
     & 20 & 5.4k & 82.82$\pm$0.24 & 64.06$\pm$0.20 \\\cmidrule{3-6} \cmidrule{8-11}
     TnT-AdaBoost & & 20 & 820  & 100 & 96.35$\pm$0.30 &
     & 20 &  580  & \textbf{73.15} & 62.92 \\
     AdaBoost & & 20 &  820 & 100 & \textbf{96.63} &
     & 20 &  580 &  72.03 & \textbf{64.03} \\\cmidrule{3-6} \cmidrule{8-11}
     
     TnT-bagging & & 100 &  11k  & 99.69$\pm$0.04 & \textbf{95.69$\pm$0.16} &
     & 100 &  0.3k  & 86.71$\pm$0.21 & \textbf{66.63$\pm$0.30}  \\
     Random Forest & & 100 & 20k & \textbf{100} & 95.31$\pm$0.22 &
     & 100 & 1.5k & \textbf{100} & 66.34$\pm$0.09\\
     
    %% dataset 7,8
    \midrule
    TnT-bagging & \multirow{14}{*}{ \rotatebox{90}{SenseIT (78.8k/19.7k*100, 3)}} 
      & 5 &  910  & \textbf{83.92$\pm$0.12} & \textbf{82.27$\pm$0.12} &
     \multirow{14}{*}{ \rotatebox{90}{USPS (7.3k/2k*256, 2)}} 
      & 5 &  540  & \textbf{98.44$\pm$0.13} & \textbf{91.29$\pm$0.34}\\
     Random Forest & & 5 & 910 & 83.06$\pm$0.18  & 80.95$\pm$0.31  &
      & 5 & 540 & 97.27$\pm$0.16 & 90.06$\pm$0.39  \\\cmidrule{3-6} \cmidrule{8-11}
     TnT-AdaBoost & & 5 &  110  & \textbf{77.98} & \textbf{77.47} &
     & 5 &  160  & \textbf{99.07} & \textbf{91.73}  \\
     AdaBoost & & 5 &  110 & 77.83 & 77.03 &
     & 5 &  160 & 97.63 & 90.53   \\ \cmidrule{3-6} \cmidrule{8-11}
     
     TnT-bagging& & 10 &  1.8k  & \textbf{84.52$\pm$0.08} & \textbf{82.87$\pm$0.20} &
     & 10 &  1.1k  & \textbf{98.75$\pm$0.06} & \textbf{91.90$\pm$0.16}  \\
     Random Forest & & 10 & 1.8k & 83.48$\pm$0.18 & 81.41$\pm$0.22 &
     & 10 & 1.1k & 97.85$\pm$0.19 & 90.53$\pm$0.26   \\\cmidrule{3-6} \cmidrule{8-11}
     TnT-AdaBoost & & 10 &  170  & \textbf{79.06} & \textbf{78.46} &
     & 10 &  350  & \textbf{100} & \textbf{92.83} \\
     AdaBoost & & 10 &  170 &  78.82 & 78.21 &
     & 10 &  350 & 99.95 & 92.50$\pm$0.40  \\\cmidrule{3-6} \cmidrule{8-11}
     
     TnT-bagging & & 20 &  3.6k  & \textbf{84.88$\pm$0.03} & \textbf{83.19$\pm$0.13} &
     & 20 &  2.2k  & \textbf{99.20$\pm$0.09} & \textbf{92.72$\pm$0.39} \\
     Random Forest & & 20 & 3.6k & 83.77$\pm$0.15 & 81.64$\pm$0.19 &
     & 20 & 2.2k & 98.16$\pm$0.04 & 91.29$\pm$0.42  \\\cmidrule{3-6} \cmidrule{8-11}
     TnT-AdaBoost & & 20 &  280  & \textbf{80.00} & 79.18 &
     & 20 &  740  & 100 & \textbf{94.37} \\
     AdaBoost & & 20 &  280 & 79.96 & \textbf{79.19} &
     & 20 &  740 & 100 & 94.03$\pm$0.25 \\\cmidrule{3-6} \cmidrule{8-11}
     
     TnT-bagging & & 100 &  116k  & 90.92$\pm$0.02 & \textbf{84.09$\pm$0.09} &
     & 100 &  11k  & 99.29$\pm$0.05 & \textbf{93.18$\pm$0.28}  \\
     Random Forest & & 100 & 590k & \textbf{99.98} & 83.83$\pm$0.11 &
     & 100 & 24k & \textbf{100} & 92.67$\pm$0.28\\
     
     \midrule
     \multirow{2}{*}{Summary} & \multicolumn{3}{c}{\textbf{TnT-bagging wins}} & \textbf{test accuracy: 31} & & \multicolumn{3}{c}{Random Forest wins}  & test accuracy: 1\\ \cmidrule{2-11}
     &\multicolumn{3}{c}{\textbf{TnT-AdaBoost wins}}  & \textbf{test accuracy: 18} & & \multicolumn{3}{c}{AdaBoost wins}  & test accuracy: 6\\
    \bottomrule
  \end{tabular}
  }
\end{table}



\bibliographystyle{unsrtnat}
\bibliography{MyCitation}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Checklist}

\begin{enumerate}

\item For all authors...
\begin{enumerate}
  \item Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?
    \answerYes{See Section 1, Introduction.}
  \item Did you describe the limitations of your work?
    \answerYes{See Section 6, Limitations.}
  \item Did you discuss any potential negative societal impacts of your work?
    \answerNA{This paper introduces a new classifier. We do not see any potential negative societal impacts.}
  \item Have you read the ethics review guidelines and ensured that your paper conforms to them?
    \answerYes{}
\end{enumerate}

\item If you are including theoretical results...
\begin{enumerate}
  \item Did you state the full set of assumptions of all theoretical results?
    \answerNA{}
	\item Did you include complete proofs of all theoretical results?
    \answerNA{}
\end{enumerate}

\item If you ran experiments...
\begin{enumerate}
  \item Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)?
    \answerYes{We include a code in the supplementary material. Datasets are publicly available on UCI repository and LIBSVM Data.}
  \item Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)?
    \answerYes{Data splits are discussed in Section 4. Choice of hyperparameters is discussed in the supplementary materials Section B.}
	\item Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)?
    \answerYes{Figure 4 and Table 1 report standard deviations across different trials.}
	\item Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)?
    \answerYes{Platform and training time are reported in Section 3, Time complexity.}
\end{enumerate}

\item If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...
\begin{enumerate}
  \item If your work uses existing assets, did you cite the creators?
    \answerYes{See reference [28], [29]}
  \item Did you mention the license of the assets?
    \answerYes{The scikit-learn library is under the 3-Clause BSD license. Some datasets (e.g., MNIST) are under Creative Commons Attribution-Share Alike 3.0 license.}
  \item Did you include any new assets either in the supplemental material or as a URL?
    \answerYes{}
  \item Did you discuss whether and how consent was obtained from people whose data you're using/curating?
    \answerNA{All datasets are publicly available on UCI repository and LIBSVM Data.}
  \item Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content?
    \answerNA{}
\end{enumerate}

\item If you used crowdsourcing or conducted research with human subjects...
\begin{enumerate}
  \item Did you include the full text of instructions given to participants and screenshots, if applicable?
    \answerNA{}
  \item Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable?
    \answerNA{}
  \item Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation?
    \answerNA{}
\end{enumerate}

\end{enumerate}
\end{document}