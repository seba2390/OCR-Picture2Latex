\vspace{-0.1in}
\section{Results}\label{sec:results}

Our results address \textbf{four main questions}: 
1) How efficiently can our approach identify the best snap angle? (Sec.~\ref{sec:pred_angle});
2) To what extent does the foreground ``pixel objectness'' objective properly capture objects important to human viewers? (Sec.~\ref{sec:preserve_object});
3) To what extent do human viewers favor snap-angle cubemaps over the default orientation? (Sec.~\ref{sec:human}); and 4) Might snap angles aid image recognition? (Sec.~\ref{sec:recognition}).


\vspace*{-0.1in}
\paragraph{Dataset}

We collect a dataset of 360$^{\circ}$ images to evaluate our approach; existing 360$^{\circ}$ datasets are topically narrow~\cite{xiao2012recognizing,su2016pano2vid,hu2017deep}, restricting their use for our goal.  
We \KG{use YouTube with the 360$^{\circ}$ filter to gather} videos from four activity categories---Disney, Ski, Parade, and Concert.  %---for a total of 150 videos.  %We query the corresponding keywords from Youtube and download 360 videos.
After manually filtering out frames with only text or blackness, we have 150 videos and 14,076 total frames sampled at 1 FPS.


\vspace*{-0.1in}
\paragraph{Implementation details}

We implement our model with Torch, and optimize with stochastic gradient \KG{and REINFORCE}. We set the base learning rate to 0.01 and use momentum.  % We train the network and choose the best model based on a validation set.
\KG{We fix $A = 6.25\%$ for all results after visual inspection of a few human-taken cubemaps (not in the test set).}
See Supp. for all network architecture details.  









\subsection{Efficient Snap Angle Prediction}\label{sec:pred_angle}


We first evaluate our snap angle prediction framework. 
We use all 14,076 frames, 75\% for training and 25\% for testing. We ensure testing and training data do \emph{not} come from the same video.
We define the following baselines:
\begin{itemize}
  \itemsep0em
  \item \textsc{Random rotate}:  Given a budget $T$, predict $T$ snap angles randomly (with no repetition).

    \item \textsc{Uniform rotate}: Given a budget $T$, predict $T$ snap angles uniformly sampled from all candidates.  %\textsc{Canonical view} is always included.   
      When $T=1$, \textsc{Uniform} receives the \textsc{Canonical} view.  This is a strong baseline since it exploits the human recording bias in the starting view.  \KG{Despite the 360$^{\circ}$ range of the camera, photographers still tend to direct the ``front'' of the camera towards interesting content, in which case \textsc{Canonical} has some manual intelligence built-in.}
 
\item \bx{\textsc{Coarse-to-fine search}: 
Divide the search space into two uniform intervals and search the center snap angle in each interval. Then recursively search the better interval, until the budget is exhausted.}
%Note \textsc{Coarse-to-fine search} requires $T$ to be at least 2.}


\item \bx{\textsc{Pano2Vid(P2V)~\cite{su2016pano2vid}-adapted}:
We implement a snap angle variant inspired by the pipeline of Pano2Vid~\cite{su2016pano2vid}.
We replace C3D~\cite{tran2015learning} features (which require video) used in~\cite{su2016pano2vid} with F7 features from VGG~\cite{Simonyan14c} and train a logistic classifier to learn ``capture-worthiness"~\cite{su2016pano2vid} with Web images and randomly sampled panorama subviews (see Supp.). For a budget $T$, we evaluate $T$ ``glimpses" and choose the snap angle with the highest encountered capture-worthiness score.}  \KGthree{We stress that Pano2Vid addresses a different task: it creates a normal field-of-view video (discarding the rest) whereas we create a well-oriented omnidirectional image.  Nonetheless, we include this baseline to test their general approach of learning a framing prior from human-captured data.}


\item \textsc{Saliency}:  Select the angle that centers a cube face around the maximal saliency region.  Specifically, we compute the panorama's saliency map~\cite{liu2011learning} \BX{in equirectangular form} and blur it with a Gaussian kernel.  We then identify the $P\times P$ pixel square with the highest total saliency value, and predict the snap angle as the center of the square. Unlike the other methods, this baseline is not iterative, \KG{since the maximal saliency region does not change with rotations.}  We use a window size $P=30$.
\BX{Performance is not sensitive to $P$ for $20\leq P \leq 200$.}
%\KGtwo{We use a window size $P=30$ because....EXPLAIN BRIEF....SENSITIVE TO THIS SETTING?}
\end{itemize}







\begin{figure}[t]
\centering
\renewcommand{\tabcolsep}{0pt}
\includegraphics[width=1\columnwidth]{images/rl_1.pdf}\vspace*{-0.2in}
%change to rl.eps for a single column version
\caption{Predicting snap angles in a timely manner.  Left: Given a budget, our method predicts snap angles with the least foreground disruption on cube edges.   \bo{Gains are larger for  smaller budgets, demonstrating our method's efficiency.}  Right: Our gain over the baselines \bx{(for a budget $T=4$)} as a function of the test cases' decreasing ``difficulty'', \cc{i.e., the variance in ground truth quality for candidate angles}.  See text.}
\label{fig:rl}
\vspace{-0.18in}
\end{figure}



We train our approach for a spectrum of budgets $T$, and report results in terms of \KG{the amount of foreground disruption} as a function of the budget.  Each unit of the budget corresponds to one round of rotating, re-rendering, and predicting foregrounds.  We score foreground disruption as the average  $F(P(I,\theta_t^*))$ across all four faces.

\bx{Figure~\ref{fig:rl} (left) shows the results. Our method achieves the least disruptions to foreground regions among all the competing methods.
\textsc{Uniform rotate} and \textsc{Coarse-to-fine search} perform better than \textsc{Random} because they benefit from hand-designed search heuristics.  % tailored for the data. 
%it samples snap angles evenly and inherits the \textsc{Canonical} human recording bias.  
Unlike \textsc{Uniform rotate} and \textsc{Coarse-to-fine search}, our approach is content-based and learns to trigger different search strategies depending on what it observes.
When $T=1$, \textsc{Saliency} is better than \textsc{Random} but it underperforms our method and \textsc{Uniform}. 
\KGthree{\textsc{Saliency} likely has difficulty capturing important objects in panoramas, since the saliency model is trained with standard field-of-view images.  }
Directly adapting \textsc{Pano2Vid}~\cite{su2016pano2vid} for our problem results in unsatisfactory results.
%Pano2Vid~\cite{su2016pano2vid} addresses a different problem: they create a \emph{normal FOV video}, whereas we create a well-oriented \emph{omnidirectional FOV image} (cubemap). 
A capture-worthiness classifier~\cite{su2016pano2vid} is relatively insensitive to the placement of important objects/people and therefore 
less suitable for the snap angle prediction task, which requires detailed modeling of object placement on \emph{all} faces of the cube.}





Figure~\ref{fig:rl} (right) plots our gains sorted by the test images' decreasing ``difficulty'' \bx{for a budget $T=4$}.  
In some test images, there is a \KG{high} variance, meaning certain snap angles are better than others.  However, for others,
%the %pixel objectness score for
\KGthree{all candidate rotations look similarly good}, in which case all methods will perform similarly. 
The righthand plot sorts the test images by their variance (in \KG{descending order}) in quality across all possible angles, and reports our method's gain as a function of that difficulty.  
%For each image, we therefore compute the standard deviation of pixel objectness among all possible snap angles and we sort the test cases in descending order. We want to demonstrate that for the data with high standard deviation of  pixel objectness (data with high potential to be improved) , our method can significantly outperforms other baselines. We fix $T=4$ and we plot the gain between our method and other baselines when different amount of test data, sorted by  standard deviation, are considered. See Figure~\ref{fig:.
Our method outperforms \textsc{P2V-adapted}, \textsc{Saliency}, \textsc{Coarse-to-fine search}, \textsc{Random} and \textsc{Uniform} by up to \bo{56\%, 31\%, 17\%, 14\% and 10\% (absolute),}
  %56.47\%, 31.0\%, 17.0\%,13.5\% and 9.7\%  in absolute scale,
respectively.   Overall Figure~\ref{fig:rl} demonstrates that our method predicts the snap angle more efficiently than the baselines.
  
We have thus far reported efficiency in terms of abstract budget usage.
One unit of budget entails the following: projecting 
a typical panorama of size $960\times 1920$ pixels in equirectangular form to a cubemap (8.67 seconds with our Matlab implementation) and then computing pixel objectness (0.57 seconds). Our prediction method is very efficient and takes $0.003$ seconds to execute for a budget $T=4$ with a GeForce GTX 1080 GPU.  
Thus, for a budget $T=4$, the savings achieved by our method is approximately \bx{2.4 minutes (5x speedup)} per image compared to exhaustive search. \KGthree{Note that due to our method's efficiency, even if the Matlab projections were 1000x faster for all methods, our 5x speedup over the baseline would remain the same.}    Our method achieves a good tradeoff between speed and accuracy.






\vspace{-10pt}

\subsection{Justification for Foreground Object Objective}\label{sec:preserve_object}


Next we justify empirically the pixel objectness cube-edge objective.  To this end, we have human viewers identify important objects in the source panoramas, then evaluate to what extent our objective preserves them. 

Specifically, we randomly select 340 frames among those where: 1) Each frame is at least 10-seconds apart from the rest in order to ensure diversity in the dataset; 2) The difference in terms of overall pixel objectness between our method and the canonical view method is non-neglible.  %exceeds a small threshold %(we set this to 4\%)
%to make sure we evaluate on frames where our method and canonical view method produce different projections.
We collect annotations via Amazon Mechanical Turk.   Following the interface of~\cite{hu2017deep}, we present crowdworkers the panorama and instruct them to label any ``important objects'' with a bounding box---as many as they wish.
See Supp.~for interface and annotation statistics.  



\bo{Here we consider \textsc{Pano2Vid(P2V)~\cite{su2016pano2vid}-adapted} and \textsc{Saliency} as defined in Sec.~\ref{sec:pred_angle} and two additional baselines:
1) \textsc{Canonical view}: produces a cubemap using the camera-provided orientation;
2) \textsc{Random view}: rotates the input panorama by an arbitrary angle and then generates the cubemap.}
%\begin{itemize}
%  \itemsep0em
%\item \textsc{Canonical view}: Produces a cubemap using the \KG{camera-provided} orientation.
%\item \textsc{Random view}: Rotates the input panorama by an arbitrary angle and then generates the cubemap.
%  \end{itemize}
\KGthree{Note that the other baselines in Sec.~\ref{sec:pred_angle} are not applicable here, since they are search mechanisms.}  

\BX{Consider the cube face $X$ that contains the largest number of foreground pixels from a given bounding box after projection.}
We evaluate the cubemaps of our method and the baselines based on the overlap score \KG{(IoU)} between
\BX{the foreground region from the cube face $X$ and the corresponding human-labeled important object,}
%\BX{Consider the cube face $X$ that contains the largest number of foreground pixels from a given bounding box after projection.} 
%We evaluate the cubemaps of our method and the baselines based on the overlap score \KG{(IoU)} between 
%\BX{the number of pixels from that largest cube face and the number of pixels from the human-labeled important objects in the entire cubemap,} 
%the human-labeled important objects and the maximum number of pixels that are in the same face, 
for each bounding box.  This metric is maximized when all pixels for the same object project to the same cube face; higher overlap indicates better preservation of important objects.



\begin{table}[t]
\centering
{\footnotesize \begin{tabular}{ |c|c|c|c|c|c||c|}
\hline

 & \textsc{Canonical} & \textsc{Random} & \textsc{Saliency}& \textsc{P2V-adapted}& \textsc{Ours} & \textsc{UpperBound}    \\ \hline

Concert  & 77.6\% & 73.9\% & 76.2\% & 71.6\% & \textbf{81.5\%} & 86.3\% \\ \hline
Ski      & 64.1\% & 72.5\% & 68.1\% & 70.1\% & \textbf{78.6\%} & 83.5\% \\ \hline
Parade   & 84.0\% & 81.2\% & 86.3\% & 85.7\% & \textbf{87.6\%} & 96.8\% \\ \hline
Disney   & 58.3\% & 57.7\% & 60.8\% & 60.8\% & \textbf{65.5\%} & 77.4\% \\ \hline
All      & 74.4\% & 74.2\% & 76.0\% & 75.0\% & \textbf{81.1\%} & 88.3\% \\ \hline
\end{tabular}}
\vspace{4pt}
\caption{Performance on preserving the integrity of objects explicitly identified as important by human observers.  Higher overlap scores are better. Our method outperforms \boc{all} baselines.} \vspace*{-0.2in}
\label{tab:imp_objects}

\vspace{-10pt}

\end{table}



Table~\ref{tab:imp_objects} shows the results.
Our method outperforms \bo{all} baselines by a large margin.
This supports our hypothesis that avoiding foreground objects along the cube edges helps preserve objects of interest to a viewer.  Snap angles achieve this goal much better than the baseline cubemaps.
%Our proposed method predicts the snap angle and can preserve important objects by minimizing pixel objectness on cube boundaries.
%The experiment that default cubemaps do not necessarily preserve object integrity. 
The \textsc{UpperBound} corresponds to the maximum possible overlap achieved if exhaustively evaluating \emph{all} candidate angles, and helps gauge the difficulty of each category.   Parade and Disney have the highest and lowest upper bounds, respectively.
In Disney images, the camera is often carried by the recorders, so important objects/persons appear relatively large in the panorama and cannot fit in a single cube face, hence a lower upper bound score.
On the contrary, in Parade images the camera is often placed in the crowd and far away from important objects, so each can be confined to a single face.
The latter also explains why the baselines do best (though still weaker than ours) on Parade images. 
\boc{An ablation study decoupling the pixel objectness performance from snap angle performance pinpoints the effects of foreground quality on our approach (see Supp.).}







\vspace{-10pt}
\subsection{User Study: Perceived Quality}\label{sec:human}








\begin{table}[t]
\centering

{\footnotesize \begin{tabular}{ cccc||ccc}
\toprule

& Prefer \textsc{Ours} &Tie& Prefer \textsc{Canonical} & Prefer \textsc{Ours} &Tie& Prefer \textsc{Random}  \\ \midrule
Parade & 54.8\%& 16.5\% &  28.7\%   & 70.4\%& 9.6\% &  20.0\%\\
Concert &48.7\%&16.2\%  &35.1\%     &52.7\%&16.2\%  &31.1\% \\
Disney &44.8\%&17.9\%  &37.3\%      &72.9\%&8.5\%  &18.6\%\\
Ski &64.3\%&8.3\%  &27.4\%          &62.9\%&16.1\%  &21.0\%\\
All &53.8\%&14.7\%&31.5\%           &65.3\%&12.3\%&22.4\%\\
\bottomrule

\end{tabular}}
\vspace{4pt}
\caption{User study result comparing cubemaps outputs for perceived quality. \bx{Left: Comparison between our method and \textsc{Canonical}. Right: Comparison between our method and \textsc{Random}.}}



\label{tab:human_study}
\vspace{-15pt}
\end{table}






\begin{table}[t]

\centering
{\scriptsize \begin{tabular}{ |c|c|c|c|c|c|}
\hline

   & Concert           & Ski              & Parade                  & Disney                         &  All (normalized)    \\ \hline
\multicolumn{6}{|c|}{Image Memorability~\cite{ICCV15_Khosla}} \\ \hline 
\textsc{Canonical} & \textbf{71.58}    & 69.49          & 67.08          & 70.53                 & 46.8\%                 \\ \hline
\textsc{Random}    & 71.30             & 69.54          & 67.27          &70.65                 &  48.1\%                 \\ \hline 
\textsc{Saliency} & 71.40    & 69.60          & 67.35         & 70.58                 & 49.9\%                 \\ \hline
\textsc{P2V-adapted}    & 71.34             & 69.85          & 67.44          &70.54                 &  52.1\%                 \\ \hline
\textsc{Ours}      & 71.45             & \textbf{70.03} & \textbf{67.68} & \textbf{70.87}        &  \textbf{59.8\%} \\ \hline\hline
\textsc{Upper}     & 72.70             & 71.19          & 68.68          & 72.15                 &  --        \\ \hline
\multicolumn{6}{|c|}{Image  Aesthetics~\cite{kong2016aesthetics}}  \\ \hline
\textsc{Canonical} & 33.74             & 41.95           & 30.24          & 32.85                 &  44.3\% \\ \hline
\textsc{Random}    & 32.46             & 41.90           & 30.65          & 32.79                 &  42.4\% \\ \hline
\textsc{Saliency} & 34.52    & 41.87          & 30.81          & 32.54                 & 47.9\%                 \\ \hline
\textsc{P2V-adapted}    & 34.48             & 41.97          & 30.86          &\textbf{33.09}                 &  48.8\%                 \\ \hline
\textsc{Ours}      & \textbf{35.05}    & \textbf{42.08}  & \textbf{31.19}  & 32.97      &  \textbf{52.9\%} \\ \hline\hline
\textsc{Upper}     & 38.45                  & 45.76                & 34.74               & 36.81                      &  --    \\ \hline
\end{tabular}}

\vspace{4pt}
\caption{Memorability and aesthetics scores.}

\label{tab:img_pro}
\vspace{-25pt}
\end{table}





\begin{figure*}[t!]
\centering
\renewcommand{\tabcolsep}{0pt}
\includegraphics[width=1\columnwidth]{images/qual_eccv.pdf}%change to qual_pdf for a larger one
\caption{Qualitative examples of default \textsc{Canonical} cubemaps and our snap angle cubemaps.  Our method produces cubemaps that place important objects/persons in the same cube face to preserve the foreground integrity. Bottom two rows show failure cases. In the bottom left, pixel objectness~\cite{jain2017pixel} does not recognize the round stage as foreground, and therefore our method splits the stage onto two different cube faces, creating a distorted heart-shaped stage. \boc{In the bottom right, the train is too large to fit in a single cube.}}
\label{fig:qual}
\end{figure*}






\KGthree{Having justified the perceptual relevance of the cube-edge foreground objective (Sec.~\ref{sec:preserve_object})}, next we perform a user study to gauge perceptual quality of our results.  Do snap angles produce cube faces that look like human-taken photos?
We evaluate on the same image set used in Sec.~\ref{sec:preserve_object}.


We present cube faces produced by our method and one of the baselines at a time in arbitrary order and inform subjects the two sets are photos from the same scene but taken by different photographers. We instruct them to consider composition and viewpoint in order to decide which set of photos is more pleasing (see Supp.).  
To account for the subjectivity of the task, we issue each sample to 5 distinct workers and aggregate responses with majority vote.  $98$ unique MTurk crowdworkers participated in the study.



\bx{Table~\ref{tab:human_study} shows the results. Our method outperforms the \textsc{Canonical} baseline by more than 22\% and the \textsc{Random} baseline by 42.9\%.} 
This result supports our claim that by preserving object integrity, our method produces cubemaps that align better with human perception of quality photo composition.  
Figure~\ref{fig:qual} shows qualitative examples.  As shown in the first two examples (top two rows), our method is able to place an important person in the same cube face whereas the baseline splits each person and projects a person onto two cube faces.  We also present two failure cases in the last two rows. In the bottom left, pixel objectness does not recognize the stage as foreground, and therefore our method places the stage on two different cube faces, creating a distorted heart-shaped stage. \boc{Please see Supp. for pixel objectness map input for failure cases.}






So far, Table~\ref{tab:imp_objects} confirms empirically that our foreground-based objective does preserve those objects human viewers deem important, and Table~\ref{tab:human_study} shows that human viewers have an absolute preference for snap angle cubemaps over other projections. 
%\boc{The human subject study is the most conclusive and direct metric.}
As a final test of snap angle cubemaps' perceptual quality, we score them using state-of-the-art metrics for \emph{aesthetics}~\cite{kong2016aesthetics} and \emph{memorability}~\cite{ICCV15_Khosla}.
Since both models are trained on images annotated by people (for their aesthetics and memorability, respectively), higher scores indicate higher correlation with these perceived properties (though of course no one learned metric can perfectly represent human opinion).

%We compare with both \textbf{canonical view}  and \textbf{random view} baselines. 
Table~\ref{tab:img_pro} shows the results.
We report the raw scores $s$ per class as well as the score over all classes, normalized as $\frac{s-s_{min}}{s_{max}-s_{min}}$, where $s_{min}$ and $s_{max}$ denote the lower and upper bound, respectively.
Because the metrics are fairly tolerant to local rotations, there is a limit to how well they can capture subtle differences in cubemaps.  Nonetheless, our method outperforms the baselines overall. 
\boc{Given these metrics' limitations, the user study in Table~\ref{tab:human_study} offers the most direct and conclusive evidence for snap angles' perceptual advantage. }







\vspace{-5pt}


\subsection{Cubemap Recognition from Pretrained Nets}\label{sec:recognition}







\begin{table}[t]\centering
\begin{tabular}{c|ccc}\toprule
 &   \textsc{Canonical}   &   \textsc{Random}       &   \textsc{Ours} \\ \midrule
Single  &   68.5       & 69.4         &  \textbf{70.1}        \\
Pano    &   66.5       & 67.0         & \textbf{ 68.1}         \\
\bottomrule
\end{tabular}
\vspace{4pt}
\caption{Image recognition accuracy (\%). Snap angles help align the 360$^{\circ}$ data's statistics with that of normal FOV Web photos, enabling easier transfer from conventional pretrained networks.}
\label{tab:reg_google}
\vspace{-25pt}
\end{table}





Since snap angles provide projections that better mimic human-taken photo composition, we hypothesize that they also align better with conventional FOV images, compared to cubemaps in their canonical orientation.  This suggests that snap angles may better align with Web photos (typically used to train today's recognition systems), which in turn could help standard recognition models perform well on 360$^{\circ}$ panoramas.  We present a preliminary proof-of-concept experiment to test this hypothesis.



We train a multi-class CNN classifier to distinguish the four activity categories in our 360$^{\circ}$ dataset (Disney, Parade, etc.).  The classifier uses ResNet-101~\cite{he2016deep} pretrained on ImageNet~\cite{russakovsky2015imagenet} and fine-tuned on 300 training images per class downloaded from Google Image Search (see Supp.).
Note that in all experiments until now, the category labels on the 360$^{\circ}$ dataset were invisible to our algorithm.
We randomly select 250 panoramas per activity as a test set.   Each panorama is projected to a cubemap with the different projection methods, and we compare the resulting recognition rates.



Table~\ref{tab:reg_google} shows the results.  We report recognition accuracy in two forms: \emph{Single}, which 
treats each individual cube face as a test instance, and \emph{Pano}, which classifies the entire panorama by multiplying the predicted posteriors from all cube faces.  For both cases, snap angles produce cubemaps that achieve the best recognition rate. 
%Note both methods are tested on the same classifier so this demonstrate that our methods produces cubemap that can better preserve activity identity. Our method can also be used to improve activity recognition.
\boc{This result hints at the potential for snap angles to be a bridge between pretrained normal FOV networks on the one hand and 360$^{\circ}$ images on the other hand. \KGtwo{That said, the margin is slim, and} the full impact of snap angles for recognition warrants further exploration.  }






