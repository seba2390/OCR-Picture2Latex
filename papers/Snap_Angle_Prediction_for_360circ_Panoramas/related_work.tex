\section{Related Work} \label{sec:related}

\paragraph{Spherical image projection}

Spherical image projection models project either a limited FOV~\cite{sharpless2010pannini,chang2013rectangling} or the entire panorama~\cite{snyder1997flattening,zelnik2005squaring,greene1986environment}.  The former group includes rectilinear and Pannini~\cite{sharpless2010pannini} projection; the latter includes equirectangular, stereographic, and Mercator projections (see~\cite{snyder1997flattening} for a review).  \BX{Rectilinear and Pannini} %and others~\cite{chang2013rectangling} 
prioritize preservation of lines in various ways, \KG{but always independent of the specific input image}. Since any projection of the full sphere must incur distortion, multi-view projections can be perceptually stronger than a single global projection~\cite{zelnik2005squaring}.  Cubemap~\cite{greene1986environment}, the subject of our snap angle approach, is a multi-view projection method; as discussed above, current approaches simply consider a cubemap in its default orientation.






\vspace*{-0.1in}
\paragraph{Content-aware projection}
Built on spherical projection methods, content-based projections make \KG{image-specific choices} to reduce distortion.
%Automatic content-based methods~\cite{kim2017automatic} detect straight lines and scene saliency to identify important objects to preserve.
Recent work~\cite{kim2017automatic} optimizes the parameters in the Pannini projection~\cite{sharpless2010pannini} to preserve regions with greater low-level saliency and straight lines.  Interactive methods~\cite{carroll2009optimizing,tehrani2016correcting,carroll2010image,kopf2009locally} require a user to outline regions of interest that should be preserved
\BX{or require input from a user to determine projection orientation~\cite{wang2015panorama}.}
%However, it is computationally expensive and requires non-trivial human efforts to specifying straight lines to be preserved.  
Our approach is content-based \KGthree{and fully automatic}.  Whereas prior automatic methods assume a fixed viewpoint for projection, we propose to actively predict snap angles for rendering.  
%In a similar spirit to content-based projection method, we also want to perform content-based rendering. Unlike previous approach that assumes a fixed projection viewpoint, we propose to actively predict snap angles for rendering. In fact, our proposed method is not limited to a single projection method and any advanced projection method can be easily incorporated into our framework. 
Thus, our idea is orthogonal to 360$^{\circ}$ content-aware projection.  Advances in the projection method could be applied in concert with our algorithm, \KG{e.g., as post-processing to enhance the rotated faces further.}  For example, when generating cubemaps, one could replace rectilinear projection with others~\cite{sharpless2010pannini,kim2017automatic,carroll2009optimizing} and keep the rest of our \BX{learning} framework unchanged.
Furthermore, the proposed snap angles respect high-level image content---detected foreground objects---as opposed to typical lower-level cues like line straightness~\cite{carroll2010image,carroll2009optimizing} or low-level saliency metrics~\cite{kim2017automatic}.

\vspace*{-0.1in}
\paragraph{Viewing wide-angle panoramas}
%quickly attracts more interests in the research community thanks to the growing of 360$^{\circ}$ cameras and VR technology.
Since viewing 360$^{\circ}$ and wide-angle data is non-trivial, there are vision-based efforts to facilitate.  The system of~\cite{kopf2007capturing} helps efficient exploration of gigapixel panoramas.  
%Gigapixel~\cite{kopf2007capturing} presents a system to capture and view very high resolution, high dynamic range, and wide angle imagery. 
More recently, several systems automatically extract normal FOV videos from 360$^{\circ}$ video, ``piloting'' a virtual camera by selecting the viewing angle and/or zoom level most likely to interest a human viewer~\cite{su2016pano2vid,su2017making,hu2017deep,lai2017semantic}.






\vspace*{-0.1in}
\paragraph{\KG{Recurrent networks for attention}}


\KG{Though treating very different problems than ours, multiple recent methods incorporate deep recurrent neural networks (RNN) to make sequential decisions about where to focus attention.}  The influential work of~\cite{mnih2014recurrent}
  learns a policy for visual attention in image classification.  \boc{Active perception systems use RNNs and/or reinforcement learning to select places to look in a novel image~\cite{caicedo2015active,mathe2016reinforcement}, environment~\cite{jayaraman2016look,jayaraman2017learning,jayaraman2018end}, or video~\cite{yeung2016end,alwassel2017action,singh2016multi,su2016leaving} to detect certain objects or activities efficiently.} Broadly construed, we share the general goal of efficiently converging on a desired target ``view'', but our problem domain is entirely different.
  









  
