\section{Supplementary Material}







\subsection{Justification for predicting snap angle in azimuth only}

This section accompanies Sec. 3.1 in the main paper.

As discussed in the paper, views from a horizontal camera position (elevation 0$^{\circ}$) are typically more plausible than other elevations due to the human recording bias.  The human photographer typically holds the camera with the ``top" to the sky/ceiling.  

Figure~\ref{fig:supp_elevation} shows examples of rotations in elevation for several panoramas.
We see that the recording bias makes the 0 (canonical) elevation fairly good as it is.  In contrast, rotation in elevation often makes the cubemaps appear tilted (e.g., the building in the second row). Without loss of generality, we focus on snap angles in azimuth only, and jointly
optimize the front/left/right/back faces of the cube.






\begin{figure*}[t!]
\centering
\renewcommand{\tabcolsep}{0pt}
\includegraphics[width=1\columnwidth]{images/eccv_supp.pdf}%change to qual_pdf for a larger one

\caption{Example of cubemaps when rotating in elevation. Views from a horizontal camera position (elevation 0$^{\circ}$) are more informative than others due to the natural human recording bias. In addition, rotation in elevation often makes cubemap faces appear tilted (e.g., building in the second row).  Therefore, we optimize for the azimuth snap angle only.}
\label{fig:supp_elevation}
\end{figure*}


%\clearpage





\subsection{Details on network architecture}

This section accompanies Sec. 3.2 in the main paper.

\begin{figure*}[t]
\centering
\renewcommand{\tabcolsep}{0pt}
\includegraphics[width=0.95\columnwidth]{images/supp_net.pdf}%change to qual_pdf for a larger one
\caption{A detailed diagram showing our network architecture. In the top, the small schematic shows the connection between each network module. Then we present the details of each module in the bottom. Our network proceeds from left to right. The \textit{feature extractor} consists of a sequence of convolutions (with kernel size and convolution stride written under the diagram) followed by a fully connected layer. In the bottom, ``FC'' denotes a fully connected layer and ``ReLU'' denotes a rectified linear unit. The \textit{aggregator} is a recurrent neural network. The ``Delay'' layer stores its current internal hidden state and outputs them at the next time step. In the end, the \textit{predictor} samples an action stochastically based on the multinomial pdf from the Softmax layer.}
\label{fig:supp_net}
\end{figure*}
    





Recall that our framework consists of four modules: a \textit{rotator}, a \textit{feature extractor}, an \textit{aggregator}, and a \textit{snap angle predictor}. At each time step, it processes the data and produces a cubemap (\textit{rotator}), extracts learned features (\textit{feature extractor}), integrates information over time (\textit{aggregator}), and predicts the next snap angle (\textit{snap angle predictor}). We show the details of each module in Figure~\ref{fig:supp_net}.



\subsection{Training details for \textsc{Pano2Vid(P2V)~\cite{su2016pano2vid}-adapted}}\label{sec:pano}

This section accompanies Sec 4.1 in the main paper.

For each of the activity categories in our 360$^{\circ}$ dataset (Disney, Parade, etc.), 
we query Google Image Search engine and then manually filter out irrelevant images. We obtain about $300$ images for each category.   
We use the Web images as positive training samples and randomly sampled panorama subviews as negative training sampling.

\clearpage

\subsection{Interface for collecting important objects/persons}
 
This section accompanies Sec. 4.2 in the main paper.

We present the interface for Amazon Mechanical Turk that is used to collect important objects and persons. The interface is developed based on~\cite{profx}. We present crowdworkers the panorama and instruct them to label any important objects with a bounding box---as many as they wish. A total of $368$ important objects/persons are labeled. The maximum number of labeled important objects/persons for a single image is $7$.


\begin{figure*}[t]
\centering
\renewcommand{\tabcolsep}{0pt}
\includegraphics[width=0.95\columnwidth]{images/supp_label_object.pdf}%change to qual_pdf for a larger one
\caption{Interface for Amazon Mechanical Turk when collecting important objects/persons.
We present crowdworkers the panorama and instruct them to label any important objects with a bounding box---as many as they wish.}
\label{fig:supp_important}
\end{figure*}




\subsection{More results when ground-truth objectness maps are used as input}
\bo{This section accompanies Sec. 4.2 in the main paper.}



  
\bo{We first got manual labels for the pixel-wise foreground for 50 randomly selected panoramas ($200$ faces). The IoU score between the pixel objectness prediction and ground truth is 0.66 with a recall of 0.82, whereas alternate foreground methods we tried \cite{5432215,jiang2013saliency} obtain only 0.35-0.40 IoU and 0.67-0.74 recall  on the same data.}
 
 \bo{We then compare results when either ground-truth foreground or pixel objectness is used as input (without re-training our model) and report the foreground disruption with respect to the ground-truth. ``Best possible" is the oracle result. Pixel objectness serves as a good proxy for ground-truth foreground maps. Error decreases with each rotation. Table~\ref{tab:gt} pinpoints to what extent having even better foreground inputs would also improve snap angles.}
 
 
 
 
 
 
 
 
 \begin{table}[t]
\centering
 \begin{tabular}{ |c|c|c|c|c|c|c|c|}
\hline

Budget (T)  & 2 & 4 & 6 & 8  & 10 & Best Possible & \textsc{Canonical}    \\ \hline
Ground truth input & 0.274 & 0.259 & 0.248 & 0.235 & 0.234 & 0.231 & 0.352\\ \hline
Pixel objectness input & 0.305 & 0.283 & 0.281 & 0.280  & 0.277 & 0.231 & 0.352\\ \hline



\end{tabular}

\vspace{4pt}
\caption{Decoupling pixel objectness and snap angle performance: 
 error (lower is better) when ground truth or pixel objectness is used as input.}
\label{tab:gt}
\end{table}

 
 
 


\subsection{Interface for user study on perceived quality}

This section accompanies Sec. 4.3 in the main paper.

We present the interface for the user study on perceived quality in Figure~\ref{fig:supp_human}. Workers were required to rate each image into one of five categories: (a) The first set is significantly better, (b) The first set is somewhat better, (c) Both sets look similar, (d) The second set is somewhat better and (e) The second set is significantly better. We also instruct them to avoid to choose option (c) unless it is really necessary. Since the task can be ambiguous and subjective, we issued each task to 5 distinct workers. Every time a comparison between the two sets receives a rating of category (a), (b), (c), (d) or (e) from any of the 5 workers, it receives 2, 1, 0, -1, -2 points, respectively. We add up scores from all five workers to collectively decide which set is better.


\begin{figure*}[t]
\centering
\renewcommand{\tabcolsep}{0pt}
\includegraphics[width=0.95\columnwidth]{images/supp_human.pdf}%change to qual_pdf for a larger one
%\vspace{-10pt}
\caption{Interface for user study on perceived quality. Workers were required to rate each image into one of five categories. We issue each sample to 5 distinct workers.}
\label{fig:supp_human}
\end{figure*}











\subsection{The objectness map input for failure cases}
\bo{This section accompanies Sec. 4.3 in the main paper.}

\bo{Our method fails to preserve object integrity if pixel objectness fails to recognize foreground objects. Please see Fig.~\ref{fig:fail} for examples. The round stage is not found in the foreground, and so ends up distorted by our snap angle prediction method. In addition, the current solution cannot effectively handle the case when a foreground object is too large to fit in a single cube face. 
}



\begin{figure}

\centering
\renewcommand{\tabcolsep}{0pt}
\includegraphics[width=1\columnwidth]{images/snap_angle_fail_pixelmap.pdf}%change to rl.eps for a single column version

\caption{\bo{Pixel objectness map (right) for failure cases of snap angle prediction. In the top row, the round stage is not found in the foreground, and so ends up distorted by our snap angle.}}
\label{fig:fail}
\end{figure}

\clearpage








\subsection{Additional cubemap output examples}

This section accompanies Sec. 4.3 in the main paper.

Figure~\ref{fig:supp_qual1} presents additional cubemap examples.  
Our method produces cubemaps that place important objects/persons in the same cube face to preserve the foreground integrity. For example, in the top right of Figure~\ref{fig:supp_qual1}, our method places the person in a single cube face whereas the default cubemap splits the person onto two different cube faces.

Figure~\ref{fig:supp_qual3} shows failure cases.
A common reason for failure is that pixel objectness~\cite{jain2017pixel} does not recognize some important objects as foreground. For example, in the top right of Figure~\ref{fig:supp_qual3}, our method creates a distorted train by
splitting the train onto three different cube faces because pixel objectness does not recognize the train as foreground.



\begin{figure*}[t]
\centering
\renewcommand{\tabcolsep}{0pt}
\includegraphics[width=1\columnwidth]{images/supp_s1.pdf}%change to qual_pdf for a larger one
\vspace{-10pt}
\caption{Qualitative examples of default \textsc{Canonical} cubemaps and our snap angle cubemaps.  Our method produces cubemaps that place important objects/persons in the same cube face to preserve the foreground integrity.}
\label{fig:supp_qual1}
\end{figure*}








\begin{figure*}[t]
\centering
\renewcommand{\tabcolsep}{0pt}
\includegraphics[width=1\columnwidth]{images/supp_failure.pdf}%change to qual_pdf for a larger one
\caption{Qualitative examples of default \textsc{Canonical} cubemaps and our snap angle cubemaps. We show failure cases here. In the top left, pixel objectness~\cite{jain2017pixel} does not recognize the stage as foreground, and therefore our method splits the stage onto two different cube faces, creating a distorted stage. In the top right, our method creates a distorted train by
splitting the train onto three different cube faces because pixel objectness does not recognize the train as foreground.}
\label{fig:supp_qual3}
\end{figure*}


\subsection{Setup details for recognition experiment}

This section accompanies Sec. 4.4 in the main paper.

Recall our goal for the recognition experiment is to distinguish the four activity categories in our 360 dataset (Disney, Parade, etc.). We build a training dataset by querying Google Image Search engine for each activity category and then manually filtering out irrelevant images.  These are the same as the positive images in Sec.~\ref{sec:pano} above.

We use Resnet-101 architecture~\cite{he2016deep} as our classifier. The network is first pre-trained on Imagenet~\cite{russakovsky2015imagenet} and then we finetune
 it on our dataset with SGD and a mini-batch size of 32. The learning rate starts from 0.01 with a weight decay of 0.0001 and a momentum of 0.9. We train the network 
until convergence and select the best model based on a validation set.



