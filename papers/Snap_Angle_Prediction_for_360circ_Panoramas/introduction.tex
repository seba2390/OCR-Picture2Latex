\section{Introduction}\label{sec:introduction}


The recent emergence of inexpensive and lightweight 360$^{\circ}$ cameras enables exciting new ways to capture our visual surroundings.
Unlike traditional cameras that capture only a limited field of view, 360$^{\circ}$ cameras capture the entire visual world from their optical center.  %In addition to simple usage and affordable accessibility of 360$^{\circ}$ cameras , the growing
\boc{Advances in virtual reality  technology and promotion from social media platforms like YouTube and Facebook are further boosting the relevance of 360$^{\circ}$ data.}


However, viewing 360$^{\circ}$ content presents its own challenges.  
%To automatically facilitate high-quality viewing of 360$^{\circ}$ imagery, there are
Currently three main directions are pursued: manual navigation, field-of-view (FOV) reduction, and content-based projection.
In manual navigation scenarios, a human viewer chooses which normal field-of-view subwindow to observe, e.g., via continuous head movements in a VR headset, or mouse clicks on a screen viewing interface.  
In contrast, FOV reduction methods generate normal FOV videos by learning to render the most interesting or capture-worthy portions of the viewing \boc{sphere}~\cite{su2016pano2vid,su2017making,hu2017deep,lai2017semantic}.  While these methods relieve the decision-making burden of manual navigation, they severely limit the information conveyed by discarding all unselected portions.  
%Current viewing tools that render a normal-field-of-view often require users to select a viewing angle through a sequence of mouse clicks or continuous head movements with a VR headset.
Projection methods render a wide-angle view, or the entire sphere, onto a single plane (e.g., equirectangular or Mercator)~\cite{snyder1997flattening} or multiple planes~\cite{greene1986environment}.  While they avoid discarding content, any projection inevitably introduces distortions that can be unnatural for viewers.  Content-based projection methods can help reduce perceived distortions by prioritizing preservation of straight lines, conformality, or other low-level cues~\cite{sharpless2010pannini,kim2017automatic,li2015geodesic}, optionally using manual input to know what is worth preserving~\cite{carroll2009optimizing,tehrani2016correcting,carroll2010image,kopf2009locally,wang2015panorama}.




\begin{figure}[t]
\centering
\renewcommand{\tabcolsep}{0pt}
\includegraphics[width=1\columnwidth]{images/intro1.pdf}%change to network_2 for a single column version
\caption{Comparison of a cubemap before and after snap angle prediction (dotted lines separate each face). 
Unlike prior work that assumes a fixed angle for projection, we propose to predict the cube rotation that will best preserve foreground objects in the output.  For example, here our method better preserves
the truck (third picture C in the second row).  We show four (front, right, left, and back) out of the six faces for visualization purposes. Best viewed in color or pdf.}%Our goal is to predict snap angle so the resulting cubemap can preserve object integrity.}
\label{fig:intro}
\vspace{-13pt}
\end{figure}


However, all prior \BX{automatic} content-based projection methods implicitly assume that the \emph{viewpoint} of the input 360$^{\circ}$ image is fixed.  That is, the spherical image is processed in some default coordinate system, e.g., \KG{as the equirectangular projection provided by the camera manufacturer.} This assumption limits the quality of the output image.  Independent of the content-aware projection eventually used, a fixed viewpoint means some \emph{arbitrary portions of the original sphere will be relegated to places where distortions are greatest}---or at least where they will require most attention by the content-aware algorithm to ``undo''.  

We propose to eliminate the fixed viewpoint assumption.  Our key insight is that an intelligently chosen viewing angle can immediately lessen distortions, even when followed by a conventional projection approach.  In particular, we consider the widely used \boc{cubemap} projection~\cite{greene1986environment,fb2015cubemap,google2017eac}.  A cubemap visualizes the entire sphere by first mapping the sphere to a cube with rectilinear projection (where each face captures a 90$^{\circ}$ FOV) and then unfolding the faces of the cube.  Often, an important object can be projected across two cube faces, destroying object integrity.  In addition, rectilinear projection distorts content near cube face boundaries more.  See Figure~\ref{fig:intro}, top.  However, intuitively, some viewing angles---some cube orientations---are less damaging than others.

We introduce an approach to automatically predict \emph{snap angles}: the rotation of the cube that will yield a set of cube faces that, among all possible rotations, most look like nicely composed human-taken photos originating from the given 360$^{\circ}$ panoramic image.   While what comprises a ``well-composed photo'' is itself the subject of active research~\cite{kong2016aesthetics,isola2011makes,xiong2014detecting,gygli2013interestingness,ICCV15_Khosla}, we concentrate on a high-level measure of good composition, where the goal is to consolidate each (automatically detected) foreground object within the bounds of one cubemap face.   See Figure~\ref{fig:intro}, bottom.

Accordingly, we formalize our snap angle objective in terms of minimizing the spatial mass of foreground objects near cube edges.  We develop a reinforcement learning (RL) approach to infer the optimal snap angle given a 360$^{\circ}$ panorama.  We implement the approach with a deep recurrent neural network that is trained end-to-end.  The sequence of rotation ``actions'' chosen by our RL network can be seen as a \boc{\emph{learned}} coarse-to-fine adjustment of the camera viewpoint, in the same spirit as how people refine their camera's orientation just before snapping a photo.




 



We validate our approach on a variety of 360${^\circ}$ panorama \KGthree{images}.  Compared to several informative baselines, we demonstrate that  1) snap angles better preserve important objects, 2) our RL solution efficiently pinpoints the best snap angle, 3) cubemaps unwrapped after snap angle rotation suffer less perceptual distortion than the status quo cubemap, and 4) snap angles even have potential to impact recognition applications, by orienting 360$^{\circ}$ data in ways that better match the statistics of normal FOV photos used for today's pretrained recognition networks.








