\section{Results and Discussion}
We present results separately for factual change detection, question generation and the full system. We also report results separately for the overall performance and the performance on only the subset of questions that are human written; those sentences labeled as Edit or Context in the annotation phase. Selected examples with model outputs are provided in \Cref{sec:appendix:failure} to illustrate the capabilities and typical errors baseline.
%In general, after observing that question generation performance seems to degrade on the human written questions.

\subsection{Factual Change Detection}
\label{subsec:results:fact}
\input{tab/diff_detection}

\autoref{tab:diff} compares the performance of various systems on the factual change detection task.
We find that QA-equivalence performs better than heuristic baseline methods. In particular, it better handles cases where the answer span text is unchanged, but the surrounding context has changed. For example, in the passage \textit{``On the New Hampshire Executive Council, Laconia is in the 1st District, represented by <ADD: Republican Joe Kenney> <DEL: Democrat Michael J. Cryans>."}, QA-equivalence correctly captures the new information associated with the answer span \textit{``New Hampshire Executive Council"} in the form of the question \textit{``What state council does Joe Kenney represent Laconia in?"} 					
% ``The Rose Bowl, \emph{also known as Spieker Field at the Rose Bowl}, is an American outdoor athletic stadium..'', QA-equivalence correctly captures the new information associated with the answer span \emph{"Rose Bowl"} in the form of the question \emph{"Where is the Spieker Field stadium located?"}.
However, the method is prone to detecting spurious changes even when the passages have no semantic edit as illustrated in \Cref{sec:appendix:failure}.




The joint system finetuned on silver data from QA-equivalence does not seem to improve upon QA-equivalence. 
% (see \autoref{tab:overall}), 
While it seemingly benefits from the additional context, it still struggles with long and complex edits. However, this model only requires a single inference to do both tasks.

The \vitaminc{} trained model, despite having access to additional data, was also unable to improve on our baseline. \vitaminc{} style edits are substantially different than \dataset{} edits, generally only consisting of small changes. Thus, the model finetuned on \vitaminc{} performs poorly on large phrase changes or sentence refactors. 
% It performed reasonably only after substantial tinkering with the sampling method for negative examples. 


\subsection{Question Generation}
\input{tab/question_gen}

In \autoref{tab:qg}, we compare our question generation baseline models on the subset of the positive examples. In \autoref{tab:qg_edited_tp}, we examine the same models on the subset of those that are human written: examples with a change from \Cref{tab:data}.

% \input{tab/qgen_all}
\input{tab/edited_tp}


The primary goal of this evaluation is to test whether the questions directly produced by the seed model described in \Cref{sec:seed} are still useful for evaluating systems on \dataset{}. We find from sampling from that same model and from retraining with the same process (as described in \Cref{Method:Question Generation}) that performance on the overall set degrades considerably. This suggests that unless someone had access to the same model, these questions that are human-verified but not human written can still be useful for evaluation.
%
Nevertheless, the seed model can be thought of as a rough ceiling on current question generation performance on \dataset{}.

The human written questions (see \autoref{tab:qg_edited_tp}) seem to be much more challenging for the question generation models to replicate. Performance degrades substantially: naturally it degrades the most for the seed model that wrote some of the questions in the overall dataset, which it should exactly match. 

We note also that a question generation model finetuned on Natural Questions  \citep{kwiatkowski-etal-2019-natural} yields a significantly different question style than SQuaD. This is likely because SQuAD questions are originally generated from passages, while Natural Questions are more free form. In addition the  Natural Questions  model is found to hallucinate in numerous scenarios.
This reflects on the poor performance of the Natural Questions-trained question generation model on \dataset{}. % as also observed in TODO: add reference


%\pj{re-seeded is a bit confusing. maybe we can call this re-trained} also substantially lowers performance. 
As a caveat, the possible universe of questions written to summarize a factual change can be very large. While restricting to a single answer span reduces this space, we still find scenarios with multiple valid questions. Thus, there may be some disagreements where the model generates a completely valid question that is simply not the most pertinent one according to our annotators. 




% Note that sampling instead of greedy decoding or re-training with a different seed \pj{re-seeded is a bit confusing. maybe we can call this re-trained} also substantially lowers performance, as expected, but that the SQuAD trained model still has a substantial gap over the Natural Questions trained model. The original seeded model is close to a ceiling on what we may expect question generation systems to presently be capable of. As can be seen, there is still a large gap with what questions humans would write, even when they are biased by the initial model generated question. 

% As discussed in \autoref{Data Collection} the possible universe of questions for a factual change can be very large. While restricting to an answer span reduces this space, we still find scenarios with multiple valid questions. Thus, there may be some cases where the model generates a completely valid question that is simply not the most pertinent one. 



% For all the experiments where a separate question generation model is required, we use a SQuaD model trained in exactly the same manner as the model we use for seeding the annotations, but with a different seed.
% this setup has an unfair advantage as about 1/3 of the questions would be an exact match. However, the procedure allows us to estimate an upper bound on the \dataset{} performance. 

% As expected, the joint systems with a different question generation technique yield lower performance compared to \textit{Q-Dedup}.


\subsection{Full System}
\input{tab/main_results}

Full \dataset{} metrics are presented in \autoref{tab:overall} and include the two finetuned systems that are trained on \vitaminc{} and QA-equivalence, respectively, as well as two pipelined systems with factual change detection models attached to a question generation model. For the pipelined experiments, we use the retrained SQuaD model described in \Cref{Method:Question Generation}. We evaluate these models on the full \dataset{} as well as human written subset.  

Overall, all of the systems are relatively close in performance. QA-equivalence works the best, with the finetuned version and simple heuristic model close behind, indicating substantial room for future innovation. 
%Overall, our best proposed system QA-equivalence performs comparably to the heuristic baseline on the task indicating sufficient room for improvement. Moreover, the finetuned systems lag behind on both factual change detection and question generation.
On the human written subset, the performance drops significantly further highlighting the challenge of the human written questions.  


% We also evaluate various baselines on the subset of \dataset{} questions that have been re-written by annotators i.e labelled as either \textit{Edit} or \textit{Context}. This consists of 494 questions in all. 
% Also note that here \textit{Q-Dedup} gains a significant margin over the heuristic baseline.


