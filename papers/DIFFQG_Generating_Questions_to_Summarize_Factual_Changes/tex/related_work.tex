\section{Related Work}

\paragraph{Factual Edits}
% Understanding the pieces of information that have changed between two passages is a key requirement for fact verification tasks such as Fever \citep{thorne-etal-2018-Fever}.
Factual change detection has been of recent interest to the community. For instance, \wikiatomicedits{} \citep{faruqui-etal-2018-wikiatomicedits} rely on Wikipedia revisions to learn to discriminate factual edits. Closest to our work is \vitaminc{} \citep{schuster-etal-2021-get} which aims to generate a discriminating claim given a pair of edited sentences. However, both of these datasets primarily rely on smaller edits, frequently consisting of a single entity or number substitution. For instance, \vitaminc{} examples have a median of four token changes and \wikiatomicedits{} examples have a median of two token changes. Moreover, these edits are easier to detect using heuristics such as  noun or entity overlap. On the other hand, \dataset{} examples have a median of thirteen token changes that can involve  multiple entity updates. Further, the surrounding contextual information for an entity could be updated even when the entity itself is present in both passages. This makes \dataset{} edits harder to summarize and substantially different than previous work; this is also observed in \Cref{subsec:results:fact} where using \vitaminc{} training data to solve \dataset{} yields poor performance.

Recent work such as Fruit \citep{iv-etal-2022-fruit} and PEER \citep{schick2022peer} also operate on more complicated edits. Fruit generates updated sentences from a base passage given the new evidence in a Wikipedia article. PEER attempts to imitate the editing process using a sequence of planning steps. However, both of these primarily focus on generating the target update, while we focus on succinctly capturing the edited information. Further, the use of question generation as a device for discrimination is novel to the best of our knowledge.

\paragraph{Question Generation}
Question generation has been successfully applied to various purposes, including augmenting question answering systems \citep{duan-etal-2017-question, lewis-etal-2021-paq}, capturing implicit information written about text \citep{pyatkin-etal-2021-asking}, and building soft knowledge bases
 \citep{chen-etal-2022-qamat}. In this work, we apply question generation to the task of discriminating edited sentences. As far as we are aware, there is no prior work on evaluating question generation systems. 
