\section{Methods}\label{sec:methods}
As mentioned, \dataset{} can be thought of as a composition of two tasks: factual change detection and discriminating question generation.
Our simple baseline systems thus treat this as a pipeline, first predicting whether or not there is a factual change and then generating a discriminating question if there is. We also present baseline models that solve both tasks jointly with a single prediction. 
Our methods are illustrated in \autoref{fig:method}.
\input{fig/method}


Note that none of our methods use any part of \dataset{} as training data, as the dataset is only intended to be used for evaluation. Models are instead trained on larger existing datasets for question generation and factual change detection. 
% Most of our baselines use a similar question generation model as described earlier in \Cref{sec:seed}: a T5-XXL \citep{raffel2020exploring} model trained on SQuAD to produce questions. 
%which predict at the same time whether or not there is a factual change and what the discriminating question would be if there is one.

% \dataset{} considers factual changes at the passage level. However, we can also categorize our baseline systems based on the amount of information they use to do factual change detection. The simplest versions only look at answer spans while more complicated systems examine their own generated questions. Our methods are illustrated in \autoref{fig:method}.

\subsection{Factual Change Detection}\label{subsec:fact}
We propose five baselines based on answer equivalence or both question and answer equivalence. 
% Here we do not consider any methods directly comparing entire passages due to the difficulty of separating the concepts of new information in the passage compared to new information about the answer span in the passage. 

\subsubsection*{Answer Equivalence Baselines}
\label{Method:Overlap}
Our trivial baseline (\textit{Overlapping Answer}) classifies an example as having a factual change if and only if the answer span is not present in the base passage. The span is normalized before looking for token overlap with the passage. 

Our simple model-based baseline is similar but uses an Answer Equivalence model \citep{bulian2022tomayto}. It compares the target answer span against all valid base answer spans, finding a factual change if it does not match any of them. The Answer Equivalence model additionally takes as input a candidate question for each answer span. 


\subsubsection*{Question-Answer Equivalence Baselines}
\label{Method:Query Similarity}
The previous methods only consider the answer span, ignoring the context. Here, we consider methods that also use a question generation model on the passages and answer span to determine if there is new information.
%Next, we move on to methods that represent the associated information as a question, retaining the context while also easier to compare than full passages. 

% For both of the following methods, we generate a discriminating question for the target answer span. 
% We generate a candidate question for the given target answer span. Our goal then is to determine if there is an equivalent question-answer pair for the base passage. 
%If no duplicate is found, the target answer span is determined to have a factual change. Thus, the challenging part of the method concerns how to classify the question-answer pairs as duplicates. 
For the first method, we find base answer spans equivalent to the target answer span using the \textit{Overlapping Answer} method. Then, we want to see if the questions generated from those answers would also be equivalent in both passages. To do so, 
we use a T5-XXL \citep{raffel2020exploring} model trained on Quora Question Pairs \footnote{https://www.kaggle.com/c/quora-question-pairs} to predict whether the pair of questions is ``duplicate'' or ``not duplicate'' . If the question is not a duplicate, then we consider this example to have a factual change. Thus, answer spans present in both passages but with different contexts could now be identified as having a factual change. This will increase the recall of the \textit{Overlapping Answer} method.

%to predict whether any base question is a duplicate of the discriminating question.
% This method uses the trivial answer equivalence baseline to classify if there is an equivalent answer. 
% If the model classifies the discriminating question to be a duplicate with any of the generated base questions, we consider the example to have no factual change.

The second approach adds a cross questioning filter (\textit{Cross-Q}). Given a candidate question generated from target passage, we attempt to answer the question with the base passage
% that the Query Similarity model classified as not duplicates, 
using a reading comprehension model. We train a T5-XXL model  on SQuaD v2 \citep{rajpurkar-etal-2018-know} 
question-answering dataset to take the passage and question as input and output the answer. If the model predicts no answer or a different answer from the target span, we classify the example as having a factual change. 
% Note that this approach by itself is too low in recall to form an effective method. 
Finally, the \textit{QA-equivalence} method combines both the query similarity model and cross question model to boost precision. In this case, we consider an example to have a factual change only when both methods determine a factual change.
%Query-Deduplication or \textit{Q-Dedup} method combines both \textit{Q-sim} and \textit{QA-equivalence} i.e there is a factual change only when both methods determine a factual change.
\label{Method:Cross Question Answering}


\subsection{Question Generation}
\label{Method:Question Generation}
Each of our factual change detection baselines is then combined with a question generation model. We use a similar T5-XXL model finetuned on SQuAD as described in \Cref{sec:seed}.
% in \Cref{sec:methods} for the factual change detection metrics. 
Unsurprisingly, the model we use to seed the questions can do well on the questions that it wrote originally; however, this is an unfair baseline. Thus, we additionally test a version of the model that is sampled and also a retrained version using a different seed. We also test training a similar model trained on Natural Questions \citep{kwiatkowski-etal-2019-natural}.

\subsection{Joint systems}
Many of the techniques described \Cref{subsec:fact} are inefficient, requiring multiple runs of various models. For instance, the Query Similarity method requires one model run for each answer span in the base passage per example, which corresponds to quadratic runs for each pair of passages. We also explore methods that can directly compare the base and target passage without the need for any intermediate steps. These methods instead jointly detect if there is a factual change and generate a discriminating question.

\subsubsection*{Finetuning on Silver Data}
\label{Method:Finetuning on QA-equivalencea}
We mine additional pairs of Wikipedia passages using the same process as in \Cref{sec:initial_data}. We then identify every possible answer span from the target passage. We create silver training examples for the factual change detection component of the task by labeling each target answer span using our best heuristic method, \textit{QA-equivalence}.

We then convert these labels into a text-to-text task. For each example with a factual change, we use the question generated by the SQuAD model (\Cref{Method:Question Generation}) as the target. For questions without a factual change, we use ``None'' as the target. The input to the model is the concatenation of the base and target passages with the target answer span marked by a special token.

The model is a T5-XXL initialized from the same question generation model as the model that produced the original questions.


%The model then uses silver data labeled by the QA-equivalence method learns to predict the discriminating question obtained from the output of \textit{Q-Dedup} presented in \autoref{Method:Cross Question Answering}. It is set to "None" when there is no factual change. 

\subsubsection*{Finetuning on \vitaminc{}}
The \vitaminc{} task \cite{schuster-etal-2021-get} also has a factual change detection component. We sample negatives from the \vitaminc{} Revision Flagging dataset, using negative examples with a random noun phrase chosen as the answer span.

For positive examples, we need to identify a specific answer span that contains a factual change as well as the corresponding discriminating question. While there is no direct counterpart of this task in \vitaminc{}, the Fact Verification task is somewhat similar. The dataset consists of an evidence \emph{e}, a simplified claim \emph{c} supporting \emph{e}, a companion edited sentence \emph{e'} and an edited-claim \emph{c'} refuting \emph{e} and supporting \emph{e'}. An answer span \emph{a} is identified based on the token-level diff between \emph{(c, c')} and a question generated from \emph{c} conditioned on \emph{a} using the question generation model in \Cref{Method:Question Generation}. Because \emph{c} is a simple sentence, we anecdotally find the generated questions to be of high quality.

The dataset (\emph{e}, \emph{e'}, \emph{a}) is converted to a text-to-text task and used to finetune a T5-XXL model following the same steps as above.
Note that a model trained on an equal amount of positives and negatives yielded poor performance on \dataset{}. In our final \vitaminc{} silver dataset, we used only 10\%  negatives to achieve a reasonable performance.