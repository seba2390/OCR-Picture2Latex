\section{Introduction}

Given a pair of statements, how can we identify the difference in their information content? %find if they convey the same information, and how do you present the binary prediction in an interpretable fashion?
This problem has existed in different forms across \nlp{} research, such as \emph{recognizing textual entailment}~ \cite{dagan-etal-2010-recognizing} and
\emph{natural language inference}~\cite{bowman-etal-2015-large}. The initial focus of this type of research was finding the logical implication relations between sentences.

\input{fig/diffqg}

More recently, specialized entailment-like resources and models have been applied to fact verification~\citep{ws-2018-fact} with applications to science, education and journalism. This trend has exposed the limited transfer
%learning 
between logical entailment and general factual change detection~\citep{thorne-etal-2018-fever} as well as the need for interpretable models for this task~\citep{kumar-talukdar-2020-nile}.

Wikipedia revisions across time provide a large scale and highly available source of sentence pairs, leading to new resources such as \wikiatomicedits{} \cite{faruqui-etal-2018-wikiatomicedits} and \vitaminc{} \cite{schuster-etal-2021-get}. However, prior work is limited to minimal changes that concern only a single factual addition or change.
We introduce \dataset{}, a manually annotated dataset spanning changes over multiple years. \dataset{} consists of paired passages with complex factual changes including multiple additions and deletions within the same example. 
Additionally, it provides a way to interpret the prediction in the form of a discriminative question-answer pair that identifies the change.

Question-answer pairs provide a semi-structured summary of a change: more flexible than knowledge graph triples and more useful than free-form text. 
%Question-answer pairs provide a flexible framework that can capture a wide variety of semantics without being restricted to knowledge base schemas. 
% On the other hand, complex edits are difficult to summarize with a single piece of declarative text and may easily result in ambiguity. Questions are generated in context of an answer span which helps reduce this ambiguity.
%
% Question-answer pairs also integrate easily with 
%
For instance, question-answer pairs can represent different types of updates: a new prime minister may update an answer, while a new type of minister would add an entirely new question. 
%
%
%
%This paradigm has the advantage of producing point-fixable and explainable natural language processing applications. \pj{Further, QA can act as key-value pairs allowing us to separate new content from updated numbers and relations}\je{Not sure I understand this, how can QA do that?}Questions and answers are more point-fixable and explainable than knowledge graphs or distributed representations.For example, The task itself inverts the reading comprehension paradigm: produce a question that a document answers.are treated

Question generation (\qg{}) is a new \nlp{} task that consists of generating a question that a provided document answers. There are various successful applications of this approach, including augmenting datasets to train question answering systems \citep{duan-etal-2017-question, lewis-etal-2021-paq}, capturing implicit information written about text \citep{pyatkin-etal-2021-asking}, and building soft knowledge bases \citep{chen-etal-2022-qamat}. Previous work in \qg{} treated the underlying passages as static \citep{lewis-etal-2021-paq}, while real life documents are constantly updated~\citep{dhingra-etal-2022-time}. As the source corpus is updated, new question-answer pairs must be added and existing ones must be updated. %There is presently no simple way to do so, which is a critical limitation of existing work.
%A current limitation of existing workPresently, , and currently there is no simple way to do so without re-running the entire pipeline, which is often costly.
% This is a critical but often overlooked limitation of this type of approach.%However, semantic content changes over time, which presents a challenge of diachronic degradation

\dataset{} thus 
%Our work on generating discriminative QA pairs based on edited pair of articles 
addresses two challenges simultaneously: providing an interpretable summarization of factual changes and updating soft knowledge bases consisting of question answer pairs. %Despite the growing interest in \qg{} systems, evaluation remains scarce. Researchers generally can only measure the effects of performance on downstream tasks which limits their ability to measure improvements.
% short of human evaluation of these specific systems, or looking at performance in downstream tasks, evaluations of question generation models remain scarce, limiting the ability of researchers to measure improvements. 
We hope that this dataset can also help evaluate the quality of \qg{} models in producing natural, semantically \ correct, unambiguous, and information-seeking questions.
% ~\citep{rodriguez-boyd-graber-2021-evaluation}. 
The dataset and code for our experiments will be open sourced.\footnote{\url{https://github.com/google-research/language/tree/master/language/diffqg}}

% There exist many possible desiderata for \qg{} systems. Beyond semantic and syntactic correctness, naturalness and controllability, we argue a more subtle requirement is to generate questions that users might ask in an information seeking context.
% This corresponds to the Cranfield paradigm described in \citet{rodriguez-boyd-graber-2021-evaluation}, in contrast to the Manchester paradigm, tailored to knowledge probing and more similar to trivia questions.
% For applications requiring high coverage of possible questions, one might also want that the generated questions go beyond simple rewriting of the original passage and unambiguously point to the expected answer without needing further context. Within the space of controlling the generated questions, it is natural to require not just a specific answer but also a specific factual relationship (in a hypothetical knowledge graph) that the question could reference. 
% Beyond the types of questions that are output, it is also important that the questions are at the correct degree of specificity, capturing the novel information present in the text.

% For these reasons, we want to control \qg{} systems to be able to distinguish among similar passages as information changes and to be able to highlight the differences by producing specifically targeted questions. In other words, if there is a shift in the underlying information between two pieces of text, a model should be able to detect it and optionally produce a Cranfield-paradigm question that elicits a different answer for each passage or is only answerable based on one passage and not the other. This would allow users to easily inspect the updated questions produced by switching to new versions of the source documents. 

% In this work, we focus on factual edits as a naturally occurring source of examples that happen regularly on Web corpora. In particular, we examine paired Wikipedia revisions where factual information in a passage has been edited or added. We manually annotated each passage pair with information-seeking questions that can highlight whether the underlying factual information has changed.

% In general, one might hope that a question generation system generates realistic and specific enough questions that go beyond textual similarity. For instance, given two pieces of similar text that nonetheless encode different information, the model should generate different questions. If some question is only answered by one piece of text, the model should generate it for that one but not for the other. 

Our contributions are the following:

\noindent \textbf{(a)} We introduce \dataset{}, an expert-annotated \textit{evaluation} dataset that consists of questions that summarize the difference between two passages. To the best of our knowledge, no prior dataset exists that covers such long and complicated edits.

%question generation models on identifying factual changes in text.

\noindent \textbf{(b)} We propose a set of metrics that can be used to measure improvements in question generation or factual change detection. 

\noindent \textbf{(c)} We evaluate a comprehensive set of baselines that surface the shortcomings of current systems.
%, and will help guide future work in this area.
