\section{Data Collection}
\label{Data Collection}
Collecting such discriminating questions is a non-trivial process. Thus, we introduce a staged annotation process with expert annotators (the authors of this paper) and use a question generation model to aid annotation. We describe our process below (visualized in \autoref{fig:data}).



% Each pair of passages has a non-exhaustive list of \textit{answer spans} in the target passage which may contain new information. Each example thus consists of the pair of passages as well as an answer span. The goal of the task is to write a \textit{discriminating question} that differentiates the information present in the target passage from the base passage as it relates to that answer span. 

% In such cases where a discriminating question exists, we say this example has a \textit{factual change}. We also collect negative examples: triples of target passage, base passage, and answer span where the annotator feels confident that no discriminating question exists. 


\input{fig/data}

\subsection{Input Passage Pair Selection}\label{sec:initial_data}
% We sample passages from English Wikipedia article present in the Natural Questions (NQ) training set \citep{kwiatkowski-etal-2019-natural}. 
First, we extract the Wikipedia pages for entities from the Natural Questions (NQ) training set~\citep{kwiatkowski-etal-2019-natural}. In particular, we find the pages for Wikipedia snapshots between the years 2008 and 2020. After sampling a base document, we find the version of that document one year later and use this as the target document. Using the two documents as a corpus, we compute cosine similarity between the TF-IDF vectors over each sentence pair and pick the pair with the highest similarity.
Sentence pairs with similarity either greater than $0.8$ or less than $0.25$ are discarded. In order to retain meaningful changes, we ensure at least one noun or number is edited and up-sample instances where either a named entity or at least five tokens have been edited. 

% \footnote{https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html}. 
% Several existing edit-based datasets such as \vitaminc{} \citep{schuster-etal-2021-get} or \wikiatomicedits{} \citep{faruqui-etal-2018-wikiatomicedits}  tend to use atomic edits with only a few tokens edited or added. 

This process thus focuses on edits accumulated over a year and consists of changes ranging from five to twenty tokens, making these semantically richer and more widely applicable than existing factual change detection datasets.

\subsection{Seed QA Pair Generation}\label{sec:seed}
% We started off with the task of writing all possible questions that capture the diff between base-target pair. However, exhaustively capturing all questions ended up to be a difficult task with high variance between annotators.
% Therefore, we resort to a more tractable task of classifying whether an answer span has new information and writing a question to capture this information.
%Coming up with a question answers for each example is nontrivial process as there can be many such QA pairs. To assist this annotation process, we first provide annotator with a seed QA pair. We first select potential answer span. A sentence can have a very large number of possible answer spans. We restrict to only noun phrases identified from constituency parser \citep{kitaev-klein-2018-constituency}. 
Each target passage has a very large number of possible answer spans; for convenience, we restrict them to only noun phrases identified using the Berkeley Neural Parser \citep{kitaev-klein-2018-constituency}. 
%These spans are also more meaningful in context of our task. 
%To increase annotation speed, each example starts with a \textit{seed question} from a question generation model using the target passage and answer span as input. In particular, we use a T5-XXL model \citep{raffel2020exploring} that has been finetuned on the SQuAD dataset \cite{rajpurkar-etal-2016-Squad}.
To increase annotation speed, each example starts with a \textit{seed question} that is generated by a question generation model from the target passage and answer span. In particular, we use a T5-XXL model \citep{raffel2020exploring} that has been finetuned on the SQuAD dataset \cite{rajpurkar-etal-2016-Squad}.



\subsection{Annotation Process}
\dataset{} annotation was done in three phases by six expert volunteers. First, annotators are given the paired passages described above along with the answer span and seed question, which corresponds to one \textit{example candidate}. Then, they label each example candidate with one of the five options:

\paragraph{Accept} The seed question follows all requirements for discriminating questions as is.
\paragraph{Context} The seed question asks about the appropriate topic but is not answerable outside of the context of the passage. For instance, questions like ``What did he win?'' or ``Where were the Olympics held?'' both lack context in order to answer the question successfully. 
\paragraph{Edit} The example candidate answer has a discriminating question, but the question is different than the seed question. Sometimes, this is because the seed question does not capture the new information contained in the passage; other times, the seed question is simply nonsense. 
\paragraph{Reject} This example candidate has no valid discriminating question. In other words, these are negative examples. Sometimes, the target passage contains no new information at all; however, it may contain new information about other answer spans but not the one in the example candidate. In our previous John Doe example, there is no new information about ``the Olympics'', except indirectly.
\paragraph{Skip} It is unclear if there is a valid discriminating question for this example candidate. This could be due to awkward or cumbersome answer spans: for instance ``two gold medals at the Olympics in 2012.'' Alternatively, it could seem unclear if there is new information about an answer span due to its indirect relationships with other entities. Finally, it could be difficult to write an information-seeking question even though there is obviously new information: for instance, writing a question with the answer span ``John Doe'' in the previous example. 


Each example candidate is considered by two annotators. Unless both annotators agree to Add, Reject, or Skip, a third annotator decides. In examples where one annotator chose Context or Edit, the third annotator is responsible for writing the correct question according to the guidelines. If one annotator chose Add or Reject and the other skipped, the third annotator can confirm the Add or Reject or also skip if they cannot decide. See \Cref{sec:interface} for the annotation interface.

\subsection{Question Writing Guidelines}
Note that writing a single, context-free, and information-seeking question that summarizes the difference between the two passages can be challenging. In cases where it seemed impossible, annotators are encouraged to skip the example. For cases where additional Context was needed, annotators are encouraged to add as much context without sacrificing fluency, so that the question can be answered without awareness of the source passage. When an annotator writes a question from scratch in the Edit case, they are encouraged to think of a question that either would have a different answer, be unanswerable, or have a false precondition if posed against the base passage. While conditioning on a single answer span reduces ambiguity, the task is still ambiguous, which is unavoidable when handling large and complex edits. 
 
 \input{tab/data_stats}

 
\subsection{Data Statistics}
Our initial annotation process starts with $8,530$ example candidates drawn from $999$ passage pairs. Annotators skipped nearly $75\%$ of the example candidates, leaving $1,912$ examples. Of those, roughly $40\%$, or $759$, had a factual change and thus a discriminating question written about them, leaving $1153$ negative examples.
% 
% ec{I don't understand this statistics... isn't every sentence annotated by two?}\pj{This statistics is computed over all Phase-1 examples even where the annotators disagree and the example was discarded. Come to think, this might not be super relevant. Removing it}
% On the factual change detection task where both annotators provided a label, we found the binary agreement to be 0.86 and Cohen's Kappa to be 0.73. 
Of the spans where a factual change was detected, annotators  modified the question in $65\%$, or $494$, of the examples: $45\%$ are labeled as Context and $20\%$ as Edit. Detailed dataset statistics can be found in \autoref{tab:data}.

Note that on all cases where a question was accepted as is or considered a negative example, at least two annotators agreed on that rating. However, human written questions are not verified; both annotators agree that there exists a discriminating question but not necessarily what it is. To address this, we evaluate a small set of fifty questions and found that a second annotator would write an equivalent question around 85\% of the time.  





