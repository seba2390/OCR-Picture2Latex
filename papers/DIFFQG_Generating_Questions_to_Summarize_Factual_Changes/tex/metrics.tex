\section{Metrics}\label{sec:metrics}
\dataset{} can be used to measure performance on three related tasks. % \textit{Factual change detection}:
%: in other words, whether or not a question should be generated. 
% \textit{Discriminating Question Generation} metrics measure how similar generated questions are to annotated questions. \textit{Full System} metrics measure a system's performance on the overall task. 

\paragraph{Factual Change Detection} \label{Metric:Diff Detection}  Given an example consisting of a base passage, target passage, and answer span, the goal is to determine whether there exists a valid differentiating question. In other words, whether there is new information about this answer span that is present in the target passage when compared to the base passage. To measure this, we report accuracy, precision, recall and F1 score over the existence of a differentiating question in our annotations. Note that always predicting no change achieves 60.3\% accuracy but 0\% F1, but random guessing corresponds to 44.1\% F1. 

\paragraph{Discriminating Question Generation} Given a target passage and answer span, write a specific, unambiguous and information-seeking query that can be answered  with the target passage. To measure this, we compare machine generated questions to those that humans verified, edited, or hand wrote. We use two model-free metrics Rouge-1 and Rouge-L \citep{lin2004rouge} which measure the token-level overlap and longest subsequence overlap of the questions, respectively. We also consider two model-based metrics, BLEURT \citep{sellam-etal-2020-learning}, which is a learned evaluation for text similarity based on BERT \citep{devlin-etal-2019-bert}, and a query similarity model \citep{reimers-2019-sentence-bert} trained on Quora Question Pairs \footnote{\href{https://huggingface.co/cross-encoder/quora-roberta-large}{huggingface.co/cross-encoder/quora-roberta-large}}.

Note that we evaluate discriminating question generation despite using a question generation model in our annotation procedure. Note that all of these questions are reviewed by humans and only the very fluent ones are kept. As question generation models vary in which of their productions are very fluent, this set is less trivial than it would initially appear. Nonetheless, we also separate human-written or edited questions and evaluate that set independently. 
% The query similarity metric is based on a T5-XXL model \citep{raffel2020exploring} trained on Quora Question Pairs\footnote{\href{https://www.kaggle.com/c/quora-question-pairs}{https://www.kaggle.com/c/quora-question-pairs}} to predict either ``duplicate'' or ``not duplicate'', where a model gets 1.0 if the question it produces is considered a ''duplicate'' and 0.0 otherwise. 
%Note that these metrics are only computed over the true positive examples, 
%\pj{This is slightly confusing in the Metrics section. There already seems to be some explanation in Results. Maybe we can omit it here.} 
%so question generation models can only be compared easily based on the same set of data; i.e., producing questions based on the same factual change detection model. 

\paragraph{Full System} This is the overall measure of performance on \dataset{}. We reuse the metrics from discriminating question generation, using 0.0 for BLEURT, ROUGE-1, ROUGE-L, and Query Similarity if the factual change detection is incorrect.  
%and is scored in a standard way otherwise. Note that certain systems produce a question or ``None'' in the same output space, while others compose a factual change detection method and question generation method.