% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
\usepackage{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{hyperref}
\usepackage[nameinlink]{cleveref}



% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\usepackage{todonotes}
\newcommand{\je}[1]{\todo[color=magenta]{\small JE: #1}} % Julian's comments
% \newcommand{\lbs}[1]{\todo[color=cyan]{LBS: #1}}  % Livio's TODOs
% \newcommand{\nf}[1]{\todo[color=green]{NF: #1}}   % Nicholas's TODOs
% \newcommand{\tf}[1]{\todo[color=yellow]{TF: #1}}  % Thibault's TODOs
\newcommand{\ec}[1]{\todo[color=green]{\small EC: #1}}  % Eunsol comments
\newcommand{\pj}[1]{\todo[color=yellow]{\small PJ: #1}} % Palak's comments

% \renewcommand{\je}[1]{{}} % Julian's comments
% \renewcommand{\ec}[1]{{}}  % Eunsol comments
% \renewcommand{\pj}[1]{{}} % Palak's



\title{\dataset{}: Generating Questions to Summarize Factual Changes}% Evaluating Question Generation Systems}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a seperate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{Jeremy R. Cole$^1$\thanks{\enskip Equal Contribution.} \qquad Palak Jain$^1$\footnotemark[1] \qquad Julian Martin Eisenschlos$^1$ \\ \qquad \textbf{Michael J.Q. Zhang}$^3$ \qquad \textbf{Eunsol Choi}$^{3}$ \qquad \textbf{Bhuwan Dhingra}$^{1,2}$ \\
$^1$ Google Research  \qquad  $^2$ Duke University  \qquad $^3$ The University of Texas at Austin \\
\texttt{\{jrcole,palakj,eisenjulian,bdhingra\}@google.com} \\ \texttt{\{mjqzhang,eunsol\}@utexas.edu}}

\newcommand{\dataset}{\textsc{DiffQG}}
\newcommand{\qg}{\textsc{qg}}
\newcommand{\paq}{\textsc{PAQ}}
\newcommand{\ambig}{\textsc{AmbigQA}}
\newcommand{\situated}{\textsc{SituatedQA}}
\newcommand{\conditional}{\textsc{ConditionalQA}}
\newcommand{\vitaminc}{\textsc{VitaminC}}
\newcommand{\wikiatomicedits}{\textsc{WikiAtomicEdits}}
\newcommand{\nlp}{\textsc{nlp}}

\begin{document}
\maketitle
\begin{abstract}
Identifying the difference between two versions of the same article is useful to update knowledge bases and to understand how articles evolve. 
Paired texts occur naturally in diverse situations: reporters write similar news stories and maintainers of authoritative websites must keep their information up to date. 
We propose representing factual changes between paired documents as question-answer pairs, where the answer to the same question differs between two versions. We find that question-answer pairs can flexibly and concisely capture the updated contents. Provided with paired documents, annotators identify questions that are answered by one passage but answered differently or cannot be answered by the other. We release \dataset{} which consists of 759 QA pairs and 1153 examples of paired passages with no factual change. These questions are intended to be both unambiguous and information-seeking and involve complex edits, pushing beyond the capabilities of current question generation and factual change detection systems. Our dataset summarizes the changes between two versions of the document as questions and answers, studying automatic update summarization in a novel way. %serve as a useful tool for researchers in a variety of areas and can move us closer to automatic update summarization.
% Moreover, as annotators also marked when no such question exists, it serves as a new evaluation for factual change detection, which also lacks evaluations with as much diversity as \dataset{}. 
% These questions are intended to be both unambiguous and information-seeking, pushing the bounds of current question generation systems' capabilities.  due to their flexibility in separating \textit{new} content from \textit{updated} numbers and relations%annotations that write \textit{discriminating questions} between two related pieces of text.
% We hope that this dataset will be of value to researchers as they seek to improve such systems for a variety of purposes.

%important to understand how the article evolves and to update knowledge bases. Given two similar documents, immediately recognizing the \textit{factual changes} between them is very useful both to casual readers and maintainers of various knowledge bases. 

%: newspapers may cover the same story using the same sources and update as the news breaks, while authoritative websites may need to keep their facts and figures up to date as the world evolves.
% Question Generation has been emerging as new method to improve QA systems and represent factual information in text. 
% However, despite the rash of new work on the topic, there is still no obvious method to evaluate such systems.
% Here we present \dataset{}, a method to evaluate the precision and recall of question generation systems. 
% \dataset{} consists of \textit{expert labeled} annotations, focusing on the particularly challenging task of generating questions from \textit{similar} pieces of text. 

\end{abstract}

\input{tex/introduction}
\input{tex/task}
\input{tex/data_prep}
\input{tex/motivation}
\input{tex/metrics}
\input{tex/methods}
\input{tex/results}
\input{tex/related_work}
\input{tex/conclusion}
\input{tex/limitations}

\bibliography{anthology,custom}
\bibliographystyle{acl_natbib}

\clearpage
\appendix
\input{tex/appendix}
\label{sec:appendix}

\end{document}
