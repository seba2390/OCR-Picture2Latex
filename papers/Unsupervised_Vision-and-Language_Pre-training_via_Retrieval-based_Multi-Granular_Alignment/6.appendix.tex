

\section{Details of Motivation Study}
As introduced in Section~\ref{section:intro}, we try to answer two questions: $(i)$ whether presenting a joint image-text data from non-parallel sources would improve the learned joint embedding space than alternatively presenting uni-modal data during pre-training. $(ii)$ If we fed joint image-text data to the model, how does its existing latent alignment affect the cross-modal representation learning. 

We conduct the unsupervised vision and language pre-training on Conceptual Captions (CC) by shuffling the image-text pairs. 
For pre-training objectives, we apply standard MLM + MRM. 
All other pre-training setup is the same as introduced in Section~\ref{sec:training_setup}. 
We first compare the round-robin and joint MLM + MRM pre-training, whose results are shown in Table~\ref{tab:data-fedding}.
We then evaluate how the alignment degree of the pre-training dataset affects the model performance, where the degree is controlled by the ratio of originally aligned image-text data in Conceptual Captions.
Table~\ref{tab:paired-ratio} shows the detailed results of each downstream task.
Their Meta-Ave scores are also plotted in Fig.~\ref{fig:intro}.
From these results, we obtained two important messages: 
$(i)$ joint image-and-text input is more optimal for UVLP than alternatively presenting uni-modal data from unparallel image and text corpus. 
$(ii)$ The more the latent semantic alignment exists in the image-text data the better the pre-trained model performs. 

We further explore the realistic unsupervised V+L pre-training, where the images and texts are from two different sources.
Specifically, we sample the images from Conceptual Captions and the texts from Book Corpus respectively.
Table~\ref{tab:bc_alignment} shows that the pre-trained model on our weakly aligned CC image and BC sentence corpus far outperforms that on random pairs, indicating it also holds that better latent image-text alignment leads to better pre-trained model's performance under realistic setting.
\begin{table}[!h]
\centering
\small
\tablestyle{5pt}{0.80}
\begin{tabular}{l|ccccc}\toprule
\multirow{2}{*}{} &VQA2 &NLVR2 &VE & RefCOCO+ & \multirow{2}{*}{ Meta-Ave } \\
&Test-Dev &Test-P &Test &Devs & \\\cmidrule{1-6}
random &70.3 &51.2 &75.3 &76.5 & 68.3 \\
proposed & \bf 71.2 & \bf 67.1 & \bf 77.1 & \bf 79.7 & \bf 73.8 \\
\bottomrule
\end{tabular}
\vspace{-0.3cm}
\caption{Pre-training on realistic CC + BC data}
\label{tab:bc_alignment}
\end{table}

\section{Effectiveness of Weighted ITM}
We compared the performance of pre-training our model with or without weighted ITM. 
The models are pre-trained on CC images and texts. 
As shown in Table~\ref{tab:WITM}, weighted ITM are consistently better than treating all the retrieved pairs with the same weight. 


\begin{table}[!htp]\centering
\footnotesize
\tablestyle{3pt}{0.80}
\begin{tabular}{l|ccccc}\toprule
\multirow{2}{*}{} &VQA2 &NLVR2 &VE & RefCOCO+ & \multirow{2}{*}{ Meta-Ave } \\
&Test-Dev &Test-P &Test &Devs & \\\cmidrule{1-6}
w/o $w_{\text{ITM}}$ &71.9 &72.6 &77.0 &79.7 & 75.3 \\
$w_{\text{ITM}}$ & \bf 72.1 & \bf 73.4 & \bf 77.3 & \bf 80.3 & \bf 75.8 \\
\bottomrule
\end{tabular}
\vspace{-0.3cm}
\caption{Ablation Study on weighted ITM}
\label{tab:WITM}
\end{table}


\begin{table*}[!ht]\centering
\small
\begin{tabular}{l|c|c|c|ccc|c}\toprule
\multirow{2}{*}{ Pre-training } &VQA2 &NLVR2 &VE & \multicolumn{3}{c|}{RefCOCO+} & \multirow{2}{*}{ Meta-Ave } \\
&Test-Dev &Test-P &Test &Dev &TestA &TestB & \\\cmidrule{1-8}
Round-Robin MLM+MRM &70.4 &51.1 &74.8 &73.3 &78.3 &\textbf{67.4} & 67.4 \\
Joint MLM+MRM &\textbf{70.6} &\textbf{52.4} &\textbf{74.9} &\textbf{74.5} &\textbf{79.4} &66.8 & \textbf{68.1} \\
\bottomrule
\end{tabular}
\vspace{-0.3cm}
\caption{Detailed evaluation results on four V+L downstream tasks with two different data feeding strategy for UVLP: (1) joint image-text data (joint MLM+MRM); (2) alternative uni-modal data (round-robin MLM+MRM).}
\label{tab:data-fedding}
\end{table*}

\begin{table*}[!ht]\centering
\small
\begin{tabular}{l|c|c|c|ccc|c}\toprule
\multirow{2}{*}{ Paired Ratio } &VQA2 &NLVR2 &VE & \multicolumn{3}{c|}{RefCOCO+} & \multirow{2}{*}{ Meta-Ave } \\
&Test-Dev &Test-P &Test &Dev &TestA &TestB & \\\cmidrule{1-8}
0\% &70.6 &52.4 &74.9 &74.5 &79.4 &66.8 & 68.1 \\
20\% &71.1 &70.0 &76.4 & 76.3 &80.3 &67.5 & 73.5 \\
40\% &71.4 &71.6 &77.2 &77.9 &82.4 &68.8 & 74.5 \\
60\% &71.9 &74.5 &77.8 &79.9 &84.4 &69.9 & 76.0 \\
80\% &72.2 &75.7 &78.4 &80.9 &85.7 &71.8 & 76.8 \\
100\% &72.5 &75.9 &78.7 &82.1 &86.6 &75.0 & 77.3 \\
\bottomrule
\end{tabular}
\vspace{-0.3cm}
\caption{Detailed evaluation results on four V+L downstream tasks with 6 sets of image and text corpus of different latent cross-modal alignment degree. The alignment degree is controlled by changing the ratio of original aligned image-text data from 0\% to 100\%.}
\label{tab:paired-ratio}
\end{table*}


\begin{figure*}[h!]
\centering
\includegraphics[width=14cm]{figures/Pos_Retrieve.png}
\vspace{-0.3cm}
\caption{Examples of retrieved text from both CC and BC. The covered grounded noun phrases in retrieved sentences are highlighted in green bar for positive examples.}
\label{fig:pos-ret}
\end{figure*}

\section{Downstream Task Details}
We describe the details of fine-tuning on the four different downstream tasks: Visual Question Answering (VQA2), Natural Language for Visual Reasoning (NLVR2), Visual Entailment (VE), and Referring Expression (RefCOCO+). We mainly follow the setup of UNITER\cite{chen2020uniter} for each downstream task with minor adjustments.  

\noindent\textbf{VQA2}
Given a question about an image, the task is to predict the answer to the question. Following \cite{yu2019mcan}, we take 3,129 most frequent answers as answer candidates. We use both training and validation sets from VQA 2.0 for fine-tuning. Following UNITER, we also leverage data from Visual Genome\cite{krishna2017visualgenome} to augment the best performance on the test-dev split. We fine-tune the model with a binary cross-entropy loss with a peak learning rate of $6\times10^{-5}$ for 20 epochs. The training batch size is set as 480. 

\noindent\textbf{NLVR2}
NLVR2 is a task for visual reasoning. The objective is to determine whether a natural language statement is true or not given a pair of input images. 
We follow UNITER's setup treating each data point as two text-image pairs with repeated text. 
The two [CLS] outputs from the model are then concatenated as the joint embedding for the example. We apply a multi-layer perceptron (MLP) classifier on top of this joint embedding for the final classification. Unlike~\cite{li2020unsupervised} that conducts additional ``pre-training" on NLVR2 datasets, we only fine-tune the model with the task-specific objective to maintain the same setting as all other downstream tasks. We train the model for 8 epochs with a batch size of 60 and a peak learning rate of $3\times10^{-5}$. 

\noindent\textbf{VE}
Visual Entailment is a task built on Flickr30k Images\cite{young-etal-2014-image}, where the goal is to determine the logical relationship between a natural language statement and an image. Similar to the Natural Language Inference problem in NLP, this task is formatted as a 3-way classification problem to predict if the language statement entails, contradicts, or is undetermined with respect to the given image. An MLP transformer classifier is applied to the output of the $\text{[CLS]}$ token to make the final prediction. The model is fine-tuned using cross-entropy loss. We set the batch size as 480 and the peak learning rate as $8\times10^{-5}$. The model is fine-tuned for 4 epochs for this downstream task. 

\noindent\textbf{RefCOCO+}
The referring expression task involves locating an image region given a natural language phrase. We use RefCOCO+ \cite{yu2016modeling} as the evaluation dataset. Bounding box proposals from VinVL object detectors are used for fine-tuning. A proposal box is considered correct if it has an IoU with a gold box larger than 0.5. We add an MLP layer on top of the region outputs from the Transformer to compute the alignment score between the language phrase and each proposed region. We fine-tune our model for 20 epochs with a peak-learning rate of $2\times10^{-4}$.


\begin{figure*}[h!]
\centering
\includegraphics[width=14cm]{figures/Neg_Retrieve.png}
\vspace{-0.1cm}
\caption{Examples of retrieved text from both CC and BC. The mistakenly covered grounded noun phrases in retrieved sentences are highlighted in red bar for negative examples.}
\label{fig:neg-ret}
\end{figure*}

\begin{figure}[h!]
\centering
\includegraphics[width=0.7\linewidth]{figures/attention_viz_1.jpg}
\vspace{-0.2cm}
\caption{Text-to-image attention given the aligned pair whose caption is ``person in a leather jacket riding a motorcycle on the road".}
\label{fig:attn_viz_1}
\end{figure}

\begin{figure}[h!]
\centering
\includegraphics[width=0.7\linewidth]{figures/attention_viz_2.jpg}
\vspace{-0.2cm}
\caption{Text-to-image attention given the aligned pair whose caption is ``girl in a blue kayak floating on the picturesque river at sunset".}
\label{fig:attn_viz_2}
\end{figure}

\begin{figure}[h!]
\centering
\includegraphics[width=0.7\linewidth]{figures/attention_viz_3.jpg}
\vspace{-0.2cm}
\caption{Text-to-image attention given the aligned pair whose caption is ``people walking by the christmas tree and stage area".}
\label{fig:attn_viz_3}
\end{figure}

\begin{figure}[h!]
\centering
\includegraphics[width=0.7\linewidth]{figures/attention_viz_4.jpg}
\vspace{-0.2cm}
\caption{Text-to-image attention given the aligned pair whose caption is ``single cowboy guiding a line of horses through the desert".}
\label{fig:attn_viz_4}
\end{figure}
\section{Additional Visualization}
We present additional examples of retrieved text from both CC and BookCorpus. Specifically, we demonstrate more positive examples in Fig \ref{fig:pos-ret} that covers the appropriate grounded noun phrases. We also share some negative examples in Fig \ref{fig:pos-ret}. As analyzed in the limitation section, the current language embedding model weighs all the object tags equally to generate the joint embedding representation. This can lead to mistakenly focused object tags when retrieving the text. In row 1 of Fig \ref{fig:neg-ret}, texts retrieved cover less important noun phrases such as ``finger" and ``hair" instead of the more important noun phrase "baby". Row 2 of Fig \ref{fig:neg-ret} demonstrate mistakenly retrieved texts due to the limitation of the pre-defined object categories in the object detector. In this example,  the students in the image are detected as ``person" or ``man", which leads to the failure of retrieving any valid text.    

We also demonstrate more examples on text-to-image attention between the pre-trained U-VisualBert and {\ModelName } on the Conceptual Captions Validation set in Fig \ref{fig:attn_viz_1}, \ref{fig:attn_viz_2}, \ref{fig:attn_viz_3}, \ref{fig:attn_viz_4}. These examples provide additional evidence on the better local alignment captured by \ModelName. 