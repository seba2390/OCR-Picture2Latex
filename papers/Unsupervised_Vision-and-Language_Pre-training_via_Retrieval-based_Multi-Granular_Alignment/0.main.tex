% CVPR 2022 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
%\usepackage[review]{cvpr}      % To produce the REVIEW version
% \usepackage{cvpr}              % To produce the CAMERA-READY version
\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{multirow}

\usepackage{booktabs, multirow} % for borders and merged ranges
\usepackage{soul}% for underlines
\usepackage[table]{xcolor} % for cell colors
\usepackage{changepage,threeparttable} % for wide tables
% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref=true,breaklinks=true,colorlinks,bookmarks=false]{hyperref}

% Authors
\usepackage{xcolor}
% define colors from here: http://latexcolor.com/
\definecolor{bittersweet}{rgb}{1.0, 0.44, 0.37}
\definecolor{mygreen}{rgb}{0.29, 0.7, 0.48}
% colors: red, orange, cyan, 
\newcommand{\mingyang}[1]{\textcolor{red}{\small{\bf [ #1 -- Mingyang ]}}}
\newcommand{\licheng}[1]{\textcolor{mygreen}{\small{\bf [ #1 -- Licheng ]}}}
\newcommand{\ning}[1]{\textcolor{orange}{\small{\bf [ #1 -- Ning ]}}}
\newcommand{\aman}[1]{\textcolor{cyan}{\small{\bf [ #1 -- Aman ]}}}
\newcommand{\mj}[1]{\textcolor{bittersweet}{\small{\bf [ #1 -- MJ ]}}}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}

\newcommand{\myparagraph}[1]{\vspace{0.25em}\noindent\textbf{#1}}
\newcommand{\tablestyle}[2]{\setlength{\tabcolsep}{#1}\renewcommand{\arraystretch}{#2}\centering\footnotesize}
%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{5426} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2022}
\def\TaskName{UVLP}
\def\TaskFullName{unsupervised V+L pre-training without parallel data}
\def\ModelName{$\mu$-VLA}
\def\ModelFullName{Unsupervised Vision-and-Language Pre-training via Retrieval-based Multi-Granular Alignment}
\def\uvisualbert{U-VisualBERT}
\newcommand{\head}[1]{\noindent\textbf{#1}}


\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{Unsupervised Vision-and-Language Pre-training via Retrieval-based Multi-Granular Alignment} 

\author{
Mingyang Zhou$^1$\thanks{The two authors contribute equally.} \quad Licheng Yu$^3$\footnotemark[1]  \quad  Amanpreet Singh$^3$  \quad  Mengjiao Wang$^3$  \quad  Zhou Yu$^2$  \quad  Ning Zhang$^3$
\\ 
$^1$Uiversity of California, Davis   \quad  $^2$ Columbia University  \\  $^3$Meta AI\\
\tt{\small minzhou@ucdavis.edu}, \tt{\small zy2461@columbia.edu}, \tt{\small\{lichengyu, asg, mengjiaow, ningzhang\}@fb.com} \\
}

\maketitle

%%%%%%%%% ABSTRACT
\begin{abstract}
Vision-and-Language (V+L) pre-training models have achieved tremendous success in recent years on various multi-modal benchmarks.
However, the majority of existing models require pre-training on a large set of parallel image-text data, which is costly to collect, compared to image-only or text-only data.
In this paper, we explore unsupervised Vision-and-Language pre-training (\TaskName) to learn the cross-modal representation from non-parallel image and text datasets. 
We found two key factors that lead to good unsupervised V+L pre-training without parallel data:  $(i)$ \textit{joint image-and-text input} $(ii)$ \textit{overall image-text alignment (even for non-parallel data)}. 
Accordingly, we propose a novel unsupervised V+L pre-training curriculum for non-parallel texts and images. 
We first construct a weakly aligned image-text corpus via a retrieval-based approach, then apply a set of multi-granular alignment pre-training tasks, including region-to-tag, region-to-phrase, and image-to-sentence alignment, to bridge the gap between the two modalities.
A comprehensive ablation study shows each granularity is helpful to learn a stronger pre-trained model.
We adapt our pre-trained model to a set of V+L downstream tasks, including VQA, NLVR2, Visual Entailment, and RefCOCO+. 
Our model achieves the state-of-art performance in all these tasks under the unsupervised setting.
\end{abstract}

%%%%%%%%% BODY TEXT
\input{1.introduction}
\input{2.related_work}
\input{3.model}
\input{4.experiments}



\section{Conclusion}
We propose an unsupervised vision-and-language pre-training approach via retrieval-based multi-granular alignment to learn strong vision and language joint representations with un-aligned text and image sources. We introduce two core designs of our proposed approach: (1) construct a retrieval-based weakly-aligned image-text corpus. (2) multi-granular pre-training objectives to enable the model to capture the cross-modal alignment at different granularity levels. 
Our experiments show that our model can consistently outperform the previous state-of-the-art unsupervised pre-trained models and can achieve similar performance as the fully-aligned pre-trained models. 
\vspace{-0.4cm}
\paragraph{Limitations:} 
As we only consider the detected object list to retrieve the candidate sentences, the retrieved sentences often do not cover other visually grounded information compared to the ground truth captions.
Besides, the detected object tags are often those general concepts lacking diversity. 
% For example, in row 3 in Fig.~\ref{fig:visualization}, the stadium in the image is detected as ``building", which leads to the failure of retrieving any candidate that covers this object. Another limitation is that each object in the object list contributes equally during the retrieval. 
% This is sub-optimal as different objects should be weighted differently based on their detection confidence and visual importance. 
% Otherwise, the retrieved candidate sentences might focus on less important objects. For example in row 3, the retrieved sentences only cover ``tree" or ``water" from the object list, which leads to semantic discrepancy between the retrieved sentences and the image. 
Our retrieval results and in turn our pre-trained models could be affected by such limitations.
We hope to address the issue by learning a Siamese network between visual concepts and sentence for better retrieval and exploiting even larger uni-modal datasets to increase the diversity in the future research.
\vspace{-0.5cm}
\paragraph{Societal Impact:} 
The models are trained on the public datasets widely used in the community.
However, these datasets are known with biases, which may in turn affect our model predictions.
We do not recommend relying on the models to make real-world decisions.
%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

%%%%%%%%% Appendix
\clearpage
\appendix
\input{6.appendix}

\end{document}
