\begin{figure}[t!]
\centering
\includegraphics[width=0.9\linewidth]{figures/intro.pdf}
\caption{Meta average scores of VQA, NLVR2, VE, and RefCOCO+ fine-tuned from different pre-trained models. All pre-training are conducted on Conceptual Captions (CC) with different ratio of parallel data, i.e., a fixed amount of data is originally aligned while the rest is randomly shuffled. 0\% refers to the case of unsupervised V+L pre-training. We also plot the performance of our proposed approach against \uvisualbert \cite{li2020unsupervised}.
Breakdown of the accuracy of each task is listed in the supplementary file.
} 
\label{fig:intro}
\end{figure}

\section{Introduction}\label{section:intro}

% why we need unpaired pre-training
Vision-and-Language pre-trained (VLP) models~\cite{lu202012,chen2020uniter,lu2019vilbert,tan2019lxmert,yu2020ernie,li2020unicoder,su2019vl,kim2021vilt,huang2020pixel,wang2021simvlm,li2020unimo,li2021albef,huang2021soho} that learn the joint cross-modal representation have revolutionized the research on various vision-and-language tasks in recent years.
However, the success of VLP models relies on the availability of a large-scale aligned image-text corpora. 
The widely used crowd-sourced pre-training datasets such as MS COCO \cite{lin2014microsoft, chen2015microsoft} and Visual Genome \cite{krishna2017visualgenome} require expensive human annotations which are hard to scale up. 
Recently, the web crawled image-text datasets like Conceptual Captions 3M~\cite{sharma2018conceptual} and CC12M~\cite{changpinyo2021cc12m}, and SBU Captions~\cite{ordonez2011sbu}, \etc have dramatically reduced the need for massive human annotation but still require heavy post-cleaning procedures to get aligned image-text pairs.
In comparison, the language corpora and image collection are readily available from the web.
The convenience of getting a large-scale single-modality data has benefited the self-supervised learning of vision~\cite{bao2021beit,moco,simclr} and language~\cite{devlin2018bert,roberta} domains respectively.  
This raises a question: Can we take advantage of easily-accessible large-scale single-modality data to perform unsupervised V+L pre-training without parallel text and images (\TaskName)? 


We define \TaskName~as follows: 
given the separately crawled image collection $\mathbf I = \{\mathbf i_1, \mathbf i_2, \dots, \mathbf i_{n^I}\}$ and text corpus $\mathbf T = \{\mathbf t_1, \mathbf t_2, \dots, \mathbf t_{n^T}\}$, we aim to pre-train a multi-modal model from such data.
\uvisualbert \cite{li2020unsupervised} is the first \TaskName~work, where the authors have trained their model on un-aligned text and image data in a round-robin fashion and simply use object tags as an anchor point to bridge the gap between the two modalities. 
Their research demonstrates that a shared multi-modal embedding can be learned by just presenting a single modality at a time.
This however introduces an input discrepancy between pre-training and fine-tuning stages as each downstream V+L task requires both modalities (image, text) as the input. 
In this work, we investigate ($i$) whether presenting a joint image-text data from non-parallel data  would improve the learned joint embedding space.
Furthermore, ($ii$) if joint image-text data is fed into the model, how does its latent alignment affect the cross-modal representation learning? 

To explore these two questions, we simply use the images and captions from Conceptual Captions (CC) dataset \cite{sharma2018conceptual} as independently collected uni-modal corpus and perform the following analysis.
First, we compare the pre-trained model's performance between the two data input strategies:
one is presenting one image or text at a time (round-robin) and the other is presenting a concatenation of a pair of randomly sampled image and text (0\% alignment ratio). 
Second, we prepare five sets of image-text pairs from Conceptual Captions with different levels of pairwise alignment by controlling the ratio of original aligned image-text data from 20\% to 100\% (while the remaining is randomly sampled from each modality). 
A single-stream transformer is used for all experiments with the standard pre-training objectives: masked language modeling (MLM) on language input and masked region modeling (MRM) on vision input. After pre-training, we adapt the model to a series of four downstream V+L tasks, including VQA~\cite{antol2015vqa}, NLVR2~\cite{suhr2018corpus}, Visual Entailment (VE)~\cite{xie2019visual}, and RefCOCO+~\cite{yu2016modeling}. 
The performance is measured as the meta average of all tasks after fine-tuning. 
The results are summarized in Fig.~\ref{fig:intro}. 
From Fig.~\ref{fig:intro}, it is clear that joint MLM+MRM learning outperforms
%brings a higher meta average of 68.1 than the 67.4 from 
round-robin MLM/MRM. 
Such gains show that \textbf{joint image-and-text input is necessary for \TaskName}~even when the input is un-aligned. 
We also observe a strong positive correlation between the alignment of image-text pairs and the meta average of the fine-tuned downstream tasks of the resulting model. 
This conveys a seemingly intuitive but quite important message that \textbf{the more aligned the image-text data is the better the pre-trained model performs}. 

Inspired by these analyses, we propose \ModelFullName~(\ModelName), which uses our novel unsupervised V+L pre-training curriculum for non-parallel data.
We first construct a weakly-aligned image-text dataset via retrieval.
% construction and let the model gradually learn a multi-granularity alignment during the pre-training.
Given an image, we take its detected object tags as the reference sentence and retrieve the closest sentences from the text corpus via sentence BERT embedding~\cite{reimers-gurevych-2019-sentence} similarity.
Though the constructed pairs are noisy, the mere weak alignment of concepts is key to learning the latent alignment.
We propose to let the model gradually learn a multi-granular alignment, i.e., region-to-object tag level, region-to-noun phrase level, and image-to-sentence level to more effectively bridge the gap between the two modalities.
We show how each granularity learned from the weakly-aligned pairs contributes to the final pre-trained model's performance.
Experiments show our approach achieves the state-of-art performance
%meta average of 75.5\% 
(in Fig.~\ref{fig:intro}), with a clear gain over~\cite{li2020unsupervised} on the 4 downstream tasks.

Towards practical applications, we also validate the effectiveness of our approach under a more realistic setting, where the images are from CC and the captions are from BookCorpus (BC)~\cite{Zhu_2015_ICCV}.
Similar performance gains are achieved in this harder setting, showing the robustness of our approach.

To summarize, our contributions are three-fold: 
(\textit{i}) We analyze what leads to a good unsupervised V+L pre-training and found two key factors: joint image-and-text input, and overall alignment between image-text pairs.
(\textit{ii}) Accordingly, we propose a novel retrieval-based pre-training curriculum, which applies multi-granular alignment pre-training tasks between weakly aligned image-text pairs to bridge the gap between the two modalities.
(\textit{iii}) We provide comprehensive experiments and analyses showing the robustness of our approach when compared to SOTA supervised and unsupervised V+L pre-training methods.






