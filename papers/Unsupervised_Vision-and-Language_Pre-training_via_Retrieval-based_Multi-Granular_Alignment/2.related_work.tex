\section{Related Work}
\begin{figure*}[ht!]
\centering
\includegraphics[width=15.4cm]{figures/upvlp_model_4.pdf}
\caption{Overview of our method. On the left we form three types of image-text pairs as input data to learn cross-modal alignment on three different granularities: region-tag alignment, region-phrase alignment, and image-text alignment. The models is iteratively pre-trained on each granularity and the model parameters are shared. On the right-hand side, we demonstrate the details of the pre-training objectives for each granularity.  
}
\label{fig:model}
\end{figure*}

\paragraph{Vision-and-Language Pre-training}  
Inspired by the success of natural language processing~\cite{devlin2018bert, brown2020language}, there is a recent surge of interest in pre-training for vision and language.
For example, there are different architectures (\eg two-stream models~\cite{lu2019vilbert,tan2019lxmert,lu202012,yu2020ernie,li2021albef,kamath2021mdetr} vs. single-stream models~\cite{li2019visualbert,li2020unicoder,su2019vl,chen2020uniter,li2020unimo}), features (\eg regions~\cite{anderson2018bottom} \vs grids~\cite{huang2020pixel}), backbones (\eg ConvNets~\cite{huang2020pixel} \vs Transformers~\cite{kim2021vilt}) \etc.
All these works aim to exploit the large-scale aligned image-text corpora~\cite{lin2014microsoft,krishna2017visualgenome,sharma2018conceptual,ordonez2011sbu,kamath2021mdetr} to pre-train a powerful multi-modal model, which is then adapted to various downstream V+L tasks, such as VQA~\cite{antol2015vqa}, NLVR2~\cite{suhr2018corpus}, Visual Entailment (VE)~\cite{xie2019visual}, Referring Expression Comprehension~\cite{yu2016modeling}, and Image-Text Retrieval.

Various pre-training tasks have been introduced to achieve this, including the most notable Masked Language Modeling (MLM), Masked Region Modeling (MRM), and Image-Text Matching (ITM).
Several other variants have also been explored, such as predicting the object tags~\cite{li2020oscar, hu2020vivo}, sequence generation~\cite{zhou2020unified, wang2021simvlm}, word-region alignment~\cite{chen2020uniter}.
In this paper, we propose learning a multi-granular alignment between word and region, phrase and region, and image and sentence to better bridge the gap between vision and language.

\paragraph{Unsupervised Vision-and-Language Pre-training without Parallel Data}
Inspired by the works on multi-lingual contextual language modeling~\cite{conneau2019unsupervised,lample2018phrase,lample2017unsupervised, conneau2017word}, \uvisualbert~\cite{li2020unsupervised} first propose the \textit{unsupervised} vision-and-language pre-training without parallel data (\TaskName). 
\uvisualbert~\cite{li2020unsupervised} conducts the masked prediction on the text-only and image-only corpora and introduce the object tags as anchor points to bridge the two modalities.
The authors treat the tags as a sentence when performing MLM, where tags provide alignment with the regions in a picture and implicitly learn a tag-region-level alignment.
However, the anchor tags are still quite different from the text input, missing the sentence completeness and naturalness.
Besides, the latent cross-modal alignment is shown to be important in our analysis (from Fig.~\ref{fig:intro}).
As comparison, our pre-training involves a retrieval-based weakly aligned V+L data construction and learns a more comprehensive multi-granular cross-modal alignment.
With same data as \uvisualbert, our approach achieves a clear and consistent gain across all the downstream tasks in our experiments.


