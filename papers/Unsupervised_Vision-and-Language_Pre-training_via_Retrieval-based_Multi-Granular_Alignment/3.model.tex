%\section{\ModelFullName}
\section{Method}
In this section, we introduce the two core components of our \ModelName's architecture for~\TaskFullName: 
(1) construct a weakly aligned image-text corpus from independent vision and language data sources; 
(2) our novel pre-training curriculum to enable the model to capture the cross-modal alignment on three granularity including region-to-tag level alignment (RT),  region-to-noun phrase level alignment (RN), and image-to-sentence level alignment (IS). 

\subsection{Model Overview}
We use the well-known single-stream model architecture for our experiments as~\cite{li2019visualbert,li2020unicoder,su2019vl,chen2020uniter,li2020unimo}.
As shown in Fig.~\ref{fig:model}, our main backbone is a single transformer, where we feed the concatenation of visual embeddings of an image and the tokens of a caption as its input.
Given an image $\mathbf i$, we first use an off-the-shelf Faster R-CNN (VinVL~\cite{Zhang_2021_CVPR}) to detect the objects $\mathbf v = \{ v_1, ..., v_{k^v} \}$.
The visual embedding of each region is then encoded as the sum of its regional feature, its location embedding\footnote{The 5-dimensional vector [$\frac{x_1}{W}$,$\frac{y_1}{H}$,$\frac{x_2}{W}$,$\frac{y_2}{H}$,$\frac{(y_2-y_1)(x_2-x_1)}{W.H}$] is projected to the visual embedding space. $(x_1,y_1), (x_2,y_2)$ are the coordinates of the top left and bottom right point of the detected region, and $W,H$ are the image width and height.}, and the modality embedding.
For a given caption $\mathbf t$, we denote its tokenized sequence as $\mathbf t = \{ t_1, ..., t_{k^t} \} $.
After multiple layers of self-attention, the two modalities are fused together and the output hidden vectors can be used for various pre-training tasks.

\subsection{Weakly-aligned Image-Text Corpus}
\label{section:data_aug}
As in the analysis of Sec~\ref{section:intro}, we observe a strong correlation between the degree of image-text alignment in the training data and the performance of the pre-trained model.
% Given the unpaired image collection $I =\{I_1, I_2, \dots, I_{n_I}\}$ and text corpus $T = \{T_1, T_2, \dots, T_{n_T}\}$, 
Inspired by this finding, we believe it important to initialize some weak semantic alignment between the two modalities as the input source.
Specifically, we retrieve $\mathrm{k}$ sentences that are semantically closed to a given $I_i$. 
Previous work~\cite{tan2020vokenization} shows the visually grounded caption covers a good ratio of words that are naturally related to specific visual contents, \eg concrete nouns. 
Thus, we utilize the semantic association between the objects that appear in the image and a candidate sentence as the indicator to measure the alignment degree.

Specifically, we take the object tags $\mathbf o = \{ o_1, ..., o_{k^o} \}$ from the above detected $\mathbf v$ and feed the sequence into an off-the-shelf sentence BERT embedding model~\cite{reimers-gurevych-2019-sentence} to obtain the query embedding $\mathbf e_{\mathbf o}$.
Similarly, we feed each candidate sentence into the same model getting the candidate embedding $\mathbf e_{\mathbf t}$.
We retrieve the top $\mathrm{K}$ candidates with the highest cosine similarity score to form an initial weakly-aligned image-text pairs for a given image $\mathbf i$.
We denote the retrieved captions as $\{ \mathbf t^r (\mathbf i) \}_{r=1}^K$ and the overall weakly aligned corpus as $\mathbf R$.

% Specifically, we filter objects that are detected with confidence score lower than 0.2 and with small bounding box region size that is less than 0.05 of the image size  to focus on critical objects shown in the image. \mingyang{Should appear in the training set up.}
% Specifically, we feed the object-tag sentence and a candidate sentence into the pre-trained off-the-shelf Sentence Bert Embedding model \cite{reimers-gurevych-2019-sentence} to obtain their corresponding embeddings $\mathbf e_{\mathbf{o}}$ and $\mathbf e_{\mathbf{t}}$. 
% We then select the top $\mathrm{k}$ sentence candidates $T_{\text{ret}_i} = {T_{\text{ret}_i}^1, \dots,T_{\text{ret}_i}^k}$ with the highest cosine similarity score with the object list embedding vector $e_{O_i}$
% Eventually, we would have k retrieved sentences $T_{ret} = {T_{ret}^{1}, \dots,T_{ret}^{k}}$ for each image 
% to form the weakly-aligned image-text pairs with the query image $I_i$. 

\subsection{Pre-training Tasks}
% We consider that the success of vision and language pre-training relies on the capability to understand the cross-modal alignment on various granularity including: region-to-object tag level, region-to-noun phrase level, and image-to-sentence level. 
In this subsection, we introduce a set of pre-training objectives that we designed to facilitate the model to capture the different levels of vision and language alignment.
Fig.~\ref{fig:model} shows the overview of our model and its pre-training tasks.

% Unlike \uvisualbert \cite{li2020unsupervised} that focus on just capturing the region to object-tag alignment, we consider the successful unsupervised V+L pre-training should understand the cross-modal alignment on various granularity. Thus, we propose pre-trianing cu

\subsubsection{Region-Tag Alignment Learning}
We first propose to align the object tags onto the image regions.
As shown in Fig.~\ref{fig:model}(a), We concatenate the object tags detected from each image with its source image to form an input pair $[\mathbf o, \mathbf v]$ fed into the model. 
% The detected tags are processed similarly as the normal caption as a sequence of tag tokens $o= [o_{1:l}]$. 
%Following \cite{li2020unsupervised}, the position embedding for each tag token is the spatial box coordinate embedding of its corresponding region. 
% The position embeddings allow the model to distinguish the tags from different regions. 
% Given the pair of object list and image [$o$, $v$] from the training dataset $D$, 
% We randomly mask some tag tokens $o_k$, and some regions $v_j$, and train our model to predict the masked tag tokens and the properties of the masked regions. 
We denote the mask indices as $\mathbf{m}\in \mathbb{N}^M$\footnote{$\mathbb{N}$ is the natural numbers, $M$ is the vocabulary size, and $\mathbf{m}$ is the set of masked indices.}. 
We randomly mask out the object tags and regions, and apply masked language modeling (MLM) and masked region modeling (MRM) for the pre-training.

Specifically, MLM on the object tags is formulated as
\begin{equation*}
    \mathcal{L}_{\text{MLM}}^{\text{R-T}} = - \mathbb{E}_{(\mathbf o, \mathbf v)\sim \mathbf I} \log{P(\mathbf o_{\mathbf m} |\mathbf o_{\backslash \mathbf m}, \mathbf v)},
\end{equation*}
where the goal is to predict the masked object tags based on the observation of their surrounding tags $\mathbf o_{\backslash \mathbf m}$ and image regions $\mathbf v$.
On the vision side, MRM includes both masked region classification loss (MRC) and masked region feature regression loss (MRFR):
\begin{equation*}
    \begin{split}
    \mathcal{L}_{\text{MRM}}^{\text{R-T}} = \mathbb{E}_{(\mathbf o, \mathbf v)\sim \mathbf I}  [f_{\text{MRC}}(\mathbf v_{\mathbf m} | \mathbf v_{\backslash \mathbf m}, \mathbf o) + f_{\text{MRFR}}(\mathbf v_{\mathbf m} | \mathbf v_{\backslash \mathbf m}, \mathbf o) ].
    \end{split}
\end{equation*}
% To calculate the $\mathcal{L}_{\text{MRC}}$ and $\mathcal{L}_{\text{MRFR}}$, we first obtain the transformer output $h_j$ of the masked region $i=v_j$ at the final layer. For $\text{MRC}$, a fully connected (FC) layer $\phi_{\text{MRC}}$ is applied to predict the object category as a normalized distribution over the total number of $K$ classes of the object categories. 
% Thus, $\mathcal{f}_{\text{MRC}}=CE(\phi_{\text{MRC}(h_j)}, c_j)$ is the standard cross-entropy loss. 
% Additionally, for $\text{MRFR}$ we have another FC layer $\phi_{\text{MRFR}}$ to project $h_j$ into the same dimension space of the ROI feature of the masked region $f_j$. Then we apply L2 regression to compute the loss: $f_{\text{MRFR}}=||\phi_{\text{MRFR}(h_j)}- f_j||_2^2$.
Between the two, MRC learns to predict the object semantic class for each masked region  $c(\mathbf v_{\mathbf m})$.
We feed the last hidden output of the masked region $\mathbf v_{\mathbf m}$ into a FC layer and softmax function to predict the classification probabilities $g_{\theta} ( \mathbf v_{\mathbf m} )$.
The objective is to minimize the cross-entropy of
$ f_{\text{MRC}}(\mathbf v_{\mathbf m} | \mathbf v_{\backslash \mathbf m}, \mathbf o) = \mbox{CE}( c(\mathbf v_{\mathbf m}) , g_{\theta} ( \mathbf v_{\mathbf m} ) ) $.
MRFR learns to regress the transformer output of each masked region $\mathbf v_{\mathbf m}$ to its visual features. 
We apply a FC layer to convert its hidden output to a vector $h_\theta (\mathbf v_{\mathbf m})$ of the same dimension as the input regional feature $r(\mathbf v_{\mathbf m})$.
We apply L2 regression:
$f_{\text{MRFR}}(\mathbf v_{\mathbf m} | \mathbf v_{\backslash \mathbf m}, \mathbf o)  = || h_\theta (\mathbf v_{\mathbf m}) - r(\mathbf v_{\mathbf m}) ||^2_2$.

For region-tag alignment learning, we have our pretraining objective function as 
\begin{equation}\nonumber
\mathcal{L}^{\text{R-T}} =  \mathcal{L}_{\text{MLM}}^{\text{R-T}} + \mathcal{L}_{\text{MRM}}^{\text{R-T}} 
\end{equation}

\subsubsection{Region-Noun Phrase Alignment Learning}

Due to the small vocabulary size of object tags, the region-tag alignment learning can only capture a limited amount of localized concepts.
To increase the diversity of concepts, we propose to align the noun phrases from the retrieved sentences to the corresponding regions as well.
As in Fig.~\ref{fig:model}(b), given an image $\mathbf i$ and its retrieved weakly aligned caption $\mathbf t^r (\mathbf i)$, we first detect the noun phrases from the caption using spacy~\cite{spacy2}.
Note the detected noun phrases sometimes contain the attribute words, which further benefits this pre-training task.
We link the noun phrase to its closest visual region by computing the word2vec similarity between the phrase and object tag (associated to each region).
The pre-training still consists of MLM and MRM but are performed with different masking strategy and supervision signal.

Specifically, for both MRM and MLM, we only mask the linked noun phrases from the caption or the linked object regions.
We make the masking probability proportional to the linked similarity score.
Each time we only mask out one modality (phrase or region) to encourage it to be recovered by its linked content.
The region-to-phrase MLM is then formulated as
$\mathcal{L}_{\text{MLM}}^{\text{R-P}} = - \mathbb{E}_{(\mathbf v, \mathbf t^r)\sim \mathbf R} \log{P(\mathbf t^r_{\mathbf m} |\mathbf t^r_{\backslash \mathbf m}, \mathbf v)}$.

On the vision side, we propose using the phrase-guided masked region-to-token classification (p-MRTC) on the masked regions:
\begin{equation*}
    \begin{split}
    \mathcal{L}_{\text{MRM}}^{\text{R-P}} = \mathbb{E}_{(\mathbf v, \mathbf t^r)\sim \mathbf R}  f_{\text{p-MRTC}}(\mathbf v_{\mathbf m} | \mathbf v_{\backslash \mathbf m}, \mathbf t^r),
    \end{split}
\end{equation*}
where we directly classify the masked region to its linked noun phrase (sub-word tokens) in BERT vocabulary.
Enlarging the vocabulary has shown to be beneficial to MRM~\cite{Zhou_2021_CVPR}.
Our proposed p-MRTC leverages the additional noun-phrase to encourage more diverse local region to language alignment.

For region-noun phrase alignment learning, we have our pretraining objective function as 
\begin{equation}\nonumber
\mathcal{L}^{\text{R-P}} =  \mathcal{L}_{\text{MLM}}^{\text{R-P}} + \mathcal{L}_{\text{MRM}}^{\text{R-P}} 
\end{equation}


\subsubsection{Image-Sentence Alignment Learning}
\label{section:itm}
We apply image-text matching (ITM) objective as the previous supervised V+L pre-training research \cite{chen2020uniter,li2020unicoder} to learn the cross-modal sentence-level alignment. 
As in Fig.~\ref{fig:model}(c), given an input pair [$\mathbf v$, $\mathbf t^r$], the final hidden vector of the special token $\text{[CLS]}$ is fed through a FC layer to output a single score $\mathbf s_{\theta}(\mathbf v, \mathbf t^r)$, which predicts if the given image-text input is a semantically matched pair or not. 
We use the label $y\in \{0,1\}$ to indicate if a retrieved pair is a match.
The training objective for the ITM task is to minimize the binary cross-entropy loss:
$
\mathcal{L}_{\text{ITM}} =\mbox{CE}( y ,s_{\theta}(\mathbf{v}, \mathbf{t}^r) ) 
$.
% \begin{equation*}
%     \mathcal{L}_{\text{ITM}} = -\mathbb{E}_{(\mathbf{v},\mathbf{t^r})\sim \mathbf{R}} [y \log s_{\theta}(\mathbf{v}, \mathbf{t}^r) + (1-y) \log (1-s_{\theta}(\mathbf{v}, \mathbf{t}^r))]
% \end{equation*}
On the language side, we also apply standard MLM to help the model learn to align other language tokens besides noun phrases and object tags to the visual context.
The objective function is then formulated as $\mathcal{L}_{\text{MLM}}^{\text{I-S}} = - \mathbb{E}_{(\mathbf v, \mathbf t^r)\sim \mathbf R} \log{P(\mathbf t^r_{\mathbf m} |\mathbf t^r_{\backslash \mathbf m}, \mathbf v)}$.
The image-sentence level alignment pretraining objective function is
\begin{equation}\nonumber
\mathcal{L}^{\text{I-S}} =  \mathcal{L}_{\text{MLM}}^{\text{I-S}} + \mathcal{L}_{\text{ITM}}
\end{equation}


\subsection{Multi-Granular Pre-training Curriculum}
We propose a multi-granular curriculum to iteratively pre-train the model on the region-to-tag, region-to-noun phrase, and image-to-sentence level. 
According to our findings in Sec.~\ref{section:intro}, learning from image-text pairs with higher degree of cross-modal alignment is beneficial to the performance of unsupervised V+L pre-trained model. 
Therefore, we propose using an estimated image-text alignment score to guide our multi-granular pre-training. 
Specifically, we have an ITM header defined in Sec.~\ref{section:itm} to learn the image-text alignment. 
We also use it to predict matching score as a weight to modulate the input data for each of our retrieval-based pre-training tasks. 
This allows us to provide more importance to relatively more aligned image-text pairs over time to help our model to learn better cross-modal alignment on multiple granularities. 

To train the alignment model's ITM classifier, we use our retrieved corpus $\mathbf{R}$ as positive samples and randomly shuffled pairs as negative samples in the first $m$ epochs. This warms up the models to make reasonable estimations on the alignment of image-text input pairs.
After $m$ epochs, we start to incorporate the alignment prediction score $w_{\text{ITM}}$ in our training objective. 
To summarize, our multi-granular pre-training loss is 
\begin{equation}\nonumber
\mathcal{L}=  
\begin{cases}
      \mathcal{L}^{\text{R-T}} + \mathcal{L}^{\text{R-P}} +\mathcal{L}^{\text{I-S}}   &\text{if epoch} < m\\ 
      \mathcal{L}^{\text{R-T}} + w_{\text{ITM}} (\mathcal{L}^{\text{R-P}}  +  \mathcal{L}^{\text{I-S}}) &\text{if epoch} \geq m, \\
    \end{cases}
\end{equation}
where $\mathcal{L}^{\text{R-T}}$, $\mathcal{L}^{\text{R-P}}$, and $\mathcal{L}^{\text{I-S}}$ are the loss functions for region-tag alignment pre-training, region-noun phrase alignment pre-training, and image-sentense alignment pre-training. We set m as 1 in our final implementation.
