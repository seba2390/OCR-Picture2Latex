\begin{table*}[!ht]\centering
\small
\begin{tabular}{l|c|c|c|ccc|c}\toprule
\multirow{2}{*}{ Model } &VQA2 &NLVR2 &VE & \multicolumn{3}{c|}{RefCOCO+} & \multirow{2}{*}{ Meta-Ave } \\
&Test-Dev &Test-P &Test &Dev &TestA &TestB & \\\cmidrule{1-8}
ViLBERT\cite{lu2019vilbert} &70.6 &- &- &72.3 &78.5 &62.6 & - \\
VL-BERT\cite{su2019vl} &71.2 &- &- &71.6 &77.7 &61.0 & - \\
$\text{UNITER}_{\text{CC}}$\cite{chen2020uniter} &71.2 &- &- &72.5 &79.4 &63.7 & - \\
VisualBERT \cite{li2019visualbert,li2020unsupervised} &70.9 &73.9 &- &73.7 &79.5 &64.5 & - \\
Aligned VLP &\textbf{72.5} &\textbf{75.9} &\textbf{78.7} &\textbf{82.1} &\textbf{86.6} &\textbf{75.0} & \textbf{77.3} \\
\midrule
Base &70.1 &51.2 &73.2 &69.4 &74.8 &60.3 & 65.9 \\
\uvisualbert \cite{li2020unsupervised} &71.8 &53.2 &76.8 &78.2 &83.6 &69.9 & 70.0\\
$\text{\ModelName}_{\text{CC}}$ &\textbf{72.1} &\textbf{73.4} &\textbf{77.3} &\textbf{80.3} &\textbf{85.5} & \textbf{73.7} & \textbf{75.8} \\
$\text{\ModelName}_{\text{BC}}$ &71.2 &67.1 &77.1 &79.7 &85.0 &72.7 & 73.8 \\
\bottomrule
\end{tabular}
\caption{Evaluation results on four V+L downstream tasks. Our model trained with un-aligned data ($\text{\ModelName}_{\text{CC}}$, $\text{\ModelName}_{\text{BC}}$) achieves comparable performance with the supervised model trained with aligned data (Aligned VLP). $\text{\ModelName}_{\text{CC}}$ and $\text{\ModelName}_{\text{BC}}$ also outperform {\uvisualbert } on nearly all tasks.}
\label{tab:main}
\end{table*}

\begin{table*}[!ht]\centering
\small
\begin{tabular}{l|c|c|c|ccc|c}\toprule
\multirow{2}{*}{V+L Alignment} &VQA &NLVR2 &VE &\multicolumn{3}{c|}{RefCOCO+} & \multirow{2}{*}{Meta-Ave}\\
&Test-Dev &Test-P &Test &Dev &TestA &TestB & \\\cmidrule{1-8}
$\text{\ModelName}_{\text{CC}}$ (R-T)  &71.7 &52.0 &75.6 &78.7 &83.3 &70.0 & 69.5\\
$\text{\ModelName}_{\text{CC}}$ (R-N)  &71.4 &69.4 &76.5 &77.4 &81.5 & 68.7 & 73.7 \\
$\text{\ModelName}_{\text{CC}}$ (I-S)  &71.6 &71.5 &76.8 &75.7 &80.3 &67.9 & 73.9\\
$\text{\ModelName}_{\text{CC}}$ (R-T + R-N) &71.9 &72.4 &76.4 &79.3 & 84.5 & 71.7 & 75.0 \\
$\text{\ModelName}_{\text{CC}}$ (R-T + R-N + I-S) &\textbf{72.1} & \textbf{73.4} &\textbf{77.3} &\textbf{80.3} & \textbf{85.0} &\textbf{73.7} & \textbf{75.8}\\
\bottomrule
\end{tabular}
\caption{Effect of cross-modal alignment on the three types of granularities: region-tag alignment(R-T), region-noun phrase alignment(R-N), and image-sentence alignment(I-S)}\label{tab:ablation_align}
\end{table*}


\section{Experiments}
In this section, we provide the detailed experimental set up to evaluate our proposed {\ModelName } against previous supervised and unsupervised VLP models. More specifically, we introduce our pre-training dataset, baselines, and our pre-training setting.

\subsection{Pre-training Datasets}
We prepare the un-aligned data under two different settings: (1) We use images and text separately from Conceptual Captions (CC) \cite{sharma2018conceptual} ignoring the alignment information; (2) We use images from Conceptual Captions (CC) \cite{sharma2018conceptual} and text from BookCorpus (BC) \cite{Zhu_2015_ICCV}. 
Setting (1) sets up a fair comparison with previous supervised methods by keeping the domain and the quality of training data consistent. Our proposed model trained in this setting is called \ModelName$_{CC}$. 
Setting (2) mimics a more realistic challenge where we have large-scale images and text data from different domains, in particular the text sources are not similar to captions of the images. \ModelName$_{BC}$ has been trained in this setting.

As introduced in section \ref{section:data_aug}, for each image we retrieve 5 text data points (captions from CC or sentences from BC) from the text corpus that are semantically similar to the detected objects in the image. 
This creates weakly-aligned image-text pairs for our pre-training models. 

\subsection{Baselines}
We compare the performance of our proposed {\ModelName } to the following baselines: 

\head{Base Model} VisualBERT that is initialized from BERT. It does not undergo any pre-training but is directly fine-tuned on the downstream tasks. 

\head{Supervised Pre-trained Models} Supervised pre-trained VLP models that are trained only on CC, including VILBERT\cite{lu2019vilbert}, VL-BERT\cite{su2019vl}, and UNITER\cite{chen2020uniter}. We also report the numbers on the Supervised VisualBERT implemented in \uvisualbert\cite{li2020unsupervised} that is trained on CC and an additional 2.5 Million text segments from BC. 
For fair comparison with our proposed method, we also introduce the aligned vision-language pre-training model (Aligned VLP) that is pre-trained on the 3M (image, caption) pairs from CC and 3M (image, object tag) pairs. 

\head{Unsupervised Pre-trained Models} 
{\uvisualbert } is pre-trained on individual image or text corpus in a round-robin fashion and captures the cross-modal alignment by using detected object tags as the anchor point. 
For fair comparison, we re-implemented this method to pre-train with the VinVL object features\cite{zhang2021vinvl} and BC. 
\subsection {Training Setup}\label{sec:training_setup}
Our transformer architecture consists of 12 layers of transformer blocks, where each block has 768 hidden units and 12 self-attention heads. 
We initialize the model from $\text{BERT}_{base}$ and pre-train for 20 epochs on their respective pre-training datasets with a batch size of 480. The region features for images are obtained from the pre-trained VinVL object detectors \cite{zhang2021vinvl}. We use Adam optimizer \cite{ADAM} with a linear warm-up for the first 10\% of training steps, and set the peak learning rate as 6e-5. After warm up, a linear-decayed learning-rate scheduler gradually drops the learning rate for the rest of training steps. 
All models were trained on 4 NVIDIA A100 GPUs, with 40GB of memory per GPU using MMF\cite{singh2020mmf}.
The pre-training takes 3 days.
We evaluate our pre-trained models on four downstream tasks: Visual Question Answering (VQA 2.0)\cite{anderson2018bottom}, Natural Language for Visual reasoning\cite{suhr2018corpus} ($\text{NLVR}^2$), Visual Entailment\cite{xie2019visual} (VE), and Referring Expression\cite{yu2016modeling} (RefCOCO+). 
% To validate our proposed sentence-image alignment pre-training, we also conduct a zero-shot evaluation with image-text retrieval task on Flickr30K\cite{young-etal-2014-image}. 
Detailed training settings for each task can be found in our supplementary material. 
\subsection{Experimental Results}
We first compare {\ModelName } to various supervised models that are pre-trained on CC and to the state-of-the-art unsupervised V+L pre-training method, {\uvisualbert } on the four downstream tasks. Besides reporting scores for each individual task, we also compute the meta-average score to reflect the overall performance across all tasks. 
The results are summarized in Table \ref{tab:main}.

\myparagraph{Compared to Base.} It is clear from Table~\ref{tab:main} that both \ModelName$_{CC}$ and \ModelName$_{BC}$ outperform the Base model by a large margin on all benchmarks. 


\myparagraph{Compared to Aligned VLP.} It also achieves better performance than existing supervised models like VilBERT\cite{lu2019vilbert}, which is potentially due to the usage of better visual regional features of VinVL~\cite{Zhang_2021_CVPR}. When compared to Aligned VLP, which is trained with the same architecture and visual features, our model is only slightly worse. 
This shows the effectiveness of our proposed pre-training curriculum which can learn comparable universal representation across vision and language as the supervised models without any parallel image-text corpus. 

\myparagraph{Compared to UVLP. }Our {\ModelName } also achieves consistently better performance than the previous UVLP method: \uvisualbert. 
This improvement shows how our proposed cross-modal alignment pre-training curriculum effectively bridges the gap across the two modalities.
In particular, our model outperforms {\uvisualbert } in the task of NLVR2 by more than 20\%. 
As NLVR2 is known to benefit more from image-sentence cross-modal alignment from previous supervised V+L pre-training research \cite{chen2020uniter}, this observation indicates that our model is able to capture the instance-level cross-modal alignment without parallel data. 
When {\ModelName } is trained on BC text and CC images \ie \ModelName$_{BC}$, it still achieves comparable or better performance than {\uvisualbert } except for VQA.  
The slight advantage  {\uvisualbert } has over \ModelName$_{BC}$ in VQA is potentially due the similar style between the VQA text and the pre-trained CC captions. 
However, this does not overshadow the overall better performance of {\ModelName}. 
It shows that our proposed method is more robust than {\uvisualbert } training on the uni-modal datasets collected from separate domains, which makes it more useful in practical settings.    
\begin{figure}[h!]
\centering
\includegraphics[width=0.8\linewidth]{figures/number_candidate_plot.pdf}
\caption{Meta average scores of non-parallel V+L pre-training with different number of retrieved candidate sentences.}
\label{fig:ablation_ncandidate}
\end{figure}


\begin{figure*}[h!]
\centering
\includegraphics[width=16cm]{figures/retrieved_positives.pdf}
\caption{Examples of retrieved text from both CC and BC. The covered grounded noun phrases in retrieved sentences are highlighted in green bar for positive examples.}
\label{fig:visualization}
\end{figure*}

\subsubsection{Ablation Study on Multi-Granular Alignment}
We conduct ablation study to verify the effectiveness of the three types of visual-language alignment for unsupervised V+L pre-training, namely region-tag alignment (R-T), region-noun Phrase alignment (R-N), and image-sentence alignment (I-S). We first evaluate each individual type of alignment to measure its usefulness for different downstream tasks. Then, we gradually add each type of alignment into the UVLP.  For this ablation study we pre-train {\ModelName } on CC images and text, and the results are summarized in Table \ref{tab:ablation_align}. 

From Table~\ref{tab:ablation_align}, we can see that aligning local regions to object tags (R-T) and noun phrases (R-N) are especially helpful for the task of RefCOCO+, which requires the model to understand specific objects that natural expressions describe. Meanwhile, aligning the image and sentence at instance-level (I-S) benefits NLVR2 and VE. Especially on NLVR2, the model that captures the global vision and language alignment \ModelName$_{CC}$ (I-S) obtains 19.5\% gain over the model that only learns the local alignments between regions and object tags \ModelName$_{CC}$ (R-T). This observation is consistent with previous research \cite{chen2020uniter}, where the performance of model on NLVR2 is boosted after introducing pre-training objectives that capture the cross-modal alignment in the image-text pairs. Our results demonstrate that even with just weakly-aligned sentences, we can still effectively learn the instance-level cross-modal alignment. 
% \mingyang{Maybe also show zero-shot Image-Sentence Alignment performance to further proves the learned cross-modal alignment on vision and language;} 
Combining the region-tag and region-noun phrase alignment (R-T+R-N) for UVLP, we observe that these two types of grounding and matching compensate each other. \ModelName$_{CC}$ (R-T+R-N) shows a marginal but consistent improvement over models that only learn a single type of local region-to-language alignment (R-T, R-N). After adding object-phrase level alignment we can further improve the performance on NLVR2 and VE, which gives us our best performing model \ModelName$_{CC}$ (R-T + R-N + I-S). 

\subsubsection{Ablation Study on Number of Retrieved Candidates}
We conduct experiments to verify the impact the number of retrieved candidate text for each image has on the performance. We create three variants of pre-training corpus, where the number of retrieved candidate are 1, 5, and 10 based on the rank of the similarity of each candidate text to the query image's detected object tags. The candidate text is sampled from CC. We pre-train our {\ModelName } model with only the pre-training objectives to capture the sentence-image alignment (I-S). For each variant of pre-training corpus, we train the model for the same number of steps. We compute the meta average score for the three resulting pre-trained models and visualize them in Fig.~\ref{fig:ablation_ncandidate}. 

Fig.~\ref{fig:ablation_ncandidate} shows that retrieving more than one candidate text for an image greatly benefits the pre-trained model to learn a better joint representation between vision and language, demonstrated by stronger performance in the downstream tasks. 
We suspect this is because the closeness between the candidate caption and the detected object tags in language embedding space does not always mean a better alignment between the candidate caption and the image. A better and more semantically similar caption candidate for the image could be found in the other caption candidates. However, when we increase the number of candidate captions to 10, we observe a slight drop on the overall performance compared to the model that is pre-trained on corpus with 5 candidate captions. This indicates that having too many candidate captions to form the weakly-aligned pairs with the query image for V+L pre-training may also introduce unnecessary noise. Hence, we set the number of retrieved captions in our experiments to 5.
 

\subsubsection{Visualization}
To get a sense of the quality of the retrieved sentences, we show some examples of retrieved text from both CC and BC in Fig.~\ref{fig:visualization}. The first row demonstrates a positive case of retrieved captions from CC, where we observe a good coverage of the objects in the image such as ``young woman", ``sofa", and ``couch" in the top retrieved sentences. Similarly, our retrieval method can also retrieve good candidates from BC that describe many visual objects from the image as depicted in row 2. This observation demonstrate the effectiveness of picking candidates based on their closeness to the object list in the language embedding space. 

We also compare the text-to-image attention between the pre-trained \uvisualbert~and \ModelName~without task-specific fine-tuning as~\cite{chen2020uniter,Zhou_2021_CVPR}.
As shown in Fig.~\ref{fig:attention}, we feed into the models an aligned pair whose caption is ``young woman seated on the beach", we visualize the local cross-modality alignment between regions and tokens.
we found our full model \ModelName~can better attend on the described regions, showing higher-quality alignment is learned through the proposed pre-training.
More visualizations are in the supplementary file.



\begin{figure}[h!]
\centering
\includegraphics[width=0.8\linewidth]{figures/attention.pdf}
\vspace{-0.5cm}
\caption{Text-to-image attention given the aligned pair whose caption is ``young woman seated on the beach".}
\label{fig:attention}
\end{figure}


