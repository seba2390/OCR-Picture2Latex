%
%
%
%

\documentclass[letterpaper,conference]{IEEEtran} 
\usepackage{mathptmx} %

\newcommand{\ignore}[1]{}
\usepackage{fancyhdr}
\usepackage[normalem]{ulem}
\usepackage[hyphens]{url}
%

\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{subcaption}
\usepackage{xspace}
\usepackage{bbold}
\usepackage{pbox}
\usepackage[rightcaption]{sidecap}
\usepackage{flushend}
\sidecaptionvpos{figure}{c}

%
%
%

\newcommand{\fixme}[1]{\textcolor{red}{#1}}
\newcommand{\JA}[1]{\textcolor{blue}{JA:{#1}}}
\newcommand{\PJ}[1]{\textcolor{red}{PJ:{#1}}}
\definecolor{Awesome}{rgb}{1.0, 0.08, 0.58}
\newcommand{\am}[1]{{\color{Awesome}{AM: #1}}}
\newcommand{\TAYH}[1]{\textcolor{green}{TH:{#1}}}
\newcommand{\nej}[1]{\textcolor{purple}{NEJ:{#1}}}


%
\newcommand{\ZFL}{\textit{Cnvlutin}\xspace}
\newcommand{\ZF}{\textit{CNV}\xspace}
\newcommand{\ZFLn}{\textit{Cnvlutin\textsuperscript{2}}\xspace}
\newcommand{\ZFn}{\textit{CNV}\textsuperscript{2}\xspace}
%
%
%

\newcommand{\ZFNAf}{ZFNAf\xspace}
\newcommand{\BASE}{DaDianNao\xspace}

\newcommand{\ideal}{\textit{NoIdle~}}
%
\newcommand{\hpcasubmissionnumber}{33}
%
\newcommand{\textsuper}[1]{$^\textnormal{#1}$}
 \fancypagestyle{firstpage}{
   \fancyhf{}
 \setlength{\headheight}{50pt}
 \renewcommand{\headrulewidth}{0pt}
%
%
%
}  

\begin{document}

%
%
\title{Cnvlutin\textsuperscript{2}: Ineffectual-Activation-and-Weight-Free Deep Neural Network Computing}
%
%
%
\author{Patrick Judd \quad Alberto Delmas Lascorz \quad Sayeh Sharify \quad Andreas Moshovos\\ 
\\
  University of Toronto}

\author{
Patrick Judd,
Alberto Delmas,
Sayeh Sharify
\& Andreas Moshovos\\
Department of Electrical and Computer Engineering, University of Toronto\\ 
\texttt{\{juddpatr, delmasl1, sayeh, moshovos\}@ece.utoronto.ca} \\ 
}

%

\maketitle
\thispagestyle{firstpage}

\pagestyle{plain}

\begin{abstract}
%
We discuss several modifications and extensions over the previous proposed \textit{Cnvlutin}
(\ZF) \ accelerator for convolutional and fully-connected layers of Deep Learning Network. 
We first describe different encodings of the activations that are deemed ineffectual.  The encodings have different memory overhead and energy characteristics.  We propose using a level of indirection when accessing activations from memory to reduce their memory footprint by storing only the effectual activations. We also present a modified organization that detects the activations that are deemed as ineffectual while fetching them from memory. This is different than the original design that instead detected them at the output of the preceding layer. Finally, we 
present an extended \ZF that can also skip ineffectual weights.



%

\end{abstract}


\input{intro.tex}
%
%
\input{design.tex}
%
%

%

\section{Conclusion}
\label{sec:theend}

%

We presented a set of modification and extension to the \ZF CNN accelerator. The modifications change the memory storage and energy tradeoff by encoding ineffectual activations differently than the original \ZF proposal. We also presented a technique that identifies the ineffectual activations while reading them from memory imposing no memory storage and access overheads. Finally, we decribed an extension to \ZF that can benefit from ineffectual weights also.
%

%
 
%

%

%

%
%
%
%
%
%
%
\section*{Acknowledgments} This work was supported by an NSERC Discovery Grant.

%

%
\Urlmuskip=0mu plus 1mu\relax
\bibliographystyle{ieeetr}
\bibliography{ref}
%

\end{document}
