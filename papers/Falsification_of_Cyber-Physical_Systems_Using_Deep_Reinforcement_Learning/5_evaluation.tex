%!TEX root = 0_main.tex
\section{Preliminary Results}\label{sec:exp}
%To evaluate the efficiency and effectiveness of our work, we conduct experiments with well known CPS models in Matlab/Simulink.
%We compare our reinforcement learning based technique with existing methods, i.e., simulated annealing and cross entropy based methods, and analyze the results.
%\ytodo{Mention Breach when we finish the comparison with Breach}

%We discuss our implementation and report our preliminary evaluation result in this paper.

%\subsection{Implementation}
%\vspace{5mm}
\noindent\textbf{Implementation}
%\subsubsection{Implementation}
\begin{figure}[tp]
\centering
\scriptsize
\includegraphics[scale=0.44]{./fig/Architecture.pdf}
\caption{Architecture of our system}
\label{fig:arch}
\vspace{-7mm}
\end{figure}
%
The overall architecture of our system is shown in Fig.~\ref{fig:arch}.
Our implementation consists of three components, i.e., input generation, output handling and simulation.
The input generation component adopts reinforcement learning techniques and is implemented based on the ChainerRL library~\cite{ChainerRL}.
We use default hyper-parameters in the library or sample programs without change.
The output handling component conducts reward calculation using dp-TaliRo~\cite{S-TaliRo}.
The simulation is conducted with Matlab/Simulink models, which are encapsulated by the openAI gym library~\cite{1606.01540}.
%The output of the system is normalized (linearly mapped) into the interval $[-1, 1]\subseteq \mathbb{R}$, since deep reinforcement learning has the best performance for normalized inputs.
%The robustness and reward are calculated using normalized outputs.
%All input to the system are first mapped into the interval $[-1, 1]$ and then linearly transformed to the actual range, which the system accepts.
%%%
%%
%
%To falsify a CPS model, we run the model step by step using a fixed sampling rate, which is a period of the time obeyed by the model.
%The reward is calculated based on the trace of states, from the starting state to the current state of the run, based on the formula defined in~\ref{eq:reward}.
%For each step, the state of the model and the reward are used as input to the reinforcement learning agent.
%The reinforcement learning agent then returns the next action to take.
%Using the suggested action as an input, we restart the simulation cycle.
%
%%
%%%
%To calculate the robustness value of the monitoring formula, we use a function in S-TaliRo for calculating robustness.
%S-TaliRo provides two kinds of functions to calculate robustness,  one is a MATLAB function \verb|dp-taliro| and its variants; the other is a runtime monitoring function, which is a Simulink block.
%Our experiment uses the \verb|dp-taliro| function to calculate robustness, \textcolor{red}{because...}.
%However, \verb|dp-taliro| does not support past-dependent formulas.
%Therefore, to calculate robustness, we revert the order of the trace, i.e., from the current state to the beginning state.
%We also revert the time flow of the monitoring formula.
%For example, $\square_{[-3, 0]}p$ is converted to $\square_{[0, 3]}p$ and $\square_{[-5, -3]}p \rightarrow \square_{[-3, 0]}q$ is converted to $\square_{[3, 5]}p \rightarrow \square_{[0, 3]}q$.
%However, this conversion is not always possible.
%The until operator $\mathcal{U}_I$, the next operator $X$ and nested temporal operators seem resist from being converted.
%Therefore, our implementation can only handle limited properties.
%\stodo{The sentence ``The until operator $\mathcal{U}_I$, the next operator $X$ and nested temporal operators seem resist from being converted.
%Therefore, our implementation can only handle limited properties.'' uses the word ``seem'', which is not good. We need to make assured claims, i.e., give clear claims, whether those operators resist from being converted or not. }

%Then, we input the current state and the calculated reward to a reinforcement learning agent.
%The reinforcement learning agent then returns the next \emph{action} to take.
%Using the suggested action as an input, we restart and execute the simulation until the next step.
%We assume that the input (\textcolor{red}{to the simulation model?}) is constant during each step of the simulation.
%
%%%
%Finally, if the simulation reaches the end, we calculate robustness of the property which we want to falsify using the full trajectory.
%We use \verb|dp-taliro| for this purpose.
%%%
%
%but we do not revert the order of the trace nor the time flow of the formula as the case of the monitoring formula, because
%
%If the robustness of the property is negative, we successfully falsify the property and terminate the process.
%Otherwise, we reset the simulation and signifies the reinforcement learning agent at the end of the \emph{episode}.

%Our implementation has a large overhead in computation time.
%This is mostly due to the fact that, we utilized step-based reinforcement learning algorithms, and after getting the feedback from the RL agent, we stop and restart the simulation for each sampling cycle.
%Another reason is that reinforcement learning agents are implemented by Python while robustness calculation and simulation are implemented by MATLAB/Simulink.
%This creates another overhead to interpolate Python and MATLAB runtime.

%\ytodo{The part below is better to be moved to the preliminary.}
%\noindent\textbf{Double DQN}
%We adopt DDQN to accelerate the process of choosing inputs which may falsify the robustness properties. We describe the implementation of DDQN in our approach.  For the Autonomous system, the inputs to the model are the values of throttle and break, which are normalized to the range of ...; and the outputs of the model are Engine speed $\omega$ (RPM) and vehicle speed $\mathbf(v)$ (mph).In order to use DDQN to find the counterexample, which falsifies the robustness property of the autonomous model, we need to find a way to link the robustness value with the rewards calculated by DDQN.
%In Deep Q Network~\cite{mnih2013playing},the outputs are q-values of each action available at the current state. However, the actions taken (throttle and break value pairs) by the autonomous model is contentious, which results in an infinite input space. To solve this problem, we use Normalized Advantage Functions (NAF)~\cite{pmlr-v48-gu16}, which leads into Advantage. The relationship between Q(q-value),A(advantage) and V(value) is
%\begin{equation}
% Q(s,a) = A(s,a) + V(s)
%\end{equation}
%Then we can limit A less than 0, and select the A which equals 0.

%   The output of the DQN is the best action to take in the current state and it's corresponding q-value. Moreover, to avoid the problem that may be caused by over estimation, we adopt double DQN which uses Current Q-network to select actions and older Q-network to evaluate actions. Current Q-network use max to find the best action, overestimation may happen here. But another Q-network which doesn't use max evaluates q-value of the selected action. It may high or low. So over estimation is solved.


%\subsection{Evaluation Settings}
\vspace{4mm}
\noindent \textbf{Evaluation Settings}
%\subsubsection{Evaluation Settings}
We use a widely adopted CPS model, automatic transmission control system ($\ATmodel$) ~\cite{bardh2014benchmarks}, to evaluate our method.
%
%The system model is from a public demonstration of modeling an automatic transmission control with the Stateflow~\cite{ATinSF}
%%~\footnote{https://mathworks.com/products/stateflow.html}
%package in MATLAB.
%
$\ATmodel$ has throttle and brake as input ports, and the output ports are the vehicle velocity $v$, the engine rotation speed $\omega$ and the current gear state $g$.
%Although the size of the model is relatively small comparing the actual systems in industry, but its dynamics contains both discrete and continuous values.
%Therefore, it is suitable as a benchmark of falsification on CPSs.
%
We conduct our evaluation with the formulas in Table~\ref{tab:formulas}.
Formulas $\varphi_1$--$\varphi_6$ are rewriting of  $\varphi^{AT}_1$--$\varphi^{AT}_6$ in benchmark~\cite{bardh2014benchmarks} into life-long properties in our approach.
%We do not use the original benchmark because we focus on life-long properties.
In addition, we propose three new formulas $\varphi_{7}$--$\varphi_{9}$.
%\textcolor{red}{For all formulas, we tune parameters in the formulas such that they are difficult to falsify, therefore can differentiate performance of each method.}
%
For each formula $\varphi_1$--$\varphi_9$, we compare the performance of our approaches (A3C, DDQN), with the baseline algorithms, i.e., simulated annealing ($\SA$) and cross entropy ($\CE$).
%
For each property, we run the falsification procedure 20 times.
For each falsification procedure, we execute simulation episodes up to 200 times and measure the number of simulation episodes required to falsify the property.
If the property cannot be falsified within 200 episodes, the procedure fails.
%We record whether each falsification procedure is successful or not.
%
% Further, for fair comparison, we change sampling rate and choose the best performing rate for each combination of a method and formula.
% Here, the best means the smallest median of the number of simulations runs required to falsify the formula.
% If the tie occurs, we further compares success rates of the falsification procedure.
% Finally, if still we cannot decide we use the media of the execution time of the falsification processes.
%
We observe that $\Delta_{T}$ may strongly affect the performance of each algorithm.
%For example, simulated annealing tends to perform badly if we use high sampling rate.
%On the other hand, reinforcement learning based methods are not affected by high sampling rate.
%Based on the above observation,
Therefore, we
%choose the $\Delta_{T}$ which gives the best performance (prioritized by numEpisode and success rate) of each algorithm.
vary $\Delta_{T}$ (among \{1, 5, 10\} except for the cases of $\AAAC$ and $\DQN$ for $\varphi_7$--$\varphi_9$ among we use \{5, 10\}~\footnote{These methods with $\Delta_{T}=1$ for $\varphi_7$--$\varphi_9$ shows bad performance and did not terminate in 5 days.}) and report the setting (of $\Delta_{T}$) which leads to the best performance (the least episode number and highest success rate) for each algorithm.
%We change $\Delta_{T}$ from 1, 5, 10 for $\varphi_1$--$\varphi_6$ and 5, 10 for $\varphi_7$--$\varphi_9$.
%$\Delta_{T}$ 1 for $\varphi_5$--$\varphi_7$ is omitted because of the performance reason.



\begin{table}[tp]
  \centering
  \begin{minipage}[t]{.48\textwidth}
    \centering
    \scriptsize
    \begin{tabular}{c||c}
      id & Formula\\
      \hline
      \hline
      $\varphi_1$ & $\square \omega \leq \overline{\omega}$\\
      $\varphi_2$ & $\square (v \leq \overline{v} \wedge \omega \leq \overline{\omega})$\\
      $\varphi_3$
      & $\square ((g_2 \wedge \diamond_{[0, 0.1]} g_1) \rightarrow \square_{[0.1, 1.0]} \neg g_2)$\\
      $\varphi_4$
      & $\square ((\neg g_1 \wedge \diamond_{[0, 0.1]} g_1) \rightarrow \square_{[0.1, 1.0]} g_1)$\\
      $\varphi_5$
      & $\square \bigwedge_{i=1}^4 ((\neg g_i \wedge \diamond_{[0, 0.1] g_i}) \rightarrow \square_{[0.1, 1.0]} g_i)$\\
    \end{tabular}
  \end{minipage}
  \begin{minipage}[t]{.48\textwidth}
    \centering
    \scriptsize
    \begin{tabular}{c||c}
      id & Formula\\
      \hline
      \hline
      $\varphi_6$
      & $\square (\square_{[0, t_1]} \omega \leq \overline\omega \rightarrow \square_{[t_1, t_2]} v \leq \overline{v})$\\
      $\varphi_7$
      & $\square v \leq \overline{v}$\\
      $\varphi_8$
      & $\square \diamond_{[0,25]} \neg (\underline{v} \leq v \leq  \overline{v})$\\
      $\varphi_9$
      & $\square \neg \square_{[0,20]} (\neg g_4 \wedge \omega \geq \overline{\omega})$\\
    \end{tabular}
  \end{minipage}
  \caption{The list of the evaluated properties on $\ATmodel$.}
  \label{tab:formulas}
  \vspace{-5mm}
\end{table}
\begin{table}[tp]
  \centering
    \centering
    \scriptsize
    \begin{tabular}[t]{c||c c c c|c c c c|c c c c|}
      id & \multicolumn{4}{|c|}{$\Delta_T$} & \multicolumn{4}{|c|}{Success rate} & \multicolumn{4}{|c|}{$\mathsf{numEpisode}$}\\
      \hline
      & $\AAAC$ & $\DQN$ & $\SA$ & $\CE$ & $\AAAC$ & $\DQN$ & $\SA$ & $\CE$ & $\AAAC$ & $\DQN$ & $\SA$ & $\CE$ \\
      \hline
      \hline
      $\varphi_1$ & 5 & 1 & 10 & 5 & $\textbf{100}\%^*$ & $\textbf{100}\%^*$ & 65.0\% & 10.0\% & $\textbf{16.5}^{**}$ & 24.5 & 118.5 & 200.0\\
      $\varphi_2$ & 5 & 1 & 10 & 5 & $\textbf{100}\%^*$ & $\textbf{100}\%^*$ & 65.0\% & 10.0\% & $\textbf{11.5}^{**}$ & 27.5 & 118.5 & 200.0\\
      $\varphi_3$ & 1 & 1 & 1 & 1 & 75.0 & 5.0\% & 20.0\% & \textbf{85.0}\% & 44.0 & 200.0 & 200.0 & \textbf{26.5}\\
      $\varphi_4$ & 1 & 1 & 1 & 1 & 75.0 & 10.0\% & 20.0\% & \textbf{85.0}\% & 67.5 & 200.0 & 200.0 & $\textbf{26.5}^{*}$\\
      $\varphi_5$ & 1 & 1 & 1 & 1 & \textbf{100}\% & \textbf{100}\% & \textbf{100}\% & \textbf{100}\% & \textbf{1.0} & 2.0 & \textbf{1.0} & \textbf{1.0} \\
      $\varphi_6$ & 10 & 10 & 10 & 10 & $\textbf{100}\%^*$ & $\textbf{100}\%^*$ & 70.0\% & 50.0\% & $\textbf{3.5}^{**}$ & $\textbf{3.5}^{**}$ & 160.5 & 119.0\\
      $\varphi_7$ & 5 & 5 & 1 & 1 & 65.0\% & $\textbf{100}\%^{**}$ & 0.0\% & 0.0\% & 125.0 & $\textbf{63.0}^{**}$ & 200.0 & 200.0\\
      $\varphi_8$ & 10 & 10 & 10 & 1 & 80.0\% & \textbf{95.0}\% & 90.0\% & 75.0\% & 72.0 & 52.0 & 83.0 & \textbf{21.0}\\
      $\varphi_9$ & 10 & 10 & 10 & 10 & 95.0\% & $\textbf{100}\%^{**}$ & 15.0\% & 5.0\% & 46.0 & $\textbf{12.0}^{**}$ & 200.0 & 200.0 \\
      \hline
  \end{tabular}
  \caption{The experimental result on $\ATmodel$.}
  \label{tab:ARCH2014}
   \vspace{-5mm}
\end{table}



%\subsection{Evaluation Results}\label{sec:result}
\vspace{4mm}
\noindent \textbf{Evaluation Results}
%\subsubsection{Evaluation Results}
The preliminary results are presented in Table.~\ref{tab:ARCH2014}.
%The algorithms indicated by bold face are our approach and others are baselines.
%%
%
%$\SA$ is simulated annealing and $\CE$ is cross entropy method.
%
%%
The $\Delta_{T}$ columns indicate the best performing $\Delta_{T}$ for each algorithm.
The ``Success rate'' columns indicate the success rate of falsification process.
The ``numEpisode'' columns show the median (among the 20 procedures) of the number of simulation episodes required to falsify the formula.
If the falsification procedure fails, we consider the number of simulation episodes to be the maximum allowed episodes (200).
We use median since the distribution of the number of simulation episodes tends to be skewed.
%Another reason is that our sample size is relatively small so that we need to avoid the effects of outliers.

The best results (success rate and numEpisode) of each formula are highlighted in bold.
If the difference between the best entry of our methods and the best entry of the baseline methods is statistically significant by Fisher's exact test and the Mann Whitney U-test~\cite{corder2014nonparametric}, we mark the best entry with $*$ ($p < 0.05$) or $**$ ($p < 0.001$), respectively.
%\todo{This is again a little misleading.  We do not compare, for example, A3C and DDQN.  Only between the best one among our method and the best one among baselines.}
%To test the success rate, we use Fisher's exact test of independence~\cite{corder2014nonparametric}.
%To test iteration numbers, we use the Mann-Whitney U-test~\cite{corder2014nonparametric}.

As shown in Table~\ref{tab:ARCH2014}, RL based methods almost always outperforms baseline methods on success rate, which means RL based methods are more likely to find the falsified inputs with a limited number of episodes.
This is because RL based methods learn knowledge from the environment and generate input signals adaptively during the simulations.
%On the other hand, the result on iteration numbers is more mixed.
Among the statistically significant results of numEpisode, our methods are best for five cases ($\varphi_1, \varphi_2,\varphi_6,\varphi_7,\varphi_9$), while the baseline methods are best for one case ($\varphi_4$).
%Although, for $\varphi_3$--$\varphi_5$ and $\varphi_8$ the cross entropy method is best in episode numbers, only in one case the result is statistically significant.
For the case of $\varphi_4$, it is likely because that
all variables in this formula take discrete values,
thus, reinforcement learning is less effective.
%the reward in reinforcement learning tends to be constant.
%\todo{Still A3C performs not so bad.  Also what is the reason of good performance of CE?}
Further, DDQN tends to return extreme values as actions,
which are not solutions to falsify $\varphi_3$ and $\varphi_4$.
This explains poor performance of DDQN for the case of $\varphi_3$ and $\varphi_4$.

%% \begin{table}
%% \centering
%% \scriptsize
%% \begin{tabular}{|c|c|c|c|c|c|}
%% \hline
%% Property id \& Formula & Algorithm & sampling rate & falsified? & iteration number\\
%% \hline
%% \hline
%% \multirow{4}{*}{$\varphi_1 \colon \square ((g_2 \wedge \diamond_{[0, 0.1]} g_1) \rightarrow \square_{[0.1, 1.0]} \neg g_2)$}
%%               & $\AAAC$ & 1 & \textbf{90.0\%} & 46.0 \\
%% 							& $\DQN$ & 1 & 5.0\% & 200.0 \\
%%               & $\SA$  & 1 & 20.0\% & 200.0 \\
%% 							& $\CE$  & 1 & 85.0\% & \textbf{26.5} \\

%% \hline
%% \multirow{4}{*}{$\varphi_2 \colon \square ((\neg g_1 \wedge \diamond_{[0, 0.1]} g_1) \rightarrow \square_{[0.1, 1.0]} g_1)$}
%%               & $\AAAC$ & 1 & \textbf{90.0\%} & 63.5 \\
%%               & $\DQN$ & 1 & 5.0\% & 200.0 \\
%% 							& $\SA$ & 1 & 20.0\% & 200.0 \\
%% 							& $\CE$  & 1 & 85.0\% & $\textbf{26.5}^*$ \\
%% \hline
%% \multirow{4}{*}{$\varphi_3 \colon \square \bigwedge_{i=1}^4 ((\neg g_i \wedge \diamond_{[0, 0.1] g_i}) \rightarrow \square_{[0.1, 1.0]} g_i)$}
%%               & $\AAAC$ & 1 & 75.0\% & 73.0 \\
%%               & $\DQN$ & 1 & 10.0\% & 200.0 \\
%% 							& $\SA$ & 1 & 20.0\% & 200.0 \\
%% 							& $\CE$ & 1 & \textbf{85.0\%} & \textbf{26.5} \\
%% \hline
%% \multirow{4}{*}{$\varphi_4 \colon \square (\square_{[0, t_1]} \omega \leq \overline\omega \rightarrow \square_{[t_1, t_2]} v \leq \overline{v})$}
%%               & $\AAAC$ & 10 & $\textbf{100\%}^*$ & $\textbf{2.5}^{**}$ \\
%%               & $\DQN$ & 5 & $\textbf{100\%}^*$ & 4.0 \\
%%               & $\SA$ & 10 & 70.0\% & 160.5  \\
%% 							& $\CE$ & 10 & 50.0\% & 119.0 \\
%% \hline
%% \multirow{4}{*}{$\varphi_5 \colon \square v \leq \overline{v}$}
%%               & $\AAAC$ & 5 & 80.0\% & $\textbf{45.0}^{**}$ \\
%%               & $\DQN$ & 5 & $\textbf{100}\%^{**}$ & 57.5 \\
%%               & $\SA$ & 10 & 0.0\% & 200.0 \\
%%               & $\CE$ & 10 & 0.0\% & 200.0 \\

%% \hline
%% \multirow{4}{*}{$\varphi_{6} \colon \square \diamond_{[0,25]} \neg (\underline{v} \leq v \leq  \overline{v})$}
%%               & $\AAAC$ & 10 & 75.0\% & 52.0 \\
%%               & $\DQN$ & 10 & \textbf{100}\% & 37.5 \\
%% 							& $\SA$ & 10 & 90.0\% & 83.0 \\
%% 							& $\CE$ & 10 & 65.0\% & \textbf{35.5} \\
%% \hline
%% \multirow{4}{*}{$\varphi_{7} \colon \square \neg \square_{[0,20]} (\mathrm{gearLow} \wedge \mathrm{highRPM})$}
%% 							& $\AAAC$ & 10 & 95.0\% & 49.5 \\
%%               & $\DQN$ & 10 & $\textbf{100\%}^{**}$ & $\textbf{12.0}^{**}$ \\
%% 							& $\SA$ & 10 & 15.0\% & 200.0 \\
%% 							& $\CE$ & 10 & 5.0\% & 200.0 \\
%% \hline
%% \end{tabular}
%% \caption{The experimental result on $\ATmodel$.}
%% \label{tab:ARCH2014}
%% \end{table}


%
% \begin{table}
%   \centering
%   \begin{minipage}[t]{.4\textwidth}
%     \centering
%     \scriptsize
%     \begin{tabular}[t]{c||c|c|c|c}
%       id & Algorithm & $\Delta_T$ & falsified? & $\mathsf{numEpisode}$\\
%       \hline
%       \hline
%       \multirow{4}{*}{$\varphi_1$}
%       & $\AAAC$ & 1 & \textbf{90.0\%} & 46.0 \\
%       & $\DQN$ & 1 & 5.0\% & 200.0 \\
%       & $\SA$  & 1 & 20.0\% & 200.0 \\
%       & $\CE$  & 1 & 85.0\% & \textbf{26.5} \\
%       \hline
%       \multirow{4}{*}{$\varphi_2$}
%       & $\AAAC$ & 1 & \textbf{90.0\%} & 63.5 \\
%       & $\DQN$ & 1 & 5.0\% & 200.0 \\
%       & $\SA$ & 1 & 20.0\% & 200.0 \\
%       & $\CE$  & 1 & 85.0\% & $\textbf{26.5}^*$ \\
%       \hline
%       \multirow{4}{*}{$\varphi_3$}
%       & $\AAAC$ & 1 & 75.0\% & 73.0 \\
%       & $\DQN$ & 1 & 10.0\% & 200.0 \\
%       & $\SA$ & 1 & 20.0\% & 200.0 \\
%       & $\CE$ & 1 & \textbf{85.0\%} & \textbf{26.5} \\
%       \hline
%       \multirow{4}{*}{$\varphi_4$}
%       & $\AAAC$ & 10 & $\textbf{100\%}^*$ & $\textbf{2.5}^{**}$ \\
%       & $\DQN$ & 5 & $\textbf{100\%}^*$ & 4.0 \\
%       & $\SA$ & 10 & 70.0\% & 160.5  \\
%       & $\CE$ & 10 & 50.0\% & 119.0 \\
%     \end{tabular}
%   \end{minipage}
%   \begin{minipage}[t]{.4\textwidth}
%     \centering
%     \scriptsize
%     \begin{tabular}[t]{c||c|c|c|c}
%       id & Algorithm & $\Delta_T$ & falsified? & $\mathsf{numEpisode}$\\
%       \hline
%       \hline
%       \multirow{4}{*}{$\varphi_5$}
%       & $\AAAC$ & 5 & 80.0\% & $\textbf{45.0}^{**}$ \\
%       & $\DQN$ & 5 & $\textbf{100}\%^{**}$ & 57.5 \\
%       & $\SA$ & 10 & 0.0\% & 200.0 \\
%       & $\CE$ & 10 & 0.0\% & 200.0 \\
%       \hline
%       \multirow{4}{*}{$\varphi_{6}$}
%       & $\AAAC$ & 10 & 75.0\% & 52.0 \\
%       & $\DQN$ & 10 & \textbf{100}\% & 37.5 \\
%       & $\SA$ & 10 & 90.0\% & 83.0 \\
%       & $\CE$ & 10 & 65.0\% & \textbf{35.5} \\
%       \hline
%       \multirow{4}{*}{$\varphi_{7}$}
%       & $\AAAC$ & 10 & 95.0\% & 49.5 \\
%       & $\DQN$ & 10 & $\textbf{100\%}^{**}$ & $\textbf{12.0}^{**}$ \\
%       & $\SA$ & 10 & 15.0\% & 200.0 \\
%       & $\CE$ & 10 & 5.0\% & 200.0 \\
%     \end{tabular}
%   \end{minipage}
%   \caption{The experimental result on $\ATmodel$.}
%   \label{tab:ARCH2014}
% \end{table}
