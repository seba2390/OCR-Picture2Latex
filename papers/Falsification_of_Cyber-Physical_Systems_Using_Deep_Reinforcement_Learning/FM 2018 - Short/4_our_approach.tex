%!TEX root = 0_main.tex
\section{Our Approach}\label{sec:overview}

\begin{algorithm}[tp]
\scriptsize
  \caption{Falsification for $\psi = \square \varphi$ by reinforcement learning}
  \label{algo:RLfalsification}
  \begin{algorithmic}[1]
    \INPUT A past-dependent life-long property $\psi = \square \varphi$, a system $\mathcal{M}$,
    an agent $\mathcal{A}$
    \OUTPUT A counterexample input signal $\mathbf{u}$ if exists
    \PARAMETERS A step time $\Delta_T$, the end time $T_{\mathsf{end}}$, the maximum number of the episode $N$
 %   \Ensure something
    \For{$\mathsf{numEpisode} \gets$ $1$ to $N$}
    \State $i \gets 0$, $r \gets 0$, $x$ be the initial (output) state of $\mathcal{M}$
    \State $\mathbf{u}$ be the empty input signal sequence
    \While{$i \Delta_T < T_{\mathsf{end}}$}
    \State $u \gets \mathcal{A}.\mathsf{step}(x, r)$,  $\mathbf{u} \gets \mathsf{append}(\mathbf{u}, (i \Delta_T, u))$
    \Comment choose the next input by the agent
    \State $\mathbf{x} \gets \mathcal{M}(\mathbf{u})$, $x \gets \mathbf{x}((i+1)\Delta_T)$
    \Comment simulate, observe the new output state
    \State $r \gets \reward(\mathbf{x}, \psi)$
    \State $i \gets i+1$
    \Comment calculate the reward by following eq.~(\ref{def:reward})
    \EndWhile
    \If{$\mathbf{x} \not\models \psi$}
    %% \Then
    \Return $\mathbf{u}$ as a falsifying input
    \EndIf
    \State $\mathcal{A}.\mathsf{reset}(x, r)$
    \EndFor
   \end{algorithmic}
\end{algorithm}
%
% In this section, we describe our method
% of enforcing the CPS model to falsify the given STL specification
% by reinforcement learning.

\subsection{Overview of our algorithm}\label{subsec:algorithm}
Let us consider the falsification problem to find a counterexample of the life-long property $\psi \equiv \square \varphi$.
If the output signal is infinitely long to past and future directions, $\psi$ is logically equivalent to a past-dependent life-long property $\square \boxminus_{[\hrz(\varphi), \hrz(\varphi)]} \varphi$.
In general, the output signal is not infinitely long to some direction but using this conversion we convert all life-long properties to past-dependent life-long properties.
Our evaluation in Section \ref{sec:exp} suggests that this approximation does not adversely affect the performance.
%
Therefore, assume $\psi$ is a past-dependent life-long property, we generate an input signal $\mathbf{u}$ for system $\mathcal{M}$,
such that the corresponding output signal $\mathcal{M}(\mathbf{u})$ does not satisfy $\psi$.

In our algorithm,
we fix the simulation time to be $T_{\mathsf{end}}$
and
call one simulation until time $T_\mathsf{end}$
an \emph{episode} in conformance with the reinforcement learning terminology.
We fix the discretization of time to a positive real number $\Delta_T$.
%For an agent $\mathcal{A}$,
%in each episode,
%it generates an input signal $\mathbf{u}(t)$
%%adaptively
%based on the observed current system output and the reward.
%More precisely,
The agent $\mathcal{A}$ generates the piecewise-constant input signal
$\mathbf{u} = \big[(0, u_0), (\Delta_T, u_{1}), (2\Delta_T, u_{2}), \dots \big]$
by iterating the following steps:

%\begin{enumerate}
%\item
% At time $i \Delta_T$ ($i=0,1,\dots$),
%  the agent $\mathcal{A}$ choose the next input value $u_i$.
%  The generated input signal is extended to
%  $\mathbf{u} = \big[(0, u_0), \dots, (i\Delta_T, u_i) \big]$. \\
%\item
%  Our algorithm obtains the corresponding output signal $\mathbf{x} = \mathcal{M}(\mathbf{u})$
%  by stepping forward one simulation on the model $\mathcal{M}$
%  from time $i \Delta_T$ to $(i+1) \Delta_T$ with input $u_{i}$. \\
%\item
% Let $x_{i+1} = \mathbf{x}((i+1)\Delta_T)$ be the new (observed) state (i.e., output) of the system. \\
%\item
% We compute reward $r_{i+1}$ by $\reward(\varphi, \mathbf{x}, (i+1)\Delta_T)$ (defined in Section \ref{subsec:reward}). \\
%\item
% $\mathcal{A}$ updates its action based on the new state $x_{i+1}$ and the reward $r_{i+1}$.
%\end{enumerate}

(1) At time $i \Delta_T$ ($i=0,1,\dots$),
  the agent $\mathcal{A}$ chooses the next input value $u_i$.
  The generated input signal is extended to
  $\mathbf{u} = \big[(0, u_0), \dots, (i\Delta_T, u_i) \big]$. \\
%\item
\indent (2) Our algorithm obtains the corresponding output signal $\mathbf{x} = \mathcal{M}(\mathbf{u})$
  by stepping forward one simulation on the model $\mathcal{M}$
  from time $i \Delta_T$ to $(i+1) \Delta_T$ with input $u_{i}$. \\
%\item
\indent (3) Let $x_{i+1} = \mathbf{x}((i+1)\Delta_T)$ be the new (observed) state (i.e., output) of the system. \\
%\item
\indent (4) We compute reward $r_{i+1}$ by $\reward(\varphi, \mathbf{x}, (i+1)\Delta_T)$ (defined in Section \ref{subsec:reward}). \\
%\item
\indent (5) The agent $\mathcal{A}$ updates its action based on the new state $x_{i+1}$ and reward $r_{i+1}$.

At the end of each episode,
we obtain the output signal trajectory $\mathbf{x}$,
and check whether it satisfies the property $\psi = \square \varphi$ or not.
If it is falsified, return the current input signal $\mathbf{u}$ as a counterexample.
Otherwise, we discard the current generated signal input
and restart the episode from the beginning.

The complete algorithm of our approach is shown in Algorithm~\ref{algo:RLfalsification}.
The method call $\mathcal{A}.\mathsf{step}(x, r)$ represents
the agent $\mathcal{A}$ push the current state reward pair ($x$, $r$) into its memory
and returns the next action $u$ (the input signal in the next step).
The method call $\mathcal{A}.\mathsf{reset}(x, r)$ notifies the agent that the current episode is completed, and returns the current state and reward.
%\ttodo{This sentence seems misleadning as is discussed in our skype chat.}
Function $\reward(\mathbf{x}, \psi)$ calculates the reward based on Definition~\ref{def:reward}.



\subsection{Reward definition for life-long property falsification}\label{subsec:reward}
Our goal is to find the input signal $\mathbf{u}$ to the
system $\mathcal{M}$ which minimizes $\rob(\psi, \mathcal{M}(\mathbf{u}), 0)$ where $\psi = \square \varphi$ and $\rho$ is a robustness.
We determine $u_0, u_1, \ldots$ in a greedy way.
Assume that $u_0, \ldots, u_i$ are determined.
$u_{i+1}$ can be determined by
\begin{align}
\scriptstyle
\label{eq:action}
  u_{i+1} &= \argmin_{u_{i+1}} \min_{u_{i+2}, \ldots} \rob(\square \varphi, \mathcal{M}(\left[(0, u_0), (\Delta_T, u_1), \ldots \right]), 0) \\
  &\sim \argmax_{u_{i+1}} \max_{u_{i+2}, \ldots} \sum_{k=i+1}^\infty \{ e^{- \rob(\varphi, \mathcal{M}(\left[(0, u_0), \ldots, (k\Delta_T, u_k) \right]), k\Delta_T)} - 1\} \label{eq:r}
\end{align}
The detailed derivation steps can be found in Appendix~\ref{sec:appendix}.
%\eqref{eq:disc} uses the fact $\varphi$ is past-dependent and \eqref{eq:logsum} uses an approximation of minimum by the log-sum-exp function~\cite{cook2011basic}.

In our reinforcement learning base approach, we use discounting factor $\gamma=1$ and reward $r_i = e^{- \rob(\varphi, \mathcal{M}(\left[(0, u_0), \ldots, (i\Delta_T, u_i)\right]), i\Delta_T)} - 1$  to approximately compute action $u_{i+1}$, from $u_0, \ldots, u_i$, $\mathcal{M}(\left[(0, u_0), \ldots, (i\Delta_T, u_i) \right])$ and $r_1, \ldots, r_i$.
%\ttodo{No guarantee that u is approximately computed.
%  We cannot estimate the approximation error.
%  I prefer we claim
%  ``we use the discounting factor and reward to hopefully compute the approximation of the next action u...''
%}
%If the reward $r_i = e^{- \rob(\varphi, \mathcal{M}(\left[(0, u_0), \ldots, (i\Delta_T, u_i)\right]), i\Delta_T)} - 1$ and discounting factor $\gamma=1$ are used, we expect a reinforcement learning algorithm
%approximately computes $u_{i+1}$ as an action from $u_0, \ldots, u_i$, $\mathcal{M}(\left[(0, u_0), \ldots, (i\Delta_T, u_i) \right])$ and $r_1, \ldots, r_i$.

\begin{definition}[reward]\label{def:reward}
  Let $\psi \equiv \square \varphi$ be a past-dependent formula
  and $\mathbf{x} = \mathcal{M}(\mathbf{u})$ be a finite length signal until the time $t$.
  We define the reward $\reward(\psi, \mathbf{x})$ as
  \begin{equation}\label{eq:reward}
    \reward(\psi, \mathbf{x}) =
      \exp(- \rob(\varphi, \mathbf{x}, t)) - 1
  \end{equation}
\end{definition}
