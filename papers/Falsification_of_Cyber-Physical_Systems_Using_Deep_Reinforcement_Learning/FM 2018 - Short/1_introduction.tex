%!TEX root = 0_main.tex
\section{Introduction}\label{sec:introduction}

\emph{Cyber-Physical Systems} (CPS) are more and more widely adopted in safety-critical domains, which makes it extremely important to guarantee the correctness of CPS systems.
% In model-based development, testing and verification are conducted on models before starting implementations, to detect defects early and avoid unnecessary costs.
Testing and verification on \emph{models} of CPS are common methods to guarantee the correctness.
However, it is hard for testing to achieve a high coverage; verification techniques are usually expensive and  undecidable~\cite{DBLP:conf/allerton/AbbasF12} due to the infinite state space
%(complexities involved in the software system, the physical systems as well as interactions between physical and software components)
of CPS models.
Therefore, robustness guided falsification~\cite{TLF_CPS_2013,DBLP:conf/tacas/AnnpureddyLFS11} method is introduced to detect defects efficiently.
In robustness guided falsification, \emph{Signal Temporal Logic} (STL)~\cite{bartocci2018specification} formulas are usually used to specify properties which must be satisfied by a CPS model.
Robustness of an STL formula, which is a numeric measure of how ``robust'' a property holds in the given CPS model, is defined.
The state space of the CPS model is explored and a trajectory which minimizes the robustness value is identified as a good candidate for testing.
%(which potentially leads to defects).
In this way, robustness guided falsification aids to generate defect-leading inputs (counterexamples), which enables more efficient, yet automatic detection of defects.
Although non-termination of robustness guided falsification does not mean the absence of counterexamples, it suggests the correctness of the CPS model to some extent.

Existing approaches adopt various kinds of stochastic global optimization algorithms e.g., simulated annealing~\cite{DBLP:conf/allerton/AbbasF12} and cross-entropy~\cite{DBLP:conf/hybrid/SankaranarayananF12}, to minimize robustness.
These methods take a full trajectory (a sequence of actions) as input, and adjusting input during the simulation is not supported. As a result, a large number of simulation runs are required in the falsification process.
Existing methods cannot guarantee finding a counterexample of practical CPS models in a limited time window because the simulation would then be tremendous.
% A smarter, knowledge guided method is required to accommodate faster feedback and accelerate the process of finding the counterexamples.

In this paper, we adopt \emph{deep reinforcement learning}   (DRL)~\cite{mnih2015human} algorithms to solve the problem of falsification of STL properties for CPS models.
Reinforcement learning techniques can observe feedbacks from the environment, and adjust the input action immediately. In this way, we are able to converge faster towards minimum robustness value.
%\ttodo{It becomse easy to read if we state
%  ``what is the environment, input action, and episode in our RL setting''.}
In particular, we adopt two state-of-the-art DRL techniques, i.e., \emph{Asynchronous Advanced Actor Critic} (A3C) and Double \emph{Deep-Q Network} (DDQN).
Our contributions are two folds:
(1) we show how to transform the problem of falsifying CPS models into a reinforcement learning problem; and
(2) we implement our method and conduct preliminary evaluations to show DRL technology can help reduce the number of simulation runs required to find a falsifying input for CPS models.
Reducing the number of simulation runs is important because during falsification, the majority of execution time is spent for simulation runs if CPS models are complex.

\subsubsection{Related Work}
There are two kinds of works, i.e., robustness guided falsification and controller synthesis, which are most related to our approach.

In robustness guided falsification methods, quantitative semantics
%called \emph{robust semantics}~\cite{DBLP:conf/fates/FainekosP06,DBLP:journals/tcs/FainekosP09}
over \emph{Metric Interval Temporal Logic} (MITL) and its variants STL~\cite{DBLP:conf/formats/MalerN04,DBLP:conf/formats/DonzeM10} are employed.  %Signal Temporal Logic (STL)
%Intuitively, these quantitative truth values stand for ``how robustly the formula is satisfied.''
Then the fault detection problem is translated into the numerical minimization problem.
%in 2 steps:
%(1) The parameterization scheme from the input space of the system is mapped into the finite dimensional Euclidian space $\mathbb{R}^m$. Typically, a continuous-time input signal $\sigma_\theta$ is represented by a finite sequence of its values $\theta \in \mathbb{R}^m$.
%(2) Pick a parameter $\theta \in \mathbb{R}^m$, simulate the system on the input signal $\sigma_\theta$, and measure the robustness of $\varphi$. Iterate the process and the next input parameter $\theta$ is chosen with the expectation of getting a smaller robustness value.
Several tools e.g., S-TaLiRo~\cite{DBLP:conf/tacas/AnnpureddyLFS11,DBLP:conf/cpsweek/HoxhaAF14a} and Breach~\cite{DBLP:conf/cav/Donze10} are developed to realize this approach.
%the robustness guided falsification approaches.
Moreover, various kind of numerical optimization techniques, e.g., simulated annealing~\cite{DBLP:conf/allerton/AbbasF12},
cross-entropy~\cite{DBLP:conf/hybrid/SankaranarayananF12}, and
Gaussian process optimization~\cite{DBLP:journals/corr/BartocciBNS13,DBLP:journals/tcs/BartocciBNS15,DBLP:conf/rv/Akazaki16,DBLP:conf/ifm/SilvettiPB17}, are studied
to solve the falsification problem efficiently.
% One benefit of robustness guided falsification approach is that the system is treated as a blackbox, which makes it applicable even if details of the system is complex and not well understood, which is common for CPS.
All these methods optimize the whole output trajectory of a CPS by changing the whole input trajectory.
As stated above, we use reinforcement learning which can observe feedbacks from a CPS and adjust the input immediately.
Thus, our method can be expected to arrive the falsifying input faster.

%% ========================================================================
%The other kind of approach is to apply the controller synthesis techniques.
In contrast to robustness guided falsification,
controller synthesis techniques enable choosing the input signal at a certain step based on observations of output signals.
There are works that synthesize the controller to enforce the \emph{Markov decision process}
to satisfy a given LTL formula~\cite{DBLP:conf/cdc/SadighKCSS14,DBLP:conf/wafr/LunaLMK14,DBLP:journals/tac/DingSBR14,DBLP:conf/hybrid/SoudjaniM17,DBLP:conf/cdc/DingSBR11}.
%% The important assumption for controller synthesis techniques is that
%% the target system satisfies Markov property.
The most closest related works~\cite{DBLP:conf/iros/LiVB17,DBLP:journals/corr/abs-1709-09611}
apply reinforcement learning techniques to enforce the small robotic system to satisfy the given LTL formula.
%TODO DRL and non-Markovian system are not related.  "Standard" theory of RL assumes the system Markovian and DRL is no different.  Rather, the difference of these works and our works are: our works falsify the properties while the control synthesis keep the properties (the difference is intent and application, rather than mathematical content.)  Another difference is we use DRL, which enables us to use complex non-linear functions for a Q-function or a policy function.
%The difference between these work and ours are as follows:
Our work is different from those works in two aspects: (1) we falsify the properties while the control synthesis methods try to satisfy the properties; and
(2) with DRL, we could employ complex non-linear functions to learn and model the environment, which is suitable to analyze the complex dynamics of CPS.
%% ######
%2) we give some experimental results on the CPS models. To conduct the experiment, we also propose some heuristics
%to apply the reinforcement learning techniques to the continuous-time systems.
%% %The other kind of approach is to apply the controller synthesis techniques.
%% In contrast to robustness guided falsification,
%% controller synthesis techniques enable choosing the input signal at a certain step based on observations of output signals.
%% There are several works to enforce the system to satisfy the given LTL formula~\cite{DBLP:conf/cdc/SadighKCSS14,DBLP:conf/wafr/LunaLMK14,DBLP:journals/tac/DingSBR14,DBLP:conf/hybrid/SoudjaniM17,DBLP:conf/cdc/DingSBR11}.
%% The important assumption for controller synthesis techniques is that
%% the target system satisfies Markov property.\todo{is this relevant?}
%% %The application of various kinds of learning algorithms, such as Q-learning~\cite{DBLP:conf/cdc/AksarayJKSB16} and actor-critic methods~\cite{DBLP:journals/ijrr/WangDLPB15}, are studied.
%% The most closest related work~\cite{DBLP:conf/iros/LiVB17,DBLP:journals/corr/abs-1709-09611} try to apply reinforcement learning techniques to enforce the system to satisfy the given LTL formula.
%% %TODO DRL and non-Markovian system are not related.  "Standard" theory of RL assumes the system Markovian and DRL is no different.  Rather, the difference of these works and our works are: our works falsify the properties while the control synthesis keep the properties (the difference is intent and application, rather than mathematical content.)  Another difference is we use DRL, which enables us to use complex non-linear functions for a Q-function or a policy function.
%% The difference between these work and ours are as follows:
%% 1) Our method employs deep reinforcement learning techniques, and is applicable to the non-Markovian systems, which is a typical assumption in the industrial CPSs.\todo{DRL also usually assumes Markovian systems.  Using deep learning is not related to Markovianess so I propose to delete this sentence.} DRL enables us to use complex non-linear functions for a Q-function or a policy function.\todo{because it can learn complex non-linear functions by deep learning}
%% 2) our work tries to falsify the properties while the control synthesis methods try to satisfy the properties.
%% %2) we give some experimental results on the CPS models. To conduct the experiment, we also propose some heuristics
%% %to apply the reinforcement learning techniques to the continuous-time systems.
%%============================================================
