\section{Preliminary}\label{sec:preliminary}

$\mathbf R$ is the set of real numbers.

\subsection{Problem definition}

(... Motivating example ...)

\subsection{Robustness of metric temporal logic formulas (MTLs)}

(... Definition of MTL...)

(... Definition of robustness of MTL...)

\subsection{Runtime estimation of robustness}

\subsection{Reinforcement learning}

Reinforcement learning is one of machine learning techniques in which a learning agent simultaneously learns the structure of the environment based on observations, and maximizes the rewards by acting according to the learnt knowledge.
Fig. \ref{fig:RL} shows the setting of reinforcement learning.

\begin{figure}
  \centering
  \small
  \includegraphics[width=0.8\textwidth]{fig/RL.pdf}
  \caption{Reinforcement learning setting}
  \label{fig:RL}
\end{figure}

Reinforcement learning is often formulated as a \emph{Markov decision process (MDP)}~\cite{Szepesvari2010}.
A MDP is a triple $\mathcal M = (\mathcal X, \mathcal A, \mathcal P_0)$.
\state is a set of states, \action is a set of actions and \probkernal is the \emph{transition probability kernel}.
\textcolor{red}{LS: need to explain $\mathbf R$ before referring to it.}
A transition probability kernel \probkernal assigns a probability distribution (over $\mathcal X \times \mathbf R$, which is a distribution over the next states and the \emph{reward} when the agent takes an action $a$ at the state $x$.) to each state-action pair $(x, a) \in \mathcal X \times \mathcal A$. 
\DIFdelbegin \DIFdel{Informally put}\DIFdelend \DIFaddbegin \todo[inline]{A illustration of sequantial behavior of MDP here.}
\DIFadd{Informally}\DIFaddend , the goal of a MDP is for each step $n$, generating an actions $a_n$ knowing the sequence of previous states $x_0, \ldots, x_{n_1}$, rewards $r_1, \ldots, r_{n}$ and actions $a_0, \ldots, a_{n-1}$ which maximizes expected value of the \textcolor{red}{discounted $->$ weighted?} sum of rewards:
\begin{equation}\DIFdelbegin \DIFdel{
  r }\DIFdelend \DIFaddbegin \DIFadd{
  R }\DIFaddend = \sum_{t = 0}^\infty \gamma^t r_{t+1}
\end{equation}
where $0 < \gamma \leq 1$ is a discount rate.
\DIFaddbegin 

\DIFaddend For each state $x \in \mathcal X$, let $V^*(x)$ be the highest achievable expected value of $r$ above when $x_0 = x$.
\DIFaddbegin \todo{Question: what ``the highest'' means here? $V^*(x) = \mathsf{argmax}_{a_0, \dots a_n} \mathbf{E}[R] ?$}

%DIF >  One of the most common algorithm is \emph{$Q$-learning} \cite{}.
\DIFaddend For each action-state pair $(x, a)$, let \emph{optimal action-value function} $Q^*(x, a)$ be the highest achievable expected value of $r$ when $x_0 = x$ and $a_0 = a$.
Once we know $Q^*$, choosing $a$ which maximizes $Q^*(x, a)$ for the current state $x$ (following the greedy policy) is an optimal strategy.
One approach of reinforcement learning is to directly estimate $Q^*$ and use this estimated value to determine actions.
This approach is called \emph{$Q$-learning} \DIFdelbegin \DIFdel{.
}\DIFdelend \DIFaddbegin \DIFadd{\mbox{%DIFAUXCMD
\cite{}
}%DIFAUXCMD
.
}\todo{You should tell the name of the algorithm first for the readers who know it.}
\DIFaddend 

Other approaches are called \emph{actor-critic} methods.
A \emph{stochastic stationary policy} (or just \emph{policy}) $\pi$ maps states in \state to probability distributions over actions in \action.
The set of all policies is denoted by $\Pi_{\mathrm{stat}}$.
Each policy $\pi$ gives rise to a \emph{Markov reward process (MRP)} $\mathcal M = (\mathcal X, \mathcal P_0)$.
In a MRP, the state makes transitions as a Markovian process and generates a sequence of rewards $r_1, r_2, \ldots$.
The action-value function $Q^\pi$ is defined by
\begin{equation}
  Q^\pi(x, a) = \mathbf E \left[ \sum_{t = 0}^\infty \gamma^t R_{t+1} \middle| x_0 = x, a_0 = a \right]
\end{equation}
where $\mathbf E$ signifies the expect value.
An actor-critic method works as follows.
First, it starts with a random policy $\pi_0$ and the ``actor'' follows $\pi_0$ some duration of time.
Then, the the ``critic'' estimates $Q^{\pi_0}$ by the results of the run.
In the next phase, the a greedy policy $\pi_1$ determined by estimated $Q^{\pi_0}$ is generated and the actor follows $\pi_1$.
The actor-critic method repeats this process.

\emph{Deep reinforcement learning} is a reinforcement learning technique which uses a \emph{deep neural network} to represent a $Q$-function and/or a policy $\pi$.
The first successful deep reinforcement learning algorithm is \emph{Deep Q Network (DQN)}~\cite{mnih2015human}, which equals or outperforms human-beings on Atari 2600 video games.
The performance of DQN is also surpassed by successors~\cite{Mnih2016,Wang2016}.
This paper evaluates two algorithms, \emph{Asynchronous Advantage Actor-Critic (A3C)}~\cite{Mnih2016} and \emph{Actor-Critic with Experience Replay (ACER)}~\cite{Wang2016} for the purpose of falsification of CPSs.
\\

%added by LS
There are different kinds of reinforcement learning algorithms proposed in the literature. 
In this work, we particularly adopted ... different algorithms to solve our problem. 
We briefly review these methods in the following.
%
\subsubsection{A3C: Asynchronous Advantage Actor-Critic}

\subsubsection{ACER: Actor-Critic with Experience Replay}
