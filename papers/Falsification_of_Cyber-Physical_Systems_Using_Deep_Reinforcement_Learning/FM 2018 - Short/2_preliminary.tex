%!TEX root = 0_main.tex
\section{Preliminary}\label{sec:preliminary}
%We briefly introduce the preliminary concepts used in our work.

%\vspace{4mm}
%\subsection{Metric Temporal Logic}
%\subsubsection{Robustness guided falsification}
\subsubsection{{Robustness guided falsification}}
%
In this paper, we employ a variant of \emph{Signal Temporal Logic ($\STL$)} defined in~\cite{bartocci2018specification}. The syntax is defined in the equation (\ref{eq:MTL}),
\begin{equation}\label{eq:MTL}
  \varphi ::= v \sim c \mid p \mid \neg \varphi \mid
  \varphi_1 \vee \varphi_2
  \mid \varphi_1 \UNTIL_I \varphi_2
  \mid \varphi_1 \SINCE_I \varphi_2
\end{equation}
where $v$ is \emph{real} variable,
$c$ is a rational number,
$p$ is atomic formula,
$\sim \in \{<, \leq\}$
and $I$ is an interval over non-negative real numbers.
%\ttodo{Propositional variables are omitted.
%  I think we need them to treat True in the definition of box operator,
%  and gears in our experiment.}
If $I$ is $[0, \infty]$, $I$ is omitted.
We also use other common abbreviations,
e.g., $\square_I \varphi \equiv \mathsf{True} \UNTIL_I \varphi$ and
$\boxminus_I \varphi \equiv \mathsf{True} \SINCE_I \varphi$.
For a given formula $\varphi$, an output signal $\mathbf{x}$ and time $t$, we adopt the notation of work~\cite{bartocci2018specification} and denote the \emph{robustness degree} of output signal $\mathbf{x}$ satisfying $\varphi$ at time $t$ by $\rob(\varphi, \mathbf{x}, t)$.
%Note that,
%intuitively,
%the robustness degree $\rob(\varphi, x, t)$
%stands for
%how ``robust'' the signal $x$ satisfies the formula $\varphi$ at time $t$.

%% Robustness~\cite{TLF_CPS_2013} of a MTL formula $\phi$ for a output trace $y$ at the time instant $t_n$ is defined as
%% \todo{Does n means $t_n$ in function rob()?}
%% $\rob(\mathbf{y}, n, \phi)$ , and is a measure of how
%% ``robust'' $\phi$ holds.
%% For atomic formula $p$, the robustness $\rob(\mathbf{y}, n, p)$ is defined as the infimum of the distance of the point $y$ which does not satisfies $p$ from $y_n$.
%% \todo{$y_n$ means $y_t$? or $y_tn$? We either use t or n, be consistent}
%% %The distance between two states $\dist(y, y_n)$ can be any metric, but
%% \textcolor{red}{
%% In this paper we use Euclidian metric to define the distance between two states $\dist(y, y_n)$.
%% For example, if $y_n = 0$ and $p = \{ y \mid y < 1 \}$, $\rob(\mathbf{y}, n, p) = \dist(y_n, y) = 1$.}
%% %

We also adopt the notion of \emph{future-reach} $\hrz(\varphi)$ and
\emph{past-reach} $\hst(\varphi)$ following~\cite{DBLP:conf/rv/HoOW14}.
%% The \emph{horizon} $\hrz(\phi)$ of a MTL formula $\phi$ is the time in future which is required to determine the truth value of the formula $\phi$.
%Generally speaking, for a given formula $\varphi$,
Intuitively, $\hrz(\varphi)$ is the time in future which is required to determine the truth value of formula $\varphi$, and $\hst(\varphi)$ is the time in past.
For example,
$\hrz(p) = 0$, $\hrz(\square_{[0, 3]}p) = 3$ and $\hrz(\boxminus_{[0,3]}p) = 0$.
%%Similarly we define the \emph{history} $\hst(\varphi)$.
Similarly, for past-reach,
$\hst(p) = 0$, $\hst(\square_{[0, 3]}p) = 0$, $\hst(\boxminus_{[0,3]}p) = 3$.

%% \begin{lemma}[Robustness of a past dependent formula]\label{lem:rob-past}
%%   Let $\mathbf{y}$ be a finite trace of system states $y_0, \ldots, y_n$.
%%   Let $\overline{\mathbf{y}}_1$ and $\overline{\mathbf{y}}_2$ be two infinite extensions of $\mathbf{y}$.
%%   If $\phi$ is past dependent,
%%   \begin{equation}
%%     \rob(\overline{\mathbf{y}}_1, t, \phi) = \rob(\overline{\mathbf{y}}_2, t, \phi)
%%   \end{equation}
%% \end{lemma}

%% By Lemma \ref{lem:rob-past}, robustness of a past dependent formula at instant $n$ is completely determined by $y_0, \ldots, y_n$.
%% Therefore, we use the notation $\rob(\mathbf{y}, t, \phi)$ for robustness of a past-dependent formula $\phi$ on a finite trace $\mathbf{y}$.

%\subsection{Past-dependent life-long property falsification}
In this paper, we focus on a specific class of the temporal logic formula called \emph{life-long property}.
%to employ our approach.

\begin{definition}[life-long property]
  A \emph{life-long property} is an $\STL$ formula $\psi \equiv \square \varphi$ where $\hrz(\varphi),
  \hst(\varphi)$ are finite.
  If $\hrz(\varphi) = 0$, we call $\psi$ \emph{past-dependent life-long property}.

\end{definition}

%% %TODO life-long property, not past-dependent
%% \begin{definition}[Past-dependent life-long property]
%%   A \emph{life-long property} is an $\mathsf{MTL}$ formula $\psi \equiv \square \varphi$ where $\varphi$ only has a finite horizon.
%%   In particular, if $\varphi$ only contains bounded modal operators, $\psi$ is a life-long property.
%%   If $\varphi$ is past-dependent, then $\square \varphi$ is called \emph{past-dependent life-long property}.
%% \end{definition}


%\vspace{4mm}
\subsubsection{Reinforcement Learning}
%\subsection{Reinforcement learning}
%\subsubsection{Reinforcement Learning}
Reinforcement learning is one of machine learning techniques in which an agent learns the structure of the environment based on observations, and maximizes the rewards by acting according to the learnt knowledge.
% Reinforcement learning is first proposed and used in the domain of audio and image processing to improve the analysis performance. Reinforcement learning has shown its power and potential in training AlphaGo Zero~\cite{AlphaGo0}, which became the world's best Go player in 40 days, from scratch.
%In this work, we are using reinforcement learning techniques to reduce the accelerate the process of finding the counterexample, which falsifies the robustness property defined for a CPS.
%In particular, we adopt Asynchronous Advantage Actor-Critic (A3C) and Double Deep Q Network (DDQN) in our problem.
%
%Fig. \ref{fig:RL} shows the standard setting of reinforcement learning.
%%
%
%\begin{figure}
%  \centering
%  \scriptsize
%  \includegraphics[scale=0.67]{fig/RL.pdf}
%  \caption{Reinforcement learning setting}
%  \label{fig:RL}
%  \vspace{-7mm}
%\end{figure}
%
%%
The standard setting of a reinforcement learning problem consists of an agent and an environment. %, as shown in Fig.~\ref{fig:arch}.
The agent observes the current state and reward from the environment, and returns the next action to the environment.
%
%%
%%%
% Reinforcement learning is often formulated as a \emph{Markov decision process (MDP)}~\cite{Szepesvari2010}.
% A MDP is a triple $\mathcal M = (\mathcal X, \mathcal A, \mathcal P_0)$.
% \state is a set of states, \action is a set of actions and \probkernal is the transition probability kernel.
% A transition probability kernel \probkernal assigns a probability distribution (over $\mathcal X \times \mathbb R$, which is a distribution over the next states and the reward when the agent takes an action $a$ at the state $x$.), to each state-action pair $(x, a) \in \mathcal X \times \mathcal A$.
The goal of reinforcement learning is for each step $n$, given the sequence of previous states $x_0, \ldots, x_{n-1}$, rewards $r_1, \ldots, r_{n}$ and actions $a_0, \ldots, a_{n-1}$, generate an action $a_n$, which maximizes expected value of the sum of rewards:
%\begin{equation}
 $ r = \sum_{k = n}^\infty \gamma^k r_{k+1}$
%\end{equation}
, where $0 < \gamma \leq 1$ is a discount factor.
%
% For each state $x \in \mathcal X$, $V^*(x)$ is used to denote the highest achievable expected value of reward $r$, when $x_0 = x$.
%There are different kinds of reinforcement learning algorithms proposed in the literature. These approaches mainly falls into 2 different categories, i.e., value based and policy based, categorized by types of agents.
%
% \emph{$Q$-learning} \cite{} is the representative method for value-based reinforcement learning algorithm.
% For each action-state pair $(x, a)$, let \emph{optimal action-value function} $Q^*(x, a)$ be the highest achievable expected value of $r$ when $x_0 = x$ and $a_0 = a$.
% Once the value of $Q^*$ is known, the optimal strategy is to choose action $a$ which maximizes $Q^*(x, a)$ for the current state $x$ (following the greedy policy).
% One approach of reinforcement learning is to directly estimate $Q^*$ and use this estimated value to determine best actions.
% This approach is called \emph{$Q$-learning} \cite{}.
%
% \emph{actor-critic} method is the representative method for the other kind of approaches, policy-based algorithms.
% A \emph{stochastic stationary policy} (or just \emph{policy}) $\pi$ maps states in \state to probability distributions over actions in \action.
% The set of all policies is denoted by $\Pi_{\mathrm{stat}}$.
% Each policy $\pi$ gives rise to a \emph{Markov reward process (MRP)} $\mathcal M = (\mathcal X, \mathcal P_0)$.
% In a MRP, the state makes transitions as a Markovian process and generates a sequence of rewards $r_1, r_2, \ldots$.
% The action-value function $Q^\pi$ is defined by
% \begin{equation}
%   Q^\pi(x, a) = \mathbf E \left[ \sum_{t = 0}^\infty \gamma^t R_{t+1} \middle| x_0 = x, a_0 = a \right]
% \end{equation}
% where $\mathbf E$ signifies the expect value.
% An actor-critic method works as follows.
% First, it starts with a random policy $\pi_0$ and the ``actor'' follows $\pi_0$ some duration of time.
% Then, the the ``critic'' estimates $Q^{\pi_0}$ by the results of the run.
% In the next phase, the a greedy policy $\pi_1$ determined by estimated $Q^{\pi_0}$ is generated and the actor follows $\pi_1$.
% The actor-critic method repeats this process.
%
%%
%%%
Deep reinforcement learning is a reinforcement learning technique which uses a \emph{deep neural network} for learning.  % to represent a $Q$-function and/or a policy $\pi$.
In this work, we particularly adopted 2 state-of-the-art deep reinforcement learning algorithms, i.e., \emph{Asynchronous Advantage Actor-Critic} (A3C)~\cite{Mnih2016} and \emph{Double Deep Q Network} (DDQN)~\cite{pmlr-v48-gu16}.
% We briefly review these methods in the following.
% %
%
% \noindent \textbf{A3C: Asynchronous Advantage Actor-Critic}
% Asynchronous Advantage Actor-Critic (A3C)~\cite{Mnih2016} utilizes multiple processes to accelerate the training process. All processes run the same training algorithm and the information is collected by a central process. In this way, the algorithm and train models much faster.
%
% \noindent \textbf{Double Deep Q Network}
% DQN~\cite{mnih2013playing} combines CNN and Q-learning. A CNN network is used to analyze the Q-value. DQN can is proposed to solve large problems, which is hard to tackle with the normal table-based Q-learning algorithms.
%
%The output of the DQN is the best action to take in the current state and it's corresponding q-value. Moreover, to avoid the problem that may be caused by over estimation, we adopt double DQN  (DDQN) which uses Current Q-network to select actions and older Q-network to evaluate actions. Current Q-network use max to find the best action, overestimation may happen here. But another Q-network which doesn't use max evaluates q-value of the selected action. It may high or low. So over estimation is solved.

% Double DQN is an improvement of DQN, it has two networks to conduct action selection and Q-value evaluation separately. Double DQN learns faster and can avoid the over-estimation problem of DQN.
%The loss function is
%\begin{equation}
% L(w) =  \mathbb{E}\left[(r+\gamma\mathop {\max }\limits_{a'} Q(s',a',w) - Q(s,a,w))^2\right]
%\end{equation}
%
%DQN is short for Deep Q network\cite{mnih2013playing}. Q-learning is a Reinforcement Learning algorithm which for finite states and actions. Practical problems may have too many states therefore Q-learning has some limits. So q-values based on neural network comes true.
%
%\begin{algorithm}
%  \caption{Deep Q-learning with Experience Replay}
%  \begin{algorithmic}
%  \State Initialize replay memory D to capacity N % \State
%  \State Initialize action-value function Q with random weights
%    \For{episode = 1;M}
%      \State Initialise sequence $s_1 = \left\{ x_1 \right\}$ and preprocessed sequenced $\phi_1 = \phi(s_1) $
%    \EndFor

%  \end{algorithmic}
%\end{algorithm}
%
