\section{Mean Estimation}
\label{sec:mean_estimation}
In this section, we focus on the mean estimation problem, which is a canonical statistical task in distributed estimation with applications in distributed stochastic gradient descent, federated learning, etc. 
Let the input space $\mcal{X}$ be the $d$-dimensional unit $\ell_2$ sphere, i.e.,  $\mcal{X} = \sphere^{d-1}$.
Consider $n$ users where user $i$  
% let each of $n$ users $i = 1,...,n$
has some data $\svbx_i \in \mcal{X}$. For every $i \in [n]$, let $\svbx_i$ be privatized using an $\varepsilon$-LDP mechanism $q(\cdot|\svbx_i)$ and potentially post-processed to obtain an estimate $\hat{\svbx}_i$ of $\svbx_i$. We are interested in estimating the \emph{empirical mean} $\svbmu \eqDef \frac{1}{n}\sum_i \svbx_i$ using $\hat{\svbx}_1,\cdots,\hat{\svbx}_n$ such that the mean estimation error defined below is minimized
\begin{equation}\label{eq:rme_def}
     r_{\msf{ME}} \lp \hat{\svbmu}, q \rp \eqDef \max_{\svbx^n \in \mcal{X}^n} \E\lb \left\| \hat{\svbmu}\lp \hat{\svbx}_1,\cdots ,\hat{\svbx}_n \rp - \svbmu \right\|^2_2 \rb,
\end{equation}
where $\hat{\svbmu}$ is an estimate of $\svbmu$ and
% $q(\cdot|\svbx)$ is an $\varepsilon$-LDP mechanism and $ \hat{\svbx}_i \sim q(\cdot|\svbx_i)$ for all $i \in [n]$ are the privatized samples. 
the expectation is with respect to $q(\cdot|\svbx_i) $ as well as all (possibly shared) randomness used by $q(\cdot|\svbx_i)~\forall i \in [n]$.

\cite{BDFKR2018} show that $\PrivUnit$ achieves the order-optimal privacy-accuracy trade-off for mean estimation, i.e., $r_{\msf{ME}} \lp \hat{\svbmu}^\texttt{pu}, \qPU \rp = \Theta\lp\frac{d}{\min\lp\varepsilon, (e^\varepsilon-1)^2, d\rp}\rp $ where $\hat{\svbmu}^\texttt{pu} \coloneqq \frac{1}{n}\sum_i \hat{\svbx}^\texttt{pu}_i$.
% , meaning that $\PrivUnit$ achieves the order-optimal privacy-accuracy trade-off.
Moreover, compared to other (order-optimal) $\varepsilon$-LDP mean estimation mechanisms, $\PrivUnit$ admits the best constants and gives the smallest $\ell_2$ error in practice (see \cite{FT21}). However, $\PrivUnit$ requires each user to send a $d$-dimensional real vector, so without any compression, the communication needed is $\Theta(d)$ bits, which can be an issue in many practical scenarios. 
% To address this issue, we show that by using MMRC (see Section \ref{subsec:mmrc}) to simulate $\PrivUnit$, we can significantly compress $\PrivUnit$ while preserving its accuracy.
 
To compress and simulate $\PrivUnit$, one can directly apply the generic $\MMRC$ mechanism defined in Section~\ref{subsec:mmrc}. However, for a fixed number of candidates $N$, $\MMRC$ yields a biased estimate of $\svbx$ and hence cannot get the correct (optimal) order of estimation error in \eqref{eq:rme_def}, i.e., the error would not decay with $n$\footnote{We note that this does not undermine the significance of Theorem \ref{theorem:mrc_accuracy_local} and Theorem \ref{theorem:mmrc_accuracy_local_wrt_mrc}. These are useful in single-user settings (i.e., $n = 1$) and are generic as they can compress (near-losslessly) any $\varepsilon$-LDP and $\varepsilon$-LDP cap-based mechanism, respectively.}. Fortunately, we show (in Section~\ref{subsec:mmrc_privunit}) that the bias can be corrected by appropriately scaling the privatized version of $\svbx$, i.e., by using an estimator which is
slightly different compared to the original estimator of $\PrivUnit$. Further, we also show (in Section \ref{subsec:sim_pu1}) that the resulting unbiased estimator for mean estimation ($\hat{\svbmu}^{\texttt{mmrc}}$) can simulate $\PrivUnit$ closely while only using on the order of $\varepsilon$ bits of communication.
% Analogously, $\PrivUnit$ can be simulated using MRC as well but, as described earlier, one has to either pay a factor of $2$ is the accuracy or settle for approximate privacy guarantee instead of pure privacy. 
% Analogously, we could  simulate $\PrivUnit$ with  $\MRC$ but we defer the details Appendix BLAH. 
% and focus on simulating $\PrivUnit$ using MMRC below.


% \subsection{Simulating $\PrivUnit$ using MRC}
% % Consider the $\PrivUnit$ $\varepsilon$-LDP mechanism $\qPU$ described in Section BLAH with parameters $p$ and $\gamma$. Let $\piMRC$ be the distribution and $\svbz_1, \svbz_2,...,\svbz_N$ be the candidates obtained from Algorithm \ref{alg:mrc} when the reference distribution is $\Unif(\sphere^{d-1})$. Let  $K \sim \piMRC(\cdot)$. Define $p_{\texttt{mrc}} \coloneqq \Probability(\svbz_K \in \{\svbz \in \sphere^{d-1} \mid
% % \lan \svbz, \svbx  \ran \ge \gamma\})$
% % % Let $p_{\texttt{mrc}} = \Probability(\svbz_K \in \msf{Cap}_{\svbx}_{\varepsilon, d}(\svbx))$
% % to be the probability with which the sampled candidate $\svbz_K$ belongs to the spherical cap associated with $\PrivUnit$.
% % % $\msf{Cap}_{\svbx}_{\varepsilon, d}(\svbx))$ and 
% % Define $m_{\texttt{mrc}}$ as the scaling factor in \eqref{eq:m} when $p_0$ in \eqref{eq:m} is replaced by $p_{\texttt{mrc}}$. The following Theorem shows that the scaling factor $m_{\texttt{mrc}}$ yields an unbiased estimator for MRC simulating $\PrivUnit$.
% % % $\svbz_K / m_{\texttt{mrc}}$ is an unbiased estimate of $\svbx$.
% % % Let $m_{\texttt{pu}}$ be defined in \eqref{eq:m}.  The following theorem shows that using $m_{\texttt{mrc}}$, which is defined as replacing $p_0$ in \eqref{eq:m} with $p_{\texttt{mrc}}$, as the scaling factor yields an unbiased estimator for MRC simulating $\PrivUnit$.

% % \begin{restatable}{theorem}{mrcprivunitbias}\label{theorem:mrc_privunit_bias}
% % % Let $\qPU(\svbz | \svbx)$ be the  $\varepsilon$-LDP \emph{$\PrivUnit$} mechanism. 
% % % Let $\cZ \coloneqq \sphere^{d-1}$ and let $\svbz_1, \svbz_2,...,\svbz_N$ be generated from 
% % % $ \msf{uniform}\lp \mcal{Z} \rp$. 
% % % Let $\piMRC$ be the distribution and $\svbz_1, \svbz_2,...,\svbz_N$ be the candidates obtained from Algorithm \ref{alg:mrc} when the $\varepsilon$-LDP mechanism is $\qPU$, the reference distribution is $\Unif(\sphere^{d-1})$ and the number of candidates $N \geq 1$. Let  $K \sim \piMRC(\cdot)$.
% % Let $m_{\texttt{mrc}}$ be as defined above. Then, $\Expectation[\svbz_K/m_\texttt{mrc}] = \svbx$.
% % % by replacing $p_0$ in \eqref{eq:m} with $p_{\texttt{mrc}} = \Probability(\svbz_K \in \msf{Cap}_{\svbx}_{\varepsilon, d}(\svbx))$. 
% % % Then, $\qMRC$ defined in \eqref{eq:qmrc} is such that, for any $\svbx \in \cX$, $\Expectation_{\qMRC(\cdot | \svbx)} [\svbz_K/m_\texttt{mrc}] = \svbx$.
% % \end{restatable}
% % Theorem~\ref{theorem:mmrc_privunit_bias} suggests that when $\svbx_K \sim \qMRC(\cdot|\svbx)$, $\hat{x}_\texttt{mrc}(\svbz_K) \coloneqq \svbz_K / m_\texttt{mrc}$ is an unbiased estimate of $\svbx$. Thus with a slight abuse of notation, we define $\hat{\svbx}_{\texttt{mrc}, i}$ to be the corresponding unbiased estimator of MRC at user $i$, (and thus $\E[\hat{\svbx}_{\texttt{mrc}, i}] = \svbx_i$).


% % It can be shown that when the number of candidates $N$ is sufficiently high, then the scaling parameters associated with $\PrivUnit$ and the MRC mechanism simulating $\PrivUnit$ are close. See Appendix~\ref{sec:mrc_pu_app} for more details.


% % Therefore, the next theorem ensures that as long as $N$ large enough, the accuracy of MRC converges to $\PrivUnit$.
% \begin{restatable}{theorem}{mrcprivunit}\label{theorem:mrc_privunit}
% Let $\qPU(\svbz | \svbx)$ be the  $\varepsilon$-LDP \emph{$\PrivUnit(\svbx,\gamma, p)$} mechanism with output $\hat{\svbx}_{\texttt{pu}}$. 
% Let $\qMRC(\svbz|\svbx)$ denote the MRC privatization mechanism simulating \emph{$\PrivUnit$} with output $\hat{\svbx}_{\texttt{mrc}}$. 
% Let $N$ denote the number of candidates. 
% Consider any $\lambda > 0$. Then,
% \begin{align}
%      &\Expectation_{\qMRC(\cdot|\svbx)} \big[ \lV  \hat{\svbx}_\texttt{mrc} - \svbx \rV^2_2  \big] 
%      \leq  \lp 1+\lambda \rp^2  \Expectation_{\qPU(\cdot | \svbx)}\big[\|\hat{\svbx}_{\texttt{pu}} - \svbx\|^2\big] \\
%     & \qquad + 2(1+\lambda)(2+\lambda) \sqrt{\Expectation_{\qPU(\cdot | \svbx)}\big[\|\hat{\svbx}_{\texttt{pu}} - \svbx\|^2\big]} + (2+\lambda)^2
% \end{align}
% as long as 
% \begin{align}\label{eq:N_bdd_mrc_pu}
%     N \geq   2e^{2\varepsilon}\lp\frac{2 (1+\lambda)}{\lambda \lp p -1/2 \rp}\rp^2 \log\lp \frac{2(1+\lambda)}{\lambda \lp p -1/2 \rp} \rp.
% \end{align}
% \end{restatable}
% Since each $\hat{\svbx}_{\texttt{mrc}, i}$ is unbiased and independent to each other, averaging across $i \in [n]$ (i.e., set $\hat{\svbmu}_{\texttt{mrc}} \coloneqq \frac{1}{n}\sum_i \hat{\svbx}_{\texttt{mrc}, i}$) we immediately obtain the following corollary:
% \begin{corollary}
% As long as $N$ satisfies \eqref{eq:N_bdd_mrc_pu},
% \begin{align}
%     &r_{\msf{ME}} \lp \hat{\svbmu}_\texttt{mrc}, \qMRC \rp \leq  
%     \lp 1+\lambda \rp^2 r_{\msf{ME}} \lp \hat{\svbmu}^\texttt{pu}, \qPU \rp\\
%     & \qquad + 2(1+\lambda)(2+\lambda) \sqrt{\frac{r_{\msf{ME}} \lp \hat{\svbmu}^\texttt{pu}, \qPU \rp}{n}} + \frac{(2+\lambda)^2}{n}.
% \end{align}
% In particular, for $\varepsilon = o(d)$, 
% $$ r_{\msf{ME}} \lp \hat{\svbmu}_\texttt{mrc}, \qMRC \rp \ra r_{\msf{ME}} \lp \hat{\svbmu}^\texttt{pu}, \qPU \rp $$ as long as $\lambda \ra 0$ and $n \ra \infty$.
% % r_{\msf{ME}} \lp \hat{\svbmu}^\texttt{pu}, \qPU \rp = \Theta\lp \frac{d}{n\min\lp \varepsilon, \varepsilon^2 \rp} \rp
% \end{corollary}


\subsection{Debiasing \texorpdfstring{$\MMRC$}{MMRC} to simulate \texorpdfstring{$\PrivUnit$}{PrivUnit}}
\label{subsec:mmrc_privunit}

Let us focus on a single user and consider some data $\svbx \in \cX$. Recall the $\PrivUnit$ $\varepsilon$-LDP mechanism $\qPU$ described in Section \ref{sec:preliminaries} with parameters $p_0$ and $\gamma$. $\PrivUnit$ is a cap-based mechanism with $\msf{Cap}_{\svbx} = \{\svbz \in \sphere^{d-1} \mid
\lan \svbz, \svbx  \ran \ge \gamma\}$ (see Appendix \ref{appendix:privunit} for details).
% and with appropriate $c_1(\varepsilon,d)$, $c_2(\varepsilon,d)$ and $\Probability_{\svbz \sim \Unif(\cZ)}\lp \svbz \in \msf{Cap}_{\svbx} \rp$ defined in Appendix BLAH.
% Recall the definitions of lower threshold $t_l$ and upper threshold $t_u$ from Section \ref{subsec:mmrc}. 
Let $\piMMRC$ be the distribution and $\svbz_1, \svbz_2,...,\svbz_N$ be the candidates obtained from Algorithm \ref{alg:mmrc} when the reference distribution is $\Unif(\sphere^{d-1})$. Let  $K \sim \piMMRC(\cdot)$. Therefore, $\svbz_K$ is the privatized version of $\svbx$ using $\MMRC$.

Define $p_{\texttt{mmrc}} \coloneqq \Probability(\svbz_K \in \msf{Cap}_{\svbx})$
% Let $p_{\texttt{mrc}} = \Probability(\svbz_K \in \msf{Cap}_{\svbx}_{\varepsilon, d}(\svbx))$
to be the probability with which the sampled candidate $\svbz_K$ belongs to the spherical cap associated with $\PrivUnit$.
Define $m_{\texttt{mmrc}}$ as the scaling factor in \eqref{eq:m} when $p_0$ in \eqref{eq:m} is replaced by $p_{\texttt{mmrc}}$. Define $\hat{\svbx}^\texttt{mmrc} \coloneqq \svbz_K / m_\texttt{mmrc}$ as the estimator of the $\MMRC$ mechanism simulating $\PrivUnit$.  The following Lemma shows that $\hat{\svbx}^\texttt{mmrc}$ is an unbiased estimator. See Appendix \ref{appendix:mmrc_pu_debias} for a proof.

\begin{restatable}{lemma}{mmrcprivunitbias}\label{lemma:mmrc_privunit_bias}
Let $\hat{\svbx}^\texttt{mmrc}$ be the estimator of the \emph{$\MMRC$} mechanism simulating \emph{$\PrivUnit$} as defined above. Then, $\Expectation_{\qMMRC}[\hat{\svbx}^\texttt{mmrc}] = \svbx$.
\end{restatable}

% Thus with a slight abuse of notation, we define $\hat{\svbx}_{\texttt{mrc}, i}$ to be the corresponding unbiased estimator of MRC at user $i$, (and thus $\E[\hat{\svbx}_{\texttt{mrc}, i}] = \svbx_i$).


% Although the accuracy of MRC converge to that of $\PrivUnit$, the MRC only guarantees $2\varepsilon$-LDP. To obtain the exact $\varepsilon$-LDP, we need to leverage the MMRC to simulate $\PrivUnit$. In this section, we show that MMRC can also be de-baised and its accuracy converges to that of $\PrivUnit$ as well.

%\paragraph{De-biasing MMRC simulating $\PrivUnit$}
% As in MRC, the sampled candidate  $\svbz_{\tilde{K}}$ needs to be scaled by an appropriate factor to ensure unbiasedness. In fact, if we define $p_\texttt{mmrc} \coloneqq \Probability_{\piMMRC}(\svbz_{\tilde{K}} \in \msf{Cap}_{\svbx}_{\varepsilon, d}(\svbx))$, it can be shown that this scaling factor is equal to \eqref{eq:m} with $p_0$ being replaced by $p_\texttt{mmrc}$. See Appendix~\ref{sec:mmrc_pu_app} for more details.
% Therefore, if let $\svbz_K \sim \qMMRC$, $\hat{x}_\texttt{mmrc}(\svbz_K) \coloneqq \svbz_K / m_\texttt{mmrc}$ is an unbiased estimate of $\svbx$. 
% \textRed{The following Lemma (i) shows that $\qMMRC$ can compress $\qPU$ to the order of $\varepsilon$-bits of communication and (ii) bounds the $\ell_2$ error of the estimator $\hat{\svbx}^\texttt{mmrc}$ in terms of the $\ell_2$ error of the estimator $\hat{\svbx}^\texttt{pu}$.}
% % while offering order-optimal privacy-accuracy tradeoffs  
% % simulate it in a near-lossless fashion ().
% Proof can be found in Appendix \ref{appendix:mmrc_pu_utility}. The key idea in the proof is to show that when the number of candidates $N$ is exponential in $\varepsilon$, the scaling factor $m_{\texttt{mmrc}}$ is close to the scaling parameter associated with $\PrivUnit$ (i.e., $m_{\texttt{pu}}$ defined in \eqref{eq:m}).
% and the scaling factor $m_{\texttt{mmrc}}$ of the MMRC mechanism simulating $\PrivUnit$ are close.
% In order to show that the MMRC mechanism with the estimator $\hat{\svbx}^\texttt{mmrc}$ preserves the accuracy of $\PrivUnit$, we first show that when the number of candidates $N$ is exponential in $\varepsilon$, the scaling parameters associated with $\PrivUnit$ (i.e., $m_{\texttt{pu}}$ defined in \eqref{eq:m}) and the scaling factor $m_{\texttt{mmrc}}$ of the MMRC mechanism simulating $\PrivUnit$ are close (see Appendix BLAH for more details). This allows us to show, in the following Theorem, that $\qMMRC$ can compress $\qPU$ to the order of $\varepsilon$-bits of communication as well as simulate it in a near-lossless fashion. Proof can be found in Appendix BLAH.

% Similar to MRC, the scaling parameters associated with $\PrivUnit$ and the MMRC are close when $N$ is large enough, which gives the following accuracy guarantee of the MMRC mechanism.
% \begin{restatable}{lemma}{mmrcprivunit}\label{theorem:mmrc_accuracy_privunit}
% Let $\qPU(\svbz | \svbx)$ be the  $\varepsilon$-LDP \emph{$\PrivUnit$} mechanism with parameters $p_0$ and $\gamma$ and estimator $\hat{\svbx}^{\texttt{pu}}$. Let $\qMMRC(\svbz|\svbx)$ denote the \emph{$\MMRC$} privatization mechanism simulating \emph{$\PrivUnit$} with $N$ candidates and estimator $\hat{\svbx}^{\texttt{mmrc}}$ as defined above. 
% Consider any $\lambda > 0$. Then,
% \begin{align}
%   &\Expectation_{\qMMRC} \big[ \lV  \hat{\svbx}^\texttt{mmrc} \hspace{-0.5mm} - \hspace{-0.5mm} \svbx \rV^2_2  \big] 
%      \leq  \lp 1+\lambda \rp^2  \Expectation_{\qPU}\big[\|\hat{\svbx}^{\texttt{pu}} \hspace{-0.5mm} - \hspace{-0.5mm} \svbx\|^2\big] \\
%     & \hspace{2mm} + 2(1+\lambda)(2+\lambda) \sqrt{\Expectation_{\qPU}\big[\|\hat{\svbx}^{\texttt{pu}} \hspace{-0.5mm} - \hspace{-0.5mm}  \svbx\|^2\big]} + (2+\lambda)^2
% \end{align}
% as long as 
% \begin{align}\label{eq:N_bdd_mmrc_pu}
% \textstyle
%     N \geq   \frac{e^{2\varepsilon}}{2}\lp\frac{2 (1+\lambda)}{\lambda \lp p_0 -1/2 \rp}\rp^2 \ln\lp \frac{4(1+\lambda)}{\lambda \lp p_0 -1/2 \rp} \rp.
% \end{align}
% \end{restatable}

\subsection{Simulating \texorpdfstring{$\PrivUnit$}{PrivUnit} using \texorpdfstring{$\MMRC$}{MMRC}}
\label{subsec:sim_pu1}

Finally, we consider estimating the empirical mean $\svbmu$ defined earlier using the $\MMRC$ scheme simulating $\PrivUnit$. To that end, consider $n$ users and let $\hat{\svbx}^{\texttt{mmrc}}_i$ be the unbiased estimator of $\svbx_i$ at the $i^{th}$ user. Let the (unbiased) estimate of $\svbmu$ be $\hat{\svbmu}^{\texttt{mmrc}} \coloneqq \frac{1}{n}\sum_i \hat{\svbx}^{\texttt{mmrc}}_i$. 

The following Theorem shows that, for mean estimation, $\MMRC$ can simulate $\PrivUnit$ in a near-lossless manner (when $n$ is large and $\lambda$ is small) while only using on the order of $\varepsilon$ bits of communication.
A proof can be found in Appendix \ref{appendix:mmrc_pu_utility}. The key idea in the proof is to show that when the number of candidates $N$ is exponential in $\varepsilon$, the scaling factor $m_{\texttt{mmrc}}$ is close to the scaling parameter associated with $\PrivUnit$ (i.e., $m_{\texttt{pu}}$ defined in \eqref{eq:m}).
% For all $i \in [n]$, since $\hat{\svbx}^{\texttt{mmrc}}_i$ are independent of each other as well as unbiased, we obtain the following Theorem directly from Lemma~\ref{theorem:mmrc_accuracy_privunit}.

% Applying Theorem~\ref{theorem:mmrc_privunit} and setting 
% $$\hat{\svbmu}_{\texttt{mmrc}} \coloneqq \frac{1}{n}\sum_i \hat{\svbx}_{\texttt{mmrc}, i},$$ we obtain the following result:
\begin{restatable}{theorem}{mmrcpu}
\label{thm:me_mmrc_pu}
Let $r_{\msf{ME}} \lp \hat{\svbmu}^\texttt{pu}, \qPU \rp$ and $r_{\msf{ME}} \lp \hat{\svbmu}^\texttt{mmrc}, \qMMRC \rp$ be the empirical mean estimation error for \emph{$\PrivUnit$} with parameter $p_0$ and \emph{$\MMRC$} simulating \emph{$\PrivUnit$} with $N$ candidates respectively. Consider any $\lambda > 0$. Then,
\begin{align}
    r_{\msf{ME}} \lp \hat{\svbmu}^\texttt{mmrc}, \qMMRC \rp \leq \lp 1+\lambda \rp^2 r_{\msf{ME}} \lp \hat{\svbmu}^\texttt{pu}, \qPU \rp + 2(1+\lambda)(2+\lambda) \sqrt{\frac{r_{\msf{ME}} \lp \hat{\svbmu}^\texttt{pu}, \qPU \rp}{n}} + \frac{(2+\lambda)^2}{n}.
\end{align}
as long as 
\begin{align}\label{eq:N_bdd_mmrc_pu}
% \textstyle
    N \geq   \frac{e^{2\varepsilon}}{2}\lp\frac{2 (1+\lambda)}{\lambda \lp p_0 -1/2 \rp}\rp^2 \ln\lp \frac{4(1+\lambda)}{\lambda \lp p_0 -1/2 \rp} \rp.
\end{align}
% $N$ satisfies \eqref{eq:N_bdd_mmrc_pu}.
\end{restatable}
We note that while a specific value of $\lambda$ can be chosen (say $0.1$ or smaller) in \eqref{eq:N_bdd_mmrc_pu}, in practice, the number of bits could be fixed (see Section \ref{subsec:mmrc_privunit_empirical}), determining the value of $\lambda$.

\subsection{Empirical Comparisons}
\label{subsec:mmrc_privunit_empirical}
Next, we empirically demonstrate the privacy-accuracy-communication tradeoffs of $\MMRC$ simulating $\PrivUnit$. Along with $\PrivUnit$, we compare against the SQKR algorithm of \cite{CKO20} which offers order-optimal privacy-accuracy tradeoffs while requiring only $\varepsilon$ bits. Following \cite{CKO20}, we generate data independently but non-identically to capture the distribution-free setting as well as ensure that the data non-central, i.e. $\svbmu \neq 0$. More specifically, we set $\svbx_1,\cdots,\svbx_{n/2} \diid N(1,1)^{\otimes d}$ and $\svbx_{n/2+1},\cdots,\svbx_{n} \diid N(10,1)^{\otimes d}$. Further, to ensure that each data lies on $\sphere^{d-1}$, we normalize each $\svbx_i$ by setting $\svbx_i \leftarrow \svbx_i/\lV \svbx_i \rV_2$. We report the average $\ell_2$ estimation error over 10 runs. See more variations in Appendix \ref{appendix:mmrc_pu_emp}. 
% See more details and variations in Appendix \ref{appendix:mmrc_pu_emp}.

\begin{figure}[h]
\centering
\includegraphics[width=0.45\linewidth]{figures/cc6.pdf} \qquad
\includegraphics[width=0.45\linewidth]{figures/eps1.pdf}%
\caption{Comparing $\PrivUnit$, $\MMRC$ simulating $\PrivUnit$ and SQKR for mean estimation with $d = 500$ and $n = 5000$. \textbf{Left:} $\ell_2$ error vs $\#$bits for $\varepsilon = 6$. \textbf{Right:} $\ell_2$ error vs $\varepsilon$ for $\#$bits $= \max\{( \varepsilon/ \ln 2) + 2, 8\}$. SQKR uses $\#$-bits $= \varepsilon$ for both because it leads to a poor performance if $\#$-bits $ > \varepsilon$.}
\label{fig:mean}
\end{figure}

In Figure \ref{fig:mean} (Left), we show the communication-accuracy tradeoffs. We see that with correct order of bits, the accuracy of $\MMRC$ simulating $\PrivUnit$ converges to the accuracy of the uncompressed $\PrivUnit$. In Figure \ref{fig:mean} (Right), we show the privacy-accuracy tradeoffs. We see that $\MMRC$ simulating $\PrivUnit$ can attain accuracy of the uncompressed $\PrivUnit$ for the range of  $\varepsilon$'s typically considered by LDP mechanisms while only using $\max\{\lceil( \varepsilon/ \ln 2)\rceil + 2, 8\}$ bits.