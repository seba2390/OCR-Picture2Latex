\section{Preliminaries}
\label{sec:preliminaries}
% \paragraph{Locally Differentially Private (LDP).}
\subsection{Locally Differentially Private (LDP)}
% \label{subsec:ldp}
Suppose $\svbx \in \cX$ is some user's data that must remain private.  A privatization mechanism $q$ is a randomized mapping that maps $\svbx \in \cX$ to $\svbz \in \cZ$ with probability $q(\svbz|\svbx)$ where $\cZ$ can be arbitrary. The user transmits $\svbz \sim q(\cdot|\svbx)$, i.e., a privatized version of $\svbx$ to the server. Further, $q$ is $\varepsilon$-LDP if 
\begin{align}
    \forall \svbx, \svbx' \in \cX, \svbz \in \cZ, ~ q(\svbz|\svbx) \leq e^{\varepsilon} q(\svbz|\svbx')  \label{eq:ldp}
\end{align}
and $q$ is $(\varepsilon, \delta)$-LDP if $\forall \svbx, \svbx' \in \cX, Z \subseteq \cZ,$
$$\sum_{\svbz \in Z} q(\svbz|\svbx) \leq e^{\varepsilon} \sum_{\svbz \in Z} q(\svbz|\svbx') + \delta.$$
% \begin{align}
%      \sum_{\svbz \in Z} q(\svbz|\svbx) \leq \exp(\varepsilon) \sum_{\svbz \in Z} q(\svbz|\svbx') + \delta. \label{eq:ldp2}
% \end{align}
Here, we focus on $\varepsilon$-LDP mechanisms where $\varepsilon \geq 1$.

% From now onwards, we will denote an $\varepsilon$-LDP $q$ by $q^\varepsilon$.

% \paragraph{Shared Randomness.}
\subsection{Shared Randomness}
\label{subsec:shared_randomness}
% \begin{remark}
Here, we allow $\varepsilon$-LDP mechanisms to use \emph{shared randomness}. That is, $q$ can depend on a random variable $\svbu \in \cU$ that is known to both the user and the server (but $\svbu$ is independent of $\svbx$). The corresponding $\varepsilon$-LDP constraint is
$$\forall \svbx, \svbx' \in \cX, \svbz \in \cZ, \svbu \in \cU, ~ q(\svbz|\svbx, \svbu) \leq \exp(\varepsilon)q(\svbz|\svbx', \svbu). $$
The server wishes to reconstruct $\svbx$ from $\svbz$ and the corresponding estimator is allowed to implicitly depend on $\svbu$.
% We allow the estimator of $\svbx$ at the server to implicitly depend on $\svbu$. 
However, for simplicity, we suppress the dependence on $\svbu$ in our notation. In practice, shared randomness can be achieved via downlink communication, that is, the server generates $\svbu$ (e.g., a random seed) and communicates it to the user. Further, we note that such shared randomness can be established well before the advent of any private data\footnote{Quantifying the amount of such shared randomness required remains an open question. See Section \ref{sec:conclusion}.}.
% \end{remark}

%\subsection{Shared Randomness}
\subsection{\texorpdfstring{PrivUnit$_2$}{PrivUnit}}
% \label{subsec:privunit}
% \paragraph{PrivUnit$_2$.}
The $\PrivUnit$ mechanism $\qPU$, proposed by \cite{BDFKR2018}, is an $\varepsilon$-LDP sampling scheme when the input alphabet $\cX$ is the $d-$dimensional unit $\ell_2$ sphere $\sphere^{d-1}$. Formally, given a vector $\svbx \in \sphere^{d-1}$, $\PrivUnit$ draws a random vector $\svbz$ 
% from $\sphere^{d}$ with the following conditional probability:
from a spherical cap $\{\svbz \in \sphere^{d-1} \mid
\lan \svbz, \svbx  \ran \ge \gamma\}$ with probability $p_0$ or from its complement $\{\svbz \in \sphere^{d-1} \mid \lan \svbz, \svbx \ran < \gamma\}$  with probability $1 - p_0$, 
% \begin{align}\label{eq:q_ss}
%     q^{\varepsilon}(\svbz|\svbx) \coloneqq \begin{cases}
%     \frac{e^{\varepsilon}}{{\binom{d-1}{s-1}}e^{\varepsilon}+{\binom{d-1}{s}}} & \text{if}\ \svbz : \lan \svbz, \svbx  \ran \ge \gamma
%     \\[10pt]
%     \frac{1}{{\binom{d-1}{s-1}}e^{\varepsilon}+{\binom{d-1}{s}}} & \text{if}\ \svbz : \lan \svbz, \svbx  \ran < \gamma
%     \end{cases}
% \end{align}
where $\gamma \in [0, 1]$ and $p_0  \geq 1/2$ are parameters (depending on $\varepsilon$ and $d$) that trade accuracy and privacy (see Appendix \ref{appendix:privunit}). In other words, $\qPU$ is as follows:
\begin{align}
    \qPU(\svbz | \svbx)   =   
    \begin{cases}
        \dfrac{2 p_0}{A(1,d)I_{1-\gamma^2}(\frac{d-1}{2},\frac{1}{2})} \hspace{10pt} \text{if}\ \langle \rvbx, \rvbz \rangle \geq \gamma \\[10pt]
       \dfrac{2(1-p_0)}{2A(1,d)  -  A(1,d)I_{1-\gamma^2}(\frac{d-1}{2},\frac{1}{2})} \hspace{6pt} \text{otherwise}
    \end{cases}
\end{align}
where $A(1,d)$ denotes the area of $\sphere^{d-1}$ and $I_x(a,b)$ denotes the regularized incomplete beta function. The estimator of the $\PrivUnit$ mechanism (denoted by $\hat{\svbx}^{\texttt{pu}}$) is obtained by dividing every coordinate of $\svbz$ by $m_{\texttt{pu}}$ i.e., $\hat{\svbx}^{\texttt{pu}} \coloneqq \svbz/m_{\texttt{pu}}$ where 
\begin{align}
    m_{\texttt{pu}} \coloneqq  \frac{(1 - \gamma^2)^\alpha}{2^{d-2} (d - 1)}
  \left[\frac{p_0}{B(1; \alpha,\alpha) - B(\tau; \alpha,\alpha)}
     - \frac{1 - p_0}{B(\tau; \alpha, \alpha)}\right]
  \label{eq:m}
\end{align}
%\begin{align}
    %m_{\texttt{pu}}   \coloneqq    \frac{(1   -   \gamma^2)^\alpha}{2^{d-2} (d   -   1)}
  %  \left[\frac{p_0}{B(1; \alpha,\alpha)   -   B(\tau; \alpha,\alpha)}
     %  -   \frac{1 - p_0}{B(\tau; \alpha, \alpha)}\right]
  %\label{eq:m}
%\end{align}
with $\alpha = (d-1)/2$, $\tau = (1+\gamma)/2$, and $B(x;\alpha,\beta)$ denoting the incomplete beta function. The estimator $\hat{\svbx}^{\texttt{pu}}$ is (a) unbiased i.e., $\Expectation[\hat{\svbx}^{\texttt{pu}} | \svbx] = \svbx$, (b) has order-optimal utility i.e., $\E[\ltwo{\hat{\svbx}^{\texttt{pu}} - \svbx}^2] = \Theta\lp
   \frac{d}{\min\lp\varepsilon, (e^\varepsilon-1)^2, d\rp}\rp$, and (c) achieves the best known constants for mean estimation. See Appendix \ref{appendix:privunit} for more details on $\PrivUnit$.

% In order to have an unbiased estimator, the vector $\svbz$ is scaled by $1/m$ where 
% \begin{align}
%     m \coloneqq \frac{(1 - \gamma^2)^\alpha}{2^{d-2} (d - 1)}
%     \left[\frac{p}{B(1; \alpha,\alpha)   -   B(\tau; \alpha,\alpha)}
%        -   \frac{1 - p}{B(\tau; \alpha, \alpha)}\right]
%   \label{eq:m}
% \end{align}
% with $\alpha = (d-1)/2$, $\tau = (1+\gamma)/2$, and $B(x;\alpha,\beta)$ denoting the incomplete beta function.
% More precisely, the estimator of the $\PrivUnit$ mechanism (denoted by $\hat{\svbx}^{\texttt{pu}}$) is defined as $\svbx/m$ and is such that $\Expectation[\hat{\svbx}^{\texttt{pu}} | \svbx] = \svbx$.
% For appropriate choices of the spherical cap level $\gamma$ and more details about the $\PrivUnit$ mechanism, see Appendix. 


\subsection{Subset Selection}
% \label{subsec:subsetselection}
% \paragraph{Subset Selection.}
The $\SubsetSelection$ mechanism $\qSS$, proposed by \cite{YB18}, is an $\varepsilon$-LDP sampling scheme when the input alphabet $\cX$ can take $d$ different values.
% $\cX$ is $[d] = \{1,\cdots,d\}$. 
Without loss of generality, let 
$\cX \coloneqq \lbp e_1, e_2,...,e_d \rbp$, where $e_j \in \{0, 1\}^d$ is the $j^{th}$ standard unit vector, i.e., the one-hot encoding of $j$.
The output alphabet $\cZ$ is the set of all $d$-bit binary strings with Hamming weight $s \coloneqq \lceil \frac{d}{1+e^{\varepsilon}}\rceil$, i.e.,
\begin{align*}
\textstyle
\cZ = \lbp \svbz = (z^{(1)}, \cdots ,z^{(d)}) \in \{0, 1\}^d: \sum_{i=1}^d z^{(i)} = s \rbp.
\end{align*}
Given $\svbx \in \cX$,  $\SubsetSelection$ maps it to $\svbz \in \cZ$ with the following conditional probability:
% Given a symbol $x \in [d]$, $\SubsetSelection$ first maps $x$ to its one-hot encoding
% $\svbx = (x^{(1)}, \cdots ,x^{(d)}) \in \{0, 1\}^d$ and then maps $\svbx$ to $\svbz \in \cZ$ with the following conditional probability:
\begin{align}\label{eq:q_ss}
    \qSS(\svbz|\svbx) \coloneqq \begin{cases}
    \frac{e^{\varepsilon}}{{ \binom{d-1}{s-1} }e^{\varepsilon}+{ \binom{d-1}{s} }} & \text{if}\ \svbz \in \cZ_{\svbx}
    \\[10pt]
    \frac{1}{{ \binom{d-1}{s-1} }e^{\varepsilon}+{ \binom{d-1}{s} }} & \text{if}\ \svbz \in \cZ \setminus \cZ_{\svbx}
    \end{cases}
\end{align}
where $\cZ_{\svbx} = \lbp \svbz = (z^{(1)}, \cdots ,z^{(d)}) \in \cZ : z^{(x)} = 1 \rbp$ is the set of elements in $\cZ$ with $1$ in the $x^{th}$ location. The estimator of the $\SubsetSelection$ mechanism (denoted by $\hat{\svbx}^{\texttt{ss}}$) is obtained by subtracting $b_{\texttt{ss}}$ from every component of $\svbz$ and dividing every component of the result by $m_{\texttt{ss}}$ i.e., $\hat{\svbx}^{\texttt{ss}} \coloneqq (\svbz - b_{\texttt{ss}})/m_{\texttt{ss}}$ where 
\begin{align}\label{eq:m_ss}
    m_{\texttt{ss}} \coloneqq \frac{s(d  -  s)(e^{\varepsilon}-1)}{(d  -  1)(s(e^{\varepsilon}  -  1)+d)},\  b_{\texttt{ss}} \coloneqq  \frac{s((s  -  1)e^{\varepsilon} +(d  -  s))}{(d  -  1)(s(e^{\varepsilon}  -  1)+d)}.
\end{align}
% \begin{align}\label{eq:m_ss}\textstyle
%     m_{\texttt{ss}}   \coloneqq   \frac{s(d   -   s)(e^{\varepsilon}-1)}{(d   -   1)(s(e^{\varepsilon}   -   1)+d)},
%     \quad b_{\texttt{ss}}   \coloneqq   \frac{s((s   -   1)e^{\varepsilon} +(d   -   s))}{(d   -   1)(s(e^{\varepsilon}   -   1)+d)}.
% \end{align}
% first maps it to its one-hot encoding $\svbx = (x^{(1)}, \cdots ,x^{(d)}) \in \{0, 1\}^d$. Then, with $t \coloneqq \lceil \frac{d}{1+e^{\varepsilon}}\rceil$, 
The estimator $\hat{\svbx}^{\texttt{ss}}$ is (a) unbiased i.e., $\Expectation[\hat{\svbx}^{\texttt{ss}} | \svbx] = \svbx$, (b) has optimal utility i.e., $\E[\ltwo{\hat{\svbx}^{\texttt{ss}} - \svbx}^2] = \Theta\lp \frac{d}{\min\lp e^{\varepsilon}, \lp e^{\varepsilon}-1 \rp^2, d \rp}\rp$ and (c) achieves the best known constants for frequency estimation. See Appendix \ref{appendix:ss} for more details on $\SubsetSelection$.