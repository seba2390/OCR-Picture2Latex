\section{Introduction}
\label{sec:introduction}
Machine learning and data analytics are critical tools for designing better products and services. So far, these tools have been predominantly applied in datacenters on data that was curated from millions of users. However, centralized data collection and processing can expose individuals to privacy risks and organizations to legal risks if data is not properly managed. Indeed, increasing privacy concerns are fueling the demand for distributed learning and analytics systems that ensure that the underlying data remains private and secure. This is evident from the recent surge of interest in federated learning and analytics \citep[e.g.,][]{RM20,KM21}.

Designing private and efficient distributed learning and analytics systems involves addressing three main challenges: (a) preserving the privacy of the user's local data, (b) communicating the privatized data efficiently to a central server, and (c) achieving high accuracy on a task (e.g., mean or frequency estimation). Privacy is often achieved by enforcing $\varepsilon$-local differential privacy
% \footnote{See Section \ref{sec:preliminaries} for a rigorous definition.}
($\varepsilon$-LDP) \citep{warner1965randomized, evfimievski2003limiting, dwork2006calibrating, kasiviswanathan2011can}, which guarantees that the outcome from a privatization mechanism will not release too much individual information statistically. Efficient communication, on the other hand, is achieved via compression and dimensionality reduction techniques  \citep{an2016distributed, alistarh17qsgd, wen2017terngrad, wang2018atomo, han2018distributed, han2018geometric, agarwal2018cpsgd, g2019vqsgd, barnes2020rtopk, CKO21}. 

Most existing works focus on addressing two of the three above-mentioned challenges, such as achieving good privacy-accuracy or good communication-accuracy tradeoffs separately. However, doing so can lead to suboptimal performance where all three desiderata are concerned. It is thus important to investigate the joint privacy-communication-accuracy tradeoffs when designing communication-efficient and private distributed algorithms. Under $\varepsilon$-LDP constraints, \cite{CKO20} presents minimax order-optimal mechanisms for frequency and mean estimation that require only $\varepsilon$ bits (independent of the underlying dimensionality of the problem) by using shared randomness\footnote{We assume that the encoder and the decoder can depend on a random quantity that both the server and the user have access to. See Section \ref{subsec:shared_randomness} for details.}.
However,  as noted by \cite{FT21}, the algorithms of \cite{CKO20} are not competitive in terms of accuracy with the best known schemes -- $\SubsetSelection$ for frequency estimation \citep{YB18} and $\PrivUnit$ for mean estimation \citep{BDFKR2018}.
Motivated by this fact, the present work addresses the following fundamental question: \emph{Can we attain the best known accuracy under $\varepsilon$-LDP while only using on the order of $\varepsilon$ bits of communication}? We answer this question affirmatively by leveraging a technique based on importance sampling called Minimal Random Coding \citep{HPHJ19,C08,SCV16}. 
% \textRed{More concretely, we show that schemes based on Minimal Random Coding can simulate any $\varepsilon$-LDP mechanism using roughly $\varepsilon$ bits in a manner which preserves the privacy-accuracy tradeoffs. Building on top of this, we will simulate the best-known schemes for mean and frequency estimation using roughly $\varepsilon$ bits in a nearly-lossless fashion.}

\subsection{Our Contributions}
\label{subsec:contributions}
We first demonstrate that Minimal Random Coding ($\MRC$) can compress any $\varepsilon$-LDP mechanism in a near-lossless fashion using only on the order of $\varepsilon$ bits of communication (see Theorem \ref{theorem:mrc_accuracy_local}). We also prove that the resulting compressed mechanism is $2\varepsilon$-LDP (see Theorem \ref{theorem:mrc_pure_privacy}).
%Together, these results imply that to obtain $\varepsilon$-LDP using on the order $\varepsilon$-bits of communication, one has to pay a factor of 2 in accuracy. 
Thus, to achieve $\varepsilon$-LDP, one has to simulate an $\varepsilon/2$ mechanism and pay the corresponding penalty in accuracy. 
Similar to \cite{CKO20}, this approach can achieve the order optimal privacy-accuracy tradeoffs with about $\varepsilon$ bits of communication but is not competitive with the best known LDP schemes. However, we show that this approach is optimal if one is willing to accept approximate LDP with a small $\delta$ (see Theorem~\ref{theorem:mrc_approximate_privacy}).
% 2

To overcome the limitations of $\MRC$ in the pure LDP case, we present a modified version ($\MMRC$) such that the resulting compressed mechanism is $\varepsilon$-LDP (see Theorem \ref{theorem:mmrc_privacy}). We show that $\MMRC$ can simulate a large class of LDP mechanisms in a near-lossless fashion using only on the order of $\varepsilon$ bits of communication (see Theorem \ref{theorem:mmrc_accuracy_local_wrt_mrc} in conjunction with Theorem \ref{theorem:mrc_accuracy_local}).

% 3
While the class of LDP mechanisms $\MMRC$ can simulate includes the best-known schemes for mean and frequency estimation, $\MMRC$ (similar to $\MRC$) is biased for a fixed number of bits of communication. We show that $\MMRC$ simulating $\PrivUnit$ and $\SubsetSelection$ can be debiased (see Lemma \ref{lemma:mmrc_privunit_bias} and Lemma \ref{lemma:mmrc_ss_bias}), while preserving the corresponding accuracy guarantees (see Theorem \ref{thm:me_mmrc_pu} and Theorem \ref{thm:fe_mmrc_ss}).

% 4
% Finally, we empirically demonstrate the three-way privacy-accuracy-communication trade-offs of $\MRC$ and $\MMRC$ simulating $\PrivUnit$ and $\SubsetSelection$ in comparison to existing state-of-the-art methods on a variety of synthetic datasets.

Finally, we empirically demonstrate that $\MMRC$ achieves an accuracy comparable to $\PrivUnit$ and $\SubsetSelection$ (see Section \ref{subsec:sim_pu1} and Section \ref{subsec:sim_ss1})\footnote{The source code of our implementation is available at 
\url{https://tinyurl.com/rcc-dp}.} while only using about $\varepsilon$ bits.


We discuss interesting open problems in Section \ref{sec:conclusion} and defer all additional results and experiments to the Appendix. 


\subsection{Related Work}
\label{subsec:related_works}
Recent works have examined approaches for compressing LDP schemes in the presence of shared randomness. 
When $\varepsilon \leq 1$, for frequency estimation, \cite{BS15} showed that a single bit is enough to simulate any LDP randomizer with (almost) no impact on its utility although with a large amount of shared randomness. Their result was improved upon, in terms of the amount of shared randomness required, by \cite{bassily2017practical}, \cite{BNS19}, and \cite{acharya2019communication}.
% Other communication efficient frequency estimation schemes include \cite{bassily2017practical}, \cite{BNS19}, and \cite{acharya2019communication}.}
% Their result was improved upon (either in terms of computational cost or amount of shared randomness required) by \cite{bassily2017practical}, \cite{BNS19}, \cite{acharya2019communication}.}
% When $\varepsilon = O(1)$, \cite{BS15} show that a single bit is enough to simulate any LDP randomizer with (almost) no impact on its utility, and 
% 2. \cite{bassily2017practical}, \cite{BNS19}, \cite{acharya2019communication} propose $1$-bit order-optimal schemes for frequency estimation. 

\cite{CKO20} generalized these methods to arbitrary $\varepsilon$'s, and provided order-optimal schemes for both frequency and mean estimation that only use on the order of $\varepsilon$ bits. However, their method is only order-optimal and cannot achieve the accuracy of the best known schemes: $\PrivUnit$ \citep{BDFKR2018} for mean estimation and $\SubsetSelection$ \citep{YB18}\footnote{$\SubsetSelection$ is similar to asymmetric RAPPOR \citep{erlingsson14rappor} in the sense that both have the same marginal distribution. Here, we focus on simulating $\SubsetSelection$.} 
for frequency estimation. We show how one can achieve the accuracy of these schemes with on the order of $\varepsilon$ bits of communication (when $\varepsilon \geq 1$). While we don't advocate large $\varepsilon$ (our methods work for $\varepsilon=1$ as well), we note that larger $\varepsilon$ are both of theoretical and practical interest 
%\citep[e.g.,][]{FT21} 
since \textit{amplification via shuffling} can convert a local $\varepsilon > 1$ to a small central $\varepsilon$ \citep{erlingsson2019amplification,balle2019privacy, erlingsson2020encode}.

% have been used in the literature (e.g., Feldman and Talwar, 2021) since `amplification via shuffling' (see `Amplification by Shuffling: From Local to Central Differential Privacy via Anonymity' by Erlingsson et al., 2019) can convert a local $\varepsilon > 1$ to a small central $\varepsilon$.
% We note that the general privacy regime (i.e. ε = Ω(1)) is also of both theoretical and practical interest. For instance, when n = Ω (d), one can
% combine LDP with amplification techniques [7, 19, 20] to ensure stronger central differential privac 
%However, the are established only for $\varepsilon = O(1)$, and in many practical scenarios such as mean or frequency estimation, the accuracy under LDP suffers from a large error when $\varepsilon$ is small (see \citep{duchi2013local,duchi2018minimax, erlingsson2019amplification}). In this paper, we focus on the regime where $\varepsilon > 1$.
%When $\varepsilon = O(1)$, it is established in \cite{BS15} that one can compress the output of any LDP randomizer to a single bit without impacting its accuracy. However, it is all well established that for $\varepsilon = O(1)$, the accuracy of mean (or distribution) estimation under LDP suffers from a Thus, we focus on the regime where $\varepsilon > 1$.
In the absence of shared randomness, \cite{girgis2021a}, \cite{girgis2021b}, \cite{CKO20} provided order-optimal mechanisms for frequency and mean estimation but their mechanisms do not achieve the best known accuracy. \citet{FT21} presented an approach for compressing $\varepsilon$-LDP schemes in a lossless fashion using a pseudorandom generator (PRG). Their approach, which relies on cryptographic hardness of the PRG, can compress $\SubsetSelection$ to $O(\ln d)$ bits and $\PrivUnit$ to $O(\varepsilon+\ln d)$ bits, where $d$ is the dimension of the underlying problem. 
Their approach, similar to ours, can achieve the privacy vs accuracy tradeoffs of the best known schemes, i.e., $\SubsetSelection$ and $\PrivUnit$. Nevertheless, their approach is designed to work without shared randomness, therefore requiring more bits than necessary if shared randomness is available, as in our work. 
% \textRed{In terms of privacy vs accuracy tradeoffs, our approach is similar to \citet{FT21} as both simulate optimal LDP mechanisms.}
%Furthermore, in order to get the exact same $\varepsilon$-LDP guarantees, their results rely on strong cryptographic assumptions. 

Unlike previous work, our technique of compressing generic LDP schemes relies on Minimal Random Coding ($\MRC$), which was designed to simulate noisy channels. Several papers in information theory and related fields have studied the problem of efficiently simulating noisy channels over digital channels \cite[e.g.,][]{BS02,HJMR07,LE18} and proposed general solutions. In particular, these papers showed that any noisy channel can be simulated at a bit-rate which is close to the mutual information between the information available to the sender and the receiver. However, this result only holds if a shared source of randomness is available. Without such a source, the achievable rate has been shown to be close to Wyner's common information \citep{W75,C08}, which can be significantly larger than the mutual information \citep{XLC11}. While promising as a recipe for simulating arbitrary differentially private mechanisms, the general coding schemes discussed in these papers have not been analyzed for their effect on differential privacy guarantees. $\MRC$ \citep{HPHJ19}, which we analyze and build upon here, is one of these schemes and is also known as likelihood encoder in information theory \citep{C08,SCV16}. 

Finally, mean and frequency estimation under LDP constraints, two canonical problems in distributed learning and analytics, have been widely studied \citep{duchi2013local, nguyn2016collecting, BDFKR2018, wang2019locally, g2019vqsgd, erlingsson14rappor, BS15, kairouz16discrete, YB18, acharya2019hadamard}.
%Unfortunately, neither of these two mechanisms are communication efficient; in particular, the number of bits required to represent the privatized outcomes scale linearly in the dimension for mean estimation and in the domain size for the frequency estimation. 
%To address the communication issue, we show that using $\MRC$, we can compress these mechanisms to $\Theta(\varepsilon)$ (almost) losslessly.