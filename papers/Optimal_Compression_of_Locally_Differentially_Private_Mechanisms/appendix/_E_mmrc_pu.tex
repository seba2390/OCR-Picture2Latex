\section{Modified Minimal Random Coding Simulating \texorpdfstring{$\PrivUnit$}{PrivUnit}}
\label{appendix:mmrc_pu}
In this section, we prove Lemma \ref{lemma:mmrc_privunit_bias} (in Appendix \ref{appendix:mmrc_pu_debias}) and Theorem \ref{thm:me_mmrc_pu} (in Appendix \ref{appendix:mmrc_pu_ut}). To prove Theorem \ref{thm:me_mmrc_pu}, first, in Appendix \ref{appendix:scaling_mmrc_pu}, we show that when the number of candidates $N$ is exponential in $\varepsilon$, the scaling factor $m_{\texttt{mmrc}}$ is close to the scaling parameter associated with $\PrivUnit$ (i.e., $m_{\texttt{pu}}$). Next, in Appendix \ref{appendix:mmrc_pu_scaling_mse}, we provide the relationship between the mean squared error associated with $\MMRC$ simulating $\PrivUnit$ and the mean squared error associated with $\PrivUnit$.
Finally, in Appendix \ref{appendix:mmrc_pu_emp}, we provide some empirical comparisons in addition to the ones in Section \ref{subsec:mmrc_privunit_empirical} between $\MMRC$ simulating $\PrivUnit$ and $\PrivUnit$.

% The modified minimal random coding technique relies on $\pi$ defined in \eqref{eq:pi_mrc} and $\theta$ (more specifically $\Expectation_{\theta}[\theta]$) defined in \eqref{eq:theta}. 

\subsection{Unbiased Modified Minimal Random Coding simulating \texorpdfstring{$\PrivUnit$}{PrivUnit}}\label{appendix:mmrc_pu_debias}
% As in $\PrivUnit$ and MRC, the sampled candidate  $\svbz_K$ needs to be scaled by an appropriate factor to ensure unbiasedness of the MMRC scheme that simulates $\PrivUnit(\svbx,\gamma, p)$. As we show in the following theorem, this scaling factor is equal to
% \begin{align}
%     m_\texttt{mmrc} \coloneqq m(p_\texttt{mmrc})
% \end{align}
% where $p_\texttt{mmrc} = \Probability(\svbz_K \in \msf{Cap}_{\svbx})$ is the probability with which the sampled candidate $\svbz_K$ belongs to the spherical cap $\msf{Cap}_{\svbx})$ and $m(\cdot)$ is as defined in \eqref{eq:function_m}. In other words, $m_\texttt{mmrc}$ has the same functional form as $m$ except $p_\texttt{mmrc}$ replaces $p$. Further, in this case, we denote the final estimator as $\tilde{x}(\svbz) = \svbz_K / m_\texttt{mmrc}$.

% \begin{theorem}\label{theorem:mmrc_privunit_bias}
% Let $\qPU(\svbz | \svbx)$ be the  $\varepsilon$-LDP \emph{$\PrivUnit$} mechanism. 
% Let $\cZ \coloneqq \sphere^{d-1}$ and let $\svbz_1, \svbz_2,...,\svbz_N$ be generated from 
% $ p(\svbz) \coloneqq \msf{uniform}\lp \mcal{Z} \rp$. Let $\tilde{\pi}$ be the distribution over $[N]$ as defined in Algorithm \ref{alg:tilde_pi} corresponding to $\pi$ defined in  \eqref{eq:pi_mrc}. Let  $K \sim \tilde{\pi}(k)$. Let $m_\texttt{mmrc} \coloneqq m(p_\texttt{mmrc})$ where $p_\texttt{mmrc} = \Probability(\svbz_K \in \msf{Cap}_{\svbx})$ and let $\tilde{x}(\svbz) = \svbz_K / m_\texttt{mmrc}$. Then, $\qMMRC$ defined in \eqref{eq:q_mmrc} is such that, for any $\svbx \in \cX$, $\Expectation_{\qMMRC(\svbz | \svbx)} [\tilde{x}(\svbz)] = \svbx$.
% \end{theorem}
Consider the $\PrivUnit$ $\varepsilon$-LDP mechanism $\qPU$ described in Section \ref{sec:preliminaries} with parameters $p_0$ and $\gamma$. $\PrivUnit$ is a cap-based mechanism with $\msf{Cap}_{\svbx} = \{\svbz \in \sphere^{d-1} \mid
\lan \svbz, \svbx  \ran \ge \gamma\}$ as discussed in Appendix \ref{appendix:privunit}. Let $\piMMRC$ be the distribution and $\svbz_1, \svbz_2,...,\svbz_N$ be the candidates obtained from Algorithm \ref{alg:mmrc} when the reference distribution is $\Unif(\sphere^{d-1})$. Let  $K \sim \piMMRC(\cdot)$. Define $p_{\texttt{mmrc}} \coloneqq \Probability(\svbz_K \in \msf{Cap}_{\svbx})$
% Let $p_{\texttt{mrc}} = \Probability(\svbz_K \in \msf{Cap}_{\svbx}_{\varepsilon, d}(\svbx))$
to be the probability with which the sampled candidate $\svbz_K$ belongs to the spherical cap associated with $\PrivUnit$.
Define $m_{\texttt{mmrc}}$ as the scaling factor in \eqref{eq:m} when $p_0$ in \eqref{eq:m} is replaced by $p_{\texttt{mmrc}}$. Define $\hat{\svbx}^\texttt{mmrc} \coloneqq \svbz_K / m_\texttt{mmrc}$ as the estimator of the $\MMRC$ mechanism simulating $\PrivUnit$. 
\mmrcprivunitbias*
\begin{proof}
The proof is similar to the proof of Lemma \ref{theorem:mrc_privunit_bias}.
\end{proof}

\subsection{Utility of Modified Minimal Random Coding simulating \texorpdfstring{$\PrivUnit$}{PrivUnit}}\label{appendix:mmrc_pu_utility}

\subsubsection{The scaling factors of \texorpdfstring{$\PrivUnit$}{PrivUnit} and \texorpdfstring{$\MMRC$}{MMRC} are close when \texorpdfstring{$N$}{N} is of the right order}\label{appendix:scaling_mmrc_pu}
In the following Lemma, we show that when the number of candidates $N$ is exponential in $\varepsilon$, then the scaling parameters associated with $\PrivUnit$ and the $\MMRC$ scheme simulating $\PrivUnit$ are close.

\begin{lemma}\label{lemma:mmrc_privunit_approximation_error}
Let $N$ denote the number of candidates used in the \emph{$\MMRC$} scheme. Let $K \sim \piMMRC$ where $\piMMRC$ is the distribution over the indices $[N]$ associated the \emph{$\MMRC$} scheme simulating \emph{$\PrivUnit(\svbx,\gamma, p_0)$}. Consider any $\lambda > 0$.
Then, the scaling factor $m_{\texttt{pu}}$ associated with \emph{$\PrivUnit$} and the scaling factor $m_\texttt{mmrc}$ associated with the \emph{$\MMRC$} scheme simulating \emph{$\PrivUnit$} are such that
\begin{align}
    m_{\texttt{pu}} - m_\texttt{mmrc} \leq \lambda\cdot m_\texttt{mmrc}
\end{align}
as long as
\begin{align}
    N \geq   \frac{e^{2\varepsilon}}{2} \lp\frac{2 (1+\lambda)}{\lambda \lp p_0 -1/2 \rp}\rp^2 \ln\lp \frac{4(1+\lambda)}{\lambda \lp p_0 -1/2 \rp} \rp.
\end{align}
\end{lemma}
\begin{proof}
The proof follows a structure similar to the proof of Lemma \ref{lemma:mrc_privunit_approximation_error}. As in the proof of Lemma \ref{lemma:mrc_privunit_approximation_error}, we have
\begin{align}
    \frac{1}{m_\texttt{mmrc}} - \frac{1}{m_{\texttt{pu}}} \leq \frac{1}{m_{\texttt{pu}}}\lp\frac{ 1 }{ \dfrac{p_0 -1/2}{p_0 - p_\texttt{mmrc}}-1}\rp
    % \stackrel{(a)}{\leq} \frac{\lambda}{m} 
    \label{eq:mmrc_privunit_approximation_error_1}
\end{align}
We will now upper bound $\dfrac{p_0 - p_\texttt{mmrc}}{p_0 -1/2}$. We start by obtaining expressions for $p_\texttt{mmrc}$ and $p_0$.

To compute $p_\texttt{mmrc} \coloneqq \Pr\lbp \svbz_K \in \msf{Cap}_{\svbx} \rbp$, recall that $\theta$ denotes the fraction of candidates that belong inside the $\msf{Cap}_{\svbx}$. Let $c_1(\varepsilon, d)$ and $c_2(\varepsilon, d)$ be as defined in \eqref{eq:privunit_density}. Let $\bar{c}_1(\varepsilon, d) = c_1(\varepsilon, d) \times A(1,d)$ and $\bar{c}_2(\varepsilon, d) = c_2(\varepsilon, d) \times A(1,d)$. It is easy to see from Algorithm \ref{alg:privunit} and \eqref{eq:privunit_density} that $\Probability(\svbz_k \in \msf{Cap}_{\svbx}) =  \bar{c}_1(\varepsilon, d) / p_0$. 
Further, since $\svbz_k$ are generated uniformly at random,
$$ \theta \sim \frac{1}{N} \msf{Binom}\lp N, \frac{ \bar{c}_1(\varepsilon, d)}{p_0}\rp,$$ so we have
\begin{align}
    p_\texttt{mmrc} = \Pr\lbp \svbz_K \in \msf{Cap}_{\svbx} \rbp  & = \E\lb \Pr\lbp \svbz_K \in \msf{Cap}_{\svbx}| \theta \rbp \rb  \\
    & \stackrel{(a)}{=} \E\bigg[ \frac{\theta  \bar{c}_1(\varepsilon,d) }{ \bar{c}_2(\varepsilon,d) + \E\lb \theta \rb \lp  \bar{c}_1(\varepsilon,d) -  \bar{c}_2(\varepsilon,d) \rp} \times \Indicator \lp \theta \leq \E\lb \theta \rb \rp  \\
    & \qquad + \frac{\E \lb \theta \rb   \bar{c}_1(\varepsilon,d) + (\theta - \E \lb \theta \rb)  \bar{c}_2(\varepsilon,d)}{ \bar{c}_2(\varepsilon,d) + \E\lb \theta \rb \lp  \bar{c}_1(\varepsilon,d) -  \bar{c}_2(\varepsilon,d) \rp}  \times \Indicator \lp \theta > \E\lb \theta \rb \rp \bigg]
    % & \stackrel{(a)}{=} \E\lb \frac{ \bar{c}_1 \theta}{ \bar{c}_1 \theta +  \bar{c}_2(1-\theta)} \rb \\
    % & \stackrel{(b)}{=} \E\lb \frac{e^\varepsilon \theta}{e^\varepsilon \theta + (1-\theta)} \rb \\
    % & \stackrel{(c)}{=} \frac{e^\varepsilon}{(e^\varepsilon-1)^2}\E\lb e^\varepsilon-1-\frac{1}{\theta+\frac{1}{e^\varepsilon-1}} \rb, 
    \label{eq:mmrc_privunit_approximation_error_2}
\end{align}
where $(a)$ follows from Algorithm \ref{alg:mmrc}.

Similarly, with some simple manipulations on the definition of $p_0$, we have 
\begin{align}
p_0 
& = \frac{\E\lb \theta \rb  \bar{c}_1(\varepsilon,d)}{ \bar{c}_2(\varepsilon,d) + \E\lb \theta \rb \lp  \bar{c}_1(\varepsilon,d) -  \bar{c}_2(\varepsilon,d) \rp} \label{eq:mmrc_privunit_approximation_error_3}
\end{align}
From \eqref{eq:mmrc_privunit_approximation_error_2} and \eqref{eq:mmrc_privunit_approximation_error_3}, we have
\begin{align}
    p_0 - p_\texttt{mmrc} 
    & =  \frac{\E\lb  \bar{c}_1(\varepsilon,d) (\E[\theta] - \theta) \times \Indicator \lp \theta \leq \E\lb \theta \rb \rp +  \bar{c}_2(\varepsilon,d) (\E[\theta] - \theta) \times \Indicator \lp \theta > \E\lb \theta \rb \rp \rb}{ \bar{c}_2(\varepsilon,d) + \E\lb \theta \rb \lp  \bar{c}_1(\varepsilon,d) -  \bar{c}_2(\varepsilon,d) \rp}
     \\
    & \stackrel{(a)}{\leq} \frac{\E\lb  \bar{c}_1(\varepsilon,d) (\E[\theta] - \theta) \times \Indicator \lp \theta \leq \E\lb \theta \rb \rp \rb}{ \bar{c}_2(\varepsilon,d) + \E\lb \theta \rb \lp  \bar{c}_1(\varepsilon,d) -  \bar{c}_2(\varepsilon,d) \rp}
\end{align}
where $(a)$ follows because $(\E[\theta] - \theta) \times \Indicator \lp \theta > \E\lb \theta \rb \rp \leq 0$. Now, using the Hoeffding's inequality, we have $\Probability\lbp \lba \theta - \E[\theta] \rba \geq \sqrt{\frac{\ln\lp 2/\beta \rp}{2N}} \rbp \leq \beta$. Conditioned on the event $\lbp \lba \theta - \E[\theta] \rba \leq \sqrt{\frac{\ln\lp 2/\beta \rp}{2N}} \rbp$ and using the fact that $\lba p_0 - p_\texttt{mmrc}  \rba\leq 1$, we have
\begin{align}
    p_0 - p_\texttt{mmrc} & \leq \frac{ \bar{c}_1(\varepsilon,d) \sqrt{\frac{\ln(2/\beta)}{2N}}}{ \bar{c}_2(\varepsilon,d) + \E\lb \theta \rb \lp  \bar{c}_1(\varepsilon,d) -  \bar{c}_2(\varepsilon,d) \rp}  + \beta \\
    & \stackrel{(a)}{\leq}  \frac{ \bar{c}_1(\varepsilon,d)}{ \bar{c}_2(\varepsilon,d)} \sqrt{\frac{\ln(2/\beta)}{2N}} + \beta
    \stackrel{(b)}{\leq} \exp(\varepsilon) \sqrt{\frac{\ln(2/\beta)}{2N}} + \beta \stackrel{(c)}{\leq} \frac{\lambda(p_0 - 1/2)}{1+\lambda}. \label{eq:mmrc_privunit_approximation_error_6}
\end{align}
where $(a)$ follows because $\E\lb \theta \rb \geq 0$, $(b)$ follows because $ \bar{c}_1(\varepsilon,d)/ \bar{c}_2(\varepsilon,d) \leq e^{\varepsilon}$, and $(c)$ follows if we pick 
\begin{align*}
    \beta & \leq \frac{\lambda(p_0 - 1/2)}{2(1+\lambda)},\\
    N & \geq\frac{\exp(2\varepsilon) \ln(2/\beta)}{2\lp \frac{\lambda(p_0 - 1/2)}{1+\lambda} - \beta \rp^2}
    = \frac{\exp(2\varepsilon)}{2}\lp\frac{2 (1+\lambda)}{\lambda \lp p_0 -1/2 \rp}\rp^2 \ln\lp \frac{4(1+\lambda)}{\lambda \lp p_0 -1/2 \rp} \rp.
\end{align*}
The rest of the proof is similar to the proof of Lemma \ref{lemma:mrc_privunit_approximation_error}.
\end{proof}

\subsubsection{Relationship between the mean squared errors associated with \texorpdfstring{$\PrivUnit$}{PrivUnit} and \texorpdfstring{$\MMRC$}{MMRC} simulating \texorpdfstring{$\PrivUnit$}{PrivUnit}}\label{appendix:mmrc_pu_scaling_mse}
In the following Proposition,
we show that if the scaling factor $m_{\texttt{mmrc}}$ is close to the scaling parameter $m_{\texttt{pu}}$, then the mean squared error associated with $\MMRC$ simulating $\PrivUnit$ (i.e., $\Expectation_{\qMMRC} \big[ \lV  \hat{\svbx}^\texttt{mmrc} - \svbx \rV^2_2  \big]$) is close to the mean squared error associated with $\PrivUnit$ (i.e., $\Expectation_{\qPU} \big[ \lV  \hat{\svbx}^\texttt{pu} - \svbx \rV^2_2  \big]$).
\begin{proposition}\label{proposition:mmrc_mse_wrt_privunit}
Let $\qPU(\svbz | \svbx)$ be the  $\varepsilon$-LDP \emph{$\PrivUnit$} mechanism with parameters $p_0$ and $\gamma$ and estimator $\hat{\svbx}^{\texttt{pu}}$. Let $\qMMRC(\svbz|\svbx)$ denote the \emph{$\MMRC$} privatization mechanism simulating \emph{$\PrivUnit$} with $N$ candidates and estimator $\hat{\svbx}^{\texttt{mmrc}}$.
Let $m_{\texttt{pu}}$ denote the scaling factor  associated with \emph{$\PrivUnit$} and $m_\texttt{mmrc}$ denote the scaling factor  associated with the \emph{$\MMRC$} scheme simulating \emph{$\PrivUnit$}. Consider any $\lambda > 0$. If $m_{\texttt{pu}} - m_\texttt{mmrc} \leq \lambda \cdot m_\texttt{mmrc}$, then 
  \begin{align}
    \Expectation_{\qMMRC} \big[ \lV  \hat{\svbx}^\texttt{mmrc} - \svbx \rV^2_2  \big]  \leq  \lp 1+\lambda \rp^2  \Expectation_{\qPU}\big[\|\hat{\svbx}^{\texttt{pu}} - \svbx\|^2\big] + 2(1+\lambda)(2+\lambda) \sqrt{\Expectation_{\qPU}\big[\|\hat{\svbx}^{\texttt{pu}} - \svbx\|^2\big]} + (2+\lambda)^2.
\end{align}
\end{proposition}
\begin{proof}
The proof is similar to the proof of Proposition \ref{proposition:mrc_mse_wrt_privunit}.
\end{proof}

% \subsubsection{Simulating \texorpdfstring{$\PrivUnit$}{PrivUnit} using Modified Minimal Random Coding}\label{appendix:mmrc_pu_ut}
In the following Lemma, we show that with on the order of $\varepsilon$-bits of communication, the mean squared error associated with $\MMRC$ simulating $\PrivUnit$ (i.e., $\Expectation_{\qMMRC} \big[ \lV  \hat{\svbx}^\texttt{mmrc} - \svbx \rV^2_2  \big]$) is close to the mean squared error associated with $\PrivUnit$ (i.e., $\Expectation_{\qPU} \big[ \lV  \hat{\svbx}^\texttt{pu} - \svbx \rV^2_2  \big]$).
% Now, we provide a proof of Lemma \ref{theorem:mmrc_accuracy_privunit}.
\begin{restatable}{lemma}{mmrcprivunit}\label{theorem:mmrc_accuracy_privunit}
Let $\qPU(\svbz | \svbx)$ be the  $\varepsilon$-LDP \emph{$\PrivUnit$} mechanism with parameters $p_0$ and $\gamma$ and estimator $\hat{\svbx}^{\texttt{pu}}$. Let $\qMMRC(\svbz|\svbx)$ denote the \emph{$\MMRC$} privatization mechanism simulating \emph{$\PrivUnit$} with $N$ candidates and estimator $\hat{\svbx}^{\texttt{mmrc}}$ as defined above. 
Consider any $\lambda > 0$. Then,
\begin{align}
  \Expectation_{\qMMRC} \big[ \lV  \hat{\svbx}^\texttt{mmrc} - \svbx \rV^2_2  \big] 
     \leq  \lp 1+\lambda \rp^2  \Expectation_{\qPU}\big[\|\hat{\svbx}^{\texttt{pu}} - \svbx\|^2\big]  + 2(1+\lambda)(2+\lambda) \sqrt{\Expectation_{\qPU}\big[\|\hat{\svbx}^{\texttt{pu}} - \svbx\|^2\big]} + (2+\lambda)^2
\end{align}
as long as 
\begin{align}
N \geq   \frac{e^{2\varepsilon}}{2}\lp\frac{2 (1+\lambda)}{\lambda \lp p_0 -1/2 \rp}\rp^2 \ln\lp \frac{4(1+\lambda)}{\lambda \lp p_0 -1/2 \rp} \rp.
\end{align}
\end{restatable}
\begin{proof}
The proof follows from Proposition \ref{proposition:mmrc_mse_wrt_privunit} and Lemma \ref{lemma:mmrc_privunit_approximation_error}.
\end{proof}

\subsubsection{Simulating \texorpdfstring{$\PrivUnit$}{PrivUnit} using Modified Minimal Random Coding}\label{appendix:mmrc_pu_ut}
Now, we provide a proof of Theorem \ref{thm:me_mmrc_pu}.
\mmrcpu*
\begin{proof}
The proof follows directly from Lemma~\ref{theorem:mmrc_accuracy_privunit} since for all $i \in [n]$, $\hat{\svbx}^{\texttt{mmrc}}_i$ are independent of each other as well as unbiased.
\end{proof}

\subsection{Additional Empirical Comparisons}\label{appendix:mmrc_pu_emp}
In Section \ref{subsec:mmrc_privunit_empirical}, we empirically demonstrated the privacy-accuracy-communication tradeoffs of $\MMRC$ simulating $\PrivUnit$ against $\PrivUnit$ and SQKR in terms of $\ell_2$ error vs $\#$bits and $\ell_2$ error vs $\varepsilon$ (see Figure \ref{fig:mean}). In this section, we provide comparisons between these methods in terms of $\ell_2$ error vs $d$ (see Figure \ref{fig:mean_app} (left)) and $\ell_2$ error vs $n$ (see Figure \ref{fig:mean_app} (right)) for a fixed $\varepsilon$ (=6) and a fixed $\#$bits (=11). As before, SQKR uses $\#$bits $= \varepsilon$ for both because it leads to a poor performance if $\#$bits $ > \varepsilon$.
\begin{figure}[h]
\centering
\includegraphics[width=0.45\linewidth]{figures/d6.pdf} \qquad \includegraphics[width=0.45\linewidth]{figures/n6.pdf}%
\caption{Comparing $\PrivUnit$, $\MMRC$ simulating $\PrivUnit$ and SQKR for mean estimation with $\varepsilon=6$ and $\#$bits $=11$. \textbf{Left:} $\ell_2$ error vs $d$ for $n = 5000$. \textbf{Right:} $\ell_2$ error vs $n$ for $d = 500$.}
\label{fig:mean_app}
\end{figure}