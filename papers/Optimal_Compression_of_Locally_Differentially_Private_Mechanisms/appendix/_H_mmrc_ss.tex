\section{Modified Minimal Random Coding Simulating \texorpdfstring{$\SubsetSelection$}{Subset Selection}}
\label{sec:mmrc_ss_app}

In this section, we prove Lemma \ref{lemma:mmrc_ss_bias} (in Appendix \ref{appendix:mmrc_ss_debias}) and Theorem \ref{thm:fe_mmrc_ss} (in Appendix \ref{appendix:mmrc_ss_utility}). To prove Theorem \ref{thm:fe_mmrc_ss}, first, in Appendix \ref{appendix:scaling_mmrc_ss}, we show that when the number of candidates $N$ is exponential in $\varepsilon$, the scaling factor $m_{\texttt{mmrc}}$ is close to the scaling parameter associated with $\SubsetSelection$ (i.e., $m_{\texttt{ss}}$). Next, in Appendix \ref{appendix:mmrc_ss_scaling_mse}, we provide the relationship between the mean squared error associated with $\MMRC$ simulating $\SubsetSelection$ and the mean squared error associated with $\SubsetSelection$. Finally, in Appendix \ref{appendix:mmrc_ss_emp}, we provide some empirical comparisons in addition to the ones in Section \ref{subsec:mmrc_ss_empirical} between $\MMRC$ simulating $\SubsetSelection$ and $\SubsetSelection$.

\subsection{Unbiased Modified Minimal Random Coding simulating \texorpdfstring{$\SubsetSelection$}{Subset Selection}}\label{appendix:mmrc_ss_debias}
Consider the $\SubsetSelection$ $\varepsilon$-LDP mechanism $\qSS$ described in Section~\ref{sec:preliminaries} with $s \coloneqq \lceil \frac{d}{1+e^\varepsilon}\rceil$. $\SubsetSelection$ is cap-based mechanism as discussed in Section \ref{sec:main_results} and Appendix \ref{appendix:ss} with $\msf{Cap}_{\svbx} = \cZ_{\svbx}$ and $\Probability_{\svbz \sim \Unif(\cZ)}\lp \svbz \in \msf{Cap}_{\svbx} \rp = s/d$.
Let $\piMMRC$ be the distribution and $\svbz_1, \svbz_2,...,\svbz_N$ be the candidates obtained from Algorithm \ref{alg:mmrc} when the reference distribution is $\Unif(\cZ)$ where $\cZ$ is as defined in \eqref{eq:z_ss}.
Let $\theta$ denote the fraction of candidates inside $\msf{Cap}_{\svbx} = \cZ_{\svbx}$ where $\cZ_{\svbx}$ is the set of elements in $\cZ$ with $1$ in the same location as $\svbx$. 
It is easy to see that $\theta \sim \frac{1}{N}\msf{Binom}\lp N, \frac{s}{d} \rp$.
% $\SubsetSelection$ is a cap-based mechanism with $\msf{Cap}_{\svbx} = \cZ_{\svbx}$ as discussed in Appendix \ref{appendix:ss} where $\cZ_{\svbx}$ is the set of elements in $\cZ$ with $1$ in the same location as $\svbx$. 
Let $\qMMRC_i = \Probability(z_i = 1)$ where $\svbz \sim \qMMRC(\cdot|\svbx)$ i.e., $\qMMRC_i = \Probability\lbp(\svbz_K)_i = 1\rbp$ where $K \sim \piMMRC(\cdot)$. 

\begin{restatable}{lemma}{mrmcssbias}\label{thm:mmrc_ss_bias_0}
Let $K \sim \piMMRC(\cdot)$ and $\qMMRC_i = \Probability\lbp(\svbz_K)_i = 1\rbp$ for $i \in [d]$. Then, 
\begin{align}
    \qMMRC_i = p_i  m_\texttt{mmrc} + b_\texttt{mmrc}
\end{align}
where
\begin{align}
m_\texttt{mmrc} &\coloneqq \frac{d}{d-1} \E\lb \frac{e^\varepsilon\theta}{e^\varepsilon \E\lb\theta\rb + (1-\E \lb\theta\rb)}  \cdot \Indicator \lp\theta\leq \E\lb\theta\rb \rp  + \frac{e^\varepsilon \E \lb\theta\rb +\theta- \E \lb\theta\rb}{e^\varepsilon \E\lb\theta\rb + (1-\E \lb\theta\rb)}  \cdot \Indicator \lp\theta> \E\lb\theta\rb \rp \rb -\frac{s}{d-1} \label{eq:m_mmrc_ss}\\
b_\texttt{mmrc} &\coloneqq \frac{1}{d-1}\lp s - \E\lb \frac{e^\varepsilon\theta}{e^\varepsilon \E\lb\theta\rb + (1-\E \lb\theta\rb)}  \cdot \Indicator \lp\theta\leq \E\lb\theta\rb \rp  + \frac{e^\varepsilon \E \lb\theta\rb +\theta- \E \lb\theta\rb}{e^\varepsilon \E\lb\theta\rb + (1-\E \lb\theta\rb)}  \cdot \Indicator \lp\theta> \E\lb\theta\rb \rp \rb \rp. \label{eq:b_mmrc_ss}
\end{align}
\end{restatable}
\begin{proof}
Following the proof of Lemma ~\ref{thm:mrc_ss_bias}, we compute 
$\Pr\lbp (\svbz_{K})_i = 1 | \svbx = i \rbp $ and $\Pr\lbp (\svbz_{K})_i = 1 | \svbx = j  \rbp $ separately.

To compute $\Pr\lbp (\svbz_K)_i = 1 \mv \svbx = i\rbp$, recall that $\theta$ denotes the fraction of candidates that belong inside the $\msf{Cap}_{\svbx}$ i.e., have $1$ in the same location as $\svbx$. From Appendix \ref{appendix:ss_cap}, recall that $c_1(\varepsilon,d) \coloneqq \dfrac{e^\varepsilon}{\binom{d-1}{s-1}e^\varepsilon + \binom{d-1}{s}}$, $c_2(\varepsilon,d) \coloneqq \dfrac{1}{\binom{d-1}{s-1}e^\varepsilon + \binom{d-1}{s}}$. Further, since $\svbz_k$ are generated uniformly at random,
$$ \theta \sim \frac{1}{N} \msf{Binom}\lp N, \frac{\binom{d-1}{s-1}}{\binom{d}{s}} \rp = \frac{1}{N} \msf{Binom}\lp N, \frac{s}{d} \rp,$$ so we have
\begin{align}
     & \Pr\lbp (\svbz_K)_i = 1 | \svbx = i \rbp \\
     &\qquad  = \Pr\lbp \svbz_K \in \msf{Cap}_{\svbx} | \svbx = i \rbp  \stackrel{(a)}{=} \E\lb \Pr\lbp \svbz_K  \in \msf{Cap}_{\svbx} |\svbx=i, \theta \rbp\rb \\
   % & = \E_{n\sim \msf{Binom}\lp N, \frac{t}{d} \rp}\lb \frac{n c_1}{nc_1+(N-n)c_2} \rb \\ 
    &\qquad \stackrel{(b)}{=} \E\lb \frac{e^\varepsilon \theta}{e^\varepsilon \E\lb \theta \rb + (1-\E \lb \theta \rb)}  \cdot \Indicator \lp \theta \leq \E\lb \theta \rb \rp  + \frac{e^\varepsilon \E \lb \theta \rb + \theta - \E \lb \theta \rb}{e^\varepsilon \E\lb \theta \rb + (1-\E \lb \theta \rb)}  \cdot \Indicator \lp \theta > \E\lb \theta \rb \rp \rb  \label{eq:mmrc_ss_step_1}
\end{align}
where $(a)$ follows by the law of total probability and $(b)$ is due to Algorithm \ref{alg:mmrc} and $c_1(\varepsilon,d)/c_2(\varepsilon,d) = e^\varepsilon$.

%%%%%%%%

To compute $\Pr\lbp (\svbz_K)_i = 1 \mv \svbx = j \rbp$, we decompose it into
\begin{align}
    \Pr\lbp (\svbz_K)_i = 1 \mv \svbx = j \rbp = \Pr\lbp (\svbz_K)_i = 1, (\svbz_K)_j = 1 \mv \svbx = j\rbp + \Pr\lbp (\svbz_K)_i = 1, (\svbz_K)_j = 0 \mv \svbx = j\rbp, \label{eq:mmrc_Step0}
\end{align}
for any $j \neq i$ and calculate each of the terms separately.

As before, let $\theta$ denotes the fraction of candidates that belong inside the $\msf{Cap}_{\svbx}$ i.e., have $1$ in the same location as $\svbx$. Further, let $\bar{\theta}$ denotes the fraction of candidates that belong inside the $\msf{Cap}_{\svbx}$ i.e., have $1$ in the same location as $\svbx$ as well as have $1$ in the $j^{th}$ location. Since $\svbz_k$ are generated uniformly at random,
$$ \bar{\theta} \sim \frac{1}{N} \msf{Binom}\lp N\theta, \frac{\binom{d-2}{s-2}}{\binom{d-1}{s-1}} \rp = \frac{1}{N} \msf{Binom}\lp N\theta, \frac{s-1}{d-1} \rp,$$ so we have
\begin{align}
    &\Pr\lbp (\svbz_K)_i = 1, (\svbz_K)_j = 1 | \svbx = j \rbp 
    \stackrel{(a)}{=} \E_{\theta}\lb \E_{\bar{\theta}}\lb \Pr\lbp (\svbz_K)_i = 1, (\svbz_K)_j = 1 \mv \svbx = j, \bar{\theta}, \theta \rbp \rb\rb \\
    & \stackrel{(b)}{=} \E_{\theta}\lb \E_{\bar{\theta}}\lb \frac{e^\varepsilon}{e^\varepsilon \E\lb \theta \rb + (1-\E \lb \theta \rb)} \times \bar{\theta} \rb \Indicator \lp \theta \leq \E\lb \theta \rb \rp
    +\E_{\bar{\theta}}\lb\frac{e^\varepsilon \E \lb \theta \rb + \theta - \E \lb \theta \rb}{\theta\lp e^\varepsilon \E\lb \theta \rb + (1-\E \lb \theta \rb)\rp }\times \bar{\theta}\rb \Indicator \lp \theta > \E\lb \theta \rb \rp\rb \\ 
    & \stackrel{(c)}{=} \frac{s-1}{d-1}\E\lb \frac{e^\varepsilon \theta}{e^\varepsilon \E\lb \theta \rb + (1-\E \lb \theta \rb)}  \cdot \Indicator \lp \theta \leq \E\lb \theta \rb \rp  + \frac{e^\varepsilon \E \lb \theta \rb + \theta - \E \lb \theta \rb}{e^\varepsilon \E\lb \theta \rb + (1-\E \lb \theta \rb)}  \cdot \Indicator \lp \theta > \E\lb \theta \rb \rp \rb. \label{eq:mmrc_Step1}
\end{align}
where $(a)$ follows by the law of total probability, $(b)$ follows 
from Algorithm \ref{alg:mmrc}, and $(c)$ is because $\E[\bar{\theta}] = \frac{s-1}{d-1} \times \theta$.

Similarly, to compute the term $\Pr\lbp (\svbz_K)_i = 1, (\svbz_K)_j = 0 | \svbx = j \rbp$, let $\bar{\theta}$ denote the fraction of candidates that belong inside the $\msf{Cap}_{\svbx}$ i.e., have $1$ in the same location as $\svbx$ as well as have $0$ in the $j^{th}$ location. Since $\svbz_k$ are generated uniformly at random,
$$ \bar{\theta} \sim \frac{1}{N} \msf{Binom}\lp N(1-\theta), \frac{\binom{d-2}{s-1}}{\binom{d-1}{s}} \rp = \frac{1}{N} \msf{Binom}\lp N(1-\theta), \frac{s}{d-1} \rp,$$ so we have
\begin{align}
    & \Pr\lbp (\svbz_K)_i = 1, (\svbz_K)_j = 0 | \svbx = j \rbp 
    \stackrel{(a)}{=} \E_{\theta}\lb \E_{\bar{\theta}}\lb \Pr\lbp (\svbz_K)_i = 1, (\svbz_K)_j = 0 \mv \svbx = j, \bar{\theta}, \theta \rbp \rb\rb \\
    & \stackrel{(b)}{=} \E_{\theta}\lb \E_{\bar{\theta}}\lb \frac{\bar{\theta}}{e^\varepsilon \E\lb \theta \rb + (1-\E\lb \theta \rb)} \cdot \Indicator \lp \theta > \E\lb \theta \rb \rp  + \frac{(1-\E\lb \theta \rb) + \lp \E \lb \theta \rb - \theta \rp e^{\varepsilon}}{(1-\theta) (e^\varepsilon \E\lb \theta \rb + (1-\E\lb \theta \rb))} \bar{\theta} \cdot \Indicator \lp \theta \leq \E\lb \theta \rb \rp  \rb \rb\\ 
    & \stackrel{(c)}{=} \frac{s}{d-1}\E\lb \frac{(1-\theta)}{e^\varepsilon \E\lb \theta \rb + (1-\E\lb \theta \rb)} \cdot \Indicator \lp \theta > \E\lb \theta \rb \rp  + \frac{(1-\E\lb \theta \rb) + \lp \E \lb \theta \rb - \theta \rp e^{\varepsilon}}{e^\varepsilon \E\lb \theta \rb + (1-\E\lb \theta \rb)} \cdot \Indicator \lp \theta \leq \E\lb \theta \rb \rp  \rb. \label{eq:mmrc_Step2}
\end{align}
where $(a)$ follows by the law of total probability, $(b)$ follows 
from Algorithm \ref{alg:mmrc}, and $(c)$ is because $\E[\bar{\theta}] = \frac{s}{d-1} \times \theta$. Using \eqref{eq:mmrc_Step1} and \eqref{eq:mmrc_Step2} in \eqref{eq:mmrc_Step0}, we have
\begin{align}
    & \Pr\lbp (\svbz_K)_i = 1 | \svbx = j \rbp \\
    & \qquad = \Pr\lbp (\svbz_K)_i = 1, (\svbz_K)_j = 1 | \svbx = j \rbp+\Pr\lbp (\svbz_K)_i = 1, (\svbz_K)_j = 0 | \svbx = j \rbp \\
    &\qquad  = \frac{1}{d-1}\lp s - \E\lb \frac{e^\varepsilon \theta}{e^\varepsilon \E\lb \theta \rb + (1-\E \lb \theta \rb)}  \cdot \Indicator \lp \theta \leq \E\lb \theta \rb \rp  + \frac{e^\varepsilon \E \lb \theta \rb + \theta - \E \lb \theta \rb}{e^\varepsilon \E\lb \theta \rb + (1-\E \lb \theta \rb)}  \cdot \Indicator \lp \theta > \E\lb \theta \rb \rp \rb \rp.\label{eq:mmrc_ss_step_2}
\end{align}
% Since $  \tilde{q}_i = p_i \cdot \Pr\lbp (\svbz_{K})_i = 1 | \svbx = i \rbp + (1-p_i)\cdot \Pr\lbp (\svbz_{K})_i = 1 | \svbx = j \rbp$, we arrive at \eqref{eq:tilde_mb_def}.
%%%%%%%%%%%5
Combining everything, we have
\begin{align}
    \qMMRC_i  & =   \Probability\lbp(\svbz_K)_i  =   1\rbp \\
    & =   p_i \times \lb \Pr\lbp (\svbz_K)_i   =   1 \mv \svbx = i\rbp - \Pr\lbp (\svbz_K)_i   =   1 \mv \svbx = j\rbp \rb + \Pr\lbp (\svbz_K)_i   =   1 \mv \svbx = j\rbp  \\
    & \stackrel{(a)}{=}   p_i  m_\texttt{mmrc} + b_\texttt{mmrc}
\end{align}
where $(a)$ follows from \eqref{eq:mmrc_ss_step_1} and \eqref{eq:mmrc_ss_step_2}, and the definitions of $m_\texttt{mmrc}$ and $b_\texttt{mmrc}$. 
\end{proof}

\mmrcssbias*
\begin{proof}
Given Lemma \ref{thm:mmrc_ss_bias_0}, the proof follows from the proof of Lemma \ref{thm:mrc_ss_bias}.
\end{proof}

 \subsection{Utility of Modified Minimal Random Coding simulating \texorpdfstring{$\SubsetSelection$}{Subset Selection}}\label{appendix:mmrc_ss_ut}
 
 \subsubsection{The scaling factors of \texorpdfstring{$\SubsetSelection$}{Subset Selection} and \texorpdfstring{$\MMRC$}{MMRC} are close when \texorpdfstring{$N$}{N} is of the right order}\label{appendix:scaling_mmrc_ss}
 In the following Lemma, we show that when the number of candidates $N$ is exponential in $\varepsilon$, then the scaling parameters associated with $\SubsetSelection$ and the $\MMRC$ scheme simulating $\SubsetSelection$ are close.
 
\begin{lemma}\label{lemma:MMRC_SS_N}
Let $N$ denote the number of candidates used in the \emph{$\MMRC$} scheme. Let $K \sim \piMMRC$ where $\piMMRC$ is the distribution over the indices $[N]$ associated the \emph{$\MMRC$} scheme simulating \emph{$\SubsetSelection$}. Consider any $\lambda > 0$.
Then, the scaling factors $m_{\texttt{ss}}$ and $b_{\texttt{ss}}$ associated with \emph{$\SubsetSelection$} and the scaling factors $m_\texttt{mmrc}$ and $b_{\texttt{mmrc}}$ associated with the \emph{$\MMRC$} scheme simulating \emph{$\SubsetSelection$} are such that
\begin{align}
    m_{\texttt{ss}} - m_\texttt{mmrc} \leq \lambda\cdot m_\texttt{mmrc}
\end{align}
and $b_{\texttt{ss}} \leq b_\texttt{mmrc}$
% \begin{align}
%     b_{\texttt{ss}} \leq b_\texttt{mrc} 
% \end{align}
as long as
\begin{align}
    N\geq \frac{2(e^{\varepsilon}+1)^2(1+\lambda)^2}{0.24^2\lambda^2}\ln\lp \frac{8(1+\lambda)}{0.24\lambda}\rp.
\end{align}
\end{lemma}
\begin{proof}
The proof is similar to the proof of Lemma \ref{lemma:mrc_ss_approximation_error}. We only show the key steps here.

From \eqref{eq:b_mmrc_ss} and \eqref{eq:check_mb_def_2}, we have
\begin{align}
    b_\texttt{mmrc} - b_{\texttt{ss}} & =  \frac{1}{d-1} \cdot \frac{1}{e^\varepsilon \E [\theta] +(1-\E [\theta] )} \cdot \E\lb e^\varepsilon (\E [\theta]  - \theta) \cdot \Indicator \lp \theta \leq \E\lb \theta \rb \rp + (\E [\theta]  - \theta) \cdot \Indicator \lp \theta > \E\lb \theta \rb \rp \rb \\
    & \stackrel{(a)}{\geq} \frac{1}{d-1} \cdot \frac{1}{e^\varepsilon \E [\theta] +(1-\E [\theta] )} \cdot \E\lb  (\E [\theta]  - \theta) \cdot \Indicator \lp \theta \leq \E\lb \theta \rb \rp + (\E [\theta]  - \theta) \cdot \Indicator \lp \theta > \E\lb \theta \rb \rp \rb \\
    & = \frac{1}{d-1} \cdot \frac{1}{e^\varepsilon \E [\theta] +(1-\E [\theta] )} \cdot \E\lb (\E [\theta]  - \theta) \rb = 0.
\end{align}
where $(a)$ follows because $e^\varepsilon \geq 1$.
% On the other hand, from the proof of Lemma~\ref{lemma:SS_N}, we know that $m \geq \frac{1}{8}$ when $\varepsilon \geq \frac{1}{24}$ (here we only focus on $\varepsilon = \Omega(1)$)
From \eqref{eq:m_mmrc_ss} and \eqref{eq:check_mb_def_1}, we have
\begin{align}
    m_{\texttt{ss}} - m_\texttt{mmrc} & = \frac{d}{d-1} \cdot \frac{1}{e^\varepsilon \E [\theta] +(1-\E [\theta] )} \cdot \E\lb e^\varepsilon (\E [\theta]  - \theta) \cdot \Indicator \lp \theta \leq \E\lb \theta \rb \rp + (\E [\theta]  - \theta) \cdot \Indicator \lp \theta > \E\lb \theta \rb \rp \rb \\
    & \leq \frac{d}{d-1} \cdot \frac{1}{e^\varepsilon \E [\theta] +(1-\E [\theta] )} \cdot \E\lb e^\varepsilon (\E [\theta]  - \theta) \cdot \Indicator \lp \theta \leq \E\lb \theta \rb \rp \rb\\
    & \stackrel{(a)}{\leq}  \frac{2}{e^\varepsilon \E [\theta] +(1-\E [\theta] )} \cdot \E\lb e^\varepsilon (\E [\theta]  - \theta) \cdot \Indicator \lp \theta \leq \E\lb \theta \rb \rp \rb \label{eq:ss_mmrc_minus_m1}
    % & \leq \frac{2}{e^\varepsilon \E [\theta] +(1-\E [\theta] )} \cdot e^{\varepsilon} \cdot \sqrt{\frac{\log(1/\delta)}{2N}} + 2 \delta \\
    % & \leq 2e^{\varepsilon} \cdot \sqrt{\frac{\log(1/\delta)}{2N}} + 2 \delta 
    % & \leq \frac{\lambda}{8(1+\lambda)}\leq \frac{\lambda}{1+\lambda}m\label{eq:tilde_m_diff},
\end{align}
where $(a)$ holds since $d\geq 2$. Next, we condition on the event $\mcal{E} \coloneqq \lbp \lba \E[\theta]-\theta\rba \leq \sqrt{\frac{\ln(2/\beta)}{2N}} \rbp$, which has probability $\Pr_\theta\lbp \mcal{E} \rbp \geq 1-\beta$ by Hoeffding's inequality. We continue to upper bound \eqref{eq:ss_mmrc_minus_m1}:
\begin{align}
m_{\texttt{ss}} - m_\texttt{mmrc}
&= 2 \bigg( \Pr\lbp \mcal{E} \rbp\E\lb \frac{ e^\varepsilon (\E [\theta]  - \theta) \cdot \Indicator \lp \theta \leq \E\lb \theta \rb \rp }{e^\varepsilon \E [\theta] +(1-\E [\theta] )} \mv \mcal{E} \rb \\
& +\Pr\lbp \mcal{E}^c \rbp\E\lb \frac{ e^\varepsilon (\E [\theta]  - \theta) \cdot \Indicator \lp \theta \leq \E\lb \theta \rb \rp }{e^\varepsilon \E [\theta] +(1-\E [\theta] )} \mv \mcal{E}^c \rb \bigg)\\
& \stackrel{(a)}{\leq} 2 \lp \E\lb \frac{ e^\varepsilon (\E [\theta]  - \theta) \cdot \Indicator \lp \theta \leq \E\lb \theta \rb \rp }{e^\varepsilon \E [\theta] +(1-\E [\theta] )} \mv \mcal{E} \rb +\beta \rp\\
& \stackrel{(b)}{\leq} (1+e^\varepsilon) \sqrt{\frac{\ln(2/\beta)}{2N}}  + 2\beta
\end{align}
where $(a)$ holds since 
 $$ \frac{ e^\varepsilon (\E [\theta]  - \theta) \cdot \Indicator \lp \theta \leq \E\lb \theta \rb \rp }{e^\varepsilon \E [\theta] +(1-\E [\theta] )} = \frac{ e^\varepsilon \E [\theta]  \cdot \Indicator \lp \theta \leq \E\lb \theta \rb \rp }{e^\varepsilon \E [\theta] +(1-\E [\theta] )} -  \frac{ e^\varepsilon \theta \cdot \Indicator \lp \theta \leq \E\lb \theta \rb \rp }{e^\varepsilon \E [\theta] +(1-\E [\theta] )} \leq 1,$$ and $(b)$ holds since $\E[\theta] = s/d \geq 1/(1+e^\varepsilon)$. 

The rest of the proof is similar to the proof of Lemma \ref{lemma:mrc_ss_approximation_error}.
% as long as we pick
% \begin{align*}
%     & \delta = \frac{\lambda}{32(1+\lambda)}\\
%     & N \geq \frac{2e^{2\varepsilon}\log(1/\delta)}{\lp\lambda/(8+8\lambda) - 2\delta\rp^2} = \frac{512e^{2\varepsilon}(1+\lambda)^2\log\lp\frac{32+32\lambda}{\lambda}\rp}{\lambda^2}.
% \end{align*}
% Rearranging \eqref{eq:tilde_m_diff}, we conclude that $m - \tilde{m} \leq \lambda \tilde{m}$ as long as $N \geq \frac{512e^{2\varepsilon}(1+\lambda)^2\log\lp\frac{32+32\lambda}{\lambda}\rp}{\lambda^2}$.
\end{proof}

\subsubsection{Relationship between the mean squared errors associated with \texorpdfstring{$\SubsetSelection$}{Subset Selection} and \texorpdfstring{$\MMRC$}{MMRC} simulating \texorpdfstring{$\SubsetSelection$}{Subset Selection}}\label{appendix:mmrc_ss_scaling_mse}
In the following Proposition,
we show that if $m_{\texttt{mmrc}}$ is close to $m_{\texttt{ss}}$ and $b_{\texttt{mmrc}} \geq b_{\texttt{ss}}$, then the mean squared error associated with $\MMRC$ simulating $\SubsetSelection$ (i.e., $\Expectation_{\qMMRC} \big[ \lV  \hat{\svbx}^\texttt{mmrc} - \svbx \rV^2_2  \big]$) is close to the mean squared error associated with $\SubsetSelection$ (i.e., $\Expectation_{\qSS} \big[ \lV  \hat{\svbx}^\texttt{ss} - \svbx \rV^2_2  \big]$).
\begin{proposition}\label{proposition:mmrc_mse_wrt_ss}
Let $\qSS(\svbz | \svbx)$ be the  $\varepsilon$-LDP \emph{$\SubsetSelection$} mechanism with estimator $\hat{\svbx}^{\texttt{ss}}$. Let $\qMMRC(\svbz|\svbx)$ denote the \emph{$\MMRC$} privatization mechanism simulating \emph{$\SubsetSelection$} with $N$ candidates and estimator $\hat{\svbx}^{\texttt{mmrc}}$.
Let $m_{\texttt{ss}}$ and $b_{\texttt{ss}}$ denote the scaling factors  associated with \emph{$\SubsetSelection$} and $m_\texttt{mmrc}$ and $b_\texttt{mmrc}$ denote the scaling factors associated with the \emph{$\MMRC$} scheme simulating \emph{$\SubsetSelection$}. Consider any $\lambda > 0$. If $m_{\texttt{pu}} - m_\texttt{mmrc} \leq \lambda \cdot m_\texttt{mmrc}$ and $b_{\texttt{mmrc}} \geq b_{\texttt{ss}}$, then 
% $m - m_\texttt{mrc} \leq \lambda\cdotm_\texttt{mrc}$ and $b_\texttt{mrc} \geq b$
  \begin{align}
    \Expectation_{\qMMRC} \big[ \lV  \hat{\svbx}^\texttt{mmrc} - \svbx \rV^2_2  \big]  \leq  \lp 1+4\lambda +5\lambda^2 + 2\lambda^3 \rp  \Expectation_{\qSS}\big[\|\hat{\svbx}^{\texttt{ss}} - \svbx\|^2\big]
\end{align}
\end{proposition}
\begin{proof}
The proof is similar to the proof of Proposition \ref{proposition:mrc_mse_wrt_ss}.
\end{proof}

In the following Lemma, we show that with on the order of $\varepsilon$-bits of communication, the mean squared error associated with $\MMRC$ simulating $\SubsetSelection$ (i.e., $\Expectation_{\qMMRC} \big[ \lV  \hat{\svbx}^\texttt{mmrc} - \svbx \rV^2_2  \big]$) is close to the mean squared error associated with $\SubsetSelection$ (i.e., $\Expectation_{\qSS} \big[ \lV  \hat{\svbx}^\texttt{ss} - \svbx \rV^2_2  \big]$).
 \begin{restatable}{lemma}{ssmmrc}\label{theorem:mmrc_accuracy_ss}
 Let $\qSS(\svbz | \svbx)$ be the  $\varepsilon$-LDP \emph{$\SubsetSelection$} mechanism with parameters $d$ and $s=\lceil \frac{d}{1+e^\varepsilon} \rceil$ and estimator $\hat{\svbx}^{\texttt{ss}}$. Let $\qMMRC(\svbz|\svbx)$ denote the \emph{$\MMRC$} privatization mechanism simulating \emph{$\SubsetSelection$} with $N$ candidates and estimator $\hat{\svbx}^{\texttt{mmrc}}$ as defined above. Consider any $\lambda > 0$. Then,
 \begin{align}
      & \E_{\qMMRC} \lb \lV \hat{\svbx}^{\texttt{mmrc}}  -  \svbx\rV^2_2\rb   \leq  (1 + 4\lambda + 5\lambda^2 + 2\lambda^3) \E_{\qSS} \lb \lV \hat{\svbx}^{\texttt{ss}} - \svbx\rV^2_2\rb,
 \end{align}
  as long as 
 \begin{align}
      N\geq \frac{2(e^{\varepsilon}+1)^2(1+\lambda)^2}{0.24^2\lambda^2}\ln\lp \frac{8(1+\lambda)}{0.24\lambda}\rp.
  \end{align}
 \end{restatable}
 \begin{proof}
The proof follows from Proposition \ref{proposition:mmrc_mse_wrt_ss} and Lemma \ref{lemma:MMRC_SS_N}.
\end{proof}
 
% Combining Proposition~\ref{proposition:mse_wrt_ss} and Lemma~\ref{lemma:SS_N} together, we arrive at the following result:
% \ssmmrc*
% \iffalse
% \begin{theorem}
%  As long as $N \geq \frac{512e^{2\varepsilon}(1+\lambda)^2\log\lp\frac{32+32\lambda}{\lambda}\rp}{\lambda^2}$, 
%  $$ \E\lb \lV \tilde{p}-p\rV^2_2\rb  \leq (1+4\lambda+o(\lambda)) \E\lb \lV \hat{p}_\msf{SS}-p\rV^2_2\rb.$$
% \end{theorem}
% \fi
\subsubsection{Simulating \texorpdfstring{$\SubsetSelection$}{Subset Selection} using Modified Minimal Random Coding}\label{appendix:mmrc_ss_utility}
Now, we provide a proof of Theorem \ref{thm:fe_mmrc_ss}.
\mmrcss*
\begin{proof}
The proof follows directly from Lemma~\ref{theorem:mmrc_accuracy_ss} since for all $i \in [n]$, $\hat{\svbx}^{\texttt{mmrc}}_i$ are independent of each other as well as unbiased.
\end{proof}

\subsection{Additional Empirical Comparisons}\label{appendix:mmrc_ss_emp}
In Section \ref{subsec:mmrc_ss_empirical}, we empirically demonstrated the privacy-accuracy-communication tradeoffs of $\MMRC$ simulating $\SubsetSelection$ against $\SubsetSelection$ and RHR in terms of $\ell_2$ error vs $\#$bits and $\ell_2$ error vs $\varepsilon$ (see Figure \ref{fig:freq}). In this section, we provide comparisons between these methods in terms of $\ell_2$ error vs $d$ (see Figure \ref{fig:freq_app} (left)) and $\ell_2$ error vs $n$ (see Figure \ref{fig:freq_app} (right)) for a fixed $\varepsilon$ (=6) and a fixed $\#$bits (=14). As before, RHR uses $\#$bits $= \varepsilon$ for both because it leads to a poor performance if $\#$bits $ > \varepsilon$.
\begin{figure}[h]
\centering
\includegraphics[width=0.45\linewidth]{figures/z_d6.pdf} \qquad \includegraphics[width=0.45\linewidth]{figures/z_n6.pdf}%
\caption{Comparing $\SubsetSelection$, $\MMRC$ simulating $\SubsetSelection$ and RHR for frequency estimation with $\varepsilon=6$ and $\#$bits $=14$. \textbf{Left:} $\ell_2$ error vs $d$ for $n = 5000$. \textbf{Right:} $\ell_2$ error vs $n$ for $d = 500$.}
\label{fig:freq_app}
\end{figure}
