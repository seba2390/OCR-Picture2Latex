\section{Simulating \texorpdfstring{$\SubsetSelection$}{Subset Selection} using Minimal Random Coding}\label{appendix:mrc_ss}

In this section, we simulate $\SubsetSelection$ using $\MRC$ analogous to how we simulate $\SubsetSelection$ using $\MMRC$ in Section \ref{sec:frequency_estimation}. 
First, in Appendix \ref{appendix:debias_mrc_ss}, we provide an unbiased estimator for $\MRC$ simulating $\SubsetSelection$.
Next, in Appendix \ref{appendix:mrc_ss_ut} we provide the utility guarantee associated with $\MRC$ simulating $\SubsetSelection$. 
To do that, first, in Appendix \ref{appendix:scaling_mrc_ss}, we show that when the number of candidates $N$ is exponential in $\varepsilon$, the scaling factor $m_{\texttt{mrc}}$ is close to the scaling parameter associated with $\SubsetSelection$ (i.e., $m_{\texttt{ss}}$). 
Next, in Appendix \ref{appendix:mrc_ss_scaling_mse}, we provide the relationship between the mean squared error associated with $\MRC$ simulating $\SubsetSelection$ and the mean squared error associated with $\SubsetSelection$. In Appendix \ref{appendix:mrc_ss_utility}, we combine everything and show that, for frequency estimation, $\MRC$ can simulate $\SubsetSelection$ in a near-lossless manner while only using on the order of $\varepsilon$-bits of communication. Finally, in Appendix \ref{appendix:mrc_ss_emp}, we provide some empirical comparisons.

% Next, in Appendix \ref{appendix:mrc_ss_scaling_mse}, we show that if $m_{\texttt{mrc}}$ is close to $m_{\texttt{ss}}$ and $b_{\texttt{mrc}} \geq b_{\texttt{ss}}$, then the mean squared error associated with $\MRC$ simulating $\SubsetSelection$ is close to the mean squared error associated with $\SubsetSelection$. 
% In Appendix \ref{appendix:mrc_ss_utility}, we combine everything and show that $\qMRC$ can compress $\qSS$ to the order of $\varepsilon$-bits of communication as well as simulate it in a near-lossless fashion. Finally, in Appendix \ref{appendix:mrc_ss_emp}, we provide some empirical comparisons.
\subsection{Unbiased Minimal Random Coding simulating \texorpdfstring{$\SubsetSelection$}{Subset Selection}}\label{appendix:debias_mrc_ss}
Consider the $\SubsetSelection$ $\varepsilon$-LDP mechanism $\qSS$ with parameter $s$ as described in Section \ref{sec:preliminaries} and Appendix \ref{appendix:ss}. 
Let $\piMRC$ be the distribution and $\svbz_1, \svbz_2,...,\svbz_N$ be the candidates obtained from Algorithm \ref{alg:mrc} when the reference distribution is $\Unif(\cZ)$ where $\cZ$ is as defined in \eqref{eq:z_ss}.
Let $\theta$ denote the fraction of candidates inside $\msf{Cap}_{\svbx} = \cZ_{\svbx}$ where $\cZ_{\svbx}$ is the set of elements in $\cZ$ with $1$ in the same location as $\svbx$. 
It is easy to see that $\theta \sim \frac{1}{N}\msf{Binom}\lp N, \frac{s}{d} \rp$.
% $\SubsetSelection$ is a cap-based mechanism with $\msf{Cap}_{\svbx} = \cZ_{\svbx}$ as discussed in Appendix \ref{appendix:ss} where $\cZ_{\svbx}$ is the set of elements in $\cZ$ with $1$ in the same location as $\svbx$. 
Let $\qMRC_i = \Probability(z_i = 1)$ where $\svbz \sim \qMRC(\cdot|\svbx)$ i.e., $\qMRC_i = \Probability\lbp(\svbz_K)_i = 1\rbp$ where $K \sim \piMRC(\cdot)$. 
% Define $p_{\texttt{mrc}} \coloneqq \Probability(\svbz_K \in \msf{Cap}_{\svbx})$
% % Let $p_{\texttt{mrc}} = \Probability(\svbz_K \in \msf{Cap}_{\svbx}_{\varepsilon, d}(\svbx))$
% to be the probability with which the sampled candidate $\svbz_K$ belongs to the spherical cap associated with $\PrivUnit$.
% Define $m_{\texttt{mrc}}$ as the scaling factor in \eqref{eq:m} when $p_0$ in \eqref{eq:m} is replaced by $p_{\texttt{mrc}}$. Define $\hat{\svbx}^\texttt{mrc} \coloneqq \svbz_K / m_\texttt{mrc}$ as the estimator of the $\MRC$ mechanism simulating $\PrivUnit$.  The following Lemma shows that $\hat{\svbx}^\texttt{mrc}$ is an unbiased estimator.

The following lemma shows that the marginal distribution of $\qMRC_i$ can be written as a linear function of $p_i$ similar to $\qSS_i$ in \eqref{eq:ss_marginal}. This allows us to provide an unbiased estimator for $\MRC$ simulating $\SubsetSelection$.
\begin{restatable}{lemma}{mrcssbias}\label{thm:mrc_ss_bias}
Let $K \sim \piMRC(\cdot)$ and $\qMRC_i = \Probability\lbp(\svbz_K)_i = 1\rbp$ for $i \in [d]$. Then, 
\begin{align}
    \qMRC_i = p_i  m_\texttt{mrc} + b_\texttt{mrc}
\end{align}
where
\begin{align}\label{eq:q_check}
m_\texttt{mrc} & \coloneqq \E\lb \frac{\theta e^\varepsilon}{e^\varepsilon \theta+(1-\theta)} \rb - \frac{1}{d-1}\E\lb s-\frac{e^\varepsilon \theta}{e^\varepsilon \theta+(1-\theta)} \rb, \nonumber\\
b_\texttt{mrc} & \coloneqq \frac{1}{d-1}\E\lb s-\frac{e^\varepsilon \theta}{e^\varepsilon \theta + (1-\theta)} \rb.
\end{align}
Further, $\hat{\svbx}_\texttt{mrc} \coloneqq (\svbz_K - b_\texttt{mrc}\cdot\mb{1}_d)/m_\texttt{mrc}$ is an unbiased estimator of $\svbx$, i.e., $\E[\hat{\svbx}_\texttt{mrc}] = \svbx$.
\end{restatable}
\begin{proof}
%{\color{red}\texttt{TODO: add (45), (51), and (56).}}\\
We have
\begin{align}
    \Pr\lbp (\svbz_K)_i  = 1 \rbp 
    & = \sum_j p_j \Pr\lbp (\svbz_K)_i = 1 \mv \svbx = j\rbp  \\
    & \stackrel{(a)}{=} p_i\Pr\lbp (\svbz_K)_i = 1 \mv \svbx = i\rbp + (1-p_i) \Pr\lbp (\svbz_K)_i = 1 \mv \svbx = j\rbp. \label{eq:ss_step_1}
\end{align}
where $(a)$ follows by symmetry. Next, we compute $\Pr\lbp (\svbz_K)_i = 1 \mv \svbx = i\rbp$ and $\Pr\lbp (\svbz_K)_i = 1 \mv \svbx = j \rbp$ separately.

To compute $\Pr\lbp (\svbz_K)_i = 1 \mv \svbx = i\rbp$, recall that $\theta$ denotes the fraction of candidates that belong inside the $\msf{Cap}_{\svbx}$ i.e., have $1$ in the same location as $\svbx$. From Appendix \ref{appendix:ss_cap}, recall that $c_1(\varepsilon,d) \coloneqq \dfrac{e^\varepsilon}{\binom{d-1}{s-1}e^\varepsilon + \binom{d-1}{s}}$, $c_2(\varepsilon,d) \coloneqq \dfrac{1}{\binom{d-1}{s-1}e^\varepsilon + \binom{d-1}{s}}$. Further, since $\svbz_k$ are generated uniformly at random,
$$ \theta \sim \frac{1}{N} \msf{Binom}\lp N, \frac{\binom{d-1}{s-1}}{\binom{d}{s}} \rp = \frac{1}{N} \msf{Binom}\lp N, \frac{s}{d} \rp,$$ so we have
\begin{align}
    \Pr\lbp (\svbz_K)_i = 1 | \svbx = i \rbp = \Pr\lbp \svbz_K \in \msf{Cap}_{\svbx} | \svbx = i \rbp  & \stackrel{(a)}{=} \E\lb \Pr\lbp \svbz_K  \in \msf{Cap}_{\svbx} |\svbx=i, \theta \rbp\rb\\
    % & = \E_{n\sim \msf{Binom}\lp N, \frac{t}{d} \rp}\lb \frac{n c_1(\varepsilon,d)}{nc_1(\varepsilon,d)+(N-n)c_2(\varepsilon,d)} \rb \\ 
    & = \E\lb \frac{c_1(\varepsilon,d) \theta}{c_1(\varepsilon,d) \theta + (1-\theta)c_2(\varepsilon,d)} \rb \\
    & \stackrel{(b)}{=} \E\lb \frac{e^\varepsilon \theta}{e^\varepsilon \theta + (1-\theta)} \rb, \label{eq:ss_step_2}
\end{align}
where $(a)$ follows by the law of total probability and $(b)$ is due to $c_1(\varepsilon,d)/c_2(\varepsilon,d) = e^\varepsilon$.

To compute $\Pr\lbp (\svbz_K)_i = 1 \mv \svbx = j \rbp$, we decompose it into
\begin{align}
    \Pr\lbp (\svbz_K)_i = 1 \mv \svbx = j \rbp = \Pr\lbp (\svbz_K)_i = 1, (\svbz_K)_j = 1 \mv \svbx = j\rbp + \Pr\lbp (\svbz_K)_i = 1, (\svbz_K)_j = 0 \mv \svbx = j\rbp, \label{eq:Step0}
\end{align}
for any $j \neq i$ and calculate each of the terms separately.

As before, let $\theta$ denotes the fraction of candidates that belong inside the $\msf{Cap}_{\svbx}$ i.e., have $1$ in the same location as $\svbx$. Further, let $\bar{\theta}$ denotes the fraction of candidates that belong inside the $\msf{Cap}_{\svbx}$ i.e., have $1$ in the same location as $\svbx$ as well as have $1$ in the $j^{th}$ location. Since $\svbz_k$ are generated uniformly at random,
$$ \bar{\theta} \sim \frac{1}{N} \msf{Binom}\lp N\theta, \frac{\binom{d-2}{s-2}}{\binom{d-1}{s-1}} \rp = \frac{1}{N} \msf{Binom}\lp N\theta, \frac{s-1}{d-1} \rp,$$ so we have
\begin{align}
    \Pr\lbp (\svbz_K)_i = 1, (\svbz_K)_j = 1 | \svbx = j \rbp 
    & \stackrel{(a)}{=} \E_{\theta}\lb \E_{\bar{\theta}}\lb \Pr\lbp (\svbz_K)_i = 1, (\svbz_K)_j = 1 \mv \svbx = j, \bar{\theta}, \theta \rbp \rb\rb \\
    & = \E_{\theta}\lb \E_{\bar{\theta}}\lb \frac{ c_1(\varepsilon,d)\bar{\theta}}{ c_1(\varepsilon,d) \theta  + (1 - \theta) c_2(\varepsilon,d)} \rb\rb \\ 
    & \stackrel{(b)}{=} \frac{s-1}{d-1}\E_{\theta}\lb \frac{ c_1(\varepsilon,d) \theta}{ c_1(\varepsilon,d)\theta +(1-\theta) c_2(\varepsilon,d)} \rb \\
    & \stackrel{(c)}{=} \frac{s-1}{d-1}\E\lb \frac{e^\varepsilon \theta}{e^\varepsilon \theta + (1-\theta)} \rb \label{eq:Step1}
\end{align}
where $(a)$ follows by the law of total probability, $(b)$ follows because $\E[\bar{\theta}] = \frac{s-1}{d-1} \times \theta$, and $(c)$ is due to $c_1(\varepsilon,d)/c_2(\varepsilon,d) = e^\varepsilon$.


Similarly, to compute the term $\Pr\lbp (\svbz_K)_i = 1, (\svbz_K)_j = 0 | \svbx = j \rbp$, let $\bar{\theta}$ denote the fraction of candidates that belong inside the $\msf{Cap}_{\svbx}$ i.e., have $1$ in the same location as $\svbx$ as well as have $0$ in the $j^{th}$ location. Since $\svbz_k$ are generated uniformly at random,
$$ \bar{\theta} \sim \frac{1}{N} \msf{Binom}\lp N(1-\theta), \frac{\binom{d-2}{s-1}}{\binom{d-1}{s}} \rp = \frac{1}{N} \msf{Binom}\lp N(1-\theta), \frac{s}{d-1} \rp,$$ so we have
\begin{align}
    \Pr\lbp (\svbz_K)_i = 1, (\svbz_K)_j = 0 | \svbx = j \rbp 
    & \stackrel{(a)}{=} \E_{\theta}\lb \E_{\bar{\theta}}\lb \Pr\lbp (\svbz_K)_i = 1, (\svbz_K)_j = 0 \mv \svbx = j, \bar{\theta}, \theta \rbp \rb\rb \\
    & = \E_{\theta}\lb \E_{\bar{\theta}}\lb \frac{ c_2(\varepsilon,d)\bar{\theta}}{ c_1(\varepsilon,d) \theta  + (1 - \theta) c_2(\varepsilon,d)} \rb\rb \\ 
    & \stackrel{(b)}{=} \frac{s}{d-1}\E_{\theta}\lb \frac{ c_2(\varepsilon,d) (1-\theta)}{ c_1(\varepsilon,d)\theta +(1-\theta) c_2(\varepsilon,d)} \rb \\ 
    % & \stackrel{(c)}{=} \frac{s-1}{d-1}\E\lb \frac{e^\varepsilon \theta}{e^\varepsilon \theta + (1-\theta)} \rb. \\
    % &  = \frac{t}{d-1}\E_{n\sim \msf{Binom}\lp N, 1-\frac{t}{d} \rp}\lb \frac{n c_2(\varepsilon,d)}{nc_2(\varepsilon,d)+(N-n)c_1(\varepsilon,d)} \rb \\ 
    & \stackrel{(c)}{=} \frac{s}{d-1}\E\lb \frac{(1-\theta)}{e^\varepsilon \theta + (1-\theta)} \rb, \label{eq:Step2}
\end{align}
where $(a)$ follows by the law of total probability, $(b)$ follows because $\E[\bar{\theta}] = \frac{s}{d-1} \times (1-\theta)$, and $(c)$ is due to $c_1(\varepsilon,d)/c_2(\varepsilon,d) = e^\varepsilon$.
% where in the second equality we set $\theta = 1- \frac{n}{N}$.
Using \eqref{eq:Step1} and \eqref{eq:Step2} in \eqref{eq:Step0}, we have
\begin{align}
    \Pr\lbp (\svbz_K)_i = 1 | \svbx = j \rbp 
    & = \Pr\lbp (\svbz_K)_i = 1, (\svbz_K)_j = 1 | \svbx = j \rbp+\Pr\lbp (\svbz_K)_i = 1, (\svbz_K)_j = 0 | \svbx = j \rbp \\
    & = \frac{s-1}{d-1}\E\lb \frac{e^\varepsilon \theta}{e^\varepsilon \theta + (1-\theta)} \rb + \frac{s}{d-1}\E\lb \frac{(1-\theta)}{e^\varepsilon \theta + (1-\theta)} \rb \\
    & = \frac{1}{d-1}\lp s - \E\lb \frac{e^\varepsilon \theta}{e^\varepsilon \theta +(1-\theta)}\rb \rp \label{eq:ss_step_3}
\end{align}
Combining everything, we have
\begin{align}
    \qMRC_i & = \Probability\lbp(\svbz_K)_i = 1\rbp \\
    & \stackrel{(a)}{=} p_i \times \lb \Pr\lbp (\svbz_K)_i = 1 \mv \svbx = i\rbp - \Pr\lbp (\svbz_K)_i = 1 \mv \svbx = j\rbp \rb + \Pr\lbp (\svbz_K)_i = 1 \mv \svbx = j\rbp. \\
    & \stackrel{(b)}{=} p_i \times \lb \E\lb \frac{e^\varepsilon \theta}{e^\varepsilon \theta + (1-\theta)} \rb - \frac{1}{d-1}\lp s - \E\lb \frac{e^\varepsilon \theta}{e^\varepsilon \theta +(1-\theta)}\rb \rp \rb \\
    &  + \frac{1}{d-1}\lp s - \E\lb \frac{e^\varepsilon \theta}{e^\varepsilon \theta +(1-\theta)}\rb \rp \\
    & \stackrel{(c)}{=} p_i  m_\texttt{mrc} + b_\texttt{mrc}
\end{align}
where $(a)$ follows from \eqref{eq:ss_step_1}, $(b)$ follows from \eqref{eq:ss_step_2} and \eqref{eq:ss_step_3}, and $(c)$ follows from the definitions of $m_\texttt{mrc}$ and $b_\texttt{mrc}$. 

Note that the above conclusion holds for all prior distribution $\svbp = (p_1, ..., p_d)$ such that $\svbx \sim \svbp$. Thus by setting $\svbp = \svbx$ (here $\svbx$ is viewed as a one-hot vector), i.e., letting $\svbp$ be the point mass distribution at $\svbx$, we have
%Now, let us evaluate $\E[\hat{\svbx}_\texttt{mrc}]$. We have
\begin{align}
    \E[\hat{\svbx}_\texttt{mrc}] = (\E[ \svbz_K ] - b_\texttt{mrc}\cdot\mb{1}_d)/m_\texttt{mrc}
    %& = (\E[ \svbz_K ] - b_\texttt{mrc})/m_\texttt{mrc} \\
    % & = ( \Probability\lbp(\svbz_K)_i = 1\rbp - b_\texttt{mrc})/m_\texttt{mrc} \\
    & = ( \qMRC - b_\texttt{mrc}\cdot\mb{1}_d)/m_\texttt{mrc} \\
    & = \lp\lp m_\texttt{mrc}\cdot \svbp + b_\texttt{mrc}\cdot\mb{1}_d\rp -b_\texttt{mrc}\cdot\mb{1}_d\rp/m_\texttt{mrc} = \svbp \stackrel{(a)}{=} \svbx,
\end{align}
where $(a)$ is due to our construction of $\svbp$.
\end{proof}


 \subsection{Utility of Minimal Random Coding simulating \texorpdfstring{$\SubsetSelection$}{Subset Selection}}\label{appendix:mrc_ss_ut}
 
 \subsubsection{The scaling factors of \texorpdfstring{$\SubsetSelection$}{Subset Selection} and \texorpdfstring{$\MRC$}{MRC} are close when \texorpdfstring{$N$}{N} is of the right order}\label{appendix:scaling_mrc_ss}
 In the following Lemma, we show that when the number of candidates $N$ is exponential in $\varepsilon$, then the scaling parameters associated with $\SubsetSelection$ and the $\MRC$ scheme simulating $\SubsetSelection$ are close.
 
\begin{lemma}\label{lemma:mrc_ss_approximation_error}
Let $N$ denote the number of candidates used in the \emph{$\MRC$} scheme. Let $K \sim \piMRC$ where $\piMRC$ is the distribution over the indices $[N]$ associated the \emph{$\MRC$} scheme simulating \emph{$\SubsetSelection$}. Consider any $\lambda > 0$.
Then, the scaling factors $m_{\texttt{ss}}$ and $b_{\texttt{ss}}$ associated with \emph{$\SubsetSelection$} and the scaling factors $m_\texttt{mrc}$ and $b_{\texttt{mrc}}$ associated with the \emph{$\MRC$} scheme simulating \emph{$\SubsetSelection$} are such that
\begin{align}
    m_{\texttt{ss}} - m_\texttt{mrc} \leq \lambda\cdot m_\texttt{mrc}
\end{align}
and $b_{\texttt{ss}} \leq b_\texttt{mrc}$
% \begin{align}
%     b_{\texttt{ss}} \leq b_\texttt{mrc} 
% \end{align}
as long as
\begin{align}
    % N \geq   2e^{2\varepsilon}\lp\frac{2 (1+\lambda)}{\lambda \lp p_0 -1/2 \rp}\rp^2 \log\lp \frac{2(1+\lambda)}{\lambda \lp p_0 -1/2 \rp} \rp.
    N\geq \frac{2(e^{\varepsilon}+3)^2(1+\lambda)^2}{0.24^2\lambda^2}\ln\lp \frac{8(1+\lambda)}{0.24\lambda}\rp.
\end{align}
\end{lemma}
\begin{proof}
First, we will obtain convenient expressions for $m_{\texttt{ss}}$ and $b_{\texttt{ss}}$ defined in \eqref{eq:scaling_factors_ss}. We can write 
\begin{align}
& m_{\texttt{ss}} \coloneqq  \lp \frac{\E[\theta]e^\varepsilon}{e^\varepsilon \E[\theta]+(1-\E[\theta])}\rp - \frac{1}{d-1}\lp s-\frac{e^\varepsilon \E[\theta]}{e^\varepsilon \E[\theta]+(1-\E[\theta])} \rp \label{eq:check_mb_def_1}\\
& b_{\texttt{ss}} \coloneqq \frac{1}{d-1}\lp s-\frac{e^\varepsilon\E[\theta]}{e^\varepsilon \E[\theta] + (1-\E[\theta])} \rp. \label{eq:check_mb_def_2}   
\end{align}
To verify these, we simply plug  $\E[\theta] = \frac{s}{d}$ into \eqref{eq:check_mb_def_1} resulting in:
\begin{align*}
    m_{\texttt{ss}} & = \frac{d}{d-1}\frac{se^\varepsilon}{se^\varepsilon + (d-s)} - \frac{s}{d-1}  = \frac{d s e^\varepsilon - s^2 e^\varepsilon - s(d-t)}{(d-1)\lp s e^\varepsilon+d-s\rp}  = \frac{s(d-s)(e^\varepsilon-1)}{(d-1)\lp se^\varepsilon+d-s\rp}.
\end{align*}
and into \eqref{eq:check_mb_def_2} resulting in:
\begin{align*}
    b_{\texttt{ss}} = \frac{1}{d-1}\lp s - \frac{se^\varepsilon}{se^\varepsilon + d - s} \rp 
    & =  \frac{1}{d-1}\lp \frac{s^2e^\varepsilon + s(d -s)-se^\varepsilon}{se^\varepsilon + d - s} \rp \\
    & = \frac{1}{d-1}\lp \frac{s(s-1)e^\varepsilon + s(d -s)}{se^\varepsilon + d - s} \rp.
\end{align*}
% Thus we verified that \eqref{eq:check_mb_def} is equivalent to $\qSS_i = m\cdot p_i +b$. Note that together with Theorem~\ref{claim:check_mb}, it suggests that asymptotically, the output of $\qMRC$ conveges to $q^\varepsilon_\msf{SS}$ in expectation since $m_{\texttt{mrc}} \ra m$ and $\hat{b} \ra b$ as the coding cost $N$ large enough. 

Recall the definitions of
$b_{\texttt{ss}}$ and $m_{\texttt{ss}}$ from Lemma \ref{thm:mrc_ss_bias}. Applying Jensen's inequality on the concave function $x\mapsto \frac{x}{x+c}$ for some $c >0$ yields $m_{\texttt{mrc}}\leq m_{\texttt{ss}}$ and $b_{\texttt{mrc}} \geq b_{\texttt{ss}}$.

Now, we will bound $\lba m_{\texttt{mrc}} - m_{\texttt{ss}} \rba$:
\begin{align}
    \lba m_{\texttt{mrc}} - m_{\texttt{ss}} \rba 
    & = \lp\frac{d}{d-1}\rp\lp \frac{\E[\theta]e^\varepsilon}{e^\varepsilon \E[\theta]+(1-\E[\theta])} - \E\lb \frac{\theta e^\varepsilon}{e^\varepsilon \theta+(1-\theta)} \rb\rp \\
    &\stackrel{(a)}{\leq} 2 \lp \frac{\E[\theta]e^\varepsilon}{e^\varepsilon \E[\theta]+(1-\E[\theta])} - \E\lb \frac{\theta e^\varepsilon}{e^\varepsilon \theta+(1-\theta)} \rb\rp \\
    & = 2 \lp \E\lb \frac{\lp \E[\theta]-\theta\rp e^\varepsilon}{\lp(e^\varepsilon-1) \E[\theta]+1\rp\lp(e^\varepsilon-1) \theta+1\rp} \rb\rp\label{eq:ss_m_hat_minus_m_eq_1},
\end{align}
where $(a)$ holds since $d\geq 2$.
Next, we condition on the event $\mcal{E} \coloneqq \lbp \lba \E[\theta]-\theta\rba \leq \sqrt{\frac{\ln(2/\beta)}{2N}} \rbp$, which has probability $\Pr_\theta\lbp \mcal{E} \rbp \geq 1-\beta$ by Hoeffding's inequality. We continue to upper bound \eqref{eq:ss_m_hat_minus_m_eq_1}:
\begin{align}
\lba m_{\texttt{mrc}}    -    m_{\texttt{ss}} \rba 
&= 2 \bigg( \Pr\lbp \mcal{E} \rbp\E\lb \frac{\lp  \E[\theta]-\theta\rp e^\varepsilon}{\lp(e^\varepsilon    -    1) \E[\theta]    +    1\rp\lp(e^\varepsilon    -    1) \theta    +    1\rp} \mv \mcal{E} \rb    \\
& \qquad +    \Pr\lbp \mcal{E}^c \rbp\E\lb \frac{\lp  \E[\theta]-\theta\rp e^\varepsilon}{\lp(e^\varepsilon    -    1) \E[\theta]   +    1\rp\lp(e^\varepsilon    -    1) \theta    +    1\rp} \mv \mcal{E}^c \rb \bigg)\\
& \stackrel{(a)}{\leq} 2 \lp \E\lb \frac{\lp  \E[\theta]-\theta\rp e^\varepsilon}{\lp(e^\varepsilon-1) \E[\theta]+1\rp\lp(e^\varepsilon-1) \theta+1\rp} \mv \mcal{E} \rb +\beta \rp\\
& \stackrel{(b)}{\leq} 2 \lp \E\lb \frac{\lp  \E[\theta]-\theta\rp e^\varepsilon}{\lp(e^\varepsilon-1) \E[\theta]+1\rp\lp(e^\varepsilon-1) E[\theta]/2+1\rp} \mv \mcal{E} \rb +\beta \rp\\
    & \leq 4\E\lb \frac{\lp \E[\theta] - \theta \rp e^\varepsilon}{\lp(e^\varepsilon-1) \E[\theta]+1\rp^2} \mv \mcal{E} \rb + 2\beta \stackrel{(c)}{\leq} 4\sqrt{\frac{\ln(2/\beta)}{2N}} \frac{e^\varepsilon(1+e^\varepsilon)^2}{4e^{2\varepsilon}}+ 2\beta \\
    & = \sqrt{\frac{\ln(2/\beta)}{2N}}\lp e^\varepsilon+2+\frac{1}{e^\varepsilon} \rp + 2\beta \leq \sqrt{\frac{\ln(2/\beta)}{2N}}(e^\varepsilon+3)+2\beta, \label{eq:mrc_ss_nrel1}
\end{align}
 where $(a)$ holds since 
 $$ \frac{\lp  \E[\theta]-\theta\rp e^\varepsilon}{\lp(e^\varepsilon-1) \E[\theta]+1\rp\lp(e^\varepsilon-1) \theta+1\rp} = \frac{\E[\theta]e^\varepsilon}{e^\varepsilon \E[\theta]+(1-\E[\theta])} -  \frac{\theta e^\varepsilon}{e^\varepsilon \theta+(1-\theta)} \leq 1,$$
 $(b)$ holds if we pick $N$ large enough so that $\lba \theta - \E[\theta]\rba \leq \frac{\E[\theta]}{2} $ for which a sufficient condition is $ \sqrt{\frac{\ln(2/\beta)}{2N}} \leq \frac{\E[\theta]}{2}  $ i.e., $N\geq \frac{2\ln(2/\beta)}{\E[\theta]^2} = 2(d/s)^2\ln(2/\beta)$,
% $$ \lba \theta - \E[\theta]\rba \leq \frac{\E[\theta]}{2} \Longleftrightarrow N\geq \frac{8\ln(1/\beta)}{\E[\theta]^2} = 8\ln(1/\beta)(d/s)^2, $$% = 8\ln(1/\beta)(1+e^\varepsilon)^2, $$
and $(c)$ holds since $\E[\theta] = s/d \geq 1/(1+e^\varepsilon)$. Notice that the constraint $N\geq  2(d/s)^2 \ln(2/\beta)$ in inequality $(b)$ can be further satisfied as long as $ N \geq 2\ln(2/\beta)(1+e^\varepsilon)^2 $
since $s/d \geq 1/(1+e^\varepsilon)$.

Next, we lower bound $m_{\texttt{ss}}$ in \eqref{eq:check_mb_def_1}:
\begin{align}
    m_{\texttt{ss}} & = \lp\frac{d}{d-1}\rp\lp \frac{\E[\theta]e^\varepsilon}{e^\varepsilon \E[\theta]+(1-\E[\theta])} - \frac{s}{d} \rp  \geq \frac{\E[\theta]e^\varepsilon}{e^\varepsilon \E[\theta]+(1-\E[\theta])} - \frac{s}{d}\\
    &\stackrel{(a)}{=}\frac{s}{d}\lb \frac{(e^\varepsilon-1)(d-s)}{(e^\varepsilon-1)\cdot s +d} \rb =\frac{s}{d}\lb \frac{(e^\varepsilon-1)(1-s/d)}{(e^\varepsilon-1)\cdot s/d +1} \rb \\
    & \stackrel{(b)}{\geq} \frac{1}{1+e^\varepsilon}\lb \frac{(e^\varepsilon-1)\lp\frac{e^\varepsilon}{1+e^\varepsilon}-\frac{1}{d}\rp}{(e^\varepsilon-1)\lp\frac{1}{1+e^\varepsilon}+\frac{1}{d}\rp+1} \rb \\
    & \stackrel{(c)}{\geq} \frac{1}{1+e^\varepsilon}\lb \frac{(e^\varepsilon-1)\frac{e^\varepsilon-1}{1+e^\varepsilon}}{(e^\varepsilon-1)\lp\frac{2}{1+e^\varepsilon}\rp+1} \rb = \frac{\lp e^\varepsilon-1\rp^2}{(3e^\varepsilon-1)(e^\varepsilon+1)} \\
    &\stackrel{(d)}{\geq} \frac{(e-1)^2}{(3e-1)(e+1)} \geq 0.24, \label{eq:mrc_ss_nrel2}
\end{align}
where $(a)$ holds by plugging in $\E[\theta] = s/d$, $(b)$ holds since $s = \lceil d/(1+e^\varepsilon)\rceil$ (so $\frac{1}{1+e^\varepsilon} \leq \frac{s}{d} \leq \frac{1}{1+e^\varepsilon}+\frac{1}{d}$), $(c)$ holds since we only focus on the regime where $\varepsilon \leq d-1$ (so $\frac{1}{d} \leq \frac{1}{1+\varepsilon}$), and $(d)$ holds by observing that $f(x) \coloneqq \frac{(x-1)^2}{(3x-1)(x+1)}$ is an increasing function for $x \geq 1$ and we have $\varepsilon \geq 1$. Putting things together, we obtain
\begin{align}
    \frac{m_{\texttt{ss}}-m_{\texttt{mrc}}}{m_{\texttt{mrc}}} =  \frac{m_{\texttt{ss}}-m_{\texttt{mrc}}}{m_{\texttt{ss}}-(m_{\texttt{ss}}-m_{\texttt{mrc}})} \stackrel{(a)}{\leq} \frac{\sqrt{\frac{\ln(2/\beta)}{2N}}(e^\varepsilon+3)+2\beta}{0.24-\lp \sqrt{\frac{\ln(2/\beta)}{2N}}(e^\varepsilon+3)+2\beta \rp} \stackrel{(b)}{\leq} \lambda,
\end{align}
where $(a)$ follows from \eqref{eq:mrc_ss_nrel1} and \eqref{eq:mrc_ss_nrel2} and $(b)$ follows as long as
\begin{align}
    \sqrt{\frac{\ln(2/\beta)}{2N}}(e^\varepsilon+3)+2\beta \leq \frac{0.24 \lambda}{1+\lambda}. \label{eq:mrc_ss_nrel3}
\end{align}
To ensure \eqref{eq:mrc_ss_nrel3}, we let
\begin{align*}
    \beta \leq \frac{0.24\lambda}{4(1+\lambda)} \qquad \text{and} \qquad N \geq \frac{1}{2}\lp\frac{(e^\varepsilon+3)}{\frac{0.24\lambda}{(1+\lambda)}-2\beta}\rp^2\ln(2/\beta) = \frac{2(e^{\varepsilon}+3)^2(1+\lambda)^2}{0.24^2\lambda^2}\ln\lp \frac{8(1+\lambda)}{0.24\lambda}\rp.
\end{align*}
It is easy to verify that this choice of $N$ satisfies $ N \geq 2\ln(2/\beta)(1+e^\varepsilon)^2 $.
\end{proof}


\subsubsection{Relationship between mean squared errors associated with \texorpdfstring{$\SubsetSelection$}{Subset Selection} and \texorpdfstring{$\MRC$}{MRC} simulating \texorpdfstring{$\SubsetSelection$}{Subset Selection}}\label{appendix:mrc_ss_scaling_mse}
In the following Proposition,
we show that if $m_{\texttt{mrc}}$ is close to $m_{\texttt{ss}}$ and $b_{\texttt{mrc}} \geq b_{\texttt{ss}}$, then the mean squared error associated with $\MRC$ simulating $\SubsetSelection$ (i.e., $\Expectation_{\qMRC} \big[ \lV  \hat{\svbx}^\texttt{mrc} - \svbx \rV^2_2  \big]$) is close to the mean squared error associated with $\SubsetSelection$ (i.e., $\Expectation_{\qSS} \big[ \lV  \hat{\svbx}^\texttt{ss} - \svbx \rV^2_2  \big]$).
\begin{proposition}\label{proposition:mrc_mse_wrt_ss}
Let $\qSS(\svbz | \svbx)$ be the  $\varepsilon$-LDP \emph{$\SubsetSelection$} mechanism with estimator $\hat{\svbx}^{\texttt{ss}}$. Let $\qMRC(\svbz|\svbx)$ denote the \emph{$\MRC$} privatization mechanism simulating \emph{$\SubsetSelection$} with $N$ candidates and estimator $\hat{\svbx}^{\texttt{mrc}}$.
Let $m_{\texttt{ss}}$ and $b_{\texttt{ss}}$ denote the scaling factors  associated with \emph{$\SubsetSelection$} and $m_\texttt{mrc}$ and $b_\texttt{mrc}$ denote the scaling factors associated with the \emph{$\MRC$} scheme simulating \emph{$\SubsetSelection$}. Consider any $\lambda > 0$. If $m_{\texttt{pu}} - m_\texttt{mrc} \leq \lambda \cdot m_\texttt{mrc}$ and $b_{\texttt{mrc}} \geq b_{\texttt{ss}}$, then 
% $m - m_\texttt{mrc} \leq \lambda\cdotm_\texttt{mrc}$ and $b_\texttt{mrc} \geq b$
  \begin{align}
    \Expectation_{\qMRC} \big[ \lV  \hat{\svbx}^\texttt{mrc} - \svbx \rV^2_2  \big]  \leq  \lp 1+4\lambda +5\lambda^2 + 2\lambda^3 \rp  \Expectation_{\qSS}\big[\|\hat{\svbx}^{\texttt{ss}} - \svbx\|^2\big]
\end{align}
\end{proposition}
\begin{proof}
% We first show that the  MSE (i.e. the $\ell^2_2$ error) can be computed by 
% $$ \E\lb \lV \check{p}-p\rV^2_2\rb  = \lp\frac{1}{m_\texttt{mrc}}\rp^2 \sum_i \qMRC_i (1-\qMRC_i), $$
% where $\qMRC_i \coloneqq \E_{\qMRC}[\lp\svbz_K\rp_i]$.
% To see this, note that $\lp \svbz_K \rp_i\sim\msf{Ber}(\qMRC_i)$, so $\Var\lp \check{p}_i \rp = \lp \frac{1}{m_\texttt{mrc}}\rp^2\qMRC_i (1-\qMRC_i)$, so
We have
$$  \Expectation_{\qMRC} \big[ \lV  \hat{\svbx}^\texttt{mrc} - \svbx \rV^2_2  \big]  \stackrel{(a)}{=} \sum_{i=1}^d \Var\lp \hat{\svbx}^\texttt{mrc}_i \rp \stackrel{(b)}{=} \lp\frac{1}{m_\texttt{mrc}}\rp^2 \sum_i \Var \lp \lp \svbz_K \rp_i\rp \stackrel{(c)}{=} \lp\frac{1}{m_\texttt{mrc}}\rp^2 \sum_i \qMRC_i (1-\qMRC_i). $$
where $(a)$ follows because $\svbx$ is a constant, $(b)$ follows because $\hat{\svbx}_\texttt{mrc} =(\svbz_K - b_\texttt{mrc})/m_\texttt{mrc}$, and $(c)$ follows because $\lp \svbz_K \rp_i\sim\msf{Ber}(\qMRC_i)$. Similarly, we have
We have
$$  \Expectation_{\qSS} \big[ \lV  \hat{\svbx}^\texttt{ss} - \svbx \rV^2_2  \big]  \stackrel{(a)}{=} \sum_{i=1}^d \Var\lp \hat{\svbx}^\texttt{ss}_i \rp \stackrel{(b)}{=} \lp\frac{1}{m_\texttt{ss}}\rp^2 \sum_i \Var \lp z_i\rp \stackrel{(c)}{=} \lp\frac{1}{m_\texttt{ss}}\rp^2 \sum_i \qSS_i (1-\qSS_i). $$
where $(a)$ follows because $\svbx$ is a constant, $(b)$ follows because $\hat{\svbx}^\texttt{ss} =(\svbz -  b_\texttt{ss})/m_\texttt{ss}$, and $(c)$ follows because $z_i\sim\msf{Ber}(\qSS_i)$.

Now, let us look at the difference i.e.,
% Now our goal is to show that the MSE of $\check{p}$ is of the same order of that of SS (which is $\frac{1}{m^2}\sum_i \qSS_i(1-\qSS_i)$).
% Consider the difference
\begin{align*}
    & \Expectation_{\qMRC} \big[ \lV  \hat{\svbx}^\texttt{mrc} - \svbx \rV^2_2  \big] - \Expectation_{\qSS} \big[ \lV  \hat{\svbx}^\texttt{ss} - \svbx \rV^2_2  \big] \\
    & = \lp\frac{1}{m_\texttt{mrc}}\rp^2 \sum_i \qMRC_i (1-\qMRC_i) - \lp\frac{1}{m_\texttt{ss}}\rp^2 \sum_i \qSS_i (1-\qSS_i)\\
    & \leq \lp\frac{1}{m_\texttt{mrc}}\rp^2\sum_{i}\lp \qMRC_i (1-\qMRC_i) - \qSS_i (1-\qSS_i) \rp + \lb \frac{1}{m_\texttt{mrc}^2}-\frac{1}{m_\texttt{ss}^2}\rb\lp \sum_i \qSS_i (1-\qSS_i) \rp.
\end{align*}

% Before we proceed into the main proof we note that $t$ and $k$ will be picked such that  $t = \lceil k/(1+e^\varepsilon)\rceil$ as suggested by \cite{YB18}.

% To begin with, we argue that as long as $N$ large enough such that 
% \begin{equation}\label{eq:m_hat_bdd}
%     m-m_\texttt{mrc} \leq \lambda\cdot m_\texttt{mrc},
% \end{equation} 
% both (a) and (b) can be well-controlled. 

% \paragraph{Bounding (a):}
Now, first, we will bound $\lp\frac{1}{m_\texttt{mrc}}\rp^2\sum_{i}\lp \qMRC_i (1-\qMRC_i) - \qSS_i (1-\qSS_i) \rp$. To that end, observe that $m_{\texttt{pu}} - m_\texttt{mrc} \leq \lambda \cdot m_\texttt{mrc}$ implies 
\begin{equation}\label{eq:m_hat_bdd_2}
    \frac{1}{m_\texttt{mrc}} \leq (1+\lambda)\frac{1}{m_{\texttt{ss}}}.
\end{equation} 
Further, we have
\begin{align}
\qMRC_i & \stackrel{(a)}{=} m_\texttt{mrc} p_i+b_\texttt{mrc} \stackrel{(b)}{=} \qSS_i + (m_\texttt{mrc}-m_\texttt{ss})p_i+(b_\texttt{mrc}-b_\texttt{ss}) \stackrel{(c)}{\geq} \qSS_i - \lambda \cdot m_\texttt{mrc} \cdot p_i +(b_\texttt{mrc}-b_\texttt{ss}) \\ & 
\stackrel{(d)}{\geq} \qSS_i - \lambda \cdot m_\texttt{mrc} \cdot p_i
\stackrel{(e)}{\geq} \qSS_i - \lambda \cdot m_\texttt{ss} \cdot p_i
\stackrel{(f)}{\geq} (1-\lambda)\qSS_i, \label{eq:q_rel}
\end{align}
where $(a)$ follows from Lemma \ref{thm:mrc_ss_bias}, $(b)$ follows from \eqref{eq:ss_marginal}, $(c)$ follows because $m_{\texttt{pu}} - m_\texttt{mrc} \leq \lambda \cdot m_\texttt{mrc}$, $(d)$ follows because $b_\texttt{mrc} \geq b_\texttt{ss}$, $(e)$ follows because $m_\texttt{ss} \geq m_\texttt{mrc}$ as seen in Lemma \ref{lemma:mrc_ss_approximation_error}, and $(f)$ follows because $b_\texttt{ss} \geq 0$. Next, we have
\begin{align}
    \frac{\qMRC_i(1-\qMRC_i) - \qSS_i(1-\qSS_i)}{\qSS_i(1-\qSS_i)} 
     = \frac{(\qSS_i - \qMRC_i)(\qSS_i+\qMRC_i -1)}{\qSS_i(1-\qSS_i)} 
     \stackrel{(a)}{\leq} \frac{\lambda \qSS_i(\qSS_i+\qMRC_i-1)}{\qSS_i(1-\qSS_i)}
     \stackrel{(b)}{\leq} \frac{\lambda }{1-\qSS_i} \label{eq:mrc_ss_inter1}
\end{align}
where $(a)$ follows from \eqref{eq:q_rel} and $(b)$ follows since $\qSS_i \leq 1$ and $\qMRC_i \leq 1$.
% and $(c)$ holds because whenever  $s = \lceil \frac{d}{1+e^\varepsilon} \rceil$, we have $m_\texttt{ss} + b_\texttt{ss} \leq \frac{1}{2}$, so $\qSS_i \coloneqq m_\texttt{ss} \cdot p_i + b_\texttt{ss} \leq 1/2$. 

Let us now upper bound $\qSS_i$. We have 
\begin{align}
    \qSS_i  = m_\texttt{ss} \cdot p_i +
b_\texttt{ss} \stackrel{(a)}{\leq} m_\texttt{ss} + b_\texttt{ss} \stackrel{(b)}{=} \lp \frac{\E[\theta]e^\varepsilon}{e^\varepsilon \E[\theta]+(1-\E[\theta])}\rp  \stackrel{(c)}{\leq} \frac{1}{2} \label{eq:mrc_ss_inter2}
\end{align}
where $(a)$ follows because $p_i \leq 1$, $(b)$ follows from \eqref{eq:check_mb_def_1} and \eqref{eq:check_mb_def_2}, and $(c)$ follows because $\E[\theta] = \frac{s}{d} \geq \frac{1}{e^\varepsilon+ 1}$. Combining \eqref{eq:mrc_ss_inter1} and \eqref{eq:mrc_ss_inter2}, and then re-arranging results in
$$ \sum_i \qMRC_i(1-\qMRC_i) - \sum_i \qSS_i(1-\qSS_i)\leq 2\lambda \sum_i \qSS_i(1-\qSS_i). $$
Together with \eqref{eq:m_hat_bdd_2}, we obtain
$$ \lp\frac{1}{m_\texttt{mrc}}\rp^2\sum_{i}\lp \qMRC_i (1-\qMRC_i) - \qSS_i (1-\qSS_i) \rp \leq  \frac{2\lambda(1+\lambda)^2}{m_\texttt{ss}^2}\sum_i \qSS_i(1-\qSS_i).$$


To bound $\lb \frac{1}{m_\texttt{mrc}^2}-\frac{1}{m_\texttt{ss}^2}\rb\lp \sum_i \qSS_i (1-\qSS_i) \rp$, simply note that \eqref{eq:m_hat_bdd_2} implies $\frac{1}{m_\texttt{mrc}^2} \leq (1+\lambda)^2\frac{1}{m_{\texttt{ss}}^2}$ resulting in
$$\lb \frac{1}{m_\texttt{mrc}^2}-\frac{1}{m_\texttt{ss}^2}\rb\lp \sum_i \qSS_i (1-\qSS_i) \rp \leq \frac{2\lambda + \lambda^2}{m_\texttt{ss}^2} \lp \sum_i \qSS_i(1-\qSS_i) \rp.$$

Combining everything, we have
\begin{align}
    \Expectation_{\qMRC} \big[ \lV  \hat{\svbx}^\texttt{mrc}    -     \svbx \rV^2_2  \big]   & \leq    \lp 1+2\lambda(1+\lambda)^2 + 2\lambda + \lambda^2 \rp\frac{1}{m_\texttt{mrc}^2}\sum_i \qSS_i(1-\qSS_i)\\
    & =     \lp 1+4\lambda +5\lambda^2 + 2\lambda^3 \rp  \Expectation_{\qSS}\big[\|\hat{\svbx}^{\texttt{ss}}    -    \svbx\|^2\big]
\end{align}
\end{proof}


In the following Lemma, we show that with on the order of $\varepsilon$-bits of communication, the mean squared error associated with $\MRC$ simulating $\SubsetSelection$ (i.e., $\Expectation_{\qMRC} \big[ \lV  \hat{\svbx}^\texttt{mrc} - \svbx \rV^2_2  \big]$) is close to the mean squared error associated with $\SubsetSelection$ (i.e., $\Expectation_{\qSS} \big[ \lV  \hat{\svbx}^\texttt{ss} - \svbx \rV^2_2  \big]$).
\begin{restatable}{lemma}{mrcss}\label{theorem:mrc_ss}
Let $\qSS(\svbz | \svbx)$ be the  $\varepsilon$-LDP \emph{$\SubsetSelection$} mechanism with estimator $\hat{\svbx}^{\texttt{ss}}$. Let $\qMRC(\svbz|\svbx)$ denote the \emph{$\MRC$} privatization mechanism simulating \emph{$\SubsetSelection$} with $N$ candidates and estimator $\hat{\svbx}^{\texttt{mrc}}$. 
Consider any $\lambda > 0$. Then,
\begin{align}
   \Expectation_{\qMRC} \big[ \lV  \hat{\svbx}^\texttt{mrc} - \svbx \rV^2_2  \big]  \leq  \lp 1+4\lambda +5\lambda^2 + 2\lambda^3 \rp  \Expectation_{\qSS}\big[\|\hat{\svbx}^{\texttt{ss}} - \svbx\|^2\big]
\end{align}
as long as 
\begin{align}
    N \geq  \frac{2(e^{\varepsilon}+3)^2(1+\lambda)^2}{0.24^2\lambda^2}\ln\lp \frac{8(1+\lambda)}{0.24\lambda}\rp.
    % N\geq \frac{64e^{2\varepsilon}(1+\lambda)}{\lambda^2\lp\min\lp 1/8, 3\varepsilon\rp\rp^2}\log\lp \frac{4(1+\lambda)}{\lambda\min\lp 1/8, 3\varepsilon \rp}\rp,
\end{align}
\end{restatable}
\begin{proof}
The proof follows from Proposition \ref{proposition:mrc_mse_wrt_ss} and Lemma \ref{lemma:mrc_ss_approximation_error}.
\end{proof}

\subsubsection{Simulating \texorpdfstring{$\SubsetSelection$}{Subset Selection} using Minimal Random Coding}\label{appendix:mrc_ss_utility}
The following Theorem shows that, for frequency estimation, $\MRC$ can simulate $\SubsetSelection$ in a near-lossless manner (when $\lambda$ is small) while only using on the order of $\varepsilon$ bits of communication.

\begin{restatable}{theorem}{mrcssut}
Let $r_{\msf{FE}} \lp \hat{\Pi}^\texttt{ss}, \qSS \rp$ and $r_{\msf{FE}} \lp \hat{\Pi}^\texttt{mrc}, \qMRC \rp$ be the empirical frequency estimation error for \emph{$\SubsetSelection$} and \emph{$\MRC$} simulating \emph{$\SubsetSelection$} with $N$ candidates respectively. Consider any $\lambda > 0$. Then
 \begin{equation}
     r_{\msf{FE}} \lp \hat{\Pi}^{ \texttt{mrc}}, \qMRC \rp \leq  
     \lp 1+4\lambda+5\lambda^2+2\lambda^3 \rp r_{\msf{FE}} \lp \hat{\Pi}^{ \texttt{ss}}, \qSS \rp,
 \end{equation}
as long as 
\begin{align}
% \textstyle
   N \geq  \frac{2(e^{\varepsilon}+3)^2(1+\lambda)^2}{0.24^2\lambda^2}\ln\lp \frac{8(1+\lambda)}{0.24\lambda}\rp.
\end{align}
% $N$ satisfies \eqref{eq:N_bdd_mmrc_pu}.
\end{restatable}
\begin{proof}
The proof follows directly from Lemma~\ref{theorem:mrc_ss} since for all $i \in [n]$, $\hat{\svbx}^{\texttt{mrc}}_i$ are independent of each other as well as unbiased.
\end{proof}


\subsection{Empirical Comparisons}
\label{appendix:mrc_ss_emp}
In this section, we compare $\MRC$ simulating $\SubsetSelection$ (using its approximate DP guarantee) against $\SubsetSelection$ and RHR for frequency estimation with $d = 500$ and $n = 5000$. We use the same data generation scheme described in Section \ref{subsec:mmrc_ss_empirical} and set $\delta = 10^{-6}$. As before, RHR uses $\#$-bits $= \varepsilon$ because it leads to a poor performance if $\#$-bits $ > \varepsilon$. We show the privacy-accuracy tradeoffs for these three methods in Figure \ref{fig:a_freq}. We see that $\MRC$ simulating $\SubsetSelection$ can attain the accuracy of the uncompressed $\SubsetSelection$ for the range of  $\varepsilon$'s typically considered by LDP mechanisms while only using $(3\varepsilon/ \ln 2) + 6$ bits. In comparison with the results from Section \ref{subsec:mmrc_ss_empirical}, the results in this section come with an approximate guarantee ($\delta = 10^{-6}$) and with a higher number of bits of communication. In other words, along with the obvious gains of pure privacy instead of approximate privacy, $\MMRC$ results in a lower communication cost (and therefore a lower computation cost) compared to $\MRC$.

\begin{figure}[h]
\centering
\includegraphics[width=0.45\linewidth]{figures/a_z_eps1.pdf}
\caption{Comparing $\SubsetSelection$, $\MRC$ simulating $\SubsetSelection$ and SQKR for frequency estimation in terms of $\ell_2$ error vs $\varepsilon$ with $d = 500$, $n = 5000$, and $\#$bits $= (3\varepsilon/ \ln 2) + 6$.}
\label{fig:a_freq}
\end{figure}