\section{Modified Minimal Random Coding}
\label{appendix:mmrc}

Let $q(\svbz|\svbx)$ be an $\varepsilon$-LDP cap-based mechanism (see definition \ref{def:cap}) for all $\svbx \in \cX$ and $\svbz \in \cZ$. Let $p(\svbz)$ be the uniform distribution over $\cZ$ and let $\{\svbz_k\}_{k=1}^{N}$ be $N$ candidates drawn from $p(\svbz)$. Let $\theta$ denote the fraction of candidates inside the $\msf{Cap}_{\svbx}$ associated with $q(\svbz|\svbx)$. Let $\piMMRC$ be the distribution over the indices $k \in [N]$ under modified minimal random coding $(\MMRC)$ obtained from Algorithm \ref{alg:mmrc}.
Recall that $\piMMRC(k)$ is bounded by an upper threshold $t_u$ and a lower threshold $t_l$ (Section~\ref{subsec:mmrc}),
\begin{align}
    t_u &= \frac{1}{N} \times \frac{c_1(\varepsilon, d)}{\Expectation[\theta] c_1(\varepsilon, d) + (1 - \Expectation[\theta]) c_2(\varepsilon, d)}, & 
    t_l &= \frac{1}{N} \times \frac{c_2(\varepsilon, d)}{\Expectation[\theta] c_1(\varepsilon, d) + (1 - \Expectation[\theta]) c_2(\varepsilon, d)}.
\end{align}

Similar to $\piMRC$, $\piMMRC$ can be be viewed as a function that maps $\svbx$ and $(\svbz_1,...,\svbz_N)$ to a distribution in $[N]$. However, to reduce clutter, we will generally omit the dependence on $\svbx$ and $(\svbz_1,...,\svbz_N)$. Further, since  $\piMMRC$ depends on $(\svbz_1,...,\svbz_N)$ only through $\theta$, we will sometimes show this dependence as $\piMMRC_{\svbx, \theta}$. 

Finally, let $\qMMRC$ denote the distribution of $\svbz_K$ where $K \sim \piMMRC$.
That is, with $\delta$ denoting the Dirac delta function:
\begin{align}
    \qMMRC(\svbz | \svbx) \coloneqq \sum_{k} \piMMRC(k) \delta(\svbz - \svbz_k). \label{eq:qmmrc}
\end{align}

% \textRed{Need to add the definition of step mechanisms.}

% \begin{algorithm}
% % \DontPrintSemicolon
% \KwInput{ $\theta, \Expectation[\theta], c_1, c_2, N$}
% \KwOutput{$\piMMRC_{\svbx, \theta}(\cdot)$}
% \KwInitialization{
% $\piMRC_{\svbx, \theta}(k) = 
%      \frac{1}{N} \frac{c_1}{c_2(\varepsilon,d) + \theta \times (c_1(\varepsilon,d) - c_2(\varepsilon,d))} $ for all $k$ such that $\svbz_k \in \msf{Cap}_{\svbx}$}
% \KwInitialization{
% $\piMRC_{\svbx, \theta}(k) = \frac{1}{N} \frac{c_2}{c_2(\varepsilon,d) + \theta \times (c_1(\varepsilon,d) - c_2(\varepsilon,d))} $ for all $k$ such that $\svbz_k \notin \msf{Cap}_{\svbx}$}
%     \If{$\theta = \Expectation[\theta]$}
%     {
%     $\piMMRC_{\svbx, \theta}(k) = \piMRC_{\svbx, \theta}(k)$; for all $k$\\
%     }
%     \ElseIf{$\theta > \Expectation[\theta]$}
%     { 
%     $\piMMRC_{\svbx, \theta}(k) = \piMRC_{\svbx, \theta}(k) \times \frac{c_2(\varepsilon,d) + \theta \times (c_1(\varepsilon,d) - c_2(\varepsilon,d))}{c_2(\varepsilon,d) + \Expectation[\theta] \times (c_1(\varepsilon,d) - c_2(\varepsilon,d))};$ for all $k$ such that $\svbz_k \notin \msf{Cap}_{\svbx}$\\
%     $\piMMRC_{\svbx, \theta}(k) = \piMRC_{\svbx, \theta}(k) \times \frac{c_2(\varepsilon,d) + \theta \times (c_1(\varepsilon,d) - c_2(\varepsilon,d))}{c_2(\varepsilon,d) + \Expectation[\theta] \times (c_1(\varepsilon,d) - c_2(\varepsilon,d))} \times \frac{\Expectation[\theta] \times c_1 + (\theta - \Expectation[\theta]) \times c_2}{\theta \times c_1}$; for all $k$ such that $\svbz_k \in \msf{Cap}_{\svbx}$\\
%     }
%     \ElseIf{$\theta < \Expectation[\theta]$}
%     {
%     $\piMMRC_{\svbx, \theta}(k) = \piMRC_{\svbx, \theta}(k) \times \frac{c_2(\varepsilon,d) + \theta \times (c_1(\varepsilon,d) - c_2(\varepsilon,d))}{c_2(\varepsilon,d) + \Expectation[\theta] \times (c_1(\varepsilon,d) - c_2(\varepsilon,d))} $; for all $k$ such that $\svbz_k \in \msf{Cap}_{\svbx}$\\
%     $\piMMRC_{\svbx, \theta}(k) = \piMRC_{\svbx, \theta}(k) \times \frac{c_2(\varepsilon,d) + \theta \times (c_1(\varepsilon,d) - c_2(\varepsilon,d))}{c_2(\varepsilon,d) + \Expectation[\theta] \times (c_1(\varepsilon,d) - c_2(\varepsilon,d))} \times \frac{(1 - \Expectation[\theta]) \times c_2 + (\Expectation[\theta] - \theta) \times c_1}{(1-\theta) \times c_2}$; for all $k$ such that $\svbz_k \notin \msf{Cap}_{\svbx}$\\
%     }
% \caption{Modifying $\piMRC_{\svbx, \theta}(\cdot)$ to $\piMMRC_{\svbx, \theta}(\cdot)$}
% \label{alg:tilde_pi}
% \end{algorithm}

\subsection{Privacy of \texorpdfstring{$\MMRC$}{MMRC}}\label{appendix:privacy_mmrc}
In this section, we prove Theorem \ref{theorem:mmrc_privacy} i.e., we show that $\piMMRC$ is a $\varepsilon$-LDP mechanism.

\mmrcprivacy*
\begin{proof}
For any $\varepsilon$-LDP cap-based $q(\cdot | \svbx)$, we have the following from \eqref{eq:ldp} and \eqref{eq:cap_mechanism}:
\begin{align}
 \frac{c_1(\varepsilon,d)}{c_2(\varepsilon,d)} \leq \exp(\varepsilon). \label{eq:cap-based-bound}
\end{align}
Further, the modification of $\piMRC$ to $\piMMRC$ ensures that \eqref{eq:modification} is true, that is,
\begin{align}
    t_l \leq \piMMRC(k) \leq t_u ~ \forall k \in [N].
\end{align}
Therefore, for any $\svbx, \svbx' \in \cX$ and $k \in [N]$, we have
\begin{align}
    \frac{\piMMRC_{\svbx}(k)}{\piMMRC_{\svbx'}(k)} & \leq \frac{t_u}{t_l} \stackrel{(a)}{=} \frac{c_1(\varepsilon,d)}{c_2(\varepsilon,d)} \stackrel{(b)}{\leq} \exp(\varepsilon),
\end{align}
where $(a)$ follows from the definitions of $t_u$ and $t_l$ and $(b)$ follows from \eqref{eq:cap-based-bound}.
\end{proof}

\subsection{Supporting Lemmas to prove the utility of \texorpdfstring{$\MMRC$}{MMRC}}
\label{appendix:supporting_lemma_mmrc_utility}

To prove Theorem~\ref{theorem:mmrc_accuracy_local_wrt_mrc} (Section \ref{appendix:utility_mmrc}), we prove that the expected KL divergence between $\piMRC$ and $\piMMRC$ can be controlled arbitrarily when the number of candidates is of the right order (Lemma \ref{lemma:expected_kl}).
%(where the expectation is over the fraction of candidates inside the $\msf{Cap}_{\svbx}$).
To prove Lemma~\ref{lemma:expected_kl}, we first show that the KL divergence between $\piMRC$ and $\piMMRC$, for a given fraction of candidates inside the $\msf{Cap}_{\svbx}$, can be bounded in terms of $\varepsilon$ (Lemma \ref{lemma:kl}).

\subsubsection{The KL divergence between \texorpdfstring{$\piMRC$}{} and \texorpdfstring{$\piMMRC$}{} is small}
\begin{lemma}\label{lemma:kl}
Let $q(\svbz|\svbx)$ be an $\varepsilon$-LDP cap-based mechanism. Let $p(\svbz)$ be the uniform distribution over $\cZ$ and let $\{\svbz_k\}_{k=1}^{N}$ be $N$ candidates drawn from $p(\svbz)$. Let $\theta$ denote the fraction of candidates inside the $\msf{Cap}_{\svbx}$ associated with $q(\svbz|\svbx)$. Let $\piMRC$ be the distribution over the indices $k \in [N]$ under $\MRC$ obtained from Algorithm \ref{alg:mrc} and $\piMMRC$ be the distribution over the indices $k \in [N]$ under $\MMRC$ obtained from Algorithm \ref{alg:mmrc}. Then,
\begin{align}
    \KLD{\piMRC_{\svbx, \theta}(\cdot)}{\piMMRC_{\svbx, \theta}(\cdot)} \leq \varepsilon \log e
\end{align}
\end{lemma}
\begin{proof}
We consider three different cases depending on whether $\theta = \Expectation[\theta]$, $\theta < \Expectation[\theta]$ or $\theta > \Expectation[\theta]$.
\begin{enumerate}
    \item[1.]  For $\theta = \Expectation[\theta]$, we have $\piMRC_{\svbx, \theta}(\cdot) = \piMMRC_{\svbx, \theta}(\cdot)$. Therefore,
    \begin{align}
    \KLD{\piMRC_{\svbx, \theta}(\cdot)}{\piMMRC_{\svbx, \theta}(\cdot)} = \KLD{\piMRC_{\svbx, \theta}(\cdot)}{\piMRC_{\svbx, \theta}(\cdot)} = 0 \leq \varepsilon \log e. \label{eq:KL_0}
    \end{align}
    \item[2.]
If $\theta < \Expectation[\theta]$, then $\piMRC$ violates the upper threshold $t_u$ so that $\piMMRC(k) = t_u$ for all $k \in \msf{Cap}_{\svbx}$ and we have
\begin{align}
    & \KLD{\piMRC_{\svbx, \theta}(\cdot)}{\piMMRC_{\svbx, \theta}(\cdot)} = \sum_{k} \piMRC_{\svbx, \theta}(k) \log \dfrac{\piMRC_{\svbx, \theta}(k)}{\piMMRC_{\svbx, \theta}(k)} \\
    % & = \frac{\theta c_1}{c_2(\varepsilon,d) +  \theta (c_1(\varepsilon,d) - c_2(\varepsilon,d))} \log \dfrac{c_2(\varepsilon,d) + \Expectation[\theta] \times (c_1(\varepsilon,d) - c_2(\varepsilon,d))}{c_2(\varepsilon,d) +  \theta \times (c_1(\varepsilon,d) - c_2(\varepsilon,d))} \\ 
    % & \qquad + \dfrac{(1-\theta)}{c_2(\varepsilon,d) +  \theta (c_1(\varepsilon,d) - c_2(\varepsilon,d))} \log \dfrac{(1-\theta) \times  (c_2(\varepsilon,d) + \Expectation[\theta] \times (c_1(\varepsilon,d) - c_2(\varepsilon,d)))}{(c_2(\varepsilon,d) +  \theta \times (c_1(\varepsilon,d) - c_2(\varepsilon,d))) \times ((1-\Expectation[\theta]) \times c_2 + (\Expectation[\theta] - \theta) c_1)} \\
    & \stackrel{(a)}{=} \sum_{k \in \msf{Cap}_{\svbx}} \frac{1}{N} \times \frac{ c_1(\varepsilon,d)}{c_2(\varepsilon,d) +  \theta (c_1(\varepsilon,d) - c_2(\varepsilon,d))} \log \dfrac{c_2(\varepsilon,d) + \Expectation[\theta] \times (c_1(\varepsilon,d) - c_2(\varepsilon,d))}{c_2(\varepsilon,d) +  \theta \times (c_1(\varepsilon,d) - c_2(\varepsilon,d))}\\ 
    &  \qquad  + \sum_{k \notin \msf{Cap}_{\svbx}} \frac{1}{N} \times \dfrac{c_2(\varepsilon,d)}{c_2(\varepsilon,d) +  \theta (c_1(\varepsilon,d) - c_2(\varepsilon,d))} \bigg[ \log \dfrac{c_2(\varepsilon,d)+ \Expectation[\theta] \times (c_1(\varepsilon,d) - c_2(\varepsilon,d))}{c_2(\varepsilon,d) +  \theta \times (c_1(\varepsilon,d) - c_2(\varepsilon,d))} \\
    &  \qquad \qquad + \log \dfrac{(1-\theta) \times c_2(\varepsilon,d)}{ (1-\Expectation[\theta]) \times c_2(\varepsilon,d) + (\Expectation[\theta] - \theta) c_1(\varepsilon,d)} \bigg] \\
    & \stackrel{(b)}{=} \dfrac{\theta c_1(\varepsilon,d)}{c_2(\varepsilon,d) +  \theta (c_1(\varepsilon,d) - c_2(\varepsilon,d))} \log \dfrac{c_2(\varepsilon,d) + \Expectation[\theta] \times (c_1(\varepsilon,d) - c_2(\varepsilon,d))}{c_2(\varepsilon,d) +  \theta \times (c_1(\varepsilon,d) - c_2(\varepsilon,d))} \\ 
    & \qquad + \dfrac{(1-\theta)c_2(\varepsilon,d)}{c_2(\varepsilon,d) +  \theta (c_1(\varepsilon,d) - c_2(\varepsilon,d))} \bigg[ \log \dfrac{c_2(\varepsilon,d)+ \Expectation[\theta] \times (c_1(\varepsilon,d) - c_2(\varepsilon,d))}{c_2(\varepsilon,d) +  \theta \times (c_1(\varepsilon,d) - c_2(\varepsilon,d))} \\
    &  \qquad \qquad + \log \dfrac{(1-\theta) \times c_2(\varepsilon,d)}{ (1-\Expectation[\theta]) \times c_2(\varepsilon,d) + (\Expectation[\theta] - \theta) c_1(\varepsilon,d)} \bigg]  \\
    & = \log \dfrac{c_2(\varepsilon,d) + \Expectation[\theta] \times (c_1(\varepsilon,d) - c_2(\varepsilon,d))}{c_2(\varepsilon,d) +  \theta \times (c_1(\varepsilon,d) - c_2(\varepsilon,d))} \\
    &  \qquad + \dfrac{(1-\theta)c_2(\varepsilon,d)}{c_2(\varepsilon,d) +  \theta (c_1(\varepsilon,d) - c_2(\varepsilon,d))} \bigg[ \log \dfrac{(1-\theta) \times c_2(\varepsilon,d)}{ (1-\Expectation[\theta]) \times c_2(\varepsilon,d) + (\Expectation[\theta] - \theta) c_1(\varepsilon,d)} \bigg]  \\
    & \stackrel{(c)}{\leq} \log \dfrac{c_2(\varepsilon,d) + \Expectation[\theta] \times (c_1(\varepsilon,d) - c_2(\varepsilon,d))}{c_2(\varepsilon,d) +  \theta \times (c_1(\varepsilon,d) - c_2(\varepsilon,d))} \\ & \stackrel{(d)}{\leq}  \log \bigg( \frac{c_2(\varepsilon,d) + \Expectation[\theta] \times (c_1(\varepsilon,d) - c_2(\varepsilon,d))}{c_2(\varepsilon,d)} \bigg) \label{eq:KL_1} \\
    & \stackrel{(e)}{\leq} \log \frac{c_1(\varepsilon,d)}{c_2(\varepsilon,d)}  \stackrel{(f)}{\leq} \varepsilon \log e
    % & = \log \dfrac{c_2(\varepsilon,d) + \Expectation[\theta] \times (c_1(\varepsilon,d) - c_2(\varepsilon,d))}{c_2(\varepsilon,d) +  \theta \times (c_1(\varepsilon,d) - c_2(\varepsilon,d))} + \dfrac{(1-\theta)}{c_2(\varepsilon,d) +  \theta (c_1(\varepsilon,d) - c_2(\varepsilon,d))} \log \dfrac{(1-\theta)}{ (1-\Expectation[\theta]) \times c_2 + (\Expectation[\theta] - \theta) c_1}
\end{align}
where $(a)$ follows from the definition of $\piMRC_{\svbx, \theta}(k)$ and $\piMMRC_{\svbx, \theta}(k)$, $(b)$ follows because $|\{k : k \in \msf{Cap}_{\svbx}\}| = \theta N$ and $|\{k : k \notin \msf{Cap}_{\svbx}\}| = (1-\theta) N$, $(c)$ follows because $\log \frac{(1-\theta) \times c_2(\varepsilon,d)}{ (1-\Expectation[\theta]) \times c_2(\varepsilon,d) + (\Expectation[\theta] - \theta) c_1(\varepsilon,d)} \leq 0$, $(d)$ follows because $\theta \geq 0$, $(e)$ follows because $\Expectation[\theta] \leq 1$, and $(f)$ follows because $c_1(\varepsilon,d) / c_2(\varepsilon,d) \leq \exp{(\varepsilon)}$.

\item[3.]
For $\theta > \Expectation[\theta]$, we have
\begin{align}
  & \KLD{\piMRC_{\svbx, \theta}(\cdot)}{\piMMRC_{\svbx, \theta}(\cdot)} = \sum_{\svbz_i} \piMRC_{\svbx, \theta}(k) \log \dfrac{\piMRC_{\svbx, \theta}(k)}{\piMMRC_{\svbx, \theta}(k)} \\
  & \stackrel{(a)}{=} \sum_{k \notin \msf{Cap}_{\svbx}} \frac{1}{N} \times  \dfrac{c_2(\varepsilon,d)}{c_2(\varepsilon,d) +  \theta (c_1(\varepsilon,d) - c_2(\varepsilon,d))} \log \dfrac{c_2(\varepsilon,d) + \Expectation[\theta] \times (c_1(\varepsilon,d) - c_2(\varepsilon,d))}{c_2(\varepsilon,d) +  \theta \times (c_1(\varepsilon,d) - c_2(\varepsilon,d))} \\ 
    & \qquad +  \sum_{k \in \msf{Cap}_{\svbx}} \frac{1}{N} \times  \dfrac{c_1(\varepsilon,d)}{c_2(\varepsilon,d) +  \theta (c_1(\varepsilon,d) - c_2(\varepsilon,d))} \bigg[ \log \dfrac{c_2(\varepsilon,d)+ \Expectation[\theta] \times (c_1(\varepsilon,d) - c_2(\varepsilon,d))}{c_2(\varepsilon,d) +  \theta \times (c_1(\varepsilon,d) - c_2(\varepsilon,d))} \\
    &  \qquad \qquad + \log \dfrac{\theta c_1(\varepsilon,d)}{\Expectation[\theta] c_1(\varepsilon,d) + (\theta - \Expectation[\theta]) \times c_2(\varepsilon,d)} \bigg] \\
    & \stackrel{(b)}{=}   \dfrac{(1-\theta) \times c_2(\varepsilon,d)}{c_2(\varepsilon,d) +  \theta (c_1(\varepsilon,d) - c_2(\varepsilon,d))} \log \dfrac{c_2(\varepsilon,d) + \Expectation[\theta] \times (c_1(\varepsilon,d) - c_2(\varepsilon,d))}{c_2(\varepsilon,d) +  \theta \times (c_1(\varepsilon,d) - c_2(\varepsilon,d))} \\ 
    & \qquad + \dfrac{\theta c_1(\varepsilon,d)}{c_2(\varepsilon,d) +  \theta (c_1(\varepsilon,d) - c_2(\varepsilon,d))} \bigg[ \log \dfrac{c_2(\varepsilon,d)+ \Expectation[\theta] \times (c_1(\varepsilon,d) - c_2(\varepsilon,d))}{c_2(\varepsilon,d) +  \theta \times (c_1(\varepsilon,d) - c_2(\varepsilon,d))}  \\
    &  \qquad \qquad + \log \dfrac{\theta c_1(\varepsilon,d)}{\Expectation[\theta] c_1(\varepsilon,d) + (\theta - \Expectation[\theta]) \times c_2(\varepsilon,d)} \bigg] \\
    & \stackrel{(c)}{\leq}  \dfrac{\theta c_1(\varepsilon,d)}{c_2(\varepsilon,d) +  \theta (c_1(\varepsilon,d) - c_2(\varepsilon,d))} \log \bigg( \dfrac{ \theta c_1(\varepsilon,d)}{\Expectation[\theta] c_1(\varepsilon,d) + (\theta - \Expectation[\theta]) \times c_2(\varepsilon,d)} \bigg) \label{eq:KL_2}\\
    & \stackrel{(d)}{\leq} \log \bigg( \dfrac{ c_1(\varepsilon,d)}{\Expectation[\theta] c_1(\varepsilon,d) + (1 - \Expectation[\theta]) \times c_2(\varepsilon,d)} \bigg)  \stackrel{(e)}{\leq} \log \frac{c_1(\varepsilon,d)}{c_2(\varepsilon,d)} \stackrel{(f)}{\leq} \varepsilon \log e  
    % & = \log \dfrac{c_2(\varepsilon,d) + \Expectation[\theta] \times (c_1(\varepsilon,d) - c_2(\varepsilon,d))}{c_2(\varepsilon,d) +  \theta \times (c_1(\varepsilon,d) - c_2(\varepsilon,d))} + \dfrac{\theta c_1}{c_2(\varepsilon,d) +  \theta (c_1(\varepsilon,d) - c_2(\varepsilon,d))} \log \dfrac{\theta c_1}{\Expectation[\theta] c_1 + \theta - \Expectation[\theta]}
\end{align}
where $(a)$ follows from the definition of $\piMRC_{\svbx, \theta}(k)$ and $\piMMRC_{\svbx, \theta}(k)$, $(b)$ follows because $|\{k : k \in \msf{Cap}_{\svbx}\}| = \theta N$ and $|\{k : k \notin \msf{Cap}_{\svbx}\}| = (1-\theta) N$, $(c)$ follows because $\log \dfrac{c_2(\varepsilon,d) + \Expectation[\theta] \times (c_1(\varepsilon,d) - c_2(\varepsilon,d))}{c_2(\varepsilon,d) +  \theta \times (c_1(\varepsilon,d) - c_2(\varepsilon,d))} \leq 0$, $(d)$ follows because $\theta \leq 1$, $(e)$ follows because $\Expectation[\theta] \geq 0$, and $(f)$ follows because $c_1(\varepsilon,d) / c_2(\varepsilon,d) \leq \exp{(\varepsilon)}$.
\end{enumerate}
\end{proof}

\subsubsection{The expected KL divergence between the distribution of indices in \texorpdfstring{$\MRC$}{MRC} and \texorpdfstring{$\MMRC$}{MMRC} can be controlled arbitrarily when \texorpdfstring{$N$}{N} is in the right order} 
\begin{lemma}\label{lemma:expected_kl}
Let $q(\svbz|\svbx)$ be an $\varepsilon$-LDP cap-based mechanism. Let $p(\svbz)$ be the uniform distribution over $\cZ$ and let $\{\svbz_k\}_{k=1}^{N}$ be $N$ candidates drawn from $p(\svbz)$. Let $\theta$ denote the fraction of candidates inside the $\msf{Cap}_{\svbx}$ associated with $q(\svbz|\svbx)$. Let $\piMRC$ be the distribution over the indices $k \in [N]$ under $\MRC$ obtained from Algorithm \ref{alg:mrc} and $\piMMRC$ be the distribution over the indices $k \in [N]$ under $\MMRC$ obtained from Algorithm \ref{alg:mmrc}. Then,
\begin{align}
    \Expectation_{\theta}\lb\KLD{\piMRC_{\svbx, \theta}(\cdot)}{\piMMRC_{\svbx, \theta}(\cdot)}\rb \leq \rho \times \log e \times ( 1  +  \varepsilon)
\end{align}
where $\rho \in (0,1)$ is a free variable that is related to $N$ as follows:
\begin{align}
    N = \frac{2\lp\exp(\varepsilon)-1\rp^2}{\rho^2}  \ln\frac{2}{\rho}.  
\end{align}
\end{lemma}
\begin{proof}
Let $\theta$ denote the fraction of candidates inside the cap, i.e.,
\begin{align}
\theta = \dfrac{1}{N} \sum_{k =1}^{N} \Indicator(\svbz_k \in \msf{Cap}_{\svbx}).
\end{align}
Therefore, we have
\begin{align}
\Expectation[\theta] = \Probability_{\svbz_k \sim \Unif(\cZ)}\lp \svbz_k \in \msf{Cap}_{\svbx} \rp = \Probability_{\svbz \sim \Unif(\cZ)}\lp \svbz \in \msf{Cap}_{\svbx} \rp. \label{eq:expec_prob_rel}
\end{align}
Now, using the Hoeffding's inequality, we have $\Probability\lbp \lba \theta- \E[\theta] \rba \geq \sqrt{\frac{\ln\lp 2/\rho \rp}{2N}} \rbp \leq \rho$. Letting $\hat{\rho} = \sqrt{\frac{\ln\lp 2/\rho \rp}{2N}}$, we have
\begin{align}
    & \Expectation_{\theta}\lb\KLD{\piMRC_{\svbx, \theta}(\cdot)}{\piMMRC_{\svbx, \theta}(\cdot)}\rb \\
    & \qquad = \sum_{\theta} \Probability(\theta) \times \KLD{\piMRC_{\svbx, \theta}(\cdot)}{\piMMRC_{\svbx, \theta}(\cdot)}\\
    & \qquad = \sum_{\theta : \lba \theta- \E[\theta] \rba \leq \hat{\rho}} \Probability(\theta) \KLD{\piMRC_{\svbx, \theta}(\cdot)}{\piMMRC_{\svbx, \theta}(\cdot)} +  \hspace{-5mm} \sum_{\theta : \lba \theta- \E[\theta] \rba > \hat{\rho}} \Probability(\theta) \KLD{\piMRC_{\svbx, \theta}(\cdot)}{\piMMRC_{\svbx, \theta}(\cdot)}
    % & \leq \sum_{\theta : \lba \theta- \E[\theta] \rba \leq \hat{\rho}} \Probability(\theta) \KLD{\piMRC_{\svbx, \theta}(\cdot)}{\piMMRC_{\svbx, \theta}(\cdot)} +  \varepsilon  \times \rho 
    \label{eq:expected_kl_1}
\end{align}
Now, we will upper bound $\KLD{\piMRC_{\svbx, \theta}(\cdot)}{\piMMRC_{\svbx, \theta}(\cdot)}$ whenever $\theta$ is such that $\lba \theta- \E[\theta] \rba \leq \hat{\rho}$. As in the proof of Lemma \ref{lemma:kl}, we have three different cases depending on whether $\theta = \Expectation[\theta]$, $\theta < \Expectation[\theta]$ or $\theta > \Expectation[\theta]$. 
\begin{enumerate}
\item[1.]  For $\theta = \Expectation[\theta]$, using \eqref{eq:KL_0}, we have $\KLD{\piMRC_{\svbx, \theta}(\cdot)}{\piMMRC_{\svbx, \theta}(\cdot)} = 0$. 
\item[2.]
For $\theta < \Expectation[\theta]$, using \eqref{eq:KL_1}, we have
\begin{align}
\KLD{\piMRC_{\svbx, \theta}(\cdot)}{\piMMRC_{\svbx, \theta}(\cdot)} & \leq  \log \dfrac{c_2(\varepsilon,d) + \Expectation[\theta] \times (c_1(\varepsilon,d) - c_2(\varepsilon,d))}{c_2(\varepsilon,d) +  \theta \times (c_1(\varepsilon,d) - c_2(\varepsilon,d))} \\
& \stackrel{(a)}{=} \log \dfrac{c_2(\varepsilon,d) + \Expectation[\theta] \times (c_1(\varepsilon,d) - c_2(\varepsilon,d))}{c_2(\varepsilon,d) +  \lp\Expectation[\theta] - t\rp \times (c_1(\varepsilon,d) - c_2(\varepsilon,d))} \\
& = \log \lp 1+ \dfrac{t \times (c_1(\varepsilon,d) - c_2(\varepsilon,d))}{c_2(\varepsilon,d) +  \lp\Expectation[\theta] - t\rp \times (c_1(\varepsilon,d) - c_2(\varepsilon,d))} \rp\\
& \stackrel{(b)}{\leq} \dfrac{\log e \times t \times (c_1(\varepsilon,d) - c_2(\varepsilon,d))}{c_2(\varepsilon,d) +  \lp\Expectation[\theta] - t\rp \times (c_1(\varepsilon,d) - c_2(\varepsilon,d))}\\
& \stackrel{(c)}{\leq} \log e \times t \times \lp \frac{c_1(\varepsilon,d) - c_2(\varepsilon,d)}{c_2(\varepsilon,d)} \rp\\ 
& \stackrel{(d)}{\leq} \log e \times \hat{\rho} \times \lp \frac{c_1(\varepsilon,d) - c_2(\varepsilon,d)}{c_2(\varepsilon,d)} \rp \label{eq:ekl_1}
\end{align}
where $(a)$ follows by letting $\theta = \Expectation[\theta] - t$ with $t > 0$, $(b)$ follows by using $\log (1+x) \leq x \log e$ for $x = \frac{t \times (c_1(\varepsilon,d) - c_2(\varepsilon,d))}{1+ \lp\Expectation[\theta] - t\rp \times (c_1(\varepsilon,d) - c_2(\varepsilon,d))} > 0$, $(c)$ follows because $\Expectation[\theta] - t = \theta \geq 0$, and $(d)$ follows because $t = \Expectation[\theta] - \theta \leq \hat{\rho}$.
\item[3.]
    For $\theta > \Expectation[\theta]$, using \eqref{eq:KL_2}, we have
\begin{align}
& \KLD{\piMRC_{\svbx, \theta}(\cdot)}{\piMMRC_{\svbx, \theta}(\cdot)} \\
& \qquad \leq \dfrac{\theta c_1(\varepsilon,d)}{c_2(\varepsilon,d) +  \theta (c_1(\varepsilon,d) - c_2(\varepsilon,d))} \log \bigg( \dfrac{ \theta c_1(\varepsilon,d)}{\Expectation[\theta] c_1(\varepsilon,d) + (\theta  - \Expectation[\theta] ) \times c_2(\varepsilon,d)} \bigg) \\
&  \qquad \stackrel{(a)}{\leq} \log \bigg( \dfrac{ \theta c_1(\varepsilon,d)}{\Expectation[\theta] c_1(\varepsilon,d) + (\theta  - \Expectation[\theta]) \times c_2(\varepsilon,d)} \bigg) \\
&  \qquad \stackrel{(b)}{=} \log \bigg( \dfrac{ (\Expectation[\theta] + t)c_1(\varepsilon,d)}{\Expectation[\theta] c_1(\varepsilon,d) + t c_2(\varepsilon,d)} \bigg) \\
& \qquad = \log \bigg(1+ \dfrac{ t\lp c_1(\varepsilon,d)-c_2(\varepsilon,d) \rp}{\Expectation[\theta] c_1(\varepsilon,d) + t c_2(\varepsilon,d)} \bigg) \\
& \qquad \stackrel{(c)}{\leq} \frac{\log e \times  t\lp c_1(\varepsilon,d)-c_2(\varepsilon,d) \rp}{\Expectation[\theta] c_1(\varepsilon,d) + t c_2(\varepsilon,d)} \\
& \qquad \stackrel{(d)}{\leq} \frac{\log e \times t\lp c_1(\varepsilon,d)-c_2(\varepsilon,d) \rp}{\Expectation[\theta] c_1(\varepsilon,d)} \stackrel{(e)}{\leq} \frac{\log e \times \hat{\rho}\lp c_1(\varepsilon,d)-c_2(\varepsilon,d) \rp}{\Expectation[\theta] c_1(\varepsilon,d)} \label{eq:ekl_2}
\end{align}
where $(a)$ follows because $\theta \leq 1$, $(b)$ follows by letting $\theta = \Expectation[\theta] + t$ with $t > 0$, $(c)$ follows by using $\log (1+x) \leq x \log e$ for $x = \frac{ t\lp c_1(\varepsilon,d)-c_2(\varepsilon,d) \rp}{\Expectation[\theta] c_1(\varepsilon,d) + t c_2(\varepsilon,d)} > 0$, $(d)$ follows because $t > 0$, and $(e)$ follows because $t = \theta - \Expectation[\theta]  \leq \hat{\rho}$.
\end{enumerate}
Therefore, for $\theta$ such that $\lba \theta- \E[\theta] \rba \leq \hat{\rho}$, we have the following from \eqref{eq:ekl_1} and \eqref{eq:ekl_2}:
\begin{align}
    \KLD{\piMRC_{\svbx, \theta}(\cdot)}{\piMMRC_{\svbx, \theta}(\cdot)} & \leq \dfrac{  \log e \times \hat{\rho}\lp c_1(\varepsilon,d)-c_2(\varepsilon,d) \rp}{\min \lbp c_2(\varepsilon,d), \Expectation[\theta] c_1(\varepsilon,d)\rbp} \\
    & \stackrel{(a)}{=} \dfrac{ \log e \times \hat{\rho}\lp c_1(\varepsilon,d)-c_2(\varepsilon,d) \rp}{\min \lbp c_2(\varepsilon,d), c_1(\varepsilon,d) \Probability_{\svbz \sim \Unif(\cZ)}\lp \svbz \in \msf{Cap}_{\svbx} \rp \rbp} \\
    & \stackrel{(b)}{\leq} \dfrac{ 2 \log e \times   \hat{\rho}\lp c_1(\varepsilon,d) - c_2(\varepsilon,d) \rp}{c_2(\varepsilon,d)} \\
    & \stackrel{(c)}{\leq} 2 \log e \times  \hat{\rho} \lp \exp(\varepsilon) - 1 \rp
    \label{eq:expected_kl_2}
\end{align}
where $(a)$ follows from \eqref{eq:expec_prob_rel}, $(b)$ follows because $\Probability_{\svbz \sim \Unif(\cZ)}\lp \svbz \in \msf{Cap}_{\svbx} \rp \geq c_2(\varepsilon,d)/2c_1(\varepsilon,d)$ from the definition of cap-based mechanisms, and $(c)$ follows because $c_1(\varepsilon,d) / c_2(\varepsilon,d) \leq \exp{(\varepsilon)}$.


Using \eqref{eq:expected_kl_2} and Lemma \ref{lemma:kl} in \eqref{eq:expected_kl_1}, we have
\begin{align}
 & \Expectation_{\theta}\lb\KLD{\piMRC_{\svbx, \theta}(\cdot)}{\piMMRC_{\svbx, \theta}(\cdot)}\rb \\
 & \qquad \leq \sum_{\theta : \lba \theta- \E[\theta] \rba \leq \hat{\rho}} \Probability(\theta) \times 2\log e \times \hat{\rho} \lp \exp(\varepsilon) - 1 \rp +  \sum_{\theta : \lba \theta- \E[\theta] \rba > \hat{\rho}} \Probability(\theta) \times \varepsilon \log e \\
 & \qquad \stackrel{(a)}{\leq} 2 \log e \times \hat{\rho} \lp \exp(\varepsilon) - 1 \rp + \rho \varepsilon \log e
 \label{eq:expected_kl_3} \\
  & \qquad \stackrel{(b)}{\leq} 2 \log e \times \sqrt{\dfrac{\ln\lp 2/\rho \rp}{2N}} \lp \exp(\varepsilon) - 1 \rp + \rho \varepsilon \log e\\
  & \qquad \stackrel{(c)}{\leq} \log e \times \rho (1+\varepsilon)
\end{align}
where $(a)$ follows because $\Probability \lp\lba \theta- \E[\theta] \rba \leq \hat{\rho}\rp \leq 1$ and $\Probability\lp\lba \theta- \E[\theta] \rba \geq \hat{\rho}\rp\ \leq \rho$, $(b)$ follows by plugging in $\hat{\rho} = \sqrt{\frac{\ln\lp 2/\rho \rp}{2N}}$, and $(c)$ follows by plugging in $N$.
\end{proof}

\subsection{Utility of \texorpdfstring{$\MMRC$}{MMRC}}\label{appendix:utility_mmrc}

In this section, we first prove Theorem \ref{theorem:mmrc_accuracy_local_wrt_mrc} i.e., we show that, with number of candidates exponential in $\varepsilon$, samples drawn from $\qMMRC$ will be similar to the samples drawn from $\qMRC$ in terms of $\ell_2$ error.

Then, in Theorem \ref{theorem:mmrc_accuracy_local}, we show that $\MMRC$ can simulate any $\varepsilon$-LDP cap-based mechanism in a nearly lossless fashion with about $\varepsilon$ bits of communication. 


% Recall Lemma \ref{lemma:expected_kl} from Appendix \ref{appendix:supporting_lemma_mmrc_utility} where we show that the expected KL divergence between $\piMRC$ and $\piMMRC$ can be controlled arbitrarily when the number of candidates is of the right order (where the expectation is over the fraction of candidates inside the $\msf{Cap}_{\svbx}$).

\subsubsection{Utility of \texorpdfstring{$\MMRC$}{MMRC} with respect to \texorpdfstring{$\qMRC$}{}}\label{appendix:utility_mmrc_mrc}
\mmrcaccuracylocalwrtmrc*
\begin{proof}
We will first upper bound the difference between $\Expectation_{\qMMRC} \big[ \lV  \svbz - \svbx \rV^2_2  \big]$ and $\Expectation_{\qMRC} \big[ \lV  \svbz - \svbx \rV^2_2  \big]$ in terms of the total variation distance between $\qMRC$ and $\qMMRC$. Due to a property of the total variation distance \citep[e.g.,][]{SCV16}, we have
\begin{align}
  \Expectation_{\qMMRC} \big[ \lV  \svbz - \svbx \rV^2_2  \big]   - \Expectation_{\qMRC}\big[ \lV  \svbz - \svbx \rV^2_2  \big] \leq \max_{\svbx, \svbz} \lV  \svbz- \svbx \rV^2_2  \times   \TV{\qMRC(\svbz | \svbx)}{\qMMRC(\svbz | \svbx)}. \label{eq:mmrc_generic_1}
\end{align}
Next, we will upper bound the total variation distance between $\qMRC$ and $\qMMRC$ using Pinsker's inequality as follows:
\begin{align}
  \TV{\qMRC(\svbz | \svbx)}{\qMMRC(\svbz | \svbx)} \leq \sqrt{\frac{1}{2 \log e} \KLD{\qMRC(\svbz | \svbx)}{\qMMRC(\svbz | \svbx)}}. \label{eq:mmrc_generic_2}
\end{align}
Next, we will upper bound the KL divergence between $\qMRC(\svbz | \svbx)$ and $\qMMRC(\svbz | \svbx)$. To that end, for every $\svbx \in \cX$, let $\pMRC(\svbz_1, \cdots, \svbz_N, K, \svbz_K | \svbx)$ denote the joint distribution of the candidates $\svbz_1, \cdots, \svbz_N$ drawn from $p(\rvbz)$,  the transmitted index $K$ under $\MRC$, and the sample $\svbz_K$ corresponding to $K$. We have
\begin{align}
& \pMRC(\svbz_1, \cdots, \svbz_N, K, \svbz_K | \svbx)\\
& \qquad =  p(\svbz_1, \cdots, \svbz_N | \svbx)  \times \pMRC(K | \svbz_1, \cdots, \svbz_N, \svbx) \times \pMRC(\svbz_K | \svbz_1, \cdots, \svbz_N, K, \svbx)\\
& \qquad \stackrel{(a)}{=} p(\svbz_1, \cdots, \svbz_N) \times \pMRC(K | \svbz_1, \cdots, \svbz_N, \svbx) \times \pMRC(\svbz_K | \svbz_1, \cdots, \svbz_N, K, \svbx)\\
& \qquad \stackrel{(b)}{=} p(\svbz_1, \cdots, \svbz_N) \times \piMRC_{\svbx, \theta}(k) \times \pMRC(\svbz_K | \svbz_1, \cdots, \svbz_N, K, \svbx)\\
& \qquad \stackrel{(c)}{=} p(\svbz_1, \cdots, \svbz_N) \times \piMRC_{\svbx, \theta}(k) \label{eq:mmrc_generic_3}
\end{align}
where $(a)$ follows because $\svbz_1, \cdots, \svbz_N$ are independent of $\svbx$, $(b)$ follows because $\pMRC(K | \svbz_1, \cdots, \svbz_N, \svbx) = \piMRC_{\svbx, \theta}(k)$, and $(c)$ follows because $\pMRC(\svbz_K | \svbz_1, \cdots, \svbz_N, K, \svbx) = 1$ (note that $\svbz_K$ can be viewed as a function of $(\svbz_1,...,\svbz_N, K)$). 

Similarly, for every $\svbx \in \cX$, let $\pMMRC(\svbz_1, \cdots, \svbz_N, K, \svbz_K | \svbx)$ denote the joint distribution of the candidates $\svbz_1, \cdots, \svbz_N$ drawn from $p(\rvbz)$,  the transmitted index $K$ under $\MMRC$, and the sample $\svbz_K$ corresponding to $K$. We have
\begin{align}
& \pMMRC(\svbz_1, \cdots, \svbz_N, K, \svbz_K | \svbx) \\
& \qquad =  p(\svbz_1, \cdots, \svbz_N | \svbx)  \times \pMMRC(K | \svbz_1, \cdots, \svbz_N, \svbx) \times \pMMRC(\svbz_K | \svbz_1, \cdots, \svbz_N, K, \svbx)\\
& \qquad \stackrel{(a)}{=} p(\svbz_1, \cdots, \svbz_N) \times \pMMRC(K | \svbz_1, \cdots, \svbz_N, \svbx) \times \pMMRC(\svbz_K | \svbz_1, \cdots, \svbz_N, K, \svbx)\\
& \qquad \stackrel{(b)}{=} p(\svbz_1, \cdots, \svbz_N) \times \piMMRC_{\svbx, \theta}(k) \times \pMMRC(\svbz_K | \svbz_1, \cdots, \svbz_N, K, \svbx)\\
& \qquad \stackrel{(c)}{=} p(\svbz_1, \cdots, \svbz_N) \times \piMMRC_{\svbx, \theta}(k) \label{eq:mmrc_generic_4}
% & \stackrel{()}{=} \lp\prod_{i=1}^{N} p(\svbz_i)\rp \times
\end{align}
where $(a)$ follows because $\svbz_1, \cdots, \svbz_N$ are independent of $\svbx$, $(b)$ follows because $\pMMRC(K | \svbz_1, \cdots, \svbz_N, \svbx) = \piMMRC_{\svbx, \theta}(k)$, and $(c)$ follows because $\pMMRC(\svbz_K | \svbz_1, \cdots, \svbz_N, K, \svbx) = 1$. 

We are now in a position to upper bound the KL divergence between $\qMRC(\svbz_K | \svbx)$ and $\qMMRC(\svbz_K | \svbx)$:
\begin{align}
 \KLD{\qMRC(\svbz | \svbx)}{\qMMRC(\svbz | \svbx)} &  \stackrel{(a)}{\leq}  \KLD{\pMRC(\svbz_1, \cdots, \svbz_N, K, \svbz_K | \svbx)}{\pMMRC(\svbz_1, \cdots, \svbz_N, K, \svbz_K | \svbx)}\\
 &  \stackrel{(b)}{=}   \KLD{p(\svbz_1, \cdots, \svbz_N) \times \piMRC_{\svbx, \theta}(k)}{p(\svbz_1, \cdots, \svbz_N) \times \piMMRC_{\svbx, \theta}(k)}\\
 &  \stackrel{(c)}{=} \Expectation_{\svbz_1, \cdots, \svbz_N} \lb\KLD{\piMRC_{\svbx, \theta}(k)}{\piMMRC_{\svbx, \theta}(k)}\rb \\
 &  \stackrel{(d)}{=}\Expectation_{\theta}\lb\KLD{\piMRC_{\svbx, \theta}(\cdot)}{\piMMRC_{\svbx, \theta}(\cdot)}\rb  \stackrel{(e)}{\leq} \rho \times \log e \times (1+\varepsilon) \label{eq:mmrc_generic_5}
\end{align}
where $(a)$ follows because by the chain rule for KL-divergence, $(b)$ follows from \eqref{eq:mmrc_generic_3} and \eqref{eq:mmrc_generic_4}, $(c)$ follows by the definition of KL-divergence, $(d)$ follows because $\piMRC$ and $\piMMRC$ depend on $\svbz_1, \cdots, \svbz_N$ only via $\theta$ for cap-based mechanisms, and $(e)$ follows from Lemma \ref{lemma:expected_kl} because $N = \frac{2\lp\exp(\varepsilon)-1\rp^2}{\rho^2}  \ln\frac{2}{\rho}  $. Combining \eqref{eq:mmrc_generic_1}, \eqref{eq:mmrc_generic_2},and \eqref{eq:mmrc_generic_5}, we have 
\begin{align}
    \Expectation_{\qMMRC} \big[ \lV  \svbz - \svbx \rV^2_2  \big]  \leq \Expectation_{\qMRC} \big[ \lV  \svbz - \svbx \rV^2_2  \big]  + \sqrt{\frac{\rho (1+\varepsilon)}{2}} \times \max_{\svbx, \svbz} \lV  \svbz - \svbx \rV^2_2. \label{eq:mrc_mmrc_relation}
\end{align}
\end{proof}

\begin{remark}\label{rmk:error_max}
For bounded $\varepsilon$-LDP mechanisms such as \emph{$\PrivUnit$} and \emph{$\SubsetSelection$}, the term $\max_{\svbx, \svbz} \lV  \svbz - \svbx \rV^2_2$ in \eqref{eq:mmrc_utility} is of the same order as $\Expectation_{q}\big[\|\svbz - \svbx\|^2\big]$. Therefore, by picking a large $N$ in Theorem \ref{theorem:mmrc_accuracy_local_wrt_mrc} (i.e. $\log N \geq C \varepsilon$ for a sufficiently large $C$), $\rho$ can be made arbitrarily small and the estimation error of \emph{$\MMRC$} can be arbitrarily close to the estimation error of \emph{$\MRC$}.
\end{remark}

\subsubsection{Utility of \texorpdfstring{$\MMRC$}{MMRC} with respect to \texorpdfstring{$q$}{q}}\label{appendix:utility_mmrc_q}

\begin{theorem}\label{theorem:mmrc_accuracy_local}
Consider any input alphabet $\cX$, output alphabet $\cZ$, data $\svbx \in \cX$, and $\varepsilon$-LDP cap-based mechanism $q(\cdot|\svbx)$.
Let the reference distribution $p(\cdot)$ be the uniform distribution on $\cZ$.
Let $N$ denote the number of candidates. Then, $\qMMRC$ is such that 
\begin{align}
    \Expectation_{\qMMRC} \big[ \lV  \svbz - \svbx \rV^2_2  \big]  \leq \Expectation_{q} \big[ \lV  \svbz - \svbx \rV^2_2  \big]  + \sqrt{\frac{\rho (1+\varepsilon)}{2}} \times \max_{\svbx, \svbz} \lV  \svbz - \svbx \rV^2_2  + \frac{2\alpha }{1-\alpha} \times \sqrt{\Expectation_{q}[\|\svbz - \svbx\|^4]}
\end{align}
holds with probability at least $1 - 2\alpha$ where
\begin{align}
\alpha = \sqrt{2^{-c\varepsilon} + 2^{-c^2/\log e + 1}}.
\end{align}
and $c$ and $\rho \in (0,1)$ are free variables such that
\begin{align}
    N = \max\bigg\{ 2^{(\log e + 4c) \varepsilon}, \frac{2\lp\exp(\varepsilon)-1\rp^2 }{\rho^2}  \ln\frac{2}{\rho} \bigg\}
\end{align}
\end{theorem}
\begin{proof}
The proof follows from Theorem \ref{theorem:mrc_accuracy_local} and Theorem \ref{theorem:mmrc_accuracy_local_wrt_mrc}.
\end{proof}
