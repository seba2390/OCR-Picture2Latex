\section{Simulating \texorpdfstring{$\PrivUnit$}{PrivUnit} using Minimal Random Coding}\label{appendix:mrc_pu}

In this section, we simulate $\PrivUnit$ using $\MRC$ analogous to how we simulate $\PrivUnit$ using $\MMRC$ in Section \ref{sec:mean_estimation}. First, in Appendix \ref{appendix:debias_mrc}, we provide an unbiased estimator for $\MRC$ simulating $\PrivUnit$.
Next, in Appendix \ref{appendix:mrc_pu_ut} we provide the utility guarantee associated with $\MRC$ simulating $\PrivUnit$. To do that, first, in Appendix \ref{appendix:scaling_mrc_pu}, we show that when the number of candidates $N$ is exponential in $\varepsilon$, the scaling factor $m_{\texttt{mrc}}$ is close to the scaling parameter associated with $\PrivUnit$ (i.e., $m_{\texttt{pu}}$). Next, in Appendix \ref{appendix:mrc_pu_scaling_mse}, we provide the relationship between the mean squared error associated with $\MRC$ simulating $\PrivUnit$ and the mean squared error associated with $\PrivUnit$. In Appendix \ref{appendix:mrc_pu_utility}, we combine everything and show that, for mean estimation, $\MRC$ can simulate $\PrivUnit$ in a near-lossless manner while only using on the order of $\varepsilon$-bits of communication. Finally, in Appendix \ref{appendix:mrc_pu_emp}, we provide some empirical comparisons.

% The minimal random coding technique relies on the conditional distribution of the output of the $\PrivUnit$ mechanism and therefore we start by obtaining a convenient expression for the same.

% \subsection{The conditional distribution of the output of the $\PrivUnit$ mechanism}


% \subsection{The distribution $\pi$ specialized to $\PrivUnit$}
% \label{subsec:mrc_pi_privunit}
% To apply the minimal random coding technique to $\PrivUnit$, let $\cZ \coloneqq \sphere^{d-1}$ and let the candidates $\svbz_1, \svbz_2,...,\svbz_N$ be generated from 
% $ p(\svbz) \coloneqq \msf{uniform}\lp \mcal{Z} \rp$. 
% Let $\theta$ fraction of these candidates that belong inside the $\msf{Cap}_{\svbx}$ i.e.,
% \begin{align}
%     \theta = \frac{1}{N}\sum_{k \in [N]} \Indicator(\svbz_k \in \msf{Cap}_{\svbx}) \label{eq:theta}
% \end{align}
% We can simplify the distribution $\pi$ over the indices $k \in [N]$ as follows:
% \begin{align}
%   \piMRC_{\svbx, \svbz_1,...,\svbz_N}(k) &  \coloneqq \dfrac{\qPU(\svbz_k|\svbx)/p(\svbz_k)}{\sum_{k'} \qPU(\svbz_{k'}|\svbx)/p(\svbz_{k'})} \\
%   & \stackrel{(a)}{=}  \dfrac{\qPU(\svbz_k|\svbx)}{\sum_{k'} \qPU(\svbz_{k'}|\svbx)} \\
%   & \stackrel{(b)}{=}
%   \begin{cases}
%      \dfrac{1}{N} \dfrac{c_1(\varepsilon, d)}{\theta c_1(\varepsilon, d) + (1-\theta) c_2(\varepsilon, d)} & \text{if}\ \svbz_k \in \msf{Cap}_{\svbx}
%       \\[10pt]
%      \dfrac{1}{N} \dfrac{c_2(\varepsilon, d)}{\theta c_1(\varepsilon, d) + (1-\theta) c_2(\varepsilon, d)} & \text{if}\ \svbz_k \notin \msf{Cap}_{\svbx}
%     \end{cases}\label{eq:pi_mrc}
% \end{align}
% where $(a)$ follows because $p(\svbz) \coloneqq \msf{uniform}\lp \mcal{Z} \rp$ and $(b)$ follows from \eqref{eq:privunit_density} and \eqref{eq:theta}. There are two important takeaways here : (a) for a given $\svbx, \svbz_1,...,\svbz_N$, $\piMRC_{\svbx, \svbz_1,...,\svbz_N}(k)$ takes only two values; (b) for a given $\svbx, \svbz_1,...,\svbz_N$ and $k$, $\piMRC_{\svbx, \svbz_1,...,\svbz_N}(k)$ depends only on whether $\svbz_k \in \msf{Cap}_{\svbx}$ or $\svbz_k \notin \msf{Cap}_{\svbx}$. As described in Section \ref{sec:mrc}, the MRC scheme $\qMRC(\svbz | \svbx)$ samples a candidate $\svbz_K \in \{\svbz_k\}_{k=1}^{N}$ where $K\sim \piMRC_{\svbx, \svbz_1,...,\svbz_N}(k)$.



\subsection{Unbiased Minimal Random Coding simulating \texorpdfstring{$\PrivUnit$}{PrivUnit}}\label{appendix:debias_mrc}
Consider the $\PrivUnit$ $\varepsilon$-LDP mechanism $\qPU$ described in Section \ref{sec:preliminaries} with parameters $p_0$ and $\gamma$. $\PrivUnit$ is a cap-based mechanism with $\msf{Cap}_{\svbx} = \{\svbz \in \sphere^{d-1} \mid
\lan \svbz, \svbx  \ran \ge \gamma\}$ as discussed in Appendix \ref{appendix:privunit}. Let $\piMRC$ be the distribution and $\svbz_1, \svbz_2,...,\svbz_N$ be the candidates obtained from Algorithm \ref{alg:mrc} when the reference distribution is $\Unif(\sphere^{d-1})$. Let  $K \sim \piMRC(\cdot)$. Define $p_{\texttt{mrc}} \coloneqq \Probability(\svbz_K \in \msf{Cap}_{\svbx})$
% Let $p_{\texttt{mrc}} = \Probability(\svbz_K \in \msf{Cap}_{\svbx}_{\varepsilon, d}(\svbx))$
to be the probability with which the sampled candidate $\svbz_K$ belongs to the spherical cap associated with $\PrivUnit$.
Define $m_{\texttt{mrc}}$ as the scaling factor in \eqref{eq:m} when $p_0$ in \eqref{eq:m} is replaced by $p_{\texttt{mrc}}$. Define $\hat{\svbx}^\texttt{mrc} \coloneqq \svbz_K / m_\texttt{mrc}$ as the estimator of the $\MRC$ mechanism simulating $\PrivUnit$.  The following Lemma shows that $\hat{\svbx}^\texttt{mrc}$ is an unbiased estimator.
% As in $\PrivUnit$, the sampled candidate  $\svbz_K$ needs to be scaled by an appropriate factor to ensure unbiasedness of the MRC scheme that simulates $\PrivUnit(\svbx,\gamma, p)$. As we show in the following theorem, this scaling factor is equal to
% \begin{align}
%     m_\texttt{mrc} \coloneqq m(p_{\texttt{mrc}})
% \end{align}
% is analogous to $\PrivUnit$'s scaling factor in \eqref{eqn:norm-of-W} i.e.,
% \begin{align}
%   m_\texttt{mrc} = \frac{(1 - \gamma^2)^\alpha}{2^{d-2} (d - 1)}
%   \left[\frac{p_{\texttt{mrc}}}{B(\alpha,\alpha) - B(\tau; \alpha,\alpha)}
%     - \frac{1 - p_{\texttt{mrc}}}{B(\tau; \alpha, \alpha)}\right]
%   \label{eq:hat_m} 
% \end{align}
% where $p_{\texttt{mrc}} = \Probability(\svbz_K \in \msf{Cap}_{\svbx})$ is the probability with which the sampled candidate $\svbz_K$ belongs to the spherical cap $\msf{Cap}_{\svbx})$ and $m(\cdot)$ is as defined in \eqref{eq:function_m}. In other words, $m_\texttt{mrc}$ has the same functional form as $m$ except $p_{\texttt{mrc}}$ replaces $p$. Further, in this case, we denote the final estimator as $\hat{\svbx}^\texttt{mrc} = \svbz_K / m_\texttt{mrc}$. 

% (\svbx,\gamma, p)$.


\begin{lemma}\label{theorem:mrc_privunit_bias}
Let $\hat{\svbx}^\texttt{mrc}$ be the estimator of the \emph{$\MRC$} mechanism simulating \emph{$\PrivUnit$} as defined above. Then, $\Expectation_{\qMRC}[\hat{\svbx}^\texttt{mrc}] = \svbx$.
\end{lemma}
\begin{proof}
For $k \in [N]$, let $A_k \coloneqq \Indicator(\svbz_k \in \msf{Cap}_{\svbx})$. Then, $p_{\texttt{mrc}} = \Probability(A_K = 1)$. Using the definition of $\hat{\svbx}^\texttt{mrc}$, we have
\begin{align}
\Expectation_{\qMRC} [\hat{\svbx}^\texttt{mrc}] & = \frac{1}{m_\texttt{mrc}} \Expectation_{\qMRC} [\svbz_K ].
\end{align}
Let us evaluate $\Expectation_{\qMRC} [\svbz_K ]$. We have
\begin{align}
\Expectation_{\qMRC} [\svbz_K ] & \stackrel{(a)}{=} \Expectation_{K, \svbz_1, \cdots, \svbz_N} [\svbz_K ] \\
& \stackrel{(b)}{=}  \Expectation_{\svbz_1, \cdots, \svbz_N} \Big[\sum_{k = 1}^{N}  \piMRC_{\svbx, \svbz_1,...,\svbz_N}(k) \times \svbz_k \Big] \\
& \stackrel{(c)}{=} \Expectation_{A_1, \cdots,A_N}\bigg[ \Expectation_{\svbz_1, \cdots, \svbz_N} \Big[\sum_{k = 1}^{N} \piMRC_{\svbx, \svbz_1,...,\svbz_N}(k) \times \svbz_k \Big| A_1, \cdots,A_N\Big]\bigg] \\
& \stackrel{(d)}{=} \sum_{k = 1}^{N} \Expectation_{A_1, \cdots,A_N}\bigg[   \Expectation_{\svbz_1, \cdots, \svbz_N} \Big[ \piMRC_{\svbx, \svbz_1,...,\svbz_N}(k) \times \svbz_k  \Big| A_1, \cdots,A_N\Big]\bigg] \\
& \stackrel{(e)}{=} \sum_{k = 1}^{N} \Expectation_{A_1, \cdots,A_N}\bigg[  \piMRC_{\svbx, A_1,...,A_N}(k) \Expectation_{\svbz_1, \cdots, \svbz_N} \Big[ \svbz_k  \Big| A_1, \cdots,A_N\Big]\bigg] \\
& \stackrel{(f)}{=} \sum_{k = 1}^{N} \Expectation_{A_1, \cdots,A_N}\bigg[\piMRC_{\svbx, A_1,...,A_N}(k) \Expectation_{\svbz_1, \cdots, \svbz_N} \Big[ \svbz_k  \Big| A_k \Big]\bigg] \\
& \stackrel{(g)}{=} \sum_{k = 1}^{N} \Expectation_{A_1, \cdots,A_N}\bigg[ \piMRC_{\svbx, A_1,...,A_N}(k) \Expectation_{\svbz_k} \Big[ \svbz_k  \Big| A_k\Big]\bigg] \\
& \stackrel{(h)}{=} \sum_{k = 1}^{N} \Expectation_{A_k} \bigg[\Expectation_{A_1, \cdots,A_N}\Big[ \piMRC_{\svbx, A_1,...,A_N}(k) \Expectation_{\svbz_k} \big[ \svbz_k  \big| A_k\big] \Big| A_k \Big] \bigg]\\
& \stackrel{(i)}{=} \sum_{k = 1}^{N} \Probability(A_k = 1) \bigg[\Expectation_{A_1, \cdots,A_N}\Big[ \piMRC_{\svbx, A_1,...,A_N}(k) \Expectation_{\svbz_k} \big[ \svbz_k  \big| A_k\big] \Big| A_k = 1\Big] \bigg] \\
& + \sum_{k = 1}^{N} \Probability(A_k = 0) \bigg[\Expectation_{A_1, \cdots,A_N}\Big[ \piMRC_{\svbx, A_1,...,A_N}(k) \Expectation_{\svbz_k} \big[ \svbz_k  \big| A_k\big] \Big| A_k = 0\Big] \bigg] \\
& = \sum_{k = 1}^{N} \Probability(A_k = 1) \bigg[\Expectation_{A_1, \cdots,A_N}\Big[\piMRC_{\svbx, A_1, \cdots,A_k = 1, \cdots, A_N}(k) \Expectation_{\svbz_k} \big[ \svbz_k  \big| A_k = 1\big] \Big] \bigg] \\
&  + \sum_{k = 1}^{N} \Probability(A_k = 0) \bigg[\Expectation_{A_1, \cdots,A_N}\Big[ \piMRC_{\svbx, A_1, \cdots,A_k = 0, \cdots, A_N}(k) \Expectation_{\svbz_k} \big[ \svbz_k  \big| A_k = 0\big] \Big] \bigg]\\
& \stackrel{(j)}{=} \Expectation_{\svbz}\big[ \svbz  \big| A = 1\big]  \sum_{k = 1}^{N} \Probability(A_k = 1) \bigg[\Expectation_{A_1, \cdots,A_N}\Big[ \piMRC_{\svbx, A_1, \cdots,A_k = 1, \cdots, A_N}(k)  \Big] \bigg] \\
&  + \Expectation_{\svbz}\big[ \svbz  \big| A = 0\big] \sum_{k = 1}^{N} \Probability(A_k = 0) \bigg[\Expectation_{A_1, \cdots,A_N}\Big[ \piMRC_{\svbx, A_1, \cdots,A_k = 0, \cdots, A_N}(k) \Big] \bigg]\\
& \stackrel{(k)}{=} \Expectation_{\svbz}\big[ \svbz  \big| A = 1\big]  \sum_{k = 1}^{N} \Probability(A_k = 1) \piMRC_{\svbx, A_k = 1}(k) \\
&+  \Expectation_{\svbz}\big[ \svbz  \big| A = 0\big]  \sum_{k = 1}^{N} \Probability(A_k = 0) \piMRC_{\svbx, A_k = 0}(k) \\
& \stackrel{(l)}{=} \Expectation_{\svbz}\big[ \svbz  \big| A = 1\big] \Probability(A_K = 1) + \Expectation_{\svbz}\big[ \svbz  \big| A = 0\big] \Probability(A_K = 0)\\
& \stackrel{(m)}{=} m_\texttt{mrc} \svbx \label{eq:miracle_zero_bias}
\end{align}
where $(a)$ follows because the randomness in $\qMRC$ comes from the randomness in $K, \svbz_1, \cdots, \svbz_N$, $(b)$ follows by calculating the expectation over $K$ and showing the dependence of $\piMRC$ on $\svbz_1, \cdots, \svbz_N$ explicitly, $(c)$ follows by the tower property of expectation, $(d)$ follows by linearity of expectation, $(e)$ follows because $\piMRC_{\svbx, \svbz_1,...,\svbz_N}(k) = \piMRC_{\svbx, A_1,...,A_N}(k)$ since $\piMRC$ depends on $\svbz_1,...,\svbz_N$ via $A_1, \cdots,A_N$, $(f)$ follows because $\svbz_k$ is independent of $A_1, \cdots,A_{k-1}, A_{k+1}, \cdots,A_N$ given $A_k$, $(g)$ follows by marginalizing $\svbz_1, \cdots, \svbz_{k-1}, \svbz_{k+1}, \cdots, \svbz_N$, $(h)$ follows by the tower property of expectation, $(i)$ follows by evaluating the expectation over $A_k$, $(j)$ follows because $\Expectation_{\svbz} \big[ \svbz \big| A = 1\big] \coloneqq \Expectation_{\svbz_k} \big[ \svbz_k  \big| A_k = 1\big]$ and $\Expectation_{\svbz} \big[ \svbz \big| A = 0\big] \coloneqq \Expectation_{\svbz_k} \big[ \svbz_k  \big| A_k = 0\big]$ are constants for every $k \in [N]$, $(k)$ follows by marginalizing $A_1, \cdots, A_N$, $(l)$ follows from the definitions of $\Probability(A_K = 1)$ and $\Probability(A_K = 0)$, and $(m)$ follows from rotational symmetry (see the proof of Lemma 4.1 in \cite{BDFKR2018} for details). Therefore, we can write
\begin{align}
\Expectation_{\qMRC} [\hat{\svbx}^\texttt{mrc}] & = \frac{1}{m_\texttt{mrc}} \Expectation_{\qMRC} [\svbz_K ] \stackrel{(a)}{=} \svbx
\end{align}
where $(a)$ follows from \eqref{eq:miracle_zero_bias}.
\end{proof}


% \subsection{Minimal Random Coding Simulating PrivUnit}
\subsection{Utility of Minimal Random Coding simulating \texorpdfstring{$\PrivUnit$}{PrivUnit}}\label{appendix:mrc_pu_ut}

\subsubsection{The scaling factors of \texorpdfstring{$\PrivUnit$}{PrivUnit} and \texorpdfstring{$\MRC$}{MRC} are close when \texorpdfstring{$N$}{N} is of the right order}\label{appendix:scaling_mrc_pu}

% Now define the sampling weights 
% $$\pi(k|\svbx, (\svbz_1,...,\svbz_N)) \coloneqq \dfrac{\qPU(\svbz_k | \svbx)/p(\svbz_k)}{\sum_{k'} \qPU(\svbz_{k'} | \svbx)/p(\svbz_{k'})}$$
% for all $k \in [N]$\footnote{Note that $\pi$ can be viewed as a function that maps $\svbx$ and $(\svbz_1,...,\svbz_N)$ to a law in $[N]$. However for notational convenience, when the context is clear, we will omit the dependence on $\svbx$ and $(\svbz_1,...,\svbz_N)$.}. The corresponding MRC scheme can be written as $\svbz_K$ with $K\sim \pi(k)$. Note that the law of $\svbz_K$ is 
% $$ \qMRC(\svbz | \svbx) \coloneqq \sum_{k} \pi(k)\cdot\bbm{1}_{\lbp \svbz = \svbz_k \rbp}.$$



% \subsubsection{Relationship between $N$ and the debiasing scaling factors}
% \begin{lemma}\label{lemma:mrc_privunit_approximation_error}
% Let $\check{p}_0$ be such that $1/2 < \check{p}_0 \leq p_0$. Let $\lambda \coloneqq \frac{p_0 - \check{p}_0}{\check{p}_0 - 1/2}$. Then, $\check{m} \coloneqq m(\check{p}_0)$ is such that 
% \begin{align}
%     m - \check{m} \leq \lambda\cdot\check{m}
% \end{align}
% \end{lemma}
% \begin{lemma}\label{lemma:mrc_privunit_approximation_error}
% Suppose $\exists \lambda > 0$ such that $\lambda = \frac{p_0 - \check{p}_0}{\check{p}_0 - 1/2}$. Then, $\check{m} \coloneqq m(\check{p}_0)$ is such that 
% \begin{align}
%     m - \check{m} \leq \lambda\cdot\check{m}
% \end{align}
% \end{lemma}
% \begin{proof}
% Following the proofs of Lemma 4.1 and Proposition 4 in \cite{BDFKR2018}, we can write $m = \gamma_+ p_0 + \gamma_-(1-p_0)$ and $\check{m} = \gamma_+ \check{p}_0 + \gamma_-(1-\check{p}_0)$ where
% \begin{align}
%     \gamma_+ \eqDef \frac{\lp 1-\gamma^2\rp^\alpha}{2^{d-2}(d-1)\lp B(\alpha, \alpha)-B(\tau; \alpha, \alpha) \rp}, \qquad \mathrm{and} \qquad 
%     \gamma_- \eqDef \frac{\lp 1-\gamma^2\rp^\alpha}{2^{d-2}(d-1)\lp B(\tau; \alpha, \alpha) \rp}.
% \end{align}
% Therefore, we have
% \begin{align}
%   \frac{1}{\check{m}} - \frac{1}{m}=
%   \frac{m-\check{m}}{m\cdot \check{m}} = \frac{1}{m} \lp \frac{(\gamma_+-\gamma_-) \cdot \lp p_0 - \check{p}_0 \rp}{\lp (\gamma_+ - \gamma_-) \check{p}_0+\gamma_-\rp} \rp = \frac{1}{m}\lp\frac{ p_0 - \check{p}_0 }{ \check{p}_0 +\frac{\gamma_-}{\gamma_+-\gamma_-}}\rp \label{eq:mrc_privunit_approximation_error_0}
% \end{align}
% From \cite{BDFKR2018}, we have $\gamma_- \leq 0 \leq \gamma_+$ and $\lba \gamma_+ \rba \geq \lba \gamma_- \rba$. These inequalities imply $\frac{\gamma_-}{\gamma_+-\gamma_-} \geq -\frac{1}{2}$. Plugging this in \eqref{eq:mrc_privunit_approximation_error_0},
% \begin{align}
%     \frac{1}{\check{m}} - \frac{1}{m} \leq \frac{1}{m}\lp\frac{ p_0 - \check{p}_0 }{ \check{p}_0 -\frac{1}{2}}\rp  \stackrel{(a)}{=} \frac{\lambda}{m} \label{eq:mrc_privunit_approximation_error_1}
% \end{align}
% where $(a)$ follows from the definition of $\lambda$. Rearranging \eqref{eq:mrc_privunit_approximation_error_1} completes the proof.
% \end{proof}
In the following Lemma, we show that when the number of candidates $N$ is exponential in $\varepsilon$, then the scaling parameters associated with $\PrivUnit$ and the $\MRC$ scheme simulating $\PrivUnit$ are close.
\begin{lemma}\label{lemma:mrc_privunit_approximation_error}
Let $N$ denote the number of candidates used in the \emph{$\MRC$} scheme. Let $K \sim \piMRC$ where $\piMRC$ is the distribution over the indices $[N]$ associated the \emph{$\MRC$} scheme simulating \emph{$\PrivUnit(\svbx,\gamma, p_0)$}. Consider any $\lambda > 0$.
Then, the scaling factor $m_{\texttt{pu}}$ associated with \emph{$\PrivUnit$} and the scaling factor $m_\texttt{mrc}$ associated with the \emph{$\MRC$} scheme simulating \emph{$\PrivUnit$} are such that
\begin{align}
    m_{\texttt{pu}} - m_\texttt{mrc} \leq \lambda\cdot m_\texttt{mrc}
\end{align}
as long as
\begin{align}
    N \geq   2e^{2\varepsilon}\lp\frac{2 (1+\lambda)}{\lambda \lp p_0 -1/2 \rp}\rp^2 \ln\lp \frac{4(1+\lambda)}{\lambda \lp p_0 -1/2 \rp} \rp.
\end{align}
\end{lemma}
\begin{proof}
Following the proofs of Lemma 4.1 and Proposition 4 in \cite{BDFKR2018}, we can write $m_{\texttt{pu}} = \gamma_+ p_0 + \gamma_-(1-p_0)$ and $m_\texttt{mrc} = \gamma_+ p_{\texttt{mrc}} + \gamma_-(1-p_{\texttt{mrc}})$ where
\begin{align}
    \gamma_+ \eqDef \frac{\lp 1-\gamma^2\rp^\alpha}{2^{d-2}(d-1)\lp B(\alpha, \alpha)-B(\tau; \alpha, \alpha) \rp}, \qquad \mathrm{and} \qquad 
    \gamma_- \eqDef \frac{\lp 1-\gamma^2\rp^\alpha}{2^{d-2}(d-1)\lp B(\tau; \alpha, \alpha) \rp}.
\end{align}
Therefore, we have
\begin{align}
  \frac{1}{m_\texttt{mrc}} - \frac{1}{m_{\texttt{pu}}} & =
  \frac{m_{\texttt{pu}}-m_\texttt{mrc}}{m_{\texttt{pu}} \cdot m_\texttt{mrc}} = \frac{1}{m_{\texttt{pu}}} \lp \frac{(\gamma_+-\gamma_-) \cdot \lp p_0 - p_{\texttt{mrc}} \rp}{\lp (\gamma_+ - \gamma_-) p_{\texttt{mrc}}+\gamma_-\rp} \rp \\
  & = \frac{1}{m_{\texttt{pu}}}\lp\frac{ p_0 - p_{\texttt{mrc}} }{ p_{\texttt{mrc}} +\dfrac{\gamma_-}{\gamma_+-\gamma_-}}\rp \label{eq:mrc_privunit_approximation_error_0}
\end{align}
From \cite{BDFKR2018}, we have $\gamma_- \leq 0 \leq \gamma_+$ and $\lba \gamma_+ \rba \geq \lba \gamma_- \rba$. These inequalities imply $\frac{\gamma_-}{\gamma_+-\gamma_-} \geq -\frac{1}{2}$. Plugging this in \eqref{eq:mrc_privunit_approximation_error_0}, we have
\begin{align}
    \frac{1}{m_\texttt{mrc}} - \frac{1}{m_{\texttt{pu}}} \leq \frac{1}{m_{\texttt{pu}}}\lp\frac{ p_0 - p_{\texttt{mrc}} }{ p_{\texttt{mrc}} - 1/2}\rp = \frac{1}{m_{\texttt{pu}}}\lp\frac{ 1 }{ \dfrac{p_0 - 1/2}{p_0 - p_{\texttt{mrc}}}-1}\rp
    % \stackrel{(a)}{\leq} \frac{\lambda}{m} 
    \label{eq:mrc_privunit_approximation_error_1}
\end{align}
% where $(a)$ is true if $p - p_{\texttt{mrc}} \leq \lambda( p_{\texttt{mrc}} -1/2)$. Therefore, a sufficient condition for $m - m_\texttt{mrc} \leq \lambda\cdotm_\texttt{mrc}$ is $p - p_{\texttt{mrc}} \leq \lambda( p_{\texttt{mrc}} -1/2)$ which can be re-arranged as follows:
% \begin{align}
%     p - p_{\texttt{mrc}} \leq  \frac{\lambda(p -1/2)}{1+\lambda}
% \end{align}
We will now upper bound $\dfrac{p_0 - p_{\texttt{mrc}}}{p_0 - 1/2}$. We start by obtaining convenient expressions for $p_{\texttt{mrc}}$ and $p_0$. To compute $p_{\texttt{mrc}} = \Probability(\svbz_K \in \msf{Cap}_{\svbx})$, recall that $\theta$ denotes the fraction of candidates that belong inside the $\msf{Cap}_{\svbx}$. Let $c_1(\varepsilon, d)$ and $c_2(\varepsilon, d)$ be as defined in \eqref{eq:privunit_density}. Let $\bar{c}_1(\varepsilon, d) = c_1(\varepsilon, d) \times A(1,d)$ and $\bar{c}_2(\varepsilon, d) = c_2(\varepsilon, d) \times A(1,d)$. It is easy to see from Algorithm \ref{alg:privunit} and \eqref{eq:privunit_density} that $\Probability(\svbz_k \in \msf{Cap}_{\svbx}) =  \bar{c}_1(\varepsilon, d) / p_0$. 
% let $\rvv \in [N]$ denote the random variable that indicates the number of $\svbz_k$, $k=1,...,N$ such that $\svbz_k \in \msf{Cap}_{\svbx}$ i.e., $\rvv = N \theta$ where $\theta$ is defined in \eqref{eq:theta}. 
Further, since $\svbz_k$ are generated uniformly at random,
$$ \theta \sim \frac{1}{N} \msf{Binom}\lp N, \frac{ \bar{c}_1(\varepsilon, d)}{p_0}\rp,$$ so we have
\begin{align}
    p_{\texttt{mrc}} = \Pr\lbp \svbz_K \in \msf{Cap}_{\svbx} \rbp  & = \E\lb \Pr\lbp \svbz_K \in \msf{Cap}_{\svbx}| \theta \rbp \rb  \\
    % = \E_{\rvv}\lb \frac{  \bar{c}_1\rvv }{  \bar{c}_1\rvv + \bar{c}_2(N-\rvv)} \rb  & 
    & \stackrel{(a)}{=} \E\lb \frac{ \bar{c}_1(\varepsilon, d) \theta}{ \bar{c}_1(\varepsilon, d) \theta +  \bar{c}_2(\varepsilon, d)(1-\theta)} \rb \\
    % & \stackrel{(b)}{=} \E\lb \frac{e^\varepsilon \theta}{e^\varepsilon \theta + (1-\theta)} \rb \\
    % & \stackrel{(c)}{=} \frac{e^\varepsilon}{(e^\varepsilon-1)^2}\E\lb e^\varepsilon-1-\frac{1}{\theta+\frac{1}{e^\varepsilon-1}} \rb,
    & = \frac{ \bar{c}_1(\varepsilon, d)}{ \bar{c}_1(\varepsilon, d)- \bar{c}_2(\varepsilon, d)}\E\lb \frac{( \bar{c}_1(\varepsilon, d) -  \bar{c}_2(\varepsilon, d))\theta}{( \bar{c}_1(\varepsilon, d) -  \bar{c}_2(\varepsilon, d))\theta +  \bar{c}_2(\varepsilon, d)} \rb \\
    & \stackrel{(b)}{=} \frac{ \bar{c}_1(\varepsilon, d) \bar{c}_2(\varepsilon, d)}{( \bar{c}_1(\varepsilon, d)- \bar{c}_2(\varepsilon, d))^2}\E\lb \frac{ \bar{c}_1(\varepsilon, d)- \bar{c}_2(\varepsilon, d)}{ \bar{c}_2(\varepsilon, d)}-\frac{1}{\theta+\dfrac{ \bar{c}_2(\varepsilon, d)}{ \bar{c}_1(\varepsilon, d)- \bar{c}_2(\varepsilon, d)}} \rb
\label{eq:mrc_privunit_approximation_error_2}
\end{align}
where $(a)$ follows from \eqref{eq:special_pi_mrc} because $\qPU$ is a cap-based mechanism and $(b)$ follows by simple manipulations. 

To compute $p_0$, observe that we have the following relationship between $ \bar{c}_1(\varepsilon, d)$, $ \bar{c}_2(\varepsilon, d)$, and $p_0$ from \eqref{eq:privunit_density}:
\begin{align}
    \frac{ p_0}{\bar{c}_1(\varepsilon, d)} + \frac{ 1-p_0}{\bar{c}_2(\varepsilon, d)} = 1 \label{eq:c1c2p}
\end{align}
Using this and with some simple manipulations, we have 
\begin{align}
p_0
& = \frac{ \bar{c}_1(\varepsilon, d) \bar{c}_2(\varepsilon, d)}{( \bar{c}_1(\varepsilon, d)- \bar{c}_2(\varepsilon, d))^2} \lp\frac{ \bar{c}_1(\varepsilon, d)- \bar{c}_2(\varepsilon, d)}{ \bar{c}_2(\varepsilon, d)}-\frac{1}{\E\lb \theta \rb+\dfrac{ \bar{c}_2(\varepsilon, d)}{ \bar{c}_1(\varepsilon, d)- \bar{c}_2(\varepsilon, d)}}\rp \label{eq:mrc_privunit_approximation_error_3}
\end{align}
From \eqref{eq:mrc_privunit_approximation_error_2} and \eqref{eq:mrc_privunit_approximation_error_3}, we have
\begin{align}
    p_0 - p_{\texttt{mrc}} 
    &= \frac{ \bar{c}_1(\varepsilon, d) \bar{c}_2(\varepsilon, d)}{( \bar{c}_1(\varepsilon, d)- \bar{c}_2(\varepsilon, d))^2} \lp \E\lb \frac{1}{\theta+\dfrac{ \bar{c}_2(\varepsilon, d)}{ \bar{c}_1(\varepsilon, d)- \bar{c}_2(\varepsilon, d)}} - \frac{1}{\E[\theta]+\dfrac{ \bar{c}_2(\varepsilon, d)}{ \bar{c}_1(\varepsilon, d)- \bar{c}_2(\varepsilon, d)}} \rb \rp\\
    & = \frac{ \bar{c}_1(\varepsilon, d) \bar{c}_2(\varepsilon, d)}{( \bar{c}_1(\varepsilon, d)- \bar{c}_2(\varepsilon, d))^2} \lp \E\lb \frac{\E[\theta]-\theta}{\lp \theta+\dfrac{ \bar{c}_2(\varepsilon, d)}{ \bar{c}_1(\varepsilon, d)- \bar{c}_2(\varepsilon, d)} \rp \lp \E[\theta]+\dfrac{ \bar{c}_2(\varepsilon, d)}{ \bar{c}_1(\varepsilon, d)- \bar{c}_2(\varepsilon, d)} \rp} \rb \rp.
\end{align} 
Now, using the Hoeffding's inequality, we have $\Probability\lbp \lba \theta - \E[\theta] \rba \geq \sqrt{\frac{\ln \lp 2/\beta \rp}{2N}} \rbp \leq \beta$. Conditioned on the event $\lbp \lba \theta - \E[\theta] \rba \leq \sqrt{\frac{\ln \lp 2/\beta \rp}{2N}} \rbp$ and using the fact that $\lba p_0 - p_{\texttt{mrc}}\rba\leq 1$, we have
\begin{align}
    p_0   -   p_{\texttt{mrc}}  &  \leq   \dfrac{ \bar{c}_1(\varepsilon, d) \bar{c}_2(\varepsilon, d)}{( \bar{c}_1(\varepsilon, d)   -   \bar{c}_2(\varepsilon, d))^2}  \times \\
    & \lp \frac{\sqrt{\frac{\ln \lp 2/\beta \rp}{2N}}}{\lp \dfrac{p_0}{ \bar{c}_1(\varepsilon, d)}   -   \sqrt{\dfrac{\ln \lp 2/\beta \rp}{2N}}   +   \dfrac{ \bar{c}_2(\varepsilon, d)}{ \bar{c}_1(\varepsilon, d)   -   \bar{c}_2(\varepsilon, d)} \rp \lp \dfrac{p_0}{ \bar{c}_1(\varepsilon, d)}   +  \dfrac{ \bar{c}_2(\varepsilon, d)}{ \bar{c}_1(\varepsilon, d)   -   \bar{c}_2(\varepsilon, d)} \rp} \rp \hspace{-1mm} + \hspace{-1mm} \beta \label{eq:mrc_privunit_approximation_error_4}
    \end{align}
    where we have also plugged in $\E[\theta] = \dfrac{p_0}{ \bar{c}_1(\varepsilon,d) }$. Now, we can lower bound $\lp \dfrac{p_0}{ \bar{c}_1(\varepsilon, d)}+\dfrac{ \bar{c}_2(\varepsilon, d)}{ \bar{c}_1(\varepsilon, d)- \bar{c}_2(\varepsilon, d)} \rp$ as follows:
    \begin{align}
        \lp \frac{p_0}{ \bar{c}_1(\varepsilon, d)}+\frac{ \bar{c}_2(\varepsilon, d)}{ \bar{c}_1(\varepsilon, d)- \bar{c}_2(\varepsilon, d)} \rp \stackrel{(a)}{\geq} \frac{ \bar{c}_2(\varepsilon, d)}{ \bar{c}_1(\varepsilon, d)- \bar{c}_2(\varepsilon, d)}
         \stackrel{(b)}{\geq} \frac{1}{\exp(\varepsilon)-1}
    \end{align}
    where $(a)$ follows by lower bounding $ p_0/\bar{c}_1(\varepsilon, d)$ by 0 and $(b)$ follows because we have $ \bar{c}_1(\varepsilon,d)/  \bar{c}_2(\varepsilon,d) \leq \exp(\varepsilon)$. Further, if we pick $N \geq 2 \ln \lp 2/\beta \rp \lp\exp(\varepsilon)-1\rp^2$, then
    \begin{align}
        \sqrt{\frac{\ln \lp 2/\beta \rp}{2N}} \leq \frac{1}{2} \times \frac{1}{\exp(\varepsilon)-1}\leq \frac{1}{2}   \lp \frac{p_0}{ \bar{c}_1(\varepsilon, d)}+\frac{ \bar{c}_2(\varepsilon, d)}{ \bar{c}_1(\varepsilon, d)- \bar{c}_2(\varepsilon, d)} \rp. \label{eq:mrc_privunit_approximation_error_5}
    \end{align}
    Using \eqref{eq:mrc_privunit_approximation_error_5} in \eqref{eq:mrc_privunit_approximation_error_4}, we have 
    \begin{align}
p_0 - p_{\texttt{mrc}} & \leq  \frac{ \bar{c}_1(\varepsilon, d) \bar{c}_2(\varepsilon, d)}{( \bar{c}_1(\varepsilon, d)- \bar{c}_2(\varepsilon, d))^2} \times \\
& \qquad \lp \frac{2\sqrt{\frac{\ln \lp 2/\beta \rp}{2N}}}{ \lp \dfrac{p_0}{ \bar{c}_1(\varepsilon, d)}+\dfrac{ \bar{c}_2(\varepsilon, d)}{ \bar{c}_1(\varepsilon, d)- \bar{c}_2(\varepsilon, d)} \rp  \lp \dfrac{p_0}{ \bar{c}_1(\varepsilon, d)}+\dfrac{ \bar{c}_2(\varepsilon, d)}{ \bar{c}_1(\varepsilon, d)- \bar{c}_2(\varepsilon, d)} \rp} \rp + \beta \\
& = \lp \frac{2 \bar{c}_1(\varepsilon, d) \bar{c}_2(\varepsilon, d) \sqrt{\frac{\ln \lp 2/\beta \rp}{2N}}}{\lp  p_0 \lp1-\dfrac{ \bar{c}_2(\varepsilon, d)}{ \bar{c}_1(\varepsilon, d)}\rp + \bar{c}_2(\varepsilon, d) \rp^2} \rp + \beta\\
& \stackrel{(a)}{\leq}\lp \frac{2 \bar{c}_1(\varepsilon, d)}{ \bar{c}_2(\varepsilon, d)}  \sqrt{\frac{\ln \lp 2/\beta \rp}{2N}} \rp+ \beta \stackrel{(b)}{\leq}\lp 2 \exp(\varepsilon) \sqrt{\frac{\ln \lp 2/\beta \rp}{2N}} \rp+ \beta \stackrel{(c)}{\leq} \frac{\lambda(p_0 - 1/2)}{1+\lambda}. \label{eq:mrc_privunit_approximation_error_6}
\end{align}
where $(a)$ follows because $p_0 \lp1-\frac{ \bar{c}_2(\varepsilon, d)}{ \bar{c}_1(\varepsilon, d)}\rp \geq 0$, $(b)$ follows because we have $ \bar{c}_1(\varepsilon,d)/  \bar{c}_2(\varepsilon,d) \leq \exp(\varepsilon)$ and $(c)$ follows if we pick 
\begin{align}
    \beta & \leq \frac{\lambda(p_0 - 1/2)}{2(1+\lambda)}, \\
    N & \geq\frac{2\exp(2\varepsilon)\ln \lp 2/\beta \rp}{\lp \dfrac{\lambda(p_0 - 1/2)}{1+\lambda} - \beta \rp^2}
    = 2\exp(2\varepsilon)\lp\frac{2 (1+\lambda)}{\lambda \lp p_0 -1/2 \rp}\rp^2 \ln\lp \frac{4(1+\lambda)}{\lambda \lp p_0 -1/2 \rp} \rp. \label{eq:mrc_privunit_approximation_error_7}
\end{align}
Further, it is easy to verify that \eqref{eq:mrc_privunit_approximation_error_5} holds since the choice of $N$ in \eqref{eq:mrc_privunit_approximation_error_7} is such that $N \geq \frac{1}{2} \ln \lp 2/\beta \rp \lp\exp(\varepsilon)-1\rp^2$. Now, rearranging \eqref{eq:mrc_privunit_approximation_error_6} gives us an upper bound on $\dfrac{p_0 - p_{\texttt{mrc}}}{p_0 -1/2}$, i.e.,
\begin{align}
    \frac{p_0 - p_{\texttt{mrc}}}{p_0 -1/2} \leq \frac{\lambda}{1+\lambda}. \label{eq:mrc_privunit_approximation_error_8}
\end{align}
Using \eqref{eq:mrc_privunit_approximation_error_8} in \eqref{eq:mrc_privunit_approximation_error_1}, we have
\begin{align}
    \frac{1}{m_\texttt{mrc}} - \frac{1}{m_{\texttt{pu}}} \leq \frac{\lambda}{m_{\texttt{pu}}}. \label{eq:mrc_privunit_approximation_error_9}
\end{align}
Rearranging \eqref{eq:mrc_privunit_approximation_error_9} completes the proof.
\end{proof}

\subsubsection{Relationship between mean squared errors associated with \texorpdfstring{$\PrivUnit$}{PrivUnit} and \texorpdfstring{$\MRC$}{MRC} simulating \texorpdfstring{$\PrivUnit$}{PrivUnit}}\label{appendix:mrc_pu_scaling_mse}
In the following Proposition,
we show that if the scaling factor $m_{\texttt{mrc}}$ is close to the scaling parameter $m_{\texttt{pu}}$, then the mean squared error associated with $\MRC$ simulating $\PrivUnit$ (i.e., $\Expectation_{\qMRC} \big[ \lV  \hat{\svbx}^\texttt{mrc} - \svbx \rV^2_2  \big]$) is close to the mean squared error associated with $\PrivUnit$ (i.e., $\Expectation_{\qPU} \big[ \lV  \hat{\svbx}^\texttt{pu} - \svbx \rV^2_2  \big]$).
\begin{proposition}\label{proposition:mrc_mse_wrt_privunit}
Let $\qPU(\svbz | \svbx)$ be the  $\varepsilon$-LDP \emph{$\PrivUnit$} mechanism with parameters $p_0$ and $\gamma$ and estimator $\hat{\svbx}^{\texttt{pu}}$. Let $\qMRC(\svbz|\svbx)$ denote the \emph{$\MRC$} privatization mechanism simulating \emph{$\PrivUnit$} with $N$ candidates and estimator $\hat{\svbx}^{\texttt{mrc}}$.
Let $m_{\texttt{pu}}$ denote the scaling factor  associated with \emph{$\PrivUnit$} and $m_\texttt{mrc}$ denote the scaling factor  associated with the \emph{$\MRC$} scheme simulating \emph{$\PrivUnit$}. Consider any $\lambda > 0$. If $m_{\texttt{pu}} - m_\texttt{mrc} \leq \lambda \cdot m_\texttt{mrc}$, then 
  \begin{align}
    \Expectation_{\qMRC} \big[ \lV  \hat{\svbx}^\texttt{mrc} - \svbx \rV^2_2  \big]  \leq  \lp 1+\lambda \rp^2  \Expectation_{\qPU}\big[\|\hat{\svbx}^{\texttt{pu}} - \svbx\|^2\big] + 2(1+\lambda)(2+\lambda) \sqrt{\Expectation_{\qPU}\big[\|\hat{\svbx}^{\texttt{pu}} - \svbx\|^2\big]} + (2+\lambda)^2.
\end{align}
\end{proposition}
\begin{proof}
We will start by upper bounding $1/m_{\texttt{pu}}$ in terms of $\Expectation_{\qPU}\big[\|\hat{\svbx}^{\texttt{pu}} - \svbx\|^2\big]$. First, observe that
\begin{align}
    \|\hat{\svbx}^{\texttt{pu}} - \svbx\|  \stackrel{(a)}{\geq} \lV \hat{\svbx}^{\texttt{pu}}\rV - \lV \svbx \rV  \stackrel{(b)}{\geq}
    \frac{1}{m_{\texttt{pu}}}  - 1  \label{eq:mrc_privunit_variance_1}
\end{align}
where $(a)$ follows from the triangle inequality and $(b)$ follows because $\|\hat{\svbx}^{\texttt{pu}}\| = 1/m_{\texttt{pu}}$ and $\|\svbx\| \leq 1$. Next, we have
\begin{align}
    \frac{1}{m_{\texttt{pu}}} = \frac{1}{m_{\texttt{pu}}} - 1 + 1 \stackrel{(a)}{\leq} \sqrt{\Expectation_{\qPU}\big[\|\hat{\svbx}^{\texttt{pu}} - \svbx\|^2\big]} + 1 \label{eq:mrc_privunit_variance_2}
\end{align}
where $(a)$ follows from \eqref{eq:mrc_privunit_variance_1}. We will now upper bound $\Expectation_{\qMRC}[\| \hat{\svbx}^\texttt{mrc} - \svbx\|^2]$. We have
\begin{align}
    \Expectation_{\qMRC}[\| \hat{\svbx}^\texttt{mrc} - \svbx\|^2] 
    & = \Expectation_{\qMRC} [\| \hat{\svbx}^\texttt{mrc}\|^2] + \lV \svbx \rV^2_2 - 2\lan \Expectation_{\qMRC} [\hat{\svbx}^\texttt{mrc}], \svbx\ran \\
    & \stackrel{(a)}{\leq} \Expectation_{\qMRC} [\| \hat{\svbx}^\texttt{mrc}\|^2] + \lV \svbx \rV^2_2 + 2\sqrt{ \Expectation_{\qMRC} [\lV \hat{\svbx}^\texttt{mrc} \rV^2] \cdot \lV \svbx \rV^2 } \\
    & \stackrel{(b)}{\leq} \lp\frac{1}{m_\texttt{mrc}}\rp^2  + 1 + \frac{2}{m_\texttt{mrc}}\\
    & \stackrel{(c)}{\leq} \lp\frac{1+\lambda}{m_{\texttt{pu}}}\rp^2 + 1 + \frac{2(1+\lambda)}{m_{\texttt{pu}}} \\
    & \stackrel{(d)}{\leq}
     \lp 1+\lambda \rp^2  \Expectation_{\qPU}\big[\|\hat{\svbx}^{\texttt{pu}} - \svbx\|^2\big] + 2(1+\lambda)(2+\lambda) \sqrt{\Expectation_{\qPU}\big[\|\hat{\svbx}^{\texttt{pu}} - \svbx\|^2\big]} + (2+\lambda)^2
\end{align}
where $(a)$ follows from Cauchyâ€“Schwarz inequality, $(b)$ follows because $\|\hat{\svbx}^\texttt{mrc}\| = 1/m_\texttt{mrc}$ and $\|\svbx\| \leq 1$, (c) follows from Lemma~\ref{lemma:mrc_privunit_approximation_error} (which shows $m_{\texttt{pu}} - m_\texttt{mrc} \leq \lambda\cdot m_\texttt{mrc}$), and $(d)$ follows using \eqref{eq:mrc_privunit_variance_2} and some simple manipulations.
\end{proof}

In the following Lemma, we show that with on the order of $\varepsilon$-bits of communication, the mean squared error associated with $\MRC$ simulating $\PrivUnit$ (i.e., $\Expectation_{\qMRC} \big[ \lV  \hat{\svbx}^\texttt{mrc} - \svbx \rV^2_2  \big]$) is close to the mean squared error associated with $\PrivUnit$ (i.e., $\Expectation_{\qPU} \big[ \lV  \hat{\svbx}^\texttt{pu} - \svbx \rV^2_2  \big]$).

\begin{restatable}{lemma}{mrcprivunit}\label{theorem:mrc_privunit}
Let $\qPU(\svbz | \svbx)$ be the  $\varepsilon$-LDP \emph{$\PrivUnit$} mechanism with parameters $p_0$ and $\gamma$ and estimator $\hat{\svbx}^{\texttt{pu}}$. Let $\qMRC(\svbz|\svbx)$ denote the \emph{$\MRC$} privatization mechanism simulating \emph{$\PrivUnit$} with $N$ candidates and estimator $\hat{\svbx}^{\texttt{mrc}}$. 
Consider any $\lambda > 0$. Then,
\begin{align}
   \Expectation_{\qMRC} \big[ \lV  \hat{\svbx}^\texttt{mrc} - \svbx \rV^2_2  \big] 
     \leq  \lp 1+\lambda \rp^2  \Expectation_{\qPU}\big[\|\hat{\svbx}^{\texttt{pu}} -\svbx\|^2\big] + 2(1+\lambda)(2+\lambda) \sqrt{\Expectation_{\qPU}\big[\|\hat{\svbx}^{\texttt{pu}} - \svbx\|^2\big]} + (2+\lambda)^2
\end{align}
as long as 
\begin{align}
    N \geq   2e^{2\varepsilon}\lp\frac{2 (1+\lambda)}{\lambda \lp p_0 -1/2 \rp}\rp^2 \ln\lp \frac{4(1+\lambda)}{\lambda \lp p_0 -1/2 \rp} \rp.
\end{align}
\end{restatable}
\begin{proof}
The proof follows from Proposition \ref{proposition:mrc_mse_wrt_privunit} and Lemma \ref{lemma:mrc_privunit_approximation_error}.
\end{proof}

\subsubsection{Simulating \texorpdfstring{$\PrivUnit$}{PrivUnit} using Minimal Random Coding }\label{appendix:mrc_pu_utility}
The following Theorem shows that, for mean estimation, $\MRC$ can simulate $\PrivUnit$ in a near-lossless manner (when $n$ is large and $\lambda$ is small) while only using on the order of $\varepsilon$ bits of communication.

\begin{restatable}{theorem}{mrcpu}
\label{thm:me_mrc_pu}
Let $r_{\msf{ME}} \lp \hat{\svbmu}^\texttt{pu}, \qPU \rp$ and $r_{\msf{ME}} \lp \hat{\svbmu}^\texttt{mrc}, \qMRC \rp$ be the empirical mean estimation error for \emph{$\PrivUnit$} with parameter $p_0$ and \emph{$\MRC$} simulating \emph{$\PrivUnit$} with $N$ candidates respectively. Consider any $\lambda > 0$. Then,
\begin{align}
    r_{\msf{ME}} \lp \hat{\svbmu}^\texttt{mrc}, \qMRC \rp \leq  
    \lp 1+\lambda \rp^2 r_{\msf{ME}} \lp \hat{\svbmu}^\texttt{pu}, \qPU \rp + 2(1+\lambda)(2+\lambda) \sqrt{\frac{r_{\msf{ME}} \lp \hat{\svbmu}^\texttt{pu}, \qPU \rp}{n}} + \frac{(2+\lambda)^2}{n}.
\end{align}
as long as 
\begin{align}\label{eq:N_bdd_mrc_pu}
% \textstyle
    N \geq   2e^{2\varepsilon}\lp\frac{2 (1+\lambda)}{\lambda \lp p_0 -1/2 \rp}\rp^2 \ln\lp \frac{4(1+\lambda)}{\lambda \lp p_0 -1/2 \rp} \rp.
\end{align}
% $N$ satisfies \eqref{eq:N_bdd_mmrc_pu}.
\end{restatable}
\begin{proof}
The proof follows directly from Lemma~\ref{theorem:mrc_privunit} since for all $i \in [n]$, $\hat{\svbx}^{\texttt{mrc}}_i$ are independent of each other as well as unbiased.
\end{proof}

\subsection{Empirical Comparisons}
\label{appendix:mrc_pu_emp}
In this section, we compare $\MRC$ simulating $\PrivUnit$ (using its approximate DP guarantee) against $\PrivUnit$ and SQKR for mean estimation with $d = 500$ and $n = 5000$. We use the same data generation scheme described in Section \ref{subsec:mmrc_privunit_empirical} and set $\delta = 10^{-6}$. As before, SQKR uses $\#$-bits $= \varepsilon$ because it leads to a poor performance if $\#$-bits $ > \varepsilon$. We show the privacy-accuracy tradeoffs for these three methods in Figure \ref{fig:a_mean}. We see that $\MRC$ simulating $\PrivUnit$ can attain the accuracy of the uncompressed $\PrivUnit$ for the range of  $\varepsilon$'s typically considered by LDP mechanisms while only using $(3\varepsilon/ \ln 2) + 6$ bits. In comparison with the results from Section \ref{subsec:mmrc_privunit_empirical}, the results in this section come with an approximate guarantee ($\delta = 10^{-6}$) and with a higher number of bits of communication. In other words, along with the obvious gains of pure privacy instead of approximate privacy, $\MMRC$ results in a lower communication cost (and therefore a lower computation cost) compared to $\MRC$.
\begin{figure}[h]
\centering
\includegraphics[width=0.45\linewidth]{figures/a_eps1.pdf}
\caption{Comparing $\PrivUnit$, $\MRC$ simulating $\PrivUnit$ and SQKR for mean estimation in terms of $\ell_2$ error vs $\varepsilon$ with $d = 500$, $n = 5000$, and $\#$bits $= (3\varepsilon/ \ln 2) + 6$.}
\label{fig:a_mean}
\end{figure}

