\section{Conclusion and Future Work}
\label{sec:conclusion}
% computational cost
We demonstrated how Minimal Random Coding can be used to simulate a class of $\varepsilon$-LDP mechanisms in a manner which is communication efficient while preserving accuracy and differential privacy guarantees. Further, for mean and frequency estimation, we proposed unbiased versions of our schemes (relying only on translation and scaling) that attain the privacy-accuracy tradeoffs of the best known schemes i.e., $\PrivUnit$ and $\SubsetSelection$, while requiring on the order of $\varepsilon$ bits of communication.

We now briefly discuss a few non-trivial and interesting open questions. \\

\noindent\textbf{Computational Cost.} The computational cost of our approach, similar to \cite{FT21} grows linearly in $d$ and exponentially in $\varepsilon$ (as we need $N = \exp(O(\varepsilon))$ candidates to properly simulate the optimal mechanisms). An important question for future research is therefore how to increase the computational efficiency of $\MRC$ and $\MMRC$ with respect to $\varepsilon$.\\
% However, we did not discuss the computational cost of these methods, which grows exponentially with $\varepsilon$. 

\noindent\textbf{Privacy Amplification via Shuffling.} As mentioned in Section \ref{subsec:related_works}, privacy amplification via shuffling techniques ensure a central $\varepsilon \approx 1$ even when the local $\varepsilon >1$. While our method could be combined with these amplification techniques in principle, we leave the analysis of the privacy, utility, and communication guarantees of the resulting scheme as a question for future research.\\
% Likewise, we did not explore how to optimally combine our method with privacy amplification via shuffling techniques. This is an important future research to ensure a central $\varepsilon \approx 1$ even when the local $\varepsilon >1$.}

% approaches other than MRC
\noindent\textbf{Other schemes to simulate noisy channels.} $\MRC$ is only one of several channel simulation schemes studied in information theory which could be considered for compression of $\varepsilon$-LDP mechanisms. 
Similar to $\MRC$, other channel simulation schemes, e.g., rejection sampling \citep{HJMR07} or schemes based on the Poisson functional representation \citep{LE18}, can also compress noisy signals to a number of bits which is close to the information contained in the signal (which decreases as noise increases). 
% Other schemes not yet studied for this purpose include Rejection Sampling \citep{HJMR07} or the Poisson Functional Representation \citep{LE18}. 
Analyzing these schemes for their effect on differential privacy guarantees is an interesting open question.\\

% shared randomness
\noindent\textbf{Shared Randomness.} Finally, here we assumed the existence of a shared source of randomness. We further assumed that each user is using a different source of shared randomness. While shared randomnesss only adds to the cost of downlink and not uplink communication (which is usually the bottleneck in settings like federated learning), a question left for future research is how much communication is required to establish and select these sources of randomness.