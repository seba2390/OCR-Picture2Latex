\section{Main Results}
\label{sec:main_results}
In this section, first, we describe the Minimal Random Coding algorithm for compressing any $\varepsilon$-LDP  mechanism and prove its order-optimal privacy-accuracy-communication tradeoffs. Then, we propose the Modified Minimal Random Coding algorithm for compressing any $\varepsilon$-LDP \emph{cap-based mechanism}\footnote{The family of cap-based mechanisms includes
% the best known schemes for mean and frequency estimation, such as 
$\PrivUnit$ and $\SubsetSelection$. See Definition \ref{def:cap}.} and prove that it achieves optimal privacy-accuracy-communication tradeoffs.

\subsection{Minimal Random Coding (\texorpdfstring{$\MRC$}{MRC})}
\label{subsec:mrc}
Consider an $\varepsilon$-LDP mechanism $q(\cdot|\svbx)$ that we wish to compress. Under $\MRC$, first, a number of candidates $\svbz_1, \cdots, \svbz_N$ are drawn from a fixed reference distribution $p(\cdot)$ (known to both the user and the server). This can be achieved via a pseudorandom number generator with a known seed. Next, the user transmits an index $K \in [N]$ to the server where $K$ is drawn according to some distribution $\piMRC(\cdot)$ such that $\svbz_K \sim q(\cdot|\svbx)$ approximately. The distribution $\piMRC$ is such that, $\forall k \in [N]$,  $\piMRC(k) \propto w(k)$ where $w(k) \coloneqq q(\svbz_k|\svbx) / p(\svbz_k)$ are the importance weights\footnote{We suppress dependence of $\piMRC$ \& $w$ on $\svbx$ for simplicity.} (see Algorithm \ref{alg:mrc}). To communicate the index $K$ of $\MRC$, $\log N$ bits are required.

\begin{algorithm}
% \DontPrintSemicolon
\KwInput{$\varepsilon$-LDP mechanism $q(\cdot|\svbx)$, reference distribution $p(\cdot)$, number of candidates $N$}
Draw samples $\svbz_1 , \cdots, \svbz_N$ from $p(\svbz)$ \tcp*[h]{Using the shared source of randomness}\\
\For {$k \in \{1,\cdots, N\}$}
{
% $w(z_k) = \frac{q(\svbz_k)}{p(\svbz_k)}$\\
% $\piMRC(k) = \frac{w(\svbz_k)}{\sum_{k'}w(\svbz_{k'})}$
$w(k) \leftarrow q(\svbz_k|\svbx) / p(\svbz_k) $
}
$\piMRC(\cdot) \leftarrow w(\cdot) / \sum_{k}w(k)$\\
\KwOutput{$\piMRC(\cdot), \{\svbz_1 , \cdots, \svbz_N \}$}
\caption{$\MRC$}
\label{alg:mrc}
\end{algorithm}

% \begin{algorithm}
% % \DontPrintSemicolon
% \KwInput{ $\pi(\cdot)$}
% \KwOutput{$K \sim \pi(\cdot)$}
% \caption{Encoder}
% \label{alg:encoder}
% \end{algorithm}

% \begin{algorithm}
% % \DontPrintSemicolon
% \KwInput{ $\{\svbz_1 , \cdots, \svbz_N \}, m, b, K$}
% \KwOutput{$\hat{\svbx} = (\svbz_K - b)/m$}
% \caption{Decoder}
% \label{alg:decoder}
% \end{algorithm}

Let $\qMRC$ denote the distribution of $\svbz_K$ where $K \sim \piMRC(\cdot)$.
% i.e., with $\delta(\cdot)$ denoting the Dirac delta function:
% \begin{align}
%     \qMRC(\svbz | \svbx) \coloneqq \sum_{k} \piMRC(k) \delta(\svbz - \svbz_k). \label{eq:qmrc}
% \end{align}
The following theorem shows that when the number of candidates is exponential in $\varepsilon$, samples drawn from $\qMRC$ will be similar to samples drawn from $q(\cdot|\svbx)$ in terms of $\ell_2$ error. In other words, $\qMRC$ can compress $q(\cdot|\svbx)$ to the order of $\varepsilon$ bits of communication as well as simulate it in a near-lossless fashion. A proof can be found in Appendix \ref{appendix:utility_mrc}.

\begin{restatable}[Utility of $\MRC$]{theorem}{mrcaccuracylocal}\label{theorem:mrc_accuracy_local}
Consider any input alphabet $\cX$, output alphabet $\cZ$, data $\svbx \in \cX$, and $\varepsilon$-LDP mechanism $q(\cdot|\svbx)$. Consider any reference distribution $p(\cdot)$ such that $| \ln (q(\svbz|\svbx) / p(\svbz)) | \leq \varepsilon$ $\forall ~ \svbx \in \cX, \svbz \in \cZ$.\footnote{Note that this condition holds for many reference distributions $p(\cdot)$. For example, one can simply choose $p(\cdot) = q(\cdot|\svbx^*)$ for some $\svbx^* \in \cX$.} Let the number of candidates be $N = 2^{(\log e + 4c) \varepsilon}$ for some constant $c \geq 0$. Then, for $\alpha \in [0,1/2]$, $\qMRC$ is such that 
\begin{align}\textstyle
     \Big| \Expectation_{\qMRC}\big[\|\svbz - \svbx\|^2\big] - \Expectation_{q}\big[\|\svbz - \svbx\|^2\big] \Big| \leq \frac{2\alpha \sqrt{\Expectation_{q}[\|\svbz - \svbx\|^4]} }{1-\alpha}  \label{eq:mrc_utility}
 \end{align}
% \begin{align}
%      \Big| \Expectation_{\qMRC(\svbz|\svbx)}\big[\|\svbz - \svbx\|^2\big] - \Expectation_{q(\svbz|\svbx)}\big[\|\svbz - \svbx\|^2\big] \Big| \\
%      \qquad \leq \frac{2\alpha }{1-\alpha} \times \sqrt{\Expectation_{q(\svbz|\svbx)}[\|\svbz - \svbx\|^4]} \label{eq:mrc_utility}
%  \end{align}
 holds with probability at least $1 - 2\alpha$, with $c$ and $\alpha$ related by the following: $\alpha = \sqrt{2^{-c\varepsilon} + 2^{-c^2/\log e + 1}}$.
%  \begin{align}
%     \alpha = \sqrt{2^{-c\varepsilon} + 2^{-c^2/\log e + 1}}.
%  \end{align}
\end{restatable}
In general, $\Expectation_{q}\lb \|\svbz - \svbx\|^4\rb$ in \eqref{eq:mrc_utility} can be well-controlled. See Remark~\ref{rmk:error_forth_moment} in Appendix \ref{appendix:utility_mrc} for more details.
\iffalse
\begin{remark}\label{rmk:error_forth_moment}
Note that for most $\varepsilon$-LDP mechanisms, $\Expectation_{q(\svbz|\svbx)}\lb \|\svbz - \svbx\|^4\rb$ can be well-controlled. For instance, for $\SubsetSelection$ and $\PrivUnit$, the output spaces are bounded, so $\sqrt{\Expectation_{q(\svbz|\svbx)}[\|\svbz - \svbx\|^4]}$ is of the same order as $\Expectation_{q(\svbz|\svbx)}\big[\|\svbz - \svbx\|^2\big]$. Therefore, by making $\alpha$ small enough (i.e. by increasing $c$), we can make the estimation error arbitrary small.
\end{remark}
\fi
% \subsubsection{MRC is locally differentially private}

In the next Theorem, we show that $\piMRC$ is $2\varepsilon$-LDP. Hence, the compressed mechanism $\qMRC$ is 2$\varepsilon$-LDP. 
% the importance weights can be bounded for $\varepsilon$-LDP mechanisms.
% and 
% See Appendix \ref{appendix:pure_privacy_mrc} for the complete proof.
% A proof can be found in Appendix \ref{appendix:pure_privacy_mrc}.
\begin{restatable}[Pure DP guarantee of $\MRC$]{theorem}{mrcpureprivacy}\label{theorem:mrc_pure_privacy}
Consider any input alphabet $\cX$, output alphabet $\cZ$, and data $\svbx \in \cX$.
Consider any $\varepsilon$-LDP mechanism $q(\cdot|\svbx)$, reference distribution $p(\cdot)$, and number of candidates $N\geq 1$. Then, $\piMRC(\cdot)$ obtained from Algorithm \ref{alg:mrc} is a  2$\varepsilon$-LDP mechanism.
\end{restatable}
A proof is provided in Appendix \ref{appendix:pure_privacy_mrc} and it relies on fact that the following ratio can be bounded by $e^{2\varepsilon}$:
 \begin{align}
    \frac{\piMRC_{\svbx}(k)}{\piMRC_{\svbx'}(k)} = \frac{q(\svbz_k | \svbx)}{q(\svbz_k | \svbx')} \cdot \frac{\sum_{k'} q(\svbz_{k'} | \svbx')/p(\svbz_{k'})}{\sum_{k'} q(\svbz_{k'} | \svbx)/p(\svbz_{k'})}.
\end{align}


% The proof is 
% \begin{align}
%     \frac{\piMRC_{\svbx}(k)}{\piMRC_{\svbx'}(k)} = \frac{q(\svbz_k | \svbx)}{q(\svbz_k | \svbx')} \cdot \frac{\sum_{k'} q(\svbz_{k'} | \svbx')}{\sum_{k'} q(\svbz_{k'} | \svbx)}
% \end{align}
In the following Theorem, we show that $\piMRC$ is $(\varepsilon + \varepsilon_0, \delta)$-LDP implying that the compressed mechanism $\qMRC$ is $(\varepsilon + \varepsilon_0, \delta)$-LDP where $\varepsilon_0 > 0$ and $\delta \leq 1$ are free parameters. This Theorem can be viewed complementary to Theorem \ref{theorem:mrc_pure_privacy} where a stronger privacy parameter can be achieved (i.e., $\varepsilon + \varepsilon_0$ which can get arbitrarily close to $\varepsilon$ as opposed to $2\varepsilon$) albeit at the cost of trading pure privacy for approximate privacy. A proof is provided in Appendix \ref{appendix:approx_privacy_mrc}.

\begin{restatable}[Approximate DP guarantee of $\MRC$]{theorem}{mrcapproxprivacy}\label{theorem:mrc_approximate_privacy}
Consider any input alphabet $\cX$, output alphabet $\cZ$, data $\svbx \in \cX$, and $\varepsilon$-LDP mechanism $q(\cdot|\svbx)$. Consider any reference distribution $p(\cdot)$ such that $ | \ln (q(\svbz|\svbx) / p(\svbz)) | \leq \varepsilon$  $\forall ~ \svbx \in \cX, \svbz \in \cZ$.\textBlue{\footnotemark[5]}  Let $c_0 \geq 0$ be some constant and let the number of candidates $N = \exp(2\varepsilon + 2c_0)$. Then, for any $\delta \leq 1$, $\piMRC(\cdot)$ obtained from Algorithm \ref{alg:mrc} is $(\varepsilon + \varepsilon_0, \delta)$-LDP mechanism where
\begin{equation}
 \textstyle
    \varepsilon_0 \coloneqq \ln\dfrac{1+a_0}{1-a_0} \qquad \text{and} \qquad a_0 \coloneqq \exp(-c_0)\sqrt{\frac{1}{2}\ln\frac{2}{\delta}}.
\end{equation}
\end{restatable}


% \subsubsection{MRC can simulate any $\varepsilon$-LDP mechanism in a nearly lossless fashion with $O(\varepsilon)$ bits of communication}


\subsection{Modified Minimal Random Coding (\texorpdfstring{$\MMRC$}{MMRC})}
\label{subsec:mmrc}
While the results regarding $\MRC$ in Section \ref{subsec:mrc} are general and offer order optimal privacy-accuracy tradeoffs with about $\varepsilon$ bits of communication, the resulting compressed mechanism is not exactly $\varepsilon$-LDP. More specifically, Theorem \ref{theorem:mrc_pure_privacy} introduces an additional factor of $2$ in the LDP guarantee and Theorem \ref{theorem:mrc_approximate_privacy} provides an approximate privacy guarantee instead of a pure privacy guarantee. To address these limitations, we focus on a class of $\varepsilon$-LDP mechanisms which we call \emph{cap-based} mechanisms and propose a modification to $\MRC$ such that the resulting compressed mechanism is $\varepsilon$-LDP. Further, like $\MRC$, $\MMRC$ can simulate the underlying $\varepsilon$-LDP mechanism in a near-lossless fashion while using only on the order of $\varepsilon$ bits.

% they fail to achieve our goal of getting as close as possible to the best known accuracy for analytics and learning under $\varepsilon$-LDP while only using on the order of $\varepsilon$ bits of communication. 
% In order words, c falls short because of the additional factor of $2$ in the LDP guarantee and Theorem \ref{theorem:mrc_approximate_privacy} falls short because of approximate privacy guarantee instead of pure privacy. To that end, we focus on a class of $\varepsilon$-LDP mechanisms which we call \emph{cap-based} mechanisms (defined below) that subsumes $\PrivUnit$ and $\SubsetSelection$. For this class of mechanisms, we propose a modification to the minimal random coding such that the resulting compressed mechanism is $\varepsilon$-LDP while retaining the 
% and learning under $\varepsilon$-LDP while only using on the order of $\varepsilon$ bits of communication.

We start with the definition of cap-based mechanism which is inspired from the structure of $\PrivUnit$ and $\SubsetSelection$.
\begin{definition}[Cap-based Mechanisms]\label{def:cap}
An $\varepsilon$-LDP mechanism $q(\svbz | \svbx)$ with  input alphabet $\cX$ and output alphabet $\cZ$ is a cap-based mechanism if it can be written in the following way:
\begin{align}
\textstyle
    q(\svbz | \svbx) = 
    \begin{cases}
      c_1(\varepsilon, d) & \text{if}\ \svbz \in \msf{Cap}_{\svbx}
      \\[10pt]
      c_2(\varepsilon, d) & \text{if}\ \svbz \notin \msf{Cap}_{\svbx}
    \end{cases} \label{eq:cap_mechanism}
\end{align}
where (a) $c_1(\varepsilon, d)$ and $c_2(\varepsilon, d)$ are constants with respect to $\svbx$ and $\svbz$ such that $c_1(\varepsilon, d) \geq c_2(\varepsilon, d)$, and (b) $\msf{Cap}_{\svbx} \subseteq \cZ$ such that $\Probability_{\svbz \sim \Unif(\cZ)}\lp \svbz \in \msf{Cap}_{\svbx} \rp$ is independent of $\svbx$ and is at least $c_2(\varepsilon, d) / 2 c_1(\varepsilon, d)$.
% \end{enumerate}
% $\frac{c_1}{c_2} \Probability\lp \svbz \in \cZ_0 \rp = \frac{p(2-I)}{I(1-p)} \times \frac{I}{2} = \frac{p(2-I)}{2(1-p)} = \frac{e^{\varepsilon_0} (2-I)}{2}$\\
% $\frac{c_1}{c_2} \Probability\lp \svbz_k \in \msf{Cap}_{\svbx}_{\varepsilon}(\svbx) \rp = \frac{e^{\varepsilon}}{k} \lceil \frac{k}{e^{\varepsilon}+1} \rceil$
\end{definition}

In words, a cap-based $\varepsilon$-LDP mechanism samples uniformly either from $\msf{Cap}_{\svbx}$ or from $\cZ \setminus \msf{Cap}_{\svbx}$ where $\msf{Cap}_{\svbx} \subseteq \cZ$ is such that if $\svbz$ is sampled uniformly from $\cZ$, it will belong to $\msf{Cap}_{\svbx}$ with probability at least $c_2(\varepsilon, d) / 2 c_1(\varepsilon, d)$. 
It is easy to see that $\qSS$ defined in \eqref{eq:q_ss} is a cap-based mechanism with $\msf{Cap}_{\svbx} = \cZ_{\svbx}$, $c_1(\varepsilon, d) = \frac{e^{\varepsilon}}{{ \binom{d-1}{s-1} }e^{\varepsilon}+{ \binom{d-1}{s} }}$, and $c_2(\varepsilon, d) = \frac{1}{{ \binom{d-1}{s-1} }e^{\varepsilon}+{ \binom{d-1}{s} }}$. See Appendix \ref{appendix:ss} where we evaluate $\Probability_{\svbz \sim \Unif(\cZ)}\lp \svbz \in \cZ_{\svbx} \rp$ and show that it is at least $ 1/2e^{\varepsilon}$. In Appendix \ref{appendix:privunit}, we show that $\qPU$ is a cap-based mechanism.

For a cap-based $\varepsilon$-LDP mechanism $q(\svbz | \svbx)$ and a uniform reference distribution $p(\cdot)$, the distribution $\piMRC$ obtained from Algorithm \ref{alg:mrc} takes a special form:
\begin{align}
   \piMRC(k) =
  \begin{cases}
     \frac{1}{N} \times \frac{c_1(\varepsilon, d)}{\theta c_1(\varepsilon, d) + (1 - \theta) c_2(\varepsilon, d)} \ \text{if}\ \svbz_k \in \msf{Cap}_{\svbx}
      \\[10pt]
     \frac{1}{N} \times \frac{c_2(\varepsilon, d)}{\theta c_1(\varepsilon, d) + (1 - \theta) c_2(\varepsilon, d)} \ \text{if}\ \svbz_k \notin \msf{Cap}_{\svbx}
    \end{cases}\label{eq:special_pi_mrc}
\end{align}
where $\theta$ is the fraction of candidates inside the $\msf{Cap}_{\svbx}$, i.e.,  $\theta = \frac{1}{N} \sum_{k} \Indicator(\svbz_k \in \msf{Cap}_{\svbx})$. As is, $\piMRC$ in \eqref{eq:special_pi_mrc} is not necessarily $\varepsilon$-LDP because $\theta$ can be different for $\svbx$ and $\svbx'$. However, as $N \to \infty$, $\theta \to  \Expectation[\theta] = \Probability_{\svbz \sim \Unif(\cZ)}\lp \svbz \in \msf{Cap}_{\svbx} \rp$, which is not a function of $\svbx$, implying that $\piMRC_{\svbx}(k) / \piMRC_{\svbx'}(k) \leq c_1(\varepsilon,d)/c_2(\varepsilon, d) \leq \exp(\varepsilon)$\footnote{This follows from \eqref{eq:ldp} and \eqref{eq:cap_mechanism} because $q(\cdot | \svbx)$ is $\varepsilon$-LDP.}. This shows that $\piMRC$ is $\varepsilon$-LDP when $N \to \infty$. This motivates us to modify $\piMRC$ to $\piMMRC$ such that $\piMMRC$ is $\varepsilon$-LDP irrespective of $N$. Further, when $N$ is large enough, the modification is not by much, i.e., a sample from $\piMRC$ is similar to a sample from $\piMMRC$.

To that end, define an upper threshold $t_u = \frac{1}{N} \times \frac{c_1(\varepsilon, d)}{\Expectation[\theta] c_1(\varepsilon, d) + (1 - \Expectation[\theta]) c_2(\varepsilon, d)}$ and a lower threshold  $t_l = \frac{1}{N} \times \frac{c_2(\varepsilon, d)}{\Expectation[\theta] c_1(\varepsilon, d) + (1 - \Expectation[\theta]) c_2(\varepsilon, d)}$, and initialize $\piMMRC$ to be equal to $\piMRC$. We want to modify $\piMMRC$ so as to ensure:
\begin{align}
    t_l \leq \piMMRC(k) \leq t_u ~ \forall k \in [N], \label{eq:modification}
\end{align}
which, as argued above, guarantees $\varepsilon$-LDP irrespective of the choice of $N$.
First, it is easy to see that $\theta c_1(\varepsilon, d) + (1 - \theta) c_2(\varepsilon, d)$ is an increasing function of $\theta$. Next, we will look at 3 cases depending on the relationship between $\theta$ and $\E[\theta]$: (A) If $\theta = \E[\theta]$, then $\piMMRC$ already satisfies \eqref{eq:modification}; (B) If $\theta < \E[\theta]$, then only the upper threshold is violated and we set $\piMMRC(k) = t_u ~ \forall k : \svbz_k \in \msf{Cap}_{\svbx}$ and re-normalize the remaining $\piMMRC(k)$; (C) If $\theta > \E[\theta]$, then only the lower threshold is violated, we set $\piMMRC(k) = t_l ~ \forall k : \svbz_k \notin \msf{Cap}_{\svbx}$ and re-normalize the remaining $\piMMRC(k)$. The re-normalization step does not violate \eqref{eq:modification}. We provide pseudo-code to calculate $\piMMRC$ in Algorithm \ref{alg:mmrc}. 

\begin{algorithm}
% \DontPrintSemicolon
\KwInput{$\varepsilon$-LDP cap-based mechanism $q(\cdot|\svbx)$, the associated $\msf{Cap}_{\svbx}$, reference distribution $p(\cdot)$, number of candidates $N$, lower threshold $t_l$, upper threshold $t_u$}
$\piMRC(\cdot), \{\svbz_1 , \cdots, \svbz_N \} \leftarrow$ $\MRC(p(\cdot), q(\cdot|\svbx), N)$\\
$\theta \leftarrow \frac{1}{N} \sum_{k} \Indicator(\svbz_k \in \msf{Cap}_{\svbx})$ \tcp*[h]{Compute the fraction of candidates inside the cap}\\
\KwInitialization{$\piMMRC(\cdot) \leftarrow \piMRC(\cdot)$}
\If{$\max_{k} \piMMRC(k) > t_u$} 
  {\tcp*[h]{Upper threshold is violated}\\
  $\piMMRC(k) \leftarrow t_u$, $\forall ~ k : \svbz_k \in \msf{Cap}_{\svbx}$
  $\piMMRC(k) \leftarrow \frac{1-N \theta t_u }{N(1 - \theta)}$, $\forall ~ k : \svbz_k \notin \msf{Cap}_{\svbx}$ 
  }
\ElseIf{{$\min_{k} \piMMRC(k) < t_l$}}
    {\tcp*[h]{Lower threshold is violated}\\
  $\piMMRC(k) \leftarrow t_l$, $\forall ~ k : \svbz_k \notin \msf{Cap}_{\svbx}$
  $\piMMRC(k) \leftarrow \frac{1-N (1-\theta) t_l }{N \theta }$, $\forall ~ k : \svbz_k \in \msf{Cap}_{\svbx}$
  }
 \KwOutput{$\piMMRC(\cdot), \{\svbz_1 , \cdots, \svbz_N \}$}
% \Else
% {\tcp*[h]{No threshold is violated}\\
% $\piMMRC(k) \leftarrow \piMMRC(k)$, $\forall ~ k$
% }
\caption{$\MMRC$}
\label{alg:mmrc}
\end{algorithm}

% \subsubsection{MMRC is locally differentially private}
Let $\qMMRC$ denote the distribution of $\svbz_K$ where $K \sim \piMMRC(\cdot)$.
% i.e., with $\delta(\cdot)$ denoting the Dirac delta function:
% \begin{align}
%     \qMMRC(\svbz | \svbx) \coloneqq \sum_{k} \piMMRC(k) \delta(\svbz - \svbz_k). \label{eq:qmmrc}
% \end{align}
In the following Theorem, we show that $\piMMRC$ is $\varepsilon$-LDP implying that the compressed mechanism $\qMMRC$ is $\varepsilon$-LDP. The proof follows from \eqref{eq:modification} and can be found in Appendix \ref{appendix:privacy_mmrc}.
\begin{restatable}[DP guarantee of $\MMRC$]{theorem}{mmrcprivacy}\label{theorem:mmrc_privacy}
Consider any input alphabet $\cX$, output alphabet $\cZ$, data $\svbx \in \cX$, and $\varepsilon$-LDP cap-based mechanism $q(\cdot|\svbx)$.
Let the reference distribution $p(\cdot)$ be the uniform distribution on $\cZ$. Consider any number of candidates $N \geq 1$. Then, $\piMMRC(\cdot)$ obtained from Algorithm \ref{alg:mmrc} is an $\varepsilon$-LDP mechanism.
\end{restatable}
% \begin{proof}
% The proof follows from \eqref{eq:modification} after observing that
% \begin{align}
%     \frac{t_u}{t_l} = \frac{c_1(\varepsilon,d)}{c_2(\varepsilon,d)} \leq \exp(\varepsilon)
% \end{align}
% \end{proof}
% \subsubsection{MMRC can simulate any $\varepsilon$-LDP step mechanism in a nearly lossless fashion with $O(\varepsilon)$ bits of communication} 
The following Theorem shows that, with number of candidates exponential in $\varepsilon$, samples drawn from $\qMMRC$ will be similar to the samples drawn from $\qMRC$ in terms of $\ell_2$ error. A proof can be found in Appendix \ref{appendix:utility_mmrc}.

\begin{restatable}[Utility of $\MMRC$]{theorem}{mmrcaccuracylocalwrtmrc}\label{theorem:mmrc_accuracy_local_wrt_mrc}
Consider any input alphabet $\cX$, output alphabet $\cZ$, data $\svbx \in \cX$, and $\varepsilon$-LDP cap-based mechanism $q(\cdot|\svbx)$.
Let the reference distribution $p(\cdot)$ be the uniform distribution on $\cZ$.
Let $N$ denote the number of candidates. Then, $\qMMRC$ is such that 
\begin{align}
    \Expectation_{\qMMRC} \big[ \lV  \svbz - \svbx \rV^2_2  \big]  \leq \Expectation_{\qMRC} \big[ \lV  \svbz - \svbx \rV^2_2  \big] + \sqrt{\frac{\rho (1+\varepsilon)}{2}}  \max_{\svbx, \svbz} \lV  \svbz - \svbx \rV^2_2  \label{eq:mmrc_utility}
\end{align}
 where $\rho \in (0,1)$ is such that $N = \frac{2\lp\exp(\varepsilon)-1\rp^2}{\rho^2}  \ln\frac{2}{\rho}.$
% \begin{align}
% \textstyle
%     N = \frac{2\lp\exp(\varepsilon)-1\rp^2}{\rho^2}  \ln\frac{2}{\rho}.
% \end{align}
\end{restatable}
For bounded mechanisms, $\max_{\svbx, \svbz} \lV  \svbz - \svbx \rV^2_2$ in \eqref{eq:mmrc_utility} can be well-controlled. See Remark~\ref{rmk:error_max} in Appendix \ref{appendix:utility_mmrc} for a discussion.

In conjunction with Theorem \ref{theorem:mrc_accuracy_local}, Theorem \ref{theorem:mmrc_accuracy_local_wrt_mrc} implies that $\qMMRC$ can compress $q(\cdot|\svbx)$ to the order of $\varepsilon$ bits of communication and simulate it in a near-lossless fashion. This is stated formally and proved in Appendix \ref{appendix:utility_mmrc}.