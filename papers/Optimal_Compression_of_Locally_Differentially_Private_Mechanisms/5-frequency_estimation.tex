\section{Frequency Estimation}
\label{sec:frequency_estimation}
In this section, we study the frequency estimation problem, which is another canonical statistical task in distributed distribution estimation, with application to federated analytics \citep{RM20}. 

Let $\mcal{X}$ be a set of $d$ distinct symbols and without loss of generality $\mcal{X} \coloneqq \lbp e_1, e_2,...,e_d \rbp$, where $e_j \in \{0, 1\}^d$ is the $j^{th}$ standard unit vector i.e., $e_j$ is the one-hot encoding of $j$. Consider $n$ users where user $i$  
% let each of $n$ users $i = 1,...,n$ 
has some data $\svbx_i \in \mcal{X}$. For every $i \in [n]$, let $\svbx_i$ be privatized using an $\varepsilon$-LDP mechanism $q(\cdot|\svbx_i)$ and potentially post-processed to obtain an estimate $\hat{\svbx}_i$ of $\svbx_i$.
% Consider $n$ users where user $i$ observes a sample $\svbx_i \in \mcal{X}$.
We are interested in estimating the \emph{empirical distribution} of $\svbx_1, \cdots, \svbx_n$ defined as $\Pi \eqDef \frac{1}{n}\sum_i \svbx_i$ using $\hat{\svbx}_1,\cdots,\hat{\svbx}_n$ such that the estimation error defined below is minimized:
\begin{equation}\label{eq:rfe_def}
r_{\texttt{FE}} \lp \hat{\Pi}, q, \ell \rp \eqDef \max_{\svbx^n \in \mcal{X}^n} \E\lb \ell \lp  \hat{\Pi}(\hat{\svbx}_1,...,\hat{\svbx}_n), \Pi \rp \rb,
\end{equation}
where $\ell =  \lVert \cdot \rVert_1$ or $\lVert \cdot \rVert^2_2$, $\hat{\Pi}$ is an estimate of $\Pi$ and
% $q(\cdot|\svbx)$ is an $\varepsilon$-LDP mechanism and $ \hat{\svbx}_i \sim q(\cdot|\svbx_i)$ for all $i \in [n]$ are the privatized samples. 
the expectation is with respect to $q(\cdot|\svbx_i) $ as well as all (possibly shared) randomness used by $q(\cdot|\svbx_i)~\forall i \in [n]$. For simplicity, we only focus on $\ell_2$ error i.e., $\ell = \lVert \cdot \rVert^2_2$.
% $q(\cdot|\svbx)$ is an $\varepsilon$-LDP scheme and $ \hat{\svbx}_i \sim q(\cdot|\svbx_i)$ for all $i \in [n]$ are the privatized samples. The expectation is with respect to $q(\cdot|\svbx_i)$ and all (possibly shared) randomness used by the local randomizers.

\cite{YB18} show that the $\SubsetSelection$ achieves the
order-optimal privacy-accuracy trade-off for frequency estimation i.e., $r_{\msf{FE}} \bigl( \hat{\Pi}^\texttt{ss}, \qSS \bigr) = \Theta\Bigl(
  \frac{d}{\min( e^\varepsilon, (e^\varepsilon-1)^2, d )}\Bigr)$ (where $\hat{\Pi}^\texttt{ss} \coloneqq \frac{1}{n}\sum_i \hat{\svbx}^{\texttt{ss}}_i$). Like $\PrivUnit$, compared to other (order-optimal) $\varepsilon$-LDP frequency estimation mechanisms, $\SubsetSelection$ admits the best constants and gives the smallest $\ell_2$ error in practice (see \cite{CKO20}).
% for a detailed comparison. 
However,  the communication cost associated with $\SubsetSelection$ is $O\bigl(\frac{d}{e^\varepsilon+1}\bigr)$ bits per user, which which can be
an issue for small and moderate $\varepsilon$. 
% To address this issue, we show that by using MMRC (Section \ref{subsec:mmrc}) to simulate $\SubsetSelection$, we can significantly compress $\SubsetSelection$ while preserving its accuracy.
 
Similar to $\PrivUnit$, one could apply the generic $\MMRC$ scheme defined in Section~\ref{sec:main_results} to compress and simulate $\SubsetSelection$. However, for a fixed number of candidates $N$, it yields a biased estimate of $\svbx$ and hence cannot get the correct (optimal) order of estimation error in \eqref{eq:rfe_def} i.e., the error would not decay with $n$. Fortunately, similar to $\PrivUnit$, we show (in Section~\ref{subsec:mmrc_ss}) that the bias can be corrected by appropriately translating and scaling the privatized version of $\svbx$ i.e., by using an estimator which is slightly different compared to the original estimator of $\SubsetSelection$.
Further, we also show (in Section \ref{subsec:sim_ss1}) that the resulting unbiased estimator for frequency estimation ($\hat{\Pi}^{\texttt{mmrc}}$) can simulate $\SubsetSelection$ closely while only using on the order of $\varepsilon$-bits communication.  
% Analogously, we could simulate $\SubsetSelection$ with $\MRC$ but we defer the details to Appendix BLAH.
% and focus on simulating $\SubsetSelection$ with MMRC below.

% \subsection{Simulating $\PrivUnit$ using MRC}
% First, we show that by using a different translating and scaling constants, $\qMRC$ associates an unbiased estimator.
% \begin{restatable}{theorem}{mrcssbias}\label{thm:mrc_ss_bias}
% Let $\qSS(\svbz | \svbx)$ be the  $\varepsilon$-LDP \emph{$\SubsetSelection$} mechanism. 
% Let $\svbz_1, \svbz_2,...,\svbz_N$ be generated from 
% $ \msf{uniform}\lp \mcal{Z}_{d, s} \rp$. Let $\piMRC$ be the distribution over $[N]$ as defined in \eqref{eq:pi_mrc}. Let $K \sim \piMRC(k)$. Define $\theta \sim \frac{1}{N}\msf{Binom}\lp N, \frac{s}{d} \rp$ and
% \begin{align}\label{eq:q_check}
% m_\texttt{mrc} & \coloneqq \E\lb \frac{\theta e^\varepsilon}{e^\varepsilon \theta+(1-\theta)} \rb - \frac{1}{d-1}\E\lb s-\frac{e^\varepsilon \theta}{e^\varepsilon \theta+(1-\theta)} \rb \nonumber\\
% b_\texttt{mrc} & \coloneqq \frac{1}{d-1}\E\lb s-\frac{e^\varepsilon \theta}{e^\varepsilon \theta + (1-\theta)} \rb
% \end{align}

% Then $ \hat{\svbx}_\texttt{mrc} \coloneqq (\svbz_K - b_\texttt{mrc})/m_\texttt{mrc}$ is an unbiased estimator of $\svbx$, i.e., $\E_{\qMRC(\cdot|\svbx)}\lb \hat{\svbx}_\texttt{mrc}(\svbz_K) \rb = \svbx$.
% \end{restatable}
% When the number of candidates $N$ is sufficiently high, then the scaling and translating parameters associated with of the MRC scheme converge to that of the $\SubsetSelection$ scheme (see Appendix~\ref{sec:mrc_ss_app} for more details). 

% The following theorem ensures that the accuracy of the MRC also converges to that of the $\SubsetSelection$.

% \begin{restatable}{theorem}{ssmrc}\label{thm:ss_mrc}
% Let $\qPU(\svbz | \svbx)$ be the  $\varepsilon$-LDP \emph{$\SubsetSelection$} mechanism with output $\hat{\svbx}_{\texttt{ss}}$. 
% Let $\qMRC(\svbz|\svbx)$ denote the MRC privatization mechanism simulating \emph{$\SubsetSelection$} with output $\hat{\svbx}_{\texttt{mrc}}$. 
% Let $N$ denote the number of candidates. 
% As long as 
% \begin{equation}\label{eq:N_bdd_mrc_ss}
%      N\geq \frac{64e^{2\varepsilon}(1+\lambda)}{\lambda^2\lp\min\lp 1/8, 3\varepsilon\rp\rp^2}\log\lp \frac{4(1+\lambda)}{\lambda\min\lp 1/8, 3\varepsilon \rp}\rp,
%  \end{equation}
%  $$ \E\lb \lV \hat{\svbx}_{\texttt{mrc}}-\svbx\rV^2_2\rb  \leq (1+4\lambda+o(\lambda)) \E\lb \lV \hat{\svbx}_{\texttt{ss}}-\svbx\rV^2_2\rb.$$
% \end{restatable}

% Again, setting $\hat{\Pi}_{ \texttt{mrc}} \coloneqq \frac{1}{n}\sum_i \hat{\svbx}_{\texttt{mrc}, i}$ and applying Theorem~\ref{thm:ss_mrc}, we obtain the following result.

% \begin{corollary}
% As long as $N$ satisfies \eqref{eq:N_bdd_mrc_pu},
% \begin{align}
%     &r_{\msf{FE}} \lp \hat{\Pi}_{ \texttt{mrc}}, \qMRC \rp \leq  
%     \lp 1+4\lambda+o(\lambda) \rp^2 r_{\msf{FE}} \lp \hat{\Pi}_{ \texttt{ss}}, \qSS \rp
% \end{align}
% In particular,
% $ r_{\msf{FE}} \lp \hat{\Pi}_{ \texttt{mrc}}, \qMRC \rp \ra  r_{\msf{FE}} \lp \hat{\Pi}_{ \texttt{ss}}, \qSS \rp, $
%  as long as $\lambda \ra 0$.
% % r_{\msf{FE}} \lp \hat{\svbmu}_\texttt{pu}, \qPU \rp = \Theta\lp \frac{d}{n\min\lp \varepsilon, \varepsilon^2 \rp} \rp
% \end{corollary}


\subsection{Debiasing \texorpdfstring{$\MMRC$}{MMRC} to simulate \texorpdfstring{$\SubsetSelection$}{Subset Selection}}
\label{subsec:mmrc_ss}
Let us focus on a single user and consider some data $\svbx \in \cX$.
Recall the $\SubsetSelection$ $\varepsilon$-LDP mechanism $\qSS$ described in Section~\ref{sec:preliminaries} with $s \coloneqq \lceil \frac{d}{1+e^\varepsilon}\rceil$. $\SubsetSelection$ is cap-based mechanism as discussed in Section \ref{sec:main_results} and Appendix \ref{appendix:ss} with $\msf{Cap}_{\svbx} = \cZ_{\svbx}$ and $\Probability_{\svbz \sim \Unif(\cZ)}\lp \svbz \in \msf{Cap}_{\svbx} \rp = s/d$.
% with appropriate $c_1(\varepsilon,d)$ and $c_2(\varepsilon,d)$ defined in Section BLAH and $\Probability_{\svbz \sim \Unif(\cZ)}\lp \svbz \in \msf{Cap}_{\svbx} \rp$ evaluated in Appendix BLAH. Recall the definitions of lower threshold $t_l$ and upper threshold $t_u$ from Section \ref{subsec:mmrc}. 
\iffalse
Let $\piMMRC$ be the distribution and $\svbz_1, \svbz_2,...,\svbz_N$ be the candidates obtained from Algorithm \ref{alg:mmrc} when the reference distribution is uniform on $\cZ$ defined in Section BLAH. Let $K \sim \piMMRC(\cdot)$. Recall that $\theta$ is the fraction of candidates inside the $\msf{Cap}_{\svbx}$. It is easy to see that $\theta \sim \frac{1}{N}\msf{Binom}\lp N, \frac{s}{d} \rp$. Further, define 
\begin{align}
    b_\texttt{mmrc} \coloneqq \frac{1}{d-1} \bigg(s - \E \bigg[ \frac{e^\epsilon\theta \cdot \Indicator \lp\theta\leq \E\lb\theta\rb \rp}{e^\epsilon \E\lb\theta\rb + (1-\E \lb\theta\rb)}   \\
    + \frac{e^\epsilon \E \lb\theta\rb +\theta- \E \lb\theta\rb}{e^\epsilon \E\lb\theta\rb + (1-\E \lb\theta\rb)}  \cdot \Indicator \lp\theta> \E\lb\theta\rb \rp \bigg] \bigg)
\end{align}
and $m_\texttt{mmrc} \coloneqq s - d\cdot b_\texttt{mmrc}$. Define $\hat{\svbx}^\texttt{mmrc} \coloneqq (\svbz_K- b_\texttt{mmrc} )/ m_\texttt{mmrc}$ as the estimator of the $\MMRC$ mechanism simulating $\SubsetSelection$.  
\fi
Similar to Section \ref{subsec:mmrc_privunit}, let $\svbz_K$ be the privatized version of $\svbx$ using $\MMRC$. We define $\hat{\svbx}^\texttt{mmrc} \coloneqq (\svbz_K- b_\texttt{mmrc} )/ m_\texttt{mmrc}$ as the estimator of the $\MMRC$ mechanism simulating $\SubsetSelection$ where $m_\texttt{mmrc}$ and $b_\texttt{mmrc}$ (defined in Appendix \ref{appendix:mmrc_ss_debias}) are translation and scaling factor analogous to $m_\texttt{ss}$ and $b_\texttt{ss}$ in \eqref{eq:m_ss}. The following Lemma shows that $\hat{\svbx}^\texttt{mmrc}$ is an unbiased estimator. See Appendix \ref{appendix:mmrc_ss_debias} for a proof.

\begin{restatable}{lemma}{mmrcssbias}\label{lemma:mmrc_ss_bias}
Let $\hat{\svbx}^\texttt{mmrc}$ be the estimator of the \emph{$\MMRC$} mechanism simulating \emph{$\SubsetSelection$} as defined above. Then, $\Expectation[\hat{\svbx}^\texttt{mmrc}] = \svbx$.
\end{restatable}

% \textRed{The following Lemma (i) shows that $\qMMRC$ can compress $\qSS$ to the order of $\varepsilon$-bits of communication and (ii) bounds the $\ell_2$ error of the estimator $\hat{\svbx}^\texttt{mmrc}$ in terms of the $\ell_2$ error of the estimator $\hat{\svbx}^\texttt{ss}$.}
% The following Theorem shows that $\qMMRC$ can compress $\qSS$ to the order of $\varepsilon$-bits of communication as well as simulate it in a near-lossless fashion. 
% A proof can be found in Appendix \ref{appendix:mmrc_ss_ut}. Similar to $\PrivUnit$, the key idea in the proof is to show that when the number of candidates $N$ is exponential in $\varepsilon$, the scaling factor $m_{\texttt{mmrc}}$ is close to the scaling parameter associated with $\qSS$ (i.e., $m_{\texttt{ss}}$ defined in \eqref{eq:m_ss}).
% and the scaling factor $m_{\texttt{mmrc}}$ of the MMRC mechanism simulating $\SubsetSelection$ are close.
% Recall that to obtain the exact $\varepsilon$-LDP guarantee, we need to leverage the MMRC scheme to simulate $\SubsetSelection$. We apply the same technique to correct the bias and the resulting accuracy also converges to the uncompressed $\SubsetSelection$ scheme. We summarize the result in the following theorem, and the proofs and the precise bias correction steps are deferred to Appendix~\ref{sec:mmrc_ss_app}.
%  \begin{restatable}{lemma}{ssmmrc}\label{theorem:mmrc_accuracy_ss}
%  Let $\qSS(\svbz | \svbx)$ be the  $\varepsilon$-LDP \emph{$\SubsetSelection$} mechanism with parameters $d$ and $s=\lceil \frac{d}{1+e^\varepsilon} \rceil$ and estimator $\hat{\svbx}^{\texttt{ss}}$. Let $\qMMRC(\svbz|\svbx)$ denote the \emph{$\MMRC$} privatization mechanism simulating \emph{$\SubsetSelection$} with $N$ candidates and estimator $\hat{\svbx}^{\texttt{mmrc}}$ as defined above. Consider any $\lambda > 0$. Then,
%  \begin{align}
%       & \E_{\qMMRC}\hspace{-0.5mm}\lb \lV \hat{\svbx}^{\texttt{mmrc}} \hspace{-1.0mm}- \hspace{-0.5mm}\svbx\rV^2_2\rb  \hspace{-1.0mm}\leq\hspace{-1.0mm} (1\hspace{-0.75mm}+\hspace{-0.75mm}4\lambda\hspace{-0.75mm}+\hspace{-0.75mm}5\lambda^2\hspace{-0.75mm}+\hspace{-0.75mm}2\lambda^3) \E_{\qSS}\hspace{-0.5mm}\lb \lV \hat{\svbx}^{\texttt{ss}}\hspace{-1.0mm}-\hspace{-0.5mm}\svbx\rV^2_2\rb,
%  \end{align}
%  \vspace{-3mm}
%   as long as 
%  \begin{align}\label{eq:N_bdd_mmrc_ss}
%   \textstyle
%       N\geq \frac{2(e^{\varepsilon}+1)^2(1+\lambda)^2}{0.24^2\lambda^2}\ln\lp \frac{8(1+\lambda)}{0.24\lambda}\rp.
%   \end{align}
%  \end{restatable}

\subsection{Simulating \texorpdfstring{$\SubsetSelection$}{Subset Selection} using \texorpdfstring{$\MMRC$}{MMRC}}
\label{subsec:sim_ss1}

Finally, we consider estimating the empirical frequency $\Pi$ defined earlier using the $\MMRC$ scheme simulating $\SubsetSelection$. To that end, consider $n$ users and let $\hat{\svbx}^{\texttt{mmrc}}_i$ be the unbiased estimator of $\svbx_i$ at the $i^{th}$ user. Let the (unbiased) estimate of $\Pi$ be $\hat{\Pi}^{ \texttt{mmrc}} \coloneqq \frac{1}{n}\sum_i \hat{\svbx}^\texttt{mmrc}_i$. 
The following Theorem shows that, for frequency estimation, $\MMRC$ can simulate $\SubsetSelection$ in a near-lossless manner (when $\lambda$ is small) while only using on the order of $\varepsilon$ bits of communication. A proof can be found in Appendix \ref{appendix:mmrc_ss_ut}. Similar to $\PrivUnit$, the key idea in the proof is to show that when the number of candidates $N$ is exponential in $\varepsilon$, the scaling factor $m_{\texttt{mmrc}}$ is close to the scaling parameter associated with $\qSS$ (i.e., $m_{\texttt{ss}}$ defined in \eqref{eq:m_ss}).

% For all $i \in [n]$, since $\hat{\svbx}^{\texttt{mmrc}}_i$ are independent of each other as well as unbiased, we obtain the following Theorem directly from Lemma~\ref{theorem:mmrc_accuracy_ss}.

\begin{restatable}{theorem}{mmrcss}
\label{thm:fe_mmrc_ss}
 Let $r_{\msf{FE}} \lp \hat{\Pi}^\texttt{ss}, \qSS \rp$ and $r_{\msf{FE}} \lp \hat{\Pi}^\texttt{mmrc}, \qMMRC \rp$ be the empirical mean estimation error for \emph{$\SubsetSelection$} and \emph{$\MMRC$} simulating \emph{$\SubsetSelection$} with $N$ candidates respectively. Consider any $\lambda > 0$. Then
 \begin{equation}
     r_{\msf{FE}} \lp \hat{\Pi}^{ \texttt{mmrc}}, \qMMRC \rp \leq  
     \lp 1+4\lambda+5\lambda^2+2\lambda^3 \rp r_{\msf{FE}} \lp \hat{\Pi}^{ \texttt{ss}}, \qSS \rp,
 \end{equation}
 as long as 
  \begin{align}\label{eq:N_bdd_mmrc_ss}
      N\geq \frac{2(e^{\varepsilon}+1)^2(1+\lambda)^2}{0.24^2\lambda^2}\ln\lp \frac{8(1+\lambda)}{0.24\lambda}\rp.
  \end{align}
 \end{restatable}

Similar to mean estimation, while a specific value of $\lambda$ can be chosen in \eqref{eq:N_bdd_mmrc_ss}, in practice, the number of bits could be fixed (see Section \ref{subsec:mmrc_ss_empirical}), determining the value of $\lambda$.

\subsection{Empirical Comparisons.}
\label{subsec:mmrc_ss_empirical}
Next, we empirically demonstrate the privacy-accuracy-communication tradeoffs of $\MMRC$ simulating $\SubsetSelection$. Along with $\SubsetSelection$, we compare against the RHR algorithm of \cite{CKO20} which offers order-optimal privacy-accuracy tradeoffs while requiring only $\varepsilon$ bits. Following \cite{acharya2019hadamard}, we generate $\svbx_1, \cdots, \svbx_n$ from the Zipf distribution with degree 1. We report the average $\ell_2$ estimation error over 10 runs. See more variations in 
% (more details and variations in 
Appendix \ref{appendix:mmrc_ss_emp}.

\begin{figure}[h]
\centering
\includegraphics[width=0.45\linewidth]{figures/z_cc6} \qquad
\includegraphics[width=0.45\linewidth]{figures/z_eps1}
\caption{Comparing $\SubsetSelection$, $\MMRC$ simulating $\SubsetSelection$ and RHR for frequency estimation with $d = 500$ and $n = 5000$. \textbf{Left:} $\ell_2$ error vs $\#$bits for $\varepsilon = 6$. \textbf{Right:} $\ell_2$ error vs $\varepsilon$ for $\#$bits $= \max\{\lceil( \varepsilon/ \ln 2)\rceil + 3, 8\}$. RHR uses $\#$-bits $= \varepsilon$ for both as it leads to a poor performance if $\#$-bits $ > \varepsilon$.}
\label{fig:freq}
\end{figure}

In Figure \ref{fig:freq} (Left), we show the communication-accuracy tradeoffs. We see that with correct order of bits, the accuracy of $\MMRC$ simulating $\SubsetSelection$ converges to the accuracy of the uncompressed $\SubsetSelection$. In Figure \ref{fig:freq} (Right), we show the privacy-accuracy tradeoffs. More specifically, $\MMRC$ simulating $\SubsetSelection$ can attain the accuracy of the uncompressed $\SubsetSelection$ for the range of  $\varepsilon$'s typically considered by LDP mechanisms while only using $\max\{\lceil( \varepsilon/ \ln 2)\rceil + 3, 8\}$ bits. 