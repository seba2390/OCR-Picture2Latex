\section{Related Works}
\label{sec:related_works}
\vspace{-9pt}
Machine learning has brought significant advancements in many fields, and program analysis and software engineering are no exceptions. 
However, Machine Learning (ML) and Deep Learning (DL) models can not directly process raw source code to reason about programs. Therefore, researchers have explored different approaches to represent applications in a format suitable for DL models.
Generally, there are three types of commonly used program presentations: sequence of tokens, Abstract Syntax Tree (AST), and Intermediate Representation (IR).

\textbf{Sequence of tokens:} The initial attempts \cite{dam2016deep, gu2016deep, levy2017learning} represented source code as a sequence of tokens, such as identifiers, variable names, or operators. This approach intuitively treats programming languages similarly to natural languages.
It allows for the utilization of advanced natural language process (NLP) techniques, such as large language models \cite{feng2020codebert, guo2020graphcodebert, guo2022unixcoder}. 
However, this token-based representation overlooks the inherent dependency information within the program's structure. It fails to capture the unique relationships and dependencies between different elements of the code, which can limit its effectiveness in tasks such as compiler optimization and code optimization.

\textbf{AST:} An AST represents the structure of a program by capturing its hierarchical organization. It is constructed based on the syntactic rules of the programming language and provides a high-level abstraction of the code.
Previous works have leveraged ASTs as inputs to tree-based models for various code analysis tasks like software defect prediction ~\cite{dam2019lessons} and code semantic study ~\cite{chen2019capturing}. Moreover, there have been efforts to augment ASTs into graphs that incorporate program analysis flows such as control flow and data flow. These AST-based graph representations capture more comprehensive code dependency information and have shown superior results compared to traditional approaches in previous works \cite{allamanis2022graph, allamanis2017learning}. 

\textbf{IR: }
IR is an intermediate step between the source code and the machine code generated by a compiler. Previous work ~\cite{venkatakeerthy2020ir2vec} has utilized IR to train an encoding infrastructure for representing programs as a distributed embedding in continuous space. It augments the Symbolic encodings with the flow of information to capture the syntax as well as the semantics of the input programs. However, it generates embedding at the program or function level and also requires a data-flow analysis type for generating the embedding. In contrast, our approach derives embedding from the representation and works at the more fine-grained instruction level. More recent works ~\cite{ben2018neural, cummins2020programl, brauckmann2020compiler} have leveraged IR-based graph representation to better capture essential program information, such as control flow, data flow, and dependencies.
However, despite their success, IR-based graph representations have certain limitations.
For example, these representations may not be numeric-aware or may lack the ability to adequately represent aggregate data types. In this work, we propose \ourtool, a graph representation based on IR, to address these limitations.