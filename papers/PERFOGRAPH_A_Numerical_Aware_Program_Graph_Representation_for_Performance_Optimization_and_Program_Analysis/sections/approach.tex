\section{\ourtool: A fined-grained numerical aware graph representation}
\label{sec:approach}
\vspace{-4pt}
\ourtool is graph representation based on LLVM IR. It is built on top of \programl; however, it does not suffer from the limitations that the \programl has, helping DL models to reason over the complex structure of the programs and enabling them to make more accurate optimization decisions, especially in terms of performance optimization.
Figure \ref{fig:overview_of_perfograph} shows how various enhancements and improvements are applied to construct a more precise representation. 
Consider a simple code example of defining a variable and increasing it by one \{\mintinline{cpp}|int i = 0; i++;|\}. Figure \ref{fig:programl_variable_definition} shows the \programl representation of this code example. It has two \texttt{store} nodes, as one is responsible for storing 0 and the other one storing the incremented value of \texttt{i}.
In the following subsection, we will explain how \ourtool is constructed by addressing the limitations shown in Figure \ref{fig:programl_variable_definition}.

\begin{figure}[H]
\vspace{-10pt}
      \setkeys{Gin}{width=\linewidth}
     \begin{subfigure}[t]{0.22\textwidth}
         \includegraphics{images/pgplus_examples/pg_vanila.pdf}
         \caption{Initial graph}
         \label{fig:programl_variable_definition}
     \end{subfigure}
     \hfill
     \begin{subfigure}[t]{0.18\textwidth}
         \centering
         \includegraphics{images/pgplus_examples/pg_one_child_alloca.pdf}
         \caption{Local variables}
         \label{fig:presenting_local_variables}
     \end{subfigure}
     \hfill
     \begin{subfigure}[t]{0.17\textwidth}
         \centering
         \includegraphics{images/pgplus_examples/pg_after_store.pdf}
         \caption{\texttt{store} nodes}
         \label{fig:store_nodes}
     \end{subfigure}
     \hfill
     \begin{subfigure}[t]{0.18\textwidth}
         \centering
         \includegraphics{images/pgplus_examples/pg_after_vector_with_number.pdf}
         \caption{Numbers}
         \label{fig:numerical_values}
     \end{subfigure}
        \caption{\ourtool addresses the existing limitations in program representation.}
        \label{fig:overview_of_perfograph}
    \vspace{-15pt}
    \end{figure}

\subsection{Representing Local Identifiers and \texttt{store} instruction}
\textbf{Local Identifiers:} Local identifiers' names are preceded by \texttt{\%} in LLVM Intermediate representation.
Memory allocation on the stack is done by \texttt{alloca} instruction.
One of the limitations of the current state-of-the-art program representation, \programl, is that it is unable to carry out information regarding the operations that happen to a memory location.
For instance, in Figure \ref{fig:programl_variable_definition}, the two \texttt{store} nodes represent storing values of 0 and 1 to variable \texttt{i}. However, as shown, each \texttt{store} instruction node is connected to a separate variable node, making it difficult for the graph neural network to reason over the operations that happen to a memory location. For the embedding vector of the second \texttt{store} node in \ref{fig:programl_variable_definition} to represent the fact that some information regarding the variable \texttt{i} has changed, one has to increase the number of GNN layers to 3 to support up to 3 hops when propagating the messages in GNN. This can potentially limit the ability of the GNN model if there are a greater number of hops between the two \texttt{store} nodes shown in Figure \ref{fig:programl_variable_definition}.
To address this limitation, instead of having more than one variable node (oval-shape nodes) per identifier, \ourtool only considers one variable node in its graph representation. Any \texttt{load} or \texttt{store} instruction will refer to the same variable node. These changes are shown in Figure \ref{fig:presenting_local_variables}. We see that the \texttt{store} nodes in Figure \ref{fig:presenting_local_variables} access the same memory location, thus representing the fact that those \texttt{store} instructions are modifying the same memory location.

\textbf{\texttt{Store} instruction:} 
LLVM uses \texttt{store} instruction to write into memory. \texttt{store} instruction has two arguments: a value to store and the address to which it will store the value.
\programl differentiates between these two arguments by adding a \texttt{position} feature to the edges as shown in Figure \ref{fig:programl_variable_definition}.
However, since the \texttt{store} instruction modifies the contents at the corresponding memory address, we posit that it is better to reflect the fact the content of the identifier has changed. 
To present this information, \ourtool adds an extra edge from the \texttt{store} node to node representing the identifier whose value is modified by the \texttt{store} instruction. 
Figure \ref{fig:store_nodes} shows these changes in the graph constructed by \ourtool.

\textbf{Numbers:} Numbers can be a significant factor in optimization decisions. For example, they can show the loop bound, and different optimizations can be considered depending on the loop bound. 
\ourtool, unlike \programl, not only considers the type of numbers such as \texttt{i32}, \texttt{i64}, \texttt{float} but also the actual values of the numbers.
%\ourtool presents numbers as constant nodes (shown as diamond nodes in Figure \ref{fig:numerical_values}.
As illustrated in Figure \ref{fig:numerical_values}, numerical constant nodes have the actual value of the number in their feature set in addition to the type of the number. Even though numerical constant nodes have the value of the number as one of their features, there is a need to embed the numbers in a way that unknown numbers will not be seen in the inference.
Unlike other tokens, numbers are harder to embed as an infinite amount of numbers exists, and to handle all ranges of numbers, we need to have a very large vocabulary set.
\vspace{-7pt}
\subsection{Numerical Awareness}
\begin{wrapfigure}{r}{0.4\textwidth}
\vspace{-50pt}
  \begin{center}
\includegraphics[width=0.4\textwidth]{images/digit_embedding.pdf}
\caption{Overview of the digit embedding.}
\label{figs:digit_emb}
  \end{center}
  \vspace{-10pt}
\end{wrapfigure}
However, with a very large vocabulary size, the DL models may still encounter numbers in the inference phase that they have not seen in the training phase.
We propose a novel way of embedding numbers called Digit Embedding.
Figure \ref{figs:digit_emb} shows our approach. To embed a number, we first break down the number to its digits; then, we consider a position for each one of the digits. The goal is to let DL models realize the place value of each digit. Then, each digit and its corresponding position are embedded and summed together. Therefore, we will have an embedding representing the information about the digits and their positions. For instance, in Figure \ref{figs:digit_emb}, we embed each digit and its corresponding position with an output dimension of 3. Since the number has four digits, the results would be a vector/tensor of size $4\times3$. To make sure the Digit Embedding of numbers has the same length across numbers with varying sizes of digits, we apply an aggregation function over the embedding dimension. Since the output embedding dimension is three in this example, we would have one vector of length three representing the number after aggregation. The aggregation function can be of any type (Max, Mean, etc.). 
\vspace{-10pt}
\subsection{Aggregate Data Types}
\vspace{-8pt}
    \begin{wrapfigure}{R}{.4\textwidth}
    \vspace{-20pt}
        \vspace*{\fill}
        \centering
        {\includegraphics[width=.4\textwidth]{images/pgplus_examples/pg_before_vector.pdf}
        \subcaption{No support for aggregate data types.}
        \label{fig:before_vector}}\par\vfill
        {\includegraphics[width=.4\textwidth]{images/pgplus_examples/pg_after_vector.pdf}
        \subcaption{Break-down of aggregate data types.}
        \label{fig:after_vector}}
        \caption{\ourtool supports aggregate data types.}
        \label{fig:vector_support}
        \vspace{-10pt}
    \end{wrapfigure}
Aggregate data types, such as arrays and vectors, are an essential part of applications. They play an important role in many applications, such as matrix multiplications. Thus, presenting these data types helps the DL models better understand programs. 
Current LLVM IR-based program representations fail to present aggregate data types appropriately.
For example, consider a three-dimensional integer array. In LLVM IR, this array is shown as \mintinline{llvm}|3 x [2 x [3 x i32]]]*|.
As can be seen, the length of the arrays and their data types are inferable. However, without proper representation, the DL model's capacities will be spent on learning these deterministic facts (i.e., the length of the arrays and their type).
\ourtool considers aggregate data types as a new node type in its representations.
Figure \ref{fig:after_vector} shows how aggregate data types are supported by \ourtool.
Unlike other LLVM IR-based representations, \ourtool supports multi-dimensional arrays and vectors.
\ourtool creates a chain of nodes to present the different dimensions of the arrays. In Figure \ref{fig:before_vector}, we see there is a node representing the three-dimensional array \mintinline{llvm}|[3 x [2 x [3 x i32]]]*|. \ourtool breaks down the corresponding node into three (since it is a three-dimensional array) white nodes as shown in Figure \ref{fig:after_vector}. Then, each node has a context representing that specific dimension of the array. For example, the context for the third dimension is \mintinline{llvm}|[3 x i32]|, whereas for the second dimension, the context is \mintinline{llvm}|[2 x [3 x i32]]|.
For each aggregate type node, in addition to the context of the node, we specifically add the length of the array and its type as additional features.
For aggregate data types whose lengths are not known during compile time, we follow the LLVM conventions by considering the length of those data types as \texttt{vscale}.
These enhancements will help the DL models to reason over the dimensions and types of aggregate data types. As a result, \ourtool will ultimately enable the DL models to have more accurate predictions for applications that deal with arrays and vectors.

\vspace{-11pt}