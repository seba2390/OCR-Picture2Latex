\subsection{Ablation Study}

We further analyzed how each one of the enhancements in \ourtool affects the end results. We performed an ablation study on the Device Mapping task and trained our GNN models on variations of \ourtool.

\textbf{Results without aggregate data type nodes:}
First, aggregate data type nodes are removed from  \ourtool representation.
Please note that in this setup, the Digit Embedding is still applied. Table \ref{tab:final-amd} and \ref{tab:final-nvidia} shows the results. It can be seen that when the representation does not support aggregate data type, the error rate increases to 13\% in AMD and 15\% in the NVIDIA dataset. This clearly indicates that having aggregate-type nodes in the representation helped the model to learn the code features more accurately.

\textbf{Results without Digit Embedding:}
Then Digit Embedding is removed from the pipeline for the second experiment and the aggregate data type nodes are kept. Table \ref{tab:final-amd} and \ref{tab:final-nvidia} shows the results. We can see that unlike having aggregate data type nodes, removing digit embedding does not hurt the error rate that much for the task of device mapping. However, we can still see a small increase (1.1\%) in the error rate for the AMD dataset. For the NVIDIA dataset, the error rate increases from 10.0 to 10.6\%.

\begin{table}[H]
\vspace{-15pt}
\captionsetup{justification=centering}
  \caption{Summarizing \ourtool results for AMD device.}
  \small
  \setlength\tabcolsep{3.5pt}
  \label{tab:final-amd}
  \centering
  \begin{tabular}{cccccc}
    \toprule
    Approach & Error (\%) \\
    \midrule
    DeepTune ~\cite{cummins2017end} & 28.1 \\
    inst2vec ~\cite{ben2018neural} & 19.7 \\
    \programl ~\cite{cummins2020programl} & 13.4 \\
    \ourtool (without aggregate data type nodes) & 13.0 \\
    \ourtool (without digit embedding) & 7.1 \\
    \ourtool (aggregate data type nodes + digit embedding) & 6.0 \\
    \bottomrule
  \end{tabular}
\end{table}
\vspace{-15pt}
\begin{table}[H]
\captionsetup{justification=centering}
  \caption{Summarizing \ourtool results for NVIDIA device.}
  \small
  \setlength\tabcolsep{3.5pt}
  \label{tab:final-nvidia}
  \centering
  \begin{tabular}{cccccc}
    \toprule
    Approach & Error (\%) \\
    \midrule
    DeepTune ~\cite{cummins2017end} & 39.0 \\
    inst2vec ~\cite{ben2018neural} & 21.5 \\
    \programl ~\cite{cummins2020programl} & 20.0 \\
    \ourtool (without aggregate data type nodes) & 15.0 \\
    \ourtool (without digit embedding) & 10.6 \\
    \ourtool (aggregate data type nodes + digit embedding) & 10.0 \\
    \bottomrule
  \end{tabular}
\end{table}

Finally, we can conclude that both components in our representation helped the model to learn the code features better to some extent. However, aggregate data type nodes in the embedding helped our model more than Digit Embedding for the task of device mapping. The reason can be that there are not many numbers in the dataset. However, in tasks where there are many numbers, Digit Embedding can play a significant role.