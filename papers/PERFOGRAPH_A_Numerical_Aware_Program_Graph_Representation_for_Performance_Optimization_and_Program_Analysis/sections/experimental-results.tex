\section{Experimental Results and Downstream Tasks}
\label{sec:results}
\vspace{-8pt}
In this section, we evaluate \ourtool on six downstream tasks.
For each downstream task, we will explain the task itself, the dataset, and the baselines.
%More details regarding the dataset, the models that are experimented with, and the baselines can be found in the supplementary material section.
\vspace{-4pt}
\subsection{Experimental Setup}
\vspace{-4pt}

In our experiments, we use DGL's \cite{wang2019deep} RGCN \cite{schlichtkrull2018modeling} implementation for \ourtool representation. The graphs from \ourtool are treated as heterogeneous and managed via the \texttt{HeteroGraphConv} module. We use a hardware setup of two 18-core Intel Skylake 6140 CPUs and two NVIDIA Tesla V100-32GB GPUs. The embedding space for numbers is generated by extracting digits and positions from a numeric token of an IR statement, then passed to a PyTorch \cite{NEURIPS2019_9015} embedding layer for digit and position embeddings. These are combined for the final numeric token embedding. Non-numeric tokens directly go through the PyTorch embedding layer. Each \ourtool heterogeneous node converts to a 120-dimensional vector via this embedding. We use the \texttt{Adam} \cite{kingma2014adam} Optimizer, \texttt{relu} \cite{agarap2018deep} activation function, a learning rate of $0.01$, and \texttt{hidden\_dim} parameter of $60$. Mean aggregation is applied to combine different node-type results before a linear classification layer, which outputs a probability distribution for each class. The class with the highest probability is the prediction.
\vspace{-4pt}

\subsection{Device Mapping}
\vspace{-4pt}
\begin{wrapfigure}{R}{0.5\textwidth}
\vspace{-20pt}
  \begin{center}
\includegraphics[width=0.45\textwidth]{images/results/device_mapping_error.pdf}
\vspace{-8pt}
\captionsetup{justification=centering}
\caption{Performance comparison the device mapping task with state-of-the-art models [lower is better].}
\vspace{-15pt}
\label{figs:device_mapping_error}
  \end{center}
\end{wrapfigure}
\textbf{Problem Definition:}
We apply \ourtool to the challenging heterogeneous device mapping \cite{cummins2017end} problem.
In this task, there are a number of OpenCL kernels that we need to predict which accelerator (CPU or GPU) yields higher performance. We compare \ourtool against DeepTune \cite{cummins2017end}, Inst2Vec \cite{ben2018neural}, and \programl \cite{cummins2020programl}. The results of the baselines are quoted from \cite{cummins2020programl}.

\textbf{Dataset:}
For this task, we use the dataset published in \cite{cummins2017end}. In this dataset, there are 256 OpenCL kernels available, and 680 LLVM IR instances are extracted from them.
There are two types of GPUs: AMD and NVIDIA. For each of the GPUs, the runtimes of the kernels are recorded in the dataset. For AMD, 276 kernels show better performance in GPU, while 395 kernels show better performance in CPU. Whereas for NVIDIA, 385 kernels have better runtimes with GPU, and 286 kernels have better runtimes with CPU. We consider this as a binary CPU or GPU classification problem.


\textbf{Results:} 
As the dataset is small, we use the same 10-fold validation (with 80\% training, 10\% validation, and 10\% testing) like \programl \cite{cummins2020programl} and chose the model with the highest validation accuracy.
The hand-crafted features of \cite{grewe2013portable} are also used as graph-level features in our model to enhance the performance following the approach in \cite{cummins2020programl}.
Table \ref{tab:perf-matr-1} and \ref{tab:perf-matr-2} show the final precision, call, f1-score, and accuracy for AMD and NVIDIA devices. Figure \ref{figs:device_mapping_error} compares \ourtool with state-of-the-art models on the Device Mapping dataset.
We can see that \ourtool sets new state-of-the-art results by achieving the lowest error rate among the baselines both for AMD and NVIDIA, indicating the effectiveness of \ourtool.

\vspace{-5mm}

\vspace{-5pt}
\begin{minipage}{.37\textwidth}
\begin{table}[H]
\captionsetup{justification=centering}
  \caption{\ourtool results for AMD devices.}
  \small
  \setlength\tabcolsep{3.5pt}
  \label{tab:perf-matr-1}
  \centering
  \begin{tabular}{cccccc}
    \toprule
     & Precision & Recall & F1-score & Accuracy \\
    \midrule
    CPU  & 0.94 & 0.94 & 0.94 & \multirow{2}{*}{0.94}\\
    GPU  & 0.94 & 0.94 & 0.94 & \\
    \bottomrule
  \end{tabular}
\end{table}
\end{minipage}
\hspace{20mm}
\begin{minipage}{.37\textwidth}
\begin{table}[H]
\captionsetup{justification=centering}
  \caption{\ourtool results for NVIDIA devices.}
  \small
  \setlength\tabcolsep{3.5pt}
  \label{tab:perf-matr-2}
  \centering
  \begin{tabular}{cccccc}
    \toprule
     & Precision & Recall & F1-score & Accuracy \\
    \midrule
    CPU  & 0.87 & 0.90 & 0.89 & \multirow{2}{*}{0.90}\\
    GPU  & 0.92 & 0.89 & 0.90 & \\
    \bottomrule
  \end{tabular}
\end{table}
\end{minipage}

\vspace{-5pt}
\subsection{Parallelism Discovery}

\textbf{Problem Definition:}
In this problem, given a sequential loop, we try to predict whether a loop can be executed in parallel. We treat this problem as a binary classification problem with two classes: Parallel and Non-Parallel.

\textbf{Dataset:}
The OMP\_Serial dataset \cite{chen2023learning} is used for this task. It contains around 6k compilable source \texttt{C} files with Parallel and Non-Parallel loops.
The training dataset contains around 30k IR files. The OMP\_Serial dataset has three test subsets to compare the performance with three traditional parallelism assistant tools: Pluto (4032 IR files), AutoPar (3356 IR files), and DiscoPoP (1226 IR files). 

\textbf{Results:}
We evaluate \ourtool on all three subsets and compare it with traditional rule-based tools: Pluto \cite{pluto}, AutoPar \cite{quinlan2011rose}, DiscoPoP \cite{li2015discopop}, and also Deep Learning based tools: Graph2Par \cite{chen2023learning}, \programl. Table \ref{tab:par-1} shows the results. The results of Pluto and Graph2par are reported from \cite{chen2023learning}. As \programl does not have this downstream task in their paper, we used the \programl representation in our pipeline to generate the results. Results show that traditional rule-based tools have the highest precision but the lowest accuracy because those tools are overly conservative while predicting parallel loops. So, they miss out on a lot of parallelism opportunities. \ourtool achieves considerably good precision scores across all the test subsets. In terms of accuracy, \ourtool surpasses the current state-of-the-art approaches by 2\% in the Pluto and AutoPar subset. In the DiscoPoP subset, it achieves an impressive 99\% accuracy and surpasses \programl by 9\%.
\vspace{-50pt}
\begin{table}[H]

\captionsetup{justification=centering}
\caption{Performance comparison of \ourtool on the OMP\_Serial dataset.}
\small
\setlength\tabcolsep{3.5pt}
\centering
\begin{tabular}{cccccc}
\hline
Subset           & Approach & Precision & Recall & F1-score & Accuracy \\ \hline
\multirow{4}{*}{Pluto}    & Pluto             & 1                  & 0.39            & 0.56              & 0.39              \\ 
                          & Graph2par         & 0.88               & 0.93            & 0.91              & 0.86              \\  
                          & \programl          & 0.88               & 0.88            & 0.87              & 0.89              \\  
                          & \textbf{\ourtool}  & 0.91               & 0.90            & 0.89              & \textbf{0.91}     \\ \hline
\multirow{4}{*}{autoPar}  & AutoPar           & 1                  & 0.14            & 0.25              & 0.38              \\  
                          & Graph2par         & 0.90               & 0.79            & 0.84              & 0.80              \\  
                          & \programl          & 0.92               & 0.69            & 0.67              & 0.84              \\  
                          & \textbf{\ourtool}  & 0.85               & 0.91            & 0.85              & \textbf{0.86}     \\ \hline
\multirow{4}{*}{DiscoPoP} & DiscoPoP          & 1                  & 0.54           & 0.70             & 0.63             \\  
                          & Graph2par         & 0.90               & 0.79            & 0.84              & 0.81              \\  
                          & \programl          & 0.92               & 0.94            & 0.92              & 0.91              \\  
                          & \textbf{\ourtool}  & 0.99               & 1               & 0.99              & \textbf{0.99}     \\ \hline
\end{tabular}
\label{tab:par-1}
\vspace{-50pt}
\end{table}


\subsection{Parallel Pattern Detection}
\vspace{-5pt}
\textbf{Problem Definition:}
Parallel loops often follow some specific patterns. Identifying parallel patterns is important because it helps developers understand how to parallelize a specific program since each parallel pattern needs to be treated differently. As a result, we apply \ourtool to identify potential parallel patterns in sequentially written programs.
Only the three most common parallel patterns are considered: Do-all (Private), Reduction, and Stencil \cite{reinders2021common}. Given a loop, the task is to predict the pattern.

\textbf{Dataset:}
For this experiment, we also use the OMP\_Serial dataset \cite{chen2023learning}. This dataset contains source codes of different parallel patterns. These programs are collected from well-known benchmarks like NAS Parallel Benchmark \cite{jin1999openmp}, PolyBench \cite{pouchet2017polybench},
BOTS benchmark \cite{duran2009barcelona}, and the Starbench benchmark \cite{andersch2013benchmark}. Then, template programming packages like Jinja \cite{ronacher2008jinja2} are used to create synthetic programs from the templates collected from the mentioned benchmarks. The dataset contains 200 Do-all (Private), 200 Reduction, and 300 Stencil loops. 

\textbf{Results:}
We used 80\% of the dataset for training and 20\% for testing. Table \ref{tab:par-pattern-1} represents our findings. The results of Pragformer~\cite{kadosh2023pragformer} and Graph2par~\cite{chen2023learning} are reported from \cite{chen2023learning}. We compare with these two approaches as they are specifically developed for solving this problem. For generating the results with \programl, we used the \programl representation in our pipeline. We can see \ourtool achieves an impressive 99\% accuracy on the OMP\_Serial Parallel Pattern dataset. It surpasses the state-of-the-art \programl model by 3\%. This indicates the strength of \ourtool to capture the syntactic and structural patterns embedded into source programs. From Table \ref{tab:par-pattern-1}, we can also see \ourtool has high precision for all three patterns and achieves a high precision score for Do-all and Stencil patterns while maintaining very good accuracy.

\begin{table}[H]
\vspace{-17pt}
\captionsetup{justification=centering}
\caption{Performance comparison for the parallel pattern detection task with \ourtool on the OMP\_Serial Dataset.}
\small
\setlength\tabcolsep{3.5pt}
\centering
\begin{tabular}{cccccc}
\hline
Approach             & Pattern & Precision & Recall & F1-score & Accuracy              \\ \hline
\multirow{3}{*}{Pragformer}   & Do-all           & 0.86               & 0.85            & 0.86              & \multirow{3}{*}{0.86}          \\ 
                              & Reduction        & 0.89               & 0.87            & 0.87              &                                \\ 
                              & Stencil          & N/A                & N/A             & N/A               &                                \\ \hline
\multirow{3}{*}{Graph2Par}    & Do-all           & 0.88               & 0.87            & 0.87              & \multirow{3}{*}{0.9}           \\ 
                              & Reduction        & 0.9                & 0.89            & 0.91              &                                \\ 
                              & Stencil          & N/A                & N/A             & N/A               &                                \\ \hline
\multirow{3}{*}{\programl}     & Do-all           & 0.92               & 0.90            & 0.91              & \multirow{3}{*}{0.96}          \\ 
                              & Reduction        & 0.92               & 0.92            & 0.92              &                                \\ 
                              & Stencil          & 0.98               & 1               & 0.99              &                                \\ \hline
\multirow{3}{*}{\textbf{\ourtool}} & Do-all           & 1                  & 0.97            & 0.99              & \multirow{3}{*}{\textbf{0.99}} \\ 
                              & Reduction        & 0.97               & 1               & 0.99              &                                \\ 
                              & Stencil          & 1                  & 1               & 1                 &                                \\ \hline
\end{tabular}
\label{tab:par-pattern-1}
\vspace{-10pt}
\end{table}

\subsection{NUMA and Prefetchers Configuration Prediction}
\vspace{-4pt}
\textbf{Problem Definition:} 
An appropriate configuration of Non-Uniform Memory Access (NUMA) and hardware prefetchers significantly impacts program performance. In this experiment, we define the task of NUMA and prefetcher selection as predicting the right configuration within a given tuning parameter search space. We evaluate the performance of both \programl and \ourtool for this task by converting each program in the dataset to \programl and \ourtool graphs following the approach in \cite{tehranijamsaz2022learning}. 

\begin{figure}[H]
\vspace{-10pt}
      \setkeys{Gin}{width=\linewidth}
     \begin{subfigure}[t]{0.5\textwidth}
         \includegraphics{images/results/sandy_bridge.pdf}
         \caption{Error distribution for Sandy Bridge architecture.}
         \label{fig:res_NUMA_sandy}
     \end{subfigure}
     \hfill
     \begin{subfigure}[t]{0.5\textwidth}
         \centering
         \includegraphics{images/results/skylake.pdf}
         \caption{Error distribution for Skylake architecture.}
         \label{fig:res_NUMA_skylake}
     \end{subfigure}
     \caption{Breakdown of the NUMA and prefetchers configuration prediction per fold [lower is better].}
     \label{fig:res_NUMA}
     \vspace{-10pt}
    \end{figure}

\vspace{-5pt}
\textbf{Dataset:} We use the dataset in \cite{tehranijamsaz2022learning}, which includes
a diverse set of intermediate representation files coupled with the optimal configuration~\cite{sanchez2020modeling}. The dataset incorporates various LLVM compiler optimization flags to produce different forms of the same program. There are 57 unique kernels (IR files) in this dataset, and around 1000 optimization flags are applied, resulting in 57000 IR files in total. Each IR file within the dataset is accompanied by its runtime on two architectures, Sandy Bridge and Skylake, across thirteen different NUMA and prefetcher configurations.
    
\textbf{Results:} Following the approach in the study of TehraniJamsaz \textit{et al.}, we partition the dataset into ten folds for cross-validation. Figure \ref{fig:res_NUMA_sandy} and \ref{fig:res_NUMA_skylake} illustrate the performance results in terms of error rates. On average, \ourtool outperforms \programl by achieving 3.5\% and 1.8\% better error rates on average for the Sandy Bridge and Skylake architecture, respectively. These improvements demonstrate the effectiveness of \ourtool compared to the state-of-the-art \programl.
\vspace{-8pt}

\subsection{Thread Coarsening Factor (TCF) Prediction}
\textbf{Problem Definition:} Thread coarsening is an optimization technique for parallel programs by fusing the operation of two or more threads together. The number of threads that can be fused together is known as the Thread Coarsening Factor (TCF). For a given program, the task is to predict the coarsening factor value (1, 2, 4, 8, 16, 32) that leads to the best runtime. The running time with coarsening factor 1 is used as the baseline for calculating speedups.
For this task, we compare \ourtool against DeepTune \cite{cummins2017end}, Inst2Vec \cite{ben2018neural} and \programl \cite{cummins2020programl}. The results of the baselines are quoted from \cite{ben2018neural}. Since \programl has not been evaluated on this task in the past, we apply \programl representation in our setup for comparison.
\begin{wrapfigure}{r}{0.5\textwidth}
\vspace{-20pt}
  \begin{center}
\includegraphics[width=0.45\textwidth]{images/results/thread-coarsening-kernel-count.pdf}
\vspace{-12pt}
\captionsetup{justification=centering}
\caption{Correct TCF found by \programl vs \ourtool [higher is better].}
\label{figs:thread-coarsening-kernel-count}
  \end{center}
\vspace{-10pt}
\end{wrapfigure}
\textbf{Dataset:} We use the dataset of Ben-Nun et al. \cite{cummins2017end}. The dataset contains only 17 OpenCL kernels. For each kernel, the dataset has the runtime information on four different GPUs for the different thread coarsening factor values. Hence, for each kernel, we have the runtime corresponding to each thread coarsening factor value on a specific GPU device.

\textbf{Results:}
we design the problem as a multi-class classification problem where, given a kernel, we try to predict which thread coarsening factor provides the highest performance. As the dataset is very small, we apply a 17-fold cross-validation approach. In each fold, we train our model on 16 data points, and the model is tested on the one unseen data point that is left out of the training set.
Figure \ref{figs:thread-coarsening-kernel-count} shows the comparison of kernels with the correct Thread Coarsening Factor (TCF) found by \programl and \ourtool. Across the four platforms in total \ourtool is able to correctly predict the TCF for 17 cases, whereas \programl is able to find only 9 cases. In two of the platforms (AMD Radeon HD 5900 and NVIDIA GTX 480) where \programl failed to find any kernel with the correct TCF, \ourtool can find three kernels in both of the platforms with the correct TCF value. As shown in \ref{tab:coarsening}, even though \ourtool outperforms \programl on most computing platforms, it falls behind inst2vec.
We posit the reason is that inst2vec has a pretraining phase where it is trained using skip-gram. 
On the other hand, 17 kernels are very small. Therefore, a DL-based model is not able to generalize enough. However, we can see that even on a smaller dataset, \ourtool achieved comparable speedups with respect to the current state-of-the-art models. 

\begin{table}[H]
  \caption{Speedups achieved by coarsening threads}
  \small
  \setlength\tabcolsep{0.7pt}
  \label{tab:coarsening}
  \centering
  \begin{tabular}{cccccc}
    \toprule
    Computing Platform & DeepTune & inst2vec & \programl & \textbf{\ourtool} \\
    \midrule
    AMD Radeon HD 5900 & 1.1   & \textbf{1.37}  &  1.15  &  1.19  \\
    AMD Tahiti 7970    & 1.05  & 1.1   &  1.00  &  \textbf{1.14}  \\
    NVIDIA GTX 480     & \textbf{1.1}   & 1.07  &  0.98  &  1.03  \\
    NVIDIA Tesla K20c  & 0.99  & \textbf{1.06}  &  1.03  &  1.01  \\
    \bottomrule
  \end{tabular}
\end{table}

\vspace{-7pt}

\subsection{Algorithm Classification}
\vspace{-4pt}

\textbf{Problem Definition:} Previous downstream tasks showed that in most of the cases, \ourtool outperforms the baselines. Those tasks were mostly performance-oriented. We go further by applying \ourtool on a different downstream task, which is algorithm classification. The task involves classifying a source code into 1 of 104 classes. In this task, we compare the results of \ourtool to those of inst2vec, \programl. The results of the baselines are quoted from \cite{cummins2020programl}.

\textbf{Dataset:} We use the POJ-104 dataset \cite{mou2016convolutional} in a similar setup as \cite{cummins2020programl} that contains around 240k IR files for training and 10k files for testing. 

\textbf{Results:} For this task, inst2vec has error rate of 5.17, whereas \programl has error rate of 3.38. \ourtool yields an error rate of 5.00, which is better than inst2vec and slightly behind \programl. One of the reasons is that \programl already has a very small error rate in this task, leaving a very small gap for improvement; however still \ourtool's result is very close to that of \programl.
We could not reproduce the results in \programl paper in our setup. When we applied \programl in our setup, the error rate of \programl was 6.00.
Moreover, we posit that for algorithm classification, numbers are not a significant factor. Therefore, numerical awareness can confuse the models a little bit.
However, this experiment shows that \ourtool is very close to \programl's performance in this task and shows the applicability of \ourtool to a wider range of downstream tasks.
\vspace{-10pt}

\input{sections/ablation}