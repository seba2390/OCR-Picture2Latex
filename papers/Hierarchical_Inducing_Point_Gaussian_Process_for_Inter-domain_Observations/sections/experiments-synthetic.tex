% two-dimensional experiment
\subsubsection{Synthetic Two-Dimensional}
\label{sec:experiment-synthetic}
 We synthesize two-dimensional observations function of varying complexity
 from random neural networks with sinusoidal non-linearities,
see the top row of Figure~\ref{fig:synthetic-experiment}.
Below we describe predictive performance as we vary three algorithmic choices:
the number of inducing points $M$, the complexity of the posterior approximate
covariance $\bS$, and the maximum number of PCG iterations allowed for each
solve.

\paragraph{Scaling inducing points}
We compare held out mean absolute eror and
the training time as a function the of number of
inducing points $M$ and the variational approximation complexity.
In this simulation, we fit a model to $N=20{,}000$ observations with
observation noise $\sigma_n = .1$.
%the stochastic gradient descent learning rate to $2^{-2}$ for mean field variational family and $1^{-2}$ for block diagonal variational family, and batch size
%to $100$ and number of epochs to $10$.
For the block-diagonal approximation, we fix the block size $M_b = 100~(10\times10)$.
The results are displayed in Figure~\ref{fig:mae-by-M} and Figure~\ref{fig:time-by-M}.
From this experiment we conclude:
(i) increasing $M$ increases prediction quality;
(ii) the block-diagonal variational covariance outperforms the more
constrained mean field parameterization, and is comparable to full-rank approximation
when the full-rank is feasible; (iii) the HIP-GP is almost indistinguishable to SVGP in predictive performance,
but is faster in large $M$ settings.

%%%%%%%%%%%%%%%%%%%%%%%%%
% Test Functions
%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[t!]
  \centering
  \begin{subfigure}{.32\columnwidth}
    \includegraphics[width=\columnwidth]{figures/synthetic/true-fgrid-simple.pdf}
    \caption{simple $\rho(x)$}
		\label{fig:ftrue-simple}
  \end{subfigure}
  \begin{subfigure}{.32\columnwidth}
    \includegraphics[width=\columnwidth]{figures/synthetic/true-fgrid-medium.pdf}
    \caption{medium $\rho(x)$}
  \end{subfigure}
  \begin{subfigure}{.32\columnwidth}
    \includegraphics[width=\columnwidth]{figures/synthetic/true-fgrid-hard.pdf}
    \caption{complex $\rho(x)$}
		\label{fig:ftrue-complex}
  \end{subfigure}
	%\%vspace{.5em}
  \begin{subfigure}{\columnwidth}
		\centering
    \includegraphics[width=.8\columnwidth]{{figures/synthetic/synthetic-mae.pdf}}
    \caption{Predictive error (complex $\rho(x)$).}
    \label{fig:mae-by-M}
  \end{subfigure}
	%\vspace{.5em}
  \begin{subfigure}{\columnwidth}
    \centering
		\includegraphics[width=.8\columnwidth]{{figures/synthetic/synthetic-time.pdf}}
    \caption{Training wallclock time for 10 epochs(complex $\rho(x)$).}
    \label{fig:time-by-M}
  \end{subfigure}
	\caption{HIP-GP performance on synthetic data. Top: two-dimensional synthetic
		test functions.  Middle: predictive mean absolute error (MAE) as a function
		of $M$ and $\bS$ type --- mean field (MF), block diagonal (BD) or full
		rank (which did not scale past $M=3{,}000$).
    %Note that the full-rank lines and the block lines overlap for $M=1{,600}$ to $3{,}600$.
		Bottom: Training wall clock time for 10 epochs.
    HIP-GP takes less time to train for $M \geq10{,}000$.
  }
  \label{fig:synthetic-experiment}
\end{figure}


\paragraph{PCG Iteration Early Stopping}
Additionally, we examine the effect of the maximum number of PCG iterations
when computing $\bk_n = \bK_{\bu,\bu}^{-1}\bK^{}_{\bu,n}$ on approximation
quality.  Figure~\ref{fig:pcg-iter} depicts test
log-likelihood as a function of PCG iteration for a modest-sized $M = 1{,}600$
on the simple test function (in Figure~\ref{fig:ftrue-simple}) for $N=20{,}000$
data.  The full rank solution is computed by a direct solve, however the mean
field and block-diagonal use PCG.  The final approximation
quality is robust to the number of PCG iterations used.  We observe similar
behavior on the medium and complex functions for larger values of $M$.  The
upshot is that the HIP-GP needs only a small number of PCG iterations
to be effective.This early stopping advantage is consistent with conventional
wisdom in the optimization literature.


\begin{figure}[t!]
  \centering
  \includegraphics[width=.7\columnwidth]{figures/synthetic/maxiter-plots/maxitercg-loglike-func=simple.pdf}
  \caption{Stochastic optimization is robust to early stopping PCG iterations.
     %Because of this, we can stop after only 5 to 10 iterations, saving on
     %computation time per iteration.
  }
  \label{fig:pcg-iter}
  %\vspace{-1em}
\end{figure}
