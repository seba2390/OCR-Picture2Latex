\section{INTRODUCTION}
\label{sec:intro}
Gaussian processes (GPs) are a widely used statistical tool for inferring
unobserved functions \citep{cressie1992statistics, cressie1990origins,
rasmussen2006gaussian}. The classic goal of GPs is to infer the
unknown function given noisy observations. Here, we are interested in a more
general setting, inter-domain GPs,
where the observed data is related to the latent function via some linear transformation,
such as integration or differentiation,
while an identity transformation recovers the standard GP problem.
One motivating example is an
astrophysics problem: mapping the three-dimensional spatial distribution of dust in the Milky way \citep{green2015three,
leike2019charting, kh2017inferring}.
Interstellar dust is a latent function that can be inferred from star observations. However, because we are embedded in our own dust field,
we can only observe some noisy \textit{integral} of the dust function along the line of sight between Earth and a star.
Beyond this example, inter-domain GPs arise often in the literature: integrated observations have been used in probabilistic construction of optimization algorithms \citep{wills2017construction}, quadrature rules \citep{minka2000deriving}, and tomographic reconstructions \citep{jidling2018probabilistic};
while derivative observations have been used in dynamical systems \citep{solak2003derivative}, %chemical physics \citep{koistinen2016minimum},
modeling monotonic functions \citep{riihimaki2010gaussian} and Bayesian optimization \citep{garnett2010bayesian, siivola2018correcting}.

In practice, this type of inter-domain GP problem poses two interwined obstacles that are beyond the reach of current techniques.
First, large-scale exact modeling is usually intractable.
The joint distribution of inter-domain observations and the underlying GP involves 
the transformed-domain and inter-domain kernel expressions,
which rarely admits analytical solutions
and requires approximations \citep{lazaro2009inter, hendriks2018evaluating}.
Common approximations are often handled
by numerical integration, which is infeasible for big datasets since it requires
integrating all pairwise correlations.

% scalability
Moreover, inter-domain GPs suffer from the
same scalability issues as regular GPs.
%How to scale GP inference in the inter-domain context remains another difficulty.
For a dataset with $N$ observations,
the likelihood function depends on $N^2$ pairwise correlations.
The leading strategy to scale standard GP inference is to use $M \ll N$ \emph{inducing points}
to represent the global behavior of GP functions \citep{rasmussen2006gaussian}.
%Classical inducing point methods have $O(NM)$ memory requirement, while
%more modern approaches lower this requirement by various techniques.
One popular inducing point method is stochastic variational Gaussian process (SVGP),
which factorizes the objective over mini-batches of data
and requires only $O(M^2)$ storage and $O(M^3)$ computation \citep{hensman2013gaussian}.
%We will show that SVGP offers a possibility to deal with inter-domain observations.
In the current practice of SVGP, $M$ is limited to under $10{,}000$ \citep{wilson2015kernel, izmailov2018scalable}.
However, many inter-domain problems are spatial or temporal in nature,
and the data do not lie in some small manifold in that space.
In the interstellar dust problem, for example, we aim to make inference at
every point in a dense 3D space.
A small set of inducing points
is incapable of resolving the resolution of interest,
which is around 4 orders of magnitude smaller than the domain size.
%See figure....
Furthermore, \citet{bauer2016understanding} shows that more inducing points are needed
to reduce the overestimated observation noise parameter induced by SVGP.
All of these facts necessitate the need to scale both $N$ and $M$ to larger quantities.

To this end, we develop the hierarchical inducing point GP (HIP-GP),
a method to scale GP inference
to millions of inducing points and observations for spatial-temporal inter-domain problems.
In particular,
%\vspace{-1em}
\begin{itemize}
  \item We adapt the SVGP framework to inter-domain settings
  by decoupling observations and inducing points into different domains.
  This framework alleviates the difficulties of computing the full transformed kernel matrices,
  and enables the exploitations of the latent kernel structure.
  \item We then develop the HIP-GP algorithm to address the computational bottlenecks
   of standard SVGP objectives, employing two core strategies:
   \begin{itemize}
     \item  Fast matrix inversion with conjugate gradient method
     using the \emph{hierachical Toeplitz structure}.
      Upon this structure, we design a novel \emph{preconditioner}
     and a new \emph{whitening strategy} to further speed up computations;
    \item A \emph{structured variational approximation} of
    the posterior over inducing point values.
  \end{itemize}
\end{itemize}
%\vspace{-1em}
HIP-GP is suitable for low-dimensional inter-domain GP problems,  and applies in
settings where the kernel function is stationary and inducing points fall on a fixed, evenly-spaced grid.
In addition, the technical innovations in developing HIP-GP
are useful in a variety of more general settings.

% To fill a gap.

% to cite
%\todo[inline]{
%cite these remaining ones
%\citep{wilson2015thoughts}
%\citep{wilson2015kernel}
%\citep{filippone2014pseudo}
%\citep{oates2017control}
%\citep{datta2016hierarchical}
%}
