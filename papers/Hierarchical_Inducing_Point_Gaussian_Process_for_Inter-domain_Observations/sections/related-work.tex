\section{RELATED WORK}
\paragraph{Inter-domain GPs} The idea of the inter-domain Gaussian processes has been discussed
in \citep{lazaro2009inter, van2020framework}.
However, their primary interests are 
using inter-domain transformations to define inducing variables for specifying GP approximations, whereas our work explores the usage of SVGP framework to perform scalable modeling and inference
with inter-domain observations.
%HIP-GP addresses the latter problem under the SVGP framework.


% kernel interpolation methods
\paragraph{Scalable Inducing Point Methods}
% variational GP methods
We note several recent approaches to scaling
the number of inducing points in GP approximations.
\citet{shi2020sparse} takes an orthogonal strategy to ours by approximating GP with inducing points in two independent directions, whereas HIP-GP requires inducing points to densely cover the input space. However, while improved over standard SVGP, their method still remains a cubic complexity.
\citet{izmailov2018scalable} introduces
the tensor train decomposition into the variational approximation.
%They use a factorized approach that does not have to explicitly represent all
%$M$ or ($M \times M$) variational parameters.
Alternatively, \citet{evans2018scalable} directly approximate the kernel with a
finite number of eigenfunctions evaluated on a dense grid of inducing points.
Both methods rely on \emph{separable} covariance kernels
to utilize the Kronecker product structure. This limits the
class of usable kernels.
The Mat√©rn kernel, for example, is not separable across dimensions.
To fill that gap, we instead focus on the class of \emph{stationary kernels}.  

Another line of inducing point work is based on \emph{sparse kernel interpolations}.
KISS-GP uses a local kernel interpolation of inducing points
to reduce both the space and time complexity to $O(N+M^2)$ \citep{wilson2015kernel}.
SV-DKL also uses local kernel interpolation, and
exploits separable covariance structures and deep learning techniques
to address the problem of multi-output classification \citep{wilson2016stochastic}.
But these kernel interpolation methods are not applicable to
inter-domain observations under transformations.
More specifically, for standard (non-inter-domain) problems,  kernel interpolation methods approximate the $N \times N$ covariance matrix $\mathbf{K}_{\mathbf{N,N}}$ with $\mathbf{W} \mathbf{K}_{\mathbf{u, u}}\mathbf{W}^T$, where $\mathbf{W}$ is a sparse interpolation weight matrix. 
 However, for problems with integral observations, we must compute the integrated kernel $\mathbf{K}^{**}_{\mathbf{N,N}} = [\int \int Cov(\rho(\mathbf{x}_i), \rho(\mathbf{x}_j)) d\mathbf{x}_i d \mathbf{x}_j]_{i,j=1}^N$.
  Approximating this integral with local interpolation is not straightforward, and computing every integrated cross-covariance term is costly.
  Alternatively, HIP-GP decouples observations and inducing points into different domains
  through the \textit{inter-domain prior} (Equation~\ref{eq:inter-domain-GP}).
  This decoupled prior enables mini-batch processing of $k_{n,n}^{**}$, eliminates the need to compute cross-covariance terms  $k_{{n_i}, {n_j}}^{**}$,
  while still maintaining structure exploitation of $\mathbf{K}_{\mathbf{u,u}}$. 
  
 \paragraph{Fast Whitening Strategy} As is mentioned before, the classical whitening strategy is the Cholesky decomposition that has $O(M^2)$ space and $O(M^3)$ time complexity. 
 \citet{pleiss2020fast} provides a more general purpose method for fast matrix roots and is in particular applicable to whitening GP. Their method is an MVM-based  approach that leverages the contour integral quadrature and  requires $O(M \log M+QM)$ time for $Q$ quadrature points. Our whitening strategy specifically targets gridded inducing points and achieves more complexity savings ($O(M\log M)$ time). 
  
%The integral transformation, for example,
%operates on the whole latent function,
%making a direct local interpolation impossible.
%We note that the SV-DKL also considers a mean-field variational approximation.
%Here we extend it to the more general block-independent approximation.
 
