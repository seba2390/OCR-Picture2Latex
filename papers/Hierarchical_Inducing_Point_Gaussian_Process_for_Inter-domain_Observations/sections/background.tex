\section{BACKGROUND}
\label{sec:background}

\subsection{Inter-domain GPs}

Following the notations in \citet{van2020framework},
we consider a statistical model of the form
\begin{align}
  \rho &\sim GP\left( 0, k_{\theta}(\cdot, \cdot) \right) \\
  \rho^* &= \mathcal{L} \circ \rho \\
  y_n \given \bx_n, \rho^* &\sim \mathcal{N}(\rho^*(\bx_n), \sigma_n^2)
\end{align}
for a dataset of $N$ observations $\mathcal{D} \triangleq \{ y_n, \bx_n,
\sigma_n^2 \}_{n=1}^N$, where $\mathcal{L}$ is a linear operator
and $k_{\theta}(\cdot, \cdot)$ is the covariance function that encodes prior assumptions
about the function $\rho$. %, such as stationarity and smoothness
Note that GPs are closed under linear operators,
therefore $\rho^*$ is also a GP \citep{rasmussen2006gaussian}.

One common linear operator is the integral operator,
$\mathcal{L} \circ \rho (\cdot)  = \int \rho( \bx) w (\bx) d \bx$,
as used in \citet{lazaro2009inter}.
We see that this $\mathcal{L}$ maps the entire function
$\rho (\cdot)$ to a single real value.
Another example is the derivative of the $d$th input dimension
$\mathcal{L} \circ \rho (\cdot) = \frac{ \partial \rho }{\partial x_d } (\bx_n)$.
In this case, the operator only depends on the neighborhood around $\bx_n$.
Derivative observations are often useful for algorithmic purposes, e.g. in \citet{riihimaki2010gaussian}.
In application problems, they
could be either collected, e.g.  velocity measured by physical detectors,
or identified from function observations \citep{solak2003derivative}.
We also notice that setting $\mathcal{L}$ to an identity map fits
 regular GPs into this framework.

The goal of inter-domain GPs is to infer the underlying
function $\rho(\bx)$ --- either to compute $p(\rho(\bx_*) \given \mathcal{D})$
for new test locations $\bx_*$ or to improve estimates of $p(\rho(\bx_n) \given
\mathcal{D})$ for observed location $\bx_n$ given \emph{all} observations.

\subsection{Stochastic Variational Gaussian Process}
\label{sec:SVGP}
The stochastic variational Gaussian process (SVGP) is an approximate method that
scales GP inference to large $N$ \citep{hensman2013gaussian}.
%SVGP uses $M \ll N$ inducing points placed in
%the input space to \emph{decouple} the $N$ observations
%\citep{quinonero2005unifying}.
%Then, importantly, it introduces a
%variational posterior which generates an objective that can be
%estimated with mini-batches of the large dataset.
Denote the $M$ inducing point \emph{locations} $\bar{\bx} = \left( \bar{\bx}_1, \dots,
\bar{\bx}_M \right)$, and the vector of inducing point \emph{values}
$\bu \triangleq \left( \rho(\bar{\bx}_1), \dots, \rho(\bar{\bx}_M) \right)$.
SVGP defines a variational distribution over the inducing point 
values $\bu$ and the latent process values
$\brho \triangleq \left(\rho(\bx_1), \dots, \rho(\bx_N) \right)$ of the form
\begin{align*}
    q(\bu, \brho) = q_{\blambda}(\bu) p(\brho \given \bu) \, , \quad
    q_{\blambda}(\bu) = \mathcal{N}(\bu \given \bm, \bS) \,,
\end{align*}
where $q_\blambda(\bu)$ is a multivariate Gaussian,
$p(\brho \given \bu)$ is determined by the GP prior
%(i.e.~the prior covariance between $\bu$ and $\brho$)
and $\blambda \triangleq (\bm, \bS)$ are variational parameters.
This choice of variational family induces a convenient cancellation, resulting
in a separable objective
\citep{titsias2009variational}
\begin{align}
&\mathcal{L}(\blambda) \label{eq:svgp-elbo} \\
  %&= \mathbb{E}_{q_{\blambda}}\left[
  %       \ln p(\brho, \bu, \by)- q_{\blambda}(\rho, \bu)
  %   \right] \nonumber \\
  &= \underbrace{\mathbb{E}_{q_{\blambda}(\bu)}\left[
      \mathbb{E}_{p(\brho \given \bu)} \left[ \ln p(\by \given \brho) \right]
    \right]}_{\circled{a}} -
    \underbrace{KL(q_{\blambda}(\bu)\,||\,p(\bu))}_{\circled{b}} \nonumber\,.
\end{align}
We can write \circled{a} as a sum over $N$ observations
\begin{align}
    \circled{a}
    %&= \mathbb{E}_{q_{\blambda}(\bu)}\left[
    %     \mathbb{E}_{p(\brho \given \bu)} \left[
    %       \sum_{n=1}^N \ln p(y_n \given \rho_n)
    %     \right]
    %   \right] \\
    &= \sum_{n=1}^N \underbrace{
          \mathbb{E}_{q_{\blambda}(\bu)}\left[
              \mathbb{E}_{p(\rho_n \given \bu)} \left[
                  \ln p(y_n \given \rho_n)
              \right]
          \right]
        }_{\triangleq \circled{$a_n$}} \,.
\end{align}
The factorization of \circled{$a_n$}
enables the objectives to be estimated with mini-batches in a large dataset.
However, notice that \circled{$b'$}, the KL-divergence of two Gaussians,
will involve a term $\ln |\bK_{\bu,\bu}|$
which requires $O(M^3)$ computation.

\iffalse
Observe that \circled{$a_n$} and \circled{b} are both functions of the
$M\times M$ Gram matrix over inducing points, $\bK_{\bu,\bu}$,
where $\left( \bK_{\bu,\bu} \right)_{i,j} \triangleq k(\bar{x}_i, \bar{x}_j)$.
Note this is an alternative derivation of the same objective presented in
\citet{hensman2013gaussian}.
\fi


\subsection{Matrix Solves with Conjugate Gradients}
%The major computations in SVGP requires matrix solves with kernel matrix $\bK_{\bu, \bu}$.
%One could perform a direct solve via Cholesky decomposition which takes $O(M^3)$ time.
%Alternatively, iterative methods frame it as an optimization problem through as series of matrix vector multiplications (MVM).
Conjugate gradients (CG) is an iterative algorithm for solving a linear
system using only matrix-vector multiplies (MVM).
CG computes $\bK^{-1} \bp$ for any $\bp \in \mathbb{R}^M$ by computing
$\bK \bv$ for a sequence of vectors $\bv \in \mathbb{R}^M$ determined by the algorithm.
%by only computing $\bK \bv$ for a sequence of similarly-sized vectors $\bv$
%that is determined by the progress of the algorithm.
For $\bK$ of size $M \times M$, CG computes the exact solution after $M$
iterations,
% CG can be faster than direct solvers if the MVM $\bK \bp$ can be computed efficiently.
and typically converges after some smaller number of steps $S < M$
\citep{hestenes1952methods, nocedal2006numerical}.

Preconditioned conjugate gradients (PCG) is an augmented version of CG that
solves the system in a transformed space.  A good preconditioner can
dramatically speed up convergence \citep{shewchuk1994introduction,
cutajar2016preconditioning}.
%We show that PCG is a natural algorithm to use with hierarchical Toeplitz
%matrices --- a structured matrix that admits fast multiplication and
%efficient representation.
