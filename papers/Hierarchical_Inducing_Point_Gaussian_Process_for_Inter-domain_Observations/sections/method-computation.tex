\subsection{Computational Accelerations}
\label{sec:computational}
We now turn to the first bottleneck --- how to design an efficient whitening strategy
to compute the term $\bk_n$. %for
%low-dimensional dense spatial-(temporal) problems.
To do so,
we rely on judicious placement of inducing points and assume a stationary
covariance function, a general and commonly used class.
We describe three key ingredients below.
%Together they present an
%efficient algorithm for computing $\bk_n$.

%The following section gives background on the method of conjugate gradients
%(CG) and preconditioned conjugate gradients (PCG) to iteratively and
%efficiently solve big linear systems with matrix multiplies.
%Section~\ref{sec:toeplitz-structure} describes efficient matrix multiplies
%we use by exploiting hierarchical Toeplitz structure in $\bK_{\bu,\bu}$.
%Section~\ref{sec:toeplitz-preconditioner} describes a novel preconditioner
%for hierarchical Toeplitz matrices that dramatically speeds up the iterative
%solver compared to standard conjugate gradients.


%Above, we established that we can compute matrix multiplies quickly, $K
%\bv$ for hierarchical Toeplitz matrices.  However, for use within the SVGP
%framework, we need to compute $K^{-1}\bv$.  This can be done efficiently
%with a preconditioned conjugate gradient (PCG) method.  The PCG method is
%an iterative linear system solver that only uses matrix multiplies.  The
%solution is exact after $M$ steps (for a solution of size $M$), but in
%practice it can be made much more efficient by stopping early at a near
%optimal solution (e.g.~after 10 or 20 steps).

\iffalse
A symmetric Toeplitz matrix is a symmetric matrix that has \emph{constant
diagonals}. For a stationary kernel, the covariance matrix evaluated
at evenly-spaced grid points forms a symmetric Toeplitz matrix.
One-dimensional Toeplitz matrices have been used for GP \citep{cunningham2008fast},
and the multi-dimensional extension, which we term as the \emph{hierarchical Toeplitz structure},
is explored in \citet{wilson2015thoughts}.
\fi

\paragraph{Hierarchical Toeplitz Structure}
\label{sec:toeplitz-structure}
Consider a $D$-dimensional grid of evenly spaced points
of size $M \triangleq M_1 \times \cdots \times M_D$, characterized by
one-dimensional grids of size $M_i$ along dimension $i, i=1:D$,
where $D$ is the input dimension.
Under a stationary kernel, we construct a covariance matrix
for this set of points in $x$-major
order (i.e.~\texttt{C}-order).
%The covariance matrix $\bK$ over this ordering is \emph{block Toeplitz
%with Toeplitz blocks} with constant block diagonals.
Such a matrix will have \emph{hierarchical Toeplitz structure},
which means the diagonals of the matrix are constant.
Because of this data redundancy, a hierarchical Toeplitz matrix
is characterized by its first row.
Now we place the inducing points along
a fixed, equally-spaced grid, resulting in a $M \times M$ hierarchical
Toeplitz Gram matrix $\bK^{}_{\bu,\bu}$.
The efficient manipulation of $\bK_{\bu, \bu}$ is through its circulant embedding:
\begin{align}
    \bC =
    \begin{pmatrix}
    \bK_{\bu, \bu} & \tilde{\bK} \\
    \tilde{\bK}^\top & \bK_{\bu, \bu}
    \end{pmatrix}
\end{align}
where $\tilde{\bK}$ is the appropriate reversal of $\bK_{\bu, \bu}$ to make $\bC$ circulant.
$\bC$ admits a convenient diagonalization
\begin{align}
  \bC = \bF^\top \bD \bF
      = \bF^\top \text{diag} \left(\bF \bc \right) \bF \,\,,
\end{align}
where $\bF$ is the fast Fourier transform matrix, $\bD$ is a
diagonal matrix of $\bC$'s eigenvalues, and $\bc$ is the first row of $\bC$.
%that represents the circulant matrix.
This diagonalization enables fast MVMs with $\bC$ and
hence the embedded $\bK_{\bu, \bu}$ via the FFT algorithm
in $O(M \ln M)$ time, further making it sufficient for use
within CG to efficiently solve a linear system.

The fast solves afforded by Toeplitz structure have been previously
utilized for exact GP inference \citep{cunningham2008fast, wilson2015thoughts}.
Here, we extend the applicability of Toeplitz structure
to the variational inter-domain case
by introducing a fast whitening procedure and an effective preconditioner for CG.


\paragraph{Fast Whitening Strategy}
\label{sec:toeplitz-whitening}
%To avoid the cubic cost of Cholesky decomposition,
%we develop a new whitening strategy
%utilizing the hierahical Toeplitz structure.
Similar to the Cholesky decomposition, we aim to
find a whitened matrix $\bR$ that serves as a root of $\bK_{\bu, \bu}$,
i.e. $\bR \bR^T = \bK_{\bu, \bu}$.
Directly solving $\bK_{\bu, \bu}^{1/2}$ is not trivial. Alternatively, we access the root from the circulant embedding of $\bK_{\bu, \bu}$.  We consider the square root of the circulant matrix
\begin{equation}\label{eq:csquared-fft}
  \bC^{1/2} = \bF^\intercal \bD^{1/2} \bF,
\end{equation}
and its block representation
\begin{equation}
  \bC^{1/2} = \begin{pmatrix}
  \bA & \bB \\
  \bB^\top & \bD \\
\end{pmatrix}.
\end{equation}
We make a key observation that the first row-block $(\bA, \bB)$ can be viewed as a
``rectangular root" of $\bK_{\bu, \bu}$. That is, we define a
 \emph{non-square} whitening matrix $\bR$ and the correlation vector $\bk_n$ as follows 
\begin{align}\label{eq:csquared}
\bR &\triangleq \begin{pmatrix}
\bA & \bB
\end{pmatrix} \,, \quad \bk_n \triangleq \bR^T \bK_{\bu, \bu}^{-1} \bk^*_{\bu, n}.
\end{align}
One can verify such $\bR$ and $\bk_n$ satisfy
Equation~\ref{eq:whitened-criteria}, thus offering a valid whitening strategy.
We note that since $\bR$ is non-square and $\bu = \bR \bepsilon$,
this strategy doubles the number of variational parameters
in each dimension of the whitened space.

Now we address how to efficiently compute $\bk_n$  defined in Equation~\ref{eq:csquared}. We first compute the intermediate quantity $\bk_n'= \bK_{\bu, \bu}^{-1} \bk^*_{\bu, n}$ via CG in $O(M \ln M)$ time. We then compute $\bk_n = \bR^\top \bk_n'$. Note that $\bR^{T}$ is embedded in the matrix $\bC^{1/2}$ which also admits the FFT diagonalization (Equation \ref{eq:csquared-fft}). Hence,  MVM with $\bR^\top$ can be also done in $O(M \ln M)$ time.  

Lastly, we show how to make CG's computation of $\bK_{\bu, \bu}^{-1} \bk^*_{\bu, n}$ faster with a well-structured preconditioner. 

\paragraph{Efficient Preconditioner}
\label{sec:toeplitz-preconditioner}
The ideal preconditioner $\bP$ is a matrix that whitens the matrix to be
inverted --- the ideal $\bP$ is $\bK_{\bu, \bu}^{-1}$.  However, we cannot
efficiently compute $\bK_{\bu, \bu}^{-1}$. But due to the convenient
diagonalization of the circulant embedding matrix $\bC$, we can
efficiently compute the inverse of $\bC$: 
\begin{align}
    \bC^{-1} = \bF^\top \bD^{-1} \bF \, .
\end{align}
%Naively, one might think this is a direct way to compute $\bK_{\bu, \bu}^{-1} \bp$,
%obviating PCG in the first place.
Note that the upper left block of
$\bC^{-1}$ \emph{does not} correspond to $\bK_{\bu, \bu}^{-1}$
% so directly computing $\bK^{-1} \bp'$ via FFTs and inverse FFTs is unavailable.
as we explicitly write out
\begin{align}
    \bC^{-1} =
    \begin{pmatrix}
    \left( \bK_{\bu, \bu} - \tilde{\bK} \bK_{\bu, \bu}^{-1} \tilde{\bK}^{\intercal}\right)^{-1} & ... \quad\\
    ... & ... \quad
    \end{pmatrix} \,\,.
\end{align}

However, when the number of inducing points are large enough,
$\bK_{\bu,\bu}$ approaches a banded matrix,
and so $\tilde{\bK}$ is increasingly sparse.
Therefore, the upper left block of $\bC^{-1}$ would be close to $\bK_{\bu, \bu}^{-1}$, suggesting that it can serve as an effective \emph{preconditioner} within PCG,
and therefore an effective strategy for solving a linear system with the 
kernel matrix. % \citep{cutajar2016preconditioning}.
We note that this banded property is often exploited in developing
effective preconditioners \citep{chan1996conjugate, saad2003iterative}.
To justify this intuition, we anlayze the PCG convergence speed under various settings of 
kernel functions and inducing point densities in appendix.
We compare the performance of PCG and CG in systems of varying size
in Section~\ref{sec:experiments-preconditioner}.
%depicted in Figure~\ref{fig:preconditioner} and summarized
We find that PCG
converges faster than CG across all systems, 
taking only a fraction of the number
of iterations that standard CG requires to converge.
This speedup is crucial --- PCG is a subroutine we use to compute the gradient
term corresponding to each observation $n$.

\paragraph{Summary of Fast Computation for $\bk_n$}
To summarize, we exploit additional computational benefits of
the hierarchical Toeplitz matrix through its circulant embedding matrix, which enables fast matrix square-root and matrix inverse. We further utilize these fast operations to design novel whitening and preconditioning strategies. 
Thus, the whitened correlation term $\bk_n = \bR^T \bK_{\bu, \bu}^{-1} \bk^*_{\bu, n}$
can be efficiently processed as follows:
\begin{enumerate}
  \item embed $\bK_{\bu, \bu}$ into a larger circulant matrix $\bC$;
  \item solve $\bK_{\bu, \bu} \bk'_n = \bk^*_{\bu, n}$ for the  intermediate term $\bk'_n$ with PCG,
  where we utilize the FFT diagonalization of $\bC$ and $\bC^{-1}$;
  \item compute $\bk_n = \bR^\top \bk'_n$ , where we utilize the FFT diagonalization of $\bC^{1/2}$.
\end{enumerate}
%Note that the circulant embedding takes linear time, and only the row representation of $\bC$ is stored.
The space and time complexity of this procedure are $O(M)$ and $O(M \ln M)$.
This offers a speed-up over the Cholesky decomposition which has $O(M^2)$ space and $O(M^3)$ time complexity, respectively.
In Section~\ref{sec:experiments-whitened}, we examine this acceleration by comparing the time of computing $\bk_n$ using Cholesky and using HIP-GP, as the system size $M$ varying from $10^3$ to $10^6$. HIP-GP's strategy outperforms Cholesky for small values of $M$, and scales to larger $M$ where Cholesky is no longer feasible. We present HIP-GP's algorithmic details in appendix. 

