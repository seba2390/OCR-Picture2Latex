\section{SCALING $M$: HIP-GP for INTER-DOMAIN PROBLEMS}
\label{sec:hip-gps}
We first formulate the SVGP framework for inter-domain observations, and identify
its computational bottlenecks in Section~\ref{sec:method-interdomain-svgp}.
We then address these bottlenecks by the HIP-GP algorithm using the techniques developed in
Section~\ref{sec:computational} - \ref{sec:variational-families}.
In Section~\ref{sec:method-summary}, we summarize our methods and discuss
optimization procedures for HIP-GP.
\input{sections/method-inter-domain-svgp.tex}

%\input{sections/method-whitened.tex}
\input{sections/method-computation.tex}
\input{sections/method-variational-approx.tex}
\subsection{Method Summary}
\label{sec:method-summary}
%Sections~\ref{sec:whitened}, \ref{sec:computational}, and
%\ref{sec:variational-families} address the three computational bottlenecks
%described in Section~\ref{sec:bottlenecks}.
The modeling difficulty of inter-domain GP problems arises from the numerical intractability
of computing the full transformed-domain covariance
$\bK^{**}_{N,N}$ of size $N \times N$. We avoid this difficulty by decoupling the
 the observations and the inducing points into different domains under the SVGP framework.
%Hence, the costly transformed-domain covariance is only required for $N$ single data points.
Moreover, we leverage the kernel structure of the Gram matrix
$\bK^{}_{\bu,\bu}$ in the latent domain for efficient computations.

The computational
difficulty stems from the computations with the kernel matrix $\bK_{\bu,\bu}$ and the variational covariance $\bS$.  We avoid having to
compute $\ln|\bK^{}_{\bu,\bu}|$ by using a \emph{whitened parameterization};
we develop a \emph{fast whitening strategy} to compute the whitened correlation term
 $\bk_n = \bR^\top \bK_{\bu, \bu}^{-1} \bk^{*}_{\bu,n}$
by exploiting  the \emph{hierarchical Toeplitz structure} with a \emph{novel
preconditioner};
and finally we explore a \emph{structred representation} for $\bS$.

\paragraph{Optimization} We perform natural gradient descent on
variational parameters using closed-form gradient updates.
For gradient-based learning of kernel hyperparameters, automatically differentiating
through the CG procedure is not numerically stable.
Fortunately, we can efficiently compute the analytical gradient of CG solves 
utilizing the hierarchical Toeplitz structure,
without increasing the computational complexity.
See appendix for more details on gradient derivations.
