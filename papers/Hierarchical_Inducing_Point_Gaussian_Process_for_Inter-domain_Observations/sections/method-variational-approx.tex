\subsection{Structured Variational Approximation}
\label{sec:variational-families}
Finally, we turn to the second bottleneck: how to represent
and manipulate variational parameters of mean $\bm$ and covariance $\bS$.
%Inducing point methods were introduced to alleviate the cubic scaling of
%exact Gaussian process inference.  We can view this approximation as
%creating a low-rank covariance over all observations
%\citep{quinonero2005unifying}.  In that same vein, we introduce
%hierarchical structure into the inducing points in order to scale to larger
%number of inducing points.
We propose the \emph{block independent} variational family
%two variational families: \emph{independent} (i.e.~mean field)
\iffalse
\begin{align}
  q(\bu) &= \prod_{m}^M \mathcal{N}(\bu_{m} \given \bm_{m}, \bs^2_m) \,,
\end{align}
\fi
%and \emph{block independent} (i.e.~structured)
\begin{align}
  q(\bu) &= \prod_{b}^B \mathcal{N}(\bu_{b} \given \bm_{b}, \bS_{b}) \,,
  \label{eq:block-cov}
\end{align}
where $\bu_{b}$ denotes a subset of inducing points of size $M_b < M$
and $\bS_b$ is the $M_b \times M_b$ variational covariance for that
subset. Note that when $M_b=1$, it reduces to the $\emph{mean-field}$ variational family,
and when $M_b = M$, it is the \emph{full-rank} variational family.
%In both cases, manipulating the variational covariance $\bS$ is computationally
%feasible.
%For diagonal $\bS$, calculating the inverse and log-determinant are
%both linear in $M$.
Calculations of the inverse and log-determinant of block independent $\bS$ scale $O(B M_b^3)$
--- we must choose $M_b$ to be small enough to be practical.


We note that independence in the posterior is a more reasonable approximation
constraint \emph{in the whitened parameterization} than the original space.
The original GP prior, $p(\bu)$, is designed to have high correlation, and therefore
data are unlikely to decorrelate inducing point values.
In the whitened space, on the other hand, the prior is already uncorrelated.
Hence the whitened posterior
is not spatially correlated as much as the original posterior.
%To see this, the correlation structure in the original posterior:
%$\tilde{\bS} = \bK_{\bu, \bu}^{1/2} \bS \bK_{\bu, \bu}^{1/2}$
%is preserved through the transform of $\bK_{\bu,\bu}^{1/2}$
%even if $\bS$ is mean-field. % as is also observed in \citet{wilson2016stochastic}.
This is in addition to the benefits of optimizing in the whitened space
due to better conditioning.

\paragraph{Constructing Blocks}
The block independent approximation of Equation~\ref{eq:block-cov}
requires assigning inducing points to $B$
blocks.  Intuitively, blocks should include nearby points, and so we focus on
blocks of points that tile the space.
% In two-dimensions we
%divide each one-dimensional grid into $\tilde{B}$ segments, resulting in a
%grid of $B = \tilde{B}^2$ blocks.
%We note that the \emph{Toeplitz ordering} and \emph{block ordering} may not be
%the same.  This complicates implementation, as we represent the block diagonal
%matrix $\bS$ as a $B \times M_b$ (blocks by block-size) array.
To reconcile the Toeplitz ordering and the block orderings (they may not be the same),
we simply have to permute any $M$-length vector
(e.g.~$\bk^*_{\bu,n}$ or $\bm$) before multiplication with $\bS$ and then undo
the permutation after multiplication.  Fortunately, this permutation is linear
in $M$.
