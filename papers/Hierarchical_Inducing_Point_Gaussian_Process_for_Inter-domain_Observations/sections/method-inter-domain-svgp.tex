\subsection{Inter-domain SVGP Formulation}
\label{sec:method-interdomain-svgp}
We show that the inter-domain observations can be easily incorporated into the SVGP framework.
We place a set of inducing points $\bu = \rho(\bar{\bx})$ in the latent domain
at input locations $\bar{\bx} = \left( \bar{\bx}_1, \cdots, \bar{\bx}_M \right)$.
Connections to the observations are made through the
inter-domain covariance,
while the observations are characterized by
the transformed-domain covariance.
Formally, we have the inter-domain GP prior:
\begin{align}
  \label{eq:inter-domain-GP}
\begin{pmatrix}
\rho_n^* \\
\bu
\end{pmatrix}
\sim \mathcal{N} \left( 0,
\begin{pmatrix}
 k^{**}_{n,n} & \bk^*_{n, \bu} \\
 \bk^*_{\bu, n} & \bK_{\bu, \bu} \\
\end{pmatrix}
\right),
\end{align}
where the inter-domain covariance and the transformed-domain covariance are defined as
\begin{align}
\label{eq:domain-kernel}
\bk^*_{\bu, n} &\triangleq Cov \left(\rho(\bar{\bx}), \rho^*(\bx_n)\right)
 = Cov \left(\bu, \rho^*_n \right)  \, , \\
\label{eq:domain-kernel2}
k^{**}_{n,n} &\triangleq Cov \left(\rho^*(\bx_n), \rho^*(\bx_n) \right)
 = Cov \left( \rho^*_n, \rho^*_n  \right) \, , 
\end{align}
and the latent domain covariance is
\begin{align}
  \bK_{\bu, \bu} &\triangleq Cov \left( \rho (\bar{\bx}), \rho (\bar{\bx}) \right)
    = Cov \left( \bu, \bu \right).
\end{align}
This form of the prior suggests formulating the inter-domain SVGP objective
as follows
\begin{align}
\mathcal{L}(\blambda) \label{eq:inter-domain-svgp} &= \sum_{n=1}^N \mathbb{E}_{q_{\blambda}(\bu)}\left[
      \mathbb{E}_{p(\rho_n^* \given \bu)} \left[ \ln p(y_n \given \rho_n^*) \right] \right] \\
& \qquad  - KL(q_{\blambda}(\bu)\,||\,p(\bu)) \nonumber\,,
\end{align}
where
\begin{align*}
  p(\rho_n^* | \bu) &= N(\rho_n^* | \bk_{n,\bu}^* \bK_{\bu, \bu}^{-1} \bu,
  k_{n,n}^{**} - \bk_{n,\bu}^* \bK_{\bu, \bu}^{-1} \bk_{\bu, n}^*)\,.
\end{align*}
Note that this framework can be extended to observations in multiple domains
by including them with their corresponding inter-domain and transformed-domain covariances.
Under this formulation, we avoid computing the $N \times N$
transformed-domain covariance matrix $\bK_{N,N}^{**}$ that appears in the exact GP objective.
Instead, only $N$ terms of variance $k_{n,n}^{**}$ need to be evaluated. 
Importantly, the disentanglement of observed and latent domains
enables us to exploit structure of  $\bK_{\bu, \bu}$ for efficient computations.
Such exploitation would be difficult without a variational approximation,
especially in the case of mixed observations from multiple domains.

\paragraph{Whitened Parameterization}
Whitened parameterizations are used to improve
inference in models with correlated priors because they offer a
better-conditioned posterior
\citep{murray2010slice, hensman2015mcmc}.
Here we will show an additional computational benefit
in the variational setting --- the whitened posterior allows us to
avoid computing $\ln |\bK_{\bu,\bu}|$ which appears in the KL term
in Equation~\ref{eq:inter-domain-svgp}.
%a fact that has not been observed in the literature.
To define the whitened parameterization, we describe the GP prior over
$\bu$ as a deterministic function of standard normal parameters $\bepsilon$: 
\begin{align}
  \bepsilon \sim \mathcal{N}(0, I) \,, \quad
  \bu = \bR \bepsilon.
\end{align}
To preserve the covariance structure in the prior distribution of $(\rho^*_n, \bu)$ (Equation~\ref{eq:inter-domain-GP}),
the transformation $\bR$ and
the whitened correlation $\bk_n \triangleq Cov(\bepsilon, \rho^*_n)$ need to satisfy the following two equalities:  
\begin{equation}
  \begin{aligned}
  \label{eq:whitened-criteria}
  \bK_{\bu, \bu} &=  Cov( \bR \bepsilon, \bR \bepsilon) =   \bR \bR^\top \, , \\
  \quad  \bk^{*}_{\bu, n} &= Cov(\bR \bepsilon, \rho^*_n) = \bR \bk_n \, . 
\end{aligned}
\end{equation}
The classical whitening strategy in GP inference is to use the Cholesky decomposition: $\bK_{\bu, \bu} = \bL \bL^\top$ where  $\bL$ is a lower triangular matrix. In this case, $\bR = \bL$ 
and $\bk_n = \bL^{-1} \bk^*_{\bu, n}$.

\iffalse
\begin{align}
  \bepsilon \sim \mathcal{N}(0, I) \,, \quad
  \bu = \bK_{\bu,\bu}^{1/2} \bepsilon \sim \mathcal{N}(0, \bK^{}_{\bu,\bu}) \,.
   \label{eq:matrix-sqrt}
\end{align}
\fi
Now we can target the variational posterior over
the whitened parameters
$\bepsilon$: 
${q_{\blambda}(\bepsilon) = \mathcal{N}(\bepsilon \given \bm, \bS)}$.
The resulting \emph{whitened variational objective} is
\begin{align}
\label{eq:whitened-elbo}
\mathcal{L}(\blambda)
  %&= \mathbb{E}_{q_{\blambda}(\epsilon) p(\brho \given \epsilon)}
  %    \left[ \ln p(\by \given \brho) \right] -
  %    KL(q_{\blambda}(\epsilon) \,||\, p(\epsilon)) \\
  &= \sum_{n} \underbrace{
  	    \mathbb{E}_{q_{\blambda}(\bepsilon) p(\rho_n^* \given \bepsilon)} \left[
	    	\ln p(y_n \given \rho_n^*)
	    \right]
	  }_{\circled{$a'_n$}} -
      \underbrace{
	  	KL(q_{\blambda}(\bepsilon) \,||\, p(\bepsilon))
	  }_{\circled{b$'$}} \,
\end{align}
where
\begin{align}
\circled{$a'_n$}
&= -\frac{1}{2}\ln \sigma_n^2 - \frac{1}{2\sigma_n^2}
    \Big( y_n^2 + k^{**}_{n,n} - \bk_n^\intercal \bk^{}_n  \nonumber \\
&\quad + \bk_n^\intercal
     \left( \bS + \bm \bm^\intercal \right) \bk_n - 2 y^{}_n \bk_n^\intercal \bm
    \Big) \,\,,\\
\circled{b$'$}
%KL(q_{\blambda}(\epsilon) \,||\, p(\epsilon))
  &= \frac{1}{2}\left( \text{tr}(\bS) + \bm^\intercal \bm - \ln |\bS| - M \right) .
\label{eq:kl-term}
\end{align}
\iffalse
 and we define the correlation term for clarity
 \begin{align}
    \bk_n \triangleq Cov(\bepsilon, \be_n)  = \bK_{\bu,\bu}^{-1/2}\bK^*_{\bu,n}  \,\,.
    \label{eq:k-vector}
 \end{align}
 \fi
%Becuase $p(\bepsilon)$ is a standard normal we have simplified term \circled{$b'$},
%by avoiding computing $\ln |\bK_{\bu,\bu}|$.
\paragraph{Computational Bottlenecks}
The whitened objective above still factorizes over data points. However, there remain two computational bottlenecks. First, the correlation term $\bk_n$ in \circled{$a_n'$}
depends on the choice of the whitening strategy.
The common Cholesky strategy requires $O(M^3)$ computation and $O(M^2)$ storage
which is infeasible for large $M$.
We address this bottleneck in Section~\ref{sec:computational}.
The second bottleneck lies in the variational covariance $\bS$ which is an $M \times M$ matrix,
requiring $O(M^2)$ to store and $O(M^3)$ to compute the $\ln |\bS|$ in \circled{$b'$}.
We will address this problem by a structured
variational approximation in Section~\ref{sec:variational-families}.
