% new_TLP2egui.tex / guide for TLP
% v2.12, released 23-apr-2003
%   (based on JFP2egui.tex v1.01) and tlp2egui.tex
% Copyright (C) 2000,2001,2002,2003, 2012 Cambridge University Press

\NeedsTeXFormat{LaTeX2e}

\documentclass{new_tlp}
\usepackage{mathptmx}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{picinpar}
\usepackage{xcolor}
\usepackage{amsmath}

%%% Macros for the guide only %%%
\hyphenation{either}
\providecommand\AMSLaTeX{AMS\,\LaTeX}
\newcommand\eg{\emph{e.g.}\ }
\newcommand\etc{\emph{etc.}}
\newcommand\bcmdtab{\noindent\bgroup\tabcolsep=0pt%
  \begin{tabular}{@{}p{10pc}@{}p{20pc}@{}}}
\newcommand\ecmdtab{\end{tabular}\egroup}
\newcommand\rch[1]{$\longrightarrow\rlap{$#1$}$\hspace{1em}}
\newcommand\lra{\ensuremath{\quad\longrightarrow\quad}}

\usepackage{picinpar}
\usepackage{times}
\usepackage{helvet}
\usepackage{courier}

 \usepackage{todonotes}
 
\usepackage{graphicx} % support the \includegraphics command and options
%\usepackage{amsmath, amsthm, amssymb} 
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{epstopdf}
\usepackage{url} 


\usepackage{amsfonts, amsmath, amssymb, mathrsfs}
\usepackage{times, helvet, courier}
\usepackage{graphicx, epsfig, epstopdf}
\usepackage{wrapfig, caption, multirow, url}
%\usepackage[algo2e,ruled,linesnumbered,noresetcount,vlined]{algorithmic,algorithm}

\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage[font=small,format=hang,parskip=1pt]{caption}

\usepackage{stmaryrd}
%\DontPrintSemicolon
\usepackage{url}
\usepackage{csquotes}
\usepackage{listings}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{todonotes}

  \title[Creating Phylotastic]
        {Phylotastic: An Experiment in Creating, Manipulating, and Evolving
        	 Phylogenetic Biology Workflows Using Logic Programming}

  \author[T. Nguyen, E. Pontelli, T. C. Son]
         {THANH HAI NGUYEN, ENRICO PONTELLI, TRAN CAO SON\\
         	Department of Computer Science,
         	New Mexico State University\\
         \email{thanhnh | epontell | @nmsu.edu, tson@cs.nmsu.edu}}

\jdate{February 2018}
\pubyear{2018}
\pagerange{\pageref{firstpage}--\pageref{lastpage}}
\doi{S1471068401001193}

\newtheorem{lemma}{Lemma}[section]

%\usepackage{comments}

\sloppy
\allowdisplaybreaks

\setlength{\belowcaptionskip}{-15pt}

\begin{document}
%\nocite{*}% includes all entries of BibTeX database into the list of references.


\label{firstpage}

\maketitle
 
%%%%%% \input{section_1_2_3} 


  \begin{abstract}
    Evolutionary Biologists have long struggled with the challenge of developing 
    analysis workflows in a flexible manner, thus facilitating the reuse of phylogenetic
    knowledge. An evolutionary biology workflow can be viewed as a plan which composes web
    services that can retrieve, manipulate, and produce phylogenetic trees.
    The Phylotastic project was launched two years ago as a collaboration between evolutionary
    biologists and computer scientists, with the goal of developing an open architecture to facilitate
    the creation of such analysis workflows. While composition of web services is a problem that
    has been extensively explored in the literature, including within the logic programming domain,
    the incarnation of the problem in Phylotastic provides a number of additional challenges. 
    Along with the need to integrate preferences and formal ontologies in the description of the
    desired workflow, evolutionary biologists tend to construct workflows in an incremental manner,
    by successively refining the workflow, by indicating desired changes (e.g., exclusion of
    certain services, modifications of the desired output). This leads to the need of 
    successive iterations of incremental replanning, to develop a new workflow that integrates
    the requested changes while minimizing the changes to the original workflow. 
  This paper illustrates how Phylotastic has addressed the challenges of creating and refining
  phylogenetic analysis workflows using logic programming technology and how such solutions have
  been used within the general framework of the Phylotastic project. 
  \end{abstract}

  \begin{keywords}
    Bioinformatics, workflows, web services, planning, composition, re-composition, similarity, quality of 
    service
  \end{keywords}

\section{Introduction}


A \emph{phylogeny} (phylogenetic tree) is a representation of the evolutionary history of a set of entities---in the context of
this work, we focus on phylogenies describing biological entities (e.g., organisms). Typically, 
a phylogeny is a branching diagram showing relationships between species, but phylogenies can be drawn for individual 
genes, for populations, or for other entities (e.g., non-biological applications of phylogenies include the study
of evolution of languages). Phylogenies are built by analyzing specific properties of the species
(i.e., \emph{characters}), such as morphological traits (e.g., body shape, placement of bristles or shapes of cell
walls), biochemical, behavioral or molecular features of species or other groups. In building a tree, species are organized into nested groups based on
shared derived traits (traits different from those of the group's ancestor). Closely related species typically have fewer 
differences among the value of their characters,  while less related 
species tend to have more.
Currently, phylogenetic trees can be either explicitly constructed (e.g., from a collection of descriptions of species), or extracted from repositories phylogenies, such as {\small \tt OpenTree} \footnote{{\small \url{http://tree.opentreeoflife.org}}} and {\small \tt TreeBASE} \footnote{{\small \url{https://treebase.org}}}. In a phylogeny, the topology is the branching structure of the tree. It is of  biological significance, because it indicates patterns of relatedness among {\small\tt taxa}, meaning that trees with the same topology provide the same biological interpretation. Branches show the path of transmission from one generation to the next. Branch lengths indicate genetic change, i.e., the longer the branch, the more genetic change (or divergence) has occurred. A variety of methods have been devised to estimate a phylogeny from the traits of the taxa (e.g., \cite{felse}).

 Phylogenies are
useful in all areas of biology,  to provide a hierarchical framework for classification and for
process-based models that allow scientists to make robust inferences from comparisons of evolved
things. 
A standing dream in the field of evolutionary biology is the assembling a \emph{Tree of Life (ToL),} 
a phylogeny broadly covering some
$10^7$ species \cite{ph1,ph2}. The first draft of a grand phylogenetic synthesis---a single tree with perhaps $2.5 \times 10^6$ 
species---is emerging  from the Open Tree of Life (OpenTree) project. Yet, the current state of the ToL is a
collection of trees, and there are various reasons to expect that this situation will persist. When we
refer to ``the ToL'' here, we mean the dispersed set of available species trees, with a strong focus on
larger trees from reputable published sources (e.g., \cite{ph4,birds,ph5}).

While experts continue expanding, consolidating, and improving the ToL, our motivation is to put this
expert knowledge into the hands of everyone: ordinary researchers, educators, students, and the public.
To achieve this, we launched the \emph{Phylotastic} project, aimed at building a community-sustainable architecture to support
flexible on-the-fly delivery of expert phylogenetic knowledge.
The premise of disseminating knowledge is that it will be re-used. How do trees get re-used? On a
per-tree basis, re-use is rare---most trees are inferred \emph{de novo} for a specific study, rendered as
images in a published report, stored on someone's hard drive, and (apparently) not used again \cite{ph3}.
Yet, large species trees are re-used in ways that small species trees (and sequence-family trees)
are not. In a sample of 40 phylogeny articles, \citeN{ph3} found 6 cases in which scientists
obtained a custom tree by extraction from a larger species tree. These studies implicate diverse
uses: functional analyses of leaf traits or lactation traits; phylogenetic
diversity of forest patches; analyzing niche-diversity correlations, spatial distribution of wood
traits, and spatial patterns of diversity. The implicated trees include those covering $4,510$
extant mammals, $55,473$ angiosperm species, and $1,566$ angiosperm taxa.

The Phylotastic project offers a solution to  the reuse of phylogenetic knowledge problem, by adopting a web services
composition approach. The \emph{phyloinformatic} community has been very active in the development of a diversity
of data repositories and software tools, to collect and analyze artifacts relevant to evolutionary analysis. These
tools are sufficient to realize all of the studies in the previously
mentioned papers, when properly integrated in a coherent workflow. 
Furthermore, the analysis protocols adopted in such studies are often
the result of an iterative process, where the protocol is successively refined to better suit the available
data and produce results of adequate quality \cite{ph3}.

In this paper, we describe the infrastructure used by Phylotastic to achieve the following goals:
{\bf (1)} Allow a user to provide the available knowledge about the desired protocol (e.g., input, type of
output, selected classes of operations that should be used); 
{\bf (2)} Derive a collection of workflows
that satisfy the desired conditions, through a web service discovery and composition process; 
{\bf (3)} Allow the user to suggest manipulations of a chosen workflow (e.g., exclusion of a service, addition
of another output); 
{\bf (4)} Determine new workflows that satisfy the requested manipulations while 
maintaining maximum similarity with the chosen workflow. All the manipulations of the workflows---i.e., 
composition of web services, modification of workflows, computation of similarity between workflows---are
realized in Answer Set Programming (ASP). ASP provides a clear advantage, allowing the simple integration
of different forms of knowledge (e.g., ontologies describing services and artifacts, user preferences) and
facilitating the encoding of the composition and re-composition problems as ASP planning problems. We present
the overall structure of the Phylotastic architecture, describe how web service composition is encoded in ASP,
and analyze how workflow refinements are achieved. We also demonstrate various aforementioned features 
through a use-case. 
 
\section{Background: The Phylotastic Project and Its Implementation}
 
\subsection{Architecture}

The Phylotastic project proposes  a flexible system for on-the-fly delivery of custom 
phylogenetic trees that would support many kinds of tree re-use, and be open 
for both users and data providers. Phylotastic proposes an open architecture, composed of a collection of \emph{web services} relevant 
to creation, storage, and reuse of phylogenetic knowledge, that can be assembled in user-defined workflows through a Web portal and mobile applications (Fig. \ref{overall}).

\begin{figure}[htbp]
  \centering \includegraphics[width=0.7\textwidth]{overall1}
  \caption{Overall Phylotastic Structure}
  \label{overall}
\end{figure}

 
Figure~\ref{overview} shows an overview of our web service composition framework for Phylotastic. It consists of
a web service registry, an ontology, a planning engine,  a web service execution monitoring system, and a
workflow description tool. The flow of execution of the architecture starts with the \emph{workflow description tool}---a graphical
user interface that allows the user to provide information about the desired requirements of the phylogenetic trees generation
or extraction process.
The information collected from the user interface are mapped to the components of a planning problem instance, that will drive the
web service composition process. The planning problem instance representing the web service composition problem is obtained by
integrating the user goals with the description of web services, obtained from the service registry and the ontology. The planning engine
is responsible for deriving an executable workflow, which will be enacted and monitored by a web service execution system. The final outcome
of the service composition and execution is presented  to the user using the same workflow description tool.
\begin{figure}[htbp]
  \centering \includegraphics[width=0.9\textwidth]{overall}
  \caption{The Phylotastic Web Service Composition Framework}
  \label{overview}
\end{figure}




%%%% \input{section_4}

%\section{Workflow Creation as a Planning Problem}
Thus, the   components of the Phylotastic web service composition framework are:
\begin{list}{$\bullet$}{\itemsep=0pt \topsep=1pt \parsep=0pt \leftmargin=10pt} 

\item \emph{The Phylotastic Ontology}: this ontology is composed of
two parts: an ontology that describes the artifacts manipulated by the services (e.g., alignment matrices, phylogenetic trees, species names)  (the CDAO ontology \cite{cdao1}) and an 
ontology that describes the actual operations and transformations performed by the services.  Each class of services is associated with a name, inputs, parameters, and outputs. Instances of the services will also be associated with the data formats of their inputs, outputs, and parameters.  
For example, a service in the class \textit{taxon\_based\_ext} takes \textit{bio\_taxa} as an input and produces outputs \textit{species\_tree} and \textit{http\_code}. An instance of this class is \textit{get\_PhyloTree\_OT\_V1} whose input (\textit{bio\_taxa}) has the \textit{list\_of\_strings} format and its outputs (\textit{species\_tree}, \textit{http\_code}) have the \textit{newickTree} and \textit{integer} format, respectively. This information can be encoded in ASP as follows:\\
\texttt{\small
\hspace*{.5cm}op\_cl(tree\_ext). op\_cl(taxon\_based\_ext). op(get\_PhyloTree\_OT\_V1). \\
\hspace*{.5cm}subcl(taxon\_based\_ext,tree\_ext). cl(bio\_taxa). cl(species\_tree). \\
\hspace*{.5cm}t\_of(get\_PhyloTree\_OT\_V1,taxon\_based\_ext).\\
\hspace*{.5cm}has\_input(taxon\_based\_ext,set\_of\_names\_1,bio\_taxa). \\
\hspace*{.5cm}has\_output(taxon\_based\_ext,phylo\_tree\_1,species\_tree). \\
\hspace*{.5cm}has\_output(taxon\_based\_ext,http\_code\_1,http\_code).\\
\hspace*{.5cm}has\_inp\_df(get\_PhyloTree\_OT\_V1,bio\_taxa,list\_of\_strings). \\
\hspace*{.5cm}has\_out\_df(get\_PhyloTree\_OT\_V1,species\_tree,newickTree). \\
\hspace*{.5cm}has\_out\_df(get\_PhyloTree\_OT\_V1,http\_code,integer).
}


\item \emph{Web Service Composition as Planning}:  
In Phylotastic, we adopt the  view, advocated by several researchers, of mapping the web service
composition problem to a \emph{planning problem} \cite{compose4,mcisonzen01,compose7}. In this perspective, available web services
are viewed as \emph{actions} (or \emph{operations}) that can be performed by an agent, and the problem of determining the overall workflow can be reduced to a planning problem.
 In general, a planning problem can be described as a five-tuple $(S, S_0, G, A , I')$, where S is set of all possible states of the world, $S_0 \subseteq S$ denotes the initial state(s) of the world, $G \subseteq S$ denotes the goal states  the planning system attempts to reach, $A$ is the set of actions   to change one state of the world to another state, and the transition relation $I' \subseteq S \times A \times S$ defines the precondition and effects for the execution of each action. In term of web services, $S_0$ and $G$ are the initial state and the goal state specified in the requirement of web service requesters (e.g., the available input and the desired output of the workflow). The set  $A$ is a set of available services; $I'$  describes the effect of the execution of each service (e.g., data produced).
 
\item \emph{Web Service Engine}: The planning engine implemented in the Phylotastic project employs  
Answer Set Planning (ASP) \cite{Lifschitz02} and is responsible
for creating an executable workflow from the incomplete workflow and/or from the user specifications. The basic
planning algorithm has been discussed by \citeN{NguyenSP18}. This engine differs from the usual ASP-planning system 
in that it uses a two-stage process in computing solutions. In the first stage, planning is done at the 
abstract-level and the engine considers only service classes, matching their inputs and outputs. The result is 
a workflow whose elements are web services described at the abstract level. The second stage instantiates 
the abstract web services from the first stage with concrete services. In the process, it might need
to solve another planning problem, to address the issues of mismatches between formats of different 
services; for example,  there are several concrete services to recognize gene names and their outputs are sets
of scientific names of the genes; however, they are saved in different formats. 
This program consists of following ASP-rules encoding the operations and the initial state. 
The key rules are 

\begin{list}{$\circ$}{\itemsep=0pt \topsep=1pt \parsep=0pt} 
\item  {\texttt{\small ext(I,0) :- initially(I,DF$_I$).}} \:\: This rule states that the resource \texttt{I} with data format \texttt{DF$_I$} exists at the time moment 0. 

\item The rule encoding the operations and their executability are as follows: \\
\texttt{\small
\hspace*{.5cm} \{executable(A,T)\} :- op\_cl(A).   \\
\hspace*{.5cm} :- executable(A,T), has\_input(A,N,I), not match(A, I, T).  \\
\hspace*{.5cm} p\_m(A,I,T,O,T$_1$) :- op\_cl(A), has\_input(A,N,I), T$_1{\le}$T,  \\
\hspace*{4cm} ext(O,T$_1$),subcl(O,I).  \\
\hspace*{.5cm} 1 \{map(A,I,T,O,T$_1$) : p\_m(A,I,T,O,T$_1$)\}1.\\
\hspace*{.5cm} match(A,I,T) :- map(A,I,T,\_,\_). 
}
\item The next rules are used to generation operation occurrences:  \\
\texttt{\small
\hspace*{.5cm} 1 \{ occ(A,T): op\_cl(A) \} 1.   \\
\hspace*{.5cm} :- occ(A,T), not executable(A,T).\\
\hspace*{.5cm} ext(O,T+1) :- occ(A,T), has\_output(A,N,O). 
}
\end{list} 
%
In all the rules, \texttt{T} or \texttt{T1} denotes a time step.  
 From now on, we will denote with $\Pi_L$ the ASP-program for web service composition in the Phylotastic 
project.  

\end{list} 
%%% \input{section_5}
\section{Selecting The Most Preferred Workflow: A Qualitative Approach}

Currently, $\Pi_L$ receives from the Workflow Configuration Tool a description of the desired workflow, e.g., 
desired input, desired output, specific classes of operations that should occur in the workflow.
Using
this information, it generates 
a concrete workflow meeting the desired requirements, 
sends it to the execution monitoring component which will execute the workflow and output 
the desired phylogenetic tree.  Frequently, there are several ways to construct a tree given the input, i.e., 
there are several solutions that $\Pi_L$ could return. Due to the differences in web services, 
not all solutions will produce the same result at execution (e.g., produce the same phylogeny), e.g., because of lack of agreement on the
evolutionary relationships between certain species or the ambiguity of certain species names. 
 In this paper, we present two enhancements of the 
system. In both enhancements, the notion of a \emph{most preferred workflow} is defined and 
users can interact with the system to select their most preferred workflow(s). 


One way to compare the workflows  is to rely on the notion of  \emph{quality of service (QoS)} of web services.
QoS of a web service can be used as a discriminating factor that differentiates  
functionally similar web services. In general, QoS of a web service is characterized by several attributes, such as performance, reliability, scalability, accuracy, integrity, availability, and accessibility \cite{RajendranB09}. For 
the web services used in the Phylotastic project, we  collect the following attributes that influence the performance of a web service: {\bf (1)} \emph{response time}, {\bf (2)} \emph{throughput},
{\bf (3)} \emph{availability}, {\bf (4)} \emph{reliability}. Specifically, 
\begin{list}{$\bullet$}{\itemsep=0pt \parsep=1pt \topsep=1pt \leftmargin=10pt}
    \item {\em Response time}: Given a service $s$, the response time $q_{rt}(s)$ measures the  delay in 
    seconds between the moment when a request is sent and the moment when the results are received.
    \item {\em Throughput}: $q_{tp}(s)$ is the average number of successful responses for a give period of time.
    \item {\em Availability}: The availability $q_{av}(s)$ of a service $s$ is the probability that the service is accessible for a given period of time. 
    The value of the availability of a service $s$ is computed using the following expression $q_{av}(s) = T_a(s) / 
    \theta$, where $T_a$ is the total amount of time (in seconds) in which service $s$ is available during the 
    last $\theta$ seconds ($\theta$ is a constant set by an administrator of the service community).
        
    \item {\em Reliability}: The reliability $q_{re}(s)$ is the average operation time of service $s$ in which service $s$ is accessible and processes clients     requests successfully. It is measured by total operation time of  service $s$ divided by the number of failures. 
\end{list}
The quality vector of a service $s$ is denoted by the tuple  
$q(s) = (q_{rt}(s), q_{tp}(s), q_{av}(s), q_{re}(s))$. For the web services in the Phylotastic project, this information 
is maintained in the Service Registry (along with the ontology-based description of each service). 
We next define the QoS of a workflow based on the QoS of the web services. 

\subsection{QoS of Workflows}
\label{qos_composite}


%The quality criteria defined above in the context of single web service, are also used to evaluate the QoS of 
%composite services. Assume that, there is a composite services $p$ is a sequence of $n$ single web services $p = (s_1,s_2,...,s_n)$:

Let $p = (s_1,s_2,...,s_n)$ be a workflow of web services. The quality of services of $p$, denoted by $q(p)$, 
is defined by  $q(p) = (q_{rt}(p), q_{tp}(p), q_{av}(p), q_{re}(p))$ where 
%
\begin{list}{$\bullet$}{\itemsep=0pt \parsep=0pt \topsep=0pt \leftmargin=10pt}
    \item {\em Response time}: The response time $q_{rt}(p)$ is defined as total response time of all  services in the workflow $p$: $q_{rt}(p) = \sum_{i=1}^{n} q_{rt}(s_i)$. 
    
    \item {\em Throughput}: The throughput $q_{tp}(p)$ of plan $p$ is the average of the throughputs of the services that participate in $p$: $q_{tp}(p) = \frac{\sum_{i=1}^{n} q_{tp}(s_i)}{n}$. 
    
    \item {\em Availability}: In general, the availability of the services for $p$ should be defined as 
    $q_{av}(p) = \prod_{i=1}^{n} Pr(s_i \mid s_1,\ldots,s_{i-1})$ where $Pr(s_i \mid s_1,\ldots,s_{i-1})$ is the conditional probability of $s_i$ is available given that $s_1,\ldots,s_{i-1}$ have been successfully executed. For simplicity of  representation, we assume that the services are mutually independent, then  $q_{av}(p) = \prod_{i=1}^{n} q_{av}(s_i)$. In our current implementation, we use $q_{av}(p) = \min \{q_{av}(s_i) \mid i =1,\ldots,n\}$, as an approximation which avoids extensive floating point operations.

%    
%    
%    The availability $q_{av}(p)$ of a services composition $p$ is computed as:
%    %\[q_{av}(p) = \prod_{i=1}^{n} q_{av}(s_i) \]
%    \[q_{av}(p) = \frac{\sum_{i=1}^{n} q_{av}(s_i)}{n} \]
%    {\color{red}
%    The best computation for the availability of workflow $q_{av}(p)$ should be a product of availability 
%    values of all single services in the workflow. However, there are some issues that prevent us implement it in experiment: (1) 
%    there is no aggregate build-in function for product of items in set in ASP, (2) ASP does not work with 
%    floating-point number conveniently. In order to solve (1), we can implement an ASP encoding to compute product of all values 
%    based on search algorithm. The performance may not good as aggregate but it is possible. However, for issue (2), availability value of service 
%    has been stored in ASP as integer number to compute further by rough of the multiplication of real availability value and 100.  In the case, there is 
%    more than 10 nodes in the workflow. If we have applied this computation, the value of  $q_{av}(p)$ would be a huge integer number that 
%    ASP cannot work on it.    
%    }
    %\todo[inline]{Provide an intuition why you take the average here, not obvious}
    \item {\em Reliability}: The reliability $q_{re}(p)$ is calculated as the average of reliability values of element services in $p$: $q_{re}(p) = \frac{\sum_{i=1}^{n} q_{re}(s_i)}{n}$. 
\end{list}

%The quality vector of execution plan $p$ is defined as : $q(p) = (q_{rt}(p), q_{tp}(p), q_{av}(p), q_{re}(p))$.  

%In order to measure the value of $q(p)$, we implemented two steps as following:\\
%\smallskip\noindent
%{\bf Scaling - Normalization values.} Some of the criteria could be negative,--e.g. the higher the value, the 
%lower the quality such as response time. Other criteria are positive,--e.g., the higher the value, the higher 
%the quality. For negative criteria, values are normalized according to equation:
%\[V_{i,j} = 
%     \begin{cases}
%          \frac {Q_j^{max} - Q_{i,j}} {Q_j^{max} - Q_j^{min}}, & \text{if }  Q_j^{max} - Q_j^{min} \neq 0 \\
%          1                                                                                               , & \text{if }  Q_j^{max} - Q_j^{min} = 0           
%      \end{cases}
%\]
%For the positive criteria, values are scaled as following equation:
%\[V_{i,j} = 
%     \begin{cases}
%          \frac {Q_{i,j} - Q_j^{min}} {Q_j^{max} - Q_j^{min}}, & \text{if }  Q_j^{max} - Q_j^{min} \neq 0 \\
%          1                                                                                               , & \text{if }  Q_j^{max} - Q_j^{min} = 0           
%      \end{cases}
%\]
%In the above equations, $Q_j^{max}$, $Q_j^{min}$ are the maximal and minimal values of a quality criteria in matrix $Q$
%-- e.g. $Q_j^{max} = Max(Q_{i,j}), 1 \leq i \leq n$;  $Q_j^{min} = Min(Q_{i,j}), 1 \leq i \leq n$. By applying these two equations on $Q$, we obtain a matrix 
%$V = (V_{i,j}, 1 \leq i \leq n,$ j is one of 4 above quality criteria$)$. After this step, the normalized quality vector 
%of execution plan $p$ is : 
%\[q(p) = (q_{rt}^{norm}(p), q_{tp}^{norm}(p), q_{av}^{norm}(p), q_{re}^{norm}(p))\]  
%\smallskip\noindent
%{\bf Weighting.} The following formula is used to computed the overall quality score for a services composition $p$:  
%\[ScoreQoS(p) = q_{rt}^{norm}(p)*W_{rt} +q_{tp}^{norm}(p)*W_{tp} + q_{av}^{norm}(p)*W_{av} + q_{re}^{norm}(p)*W_{re} \] 
%where $W_{j} \in [0,1]$ and $\sum_{j} W_{j} = 1$. and $W_j$ represents the weight of criterion $j$. As stated 
%before, end users express their preferences regarding QoS by providing values  for the wights $W_j$.
%
%\comment{
%   what are the $Q_i$ - where do they come frome? $q(p)$ is defined two times? 
%}

 \subsection{ASP Encoding of QoS}
%{%\color{red}
%\comment{Thanh: rework this section} 

%\todo[inline]{THANH : Need to be consistent with terminology; either you talk of sequence of services or you talk of workflows. I would prefer the second. Also sequence is confusing - in reality it is a graph}
The QoS of a workflow can be computed using ASP as follows. 
%As with the actions used in the web composition process, we extract the QoS information of services and represent it as ASP-facts a
%of the forms: $has\_qos\_pre(service, value)$ where $pre \in \{rt, av, tp, re\}$, $service$ is the service identifier, and $value$ is 
%the QoS value of the corresponding attribute. The computations of QoS attributes for a workflow are encoded as following:
As with the actions used in the web composition process, we extract the QoS information of services and represent it as ASP-facts  
of the forms: $has\_qos\_rt(s, v),has\_qos\_av(s, v),has\_qos\_tp(s, v),has\_qos\_re(s, v)$ where $s$ is the service identifier, and $v$ is 
the QoS value of the corresponding attribute. The computations of QoS attributes for a workflow are encoded as following:


%%
%
%
%%\todo[inline]{explain the rules. For example, what is that huge number in the first rule???}
%

{\footnotesize
\[
	\begin{array}{ll}
%	has\_qos\_rt\_inverse(X,Z) & :- has\_qos\_rt(X,Y), Z = (1000000 / Y).   \\
%	has\_qos\_rt\_involved(X,T,RT)& :- occ\_concrete(X,T), has\_qos\_rt(X,RT). \\
%	has\_qos\_tp\_involved(X,T,TP)& :- occ\_concrete(X,T), has\_qos\_tp(X,TP). \\
%	has\_qos\_re\_involved(X,T,RE)& :- occ\_concrete(X,T), has\_qos\_re(X,RE). \\
%	has\_qos\_av\_involved(X,T,AV)& :- occ\_concrete(X,T), has\_qos\_re(X,AV). \\
	qos\_rt\_wf(RT_{W}) & {:}{- }\quad RT_{W} = \#sum\{RT,T,X : occ\_concrete(X,T),has\_qos\_rt(X,RT)\}. \\
	qos\_tp\_wf(TP_{S}/nsteps)& {:}{-}\quad TP_{S} = \#sum\{TP,T,X : occ\_concrete(X,T),has\_qos\_tp(X,TP)\}.\\
%	& TP\_N = \#count\{TP,T,X : occ\_concrete(X,T),has\_qos\_tp(X,T,TP)\},\\
%	& TP\_W = (TP\_S / TP\_N).\\
	qos\_re\_wf(RE_{S}/nsteps) &{:}{-}\quad RE_{S} = \#sum\{RE,T,X : occ\_concrete(X,T),has\_qos\_re(X,RE)\}.\\
%	& RE\_N = \#count\{RE,T,X : occ\_concrete(X,T),has\_qos\_re(X,T,RE)\},\\
%	& RE\_W = (RE\_S / RE\_N).\\
	%qos\_av\_wf(AV\_W) &:- AV\_S = \#sum\{AV,T,X : has\_qos\_av\_involved(X,T,AV)\},\\
          %& AV\_N = \#count\{AV,T,X : has\_qos\_av\_involved(X,T,AV)\},\\
          %&AV\_W = (AV\_S / AV\_N). 
          qos\_av\_wf(AV_{W}) &{:}{-}\quad AV_{W} = \#min\{AV,T,X : occ\_concrete(X,T),has\_qos\_av(X,AV)\}.
\end{array}
\]}

%
In the above rules, $nsteps$ is the number of services  in the plan
that is computed by the planning module. 
%{\color{red}
%In reality, most of QoS values are real numbers with floating decimal point and working with floating-point numbers in ASP is not 
%convenient so we stored these values in Ontology and ASP encoding as integer numbers by using rough of the multiplication of QoS real value and 1000
%(e.g., response time of a service s is 2.03835 seconds then its encoding is {\tt \small has\_qos\_rt(s,2038)}).  Response time is a negative criterion (--e.g. 
%the higher the value, the lower the quality) and the first rule measures the inverse value of response time which is positive criterion by 1000 divided by 
%encoded response time value. In order to keep the inverse value of response time as integer number in ASP computing, this value is multiplied with 1000 
%again.  This computation is only applied for response time, others criteria (availability, reliability and throughput) 
%are positive. The next four rules select QoS values that are corresponding to concrete services in the workflow. Based on these selected QoS values,
% the next four rules compute QoS values for whole workflow following the equations in \ref{qos_composite}.
%}\\
Since the QoS of a service (or a workflow) is a tuple of values representing different attributes, 
there are different ways for comparing services (or workflows). 
Different users might have different preferences over these attributes (e.g., response time is the most important factor, or reliability is the most important factor, etc.). 
We discuss two possibilities: 

\begin{list}{$\bullet$}{\itemsep=0pt \topsep=1pt \parsep=1pt \leftmargin=10pt} 
\item \emph{Weighted QoS}: A user specifies the weights $W_{rt}$, $W_{tp}$, $W_{av}$, and $W_{re}$ that he/she would like to assign for 
the response time, the throughput, the availability, and the reliability, respectively. 
The weighted QoS of a plan $p$ is then computed by $w(p) = q_{rt}(p)*W_{rt} +q_{tp}(p)*W_{tp} + q_{av}(p)*W_{av} + q_{re}(p)*W_{re}$. 
Under this view, $w(p)$ can be computed as follows: 
%
$
 \hspace*{.5cm}score\_qos\_wf(Sc)\:{:}{-} \:\:\:\: qos\_rt\_wf(RT_{W}),wei\_rt(W_{rt}),qos\_tp\_wf(TP_{W}),wei\_tp(W_{tp}), \newline  
 \hspace*{3.5cm}qos\_re\_wf(RE_{W}),wei\_re(W_{re}),qos\_av\_wf(AV_{W}),wei\_av(W_{av}),  \newline
 \hspace*{3.5cm}Sc = RT_{W}*W_{rt} + TP_{W}*W_{tp} + RE_{W}*W_{re} + AV_{W}*W_{av}.
$

\noindent To select workflows with the best QoS, we will only need to add the statement 
\[
\mathit{\#maximize\{ScoreQoS : score\_qos\_wf(ScoreQoS)\}.} 
\]

\item \emph{Specified Preferences QoS}: An alternative to the weighted QoS is to allow users to specify a partial ordering over the set of attributes
that will be used in identifying most preferred workflows by a lexical ordering in accordance to the preferences. 
For example, assume that the preference ordering is $x_1 > x_2 > x_3 > x_4$ where $x_i > x_j$ means that attribute $x_i$ is 
 preferred to the 
attribute $x_i \ne x_j$ and  $x_i  \in \{rt, av, tp, re\}$. 
As the values in the QoS of a service behave differently, we write $q_{x}(s) \prec q_{x}(s')$ to denote that $s$ 
is better than $s'$ w.r.t. the attribute $x$.  
 The most preferred workflow is defined via a lexicographic ordering: $p \prec p'$ if there is $1 \leq i \leq 4$ such that $q_{x_j}(p) = q_{x_j}(p')$ for $j < i$ and $q_{x_i}(p) \prec q_{x_i}(p')$. 
 This can easily be implemented using the priority level in \textbf{clingo}. \\
 %
\textit{\small 
  \#maximize \{$q_{x_1}$@4\}.\:\:\:\#maximize \{$q_{x_2}$@3\}.\:\:\:\#maximize \{$q_{x_3}$@2\}.\:\:\:\#maximize \{$q_{x_4}$@1\}.
} 

\noindent For a concrete example, assume that the preference ordering is $rt > re > tp > av$, the corresponding ASP encoding is as follow:
\begin{align*} 
  \#maximize \{RT_{W}@4 : qos\_rt\_wf(RT_{W})\}. \quad
  \#maximize \{RE_{W}@3 : qos\_re\_wf(RE_{W})\}. \\
  \#maximize \{TP_{W}@2 : qos\_tp\_wf(TP_{W})\}. \quad
  \#maximize \{AV_{W}@1 : qos\_av\_wf(AV_{W})\}. 
\end{align*}
\end{list} 
The above feature is  implemented in the Phylotastic system. 
In either case, we can display a set of workflows for the users to decide which workflow should be executed.  

 

\section{Refining a Workflow}
\label{sub:replanning_problem}


Evidence that emerged in the Phylotastic project as described by \citeN{phylotastic1} shows that evolutionary biologists tend to develop their analysis protocols in an incremental manner, through successive refinements, often driven by the specific properties of the dataset being processed and the opportunities revealed by intermediate results.
 Users of the Phylotastic system also often have strong preferences about certain type of services that they want (or do not want) to use. For this reason,  the Workflow Configuration Tool  allows a user to update a given workflow and resubmit it to the ASP-planner. Presently, the ASP-planner considers this as a new request and restarts the computation. This approach is simple but also has some drawbacks. First, the new workflow and the original workflow can be very different in the services that they use, which is often unexpected to the user (and undesired, since changing services might lead to a different phylogeny). Second, this approach can be computational expensive as it cannot reuse the original workflow. We propose an approach to 
 address the changes requested by a user that can preserve \emph{as much as possible the original workflow}. We focus on the following four categories of changes:
\begin{list}{$\bullet$}{\itemsep=0pt \parsep=1pt \topsep=1pt}
    \item {\em IO request}: Request to change input and/or output.
    \item {\em Avoidance request}: Avoid using one class of services.
    \item {\em Inclusion request}: Request that a particular service to be used in the workflow.
    \item {\em Insertion request}: Request that a service is inserted at a particular position.   
\end{list}


%snippets of logic programming encoding
\subsection{Similarity Between Workflows: Formalization} 
\label{sub:similarity_lib}
%
{ 

Given two workflow $p$ and $p'$, we define the concept of  \emph{similarity} between $p$ to $p'$.  
In the following, we view a workflow as a directed acyclic graph  $G = (V, E)$, where  $V$ is the set of nodes  and each node is a service; $E$ is the set of edges and each edge is an exchange of resources between two services in the workflow. Observe that each service $v$ will have a set of inputs, a set of outputs, and some description. 
For each service $v$, $input(v)$ and $output(v)$ denote the set of inputs and outputs of $v$, respectively.  
The \emph{similarity between two workflows} is defined as a combination of \emph{nodes similarity}, \emph{edges similarity}, and \emph{contextual and topology similarity} \cite{BeckerL12,AntunesBBCDFDFG15}. 
Let $G_1 = (V_1, E_1)$  and $G_2 = (V_2, E_2)$ be two graphs. 
The similarity between $G_1$ and $G_2$, 
denoted by  \texttt{sim\_workflows($G_1,G_2$)},  is defined next.  

\begin{list}{$\bullet$}{\itemsep=0pt \parsep=1pt \topsep=1pt \leftmargin=10pt}
\item 
{\bf Node Similarity.} Let $v_{1} \in V_1$ and $v_{2} \in V_2$.  We fist define
the similarity between two nodes and then use this measure to define the similarity between nodes of workflows. 
As a node represent a web service, the similarity of two nodes can be determined based on their mutual 
position in the ontology (note that the Phylotastic ontology classifies services based on a taxonomy of 
classes of services). Thus, two nodes can be considered to be  similar if they are close in the ontology, share the same 
inputs, outputs, or have similar descriptions. These features are considered in the following definitions. 
%
%Assume that, $V_1 = (v_{11},v_{12},...,v_{1n})$ and $V_2 = (v_{21},v_{22},...,v_{2m})$, and $v_{1} \in V_1$ and $v_{2j} \in V_2$ are arbitrary services nodes with $1\le i \le n$, $1\le j \le m$. We fist define
%the similarity between two nodes and then use this measure to define the similarity between nodes of workflows. 
%As a node represent a web service, two nodes can be similar if they are close within the ontology, sharing the same 
%inputs, outputs, or have similar descriptions. These features are considered in the following definitions. 
%
\begin{list}{$\circ$}{\itemsep=0pt \topsep=0pt \parsep=0pt \leftmargin=6pt}
	\item \texttt{sim\_nodes\_onto($v_{1},v_{2}$)}: measures the similarity between $v_{1}$ and $v_{2}$ by considering them as nodes in the ontology: \\%% based on the ontology semantic comparison.
			$sim\_nodes\_onto(v_{1},v_{2}) = \frac{1}{1 + d\_nodes\_onto(v_{1},v_{2})}, \:\: \textnormal{where}$\\
		   $d\_nodes\_onto(v_{1},v_{2}) = path\_len(lca(v_{1},v_{2}),v_{1}) + path\_len(lca(v_{1},v_{2}),v_{2})$.
%		   $lca(v_{1},v_{2})$ denotes the lowest common ancestor of nodes $v_{1}$ and $v_{2}$ in ontology hierarchical structure and $path\_len(x,y)$ denotes length of the path from ancestor x to preceptor y in a graph.
		   Intuitively, the similarity between two nodes in an ontology 
		   is disproportional to the distance ($path\_len(.)$) between their lowest common ancestors ($lca(.)$) of 
		   them. 
		   
		   
		   %\comment{THANH : lca means least common ancestor or lowest common ancestor? }  
	
	\item \texttt{sim\_nodes\_inp($v_{1},v_{2}$)}: measures the similarity the nodes by considering their inputs, the more inputs they share the more similiar they are. Thus, \\
	%between input parameters of two services nodes $v_{1}$ and $v_{2}$.
			$$sim\_nodes\_inp(v_{1},v_{2}) = 2*\frac{| input(v_{1}) \cap input(v_{2}) |}{| input(v_{1}) | + | input(v_{2}) |}.$$ 
%		   $input(v_{1})$ and $input(v_{2})$ are sets of input components of service class $v_{1}$ and $v_{2}$ respectively.
	
	\item \texttt{sim\_nodes\_oup($v_{1},v_{2}$)}: measures the similarity between two nodes by considering their outputs, computed similarly to \texttt{sim\_nodes\_oup($v_{1},v_{2}$)}. So,\\
	%  parameters of two services nodes $v_{1}$ and $v_{2}$.
			$$sim\_nodes\_oup(v_{1},v_{2}) = 2*\frac{| ouput(v_{1}) \cap output(v_{2}) |}{| output(v_{1}) | + | output(v_{2}) |}.$$
%		   $output(v_{1})$ and $output(v_{2})$ are sets of output components of service class $v_{1}$ and $v_{2}$ respectively.
	
	\item \texttt{sim\_nodes\_des($v_{1},v_{2}$)}: measures the similarity between the English descriptions of the two nodes. 
	We use off-the-shelf libraries {\small \tt Stanford CoreNLP} \footnote{ {\small \url{https://stanfordnlp.github.io/CoreNLP}}}, 
	{\small \tt NLTK}\footnote{ {\small \url{http://www.nltk.org/}}}, and {\small \tt Scikit-Learn}\footnote{ \small{\url{http://scikit-learn.org/}}} to 
	process the descriptions and transform these text descriptions to a matrix of {\small \tt TF-IDF} (term frequency-inverse document frequency) of 
	the two documents. From each document we derive a real-value {\small \tt TF-IDF} vector;  the similarity index between two text 
	descriptions is computed based on cosine similarity between their {\small \tt TF-IDF} vectors.    
	
%\comment{Thanh: how does the TF-IDF matrix becomes a number?} 
	
%	measure the similarity between the English descriptions of service node $v_{1}$ and $v_{2}$. Due to space of limitation of paper, we only briefly describe the basic idea of this function. This implementation is based on {\small \tt NLTK} \footnote{ {\small \url{http://www.nltk.org/}}}  and {\small \tt Scikit-Learn} \footnote{ \small{\url{http://scikit-learn.org/}}} libraries. In which, {\small \tt NLTK} that is built on top of {\small \tt Stanford CoreNLP} \footnote{ {\small \url{https://stanfordnlp.github.io/CoreNLP}}}  is a library to work with human language data and provides support for a wide variety of text processing tasks: tokenization, stemming, proper name identification, part of speech identification and so on; and {\small \tt Scikit-Learn} provides advanced analytic tasks: TF-IDF (term frequency-inverse document frequency), clustering, classification, etc. Given two text documents - English descriptions of services, our implementation works as follows: (1) Use {\small \tt Tokenization in NLTK} to chop document up into peaces, called \emph{tokens}, (2) Use {\small \tt NLTK} to remove \emph{stop words} out of \emph{tokens}, (3) Using {\small \tt Stemming and Lemmatization in NLTK} - crude heuristic processes - in which {\small \tt Stemming} chops off the ends of words and {\small \tt Lemmatization} uses of a vocabulary and morphological analysis of words in the hope of achieving the goal that is to reduce different forms of a word (such as \emph{organize, organizes, organizing}) and families of derivationally related works with similar meanings (such as democracy, democratic, democratization) to a common base form. (4) After cleaned up text, we use {\small \tt TF-ID in Scikit-Learn} to convert cleaned up documents to a matrix of {\small \tt TF-IDF} features and generate similarity index between two documents.
	 

\end{list}
The similarity between two nodes, denoted by \textbf{sim\_nodes($v_{1},v_{2}$)}, is then defined as the weighted sum between their four similarity 
measures 	  
%{\textbf{sim\_nodes($v_{1},v_{2}$)}: measures the similarity between two services nodes $v_{1}$ and $v_{2}$ based on summary of four above attributes: ontology semantic, input and output comparison and English description of services:
%			\[sim\_nodes(v_{1},v_{2}) = A_{onto} + A_{inp} + A_{oup} + A_{des}\]
%		In which, 
%		   \[ A_{onto} = w_{onto}*sim\_nodes\_onto(v_{1},v_{2}) \]
%		   \[ A_{inp} = w_{inp}*sim\_nodes\_inp(v_{1},v_{2}) \]
%		   \[ A_{oup} = w_{oup}*sim\_nodes\_oup(v_{1},v_{2}) \]
%		   \[ A_{des} = w_{des}*sim\_nodes\_des(v_{1},v_{2}) \]
%
			\[\begin{array}{ll} 
			sim\_nodes(v_{1},v_{2}) = &
			                                              w_{onto}*sim\_nodes\_onto(v_{1},v_{2}) +   
			                                              w_{inp}*sim\_nodes\_inp(v_{1},v_{2}) + \\
			                                             & w_{oup}*sim\_nodes\_oup(v_{1},v_{2}) + 
			                                              w_{des}*sim\_nodes\_des(v_{1},v_{2}) 
			                                              \end{array} 
			                                              \]
%
where $ w_{onto}, w_{inp}, w_{oup}, w_{des}$ are weight values assigned to each attribute, 
$w_x \in [0,1]$ ($x \in \{onto, inp, oup, des\}$) and $w_{onto} +w_ {inp} + w_{oup} + w_{des}= 1$. In our 
implementation, the values of $w_{onto}, w_ {inp} , w_{oup}, w_{des}$ are $0.6, 0.15, 0.15$ and $0.1$ 
respectively. The intuition behind these values lies in the fact that within an ontology, the similarity between objects depends heavily on their relative position to their lowest common ancestor; thus $w_{onto}$ should play the deciding factor. $ w_ {inp} = w_{oup}$ because of the symmetry between inputs and outputs. 
$w_{des}$ is smaller than $ w_ {inp}$ or $w_{oup}$ because our current ontology has different levels of detail in the 
English description of services.
%
%{\color{red} 
%In which, the contribution of {\tt \small sim\_nodes\_onto} value is dominant because it is based on Ontology semantic and therefore should be associated
%with a high weight value (0.6). At the moment, the descriptions of services have not completed yet in Ontology (some of them are updating and text of others 
%than that are not clear enough for matching) so the value of  {\tt \small sim\_nodes\_des} is a less important 
%attribute and associated with a low weight value (0.1).     
%}

%\todo[inline]{Provide intution of why the values of weights were chosen like that}

%\comment{Thanh: what values do you use for $w_{onto}$ ... in the implementation? } 
%
Finally, \textbf{sim\_nodes\_workflows($G_1,G_2$)} is defined as follows:
		   \begin{equation} \label{simnode}
		   sim\_nodes\_workflows(G_1,G_2) = 2 * \frac{\sum_{v_1 \in V_1} \sum_{v_2 \in V_2} sim\_nodes(v_{1}, v_{2})}{|V_1| + |V_2|}\end{equation}
	

\item {\bf Edge Similarity.} Let $e_1 \in E_1$ and $e_2 \in E_2$. 
As an edge connected two services (nodes) in a workflow and denotes an exchange between two nodes (output of one is input of the other). 
For this reason, the similarity between two edges can be defined via the similarity between the nodes relating to them and the descriptions of their 
inputs and outputs. For an edge $e$, let $s(e)$ and $d(e)$ denote the source and destination of $e$, respectively; 
Furthermore, let $lab(e) = (o_{s(e)},i_{d(e)})$, where $o_{s(e)}$ is the
output of $s(e)$ that is used as the input $i_{d(e)}$ of $d(e)$. We define:
%
%
%%Let $s(e)$ and $d(e)$ denote the source and destination node of an edge $e$, respectively. 
%
%
%%Let $io(e)$ denote the set of inputs and outputs of an edge $e$. We define - comments Dr Son
%{\color{red}The label of an edge is defined by the  resource being exchanged between 
%the source and destination node. Let $lab(e)$ denote the label of an edge $e \in E$ and $lab(e) = (o_s,i_d)$ where $o_s$ is output  
%resource of source node and $i_d$ is input resource of destination node in $e$ }. We define:
%%
% 
%we defined an edge e as 4-tuple $e = (v^{s},o^{s},i^{d},v^{d})$ in which, $v^{s}, v^{d}$ denote source and destination nodes of edge e and $o^{s}, i^{d}$ denote the exchange resources between two services of classes $v^{s}, v^{d}$. It means output component $o^{s}$ of  $v^{s}$ will be mapped with input $i^{d}$ of $v^{d}$. For any arbitrary edge $e_{1p}$ in $G_1$ and $e_{2q}$ in $G_2$, we have $e_{1p} = (v^{s}_{1p},o^{s}_{1p},i^{d}_{1p},v^{d}_{1p})$ and  $e_{2q} = (v^{s}_{2q},o^{s}_{2q},i^{d}_{2q},v^{d}_{2q})$. We defined some functions as following:



%Suppose that, $E_1 = (e_{11},e_{12}, ... , e_{1h})$, 
%$E_2 = (e_{21},e_{22}, ... , e_{2k})$, and $e_{1p}$, $e_{2q}$ are arbitrary edges in $G_1$ and $G_2$ with $1 <= p <= h$, $1 <= q <= k$. We defined an edge e as 4-tuple $e = (v^{s},o^{s},i^{d},v^{d})$ in which, $v^{s}, v^{d}$ denote source and destination nodes of edge e and $o^{s}, i^{d}$ denote the exchange resources between two services of classes $v^{s}, v^{d}$. It means output component $o^{s}$ of  $v^{s}$ will be mapped with input $i^{d}$ of $v^{d}$. For any arbitrary edge $e_{1p}$ in $G_1$ and $e_{2q}$ in $G_2$, we have $e_{1p} = (v^{s}_{1p},o^{s}_{1p},i^{d}_{1p},v^{d}_{1p})$ and  $e_{2q} = (v^{s}_{2q},o^{s}_{2q},i^{d}_{2q},v^{d}_{2q})$. We defined some functions as following:

\begin{list}{$\circ$}{\itemsep=0pt \topsep=1pt \parsep=1pt \leftmargin=10pt}
	\item \texttt{sim\_ed\_nod($e_{1},e_{2}$)}: measures the similarity of two edges by considering the similarity of the nodes related to the edges and is defined by \\
%	measures the similarity between $e_{1p}$ and $e_{2q}$ based on the similarity index of source and destination nodes of two edges.
			$sim\_ed\_nod(e_{1},e_{2}) = \frac{1}{2}*(sim\_nodes(s(e_1),s(e_2)) + sim\_nodes(d(e_{1}), d(e_{2})))$.
%	\item \texttt{sim\_edges\_nodes($E_{1},E_{2}$)}: measures the similarity of two set of edges by considering  all single similarity \texttt{sim\_ed\_nod($e_{1},e_{2}$)}.
%	measures the similarity between $e_{1p}$ and $e_{2q}$ based on the similarity index of source and destination nodes of two edges.
%			\[sim\_edges\_nodes(E_{1},E_{2}) = 2*\frac{\sum_{e_1\in E_1} \sum_{e_2 \in E_2} sim\_ed\_nod(e_{1},e_{2})}{|E_1|+|E_2|}\]  
 
	\item \texttt{sim\_ed\_re($e_1,e_2$)}:  measures the similarity of two edges by considering distance between their labels
	and is defined by \\
%	Assume that we have two edges $e_1, e_2$ and their labels are $lab(e_1) =(o_s^1,  i_d^1)$ and $lab(e_2) =(o_s^2,  
%	i_d^2)$, 
			$sim\_ed\_re(e_{1},e_{2}) = \frac{1}{1 + d\_ed\_ont(lab(e_{1}),lab(e_{2}))}, \:\: \textnormal{where}$ 
	            $
                      d\_ed\_ont(lab(e_{1}),lab(e_{2})) = \frac{1}{2} *(d\_nodes\_onto(o_{s(e_1)} ,o_{s(e_2)}) + d\_nodes\_onto(i_{d(e_1)},i_{d(e_2)}))                                                                    
                      $.	
			
			 
%          {\color{red}
%	\item \texttt{sim\_ed\_re($e_1,e_2$)}:  measures the similarity of two edges by considering distance between their labels. 
%	Assume that we have two edges $e_1, e_2$ and their labels are $lab(e_1) =(o_s^1,  i_d^1)$ and $lab(e_2) =(o_s^2,  
%	i_d^2)$,
%%	 measures the similarity between two sets of edges labels of  $E_1$ and $E_2$. We defined that label of an edge $e = (v^{s},o^{s},i^{d},v^{d})$ is a 2-tuple $(o^{s},i^{d})$ and $label(E_1)$, $label(E_2)$ are sets of edges labels of $E_1$ and $E_2$ respectively. 
%			%\[sim\_ed\_io(E_1,E_2) = \sum_{e_1 \in E_1} \sum_{e_2 \in E_2}2*\frac{| io(e_1) \cap io(e_2) |}{| io(e_1) | + | io(e_2) |}\]
%			%\[sim\_ed\_io(e_1,e_2) = 2*\frac{| io(e_1) \cap io(e_2) |}{| io(e_1) | + | io(e_2) |}\]
%			\[sim\_ed\_re(e_{1},e_{2}) = \frac{1}{1 + d\_ed\_ont(lab(e_{1}),lab(e_{2}))}, \:\: \textnormal{where}\]
%	              \[\begin{array}{ll} 
%                      d\_ed\_ont(lab(e_{1}),lab(e_{2})) = \frac{1}{2} *(d\_nodes\_onto(o_s^{1},o_s^{2}) + d\_nodes\_onto(i_d^{1},i_d^{2}))                                                                    
%                        \end{array} \]		
%			%\[sim\_ed\_io(E_1,E_2) =2*\frac{| label(E_1) \cap label(E_2) |}{| label(E_1) | + | label(E_2) |}\]
%				%\todo[inline]{one question is why don't we compare the resource being exchanged as similarity in the ontology, just as we did for nodes}
% %       \item \texttt{sim\_edges\_res($E_{1},E_{2}$)}: measures the similarity of two set of edges by considering  all single similarity \texttt{sim\_ed\_re($e_{1},e_{2}$)}.
%%	measures the similarity between $e_{1p}$ and $e_{2q}$ based on the similarity index of source and destination nodes of two edges.
%%			\[sim\_edges\_res(E_{1},E_{2}) = 2*\frac{\sum_{e_1\in E_1} \sum_{e_2 \in E_2} sim\_ed\_re(e_{1},e_{2})}{|E_1|+|E_2|}\]  
%         }
\end{list}
%
The similarity between two edges, denoted by \textbf{sim\_edges($e_{1},e_{2}$)}, is then defined as the weighted sum between their two similarity 
measures.
$sim\_edges(e_1,e_2) =  w_{node} * sim\_ed\_nod(e_{1},e_{2})  + w_{label}*sim\_ed\_re(e_1,e_2)$. 
where $0 \le w_{node}, w_{label}\le 1$ are weight values of corresponding attributes contributions such that 
$w_{node} + w_{label} = 1$. In our implementation, we assign the values of $w_{node}, w_{label}$ are $0.5$ and $0.5$ respectively.
We define $$sim\_edges\_workflows(G_1,G_2) = 2 * \frac{\sum_{e_1\in E_1} \sum_{e_2 \in E_2} sim\_edges(e_{1},e_{2})}{|E_1|+|E_2|}.$$
			                                              
 

\item {\bf  Topological Similarity.} 
By considering workflows as graphs, we can also consider their similarity based on the amount of changes needed to convert one to the other. The notion 
of an \emph{Edit-Distance}, denoted by $dist\_topo(G_1,G_2)$, between two graphs---the smallest number of changes (insertions, deletions, 
substitutions, etc.) required to transform one structure to another---has been introduced and algorithms for computing it have been 
developed by \citeN{ZhangS89}. The topological similarity between two graphs is defined by  
          $sim\_topo(G_1,G_2) =  \frac{1}{1 + dist\_topo(G_1,G_2)}$.

 
\end{list}   
  

Having defined various types of similarities between elements of the workflows, we can now define the similarity between two workflows as a weighted sum of these similarities:  
  \[\begin{array}{ll} 
  sim\_workflows(G_1,G_2) =  & w_{no}*sim\_nodes\_workflows(G_1,G_2) +  \\
   & w_{ed}*sim\_edges\_wf(G_1,G_2) + w_{to}*sim\_topo(G_1,G_2)
  \end{array} \]
 where  $w_{no}, w_{ed}, w_{to} \in [0,1]$ and  
 $w_{no} + w_{ed} + w_{to} = 1$. 
  In our implementation, we use  $w_{no}=0.45, w_{ed}=0.35$  and $w_{to}=0.2$. Here, the emphasis is still the similarity between nodes. This is because the nodes are the main components of a workflow. 
  For this reason, we place greater emphasis  on the edges than the topology of a workflow, because the labels of the edges are also critical to the workflow. 
Due to the fact that the computation of the similarity between two workflows is deterministic,  deals mostly with real numbers,
 and the fact that new answer set solvers allow for the integration of external atoms as described by \cite{KaminskiSW17}, 
we implemented a package for computing the similarity between two workflows as a Python library and used this as external predicates in ASP. 

 
%
\subsection{ASP-Code for Replanning}
\label{sub:asp_preferences_desires}

Given a workflow and some modifications requested by a user, the similarity measure introduced in the previous subsection can be used to select the 
workflow that satisfies the user's requests and is most similar to the original one. 
We next present the ASP implementation addressing each of the change categories discussed at the beginning of this section and then selecting the 
\emph{most similar workflow}. 
We will use a simple original workflow for generating a \emph{gene-species reconciliation tree} from \emph{a set of gene names} (Figure~\ref{original_workflow}) as a running example. In this workflow,  each node (circle) represents a service (e.g., $a$ is a service) and each connection between two nodes represent an edge with its label 
(e.g., the edge from $a$ to $b$ has $oa$ as an output of $a$ which is used as input $ib$ of $b$). Here,
$a$, $b$, $c$, $d$, $e$ and $f$ are the short name for the services  \emph{Get\_GeneTree\_from\_Genes, Ext\_Species\_from\_GeneTree, Resolved\_Names\_OT, Get\_PhyloTree\_OT\_V1, GeneTree\_Scaling\_V1} and 
\emph{Get\_ReconciliationTree} respectively. 
 %
\begin{figure*}[h]
		\centerline{\includegraphics[width=0.65\textwidth]{simple_original_workflow}}
	\caption{Original workflow}
	\label{original_workflow}
\end{figure*}
%
 
Intuitively, the workflow in Figure~\ref{original_workflow} represents a workflow generated by the planning engine, encoded   by the set of atoms: 
\begin{align*} 
 \{initially(init\_res,dfi),occ(a,1), occ(b,2),occ(c,3), occ(d,4), 
  occ(e,5),occ(f,6), \\ finally(goal\_res,dfg), map(a,ia,1,init\_res,0), 
 map(b,ib,2,oa,2), map(c,ic,3,ob,3), \\ 
 map(d,id,4,oc,4), 
 map(e,ie,5,oa,2), map(f,if1,6,od,5), map(f,if2,6,oe,6).\}
\end{align*} 
In this encoding, $initially/2$ ($finally/2$) states that the input (goal) with its data type; $occ(x,i)$ states that the service $x$ must occur at the step 
$i$; $map(s, i, t1, o, t2)$ says that an output $o$ of step $t2$ is an input $i$ of service $s$ at step $t1$.
\subsubsection{IO Request}

%If users want to change resources in input and/or desired output, ASP encoding will be updated relative facts \texttt{\small initially} and/or \texttt{\small finally} -- e.g. in this example,  input resource is changed from \emph{init\_res} to \emph{new\_init\_res} and/or from \emph{goal\_res} to \emph{new\_goal\_res} for output then facts are updated as \texttt{\small initially(new\_init\_res,new\_dfi)} and/or \texttt{\small finally(new\_goal\_res,new\_dfg)}. The planning engine will be executed with new updates of input and/or output data and generate new workflow(s).


For an IO request, all that needs to be changed is the ASP encoding sent to the ASP-planner, specifically atoms of the form \texttt{\small initially} and/or \texttt{\small finally} will be updated to reflect the request. The planning engine will be executed and returned the most similar workflow to the current one.  

%\todo[inline]{don't understand. where is the maximum similarity included?}

%If users want to change resources in input and/or desired output, ASP encoding will be updated relative facts \texttt{\small initially} and/or \texttt{\small finally} -- e.g. in this example,  input resource is changed from \emph{init\_res} to \emph{new\_init\_res} and/or from \emph{goal\_res} to \emph{new\_goal\_res} for output then facts are updated as \texttt{\small initially(new\_init\_res,new\_dfi)} and/or \texttt{\small finally(new\_goal\_res,new\_dfg)}. The planning engine will be executed with new updates of input and/or output data and generate new workflow(s).


\subsubsection{Avoidance Request}
%{\bf  Request avoidance of one service.} 

A request to avoid using a service $s$ (or a class of services $c$) will be translated to the atom $do\_not\_use(s)$ 
($do\_not\_use(c)$) and supplied to the planning engine. 
We use the following ASP-rules to enforce this request:  
%
\begin{align}
  is\_used(C)\: {:}{-} \:\:\:\: member(X, C), \: occ(X, \_). \label{r1}\\
  is\_used(X)\: {:}{-} \:\:\:\: occ(X, \_). \label{r3}\\  
  {:}{-} \:\:\:\: is\_used(X), \: do\_not\_use(X). \label{r3}
\end{align}
%
The first two rules determine the class of service \texttt{C} or the service $X$ is used in the workflow. 
The third enforces the request of the user to avoid the use of the service or a class of services.  
Some possible resulting workflows satisfying the request $do\_not\_use(d)$ are shown in Figure \ref{avoid_service_wf}.
%
%The first rule determines that class of service \texttt{C} is occurred and used in the workflow. The second rule is a constraint to guarantee that a service of class has never been involved in workflow -- e.g., if \texttt{do\_not\_use(d)} is specified, service of class \texttt{d} will no longer allow to occur in new workflow. Figure \ref{avoid_service_wf} presents some cases of updated workflows.
%
\begin{figure*}[t]
		\centerline{\includegraphics[width=0.95\textwidth]{avoid_service_wf}}
	\caption{Some updated workflows with avoidance the service $d$}
	\label{avoid_service_wf}
\end{figure*}
%
\subsubsection{Inclusion Request} 
%{\bf  Request inclusion of one service.} 
A request to include a service $s$ (or a class of services $c$) will be translated to the atom $must\_used(s)$ 
($used(c)$) and supplied to the planning engine. The rules \eqref{r1}-\eqref{r3}  are used with  
the rule:  
%
%The constraint rules in planning engine for inclusion of service(s) are described as follows: \\
%
\begin{align} 
  {:}{-} \:\:\:\: must\_used(X), \: not \:\: is\_used(X).  \label{r4}
\end{align} 
to make sure that the request to include some service is satisfied.  
Figure \ref{include_service_wf} displays some possible updated workflows with the request of using \textbf{(1)} \texttt{\small e2} (\emph{GeneTree\_Scaling\_V2}) 
or  \textbf{(2)} \texttt{\small e3} (\emph{GeneTree\_Scaling\_V3})
%
\begin{figure*}[h]
		\centerline{\includegraphics[width=1.0\textwidth]{include_service_wf}}
	\caption{Updated workflows with inclusion request of \texttt{\small e2} or \texttt{\small e3}}
	\label{include_service_wf}
\end{figure*}
%
\subsubsection{Insertion Request} 
%% {\bf  Insertion of one service/class in an arbitrary point.} 
This is a special case of an inclusion request. Specifically, it specifies where the service should be included. This request is translated into the ASP atoms of the form $before(x,y)$ (service $x$ must be executed before service $y$) and $must\_used(x)$. To implement this, we add to the 
%
%\todo[inline]{this is different than requesting the service at a particular point; probably there should be not only
before but also some after statements}
\begin{align}
  is\_before(C,D)\: {:}{-} \:\:\:\:  occ(C,T), \: occ(D,T_1), \: T<T_1. \label{r5} \\
  {:}{-} \:\:\:\: before(C,D), \: not \: \: is\_before(C,D). \label{r6} 
\end{align} 
Figure \ref{ordered_serives_wf} shows some workflows accommodating the request ``\emph{use service $e$ after $a$ and before $b$}'', encoded by
$ \{must\_used(e), \:  before(a,e), \: before(e,c).\}$.   
%This change is a particular case of request inclusion of one service of class. In this case, a service is inserted to selected point in the current workflow which means it has to be in order with specified previous and next nodes. The ASP encoding is implemented as follows: 
%\texttt{\small
%\hspace*{.5cm} is\_ordered(C,D):- op\_cl(C),occ(C,T),op\_cl(D),occ(D,T$_1$),T$<$T$_1$.\\
%\hspace*{.5cm} :- before(C,D), not is\_ordered(C,D). \\
%\hspace*{.5cm} used(C), used(D) :- is\_ordered(C,D). \\
%}
%These rules guarantee that a service of class \texttt{C} must be used before a service of class \texttt{D} if \texttt{before(C,D)} is specified.--e.g., if users want to insert service of class \texttt{x} between \texttt{a} and \texttt{c} in our example, the encoding should be \{\texttt{\small must\_used(x). before(a,x). before(x,c).}\}. Figure \ref{ordered_serives_wf} shows some possible cases of this change.
%
\begin{figure*}[h]
		\centerline{\includegraphics[width=1.0\textwidth]{ordered_serives_wf}}
	\caption{Service e is executed after a and before c}
	\label{ordered_serives_wf}
\end{figure*}
%
%
\subsubsection{Selecting a Most Similar Workflow}

Let $\Pi_R$ be the set of rules \eqref{r1}-\eqref{r6} for enforcing the requests of the users and $C$ denote the set of atoms encoding the requests of an user. Our goal is to compute a new workflow that satisfies $C$ and is as  similar as possible to the original workflow. It is easy to see that $\Pi_L \cup \Pi_R \cup C$ will return workflows satisfying $C$. As such, we only need to identify, among all solutions provided by $\Pi_L \cup \Pi_R \cup C$, the workflow that is most similar to the original workflow. This can be achieved by encoding the original workflow, providing it as input, and exploiting the Python package for computing the similarity of two workflows (subsection~\ref{sub:similarity_lib}). 
To do so, let us assume that the original workflow is encoded  as a set of atoms $O$ of the form $old\_occ(s,t)$ and 
$old\_map(s, i, t_1, o, t_2)$. There are different possibilities here: 

\begin{list}{$\bullet$}{\itemsep = 0pt \parsep=1pt \topsep=1pt \leftmargin=10pt} 
\item {\bf Computing exact value of similarity using ASP}: theoretically, the exact value of similarity between the constructed workflow and the original one can be computed in the ASP using an external call to $sim\_workflows$ and the most similar workflow can be computed using the 
$\mathtt{maximize}$ statement by 
% 
\begin{align} 
sim(V)\: {:}{-}\:\:\:\: V = @sim\_workflows(wf_1, wf_2). \\
\# \mathtt{maximize} \{V : sim(V)\}. 
\end{align}
%
where $wf_1$ and $wf_2$ are two sets of atoms encoding the two workflows (the original and the computed one), this is essentially the sets 
of atoms of the forms $old\_occ(.)$, $old\_map(.)$, $occ()$, and $map()$ that is generated by the planning engine and supplied in $O$.  
This means that we need a set data structure to implement this approach. We did not implement this due to the fact that  
{\bf clingo} does not yet provide a construct for set. We hope to work with the {\bf clingo} group to introduce set as a basic data type for use besides the aggregate functions. 

%this approach is not practical. In our experiments, the approach takes a significant amount
%of time---please see next section for more details.
%
%\todo[inline]{very unclear what the params are; please give more details, it is not sufficient that is not practical}
% We experimented with this approach and found that the approach is 

\item {\bf Approximating the value of similarity using ASP}: Since the set of facts of the form $occ(s,t)$ correspond to the nodes in the workflow, we can approximate the similarity of  two workflows by considering only the similarity between nodes of the two workflows (old and new). This can be realized by the following rules: 
\begin{align*} 
single\_sim\_nodes(X,Y,Z)\:{:}{-}\:\:\:\: old\_occ(X,I_1),occ(Y,I_2), Z=@sim\_nodes(X,I_1,Y,I_2).\\
sum\_sim\_nodes(S)\: {:}{-}\:\:\:\: S = \#sum\{Z,X,Y : single\_sim\_nodes(X,Y,Z)\}. \\
sim\_nodes\_workflows(R)\: {:}{-}\:\:\:\: sum\_sim\_nodes(S), NO =  \#count \{X,I_1: old\_occ(X,I_1)\}, \\
 NN = \#count\{Y,I_2: occ(Y,I_2)\}, R = 2*S/(NO + NN). \\
 \#\mathtt{maximize}\{R : sim\_nodes\_workflows(R)\}. 
\end{align*} 
It is easy to see that the above rules implement formula \eqref{simnode}.  

\item {\bf Computing exact value of similarity using multi-shot ASP}: 
This approach has been implemented using the multi-shot ASP \cite{KaminskiSW17}. Basically, a Python wrapper is used to control the search for the most similar workflow to the original one. It implements the following loop: \\
\texttt{\footnotesize
\hspace*{.25cm}  \textbf{for} each answer set of $\Pi_L \cup \Pi_R \cup C$ \\ 
\hspace*{.5cm} compute the similarity $v$ of the solution and the original workflow \\
\hspace*{.5cm}  \textbf{if} it is greater than the current value (initiated with -1) \\
\hspace*{.5cm} \textbf{then} keep the solution 
}




\end{list} 
%
\subsection{Refining a Workflow: Case-Studies}
%
We illustrate the new features of our system using three use cases developed in the Phylotastic project. 

\subsubsection{From a Set of Gene Names to a Reconciliation Tree in \texttt{\small newick} Format} 
In this use case, the user wants to generate a 
phylogenetic reconciliation tree in the \texttt{\small newick} format from a set of gene names, whose format is 
\texttt{\small set\_of\_strings}. % {\color{red}  
The user can use the  {\em Workflow Configuration Module} to design an initial workflow with two nodes (initial node and goal node) 
with this information as described by \citeN{NguyenSP18}. The module will then convert this information into ASP-atoms \texttt{\small initially(setOfGeneNames,set\_of\_strings)} and \texttt{\small finally(reconciliationTree,newickTree)}, which will be sent 
the planning engine.  
The result is the workflow shown in Figure \ref{eval_original_workflow}, where the triangles identify the input and output and the name of the service that should be executed at each step. For example, a \texttt{\small gene\_based\_extraction} service should be executed at step 1 and a \texttt{\small names\_extraction\_tree} service needs to be executed at step 2. The workflow shown in the figure is also the one with highest QoS (1.4224).

\begin{figure}[h]
	\centering	\includegraphics[width=\textwidth]{eval_original_workflow}
	\caption{Original Workflow \label{eval_original_workflow}}
\end{figure}
%

The user, after examining  the workflow,  determines that the input \texttt{\small GeneTree} of \texttt{\small tree\_reconciliation} service needs to be scaled before being processed by \texttt{\small tree\_reconciliation}; 
and, this requires that the service \texttt{\small gene\_tree\_scaling} should be inserted after \texttt{\small gene\_based\_extraction} and before  \texttt{\small tree\_reconciliation} service. 
The experimentation has been performed on a machine running MacOS 10.13.3 with 8GB DDRam 3 and a 2.5GHZ Intel-Core i5 (3rd Generation)
with 54 classes of services and 125 concrete specific services in Ontology domain. %}

\begin{list}{$\bullet$}{\itemsep = 0pt \parsep=1pt \topsep=1pt \leftmargin=10pt}  

\item {\bf Approximating the value of similarity using ASP}: Using this approach, there are different updates to the original workflow. 
Four of them with the highest similarity values are displayed in Figure \ref{eval_original_workflow_app_2_3}. The highest similarity value of 0.5779 comes 
from  \texttt{\small$WF_2$} and \texttt{\small$WF_3$}. Observe that these workflows have only one modification (\texttt{\small gene\_tree\_scaling} 
occurs at step 4 in \texttt{\small$WF_2$} and step 1 in \texttt{\small$WF_3$}). Furthermore, \texttt{\small$WF_1$} has two changes 
(adding \texttt{\small gene\_tree\_scaling} and replacing \texttt{\small taxonomy\_based\_extraction}  by \texttt{\small phylogeny\_based\_extraction}); 
and \texttt{\small$WF_4$} has three changes. The total processing time of this approach is $35.183$ seconds.
 

\item {\bf Computing exact value of similarity using multi-shot ASP}: The previous approach only approximates the similarity values of the new solutions 
and the original workflow. Using the multi-shot ASP, we can calculate the exact value of the similarity. Figure \ref{eval_original_workflow_app_2_3} shows 
the same 4 workflows considered in the previous approach with their exact value of similarity. The clear winner is $WF_2$. The 
system takes $37.937$ seconds to compute these updates.
\begin{figure}[t]
	\centering	\includegraphics[width=\textwidth]{eval_original_workflow_app_2_3}
	\caption{Possible updated workflows by 2 approaches  \label{eval_original_workflow_app_2_3}}
\end{figure}
\end{list}   

 
\subsubsection{Generating a Species Tree in  \texttt{\small newick} Format from Free Plain-Text}

The second use case is concerned with the generation of a species tree in  \texttt{\small newick} format from a text document  (\texttt{\small plain\_text} format). Figure \ref{user_case_2_experiement} (Workflow 0) displays the workflow with the highest QoS value. The first revision that an user asked is the system is to avoid using the \texttt{\small Get\_PhyloTree\_Phylomatic\_V2} service. The resulting most similar workflow is displayed in 
Figure \ref{user_case_2_experiement} (Workflow 1) in which the service is replaced by the service \texttt{\small Get\_PhyloTree\_OT\_V2}. However, for this service to be used, the service \texttt{\small convert\_taxa\_GNR\_to\_Phylomatic} must be replaced with \texttt{\small convert\_taxa\_GNR\_to\_OT\_format}. The second revision was to force the use of the service  \texttt{\small Resolved\_Names\_GNR\_V2}, the third workflow Figure \ref{user_case_2_experiement} (Workflow 2) satisfies this request. 

\begin{figure}[t]
	\centering	\includegraphics[width=\textwidth]{user_case_2_experiement}
	\caption{Generating a Species Tree from Free Text  \label{user_case_2_experiement}}
\end{figure}


%In the use case \textbf{(1)}, the users want to generate a species tree in the \texttt{\small newick} format from a text document (\texttt{\small plain\_text} format). The result of the original workflow is shown in the first block in Figure \ref{user_case_2_experiement} with the highest QoS value. In this use case, users can request many updates to change the workflow. The first possible change is a request to avoid all version of \texttt{\small Get\_Phylo\_Tree\_Phylomatic} in the workflow and the result workflow of this change is presented in the second block in Figure \ref{user_case_2_experiement}, in which service \texttt{\small Get\_PhyloTree\_Phylomatic\_V2} has to be replaced by \texttt{\small Get\_PhyloTree\_OT\_V2} and some related services such as \texttt{\small convert\_taxa\_GNR\_to\_Phylomatic} is changed as well. In addition, if users want to keep results of the first change and continue updating with a request to use \texttt{\small Resolved\_Names\_GNR\_V2} then this service can be included in the new workflow as presented in the third block of Figure \ref{user_case_2_experiement}.

\subsubsection{Generating a Chronogram from a Web-site Content}

A \texttt{\small chronogram} is a scaled species tree with branch lengths. In this use case, 

\begin{list}{$\bullet$}{\itemsep=0pt \parsep=1pt \topsep=1pt} 

\item An user has an initial request to create a chronogram in the \texttt{\small newick} format and meta-data of this tree in the \texttt{\small set\_of\_strings} format from a web-site content, which is specified by a URL (\texttt{\small http\_url} format), and a name of scaling method (\texttt{\small string} format). The workflow  generated by the planning engine is shown in Figure \ref{user_case_3_experiement} (Workflow 0).
In this workflow, the output components \texttt{\small chronogram} and \texttt{\small meta-data} are produced by services \texttt{\small Get\_Chronogram\_ScaledTree\_DL\_V2} and \texttt{\small Get\_MetaData\_Chronogram\_DL\_V2} respectively. Both of them use \texttt{\small species\_tree} as an input and this resource is generated by service \texttt{\small Get\_PhyloTree\_PhyloT\_V2} in previous step. 

\item The user then requests that the services \texttt{\small Get\_PhyloTree\_OT\_V2} and \texttt{\small Resolved\_Names\_OT\_V2} have to be used.  
The resulting most similar workflow is presented in Figure \ref{user_case_3_experiement} (Workflow 1) in which services \texttt{\small Resolved\_Names\_GNR\_V1}, \texttt{\small convert\_taxa\_GNR\_to\_phyloT} and \texttt{\small Get\_PhyloTree\_PhyloT\_V2} are replaced by \texttt{\small Resolved\_Names\_OT\_V2}, \texttt{\small Get\_PhyloTree\_OT\_V2} and \texttt{\small convert\_Tree\_to\_Newick} respectively. 

\item Instead of the above request, the user requests that \texttt{\small Get\_MetaData\_Chronogram\_DL\_V2} is executed \textbf{after}   \texttt{\small Get\_Chronogram\_ScaledTree\_DL\_V2}. 
Figure \ref{user_case_3_experiement} (Workflow 2) shows the most similar workflow to Workflow 0 that satisfies this request. 
In this workflow,  \texttt{\small Get\_MetaData\_Chronogram\_DL\_V2} will use the output of 
\texttt{\small Get\_Chronogram\_ScaledTree\_DL\_V2} as its input instead of the output from  \texttt{\small Get\_PhyloTree\_PhyloT\_V2}.

%In addition, \texttt{\small Get\_MetaData\_Chronogram\_DL\_V2} uses resource \texttt{\small species\_tree\_branch\_lengths} which is produced by \texttt{\small Get\_Chronogram\_ScaledTree\_DL\_V2} in the previous step as an input (\texttt{\small species\_tree\_branch\_lengths} is sub-class of \texttt{\small species\_tree}).  
%In this case, \texttt{\small Get\_MetaData\_Chronogram\_DL\_V2} will use \texttt{\small species\_tree\_branch\_lengths} from \texttt{\small Get\_Chronogram\_ScaledTree\_DL\_V2} as an input  instead of \texttt{\small species\_tree} from \texttt{\small Get\_PhyloTree\_PhyloT\_V2}. The third workflow in Figure \ref{user_case_3_experiement} (Workflow 2) satisfies this request.

\end{list} 

%Given a web-site address (\texttt{\small http\_url} format) and a name of scaling method (\texttt{\small string} format), the goal of the use case \textbf{(2)} is to acquire a chronogram (a scaled species tree with branch lengths) in the \texttt{\small newick} format and meta-data of this tree in the \texttt{\small set\_of\_strings} format. With these inputs and outputs, the original workflow is generated by planning engine as shown in the first block of Figure \ref{user_case_3_experiement}. In the original workflow, the output components \texttt{\small chronogram} and \texttt{\small meta-data} are produced by services \texttt{\small Get\_Chronogram\_ScaledTree\_DL\_V2} and \texttt{\small Get\_MetaData\_Chronogram\_DL\_V2} respectively. Both of them use \texttt{\small species\_tree} as an input and this resource is generated by service \texttt{\small Get\_PhyloTree\_PhyloT\_V2} in previous step. However, some users want to change the workflow as following: service \texttt{\small Get\_MetaData\_Chronogram\_DL\_V2} has to be executed \textbf{after} service \texttt{\small Get\_Chronogram\_ScaledTree\_DL\_V2} and use output \texttt{\small species\_tree\_branch\_lengths} of this service as an input (\texttt{\small species\_tree\_branch\_lengths} is sub-class of \texttt{\small species\_tree}). In this case, \texttt{\small Get\_MetaData\_Chronogram\_DL\_V2} consumes \texttt{\small species\_tree\_branch\_lengths} from \texttt{\small Get\_Chronogram\_ScaledTree\_DL\_V2}  instead of \texttt{\small species\_tree} from \texttt{\small Get\_PhyloTree\_PhyloT\_V2}. The result workflow of this change is shown in the third block of Figure \ref{user_case_3_experiement}.





%A \emph{Chronogram} is a scaled species tree with branch lengths. In the third use case, an user has an initial request to create a chronogram from a web-site content. Given a web-site address (\texttt{\small http\_url} format) and a name of scaling method (\texttt{\small string} format), the goal of the use case \textbf{(2)} is to acquire a chronogram (a scaled species tree with branch lengths) in the \texttt{\small newick} format and meta-data of this tree in the \texttt{\small set\_of\_strings} format. With these inputs and outputs, the original workflow is generated by planning engine as shown in the first block of Figure \ref{user_case_3_experiement}. 
%In the original workflow, the output components \texttt{\small chronogram} and \texttt{\small meta-data} are produced by services \texttt{\small Get\_Chronogram\_ScaledTree\_DL\_V2} and \texttt{\small Get\_MetaData\_Chronogram\_DL\_V2} respectively. Both of them use \texttt{\small species\_tree} as an input and this resource is generated by service \texttt{\small Get\_PhyloTree\_PhyloT\_V2} in previous step. 
%However, some users want to change the workflow as following: service \texttt{\small Get\_MetaData\_Chronogram\_DL\_V2} has to be executed \textbf{after} service \texttt{\small Get\_Chronogram\_ScaledTree\_DL\_V2} and use output \texttt{\small species\_tree\_branch\_lengths} of this service as an input (\texttt{\small species\_tree\_branch\_lengths} is sub-class of \texttt{\small species\_tree}). In this case, \texttt{\small Get\_MetaData\_Chronogram\_DL\_V2} consumes \texttt{\small species\_tree\_branch\_lengths} from \texttt{\small Get\_Chronogram\_ScaledTree\_DL\_V2}  instead of \texttt{\small species\_tree} from \texttt{\small Get\_PhyloTree\_PhyloT\_V2}. The result workflow of this change is shown in the third block of Figure \ref{user_case_3_experiement}.

\begin{figure}[t]
	\centering	\includegraphics[width=\textwidth]{user_case_3_experiement}
	\caption{Generating a Scaled Species Tree and its Metadata from Content of a Web-site \label{user_case_3_experiement}}
\end{figure}

%{\color{red}
%\section{Related Works}
%On the theoretical side, there are several frameworks for automatic web service composition have been proposed (e.g., 
%\cite{BartalosB11,Lemos:2015,compose7,Rao:2004:SAW:2158351.2158360,13244047}). Nevertheless, none of such approaches seems to be able to fully realize the potential of web service 
%composition \cite{Lemos:2015}. In fact, none of the frameworks for web service composition mentioned in the surveys \cite{BartalosB11,Lemos:2015,compose7,Rao:2004:SAW:2158351.2158360,13244047} 
%seems to function nowadays or be publicly available.  {\tt{\small SWORD}} \cite{Ponnekanti2002}  - that is a developer toolkit for 
%services composition developed based on logic programming rule-based expert system. 
%To the best of our knowledge, this software is only a toolset for developers working with composition services and not a software platform that 
%can be used by normal end-users. In addition, {\tt{\small SWORD}} has some limitations such as semantic integration, handling services with 
%various side-effects (multiple outputs), plan optimization, re-composition with users preferences, error handling and failure recovery. As another example, 
 %{\tt{\small OWLS-XPlan}} \cite{klusch2005semantic} is a good service composition framework that allows for fast and flexible composition of OWL-S services in the semantic 
 %Web. {\tt{\small OWLS-XPlan}} converts OWL-S 1.1 services to equivalent problem and domain descriptions that 
 %are specified in the planning domain description language PDDL 2.1 and invokes an efficient AI planner {\tt{\small Xplan}} 
 %to generate a service composition plan sequence that satisfies a given goal. {\tt{\small Xplan}} extends an 
 %action based FastForward-planner with a HTN planning and re-planning component. In addition, {\tt{\small OWLS-XPlan}}  
 %allows doing a post optimization of the composed plan with respect to a set of given QoS parameters. However, in newest version (2005), 
 %this framework has not mentioned the re-composition problem with changes as well as services composition automatic execution yet.  
%}
 
  
\section{Conclusion and Future Works}

In this paper, we described two enhancements to the Phylotastic project 
that allow users to select their most preferred workflow and modify a
workflow and obtain the most similar workflow to the original one. 
In the process, we defined the notion of quality of service of a workflow 
and the notion of similarity between two workflows. We discuss their 
implementation and their use in enhancing the capabilities of the Phylotastic system. 
The proposed system is currently begin evaluated by biology researchers participating to the Phylotastic 
project. Our immediate future considerations are: {\em (i)} investigate the use of node QoS or similarity
in the ASP-planning engine to assist the computation of most preferred (or similar) workflow;
{\em (ii)} study other extensions of {\bf clingo} (e.g., clingcon) that allow a tighter integration of 
CSP and ASP in computing most preferred (or similar) workflows; 
{\em (iii)} evaluate the scalability and efficiency of the system when more web services are 
registered to Phylotastic. 
 



\bibliographystyle{acmtrans}
%%\bibliography{../../../../../bibtex/enrico,../../../../../bibtex/bibfile,../../../../../bibtex/bib2010,../../../../../bibtex/tnguyen2018}
\bibliography{./enrico,./bibfile,./bib2010,./tnguyen2018}
\label{lastpage}
\end{document}

% end of new_TLP2egui.tex
