%  LaTeX support: latex@mdpi.com
%  In case you need support, please attach all files that are necessary for compiling as well as the log file, and specify the details of your LaTeX setup (which operating system and LaTeX version / tools you are using).

% You need to save the "mdpi.cls" and "mdpi.bst" files into the same folder as this template file.

%=================================================================
\documentclass[entropy,article,
submit,
moreauthors,pdftex,10pt,a4paper]{mdpi} 

%--------------------
% Class Options:
%--------------------
% journal
%----------
% Choose between the following MDPI journals:
% actuators, admsci, aerospace, agriculture, agronomy, algorithms, animals, antibiotics, antibodies, antioxidants, applsci, arts, atmosphere, atoms, axioms, batteries, behavsci, beverages, bioengineering, biology, biomedicines, biomimetics, biomolecules, biosensors, brainsci, buildings, carbon, cancers, catalysts, cells, challenges, chemosensors, children, chromatography, climate, coatings, computation, computers, condensedmatter, cosmetics, cryptography, crystals, data, dentistry, designs, diagnostics, diseases, diversity, econometrics, economies, education, electronics, energies, entropy, environments, epigenomes, fermentation, fibers, fishes, fluids, foods, forests, futureinternet, galaxies, games, gels, genealogy, genes, geosciences, geriatrics, healthcare, horticulturae, humanities, hydrology, informatics, information, infrastructures, inorganics, insects, instruments, ijerph, ijfs, ijms, ijgi, inventions, jcdd, jcm, jdb, jfb, jfmk, jimaging, jof, jintelligence, jlpea, jmse, jpm, jrfm, jsan, land, languages, laws, life, literature, lubricants, machines, magnetochemistry, marinedrugs, materials, mathematics, mca, mti, medsci, medicines, membranes, metabolites, metals, microarrays, micromachines, microorganisms, minerals, molbank, molecules, mps, nanomaterials, ncrna, neonatalscreening, nutrients, particles, pathogens, pharmaceuticals, pharmaceutics, pharmacy, philosophies, photonics, plants, polymers, processes, proteomes, publications, recycling, religions, remotesensing, resources, risks, robotics, safety, sensors, separations, sexes, sinusitis, socsci, societies, soils, sports, standards, sustainability, symmetry, systems, technologies, toxics, toxins, universe, urbansci, vaccines, vetsci, viruses, water
%---------
% article
%---------
% The default type of manuscript is article, but can be replaced by: 
% addendum, article, book, bookreview, briefreport, casereport, changes, comment, commentary, communication, conceptpaper, correction, conferencereport, expressionofconcern, meetingreport, creative, datadescriptor, discussion, editorial, essay, erratum, hypothesis, interestingimage, letter, newbookreceived, opinion, obituary, projectreport, reply, retraction, review, sciprints, shortnote, supfile, technicalnote
% supfile = supplementary materials
%----------
% submit
%----------
% The class option "submit" will be changed to "accept" by the Editorial Office when the paper is accepted. This will only make changes to the frontpage (e.g. the logo of the journal will get visible), the headings, and the copyright information. Also, line numbering will be removed. Journal info and pagination for accepted papers will also be assigned by the Editorial Office.
%------------------
% moreauthors
%------------------
% If there is only one author the class option oneauthor should be used. Otherwise use the class option moreauthors.
%---------
% pdftex
%---------
% The option pdftex is for use with pdfLaTeX. If eps figure are used, remove the option pdftex and use LaTeX and dvi2pdf.

%=================================================================
\firstpage{1} 
\makeatletter 
\setcounter{page}{\@firstpage} 
\makeatother 
\articlenumber{x}
\doinum{10.3390/------}
\pubvolume{xx}
\pubyear{2018}
\copyrightyear{2018}
%\externaleditor{Academic Editor: name}
\history{Received: date; Accepted: date; Published: date}
\preto{\abstractkeywords}{\nolinenumbers}
%------------------------------------------------------------------
% The following line should be uncommented if the LaTeX file is uploaded to arXiv.org
%\pdfoutput=1

%=================================================================
% Add packages and commands here. The following packages are loaded in our class file: fontenc, calc, indentfirst, fancyhdr, graphicx, lastpage, ifthen, lineno, float, amsmath, setspace, enumitem, mathpazo, booktabs, titlesec, etoolbox, amsthm, hyphenat, natbib, hyperref, footmisc, geometry, caption, url, mdframed

\usepackage{amsmath,amssymb}

\newcommand{\Fo}{{\rm F}_{\rm o}}
%\newcommand{\F1}{{\rm F}_{1}}
\def\F1{{\rm F}_{1}}
\newcommand{\mem}{I_\mathrm{mem}}
\newcommand{\pred}{I_\mathrm{pred}}
\newcommand{\Nos}{I_\mathrm{nos}}
%I_{\rm ss}^{\rm nos}
\newcommand{\NosSS}{\Nos^\mathrm{ss}}
\newcommand{\phiSS}{\phi^{\mathrm{ss}}}
%\newcommand{\muSS}{\mu\rvert_{\mathrm{SS}}}
\newcommand{\kenv}{\kappa_\mathrm{env}}
\newcommand{\kenvAv}{\bar{\kappa}_\mathrm{env}}
\newcommand{\ksys}{k_\mathrm{sys}}
\newcommand{\WdissSS}{\beta \left\langle W_{\mathrm{diss}}^{ss} \right\rangle}
\newcommand{\kstar}{\ksys/\kenv}

% Get nice looking overbars
\newcommand*\xbar[1]{%
   \hbox{%
     \vbox{%
       \hrule height 0.5pt % The actual bar
       \kern0.4ex%         % Distance between bar and symbol
       \hbox{%
         \kern-0.1em%      % Shortening on the left side
         \ensuremath{#1}%
         \kern-0.1em%      % Shortening on the right side
       }%
     }%
   }%
} 

%=================================================================
%% Please use the following mathematics environments:
%  \theoremstyle{mdpi}
%  \newcounter{thm}
%  \setcounter{thm}{0}
%  \newcounter{ex}
%  \setcounter{ex}{0}
%  \newcounter{re}
%  \setcounter{re}{0}

%  \newtheorem{Theorem}[thm]{Theorem}
%  \newtheorem{Lemma}[thm]{Lemma}
%  \newtheorem{Corollary}[thm]{Corollary}
%  \newtheorem{Proposition}[thm]{Proposition}

%  \theoremstyle{mdpidefinition}
%  \newtheorem{Characterization}[thm]{Characterization}
%  \newtheorem{Property}[thm]{Property}
%  \newtheorem{Problem}[thm]{Problem}
%  \newtheorem{Example}[ex]{Example}
%  \newtheorem{ExamplesandDefinitions}[ex]{Examples and Definitions}
%  \newtheorem{Remark}[re]{Remark}
%  \newtheorem{Definition}[thm]{Definition}
%% For proofs, please use the proof environment (the amsthm package is loaded by the MDPI class).

%=================================================================
% Full title of the paper (Capitalized)
\Title{Energy Dissipation and Information Flow in Coupled Markovian Systems}

% Authors, for the paper (add full first names)
\Author{Matthew E.~Quenneville $^{1,2,}$* and David A.~Sivak $^{1,}$*}
% Authors, for metadata in PDF
\AuthorNames{Matthew E.~Quenneville and David A.~Sivak}

% Affiliations / Addresses (Add [1] after \address if there is only one affiliation.)
\address{%
$^{1}$ \quad Department of Physics, Simon Fraser University, Burnaby, British Columbia, Canada\\
%; e-mail@e-mail.com\\
$^{2}$ \quad Department of Physics, University of California, Berkeley, California, USA\\}
%; e-mail@e-mail.com}

% Contact information of the corresponding author
\corres{Correspondence: mquenneville@berkeley.edu, dsivak@sfu.ca}
%; Tel.: +x-xxx-xxx-xxxx}

% Current address and/or shared authorship
%\firstnote{Current address: Affiliation 3} 
%\secondnote{These authors contributed equally to this work.}

% Simple summary
%\simplesumm{}

% Abstract (Do not use inserted blank lines, i.e. \\) 
\abstract{
%A single paragraph of about 200 words maximum. For research articles, abstracts should give a pertinent overview of the work. We strongly encourage authors to use the following style of structured abstracts, but without headings: 1) Background: Place the question addressed in a broad context and highlight the purpose of the study; 2) Methods: Describe briefly the main methods or treatments applied; 3) Results: Summarize the article's main findings; and 4) Conclusion: Indicate the main conclusions or interpretations. The abstract should be an objective representation of the article: it must not contain results which are not presented and substantiated in the main text and should not exaggerate the main conclusions.
A stochastic system under the influence of a stochastic environment is correlated with both present and future states of the environment. 
Such a system can be seen as implicitly implementing a predictive model of future environmental states. The non-predictive model complexity 
%in such a model 
has been shown to lower-bound
%a form of 
%bounded by
the
thermodynamic dissipation. 
Here we explore 
%this abstract result in concrete models to illustrate its predictions. 
these statistical and physical quantities at steady state in simple models. 
%In steady state, 
We show that 
under quasi-static driving
this model complexity 
%accounts for all of 
saturates
the dissipation.
%when the environment drives the system quasi-statically. We extend this concept 
Beyond the quasi-static limit, 
%through 
we demonstrate 
a lower bound on the ratio of this model complexity to total dissipation, that is realized 
%. The ratio of model complexity to total dissipation approaches this lower bound 
in the limit of weak driving. 
%We show that, in steady state, these results can be recast in terms of learning rates and heat dissipation.
%Currently 103 words
}

% Keywords
\keyword{work; dissipation; quasi-static; information; prediction; learning; nostalgia. 
%List three to ten pertinent keywords specific to the article, yet reasonably common within the subject discipline.
}

% The fields PACS, MSC, and JEL may be left empty or commented out if not applicable
%\PACS{J0101}
%\MSC{}
%\JEL{}

% If this is an expanded version of a conference paper, please cite it here: enter the full citation of your conference paper, and add $^\S$ in the end of the title of this article.
%\conference{}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Only for the journal Data:

%\dataset{DOI number or link to the deposited data set in cases where the data set is published or set to be published separately. If the data set is submitted and will be published as a supplement to this paper in the journal Data, this field will be filled by the editors of the journal. In this case, please make sure to submit the data set as a supplement when entering your manuscript into our manuscript editorial system.}

%\datasetlicense{license under which the data set is made available (CC0, CC-BY, CC-BY-SA, CC-BY-NC, etc.)}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Sections that are not mandatory are listed as such. The section titles given are for Articles. Review papers and other article types have a more flexible structure. 

%% Only for the journal Gels: Please place the Experimental Section after the Conclusions

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\setcounter{section}{-1} %% Remove this when starting to work on the template.
%\section{How to Use this Template}
%The template details the sections that can be used in a manuscript. Sections that are not mandatory are listed as such. The section titles given are for Articles. Review papers and other article types have a more flexible structure. For any questions, please contact the editorial office of the journal or support@mdpi.com. For LaTeX related questions please contact Janine Daum at latex-support@mdpi.com.


\section{Introduction}
%The introduction should briefly place the study in a broad context and highlight why it is important. It should define the purpose of the work and its significance. The current state of the research field should be reviewed carefully and key publications should be cited. Please highlight controversial and diverging hypotheses when necessary. Finally, briefly mention the main aim of the work and highlight the main conclusions. As far as possible, please keep the introduction comprehensible to scientists outside your particular field of research. Citing a journal paper \cite{ref-journal}. And now citing a book reference \cite{ref-book}.

Information theory has long been recognized as fundamentally linked to statistical mechanics~\cite{Jaynes:1957ua}. Perhaps most prominently, Landauer showed that information processing can require unavoidable dissipative costs~\cite{landauer1961irreversibility}; for example, bit erasure requires that some free energy be dissipated~\cite{Berut2012,Jun2014}. 

A stochastic system processes information through interaction with its environment: 
%the system gains information about the environment such that its state is correlated with that of the environment~\cite{Cheong:2011jp,Mehta:2012ji}. 
through environment-dependent dynamics the system responds to environmental changes and thereby gains information about the environment~\cite{Cheong:2011jp,Mehta:2012ji}.
%If this 
For an 
environment exhibiting temporal correlations, the system carries information about the past, present, and future environmental states.
%of the environment, 
%as well as future states. 
In this way, the system implicitly implements a predictive model of future environmental states~\cite{Still2012}.

%One way to 
One can
quantify this model's inefficiency
%of such a model is through 
by
the unnecessary model complexity: 
%additional complexity 
information the model retains about the past 
that does not aid in predicting the future. Recent work established the equivalence between this predictive inefficiency and thermodynamic inefficiency~\cite{Still2012}, providing another fundamental connection between information theory and statistical mechanics. This connection hints at 
%possible applications, such as 
a design principle for 
%synthetic 
molecular
machines operating out of equilibrium~\cite{Hess:2011:AnnuRevBiomedEng,brown2017toward}. 

These results are potentially applicable to many systems. For example, biological molecular machines generally operate far from equilibrium within highly stochastic environments. ATP synthase, a molecular machine which synthesizes adenosine triphosphate (ATP), is composed of two sub-units. The first ($\Fo$) drives the second sub-unit ($\F1$), which in turn produces ATP. The crankshaft rotation of $\Fo$ that mechanically drives $\F1$ is stochastic. In this way, $\F1$ contains an implicit prediction of future rotations of $\Fo$. In order for ATP synthesis to proceed at minimum energetic cost, the implicit model should contain little extraneous model complexity~\cite{okuno2011rotation}.
Examples like this occur throughout 
%the field of 
biology with organisms~\cite{Tagkopoulos:2008ct}, neurons~\cite{Laughlin:1981wn}, reaction networks~\cite{McGregor:2012ij}, and even potentially single molecules learning statistical patterns in their respective environments.

%In this paper, to 
To further illuminate this abstract connection between model complexity and thermodynamic dissipation, 
here
we analytically and numerically explore these statistical and physical quantities in illustrative models. 
%We 
%quantify
%explore 
We demonstrate the information learned by the system about its environment per unit energy dissipated (equivalently the ratio of dissipation during system and environmental dynamics) in the limits of quasi-static driving (Table~\ref{tab:bounds}) and weak driving~\eqref{eq:limit_smallprobs}, which forms the lower bound for generic driving. 
The dependence of these quantities on the 
%physical 
system and environmental 
parameters 
%of the system and environment, motivating 
motivates
a potential guiding principle 
%to maximize the information learned by the system about its environment per unit energy dissipated. 
for functional performance. 
%We relate the analytic and numerical results to other theoretical developments in this area.


\section{Theoretical Background}
Consider a stochastic process $\{ X_{t} | t \in \{0,\Delta t, ..., \tau-\Delta t, \tau\}\}$ representing the dynamical evolution of some environmental variable. At a given time, the environment can occupy any of the states $\mathcal{X}$. The time evolution of the environment, $X_{t}$, is governed by the transition probabilities $p(x_t|\{ x_{t'} \}_{t'=0}^{t-\Delta t}) \equiv p(X_{t}=x_t| \{ X_{t'} = x_{t'} \}_{t'=0}^{t-\Delta t})$ for $x_t,x_{t'} \in \mathcal{X}$. 
Let another stochastic process $\{ Y_{t} | t \in \{0,\Delta t, ..., \tau-\Delta t, \tau\}\}$ represent the system of interest, which can occupy states $\mathcal{Y}$. Take the dynamics of $Y_{t}$ to depend on the environmental state via the time-independent conditional transition probabilities $p(y|y',x) \equiv p(Y_{t+\Delta t}=y|Y_{t}=y',X_{t+\Delta t}=x)$, where $y, y' \in \mathcal{Y}$.
We model the evolution of these two stochastic processes using an alternating time-step pattern illustrated in Fig.~\ref{fig:timestep}. One complete time-step is composed of two sub-steps: one work step of environmental dynamics, when the environment does work on the system, followed by one relaxation step of system dynamics, when the system exchanges heat with a thermal bath maintained at temperature $T$ and inverse temperature $\beta \equiv (k_{\rm B}T)^{-1}$. 

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{time_step.png}
\caption{{\bf Discrete-time system and environmental dynamics.} 
The system $Y_t$ and environment $X_t$ alternate steps, with system evolution during relaxation steps, and environment evolution during work steps.}
\label{fig:timestep}
\end{figure}

System dynamics $Y_{t}$ obey the principle of microscopic reversibility~\cite{Chandler1987a}. Ref.~\cite{Still2012} used such a framework to study the relationship between thermodynamic and information-theoretic quantities. One prominent information-theoretic quantity is the \emph{nostalgia} $\Nos(t) \equiv \mem(t)-\pred(t)$, where the mutual information $\mem(t) \equiv I[X_{t},Y_{t}]$ \cite{cover2012elements} between the current system state and past environmental state represents the memory stored by the system about the environment, and the mutual information $\pred(t) \equiv I[X_{t+\Delta t},Y_{t}]$ between current system state and future environmental state represents the ability of the system to predict future environmental states. Ref.~\cite{Still2012} showed that
\begin{equation}
  \label{eq:ToP}
    \beta \left\langle W_{\mathrm{diss}}(t) \right\rangle = \mem(t) - \pred(t) - \beta \left\langle \Delta F_{\mathrm{neq}}^{\mathrm{relax}}(t) \right\rangle \ ,
\end{equation}
where $\left\langle W_{\mathrm{diss}}(t) \right\rangle$ is the total dissipation over the step from $t$ to $t+\Delta t$, and $\left\langle \Delta F_{\mathrm{neq}}^{\mathrm{relax}}(t) \right\rangle$ is the change in (nonequilibrium) free energy over the relaxation step from $t$ to $t+\Delta t$.
Since $\beta \left\langle \Delta F_{\mathrm{neq}}^{\mathrm{relax}}(t) \right\rangle \le 0$~\cite{Schnakenberg:1976p55305},
\begin{equation}
  \label{eq:ToPinequality}
  \beta \left\langle W_{\mathrm{diss}}(t) \right\rangle \ge \mem(t) - \pred(t) \ .
\end{equation}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Results}
%This section may be divided by subheadings. It should provide a concise and precise description of the experimental results, their interpretation as well as the experimental conclusions that can be drawn.

We explore the tightness of the bound~\eqref{eq:ToPinequality} through the ratio of nostalgia to dissipation,
\begin{equation}
\label{eq:phi}
\phi(t) \equiv \frac{\mem(t) - \pred(t)}{\beta \left\langle W_{\mathrm{diss}}(t) \right\rangle} \ .
\end{equation}
This nostalgia-dissipation ratio is bounded by $0 \le \phi(t) \le 1$ and (after substituting Eq.~(14) from \cite{Still2012}) can be interpreted as the fraction of dissipation which occurs over work steps,
\begin{equation}
\label{eq:phiwork}
\phi(t) = \frac{\left\langle W_{\mathrm{diss}}[x_t \rightarrow x_{t+\Delta t}] \right\rangle}{\left\langle W_{\mathrm{diss}}(t) \right\rangle} \ .
\end{equation}

When the environment and system reach steady state, $\phi$ can be rewritten as:
\begin{equation}
\label{eq:phi_ell}
\phiSS = \frac{\ell(t)}{-\beta \left\langle Q \right\rangle},
\end{equation}
where $\ell(t) \equiv I[X_{t+\Delta t},Y_{t+\Delta t}]-I[X_{t+\Delta t},Y_{t}]$ is a learning rate which quantifies the information gained by the system about the current environmental state~\cite{Brittain2017}. 
The denominator follows from the facts that at steady state $-\left\langle Q \right\rangle= \left\langle W \right\rangle$ (due to energy conservation) 
%at steady state, together with the fact that 
and
$\left\langle W \right\rangle=\left\langle W_\mathrm{diss} \right\rangle$~\cite{Still2012}.
%at steady state~\cite{Still2012}.
\cite{Barato2014} and \cite{hartich2016sensory} identify the ratio in Eq.~\eqref{eq:phi_ell} as an informational efficiency quantifying the rate at which the system learns about the environment, relative to the total thermodynamic entropy production. 
By considering \eqref{eq:phiwork}, these results can be recast in terms of dissipative energy flows.

In order to explore the physical implications of \eqref{eq:ToP} and \eqref{eq:ToPinequality}, we investigate the behavior of the relevant information-theoretic and thermodynamic quantities in concrete models that provide physical intuition. We initially restrict our attention to a simple environment model, consisting of two states with a constant transition rate $\kenv$.


\subsection{Alternating Energy Levels}
\label{sec:antisym}
One of the simplest possible system models with non-trivial behavior is a two-state system with dynamics described by two kinetic rates, $k_{+}$ and $k_{-}$ (Fig.~\ref{fig:statemaps}a). 
This model possesses a symmetry such that it is unchanged when both the system-state labels and environment-state labels are interchanged. Due to this symmetry, we take $k_{+} \ge k_{-}$ without loss of generality. 

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{statemaps.png}
\caption{{\bf Model kinetics.} 
States and transition rates for models with two system states and two environment states. (\textbf{a}) System equilibration rate and energy gap magnitude and environment transition rate are independent of environment state, but the direction of the energy gap switches with environment state. (\textbf{b}) System equilibration rate and energy gap vary with environment state. Environment transition rate is fixed. (\textbf{c}) System equilibration rate and energy gap and environment transition rate vary with environment state. 
}
\label{fig:statemaps}
\end{figure}

Given the constraint of detailed balance~\cite{Chandler1987a}, such a model describes a two-state system with an energy gap (normalized by temperature) $\beta \Delta E = \ln \frac{k_{+}}{k_{-}}$ that flips according to the environment state. 
System states $y^1$ and $y^2$ are separated by $\Delta E^{A}_{12}=-\Delta E$ when the environment is in state $x^A$ and $\Delta E^{B}_{12}=\Delta E$ for environmental 
state $x^B$. The characteristic rate at which the system equilibrates, and thus becomes correlated with the current environment (and decorrelated with past environmental states), is the harmonic mean of the two transition rates, 
\begin{equation}
\ksys \equiv \frac{2}{\frac{1}{k_{+}} + \frac{1}{k_{-}}} \ . 
\end{equation}
The transition ratio $\kstar$ expresses this rate relative to the environmental transition rate.
Figure~\ref{fig:nos_hm} shows the steady-state nostalgia $\NosSS$, which increases with both $\kstar$ and $\beta \Delta E$, and tends to 0 as either $\kstar$ or $\beta \Delta E$ approach 0.

\begin{figure}[H]
\centering
\includegraphics[width=0.56\textwidth]{img_nos_hmap.png}
\caption{{\bf Nostalgia increases with energy gap and system equilibration rate.} Nostalgia $\NosSS$ as a function of the energy gap $\beta \Delta E$ and transition ratio $\kstar$. ($\kenv \Delta t = 10^{-12}$.)}
\label{fig:nos_hm}
\end{figure}

The dissipation ratio $\phi(t)$ approaches a steady-state value $\phiSS$ for each choice of parameters. Figure~\ref{fig:phi_hm} shows that $\phiSS$ follows the same general trends as $\NosSS$, increasing with both energy gap magnitude $\beta \Delta E$ and transition ratio $\kstar$.

\begin{figure}[H]
\centering
\includegraphics[width=0.56\textwidth]{img_phi_hmap.png}
\caption{{\bf 
%Steady-state 
Dissipation ratio increases with energy gap and system equilibration rate.} Steady-state dissipation ratio $\phiSS\equiv\NosSS/\WdissSS$ as a function of the energy gap $\beta \Delta E$ and transition ratio $\kstar$. ($\kenv \Delta t = 10^{-12}$.)}
\label{fig:phi_hm}
\end{figure}

%However, 
In the limit of large temperature, when the energy gap is small compared to the ambient thermal energy ($\beta \Delta E \ll 1$), $\phiSS$ reduces to a positive function of the equilibration rates of the system ($\ksys$) and environment ($\kenv$):
\begin{equation}
	\label{eq:limit}
	\phiSS = \frac{1-\kenv\Delta t}{1-2\kenv\Delta t+\kenv/\ksys} , \quad \beta\Delta E \ll 1 \ . 
\end{equation}
This is found by explicitly calculating the steady-state probability distribution.
In moving from discrete-time steps to a continuous-time parameter, 
time step size becomes small compared to system and environment transition times,
%If I don't take k_sys\Delta t->0 as well, then k_env/k_sys->0 and you'd wind up with mu=1 (ie. k_env/k_sys=(k_env\Delta t)/(k_sys\Delta t)). 
%($\ksys\Delta t \ll 1$ and $\kenv\Delta t \ll 1$),
reducing \eqref{eq:limit} to
\begin{equation}
	\label{eq:limit_smallprobs}
	\phiSS = \frac{1}{1+\kenv/\ksys} \ ,\quad \kenv\Delta t, \ksys\Delta t, \beta\Delta E \ll 1 \ .
\end{equation}

Thus, in the weak driving (high-temperature) limit ($\beta \Delta E \ll 1$), if the system evolves quickly compared to the environment, most of the dissipation occurs during work steps, the learning rate approaches the total thermodynamic entropy production, and the bound~\eqref{eq:ToPinequality} approaches saturation. 
Conversely (still restricting to high temperature), when the system evolves slowly compared to the environment, most of the dissipation occurs during relaxation steps, the learning rate is small compared to the total thermodynamic entropy production, and the nostalgia is small compared to the bound in \eqref{eq:ToPinequality}.

Further, Fig.~\ref{fig:phi_hm} shows that $\phiSS$ increases with $\beta \Delta E$. 
%DAS: so we have empirical evidence, but haven't actually theoretically derived this, right? 
Thus, this weak-driving limit gives a non-zero lower bound on $\phiSS$,
\begin{equation}
	\label{eq:bound}
	\frac{1-\kenv\Delta t}{1-2\kenv\Delta t+\kenv/\ksys} \le \phiSS \le 1 \ ,
\end{equation}
or in the limit of small time steps,
%(compared to characteristic environmental and system transition times),
\begin{equation}
	\label{eq:bound_smallprobs}
	\frac{1}{1+\kenv/\ksys} \le \phiSS \le 1 \ , \quad 
    \kenv\Delta t, \ksys\Delta t\ll 1 \ .
\end{equation}
If the system evolves quickly compared to its environment, nostalgia is the dominant form of dissipation, regardless of $\beta \Delta E$. 
The limit of quasi-static driving is defined by $\kstar \gg 1$. In this limit, $\phiSS=1$, and therefore the nostalgia (the implicit predictive model inefficiency) is equal to the total dissipation (the thermodynamic inefficiency). The bounds in Eqs.~\eqref{eq:bound} and \eqref{eq:bound_smallprobs} therefore hold beyond the quasi-static limit. The bound in Eq.~\eqref{eq:ToPinequality} can be looser for systems farther from the limit of quasi-static driving. 
These limits on $\phiSS$ are laid out in Table~\ref{tab:bounds}.

%MEQ: It seems like there should be some really nice interpretation here in terms of reversibility (ie. quasi-static processes tend to be reversible, so we dissipate only the amount of energy required to account for the nostalgia), but I couldn't really get anything coherent. 
%DAS: Quasi-static processes are reversible, meaning the total dissipated work goes to zero.  Apparently it does so in such a way that the leading-order contribution to dissipation is in the work steps, not the heat/relaxation steps.  Don't know what to do with that.  Should we mention this or just let sleeping dogs lie?  

\begin{table}[H]
    \caption{    \label{tab:bounds}
{\bf Limiting behavior of dissipation ratio.} Steady-state dissipation ratio $\phiSS$ in the various limits of driving strength and speed. These limits are given by the bound in Eq.~\eqref{eq:bound_smallprobs}, valid in the limit of continuous time.}
\centering
    \begin{tabular}{cc|c|c}
    ~             & \textbf{Driving Strength}     & Weak                        & Strong                                \\
    \textbf{Driving Speed} & ~                    & ($\beta\Delta E \ll 1$)     & ($\beta\Delta E \gtrsim 1$)        \\ \hline
    Quasi-static  & ($\kenv \ll \ksys$)  & $\phiSS=1$                    & $\phiSS=1$                              \\
%    Intermediate & ($\kenv \sim \ksys$) & $\phiSS=\left(1+\frac{\kenv}{\ksys}\right)^{-1}$ & $\left(1+\frac{\kenv}{\ksys}\right)^{-1} \le \phiSS \le 1$ \\
    Intermediate & ($\kenv \sim \ksys$) & $\phiSS=(1+\kenv/\ksys)^{-1}$ & $(1+\kenv/\ksys)^{-1} \le \phiSS \le 1$ \\
%    Fast          & ($\kenv \gg \ksys$)  & $\phiSS=\frac{\ksys}{\kenv}$           & $\frac{\ksys}{\kenv} \le \phiSS \le 1$          \\ \hline
    Fast          & ($\kenv \gg \ksys$)  & $\phiSS=\ksys/\kenv$           & $\ksys/\kenv \le \phiSS \le 1$          \\ \hline
    \end{tabular}
\end{table}

The transition ratio $\ksys/\kenv$ is also equal to the ratio of characteristic timescales $\tau_{\rm env}/\tau_{\rm sys}$. Thus the bound for steady-state dissipation ratio~\eqref{eq:bound_smallprobs} can be recast as 
\begin{equation}
\frac{1}{1+N} \le \phiSS \le 1 \ , \quad 
    \kenv\Delta t, \ksys\Delta t\ll 1 \ ,
\end{equation}
for $N$ independent `measurements' the system makes during each environment state~\cite{Govern:2014ez}.  From this perspective, the bound is proportional (up to a multiplicative constant) to the Berg-Purcell lower bound on environmental measurement precision of a single receptor~\cite{Berg:1977bp}.  


\subsection{Arbitrary System Rates}
The results of the previous section were derived for a simple two-state system, in which the energy difference between system states flips with environment transitions, and the system's equilibration rate is independent of the environment state. We generalize this model to a two-state system with arbitrary rates and hence---by detailed balance---arbitrary energies (Fig.~\ref{fig:statemaps}b). Given the four transition rates $k_{12}^{A}$, $k_{21}^{A}$, $k_{12}^{B}$, and $k_{21}^{B}$, when the environment is in state $X=x^{A}$ the system has energy gap (normalized by temperature) $\beta \Delta E^A_{12} = \ln \frac{k_{21}^{A}}{k_{12}^{A}}$ between state $y^1$ and $y^2$, and equilibration rate $k^\mathrm{A}=2/(1/k_{12}^{A}+1/k_{21}^{A})$. Similarly, when the environment is in state $X=x^{B}$, the corresponding parameters are $\beta \Delta E^B_{12} = \ln \frac{k_{21}^{B}}{k_{12}^{B}}$ and $k^\mathrm{B}=2/(1/k_{12}^{B}+1/k_{21}^{B})$. Let $\Delta E^A=\lvert \Delta E^A_{12} \rvert$ and $\Delta E^B=\lvert \Delta E^B_{12} \rvert$ be the magnitudes of the energy gaps in environment states $x^A$ and $x^B$, respectively. 
The energy gaps $\Delta E^A$ and $\Delta E^B$ are free to be aligned ($\Delta E^A_{12} \Delta E^B_{12} > 0$) or anti-aligned ($\Delta E^A_{12} \Delta E^B_{12} < 0$).
The system's overall equilibration rate is thus
\begin{equation}
	\label{eq:ksys_unweighted}
	k_\mathrm{sys}=\frac{2}{\frac{1}{k^A}+\frac{1}{k^B}} \ . 
\end{equation}

Equations~\eqref{eq:limit} and \eqref{eq:limit_smallprobs} also apply in this case of arbitrary system rates.
Figure~\ref{fig:phi_bound} shows that across the explored parameter space, the steady-state dissipation ratio $\phiSS$ lies above the bound~\eqref{eq:bound}, with $\phiSS$ approaching the bound in the weak-driving limit, $\beta (\Delta E^A+\Delta E^B) \ll 1$. 
We conclude that Eqs.~\eqref{eq:bound} and \eqref{eq:bound_smallprobs} apply for arbitrary system rates.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{phi_bounds.png}
\caption{{\bf Lower bound on dissipation ratio for fixed environment transition rate.}
The steady-state dissipation ratio $\phiSS$ is lower-bounded by the black curve~\eqref{eq:bound} for all values of the transition ratio $\kstar$. Each point corresponds to a particular set of parameters $k^A$, $k^B$, $\beta \Delta E^A$, and $\beta \Delta E^B$. (\textbf{a}) Models in which the energy gaps $\Delta E^A$ and $\Delta E^B$ are anti-aligned. (\textbf{b}) Models in which the energy gaps $\Delta E^A$ and $\Delta E^B$ are aligned. ($\kenv\Delta t = 10^{-12}$.)}
\label{fig:phi_bound}
\end{figure}


\subsection{Arbitrary Environment Rates}
Here we generalize our previous assumption of a fixed environmental transition rate $\kenv$, independent of the present environmental state.
We now allow for two different transition rates, $\kappa^{AB}$ and $\kappa^{BA}$, out of the two states $A$ and $B$ (Fig. \ref{fig:statemaps}c).

As above, we define the system equilibration rate $k^A$ and $k^B$ when the environment is in states $X=x^A$ and $X=x^B$, respectively. The overall system equilibration rate is the harmonic mean of the system transition rates for each environment state, weighted by the steady-state probabilities
\begin{equation}
	\label{eq:ksys}
    %\ksys=\frac{\kappa^{AB}+\kappa^{BA}}{\kappa^{BA}/k^{A}+\kappa^{AB}/k^B} \ .
    \ksys=\frac{1}{\frac{p^{\rm ss}(x^A)}{k^{A}}+\frac{p^{\rm ss}(x^B)}{k^B}} \ .
\end{equation}
For a uniform environmental transition rate (independent of environment state), this reduces to the previous un-weighted harmonic mean~\eqref{eq:ksys_unweighted}. 
Here we define $\kenvAv$ as the arithmetic mean of the transition rates between the environment states
\begin{equation}
	\label{eq:kenv}
\kenvAv=\frac{\kappa^{AB}+\kappa^{BA}}{2}. \
\end{equation}

With these definitions, Eqs.~\eqref{eq:limit} and \eqref{eq:limit_smallprobs} (replacing $\kenv$ with $\kenvAv$) apply to this case of arbitrary transition probabilities. Figure~\ref{fig:phi_bound_diffenv} shows that across a range of system and environment parameter values, bounds \eqref{eq:bound} and \eqref{eq:bound_smallprobs} hold.
The proposed bound depends on the system only through $\ksys$, and hence the environmental-state-dependent equilibration rates $k^A, k^B$.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{phi_bounds_diffenv.png}
\caption{{\bf Lower bound on dissipation ratio for varying environment transition rate.}
The steady-state dissipation ratio $\phiSS$ is lower-bounded by the black curve~\eqref{eq:bound} for all values of the transition ratio $\kstar$. Each point corresponds to a particular set of parameters $k^A$, $k^B$, $\beta \Delta E^A$, and $\beta \Delta E^B$. The environment transition rates are 
%taken to be 
$\kappa^{AB}=1.8\kenvAv$ and $\kappa^{BA}=0.2\kenvAv$. (\textbf{a}) Models in which the energy gaps $\Delta E^A$ and $\Delta E^B$ are anti-aligned. (\textbf{b}) Models in which the energy gaps $\Delta E^A$ and $\Delta E^B$ are aligned. ($\kenvAv\Delta t = 10^{-12}$.)}
\label{fig:phi_bound_diffenv}
\end{figure}


\subsection{Beyond Two System and Environment States}
The expressions above suggest natural generalizations beyond the two environment and system states studied here. 
%Intuitively, these are the rates that one may expect to enter such an expression. Because we are studying a system that is evolving under the influence of some environment, the important quantities are the rate at which the environment changes state and the rate at which the system equilibrates in some environment state. Thus, for the system, we should use a 
The rate at which the system responds to the environment is the overall system equilibration rate, which equals the 
harmonic mean 
of system transition rates given a particular environment state (because equilibration is a `series' process, with time scales [i.e., reciprocals of rates] adding),
%which properly captures the potential for rate-limiting steps in equilibration. The system equilibration rate should be 
weighted by the 
%amount of time spent in the corresponding environment state, 
%which is captured by the 
steady-state probability distribution over environment states. 
%On the other hand, the equilibration timescale for the environment state is irrelevant. All that matters is how often the environment changes state, driving the system away from equilibrium. This is captured by the arithmetic, rather than harmonic mean. 
%MEQ: The bound is written in terms of \kappa, not \bar{\kappa}. Is that fine?
Therefore Eq.~\eqref{eq:ksys} can be written as 
%the harmonic mean of system transition rates in each environment state, weighted by the steady-state probabilities of those environment states:
\begin{equation}
	\label{eq:ksys_general}
	\ksys=\left[\sum_{i \in \mathcal{X}}\frac{p^\mathrm{ss}(x^i)}{k^{i}}\right]^{-1} \ .
\end{equation}
This definition extends to additional states simply by summing over the entire environment state space. 

The definition of $\kenvAv$ in Eq.~\eqref{eq:kenv} can also be generalized to additional states. 
%One might expect the most relevant environmental rate to be the rate at which the environment changes state. Such a rate would be reflected by an 
The speed of environmental driving is determined by the overall rate of environmental change, which equals the 
arithmetic mean of transition rates out of environment states (because environmental change is a `parallel' process, with more transitions increasing the rate of change).
%, rather than the harmonic mean used for equilibration rates. 
%If we define 
For total transition rate $\kappa_\mathrm{out}^{i}$ out of state $i \in \mathcal{X}$, in the simple two-state system $\kappa_\mathrm{out}^A=\kappa^{AB}$ and $\kappa_\mathrm{out}^B=\kappa^{BA}$. Thus, the rate in Eq.~\eqref{eq:kenv} can be written as 
\begin{equation}
	\label{eq:kenv_general}
\kenvAv=\frac{1}{N_\mathcal{X}}\sum_{i\in \mathcal{X}} \kappa_\mathrm{out}^{i} \ ,
\end{equation}
for $N_\mathcal{X}=\sum\limits_{i\in \mathcal{X}} 1$ distinct environment states. 
Since each of the quantities in the bound~\eqref{eq:bound_smallprobs} (with $\kenvAv$ replacing $\kenv$) are well-defined outside of the simple two-state system and environment studied here, it is 
%possible 
intuitive
that this bound 
should
generalize as well. 


% %% If the documentclass option "submit" is chosen, please insert a blank line before and after any math environment (equation and eqnarray environments). This ensures correct linenumbering. The blank line should be removed when the documentclass option is changed to "accept" because the text following an equation should not be a new paragraph. 
% Please punctuate equations as regular text. Theorem-type environments (including propositions, lemmas, corollaries etc.) can be formatted as follows:
% %% Example of a theorem:
% \begin{Theorem}
% Example text of a theorem.
% \end{Theorem}
% The text continues here. Proofs must be formatted as follows:

% %% Example of a proof:
% \begin{proof}[Proof of Theorem 1]
% Text of the proof. Note that the phrase `of Theorem 1' is optional if it is clear which theorem is being referred to.
% \end{proof}
% The text continues here.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Discussion}
%This section may be divided by subheadings. Authors should discuss the results and how they can be interpreted in perspective of previous studies and of the working hypotheses. The findings and their implications should be discussed in the broadest context possible. Future research directions may also be highlighted.

Ref.~\cite{Still2012} described a relationship between dissipation and nostalgia, a novel abstract information-theoretical concept quantifying the information the system stores about its environment that fails to be predictive of future environmental states. Energetically efficient performance requires avoiding this nostalgia. This framework suggests applications in biology, where living things are influenced by, and thus learn about, their environments. 
Recent explorations of the implications of this relationship have illuminated its behavior in model neurons~\cite{mcintosh2012information}, its relation to sensor performance~\cite{hartich2016sensory}, and the variation of it and related quantities across several biophysical model systems~\cite{Brittain2017}.

Here we focused on a
physical understanding of the relationships between the information-theoretic and thermodynamic quantities. We calculated the nostalgia in some model systems, alongside other thermodynamic and information-theoretic quantities of interest. 
Calculating these quantities over the parameter space of simple systems helps to establish an intuitive picture: 
when the system is quick to relax and strongly driven by the environment (energy gaps vary strongly with environment state), the nostalgia provides a tight lower bound on the steady-state dissipation~\eqref{eq:limit_smallprobs}; equivalently, the system learns more about the environment per unit heat dissipated. 

For fixed equilibration rates, we found the ratio of nostalgia to total dissipation is minimized in the weak-driving limit. Further, the ratio of nostalgia to total dissipation is bounded from below by this weak-driving limit~\eqref{eq:bound_smallprobs}, which depends on the system only through its overall equilibration rate. If the system is driven quasi-statically by its environment, this bound dictates that the predictive inefficiency (nostalgia) is responsible for all thermodynamic inefficiency (dissipation). 
Contexts further from the quasi-static limit can be further from saturating the bound in Eq.~\eqref{eq:ToPinequality}, and hence have a smaller relative contribution from model inefficiency. 
%Thus, in this sense, predictive inefficiency (the nostalgia) is equal to the thermodynamic inefficiency (the dissipation) in the quasi-static limit. 

One could explore more complex models than the simple Markovian two-state systems and environments described here. 
One could expand the system to more states~\cite{Barato2014}, or expand the environmental behavior through additional states or non-Markovian dynamics, since this theoretical framework does not restrict the form of these transitions. 
%In particular, it is worth exploring whether the bound given in Eq.~\eqref{eq:bound} can be generalized beyond two system states and two environment states in steady state.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \section{Materials and Methods}

% This section should be divided by subheadings. Materials and Methods should be described with sufficient details to allow others to replicate and build on published results. Please note that publication of your manuscript implicates that you must make all materials, data, and protocols associated with the publication available to readers. Please disclose at the submission stage any restrictions on the availability of materials or information. New methods and protocols should be described in detail while well-established methods can be briefly described and appropriately cited.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \section{Conclusions}
% This section is not mandatory, but can be added to the manuscript if the discussion is unusually long or complex.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{6pt} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\acknowledgments{
%All sources of funding of the study should be disclosed. Please clearly indicate grants that you have received in support of your research work. Clearly state if you received funds for covering the costs to publish in open access.
The authors thank Volodymyr Polyakov (Physics and Technology, Igor Sikorsky Kyiv Polytechnic Institute) for insightful comments on the manuscript. 
This work is supported by a Natural Sciences and Engineering Research Council of Canada (NSERC) Discovery Grant (D.A.S.) and a Tier-II Canada Research Chair (D.A.S.).
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\authorcontributions{
%For research articles with several authors, a short paragraph specifying their individual contributions must be provided. The following statements should be used 
M.E.Q. and D.A.S. conceived and designed the study; M.E.Q. performed the analytic and numerical calculations; M.E.Q. analyzed the data; 
%W.W. contributed reagents/materials/analysis tools; 
M.E.Q. and D.A.S. wrote the paper. 
%Authorship must be limited to those who have contributed substantially to the work reported.
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\conflictofinterests{
The authors declare no conflict of interest. The funding sponsors had no role in the design of the study; in the collection, analyses, or interpretation of data; in the writing of the manuscript, and in the decision to publish the results.
} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% optional
% \abbreviations{The following abbreviations are used in this manuscript:\\
% \noindent MDPI: Multidisciplinary Digital Publishing Institute\\
% DOAJ: Directory of open access journals\\
% TLA: Three letter acronym\\
% LD: linear dichroism}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% optional
% \appendix
% \section{}
% The appendix is an optional section that can contain details and data supplemental to the main text. For example, explanations of experimental details that would disrupt the flow of the main text, but nonetheless remain crucial to understanding and reproducing the research shown; figures of replicates for experiments of which representative data is shown in the main text can be added here if brief, or as Supplementary data. Mathematical proofs of results not central to the paper can be added as an appendix.

% \section{}
% All appendix sections must be cited in the main text. In the appendixes, Figures, Tables, etc. should be labeled starting with `A', e.g., Figure A1, Figure A2, etc. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibliographystyle{mdpi}

%\bibliography{Thesis}
\begin{thebibliography}{-------}
\providecommand{\natexlab}[1]{#1}

\bibitem[Jaynes(1957)]{Jaynes:1957ua}
Jaynes, E.T.
\newblock Information theory and statistical mechanics.
\newblock {\em Phys. Rev.} {\bf 1957}, {\em 106},~620--630.

\bibitem[Landauer(1961)]{landauer1961irreversibility}
Landauer, R.
\newblock Irreversibility and heat generation in the computing process.
\newblock {\em IBM Journal of Research and Development} {\bf 1961}, {\em
  5},~183--191.

\bibitem[B{\'{e}}rut \em{et~al.}(2012)B{\'{e}}rut, Arakelyan, Petrosyan,
  Ciliberto, Dillenschneider, and Lutz]{Berut2012}
B{\'{e}}rut, A.; Arakelyan, A.; Petrosyan, A.; Ciliberto, S.; Dillenschneider,
  R.; Lutz, E.
\newblock {Experimental verification of Landauerâ€™s principle linking
  information and thermodynamics}.
\newblock {\em Nature} {\bf 2012}, {\em 483},~187--189,
  \href{http://xxx.lanl.gov/abs/arXiv:1503.06537v1}{{\normalfont
  [arXiv:1503.06537v1]}}.

\bibitem[Jun \em{et~al.}(2014)Jun, Gavrilov, and Bechhoefer]{Jun2014}
Jun, Y.; Gavrilov, M.; Bechhoefer, J.
\newblock High-precision test of Landauer's principle in a feedback trap.
\newblock {\em Phys. Rev. Lett.} {\bf 2014}, {\em 113},~190601.

\bibitem[Cheong \em{et~al.}(2011)Cheong, Rhee, Wang, Nemenman, and
  Levchenko]{Cheong:2011jp}
Cheong, R.; Rhee, A.; Wang, C.J.; Nemenman, I.; Levchenko, A.
\newblock Information transduction capacity of noisy biochemical signaling
  networks.
\newblock {\em Science} {\bf 2011}, {\em 334},~354--358,
  \href{http://xxx.lanl.gov/abs/http://science.sciencemag.org/content/334/6054/354.full.pdf}{{\normalfont
  [http://science.sciencemag.org/content/334/6054/354.full.pdf]}}.

\bibitem[Mehta and Schwab(2012)]{Mehta:2012ji}
Mehta, P.; Schwab, D.J.
\newblock Energetic costs of cellular computation.
\newblock {\em Proceedings of the National Academy of Sciences} {\bf 2012},
  {\em 109},~17978--17982,
  \href{http://xxx.lanl.gov/abs/http://www.pnas.org/content/109/44/17978.full.pdf}{{\normalfont
  [http://www.pnas.org/content/109/44/17978.full.pdf]}}.

\bibitem[Still \em{et~al.}(2012)Still, Sivak, Bell, and Crooks]{Still2012}
Still, S.; Sivak, D.A.; Bell, A.J.; Crooks, G.E.
\newblock Thermodynamics of prediction.
\newblock {\em Phys. Rev. Lett.} {\bf 2012}, {\em 109},~120604.

\bibitem[Hess(2011)]{Hess:2011:AnnuRevBiomedEng}
Hess, H.
\newblock Engineering applications of biomolecular motors.
\newblock {\em Annual Review of Biomedical Engineering} {\bf 2011}, {\em
  13},~429--450,
  \href{http://xxx.lanl.gov/abs/https://doi.org/10.1146/annurev-bioeng-071910-124644}{{\normalfont
  [https://doi.org/10.1146/annurev-bioeng-071910-124644]}}.

\bibitem[Brown and Sivak(2017)]{brown2017toward}
Brown, A.I.; Sivak, D.A.
\newblock Toward the design principles of molecular machines.
\newblock {\em Physics in Canada} {\bf 2017}, {\em 73},~61--66.

\bibitem[Okuno \em{et~al.}(2011)Okuno, Iino, and Noji]{okuno2011rotation}
Okuno, D.; Iino, R.; Noji, H.
\newblock Rotation and structure of FoF1-ATP synthase.
\newblock {\em The Journal of Biochemistry} {\bf 2011}, {\em 149},~655--664.

\bibitem[Tagkopoulos \em{et~al.}(2008)Tagkopoulos, Liu, and
  Tavazoie]{Tagkopoulos:2008ct}
Tagkopoulos, I.; Liu, Y.C.; Tavazoie, S.
\newblock {Predictive Behavior Within Microbial Genetic Networks}.
\newblock {\em Science} {\bf 2008}, {\em 320},~1313--1317.

\bibitem[Laughlin(1981)]{Laughlin:1981wn}
Laughlin, S.
\newblock {A simple coding procedure enhances a neuron's information capacity.}
\newblock {\em Z. Naturforsch., C, Biosci.} {\bf 1981}, {\em 36},~910--912.

\bibitem[McGregor \em{et~al.}(2012)McGregor, Vasas, Husbands, and
  Fernando]{McGregor:2012ij}
McGregor, S.; Vasas, V.; Husbands, P.; Fernando, C.
\newblock {Evolution of Associative Learning in Chemical Networks}.
\newblock {\em PLoS Comput. Biol.} {\bf 2012}, {\em 8},~e1002739--19.

\bibitem[Chandler(1987)]{Chandler1987a}
Chandler, D.
\newblock {\em Introduction to Modern Statistical Mechanics}; Oxford University
  Press: Oxford,  1987.

\bibitem[Cover and Thomas(2012)]{cover2012elements}
Cover, T.M.; Thomas, J.A.
\newblock {\em Elements of information theory}; John Wiley \& Sons,  2012.

\bibitem[Schnakenberg(1976)]{Schnakenberg:1976p55305}
Schnakenberg, J.
\newblock Network theory of microscopic and macroscopic behavior of master
  equation systems.
\newblock {\em Rev. Mod. Phys.} {\bf 1976}, {\em 48},~571--585.

\bibitem[Brittain \em{et~al.}()Brittain, Jones, and Ouldridge]{Brittain2017}
Brittain, R.A.; Jones, N.S.; Ouldridge, T.E.
\newblock What we learn from the learning rate.
\newblock {\em Journal of Statistical Mechanics: Theory and Experiment}, {\em
  2017},~063502.

\bibitem[Barato \em{et~al.}(2014)Barato, Hartich, and Seifert]{Barato2014}
Barato, A.C.; Hartich, D.; Seifert, U.
\newblock Efficiency of cellular information processing.
\newblock {\em New Journal of Physics} {\bf 2014}, {\em 16},~103024.

\bibitem[Hartich \em{et~al.}(2016)Hartich, Barato, and
  Seifert]{hartich2016sensory}
Hartich, D.; Barato, A.C.; Seifert, U.
\newblock Sensory capacity: An information theoretical measure of the
  performance of a sensor.
\newblock {\em Phys. Rev. E} {\bf 2016}, {\em 93},~022116.

\bibitem[Govern and ten Wolde(2014)]{Govern:2014ez}
Govern, C.C.; ten Wolde, P.R.
\newblock {Energy Dissipation and Noise Correlations in Biochemical Sensing}.
\newblock {\em Phys. Rev. Lett.} {\bf 2014}, {\em 113},~258102.

\bibitem[Berg and Purcell(1977)]{Berg:1977bp}
Berg, H.C.; Purcell, E.M.
\newblock {Physics of chemoreception}.
\newblock {\em Biophys. J.} {\bf 1977}, {\em 20},~193--219.

\bibitem[McIntosh(2012)]{mcintosh2012information}
McIntosh, L.
\newblock Information processing and energy dissipation in neurons.
\newblock PhD thesis, University of Hawaii at Manoa,  2012.

\end{thebibliography}

\end{document}