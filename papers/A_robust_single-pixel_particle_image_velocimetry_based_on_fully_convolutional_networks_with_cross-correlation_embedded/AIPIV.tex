% ****** Start of file aipsamp.tex ******
%
%   This file is part of the AIP files in the AIP distribution for REVTeX 4.
%   Version 4.1 of REVTeX, October 2009
%
%   Copyright (c) 2009 American Institute of Physics.
%
%   See the AIP README file for restrictions and more information.
%
% TeX'ing this file requires that you have AMS-LaTeX 2.0 installed
% as well as the rest of the prerequisites for REVTeX 4.1
% 
% It also requires running BibTeX. The commands are as follows:
%
%  1)  latex  aipsamp
%  2)  bibtex aipsamp
%  3)  latex  aipsamp
%  4)  latex  aipsamp
%
% Use this file as a source of example code for your aip document.
% Use the file aiptemplate.tex as a template for your document.
\documentclass[%
 aip,
% jmp,
% bmf,
% sd,
% rsi,
 amsmath,amssymb,
%preprint,%
 reprint,%
%author-year,%
%author-numerical,%
% Conference Proceedings
]{revtex4-1}

\usepackage{graphicx}% Include figure files
\usepackage{dcolumn}% Align table columns on decimal point
\usepackage{bm}% bold math
%\usepackage[mathlines]{lineno}% Enable numbering of text and display math
%\linenumbers\relax % Commence numbering lines

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{mathptmx}
\usepackage{etoolbox}

\usepackage{xcolor}
\usepackage{multirow}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{subfigure}
\usepackage{array}
\usepackage{dcolumn}

%% Apr 2021: AIP requests that the corresponding 
%% email to be moved after the affiliations
\makeatletter
\def\@email#1#2{%
 \endgroup
 \patchcmd{\titleblock@produce}
  {\frontmatter@RRAPformat}
  {\frontmatter@RRAPformat{\produce@RRAP{*#1\href{mailto:#2}{#2}}}\frontmatter@RRAPformat}
  {}{}
}%
\makeatother
\begin{document}

\preprint{AIP/123-QED}

\title[Sample title]{A robust single-pixel particle image velocimetry based on fully convolutional networks with cross-correlation embedded}
% Force line breaks with \\
\author{Qi Gao}
\affiliation{State Key Laboratory of Fluid Power and Mechatronic Systems, School of Aeronautics and Astronautics, Zhejiang University, Hangzhou 310027, China.}

\author{Hongtao Lin}
\affiliation{MicroVec., Inc, Beijing 100083, China.}

\author{Han Tu}
 \homepage{Electronic mail: hantu@zju.edu.cn.}
\affiliation{State Key Laboratory of Fluid Power and Mechatronic Systems, School of Aeronautics and Astronautics, Zhejiang University, Hangzhou 310027, China.}

\author{Haoran Zhu}
\affiliation{State Key Laboratory of Fluid Power and Mechatronic Systems, School of Aeronautics and Astronautics, Zhejiang University, Hangzhou 310027, China.}

\author{Runjie Wei}
\affiliation{MicroVec., Inc, Beijing 100083, China.}

\author{Guoping Zhang}
\affiliation{China Ship Scientific Research Center, Wuxi 214082, China.}

\author{Xueming Shao}
 \homepage{Electronic mail: mecsxm@zju.edu.cn.}
\affiliation{State Key Laboratory of Fluid Power and Mechatronic Systems, School of Aeronautics and Astronautics, Zhejiang University, Hangzhou 310027, China.}


\date{\today}% It is always \today, today,
             %  but any date may be explicitly specified

\begin{abstract}
Particle image velocimetry (PIV) is essential in experimental fluid dynamics. In the current work, we propose a new velocity field estimation paradigm, which achieves a synergetic combination of the deep learning method and the traditional cross-correlation method. 
Specifically, the deep learning method is used to optimize and correct a coarse velocity guess to achieve a super-resolution calculation. 
And the cross-correlation method provides the initial velocity field based on a coarse correlation with a large interrogation window. 
As a reference, the coarse velocity guess helps with improving the robustness of the proposed algorithm.
This fully convolutional network with embedded cross-correlation is named as CC-FCN.
CC-FCN has two types of input layers, one is for the particle images, and the other is for the initial velocity field calculated using cross-correlation with a coarse resolution.
Firstly, two pyramidal modules extract features of particle images and initial velocity field respectively.
Then the fusion module appropriately fuses these features.
Finally, CC-FCN achieves the super-resolution calculation through a series of deconvolution layers to obtain the single-pixel velocity field.
%Transfer layers are also set up to further improve the prediction accuracy and precision. 
As the supervised learning strategy is considered, synthetic data sets including ground-truth fluid motions are generated to train the network parameters. 
Synthetic and real experimental PIV data sets are used to test the trained neural network in terms of accuracy, precision, spatial resolution and robustness. 
The test results show that these attributes of CC-FCN are further improved compared with those of other tested PIV algorithms. 
The proposed model could therefore provide competitive and robust estimations for PIV experiments.
%Among these enhanced features, the improvement of robustness is the most conspicuous. 

\end{abstract}

\maketitle
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{quotation}
%The ``lead paragraph'' is encapsulated with the \LaTeX\ 
%\verb+quotation+ environment and is formatted as a single paragraph before the first section heading. 
%(The \verb+quotation+ environment reverts to its usual meaning after the first sectioning command.) 
%Note that numbered references are allowed in the lead paragraph.
%
%The lead paragraph will only be found in an article being prepared for the journal \textit{Chaos}.
%\end{quotation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}
\label{sect1}
% The very first letter is a 2 line initial drop letter followed
% by the rest of the first word in caps.
% 
% form to use if the first word consists of a single letter:
% \IEEEPARstart{A}{demo} file is ....
% 
% form to use if you need the single drop letter followed by
% normal text (unknown if ever used by the IEEE):
% \IEEEPARstart{A}{}demo file is ....
% 
% Some journals put the first two words in caps:
% \IEEEPARstart{T}{his demo} file is ....
% 
% Here we have the typical use of a "T" for an initial drop letter
% and "HIS" in caps to complete the first word.
%\IEEEPARstart{T}{his} demo file is intended to serve as a ``starter file''

A wide range of problems in engineering applications and scientific research involve extracting physical displacement fields from data \citep{Adrian1991,LiAI2008,Juan2009,Taylor2011},
such as analyzing the internal flow in a cardiac interventional pump \citep{triep2006,triep2008}, studying wake characteristics of fish for the investigation of bio-inspired swimming mechanism \citep{Tytell2008,Ting2009,Flammang2011,Shen2012}, and studying coherent structures in turbulent flows \citep{wang2019experimental,Wang2021}.
Therefore, relevant velocimetry technologies are becoming increasingly important in the field of experimental fluid mechanics as they help researchers to get further understanding and deeper insight into the complex flow phenomena \citep{cai2019}.
As a non-invasive measurement technique, particle image velocimetry (PIV)\citep{Raffel2007,Adrian2011} has been widely applied to optical, quantitative, and non-contact surface or spatial fluid analysis.
The use of PIV allows the spatial development of the flow field to be investigated in detail \citep{Pratt2013}.
Velocity fields can be obtained by this method. Then related properties of the flow, such as vorticity, acceleration \citep{IPTA} and pressure \citep{irro-pre,PCS}, can be derived from the velocity measurement result.
The basic principles of PIV can be summarized as follows. 
Tracer particles, which are sufficiently small to faithfully follow the flow motions, are uniformly seeded into the flow medium. 
The fluid with entrained particles is illuminated by a laser sheet so that the particles become visible in the measurement region.
Successive images with illuminated particles are recorded by one or multiple cameras, depending on the type of the PIV technique.
Finally, the speed and direction (i.e., the velocity vector field) of the flow in the measurement region are obtained by calculating the motion of the tracer particles recorded in particle images.

In practical applications, what researchers ultimately need is the high-precision and high-resolution velocity field. 
Therefore, how to properly estimate the velocity field of the flow from particle images becomes very important. 
The image analysis technique for calculating the velocity field from particle images has been developed for decades and several different calculation methods have gradually formed.
One of them is the cross-correlation approach, which could provide sparse motion field by searching the maximum of the cross-correlation between two interrogation windows of an image pair \citep{Raffel2007,Adrian2011,Westerweel1999}.
Regarding the displacement vector as the average velocity in the window, the cross-correlation method is widely used in commercial software due to its simplicity and efficiency.
\citet{Scarano2001} proposed a window deformation iterative multi-grid (WIDIM) method, which could optimize the numerical calculation approach and improve the accuracy and resolution of the calculation results. 
The WIDIM method achieves a decoupling between the spatial resolution and the dynamic range by using an iterative evaluation program with integrated window refinement. It achieves good performance evaluation in the International PIV Challenges \citep{Stanislas}.
Another approach to obtaining the velocity vector field from particle images is the optical flow computation, which was first proposed by \citet{Horn1981}.
This method has been very popular in the computer vision community and attracts researchers in experimental fluid mechanics since it gives dense motion field for the whole image. 
At present, a series of different types of optical flow methods have been developed in order to obtain better results in fluid motion analysis, such as \citet{Corpetti2002,Ruhnau2007,Zhong2017,lu2021accurate}.
However, optical flow methods are generally time-consuming as they contain optimization processes to minimize the objective function \citep{cai2019,cai2019particle}. In addition, the brightness constancy constraint in optical flow methods is sensitive to the image noise \citep{cai2019particle}.
Different from the above-mentioned reconstruction methods in the Eulerian description, particle tracking velocimetry (PTV) is a Lagrangian method which tracks the trajectory of particles. 
This approach could potentially offer a higher spatial resolution than PIV provides, since it avoids the spatial filtering effect inherent to the cross-correlation algorithm \citep{Schneiders2016}. 
The downside of PTV is the limitation in particle image density \citep{Schneiders2016,Schanz2016}.
With the development of Shake-The-Box (STB)\citep{Schanz2016} and advanced iterative particle reconstruction (IPR)\citep{Wieneke}, the Lagrangian particle tracking method can have seeding concentrations on a higher order. 

Compared with the manually designed operations, deep learning is a completely different approach to solving problems. 
This method can learn an estimating model which can approximate any desired complex mapping function from data. 
Deep learning has achieved great success in the computer image processing, and it presents a great potential in the field of PIV.
Although PIV deals with the physics-related problem, estimating the motions of particles from images is considered as an issue of image processing or computer vision to some extent.
Hence, designing a deep neural network for the flow motion estimation becomes a promising direction.
\citet{rabault2017} reported the first application of Convolutional Neural Networks (CNNs) and Fully Connected Neural Networks (FCNNs) in performing end-to-end PIV. 
Although the proposed models in \citet{rabault2017} did not outperform the traditional PIV methods with respect to root mean square error, the study aroused attention and inspired other researchers to involve in this topic. 
\citet{lee2017} designed a four-layer cascaded network architecture (PIV-DCNN) to improve the performance and spatial resolution of the large displacement estimation. 
\citet{wang2020} developed a convolutional neural network to measure the velocity field of near-wall turbulence. 
The proposed machine learning based model can improve the near-wall velocity prediction and spatial resolution of the wall turbulence velocity field obtained by PIV.
\citet{dosovitskiy2015} proposed the first networks (FlowNetS and FlowNetC) for optical flow estimation. As the successor, FlowNet2 \citep{Ilg2017} was designed as a cascade of variants of the FlowNet.
\citet{Sun2017} proposed the PWC-Net model for optical flow estimation.
\citet{cai2019} modified the FlowNetS and used it to estimate the dense velocity field of PIV.
Additionally, they also modified the LiteFlowNet \citep{hui2018} convolutional neural network to further improve the estimation performance of the machine learning method \citep{cai2019particle}. 
The original network is improved mainly by adding a deconvolution layer and adjusting the weight of the loss function, so that it can extract a more accurate and precise velocity field from the particle image pair.

%Although the PIV technique based on machine learning has shown advantages such as higher accuracy and higher spatial resolution, the stability and robustness of these algorithms still cannot meet the requirements of practical applications. When the particle images have severe noise, these algorithms often fail to get correct results.

Although the PIV technique based on machine learning has shown advantages such as higher accuracy and higher spatial resolution, the stability and robustness of relevant models still need be further improved for practical applications.
Severe noise in particle images brings challenge to the estimation.
Moreover, system complexity and training determine the generalization capability of the network. 
If the network is over-trained or system complexity is more than the training dataset, poor generalization could be observed \citep{Urolagin}. In other words, traditional CNN-based flow motion estimators may give unsatisfactory results when predicting unseen or complicated flow fields in real applications.
In the current work, we propose an innovative deep learning model embedded with cross-correlation, which can suppress noise and obtain satisfying results for practical applications.

The article is organized as follows. In Section~\ref{sect2}, the basic concept and principles of the proposed model are provided.
In Section~\ref{sect3}, we describe the structure of the proposed neural network and the generation of synthetic training data. 
The test results on real and synthetic PIV experimental data are discussed in Section~\ref{sect4}, followed by the conclusions in Section~\ref{sect5}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Proposed method}
\label{sect2}
In the PIV experiment, in order not to affect the fluid dynamics of the flow being studied, the density of the tracer particles seeded into the flow medium cannot be infinitely high.
Therefore, it is impossible to have tracer particles at every pixel.
Even if there is an ideal method which can obtain the displacement of each tracer particle, it still cannot get the accurate displacement information at the region where there is no particle. 
As a result, PIV cannot obtain the flow information of arbitrary places in the measurement region, but can only obtain the average velocity within the interrogation window.
To sum up, the information about fluid motion recorded by the PIV experiment is filtered. 
This characteristic of PIV fundamentally limits the extraction of single-pixel-level high-resolution flow fields through direct calculation methods, such as matching tracer particles of two adjacent particle images.

Through the above analysis, the PIV experiment can be considered as a down-sampling operation of the flow field, which records the down-sampled flow motion information in the form of particle images. 
Given a real flow field, denoted by $I_{x}$, the output of the PIV experiment carried out on it can be modeled as
\begin{equation}
I_{y}=D(I_{x},\delta)+\eta,
\label{eqn1}
\end{equation}
where $D$ denotes a down-sampling function, $I_{y}$ is the particle image, $I_{x}$ is the ground truth flow field, $\delta$ represents the parameters of the down-sampling process, and $\eta$ is the noise introduced during the PIV experiment. 
Generally, details of the down-sampling process (i.e., $D$ and $\delta$) and the noise $\eta$ are unknown. 
In this case, our goal is to recover an approximation $\hat{I}_{x}$ of the ground truth $I_{x}$ from the particle image $I_{y}$, following:
\begin{equation}
\hat{I}_{x}=F(I_{y},\theta).
\label{eqn2}
\end{equation}
Here $F$ is the reconstruction model and $\theta$ denotes the parameters of $F$. 
The function of $F$ is to recover the entire flow field from $I_{y}$, which records the filtered information of the ground truth flow field.
This is essentially a super-resolution problem, and the noise $\eta$ is an important factor for a successful execution of the calculation.
Improper handling of the noise $\eta$ could lead to calculation failure during the reconstruction process of the deep learning method.
%Many researchers have attempted to use deep learning methods to achieve such reconstruction process. But when these reconstruction algorithms are applied to actual cases, calculation failure often occurs. The noise $\eta$ is the most important reason for the calculation failure. 
In the process of neural network model training, the actual noise $\eta$ is usually reduced as the Gaussian noise.  
However, in addition to the Gaussian noise, there are many kinds of noise in actual PIV experiments, which vary from experiment to experiment.
Therefore, it is impractical to summarize these different kinds of noise and put them into the training set to search for a well-trained neural network model.
When a new form of noise appears in the particle image, it may cause the calculation to fail.

The key to meeting the above-mentioned challenge is to find a way to effectively suppress the noise $\eta$ in PIV particle images, and let the neural network perform reconstruction calculations based on as much accurate information as possible.
It is well known that the cross-correlation algorithm has strong robustness and can resist the influence of noise in particle images.
The output of the cross-correlation algorithm can be modeled as follows:
\begin{equation}
I_{z}=C(I_{y},\delta_{c})+\eta_{c},
\label{eqn3}
\end{equation}
where $C$ denotes the cross-correlation function, $I_{y}$ is the particle image, $\delta_{c}$ is the parameters of the cross-correlation function and $\eta_{c}$ is the noise error.
$I_{z}$ is the calculated velocity field obtained by the cross-correlation algorithm.
Although its resolution is severely reduced, $I_{z}$ still gives most of the information about the flow field. 
More importantly, $\eta_{c}$ is small enough so that the result of the cross-correlation calculation can be used as the standard reference. 
The role of the cross-correlation algorithm is equivalent to an information converter which filters noise. Specifically, the cross-correlation algorithm converts and down-samples the ground truth information to the form of the sparse velocity field. This procedure filters most of the noise.
Based on the above analysis, a new reconstruction model based on deep learning is proposed as follows:
\begin{equation}
\hat{I}_{x}=F_{c}(\lambda_{1}I_{y},\lambda_{2}I_{z},\theta_{c}),
\label{eqn4}
\end{equation}
which can be written in a more specific form:
\begin{equation}
\hat{I}_{x}=F_{c}(\lambda_{1}D(I_{x},\delta)+\lambda_{1}\eta,\lambda_{2}C(I_{y},\delta_{c})+\lambda_{2}\eta_{c},\theta_{c}).
\label{eqn5}
\end{equation}
Here $F_{c}$ is the reconstruction model based on deep learning proposed in the current work. 
$\lambda_{1}$, $\lambda_{2}$ and $\theta_{c}$ denote the parameters of $F_{c}$, which can be learned automatically in the process of neural network model training.
The new reconstruction model has two different inputs, one is the particle images $I_{y}$, and the other is the velocity field $I_{z}$ calculated by the cross-correlation method. 
In the new reconstruction model, the noise introduced in the PIV experiment becomes $\lambda_{1}\eta$.
Specifically, the neural network model can get a proper parameter $\lambda_{1}$ by learning to suppress the noise.
Then, the noise can be reduced to an appropriate level to avoid calculation failure. 
Although the parameter $\lambda_{1}$ also affects the neural network model to obtain information from the particle images, calculations on actual cases show that the neural network model can extract enough information from the flow field $I_{z}$ with small noise error $\eta_{c}$ to complete the velocity field extraction.

To this end, the objective of the reconstruction model proposed in the current work is to minimize the mean square error (MSE) between the generated flow field $\hat{I}_{x}$ and the ground truth flow field $I_{x}$ as follows:
\begin{equation}
\hat{\theta_{c}}=argmin_{\theta_{c}}L(\hat{I}_{x},I_{x}),
\label{eqn6}
\end{equation}
where $L(\hat{I}_{x},I_{x})$ represents the loss function between $\hat{I}_{x}$ and $I_{x}$. The loss function used in this work is the most popular pixel-wise mean square error.
The $argmin_{\theta_{c}}$ finds the smallest possible value of the loss function $L$ by adjusting $\theta_{c}$. 
And $\hat{\theta_{c}}$ represents the optimal parameters for the reconstruction model $F_{c}$.

\begin{figure*}
\centering
  \includegraphics[width=1\textwidth]{./Figs/fig1.png}
\caption{Architecture of the CC-FCN model.}
\label{fig1}  
\end{figure*}

Tests on real PIV images show that the reconstruction model proposed in the current work combines the advantages of traditional cross-correlation-based algorithms and purely artificial intelligence (AI) based algorithms. 
It is not only robust to significant noise, but also capable of obtaining high-resolution flow fields with the single-pixel level.

\section{Neural Network design and dataset training}
\label{sect3}
\subsection{Neural Network for PIV}
\label{sect3.1}


The neural network architecture proposed in this paper is based on the Fully Convolutional Networks (FCN)\citep{long2015}.
FCN is a special case of convolutional networks (ConvNets). The training process of FCN is end-to-end (pixel-to-pixel) and embodies less network parameters. 
Thus, it can simplify and speed up the processes of learning and inference in the networks. As a result, the time cost of the network training is reduced while the computational efficiency of the network is improved.  
More importantly, FCN is able to take input of arbitrary size and generate output of the same size.
It can generate dense pixel output via deconvolution layers \citep{nie20183}. 
FCN has achieved state-of-the-art performance in many image processing applications. 
Because of these outstanding features, we employ and further extend FCN, and then propose a new network structure based on fully convolutional networks with cross-correlation embedded, named as CC-FCN.
The structure of CC-FCN is demonstrated in Fig.~\ref{fig1} and a brief introduction is given as below.

As illustrated in Fig.~\ref{fig1}, CC-FCN has two separate input layers, including the image input layer and the velocity input layer. 
Particle images are used as input in the image input layer, and the velocity field calculated by the cross-correlation method with very coarse resolution is used as a reference velocity in the velocity input layer. 
After the two input layers, there are two pyramid-shaped contraction network architectures composed of multiple convolution layers and pooling layers for gradually extracting features from the particle images and the coarse velocity field respectively. 
The kernel of the convolution layers is set to (3,3) and the Leaky linear correction unit (LeakyReLU)\citep{xu2015empirical} is used as the activation function. 
Maximum pooling layers with a step size of 2 are used in the pyramid-shaped contraction network structures. 
Then a concatenate layer is used to merge features extracted from the particle images and the coarse velocity field. 
After the concatenate layer, a convolution layer is used to extract features from the concatenate layer, thereby further enhancing the fusion of the two types of input information.
Finally, an expanding part composed of a number of deconvolution layers is used to gradually up-sample the fused features, and ultimately the velocity field with the same size as the input particle images is obtained.

According to the above introduction, the CC-FCN model consists of three major operations: two down-sampling and one up-sampling. 
The two down-sampling operation streams (i.e., convolution and pooling), extracting information from the particle images and the velocity field calculated by the cross-correlation respectively, usually result in a coarse and global prediction based on the entire input of the network.
And the up-sampling stream (i.e., deconvolution) can generate the dense prediction through finer inference. 
The information reaching up-sampling phases is highly abstracted after a series of pooling operations. Thus, it is easy to lose small-scale structures.
To solve this issue, an information transfer architecture for CC-FCN is designed.
Specifically, we transfer the context information of the coarse feature maps obtained from the set of down-sampling operations to the up-sampling phases, and concatenate these transferred features with features obtained from deconvolution.
The transfer operation not only copies the feature maps, but also uses the convolutional layer in the transfer structure to adjust the number of feature maps from the lower layer to be comparable to the number of those from the corresponding higher layer.
More importantly, we employ fusion modules (i.e., extra convolutional layers) after the concatenation layers to enhance the fusion of the low-level and high-level features. 
As a consequence, the deconvolution layers during the up-sampling phases can generate more precise outputs based on the assembled feature maps.

In order to train the neural network more effectively, we design four output layers to output the predicted velocity fields at different levels. 
On the one hand, the predicted velocity fields are used to construct the loss function. On the other hand, they are up-sampled and concatenated to the next level.
For CC-FCN, the loss function is composed of the prediction errors at different levels of the expanded network architecture:
\begin{equation}
Loss=\Sigma_{i}\lambda_{i}e_{i},
\label{eqn7}
\end{equation}
where $i$ is the level index, $e_{i}$ denotes the error metric between the ground truth and the prediction, and $\lambda_{i}$ represents the preset weight of different levels.


\subsection{Training dataset}
\label{sect3.2}

\begin{figure*}
\begin{center}
  \includegraphics[width=1\textwidth]{./Figs/fig2.png}
\caption{Generation of training data sets.}
\label{fig2}   
\end{center}
\end{figure*}

\begin{table*}
\begin{center}
\caption{Descriptions of the motion fields}\label{tab1}%
\begin{tabular}{ p{3cm} p{6cm} p{3cm}  p{2cm} }  %{@{}llll@{}}
\hline
Case name & Description  & Condition & Quantity\\
\hline
Uniform    & Uniform flow   & $|dx|\in[0;5]$  & 1000  \\
\hline
\multirow{4}*{Back-step} & \multirow{4}*{Backward stepping flow} & $Re=800$ & 600 \\
~ & ~ & $Re=1000$ & 600 \\
~ & ~ & $Re=1200$ & 1000 \\
~ & ~ & $Re=1500$ & 1000 \\
\hline
\multirow{5}*{Cylinder} & \multirow{5}*{Flow over a circular cylinder} & $Re=40$ & 50 \\
~ & ~ & $Re=150$ & 500 \\
~ & ~ & $Re=200$ & 500 \\
~ & ~ & $Re=300$ & 500 \\
~ & ~ & $Re=400$ & 500 \\
\hline
DNS-turbulence    & A homogeneous and isotropic turbulence flow   & -  & 2000  \\
\hline
SQG    & Sea surface flow driven by SQG model   & -  & 2000  \\
\hline
Channel    & Channel flow   & -  & 1600  \\
\hline
MHD1024    & Forced MHD turbulence Coarse   & -  & 1000  \\
\hline
Isotropic1024 Coarse    & JHTDB-isotropic1024 Coarse   & -  & 2000  \\
\hline
Isotropic1024 Fine    & JHTDB-isotropic1024 Fine   & -  & 1000  \\
\hline
Mixing    & JHTDB-Mixing   & -  & 1000  \\
\hline
\end{tabular}
\end{center}
\end{table*}

Training the network model requires a large amount of ground truth data sets to optimize the model parameters.
However, it is difficult to obtain accurate velocity fields from real PIV experiments. 
Hence, a synthetic dataset is used for training and evaluation of the network. 
Fig.~\ref{fig2} describes the generation of the synthetic dataset, which consists of the following three main parts.

\textbf{\textit{Flow fields}}. The dataset includes various flow motions generated by computational fluid dynamics (CFD).
Specifically, data sets such as uniform flow (Uniform), backward stepping flow (Back-step) and vortex shedding over a circular cylinder (Cylinder) are provided in \citet{cai2019particle,cai2019}. Corresponding data sets can be downloaded from the website: \url{https://github.com/shengzesnail/PIV_dataset}.
In addition, DNS-turbulence and surface quasi-geostrophic (SQG) sets are provided in \citet{carlier2005second} and \citet{Resseguier2016}, respectively. 
Other turbulence data sets, such as JHTDB-isotropic1024-hd, JHTDB-mhd1024-hd and JHTDB-channel, are provided in Johns Hopkins Turbulence Databases (JHTDB, \url{http://turbulence.pha.jhu.edu/}).
Detailed descriptions of the dataset are shown in Table~\ref{tab1}. 
Each data set contains velocity fields at successive moments, which constitute the velocity field sequence shown in the top row of Fig.~\ref{fig2}.
% the source and naming of sets are not very clear

%This work has been done by many researchers, such as S. Cai simulated uniform flow (Uniform), backward stepping flow (Back-step) and vortex shedding over a circular cylinder (Cylinder) by computational fluid dynamics (CFD) in his work[13]. In addition, other flow patterns such as DNS-turbulence, SQG, JHTDB-isotropic1024-hd, JHTDB-mhd1024-hd and JHTDB-channel are also provided in his work[10] and [13]. More importantly, these precious and important data were uploaded to the world's largest code hosting platform-githug. We downloaded these dataset from the website (\url{https://github.com/shengzesnail/PIV_dataset}). We also downloaded some other flow patterns in the website(\url{http://turbulence.pha.jhu.edu/}), which is a portal to an Open Numerical Turbulence Laboratory that enables access to multi-Terabyte turbulence databases. Detailed descriptions of the motion fields is shown in Table 1. Each flow pattern contains velocity fields at successive moments, which constitutes the sequence of velocity field shown as the top line in Fig.~\ref{fig2}.

\textbf{\textit{Particle images}}. The generation of synthetic particle images is shown in Fig.~\ref{fig2}.
The first particle image is constructed by randomly seeding particles whose intensity satisfies a two-dimensional Gaussian function into the image domain.
Next, the underlying flow motions are extracted from the first ground truth velocity field through velocity inquiry. 
Then the motion field is applied to the particles in the first image to obtain the second particle image.
Repeat the above steps to extract the underlying flow motions from the second flow field, and apply them to the particles in the second image to generate the third particle image, and so on.
Eventually, a sequence of particle images can be obtained, as shown in the middle row of Fig.~\ref{fig2}.
The resolution of the generated particle images is 256 $\times$ 256 pixels.
Parameters for defining a particle image are randomly selected in a proper range, as shown in Table~\ref{tab2}.
All particle images are added with the Gaussian noise, which has a sigma uniformly sampled from [0, 0.04].
Starting with different randomly generated particle images, a large number of particle image sequences can be generated based on the velocity field sequences of different flow motions in the dataset. 
Basically we follow the strategy in \citet{cai2019} to generate synthetic particle images. Please refer to this article for more details.

\begin{table}[h]
\begin{center}
\caption{The ranges of parameters for determining a synthetic particle image}\label{tab2}%
\begin{tabular}{ p{3cm} p{2cm} p{2cm}  }
\hline
Parameter & Range  & Unit \\
\hline
Seeding density $\rho$    & 0.05-0.1 & ppp  \\
Particle diameter $d_{p}$    & 1-4 & pixel  \\
Peak intensity $I_{0}$    & 200-255 & grey value  \\
\hline
\end{tabular}
\end{center}
\end{table}

\textbf{\textit{Initial velocity field}}. 
As shown at the bottom of Fig.~\ref{fig2}, the coarse velocity field sequence can be obtained by applying the cross-correlation method to two adjacent particle images in the image sequence.
The commercial software MicroVec 3.6.2 (from MicroVec, Inc.) is used for executing the correlation calculation. 
The window size of the cross-correlation method is 16 $\times$ 16 pixels, and the overlap rate is 50\%, leading to a vector number of the initial velocity field of 32 $\times$ 32. In theory, a coarser initial velocity field can be used, which can further reduce the computational cost of training.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Results and discussions}
\label{sect4}
As introduced in Section~\ref{sect3}, the neural network model is trained using a synthetic PIV dataset. To further demonstrate the performance and practicality of the particle image velocity measurement algorithm based on deep neural network, test results of the proposed CC-FCN with both synthetic and real PIV experimental data are presented in Section~\ref{sect4}.
Due to the various sizes of particle images taken in real PIV experiments, we have developed a set of image segmentation and merging codes following the strategy in \citet{cai2019,cai2019particle}.
In specific, the original input particle images are firstly divided into several parts with the standard size of 256 $\times$ 256 pixels. Then, these parts are processed using CC-FCN and corresponding partial velocity fields are obtained. Finally, the calculation results of each part are orderly combined to get the ultimate output, which is the same size as the original input.

\subsection{Test on spatial accuracy and precision}
\label{sect4.1}

\begin{figure*}
	\begin{center}
	    \subfigure[]{			\includegraphics[height=.35\textwidth]{./Figs/truefield.png}
			\label{fig3a}}
	    \subfigure[]{			\includegraphics[height=.35\textwidth]{./Figs/ccfield.png}
			\label{fig3b}}\\
		\subfigure[]{			\includegraphics[height=.35\textwidth]{./Figs/ai2field.png}
			\label{fig3c}}
	    \subfigure[]{			\includegraphics[height=.35\textwidth]{./Figs/ai1field.png}
			\label{fig3d}}
		\caption{Estimated velocity fields from (a) the ground truth, (b) the cross-correlation method, (c) the CC-FCN model (d) the LiteFlowNet-en model. The color map demonstrates the velocity magnitude.}
		\label{fig3}
	\end{center}
\end{figure*}

\begin{figure*}[t]
	\begin{center}
	    \subfigure[]{			\includegraphics[height=.24\textwidth]{./Figs/ccerr.png}
			\label{fig4a}}
	    \subfigure[]{			\includegraphics[height=.24\textwidth]{./Figs/ai2err.png}
			\label{fig4b}}
		\subfigure[]{			\includegraphics[height=.24\textwidth]{./Figs/ai1err.png}
			\label{fig4c}}
		\caption{Error distributions of (a) the cross-correlation algorithm, (b) the CC-FCN model, (c) the LiteFlowNet-en model.}
		\label{fig4}
	\end{center}
\end{figure*}

\begin{figure*}
	\begin{center}
	    \subfigure[]{			\includegraphics[height=.24\textwidth]{./Figs/cccurve.png}
			\label{fig5a}}
	    \subfigure[]{			\includegraphics[height=.24\textwidth]{./Figs/ai2curve.png}
			\label{fig5b}}
		\subfigure[]{			\includegraphics[height=.24\textwidth]{./Figs/ai1curve.png}
			\label{fig5c}}
		\caption{Error distribution histograms of (a) the cross-correlation algorithm, (b) the CC-FCN model, (c) the LiteFlowNet-en model.}
		\label{fig5}
	\end{center}
\end{figure*}

Accuracy represents the closeness of the measurement values to the ground truth, and precision exhibits the closeness of the measurement values to each other.
To quantitatively demonstrate the accuracy and precision of the proposed CC-FCN model, we first investigate the model on synthetic particle images. The synthetic particle images are generated from the velocity field as follows:
\begin{equation}
\begin{split}
u(i,j)&=4cos\left[\frac{2\pi i}{256-0.3i}\right]sin\left[\frac{2\pi j}{256-0.3j}\right],\\
v(i,j)&=-4sin\left[\frac{2\pi i}{256-0.3i}\right]cos\left[\frac{2\pi j}{256-0.3j}\right],
\label{eqn8}
\end{split}
\end{equation}
where $i$ and $j$ denote the index of the image pixel along $x$- and $y$-direction, respectively.
These synthetic particle images correspond to vortical flows with varying wavelength, which constitute a standard test case in the PIV community.
For comparison, we also demonstrate the results of the cross-correlation method with WIDIM and LiteFlowNet-en. 
The cross-correlation method uses interrogation window of 16 $\times$ 16 pixels and step size of 8 pixels.
The result of the cross-correlation method is regarded as the baseline of PIV estimation.
The LiteFlowNet-en model is a velocity measurement algorithm based on deep neural network developed by \citet{cai2019particle}, and its result is regarded as the baseline of traditional CNN-based model's estimation.
In the following test cases, estimation results of the cross-correlation method and LiteFlowNet-en are also used as benchmarks for comparison.
The ground truth motion field is shown in Fig.~\ref{fig3}(a), which contains a series of vortices of different sizes. 
Figs.~\ref{fig3}(b)-(d) show the results estimated by the traditional cross-correlation algorithm, the CC-FCN model and the LiteFlowNet-en model, respectively.
%According to Fig.~\ref{fig3}(b), the cross-correlation algorithm fails to estimate the small-scale vortices.
The cross-correlation algorithm (Fig.~\ref{fig3}b) is able to capture vortices at different scales, but it gives a relatively poor estimation of speed magnitude in the region of small-scale vortices.
However, the small-scale structures can be estimated very well by CC-FCN and LiteFlowNet-en, as shown in Figs.~\ref{fig3}(c)-(d).
As for the estimation of large-scale vortices, the results obtained by these methods are consistent with the ground truth.
It is worth mentioning that the vector number of the results estimated by CC-FCN and LiteFlowNet-en is 512 $\times$ 512, while the vector number of the flow field estimated by the traditional cross-correlation method is only 63 $\times$ 63.
Although testing on spatial resolution is not the topic of this sub-section, it is not difficult to see that the deep learning based PIV methods show great advantages in spatial resolution as they are single-pixel algorithms. Because it depends on the size of interrogation window, the estimation resolution of the cross-correlation method is inevitably lower than that of single-pixel algorithms.

In order to further compare the calculation quality, we quantify the error of the above methods.
Due to the resolution difference among the results, we first convert the vector number of the flow fields to the same value as 63 $\times$ 63.
Then the error distribution fields are obtained by subtracting the ground truth velocity field from the predicted velocity fields obtained by these three methods. 
The error distributions are shown in Fig.~\ref{fig4}.
It is obvious that the error of the cross-correlation method is quite larger than that of the other two methods for estimating the small-scale vortices. 
The cross-correlation method underestimates the velocity magnitude of small-scale vortices regions.
As shown in Fig.~\ref{fig4}(b), the estimation error is greatly reduced in the same region, which means that CC-FCN can properly extract small-scale flow motions.
Still, more training data related to the flow patterns near the top and right boundaries could be considered.
Fig.~\ref{fig4}(c) indicates that LiteFlowNet-en gives an overall outstanding estimation.

As a quantitative supplement, Fig.~\ref{fig5} shows the histogram of the error of these methods. The mean value and standard deviation of the errors are also denoted in the figure.
%For a more intuitive comparison, the corresponding Gaussian distribution curves according to the mean value and standard deviation of each error distribution are shown as the red dotted lines. 
For a more intuitive comparison, black dotted and solid lines are used to exhibit the mean error and zero, respectively.
%It can be seen that all the error distributions are close to the normal distribution.
%However, the error of the cross-correlation method has the largest standard deviation, which means that the distribution of error is relatively scattered. Additionally, the mean value of the error of the cross-correlation method is the farthest from zero.
Due to the underestimation of the velocity in the small-scale vortices region, the mean value of the error of the cross-correlation method is the worst away from zero. Additionally, the error of the cross-correlation method has the largest standard deviation, which means that the distribution of error is relatively scattered.
By contrast, CC-FCN can correctly extract flow motions in regions where the cross-correlation estimation is poor, even though the proposed model is based on the result of cross-correlation.
The mean error of CC-FCN is greatly reduced, which is the one with the least deviation from zero. 
It means that CC-FCN is the most accurate approach among the three methods.
The error standard deviation of CC-FCN is also smaller than that of cross-correlation.
Hence, CC-FCN presents an excellent ability to fix defects in the sparse velocity field as input when giving the dense output.
%Because the CC-FCN model is based on the result of cross-correlation, the precision of CC-FCN calculation result will inevitably be affected by the result of cross-correlation. 
%Hence, it is not surprising to see that the standard deviation of the error of CC-FCN is larger than that of LiteFlowNet-en. 
%However, the corresponding mean error is greatly reduced, which is the one with the least deviation from zero. 
%It means that CC-FCN is the most accurate approach among the three methods.
The standard deviation of error of LiteFlowNet-en is the closest to zero, which means that the error distribution of LiteFlowNet-en is less scattered. 
But the corresponding mean error of LiteFlowNet-en is larger than that of CC-FCN, so the result calculated by LiteFlowNet-en is less close to the ground truth.
%The standard deviation of error of LiteFlowNet-en is the closest to zero, which means that the result calculated by LiteFlowNet-en is generally the closest to the ground truth. 
%But the corresponding standard deviation of LiteFlowNet-en is larger than that of CC-FCN, so the error distribution of LiteFlowNet-en is slightly more scattered. 
In summary, CC-FCN has the highest accuracy but its precision cannot outperform the LiteFlowNet-en model.

\begin{figure*}
	\begin{center}
	    \subfigure[]{			\includegraphics[height=.25\textwidth]{./Figs/fig6a.png}
			\label{fig6a}}
	    \subfigure[]{			\includegraphics[height=.25\textwidth]{./Figs/fig6b.png}
			\label{fig6b}}
		\caption{(a) Schematic diagram of the jet flow experiment and (b) a particle image sample.}
		\label{fig6}
	\end{center}
\end{figure*}

\begin{figure*}
\begin{center}
  \includegraphics[width=1\textwidth]{./Figs/fig7test.png}
\caption{Estimated jet flow velocity fields from (a) the cross-correlation method, (b) the proposed CC-FCN model, (c) the LiteFlowNet-en model. Corresponding partial enlargements of the secondary vortex region are shown in (d)-(f), respectively. The color map demonstrates the velocity magnitude. The centers of the two near-wall vortices and the saddle are marked with yellow and white dots, respectively.}
\label{fig7}   
\end{center}
\end{figure*}

\begin{figure*}
	\begin{center}
	    \subfigure[]{			\includegraphics[height=.2\textwidth]{./Figs/fig8.png}
			\label{fig8a}}
	    \subfigure[]{			\includegraphics[height=.2\textwidth]{./Figs/fig9.png}
			\label{fig8b}}
		\caption{(a) Particle images at adjacent times of the agricultural dripper micro-channel PIV experiment and (b) mask of the agricultural dripper micro-channel test. Note that the brightness of the sample images in (a) is increased for a better display. The brightness of the particle images input to the AI-based models is not adjusted.}
		\label{fig8}
	\end{center}
\end{figure*}

\begin{figure*}
\begin{center}
  \includegraphics[width=1\textwidth]{./Figs/fig10.png}
\caption{Estimated micro-channel flow velocity fields from (a) the cross-correlation method, (b) the proposed CC-FCN model, (c) the LiteFlowNet-en model. The color map demonstrates the velocity magnitude.}
\label{fig10}   
\end{center}
\end{figure*}

\subsection{Test on spatial resolution}
\label{sect4.2}
The jet flow experiment is a common configuration in experimental fluid mechanics, which can form typical flow structures such as jet, velocity gradient change, vortex, separation flow, etc. 
We adopt the jet flow to assess the effective spatial resolution of the CC-FCN model.
Fig.~\ref{fig6} shows the schematic and a particle image sample of the jet flow PIV experiment.
When the incoming flow passes through the upstream semi-circular small hole, a jet is formed. A vertical plate is placed downstream, which is used to induce separation and recirculation flow.
Particles are illuminated by a semiconductor continuous laser (5 W, 532 $\mu$m wavelength), and particle images (640 $\times$ 480 pixels) are recorded by a CCD camera.
The window size and step size of the cross-correlation method are 16 $\times$ 16 pixels and 4 pixels, respectively.

Figs.~\ref{fig7}(a)-(c) present the velocity fields estimated by the cross-correlation method, CC-FCN and LiteFlowNet-en, respectively.
For the sake of comparison, we adjust the resolution of the velocity fields to the same level. Namely, sparse velocity fields with 16 $\times$ 16 pixels for one vector are shown in the above figures.
Thanks to the high-quality particle images obtained in the PIV experiment, all three methods obtain overall good velocity field estimation results.
In order to further compare details of the flow field extracted by these three methods, partial enlargements of a typical near-wall region (marked by the black dotted box) with abundant flow structures are presented in Figs.~\ref{fig7}(d)-(f).
For simplicity of observation, the two partial enlargements of AI-based models are presented with 2 $\times$ 2 pixels for one vector. The velocity field in Fig.~\ref{fig7}(d) is 4 $\times$ 4 pixels for one vector, since  the cross-correlation method can only give sparse estimation.
Nevertheless, as a mature flow motion estimation technology, the cross-correlation method could extract flow structures with relatively high reliability.
Two vortices and one saddle point in the near-wall region are detected by all three methods. Vortex centers and the saddle are marked by yellow and white dots, respectively. 
The coordinate locations are labeled below the dots, through which we can see that the spatial information of typical structures (i.e., two vortex centers and one saddle) captured by CC-FCN is closer to that obtained by cross-correlation rather than by LiteFlowNet-en. 
CC-FCN and the cross-correlation method extract the relatively high velocity around the vortices. Obviously, the former gives more details of the high-speed regions.
By contrast, the flow field extracted by LiteFlowNet-en exhibits smaller high-speed regions around the two vortices, but an abnormal high-speed region close to the wall.
Combining the case shown in Section~\ref{sect4.1}, it can be seen that the deep learning based PIV methods are superior than the traditional cross-correlation method in terms of spatial resolution.
%More importantly, the proposed CC-FCN model can provide the dense velocity field without overestimating or underestimating the flow speed.
Compared with LiteFlowNet-en, the proposed CC-FCN model can provide the dense velocity field closer to the reliable estimation given by the cross-correlation algorithm.

%More importantly, the proposed CC-FCN model can correctly extract the flow motion without over-estimation or under-estimation.


%However, Figs.~\ref{fig7}(b) and (c) apparently give more details of the flow field, especially in the region of the secondary vortex (marked by black dotted box).
%In order to further exhibit these details, partial enlargements of the secondary vortex region are shown in Figs.~\ref{fig7}(d)-(f).
%Since the cross-correlation method can only give sparse estimation, the velocity field in Fig.~\ref{fig7}(d) is still 16 $\times$ 16 pixels for one vector.
%As a result, only a few vectors describe the secondary structures.
%On the contrary, CC-FCN and LiteFlowNet-en can give dense estimation as shown in Figs.~\ref{fig7}(e) and (f).
%The number of vectors is abundant in the near-wall region.
%Models based on deep learning, especially CC-FCN, can provide more details of small-scale flow structures (e.g., the secondary vortex and other derivative motions in Fig.~\ref{fig7}e).
%Combining the case shown in Section~\ref{sect4.1}, the deep learning based PIV methods are much superior than the traditional cross-correlation method in terms of spatial resolution.

%With abundant velocity vectors in the near-wall region, the deep learning based models, especially CC-FCN, can provide more details of small-scale flow structures (e.g., the secondary vortex and other derivative motions in Fig.~\ref{fig7}(e)).

%\textcolor{blue}{It is worth noting that the CC-FCN model can even detect the tertiary vortex (the small-scale vortex between the jet and the secondary vortex) and near-wall high velocity regions induced by the secondary and tertiary vortices. However, the flow field detected by LiteFlowNet-en is over-smoothed to some extent. Therefore, above-mentioned small-scale structures and related velocity gradient are not obvious in Fig.~\ref{fig7}(f).}


\subsection{Test on robustness}
\label{sect4.3}

\begin{figure*}
\begin{center}
  \includegraphics[width=0.5\textwidth]{./Figs/fig10re.png}
\caption{Particle images at adjacent times of the zebrafish PIV experiment.}
\label{fig11}   
\end{center}
\end{figure*}

\begin{figure*}
\begin{center}
  \includegraphics[width=0.8\textwidth]{./Figs/fig12re.png}
\caption{Velocity and vorticity fields estimated by the CC-FCN model at different moments. The color map demonstrates the vorticity of the flows.}
\label{fig12}   
\end{center}
\end{figure*}

\begin{figure*}
	\begin{center}
	    \subfigure[]{			\includegraphics[height=.28\textwidth]{./Figs/ccps2.png}
			\label{fig13a}}
	    \subfigure[]{			\includegraphics[height=.28\textwidth]{./Figs/ai2ps2.png}
			\label{fig13b}}
		\subfigure[]{			\includegraphics[height=.28\textwidth]{./Figs/ai1ps2.png}
			\label{fig13c}}
		\caption{Estimated velocity fields at the tail of zebrafish from (a) the cross-correlation method, (b) the CC-FCN model, (c) the LiteFlowNet-en model. The color map demonstrates the velocity magnitude. The color map demonstrates the dimensionless $\lambda_{ci}^{\ast}$. Vortices are denoted from V1 to V5, and their centers are marked with red dots. The saddle near the tail fin (black mask) is denoted as S1 and it is marked with the green dot. Orientations of jets J1 and J2 are indicated by the thick black arrows.}
		\label{fig13}
	\end{center}
\end{figure*}

Robustness is the ability of an algorithm to resist or overcome adverse conditions.
Robustness directly determines the feasibility of the algorithm for practical applications.
In Section~\ref{sect4.1} and Section~\ref{sect4.2}, high-quality particle images are used in the spatial accuracy and resolution test cases.
However, due to the limitation of experimental conditions, the quality of particle images in many PIV experiments may be poor. 
Therefore, it is necessary to investigate robustness of the current proposed model. 
Fig.~\ref{fig8}(a) shows the particle image samples for the robustness test, which is a PIV experiment of agricultural dripper micro-channel. 
Water only flows in the micro-channel, so there are no tracer particles in the area outside the micro-channel.
Particle images (1600 $\times$ 1200 pixels) are captured by a CCD camera equipped with a SM-MICROL-X10 lens.
Illumination is provided by a dual-head
Nd:YAG laser (200 mJ/pulse, 532 nm wavelength), and fluorescent particles about 8 $\mu$m diameter are applied.
The cross-correlation method employs interrogation window of 32 $\times$ 32 pixels and step size of 16 pixels.
A mask (Fig.~\ref{fig8}b) is used to filter the calculation result, so that only the velocity vectors in the white area are displayed. The velocity in the remaining areas is set to zero.

Figs.~\ref{fig10}(a)-(c) show the velocity fields estimated by the cross-correlation method, CC-FCN and LiteFlowNet-en, respectively.
Similar to Section~\ref{sect4.2}, we adjust the resolution of the velocity fields to the same level (16 $\times$ 16 pixels for one vector).
It can be clearly seen from Figs.~\ref{fig10}(a)-(c) that all three methods can resist severe noise and obtain the correct flow motion pattern.
In other words, CC-FCN and LiteFlowNet-en present good robustness when estimating this complicated flow field with a complex boundary.
Small-scale flow motions, such as recirculation flows at the corners, are extracted by these two AI-based models.
Especially, CC-FCN clearly captures structures of those vortical flows with abundant details, including nearby relatively high-speed flows and velocity gradients.
Compared with the cross-correlation result, however, recirculation flows at corners extracted by AI-based models exhibit a higher velocity magnitude.
Meanwhile, the high-speed regions (red/orange contours) extracted by LiteFlowNet-en are slightly different to the corresponding regions detected by the cross-correlation method and CC-FCN.
In specific, high-speed regions in Fig.~\ref{fig10}(c) have a significantly higher velocity magnitude in the left and a lower velocity magnitude in the middle of the micro-channel.
Such distribution may hint an overestimation or underestimation of the velocity magnitude.
It must be mentioned that the brightness of the original input particle images is much lower than that of the sample images shown in Fig.~\ref{fig8}(c).
Hence, the current test also shows that CC-FCN and LiteFlowNet-en are robust against poor illumination.

%However, as shown in Fig.~\ref{fig10}(c), the velocity field obtained by the LiteFlowNet-en model deviates from the expected pattern. 
%Fig.~\ref{fig10}(c) exhibits a relatively high velocity in the near-wall regions and presents a discontinuous high-speed distribution (isolated oval red/orange contours).
%Intuitively, the flow near the wall or corner of the micro-channel will not exhibit a high velocity, and the high-speed distribution should have greater continuity since there are no obstacles inside the micro-channel.
%In addition, the LiteFlowNet-en model fails to capture these small-scale vortices in the recirculation regions (i.e., vortices at the corners of the micro-channel).
%It must be mentioned that the great brightness change of the particle images brings a negative impact on the velocity field extraction by LiteFlowNet-en.
%The current test shows that CC-FCN has a significant improvement in the measurement of complex real flow fields compared with traditional CNN-based methods.

%The current test shows that the traditional CNN based models, such as LiteFlowNet-en, have poor robustness and are difficult to meet actual needs. The inability to obtain correct results in practical applications prevents these algorithms from being widely adopted by commercial software.

%\subsection{Test on fish wake flows}
%\label{sect4.4}
%The above testing cases show that the current proposed CC-FCN model has many advantages compared with previous algorithms.
The above testing cases show that the current proposed CC-FCN model has many advantages.
In order to further evaluate the feasibility of CC-FCN in practical applications, we employ a more complex PIV experiment case to examine the performance of CC-FCN.
The experiment aims to extract the wake flows generated by an adult zebrafish, which freely swims in a water tank filled with fresh water. 
Particle images (1024 $\times$ 1024 pixels) are recorded by a high-speed camera with a 200 mm Nikon macro lens. Particles with a mean diameter of 20 $\mu$m are illuminated by a dual-head high-speed ND:YAG laser (50 mJ/pulse, 527 nm wavelength).
The window size and step size of the cross-correlation method are 16 $\times$ 16 pixels and 8 pixels, respectively. 
Fig.~\ref{fig11} shows a pair of particle images at adjacent times in the particle image sequence. 
The whole particle image sequence consists of 140 particle images.

Fig.~\ref{fig12} exhibits some typical frames of the velocity field sequence calculated by CC-FCN. Velocity vectors are superimposed on the vorticity contours, where red represents positive vorticity and blue represents negative vorticity.
Vorticity accumulates around the tail fin tip when the fish oscillates its tail fin. As a consequence, a vortex is generated and temporally attached to the fin. The vortex gradually grows and finally it is shed into the wake.
As the fish continues to oscillate the fin, vortices are shed in order, and a so-called reverse von K\'{a}rm\'{a}n street \citep{Triantafyllou1993} is formed.
The vortex street and induced jets are clearly detected by CC-FCN.
It is obvious that the CC-FCN model can resist noise and capture flow structures with details for complex practical scenarios, such as the flow field near a zebrafish.

As mentioned earlier, the robustness of the cross-correlation algorithm is very strong. To compare the robustness of cross-correlation, CC-FCN and LiteFlowNet-en in this particular case, we extract two adjacent particle images at a certain moment in the particle image sequence and process them by these three methods, as shown in Fig.~\ref{fig13}.
Dimensionless $\lambda_{ci}^{\ast}$ ($\lambda_{ci}^{\ast}=\lambda_{ci}/\lambda_{ci,max}$) contours are superimposed with velocity vectors. 
The tail fin that sweeps down is represented by the black mask.
Vortices in the wake are denoted from V1 to V5 and their centers are marked with red dots. The saddle point (green dot) near the tail fin is labeled as S1. 
The high-speed flows J1 and J2 induced by the tail fin and the vortex ring composed of V1 and V2 are clearly captured by all three methods, though the structure of J1 and V2 slightly varies from Figs.~\ref{fig13}(a)-(c).
%The vortex ring composed of V1 and V2 is clearly captured by all three methods.
%The high-speed flows J1 and J2 induced by the tail fin are also detected by each method. The structure of J1 varies from Figs.~\ref{fig13}(a)-(c).
Specifically, the $\lambda_{ci}^{\ast}$ magnitude of V2 and the high-speed region corresponding to J1 extracted by LiteFlowNet-en are obviously smaller than that extracted by the cross-correlation method and CC-FCN. 
The swirling strength of V2 and the momentum of J1 may be underestimated by LiteFlowNet-en.
%We believe that in this case, the relatively low density concentration of particles has a negative impact on extracting velocity fields by AI-based models.
%Apart from that, veloity fields obtained by LiteFlowNet-en and CC-FCN obviously contain more details, thanks to their high resolution.
We believe that in this case, complex flow structures, boundaries with complicated shapes (i.e., the fish tail) and noise in the near-body regions bring challenges to the extraction of velocity fields.
The LiteFlowNet-en model successfully detects the small vortex V3 attached to the tail fin. CC-FCN can extract a more satisfactory deduced velocity field, since it detects the other two small near-body vortices (V4 and V5) and the saddle point S1 under the influence of above-mentioned negative factors.
Again, the locations of vortex centers and saddle captured by CC-FCN are closer to that obtained by cross-correlation rather than by LiteFlowNet-en. 
The test of the zebrafish experiment again ensures the good robustness of CC-FCN and its feasibility to complex practical applications. 

%One typical region is near the tail fin, as marked by the blue circle. The undulation of the fish body leads to the generation of bound voticity, which will move toward the tail tip and shed into the wake \citep{Wolfgang1999}. Such structure is not clearly captured by the cross-correlation method. Noise close to the fish body in the raw particle images may be the main reason for the unsatisfactory deduced velocity field. 
%By contrast, CC-FCN and LiteFlowNet-en capture the bound vorticity. Another typical region is near the pelvic fin, as marked by the red circle. The motions of the fish body and the pelvic fin cause the generation of a near-body vortex. Compared with the cross-corelation method and CC-FCN, the LiteFlowNet-en model fails to detect the near-body vortex.


%Moreover, the LiteFlowNet-en model is unable to capture the wake vortex. The complex inner boundary/mask may be the reason for the failure of the flow structure extraction. By contrast, the wake vortex and the downwashed jet are clearly detected by CC-FCN. 
 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Conclusion}
\label{sect5}
In this paper, we propose a new neural network structure, which is named as CC-FCN.
Considering that the cross-correlation method is fairly robust and the deep learning method can obtain super-resolution estimation, CC-FCN is designed to achieve a synergetic combination of the cross-correlation method and deep learning. Their advantages (e.g., robustness, super-resolution and high-accuracy) are inherited by the CC-FCN model.
%In this paper, we investigate the existing velocity field estimation methods, including the traditional cross-correlation algorithm and methods based on deep learning.
%The characteristics of these methods in the PIV experiment are analyzed and summarized. 
%It is found that the PIV experiment is equivalent to a down-sampling process of the real flow field, and the reconstruction of dense velocity field from particle images is essentially a super-resolution problem.
%Based on the above analysis, we propose a new neural network structure, which is named as CC-FCN. 
This neural network structure has two types of inputs, one is the particle images and the other is the low-resolution initial velocity field. 
The output of the neural network is a dense velocity field with single-pixel-resolution. 
As the supervised learning strategy is considered, a synthetic dataset including various ground-truth fluid motions is generated to train the parameters of the network. 
Finally, CC-FCN is tested with the real PIV experimental data. The velocity fields estimated by CC-FCN are compared with the results obtained by the cross-correlation method and LiteFlowNet-en, which is a state-of-the-art deep learning based velocity estimation method. 
The proposed CC-FCN model gives flow estimation with the highest accuracy in the synthetic multiple vortices test. And it also exhibits good robustness in practical experimental tests, which involve complicated flow structures with complex inner/outer boundaries commonly seen in PIV experiments.
%Based on the result of cross-correlation, CC-FCN is robust against complicated flow structures with complex inner/outer boundaries commonly seen in PIV experiments.
Slightly differences exist in the estimations of these three methods in experimental tests, but the estimations of CC-FCN and the cross-correlation algorithm (namely, a reliable reference) have greater closeness.
The proposed neural network can properly fuse the two kinds of input information and successfully achieve the competitive super-resolution estimation. 



%Therefore, CC-FCN can detect more small-scale flow structures from particle images with high accuracy.
%The CC-FCN model can obtain high-precision and accurate flow estimation.
%More importantly, robustness of CC-FCN has been greatly improved, which is not only superior than the current methods based on deep learning, but also better than the current PIV commercial software based on the cross-correlation algorithm.
%CC-FCN is robust against complicated flow structures with complex inner/outer boundaries commonly seen in PIV experiments. 
%The proposed model could provide competitive estimation results for practical experimental data.





%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Acknowledgment}
This work was supported by the National Key Research and Development Program of China (Grant No. 2020YFA040070), the National Natural Science Foundation of China (Grant No. 12072320) and the State Key Program of National Natural Science Foundation of China (Grant No. 91852204).
The authors would like to thank Dr. Shengze Cai for his help in data analysis with the LiteFlowNet-en model.

\section*{Data Availability Statement}

The data that support the findings of this study are available from the corresponding author upon reasonable request.

\bibliography{AIbib}% Produces the bibliography via BibTeX.

\end{document}
%
% ****** End of file aipsamp.tex ******