\section{Pose estimation from synthetic RGB images}

In the previous two sections, 3DP3 was used with a depth-rendering-based likelihood on depth images since an RGB-D image was given as input. In this section, we show that 3DP3 can be used to do pose estimation without depth data i.e. from just an RGB image. Instead of a depth likelihood, we substitute an RGB renderer and simple color likelihood. We qualitatively compare with Attend, Infer, Repeat (AIR) \cite{eslami2016attend}, an amortized inference approach based on recurrent neural networks which can be applied to infer poses of 3D objects. We generated scenes that resemble the tabletop scenes on which AIR qualitatively assessed pose inference accuracy. Figure \ref{fig:air-comparison} shows pairs of input RGB images and corresponding reconstructions from pose inferences made by 3DP3. Qualitatively, our system produces pose inferences of better or equal accuracy to AIR. Importantly, our system does not require training. In contrast, AIR takes approximately 3 days for training to converge. Also at these lower resolutions, our inference can run in 0.5s per frame.
\begin{figure}[h!]
	\centering
	\includegraphics[width=\textwidth]{paper_figures/air.pdf}
	\caption{%
		A variant of our scene graph inference algorithm that uses a RGB-based likelihood applied to synthetic RGB scenes designed to resemble those used in the evaluation of AIR~\citep{eslami2016attend}.
		Our algorithm gives accurate reconstructions with 0.5 seconds of inference time on these scenes and no training.
	}
	\label{fig:air-comparison}
\end{figure}


\section{Shape Learning Accuracy Quantitative Evaluation}
\label{sec:shape_learning_quantitative_evaluation}

\begin{table}[H]
	%\scriptsize
	\fontsize{7.5pt}{7.5pt}\selectfont
	\setlength{\tabcolsep}{3pt}
	\centering
	\centering
	\begin{tabular}{|l|l|}
		\toprule
Object Type&IoU\\
\midrule
002\_master\_chef\_can & 0.9544  \\
003\_cracker\_box & 0.9716  \\
004\_sugar\_box & 0.9484  \\
005\_tomato\_soup\_can & 0.9433 \\
006\_mustard\_bottle & 0.9671 \\
007\_tuna\_fish\_can & 0.9696 \\
008\_pudding\_box & 0.9617 \\
009\_gelatin\_box & 0.9451 \\
010\_potted\_meat\_can & 0.9654  \\
011\_banana & 0.9599 \\
019\_pitcher\_base & 0.9808 \\
021\_bleach\_cleanser & 0.9582 \\
024\_bowl & 0.9694 \\
025\_mug & 0.9621 \\
035\_power\_drill & 0.966 \\
036\_wood\_block & 0.9679 \\
037\_scissors & 0.9505 \\
040\_large\_marker & 0.9767 \\
051\_large\_clamp & 0.9218 \\
052\_extra\_large\_clamp & 0.9228 \\
061\_foam\_brick & 0.9405 \\
\bottomrule
	\end{tabular}
	\caption{We include a quantitative evaluation comparing the learned shape models to the ground truth shape models. To get a shape model from the learned shape prior, we take all voxels which the prior says are more likely to be occupied than unoccupied and compute the IoU between that volume and the ground truth object volume.}
\end{table}


\section{YCB-Challenging Dataset}

YCB-Challenging is a synthetic test dataset of 2000 RGB-D images, 500 in each of the following 4 categories:

\begin{minipage}[b]{0.48\linewidth}
	\centering
	\setlength{\fboxsep}{0pt}%
	\setlength{\fboxrule}{1pt}%
	{\small \textbf{Single object}: Single object in contact with table}\\
	\fbox{\includegraphics[width=0.33\textwidth]{imgs/scene-types/1_1.png}}%
	\fbox{\includegraphics[width=0.33\textwidth]{imgs/scene-types/1_2.png}}%
	\fbox{\includegraphics[width=0.33\textwidth]{imgs/scene-types/1_3.png}}%
\end{minipage}\hfill%
\begin{minipage}[b]{0.48\linewidth}
	\centering
	\setlength{\fboxsep}{0pt}%
	\setlength{\fboxrule}{1pt}%
	{\small \textbf{Stacked}: Stack of two objects on a table}\\
	\fbox{\includegraphics[width=0.33\textwidth]{imgs/scene-types/2_1.png}}%
	\fbox{\includegraphics[width=0.33\textwidth]{imgs/scene-types/2_2.png}}%
	\fbox{\includegraphics[width=0.33\textwidth]{imgs/scene-types/2_3.png}}%
\end{minipage}\\
~\\
\begin{minipage}[b]{0.48\linewidth}
	\centering
	\setlength{\fboxsep}{0pt}%
	\setlength{\fboxrule}{1pt}%
	{\small \textbf{Partial view}: Single object not fully in field-of-view}\\
	\fbox{\includegraphics[width=0.33\textwidth]{imgs/scene-types/3_1.png}}%
	\fbox{\includegraphics[width=0.33\textwidth]{imgs/scene-types/3_2.png}}%
	\fbox{\includegraphics[width=0.33\textwidth]{imgs/scene-types/3_3.png}}%
\end{minipage}\hfill%
\begin{minipage}[b]{0.48\linewidth}
	\centering
	\setlength{\fboxsep}{0pt}%
	\setlength{\fboxrule}{1pt}%
	{\small \textbf{Partially Occluded}: One object occluded by another}\\
	\fbox{\includegraphics[width=0.33\textwidth]{imgs/scene-types/4_1.png}}%
	\fbox{\includegraphics[width=0.33\textwidth]{imgs/scene-types/4_2.png}}%
	\fbox{\includegraphics[width=0.33\textwidth]{imgs/scene-types/4_3.png}}%
\end{minipage}\\


\newpage


\section{YCB-Challenging Extended Experimental Results}

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.8\textwidth]{imgs/synthetic_results_with_labels.pdf}
	\caption{Accuracy of our method and two deep learning baselines (DenseFusion~\cite{wang2019densefusion} and Robust6D~\cite{tian2020robust}) on the task of 6DoF pose estimation in our synthetic `YCB-Challenging' dataset.
		For each of the 4 scene types (rows) and 5 object types (columns), we measure accuracy for a range of ADD-S thresholds.
		%The deep learning baselines are DenseFusion \cite{wang2019densefusion} and RGDP \cite{tian2020robust}.
		3DP3* denotes an ablated version of our method without inference of the scene graph structure and thus object-object contact (i.e. we fix the scene graph to $G_0$)}
	
	\label{fig:synthetic_accuracy}
\end{figure}



\section{YCB-Video Dataset}


\begin{figure}[h!]
	\centering
	\includegraphics[width=0.75\textwidth]{paper_figures/ycb_schematic.pdf}
	\caption{The YCB-Video \cite{calli2015benchmarking} test data set consists of 2,949 real RGB-D images featuring the 21 YCB objects. These 2,949 images are collected from videos of 12 different scenes where the camera pans around the scene to view it from different perspectives.  The 12 scenes contain different subsets of the 21 YCB objects, and some objects appear in multiple scenes (e.g 007\_tuna\_fish\_can appears in Scenes 1, 2, and 12).}
	\label{fig:ycb_schematic}
\end{figure}


\section{Ablation Qualitative Results on YCB-Video}

\begin{figure}[hp]
	\centering
	\includegraphics[width=\textwidth]{paper_figures/ablation_qualitative.pdf}
	\caption{Comparison with ablated model on YCB-V real scenes. For each of the 21 YCB objects, we show 2 images of scenes containing that object and the poses estimated by our full method (3DP3) and an ablated version of our method (3DP3*) that does not model contact relationships.}
	\label{fig:ycb_ablation_qualitative}
\end{figure}


\section{Qualitative Results on YCB-Video}

\begin{figure}[hp!]
	\centering
	\includegraphics[width=0.75\textwidth]{paper_figures/full_page_1.pdf}
	\caption{YCB frames for each object, overlayed with pose estimates of DenseFusion (cyan) and 3DP3 (green), where there is a large performance difference between the two methods.}
	\label{fig:ycb_full_page_1}
\end{figure}



\begin{figure}[hp!]
	\centering
	\includegraphics[width=0.75\textwidth]{paper_figures/full_page_3.pdf}
	\caption{YCB frames for each object, overlayed with pose estimates of DenseFusion (cyan) and 3DP3 (green), where there is almost no performance difference between the two methods.}
	\label{fig:ycb_full_page_3}
\end{figure}


\newpage


