\section{3DP3 generative modeling framework} \label{sec:model}

\begin{figure}[t]
    \begin{subfigure}[t]{\textwidth}
	\centering
	\includegraphics[width=\textwidth]{paper_figures/inference_new.pdf}
\caption{%
Inferring a hierarchical 3D scene graph from an RGB-D image with 3DP3.
Our model knows that objects often lay flat on other objects, which allows for the depth pixels of one object to inform the pose of other objects.
Our algorithm also infers when this knowledge is relevant (e.g. the clamp on the left, represented by the purple node, is laying flat on the box), and when it is not (e.g. the clamp on the right, represented by the red node, is not laying flat on any other object).
}
    \end{subfigure}
    \begin{subfigure}[t]{\textwidth}
	\centering
\vspace{2mm}
	\includegraphics[width=\textwidth]{paper_figures/probabilistic_program_new.pdf}
\caption{%
3DP3 uses a structured generative model of 3D scenes, represented as a probabilistic program.
The model uses a prior over object shapes that can be learned from data,
a prior over scene structure that is a probability distribution on graphs,
a traversal of the scene graph starting at the world node $r$ to compute object poses,
and a robust likelihood model for depth images.
In the graph at right, the world node $r$ (not shown) is the parent of the grey node (box) and the red node (right clamp) because those objects are not layting flat on other objects.
}
    \end{subfigure}
\caption{%
(a) A scene graph inference task and (b) the 3DP3 generative model.
}
	\label{fig:model}
\end{figure}

The core of 3DP3 is a generative modeling framework that represents a scene in terms of discrete objects, the 3D shape of each object, and a hierarchical structure called a scene graph that relates the poses (position and orientation) of the objects.
This section describes 3DP3's object shape and scene graph latent representations,
a family of prior distributions on these latent representations, and 
an observation model for image-based observations of scenes.
Figure~\ref{fig:model} shows the combined generative model written as a probabilistic program.



\subsection{Objects} \label{sec:objects}

The most basic element of our generative modeling framework are rigid objects.
The first stage in our generative model prior encodes uncertainty about the 3D shape of $M$ types of rigid objects that may or may not be encountered in any given scene.


\paragraph{Voxel 3D object shapes}
We model the coarse 3D shape of rigid objects using a voxel grid with dimensions $h,w,l \in \mathbb{N}$ and cells indexed by $(i,j,\ell) \in [h]\times[w]\times[l]$.
Each cell has dimension $s \times s \times s$ for resolution $s \in \mathbb{R}^{+}$, so that the entire voxel grid represents the cuboid $[0, h\cdot s] \times [0, w\cdot s] \times [0, l\cdot s]$.
All objects are assumed to fit within the cuboid.
An object's \textit{shape} is defined by a binary assignment $\mathbf{O} \in \{0, 1\}^{h \times w \times l}$ of occupancy states to each cell in the voxel grid, where $O_{ij\ell} = 1$ indicates that cell $(i, j, \ell)$ is occupied and $O_{ij\ell} = 0$ indicates it is free.
Each object also has a finite set of \emph{contact planes} through which the object may be in flush contact with the contact planes of other objects in physically stable scenes.
For example, in Figure~\ref{fig:scene-graph}, the table has a contact plane for its top surface, the yellow sugar box has six contact planes, one for each of its six faces, and the bottom contact plane of the sugar box is in flush contact with the top contact plane of the table.
The pose of a contact plane relative to its object is a function of the object shape $\mathbf{O}$.
To simplify notation, we denote the set of contact planes for any object by $F$.


\paragraph{Prior distributions on 3D object shape}
We assume there are $M$ distinct object types, and each object type $m \in \{1, \ldots, M\}$ has an a-priori unknown shape, denoted $\mathbf{O}^{(m)}$.
Let $\mathbf{O}^{(1:M)} := (\mathbf{O}^{(1)}, \ldots, \mathbf{O}^{(M)})$.
The prior distribution on the shape of each object type $m$ is denoted $p(\mathbf{O}^{(m)})$.
Although our inference algorithm (Section~\ref{sec:inference}) only requires the ability to sample jointly from $p(\mathbf{O}^{(1:M)})$, we assume shapes of object types are independent in the prior ($p(\mathbf{O}^{(1:M)}) = \prod_{m=1}^M p(\mathbf{O}^{(m)})$).
Section~\ref{sec:learning-shapes} shows how to learn a specific shape prior $p(\mathbf{O}^{(m)})$ for an object type from depth images.

\subsection{Scenes} \label{sec:scenes} %\label{sec:scenegraph}

Given a collection of $M$ known object types and their shapes, our model generates scenes with $N$ objects by randomly selecting an object type for each object and then sampling a 6DoF object pose for each object.
Instead of assuming that object poses are independent, 
our model encodes an inductive bias about the regularities in real-world scenes:
objects are often resting in flush contact with other objects (e.g. see Figure~\ref{fig:scene-graph}).
We jointly sample \emph{dependent} object poses using a flexible hierarchical scene graph, while maintaining uncertainty over the structure of the graph. 

\begin{figure}[t]
	\centering
	%https://docs.google.com/drawings/d/1DUEfUrhegsgzsQPrfpbTrW4nzR1Eqffa6DFlKpDCwNg/edit?usp=sharing
    \begin{minipage}{0.6\textwidth}
	\includegraphics[width=\textwidth]{paper_figures/fig1.pdf}
    \end{minipage}%
    \begin{minipage}{0.4\textwidth}
    	\tiny
    \begin{tabular}{l|l|l}
Notation & Meaning & Section\\
\hline
$\mathbf{O}^{(m)}$ & Object shape & \ref{sec:objects}\\
$M$ & Number of object types & \ref{sec:objects}\\
$N$ & Number of objects & \ref{sec:scenes}\\
$c_i$ & Type of object $i$ & \ref{sec:scenes}\\
$G = (V, E)$ & Scene graph structure & \ref{sec:scenes}\\
$r \in V$ & World coord. frame & \ref{sec:scenes}\\
$v \in V \setminus \{r\}$ & Object coord. frame & \ref{sec:scenes}\\
$\theta_v$ & Parameters of $v$ & \ref{sec:scenes}\\
$f_v, f_v'$ & Two contact planes & \ref{sec:scenes}\\
$(a_v, b_v, z_v, \phi_v)$ & Planar contact parameters & \ref{sec:scenes}\\
$\mathbf{x}_v \in SE(3)$ & 6DoF pose of $v$ w.r.t. $r$ & \ref{sec:scenes}\\
$\Delta \mathbf{x}_v(\theta_v)$ & 6DoF pose of $v$ w.r.t parent & \ref{sec:scenes}\\
$\tilde{\mathbf{I}}$ & Rendered depth image & \ref{sec:images}\\
$\tilde{\mathbf{Y}}$ & Rendered point cloud & \ref{sec:images}\\
$\mathbf{Y}$ & Observed point cloud & \ref{sec:images}
    \end{tabular}
    \end{minipage}%
	\caption{Our hierarchical scene graphs encode a tree of coordinate frames representing entities in a scene and their geometric relationships (e.g. flush contact between faces of two objects).}
	%Left: . Right: The tree of coordinate frames associated with a scene graph.}
	\label{fig:scene-graph}
\end{figure}



\paragraph{Hierarchical scene graphs}
We model the geometric state of a scene as a \emph{scene graph} $\mathcal{G}$ (Figure~\ref{fig:scene-graph}), which is a tuple $\mathcal{G} = (G, \bm{\theta})$ where
$G = (V, E)$ is a directed rooted tree and $\bm\theta$ are parameters.
The vertices $V:= \{r,v_1,\ldots,v_N\}$ represent $N+1$ 3D coordinate frames, with $r$ representing the world coordinate frame. 
An edge $(u,v) \in E$ indicates that coordinate frame $v$ is parametrized relative to frame $u$, with parameters $\theta_v$.
The 6DoF pose of frame $v$ relative to frame $u$ with pose $\mathbf{x}_u \in SE(3)$ is given by a function $\Delta \mathbf{x}_v$, where $\Delta \mathbf{x}_v(\theta_v) \in SE(3)$ and  $\mathbf{x}_v := \mathbf{x}_u \cdot \Delta \mathbf{x}_v(\theta_v)$.
Here, $\cdot$ is the $SE(3)$ group operation, and the world coordinate frame is defined as the identity element ($\mathbf{x}_r := I$).


\paragraph{Modeling flush contact between rigid objects}
While the vertices of scene graphs $\mathcal{G}$ can represent arbitrary coordinate frames in a scene (e.g. the coordinate frames of articulated joints, object poses), in the remainder of this paper we assume that each vertex $v \in V \setminus \{r\}$ corresponds to the pose of a rigid object.
We index objects by $1, \ldots, N$, with corresponding vertices $v_1, \ldots, v_N$.
We assume that each object $i$ has an object type $c_i \in \{1, \ldots, M\}$.
For vertices $v$ that are children of the root vertex $r$, $\theta_v \in SE(3)$ defines the absolute 6DoF pose of the corresponding object ($\Delta \mathbf{x}_v(\theta_v) = \theta_v$). For vertices $v$ that are children of a non-root vertex $u$, the parameters take the form $\theta_v = (f_v, f_v', a_v, b_v, z_v, \phi_v)$ and represent a contact relationship between the two objects:
$f_v$ and $f_v'$ indicate which contact planes of the parent and child objects, respectively, are in contact.
$(a_v, b_v) \in \mathbb{R}^2$ is the in-plane offset of the origin of plane $f_v$ of object $v$ from the origin of plane $f_v'$ of object $u$.
$z_v \in \mathbb{R}$ is the perpendicular distance of the origin of plane $f_v$ of object $v$ from plane $f_v'$ of object $u$. $\phi_v \in S^2 \times S^1$ represents the deviation of the normal vectors of the two contact planes from anti-parallel (in $S^2$) and a relative in-plane rotation of the two contact planes (in $S^1$).
The relative pose $\Delta \mathbf{x}_v(\theta_v)$ of $v$ with respect to $u$ is the composition (in $SE(3)$) of three relative poses:
(i) $v$ with respect to its plane $f_v$, (ii) $v$'s plane $f_v$ with respect to $u$'s plane $f_v'$, and (iii) $u$'s plane $f_v'$ with respect to $u$.
The 6DoF poses of all objects ($\mathbf{x}_v$ for $v \in V \setminus \{r\}$) are computed by
traversing the scene graph while taking products of relative poses along paths from the root $r$.

\paragraph{Prior distributions on scene graphs}
We now describe our prior on scene graphs, given object models $\mathbf{O}^{(1:M)}$.
We assume the number of objects $N$ in the scene is known (see the supplement for a generalization to unknown $N$).
We first sample the types $c_i \in \{1, \ldots, M\}$ of all objects from an exchangeable distribution $p(\mathbf{c})$ where $\mathbf{c} := (c_1, \ldots, c_N)$.
This includes as a special case distributions where all types are represented at most once among the objects ($\sum_{i=1}^N \mathbf{1}[c_i = c] \le 1$), which is the case in our experiments.
Next, we sample the scene graph structure $G$ from $p(G)$.
We experiment with two priors $p(G)$:
(i) a uniform distribution on the set of $(N+1)^{N-1}$ directed trees that are rooted at a vertex $r$,
and (ii) $\delta_{G_0}(G)$, where $G_0$ is a graph on $N+1$ vertices where $(r, v) \in E$ for all $v \in V \setminus \{r\}$ so that each object vertex has an independent 6DoF pose.
For objects whose parent is $r$ (the world coordinate frame), we sample the pose $\theta_v \sim p_{\mathrm{unif}}$, which samples the translation component uniformly from a cuboid scene extent, and the orientation uniformly over $SO(3)$.
For objects whose parent is another object $u$, we sample the choice of contact planes $(f_v, f_v') \in F \times F$ uniformly, $(a_v, b_v) \sim \mathrm{Uniform}([-50{\small\mbox{cm}}, 50{\small\mbox{cm}}]^2)$,
$z_v \sim \mathrm{N}(0, 1{\small\mbox{cm}})$,
the $S^2$ component of $\phi_v$ from a von Mises--Fisher (vMF) distribution concentrated ($\kappa = 250$) on anti-parallel plane normals,
and the $S^1$ component from $\mathrm{Uniform}(S^1)$.
We denote this distribution $p_{\mathrm{cont}}(\theta_v)$.
Note that the parameters of $p_{\mathrm{cont}}$ were not tuned or tailored in any detailed way---they were chosen heuristically based on the rough dimensions of table-top objects.
The resulting prior over all of the latent variables is:
\begin{align}
&p(\mathbf{O}^{(1:M)}, \mathbf{c}, G, \bm{\theta}) =
\displaystyle \left(\prod_{m=1}^M p(\mathbf{O}^{(m)}) \right)
\frac{1}{(N+1)^{N-1}}
\,
p(\mathbf{c})
%\left(
    \prod_{\substack{v \in V: \\ (r, v) \in E}}
    p_{\mathrm{unif}}(\theta_v)
%\right)
%\left(
    \prod_{\substack{(u, v) \in E: \\ u \neq r}}
    p_{\mathrm{cont}}(\theta_v)
%\right)
%\end{array}
\label{eq:prior}
\end{align}

\subsection{Images} \label{sec:images}

Our generative model uses an observation model that generate synthetic image data
given object shapes $\mathbf{O}^{(1:M)}$ and a scene graph $\mathcal{G}$ containing $N$ objects.
We now describe the observation model for depth images that is used in our main experiments (Section~\ref{sec:experiments}).

% TODO put the RGB likelihood here, or in the supplement?

\paragraph{Likelihood model for depth images}

We first convert an observed depth image into a point cloud $\mathbf{Y}$.
To model a point cloud $\mathbf{Y} \in \mathbb{R}^{K \times 3}$ with $K$ points denoted $\mathbf{y}_i \in \mathbb{R}^3$, we use a likelihood model based on rendering a synthetic depth image of the scene graph.
Specifically, given the object models $\mathbf{O}^{(m)}$ for each $m \in \{1, \ldots, M\}$, the object types $\mathbf{c}$, the scene graph $\mathcal{G}$, and the camera intrinsic and extrinsic parameters relative to the world frame,
we (i) compute meshes from each $\mathbf{O}^{(m)}$, (ii) compute the 6DoF poses ($\mathbf{x}_v$) of objects with respect to the world frame by traversing the scene graph $\mathcal{G}$, 
and (iii) render a depth image $\tilde{\mathbf{I}}$ of $\mathcal{G}$ using an OpenGL depth buffer, and
(iv) unproject the rendered depth image to obtain a point cloud $\tilde{\mathbf{Y}}$
with $\tilde{K}$ points ($\tilde{K}$ is the number of pixels in the depth image).
We then generate an observed point cloud $\mathbf{Y} \in \mathbb{R}^{K \times 3}$ by drawing each point from a mixture:
\begin{alignat}{2}
&p(\mathbf{Y} | \mathbf{O}^{(1:M)}, \mathbf{c}, G, \bm{\theta}) := \prod_{i = 1}^{K} \left( C \cdot \frac{1}{B} + \frac{1-C}{\tilde{K}} \sum_{j=1}^{\tilde{K}}
\frac{ \mathbf{1}[||\mathbf{y}_i - \tilde{\mathbf{y}}_{j}||_2 \leq r]}{\frac{4}{3} \pi r^3}\right)
\label{eq:likelihood}
\end{alignat}
for some $0 < C < 1$ and some $r > 0$.
The components of this mixture are uniform distributions over the balls of radius $r$ centered at each point in $\tilde{\mathbf{Y}}$ (with weights $(1-C)/\tilde{K}$) and
a uniform distribution over the scene bounding volume $B$ (weight $C$).%
\footnote{By using a distribution that is uniform over a small, spherical region rather than a Gaussian distribution, we avoid (via k-d trees) computing pairwise distances between all points in $\mathbf{Y}$ and $\tilde{\mathbf{Y}}$, resulting in  $\approx10$x speedup.}

