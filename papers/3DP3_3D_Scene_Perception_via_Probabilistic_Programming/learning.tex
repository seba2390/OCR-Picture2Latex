\section{Learning object shape models} \label{sec:learning-shapes}


\begin{figure}[!t]
	\centering
	\includegraphics[width=\textwidth]{paper_figures/real_data_shape_acquisition.pdf}
	\caption{%
		%Learning shape models. \textcolor{red}{update caption}
		Learning a voxel-based shape models $p(\mathbf{O}^{(m)})$ for a novel object from a set of 5 depth images.
		Our shape priors capture uncertainty about voxel occupancy due to self-occlusion (right).
	}
	\label{fig:shapes}
\end{figure}

3DP3 does not require hard-coded shape models. Instead, it uses
probabilistic inference to learn non-parametric models of 3D object
shape $p(\mathbf{O}^{(m)})$ that account for uncertainty due to
self-occlusion. We focus on the restricted setting of learning from
scenes containing a single isolated object ($N=1)$ of known type
($c_1$). Our approach works best for views that lead to minimal
uncertainty about the exterior shape of the object; more general,
flexible treatments of shape learning and shape uncertainty are beyond
the scope of this paper.

First, we group the depth images by the object type ($c_1$), so that
we have $M$ independent learning problems.  Let $\mathbf{I}_{1:T} :=
(\mathbf{I}_1, \ldots, \mathbf{I}_T)$ denote the depth observations
for one object type, with object shape denoted $\mathbf{O}$.  The
learning algorithm uses Bayesian inference in another generative model
$p'$.  The posterior $p'(\mathbf{O} | \mathbf{I}_{1:T})$ produced by
this algorithm becomes the prior $p(\mathbf{O})$ used in
Section~\ref{sec:objects}.

We start with a uninformed prior distribution 
$p'(\mathbf{O}) := \prod_{i=1}^h \prod_{j=1}^w \prod_{\ell=1}^l p_{\mathrm{occ}}^{O_{ij\ell}} (1 - p_{\mathrm{occ}})^{(1 - O_{ij\ell})}$
on the 3D shape of an object type,
for a per-voxel occupancy probability $p_{\mathrm{occ}}$ (in our experiments, 0.5).
We learn about the object's shape by observing a sequence of depth images $\mathbf{I}_{1:T}$ that contain views of the object, which is assumed to be static relative to other contents of the scene, which we call the `map' $\mathbf{M}$.
(In our experiments the map contains the novel object, a floor, a ceiling, and four walls of a rectangular room).
We posit the following joint distribution over object shape ($\mathbf{O}$) and the observed depth images,
conditioned on the map ($\mathbf{M}$) and the poses of the camera relative to the map over time ($\mathbf{x}_1, \ldots, \mathbf{x}_T \in SE(3)$): $p'(\mathbf{O}, \mathbf{I}_{1:T} | \mathbf{M}, \mathbf{x}_{1:T})
:=
p'(\mathbf{O})
\prod_{t=1}^T p'(\mathbf{I}_t | \mathbf{O}, \mathbf{M}, \mathbf{x}_t)$.


The likelihood $p'$ is a 
depth image likelihood on a latent 3D voxel occupancy grid (see supplement for details).
For this model, we can compute $p'(\mathbf{O} | \mathbf{M}, \mathbf{x}_{1:T}, \mathbf{I}_{1:T})
= \prod_{ij\ell} p'(O_{ij\ell} | \mathbf{M}, \mathbf{x}_{1:T}, \mathbf{I}_{1:T})
$ exactly using ray marching to decide if a voxel cell is occupied, unoccupied, or unobserved (due to being occluded by another occupied cell), and the resulting distribution on $\mathbf{O}$
can be compactly represented as an array of probabilities ($\in [0, 1]^{h \times w \times l}$).
However, in real-world scenarios the map $\mathbf{M}$ and the camera poses $\mathbf{x}_{1:T}$ are not known with certainty.
To handle this, our algorithm takes as input uncertain beliefs
about $\mathbf{M}$ and $\mathbf{x}_{1:T}$
($q_{\mathrm{SLAM}}(\mathbf{M}, \mathbf{x}_{1:T}) \approx p'(\mathbf{M}, \mathbf{x}_{1:T} | \mathbf{I}_{1:T})$)
that are produced by
a separate probabilistic SLAM (simultaneous localization and mapping) module,
and take the form of a weighted collection of $K$ particles $(\mathbf{M}^{(k)}, \mathbf{x}_{1:T}^{(k)})$: $q_{\mathrm{SLAM}}(\mathbf{M}, \mathbf{x}_{1:T}) = \sum_{k=1}^K w_k \delta_{\mathbf{M}^{(k)}}(\mathbf{M}) \delta_{\mathbf{x}_{1:T}^{(k)}}(\mathbf{x}_{1:T})$.
Various approaches to probabilistic SLAM can be used; we implemented it
using sequential Monte Carlo (SMC) in Gen (more detail in supplement).
From the beliefs $q_{\mathrm{SLAM}}(\mathbf{M}, \mathbf{x}_{1:T})$ produced by SLAM, we approximate the object shape posterior via:
\[
\hat{p}'(\mathbf{O} | \mathbf{I}_{1:T})
:= \iint p'(\mathbf{O} | \mathbf{M}, \mathbf{x}_{1:T}, \mathbf{I}_{1:T}) q_{\mathrm{SLAM}}(\mathbf{M}, \mathbf{x}_{1:T}) d\mathbf{M} d\mathbf{x}_{1:T}
= \sum_{k=1}^K w_k p'(\mathbf{O} | \mathbf{M}^{(k)}, \mathbf{x}_{1:T}^{(k)}, \mathbf{I}_{1:T})
\]
Note that while $p'(\mathbf{O} | \mathbf{M}^{(k)}, \mathbf{x}_{1:T}^{(k)}, \mathbf{I}_{1:T})$ for each $k$
can be compactly represented,
the mixture distribution $\hat{p}'(\mathbf{O} | \mathbf{I}_{1:T})$ lacks the conditional independencies that make this possible.
%However, this object shape posterior does not have an analytic form and thus would require us to store the full particle approximation (from SLAM) and all depth images for each object we ever observe.
To produce a more compact representation of beliefs about the object's shape,
we fit a variational approximation $q_{\varphi}(\mathbf{O})$
that assumes independence among voxels 
($q_{\varphi}(\mathbf{O}) := \prod_{i\in[h]} \prod_{j\in[w]} \prod_{\ell\in[l]} \varphi_{ij\ell}^{O_{ij\ell}} \cdot (1 - \varphi_{ij\ell})^{(1-O_{ij\ell})}$) to $\hat{p}'(\mathbf{O} | \mathbf{I}_{1:T})$
using $\varphi^* := \argmin_{\varphi} \mathrm{KL}(\hat{p}'(\mathbf{O} | \mathbf{I}_{1:T}) || q_{\varphi}(\mathbf{O}))$
(see supplement for details).
This choice of variational family is sufficient for representing uncertainty about the occupancy of voxels in the \emph{interior} of an object shape.
Note that our shape-learning experiments did not result in significant uncertainty about the \emph{exterior} shape of objects%
\footnote{%
The lack of significant exterior shape uncertainty in shape-learning experiments allowed us to
implement an optimization:
Instead of the relative poses of an object's contact planes depending on $\mathbf{O}$ as described in Section~\ref{sec:model},
we assign each object type a set of six contact planes derived from the faces of the smallest axis-aligned bounding cuboid that completely contains all occupied voxels in one sample $\mathbf{O}$ from the learned prior $p(\mathbf{O}) := q_{\bm{\varphi}}(\mathbf{O})$.
},
and in the presence of such uncertainty, a less severe variational approximation may be needed for robust inference of scene graphs from depth images.
Fig. \ref{fig:shapes} shows input depth images ($\mathbf{I}_{1:T}$) and resulting shape prior learned from $T=5$ observations.
After learning these shape distributions $q_{\bm{\varphi}}(\mathbf{O}) \approx \hat{p}'(\mathbf{O} | \mathbf{I}_{1:T})$ for each distinct object type, we use them as the shape priors $p(\mathbf{O}_i)$ within the generative model of Section~\ref{sec:model}. The supplement includes the results of a quantitative evaluation of the accuracy of shape learning.

