\section{Introduction}
\vspace{-1mm} 
A striking feature of human visual intelligence is our ability to
learn representations of novel objects from a limited amount of data
and then robustly percieve 3D scenes containing those objects. We can
immediately generalize across large variations in viewpoint,
occlusion, lighting, and clutter. How might we develop computational vision
systems that can do the same?

This paper presents a generative model for 3D scene perception, called
3DP3.  Object shapes are learned via probabilistic inference in a
voxel occupancy model that coarsely captures 3D shape and uncertainty
due to self-occlusion (Section \ref{sec:learning-shapes}). Scenes are modeled via hierarchical 3D scene
graphs that can explain planar contacts between objects without forcing
scenes to fit rigid structual assumptions (Section \ref{sec:model}). Images are modeled by
real-time graphics and robust likelihoods on point clouds. We cast 3D scene understanding as approximate probabilistic
inference in this generative model.  We develop a novel inference
algorithm that combines data-driven Metropolis-Hastings kernels over
object poses, involutive MCMC kernels over scene graph structure,
pseudo-marginal integration over uncertain object shape, and existing
deep learning object detectors and pose estimators (Section \ref{sec:inference}). This architecture
leverages inference in the generative model to provide common sense
constraints that fix errors made by bottom-up neural detectors. Our
experiments show that 3DP3 is more accurate and robust than deep
learning baselines at 6DoF pose estimation for challenging synthetic
and real-world scenes (Section \ref{sec:experiments}). Our model and inference algorithm are implemented in the
Gen~\citep{cusumano2019gen} probabilistic programming system.