% !TEX root = ../CRCBookPlanning.tex


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Init
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\graphicspath{{./planning/figures/}}



\chapter{Behavior Trees and Automated Planning}
\label{ch:planning}


In this chapter, we describe how automatic planning can be used to create BTs, exploiting ideas from~\cite{florez2008dynamic, weber2011building, weber2010reactive, colledanchise2016towards}.
First, in Section~\ref{planning.robotics},  we present an extension of the Backchaining design principle, introduced in Section~\ref{design:sec:back_chaining}, including a robotics example. Then we present an alternative approach using A Behavior Language (ABL), in Section~\ref{planning.MS}, including a game example.

In classical planning research, the world is often assumed to be static and known, with
all changes occurring as a result of the actions executed by one controlled agent  \cite{ghallab2016automated}.
Therefore, most approaches return a static plan, i.e. a sequence of actions that brings the system from the initial state to the goal state, with a corresponding execution handled by a classical FSM. 

However, many agents, both real and virtual, act in an uncertain world populated by
 other agents, each with their own set of goals and objectives. 
  Thus, the effect of an action can be unexpected, diverging from the planned state trajectory, making the next planned action infeasible.
  A common way of handling this problem is to  re-plan from scratch on a regular basis, which can be expensive both in terms of time and computational load.
 To address these problems, the following two open challenges were
 identified within the planning community~\cite{Ghallab14}:
%\cite{Nau15Challenges}
\begin{itemize}
\item \say{Hierarchically organized deliberation. This principle goes beyond existing hierarchical planning techniques; its requirements and scope are significantly different. The actor performs its deliberation online}

\item \say{Continual planning and deliberation. The actor monitors, refines, extends, updates, changes and repairs its plans throughout the acting process, using both descriptive and operational models of actions.}
\end{itemize}
Similarly, the recent book \cite{ghallab2016automated} describes the need for an actor that 
\say{reacts to events and extends, updates, and repairs its plan on the basis of its perception}.

Combining planning with BTs is one way of addressing these challenges.
 The reactivity of BTs
enables the agent to re-execute previous subplans without having
to replan at all, and the modularity enables extending the plan
recursively, without having to re-plan the whole task. 
Thus, using BTs as the control architecture in an automated planning algorithm  addresses the above challenges by enabling a reasoning process that is both hierarchical and modular in its deliberation, and can monitor, update, and extend its plans while acting.

In practice, and as will be seen in the examples below,
using BTs enables reactivity, in the sense that if an object slips out of a robot's gripper, the robot will automatically stop and pick it up again without the need to replan or change the BT, see Fig.~\ref{planning:SI.fig.yousimplescreen}.
Using BTs also enables iterative plan refinement, in the sense that if an object is moved to block the path, the original BT can be extended to include a removal of the blocking obstacle. Then, if the obstacle is removed by an external actor, the robot reactively skips the obstacle removal, and goes on to pick up the main object without having to change the BT, see Fig.~\ref{planning:DSI.fig.youscreen2bis}. 



% the use of BTs in automated planning applied in multi-scale games (Section~\ref{planning.MS}) and robotics (Section~\ref{planning.robotics}). Part of this chapter was published in. For  multi-scale games, we will introduce the approach using the simple Pac-Man example of Section~\ref{sec:pacman} \todopetter{the numbering is shown when the book is compiled as a whole} and then we will show the result using the Blizzard's video game \emph{StarCraft} since it is played at a professional level and a large amount of data is available for evaluating the approach. For robotics applications,  we will introduce the approach using the simple YouBot example of Section~\ref{sec:youbot} and then we will show the results using more complex scenarios.
%
%

\section{The Planning and Acting (PA-BT) approach}
\label{planning.robotics}
In this section, we describe an extension of the Backchaining approach, called \emph{Planning and Acting using Behavior Trees} (PA-BT). 

%
%Task planning problems in robotics have additional challenges to the ones seen in the previous section.
%Robotic systems, especially the ones that shares its environment with humans, must be \emph{reactive, deliberative, fault-tolerant} and  \emph{safe},
%whether it is in a collaborative manufacturing setting, or in a home providing household services.
%It must be  \emph{reactive} in the sense that if it drops an object, it picks it up again, if another agent places an item in its path, it removes it,
%if another agent helps it by doing a subtask, it notices and adapts.
%It must be  \emph{deliberative} in the sense that it is able to reason about consequences of actions, and executing a set of actions that leads to the satisfaction of its goal.
%It must be  \emph{fault-tolerant}, in the sense that if a fault prevents it from performing an action in a particular way, it tries to adapt by using an alternative approach.
%Finally, it must be  \emph{safe} in the sense that there can be guarantees for avoiding a few critical events (such as falling down the stairs or hurting someone) while still allowing freedom for the deliberation to find new ways of reaching the goals.
%
%Hence, pre-defining the sequence of actions to achieve a task is unfeasible for robotics applications. To tackle this problem, robotics community has considered blending acting and planning in a single reasoning process.  
%To illustrate the need for blending reactive planning and acting, we consider the following simple example, depicted in Figure~\ref{planning:IN.fig.front}.
%A robot must plan and execute the actions needed to pick up an object and place it in a given location. The environment is however dynamic and unpredictable. After pickup, the object might slip out of the robot gripper, or, as shown in Figure~\ref{planning:IN.fig.front}(b), external objects might move and block the path to the goal location. These type of problems led the AI community to the assertion of two key open challenges~\cite{Ghallab14}:
%%\cite{Nau15Challenges}
%\begin{itemize}
%\item \say{Hierarchically organized deliberation. This principle goes beyond existing hierarchical planning techniques; its requirements and scope are significantly different. The actor performs its deliberation online}
%
%\item \say{Continual planning and deliberation. The actor monitors, refines, extends, updates, changes and repairs its plans throughout the acting process, using both descriptive and operational models of actions.}
%\end{itemize}
%Similarly, the recent book \cite{ghallab2016automated} describes the need for an actor that 
%\say{reacts to events and extends, updates, and repairs its plan on the basis of its perception}.
%Finally,  \cite{ghallab2016automated} also notes that most of the current work in action planning yields a static plan, i.e. a sequence of actions that brings the system from the initial state to the goal state. Its execution is usually represented as a classical FSM. However, due to changes in the environment, the effect of an action can be unexpected. This may lead to situations where the agent re-plans from scratch on a regular basis, which can be expensive in terms of both time and computational load. At this point of the book, it is evident that the use of BTs may be helpful in addressing the aforementioned challenges: the reactivity of BTs
%enables the agent to re-execute previous plans without having
%to replan at all. The modularity enables extending the plan
%recursively, without having to re-plan the whole task. Thus, using BTs as the CA in an automated planning algorithm, allows having an autonomous agent whose reasoning process is hierarchical and modular in its deliberation, and it does monitor, update, and extend its plans while acting.
%The BT includes reactivity, in the sense that if the object slips out of the robot gripper, the robot will automatically stop and pick it up again without the need to replan or change the BT.
%The BT also supports iterative plan refinement, in the sense that if an object moves to block the path, the original BT is extended to include a removal of the blocking obstacle. Then, if the obstacle is removed by an external actor, the robot reactively skips the obstacle removal, and goes on to pick up the main object without having to change the BT. 


% Finally, \cite{Nau15Challenges}  states that the AP community has underestimated the challenges of integrating acting and planning, appealing for a further investigation for novel techniques to make a wider impact on practical problems.

 
%We believe that BTs can provide an important step towards addressing these challenges. {In particular, we show that BTs enable  blending planning and acting in a way that is  \emph{reactive}, \emph{hierarchical}, \emph{safe}, and \emph{fault tolerant}. The proposed interleaved plan-and-act process is similar to the one of \emph{Hierarchical Planning in the Now} (HPN)~\cite{kaelbling2011hierarchical} with the addition of improved reactivity, safety and fault tolerance.}



PA-BT was inspired by the Hybrid Backward-Forward (HBF) algorithm, a task planner for dealing with infinite state spaces~\cite{garrettbackward}. 
The HBF algorithm has been shown  to efficiently solve problems with large state spaces.
Using an HBF algorithm we can
 refine the acting process by mapping the \emph{descriptive} model of actions, which describes \emph{what} the actions do, onto an \emph{operational} model, which defines \emph{how} to perform an action in certain circumstances.



The PA-BT framework combines the planning capability in an infinite state space from HBF
with the advantages of BTs compared to FSMs in terms of \emph{reactivity} and \emph{modularity}.
Looking back at the example above, the \emph{reactivity} of BTs enables the robot to pick up a dropped object without 
having to replan at all. The \emph{modularity} enables extending the plan by adding actions for handling the blocking sphere, without having to
replan the whole task. Finally, when the sphere moves away, once again the \emph{reactivity} enables the correct execution without
changing the plan.
Thus, PA-BT is indeed hierarchical and modular in its deliberation, and it does monitor, update and extend its plans while acting,
addressing the needs described in~\cite{ghallab2016automated, Ghallab14}.
The interleaved plan-and-act process of PA-BT is similar to the one of \emph{Hierarchical Planning in the Now} (HPN)~\cite{kaelbling2011hierarchical} with the addition of improved reactivity, safety and fault-tolerance.



%
%\subsection{The PA-BT Approach}
%\label{planning:sec:proposed_approach}
%In this section we present the PA-BT approach, which is an extension and formalization of the Backchaining ideas described in Section~\ref{sec:design:back_chaining}. 

The core concept in BT Backchaining was to replace a condition by a small BT achieving that condition,
on the form of a PPA BT (see Section~\ref{design:sec:back_chaining}), as shown in Figure~\ref{planning:fig:back_chaining_general}.

\begin{figure}[h]
\centering
\includegraphics[width=\columnwidth]{../../design/back_chaining_general}
\caption{Copy of Figure~\ref{design:fig:back_chaining_general}. The general format of a PPA BT. The Postcondition $C$ can be achieved by either one of actions $A_1$ or $A_2$, which have Preconditions $C_{1i}$ and $C_{2i}$ respectively.)}
\label{planning:fig:back_chaining_general}
\end{figure}





%The approach assumes real-world properties for the environment and robot, as described in~\cite{colledanchise16towards}. 




To get familiar with PA-BT, we look at a simple planning example. 
The planning algorithm iteratively creates the BTs in Figure~\ref{planning:PA.fig.it1to4} 
with the final BT used  in the example in            
 Figure~\ref{planning:PA.fig.it4}. The setup is shown in Figure~\ref{planning:IN.fig.front}.
 We can see how  each of the BTs in the figure is the result of applying the PPA expansion to a condition in the previous BT.
 However, a number of details needs to be taken care of, as explained in sections below.


\begin{figure}[t]

        \centering
          \begin{subfigure}[b]{0.25\columnwidth}
                \centering
\includegraphics[width=1\columnwidth]{simpleExampleIteration0.pdf}
                \caption{The initial BT.}
                \label{planning:PA.fig.it0}
        \end{subfigure}%
        
        \begin{subfigure}[b]{0.5\columnwidth}
                \centering
\includegraphics[width=1\columnwidth]{simpleExampleIteration1.pdf}
                \caption{The BT after one iteration.}
                \label{planning:PA.fig.it1}
        \end{subfigure}%
       ~ %add desired spacing between images, e. g. ~, \quad, \qquad etc.
          %(or a blank line to force the subfigure onto a new line)
        \begin{subfigure}[b]{0.5\columnwidth}
                \centering
\includegraphics[width=1\columnwidth]{simpleExampleIteration2.pdf}
                \caption{The BT after two iterations.}
                \label{planning:PA.fig.it2}              
        \end{subfigure}
        ~ %add desired spacing between images, e. g. ~, \quad, \qquad etc.
          %(or a blank line to force the subfigure onto a new line)

        \centering
        \begin{subfigure}[b]{0.45\columnwidth}
                \centering
\includegraphics[width=1\columnwidth]{simpleExampleIteration3.pdf}
                \caption{The BT after three iterations.}
                \label{planning:PA.fig.it3}
        \end{subfigure}%
       ~ %add desired spacing between images, e. g. ~, \quad, \qquad etc.
          %(or a blank line to force the subfigure onto a new line)
        \begin{subfigure}[b]{0.55\columnwidth}
                \centering
\includegraphics[width=1\columnwidth]{simpleExampleIteration4.pdf}
                \caption{The BT after four iterations, the final version}
                \label{planning:PA.fig.it4}              
        \end{subfigure}
        
        \caption{BT updates during the execution.}
         \label{planning:PA.fig.it1to4}              
        ~ %add desired spsequential composition analysisacing between images, e. g. ~, \quad, \qquad etc.
          %(or a blank line to force the subfigure onto a new line)        

\end{figure}


\begin{figure}[t!]
    \centering
    \begin{subfigure}[t]{0.485\columnwidth}
        \centering
\includegraphics[width = \columnwidth]{Scenario1Top.jpg}
        %\caption{The nominal plan is: \emph{MoveTo(C)$\to$Pick(C)$\to$MoveTo(G)} 
        %\emph{$\to$Drop()}.\\  The sphere moves in front of the goal before the robot reaches it. The robot needs a new plan that pushes the sphere away.}
         \caption{}
    \end{subfigure}%
    ~ 
    \begin{subfigure}[t]{0.485\columnwidth}
        \centering
\includegraphics[width = \columnwidth]{Scenario2Top.jpg}
        %\caption{The nominal plan is: \emph{MoveTo(S)$\to$Push(S)$\to$MoveTo(C)} 
        %\emph{$\to$Pick(C)$\to$MoveTo(G)$\to$Drop()}. The sphere moves  away from the goal before the robot is able to push it. The execution of \emph{ MoveTo(S)} and \emph{Push(S)} is no longer required.}
   \caption{}
    \end{subfigure}
    \vspace{1em}
    \caption{
    A simple example scenario where the
    goal is to place the green cube $C$ onto the goal region $G$. The fact that the sphere $S$  is suddenly moved (red arrow) by an external agent to  block the path must be handled.
    In (a) the nominal plan is to \emph{MoveTo(c)$\to$Pick(c)$\to$MoveTo(g)$\to$Drop()} when the sphere suddenly moves to block the path. 
    In (b), after refining the plan, the extended plan is to \emph{MoveTo(s)$\to$Push(s)$\to$MoveTo(c)} 
        \emph{$\to$Pick(c)$\to$MoveTo(g)$\to$Drop()} when the sphere is again suddenly moved by another agent, before being pushed. Thus our agent must smoothly revert to the original set of actions. PA-BT does this without re-planning. Note that when $S$ appears, $\tau_\sigma \subset C_{collFree}$ returns Failure and the BT in Figure~\ref{planning:PA.fig.it4} is expanded further, see Example~\ref{planning:PA.exa.complex},  to push it out of the way. }
    \label{planning:IN.fig.front}
\end{figure}


\begin{example}
\label{planning:PA.exa.simple}

The robot in Figure~\ref{planning:IN.fig.front} is given the task to move the green cube into the rectangle marked GOAL
(the red sphere is handled in Example~\ref{planning:PA.exa.complex} below, in this inital example it is ignored).
The BT  in Figure~\ref{planning:PA.fig.it4} is executed, and in  each time step the root of the BT is ticked. The root is a Fallback node, which ticks is first child, the condition $o_c \in GoalRect$ (cube on goal). If the cube is indeed in the rectangle we are done, and the BT returns Success.

If not, the second child, a Sequence node, is ticked. The node ticks its first child, which is a Fallback, which again ticks its first child, the condition 
$h = c$ (object in hand is cube). If the cube is indeed in the hand, the Condition node returns Success, its parent, the Fallback node returns Success, and its parent, the Sequence node ticks its second child, which is a different Fallback, ticking its first child which is the condition $o_r \in \mathcal{N}_{p_g}$ (robot in the neighborhood of $p_g$). If the robot is in the neighborhood of the goal, the condition and its parent node (the Fallback) returns Success, followed by the sequence ticking its third child, the action $Place(c,p_g)$ (place cube in a position $p_g$ on the goal), and we are done.

If $o_r \in \mathcal{N}_{p_g}$ does not hold, the action $MoveTo(p_g,\tau_g)$ (move to position $p_g$ on the goal region using the trajectory $\tau_g$) is executed, given that the trajectory is free $\tau \subset C_{ollFree}$.
Similarly, if the cube is not in the hand, the robot does a $MoveTo(p_c,\tau_c)$ (move to cube, using the trajectory $\tau_c$) followed by a $Pick(c)$ after checking that the 
hand is empty, the robot is not in the neighborhood of $c$ and that the
corresponding trajectory is free.

We conclude the example by noting that the BT is ticked every timestep, e.g. every 0.1 second. Thus, when actions return Running (i.e. they are not finished yet) the return status of Running is progressed up the BT and the corresponding action is allowed to control the robot. However, if e.g., the cube slips out of the gripper, the condition $h = c$ instantly returns Failure, and the robot starts checking if it is in the neighborhood of the cube or if it has to move before picking it up again.
\end{example}




We are now ready to study PA-BT in detail. The approach is described in Algorithms~\ref{planning:PA.alg.main} (finding what condition to replace with a PPA) and~\ref{planning:PA.alg.update} (creating the PPA and adding it to the BT).
First we will give an overview of the algorithms and see how they are applied to the robot in Figure~\ref{planning:IN.fig.front},
to iteratively create the BTs of Figure~\ref{planning:PA.fig.it1to4}.
We will then discuss the key steps in more detail.


\begin{algorithm2e}[h!]
\caption{Main Loop, finding conditions to expand and resolving conflicts} % input($\mathcal{P}$)}
 \label{planning:PA.alg.main}
\DontPrintSemicolon
%$\mathcal{T} \gets \emptyset$
%
%$\Gamma \gets \mathcal{C}_{goal}$
\SetKwProg{myalg}{algorithm2e}{}{}
 $\mathcal{T} \gets \emptyset$ \\
\For{$c$ in $\mathcal{C}_{goal}$} {
 $\mathcal{T} \gets $SequenceNode($\mathcal{T}$, $c$)}
\While{True\label{planning:PA.alg.main.while}}{
$T\gets$\FuncSty{RefineActions($\bt$)} \label{planning:PA.alg.main.refine}\\
  \Do{\label{planning:PA.alg.main.do} $r \neq \mbox{\emph{Failure}}$ \label{planning:PA.alg.main.fail}} {
    $r \gets$ \FuncSty{Tick($T$)} \label{planning:PA.alg.main.fail2}}
 $c_f \gets$ \FuncSty{GetConditionToExpand($\bt$)}  \label{planning:PA.alg.expand.getcon}\\
 $\mathcal{T}, \mathcal{T}_{new\_subtree}\gets$ \FuncSty{ExpandTree($\bt$,$c_f$)}\label{planning:PA.alg.main.expand} \\
   \While{ $ \mbox{Conflict}(\bt)$  \label{planning:PA.alg.main.feas}} {  $\mathcal{T}\gets$ \FuncSty{IncreasePriority($\mathcal{T}_{new\_subtree}$)} \label{planning:PA.alg.main.incprio} } }
 
\end{algorithm2e}

\begin{algorithm2e}[h]
\caption{Behavior Tree Expansion, Creating the PPA}
      \label{planning:PA.alg.update}
  \SetKwFunction{algo}{ExpandTree}
  \SetKwFunction{proc}{proc}
  \SetKwProg{myalg}{Function}{}{}
  \myalg{\algo{$\mathcal{T}$, $c_f $}}{

      % $C_f \gets$ \FuncSty{GetConditionNodes($\mathcal{T}_f$) }\\
	%$\mathcal{T}_{new} \gets \emptyset$ \\

 $A_T \gets $ \FuncSty {GetAllActTemplatesFor($c_f$) \label{planning:PA.alg.expand.getact}}  \\
 	$\mathcal{T}_{fall} \gets c_f$\\ 
\For{ $a$ in $A_T$} {
%\LState $a \gets$ BindVariables($a_t$,$c_f$)
	$\mathcal{T}_{seq} \gets \emptyset$\\
\For{ $c_a$ in $a.con$}{
 $\mathcal{T}_{seq} \gets $ SequenceNode($\mathcal{T}_{seq}$,$c_a$)   \\  
}
 $\mathcal{T}_{seq} \gets $ SequenceNode($\mathcal{T}_{seq}$,$a$) \\ 
 $\mathcal{T}_{fall} \gets $ FallbackNodeWithMemory($\mathcal{T}_{fall}$,$\mathcal{T}_{seq}$) \\ 
}
 %$\mathcal{T}_{new} \gets$ Sequence $(\mathcal{T}_{new}, \mathcal{T}_{fall}) $


 $\mathcal{T} \gets$  Substitute($\mathcal{T}$,$c_f$,$\mathcal{T}_{fall}$)\\
 \Return{$\mathcal{T}$, $\mathcal{T}_{fall}$}
 }
 
 \end{algorithm2e}

\begin{algorithm2e}[h]


\caption{Get Condition to Expand}
      \label{planning:PA.alg.getcond}
  \SetKwFunction{algo}{GetConditionToExpand}
  \SetKwFunction{proc}{proc}
  \SetKwProg{myalg}{Function}{}{}
  \myalg{\algo{$\mathcal{T}$}}{
	\For{$c_{next} $ in $\FuncSty{GetConditionsBFS()}$}
	{
		\If{$c_{next}.status =\mbox{\emph{Failure}}$ \textbf{and} $c_{next} \notin \mbox{ExpandedNodes}$ \label{planning:PA.alg.getcond.notexpanded}}
		{
		\mbox{ExpandedNodes}.\FuncSty{push\_back($c_{next}$)}
			\Return{$c_{next}$}		
		}
	} 
	\Return{$None$}\label{planning:PA.alg.getcond.nocond}
 } 
 


\end{algorithm2e}

{
\begin{remark}
Note that the conditions of an action template can contain a disjunction of propositions. This can be encoded by a Fallback composition of the corresponding Condition nodes.
\end{remark}
}





\subsection{Algorithm Overview}
Running Algorithm~\ref{planning:PA.alg.main} we have the set of goal constraint  $\mathcal{C}_{goal} = \{o_c \in \{\mbox{GoalRect}\} \}$, thus the initial BT
is composed of a single condition
 $\bt = (o_c \in \{\mbox{GoalRect}\})$, as shown in Figure~\ref{planning:PA.fig.it0}.
 The first iteration of the loop starting on Line~\ref{planning:PA.alg.main.while} of Algorithm~\ref{planning:PA.alg.main} now produces the next BT shown in Figure~\ref{planning:PA.fig.it1}, the second iteration produces the BT in Figure \ref{planning:PA.fig.it2} and so on until the final BT in Figure \ref{planning:PA.fig.it4}.

In detail, at the initial state, running $\bt$ on Line 7 % \ref{planning:PA.alg.main.fail2} 
returns a Failure, since the cube is not in the goal area. Trivially, the \emph{GetConditionToExpand} returns  $c_f=(o_c \in \{\mbox{GoalRect}\})$,
and a call to ExpandTree (Algorithm~\ref{planning:PA.alg.update}) is made on Line \ref{planning:PA.alg.main.expand}. 
On Line~2 of Algorithm~\ref{planning:PA.alg.update} we get  all action templates that satisfy $c_f$ i.e. $A_T=Place$.
Then on Line 7 and 8 a Sequence node $\bt_{seq}$ is created of the conditions of $Place$ (the hand holding the cube, $h = c$, and the robot being near the goal area, $o_r \in \mathcal{N}_{p_g}$) and $Place$ itself.
On Line 9 a Fallback node $\bt_{seq}$ is created of $c_f$ and the sequence above.
Finally, a BT is returned where this new sub-BT is replacing $c_f$.
The resulting BT is shown in Figure~\ref{planning:PA.fig.it1}. 

Note that Algorithm~\ref{planning:PA.alg.update} describes \emph{the core principle} of the PA-BT approach. \emph{A condition is replaced by the corresponding PPA, a check if the condition is met, and an action 
to meet it. The action is only executed if needed.} If there are several such actions, these are added in Fallbacks {with memory}. Finally, the action is preceded by conditions checking its own preconditions. If needed,
these conditions will be expanded in the same way in the next iteration.

Running the next iteration of Algorithm~\ref{planning:PA.alg.main}, a similar expansion of the condition $h = c$ transforms the BT in Figure~\ref{planning:PA.fig.it1} to the BT in Fig.~\ref{planning:PA.fig.it2}.
Then, an expansion of the condition $o_r \in \mathcal{N}_{o_c}$ transforms the BT in Figure~\ref{planning:PA.fig.it2} to the BT in Figure~\ref{planning:PA.fig.it3}.
Finally, an expansion of the condition $o_r \in \mathcal{N}_{p_g}$ transforms the BT in Figure~\ref{planning:PA.fig.it3} to the BT in Figure~\ref{planning:PA.fig.it4},
and this BT is able to solve the problem shown in Figure~\ref{planning:IN.fig.front}, until the red sphere shows up.
Then additional iterations are needed.
 




%In detail:
%The refined BT $T$  returns success if and only if the goal constraint is reached. We execute $T$ which returns failure since the cube is not in the goal area. Hence $\bt$ must be expanded. Among the available action templates $\alpha$, we choose the ones whose effect involves the position of a generic object and its COS has a non-null intersection with $\{\mbox{GoalRect}\}$. In this case $Place$ is the such action template (and the only one). From $\Omega_{Place}$ we get the refinement $V_{Place} = \{o_c\}$ subject to the constraint $C_{Place} =\{o_c \in {Balanced}\}$ refining the descriptive action $Place(i,p)$ into $Place(c,p)$ that places the object $c$ into a generic position $p$. Then we further refine by  finding a set of positions that lies in both the goal constraint and $\Omega_{Place}$. Those samples are taken from the set $\{ {GoalRect} \cap {Balanced}\}$ without loss of generality we consider $\{ {GoalRect} \cap {Balanced}\} = {GoalRect}$.  The action template added to $\bt$ is then $Place(c, GoalRect)$. 
%Using Algorithm \ref{planning:PA.alg.main} we get
% the following template BT: $\bt' = \mbox{sequence}(h = c, o_r \in \mathcal{N}_{GoalRect},  \mbox{Place}(c,GoalRect))$.
%\begin{remark}
%$\bt'$ moves the constraint state $c' = (o_r \in \mathcal{N}_{GoalRect} \land  h=c )$ to $c_{goal}$. Then any system state $s' \vdash c'$ can reach a final state  $s_{final} \vdash c_{goal}$ when executing a refined instance of $\bt'$.
%\end{remark}
%
%
%Now, we expand $\bt$ as follows:  $\bt = \mbox{fallback}(\bt, \bt') $. To refine it we sample a point $p_g \in GoalRect$. The new refined BT is executable. The tree is shown in~\ref{planning:PA.fig.it1}. 
%
%We execute the new $T$. It returns failure since the $s_{init} {\nvdash }c'$ as the robot is not holding the cube and the condition  $h = c$ returns failure. As previously  done, we must expand  $\bt'$, we find a template BT that lead to the constraint state $c'$. $ \bt'' = \mbox{sequence}(h = \emptyset, o_r \in \mathcal{N}_{o_c},  \mbox{Pick}(c))$ is such BT. Then $\bt'$ is updated as follows: $ \bt' = \mbox{sequence}(\mbox{fallback}( h = c,\bt''), o_r \in \mathcal{N}_{p_{GoalRect}},  \mbox{Place}(c,GoalRect))$  and its (executable) refinement is shown in~\ref{planning:PA.fig.it2}. 
%
%Executing the updated BT, the condition  $h = c$ returns failure and the execution is passed down to the new subtree (see Fig~\ref{planning:PA.fig.it1}). The condition $h = \emptyset$ returns success but $o_r \in \mathcal{N}_{o_c}$ returns failure. We need to expand $\bt''$ further.  We need to satisfy $o_r \in \mathcal{N}_{o_c}$ and the template BT is expanded accordingly. We refine it by selecting a point $p_1 \in \mathcal{N}_{o_c}$, and a straight line trajectory $\tau_s$. The updated BT shown in Fig.~\ref{planning:PA.fig.it3}. 
%
%Executing the updated $T$, the condition $h = \emptyset$ returns success, $o_r \in \mathcal{N}_{o_c}$ returns failure, $o_c \in \mathcal{C}_{c}^{\tau_s}$ return success, and $o_s \in \mathcal{C}_{s}^{\tau_s}$ returns success. Then the action $\mbox{MoveTo}(p_1,\tau_s)$ is executed. Once the robot reaches $p_1$ the pre-conditions of $\mbox{Pick}(c)$ are satisfied. After executing $\mbox{Pick}(c)$ the condition $h = c$ finally returns success. Then $o_r \in \mathcal{N}_{p_g}$ returns failure (the robot is not close to the goal area) and the BT is updated accordingly. The final BT is depicted in Fig.~\ref{planning:PA.fig.it4}.


%\begin{figure}[t *-----]



%
%
%We begin describing the algorithm informally with a simple example. Then we state a few definitions needed to give a formal description with a more complex example.


\subsection{The Algorithm Steps in Detail}

\paragraph*{\textbf{Refine Actions (Algorithm~\ref{planning:PA.alg.main} Line~\ref{planning:PA.alg.main.refine})}\\}
%\todopetter{This section needs to be clarified. Why do we need Reachability Graphs? }

PA-BT is based on the definition of the \emph{action templates}, which contains the descriptive model of an action. An action template is characterized by conditions \emph{con} (sometimes called preconditions) and effects \emph{eff} (sometimes called postconditions) that are both constraints on the world (e.g. door open, robot in position). An action template is mapped online into an \emph{action primitive}, which contains the operational model of an action and is executable. Figure~\ref{planning.pabt.fig.acttemplate} shows an example of an action template and its corresponding action refinement.


To plan in infinite state space, PA-BT relies on a so-called Reachability Graph (RG) provided by the HBF algorithm, see \cite{garrettbackward} for details.
The RG provides efficient sampling for the actions in the BT, allowing us to map the descriptive model of an action into its operational model. 

%The samples provided by the RG are not in conflict with each other, and the process implements the action refinement described in~\cite{Nau15Challenges}.
%
%Figure~\ref{planning:PA.fig.RG} shows the resulting RG used for action refinements (Algorithm~\ref{planning:PA.alg.main} Line~\ref{planning:PA.alg.main.refine}), see~\cite{garrettbackward} for details.
%
%
%
%\begin{figure}[t]
%                \centering
%\includegraphics[width=0.5\columnwidth]{RGSimpleExample.pdf}
%        \caption{The Reachability Graph used to find the action refinement for the final tree of Figure~\ref{planning:PA.fig.it4}.}
%         \label{planning:PA.fig.RG}              
%         %add desired spsequential composition analysisacing between images, e. g. ~, \quad, \qquad etc.
%          %(or a blank line to force the subfigure onto a new line)
%          
%\end{figure}

\begin{figure}
\centering
          \begin{subfigure}[b]{0.35\columnwidth}
\begin{equation*}
\begin{aligned}
&\mbox{Pick}(i)   \\   
&\mbox{con}: o_r \in \mathcal{N}_{o_i} \hspace{2em} \\ 
&\hspace{2.2em}h  =  \emptyset \\	  
&\mbox{eff}:\hspace{0.4em}  h = i \hspace{1em} 
\end{aligned}
\end{equation*}
\caption{Action Template for picking a generic object denoted $i$.\\}
\end{subfigure}
~
\begin{subfigure}[b]{0.35\columnwidth}
\begin{equation*}
\begin{aligned}
&\mbox{Pick}(cube)   \\   
&\mbox{con}: o_r \in \mathcal{N}_{o_{cube}} \hspace{2em} \\ 
&\hspace{2.2em}h  =  \emptyset \\	  
&\mbox{eff}:\hspace{0.4em} h = cube \hspace{1em} 
\end{aligned}
\end{equation*}
\caption{Action primitive created from the Template in (a), where the object is given as $i= cube$.}
\end{subfigure}
\caption{\emph{Action Template} for Pick and its corresponding \emph{Action primitive}. $o_r$ is the robot's position, $\mathcal{N}_{o_i}$ is a set that defines a neighborhood of the object $o_i$, $h$ is the object currently in the robot's hand. The conditions are that the robot is in the neighborhood of the object, and that the robot hand is empty. The effect is that the object is in the robot hand.}
\label{planning.pabt.fig.acttemplate}
\end{figure}



\paragraph*{\textbf{Get Condition To Expand and Expand Tree (Algorithm~\ref{planning:PA.alg.main} Lines~9 and~10) }\\}

If the BT returns Failure, Line~\ref{planning:PA.alg.expand.getcon} of Algorithm~\ref{planning:PA.alg.main} invokes Algorithm~\ref{planning:PA.alg.getcond}, which finds the condition to expand by searching through the conditions returning Failure using a Breadth First Search~(BFS). If no such condition is found (Algorithm~\ref{planning:PA.alg.getcond} Line~\ref{planning:PA.alg.getcond.nocond}) that means that an action returned Failure due to an old refinement that is no longer valid. In that case, at the next loop of Algorithm~\ref{planning:PA.alg.main} a new refinement is found (Algorithm~\ref{planning:PA.alg.main} Line~\ref{planning:PA.alg.main.refine}), assuming that such a refinement always exists. If Algorithm~\ref{planning:PA.alg.getcond} returns a condition, this will be expanded (Algorithm~\ref{planning:PA.alg.main} Line 10), as shown in the example of Figure~\ref{planning:PA.fig.it1to4}. Example~\ref{planning:PA.ex.graph}  highlights the BFS expansion.
Thus, $\bt$ is expanded until it can perform an action (i.e. until $\bt$ contains an action template whose condition are satisfied in the current state). In~\cite{colledanchise2016towards} is proved that $\bt$ is expanded a finite number of times.
If there exists more than one valid action that satisfies a condition, their respective trees (sequence composition of the action and its conditions) are collected in a Fallback composition {with memory}, which implements the different options the agent has, to satisfy such a condition. {If needed, these options will be executed in turn.}
PA-BT does not investigate which action is the optimal one. As stressed in~\cite{ghallab2016automated} the cost of minor mistakes (e.g. non-optimal actions order) is often much lower than the cost of the extensive modeling, information gathering and thorough deliberation needed to achieve optimality.


\paragraph*{\textbf{Conflicts and Increases in Priority (Algorithm~\ref{planning:PA.alg.main} Lines~\ref{planning:PA.alg.main.feas} and~\ref{planning:PA.alg.main.incprio})}\\}
Similar to any STRIPS-style planner, adding a new action  in the plan can cause a \emph{conflict} (i.e. the execution of this new action reverses the effects of a previous action). % a mismatch between effects and preconditions in the progress of the plan). 
In PA-BT, this possibility is checked in Algorithm~\ref{planning:PA.alg.main}  Line~\ref{planning:PA.alg.main.feas} by analyzing the conditions of the new action added with the effects of the actions that the subtree executes before executing the new action. If this effects/conditions pair is in conflict, the goal will not be reached.  An example of this situation is described in Example~\ref{planning:PA.exa.complex} below.

%that at condition $c$ there is no action template $alpha$ that satisfy Eq.\eqref{planning:PA.eq.consdynamic}.

Again, following the approach used in STRIPS-style planners, we resolve this conflict by finding the correct action order. Exploiting the structure of BTs we can do so by moving the tree composed by the new action and its condition leftward (a BT executes its children from left to right, thus moving a subtree leftward implies executing the new action earlier). If it is the leftmost one, this means that it must be executed before its parent (i.e. it must be placed at the same depth of the parent but to its left). This operation is done in Algorithm~\ref{planning:PA.alg.main}  Line~\ref{planning:PA.alg.main.incprio}. PA-BT incrementally increases the priority of this subtree in this way, until it finds a feasible tree. In~\cite{colledanchise2016towards} it is proved that, under certain assumptions, a feasible tree always exists .





\paragraph*{\textbf{Get All Action Templates}\\}
Let's look again at Example~\ref{planning:PA.exa.simple} above and see how the BT in Figure~\ref{planning:PA.fig.it4} was created using the PA-BT approach. In this example, the action templates are summarized below with conditions and effect:
\begin{equation*}
\begin{aligned}
&\mbox{MoveTo}(p,\tau)   \\   
&\mbox{con}:\tau \subset C_{ollFree} \hspace{2em}  \\ 
& \\ 
&\mbox{eff}:o_r = p
\end{aligned}
\begin{aligned}
&\mbox{Pick}(i)   \\   
&\mbox{con}: o_r \in \mathcal{N}_{o_i} \hspace{2em} \\ 
	 &\hspace{2.3em}h  =  \emptyset \\	  
&\mbox{eff}: h = i \hspace{1em} 
\end{aligned}
%\begin{aligned}
%&\mbox{Push}(i,p)   \\   
%&\mbox{con}: o_r \in \mathcal{N}_{o_i} \hspace{0.2em} \\ 
%	 &\hspace{2.3em}o_i \in \mathcal{N}_{p}  \\	  
%&\mbox{eff}: o_i = p \hspace{1em} 
%\end{aligned}
\begin{aligned}
&\mbox{Place}(i,p)   \\   
&\mbox{con}: o_r \in \mathcal{N}_{p} \hspace{2em}  \\ 
	 &\hspace{2.3em}h = i  \\	  
&\mbox{eff}: o_i = p \hspace{1em} 
\end{aligned}
\end{equation*}
where $\tau$ is a trajectory, $C_{ollFree}$ is the set of all collision free trajectories,  $o_r$ is the robot pose, $p$ is a pose in the state space, $h$ is the object currently in the end effector, $i$ is the label of the $i$-th object in the scene, and  $\mathcal{N}_{x}$ is the set of all the poses near the pose $x$. %In this example $\mathcal{N}_{x}$ is the set of all the poses within $20 cm$ of $x$.



%\paragraph*{Actions}
%There are three actions: \emph{MoveTo}; \emph{Pick}; and \emph{Place}.

The descriptive model of the action \emph{MoveTo}  is parametrized over the destination $p$ and the trajectory $\tau$. It requires that the trajectory is collision free ($\tau \subset C_{ollFree}$). As effect the action \emph{MoveTo} places the robot at $p$ (i.e. $o_r = p$), the descriptive model of the action \emph{Pick} is parametrized over object $i$. It requires having the end effector free (i.e. $h  =  \emptyset$) and the robot to be in a neighborhood $\mathcal{N}_{o_i}$ of the object $i$. (i.e. $o_r \in \mathcal{N}_{o_i}$). As effect the action Pick sets the object in the end effector to $i$ (i.e $h = i$); %The action \emph{Push} is parametrized over object $i$ and final position $p$. It requires the object to be in the neighbourhood of the final position $p$ and the robot to be in the neighbourhood of the object $i$. (Note that these two Neighbourhood can be defined differently. But for simplicity and without loss of generality we keep the neighbourhood constraints to be the same.). As effect the action Push places the object $i$ at $p$; 
Finally, the  descriptive model of the action \emph{Place} is parametrized over object $i$ and final position $p$. It requires the robot to hold $i$ (i.e. $h = i$), and the robot to be in the neighborhood of the final position $p$. As effect the action \emph{Place} places the object $i$ at $p$ (i.e. $o_i = p$).

\begin{example}
\label{planning:PA.exa.complex}

Here we show a more complex example highlighting two main properties of PA-BT: the livelock freedom and the continual deliberative plan and act cycle. This example is an extension of Example~\ref{planning:PA.exa.simple} where, due to the dynamic environment, the robot needs to replan.

%Consider the scenario in Fig.~\ref{planning:PA.fig.complexExample}. The goal is to place the green cube on the designed area and the blue cube on top of the green one. 

Consider the execution of the final BT in Figure~\ref{planning:PA.fig.it4} of Example \ref{planning:PA.exa.simple}, where the robot is carrying the desired object to the goal location. Suddenly, as in Figure~\ref{planning:IN.fig.front}~(b), an object $s$ obstructs the (only possible) path. Then the condition $\tau \subset C_{ollFree}$  returns Failure and Algorithm \ref{planning:PA.alg.main} expands the tree accordingly (Line \ref{planning:PA.alg.main.expand}) as in Figure~\ref{planning:PA.fig.it7}.

The new subtree has as condition $h = \emptyset$ (no objects in hand) but the effect of the left branch (i.e. the main part in Figure \ref{planning:PA.fig.it4}) of the BT is $h = c$ (cube in hand) (i.e. the new subtree will be executed if and only if $h = c$ holds). Clearly  the expanded tree has a conflict (Algorithm~\ref{planning:PA.alg.main} Line \ref{planning:PA.alg.main.feas}) and the priority of the new subtree is increased  (Line~\ref{planning:PA.alg.main.incprio}), until the expanded tree is in form of Figure~\ref{planning:PA.fig.it8}. Now the BT is free from conflicts as the first subtree has as effect $h = \emptyset$  and the second subtree has a condition $h = \emptyset$. Executing the tree the robot approaches the obstructing object, now the condition $h = \emptyset$ returns Failure and the tree is expanded accordingly, letting the robot drop the current object grasped, satisfying $h = \emptyset$, then it picks up the obstructing object and places it on the side of the path. Now the condition $\tau \subset C_{ollFree}$ finally returns Success. The robot can then again approach the desired object and move to the goal region and place the object in it.  

\end{example}



\begin{figure}[h!]
        \centering
        \begin{subfigure}[b]{0.5\columnwidth}
                \centering
\includegraphics[width=1\columnwidth]{simpleExampleIteration7.pdf}
                \caption{Unfeasible expanded tree. The new subtree is highlighted in red.}
                \label{planning:PA.fig.it7}
        \end{subfigure}\\
       ~ %add desired spacing between images, e. g. ~, \quad, \qquad etc.
          %(or a blank line to force the subfigure onto a new line)
        \begin{subfigure}[b]{1\columnwidth}
                \centering
\includegraphics[width=1\columnwidth]{simpleExampleIteration8.pdf}
                \caption{Expanded Feasible subtree.}
                \label{planning:PA.fig.it8}              
        \end{subfigure}
        ~ %add desired spacing between images, e. g. ~, \quad, \qquad etc.
          %(or a blank line to force the subfigure onto a new line)
        \caption{Steps to increase the priority of the new subtree added in Example~\ref{planning:PA.exa.complex}.}
        ~ %add desired spsequential composition analysisacing between images, e. g. ~, \quad, \qquad etc.
          %(or a blank line to force the subfigure onto a new line)
\end{figure}

\subsection{Comments on the Algorithm}

It is clear that this type of continual planning and acting exhibits both the important principles of deliberation stressed in \cite{ghallab2016automated, Ghallab14}: Hierarchically organized deliberation and continual online deliberation. For example, if the robot drops the object, then the condition $h = c$ is no longer satisfied and the BT will execute the corresponding subtree to pick the object, with no need for re-planning. This type of deliberative reactiveness is built into BTs.  On the other hand, if during its navigation a new object pops up obstructing the robot's path, the condition $\tau \subset C_{ollFree}$  will no longer return Success and the BT will be expanded accordingly. This case was  described  in Example~\ref{planning:PA.exa.complex}. Moreover, note that PA-BT refines the BT every time it returns Failure. This is to encompass the case where an older refinement is no longer valid. Is such cases an action will return Failure. This Failure is propagated up to the root. The function \emph{ExpandTree} (Algorithm~\ref{planning:PA.alg.main} Line~\ref{planning:PA.alg.main.expand}) will return the very same tree (the tree needs no extension as there is no failed condition of an action) which gets re-refined in the next loop (Algorithm~\ref{planning:PA.alg.main} Line~\ref{planning:PA.alg.main.refine}). For example, if the robot planned to place the object in a particular position on the desk but  this position was no longer feasible (e.g. another object was placed in that position by an external agent). 




\subsection{Algorithm Execution on Graphs}
Here, for illustrative purposes, we show the result of PA-BT when applied to a standard shortest path problem in a graph.


\begin{example}
\label{planning:PA.ex.graph}
Consider an agent moving in different states modeled by the graph in Figure~\ref{planning:PA.fig.graph} where the initial state is $s_0$ and the goal state is $s_g$. Every arc represents an action that moves an agent from one state to another. The  action that moves the agent from a state $s_i$ to a state $s_j$ is denoted by $s_i \to s_j$. 
The initial tree, depicted in Figure~\ref{planning:PA.fig.graphit0}, is defined as a Condition node $s_g$ which returns Success if and only if the robot is at the state $s_g$ in the graph. The current state is $s_0$ (the initial state). Hence the BT returns a status of \emph{Failure}. Algorithm~\ref{planning:PA.alg.main} invokes the BT expansion routine. The state $s_g$ can be reached from the state $s_5$, through the action $s_5\to s_g$, or from the state $s_3$, through the action $s_3\to s_g$. The tree is expanded accordingly as depicted in Figure~\ref{planning:PA.fig.graphit1}. Now executing this tree, it returns a status of \emph{Failure}. Since the current state is neither $s_g$ nor $s_3$ nor $s_5$.
Now the tree is expanded in a BFS fashion, finding a subtree for condition $s_5$ as in Figure~\ref{planning:PA.fig.graphit2}. The process continues for two more iterations.
Note that at iteration 4 (See Figure~\ref{planning:PA.fig.graphit4}) Algorithm~\ref{planning:PA.alg.main} did not expand the condition $s_g$ as it was previously expanded (Algorithm~\ref{planning:PA.alg.getcond} line~\ref{planning:PA.alg.getcond.notexpanded}) this avoids infinite loops in the search. The same argument applies for conditions $s_4$ and $s_g$ in iteration 5 (See Figure~\ref{planning:PA.fig.graphit5}). The BT at iteration 5 includes the action $s_0 \to s_1$ whose precondition is satisfied (the current state is $s_0$). The action is then executed. Performing that action (and moving to $s_1$), the condition $s_1$ is satisfied. The BT executes the action  $s_1 \to s_3$ and then $s_3 \to s_g$, making the agent reach the goal state.

It is clear that the resulting execution is  the same as a BFS on the graph would have rendered. Note however that PA-BT is designed for more complex problems than graph search.

\end{example}


\begin{figure}[h]

                \centering
                \includegraphics[width=0.55\columnwidth]{graphex}
        \caption{Graph representing the task of Example~\ref{planning:PA.ex.graph}.}
        \label{planning:PA.fig.graph}
\end{figure}

\begin{figure}[h]
                \centering

        \begin{subfigure}[b]{0.2\columnwidth}
                \centering
                \includegraphics[width=0.5\columnwidth]{graphExampleIteration0}
                %\caption{Action node. The label describes the action performed}
                \caption{The initial BT. }
                \label{planning:PA.fig.graphit0}              
        \end{subfigure}          
  ~       
                   \centering

        \begin{subfigure}[b]{0.6\columnwidth}
                \centering
                \includegraphics[width=0.7\columnwidth]{graphExampleIteration1}
                %\caption{Action node. The label describes the action performed}
                \caption{BT after one iteration.}
                \label{planning:PA.fig.graphit1}              
        \end{subfigure}
 ~         %(or a blank line to force the subfigure onto a new line)
        \begin{subfigure}[b]{0.6\columnwidth}
                \centering
                \includegraphics[width=1\columnwidth]{graphExampleIteration2}
                %\caption{Condition node. The label describes the condition verified}
                \caption{BT after two iterations.}
                \label{planning:PA.fig.graphit2}
        \end{subfigure}  
        \caption{First three BT updates during execution of Example~\ref{planning:PA.ex.graph}. The numbers represent the index of the BFS of Algorithm~\ref{planning:PA.alg.getcond}. {Note that the node labeled with $?^*$ is a Fallback node with memory.}}
     
  \end{figure}


\begin{figure}[h!]

  \centering    
        \begin{subfigure}[b]{1\columnwidth}
                \centering
                \includegraphics[width=1\columnwidth]{graphExampleIteration3}
                %\caption{Condition node. The label describes the condition verified}
                \caption{BT after three iterations.}
                \label{planning:PA.fig.graphit3}
        \end{subfigure}  
        
~
      \begin{subfigure}[b]{1\columnwidth}
                \centering
                \includegraphics[width=1\columnwidth]{graphExampleIteration4}
                %\caption{Condition node. The label describes the condition verified}
                \caption{BT after four iterations.}
                \label{planning:PA.fig.graphit4}
        \end{subfigure}  
        
 ~       
              \begin{subfigure}[b]{1\columnwidth}
                \centering
                \includegraphics[width=1\columnwidth]{graphExampleIteration5}
                %\caption{Condition node. The label describes the condition verified}
                \caption{BT after five iterations.}
                \label{planning:PA.fig.graphit5}
        \end{subfigure}        
        \caption{Next BT updates during execution of Example~\ref{planning:PA.ex.graph}. The numbers represent the index of the BFS of Algorithm 3. {Note that the node labeled with $?^*$ is a Fallback node with memory.}}
\end{figure}

\subsection{Algorithm Execution on an existing Example}
In this section we apply the PA-BT approach in a more complex example adapted from~\cite{kaelbling2011hierarchical}.

\begin{figure}[h!]
    \centering
\includegraphics[width = \columnwidth,trim={1cm 3cm 1cm 3cm},clip]{washingRobot} %trim={<left> <lower> <right> <upper>}
    \caption{
    An example scenario from~\cite{kaelbling2011hierarchical},
    with the addition of two externally controlled robots (semi-transparent) providing disturbances. The robot must wash the object "A" and then put in into the storage.
    }
    \label{planning:BG.fig.youbot}
\end{figure}
\begin{example}[From~\cite{kaelbling2011hierarchical}]
\label{planning:BG.ex.simple}
Consider a multipurpose robot that is asked to clean the object $A$ and then put it in the storage room as shown in Figure~\ref{planning:BG.fig.youbot} (in this first example we ignore the other robots as they are not in ~\cite{kaelbling2011hierarchical}). The goal is specified as a conjunction $Clean(A) \land In(A,storage)$. Using PA-BT, the initial BT is defined as a sequence composition of $Clean(A)$ with $In(A,storage)$ as in Figure~\ref{planning:BG.fig.it0}. At execution, the Condition node $Clean(A)$ returns \emph{Failure} and the tree is expanded accordingly, as in Figure~\ref{planning:BG.fig.it1}. Executing the expanded tree, the Condition node $In(A,Washer)$ returns \emph{Failure} and the BT is expanded again, as in Figure~\ref{planning:BG.fig.it2}. This iterative process of planning and acting continues until it creates a BT such that the robot is able to reach  object $C$ and remove it. After cleaning  object $A$, the approach constructs the tree to satisfy the condition $In(A,storage)$ as depicted in Figure~\ref{planning:BG.fig.it6}. This subtree requires  picking  object $A$ and then placing it into the storage. However after the BT is expanded to place $A$ into the storage it contains a conflict: in order to remove  object $D$ the robot needs to grasp it. But to let the ticks reach this tree, the condition $Holding()=a$ needs to returns \emph{Success}. Clearly the robot cannot hold both $A$ and $D$. The new subtree is moved in the BT to a position with a higher priority (See Algorithm~\ref{planning:PA.alg.main} Line~\ref{planning:PA.alg.main.incprio}) and the resulting BT is the one depicted in Figure~\ref{planning:BG.fig.it7}.
\end{example}
Note that the final BT depicted in Figure~\ref{planning:BG.fig.it7} is similar to the planning and execution tree of~\cite{kaelbling2011hierarchical} with the key difference that the BT enables the continual monitoring of the plan progress, as described in~\cite{Nau15Challenges}. For example, if  $A$ slips out of the robot gripper, the robot will automatically stop and pick it up again without the need to replan or change the BT. Moreover if $D$ moves away from the storage while the robot is approaching it to remove it, the robot aborts the plan to remove $D$ and continues with the plan to place $A$ in the storage room. Hence we can claim that the PA-BT is  reactive. The execution is exactly the same as~\cite{kaelbling2011hierarchical}: the robot removes the obstructing objects $B$ and $C$ then places $A$ into the washer. When $A$ is clean, the robot picks it up, but then it has to unpick it since it has to move  $D$ away from the storage. This is a small drawback of this type of planning algorithms. Again, as stressed in \cite{ghallab2016automated} the cost of a non-optimal plan is often much lower than the cost of extensive modeling, information gathering and thorough deliberation needed to achieve optimality.
%\begin{remark}
%As mentioned above PA-BT tackles both important challenegs of deliberation stressed in \cite{ghallab2016automated,Ghallab14}: Hierarchically organized deliberation and Continual online deliberation. 
%\end{remark}




\begin{figure}[h!]
        \centering
          \begin{subfigure}[t]{0.3\columnwidth}
                \centering
\includegraphics[width=1\columnwidth]{HPNExampleIteration1.pdf}
                \caption{The initial BT.}
                \label{planning:BG.fig.it0}
        \end{subfigure}%


        \begin{subfigure}[t]{0.4\columnwidth}
\includegraphics[width=1\columnwidth]{HPNExampleIteration2.pdf}
                \caption{BT after one iteration.}
                \label{planning:BG.fig.it1}
        \end{subfigure}%
        
       ~ %add desired spacing between images, e. g. ~, \quad, \qquad etc.
          %(or a blank line to force the subfigure onto a new line)
        \begin{subfigure}[t]{0.45\columnwidth}
                \centering
\includegraphics[width=1\columnwidth]{HPNExampleIteration3.pdf}
                \caption{BT after two iterations.}
                \label{planning:BG.fig.it2}              
        \end{subfigure}
                \caption{BT updates during the execution of Example~\ref{planning:BG.ex.simple}.}
\end{figure}



\begin{figure}[h!]
        \centering
        \begin{subfigure}[b]{0.75\columnwidth}
                \centering
\includegraphics[width=1\columnwidth]{HPNExampleIteration4.pdf}
                \caption{BT after three iterations.}
                \label{planning:BG.fig.it3}
        \end{subfigure}%
       ~ %add desired spacing between images, e. g. ~, \quad, \qquad etc.
          %(or a blank line to force the subfigure onto a new line)
       
        \begin{subfigure}[b]{1\columnwidth}
                \centering
\includegraphics[width=0.85\columnwidth]{HPNExampleIteration5.pdf}
                \caption{BT after four iterations.}
                \label{planning:BG.fig.it4}              
        \end{subfigure}
        
        \caption{BT updates during the execution of Example~\ref{planning:BG.ex.simple}.}
\end{figure}
\begin{landscape}
\begin{figure}[h!]
        \centering
\includegraphics[width=\columnwidth]{HPNExampleIterationFinal.pdf}
        \caption{BT containing a conflict. The subtree created to achieve $ClearX(swept\_a,storage)$ is in conflict with the subtree created to achieve $holding()=a$. }
         \label{planning:BG.fig.it6}              
        ~ %add desired spsequential composition analysisacing between images, e. g. ~, \quad, \qquad etc.
          %(or a blank line to force the subfigure onto a new line)
\end{figure}
\end{landscape}

\begin{landscape}

\begin{figure}[h]
        \centering
\includegraphics[width=\columnwidth]{HPNExampleIterationConflictFree.pdf}
        \caption{Conflict free BT obtained.}
         \label{planning:BG.fig.it7}              
        ~ %add desired spsequential composition analysisacing between images, e. g. ~, \quad, \qquad etc.
          %(or a blank line to force the subfigure onto a new line)
\end{figure}
\end{landscape}

\subsection{Reactiveness}
\label{planning:RE}
In this section we show how BTs enable a \emph{reactive} blended acting and planning, providing concrete examples that highlight the importance of reactiveness in robotic applications.

Reactiveness is a key property for online deliberation. By reactiveness we mean \emph{the capability of dealing with drastic changes in the environment in short time}. The domains we consider are highly dynamic and unpredictable. To deal with such domains, a robot must be reactive in both planning and acting.



If an external event happens that the robot needs to react to, one or more conditions will change.
The next tick after the change will happen at most a time $\frac{1}{f_t}$ after the change. Then a subset of all
Conditions, Sequences and Fallbacks will be evaluated to either reach an Action,
or to invoke additional planning. This takes less than $\Delta t$.
The combined reaction time is thus bounded above by $\frac{1}{f_t}+\Delta t$.


\begin{remark}
Note that $\Delta t$ is strictly dependent on the real world implementation. Faster computers and better code implementation allows a smaller $\Delta t$.
\end{remark}


We are now ready to show three colloquial examples, one highlighting the reactive acting (preemption and re-execution of subplans if needed), and two highlighting the reactive planning, expanding the current plan as well as exploiting serendipity~\cite{levihn2013foresight}. 

\begin{example}[Reactive Acting]
\label{planning:RE.ex.acting}
Consider the robot in Figure~\ref{planning:BG.fig.youbot}, running the BT of Figure~\ref{planning:BG.fig.it7}. The object $A$ is not clean and it is not in the washer, however the robot is holding it. This results in the Condition nodes \emph{Clean($a$)} and \emph{In($a$,$washer$)} returning  \emph{Failure} and the Condition node \emph{holding()=$a$}  \emph{Success}.  According to the BT's logic, the ticks traverse the tree and reach the action \emph{Place($a$,$washer$)}. Now, due to vibrations during movements, the object slips out of the robot's grippers. In this new situation the Condition node \emph{holding()=$a$} now returns  \emph{Failure}. The ticks now traverse the tree and reach the action \emph{Pick($a$,$aCurrent$)} (i.e. pick $A$ from the current position, whose preconditions are satisfied). The robot then re-picks the object, making the Condition node  \emph{holding()=$a$} returning  \emph{Success} and letting the robot resume the execution of \emph{Place($a$,$washer$)}.
\end{example}

%the multipurpose robot of Example~\ref{planning:BG.ex.simple} in the situation depicted
\begin{example}[Reactive Planning]
\label{planning:RE.ex.planning}
Consider the robot in Figure~\ref{planning:BG.fig.youbot}, running the BT of Figure~\ref{planning:BG.fig.it7}. The object $A$ is clean, it is not in the storage and the robot is not holding it. This results in the Condition nodes \emph{holding()=$a$}  and \emph{In($a$,$storage$)} returning  \emph{Failure} and the Condition nodes \emph{holding()=$\emptyset$}, \emph{Clean($a$)} and \\ \emph{ClearX($swept\_a$,$washer$)} returning  \emph{Success}.  According to the BT's logic,  the ticks traverse the tree and reach the action \emph{Pick($a$,$washer$)} that let the robot approach the object and then grasp it. While the robot is approaching $A$, an external uncontrolled robot places an object in front of $A$, obstructing the passage. In this new situation the Condition node \emph{ClearX($swept\_a$,$washer$)} now returns  \emph{Failure}. The action \emph{Pick($a$,$washer$)}  no longer receives ticks and it is then preempted. The BT is expanded accordingly finding a subtree to make \emph{ClearX($swept\_a$,$washer$)} return  \emph{Success}. This subtree will make the robot pick the obstructing object and remove it. Then the robot can finally reach $A$ and place it into the storage.
\end{example}

\begin{example}[Serendipity Exploitation]
\label{planning:RE.ex.serendipity}
Consider the robot in Figure~\ref{planning:BG.fig.youbot}, running the BT of Figure~\ref{planning:BG.fig.it7}. The object $A$ is not clean, it is not in the washer and the robot is not holding it. According to the BT logic, the ticks traverse the tree and reach the action \emph{Pick($b$,$bStart$)}. While the robot is reaching the object, an external uncontrolled agent picks $B$ and removes it. Now the condition \emph{Overlaps($b$,$swept\_a$) = False} returns  \emph{Success} and the BT preempts the execution of \emph{Pick($b$,$bStart$)} and skips the execution of \emph{Places($b$,$ps28541$)} going directly to execute \emph{Pick($a$,$aStart$)}.
\end{example}

Hence the BT encodes reactiveness, (re)satisfaction of subgoals whenever these are no longer achieved, and the exploitation of  external subgoal satisfaction after a change in the environment. Note that in the Examples~\ref{planning:RE.ex.acting} and~\ref{planning:RE.ex.serendipity} above, PA-BT did not replan.

\begin{figure}[b]
    \centering
\includegraphics[width = 0.5\columnwidth]{SafeTree}
    \caption{  A safe BT for Example \ref{planning:SA.ex.safe}. The BT guaranteeing safety is combined with the mission objective constraint.}
    \label{planning:SA.fig.tree}
\end{figure}

\subsection{Safety}
\label{planning:SA}
In this section we show how BTs allow  a \emph{safe} blended acting and planning, providing a concrete example that highlights the importance of safety in robotics applications.

Safety is a key property for robotics applications. By safety we mean \emph{the capability of avoiding undesired outcomes.} The domains we usually consider have few catastrophic outcomes of actions and,  as highlighted in~\cite{kaelbling2011hierarchical}, the result of an action can usually be undone by a finite sequence of actions. However there are some cases in which the outcome of the plan can damage the robot or its surroundings. 
These cases are assumed to be identified  in advance by human operators, who then add the corresponding sub-BT to guarantee the avoidance of them. Then, the rest of the BT is expanded using the algorithm described above.

We are now ready to show a colloquial example.
\begin{example}[Safe Execution]
\label{planning:SA.ex.safe}
Consider the multipurpose robot of Example~\ref{planning:BG.ex.simple}. Now, due to overheating, the robot has to stop whenever the motors' temperatures reach a given threshold, allowing them to cool down. This situation is relatively easy to model, and a subtree to avoid it can be designed as shown  in Figure~\ref{planning:SA.fig.tree}. When running this BT, the robot will preempt any action whenever the temperature is above the given threshold and stay inactive until the motors have cooled down to the temperature where Cool Down Motors return Success. Note that the Not Cooling Down part of the condition is needed to provide hystereses. The robot stops when $T_{\mbox{MAX}}$ is reached, and waits until Cool Down Motors return Success at some given temperature below $T_{\mbox{MAX}}$.
To perform the actual mission, the BT in Figure~\ref{planning:SA.fig.tree} is executed and expanded as explained above.
\end{example}

Thus, we first identify and handle the safety critical events separately, and then progress as above without 
jeopardizing the safety guarantees.
Note that the tree in Figure~\ref{planning:SA.fig.tree}, as well as all possible expansions of it using the PA-BT algorithm is \emph{safe} (see Section~\ref{properties:sec:safety}).



\subsection{Fault Tolerance}
\label{planning:FT}
In this section we show how BTs enable a \emph{fault tolerant} blended acting and planning, providing concrete examples that highlight the importance of fault tolerance in robotics application.

Fault tolerance is a key property for real world problem domains where no actions are guaranteed to succeed. By fault tolerant we mean \emph{the capability of operating properly in the event of Failures.} The robots we consider usually have more than a single way of carrying out a high level task. %(e.g. a dual armed robot can pick an object with either hand). 
With multiple options, we can define a priority measure for these options. In PA-BT, the actions that achieve a given goal are collected in a Fallback composition (Algorithm~\ref{planning:PA.alg.update} Line 9). The BT execution is such that if, at execution time, one option fails (e.g. a motor breaks) the next option is chosen without replanning or extending the BT.
%\begin{definition}[$n$-fault tolerant policy. Adapted from~\cite{jensen2004fault}]
%A $n$-fault tolerant policy is a policy such that any execution,
%where at most $n$ action templates are disabled by faults, eventually reaches a goal state.
%\end{definition}
%
%\begin{proposition}
%A policy $\pi$ derived by using PA-BT is $n$-fault tolerant if for every precondition of an action template there exist $n+1$ action templates whose effect satisfy the precondition.
%
%\begin{proof}
% Every precondition is a subgoal. If every subgoal can be achieved by $n+1$ action templates, these are collected in a Fallback composition. If one fails another one is executed. Up to $n$ actions can fail and still having an action to execute to satisfy a subgoal.
%\end{proof}
%\end{proposition}
%We are now ready to show a colloquial example.
\begin{example}[Fault Tolerant Execution]
Consider a more advanced version the multipurpose robot of Example~\ref{planning:BG.ex.simple} having a second arm. Due to this redundancy, all the pick-and-place tasks can be carried out with either hands. In the approach (Algorithm~\ref{planning:PA.alg.update} Line 9) all the actions that can achieve a given subgoal are collected in a Fallback composition. Thus, whenever the robot fails to pick an object with a gripper, it can directly try to pick it with the other gripper.
\end{example}



\newpage

\subsection{Complex Execution on Realistic Robots}
\label{planning:sec:simulations}

In this section, we show how the PA-BT approach scales to complex problems using two different scenarios. First, a KUKA Youbot scenario, where we show the applicability of PA-BT on dynamic and unpredictable environments, highlighting the importance of continually planing and acting. Second, an  ABB Yumi industrial manipulator scenario, where we highlight the applicability of PA-BT to real world plans that require the execution of a long sequence of actions. The experiments were carried out using the physics simulator V-REP, in-house implementations of low level controllers for actions and conditions, and an open source BT library\footnote{\url{http://wiki.ros.org/behavior_tree}}.  
 Figures~\ref{planning:SI.fig.yousimplescreen} and~\ref{planning:DSI.fig.youscreen} show the execution of two KUKA youbot experiments and Figure~\ref{planning:SI.fig.yumiscreen} show the execution of one ABB Yumi robot experiment. A video showing the executions of all the experiments is publicly available.\footnote{\url{https://youtu.be/mhYuyB0uCLM}} {All experiments show the reactiveness of the PA-BT approach, one experiment is then extended to show safety maintenance and fault tolerance.}




%%experimental results and collect some qualitative measure of our approach compared with the classical ones. In particular we are interested in the number of new nodes generated.  %Table \ref{planning:SI.tab} summarizes the changes on the execution framework during the experiments that needed replanning, comparing  our proposed approach with the state of the art.
%
%We applied our framework on two different platforms. One using a KUKA youbot, 	
%
%%\red{
%\begin{table}[h]
%\centering
%  \begin{tabular}{| l | c | c | c |}\hline
%    Scenario &  replan \# & Proposed Approach & Canonical Approach\\ \hline \hline 
%       \multirow{2}{*}{Scenario 1} & 1 & 48\% & 83\%\\ \cline{2-4} & 2 & \;\;0\% & 25\% \\ \hline  
%       \multirow{2}{*}{Scenario 2} & 1 & 30\% & 68\% \\ \cline{2-4} & 2 & \;\;0\% & 19\% \\ \hline
%       \multirow{1}{*}{Scenario 3} & 1 & 33\% & 50\%\\ \hline 
% 
%               \hline
%  \end{tabular}
%  \caption{Table collecting the percentage of new nodes after a replan.}
%  \label{planning:SI.tab}	
%  \end{table}
%}
%
\subsubsection{KUKA Youbot experiments}


In these scenarios, which are an extension of Examples~1 and~2, a KUKA Youbot has to place a green cube on a goal area, see Figures~\ref{planning:SI.fig.yousimplescreen} and~\ref{planning:DSI.fig.youscreen}. The robot is equipped with a single or dual arm with a simple parallel gripper. Additional objects may obstruct the feasible paths to the goal, and the robot has to plan when to pick and where to place  the obstructing objects. Moreover, external actors  co-exist in the scene and may force the robot to replan by modifying the environment, e.g. by picking and placing the objects.
%\begin{figure}[h]
%\centering
%\includegraphics[width=\columnwidth]{youbotScene.jpg}
%\caption{Scenario of the KUKA youbot experiments.}
%\label{planning:SI.fig.you}
%\end{figure}
{
\begin{experiment}[Static Environment]
\label{planning:results.ex.1}
In this experiment the single armed version of robot is asked to place the green cube in the goal area. First expansions of the BT allow the robot to pick up the desired object (see Figure~\ref{planning:SI.fig.youbotstep1}). Now the robot has to find a collision free path to the goal. Due to the shape of the floor and the position of the obstacle (a blue cube) the robot has to place the obstacle to the side. To do so the robot has to reach the obstacle and pick it up. Since this robot has a single arm, it needs to ungrasp the green cube (see Figure~\ref{planning:SI.fig.youbotstep2}) before placing the blue cube on the side (see Figure~\ref{planning:SI.fig.youbotstep3}). The robot then can re-grasp the green cube (without extending the plan) and approach the goal region. Now, due to vibrations, the green cube slips out of the robot gripper (see Figure~\ref{planning:SI.fig.youbotstep4}). The robot aborts its subplan to reach the goal and re-executes the subplan to grasp the object.  Finally the robot places the green cube in the desired area (see Figure~\ref{planning:SI.fig.youbotstep5}).
\end{experiment}
\begin{figure}[h]
        \centering
        \begin{subfigure}[b]{1\columnwidth}
                \centering
                \includegraphics[width=1\columnwidth,trim={1cm 1.5cm 10cm 9cm},clip]{YoubotSimpleSceneStep1}
                %\caption{Action node. The label describes the action performed}
                \caption{The robot picks the desired object: a green cube.  }
                \label{planning:SI.fig.youbotstep1}              
        \end{subfigure}       
        ~         
        \begin{subfigure}[b]{1\columnwidth}
                \centering
                \includegraphics[width=1\columnwidth,trim={1cm 1.5cm 10cm 9cm},clip]{YoubotSimpleSceneStep2}
                %\caption{Action node. The label describes the action performed}
                \caption{The robot has to move the blue cube away from the path to the goal. But the robot is currently grasping the green cube. Hence the subtree created to move the blue cube needs to have a higher priority. }
 			\label{planning:SI.fig.youbotstep2}  
         \end{subfigure}
~
          %(or a blank line to force the subfigure onto a new line)
        \begin{subfigure}[b]{1\columnwidth}
                \centering
                \includegraphics[width=1\columnwidth,trim={1cm 1.5cm 10cm 9cm},clip]{YoubotSimpleSceneStep3}
                %\caption{Condition node. The label describes the condition verified}
                \caption{The blue cube is moved to the side. }
                 \label{planning:SI.fig.youbotstep3}  
        \end{subfigure}   
        \caption{Execution of a Simple KUKA Youbot experiment.}
        \label{planning:SI.fig.yousimplescreen2}
\end{figure}
\clearpage

\begin{figure}[t]
        \centering
        \begin{subfigure}[b]{1\columnwidth}
                \centering
                \includegraphics[width=1\columnwidth,trim={1cm 1.5cm 10cm 9cm},clip]{YoubotSimpleSceneStep4}
                %\caption{Condition node. The label describes the condition verified}
                \caption{While the robot is moving towards the goal region, the green cube slips out of the gripper. The robot reactively preempts the subtree to move to the goal and re-executes the subtree to grasp the green cube. Without replanning. }
                 \label{planning:SI.fig.youbotstep4}  
        \end{subfigure} 
                ~            
        \begin{subfigure}[b]{1\columnwidth}
                \centering
                \includegraphics[width=1\columnwidth,trim={1cm 1.5cm 10cm 9cm},clip]{YoubotSimpleSceneStep5}
                %\caption{Condition node. The label describes the condition verified}
                \caption{The robot places the object onto the desired location. }
                 \label{planning:SI.fig.youbotstep5}  
        \end{subfigure} 
        \caption{Execution of a Simple KUKA Youbot experiment.}
        \label{planning:SI.fig.yousimplescreen}
\end{figure}

\vspace*{\fill}

\clearpage

\begin{experiment}[Safety]
In this experiment the robot is asked to perform the same task as in Experiment~\ref{planning:results.ex.1} with the main difference that now the robot's battery can run out of power. To avoid this undesired irreversible outcome, the initial BT is manually created  in a way that is similar to the one in Figure~\ref{planning:SA.fig.tree}, managing the battery charging instead. As might be expected, the execution  is similar to the one described in Experiment~\ref{planning:results.ex.1} with the difference that the robot reaches the charging station whenever needed: The robot first reaches the green cube (see Figure~\ref{planning:SI.fig.youbotsafestep1}). Then, while the robot is approaching the blue cube, the battery level becomes low. Hence the subplan to reach the blue cube is aborted and the subplan to charge the battery takes over (see Figure~\ref{planning:SI.fig.youbotsafestep2}). When the battery is charged the robot can resume its plan (see Figure~\ref{planning:SI.fig.youbotsafestep3}) and complete the task (see Figure~\ref{planning:SI.fig.youbotsafestep4}).
\end{experiment}
\begin{figure}[h]
        \centering
        \begin{subfigure}[b]{	0.9\columnwidth}
                \centering
                \includegraphics[width=1\columnwidth,trim={5cm 5cm 28cm 15cm},clip]{YoubotSafeSceneStep1}%trim={<left> <lower> <right> <upper>}
                %\caption{Action node. The label describes the action performed}
                \caption{The robot picks the desired object, a green cube. }
                \label{planning:SI.fig.youbotsafestep1}              
        \end{subfigure}       
        ~         
        \begin{subfigure}[b]{0.9\columnwidth}
                \centering
                \includegraphics[width=1\columnwidth,trim={5cm 5cm 28cm 15cm},clip]{YoubotSafeSceneStep2}
                %\caption{Action node. The label describes the action performed}
                \caption{Due to the low battery level, the robot moves to the charging station.}
 			\label{planning:SI.fig.youbotsafestep2}  
         \end{subfigure}
                 \caption{Execution of a KUKA Youbot  experiment illustrating safety.}
        \label{planning:SI.fig.yousafescreen}
\end{figure}
\clearpage

         \begin{figure}[t]
\centering
          %(or a blank line to force the subfigure onto a new line)
        \begin{subfigure}[b]{0.9\columnwidth}
                \centering
                \includegraphics[width=1\columnwidth,trim={5cm 5cm 28cm 15cm},clip]{YoubotSafeSceneStep3}
                %\caption{Condition node. The label describes the condition verified}
                \caption{Once the battery is charged, the robot resumes its plan. }
                 \label{planning:SI.fig.youbotsafestep3}  
        \end{subfigure}   
        ~            
        \begin{subfigure}[b]{0.9\columnwidth}
                \centering
                \includegraphics[width=1\columnwidth,trim={5cm 5cm 28cm 15cm},clip]{YoubotSafeSceneStep4}
                %\caption{Condition node. The label describes the condition verified}
                \caption{The robot places the object onto the desired location.}
                 \label{planning:SI.fig.youbotsafestep4}  
        \end{subfigure} 
        \caption{Execution of a KUKA Youbot  experiment illustrating safety.}
        \label{planning:SI.fig.yousafescreen2}
\end{figure}

\vspace*{\fill}

\clearpage

\begin{experiment}[Fault Tolerance]
In this experiment the robot is asked to perform the same task as in Experiment~\ref{planning:results.ex.1} with the main difference that the robot is equipped with an auxiliary arm and a fault can occur to either arm, causing the arm to stop functioning properly. The robot starts the execution as in the previous experiments (see Figure~\ref{planning:SI.fig.youbotftstep1}). However while the robot is approaching the goal area, the primary arm breaks, making the green cube fall on the ground (see Figure~\ref{planning:SI.fig.youbotftstep2}). The robot now tries to re-grasp the object with the primary arm, but this action fails since the grippers are no longer attached to the primary arm, hence the robot tries to grasp the robot with the auxiliary arm. However the auxiliary arm is too far from the object, and thus the robot has to move in a different position (see Figure~\ref{planning:SI.fig.youbotftstep3}) such that the object can  be grasped (see Figure~\ref{planning:SI.fig.youbotftstep4}) and the execution can continue.

\end{experiment}

\begin{figure}[h!]
      \begin{subfigure}[b]{1\columnwidth}
                \centering
                \includegraphics[width=0.95\columnwidth,trim={6cm 7cm 20cm 7cm},clip]{YoubotFTSceneStep1}%trim={<left> <lower> <right> <upper>}
                %\caption{Condition node. The label describes the condition verified}
                \caption{The robot moves the blue cube away from the path to the goal.}
                 \label{planning:SI.fig.youbotftstep1}  
        \end{subfigure}         
        ~
              \begin{subfigure}[b]{1\columnwidth}
                \centering
                \includegraphics[width=0.95\columnwidth,trim={6cm 7cm 20cm 7cm},clip]{YoubotFTSceneStep2}
                %\caption{Condition node. The label describes the condition verified}
                \caption{A fault occurs on the primary arm (the grippers break) and the green cube falls to the floor.}
                 \label{planning:SI.fig.youbotftstep2}  
        \end{subfigure} 
        \caption{Execution of a KUKA Youbot  experiment illustrating fault tolerance.}
        \label{planning:SI.fig.youftscreen2}
\end{figure}
\clearpage

\begin{figure}[t]
              \begin{subfigure}[b]{1\columnwidth}
                \centering
                \includegraphics[width=0.95\columnwidth,trim={6cm 7cm 20cm 7cm},clip]{YoubotFTSceneStep3}
                %\caption{Condition node. The label describes the condition verified}
                \caption{The robot rotates to have the object closer to the auxiliary arm.}
                 \label{planning:SI.fig.youbotftstep3}  
        \end{subfigure}      
        ~   
              \begin{subfigure}[b]{1\columnwidth}
                \centering
                \includegraphics[width=0.95\columnwidth,trim={6cm 7cm 20cm 7cm},clip]{YoubotFTSceneStep4}
                %\caption{Condition node. The label describes the condition verified}
                \caption{The robot grasps the object with the auxiliary arm.}
                 \label{planning:SI.fig.youbotftstep4}  
        \end{subfigure}        
        \caption{Execution of a KUKA Youbot  experiment illustrating fault tolerance.}
        \label{planning:SI.fig.youftscreen2bis}
\end{figure}

\vspace*{\fill}

\clearpage
\begin{experiment}[Dynamic Environment]
In this experiment the single armed version of the robot co-exists with other uncontrolled external robots. The robot is asked to place the green cube in the goal area on the opposite side of the room. The robot starts picking up the green cube and moves towards an obstructing object (a blue cube) to place it to the side (see Figure~\ref{planning:DSI.fig.youbotstep1}). Being single armed, the robot has to ungrasp the green cube (see Figure~\ref{planning:DSI.fig.youbotstep2}) to grasp the blue one (see Figure~\ref{planning:DSI.fig.youbotstep3}). While the robot is placing the blue cube to the side, an external robot places a new object between the controlled robot and the green cube (see Figure~\ref{planning:DSI.fig.youbotstep4}). The plan of the robot is then expanded to include the removal of this new object (see Figure~\ref{planning:DSI.fig.youbotstep5}). Then the robot can continue its plan by re-picking the green cube, without replaning. Now the robot approaches the yellow cube to remove it (see Figure~\ref{planning:DSI.fig.youbotstep6}), but before the robot is able to grasp the yellow cube, another external robot picks up the yellow cube (see Figure~\ref{planning:DSI.fig.youbotstep7}) and places it to the side. The subplan for removing the yellow cube is skipped (without replaning) and the robot  continues its task until the goal is reached (see Figure~\ref{planning:DSI.fig.youbotstep8}).	
\end{experiment}

\begin{figure}[h]
        \centering
        \begin{subfigure}[b]{1\columnwidth}
                \centering
                \includegraphics[width=0.85\columnwidth,trim={0 0 1cm 7cm},clip]{YoubotSceneStep1}
                %\caption{Action node. The label describes the action performed}
                \caption{The robot picks the desired object, a green cube.}
                \label{planning:DSI.fig.youbotstep1}              
        \end{subfigure}       
               
        \begin{subfigure}[b]{1\columnwidth}
                \centering
                \includegraphics[width=0.85\columnwidth,trim={0 0 1cm 7cm},clip]{YoubotSceneStep2}
                %\caption{Action node. The label describes the action performed}
                \caption{The blue cube obstructs the path to the goal region. The robot drops the green cube in order to pick the blue cube.}
 			\label{planning:DSI.fig.youbotstep2}  
         \end{subfigure}
        \caption{Execution of a complex KUKA Youbot experiment.}
        \label{planning:DSI.fig.youscreen}
\end{figure}
\clearpage

\begin{figure}[h]
\centering
          %(or a blank line to force the subfigure onto a new line)
        \begin{subfigure}[b]{1\columnwidth}
                \centering
                \includegraphics[width=0.85\columnwidth,trim={0 0 1cm 7cm},clip]{YoubotSceneStep3}
                %\caption{Condition node. The label describes the condition verified}
                \caption{The robot picks the blue cube.}
                 \label{planning:DSI.fig.youbotstep3}  
        \end{subfigure}   
        ~            
        \begin{subfigure}[b]{1\columnwidth}
                \centering
                \includegraphics[width=0.85\columnwidth,trim={0 0 1cm 7cm},clip]{YoubotSceneStep4}
                %\caption{Condition node. The label describes the condition verified}
                \caption{While the robot moves the blue cube away from the path to the goal, an external agent places a red cube between the robot and the green cube.}
                 \label{planning:DSI.fig.youbotstep4}  
        \end{subfigure} 
~

\centering

      \begin{subfigure}[b]{1\columnwidth}
                \centering
                \includegraphics[width=0.85\columnwidth,trim={0 0 1cm 7cm},clip]{YoubotSceneStep5}
                %\caption{Condition node. The label describes the condition verified}
                \caption{The robot moves the red cube away from the path to the goal.  }
                 \label{planning:DSI.fig.youbotstep5}  
        \end{subfigure}         
              \caption{Execution of a complex KUKA Youbot experiment.}
        \label{planning:DSI.fig.youscreen2}
\end{figure}
        \begin{figure}[h]
              \begin{subfigure}[b]{1\columnwidth}
                \centering
                \includegraphics[width=0.85\columnwidth,trim={0 0 1cm 7cm},clip]{YoubotSceneStep6}
                %\caption{Condition node. The label describes the condition verified}
                \caption{The yellow cube obstructs the path to the goal region. The robot drops the green cube in order to pick the yellow cube.}
                 \label{planning:DSI.fig.youbotstep6}  
        \end{subfigure} 

        \centering
              \begin{subfigure}[b]{1\columnwidth}
                \centering
                \includegraphics[width=0.85\columnwidth,trim={0 0 1cm 7cm},clip]{YoubotSceneStep7}
                %\caption{Condition node. The label describes the condition verified}
                \caption{While the robot approaches the yellow cube, an external agent moves the yellow cube away.}
                 \label{planning:DSI.fig.youbotstep7}  
        \end{subfigure}      
        ~   
              \begin{subfigure}[b]{1\columnwidth}
                \centering
                \includegraphics[width=0.85\columnwidth,trim={0 0 1cm 7cm},clip]{YoubotSceneStep8}
                %\caption{Condition node. The label describes the condition verified}
                \caption{The robot picks the green cube and places it onto the goal region. }
                 \label{planning:DSI.fig.youbotstep8}  
        \end{subfigure}        
        \caption{Execution of a complex KUKA Youbot experiment.}
        \label{planning:DSI.fig.youscreen2bis}
\end{figure}
\clearpage


\subsubsection{ABB Yumi experiments}


In these scenarios, an ABB Yumi has to assemble a cellphone whose parts are scattered across a table, see Figure~\ref{planning:SI.fig.yumiscreen}. 
The robot is equipped with two arms with simple parallel grippers, which are not suitable for dexterous manipulation.
Furthermore, some parts must be grasped in a particular way to enable the assembly operation. 
\begin{experiment}
In this experiment, the robot needs to re-orient some cellphone's parts to expose them for assembly. Due to the gripper design, the robot must reorient the parts by performing multiple grasps transferring the part to the other gripper, see Figure~\ref{planning:SI.fig.Yumistep2}, effectively changing its orientation (see Figures~\ref{planning:SI.fig.Yumistep3}-\ref{planning:SI.fig.Yumistep4}).
%To assemble the cellphone, the opening of the cellphone's chassis needs to face away from the robot's arm, exposing it for the assembly.
%However, the initial position of a part is such that it requires multiple grasps transferring the part to the other gripper, effectively changing its orientation w.r.t the grasping gripper (see Figure~\ref{planning:SI.fig.yumiscreen}).	
\end{experiment}
}


%\begin{figure}[h]
%\centering
%\includegraphics[width=\columnwidth]{yumiScene5.jpg}
%\caption{Scenario of the ABB Yumi experiments.}
%\label{planning:SI.fig.yum}
%\end{figure}



\begin{figure}[h!]
        \centering
        \begin{subfigure}[b]{1\columnwidth}
                \centering
                \includegraphics[width=0.7\columnwidth,trim={11cm 4cm 16cm 7cm},clip]{YumiSceneStep1}
                %\caption{Action node. The label describes the action performed}
                \caption{The robot picks the cellphone's chassis. The chassis cannot be assembled with this orientation. }
                \label{planning:SI.fig.Yumistep1}              
        \end{subfigure}       
        ~         
        \begin{subfigure}[b]{1\columnwidth}
                \centering
                \includegraphics[width=0.7\columnwidth,trim={11cm 4cm 16cm 7cm},clip]{YumiSceneStep2}
                %\caption{Action node. The label describes the action performed}
                \caption{The chassis is handed over the other gripper. }
 			\label{planning:SI.fig.Yumistep2}  
         \end{subfigure}
        \caption{Execution of an ABB Yumi experiment.}
        \label{planning:SI.fig.yumiscreen}
\end{figure}
\clearpage
\begin{figure}[h]
\centering
          %(or a blank line to force the subfigure onto a new line)
        \begin{subfigure}[b]{1\columnwidth}
                \centering
                \includegraphics[width=0.7\columnwidth,trim={11cm 4cm 16cm 7cm},clip]{YumiSceneStep3}
                %\caption{Condition node. The label describes the condition verified}
                \caption{The chassis is placed onto the table with a different orientation than before (the opening part is facing down now).}
                 \label{planning:SI.fig.Yumistep3}  
        \end{subfigure}   
        ~            
        \begin{subfigure}[b]{1\columnwidth}
                \centering
                \includegraphics[width=0.7\columnwidth,trim={11cm 4cm 16cm 7cm},clip]{YumiSceneStep4}
                %\caption{Condition node. The label describes the condition verified
                \caption{The robot picks the chassis with the new orientation. }
                 \label{planning:SI.fig.Yumistep4}  
        \end{subfigure} 
~

\centering

      \begin{subfigure}[b]{1\columnwidth}
                \centering
                \includegraphics[width=0.7\columnwidth,trim={11cm 4cm 16cm 7cm},,clip]{YumiSceneStep5}
                %\caption{Condition node. The label describes the condition verified}
                \caption{The chassis can be assembled with this orientation.}
                 \label{planning:SI.fig.Yumistep5}  
        \end{subfigure}         
                \label{planning:SI.fig.yumiscreen2}
                        \caption{Execution of an ABB Yumi experiment.}
\end{figure}
\clearpage
\begin{figure}[h]
              \begin{subfigure}[b]{1\columnwidth}
                \centering
                \includegraphics[width=0.7\columnwidth,trim={11cm 4cm 16cm 7cm},clip]{YumiSceneStep6}
                %\caption{Condition node. The label describes the condition verified}
                \caption{The robot pick the next cellphone's part to be assembled (the motherboard). }
                 \label{planning:SI.fig.Yumistep6}  
        \end{subfigure} 
        ~
        
        \centering
              \begin{subfigure}[b]{1\columnwidth}
                \centering
                \includegraphics[width=0.7\columnwidth,trim={11cm 4cm 16cm 7cm},,clip]{YumiSceneStep7}
                %\caption{Condition node. The label describes the condition verified}
                \caption{The motherboard and the chassis are assembled.}
                 \label{planning:SI.fig.Yumistep7}  
        \end{subfigure}      
        ~   
              \begin{subfigure}[b]{1\columnwidth}
                \centering
                \includegraphics[width=0.7\columnwidth,trim={11cm 4cm 16cm 7cm},clip]{YumiSceneStep8}
                %\caption{Condition node. The label describes the condition verified}
                \caption{The robot assembles the cellphone correctly. }
                 \label{planning:SI.fig.Yumistep8}  
        \end{subfigure}        
        \caption{Execution of an ABB Yumi experiment.}
        \label{planning:SI.fig.yumiscreen2bis}
\end{figure}
\clearpage





\section{Planning using A Behavior Language (ABL)}
\label{planning.MS}

To contrast the PA-BT approach described above we will more briefly present planning using A Behavior Language
\footnote{In the first version of ABL, the tree structure that stores all the goals is called \say{Active Behavior Tree}. This tree 
is related to, but different from the BTs we cover in this book (e.g. no Fallbacks and no ticks).
  Later work used the classic BT formulation also for ABL.}
 (ABL, pronounced ``able")~\cite{weber2010reactive,weber2011building}. 

ABL was designed for the dialoge game Facade \cite{mateas2002abl}, but is also appreciated for its ability to handle the planning and acting on multiple scales 
that is often needed in both robotics and games, and in particular essential in so-called Real-Time Strategy games.
In such games, events take place both on a long term time scale, where strategic decisions has to be made regarding e.g., what buildings to construct
in the next few minutes, and where to locate them, and on a short term  time scale, where tactical decisions has to be made regarding e.g., what opponents to attack in the next few seconds.
Thus, performing well in multi-scale games requires the ability to execute short-term tasks while working towards long-term goals. 
In this section we will first use the the Pac-Man game as an example, 
where the short term decisions concern avoiding ghosts and the long term decisions concern eating all the available pills.

%
%In multi-scale games, the autonomous agents need to perform on a level that is comparable to human experts.
%% That demonstrates the need for heterogeneous agent architectures~\cite{weber2011building}. 
%FSMs are still widely used for encoding the task execution of the agents, due to their simplicity and expressivity. However, 
%BTs are becoming more popular, due to many of the reasons described in this book
%~\cite{weber2010reactive,weber2011building, isla2005handling,florez2008dynamic}.

%multi-scale game developers criticized FSMs for their inability to allow logic to be reused across different context and for lacking support parallel decision making about multiple goals, finding instead BTs as the tool that provides methods for dealing with modern games~\cite{weber2010reactive,weber2011building, isla2005handling,florez2008dynamic}. %In this section, we show how to create systematically reactive plans for multi-scale NPCs based on the ABL reactive planning language. We will introduce the approach using the simple Pac-Man example of Section~\ref{sec:pacman} and then we will show the results in the video game \emph{StarCraft} since it is played at a professional level and a large amount of data is available for evaluating the approach.





\subsection{An ABL Agent}
%The most used approach for planning in multi-scale games involving BTs is A Behavior Language (ABL)~\cite{mateas2002abl}.
%ABL is a reactive planning language in which an agent
%has an active set of goals to achieve. ABL is effective for building multi-scale game AI as it enables agents to pursue multiple concurrent goals~\cite{weber2010reactive}.

\begin{figure}[h]
\centering
\includegraphics[width=0.7\columnwidth]{ABLArchitecture.pdf}
\caption{Architecture of a ABL agent.}
\label{planning:rt.fig.ablarchitecture}
\end{figure}

%\subsubsection{ABL Agent}
An agent running an ABL planner is called an \emph{ABL Agent}. Figure~\ref{planning:rt.fig.ablarchitecture} depicts the architecture of an ABL agent. The \textbf{behavior library} is a repository of pre-defined behaviors where each behavior consists of a set of actions to execute to accomplish a goal (e.g. move to given location). There are two kinds of behaviors in ABL, \emph{sequential behaviors} and \emph{parallel behaviors}.
The \textbf{working memory} is a container for any information the agent must access during execution (e.g. unit's position on the map). The \textbf{sensors} report information about changes in
the world by writing that information into the working memory (e.g. when another agent is within sight). The \textbf{tree} (henceforth denoted ABL tree to avoid confusion) is an execution structure that describes how the agent will act, and it is dynamically extended. The ABL tree is initially defined as a collection of all the agent's goals, then it is recursively extended using a set of instructions that describe how to expand the tree.  Figure~\ref{planning:abl.fig.abtintro1} shows the initial ABL tree for the ABL Pac-Man Agent. Below we describe the semantic of  ABL tree instructions.



\begin{figure}[h]
\centering
\begin{lstlisting}
initial-tree{
	subgoal handleGhosts();
	subgoal eatAllPills();
	}
\end{lstlisting}
\caption{Example of an initial ABL tree instruction of the ABL agent for Pac-Man.}
\label{planning:abl.fig.abtintro1}
\end{figure}

A \textbf{subgoal} instruction establishes goals that must be accomplished in order to achieve
the main task.


An \textbf{act} instruction describes an action what will change the physical state of the agent or the environment.
A \textbf{mental act} instruction describes pure computation, as mathematical
computations or modifications to working memory. 

Act and mental act are both parts of behaviors, listed in  the behavior library, as the examples in Figures~\ref{planning:abl.fig.abtintro2}-\ref{planning:abl.fig.abtintro2bis}.

\begin{figure}[h]
\centering
\begin{lstlisting}
sequential behavior eatAllPills(){
	mental_act computeOptimalPath();
	act followOptimalPath();
}
\end{lstlisting}
\caption{Example of the content of the behavior library.}
\label{planning:abl.fig.abtintro2}
\end{figure}

\begin{figure}[h]
\centering
\begin{lstlisting}
parallel behavior eatAllPills(){
	mental_act recordData();
	act exploreRoom();
}
\end{lstlisting}
\caption{Example of the content of the behavior library.}
\label{planning:abl.fig.abtintro2bis}
\end{figure}

A \textbf{spawngoal} instruction is the key component for expanding the BT. It defines the subgoals that must be accomplished to achieve a behavior.

\begin{figure}[h]
\centering
\begin{lstlisting}
parallel behavior handleGhosts(){
	spawngoal handleDeadlyGhosts();
	spawngoal handleScaredGhosts();
}
\end{lstlisting}
\caption{Example of the content of the behavior library. }
\label{planning:rt.fig.abtintro3}
\end{figure}




\begin{remark}
The main difference between the instructions \emph{subgoal} and \emph{spawngoal} is that the spawngoal instruction is evaluated in a lazy fashion, expanding the tree only when the goal spawned is needed for the first time, whereas the subgoal instruction is evaluated in a greedy fashion, requiring the details on how to carry out the subgoal at design time.   
\end{remark}


\begin{figure}[h]
\centering
\begin{lstlisting}
sequential behavior handleDeadlyGhost(){
	precondition {
	(deadlyGhostClose);
	}
	act keepDistanceFromDeadlyGhost();
}
\end{lstlisting}
\caption{Example of a precondition instruction of the ABL agent for Pac-Man.}
\label{planning:abl.fig.abtintro4}
\end{figure}


A \textbf{precondition} instruction specifies
under which conditions the behavior can be selected. When all
of the preconditions are satisfied, the behavior
can be selected for execution or expansion, as in Figure~\ref{planning:abl.fig.abtintro4}. 



%\subsubsection{Conflicts}
\begin{figure}[h]
\centering
\begin{lstlisting}
conflict keepDistanceFromDeadlyGhost followOptimalPath; 
\end{lstlisting}
\caption{Example of a conflict instruction of the ABL agent for Pac-Man.}
\label{planning:rt.fig.abtintro5}
\end{figure}

A \textbf{conflict} instruction specifies priority order if two or more actions are scheduled for execution at the same time, using the same (virtual) actuator. 

\subsection{The ABL Planning Approach}

In this section, we present the ABL planning approach.
Formally, the  approach is described in Algorithms \ref{planning:ABL.alg.main}-\ref{planning:ABL.alg.Exec}.
First, will give an overview of the algorithms and see how they are applied to the problem described in \emph{Example~\ref{planning:ex:pacman}},
to iteratively create the BTs of Figure \ref{planning:abl.fig.abtpacman2BT}.
Then, we will discuss the key steps in more detail.


\begin{algorithm2e}[h!]
\caption{main loop - input(initial ABL tree)}
 \label{planning:ABL.alg.main}
\DontPrintSemicolon
%$\mathcal{T} \gets \emptyset$
%
%$\Gamma \gets \mathcal{C}_{goal}$
\SetKwProg{myalg}{algorithm2e}{}{}
 $\mathcal{T} \gets$ ParallelNode \label{planning:ABL.alg.main.T} \\
\For{$subgoal$ in $initial-tree$} {
 $\mathcal{T}_g \gets$ \FuncSty{GetBT(subgoal)} \label{planning:ABL.alg.main.subgoal}
 
 $\mathcal{T}$.\FuncSty{AddChild}($\mathcal{T}_g$)\\

 }
\While{True\label{planning:ABL.alg.main.while}}{
    \FuncSty{Execute($\mathcal{T}$)} \label{planning:ABL.alg.main.fail2}}
 
 
\end{algorithm2e}






%\todopetter{Alg2, line 6-8: Why is there a Fallback with negation? (instead of a Sequence)}

\begin{algorithm2e}[h!]
\caption{GetBT - input(goal)}
 \label{planning:ABL.alg.getbt}
\DontPrintSemicolon
%$\mathcal{T} \gets \emptyset$
%
%$\Gamma \gets \mathcal{C}_{goal}$
\SetKwProg{myalg}{algorithm2e}{}{}

 $\mathcal{T}_g \gets \emptyset$\\

            \If{goal.behavior is sequential}{
 $\mathcal{T}_g \gets$ SequenceNode \label{planning:ABL.alg.main.sequence}\\
            	 }
            \Else{
             $\mathcal{T}_g \gets $ ParallelNode \label{planning:ABL.alg.main.parallel}\\
            }


 $Instructions \gets $\FuncSty{GetInstructions($goal$)}

 \For{$instruction$ in $Instructions$} {
\Switch{instruction}{

            \Case{act\label{planning:ABL.alg.main.act}}{
            	 $\mathcal{T}_g$.\FuncSty{AddChild} (ActionNode(act))\\
            } 
            \Case{mental act}{      
                $\mathcal{T}_g$.\FuncSty{AddChild} (ActionNode(mental act))\\ \label{planning:ABL.alg.main.mentalact}
                }
            \Case{spawngoal\label{planning:ABL.alg.main.spawngoal}}{
                $\mathcal{T}_g$.\FuncSty{AddChild} (PlaceholderNode(spawngoal))
            } 
 }
 }
 
             \If{goal.precondition is not empty \label{planning:ABL.alg.main.preconditions}}{
             	 $\mathcal{T}_{g'} \gets$ SequenceNode \\
				 \For{$proposition$ in $precondition$} {
                $\mathcal{T}_{g'}$.\FuncSty{AddChild}(ConditionNode($proposition$))\\
                }
            $\mathcal{T}_{g'}$.\FuncSty{AddChild}($\mathcal{T}_{g}$)\\
            \Return{$\mathcal{T}_{g'}$}
            }\Else{
 \Return{$\mathcal{T}_g$}
 }
\end{algorithm2e}




\begin{algorithm2e}[h!]
\caption{Execute - input(node)}
 \label{planning:ABL.alg.Exec}
\Switch{node.Type}{
            \Case{ActionNode}{
            \If{not in conflict}{
            	 \FuncSty{Tick}(node)
            	 }
            } 
            \Case{PlaceholderNode}{
                node $\gets$\FuncSty{GetBT}(node.goal)\label{planning:ABL.alg.placeholder} \\
                    \FuncSty{Execute}(node) 

            } 
            \Other{\FuncSty{Tick}(node)}
 }
\end{algorithm2e}

The execution of the algorithm is simple. It first creates a BT from the initial ABL tree~$t$, collecting all the subgoals in a BT Parallel composition (Algorithm~\ref{planning:ABL.alg.main}, Line~\ref{planning:ABL.alg.main.T}), then, the tree is extended by finding a BT for each subgoal (Algorithm~\ref{planning:ABL.alg.main}, Line~\ref{planning:ABL.alg.main.subgoal}). Each subgoal is translated in a corresponding BT node (Sequence or Parallel, according to the behavior in the behavior library) whose children are the instruction of the subgoal. If a behavior has precondition instructions, they are translated into BT Condition nodes, added first as children (Algorithm~\ref{planning:ABL.alg.getbt}, Line~\ref{planning:ABL.alg.main.preconditions}). If a behavior has act or mental act instruction, they are translated into BT Action nodes (Algorithm~\ref{planning:ABL.alg.getbt}, Lines~\ref{planning:ABL.alg.main.act}-\ref{planning:ABL.alg.main.mentalact}) and set as children. If a behavior has spawngoal instruction (Algorithm~\ref{planning:ABL.alg.getbt}, Line~\ref{planning:ABL.alg.main.spawngoal}), this is added as a \emph{placeholder node}, which, when ticked, extends itself as done for the subgoals (Algorithm~\ref{planning:ABL.alg.Exec}, Line~\ref{planning:ABL.alg.placeholder}). 

%\begin{remark}
%The BT Sequence node in Algorithm~\ref{planning:ABL.alg.getbt} Line~\ref{planning:ABL.alg.main.sequence} can be with or without memory (Section~\ref{sec:design:memory} for details).
%\end{remark}

We are now ready to see how the algorithm is executed in a simple Pac-Man game.

\begin{example}[Simple Execution in Pac-Man]
\label{planning:ex:pacman}
While Pac-Man has to avoid being eaten by the ghosts, he has to compute the path to take in order to eat all Pills. The ABL tree Pac-Man agent is shown in Figure~\ref{planning:abl.fig.abtpacman1code}.
Running Algorithm~\ref{planning:ABL.alg.main}, the initial tree is translated into the BT in Figure~\ref{planning:abl.fig.abtpacman1BT}.
The subgoal \emph{eatAllPills} is expanded as the sequence of the two BT's Action nodes \emph{computeOptimalPath} and \emph{followOptimalPath}. The subgoal \emph{handleGhosts} is extended as a Sequence compostion of the Condition node \emph{ghostClose} (which is a precondition for handleGhosts) and a Parallel composition of placeholder nodes \emph{handleDeadlyGhosts} and \emph{handleScaredGhosts}. 
The BT is ready to be executed. Let's imagine that for a while Pac-Man is free to eat pills without being disturbed by the ghosts. For this time the condition  ghostClose is always false and the spawn of neither handleDeadlyGhosts nor handleScaredGhosts is invoked. Imagine now that a Ghost is close for the first time. This will trigger the expansion of  handleDeadlyGhosts and handleScaredGhosts. The expanded tree is shown in Figure~\ref{planning:abl.fig.abtpacman2BT}.
\begin{figure}[h]
\centering
\begin{lstlisting}

pacman_agent{
	initial-tree{
		subgoal handleGhosts();
		subgoal eatAllPills();
		}

	sequential behavior eatAllPills(){
		mental_act computeOptimalPath();
		act followOptimalPath();
	}
	parallel behavior handleGhosts(){
		precondition {
		    (ghostClose);
		}
		spawngoal handleDeadlyGhosts();
		spawngoal handleScaredGhosts();

	}
	sequential behavior handleScaredGhost(){
		precondition {
		    (scaredGhostClose);
		}
		act moveToScaredGhost();
	}
	sequential behavior handleDeadlyGhost(){
		precondition {
		    (deadlyGhostClose);
		}
		act keepDistanceFromDeadlyGhost();
	}

conflict keepDistanceFromDeadlyGhost moveToScaredGhost followOptimalPath; 
}
\end{lstlisting}
\caption{ABT tree for Pac-Man.}
\label{planning:abl.fig.abtpacman1code}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=\columnwidth]{ABTPacMan.pdf}
\caption{Initial BT of Example~\ref{planning:ex:pacman}.}
\label{planning:abl.fig.abtpacman1BT}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=\columnwidth]{ABTPacMan2.pdf}
\caption{Final BT of Example~\ref{planning:ex:pacman}.}
\label{planning:abl.fig.abtpacman2BT}
\end{figure}


\end{example}

%
%\begin{remark}
%As for exercise, the reader could modify the ABL agent for Pac-Man to spawn the goal for handleDeadlyGhosts and handleScaredGhosts separately.
%\end{remark}

%\subsection{Theoretical Analysis}
%%\begin{lemma}
%%An ABL planning domain $D_{ABL}$ can be translated as a STN planning domain $D_{STN}$.
%%\end{lemma}
%%
%%\begin{lemma}
%%An ABL planning problem can be translated as a STN planning problem.
%%\end{lemma}
%
%%\begin{lemma}
%%A solution $\pi$ of a ABL planning is a solution of the translated STN planning problem.
%%\end{lemma}



%\subsection{Computational Complexity}
%In this section we discuss the computational complexity and the expressivity of the ABL planning problem. We show that the complexity is PSPACE-hard if we do not allow parametrization (i.e. we defines operators with parameters) and it is EXPSPACE-hard if we allow parametrization.
%
%\begin{lemma}
%\label{planning:abl.lemma.stn}
%Let $p = \langle s_0, w,O,M \rangle$ be a totally ordered STN planning problem where $s_0$ is the initial state, $w = \langle U,C\rangle$ is the initial task network with $U$ being the task nodes and $C$ the constraints, $O$ is a set of operators, and $M$ is a set of STN methods. 
%Each subgoal $t_{SG}$ of a initial tree $t$ of a ABL planning problem can be formulated as a HTN planning problem $p$.
%
%\begin{proof}
%By defining the $U$ as the behaviors and spawngoal instruction specified in $t_{SG}$ ; $C$ as the precondition instruction in $t_{SG}$, the defined order of the instructions of $t_{SG}$, and the constraints instuctionsin $t_{SG}$;  $O$ as all ABL Operators $M$ as Algorithm~\ref{planning:ABL.alg.main.expand}, the ABL planning problem is defined as a totally ordered STN planning problem.
%
%\end{proof}
%\end{lemma}
%
%
%\begin{lemma}
%The complexity of ABL planning is PSPACE-hard.
%\begin{proof}
%By Lemma~\ref{planning:abl.lemma.stn}, a ABL planning problem can be translated in a STN planning problem. Hence, according to~\cite{erol1994htn}, the complexity of the proposed approach PSPACE-hard as we have ordering restriction on primitive and non primitive tasks and we do not allow parametrization.
%\end{proof}
%\end{lemma}
%
%\begin{corollary}
%If ABL language is extended to allow parametrization, the complexity is EXPSPACE-hard.
%\begin{proof}
%According to~\cite{erol1994htn}, the complexity of the proposed approach EXPSPACE-hard as we have ordering restriction on primitive and non primitive tasks and we allow parametrization.
%\end{proof}
%
%\end{corollary}

\clearpage
\subsection{Brief Results of a Complex Execution in StarCraft}

\begin{figure}[h]
\centering
\includegraphics[width=0.6\columnwidth]{StarCraft.jpg}
\caption{A screenshot of StarCraft showing two players engaged in combat~\cite{weber2011building}.}
\label{planning:abl.fig.starcraft}
\end{figure}

We will now very briefly describe the results from a more complex scenario, from~\cite{weber2011building}.
%. The example is an adaptation of the results of The adaptation consists in the translation of the semantic of the BT used.
One of the most well known strategy computer games that require
multi-scale reasoning is the real-time strategy game StarCraft. In StarCraft the players manage groups of units to compete for the control of the map by gathering resources
to produce buildings and units, and by researching
technologies that unlock more advanced abilities.
Building agents that perform well in this domain is challenging
due to the large decision space~\cite{aha2005learning}. StarCraft
is also a very fast-paced game, with top players performing above
300 actions per minute during peak intensity episodes~\cite{mccoy2008integrated}. This shows
that a competitive agent for StarCraft must reason quickly
at multiple scales in order to achieve efficient
game play.


\begin{example}
\label{planning:abl.ex.starcraft}

The StarCraft ABL agent is composed of three high-level managers: Strategy manager, responsible for the strategy selection and attack timing competencies; Production manager, responsible for the worker units, resource collection,
and expansion; and Tactics manager, responsible for the combat tasks and micromanagement
unit behaviors. The initial ABL tree takes the form of Figure~\ref{planning:abl.fig.starcrafttree}.


\begin{figure}[h]
\centering
\begin{lstlisting}
initial-tree{
	subgoal ManageTactic();
	subgoal ManageProduction();
	subgoal ManageStrategy();
	}
\end{lstlisting}
\caption{ABT tree for StarCraft.}
\label{planning:abl.fig.starcrafttree}
\end{figure}
Further discussions of the specific managers and behaviors  are available in~\cite{weber2011building}, and  a portion of the BT after some rounds of the game is shown in Figure~\ref{planning:abl.fig.starcraftbt}. 


\begin{figure}[h]
\centering
\includegraphics[width=\columnwidth]{StarCraftBT}
\caption{A Portion of the tree of the ABL StarCraft agent.}
\label{planning:abl.fig.starcraftbt}
\end{figure}

\end{example}


\begin{table}
\begin{center}

   \begin{tabular}{| l | l | l | l |}
    \hline
    Map / Race &  Protoss & Terran & Zerg \\ \hline \hline
	Andromeda & 85\% & 55\% & 75\% \\
	Destination & 60\% & 60\% & 45\% \\
	Heartbreak Ridge & 70\% & 70\% & 75\% \\
	Overall & 72\% & 62\% & 65\% \\
    \hline
    \end{tabular}
    \end{center} 
    \caption{Win rate on different map/race combination over 20 trials~\cite{weber2010reactive}.}
    \label{planning:abl.tab.starcraftresult}
\end{table}


In~\cite{weber2010reactive} the ABL agent of Example~\ref{planning:abl.ex.starcraft} was evaluated against the build-in StarCraft AI. The agent was tested against three professional gaming maps: Andromeda, Destination, and Heartbreak Ridge; against all three races: Protoss, Terran, and Zerg over 20 trials. The result are shown in Table~\ref{planning:abl.tab.starcraftresult}. The ABL agent scored an overall win rate of over 60\%, additionally, the agent was capable to perform over 200 game actions per minute, highlighting the capability of the agent to combine low-level tactical task with high-level strategic reasoning.

\section{Comparison between PA-BT and ABL}
So, faced with a BT planning problem, should we choose PA-BT or ABL?
The short answer is that PA-BT is focused on creating a BT using a planning approach,
whereas ABL is a complete planning language in itself, using BT as an execution tool.
PA-BT is also better at exploiting the Fallback constructs of BTs, by iteratively expanding conditions into  PPAs,
that explicitly include fallback options for making a given condition true.
%(add text here Michele!)

%\blue{To address high-level strategic reasoning needs, our strategy
%manager makes decisions about what buildings to build.
%However, the construction manager handles the details of
%building placement and specific unit orders when instructed
%to build something by the strategy manager. Thus we employ
%manager and message-passing patterns to help make the code
%more modular and to effectively reason cooperatively about
%a task like construction.
%Squad-based tactics and micromanagement are handled by
%the tactics manager. For some units, each individual unit has
%its own behavior hierarchy that directs its actions, authored
%using the micromanagement behavior pattern. This strategy
%is effective for quick harassing units that operate independently.
%However, it becomes cumbersome when coordinated
%tactics are required, because individual units cannot reason
%efficiently about the context of the entire battle. For this
%reason, some units are managed in groups, using behaviors
%written at the squad level.
%In StarCraft, vultures are a versatile unit effective for harassing
%enemy melee units, laying mine fields and supporting
%tanks. These tasks require different levels of cooperation.
%When harassing enemy forces, vultures are controlled at a
%per-unit level to avoid taking damage from enemy melee
%units. When supporting tanks, vultures work as a squad and
%provide the first line of defense. These dual roles exemplify
%the problem of multi-scale reasoning.
%A subset of the agents behaviors is shown in Figure 9.
%The root behavior starts several daemon behaviors that spawn
%the different managers. Each manager then continuously
%pursues a set of subgoals concurrently. In this example, the
%strategy manager is responsible for deciding when to produce
%factories and when to attack the opponent, the production
%manager constantly trains vultures, and the tactics manager
%spawns micromanagement behaviors for produced vultures.
%The agent coordinates squad behavior through the use
%of squad WMEs. The attack enemy behavior is a message
%producer that adds a squad WME to working memory
%when executed. The squad task behavior is an event-driven
%behavior that reacts to squad WMEs. Upon retrieval of a
%squad WME, a vulture will abort any micromanagement task
%that it is engaged in, and defer to orders issued by a squad
%behavior. This is accomplished by a context condition within
%the micromanagement behavior that suspends it when the
%vulture is assigned to a squad. The key difference between
%this scheme and one where a squad-specific behavior was
%implemented within the micromanagement behavior is that
%the squad behavior reasons at a higher level than the individual
%unit, and so it can give an order to a particular vulture
%based on a larger context. The EISBot manages individual
%units as well as the formulation of squads within a unified
%environment, enabling the agent to dynamically assign units
%to roles based on the current situation.
%By using managers, daemon behaviors, and message passing,
%our agent is able to reason about different goals at different
%scales simultaneously, and to coordinate that reasoning
%to achieve a coherent result. Goals at different scales can
%not only override one another when necessary, but they can
%pass messages to influence or direct each others behavior.
%This leads ultimately to an agent that is responsive, flexible,
%and extensible: an agent that is able to respond to highly
%specific circumstances appropriately without losing track of
%long-term goals.
%}


%\subsection{Complexity and Expressivity}
%\subsection{Theoretical Analysis}
%
%
%\subsubsection{Algorithm Execution on Graphs}

%\section{Conclusions}
%\label{planning:sec:conclusions}
%{
%
%In this chapter, we showed ABL and PA-BT, two BT-based automated planning approaches applied in multi-scale games and robotics respectively.
%
%
%ABL is a an automated planning framework with ability to plan short-term tasks while working towards long-term goals. The framework takes advantages of the modular structure of BTs by allowing the user to 
%describe subgoals in an abstract fashion. The details of each subgoal are then described in a later stage of the design process. The BT is expanded at runtime where all the details for the subgoals are available.
%We provided different experiments of the approach illustrating the applicability of ABL in a real-world video game.
%
%PA-BT is a framework to automatically synthesize a BT that satisfies a goal given a set of abstract action and condition nodes. The framework combines the advantages of BTs, in terms of modularity and reactivity, with the infinite dimensional planning capability of the HBF algorithm showed its properties in terms of reactiveness, safety, and fault tolerance. We provided  several experiments illustrating the  applicability of PA-BT in dynamic and unpredictable scenarios. The resulting approach is an attempt to address the challenges regarding blending planning and acting that have been identified in the planning community.
%}





%\bibliographystyle{IEEEtran}
%\bibliography{actionPlanningRefs,behaviorTreeRefs}

\endinput
