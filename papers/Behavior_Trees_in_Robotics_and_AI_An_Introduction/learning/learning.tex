% !TEX root = ../CRCBookLearning.tex

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Init
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\graphicspath{{./learning/figures/}}



\chapter{Behavior Trees and Machine Learning}
\label{ch:learning}

In this chapter, we describe how learning algorithms can be used to automatically create BTs, using ideas from~\cite{pereira2015framework,colledanchise2015learning}.
First, in Section~\ref{sec.gp}, we present a mixed learning strategy that combines a greedy element with Genetic Programming (GP) and show the result in a game and a robotic example. Then, in Section~\ref{sec:rl}, we present a Reinforcement Learning (RL) algorithm applied to BTs and show the results in a game example. Finally in Section~\ref{sec.dem} we overview the main approaches used to learn BTs from demonstration.




\section{Genetic Programming Applied to BTs}
\label{BG.GP}
The capability of an agent to learn from its own experience can be realized by imitating natural evolution. GP is an optimization algorithm that takes inspiration from biological evolution~\cite{rechenberg1994evolution} where a set of \emph{individual} policies are evolved until one of them solves a given optimization problem good enough.

In a GP approach a  particular set of individuals is called a \emph{generation}. At each GP iteration, a new generation is created from the previous one. First, a set of individuals are created by applying the operations \emph{cross-over} and \emph{mutation} to the previous generation. Then a subset of the individuals are chosen through \emph{selection} for the next generation based upon a user-defined reward. We will now describe how these three operators can be applied to BTs.


%Exploiting the fact that BTs generalize And-Or-Trees (see Section XXX \todo{add Section}) and also provide very natural chromosome mappings, 
%it is possible to combine the performance of Genetic Programming with a greedy element. The connection to And-Or-Trees makes sure that the learning outcome is not black box, but perfectly human readable. 
%
%To illustrate the approach we solve two different problems, one in a gaming environment and one on a real robot.
%Using the open-source \textit{Mario AI} benchmark, we show the applicability of the framework on complex dynamic environments
%and compare the performance of the approach with the state-of-the art. Using a KUKA Youbot robot, we show the applicability of the framework on real robot problems.


\textbf{Crossover of two BTs}
The crossover is performed by randomly swapping a subtree from one BT with a subtree of another BT at any level. Figure~\ref{PA.fig.CrossoverBefore} and Figure~\ref{PA.fig.CrossoverAfter} show two BTs before and after a cross-over operation.

\begin{figure}[h]
\centering
\begin{subfigure}[b]{0.7\columnwidth}
\includegraphics[width=\columnwidth]{CrossOverBefore.pdf}
\caption{BTs before the cross-over of the highlighted subtrees.}
\label{PA.fig.CrossoverBefore}
\end{subfigure}
~
\begin{subfigure}[b]{0.7\columnwidth}
\centering
\includegraphics[width=\columnwidth]{CrossOverAfter.pdf}
\caption{BTs after the cross-over of the highlighted subtrees.}
\label{PA.fig.CrossoverAfter}
\end{subfigure}
\caption{Cross-over operation on two BTs.}
\label{PA.fig.Crossover}
\end{figure}

\begin{remark}
The use of BTs as the knowledge representation framework in the GP avoids the problem of logic violation during cross-over stressed in~\cite{fu2003genetic}. The logic violation occurs when, after the cross-over, the resulting individuals might not have a  consistent logic structure. One example of this is the crossover combination of two FSMs that might lead to a logic violation in terms of some transitions not leading to an existing state.
\end{remark}

\textbf{Mutation of a BT}
\label{bg.mutation}
The mutation is an unary operation that replaces a node in a BT with another node of the same type (i.e. it does not replace an execution node with a control flow node or vice versa). Mutations increase diversity, which is crucial in GP. To improve convergence properties it is common to use  so-called \emph{simulated annealing}, performing the mutation on a large number of nodes of the first generation of BTs and gradually reducing  the number of mutated nodes in each new generation. In this way we start with a very high diversity to avoid getting stuck in possible local minima of the objective function of the optimization problem, and  reduce diversity over time as we get closer to the goal.

\textbf{Selection of BTs}
In the selection step, a subset of the individuals created by mutation and crossover are selected for the next generation.
 The  process is  random,
giving each BT  a survival probability $p_i$. This probability is based upon  the \emph{reward function} which quantitatively measures the fitness of the agent, i.e., how close the agent gets to the goal. 
A common method to compute the survival probability of an individual is the Rank Space Method~\cite{mitchell1997machine},
where the designer first sets
 $P_c$ as the probability of the highest ranking individual, then we sort the BTs in  descending order w.r.t. the reward. Finally, the probabilities are defined as follows:
\begin{eqnarray}
p_k = &(1-P_c)^{k-1}P_c &\; \forall k \in \{1,2,\ldots,N-1 \} \\
p_N = &(1-P_c)^{N-1}&
\end{eqnarray}
where $N$ is the number of individuals in a generation.



%\section{Problem Formulation}
%In this section we first make a set of 
%assumptions, then we state the main problem.
%
%
%\begin{assumption}
%\label{pf.ass.actcond}
%
%The agent can perform $N_a$ actions and check $N_c$ binary conditions.
%\end{assumption}
%
%
%\begin{assumption}
%The agent can verify if an action has succeeded, failed or if it is running.
%\end{assumption}
%
%\begin{assumption}
%There exists a reward function $\rho: \mathbb{R}^n \to [0,1]$. It takes the value $1$ if and only if the goal is reached.
%\end{assumption}
%
%
%\begin{assumption}%[Replacing the following two?]
%\label{pf.ass.safe2}
%For each safe state (i.e. a state from which the system can avoid an undesired outcome), there exists a finite sequence of actions that, when executed, leads to the goal (i.e. leads to a state $\tilde x$ such that $\rho(\tilde x)=1$), without entering an un-safe state.
%\end{assumption}
%
%
%
%
%\begin{assumption}
%The system dynamics can be described by the following continuous time equations:
%\begin{eqnarray}
%\dot x(t) =& f(x(t),u(x(t)))
%\label{PF.dynamic1}\\
%y(t) =& g(x(t))
%\label{PF.dynamic2}\\
%u(x(t)) =& \sum_{k=1}^{N_a} \phi(k,y(t))\cdot \alpha_k(x(t))
%\label{PF.dynamic}\end{eqnarray}
%where $x \in \mathbb{R}^n$ is the system's state, $y \in Y \subset 2^{  \{0,1\} \times \{1,2,\cdots, N_c\}} \times 2^{ \{0,1,2\} \times \{1,2,\cdots, N_a\}} $ is the system's output which collects the output value of conditions (true or false) and action (running, success, or failure), $u \in \mathbb{R}^p$ is the system's input. $f: \mathbb{R}^n \times \mathbb{R}^p \to\mathbb{R}^n $ describes how the system evolves, $g: \mathbb{R}^n \to Y$ describes what can be measured from the system, $\alpha_k: \mathbb{R}^n \to \mathbb{R}^p$ describes the effect of an action $k$, $\phi: \mathbb{N} \times Y \to \{0,1\}$ describes which action is executed in the BT, i.e. $\phi(\tilde k, \tilde y)=1$ if and only if the action $\tilde k$ is executed whenever the system measures (the conditions) $\tilde y$.
%\end{assumption}
%
%\begin{problem}
%Given the number of actions available $N_a$ and the system output $y(t)$ at any time $t$ described as above, define a function $\phi$ such that $\displaystyle \lim_{t \to \infty} \rho(x(t)) = 1$. That is define a BT such that the system reaches the goal.
%\label{PF.problem}
%\end{problem}
%\begin{remark}
%Note that the approach starts without knowing the effects of actions and meaning of conditions.
%\end{remark}


\section{The GP-BT Approach}
\label{sec.gp}
In this section we outline the GP-BT approach \cite{colledanchise2015learning}. We begin with an example, describing the algorithm informally, and then  give a  formal description in Algorithm~\ref{PS.ALG.LA}.
GP-BT follows a mixed learning strategy,  trying a greedy algorithm first and then applying a GP algorithm when needed. This mixed approach reduces the learning time significantly, compared to using pure GP, while still resulting in a fairly compact BT that achieves the given objective.


%
%
We now provide an example to describe the algorithm informally.
%Then we state a few
%definitions needed to give a formal
%description in Algorithm~\ref{PS.ALG.LA}.

\begin{figure}[h]
\centering
\includegraphics[width=0.3\columnwidth]{InitialBT}
\caption{The initial BT is a combination of the BT guaranteeing safety, and the BT \emph{Learn} that will be expanded during the learning.  }
\label{PA.fig.InitialBT}
\end{figure}


\begin{example}
Consider the case of the Mario AI setup in Figure \ref{res.fig.mario},  starting with the BT in Figure~\ref{PA.fig.InitialBT}.

The objective of Mario is to reach the rightmost end of the level.
The safety BT is optional, but motivated by the need 
to enable guarantees that the agent avoids some regions of the state space that are known by the user to be un-safe.
Thus, this is the only part of GP-BT that requires user input.

The un-safe regions must have a conservative margin to enable the safety action to act before it is too late (see Section~\ref{properties:sec:safety}).
Thus we cannot use the enemies as unsafe regions as
Mario needs to move very close to those to complete the level.
Therefore, for illustrative purposes, we let the safety action guarantee that Mario never reaches the leftmost wall of the level.

Mario starts really close to the left most wall, so the first thing that happens is that the safety action moves Mario a bit to the right. Then the Learn action is executed.

This action first checks all inputs and creates a BT, $\bt_{cond_\tau}$, of conditions that returns Success if and only if all inputs correspond to the current ``situation", as will be explained below.

Then the learning action executes all single actions available to Mario, e.g. go left, go right, jump, fire etc. and checks the resulting  reward.

All actions yielding an increase in the reward are collected in a Fallback composition BT, $\bt_{acts_i}$, sorted with respect to performance,  with the best performing action first.

If no single action results in a reward increase, the GP is invoked to find a combination of actions (arbitrary BTs are explored, possibly including parallel, Sequence and Fallback nodes) that produces an increase. Given some realistic assumptions, described in~\cite{colledanchise2015learning}, such a combination exists and will eventually be found by the GP according to previous results in \cite{rudolph1994convergence}, and stored in $\bt_{acts_i}$.

%Now the learning action checks if this particular set of actions have already been assigned as good options for other situations in the BT. If so, the two conditions BTs $\bt_{cond_i}$ and $\bt_{cond_j}$ are merged into a BT of conditions returning Success if and only if at least one of the two situations occur. This is a key part of the algoritm that prevents the learnt BT to explode in size, see Figure~\ref{PA.fig.merge}. 
%In the Mario example, there will be many different situations where the proper response is to move right. 
%All these are merged as described above.
%
Then, the condition BT, $\bt_{cond_\tau}$, is composed with the corresponding action BT, $\bt_{acts_i}$, in a Sequence node and the result is added to the previously learned BT, with a higher priority than the learning action.

Finally, the new BT is executed until once again the learning action is invoked.
\end{example}


\begin{algorithm2e}[h]

  \ArgSty{$\bt$} $\gets$ \FuncSty{"Action Learn"}\\
  \Do{$\rho < 1$}
  {
  	\FuncSty{Tick(SequenceNode\ArgSty{($\bt_{safe}$,$\bt$)})}\\

	  
    \If{\FuncSty{IsExecuted(Action Learn)}
}
    {
  		\ArgSty{$\bt_{cond}$} $\gets$ \FuncSty{GetSituation() \%Eq(\ref{eq:BTcond})}\\
  		\ArgSty{$\bt_{acts}$} $\gets$ \FuncSty{LearnSingleAction(\ArgSty{$\bt$}) \%Eq(\ref{eq:BTactSingle})}   \\
		\If{\ArgSty{$\bt_{acts}.NumOfChildren$} $=$ \ArgSty{0} }
		    {
			\ArgSty{$\bt_{acts}$} $\gets$ \FuncSty{GetActionsUsingGP(\ArgSty{$\bt$}) \%Eq(\ref{eq:BTactGP}) }\\
		    }
		    	\eIf{\FuncSty{IsAlreadyPresent(\ArgSty{$\bt_{acts}$})}}
		    {
		    \ArgSty{$\bt_{cond_{exist}}$} $\gets$ \FuncSty{GetConditions(\ArgSty{$\bt_{acts}$})}\\
		    \ArgSty{$\bt_{cond_{exist}}$} $\gets$ \FuncSty{Simplify}(FallbackNode((\ArgSty{$\bt_{cond_{exist}}$},\ArgSty{$\bt_{cond}$})) \\
		     %\ArgSty{$\bt$} $\gets$\ArgSty{$\bt$}   \%Eq(\ref{PA.eq.newBT2}) \\
		    }
		    {
		    \ArgSty{$\bt$} $\gets$ \FuncSty{FallbackNode(\FuncSty{SequenceNode(\ArgSty{$\bt_{cond}$},\ArgSty{$\bt_{acts}$})},\ArgSty{$\bt$})   \%Eq(\ref{PA.eq.newBT})}
		    }
		%\ArgSty{$\tilde \bt$} $\gets$ \FuncSty{Sequence(\ArgSty{$\bt_{cond}$},\ArgSty{$\bt_{acts}$})}\\

    }
	%    \ArgSty{$\bt$} $\gets$ \FuncSty{Fallback(\ArgSty{$\tilde \bt$},\ArgSty{$\bt$})}\\
	  %\ArgSty{$\rho$} $\gets$ \FuncSty{GetReward(\ArgSty{$\bt$})}\\ 
	  \ArgSty{$\rho$} $\gets$ \FuncSty{GetReward(SequenceNode\ArgSty{($\bt_{safe}$,$\bt$)})}\\
	  

  }
  \Return{$\bt$}\\
  \caption{Pseudocode of the learning algorithm}
  \label{PS.ALG.LA}
  
\end{algorithm2e}

\vspace*{-2em}
\subsection{Algorithm Overview}
\label{LA.AO}

To describe the algorithm in detail, we need a set of definitions.
First we let a situation be a collection of all conditions, sorted on whether they are true or not, then we define the following:

\paragraph*{\textbf{Situation}}
$\mathcal{S}(t) = [C_{T}^{(t)},C_{F}^{(t)}]$ is the situation vector, where $C_{T}^{(t)}=\{C_{T1},\ldots, C_{TN}\}$ is the set of conditions that are true at time $t$ and $C_{F}^{(t)}=\{C_{F1},\ldots, C_{FM}\}$ is the set of conditions that are false at time $t$.

Then, using the analogy between AND-OR trees and BTs \cite{colledanchise2016behavior}, we create the BT that returns success only when a given situation occurs.
\paragraph*{$\mathbfcal{\bt}_\mathbf{cond_\tau}$}
$\bt_{cond_\tau}$ is the BT representation of $\mathcal{S}(\tau)$.
\begin{eqnarray}
 \bt_{cond_\tau}&\triangleq& \mbox{Sequence}(\mbox{Sequence}(C_{T1},\ldots, C_{TN}), \nonumber \\
 &&\mbox{Sequence
 %\footnote{The decorator \emph{invert} invert the Success/Failure starus of its child}
 }(\mbox{invert}(C_{F1}),\ldots,\mbox{invert} (C_{FM})) ) \hfill\label{eq:BTcond}
\end{eqnarray}
Figure~\ref{PA.fig.BTcond} shows a BT composition representing a situation $\mathcal{S}(\tau)$. 

\begin{figure}[h]
\centering
\includegraphics[width=0.6\columnwidth]{BTConditions.pdf}
\caption{Graphical representation of $\bt_{cond_\tau}$, $c_{Fi} \in C_F^\tau$, $c_{Tj} \in C_T^\tau$. The Decorator is the negation Decorator  (i.e it inverts the Success/Failure status). This BT returns success only when all $c_{Tj}$ are true and all $c_{Fi}$ are false. }
\label{PA.fig.BTcond}
\end{figure}


\paragraph*{$\mathbfcal{\bt}_\mathbf{acts_i}$}
Given a small $\epsilon > 0$,
if at least one action results in an increase in reward, $\Delta \rho > \epsilon$,
 let $A_{P1},\ldots, A_{P\tilde N}$ be the list of actions that result in such an improvement,
 sorted on the size of $\Delta \rho$,
then, $\bt_{acts_i}$ is defined as:
\begin{equation}
 \bt_{acts_i}= \mbox{FallbackWithMemory}(A_{P1},\ldots, A_{PN}) \label{eq:BTactSingle}
\end{equation}
 else, $ \bt_{acts_i}$ is defined as the solution of a GP algorithm that terminates when an improvement such that $\Delta \rho > \epsilon$ or $\rho(x)=1$ is found, i.e.:
 \begin{equation}
 \bt_{acts_i}= GP(\mathcal{S}(t)) \label{eq:BTactGP}
\end{equation}
\paragraph*{\textbf{New BT}}
 If $\bt_{acts_i}$ is not contained in the BT, the new BT learned is given as follows: 
\begin{equation}
\label{PA.eq.newBT}
\bt_i\triangleq \mbox{Fallback}(\mbox{Sequence}(\bt_{cond_i}, \bt_{acts_i}), \bt_{i-1})
\end{equation}
Else, if $\bt_{acts_i}$ is contained in the BT, i.e., there exists an index $j \neq i$ such that $\bt_{acts_i}=\bt_{acts_j}$,
this means there is already a situation identified where $\bt_{acts_i}$ is the appropriate response. 
Then, to reduce the number of nodes in the BT, 
we generalize the two special cases where this response is needed.
This is done by combining $\bt_{cond_i}$ with $\bt_{cond_j}$, that is, we find the BT, $\bt_{cond_{ij}}$, that returns success if and only if $\bt_{cond_i}$ or $\bt_{cond_j}$ return success. Formally, this can be written as
\begin{equation}
\label{PA.eq.newBT2}
\bt_i\triangleq \bt_{i-1}.\mbox{replace}(\bt_{cond_i},\mbox{simplify}((\bt_{cond_i},\bt_{cond_j})))
\end{equation}

Figure~\ref{PA.fig.merge} shows an example of this simplifying procedure. As can be seen this simplification generalizes the policy by iteratively removing conditions that are not relevant for the application of the specific action. It is thus central for keeping the number of nodes low, as seen below in Fig.~\ref{ER.fig.nodes}.
\begin{figure}[h]
        \centering
        \begin{subfigure}[b]{0.3\columnwidth}
                \centering
                \includegraphics[width=\columnwidth,clip]{ExampleMerge1}
                \caption{The action \emph{Avoid Obstacle} is executed if there is an obstacle in front and the sun is {\bf not} shining. }
        \end{subfigure}%
       ~ %add desired spacing between images, e. g. ~, \quad, \qquad etc.
          %(or a blank line to force the subfigure onto a new line)
        \begin{subfigure}[b]{0.3\columnwidth}
                \centering
                \includegraphics[width=\columnwidth,clip]{ExampleMerge2}
                \caption{The action \emph{Avoid Obstacle} is executed if there is an obstacle in front and the sun is shining. \;\;\;\;}
        \end{subfigure}
        ~ %add desired spacing between images, e. g. ~, \quad, \qquad etc.
          %(or a blank line to force the subfigure onto a new line)
        \begin{subfigure}[b]{0.3\columnwidth}
                \centering
                \includegraphics[width=\columnwidth,clip]{ExampleMerge3}
                \caption{Simplified merged Tree. The action \emph{Avoid Obstacle} is executed if there is an obstacle in front. \;\; \;\;\;\;\;\;\;\;}
        \end{subfigure}
        \caption{Example of the simplifying procedure in (\ref{PA.eq.newBT2}). The two learned rules (a) and (b) are combined into (c): The important condition appears to be  \emph{Obstacle in Front}, and there is no reason to check the condition \emph{The Sun is Shining}. These simplifications generalize the policies, and keep the BT sizes down.  }
                        \label{PA.fig.merge}

\end{figure}


Given these definitions, we can go through the steps listed in Algorithm~\ref{PS.ALG.LA}.
Note that the agent runs $\bt_i$ until a new situation is encountered
which requires learning an expanded BT, or the goal is reached.

The BT $\bt$ is first initialized to be a single action, \emph{Action Learn}, which will be used to trigger the learning algorithm. Running Algorithm~\ref{PS.ALG.LA}  we execute the Sequence composition of the safe subtree $\bt_{safe}$ (generated manually, or using a non-learning approach) with the current tree $\bt$ (Algorithm~\ref{PS.ALG.LA} Line $3$). The execution of $\bt_{safe}$ overrides the execution of $\bt$ when needed to guarantee safety. If the action \emph{Action Learn} is executed, it means that the current situation in not considered in neither $\bt_{safe}$ nor $\bt$, hence a new action, or action composition, must be learned. The framework first starts with the greedy approach (Algorithm~\ref{PS.ALG.LA}, Line $6$) where it tries each action and stores the ones that increase the reward function, if no such actions are available (i.e. the reward value is a local maximum), then the framework starts learning the BT composition using the GP component (Algorithm~\ref{PS.ALG.LA}, Line $8$). Once the tree $\bt_{acts}$ is computed, by either the greedy or the GP component, the algorithm checks if $\bt_{acts}$ is already present in the BT as a response to another situation, and the new tree $\bt$ can be simplified using a generalization of the two situations (Algorithm~\ref{PS.ALG.LA}, Line $11$). Otherwise, the new tree $\bt$ is composed by the selector composition of the old $\bt$ with the new tree learned (Algorithm~\ref{PS.ALG.LA}, Line $13$). The algorithm runs until the goal is reached. The algorithm is guaranteed to lead the agent to the goal, under reasonable  assumptions~\cite{colledanchise2015learning}.




\subsection{The Algorithm Steps in Detail}
\label{LA.AS}
We now discuss Algorithm~\ref{PS.ALG.LA} in detail.
\subsubsection{GetSituation (Line 5)}
This function returns the tree $\bt_{cond}$ which represents the current situation. $\bt_{cond}$ is computed according to Equation~\eqref{eq:BTcond}.
\subsubsection{LearnSingleAction (Line 6)}
This function returns the tree $\bt_{acts}$ which represent the action to execute whenever the situation described by $\bt_{cond}$ holds. $\bt_{acts}$ is a Fallback composition with memory of all the actions that, if performed when $\bt_{cond}$ holds, increases the reward. The function \emph{LearnSingleAction} runs the same episode $N_a$ (number of actions) times executing a different action whenever $\bt_{cond}$ holds. When trying a new action, if the resulting reward increases, this action is stored. All the actions that lead to an increased reward are collected in a Fallback composition, ordered by the reward value. This Fallback composition, if any, is then returned to Algorithm~\ref{PS.ALG.LA}. 
\subsubsection{LearnActionsUsingGP (Line 8)}
If \emph{LearnSingleAction} has no children (Algorithm~\ref{PS.ALG.LA}, Line 7) then there exists no single action that can increase the reward value when
the situation described by $\bt_{cond}$ holds. In that case the algorithm learns a BT composition of actions and conditions that must be executed whenever $\bt_{cond}$ holds. This composition is derived as described in Section~\ref{BG.GP}.
\subsubsection{Simplify (Line 11)}
If the resulting $\bt_{acts}$ is present in $\bt$ (Algorithm~\ref{PS.ALG.LA}, Line 9) this means that there exist another situation $\mathcal{S}_{exist}$ described by the BT $\bt_{cond_{exist}}$, where the response in $\bt_{acts}$ is appropriate. To reduce the number of nodes in the updated tree, we create a new tree that captures both situations $\mathcal{S}$ and $\mathcal{S}_{exist}$. This procedure  removes from $\bt_{cond_{exist}}$ a single condition $c$ that is present in $C_F$  for one situation ($\mathcal{S}$ or $\mathcal{S}_{exist}$) and $C_S$ for the other situation. 

\begin{remark}
Note that the GP component is invoked exclusively whenever the greedy component fails to find a single action.
\end{remark}

%
%
%We follow a metaheuristic learning strategy, where we use a greedy algorithm first and when it fails, we use the GP. The GP will also be used when the greedy algorithm cannot provide any results or when the complexity of the solution is increased. This mixed-learning based heuristic approach reduces the learning time significantly compared to using pure GP, while still achieving an compact BT that satisfies a given goal.
%
%At the beginning we start with a BT that consists of only one node ``Learn" which initiates the proposed learning framework. The framework stores the current situation in a BT composition (i.e. a BT that return success if and only if the situation occur). To learn the action, we use a greedy search process where each action is executed. The set of actions that increases the reward are stored in a BT composition. If no actions are found, the GP process will be initiated with a population of binary trees (two nodes in a BT) with random node assignments consisting of combination of action nodes, and results in an initial BT that increases the reward value the most. We combine the new tree learned with the current one. The example below illustrate the approach.
%\begin{example}
%
%In this example we build a BT composition when a new situation is encountered and a set of actions is learnt. Let us imagine a robot that has to leave a room. The door is closed and locked and the robot has no key. Then the robot can either find another path to leave the room or break the door. 
%At time t, the continuations that are false are: $\{$\emph{Door Open}, \emph{Has Key}$\}$ and the conditions that are true are  \emph{Door Locked}; the set of available actions is $\{$\emph{Find Another Path}, \emph{Open The Door}, \emph{Break The Door}$\}$. When we execute the learning algorithm the two actions that increase the rewards are: $\{$\emph{Find Another Path}, \emph{Break The Door}$\}$. The resulting BT composition is depicted in Figure~\ref{PA.fig.EX.LE}.
%
%\begin{figure}[h]
%\centering
%\includegraphics[width=0.7\columnwidth]{ExampleLearn}
%\caption{BT Learned .}
%\label{PA.fig.EX.LE}
%\end{figure} 
%Then the robot executes the action $Find Another Path$. If it fails (no other paths are available), it executes the action $Break the Door$. 
%
%\label{PA.EX.LE}
%\end{example}
%
%In the next stages, the resulting BT is executed again until the action ``Learn" is executed and we repeat the process until the goal is reached. 
%\begin{remark}
%The action ``Learn" is executed whenever a new situation occur. If at time $t_i$ the situation $\mathcal{S}(t_i)$ was already encountered another action will be executed. Hence we do not need to store the situations encountered.
%\end{remark}
%
%To drastically reduce the size of the BT, we merge two situations that require the execution of the same action as described later.
%Finally, when the goal is reached, we remove the possible unnecessary nodes in the BT by applying anti-bloat control operation. 
%
%\subsection{The Algorithm}
%\label{PA.LA}
%Algorithm~\ref{PS.ALG.LA} presents the pseudo-code for the learning algorithm. The learning algorithm has $2$ steps. The first step aims to identify which conditions have to be verified in order to perform some actions. The second step aims to learn the actions to perform.
%
%As mentioned earlier, the framework starts at the situation $\mathcal{S}(t_0)$. A greedy algorithm is used to try each action storing the ones that increase the reward. If no actions are found, it start the GP to learn a BT composition of actions. The selection mechanism used in this paper is a \emph{elitist rank space method} a variant of the rank space method where it always keeps the individual with highest reward. The mutation mechanism uses the  simulated annealing mentioned in section \ref{bg.mutation}. We call the learned BT $\bt_0$.
%The learning procedure is executed again until a new situation $\mathcal{S}(t_1)$ is encountered and a new BT $\bt_1$ is learned. The procedure stops when the goal is reached.
%The framework runs the learned BT, if any, as long as the fitness function increases. During this time the framework monitors the values of the conditions. When the fitness stops to increase, the conditions that have changed in a time window $\tau$ are stored in a BT composition. The user can choose the time window. Now, we need to create a BT composition of those conditions. The BT composition returns success only when the conditions changed during $\tau$ return the corresponding value i.e. the conditions that have changed to true returns success and the conditions that have changed to false returns failure. 


%\subsection{Genetic Programming}

%\begin{algorithm}[h]
%    \ArgSty{$\bt$} $\gets$ \FuncSty{MakeBinary(\ArgSty{$\bt$})}\\
%
%    \ArgSty{i} $\gets$  0 \\
%
%  \While{$i \leq$  \FuncSty{GetNodesNumber(\ArgSty{$\bt$})}}
%  {
%    \ArgSty{i} $\gets$  \ArgSty{i} + 1 \\
%    \ArgSty{$\bt_{rem}$} $\gets$ \FuncSty{RemoveSubtree(\ArgSty{$\bt$},$i$)}\\
%    \If{\FuncSty{GetReward(\ArgSty{$\bt_{rem}$})} $\geq$ \FuncSty{GetReward(\ArgSty{$\bt$})}}
%    {
%              \ArgSty{$\bt$} $\gets$  \ArgSty{$\bt_{rem}$} \\
%               \ArgSty{i} $\gets$  0 \\
%    }
%  }
%  \Return{\ArgSty{$\bt$}}
%  \caption{Pseudocode of a anti-bloat control for inefficient subtree(s) removal.}
%  \label{PS.ALG.REM}
%\end{algorithm}
%




%\begin{remark}
%The procedure is trivially implementable when the policy is encoded by using a BT due to its tree structure.   
%\end{remark}


%
%\section{Safety and Robustness}
%\begin{figure}[h]
%\centering
%\includegraphics[width=0.4\columnwidth]{SafeLearning}
%\caption{Example of Safe Learning. The BT learned and the learning algorithm are executed only if the system is safe.  }
%
%\label{PA.fig.SAFE}
%\end{figure}
%
%Using BT as the knowledge representations we can achieve a \emph{safe} learning. We can add an handcrafted subtree to guarantee safety requirements during the learning. Fig \ref{PA.fig.SAFE} shows an example. More details on safety using BT can be found in~\cite{colledanchise14}.
%
%Moreover the learning procedure can achieve some degree of robustness in sense of~\cite{colledanchise14}. For example, the BT learned in Fig~\ref{PA.fig.EX.LE} is robust as the robot can leave the room in two different ways (finding another path or breaking the door).
%
\subsection{Pruning of Ineffective Subtrees}
\label{PA.removal}
Once obtained the BT that satisfies the goal, we can search for ineffective subtrees, i.e. those action compositions that are superfluous for reaching the goal. To identify the redundant or unnecessary subtrees, we enumerate the subtrees with a Breadth-first enumeration. We run the BT without the first subtree and checking whether the reward function has a lower value or not. In the former case the subtree is kept, in the latter case the subtree is removed creating a new BT without the subtree mentioned. Then we run the same procedure on the new BT. The procedure stops when there are no ineffective subtree found. This procedure is optional.




%\subsection{Relation to general APs}
%
%
%According to~\cite{mitchell1997machine}, there are four aspects that should be
%described for every suggested AP algorithm.
%The \emph{knowledge representation}. That is the type of knowledge that an AP will learn must be defined; The \emph{extraction of experience}. That is how learning examples are collected; The \emph{learning algorithm}. That is how to capture patterns from the collected experience; and the \emph{exploitation of collected knowledge}. That is how the AP benefits from the learned knowledge. We now address these items in turn.
%
%\textbf{Knowledge representation}
%The knowledge is represented as a BT. The BT can be seen as a modular, transparent, and human readable mapping between conditions and actions.
%
%\textbf{Extraction of the experience}
%%For every new initial state $s_0$ the framework learns a new BT. We make fallback composition of learned BTs. For each $s_0 \in \mathcal{S}$ the BT runs the related sub tree.
%The experiences (know\-ledge) are extracted in terms of conditions observed from the environment using sensory perceptions in the autonomous agents. Examples of conditions for a agent could be ``enemy in front", ``obstacle in front", ``level reached", ``no bullets left", etc.
%Given a description of the situation, different actions or action combinations are executed, and the corresponding reward is gathered.
%
%\textbf{Learning algorithm}
%The learning algorithm is sketched in Algorithm~\ref{PS.ALG.LA} and described in Sections~\ref{LA.AO} and~\ref{LA.AS}.
%
%\textbf{Exploitation of the collected knowledge}
%For each new situation, an action or set of actions leading to improved reward is found. Sometimes it is a set of different single actions, that can all be tried in a similar situation, starting with the most promising and, if that fails due to uncertainties, continuing with the second most promising and so on, see Equation (\ref{eq:BTactSingle}).
%When needed the action is an elaborate combination of e.g. parallel actions, produced by the GP, see (\ref{eq:BTactGP}).
%These actions are added to the BT in two different ways, depending on if a similar set of actions were found for a different situation or not, see Equations (\ref{PA.eq.newBT}-\ref{PA.eq.newBT2}).

\subsection{Experimental Results}

\begin{figure}[t]
        \centering
        \begin{subfigure}[b]{0.5\columnwidth}
                \centering
                \includegraphics[width=\columnwidth,height=4cm,clip]{receptive.png}
                \caption{Mario AI benchmark. }
                \label{res.fig.mario}
        \end{subfigure}%
       ~ %add desired spacing between images, e. g. ~, \quad, \qquad etc.
          %(or a blank line to force the subfigure onto a new line)
        \begin{subfigure}[b]{0.5\columnwidth}
                \centering
                \includegraphics[width=\columnwidth,height=4cm,clip]{ExpSetupYoubot.png}
                \caption{KUKA Youbot benchmark}
                \label{res.fig.youbot}              
        \end{subfigure}
         %add desired spacing between images, e. g. ~, \quad, \qquad etc.
          %(or a blank line to force the subfigure onto a new line)
        \caption{Benchmarks used to validate the framework.}
         \label{res.fig}        
\end{figure}


In this section we apply GP-BT to two problems. One is the \emph{Mario AI benchmark}  (Figure~\ref{res.fig.mario}) and one is  a real robot, the \emph{KUKA Youbot} (Figure~\ref{res.fig.youbot}). The results on the Mario AI benchmark shows the applicability of the GP-BT on a highly complex dynamic environment and
also allows us to compare the approach to the state-of-the-art. The results on the KUKA Youbot shows the applicability of GP-BT on a real robot.  

\subsubsection{Mario AI}
The Mario AI benchmark~\cite{karakovskiy2012mario} is an open-source software clone of Nintendo's Super Mario Bros used to test learning algorithms and game AI techniques. The task consists of moving the controlled character, Mario, through two-dimensional levels, which are viewed from the side. {Mario can walk and run to the right and left, jump, and (depending on the mode, explained below) shoot fireballs. Gravity acts on Mario, making it necessary to jump over gaps to get past them. Mario can be in one of three modes: \emph{Small}, \emph{Big}, and \emph{Fire} (can shoot fireballs).
The main goal of each level is to get to the end of the level, which means traversing it from left to right. Auxiliary goals include collecting as many coins as possible, finishing the level as fast as possible, and collecting the highest score, which in part depends on the number of enemies killed.
Gaps and moving enemies make the task more complex. If Mario falls down a gap, he loses a life. If he runs into an enemy, he gets hurt; this means losing a life if he is currently in the Small mode. Otherwise, his mode degrades from Fire to Big or from Big to Small. }

 
\textbf{Actions}
In the benchmark there are five actions available: \emph{Walk Right}, \emph{Walk Left}, \emph{Crouch}, \emph{Shoot}, and \emph{Jump}.

\textbf{Conditions}
In the benchmark there is a receptive field of observations as shown in Figure~\ref{res.fig.mario}. For each of the $25$ cells in the grid there are $2$ conditions available: \emph{the box is occupied by an enemy} and  \emph{the box is occupied by an obstacle}. There are two other conditions: \emph{Can Mario Shoot} and \emph{Can Mario Jump},
creating  a total of $52$ conditions.

%\begin{remark}
%Mario occupies at least a box in the grid. Each box in the grid can be occupied by an enemy; occupied by an obstacle; or free and there are two other conditions. If one may want to use a FSM-based learning approach, there will be $3^24 * 2^2 \approx 1.12 * 10 ^{12}$ states to explore. As mentioned above, the learned BT, on the long run, gets simpler and simpler due to the merging of situation and anti-bloat control.
%\end{remark}

\textbf{Reward Functions}
The reward function is given by a non linear function of the distance passed, enemies killed, number of times Mario is hurt, and time left when the end of the level is reached. The reward function is the same for every scenario.
%Figure\ref{Intro.fig.mario} illustrates the receptive field around Mario, used our experiments.

{
\textbf{Cross-Validation}
To evaluate the learned BT in an episode, we run a different episode of the same complexity (in terms of type of enemies; height of obstacles and length of gaps). In the Mario AI framework, this is possible by choosing different so-called \emph{seeds} for the learning episode and the validating episode. The result shown below are cross-validated in this way.
}

\textbf{GP Parameters}
Whenever the GP part is invoked, it starts with $4$ random BTs composed by one random control flow node and $2$ random leaf nodes. The  number of individuals in a generation is set to $25$.

\textbf{Scenarios}
We ran the algorithm in five different scenarios of increasing difficulty. {The first scenario had no enemies and no gaps, thus only requiring motion to the right and jumping at the proper places. The resulting BT can be seen in Figure~\ref{ER.fig.BTs1} where the action \emph{Jump} is executed if an obstacle is in front of Mario and the action \emph{Go Right} is executed otherwise. The second scenario has no obstacles but it has gaps. The resulting BT can be seen in Figure~\ref{ER.fig.BTs2}  where the action \emph{Jump} is executed if Mario is close to a gap. The third scenario has high obstacles, gaps and walking enemies. The resulting BT can be seen in Figure~\ref{ER.fig.BTs3} which is similar to a combination of the previous BTs with the addition of the action \emph{Shoot} executed as soon as Mario sees an enemy (cell $14$), and to \emph{Jump} higher obstacles Mario cannot be too close. Note that to be able to show the BTs in a limited space, we used the \emph{Pruning} procedure mentioned in Section~\ref{PA.removal}}.
A video is available that shows the performance of the algorithm in all 5 scenarios.\footnote {\url{https://youtu.be/QO0VtUYkoNQ}}

\begin{figure}[h]
\centering
\includegraphics[width=0.4\columnwidth]{Receptive_annotated}
\caption{Receptive field with cells' numbers.}
\label{ER.fig.receptiveannotated}
\end{figure}

\begin{figure}[h]
\centering
        \begin{subfigure}[t]{0.4\columnwidth}
\includegraphics[width=\columnwidth]{Scenario1.pdf}
\caption{Final BT for Scenario 1.}
\label{ER.fig.BTs1}
        \end{subfigure}%
\hfill
        \begin{subfigure}[t]{0.3\columnwidth}
        \centering
\includegraphics[width=\columnwidth]{Scenario2.pdf}
\caption{Final BT for Scenario 2.}
\label{ER.fig.BTs2}
        \end{subfigure}%
        
        \begin{subfigure}[b]{0.6\columnwidth}
        \centering
\includegraphics[width=\columnwidth]{Scenario3}
\caption{Final BT for Scenario 3.}
\label{ER.fig.BTs3}
        \end{subfigure}%
        \caption{Final BTs learned for Scenario 1-3.}
\end{figure}
\begin{figure}[t]
        \centering
        \begin{subfigure}[b]{0.5\columnwidth}
                \centering
                \includegraphics[width=\columnwidth]{rewardScenario1.pdf}
                \caption{ Comparison for Scenario 1}
                \label{ER.fig.rs1}
        \end{subfigure}%
       ~ %add desired spacing between images, e. g. ~, \quad, \qquad etc.
          %(or a blank line to force the subfigure onto a new line)
        \begin{subfigure}[b]{0.5\columnwidth}
                \centering
                \includegraphics[width=\columnwidth]{rewardScenario5.pdf}
                \caption{ Comparison for Scenario 5}
                \label{ER.fig.rs5}              
        \end{subfigure}
         %add desired spacing between images, e. g. ~, \quad, \qquad etc.
          %(or a blank line to force the subfigure onto a new line)
       % \caption{Reward value comparison. The blue solid line refers to our approach. The red dash-dotted line refers to the pure GP-based algorithm. The green dashed line refers to the FSM-based algorithm.}
        %\label{ER.fig.reward}
        \begin{subfigure}[b]{0.5\columnwidth}
                \centering
                \includegraphics[width=\columnwidth,clip]{growthScenario1.pdf}
                \caption{ Comparison for Scenario 1}
                \label{ER.fig.nns1}
        \end{subfigure}%
       ~ %add desired spacing between images, e. g. ~, \quad, \qquad etc.
          %(or a blank line to force the subfigure onto a new line)
        \begin{subfigure}[b]{0.5\columnwidth}
                \centering
                \includegraphics[width=\columnwidth,clip]{growthScenario5.pdf}
                \caption{ Comparison for Scenario 5}
                \label{ER.fig.nns5}              
        \end{subfigure}
         %add desired spacing between images, e. g. ~, \quad, \qquad etc.
          %(or a blank line to force the subfigure onto a new line)
        \caption{ Reward value comparison (a and b) and nodes number comparison (c and d). The blue solid line refers to GP-BT. The red dash-dotted line refers to the pure GP-based algorithm. The green dashed line refers to the FSM-based algorithm.}
                \label{ER.fig.nodes}
\end{figure}


We compared the performance of the GP-BT approach to a FSM-based algorithm of the type described in~\cite{garcia2000integrated} and to a pure GP-based algorithm of the type described in~\cite{scheper2014}.

%XXX the other GP with BTs does not start from scratch.
For all three algorithms, we measure the performance, in terms of the reward function, and the 
complexity of the learnt solution in terms of the number of nodes in the BT/FSM respectively. The data for the simplest and most complex scenario, 1 and 5, can be found in Figure~\ref{ER.fig.nodes}.
As can be seen in Figure~\ref{ER.fig.nodes}, the FSM approach generates a number of nodes that increases exponentially over time,
while the growth rate in GP-BT tends to decrease over time. We believe that this is due to the simplification step, described in Equation (\ref{PA.eq.newBT2}), where two different situations requiring the same set of actions are generalized by a merge in the BT.
The growth of the pure GP algorithm is very slow, as each iteration needs very long time to find a candidate improving the reward. {This is due to the fact that the number of conditions is larger than the number of actions, hence the pure GP approach often constructs BTs that check a large amount of conditions, while performing very few actions, or even none. Without a greedy component and the AND-OR-tree generalization with the conditions, a pure GP approach, like the one in~\cite{scheper2014},  is having difficulties  without any a-priory information.
}
Looking at the performance in Figure~\ref{ER.fig.nodes} we see that GP-BT is the only one who reaches a reward of $1$ (i.e. the task is completed) within the given execution time, not only for Scenario $5$, but also for the less complex Scenario $1$. 

 \begin{remark}
 Note that we do not compare GP-BT with the ones of the Mario AI challenge,
as we study problems with no a-priori information or model, whereas the challenge
provided full information about the task and environment.
 When the GP-BT learning procedure starts, the agent (Mario) does not even know that the enemies should be killed or avoided.
 \end{remark}

% We illustrate a quantitative comparison of the number of nodes and the reward value for Scenario~1 and~5. We executed the algorithm using the same scenarioâ€™s parameters with the same computer. Figure \ref{ER.fig.reward} shows that our approach converges to the solution faster than the compared algorithms. The other algorithms did not converge after a reasonable time. Fig \ref{ER.fig.nodes} shows that the number of nodes is relatively low. 

%\subsubsection{Scenario 1: No enemies and no cliffs}
%This is a simple case. The agent has to learn how to move towards the end of the level and how to jump obstacles. Due to the low complexity of the scenario, a single action was always enough to increase the reward (i.e. no GP needed).
%A youtube video shows the learning phase in real time (https://youtu.be/uaqHbzRbqrk).
%Figure \ref{ER.fig.marioAIBT} illustrates a resulting BT learned for the Scenario 1.
%
%
%
%
%\subsubsection{Scenario 2: Walking Enemies and No Cliffs}
%
%This is slightly more complex than Scenario 1. The agent has to learn how to move towards the end of the level, how to jump obstacles, and how to kill the enemies. A youtube video shows the learning phase in real time (https://youtu.be/phy98jbdgQc).
%
%Figure \ref{ER.fig.marioAIBT2} illustrates a resulting BT learned for the Scenario 2.
%
%
%\subsubsection{Scenario 3: Flying Enemies and Cliffs}
%For this scenario we used a newer version of the Benchmark. The agent has to learn how to move towards the end of the level, how to jump obstacles, and how to kill the enemies and how to avoid cliffs. The enemies in this testbed can fly. The selection method in the GP is the rank-space method.
%A youtube video shows the final BT (https://www.youtube.com/watch?v=YfvdHY-DXwM).
%
%We avoid to depict the final generated BT because of size restrictions (about 20 nodes).
%
The other scenarios have similar result. We chose to depict the simplest and the most complex ones.



\subsubsection{KUKA Youbot}
As mentioned above, we use the KUKA Youbot to verify GP-BT on a real scenario. We consider three scenarios, one with a partially known environment and two with completely unknown environments.

Consider the Youbot in Fig.~\ref{res.fig.youbot}, the conditions are given in terms of the 10 receptive fields and binary conditions regarding a number of different objects, e.g. larger or smaller obstacles. The corresponding actions are: go left/right/forward, push object, pick object up etc. Again, the problem is to learn a switching policy mapping conditions to actions.



\textbf{Setup}
The robot is equipped with a wide range HD camera and uses markers to recognize the objects nearby. The recognized objects are mapped into the robot simulation environment V-REP~\cite{freese2010virtual}. The learning procedure is first tested on the simulation environment and then executed on the real robot.

\textbf{Actions}
Move Forward, Move Left, Move Right, Fetch Object, Slide Object to the Side, Push Object. 

\textbf{Conditions}
Wall on the Left, Wall on the Right, Glass in Front, Glass on the Left, Glass on the Right, Cylinder in Front, Cylinder on the Left, Cylinder on the Right, Ball in Front, Ball on the Left, Ball on the Right, Big Object in Front, Big Object on the Left, Big Object on the Right.



\textbf{Scenarios}
 In the first scenario, the robot has to traverse a corridor dealing with different objects that are encountered on the way.
The destination and the position of the walls is known a priori for simplicity. The other objects are recognized and mapped once they enter the field of view of the camera. 
%If an object gets outside the camera's field of view, its position is estimated using odometry.
The second scenario illustrates the reason why GP-BT performs the learning procedure for each different situation. 
The same type of cylinder is dealt with differently in two different situations.

%The robot has to move forward without colliding with the objects in the room. In the room there are two chairs and two cylinders. The first cylinder is close to the chairs. When the robot encounters the first cylinder, the only action possible is to move the cylinder to the side. When the robot encounters the second cylinder it can either move the cylinder or avoid it. Then, according to the situation, the robot deals with the cylinder in a different way.
In the third scenario, a single action is not sufficient to increase the reward. The robot has to learn an
action composition using GP to reach the goal.

A YouTube video is available that shows all three scenarios in detail\footnote{\url{https://youtu.be/P9JRC9wTmIE} and \url{https://youtu.be/dLVQOlKSqGU}}.



\subsection{Other Approaches using GP applied to BTs}
There exists several other approaches using GP applied to BTs. 

Grammatical Evolution (GE), a grammar-based form of GP, that specifies the syntax of possible solutions using a context-free grammar was used to synthesize a BT~\cite{perez2011evo}. The root node consists of a Fallback node, with a variable number
of subtrees called \emph{BehaviourBlocks}. Each BehaviourBlock consists of a sequence of one or more conditions created through a GE algorithm,
followed by a sequence of actions (possibly with some custom made Decorator) that are also created through a GE algorithm. The root node has as right most child a BehaviourBlocks called \emph{DefaultSequence}: a sequence with memory with only actions, again created through a GE algorithm. The approach works as follows: When the BT is executed, the root node will
route ticks to one BehaviourBlock; if none of those BehaviourBlocks execute actions, the ticks reach the DefaultSequence. The approach was used to compete at the 2010 Mario AI competition, reaching the fourth place of the gameplay track. 

Another similar approach was used to synthesize sensory-motor coordinations of a micro air vehicle using a pure GP algorithm on a initial population of randomly generated BTs~\cite{scheper2014}. The mutation operation is implemented using two methods: \emph{micro mutation} and \emph{macro mutation}. Micro mutation affects only leaf nodes and it is used to modify the parameter of a node. Macro mutation was used to replace a randomly selected node with a randomly selected tree.





\section{Reinforcement Learning applied to BTs}
\label{sec:rl}
In this section we will describe an approach proposed in \cite{pereira2015framework} for combining BTs with Reinforcement Learning (RL).

\subsection{Summary of Q-Learning}
A general class of problems that can be addressed by RL is the
Markov Decision Processes (MDPs). 
Let $s_t \in S$ be the system state at time step $t$, $a_t \in {A}_{s_t}$ the action executed at time $t$, where ${A}_s$ is a finite set of admissible actions for the state $s$ (i.e. the actions that can be performed by the agent while in $s$). The goal of the agent is to learn a policy $\pi : {S} \to {A}$ (i.e. a map from state to  action), where $A = \bigcup_{s \in {S}}{A}_s$, that maximizes the expected long-term reward from each state $s$,~\cite{sutton1999}.
 
One of the most used Reinforcement Learning techniques is \emph{Q-Learning}, which can handle problems with stochastic transitions and rewards. It uses a function called \emph{Q-function} $Q: S \times A \to \mathbb{R}$ to capture the expected future reward of each state-action combination (i.e. how good it is to perform a certain action when the agent is at a certain state). First, for each state-action pair, $Q$ is initialized to an arbitrary fixed value. Then, at each time step of the algorithm, the agent selects an action and observes a reward and a new state. $Q$ is updated according to a \emph{simple value iteration} function~\cite{mitchell1997machine}, using the weighted average of the old value and a value based on  the new information, as follows:


\begin{equation}
  Q_{k+1}(s, a) = (1 - \alpha_k) Q_k(s, a) + \alpha_k \left[ r + \gamma \max_{a' \in A_{s'}} Q_k(s', a') \right],
\end{equation}
where $k$ is the iteration index (increasing every time the agent receives an update), $r$ is the reward, 
$\gamma$ is the discount factor that trades off the influence of early versus late rewards, and $\alpha_k$ is the learning rate that trades off the influence of newly acquired information versus old information.
% (the so-called \emph{exploration versus exploitation}).

The algorithm converges to an optimal policy that maximizes the reward if all admissible state-action pairs are updated infinitely often %and $\alpha_k$, regardless of the policy chosen
\cite{sutton1998,barto2003}. At each state, the optimal policy selects  the action that maximizes the $Q$ value.


%
%\begin{align}
%  V^\pi(s) &= \mathbb{E}\{r_{t+1} + \gamma r_{t+2} + \gamma^2 r_{t+3} + \ldots|\pi, s_t=s \} \label{eq:valuefunction} \\
%           &= \mathbb{E}\{r_{t+1} + \gamma V^\pi(s_{t+1})|\pi, s_t=s\} \notag \\
%           &= \sum_{a \in \mathcal{A}_s} \pi(s, a) \left[ R(s, a) + \gamma\sum_{s'} P(s'|s, a)V^\pi(s') \right], \label{eq:valuebellman}
%\end{align}
%
%where $\pi(s, a)$ is the probability of a policy $\pi$ choosing the action $a$ in the state $s$, and $\gamma \in [0, 1]$ is a discount-rate parameter, it improve the convergence of the algorithm~\cite{sutton1998}. $V^\pi(s)$ is the  the \emph{value function} for $\pi$ at state $s$, it measured how good it is to be in state $s$ under policy $\pi$.
%
%An optimal policy $\pi^*$ is any policy that corresponds to the unique optimal value function $V^*(s)$, defined as:
%
%\begin{align}
%  V^*(s) &= \max_\pi V^\pi(s) \label{eq:optimalvaluefunction} \\
%         &= \max_{a \in \mathcal{A}_s} \mathbb{E}\{r_{t+1} + \gamma V^\pi(s_{t+1})|s_t=s, a_t=a\} \notag \\
%         &= \max_{a \in A_s} \left[ R(s, a) + \gamma \sum_{s'} P(s'|s, a)V^*(s') \right]. \label{eq:optimalvaluebellman}
%\end{align}
%
%The solutions to Equations \ref{eq:valuebellman} and \ref{eq:optimalvaluebellman} are the Equations \ref{eq:valuefunction} and \ref{eq:optimalvaluefunction} \cite{sutton1998} respectively. They cannot be computed without knowing $R(s, a)$ and $P(s'|s, a)$, which are often not available. Alternatively, we can define a \emph{state value function} $Q^\pi(s, a)$ that are computed on states and actions, instate of just states. $Q^\pi(s, a)$ is the value of taking an action $a$ while in a state $s$ under policy $\pi$. $Q^\pi(s, a)$ is computed as follows:
%
%\begin{align}
%  Q^\pi(s, a) &= \mathbb{E}\{r_{t+1} + \gamma r_{t+2} + \gamma^2 r_{t+3} + \ldots|\pi, s_t=s, a_t=a \} \notag \\ %\label{eq:qfunction} \\
%              &= R(s, a) + \gamma \sum_{s'} P(s'|s, a)V^\pi(s') \notag \\
%              &= R(s, a) + \gamma \sum_{s'} P(s'|s, a) \sum_{a'}\pi(s', a')Q^\pi(s', a'), \notag %\label{eq:qbellman}
%\end{align}
%
%and its optimal value $Q^*(s, a)$ as:
%
%\begin{align}
%  Q^*(s, a) &= \max_\pi Q^\pi(s, a) \notag \\
%            &= R(s, a) + \gamma \sum_{s'} P(s'|s, a) \max_{a'} Q^*(s', a'). \notag
%\end{align}
%
%$Q^*(s, a)$ still cannot be computed without knowing $R(s, a)$ and $P(s'|s, a)$. For that reason it is approximated by using its estimated value  $Q_k(s, a)$, computed as:

%\begin{equation}
%  Q_{k+1}(s, a) = (1 - \alpha_k) Q_k(s, a) + \alpha_k \left[ r + \gamma \max_{a' \in A_{s'}} Q_k(s', a') \right],
%\end{equation}
%
%where $\alpha_k$ is a time-varying learning-rate parameter. 




\textbf{Hierarchical Reinforcement Learning}

The vast majority of RL algorithms are based upon the MDP model above. However, Hierarchical RL (HRL) algorithms have their basis in Semi MDPs (SMDPs)~\cite{barto2003}. SMDPs enable a special treatment of the temporal aspects of the problem and, thanks to their hierarchical structure, reduce the impact of the curse of dimensionality by dividing the main problem into several subproblems.

%In a SMDP, the learning agent takes into account the time spent to move from a state to the next one. This considerateness is implemented by rewriting the joint transition probability as $P(s', \tau|s, a)$ and the expected immediate reward function as $R(s, a)$~\cite{barto2003}.
%
%The equation for the optimal value function $V^*(s)$ is rewritten as:

%\begin{equation}
%V^*(s) = \max_{a \in A_s} \left[ R(s, a) + \sum_{s', \tau} \gamma^\tau P(s', \tau|s, a)V^*(s') \right],
%\end{equation}
%
%and the optimal state value function $Q^*(s,a)$ as:
%
%\begin{equation}
%  Q^*(s, a) = R(s, a) + \sum_{s', \tau} \gamma^\tau P(s', \tau|s, a) \max_{a'} Q^*(s', a').
%\end{equation}

Finally, the \emph{option} framework~\cite{sutton1999} is a SMDP-based HRL approach used to  efficiently compute the $Q$-function. In this approach, the options are a generalization of actions, that can call other options upon execution in a hierarchical fashion until a \emph{primitive option} (an action executable in the current state) is found. 

%Let $\mathcal{I} \subseteq S$ be an input set, $\mu : S \times \mathcal{O} \rightarrow [0, 1]$ (with $\mathcal{O} = \bigcup_{s \in S} \mathcal{O}_s$) be a semi-Markov policy, and $\beta : S \rightarrow [0, 1]$ be a termination condition, an is defined as 3-tuple $\langle \mathcal{I}, \mu, \beta \rangle$.
%
%A given option can only be considered for execution if, and only if, the current state $s$ is an element of $\mathcal{I}$. During the execution, the policy chooses the next option $o$ with probability $\mu(s, o)$, the environment changes to state $s'$, where the option terminates with probability $\beta(s')$. 

%\begin{remark}
%$\mu$ is a semi-Markov policy over policies, i.e., it can choose the next option based on the entire history of states, actions, and rewards since the option was initiated \cite{barto2003}.
%\end{remark}
%
%Using this approach, the Q-function for the policy $\mu$ is computed as:

%\begin{equation}
%  Q^\mu(s, o) = \mathbb{E}\{r_{t+1} + \gamma r_{t+2} + \ldots + \gamma^{\tau-1} r_{t+\tau} + \ldots| \mathcal{E}(o\mu, s, t) \},
%\end{equation}
%where $\mathcal{E}(o\mu, s, o)$ is the event of $\mu$ being initiated at time $t$ in $s$, and $o\mu$ is the semi-Markov policy that follows $o$ until it terminates after $\tau$ time steps and then continues according to $\mu$. 
%Hence the optimal function is rewritten as:
%
%\begin{equation}
%  Q^*_\mathcal{O}(s, o) = R(s, o) + \sum_{s'} P(s'|s, o) \max_{o' \in \mathcal{O}_{s'}} Q^*_\mathcal{O}(s', o'), 
%\end{equation}
%
%where:
%
%\begin{equation}
%  P(s'|s, o) = \sum^{\infty}_{\tau=1} p(s', \tau)\gamma^\tau
%\end{equation}
%
%for all $s \in S$, where $p(s', \tau)$ is the probability that $o$ terminates in $s'$ after $\tau$ steps when initiated in state $s$.
%The corresponding Q-learning update becomes:

%\begin{equation}
%  Q_{k+1}(s, o) = (1 - \alpha_k) Q_k(s, o) + \alpha_k \left[ r + \gamma^\tau \max_{o' \in \mathcal{O}_{s'}} Q_k(s', o') \right],
%\end{equation}
%
%where $o\mu$ is the semi-Markov policy that follows $o$ until it terminates after $\tau$ time steps and then continues according to $\mu$. 

% \clearpage
\subsection{The RL-BT Approach}
In this section, we outline the approach proposed in \cite{pereira2015framework}, that we choose to denote RL-BT. 
In order to combine the advantages of BTs and RL,
 the RL-BT starts from a manually designed BT where a subset of nodes are replaced with so-called \emph{Learning Nodes},
 which can be both actions and flow control nodes.
 
 In the \emph{Learning Action node}, the node encapsulates a Q-learning algorithm,
 with  states $s$, actions $A$, and reward $r$ defined by the user. 
 For example, imagine a robot with an Action node that need to ``pick an object''. This learning Action node uses Q-learning to learn how to grasp an object in different positions. Then, the state is defined as the pose of the object with respect to the robot's frame, the actions as the different grasp poses, and the reward function as the grasp quality.
 
In the \emph{Learning Control Flow Node}, the node is a standard Fallback with flexible node ordering, quite similar to the ideas described in Chapter~\ref{ch:extensions}. Here, the designer chooses the representation for the state $s$, while the actions in $A$ are the children of the Fallback node, which can be learning nodes or regular nodes. Hence, given a state $s$, the Learning Control Flow Node selects the order of its own children based on the reward. The reward function can be task-depended (i.e. the user finds a measure related to the specific task) or return value-dependent (i.e., it gives a positive reward for Success and negative reward for Failure). For example, consider a NPC that has three main goals: find resources, hide from stronger players, and attack weaker ones. 
The Learning Control Flow node has 3 subtrees, one for each goal. Hence the state $s$ can be a vector collecting information about players' position, weapons position, health level, etc. The reward function can be a combination of health level, score, etc. 

In \cite{pereira2015framework},
a formal definition of the learning node is made, with analogies to the Options approach for HRL to compute the $Q$-function,
and connections between BTs and the recursive nature of the approach.




\subsection{Experimental Results}
\label{sec:experiments}

In this section, we briefly describe the results of two experiments from \cite{pereira2015framework}. Both experiments execute the approach in 30 different episodes, each with 400 iterations. At the beginning of each experiment, the initial state is reset. 
The environment is composed of a set of rooms. In each room, the agent can perform $3$ actions: \emph{save victim}, \emph{use extinguisher X}, and \emph{change room}. Each room has a probability of $0.5$ to have a victim; if there is a victim, the agent must save it first. Moreover, each room has a probability of $0.5$ to have one of $3$ types of fire ($1, 2$ and $3$, with probability $1/3$ each); if there is a fire, the agent must extinguish it before leaving the room. The agent is equipped with $3$ extinguishers (types $A, B$ and $C$), and each extinguisher can successfully extinguish only one type of fire, randomly chosen at the beginning of the episode and unknown to the agent.


\textbf{Scenario 1}
\label{sec:experiments_scenario1}

\begin{figure}
  \centering
  \includegraphics[width=0.85\textwidth]{RLBTEx1}
  \caption{The BT model for the first agent. Only a single Action node (Use Extinguisher) has learning capabilities.}
  \label{fig:model_scenario1}
\end{figure}

In this scenario, we evaluate  the capability of  \emph{Use Extinguisher X} to learn the correct extinguisher to use for a specific type of fire.

Figure~\ref{fig:model_scenario1} depicts the BT modeling the behavior of the learning agent. In the learning Action node \emph{Use Extinguisher X} the state is defined as  $s = \langle \text{\emph{fire type}} \rangle$, where $\text{\emph{fire type}} = \{1, 2, 3\}$, and the available actions are as follows:\\ $A = \{\mbox{Use Extinguisher A}, \mbox{Use Extinguisher B}, \mbox{Use Extinguisher B}\}$. The reward is defined as  $10$ if the extinguisher can put out the fire and $-10$ otherwise.
%
%Table \ref{tab:results_interference} shows the ratio between the correct activations of the three main behaviors over the total expected activations (i.e., the accuracy for behavior usage). All behaviors are called correctly 100\% of the time, this is due to the tree dynamics that allow the expert to model a strict sequence of behaviors. Notice that, this table also shows that the learning node (Use Extinguisher) does not affect the execution of other behaviors in the tree. 
The results in \cite{pereira2015framework} show that the accuracy converges to 100\%.
%\todopetter{we cannot include plots from other papers}

%Figure~\ref{fig:results_interference} shows the convergence of the node's accuracy during the experiment, compared with the random baseline.  BT accuracy converges to 100\%, as the expert modeled the correct actions sequence.
%
%\begin{figure}
%  \centering
%  \includegraphics[width=0.85\textwidth]{results_interference.png}
%  \caption{Accuracy of the ``Use Extinguisher'' node (a learning action node) compared with the random baseline \cite{pereira2015framework}.}
%  \label{fig:results_interference}
%\end{figure}
%\begin{table}
%  \centering
%  \caption{Accuracy of behavior usage.}
%  \label{tab:results_interference}
%  \begin{tabular}{ l c }
%    \textbf{Behavior} & \textbf{Accuracy} \\
%    \hline 
%    Save Victim       & 1.0 \\
%    Use Extinguisher  & 1.0 \\
%    Change Room       & 1.0 \\
%  \end{tabular}
%\end{table}



\textbf{Scenario 2}

This scenario is a more complex version of the one above, where we consider the time spent to execute an action.

The actions \emph{Save Victim} and \emph{Use Extinguisher X} now take time to complete, depending on the fire intensity. Any given fire has an intensity $\text{\emph{fire intensity}} \in \{1, 2, 3\}$, chosen randomly for each room. The fire intensity specifies the time steps needed to extinguish a fire. The fire is extinguished when its intensity is reduced to $0$.

The fire intensity is reduced by $1$ each time the agent uses the correct extinguisher. The action \emph{change room} is still executed instantly and the use of the wrong extinguisher makes the agent lose the room. 

\begin{figure}
  \centering
  \includegraphics[width=0.85\textwidth]{RLBTEx2}
  \caption{The BT model for the second agent using two nested Learning Nodes.}
  \label{fig:model_scenario2}
\end{figure}

Figure~\ref{fig:model_scenario2} shows the BT modeling the learning agent. 
It uses $2$ learning nodes. The first, similar to the one used in Scenario~1, is a learning Action node using. In that node, the state is defined as $s = \langle \text{\emph{fire type}} \rangle$, where $\text{\emph{fire type}} = \{1, 2, 3\}$,  the action set as $a = \{A, B, C\}$, and the reward as $\frac{10}{\text{\emph{fire intensity}}}$ if the extinguisher can extinguish the fire and $-10$ otherwise.

%\begin{table}
%  \centering
%  \caption{Accuracy of behavior usage.}
%  \label{tab:results_nested}
%  \begin{tabular}{ l c }
%    \textbf{Behavior} & \textbf{Accuracy} \\
%    \hline 
%    Save Victim       & 0.974 \\
%    Use Extinguisher  & 0.991 \\
%    Change Room       & 0.991 \\
%  \end{tabular}
%\end{table}

The second learning node is a learning control flow node. It learns the behavior that must be executed given the state $s = \langle \text{\emph{has victim?}}, \text{\emph{has fire?}}\rangle$. The node's children are the actions $A = \{\text{\emph{save victim}}, \text{\emph{use extinguisher X}}, \text{\emph{change room}}\}$. 
This learning node receives a cumulative reward, $-10$ if the node tries to save and there is no victim, $-1$ while saving the victim, and $+10$ when the victim is saved; $-10$ if trying to extinguish a non-existing fire, $-1$ while extinguishing it, and $+10$ if the fire is extinguished;  $+10$ when the agent leaves the room at the right moment and $-10$ otherwise.

%\begin{figure}
%  \centering
%  \includegraphics[width=0.85\textwidth]{results_nested.png}
%  \caption{Accuracy of the composite and action Learning Nodes.}
%  \label{fig:results_nested}
%\end{figure}


The results in \cite{pereira2015framework} show that the accuracy converges to 97-99\%.
The deviation from 100\% is due to the fact that the
learning control flow node needs some steps of trial-and-error to learn the most effective action order.


%\todopetter{we cannot include plots from other papers}
%
%Figure \ref{fig:results_nested} shows the convergence the nodes' accuracy during the episodes compared with the random baseline.  In this scenario, the nodes have an accuracy 97\% to 99\% of the time since the learning control flow node needs some steps of trial-and-error to learn the most effective action order.
%



\section{Comparison between GP-BT and RL-BT}
How do we choose between GP-BT and RL-BT for a given learning problem?

GP-BT and RL-BT operate on the same basic premise: they both try something, receive a reward depending on ``how good" the result is, and then try and do something different to get a higher reward. 

The key difference is that GP-BT operates on the BT itself, creating the BT from scratch and extending it when needed.
RL-BT on the other hand starts with a fixed BT structure that is not changed by the learning. Instead, the behaviors of 
a set of designated nodes are improved. If RL-BT was started from scratch with a BT consisting of a single action, there
 would be no difference between RL-BT and standard RL. Instead, the point of combining BTs with RL is to 
 address the curse of dimensionality, and do RL on smaller state spaces, while the BT connects these subproblems in a modular way,
 with the benefits described in this book.
 RL-BT thus needs user input regarding both the BT structure, and the actions, states and local reward functions to be considered in the subproblems addressed by RL. GP-BT on the other hand only needs user input regarding the single global reward function, and the conditions (sensing) and actions available to the agent.

%
%The key difference lies in how they try to improve. The RL algorithms use  a mathematically-defined frameworks whereas GP randomly mutates and selects a population of BTs.
%
%Another difference is that GP-BT starts from the very beginning of the agent intelligence, it does assume any knowledge of the effect of the action, which could be excessive. RL-BT starts from a pre-defined BTs and learns the correct action refinement and subtree's order.   

%So, the real question is whether we should choose a RL or a GP algorithm for a given problem considering their strengths and weaknesses, as described in~\cite{russell1995modern}.


\section{Learning from Demonstration applied to BTs}
\label{sec.dem}
Programming by demonstration  has a straightforward application in 
both robotics and 
the development of the AI for NPCs in games.

In robotics, the Intera5 software for the Baxter and Sawyer robots provides learning by demonstration support\footnote{\url{http://mfg.rethinkrobotics.com/intera/Building_a_Behavior_Tree_Using_the_Robot_Screen}}.

In computer games, one can imagine a game designer controlling the NPC during a training session in the game,  demonstrating the expected behavior for that character in different situations.
One such approach called \emph{Trained BTs (TBTs)} was  proposed in~\cite{olivenza2017trained}.

TBT applies the following approach:  
it first records traces of actions executed by the designer controlling the NPC to be programmed, then it processes the traces to generate a BT that can 
control the same NPC in the  game. The resulting BT can then be further edited with a  visual editor. 
The process starts with the creation of a minimal BT
using the provided BT editor. This initial BT contains a
special node called a Trainer Node (TN). Data for this node are
collected  in a game session where the designer simulates the intended behavior of the NPC. 
%The knowledge
%acquired in this training session is later used at run-time
%when the player executes the game. 
Then, the trainer node is
replaced by a machine-generated sub-behavior that, when
reached, selects the task that best fits the actual state of
the game and the traces stored  during the training session. 
The approach combines the advantages of programming
by demonstration with the ability to fine-tune the learned
BT.

Unfortunately, using learning from demonstration approaches the learned BT easily becomes  very large, as each trace is directly mapped into a new sub-BT. Recent approaches address this problem by finding common patterns in the BT and generalizing them~\cite{robertson2015building}.
First, it creates a maximally specific BT from the given traces. Then
iteratively reduces the BT in size by finding and combining common
patterns  of  actions.  The  process  stops when the BT has no common patters.    
Reducing  the  size  of  the  BT also improves readability.

%
%
%\section{Conclusions}
%
%
%In this chapter 
%
%
%
%In this paper we presented a model-free AP for  autonomous agents using a optimization approach involving a combination of GP and greedy-based algorithms to generate BTs. The GP component is invoked only when a single action improving the reward cannot be found, and conditions are combined using the And-Or-Tree analogy.
%The resulting algorithm is safe, convergent, provides human readable output and solutions to fairly complex dynamic problems.
%The approach was illustrated and validated using the Mario AI software, which allowed us to  compare the approach to the state-of-the-art.
%In comparison to a pure GP approach and a FSM-based approach,
%the proposed approach generated solutions with 
%significantly fewer nodes than the FSM approach, and
%better performance than both. However, for those applications that require the GP part to be invoked, the approach requires good computational power to have a result in reasonable time.
%
%
%Two applications were described, a Youbot and the Mario AI software.
%the proposed approach was tested in two different benchmarks highlighting the applicability of the approach on complex environment and on real robots.
%
%
%
%
%













\endinput