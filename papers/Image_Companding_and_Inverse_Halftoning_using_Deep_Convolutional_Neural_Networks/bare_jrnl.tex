
%% bare_jrnl.tex
%% V1.4b
%% 2015/08/26
%% by Michael Shell
%% see http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.8b or later) with an IEEE
%% journal paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/pkg/ieeetran
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE!
%% User assumes all risk.
%% In no event shall the IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%*************************************************************************


% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. The IEEE's font choices and paper sizes can   ***
% *** trigger bugs that do not appear when using other class files.       ***                          ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/



\documentclass[journal]{IEEEtran}
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[journal]{../sty/IEEEtran}





% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/pkg/ifpdf
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.


% *** CITATION PACKAGES ***
%
%\usepackage{cite}
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of the IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off
% such as if a citation ever needs to be enclosed in parenthesis.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 5.0 (2009-03-20) and later if using hyperref.sty.
% The latest version can be obtained at:
% http://www.ctan.org/pkg/cite
% The documentation is contained in the cite.sty file itself.




% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  % \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation
% can be obtained at:
% http://www.ctan.org/pkg/graphicx
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found at:
% http://www.ctan.org/pkg/epslatex
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). The IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex



\usepackage[pdftex]{graphicx}
\usepackage{multirow}

% *** MATH PACKAGES ***
%
%\usepackage{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics.
%
% Note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/amsmath



% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as the IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/pkg/algorithms
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/pkg/algorithmicx




% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/array


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.




% *** SUBFIGURE PACKAGES ***
%\ifCLASSOPTIONcompsoc
%  \usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
%\else
%  \usepackage[caption=false,font=footnotesize]{subfig}
%\fi
% subfig.sty, written by Steven Douglas Cochran, is the modern replacement
% for subfigure.sty, the latter of which is no longer maintained and is
% incompatible with some LaTeX packages including fixltx2e. However,
% subfig.sty requires and automatically loads Axel Sommerfeldt's caption.sty
% which will override IEEEtran.cls' handling of captions and this will result
% in non-IEEE style figure/table captions. To prevent this problem, be sure
% and invoke subfig.sty's "caption=false" package option (available since
% subfig.sty version 1.3, 2005/06/28) as this is will preserve IEEEtran.cls
% handling of captions.
% Note that the Computer Society format requires a larger sans serif font
% than the serif footnote size font used in traditional IEEE formatting
% and thus the need to invoke different subfig.sty package options depending
% on whether compsoc mode has been enabled.
%
% The latest version and documentation of subfig.sty can be obtained at:
% http://www.ctan.org/pkg/subfig




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure.
% Be aware that LaTeX2e kernels dated 2015 and later have fixltx2e.sty's
% corrections already built into the system in which case a warning will
% be issued if an attempt is made to load fixltx2e.sty as it is no longer
% needed.
% The latest version and documentation can be found at:
% http://www.ctan.org/pkg/fixltx2e


%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/stfloats
% Do not use the stfloats baselinefloat ability as the IEEE does not allow
% \baselineskip to stretch. Authors submitting work to the IEEE should note
% that the IEEE rarely uses double column equations and that authors should try
% to avoid such use. Do not be tempted to use the cuted.sty or midfloat.sty
% packages (also by Sigitas Tolusis) as the IEEE does not format its papers in
% such ways.
% Do not attempt to use stfloats with fixltx2e as they are incompatible.
% Instead, use Morten Hogholm'a dblfloatfix which combines the features
% of both fixltx2e and stfloats:
%
% \usepackage{dblfloatfix}
% The latest version can be found at:
% http://www.ctan.org/pkg/dblfloatfix




%\ifCLASSOPTIONcaptionsoff
%  \usepackage[nomarkers]{endfloat}
% \let\MYoriglatexcaption\caption
% \renewcommand{\caption}[2][\relax]{\MYoriglatexcaption[#2]{#2}}
%\fi
% endfloat.sty was written by James Darrell McCauley, Jeff Goldberg and
% Axel Sommerfeldt. This package may be useful when used in conjunction with
% IEEEtran.cls'  captionsoff option. Some IEEE journals/societies require that
% submissions have lists of figures/tables at the end of the paper and that
% figures/tables without any captions are placed on a page by themselves at
% the end of the document. If needed, the draftcls IEEEtran class option or
% \CLASSINPUTbaselinestretch interface can be used to increase the line
% spacing as well. Be sure and use the nomarkers option of endfloat to
% prevent endfloat from "marking" where the figures would have been placed
% in the text. The two hack lines of code above are a slight modification of
% that suggested by in the endfloat docs (section 8.4.1) to ensure that
% the full captions always appear in the list of figures/tables - even if
% the user used the short optional argument of \caption[]{}.
% IEEE papers do not typically make use of \caption[]'s optional argument,
% so this should not be an issue. A similar trick can be used to disable
% captions of packages such as subfig.sty that lack options to turn off
% the subcaptions:
% For subfig.sty:
% \let\MYorigsubfloat\subfloat
% \renewcommand{\subfloat}[2][\relax]{\MYorigsubfloat[]{#2}}
% However, the above trick will not work if both optional arguments of
% the \subfloat command are used. Furthermore, there needs to be a
% description of each subfigure *somewhere* and endfloat does not add
% subfigure captions to its list of figures. Thus, the best approach is to
% avoid the use of subfigure captions (many IEEE journals avoid them anyway)
% and instead reference/explain all the subfigures within the main caption.
% The latest version of endfloat.sty and its documentation can obtained at:
% http://www.ctan.org/pkg/endfloat
%
% The IEEEtran \ifCLASSOPTIONcaptionsoff conditional can also be used
% later in the document, say, to conditionally put the References on a
% page by themselves.




% *** PDF, URL AND HYPERLINK PACKAGES ***
%
%\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/url
% Basically, \url{my_url_here}.




% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )

\begin{document}
%
% paper title
% Titles are generally capitalized except for words such as a, an, and, as,
% at, but, by, for, in, nor, of, on, or, the, to and up, which are usually
% not capitalized unless they are the first or last word of the title.
% Linebreaks \\ can be used within to get better formatting as desired.
% Do not put math or special symbols in the title.
\title{Image Companding and Inverse Halftoning using Deep Convolutional Neural Networks}
%
%
% author names and IEEE memberships
% note positions of commas and nonbreaking spaces ( ~ ) LaTeX will not break
% a structure at a ~ so this keeps an author's name from being broken across
% two lines.
% use \thanks{} to gain access to the first footnote area
% a separate \thanks must be used for each paragraph as LaTeX2e's \thanks
% was not built to handle multiple paragraphs
%

%\author{Michael~Shell,~\IEEEmembership{Member,~IEEE,}
%        John~Doe,~\IEEEmembership{Fellow,~OSA,}
%        and~Jane~Doe,~\IEEEmembership{Life~Fellow,~IEEE}% <-this % stops a space
%
\author{Xianxu~Hou and Guoping~Qiu

\thanks{X. Hou is with the School of Computer Science, University of Nottingham Ningbo China and a visiting student in the College of Information Engineering, Shenzhen University, China. email: xianxu.hou@nottingham.edu.cn}
% <-this % stops a space
\thanks{G. Qiu is with the College of Information Engineering, Shenzhen University, China and with the School of Computer Science, the University of Nottingham, United Kingdom. email: qiu@szu.edu.cn and guoping.qiu@nottingham.ac.uk}}% <-this % stops a space
% \thanks{Manuscript received April 19, 2005; revised August 26, 2015.}}

% note the % following the last \IEEEmembership and also \thanks -
% these prevent an unwanted space from occurring between the last author name
% and the end of the author line. i.e., if you had this:
%
% \author{....lastname \thanks{...} \thanks{...} }
%                     ^------------^------------^----Do not want these spaces!
%
% a space would be appended to the last name and could cause every name on that
% line to be shifted left slightly. This is one of those "LaTeX things". For
% instance, "\textbf{A} \textbf{B}" will typeset as "A B" not "AB". To get
% "AB" then you have to do: "\textbf{A}\textbf{B}"
% \thanks is no different in this regard, so shield the last } of each \thanks
% that ends a line with a % and do not let a space in before the next \thanks.
% Spaces after \IEEEmembership other than the last one are OK (and needed) as
% you are supposed to have spaces between the names. For what it is worth,
% this is a minor point as most people would not even notice if the said evil
% space somehow managed to creep in.



% The paper headers
% \markboth{Journal of \LaTeX\ Class Files,~Vol.~14, No.~8, August~2015}%
% {Hou \MakeLowercase{\textit{et al.}}: Bare Demo of IEEEtran.cls for IEEE Journals}
% The only time the second header will appear is for the odd numbered pages
% after the title page when using the twoside option.
%
% *** Note that you probably will NOT want to include the author's ***
% *** name in the headers of peer review papers.                   ***
% You can use \ifCLASSOPTIONpeerreview for conditional compilation here if
% you desire.




% If you want to put a publisher's ID mark on the page you can do it like
% this:
%\IEEEpubid{0000--0000/00\$00.00~\copyright~2015 IEEE}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.



% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}




% make the title area
\maketitle

% As a general rule, do not put math, special symbols or citations
% in the abstract or keywords.
\begin{abstract}
This paper presents a deep learning technology for tackling two traditional low-level image processing problems, companding and inverse halftoning. This paper makes two main contributions. First, to the best knowledge of the authors, this is the first work that has successfully developed deep learning based solutions to these two traditional low-level image processing problems. As well as introducing new methods to solving well-known image processing problems, this paper also contributes to a growing literature that demonstrates the power of deep learning in solving traditional signal processing problems. Second, building on insights into the properties of visual quality of images and the internal representation properties of a deep convolutional neural network (CNN) and inspired by recent success of deep learning in other image processing applications, this paper has developed an effective deep learning method that trains a deep CNN as a nonlinear transformation function to map a lower bit depth image to higher bit depth or from a halftone image to a continuous tone image, and at the same time employs another pretrained deep CNN as a feature extractor to derive visually important features to construct the objective function for the training of the transformation CNN. Extensive experimental results are presented to show that the new deep learning based solution significantly outperforms previous methods and achieves new state-of-the-art results.



%We present a learning-based method for image companding and inverse halftoning to improve the image quality when converting images from lower bit to higher bit. These lower bit images could suffer severe degradation for different image manipulation such as scaling, sharpening and rotation. Our proposed method uses deep convolutional neural networks (CNNs) to learn end-to-end non-linear mappings to achieve image expanding and inverse halftoning. Instead of using per-pixel loss, we employ a pre-trained deep convolutional network and use its hidden representations to construct perceptual loss for the training. We show that the same approach can be successfully applied for both companding and inverse halftoning, which traditionally require very different methods with human-engineered design. We demonstrate that our deep models can effectively achieve state-of-the-art results for low-level vision problems and produce high-quality images.
\end{abstract}

% Note that keywords are not normally used for peerreview papers.
\begin{IEEEkeywords}
Image Companding, Inverse Halftoning, CNNs, Perceptual Loss.
\end{IEEEkeywords}



% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle


\begin{figure*}
\centering
  \includegraphics[width=\textwidth]{image/overview.pdf}
  \caption{Method overview. A transformation convolutional neural network (CNN) to expand lower-bit images to higher-bit ones. A pretrained deep CNN for constructing perceptual loss to train the transformation network.}
  \label{fig:overview}
\end{figure*}


\section{Introduction}
\IEEEPARstart{C}{ompanding} is a process of compression and then expanding, allowing signals with a higher dynamic range to be transmitted with a lower dynamic range by reducing the number of bits. This technique is widely used in telecommunication and signal processing such as audio processing. For image processing, companding could be regarded as an encoding and decoding framework. The encoding or quantized compression process, while fairly simple and efficient, could also produce a lot of undesirable artifacts, such as blocking artifacts, contouring and ringing effects (see Fig. \ref{fig:overview}). These degraded artifacts become more obvious with lower bit quantization.

Inverse halftoning, another similar image processing problem considered in this paper, is the inversed process for halftoning. Halftone images are binary images served as analog representation and widely used in digital image printing, trying to convey the illusion of having a higher number of bit levels (continuous-tone) to maintain the overall structure of the original images. As a result, distortions will be introduced to the halftone images due to a considerable amount of information being discarded. Inverse halftoning, on the other hand, addresses the problem of recovering a continuous-tone image from the corresponding halftoned version. This inversed process is needed since typical image processing techniques such as compression and scaling can be successfully applied to continuous-tone images but very difficult to halftone images.

However, these two problems are ill-posed considering that there could be an infinite number of possible solutions. They are essentially one-to-many mappings and the input image could be transformed into an arbitrary number of plausible outputs even if the compression and halftone methods are known in advance. Solving both problems requires to find a way to estimate and add more information into the images that do not exist. There are no well-defined mathematic functions or guidelines to describe the mappings to produce high-quality images. 

In this paper, we take advantage of the recent development in machine learning, in particular deep convolutional neural networks (CNNs), which have become the state-of-the-art workforce for most computer vision tasks \cite{krizhevsky2012imagenet,simonyan2014very}. Unlike previous human-engineered methods \cite{li2005compressing,kite2000fast,shen2001robust,easley2009inverse}, we formulate the two image processing problems, i.e., companding and inverse halftoning, from the perspective of machine learning. We train deep convolutional neural networks as non-linear mapping functions in a supervised manner to expand images from a lower bit depth to a higher bit depth to reduce artifacts in image companding and to produce continuous-tone images in inverse halftoning. Moreover, we also investigate the effect to construct loss functions based on different level convolutional layers, which have shown different properties when applying an inverting processing to reconstruct the encoded images \cite{mahendran2015understanding}.

Our core contributions in this work are two folds. Firstly, to the best knowledge of the authors, this is the first work that has successfully developed deep learning based solutions to these two traditional image processing problems. This not only introduces new methods to tackle well-known image processing problems but also contributes to the literature that demonstrates the power of deep learning in solving traditional signal processing problems. Secondly, building on insights into the properties of visual quality of images and the hidden representation properties of deep CNNs, and also inspired by recent works that use deep CNNs in other image processing applications \cite{gatys2015neural,johnson2016perceptual,hou2017deep}, we take full advantage of the convolutional neural networks both in the nonlinear mapping functions and in the neural networks loss functions for low-level image processing problems. We not only use a deep CNN as a nonlinear transformation function to map a low bit depth image to a higher bit depth image or from a halftone image to a continuous tone image, but also employ another pre-trained deep CNN as a feature extractor or convolutional spatial filter to derive visually important features to construct the objective function for the training of the transformation neural network. Through these two low-level image processing case studies, we demonstrate that a properly trained deep CNN can capture the spatial correlations of pixels in a local region and other visually important information, which can be used to help a deep CNN to infer  the ``correct" values of pixels and their neighbors. Our work further demonstrates that halftone images and heavily compressed low bit depth images, even though showing visually annoying artifacts, they have preserved the overall structures of the images which are sufficient to enable deep neural networks to recover the original signals to a high degree of fidelity. %We find that the ability of CNN to preserve the spatial relationship between pixels by learning internal feature representations using only a local region of the input data is quite useful to reason the relevant spatial correlation and semantic between different pixels, thus infer the ``correct" values for a single pixel based on its neighbors.


%In order to train the deep models, a loss function should be given to tell what is needed to minimize for the deep CNNs. One approach is to use per-pixel loss to measure the difference between the output images and the ground-truth \cite{dong2016image,cheng2015deep}. However per-pixel measurement is problematic that it cannot capture the perceptual difference between the output images and the ground-truth, usually yielding blurred results. For instance, the same image shifted by a few pixels have little perceptual difference for humans, however it could mathematically have a very high per-pixel loss. Instead recent works \cite{hou2017deep,johnson2016perceptual,gatys2015neural,simonyan2013deep} try to replace it with a perceptual loss based on features extracted from pretrained deep convolutional neural networks and successfully generating high-quality images. We take advantage of these new outcomes and guide our deep CNN models' training by perceptual loss function depended on the convolutional layers (features) from pretrained CNNs. Our core insight is trying to keep output images and the ground-truth consistent in the feature level, which can ensure the spatial correlation and contrast of the outputs consistent with that of the ground-truth. Thus, the trained high-capacity convolutional neural networks could learn to reason the relevant spatial correlation and semantic between different pixels, thus infer the ``correct" values for a single pixel based on its neighbors.


% Recent years the community has taken a significant step in this direction with powerful presentation ability of deep convolutional neural networks (CNN), which has become a state-of-the-art method for most computer vision problems.



\section{Related Works}

\subsection{Image Companding}
Companding, a combination of the words \textbf{com}pressing and ex\textbf{panding}, is a signal processing technique to allow signals with a large dynamic range transmitted in a smaller dynamic range format. This technique is widely used in digital telephony systems. Image companding \cite{yang2004integer,bhooshan20102d,li2005compressing} is designed to squeeze higher-bit images to lower bit ones, based on which to reproduce outputs with higher bits. Multi-scale subband architecture \cite{li2005compressing} successfully compressed high dynamic range (HDR) images to displayable low dynamic range (LDR) ones. They also demonstrated that the compression process can be inverted by following the similar scheme as the previous compression. As a result, low dynamic range images can be expanded to approximate the original higher-bit ones with minimal degradation.


\subsection{Halftoning and Inverse Halftoning}
The typical digital halftoning process is considered as a technique of converting a continuous-tone grayscale image with 255 color levels (8 bits) into a binary black-and-white image with only 0 and 1 two color levels (1 bit). These binary images could be reproduced to ``continuous-tone" images for humans based on an optical illusion that tiny dots are blended into smooth tones by human eyes at a macroscopic level. In this work, we focus on the most popular halftoning technique known as error diffusion, in which the residual quantization error of a pixel is distributed to neighboring pixels. Floydâ€“Steinberg dithering is commonly used by image manipulation software to achieve error diffused halftoning based on a simple kernel. The reversed processing known as inverse halftoning is to reconstruct the continuous-tone images from halftones. Many approaches to addressing this problem have been proposed in the literature, including non-linear filtering \cite{shen2001robust}, vector quantization \cite{ting1994error}, projection onto convex sets \cite{unal2001restoration}, MAP projection \cite{stevenson1997inverse}, wavelets-based \cite{neelamani2002winhd}, anisotropic diffusion \cite{kite2000fast}, Bayesian-based \cite{liu2011inverse}, a method by combining low-pass filtering and super-resolution \cite{minami2012inverse}, Look-up table \cite{mese2001look}, sparse representation \cite{son2012inverse}, local learned dictionaries \cite{son2014local} and coupled dictionary training \cite{freitas2016enhancing}.

\subsection{Deep Learning for Image Transformation}
In this work, we seek to formulate the image companding and inverse halftoning as image transformation problems and employ deep convolutional neural networks as non-linear functions to map input images to output images for different purposes. Recent deep CNNs have become a common workhorse behind a wide variety of image transformation problems. These problems can be formulated as per-pixel classification or regression by defining low level loss. Semantic segmentation methods \cite{long2015fully,eigen2015predicting,noh2015learning} use fully convolutional neural networks trained by per-pixel classification loss to predict dense scene labels. End-to-end automatic image colorization techniques \cite{iizuka2016let,larsson2016learning} try to colorize grayscale image based on low level losses. Other works for depth \cite{eigen2014depth,liu2015deep} and edge detection \cite{xie2015holistically} are also similar to transform input images to meaningful output images through deep convolutional neural networks, which are trained with per-pixel classification or regression loss. However the per-pixel measurement essentially treats the output images as ``unstructured" in a sense that each pixel is independent with all other pixels for a given image.

\begin{figure*}
\centering
  \includegraphics[width=\textwidth]{image/architecture.pdf}
  \caption{The architecture of the transformation networks. The ``U-Net" network is an encoder-decoder with skip connections between the encoder and decoder. The dash-line arrows indicate the features from the encoding layers are directly copied to the decoding layers and form half of the corresponding layers' features.}
  \label{fig:architecture}
\end{figure*}

Considering the shortcoming of per-pixel loss, other ``structured" measurements have been proposed such as structural similarity index measure (SSIM) \cite{wang2004image} and conditional random fields (CRF) \cite{chen2014semantic}, which take context into account. These kinds of ``structured" loss have been successfully applied to different image transformation problems. However these measurements are human-crafted, the community has successfully developed structure loss directly learned from images. Generative adversarial networks (GANs) \cite{goodfellow2014generative} are able to generate high-quality images based on adversarial training. Many works have tried to apply GANs in conditional settings such as discrete labels \cite{mirza2014conditional}, texts \cite{reed2016generative} and of course images. Image-conditioned GANs involve style transfer \cite{li2016precomputed}, inpainting \cite{pathak2016context}, frame prediction \cite{mathieu2015deep}. In addition, image-to-image translation framework \cite{isola2016image} based on adversarial loss can effectively synthesize photos under different circumstances.

Another way to improve per-pixel loss is to generate images by optimizing a perceptual loss which is based on high level features extracted from pretrained deep convolutional neural networks. By optimizing individual deep features \cite{yosinski2015understanding} and maximizing classification score \cite{simonyan2013deep}, images can be generated for a better understanding of hidden representations of trained CNNs. By inverting convolutional features \cite{dosovitskiy2016inverting}, the colors and the rough contours of an image can be reconstructed from activations in pretrained CNNs. In addition, artistic style transfer \cite{gatys2015neural} can be achieved by jointly optimizing the content and style reconstruction loss based on deep features extracted from pretrained CNNs. A similar method is also used for texture synthesis \cite{gatys2015texture}. Similar strategies are also explored to achieve real-time style transfer and super-resolution \cite{johnson2016perceptual}. Deep feature consistent variational autoencoder \cite{hou2017deep} is proposed to generate sharp face images and manipulate facial attributes by minimizing the difference between the deep features of the output images and target images.


\section{Method}
In this work, we propose to use deep convolutional neural networks with skip connections as non-linear mapping functions to expand images from a lower bit depth to a higher bit depth. The objective of generating the higher bit depth version of the image is to ensure that this image is visually pleasing and to capture the essential and visual important properties of the original version of the image. Instead of using per-pixel losses, i.e. measuring pixel-wise difference between the output image and its target (the original) image, we measure the difference between the output image and target image based on the high level features extracted from pretrained deep convolutional neural networks. The key insight is that the pretrained networks have already encoded perceptually useful information we desired, such as the spatial relationship between pixels nearby. Our system is diagrammatically illustrated in Fig. \ref{fig:overview}, which consists of two parts: an autoencoder transformation neural network $T(x)$ to achieve end-to-end mapping from an input image to an output image, and a pretrained neural network $\Phi(x)$ to define the loss function.



\subsection{Network Architecture}
Our non-linear mappings are deep convolutional neural networks, which have been demonstrated to have state-of-the-art performances in many computer vision tasks. Successful network architecture like AlexNet \cite{krizhevsky2012imagenet}, VGGNet \cite{simonyan2014very} and ResNet \cite{he2016deep} are designed for high level tasks like image classification to output a single label, and they cannot be directly applied to image processing problems. Instead, previous works have employed an encoder-decoder architecture \cite{hou2017deep,johnson2016perceptual} to firstly encode the input images through several convolutional layers until a bottleneck layer, followed by a reversed decoding process to produce the output images. Such encoder-decoder architecture forces all the information to pass through the networks layer by layer. Thus the final generated images are produced by higher layers' features. However for image processing, the output images can retain a great deal of lower layers' information of the input images, and it would be better to incorporate lower layers features in the decoding process. Based on the architecture guidelines of previous work on image segmentation \cite{ronneberger2015u}, image-to-image translation \cite{isola2016image} and DCGAN \cite{radford2015unsupervised}, we add skip connections to construct a ``U-Net" network to fuse lower layers and higher layers features and employ fully convolutions for image transformation.

The details of our model are shown in Fig. \ref{fig:architecture}, we first encode the input image to lower dimension vector by a series of stride convolutions, which consists of 4 x 4 convolution kernels and 2 x 2 stride in order to achieve its own downsampling. We also use a similar approach for decoding to allow the network to learn its own upsampling by using deconvolutions \cite{long2015fully}. Spatial batch normalization \cite{ioffe2015batch} is added to stabilize the deep network training after each convolutional layer except the input layer of the encoder and the last output layer of decoder as suggested in \cite{radford2015unsupervised}. Additionally leaky rectified activation (LeakyReLU) and ReLU are served as non-linear activation functions for encoder and decoder respectively. Finally we directly concatenate all the encoding activations to the corresponding decoding layers to construct a symmetric ``U-Net" structure \cite{ronneberger2015u} to fuse the features from both low layers and high layers.

\begin{figure*}
\centering
  \includegraphics[width=\textwidth]{image/depths_companding1.pdf}
  \caption{Results on color images from Microsoft COCO validation split for blocking and contour artifacts reduction. A pair of compressed 2 bit images and the corresponding expanded ones are shown together. Additionally an enlarged sub-image of each image is given at the bottom for better comparison.}
  \label{fig:depths_companding1}
\end{figure*}

\begin{figure*}
\centering
  \includegraphics[width=\textwidth]{image/depths_companding2.pdf}
  \caption{Results on grayscale images from Microsoft COCO validation split for blocking and contour artifacts reduction. A pair of compressed 2 bit and 4 bit images and the corresponding expanded ones are shown together. Additionally an enlarged sub-image of each image is given at the bottom for better comparison.}
  \label{fig:depths_companding2}
\end{figure*}

\begin{figure}[!tb]
  \centering
  \includegraphics[width=9cm]{image/hist_companding.pdf}
  \caption{Intensity histogram of different compressed and expanded images.}
  \label{fig:hist_companding}
\end{figure}

\begin{figure*}[!tb]
  \centering
  \includegraphics[width=17cm]{image/depth3_companding.pdf}
  \caption{Results on color images for blocking and contour artifacts reduction. The compressed images are fixed to 3 bits with 8 color levels for each channel. The conv1\_1, conv3\_1, conv5\_1 are the expanded results produced by the models trained with perceptual loss constructed by corresponding convolutional layers. Additionally an enlarged sub-image of each image is given at the bottom for better comparison.}
  \label{fig:depth3_companding}
\end{figure*}


\subsection{Perceptual Loss}
It is well known that per-pixel loss for regression and classification is problematic and could produce blurry outputs or other visual artifacts. This is because each pixel is regarded as an individual object for optimization, resulting in average outputs to some degree. A better strategy is to construct the loss by incorporating the spatial correlation information. Rather than encouraging matching each individual pixels of input and output images, we follow previous works \cite{hou2017deep,johnson2016perceptual,gatys2015neural} to measure the difference between two images at various deep feature levels based on pretrained deep convolutional neural networks. We seek to capture the input images' spatial correlations by means of convolution operations in the deep CNNs.

We denote the loss function as $\mathcal{L}(\hat{y}, y)$ to measure the perceptual difference between two images. As illustrated in Fig. \ref{fig:overview}, both the output image $\hat{y} = T(x)$ generated by the transformation network and the corresponding target image $y$ are fed into a pretrained deep CNN $\Phi$ for feature extraction. We use $\Phi_i(y)$ to represent the hidden representations of image $y$ at $i^{th}$ convolutional layer. $\Phi_i(x)$ is a 3D array of shape [$C_i$, $W_i$, $H_i$], where $C_i$ is the number of filters, $W_i$ and $H_i$ are the width and height of the given feature map of the $i^{th}$ convolutional layer. The final perceptual loss of two images at $i^{th}$ layer is the Euclidean distance of the corresponding 3D arrays as following:


\begin{equation}
\footnotesize
\mathcal{L}_i(\hat{y}, y) =  \frac{1}{C_iW_iH_i}  \sum_{c=1}^{C_i}  \sum_{w=1}^{W_i}  \sum_{h=1}^{H_i} (\Phi_i(\hat{y})_{c,w,h} - \Phi_i(y)_{c,w,h})^2
\end{equation}

In fact, above loss still follows the per-pixel manner if we treat the hidden features which are 3D arrays as ``images" with more than 3 color channels. However this kind of loss has already incorporated the spatial correlation information because the ``pixels" in these images are the combinations of the original pixels through convolution operations.



\subsection{Training Details}
Our implementation uses open source machine learning framework Torch \cite{collobert2011torch7} and a Nvidia Tesla K40 GPU to speed up training. The pretrained 19-layer VGGNet \cite{simonyan2014very} is chosen as the loss network for deep feature extraction which is fixed during the training. In addition, due to the similar convolutional architecture, the loss network can be seamlessly stacked to our ``U-Net" neural network to achieve end-to-end training. The training images are of the shape 256$\times$256 and we train our model with a batch size of 16 for 30,000 iterations. Adam optimizer \cite{kingma2014adam} is used for stochastic optimization with a learning rate of 0.0002. For the LeakyReLU in the encoder, the slope of the leak is set to 0.2 in all layers. Additionally we experiment with conv1\_1, conv2\_1, conv3\_1, conv4\_1 and conv5\_1 layers in VGGNet to construct perceptual loss for comparison.

\section{Experimental Results}
In our experiments, we use Microsoft COCO dataset \cite{lin2014microsoft} which is a large-scale database containing more than 300,000 images as our training images. We resize the training images to 256$\times$256 as our inputs to train our models. We perform experiments on two image processing problems: image companding and inverse halftoning.



\subsection{Image Companding}
One essential part of image companding is to expand lower bit images to higher bit outputs. This technique has been investigated in the context of high dynamic range (HDR) imaging \cite{li2005compressing}, firstly compressing the range of an HDR image into an LDR image, at which point the process is then reversed to retrieve the original HDR image.

Since it is impossible to display a true HDR image with more than 8 bits, we use 8 bit images as our highest bit depth images in the experiments. The 8 bit images are reduced by different depths as the lower bit depth images, and then expanded back to 8 bits. Take 4 bit images for example, they can only have 16 different levels for each color channel while there are 256 different levels for 8 bit images. The default approach \cite{li2005compressing} for converting 8 bit images to 4 bit images is to divide by 16 to quantize the color level from 256 to 16, which will be then scaled up to fill the full range of the display. Mathematically we can use the formula below to easily convert 8 bit images to different lower bit outputs. This operation can be applied to both grayscale images and color images by processing each channel separately.


\begin{equation}
\footnotesize
I_{low} = \lfloor \frac{I_{high}}{2^{(h - l)}} \rfloor 2^{(h - l)}
\end{equation}

where the $I_{low}$ and $I_{high}$ are the pixel intensity of converted lower and higher bit depth images respectively, $l$ and $h$ are the bit depth for lower and higher bit depth images.


We first preprocess the training images to different lower-bit ones as input data, and use the original images as higher-bit targets we want to retrieve. After training, the validation split of Microsoft COCO is used for testing. We first compare the results of different lower-bit input images, and then evaluate how the perceptual loss constructed from different convolutional layers affects the expanding quality.


% Please add the following required packages to your document preamble:
% \usepackage{multirow}
\begin{table}[]
\scriptsize
\centering
\caption{The average companding results of PSNR(dB) and SSIM for 100 color and grayscale images randomly selected from Microsoft COCO validation split. The expanded results were based on a perceptual loss constructed using conv1\_1 layer.}
\label{table:psnr_ssim_conv12_companding}
\begin{tabular}{c|c|c|c|c|c}
\hline
\multicolumn{2}{c|}{\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Input Bit-depth\end{tabular}}} & \multicolumn{2}{c|}{PSNR} & \multicolumn{2}{c}{SSIM} \\ \cline{3-6}
\multicolumn{2}{c|}{}                                        & Compressed     & Expanded     & Compressed     & Expanded     \\ \hline
\multirow{2}{*}{Bit 1}               & Color                  & 11.73          & 18.67    & 0.40           & 0.55     \\ \cline{2-6}
                                     & Grayscale              & 11.58          & 18.81    & 0.35           & 0.49     \\ \hline
\multirow{2}{*}{Bit 2}               & Color                  & 17.37          & 25.65    & 0.67           & 0.81     \\ \cline{2-6}
                                     & Grayscale              & 17.29          & 25.84    & 0.61           & 0.74     \\ \hline
\multirow{2}{*}{Bit 3}               & Color                  & 23.13          & 30.79    & 0.85           & 0.90     \\ \cline{2-6}
                                     & Grayscale              & 23.16          & 31.33    & 0.78           & 0.87     \\ \hline
\multirow{2}{*}{Bit 4}               & Color                  & 29.03          & 34.52    & 0.94           & 0.95     \\ \cline{2-6}
                                     & Grayscale              & 29.19          & 36.69    & 0.90           & 0.94     \\ \hline
\multirow{2}{*}{Bit 5}               & Color                  & 34.85          & 37.59    & 0.98           & 0.97     \\ \cline{2-6}
                                     & Grayscale              & 35.08          & 40.24    & 0.96           & 0.97     \\ \hline
\end{tabular}
\end{table}

\subsubsection{Different Bit Depths}

We have separately trained models for different lower-bit input images for comparison and use the conv1\_1 layer of VGGNet to construct the perceptual loss for all the models.

\textbf{Qualitative Results.}
Fig. \ref{fig:depths_companding1} and Fig. \ref{fig:depths_companding2} show the qualitative results for a variety of color and grayscale images taken from Microsoft COCO 2014 validation split. We can see that the linearly quantized lower-bit images display severe blocking and contouring artifacts. The compression process amplifies low amplitudes and high frequencies which dominate the quantization artifacts because we try to show a lower dynamic range image on a higher dynamic range displayable device. For instance, our device is appropriate for the original 8 bit targets with 256 color levels. We could drop the bit depths of the original images by 5 bits and linearly quantize them to 3 bit images with only 8 color levels. Since the compressed images contain 5 fewer bits, they should be theoretically displayed on 1/32 dynamic range device. It is obvious that this kind of lossy compression introduces visible artifacts in pixel blocks and at block boundaries.

We also show the corresponding expanded images (Fig. \ref{fig:depths_companding1} and Fig. \ref{fig:depths_companding2}) retrieved from our models. The blocking and contouring artifacts are effectively reduced to show smooth appearance in the expanded outputs. For example in the airplane image in Fig. \ref{fig:depths_companding2}, the compressed images show obvious contouring artifacts in the sky while the expanded images have homogeneous gradually changing colors. And this can be further validated from the distribution of intensity histograms. Fig. \ref{fig:hist_companding} shows the intensity histograms for the compressed 2 and 4 bit airplanes and the expanded ones in Fig. \ref{fig:depths_companding2}. It is clear that our methods are able to infer the ``correct" values for a single pixel based on its neighbors, and convey a more attractive impression with rich and saturated colors.


% similar in the image of a bedroom with dogs, the wall and window in the expanded ones are much smoother and look more natural with less blocking artifacts.

% \textsc{results images needs work ... the description and what is actually shown are different. also, need to pick more representative images, the size of the display is still too small, should make then at least double the current size.}

\textbf{Quantitative Results.}
In order to have a comprehensive quantitative evaluation for our models, we report peak signal-to-noise ratio (PSNR) and structural similarity index measure (SSIM) \cite{wang2004image} for quality assessment. PSNR is per-pixel based measurement defined via the mean squared error (MSE) while SSIM index is known as perceptual-aware method for measuring the similarity between two images. For both measurements, a higher value indicates better quality. Table \ref{table:psnr_ssim_conv12_companding} summarizes the average PSNR (dB) and SSIM values of 100 images selected from COCO validation split. Similar to qualitative results, the higher bit images have higher PSNR and SSIM values, indicating better image quality. Additionally, the expanded images produced by our method have significantly higher PSNR and SSIM values compared to the corresponding compressed ones. It is clear that our method can effectively improve the image quality especially for lower bit depth images.


\begin{figure*}[!tb]
\centering
  \includegraphics[width=17cm]{image/halftone.pdf}
  \caption{Inverse halftoning results on images from Microsoft COCO validation split. The conv1\_1, conv5\_1 are the results produced by the models trained by the perceptual losses of corresponding convolutional layers. Additionally an enlarged sub-image of each image is given at the bottom for better comparison.}
  \label{fig:halftone}
\end{figure*}

\subsubsection{Perceptual Loss at Different Convolutional Layers}
Due to the multi-layer architecture of deep convolutional neural networks, the perceptual loss can be defined by different convolutional layers. Therefore, we conduct experiments to investigate the performance for different perceptual losses. In all the experiment we use 3 bit depths (8 color levels) as input images to train our deep networks for both color and grayscale images.

\textbf{Qualitative Results.}
As shown in Fig. \ref{fig:depth3_companding}, all the expanded outputs can effectively reduce the blocking and contouring artifacts and reveal continuous-tone results in general. However, the reconstruction based on perceptual loss of higher-level layers could introduce new artifacts such as grid patterns as shown in images for conv3\_1 and conv5\_1 layers. We observed similar phenomenons for input images of different bit-depths. One explanation to this is that, the higher layers will cover a larger area in the input image, and the areas covered by conv3\_1 and conv5\_1 layers are too large to construct a natural looking image. That is, spatial correlations across a large area of the image do not capture natural appearances of an image. Expanding based on perceptual loss of deep features of conv1\_1 layer or lower layers does not have this kind of artifacts. This could be also validated by previous work \cite{mahendran2015understanding} that tries to compute an approximate inverse image from its deep features. It shows that the first few layers in a pretrained CNN are essentially an invertible code of the image and maintain a photographically faithful representations, and the higher level features are corresponding to a more coarse space area of the encoded image.

%It is clear that low-level features are better to balance the old artifacts reduction and new artifacts introduction for image companding.

\textbf{Quantitative Results.}
Table \ref{table:psnr_ssim_depth3_companding} shows the average PSNR and SSIM values for 100 COCO testing images based on perceptual losses constructed with different convolutional layers. On the one hand, both the PSNR and SSIM of our expanded images are much higher than those of the compressed lower bit images, and the compressed images can be significantly improved by our method. On the other hand, the expanded images based on perceptual losses of lower level layers have higher PSNR and SSIM values. This is because new artifacts like grid pattern will be introduced (Fig. \ref{fig:depth3_companding}) although the blocking artifacts can be reduced.

% \subsubsection{JPEG Compression Artifacts Reduction}
% We have seen that our method can effectively reduce blocking artifacts produced by reducing bit depth for a given image. It is natural to ask whether it could be useful for artifact reduction caused lossy image compression, which is indispensable and inevitable for websites and companies to save storage space and bandwidth. Therefore, we then take JPEG compression as an example to investigate how our method can be applied for compression artifacts reduction. The process of JPEG compression first divides an image into 8$\times$8 pixel blocks and applies block discrete cosine transformation (DCT) for each block, which will be further processed by quantization in order to save storage space. Thus, blocking and ringing artifacts could arise due to the discontinuities between different 8$\times$8 blocks and quantizations.

% In our experiment, we directly apply the trained model above for 3 bit images by perceptual loss constructed from conv1\_1 layer to JPEG compressed images. We use the quality settings $q = 10$ for stand JPEG compression. From Fig. \ref{fig:jpeg}, we can see that our method can also effectively reduce the JPEG compression blocking artifacts. 



\begin{table}[]
\scriptsize
\centering
\caption{The average companding results of PSNR(dB) and SSIM for 100 color and grayscale testing images. The compressed input images are 3 bits, and the expanded results based on perceptual loss constructed with different convolutional layers are shown.}
\label{table:psnr_ssim_depth3_companding}
\begin{tabular}{c|c|c|c|c|c}
\hline
\multicolumn{2}{c|}{\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Perceptual\\ Loss Layer\end{tabular}}} & \multicolumn{2}{c|}{PSNR} & \multicolumn{2}{c}{SSIM} \\ \cline{3-6}
\multicolumn{2}{c|}{}                                                                                & Compressed     & Expanded     & Compressed     & Expanded     \\ \hline
\multirow{2}{*}{Conv1}                                   & Color                                      & 23.13          & 32.64    & 0.85           & 0.93     \\ \cline{2-6}
                                                         & Grayscale                                  & 23.16          & 32.57    & 0.78           & 0.91     \\ \hline
\multirow{2}{*}{Conv2}                                   & Color                                      & 23.13          & 30.00    & 0.85           & 0.88     \\ \cline{2-6}
                                                         & Grayscale                                  & 23.16          & 30.74    & 0.78           & 0.85     \\ \hline
\multirow{2}{*}{Conv3}                                   & Color                                      & 23.13          & 28.17    & 0.85           & 0.87     \\ \cline{2-6}
                                                         & Grayscale                                  & 23.16          & 29.60    & 0.78           & 0.81     \\ \hline
\multirow{2}{*}{Conv4}                                   & Color                                      & 23.13          & 25.74    & 0.85           & 0.84     \\ \cline{2-6}
                                                         & Grayscale                                  & 23.16          & 29.54    & 0.78           & 0.82     \\ \hline
\multirow{2}{*}{Conv5}                                   & Color                                      & 23.13          & 25.43    & 0.85           & 0.87     \\ \cline{2-6}
                                                         & Grayscale                                  & 23.16          & 27.50    & 0.78           & 0.77     \\ \hline
\end{tabular}
\end{table}


\begin{figure*}[!htb]
\centering
  \includegraphics[width=17cm]{image/other_hafltone.pdf}
  \caption{A comparison of inverse halftoning results on grayscale \textit{Lena} and \textit{Peppers} images by different methods. We compare our CNN Inverse method with those of Fastiht2 \cite{kite2000fast} and Wavelet-based WInHD \cite{neelamani2002winhd}. We report PSNR / SSIM for each example.}
  \label{fig:other_hafltone}
\end{figure*}

\begin{figure*}[!htb]
\centering
  \includegraphics[width=17cm]{image/lena_color_halftone.pdf}
  \caption{A comparison of inverse halftoning results on color \textit{Koala} and \textit{Cactus} images by different methods. We compare our CNN Inverse method with those of GLDP \cite{son2012inverse} and LLDO \cite{son2014local}. We report PSNR / SSIM for each example.}
  \label{fig:lena_color_halftone}
\end{figure*}




\subsection{Inverse halftoning}
Another similar image processing problem we are interested in is inverse halftoning. This task is to generate a continuous-tone image from halftoned binary images. This problem is also inherently ill-posed since there could exist multiple continuous-tone images corresponding to the halftoned ones. In our experiments we try to use deep feature based perceptual loss to allow the inversed halftones perceptually similar to the given targets. We experiment with both color and grayscale images by using the same approach and employ error diffusion based Floydâ€“Steinberg dithering for halftoning.



\textbf{Qualitative Results.}
We test our models on random samples of images from Microsoft COCO validation split. The inverse halftoning results are shown in Fig. \ref{fig:halftone}. We can see that the inversed outputs produced by our method are visually similar to the original images. All the outputs can show much smoother textures and produce sharper edges. For instance, sharp kite line and smooth sky can be reconstructed in the kite image. When comparing with the inversed outputs produced by using perceptual loss of different level layers, the outputs from lower-level layer is visually better than those from higher-level layer. Like image companding, grid pattern artifacts can be introduced when using higher-level layer to construct perceptual loss.


\begin{table}[]
\scriptsize
\centering
\caption{The average inversed halftoning results of PSNR(dB) and SSIM for 100 color and grayscale images selected from Microsoft COCO validation split.}
\label{table:halftone}
\begin{tabular}{c|c|c|c|c|c}
\hline
\multicolumn{2}{c|}{\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Perceptual\\ Loss Layer\end{tabular}}} & \multicolumn{2}{c|}{PSNR} & \multicolumn{2}{c}{SSIM} \\ \cline{3-6}
\multicolumn{2}{c|}{}                                                                               & Halftone      & CNN Inverse      & Halftone      & CNN Inverse      \\ \hline
\multirow{2}{*}{Conv1}                                  & Color                                      & 8.08          & 31.43     & 0.20          & 0.91      \\ \cline{2-6}
                                                        & Grayscale                                  & 7.92          & 31.36     & 0.14          & 0.90      \\ \hline
\multirow{2}{*}{Conv2}                                  & Color                                      & 8.08          & 20.98     & 0.20          & 0.59      \\ \cline{2-6}
                                                        & Grayscale                                  & 7.92          & 23.98     & 0.14          & 0.67      \\ \hline
\multirow{2}{*}{Conv3}                                  & Color                                      & 8.08          & 24.05     & 0.20          & 0.73      \\ \cline{2-6}
                                                        & Grayscale                                  & 7.92          & 27.44     & 0.14          & 0.74      \\ \hline
\multirow{2}{*}{Conv4}                                  & Color                                      & 8.08          & 26.48     & 0.20          & 0.85      \\ \cline{2-6}
                                                        & Grayscale                                  & 7.92          & 27.82     & 0.14          & 0.76      \\ \hline
\multirow{2}{*}{Conv5}                                  & Color                                      & 8.08          & 25.47     & 0.20          & 0.84      \\ \cline{2-6}
                                                        & Grayscale                                  & 7.92          & 26.48     & 0.14          & 0.69      \\ \hline
\end{tabular}
\end{table}



In addition, we also compare our method on two widely used grayscale images \textit{Lenna} and \textit{Peppers} with other algorithms. Fig. \ref{fig:other_hafltone} shows comparative grayscale results against previous Fastiht2 \cite{kite2000fast} and Wavelet-based WInHD \cite{neelamani2002winhd} algorithms. We also report the PSNR / SSIM measurement for each image. It is clear that our learning-based method can achieve state-of-the-art results and produce sharp edges and fine details, such as the hat in the Lenna image. Our deep models can effectively and correctly learn the relevant spatial correlation and semantic between different pixels and infer the ``best" values for a single pixel based on its neighbors. Moreover, our method can be naturally adapted to color images and produce high-quality continuous-tone color images from corresponding halftones. Fig. \ref{fig:lena_color_halftone} shows the resulting images for the \textit{Koala} and \textit{Cactus} image, which include fine textures and structures. We compare our results (CNN Inverse) with those of two recent methods GLDP \cite{son2012inverse} and LLDO \cite{son2014local}. We can see that our method can provide better resulting images with well expressed fur and bark in the \textit{Koala} image, and distinct boundaries of the fine sand and sharpened edges of splines in the \textit{Cactus} image.


\begin{table*}[!htb]
\centering
\caption{PSNR (dB) and SSIM comparison of different inverse halftoning methods for color images: MAP \cite{stevenson1997inverse}, ALF \cite{kite2000fast}, LPA-ICI \cite{foi2004inverse}, GLDP \cite{son2012inverse}, LLDO \cite{son2014local} and our CNN Inverse.}
\label{table:color_other_inverse}
\begin{tabular}{c|c|c|c|c|c|c|c|c|c|c|c|c}
\hline
\multirow{2}{*}{Image} & \multicolumn{2}{c|}{ALF} & \multicolumn{2}{c|}{MAP} & \multicolumn{2}{c|}{LPA-ICI} & \multicolumn{2}{c|}{GLDP} & \multicolumn{2}{c|}{LLDO} & \multicolumn{2}{c}{CNN Inverse} \\ \cline{2-13} 
                       & PSNR        & SSIM       & PSNR        & SSIM       & PSNR          & SSIM         & PSNR        & SSIM        & PSNR         & SSIM       & PSNR            & SSIM           \\ \hline
Koala                  & 22.36       & 0.66       & 23.33       & 0.74       & 24.17         & 0.76         & 24.58       & 0.78       & 25.01        & 0.80       & 27.63           & 0.89           \\ \hline
Cactus                 & 22.99       & 0.64       & 23.95       & 0.77       & 25.04         & 0.79         & 25.40       & 0.81        & 25.55        & 0.82       & 27.69           & 0.92           \\ \hline
Bear                   & 21.82       & 0.62       & 22.63       & 0.72       & 23.14         & 0.72         & 23.66       & 0.77        & 24.17        & 0.78       & 26.35           & 0.89           \\ \hline
Barbara                & 25.41       & 0.71       & 26.24       & 0.78       & 27.88         & 0.83         & 27.12       & 0.80        & 28.48        & 0.85       & 31.79           & 0.92           \\ \hline
Shop                   & 22.14       & 0.64       & 22.46       & 0.69       & 24.12         & 0.77         & 23.86       & 0.75        & 24.61        & 0.80       & 27.27           & 0.89           \\ \hline
Peppers                & 30.92       & 0.87       & 28.25       & 0.77       & 30.70         & 0.87         & 30.92       & 0.87        & 31.07        & 0.87       & 31.44           & 0.89           \\ \hline
\end{tabular}
\end{table*}

\textbf{Quantitative Results.}
We use PSNR and SSIM as quality metrics to quantitatively evaluate our inverse halftoning results. Table \ref{table:halftone} shows the average PSNR and SSIM values for 100 COCO testing images constructed from different convolutional layers.
It is clear that based on these image evaluation metrics, our method can improve the images by a large margin for both color and grayscale images. In our experiment, the best results are produced by the model trained with conv1\_1 layer.
When using perceptual loss based on higher layers gives rise to a slight grid pattern artifacts visible under magnification, which harms the PSNR and SSIM.

Moreover, we conduct experiments to compare with several previous methods. We use 6 images \textit{Koala}, \textit{Cactus}, \textit{Bear}, \textit{Barbara}, \textit{Shop} and \textit{Peppers}, the same as \cite{son2014local} for testing. Table \ref{table:color_other_inverse} shows the PSNR and SSIM results for conventional methods based on MAP estimation \cite{stevenson1997inverse}, ALF \cite{kite2000fast}, LPA-ICI \cite{foi2004inverse} and recent GLDP \cite{son2012inverse} and LLDO \cite{son2014local}. We can see that our algorithm (CNN Inverse) can achieve new state-of-the-art results and significantly outperform previous methods for inverse halftoning.





\section{Discussion}
Image companding and inverse halftoning are two similar image processing problems in the sense that they attempt to use a lower bit depth image to represent a higher bit depth version of the same image. The naive bit depth compression in image companding is directly applying image quantization technique. It can retain the overall structure and color contrast, however blocking and contouring artifacts will be introduced that make the compressed images look unnatural with visually annoying artifacts. Halftone images try to simulate continuous-tone imagery through the use of dots with only two color levels per channel. The reproduction of halftones for humans relies on an optical illusion that tiny halftone dots could be blended into smooth tones by human eyes. In order to expand the compressed images and inverse the halftones, traditional methods usually need to design expanding and inverse operators manually. For example, the halftone technique such as the specific dithering algorithms should be given in advance in order to design an inverse operator. In this paper, we show that a learning based method can formulate the two problems in the same framework and a perceptual loss based on pretrained deep networks can be used to guide the training. This paper demonstrates that deep convolutional neural networks can not only be applied to high-level vision problems like image classification, but also to traditional low-level vision problems. Although we can use popular metrics like PSNR and SSIM to quantitatively measure the image quality, it is worth pointing out that the assessment of image quality is still a challenging problem. PSNR and SSIM could correlate poorly with human assessment of visual quality and further works are needed for perceptually better image measurement.


\section{Conclusion}
In this paper, we propose to train deep convolutional neural networks with a perceptual loss for two low-level image processing problems: image companding and inverse halftoning. Our method is very effective in dealing with compressed blocking and contouring artifacts for companding and reproduces state-of-the-art continuous-tone outputs from binary halftone images. In addition, we systematically investigated how the perceptual loss constructed with different convolutional layers of the pretrained deep network affects the generated image quality.


% An example of a floating figure using the graphicx package.
% Note that \label must occur AFTER (or within) \caption.
% For figures, \caption should occur after the \includegraphics.
% Note that IEEEtran v1.7 and later has special internal code that
% is designed to preserve the operation of \label within \caption
% even when the captionsoff option is in effect. However, because
% of issues like this, it may be the safest practice to put all your
% \label just after \caption rather than within \caption{}.
%
% Reminder: the "draftcls" or "draftclsnofoot", not "draft", class
% option should be used if it is desired that the figures are to be
% displayed while in draft mode.
%
%\begin{figure}[!t]
%\centering
%\includegraphics[width=2.5in]{myfigure}
% where an .eps filename suffix will be assumed under latex,
% and a .pdf suffix will be assumed for pdflatex; or what has been declared
% via \DeclareGraphicsExtensions.
%\caption{Simulation results for the network.}
%\label{fig_sim}
%\end{figure}

% Note that the IEEE typically puts floats only at the top, even when this
% results in a large percentage of a column being occupied by floats.


% An example of a double column floating figure using two subfigures.
% (The subfig.sty package must be loaded for this to work.)
% The subfigure \label commands are set within each subfloat command,
% and the \label for the overall figure must come after \caption.
% \hfil is used as a separator to get equal spacing.
% Watch out that the combined width of all the subfigures on a
% line do not exceed the text width or a line break will occur.
%
%\begin{figure*}[!t]
%\centering
%\subfloat[Case I]{\includegraphics[width=2.5in]{box}%
%\label{fig_first_case}}
%\hfil
%\subfloat[Case II]{\includegraphics[width=2.5in]{box}%
%\label{fig_second_case}}
%\caption{Simulation results for the network.}
%\label{fig_sim}
%\end{figure*}
%
% Note that often IEEE papers with subfigures do not employ subfigure
% captions (using the optional argument to \subfloat[]), but instead will
% reference/describe all of them (a), (b), etc., within the main caption.
% Be aware that for subfig.sty to generate the (a), (b), etc., subfigure
% labels, the optional argument to \subfloat must be present. If a
% subcaption is not desired, just leave its contents blank,
% e.g., \subfloat[].


% An example of a floating table. Note that, for IEEE style tables, the
% \caption command should come BEFORE the table and, given that table
% captions serve much like titles, are usually capitalized except for words
% such as a, an, and, as, at, but, by, for, in, nor, of, on, or, the, to
% and up, which are usually not capitalized unless they are the first or
% last word of the caption. Table text will default to \footnotesize as
% the IEEE normally uses this smaller font for tables.
% The \label must come after \caption as always.
%
% \begin{table}[!t]
% %% increase table row spacing, adjust to taste
% %\renewcommand{\arraystretch}{1.3}
% % if using array.sty, it might be a good idea to tweak the value of
% % \extrarowheight as needed to properly center the text within the cells
% \caption{An Example of a Table}
% \label{table_example}
% \centering
% %% Some packages, such as MDW tools, offer better commands for making tables
% %% than the plain LaTeX2e tabular which is used here.
% \begin{tabular}{|c||c|}
% \hline
% One & Two\\
% \hline
% Three & Four\\
% \hline
% \end{tabular}
% \end{table}


% Note that the IEEE does not put floats in the very first column
% - or typically anywhere on the first page for that matter. Also,
% in-text middle ("here") positioning is typically not used, but it
% is allowed and encouraged for Computer Society conferences (but
% not Computer Society journals). Most IEEE journals/conferences use
% top floats exclusively.
% Note that, LaTeX2e, unlike IEEE journals/conferences, places
% footnotes above bottom floats. This can be corrected via the
% \fnbelowfloat command of the stfloats package.





% if have a single appendix:
%\appendix[Proof of the Zonklar Equations]
% or
%\appendix  % for no appendix heading
% do not use \section anymore after \appendix, only \section*
% is possibly needed

% use appendices with more than one appendix
% then use \section to start each appendix
% you must declare a \section before using any
% \subsection or using \label (\appendices by itself
% starts a section numbered zero.)
%


% \appendices
% \section{Proof of the First Zonklar Equation}
% Appendix one text goes here.

% you can choose not to have a title for an appendix
% if you want by leaving the argument blank
% \section{}
% Appendix two text goes here.


% use section* for acknowledgment
% \section*{Acknowledgment}


% The authors would like to thank...


% Can use something like this to put references on a page
% by themselves when using endfloat and the captionsoff option.
\ifCLASSOPTIONcaptionsoff
  \newpage
\fi



% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://mirror.ctan.org/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
%\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)

\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,IEEEexample}



% biography section
%
% If you have an EPS/PDF photo (graphicx package needed) extra braces are
% needed around the contents of the optional argument to biography to prevent
% the LaTeX parser from getting confused when it sees the complicated
% \includegraphics command within an optional argument. (You could create
% your own custom macro containing the \includegraphics command to make things
% simpler here.)
% \begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{image/halftone.png}}]{Michael Shell}
% or if you just want to reserve a space for a photo:

% \begin{IEEEbiography}
% Biography text here.
% \end{IEEEbiography}

% % if you will not have a photo at all:
% \begin{IEEEbiographynophoto}{John Doe}
% Biography text here.
% \end{IEEEbiographynophoto}

% % insert where needed to balance the two columns on the last page with
% % biographies
% %\newpage

% \begin{IEEEbiographynophoto}{Jane Doe}
% Biography text here.
% \end{IEEEbiographynophoto}

% You can push biographies down or up by placing
% a \vfill before or after them. The appropriate
% use of \vfill depends on what kind of text is
% on the last page and whether or not the columns
% are being equalized.

%\vfill

% Can be used to pull up biographies so that the bottom of the last one
% is flush with the other column.
%\enlargethispage{-5in}



% that's all folks
\end{document}


