
\begin{table}[t]
\centering
\footnotesize
\renewcommand{\arraystretch}{1.3}
\begin{tabular}{l | c | c | c}
\toprule
\textbf{Model} & \bf SST & \bf CoLA & \bf Others (avg) \\
\midrule 
% \multicolumn{10}{l}{\textit{Transformer Models}}\\
% Base &  &  &  &  &  &  &  &  &  \\
% \quad + Masked &  &  &  &  &  &  &  &  &  \\
% \quad + MLM &  &  &  &  &  &  &  &  &  \\
% Large &  &  &  &  &  &  &  &  &  \\
% \quad + Masked &  &  &  &  &  &  &  &  &  \\
% \quad + MLM &  &  &  &  &  &  &  &  &  \\
% \midrule 
% 84.4 88.4 86.7 92.7
% $\text{BERT}$  & 84.3 & 88.4 &  &  & 92.7 & 86.7 &  &  &  \\
% $\text{BARNEY}_\text{BERT}$ &  &  &  &  &  &  &  &  &  \\
RoBERTa-base & 94.8 & 63.6 & 88.7 \\
\textsc{Autobot}-base & \textbf{95.0} & \textbf{66.0} & 88.7 \\
\midrule 
% $\text{BERT}$ & 86.6 & 92.3 & 91.3 & 70.4 & 93.2 & 88.0 & 60.6 & 90.0 &  -\\
% $\text{BARNEY}_\text{BERT}$ &  &  &  &  &  &  &  &  &  \\
RoBERTa-large & 96.4 & 68.0 & 91.1 \\
\textsc{Autobot}-large & \textbf{96.9} & \textbf{70.2} & 91.1 \\
\bottomrule
\end{tabular} % 87.6 92.8 91.9 78.7 94.8 90.2 63.6 91.2
\caption{%\nikos{can we fit the other tasks here too? if available (otherwise in appendix)} \ivan{I could add a col. for avg. of other tasks} \nikos{great idea! we should definitely do that} 
%\nikos{here we are not comparing with SRoberta?} \ivan{Hmmm, SRoBERTa was only intended for sentence representations and never evaluated for classification, so I didn't do that experiment}
Single-sentence GLUE classification dev.~results. Median accuracy is reported over over three random seeds. Our model improves performance on single-sentence classification tasks over both base and large RoBERTa models while maintaining their performance on the remaining multi-sentence tasks. % \ivan{Explain why we exclude other tasks}
% We exclude other GLUE task metrics as AUTOBOT achieves an identical performance as the underlying transformer in the encoder on dual sentence tasks.
% \ivan{Perhaps reduce this table to only show the single-sentence results? (MNLI, SST, CoLA) And note results unchanged for others} \nikos{good idea, let's keep the results for other non sentence similarity tasks to support the point that performance is largely maintained on the other tasks (unclear if this is the case with sentence bert).}
}


% Averages are obtained from the GLUE leaderboard.
\label{tab:glue}
\end{table}

% take the median, cite jesse paper



% \begin{table*}[ht]
% \centering
% \begin{tabular}{lcccccccccc}
% \toprule
% & \bf MNLI & \bf QNLI & \bf QQP & \bf RTE & \bf SST & \bf MRPC & \bf CoLA & \bf STS & \bf Avg \\
% \midrule 
% % \multicolumn{10}{l}{\textit{Transformer Models}}\\
% % Base &  &  &  &  &  &  &  &  &  \\
% % \quad + Masked &  &  &  &  &  &  &  &  &  \\
% % \quad + MLM &  &  &  &  &  &  &  &  &  \\
% % Large &  &  &  &  &  &  &  &  &  \\
% % \quad + Masked &  &  &  &  &  &  &  &  &  \\
% % \quad + MLM &  &  &  &  &  &  &  &  &  \\
% % \midrule 
% \multicolumn{10}{l}{\textit{Base Models}}\\
% % 84.4 88.4 86.7 92.7
% % $\text{BERT}$  & 84.3 & 88.4 &  &  & 92.7 & 86.7 &  &  &  \\
% % $\text{BARNEY}_\text{BERT}$ &  &  &  &  &  &  &  &  &  \\
% $\text{RoBERTa-base}$ & 87.6 & 92.8 & 91.9 & 78.7 & 94.8 & 90.2 & 63.6 & 91.2 & \\
% $\text{BARNEY}_{\text{RoBERTA-base}}$ & \textbf{88.0} &  & 91.9 &  & \textbf{95.0} & & \textbf{66.0} & &   \\
% \midrule 
% \multicolumn{10}{l}{\textit{Large Models}}\\
% % $\text{BERT}$ & 86.6 & 92.3 & 91.3 & 70.4 & 93.2 & 88.0 & 60.6 & 90.0 &  -\\
% % $\text{BARNEY}_\text{BERT}$ &  &  &  &  &  &  &  &  &  \\
% $\text{RoBERTa-large}$ & 90.2 & 94.7 & 92.2 & 86.6 & 96.4 & 90.9 & 68.0 & 92.4 \\
% $\text{BARNEY}_\text{RoBERTA-large}$ & 90.5 &  &  &  & 96.9 & & \textbf{70.2} &  &  \\
% \bottomrule
% \end{tabular} % 87.6 92.8 91.9 78.7 94.8 90.2 63.6 91.2
% \caption{
% Dev results on GLUE.
% % Results on GLUE. All results are based on a 24-layer architecture.
% % \bertlarge{} and \xlnetlarge{} results are from \newcite{devlin2018bert} and \newcite{yang2019xlnet}, respectively.
% % \ourmodel{} results on the development set are a median over five runs.
% % \ourmodel{} results on the test set are ensembles of \emph{single-task} models.
% \ivan{Perhaps reduce this table to only show the single-sentence results? (MNLI, SST, CoLA) And note results unchanged for others} \nikos{good idea, let's keep the results for other non sentence similarity tasks to support the point that performance is largely maintained on the other tasks (unclear if this is the case with sentence bert).}
% For RTE, STS and MRPC we finetune starting from the MNLI model instead of the baseline pretrained model.}


% % Averages are obtained from the GLUE leaderboard.
% \label{tab:roberta_glue}
% \end{table*}