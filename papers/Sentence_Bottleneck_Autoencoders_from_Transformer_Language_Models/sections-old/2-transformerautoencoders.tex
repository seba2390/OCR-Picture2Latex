

\section{Proposed Method: \textsc{Autobots}}

\subsection{Encoder model}

\subsubsection{Pretrained transformers} 
\nikos{should also mention here the finetuning options} 

\subsubsection{Context Attention Bottleneck} 

\subsection{Decoder model}

\subsection{Training objective}




\nikos{I'd be in favor of merging the two sections below into one section with the structure above. Let's not overwhelm the reader with two proposed ideas. Let's keep it simple.}
\section{Transformer Autoencoders}
%\nikos{The two sections could probably be merged together to simply BARNEY, unless we decide to pitch it as "Transformer autoencoders for language understanding (Barney) and generation (emb2emb)."}
\nikos{this paragraph is a bit redundant. we could move it in the introduction.}
We describe here the proposed Transformer Autoencoders (TAEs). The main idea is to convert the encoder-decoder transformer architecture proposed by \citet{vaswani17} to an autoencoder architecture. The main difficulty we are addressing here is that the original encoder-decoder transformer operates with variable-length vectors. We show below how it can be adapted to work with a bottleneck using fixed-length vectors. The resulting autoencoder can be used for monolingual \cite{devlin-etal-2019-bert} pretraining, multilingual pretraining \cite{artetxe18} or it can be used for solving sequence-to-sequence tasks such as machine translation \citep{vaswani17}.=
%\nikos{By the way if this works we could even test it for translation in the future.} \ivan{Good point! I had ideas of using it with mBERT and possibly creating better language-agnostic representations, a similar setup to Facebook's LASER}
%- \url{https://arxiv.org/pdf/1812.10464.pdf} \\
% \ivan{I have some ideas for the MT idea (but probably shouldn't be in the scope of this paper)}

\subsection{Context Attention Bottleneck}
\nikos{see comment above}
The Transformer encoder architecture outputs a hidden representation $\mathbf{h_t}$ for each input token $x_t$. We wish to reduce these representations to a single, fixed representation $\mathbf{z}$. The self-attention layers used in Transformers produce a many-to-many representation by making the keys, queries, and values come from the same input. In order to produce a many-to-one final representation, we introduce a context vector $\mathbf{u}$ to replace the query. 
% \ivan{I commented out option (1), which we could explore later, option (2) boils down to multihead attention with nhead=1, which we can examine in an ablation study}
\begin{align*}
    \mathbf{z} &= \text{CAB}(\mathbf{u}, \mathbf{K}, \mathbf{V}) \\
    &= \text{MultiHead}(\mathbf{u}^\top, \mathbf{K}, \mathbf{V}) \\
       &= \text{Concat}(\text{head}_1, ..., \text{head}_n) \\%\mathbf{W_C} \nikos{is this transformation needed?}
      \text{head}_i = & \text{Attention}(\mathbf{u}^\top \mathbf{W^{(i)}_Q}, \mathbf{K} \mathbf{W^{(i)}_K}, \mathbf{V} \mathbf{W^{(i)}_V})
\end{align*} 
% As such, we explore the following options to compute the bottleneck representation \\

% \vspace{1mm}

% \noindent \textbf{Option 1:} Use an attention function akin to the one used for parametric pooling at different levels in hierarchical attention networks \cite{yang-etal-2016-hierarchical}
% %
% $$a_t = \frac{ \exp\big[{\mathbf{u} \cdot \sigma(\mathbf{W_a}  \mathbf{h_t} + \mathbf{b_a})}\big]}{\sum_{i=1}^T  \exp\big[{\mathbf{u} \cdot \sigma(\mathbf{W_a} \cdot \mathbf{h_i} + \mathbf{b_a})\big]}} $$
% $$\mathbf{z} = \sum_{t=1}^T a_t \cdot (\mathbf{h_t}  \mathbf{W_v})$$
% %
% where $\sigma(\cdot)$ is an activation function, here \textit{tanh} and $W_k$, $b_k$ are the parameters of the attention transformation.  

% \vspace{2mm}

% \noindent \textbf{Option 2:} Modify the dot-product self-attention used in transformers \cite{vaswani17} to be a learned-context attention, as follows
% %
% $$\mathbf{z} = \text{Attention}(\mathbf{u}^\top, \mathbf{K}, \mathbf{V}) = \text{softmax}(\frac{\mathbf{u}^\top \mathbf{K}^\top}{\sqrt{d_k}})\mathbf{V}$$
% %
% \noindent where the matrices $\mathbf{K}, \mathbf{V}$ correspond to the key  and the value matrices from the original transformer. Note that the query $\mathbf{Q}$ matrix is not used in the computation. \nikos{Instead of learning $\mathbf{u}$, couldn't we make use of $\mathbf{Q}$ by setting $u = \sum_j Q_j$?} \ivan{I like this idea too. Similar commentary as the [CLS] below}

% \vspace{2mm}

% \noindent\textbf{Option 3:} Extend option two above to a multi-head learned-context attention as follows
%

% \nikos{if we set u to [CLS] representation at the N-1 layer above do we recover the final [CLS] representation in BERT?}
% \ivan{That's actually a great idea/point, I think BARNEY should be the N+1th layer with [CLS] being it's u. We do recover it, but since our objective is different, it makes this method interesting}
% \ivan{That's really good suggestion/point! In the general case of the "learned context attention bottleneck", we can just say the context vector is learned, but in the BARNEY case, the context vector is just the [CLS] token. Perhaps our bottleneck should be called a "Context Attention Bottleneck" instead} \ivan{But to your question, it pretty much would, except for the fact that we're ramping up the amount of heads. Having $N_{head}$=$d_{model}$ is motivated by the fact that the gating mechanism is element-wise, and having that many heads will make the model do separate attention per element}
%
% where the number of heads $n$ can be \textit{large} ($n=d_{model}$), permitted by  the fact that the query is a single context vector. This allows significantly larger expressivity than the previous two options with comparable computational overhead as a single transformer layer.
The learned-context attention bottleneck module can be applied to the output of any transformer encoder architecture.

% \vspace{2mm}
% Of course, there are other  ways to define the bottleneck that does not require learning any parameters such as max-pooling or averaging with which we plan to compare in Section \ref{sec:experiments}
\noindent 
% \cite{Reimers2019SentenceBERT} explore non-parametric pooling bottlenecks such as max and averaging in addition to the [CLS] token of BERT, and we compare the performance of our method using their framework in Section \ref{sec:experiments}.

%\noindent \nikos{Would it make sense to learn a different u for different lengths of the context? e.g. sentence vs phrase, vs paragraph? By the way the learned context attention could be furthermore conditioned on other kinds of information in future work (e.g. sentiment). } 
%\ivan{That's a good point. Iteration 1 comes directly from the following paper: \url{https://www.aclweb.org/anthology/N16-1174.pdf}. Iteration 2 is pure decomposition of attention in the many-to-one case. Idea 3 (new!) is to use the context vector in Multihead Attention, and since the context is simply a single query vector, we can make the number of heads \textit{large}. It's a lot more expressive, and I like the phrase \textbf{BARNEY has \textit{many} heads}} \\\\

% \ivan{I'd like to test out this bottleneck against simple pooling methods, such as max/mean. I'd also like to see how gating does against no gating. Use to evaluate on the task? Just evaluate validation loss/BLEU?}
% \nikos{Good idea. We know from prior work that learned context attention is better than average (https://arxiv.org/pdf/1707.00896.pdf) but it would be nice to verify and compare with max pooling too. When you say gating you mean element-wise product with a learned vector + sigmoid? } \ivan{Yes! When I refer to "gating", I refer to the modifications to the "gating" solution in the decoder section} \nikos{Evaluating on validation loss makes sense but I am not sure we can afford training until convergence for each of these options. Perhaps, test validation scores for a fixed number of epochs? it's definitely not ideal but it could give us some initial clues. We should probably put more weight on the option 3 which is close to the way the [CLS] token is computed.   }
% \ivan{Very good point. I initially had the options to show the iterations from the initial bottleneck idea to a more expressive one (the multihead one). The SentenceBERT specifically aims at making sentence representations and explores max vs. mean vs. sum, and find that mean works best. I feel like focusing on just the multihead (option 3) would be best, and we can do an ablation study on the STS task for the amount of heads (and optionally options 1 and 2)}

\input{tables/glue}
\subsection{Encoder}
\nikos{see comment above}
Let $\text{TE}(x)$ be an arbitrary transformer encoder and $\text{CAB}(x)$ be the Context Attention Bottleneck specified above. Then, the encoder of our transformer autoencoder is defined as the following.
$$\mathbf{z} = \text{Enc}_{\text{TAE}}(\mathbf{x}) = \text{CAB}(\mathbf{u}, \text{TE}(\mathbf{x}), \text{TE}(\mathbf{x}))$$
The context vector $u$ can be either be represented as a learned parameter, or a pooling of the $\text{TE}(\mathbf{x})$. This context vector could additionally used to condition on different contexts to adapt a particular bottleneck to different types of information.
% An interesting property of the context attention is that the context vector can be specific to different types of contexts e.g. sentences or paragraphs. An extension would be to condition the attention function with other kinds of information (e.g. sentiment) as done in autoencoders for style transfer. 

% \nikos{We may want to optionally share the parameters of encoder and decoder for efficiency. }
\subsection{Decoder}
\nikos{see comment above}
The "encoder-decoder attention" layer in the Transformer decoder architecture by \citet{vaswani17}  expects hidden representations for every token input from the encoder in order for each output candidate to attend to each input token. In the situation where only a single representation comes from the encoder, we get the following:
% Analyzing the self attention in the context of \textbf{TAE}, both the memory keys and values in the self attention come from the single hidden representation $z$, which reduces to simply the linear transformation of the latent representation of the input sequence for each output candidate. \ivan{Using the notation in the original paper, the derivation would be the following:}
$$\text{Attention}(\mathbf{Q}, \mathbf{z}^\top \mathbf{W_K}, \mathbf{z}^\top \mathbf{W_V}) = \mathbf{z}^\top \mathbf{W_V}$$
Note that the queries Q, which come from the previous masked self-attention layer, aren't taken into account, and each step in the decoder will recieve the exact same $\mathbf{z}^\top \mathbf{W_V}$ as a result.  In order to mitigate this, we propose a gating method to allow the input query to control the per-element amount of information to include from the hidden representation vector. %\nikos{nice!}  
Concretely, let $\mathbf{Q_t}$ be the $t$-th query representation. Then, the $t$-th output $\mathbf{o_t}$ of the "encoder-decoder" layer will be the following:
$$\mathbf{g_t} = \sigma(\mathbf{W_{gh}}  \mathbf{Q_t} + \mathbf{W_{gz}}  \mathbf{z} +  \mathbf{b_g})$$
% \nikos{the gate $g_t$ doesn't look at the \mathbf{z} vector at all, so it's output will be independent of the z vector. we should probably add z as input to make sure features from both $h_t$ and $z_t$ are considered. e.g. $W_g h_t + b_g + W_z z + b_z$}
$$\mathbf{o_t} = \mathbf{g_t} \odot \mathbf{z}^\top\mathbf{W_V}$$
\noindent where $\sigma(\cdot)$ is the \textit{sigmoid} activation function and $\mathbf{W_g}$, $\mathbf{b_g}$ are the parameters of the transformation for the gate.
One can view the role of the gate here as determining how much information from the linear transformation of the latent representation to extract for the current layer and timestep.
% \ivan{Because of this per-element gating, I suspect that the case where $\text{nhead}=d_model$ in the context attention bottleneck will do best.}
% \ivan{Side thought: If the idea is to "extract" per element information from the hidden vector, should the bottleneck's objective be to "populate" per element information. Kinda like multihead attention where nheads=hidden size}