
\section{BARNEY} % Use case: Language Model Pretraining
\nikos{this first paragraph could be skipped, let's keep only the essential part the equation and the way the representations are selected from a pretrained model. would be good to start for Section 2}
The proposed autoencoder architecture can be used in tandem with any kind of transformer encoder network and can take advantage of pretrained language models such as BERT. In particular, we develop a new language model pretraining framework based on transformer autoencoders using BERT as an encoder function. Our framework is called \textbf{BARNEY} which stands for \textbf{B}ERT \textbf{A}ggregated \textbf{R}epresentations by Reduci\textbf{n}g and R\textbf{e}constructing Full\textbf{y}. The final fixed-length representation which can be used for downstream tasks is thus computed as follows:
$$\text{Enc}_\text{BARNEY}(\mathbf{x})=\text{CAB}(\text{BERT}(\mathbf{x})_0, \text{BERT}(\mathbf{x})_{1:})$$
 
Particularly, we set the context vector $u$ equal to the representation of the $\text{[CLS]}$ token, tasking the $\text{[CLS]}$ representation with learning task-specific context. In effect, recovers the $\text{[CLS]}$ representation at the $N+1$th layer, fully utilizing the $N$th layer representations and reducing the unnecessary computation at the $N$th layer when utilizing only the $\text{[CLS]}$ token.

The amount of decoder layers depends on the targeted downstream task. For classification tasks, a shallow 1 layer decoder would emphasize better encoder representations. For generation tasks, more decoder layers are preferred to adequately model the language.

% If one's goal is to use BARNEY for encoding or classification tasks, one would choose a shallow 1 layer decoder to emphasize better encoder representations. Alternatively, if one wants to use the 
% \ivan{This addresses the wasted computation when just using [CLS]}
% And then a transformer decoder is trained on top.
\subsection{Training objective}
\ivan{TODO: Condense this section. The training methods (normal, denoising, MLM+denoising can be consolidated) }\nikos{+1, do we make use all of these options? denoising vs normal etc? make sure we keep only the ones that we actually experiment with.}
%\nikos{We should describe this in detail and discuss it because it deviates completely from the training objective of bert (masked language modeling, next sentence prediction). } \ivan{Good point!}
Unlike the masked language modeling and next sentence prediction pre-training objectives of BERT, we introduce the unsupervised reconstruction objective by pairing \textbf{BARNEY} with a transformer decoder and train on the task of reconstructing its own input. Below are the methods that consider both fixed and trainable BERT:

\subsubsection{Fixed BERT}
One can use the fixed weights from BERT, and only finetune the bottleneck of the encoder with the decoder. Inspired by the objective of preserving the BERT representations, this method experiences speed benefits from not needing to backpropogate into BERT. The two methods this can be applied to are the following:

\paragraph{Normal}
Pretraining on only the reconstruction objective when encoding and decoding the input.

\paragraph{Denoising}
Passing in corrupted input and training on the objective of reconstructing the original, non-corrupted input, as denoising has been shown to improve autoencoder representations. \citep{vincent2008denoising}


\subsubsection{Trainable BERT}
In pretraining and other uses, one can consider training BARNEY end-to-end with BERT being trainable. The main appeal of this method is if one was to start from scratch, 

\paragraph{Normal/Denoising}
Both of the above Normal and Denoising methods can be used with a trainable BERT.

\paragraph{MLM + Denoising}\nikos{is this objective used in the experiments or not?}
One concern is during \textbf{BARNEY} pretraining with trainable BERT is that the BERT final representations become saturated. This method aims to alleviate this problem by using the masked langauge modeling input as the "corrupted" input, and the masked language modeling objective will be used on top of the BERT representations during training in conjunction with the reconstruction objective. This method can be done from scratch; MLM only updates a handful of tokens in each gradient step, whereas the autoencoding objective takes into account \textit{all} token representations, therefore making greater use of a single backpopogation step and enriching the tokens with sentence-level information.

% \paragraph{Fixed BERT}

% Same as the Normal method, except o
% \paragraph{Denoising, Fixed BERT}
% Same as the Denoising method, except one would use the fixed weights from BERT \\


% The non-fixed BERT methods can also be considered when starting pretraining from scratch, but we will leave that study for further work due to computational restraints. 

% One concern is during \textbf{BARNEY} pretraining is that the BERT final representations become saturated. Additionally, it has been shown that the denoising improves autoencoder representations \citep{vincent2008denoising}. As such, we experiment with using the masked language modeling (MLM) input for the encoder during training. We test out having the MLM objective, to prevent saturating the original BERT representations, and without, to train \textbf{BARNEY} as a denoising autoencoder.
 
\subsection{Finetuning on downstream tasks} \nikos{this section could be condensed quite a bit and be described in the experiments section. note also that we don't focus on two-sentence tasks here, the sentence below about them could go into a footnote or in the supplementary.}

\paragraph{Classification} One can finetune a linear classification head on top of the \textbf{BARNEY} final representation, with all parameters finetunable. In the scenario where multiple sentences are given as input, one could either input both into the model to take advantage of cross attention, or encode the sentences separately and finetune a linear layer on a combination of the sentence representations.

% we experiment with inputting the two sentences into \textbf{BARNEY} versus encoding each sentence separately then finetuning a linear classification head on top of a combination of the features.
\paragraph{Sentence Representations} 
\citet{conneau2017supervised} show that one can obtain universal sentence representations by finetuning on natural language inference data. One encodes both sentences separately with BARNEY to obtain representations $u$ and $v$, then finetune a linear classifier on the $[u, v, |u-v|]$ concatenation of the representations.


\paragraph{Generation}
A generation task that requires generating a sentence from a single vector representation can easily take advantage of the transformer decoder


% \paragraph{Variable-Length Outputs} While not trained explicitly for this, one can evaluate the underlying BERT model. We suspect that the MLM + reconstruction would do best


% \ivan{For classification it would be simply putting a linear classification head on top of \textbf{BARNEY}, and finetuning to the given task. We might need to discuss ideas about tasks where two input sentences are given}
% \nikos{The main challenge is when we have tasks that involve multiple inputs e.g. entailment or question answering. In the BERT/GPT papers they tried to minimize task-specific modifications by serializing them with delimiters to process all of them as a single sentence. Eventually, they use [CLS] token embedding for the final classification. Not sure if doing the same with our model makes much sense though. It's going to be difficult avoiding task-specific modifications. Perhaps we could project them into the latent space separately and then add/multiply/concat them together to a fixed vector.} \ivan{That's a very good point. Given the overparameterization of BERT, I was thinking we could \textit{continue} with the MLM task in addition to the reconstruction objective (I explain it below)}
