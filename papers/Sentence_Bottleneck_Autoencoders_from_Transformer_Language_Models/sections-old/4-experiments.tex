
\section{Experiments}\label{sec:experiments}
Here, we present BARNEY finetuning performance on several different tasks. \nikos{we need to be more specific here. what are the questions we would like to answer specifically or our hypothesis. make sure the points we make correspond to the claims that we make in the intro/abstract and are clearly stated.}
For fair comparison, we pretrain \textbf{BARNEY} on the same Wikipedia + BooksCorpus dataset from BERT. To avoid training from scratch we use the pretrained BERT model as a starting point for our encoder function. We consider several different methods of pretraining. \nikos{these sound like experimental details, I would mention the basic ones in a subsection here and defer the rest for the supplementary (make sure we include everything there).}


% \subsection{BARNEY Pretraining for Classification}
% We outline several methods to pretrain BARNEY, and perform experiments on the sentence similarity task (SST2) with RoBERTa-base BARNEY, to observe the relative performance on single-sentence classification.

% \input{tables/barney_training_sst}

\noindent \textbf{Datasets} \nikos{Mention some basic details about the datasets used here.}
 
\noindent \textbf{Model configuration} \nikos{Only the basic ones and defer to the supplementary for the details such as number of epochs, learning rate etc. (e.g. we follow configuration from ...)}

\noindent \textbf{Baselines} \nikos{Describe our baselines and the versions of our model that we examined.}

\subsection{GLUE}
The General Language Understanding Evaluation (GLUE) benchmark \citep{wang2018glue} is a collection of diverse natural language understanding text classification tasks. We evaluate BARNEY pretrained with the Denoising, Fixed methodology report its performance on the dev set of each task in Table 1. \\
We find that on multiple tasks BARNEY on the base models end up performing on par with the large models with a mere fraction of additional parameters and compute. 

% \ivan{Since these are classification tasks, I feel like BARNEY would have a good chance at beating BERT as a baseline. These classification task require backpropagation through BARNEY, in addition to the linear classification layer, in the finetuning process.}
% \nikos{We should definitely try this. I am curious to see how the autoencoder pretraining objective impacts the results. } 



% \subsection{SentEval}
% SentEval [cite] is an evaluation framework for fixed sentence embeddings on 17 downstream tasks. We follow a setup similar to \cite{Reimers2019SentenceBERT} by finetuning on a combination of Natural Language Inference and Semantic Textual Similarity training sets, then finetuning a linear regression head on the fixed representations for each task. We examine the perfomrance of both BARNEY and BERT + CAB.
% % \ivan{These are 17 downstream tasks that \textit{don't} backpropagate through the sentence encoder, but rather, evaluate the fixed-length sentence embeddings themselves. \url{https://github.com/facebookresearch/SentEval}. If we include BERT's [CLS] token in this framework, we definitely have a good shot here}
% \nikos{I don't believe that this evaluation will measure the full potential of the method since it's best on fixed embeddings. By the way some of these tasks overlap with GLUE where we plan to test anyway with finetuning (e.g.SST/MRPC). So, I'd rather suggest to look for some controlled generation task such as sentiment style transfer (e.g. like the one used by Shen et al 2020) to demonstrate the benefits for a real  generation task (other than the reconstruction tests below).}
% % \ivan{Gotcha. In SentenceBERT they test the fixed embeddings after finetuning to the NLI dataset, which I feel like a good table to include would be an extra row for our method on table 5: \url{https://arxiv.org/pdf/1908.10084.pdf}}


\subsection{Sentence Representations}
\citet{Reimers2019SentenceBERT} perform the methodology of \citet{conneau2017supervised} of finetuning on NLI by using simple pooling methods of BERT representations, such as mean, max, and cls, and evaluate their performance on sentence similarity task. We train and evaluate BARNEY with a similar setup and its unique context attention bottleneck to compare against their results.
 
% \citet{conneau2017supervised} show that one can obtain universal sentence representations by finetuning on natural langauge inference data. \citet{Reimers2019SentenceBERT} show how this methodology can be applied to pooled BERT representations to obtain such sentence representations. We compaire

\input{tables/pooling}

We find that using the context attention bottleneck provides significant gains over using the other simple pooling methods. We suspect is due to the bottleneck acting as "weighted pooling" by attending over all the final tokens, to compute the final representation rather than mean/max equally considering all tokens or cls considering the representations before the final layer.

\input{tables/sts}

Using this setup, we compare directly the performance of BARNEY to other models on the sentence similarity task, ones which have not been trained on STS data. We find that BARNEY ends up performing singificantly better, and ends up achieving SBERT-large level performance with significantly less parameters.


% \subsection{BARNEY Pretraining for Generation}
% We outline several methods to pretrain BARNEY, and perform experiments on the sentence similarity task (SST2) with RoBERTa-base BARNEY, to observe the relative performance on single-sentence classification.


\subsection{Unsupervised Style Transfer}
% sentiment style transfer (e.g. like the one used by Shen et al 2020)
To evaluate properties of the latent space of BARNEY, we perform the experiment of \citet{shen2019educating} were we compute a “sentiment vector” $v$ from 100 negative and positive sentences, and change the sentiment of a sentence by encoding it, adding a multiple of the sentiment vector to the sentence representation, then decoding the resulting representation. We observe how the accuracy, BLEU, and perplexity change as we add a larger multiple of the sentiment vector to the representation in Table 4.

% and use it to change the sentiment of the test sentences.

\input{tables/generation}

% \subsection{Reconstruction Quality}
% \ivan{Perhaps we should evaluate the reconstruction ability compared to other autoencoders? We could focus on just BooksCorpus, create our own test set, and test reconstruction quality. If TAE/BARNEY is really good, we could introduce EM as a metric}
% \nikos{ What do you mean by EM? I worry that we may not have the space for introducing a new evaluation too (we have new architecture + pretraining framework already). How about using the Yelp dataset to compare directly with Shen et al 2020 in their own setup? }


% \subsection{Latent Properties}
% \ivan{We could also show an example of encoding two sentences, then showing the decoding of the linear interpolation of between the two? Perhaps a 2D dim-red of the embeddings?} \nikos{Sure, that'd be great. E.g. like the ones here \url{https://arxiv.org/pdf/1511.06349.pdf}}



% \subsection{Multilingual}

% \subsection{Pretraining}
% \begin{itemize}
%     \item Train two models on the exact same data for the exact same amount of training steps. To simulate the same amount of parameters, use one extra layer for the MLM approach
%     \item Model 1: 7 layer, 512 hidden size transformer encoder trained on just the MLM objective
%     \item Model 2: 6 layer, 512 hidden size BARNEY trained on MLM objective in conjunction with reconstruction objective.
%     \item Show the down-stream MNLI performance difference after certain amounts of steps (could be a plot)
%     \item Show the downstream SQuAD (MNLI?) performance difference at the end of BARNEY wihtout the conetext attention bottleneck (see if the reconstruction objective helps with better token-level representations, since all tokens are updated each step, rather than only 15\% of them in MLM)
% \end{itemize}