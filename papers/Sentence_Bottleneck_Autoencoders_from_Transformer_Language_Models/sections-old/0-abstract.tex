
\begin{abstract} %  
Pretrained language models based on transformers have been shown to produce sub-optimal sentence representations for tasks requiring text similarity \cite{Reimers2019SentenceBERT}.
Previous methods that ameliorate this issue have introduced auxiliary sentence similarity objectives that rely on annotated data and compose sentence representations with non-parametric pooling that exhibits limited desirable properties and is specific to text similarity tasks.
To this end, we propose \textsc{Autobots}, a new class of text autoencoders with a sentence bottleneck derived from pretrained transformers on unlabeled data.
Our training objective is to to reconstruct sentences fully from a learned bottleneck representation as opposed to partially from variable-length representations in existing masked language modeling objectives.
We demonstrate that our resulting sentence representations outperform previous methods on text similarity tasks  and can also be used for controlled generation tasks such as style transfer.
Notably our method maintains the performance of the pretrained language model on other supervised  downstream tasks.
\nikos{new take on the abstract, please check. I tried to emphasize on the sentence BERT method to have a clear message about our goal of the paper. There are some nice points in the previous abstract that should probably be incorporated in the introduction but it would be crucial not to make big claims about pretraining in general since it would be hard to compete with big LM papers. }


%Methods for obtaining unsupervised language representations have gained much attention recently for their performance on downstream tasks, and have largely relied on partial reconstruction of the input though masked language modeling or next token prediction through language modeling. Autoencoders, trained on full reconstruction of the input through a single fixed length, bottlenecked representation, have not been extensively studied for pretraining sentence-level representations, especially with transformers.
%In this paper, we propose the \textit{Transformer Autoencoder} by converting the architecture of \citet{vaswani17} to an autoencoder that uses attention to bottleneck the input to a single, fixed vector from which the input can be fully reconstructed. Leveraging the encoder advancements of \citet{devlin-etal-2019-bert}, we introduce \textbf{BARNEY}, which stands for \textbf{B}ERT \textbf{A}ggregated \textbf{R}epresentations by Reduci\textbf{n}g and R\textbf{e}constructing Full\textbf{y}, and demonstrate the significant performance of this novel pretraining objective in downstream classification, sentence representation, reconstruction, and controlled generation tasks.

\end{abstract}

% Autoencoders have 

% Unsupervised pretraining has gained much attention recently 
% We propose the transformer

% Methods for obtaining unsupervised language representations have gained much attention recently for their performance on downstream tasks, and have largely relied on partial reconstruction of the input though masked language modeling or next token prediction through language modelling.

% Text autoencoders obtain an unsupervised sentence-level representation by bottlenecking with reconstruction abilities, but largely rely on recurreence in the decoder.

% In this paper, we propose the \textit{Transformer Autoencoder} model by converting the architecture of \citet{vaswani17} to an autoencoder that uses attention to bottleneck the input to a single, fixed vector from which the input can be fully reconstructed. Leveraging the encoder advancements of \citet{devlin-etal-2019-bert}, we introduce textbf{BARNEY}, which stands for \textbf{B}ERT \textbf{A}ggregated \textbf{R}epresentations by Reduci\textbf{n}g and R\textbf{e}constructing Full\textbf{y}, to demonstrate the performance of this novel pretraining objective in downstream classification, sentence representation, reconstruction, and controlled genration tasks.

% by leveraging the encoder advancements of \citet{devlin-etal-2019-bert} in a transformer auto


% effectively enriching the resulting sentence representations with reconstructive properties. Concretely, we introduce a context attention bottleneck after the original encoder, and a modify the decoder to perform full reconstruction of the input conditioned on the bottleneck representation.
 
% Currently langauge representat
% 
 

% VERSION 1:

% % to learn 
% % \textit{BERT isn't an autoencoder, but BARNEY is!}

% Autoencoders have gained much attention for their unsupervised ability to reduce variable length sequences to a single latent space with reconstruction capabilities, yet current architectures rely heavily on recurrence. Similar to autoencoders, current state-of-the-art pretrained transformer models produce variable-length token represenations by learning to partially reconstruct their input, yet rely on a special token to produce sentence representations for most downstream natural language understanding tasks requiring a single vector.


% % Current state-of-the-art pretrained language models produce variable-length token representations by learning to partially reconstruct the input, and rely on a special token to produce sentence representations for most downstream natural language understanding tasks requiring a single vector. 
% %\nikos{we may want to mention that it's not possible to produce the full sentence based on cls alone} 
% %\nikos{the token level representations are used for word-level tasks though, so we have to make sure we don't overclaim here.}

% In this paper, we propose the \textit{Transformer Autoencoder} model that results from  converting the architecture of \citet{vaswani17} to an autoencoder that projects the input to a single, fixed vector from which the input can be fully reconstructed, effectively enriching the resulting sentence representations with reconstructive properties. 
% Concretely, we introduce a context attention bottleneck after the original encoder, and a modify the decoder to perform full reconstruction of the input %(instead of partial) 
% conditioned on the bottleneck representation.
% %to the "encoder-decoder" layer in the decoder stacks.
% %
% %We preserve the parallelism in the original transformer by analyzing dot-product attention's decomposition in the many-to-one and one-to-many settings. \nikos{Not sure I fully get this point. }
% %
% %\textbf{TAE} can be pre-trained on unlabeled corpora to produce a single sequence-level representation. \nikos{this point could be skipped}

% % Lastly, we develop an example framework called \textbf{BARNEY} based on transformer autoencoders for pretraining akin to BERT \citet{devlin-etal-2019-bert}
% Lastly, we introduce \textbf{BARNEY}, which stands for \textbf{B}ERT \textbf{A}ggregated \textbf{R}epresentations by Reduci\textbf{n}g and R\textbf{e}constructing Full\textbf{y}, which both leverages advancements of \citet{devlin-etal-2019-bert} and introduces a novel pretraining objective for transformers. We evaluate its sentence representations on downstream classification, reconstruction and controlled generation tasks.\footnote{Our code will be made available at [link]} 


% %, which stands for \textbf{B}idirectional \textbf{A}ggregated Transformer \textbf{R}epresentations by Reduci\textbf{n}g and R\textbf{e}constructing Full\textbf{y}. \nikos{is it ok if we skip the abbreviations until we decide the proper framing?}
% % and  we evaluate its sentence representations on downstream classification, reconstruction and controlled generation tasks.\footnote{Our code will be made available at [link]} 