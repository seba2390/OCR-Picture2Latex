{\color{red}TODOS
\begin{itemize}
    \item Nikos: Take a pass thorugh abstract, introduction, transformer autoencoders, and BARNEY sections for quick feedback/throughts/quick edits \nikos{done}
    \item Ivan: Finalize tables, experiments, and figures, then work on text
    \item Ivan $\&$ Nikos: Related Work \nikos{let's incorporate everything in the introduction. dedicating a whole section for this does not make much sense for a short paper.}
    \item Ivan: Implement Nikos's feedback and improve first draft
    \item Ivan: Shorten to 4 pages for EMNLP short paper submission
\end{itemize}
}

\section{Introduction} 
\nikos{need to convert to full text}
{\textit{Contributors}}: Ivan, Nikos, Noah

\vspace{3mm} 
\noindent \textit{Significance:} Recently, research has focused on devising new pretraining objectives for transformer-like architectures all involve some form of  language modeling (masked, generalized, etc) with much success for downstream tasks. Surprisingly, the direction of creating a complete autoencoder-like pretraining objective for transformers  still remains unexplored. We suspect this is mainly due to the variable-length nature of decoders in the original encoder-decoder transformer architecture by \citet{vaswani17}.

\begin{figure}[ht]
\centering
\hspace{-2mm}\includegraphics[width=0.5\textwidth]{figures/fig.png}
\caption{\ivan{TODO: Better quality diagram}\nikos{+1, I would keep it minimal and relatively small to save space e.g. even simpler than \url{https://paperswithcode.com/media/methods/Autoencoder_schema.png} perhaps with some indication that the encoder is a pretrained transformer.}}
% \caption{\label{fig:overview} We propose a novel task setup to multilingual question answering. A high resource language (HRL) question-answer database is leveraged by mapping a low resource language (LRL) query to a query in the database, then generating the LRL answer from the HRL answer.} % We examine using multilingual paraphrase detection in exploring the most reliable method to transfer knowledge from multilingual queries to English.
\end{figure}


\noindent \textit{Problem}:
\nikos{here we need also to highlight the issue that was identified for sentence representations extracted from BERT in the SBERT paper and cite several studies that focus on sentence representations. By the way, do we have any comparison with other methods than SBERT? In that paper, they compare with other methods. Could we also compare with those?}
Most pretrained language models rely on a special token to produce sentence representations for downstream classification tasks (BERT, ELECTRA) which does not guarantee perfect reconstruction of the input, a property that could be useful for tasks where autoencoders are useful such as controlled generation.
Moreover, they produce variable-length token representations which are often not needed for the downstream tasks and are potentially redundant. 
Additionally, during masked language modeling, only select tokens are updated each gradient step, encompassing wasted computation. To our knowledge, transformers decoders have not been explored in the context of a single encoder representation as input.
% \ivan{The [CLS] token, for BERT, is trained on the NSP task, which isn't sufficient for single-sentence representations. Additionally, in RoBERTa, they simply drop the NSP task altogether}

%\ivan{The two goals of this architecture: Improve BERT pretrained sentence representations, and provide a novel autoencoder (with better reconstruction properties)} 
\vspace{3mm} 

\noindent \textit{Questions}:
\begin{enumerate}
    \item Can we train BERT with a bottleneck representation from which the full input can be reconstructed?
    \item How well will such representations perform on different tasks compared to the [CLS] token from BERT?
    \item Will this autoencoder produce a latent representation that will do better on controlled generation tasks?
    \item Can we obtain better token representations from BERT when pretraining on both the MLM and reconstruction objective?
    %\item What do we want to make the scope of this paper be?
    %\item Will this autoencoder do well in multilingual settings?
\end{enumerate}

\vspace{3mm}

\noindent \textit{Method}
We introduce a context attention bottleneck for the encoder of \citet{vaswani17}, and an alternative decoder trained to perform full reconstruction of the input which is conditioned on the bottleneck representation.
As a use case we develop a language model pretraining framework based on our autoencoders akin to BERT. 

\vspace{3mm} 

\noindent \textit{Evaluation} We plan to evaluate on natural language understanding tasks from GLUE benchmark, on universal sentence representation tasks, and on autoencoder-related tasks related to reconstruction and  controlled generation such as style transfer or unconditional text generation. \nikos{let's make sure we include all the experimental details in the supplementary material (following emnlp guidelines) plus qualitative examples for all tasks that we examine.}
 %\ivan{Perhaps on reconstruction ability as well?}

% \textit{Plan:}
% \begin{enumerate}
%     \item Classification tasks: We will primarily look into the GLUE framework to see what improvements BARNEY offers over BERT for classification, and see (using NLI/STS) its performance at creating sentence representations.
%     \item Generation tasks: We will evaluate on the Shen et. al. task to evaluate the generation capabilities of BARNEY and how it does as an autoencoder.
% \end{enumerate}

% \\
% \ivan{TODO List: \begin{enumerate}
%     \item Discuss the method more
%     \item Show the tables we would like to express, and what results we want to see.
% \end{enumerate}}
%\section{Context}
% Attention is All You Need \\
% RNN Autoencoder: \\
% Can get metrics in a multilingual setting (LASER): https://arxiv.org/pdf/1812.10464.pdf \\
% BERT: \\

% Can evaluate on all the tasks for reconstruction in emb2emb, and add classification on top. Emb2emb already contains everyting needed to evaluate

% Tasks can be multilingual (use multilingual BERT, eval against LASER)
