\section{Ablation Study}


\subsection{Autoencoding Steps}

\input{tables/pretraining_steps}

\subsection{Finetunable BERT Layers}

\input{tables/finetunable_layers}



% \section{Ablation Study}
% We outline several methods to pretrain BARNEY, and perform experiments on the sentence similarity task (SST2) and reconstruction objective with RoBERTa-base BARNEY, to observe the relative performance on single-sentence classification.


% \input{tables/barney_training_ablation}

% For classification, we observe that... \ivan{Preliminary experiments show Denoising, Fixed performs best}

% For generation, we observe that....

% \ivan{The following are parameters to do ablation studies on their effect on classification and generation: The different training methods, the number of decoder layers, the amount of attention heads in the bottleneck, the effect of the gating method}


% \subsection{Context Attention Bottleneck Heads}
% To evaluate the importance of the number of heads in the context attention bottleneck, we finetune BERT with the Context Attention Bottleneck on a combination of the SNLI (\url{https://www.aclweb.org/anthology/D15-1075.pdf}) and Multi-Genre NLI (\url{https://www.aclweb.org/anthology/N18-1101.pdf}) datasets and evaluate the the Spearman correlation of the cosine similarity between sentence embeddings on the STS benchmark test set

% \subsection{Semantic Textual Similarity}
% \ivan{Here, we can run the semantic textual similarity benchmark as in the SentenceBERT paper (Train on AllNLI, eval on STS) \url{https://arxiv.org/pdf/1908.10084.pdf}, first by just plugging in our bottleneck and comparing scores to show our bottleneck is better than simply summing/averaging/maxing the contextualized representations. We can then show a plot on how (for a fixed seed) the score changes as we vary the amount of heads in the bottleneck}
% \begin{table}[h!]
%     \centering
%     \begin{tabular}{c|c}
%         \textbf{Model} & \textbf{Spearman}  \\
%         \hline
%         Avg. GloVe embeddings & 58.02  \\
%         Avg. BERT embeddings & 46.35  \\
%         InferSent - GloVe & 68.03  \\
%         Universal Sentence Encoder & 74.92 \\
%         SBERT-NLI-base & 77.03 \\
%         SBERT-NLI-large & 79.23 \\
%         BARNEY-bert-base h=6 & TBD \\
%         BARNEY-bert-base h=12 & TBD \\
%         BARNEY-bert-base h=24 & 0.7423 \\
%         BARNEY-bert-base h=48 & 0.7400 \\
%         BARNEY-bert-base h=96 & 0.7357 \\
%         BARNEY-bert-base h=192 & 0.7389 \\
%         BARNEY-bert-base h=384 & 0.7421 \\
%         BARNEY-bert-base h=768 & 0.7343 \\
%         BARNEY-bert-large h=X & TBD \\
%     \end{tabular}
%     \caption{Evaluation of the Spearman correlation of the cosine similarity of sentence embeddings on the STS benchmark test set for models trained on the combination of the SNLI (\url{https://www.aclweb.org/anthology/D15-1075.pdf}) and Multi-Genre NLI (\url{https://www.aclweb.org/anthology/N18-1101.pdf}) dataset}
%     \label{tab:offsetablation}
% \end{table}
% \ivan{The above can be reduced to a plot}



% \ivan{They present results for "train NLI, eval STS", "train STS, eval STS", "train STS+NLI, eval STS"}

% \section{Results}
% Some preliminary training plots for training on 320k common crawl sentences: \\
% \textbf{TAE}: The following was obtained by using the base parameters specified in the original Attention is All You Need paper, along with the inverse square root learning rate annealing, and a hidden representation of 512. \\
% \includegraphics[width=.375\textwidth]{TAE.png} \\
% \textbf{BARNEY}: The following was obtained by using BERT (base-uncased) as the transformer encoder, projecting to a hidden representation of 512, and using a decoder identical to TAE. \\
% \includegraphics[width=.375\textwidth]{BARNEY.png} \\
