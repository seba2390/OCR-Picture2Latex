% {\color{red}TODOS
% \begin{itemize}
%     \item Ivan: Finalize tables, experiments, and figures, then work on text
%     \item Ivan $\&$ Nikos: Related Work \nikos{let's incorporate everything in the introduction. dedicating a whole section for this does not make much sense for a short paper.}
% \end{itemize}
% }



\section{Introduction} 

%\nikos{need to convert to full text}
%{\textit{Contributors}}: Ivan, Nikos, Noah

%\vspace{3mm} 
%\noindent \textit{Significance:} 
Recent research has focused on devising new unsupervised pretraining methods from unlabeled data that involves some form of language modeling, primarily autoregressive \citep{peters-etal-2018-deep,radford2019language}, masked \citep{devlin-etal-2019-bert, liu2019RoBERTa, conneau2020unsupervised} and generalized \citep{radford2019language, brown2020language,pmlr-v97-song19d}, with much success on downstream tasks.  Under the hood, most of these methods use transformers \citep{vaswani17} for encoding text sequences, which allows them to learn powerful contextual word representations that have been used widely for building models in NLP. However, this  does not hold for sentence representations derived from pretrained transformer language models based on a special token or basic pooling operations.
% In fact, previous studies have shown  that such sentence representations are sub-optimal for sentence similarity and  classification tasks \nascomment{add citation}.
To this end, representation learning methods have been designed to better capture semantic information from pretrained transformer language models, e.g., using Siamese networks trained with a triplet loss \cite{Reimers2019SentenceBERT} or transforming the desired sentence distribution to a Gaussian distribution through normalizing flows \cite{li-etal-2020-sentence}. 

Existing sentence representations directly derived from pretrained language models or learned by specialized methods cannot guarantee perfect reconstruction of the input, a property that can enhance the structure of their semantic space and enable their use for controlled generation tasks. For the latter, a few recent studies have looked into ways to steer generation of pretrained language models towards a particular style \citep{Dathathri2020Plug,krause2021gedi}, although they require following the gradient during the sampling process and rely on style text classifiers which might not be always available. 
% Other work has explored properties of autoencoder representations from scratch \citep{zhang2018learning}, but end up discarding the decoder after training.
The latent space of a text autoencoder allows one to perform controlled text generation by directly manipulating  sentence representations using basic numerical operations \cite{pmlr-v119-shen20c}. Yet, how to convert pretrained transformer language models to autoencoders with such properties still remains unexplored. 

%as variable-length denoising has been explored \citep{lewis2019bart}. 
%Surprisingly, the direction of creating a complete autoencoder-like pretraining objective for transformers, where intermediate representation is of fixed size, still remains largely unexplored. 
% \citet{lewis2019bart} perform denoising, but accept a variable length hidden size from the encoder to the decoder. 
%\noindent \textit{Problem}:
% \nikos{here we need also to highlight the issue that was identified for sentence representations extracted from BERT in the SBERT paper and cite several studies that focus on sentence representations. By the way, do we have any comparison with other methods than SBERT? In that paper, they compare with other methods. Could we also compare with those?}
% (BERT, ELECTRA)
%Moreover, they produce variable-length token representations which are often not needed for the downstream tasks and are potentially redundant. 
%Additionally, during masked language modeling, only select tokens are updated each gradient step, encompassing wasted computation.
%To our knowledge, transformers decoders have not been explored in the context of a single encoder representation as input. \\
% \ivan{The [CLS] token, for BERT, is trained on the NSP task, which isn't sufficient for single-sentence representations. Additionally, in RoBERTa, they simply drop the NSP task altogether}

%\ivan{The two goals of this architecture: Improve BERT pretrained sentence representations, and provide a novel autoencoder (with better reconstruction properties)} 
% \vspace{3mm} 

% \noindent \textit{Questions}:
% \begin{enumerate}
%     \item Can we train BERT with a bottleneck representation from which the full input can be reconstructed?
%     \item How well will such representations perform on different tasks compared to the [CLS] token from BERT?
%     \item Will this autoencoder produce a latent representation that will do better on controlled generation tasks?
%     \item Can we obtain better token representations from BERT when pretraining on both the MLM and reconstruction objective?
%     %\item What do we want to make the scope of this paper be?
%     %\item Will this autoencoder do well in multilingual settings?
% \end{enumerate}

% \vspace{3mm}

%\noindent \textit{Method}:
% \nikos{a class of models for...} 
To fill in this gap, we introduce \textsc{Autobot}, a new autoencoder model for learning sentence
% \nascomment{I thought we should explain the term on first use -- ok?}
``bottleneck'' (i.e., fixed-size) representations from pretrained transformers that is useful for similarity,  generation, and classification, displayed in Figure~\ref{fig:model}. Our model has two unique components: (i) a  transformation that uses dot product attention to dynamically pool semantic information from the pretrained model's hidden states into a sentence bottleneck representation, and (ii)  a shallow transformer decoder that is modified to operate based on the bottleneck representation. Instead of training our autoencoder from scratch, we directly finetune it using an input reconstruction objective on the unlabeled data on which the original pretrained transformer was trained. We keep the underlying pretrained transformer encoder fixed, which makes it more efficient than training from scratch
% \nascomment{reworked here, sentence was too long and some parts were misplaced}
% The additional computational overhead is 
%\nikos{can we make a comment here about the efficiency of this finetuning approach? how long does it take?} 
% \nikos{marginal? or mention percentage if available}
% marginal \nascomment{make quantitative} \nikos{number of steps divided by the number of steps in Roberta = ~20\%, we could say it takes 1/5 of the time it takes to train from scratch.} \ivan{part of the consideration here is that we don't need to back propgate into Roberta during finetuning, which is where the most speed gain we get comes from}
% compared to the total  time to required to pretrain
% \nascomment{pretrain?}
%  letâ€™s remove marginal and mention that is more efficient than training from scratch because main model is kept frozen
% from scratch 
and proves beneficial even if we compare to pretrained transformers trained for an equal number of steps. 

Our evaluation on representative sentence similarity, classification, and generation tasks demonstrates that the   resulting sentence representations are compact, better capture semantic similarity at the sentence-level than strong sentence representation methods \cite{Reimers2019SentenceBERT}, and can be used for controlled generation tasks.  Lastly, our model performs almost on par with the large RoBERTa model \citep{liu2019RoBERTa} even though it only introduces 1.6$\%$ additional
% \nascomment{make quantitative, e.g., a percentage} 
parameters relative to the base RoBERTa model. 

% \nikos{what about the experiments where we finetune some of the layers of the pretrained encoder? please clarify. } \ivan{they were only used in generation (figure 2), but it's a question if we want to include them or not (and make everything strictly using a fixed encoder).The current setup of the paper doesn't not include them}

% \nikos{ is it always the case that the pretrained encoder is fixed and what do we gain from this objective? }
% \textsc{autobots} use these components to perform full reconstruction of the input with a single, fixed length 
% which together are trained to perform full reconstruction of the input which is conditioned on the bottleneck representation.
% We fix the under
% As a use case we develop a language model pretraining framework based on our autoencoders akin to BERT. 
 
%\noindent \textit{Evaluation} 

% \nikos{let's make sure we include all the experimental details in the supplementary material (following emnlp guidelines) plus qualitative examples for all tasks that we examine.}
 %\ivan{Perhaps on reconstruction ability as well?}
 
%  \nikos{these contributions could be merged with the second to last paragraph above.}
% \noindent We summarize our contributions as:
% \begin{itemize}
%   \item \textsc{AUTOBOTS}: A new class of text autoencoders with a sentence bottleneck derived from pretrained transformers
%   \item An analysis of \textsc{AUTOBOTS} performance gains on single-sentence representation, generation, and classification tasks.
% \end{itemize} 

% \nikos{Please cite / discuss some of the most relevant ones in the end of second paragraph (before "To our knowledge").}
% \ivan{Papers to incorporate into introduction, mostly on autoencoding} \\

% - \url{https://www.ijcai.org/Proceedings/2019/0727.pdf}\\
% - \url{https://arxiv.org/pdf/1809.11155.pdf} \\
% - \url{https://arxiv.org/pdf/1905.02450.pdf} \\
% - funnel \url{https://arxiv.org/pdf/2006.03236.pdf} \\

% - \url{https://arxiv.org/pdf/1506.02216.pdf} \\
% - \url{https://arxiv.org/pdf/1812.10464.pdf} \\
% \nikos{ These are some recent transformer papers related to autoencoders (none of them focuses on pretraining):} \\



% \textit{Plan:}
% \begin{enumerate}
%     \item Classification tasks: We will primarily look into the GLUE framework to see what improvements BARNEY offers over BERT for classification, and see (using NLI/STS) its performance at creating sentence representations.
%     \item Generation tasks: We will evaluate on the Shen et. al. task to evaluate the generation capabilities of BARNEY and how it does as an autoencoder.
% \end{enumerate}

% \\
% \ivan{TODO List: \begin{enumerate}
%     \item Discuss the method more
%     \item Show the tables we would like to express, and what results we want to see.
% \end{enumerate}}
%\section{Context}
% Attention is All You Need \\
% RNN Autoencoder: \\
% Can get metrics in a multilingual setting (LASER): https://arxiv.org/pdf/1812.10464.pdf \\
% BERT: \\

% Can evaluate on all the tasks for reconstruction in emb2emb, and add classification on top. Emb2emb already contains everyting needed to evaluate

% Tasks can be multilingual (use multilingual BERT, eval against LASER)
