
\section{Experiments}\label{sec:experiments}
To assess the quality of the sentence representations learned by our model we evaluate on sentence similarity (Section \ref{sec:sim}), classification (Section \ref{sec:clf}), and generation tasks (Section \ref{sec:gen}).

%\nikos{we need to be more specific here. what are the questions we would like to answer specifically or our hypothesis. make sure the points we make correspond to the claims that we make in the intro/abstract and are clearly stated.}
% For fair comparison, we pretrain \textsc{Autobots} on the same Wikipedia + BooksCorpus dataset from BERT. To avoid training from scratch we use the pretrained BERT model as a starting point for our encoder function. We consider several different methods of pretraining. \nikos{these sound like experimental details, I would mention the basic ones in a subsection here and defer the rest for the supplementary (make sure we include everything there).}

\subsection{Settings} 
%\nikos{Add some basic details here. What data do we use + citation, where do we pretrain RoBERTa and for how long? fixed vs finetuning upper layers enc etc}
\paragraph{Datasets}
Since the RoBERTa dataset is not publicly available, we use for pretraining the exact same dataset as BERT \citep{devlin-etal-2019-bert}, which is composed of BooksCorpus \citep{zhu2015aligning} and English Wikipedia. For sentence similarity, we use the Natural Language Inference (NLI) dataset \citep{bowman2015snli} for finetuning and evaluate on the Semantic Textual Similarity (STS) dataset \citep{cer2017semeval}, following \citet{conneau2017supervised}.  For classification, we use mainly single-sentence datasets from the GLUE benchmark \citep{wang2018glue}, namely Stanford Sentiment Treebank (SST) and Corpus of Linguistic Acceptability (CoLA) datasets, but we also report the average performance on the remaining datasets. For generation, we use the Yelp reviews dataset \citep{shen2017style}.

% \nikos{we need to clarify if we use the exact same dataset and why. is it a subset of it or not?} \nikos{let's also mention here the details about datasets we use to evaluate each of the tasks for similarity, classsification and generation.}



% BooksCorpus (800M words) (Zhu et al.,
% 2015) and English Wikipedia (2,500M words). 
\paragraph{Baselines}
%\nikos{Mention which sentence representation methods we compare to (basic pooled, SBERT, etc) for each task.} 
For sentence similarity, we compare to SBERT which is a competitive method for deriving informative sentence representations from pretrained language models \cite{Reimers2019SentenceBERT}.
% \footnote{Note that, in contrast, our model does not require additional data during the pretraining phase. }
They obtain sentence representations by using simple pooling methods over BERT representations such as mean and max (instead of the CLS token representation) then finetuning the whole pretrained model using Siamese networks on a combination of natural language inference data.  To compare with them on sentence similarity, we incorporate our model within their framework and follow their settings and training/evaluation protocol (details in Appendix~\ref{adx:sentrep}). 

For sentence classification, we compare our model to RoBERTa-base and RoBERTa-large models \cite{liu2019RoBERTa}. Note that BART \cite{lewis2019bart} achieves similar results to RoBERTa, so a similar comparison can be made.  % They achieve sentence representations by encoding the premise $x$ and hypothesis $y$ sentences separately to get $\mathbf{z_x}$ and $\mathbf{z_y}$ respectively, then finetune a linear classification layer with softmax loss over the concatenation of $[\mathbf{z_x};\mathbf{z_y};|\mathbf{z_x}−\mathbf{z_y}|]$. \nikos{I feel this part can be skipped, is it that important to mention it in detail? Mentioned a high-level view of this.}

For sentence generation tasks, we compare to a strong and efficient style transfer method by \citet{shen2019educating}, which is a recurrent network-based denoising text autoencoder on in domain data. The style transfer is achieved  through vector arithmetic, namely computing a “sentiment vector” $\mathbf{v}$ by taking the vector difference between 100 negative and positive sentences, then evaluating by taking an input sentence, encoding it, adding a multiple of the sentiment vector to the sentence representation, then decoding the resulting representation. In addition to the denoising auto encoder (DAE) of \citet{shen2019educating}, we include more sophisticated methods for style transfer that are more computationally expensive such as fast gradient iterative modification (FGIM) of \citet{wang2019controllable} and Emb2Emb of \citet{mai2020plug} for reference.
 

% \nikos{Make clear on which tasks we finetune the whole model and which ones RoBERTa model is kept fixed. This is not clear at all up to this point.} \ivan{they were only used in generation (figure 2), but it's a question if we want to include them or not (and make everything strictly using a fixed encoder). The current setup of the paper doesn't not include them} \nikos{I meant when finetuning on a downstream task. regarding finetuning with the autoencoding objective we still need to mention what options we consider; hmm, yes let's describe everything and we decide after. }

% \subsection{BARNEY Pretraining for Classification}
% We outline several methods to pretrain BARNEY, and perform experiments on the sentence similarity task (SST2) with RoBERTa-base BARNEY, to observe the relative performance on single-sentence classification.

% \input{tables/barney_training_sst}

% \noindent \textbf{Datasets} \nikos{Mention some basic details about the datasets used here.}
% In our experiments
% NLI, GLUE, STS
 
% \noindent \textbf{Model configuration} \nikos{Only the basic ones and defer to the supplementary for the details such as number of epochs, learning rate etc. (e.g. we follow configuration from ...)}

% \noindent \textbf{Baselines} \nikos{Describe our baselines and the versions of our model that we examined.}



% \noindent \textbf{Datasets} \nikos{Mention some basic details about the datasets used here.}
% In our experiments
% NLI, GLUE, STS
 
% \noindent \textbf{Model configuration} \nikos{Only the basic ones and defer to the supplementary for the details such as number of epochs, learning rate etc. (e.g. we follow configuration from ...)}

% \noindent \textbf{Baselines} \nikos{Describe our baselines and the versions of our model that we examined.}


% \ivan{For each experiment, I will follow your Datasets, Model Configuration, Baselines format in the description}

\subsection{Sentence Similarity} \label{sec:sim}
%\nikos{you could mention the more general idea behind experiments in the first paragraph of this section and point to subsections (e.g. that we evaluate the representation and sentence generation quality of sentence representations extracted from pretrained models.}
% {
%\citet{conneau2017supervised} show that universal sentence representations can be obtained from finetuning on the NLI training data and evaluating the model's efficacy using the STS benchmark data. \nikos{Mentioned in the dataset section?} 

% The spearmen
 The results on the sentence similarity task are displayed in Table~\ref{tab:nli_sts}.
%  \nascomment{this reference is wrong, it refers to a section not a table; probably the label command isn't inside the table's caption?}. 
 Due to resource constraints and unreported results by prior work, we report our model only with RoBERTa-base.
 We can observe that \textsc{Autobot} applied to RoBERTa-base significantly outperforms other supervised base transformer methods. Additionally,  \textsc{Autobot} approaches the performance of large  transformers while having a minimal   parameter   overhead of 1.6\%.    % \nascomment{make this quantitative}. \nikos{done}
%  \ivan{finish}
% \nascomment{maybe footnote this:}

We also find that \textsc{Autobot} without any supervision (\textsc{Autobot}-base unsup.) outperforms all of the unsupervised methods, and most notably improves upon average BERT embeddings by 26.1\%. This demonstrates that our approach is effective in both supervised and unsupervised settings.
%  \nikos{here, we should describe the findings for two settings supervised and unsupervised. currently, there is no discussion of the result in the unsupervised setting. }
% \nascomment{this is strange wording; do you mean that previous work didn't report it?}, we report our  model only applied to RoBERTa-base and leave the hyperparameter search required for large models to future work.
 
%  \nascomment{tables are perhaps ordered wrongly?  in the table currently numbered 2, mark which system(s) are ours.}
 
\input{tables/sts}   \input{tables/pooling}
 
% We directly compare to the results of \citet{Reimers2019SentenceBERT} as well as 
% }\ivan{Finish
% We directly compare against finetuning the original pretrained versions of BERT and RoBERTa, as well as \citet{Reimers2019SentenceBERT}'s 
% \citet{Reimers2019SentenceBERT} perform the methodology of  by using simple pooling methods of BERT representations, such as mean, max, and cls, and evaluate their performance on sentence similarity task. We train and evaluate \textsc{Autobots} with a similar setup and its unique context attention bottleneck to compare against their results.
% \citet{conneau2017supervised} show that one can obtain universal sentence representations by finetuning on natural langauge inference data. \citet{Reimers2019SentenceBERT} show how this methodology can be applied to pooled BERT representations to obtain such sentence representations. We compaire
We find in Table~\ref{tab:pooling} that using the proposed sentence bottleneck based on learned context  provides 
% \nascomment{do not use this word if you're not doing hypothesis tests} significant 
noticeable gains over using simpler pooling methods from prior work. We suspect this is due to the additional flexibility provided by our bottleneck acting as ``weighted pooling'' by attending over all tokens to compute the final representation, as opposed to equal contribution of all tokens regardless of the input. 




% We also find it maintains same relative performance in the large transformer models. Due to resource constraints, we use SBERT's defaults leave further hyperparameter optimization of the large transformer models to future work.

% we compare directly the performance of \textsc{Autobots} to other models on the sentence similarity task, ones which have not been trained on STS data. We find that \textsc{Autobots} ends up performing singificantly better, and ends up achieving SBERT-large level performance with significantly less parameters.


\subsection{Sentence Classification} \label{sec:clf}

The results on single-sentence classification tasks and other tasks from the GLUE benchmark are displayed in Table \ref{tab:glue}. We find that \textsc{Autobot} provides a 
% \nascomment{do not use this word if there's no test} significant
noticable performance increase on single-sentence tasks, specifically on the CoLA datasets when using both the RoBERTa-base and RoBERTa-large models.
% This demonstrates that our model is effective regardless \nascomment{too strong; we only tested two model sizes!} of the pretrained language model's size used.
Additionally, we also find that \textsc{Autobot}, when fed both sentences concatenated for dual sentence GLUE tasks, maintains the original performance of the underlying pretrained encoder. \input{tables/glue} Hence, our model improves the quality of the sentence representations from pretrained transformer models without hurting their performance.  

%The GLUE benchmark is a collection of diverse natural language understanding text classification, and involves both single and dual sentence tasks. 
%We fully finetune \textsc{autobots} single sentence tasks and compare performance with \citet{liu2019RoBERTa} in Table~\ref{tab:glue}. \nikos{moved them above}

% We evaluate \textsc{Autobots} pretrained with the Denoising, Fixed methodology report its performance on the dev set of each task in Table 1. \\
% We find that on multiple tasks \textsc{Autobots} on the base models end up performing on par with the large models with a mere fraction of additional parameters and compute. 



% \ivan{describe finetuning: fully finetuned autobots}
% \ivan{Should we include results where the final layers of the pretrained model are trainable? It would make figure 2 a bit more crowded}\nikos{let's add them in the supplementary section.}

% \ivan{Since these are classification tasks, I feel like BARNEY would have a good chance at beating BERT as a baseline. These classification task require backpropagation through BARNEY, in addition to the linear classification layer, in the finetuning process.}
% \nikos{We should definitely try this. I am curious to see how the autoencoder pretraining objective impacts the results. } 



% \subsection{SentEval}
% SentEval [cite] is an evaluation framework for fixed sentence embeddings on 17 downstream tasks. We follow a setup similar to \cite{Reimers2019SentenceBERT} by finetuning on a combination of Natural Language Inference and Semantic Textual Similarity training sets, then finetuning a linear regression head on the fixed representations for each task. We examine the perfomrance of both BARNEY and BERT + CAB.
% % \ivan{These are 17 downstream tasks that \textit{don't} backpropagate through the sentence encoder, but rather, evaluate the fixed-length sentence embeddings themselves. \url{https://github.com/facebookresearch/SentEval}. If we include BERT's [CLS] token in this framework, we definitely have a good shot here}
% \nikos{I don't believe that this evaluation will measure the full potential of the method since it's best on fixed embeddings. By the way some of these tasks overlap with GLUE where we plan to test anyway with finetuning (e.g.SST/MRPC). So, I'd rather suggest to look for some controlled generation task such as sentiment style transfer (e.g. like the one used by Shen et al 2020) to demonstrate the benefits for a real  generation task (other than the reconstruction tests below).}
% % \ivan{Gotcha. In SentenceBERT they test the fixed embeddings after finetuning to the NLI dataset, which I feel like a good table to include would be an extra row for our method on table 5: \url{https://arxiv.org/pdf/1908.10084.pdf}}


% \subsection{BARNEY Pretraining for Generation}
% We outline several methods to pretrain BARNEY, and perform experiments on the sentence similarity task (SST2) with RoBERTa-base BARNEY, to observe the relative performance on single-sentence classification.





% \subsection{Multilingual}

% \subsection{Pretraining}
% \begin{itemize}
%     \item Train two models on the exact same data for the exact same amount of training steps. To simulate the same amount of parameters, use one extra layer for the MLM approach
%     \item Model 1: 7 layer, 512 hidden size transformer encoder trained on just the MLM objective
%     \item Model 2: 6 layer, 512 hidden size BARNEY trained on MLM objective in conjunction with reconstruction objective.
%     \item Show the down-stream MNLI performance difference after certain amounts of steps (could be a plot)
%     \item Show the downstream SQuAD (MNLI?) performance difference at the end of BARNEY wihtout the conetext attention bottleneck (see if the reconstruction objective helps with better token-level representations, since all tokens are updated each step, rather than only 15\% of them in MLM)
% \end{itemize}

\subsection{Sentence Generation} \label{sec:gen}
% sentiment style transfer (e.g. like the one used by Shen et al 2020)
%\citet{mikolov2013linguistic} previously discovered that embeddings from unsupervised learning can capture linguistic relationships via simple arithmetic. \nikos{let's keep this for the longer version?}

% A canonical example is the embedding arithmetic “King” - “Man” + “Woman” $≈$ “Queen”.

%To evaluate latent space properties of autoencoders, \citet{shen2019educating} propose evaluating unsupervised style transfer through vector arithmetic computing a “sentiment vector” $v$ by taking the vector difference between 100 negative and positive sentences, then evaluating by taking an input sentence, encoding it, adding a multiple of the sentiment vector to the sentence representation, then decoding the resulting representation. \nikos{Moving it up}

% \nascomment{missing number in lower right of table 3 is not explained}

For sentence generation, we focus on the sentiment transfer task proposed by \citet{shen2019educating} both with and without further training on in-domain data from Yelp. When finetuning, we perform an additional 10K optimization steps using the Yelp dataset.  Note that all the baselines require training on in-domain data, while this is optional for our model.  In Figure~\ref{fig:generation}
% \nascomment{reference goes to a section not a figure}
we find that the \textsc{Autobot} model not exposed to the Yelp dataset during finetuning performed on par with the 
% \nascomment{DAE is not explained, either in the text or the figure -- reader doesn't know what it is}
DAE that was trained specifically on Yelp. Additionally, \textsc{Autobot} outperforms the DAE in the above-40 percent accuracy range when finetuned on in-domain data.
% \nascomment{what follows is a bit too vague.  either explain more fully or cut} Lastly, we additionally find that with partial finetuning of the pretrained transformer encoder \textsc{Autobot} achieves higher BLEU for a given accuracy, which we defer to the appendix.
We include \textsc{autobot} results with partial finetuning of the encoder in the appendix, which we find considerably improves the Self-BLEU metric.
% \nascomment{there's no supplementary ... I think we referred to it as appendix before?}.
Since \textsc{AUTOBOT} uses vector arithmetic, inference is as fast as the DAE and over twice that of other methods.
% \nikos{say something about speed compared to previous methods}
% and compare directly with We compare directly with \citet{shen2019educating} and their denoising autoencoder (DAE) trained on Yelp, and include more sophisiticated methods for style transfer such as fast gradient iterative modification (FGIM) of \citet{wang2019controllable} and Emb2Emb of \citet{mai2020plug} for reference \citet{shen2019educating} and their denoising autoencoder (DAE) trained on Yelp. We evaluate \textsc{autobots}
% wang2019controllable

% , and explore the effect of making several last layers of the pretrained transformer trainable during autoencoding. 

% To evaluate properties of the latent space of \textsc{Autobots}, we perform the experiment of \citet{shen2019educating} were we compute a “sentiment vector” $v$ from 100 negative and positive sentences, and change the sentiment of a sentence by encoding it, adding a multiple of the sentiment vector to the sentence representation, then decoding the resulting representation.

% We plot the sentiment classification accuracy versus self-BLEU curve as we vary the multiple of the sentiment vector added used in .


% FGIM \citep{wang2019controllable} fast gradient iterative modification


% We also observe that when more of the final layers of the pretrained encoder are allowed to be finetuned with the autoencoding objective, \textsc{autobots} obtain higher Self-BLEU in the sub-90 percent accuracy range.

% \ivan{describe finetuning: fixed autobots evaluated}
% and use it to change the sentiment of the test sentences.

% \input{tables/generation}

% \subsection{Reconstruction Quality}
% \ivan{Perhaps we should evaluate the reconstruction ability compared to other autoencoders? We could focus on just BooksCorpus, create our own test set, and test reconstruction quality. If TAE/BARNEY is really good, we could introduce EM as a metric}
% \nikos{ What do you mean by EM? I worry that we may not have the space for introducing a new evaluation too (we have new architecture + pretraining framework already). How about using the Yelp dataset to compare directly with Shen et al 2020 in their own setup? }


% \subsection{Latent Properties}
% \ivan{We could also show an example of encoding two sentences, then showing the decoding of the linear interpolation of between the two? Perhaps a 2D dim-red of the embeddings?} \nikos{Sure, that'd be great. E.g. like the ones here \url{https://arxiv.org/pdf/1511.06349.pdf}}


\vspace{-3mm}
\begin{figure}[ht]
\centering
\hspace{-2mm}\includegraphics[width=0.49\textwidth]{figures/generation_more_points.png}
\caption{ 
% \nikos{it'd be nice also to add some points from prior work (pnp paper models)?} \ivan{I'll try finding and adding some} 
% \nikos{great!} \nikos{the fontsize on legends, xticks, yticks, xlabel, ylabel still look quite small. }
% \nikos{could we remove two of the autobot models to make the diagram more clear? it'd be nice also to add some points from prior work (pnp paper models)? no need to have a line for these others. could you me the markers equal size and a bit bigger? I'd use a solid or dotted line without markers for the baseline.}
% \nikos{please also make font for the axes and their titles bigger. I'd also remove the top title from the figure, caption is sufficient and you save up space.}
Automatic evaluations of vector arithmetic for sentiment transfer, plotted as accuracy vs. self-BLEU. Accuracy (ACC) is measured by a sentiment classifier, and values for varying multiples of the sentiment vector are plotted. Upper right is better.
% \ivan{We also have examples where the last 3 layers of the pretrained transformer were allowed to be finetuned} \nikos{these results are without finetuning? yeah if it is very crowded we should include the ones with finetuning in the supplementary.  }\nikos{can we increase the font sizes a bit more and the thickness of the lines? I'd use dashes instead of dots for the baseline with thicker lines because it looks more distinctive.}
% \nikos{add efficiency column that compares speed, add more tables from pnp autoencoder paper. remove ppl if the results are not calculated with the same vocab}
\label{fig:generation}
}
\end{figure}