\label{sec:appendix}

\section{Reproducibility}
% We first describe the experimental details that are common to all experiments. Dataset-specific choices are then listed in their respective subsections.

\subsection{Experimental Setup}

\paragraph{Computing Infrastructure}
For all of our experiments involving base models, we use a computation cluster with 5 NVIDIA RTX 2080 TI GPU, 11GB GPU memory, and 128GB RAM. For large models, we use a computation cluster with 4 NVIDIA TITAN RTX GPUs, 24GB GPU memory and 256GB RAM.

\paragraph{Implementation}
We will make our implementation available on Github.\footnote{\url{https://github.com/ivanmontero/autobot}} We used Python 3.7, PyTorch  1.6.0, and Sentence Transformers 0.3.7. We use modified versions of Fairseq 0.9.0 and Transformers 3.3.1. We obtain our datasets from the citations specified in the main paper.


\paragraph{\textsc{Autobot} Training}\label{adx:autobottraining}
We extract the sentences from the BooksCorpus and English Wikipedia datasets to recreate the BERT dataset, and use RoBERTa-base's pretrained tokenizer for tokenization. We report our hyperparameters for \textsc{Autobot}-base in Table~\ref{tab:autobotbase}. Our decoder only has one single layer, and RoBERTa-base remains fixed during finetuning.


\begin{table}[h]
    \small
    \centering
    \begin{tabular}{ll}
        \toprule
        \textsc{Model Parameters} & \textsc{Value} \\
        \midrule
        \textbf{Fixed Parameters} & {} \\
        \midrule
        Transformer Encoder & RoBERTa-base \\
        Transformer Encoder Fixed & True \\
        Warmup Steps & 4000 \\
        Dropout & 0.1 \\
        Optimizer & Adam \\
        Learning Rate Schedule & Linear Decay \\
        Max Sequence Length & 128 \\
        Max Tokens & 24576 \\
        Bottleneck Heads & 12 \\
        Hidden Size & 768 \\
        Decoder Layers & 1 \\
        \midrule
        \textbf{Tuned Parameters} & {} \\
        \midrule
        Num Steps & $\{1k, 10k, 100k\}$ \\
        Learning Rate & $\{$1e-3, 1e-4, 1e-5$\}$ \\
        \midrule
        \textbf{Optimal Parameters} & {} \\
        \midrule
        Num Steps & $100k$ \\
        Learning Rate & $1e-3$ \\
        \midrule
        \textbf{Extra Info} & {} \\
        \midrule
        Model Size (\# params) & $127M$ \\
        % Vocab Size & 250,002 \\
        \bottomrule
    \end{tabular}
    % \end{center}
    % \end{small}
    \caption{\label{tab:autobotbase}Hyperparamters for \textsc{AUTOBOT}-base}

\end{table}


\subsection{Sentence Representations}\label{adx:sentrep}

We use the Sentence Transformers framework for training and evaluation of \textsc{Autobot}. We use the default settings in their framework to train on NLI, and evaluate using the Spearman correlation of the cosine similarity. During NLI finetuning, we only use the encoder and bottlneck, with the bottleneck representation used as the sentence representation, and allow for all parameters to be finetuned.

\subsection{Sentence Generation}

We use a modified version of Fairseq's generation code for encoder-decoder models to perform vector arithmetic for sentiment transfer. We follow the instructions of \citet{mai2020plug} to finetune a sentiment classifier using DistilBERT from the Huggingface transformers library.

For the \textsc{Autobot} models finetuned to the Yelp dataset, we follow the exact same steps as Appendix~\ref{adx:autobottraining} except beginning with the \textsc{Autobot}-base model, using the Yelp training set, and performing 10k optimization steps.

\subsection{Sentence Classification}

We use the Huggingface library to perform sentence classification using \textsc{Autobot}.  During finetuning, we only use the encoder and bottleneck, with the bottleneck representation used as a \texttt{CLS} representation, and allow for all parameters to be finetuned. We perform a hyperparameter search similar to that of RoBERTa by comparing development performances when using $\{$1e-5, 2e-5, 3e-5$\}$ for the learning rate.

% \begin{table}[h]
% \small
% \centering
% % \begin{small}
% % \begin{center}
% \begin{tabular}{ll}
% \toprule
% \textsc{Model Parameters} & \textsc{Value} \\
% \midrule
% Model & Roberta-base \\
% Num Steps & 100k \\
% Warmup Steps & 4000 \\
% Dropout & 0.1 \\
% Optimizer & Adam \\
% Learning Rate Schedule & Linear Decay \\
% Max Sequence Length & 128 \\
% Max Tokens & 24576 \\
% Learning Rate & $1e-3$ \\
% \midrule
% Extra Info & {} \\
% \midrule
% Model Size (\# params) & $127M$ \\
% % Vocab Size & 250,002 \\
% \bottomrule
% \end{tabular}
% % \end{center}
% % \end{small}
% \caption{\label{tab:ceparams}TODO}
% % \textbf{Cross Encoder Hyperparameter Selection And Tuning Ranges} The hyper parameters we chose and searched over for XLM-Roberta large on the query paraphrase detection datasets.
% % }
% \end{table}


\section{Additional Results}
We provide additional results in addition to our experiments below.

\subsection{Autoencoding Steps}

We perform an ablation study on the effect of autoencoding finetuning steps of the underlying pretrained encoder during autoencoding on the downstream sentence representation performance. We provide the detailed performances of performing Table~\ref{tab:autobotbase} when using a learning rate of 1e-3 in Table~\ref{tab:autosteps}.

\begin{table}[ht!]
	\centering 
	\footnotesize
	\begin{tabular}{ l | c }
		\toprule
		\textbf{Training Steps} & \textbf{Spearman} \\ \midrule
		1 & 74.38 \\
		1k & 75.45 \\
		10k & 78.01 \\
		100k & \textbf{78.59}  \\
		\hline
		baseline & 77.03 \\
        \bottomrule
	\end{tabular}
	\caption{\label{tab:autosteps}\textsc{Autobot} pretraining steps vs. sentence representation performance when training on NLI and evaluating on STS}
	
\end{table}


\subsection{Finetunable Encoder Layers}\label{adx:finetune}
We perform an ablation study on the effect of finetuning the underlying pretrained encoder during autoencoding on downstream sentence representation performance. We provide the detailed performances of performing Table~\ref{tab:autobotbase} with the optimal parameters, but varying how many of the last layers of RoBERTa-base to finetune. Results are in Table~\ref{tab:finetune}

\begin{table}[ht!]
	\centering 
	\footnotesize

	\begin{tabular}{ l | c }
		\toprule
		\textbf{Finetunable Layers} & \textbf{Spearman} \\ \midrule
		None & \textbf{78.59} \\
		1 & 77.24 \\
		2 & 76.17 \\
		3 & 76.20 \\
		\hline
		baseline & 77.03 \\
        \bottomrule
	\end{tabular}
	\caption{\label{tab:finetune} \textsc{Autobot} finetunable layers vs. sentence representation performance when training on NLI and evaluating on STS}
	
\end{table} 


\subsection{Finetunable Encoder Generation}
We provide an appended generation table from Section~\ref{sec:gen} to include the generation results we obtained by allowing the top three layers of RoBERTa-base to be finetuned during autoencoding on the style generation task. The results are shown in Figure~\ref{fig:gen_plus_finetune}. The same model as used in Appendix~\ref{adx:finetune} is used.
% \ivan{TODO: description}
\begin{figure}[ht]
\centering
\hspace{-2mm}\includegraphics[width=0.49\textwidth]{figures/generation_more_points_appendix.png} 
\caption{\label{fig:gen_plus_finetune} % \nikos{new take}
Automatic evaluations of vector arithmetic for sentiment transfer, plotted as accuracy vs. self-BLEU. Accuracy is measured by a sentiment classifier, and values for varying multiples of the sentiment vector are plotted. Upper right is better.
}
% \nikos{+1, I would keep it minimal and relatively small to save space e.g. even simpler than \url{https://paperswithcode.com/media/methods/Autoencoder_schema.png} perhaps with some indication that the encoder is a pretrained transformer.}}
% \caption{\label{fig:overview} We propose a novel task setup to multilingual question answering. A high resource language (HRL) question-answer database is leveraged by mapping a low resource language (LRL) query to a query in the database, then generating the LRL answer from the HRL answer.} % We examine using multilingual paraphrase detection in exploring the most reliable method to transfer knowledge from multilingual queries to English.
\end{figure}

\subsection{Style Transfer Results}
We provide Table~\ref{tab:pnp} that reports results on the Yelp sentiment transfer test set from the generation table in Section~\ref{sec:gen}, appending to the table \citep{mai2020plug}. We outline the relative time differences during inference. We can observe that our model not only provides competitive speed-quality tradeoff.

\begin{table}[ht!]
    \centering
    \footnotesize
  \def\arraystretch{1.1}\tabcolsep=3.5pt
    \begin{tabular}{l|r|r|r}
        \toprule
        \textbf{Model} & \textbf{Acc.} & \textbf{BLEU} & \textbf{+Time} \\
        \toprule
        \hline
        FGIM & 94.9 & 10.8 & 70.0$\times$ \\
        Mai et al. 2020 + FGIM & 93.1 & 18.1 & 2820.0$\times$ \\ 
        Mai et al. 2020 & 87.1 & 22.1  & 1.0$\times$ \\ \hline
        Shen et al. (2019)&  96.8 & 6.5 & 0.5$\times$ \\ 
        AUTOBOT-base (ours) & 95.6 & 11.90 & 0.5$\times$\\\bottomrule
    \end{tabular}\vspace{-1mm}
    \caption{Self BLEU on the Yelp sentiment transfer test set
    %\nascomment{in Figure~\ref{fig:sentiment-transfer}?} 
    with highest transfer accuracy (``Acc.'').  %\nascomment{check:} 
    ``+Time'' reports the inference-time slowdown factor due to each method's additional computation relative to the method by \citet{mai2020plug}.%\nikos{why not to ours?}\florian{fixed that}
    }\vspace{-3mm}
    \label{tab:pnp}
\end{table}  

\subsection{Detailed Sentence Classification Results}
Section~\ref{sec:clf} provides a summary of the GLUE results, while outlining the specific single-sentence classification performances. We provide the results for each task in Table~\ref{tab:roberta_glue}

% \subsection{Style Transfer Examples}
% We provide specific examples from the evaluation in Section~\ref{sec:gen} comparing \textsc{Autobot}-base (finetuned to Yelp) to \cite{shen2019educating} and show how the output evolves as the multiplier increases. The examples are shown in tables \ref{tab:ex0},  \ref{tab:ex1}, and \ref{tab:ex2}


\begin{table*}[ht]
\centering
\begin{tabular}{lccccccccc}
\toprule
& \bf MNLI & \bf QNLI & \bf QQP & \bf RTE & \bf SST & \bf MRPC & \bf CoLA & \bf STS \\
\midrule 
% \multicolumn{10}{l}{\textit{Transformer Models}}\\
% Base &  &  &  &  &  &  &  &  &  \\
% \quad + Masked &  &  &  &  &  &  &  &  &  \\
% \quad + MLM &  &  &  &  &  &  &  &  &  \\
% Large &  &  &  &  &  &  &  &  &  \\
% \quad + Masked &  &  &  &  &  &  &  &  &  \\
% \quad + MLM &  &  &  &  &  &  &  &  &  \\
% \midrule 
% \multicolumn{9}{l}{\textit{Base Models}}\\
% 84.4 88.4 86.7 92.7
% $\text{BERT}$  & 84.3 & 88.4 &  &  & 92.7 & 86.7 &  &  &  \\
% $\text{BARNEY}_\text{BERT}$ &  &  &  &  &  &  &  &  &  \\
$\text{RoBERTa-base}$ & 87.6 & 92.8 & 91.9 & 78.7 & 94.8 & 90.2 & 63.6 & 91.2 \\
$\text{AUTOBOT-base}$ & 88.0 & 92.7 & 91.9 & 79.5 & 95.0 & 88.4 & 66.0 & 91.4 \\
% 88.0 & 92.7 & 91.9 & 79.5 & 88.4 &91.4
\midrule 
% \multicolumn{9}{l}{\textit{Large Models}}\\
% $\text{BERT}$ & 86.6 & 92.3 & 91.3 & 70.4 & 93.2 & 88.0 & 60.6 & 90.0 &  -\\
% $\text{BARNEY}_\text{BERT}$ &  &  &  &  &  &  &  &  &  \\
$\text{RoBERTa-large}$ & 90.2 & 94.7 & 92.2 & 86.6 & 96.4 & 90.9 & 68.0 & 92.4 \\
$\text{AUTOBOT-large}$ & 90.5 & 94.6 & 92.2 & 87.6 & 96.9 & 89.0 & 70.2 & 92.4 \\
\bottomrule
\end{tabular} % 87.6 92.8 91.9 78.7 94.8 90.2 63.6 91.2
\caption{
Dev.~results on GLUE.
% Results on GLUE. All results are based on a 24-layer architecture.
% \bertlarge{} and \xlnetlarge{} results are from \newcite{devlin2018bert} and \newcite{yang2019xlnet}, respectively.
% \ourmodel{} results on the development set are a median over five runs.
% \ourmodel{} results on the test set are ensembles of \emph{single-task} models.
% \ivan{Perhaps reduce this table to only show the single-sentence results? (MNLI, SST, CoLA) And note results unchanged for others} \nikos{good idea, let's keep the results for other non sentence similarity tasks to support the point that performance is largely maintained on the other tasks (unclear if this is the case with sentence bert).}
For RTE, STS and MRPC we finetune starting from the MNLI model instead of the baseline pretrained model.}

 

% Averages are obtained from the GLUE leaderboard.
\label{tab:roberta_glue}
\end{table*}

% roberta-base:  (87.6+92.8+91.9+78.7+90.2+91.2)/6 = 88.7
% roberta-large: (90.2+94.7+92.2+86.6+90.9+92.2)/6 = 91.1

% For all reported experimental results:

% A clear description of the mathematical setting, algorithm, and/or model
% A link to a downloadable source code, with specification of all dependencies, including external libraries (recommended for camera ready)
% A description of computing infrastructure used
% The average runtime for each model or algorithm, or estimated energy cost
% The number of parameters in each model
% Corresponding validation performance for each reported test result
% A clear definition of the specific evaluation measure or statistics used to report results.

% For all results involving multiple experiments, such as hyperparameter search:
% The exact number of training and evaluation runs
% The bounds for each hyperparameter
% The hyperparameter configurations for best-performing models
% The method of choosing hyperparameter values (e.g., manual tuning, uniform sampling, etc.) and the criterion used to select among them (e.g., accuracy)
% Summary statistics of the results (e.g., mean, variance, error bars, etc.)

% For all datasets used:
% Relevant statistics such as number of examples and label distributions
% Details of train/validation/test splits
% An explanation of any data that were excluded, and all pre-processing steps
% For natural language data, the name of the language(s)
% A link to a downloadable version of the dataset or simulation environment
% For new data collected, a complete description of the data collection process, such as instructions to annotators and methods for quality control







% \begin{table}[h]
% \small
% \centering
% % \begin{small}
% % \begin{center}
% \begin{tabular}{ll}
% \toprule
% \textsc{Model Parameters} & \textsc{Value/Range} \\
% \midrule
% \textbf{Fixed Parameters} & {} \\
% \midrule
% Model & Roberta-base \\
% Num Steps & 100k \\
% Warmup Steps & 4000 \\
% Dropout & 0.1 \\
% Optimizer & Adam \\
% Learning Rate Schedule & Linear Decay \\
% Max Sequence Length & 128 \\
% \midrule
% \textbf{Tuned Parameters} & {} \\
% \midrule
% Batch Size & [8, 120] \\
% Learning Rate & [$9e-4$, $1e-6$] \\
% \midrule
% \textbf{Extra Info} & {} \\
% \midrule
% Model Size (\# params) & $550M$ \\
% Vocab Size & 250,002 \\
% Trials & 30 \\
% \bottomrule
% \end{tabular}
% % \end{center}
% % \end{small}
% \caption{\label{tab:ceparams}TODO}
% % \textbf{Cross Encoder Hyperparameter Selection And Tuning Ranges} The hyper parameters we chose and searched over for XLM-Roberta large on the query paraphrase detection datasets.
% % }
% \end{table}



% \begin{table*}[ht]
%     \centering
%     \begin{tabulary}{\textwidth}{|C
%     |L|L|}
%     \hline
%     multiplier & Shen et al. (2019) & \textsc{Autobot}-base \\
%     \hline
%     1.5 & i will be back . &  i will definitely be back . \\
%     \hline
%     2.0 & i will be back back &  i will be back . \\
%      \hline
%     2.5 &  i will definitely be back . &  i will be back . \\
%      \hline
%     3.0 & i love this place ! & i will be back . \\
%     \hline
%     \end{tabulary}
%     \caption{\textbf{Input}: i will never be back .}
%     \label{tab:ex0}
% \end{table*}

% \begin{table*}[ht]
%     \centering
%     \begin{tabulary}{\textwidth}{|C|L|L|}
%     \hline
%     multiplier & Shen et al. (2019) & \textsc{Autobot}-base \\
%     \hline
%     1.5 & the cash area was great and the the best staff & the front counter counter was friendly and there was the store available . \\
%     \hline
%     2.0 & the cash register area was empty and no one was watching the store front . & the front counter counter was fresh and the store is great . \\
%      \hline
%     2.5 & the cash bar area was great and no one was the friendly staff . & the food area was fresh and the shop is great and friendly . \\
%      \hline
%     3.0 & the great noda area and great and wonderful staff . & the food area was fresh and the shop is great and friendly . \\
%     \hline
%     \end{tabulary}
%     \caption{\textbf{Input}: the cash register area was empty and no one was watching the store front . \textbf{Reference}: the store front was well attended}
%     \label{tab:ex1}
% \end{table*}

% \begin{table*}[]
%     \centering
%     \begin{tabulary}{\textwidth}{|C|L|L|}
%     \hline
%     multiplier & Shen et al. (2019) & \textsc{Autobot}-base \\
%     \hline
%       1.5 & definitely disappointed that i 'm not my birthday ! & def initely happy that i enjoyed my dinner menu ! \\
%     \hline
%      2.0 & definitely disappointed that i have a great ! & i highly recommend that i had a great lunch and happy meal ! \\
%      \hline
%      2.5 & definitely super disappointed and i 'll definitely have a great gift ! & i enjoyed my dinner and i can enjoy it ! \\
%      \hline
%      3.0 & definitely delicious and i love the ! & i enjoyed my dinner and i can enjoy it ! \\
%     \hline
%     \end{tabulary}
%     \caption{\textbf{Input}: definitely disappointed that i could not use my birthday gift ! \textbf{Reference}: definitely not disappointed that i could use my birthday gift !}
%     \label{tab:ex2}
% \end{table*}
