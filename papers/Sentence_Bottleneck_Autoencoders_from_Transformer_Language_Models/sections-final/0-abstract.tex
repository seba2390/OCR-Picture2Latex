
\begin{abstract} %  


% Revised version from Noah's version
Representation learning for text via pretraining a language model on a large corpus has become a standard starting point for building NLP systems.  This approach stands in contrast to autoencoders, also trained on raw text, but with the objective of learning to encode each input as a vector that allows full reconstruction.  Autoencoders are attractive because of their latent space structure and generative properties. We therefore explore the construction of a sentence-level autoencoder from a pretrained, frozen transformer language model. We adapt the masked language modeling objective as a generative, denoising one, while only training a sentence bottleneck and a single-layer modified transformer decoder.
We demonstrate that the sentence representations discovered by our model achieve better quality than previous methods that extract representations from pretrained transformers on text similarity tasks, style transfer (an example of controlled generation), and single-sentence classification tasks in the GLUE benchmark, while using fewer parameters than large pretrained models.\footnote{Our code is available at: \url{https://github.com/ivanmontero/autobot}} 



% % Noah's 5/10 version with comments
% Representation learning for text via pretraining a language model on a large corpus has become a standard starting point for building NLP systems.  This approach stands in contrast to autoencoders, also trained on raw text, but with the objective of learning to encode each inputs as a vector that allows full reconstruction.  Autoencoders are attractive because of their generative latent space properties. We therefore explore the construction of a sentence-level autoencoder from a pretrained, frozen transformer language model. We adapt the masked language modeling objective as a generative, denosing one, We demonstrate that the sentence representations discovered by our method are a \nascomment{nail down claims about effiency, it's not clear to me right now} and perform competitively with the state of the art \nascomment{check that claim!} on text similarity tasks, style transfer (an example of controlled generation), and sentence classification tasks in the GLUE benchmark.

% \nascomment{fill this in}. (to keep computational cost low) 

% \nascomment{one thing not clear from the paper and possibly worth raising even here:  what data gets used to learn the autoencoder?  also, we need to nail down terminology.  in some places you say that the autoencoder is ``pretrained'' but I think that's confusing since we start from a pretrained model.  do we want to say that we finetune the autoencoder?} 


% old version commented out 5/10 by Noah
%Methods for obtaining unsupervised language representations have gained much attention recently for their performance on downstream tasks, and have largely relied on partial reconstruction of the input though masked language modeling or next token prediction through language modeling.
%Autoencoders, trained on full reconstruction of the input through a single fixed length, bottlenecked representation, have not been extensively studied for pretraining sentence-level representations, especially with transformers.
%To this end, we propose \textsc{Autobots}, a new class of text autoencoders with a sentence bottleneck derived from pretrained transformers on unlabeled data.
%Our training objective is to reconstruct sentences fully from a learned bottleneck representation while keeping the underlying pretrained model fixed.
%We demonstrate that the resulting sentence representations outperform previous methods on text similarity tasks while being parameter efficient, and can also be used for controlled generation tasks such as style transfer.
%Notably our method maintains the performance of the pretrained language model on other supervised  downstream tasks.







% Pretrained language models based on transformers have been shown to produce sub-optimal sentence representations for tasks requiring text similarity \cite{Reimers2019SentenceBERT}.
% Previous methods that ameliorate this issue have introduced auxiliary sentence similarity objectives that rely on annotated data and compose sentence representations with non-parametric pooling that exhibits limited desirable properties and is specific to text similarity tasks.
% To this end, we propose \textsc{Autobots}, a new class of text autoencoders with a sentence bottleneck derived from pretrained transformers on unlabeled data.
% Our training objective is to to reconstruct sentences fully from a learned bottleneck representation as opposed to partially from variable-length representations in existing masked language modeling objectives.
% We demonstrate that our resulting sentence representations outperform previous methods on text similarity tasks while being parameter efficient, and can also be used for controlled generation tasks such as style transfer.
% Notably our method maintains the performance of the pretrained language model on other supervised  downstream tasks.

% \nikos{new take on the abstract, please check. I tried to emphasize on the sentence BERT method to have a clear message about our goal of the paper. There are some nice points in the previous abstract that should probably be incorporated in the introduction but it would be crucial not to make big claims about pretraining in general since it would be hard to compete with big LM papers. }


% ===== nikos =====
% Pretrained language models based on transformers have been shown to produce sub-optimal sentence representations for tasks requiring text similarity \cite{Reimers2019SentenceBERT}.
% Previous methods that ameliorate this issue have introduced auxiliary sentence similarity objectives that rely on annotated data and compose sentence representations with non-parametric pooling that exhibits limited desirable properties and is specific to text similarity tasks.
% To this end, we propose \textsc{Autobots}, a new class of text autoencoders with a sentence bottleneck derived from pretrained transformers on unlabeled data.
% Our training objective is to to reconstruct sentences fully from a learned bottleneck representation as opposed to partially from variable-length representations in existing masked language modeling objectives.
% We demonstrate that our resulting sentence representations outperform previous methods on text similarity tasks while being parameter efficient, and can also be used for controlled generation tasks such as style transfer.
% Notably our method maintains the performance of the pretrained language model on other supervised  downstream tasks.
% =================





% Methods for obtaining unsupervised language representations have gained much attention recently for their performance on downstream tasks, and have largely relied on partial reconstruction of the input though masked language modeling or next token prediction through language modeling. Autoencoders, trained on full reconstruction of the input through a single fixed length, bottlenecked representation, have not been extensively studied for pretraining sentence-level representations, especially with transformers.
%In this paper, we propose the \textit{Transformer Autoencoder} by converting the architecture of \citet{vaswani17} to an autoencoder that uses attention to bottleneck the input to a single, fixed vector from which the input can be fully reconstructed. Leveraging the encoder advancements of \citet{devlin-etal-2019-bert}, we introduce \textbf{BARNEY}, which stands for \textbf{B}ERT \textbf{A}ggregated \textbf{R}epresentations by Reduci\textbf{n}g and R\textbf{e}constructing Full\textbf{y}, and demonstrate the significant performance of this novel pretraining objective in downstream classification, sentence representation, reconstruction, and controlled generation tasks.

\end{abstract}

% Autoencoders have 

% Unsupervised pretraining has gained much attention recently 
% We propose the transformer

% Methods for obtaining unsupervised language representations have gained much attention recently for their performance on downstream tasks, and have largely relied on partial reconstruction of the input though masked language modeling or next token prediction through language modelling.

% Text autoencoders obtain an unsupervised sentence-level representation by bottlenecking with reconstruction abilities, but largely rely on recurreence in the decoder.

% In this paper, we propose the \textit{Transformer Autoencoder} model by converting the architecture of \citet{vaswani17} to an autoencoder that uses attention to bottleneck the input to a single, fixed vector from which the input can be fully reconstructed. Leveraging the encoder advancements of \citet{devlin-etal-2019-bert}, we introduce textbf{BARNEY}, which stands for \textbf{B}ERT \textbf{A}ggregated \textbf{R}epresentations by Reduci\textbf{n}g and R\textbf{e}constructing Full\textbf{y}, to demonstrate the performance of this novel pretraining objective in downstream classification, sentence representation, reconstruction, and controlled genration tasks.

% by leveraging the encoder advancements of \citet{devlin-etal-2019-bert} in a transformer auto


% effectively enriching the resulting sentence representations with reconstructive properties. Concretely, we introduce a context attention bottleneck after the original encoder, and a modify the decoder to perform full reconstruction of the input conditioned on the bottleneck representation.
 
% Currently langauge representat
% 
 

% VERSION 1:

% % to learn 
% % \textit{BERT isn't an autoencoder, but BARNEY is!}

% Autoencoders have gained much attention for their unsupervised ability to reduce variable length sequences to a single latent space with reconstruction capabilities, yet current architectures rely heavily on recurrence. Similar to autoencoders, current state-of-the-art pretrained transformer models produce variable-length token represenations by learning to partially reconstruct their input, yet rely on a special token to produce sentence representations for most downstream natural language understanding tasks requiring a single vector.


% % Current state-of-the-art pretrained language models produce variable-length token representations by learning to partially reconstruct the input, and rely on a special token to produce sentence representations for most downstream natural language understanding tasks requiring a single vector. 
% %\nikos{we may want to mention that it's not possible to produce the full sentence based on cls alone} 
% %\nikos{the token level representations are used for word-level tasks though, so we have to make sure we don't overclaim here.}

% In this paper, we propose the \textit{Transformer Autoencoder} model that results from  converting the architecture of \citet{vaswani17} to an autoencoder that projects the input to a single, fixed vector from which the input can be fully reconstructed, effectively enriching the resulting sentence representations with reconstructive properties. 
% Concretely, we introduce a context attention bottleneck after the original encoder, and a modify the decoder to perform full reconstruction of the input %(instead of partial) 
% conditioned on the bottleneck representation.
% %to the "encoder-decoder" layer in the decoder stacks.
% %
% %We preserve the parallelism in the original transformer by analyzing dot-product attention's decomposition in the many-to-one and one-to-many settings. \nikos{Not sure I fully get this point. }
% %
% %\textbf{TAE} can be pre-trained on unlabeled corpora to produce a single sequence-level representation. \nikos{this point could be skipped}

% % Lastly, we develop an example framework called \textbf{BARNEY} based on transformer autoencoders for pretraining akin to BERT \citet{devlin-etal-2019-bert}
% Lastly, we introduce \textbf{BARNEY}, which stands for \textbf{B}ERT \textbf{A}ggregated \textbf{R}epresentations by Reduci\textbf{n}g and R\textbf{e}constructing Full\textbf{y}, which both leverages advancements of \citet{devlin-etal-2019-bert} and introduces a novel pretraining objective for transformers. We evaluate its sentence representations on downstream classification, reconstruction and controlled generation tasks.\footnote{Our code will be made available at [link]} 


% %, which stands for \textbf{B}idirectional \textbf{A}ggregated Transformer \textbf{R}epresentations by Reduci\textbf{n}g and R\textbf{e}constructing Full\textbf{y}. \nikos{is it ok if we skip the abbreviations until we decide the proper framing?}
% % and  we evaluate its sentence representations on downstream classification, reconstruction and controlled generation tasks.\footnote{Our code will be made available at [link]} 