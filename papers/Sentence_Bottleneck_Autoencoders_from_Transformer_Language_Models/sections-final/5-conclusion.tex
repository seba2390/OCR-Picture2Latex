\section{Conclusion}
% \ivan{make a conclusion}
%In this paper, we present \textsc{Autobots}, the construction of a sentence-level autoencoder from a pretrained, frozen transformer language model that adapts the masked language modeling objective as a generative, denoising one.
We proposed an approach that converts a pretrained transformer language model into a  
% \nascomment{I think ``standalone'' might be better than ``fully fledged''?  though I'm not sure standalone is accurate because we still need Roberta to do the encoding} \nikos{[right. perhaps, we could say "standard" or avoid specifying}
sentence-level autoencoder that is able to reconstruct its pretraining data. The resulting model improves the performance of the pretrained model on sentence-level tasks while maintaining its performance on multi-sentence tasks. In addition, the new sentence representations are suitable for efficient conditional text generation such as sentiment transfer without the need for training on in-domain data. 
% We, there-fore,  explore  the  construction  of  a  sentence-level  autoencoder  from  a  pretrained,  frozentransformer  language  model.    We  adapt  themasked language modeling objective as a gen-erative,  denoising  one,  while  only  training  asentence  bottleneck  and  a  single-layer  modi-fied  transformer  decoder.



% In this paper, we present Emb2Emb, a framework
% that reduces conditional text generation tasks to
% learning in the embedding space of a pretrained
% autoencoder. We propose an adversarial method
% and a neural architecture that are crucial for our
% methodâ€™s success by making learning stay on the
% manifold of the autoencoder. Since our framework
% can be used with any pretrained autoencoder, it
% will benefit from large-scale pretraining in future
% research.