\section{Related Work}
Reconstructing text with autoencoders is an active area of research that has lead to several advancements
% \nikos{I'd rephrase to 'Reconstructing text with autoencoders is an active area of research that has lead to several advancements such as'}
such as denoising~\cite{vincent2010stacked}, variational~\cite{kingma2013autoencoding, Higgins2017betaVAELB, dai2019diagnosing}, adversarial~\cite{makhzani2015adversarial, zhao2018adversarially}, and regularized~\cite{Ghosh2020From} autoencoders. They have been found especially useful in controlled text generation \citep{hu2017toward, logeswaran2018content, bowman2016generating}, especially in sentiment style transfer \citep{mai2020plug, shen2017style}.
% \nikos{can we give some concrete tasks as an example? e.g. sentiment style transfer etc}

The encoder-decoder structure for obtaining representations has been used in pretraining \citep{lewis2019bart}, sentence infilling \citep{huang2020inset}, and multilingual \citep{artetxe18} scenarios. In particular, \citet{lewis2019bart} treat denoising as translation task to perform pretraining from scratch, but their approach does not induce a sentence representation space with generative properties.
% \nikos{I would also mention that this approach does not induce a sentence representation space based on which generation can be made.}
In contrast, our method makes use of a frozen pretrained transformer to learn a shallow, sentence bottleneck autoencoder on top.
% \nikos{do we cite the paper that was brought to our attention via email recently?} 

% Denoising 
% The encoder-decoder structure has found use in pretraining \citep{lewis2019bart}, multilingual representations \citep{artetxe18}, sentence infilling \cite{}, and style transfer \citep{mai2020plug}. \citet{lewis2019bart} treat denoising as translation task to perform pretraining. 

% Utilizing an auxiliary decoder to obtain representations has been explored multilingually . 
% \citet{lewis2019bart} treat denoising as translation task to perform pretraining. 


% In contrast to the methods above, we make use of a frozen pretrained transformer to learn a shallow, sentence bottleneck autoencoder, rather than training from scratch or finetuning.


% explore the use of a denoising autoencoder for pretraining, but allow multiple encoder representations, rather than a single, sentence-elvel bottleneck representation

% Compared to other methds Our method differs from other methods in that we fix the pretrained transformer completely. Several methods exploring autoencoder representations \citep{artetxe18, zhang2018learning} end up discarding the decoder after training. \citet{lewis2019bart} explore the use of a denoising autoencoder for pretraining, but allow multiple encoder representations, rather than a single, sentence-elvel bottleneck representation, to be passed to the decoder. \ivan{Need to word better}