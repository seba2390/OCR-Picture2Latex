
\section{Model: \textsc{Autobot}}\label{sec:model}
% \nikos{Write a short sentence to summarize the key idea of the model and state that we take inspiration from relevant research  research on text autoencoders (+ cite a key one)}
%Our model aims to pool the variable-length pretrained transformer language model representations into a single, sentence-level representation with reconstructive capabilities. 
Taking inspiration from recent research on text autoencoders \citep{bowman2015generating,shen2019educating,mai2020plug}, we extend standard autoregressive text autoencoders, which have been predominantly based on recurrent networks, to a transformer-based architecture and integrate them with pretrained language models; here we focus on RoBERTa \cite{liu2019RoBERTa}.

%\nikos{Introduce here the basic formulation of autoencoders and keep only the novel parts in the following subsections}
Autoencoders generally follow the encoder-decoder model structure to reconstruct their input with the constraint that the encoder produces a single, fixed-length hidden representation $\text{enc}(\boldsymbol{x}) = \mathbf{z}$:
\begin{equation}
\text{AE}(\boldsymbol{x}) = \text{dec}(\text{enc}(\boldsymbol{x})) = \boldsymbol{x}'.
\end{equation}
\noindent Here, we focus on denoising autoencoders that aim to reconstruct a perturbed version of the input  \citep{vincent2010stacked,shen2019educating}, which is compatible with many of the pretrained language models that are based on masked language modeling. In our experiments, we use the same masking procedure as \citet{devlin-etal-2019-bert} to perturb the input.
\subsection{Encoder}
\begin{figure}[ht]
\centering
\hspace{-2mm}\includegraphics[width=0.4\textwidth]{figures/autobots} 
\caption{\label{fig:model} % \nikos{new take}
Our autoencoder consists of a pretrained transformer encoder $\text{enc}$, a function $\beta$ that compresses the encoder's  final representations $\mathbf{H}$ of size $T \times d$ 
% \nascomment{need to give dimensions, you're assuming the reader will infer this is a matrix whose size depends on the number of words}
to a sentence bottleneck representation $\mathbf{z}$ of size $d$, and a transformer decoder $\text{dec}$ that is trained to fully reconstruct the training sentence $\boldsymbol{x}$. }
% \nikos{+1, I would keep it minimal and relatively small to save space e.g. even simpler than \url{https://paperswithcode.com/media/methods/Autoencoder_schema.png} perhaps with some indication that the encoder is a pretrained transformer.}}
% \caption{\label{fig:overview} We propose a novel task setup to multilingual question answering. A high resource language (HRL) question-answer database is leveraged by mapping a low resource language (LRL) query to a query in the database, then generating the LRL answer from the HRL answer.} % We examine using multilingual paraphrase detection in exploring the most reliable method to transfer knowledge from multilingual queries to English.
\end{figure}Standard approaches use encoders that reduce the input to a single representation $\mathbf{z}$. To use a pretrained transformer for this purpose we need to reduce its output hidden representations $\mathbf{H}$ after processing the input to a single vector. Since using the special token representation or basic pooling methods have been shown sub-optimal in prior work \cite{Reimers2019SentenceBERT}, here we opt to keep the original encoder fixed and train a transformation $\beta$ that will learn to compress $\mathbf{H}$ into a single representation   $\mathbf{z} = \beta(\mathbf{H}; \mathbf{\theta})$, with $\theta$ being an additional set of parameters to be learned during finetuning. We choose $\beta$ to be a multi-head attention mechanism that takes as input the keys  $\mathbf{K}$ and values $\mathbf{V}$ corresponding to the final representations $\mathbf{H}$ from the pretrained model
% \nascomment{confusing here:  all layers?  the last one?}
and a query vector $\mathbf{q}$ corresponding to a context vector $\mathbf{u}$ that we choose to be the \texttt{CLS} vector from the pretrained model:
%
\begin{align}
   \beta(\mathbf{H}; \mathbf{\theta}) &= \text{MultiHead}(\mathbf{q}, \mathbf{K}, \mathbf{V}) 
\end{align}
%
\noindent where the parameters to be learned, $\theta$, include the weights that are used to transform the query, keys, and values which amount to $ 3 d^2 $ with $d$ being the dimensionality of each head ($d = 64$ in our experiments).
% \nascomment{($d =$ (fill in) in our experiments)}. 

% \nikos{new simplified version above}
%\subsubsection{Sentence Bottleneck} 
%The Transformer encoder architecture outputs a hidden representation $\mathbf{h_t}$ for each input token $x_t$. We wish to reduce these representations to a single, fixed representation $\mathbf{z}$. The self-attention layers used in Transformers produce a many-to-many representation by making the keys, queries, and values come from the same input. In order to produce a many-to-one final representation, we introduce a context vector $\mathbf{u}$ to replace the query. 
% \ivan{I commented out option (1), which we could explore later, option (2) boils down to multihead attention with nhead=1, which we can examine in an ablation study}
%\begin{align*}
%    \mathbf{z} &= \text{SB}(\mathbf{u}, \mathbf{K}, \mathbf{V}) \\
%    &= \text{MultiHead}(\mathbf{u}^\top, \mathbf{K}, \mathbf{V}) \\
%       &= \text{Concat}(\text{head}_1, ..., \text{head}_n) \\%\mathbf{W_C} \nikos{is this transformation needed?}
%      \text{head}_i = & \text{Attention}(\mathbf{u}^\top \mathbf{W^{(i)}_Q}, \mathbf{K} \mathbf{W^{(i)}_K}, \mathbf{V} \mathbf{W^{(i)}_V})
%\end{align*} 

%\subsubsection{Pretrained transformers} 
% \nikos{should also mention here the finetuning options} 

%Let $\text{TE}(x)$ be an arbitrary transformer encoder and $\text{SB}(x)$ be the Sentence Bottleneck specified above. Then, the encoder of our transformer autoencoder is defined as the following. $$\mathbf{z} = \text{Enc}(\mathbf{x}) = \text{SB}(\mathbf{u}, \text{TE}(\mathbf{x}), \text{TE}(\mathbf{x}))$$ The context vector $u$ can be either be represented as a learned parameter, or a pooling of the $\text{TE}(\mathbf{x})$. This context vector could additionally used to condition on different contexts to adapt a particular bottleneck to different types of information. 

% For \textsc{AUTOBOTS}, we choose $\mathbf{u}$ to be learned through the $\texttt{CLS}$ representation.

\subsection{Decoder}

The %"encoder-decoder attention" 
cross-attention layer in the Transformer decoder architecture by \citet{vaswani17}  expects hidden representations for every token input from the encoder in order for each output candidate to attend to each input token. In the situation where only a single representation comes from the encoder, we have
% Analyzing the self attention in the context of \textbf{TAE}, both the memory keys and values in the self attention come from the single hidden representation $z$, which reduces to simply the linear transformation of the latent representation of the input sequence for each output candidate. \ivan{Using the notation in the original paper, the derivation would be the following:}
\begin{equation}
    \text{Attention}(\mathbf{Q}, \mathbf{z}^\top \mathbf{W}_K, \mathbf{z}^\top \mathbf{W}_V) = \mathbf{z}^\top \mathbf{W}_V
\end{equation}
% \nascomment{changed $\mathbf{W_V}$ and similar to $\mathbf{W}_V$ and similar, note the difference}
%\nikos{ I don't get this. why the queries are ignored? isn't like doing cross-attention with a single token as source?} \ivan{Correct, so since this is the cross-attention layer, "queries" are the the current hidden states of the decoder. In other words, every timestep of the decoder will pull the exact same $\mathbf{z}^\top \mathbf{W_V}$ regardless of the current timestep}\nikos{hmm, then we could simply say that every query attends to the same sentence representation z instead of each token representation in the source sentence. saying that queries are ignored is a bit confusing.  }
Note that the queries $\mathbf{Q}$, which come from the previous masked self-attention layer, are not  taken into account, and each step in the decoder will receive the exact same $\mathbf{z}^\top \mathbf{W}_V$ as a result.  In order to mitigate this, we propose a gating method inspired by \citet{hochreiter1997long}. %\nikos{nice!}  
%  to allow the input query to control the per-element amount of information to include from the hidden representation vector
Concretely, let $\mathbf{Q_t}$ be the $t$th query representation. Then, the $t$th output $\mathbf{o}_t$ of the %"encoder-decoder"
cross-attention layer is computed as follows 
\begin{equation}
    \mathbf{g}_t = \sigma(\mathbf{G}  \mathbf{Q}_t + \mathbf{G'}  \mathbf{z});\ \  
        \mathbf{o_t} = \mathbf{g}_t \odot \mathbf{z}^\top\mathbf{W}_V
\end{equation}
% \nikos{the gate $g_t$ doesn't look at the \mathbf{z} vector at all, so it's output will be independent of the z vector. we should probably add z as input to make sure features from both $h_t$ and $z_t$ are considered. e.g. $W_g h_t + b_g + W_z z + b_z$}
\noindent where $\sigma(\cdot)$ is the \textit{sigmoid} activation function and $\mathbf{G}$ and $\mathbf{G}'$
%, $\mathbf{b_g}$ \nikos{let's suppress the biases for brevity}
are the parameters of the transformation for the gate. %\nikos{do we use different parameters here or it's one transformation for both Q and z?} \ivan{different parameters}
One can view the role of the gate as determining the amount of per-element information from the linear transformation of the latent representation to keep for the current layer and timestep. Preliminary experiments found this method beneficial for generation.

\paragraph{Training considerations} To avoid training our model from scratch, we finetune it for 100K optimization  steps on a pretraining dataset using the base RoBERTa model \cite{liu2019RoBERTa} on the encoder side and a single layer 
% transformer \nascomment{this doesn't align with the decoder descirbed above which I don't think is a transformer}
decoder side for efficiency purposes \citep{kasai2021deep}. The model is trained using an input reconstruction loss by minimizing the negative log-likelihood computed over the reconstructed inputs. Note that only the parameters of the sentence bottleneck and the decoder are learned; the encoder parameters are kept fixed.

%Unlike the masked language modeling and next sentence prediction pre-training objectives of BERT, we introduce the unsupervised reconstruction objective by pairing \textsc{Autobots} with a transformer decoder and train on the task of reconstructing its own input. \nikos{this is already mentioned?}
%Previous work has shown denoising autoencoders provide better representations \cite{vincent2010stacked, shen2019educating}, so we use the masked language modeling input and task the decoder with producing the entire, unmasked sequence.
%We propose keeping BERT fixed throughout this process, and learning a representation from the final, fixed BERT output.
% Below are the methods that consider both fixed and trainable BERT:

% \subsubsection{Fixed BERT}
% One can use the fixed weights from BERT, and only finetune the bottleneck of the encoder with the decoder. Inspired by the objective of preserving the BERT representations, this method experiences speed benefits from not needing to backpropogate into BERT. The two methods this can be applied to are the following:
