\section{Detecting Queue Configurations}
\label{sec:charact}
In the previous sections, we described our sensing setup composed by a network of depth cameras that provides 3D data. 
With this data, we are able to detect people and compute an occupancy grid map -- a descriptor that does not rely on tracking to encode people movements.

With the occupancy maps for a given period of time, we aim to cluster them into representative classes, where each class corresponds to a different queue configuration. 
Fig. \ref{fig:goal} illustrates this goal, with similar patterns grouped in the same cluster. 
Each cluster is represented in different color and with label $l_i$, where $i$ is the class index.
In this example, we illustrate three clusters with regular patterns ($l_1$ to $l_3$) and a white label ($l_0$) for irregular patterns.
\begin{figure}[tbh]
\centering
\includegraphics[width=0.43\textwidth]{./imgs/goal.pdf}
\caption{This figure illustrates the goal of out methodology: going from occupancy maps to labels corresponding to the different queue configurations. Here, we illustrate three regular clusters ($l_1$ to $l_3$) and a cluster with irregular patterns ($l_0$, white label). Each cluster is composed by several similar patterns.}
\label{fig:goal}
\end{figure}
%
Since we do not know the classes \emph{a priori}, a supervised approach is not adequate. 
On the other hand, the traditional unsupervised clustering methods do not guarantee a solution to our problem, as they require the number of classes to be known \emph{a priori}, and do not deal with outliers. 
As Fig. \ref{fig:goal} shows, we may have a considerable percentage of outliers in the data (white label, $l_0$). 

We model a typical pattern as a linear combination of other maps in the same class and compute the combination coefficients by solving a convex optimization problem. 
Because typical patterns appear regularly over time, we draw inspiration from the \emph{self-expressiveness} property~\cite{soltanolkotabi2012geometric,elhamifar2013sparse}, which states that a point in a subspace can be represented as a linear combination of points in the same subspace.
So, each class is modeled as a subspace, where each pattern of a given class is a point in the corresponding subspace. 
Solving this convex optimization problem, we have a measure of irregularity that assesses how distant a pattern is from the linear model. If this measurement is too high, the pattern is considered irregular (outlier).

Our method consists of two main steps: \emph{identifying regular and irregular maps}; and \emph{clustering regular maps}. 
Fig. \ref{fig:method} summarizes the methodology.
\begin{figure}[tbh]
\centering
\includegraphics[width=0.4\textwidth]{./imgs/method_v2.pdf}
\caption{Diagram summarizing the two main steps of the proposed queue classification methodology.}
\label{fig:method}
\end{figure}
%
\subsection{Identifying Regular and Irregular Maps}
\label{sec:mfs}
%
Consider the data matrix
\begin{equation}
    \mathbf{X}=
\begin{bmatrix}
\vertbar & \vertbar & & \vertbar\\
\mathbf{x}_1 & \mathbf{x}_2 &  \dots  & \mathbf{x}_m \\
\vertbar & \vertbar &  & \vertbar \\
\end{bmatrix}
\in \mathbb{R}^{d\times m},
\end{equation}
where column $\mathbf{x}_i$ is a vectorized occupancy map, $d$ is the data dimension and $m$ is the number of maps\footnote{Bold capital letters, $\mathbf{A}$, represent matrices. Bold lower-case letters, $\mathbf{a}$, represent column vectors. Bold lower-case letters with subscript, $\mathbf{a}_i$, represent the $i^{th}$ column of matrix $\mathbf{A}$. Scalars are denoted by non-bold lower-case letters, $a$. The scalar element in row $j$ and column $i$ of matrix $\mathbf{A}$ is denoted by a non-bold lower-case letter with two subscripts, $a_{ji}$.}. 


A map $\mathbf{x}_i$ is represented by a convex combination of other maps, being modeled as
\begin{align}
    &\mathbf{x}_i = \sum_{j=1, j\neq i}^m \mathbf{x}_j c_{ji} \label{eq:map-model}\\
    &\sum_{j=1, j \neq i}^{m} c_{ji} = 1, \quad c_{ji} \geq 0 \nonumber 
\end{align}
where $c_{ji}$ is the coefficient that relates map $j$ with map $i$.
%
The convex combination ensures that the map $\mathbf{x}_i$ is in the convex hull of other maps and, because of $c_{ji} \geq 0$, that the maps in the convex hull have similarities with $\mathbf{x}_i$.
Because a map should be expressed as a combination of maps belonging to the same cluster/subpace, our goal is to find the subspaces that best explain all the data.
In matrix form, the model is given by
%
\begin{equation}
    \mathbf{X}=\mathbf{XC} \label{eq:model-matrix}
\end{equation}
with
\begin{equation}
   \mathbf{1}_m^T \mathbf{C} = \mathbf{1}_m^T\mathrm{,}\hspace*{3mm} \mathbf{C} \geq \mathbf{0}_{m\times m}\mathrm{,}\hspace*{3mm} diag(\mathbf{C}) = 0 \nonumber
\end{equation}
where $\mathbf{C} \in \mathbb{R}^{m\times m}$ is the coefficients matrix and $\mathbf{1}_m$ is a vector of ones with dimension $m$. 
Imposing the null diagonal of $\mathbf{C}$, $diag(\mathbf{C})=0$, excludes the trivial solution of a map explaining itself.
The irregularity of a map is measured by how much the reconstruction of that map violates the linear model.
%
This is given by 
\begin{equation}
    \mathbf{\mathcal{I}}_i=\norm{\mathbf{x}_i-\mathbf{Xc}_i}_1,
\label{eq:irreg}
\end{equation}
where the operator $\norm{.}_1$ is the \emph{$\ell_1$-norm} and $\mathbf{c}_i$ is the $i$-th column of $\mathbf{C}$.
Our goal is to find the convex combination that best reconstructs each map, in the sense of minimizing the irregularity measure.
Then, we wish to generate map $\mathbf{x}_i$ with a small number of maps.
By using the \emph{$\ell_1$-norm} in \eqref{eq:irreg}, we are accounting for sparse error in the data.

The coefficients are obtained by solving the following optimization problem
\begin{align}
	\min_{\mathbf{C}}\quad & \mathcal{I} \label{eq:mfs} \\
	\textup{s.t.}\quad & diag(\mathbf{C})=0\nonumber \\
	& \mathbf{1}_m^T \mathbf{C} = \mathbf{1}_m^T \nonumber\\
	& \mathbf{C} \geq \mathbf{0}_{m\times m},\nonumber 
\end{align}
where
\begin{equation}
	\mathcal{I}=\norm{\mathbf{X-XC}}_{1} 
\end{equation}
is the global irregularity measure of the data and operator $\norm{.}_{1}$ is the $L_{1}$ matrix norm. The solution is computed in parallel by solving for each $\mathbf{c}_i$ separately \cite{oat2016mfs}.

By solving \eqref{eq:mfs} we generate the map $\mathbf{x}_i$ with the other maps in $\mathbf{X}$. 
However, when the error $\mathcal{I}_i$ is too large, a map is an outlier (a unique pattern). 
Therefore, the irregularity $\mathcal{I}_i$ is in fact a measure of uniqueness of the map $\mathbf{x}_i$ within the data set. 
Fig. \ref{fig:irreg} shows the irregularity measure, $\mathcal{I}_i$, for $m$ occupancy maps corresponding to a full day.
\begin{figure}[tbh]
\centering
\includegraphics[width=0.43\textwidth]{./imgs/irreg-plot-maps.pdf}
\caption{Irregularity measure for a complete day. Some of the corresponding occupancy maps are presented to show that higher $\mathcal{I}_i$ values correspond to more peculiar patterns.}
\label{fig:irreg}
\end{figure}
As expected, maps with lower $\mathcal{I}_i$ are maps capturing typical states of the queue. 
On the other hand, large values correspond to the occasional patterns.

Choosing a value of $p$, we label as \emph{irregular} the $p\%$\footnote{This is equivalent to apply a threshold $\mu$ to $\mathcal{I}_i$\cite{oat2016mfs}.} of maps with highest irregularity and the others as \emph{regular}\footnote{Note that in \cite{oat2016mfs} these irregular observations correspond to physical keypoints of a shape/model. 
Whereas here we interpret them as maps corresponding to unique patterns in the data. }. With the split and merge approach, presented in next section, the methodology is robust to a large range of $p$ values.

\subsection{Clustering Regular Maps}
\label{sec:clustering}
%
Solving \eqref{eq:mfs}, we are able to identify regular and irregular (outlier) patterns. Filtering out outliers, the data can be clustered using common clustering methods. Here, we take advantage of the sparse coding framework.
Note that $\mathbf{C} + \mathbf{C}^T$ defines an undirected graph where the nodes represent the maps and edges represent association (non-null coefficients) between maps.
Because, the number of classes $K$ is unknown \emph{a priori}, we estimate $K$ by estimating the number of zero eigenvalues of the Laplacian of this graph.
However, small clusters (with few points) have small impact in the eigenvalues and, when such clusters exist, $K$ is underestimated. 
To retrieve the smaller clusters, we segment the data into a larger number, $\gamma$, of linear subspaces and use a measure intrinsic to the model to compute the distance between the subspaces. If this distance is small, clusters are merged.

The criterion we use is the Normalized Subspace Inclusion (NSI), a distance between subspaces of any dimension \cite{silva2009normalized}.
Formally, it is defined as
\begin{equation}
    NSI(\mathcal{L}_1,\mathcal{L}_2) = \frac{tr(\mathbf{U_1^T U_2 U_2^T U_1})}{\min (d_1,d_2)},
\end{equation}
where $\mathcal{L}_1$ and $\mathcal{L}_2$ are linear subspaces in $\mathbb{R}^n$, $\mathbf{U}_1$ and $\mathbf{U}_2$ are their orthonormal basis and $d_1$, $d_2$ are the subspaces dimensions.
This criterion measures inclusion of subspaces, generalizing the angle between two subspaces.
Then, choosing this criterion comes naturally from the assumptions we made previously, that the data lies in the union of linear subspaces. 
    Similarly, and to exploit this subspace structure in all the clustering process, we over-segment the regular data using \emph{Spectral Clustering} with $\mathbf{C+C^T}$ as adjacency matrix.

%
Fig. \ref{fig:nsi-matrix} shows an affinity matrix with the NSI for six clusters. For each cluster we show here only one of its maps.  
It is clear that the NSI value between clusters that correspond to very similar configurations is close to $1$. 
%
\begin{figure}[thb]
\centering
\includegraphics[width=0.295\textwidth]{./imgs/nsi1_parula_final.png}
\caption{NSI affinity matrix for six exemplifying clusters. For each cluster, we show a map belonging to it. Pairs of clusters corresponding to similar configuration have NSI value close to $1$.}
\label{fig:nsi-matrix}
\end{figure}
%%

Clusters are merged if their pair-wise NSI value is above a threshold.
Similarly, after this step, we reassess the maps labeled as \emph{irregular}. We compute the pair-wise NSI affinity for each irregular map and cluster. The map is unified to the cluster with larger affinity if the value is above a threshold, otherwise, the maps remain labeled \emph{irregular}.
This consolidation step, allows a large range for $p\%$ in the selection of irregular/regular patterns. 
%
\subsection{Methodology Summary}

The methodology proposed in this section is summarized in Algorithm \ref{alg:method}.
First, we solve \eqref{eq:mfs} to compute the coefficients that relate the patterns (line 1) and then we rank the maps by irregularity to identify regular and irregular maps (line 2). Next, we estimate the number of clusters and partition regular data accordingly (lines 3 and 4). Then, clusters are consolidated using the affinity measure (lines 5 and 6). Finally, irregular maps are classified according to the consolidated clusters (line 7). The output is the labeling for all data points and the corresponding partition into regular and irregular patterns.

Next, we present results of applying this methodology to real data.
%
\begin{algorithm}
\caption{Identifying and clustering regular occupancy maps in the data.}
\label{alg:method}
\begin{algorithmic}[1]
\Statex \textbf{Input:} $\mathbf{X}$ - Data matrix
\Statex \qquad\quad $p$ - \% of maps to label as irregular
\Statex \qquad\quad $th$ - NSI threshold
\Statex \textbf{Output:} $\mathbf{X}_{reg}$ - Regular data
\Statex \qquad\quad\quad\!$\mathbf{X}_{irreg}$ - Irregular data
\Statex \qquad\quad\quad\!\!$cl_{reg}$ - Regular data class labels
\Statex
\Statex \emph{Step 1: Identifying Regular and Irregular Maps}
\State $\mathcal{I} \gets \text{Solve \eqref{eq:mfs} for } \mathbf{C}$
\State $\mathbf{X}_{reg}, \mathbf{X}_{irreg} \gets $ Divide regular and irregular data 
\Statex \hspace{23.3mm} according to $p$ and $\mathcal{I}$
\Statex
\Statex \emph{Step 2: Clustering Regular Maps}
\State $\gamma \gets$ Estimate number of clusters
\State $cl_{reg} \gets $ Segment $\mathbf{X}_{reg}$ into $\gamma$ clusters \label{lin:segm}
\State $affinity \gets $ Compute NSI between $\gamma$ clusters
\State $cl_{NSI} \gets $ Merge clusters in $cl_{reg}$ with affinity $> th_{NSI}$
\State $cl_{reg},\mathbf{X}_{reg},\mathbf{X}_{irreg}\gets $ Reclassify $\mathbf{X}_{irreg}$ according 
\Statex \hspace{33mm}to $cl_{NSI}$
\end{algorithmic}
\end{algorithm}
%%
