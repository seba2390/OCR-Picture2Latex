\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2023


% ready for submission
\usepackage[preprint]{neurips_2023}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2023}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2023}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2023}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{colortbl}
\usepackage{caption}
\usepackage{algorithm}
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{stmaryrd}
\usepackage{natbib}
\setcitestyle{numbers,square}
\newcommand{\veryshortarrow}[1][3pt]{\mathrel{%
   \hbox{\rule[\dimexpr\fontdimen22\textfont2-.2pt\relax]{#1}{.4pt}}%
   \mkern-4mu\hbox{\usefont{U}{lasy}{m}{n}\symbol{41}}}}

\makeatletter

\setbox0\hbox{$\xdef\scriptratio{\strip@pt\dimexpr
    \numexpr(\sf@size*65536)/\f@size sp}$}

\newcommand{\scriptveryshortarrow}[1][3pt]{{%
    \hbox{\rule[\scriptratio\dimexpr\fontdimen22\textfont2-.2pt\relax]
               {\scriptratio\dimexpr#1\relax}{\scriptratio\dimexpr.4pt\relax}}%
   \mkern-4mu\hbox{\let\f@size\sf@size\usefont{U}{lasy}{m}{n}\symbol{41}}}}

\makeatother
\title{Re-Think and Re-Design Graph Neural Networks in Spaces of Continuous Graph Diffusion Functionals}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{%
  % *\thanks{Use footnote for providing further information
  %   about author (webpage, alternative address)---\emph{not} for acknowledging
  %   funding agencies.} \\
   Tingting Dan${^1}$, Jiaqi Ding${^2}$, Ziquan Wei${^1}$, Shahar Z Kovalsky${^3}$, Minjeong Kim${^4}$\\
    \textbf{Won Hwa Kim}${^5}$, \textbf{Guorong Wu}${^{1,2,*}}$\\
${^1}$ Department of Psychiatry, University of North Carolina at Chapel Hill\\
${^2}$ Department of Computer Science, University of North Carolina at Chapel Hill\\
${^3}$ Department of Mathematics, University of North Carolina at Chapel Hill\\
${^4}$ Department of Computer Science, University of North Carolina at Greensboro\\
${^5}$ Computer Science and Engineering / Graduate School of AI\\
${^*}$ grwu@med.unc.edu\\
 %  \\
%  \And
%   Jiaqi Ding \\
% University of North Carolina at Chapel Hill \\
%  %  \\
%   % address \\
%   % \texttt{email} \\
%   % examples of more authors
%   \And
%   Ziquan Wei \\
%   University of North Carolina at Chapel Hill  \\
%   % Address \\
%   % \texttt{email} \\
%   \AND
%   Coauthor \\
%  University of North Carolina at Chapel Hill  \\
%   % Address \\
%   % \texttt{email} \\
%   \And
%   Coauthor \\
% University of North Carolina at Chapel Hill \\
%   % Address \\
%   % \texttt{email} \\
%   \And
%   Coauthor \\
% University of North Carolina at Chapel Hill  \\
%   % Address \\
%   % \texttt{email} \\
}


\begin{document}


\maketitle


\begin{abstract}

%Graphs are a powerful tool for representing data in various domains, including social network analysis, natural language processing, and computer vision. Graph neural networks (GNNs) have emerged as a promising approach to analyze graph-structured data by propagating information through the graph. Conventional GNNs use a fixed set of discrete operations, such as message passing or graph convolution, to propagate information through the graph in a recursive fashion (layer after layer). However, such a manner can sometimes limit their performance on specific tasks. To address this limitation, we profit from a new approach that models the information propagation process using differential equations or other continuous mathematical operations. Our approach is on account of  an Euler-Lagrange (E-L) equation of the graph diffusion functions. This equation has been successfully used to solve the Brachistochrone problem, utilizing the method of variations. In light of this, we derive a novel differential equation and construct a principled development of a broad new class of GNN-based network architecture. Our approach can be applied to various scenarios and achieves state-of-the-art results in node classification and disease risk prediction. Our sample and general framework provide a flexible and effective way to model the information propagation process in GNNs. By leveraging the power of differential equations and continuous mathematical operations, we can unlock most of the potential of GNNs and advance research in these vital fields.
%TC:ignore
Graphs are ubiquitous in various domains, such as social networks and biological systems. Despite the great successes of graph neural networks (GNNs) in modeling and analyzing complex graph data, the inductive bias of locality assumption, which involves exchanging information only within neighboring connected nodes, restricts GNNs in capturing long-range dependencies and global patterns in graphs. Inspired by the classic \textit{Brachistochrone} problem, we seek how to devise a new inductive bias for cutting-edge graph application and present a general framework through the lens of variational analysis. The backbone of our framework is a two-way mapping between the discrete GNN model and continuous diffusion functional, which allows us to design application-specific objective function in the continuous domain and engineer discrete deep model with mathematical guarantees.
\textit{First}, we address over-smoothing in current GNNs. Specifically, our inference reveals that the existing layer-by-layer models of graph embedding learning are equivalent to a ${\ell _2}$-norm integral functional of graph gradients, which is the underlying cause of the over-smoothing problem. Similar to edge-preserving filters in image denoising, we introduce the total variation (TV) to promote alignment of the graph diffusion pattern with the global information present in community topologies. On top of this, we devise a new selective mechanism for inductive bias that can be easily integrated into existing GNNs and effectively address the trade-off between model depth and over-smoothing. \textit{Second}, we devise a novel generative adversarial network (GAN) to predict the spreading flows in the graph through a neural transport equation. To avoid the potential issue of vanishing flows, we tailor the objective function to minimize the transportation within each community while maximizing the inter-community flows. Our new GNN models achieve state-of-the-art (SOTA) performance on graph learning benchmarks such as \textit{Cora}, \textit{Citeseer}, and \textit{Pubmed}.
%TC:endignore

%propagating information through the graph, with each node aggregating information from its neighbors and updating its own state accordingly. This process is typically repeated multiple times using a fixed set of discrete operations, such as message passing or graph convolution, with information flowing through the graph in a recursive fashion (layer by layer) but sometimes limiting their performance on certain tasks. On the other hand, the information propagation process can be modeled by differential equations or other continuous mathematical operations. We rethink the GNN in continuous domain, which originates from an Euler-Lagrange (E-L) equation of the graph diffusion functions. E-L equation is applied incipiently with significant success in solving the brachistochrone problem, utilizing the method of variations. In light of this, our approach derives a novel differential equation and constructs a principled development of a broad new class of GNN-based network architecture. Our designed sample and general framework can apply to various scenarios and achieves state-of-the-art results in node classification and disease risk prediction.

\end{abstract}


\section{Introduction}

Graph is a fundamental data structure that arises in various domains, including social network analysis \cite{zhang2022improving}, natural language processing \cite{wu2023graph}, computer vision \cite{pradhyumna2021graph}, recommender systems \cite{wu2022graph}, and knowledge graphs \cite{ji2021survey} among others. Tremendous efforts have been made to operate machine learning on graph data (called graph neural networks, or GNNs) at the node \cite{kipf2016semi}, link \cite{zhang2018link}, and graph level \cite{GNNBook-ch9-morris}. The common inductive bias used in GNNs is the homophily assumption that nodes that are connected in a graph are more likely to have similar features or labels. In this context, most GNN models deploy a collection of fully-connected layers to progressively learn graph embeddings  by aggregating the nodal feature representations from its topologically-connected neighbors throughout the graph \cite{hamilton2017inductive}.
\begin{figure}[h]
    \centering
    \begin{minipage}{0.35\linewidth}
        \centering
        \includegraphics[width=\linewidth]{Fig1.pdf}
    \end{minipage}%
    \begin{minipage}{0.62\linewidth}
        \centering
        \vspace{-1.0em}
        \caption{\small Demonstration of the root cause of over-smoothing in GNNs. Nodes \#1 and \#2 are located along the boundary of two communities. The locality assumption in GNNs steers the learning of the graph representations by constraining the information exchange via node-to-node connections. However, such link-wise inductive bias opts to neutralize the contrast of node embeddings between nodes \#1 and \#2, which might undermine the node classification accuracy. Our research framework yields the solution for the over-smoothing issue by enabling heat-kernel diffusion within each community while penalizing the excessive community-to-community information exchanges (highlighted in red).}%Layer after layer, nodes have access to more graph nodes and a more graph structure-aware embedding. For the first layer, node \#1 and node \#2 interact with each other but have different one-hop neighbors, thus we may predict that their new embeddings will be different. In the second layer, node \#1 and node \#2 gain access to more parts of the graph, increasing the probability of accessing the same nodes. As a result, we can predict that their embeddings will be slightly similar, leading to inductive bias since they belong to different groups. This bias is caused by the fact that neighboring nodes tend to have more similar representations after information aggregation, and it limits the discriminative power of GNNs. In light of this, the difference between different groups should be enhanced and penalized similarities appropriately. This can be achieved by incorporating appropriate regularization techniques that can preserve the discriminative power of node features.}
        \label{demo}
    \end{minipage}
    \vspace{-1.4em}
\end{figure}

% \begin{figure}
%   \centering
%   \includegraphics[scale=0.55]{Fig1.png}
%   \caption{Layer after layer, nodes have access to more graph nodes and a more graph structure-aware embedding. For the first layer, node \#1 and node \#2 interact with each other but have different one-hop neighbors, thus we may predict that their new embeddings will be different. In the second layer, node \#1 and node \#2 gain access to more parts of the graph, increasing the probability of accessing the same nodes. As a result, we can predict that their embeddings will be slightly similar, leading to inductive bias since they belong to different groups. This bias is caused by the fact that neighboring nodes tend to have more similar representations after information aggregation, and it limits the discriminative power of GNNs. In light of this, the difference between different groups should be enhanced and penalized similarities appropriately. This can be achieved by incorporating appropriate regularization techniques that can preserve the discriminative power of node features.}
%   \label{demo}
% \end{figure}

Under the hood of GNNs, the graph representation learning process is achieved by various learnable operations, such as message passing \cite{michaelbeyond2022} or graph convolution \cite{kipf2016semi}. Due to the nature of exchanging information in a local graph neighborhood, however, it is challenging to capture global graph representations, which go beyond node-to-node relationship, by leveraging the deep architecture in GNNs while being free of overly smoothing the feature representations for the closely-connected nodes. Fig. \ref{demo} demonstrates the root cause of over-smoothing issue in current GNNs, where node color denotes the group label (no color means unlabeled) and edge thickness indicates connection strength. It is clear that nodes \#1 and \#2 are located at the boundary of two communities. The inductive bias of GNNs (i.e., locality  assumption) enforces the node embedding vectors on node \#1 and \#2 becoming similar due to being strongly connected (highlighted in red), even though the insight of global topology suggests that their node embeddings should be distinct. As additional layers are added to GNNs, the node embeddings become capable of capturing global feature representations that underlie the entire graph topology. However, this comes at the cost of over-smoothing node embeddings across graph nodes due to (1) an increased number of node-to-node information exchanges, and (2) a greater degree of common topology within larger graph neighborhoods. In this regard, current GNNs only deploy a few layers (typically two or three) \cite{li2018deeper}, which might be insufficient to characterize the complex feature representations on the graph.

It is evident that mitigating the over-smoothing problem in GNNs will enable training deeper models. From a network architecture perspective, skip connections \cite{gao2019graph,xu2018representation}, residual connections \cite{li2019deepgcns,guo2019densely}, and graph attention mechanisms \cite{velivckovic2017graph,thekumparampil2018attention} have been proposed to alleviate the information loss in GNNs, by either preserving the local feature representation or making information exchange adaptive to the importance of nodes in the graph. Although these techniques are effective to patch the over-smoothing issue in some applications, the lack of an in-depth understanding of the root cause of the problem poses the challenge of finding a generalized solution that can be scaled up to current graph learning applications.

Inspired by the success of neural ordinary differential equations in computer vision \cite{chen2018neural}, research focus has recently shifted to link the discrete model in GNNs with partial differential equation (PDE) based numerical recipes \cite{xhonneux2020continuous,poli2019graph,chamberlain2021grand, eliasof2021pde}. For example, Graph Neural Diffusion (GRAND) formulates GNNs as a continuous diffusion process \cite{chamberlain2021grand}. In their framework, the layer structure of GNNs corresponds to a specific discretization choice of temporal operators. Since PDE-based model does not revolutionize the underlying inductive bias in current GNNs, it is still unable to prevent the excessive information change between adjacent nodes as in nodes \#1 and \#2 in Fig. \ref{demo}. In this regard, using more advanced PDE solvers only can provide marginal improvements in terms of numerical stability over the corresponding discrete GNN models, while the additional computational cost, even in the feed-forward scenario, could limit the practical applicability of PDE-based methods for large-scale graph learning tasks.

In this regard, pioneering work on continuous approaches has prompted to re-think GNN as a graph diffusion process governed by the Euler-Lagrange (E-L) equation of the heat kernel. This formulation is reminiscent of the \textit{Brachistochrone} problem \footnote{The \textit{Brachistochrone} problem is a classic physics problem that involves finding the curve down which a bead sliding under the influence of gravity will travel in the least amount of time between two points.}, which emerged over 400 years ago and established the mathematical framework of classical mechanics. The powerful calculus of variations allows us to generate solutions for various mechanics questions (e.g., the slope that yields the fastest ball sliding down the curve is given by a cycloid) through the lens of E-L equation, as shown in Fig. \ref{motivate} (top).

In a similar vein, the question that arises in the context of community detection is: What graph diffusion pattern is best suited for preserving community organizations? The question for graph classification would be: What graph diffusion pattern works best for capturing the system-level characteristics of graph topology? Following the spirit of \textit{Brachistochrone} problem, we present a general research framework to customize application-specific GNNs in a continuous space of graph diffusion functionals. As shown in Fig. \ref{motivate} (bottom), we have established a fundamental structure for our framework that involves a two-way mapping between a discrete GNN model and a continuous graph diffusion functional. This allows us to develop application-specific objective functions (with an explainable regularization term) in the continuous domain and construct a discrete deep model with mathematical guarantee. We demonstrate two novel GNN models, one for addressing over-smoothing and one for predicting the flows from longitudinal nodal features, both achieving state-of-the-art performance (\textit{Cora}: 85.6\%, \textit{Citeseer}: 73.9\%, \textit{Pubmed}: 80.10\%, even in 128 network layers).

We have made four major contributions. (1) We establish a connection between the discrete model of GNNs and the continuous functional of inductive bias in graph learning by using the E-L equation as a stepping stone to bridge the discrete and continuous domains. (2) We introduce a general framework to re-think and re-design new GNNs that is less ``black-box''. (3) We devise a novel selective mechanism upon inductive bias to address the over-smoothing issue in current GNNs and achieve state-of-the-art performance on graph learning benchmarks. (4) We construct a novel GNN in the form of a generative adversarial network (GAN) to predict the flow dynamics in the graph by a neural transport equation.


%Following the above clues, we can infer a novel understanding of GNN. In the case of the quadratic Laplacian regularizer in GNN, it can be seen as a discrete model of Lagrangian mechanics, which is derived from the Euler-Lagrange equation of the graph diffusion functional. However, it has been found that the commonly used $\ell_2$-norm regularizer is ineffective in addressing the root cause of the inductive bias problem, which is the fact that neighboring nodes tend to have more similar representations after information aggregation. This can lead to neighbor nodes sharing the same label, whereas in reality, they may belong to different communities when they all are hub nodes (node \#1 and node \#2), as shown in Fig. \ref{demo}.

\begin{figure}[!t]
  \centering
   %\vspace{-2.0em}
  {\includegraphics[width=0.83\textwidth,trim=0 7 0 0,clip]{Fig2.pdf}}
  \caption{\small Top: The \textit{Brachistochrone} problem played a pivotal role in the development of classical mechanics and the powerful mathematical tool known as the calculus of variations. Bottom: We introduce a general framework to answer "\textit{Brachistochrone}" problems regarding diffusion patterns on graphs that allows us to re-think and re-design application-specific deep model of GNNs with enhanced mathematical interpretability.}
  \label{motivate}
  \vspace{-2.0em}
\end{figure}


\section{Methods}

In the following, we first elucidate the relationship between GNN, PDE, and calculus of variations (COV), which sets the stage for the \textit{GNN-PDE-COV} framework for new GNN models in Section \ref{2.2}.
% Then, we propose the TV-based graph regularization in Section \ref{2.3}.

\subsection{Re-think GNNs: Connecting dots across graph neural networks, graph diffusion process, Euler-Lagrange equation, and Lagrangian mechanics}
\label{2.1}
\textbf{Graph diffusion process.} Given graph data $\mathcal{G}=(V, W)$ with $N$ nodes $V=\{v_i |i=1,…,N\}$, the adjacency matrix $W=\left[w_{i j}\right]_{i, j=1}^N \in \mathbb{R}^{N \times N}$ describes connectivity strength between any two nodes. For each node $v_i$, we have a graph embedding vector $x_i \in \mathcal{R}^{m}$. In the context of graph topology, the graph gradient $\left(\nabla_{\mathcal{G}} x\right)_{i j}=w_{i j}\left(x_i-x_j\right)$ indicates the feature difference between $v_i$ and $v_j$ weighted by the connectivity strength $w_{ij}$, where $\nabla_{\mathcal{G}}$ is a $\mathbb{R}^N\rightarrow \mathbb{R}^{N\times N}$ operator. Thus, the graph diffusion process can be formulated as $\frac{\partial x(t)}{\partial t}=div \left(\nabla_\mathcal{G} x(t)\right)$, where the evolution of embedding vectors  $x=\left[x_i\right]_{i=1}^N$ is steered by the graph divergence operator.

\textbf{Connecting GNN to graph diffusion.} In the regime of GNN, the regularization in the loss function often measures the smoothness of embeddings $x$ over the graph by $x^T \Delta x$, where $\Delta=div(\nabla_{\mathcal{G}})$ is the graph Laplacian operator \cite{kipf2016semi}. To that end, the graph smoothness penalty encourages two connected nodes to have similar embeddings by information exchange in each GNN layer. Specifically, the new graph embedding $x^l$ in the $l^{th}$ layer is essentially a linear combination of the graph embedding $x^{l-1}$ in the previous layer, i.e., $x^l=A_{W, \Theta} x^{l-1}$, where the matrix $A$ depends on graph adjacency matrix $W$ and trainable GNN parameter $\Theta$. After rewriting $x^l=Ax^{l-1}$ into $x^l-x^{l-1}=(A-I)x^{l-1}$, updating graph embeddings $x$ in GNN falls into a discrete graph diffusion process, where the time parameter $t$ acts as a continuous analog of the layers in the spirit of Neural ODEs \cite{chen2018neural}. It has been shown in \cite{chamberlain2021grand} that running the graph diffusion process for multiple iterations is equivalent to applying a GNN layer multiple times.

\textbf{GNN is a discrete model of Lagrangian mechanics via E-L equation.}
The diffusion process $\frac{\partial x(t)}{\partial t}=div \left(\nabla_\mathcal{G} x(t)\right)$ has been heavily studied in image processing in decades ago, which is the E-L equation of the functional $\mathop {\min }\limits_x \int_\Omega  | \nabla x{|^2}dx$. By replacing the 1D gradient operator defined in the Euclidean space $\Omega$ with the graph gradient $(\nabla_\mathcal{G} x)_{ij}$, it is straightforward to find that the equation governing the graph diffusion process $\frac{\partial x(t)}{\partial t}=div \left(\nabla_\mathcal{G} x(t)\right)$ is the E-L equation of the functional $\mathop {\min }\limits_x \int_\mathcal{G}  | \nabla_{\mathcal{G}} x{|^2}dx$ over the graph topology. Since the heat kernel diffusion is essentially the mathematical description of the inductive bias in current GNNs, we have established a mapping between the mechanics of GNN models and the functional of graph diffusion patterns in a continuous domain.

% Recall that the GNN depth has been accused of causing the over-smoothing issue in graph representation learning.
\textbf{Tracing the smoking gun of over-smoothing in GNNs.} In Fig. \ref{demo}, we observed that the inductive bias of link-wise propagation is the major reason for excessive information exchange, which attributes to the over-smoothing problem in GNNs. An intuitive approach is to align the diffusion process with high-level properties associated with graph topology, such as network communities.
%Recall that GNNs have a strong inductive bias towards the local structure of the graph, meaning that they assume that the information from neighboring nodes is more important than the information from non-neighboring nodes. This bias is reflected in the design of the message passing mechanism and the aggregation function used in the GNN layers.
After connecting the GNN inductive bias to the functional of graph diffusion process, we postulate that the root cause of over-smoothing is the isotropic regularization mechanism encoded by the ${\ell _2}$-norm.
More importantly, connecting GNN to the calculus of variations offers a more principled way to design new deep models with mathematics guarantees and model mechanistic explainability.

\subsection{Re-design GNNs: Revolutionize inductive bias, derive new E-L equation, and construct deeper GNN}
\label{2.2}
The general roadmap for re-designing GNNs typically involves three major steps: (1) formulating inductive bias into the functional of graph diffusion patterns; (2) deriving the corresponding E-L equation; and then (3) developing a new deep model of GNN based on the finite difference solution of E-L equation. Since the graph diffusion functional is application-specific, we demonstrate the construction of new GNN models in the following two graph learning applications.

\subsubsection{Develop \textit{VERY} deep GNNs with a selective mechanism for link-adaptive inductive bias}
\label{TV}

\textbf{Problem formulation.} Taking the feature learning component (learnable parameters $\Theta$) out of GNNs, the graph embeddings $x^{L}$ (output of an $L$-layer GNN) can be regarded as the output of an iterative smoothing process ($L$ times) underlying the graph topology $\mathcal{G}$, constrained by the data fidelity $\left\| {x^{L} - x^{0}} \right\|_2^2$ (w.r.t. the initial graph embeddings $x^{0}$) and graph smoothness term $\mathop \int_\mathcal{G}  | \nabla_{\mathcal{G}} x{|^2}dx$.
Inspired by the great success of total variation (TV) for preserving edges in image denoising \cite{rudin1992nonlinear}, reconstruction \cite{wang2008new} and restoration \cite{chan2006total}, we proposed to address the over-smoothing issue in current GNN by replacing the quadratic Laplacian regularizer with TV on graph gradients, i.e., $\mathcal{J}_{TV}(x) = \mathop \int|\nabla_\mathcal{G} x| dx$.
Thus, the TV-based objective function for graph diffusion becomes: $\mathop {\min }\limits_x (\left\| {x - x^{0}} \right\|_2^2 + \mathcal{J}_{TV}(x))$.
%Total variation is a measure that quantifies the extent to which a signal or image changes or exhibits differences between its values or features, by utilizing a penalty term based on the gradient or difference between neighboring pixels or samples.

However, the ${\ell _1}$-norm function, denoted by $|\cdot|$ in the definition of the total variation functional $\mathcal{J}_{TV}$, is not differentiable at zero. Following the dual-optimization schema \cite{beck2009fast,chambolle2011first}, we introduce the latent auxiliary matrix $z\in \mathbb{R}^{N\times N}$ and reformulate the TV-based functional as $\mathcal{J}_{TV}(x, z) = \mathop {\max}\limits_z \mathop {\min }\limits_x \int(z \otimes \nabla_\mathcal{G} x) dx$, subject to $|z| \le \mathbf{1}^{N \times N}$, where $\otimes$ is Hadamard operation between two matrices.
%, i.e., the condition $|z|\le \textbf{1}$ is taken element-wise, we drop the $\otimes$ for simplicity.
Furthermore, we use an engineering trick of element-wise operation $z_{ij}(\nabla_\mathcal{G} x)_{ij}$ to keep the degree always non-negative (same as we take the absolute value), which makes the problem solvable.  %allows us to avoid the undifferentiable challenge.
%Taking together,
In the end, we reformulate the minimization of $\mathcal{J}_{TV}(x)$ into a dual \textit{min-max} functional as $\mathcal{J}_{TV}(x,z)$, where we maximize $z$ ($z\rightarrow \mathbf{1}^{N\times N}$) such that $\mathcal{J}_{TV}(x,z)$ is close enough to $\mathcal{J}_{TV}(x)$.
Therefore, the new objective function is reformulated as:
\begin{equation}
\label{obj_fun}
{\cal J}(x,z) = \mathop {\max }\limits_z \mathop {\min }\limits_x \left\| {x - {x^0}} \right\|_2^2 + \lambda \int {(z{\nabla _{\cal \mathcal{G}}}x} )dx,
\end{equation}
which $\lambda$ is a scalar balancing the data fidelity term and regularization term. Essentially, Eq. \ref{obj_fun} is the dual formulation with \textit{min-max} property for the TV distillation problem \cite{zhu2010duality}.

\textbf{Constructing E-L equations.} To solve Eq. \ref{obj_fun}, we present the following two-step alternating optimization schema. \textit{First}, the inner minimization problem (solving for  $x_i$) in Eq. \ref{obj_fun} can be solved by letting $\frac{\partial }{{\partial x_i}}\mathcal{J}(x_i,z_i)=0$:
 \begin{equation}
 \label{first_layer}
\begin{array}{*{20}{c}}
{\frac{\partial }{{\partial x_i}}{\cal J}(x_i,z_i) = 2(x_i - {x_i^0}) + \lambda z_i {\nabla _{\cal G}}x_i = 0}& \Rightarrow &{\hat x_i = {x_i^0} - \frac{\lambda }{2}z_i\nabla _{\cal G}x_{i}}
\end{array}
\end{equation}


Replacing $\left(\nabla_{\mathcal{G}} x\right)_{i j}$ with $w_{i j}\left(x_i-x_j\right)$, the intuition of Eq. \ref{first_layer} is that each element in $\hat{x}_i$ is essentially the combination between the corresponding initial value in $x^0_i$ and the overall graph gradients $z_i\nabla _{\cal G}x_{i}=\sum_{j\in \mathcal{N}_i}w_{ij}(x_i-x_j)z_i$ within its graph neighborhood $\mathcal{N}_i$. In this regard, Eq. \ref{first_layer} characterizes the dynamic information exchange on the graph, which is not only steered by graph topology but also moderated by the attenuation factor $z_i$ at each node.

\textit{Second}, by substituting Eq. \ref{first_layer} back into Eq. \ref{obj_fun}, the objective function of $z_i$ becomes
$\mathcal{J}(z_i) = \mathop {\max }\limits_{|z_i| \le {\mathbf{1}}} \left\| {\frac{\lambda }{2}{z_i\nabla _\mathcal{G}}x_i} \right\|_2^2 + \lambda z_i{\nabla _\mathcal{G}}({x_i^{0}} - \frac{\lambda }{2}{z_i\nabla _\mathcal{G}}x_i)$.
With simplification (in Eq. \ref{function1} to Eq. \ref{obj1} of Supplementary), the optimization of each $z_i$ is achieved by $\mathop {\arg\min }\limits_{|z_i| \le {\mathbf{1}}} z_i{\nabla _\mathcal{G}}x_i{z_i}{\nabla _\mathcal{G}}x_i - \frac{4}{\lambda }z_i{\nabla _\mathcal{G}}{x_i^{0}}$.
%\begin{equation}
%\label{opt_z}
%z_i = \mathop {\arg\min }\limits_{|z_i| \le {\mathbf{1}}} z_i{\nabla _\mathcal{G}}x_i{z_i}{\nabla _\mathcal{G}}x_i - \frac{4}{\lambda }z_i{\nabla _\mathcal{G}}{x_i^{0}}
%\end{equation}
Specifically, we employ the majorization-minimization (MM) method \cite{figueiredo2007majorization} to optimize $z_i$ by solving this constrained minimization problem (the detailed derivation process is given in Eq. \ref{function1} to Eq. \ref{final2} of Section \ref{section5.1} of Supplementary), where $z_i$ can be iteratively refined by:
\begin{equation}
\vspace{-0.3em}
\label{clip}
{z_i^l} = clip(\underbrace{{z_i^{l - 1}} + \frac{2}{{\beta \lambda }}{\nabla _\mathcal{G}}{x_i}}_b,1) = \left\{ {\begin{array}{*{20}{c}}
b\\
1\\
{ - 1}
\end{array}} \right.\begin{array}{*{20}{c}}
{|b| \le 1}\\
{b > 1}\\
{b < -1}
\end{array}
\vspace{-0.1em}
\end{equation}
$\beta$ is a hyper-parameter that is required to be no less than the largest eigenvalue of $(\nabla _\mathcal{G}x_i) (\nabla _\mathcal{G}x_i)^\intercal$.
%The $clip(\cdot)$ function is defined as:
% \begin{equation}
% \label{clip}
% clip(b,1): = \left\{ {\begin{array}{*{20}{c}}
% b\\
% 1\\
% { - 1}
% \end{array}} \right.\begin{array}{*{20}{c}}
% {|b| \le 1}\\
% {b > 1}\\
% {b < -1}
% \end{array}
%.
%\end{equation}

%where we introduce a new variable $b={z^{l - 1}_{i}} + \frac{2}{{\beta \lambda }}{\nabla _\mathcal{G}}{x}_{i}$ for clarity.

% Once we optimize the $z$ of the current layer, replacing $z$ with $({z^{l - 1}} + \frac{2}{{\beta \lambda }}{\nabla _\mathcal{G}}{x})$ in Eq. \ref{first_layer}, of which intuition is that each element in $\hat{x}_i$ is essentially the enhancement of connectivity degree within graph neighborhood since $\left(\nabla_{\mathcal{G}} x\right)_{i j}=w_{i j}\left(x_i-x_j\right)$, herein we set $j=x+1$. ${\nabla _\mathcal{G}}{z}$ measures the difference of the graph gradient, and appropriate adjustment of the balance factor $\lambda$ can reasonably moderate the smooth attenuation degree. Overall, this approach allows us to incorporate the optimized local connectivity information within the graph neighborhood to learn better embedding. This is particularly useful when dealing with graph-structured data, where the underlying connectivity pattern can greatly impact the pattern of the message passing in the graph.


% where we introduce a new variable $b={z^{l - 1}} + \frac{2}{{\beta \lambda }}{\nabla _\mathcal{G}}{x}$ for clarity.

%with $z^{0}=\mathbf{0}$ and $\beta \ge maxeig(\nabla _\mathcal{G} \nabla _\mathcal{G}^T)$. An iterative clipping function is defined as

%where $sign(\cdot)$ is the signum function and returns 1 if the element is greater than zero, 0 if it equals zero and -1 if it is less than zero.

\begin{figure}
  \centering
  {\includegraphics[width=0.9\textwidth,trim=0 0 0 0,clip]{Fig3.pdf}}
  \vspace{-0.5em}
  \caption{\small Our new deep model integrates a novel diffusion-clip (DC) layer (for selective graph diffusion) after the conventional fully-connected (FC)layer (for graph representation learning).}
  \label{gcn_clip}
 \vspace{-2.0em}
\end{figure}

\textbf{Develop new GNN network architecture with a selective inductive bias.} The building block in vanilla GNN \cite{kipf2016semi} is a FC (fully-connected) layer where the input is the embedding vectors after isotropic graph diffusion (in ${\ell _2}$-norm). Since the estimation of graph embeddings $x$ in Eq. \ref{first_layer} depends on the latest estimation of $z^{(l)}$, such recursive \textit{min-max} solution for Eq. \ref{obj_fun} allows us to devise a new network architecture that disentangles the building block in vanilla GNN into the feature representation learning and graph diffusion underling TV. As shown in Fig. \ref{gcn_clip}, we first deploy a FC layer to update the graph embeddings $x^{(l)}$. After that, we concatenate a diffusion-clip (DC) layer for selective graph diffusion, which sequentially applies (1) node-adaptive graph diffusion (blue arrow in Fig. \ref{clip}) on $x^{(l)}$ by Eq. \ref{first_layer} \footnote{Since the optimization schema has been switched to the layer-by-layer manner, the initialization $x_0$ becomes $x^{(l-1)}$ from the previous layer.}, and (2) clip operation (purple arrow in Fig. \ref{gcn_clip}) to each $x^{(l)}_i$ by Eq. \ref{clip}.

\textbf{Remarks.} Eq. \ref{clip} indicates that larger connective degree results in larger value of $z$. Thus, the DC layer shifts the diffusion patterns by penalizing the inter-community information exchange (due to strong connections) while remaining the heat-kernel diffusion within the community. The preference of such link-adaptive diffusion can be adjusted by the hyper-parameter $\lambda$ \footnote{$\lambda$ can be either pre-defined or learned from the data.} in Eq. \ref{obj_fun}. Recall our intuitive solution for over-smoothing problem in Fig. \ref{demo}, the DC layer offers the exact global insight of graph topology to keep the node embeddings distinct between nodes \#1 and \#2. We demonstrate the effect of DC layer on the real-world graph data in Fig. \ref{figs5} of Supplementary document.

\begin{figure}
  \centering
  \includegraphics[scale=0.5]{Fig4.pdf}
  \vspace{-1.5em}
  \caption{\small (a) The illustration of the computational challenge for estimating the spreading flow. (b) The GAN architecture for \textit{min-max} optimization in the network.}
  \label{fig_flownet}
  \vspace{-1.5em}
\end{figure}

\subsubsection{Predict flow dynamics through graph neural transport equation}
\label{flownet}
\textbf{Problem formulation.} We live in a world of complex systems, where everything is intricately connected in multiple ways. A holistic insight of how the system's components interact with each other and how changes in one part of the system can affect the behavior of the whole sheds new light on the dynamic behaviors of these complex systems over time. However, oftentimes it is an ill-posed problem. Taking the toy system in Fig. \ref{fig_flownet}(a) as an example, while it is simple to calculate the future focal patterns based on the focal patterns at the current time point and the node-to-node flow information, determining flow dynamics based on longitudinal nodal observations is computationally hard since the solution is not unique.

The na\"ive solution to predict the spreading flow is to (1) train a GNN to learn the intrinsic node embeddings and (2) predict the flow based on the difference of learned embeddings. However, this two-step approach might suffer from vanishing flow due to over-smoothing in GNNs. Following the spirit of \textit{Brachistochrone} problem, we ask the question "What flow field $f(t) =[f_{ij}(t)]_{i,j=1}^N$ underlines the system mechanics to the extent that it is able to predict the behaviors in the future?"
%The objective function consists of two parts. One is supervised loss that characterizes the prediction accuracy between collected $x_i(t+1)$ and prediction $\hat{x}_i(t+1)=x_i(t)+\sum\nolimits_{j = 1}^N {{f_{ij}}(t)}$. Anther one is the regularization term that we

%\paragraph{Develop flow-net based on neural transport equation}
%As commonly acknowledged, GNN utilizes message passing to facilitate information exchange between graph nodes and their neighbors. Through this process, nodes can iteratively update their embedding vectors layer by layer by aggregating their neighbors' node features while adhering to the local graph topology structure. Therefore, accurately quantifying the intrinsic flow of information, such as flux and bandwidth, is crucial for understanding information aggregation in complex networks. In this context, the overarching goal is to estimate the time-dependent flow field $f(t) =[f_{ij}(t)]_{i,j=1}^N$ of message passing such that $x_i(t+1)=x_i(t)+\sum\nolimits_{j = 1}^N {{f_{ij}}(t)}$, where $f_{ij}(t)$ stands for the directed flow from the region $v_i$ to $v_j$. To cast this ill-posed problem into a well-defined formulation, we conceptualize that the message propagation in each brain forms a unique dynamic transport system of the brain network, and the spreading flow is driven by a potential energy field $u(t)=[u_i(t)]_{i=1}^N$, where $u_i(t)$ is output of a nonlinear process $\phi$ reacting to the infromation accumulation $x_i$ at the underlying region $v_i$, i.e., $u_i=\phi(x_i)$. Similar to the gravity field driving water flow, the potential energy field is the latent dominated force of the message propagation throughout the brain. Thereby the spreading flow of message aggregates can be defined as the gradient of potential energy between two connected regions by:
%\begin{equation}
%\label{flowequation}
%  f_{i j}(t)=-\left(\nabla_\mathcal{G} u(t)\right)_{i j}=-w_{i j}\left(u_i(t)-u_j(t)\right)
%\end{equation}
%where the negative sign ahead of $\nabla_\mathcal{G}$ indicates the flow directionality. Thus, the fundamental insight of our model is that the spreading flow $f_{ij}(t)$ is formulated as an “energy transport” process of the tau potential energy field. Taking together, the output of our model is a mechanistic equation $M(\cdot)$ of the dynamic system that can predict the future flow based on the history flow sequences, i.e., $ \hat{f}_{ij}(t_T)=M(f_{ij}(t_1),...,f_{ij}(t_{T-1}))$.

In this paper, we focus on the conservative system of energy transportation \cite{arnol2013mathematical}. The system mechanics is formulated as:
\begin{equation}
\label{pdes}
  \frac{{dx}}{{dt}} + div(q) = 0
\end{equation}
where $q=[q_{ij}]_{i,j=1}^N$ is the flux field which propagates the potential energy $u(t)=[u_i(t)]_{i=1}^N$ (conserved quantity) over time. Similar to a gravity field driving water flow, the intuition of Eq. \ref{pdes} is that the change of energy density $u$ (we assume there is a non-linear mapping $\phi$ from external force $x$ to $u$, i.e., $u_i=\phi (x_i)$) leads to energy transport throughout the entire graph. As flux is closely related to the difference of energy $\nabla_{\mathcal{G}}u$ underlying the graph topology, we assume the energy flux $q$ is regulated by the potential energy field $\nabla_{\mathcal{G}}u$, i.e., $q=\alpha \otimes \nabla_{\mathcal{G}}u$, where $\alpha=[\alpha_{ij}]_{i,j=1}^N$ is a learnable matrix characterizing the link-wise contribution of each energy potential $\nabla_{\mathcal{G}}u_{ij}$ to the potential energy flux $q_{ij}$.

%As flux is often defined as the rate of flow \cite{SerwayJewett2017}, we consider the energy flux as $q_{ij}=\alpha \cdot f_{ij}$, where $\alpha$ is a learnable variable characterizing the link-wise contribution of each spreading flow $f_{ij}$ to the potential energy flux $q_{ij}$.
% Herein, we consider the spreading flow is driven by the potential energy field (PEF) $u(t)=[u_i(t)]_{i=1}^N$, where $u_i(t)$ is the output of a nonlinear process $\phi$ reacting to the information accumulation $x_i$ at the underlying region $v_i$, i.e., $u_i=\phi (x_i)$. Similar to the gravity field driving water flow, the potential energy field is the latent dominated force of the information propagation throughout the network.
%To simplify the problem, we define the directed spreading flow following the node-to-node energy deficit as: $f_{ij}=\nabla_{\mathcal{G}}u=w_{ij}(u_i-u_j)$.
By plugging $q=\alpha \otimes \nabla_{\mathcal{G}}u$ into Eq. \ref{pdes}, the energy transport process can be reformulated as:
\begin{equation}
\label{transport}
  \frac{{\partial u}}{{\partial t}} = - \phi^{-1} (\alpha \otimes div(\nabla_{\mathcal{G}}u)) = - \phi^{-1} (\alpha \otimes \Delta u),
\end{equation}
where $\Delta=div(\nabla_{\mathcal{G}})$ is the graph Laplacian operator. Since the PDE in Eq. \ref{transport} is equivalent to the E-L equation of the quadratic functional $\mathcal{J}(u)= \mathop {\min }\limits_u \int_\mathcal{G}  \alpha \otimes | \nabla_{\mathcal{G}} u{|^2}du$ (after taking $\phi$ away), a major issue is the over-smoothness in $u$ that might result in vanishing flows. In this context, we propose to replace the ${\ell _2}$-norm integral functional $\mathcal{J}(u)$ with TV-based counterpart $\mathcal{J}_{TV}(u)=\mathop {\min }\limits_u \int_\mathcal{G} \alpha \otimes | \nabla_{\mathcal{G}} u{|}du$.

\textbf{Renovate new E-L equation -- graph neural transport equations.} Following the \textit{min-max} optimization schema in Eq. \ref{obj_fun}-\ref{clip}, we introduce an auxiliary matrix $f$ to lift the undifferentialable barrier. After that, the minimization of $\mathcal{J}_{TV} (u)$ boils down into a dual \textit{min-max} functional $\mathcal{J}_{TV}(u,f)=\mathop {\min }\limits_{u} \mathop {\max}\limits_{f} \int_\mathcal{G} \alpha \otimes f (\nabla_{\mathcal{G}} u) du$. Since $u(t)$ is a time series, it is difficult to derive the deterministic solutions (as Eq. \ref{first_layer}-\ref{clip}) by letting $\frac{\partial} {{\partial u}}\mathcal{J}_{TV}=0$ and $\frac{\partial} {{\partial f}}\mathcal{J}_{TV}=0$. Instead, we use $G\hat a teaux$ variations to optimize $\mathcal{J}_{TV}(u,f)$ via the following two coupled time-dependent PDEs (please see Section \ref{secs1.2}, Eq. \ref{eqs14} to Eq. \ref{dual2}, in Supplementary for details):
\begin{equation}
\label{dual}
\left\{ {\begin{array}{*{20}{c}}
{\mathop {\max }\limits_f \frac{{df}}{{dt}} = {\alpha \otimes \nabla _\mathcal{G}}u}\\
{\mathop {\min }\limits_u \frac{{du}}{{dt}} = \alpha \otimes div(f)}
\end{array}} \right.
\end{equation}

\textbf{Remarks.} The solution to Eq. \ref{dual} is known as continuous max-flow and constitutes a continuous version of a graph-cut \cite{appleton2005globally}. Since $\alpha$ is a latent variable and potential energy $u$ is given, the maximization of $f$ opts towards maximizing the spreading flow through the lens of $\alpha$. In this regard, the mechanistic role of auxiliary matrix $f$ is essentially the latent (maximized) spreading flows that satisfy $u(t + 1)_i = u(t)_i + \sum\nolimits_{j = 1}^N {{f_{ij}}(t)}$. The potential energy $\hat{u}$ can be solved via a wave equation ($u_{tt}=div(f_t)=\alpha^2 \otimes \Delta u$), where the system dynamics is predominated by the adjusted Lagrangian mechanics $\alpha^2 \otimes \Delta u$. By determining $\alpha$ at a granularity of graph links, we devise a novel GAN model to predict the spreading flows $f$ which not only offers explainability underlying the \textit{min-max} optimization mechanism in Eq. \ref{dual} but also sets the stage to understand system dynamics through machine learning.

%Until now, we assume $\alpha$ is a global variable. However,

%Recalling the introduction at the beginning of Section \ref{2.2}, the TV-based functional is defined as $\mathcal{J}_{TV}(u, f) = \mathop {\max}\limits_f \mathop {\min }\limits_u \int(f \otimes \nabla_\mathcal{G} u) du$, subject to $|f| \le \mathbf{1}^T$. According to the definition of the derivative for regular functions, we assume $u \to u + \varepsilon a$, $f \to f + \varepsilon b$, and the directional derivatives in the directions $a$ and $b$ are defined as $\left.\frac{d \mathcal{J}}{d \varepsilon}(u+\varepsilon a)\right|_{\varepsilon \rightarrow 0}$ and $\left.\frac{d \mathcal{J}}{d \varepsilon}(f+\varepsilon b)\right|_{\varepsilon \rightarrow 0}$. Given a functional $\mathcal{J}_{TV}(u, f)$, its $G\hat a teaux$ variations is formulated by:
% \begin{equation}
%  \mathcal{J}_{T V}(u+\varepsilon a, f+\varepsilon b)=\int(f+\varepsilon b) \cdot(\nabla u+\varepsilon \nabla a) d u \left.\Rightarrow \frac{\partial \mathcal{J}}{\partial \varepsilon}\right|_{\varepsilon \rightarrow 0}=\int[(f \cdot \nabla a) \cdot(\nabla u) b)] d u
% \end{equation}

% Then, we apply the integration by parts rules: $\left.\frac{\partial \mathcal{J}}{\partial \varepsilon}\right|_{\varepsilon \rightarrow 0} = f \cdot a - \int (a \cdot \nabla f)du +  \int ( \nabla u \cdot b)du$. Since we assume either $u$ is given at the boundary (Dirichlet boundary condition), the boundary term $f\cdot a$ can be dropped. After that, the derivative of $\mathcal{J}_{TV}(x, f)$ becomes:
% \begin{equation}
% \label{dirichlet}
% \left.\frac{\partial \mathcal{J}}{\partial \varepsilon}\right|_{\varepsilon \rightarrow 0} = - \int (\nabla f \cdot a  +  \nabla u \cdot b)
% \end{equation}

%Note, we use the adjoint operate $div(f)=-\nabla f$ to approximate the discretization of $\nabla f$ \cite{hyman1997adjoint}, which allows us to link the minimization of $u$ to the classic graph diffusion process.

%Therefore, the alternative solution for Eq. \ref{dual} is that we minimize $u$ through the Lagrangian mechanics defined in the transport equation Eq. \ref{transport} where the system dynamics is predominated by the maximum flow field $f$. Since the accurate estimate of flow field $f(t)$ and PEF $u(t)$ is supposed to predict the future information accumulation $x(t+1)$ by $x_i(t+1)=\phi^{-1}((u_i(t)+\sum\nolimits_{j = 1}^N {{f_{ij}}(t)})$, we can further tailor the \textit{min-max} optimization for Eq. \ref{dual} into a supervised learning scenario, as described below.

\textbf{Develop a GAN model of flow prediction with TV-based Lagrangian Mechanics.} The overall network architecture is shown in Fig. \ref{fig_flownet} (b), which consists of a generator (red solid box) and a discriminator module (blue solid box). Specifically, the generator ($G$) consists of (1) a GCN component \cite{eliasof2021pde} to optimize $\hat{u}$ through the wave equation $u_{tt} =\alpha^2 \otimes \Delta u$ and (2) a FC layer to characterize the non-linear mapping function $\hat{x}(t+1)=\phi ^{-1} (\hat{u}(t))$. In contrast, the discriminator ($D$) is designed to (1) synthesize $\alpha$ and (2) construct the future $\tilde{u}_{t+1}$ based on the current $u_t$ and current estimation of spreading flow $f=\alpha \otimes \nabla_{\mathcal{G}}u$ (orange dash box). To make the network architecture consistent between generator and discriminator modules, we include another GCN to map the synthesized $\tilde {u}(t+1)$ to the external force $\tilde{x}(t+1)$. Note, since the working mechanism of this adversarial model underlines the min-max optimization in the energy transport equation, the nature of predicted spreading flows is carved by the characteristics of \textit{max-flow}.

The driving force of our network is to minimize (1) the MSE (mean square error) between the output of the generator $\hat {x}_{t+1}$ and the observed regional features, (2) the distance between the synthesized regional features  ${\tilde x_{t + 1}}$ (from the discriminator) and the output of generator $\hat {x}_{t+1}$  (from the generator). In the spirit of probabilistic GAN \cite{zhao2016energy}, we use one loss function $\mathcal{L}_D$ to train the discriminator ($D$) and another one $\mathcal{L}_G$ to train the generator ($G$):
\begin{equation}
    \left\{\begin{array}{c}\mathcal{L}_D=D\left(x_{t+1}\right)+\left[\xi-D\left(G\left(x_t\right)\right)\right]^{+} \\ \mathcal{L}_G=D\left(G\left(x_t\right)\right)\end{array}\right.
\end{equation}
where $\xi$ denotes the positive margin and the operator ${[\cdot]^ + } = \max (0,\cdot)$. Minimizing $\mathcal{L}_G$ is similar to maximizing the second term of $\mathcal{L}_D$ except for the non-zero gradient when $D(G(x_t))\ge \xi$.
%\footnote{We use the adjoint operate $div(f)=-\nabla f$ to approximate the discretization of $\nabla f$ \cite{hyman1997adjoint}, which allows us to link the minimization of $u$ to the classic graph diffusion process}.

%initial GNN model of the transport equation that consists of a reaction process $\phi(\cdot)$ and a graph diffusion process $GCN(\cdot)$. Specifically, the generator consists of (i) a MLP to project the input regional feature $x_t$ into the potential energy filed $u_t$ through a nonlinear reaction process $u_t=\phi (x_t)$  (green dashed box), (ii) a GCN layer to transport potential energy along the connectome pathways, resulting in the $u_{t+1}=GCN(u_t)$ (purple dashed box), and (iii) another MLP to generate $\hat {x}_{t+1}$ from $u_{t+1}$ via $\hat {x}_{t+1} = \phi ^{-1} (u_{t+1})$ (red dashed box). The discriminator module is



\section{Experiments}
In this section, we evaluate the performance of the proposed \textit{GNN-PDE-COV} framework with comparison to six graph learning benchmark methods on a wide variety of open graph datasets \cite{sen2008collective}, as well as a proof-of-concept application of uncovering the propagation pathway of pathological events in Alzheimer's disease (AD) from the longitudinal neuroimages.

 \subsection{Datasets and experimental setup}

 \textbf{Dataset and benchmark methods.} We evaluate the new GNN models derived from our proposed GNN framework in two different applications. \textit{First}, we use three standard citation networks, namely \textit{Cora}, \textit{Citeseer}, and \textit{Pubmed} \cite{sen2008collective} for node classification (the detailed data statistic is shown in Table \ref{tables3} of Supplementary). We adopt the public fixed split \cite{yang2016revisiting} to separate these datasets into training, validation, and test sets. We follow the experimental setup of \cite{chen2020simple} for a fair comparison with six benchmark GNN models (vanilla GCN \cite{kipf2016semi}, GAT \cite{velivckovic2017graph}, GCNII \cite{chen2020simple}, ResGCN \cite{li2019deepgcns}, DenseGCN \cite{li2019deepgcns}, GRAND \cite{chamberlain2021grand}). Since our DC-layer can be seamlessly integrated into existing GNNs as a plug-in. The corresponding new GNN models (with DC-layer) are denoted GCN+, GAT+, GCNII+, ResGCN+, DenseGCN+, and GRAND+, respectively.

 \textit{Second}, we apply the GAN model in Section \ref{flownet} to predict the concentration level of AD-related pathological burdens and their spreading pathways from longitudinal neuroimages. Currently, there is no \textit{in-vivo} imaging technique that can directly measure the flow of information across brain regions. Here, our computational approach holds great clinical value to understand the pathophysiological  mechanism involved in disease progression \cite{jones2017tau}. Specifically, we parcellate each brain into 148 cortical surface regions and 12 sub-cortical regions using Destrieux atlas \cite{destrieux2010automatic}. The wiring topology of these 160 brain regions is measured by diffusion-weighted imaging \cite{bammer2003basic} and tractography techniques \cite{glasser2008dti}. The regional concentration levels AD pathology including amyloid, tau, and  fluorodeoxyglucose (FDG) and cortical thickness (CoTh) are measured from PET (positron emission tomography) and MRI (magnetic resonance imaging) scans \cite{jack2018nia}. We use a total of $M=1,291$ subjects from ADNI \cite{ADNI}, each having longitudinal imaging data (2-5 time points). The details of image statistics and pre-processing are shown in Sec. \ref{datapre}. Since we apply the flow prediction model to each modality separately, we differentiate them with \textit{X-FlowNet} ($X$ stands for amyloid, tau, FGD, and CoTh).

% we use the ADNI database \cite{ADNI}, of which study aims to prevent and treat Alzheimer’s disease. We use imaging features (often referred to as biomarkers) such as standard uptake value ratio (SUVR) levels from amyloid, tau, and fluorodeoxyglucose (FDG)-positron emission tomography (PET), as well as cortical thickness from MRI. The brain regions are parceled into 148 cortical surface regions and 12 sub-cortical regions using Destrieux atlas \cite{destrieux2010automatic}. We construct a $160 \times 160$ structural network by applying tractography on diffusion-weighted imaging (DWI) to calculate the white matter fibers connecting the brain regions. The SUVR normalization is performed using Cerebellum as the reference. The prediction experiment involves a total of $M=1291$ subjects, each having longitudinal PET data (2-5 time points). Following the clinical outcomes, we partition the subjects into the cognitive normal (CN), early-stage mild cognitive impairment (EMCI), late-stage mild cognitive impairment (LMCI), and AD groups. To facilitate population counts, we regard CN and EMCI as "CN-like" group, while LMCI and AD as "AD-like" groups. Table \ref{table1} in Supplementary summarizes the statistics of two involved datasets.

 \textbf{Experimental setup}. In the node classification task, we verify the effectiveness and generality of DC-layer in various number of layers ($L={2, 4, 8, 16, 32, 64, 128}$). All baselines use their own default parameter settings. Evaluation metrics include accuracy, precision and F1-score. To validate the performance of \textit{X-FlowNet}, we examine (1) prediction accuracy (MAE) of follow-up concentration level, (2) prediction of the risk of developing AD using the baseline scan, and (3) understand the propagation mechanism in AD by revealing the node-to-node spreading flows of neuropathologies.

The main results of graph node classification and flow prediction are demonstrated in Section \ref{exp1} and \ref{exp2}, respectively. Other supporting results such as ablation study and hyper-parameter setting are shown in Section \ref{s2} of the Supplementary document.

 \begin{table} [h!]
  % \vspace{-0.5em}
  \caption{Test accuracies (\%) on citation networks. We show the mean value, the quota of increase ($\uparrow$)/decrease($\downarrow$) after adding DC layer. Statistical significance is determined from 50 resampling tests. `$*$' means statistically significance with $p \le 0.05$, `$**$' denotes  $p \le 0.01$. The missing results are due to the huge consumption of GPU memory for large graphs (DenseGCN) or gradient explosions (GAT) or non-convergence (GRAND). The best performance of baselines is denoted in blue, while the best performance after adding the DC layer is denoted in red.}
  \renewcommand\arraystretch{0.2}
  % \tabcolsep=0.01cm
  \label{table2}
  \centering
   \scalebox{0.8}{\begin{tabular}{clllllllll}
    \toprule
     \textbf{Dataset}& \textbf{Model} & $L=2$ & $L=4$ & $L=8$& $L=16$& $L=32$ & $L=64$ & $L=128$\\
    \midrule
    \multirow{34}{*}{\textbf{Cora}} & GCN  & \textcolor{blue}{81.30}  & 79.90& 75.70 &25.20 & 20.00&31.80&20.80\\
    &\textbf{GCN+} & $\textbf{\textcolor{red}{82.70}}_{1.40 \uparrow }^{**}$  & $\textbf{\textcolor{red}{82.70}}_{2.80 \uparrow }^{**}$& $\textbf{82.30}_{6.60 \uparrow }^{**}$ &$\textbf{70.60}_{45.4 \uparrow }^{**}$ & $\textbf{67.80}_{47.8 \uparrow }^{**}$&$\textbf{66.60}_{34.8 \uparrow }^{**}$&$\textbf{59.90}_{39.1 \uparrow }^{**}$\\
     \cmidrule(r){2-9}
     & GAT   & \textcolor{blue}{82.40}  & 80.30& 57,90 &31.90 & \multicolumn{1}{c}{--}&\multicolumn{1}{c}{--}&\multicolumn{1}{c}{--}\\
      &\textbf{GAT+} & $\textbf{\textcolor{red}{82.60}}_{0.20 \uparrow }^{}$ & $\textbf{80.50}_{0.20 \uparrow }^{}$& $\textbf{69.70}_{11.8 \uparrow }^{**}$ &$\textbf{66.00}_{34.1 \uparrow }^{**}$ & $\textbf{63.60}_{63.6 \uparrow }^{**}$&$\textbf{54.60}_{54.6 \uparrow }^{**}$&$\textbf{45.70}_{45.7 \uparrow }^{**}$\\
     \cmidrule(r){2-9}
    & GRAND  & 80.00  & 82.64& 82.74 &\textcolor{blue}{83.45} &81.83 &80.81&79.19\\
     &\textbf{GRAND+} & $\textbf{{81.93}}_{1.93 \uparrow }^{**}$  &$\textbf{{83.45}}_{0.81 \uparrow }^{**}$& $\textbf{{82.95}}_{0.20 \uparrow }^{}$ &$\textbf{\textcolor{red}{84.27}}_{1.32 \uparrow }^{**}$ & $\textbf{83.15}_{0.71 \uparrow }^{*}$&$\textbf{81.52}_{0.71 \uparrow }^{**}$&$\textbf{80.10}_{0.91 \uparrow }^{**}$\\
     \cmidrule(r){2-9}
    & ResGCN  & 76.30  & 77.30& 76.20 &\textcolor{blue}{77.60} & 73.30&31.90&31.90\\
     &\textbf{ResGCN+}& $\textbf{77.80}_{1.50 \uparrow }^{**}$  & $\textbf{78.70}_{1.40 \uparrow }^{**}$& $\textbf{\textcolor{red}{78.80}}_{2.60 \uparrow }^{**}$ &$\textbf{78.60}_{1.00 \uparrow }^{**}$ & $\textbf{76.90}_{3.60 \uparrow }^{**}$&$\textbf{76.80}_{44.9 \uparrow }^{**}$&$\textbf{33.60}_{1.70 \uparrow }^{**}$\\
     \cmidrule(r){2-9}
    & DenseGCN & 76.60  & \textcolor{blue}{78.50}& 76.00 &\multicolumn{1}{c}{--} &\multicolumn{1}{c}{--} &\multicolumn{1}{c}{--}&\multicolumn{1}{c}{--}\\
    &\textbf{DenseGCN+}& $\textbf{78.00}_{1.40 \uparrow }^{**}$  & $\textbf{\textcolor{red}{78.70}}_{0.20 \uparrow }^{}$& $\textbf{76.90}_{1.40 \uparrow }^{**}$ &\multicolumn{1}{c}{--} & \multicolumn{1}{c}{--}&\multicolumn{1}{c}{--}&\multicolumn{1}{c}{--}\\
     \cmidrule(r){2-9}
    &GCNII & 76.40  & 81.90& 81.50 &84.80 &84.60 &\textcolor{blue}{85.50}&85.30\\
    &\textbf{GCNII+}&$\textbf{84.70}_{8.30 \uparrow }^{**}$ & $\textbf{84.80}_{2.90 \uparrow }^{**}$& $\textbf{84.70}_{3.20 \uparrow }^{**}$ &$\textbf{85.20}_{0.40 \uparrow }^{**}$ & $\textbf{85.40}_{0.80 \uparrow }^{**}$&$\textbf{\textcolor{red}{86.30}}_{0.80 \uparrow }^{*}$&$\textbf{85.60}_{0.30 \uparrow }^{}$\\
    \cmidrule(r){1-9}
    {\multirow{34}*{\textbf{Citeseer}} }  & GCN  &\textcolor{blue}{70.20}  &62.50 & 62.90 &21.00 & 17.90&22.90&19.80\\
    &\textbf{GCN+}& $\textbf{\textcolor{red}{72.90}}_{2.70 \uparrow }^{**}$  &$\textbf{67.30}_{4.80 \uparrow }^{**}$ &$\textbf{72.00}_{9.10 \uparrow }^{**}$  &$\textbf{54.70}_{33.7 \uparrow }^{**}$ & $\textbf{50.30}_{32.4 \uparrow }^{**}$&$\textbf{48.40}_{25.5 \uparrow }^{**}$&$\textbf{46.60}_{26.8 \uparrow }^{**}$\\
     \cmidrule(r){2-9}
     & GAT   & \textcolor{blue}{71.70}  & 58.60& 26.60 &18.10 & \multicolumn{1}{c}{--}&\multicolumn{1}{c}{--}&\multicolumn{1}{c}{--}\\
      &\textbf{GAT+}& $\textbf{\textcolor{red}{73.00}}_{1.30 \uparrow }^{**}$  & $\textbf{69.50}_{10.9 \uparrow }^{**}$& $\textbf{47.60}_{21.0 \uparrow }^{**}$ &$\textbf{31.80}_{13.7 \uparrow }^{**}$ & $\textbf{31.30}_{31.3 \uparrow }^{**}$&$\textbf{30.60}_{30.6 \uparrow }^{**}$&$\textbf{29.30}_{29.3 \uparrow }^{**}$\\
     \cmidrule(r){2-9}
    & GRAND  & 71.94  & 72.58& 73.87 &75.00 &\textcolor{blue}{75.16} &72.90&69.52\\
     &\textbf{GRAND+}&$\textbf{72.26}_{0.32 \uparrow }^{*}$  & $\textbf{73.55}_{0.97 \uparrow }^{**}$& $\textbf{75.16}_{1.29 \uparrow }^{**}$ &$\textbf{\textcolor{red}{75.65}}_{0.65 \uparrow }^{*}$ & $\textbf{75.52}_{0.36 \uparrow }^{*}$&$\textbf{74.52}_{1.62 \uparrow }^{*}$&$\textbf{72.26}_{2.74 \uparrow }^{**}$\\
     \cmidrule(r){2-9}
    & ResGCN  & \textcolor{blue}{67.10}  &66.00& 63.60 &65.50 &62.3 &18.80&18.10\\
     &\textbf{ResGCN+}& $\textbf{\textcolor{red}{68.00}}_{0.90 \uparrow }^{**}$  & $\textbf{67.60}_{1.60 \uparrow }^{**}$& $\textbf{66.00}_{2.40 \uparrow }^{**}$ &$\textbf{66.00}_{0.50 \uparrow }^{*}$ & $\textbf{65.80}_{3.50 \uparrow }^{**}$&$\textbf{24.00}_{5.20 \uparrow }^{**}$&$\textbf{24.30}_{6.20 \uparrow }^{**}$\\
     \cmidrule(r){2-9}
    & DenseGCN  & \textcolor{blue}{67.40}  & 64.00& 62.20 &\multicolumn{1}{c}{--} &\multicolumn{1}{c}{--} &\multicolumn{1}{c}{--}&\multicolumn{1}{c}{--}\\
    &\textbf{DenseGCN+}& $\textbf{\textcolor{red}{67.80}}_{0.40 \uparrow }^{*}$  &$\textbf{66.60}_{2.60 \uparrow }^{**}$& $\textbf{64.70}_{2.50 \uparrow }^{**}$ &\multicolumn{1}{c}{--} & \multicolumn{1}{c}{--}&\multicolumn{1}{c}{--}&\multicolumn{1}{c}{--}\\
     \cmidrule(r){2-9}
    & GCNII  & 66.50  & 67.80& 69.30 & 71.60&\textcolor{blue}{73.10} &71.40 &70.20\\
    &\textbf{GCNII+}& $\textbf{72.40}_{5.90 \uparrow }^{**}$   & $\textbf{73.30}_{5.5 \uparrow }^{**}$ & $\textbf{73.80}_{4.50 \uparrow }^{**}$  &$\textbf{73.40}_{1.80 \uparrow }^{**}$  & $\textbf{73.80}_{0.70 \uparrow }^{**}$ &$\textbf{\textcolor{red}{74.60}}_{3.20 \uparrow }^{**}$ &$\textbf{73.90}_{3.70 \uparrow }^{**}$ \\
    \cmidrule(r){1-9}
       {\multirow{34}*{\textbf{Pubmed}} }  & GCN  & \textcolor{blue}{78.50}  & 76.50& 77.30 &40.90 & 38.20&38.10&38.70\\
    &\textbf{GCN+}& $\textbf{\textcolor{red}{79.80}}_{1.30 \uparrow }^{**}$  & $\textbf{79.10}_{2.60 \uparrow }^{**}$& $\textbf{78.20}_{0.90 \uparrow }^{**}$ &$\textbf{77.40}_{36.5 \uparrow }^{**}$ & $\textbf{76.20}_{38.0 \uparrow }^{**}$&$\textbf{75.10}_{37.0 \uparrow }^{**}$&$\textbf{73.00}_{34.3 \uparrow }^{**}$\\
     \cmidrule(r){2-9}
     & GAT   & 77.40  & 72.20& \textcolor{blue}{77.80} &40.70 & \multicolumn{1}{c}{--}&\multicolumn{1}{c}{--}&\multicolumn{1}{c}{--}\\
      &\textbf{GAT+}& $\textbf{77.90}_{0.50 \uparrow }^{*}$  & $\textbf{77.30}_{5.10 \uparrow }^{**}$& $\textbf{\textcolor{red}{78.50}}_{0.70 \uparrow }^{*}$ &$\textbf{73.50}_{32.8 \uparrow }^{**}$ & $\textbf{68.20}_{68.2 \uparrow }^{**}$&$\textbf{66.80}_{66.8 \uparrow }^{**}$&$\textbf{63.50}_{63.5 \uparrow }^{**}$\\
     \cmidrule(r){2-9}
    & GRAND  & 77.07  & 77.94& 78.29 &\textcolor{blue}{79.93} &79.12 &\multicolumn{1}{c}{--}&\multicolumn{1}{c}{--}\\
     &\textbf{GRAND+}& $\textbf{78.03}_{0.96 \uparrow }^{**}$  & $\textbf{78.34}_{0.40 \uparrow }^{*}$& $\textbf{\textcolor{red}{80.21}}_{1.92 \uparrow }^{**}$ &$\textbf{80.08}_{0.15 \uparrow }^{**}$ &$\textbf{79.32}_{0.20 \uparrow }^{}$&\multicolumn{1}{c}{--}&\multicolumn{1}{c}{--}\\
     \cmidrule(r){2-9}
    & ResGCN  & 76.30  & 77.30& 76.20 &\textcolor{blue}{77.60} &73.30 &31.90 &31.90\\
     &\textbf{ResGCN+}& $\textbf{77.80}_{1.50 \uparrow }^{**}$  &$\textbf{78.70}_{1.40 \uparrow }^{**}$& $\textbf{\textcolor{red}{78.80}}_{2.60 \uparrow }^{*}$ &$\textbf{78.60}_{1.00 \uparrow }^{**}$ & $\textbf{76.90}_{3.60 \uparrow }^{**}$&$\textbf{76.80}_{44.90 \uparrow }^{**}$&$\textbf{32.00}_{0.10 \uparrow }^{}$\\
     \cmidrule(r){2-9}
    & DenseGCN  & 75.80  & \textcolor{blue}{76.10}& 75.80 &\multicolumn{1}{c}{--} &\multicolumn{1}{c}{--} &\multicolumn{1}{c}{--}&\multicolumn{1}{c}{--}\\
    &\textbf{DenseGCN+}& $\textbf{76.10}_{0.30 \uparrow }^{}$  & $\textbf{76.70}_{0.60 \uparrow }^{*}$& $\textbf{\textcolor{red}{77.50}}_{1.70 \uparrow }^{**}$ &\multicolumn{1}{c}{--}&  \multicolumn{1}{c}{--}&\multicolumn{1}{c}{--}&\multicolumn{1}{c}{--}\\
     \cmidrule(r){2-9}
    & GCNII  & 77.30  & 78.80& 79.50 &79.70 &\textcolor{blue}{79.90} &0.7980&79.70\\
    &\textbf{GCNII+}& $\textbf{78.40}_{1.10 \uparrow }^{**}$   &$\textbf{\textcolor{red}{80.10}}_{1.30 \uparrow }^{**}$ & $\textbf{80.00}_{0.60 \uparrow }^{*}$  &$\textbf{\textcolor{red}{80.10}}_{0.30 \uparrow }^{}$  & $\textbf{80.00}_{0.20 \uparrow }^{}$ &$\textbf{80.00}_{0.20 \uparrow }^{}$ & $\textbf{\textcolor{red}{80.10}}_{0.40 \uparrow }^{*}$ \\
    \bottomrule
  \end{tabular}}
  % \vspace{-0.5em}
\end{table}

% Regarding the prediction accuracy of future tau/amyloid/FDG/cortical thickness accumulation, we use the second framework we propose to perform the experiment (as shown in Fig. \ref{fig_flownet} (b)). We conduct an ablation study using a two-step approach. First, we train the model (MLP+GNN) shown in the left panel of Fig. \ref{fig_flownet} (b) to predict the PEF based on the transport equation, then compute the flows using Eq. \ref{flowequation}, followed by a GCN-based model to obtain further biomarker accumulation. Since the deep model in this two-step approach is also formalized from the PDE, we call this degraded version as \textit{PDENet}. All parameters of comparison methods use their own default setting. We use 5-fold cross-validation to evaluate the prediction performance (i.e., the future tau/amyloid/FDG/cortical thickness accumulation) and particularly examine the spreading patterns of tau aggregates in different clinic groups (AD and CN). Specifically, We select the entorhinal cortex (EC) as the seed point and track the inward/outward tau spreading flows on EC (this region is frequently reported in tau propagation research \cite{vogel2020spread,lee2022regional}). We use mean absolute error (MAE) of prediction accuracy as evaluation criteria.

%\textit{Note:} More details about the experimental settings and results are given in Supplementary.

\subsection{Experimental results on graph node classification}
\label{exp1}
% We conduct experiments to evaluate the performance of our proposed TV filter block on different numbers of GCN layers (2, 4, 8, 16, 32, 64, 128).
We postulate that by mitigating the over-smoothing issue, we can leverage the depth of GNN models to effectively capture complex feature representations in graph data. As shown in Table \ref{table2}, we investigate the graph node classification accuracy as we increase the number of GNN layers by six benchmark GNN models and their corresponding plug-in models (indicated by '+' at the end of each GNN model name) with the DC-layer.
The results demonstrate that: (1) the new GNN models generated from the \textit{GNN-PDE-COV} framework have achieved SOTA in \textit{Cora} (86.30\% by GCNII+), \textit{Citeseer} (75.65\% by GRAND+), and \textit{Pubmed} (80.10 \% by GCNII+); (2) all of new GNN models outperforms their original counterparts with significant improvement in accuracy; (3) the new GNN models exhibit less sensitivity to the increase of model depth compared to current GNN models; (4) the new GNN models are also effective in resolving the gradient explosion problem \cite{li2018deeper} (e.g, the gradient explosion occurs when training GAT on all involved datasets with deeper than 16 layers, while our GAT+ can maintain reasonable learning performance even reaching 128 layers.)

It is important to note that due to the nature of the graph diffusion process, graph embeddings from all GNN models (including ours) will eventually become identical after a sufficiently large number of layers \cite{coifman2006diffusion}. However, the selective diffusion mechanism (i.e., penalizing excessive diffusion across communities) provided by our \textit{GNN-PDE-COV} framework allows us to control the diffusion patterns and optimize them for specific graph learning applications.

%It is clear that compared to the benchmark methods, the experimental results reveal significantly improved with the addition of our proposed \textit{DC} layer. The effectiveness of the TV regularizer is evident as it penalizes large variations and enhances boundary information across communities, resulting in better preservation of the discriminative power of node features and preventing over-smoothing. This overcomes the limitations of conventional regularization methods in GNNs. In addition, we observed an interesting phenomenon: when running GAT with 32 or more layers, the gradient explosion occurs, which prevents the model from converging. However, our method can alleviate this problem by using the proposed $clip(\cdot)$ function.

\subsection{Application for uncovering the propagation mechanism of pathological events in AD}
\label{exp2}
\textit{First}, we evaluate the prediction accuracy between the ground truth and the estimated concentration values by our \textit{X-FlowNet} and six benchmark GNN methods.
The statistics of MAE (mean absolute error) by \textit{X-FlowNet}, GCN, GAT, GRAND, ResGCN, DenseGCN and GCNII, at different noise levels on the observed concentration levels, are shown in Fig. \ref{experiments2} (a). It is clear that our \textit{X-FlowNet} consistently outperforms the other GCN-based models in all imaging modalities.

\textit{Second}, we have evaluated the potential of disease risk prediction and presented the results in Table \ref{tables2} in Supplementary document, where our \textit{GNN-PDE-COV} model not only achieved the highest diagnostic accuracy but also demonstrated a significant improvement (paired \textit{t}-test $p<0.001$) in disease risk prediction compared to other methods. These results suggest that our approach holds great clinical value for disease early diagnosis.

%(indicated by no noise). To further validate the robustness of the proposed framework, we add uncorrelated additive Gaussian noise levels (std=0.02 to std=0.1) to the observed concentration levels. The prediction accuracies with respect to different noise levels are shown in the rest bars (different patterns) of Fig. \ref{experiments2} (a).

\textit{Third}, we examine the spreading flows of tau aggregates in CN (cognitively normal) and AD groups. As the inward and outward flows shown in Fig. \ref{experiments2}(b), it is evident that there are significantly larger amount of tau spreading between sub-cortical regions and entorhinal cortex in CN (early sign of AD onset) while the volume of subcortical-entorhinal tau spreading is greatly reduced in the late stage of AD. This is consistent with current clinical findings that tau pathology starts from sub-cortical regions and then switches to cortical-cortical propagation as disease progresses \cite{lee2022regional}. However, our \textit{Tau-FlowNet} offers a fine-granularity brain mapping of region-to-region spreading flows over time, which provides a new window to understand the tau propagation mechanism in AD etiology \cite{dujardin2022tau}.

%Mounting evidence shows that the entorhinal cortex (EC) is one of the hubs that receive tau aggregates from  subcortical  regions and then spread to the entire cortex \cite{lee2022regional}. In light of this, we examine both the inward and outward spreading flows between the entorhinal cortex and subcortical regions. Fig. \ref{experiments2} (b) shows the flow degrees of tau spreading on CN and AD (shadow) groups, which is consistent with current clinical findings \cite{lee2022regional}. First, we find very active spreading of tau aggregates in the early stage of AD, while most of the tau aggregates are transported from subcortical regions to the entorhinal cortex. Second, the volume of tau spreading between the subcortex and entorhinal has been significantly reduced in the late stage of AD, as indicated by the total volume of tau spreading flows. The reduction of subcortex-entorhinal flows can be explained by the fact that the majority of tau aggregates spread all over the cortex, instead of subcortical regions.

\begin{figure}[h!]
    % \vspace{-1.1em}
  \centering
    {\includegraphics[width=0.99\textwidth,trim=0 12 0 15,clip]{Fig5.pdf}}
  \caption{\small (a) Prediction accuracy by \textit{X-FlowNet} and six benchmark GNN models w.r.t. various noise levels. (b) The subcortical$\shortrightarrow$cortical tau flows are profound in CN. But in AD, there is a diminished extent of such flows.}
  \label{experiments2}
    % \vspace{-1.8em}
\end{figure}

\section{Conclusion}
In this work, we present the \textit{GNN-PDE-COV} framework to re-think and re-design GNN models with great mathematical insight. On top of this, we devise the selective inductive bias to address the over-smoothing problem in GNN and develop new GNN model to predict the pathology flows \textit{in-vivo} via longitudinal neuroimages. Future work may involve exploring innovative graph regularization techniques and conducting further validation on a broader range of graph-based learning tasks.

\newpage

\begin{center}
  \Large
  \textbf{Supplementary}
% \maketitle
\end{center}
\section{Solving variational problems: From objective functional to E-L equations}

\subsection{Step-by-step derivation of min-max optimization in Section 2.2.1}
\label{section5.1}
By substituting Eq. 2 into Eq. 1 in the main manuscript, we can obtain the objective function of subscript $z$ (we temporarily drop $i$ for clarity):
\begin{align}
\label{function1}
{\cal J}({z}) &= \mathop {\max }\limits_{|{z}| \le {\bf{1}}} \left\| {\frac{\lambda }{2}{z}{\nabla _{\cal G}}{x}} \right\|_2^2 + \lambda {z}{\nabla _{\cal G}}(x^0 - \frac{\lambda }{2}{z}{\nabla {\cal G}}{x})\\
&= \begin{aligned}[t]
\label{function2}
&\mathop {\max }\limits_{|{z}| \le {\bf{1}}}- \frac{{{\lambda ^2}}}{4}{z}{\nabla _{\cal G}}{x}{z}{\nabla _{\cal G}}{x} + \lambda {z}{\nabla _{\cal G}}x^0
\end{aligned}
\end{align}

Next, we convert Eq. \ref{function2} into a minimization problem as follows:

\begin{equation}
\label{obj1}
    {z} = \begin{array}{*{20}{c}}
{\mathop {\arg \min }\limits_{|{z}| \le {\bf{1}}} }&{{z}{\nabla _{\cal G}}{x}{z}{\nabla _{\cal G}}{x} - \frac{4}{\lambda }{z}{\nabla _{\cal G}}x^0}
\end{array}
\end{equation}

By letting the derivative with respect to $z_i$ to zero, we have the following equation

\begin{equation}
{\nabla _{\cal G}}{x}{z}{\nabla _{\cal G}}{x} = \frac{4}{\lambda }{\nabla _{\cal G}}x^0
\end{equation}
Since $z$ might be in high dimensional space, solving such a large system of linear equations under the constraint $|z| \le 1$ is oftentimes computationally challenging. In order to find a practical solution for $z$ that satisfies the constrained minimization problem in Eq. \ref{obj1}, we resort to the majorization-minimization (MM) method \cite{figueiredo2007majorization}. First, we define:

\begin{equation}
M({z}) =z {\nabla _{\cal G}}{x}{z}{\nabla _{\cal G}}{x} - \frac{4}{\lambda }{z}{\nabla _{\cal G}}x^0
\end{equation}

By setting $z^l$ as point of coincidence, we can find a separable majorizer of $M(z)$ by adding the non-negative function

\begin{equation}
\label{MM}
(z - {z^l})^\intercal(\beta I - \nabla_{\cal G} x\nabla_{\cal G} {x^\intercal})(z - {z^l})
\end{equation}

to $M(z)$, where $\beta$ is greater than or equal to the maximum eigenvalue of $\nabla_{\cal G} x\nabla_{\cal G} {x^\intercal}$. Note, to unify the format, we use the matrix transpose property in Eq. \ref{MM}. Therefore, a majorizer of $M(z)$ is given by:
\begin{equation}
M(z) + {(z - {z^l})^\intercal}(\beta I - \nabla_{\cal G} x\nabla_{\cal G} {x^\intercal})(z - {z^l})
\end{equation}

And, using the MM approach, we can obtain the update equation for $z$ as follows:

\begin{equation}
\label{iterative}
\begin{aligned}
\mathop {{z^{l + 1}}} &={\mathop { \arg \min }\limits_{\left| z \right| \le 1} } (M(z) + {{(z - {z^l})}^\intercal}(\beta I - {\nabla _{\cal G}}x{\nabla {\cal G}}{x^\intercal})(z - {z^l}))\\
&= {\mathop { \arg \min }\limits_{\left| z \right| \le 1} } (\beta {z^\intercal}z - 2{{({\nabla _{\cal G}}(\frac{2}{\lambda }{x^0} - {\nabla {\cal G}}x{z^l}) + \beta {z^l})}^\intercal}z)\\
&= {\mathop { \arg \min }\limits_{\left| z \right| \le 1} } ({z^\intercal}z - 2{{(\frac{1}{\beta }{\nabla _{\cal G}}(\frac{2}{\lambda }{x^0} - {\nabla {\cal G}}x{z^l}) + {z^l})}^\intercal}z)\\
&= {\mathop { \arg \min }\limits_{\left| z \right| \le 1} } ({z^\intercal}z - 2{b^\intercal}z)
\end{aligned}
\end{equation}

where $b = {z^l} + \frac{1}{\beta }{\nabla _{\cal G}}(\frac{2}{\lambda }{x^0} - {\nabla _{\cal G}}x{z^l})$.

Then, the next step is to find $z \in {\cal R} ^N$ that minimizes $z^\intercal z-2bz$ subject to the constraint $|z|\le 1$. Let's first consider the simplest case where $z$ is a scalar:

\begin{equation}
\label{scala}
\begin{array}{*{20}{c}}
{\mathop {\arg \min }\limits_{\left| z \right| \le 1} }&{{z^2} - 2bz}
\end{array}
\end{equation}

The minimum of ${{z^2} - 2bz}$ is at $z=b$. If $b \le 1$, then the solution is $z=b$. If $|b| \ge 1$, then the solution is $z=sign(b)$. We can define the clipping function as:

\begin{equation}
clip(b,1): = \left\{ {\begin{array}{*{20}{c}}
{\begin{array}{*{20}{c}}
b&{\left| b \right| \le 1}
\end{array}}\\
{\begin{array}{*{20}{c}}
{sign(b)}&{\left| b \right| \ge 1}
\end{array}}
\end{array}} \right.
\end{equation}

as illustrated in the middle of Fig. 3 in the main text, then we can write the solution to Eq. \ref{scala} as $z=clip(b,1)$.

Note that the vector case Eq. \ref{iterative} is separable - the elements of $z$ are uncoupled so the constrained minimization can be performed element-wise. Therefore, an update equation for $z$ is given by:

\begin{equation}
{z^{l + 1}} = clip({z^l} + \frac{1}{\beta }{\nabla _{\cal G}}(\frac{2}{\lambda }{x^0} - {\nabla _{\cal G}}x{z^l}),1)
\end{equation}

where $l$ denotes the index of the network layer, the representation of $(l+1)^{th}$ is given by Eq. (1) in the main manuscript. Because the optimization problem is convex, the iteration will converge from any initialization. We may choose, say $z^0=0$. We call this the iterative \textit{ diffusion-clip (DC) algorithm}.

This algorithm can also be written as

\begin{equation}
   \begin{aligned}
& x^{l+1}=x^0-\frac{\lambda}{2} {\nabla _{\cal G}}^ \intercal z^{l} \\
& z^{l+1}=clip \left(z^{l}+\frac{2}{\beta\lambda} {\nabla _{\cal G}} x^{l+1}, 1\right) .
\end{aligned}
\label{final2}
\end{equation}

By scaling $z$ with a factor of $\lambda /2$, we have the following equivalent formulations:
\begin{equation}
\begin{aligned}
& x^{l+1}=x^0-{\nabla _{\cal G}}^\intercal z^{l} \\
& z^{l+1}=clip\left(z^{(i)}+\frac{1}{\beta} {\nabla _{\cal G}} x^{l+1}, \frac{\lambda}{2}\right)
\end{aligned}
\end{equation}

We summarize the process of the diffusion-clip (DC) layer in Algorithm \ref{alg} (it is similar to the iterative shrinkage threshold algorithm \cite{daubechies2004iterative}):

\begin{algorithm}
\caption{DC layer process}
The objective function:\\

${\cal J}(x)=\mathop {\min }\limits_x (\left\| {x - x^{(0)}} \right\|_2^2 + \mathcal{J}_{TV}(x))$\\

can be minimized by alternating the following two steps:\\

$ x^{l}=x^0-{\nabla _{\cal G}}x^\intercal z^{l-1} $\\

$z^{l}=clip \left(z^{l-1}+\frac{1}{\beta} {\nabla _{\cal G}} x^{l}, \frac{\lambda}{2}\right) = clip \left(z^{l-1}+\frac{2}{\beta\lambda} {\nabla _{\cal G}} x^{l}, 1\right) $\\

for $l \ge 1$ with $z^0=0$ and $\beta \ge maxeig({\nabla _{\cal G}}x^ \intercal{\nabla _{\cal G}}x )$
\label{alg}
\end{algorithm}


\subsection{The step-by-step derivation of min-max optimization schema in Section 2.2.2}
\label{secs1.2}
According to the introduction of Secction 2.2.2 (Eq. 4 and Eq. 5) in the main manuscript, we summarize the following equations,

\begin{equation}
\label{eqs14}
\left\{ {\begin{array}{*{20}{c}}
{\frac{{dx}}{{dt}} + div(q) = 0}\\
{{u_i} = \phi ({x_i})}\\
{q = \alpha  \otimes \nabla u}\\
{\Delta u = div(\nabla u)}
\end{array}} \right. \xrightarrow{\hspace*{0.5cm}\text{derive}\hspace*{0.5cm}}\left\{ {\begin{array}{*{20}{c}}
{\frac{{dx}}{{dt}} =  - div(q)}\\
{\frac{{du}}{{dt}} =  - {\phi ^{ - 1}}div(q)}\\
{\frac{{du}}{{dt}} =  - {\phi ^{ - 1}}div(\alpha  \otimes q)}\\
{\frac{{du}}{{dt}} =  - {\phi ^{ - 1}}(\alpha  \otimes \Delta u)}
\end{array}} \right.
\end{equation}

Since the PDE in Eq. 5 in the main manuscript is equivalent to the E-L equation of the quadratic functional $\mathcal{J}(u)= \mathop {\min }\limits_u \int_\mathcal{G}  \alpha \otimes | \nabla_{\mathcal{G}} u{|^2}du$ (after taking $\phi$ away), we propose to replace the ${\ell _2}$-norm integral functional $\mathcal{J}(u)$ with TV-based counterpart

\begin{equation}
\mathcal{J}_{TV}(u)=\mathop {\min }\limits_u \int_\mathcal{G} \alpha \otimes | \nabla_{\mathcal{G}} u{|}du
\end{equation}

We then introduce an auxiliary matrix $f$ to lift the undifferentiable barrier, and reformulate the TV-based functional as a dual min-max functional

\begin{equation}
\mathcal{J}_{TV}(u,f)=\mathop {\min }\limits_{u} \mathop {\max}\limits_{f} \int_\mathcal{G} \alpha \otimes f (\nabla_{\mathcal{G}} u) du
\end{equation}

where we maximize $f$ such that $\mathcal{J}_{TV}(u,f)$ is close enough to $\mathcal{J}_{TV}(u)$. Using Gâteaux variations, we assume
$u \to u + \varepsilon a$, $f \to f + \varepsilon b$, and the directional derivatives in the directions $a$ and $b$ defined as ${\left. {\frac{{d{\cal J}}}{{d\varepsilon }}(u + \varepsilon a)} \right|_{\varepsilon  \to 0}}$ and ${\left. {\frac{{d {\cal J}}}{{d\varepsilon }}(f + \varepsilon b)} \right|_{\varepsilon  \to 0}}$. Given a functional $\mathcal{J}_{TV}(u,f)$, its Gâteaux variations is formulated by:

\begin{equation}
    \begin{aligned}
& \mathcal{J}_{T V}(u+\varepsilon a, f+\varepsilon b)=\int \alpha \otimes [(f+\varepsilon b) \cdot(\nabla u+\varepsilon \nabla a)] d u \\
& \left.\left.\Rightarrow \frac{\partial \mathcal{J}}{\partial \varepsilon}\right|_{\varepsilon \rightarrow 0}=\int \alpha \otimes [(f \cdot \nabla a) +(\nabla u b)]\right. d u \\
& \left.\Rightarrow \frac{\partial \mathcal{J}}{\partial \varepsilon}\right|_{\varepsilon \rightarrow 0} = \alpha  \otimes f \cdot a - \int {\alpha  \otimes (a \cdot \nabla f)du}  + \int {\alpha  \otimes (b\nabla u)du}
\end{aligned}
\end{equation}

Since we assume either $u$ is given at the boundary (Dirichlet boundary condition), the boundary term $\alpha  \otimes f \cdot a$ can be dropped. After that, the derivative of
$\mathcal{J}_{TV}(u,f)$ becomes:

\begin{equation}
\label{EL}
\left.\frac{\partial \mathcal{J}}{\partial \varepsilon}\right|_{\varepsilon \rightarrow 0}=-\int  \alpha \otimes (\nabla f \cdot a+\nabla u \cdot b)
\end{equation}

Since the dummy functional $a$ and $b$ are related to $u$ and $f$ respectively, the E-L equation from the Gâteaux variations in Eq. \ref{EL} leads to two coupled PDEs:

\begin{equation}
\label{dual2}
\left\{ {\begin{array}{*{20}{c}}
{\mathop {\max }\limits_f \frac{{df}}{{dt}} = {\alpha \otimes \nabla _\mathcal{G}}u}\\
{\mathop {\min }\limits_u \frac{{du}}{{dt}} = \alpha \otimes div(f)}
\end{array}} \right.
\end{equation}

Note, we use the adjoint operator $div(f) = - \nabla f$ to approximate the discretization of $\nabla f$ \cite{hyman1997adjoint}, which allows us to link the minimization of $u$ to the classic graph diffusion process.

\section{Experimental details}
\label{s2}
\subsection{Implementation details}

\subsubsection{Hyperparameters \& training details}
\label{hyperpa}
Table \ref{parameter} lists the detailed parameter setting for several GNN-based models, including \textit{X-FlowNet}, \textit{PDENet},  \textit{GCN}, \textit{GAT}, \textit{ResGCN}, \textit{DenseGCN} and \textit{GCNII}.

In the node classification experiments, we set the output dimension to be the number of classes. We adopt the public fixed split \cite{yang2016revisiting} to separate these datasets into training, validation, and test sets. We use the accuracy, precision and F1-score of node classification as the evaluation metrics.

For the ADNI dataset prediction experiment, we set the input and output dimensions to be the same as the number of brain nodes cannot be altered. We use 5-fold cross-validation to evaluate the performance of different methods and measure their prediction accuracy using mean absolute error (MAE). We also conduct an ablation study using a two-step approach. First, we train a model (MLP+GNN) shown in the left panel of Fig. 4 (b) in the main manuscript to predict the potential energy filed (PEF) based on the transport equation, then compute the flows using Eq. \ref{dual}, followed by a GCN-based model to predict the further concentration level of AD-related pathological burdens. Since the deep model in this two-step approach is also formalized from the PDE, we refer to this degraded version as \textit{PDENet}.

In addition, we conduct a prediction of the risk of developing AD using the baseline scan, which can be regarded as a graph classification experiment. This experiment only uses 2 GCN layers with a hidden dimension as $64$ for all methods, while the remaining parameters follow the node classification experiment (Table \ref{parameter} top).

In this work, all experiments are conducted on a server: Intel(R) Xeon(R) Gold 5220R CPU @ 2.20GHz, NVIDIA RTX A5000. The source code is open on anonymous GitHub (https://anonymous.4open.science/r/GNN-PDE-COV-FBBD/\href{https://anonymous.4open.science/r/GNN-PDE-COV-FBBD/}) for the sake of reproducibility.



\subsubsection{Data pre-processing on ADNI dataset.}
\label{datapre}
In total, 1,291 subjects are selected from ADNI \cite{ADNI} dataset, each having diffusion-weighted imaging (DWI) scans and longitudinal amyloid, FDG, cortical thickness(CoTh) and tau PET scans (2-5 time points). The neuroimage processing consists of the following major steps:
\begin{itemize}
    \item We segment the T1-weighted image into white matter, gray matter, and cerebral spinal fluid using FSL software \cite{FSL}. On top of the tissue probability map, we  parcellate the cortical surface into 148 cortical regions (frontal lobe, insula lobe, temporal lobe, occipital lobe, parietal lobe, and limbic lobe) and 12 sub-cortical regions (left and right hippocampus, caudate, thalamus, amygdala, globus pallidum, and putamen), using the Destrieux atlas \cite{destrieux2010automatic} (yellow arrows in Fig. \ref{figs1}). Second, we convert each DWI scan to diffusion tensor images (DTI) \cite{DTI}.
    \item We apply surface seed-based probabilistic fiber tractography \cite{tractography} using the DTI data, thus producing a $160\times 160$ anatomical connectivity matrix (white arrows in Fig. \ref{figs1}). Note, the weight of the anatomical connectivity is defined by the number of fibers linking two brain regions normalized by the total number of fibers in the whole brain ($\Delta$ for graph diffusion in \textit{X-FlowNet}).
    \item Following the region parcellations, we calculate the regional concentration level (the Cerebellum as the reference) of the amyloid, FDG, CoTh and tau pathologies for each brain region (red arrows in Fig. \ref{figs1}), yielding the input $x\in {\mathcal{R}^{160}}$ for training \textit{X-FlowNet}, respectively.
\end{itemize}
 Following the clinical outcomes, we partition the subjects into the cognitive normal (CN), early-stage mild cognitive impairment (EMCI), late-stage mild cognitive impairment (LMCI), and AD groups. To facilitate population counts, we regard CN and EMCI as "CN-like" group, while LMCI and AD as "AD-like" groups. Table \ref{tables3} summarizes the statistics of the two datasets.

\begin{table}[h]
\caption{Parameters setting on Citation network (top) and ADNI data (bottom). $M$ denotes the feature dimension and $C$ denotes the number of classes. For \textit{Cora} dataset, we set $i=4$ when network layer $L=2$, $i=8$ if $L=4$, $i=10$ if $L=8, 16, 32, 64,128$.  For \textit{Citeseer} dataset, we set $i=4$ when network layer $L=2$, $i=8$ if $L=4$, $i=11$ if $L=8, 16, 32, 64,128$. For Pubmed dataset, we set $i=4$ when network layer $L=2$, $i=8$ if $L=4, 8, 16, 32, 64,128$. The hidden dimension of $l^{th}$ is twice that of layer $(l-1)^{th}$. Take Cora as an example (8 layers), the dimension of the hidden layer is: 1433 $\to $ 1024 $\to $512 $\to $ 256$\to $128$\to $64$\to $32$\to $16$\to $7. After exceeding 8 layers, the number of hidden layers is doubled according to the total network layer.}
\label{parameter}
% \vskip 0.15in
\begin{center}
\begin{small}
% \begin{sc}
\setlength{\tabcolsep}{4pt}
\begin{tabular}{lcccccc}
\toprule
 Algorithm & Optimizer & Learning rate  & Weight decay & Hidden layer & Dropout & Epoch\\
\midrule
\textit{GCN} & Adam &  0.01  & $5\times 10^{-4}$ & $M \to 2^i\to...\to 2^4 \to C$ & 0.5 &1500\\
\hline
 \textit{GAT} & Adam &  0.001  & $5\times 10^{-4}$ & head=8, $M \to 2^i ...\to C$ & 0.6 &2000\\
\hline
 \textit{RGCN} & Adam & 0.005 & $5\times 10^{-4}$& hidden dimension=64 &0.1& 2500\\
\hline
 \textit{DGCN} & Adam &  0.001  & $5\times 10^{-4}$ & hidden dimension=64 & 0.1 &2500\\
\hline
 \textit{GRAND} & Adam &  0.01  & $5\times 10^{-4}$ & hidden dimension=16 & 0.5 &200\\
\hline
  \textit{GCNII} & Adam &  0.005  & $5\times 10^{-4}$ & hidden dimension=128 & 0.6 &2000\\
\hline
\hline
\hline
   \textit{GCN} & Adam &  0.001  & $5\times 10^{-4}$ & hidden dimension=16 & 0.2 &500\\
\hline
 \textit{GAT} & Adam &  0.001  & $5\times 10^{-4}$ & head=8, hidden dimension=4 & 0.5 &800\\
\hline
 \textit{RGCN} & Adam & 0.001 & $5\times 10^{-4}$& hidden dimension=16 &0.1& 500\\
\hline
 \textit{DGCN} & Adam &  0.01  & $5\times 10^{-4}$ & hidden dimension=8 & 0.1 &500\\
\hline
  \textit{GCNII} & Adam &  0.005  & $5\times 10^{-4}$ & hidden dimension=16 & 0.6 &1500\\
\hline
  \textit{GRAND} & Adam &  0.01  & $5\times 10^{-4}$ & hidden dimension=16 & 0.5 &500\\
\hline
  \textit{X-FlowNet} & Adam &  1e-4/3e-3  & $1\times 10^{-5}$ & hidden dimension=16 & 0.5 &500\\
\hline
  \textit{PDENet} & Adam &  0.01  & $1\times 10^{-5}$ & hidden dimension=16 & 0.5 &500\\
\hline
\bottomrule
\end{tabular}
% \end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

\begin{figure}[h]
\vskip -0.2in
\centerline{\includegraphics[scale=.6]{FigS1.pdf}}
\caption{General workflows for processing T1-weighted image (yellow arrows), diffusion-weighted image (white arrows), and PET images (red arrows). The output is shown at the bottom right, including the brain network, and regional concentration level of amyloid, FDG, CoTh and tau aggregates.}
{\vspace{-15pt}}
\label{figs1}
\end{figure}

\begin{table}[h]
  \caption{Dataset statistics.}
  \centering
  \renewcommand\arraystretch{0.5}
  \scalebox{0.95}{\begin{tabular}{lcccc|lcc}
    \toprule
    \multicolumn{5}{c}{\textbf{Node classification (Citation)}}         & \multicolumn{3}{|c}{\textbf{Application on flow prediction (ADNI)}}            \\
    \cmidrule(r){1-8}
     {\multirow{2}*{Dataset} }   & \multicolumn{4}{c|}{Description}     & Features & \multicolumn{2}{c}{\# of subjects (CN/AD)} \\
     \cmidrule(r){2-8}
     & Classes & Nodes & Edges & Features & Amyloid (160) & \multicolumn{2}{c}{304/83} \\
    \midrule
    \textit{Cora} & 7  & 2708  & 5429& 1433 &Tau (160) & \multicolumn{2}{c}{124/37}\\
    \textit{Citeseer}     & 6 & 3327   &4732 & 3703&FDG (160)& \multicolumn{2}{c}{ 211/63}  \\
   \textit{Pubmed}     & 3       & 19717  &44338 &500&Cortical thickness (160)& \multicolumn{2}{c}{ 359/110}\\
    \bottomrule
  \end{tabular}}
  \label{tables3}
\end{table}


\subsection{Experiments on node classification}
Fig \ref{figs2} presents the performance of different evaluation criteria (accuracy, precision, and F1-score) across different network layers for node classification by benchmark GNN model (patterned in dash lines) and the counterpart novel GNN model from our \textit{GNN-PDE-COV} framework (patterned by solid lines), where each row is associated with a specific instance of GNN model. It is evident that our proposed \textit{GNN-PDE-COV} consistently outperforms other methods across different layers, with significantly enhanced degrees in accuracy, precision, and F1-score. Moreover, the GNN model yielded from our \textit{GNN-PDE-COV} framework consistently achieves the highest accuracy on all three datasets. Overall, these results demonstrate the state-of-the-art performance by our \textit{GNN-PDE-COV} framework in graph node classification.

\begin{figure}[h]
  \centering
  \includegraphics[scale=0.28]{FigS2.pdf}
  \vspace{-0.5em}
  \caption{The performance of node classification with respect to various GNN layers (horizontal axis) on six models. Note: dotted line: baseline, solid line: \textit{GNN-PDE-COV}, blue: Cora, purple: Citeseer, red: Pubmed.}
  \label{figs2}
  \vspace{-0.5em}
\end{figure}

The effect of anti-smoothing by clip operation is shown in Fig. \ref{figs5}. To set up the stage, we put the spotlight on the links that connect two nodes with different categorical labels. In this context, 2,006 links from \textit{Cora}, 2,408 links from \textit{Citeseer}, and 17,518 links from \textit{Pubmed} datasets are selected, called inter-class links. For each inter-class link, we calculate node-to-node similarity in terms of Pearson's correlation between two associated graph embedding vectors  \footnote{the learned feature representations for node classification} by benchmark methods (in red) and the counterpart GNN models derived from \textit{GNN-PDE-COV} framework (in green). We find that (1) more than 70\% nodes are actually associated with inter-class links which confirms the hypothesis of over-smoothing in Fig. 1 of our manuscript; (2) Our novel GNN models have the ability to learn feature representations that better preserve the discriminative power for node classification (as indicated by the distribution of node-to-node similarity shifting towards the sign of anti-correlation).

%calculate the number of neighboring nodes in the citation networks (Cora: 2006, Citeseer: 2408, Pubmed:17518) that do not belong to the same class. Our findings demonstrate that this type of node accounts for more than 70\% of the total number of nodes, which confirms the hypothesis stated in Fig. 1 of our main text. We show the histogram of the node-to-node feature correlation between Benchmark methods (in red) and our \textit{GNN-PDE-COV} (in green). The results indicate that the feature representations learned by our framework are more discriminative. This outcome further validates our claims and highlights the potential of our proposed method.

\begin{figure}[h]
  \centering
  \includegraphics[scale=0.1]{FigS5.pdf}
  %\vspace{-1.5em}
  \caption{The distribution of the node-to-node similarities (measured by Pearson's correlation between embedding vectors) by Benchmark methods (in red) and our \textit{GNN-PDE-COV} (in green) in \textit{Cora} (left), \textit{Citeseer} (middle), and \textit{Pubmed} (right).}
  \label{figs5}
  %\vspace{-1.5em}
\end{figure}


\subsection{Application on uncovering the propagation mechanism of pathological events in AD}

\textit{Firstly}, we examine the prediction accuracy for each modality of concentration (tau, amyloid, FDG, CoTh) level at different noise levels. Specifically, to evaluate the robustness of our \textit{X-FlowNet} model to noise, we conducted an experiment by adding uncorrelated additive Gaussian noise levels with standard deviation ranging from 0.02 to 1 to the observed concentration levels of tau, amyloid, FDG, and CoTh. We then evaluated the prediction accuracy (MAE) using 5-fold cross-validation. The prediction results, as shown in Fig. \ref{figs3}, indicate that our \textit{X-FlowNet} model is less sensitive to noise added to the imaging features than all other counterpart GNN methods.

\begin{figure}[h]
  \centering
  \includegraphics[scale=0.22]{FigS44.pdf}
  %\vspace{-1.5em}
  \caption{The performance of 5-fold cross-validation for predicting the future concentration (top to bottom: Tau, Amyloid, FDG and CoTh) level by (1) GCN, (2) GAT, (3) GCNII, (4) ResGCN, (5) DenseGCN, (6) GRAND, (7) PDENet (used in ablation study), and (8) our \textit{X-FlowNet}.}
  \label{figs3}
  %\vspace{-1.5em}
\end{figure}

\textit{Secondly}, we conduct an ablation study to compare our \textit{X-FlowNet} model with \textit{PDENet} (marked as \#7 in Fig. \ref{figs3}). Our model, which is in a GAN architecture and incorporates a TV constraint to avoid over-smoothing, integrates the two steps of estimating the PEF and uncovering the spreading flows into a unified neural network, resulting in significantly improved prediction accuracy compared to \textit{PDENet}.
%The results of the ablation study are displayed in the penultimate column of each noise level.

\textit{Thirdly}, we perform a disease risk prediction experiment, which can be regarded as a graph classification problem. We assume that we have baseline amyloid, tau, FDG, and CoTh scans, and evaluate the prediction accuracy, precision and F1-score of various models in forecasting the risk of developing AD. We consider two dichotomous cases: one included only AD vs. CN groups and the other involved AD/LMCI vs. CN/EMCI. The results of the mean of 5-fold cross-validation are shown in Table \ref{tables2}. Our \textit{GNN-PDE-COV} outperforms all other methods in terms of accuracy, precision and F1-score indicated by an asterisk (`*') at the significance level of 0.001.


\textit{Fourthly}, we examine the propagation pattern of tau spreading flows on an individual basis (Fig. \ref{figs4}). \textit{First}, we visualize the top flows (ranked in terms of flow volume) uncovered in a CN subject (Fig. \ref{figs4}(a)). It is apparent that subcortex-cortex flows are the predominant patterns, where most of the tau aggregates spread from subcortical regions (globus pallidus, hippocampus, and putamen) to the temporal lobe, limbic lobe, parietal lobe, and insula lobe. Note, we find inferior temporal gyrus ($t_6$) and entorhinal cortex ($t_8$) are actively involved in the subcortex-cortex flows, which are the footprints of early stage tau propagation frequently reported in many pathology studies \cite{lee2022regional,vogel2020spread}. \textit{Second}, we visualize the top flows uncovered in an AD subject (Fig. \ref{figs4}(b)). It is apparent that the propagation of tau is restricted on the brain cortex, mainly spreading from temporal lobe regions to other regions (such as frontal lobe, limbic lobe and occipital lobe), which is aligned with current clinical and pathology findings that predominant amount of tau aggregates propagate throughout brain cortex in the late stage of AD.
%It is worth noting that the propagation of tau at this stage does not involve subcortical regions.

% Please add the following required packages to your document preamble:
% \usepackage{multirow}
% \usepackage[table,xcdraw]{xcolor}
% If you use beamer only pass "xcolor=table" option, i.e. \documentclass[xcolor=table]{beamer}
\begin{table}[h]

\caption{The performance of disease risk prediction. Note: RGCN denotes ResGCN, DGCN denotes DenseGCN. `*' denotes the significant improvement (paired \textit{t}-test: $p<0.001$). Blue: Tau, red: amyloid, orange: FDG, green: CoTh.}
   % \renewcommand\arraystretch{0.5}
    \tabcolsep=0.05cm
\resizebox{\textwidth}{!}{\begin{tabular}{c|l|cc|cc|cc|cc|cc|cc}
\hline
\hline
\rowcolor[HTML]{A1DFF9}
\textbf{Tau}                                                                                                          & Unit (\%)                                                & GCN     & \textbf{GCN+}     & GAT     & \textbf{GAT+}     & GCNII   & \textbf{GCNII+}   & RGCN  & \textbf{RGCN+}  & DGCN & \textbf{DGCN+} & GRAND   & \textbf{GRAND+}   \\ \hline
\rowcolor[HTML]{A1DFF9}
\cellcolor[HTML]{A1DFF9}                                                                                              & Precision                                                & 80.15 & \textbf{90.03(*)} & 69.91 & \textbf{86.18(*)} & 83.93 & \textbf{90.03(*)} & 84.64 & \textbf{89.46(*)} & 84.03  & \textbf{91.58(*)}  & 87.95 & \textbf{88.22(*)} \\ \cline{2-14}
\rowcolor[HTML]{A1DFF9}
\cellcolor[HTML]{A1DFF9}                                                                                              & Accuracy                                                 & 82.30 & \textbf{88.74(*)} & 81.05 & \textbf{87.50(*)} & 83.79 & \textbf{88.75(*)} & 86.03 & \textbf{90.00(*)} & 85.54  & \textbf{91.25(*)}  & 88.75 & \textbf{90.12(*)} \\ \cline{2-14}
\rowcolor[HTML]{A1DFF9}
\multirow{-3}{*}{\cellcolor[HTML]{A1DFF9}\begin{tabular}[c]{@{}c@{}}AD/LMCI \\ vs. \\ CN/EMCI\end{tabular}}           & F1-score                                                 & 75.55 & \textbf{84.49(*)} & 72.87 & \textbf{84.72(*)} & 78.82 & \textbf{84.45(*)} & 83.15 & \textbf{88.54(*)} & 82.45  & \textbf{91.39(*)}  & 88.14 & \textbf{89.44(*)} \\ \hline
\rowcolor[HTML]{A1DFF9}
\cellcolor[HTML]{A1DFF9}                                                                                              & Precision                                                & 89.29 & \textbf{91.92(*)} & 87.26 & \textbf{90.13(*)} & 83.65 & \textbf{88.52(*)} & 92.61 & \textbf{95.72(*)} & 92.61  & \textbf{95.91(*)}  & 91.77 & \textbf{95.76(*)} \\ \cline{2-14}
\rowcolor[HTML]{A1DFF9}
\cellcolor[HTML]{A1DFF9}                                                                                              & Accuracy                                                 & 86.64 & \textbf{90.91(*)} & 84.86 & \textbf{88.41(*)} & 76.84 & \textbf{86.36(*)} & 91.07 & \textbf{95.45(*)} & 91.07  & \textbf{95.65(*)}  & 90.91 & \textbf{95.45(*)} \\ \cline{2-14}
\rowcolor[HTML]{A1DFF9}
\multirow{-3}{*}{\cellcolor[HTML]{A1DFF9}\begin{tabular}[c]{@{}c@{}} AD       \\      vs.       \\    CN\end{tabular}} & F1-score                                                 & 85.64 & \textbf{90.26(*)} & 83.99 & \textbf{87.16(*)} & 71.51 & \textbf{84.68(*)} & 90.45 & \textbf{95.32(*)} & 90.45  & \textbf{95.55(*)}  & 88.86 & \textbf{95.38(*)} \\ \hline
\hline
\rowcolor[HTML]{DDA7B1}
\textbf{Amyloid}                                                                                                      & \multicolumn{1}{c|}{\cellcolor[HTML]{DDA7B1}Unit   (\%)} & GCN     & \textbf{GCN+}     & GAT     & \textbf{GAT+}     & GCNII   & \textbf{GCNII+}   & RGCN  & \textbf{RGCN+}  & DGCN & \textbf{DGCN+} & GRAND   & \textbf{GRAND+}   \\ \hline
\rowcolor[HTML]{DDA7B1}
\cellcolor[HTML]{DDA7B1}                                                                                              & Precision                                                & 76.36 & \textbf{83.78(*)} & 67.73 & \textbf{71.79(*)} & 60.87 & \textbf{60.01()} & 72.53 & \textbf{83.21(*)} & 74.92  & \textbf{60.17(*)}  & 79.00 & \textbf{79.93(*)} \\ \cline{2-14}
\rowcolor[HTML]{DDA7B1}
\cellcolor[HTML]{DDA7B1}                                                                                              & Accuracy                                                 & 76.40 & \textbf{79.44(*)} & 75.43 & \textbf{77.57(*)} & 74.31 & \textbf{76.64(*)} & 75.99 & \textbf{78.50(*)} & 76.92  & \textbf{77.57(*)}  & 80.37 & \textbf{81.31(*)} \\ \cline{2-14}
\rowcolor[HTML]{DDA7B1}
\multirow{-3}{*}{\cellcolor[HTML]{DDA7B1}\begin{tabular}[c]{@{}c@{}}AD/LMCI \\ vs. \\ CN/EMCI\end{tabular}}           & F1-score                                                 & 70.33 & \textbf{72.58(*)} & 67.66 & \textbf{69.39(*)} & 63.57 & \textbf{67.31(*)} & 70.66 & \textbf{70.67(*)} & 72.68  & \textbf{67.77(*)}  & 79.25 & \textbf{79.63(*)} \\ \hline
\rowcolor[HTML]{DDA7B1}
\cellcolor[HTML]{DDA7B1}                                                                                              & Precision                                                & 81.58 & \textbf{88.37(*)} & 81.54 & \textbf{87.98(*)} & 70.59 & \textbf{79.98(*)} & 85.75 & \textbf{93.09(*)} & 83.87  & \textbf{90.56(*)}  & 65.53 & \textbf{89.62(*)} \\ \cline{2-14}
\rowcolor[HTML]{DDA7B1}
\cellcolor[HTML]{DDA7B1}                                                                                              & Accuracy                                                 & 80.77 & \textbf{88.10(*)} & 80.78 & \textbf{88.10(*)} & 75.02 & \textbf{81.24(*)} & 85.56 & \textbf{92.86(*)} & 85.29  & \textbf{90.48(*)}  & 80.95 & \textbf{87.80(*)} \\ \cline{2-14}
\rowcolor[HTML]{DDA7B1}
\multirow{-3}{*}{\cellcolor[HTML]{DDA7B1}\begin{tabular}[c]{@{}c@{}}AD           \\  vs.     \\ CN\end{tabular}}      & F1-score                                                 & 78.14 & \textbf{87.68(*)} & 78.07 & \textbf{87.98(*)} & 65.87 & \textbf{77.42(*)} & 85.34 & \textbf{92.92(*)} & 82.30  & \textbf{90.27(*)}  & 72.43 & \textbf{88.22(*)} \\ \hline
\hline
\rowcolor[HTML]{EEA887}
\textbf{FDG}                                                                                                          & \multicolumn{1}{c|}{\cellcolor[HTML]{EEA887}Unit   (\%)} & GCN     & \textbf{GCN+}     & GAT     & \textbf{GAT+}     & GCNII   & \textbf{GCNII+}   & RGCN  & \textbf{RGCN+}  & DGCN & \textbf{DGCN+} & GRAND   & \textbf{GRAND+}   \\ \hline
\rowcolor[HTML]{EEA887}
\cellcolor[HTML]{EEA887}                                                                                              & Precision                                                & 68.43 & \textbf{67.20(*)} & 55.86 & \textbf{59.29(*)} & 60.08 & \textbf{70.94(*)} & 50.45 & \textbf{55.14(*)} & 50.45  & \textbf{55.14(*)}  & 51.38 & \textbf{56.25(*)} \\ \cline{2-14}
\rowcolor[HTML]{EEA887}
\cellcolor[HTML]{EEA887}                                                                                              & Accuracy                                                 & 73.17 & \textbf{76.00(*)} & 72.17 & \textbf{77.00(*)} & 71.78 & \textbf{74.54(*)} & 70.98 & \textbf{74.26(*)} & 70.98  & \textbf{74.26(*)}  & 71.10 & \textbf{75.00(*)} \\ \cline{2-14}
\rowcolor[HTML]{EEA887}
\multirow{-3}{*}{\cellcolor[HTML]{EEA887}\begin{tabular}[c]{@{}c@{}}AD/LMCI \\ vs. \\ CN/EMCI\end{tabular}}           & F1-score                                                 & 63.94 & \textbf{68.15(*)} & 62.15 & \textbf{66.99(*)} & 61.02 & \textbf{69.07(*)} & 58.96 & \textbf{63.29(*)} & 58.96  & \textbf{63.29(*)}  & 59.42 & \textbf{64.29(*)} \\ \hline
\rowcolor[HTML]{EEA887}
\cellcolor[HTML]{EEA887}                                                                                              & Precision                                                & 81.11 & \textbf{87.25(*)} & 61.90 & \textbf{62.33(*)} & 74.31 & \textbf{81.06(*)} & 59.77 & \textbf{80.57(*)} & 66.84  & \textbf{81.77(*)}  & 70.91 & \textbf{72.24(*)} \\ \cline{2-14}
\rowcolor[HTML]{EEA887}
\cellcolor[HTML]{EEA887}                                                                                              & Accuracy                                                 & 82.17 & \textbf{84.62(*)} & 72.82 & \textbf{78.95(*)} & 79.55 & \textbf{82.05(*)} & 73.35 & \textbf{79.58(*)} & 73.87  & \textbf{80.11(*)}  & 84.21 & \textbf{86.32(*)} \\ \cline{2-14}
\rowcolor[HTML]{EEA887}
\multirow{-3}{*}{\cellcolor[HTML]{EEA887}\begin{tabular}[c]{@{}c@{}}AD            \\ vs.       \\    CN\end{tabular}} & F1-score                                                 & 79.40 & \textbf{82.04(*)} & 64.23 & \textbf{69.66(*)} & 73.88 & \textbf{80.99(*)} & 62.77 & \textbf{75.98(*)} & 63.92  & \textbf{76.58(*)}  & 76.99 & \textbf{78.06(*)} \\ \hline
\hline
\rowcolor[HTML]{B9D593}
\textbf{CoTh}                                                                                                         & \multicolumn{1}{c|}{\cellcolor[HTML]{B9D593}Unit   (\%)} & GCN     & \textbf{GCN+}     & GAT     & \textbf{GAT+}     & GCNII   & \textbf{GCNII+}   & RGCN  & \textbf{RGCN+}  & DGCN & \textbf{DGCN+} & GRAND   & \textbf{GRAND+}   \\ \hline
\rowcolor[HTML]{B9D593}
\cellcolor[HTML]{B9D593}                                                                                              & Precision                                                & 74.85 & \textbf{74.71(*)} & 62.63 & \textbf{67.15(*)} & 62.63 & \textbf{74.71(*)} & 62.63 & \textbf{68.77(*)} & 62.63  & \textbf{64.59(*)}          & 63.81 & \textbf{68.77(*)} \\ \cline{2-14}
\rowcolor[HTML]{B9D593}
\cellcolor[HTML]{B9D593}                                                                                              & Accuracy                                                 & 80.68 & \textbf{82.32(*)} & 79.10 & \textbf{79.34(*)} & 79.10 & \textbf{80.37(*)} & 79.10 & \textbf{82.93(*)} & 79.10  & \textbf{80.37(*)}          & 79.88 & \textbf{82.93(*)} \\ \cline{2-14}
\rowcolor[HTML]{B9D593}
\multirow{-3}{*}{\cellcolor[HTML]{B9D593}\begin{tabular}[c]{@{}c@{}}AD/LMCI \\ vs. \\ CN/EMCI\end{tabular}}           & F1-score                                                 & 73.55 & \textbf{75.93(*)} & 69.89 & \textbf{70.44(*)} & 69.89 & \textbf{72.72(*)} & 69.89 & \textbf{75.19(*)} & 69.89  & \textbf{71.62(*)}          & 70.94 & \textbf{75.19(*)} \\ \hline
\rowcolor[HTML]{B9D593}
\cellcolor[HTML]{B9D593}                                                                                              & Precision                                                & 83.45 & \textbf{85.77(*)}         & 71.24 & \textbf{72.80(*)} & 76.14 & \textbf{79.84(*)} & 65.04 & \textbf{80.62(*)}         & 65.04  & \textbf{81.52(*)}          & 71.24 & \textbf{74.37(*)} \\ \cline{2-14}
\rowcolor[HTML]{B9D593}
\cellcolor[HTML]{B9D593}                                                                                              & Accuracy                                                 & 84.79 & \textbf{87.16(*)}         & 81.50 & \textbf{85.32(*)} & 81.50 & \textbf{83.49(*)} & 80.59 & \textbf{83.52(*)}         & 80.59  & \textbf{82.42(*)}          & 84.40 & \textbf{86.24(*)} \\ \cline{2-14}
\rowcolor[HTML]{B9D593}
\multirow{-3}{*}{\cellcolor[HTML]{B9D593}\begin{tabular}[c]{@{}c@{}}AD            \\ vs.         \\ CN\end{tabular}}  & F1-score                                                 & 82.02 & \textbf{83.69(*)}         & 75.06 & \textbf{78.56(*)} & 74.17 & \textbf{81.07(*)} & 71.95 & \textbf{78.48(*)}         & 71.95  & \textbf{76.13(*)}          & 77.27 & \textbf{79.87(*)} \\ \hline
\end{tabular}}
\label{tables2}
\end{table}



\begin{figure}[h]
  \centering
  \includegraphics[scale=0.6]{FigS4.pdf}
  %\vspace{-1.5em}
  \caption{Visualization of tau spreading flows in an individual cognitive normal subject (a) and an individual Alzheimer’s disease subject (b). For clarity, we only display the top-ranked flows sorted by the flow volume.}
  \label{figs4}
  %\vspace{-1.5em}
\end{figure}

\subsection{Discussion and limitations}
\textit{Discussion. }In our experiments, we found adding DC layer right after every FC layer usually does not yield best performance. Instead, we empirically set to add DC layer from the first several FC layers. For example, we add DC layer after the $3^{rd}$ FC layer in an 8-layer GNN model, after the $5^{th}$ FC layer in a 16-layer GNN model, and after $8^{th}$ FC layer in a GNN model with more than 16 layers. One possible explanation is that the clip operation in DC layer depends on a good estimation of cap $b$ in Eq. 3 (in the main manuscript). Given that the estimation of $b$ may lack stability during the initial stages of graph learning, it can be advantageous to postpone the clip operation from an engineering perspective. However, delaying the addition of the DC layer too much can result in missed opportunities to address the problem of over-smoothing.

Regarding the computational time, we record the additional computational time of training our DC layer on different datasets. Specifically, the extra training time is 2.2 ms/epoch in \textit{Cora}, 9.8 ms/epoch in \textit{Citeseer}, 7.8 ms/epoch in \textit{Pubmed}, and 0.3 ms/epoch in  \textit{ADNI}, respectively, where the data descriptions are listed in Table \ref{tables3}. It is apparent that the TV-based constraint effectively addresses the over-smoothing issue in GNN without imposing a significant computational burden.

\textit{Limitations.} Our current graph learning experiments are limited to citation networks. In the future, we will evaluate our \textit{GNN-PDE-COV} framework on other graph datasets such as drug medicine and protein networks.

\textit{Societal impact.} Our major contribution to the machine learning field is a novel research framework which allows us to develop new GNN models with a system-level understanding. We have provided a new approach to address the common issue of over-smoothing in GNN with a mathematical guarantee. From the application perspective, the new deep model for uncovering the \textit{in-vivo} propagation flows has great potential to establish new underpinning of disease progression and disentangle the heterogeneity of diverse neurodegeneration trajectories.
%we rethink the GNN propagate mechanism in continuous domain and derive the graph diffusion from functions. We proposed TV regularization, which can enforce smoothness while preserving important features. In light of this, we redesign two novel TV-based GNN architectures for various application scenarios. The experiment results indicate that the proposed \textit{GNN-PDE-COV} framework mitigates the over-smoothing issue and slows down performance degradation. Additionally, our application on brain network analysis provides neuroscientifically interpretable results corroborated by previous Alzheimer's disease literature.
\bibliographystyle{splncs04}
\bibliography{nips_arxiv}
% \section*{References}

% References follow the acknowledgments in the camera-ready paper. Use unnumbered first-level heading for
% the references. Any choice of citation style is acceptable as long as you are
% consistent. It is permissible to reduce the font size to \verb+small+ (9 point)
% when listing the references.
% Note that the Reference section does not count towards the page limit.
% \medskip


% {
% \small


% [1] Alexander, J.A.\ \& Mozer, M.C.\ (1995) Template-based algorithms for
% connectionist rule extraction. In G.\ Tesauro, D.S.\ Touretzky and T.K.\ Leen
% (eds.), {\it Advances in Neural Information Processing Systems 7},
% pp.\ 609--616. Cambridge, MA: MIT Press.


% [2] Bower, J.M.\ \& Beeman, D.\ (1995) {\it The Book of GENESIS: Exploring
%   Realistic Neural Models with the GEneral NEural SImulation System.}  New York:
% TELOS/Springer--Verlag.


% [3] Hasselmo, M.E., Schnell, E.\ \& Barkai, E.\ (1995) Dynamics of learning and
% recall at excitatory recurrent synapses and cholinergic modulation in rat
% hippocampal region CA3. {\it Journal of Neuroscience} {\bf 15}(7):5249-5262.
% }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}
