%!TEX root = main.tex

%\begin{table*}[]
%\centering
%\caption{Results for our active learner and baselines for the $N$-way, $K$-shot classification settings.}
%\label{tab:res_kway_kshot}
%\begin{tabular}{@{}lllllll@{}}
%\toprule
%\multicolumn{1}{c}{\multirow{2}{*}{Model}} & \multicolumn{3}{c}{\textbf{5-way}} & \multicolumn{3}{c}{\textbf{10-way}} \\ \cmidrule(l){2-7} 
%\multicolumn{1}{c}{}                       & 1-shot     & 2-shot    & 3-shot    & 1-shot     & 2-shot     & 3-shot    \\ \midrule
%\textbf{Matching Net (random)}             & 70.1\%     & 93.2\%    & 98.5\%    & 67.4\%     & 91.1\%     & 97.5\%    \\
%\textbf{Matching Net (balanced)}           & 98.0\%     & 99.0\%    & 99.1\%    & 96.6\%     & 98.5\%     & 98.6\%    \\
%\textbf{Active MN}                         & 97.8\%     & 98.9\%    & 99.2\%    & 94.4\%     & 98.2\%     & 98.5\%    \\
%\midrule
%\textbf{Min-Max-Cos}                       & 97.7\%     & 99.3\%    & 99.4\%    & 94.0\%     & 98.4\%     & 98.8\%    \\ \bottomrule
%\end{tabular}
%\vspace{-0.25cm}
%\end{table*}

\begin{table*}[]
\centering
\caption{Results for our active learner and baselines for the $N$-way, $K$-shot classification settings.}
\label{tab:res_kway_kshot}
\begin{tabular}{@{}lllllll@{}}
\toprule
\multicolumn{1}{c}{\multirow{2}{*}{Model}} & \multicolumn{3}{c}{\textbf{5-way}} & \multicolumn{3}{c}{\textbf{10-way}} \\ \cmidrule(l){2-7} 
\multicolumn{1}{c}{}                       & 1-shot     & 2-shot    & 3-shot    & 1-shot     & 2-shot     & 3-shot    \\ \midrule
\textbf{Matching Net (random)} & 69.8\%$_{\pm 0.10}$     & 93.1\%$_{\pm 0.07}$    & 98.5\%$_{\pm 0.04}$ & 67.3\%$_{\pm 0.10}$  & 91.2\%$_{\pm 0.06}$ & 97.6\%$_{\pm 0.06}$    \\
\textbf{Matching Net (balanced)} & 97.9\%$_{\pm 0.07}$     & 98.9\%$_{\pm 0.07}$    & 99.2\%$_{\pm 0.06}$    & 96.5\%$_{\pm 0.04}$     & 98.3\%$_{\pm 0.03}$    & 98.7\%$_{\pm 0.05}$    \\
\textbf{Active MN} & 97.4\%$_{\pm 0.11}$     & 99.0\%$_{\pm 0.08}$    & 99.3\%$_{\pm 0.03}$    & 94.3\%$_{\pm 0.24}$    & 98.0\%$_{\pm 0.07}$     & 98.5\%$_{\pm 0.06}$    \\
\midrule
\textbf{Min-Max-Cos} & 97.4\%$_{\pm 0.11}$     & 99.3\%$_{\pm 0.02}$    & 99.4\%$_{\pm 0.04}$    & 93.5\%$_{\pm 0.11}$    & 98.4\%$_{\pm 0.02}$     & 98.8\%$_{\pm 0.03}$    \\ \bottomrule
\end{tabular}
\vspace{-0.25cm}
\end{table*}

\begin{figure}[t]
\begin{center}
\vspace{-0.1cm}
\hspace{-0.5cm}
\includegraphics[width=0.9\linewidth]{./og_annot.pdf}
\vspace{-0.25cm}
\caption{A rollout of our active learning policy for Omniglot, using a support set of 20 items from 10 different classes with 2 items per class. Each row represents the support set at different active iterations. For visualization purposes, each column represents a class. Unlabeled items have white background while selected items have black background. Here, the model behaves intelligently, by selecting at each step an item with a yet-unseen label.}
\label{fig:og_rollout}
\end{center}
\vspace{-0.5cm}
\end{figure}

We run our first experiments on the Omniglot dataset~\citep{lake2015human} consisting of 1623 characters from 50 different alphabets, each hand-written by 20 different people.
Following~\citet{vinyals2016matching}, we divide the dataset into 1200 characters for training and keep the rest for testing. When measuring test performance, our model interacts with characters it did not encounter during training.
%
% here, specifications about the omniglot encoder
%

For the context-free embedding function we use a three-layer convolutional network. The first two layers use $5 \times 5$ convolutions with 64 filters and downsample with a double stride. The third layer uses a $3 \times 3$ convolution with 64 filters and no downsampling. These layers produce a $7 \times 7 \times 64$ feature map that we flatten and pass through a fully connected layer. All convolutional layers use the leaky ReLU nonlinearity \cite{Maas2013}.

We setup $N$-way, $K$-shot Omniglot classification as follows. We randomly pick $N$ character classes from the available train/test classes. Then, we build a support set by randomly sampling 5 items for each character class,~e.g. in the 5-way setting, there are 25 items in the support set. The held-out set is always obtained by randomly sampling 1 item per class.
In our active learning setting, $K$-shot is proportional to how many labels the model can acquire. In the $N$-way, $K$-shot setting, the model asks for $NK$ labels before performing held-out prediction. For example, in 5-way, 1-shot classification, the model asks for 5 labels. Following each label query, we also measure anytime performance of the fast prediction module on the items remaining in $S^u_t$. Note that the 1-shot setting is particularly challenging for our model, as it needs to ask for different classes at each step, and the ability to identify missing classes is limited by the accuracy of the underlying one-shot classifier.

We compare our active learner to four baselines. To compute a pessimistic estimate of potential performance, we use a matching network where we label $NK$ items chosen at random from the full support set (Matching Net (random)). As the labels are randomly sampled, it is possible that a given class is never represented among the labeled items and the model cannot classify perfectly, even in principle. To compute an optimistic estimate of potential performance, we measure the ``ideal'' matching network accuracy by labeling a class-balanced subset of items from the full support set (Matching Net (balanced)). This baseline represents a highly-performant policy that the active learner can, in principle, learn. For the last baseline (Min-Max-Cos), we formulate a heuristic policy. At each active learning step, we select the item which has minimum maximum cosine similarity to unlabeled items in the support set. This heuristic selects item that are different from each other, a strategy well-suited to the Omniglot classification task where items are drawn from a consistent set of underlying classes.

We report the results in Table~\ref{tab:res_kway_kshot}. Matching Networks operating on a randomly sampled set of labels suffer the most in 1-shot scenarios, where the probability of all classes being represented is particularly low (especially in the 10-way case). Overall, the active policy nearly matches the performance of the optimistic balanced Matching Network baseline. Degradation of performance by 2.2\% is observed for the 1-shot, 10-way case. This is not surprising since augmenting the number of classes in the support set, while keeping the number of shots fixed, considerably increases the difficulty of the problem for the active learner.
Figure~\ref{fig:og_rollout} shows a roll-out of the model policy in the 10-way setting.

Figure~\ref{fig:mega_plot} provides results for the more challenging setting of 20-way classification. We tested two properties of our model: its anytime performance beyond the 1-shot setting, and its ability to generalize to problems with more classes than were seen during training. The model performed well on 20-way classification, and quickly approached the optimistic performance estimate after acquiring more labels. We also found that policies trained for as little as 10-way classification could generalize to the 20-way setting.

Our model relies on a number of moving parts. When designing the architecture, we followed the simple approach of minimizing changes to the original Matching Network from \citet{vinyals2016matching}. We now provide ablation test results for several parts of our model. In the 10-way, 1-shot setting accuracy dropped from 94.5 to 86.0 when we removed attention temperature from the fast prediction module. Reducing the number of matching steps from 3 to 2 or 1 had no significant effect in this setting. Removing the context-sensitive encoder also had no significant effect. Streamlining our architecture is clearly a useful topic for future work aimed at scaling our approach to more realistic settings.

\begin{figure*}
\begin{center}
\includegraphics[width=0.38\linewidth]{./mega_plot_type1.pdf}
\includegraphics[width=0.38\linewidth]{./og_gen_test_20_type1.pdf}
% \vspace{-0.2cm}
\caption{Experiment results for our model and baselines on Omniglot. The left plot shows how prediction accuracy improves with the number of labels requested in a challenging 20-way setting. After 20 label requests~(corresponding to a 20-way, 1-shot problem), the active policy outperforms random policy and random MN baselines, but is inferior to the balanced MN. After 30 labels, the active policy nearly matches the performance of the balanced MN using 40 labels (20-way, 2-shot). The right plot shows the number of unique labels with respect to the number of requested labels for models trained on problems with 5-20 classes, and tested on 20-way classification. This gives an idea of how models search for labels from unseen groups and generalize to problems with different numbers of classes.}
\label{fig:mega_plot}
\end{center}
\vspace{-0.5cm}
\end{figure*}