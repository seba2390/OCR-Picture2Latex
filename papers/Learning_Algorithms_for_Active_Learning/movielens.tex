%!TEX root = main.tex

\begin{figure}[t]
%\vspace{-0.5cm}
\begin{center}
\includegraphics[width=0.8\linewidth]{./rmse_per_step_type1.pdf}
\caption{Performance of the model and baselines measured with RMSE on the Movielens dataset.}
\label{fig:mlens_plot}
\end{center}
\vspace{-0.5cm}
\end{figure}

\subsubsection{Setup}
We test our model in the ``cold-start'' collaborative filtering scenario using the publicly available MovieLens-20M dataset.\footnote{Available at \url{http://grouplens.org/datasets/movielens/}}
The dataset contains approximately 20M ratings on 27K movies by 138K users. The ratings are on an ordinal 10-point scale, from 0.5 to 5 with intervals of 0.5. We subsample the dataset by selecting 4000 movies and 6000 users with the most ratings. After filtering, the dataset contains approximately 1M ratings. We partition the data randomly into 5000 training users and 1000 test users. The training set represents the users already in the system who are used to fit the model parameters. We use the test users to evaluate our active learning approach. For each user, we randomly pick 50 ratings to include in the support set (movies that the user can be queried about) and 10 movies and ratings for the held-out set. We ensure that movies in the held-out set and in the support set do not overlap. We train our active learner to minimize the mean-squared error (MSE) with respect to the true rating. We adapt the prediction modules of our model to output the rating for a held-out item as follows: we compute a convex combination of the ratings of ``visible'' movies in the support set (the movies the active learner has already queried about), where the weights are given by the final attention step of the slow predictor. Although more complex strategies are possible, we empirically found this simple strategy to work well in our experiments. 
For evaluation, we sample 25000 episodes~(each comprising 50 support ratings and 10 held-out ratings from some user) from the test set and we compute the average per-user root mean-squared error (RMSE). We report the average performance obtained by 3 runs with different random seeds.

\subsubsection{Movie Embeddings}
For each movie, we pretrain an embedding vector by decomposing the full user/movie rating matrix using a latent factor model~\citep{koren2010factor}. This process only uses the training set. For each user $u$ and movie $m$, we estimate the true rating $r_{u, m}$ with a linear model $\hat{r}_{u, m} = x_u^\top x_m + b_u + b_m + \beta$, where $x_u, x_m$ are the user and movie embedding respectively and $b_u, b_m, \beta$ are the user, movie, and global bias, respectively. We train the latent factor model by minimizing the mean squared error between true rating $r$ and predicted rating $\hat{r}$. We use the trained $x_m$ as input representations for the movies throughout our experiments.

\subsubsection{Results}
In Figure~\ref{fig:mlens_plot} we report the results of our active model against various baselines.
The Regression baseline performs regularized linear regression on movies from the support set whose ratings have been observed incrementally in random order.
Because of the small amount of training data, for each additional label we tune the regularization parameter by monitoring performance on a separate set of validation episodes.
The Gaussian Process baseline selects the next movie to label in proportion to the variance of the predictive posterior distribution over its rating.
This gives an idea of the impact of using MN one-shot capabilities rather than standard regression techniques.
The Popular-Entropy, Min-Max-Cos, and Entropy Sampling baselines train our model end-to-end, but using fixed selection policies.
Specifically, we train our architecture end-to-end, but instead of training an active learning policy through the select module we choose items from the support set incrementally according to a heuristic policy.
This gives an idea of the importance of learning the selection policy.
The Popular-Entropy policy, adapted from the cold-start work of~\citet{rashid2002getting}, scores each item in the support set a priori, according to the logarithm of its popularity multiplied by the entropy of the item's ratings measured across users.
% It then iterates through the support set selecting items with probability inversely proportional to their score.
This strategy aims to first collect the ratings for those movies that are both popular and have been rated differently by different users.
Although it is simplistic, the policy achieves competitive performance for bootstrapping a system from a cold-start setting~\cite{elahi2016survey}.
The Min-Max-Cos policy is identical to the synonymous baseline used for Omniglot,~i.e. it selects the unrated movie which has minimum maximum cosine similarity to any of the rated movies.
Roughly, this selects the unrated movie which differs most from the rated movies. Entropy Sampling selects movies in proportion to rating prediction entropy.

The active policy learned end-to-end outperforms the baselines in terms of RMSE, particularly after requesting only the first few labels.
After 10 ratings, our model achieves an improvement of 2.5\% in RMSE against the best baseline.
Unsurprisingly, the gap diminishes with a higher number of labels requested.
After observing 5 labels, the Popular-Entropy baseline and our architecture equipped with the Min-Max-Cos heuristic converge toward the active policy but never quite match it.
For MovieLens, where labels are user-dependent and not tied to an underlying class, a data-driven selection policy may adapt better to the task.
This contrasts with the Omniglot setting, where there is no aspect of personalization and Active MN and Min-Max-Cos perform similarly.
The Min-Max-Cos heuristic is designed to not select items similar to those it has already seen, but selecting similar items can be beneficial in personalized settings~\cite{elahi2016survey}.

% From Figure~\ref{fig:mlens_plot}(b) it is clear that the active policy also outperforms the baselines in terms of mean absolute error (MAE), defined as the average of $|\hat{r} - r|$ over ratings. This measure represents the mean shift of each model with respect to the true rating.