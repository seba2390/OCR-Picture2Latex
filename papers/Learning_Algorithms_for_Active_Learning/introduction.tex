%!TEX root = main.tex

For many real-world tasks, labeled data is scarce while unlabeled data is abundant.
It is often possible, at some cost, to obtain labels for the unlabeled data.
In active learning, a model selects which instances to label so as to maximize some combination of task performance and data efficiency.
Active learning is motivated by the observation that a model may perform better while training on less labeled data if it can choose the data on which it trains~\citep{cohn1996active}.
E.g., in SVMs \citep{scholkopf2002} only the support vectors affect the decision boundary.
%, while often constituting a small fraction of the training set.
If one could identify the support vectors in advance, the classifier trained on the resulting set of examples would obtain the same decision boundary with less data and computation.

Active learning can benefit a variety of practical scenarios. For example, preference information for a new user in a movie recommender system may be scarce, and recommendations for the new user could be improved by carefully selecting several movies for her to rate \cite{sun2013cold,houlsby2014cold,aggarwal2016recommender}.
Likewise, collecting labels for a medical imaging task may be costly because it requires a specialist~\cite{medical}, and the cost could be reduced by carefully selecting which images to label.

Various heuristics for selecting instances to label have been proposed in the active learning literature, such as choosing the instance whose label the model is most uncertain about, or the instance whose label is expected to maximally reduce the model's uncertainty about other instances~\citep{gilad2005query,settles2010active,houlsby2011bayesian}.
We propose moving away from engineered selection heuristics towards \emph{learning} active learning algorithms end-to-end via \emph{metalearning}. Our model interacts with labeled items for many related tasks in order to learn an active learning strategy for the task at hand.
In recommendation systems, for example, ratings data for existing users can inform a strategy that efficiently elicits preferences for new users who lack prior rating data, thus \emph{bootstrapping} the system quickly out of the cold-start setting \citep{golbandi2010, golbandi2011, sun2013, kawale2015}.
A learned active learning strategy could outperform task-agnostic heuristics by sharing experience across related tasks. In particular, the model's
\begin{enumerate*}[(i)]
\item data representation, 
\item strategy for selecting items to label, and
\item prediction function constructor
\end{enumerate*}
could all co-adapt.
Moving from pipelines of independently-engineered components to end-to-end learning has lead to rapid improvements in, e.g.,~computer vision, speech recognition, and machine translation \citep{krizhevsky2012, hannun2014, he2016, wu2016}.

%In this paper, we propose moving away from engineered selection heuristics towards \emph{learning} active learning algorithms end-to-end.
%We assume that labeled instances for different but related tasks are available and that these can be leveraged to learn an active learning strategy for the task at hand.
%In recommendation, for example, there is often a vast set of labeled data for existing users that can be used to learn an active learning strategy that performs well for new users.
%We are motivated by the prospect that a selection strategy learned directly from data could outperform general, task-agnostic heuristics by transferring experience across related tasks. 
%In particular, the model's
%\begin{enumerate*}[(i)]
%\item representation of the data, 
%\item prediction function, and
%\item strategy for selecting instances to label
%\end{enumerate*}
%could all co-adapt. Moving from pipelines comprising many independently-engineered components to systems which are end-to-end trainable has lead to rapid improvements in, e.g.,~computer vision, speech recognition, and machine translation \citep{krizhevsky2012, he2016, hannun2014, wu2016}. Furthermore, end-to-end learning of active learning algorithms may be particularly beneficial whenever personalized strategies (those adapted to distinct users) can improve performance, since it is unlikely that a single heuristic applies well to all users~\citep{harpale2008personalized,houlsby2014cold}.

%Our contribution is a model that learns to actively learn and is trainable end-to-end through a combination of techniques developed for deep reinforcement and one-shot learning. Our model builds on the recently proposed Matching Networks (MN) architecture of \citet{vinyals2016matching} for one-shot learning -- that is, learning to identify a class by examining just a single labeled instance of that class~\citep{santoro2016one}.
%Our model benefits significantly from the MN's performance with few labeled instances, which is a common circumstance during active learning. By enabling meaningful prediction improvements with each observed label, one-shot learning capabilities also provide a stronger signal for use in training.
%By intertwining active learning modules and MN-style predictions, our architecture efficiently builds its training set from unlabeled data, i.e.~it chooses which unlabeled instances must be labeled to perform one-shot learning for held-out instances. This extends one-shot learning to domains where labeled instances are not available \emph{a priori}, e.g.~cold-start recommendation.
%We cast active learning as a sequential decision problem: at each step, the model requests the label for a particular instance in a \emph{support set}, containing unlabeled instances whose labels may be queried, and updates its belief about how to label any unlabeled instance based on the response.

We base our model on the Matching Networks (MN) introduced by~\citet{vinyals2016matching}.
We extend the MN's one-shot learning ability to settings where labels are not available \emph{a priori}.
We cast active learning as a sequential decision problem: at each step the model requests the label for a particular item in a pool of unlabeled items, then adds this item to a labeled support set, which is used for MN-style prediction.
We train our model end-to-end with backprop and reinforcement learning.
We expedite the training process by allowing our model to observe and mimic a strong selection policy with oracle knowledge of the labels.

We demonstrate empirically that our proposed model learns effective active learning algorithms in an end-to-end fashion.
We evaluate the model on ``active'' variants of existing one-shot learning tasks for \emph{Omniglot}~\citep{lake2015human, vinyals2016matching, santoro2016one}, and show that it can learn efficient label querying strategies.
We also test the model's ability to learn an algorithm for bootstrapping a recommender system using the \emph{MovieLens} dataset, showing it holds promise for application in more practical settings.

