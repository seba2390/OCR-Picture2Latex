%!TEX root = main.tex

% Active learning is motivated by the idea that a learning algorithm can perform better while training on fewer labeled data if it can choose the data on which it trains~\citep{cohn1996active}.
Various heuristics have been proposed to guide the selection of which examples to label during active learning~\cite{settles2010active}. For instance, \citet{lewis1994sequential} and \citet{tong2001support} developed policies based on the confidence of the classifier, while \citet{gilad2005query} used the disagreement of a \emph{committee} of classifiers. \citet{houlsby2011bayesian} presented an approach based on Bayesian information theory,
in which examples are selected in order to maximally reduce the entropy of the posterior distribution over classifier parameters. 
%Heuristic design is typically (and reasonably) based on balancing computational tractability against closeness to an optimal heuristic.

% An ideal selection heuristic would closely approximate the oracle policy, which selects the items that provide maximal information about the unlabelled set.

% There are two common AL settings: \emph{stream-based} and \emph{pool-based}~\citep{settles2010active}.
% In stream-based active learning the model decides, while observing a stream of items in random order, either to predict an item's label or to pay a cost to observe its label.
% In pool-based active learning the model has access to a static collection of unlabeled data and decides for which items to observe labels and in what order.

The idea of learning an active learning algorithm end-to-end, via \emph{meta} active learning, was recently investigated  by~\citet{activeoneshot}. Building on the memory-augmented neural network (MANN)~\citep{santoro2016one}, the authors developed a \emph{stream-based} active learner. In stream-based active learning the model decides, while observing items presented in an exogenously-determined order, whether to predict each item's label or to pay a cost to observe its label. Our proposed model instead falls into the class of \emph{pool-based} active learners, i.e.~it has access to a static collection of unlabeled data and selects both the items for which to observe labels, and the order in which to observe them.

%
% This needs fixing.
%
% Active learning is useful when the cost balance between prediction error and labeling
% can vary. I.e., if prediction error is always more costly, we should always ask for labels,
% and if labeling is always more costly, we should always predict (even if wildly wrong).
%
% We expect active learning to be useful when sometimes it is more costly to make a
% prediction error (e.g. because the model may be _very_ wrong, or correctness is critical),
% and sometimes more costly to request a label.
% 
% If the relative costs of error and labeling never swapped, we would always prefer to do
% the one with lower cost, which would make active learning irrelevant.
%
Active learning can be useful when the cost incurred for labeling an item may be traded for lower prediction error, and where the model must be data efficient (e.g.~in medical imaging~\citep{medical}). We explicitly train our model to balance between task performance and labeling cost. In this sense, we build an \emph{anytime} active learner~\cite{zilberstein1996using}, with the model trained at each step to output the best possible prediction on the evaluation set.

Our model builds on the matching-networks (MN) architecture presented by~\citet{vinyals2016matching}, which enables ``one-shot'' learning, i.e.~learning the appearance of a class from just a single example of that class~\cite{santoro2016one,koch2015siamese}.~\citet{vinyals2016matching} assume that at least one example per class exists in the labeled support set available to the model. Confronted with the harder task of composing a labeled support set from a larger pool of unlabeled examples, we show that the active learning policy learnt by our model obtains, in some cases, an equally effective support set. As in the recent one-shot learning work of \citet{santoro2016one} and \citet{vinyals2016matching}, and the active learning work of \citet{activeoneshot}, we evaluate our model on the \emph{Omniglot} dataset. This dataset was developed for the foundational one-shot learning work of \citet{lake2015human}, which focused on probabilistic program induction.

%% Speak about personalization here.
The cold-start problem is ubiquitous in recommendation systems~\citep{aggarwal2016recommender,rs1,harpale2008personalized,sun2013cold,elahi2016survey}. Instead of bootstrapping from a cold-start by randomly selecting items for a user to rate, an active learner asks for particular items to help learn a strong user model more quickly. In model-free strategies~\cite{rashid2008learning}, items are selected according to general empirical statistics such as popularity or informativeness. These approaches are computationally cheap, but lack the benefits of adaptation and personalization.
Proposals for learning an adaptive selection strategy have been made in the form of Bayesian methods that learn the parameters of a user model~\cite{houlsby2014cold,harpale2008personalized}, and in the form of decision-trees learned from existing ratings~\cite{sun2013cold}.
An extensive review can be found in~\citet{elahi2016survey}.
Intuitively, our model learns a compact, parametric representation of a decision tree end-to-end, by directly maximizing task performance. We evaluate our active learner on MovieLens-20M, a standard dataset for recommendation tasks.

We provide hints to our model during training using samples from an oracle policy that knows all the labels. Related approaches have been explored in previous work on imitation learning and learning to search \cite{ross2014, chang2015}. These methods, which focus the cost of sampling from the oracle policy on states visited by the model policy, have recently been adopted by researchers working with deep networks for representation learning \cite{zhang2017, sun2017}.
