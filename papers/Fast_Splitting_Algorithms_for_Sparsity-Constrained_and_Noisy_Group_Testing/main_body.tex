\vspace*{-2ex}
\section{Introduction}

In the group testing problem, the goal is to identify a small subset $\mathcal{S}$ of defective items of size $k$ within a larger set of items of size $n$, based on a number $T$ of tests. This problem is relevant in areas such as medical testing, DNA sequencing, and communication protocols \cite[Sec.~1.7]{Ald19}, and more recently, utility in testing for COVID-19 \cite{Hogan2020,Yelin2020}.

In this paper, we present algorithms for sparsity-constrained (bounded tests-per-item or items-per-test) and noisy variants of group testing with a near-optimal sublinear decoding time, building on techniques recently proposed for the unconstrained noiseless group testing problem \cite{cher20,Eri20}. % and the sparse group testing problem \cite{Ven19,Nel20}. 
These extensions come with new challenges presented by the infeasibility of the designs in \cite{cher20,Eri20} in the sparsity-constrained setting, and the need to handle both false positive and false negative tests in the noisy setting.

\subsection{Problem Setup}

Let $n$ denote the number of items, which we label as $\{1,\dots,n\}$. Let $\mathcal{S}\subset\{1,\dots,n\}$ denote the fixed set of defective items, and let $k=|\mathcal{S}|$ be the number of defective items.  To avoid cumbersome notation, we present our algorithms in a form that uses $k$ directly; however, the analysis goes through unchanged when an upper bound $\bar{k} \ge k$ is used instead, and $\bar{k}$ replaces $k$ in the number of tests and decoding time.

We are interested in asymptotic scaling regimes in which $n$ is large and $k$ is comparatively small, and thus assume that $k=o(n)$ throughout. We let $T=T(n)$ be the \textit{number of tests} performed. In the noiseless setting, the $i$-th test takes the form 
\begin{align}
    Y^{(i)}=\bigvee_{j\in\mathcal{S}}X_j^{(i)}, \label{eq:test_outcome_formula}
\end{align}
where the test vector $X^{(i)}=\big(X_1^{(i)},\dots,X_n^{(i)}\big)\in\{0,1\}^n$ indicates which items are are included in the test, and $Y^{(i)}\in\{0,1\}$ is the resulting observation, indicating whether at least one defective item was included in the test. The goal of group testing is to design a sequence of tests $X^{(1)},\dots,X^{(T)}$, with $T$ ideally as small as possible, such that the outcomes can be used to reliably recover the defective set $\mathcal{S}$ with probability close to one, while ideally also having a low-complexity decoding procedure. We focus on the non-adaptive setting, in which all tests $X^{(1)},\dots,X^{(T)}$ must be designed prior to observing any outcomes.

We consider the \textit{for-each} recovery guarantee; specifically, we seek to develop a randomized algorithm that, for any fixed defective set $\mathcal{S}$ of cardinality $k$, produces an estimate $\widehat{\mathcal{S}}$ such that the error probability $P_e:=\mathbb{P}\big[\widehat{\mathcal{S}}\neq\mathcal{S}\big]$ is asymptotically vanishing as $n \to \infty$. For all of our algorithms, only the tests $\big\{X^{(i)}\big\}_{i=1}^T$ will be randomized, and the decoding procedure will be deterministic given the test outcomes.

\textbf{Notation.} Throughout the paper, the function $\log(\cdot)$ has base $e$, and we make use of Bachmann-Landau asymptotic
notation (i.e., $O$, $o$, $\Omega$, $\omega$, $\Theta$), as well as the notation $\widetilde{O}(\cdot)$, which omits poly-logarithmic factors in its argument.

\subsubsection{Sparsity-Constrained Setting}

In the sparsity-constrained group testing problem \cite{Ven19}, the testing procedure is subjected to one of two constraints:
\begin{itemize}
    \item Items are \textit{finitely divisible} and thus may participate in at most $\gamma=o(\log n)$ tests;
    \item Tests are \textit{size-constrained} and thus contain no more than $\rho=o(n/k)$ items per test.
\end{itemize}
For instance, in the classical application of testing blood samples for a given disease \cite{Dor43}, the $\gamma$-divisible items constraint may arise when there are limitations on the volume of blood provided by each individual, and the $\rho$-sized test constraint may arise when there are limitations on the number of samples that the machine can accept, or on the number that can be mixed together while avoiding undesirable dilution effects.

It is well known that if each test comprises of $\Theta(n/k)$ items, then $\Theta(\min\{n,k\log n\})$ tests suffice for group testing algorithms with asymptotically vanishing error probability \cite{Cha14,Ald14a,Sca15b,Joh16}.  Moreover, this scaling is known to be optimal \cite{Bay20}.  Hence, the parameter regime of primary interest in the size-constrained setting is $\rho=o(n/k)$. By a similar argument, the parameter regime of primary interest in the finitely divisible setting is $\gamma=o(\log n)$.  

\subsubsection{Noisy Setting} \label{sec:noise_intro}

Generalizing \eqref{eq:test_outcome_formula}, we consider the following widely-adopted symmetric noise model:
\begin{align}
    Y^{(i)}=\bigg(\bigvee_{j\in\mathcal{S}}X_j^{(i)}\bigg)\oplus Z, \label{eq:noise}
\end{align}
where $Z\sim\text{Bernoulli}(p)$ for some $p\in(0,1/2)$, and $\oplus$ denotes modulo-2 addition. While the symmetry assumption may appear to be restrictive, our results and analysis will hold with essentially no change under any non-symmetric random noise model where $0 \to 1$ flips and $1 \to 0$ flips both have probability at most $p$.

Throughout the paper, we will focus {\em separately} on the sparsity-constrained aspects and noisy aspects.  While their joint treatment is also of interest, it was shown in \cite{Ven19} that for finitely divisible items, if the tests are subject to random noise of the form in \eqref{eq:noise}, then the error probability is bounded away from zero regardless of the total number of tests in the finitely-divisible setting with $\gamma = o(\log k)$.  Thus, at least in most scaling regimes of interest, handling noise and finite-divisibility constraints simultaneously would require changing the noise model and/or the recovery criteria, and we make no attempt to do so. On the other hand, for noisy size-constrained tests, schemes that attain asymptotically vanishing error probability do indeed exist \cite{Ven19}.  We still focus on the size-constrained and noisy aspects separately for clarity of exposition, but the two can be combined using our techniques in a straightforward manner, as we briefly discuss in Appendix \ref{sec:non_binary}.

\subsubsection{Mathematical and Computational Assumptions}
Throughout the paper, we assume a word-RAM model of computation; for instance, with $n$ items and $T$ tests, it takes $O(1)$ time to read a single integer in $\{1,\dots,n\}$ from memory, perform arithmetic operations on such integers, fetch a single test outcome indexed by $\{1,\dots,T\}$ and so on.

For simplicity of notation, we assume throughout the analysis that $k$, $n$, and $\rho$ are powers of two. Our algorithm only requires an upper bound on the number of defectives, and hence, any other value of $k$ can simply be rounded up to a power of two. In addition, the total number of items $n$ can be increased to a power of two by adding ``dummy'' non-defective items, and $\rho$ can be rounded down without impacting our final scaling laws (we do not seek to characterize the precise constants).

\subsection{Related Work}

While extensive works have studied the number of tests for various group testing strategies (see \cite{Ald19} for a survey), relatively fewer have sought efficient ${\rm poly}(k \log n)$ decoding time.  For the standard noiseless group testing problem, the most relevant existing results come from two recent concurrent works \cite{cher20,Eri20}, which showed that there exists a non-adaptive group testing algorithm that succeeds with $O(k\log n)$ tests and has $O(k\log n)$ decoding time. We build on these splitting techniques in this paper; the existing approach is outlined in Section \ref{sec:binary_split} below, illustrations of our variants are shown Figures \ref{fig:test_constraint_diagram_3cases}, \ref{fig:size_constraint_diagram}, and \ref{fig:noisy_algo_diag} below, and we highlight the algorithmic differences and key ideas the start of each respective section.

%work are as follows:
%\begin{itemize}
%    \item Under the symmetric noise model in \eqref{eq:noise}, there exist several algorithms that require $\Omega(n)$ decoding time. The noisy combinatorial orthogonal matching pursuit (NCOMP) algorithm \cite{Cha14,Oli20a} and linear programming \cite{Cha14} succeed with $O(k\log n)$ tests and have $\Omega(n)$ decoding time. The constant factors of these bounds were further improved in \cite{Sca17b} using \textit{separate decoding of items}, and in \cite{Sca18b,Oli20a} using the noisy definite defectives (DD) algorithm. In this paper, our focus is on the scaling laws instead of the constant factors, but unlike the works just mentioned, we seek sublinear decoding time.
%    \item As for existing sublinear-time algorithms, \cite{Cai13} showed that under the noise model in \eqref{eq:noise}, the GROTESQUE algorithm succeeds with $O(k\log k\cdot\log n)$ tests and $O\big(k(\log n+\log^2k)\big)$ decoding time. Furthermore, the robustified SAFFRON algorithm succeeds with $O(k\log k\cdot\log n)$ tests and $O(k\log k\cdot\log n)$ decoding time (or $O(k\log k)$ under the word-RAM model) \cite{Lee16}.
%    \item The recent work \cite{cher20} additionally provides recovery guarantees under adversarial noise, tolerating a certain number of false positive tests or false negative tests.  However, this part of their work focuses on the for-all recovery guarantee requiring significantly more tests (see below), so is not directly comparable to our for-each style results.  In addition, it was left open how to handle both false positives and false negatives simultaneously.
    % However, a consequence of \cite[Thm.~8.1]{cher20} is that with $O(k \log n)$ tests, one can tolerate up to $O(k \log k)$ adversarially-chosen for-all style recovery guarantees (discussed further below) under various recovery guarantees.  
    % \item Under adversarial noise, recent work from \cite{cher20} shows that (i) there exists a non-adaptive algorithm that succeeds with $O(k^2\log n+e_0\log_kn)$ tests, which can tolerate up to $e_0$ worst-case false positives, and has $O(T\log n)$ decoding time, and (ii) there exists a non-adaptive algorithm that succeeds with $O(k^2\log n+e_1k)$ tests, which can tolerate up to $e_1k$ worst-case false negatives, and has $O\big(T\text{poly}(\log n)\big)$ decoding time. However, it was left open how to handle both false positives and false negatives simultaneously.
% \end{itemize}

For noiseless sparsity-constrained group testing, the most relevant existing results are summarized in Table \ref{tab:sparse_algo_summary}. Our algorithm for finitely divisible items matches that of the COMP algorithm\footnote{The COMP algorithm simply labels any item in an negative test as non-defective, and all other items as defective.} in the number of tests when $\gamma = \omega(1)$ (and comes close more generally), while having much lower decoding time. Furthermore, our algorithm for size-constrained tests uses an order-optimal $O(n/\rho)$ number of tests, and has matching $O(n/\rho)$ decoding time.

For noisy non-adaptive group testing under the noise model in \eqref{eq:noise}, the most relevant existing results are summarized in Table \ref{tab:noisy_algo_summary}.  Under $\Omega(n)$-decoding time, we note that the references shown are only illustrative examples, and that several additional works also exist with $O(k \log n)$ scaling, e.g., \cite{Mal78,Sca17b,Oli20a}.  More relevant to our work is the fundamental limitation that the works attaining $O(k \log n)$ scaling only attain a quadratic or worse dependence in $k$ in the decoding time (or $\Omega(n)$).  On the other hand, GROTESQUE and SAFFRON attain $k \, {\rm poly}(\log n)$ decoding time, but fail to attain order-optimality in the number of tests.

In a distinct but related line of works, the for-all recovery guarantee (i.e., zero error probability) was considered \cite{Che09,Ind10,Ngo11,Hus19,cher20}, with typical results for the unconstrained setting requiring $O(k^2 \log n)$ tests and ${\rm poly}(k \log n)$ decoding time.  In particular:
\begin{itemize}
    \item In the finitely divisible setting, \cite{Hus19} gives a lower bound of $\Omega\big(\min\big\{n,k^{\frac{2k}{\gamma-1+k}}n^{\frac{k}{\gamma-1+k}}\big\}\big)$ and an algorithm that requires $O\big(\min\big\{n,kn^{\frac{k}{\gamma-1+k}}\big\}\big)$ tests and runs in $\text{poly}(k)+O(T)$ time in the case of $\gamma$-divisible items, and a lower bound of $\Omega\big(k\frac{n}{\rho}\big)$ and an algorithm that requires $T = O\big(k\frac{n}{\rho}\big)$ tests and runs in $\text{poly}(k)+O(T)$ time in the case of $\rho$-sized tests. 
    \item In a setting with {\em adversarial} noise, recovery guarantees were given in \cite[Thms.~3.8 and 3.9]{cher20} with a constraint on the number of false positive tests or false negative tests.  It was left open how to handle both false positives and false negatives simultaneously.
\end{itemize}
Under all variants of the group testing problem that we consider, the stronger for-all guarantee comes at the price of requiring considerably more tests.  Thus, the two types of guarantee are both of significant interest but not directly comparable, and we omit direct comparisons.

Finally, we briefly mention that studies of sublinear-time decoding are prevalent in related problems such as sparse recovery \cite{Cor06,Gil07,Ber08a,Ind11} and the heavy hitters problem \cite{Cor05a,Cor08,Lar19}.  While algorithms for such settings typically do not transfer directly to the group testing problem, we detail one relatively direct approach for the noisy setting in Appendix \ref{sec:non_binary}, and contrast it with our own.  In addition, we note that our work builds primarily on \cite{cher20,Eri20}, which in turn built on tree-based algorithms such as \cite{Cor05a,Ind11}.

\subsection{Overview of Binary Splitting Approach} \label{sec:binary_split}

Since we build directly on the fast binary splitting approach of \cite{cher20,Eri20}, we briefly summarize it here.  An illustration is given in Figure \ref{fig:binary_split}.  The items are arranged into recursively-defined groups in a sequence of levels, where Level 0 contains all items, subsequent levels recursively split the previous groups in half, and the final level contains individual items.  The shaded groups in the left part of Figure \ref{fig:binary_split} are those containing defectives, and an equivalent tree representation (with nodes corresponding to groups) is shown on the right.

At each level,\footnote{For improved efficiency, one can skip the early levels and start where there are $k$ groups of size $\frac{n}{k}$ each, up to rounding.} a suitably-chosen number of \emph{non-adaptive random tests} is performed, where items in a group are always tested together.  Whenever a group is in a negative test, the algorithm knows (in the noiseless setting) that its items must be non-defective.  Hence, when moving from one level to the next, only the sub-groups of groups in positive tests are kept under consideration.  At the final level, sufficiently many random tests are performed to identify the status of every item that has not yet been ruled out.  We refer the reader to \cite{cher20,Eri20} for further details.

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.85\textwidth]{images/DefectiveTree.pdf}
    \caption{Illustration of the fast binary splitting approach for non-adaptive group testing proposed in \cite{cher20,Eri20}.  Here we have $n=16$ items and $k=3$ defectives; the groupings of items are shown on the left, and the tree representation (where internal nodes correspond to groups of nodes) is shown on the right.} \label{fig:binary_split}
\end{figure}

% Sparse Group testing table %
\begin{table} [t]
\centering
\begin{tabular}{|c|c|c|c|c|}
\hline
& Reference & Number of tests & Decoding time & Construction \\
\hline \hline
\multirow{5}{*}{\STAB{\rotatebox[origin=c]{90}{\makecell{$\gamma$-divis.~items~~}}}}
& Lower Bound \cite{Nel20,Oli20} & $\Omega\big(\gamma k\max\big\{k,\frac{n}{k}\big\}^{1/\gamma}\big)$ & -- & -- \\
& Gandikota {\em et al.} \cite{Ven19} & $O\big(\gamma k^2\big(\frac{n}{k^2}\big)^{1/\gamma}\big)$ & $O\big(k^2\log\big(\frac{n}{k^2}\big)\big)$ & Explicit \\
& COMP \cite{Ven19} & $\widetilde{O}(\gamma kn^{1/\gamma})$ & $\Omega(n)$ & Randomized \\
& DD \cite{Oli20} & $O\big(\gamma k\max\big\{k,\frac{n}{k}\big\}^{\frac{1}{\gamma}}\big)$ & $\Omega(n)$ & Randomized \\
& This Paper & $\widetilde{O}(\gamma kn^{1/\gamma})$ & $O(\gamma kn^{1/\gamma})$ & Randomized \\
\hline
\hline
\multirow{5}{*}{\STAB{\rotatebox[origin=c]{90}{\makecell{$\rho$-sized tests~~}}}}
& Lower Bound \cite{Ven19,Oli20} & $\Omega\big(\frac{n}{\rho}\big)$ & -- & -- \\
& Gandikota {\em et al.} \cite{Ven19} & \makecell{$O\big(\max\big\{\frac{n}{\rho}\log\rho,$\\$k^2\log\big(\frac{n}{k^2}\big)\big\}\big)$} & $O(T)$ & Explicit \\
& COMP \& DD \cite{Ven19, Oli20} & $O\big(\frac{n}{\rho}\big)$ & $\Omega(n)$ & Randomized \\
& This Paper & $O\big(\frac{n}{\rho}\big)$ & $O\big(\frac{n}{\rho}\big)$ & Randomized \\
\hline
\end{tabular}
\caption{Overview of noiseless non-adaptive sparsity-constrained group testing results under the for-each guarantee. For entries containing $\widetilde{O}(\cdot)$ notation, the results correspond to $\frac{1}{{\rm poly}(\log n)}$ error probability, but more general variants are also available. A construction is said to be explicit if its test matrix can be computed deterministically in ${\rm poly}(n)$ time; the results shown for explicit constructions additionally require $k = O(\sqrt n)$.}
\label{tab:sparse_algo_summary}
% \vspace*{-1ex}
\end{table}

% Noisy Group testing table %
\begin{table} [t]
    \centering
    \begin{tabular}{|c|c|c|c|}
        \hline
        Reference & Number of tests & Decoding time & Construction \\
        \hline \hline
        Lower Bound \cite{Mal78} & $\Omega\big(k\log\frac{n}{k}\big)$ & -- & -- \\
        Inan {\em et al.} \cite{Ina19} & $O(k\log n)$ & $\Omega(n)$ & Explicit \\
        Inan {\em et al.}~(fast) \cite{Ina20} & $O(k\log n)$ & $O\big(k^3\cdot\log k+k\log n\big)$ & Explicit \\
        NCOMP \& NDD \cite{Cha14,Sca18b} & $O(k\log n)$ & $\Omega(n)$ & Randomized \\
        GROTESQUE \cite{Cai13} & $O(k\cdot\log k\cdot\log n)$ & $O\big(k(\log n+\log^2k)\big)$ & Randomized \\
        SAFFRON \cite{Lee16} & $O(k\cdot\log k\cdot\log n)$ & $O(k\cdot\log k\cdot \log n)$ & Randomized \\
        BMC \cite{Bon19a} & $O(k\log n)$ & $O(k^2\cdot \log k\cdot \log n)$ & Randomized \\
        This Paper & $O(k\log n)$ & $O\big(\big(k\log\frac{n}{k}\big)^{1+\epsilon}\big)$ & Randomized \\
        \hline
    \end{tabular}
    \caption{Overview of noisy non-adaptive group testing results under the for-each guarantee and the noise model in \eqref{eq:noise}. A construction is said to be explicit if its test matrix can be computed deterministically in ${\rm poly}(n)$ time, and in the final row, $\epsilon$ is an arbitrarily small positive constant.}
    \label{tab:noisy_algo_summary}
    % \vspace*{-4ex}
\end{table}

\subsection{Summary of Results}

Here we informally summarize our main results, formally stated in Theorems \ref{thm:noisy_main_theorem}, \ref{thm:gamma_main_theorem}, and \ref{thm:rho_main_theorem}.

\begin{itemize}
    \item \textbf{Finitely divisible items:} A special case of our result states that for any $\beta_n = \frac{1}{{\rm poly}(\log n)}$, there exists a non-adaptive group testing algorithm that succeeds with probability $1-O(\beta_n)$ using $\widetilde{O}\big(\gamma kn^{1/\gamma}\big)$ tests and $O\big(\gamma kn^{1/\gamma}\big)$ decoding time provided that $\gamma = \omega(1)$.  The case of finite $\gamma$ will also be handled with only slightly worse scaling laws, and we will specify the precise dependence on $\beta_n$, without resorting to $\widetilde{O}(\cdot)$ notation.
    \item \textbf{Size-constrained tests:} For any $\zeta>0$, there exists a non-adaptive group testing algorithm that succeeds with probability $1-O\big(n^{-\zeta}\big)$ using $O\big(n/\rho\big)$ tests and $O\big(n/\rho\big)$ decoding time.
    \item \textbf{Noisy setting:} For any parameters $t=O(1)$ and $\epsilon\in(1/t,1)$, there exists a non-adaptive group testing algorithm that succeeds with probability $1-O\big(\big(k\log\frac{n}{k}\big)^{1-\epsilon t}\big)$ using $O(k\log n)$ tests and $O\big(\big(k\log\frac{n}{k}\big)^{1+\epsilon}\big)$ decoding time.
\end{itemize}
We observe that in the sparsity-constrained setting, our decoding time matches the number of tests, whereas previous algorithms using the same number of tests incurred $\Omega(n)$ decoding time.  Similarly, in the noisy setting, we significantly improve on the best previous known decoding time among any algorithm using an order-optimal $O(k \log n)$ number of tests.  Specifically, \cite{Bon19a} incurred a quadratic dependence on $k$, whereas we incur a near-linear dependence.

Each of the above results comes with significant differences in the algorithms and mathematical analyses compared to the noiseless unconstrained setting handled in \cite{Eri20,cher20}.  We defer discussions of these differences to the beginning of the respective sections to follow.

While our focus is on the number of tests and decoding time, another important practical consideration is the storage required.  Naively, the algorithms attaining the above results require $\Omega(n)$ storage.  However, in Appendix \ref{sec:storage_reductions}, we discuss storage reductions via hashing, attaining identical results with sublinear storage in the size-constrained and noisy settings, and similar (but slightly weaker) results in the finitely divisible setting.

% While the preceding results all require $\Omega(n)$ storage to record the test mapping(s) of each node, the storage can be reduced by using hash functions to allocate items to their respective test. This is further discussed in Section \ref{sec:storage_reductions}.

\section{Algorithm for Finitely Divisible Items}

Our algorithm (both here and in subsequent sections) resembles the non-adaptive binary splitting approach of \cite{cher20,Eri20}.  At a high level, we form large groups of items and recursively split them into smaller sub-groups, then randomly place groups into tests.  The decoder works down the resulting tree (see Figure \ref{fig:test_constraint_diagram_3cases}), eliminating groups that are believed to be defective based on the test outcomes, while recursively handling all remaining groups.

We highlight the following differences compared to the binary splitting approach \cite{cher20,Eri20}:
\begin{itemize}
    \item We use a shorter tree of height $\gamma'\leq\gamma$.  This is because a given item is placed in a single test at each level, so the assumption $\gamma = o(\log n)$ prohibits us from having $O(\log n)$ levels.  We consider $\gamma' \le \gamma$ so that the remaining budget can be used at the final level, and we later optimize $\gamma'$ to minimize the number of tests.
    \item In view of the shorter height, we use {\em non-binary} splitting; this was considered under adaptive testing in \cite{Nel20,Oli20}, and our algorithm can be viewed as a non-adaptive counterpart, in the same way that \cite{cher20,Eri20} can be viewed as a non-adaptive counterpart of Hwang's binary splitting algorithm \cite{Hwa72}.
    \item In contrast to the unconstrained setting, we cannot readily use the idea of using $N$ sequences of tests at each level while only increasing the number of tests by a factor of $N = O(1)$.  Here, such an approach turns out to be highly wasteful in terms of its use of the limited $\gamma$ budget, and we avoid it altogether.
    \item At the top level of the tree (excluding the root), we use individual testing (i.e., each node has its own test). This guarantees that no non-defective node from the second level can ``continue'' down the tree, which simplifies our analysis.
\end{itemize}


\subsection{Description of the Algorithm} \label{sec:gamma_algo_descrip}

The levels of the tree, summarized in Figure \ref{fig:test_constraint_diagram_3cases}, are indexed by $l=0,1,\dots,\gamma'$. Since testing at the root is not informative (we will always get a positive outcome), we start our testing procedure at $l=1$ (the second level of nodes in Figure \ref{fig:test_constraint_diagram_3cases}). We choose\footnote{Here and subsequently, we assume for notation convenience that $(n/k)^{1/\gamma}$ and $(n/k)^{1/\gamma'}$ are integers.  Since we focus on scaling laws, the resulting effect of rounding has no impact on our results.} $M=(n/k)^{\frac{\gamma'-1}{\gamma'}}$, $T_{\text{len}}=Ck(n/k)^{1/\gamma'}$ and $T'_{\text{len}}=\gamma'k(n/k)^{1/\gamma'}$, where $C$ is a constant.  Here the choice of $M$ is taken to match the near-optimal adaptive splitting algorithm of \cite{Nel20}, and the choices of $T_{\text{len}}$ and $T'_{\text{len}}$ are motivated by the goal of having a number of tests matching the COMP algorithm (see Table \ref{tab:sparse_algo_summary}).  Under these preceding choices, the total number of tests (excluding the last level) is given by
\begin{align}
    \underbrace{\frac{n}{M}}_{l=1}+\underbrace{\gamma'\cdot Ck\Big(\frac{n}{k}\Big)^{\frac{1}{\gamma'}}}_{l=2,\dotsc,\gamma'-2}+\underbrace{\gamma'k\Big(\frac{n}{k}\Big)^{\frac{1}{\gamma'}}}_{l=\gamma'-1}
    &=O\bigg(\gamma'k\Big(\frac{n}{k}\Big)^{\frac{1}{\gamma'}}\bigg).
\end{align} 
The overall testing procedure is described in Algorithm \ref{alg:gamma_nonadap_testing}, and the decoding procedure is described in Algorithm \ref{alg:gamma_nonadap_decoding}.  The $j$-th node at the $l$-th level is again written as $\mathcal{G}_j^{(l)}$. 

Here and subsequently, we assume that $\gamma \ge 3$.  We note that the case $\gamma = 1$ is trivial, and while $\gamma = 2$ could be handled by omitting the step at level $l = \gamma'$ containing $T''_{\rm len}$ tests, this variant is omitted for the sake of brevity.

\begin{figure}[!t]
  \centering
  \includegraphics[scale=0.4]{images/test_constraint_diag_3cases.pdf}
  \caption{Tree structure of our algorithm. After the first level, the branching factor is $M^{\frac{1}{\gamma'-1}}$.} \label{fig:test_constraint_diagram_3cases}
\end{figure}

\begin{algorithm}[!t]
    \begin{algorithmic}[1]
        \REQUIRE Number of items $n$, number of defective items $k$, divisibility of each item $\gamma$, and parameters $\gamma'$, $M$, $T_{\text{len}}$, $T_{\text{len}}'$, and $T_{\text{len}}''$
        \STATE At $l=1$, test each node separately in a single test (no randomization).
        \FOR{each $l=2,3,\dots,\gamma'-1$}{
        \STATE \textbf{if} $l=\gamma'-1$ \textbf{then} form a sequence of tests of length $T'_{\text{len}}$.
        \STATE \textbf{else} form a sequence of tests of length $T_{\text{len}}$.
        \FOR{$j=1,2,\dots,\frac{n}{M}(M)^{(l-1)/(\gamma'-1)}$}{
        \STATE Place all items from $\mathcal{G}_j^{(l)}$ into a single test within the sequence just formed, chosen uniformly at random.
        }\ENDFOR
        }\ENDFOR
        \STATE For $l=\gamma'$, form $\gamma-\gamma'+1$ sequences of tests, each of length $T''_{\text{len}}$.
        \FOR{each singleton at the final level}{
        \FOR{each of the $\gamma-\gamma'+1$ sequences of tests}{
        \STATE Place the item in one of the tests in the sequence, chosen uniformly at random.
        }\ENDFOR
        }\ENDFOR
    \end{algorithmic}
    \caption{Testing procedure for $\gamma$-divisible items \label{alg:gamma_nonadap_testing}}
\end{algorithm}

\begin{algorithm}[!t]
    \begin{algorithmic}[1]
        \REQUIRE Outcomes of $T$ non-adaptive tests, number of items $n$, number of defective items $k$, divisibility of each item $\gamma$, and parameters $\gamma'$, $M$, $T_{\text{len}}$, $T_{\text{len}}'$, and $T_{\text{len}}''$
        \STATE Initialize $\mathcal{PD}^{(l_{\text{min}})}=\big\{\mathcal{G}_j^{(l_{\text{min}})}\big\}_{j=1}^{n/M}$, where $l_{\text{min}}=1$.
        \STATE Place all nodes at $l=1$ with a positive test outcome into $\mathcal{PD}^{(l_{\text{min}})}$.
        \FOR{$l=2,3,\dots,\gamma'-1$}{
        \FOR{each group $\mathcal{G}\in\mathcal{PD}^{(l)}$}{
        \STATE Check whether the single test corresponding to $\mathcal{G}$ is positive or negative.
        \STATE \textbf{if} the test is positive \textbf{then} add all $M^{1/(\gamma'-1)}$ children of $\mathcal{G}$ to $\mathcal{PD}^{(l+1)}$
        }\ENDFOR
        }\ENDFOR
        \STATE Let the estimate $\widehat{\mathcal{S}}$ of the defective set be the elements in $\mathcal{PD}^{(\gamma')}$ that are not included in any of the negative tests from the remaining $(\gamma-\gamma'+1)T_{\text{len}}''$ tests.
        \STATE Return $\widehat{\mathcal{S}}$.
    \end{algorithmic}
    \caption{Decoding procedure for $\gamma$-divisible items \label{alg:gamma_nonadap_decoding}}
\end{algorithm}

\subsection{Algorithmic Guarantees}

\begin{theorem} \label{thm:gamma_main_theorem}
Let $\mathcal{S}$ be a fixed (defective) subset of $\{1,\dots,n\}$ of cardinality $k$, and let $\gamma=o(\log n)$ (with $\gamma \ge 3$) be the maximum number of times each item can be tested, and fix $\gamma'\in\{3,\dots,\gamma\}$ and any function $\beta_n$ decaying as $n$ increases. There exist choices\footnote{Specifically, we will set $T_{\text{len}}=O\big( k(n/k)^{1/\gamma'} \big)$, $T'_{\text{len}}=\gamma'k(n/k)^{1/\gamma'}$, and $T_{\text{len}}''=k(k/\beta_n)^{\frac{1}{\gamma-\gamma'+1}}(n/k)^{\frac{1}{\gamma'(\gamma-\gamma'+1)}}$.} of $T_{\rm len}$, $T'_{\rm len}$, and $T''_{\rm len}$ such that with 
\begin{align}
    T&=O\bigg(\gamma k\max\bigg\{\Big(\frac{n}{k}\Big)^{\frac{1}{\gamma'}},
    \Big(\frac{k}{\beta_n}\Big)^{\frac{1}{\gamma-\gamma'+1}}\Big(\frac{n}{k}\Big)^{\frac{1}{\gamma'(\gamma-\gamma'+1)}}\bigg\}\bigg),
    \label{eq:gamma_main_thm_test} 
\end{align}
the preceding algorithm satisfies the following with probability at least $1-O(\beta_n)-e^{-\Omega(k)}$:
\begin{itemize}
    \item The returned estimate $\widehat{\mathcal{S}}$ equals $\mathcal{S}$;
    \item The decoding time is\footnote{In certain scaling regimes, this decoding time may be lower than the number of tests. This is because the algorithm sequentially decides which tests outcomes to observe, and does not necessarily end up observing every outcome.} $O\big(\gamma k(n/k)^{1/\gamma'}\big)$.
    % \item The algorithm uses
    % \begin{align}
    %     O\bigg(k\Big(\frac{n}{k}\Big)^{\frac{1}{\gamma'}}\log n
    %     +n\Big(\frac{n}{k}\Big)^{-\frac{1}{\gamma'}}\log\bigg(\gamma'k\Big(\frac{n}{k}\Big)^{\frac{1}{\gamma'}}\bigg)
    %     +\gamma n\log\bigg(k\Big(\frac{k}{\beta_n}\Big)^{\frac{1}{\gamma-\gamma'+1}}\Big(\frac{n}{k}\Big)^{\frac{1}{\gamma'(\gamma-\gamma'+1)}}\bigg)\bigg)
    % \end{align}
    % bits of storage.
\end{itemize}
\end{theorem}
The proof of Theorem \ref{thm:gamma_main_theorem} is given in Appendix \ref{sec:gamma_algo_analysis}.  It consists of bounding the probabilities of non-defective nodes being ``reached'' (i.e., considered possibly defective in Line 4 of Algorithm \ref{alg:gamma_nonadap_decoding}) based on their distance to the nearest defective node.  More distant nodes have a smaller associated probability, and we can leverage this to bound the overall number of nodes visited (and hence the decoding time).  A separate analysis is also performed for the final level to show that the final estimate is correct.

\begin{figure}[!t]
  \centering
  \includegraphics[scale=0.5]{images/eta_plot_diff_gammas.pdf}
  \caption{Plot of the asymptotic quantity $\eta$ (see \eqref{eq:eta}) against the sparsity parameter $\theta$ for the converse (i.e., the lower bound) \cite{Oli20}, the DD algorithm \cite{Oli20}, the COMP algorithm \cite{Ven19}, and our splitting algorithm (with $\gamma=4$ and $\gamma = 10$).} \label{fig:eta_plot_diff_gammas}
  \vspace*{-3ex}
\end{figure}

In order to better understand this bound on $T$, we consider $k=\Theta\big(n^{\theta}\big)$ for some $\theta\in[0,1)$, and $\beta_n = \frac{1}{{\rm poly}(\log n)}$.  These choices allow us to hide the dependence on $\beta_n$ in $\widetilde{O}(\cdot)$ notation and focus on the remaining terms. Substituting $k=\Theta\big(n^{\theta}\big)$ into \eqref{eq:gamma_main_thm_test}, we obtain
\begin{align}
    T&=\widetilde{O}\Big(\gamma k\max\Big\{n^{\frac{1-\theta}{\gamma'}},
    n^{\frac{\theta}{\gamma-\gamma'+1}+\frac{1-\theta}{\gamma'(\gamma-\gamma'+1)}}\Big\}\Big). \label{eq:regime_specific_T}
\end{align}
Momentarily ignoring the integer constraint on $\gamma'$, we obtain the optimal $\gamma'$ by solving $\frac{1-\theta}{\gamma'}=\frac{\theta}{\gamma-\gamma'+1}+\frac{1-\theta}{\gamma'(\gamma-\gamma'+1)}$, which simplifies to $\gamma'=(1-\theta)\gamma$. Substituting $\gamma'=(1-\theta)\gamma$ back into \eqref{eq:gamma_main_thm_test} gives $T=\widetilde{O}\big(\gamma kn^{1/\gamma}\big)$. In addition, by the same substitution, we obtain $O\big(\gamma kn^{1/\gamma}\big)$ decoding time. In this case, the bound on $T$ is the same as the bound for the COMP algorithm (see Table \ref{tab:sparse_algo_summary}).

When $\gamma = \omega(1)$, it is straightforward to establish that the integer constraint on $\gamma'$ does not impact the above findings. However, when $\gamma = O(1)$, we need to account for the integer constraint.  One could naively search over $\gamma'\in\{3,\dots,\gamma\}$, but in Appendix \ref{sec:convex}, we use a convexity argument to show that considering $\gamma'\in\{3,\lfloor(1-\theta)\gamma\rfloor,\lceil(1-\theta)\gamma\rceil\}$ is sufficient.

To see how our algorithm compares to optimal behavior established in \cite{Oli20} (i.e., an upper bound for the DD algorithm, and a matching algorithm-independent lower bound) and the COMP algorithm \cite{Ven19} for different values of $\gamma$, we introduce the following quantity:
\begin{align}
    \eta&=\lim_{n\rightarrow\infty}\frac{\log\big(\frac{n}{k}\big)}{\gamma\log\big(\frac{T}{\gamma k}\big)}. \label{eq:eta}
\end{align}
Observe that for any fixed value of $\eta>0$, re-arranging gives $T\sim\gamma k\big(\big(\frac{n}{k}\big)^{1/\gamma}\big)^{(1+o(1))/\eta}$. With $\eta$ defined, we compare the performance in Figure \ref{fig:eta_plot_diff_gammas}. We observe that the splitting algorithm's curve quickly gets closer to the COMP algorithm's curve even for fairly low values of $\gamma$.  On the other hand, matching the DD algorithm's curve with sublinear decoding time remains an interesting open challenge for future work.

\section{Algorithm for Size-Constrained Tests}

In the case of size-constrained tests, we again modify the tree structure (see Figure \ref{fig:size_constraint_diagram}), and the main differences from the standard noiseless algorithm \cite{cher20,Eri20} are as follows:
\begin{itemize}
    \item The first level after the root is chosen to have groups of size $\rho$, since larger groups are prohibited.  In addition, at this level with nodes of size $\rho$, we test each node individually, guaranteeing that we only ``continue'' down the tree for defective nodes at that level.
    \item We use non-binary splitting, geometrically decreasing the node size at each level until the final level with size one.  We limit the number of levels to be $O(1)$, whereas binary splitting would require $O(\log \rho)$ levels, and (at least when using a similar level-by-level test design) would increase the number of tests by an $O(\log \rho)$  factor.
    \item We do not independently place nodes into tests, since doing so would cause a positive probability of violating the $\rho$-sized test constraint.  Instead, at each level, we create a random testing sub-matrix with a column weight of exactly one, and a row weight exactly equal to to $\frac{\rho}{\text{node~size}}$. A similar doubly-constant test design was also adopted in \cite{Ven19}, but without the tree structure.
    % For each level, instead of using a test sequence (where each node is randomly assigned to each test), we follow the idea of \cite{Ven19} and use a binary matrix with fixed row and column weights for the assignment of nodes to tests. The need for fixing the number of times each item is tested (i.e., fixed column weight), despite not being required by the constraint, is to improve our performance on the number of tests.
    % \item Similarly to the noisy setting in Section \ref{sec:noisy_algo_intro}, we use $N$ independent sequences of tests at each level, which helps to reduce the error probability.
\end{itemize}
We now proceed with a more detailed description.

\subsection{Description of the Algorithm} \label{sec:rho_algo_descrip}

Our algorithm works with a tree structure (see Figure \ref{fig:size_constraint_diagram}) similar to previous sections.  The $j$-th node at the $l$-th level is again denoted by $\mathcal{G}_j^{(l)}$.  A distinction here as that the tree only has a constant depth, with the final index denoted by $C = O(1)$; hence, the splits are $\rho^{1/C}$-ary.\footnote{For notational convenience, we assume that $\rho^{1/C}$ is an integer. Since we already assumed that $\rho$ is a power of two, if $\rho=O(1)$, then it will suffice to let $C$ be that power (see Lemma \ref{lem:asymp}, in which we handle the case $\rho = O(1)$ separately). Otherwise, if $\rho = \omega(1)$, then the rounding is insignificant since $C = O(1)$.}  More importantly, there are key differences in the allocation of items to tests, which we describe as follows.  

At each level $l$, we perform $N$ independent iterations to boost the error probability, as mentioned above.  Within each iteration, we make use of a random matrix, which we write as $\mathsf{X}_l=\big[x_{ti}^{(l)}\big]\in\{0,1\}^{\text{\#tests}\times\text{\#nodes}}$ (the dependence on the iteration number is left implicit), where $\text{\#tests}=n/\rho$ and $\text{\#nodes}=\frac{n}{\rho^{1-l/C}}$. We pick $\mathsf{X}_l$ by sampling uniformly from all $\frac{n}{\rho}\times\frac{n}{\rho^{1-l/C}}$ matrices with exactly $\rho^{l/C}$ nodes per test (i.e., a row weight of $\rho^{l/C}$), and each node sampled exactly once (i.e., a column weight of one).  These choices ensure that each test contains at most $\rho$ items, as required.  The column weight of one is not strictly imposed by the testing constraints, but helps in avoiding ``bad'' events where some nodes are not tested.

With this notation in place, the testing procedure is formally described in Algorithm \ref{alg:rho_nonadap_testing}, and the decoding  procedure is described in Algorithm \ref{alg:rho_nonadap_decoding}.

\begin{figure}[!t]
  \centering
  \includegraphics[scale=0.4]{images/size_constraint_diag.pdf}
  \caption{Tree structure of our algorithm. After the first level, the branching factor is $\rho^{1/C}$.} \vspace*{-2ex} \label{fig:size_constraint_diagram}
\end{figure}

\begin{algorithm}[!t]
    \begin{algorithmic}[1]
        \REQUIRE Number of items $n$, number of defective items $k$, and maximal test size $\rho$
        \STATE At level $l=0$ (see Figure \ref{fig:size_constraint_diagram}), perform an individual test for each node.
        \FOR{each $l=1,\dots,C-1$ (for some constant $C$ chosen later)}{
        \FOR{each iteration in $\{1,\dots,N\}$ (for some constant $N\geq1$)} {
        \STATE Pick a new $\mathsf{X}_l$ of size $\frac{n}{\rho}\times\frac{n}{\rho^{1-l/C}}$, with column weight 1 and row weight $\rho^{l/C}$.
        \FOR{each row $t$ in $\mathsf{X}_l$}{
        \STATE Conduct a single test containing the nodes $\mathcal{G}_j^{(l)}$ with $x_{tj}^{(l)}=1$.
        }\ENDFOR
        }\ENDFOR
        }\ENDFOR
        % \STATE For the final level $l=C$, each $\mathcal{G}_j^{(l)}=\{j\}$ is a singleton.
       % \STATE Set $l = C$ (final level)
        {\em Final level:}
        \FOR{each iteration in $\{1,\dots,C'\}$ (for some constant $C'$ chosen later)}{
        \STATE Pick a new $\mathsf{X}_C$ of size $\frac{n}{\rho}\times n$, with column weight 1 and row weight $\rho$.
        \FOR{each row $t$ in $\mathsf{X}_C$}{
        \STATE Conduct a single test containing the (singleton) nodes $\mathcal{G}_j^{(l)}$ with $x_{tj}^{(l)}=1$.
        }\ENDFOR
        }\ENDFOR
    \end{algorithmic}
    \caption{Testing procedure for $\rho$-sized tests \label{alg:rho_nonadap_testing}}
\end{algorithm}

\begin{algorithm}[!t]
    \begin{algorithmic}[1]
        \REQUIRE Outcomes of $T$ non-adaptive tests, number of items $n$, number of defective items $k$, and maximal test size $\rho$
        \STATE Initialize $\mathcal{PD}^{(0)}=\{\mathcal{G}_j^{(0)}\}_{j=1}^{n/\rho}$
        \FOR{each group $\mathcal{G}\in\mathcal{PD}^{(0)}$}{
        \STATE \textbf{if} the single test of $\mathcal{G}$ is positive \textbf{then} add all children of $\mathcal{G}$ to $\mathcal{PD}^{(1)}$
        }\ENDFOR
        \FOR{$l=1,\dots,C-1$}{
        \FOR{each group $\mathcal{G}\in\mathcal{PD}^{(l)}$}{
        \STATE \textbf{if} all $N$ tests of $\mathcal{G}$ are positive \textbf{then} add all children of $\mathcal{G}$ to $\mathcal{PD}^{(l+1)}$
        }\ENDFOR
        }\ENDFOR
        \STATE Let the estimate $\widehat{\mathcal{S}}$ be the set of elements in $\mathcal{PD}^{(C)}$ that are not included in any negative test at the final level.
        \STATE Return $\widehat{\mathcal{S}}$
    \end{algorithmic}
    \caption{Decoding procedure for $\rho$-sized tests \label{alg:rho_nonadap_decoding}}
\end{algorithm}

\subsection{Algorithmic Guarantees}

We are now ready to state our main result for the case of size-constrained tests.   In this case, we slightly strengthen the assumption $k = o(n)$ to $k = n^{1-\Omega(1)}$, and we slightly strengthen the assumption $\rho = o\big( \frac{n}{k} \big)$ (see the discussion following \eqref{eq:noise}) to $\rho=(n/k)^{1-\Omega(1)}$.  These additional restrictions only rule out scaling regimes that are very close to linear (e.g., $k = \frac{n}{\log n}$), and were similarly imposed in \cite{Ven19}.

\begin{theorem} \label{thm:rho_main_theorem}
Let $\mathcal{S}$ be a (defective) subset of $\{1,\dots,n\}$ of cardinality $k=O\big(n^{1-\epsilon_1}\big)$ for some $\epsilon_1\in(0,1]$ and the test size constraint be $\rho=O\big((n/k)^{1-\epsilon_2}\big)$ for some $\epsilon_2\in(0,1]$. For any $\zeta > 0$, there exist choices of $C,C',N=O(1)$ such that with $O\big(n/\rho\big)$ tests, the preceding algorithm satisfies the following with probability $1 - O\big(n^{-\zeta}\big)$:
\begin{itemize}
    \item The returned estimate $\widehat{\mathcal{S}}$ equals $\mathcal{S}$;
    \item The decoding time is $O\big(n/\rho\big)$.
    % \item The algorithm uses $O(n\log n)$ bits of storage.
\end{itemize}
\end{theorem}

The proof of Theorem \ref{thm:rho_main_theorem} is given in Appendix \ref{sec:rho_algo_analysis}, and follows similar ideas to that of Theorem \ref{thm:gamma_main_theorem} but with suitably modified details. As summarized in Table \ref{tab:sparse_algo_summary}, this is the first algorithm to attain $O\big(n/\rho\big)$ scaling in both the number of tests and the decoding time. 


\section{Algorithm for the Noisy Setting} \label{sec:noisy_algo_intro}

% where we first test large groups of items together, placing each group into a single randomly-chosen test among a sequence (i.e., subset) of tests. Afterwards, these groups are ``split'' into smaller sub-groups, while using the previous test outcomes to eliminate those believed to be non-defective. This process is repeated---with the elimination step ensuring that the number of groups under consideration does not grow too large---until a superset of $\mathcal{S}$ is found. Finally, $\mathcal{S}$ is deduced from the superset via a final sequence of tests. We can visualize this using a tree (see Figure \ref{fig:noisy_algo_diag}), where each level of the tree represents a stage of the splitting process, each node represents a group of items, and each split creates two edges. 

For the unconstrained noisy setting, we revert to {\em binary} splitting (see Figure \ref{fig:noisy_algo_diag}), as was used in \cite{cher20,Eri20}, though in Appendix \ref{sec:non_binary} we also outline a non-binary approach that follows one used for the heavy hitters problem \cite{Cor08,Ind11}.  The main difference between our noisy algorithm and \cite{cher20,Eri20} is that when deciding whether a given node is defective or not, we look {\em several levels further down the tree}, instead of only considering the single test outcome of the given node.  This complicates the analysis, and leads to a small increase in the decoding time.  Additionally, in order to reduce the effective noise level, each node in the tree is placed in multiple tests, rather than just one.

\subsection{Description of the Algorithm} \label{sec:noisy_algo_descrip}

\begin{figure}[t]
    \centering
    \includegraphics[scale=0.4]{images/noisy_algo_diag.pdf}
    \caption{Tree structure of our algorithm for the noisy setting.} % \vspace*{-2ex} 
    \label{fig:noisy_algo_diag}
\end{figure}

Following \cite{cher20,Eri20}, our algorithm considers a tree representation (see Figure \ref{fig:noisy_algo_diag}), in which each node corresponds to a set of items. The levels of the tree are indexed by $l=\log_2k,\dots,\log_2n$ and the $j$-th node at the $l$-th level is denoted by $\mathcal{G}_j^{(l)}\subseteq\{1,\dots,n\}$.  At the top level we have $|\mathcal{G}_j^{(\log_2 k)}| = \frac{n}{k}$, and the sizes are subsequently halved until the final level with $|\mathcal{G}_j^{(\log_2 n)}| =1$.

The algorithm works down the tree one level at a time, keeping a list of \textit{possibly defective} ($\mathcal{PD}$) nodes, and performing tests to obtain such a list at the next level. When we perform tests at a given level, we treat each node as a ``super-item''; including a node in a test amounts to including all of the items in the corresponding node $\mathcal{G}_j^{(l)}$. In addition, for the tree illustrated in Figure \ref{fig:noisy_algo_diag}, we refer to nodes containing at least one defective item as defective nodes, to all other nodes as non-defective nodes, and to the sub-tree of defective nodes as the defective tree.

The testing is performed as follows: At each level of the tree, $N$ sequences of tests are formed, each having length $T_{\text{len}}$ (i.e., a total of $NT_{\rm len}$ tests per level).  For each node and each of the $N$ sequences, the node is placed into a single test, chosen uniformly at random among the $T_{\text{len}}$ tests.

We define the \textit{intermediate label} and \textit{final label} of a given node as follows:
\begin{itemize}
    \item The intermediate is formed via majority voting of the $N$ tests the node is included in.
    \item To obtain the final label of a given node, we look at the intermediate labels of all nodes up to $r$ levels below the given node. If there exists any length-$r$ path below the given node with more than $r/2$ positive intermediate labels, then we assign the node's final label to be positive. Otherwise, we assign it to be negative. 
\end{itemize}
According to the tree structure in Figure \ref{fig:noisy_algo_diag}, once we reach the later levels, there may be fewer than $r$ levels remaining.  To account for such cases, we simply ensure that sufficiently many tests are performed at the final level so that a length-$r$ ``path'' can be formed (here, no further branching is done, and each ``node'' is the same singleton).

With the above notation and terminology in place, the overall test design is described in Algorithm \ref{alg:noisy_nonadap_testing}, and the decoding procedure in Algorithm \ref{alg:noisy_nonadap_decoding}.

%\begin{figure}[t]
%  \centering
%  \includegraphics[scale=0.35]{images/r_paths.pdf}
%  \caption{For $l\leq\log_2n-r$ (i.e., there are at least $r$ levels below the node), the diagram on the left shows an example of a length-$r$ path. Otherwise, we have $l>\log_2n-r$ (i.e., there are less than $r$ levels below the node), and the diagram on the right shows an example of a length-$r$ path.} \label{fig:r_paths}
%\end{figure}

\begin{algorithm}[!t]
    \begin{algorithmic}[1]
        \REQUIRE Number of items $n$, number of defective items $k$, and parameters $N$, $C$, and $C'$
        \STATE Initialize $T_{\text{len}}=Ck$
        \FOR{each $l=\log_2k,\dots,\log_2n-1$}{
            \FOR{each iteration in $\{1,\dots,N\}$}{
                \STATE Form a sequence of tests of length $T_{\text{len}}$
                \FOR{$j=1,2,\dots,2^l$}{
                    \STATE Place all items from $\mathcal{G}_j^{(l)}$ into a single test in the sequence just formed, chosen uniformly at random.
                }\ENDFOR
            }\ENDFOR
        }\ENDFOR
        \STATE At level $l=\log_2n$, form $C'N\log_2n$ sequences of tests, each of length $T_{\text{len}}$.
        \FOR{each singleton at the final level}{
            \FOR{each of the $C'N\log_2n$ test sequences}{
                \STATE Place the singleton in one of the $T_{\text{len}}$ tests in the sequence, chosen uniformly at random.
            }\ENDFOR
        }\ENDFOR
    \end{algorithmic}
    \caption{Testing procedure for the noisy setting \label{alg:noisy_nonadap_testing}}
\end{algorithm}

\begin{algorithm}[!t]
    \begin{algorithmic}[1]
        \REQUIRE Outcomes of $T$ non-adaptive tests, number of items $n$, number of defective items $k$, and parameters $N$, $C$, $C'$, and $r$
        \STATE Initialize $\mathcal{PD}^{(l_{\text{min}})}=\big\{\mathcal{G}_j^{(l_{\text{min}})}\big\}_{j=1}^{k}$, where $l_{\text{min}}=\log_2k$.
        \FOR{$l=\log_2k,\dots,\log_2n-1$}{
            \IF{$l\leq\log_2n-r$ (i.e., there are at least $r$ levels below the node)}
            \FOR{each group $\mathcal{G}\in\mathcal{PD}^{(l)}$}{
                \STATE Evaluate the intermediate labels of all nodes $r$ levels below $\mathcal{G}$.
            }\ENDFOR
            \ELSIF{$l>\log_2n-r$ (i.e., there are fewer than $r$ levels below the node)}
            \FOR{each group $\mathcal{G}\in\mathcal{PD}^{(l)}$}{
                \STATE Evaluate the intermediate labels of all nodes all levels below $\mathcal{G}$ except the final level.
                \FOR{each node reached at the final level}{
                    \STATE Iterate through the $C'N\log_2n$ test outcomes in batches of size $N$: Conduct a majority vote for each batch to obtain an intermediate label for the node. \label{op:sub_alg_final_lvl}
                }\ENDFOR
                \STATE Use intermediate labels from each node in the final level to make up paths of length $r$ (see Section \ref{sec:noisy_algo_descrip}).
            }\ENDFOR
            \ENDIF
            \FOR{each group $\mathcal{G}\in\mathcal{PD}^{(l)}$}{
                \STATE If $\exists$ a path with more than $r/2$ positive intermediate labels, then assign $\mathcal{G}$'s final label to be positive. Otherwise, assign $\mathcal{G}$'s final label to be negative.
                \STATE If the final label of $\mathcal{G}$ is positive, then add both children of $\mathcal{G}$ to $\mathcal{PD}^{(l+1)}$.
            }\ENDFOR
        }\ENDFOR
        \STATE At the final level, for each node (singleton), repeat step \ref{op:sub_alg_final_lvl} to obtain $C'\log_2n$ intermediate labels for the node, and conduct a majority vote for the node's intermediate labels to obtain its final label. 
        \STATE Return $\widehat{\mathcal{S}}$ containing the elements of singletons in $\mathcal{PD}^{(\log_2n)}$ with a positive final label.
    \end{algorithmic}
    \caption{Decoding procedure for the noisy setting \label{alg:noisy_nonadap_decoding}}
\end{algorithm}

% \newpage
\subsection{Algorithmic Guarantees} \label{sec:noisy_algo_guaran}

\begin{theorem} \label{thm:noisy_main_theorem}
    Let $\mathcal{S}$ be a (defective) subset of $\{1,\dots,n\}$ of cardinality $k=o(n)$. For any constants $\epsilon > 0$ and $t > 0$ satisfying $\epsilon t>1$, there exist choices of $C,C',N = O(1)$ and $r = O(\log k + \log\log n)$ such that with $O\big(k\log n\big)$ tests, the preceding algorithm satisfies the following with probability at least $1-O\big(\big(k\log\frac{n}{k}\big)^{1-\epsilon t}\big)$:
    \begin{itemize}
        \item The returned estimate $\widehat{\mathcal{S}}$ equals $\mathcal{S}$;
        \item The decoding time is $O\big(\big(k\log\frac{n}{k}\big)^{1+\epsilon}\big)$.
        % \item The algorithm uses $O(n\log k\cdot\log n)$ bits of storage.
    \end{itemize}
\end{theorem}
The proof of Theorem \ref{thm:noisy_main_theorem} is given in Appendix \ref{sec:noisy_algo_analysis}.  The main distinction compared to the noiseless proofs is that we need to bound the probabilities of intermediate labels (used in Lines 5 and 8 of Algorithm \ref{alg:noisy_nonadap_decoding}) and final labels (computed in Line 13) being wrong, to ensure that correct decisions are made at each level.  The $N$ independent repetitions at each level play the role of reducing the former, and the independence of tests across levels helps to tightly characterize the latter.


\section{Conclusion} \label{sec:conclusion}

We have provided fast splitting algorithms for sparsity-constrained and noisy group testing, maintaining the near-optimal number of tests provided by earlier works while also attaining a matching or near-matching decoding time.  Possible directions for future research include (i) in the finitely divisible setting, match the number of tests used by the DD algorithm (see Table \ref{tab:sparse_algo_summary}) with sublinear decoding time, and (ii) in the noisy setting, further reduce the $\big( k \log \frac{n}{k} \big)^{1+\epsilon}$ runtime, ideally bringing it all the way down to $O(k \log n)$.  

%Two potentially interesting directions for further work include:
%\begin{itemize}
%    \item Develop a sublinear-time algorithm for the finitely divisible setting that attains the stronger bound of the DD algorithm \cite{Oli20}, rather than only that of the COMP algorithm (see Table \ref{tab:sparse_algo_summary} and Figure \ref{fig:eta_plot_diff_gammas}).  
%    \item Handle the case that tests are simultaneously noisy and size-constrained. This appears to be non-trivial, as our variations for the noisy and size-constrained settings currently do not ``fit together'' nicely; the former looks $\omega(1)$ levels down the tree, while the latter uses a tree of depth $O(1)$. 
%\end{itemize}


