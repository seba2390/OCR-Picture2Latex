\subsection{Generative Adversarial Networks (GANs)}
The Generative Adversarial Nets \cite{goodfellow2014generative} have achieved great success in various applications, such as natural image synthesis~\cite{radford2015unsupervised,zhang2017stackgan,brock2018large}, image style translation~\cite{isola2017image,zhu2017unpaired}, image super resolution~\cite{ledig2017photo} in computer vision, and medical image segmentation~\cite{yang2017automatic,xue2018segan}, cross-modality image synthesis~\cite{nie2017medical}, image reconstruction~\cite{yang2017dagan} in medical image analysis. 
The GAN estimates generative distribution via an adversarial supervisor. Specifically, the generator $G$ attempts to imitate the data from target distribution to make the `fake' data indistinguishable to the adversarial supervisor $D$. In AsynDGAN framework, we mainly focus on the conditional distribution estimation due to the nature of health entities learning problems. However, the AsynDGAN framework can be easily adopted into general GAN learning tasks. 

\subsection{Learning with data privacy}
\textbf{Federated Learning}: The federated learning (FL) seeks to collaborate local nodes in the network to learn a globally powerful model without storing data in the cloud. Recently, FL attracts more attention as data privacy becomes a concern for users~\cite{hard2018federated,konevcny2016federated,brisimi2018federated,huang2018loadaboost}. Instead of directly exposing users' data, FL only communicates model information (parameters, gradients) with privacy mechanism so protects users' personal information. In~\cite{agarwal2018cpsgd, jayaraman2018distributed,mcmahan2016communication}, the SGD is shared in a privacy protection fashion. However, communicating gradients is dimension dependent.
Considering a ResNet101~\cite{he2016resnet} with $d=40$ million parameters, it requires at least $170$ mb to pass gradients for each client per-iteration.
Even with compression technique similar to~\cite{agarwal2018cpsgd}, the communication cost is still  non-affordable for large-size networks.

\textbf{Split Learning}: 
%The Split Learning (SL)~\cite{vepakomma2018split} separates shallow and deep layers in deep learning models and only inter-layer information is transmitted from local to center. Since only the gradients of data-connected layer in the neural network contains client information, the architecture is separated thus the center only maintains the layers that are several blocks away from the input.
%The privacy is guaranteed via a data block mechanism, i.e. the center has no direct access to the data.   This approach reduces the communication cost from model dependent to cut-layer dependent while still protects privacy of data. However, such approach does not apply to skip-connect architecture e.g. Resnet \cite{he2016resnet} type neural networks. 
The split learning (SL)~\cite{vepakomma2018split} separates shallow and deep layers in deep learning models. The central processor only maintains layers that are several blocks away from the local input, and only inter-layer information is transmitted from local to central. In this way, the privacy is guaranteed because the central processor has no direct access to data. It reduces the communication cost from model-dependent level to cut-layer-dependent layer while protecting data privacy. However, such method does not apply to neural networks with skip connections, e.g., ResNets~\cite{he2016resnet}. 

In AsynDGAN framework, the communication cost in each iteration is free of the dimension $d$. Only auxiliary data (label and masks), `fake' data and discriminator loss are passed between the central processor and local nodes in the network. For a $128\times128$ size gray-scale image, communication cost per-iteration for each node is $8$ mb with batch size $128$. Since the central processor has only access to discriminator and auxiliary data, the privacy of client is secured via separating block mechanism. 


In addition, adaptivity is an exclusive advantage of AsynDGAN framework. With rapid  evolution of machine learning methods, practitioners need to keep updated with state-of-the-art methods. However, there will be a high transaction cost to train a new model in a classical distributed learning subroutine. With the AsynDGAN system, one can maintain the generative distribution. Therefore, updating machine learning models can be done locally with the freedom of generating training data. The comparison between FL, SL and our AsynDGAN is shown in Table~\ref{tabcomp}. 


\begin{table}[t] 
	\centering
	\scalebox{0.68}{
	\begin{tabular}{lccc}
		\toprule
		& Privacy Mechanism & Data transmission                                    & Adaptivity \\ \midrule
		FL & Randon Noise           & Parameters / Gradients                                  & No         \\ 
		SL    & Data Block             & Cut Layer Gradients                                   & No         \\ 
		AsynDGAN            & Data Block       & \makecell{Fake Data, Auxiliary Variable\\ \& Discriminator Loss} & Yes        \\ \bottomrule
	\end{tabular}
	}
\vspace{0.02in}
\caption{Comparison between different learning strategies.}
\label{tabcomp}
\end{table}

%\subsection{Differential Privacy}

