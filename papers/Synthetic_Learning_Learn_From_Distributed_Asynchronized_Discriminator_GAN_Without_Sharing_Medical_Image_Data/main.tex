\documentclass[10pt,twocolumn,letterpaper]{article}
\usepackage{makecell}
%\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{graphics}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs, multirow}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{bm}
\usepackage{enumerate}
\usepackage{standalone}
\newtheorem{thm}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{cor}{Corollary}
\newtheorem{prop}{Proposition}
\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem{remark}{Remark}
\newtheorem{example}{Example}
\newtheorem{assume}{Assumption}
\newtheorem{obs}{Observation}

% Include other packages here, before hyperref.


\newcommand*{\affaddr}[1]{#1} % No op here. Customize it for different styles.
\newcommand*{\affmark}[1][*]{\textsuperscript{#1}}
\newcommand*{\email}[1]{\texttt{#1}}

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}

%\cvprfinalcopy % *** Uncomment this line for the final submission

%\def\cvprPaperID{8323} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
%\ifcvprfinal\pagestyle{empty}\fi
%\setcounter{page}{4321}
\begin{document}

%%%%%%%%% TITLE
\title{Synthetic Learning: Learn From Distributed Asynchronized Discriminator GAN Without Sharing Medical Image Data}

\author{%
Qi Chang\affmark[1]\thanks{equal contribution}, Hui Qu\affmark[1]\footnotemark[1], Yikai Zhang\affmark[1]\footnotemark[1], Mert Sabuncu\affmark[2], \\Chao Chen\affmark[3], Tong Zhang\affmark[4] and Dimitris Metaxas\affmark[2]\\
\affaddr{\affmark[1]Rutgers University}
\affaddr{\affmark[2]Cornell University}\\
\affaddr{\affmark[3]Stony Brook University}
\affaddr{\affmark[4]Hong Kong University of Science and Technology}\\
\email{\{qc58,hq43,yz422,dnm\}@cs.rutgers.edu} , \email{msabuncu@cornell.edu},\\ \email{chao.chen.cchen@gmail.com}, \email{tongzhang@tongzhang-ml.org}%
}
%\author{%
%Qi Chang\affmark^1, Hui Qu\affmark^1, Yikai Zhang\affmark^1, Mert Sabuncu\affmark^2,Chao Chen\affmark^3,Tong Zhang\affmark^4 and Dimitris Metaxas\affmark^1\\
%\affaddr{\affmark^1 Rutgers University, New Brunswick NJ}\\
%\affaddr{\affmark[2]Department of Mechanical Engineering}\\
%\email{\{A,B,C,D,E\}@university.edu}\\
%\affaddr{\LaTeX\ University}%
%}

%\author{First Author\\
%Institution1\\
%Institution1 address\\
%{\tt\small firstauthor@i1.org}
%% For a paper whose authors are all at the same institution,
%% omit the following lines up until the closing ``}''.
%% Additional authors and addresses can be added with ``\and'',
%% just like the second author.
%% To save space, use either the email address or home page, not both
%\and
%Second Author\\
%Institution2\\
%First line of institution2 address\\
%{\tt\small secondauthor@i2.org}
%}

\maketitle
%\thispagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}
In this paper, we propose a data privacy-preserving and communication efficient distributed GAN learning framework named Distributed Asynchronized Discriminator GAN (AsynDGAN). 
Our proposed framework aims to train a central generator learns from distributed discriminator, and use the generated synthetic image solely to train the segmentation model.
We validate the proposed framework on the application of \emph{health entities learning problem} which is known to be privacy sensitive. 
Our experiments show that our approach: 1) could learn the real image's distribution from multiple datasets without sharing the patient's raw data. 2) is more efficient and requires lower bandwidth than other distributed deep learning methods. 3) achieves higher performance compared to the model trained by one real dataset, and almost the same performance compared to the model trained by all real datasets. 4) has provable guarantees  that the generator could learn the distributed distribution in an \emph{all important} fashion thus is unbiased.We release our AsynDGAN source code at: https://github.com/tommy-qichang/AsynDGAN
	
	
%Can we train a deep learning model from different health entities to achieve higher performance without compromising the patient data privacy?
%We propose a Distributed Asynchronized Discriminator GAN(AsynDGAN) which learns from the real images in different health entities' dataset, and generates the synthetic images to train a deep learning model. 
%The synthetic image from Dad-GAN share the similar distribution with the real image, and let the task specific model achieve higher performance than just learn the real images from only one dataset and almost the same performance to the model learns the real images from all datasets together but without compromising the patient data privacy. 
%Our experiments show that our approach: 1) could learn the real image's distribution without sharing patient's raw data. 2) provide an architecture which could study through multiple private dataset 3) could provide future scalability for the task specific model. 4) more efficient and requires lower bandwidth than other distributed deep learning method.
%We will release our AsynDGAN source code in Github. 
% We validate the proposed algorithm on two different segmentation tasks including split 10-fold BraTS HGG(same distribution )segmentation and BraTs HGG and LGG segmentation(different distribution). The result demonstrate the synthesis image could be used for other machine learning tasks and achieve reasonable performance without loss of privacy. 
% 
% the effectiveness of the distributed architecture for improving without loss of privacy
%
%
%We demonstrate the synthetic images have the similar distribution with the real image, and could achieve higher performance than any of the dataset, and 
%
%Can we create a large medical dataset from different health entities without compromising the data privacy? Instead of directly train a task specified model like what federated learning or split learning did, we trained a Distributed Asynchronized Discriminator GAN(Dad-GAN) from the real image and then store the generator as a medical dataset for the future use. We proved that Dad-GAN could (provide a better dataset) than just learn the real images from one entities. Specifically, we introduce the architecture of Dad-GAN and also the perception loss as one of the generator loss function. Our result show that Dad-GAN could generate realistic image and by using such synthetic image, the segmentation result could achieve higher performance than the real image in any of the subset.
%
% The AsynDGAN could be treated as a dataset and any model 
%
%
%We proposed a novel method to build a Distributed Asynchronized Discriminator GAN(Dad-GAN) as a medical repository which learned medical images from different sites. Our experiments shows the segmentation tasks learnt from our Dad-GAN could be better than 
%
%One of the major challenge of the medical intelligent is the discrepancy between maintaining the confidentiality of the subject's privacy data and the acquirement of the  outstandingly large quantity of data that the deep neural network are learning from. New methods like federated learning or split learning focus on distributed architecture but ignore the  

%\footnote{*these authors contribute equally}
\end{abstract}

%%%%%%%%% BODY TEXT

\vspace{-1em}
\section{Introduction}
\label{sec:intro}
\input{sec_intro}

\section{Related Work}
\label{sec:related-work}
\input{sec_related_work}

\section{Method}
\label{sec:method}
\input{sec_method}

\section{Experiments}
\label{sec:exp}
\input{sec_exp}

\section{Conclusion}
\label{sec:conclude}
\input{sec_conclude}


\section*{Acknowledgements}
We thank anonymous reviewers for helpful comments. This work was partially supported by ARO-MURI-68985NSMUR, NSF-1909038, NSF-1855759, NSF-1855760, NSF-1733843, NSF-1763523, NSF-1747778 and NSF-1703883.

{\small
\bibliographystyle{ieee_fullname}
\bibliography{main}
}

\input{sec_appendix}


\end{document}
