In this section, we first perform experiments on a synthetic dataset to illustrate how AsynDGAN learns a mixed Gaussian distribution from different subsets, and then apply AsynDGAN to the brain tumor segmentation task on BraTS2018 dataset~\cite{bakas2018identifying} and nuclei segmentation task on Multi-Organ dataset~\cite{kumar2017dataset}.


\subsection{Datasets and evaluation metrics}
\subsubsection{Datasets}
\paragraph{Synthetic dataset}
The  synthetic dataset is generated by mixing $3$ one-dimensional Gaussian. In another word, we generate $x\in \{1,2,3\}$ with equal probabilities. Given $x$, the random variable $y$ is generated from  $y =y_1{\textbf{1}_{x=1}}+y_2{\textbf{1}_{x=2}}+y_3{\textbf{1}_{x=3}} $ where $\textbf{1}_{event}$ is the indicator function and $y_1\sim \mathcal{N}(-3,2),y_2\sim \mathcal{N}(1,1), y_3\sim\mathcal{N}(3,0.5)$. Suppose the generator learns the conditional distribution of $y$: $p(y|x)$ perfectly, the histogram should behave similarly to the shape of the histogram of mixture gaussian. 


\paragraph{BraTS2018}

This dataset comes from the Multimodal Brain Tumor Segmentation Challenge 2018~\cite{bakas2017advancing,bakas2018identifying,menze2014multimodal} and contains multi-parametric magnetic resonance imaging (mpMRI) scans of low-grade glioma (LGG) and high-grade glioma (HGG) patients. There are 210 HGG and 75 LGG cases in the training data, and each case has four types of MRI scans and three types of tumor subregion labels. In our experiments, we perform 2D segmentation on T2 images of the HGG cases to extract the whole tumor regions. The 2D slices with tumor areas smaller than 10 pixels are excluded for both GAN training and segmentation phases. In the GAN synthesis phase, all three labels are utilized to generate fake images. For segmentation, we focus on the whole tumor (regions with any of the three labels).

\paragraph{Multi-Organ}
This dataset is proposed by Kumar et al.~\cite{kumar2017dataset} for nuclei segmentation. There are 30 histopathology images of size $1000\times1000$ from 7 different organs. The train set contains 16 images of breast, liver, kidney and prostate (4 images per organ). The same organ test set contains 8 images of the above four organs (2 images per organ) while the different organ test set has 6 images from bladder, colon and stomach. In our experiments, we focus on the four organs that exist both in the train and test sets, and perform color normalization~\cite{reinhard2001color} for all images. Two training images of each organ is treated as a subset that belongs to a medical entity.

\subsubsection{Evaluation metrics}
We adopt the same metrics in the BraTS2018 Challenge~\cite{bakas2018identifying} to evaluate the segmentation performance of brain tumor: Dice score (Dice), sensitivity (Sens), specificity (Spec), and 95\% quantile of Hausdorff distance (HD95). The Dice score, sensitivity (true positive rate) and specificity (true negative rate) measure the overlap between ground-truth mask $G$ and segmented result $S$. They are defined as
\begin{equation}
Dice(G, S) = \frac{2|G \cap S|}{|G| + |S|}
\end{equation}
\begin{equation}
Sens(G, S)=\frac{|G \cap S|}{|G|}
\end{equation}
\begin{equation}
Spec(G, S)=\frac{|(1-G) \cap (1-S)|}{|1-G|}
\end{equation}
The Hausdorff distance evaluates the distance between boundaries of ground-truth and segmented masks:
\begin{equation}
HD(G, S) = \max\{\sup_{x\in\partial G}\inf_{y\in\partial S}d(x, y), \sup_{y\in\partial S}\inf_{x\in\partial G}d(x, y)\}
\end{equation}
where $\partial$ means the boundary operation, and $d$ is Euclidean distance. Because the Hausdorff distance is sensitive to small outlying subregions, we use the 95\% quantile of the distances instead of the maximum as in~\cite{bakas2018identifying}. To simplify the problem while fairly compare each experiment, we choose 2D rather than 3D segmentation task for the BraTS2018 Challenge and compute these metrics on each 2D slices and take an average on all 2D slices in the test set.

For nuclei segmentation, we utilize the Dice score and the Aggregated Jaccard Index (AJI)~\cite{kumar2017dataset}:
\begin{equation}
AJI = \frac{\sum_{i=1}^{n_\mathcal{G}} |G_i \cap S(G_i)|}{\sum_{i=1}^{n_\mathcal{G}}|G_i \cup S(G_i)| + \sum_{k\in K}|S_k|}
\end{equation}
where $S(G_i)$ is the segmented object that has maximum overlap with $G_i$ with regard to Jaccard index, $K$ is the set containing segmentation objects that have not been assigned to any ground-truth object.

\subsection{Implementation details}
In the synthetic learning phase, we use 9-blocks ResNet~\cite{he2016deep} architecture for the generator, and multiple discriminators which have the same structure as that in PatchGAN~\cite{isola2016pix2pix} with patch size $70\times70$. We resize the input image as $286\times286$ and then randomly crop the image to $256\times256$. In addition to the GAN loss and the L1 loss, we also used perceptual loss as described in \cite{Johnson2016Perceptual}. We use minibatch SGD and apply the Adam solver \cite{kingma2014adam}, with a learning rate of 0.0002, and momentum parameters $\beta_1 = 0.5$, $\beta_2 = 0.999$. The batch size we used in AsynDGAN depends on the number of discriminators. We use batch size 3 and 1 for BraTS2018 dataset and Multi-Organ dataset, respectively.

In the segmentation phase, we randomly crop images of 224$\times$224 with a batch size of 16 as input. The model is trained with Adam optimizer using a learning rate of 0.001 for 50 epochs in brain tumor segmentation and 100 epochs in nuclei segmentation. To improve performance, we use data augmentation in all experiments, including random horizontal flip and rotation in tumor segmentation and additional random scale and affine transformation in nuclei segmentation.


\subsection{Experiment on synthetic dataset}
In this subsection, we show that the proposed synthetic learning framework can learn a mixture of Gaussian distribution from different subsets. We compare the quality of learning distribution in $3$ settings: (1) \textbf{Syn-All.} Training a regular GAN using all samples in the dataset.
(2) \textbf{Syn-Subset-n.} Training a regular GAN using only samples in local subset $n$, where $n\in\{1,2,3\}$. (3) \textbf{AsynDGAN.} Training our AsynDGAN using samples in all subsets in a distributed fashion.

%1) The model only has access to local-distribution which represents the locally trained model. 2) The model is trained by an aggregated dataset which represents the globally trained model. 3) The model is trained by the proposed AsynDGAN framework. 



\begin{figure}
\centering
\begin{minipage}{0.325\linewidth}
	\centering\includegraphics[width=\linewidth]{imgs-gaussian/syn-all} \\ (a) Syn-All
\end{minipage}
\begin{minipage}{0.325\linewidth}
\centering\includegraphics[width=\linewidth]{imgs-gaussian/syn-subset} \\ (b) Syn-Subset-n
\end{minipage}
\begin{minipage}{0.325\linewidth}
\centering\includegraphics[width=\linewidth]{imgs-gaussian/syn-AsynDGAN} \\ (c) AsynDGAN
\end{minipage}
\caption{Generated distributions of different methods.}
%\vspace{-0.1in}
\label{fig:gaussian}
\vspace{-1em}
\end{figure}

The learned distributions are shown in Figure~\ref{fig:gaussian}. In particular, any local learning (indicated in Figure~\ref{fig:gaussian}(b)) can only fit one mode Gaussian due to the restriction of local information while AsynDGAN is able to capture global information thus has a comparable performance with the regular GAN using the union of separated datasets (\textbf{Syn-All}).

\subsection{Brain tumor segmentation}\label{sec:exp:hgg}
In this subsection, we show that our AsynDGAN can work well when there are patients' data of the same disease in different medical entities.
\subsubsection{Settings}
There are 210 HGG cases in the training data. Because we have no access to the test data of the BraTS2018 Challenge, we split the 210 cases into train (170 cases) and test (40 cases) sets. The train set is then sorted according to the tumor size and divided into 10 subsets equally, which are treated as data in 10 distributed medical entities. There are 11,057 images in the train set and 2,616 images in the test set. We conduct the following segmentation experiments:
(1) \textbf{Real-All.} Training using real images from the whole train set (170 cases).
(2) \textbf{Real-Subset-n.} Training using real images from the $n$-th subset (medical entity), where $n=1,2,\cdots,10$. There are 10 different experiments in this category.
(3) \textbf{Syn-All.} Training using synthetic images generated from a regular GAN. The GAN is trained directly using all real images from the 170 cases.
(4) \textbf{AsynDGAN.} Training using synthetic images from our proposed AsynDGAN.
The AsynDGAN is trained using images from the 10 subsets (medical entities) in a distributed fashion.

In all experiments, the test set remains the same for fair comparison. It should be noted that in the \textbf{Syn-All} and \textbf{AsynDGAN} experiments, the number of synthetic images are the same as that of real images in \textbf{Real-All}. The regular GAN has the same generator and discriminator structures as AsynDGAN, as well as the hyper-parameters. The only difference is that AsynDGAN has 10 different discriminators, and each of them is located in a medical entity and only has access to the real images in one subset.

\begin{figure*}[t]
	\vspace{-2em}
	\begin{center}
		\includegraphics[width=0.15\linewidth]{imgs-seg-hgg/img-1}
		\includegraphics[width=0.15\linewidth]{imgs-seg-hgg/label-1}
		\includegraphics[width=0.15\linewidth]{imgs-seg-hgg/real-1}
		\includegraphics[width=0.15\linewidth]{imgs-seg-hgg/fake-1}
		\includegraphics[width=0.15\linewidth]{imgs-seg-hgg/subset6-1}
		\includegraphics[width=0.15\linewidth]{imgs-seg-hgg/dadgan-1} \\ \vspace{0.01in}
		\begin{minipage}{0.15\linewidth}
			\centering\includegraphics[width=\linewidth]{imgs-seg-hgg/img-2} \\ (a) Image
		\end{minipage}
		\begin{minipage}{0.15\linewidth}
			\centering\includegraphics[width=\linewidth]{imgs-seg-hgg/label-2} \\ (b) Label
		\end{minipage}
		\begin{minipage}{0.15\linewidth}
			\centering\includegraphics[width=\linewidth]{imgs-seg-hgg/real-2}  \\  (c) Real-All
		\end{minipage}
		\begin{minipage}{0.15\linewidth}
			\centering\includegraphics[width=\linewidth]{imgs-seg-hgg/fake-2} \\ (d) Syn-All
		\end{minipage}
		\begin{minipage}{0.15\linewidth}
			\centering\includegraphics[width=\linewidth]{imgs-seg-hgg/subset6-2} \\ (e) Real-Subset-6
		\end{minipage}
		\begin{minipage}{0.15\linewidth}
			\centering\includegraphics[width=\linewidth]{imgs-seg-hgg/dadgan-2} \\ (f) AsynDGAN
		\end{minipage}
	\end{center}
	\caption{Typical brain tumor segmentation results. (a) Test images. (b) Ground-truth labels of tumor region. (c)-(f) are results of models trained on all real images, synthetic images of regular GAN, real images from subset-6, synthetic images of AsynDGAN, respectively.}
	\label{fig:seg:hgg}
\end{figure*}

\begin{figure}[t]
	\begin{center}
		\includegraphics[width=0.3\linewidth]{imgs-vis-gan/120_skull_mask}
		\includegraphics[width=0.3\linewidth]{imgs-vis-gan/120_syn}
		\includegraphics[width=0.3\linewidth]{imgs-vis-gan/120_real}\\ \vspace{0.01in}
		\begin{minipage}{0.3\linewidth}
			\centering\includegraphics[width=\linewidth]{imgs-vis-gan/1153_skull_mask} \\ (a) Input
		\end{minipage}
		\begin{minipage}{0.3\linewidth}
			\centering\includegraphics[width=\linewidth]{imgs-vis-gan/1153_syn} \\ (b) AsynDGAN
		\end{minipage}
		\begin{minipage}{0.3\linewidth}
			\centering\includegraphics[width=\linewidth]{imgs-vis-gan/1153_real}  \\  (c) Real
		\end{minipage}
	\end{center}
	\caption{The examples of synthetic brain tumor images from the AsynDGAN. (a) The input of the AsynDGAN network. (b) Synthetic images of AsynDGAN based on the input. (c) Real images.}
	\label{fig:syn:hgg}
	\vspace{-1em}
\end{figure}

\begin{table}[t]
	\begin{center}
		\begin{tabular}{lcccc}
			\toprule
			Method & Dice $\uparrow$ & Sens $\uparrow$  & Spec $\uparrow$  & HD95 $\downarrow$ \\
			\midrule
			Real-All & 0.7485 & 0.7983	& 0.9955 & 12.85 \\ \midrule
			Real-Subset-1 & 0.5647 &	0.5766 &	0.9945 &	26.90 \\
			Real-Subset-2 & 0.6158 &	0.6333 &	0.9941 &	21.87 \\
			Real-Subset-3 & 0.6660 &	0.7008 &	0.9950 &	21.90 \\
			Real-Subset-4 & 0.6539 &	0.6600 &	0.9962 &	21.07 \\
			Real-Subset-5 & 0.6352 &	0.6437 &	0.9956 &	19.27 \\
			Real-Subset-6 & 0.6844 &	0.7249 &	0.9935 &	21.10 \\
			Real-Subset-7 & 0.6463 &	0.6252 &	0.9972 &	15.60 \\
			Real-Subset-8 & 0.6661 &	0.6876 &	0.9957 &	18.16 \\
			Real-Subset-9 & 0.6844 &	0.7088 &	0.9953 &	18.56 \\
			Real-Subset-10 & 0.6507 &	0.6596 &	0.9957 &	17.33 \\ \midrule
			Syn-All & 0.7114 &	0.7099 &	0.9969 &	16.22 \\ \midrule
			\textbf{AsynDGAN}  & 0.7043 &	0.7295 &	0.9957 &	14.94 \\
			\bottomrule
		\end{tabular}
	\end{center}
	\caption{Brain tumor segmentation results.}	
%	\vspace{-0.1in}
	\label{tab:hgg}
	\vspace{-1em}
\end{table}

\subsubsection{Results}
The quantitative brain tumor segmentation results are shown in Table~\ref{tab:hgg}. The model trained using all real images (\textbf{Real-All}) is the ideal case that we can access all data. It is our baseline and achieves the best performance. Compared with the ideal baseline, the performance of models trained using data in each medical entity (\textbf{Real-Subset-1$\sim$10}) degrades a lot, because the information in each subset is limited and the number of training images is much smaller.

Our AsynDGAN can learn from the information of all data during training, although the generator doesn't ``see'' the real images. And we can generate as many synthetic images as we want to train the segmentation model. Therefore, the model (\textbf{AsynDGAN}) outperforms all models using single subset. For reference, we also report the results using synthetic images from regular GAN (\textbf{Syn-All}), which is trained directly using all real images. The AsynDGAN has the same performance as the regular GAN, but has no privacy issue because it doesn't collect real image data from medical entities. The examples of synthetic images from AysnDGAN are shown in Figure~\ref{fig:syn:hgg}. Several qualitative segmentation results of each method are shown in Figure~\ref{fig:seg:hgg}.


%\begin{figure*}[t]
%	\begin{center}
%	\includegraphics[width=0.16\linewidth]{imgs-vis-gan/7_skull_mask}
%	\includegraphics[width=0.16\linewidth]{imgs-vis-gan/7_syn}
%	\includegraphics[width=0.16\linewidth]{imgs-vis-gan/7_real}
%	\includegraphics[width=0.16\linewidth]{imgs-vis-gan/80_skull_mask}
%	\includegraphics[width=0.16\linewidth]{imgs-vis-gan/80_syn}
%	\includegraphics[width=0.16\linewidth]{imgs-vis-gan/80_real} \\ \vspace{0.01in}
%	\includegraphics[width=0.16\linewidth]{imgs-vis-gan/120_skull_mask}
%	\includegraphics[width=0.16\linewidth]{imgs-vis-gan/120_syn}
%	\includegraphics[width=0.16\linewidth]{imgs-vis-gan/120_real} 
%	\includegraphics[width=0.16\linewidth]{imgs-vis-gan/1093_skull_mask}
%	\includegraphics[width=0.16\linewidth]{imgs-vis-gan/1093_syn}
%	\includegraphics[width=0.16\linewidth]{imgs-vis-gan/1093_real} \\ \vspace{0.01in}
%	\begin{minipage}{0.16\linewidth}
%		\centering\includegraphics[width=\linewidth]{imgs-vis-gan/1153_skull_mask} \\ (a) Input
%	\end{minipage}
%	\begin{minipage}{0.16\linewidth}
%		\centering\includegraphics[width=\linewidth]{imgs-vis-gan/1153_syn} \\ (b) Synthetic image
%	\end{minipage}
%	\begin{minipage}{0.16\linewidth}
%		\centering\includegraphics[width=\linewidth]{imgs-vis-gan/1153_real}  \\  (c) Real image
%	\end{minipage}
%	\begin{minipage}{0.16\linewidth}
%		\centering\includegraphics[width=\linewidth]{imgs-vis-gan/1353_skull_mask} \\ (a) Input
%	\end{minipage}
%	\begin{minipage}{0.16\linewidth}
%		\centering\includegraphics[width=\linewidth]{imgs-vis-gan/1353_syn} \\ (b) Synthetic image
%	\end{minipage}
%	\begin{minipage}{0.16\linewidth}
%		\centering\includegraphics[width=\linewidth]{imgs-vis-gan/1353_real} \\ (c) Real image
%	\end{minipage}
%	\end{center}
%	\caption{The examples of synthetic images from the AsynDGAN. In every three columns, the left images are the input of the AsynDGAN network, and the middle, right images are synthetic and real image based on the mask input.}
%	\label{fig:seg:hgg}
%\end{figure*}

\begin{figure*}[t]
	\vspace{-2em}
	\begin{center}
		\includegraphics[width=0.15\linewidth]{imgs-seg-nuclei/img-1}
		\includegraphics[width=0.15\linewidth]{imgs-seg-nuclei/label-1}
		\includegraphics[width=0.15\linewidth]{imgs-seg-nuclei/real-1}
		\includegraphics[width=0.15\linewidth]{imgs-seg-nuclei/fake-1}
		\includegraphics[width=0.15\linewidth]{imgs-seg-nuclei/prostate-1}
		\includegraphics[width=0.15\linewidth]{imgs-seg-nuclei/asyndgan-1} \\ \vspace{0.01in}
		\begin{minipage}{0.15\linewidth}
			\centering\includegraphics[width=\linewidth]{imgs-seg-nuclei/img-2} \\ (a) Image
		\end{minipage}
		\begin{minipage}{0.15\linewidth}
			\centering\includegraphics[width=\linewidth]{imgs-seg-nuclei/label-2} \\ (b) Label
		\end{minipage}
		\begin{minipage}{0.15\linewidth}
			\centering\includegraphics[width=\linewidth]{imgs-seg-nuclei/real-2}  \\  (c) Real-All
		\end{minipage}
		\begin{minipage}{0.15\linewidth}
			\centering\includegraphics[width=\linewidth]{imgs-seg-nuclei/fake-2} \\ (d) Syn-All
		\end{minipage}
		\begin{minipage}{0.15\linewidth}
			\centering\includegraphics[width=\linewidth]{imgs-seg-nuclei/prostate-2} \\ (e) subset-prostate
		\end{minipage}
		\begin{minipage}{0.15\linewidth}
			\centering\includegraphics[width=\linewidth]{imgs-seg-nuclei/asyndgan-2} \\ (f) AsynDGAN
		\end{minipage}
	\end{center}
	\caption{Typical nuclei segmentation results. (a) Test images. (b) Ground-truth labels of nuclei. (c)-(f) are results of models trained on all real images, synthetic images of regular GAN, real images from prostate, synthetic images of AsynDGAN, respectively. Distinct colors indicate different nuclei.}
	\label{fig:seg:nuclei}
\end{figure*}


\subsection{Nuclei segmentation}
In this subsection, we apply the AsynDGAN to multiple organ nuclei segmentation and show that our method is effective to learn the nuclear features of different organs. 
%Therefore, the synthetic images can be used to train a good nuclei segmentation model.
\vspace{-1em}
\subsubsection{Settings}
We assume that the training images belong to four different medical entities and each entity has four images of one organ. Similar to Section~\ref{sec:exp:hgg}, we conduct the following experiments:
(1) \textbf{Real-All.} Training using the 16 real images of the train set.
(2) \textbf{Real-Subset-n.} Training using 4 real images from each subset (medical entity), where $n\in\{\text{breast, liver, kidney, prostate}\}$.
(3) \textbf{Syn-All.} Training using synthetic images from regular GAN, which is trained using all 16 real images.
(4) \textbf{AsynDGAN.} Training using synthetic images from the AsynDGAN, which is trained using images from the 4 subsets distributively.
In all above experiments, we use the same organ test set for evaluation.

\begin{table}[t]
	\vspace{-0.7em}
	\begin{center}
		\begin{tabular}{lcc}
			\toprule
			Method & Dice $\uparrow$ & AJI $\uparrow$\\
			\midrule
			Real-All & 0.7833 &	0.5608  \\ \midrule
			Real-Subset-breast & 0.7340  &	0.4942 \\
			Real-Subset-liver & 0.7639 & 0.5191 \\
			Real-Subset-kidney & 0.7416 & 0.4848 \\
			Real-Subset-prostate &0.7704 & 0.5370 \\ \midrule
			Syn-All & 0.7856 &	0.5561 \\ \midrule
			\textbf{AsynDGAN}  & 0.7930 & 0.5608 \\
			\bottomrule
		\end{tabular}
	\end{center}
	\vspace{-0.3em}
	\caption{Nuclei segmentation results.}
	\label{tab:nuclei}
	\vspace{-1em}
\end{table}

\begin{figure}[t]
%	\vspace{-1.5em}
	\begin{center}
		\includegraphics[width=0.3\linewidth]{nuclei-vis-gan/4-label}
		\includegraphics[width=0.3\linewidth]{nuclei-vis-gan/4-dadgan}
		\includegraphics[width=0.3\linewidth]{nuclei-vis-gan/4-img}\\ \vspace{0.01in}
		%\includegraphics[width=0.3\linewidth]{nuclei-vis-gan/20-label}
		%\includegraphics[width=0.3\linewidth]{nuclei-vis-gan/20-dadgan}
		%\includegraphics[width=0.3\linewidth]{nuclei-vis-gan/20-img}\\ \vspace{0.01in}
		\begin{minipage}{0.3\linewidth}
			\centering\includegraphics[width=\linewidth]{nuclei-vis-gan/13-label} \\ (a) Input
		\end{minipage}
		\begin{minipage}{0.3\linewidth}
			\centering\includegraphics[width=\linewidth]{nuclei-vis-gan/13-dadgan} \\ (b) AsynDGAN
		\end{minipage}
		\begin{minipage}{0.3\linewidth}
			\centering\includegraphics[width=\linewidth]{nuclei-vis-gan/13-img}  \\  (c) Real
		\end{minipage}
	\end{center}
	\vspace{-0.5em}
	\caption{The examples of synthetic nuclei images from the AsynDGAN. (a) The input of the AsynDGAN network. (b) Synthetic images of AsynDGAN based on the input. (c) Real images.}
	\label{fig:syn:nuclei}
	\vspace{-0.5em}
\end{figure}
\vspace{-0.5em}
\subsubsection{Results}
The quantitative nuclei segmentation results are presented in Table~\ref{tab:nuclei}. Compared with models using single organ data, our method achieves the best performance. The reason is that local models cannot learn the nuclear features of other organs. Compared with the model using all real images, the AsynDGAN has the same performance, which proves the effectiveness of our method in this type of tasks. The result using regular GAN (\textbf{Syn-All}) is slightly worse than ours, probably because one discriminator is not good enough to capture different distributions of nuclear features in multiple organs. In AsynDGAN, each discriminator is responsible for one type of nuclei, which may be better for the generator to learn the overall distribution. We present several examples of synthetic images from AsynDGAN in Figure~\ref{fig:syn:nuclei}, and typical qualitative segmentation results in Figure~\ref{fig:seg:nuclei}.





