\documentclass{article}

\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{bm}
\newtheorem{thm}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{cor}{Corollary}
\newtheorem{prop}{Proposition}
\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem{remark}{Remark}
\newtheorem{example}{Example}
\newtheorem{assume}{Assumption}
\newtheorem{obs}{Observation}

\title{The AsynDGAN}
\begin{document}
	
\clearpage
\onecolumn
	\section{Appendix: AsynDGAN learns the correct distribution}
	In this section we present an analysis of AsynDGAN and discuss the implications of the results. We show that the AsynDGAN is able to aggregates multiple separated data set and learn generative distribution in an \emph{all important} fashion.
	We first begin with a technical lemma describing the optimal strategy of the discriminator.
	\begin{lemma}\label{lem1}
		When generator $G$ is fixed,  the optimal discriminator $D_j(y,x)$ is :\\
		\begin{equation}
		D_j(y,x)=\frac{p(y|x)}{p(y|x)+q(y|x)}
		\end{equation}
	\end{lemma}
	%	The proof could be found in \cite{goodfellow}, we include here for completeness.\\
	\textbf{Proof}:\\
	%%	 By taking derivative w.r.t $D_j(y,x)$ on $V(D,G)y$ we have $\sum \pi_j\int\limits_{y} s_j(x)\int\limits_{x} \frac{p(y|x)}{D_j(y,x)}  -q(y|x)log(1-D_j(y,x)) dydx$
	\begin{equation*}
	\begin{aligned}
	&\max\limits_{D}V(D)=\max\limits_{D_1\sim D_N}\sum \pi_j\int\limits_{x} s_j(x)\int\limits_{y} p(y|x)log D_j(y,x)+q(y|x)log(1-D_j(y,x)) dydx\\
	&\leq \sum \pi_j\int\limits_{x} s_j(x)\int\limits_{y} \max\limits_{D_j} \{p(y|x)log D_j(y,x)+q(y|x)log(1-D_j(y,x)) \}dydx
	\end{aligned}
	\end{equation*}
	by setting $D_j(y,x)=\frac{p(y|x)}{p(y|x)+q(y|x)}$ we can maximize each component in the integral thus make the inequality hold with equality.\qed
	
	Suppose in each training step the discriminator achieves its maxima criterion in Lemma \ref{lem1}, the loss function for the generator becomes:\\
	\begin{equation*}
	\begin{aligned}
	&\min\limits_{G}V(G)= \mathbb{E}_{y}\mathbb{E}_{x\sim p_{data}(x|y) [logD(y,x)]} +\mathbb{E}_{z\sim p_{\hat{y}}\sim(\hat{y}|x)} [log(1-D(z,y))]\\
	&=\sum_{j\in[N]} \pi_j\int\limits_{x} s_j(x) \underbrace{\int\limits_{y} p(y|x)log\frac{p(y|x)}{p(y|x)+q(y|x)}+q(y|x)log\frac{q(y|x)}{p(y|x)+q(y|x)} dydx}_{\text {To be analyzed in Lemma \ref{lem2}}}
	\end{aligned}
	\end{equation*}
	
	\begin{lemma} \label{lem2}
		Let $a(y)$ and $b(y)$ be two probability distributions s.t. support  $\Omega(a)\subset \Omega(b)$, the loss function $L(a)=\int\limits_{y} a(y)log\frac{a(y)}{a(y)+b(y)}+b(y)log\frac{b(y)}{a(y)+b(y)} dy \geq  -2 log2$. The inequality holds iff $b(y)=a(y)$.
	\end{lemma}
	
	\textbf{Proof}:\\
	Let $\lambda$ be the Lagrangian multiplier. 
	\begin{equation}
	\begin{aligned}
	L(a,\lambda)=\int\limits_{y} a(y)log\frac{a(y)}{a(y)+b(y)}+b(y)log\frac{b(y)}{a(y)+a(y)} + \lambda a(y) \;\;dy -\lambda
	\end{aligned}
	\end{equation}
	By setting $\frac{\partial L}{\partial a} =0$ we have $log\frac{a(y)}{a(y)+b(y)}=\lambda$ holds for all $x$. The fact that $log\frac{a(y)}{a(y)+b(y)}$ is a constant enforces $a(y)=b(y)$. Plugging $a(y)=b(y)$ into loss function we have $L(a)_{|a(y)=b(y)}=-2log(2)$.  \\
	% 	\begin{lemma}
	%		$L(q)\geq 2log(\frac{1}{2})$
	%	\end{lemma}
	Now we are ready to prove our main result that AsynDGAN learns the correct distribution. Assuming in each step, the discriminator always perform optimally, we show indeed the generative distribution $G$ seeks to minimize the loss by approximating underlying generative distribution of data.
	\begin{thm}
		Suppose the discriminators $D_{1\sim N}$ always behaves optimally (denoted as $D^*_{1 \sim N}$), the loss function of generator is global optimal iff $q(y,x)=p(y,x)$ where the optimal value of $V(G,D^*_{1\sim N})$ is $-log 4$. 
	\end{thm}
	
	\textbf{Proof}:\\
	\begin{equation*}
	\begin{aligned}
	&\min\limits_{\substack{ q(y,x)>0,\\\int\limits_{y} q(y,x)=s(x)}}\sum\limits_{j\in[N]} \pi_j \int\limits_{x}s_j(x)\int\limits_{y} p(y|x)log\frac{p(y|x)}{p(y|x)+q(y|x)}+q(y|x)log\frac{q(y|x)}{p(y|x)+q(y|x)} dydx\\
	&\geq\sum\limits_{j\in[N]} \pi_j \int\limits_{x}s_j(x) \min\limits_{\substack{ q(y|x)>0,\\\int\limits_{y} q(y|x)=1}}\int\limits_{y} p(y|x)log\frac{p(y|x)}{p(y|x)+q(y|x)}+q(y|x)log\frac{q(y|x)}{p(y|x)+q(y|x)} dydx\\
	%	&\geq \sum\limits_{j\in[N]} \pi_j \int\limits_{y}s_j(x) \int\limits_{x} \min\limits_{\substack{ q(y|x),\\\int\limits_{x} q(y|x)=1}} p(y|x)log\frac{p(y|x)}{p(y|x)+q(y|x)}+q(y|x)log\frac{q(y|x)}{p(y|x)+q(y|x)} dydx\\
	%	&\geq 
	\end{aligned}\\
	\end{equation*}
	By Lemma \ref{lem2}, the optimal condition for minimizing:\\
	\begin{equation*}
	\min\limits_{\substack{ q(y|x)>0,\\\int\limits_{y} q(y|x)=1}}\int\limits_{y} p(y|x)log\frac{p(y|x)}{p(y|x)+q(y|x)}+q(y|x)log\frac{q(y|x)}{p(y|x)+q(y|x)} dydx
	\end{equation*}
	is by setting $q(y|x)=p(y|x), \forall x$. Such choice of $q(y|x)$ makes the inequality holds as an equality. Meanwhile $q(y|x)=p(y|x)$ implies $q(y,x)=p(y,x)$ given the fact that $p,q$ has the same marginal distribution on $y$. By plugging in $q(y|x)=p(y|x)$ we can derive the optimal value of $V(G,D^*_{1\sim N})$ to be $-log4$.
	
	\begin{remark}
		While analysis of AsynDGAN loss shares similar spirit with \cite{goodfellow2014generative}, it has different implications. In the distributed learning setting, data from different nodes are often dissimilar. Consider the case where $\Omega(s_j(x)) \cap \Omega(s_k(x)) =\emptyset, for k \neq j$, the information for $p(y|x), x\in \Omega(s_j(x))$ will be missing if we lose the $j$-th node. The behavior of trained generative model is unpredictable when receiving auxiliary variables from unobserved  distribution $s_j(x)$.
		The AsynDGAN framework provides a solution for unifying different datasets by collaborating multiple discriminators thus can aggregate separated datasets in an \emph{all important} fashion.
	\end{remark}
	
	
\end{document}