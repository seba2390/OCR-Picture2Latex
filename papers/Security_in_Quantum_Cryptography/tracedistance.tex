\section{Trace distance}
\label{app:op}

Many of the statements in this work use the well\-/known fact that the
distinguishing advantage between two systems that output states $\rho$
and $\sigma$ is equivalent to the trace distance between these
states. In this appendix we gather lemmas and theorems which prove
this fact and help interpret the meaning of the trace distance.

In \appendixref{app:op.definitions} we first define the trace distance
\--- as well as its classical counterpart, the total variation
distance\--- and provide some basic lemmas that can also be found in
textbooks such as \textcite{nielsen2010quantum}. In
\appendixref{app:op.distadv} we then show the connection between trace
distance and distinguishing advantage, which was originally proven by
\textcite{Hel76}. In \appendixref{app:op.failure} we prove that
we can alternatively think of the trace distance between a real and
an ideal system as a bound on the probability that a failure occurs in
the real system as suggested in \textcite{Ren05}. Finally, in
\appendixref{app:op.local} we bound two typical information theory
notions of secrecy \--- the conditional entropy of a key given the
eavesdropper's information and her probability of correctly guessing
the key \--- in terms of the trace distance. Although such measures of
information are generally ill-suited for defining cryptographic
security, they can help interpret the notion of a key being
$\eps$\-/close to uniform.



\subsection{Metric definitions}
\label{app:op.definitions}

In the case of a classical system, statistical security is defined by
the total variation (or statistical) distance between the probability
distributions describing the real and ideal settings, which is defined
as follows.\footnote{We employ the same notation $D(\cdot,\cdot)$ for
  both the total variation and trace distance, since the former is a
  special case of the latter.}

\begin{deff}[Total variation distance]
  \label{def:vdist}
  The total variation distance between two probability distributions
  $P_Z$ and $P_{\tilde{Z}}$ over an alphabet $\cZ$ is defined as
  \[D(P_Z,P_{\tilde{Z}}) \coloneqq \frac{1}{2} \sum_{z \in \cZ} \left| P_Z(z) -
    P_{\tilde{Z}}(z) \right|.\]
\end{deff}

Using the fact that $|a-b| = a+b-2 \min(a,b)$, the total variation
distance can also be written as
\begin{equation} \label{eq:vdist.alt} D(P_Z, P_{\tilde{Z}}) = 1-
  \sum_{z \in \cZ} \min[P_Z(z), P_{\tilde{Z}}(z)].\end{equation}

In the case of quantum states instead of classical random variables,
the total variation distance generalizes to the trace distance. More
precisely, the trace distance between two density operators that are
diagonal in the same orthonormal basis is equal to the total variation
distance between the probability distributions defined by their
respective eigenvalues.

\begin{deff}[trace distance]
  \label{def:tdist}
  The trace distance between two quantum states $\rho$ and $\sigma$ is
  defined as
  \[D(\rho,\sigma) \coloneqq \frac{1}{2} \tr |\rho-\sigma|.\]
\end{deff}

We now introduce some technical lemmas involving the trace distance,
which help us derive the theorems in the next sections. Proofs of
these lemmas may be found in \textcite{nielsen2010quantum}.

\begin{lem}
  \label{lem:op.definitions.pos}
  For any two states $\rho$ and $\sigma$ and any operator $0 \leq
  M \leq I$,
  \begin{equation} \label{eq:op.definitions.pos}
    D(\rho,\sigma) \geq \trace{M(\rho-\sigma)}.
  \end{equation}
  Furthermore, this inequality is tight for some value of $M$.
\end{lem}
% \begin{lem}
%   \label{lem:op.definitions.pos}
%   For any two states $\rho$ and $\sigma$ and any operator $0 \leq
%   M \leq I$, the two following inequalities hold:
%   \begin{align}
%     D(\rho,\sigma) \geq \trace{M(\rho-\sigma)} \
%     , \label{eq:op.definitions.pos} \\
%     \trace{M |\rho-\sigma|} \geq \left| \trace{M (\rho-\sigma)}
%     \right| . \label{eq:op.definitions.abs}
% \end{align} Furthermore, each of these inequalities is tight for some
% values of $M$.
% \end{lem}

The trace distance can thus alternatively be written
as \begin{equation} \label{eq:op.definitions.pos.d}
D(\rho,\sigma) = \max_M \trace{M(\rho-\sigma)}.\end{equation}

% \begin{proof}
%   We start with the proof of \eqnref{eq:op.definitions.pos}. Let
%   $\{\lambda_x,\ket{\psi_x}\}_x$ be the eigenvalues and vectors of
%   $\rho-\sigma$, and define \[Q_+ \coloneqq \sum_{x : \lambda_x \geq
%     0} \lambda_x \proj{\psi_x} \qquad \text{and} \qquad Q_- \coloneqq
%   \sum_{x : \lambda_x < 0} -\lambda_x \proj{\psi_x} .\] We have
%   $\rho-\sigma = Q_+-Q_-$ and $|\rho-\sigma| = Q_++Q_-$. Note that
%   since $\trace{Q_+-Q_-} = \trace{\rho-\sigma} = 0$, we have $\tr Q_+
%   = \tr Q_-$, hence \[D(\rho,\sigma) = \frac{1}{2} \tr |\rho-\sigma|
%   =\frac{1}{2}(\tr Q_+ + \tr Q_-) = \tr Q_+ .\] If we set $\Gamma_+
%   \coloneqq \sum_{x : \lambda_x \geq 0} \proj{\psi_x}$, the projector
%   on $Q_+$, we get \[\trace{\Gamma_+(\rho-\sigma)} = \tr Q_+ =
%   D(\rho,\sigma) .\] And for any operator $0 \leq M \leq I$,
% \begin{equation*} %\label{eq:POVM.bound}
%   \trace{M (\rho-\sigma)} = \trace{M (Q_+-Q_-)} \leq \trace{M Q_+} \leq \tr Q_+ = D(\rho,\sigma) . 
% \end{equation*}

% To prove that \eqnref{eq:op.definitions.abs} holds, note that for any
% operator $0 \leq M \leq I$,
%   \begin{align*}
%     \left| \trace{M(\rho-\sigma)} \right| & = \left|
%         \trace{M(Q_+-Q_-)} \right| \\
%     & \leq \trace{M(Q_++Q_-)}  = \trace{M|\rho-\sigma|} .
%   \end{align*}
%   \eqnref{eq:op.definitions.abs} is tight for any operator $M$ which
%   satisfies \[\left|\trace{M(Q_+-Q_-)} \right| = \trace{M(Q_++Q_-)} \
%   , \]  i.e., any operator such that either $0 \leq M \leq \Gamma_+$ or $0
%   \leq M \leq \Gamma_-$, where $\Gamma_+$ is defined as above and
%   $\Gamma_- \coloneqq \sum_{x : \lambda_x \leq 0} \proj{\psi_x}$.
% \end{proof}

Let $\{\Gamma_x\}_{x}$ be a positive operator\-/valued measure (POVM)
\--- a set of operators $0 \leq \Gamma_x \leq I$ such that
$\sum_x \Gamma_x = I$ \--- and let $P_X$ denote the outcome of
measuring a quantum state $\rho$ with $\{\Gamma_x\}_{x}$, i.e.,
$P_X(x) = \trace{\Gamma_x \rho}$. Our next lemma says that the trace
distance between two states $\rho$ and $\sigma$ is equal to the total
variation between the outcomes \--- $P_X$ and $Q_X$ \--- of an optimal
measurement on the two states.

\begin{lem}
  \label{lem:op.definitions.opt}
  For any two states $\rho$ and $\sigma$,
  \begin{equation} \label{eq:op.definitions.opt} D(\rho,\sigma) =
    \max_{\left\{\Gamma_x\right\}_x} D(P_X,Q_X),\end{equation}
  where $P_X$ and $Q_X$ are the probability distributions resulting
  from measuring $\rho$ and $\sigma$ with a POVM $\{\Gamma_x\}_x$,
  respectively, and the maximization is over all POVMs. Furthermore,
  if the two states $\rho_{ZB}$ and $\sigma_{ZB}$ have a classical
  subsystem $Z$, then the measurement satisfying
  \eqnref{eq:op.definitions.opt} leaves the classical subsystem
  unchanged, i.e., the maximum is reached for a POVM with
  elements \begin{equation} \label{eq:op.definitions.opt.classical}
    \Gamma_x = \sum_z \proj{z} \otimes M^z_x ,\end{equation} where
  $\{\ket{z}\}_z$ is the classical orthonormal basis of $Z$.
\end{lem}

% \begin{proof}
%   Using \eqnref{eq:op.definitions.abs} from
%   \lemref{lem:op.definitions.pos} we get
%   \begin{align*}D(P_X,Q_X) & = \frac{1}{2} \sum_x \left|
%       \trace{\Gamma_x(\rho-\sigma)} \right| \\
%       & \leq \frac{1}{2} \sum_x \trace{\Gamma_x|\rho-\sigma|} \\
%       & = \frac{1}{2} \tr{|\rho-\sigma|} = D(\rho,\sigma).
%   \end{align*}
%   The conditions for equality are given at the end of the proof of
%   \lemref{lem:op.definitions.pos}, e.g., a measurement with $\Gamma_x
%   = \proj{\psi_x}$, where $\{\ket{\psi_x}\}_x$ are the eigenvectors of
%   $\rho-\sigma$. If $\rho_{ZB} = \sum_z p_z \proj{z} \otimes \rho_B^z$
%   and $\sigma_{ZB} = \sum_z q_z \proj{z} \otimes \sigma_B^z$, then
%   \[ \rho-\sigma = \sum_z \proj{z} \otimes
%     \left(p_z\rho^z_B-q_z\sigma^z_B\right) ,\] and the eigenvectors
%   of $\rho-\sigma$ have the form
%   $\ket{\psi_{z,x}} = \ket{z} \otimes \ket{\varphi^z_x}_B$, where
%   $\ket{\varphi^z_x}_B$ is an eigenvector of
%   $p_z\rho^z_B-q_z\sigma^z_B$. So the optimal measurement,
%   $\Gamma_{z,x} = \proj{z} \otimes \proj{\varphi^z_x}_B$, satisfies
%   \eqnref{eq:op.definitions.opt.classical}.
% \end{proof}


\subsection{Distinguishing advantage}
\label{app:op.distadv}

\textcite{Hel76} proved that the advantage a distinguisher has in
guessing whether it was provided with one of two states with equal
priors, $\rho$ or $\sigma$, is given by the trace distance between the
two, $D(\rho,\sigma)$.\footnote{Actually, \textcite{Hel76} solved
  a more general problem, in which the states $\rho$ and $\sigma$ are
  picked with apriori probabilities $p$ and $1-p$, respectively,
  instead of $1/2$ as in the definition of the distinguishing
  advantage.} We first sketch the classical case, then prove the
quantum version.

Let a distinguisher be given a value sampled according to probability
distributions $P_Z$ or $P_{\tilde{Z}}$, where $P_Z$ and
$P_{\tilde{Z}}$ are each chosen with probability $1/2$. Suppose the
value received by the distinguisher is $z \in \cZ$. If $P_Z(z) >
P_{\tilde{Z}}(z)$, its best guess is that the value was sampled
according to $P_Z$. Otherwise, it should guess that it was
$P_{\tilde{Z}}$. Let $\cZ'\coloneqq \{z \in \cZ : P_Z(z) >
P_{\tilde{Z}}(z)\}$ and $\cZ'' \coloneqq \{z \in \cZ : P_Z(z) \leq
P_{\tilde{Z}}(z)\}$. There are a total of $2|\cZ|$ possible events:
the sample is chosen according to $P_Z$ or $P_{\tilde{Z}}$ and takes
the value $z \in \cZ$. These events have probabilities
$\frac{P_Z(z)}{2}$ and $\frac{P_{\tilde{Z}}(z)}{2}$. Conditioned on
$P_Z$ being chosen and $z$ being the sampled value, the distinguisher
has probability $1$ of guessing correctly with the strategy outlined
above if $z \in \cZ'$, and $0$ otherwise. Likewise, if $P_{\tilde{Z}}$
was selected, it has probability $1$ of guessing correctly if $z \in
\cZ''$ and $0$ otherwise. The probability of correctly guessing
whether it was given a value sampled according to $P_Z$ or
$P_{\tilde{Z}}$, which we denote $\distinguish{P_Z,P_{\tilde{Z}}}$, is
obtained by summing over all possible events weighted by their
probabilities. Hence
\begin{align*}
 &  \distinguish{P_Z,P_{\tilde{Z}}} \\
& \qquad = \sum_{z \in \cZ'} \frac{P_Z(z)}{2} + \sum_{z \in
    \cZ''} \frac{P_{\tilde{Z}}(z)}{2} \\
  & \qquad =  \frac{1}{2} \left(1- \sum_{z \in
    \cZ''} P_{Z}(z) \right) + \frac{1}{2} \left(1 - \sum_{z \in
    \cZ'} P_{\tilde{Z}}(z)\right) \\
  & \qquad = 1 - \frac{1}{2} \sum_{z \in \cZ} \min[P_Z(z), P_{\tilde{Z}}(z)] \\
  & \qquad = \frac{1}{2} + \frac{1}{2} D(P_Z,P_{\tilde{Z}}) , 
\end{align*}
where in the last equality we used the alternative formulation of the
total variation distance from \eqnref{eq:vdist.alt}.

We now generalize the argument above to quantum states with equal
priors, which is a special case of \textcite{Hel76}.

\begin{thm}
  \label{thm:op.distinguishing}
  For any states $\rho$ and $\sigma$, we have \[\distinguish{\rho,\sigma} =
  \frac{1}{2}+\frac{1}{2}D(\rho,\sigma) .\]
\end{thm}

\begin{proof}
  If a distinguisher is given one of two states $\rho$ or $\sigma$,
  each with probability $1/2$, its probability of guessing which one
  it holds is given by a maximization of all possible measurements it
  may do: it chooses some POVM $\{\Gamma_0,\Gamma_1\}$, where
  $\Gamma_0$ and $\Gamma_1$ are positive operators with
  $\Gamma_0+\Gamma_1 = I$, and measures the state it holds. If it gets
  the outcome $0$, it guesses that it holds $\rho$ and if it gets the
  outcome $1$, it guesses that it holds $\sigma$. The probability of
  guessing correctly is given by
\begin{align}
  \distinguish{\rho,\sigma} & = \max_{\Gamma_0,\Gamma_1} \left[
    \frac{1}{2}\trace{\Gamma_0\rho} +
    \frac{1}{2}\trace{\Gamma_1\sigma} \right] \notag \\
  & = \frac{1}{2}\max_{\Gamma_0} \left[ \trace{\Gamma_0\rho} +
    \trace{(I-\Gamma_0)\sigma} \right] \notag \\
  & = \frac{1}{2} + \frac{1}{2} \max_{\Gamma_0}
  \trace{\Gamma_0(\rho-\sigma)} . \label{eq:op.distinguishing.thm}
\end{align}
The proof concludes by plugging \eqnref{eq:op.definitions.pos.d} in
\eqnref{eq:op.distinguishing.thm}.
\end{proof}

\subsection{Probability of a failure}
\label{app:op.failure}

The trace distance is used as the security definition of QKD, because
the relevant measure for cryptographic security is the distinguishing
advantage (as discussed in \secref{sec:ac}), and as proven in
\thmref{thm:op.distinguishing}, the distinguishing advantage between
two quantum states corresponds to their trace distance. This
operational interpretation of the trace distance involves two worlds,
an ideal one and a real one, and the distance measure is the
(renormalized) difference between the probabilities of the
distinguisher correctly guessing to which world it is connected.

In this section we describe a different interpretation of the total
variation and trace distances. Instead of having two different worlds,
we consider one world in which the outcomes of interacting with the
real and ideal systems co-exist. And instead of these distance
measures being a difference between probability distributions, they
become the probability that any (classical) value occurring in one of
the systems does not \emph{simultaneously} occur in the other. We call
such an event a \emph{failure} \--- since one system is ideal, if the
other behaves differently, it must have failed \--- and the trace
distance becomes the probability of a failure occurring.


% As explained in \secref{sec:security.simulator}, a QKD protocol is $\eps$-secure if 
% \[ d(\rho_{A B E}, \tilde{\rho}_{A B E}) \leq \eps , \]
% where $\rho_{A B E}$ and $\tilde{\rho}_{A B E}$ are the states of
% Alice's and Bob's key ($A$ and $B$) together with the information held
% by Eve ($E$) after executing the QKD protocol, and after invoking an
% ideal key distribution scheme, respectively. The aim of this section
% is to provide an interpretation of this inequality. We hope that this
% interpretation not only clarifies the operational meaning of the
% notion of $\eps$-security, but also serves as a guidance for choosing
% the parameter $\eps$ in realistic situations.

% The criterion is particularly easy to understand in the special case
% where $\eps = 0$, i.e., when $\rho_{A B E} = \tilde{\rho}_{A B
%   E}$. The equality means that executing the QKD protocol has exactly
% the same effect as invoking the ideal key distribution system. This
% ideal system is perfectly secure by definition: it provides Alice and
% Bob with a pair of identical keys, distributed uniformly and
% uncorrelated to any information accessible to Eve. Hence, for $\eps =
% 0$, we have perfect security. 

% Unfortunately, perfect security is not achievable with QKD, although
% $\eps$ can be made arbitrarily small by an appropriate choice of the
% protocol parameters. This leads us to the problem of choosing a finite
% $\eps$ such that it is neither too small nor too large. If $\eps$ is
% too large, the generated key may not be secure enough for
% applications. Conversely, generating a key for very small $\eps$ may
% be too costly or even unfeasible (typically, it necessitates the QKD
% protocol to operate on huge blocks of data).

% The following lemma implies that $\eps$ can be interpreted as a
% maximum failure probability. We first state the technical lemma and
% then explain its ramifications.\footnote{This lemma has previously
%   been stated without proof in~\cite{RK05} and~\cite{Ren05}. Given its
%   fundamental nature and simplicity, it is likely that it has been
%   used previously in the literature, but we are not aware of any
%   explicit references.}

Given two random variables $Z$ and $\tilde{Z}$ with probability
distributions $P_Z$ and $P_{\tilde{Z}}$, any distribution
$P_{Z\tilde{Z}}$ with marginals given by $P_Z$ and $P_{\tilde{Z}}$ is
called a coupling of $P_Z$ and $P_{\tilde{Z}}$. The interpretation of
the trace distance treated in this section uses one specific coupling,
known as a \emph{maximal coupling} in probability theory~\cite{Tho00}.

\begin{thm}[Maximal coupling] \label{thm:difference}
  Let $P_Z$ and $P_{\tilde{Z}}$ be two probability distributions over
  the same alphabet $\cZ$. Then there exists a probability
  distribution $P_{Z \tilde{Z}}$ on $\cZ \times \cZ$ such that
  \begin{equation} \label{eq:equalityprob}
    \Pr[Z  = \tilde{Z}] := \sum_{z} P_{Z \tilde{Z}}(z, z) \geq 1-  D(P_Z, P_{\tilde{Z}}) 
  \end{equation}
  and such that $P_Z$ and $P_{\tilde{Z}}$ are the marginals of $P_{Z
    \tilde{Z}}$, i.e., 
  \begin{align} \label{eq:marginal1}
    P_Z(z) & = \sum_{\tilde{z}} P_{Z \tilde{Z}}(z,\tilde{z})  \quad
    (\forall z \in \cZ)
    \\ \label{eq:marginal2}
    P_{\tilde{Z}}(\tilde{z}) & = \sum_z P_{Z \tilde{Z}}(z,\tilde{z})
    \quad (\forall \tilde{z} \in \cZ) .
  \end{align}
\end{thm}

It turns out that the inequality in \eqnref{eq:equalityprob} is tight,
i.e., one can also show that for any distribution $P_{Z\tilde{Z}}$,
$\Pr[Z = \tilde{Z}] \leq 1 - D(P_Z, P_{\tilde{Z}})$. We will however
not use this fact here.

Consider now a real system that outputs values given by $Z$ and an
ideal system that outputs values according to
$\tilde{Z}$. \thmref{thm:difference} tells us that there exists a
coupling of these distributions such that the probability of the real
system producing a different value from the ideal system is bounded by
the total variation distance between $P_Z$ and $P_{\tilde{Z}}$. Thus,
the real system behaves ideally except with probability $D(P_Z,
P_{\tilde{Z}})$.

We first prove this theorem, then in \corref{cor:difference} here
below we apply it to quantum systems.

\begin{proof}[Proof of \thmref{thm:difference}]
  Let $Q_{Z \tilde{Z}}$ be the real function on $\cZ \times \cZ$
  defined by
  \begin{align*}
    Q_{Z \tilde{Z}}(z,\tilde{z}) = \begin{cases} \min[P_Z(z),
      P_{\tilde{Z}}(\tilde{z})] & \text{if $z=\tilde{z}$} \\ 0 & \text{otherwise}\end{cases} 
  \end{align*}
  (for all $z, \tilde{z} \in \cZ$).  Furthermore, let $R_Z$ and
  $R_{\tilde{Z}}$ be the real functions on $\cZ$ defined by
  \begin{align*}
    R_Z(z) & = P_Z(z) - Q_{Z \tilde{Z}}(z, z) , \\
    R_{\tilde{Z}}(\tilde{z}) & = P_{\tilde{Z}}(\tilde{z}) - Q_{Z
        \tilde{Z}}(\tilde{z}, \tilde{z})  .
  \end{align*}
  We then define $P_{Z \tilde{Z}}$ by
  \begin{align*}
    P_{Z \tilde{Z}}(z, \tilde{z}) = Q_{Z \tilde{Z}}(z, \tilde{z}) +
    \frac{1}{D(P_Z, P_{\tilde{Z}})} R_Z(z) R_{\tilde{Z}}(\tilde{z}) .
  \end{align*}
   
  We now show that $P_{Z \tilde{Z}}$ satisfies the conditions of the
  theorem. For this, we note that for any $z \in \cZ$
  \begin{align*}
    R_Z(z) = P_Z(z) - \min[P_Z(z), P_{\tilde{Z}}(z)] \geq 0, 
  \end{align*}
  i.e., $R_Z$, and, likewise, $R_{\tilde{Z}}$, are nonnegative. Since
  $Q_{Z \tilde{Z}}$ is by definition also nonnegative, we have that
  $P_{Z \tilde{Z}}$ is nonnegative, too. From \eqnref{eq:marginal1}
  or~\eqref{eq:marginal2}, which we will prove below, it follows that
  $P_{Z \tilde{Z}}$ is also normalized. Hence, $P_{Z \tilde{Z}}$ is a
  valid probability distribution.

  To show \eqnref{eq:equalityprob} we use again the non\-/negativity
  of $R_{Z}$ and $R_{\tilde{Z}}$, which implies
  \begin{align*}
    \sum_z P_{Z \tilde{Z}}(z,z) & \geq \sum_z Q_{Z \tilde{Z}}(z,z) \\
    & = \sum_z \min[P_Z(z), P_{\tilde{Z}}(z)] \\
    & = 1- D(P_Z, P_{\tilde{Z}}) ,
  \end{align*}
  where in the last equality we used the alternative formulation of the
  total variation distance from \eqnref{eq:vdist.alt}.

  To prove \eqnref{eq:marginal1}, we first note that
  \begin{align*}
    \sum_{\tilde{z}} R_{\tilde{Z}}(\tilde{z}) & = \sum_{\tilde{z}}
    P_{\tilde{Z}}(\tilde{z}) - \sum_{\tilde{z}}  Q_{Z
      \tilde{Z}}(\tilde{z},\tilde{z})  \\ & =
    1 - \sum_{\tilde{z}} \min[P_Z(\tilde{z}),
    P_{\tilde{Z}}(\tilde{z})] \\ & = D(P_Z,  P_{\tilde{Z}}) .
  \end{align*}
  Using this we find that for any $z \in \cZ$
  \begin{align*}
    & \sum_{\tilde{z}} P_{Z \tilde{Z}}(z,\tilde{z}) \\
  & \qquad = \sum_{\tilde{z}} Q_{Z \tilde{Z}}(z,\tilde{z}) + R_Z(z)
    \frac{1}{D(P_Z, P_{\tilde{Z}})} \sum_{\tilde{z}}  R_{\tilde{Z}}(\tilde{z})  \\
  & \qquad = Q_{Z \tilde{Z}}(z,z) +R_Z(z) = P_Z(z) .
  \end{align*}  
  By symmetry, this also proves \eqnref{eq:marginal2}.
\end{proof}

In the case of quantum states, \thmref{thm:difference} can be used to
couple the outcomes of any observable applied to the quantum systems.

\begin{cor}
\label{cor:difference}
For any states $\rho$ and $\sigma$ with trace distance $D(\rho,\sigma)
\leq \eps$, and any measurement given by its POVM operators
$\{\Gamma_w\}_w$ with outcome probabilities $P_W(w) = \trace{\Gamma_w
  \rho}$ and $P_{\tilde{W}}(w) = \trace{\Gamma_w \sigma}$, there
exists a coupling of $P_W$ and $P_{\tilde{W}}$ such that \[ \Pr[W \neq
\tilde{W}] \leq D(\rho,\sigma) .\]
\end{cor}

\begin{proof}
  Immediate by combining \lemref{lem:op.definitions.opt} and
  \thmref{thm:difference}.
\end{proof}

\corref{cor:difference} tells us that if two systems produce states
$\rho$ and $\sigma$, then for any observations made on those systems
there exists a coupling for which the values of each measurement will
differ with probability at most $D(\rho,\sigma)$. It is instructive to
remember that this operational meaning is not essential to the
security notion or part of the framework in any way. It is an
intuitive way of understanding the trace distance, so as to better
choose a suitable value. It allows this distance to be thought of as a
maximum failure probability, and the value for $\eps$ to be chosen
accordingly.

% Assume now that Alice and Bob run an $\eps$\=/secure QKD scheme to
% obtain keys $A$ and $B$. These keys may subsequently be used in
% further applications, e.g., for one-time pad encryption or the
% authentication of messages. In parallel, an adversary, Eve, may run an
% attack. Crucially, we do not assume that the attack is restricted to
% the QKD protocol. Rather, it may consist of an arbitrary sequence of
% operations, including interactions with the further applications in
% which Alice and Bob use their key. (For example, the adversary may
% eavesdrop a channel that Alice uses to send one-time pad encrypted
% messages to Bob, and hence get hold of the ciphertext.) We also stress
% that the attack is not limited to classical information
% processing.\footnote{We refer to~\cite{KRBM07} for an example
%   demonstrating that a restriction to classical strategies is
%   generally problematic.} However, we can assume without loss of
% generality that, once the attack is completed, the adversary is
% interested in the value of an observable, which we denote by $W$.
% Mathematically, $W$ can be modeled as the outcome of an arbitrary
% measurement process on $E$ that may depend on $A$ and $B$. We denote
% by $P_{A B W}$ the joint probability distribution of all these
% variables

% In the following, we refer to the above as the \emph{real} process.
% We also introduce an \emph{ideal} process, which is identical to the
% real one, except that the execution of the QKD scheme is replaced by
% the invocation of an ideal key distribution system (or, equivalently,
% a hypothetical QKD scheme that is perfectly secure). To distinguish
% the relevant variables from those used for the real process, we denote
% Alice's and Bob's key in the ideal process by $\tilde{A}$ and
% $\tilde{B}$, and Eve's information by $\tilde{W}$, so that their joint
% probability distribution is $P_{\tilde{A} \tilde{B} \tilde{W}}$.

% Our interpretation of $\eps$\=/security is now based on a combined
% setup, where the real and the ideal process run in parallel.  This
% combined setup may be specified by a joint probability distribution
% $P_{A B W \tilde{A} \tilde{B} \tilde{W}}$, which we choose to be the
% one given by \lemref{lem:difference}, when applied to $P_{A B W}$
% and $P_{\tilde{A} \tilde{B} \tilde{W}}$. We may then define an event
% \begin{align*}
%   \Omega_{\mathrm{perfect}} := [A = \tilde{A} \text{ and } B = \tilde{B} \text{ and } W =
%   \tilde{W}] ,
% \end{align*}
% which indicates that the real process has generated exactly the same
% outcomes as the ideal process. In other words, if
% $\Omega_{\mathrm{perfect}}$ occurs then the real process does not
% differ from the one based on the ideal key generation system, which is
% by definition perfectly secure.

% \lemref{lem:difference} now provides a lower bound on the
% probability of this event in terms of the total variation distance, i.e., 
% \begin{align*}
%   \Pr[\Omega_{\mathrm{perfect}}] \geq 1 - D(P_{A B W},P_{\tilde{A} \tilde{B} \tilde{W}}) .
% \end{align*}
% Using the definition of $\eps$-security, together with the fact that
% the trace distance is non-increasing under measurements, we conclude
% that
% \begin{align*}
%   \Pr[\Omega_{\mathrm{perfect}}] \geq 1-\eps .
% \end{align*}
% In other words, the real process is identical to the perfect one,
% except with probability at most $\eps$. In this sense, the value
% $\eps$ can be interpreted as an upper bound on the failure probability
% of the QKD scheme.


\subsection{Measures of uncertainty}
\label{app:op.local}

Non\-/composable security models often use measures of uncertainty to
quantify how much information an adversary might have about a secret,
e.g., entropy as used by Shannon to prove the security of the one-time
pad~\cite{Shannon49}. These measures are often weaker than what one
obtains using a global distinguisher, and in general do not provide
good security definitions. They are however quite intuitive and in
order to further illustrate the quantitative value of the
distinguishing advantage, we derive bounds on two of these measures of
uncertainty in terms of the trace distance, namely on the probability
of guessing the secret key in
\appendixref{app:op.local.guessing} and on the von Neumann entropy of the
secret key in \appendixref{app:op.local.entropy}.

\subsubsection{Probability of guessing}
\label{app:op.local.guessing}

Let $\rho_{KE} = \sum_{k \in \cK} p_k \proj{k}_K \otimes \rho^k_E$ be
the joint state of a secret key in the $K$ subsystem and Eve's
information in the $E$ subsystem. To guess the value of the key, Eve
can pick a POVM $\{\Gamma_k\}_{k \in \cK}$, measure her system, and
output the result of the measurement. Given that the key is $k$, her
probability of having guessed correctly is $\trace{\Gamma_k
  \rho^k_E}$. The average probability of guessing correctly for this
measurement is then given by the sum over all $k$, weighted by their
respective probabilities $p_k$. And Eve's probability of correctly
guessing the key is defined by taking the maximum over all
measurements,
\begin{equation} \label{eq:pguess} \pguess[\rho]{K|E} \coloneqq
  \max_{\{\Gamma_k\}} \sum_{k \in \cK} p_k \trace{\Gamma_k \rho^k_E}.
\end{equation}

\begin{lem}
  \label{lem:pguess}
For any bipartite state $\rho_{KE}$ with classical $K$, 
\[\pguess[\rho]{K|E} \leq
\frac{1}{|\cK|} + D\left( \rho_{KE},\tau_K \otimes \rho_E\right),
\] where $\tau_K$ is the fully mixed state.
\end{lem}

\begin{proof}
  Note that for $M \coloneqq \sum_k \proj{k} \otimes \Gamma_k$, where
  $\{\Gamma_k\}$ maximizes \eqnref{eq:pguess}, the guessing
  probability can equivalently be written \[\pguess[\rho]{K|E} =
  \trace{M \rho_{KE}}. \] Furthermore,
\[\ktrace{M \left(\tau_K \otimes \rho_{E}\right)} = \frac{1}{|\cK|}. \]
In \lemref{lem:op.definitions.pos} we proved that for any
operator $0 \leq M \leq I$,
\[\trace{M(\rho - \sigma)} \leq D(\rho,\sigma). \]
Setting $\rho = \rho_{KE}$ and $\sigma = \tau_K \otimes \rho_{E}$ in
the above inequality, we finish the proof:
\begin{align*}
& \trace{M\rho_{KE}} \leq \trace{M\left(\tau_K \otimes \rho_{E}\right)}
+ D\left( \rho_{KE} , \tau_K \otimes \rho_{E} \right) , \\
 & \implies \quad \pguess[\rho]{K|E} \leq \frac{1}{|\cK|} + D\left( \rho_{KE},\tau_K
   \otimes \rho_E\right) . \qedhere
\end{align*}
\end{proof}


\subsubsection{Entropy}
\label{app:op.local.entropy}

Let $\rho_{KE} = \sum_{k \in \cK} p_k \proj{k}_K \otimes \rho^k_E$ be
the joint state of a secret key in the $K$ subsystem and Eve's
information in the $E$ subsystem. We wish to bound the von Neumann
entropy of $K$ given $E$ \--- $\Ss(K|E)_{\rho} = \Ss(\rho_{KE}) -
\Ss(\rho_E)$, where $S(\rho) \coloneqq -\trace{\rho \log \rho}$ \---
in terms of the trace distance $D\left( \rho_{KE},\tau_K \otimes
  \rho_E\right)$. We first derive a lower bound on the von Neumann
entropy, using the following theorem from \textcite{AF04}.

\begin{thm}[From~\textcite{AF04}]
  \label{thm:AF04}
  For any bipartite states $\rho_{AB}$ and $\sigma_{AB}$ with trace
  distance $D(\rho,\sigma) = \eps \leq 1/4$ and $\dim \hilbert_A =
  d_A$, we have \[ \left| \Ss(A|B)_\rho - \Ss(A|B)_\sigma \right| \leq
  8 \eps \log d_A + 2h(2\eps) , \] where $h(p) = - p \log p - (1-p)
  \log (1-p)$ is the binary entropy.
\end{thm}

\begin{cor}
  \label{cor:AF04}
 For any state $\rho_{KE}$ with $D(\rho_{KE},\tau_K \otimes
  \rho_E) = \eps \leq 1/4$, where $\tau_K$ is the fully mixed state,
  we have
  \[ \Ss(K|E)_{\rho} \geq (1- 8\eps) \log |\cK| - 2h(2\eps) . \]
\end{cor}

\begin{proof}
  Immediate by plugging $\rho_{KE}$ and $\tau_K \otimes \rho_E$ in
  \thmref{thm:AF04}.
\end{proof}

Given the von Neumann entropy of $K$ conditioned on $E$,
$\Ss(K|E)_{\rho}$, one can also upper bound the trace distance of
$\rho_{KE}$ from $\tau_K \otimes \rho_E$ by relating $\Ss(K|E)_{\rho}$
to the relative entropy of $\rho_{KE}$ to $\tau_K \otimes \rho_E$ \---
the relative entropy of $\rho$ to $\sigma$ is defined as $\Ss(\rho \|
\sigma) \coloneqq \trace{\rho \log \rho} - \trace{\rho \log \sigma}$.

\begin{lem}
  \label{lem:entropybound}
  For any quantum state $\rho_{KE}$,
  \[ D(\rho_{KE},\tau_K \otimes \rho_E) \leq
  \sqrt{\frac{1}{2}\left(\log |\cK| - \Ss(K|E)_{\rho}\right)} . \]
\end{lem}

\begin{proof}
  From the definitions of the relative and von Neumann entropies we
  have
  \begin{align*}
    \Ss \left(\rho_{KE} \middle\| \tau_K \otimes \rho_E \right) & =
  \log |\cK| + \Ss \left(\rho_{KE} \middle\| \id_K \otimes \rho_E
  \right) \\ & = \log |\cK| - \Ss(K|E)_{\rho},\end{align*} where $\id_K$ is the
  identity matrix. We then use the following bound on the relative
  entropy \cite[Theorem~1.15]{OP93} to conclude the proof:
  \[ \Ss\left(\rho \middle\| \sigma \right) \geq 2
  \left(D(\rho,\sigma)\right)^2. \qedhere\]
\end{proof}

\corref{cor:AF04} and \lemref{lem:entropybound} can be written
together in one equation, upper and lower bounding the conditional von
Neumann entropy:
\[ (1- 8\eps) \log |\cK| - 2h(2\eps) \leq \Ss(K|E)_{\rho} \leq \log
|\cK| - 2\eps^2, \] where $\eps = D(\rho_{KE},\tau_K \otimes
\rho_E)$.

%%% Local Variables:
%%% TeX-master: "main.tex"
%%% End:
