\begin{table*}[t]
\centering
\renewcommand{\arraystretch}{1.2}
\resizebox{0.85\textwidth}{!}{%
\begin{tabular}{l|ccccccc|c}
\toprule
\textbf{Method}                                              & \textbf{PI-CD} & \textbf{PI-SP} & \textbf{IS-SD} & \textbf{IS-CS} & \textbf{LI-LI} & \textbf{LI-TS} & \textbf{ST}   & \textbf{Avg.} \\ \midrule
Text Swapping*                                    & 71.7  & 72.8  & 63.5  & 67.4  & 86.3  & \textbf{86.8}  & 66.5 & 73.6 \\
Sub(synonym)*                                & 69.8  & 72.0  & 62.4  & 65.8  & 85.2  & 82.8  & 64.3 & 71.8 \\
Sub(MLM)*                                    & 71.0  & 72.8  & 64.4  & 65.9  & 85.6  & 83.3  & 64.9 & 72.6 \\
Paraphrasing*                                   & 72.1  & 74.6  & 66.5  & 66.4  & 85.7  & 83.1  & 64.8 & 73.3 \\ \midrule
BERT-base(MNLI)                            & 70.3  & 73.7  & 53.5  & 64.8  & 85.5  & 81.6  & 69.2 & 71.2 \\ \midrule
\model &  \textbf{72.49}     &  \textbf{76.28}     &   \textbf{71.67}    & \textbf{68.75}    & \textbf{91.43}   & 82.46  & \textbf{71.90}  & \textbf{76.43}  \\ 
\bottomrule
\end{tabular}%
}
\caption{Results on the NLI adversarial test benchmark \cite{liu-etal-2020}. We compare \model with the data augmentation techniques investigated by \citet{liu-etal-2020}. BERT-base(MNLI) indicates the BERT-base model trained on the training data of MNLI. Note that training on the \model's augmented datasets significantly improves the model performance on nearly all test sets. }
\label{tab:NLI adversarial}
\end{table*}