% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
% \usepackage[review]{EMNLP2023}
\usepackage[]{EMNLP2023}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{booktabs}
% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary and may be commented out.
% However, it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}
%\usepackage{CJKutf8}
% enable Chinese
%\usepackage[UTF8]{ctex}
% This is also not strictly necessary and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}
\usepackage{xspace}
\usepackage{graphicx}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{pifont}
% \usepackage{algpseudocode}
\renewcommand{\algorithmicrequire}{\textbf{Input:}} % Use Input in the format of Algorithm
\renewcommand{\algorithmicensure}{\textbf{Output:}} % Use Output in the format of Algorithm
\def\model{\textsc{IBADR}\xspace}


% \algnewcommand\algorithmicforeach{\textbf{for each}}
% \algdef{S}[FOR]{ForEach}[1]{\algorithmicforeach\ #1\ \algorithmicdo}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{IBADR: an Iterative Bias-Aware Dataset Refinement Framework for Debiasing NLU models}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{Xiaoyue Wang$^{a,c,}\footnotemark[2]$, Xin Liu$^{a,c,}\footnotemark[2]$, Lijie Wang$^b$, Yaoxiang Wang$^{a,c}$,  Jinsong Su$^{a,c,}\footnotemark[1]$ \and Hua Wu$^b$ \\
$^a$School of Informatics, Xiamen University, Xiamen 361005, China,\\ 
%$^b$University of Michigan, Ann Arbor 48104, USA \\
$^b$Baidu Inc., Beijing 100085, China,\\
$^c$Key Laboratory of Digital Protection and Intelligent Processing  of Intangible Cultural Heritage \\ of Fujian and Taiwan (Xiamen University), Ministry of Culture and Tourism, China\\
\texttt{\{xiaoyuewang, liuxin\}@stu.xmu.edu.cn, jssu@xmu.edu.cn}}



\begin{document}
\maketitle
\begin{abstract}
As commonly-used methods for debiasing natural language understanding (NLU) models, dataset refinement approaches heavily rely on manual data analysis, and thus maybe unable to cover all the potential biased features.
In this paper, we propose \model, an Iterative Bias-Aware Dataset Refinement framework, which debiases NLU models without predefining biased features. We maintain an iteratively expanded sample pool. Specifically, at each iteration,
we first train a shallow model to quantify the bias degree of samples in the pool. Then, 
we pair each sample with a bias indicator representing its bias degree, and use these extended samples to train a sample generator. In this way,
this generator can effectively learn the correspondence relationship between bias indicators and samples. Furthermore, we employ the generator to produce pseudo samples with fewer biased features by feeding specific bias indicators. Finally,
we incorporate the generated pseudo samples into the pool. Experimental results and in-depth analyses on two NLU tasks show that \model not only significantly outperforms existing dataset refinement approaches, achieving SOTA, but also is compatible with model-centric methods. \footnote{We release our code at \url{http://github.com/DeepLearnXMU/IBADR}.}
\end{abstract}

\renewcommand{\thefootnote}{\fnsymbol{footnote}}
\footnotetext[2]{These authors contributed equally to this work.}
\footnotetext[1]{Corresponding author.}

\section{Introduction}
Although neural models have made significant progress in many natural language understanding (NLU) tasks 
 \cite{Bowman2015,gururangan-etal-2018-annotation}, recent studies have demonstrated that these models exhibit limited generalization to out-of-distribution data and are vulnerable to various types of adversarial attacks \cite{DasguptaGSGG18, McCoyPL19}. This is primarily due to their tendency to rely excessively on biased features---spurious surface patterns that are falsely associated with target labels, rather than to capture the underlying semantics. Consequently, how to effectively debias neural networks has become a prominent research topic, attracting increasing attention recently.

To alleviate this issue, researchers have proposed many methods that can be generally divided into two categories: \textit{model-centric mitigation approaches} \cite{clark-etal-2019-dont,stacey-etal-2020-avoiding,utama-etal-2020-mind,karimi-mahabadi-etal-2020-end,du2021towards} and \textit{dataset refinement approaches} \cite{lee2021crossaug,wu2022generating,ross2022}. The former mainly focuses on designing model architectures or training objectives to prevent models from exploiting dataset biases, and the latter aims to adjust dataset distributions to reduce correlations between spurious features and labels. In these two types of methods, dataset refinement approaches possess the advantage of not requiring modifications to the model architecture and training objective, while are also compatible with model-centric approaches. Therefore, in this work, we also concentrate on dataset refinement approaches, which employ controllable generation techniques \cite{zhou2020exploring, hu2022controllable} to refine the data distribution.
However, recent studies \cite{lee2021crossaug,wu2022generating,ross2022} heavily rely on manual data analysis for debiasing models. They either define perturbation rules to generate adversarial examples, or generate pseudo samples and filter out samples with identified biased features. Typically, the state-of-the-art (SOTA) method \cite{wu2022generating} first generates a large amount of samples and then applies \textit{z-filtering} involving a predefined set of biased features to eliminate samples with such features. However, these methods of manually predefining biased features may overlook some potential biased features, thus limiting their generalizability.

In this paper, we propose \textbf{\model}, an \underline{I}terative \underline{B}ias-\underline{A}ware \underline{D}ataset \underline{R}efinement framework, which iteratively generates samples to debias NLU models without predefining biased features. 
\begin{figure*}[!t]
    \centering
    \includegraphics[width=\textwidth]{figures/main.pdf}
    \caption{Overview of the iterative sample generation process, which consists of four key stages: \ding{172} Setting bias indicator; \ding{173} Finetuning sample generator; \ding{174} Generating pseudo samples; and \ding{175} Expanding sample pool. %Dotted line indicates the stage is optional. 
    Through $N_{iter}$ iterations of the above steps, we continuously augmented the sample pool with pseudo samples, which can be effectively employed to debias the NLU models.}
    \label{fig:main}
\end{figure*}
Under this framework, we create a sample pool initialized by the original training samples, and gradually expand it through multiple iterations. 
As shown in Figure \ref{fig:main},
in each iteration, we first sort and group samples in the pool according to their bias degree, determined by a shallow model trained on a limited set of training samples. Next, we concatenate samples in each group with a \emph{bias indicator} that represents its bias degree. These concatenated samples are then utilized to train a sample generator, which effectively learns the correspondence relationship between bias indicators and samples. Afterwards, as implemented in the training phase, we feed a low-degree bias indicator to the sample generator, allowing it to generate pseudo samples with fewer biased features. Finally, we add these pseudo samples back into the sample pool and repeat the above process until the maximum number of iterations is reached. 

Apparently, the above iterative process guides the sample generator towards samples with fewer biased features. However, we observe the generated pseudo samples display less diversity when we feed the lowest-degree bias indicator to the sample generator. The underlying reason is that the shallow model consistently assigns a relatively low bias degree to samples with specific patterns, such as the premise directly negating the hypothesis by inserting a word ``\textit{not}''. Consequently, the sample generator learns these patterns and tends to produce samples containing similar patterns, thereby limiting their diversity.

To address this issue, we further explore two strategies to diversify generations. 
First, instead of always using the lowest-degree bias indicator, we randomly select a low-degree bias indicator.
In this way, the sample generator is discouraged from continually creating pseudo samples containing similar patterns,
while still ensuring fewer biased features in the pseudo samples. 
Secondly, we dynamically update the shallow model by integrating the newly generated pseudo samples during the iterative generation process. By doing this, we effectively decrease the assignment of the lowest-degree bias indicator to pattern-specific samples, ultimately promoting greater diversity of the generated samples.


To summarize, the main contributions of this paper are three-fold: 
\begin{itemize}
    \item  We propose a dataset refinement framework designed to iteratively generate pseudo samples without prior analysis of biased features.
    \item We present two strategies to enhance the diversity of the pseudo samples, which further boost the performance of NLU models.
    \item To verify the effectiveness and generality of \model, we conduct experiments on two NLU tasks. The experimental results show that \model achieves SOTA performance.
\end{itemize}

\section{The \model Framework}
In this section, we give a detailed description of \model.
Under this framework, we first use a limited set of training samples to train a shallow model, which serves to measure the bias degree of samples. Then, we iteratively generate pseudo samples with fewer biased features, as illustrated in Figure \ref{fig:main}. Finally, these pseudo samples are used to debias the NLU models via retraining.



\subsection{Training a Shallow Model to Measure the Bias Degree of Samples}
\label{sec:measure}
As investigated in \cite{DBLP:conf/emnlp/UtamaMG20}, a shallow model trained on a small portion of training data tends to overfit on biased features, thus is highly confident on the samples that contain biased features. Motivated by this, we randomly select some training samples to train a shallow model, denoted as $\theta_s$, for measuring the bias degree of samples.

Let $(x^{(i)}, y^{(i)})$ denote a training sample for NLU tasks, where $y^{(i)}$ is the golden label of the input $x^{(i)}$, we directly use the model confidence $p(y^{(i)}|x^{(i)};\theta_s)$ to quantify the bias degree of $(x^{(i)}, y^{(i)})$. Apparently, if $p(y^{(i)}|x^{(i)};\theta_s) \xrightarrow{} 1$, $(x^{(i)}, y^{(i)})$ is more likely to be a biased one.

Back to our framework, our primary objective is to generate samples with a low bias degree, which can be used to reduce spurious correlations via adjusting dataset distributions.


\subsection{Iterative Pseudo Sample Generation}
\label{sec:iter}
The overview of the iterative sample generation process is shown in Figure \ref{fig:main}. 
During this process, we introduce a sample generator $\theta_g$ to iteratively generate pseudo samples, which are added into a sample pool $\mathcal{S}$. Specifically, we initialize the sample pool $\mathcal{S}$ with the original training samples, the sample generator $\theta_g$ with a generative pretrained language model. Then, we iteratively expand $\mathcal{S}$ via the following four stage:


\paragraph{\indent Step 1: Setting Bias Indicators.}
First, we use the above-mentioned shallow model to measure the bias degree of each sample in $\mathcal{S}$, as described in Section \ref{sec:measure}, and sort these samples according to their bias degree and divide them into $N_{bi}$ groups with equal size. Each group is assigned with a bias indicator $b_n$, where $1$$\leq$$n$$\leq$$N_{bi}$, $b_1$ represents the lowest-degree bias indicator and $b_{N_{bi}}$ denotes the highest-degree bias indicator.
\paragraph{\indent Step 2: Finetuning Sample Generator.}
Then, we use the samples in $\mathcal{S}$ to finetune the sample generator $\theta_g$ via the following loss function:
\begin{align}
    &\mathcal L_g=-\sum_{i}^{|\mathcal S|} \log p(x^{(i)}|b^{(i)},y^{(i)}; \theta_g), 
\end{align}
where $b^{(i)}$ represents the bias indicator assigned for the training sample $(x^{(i)}, y^{(i)})$. Through training with this objective, the generator can effectively learn the correspondence relationship between bias indicators and samples. Furthermore, in the subsequent stages, we can specify both the bias indicator and the label to control the generations of pseudo samples.

\paragraph{\indent Step 3: Generating Pseudo Samples.}
Next, we designate a bias indicator $\bar b$ representing a low degree of bias, and then feed it with a randomly-selected NLI label $\bar y$ into the generator $\theta_g$. 
This process allows us to form a pseudo sample $(\bar x, \bar y)$ by sampling $\bar x$ from the generator output distribution $p_g(\cdot | \bar b, \bar y; \theta_g)$.
By repeating this sampling process, we can obtain a set of generated pseudo samples with fewer biased features.

\paragraph{\indent Step 4: Expanding Sample Pool.}
Subsequently, to ensure the quality of generated pseudo samples, we follow \citet{wu2022generating} to filter the above generated pseudo samples with model confidence lower than a threshold $\epsilon$, and incorporate the remaining pseudo samples back into $\mathcal{S}$.


After $N_{iter}$ iterations of the above steps, our sample pool contains not only the original training samples, but also abundant pseudo samples with fewer biased features. Finally, we debias the NLU model via the retraining on these samples.  





\subsection{Diversifying Pseudo Samples}
\label{sec:diversifying}
Intuitively, the most direct way is to set the above specified bias indicator $\bar b$ to $b_1$, which denotes the lowest bias degree. However, we observe that such generated pseudo samples lack diversity and fail to cover diverse biased features. The reason behind this is that the generated pseudo samples designated with $b_1$ always follow certain patterns, exhibiting less diversity compared to those assigned with other bias indicators. 
For example, the premise directly negates the hypothesis using the word ``\emph{not}''. 
Consequently, this results in spurious correlations between $b_1$ and these certain patterns.
Hence, the generator tends to generate samples following these patterns and fails to generate samples that compass a broader range of biased features.

To address this issue, we employ the following two strategies:
(i) Instead of using the lowest-degree bias indicator $b_1$, we use a randomly-selected low-degree bias indicator: $\bar b$$=$$b_r$, where $1$$\leq$$r$$\leq$$\frac{N_{bi}}{2}$, and feed it into the generator during the iterative generation process. Upon human inspection, we observe that the generated pseudo samples not only become diverse but also still contain relatively few biased features. 
(ii) During the generation process, we update the shallow model $
\theta_s$ using a randomly-extracted portion of $\mathcal S$ at each iteration. This strategy prevents the shallow model from consistently predicting a low bias degree to pseudo samples following previously-appeared patterns, thereby enhancing the diversity of the pseudo samples.



\section{Experiments}
\subsection{Setup}

\indent \indent \textbf{Tasks and Datasets.}
We conduct experiments on two NLU tasks: natural language inference and fact verification.

\begin{itemize}
    \item \textbf{Natural Language Inference (NLI).} This task aims to predict the entailment relationship between the pair of premise and hypothesis. We conduct experiments using the MNLI \cite{WilliamsNB18_mnli} and SNLI \cite{Bowman2015} datasets. As conducted in previous studies \cite{stacey-etal-2020-avoiding, wu2022generating,lyu2022feature}, in addition to the development sets, we evaluate \model on the corresponding challenge sets for MNLI and SNLI, namely HANS \cite{McCoyPL19} and the Scramble Test \cite{DasguptaGSGG18}, respectively. These two challenge sets are specifically designed to assess whether the model relies on syntactic and word-overlap biases to make predictions.
    \item \textbf{Fact Verification.} This task is designed to determine whether a textual claim is supported or refuted by the provided evidence text. We select FEVER \cite{ThorneVCM18_fever} as our original dataset and evaluate the model performance on the development set and two challenge sets: FeverSymmetric V1 and V2 (Symm.v1 and Symm.v2) \cite{feversymm2019}, both of which are developed to mitigate biases stemming from claim-only data. 
\end{itemize}


\input{table/tab1_data}
\input{table/num_bias_indicators}
\input{table/mnli_snli_rt}
\indent \textbf{Baselines.}
We compare \model with the following baselines:
\begin{itemize}
    \item  \textbf{CrossAug} \cite{lee2021crossaug}. This method tackles negation bias in the fact verification task through a contrastive data augmentation method.
    \item \textbf{\textit{z}-filter} \cite{wu2022generating}. 
    It first defines a set of task-relevant biased features, and then trains a generator on existing datasets to generate pseudo samples, where pseudo samples with these biased features are filtered. Finally, the remaining samples are used to retrain the model.
    
    \item \textbf{Products-of-Experts (PoE)} \cite{HeZW19_POE,karimi-mahabadi-etal-2020-end}. 
  In an ensemble manner, it trains a debiased model with a bias-only one, the predictions of which heavily rely on biased features. By doing so, the debiased model is encouraged to focus on samples with fewer biased features where the bias-only model performs poorly.
    
    \item \textbf{Confidence Regularization (Conf-reg)} \cite{utama-etal-2020-mind}. This method trains a debiased model by increasing the uncertainty of samples with biased features. It first trains a bias-only model to quantify the bias degree of each sample, and then scales the output distribution of a teacher model based on the bias degree, where the re-scaled distribution can be used to enhance the debiased model.
    
   \item \textbf{Example Reweighting (Reweight)} \cite{schuster-etal-2019-towards}. This method aims to reduce the contribution of samples with biased features on the training loss by assigning them with relatively small weights.
   
   \item \textbf{Debiasing Contrastive Learning (DCT)} \cite{lyu2022feature}. It applies contrastive learning to mitigate biased latent features by utilizing a specifically designed sampling strategy.
\end{itemize}

Please note that in exception to CrossAug and \textit{z}-filter, which are dataset refinement approaches, all other approaches are model-centric.


\input{table/fever_rt}

\indent \textbf{Implementation Details.}
In our experiments, we use GPT2-large \cite{radford2019language}
to construct the sample generator and the shallow model, respectively.
To train the shallow models for different NLU tasks, we randomly select 2K, 2K, and 0.5K samples from the original training sets of MNLI, SNLI, and FEVER, individually. These shallow models are trained for 3 epochs with a learning rate of 5e-5.
When training the sample generator, we set the learning rate to 5e-5, the number of pseudo sample generated per iteration to 200K, and the iteration number $N_{iter}$ to 5.
Particularly, we train the sample generator for 3 epochs in the first iteration and for only 1 epoch in the subsequent iterations.
When updating the shallow model, we randomly select 2K samples from the sample pool to finetune it.

For the NLU models, we train them on the augmented datasets of different tasks for 8 epochs using a learning rate of 1e-5. We employ an early stop strategy during the training process. We conduct all experiments three times, each time with different random seeds, and report the average results.
Sample numbers of the augmented datasets are listed in Table \ref{tab:data num}.

\subsection{Effect of Bias Indicator Number $N_{bi}$}
The bias indicator number $N_{bi}$ on our framework is an important hyper-parameter, which determines the partition granularity of the sample pool.
Thus, we gradually vary $N_{bi}$ from 3 to 9 with an increment of 2 in each step, and compare the model performance on the development sets of MNLI. 

As shown in Table \ref{tab:num_bias_indicator}, when $N_{bi}$ is set to a smaller value, such as 3, there is a significant decrease in model performance. This is because in this case, the bias indicator can only be set to $b_1$, which reduces the diversity of generated pseudo samples, as discussed in Section \ref{sec:diversifying}. Conversely, when $N_{bi}$ is set to larger values, e.g., 7 or 9, the model performance on both development sets also decreases. We hypothesize that this decline occurs because a larger value of $N_{bi}$ results in a fine-grained partition of the sample pool, reducing the number of samples corresponding to each specific bias indicator. Consequently, this weakens the correspondence relationship between bias indicators and samples, and thus harms the performance of the sample generator. According to these results, we set $N_{bi}$ to 5 for all subsequent experiments.

\input{table/ablation_study}
\input{table/NLI_adv}
\subsection{Main Results}

Table \ref{tab:mnli_snli_rt} presents the experimental results. Overall, compared with all %the model-centric and dataset refinement 
baselines, \model is able to achieve the most significant improvements on the challenge sets (i.e. HANS, Scramble, Symm.v1, and Symm.v2). Specifically, IBADR achieves improvements of 2.57, 4.44, 1.11 and 4.16 points than the previously-reported best results, respectively.
Note that \model is effective on both the development and challenge sets of MNLI and FEVER, while other baselines, for example, Reweight and \textit{z}-filter, decline on the development sets. 

\subsection{Compatibility of \model with PoE} 
To assess the compatibility of \model with model-centric debiasing methods, we report the model performance when simultaneously using \model and PoE \cite{HeZW19_POE}, following the setting of \citet{wu2022generating}.

As shown in Table \ref{tab:poe}, on the challenge sets, the combination of \model and PoE not only yields better results than using PoE or IBADR individually, but also outperforms the combination of \textit{z}-filter and PoE. 
Thus, we believe that \model has the potential to further enhance the performance of existing model-centric methods.
\subsection{Ablation Study}
To assess the effects of special designs on \model, we also report the performance of several \model variants on MNLI:
\begin{itemize}
\item \emph{w/o USM.} In this variant, we do not \underline{U}pdate the \underline{S}hallow \underline{M}odel during the process of iterative sample generation.

\item \emph{$b_r$$\Rightarrow$$b_1$.} The sample generator uses the lowest bias indicator $b_1$ rather than the randomly-selected low-degree bias indicator $b_r$, where $1$$\leq$$r$$\leq$$\frac{N_{bi}}{2}$, to generate pseudo samples.

\item  \emph{w/o USM \& $b_r$$\Rightarrow$$b_1$.} In this variant, the sample generator uses the bias indicator $b_1$ to generate pseudo samples, and the shallow model remains fixed during the generation process. 

\item  \emph{w/o bias indicator.} This variant directly uses the samples without the bias indicator to train the sample generator.


\item  \emph{w/o iterative generation.} Instead of generating pseudo samples iteratively, we only utilize the sample generator trained in the first iteration to generate pseudo samples. 
\end{itemize}
As shown in Table \ref{tab:ablation study}, all variants exhibit performance declines on HANS, indicating the effectiveness of our special designs. Particularly, \textit{w/o bias indicator} demonstrates the most significant performance drop, which is intuitive since the bias indicator can guide the sample generator to produce pseudo samples with fewer biased features. Without the bias indicators, the generated pseudo samples will contain undesired biased features, resulting in poorer performance on the challenge set.



\subsection{Adversarial Tests for Combating Distinct Biases in NLI}
As shown in \cite{liu-etal-2020}, current debiasing approaches primarily concentrate on addressing known biases, and thus might fail to mitigate unknown biases in NLI tasks.  To assess the robustness of NLI models, \citet{liu-etal-2020} introduce several comprehensive test sets to evaluate the model performance across various types of biased features, including partial input heuristics (\textbf{PI}), inter-sentence heuristics (\textbf{IS}), logical inference ability (\textbf{LI}), and stress tests (\textbf{ST}). Moreover, they propose several data augmentation strategies to improve the model generalization:
(i) \textbf{Text Swapping}: this strategy exchanges the premise and hypothesis in each original training sample; (ii) \textbf{Sub(synonym)}: words in the hypothesis are randomly replaced with synonyms; (iii) \textbf{Sub(MLM)}: a masked language model is used to predict the randomly masked words in the hypothesis; (iv) \textbf{Paraphrasing}: the hypotheses are paraphrased through back translation.

We compare \model with the above data augmentation strategies in Table \ref{tab:NLI adversarial}. Overall, \model achieves better results on nearly all test sets. 
This reveals that \model can effectively mitigate various biases simultaneously. We attribute this success to the fact that \model does not rely on predefined biased features, which enables it to better handle unknown biases.


\input{table/generlization}
\subsection{Generalization on Different Sizes of Pre-trained Language Models}

To further explore the compatibility of \model with different sizes of pre-trained language models, we reconduct experiments by individually replacing the BERT-base model with BERT-large, RoBERTa-base and RoBERTa-large. We also compare \model with \textit{z}-filter, which is the current SOTA data refinement method. The comparisons are performed on the MNLI and SNLI datasets.\footnote{Since the code of \textit{z}-filter only supports MNLI and SNLI, we exclusively compare our results with theirs for these two datasets.}

As presented in Table \ref{tab:generlization}, \model consistently outperforms \textit{z}-filter on all test datasets. When respectively using BERT-large, RoBERTa-base, and RoBERTa-large as pretrained models, \model achieves average improvements of 3.29, 1.88, and 3.34 point, compared to \textit{z}-filter. These results clearly demonstrate the superior generalizability of \model across different sizes of pre-trained language models.

\subsection{Results on Different Sizes of Datasets}
To investigate the robustness of \model when the number of original training samples is limited, we randomly select two subsets from original training samples of MNLI, with sizes 100K and 200K, respectively. Afterwards, we employ \model to augment these subsets and subsequently retrain NLU models using the augmented datasets.

Table \ref{tab:change_data_size} presents the results of both the development and challenge sets. We can observe that \model consistently improves model performance across all test sets. Notably, even when using a limited number of original training samples, e.g. 100K, the model trained on the \model augmented dataset outperforms the full-size baseline. This suggests that \model exhibits remarkable robustness on limited original training samples.
\input{table/change_data_size}

\subsection{The Effect of Augmented Dataset Size}
To explore the influence of augmented dataset size, we retrain the NLU model on the MNLI dataset with different numbers of augmented samples: 10K, 100K, 300K, 600K, and 900K, respectively. As indicated in Table \ref{tab:aug_datasize}, the performance of \model consistently improves. Moreover, with just 100K augmented samples, \model outperforms $z$-filter across dev-m, dev-mm, and HANS. It's worth mentioning that $z$-filter utilizes a larger set of 360K augmented samples.
\input{table/datasize}

\subsection{The Compatibility with Advanced Language Models}
\label{apdx:lora}
To ensure a fair comparison with $z$-filter, we employ GPT-2 Large as the sample generator in our main study. In exploring \model's compatibility with advanced large language models (LLM), we finetune the LLaMA-7b model \cite{llama} using LORA \cite{hu2021lora} as an alternative to GPT-2 Large. The results on the MNLI dataset are listed in Table \ref{tab:lora}. We can observe that the performance of \model is further improved with LLaMA-7b, which indicates \model's generalizability.

\begin{table}[t]
\centering
\renewcommand{\arraystretch}{1.2}
\resizebox{\columnwidth}{!}{
\begin{tabular}{l|ccc}
\toprule
\multirow{2}{*}{\textbf{Data}} & \multicolumn{3}{c}{\textbf{MNLI}}                                         \\ \cline{2-4} 
                  & \multicolumn{1}{c}{dev-m} & \multicolumn{1}{c}{dev-mm} & HANS  \\ \hline
\model (GPT-2 Large)                & 85.17	 & 85.05	& 71.67  \\
\model (LLaMA-7b)               &  \textbf{85.64}	& \textbf{85.81}	& \textbf{72.78}  \\
\bottomrule
\end{tabular}%
}
\caption{Results of \model with GPT-2 Large and LLaMA-7b on MNLI.}
\label{tab:lora}
\end{table}

\section{Related Work}
Our related work primarily focuses on two categories of methods: model-centric and dataset refinement methods.
\subsection{Model-centric Data Debiasing Methods}
Numerous previous studies have adopted model-centric approaches to address biases in NLU models. 
Informed by their deep understanding of task-specific biases, they introduce innovative model architectures and training objectives aimed at preventing models from exploiting these biases.
For instance, \citet{belinkov-etal-2019-adversarial} introduce an adversarial architecture specifically designed to mitigate hypothesis-only bias \cite{gururangan-etal-2018-annotation}, while \citet{stacey-etal-2020-avoiding} enhance debiasing by employing multiple adversarial classifiers. Furthermore, there exists a complementary line of research focus on debiasing the model by down-weighting the importance of biased samples during training. The typical methods include example re-weighting (Reweight)  \cite{feversymm2019}, confidence regularization (Conf-reg) \cite{utama-etal-2020-mind, du2021towards}, and products-of-experts (POE) \cite{clark-etal-2019-dont, HeZW19_POE, karimi-mahabadi-etal-2020-end}. Typically, these methods follow a two-stage paradigm. In the first stage, a bias-only model is trained, either automatically \cite{utama-etal-2020-towards, geirhos2020shortcut,Sanh2021_ICLR} or by leveraging prior knowledge about the bias \cite{clark-etal-2019-dont,HeZW19_POE,belinkov-etal-2019-adversarial}. Then, in the second stage, the output of the bias-only model is utilized to adjust the loss function of the debiased model. Recently, \citet{lyu2022feature} propose a novel approach using contrastive learning to capture the dynamic influence of biases and effectively reduce biased features, offering an alternative perspective on addressing bias in NLU models. \citet{derc2023} observe that lower layers in Transformer models tend to capture biased features. They introduce the residual connection to integrate low-layer representations with top-layer ones, thus minimizing biased feature impact on the top layer.

\subsection{Dataset Refinement}
Several studies have explored generative data augmentation methods to enhance the model robustness in various domains. \citet{lee2021crossaug} train a generator to generate new
claims and evidence for debiasing fact verification
datasets like FEVER. \citet{ross2022} introduce TAILOR, a semantically-controlled perturbation method for data augmentation based on some manually defined perturbation strategies. \citet{wu2022generating} identify a set of biased features by \textit{z}-statistics, and then adjust the distribution of the generated samples by post-hoc filtering to remove the generated samples with biased features.

Unlike these approaches, our framework does not require data analysis to define biased features or manual perturbation rules, and hence achieves better generalizability.



\section{Conclusions}

In this work, we propose \model, an iterative dataset refinement framework for debiasing NLU models. Under this framework, we train a shallow model to quantify the bias degree of samples, and then iteratively generate pseudo samples with fewer biased features, which can be used to debias the model via retraining. We also incorporated two strategies to enhance the diversity of generated pseudo samples, further improving model performance.
On extensive experiments of two tasks, \model consistently shows superior performance compared to baseline methods. Besides, \model can better handle unknown biased features and has good compatibility with larger language models. 

In the future,
we will explore the compatibility of IBADR with other large language models, such as GPT4 \cite{DBLP:journals/corr/abs-2303-08774}.
\section*{Limitations}
The limitations of this framework are the following aspects: (i) Despite filtering the pseudo samples with low model confidence, \model might still produce pseudo samples with incorrect labels, which limits the model performance; (ii) We only conduct experiments on NLU tasks, neglecting the exploration of its applicability to a wider range of tasks.

\section*{Ethics Statement}
This paper proposes a dataset refinement framework that aims to adjust dataset distributions in order to mitigate data bias. 
All the datasets used in this paper are publicly available and widely adopted by researchers to test the performance of debiasing frameworks. Additionally, this paper does not involve any data collection or release, thus eliminating any privacy concerns. Oveall, this study will not pose any ethical issues.

\section*{Acknowledgements}
We would like to thank the anonymous reviewers for their insightful comments and suggestions. This research is supported by National Natural Science Foundation of China (No. 62276219) and Natural Science Foundation of Fujian Province of China (No. 2020J06001).

\bibliography{anthology,custom}
\bibliographystyle{acl_natbib}

\appendix


\end{document}
