%%
%% This is file `sample-sigconf.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `sigconf')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-sigconf.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%%%% Proceedings format for most of ACM conferences (with the exceptions listed below) and all ICPS volumes.
\documentclass[sigconf]{acmart}
%%%% As of March 2017, [siggraph] is no longer used. Please use sigconf (above) for SIGGRAPH conferences.

%%%% Proceedings format for SIGPLAN conferences 
% \documentclass[sigplan, anonymous, review]{acmart}

%%%% Proceedings format for SIGCHI conferences
% \documentclass[sigchi, review]{acmart}

%%%% To use the SIGCHI extended abstract template, please visit
% https://www.overleaf.com/read/zzzfqvkmrfzn

%%
%% \BibTeX command to typeset BibTeX logo in the docs


\usepackage{algorithm}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{comment}
\usepackage{amsfonts}
\usepackage{xcolor}
\usepackage{multirow}
\usepackage{verbatim}
%\usepackage{hyperref} 
\usepackage{multicol}
\usepackage{subfigure}
\usepackage{extarrows}
%\usepackage[top=2cm, bottom=2cm, left=2cm, right=2cm]{geometry}  
\usepackage{algorithm}  
\usepackage{algorithmicx}  
\usepackage{algpseudocode}  
\usepackage{amsmath} 
\usepackage{color}
\definecolor{ourdarkgreen}{RGB}{84,130,53}
\definecolor{ourdarkblue}{RGB}{68,114,195}
\definecolor{}{RGB}{255,240,245}

\usepackage{listings}
\usepackage{color}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
  language=Python,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}


\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\copyrightyear{2021} 
\acmYear{2021} 
\setcopyright{acmcopyright}\acmConference[IJCKG'21]{The 10th International Joint Conference on Knowledge Graphs}{December 6--8, 2021}{Virtual Event, Thailand}
\acmBooktitle{The 10th International Joint Conference on Knowledge Graphs (IJCKG'21), December 6--8, 2021, Virtual Event, Thailand}
\acmPrice{15.00}
\acmDOI{10.1145/3502223.3502237}
\acmISBN{978-1-4503-9565-6/21/12}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}

%%
%% end of the preamble, start of the body of the document source.
\begin{document}
%\settopmatter{printacmref=false} \setcopyright{none}
%\renewcommand \footnotetextcopyrightpermission[1]{} \pagestyle{plain}
 
%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{Normal vs. Adversarial:
Salience-based Analysis of Adversarial Samples for Relation Extraction}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.

\author{
Luoqiu Li$^{1,2*}$, 
Xiang Chen$^{4*}$, 
Zhen Bi$^{1,2*}$, 
Xin Xie$^{1,2*}$, 
Shumin Deng$^{1,2}$, 
Ningyu Zhang$^{1,2\star}$, \\
Chuanqi Tan$^{3}$, 
Mosha Chen$^{3}$, 
Huajun Chen$^{1,2\star}$
}
% \author{Ningyu Zhang$^{1,2}$*, Qianghuai Jia$^{3}$*, Shumin Deng$^{1,2}$*, Xiang Chen$^{1,2}$, Hongbin Ye$^{1,2}$, Hui Chen$^{3}$, Huaixiao Tou$^{3}$, Gang Huang$^{4}$, Zhao Wang$^{1}$,  Nengwei Hua$^{3}$, Huajun Chen$^{1,2}\dagger$}

\affiliation{
$^1$Zhejiang University\country{China} \& AZFT Joint Lab for Knowledge Engine\country{China}
$^3$Alibaba Group\country{China}
}
\affiliation{
$^2$Hangzhou Innovation Center\country{China}, Zhejiang University\country{China},
}

\email{
{luoqiu.li,xiang_chen,bizhen_zju,xx2020,231sm,zhangningyu,huajunsir}@zju.edu.cn, 
}
\email{
{chuanqi.tcq,chenmosha.cms}@alibaba-inc.com
}
%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Luoqiu Li, et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
 
\begin{abstract}
Recent neural-based relation extraction approaches, though achieving promising improvement on benchmark datasets, have reported their vulnerability towards adversarial attacks. Thus far, efforts mostly focused on generating adversarial samples or defending adversarial attacks, but little is known about the difference between normal and adversarial samples. In this work, we take the first step to leverage the salience-based method to analyze those adversarial samples. We observe that salience tokens have a direct correlation with adversarial perturbations. We further find the adversarial perturbations are either those tokens not existing in the training set or superficial cues associated with relation labels. To some extent, our approach unveils the characters against adversarial samples. We release an open-source testbed, ``\emph{DiagnoseAdv}''\footnote{The code and dataset are available in \url{https://github.com/zjunlp/DiagnoseAdv}.}, for future research purposes.
 
%Recent neural-based relation extraction approaches, though achieving promising improvement on benchmark datasets, have reported their vulnerability towards adversarial attacks. Thus far, efforts mostly focused on generating adversarial samples or defending adversarial attacks, but little is known about the difference between normal and adversarial samples. In this work, we take the first step to leverage the salience-based method to analyze those adversarial samples. We observe that salience tokens have a direct correlation with adversarial perturbations. We further find the adversarial perturbations are either those tokens not existing in the training set or superficial cues associated with relation labels. To some extent, our approach unveils the characters against adversarial samples. We release an open-source testbed, ``\emph{DiagnoseAdv}''\footnote{Work in Progress. The code and dataset are available in \url{https://anonymous.4open.science/r/25d76ae5-2dde-4c70-aa2d-cd0df40a9e66/}.}, for future research purposes.


%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\noindent\let\thefootnote\relax\footnotetext{
$*$ Equal contribution and shared co-first authorship. \\
$\star$ Corresponding author.
}
\end{abstract}
%%x
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%

\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10002951.10003317.10003347.10003352</concept_id>
       <concept_desc>Information systems~Information extraction</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Information systems~Information extraction}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{Adversarial Sample; Relation Extraction; Knowledge Graph}

%% A "teaser" image appears between the author and affiliation
%% information and the body of the document, and typically spans the
%% page.
\iffalse
\begin{teaserfigure}
  \includegraphics[width=\textwidth]{sampleteaser}
  \caption{Seattle Mariners at Spring Training, 2010.}
  \Description{Enjoying the baseball game from the third-base
  seats. Ichiro Suzuki preparing to bat.}
  \label{fig:teaser}
\end{teaserfigure}
\fi
%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle



\section{Introduction} 
Relation Extraction (RE), aiming to extract the relation between two given entities based on their related context, is an important task for knowledge graph construction \cite{DBLP:conf/www/ZhangDSCZC20} which can benefit widespread domains such recommendation system \cite{DBLP:journals/jmpt/JiaZH20}, healthcare system \cite{DBLP:journals/corr/abs-2008-10813,DBLP:conf/emnlp/ZhangDLCZC20}, stock prediction \cite{DBLP:conf/www/DengZZCPC19} and so on. 
Previous neural-based models \cite{DBLP:conf/coling/ZengLLZZ14,zhang-etal-2018-attention,DBLP:conf/naacl/ZhangDSWCZC19,DBLP:conf/wsdm/DengZKZZC20,DBLP:conf/coling/LiWZZYC20,DBLP:journals/corr/abs-2009-07022,DBLP:conf/emnlp/ZhangDBYYCHZC20,DBLP:journals/corr/abs-2009-09841,DBLP:conf/coling/YuZDYZC20,DBLP:journals/corr/abs-2009-06207} have achieved promising performance on benchmark datasets, yet they are vulnerable to adversarial examples \cite{DBLP:conf/aaai/JinJZS20,DBLP:journals/tist/ZhangSAL20,DBLP:journals/corr/abs-2009-06206}. 

The study of adversarial examples and training ushered in a new era to understand and improve natural language processing (NLP) models.
However, recent approaches mainly focus on generating adversarial examples \cite{DBLP:conf/ndss/LiJDLW19,DBLP:conf/sp/GaoLSQ18,DBLP:conf/ijcai/0002LSBLS18} or defending adversarial attacks \cite{DBLP:conf/wsdm/EntezariADP20,DBLP:conf/cvpr/TheagarajanCBZ19}, the major difference between normal and adversarial samples is still not well-understood. 
Note that understanding adversarial examples can figure out missing connections of RE models and inspire important future studies \cite{DBLP:journals/tacl/BelinkovG19}. 
To this end, we formulate the following interesting research questions:  
\begin{quote}
    \emph{1. What is the difference between normal and adversarial samples?}\\
    \emph{2. What is the reason that adversarial examples mislead the prediction?}
\end{quote}
 
Motivated by this, we leverage integrated gradients \cite{DBLP:conf/icml/SundararajanTY17} to analyze the adversarial samples for RE. 
Firstly, we observe that salience tokens have a direct correlation with adversarial perturbations.
We then analyze the salience distribution of normal and adversarial samples and find that these salience distributions change slightly (\S~\ref{sec1}). 
Secondly, we conduct experiments to probe reasons for misclassification and find that the salience tokens of adversarial samples are either not existing in the training set or superficial cues associated with relation labels (\S~\ref{sec2}). 
In summary, our main contributions include:

\begin{itemize}
    \item To the best of our knowledge, we are the first to leverage salience-based analysis for adversarial samples in NLP, which provides a new perspective of understanding the model robustness. 
    \item We propose a simple yet effective method to probe adversarial samples with salience analysis and observe new findings that may promote future researches. 
    \item We provide an open-source testbed, ``\emph{DiagnoseAdv}'',  for future research purposes. Our framework can be readily applied to other NLP tasks such as text classification and sentiment analysis. 
\end{itemize}


\section{Analyzing Adversarial Samples for RE}
\subsection{Setup}
RE is usually formulated as a sequence classification problem. 
Formally, let $X=\left\{x_{1}, x_{2}, \ldots, x_{L}\right\}$ be an input sequence, $h,t \in X$ be two entities, and $Y$ be the output relations. 
The goal of this task is to estimate the conditional probability, $P(Y|X) = P(y|X,h,t)$

In this paper, we respectively leverage the pre-trained BERT \cite{bert} and MTB \cite{baldini-soares-etal-2019-matching} as the target model. 
Certainly, other strong models (e.g., SpanBERT \cite{DBLP:journals/tacl/JoshiCLWZL20} and XLNet \cite{DBLP:conf/nips/YangDYCSL19}) can also be leveraged. 
We preprocess the sentence, $\mathbf{x}=$ $\{w_1,$ $w_2,$ $h,$ $\dots,$ $t$, $\dots$,$w_L\}$, for the input form of BERT: $\mathbf{x}=$ $\{$[CLS]$,$ $w_1,$ $w_2,$ [E1], $h,$ [/E1], $\dots,$ [E2], $t,$ [/E2],..., $w_L,$ [SEP]$\}$, where $w_i, (i \in  [1, n])$ refers to each word in a sentence and $h$ as well as $t$ are head and tail entities, respectively. [E1], [/E1], [E2], and [/E2] are four special tokens used to mark the positions of the entities. 
Our approach can be readily applied to other classification tasks such as text classification and sentiment analysis. 

\subsection{Entity-aware Adversarial Attack}
We introduce an \textbf{entity-aware} adversarial attack method for RE in this section, where entities in original samples should not be changed during the adversarial attack.
Given a set of $N$ instances, $\mathcal{X}= \{X_1, X_2,\dots, X_N\}$ with a corresponding set of labels, $\mathcal{Y}=\{Y_1, Y_2,\dots, Y_N$\}, we have a RE model trained via the input $\mathcal{X}$ and $\mathcal{Y}$, which satisfies the formula $ \mathcal{Y}  = RE(\mathcal{X})$.

The adversarial example $X_{\mathrm{adv}}$ for each sentence $X \in \mathcal{X}$  should conform to the requirements as follows: 
\begin{equation}
\small
RE\left(X_{\mathrm{adv}} \right) \neq RE(X), \text { and } \operatorname{Sim}\left(X_{\mathrm{adv}}, X\right) \geq \epsilon,
\label{eq:ad_requirement}
\end{equation}
where $\mathrm{Sim}$ is a similarity function  and $\epsilon$ is the minimum similarity between the original and adversarial examples.
Note that $X_{\mathrm{adv}}$ should have the same entity pair as $\mathcal{X}$, thus, we constrain the entity token from being perturbed and extend both score-based adversarial attack approaches: TextFooler \cite{DBLP:conf/aaai/JinJZS20}, PWWS \cite{DBLP:conf/acl/RenDHC19}, and a gradient-based method: HotFlip \cite{DBLP:conf/acl/EbrahimiRLD18} in our experiment.
Other attack methods such as SememePSO \cite{zang-etal-2020-word}, TextBugger \cite{DBLP:conf/ndss/LiJDLW19}, UAT \cite{wallace-etal-2019-universal} can also be leveraged.

\subsection{Salience-based Analysis}
We leverage integrated gradients \cite{DBLP:conf/icml/SundararajanTY17} (IG) to analyze the identify inputs relevant to the prediction.
Attention-based attribution \cite{DBLP:conf/emnlp/WiegreffeP19} is not adopted as \citet{bastings-filippova-2020-elephant} point out saliency methods are more suitable than attention mechanism in providing faithful explanations. \citet{attention-not-commonsense} also notice that attention weights are insufficient when investigating the behavior of the attention head.
Among the saliency methods, the IG method is a variation from the gradient method that assigns importance by computing gradients of the output w.r.t. the input. IG outperforms simple gradient by dealing with the gradient \textit{saturation} problem that gradients may get close to zero when the function is well-fitted. Given an input sentence's embeddings $\mathbf{x}=\left\langle\mathbf{x}_{1}, \ldots, \mathbf{x}_{n}\right\rangle$ with $\mathbf{x}_i$ being embedding of the $i$-th input token, and a model $F$, we compute:
\begin{equation}
\small
\rm{IG}(\mathbf{x}_i)=\frac{1}{m} \sum_{j=1}^{m} \nabla_{\mathbf{x}_{i}} F\left(\mathbf{b}+\frac{j}{m}\left(\mathbf{x}-\mathbf{b}\right)\right) \cdot\left(\mathbf{x}_{i}-\mathbf{b}_{i}\right),
\end{equation}
where $\mathbf{b}$ is a baseline value, which is an all-zeros vector in our experiment. By averaging over gradients with linearly interpolated inputs between the baseline and the original input $\mathbf{x}$ in $m$ steps, and taking the dot product of the averaged gradient with the input embedding $\mathbf{x}_i$ minus the baseline, we get IG vectors for input tokens. In our experiment, we then use the norm of IG vectors as tokens' attribution scores.

\begin{figure*}[h] \centering
  \includegraphics[width=1\textwidth]{figs/1.pdf}
\caption{Visualization of two types of how salience scores interact with perturbed tokens between normal samples and adversarial samples in TACRED.
The \emph{-\ -\ -} and \emph{+\ +\ +} signs mark perturbed tokens, representing token deletion in the original sample and insertion in the adversarial sample, respectively.}
\label{case1}
\end{figure*}
\section{Experiments}

We conduct experiments on two benchmark datasets: Wiki80\footnote{\url{https://github.com/thunlp/OpenNRE}} \cite{DBLP:conf/emnlp/HanZYWYLS18} and TACRED\footnote{\url{https://nlp.stanford.edu/projects/tacred/}} \cite{DBLP:conf/emnlp/ZhangZCAM17}. 
The Wiki80 dataset consisted of 80 relations, each having 700 instances. TACRED  is a large-scale RE dataset covering 42 relation types with 106,264 sentences. We provide an online GoogleColab for reproducibility\footnote{\url{https://colab.research.google.com/drive/1d4ayfzV8wqmGz0AxA1iLORfrD3JtbfYJ?usp=sharing}}.

% \subsection{Normal Samples vs. Adversarial Samples}
\subsection{What's Changed in Normal Samples?}
\label{sec1}
We conduct adversarial attacks to RE models as shown in Table \ref{adv_res}. We notice more adversarial samples are generated on the BERT model, indicating less vulnerability; among all three methods, HotFlip is most inefficient with success rates lower than 10\%.
To address \textbf{Question 1}, we leverage a token matching algorithm  to explore connections between the original and adversarial samples.

\begin{table}[h]
   %\fontsize{8}{10}\selectfont 
   \centering 
\resizebox{\linewidth}{!}{
\begin{tabular}{c|c|c}
\toprule
Model& \textbf{Wiki80} & \textbf{TACRED}\\
\midrule
BERT (Origin)&55,193/86.2 &99,008/67.5 \\
MTB (Origin)&55,225/90.3 &98,245/68.7 \\
 \midrule
BERT (HotFlip)&4,819/8.73\% &4,953/5.00\% \\
BERT (PWWS)&17,742/32.15\% &27,476/27.75\% \\
BERT (TextFooler)&26,774/48.51\% &34,892/35.24\% \\
 \midrule
MTB (HotFlip)&4,655/8.43\% &3,868/3.94\% \\
MTB (PWWS)&16,868/30.54\% &21,692/22.08\% \\
MTB (TextFooler)&25,969/47.02\% &25,751/26.21\% \\
 \bottomrule
\end{tabular}
}
\caption{Adversarial attack results from Wiki80 and TACRED dataset. The first two rows show numbers of correctly predicted samples and test performance (accuracy for Wiki80 and micro F1 for TACRED) of BERT or MTB model on two datasets, and the following rows indicate numbers of adversarial samples generated / success rate of adversarial attack with each (model, adversarial method) pair on each dataset.}
  \label{adv_res}
\end{table}
At sentence level, we have summarized two types of adversarial samples in Figure \ref{case1}: 1) the first type involves perturbations of $n$ tokens with highest salience scores in the original samples (except the irreplaceable entity tokens), while 2) the other type consists of samples in which no tokens with top salience scores are perturbed in these samples ($n=3$ in our experiment).
The ratio of samples in the first type greatly exceeds the second one among different adversarial methods on each dataset.
\begin{figure}[h] \centering
  \includegraphics[width=0.50\textwidth]{figs/2.pdf}
\caption{Salience score changes of perturbed positions during the TextFooler attack in Wiki80. The X and Y-axis coordinates stand for salience scores of perturbed positions in original samples and adversarial samples.}
\label{normal_adv}
\end{figure}

\begin{figure}[h] \centering
  \includegraphics[width=0.50\textwidth]{figs/3.pdf}
\caption{Tokens' distribution and perturbation ratio along salience scores of Wiki80.}
\label{token_dist}
\end{figure}
At a finer-grained token level, we explore salience scores of tokens at perturbed positions as shown in Figure \ref{normal_adv}. 
Each point represents a perturbed position, whose X-axis and Y-axis coordinate stand for its salience score in the original sample and the adversarial sample, respectively. Most points scatter along the diagonal $y=x$, indicating the stability of tokens' influence on predictions before and after being perturbed.
Colors of points indicate one largest cluster around (0.05, 0.05) and the second-largest cluster around (1, 1).
This phenomenon can be explained by Figure \ref{token_dist}, which reveals the distribution of all tokens in the original samples whose salience scores are mostly around 0.05 and 1.0.
It also reveals that although above 2/3 samples in the original sample involve perturbations of tokens with the top salience scores s, most perturbed tokens have low salience scores in token-level. However, from the perturbation ratio curve in Figure \ref{token_dist},  tokens with higher salience scores are more likely to be perturbed.

In conclusion, we observe the strong correlation between perturbations in the adversarial samples and high salience scores in the original samples, which is intuitive as high salience scores reflect tokens' impact on the model's predictions, perturbing those tokens are likely to change the predictions. We also argue that current adversarial methods are inefficient in RE, as they perturb many low-salience tokens in the original samples.

\subsection{Why MisClassified?}
% \subsection{What's Changed in Adversarial Samples?}
\label{sec2}
To address \textbf{Question 2} and further analyze why the model predicts differently with few perturbations, we look into the perturbed tokens in the adversarial samples.
\iffalse
\begin{figure}[H] \centering
  \includegraphics[width=0.45\textwidth]{figs/confidence.png}
\caption{The distribution of confidence drop.}
\label{confidence}
\end{figure}
\fi
\begin{table}[t]
    \centering
    \small
    \begin{tabular}{p{0.93\linewidth}}
    \toprule
          %\multicolumn{1}{c}{\textbf{C+M}}\\
    %\midrule
     Actress \textbf{Mia Farrow}  \textit{\color{red}had} \textbf{Vidal Sassoon} give her the look when she married Frank Sinatra in 1966, and she also wore it in her 1968 film ``rosemary's baby.''\\
    
         \textbf{\color{ourdarkgreen}Label}: \textbf{\texttt{no\_relation}}\\
        \textbf{\color{ourdarkblue}Prediction}: \textbf{\texttt{no\_relation}}\\
        
        \specialrule{0em}{4pt}{4pt}


        Actress \textbf{Mia Farrow}  \textit{\color{red}birth} \textbf{Vidal Sassoon} give her the look when she married Frank Sinatra in 1966, and she also wore it in her 1968 film ``rosemary's baby.''\\
        
         \textbf{\color{ourdarkgreen}Label}: \textbf{\texttt{no\_relation}}\\
        \textbf{\color{ourdarkblue}Prediction}: \textbf{\texttt{per:parents}}\\
        
    \bottomrule
    \end{tabular}
    \caption{Predictions on normal (above) and adversarial samples (bellow), where \textbf{bold} tokens are entities and {\color{red}red} represents perturbed tokens. We can observe that those perturbed tokens have superficial cues associated with corresponding relation labels.}
    \label{case2}
   % \vspace{-1em}
\end{table}

We manually examine perturbed tokens with high salience scores in the adversarial samples and observe a high ratio of superficial association between the predictions and the perturbed tokens, i.e., the model makes a wrong prediction upon seeing a frequent co-word. For example, as shown in Table \ref{case2}, the perturbed token \emph{birth} has a spurious correlation with the predicted label \emph{per:parents} in train samples, thus leading to the misclassification. We have examined 3,868 adversarial samples in TACRED (MTB, HotFlip). Such association accounts for 2,248 (58.12\%) adversarial samples, reflecting that neural networks tend to capture co-occurrence information between the token and label while ignoring low-frequency but important causal information. We argue that such artifacts and spurious correlation in the data mainly mislead the classification of the adversarial samples \cite{han-etal-2020-explaining}.

We also notice around 40\% adversarial samples contain perturbed tokens that do not appear in the training set, which leads to the input being Out-Of-Distribution (OOD). 
% Although it is much alleviated by word-piece tokenization, 
We also abserve that the OOD problem results are accompanied by a decrease in confidence, revealing that OOD problem may be annother minor reason for misclassification.

\subsection{Extra Statistics of Adversarial Samples}
%In this section, we present extra statistics of generated adversarial samples as follows:
\begin{table}[h]
%   \fontsize{8}{10}\selectfont 
   \centering 
\resizebox{\linewidth}{!}{
\begin{tabular}{c|c|c|c|c}
\toprule
Model& \textbf{Avg. Perturb} &\textbf{\% Salience} &\textbf{\% OOD} &\textbf{Avg. Confidence} \\
\midrule
BERT (HotFlip) & 6.72 & 91.99 & 49.47 & -0.24\\
BERT (PWWS) & 4.42 & 91.15 & 41.05 & -0.25\\
BERT (TextFooler) & 3.72 & 83.66 & 42.28 & -0.29\\
\midrule
MTB (HotFlip) & 6.65 & 91.69 & 48.46 & -0.28\\
MTB (PWWS) & 4.31 & 90.66 & 39.63 & -0.30\\
MTB (TextFooler) & 3.66 & 83.39 & 40.92 & -0.33\\
 \bottomrule
\end{tabular}
}
\caption{Extra statistics of Wiki80 adversarial samples.}
  \label{adv_res_wiki}
\end{table}

\begin{table}[h]
%   \fontsize{8}{10}\selectfont 
   \centering 
\resizebox{\linewidth}{!}{
\begin{tabular}{c|c|c|c|c}
\toprule
Model& \textbf{Avg. Perturb} &\textbf{\% Salience} &\textbf{\% OOD} &\textbf{Avg. Confidence} \\
\midrule
BERT (HotFlip) & 6.70 & 64.75 & 50.80 & -0.17\\
BERT (PWWS) & 4.70 & 74.20 & 40.50 & -0.27\\
BERT (TextFooler) & 4.69 & 63.48 & 54.88 & -0.36\\
\midrule
MTB (HotFlip) & 6.86 & 68.95 & 51.16 & -0.16\\
MTB (PWWS) & 4.72 & 79.17 & 41.50 & -0.25\\
MTB (TextFooler) & 4.67 & 71.87 & 53.82 & -0.32\\
 \bottomrule
\end{tabular}
}
\caption{Extra statistics of TACRED adversarial samples.}
  \label{adv_res_tacred}
\end{table}

In the Table \ref{adv_res_wiki} and \ref{adv_res_tacred}, the column ``Avg. Perturb" refers to average token perturbations from original samples, ``\% Salience" refers to the ratio of adversarial samples involving perturbations of relatively high salience scores (top 3 highest except the entity tokens), ``\% OOD" means ratio of samples containing Out-Of-Distribution tokens, and``Avg. Confidence" refers to the average decrease of prediction confidence between adversarial samples and of original samples (minus values mean lower confidence in adversarial samples).


% We filter those adversarial examples with high confidence (confidence $\ge$ 0.8).
% Then, we analyze the correlation of salience tokens and relation labels.
% We find that those salience tokens have a high correlation with those labels. 
% We find that nearly 80\% of the high confidence adversarial samples have this phenomenon. 
% Note that neural networks tend to capture co-occurrence information between the token and label while ignoring low-frequency but important causal information. 
% We argue that such artifacts in the data mislead the classification \cite{han-etal-2020-explaining}. 

% We further analyze those adversarial examples with low confidence. We observe that 70\% of perturbed tokens in adversarial samples do not exist in the train set. We think those tokens may lead to the input being Out-of-Distribution (OOD), which lowers the confidence to change the model's predictions. 

% In conclusion, we believe spurious correlation is one main reason for misclassification of the adversarial samples, and we regard the OOD problem as a weaker factor for decrease in model's prediction confidence on adversarial samples.

\section{Conclusion}
We introduce the entity-aware adversarial attack for Relation Extraction, and leverage the salience-based analysis of adversarial samples. We observe that correlation between high salience scores with token perturbations, inspiring future works of salience-aware data augmentation. 
Furthermore, we identify two factors: spurious correlation and OOD as main reasons for adversarial misclassification.
Breaking down the spurious correlation with causal analysis may help defend adversarial attacks with better generalization. 
More future works should also be taken into consideration for those OOD samples. 
We regard this study as a small step towards the understanding of adversarial samples. 

\begin{acks}
This work is funded by NSFC91846204/NSFCU19B2027.
\end{acks}
%\section*{Broader Impact Statement}
%Neural networks have achieved great success in a wide range of NLP applications, such as machine translation, question answering, dialogue systems, etc. Despite their success, the wide adoption of neural networks in real-world missions is hindered by the security concerns of neural networks because slight, imperceptible perturbations are capable of causing incorrect behaviors of neural networks. Our work focuses on unveiling adversarial samples' characters, promoting developing more robust models, and benefit lots of real-world applications. 




 


%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{sample-base}


\end{document}
\endinput
%%
%% End of file `sample-sigconf.tex'.
