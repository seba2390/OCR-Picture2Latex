%!TEX root = main.tex
\section{An ADMM Algorithm for\\ Constrained Spatial Smoothing}
\label{sec:model}

The spatial sparse recovery problem \eqref{eq:prob0} is different from \eqref{eq:minSSR} from two aspects: the loss function and the additional constraints.  As a consequence, previous SSR based on the finite element analysis can not be directly applied. More advanced algorithm needs to be developed to cope with our new loss function with constraints. 

In this section, we propose to utilize the Alternating Direction Method of Multipliers (ADMM), originated in classical paper \cite{douglas1956numerical}, to decompose our constrained optimization problem into two sub-problems that can be solved effectively by SSR and Quadratic Programming (QP) respectively. 
%Alg.~\ref{alg:ADMM} presents an ADMM algorithm to learn our model parameters.

% \begin{algorithm}[t]
% \caption{Constrained Spatial Smoothing by ADMM}
% \label{alg:ADMM}

% \KwIn{The $m$ observed volume of base stations $\mathbf z = (z_1, ..., z_m)^{\sf T}$, smoothing parameter $\lambda$, penalty parameter $\beta$, initialize $\boldsymbol{\alpha} = \boldsymbol{\alpha}^0$, $\boldsymbol{f} = \boldsymbol{f}^0$. }

% \KwOut{Spatial field and parameters $\hat{f},\ \hat{\boldsymbol{\beta}}$. Estimation values on $n$ locations $\hat{\boldsymbol{f}} = \big(\hat{f}(\mathbf p_1), \ldots, \hat{f}(\mathbf p_n)\big)$.}

% \begin{algorithmic}[1]
% \FOR{$iter = 1,\ldots,\text{maxIter}$}
% 	\STATE Update $\boldsymbol{f}$ by solving \eqref{eq:g-QR} using Quadratic Programming.

% 	\STATE Update $\boldsymbol{g}$ by solving \eqref{eq:admm-f} using Spatial Spline Regression.

% 	\STATE Update $\boldsymbol{\alpha}$ according to \eqref{eq:ADMM-g-3}.
% \ENDFOR
% \end{algorithmic}
% \end{algorithm}

%Let $\{ \mathbf{p}_j \}_{j=1}^n$ be the set of given locations.
%Define $\mathbf{f}$ as
%\begin{equation}
%\mathbf f := (f(\mathbf{p}_1), \dots, f(\mathbf{p}_n))^{\sf T}
%\end{equation}
By introducing the following indicator function $\mathds{1}_{\mathbf{f}}$,
\begin{equation}
\mathds{1}_{\mathbf{f}} = \left\{
	\begin{array}{ll}
		0 & \quad\text{if $\mathbf{f} \geq 0$ and $\mathbf{z} = \mathbf{Af}$,}\\
		\infty & \quad\text{otherwise.}\\
	\end{array}
\right.
\end{equation}
the original problem \eqref{eq:prob0} is equivalent to
\begin{equation}
\label{eq:add-auxiliary}
\begin{split}
	\underset{f}{\mbox{minimize}} \quad 		
	& \lambda \int_\Omega(\nabla^2 f)^2\ud\mathbf p + \mathds{1}_{\mathbf{f}}
\end{split}
\end{equation}
where $\lambda$ is a parameter that controls the smoothness of $f$.

In order to split the convex optimization problem into two sub-convex problems we introduce an auxiliary variable $\mathbf g$ defined as
\begin{equation}
\mathbf g := (g(\mathbf{p}_1), \dots, g(\mathbf{p}_n))^{\sf T}
\end{equation}
The problem is transformed into the standard ADMM format,
\begin{equation}
\label{eq:add-auxiliary}
\begin{split}
	\underset{f}{\mbox{minimize}} \quad 		
	& \lambda \int_\Omega(\nabla^2 f)^2\ud\mathbf p + \mathds{1}_{\mathbf{g}}\\
	\mbox{subject to} \quad 
	& \mathbf f = \mathbf g,
\end{split}
\end{equation}
The \textit{augmented Lagrangian} for \eqref{eq:add-auxiliary} is
\begin{equation}
\label{eq:aug-Lagrangian}
\begin{split}
	{\mbox{minimize}} \quad
	\mathcal L_{\rho}(\mathbf f, \mathbf g, \boldsymbol{\alpha}) =
	& \lambda\int_\Omega(\nabla^2 f)^2\ud\mathbf p + \mathds{1}_{\mathbf{g}} \\
	& + \boldsymbol{\alpha}^{\sf T} ( \mathbf{g} -  \mathbf{f}) + \frac{\rho}{2} \| \mathbf{g} -  \mathbf{f} \|_{2}^{2},
\end{split}
\end{equation}
where
$\boldsymbol{\alpha} = (\alpha_1, ..., \alpha_n)^{\sf T}$ is the dual variable, and $\rho > 0$ is the penalty parameter in ADMM. Then the ADMM consists of the following iterations
\begin{align}
	\mathbf{f}^{k + 1} := &\ \underset{\mathbf f} {\mbox{argmin}} \ \mathcal L_{\rho} (\mathbf f, \mathbf g^{k+1}, \boldsymbol{\alpha}^k ) \label{eq:ADMM-g-1}\\
	\mathbf{g}^{k + 1} := &\ \underset{\mathbf g} {\mbox{argmin}} \ \mathcal L_{\rho} (\mathbf f^k, \mathbf g, \boldsymbol{\alpha}^k ) \label{eq:ADMM-g-2}\\
	\boldsymbol{\alpha}^{k + 1} := &\ \boldsymbol{\alpha}^{k} + \rho (\mathbf f - \mathbf g). \label{eq:ADMM-g-3}
\end{align}

For the $\mathbf f$-update step in each iteration, \eqref{eq:ADMM-g-2} is equivalent to
\begin{equation}\label{eq:admm-f}
	\underset{f}{\text{minimize}}\quad \big\| \left(\boldsymbol{\alpha}^{\sf T} + \rho \mathbf g^{\sf T}\right)/2 - \mathbf f \big\|_2^2 + \lambda\int_\Omega(\nabla^2 f)^2\ud\mathbf p,
\end{equation}
which is exactly the form of \eqref{eq:minSSR} with $h_j = \left(\alpha_j + \rho g(\mathbf p_j)\right)/2$ and $\mathbf w_j = 0$, thus can be solved efficiently by SSR. 
The penalty parameter $\lambda$ controls the smoothness of $f$ by putting little emphasis on the smoothness if it is small, and the estimated surface $f$ will be over fitted. If it is too big, the surface will be too smooth, which can cause underfitting.
%It should be noted that $\lambda$ is the penalty parameter which controls the smoothness of $f$. If it is small, we put little emphasis on the smoothness, and the estimated surface $f$ will be over fitted. If it is too big, the surface will be too smooth, which can cause underfitting.


For the $\mathbf g$-update step in each iteration, \eqref{eq:ADMM-g-1} is equivalent to
\begin{equation}
\label{eq:g-QR}
\begin{split}
	\underset{\mathbf g}{\mbox{minimize}} \quad 		
	& \frac{\rho}{2}\|\mathbf g\|_2^2 + (\boldsymbol{\alpha}^{\sf T} - \rho \mathbf f^{\sf T}) \mathbf g\\
	\mbox{subject to} \quad 
	& \mathbf g \geq 0,\\
	& \mathbf z= \mathbf{Ag},
\end{split}
\end{equation}
which is a convex problem that can be solve by Quadratic Programming (QP).

For the case with attributes, the algorithm does not require major changes. We just need to replace $\mathbf f$ by $\mathbf f + \mathbf W \boldsymbol{\beta}$ in \eqref{eq:admm-f}, where $\mathbf W := (\mathbf w_1, ..., \mathbf w_n)^{\sf T}$ represents the attributes and $\boldsymbol{\beta}$ is the corresponding contributions. 

Our proposed ADMM training algorithm is able to efficiently fit the spatial field and covariates for our constrained spatial sparse recovery problem. In $\mathbf g$-update step, we enforce the constraints by solving a constrained QP with no need to worry about smoothing; in $\mathbf f$-update step, we approximate the obtained $\mathbf g$ with a smooth $f$ using the SSR-based smoothing technique. In
this way, we decouple the  handling of smoothing and constraints which was not possible in pure SSR previously.


