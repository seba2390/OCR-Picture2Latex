


\section{Introduction} \label{sec:intro}

Evidence combination is a fundamental operation of statistical inference.
When we have multiple observation models for multiple data sets, with some model parameters appearing in more than one observation model, we have a multi-modular setting in which data sets identify modules.
The modules are connected in the graphical model for the joint posterior distribution of the parameters.
% In apparent contrast, the evidence informing parameters may be a single data set and a hierarchy of prior models in which parameters at different levels of the hierarchy are auxiliary variables.
% In our Bayesian setting this kind of evidence combination is not qualitatively different from data fusion: auxiliary variables can be thought of as missing data for higher levels of the hierarchy; generative models for missing data play the same role as the observation models in data fusion, and modules are identified with levels of the hierarchy.
% \cite{Carmona2020smi} and \cite{Yu2021variationalcut} analyse an archaeological data set using a model with this sort of structure, identifying modules as above.
% Collaboration of multi-disciplinary teams in large-scale analysis often amounts to evidence combination in a multi-modular setting.

Large-scale multi-modular models are susceptible to model contamination, as a hazard for misspecification in each module accumulates as modules are added.
If any module is significantly misspecified, it may undermine inference on the joint model \citep{Liu2009modularization}.
%Models are especially vulnerable to misspecification when the dimension of missing data is large and the auxiliary variables representing the missing data are correlated, as misspecification biases are reinforced by strong feedback from high dimensional latent parameters.
Model elaboration \citep{Smith1986,Gelman2014BDA3} may be impractical or at least very challenging.
%It is simply unclear what a suitable model would look like.
In this setting we may consider ``inference elaboration'', and turn to statistically principled alternatives to Bayesian inference such as Generalised Bayes \citep{Zhang2006,Grunwald2017a,Bissiri2016}.
Some modular inference frameworks allow the analyst to break up the workflow, whilst still implementing a valid belief update \citep{Bissiri2016,Nicholls2022smi}.
This is discussed in \cite{Nicholson2021covid} in the broader context of modular ``interoperability''.

Modular Bayesian Inference \citep{Liu2009modularization, Plummer2015cut,Jacob2017together, Carmona2020smi,Nicholls2022smi} addresses misspecification in a multi-modular setting by controlling feedback from misspecified modules (see \cref{sec:mod_bayes}).
%These methods assume that the analyst has identified the misspecified modules and allow inference to proceed ``safely'' without simply discarding these modules.
Recent applications of multi-modular inference note \citep[eg.][]{Nicholson2021covid, Teh2021covid} and demonstrate \citep[eg.][]{Carmona2020smi,Yu2021variationalcut,Styring2022urban} the potential benefits of partially down-weighting the influence of modules, rather than completely removing feedback.
%Following \cite{Jacob2017together}, different modified posterior distributions are called \emph{candidate posteriors}.
% We follow this assumption for the most part, but also give an extension which simultaneously identifies and protects against misspecification across multiple modules.
Modular Bayesian Inference is characterised by a modified posterior known as the \emph{Cut} posterior \citep{Plummer2015cut}. This removes feedback from identified misspecified modules. \emph{Semi-Modular} posteriors \citep{Carmona2020smi} interpolate between Bayes and Cut, controlling feedback using an influence parameter $\eta\in [0,1]$. This can be identified with the learning rate parameter in a power posterior \citep{Walker2001,Zhang2006,Grunwald2017a}.
The Cut and \acrshort*{smi} posteriors are valid belief updates in the sense of \citep{Bissiri2016}, and part of a larger family of valid inference procedures \citep{Nicholls2022smi}.

These approaches raise computational challenges due to intractable parameter-dependent marginal factors.
% Growing interest in Modular Bayesian Inference has motivated new methods to address the computational challenges of these modified posteriors.
In the case of Cut posteriors, nested Monte Carlo samplers are given in \cite{Plummer2015cut} and
\cite{Liu2020sacut} and
used in \cite{Carmona2020smi} to target \acrshort*{smi} posteriors.
These samplers suffer from double asymptotics, though work well in practice on some target posterior distributions.
\cite{Jacob2020couplings} give unbiased Monte Carlo samplers for the Cut posterior and \cite{Pompe2021cut} analyse the asymptotics of the Cut posterior and give two methods to target it: a Laplace approximation and Posterior Bootstrap.
%In concurrent independent work \cite{Yu2021variationalcut} give variational methods approximating the Cut posterior.
%They point out that this makes it possible to apply powerful candidate-posterior selection methods \citep{nott21-prior-check} to decide between Cut and Bayes.

Work to date on Modular Inference focuses on models with a small number of modules and a single pre-specified cut module.
We simultaneously adjust the contribution of multiple modules. This allows us to take an exploratory approach and ``discover'' the misspecified modules.
Dealing with multiple cuts and a vector of \emph{influence parameters} is challenging, as each additional ``cut'' increases the dimension of $\eta$ and the space of candidate posteriors.
One natural approach for selecting a candidate posterior is to take a grid of $\eta$-values, sample each distribution (Importance Sampling is not straightforward, as ratios of candidate posteriors are intractable) and evaluate a performance metric on each distribution.
This search strategy works for a single cut, but is already inefficient and quickly becomes cumbersome when the number of cuts increases.
% , partly due to the nested Monte-Carlo methods commonly used to target the posterior.

In this work, we give a novel variational framework for \acrshort*{smi} posteriors
which scales to handle multiple cuts.
The usual \acrfull*{elbo} training utility is intractable, due to the same parameter-dependent marginals that make \acrshort*{mcmc} sampling difficult.
Moreover, the resulting approximation does not meet the original objective of having controlled feedback between modules (see \cref{sec:vi_modular}).
Our solution takes a variational family with a pattern of conditional independence  between \emph{shared, extrinsic} and \emph{module-specific, intrinsic} parameters that matches the \acrshort*{smi} target, and uses the \emph{stop-gradient} operator to define a modified variational objective. The resulting variational framework gives good approximation, controllable feedback and end-to-end optimisation.
In parallel independent work, \cite{Yu2021variationalcut} give variational methods for Cut-posteriors.
Our approaches match at the Cut posterior: just as \acrshort*{smi} interpolates Cut and Bayes, so variational \acrshort*{smi} interpolates variational-Cut and variational-Bayes exactly.
% This is not trivial, due to the auxiliary parameters present in \acrshort*{smi}.


%Variational methods offer a convenient alternative to perform Modular Inference.
% A concrete aspect of \acrshort*{smi} posteriors exploitable under a variational approach is the fact that a small change in the rate of feedback, usually results in a small variation in the posterior.
% This type of continuity suggests that the variational solution for a given $\eta$ would provide valuable information for the solution of another neighbouring $\eta'$.

% Two challenges arise in variational methods for Modular Inference.
% First, the \acrfull*{elbo} associated with the \acrshort*{smi} (and Cut-model) posterior is intractable, complicating optimisation.
% This is due to the same terms in the posterior that make \acrshort*{mcmc} sampling difficult (see \cref{sec:vi_modular}).
% Second, the accuracy of the variational approximation and expresivity of the variational family is quite relevant here.
% One of the goals of Modular Inference is to correct for model misspecification, therefore we require expresive variational families for good uncertainty quantification of the modified posterior.

% We give a training strategy that fits the intractable \acrshort*{smi} posterior in a single end-to-end optimisation loop, avoiding (yet equivalent to) two-stage optimisation procedures.

% , this allow us to restrain the flow of gradients during optimisation in such a way that \emph{shared} parameters receive limited feedback from the suspect modules.

% We prove that our variational approximation has correct behaviour at two key reference points: at the ``fully-Bayes'' end of the \acrshort*{smi} with $\eta=1$, we recover the conventional variational approximation to the model,
% and at the ``Cut'' end with $\eta=0$, we show that the variational approximation retains the key property of removing all feedback from cut modules.
% We recover the two-stage approximation for the Cut posterior given in \cite{Yu2021variationalcut}.
% This seems neat, We side-step the intractability presented in \acrshort*{smi} and is the principled way to carry out variational inference in multi-modular settings.

One of the goals of \acrshort*{smi} is to correct for model misspecification, so it is important to get a good variational fit to the \acrshort*{smi}-posterior and not make matters worse with a poor approximation.
We leverage recent work on relatively expressive variational families using \acrfullpl*{nf} \citep{Rezende2015nf, Papamakarios2021normalizing}, as we get better uncertainty quantification than less expressive mean-field approximations.
In particular, we take Flow-based models with universal-approximation capabilities \citep[see][]{Huang2018flows,Durkan2019neural,Papamakarios2021normalizing} as our default variational families.
The conditional independence structure required by the \acrshort*{smi} posterior is achieved by defining the \emph{Conditioner} functions of the flow.

%Small changes of influence parameter $\eta$ results in a small variation in the candidate posterior.
We exploit the continuity of the \acrshort*{smi} posterior with varying $\eta$ and introduce the \acrfull*{vmp}, a variational approximation to the \emph{entire collection} of posteriors indexed by $\eta$, using a single set of parameters.
We train a function that takes $\eta$ as input and produces the variational parameters for the corresponding \acrshort*{smi} posterior.
We call this function the \acrshort*{vmp}-map.
The \acrlong*{vmp} is key to scalability (as illustrated in our example with 30 potential cuts in \cref{subsec:exp_rnd_eff}).
%The \acrshort*{vmp} is the ``output'' of our analysis.

The remaining task is to select an \acrshort*{smi} posterior ($ie$, $\eta$) for downstream analysis.
The performance metric deciding the level of influence will depend on the inferential goals.
Selection criteria \citep{wu-martin-20} developed for choosing the learning rate in the power posterior, such as matching information gain \citep{holmes-walker-17}, and predictive performance \citep{Vehtari2016,Jacob2017together,wu-martin-21} are relevant.
\cite{Yu2021variationalcut} leverage tractable variational distributions to compute calibrated test statistics \citep{nott21-prior-check} measuring evidence against Bayes and for Cut.
We use the \acrfull*{elpd} \citep{Vehtari2016}, which scores predictive performance.
Variational methods commonly achieve predictive accuracy comparable with \acrshort*{mcmc} despite the variational approximation \citep{Wang2019VariationalMisspecification} so this is a happy marriage.
We estimate the \acrshort*{elpd} using the WAIC \citep{Watanabe2012}. Fast sampling is available for the variational posterior density and this supports \acrshort*{elpd}-estimation for multiple cuts.

%We obtain accurate approximation to the posteriors in a test case with multimodal distributions and sharp variations along the $\eta$ space.

In summary, our contributions include:\\[-0.25in]
\begin{itemize}
  \item a variational framework for approximation of \acrshort*{smi} posteriors suitable for modular Bayesian inference;
  \item approximation of \acrshort*{smi} posteriors with \acrlongpl*{nf}, underlining the importance of flexible variational families;
  \item the \acrfull*{vmp}, a family of variational posteriors indexed by $\eta$ which approximates a family of \acrshort*{smi} posteriors using a single set of parameters;
  \item end-to-end training algorithms using the \emph{stop-gradient} operator;
        %\item we give an end-to-end training strategy for learning the \acrlong*{vmp};
  \item variational methods for identifying misspecified modules and modulating feedback which scale to handle multiple cuts;
  \item illustrations of the method on real and synthetic data.\\[-0.25in]
\end{itemize}
We provide code reproducing all results and figures \footnote{\url{https://github.com/chriscarmona/modularbayes}}.

\section{Modular Bayesian Inference} \label{sec:mod_bayes}

In order to fix ideas, we illustrate multi-modular inference using the model structure displayed in \cref{fig:toy_multimodular_model}. 
This structure is already quite rich, as more complex models may sometimes be reduced to this form by grouping together sub-modules into nodes appropriately.
Our methods extend straightforwardly to more complex models in a similar fashion to earlier work in this field.

\begin{figure}[!htb]
  \centering
  % \large
  \def\svgwidth{0.25\textwidth}
  \import{images/}{toy_smi_model.pdf_tex}
  \caption{
    Graphical representation of a simple multi-modular model.
    Grey circles denote unknown quantities to be inferred, and white boxes are fixed quantities.
    The dashed red line indicates cut feedback from the $Y$-module into the $Z$-module.
    The addition of the $\eta$ symbol indicates modulated feedback via \acrshort*{smi}.
  }
  \label{fig:toy_multimodular_model}
\end{figure}



This generic setting has two modules with data $Y=(Y_1,...,Y_n),\ Y\in \Y^n$ and $Z=(Z_1,...,Z_m),\ Z\in \Z^m$ and continuous parameters $\varphi\in \phispace$ and $\theta\in \thetaspace$ of dimension $p_\varphi$ and $p_\theta$ respectively.
The generative models for parameters and data are $p(Z\mid \varphi)\,p(\varphi)$ and $p(Y\mid \varphi,\theta)\,p(\varphi,\theta)$.
The Bayesian posterior for this model can be written
\begin{align}
  p(\varphi,\theta \mid Z, Y) & = p(\varphi \mid Z, Y) \; p(\theta\mid Y, \varphi ) \label{eq:bayespost} \\
                              & \propto p(\varphi,\theta, Z, Y) \label{eq:bayespost2}
\end{align}
where the last line is the natural form for further computation using the joint distribution
\begin{align}\label{eq:joint_toymodel}
  p(\varphi,\theta, Z, Y) = p(\varphi,\theta)\; p(Z\mid \varphi)\; p(Y\mid \varphi,\theta)
\end{align}
\Cref{eq:bayespost} is given for contrast with the Cut model and SMI below.
Note that,
\begin{equation}\label{eq:phi_post}
  p(\varphi \mid Z, Y)\propto p(\varphi) \; p(Z\mid \varphi) \; p(Y\mid \varphi)
\end{equation}
and
\[
  p(\theta\mid Y, \varphi )\propto p(\theta\mid \varphi)\; \frac{p(Y\mid \varphi,\theta)}{p(Y\mid \varphi)}
\]
with
\[
  p(Y\mid \varphi)=\int p(Y\mid \varphi,\theta)\; p(\theta\mid \varphi) \; d\theta.
\]
In \cref{eq:bayespost} the value of $\varphi$ informs $\theta$.
In \cref{eq:phi_post} the marginal likelihood $p(Y\mid \varphi)$ can be thought of as ``feedback'' of information from the $Y$ module into the $Z$-module \citep{Liu2009modularization, Plummer2015cut, Jacob2017together}.
Any remaining normalising constants depend only on the data $Y,Z$.

\subsection*{Cutting Feedback}

Several different methods have been proposed to bring the generative models together in a joint distribution for the parameters given data.
%
Besides Bayesian inference itself, these include Markov Melding \citep{Goudie2019melding} (which focuses on settings where priors conflict across shared parameters) and Multiple Imputation \citep{Meng1994imputation}, which discusses inference for ``uncongenial'' modules, relevant here.
%
\cite{Nicholson2021covid} discusses the broader concept of ``interoperability'' of models in multi-modular settings.
%
In this paper we focus on \acrfull*{smi} defined in \cite{Carmona2020smi} and Cut model inference \citep{Plummer2015cut}, which is a special case.

Cut-model inference has proven useful in many settings, including complex epidemic models for the Covid pandemic \citep{Teh2021covid,Nicholson2021covid} and modular models linking isotope analysis and fertiliser use in Archaeological settings \citep{Styring2017extensification}, pharmaco-kinetic and -dynamic models \citep{Lunn2009pkpd} in pharmacological analysis, and health affects and air pollution \citep{Blangiardo11}.

Suppose the generative model $p(Y\mid \varphi,\theta)\; p(\varphi,\theta)$ in the $Y$-module is misspecified via $p(\theta\mid \varphi)$ or $p(Y\mid \varphi,\theta)$.
%
We hope to get a more reliable estimate of $\varphi$ by ``cutting'' the feedback from this module into the $\varphi$-estimation.
%
This is indicated by the dashed red line in \cref{fig:toy_multimodular_model}. Operationally, we drop the factor $p(Y\mid \varphi)$. Following \cite{Plummer2015cut},
\begin{align} \label{eq:cut_posterior}
  p_{\cut}(\varphi,\theta \mid Y,Z) & = p(\varphi \mid Z) \; p(\theta \mid Y,\varphi)
  %& = \frac{ p(Z \mid \varphi) p(\varphi) }{ p(Z) } \frac{ p( Y \mid \varphi,  \theta ) \; p(\varphi,\theta) }{ p( Y,\varphi)_{\theta}} \nonumber   \\
  \\ &
  \propto \frac{p(\varphi, \theta, Y, Z)}{p(Y \mid \varphi) }.  \nonumber
\end{align}
Cutting feedback leaves the Cut posterior with the intractable factor $p(Y \mid \varphi)$.
Inference with a Cut-posterior is a two-stage operation which can be seen as Bayesian Multiple Imputation. In the first stage we \emph{impute} $\varphi\sim p(\cdot \mid Z)$. This distribution of imputed $\varphi$ values is passed to the second \emph{analysis} stage where $\varphi$ are treated as randomly variable ``imputed data'' alongside $Y$, informing $\theta\sim p(\cdot \mid Y, \varphi)$. Looking ahead to \acrshort*{smi}, this setup is shown graphically in \cref{fig:toy_multimodular_model_2stg}, where $\varphi$ is imputed on the left (appearing in a grey circle as a parameter) and then conditioned on the right (appearing in a white square like $Y$). In a Cut-posterior $\eta=0$, and the $\tilde\theta,Y$ elements of the graph on the left are absent.

The Cut model posterior is a ``belief update'', in the sense of \cite{Bissiri2016}.
It is a rule $\psi$ for updating a prior measure of belief, $p_0(\varphi,\theta)$ say, using a loss $l(\varphi,\theta;Y,Z)$ connecting data and parameter (the -ve log-likelihood is a cannonical loss) to determine a posterior belief measure $p_1$ say.
They write $p_1=\psi(l,p_0)$.
\cite{Bissiri2016} require belief updates $\psi(l,p_0)$ to be coherent: in our notation, if the data are all conditionally independent given the parameters, and $Y=(Y^{(1)},Y^{(2)})$ and $Z=(Z^{(1)},Z^{(2)})$ are arbitrary partitions of the data in each module into two sets, then we should arrive at the same posterior $\psi(l(\varphi,\theta;Y,Z),p_0)$ if we take all the data $(Y,Z)$ together or if we update the prior to an intermediate posterior using $(Y^{(1)},Z^{(1)})$ and then update that intermediate posterior using the rest of the data, $(Y^{(2)},Z^{(2)})$, that is,
\begin{equation}\label{eq:coherent_bissiri}
  \psi(l(\varphi,\theta;Y,Z),p_0)=\psi(l(\varphi,\theta;Y^{(2)},Z^{(2)}),\psi(l(\varphi,\theta;Y^{(1)},Z^{(1)}),p_0)).
\end{equation}
They show with some generality that a valid belief update must be a Gibbs posterior if it is to be coherent, that is,
\[
  \psi(l(\varphi,\theta;Y,Z),p_0)\propto \exp(-l(\varphi,\theta;Y,Z))\,p_0(\varphi,\theta).
\]
Bayesian inference is coherent because the corresponding loss $l_{\text{bayes}}=-\log(p(Y\mid \varphi,\theta))-\log(p(Z\mid \varphi))$ is additive for independent data.
\cite{Carmona2020smi} show that the belief update determined by the Cut-model posterior is coherent and \cite{Nicholls2022smi} show it is valid.
This is surprising, as the loss $l_{\cut}=l_{\text{bayes}}+\log(p(Y\mid \varphi))$ is not simply additive.
This holds because the ``prior'' appearing in the marginal $p(Y^{(2)}\mid Y^{(1)},\varphi)$ in the second belief update is the posterior from the first stage and not $p_0(\theta\mid \varphi)$.

The Cut posterior can also be characterised {\it via} a constrained optimisation \citep{Yu2021variationalcut}.
Consider the class of all joint densities,
\[
  \mathcal{F}_{\cut} = \{q(\varphi,\theta) : q(\varphi) = p(\varphi\mid Z)\},
\]
for which the $\varphi$-marginal $q(\varphi)$ equals $p(\varphi\mid Z)$.
Densities in $\mathcal{F}_{\cut}$ are candidate Cut posteriors.
\cite{Yu2021variationalcut} show that, among densities in $\mathcal{F}$, the Cut posterior in \cref{eq:cut_posterior} is the best approximation to the Bayes posterior as measured by \acrshort*{kl} divergence, that is,
\begin{equation}\label{eq:cut_constrained_optim}
  p_{\cut}(\varphi,\theta \mid Y,Z)=\arg\min_{q\in \mathcal{F}_{\cut}} \kl{q(\varphi,\theta)}{ p(\varphi,\theta\mid Y,Z)}.
\end{equation}
They use this characterisation to motivate a framework for variational approximation of the Cut posterior.
%(see \cref{sec:v_cut_detail} below).
Our motivation for variational \acrshort*{smi} starts from an equivalent characterisation of \acrshort*{smi}.

Statistical inference for the Cut posterior is challenging due to the marginal likelihood factor $p(Y\mid \varphi)$.
Several approaches have been suggested.
\cite{Plummer2015cut} gives a nested \acrshort*{mcmc} scheme: run \acrshort*{mcmc} targeting $p(\varphi\mid Z)$; for each sampled $\varphi$ a separate \acrshort*{mcmc} run targets $p(\theta\mid Y,\varphi)$; this yields $(\varphi,\theta)\sim p_{\cut}$, at least approximately.
Nested \acrshort*{mcmc} for Cut models suffers from double asymptotics but is adequate in some cases \citep{Styring2017extensification,Teh2021covid,Moss2022}.
A recent nested \acrshort*{mcmc} variant \citep{Liu2020sacut} shows efficiency gains for high dimensional targets.
An exact unbiased variant of \acrshort*{mcmc} based on coalescing coupled chains \citep{Jacob2020couplings} removes the double asymptotics of the nested sampler.

%\todo{In other ML domains? Modular Generative Model Multitask VAE?}

%Definition of Cut posterior in the Generalised Update of Beliefs framework \citep{Bissiri2016}.
%Recall the associated loss function and briefly mention how the update is order-coherent.

\subsection{Semi-Modular Inference}

\acrfull*{smi} \citep{Carmona2020smi} is a modification of Bayesian multi-modular inference which allows the user to adjust the flow of information between data and parameters in separate modules.
Cut models stop misspecification in one module from causing bias in others.
However, this often leads to variance inflation.
Semi-modular posteriors determine a family of candidate posterior distributions indexed by an influence parameter $\eta\in [0,1]$.
They interpolate between the Cut and Bayesian posteriors, expanding the space of candidate distributions and including Bayesian inference and Cut-model inference as special cases.

\subsubsection{Modulating feedback from data modules}

Cut models and \acrshort*{smi} are typically presented using models like \cref{fig:toy_multimodular_model} and cutting or modulating feedback from the $Y,\varphi,\theta$ module into the $Z,\varphi$ module. However, some effective applications of Cut models and \acrshort*{smi} cut or modulate feedback from modules that have no data \citep{Jacob2017together,Styring2017extensification,Carmona2020smi,Yu2021variationalcut,Styring2022urban}. We return to this below and in \cref{sec:prior_feedback}.

The \textbf{\acrshort*{smi} posterior} for the cut in \cref{fig:toy_multimodular_model} is defined as
\begin{equation} \label{eqn:smi_01}
  p_{\smi,\eta}(\varphi,\theta,\tilde\theta\mid Y,Z) = p_{\pow, \eta}(\varphi,\tilde\theta \mid Y,Z) p(\theta \mid \varphi,Y)
\end{equation}
where $p_{\pow, \eta}( \varphi , \tilde\theta \mid Z, Y )$ is the power posterior
\begin{align}
  p_{\pow, \eta}( \varphi , \tilde\theta \mid Z, Y ) & \propto p_{\pow, \eta}( \varphi , \tilde\theta, Z, Y )
  \intertext{with}
  p_{\pow, \eta}( \varphi , \tilde\theta, Z, Y )     & = p(Z\mid \varphi) p( Y \mid \varphi, \tilde \theta )^\eta \;  p(\varphi,\tilde\theta). \label{eq:powjoint_toymodel}
\end{align}
Taking $\eta=1$ in the $\eta$-smi posterior and integrating over $\tilde\theta$  gives the conventional posterior in \cref{eq:bayespost} so that $p_{smi,1}(\varphi,\theta\mid Y,Z)=p(\varphi,\theta\mid Y,Z)$, while $\eta=0$ gives the Cut posterior in \cref{eq:cut_posterior}, with $p_{smi,0}(\varphi,\theta\mid Y,Z)=p_{\cut}(\varphi,\theta\mid Y,Z)$.

The \acrshort*{smi}-posterior in \cref{eqn:smi_01} is motivated in a similar way to the Cut-posterior.
The extra degree of freedom $\eta$ in the power posterior $p_{\pow, \eta}$ down-weights the feedback from the $Y$-module on $\varphi$.
It is chosen to give the best possible imputation of $\varphi$ in the first phase of the inference.
The parameters $\tilde\theta$ can be thought of as auxiliary parameters introduced for the purpose of imputing $\varphi$.
This two-stage process is represented in \cref{fig:toy_multimodular_model_2stg}.
As for the Cut-posterior, $\varphi$-values from the imputation stage are treated as ``imputed data'' in the second stage, so they appear as random variables (in a grey circle) on the left, and as conditioned data (in a white square) on the right.
%On integration,
%\begin{equation*}
%  p_{\smi,\eta}(\varphi,\theta \mid Y,Z) = \left[ \int p_{\pow, \eta}(\varphi,\tilde\theta \mid Y,Z) \; d \tilde\theta \right] p(\theta \mid Y,\varphi),
%\end{equation*}
%to obtain the \acrshort*{smi} belief update $p_{\smi,\eta}(\varphi,\theta \mid Y,Z)$. \cite{Carmona2020smi} prove that the \acrshort*{smi} posteriors are coherent belief updates (in the sense of \cref{eq:coherent_bissiri}) at every $\eta$.
\begin{figure}[!htb]
  \centering
  \small
  \def\svgwidth{0.5\textwidth}
  \import{images/}{toy_smi_model_2stg.pdf_tex}
  \caption{
    Graphical representation of the implicit two-stage inference process in \acrlong*{smi}.
    Grey circles denote unknown quantities to be infered, and white boxes are fixed quantities.
    %Our variational framework performs both stages of the inference in a single pass.
    % In the first stage, we infer the posterior of $\varphi$ and  $\tilde\theta$ using the powered joint probability.
    % In the second stage, we infer the posterior of $\theta$ conditioned on fixed values of $\varphi$.
  }
  \label{fig:toy_multimodular_model_2stg}
\end{figure}

In sample-based inference for \acrshort*{smi}, variants of nested \acrshort*{mcmc} \citep{Plummer2015cut} which target $p_{\pow, \eta}(\varphi,\tilde\theta \mid Y,Z)$ and then sample $p(\theta \mid Y,\varphi)$ for each sampled $\varphi$ have the same strengths and weaknesses as they do for the Cut posterior. Efficiency considerations are discussed in \cite{Carmona2020smi}.

\acrshort*{smi} can be characterised in the same way as the Cut model in \cref{eq:cut_constrained_optim}.
Consider the class of joint densities,
\[
  \mathcal{F}_{\smi,\eta} = \{q(\varphi,\theta,\tilde\theta) : q(\varphi,\tilde\theta) = p_{\pow, \eta}(\varphi,\tilde\theta \mid Z, Y)\},
\]
in which the marginal $q(\varphi,\tilde\theta)$ equals the power posterior $p_{\pow, \eta}(\varphi,\tilde\theta \mid Z, Y)$.
Densities in $\mathcal{F}_{\smi,\eta}$ are candidate \acrshort*{smi} posteriors.
At $\eta=1$ this is a duplicated Bayes posterior,
\[
  p_{\smi,\eta=1}(\varphi,\theta,\tilde\theta \mid Y,Z)=p(\varphi,\tilde\theta \mid Y,Z) p(\theta \mid Y,\varphi),
\]
in which both $p(\varphi,\tilde\theta \mid Y,Z)$ and $\int p_{\smi,1}(\varphi,\theta,\tilde\theta \mid Y,Z)\,d\tilde\theta$ equal $p(\varphi,\theta \mid Y,Z)$ in \cref{eq:bayespost}.
\begin{proposition}
  The \acrshort*{smi} posterior in \cref{eqn:smi_01} minimises the following \acrshort*{kl}-divergence over distributions in $\mathcal{F}_{\smi,\eta}$,
  % For $q\in \mathcal{F}_{\smi}$ we have
  % \[
  % KL(q(\varphi,\theta,\tilde\theta){\,\ \mid \,} p_{\smi,\eta}(\varphi,\theta,\tilde\theta\mid Y,Z))=E_{}(KL(q(\theta\mid \varphi){\,\ \mid \,} p(\theta\mid Y,\varphi)))
  % \]
  % \cite{Yu2021variationalcut} show that, among all densities in $\mathcal{F}$, the Cut posterior in \cref{eq:cut_posterior} is the best approximation to the Bayes posterior as measured by KL-divergence, that is,
  \begin{equation}\label{eqn:smi_as_F_minimiser}
    p_{\smi,\eta}(\varphi,\theta,\tilde\theta \mid Y,Z)=\arg\min_{q\in \mathcal{F}_{\smi,\eta}} \kl{q(\varphi,\theta,\tilde\theta)}{ p(\varphi,\theta,\tilde\theta\mid Y,Z)}.
  \end{equation}
\end{proposition}
\begin{proof}
  The following is similar to the proof of the corresponding result for the Cut model in \cite{Yu2021variationalcut}.
  For $q\in \mathcal{F}_{\smi,\eta}$, we have
  \[
    q(\varphi,\theta,\tilde\theta)=p_{\pow, \eta}(\varphi,\tilde\theta \mid Y,Z)q(\theta \mid \varphi,\tilde\theta),
  \]
  so it is sufficient to show that the \acrshort*{kl} divergence to the posterior is minimised by $q(\theta  \mid \varphi,\tilde\theta)=p(\theta \mid Y,\varphi)$ (as that gives $q=p_{\smi,\eta}$).
  We have,
  \begin{align*}
    \kl{q(\varphi,\tilde\theta)\,q(\theta \mid \varphi,\tilde\theta)}{ p(\varphi,\theta,\tilde\theta \mid Y,Z)} & = \kl{q(\varphi,\tilde\theta)}{ p_{\pow, \eta}(\varphi,\tilde\theta \mid Y,Z)}                                \\
                                                                                                                & \qquad +\quad E_{q(\varphi,\tilde\theta)}[\kl{q(\theta \mid \varphi,\tilde\theta)}{p(\theta \mid Y,\varphi)}] \\
                                                                                                                & =E_{p_{\pow, \eta} }[\kl{q(\theta \mid \varphi,\tilde\theta)}{p(\theta \mid Y,\varphi)}],
  \end{align*}
  and the argument of the expectation is non-negative and zero when $q(\theta \mid \varphi,\tilde\theta)=p(\theta \mid Y,\varphi), \varphi\in\Omega_\varphi, \theta, \tilde\theta\in \Omega_{\theta}$, so $q=p_{\smi,\eta}$ minimises the original target.
  %This characterisation motivates the variational approximation of the \acrshort*{smi}-posterior.
\end{proof}

\subsubsection*{Modulating prior feedback}\label{subsec:mod_prioir_feed_paper}

If a Cut is applied to a \textit{prior density} $p(\theta|\varphi)$, as in \cite{Liu2009modularization,Jacob2017together,Styring2017extensification} and we simply remove the prior factor at the imputation stage then all that remains in the imputation posterior distribution is the base measure. A detailed example is given in \cref{subsec:exp_rnd_eff}.
The ``imputation prior'' has been replaced with a constant, and this may be inappropriate in some settings.
However, we are free to choose the imputation prior and we should use this freedom, as \cite{Moss2022} illustrate. Here we outline how this is done in \acrshort*{smi}. See \cref{sec:prior_feedback} for detail.

Consider the generative model $\varphi \sim p(\cdot),\ \theta  \sim p(\cdot \mid \varphi)$ and $Y_i \sim p(\cdot \mid \varphi,\theta),\ i=1,...,n.$
This model is shown in the leftmost graph in \cref{fig:cut-prior} in \cref{sec:prior_feedback}. The posterior is
\[
  p(\varphi,\theta \mid Y)\propto p(Y \mid \varphi,\theta)p(\varphi)p(\theta \mid \varphi).
\]
The \acrshort*{smi}-posterior is
\begin{equation}\label{eq:smi_posterior_CUTPRIOR}
  p_{\smi,\eta}(\varphi,\theta,\tilde\theta \mid Y)=p_{\pow,\eta}(\varphi,\tilde\theta \mid Y)p(\theta \mid Y,\varphi),
\end{equation}
where now
\[
  p_{\pow,\eta}(\varphi,\tilde\theta \mid Y)\propto p(\varphi)p_\eta(\tilde\theta \mid \varphi)p(Y \mid \varphi,\tilde\theta).
\]
The imputation prior $p_\eta(\tilde\theta \mid \varphi)$ must satisfy $p_{\eta=1}(\tilde\theta \mid \varphi)=p(\tilde\theta \mid \varphi)$. Like the Bayes prior $p(\theta \mid \varphi)$, the ``Cut prior'', $p_{\eta=0}(\tilde\theta \mid \varphi)=\tilde p(\tilde\theta)$ say, is a modelling choice.
Typically $p(\theta \mid \varphi)$ is a Subjective Bayes prior elicited from physical considerations, but is misspecified, and $\tilde p(\tilde\theta)$ is a non-informative Objective Bayes prior.

This \acrshort*{smi}-posterior belief update which cuts feedback in a prior is order coherent in the sense of \cite{Bissiri2016} and \cite{Nicholls2022smi}.
\begin{proposition}\label{prop:smi_cut_prior_is_OK}
  The \acrshort*{smi} posterior in \cref{eq:smi_posterior_CUTPRIOR} with cut prior feedback is an order coherent belief update.
\end{proposition}
\begin{proof}
  See \cref{proof:cut_prior_is_OK}.
\end{proof}

Taking a \emph{normalised} family $p_\eta(\tilde\theta \mid \varphi),\ \eta\in [0,1]$ of interpolating priors ensures that the marginal prior for $p(\varphi)$ in the imputation doesn't depend on $\eta$. An un-normalised family such as $p_\eta(\varphi,\tilde\theta)\propto \tilde p(\tilde\theta)^{1-\eta} p(\tilde\theta \mid \varphi)^\eta$ has all the desired interpolating properties, but the marginal $p(\varphi)$ in the imputation stage will then depend on $\eta$.
In some settings (for example when working with normal priors with fixed variance) the two prior parameterisations may be equivalent as $\eta$ scales the variance.

\section{Variational Modular Inference}\label{sec:vi_modular}

We define a variational approximation for modular posteriors based on the reparametrisation approach.
Our strategy has an end-to-end training implementation which avoids two-stage procedures, but converges to the same solution.


\subsection[Variational Inference and Normalizing Flows]{Variational Inference and Normalizing Flows} \label{sec:vi_nf}

Applications of Variational Inference
%\todo{add comment on original motivation for VI} 
\citep{Jordan1999variational, Wainwright2008, Blei2017variational} were initially focused on \acrfull*{mfvi}.
This class of variational approximations is competitive with \acrshort*{mcmc} for prediction \citep{Wang2019VariationalMisspecification} but has disadvantages for uncertainty quantification in well specified models, making it less appealing for Bayesian inference for problems with small data sets where \acrshort*{mcmc} is feasible and well calibrated uncertainty measures are important.
%Although we are interested in treating misspecification, and misspecification error may dominate the approximation error and lead be competitive for prediction \citep{Wang-Blei-2019}, the management of misspecification is handled in our setting by the \acrshort*{smi} framework, so there is a strong emphasis in our setting on accurate approximation that supports the goals of \acrshort*{smi}.

Advances in variational methods have been motivated by its use in generative models in the Machine Learning literature and in particular in the context of \acrfullpl*{vae} \cite{Kingma2013vae,Kingma2019vaeintro} and applications in machine vision.
Variational families based on \acrfullpl*{nf} \citep{Rezende2015nf, Papamakarios2021normalizing, Kobyzev2020normalizing} developed in that context offer generative models which are much more expressive than \acrshort*{mfvi} and give better calibrated measures of uncertainty.
Adoption of \acrshortpl*{nf} in applications of statistical modelling and inference, where \acrshort*{mcmc} and \acrshort*{mfvi} are the de-facto approaches, has been more limited.
\acrfull*{svi} \citep{Hoffman2013svi} and \acrfull*{bbvi} \citep{Ranganath2014bbvi} offer efficient procedures to fit variational families which apply directly to \acrshort*{nf} parameterisations.
Recent advances include new methods for evaluating convergence and adequacy of variational approximation \citep{Yao2018yesbut,Xing19,Agrawal2020,Dhaka2020robust}.

% \acrshort*{nf}'s take a random vector following a \emph{base distribution} $p(\epsilon)$ and transform it to a parameter, $x$ say, using a highly-flexible transformation (\emph{diffeomorphism}), $T_\lambda$, parametrised by $\lambda$.
% Let $x = T_\lambda(\epsilon)$ and suppose $x \sim q(x)$ with
% \begin{equation}
%   q_{\lambda}(x) = p( T^{-1}(x) ) \left\vert J_{T_\lambda} \right\vert ^{-1}
% \end{equation}
% where $\left\vert J_{T_\lambda}\right\vert$ is the determinant of the Jacobian of the transformation.
% In the context of variational inference, where we need to be able to both simulate and evaluate the density $q$, the transformation is restricted by the requirement that $J_{T_\lambda}$ be computable (usually a lower triangular Jacobian).
% See \cref{sec:nf} for further relevant details on \acrlongpl*{nf}.

% To facilitate the definition and analysis of the transformation $T_\lambda$, we decompose it into a \textbf{transformer}, $\tau$, and \textbf{conditioners}, $c_i,\ i=1,...,d_{cond}$ \citep{Huang2018flows,Papamakarios2021normalizing}.
% The transformer is an invertible function that specifies how the flow acts on $z_i$ in order to output $z_i'$.
%
% The conditioner determines the parameters of the transformer by taking a subset of the input random variables $z_i$.
% The conditioners need not be bijective.
% The variational parameters $\lambda$ of the transformation $T_\lambda$ are typically the parameters of the conditioner.


% Research and applications of \acrshortpl*{nf} has been in the context of generative modeling for computer vision.
% There is still limited use of \acrshortpl*{nf} in the context of traditional parametric modelling, where \acrshort*{mcmc} is the de-facto approach.
% Nevertheless, research on \acrshortpl*{nf} has produced a significant expansion on variational families which provide a fertile field for research for parametric modelling.
% Some \acrshortpl*{nf} have the property of being \emph{universal approximators} \todo{expand}.


\subsection{Variational Bayes in multi-modular models}\label{subsec:vi_multi}

We begin by giving a standard variational approximation to the Bayes posterior for the multi-modular model.
%This establishes notation, and will allow us to show that our variational-\acrshort*{smi} interpolates variational-Cut and variational-Bayes in the same way that \acrshort*{smi} interpolates the exact Cut and Bayes posteriors.
For concreteness, we use the multi-modular model in \cref{fig:toy_multimodular_model}.
Having established our methods on this class of models, extensions to other dependence structures are straightforward, as we illustrate in \cref{subsec:exp_rnd_eff}.

We take a parametrisation of the variational posterior in terms of a product
\begin{equation}\label{eq:q_modular_product}
  q_{\beta}(\varphi,\theta) = q_{\beta_1}(\varphi)q_{\beta_2}(\theta \mid \varphi),
\end{equation}
with each factor using a disjoint subset $\beta_1\in \Lambda_1$, $\beta_2\in \Lambda_2$ of a set of variational parameters $\beta=(\beta_1,\beta_2)$, with $\beta\in B$ and $B=\Lambda_1\times\Lambda_2$. Here $\Lambda_1=\Re^{L_1}$, $\Lambda_2=\Re^{L_2}$ and $B=\Re^{L_1+L_2}$ are typically high dimensional real spaces of variational parameters.

Our notation implies a \emph{flow}-based approach but captures a number of other parameterisations.
Let $\epsilon = (\epsilon_1, \epsilon_2)$ be a vector of continuous random variables distributed according to a \emph{base} distribution $p(\epsilon)=p(\epsilon_1, \epsilon_2)$, with $\epsilon_1\in \Re^{p_\varphi}$ and $\epsilon_2\in \Re^{p_\theta}$. We can for example take $p(\epsilon_1, \epsilon_2)$ to be the $\varphi,\theta$-prior.
Consider a diffeomorphism, $T: \Re^{p_\varphi+p_\theta}\to \Re^{p_\varphi+p_\theta}$ defined by concatenating the two diffeomorphisms expressing $\varphi$ and $\theta$, so that
\begin{align}
  \varphi_{(\beta_1,\epsilon)} & =T_1(\epsilon_1;\beta_1)\nonumber                                                                        \\
  \theta_{(\beta_2,\epsilon)}  & =T_2(\epsilon_2; \beta_2, \epsilon_1)\nonumber                                                           \\
  T(\epsilon; \beta)           & = \left(T_1(\epsilon_1;\beta_1), T_2(\epsilon_2;\beta_2, \epsilon_1). \right),\label{eq:bayes_transform}
\end{align}
For flow-based densities, $T_1: \Re^{p_\varphi}\to \Re^{p_\varphi}$ and $T_2: \Re^{p_\theta}\to \Re^{p_\theta}$ have properties listed in \cref{subsec:smi_flow}  \cite[see][Sec.~3]{Kobyzev2020normalizing} which allow us to sample, differentiate and evaluate the densities $q_\beta(\varphi,\theta)$ defined below. However, other familiar variational families such as \acrshort*{mfvi} can be expressed using \cref{eq:bayes_transform}.
Note that $T_2$ is a \emph{conditional transformation} that depends on $\epsilon_1$, so it can express correlation between $\varphi_{(\beta_1,\epsilon)}$ and $\theta_{(\beta_2,\epsilon)}$ (see \cref{sec:nf}).
In a normalising flow, $T_1$ and $T_2$ are compositions of diffeomorphisms, each with their own parameters. This increases the flexibility of the transformation.
%In \cref{subsec:smi_flow} we give more details of our flow implementation.

The Jacobian matrix, $J_{T}=\partial T/\partial\epsilon$ is block lower triangular, so its determinant
is a product of determinants of $J_{T_1} = \partial T_1/\partial \epsilon_1$ and $J_{T_2} = \partial T_2/\partial \epsilon_2$,
\begin{equation*}
  \left\vert J_T \right\vert = \left\vert J_{T_1} \right\vert \left\vert J_{T_2} \right\vert
\end{equation*}
with no cross dependence on $\beta_1,\beta_2$, so that $\nabla_{\beta_2} \log \left\vert J_{T_1} \right\vert =0$ and $\nabla_{\beta_1} \log \left\vert J_{T_2} \right\vert =0$.
The joint variational distribution produced by the flow is then
\begin{align*}
  q_{\beta}(\varphi, \theta) & = p(\epsilon_1, \epsilon_2) \left\vert J_T \right\vert ^{-1} \\
                             & =q_{\beta_1}(\varphi)q_{\beta_2}(\theta\mid \varphi),
\end{align*}
where
\begin{align}
  q_{\beta_1}(\varphi)            & = p(\epsilon_1) \left\vert J_{T_1} \right\vert ^{-1}, \label{eqn:q_lambda1_from_p_jacob}              \\
  q_{\beta_2}(\theta\mid \varphi) & =p(\epsilon_2\mid \epsilon_1) \left\vert J_{T_2} \right\vert ^{-1}.\label{eqn:q_lambda2_from_p_jacob}
\end{align}
We need to be able to evaluate the determinants of the Jacobians $J_{T_1}$ and $J_{T_2}$. This works for a \acrshort*{nf} because the matrices are lower trianglular. However, other simpler designs such as \acrshort*{mfvi} also admit straightforward evaluation.

The optimal variational parameters minimise the \acrshort*{kl} divergence to the posterior, but will not in general be unique.
Let
\begin{align}
  D^*_B & =\min_{\beta\in B} \kl{ q_{\beta}(\varphi, \theta) }{ p(\varphi, \theta \mid Z, Y) },\label{eqn:variational_bayes_standard_Dstar} \\
  \intertext{and}
  B^*   & = \{\beta\in B:  \kl{ q_{\beta}(\varphi, \theta) }{ p(\varphi, \theta \mid Z, Y) }=D^*_B\},\label{eqn:variational_bayes_standard}
\end{align}
and let $\beta^*=(\beta^*_1,\beta^*_2)\in B^*$ be a generic set of parameter values minimising the \acrshort*{kl} divergence.
The definition in \cref{eqn:variational_bayes_standard} is equivalent to maximising the \acrshort*{elbo},
\begin{equation}\label{eq:elbo_modular}
  \elbo_{\bayes} = \E_{(\varphi, \theta) \sim q_{\beta}(\varphi, \theta)}[ \log p(\varphi, \theta, Z, Y) - \log q_{\beta}(\varphi, \theta) ].
\end{equation}
Using the \emph{reparametrisation trick} and expanding the joint distribution
\begin{align*}
  \elbo_{\bayes} = \E_{\epsilon \sim p(\epsilon)}[ & \log p(\varphi_{(\beta_1,\epsilon)}, \theta_{(\beta_2,\epsilon)}, Z, Y) - \log q_{\beta}(\varphi_{(\beta_1,\epsilon)}, \theta_{(\beta_2,\epsilon)}) ]                                      \\
  = \E_{\epsilon \sim p(\epsilon)}[                & \log p(Z \mid \varphi_{(\beta_1,\epsilon)}) + \log p(Y \mid \varphi_{(\beta_1,\epsilon)}, \theta_{(\beta_2,\epsilon)}) + \log p(\varphi_{(\beta_1,\epsilon)}, \theta_{(\beta_2,\epsilon)}) \\
                                                   & - \log p(\epsilon) + \log \left\vert J_{T} \right\vert  ],
\end{align*}
and the gradients of the \acrshort*{elbo} with respect to the variational parameters $(\beta_1,\beta_2)$ are
\begin{align}
  \nabla_{\beta_1} \elbo_{\bayes} = \E_{\epsilon \sim p(\epsilon)}[ & \nabla_{\varphi} \left\{ \log p(Z \mid \varphi) + \log p(Y \mid \varphi, \theta) + \log p(\varphi) \right\} \nabla_{\beta_1} \{ \varphi \} \nonumber             \\
                                                                    & + \nabla_{\beta_1} \log \left\vert J_{T_1} \right\vert  ]  \label{eq:grad_elbo_l1_full},                                                                         \\
  \nabla_{\beta_2} \elbo_{\bayes} = \E_{\epsilon \sim p(\epsilon)}[ & \nabla_{\theta} \left\{ \log p(Y \mid \varphi, \theta) + \log p(\theta \mid \varphi) \right\} \nabla_{\beta_2} \{ \theta \}                            \nonumber \\
                                                                    & + \nabla_{\beta_2} \log \left\vert J_{T_2} \right\vert  ]. \label{eq:grad_elbo_l2_full}
\end{align}
These gradients are used in \acrlong*{svi} \citep{Hoffman2013svi} to obtain the optimal variational parameters $\beta^*\in B^*$ for approximation of the Bayes posterior.
%We also use \cref{eq:grad_elbo_l1_full,eq:grad_elbo_l2_full} to verify that our \acrshort*{sgd} algorithm for the \acrshort*{smi} posterior coincides with the Bayes posterior in the case $\eta=1$.
% We give the \acrshort*{sgd} algorithm for the more general setting of \acrshort*{smi} below.

\subsection{Variational SMI}\label{subsec:vsmi}

In this section we define our variational approximation to the \acrshort*{smi} posterior.
For this, we expand the variational distribution in \cref{eq:q_modular_product} to include the auxiliary parameter $\tilde\theta$.
Again, we parametrise the variational posterior as a product,
\begin{equation}\label{eq:q_modular_smi_product}
  q_{\lambda}(\varphi,\theta, \tilde\theta) = q_{\lambda_1}(\varphi) q_{\lambda_2}(\theta \mid \varphi) q_{\lambda_3}(\tilde\theta \mid \varphi)
\end{equation}
where each factor has its own parameters, $\lambda_1\in \Lambda_1$ and $\lambda_2,\lambda_3\in \Lambda_2$ where $\Lambda_1$ and $\Lambda_2$ are defined above. Let $\lambda=(\lambda_1,\lambda_2,\lambda_3)$ with $\lambda\in \Lambda$ and $\Lambda=\Lambda_1\times \Lambda_2\times \Lambda_2$ so that $\Lambda=\Re^{L_1+2L_2}$. The parameters of $q_{\lambda_2}(\theta \mid \varphi)$ and $q_{\lambda_3}(\tilde\theta \mid \varphi)$ both match the variational Bayes parameterisation so we write $(\lambda_1,\lambda_2)\in B$ and $(\lambda_1,\lambda_3)\in B$.
Let
\[
  \mathcal{Q}=\{q_\lambda(\varphi,\theta,\tilde\theta); \lambda\in \Lambda\}
\]
denote the class of densities in our variational family.

\subsubsection{The variational-SMI approximation and its properties}

The purpose of \acrshort*{smi} is to control the flow of information from the $Y$-module into the posterior distribution for $\varphi$.
This leads us to define three basic properties that a useful variational approximation of the \acrshort*{smi} posterior must possess:
\begin{description}
  \item[(P1)] (expresses Cut) at $\eta=0$ the optimal variational posterior $q_{\lambda_1^*}(\varphi)$ is completely independent of the generative model for $Y$,
    with \[\lambda_1^*\in\{\lambda_1\in\Lambda_1: \kl{q_{\lambda_1}(\varphi)}{p(\varphi|Z)}=d^*_{\cut}\}\] and $d^*_{\cut}=\min_{\lambda_1\in\Lambda_1}\kl{q_{\lambda_1}(\varphi)}{p(\varphi|Z)}$.
  \item[(P2)] (expresses Bayes) at $\eta=1$ the marginal variational \acrshort*{smi} posterior $q_{\lambda^*_1,\lambda_2^*}(\varphi,\theta)$ is equal to the variational Bayes posterior, $q_{\beta^*_1,\beta^*_2}(\varphi,\theta)$ in \cref{eqn:variational_bayes_standard}, so $(\lambda_1^*,\lambda^*_2)\in B^*$;
  \item[(P3)] (approximates \acrshort*{smi}) for $\eta\in [0,1]$, if $p_{\smi,\eta}\in\mathcal{Q}$ then the variational approximation is equal to the target \acrshort*{smi} posterior, so $q_{\lambda^*}=p_{\smi,\eta}$.
\end{description}

Properties (P1-2) require $q_{\lambda^*}$ to interpolate a variational approximation to the Cut-posterior (removing all feedback from $Y$ into the variational approximation to the distribution of $\varphi$) and our original variational approximation to the Bayes posterior.
We will see that a standard variational approximation to the \acrshort*{smi} posterior based on the \acrshort*{kl} divergence between $q_\lambda$ and $p_{\smi,\eta}$ cannot satisfy these properties. We give a variational procedure based on the loss $\mathcal{L}^{( \smi, \eta)}(\lambda)$ in \cref{eq:vsmi_loss}, and show in \cref{prop:stop-gradient-loss} that if we take a variational approximation to $p_{\smi,\eta}$ minimising this loss then our variational approximation satisfies properties (P1-3).

\subsubsection{Defining the variational family}
As in \cref{subsec:vi_multi}, our notation it set up for flow-based approximation of the \acrshort*{smi} posterior, but captures other variational families such as \acrshort*{mfvi}.
We expand the base distribution and diffeomorphism to accommodate the auxiliary $\tilde\theta$.
The distributions $p(\epsilon_1)$ and $p(\epsilon_2 \mid \epsilon_1)$ and the transformations $T_1$ and $T_2$ are unchanged from \cref{subsec:vi_multi}. %This choice delivers property (P2).

Let $\epsilon=(\epsilon_1, \epsilon_2, \epsilon_3)$ be the vector of random variables for our base distribution, $\epsilon\sim p(\epsilon)$, with $\epsilon_1$ and $\epsilon_2$ as before and $\epsilon_3\sim \epsilon_2$ so that $\epsilon_3\in \Re^{p_\theta}$.
Consider the extended diffeomorphism $T$ defined by the transformations,
\begin{align}
  \varphi_{(\lambda_1,\epsilon)}      & =T_1(\epsilon_1;\lambda_1)\nonumber                                                                                                                     \\
  \theta_{(\lambda_2,\epsilon)}       & =T_2(\epsilon_2; \lambda_2, \epsilon_1)\nonumber                                                                                                        \\
  \tilde\theta_{(\lambda_3,\epsilon)} & =T_2(\epsilon_3; \lambda_3, \epsilon_1)\nonumber                                                                                                        \\
  T(\epsilon; \lambda)                & = \left( T_1(\epsilon_1;\lambda_1) ,\; T_2(\epsilon_2;\lambda_2, \epsilon_1) ,\; T_2(\epsilon_3;\lambda_3, \epsilon_1) \right).\label{eq:smi_transform}
\end{align}
See \cref{eq:smi_transform_details} in \cref{sec:nf} for further details of these maps in a generic \acrshort*{nf} setting.

The Jacobian of the transformation, $J_{T}=\partial T/\partial\epsilon$, is block lower triangular as before, so its determinant factorises $\left\vert J_{T} \right\vert  = \left\vert J_{T_1}(\epsilon_1) \right\vert \, \left\vert J_{T_2}(\epsilon_1,\epsilon_2) \right\vert \, \left\vert J_{T_2}(\epsilon_1,\epsilon_3) \right\vert$ where we draw attention to the different arguments in the Jacobian factors involving $T_2$ but omit the $\lambda$-dependence.
We give more details of the transformation $T$ in \cref{subsec:smi_flow}.

Our variational family approximating the \acrshort*{smi} posterior $p_{\smi,\eta}$ has variational parameters $\lambda=(\lambda_1,\lambda_2,\lambda_3)$ and conditional independence structure
\begin{align}\label{eqn:q-lambda-smi-var}
  q_{\lambda}(\varphi, \theta, \tilde\theta) & = p(\epsilon_1, \epsilon_2, \epsilon_3) \left\vert J_{T} \right\vert ^{-1}
  \\
                                             & =q_{\lambda_1}(\varphi)q_{\lambda_2}(\theta\mid \varphi)q_{\lambda_3}(\tilde\theta\mid \varphi),\nonumber
\end{align}
where $q_{\lambda_1}$ and $q_{\lambda_2}$ are given in \cref{eqn:q_lambda1_from_p_jacob,eqn:q_lambda2_from_p_jacob} (replacing $\beta\to \lambda$) and
\begin{equation}
  q_{\lambda_3}(\tilde\theta\mid \varphi)=p(\epsilon_3\mid \epsilon_1) \left\vert J_{T_2} \right\vert ^{-1}.
\end{equation}

%The subspace $(\lambda_1,\lambda_2)\in B$ coincides with the Bayes parameterisation, so
% \[
% \cup_{(\lambda_1,\lambda_2,\lambda_3)\in\Lambda}\{(\lambda_1,\lambda_2)\}=B,
% \]
% and the same for $(\lambda_1,\lambda_3)\in B$.
% Finally, let
% \[
% B(\lambda_1)=\{\lambda_2: (\lambda_1,\lambda_2)\in B\}
% \]
% denote the space of $\lambda_2$-values given $\lambda_1$.
% Similarly, $\lambda_3\in B(\lambda_1)$ when $(\lambda_1,\lambda_3)\in B$.

\subsubsection{The standard variational loss does not satisfy Properties (P1-2)}
A naive application of variational approximation to \acrshort*{smi} would minimise the \acrshort*{kl} divergence to the \acrshort*{smi} posterior at $\lambda^*=(\lambda^*_1,\lambda^*_2,\lambda^*_3)$ where
\begin{equation}\label{eqn:naive_VI_KL_basic}
  \lambda^* = \argmin_{\lambda\in \Lambda} \kl{ q_{\lambda}(\varphi, \theta, \tilde\theta) }{ p_{ \smi, \eta}(\varphi, \theta, \tilde\theta \mid Z, Y) },
\end{equation}
(ignoring non-uniqueness for brevity).
However, this presents two problems, one of principle and one of practice.

The principle of \acrshort*{smi} is to control the flow of information from the $Y$-module into the posterior distribution for $\varphi$.
This is lost in this setup.
The \acrshort*{kl}-divergence in \cref{eqn:naive_VI_KL_basic} is
\begin{align}\label{eqn:naive_VI_KL_expand}
  \kl{ q_{\lambda} }{ p_{ \smi, \eta}}
   & =\kl{q_{\lambda_1,\lambda_3}(\varphi,\tilde\theta) }{p_{\pow, \eta}(\varphi,\tilde\theta \mid Y,Z)}\nonumber
  \\
   & \qquad+\quad E_{\varphi\sim q_{\lambda_1}}[\kl{q_{\lambda_2}(\theta\mid \varphi)}{p(\theta \mid Y,\varphi)}].
\end{align}
The first term allows controlled feedback from the $Y$-module, as the $Y$-dependence in the power posterior $p_{\pow, \eta}$ is controlled by $\eta$ and vanishes entirely when $\eta=0$.
However, the second term leaks information from the $Y$-module to inform $q_{\lambda_1}(\varphi)$, even in the case $\eta=0$, therefore violating property (P1).

This variational approximation will not in general satisfy property (P2) either. In order for property (P2) to be satisfied at $\eta=1$ we must have $(\lambda^*_1,\lambda^*_2)=(\beta^*_1,\beta^*_2)$ for some $(\beta^*_1,\beta^*_2)\in B^*$ whenever $(\lambda^*_1,\lambda^*_2,\lambda^*_3)$ satisfy \cref{eqn:naive_VI_KL_basic} for some $\lambda^*_3\in \Lambda_2$, that is, the marginal variational \acrshort*{smi} distribution for $(\varphi,\theta)$ must coincide with one of the variational Bayes solutions. The optimal $\lambda^*$ minimise \cref{eqn:naive_VI_KL_expand}, so they maximise the \acrshort*{elbo},
\begin{align}\label{eq:elbo_naive_smi}
  \elbo_{\smi \text{naive}} = & \E_{(\varphi, \tilde\theta) \sim q_{\lambda_1,\lambda_3}(\varphi, \tilde\theta)}[ \log p_{\pow, \eta}(\varphi, \tilde\theta, Z, Y) - \log q_{\lambda_1,\lambda_3}(\varphi, \tilde\theta) ] \nonumber \\
                              & + \E_{(\varphi, \theta) \sim q_{\lambda_1,\lambda_2}(\varphi, \theta)}[ \log p(\varphi, \theta, Y) - \log q_{\lambda_2}(\theta \mid \varphi) ]                                                       \\
                              & - \E_{\varphi \sim q_{\lambda_1}(\varphi)}[ \log p(Y, \varphi) ]. \nonumber
\end{align}
Since the $\lambda^*$ parameters solve $\nabla_\lambda \elbo_{\smi \text{naive}}=0$ and the $\beta^*$ parameters solve $\nabla_\beta \elbo_{\bayes}=0$, a necessary condition for (P2) is that
$\left.\nabla_\beta \elbo_{\bayes}\right\vert_{\beta=(\lambda^*_1,\lambda^*_2)}=0$
(ie \cref{eq:grad_elbo_l1_full,eq:grad_elbo_l2_full} at $\beta=(\lambda^*_1,\lambda^*_2)$) at $\eta=1$.
However, using the reparameterisation trick, the $\elbo_{\smi \text{naive}}$-gradients can be written
\begin{align}
  \nabla_{\lambda_1} \elbo_{\smi \text{naive}} = \E_{\epsilon \sim p(\epsilon)}[ & \nabla_{\varphi} \left\{ \log p(Z \mid \varphi) + \eta \log p(Y \mid \varphi, \tilde\theta) + \log p(\varphi) \right\} \nabla_{\lambda_1} \{ \varphi \} \nonumber                         \\
                                                                                 & + \nabla_{\lambda_1} \log \left\vert J_{T_1}(\epsilon_1) \right\vert \nonumber                                                                                                            \\
                                                                                 & + \nabla_{\varphi} \left\{ \log p(Y \mid \varphi, \theta) + \log p(Y\mid \varphi) \right\} \nabla_{\lambda_1} \{ \varphi \} ]  \label{eq:grad_elbo_l1_naive_smi},                         \\
  \nabla_{\lambda_2} \elbo_{\smi \text{naive}} = \E_{\epsilon \sim p(\epsilon)}[ & \nabla_{\theta} \left\{ \log p(Y \mid \varphi, \theta) + \log p(\theta \mid \varphi) \right\} \nabla_{\lambda_2} \{ \theta \}                            \nonumber                        \\
                                                                                 & + \nabla_{\lambda_2} \log \left\vert J_{T_2}(\epsilon_1,\epsilon_2) \right\vert  ]. \label{eq:grad_elbo_l2_naive_smi}                                                                     \\
  \nabla_{\lambda_3} \elbo_{\smi \text{naive}} = \E_{\epsilon \sim p(\epsilon)}[ & \nabla_{\theta} \left\{ \eta \log p(Y \mid \varphi, \tilde\theta) + \log p(\tilde\theta \mid \varphi) \right\} \nabla_{\lambda_3} \{ \tilde\theta \}                            \nonumber \\
                                                                                 & + \nabla_{\lambda_3} \log \left\vert J_{T_2}(\epsilon_1,\epsilon_2) \right\vert  ]. \label{eq:grad_elbo_l3_naive_smi}
\end{align}
If these equations and $\left.\nabla_\beta \elbo_{\bayes}\right\vert_{\beta=(\lambda^*_1,\lambda^*_2)}=0$
all hold at at $\eta=1$ then
\[
  \E_{\epsilon \sim p(\epsilon)}\left[\nabla_{\varphi} \left\{ \log p(Y \mid \varphi, \tilde\theta) + \log p(Y\mid \varphi) \right\} \nabla_{\lambda_1} \{ \varphi \}\right]_{\lambda=\lambda^*}=0.
\]
Our variational framework has to satisfy (P2) for every target $p_{\smi,\eta}$ and every variational family $\mathcal{Q}$. However, if we target the loss in \cref{eqn:naive_VI_KL_basic} then $\lambda^*$ would have to satisfy an over-determined system of equations at $\eta=1$ and this will in general have no solutions.

We learn from this that the loss we seek for the variational \acrshort*{smi} approximation is not captured by the \acrshort*{kl}-divergence in \cref{eqn:naive_VI_KL_basic}.
However, there is a second practical problem with carrying out \acrlong*{svi} based on this naive variational loss.
In practice, in order to minimise \cref{eqn:naive_VI_KL_expand}, we maximise $\elbo_{\smi,\text{naive}}$ in \cref{eq:elbo_naive_smi}
using a Monte Carlo estimate of its gradients.
The last term in \cref{eq:elbo_naive_smi} involves the intractable $E_{\varphi\sim q_{\lambda_1}}[\log(p(Y, \varphi)]$, making the $\lambda_1$-variation unrealisable in practice.



\subsubsection{Loss for variational-SMI}

One way to characterise variational \acrshort*{smi} is by generalising the two-stage optimisation approach given by \cite{Yu2021variationalcut} for the Cut posterior. We will see that this approach satisfies properties (P1-3), and that the optimal variational parameters are given by minimising a customised variational loss.
Let
\begin{equation}\label{eqn:distance_to_set}
  d(q_{\lambda},\mathcal{F}_{\smi,\eta})=\min_{\tilde q\in \mathcal{F}_{\smi,\eta}}\kl{q_{\lambda}}{\tilde q}.
\end{equation}
define the divergence between the density $q_{\lambda}$ and the set of densities $\mathcal{F}_{\smi,\eta}$.

\begin{proposition}\label{prop:var_smi_distance}
  The divergence defined in \cref{eqn:distance_to_set} can be written
  \begin{equation}\label{eqn:var_smi_distance}
    d(q_{\lambda},\mathcal{F}_{\smi,\eta})=\kl{q_{\lambda_1, \lambda_3}(\varphi, \tilde\theta)}{p_{\pow, \eta}(\varphi,\tilde\theta \mid Y,Z)},
  \end{equation}
  and hence does not depend on $\lambda_2$.
\end{proposition}
\begin{proof}
  See \cref{proof:var_smi_distance}.
\end{proof}

We now define the optimal variational parameters.
These will minimise divergence from distributions in $\mathcal{F}_{\smi,\eta}$ and otherwise approximate \acrshort*{smi}.
First, exploiting \cref{prop:var_smi_distance}, $(\lambda_1,\lambda_3)$ minimise \cref{eqn:var_smi_distance}. Let
\begin{align}
  d^*_{\smi}        & =\min_{(\lambda_1,\lambda_3)\in B}\kl{q_{\lambda_1, \lambda_3}(\varphi, \tilde\theta)}{p_{\pow, \eta}(\varphi,\tilde\theta \mid Y,Z)}\nonumber \\
  \Lambda^*_{(1,3)} & = \{(\lambda_1,\lambda_3)\in B:
  \kl{q_{\lambda_1, \lambda_3}(\varphi, \tilde\theta)}{p_{\pow, \eta}(\varphi,\tilde\theta \mid Y,Z)}=d^*_{\smi}\}.
  \label{eqn:lambda13-star-defn}\end{align}
Secondly, $\lambda_2$ is chosen for best approximation of $p_{ \smi, \eta}$ at fixed $(\lambda^*_1,\lambda^*_3)\in \Lambda^*_{(1,3)}$. Let
\begin{align}
  D^*_{\smi}(\lambda^*_1)      & =\min_{\lambda_2\in \Lambda_2} E_{\varphi\sim q_{\lambda^*_1}}[\kl{q_{\lambda_2}(\theta\mid \varphi)}{p(\theta \mid Y,\varphi)}]\nonumber                                                    \\
  \Lambda^*_{(2)}(\lambda^*_1) & =\{\lambda_2\in \Lambda_2: E_{\varphi\sim q_{\lambda^*_1}}[\kl{q_{\lambda_2}(\theta\mid \varphi)}{p(\theta \mid Y,\varphi)}]  = D^*_{\smi}(\lambda^*_1)\}\label{eqn:lambda2-star-defn-equiv}
\end{align}
The following proposition shows that $\lambda^*_2\in \Lambda^*_{(2)}(\lambda^*_1)$ targets a good fit to $p_{ \smi, \eta}$.
\begin{proposition}\label{prop:var_smi_lambda2}
  The set $\Lambda^*_{(2)}(\lambda^*_1)$ defined in \cref{eqn:lambda2-star-defn-equiv} is equivalently
  \begin{align}
    \tilde D^*_{\smi}(\lambda^*_1) & =\min_{\lambda_2\in \Lambda_2} \kl{q_{(\lambda^*_1,\lambda_2,\lambda^*_3)}}{p_{ \smi, \eta}}\nonumber                                                       \\
    \Lambda^*_{(2)}(\lambda^*_1)   & =\{\lambda_2\in \Lambda_2:  \kl{q_{(\lambda^*_1,\lambda_2,\lambda^*_3)}}{p_{ \smi, \eta}} = \tilde D^*_{\smi}(\lambda^*_1)\}. \label{eqn:lambda2-star-defn}
  \end{align}
\end{proposition}
\begin{proof}
  Expand the \acrshort*{kl} divergence in \cref{eqn:lambda2-star-defn} using \cref{eqn:naive_VI_KL_expand} and substitute
  $(\lambda_1,\lambda_3)=(\lambda^*_1,\lambda^*_3)$.
  The first term does not depend on $\lambda_2$ and the second term gives \cref{eqn:lambda2-star-defn-equiv}.
\end{proof}

%We set out to find a variational approximation minimising a well defined loss and interpolating between variational approximations to the Cut and Bayes posteriors.
We now define variational \acrshort*{smi} and demonstrate (P1-3).
\begin{definition}\label{defn:var-smi}
  \emph{(Variational \acrshort*{smi})}
  A variational \acrshort*{smi} posterior density is a density $q_{\lambda^*}(\varphi,\theta,\tilde\theta)$ parameterised in  \cref{eq:q_modular_smi_product} with $\lambda^*\in\Lambda^*$ where
  \begin{equation}\label{eqn:smi-lambda-star}
    \Lambda^*=\bigcup_{(\lambda^*_1,\lambda^*_3)\in \Lambda^*_{(1,3)}}\left(\bigcup_{\lambda^*_2\in\Lambda^*_{(2)}(\lambda^*_1)}\{(\lambda^*_1,\lambda^*_2,\lambda^*_3)\}\right),
  \end{equation}
  and $\Lambda^*_{(1,3)}$ and $\Lambda^*_{(2)}(\lambda^*_1)$ are defined in \cref{eqn:lambda13-star-defn,eqn:lambda2-star-defn-equiv} respectively.
\end{definition}

% \begin{proposition}\label{prop:var_smi_is_cut_at_eta0}
%   Variational \acrshort*{smi} satisfies property (P1).
%   Let
%   \begin{align*}
%     \tilde\lambda^*_1 & =\arg\min_{\lambda_1} \kl{q_{\lambda_1}(\varphi)}{p(\varphi \mid Z)}                   \\
%     \tilde\lambda_2   & =\arg\min_{\lambda_2} \kl{q_{\lambda_2}(\theta\mid \varphi)}{p(\theta \mid Y,\varphi)}
%   \end{align*}
%   and $q_{\tilde\lambda^*_1,\tilde\lambda^*_2}(\varphi,\theta)=q_{\tilde\lambda^*_1}(\varphi)q_{\tilde\lambda^*_2}(\theta\mid \varphi)$ give a direct variational approximation to the Cut-posterior
%   $p_{\cut}(\varphi,\theta \mid Y,Z)=p(\varphi \mid Z)\,p(\theta \mid Y,\varphi)$.
%   When $\eta=0$ we find that $q_{\lambda^*_1}(\varphiremk:smi-defn-eta-depend)=q_{\tilde\lambda^*_1}(\varphi)$ and $q_{\lambda^*_2}(\theta\mid \varphi)=q_{\tilde\lambda^*_2}(\theta\mid \varphi)$.
%   It follows that variational \acrshort*{smi} targets the Cut-posterior and in particular $q_{\lambda^*_1}$ does not depend in any way on $p(Y\mid \varphi,\theta)$ or $p(\theta\mid \varphi)$.
% \end{proposition}

\begin{remark}\label{remk:smi-defn-eta-depend}
  Our discussion in this section takes $\eta$ fixed. As we vary $\eta$ the target $p_{\smi,\eta}$ varies, so the set of optimal variational parameters $\Lambda^*$ depends on $\eta$. Below we write $\Lambda^*(\eta)$ when we need to emphasise this dependence.
\end{remark}

\begin{remark}\label{remk:var-smi-roots}
  The variational \acrshort*{smi} parameters $\lambda^*\in\Lambda^*$ are roots of the equations
  \begin{align}
    \nabla_{(\lambda_1,\lambda_3)}\kl{q_{\lambda_1,\lambda_3}}{p_{\pow, \eta}}                                       & =0 \label{eqn:lam-star-roots1a} \\
    \nabla_{\lambda_2} E_{\varphi\sim q_{\lambda_1}}\kl{q_{\lambda_2}(\theta\mid \varphi)}{p(\theta \mid Y,\varphi)} & =0\label{eqn:lam-star-roots1b}
  \end{align}
  with positive curvature.
  We have not substituted $\lambda_1=\lambda^*_1$ in \cref{eqn:lam-star-roots1b}. As a system, any $\lambda_1$ satisfying \cref{eqn:lam-star-roots1b} is required to be a root (with $\lambda^*_3$) of \cref{eqn:lam-star-roots1a} so the system imposes this condition. This will allow us to solve these equations as a single system using \acrshort*{sgd} on the loss $\mathcal{L}^{\smi,\eta}$ in \cref{prop:stop-gradient-loss} below, avoiding a two-stage procedure.
\end{remark}

\begin{remark}\label{remk:var-smi-roots-gradients}
  Consider the variational family defined in \cref{eqn:q-lambda-smi-var}.
  Using the reparametrisation trick and expanding terms, \cref{eqn:lam-star-roots1a,eqn:lam-star-roots1b} are
  \begin{align}
    0 = \E_{\epsilon \sim p(\epsilon)}[ & \nabla_{\varphi} \left\{ \log p(Z \mid \varphi) + \eta \log p(Y \mid \varphi, \tilde\theta) + \log p(\varphi) \right\} \nabla_{\lambda_1} \{ \varphi \} \nonumber
    \\
                                        & + \nabla_{\lambda_1} \log \left\vert J_{T_1}(\epsilon_1) \right\vert  ],  \label{eq:vsmi_roots_l1}                                                                   \\
    0 = \E_{\epsilon \sim p(\epsilon)}[ & \nabla_{\theta} \left\{ \log p(Y \mid \varphi, \theta) + \log p(\theta \mid \varphi) \right\} \nabla_{\lambda_2} \{ \theta \} \nonumber                              \\
                                        & + \nabla_{\lambda_2} \log \left\vert J_{T_2}(\epsilon_1,\epsilon_2) \right\vert  ] \label{eq:vsmi_roots_l2}                                                          \\
    0 = \E_{\epsilon \sim p(\epsilon)}[ & \nabla_{\tilde\theta} \left\{ \eta \log p(Y \mid \varphi, \tilde\theta) + \log p(\tilde\theta \mid \varphi) \right\} \nabla_{\lambda_3} \{ \tilde\theta \} \nonumber \\
                                        & + \nabla_{\lambda_3} \log \left\vert J_{T_2}(\epsilon_1,\epsilon_3) \right\vert  ] \label{eq:vsmi_roots_l3}
  \end{align}
  Notice that the extra terms in \cref{eq:grad_elbo_l1_naive_smi} are absent in \cref{eq:vsmi_roots_l1} so $\lambda^*$ will not be over-determined when we come to match variational Bayes at $\eta=1$.
\end{remark}

Consider now property (P1). If $\lambda^*\in\Lambda^*$ with $\lambda^*=(\lambda^*_1,\lambda^*_2,\lambda^*_3)$ are some generic fitted variational parameters, then $q_{\lambda^*_1}(\varphi)$ cannot depend in any way on $p(Y\mid \varphi,\theta)$ at $\eta=0$, as the power posterior in \cref{eqn:lambda13-star-defn} is
\[
  p_{\pow, \eta=0}(\varphi,\tilde\theta \mid Y,Z)=p(\varphi \mid Z)\,p(\tilde\theta\mid \varphi).
\]
The $Y$ observation observation model doesn't enter \cref{eqn:lambda13-star-defn} at $\eta=0$.
Under an additional assumption on the variational family, we can remove any $p(\tilde\theta \mid \varphi)$-dependence (so $q_{\lambda^*}$ is ``completely independent of the generative model'' at $\eta=0$).

\begin{proposition}\label{prop:var_smi_is_cut_at_eta0}
  Variational \acrshort*{smi} satisfies property (P1) at $\eta=0$: If the set
  \[
    \Lambda^*_{(3)}=\{\lambda_3\in \Lambda_2: q_{\lambda^*_3}(\tilde\theta\mid \varphi)=p(\tilde\theta\mid \varphi)\}
  \]
  is non-empty and we set
  \[
    \Lambda^*_{(1)} =   \{\lambda_1\in \Lambda_1:
    \kl{q_{\lambda_1}(\varphi)}{p(\varphi \mid Z)}=d^*_{\cut}\}
  \]
  with $d^*_{\cut}$ defined in (P1) then $\Lambda^*_{(1,3)}$ defined in \cref{eqn:lambda13-star-defn} satisfies
  \[
    \Lambda^*_{(1,3)} = \Lambda^*_{(1)} \times \Lambda^*_{(3)},
  \]
  % \[
  % d(q_{(\lambda_1,\lambda_2,\lambda_3)})=\kl{q_{\lambda_1}(\varphi)}{p(\varphi \mid Z)}.
  % \]
  so $q_{\lambda^*_1}$ does not depend in any way on $p(Y \mid \varphi,\theta)$ or $p(\theta \mid \varphi)$ at $\eta=0$.
\end{proposition}
\begin{proof}
  See \cref{proof:var_smi_is_cut_at_eta0}.
\end{proof}

The point here is that the auxiliary variable $\tilde\theta$ is present only through its prior in the power posterior at the Cut, $\eta=0$, but this factor is perfectly expressed by a corresponding factor in the variational approximation, and hence doesnt enter the $\lambda_1$-variation.
The condition that there is $\lambda^*_3\in \Lambda_2$ such that $q_{\lambda^*_3}(\tilde\theta\mid \varphi)=p(\tilde\theta\mid \varphi)$ is met by choosing $\epsilon_2,\epsilon_3\sim p(\cdot\mid \varphi)$, the prior distribution for $\theta$ and $\tilde\theta$.
We can then find $\lambda^*_3$ to give $T_2(\epsilon_3;\lambda^*_3,\epsilon_1)=(\epsilon_1,\epsilon_3)$ equal to the identity map (possible in a flow-parameterised map, but not in general in \acrshort*{mfvi}).
At this $\lambda_3$-value, $\tilde\theta=\epsilon_3$ and hence $q_{\lambda^*_3}(\tilde\theta\mid \varphi)=p(\tilde\theta\mid \varphi)$. If we have a cut prior as in \cref{sec:prior_feedback} then take $\epsilon_2,\epsilon_3\sim \tilde p(\cdot)$, the Cut prior.

The variational approximation to the Cut-posterior defined in \cref{prop:var_smi_is_cut_at_eta0} is similar to that given in \cite{Yu2021variationalcut}.
We focus on flow-based parameterisations of the variational density $q_\lambda$, but apart from this our methods coincide at $\eta=0$.

We consider now property (P2).
Taking $\eta=1$, the power posterior is the Bayes posterior, so \cref{eqn:variational_bayes_standard,eqn:lambda13-star-defn} are identical optimisation problems as the $\lambda$-dependence is the same.
However, this shows that $q_{\lambda^*_1,\lambda^*_3}(\varphi,\tilde\theta)$ is variational Bayes at $\eta=1$, and we have to check that $q_{\lambda^*_1,\lambda^*_2}(\varphi,\theta)$ is variational Bayes.

\begin{proposition} \label{prop:smi-show-p2}
  Variational \acrshort*{smi} satisfies property (P2). Let
  \[
    \Lambda^*_{(1)}=\bigcup_{(\lambda^*_1,\lambda^*_3)\in \Lambda^*_{(1,3)}} \{\lambda_1^*\}.
  \]
  The set of Bayes and \acrshort*{smi} variational posteriors for $\varphi,\theta$ are the same, that is,
  \[
    \bigcup_{\lambda_1^*\in\Lambda^*_{(1)}}\bigcup_{\lambda^*_2\in \Lambda_{(2)}^*(\lambda^*_1)} \{(\lambda^*_1,\lambda^*_2)\}=B^*,
  \]
  when $\eta=1$.
\end{proposition}
\begin{proof}
  See \cref{proof:smi-show-p2}.
\end{proof}

\begin{proposition} \label{prop:smi-show-p3}
  Variational \acrshort*{smi} satisfies property (P3).
  If $p_{ \smi, \eta}\in\mathcal{Q}$ then $q_{\lambda^*}=p_{ \smi, \eta}$ for $\lambda^*\in \Lambda^*$.
\end{proposition}
% \begin{proof}
%   See \cref{proof:smi-show-p3}.
% \end{proof}
\begin{proof}\label{proof:smi-show-p3}
  This is usually immediate for standard variational methods but has to be checked here. If $p_{\smi, \eta}\in\mathcal{Q}$ then there exist $\lambda_1,\lambda_3$ such that  $q_{\lambda_1,\lambda_3}=p_{\pow, \eta}$ and $\lambda_2$ such that $q_{\lambda_2}(\theta\mid \varphi)=p(\theta \mid Y,\varphi)$ and since these choices minimise the \acrshort*{kl}-divergences in \cref{prop:var_smi_distance,prop:var_smi_lambda2} they are the optimal values, so $q_{\lambda^*}=p_{\smi, \eta}$.
\end{proof}

\subsubsection{The overall loss targeted by variational-SMI}

% The \acrshort*{smi} posterior in \cref{eqn:smi_as_F_minimiser} belongs to the family of distributions $\mathcal{F}_{\smi,\eta}$ with power-posterior marginal in $(\varphi,\tilde\theta)$.
% This has the utility of allowing controlled feedback through $\eta$, so we take a loss for $q_\lambda(\varphi,\theta,\tilde\theta)$ in \cref{eqn:q-lambda-smi-var}
% prioritising proximity to $\mathcal{F}_{\smi,\eta}$ and only secondly the \acrshort*{kl}-divergence to $p_{ \smi, \eta}$.
% We will see that this delivers all the properties (P1-3) listed above.

We have defined $\lambda^*$ in two steps, \cref{eqn:lambda13-star-defn,eqn:lambda2-star-defn-equiv} with two losses, $d(q_\lambda,\mathcal{F}_{\smi,\eta})$ and $\kl{q_{\lambda}}{p_{ \smi, \eta}}$.
We can bring this together into a single overall loss in two ways. The first is formal but useful for computation. The second is useful for understanding.

For computational purposes we define the loss $\mathcal{L}^{( \smi, \eta)}$ targeted by variational \acrshort*{smi} using the \texttt{stop\_gradient} operator $\cancel{\nabla}(\cdot)$ acting on $\varphi_{(\epsilon,\lambda_1)}$. 
%\todo{clarify}
The \texttt{stop\_gradient} operator protects the object it acts on from the gradient operator $\nabla_{\lambda}$.
Let
\begin{align} \label{eq:vsmi_loss}
  \mathcal{L}^{( \smi, \eta)}(\lambda) = \elbo_{\pow, \eta}(\lambda_1,\lambda_3) + \elbo_{\bayes \cancel{\nabla}(\varphi)}(\lambda_1,\lambda_2)
\end{align}
where
\begin{align}
  \elbo_{\pow, \eta}(\lambda_1,\lambda_3) = \E_{(\varphi,\tilde\theta)\sim q_{\lambda_1,\lambda_3}}[                & \log p_{\pow, \eta}(\varphi, \tilde\theta, Z, Y) - \log q_{\lambda_1,\lambda_3}(\varphi, \tilde\theta) ] \label{eq:elbo_modular_pow}                  \\
  \elbo_{\bayes \cancel{\nabla}(\varphi)}(\lambda_1,\lambda_2) = \E_{(\varphi,\theta)\sim q_{\lambda_1,\lambda_2}}[ & \log p( \cancel{\nabla}(\varphi), \theta, Z, Y) - \log q_{\lambda_1,\lambda_2}(\cancel{\nabla}(\varphi), \theta) ]. \label{eq:elbo_modular_stop_grad}
\end{align}
with the joint and powered joint distributions given as \cref{eq:joint_toymodel,eq:powjoint_toymodel}.
We are in effect defining the function and its derivative separately and so this loss is formal and cannot take the place of \cref{prop:smi-show-p3-extra} below in giving meaning to the variation.
However it is convenient for implementation, as the \texttt{stop\_gradient} operator is directly expressed in the automatic differentiation framework we use.

\begin{proposition}\label{prop:stop-gradient-loss}
  The set $\Lambda^*$ in \cref{defn:var-smi} is the set of solutions of $\nabla_{\lambda} \mathcal{L}^{( \smi, \eta)} = 0$ corresponding to minima.
\end{proposition}
\begin{proof}
  See \cref{proof:stop-gradient-loss}.
\end{proof}

An overall loss \emph{function} can be given as follows. Let $v\ge 0$ and
\begin{equation}\label{eqn:loss_weighted_var_smi}
  \mathcal{L}^{(v)}(\lambda)=d(q_\lambda,\mathcal{F}_{\smi,\eta})+
  v\cdot \kl{q_{\lambda}}{p_{ \smi, \eta}}
\end{equation}
denote a weighted loss which allows varying levels of priority to be put on proximity to $\mathcal{F}_{\smi,\eta}$ and approximation of $p_{ \smi, \eta}$.
%The variational-\acrshort*{smi} approximation $q_{\lambda^*}$ prioritises matching the power-posterior over the overall fit to $p_{ \smi, \eta}$.
\begin{proposition} \label{prop:smi-show-p3-extra}
  Let $\mathcal{L}^*(v)=\min_{\lambda\in\Lambda}\mathcal{L}^{(v)}(\lambda)$
  and
  \[
    \Lambda^*(v)=\{\lambda\in \Lambda: \mathcal{L}^{(v)}(\lambda)=\mathcal{L}^*(v)\}.
  \]
  Under regularity conditions on $\mathcal{F}_{smi,\eta}$ and $p_{ \smi, \eta}$ given in \cref{prop:use_IFT_show_loss_limit}, for every solution $\lambda^*\in \Lambda^*$ in \cref{defn:var-smi} and all sufficiently small $v\ge 0$ there exists a unique continuous function $\lambda^*(v)$ satisfying $\lambda^*(v)\in \Lambda^*(v)$ and \[\lim_{v\to 0}\lambda^*(v)=\lambda^*.\]
\end{proposition}
\begin{proof}
  See \cref{proof:smi-show-p3-extra}.
\end{proof}

% %\todo{connect this loss to the stop gradient loss - and give proof of cut and bayes continuous approximation at eta = 0,1}

The value of \cref{prop:smi-show-p3-extra} is that it allows us to interpret $q_{\lambda^*},\ \lambda^*\in\Lambda^*$ as minimising a proper loss function $\mathcal{L}^{(v)}$ at small $v$ (approximately). The minimum loss $\mathcal{L}^*(v)$ decreases as we expand the variational family $\mathcal{Q}$ and is zero when $p_{ \smi, \eta}\in\mathcal{Q}$, in which case $q_{\lambda^*}=p_{ \smi, \eta}$ for $\lambda^*\in \Lambda^*(v)$. In contrast, although $d^*_{\smi}$ decreases as $\mathcal{Q}$ expands, $D^*_{\smi}(\lambda_1^*)$ may increase, though must eventually go to zero when $\mathcal{Q}$ expands to include $p_{ \smi, \eta}$, by \cref{prop:smi-show-p3}. However, $\mathcal{L}^{(v)}$ is not a viable optimisation target at small $v$ because the second term in \cref{eqn:loss_weighted_var_smi} is intractable, as we saw in our discussion of \cref{eq:elbo_naive_smi}.

\subsubsection{Stochastic gradient descent for variational-SMI}
\Cref{alg:vsmi} gives our \acrlong*{sgd} method to target the \acrshort*{smi} posterior for a fixed value of the influence parameter, $\eta$.
The algorithm is based on the loss $\mathcal{L}^{( \smi, \eta)}$ in \cref{eq:vsmi_loss} and consists of a single training loop, using the \verb|stop_gradient| operator to avoid two-stage optimisation procedures. This is given for understanding. In \cref{sec:meta_posterior} and \cref{alg:v_meta_smi} we will train a ``meta-posterior'' approximating the whole family of \acrshort*{smi}-posteriors as a function of $\eta$.
 
\begin{algorithm}[tb]
  \caption{Variational Posterior approximation for $p_{ \smi, \eta}$} \label{alg:vsmi}
  \begin{algorithmic}
    %\STATE This algorithm assumes that we want to partially cut the influence of module 2 (with response $Y$) into $\varphi$.
    \STATE \textbf{Input:} $\mathcal{D}$: Data. $p(\varphi,\theta,\mathcal{D})$: Multi-modular probabilistic model. $q_\lambda=(p(\epsilon), T, \lambda)$: variational family. A value of $\eta \in [0,1]$: Influence parameter(s) for suspect module(s) \\[0.1in]
    \STATE \textbf{Output:} Variational approximation $q_{\hat\lambda}(\varphi,\theta,\tilde\theta)$ of the $\eta$-\acrshort*{smi} posterior. \\[0.1in]

    \STATE Initialise variational parameters $\lambda$
    \WHILE{\acrshort*{sgd} not converged}
    \STATE (Optional) Sample a random minibatch of data $\mathcal{D}^{(b)} \sim \mathcal{D}$.
    \FOR{$s = 1,\ldots,S$}
    \STATE Sample the base distribution, $\epsilon_s \sim p(\epsilon)$.
    \STATE Transform the sampled values $(\varphi_s, \theta_s, \tilde\theta_s) \leftarrow T(\epsilon_s; \lambda)$ as in \cref{eq:smi_transform}.
    \ENDFOR

    \STATE Compute the Monte Carlo estimate of the loss $\mathcal{L}^{( \smi, \eta)}$ in \cref{eq:vsmi_loss} and its gradients.
    \begin{equation}
      \hat{\mathcal{L}}^{( \smi, \eta)} = \widehat{\elbo}_{\pow, \eta} + \widehat{\elbo}_{\cancel{\nabla}(\varphi)}
    \end{equation}
    where
    \begin{align}
      \widehat{\elbo}_{\pow, \eta}               & = - \frac{1}{S} \sum_{s=1}^{S} \left[ \log p_{\pow, \eta}(\varphi_{s}, \tilde\theta_{s}, \mathcal{D}^{(b)}) - \log q(\varphi_{s}, \tilde\theta_{s}) \right]           \\
      \widehat{\elbo}_{\cancel{\nabla}(\varphi)} & = - \frac{1}{S} \sum_{s=1}^{S} \left[ \log p(\cancel{\nabla}(\varphi_{s}), \theta_{s}, \mathcal{D}^{(b)}) - \log q( \cancel{\nabla}(\varphi_{s}), \theta_{s}) \right]
    \end{align}

    \STATE Update $\lambda$ using the estimated gradient vector $\nabla_{\lambda}\hat{\mathcal{L}}^{( \smi, \eta)}$
    \STATE Check convergence of $q_{\lambda}(\varphi_s, \theta_s, \tilde\theta_s)$
    \ENDWHILE

    \RETURN $\hat\lambda=\lambda$

  \end{algorithmic}
\end{algorithm}


\subsection{Selecting the SMI posterior} \label{subsec:best_smi}

We now give a utility for selection of the influence parameter $\eta$.
This will depend on the goals of inference. Recall that $Y\in \mathcal{Y}^m$ and $Z\in \mathcal{Z}^n$.
In the following we take a predictive loss based on the \acrshort*{smi}-predictive distribution for independent new data $(y',z')\in \mathcal{Y}\times\mathcal{Z}$,
\begin{equation*}
  p_{\smi,\eta}(y',z' \mid Y,Z)=\int p(y',z' \mid \varphi,\theta)p_{\smi,\eta}(\varphi,\theta \mid Y,Z)\,d\varphi d\theta,
\end{equation*}
and a utility which is equivalent to the negative of the \acrshort*{kl} divergence to the true generative model for the new data $p^*(y',z')$.
This utility is the \acrfull*{elpd} \citep{Vehtari2016},
\begin{equation*}
  U(\eta)=\int p^*(y',z')\log(p_{\smi,\eta}(y',z' \mid Y,Z))\,dy' dz'.
\end{equation*}
In our variational setting, these quantities are replaced by estimates based on our variational approximation $q_{\lambda^*}$ to $p_{\smi,\eta}$.
The variational parameters are $\lambda^*(\eta)\in \Lambda^*(\eta)$, per \cref{remk:smi-defn-eta-depend}.
%We have suppressed this dependence in our notation so far.
We define the variational \acrshort*{smi} posterior predictive distribution
\begin{equation}\label{eq:post_predictive_var_smi}
  q_{\eta}(y',z')=\int p(y',z' \mid \varphi,\theta)q_{\lambda^*(\eta)}(\varphi,\theta)\,d\varphi d\theta,
\end{equation}
with corresponding utility
\begin{equation}\label{eq:ELPD_var_smi_def}
  u(\eta)=\int p^*(y',z')\log(q_{\eta}(y',z'))\,dy' dz'.
\end{equation}
%In \acrshort*{mfvi} these quantities may sometimes be computed analytically.
We estimate $u$ in \cref{eq:ELPD_var_smi_def} using the WAIC \citep{Watanabe2012}, following \cite{Vehtari2016}. See \cref{subsec:best_smi_vmp} for further details.
When the variational \acrshort*{smi} posterior predictive distribution can be calculated in closed form, $u(\eta)$ may be estimated using leave one out cross validation.
This is asymptotically equivalent to the WAIC, but will in general be too computationally demanding to compute.
%This is, asymptotically in the number of anchor profiles, equivalent to estimation using LOOCV,
%\begin{equation}\label{eq:ELPD_est_loocv}
%  \widehat{U}_{loocv}(\eta)=\sum_{i=1}^n\sum_{j=1}^m \log(p_{\smi,\eta}(Y_i,Z_j \mid Y_{-i},Z_{-j}).
%\end{equation}
In order to complete the inference, we select the optimal influence parameter \begin{equation}\label{eqn:eta-star-defn}
  \eta^*=\arg\min_{\eta} u(\eta),
\end{equation}
and return the final selected variational \acrshort*{smi} posterior,
$q_{\lambda^*(\eta^*)}(\varphi,\theta)$, for further analysis.

A number of other procedures have been given for selecting the influence in a pure power-posterior setting. \cite{wu-martin-20,wu-martin-21} introduce a new method and summarise and compare a selection of methods, reflecting different priorities in the inference and corresponding utilities.
If our goal is parameter estimation, then a utility that directly targets parameter estimates, rather than predictive distributions, will be preferred.
In recent work, \cite{chakraborty22_smiLF} select $\eta$ as the most Bayes-like (i.e., the largest) value that does not show goodness-of-fit violation with the Cut.
%, as measured by the distribution of the \acrshort*{kl} divergence between the $\varphi$ posteriors at $\eta>0$ and $\eta=0$, taken in the sampling distribution of the data $Y$.
\cite{Carmona2022spatial} use a utility tailored to their inference objectives.
They have data in which $\theta$ is a high dimensional vector, and some of the components of $\theta$ are directly measured.
They use the posterior mean square error for prediction of known $\theta$-components in a LOOCV framework to select $\eta$, linking $\eta$-selection to success in parameter estimation.
%This seems ideal, when information of this sort is available.

\section{The Variational Meta-Posterior}\label{sec:meta_posterior}

In order to select a posterior from the family of variational \acrshort*{smi} posteriors we need the fitted variational parameters $\lambda^*(\eta)$ as a function of $\eta$ in order to estimate the selection criterion $u(\eta)$ in \cref{eq:ELPD_var_smi_def} as a function of $\eta$ and select an optimal $\eta$-value in \cref{eqn:eta-star-defn} and the variational \acrshort*{smi} posterior $q_{\lambda^*(\eta^*)}$.

Up to this point $\eta\in [0,1]$, has been a scalar.
When $\eta$ is scalar, we can fit the variational posterior independently at a lattice of $\eta$-values, estimate the \acrshort*{elpd} at each value, smooth the estimated \acrshort*{elpd} values over $\eta\in [0,1]$ and select the $\eta$-value maximising this function.
However, when we analyse multi-modular models with multiple misspecified modules, the dimension of $\eta$ grows with the number of bad modules and so independent fitting is both inefficient and computationally prohibitive.
In this section we give two parameterisations of the \acrfull*{vmp}, $q_{\lambda(\alpha,\eta)}$ and $q_{\alpha,\eta}$. In the former, based on a ``\acrshort*{vmp}-map'', the parameters $\lambda$ of the \acrshort*{nf} are themselves parameterised as functions of $\eta$ with parameters $\alpha$. In the latter, based on a ``\acrshort*{vmp}-flow'', $\eta$ is treated as an additional input to the \acrshort*{nf} alongside $\epsilon$, with its own additional flow parameters $\mu$, and $\alpha=(\lambda,\mu)$. %We learn  $q_{\lambda^*(\alpha^*,\eta)}$ and $q_{(\lambda^*,\alpha^*),\eta}$ in a single end-to-end training framework.

\subsection{Motivation and definition}

The \acrshort*{smi}-posterior varies continuously with $\eta$.
%, in the sense that
%\[
%  \lim_{\delta\to 0} \kl{p_{\smi,\eta}}{p_{\smi,\eta+\delta}}=0,
%\]
Expanding the \acrshort*{kl} divergence at $\eta+\delta$,
\[
  \kl{p_{\smi,\eta}}{p_{\smi,\eta+\delta}}=-\delta E_{\pow, \eta}\left(\log(p(Y \mid \varphi,\tilde\theta))\right)+\log\left(E_{\pow, \eta}(p(Y \mid \varphi,\tilde\theta)^{\delta})\right),
\]
and this is continuous and has continuous derivatives in $\delta$ if the integrals exist. This motivates flow- and map- parameterisations of the variational densities $q_{\lambda(\alpha,\eta)}$ and $q_{\alpha,\eta}$ which are continuous in the same sense.

\subsubsection{The VMP-map}
Continuity holds in a stronger sense. Under regularity conditions, a continuous sequence of solutions $\lambda^*(\eta)\in \Lambda^*(\eta)$ passes through any point $\lambda^*\in \Lambda^*(\eta^*)$.
%We applied the Implicit Function Theorem in \cref{prop:use_IFT_show_loss_limit} to show the continuity of the roots $\lambda^*(v)$ of $\nabla_{\lambda}\mathcal{L}^{(v)}$ at $v=0$.
Applying the Implicit Function Theorem (as in \cref{prop:use_IFT_show_loss_limit}), to the $\eta$-dependence of the roots of the functions on the LHS of \cref{eqn:lam-star-roots1a,eqn:lam-star-roots1b} we can show that, for every $\lambda^*\in\Lambda^*(\eta^*)$, there is a unique continuous function $\lambda^*(\eta)$ satisfying $\lambda^*(\eta)\in\Lambda^*(\eta)$ for $\eta$ in an open neighborhood of $\eta^*$ and satisfying $\lambda^*(\eta^*)=\lambda^*$. The regularity conditions require the functions on the LHS of \cref{eqn:lam-star-roots1a,eqn:lam-star-roots1b} to be continuously differentiable in $\lambda$ and $\eta$, and the Jacobians of those functions (in $(\lambda_1,\lambda_3)$ and $\lambda_2$) to be invertible at $\lambda=\lambda^*$.
% \begin{align*}
%   \kl{p_{\smi,\eta}}{p_{\smi,\eta}+\delta}
%   & =
%   -\int p_{\pow, \eta}(\varphi,\tilde\theta \mid Y,Z) \log(p(Y \mid \varphi,\tilde\theta)^\delta) d\varphi d\tilde \theta
%   \\
%   & \qquad +\quad
%   \log\left(\frac{\int p(Z\mid \varphi) p( Y \mid \varphi, \tilde \theta )^{\eta+\delta} \;  p(\varphi,\tilde\theta) d\varphi d\tilde \theta}{\int p(Z\mid \varphi) p( Y \mid \varphi, \tilde \theta )^\eta \;  p(\varphi,\tilde\theta)d\varphi d\tilde \theta} \right)
%   \\
%   & =-\delta E_{\pow, \eta}\left(\log(p(Y \mid \varphi,\tilde\theta))\right)+\log\left(E_{\pow, \eta}(p(Y \mid \varphi,\tilde\theta)^{\delta})\right)
% \end{align*}
% and the limit is zero whenever the integrals exist.

This motivates a low dimensional reparameterisation of $\lambda$ which approximates $\lambda^*(\eta)$. Let $\lambda(\alpha,\eta)=f_\alpha(\eta)$, where $\alpha\in A$ is a vector of real parameters and let
%lots cut here - I put it in the ''notes'' file
\begin{equation}\label{eq:f-alpha-vmp-map-defn}
  f_\alpha: H \rightarrow \Lambda
\end{equation}
be a continuously differentiable mapping parameterised by $\alpha$. We refer to $f_\alpha$ as the \textbf{\acrshort*{vmp}-map} and
%variational parameters that approximate the entire $\mathcal{P}_{\smi}$ family.
define a \acrlong*{vmp} as a family of distributions 
\[\mathcal{Q}_{H,\alpha} = \{ q_{f_\alpha(\eta)} \;,\; \eta \in H \},\quad \alpha\in A.\] 
%and parameterised by $f_\alpha$ at fixed $\alpha\in A$.,
%\begin{equation}\label{eq:meta_post_QHA_defn}
%  \mathcal{Q}_{H,\alpha} = \{ q_{f_\alpha(\eta)} \;,\; \eta \in H \}.
%\end{equation}
%Here $\mathcal{Q}_{H,\alpha} \subset \mathcal{Q}$ is a surface of dimension $\dim(H)$ in $\mathcal{Q}$. When we vary $\alpha$ we vary $\lambda(\eta)$ at all $\eta$, so we vary $\alpha$ seeking a surface $\mathcal{Q}_{H,\alpha^*}$ with the property that $q_{f_{\alpha^*(\eta)}}\in \mathcal{Q}_{H,\alpha^*}$ is a good approximation to $p_{\smi,\eta}$ for every $\eta\in H$ at the same time.

There is a question of how the $\alpha$ parameters should contribute to the different components of $\lambda$. We take $\alpha=(\alpha_1,\alpha_2,\alpha_3)$ and
\[
  f_\alpha(\eta)=(f^{(1)}_{\alpha_1}(\eta),f^{(2)}_{\alpha_2}(\eta),f^{(3)}_{\alpha_3}(\eta))
\]
where $f^{(1)}_{\alpha_1}: H\to \Lambda_1$
and $f^{(2)}_{\alpha_2},f^{(3)}_{\alpha_3}: H\to \Lambda_2$ and set $\lambda_k(\alpha_k,\eta)=f^{(k)}_{\alpha_k}(\eta),\ k=1,2,3$. This gives $f_\alpha: H\to \Lambda$ as before, but breaks up the dependence as
\begin{equation}\label{eq:vmp-map-transform-variables-alpha123}
    (\varphi_{(\lambda_1(\alpha_1,\eta),\epsilon)},\theta_{(\lambda_2(\alpha_2,\eta),\epsilon)},\tilde\theta_{(\lambda_3(\alpha_3,\eta),\epsilon)}) \sim q_{f_\alpha(\eta)}.
\end{equation}
Changing $\alpha_1$ to improve the fit to the $\varphi$ distribution at one $\eta$ does not affect the $\theta$ distribution at another $\eta$-value, though it will affect the $\varphi$ distribution there. 
%This parameterisation converged more rapidly to lower loss solutions than a parameterisation which took a single set of parameters $\alpha=\alpha_k,\ k=1,2,3$ shared across the parameters.

A very expressive \acrshort*{vmp}-map may be undesirable due to a bias-variance trade off in the estimation of $\lambda^*$. The estimates $\hat\lambda$ of $\lambda^*$ output by \cref{alg:vsmi} are estimated independently over $\eta$ and will not in general lie in $\Lambda^*(\eta)$. Reparameterising with $f_\alpha$ and estimating $\hat\alpha$ for best fit across $\eta\in H$ smooths the output $\hat\lambda(\eta)=f_{\hat\alpha}(\eta),\ \eta\in H$ at the price of some potential bias. Properties (P1-3) hold only approximately on both outputs.

\subsubsection{The VMP-flow}

When we parameterise the \acrshort*{vmp} with a \acrshort*{nf}, we model the $\eta$-dependence of the variational densities $q$. We can parameterise the function $\lambda^*(\eta)$ using a \acrshort*{vmp}-map. Alternatively we can add an $\eta$-input to the maps $T_1,T_2$ (technically an extra conditioner, like $\epsilon_1$). The flow architecture is expanded with extra nodes and weight parameters $\mu=(\mu_1,\mu_2,\mu_3)$ with $\mu\in M$ say. The map $T$ is continuous in its inputs so $q$ will be continuous in $\eta$. Let $\alpha=(\alpha_1,\alpha_2,\alpha_3)$ with $\alpha_k=(\lambda_k,\mu_k),\ k=1,2,3$ and $\alpha\in A$ where now $A=\Lambda\times M$. The transformations $T_1,T_2$ with input $\eta$ are given in terms of the conditioners and transformers of the \acrshort*{nf} in \cref{eq:meta_new_map_phi} to \cref{eq:meta_new_map_theta_tilde} in \cref{sec:nf}. Formally,
\begin{align}
  \varphi_{(\alpha_1,\eta,\epsilon)}      & =T_1(\epsilon_1;\alpha_1,\eta)\nonumber                                                                                                                     \\
  \theta_{(\alpha_2,\eta,\epsilon)}       & =T_2(\epsilon_2; \alpha_2, (\eta,\epsilon_1))\nonumber                                                                                                        \\
  \tilde\theta_{(\alpha_3,\eta,\epsilon)} & =T_2(\epsilon_3; \alpha_3, (\eta,\epsilon_1))\nonumber                                                                                                        \\
  T(\epsilon;\alpha,\eta)                & = \left( T_1(\epsilon_1;\alpha_1,\eta))  ,\; T_2(\epsilon_2; \alpha_2, (\eta,\epsilon_1))  ,\; T_2(\epsilon_3; \alpha_3, (\eta,\epsilon_1))  \right).\label{eq:smi_transform_new_vmp}
\end{align}
We call this extended flow mapping $T$ with input $\eta$ a {\bf \acrshort*{vmp}-flow}. In terms of the new map, the variational densities are
\begin{align}\label{eqn:q-lambda-smi-var-meta-new}
  q_{\alpha,\eta}(\varphi, \theta, \tilde\theta) =q_{\alpha_1,\eta}(\varphi)q_{\alpha_2,\eta}(\theta\mid \varphi)q_{\alpha_3,\eta}(\tilde\theta\mid \varphi),
\end{align}
simply replacing $\lambda_k\leftarrow\alpha_k,\ k-1,2,3$ and making the $\eta$-dependence explicit as it is a flow input. This gives a second
\acrlong*{vmp} as the family of distributions 
\[\mathcal{Q}_{H,\alpha} = \{ q_{\alpha,\eta} \;,\; \eta \in H \}, \quad\alpha\in A.\] 

%In contrast to the \acrshort*{vmp}-map, where we smooth the variational parameters $\lambda^*(\eta)$, the optimal parameters $\alpha^*$ are not functions of $\eta$ as we enforce smoothness directly on the density $q_{\alpha^*,\eta}$ through the argument $\eta$ passed as input to the flow defining $q_{\alpha,\eta}$. 

\subsection{Learning the Variational Meta-Posterior for SMI}\label{subsec:learning_vmp_map}

The \acrlong*{vmp} for \acrshort*{smi} is characterised by a pair $(\mathcal{P}_{\smi}, \mathcal{Q}_H)$, where
$
\mathcal{P}_{\smi}=\{p_{\smi,\eta}: \eta\in H\}
$
is the family of \acrshort*{smi} posteriors indexed by $\eta$ which we want to approximate and
$ \mathcal{Q}_{H}$ is the family of all available \acrlongpl*{vmp}, which can be written $\mathcal{Q}_{H} = \cup_{\alpha\in A}  \mathcal{Q}_{H,\alpha}$
for both \acrshort*{vmp}-map and \acrshort*{vmp}-flow based \acrshortpl*{vmp}.
%is the family of all available \acrlongpl*{vmp}.
% The first element, $\mathcal{P}_{\smi}$, is defined by a joint generative model and a set of cuts identifying modules which may be misspecified, that is, by a graphical model like \cref{fig:toy_multimodular_model}. The second element, $\mathcal{Q}_H$ is given in \cref{eq:meta_post_QHA_defn} in terms of densities $q\in $ (see \cref{subsec:vsmi}) and the \acrshort*{vmp}-map $f_\alpha(\eta)$.

 







% There is a question of how the $\alpha$ parameters should contribute to the different components of $\lambda$. If $\lambda_1,\lambda_2,\lambda_3$ share $\alpha$-parameters, the random variables are
% % \[
% % (\varphi_{(\epsilon_1,\lambda_1)},\theta_{(\epsilon_2,\lambda_2)},\tilde\theta_{(\epsilon_3,\lambda_3)})\sim q_\lambda
% % \]
% % are replaced by
% \[
% (\varphi_{(\lambda_1(\alpha,\eta),\epsilon)},\theta_{(\lambda_2(\alpha,\eta),\epsilon)},\tilde\theta_{(\lambda_3(\alpha,\eta),\epsilon)})\sim q_{f_\alpha(\eta)}.
% \]
% Changing $\alpha$ to improve the fit to the $\varphi$ distribution at one $\eta$ affects the fit to the $\theta$ distribution at another $\eta$-value. This creates problems of convergence and bias. It is also an unnecessary violation of property (P1) as $\lambda_1(\alpha^*)$ depends on $Y$ at $\eta=0$.


% We found that a single shared set of $\alpha$ parameters expressed the map from $\eta$ to $\lambda^*$ well in simple problems. Higher dimensional \acrshort*{nf}-parameterisations (for example in \cite{Carmona2022spatial}) required separate \acrshort*{vmp}-maps for each vector of parameters $\varphi,\theta,\tilde\theta$.
 


%\subsubsection*{Estimating the parameters of the \acrshort*{vmp}-map and -flow}

In this section we give losses for estimation of $\alpha^*$ in the \acrshort*{vmp}-map and \acrshort*{vmp}-flow. Let $\eta_{1:R}=(\eta_r)_{r=1,...,R}$ be a given lattice of $\eta$-values and let
\[\rho(\eta)=\frac{1}{R}\sum_{r=1}^R \delta_{\eta_r}(\eta),\ \eta\in H.\]
The values in $\eta_{1:R}$ would ideally be concentrated around $\eta^*$. As this isn't known in advance, concentrating them near the points $\eta_{1:R}\in \{0,1\}^R$ is a useful rule as $\lambda^*(\eta)$ often varies rapidly with $\eta$ near Cut and Bayes. Adaptive sequential estimation and maximisation of $u(\eta)$ may be of interest in future work.

We take a meta-\acrshort*{smi} loss weighted across $\eta\in \eta_{1:R}$. For the \acrshort*{vmp}-map this is
\begin{align}
  \mathcal{L}^{(\msmi-map)}(\alpha) & =\E_{\eta\sim\rho}\left(\mathcal{L}^{(\smi,\eta)}(f_{\alpha}(\eta)\right)\nonumber                       \\
                                & =\frac{1}{R}\sum_{r=1}^R \mathcal{L}^{(\smi,\eta_r)}(f_{\alpha}(\eta_r)) \label{eq:meta_smi_target_loss}
\end{align}
where $\mathcal{L}^{(\smi,\eta)}(\lambda)$ is defined in \cref{eq:vsmi_loss} and we have taken $\lambda=f_{\alpha}(\eta)$ in order to enforce the parameterisation at each $\eta\in \eta_{1:R}$. For the \acrshort*{vmp}-flow the loss is
\begin{align}\label{eq:vsmi_loss-meta-new-main}
  \mathcal{L}^{(\msmi-flow)}(\alpha)  =\E_{\eta\sim\rho}\left(\mathcal{L}^{(\msmi-flow,\eta)}(\alpha)\right) 
\end{align}
where $\mathcal{L}^{(\msmi-flow,\eta)}(\alpha)$ is obtained by substituting $\varphi_{(\alpha_1,\eta,\epsilon)}$ etc into $\mathcal{L}^{(\smi,\eta)}$ in \cref{eq:vsmi_loss} and is defined in detail in \cref{eq:vsmi_loss-meta-new} in \cref{sec:nf}.

In the \acrshort*{vmp}-flow the optimal variational parameters $\lambda^*$ don't depend on $\eta$ and this seems to give relatively more rapid and stable convergence in \acrshort*{sgd} targeting $\mathcal{L}^{(\msmi-flow)}(\alpha)$ compared to \acrshort*{sgd} targeting in $\mathcal{L}^{(\msmi-map)}(\alpha)$ (given in \cref{alg:v_meta_smi}). It is a relatively ``lightweight'' parameterisation, as the dimension of $\alpha$ in the \acrshort*{vmp}-flow is quite a bit smaller than that of $\mu$ in the \acrshort*{vmp}-map.

In order to estimate $\alpha^*=\arg\min_{\alpha\in A} \mathcal{L}^{(\msmi)}(\alpha)$ (dropping the -map and -flow distinction, and ignoring non-uniqueness for brevity) and fit the \acrshort*{vmp}, we apply \acrshort*{sgd}, simply replacing $\mathcal{L}^{(\smi,\eta)}(\lambda)$ \cref{alg:vsmi} with $\mathcal{L}^{(\msmi)}(\alpha)$ in \cref{alg:v_meta_smi} in \cref{sec:sgd-for-meta-losses}, and updating $\alpha$ with the gradient $\nabla_\alpha \mathcal{L}^{(\msmi)}$ instead of updating $\lambda$ with the gradient $\nabla_{\lambda} \mathcal{L}^{(\smi,\eta)}$. We implement this using \acrfull*{svi}. We take a continuous density $\rho$ and sample a new batch $\eta_{1:R}$ of $\eta$-values at each pass of the \acrshort*{sgd} algorithm. We approximate the family  $\mathcal{P}_{\smi}$ in a single end-to-end optimisation, propagating the loss function gradients through the \acrshort*{vmp}-map or -flow using automatic differentiation. See \cref{alg:v_meta_smi} in \cref{sec:sgd-for-meta-losses}.

When $\dim(H)=C$ ($C$ influence parameters for $C$ cuts) we have vectors $\eta_r=(\eta_{1,r},...,\eta_{C,r}),\ r=1,...,R$. We defined $\rho$ for resampling purposes as $\eta_{c,r}\sim \mbox{beta}(a,1),\ c=1,...,C$ with $a<1$ independently for each component of $\eta$, for example $a=0.2$. This concentrates sampled $\eta$ at the Cut boundaries of $H$ as noted above.
%Specifically, we sample $\eta$ from a vector of independent Beta distributions.

\subsection{Maximising the ELPD using the Variational Meta-Posterior}\label{subsec:best_smi_vmp}

The \acrshort*{vmp} allows us to produce posterior samples for any given $\eta \in H$ efficiently. This helps us find the best influence parameter $\eta^*$ (see \cref{subsec:best_smi}), as we can estimate the utility function $u(\eta)$ accurately in fractions of a second and compare it across the family of \acrshort*{smi} posteriors. In settings with a single cut, maximising $u(\eta)$ may be as simple as linear search, but in the case of many potential cuts, with a higher-dimensional $H$ space, we require more elaborate search strategies.

In our case the utility $u(\eta)$ is the \acrshort*{elpd} and we estimate this (its negative) using the WAIC estimator given in \cite{Vehtari2016} and minimise the WAIC over $\eta\in H$ with \acrshort*{sgd}. Denote by $\psi_{\hat\alpha,\eta,\epsilon}=(\varphi_{\hat\alpha,\eta,\epsilon},\theta_{\hat\alpha,\eta,\epsilon},\tilde\theta_{\hat\alpha,\eta,\epsilon})$ a full sample parameter vector $\psi_{\alpha,\eta,\epsilon}\sim q_{\hat\alpha,\eta}$ from a fitted \acrshort*{vmp}-flow evaluated at $\eta$. Denote by $\psi_{\alpha,\eta,\epsilon_{1:J}}$ a set of $J$ iid samples from $q_{\alpha,\eta}$ and let $\mathcal{D}=(Y,Z)$ denote the data. The WAIC is a function $\widehat {-\elpd}(\psi_{\alpha,\eta,\epsilon_{1:J}},D)$ of the samples and data \citep{Vehtari2016}. In order to implement \acrshort*{sgd} we need function evaluations and derivatives of $\widehat {-\elpd}(\eta)$ wrt $\eta$ (keeping only the $\eta$ dependence). Function evaluations are very fast. In our JAX/TensorFlow setup \citep{deepmind2020jax,Dillon2017tfp} we get $\eta$-derivatives using automatic differentiation through the functions in the \acrshort*{elpd} and all the way into $\psi_{\alpha,\eta,\epsilon_{j}},\ j=1,...,J$.
This can be seen in operation in our online code.

The main difficulty (in our example in \cref{subsec:exp_rnd_eff} where $\eta\in [0,1]^{N}$) is that the \acrshort{elpd} is clearly non-convex (from our plots), and quite flat when the $\eta$ components are all close to one. We therefore initialise \acrshort*{sgd} using an (informed) greedy backward search. This uses backwards selection over cuts starting from Bayes, cutting the module which gives the greatest reduction in $\widehat {-\elpd}$ and stopping when no decrease is possible. 
% For $I\subseteq \{1,...,N\}$ let $\mathcal{I}(I)=\{I'\subseteq \{1,...,N\}: I\subset I'\}$ be the set of all subsets of $\{1,...,N\}$ containing $I$ as a strict subset. Denote by $O_{I}$ a vector with entries $O_i=\mathbb{I}_{i\not\in I}$. Bayes is $\eta=O_{\emptyset}$ and Cut is $\eta=O_{1:N}$.
% Let $I=\emptyset$ . We iterate, (1) $w=\widehat {-\elpd}(O_I)$ then (2) $I\leftarrow \arg\min_{I'\in \mathcal{I}(I)}\widehat {-\elpd}(O_I)$ until $\widehat{\elpd}(I)\ge w$.



% Assuming that $U(\eta)$ can be evaluated fast once we have a sample of the corresponding \acrshort*{smi} posterior $p_{\smi,\eta}$ (e.g.)

% \cref{eqn:eta-star-defn}
% Assuming that we can evaluate 


% In \cref{subsec:exp_epidemiology} we use a greedy algorithm, starting with Bayes $\eta_{(1)}=(1,\ldots,1)$ and comparing $\hat{u}(\eta_{(1)})$ with the best candidate produced after cutting one of modules ($N$ candidates), if the new candidate improves $\hat{u}$, we take it as our current  this process  \acrshort*{smi} posteriors which cut one of the $N$ modules,  is  which compares the ELPD of the  of  the initialise \acrshort*{sgd}.


% It is convenient to define a nomenclature to identify combinations of the functional form of $f_{\alpha}$ and the type of \acrlong*{nf} characterizing the \acrlong*{vmp}.
% We use \textbf{\acrshort*{vmp}-[flow]-[map]}.
% For example, we refer as \textbf{\acrshort*{vmp}-\acrshort*{nsf}-\acrshort*{mlp}} when using a \acrlong*{nsf} interpolated with a \acrlong*{mlp}.

% It is of interest to analise the smoothness of the counterpart family of variational approximations to
% Regarding their variational As for the  variational family is not neccesaryly a consequence of smoothness in the target

% The optimal candidate among the distributions in such family is often the one that maximises a utilitity function, such as a predictive score.

% Instead of optimizing the variational parameters of a single distribution, we define a mapping that produce variational parameters as a function of the family-defining quantity, in our case $\eta$.

% We make an assumption relating elements in the variational family.
% In the case of the \acrshort*{smi} posteriors we expect that the variational parameters are a smooth function of $\eta$.

\section{Experiments} \label{sec:experiments}

Our experiments illustrate the following points: Variational-\acrshort*{smi} with a \acrshort*{nf} and with or without a \acrshort*{vmp} accurately approximates \acrshort*{smi}-posteriors at all $\eta$ in the examples we consider; the \acrshort*{vmp}-framework allows us to select an influence parameter \emph{vector} $\eta^*\in H$, where $H=[0,1]^C$ and $C$ is the number of cuts, at values of $C$ which are completely out of reach for one-$\eta$-at-a-time \acrshort*{mcmc} or variational-\acrshort*{smi}.
\acrshort*{mcmc} is fine if we want to check the \acrshort*{vmp} at a handful of $\eta$ values. %The flow and \acrshort*{vmp} neural architectures and optimisation settings used are given in \cref{sec:experiments_extra}.
%\todo{What are the points we want to make? Could also plot the losses and show how it goes up and down etc.}

We use two examples which have become default test cases \citep[e.g.][]{Plummer2015cut,Jacob2017together,Carmona2020smi,Liu2020sacut,Nicholls2022smi}. In the first epidemiological example taken from \cite{Plummer2015cut}, we show that \acrshort*{vmp} agrees with variational-\acrshort*{smi} and nested-\acrshort*{mcmc} (which serves as ground truth) across a range of $\eta$-values and in particular at $\eta=0,1$, Cut and Bayes.  An expressive variational family is needed, so while \acrshortpl*{nf} are effective, \acrshort*{mfvi} fails.  In the second random effects example taken from \cite{Jacob2017together}, we illustrate variational-\acrshort*{smi} with multiple cuts,
and compare different methods for estimating the utility $u(\eta)$ which we take as the \acrshort*{elpd} throughout.
In a companion paper, \cite{Carmona2022spatial}, we give an analysis of a spatial model where \acrshort*{mcmc} at even one $\eta$-value is infeasible. This (third) extended example illustrates careful choice of utility for $\eta$-selection, as well as being of independent interest in the application domain.

Our variational family has \acrfull*{nsf} transformers \citep{Durkan2019neural} with \acrshort*{mlp} conditioners in eight coupling layers \citep{Dinh2016realnvp}, with a \acrshort*{mfvi} analysis for comparison. See \cref{sec:nf} for this terminology. We found this arrangement gave an expressive transformation $T=(T_1,T_2,T_2)$ that was easily trained and worked for both examples. Code to replicate all results in this section is available as an open-source repository \footnote{\url{https://github.com/chriscarmona/modularbayes}}.

Our implementation is based on DeepMind JAX Ecosystem \citep{deepmind2020jax} and TensorFlow Probability \citep{Dillon2017tfp}. Experiments were carried out using a single Cloud TPU machine type v3-8.
Qualitative runtimes to approximate a single \acrshort*{smi} posterior for the Random Effects example using our favoured \acrshort*{nf} were in the range of $10$ minutes and sampling 10000 iid samples takes less than a second. This total time is similar to the time to obtain one correlated sample of size $10,000$ at one $\eta$-value using nested \acrshort*{mcmc}. Training the \acrshort*{vmp} required between $0.5$ and $2$ hours. However, this training time is compensated by a significant reduction in the search for $\eta^*$, as we can generate samples from any $q_{\hat\alpha,\eta}$ and estimate $u(\eta)$ (the WAIC) in a fraction of a second. Optimisation using greedy initialisation and \acrshort*{sgd} requires thousands of WAIC-estimates and took about 5 minutes using the \acrshort*{vmp}, whereas each of these estimates would take 10 minutes using \acrshort*{mcmc}. Further, we cannot get gradients by automatic differentiation in nested \acrshort*{mcmc}.

\subsection{Epidemiological Model} \label{subsec:exp_epidemiology}

We revisit the well-known epidemiological model for the correlation between \acrfull*{hpv} prevalence and cervical cancer incidence \citep[see][for details]{Maucort-Boulch2008, Plummer2015cut}.
In this modular model a small ``expensive'' prospective trial controlling sample selection from the target population gives straightforward statistical modelling. A second much larger retrospective data set contains information about population parameters, but was gathered with little control over sample selection bias. This sort of data synthesis appears frequently. For example, the simplest Covid prevalence model in \cite{Nicholson2021covid}, which brings together sample survey data and walk-in testing results, belongs to this class.

The data consist of four variables observed from $n=m=13$ groups of women from $n$ different countries.
The model has two modules, a Binomial distribution for the number $Z_i,\ i=1,...,n$ of women infected with HPV in a sample of size $N_i$ from the $i$'th group and a Poisson distribution for the number of cancer cases $Y_i$ during $T_i$ women-years of followup. That is,
\begin{gather*}
  Z_i \sim Binomial(N_i, \varphi_i ). \nonumber \\
  Y_i \sim Poisson( \mu_i ) \\
  \mu_i=T_i \exp( \theta_1+\theta_2 \varphi_i ),\quad i=1,\ldots,13.\nonumber
\end{gather*}
Following previous authors, the parameter spaces are $\theta\in [0,\infty)^2$ and $\varphi\in [0,\infty)^n$ and the priors are truncated independent normal priors with variance 1000.


% Those authors use a Cut posterior for analysis. The HPV data have this structure: $Z$ is prospective and representative; $Y$ is retrospective and biased.
% The graphical model in \cref{fig:toy_multimodular_model} applies in these settings.
% Restricting the feedback from the Poisson module over the posterior of $\varphi$ is of interest, so the location and ``direction'' of the Cut is the same as well.
% The \acrshort*{smi} posterior introduces an influence parameter, $\eta$, which controls the contribution from the retrospective study (the Poisson module) into the prospective study (the Binomial module).
%In our example the \acrshort*{smi} analysis in \cite{Carmona2020smi} selects the Cut-posterior, so the data from the retrospective study should not be used in estimation of the parameters $\varphi$ of the prospective study.

Our variational approximation takes an $L=17$-dimensional independent standard Normal $p(\epsilon)=N(\epsilon; 0,\mathbb{I}_{17})$ as our \emph{base distribution} ($L_1=13$ elements for $\varphi$, $L_2=2$ for $\theta$, and so also $L_2=2$ for $\tilde\theta$).
% \begin{equation*}
%   \epsilon \sim N(0,\mathbb{I}_{17}),
% \end{equation*}
% This is transformed to give the \acrshort*{smi} posterior,
% \[
% \left(\varphi_{(\lambda_1,\epsilon)}, \theta_{(\lambda_2,\epsilon)}, \tilde\theta_{(\lambda_3,\epsilon)} \right) = \Big(T_1( \epsilon_{1:13} ; \lambda_1 ),\; T_2( \epsilon_{14:15} ; \lambda_2, \epsilon_{1:13} ),\; T_2( \epsilon_{16:17} ; \lambda_3, \epsilon_{1:13} )\Big)
% \]
% \begin{align*}
%   \varphi_{(\lambda_1,\epsilon)}      & = T_1( \epsilon_{1:13} ; \lambda_1 ) \\
%   \theta_{(\lambda_2,\epsilon)}       & = T_2( \epsilon_{14:15} ; \lambda_2, \epsilon_{1:13} ) \\
%   \tilde\theta_{(\lambda_3,\epsilon)} & = T_2( \epsilon_{16:17} ; \lambda_3, \epsilon_{1:13} ) \\
% \end{align*}
The \acrshort*{nf}-conditioner in $T_2$ (\cref{sec:nf}) takes $\epsilon_{1:13}$ as an input, allowing correlation between $\varphi$ and $\theta$ and between $\varphi$ and $\tilde\theta$, and conditional independence between $\theta$ and $\tilde\theta$ given $\varphi$. 

Samples from a \acrshort*{vmp} $q_{f_{\hat\alpha}(\eta)}$ fitted using \acrshort*{vmp}-\acrshort*{nsf}-\acrshort*{mlp} and a \acrshort*{vmp}-map are shown in  \cref{fig:epidemiology_vmp} (at $\eta\in \{0.001,0.1,1\}$, corresponding to the Cut, Bayes, and a value of $\eta$ ``halfway'' between the two \footnote{for illustration we take 0.1 instead of 0.5 because posteriors with $\eta	\gtrsim 0.2 $ are very similar}). ``Ground truth'' \acrshort*{smi} distributions obtained using nested-\acrshort*{mcmc} \citep{Plummer2015cut,Carmona2020smi} are shown in \cref{fig:epidemiology_mcmc} for comparison. Samples from the \acrshort*{vmp} $q_{\hat\alpha,\eta}$ fitted using a \acrshort*{vmp}-flow and samples from variational-\acrshort*{smi} distributions $q_{\hat\lambda(\eta)}$ (estimating $\lambda^*(\eta)$ separately at each $\eta$ without a \acrshort*{vmp}) are essentially identical to the variational $q_{f_{\hat\alpha}}(\eta)$ posteriors and are omitted. The good agreement here to \acrshort*{mcmc} shows both that the training losses $\mathcal{L}^{(\smi,\eta)}$ and $\mathcal{L}^{(\msmi)}$ we wrote down in \cref{subsec:vi_multi} and \cref{sec:meta_posterior} are doing their job and enforcing a good fit to $p_{\smi,\eta}$ over all $\eta\in [0,1]$, and at the same time interpolating variational approximations with good inferential properties to the Cut (no $Y$-module feedback) and Bayes (full feedback) posteriors.

In \cref{sec:experiments_extra} we include a comparison with \acrshort*{mfvi} (see \cref{fig:epidemiology_mfvi}). This demonstrates its failure, under-dispersed relative to the target $p_{\smi,\eta}$, and demonstrates the advantages of using an expressive flow-family.
In this example we omit the final stage of an \acrshort*{smi} analysis, that is, we do not estimate the \acrshort*{elpd} and select $\eta^*$ and $p_{\smi,\eta^*}$. As the variational posteriors match nested \acrshort*{mcmc}, this part of the analysis is the same as that given in \cite{Carmona2020smi} (though faster, as sampling our flow is \emph{much} faster than nested-\acrshort*{mcmc}).

\begin{figure}[!htb]
  \centering
  \includegraphics[width=0.48\textwidth]{images/epidemiology/spline/vmp_mlp/epidemiology_phi_eta_1.000}
  \includegraphics[width=0.37\textwidth]{images/epidemiology/spline/vmp_mlp/epidemiology_theta_eta_1.000}
  \includegraphics[width=0.48\textwidth]{images/epidemiology/spline/vmp_mlp/epidemiology_phi_eta_0.100}
  \includegraphics[width=0.37\textwidth]{images/epidemiology/spline/vmp_mlp/epidemiology_theta_eta_0.100}
  \includegraphics[width=0.48\textwidth]{images/epidemiology/spline/vmp_mlp/epidemiology_phi_eta_0.001}
  \includegraphics[width=0.37\textwidth]{images/epidemiology/spline/vmp_mlp/epidemiology_theta_eta_0.001}

  \caption[Epidemiology model VMP]{
    Samples from the \acrlong*{vmp} in \cref{subsec:exp_epidemiology}, obtained using \acrshort*{vmp}-\acrshort*{nsf}-\acrshort*{mlp}.
    Rows correspond to influence levels $\eta=(0.001,0.1,1)$.
    The left column shows the relation between HPV prevalence ($\varphi$) and cervical cancer incidence $\mu_i=\theta_1+\theta_2\varphi_i$ colored by group $i=1,...,n$ for the $n=13$ groups in the data.
    The right column shows how the joint distribution of slope ($\theta_1$) and intercept ($\theta_2$) varies with $\eta$.
    All samples are produced using a single \acrshort*{vmp}-map.
    %In the appendix (\cref{fig:epidemiology_mcmc}) we include a similar figure with samples produced by \acrshort*{mcmc} sampling to illustrate the accuracy of the variational approximation.
  }
  \label{fig:epidemiology_vmp}
\end{figure}



\subsection{Random effects model with Multiple Cuts} \label{subsec:exp_rnd_eff}

The example in this section illustrates the \acrshort*{vmp} with multiple cuts, demonstrating its convenience in settings with more than two modules. The modules are all potentially misspecified, which in this model gives thirty cuts.
The \emph{influence parameter vector} $\bmo{\eta}=(\eta_1,\ldots,\eta_{N})$, $\eta\in H$ regulates the influence of each module.
We take our random effects model and synthetic data setup from \cite{Liu2009modularization} and \cite{Jacob2017together}.

Denote by $Y_i=(y_{i,1}, \ldots, y_{i,n_i})$ the data in group $i=1,...,N$.  We take $N=30$ and $n_i=5$ below. The hierarchical Gaussian model with random effects $\beta=(\beta_i)_{i=1,...,N}$ and variances $\sigma=(\sigma_i)_{i=1,...,N}$ is specified as follows:
\begin{align*}
  Y_{i,j} \sim Normal( \beta_{i}, \sigma_{i}^2 ), & \quad i=1,\ldots,N \; ; \; j=1,\ldots,n_i \\
  \beta_{i} \mid \tau \sim Normal( 0, \tau^2 ),   &
\end{align*}
with $\sigma$ and $\tau$ priors
\begin{align*}
  p(\sigma_{i}^2)            & \propto \sigma_{i}^{-2}                                                          \\
  p(\tau \mid \bmo{\sigma} ) & \propto \frac{1}{\tau^2 + \frac{1}{N} \sum_{i=1}^{N} \frac{\sigma_{i}^2}{n_i} }.
\end{align*}
A graphical representation of the model and its cuts is displayed in \cref{fig:rnd_eff_model}.
\begin{figure}[!htb]
  \centering
  % \large
  \def\svgwidth{0.4\textwidth}
  \import{images/}{rnd_eff_model.pdf_tex}
  \caption{Graphical representation of the model for random effects.}
  \label{fig:rnd_eff_model}
\end{figure}
The Bayes posterior is
\[
  p(\bmo{\sigma},\bmo{\beta},\tau \mid Y) \propto p(Y \mid \bmo{\sigma},\bmo{\beta})p(\bmo{\beta} \mid \tau)p(\tau \mid \bmo{\sigma}) p(\bmo{\sigma}).
\]
Since this is a study on synthetic data, we follow \cite{Liu2009modularization} and take the number of groups $N$ to be large and the number of replications $n_i$ per group to be small as this gives a strong distorting effect under the misspecified model.
We simulate data from the true observational model, $p^*(Y)=p(Y\mid \bmo{\beta^*},\bmo{\sigma^*})$, with two very large random effects, $\beta^*_1=10$ and $\beta^*_2=5$ and zero for the rest, $\beta^*_i=0$ for $i=3,\ldots,N$. We take a unit scale for all groups, $\sigma^*_i=1$ for $i=1,\ldots,N$.
The data are shown in \cref{fig:rnd_eff_data}.
\begin{figure}[!htb]
  \centering
  % TODO: group index must start at 0 in the figure
  \includegraphics[width=\textwidth]{images/random_effects/rnd_eff_data.png}
  \caption[Random effect data]{Simulated data for the random effects model of \cref{subsec:exp_rnd_eff}. With $\beta_1^*=10$, $\beta_2^*=5$, $\beta^*_i=0$ for $i=3,\ldots,N$ and $\sigma^*_i=1$ for $i=1,\ldots,N$.}
  \label{fig:rnd_eff_data}
\end{figure}

Our choice of how to divide the model into modules depends on how we plan to cut.
\cite{Liu2009modularization} have two modules and one cut: Module 1 is the observation module $(Y_{i},\beta_i,\sigma_i)_{i=1,...,N}$, while Module 2 is the prior module $(\beta_i,\sigma_i,\tau)_{i=1,...,N}$. Those authors discuss how the Bayes posterior is distorted when the underlying true random effect $\beta_1$ in a single group of observations is significantly different from the rest of the groups, so the $\beta|\tau$ prior is misspecified. They take a Cut posterior $p_{\cut}(\sigma,\tilde\beta|Y)$, replacing the prior $p(\beta|\tau)$ with an improper imputation prior $\tilde p(\tilde\beta)\propto 1$. This eliminates \emph{prior feedback} as in \cref{subsec:mod_prioir_feed_paper} and \cref{sec:prior_feedback} using an implicit imputation prior. This prior is conjugate so the random effects $\tilde\beta$ are integrated out to give, at the first stage, $p_{\cut}(\sigma \mid Y)=\prod_i p(\sigma_i|Y_i)$. At the second stage the Cut distribution of $\sigma|Y$ is fed into the posterior $p(\beta,\tau|Y,\sigma)$, using the original prior $p(\beta \mid \tau)$ and conditioning on the imputed $\varphi$.

% so the Cut posterior is
% \begin{equation}
%   p_{\cut}(\bmo{\sigma}, \bmo{\beta}, \tau \mid Y) = p_{\cut}(\bmo{\sigma} \mid Y) \; p(\bmo{\beta}, \tau \mid \bmo{\sigma}, Y)
% \end{equation}
% with
% \begin{align}\label{eqn:rnd_eff_sigma_cut}
%   p_{\cut}(\bmo{\sigma} \mid Y) &= \prod_{i=1}^{N} p_{\cut}(\sigma_i \mid Y_{i})\\
%   &=\prod_{i=1}^{N} p_{\cut}(Y_{i} \mid \sigma_i)p(\sigma_i),
%   \intertext{and using}
%   p_{\cut}(Y_i \mid \sigma_i) &= \int p_{\cut}(Y_{i} \mid \tilde\beta_i,\sigma_i)\tilde p(\tilde\beta_i)\tilde\beta_i.
% \end{align}
% As the prior $\tilde p(\tilde\beta)\propto 1$ is conjugate, these marginal likelihoods are tractable inverse Gamma densities.
% This eliminates contamination from groups incompatible with the normality assumption for the $\beta_i$'s.

% Note that, by using this Cut, they are effectively eliminating prior feedback, which involves defining a default prior for the imputation stage (see \cref{sec:prior_feedback}). More precisely, the cut posterior given above can be interpreted as a two stage process: first, performing imputation on $\bmo{\sigma}$ changing the ``aspirational'' priors, $p(\beta_i \mid \tau)$,  by ``default'' priors, $\tilde{p}(\tilde{\beta}_i)\propto 1$; and secondly, the posterior of $(\beta,\tau)$ are obtained restoring the ``aspirational'' prior and conditioning on $\bmo{\sigma}$.
% By changing the prior for $\beta$ in the first stage, the intractable term
% \begin{align*}
%   p(Y \mid \bmo{\sigma})=\int p(Y \mid \bmo{\sigma}, \bmo{\beta})p( \bmo{\beta} \mid \tau)p(\tau \mid \bmo{\sigma}) d\tau d\bmo{\beta}
% \end{align*}
% becomes tractable, given by
% \begin{align*}
%   p_{\cut}(Y \mid \bmo{\sigma}) & =\int p(Y \mid \bmo{\sigma}, \bmo{\tilde\beta})\tilde p(\bmo{\tilde\beta}) d\bmo{\tilde\beta}                   \\
%                         & =\prod_{i=1}^N \int p(Y_i\mid \sigma_i, \tilde\beta_i)\tilde p(\tilde\beta_i)d\beta_i \\
%                         & =\prod_{i=1}^N p_{\cut}(Y_i\mid \sigma_i)
% \end{align*}
% which is a product of Inverse Gamma densities, proportional to the posterior $p_{\cut}(\bmo{\sigma} \mid Y)$ in \cref{eqn:rnd_eff_sigma_cut}.

In contrast to this all (Bayes) or nothing (Cut) approach, \acrshort*{smi} \emph{reduces} the influence of \emph{some} of the groups that may be causing contamination of the posterior.
Our modules are for $i=1,...,N$ the separate generative models for $(Y_i,\beta_i,\sigma_i)$ and $(\beta_i,\sigma_i,\tau)$ so we have in effect $2N=60$ modules, and we work with the joint distributions involving $\beta$. The marginals would still be available, but the higher dimensional joint distributions have interesting shapes and present more of a challenge.
Our target \acrshort*{smi} posterior is
\begin{equation}\label{eqn:rnd_eff_smi_posterior}
  p_{smi, \bmo{\eta}}(\bmo{\sigma}, \bmo{\beta}, \tau, \bmo{\tilde\beta}, \tilde\tau \mid Y) = p_{pow, \bmo{\eta}}(\bmo{\sigma}, \bmo{\tilde\beta}, \tilde\tau \mid Y) \; p(\bmo{\beta}, \tau \mid \bmo{\sigma}, Y),
\end{equation}
where
\begin{align}
  p_{pow, \bmo{\eta}}(\bmo{\sigma}, \bmo{\tilde\beta}, \tilde\tau \mid Y) & \propto p(Y \mid \bmo{\tilde\beta}, \bmo{\sigma} ) \; p(\tilde\tau \mid \bmo{\sigma}) \; \prod_{i=1}^{N} \tilde{p}_{\eta_i}(\tilde\beta_i \mid \tilde\tau), \\
  p(\bmo{\beta}, \tau \mid \bmo{\sigma}, Y)                               & \propto p(Y \mid \bmo{\beta}, \bmo{\sigma} ) \; p(\tau \mid \bmo{\sigma}) \; \prod_{i=1}^{N} p(\beta_i \mid \tau)
\end{align}
Performing \acrlong*{smi} using \cref{eqn:rnd_eff_smi_posterior} entails a 30-dimensional influence parameter $\eta \in H$ with $H= [0,1]^{30}$.
The \acrshort*{smi} imputation is
\[
  (\sigma,\tilde\beta,\tilde\tau)\sim p_{pow, \bmo{\eta}}(\bmo{\sigma}, \bmo{\tilde\beta}, \tilde\tau \mid Y),
\]
and the analysis is
\[
  (\bmo{\beta}, \tau \mid \bmo{\sigma})\sim p(\bmo{\beta}, \tau \mid \bmo{\sigma}, Y).
\]
The modulated priors $\tilde{p}_{\eta_i}$ used at the imputation stage are chosen to interpolate between the same cut prior $\tilde p(\tilde\theta)\propto 1$ and analysis priors as before. Following the discussion in \cref{sec:prior_feedback}, we define the modulated imputation prior as a normal density
\[
  \tilde{p}_{\eta_i}(\tilde\beta_i \mid \tilde\tau) = \mathcal{N}(\tilde\beta_i ; 0, \tilde\tau / \eta_i ),
\]
and $\tilde{p}_{\eta}(\tilde\beta \mid \tilde\tau)=\prod_i \tilde{p}_{\eta_i}(\tilde\beta_i \mid \tilde\tau)$. This parameterisation gives the Cut prior $\tilde{p}_{\eta}(\tilde\beta \mid \tilde\tau)\to \tilde{p}(\tilde\beta)$ as $\eta\to 0$ and the Bayes prior $\tilde{p}_{\eta}(\tilde\beta \mid \tilde\tau)\to p(\tilde\beta|\tilde\tau)$ as $\eta\to 1$.
% We could also have defined this imputation prior using an interpolation based on a power, $p(\beta_i \mid \tilde\tau)^{\eta}$. However, this would result in modifying the prior for $\sigma$ and $\tau$ (perhaps inadvertently!), as in that case $\int \tilde{p}_{\eta_i}(\tilde\beta_i \mid \tilde\tau)p(\tau \mid \sigma)$

Our goal is to optimally modulate the feedback from each group into the shared distribution of the $\beta$'s. Accurate estimates of the utility $u(\eta)$ in \cref{eq:ELPD_var_smi_def} are needed in order to locate the maximum-utility influence-vector $\eta^*$ in \cref{eqn:eta-star-defn}. We could
approximate the modular posterior, using either nested-\acrshort*{mcmc} or variational \acrshort*{smi}, at a lattice of $\eta$-values in $H=[0,1]^N$, but this quickly becomes impractical with increasing $N$. Instead we approximate the candidate \acrshort*{smi} posteriors at \emph{all} $\eta$ in a single function by learning $q_{f_{\hat\alpha}(\eta)}$ and $q_{\hat\alpha,\eta}$, the flow- and map-based \acrlongpl*{vmp}. We found we could do this fairly accurately with the essentially same \acrshort*{vmp}-\acrshort*{nsf}-\acrshort*{mlp} meta-variational setup we used in \cref{subsec:exp_epidemiology}.
%(see \cref{subsec:exp_rnd_eff_extra}).
The inputs to the flow are $\epsilon_1\sim N(0,\mathbb{I}_N)$ (these express $\sigma$), $\epsilon_2\sim N(0,\mathbb{I}_{N+1})$ (expressing $\beta, \tau$) and $\epsilon_3\sim \epsilon_2$ (expressing $\tilde\beta, \tilde\tau$). Runtimes for \cref{alg:v_meta_smi} are manageable as simulation of the \acrshort*{vmp},
\begin{equation}\label{eq:hm_sim_vsmi}
  (\sigma,\beta,\tau,\tilde\beta,\tilde\tau)\sim q_{f_\alpha(\eta)} 
\end{equation}
at given $\alpha,\eta$ is fast: set $(\lambda_1,\lambda_2,\lambda_3)=f_{\alpha}(\eta)$ and then compute $\sigma=T_1(\lambda_1,\epsilon), (\beta,\tau)=T_2(\lambda_2,\epsilon)$ and $(\tilde\beta,\tilde\tau)=T_2(\lambda_3,\epsilon)$ using the deterministic flow mapping in \cref{eq:smi_transform}. Simulation of $q_{\hat\alpha,\eta}$ is slightly faster.

Our results at $\eta=\bm{0}_N,\bm{1}_N$ are qualitatively consistent with the Cut and Bayes analyses in \citet[Sec.~4.4]{Jacob2017together}. We took two misspecified effects rather than one, but in other respects the setup is the same. Samples from the exact $p_{\smi,\eta}$ posterior produced via \acrshort*{mcmc} are shown in \cref{fig:rnd_eff_mcmc} for a selection of variable pairs.
Comparing these with the corresponding distributions given by \acrshort*{vmp}-\acrshort*{nsf}-\acrshort*{mlp} in \cref{fig:rnd_eff_vmp} using a \acrshort*{vmp}-map, we see good agreement across each of the three rows/$\eta$-configurations, despite the highly irregular contour shapes. We emphasise that \emph{all} samples are produced from a \emph{single} \acrshort*{vmp} $q_{f_{\hat\alpha}(\eta)}$, and we just plug in different $\eta$-values to get different rows in \cref{fig:rnd_eff_vmp}. The \acrshort*{vmp}-flow density $q_{\hat\alpha,\eta}$ converged much more rapidly to agreement with the ground truth than did the \acrshort*{vmp}-map density $q_{f_{\hat\alpha}(\eta)}$, but $\mathcal{Q}_{H,\hat\alpha}$ approximates $\mathcal{P}_{\smi}$ accurately in both cases. We omit the corresponding \acrshort*{vmp}-flow plots as they are essentially identical. 

Selected components of the \acrshort*{vmp}-map $f_{\hat\alpha}(\eta)$ are shown in \cref{fig:rnd_eff_vmp_map}. The surfaces in the top row show a complex structure across the two axes: varying influence parameters in the misspecified groups has a significant impact on the variational posteriors.
However, the surface in the bottom row of plots is almost constant with $\eta_3$: cutting one of these ``good'' groups with labels $3,...,30$ doesn't have a strong impact on the variational posterior.

Producing nested-\acrshort*{mcmc} plots for comparison with the corresponding \acrshortpl*{vmp} at a few $\eta$-values is undemanding. However, this is where the contribution from \acrshort*{mcmc} ends. Accurate estimation of the the utility $u(\eta)=\elpd(\eta)$ over $\eta\in H$ becomes prohibitively expensive using two-stage nested-\acrshort*{mcmc} methods.
%The $\eta$-weight distribution $\rho$ is a numerical convenience, used to concentrate the $\eta$ search in regions where accurate estimation is needed, so we could ignore the normalising constant in the \acrshort*{mcmc} Hastings-ratio, and in effect allow the $\eta$-target to become $\rho(\eta)/p_{\smi,\eta}(Y,Z)$. However, this leads to something like path-sampling, where we need to reweight the $\rho$-distribution in order to push $\eta$ into important search regions.
The posterior predictive $p(y,z|Y,Z)$ in \cref{eq:ELPD_var_smi_def} must in general be estimated using samples from the \acrshort*{vmp}. Although the density $q_{\hat\alpha,\eta}$ is available in closed form and can be sampled independently, it is nevertheless a complicated function.  However, as noted above, the simulation in \cref{eq:hm_sim_vsmi} is fast. In \cref{fig:rnd_eff_elpd} we plot (negative) $\elpd(\eta)$-surfaces using the operational WAIC-estimator. We check this estimate using direct simulation of synthetic data $y'\sim p^*(\cdot)$. In the top row, reducing the feedback from the two misspecified modules improves predictive performance (the \acrshort*{elpd} is larger at smaller $\eta_1,\eta_2$-values).
In the bottom row, where we vary $(\eta_1,\eta_3)\in[0,1]$, the rates for one misspecified and one well-specified group, we see that the \acrshort*{elpd} surface is relatively flat for $\eta_3$, the influence a well-specified group, though trending up with increasing $\eta_3$. 

\Cref{fig:rnd_eff_greedy_search} illustrates the initialisation stage for the minimisation of the WAIC. The second \acrshort*{sgd}-stage on the WAIC target terminates quickly from this initialisation. The estimated optimal values $\hat\eta^*$ (rounded the first decimal place) are 
\[\hat\eta^*=(0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,0.9,1,1,0.9,1,1,1,0.8,1,1,1,1,1,1,1).\]
% [0.          , 0.0017527284, 0.9773286   , 1.          ,
%              0.9884817   , 0.979367    , 0.9967817   , 0.9892808   ,
%              0.92853594  , 0.99279094  , 0.98333687  , 1.          ,
%              0.9996555   , 0.9517141   , 1.          , 0.8765335   ,
%              0.96036845  , 0.9856562   , 0.94023335  , 1.          ,
%              0.99676484  , 0.99690336  , 0.80675805  , 1.          ,
%              0.9999974   , 0.99999845  , 0.99589676  , 0.9926863   ,
%              0.97440845  , 0.98974717  ]
This result shows the method is working as information from the first two modules is cut while the rest are Bayes or close to Bayes. This is as expected as we have synthetic data with modules 1 and 2 misspecified.
The resulting $\hat\eta^*$ gives a $p_{\smi,\hat\eta^*}$ which is hard to distinguish from
the $p_{\smi,\eta}$ posterior at $\eta=(0,0,1,...,1)$ (shown
in \cref{fig:rnd_eff_model}) as the \acrshort*{smi}-posterior is insensitive to changes to $\eta_3,...,\eta_N$ close to Bayes values.

\begin{figure}[!htb]
  \centering
  \includegraphics[width=0.24\textwidth]{images/random_effects/spline/vmp_mlp/rnd_eff_beta_sigma_group_1_full}
  \includegraphics[width=0.24\textwidth]{images/random_effects/spline/vmp_mlp/rnd_eff_beta_sigma_group_2_full}
  \includegraphics[width=0.24\textwidth]{images/random_effects/spline/vmp_mlp/rnd_eff_beta_sigma_group_3_full}
  \includegraphics[width=0.24\textwidth]{images/random_effects/spline/vmp_mlp/rnd_eff_sigma_tau_group_1_full}

  \includegraphics[width=0.24\textwidth]{images/random_effects/spline/vmp_mlp/rnd_eff_beta_sigma_group_1_cut1}
  \includegraphics[width=0.24\textwidth]{images/random_effects/spline/vmp_mlp/rnd_eff_beta_sigma_group_2_cut1}
  \includegraphics[width=0.24\textwidth]{images/random_effects/spline/vmp_mlp/rnd_eff_beta_sigma_group_3_cut1}
  \includegraphics[width=0.24\textwidth]{images/random_effects/spline/vmp_mlp/rnd_eff_sigma_tau_group_1_cut1}

  \includegraphics[width=0.24\textwidth]{images/random_effects/spline/vmp_mlp/rnd_eff_beta_sigma_group_1_cut2}
  \includegraphics[width=0.24\textwidth]{images/random_effects/spline/vmp_mlp/rnd_eff_beta_sigma_group_2_cut2}
  \includegraphics[width=0.24\textwidth]{images/random_effects/spline/vmp_mlp/rnd_eff_beta_sigma_group_3_cut2}
  \includegraphics[width=0.24\textwidth]{images/random_effects/spline/vmp_mlp/rnd_eff_sigma_tau_group_1_cut2}

  \caption[Random Effects model VMP]{
    Samples from the \acrlong*{vmp} of the Random Effects model, obtained from a \acrshort*{vmp}-\acrshort*{nsf}-\acrshort*{mlp} architecture. Each graph shows the joint distribution of a selected pair of parameters. Rows correspond to three modular \emph{feedback} configurations between groups: (Top row) Bayes, $\eta_1=...=\eta_{30}=1$; (Middle) One Cut module, $\eta_1=0$, $\eta_2=...=\eta_{30}=1$; (Bottom) Two Cut Modules, $\eta_1=\eta_2=0$, $\eta_3=...=\eta_{30}=1$.
    Compare \acrshort*{mcmc} in \cref{fig:rnd_eff_mcmc}.
    %All samples are produced from a single \acrshort*{vmp}-map.
  }
  \label{fig:rnd_eff_vmp}
\end{figure}

\begin{figure}[!htb]
  \centering
  \includegraphics[width=0.98\textwidth]{images/random_effects/spline/vmp_mlp/rnd_eff_vmp_map_eta_0_1_sigma.png}
  \includegraphics[width=0.98\textwidth,trim={0 0 0 1cm},clip]{images/random_effects/spline/vmp_mlp/rnd_eff_vmp_map_eta_0_2_sigma.png}

  \caption[Random Effects model VMP map]{
  Trained \acrshort*{vmp}-map $\lambda^*(\eta)=f_{\alpha^*}(\eta)$ for selected components of the variational parameter vector $\lambda^*_i(\eta),\ i=0,50,100,150,200$ in a \acrlong*{nsf}.
  Here $\eta = (\eta_{1},\eta_{2},\ldots,\eta_{30})$, and we vary two selected $\eta$-components at a time, while keeping the rest constant.
  Top: vary $(\eta_1,\eta_2)\in[0,1]$ (associated with the misspecified groups) and set $\eta_3=\ldots=\eta_{30}=1$.
  Bottom: vary $(\eta_1,\eta_3)\in[0,1]$ (one misspecified group, one well-specified) and set $\eta_2=0$, $\eta_4=\ldots=\eta_{30}=1$.
  }
  \label{fig:rnd_eff_vmp_map}
\end{figure}

\begin{figure}[!htb]
\includegraphics[width=0.98\textwidth,trim={0 0 0 1.5cm},clip]{images/random_effects/spline/vmp_mlp2/greedy_search_eta_star_neg_elpd}
\caption[Random Effects model VMP map]{
  Greedy search algorithm to initialize \acrshort*{sgd} $\eta^*$-estimation for the Random Effects model. The red line is the trajectory of the negative \acrshort*{elpd} obtained by applying the initialisation algorithm in \cref{subsec:best_smi_vmp}. The blue dots represent the $-\elpd$ of the ``candidate'' models at each step for the greedy search, produced by cutting one (additional) module. The search stops after two iterations, cutting the first and second modules, as expected. Each full step takes a fraction of second to compute.
  }
\label{fig:rnd_eff_greedy_search}
\end{figure}

\begin{figure}[!htb]
  \centering
  \includegraphics[width=0.75\textwidth,trim={10cm 0 0 0},clip]{images/random_effects/spline/vmp_mlp/elpd_surface_eta_0_1.png}
  \includegraphics[width=0.75\textwidth,trim={10cm 0 0 1cm},clip]{images/random_effects/spline/vmp_mlp/elpd_surface_eta_0_2.png}
  % \includegraphics[width=0.5\textwidth,trim={10cm 0 0 1cm},clip]{images/random_effects/spline/vmp_mlp/elpd_surface_eta_bad_vs_good}

  \caption[Random Effects model ELPD]{
    Predictive scores comparing performance of the \acrshort*{smi} posterior for different values of $\eta$ in the Random Effects model.
    We show two approximations to the negative ELPD. % \citep{Vehtari2016}.
    (Left column) \acrshort*{elpd} estimated using draws from the true generative model.
    (Right columns) \acrshort*{elpd} estimated using the WAIC. These match the left column which acts as a check. % \citep{Watanabe2012}.
    %Here $\eta = (\eta_{1},\eta_{2},\ldots,\eta_{30})$, and we vary two selected $\eta$-components at a time, while keeping the rest constant.
    (Top row) varying the influence parameters associated with the two misspecified groups, $(\eta_1,\eta_2)\in[0,1]$, fixing $\eta_2=\ldots=\eta_{30}=1$. (Bottom row) varying $(\eta_1,\eta_3)\in[0,1]$, the rates for one misspecified and one well-specified group, fixing $\eta_2=0$ and $\eta_4=...=\eta_{30}=1$.
  }
  \label{fig:rnd_eff_elpd}
\end{figure}

\section{Conclusions}

We have given variational families and loss functions for approximating Cut- and \acrshort*{smi}-posterior distributions. Much of the presentation is agnostic to the details of the variational family. However, we focus on parameterisation based on \acrshortpl*{nf} as this overcomes many well known weaknesses of variational inference. We saw no sign of underdispersion relative to an \acrshort*{mcmc}-baseline in our examples. In contrast \acrshort*{mfvi} approximated the $\eta$-dependent mean of the $p_{\smi,\eta}$ target well but was significantly underdispersed.

The loss function we use (really two loss functions) in \cref{defn:var-smi} is not the standard \acrshort*{kl} divergence between the variational density and target, as that loss does not allow control of information flow between modules. Our loss removes dependence on cut modules when we impute parameters in well-specified modules at $\eta=0$ (the Cut-posterior). Just as $p_{\smi,\eta}$ interpolates between the Cut and Bayes posterior distributions, so our variational approximation $q_{\lambda^*(\eta)}$ exactly interpolates between a variational approximation to the Cut due to \cite{Yu2021variationalcut} and standard variational Bayes. Although the optimized loss need not decrease as we enlarge the variational family, it goes to zero, and we recover the exact target, as the family expands to include the target.

In variational \acrshort*{smi} our goal is to approximate distributions in the family $\mathcal{P}_{\smi}=\{p_{\smi,\eta},\ \eta\in H\}$. We gave a \acrlong*{vmp} $\mathcal{Q}_{H,\alpha}=\{q_{\alpha,\eta},\ \eta\in H\}$ which fits all the distributions in $\mathcal{P}_{\smi}$ at the same time, by taking $\eta$ as a conditioned quantity in the \acrshort*{nf}. We called the modified \acrshort*{nf} the \acrshort*{vmp}-flow. The \acrshort*{sgd} in \cref{alg:v_meta_smi} finds $\alpha^*$ which fits $\mathcal{Q}_{H,\alpha^*}$ to $\mathcal{P}_{\smi}$ in a single joint optimisation for efficient end-to-end training. We gave two parameterisations of the \acrshort*{vmp}. We favor the \acrshort*{vmp}-flow. Our second parameterisation of the \acrshort*{vmp}, $\mathcal{Q}_{H,\alpha}=\{q_{\lambda(\alpha,\eta)},\ \eta\in H\}$, trains a \acrshort*{vmp}-map $f_{\alpha}(\eta)$ to output the optimal variational parameters $\lambda^*(\eta)$ as functions of $\eta$. The two approaches gave similar variational approximations, but training the \acrshort*{vmp}-flow was faster, as \acrshort*{sgd} converged more steadily and rapidly with less tuning of optimisation hyper-parameters.

One advantage of using the \acrshort*{vmp} is that is allows us to modulate feedback from multiple modules at the same time. When we apply a Cut-posterior we have to pre-identify misspecified modules in order to give the locations of cuts. We gave an analysis in which we cut every data-module separately and estimated the associated influence parameters. This allows us to discover rather than pre-identify the cut-modules. 
Accurate estimation of the \acrshort*{elpd} over $\eta\in H$ calls for \acrshort*{smi} or variational-\acrshort*{smi} posterior samples at 
$C$-exponentially many $\eta$ values
for $C$ cuts, and this is prohibitively expensive using one-$\eta$-at-a-time methods such as \cref{alg:vsmi} or nested-\acrshort*{mcmc} methods. 
%A possible joint \acrshort*{mcmc} approach, which adds $\eta\sim \rho$ to the \acrshort*{smi} parameter space $(\sigma,\beta,\tau,\tilde\beta,\tilde\tau)$, is hindered by an intractable $p_{\smi,\eta}$ normalising constant, and leads to path-sampling.

The main weaknesses of our methods are first, a certain amount of experimentation was needed to find flow architectures that worked well for our targets. However, having found an architecture (with coupling-layer \acrshort*{mlp}-conditioners and rational spline transformers) that worked well, it worked well for all targets. Further exploration is needed to see if this holds more generally. Tuning of the initialisation and learning rate in \cref{alg:vsmi} and \cref{alg:v_meta_smi} were needed also. Another weakness is that the final step of the analysis, after the \acrshort*{vmp} is trained and we have only to estimate and maximise the \acrshort*{elpd} and select the optimal influence parameters $\eta$, is not straightforward, at least for high dimensional $\eta$. The estimated \acrshort*{elpd} is non-convex in $\eta$. Working with the \acrshort*{vmp} makes this step as easy as possible, as sampling the \acrshort*{vmp} is very fast.

%The \acrshort*{vmp} exploits the smoothness of $\lambda(\eta)$ over $\eta$ and rapid simulation of $q_{\lambda(\eta)}$ in \cref{alg:v_meta_smi} and in utility estimation. This brings multiple Cuts within reach.

%Still quite alot of messing about with architechtures and optimisation schemes.

The \acrshort*{vmp} framework may be useful outside \acrlong*{smi}. Approximating a complete family of models indexed by a set of continuous hyperparameters has potential applications in parameter selection for hyperpriors in standard Variational Bayesian inference. The marginal density  $q_{\alpha^*,\eta}(\varphi,\tilde\theta),\ \eta\in H$ gives variational approximations to the power posterior distribution $p_{\pow,\eta}(\varphi,\tilde\theta\mid Y,Z)$ at every $\eta$ at the same time, and this may be all we need if the power posterior rather than \acrshort*{smi} is the target.

%\clearpage
%\newpage

\section*{Acknowledgements}
We thank Dennis Prangle, David Nott and Kamélia Daudel for insighful discussions on Variational Methods and Modular Inference.

Research supported with Cloud TPUs from Google's TPU Research Cloud (TRC)
