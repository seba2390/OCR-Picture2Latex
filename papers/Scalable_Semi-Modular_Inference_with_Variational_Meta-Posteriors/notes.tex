
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%First attempt at cutting priors

The loss function for which $p_{cut}(\beta,\tau, \sigma|Y)\propto \exp(-l_{cut})p(\beta,\tau, \sigma)$ is a Gibbs posterior is
\begin{equation}\label{eq:HM_full_cut_loss}
  l_{cut}(Y; \beta,\tau, \sigma, \pi_0)=-\log(p(Y|\beta,\tau))-\log(p_{cut}(Y|\sigma))+\log(p(Y|\sigma))
\end{equation}
where $\pi_0(\sigma,\tau,\beta,\tilde\beta)=p(\sigma)p(\tau|\sigma)p(\beta|\tau)\tilde p(\tilde\beta)$ is the prior which we must specify as part of the loss as it appears in $p_{cut}(Y|\sigma)$ and $p(Y|\sigma)$ and changes as belief is updated.

\begin{proposition}
  The Cut-loss in \cref{eq:HM_full_cut_loss} is prequentially additive in the sense of \cite{Nicholls2022smi} for the belief update given by the Gibbs posterior and so (by Theorem~2.1 in that paper) the Cut posterior in \cref{eq:HM_full_cut_posterior} is an order coherent belief update.
\end{proposition}

\begin{proof} Let $Y^{(1)},Y^{(2)}$ be an arbitrary partition of $Y$. The loss $l_{cut}$ is prequentially additive if
  \begin{equation}\label{eq:HM_preq_add_defn}
    l_{cut}(Y; \beta,\tau, \sigma, \pi_0)=l_{cut}(Y^{(2)}; \beta,\tau, \sigma, \pi_1)+l_{cut}(Y^{(1)}; \beta,\tau, \sigma, \pi_0),
  \end{equation}
  where $\pi_1$ is the ``prior" for $\sigma,\tau,\beta,\tilde\beta$ after the first block of data $Y^{(1)}$ has arrived. By \cref{eq:HM_full_cut_posterior} this is $\pi_1(\sigma,\tau,\beta,\tilde\beta)=p_{cut}(\sigma,\tau,\beta,\tilde\beta|Y^{(1)})$.
  The LHS of \cref{eq:HM_preq_add_defn} is given in \cref{eq:HM_full_cut_loss}.
  The data are conditionally independent so $p(Y|\beta,\tau)=p(Y^{(2)}|\beta,\tau)p(Y^{(1)}|\beta,\tau)$, and the RHS is
  \begin{align*}
    l_{cut}(Y^{(2)}; \beta,\tau, \sigma, \pi_1)+l_{cut}(Y^{(1)}; \beta,\tau, \sigma, \pi_0)= &
    -\log(p(Y|\beta,\tau))                                                                                                                                                                                                                           \\
                                                                                             & -\log(p_{cut}(Y^{(2)}|\sigma,Y^{(1)}))+\log(p(Y^{(2)}|\sigma,Y^{(1)}))                                                                                \\
                                                                                             & -\log(p_{cut}(Y^{(1)}|\sigma))+\log(p(Y^{(1)}|\sigma))                                                                                                \\
    =                                                                                        & -\log(p(Y|\beta,\tau))                                                                                                                                \\
                                                                                             & -\log\left(\frac{p_{cut}(Y^{(2)},Y^{(1)}|\sigma)}{p_{cut}(Y^{(1)}|\sigma}\right)+\log\left(\frac{p(Y^{(2)},Y^{(1)}|\sigma)}{p(Y^{(1)}|\sigma)}\right) \\
                                                                                             & -\log(p_{cut}(Y^{(1)}|\sigma))+\log(p(Y^{(1)}|\sigma))                                                                                                \\
                                                                                             & =-\log(p(Y|\beta,\tau))-\log(p_{cut}(Y|\sigma))+\log(p(Y|\sigma))
  \end{align*}
  and we are done.
\end{proof}

We make some remarks on the proof and give some intuitiion for the particular objects which appear. In the expression for the prequentially additive loss, $p_{cut}(Y^{(2)}|\sigma,Y^{(1)})$ is conditioned on $Y^{(1)}$ because the prior $\pi_1(\tilde\beta|\sigma)$ used to form this marginal likelihood is
\[
  p_{cut}(\tilde\beta|\sigma,Y^{(1)})= \frac{1}{p_{cut}(Y^{(1)}|\sigma)}p(Y^{(1)}|\sigma,\tilde \beta)\tilde p(\tilde\beta).
\]
This is the prior in the imputation stage we get after a belief update from the original imputation stage prior. Similarly $p(Y^{(2)}|\sigma,Y^{(1)})$ is conditioned on $Y^{(1)}$ because the prior used to form this marginal likelihood is
\[
  p(\tau,\beta|\sigma,Y^{(1)})= \frac{p(Y^{(1)}|\sigma,\beta)}{p(Y^{(1)}|\sigma)}p(\beta|\tau)p(\tau|\sigma).
\]
This is the prior in the analysis stage we get after a belief update from the original imputation stage prior.

Now in brief for the first term on the second line of the loss expansion, $p_{cut}(Y^{(2)}|\sigma,Y^{(1)})=p_{cut}(Y^{(2)},Y^{(1)}|\sigma)/p_{cut}(Y^{(1)}|\sigma)$. This follows using conditional probability, since $p_{cut}(Y^{(2)}|\sigma,Y^{(1)})=p_{cut}(Y^{(2)},\sigma,Y^{(1)})/p_{cut}(\sigma,Y^{(1)})$.
More explicitly,
\begin{align*}
  p_{cut}(Y^{(2)}|\sigma,Y^{(1)})
   & =\int p(Y^{(2)}|\tilde\beta,\sigma,Y^{(1)}) p_{cut}(\tilde\beta|\sigma,Y^{(1)}) d\tilde\beta                                           \\
   & = \frac{1}{p_{cut}(Y^{(1)}|\sigma)}\int p(Y^{(2)}|\sigma,\tilde\beta) p(Y^{(1)}|\sigma,\tilde \beta)\tilde p(\tilde\beta) d\tilde\beta \\
   & =p_{cut}(Y|\sigma)/p_{cut}(Y^{(1)}|\sigma).
\end{align*}
In brief for the second term we have $p(Y^{(2)}|\sigma,Y^{(1)})=p(Y|\sigma)/p(Y^{(1)}|\sigma)$ by the same reasoning using the definition of conditional probability. More explicitly,
\begin{align*}
  p(Y^{(2)}|\sigma,Y^{(1)}) & =\int p(Y^{(2)}|\sigma,\beta)p(\tau,\beta|\sigma,Y^{(1)}) d\tau d\beta                                                 \\
                            & =\frac{1}{p(Y^{(1)}|\sigma)}\int p(Y^{(2)}|\sigma,\beta)p(Y^{(1)}|\tau,\beta,\sigma)p(\tau,\beta|\sigma) d\tau d\beta  \\
                            & =\frac{1}{p(Y^{(1)}|\sigma)}\int p(Y^{(2)}|\sigma,\beta)p(Y^{(1)}|\beta,\sigma)p(\beta|\tau)p(\tau|\sigma)d\tau d\beta \\
                            & =\frac{p(Y|\sigma)}{p(Y^{(1)}|\sigma)}
\end{align*}

.
.
.

When we wish to drop a prior factor (such as $p(\beta|\tau)$ in the present example) in the imputation stage, we cannot simply remove the prior for the random variable ($\beta$ here). Because it must have some prior distribution for subsequent conditioning, the imputation and analysis posteriors can differ by replacement of a prior factor, whilst they can differ by replacement or removal of a likelihood factor.
In our example we would like to stop the misspecified $p(\beta |\tau)$ prior from distorting the estimation of $\sigma$.
When we remove this prior factor for $\beta$ we have to give a replacement prior for $\beta$ for use in the imputation stage. Call this prior $\tilde p(\tilde\beta)$. The random variables $\beta$ and $\tilde\beta$ have different distributions so we distinguish them. We have now to choose this imputation prior $\tilde p$. In our example we make $\tilde p(\tilde\beta)\propto 1$.

we must have different random variables in the imputation stage (in $p_{cut}(\sigma,\tilde\beta|Y)$ where $\tilde\beta$ has prior density $\tilde p(\cdot)$) and the analysis stage (in $p(\tau,\beta|Y,\sigma)$ where $\tilde\beta$ has prior density $p(\beta|\tau)$). In previous work it has often been implicit that the prior used for replacement was the uniform distribution as that is what you get if you replace ``$p(\beta|\tau)$" with ``1".

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%Material from the Meta-post section - includes GN stuff on
%multiple modules and CC stuff setting up MP's.

Consider a multi-modular model with $M$ modules indexed $\mathcal{M}=\{1,...,M\}$. In order to fix ideas we consider the following setup. Let $\mathcal{D}=(D_1,...,D_M)$ with $D_m,\ m\in \mathcal{M}$ be the data in module $m$ and let $\Psi=(\psi_1,...,\psi_A)$ be the collection of all parameters in the model. For $m\in \mathcal{M}$ let $i_m\subseteq \{1,...,A\}$ be the subset of indices for parameters entering the generative model for data $D_m$ and no other module (intrinsic parameters) and  let $e_{m}\subseteq \{1,...,A\}$ be the indices of parameters which module $m$ shares with other modules (extrinsic).
Let $p_m(D_m \mid \psi_{i_m},\psi_{e_m})$ be the observation model for module $m$ and let $p_m(\psi_{i_m} \mid \psi_{e_m})$ be the prior for the intrinsic parameters of module $m$ given its extrinsic parameters.
Let $E=\cup_m e_m$ be the set of all extrinsic parameter indices. The joint Bayes posterior can be written
\begin{equation}
  p(\Psi|\mathcal{D}) = p(\psi_{E})\prod_{m\in \mathcal{M}} p_m(D_m \mid \psi_{i_m},\psi_{e_m})\, p_m(\psi_{i_m} \mid \psi_{e_m}).
\end{equation}
Suppose a subset of modules $\mathcal{C}\subset \mathcal{M},\ \mathcal{C}=\{c_1,...,c_N\}$ have been identified as possibly misspecified and we wish to cut or modulate their influence on other modules.
Let $H=[0,1]^N$ be the space of influence parameters $\eta\in H,\ \eta=(\eta_1,...,\eta_N)$. The joint \acrshort{smi} posterior is
\begin{align}
  p_{\text{smi},\eta}(\Psi|\mathcal{D}) & \propto p(\psi_{E})\prod_{b\in \mathcal{C}}
  p_b(D_b\mid\tilde\psi_{i_b},\psi_{e_b})^{\eta_b}\, p_b(\tilde\psi_{i_b} \mid \psi_{e_b})
  \\
                                        & \qquad\times\quad
  \prod_{m\in \mathcal{M}\setminus \mathcal{C}} p_m(D_m \mid \psi_{i_m},\psi_{e_m})\, p_m(\psi_{i_m} \mid \psi_{e_m})
  \\
                                        & \qquad\times\quad
  \prod_{c\in \mathcal{C}} p_c(\psi_{i_c} \mid D_c, \psi_{e_c})                       \\
                                        & =p_{\text{pow},\eta}()
\end{align}
This is still
For a given probabilistic model defined in a multi-modular setting $p(\Psi, \mathcal{D})$, with parameters $\Psi$, data $\mathcal{D}$ and $M$ cuts, \acrlong*{smi} defines a \emph{family} of posterior distributions $\mathcal{P}_{smi}=\{p_{ \text{smi}, \eta}(\Psi \mid \mathcal{D}) \;,\;\eta \in H \}$, characterised by $\eta \in H$, the \emph{rates of feedback} between modules, taking values in $H=[0,1]^M$.
The \acrshort*{smi} posterior at $\Psi$ is a continuous and differentiable function of $\eta$, so we would expect that $\mathcal{P}_{smi}$ is a connected set.

Given a variational family $\mathcal{Q}=\{q_{\lambda}(\Psi), \lambda \in \Lambda\}$ (with variational parameters $\lambda$) there is a subset of elements that best approximate the elements in the \acrshort*{smi} family, $\mathcal{Q}_{smi} = \{ q: \argmin_{q \in \mathcal{Q}} D(q{\,{\,\|\,}\,} p) , p \in \mathcal{P}_{smi} \} \subset \mathcal{Q}$.
Let us denote the associated set of variational parameters as $\Lambda_{smi}=\{ \lambda : q_{\lambda} \in \mathcal{Q}_{smi} \} \subset \Lambda$.

In practice, we are interested in choosing the best element of $p_{ \text{smi}, \eta} \in \mathcal{P}_{smi}$ for some performance score.
By using the variational approximation described in \cref{sec:vsmi}, we aim to solve the proxy problem of finding the best $q_{\lambda} \in \mathcal{Q}_{smi}$.

The correspondence between $\eta$ and the variational parameters $\lambda$ that best approximate $p_{ \text{smi}, \eta}$ is not one-to-one in general.
Variational families with highly flexible distributions may contain multiple elements which solve the minimisation problem.
Nevertheless, if the variational family $\mathcal{Q}$ is smooth with respect to the variational parameters, we would expect that $\Lambda_{smi}$ is composed as the union of connected sets.

When $\Lambda_{smi}$ is a single connected set, the optimal variational parameters for a given $\eta$ provides useful information for the optimisation of a neighbour $\eta+\epsilon$.
If $\Lambda_{smi}$ is composed by the union of multiple sets, independent optimisation via \acrshort*{sgd} for different values of $\eta$ may output $\hat\lambda$'s in different regions, which is not convenient for our purpose of exploration along $\eta\in H$.
This leads us to consider optimisation strategies that are able to a) favour \emph{continuity} in $\hat\Lambda_{smi}$ as a function of $\eta$, and b) share information between the optimisation of all $\eta \in H$.

We propose a method to learn a connected set of parameters $\hat\Lambda_{smi}$ by learning a continuous mapping,

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
This is equivalent to minimize the negative \acrshort*{elbo},
\begin{align}
  \text{ELBO}^{(\eta-\text{smi})} = \E_{(\varphi, \theta, \tilde\theta) \sim q(\varphi, \theta, \tilde\theta)}[ & \log p(Z \mid \varphi) + \eta \log p(Y \mid \varphi, \tilde\theta) + \log p(\varphi, \tilde\theta) \nonumber \\
                                                                                                                & + \log p(Y \mid \varphi, \theta) + \log p(\varphi, \theta) - \log p(Y, \varphi) \nonumber                    \\
                                                                                                                & - \log q(\varphi, \theta, \tilde\theta) ]
\end{align}

The gradients of $\text{ELBO}^{(\eta-\text{smi})}$ w.r.t. our variational parameters are:
\begin{align}
  \nabla_{\lambda_1} \text{ELBO}^{(\eta-\text{smi})} = \E_{\epsilon \sim p(\epsilon)}[ & \nabla_{\varphi} \left\{ \log p(Z \mid \varphi) + \eta \log p(Y \mid \varphi, \tilde\theta) + \log p(\varphi) \right\} \nabla_{\lambda_1} \{ \varphi \} \nonumber    \\
                                                                                       & + \nabla_{\varphi} \left\{ \log p(Y \mid \varphi, \theta) + \log p(\varphi) + \log p(Y, \varphi) \right\} \nabla_{\lambda_1} \{ \varphi \} \nonumber                 \\
                                                                                       & + \nabla_{\lambda_1} \log \left| J_{T_1} \right| ] \label{eq:grad_elbo_l1_smi_naive}                                                                                 \\
  \nabla_{\lambda_2} \text{ELBO}^{(\eta-\text{smi})} = \E_{\epsilon \sim p(\epsilon)}[ & \nabla_{\theta} \left\{ \log p(Y \mid \varphi, \theta) + \log p(\theta \mid \varphi) \right\} \nabla_{\lambda_2} \{ \theta \} \nonumber                              \\
                                                                                       & + \nabla_{\lambda_2} \log \left| J_{T_2} \right| ] \label{eq:grad_elbo_l2_smi}                                                                                       \\
  \nabla_{\lambda_3} \text{ELBO}^{(\eta-\text{smi})} = \E_{\epsilon \sim p(\epsilon)}[ & \nabla_{\tilde\theta} \left\{ \eta \log p(Y \mid \varphi, \tilde\theta) + \log p(\tilde\theta \mid \varphi) \right\} \nabla_{\lambda_3} \{ \tilde\theta \} \nonumber \\
                                                                                       & + \nabla_{\lambda_3} \log \left| J_{T_3} \right| ] \label{eq:grad_elbo_l3_smi}
\end{align}

By analysing these gradients, we notice that this naive approximation is not the best way to approximate the \acrshort*{smi} posterior.
When $\eta=1$, the $p_{\eta-smi}(\varphi,\theta)$ is equivalent to the conventional fully-Bayesian posterior.
Therefore it is desirable to obtain a match between \cref{eq:grad_elbo_l1_full,eq:grad_elbo_l1_smi_naive}, and also have symmetry between \cref{eq:grad_elbo_l2_full,eq:grad_elbo_l2_smi,eq:grad_elbo_l3_smi} \todo{why?}.
Nevertheless, the second line in \cref{eq:grad_elbo_l1_smi_naive} is inconvenient, creating bias in the estimation of the desired gradient.
Moreover, the term $\nabla_{\varphi} \log p(Y, \varphi) \nabla_{\lambda_1} \{ \varphi \}$, is intractable and only can be evaluated for very simple models.

We would like to remove the whole middle line from the gradient in \cref{eq:grad_elbo_l1_smi_naive} in order to obtain an equivalent approximation between \acrshort*{smi} posterior with $\eta=1$ and conventional full posterior.

To understand the role of this term in the optimization process, consider the following decomposition of our target \acrshort*{kl} divergerce (derivation in the supplement),
\begin{align*}
  \kl{ q(\varphi, \theta, \tilde\theta) }{ p_{\eta-smi}(\varphi, \theta, \tilde\theta \mid Z, Y) } & =                                                                                                             \\
  \kl{ q(\varphi, \tilde\theta) }{ p_{\eta-pow}(\varphi, \tilde\theta \mid Z, Y) }                 & + \E_{\varphi \sim q(\varphi)}\left[ \kl{ q(\theta \mid \varphi) }{ p(\tilde\theta \mid Y, \varphi) } \right]
\end{align*}

We see that $q(\varphi)$ plays in two positions here: on one hand, approximates the power posterior jointly with $\tilde\theta$; and on the other, it helps to improve the approximation to the conditional posterior $p(\theta \mid Y, \varphi)$
Removing the middle line in \cref{eq:grad_elbo_l1_smi_naive} is equivalent to removing the second "objective" for $q(\varphi)$.

Removing the problematic term is also justified intuitively in the case of $\eta=0$, where $p_{\eta-smi}(\varphi,\theta)$ is equivalent to the cut posterior.
In this case, we don't want $q(\eta)$ to receive any feedback from the $Y$ module.
Removing this term effectively accomplishes that for our variational approximation.

We propose a loss function $\mathcal{L}^{(\eta-\text{smi})}$ to approximate the \acrshort*{smi} posterior adequately,
\begin{align} \label{eq:vsmi_loss}
  \mathcal{L}^{(\eta-\text{smi})} = -\E_{(\varphi, \theta, \tilde\theta) \sim q(\varphi, \theta, \tilde\theta)}[ & \log p_{\eta-pow}(Z, Y, \varphi, \tilde\theta) \nonumber   \\
                                                                                                                 & + \log p(Z, Y, \cancel{\nabla}(\varphi), \theta) \nonumber \\
                                                                                                                 & - \log q(\varphi, \theta, \tilde\theta) ]
\end{align}
where $\cancel{\nabla}(\cdot)$ denotes the \texttt{stop\_gradient} operator, and $p_{\eta-pow}(Z, Y, \varphi, \tilde\theta) = p(Z\mid\varphi)p(Y \mid \varphi, \tilde\theta)^{\eta}p(\varphi, \tilde\theta)$


The gradients of $-\mathcal{L}^{(\eta-\text{smi})}$ are,
\begin{align}
  -\nabla_{\lambda_1} \mathcal{L}^{(\eta-\text{smi})} = \E_{\epsilon \sim p(\epsilon)}[ & \nabla_{\varphi} \left\{ \log p(Z \mid \varphi) + \eta \log p(Y \mid \varphi, \tilde\theta) + \log p(\varphi) \right\} \nabla_{\lambda_1} \{ \varphi \} \nonumber    \\
                                                                                        & + \nabla_{\lambda_1} \log \left| J_{T_1} \right| ] \label{eq:grad_smi_loss_l1}                                                                                       \\
  -\nabla_{\lambda_2} \mathcal{L}^{(\eta-\text{smi})} = \E_{\epsilon \sim p(\epsilon)}[ & \nabla_{\theta} \left\{ \log p(Y \mid \varphi, \theta) + \log p(\theta \mid \varphi) \right\} \nabla_{\lambda_2} \{ \theta \} \nonumber                              \\
                                                                                        & + \nabla_{\lambda_2} \log \left| J_{T_2} \right| ] \label{eq:grad_smi_loss_l2}                                                                                       \\
  -\nabla_{\lambda_3} \mathcal{L}^{(\eta-\text{smi})} = \E_{\epsilon \sim p(\epsilon)}[ & \nabla_{\tilde\theta} \left\{ \eta \log p(Y \mid \varphi, \tilde\theta) + \log p(\tilde\theta \mid \varphi) \right\} \nabla_{\lambda_3} \{ \tilde\theta \} \nonumber \\
                                                                                        & + \nabla_{\lambda_3} \log \left| J_{T_3} \right| ] \label{eq:grad_smi_loss_l3}
\end{align}

The \texttt{stop\_gradient} operator has the effect of removing a summand that would otherwise appear inside the expectation of \cref{eq:grad_smi_loss_l1}, namely
\begin{equation*}
  \nabla_{\varphi} \left\{ \log p(Z \mid \varphi) + \log p(Y \mid \varphi, \theta) + \log p(\varphi) \right\} \nabla_{\lambda_1} \{ \varphi \}
\end{equation*}

In the case $\eta=1$, we obtain a match between the gradients of \cref{eq:grad_elbo_l1_full,eq:grad_smi_loss_l1}, and symmetry between \cref{eq:grad_elbo_l2_full,eq:grad_smi_loss_l2,eq:grad_smi_loss_l3}, effectively approximating the full posterior.

In the case $\eta=0$, the gradient in \cref{eq:grad_smi_loss_l1} is not influenced by $Y$ or $\theta$, it only depends on elements from the first module.
The loss in this case can be interpreted as a two-stage optimisation procedure:

\begin{enumerate}
  \item Learn $q_{\lambda_1}(\varphi)$ through conventional variational inference over the module $Z$,
        \begin{equation*}
          \lambda_1^* = \argmin_{\lambda_1} \kl{ q_{\lambda_1}(\varphi) }{ p(\varphi \mid Z) }
        \end{equation*}
  \item Learn the conditional variational posterior $q_{\lambda_2}(\theta \mid \varphi)$ in a similar way to variational inference over the full model, but preserving the marginal posterior of $\varphi$ obtained from stage 1.
        \begin{equation*}
          \lambda_2^* = \argmin_{\lambda_2} \kl{ q_{\lambda_1^*,\lambda_2}(\varphi,\theta) }{ p(\varphi,\theta \mid Z, Y) }
        \end{equation*}
\end{enumerate}

This two-stage optimization procedure to target the variational \acrshort*{smi} posterior is similar to the interpretation of the cut posterior as a constrained optimization problem, as proposed by \cite{Yu2021variationalcut}.

The two-stage interpretation of our loss is also valid for other values $\eta \in (0,1)$, but the first stage would approximate the power posterior in that case.