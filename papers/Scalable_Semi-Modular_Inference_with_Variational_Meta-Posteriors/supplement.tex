\section{Modulating prior feedback}\label{sec:prior_feedback}

The material in this section does not address the main point of this paper, which is to present variational inference for \acrshort*{smi}.
However, the issue we cover here does not seem to have been addressed explicitly in the literature to date, and we need some supporting theory for one of our main examples, in \cref{subsec:exp_rnd_eff}.

In the standard Cut-model setup given above, a likelihood factor which is present in the analysis stage has been removed in the imputation stage.
For example, in \cref{eq:cut_posterior}, the likelihood $p(Y \mid \varphi,\theta)$ is absent in $p(\varphi \mid Z)$ and present in $p(\theta \mid Y,\varphi)$.
In some applications of Cut-posteriors and \acrshort*{smi}, going back to \cite{Liu2009modularization} and \cite{Jacob2017together}, and including the archaeological applications in \cite{Styring2017extensification}, \cite{Carmona2020smi} and \cite{Styring2022urban}, the misspecified module has no associated data. In these examples the feedback within a prior module is cut.
If a Cut is applied to a \textit{prior density} and we simply remove the prior factor at imputation then all that remains in the imputation posterior distribution is the base measure.
We have effectively replaced the imputation prior density with a constant, and this may be inappropriate in some settings.
However, we are free to choose the imputation prior and we should use this freedom.
%In all Cut model examples from the literature where this issue is relevant, the imputation prior distribution is taken to be proportional to volume measure.
In this section we discuss Cut-priors as a special case of Cut-posterior inference and point out that the choice of imputation prior must be justified.
%We can see this in \cref{eq:cut_posterior}: the likelihood factor $p(Y \mid \varphi,\theta)$, which is present in \cref{eq:phi_post}, the  imputation posterior $p(\varphi \mid Z,Y)$ for $\varphi$, has been removed in the Cut \cref{eq:cut_posterior}, where the imputation posterior is $p(\varphi \mid Z)$.

In order to see how this works, we take a simply structured example, as we did in \cref{fig:toy_multimodular_model}.
Consider a generative model of the form
\begin{align*}
  \varphi & \sim p(\cdot)                                  \\
  \theta  & \sim p(\cdot \mid \varphi)                     \\
  Y_i     & \sim p(\cdot \mid \varphi,\theta),\ i=1,...,n.
\end{align*}
This is the model represented by the leftmost graph in \cref{fig:cut-prior}.
\begin{figure}
  \centering
  \includegraphics[height=1.5in]{images/cut-prior.pdf}
  \caption{Graphical representation of Cut and \acrshort*{smi}-posterior inference with a Cut prior. Notation as \protect\cref{fig:toy_multimodular_model_2stg}. (Left) Graphical model with prior $p(\theta \mid \varphi)$ indicated by arrow $\varphi\to\theta$.  (Mid) First (imputation) stage, auxiliary variable $\tilde\theta$ introduced for $\varphi$ imputation. Prior $\varphi|\tilde\theta$ modulated as indicated by the red dashed line. (Right) Second (analysis) stage giving inference for $\theta$, now conditioning on the Cut distribution of $\varphi|Y$.}
  \label{fig:cut-prior}
\end{figure}
The posterior is
\begin{equation}\label{eq:bayes_in_cut_prior}
  p(\varphi,\theta \mid Y)\propto p(Y \mid \varphi,\theta)p(\varphi)p(\theta \mid \varphi).
\end{equation}
Suppose we want to cut feedback from $\theta$ into $\varphi$.
In the Cut-posterior for imputation of $\varphi$, the generative model for the data is
\begin{align*}
  \varphi      & \sim p(\cdot)                                         \\
  \tilde\theta & \sim \tilde p(\cdot)                                  \\
  Y_i          & \sim p(\cdot \mid \varphi,\tilde \theta),\ i=1,...,n.
\end{align*}
This model is represented by the middle graph in \cref{fig:cut-prior}.
The new parameter $\tilde\theta$ is an auxiliary variable with Cut prior $\tilde p(\cdot)$.
Some measure for $\tilde\theta$ is needed in the imputation of $\varphi$ as $\tilde\theta$ is unknown and present in $p(Y \mid \varphi,\tilde \theta)$.
Work to date takes volume measure in the parameter space of $\theta$ without generally remarking that a choice had to be made (for example Equation~17 in \cite{Jacob2017together}). Exceptions include \cite{Moss2022}, who take a carefully constructed Cut prior $\tilde p$, and \cite{Styring2017extensification} and \cite{Yu2021variationalcut}, who remark on the choice.
The issue doesn't arise when we remove a likelihood factor, as any parameters enter the likelihood as conditioned variables.
%and $\tilde Y$ is the data vector of random variables determined by this generative model, and we enforce $\tilde Y=Y$ in a Cut-posterior analysis - this is just the statement that we use different models in the imputation and analysis.
In the analysis stage the generative model is
\begin{align*}
  \theta & \sim p(\cdot \mid \varphi)                     \\
  Y_i    & \sim p(\cdot \mid \varphi,\theta),\ i=1,...,n.
\end{align*}
This model is represented by the rightmost graph in \cref{fig:cut-prior}.
The Cut-posterior is
\begin{equation}\label{eq:cut_posterior_CUTPRIOR}
  p_{\cut}(\varphi,\theta,\tilde\theta \mid Y)=p_{\cut}(\varphi,\tilde\theta \mid Y)p(\theta \mid Y,\varphi),
\end{equation}
where
\begin{equation}\label{eq:CUTPRIOR_cut_imputation_post}
  p_{\cut}(\varphi,\tilde\theta \mid Y)\propto p(\varphi)\tilde p(\tilde\theta)p(Y \mid \varphi,\tilde\theta),
\end{equation}
and
\begin{equation}\label{eq:cut_prior_ThetaAnalysis_post}
  p(\theta \mid Y,\varphi)=p(\theta \mid \varphi)\frac{p(Y \mid \varphi,\theta)}{p(Y \mid \varphi)}.
\end{equation}
The cut prior $\tilde p(\tilde\theta)$ used for imputation in \cref{eq:CUTPRIOR_cut_imputation_post} is a modelling choice, like the Bayes prior $p(\theta \mid \varphi)$ which appears in the posterior.
Typically $p(\theta \mid \varphi)$ is a Subjective Bayes prior expressing the relations linking $\theta$ and $\varphi$ available from physical considerations, but is misspecified, and $\tilde p(\tilde\theta)$ will typically be a non-informative Objective Bayes prior.

The loss function $l_{\cut}(Y; \varphi,\theta,\tilde\theta, \pi_0)$ for which
\[
  p_{\cut}(\varphi,\theta,\tilde\theta \mid Y)\propto \exp(-l_{\cut}(Y; \varphi,\theta,\tilde\theta, \pi_0))p(\varphi)\tilde p(\tilde\theta)p(\theta \mid \varphi)
\]
is a Gibbs posterior is
\begin{equation}\label{eq:HM_full_cut_loss}
  l_{\cut}(Y; \varphi,\theta,\tilde\theta, \pi_0)=-\log(p(Y \mid \varphi,\tilde\theta))-\log(p(Y \mid \varphi,\theta))+\log(p(Y \mid \varphi)).
\end{equation}
and $\pi_0(\varphi,\theta,\tilde\theta)=p(\varphi)\tilde p(\tilde\theta)p(\theta \mid \varphi)$ is the prior which we must specify as part of the loss as it appears in $p(Y \mid \varphi)$ and changes as belief is updated.

This Cut-posterior belief update which cuts feedback in a prior is order coherent in the sense of \cite{Bissiri2016}.
This is known \citep{Carmona2020smi} for Cut-posteriors with the ``standard'' setup of \cref{sec:mod_bayes}.
However, the Cut-posterior with imputation and analysis priors is qualitatively different.

\begin{proposition}\label{prop:cut_prior_is_OK}
  The Cut posterior in \cref{eq:cut_posterior_CUTPRIOR} with Cut-prior feedback is an order coherent belief update.
\end{proposition}
\begin{proof}\label{proof:cut_prior_is_OK}
  If we split the data $Y=(y^{(1)},Y^{(2)})$ then the imputation prior
  $\pi_0(\varphi,\tilde\theta)=p(\varphi)\tilde p(\tilde\theta)$ is updated to $\pi_1(\varphi,\tilde\theta)=p_{\cut}(\varphi,\tilde\theta \mid Y^{(1)})$ and the analysis prior $\pi_0(\theta \mid \varphi)=p(\theta \mid \varphi)$ is updated to $\pi_1(\theta \mid \varphi)=p(\theta \mid \varphi,Y^{(1)})$.
  The update is order coherent if
  \begin{align*}
    \exp(-l_{\cut}(Y; \varphi,\theta,\tilde\theta, \pi_0))\pi_0(\varphi,\theta,\tilde\theta) = \exp(-l_{\cut}(Y^{(2)}; \varphi,\theta,\tilde\theta, \pi_1))\pi_1(\varphi,\theta,\tilde\theta).
  \end{align*}
  Expanding the RHS using the updated prior $\pi_1$ and applying \cref{eq:HM_full_cut_loss},
  \begin{align*}
    RHS & = \exp(-l_{\cut}(Y^{(2)}; \varphi,\theta,\tilde\theta, \pi_1))p_{\cut}(\varphi,\tilde\theta,\theta \mid Y^{(1)})                                                                                                                                 \\
        & \propto \exp(-l_{\cut}(Y^{(2)}; \varphi,\theta,\tilde\theta, \pi_1))\ \times\ p(\varphi)\tilde p(\tilde\theta)p(Y^{(1)} \mid \varphi,\tilde\theta)\ \times\ p(\theta \mid \varphi)\frac{p(Y^{(1)} \mid \varphi,\theta)}{p(Y^{(1)} \mid \varphi)} \\
        & = \exp(-l_{\cut}(Y^{(2)}; \varphi,\theta,\tilde\theta, \pi_1)-l_{\cut}(Y^{(1)}; \varphi,\theta,\tilde\theta, \pi_0))p(\varphi)\tilde p(\tilde\theta)p(\theta \mid \varphi)                                                                       \\
        & =LHS
  \end{align*}
  where the last step holds so long as
  \[
    l_{\cut}(Y^{(2)}; \varphi,\theta,\tilde\theta, \pi_1)+l_{\cut}(Y^{(1)}; \varphi,\theta,\tilde\theta, \pi_0)=l_{\cut}(Y; \varphi,\theta,\tilde\theta, \pi_0).
  \]
  This is the property \cite{Nicholls2022smi} call prequential additivity. It is easilly verified here.
  By independence $p(Y \mid \varphi,\theta)=p(Y^{(2)} \mid \varphi,\theta)p(Y^{(1)} \mid \varphi,\theta)$ so,
  \begin{align*}
    l_{\cut}(Y^{(2)}; \varphi,\theta,\tilde\theta, \pi_1)+l_{\cut}(Y^{(1)}; \varphi,\theta,\tilde\theta, \pi_0) = &
    -\log(p(Y \mid \varphi,\tilde\theta))-\log(p(Y \mid \varphi,\theta))                                                                                                                   \\
                                                                                                                  & \ +\log(p(Y^{(2)} \mid \varphi,Y^{(1)}))+\log(p(Y^{(1)} \mid \varphi))
  \end{align*}
  with
  \begin{align*}
    \log(p(Y^{(2)} \mid \varphi,Y^{(1)}))+\log(p(Y^{(1)} \mid \varphi)) & = \log\left(\frac{p(Y^{(2)},\varphi,Y^{(1)})p(\varphi)}{p(\varphi,Y^{(1)})p(\varphi)}\right)+\log(p(Y^{(1)} \mid \varphi)) \\
                                                                        & =\log(p(Y \mid \varphi)).
  \end{align*}
\end{proof}

When we do \acrshort*{smi} with modulated priors we interpolate between the Bayes posterior in \cref{eq:bayes_in_cut_prior} and Cut posterior in \cref{eq:cut_posterior_CUTPRIOR}. Let $p_\eta(\tilde\theta \mid \varphi)$ be a family of probability densities indexed by $\eta$ and satisfying
$p_{\eta=0}(\tilde\theta \mid \varphi)=\tilde p(\tilde\theta)$ (modulated prior equals Cut prior) and $p_{\eta=1}(\tilde\theta \mid \varphi)=p(\tilde\theta \mid \varphi)$ (modulated prior equals Bayes prior).
%
We take
\begin{equation}\label{eq:smi_full_cutprior}
  p_{\smi,\eta}(\varphi,\theta,\tilde\theta \mid Y)=p_{\pow, \eta}(\varphi,\tilde\theta \mid Y)p(\theta \mid Y,\varphi)
\end{equation}
where in this setting with a Cut prior
\[
  p_{\pow, \eta}(\varphi,\tilde\theta \mid Y) \propto p(Y \mid \varphi,\tilde\theta)p(\varphi) p_\eta(\tilde\theta \mid \varphi),
\]
and $p(\theta \mid Y,\varphi)$ is given in \cref{eq:cut_prior_ThetaAnalysis_post}, so the second stage analysis is the Cut posterior analysis.
This ensures $p_{\smi,0}=p_{\cut}$ and $p_{\smi,1}$ gives the Bayes posterior. Taking a normalised family $p_\eta(\tilde\theta \mid \varphi),\ \eta\in [0,1]$ of interpolating priors ensures that the marginal prior for $p(\varphi)$ in the imputation doesn't depend on $\eta$. An un-normalised family such as $p_\eta(\varphi,\tilde\theta)\propto \tilde p(\tilde\theta)^{1-\eta} p(\tilde\theta \mid \varphi)^\eta$ has all the desired interpolating properties, but the marginal $p(\varphi)$ in the imputation stage will then depend on $\eta$.
In some settings (for example when working with normal priors with fixed variance) the two prior parameterisations may be equivalent as $\eta$ scales the variance.

\acrshort*{smi} with modulated prior feedback is an order coherent belief update. We now give \cref{prop:smi_cut_prior_is_OK}, stated in \cref{sec:mod_bayes}.

\noindent{\bf{\Cref{prop:smi_cut_prior_is_OK}}.}
{\it
  The \acrshort*{smi} posterior in \cref{eq:smi_posterior_CUTPRIOR} with cut prior feedback is an order coherent belief update.
}
\begin{proof} \label{proof:smi_cut_prior_is_OK}
  Replace $\tilde p(\tilde\theta)\to p_\eta(\tilde\theta \mid \varphi)$ in the proof of \cref{prop:cut_prior_is_OK}.
\end{proof}
In contrast to the tempered likelihood in \cref{eq:powjoint_toymodel} used in Cut-likelihood \acrshort*{smi} in \cref{eqn:smi_01}, where the tempered likelihood is not normalised over $Y$, the modulated prior $p_\eta(\tilde\theta \mid \varphi)$ is a normalised distribution over $\tilde\theta$. However, \acrshort*{smi} in \cref{eq:smi_full_cutprior} with a modulated prior is not simply Bayesian inference with a revised prior, as the priors in the imputation and analysis stages are not the same (unless $\eta=1$).

% \section{Proofs for Propositions in Section~\protect\ref{sec:mod_bayes}}
%
% \noindent{\bf{\Cref{prop:cut_prior_is_OK}}.}
% {\it
%   The Cut posterior in \cref{eq:cut_posterior_CUTPRIOR} with Cut-prior feedback is an order coherent belief update.
% }

\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Variational families} \label{sec:variational_families_details}
%=============================

\subsection{Choice of transformations for SMI} \label{subsec:smi_flow}

The transformations $T_1$ and $T_2$ introduced in \cref{eq:bayes_transform,eq:smi_transform} determine our flow-based variational approximation to the \acrshort*{smi}-posterior. We make a few standard assumptions about these transformations:
\begin{itemize}
  \item They are diffemorphisms, meaning that they must be differentiable, invertible and their inverse must be differentiable (needed as we wish to be able to evaluate, differentiate and sample the density $q_\lambda$),
  \item They map the domain of the base distribution onto the domain of the corresponding model parameter (e.g. if $\varphi$ represents a probability, we need $\varphi=T(\epsilon_1)\in[0,1]$),
  \item They support fast sampling from the flow-based density, and fast computation of the density at the sampled values. This allows efficient computation of the forward transformation and the determinant of its Jacobian matrix.
\end{itemize}
Additionally, there are two attributes of $T_1$ and $T_2$ that are important for an accurate approximation, but not strict requirements, namely:
\begin{itemize}
  \item They are expressive transformations, ideally \acrlongpl*{nf} with the property of being a universal approximator,
  \item $T_2$ is defined as a conditional transformation, so that correlation in the posterior of $\varphi$ and $\theta$ can be represented.
\end{itemize}

These requirements are met by \acrfullpl*{nf}. There are now many ways to define expressive flow-based transformations for $T_1$ and $T_2$ \citep[see][for reviews]{Kobyzev2020normalizing,Papamakarios2021normalizing}.
For our experiments in \cref{sec:experiments}, we defined these transformations by the composition of multiple \emph{coupling} layers
% \footnote{
%   Coupling layers split the input vector of random variables, $z$ say, into two parts such that $z=(z_{\leq d}, z_{>d})$. The first part is transformed elementwise independently of other dimensions. The second part is transformed elementwise in a way that depends on the first part.
% }
based on the \acrfull*{nsf} \citep{Durkan2019neural}, followed by a last layer that maps to the domain of the model parameters.
In order to allow posterior correlation between $\varphi$ and $\theta$, we defined $T_2$ as a \emph{conditional} transformation, so that the elementwise transformation from $\epsilon_2$ to $\theta$ (and from $\epsilon_3$ to $\tilde\theta$) also depends on $\epsilon_1$.



% we use neural spline flows

% As an example of how to define the conditional transformation is by


% The variational parameters $\beta_2$ are typically parameters of the conditioner $c$.

% \subsection*{Mean-Field}

% Denote $\varphi=(\varphi_1,\ldots,\varphi_{13})^t \in \mathcal{R}^{13}$ and $\theta=(\theta_1,\theta_2)^t \in \mathcal{R}^2$. We show how to perform variational inference on the SMI posterior of this model using the proposed
% two-stage optimisation procedure.

% \subsubsection*{Full-rank Gaussian}
% Define the vector $\mu$ and the lower triangular matrix $L$ (with positive diagonal elements) as

% \subsubsection*{Inverse Autoregressive Flow (IAF)}
% Following \cite{Kingma2016iaf}
% $T_1$ is a Autoregressive Neural Network, MADE-like \cite{Germain2015made}
% $T_2$ is a Conditional Autoregressive Neural Network, using $\varphi$ as context information.

% \subsubsection*{Rational Quadratic Spline Flow}
% Following \cite{Durkan2019neural}

\subsection{Normalizing Flows.} \label{sec:nf}

\subsubsection{Conditioners and transformers in a general flow}
We now define the maps $T_1: \Re^{p_\phi}\to \Re^{p_\phi}$ and $T_2: \Re^{p_\theta+p_\phi}\to \Re^{p_\theta}$ in the normalising flows in terms of their transformers and conditioners. The material in the section is based on \cite{Papamakarios2021normalizing} and further detail may be found there.
Let $x=(x_1,...,x_p)$ and $v=(v_1,...,v_q)$ be generic real vectors and let \[x'_i=\tau(x_i,h_i)\] transform $x_i$ using parameter vectors $h_i\in \Omega_h,\ i=1,...,p$. Here \[h_i=c_i(x_{<i};v,w_i)\] is the output of a conditioner parameterised by a vector $w_i$ with $c_i$ taking as input the conditioning variables $x_{<i}=(x_1,...,x_{i-1})$ and $v$. Note that the conditioner argument $x_{<i}$ in $c_i$ changes dimension as we step through the components $i=1,...,p$ but all the component updates condition on a common set of shared variables $v$. In our setting the conditioners $c_i: \Re^{i-1+q}\to \Omega_h$ are \acrshortpl*{mlp} and $w_i$ are the weights in the $i$'th net. The transformer is a strictly monotonic function. For example, if it is affine then $h_i=(m_i,s_i)$ and $\tau(x_i,h_i)=m_i+s_ix_i$. We experimented with a range of transformers and settled on \emph{rational-quadratic spline transformers} \citep{Durkan2019neural}.
 
One pass over the flow composes these maps 
\[x_i\leftarrow \tau(x_i,c_i(x_{<i};v,w_i)) \quad \mbox{for $i=1,...,p$}\] to update all components $x\to x'$.
Denote by $g: \Re^{p+q}\to \Re^p$, $x'=g(x;v,w)$ a map formed in this way, with $w$ the set of all parameters present in the composition of maps. We make multiple passes over the components, with independent sets of parameters $w$, permuting the indices in order to get an expressive flow. The auto-regressive dependence of $x'_i$ on $x_{<i}$ and the monotone transformer ensure the map is invertible, \[x=g^{-1}(x';v,w)\] given the values of the shared conditioning variables $v$ and parameters $w$, with a tractable lower triangular Jacobian.

In a variant of this setup we used a composition of \emph{coupling layer conditioners} \citep{Dinh2016realnvp} to define $g$. These have the advantage that both density evaluation and sampling are fast operations, important for fitting the flow to the target, and then sampling the related flow to estimate the utility. In this case we set $x'_i=x_i,\ i=1,...,d$ with $d=\lfloor p/2\rfloor$ and
\[x'_i=\tau(x_i,h_i),\quad\mbox{with}\quad h_i=c_i(x_{\le d};v,w_i)\qquad \mbox{ for $i=d+1,...,p$.}\]
The Jacobian determinant from one application of the map is just $\prod_{i>d} |\partial \tau(x_i,h_i)/\partial x_i|$. Again we compose the maps over different permutations of the indices of $x$ (eight times) so that each entry appears in the conditioner and as output from the transformer to get an overall map $g(x;v,w)$.

\subsubsection{Normalizing flows for variational SMI}
In variational-\acrshort*{smi} in \cref{subsec:vsmi}, the \acrshort{nf} has the form 
\begin{align}
\varphi_{(\lambda_1,\epsilon)}&=T_1(\epsilon_1;\lambda_1)\nonumber\\
&=g(\epsilon_1;\emptyset,\lambda_1)\nonumber\\
\intertext{
as $\lambda_1$ parameterises the conditioners of $\varphi$ with no additional conditioners,}
\theta_{(\lambda_2,\epsilon)}&=T_2(\epsilon_2;\lambda_2,\epsilon_1)\nonumber\\
&=g(\epsilon_2;\epsilon_1,\lambda_2)\nonumber\\
\intertext{as $\theta$ is conditioned on $\epsilon_1$ and hence $\varphi$ and} 
\tilde\theta_{(\lambda_3,\epsilon)}&=T_2(\epsilon_3;\lambda_3,\epsilon_1)\nonumber\\
&=g(\epsilon_3;\epsilon_1,\lambda_3),\nonumber
\intertext{similarly $\tilde\theta$, and together}
T(\epsilon; \lambda)                & = \left( T_1(\epsilon_1;\lambda_1) ,\; T_2(\epsilon_2;\lambda_2, \epsilon_1) ,\; T_2(\epsilon_3;\lambda_3, \epsilon_1) \right). \label{eq:smi_transform_details}
\end{align}
These equations define the maps in \cref{eq:smi_transform}. When we want to train this to approximate $p_{\smi,\eta}$ we simply retrain at each $\eta$ where $q_{\lambda^*(\eta)}$ is needed.

\subsubsection{Parameterising the VMP-map}
In the \acrshort*{vmp}, we consider two ways to parameterise the flow defining the meta-posterior.
In the first, in \cref{eq:vmp-map-transform-variables-alpha123} in \cref{subsec:learning_vmp_map}, we define a function (a \acrshort*{mlp}) \[f_\alpha(\eta)=(f^{(1)}_{\alpha_1}(\eta),f^{(2)}_{\alpha_2})(\eta),f^{(3)}_{\alpha_3}(\eta))\] in which the output components correspond to $(\lambda_1(\eta),\lambda_2(\eta),\lambda_3(\eta))$ at any $\eta\in [0,1]^C$. The corresponding mappings in terms of the \acrshortpl*{nf} and their inputs are
\begin{align}
\varphi_{(\lambda_1(\alpha_1,\eta),\epsilon)}&=T_1(\epsilon_1;\lambda_1(\eta))\nonumber\\
&=g(\epsilon_1;\emptyset,f^{(1)}_{\alpha_1}(\eta))\nonumber\\
\theta_{(\lambda_2(\alpha_2,\eta),\epsilon)}&=T_2(\epsilon_2;\lambda_2(\eta),\epsilon_1)\nonumber\\
&=g(\epsilon_2;\epsilon_1,f^{(2)}_{\alpha_2}(\eta))\nonumber\\
\tilde\theta_{(\lambda_3(\alpha_3,\eta),\epsilon)}&=T_2(\epsilon_3;\lambda_3(\eta),\epsilon_1)\nonumber\\
&=g(\epsilon_3;\epsilon_1,f^{(3)}_{\alpha_3}(\eta)).\nonumber\\
T(\epsilon; \lambda(\eta))                & = \left( T_1(\epsilon_1;\lambda_1(\eta)) ,\; T_2(\epsilon_2;\lambda_2(\eta), \epsilon_1) ,\; T_2(\epsilon_3;\lambda_3(\eta), \epsilon_1) \right). \label{eq:vmp-map-transform-variables-alpha123_details}
\end{align}
This keeps the conditioner-transformer relations unchanged from \cref{eq:smi_transform_details}, and simply injects the right parameters $\lambda$ into the flow to express $p_{\smi,\eta}$ at any particular $\eta$.

A perfectly trained universal \acrshort*{vmp}-map $f_\alpha(\eta)$ would inject an optimal set $\lambda^*(\eta)$ of parameters into the \acrlong*{nf} at every $\eta$. 
We assume $f_{\alpha}(\eta)$ is a continuous function of $\eta$, motivated by the discussion above. We require $f_{\alpha}(\eta)$ to be differentiable almost everywhere in $\alpha$ for the purpose of optimisation. These are minimal assumptions. %, and continuously differentiable if we want the theory we developed for $q_\lambda$ to extend (without detailed proof) to $q_{f_{\alpha}(\eta)}$.
In order to extend the theory from \cref{sec:vi_modular} to this setting, suppose we have $\lambda^*(\eta)\in \Lambda^*$ where $\Lambda^*(\eta)$ is given in \cref{defn:var-smi} (at $\eta$). Suppose $f_\alpha(\eta)$ is continuously differentiable in $\alpha$ at each $\eta$ and there exists $\alpha^*$ such that $f_{\alpha^*}(\eta)=\lambda^*(\eta),\ \eta\in H$. In this case substituting $\lambda=f_{\alpha}(\eta)$ into $\mathcal{L}^{\smi,\eta}$ in \cref{prop:stop-gradient-loss} and minimising over $\alpha$ will give the same $\lambda^*$ with the same properties (P1-3).

Referring to the loss in \cref{eq:vsmi_loss}, let
\[
  A^*=\{\alpha\in A: \mathcal{L}^{(\msmi-map)}(\alpha)=\min_{a\in A} \mathcal{L}^{(\msmi-map)}(a)\}
\]
be the set of optimal $\alpha$-values.
If for some $\alpha^*\in A$, the \acrshort*{vmp}-map $f_{\alpha^*}(\eta)$ expresses the function $\lambda^*(\eta)$ perfectly at $\eta\in\eta_{1:R}$, that is if $f_{\alpha^*}(\eta_r)\in \Lambda^*(\eta_r),\ r=1,...,R$ then
\[
  A^*=\{\alpha\in A: f_{\alpha}(\eta_r)\in \Lambda^*(\eta_r),\ r=1,...,R\}
\]
since these solutions in $\alpha$ and no others minimise $\mathcal{L}^{(\smi,\eta)}$ at every $\eta$ in the sum in \cref{eq:meta_smi_target_loss}.
In this case we recover $\lambda^*\in \Lambda^*$ and (P1-3) at each $\eta\in \eta_{1:R}$.

For our examples in section 5, we found three Multi-Layer Perceptrons (MLPs) in
parallel, trained with input $\eta$ and output $f^{(k)}_{\alpha_k}(\eta),\ k=1,2,3$ expressed the non-linear
relationships holding between variational parameters, and scaled well with increasing
numbers of cuts (dimension of input $\eta \in H$).
We report experiments with  \acrshort*{vmp}-maps parameterised with \acrfullpl*{gp} \citep{Rasmussen2005gp} and Cubic Splines \citep{Hastie2001esl}. These may be convenient when the dimension of $H$ is small.

We found in training $f_{\alpha}$ that $\alpha$ should be initialised to output a constant function $f_{\alpha_0}(\eta)\approx\lambda_0$ independent of $\eta$. Here $\lambda_0$ are variational parameters obtained in a ``pre-training stage'' using \cref{alg:vsmi} for a fixed central value of $\eta$.
This strategy significantly reduces training time and improves convergence.

\subsubsection{Parameterising the VMP-flow}
The other way we parameterise the meta-posterior is to modify the conditioner. Consider the coupling layer setup. In order to express the $\eta$-dependence (in say the coupling layer conditioner), $x'_i=x_i,\ i=1,...,d$ and
\[
x'_i=\tau(x_i,h_i), \quad h_i=c_i(x_{\le d};v,w_i)+c'(\eta;w'_i),\quad \mbox{for $i=d+1,...,p$}.
\]
Here $\eta$ enters via a second additive conditioner $c'$ (which is not indexed by $i$ as it always takes the same conditioning variable $\eta$ but has parameters $w'_i$ which vary across variables $x_i,\ i>d$). Like $v$, $\eta$ is a common conditioner in every transform. When we compose this map 
to form the overall map, $g(x;(v,\eta),(w,w'))$ say, \emph{each} application of the map has its own sets of parameters $w_i$ and $w'_i$ \emph{for each} $i=d+1,...,p$. When we parameterise the meta-posterior we write $\alpha_k=(\lambda_k,\mu_k),\ k=1,2,3$ and
\begin{align}
    \varphi_{(\alpha_1,\eta,\epsilon)}&=T_1(\epsilon_1;\alpha_1,\eta)\nonumber\\
    &=g(\epsilon_1;(\emptyset,\eta),(\lambda_1,\mu_1)),\label{eq:meta_new_map_phi}\\
    \theta_{(\alpha_2,\eta,\epsilon)}&=T_2(\epsilon_2;\alpha_2, (\eta,\epsilon_1))\nonumber\\
    &=g(\epsilon_2;(\epsilon_1,\eta),(\lambda_2,\mu_2)),\label{eq:meta_new_map_theta}\\
    \tilde\theta_{(\alpha_3,\eta,\epsilon)}&=T_2(\epsilon_3;\alpha_3,(\eta,\epsilon_1))\nonumber\\
    &=g(\epsilon_3;(\epsilon_1,\eta),(\lambda_3,\mu_3)),\label{eq:meta_new_map_theta_tilde}\\
    T(\epsilon;\alpha,\eta) & = \left( T_1(\epsilon_1;\alpha_1,\eta) ,\; T_2(\epsilon_2;\alpha_2, (\eta,\epsilon_1)) ,\; T_2(\epsilon_3;\alpha_3,(\eta,\epsilon_1)) \right).
\end{align}
so that $\lambda=(\lambda_1,\lambda_2,\lambda_3)$ contain the $w$-parameters of the ``old'' conditioner $c_i$ in each of the flows for $\varphi,\theta$ and $\tilde\theta$ respectively and $\mu=(\mu_1,\mu_2,\mu_3)$ are the corresponding $w'$-parameters of the new $\eta$-dependent conditioner $c'$. These are collectively  the \acrshort{vmp} parameters $\alpha=(\alpha_1,\alpha_2,\alpha_3)$.
We take $\mu\in M$ and $\alpha\in A$ with $A=\Lambda\times M$. The influence parameter $\eta$ enters through the conditioner so it is associated with $\epsilon_1$ as a conditioner for $\theta$ and $\tilde\theta$ and as a conditioner on its own for $\varphi$.

The family of variational densities $q_{\alpha,\eta}(\varphi,\theta,\tilde\theta)$ defined by the \acrshort*{vmp}-flow are given in \cref{eq:smi_transform_new_vmp}. The optimal parameters $\alpha^*=(\lambda^*,\mu^*)$, $\alpha^*\in A$, minimise a loss $\mathcal{L}^{(\msmi-flow)}(\alpha)$ which is closely related to $\mathcal{L}^{(\msmi-map)}(\alpha)$ in \cref{eq:meta_smi_target_loss}. In more detail,
\begin{align} \label{eq:vsmi_loss-meta-new}
  \mathcal{L}^{( \msmi-flow, \eta)}(\alpha) = \elbo_{\pow, \eta}(\alpha_1,\alpha_3) + \elbo_{\bayes \cancel{\nabla}(\varphi)}(\alpha_1,\alpha_2)
\end{align}
where
\begin{align}
  \elbo_{\pow, \eta}(\alpha_1,\alpha_3) = \E_{(\varphi,\tilde\theta)\sim q_{\alpha_1,\alpha_3,\eta}}[                & \log p_{\pow, \eta}(\varphi, \tilde\theta, Z, Y) - \log q_{\alpha_1,\alpha_3,\eta}(\varphi, \tilde\theta) ] \label{eq:elbo_modular_pow_meta_new}                  \\
  \elbo_{\bayes \cancel{\nabla}(\varphi)}(\alpha_1,\alpha_2,\eta) = \E_{(\varphi,\theta)\sim q_{\alpha_1,\alpha_2,\eta}}[ & \log p( \cancel{\nabla}(\varphi), \theta, Z, Y) - \log q_{\alpha_1,\alpha_2,\eta}(\cancel{\nabla}(\varphi), \theta) ]. \label{eq:elbo_modular_stop_grad_meta_new}
\end{align}
The loss in \cref{eq:meta_smi_target_loss} given for the map becomes for the flow,
\[
 \mathcal{L}^{(\msmi-flow)}(\alpha) =\E_{\eta\sim\rho}\left(\mathcal{L}^{(\msmi-flow,\eta)}(\alpha)\right).
\]
In order to apply the reparameterisation trick, the expressions for $\varphi_{(\alpha_1,\eta,\epsilon)}, \theta_{(\alpha_2,\eta,\epsilon)}$ and $\tilde\theta_{(\alpha_3,\eta,\epsilon)}$ in \cref{eq:meta_new_map_phi}-\cref{eq:meta_new_map_theta_tilde} are substituted into \cref{eq:elbo_modular_pow_meta_new} and \cref{eq:elbo_modular_stop_grad_meta_new} so that gradients in $(\alpha_1,\alpha_3)$ and $\alpha_2$ can be taken inside the expectation which is now over $\epsilon\sim p(\cdot)$.

\newpage
\section{Optimisation of the VMP loss}\label{sec:sgd-for-meta-losses}

Here we give the algorithm we use to estimate the the parameters $\alpha$ of the \acrshort*{vmp}-map and -flow. \Cref{alg:v_meta_smi} gives \acrshort*{sgd} for the \acrshort*{vmp}-map loss $\mathcal{L}^{(\msmi-map)}(\alpha)$ (notation $\mathcal{L}^{(\msmi)}(\alpha)$ as the algorithm for optimisation of $\mathcal{L}^{(\msmi-flow)}(\alpha)$ is similar).

\begin{algorithm}[tbh]
  \caption{Variational Meta-Posterior approximation for $\mathcal{P}_{\smi}=\{p_{\smi,\eta},\ \eta\in H\}$} \label{alg:v_meta_smi}
  \begin{algorithmic}
    %\STATE This algorithm assumes that we want to partially cut the influence of module 2 (with response $Y$) into $\varphi$.
    \STATE \textbf{Input:} $\mathcal{D}$: Data. $p(\varphi,\theta,\mathcal{D})$: Multi-modular probabilistic model. $q_\lambda=(p(\epsilon), T, \lambda)$: Variational family. $f_{\alpha}(\eta)$: \acrshort*{vmp}-map. $\rho$: $\eta$-weighting distribution over $H$. \\[0.1in]
    \STATE \textbf{Output:} \acrshort{vmp}-map $f_{\hat\alpha}(\eta)$ giving the Variational approximation for the $\mathcal{P}_{\smi}$ family.\\[0.1in]

    \STATE Initialise mapping parameters $\alpha$
    \WHILE{\acrshort*{sgd} not converged}
    \STATE Sample $\mathcal{D}^{(b)} \sim \mathcal{D}$ (random minibatch of data).
    \STATE Sample $R$ values of $\eta_{r}\sim \rho,\ r=1,...,R$.
    \FOR{$r = 1,\ldots,R$}
    \STATE Obtain variational parameters $\lambda_r = f_{\alpha}(\eta_r)$.
    \FOR{$s = 1,\ldots,S$}
    \STATE Sample the base distribution, $\epsilon_{r,s} \sim p(\cdot)$.
    \STATE Transform the sampled values $(\varphi_{r,s}, \theta_{r,s}, \tilde\theta_{r,s}) \leftarrow T_{\lambda_r}(\epsilon_{r,s})$ as in \cref{eq:smi_transform}.
    \ENDFOR
    \ENDFOR
    \STATE Compute the Monte Carlo estimate of the loss $\mathcal{L}^{( \msmi)}$ in \cref{eq:meta_smi_target_loss} and its gradients.
    \begin{equation}
      \widehat{\mathcal{L}}^{(\msmi)} = \widehat{\elbo}_{\pow}^{(\msmi)} + \widehat{\elbo}_{\cancel{\nabla}(\varphi)}^{(\msmi)}
    \end{equation}
    where
    \begin{align}
      \widehat{\elbo}_{\pow}^{(\msmi)}                     & = - \frac{1}{RS} \sum_{r=1}^{R}\sum_{s=1}^{S} \left[ \log p_{\pow, \eta_r}(\varphi_{r,s}, \tilde\theta_{r,s}, \mathcal{D}^{(b)}) - \log q(\varphi_{r,s}, \tilde\theta_{r,s}) \right]         \\
      \widehat{\elbo}_{\cancel{\nabla}(\varphi)}^{(\msmi)} & = - \frac{1}{RS} \sum_{r=1}^{R}\sum_{s=1}^{S} \left[ \log p(\cancel{\nabla}(\varphi_{r,s}), \theta_{r,s}, \mathcal{D}^{(b)}) - \log q( \cancel{\nabla}(\varphi_{r,s}), \theta_{r,s}) \right]
    \end{align}
    \STATE Update $\alpha$ using the estimated gradient vector $\nabla_{\alpha}\widehat{\mathcal{L}}^{(\msmi)}$
    \STATE Check convergence of $q_{f_{\alpha}(\eta)}(\varphi,\theta)$ for multiple $\eta \in H$
    \ENDWHILE

    \RETURN $\hat\alpha=\alpha$ %and $q_{f_{\hat\alpha}(\eta)}(\varphi,\theta)$.

  \end{algorithmic}
\end{algorithm}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Detailed derivation of Variational Modular posteriors.}
%=============================

%=============================
\subsection{Proofs for variational SMI properties} \label{sec:vsmi_detail}

\noindent{\bf{\Cref{prop:var_smi_distance}.}}
{\it
  The divergence defined in \cref{eqn:distance_to_set} can be written
  \[
    d(q_{\lambda},\mathcal{F}_{\smi, \eta})=\kl{q_{\lambda_1, \lambda_3}(\varphi, \tilde\theta)}{p_{\pow, \eta}(\varphi,\tilde\theta \mid Y,Z)},
  \]
  and hence does not depend on $\lambda_2$.
}
\begin{proof}\label{proof:var_smi_distance}
  Since $\tilde q\in \mathcal{F}_{\smi, \eta}$ it can be written
  $\tilde q(\varphi,\theta,\tilde\theta)=p_{\pow, \eta}(\varphi,\tilde\theta \mid Y,Z)\tilde q(\theta\mid \varphi,\tilde\theta)$
  where $\tilde q(\theta\mid \varphi,\tilde\theta)$ is an arbitrary conditional density. The divergence is
  \begin{align*}
    \min_{\tilde q\in \mathcal{F}_{\smi, \eta}}\kl{q_{\lambda}}{\tilde q} & =\min_{\tilde q(\theta\mid \varphi,\tilde\theta)}\kl{q_{\lambda_1,\lambda_3}(\varphi,\tilde\theta)q_{\lambda_2}(\theta\mid \varphi)}{p_{\pow, \eta}(\varphi,\tilde\theta \mid Y,Z)\tilde q(\theta\mid \varphi,\tilde\theta)}
    \\
                                                                          & =\kl{q_{\lambda_1, \lambda_3}(\varphi, \tilde\theta)}{p_{\pow, \eta}(\varphi,\tilde\theta \mid Y,Z)}
    \\
                                                                          & \qquad +\quad \min_{\tilde q(\theta\mid \varphi,\tilde\theta)} E_{\varphi\sim q_{\lambda_1}}[\kl{q_{\lambda_2}(\theta\mid \varphi)}{\tilde q(\theta\mid \varphi,\tilde\theta)}]
    \\
                                                                          & =\kl{q_{\lambda_1, \lambda_3}(\varphi, \tilde\theta)}{p_{\pow, \eta}(\varphi,\tilde\theta \mid Y,Z)}
  \end{align*}
  as the variation over $q(\theta\mid \varphi,\tilde\theta)$ is over \emph{all} conditional densities with optimum
  $\tilde q(\theta\mid \varphi,\tilde\theta)=q_{\lambda_2}(\theta\mid \varphi)$ since the expectation is non-negative and zero in that case.
\end{proof}

% \noindent {\bf Proposition~\ref{prop:var_smi_lambda2}}
% {\it
%   The set $\Lambda^*_{(2)}(\lambda^*_1)$ defined in \cref{eqn:lambda2-star-defn-equiv} is equivalently
%   \begin{align}
%   \tilde D^*_{\smi}(\lambda^*_1)&=\min_{\lambda_2\in \Lambda_2} \kl{q_{(\lambda^*_1,\lambda_2,\lambda^*_3)}}{p_{\smi, \eta}}\nonumber\\
%     \intertext{and}
%     \Lambda^*_{(2)}(\lambda^*_1)&=\{\lambda_2\in \Lambda_2:  \kl{q_{(\lambda^*_1,\lambda_2,\lambda^*_3)}}{p_{\smi, \eta}} = \tilde D^*_{\smi}(\lambda^*_1)\}.
% \end{align}
% }
% \begin{proof}
%   Expand the \acrshort*{kl} divergence in \cref{eqn:lambda2-star-defn} using \cref{eqn:naive_VI_KL_expand} and substitute
%   $(\lambda_1,\lambda_3)=(\lambda^*_1,\lambda^*_3)$.
%   The first term does not depend on $\lambda_2$ and the second term gives \cref{eqn:lambda2-star-defn-equiv}.
% \end{proof}



\noindent{\bf{\Cref{prop:var_smi_is_cut_at_eta0}.}}
{\it
Variational \acrshort*{smi} satisfies property (P1) at $\eta=0$: If the set
\[
  \Lambda^*_{(3)}=\{\lambda_3\in \Lambda_2: q_{\lambda^*_3}(\tilde\theta\mid \varphi)=p(\tilde\theta\mid \varphi)\}
\]
is non-empty and we set
\[
  \Lambda^*_{(1)} =   \{\lambda_1\in \Lambda_1:
  \kl{q_{\lambda_1}(\varphi)}{p(\varphi \mid Z)}=d^*_{\cut}\}
\]
with $d^*_{\cut}$ defined in (P1) then $\Lambda^*_{(1,3)}$ defined in \cref{eqn:lambda13-star-defn} satisfies
\[
  \Lambda^*_{(1,3)} = \Lambda^*_{(1)} \times \Lambda^*_{(3)},
\]
% \[
% d(q_{(\lambda_1,\lambda_2,\lambda_3)})=\kl{q_{\lambda_1}(\varphi)}{p(\varphi \mid Z)}.
% \]
so $q_{\lambda^*_1}$ does not depend in any way on $p(Y \mid \varphi,\theta)$ or $p(\theta \mid \varphi)$ at $\eta=0$.
}
\begin{proof}\label{proof:var_smi_is_cut_at_eta0}
  At $\eta=0$, $p_{\pow, \eta=0}(\varphi,\tilde\theta \mid Y,Z)=p(\varphi \mid Z)\,p(\tilde\theta\mid \varphi)$ in \cref{prop:var_smi_distance}. By \cref{eqn:lambda13-star-defn},
  \begin{align*}
    d^*_{\smi} & =\min_{(\lambda_1,\lambda_3)\in B}\kl{q_{\lambda_1,\lambda_3}}{p(\varphi \mid Z)\,p(\theta\mid \varphi)}
    \\
               & =\min_{(\lambda_1,\lambda_3)\in B} \left(\kl{q_{\lambda_1}(\varphi)}{p(\varphi \mid Z)}+E_{\varphi\sim q_{\lambda_1}}[\kl{q_{\lambda_3}(\tilde\theta\mid \varphi)}{p(\tilde\theta\mid \varphi)}]\right) \\
               & =\min_{\lambda_1\in \Lambda_1} \kl{q_{\lambda_1}(\varphi)}{p(\varphi \mid Z)}                                                                                                                           \\
               & =d^*_{cut}
  \end{align*}
  as the expectation is zero (and hence minimised for every argument) when $\lambda^*_3\in \Lambda_2$ satisfies $q_{\lambda^*_3}(\tilde\theta\mid \varphi)=p(\tilde\theta\mid \varphi)$. Such a $\lambda^*_3$ exists because $\Lambda^*_{(3)}$ is non-empty by the assumption in the proposition. It follows that $\lambda$ minimises $d(q_\lambda,\mathcal{F}_{\smi, \eta})$ at $\eta=0$ if and only if $(\lambda^*_1,\lambda_3)\in\Lambda^*_{(1)}\times \Lambda^*_{(3)}$.
\end{proof}

\noindent{\bf{\Cref{prop:smi-show-p2}.}}
{\it
  Variational \acrshort*{smi} satisfies property (P2). Let
  \[
    \Lambda^*_{(1)}=\bigcup_{(\lambda^*_1,\lambda^*_3)\in \Lambda^*_{(1,3)}} \{\lambda_1^*\}.
  \]
  The set of Bayes and \acrshort*{smi} variational posteriors for $\varphi,\theta$ are the same, that is,
  \[
    \bigcup_{\lambda_1^*\in\Lambda^*_{(1)}}\bigcup_{\lambda^*_2\in \Lambda_{(2)}^*(\lambda^*_1)} \{(\lambda^*_1,\lambda^*_2)\}=B^*,
  \]
  when $\eta=1$.
}
\begin{proof}\label{proof:smi-show-p2}
  When $\eta=1$ the power posterior for $\varphi,\tilde\theta$ is the Bayes posterior,
  \[
    p_{\pow, \eta=1}(\varphi,\tilde\theta \mid Y,Z)=p(\varphi \mid Y,Z)\,p(\tilde\theta \mid Y,\varphi)
  \]
  so \cref{eqn:lambda13-star-defn} is the same as variational Bayes as determined by \cref{eqn:variational_bayes_standard}. Since $q_{\lambda_3}(\tilde\theta \mid \varphi)$ and $q_{\beta_2}(\theta \mid \varphi)$ have the same parameterisation, we have $B^*=\Lambda^*_{(1,3)}$. For any fixed $\lambda^*_1\in \Lambda^*_{(1)}$ let
  \[
    \Lambda^*_{(3)}(\lambda^*_1)=\{\lambda_3\in \Lambda_2: \kl{q_{\lambda^*_1, \lambda_3}(\varphi, \tilde\theta)}{p_{\pow, \eta}(\varphi,\tilde\theta \mid Y,Z)}=d^*_{\smi}\}
  \]
  so that
  \begin{equation}
    \Lambda^*_{(1,3)}=\bigcup_{\lambda_1\in \Lambda^*_{(1)}}\bigcup_{\lambda_3\in \Lambda^*_{(3)}(\lambda_1)} \{(\lambda_1,\lambda_3)\}\label{eqn:proof-p2-lstar13}
  \end{equation}
  Fixing $\lambda^*_1\in \Lambda^*_{(1)}$, and taking $\eta=1$,
  \begin{align*}
    \kl{q_{\lambda^*_1, \lambda_3}(\varphi, \tilde\theta)}{p_{\pow, \eta}(\varphi,\tilde\theta \mid Y,Z)} & =\kl{q_{\lambda^*_1}}{p(\varphi \mid Y,Z)} \\ &\qquad +\quad E_{\varphi\sim q_{\lambda^*_1}}[\kl{q_{\lambda_3}(\tilde\theta\mid \varphi)}{p(\tilde\theta \mid Y,\varphi)}]
  \end{align*}
  and the target level is
  \begin{align*}
    d^*_{\smi} & =\kl{q_{\lambda^*_1}}{p(\varphi \mid Y,Z)}+ \min_{\lambda_3\in \Lambda_2} E_{\varphi\sim q_{\lambda^*_1}}[\kl{q_{\lambda_3}(\tilde\theta\mid \varphi)}{p(\tilde\theta \mid Y,\varphi)}] \\
               & =\kl{q_{\lambda^*_1}}{p(\varphi \mid Y,Z)}+D^*_{\smi}(\lambda^*_1),
  \end{align*}
  so cancelling the common $\kl{q_{\lambda^*_1}}{p(\varphi \mid Y,Z)}$ term,
  \begin{align}
    \Lambda^*_{(3)}(\lambda^*_1) & = \{\lambda_3\in \Lambda_2: E_{\varphi\sim q_{\lambda^*_1}}[\kl{q_{\lambda_3}(\tilde\theta\mid \varphi)}{p(\tilde\theta \mid Y,\varphi)}]=D^*_{\smi}(\lambda^*_1)\}\nonumber \\
                                 & =\Lambda^*_{(2)}(\lambda^*_1)\label{eqn:proof-p2-l2},
  \end{align}
  from \cref{eqn:lambda2-star-defn-equiv}, so at $\eta=1$,
  \[
    \bigcup_{\lambda_1\in \Lambda^*_{(1)}}\bigcup_{\lambda_2\in \Lambda^*_{(2)}(\lambda_1)} \{(\lambda_1,\lambda_2)\}=\bigcup_{\lambda_1\in \Lambda^*_{(1)}}\bigcup_{\lambda_3\in \Lambda^*_{(3)}(\lambda_1)} \{(\lambda_1,\lambda_3)\}
  \]
  by \cref{eqn:proof-p2-l2}, so the LHS is equal $\Lambda^*_{(1,3)}$
  by \cref{eqn:proof-p2-lstar13} and we saw that $\Lambda^*_{(1,3)}=B^*$, giving the set relation claimed in the proposition.
  We conclude from this that at $\eta=1$, $q_{\lambda^*_1,\lambda^*_2}(\varphi,\theta)$ is a marginal variational \acrshort*{smi} posterior if and only if  it is also a variational Bayes posterior.
\end{proof}

% \noindent{\bf{\Cref{prop:smi-show-p3}.}}
% {\it
% Variational \acrshort*{smi} satisfies property (P3). If $p_{\smi, \eta}\in\mathcal{Q}$ then $q_{\lambda^*}=p_{\smi, \eta}$ for $\lambda^*\in \Lambda^*$.
% }
% \begin{proof}\label{proof:smi-show-p3}
%   This usually is immediate for standard variational methods but has to be checked here. If $p_{\smi, \eta}\in\mathcal{Q}$ then there exist $\lambda_1,\lambda_3$ such that  $q_{\lambda_1,\lambda_3}=p_{\pow, \eta}$ and $\lambda_2$ such that $q_{\lambda_2}(\theta\mid \varphi)=p(\theta \mid Y,\varphi)$ and since these choices minimise the \acrshort*{kl}-divergences in \cref{prop:var_smi_distance,prop:var_smi_lambda2} they are the optimal values, so $q_{\lambda^*}=p_{\smi, \eta}$.
% \end{proof}

\noindent{\bf{\Cref{prop:stop-gradient-loss}.}}
{\it
  The set $\Lambda^*$ in \cref{defn:var-smi} is the set of solutions of $\nabla_{\lambda} \mathcal{L}^{( \smi, \eta)} = 0$ corresponding to minima.
}
\begin{proof}\label{proof:stop-gradient-loss}
  Assuming the flow-based construction of the variational family $q_{\lambda}$ defined in \cref{eqn:q-lambda-smi-var}, the (stopped) gradients of $\mathcal{L}^{( \smi, \eta)}$ are
  \begin{align}
    \nabla_{\lambda_1} \mathcal{L}^{( \smi, \eta)} = \E_{\epsilon \sim p(\epsilon)}[ & \nabla_{\varphi} \left\{ \log p(Z \mid \varphi) + \eta \log p(Y \mid \varphi, \tilde\theta) + \log p(\varphi) \right\} \nabla_{\lambda_1} \{ \varphi \} \nonumber
    \\
                                                                                     & + \nabla_{\lambda_1} \log \left\vert J_{T_1}(\epsilon_1) \right\vert  ] \label{eq:grad_smi_loss_l1}
    \\
    \nabla_{\lambda_2} \mathcal{L}^{( \smi, \eta)} = \E_{\epsilon \sim p(\epsilon)}[ & \nabla_{\theta} \left\{ \log p(Y \mid \varphi, \theta) + \log p(\theta \mid \varphi) \right\} \nabla_{\lambda_2} \{ \theta \} \nonumber
    \\
                                                                                     & + \nabla_{\lambda_2} \log \left\vert J_{T_2}(\epsilon_1,\epsilon_2) \right\vert  ] \label{eq:grad_smi_loss_l2}
    \\
    \nabla_{\lambda_3} \mathcal{L}^{( \smi, \eta)} = \E_{\epsilon \sim p(\epsilon)}[ & \nabla_{\tilde\theta} \left\{ \eta \log p(Y \mid \varphi, \tilde\theta) + \log p(\tilde\theta \mid \varphi) \right\} \nabla_{\lambda_3} \{ \tilde\theta \} \nonumber
    \\
                                                                                     & + \nabla_{\lambda_3} \log \left\vert J_{T_2}(\epsilon_1,\epsilon_3) \right\vert  ]. \label{eq:grad_smi_loss_l3}
  \end{align}

  The system of equations determined by $\nabla_{\lambda} \mathcal{L}^{( \smi, \eta)} = 0$ is the same system defined in \cref{remk:var-smi-roots-gradients} for $\Lambda^*$, namely, \cref{eq:vsmi_roots_l1,eq:vsmi_roots_l2,eq:vsmi_roots_l3}.
\end{proof}

\noindent{\bf{\Cref{prop:smi-show-p3-extra}.}}
{\it
  Let $\mathcal{L}^*(v)=\min_{\lambda\in\Lambda}\mathcal{L}^{(v)}(\lambda)$
  and
  \[
    \Lambda^*(v)=\{\lambda\in \Lambda: \mathcal{L}^{(v)}(\lambda)=\mathcal{L}^*(v)\}.
  \]
  Under regularity conditions on $\mathcal{F}_{smi,\eta}$ and $p_{ \smi, \eta}$ given in \cref{prop:use_IFT_show_loss_limit}, for every solution $\lambda^*\in \Lambda^*$ in \cref{defn:var-smi} and all sufficiently small $v\ge 0$ there exists a unique continuous function $\lambda^*(v)$ satisfying $\lambda^*(v)\in \Lambda^*(v)$ and \[\lim_{v\to 0}\lambda^*(v)=\lambda^*.\]
}
\begin{proof}\label{proof:smi-show-p3-extra}
  Take the definition of $\mathcal{L}^{(v)}(\lambda)$ in \cref{eqn:loss_weighted_var_smi}, use \cref{prop:var_smi_distance} to replace $d$ and \cref{eqn:naive_VI_KL_expand} to expand $\kl{q_{\lambda}}{p_{\smi, \eta}}$. This gives
  \begin{align*}
    \mathcal{L}^{(v)}(\lambda)
     & =(1+v)\kl{q_{\lambda_1, \lambda_3}(\varphi, \tilde\theta)}{p_{\pow, \eta}(\varphi,\tilde\theta \mid Y,Z)}
    \\
     & \qquad+\quad v\cdot E_{\varphi\sim q_{\lambda_1}}[\kl{q_{\lambda_2}(\theta\mid \varphi)}{p(\theta \mid Y,\varphi)}]
  \end{align*}
  Minima $\lambda^{(v)}\in\Lambda^*(v)$ are stationary points of $\mathcal{L}^{(v)}$ so they solve $\nabla_{\lambda}\mathcal{L}^{(v)}(\lambda)=0$ with positive curvature. Using the reparameterisation trick these equations are, for $v>0$,
  \begin{align}
    0 = \E_{\epsilon \sim p(\epsilon)}[ & (1+v)\nabla_{\varphi} \left\{ \log p(Z \mid \varphi) + \eta \log p(Y \mid \varphi, \tilde\theta) + \log p(\varphi) \right\} \nabla_{\lambda_1} \{ \varphi \} \nonumber
    \\
                                        & + v\nabla_{\varphi} \left\{ \log p(Y \mid \varphi, \theta) - \log p(Y\mid \varphi) \right\} \nabla_{\lambda_1} \{ \varphi \} \nonumber                                 \\
                                        & + \nabla_{\lambda_1} \log \left\vert J_{T_1}(\epsilon_1) \right\vert  ] \label{eq:root_loss_l1_smi_general}                                                            \\
    0 = \E_{\epsilon \sim p(\epsilon)}[ & \nabla_{\theta} \left\{ \log p(Y \mid \varphi, \theta) + \log p(\theta \mid \varphi) \right\} \nabla_{\lambda_2} \{ \theta \} \nonumber                                \\
                                        & + \nabla_{\lambda_2} \log \left\vert J_{T_2}(\epsilon_1,\epsilon_2) \right\vert  ] \label{eq:root_loss_l2_smi_general}                                                 \\
    0 = \E_{\epsilon \sim p(\epsilon)}[ & \nabla_{\tilde\theta} \left\{ \eta \log p(Y \mid \varphi, \tilde\theta) + \log p(\tilde\theta \mid \varphi) \right\} \nabla_{\lambda_3} \{ \tilde\theta \} \nonumber   \\
                                        & + \nabla_{\lambda_3} \log \left\vert J_{T_2}(\epsilon_1,\epsilon_3) \right\vert  ] \label{eq:root_loss_l3_smi_general}
  \end{align}
  % First of all $(\lambda^*_1,\lambda^*_3)=\lambda^{(0)}$ (as $\mathcal{L}^{(0)}$ equals the right hand side of \cref{eqn:lambda13-star-def}) and $\lambda^{(v)}_1,\lambda^{(v)}_3$ are continuous in $v$ so certainly $(\lambda^*_1,\lambda^*_3)=\lim_{v\to 0}(\lambda^{(0)}_1,\lambda^{(0)}_3)$. However the value of $\lambda^{(v)}_2$ must minimise $\kl{q_{\lambda^{(v)}_1,\lambda_2,\lambda^{(v)}_3}}{p_{\smi, \eta}}$... not finished.
  The system of equations \cref{eq:root_loss_l1_smi_general,eq:root_loss_l2_smi_general,eq:root_loss_l3_smi_general} converges to the system of equations \cref{eqn:lam-star-roots1a,eqn:lam-star-roots1b} as $v\to 0$. Under regularity conditions set out in \cref{prop:use_IFT_show_loss_limit} below, the solutions converge, in the sense that every point in $\Lambda^*$ is the limit as $v\to 0$ of some continuous sequence of solutions to $\nabla_\lambda\mathcal{L}^{(v)}=0$.
\end{proof}

We now state the regularity conditions and show solutions converge. Recall that $\Lambda_1=\Re^{L_1}$ and $\Lambda_2=\Re^{L_2}$ with $L=L_1+2L_2$ so that $\dim(\Lambda)=L$. The system of equations \cref{eq:root_loss_l1_smi_general,eq:root_loss_l2_smi_general,eq:root_loss_l3_smi_general} has the form $F(\lambda,v)=\mathbf{0}_L$ where
\[
  F(\lambda,v)=f(\lambda)+v\cdot g(\lambda)
\]
with $f,g:\Re^L\to \Re^L$ and $F:\Re^{L+1}\to \Re^L$. Equations \cref{eq:vsmi_roots_l1,eq:vsmi_roots_l2,eq:vsmi_roots_l3} are equivalent to $F(\lambda,0)=\mathbf{0}_L$ which is just $f(\lambda)=\mathbf{0}_L$. Now \cref{eq:vsmi_roots_l1,eq:vsmi_roots_l3} are the reparameterisation of \cref{eqn:lam-star-roots1a} and \cref{eq:vsmi_roots_l2} is the reparameterisation of \cref{eqn:lam-star-roots1b} so we identify
$f=(f_1,f_2,f_3)$ with
\begin{align*}
  f_1(\lambda_1,\lambda_3) & =\nabla_{\lambda_1}\kl{q_{\lambda_1,\lambda_3}}{p_{\pow, \eta}}                                                    \\
  f_2(\lambda_1,\lambda_2) & =\nabla_{\lambda_2} E_{\varphi\sim q_{\lambda_1}}\kl{q_{\lambda_2}(\theta\mid \varphi)}{p(\theta \mid Y,\varphi)}, \\
  f_3(\lambda_1,\lambda_3) & =\nabla_{\lambda_3}\kl{q_{\lambda_1,\lambda_3}}{p_{\pow, \eta}}
\end{align*}
where $f_1: \Re^{L_1+L_2}\to \Re^{L_1}$, $f_2: \Re^{L_1+L_2}\to \Re^{L_2}$ and $f_3: \Re^{L_1+L_2}\to \Re^{L_2}$.

\begin{proposition}\label{prop:use_IFT_show_loss_limit}
  Assume $\Lambda^*$ is not empty and let $\lambda^*\in \Lambda^*$ be given. Assume $f$ and $g$ are continuously differentiable in $\lambda$ at $\lambda=\lambda^*$ and the Hessians of $\kl{q_{\lambda_1,\lambda_3}}{p_{\pow, \eta}}$ (in $\lambda_1,\lambda_3$) and $E_{\varphi\sim q_{\lambda_1}}\kl{q_{\lambda_2}(\theta\mid \varphi)}{p(\theta \mid Y,\varphi)}$ (in $\lambda_2$) are invertible at $\lambda=\lambda^*$. For every $\lambda^*\in\Lambda^*$ there is $\delta>0$ and a unique continuous function $\lambda^*(v)$ satisfying $F(\lambda^*(v),v)=0$ for $0\le |v|\le \delta$ and $\lambda^*(0)=\lambda^*$ at $v=0$.
\end{proposition}
\begin{proof}
  If the Jacobian $\partial F/\partial \lambda$ is invertible at $\lambda=\lambda^*$ then by assumptions of the proposition and the Implicit Function Theorem, there exists $\delta>0$ and a unique continuous function $\lambda^*: \Re\to\Re^L$ satisfying $F(\lambda^*(v),v)=0$ for $0\le |v|\le \delta$. It follows that every point in $\Lambda^*$ is the limit as $v\to 0$ of some continuous sequence of solutions to $\nabla_\lambda\mathcal{L}^{(v)}=0$.

  The Jacobian $\partial F/\partial \lambda=\partial f/\partial \lambda$ at $v=0$ since the additive term in $g$ does not contribute at $v=0$.
  It follows that the Jacobian $\partial F/\partial \lambda$ is invertible at $\lambda=\lambda^*$ if $\left\vert\partial f/\partial \lambda\right\vert_{\lambda=\lambda^*}\ne 0$. The Jacobian matrix $\partial f/\partial \lambda$ has a block structure,
  \begin{align*}
    \frac{\partial f}{\partial \lambda} & =\left[\begin{array}{ccc}
                                                     \partial f_1/\partial\lambda_1 & \mathbf{0}_{L_1\times L_2}     & \partial f_1/\partial\lambda_3 \\
                                                     \partial f_2/\partial\lambda_1 & \partial f_2/\partial\lambda_2 & \mathbf{0}_{L_2\times L_2}     \\
                                                     \partial f_3/\partial\lambda_1 & \mathbf{0}_{L_1\times L_2}     & \partial f_3/\partial\lambda_3 \\
                                                   \end{array}\right] \\[0.1in]
                                        & =\left[\begin{array}{ccc}
                                                     L_1 \times L_1 & L_1\times L_2  & L_1 \times L_2 \\
                                                     L_2 \times L_1 & L_2 \times L_2 & L_2\times L_2  \\
                                                     L_2 \times L_1 & L_2\times L_2  & L_2 \times L_2 \\
                                                   \end{array}\right]
  \end{align*}
  with block dimensions in the second line. It follows that the determinant is
  \[
    \left\vert\frac{\partial f}{\partial \lambda}\right\vert=\left\vert\begin{array}{cc}
      \partial f_1/\partial\lambda_1 & \partial f_1/\partial\lambda_3 \\
      \partial f_3/\partial\lambda_1 & \partial f_3/\partial\lambda_3 \\
    \end{array}\right\vert\ \times\ \left\vert\partial f_2/\partial\lambda_2 \right\vert.\\
  \]
  This is just the product of the determinants of the Hessians of $\kl{q_{\lambda_1,\lambda_3}}{p_{\pow, \eta}}$ (in $\lambda_1,\lambda_3$) and $E_{\varphi\sim q_{\lambda_1}}\kl{q_{\lambda_2}(\theta\mid \varphi)}{p(\theta \mid Y,\varphi)}$ (in $\lambda_2$), so $\partial F/\partial\lambda$ is invertible if these Hessians are invertible at $\lambda=\lambda^*$.
  %We take as regularity conditions the requirement that the roots of $f(\lambda;v)=0$ converge to the roots of $f(\lambda;0)=0$ as $v\to 0$ (``the solutions converge if the equations converge''). Since the systems of equations do converge, these conditions are sufficient for $\Lambda^*=\lim_{v\to 0} \Lambda^*(v)$.
  %the roots $\lambda^{(v)}$ of the system $\nabla_{\lambda}\mathcal{L}^{(v)}(\lambda)=0$ with positive curvature converge as $v\to 0$ to the corresponding roots of the system in \cref{eqn:lam-star-roots1a,eqn:lam-star-roots1b} defining $\lambda^*$.
\end{proof}





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Further details of Experiments.} \label{sec:experiments_extra}
%=============================

\subsection{Epidemiological Data} \label{sec:epidemiology_extra}

\acrshort*{mcmc} samples from the posterior distribution of the epidemiological model in \cref{subsec:exp_epidemiology} are shown in \cref{fig:epidemiology_mcmc}.
Samples from the \acrshort*{mfvi} approximation fitted using variational \acrshort*{smi} separately at each $\eta$ are shown \cref{fig:epidemiology_mfvi}.

\begin{figure}[ht!]
  \centering
  \includegraphics[width=0.48\textwidth]{images/epidemiology/mcmc/eta_1.000/epidemiology_phi_eta_1.000}
  \includegraphics[width=0.37\textwidth]{images/epidemiology/mcmc/eta_1.000/epidemiology_theta_eta_1.000}
  \includegraphics[width=0.48\textwidth]{images/epidemiology/mcmc/eta_0.100/epidemiology_phi_eta_0.100}
  \includegraphics[width=0.37\textwidth]{images/epidemiology/mcmc/eta_0.100/epidemiology_theta_eta_0.100}
  \includegraphics[width=0.48\textwidth]{images/epidemiology/mcmc/eta_0.001/epidemiology_phi_eta_0.001}
  \includegraphics[width=0.37\textwidth]{images/epidemiology/mcmc/eta_0.001/epidemiology_theta_eta_0.001}
  \caption[Epidemiology model MCMC]{
    Samples from the posterior distribution of the epidemiological model, obtained via \acrshort*{mcmc}.
    Rows correspond to three rates of \emph{feedback} from the Poisson module, $\eta=(0.001,0.1,1)$.
    In the left column, we plot the relation between HPV prevalence ($\phi$) and cervical cancer incidence ($\mu$) for the 13 groups in the data.
    In the right column, the joint distribution of slope ($\theta_1$) and intercept ($\theta_2$) of such relation.
  }
  \label{fig:epidemiology_mcmc}
\end{figure}


\begin{figure}[!ht]
  \centering
  \includegraphics[width=0.48\textwidth]{images/epidemiology/mean_field/eta_1.000/epidemiology_phi_eta_1.000.png}
  \includegraphics[width=0.37\textwidth]{images/epidemiology/mean_field/eta_1.000/epidemiology_theta_eta_1.000.png}
  \includegraphics[width=0.48\textwidth]{images/epidemiology/mean_field/eta_0.100/epidemiology_phi_eta_0.100.png}
  \includegraphics[width=0.37\textwidth]{images/epidemiology/mean_field/eta_0.100/epidemiology_theta_eta_0.100.png}
  \includegraphics[width=0.48\textwidth]{images/epidemiology/mean_field/eta_0.001/epidemiology_phi_eta_0.001.png}
  \includegraphics[width=0.37\textwidth]{images/epidemiology/mean_field/eta_0.001/epidemiology_theta_eta_0.001.png}

  \caption[Epidemiology model MFVI]{
    Samples from the variational posterior distribution of the epidemiological model, obtained using a Mean-Field approximation.
    Plots are interpreted as in \cref{fig:epidemiology_mcmc}. We train one variational posterior for each rate $\eta=(0.001,0.1,1)$ separately.
    The approximations clearly underestimate posterior variance.
  }
  \label{fig:epidemiology_mfvi}
\end{figure}

% \begin{figure}[!htb]
%   \centering
%   \includegraphics[width=0.98\textwidth]{images/epidemiology/spline/vmp_mlp/epidemiology_vmp_map}
%   \includegraphics[width=0.98\textwidth]{images/epidemiology/spline/vmp_gp/epidemiology_vmp_map}
%   \includegraphics[width=0.98\textwidth]{images/epidemiology/spline/vmp_spline/epidemiology_vmp_map}
%   \caption[Epidemiology model VMP curves]{
%   \acrshort*{vmp}-map for some of the variational parameters in the Epidemiology model, using \acrshort*{vmp}-\acrshort*{nsf}-{\bf map} and exploring different \acrshort*{vmp}-map parameterisations $f_{\alpha}(\eta)$.
%   Columns correspond to different selected variational parameters in the \acrlong*{nsf}, while rows show the results from using three different \acrshort*{vmp}-map parameterisations: top \acrshort*{mlp}, middle \acrshort*{gp}, bottom Cubic spline.
%   The dependence of the \acrshort*{nf} parameters $\lambda$ on $\eta$ differ down columns - so for the same selected parameter - a great deal from one map to another, suggesting the existence of multiple compact sets of \emph{good} parameters in $\Lambda^*$.
%   }
%   \label{fig:epidemiology_vmp_map}
% \end{figure}

% \begin{figure}[!ht]
%   \centering
%   \includegraphics[width=0.8\textwidth]{images/epidemiology/mean_field/vmp_mlp/epidemiology_vmp_map_theta}
%   \includegraphics[width=0.8\textwidth]{images/epidemiology/mean_field/vmp_gp/epidemiology_vmp_map_theta}
%   \includegraphics[width=0.8\textwidth]{images/epidemiology/mean_field/vmp_spline/epidemiology_vmp_map_theta}
%   \caption[Epidemiology model VMP curves]{
%     \acrshort*{vmp} means for some of the variational parameters in the Epidemiology model, using a Mean-Field variational approximation to the posterior, i.e. independent Gaussians learning location and scale for each parameter.
%     We show the learnt variational location for $(\theta_1, \theta_2)$.
%     Observe how the location of $\theta_1$ shifts towards lower values as $\eta$ goes from zero to one, while the location of $\theta_1$ shifts towards larger values.
%     Each row uses different \acrshort*{vmp}-map, top row is based on a \acrshort*{mlp} (VMP-MF-MLP), middle row on a \acrlongpl*{gp} (VMP-MF-GP), and bottom row uses Cubic Splines (VMP-MF-Spline).
%   }
%   \label{fig:epidemiology_vmp_map_mf}
% \end{figure}

\subsection{Random effects model}\label{subsec:exp_rnd_eff_extra}

Samples from the posterior distribution of the Random Effects model, obtained via \acrshort*{mcmc} are shown in \cref{fig:rnd_eff_mcmc}.

\begin{figure}[!ht]
  \centering
  \includegraphics[width=0.24\textwidth]{images/random_effects/mcmc/full/rnd_eff_beta_sigma_group_1_full}
  \includegraphics[width=0.24\textwidth]{images/random_effects/mcmc/full/rnd_eff_beta_sigma_group_2_full}
  \includegraphics[width=0.24\textwidth]{images/random_effects/mcmc/full/rnd_eff_beta_sigma_group_3_full}
  \includegraphics[width=0.24\textwidth]{images/random_effects/mcmc/full/rnd_eff_sigma_tau_group_1_full}

  \includegraphics[width=0.24\textwidth]{images/random_effects/mcmc/cut1/rnd_eff_beta_sigma_group_1_cut1}
  \includegraphics[width=0.24\textwidth]{images/random_effects/mcmc/cut1/rnd_eff_beta_sigma_group_2_cut1}
  \includegraphics[width=0.24\textwidth]{images/random_effects/mcmc/cut1/rnd_eff_beta_sigma_group_3_cut1}
  \includegraphics[width=0.24\textwidth]{images/random_effects/mcmc/cut1/rnd_eff_sigma_tau_group_1_cut1}

  \includegraphics[width=0.24\textwidth]{images/random_effects/mcmc/cut2/rnd_eff_beta_sigma_group_1_cut2}
  \includegraphics[width=0.24\textwidth]{images/random_effects/mcmc/cut2/rnd_eff_beta_sigma_group_2_cut2}
  \includegraphics[width=0.24\textwidth]{images/random_effects/mcmc/cut2/rnd_eff_beta_sigma_group_3_cut2}
  \includegraphics[width=0.24\textwidth]{images/random_effects/mcmc/cut2/rnd_eff_sigma_tau_group_1_cut2}
  \caption[Epidemiology model MCMC]{
    Samples from the posterior distribution of the Random Effects model, obtained via \acrshort*{mcmc}.
    Each graph shows the joint distribution of a selected pair of parameters. Rows correspond to three modular \emph{feedback} configurations between groups: (Top row) Bayes, $\eta_1=...=\eta_{30}=1$; (Middle) One Cut module, $\eta_1=0$, $\eta_2=...=\eta_{30}=1$; (Bottom) Two Cut Modules, $\eta_1=\eta_2=0$, $\eta_3=...=\eta_{30}=1$.
  }
  \label{fig:rnd_eff_mcmc}
\end{figure}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \section*{Monitoring training}
% %=============================

% How to display KL or another loss that decreases during training?
