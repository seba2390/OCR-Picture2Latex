\documentclass[a4paper,UKenglish,cleveref,autoref,thm-restate]{lipics-v2021}
\usepackage{tikz}
\usetikzlibrary{intersections, calc, arrows, automata, positioning}
\newcommand{\fin}{{\mathit{fi}}}
\newcommand{\inn}{{\mathit{in}}}
%% Tikz
\RequirePackage{tikz}
\usetikzlibrary{arrows,snakes}

%\usepackage{caption}
\usepackage{subcaption}
\usepackage{graphicx}

\usepackage{pgf, verbatim}
\usetikzlibrary{arrows,automata}

\newcommand{\m}{{\sf m}}

\usepackage[linesnumbered,ruled]{algorithm2e}
\usepackage{amsmath}
\allowdisplaybreaks

\usepackage{floatrow}
% Table float box with bottom caption, box width adjusted to content
\newfloatcommand{capbtabbox}{table}[][\FBwidth]
\usepackage{tabularx}


\usepackage{xcolor,colortbl}
\usepackage{longtable}

\usepackage[font=small,labelfont=bf]{caption}


\usepackage{mathtools}
\usepackage{amssymb}
\usepackage{stmaryrd}
%\usepackage{mathabx}


\usepackage{amsthm}

%\theoremstyle{theorem}
%\newtheorem{theorem}{Theorem}[section]

%\theoremstyle{definition}
%\newtheorem{definition}[theorem]{Definition}%[section]

%\theoremstyle{proposition}
%\newtheorem{proposition}[theorem]{Proposition}%[section]

%\theoremstyle{lemma}
%\newtheorem{lemma}[theorem]{Lemma}%[section]

%\theoremstyle{corollary}
%\newtheorem{corollary}[theorem]{Corollary}%[section]

%\theoremstyle{example}
%\newtheorem{example}[theorem]{Example}%[section]

\newcommand{\commenteq}[1]{\hspace{2em} [\mbox{#1}]}

\newcommand{\suchthat}{:}%{\;\ifnum\currentgrouptype=16 \middle\fi|\;}

\usepackage[capitalise]{cleveref}


% change codes of < and >
\mathcode`<="4268
\mathcode`>="5269
\mathchardef\gr="213E
\mathchardef\ls="213C
\newcommand{\epsilonSim}{\stackrel{\epsilon}{\sim}}
\newcommand{\Dist}{\mathrm{Distr}}
\newcommand{\SubDist}{\mathrm{SubDistr}}
\newcommand{\support}{\mathrm{support}}
\newcommand{\lifting}[1]{\mathord{#1\!\!\uparrow}}
\newcommand{\nat}{\mathbb{N}}
\newcommand{\integer}{\mathbb{Z}}
\newcommand{\tv}{\mathit{tv}}

\newcommand{\glob}{\mathit{global}}
%\newcommand{\Pr}{\mathit{Pr}}
\newcommand{\local}{\mathit{local}}

\newcommand{\nxt}{\mathit{next}}
\newcommand{\M}{\mathcal{M}}
\newcommand{\pb}{\mathit{pb}}
\newcommand{\Hyp}{\mathcal{M}'}%\mathcal{M}_{\epsilon} {\widetilde{\mathcal{M}}}%
\newcommand{\errorParam}{\epsilon_2}%{\widetilde{\mathcal{M}}}%
\newcommand{\tauHyp}{\tau_{\epsilon}}
\newcommand{\R}{\mathcal{R}}
\newcommand{\Q}{\mathcal{Q}}

\newcommand{\adjust}{\mathit{adjust}}

\definecolor{OliveGreen}{rgb}{0,0.6,0}
\newcommand{\QT}[1]{{\color{OliveGreen}[#1]}}
 \newcommand{\cancel}[1]{{\color{gray}#1}}
 \newcommand{\modify}[1]{{\color{black}#1}}

 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 \bibliographystyle{plainurl}% the mandatory bibstyle

 \title{Approximate Bisimulation Minimisation} %TODO Please add

 %\titlerunning{} %TODO optional, please use if title is longer than one line

\author{Stefan Kiefer}{Department of Computer Science, University of Oxford, UK}{stekie@cs.ox.ac.uk}{https://orcid.org/0000-0003-4173-6877}{Supported by a Royal Society University Fellowship.}%TODO mandatory, please use full name; only 1 author per \author macro; first two parameters are mandatory, other parameters can be empty. Please provide at least the name of the affiliation and the country. The full address is optional

\author{Qiyi Tang}{Department of Computer Science, University of Oxford, UK}{qiyi.tang@cs.ox.ac.uk}{https://orcid.org/0000-0002-9265-3011}{}

\authorrunning{S.\ Kiefer and Q.\ Tang}%TODO mandatory. First: Use abbreviated first/middle names. Second (only in severe cases): Use first author plus 'et al.'

\Copyright{Stefan Kiefer and Qiyi Tang} %TODO mandatory, please use full first names. LIPIcs license is "CC-BY";  http://creativecommons.org/licenses/by/3.0/

 \ccsdesc{Theory of computation~Program verification}
 \ccsdesc{Theory of computation~Models of computation}
 \ccsdesc{Mathematics of computing~Probability and statistics} %TODO mandatory: Please choose ACM 2012 classifications from https://dl.acm.org/ccs/ccs_flat.cfm

 \keywords{Markov chains, Behavioural metrics, Bisimulation} %TODO mandatory; please add comma-separated list of keywords

 \category{} %optional, e.g. invited paper

\relatedversion{The paper is accepted to FSTTCS 2021.}
%\relatedversion{} %optional, e.g. full version hosted on arXiv, HAL, or other respository/website
 %\relatedversiondetails[linktext={opt. text shown instead of the URL}, cite=DBLP:books/mk/GrayR93]{Classification (e.g. Full Version, Extended Version, Previous Version}{URL to related version} %linktext and cite are optional

 %\supplement{}%optional, e.g. related research data, source code, ... hosted on a repository like zenodo, figshare, GitHub, ...
 %\supplementdetails[linktext={opt. text shown instead of the URL}, cite=DBLP:books/mk/GrayR93, subcategory={Description, Subcategory}, swhid={Software Heritage Identifier}]{General Classification (e.g. Software, Dataset, Model, ...)}{URL to related version} %linktext, cite, and subcategory are optional

 %\funding{(Optional) general funding statement \dots}%optional, to capture a funding statement, which applies to all authors. Please enter author specific funding statements as fifth argument of the \author macro.

\acknowledgements{We thank the anonymous reviewers of this paper for their constructive feedback.}%optional
%\acknowledgements{I want to thank \dots}%optional

\nolinenumbers %uncomment to disable line numbering

 %\hideLIPIcs  %uncomment to remove references to LIPIcs series (logo, DOI, ...), e.g. when preparing a pre-final version to be uploaded to arXiv or another public repository

 %Editor-only macros:: begin (do not touch as author)%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\EventEditors{Miko{\l}aj Boja\'{n}czyk and Chandra Chekuri}
\EventNoEds{2}
\EventLongTitle{41st IARCS Annual Conference on Foundations of Software Technology and Theoretical Computer Science (FSTTCS 2021)}
\EventShortTitle{FSTTCS 2021}
\EventAcronym{FSTTCS}
\EventYear{2021}
\EventDate{December 15--17, 2021}
\EventLocation{Virtual Conference}
\EventLogo{}
\SeriesVolume{213}
\ArticleNo{28}
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}\sloppy
	
\maketitle
	
%TODO mandatory: add short abstract of the document
\begin{abstract}
We propose polynomial-time algorithms to minimise labelled Markov chains whose transition probabilities are not known exactly, have been perturbed, or can only be obtained by sampling.
Our algorithms are based on a new notion of an approximate bisimulation quotient, obtained by lumping together states that are exactly bisimilar in a slightly perturbed system.
We present experiments that show that our algorithms are able to recover the structure of the bisimulation quotient of the unperturbed system.
\end{abstract}
	
\input{intro}
\section{Preliminaries}
\label{section:preliminaries}
%quotient

We write $\nat$ for the set of nonnegative integers and $\integer^{+}$ for the set of positive integers. We write $\mathbb{R}$ for the set of real numbers. Let $S$ be a finite set. We denote by $\Dist(S)$ the set of probability distributions on~$S$. By default we view vectors, i.e., elements of $\mathbb{R}^{S}$, as row vectors. For a vector $\mu \in\mathbb{R}^{S}$ we write \modify{$\|\mu\|_1 := \sum_{s \in S} |\mu(s)|$ for its $L_1$-norm. A vector $\mu \in [0, 1]^{S}$ is a distribution (resp. subdistribution) over $S$ if $\|\mu\|_1 = 1$ (resp.  $0 \ls \|\mu\|_1 \le 1$).} For a (sub)distribution $\mu$ we write $\support(\mu) = \{s \in S \suchthat \mu(s) \gr 0 \}$ for its support. %For a relation $R \subseteq S \times S$, we write $R^{-1}$ as the converse relation of $R$. %We denote column vectors by boldface letters; in particular, $\one \in \{1\}^{S}$ and  $\zero \in \{0\}^{S}$ are column vectors all whose entries are $1$ and $0$, respectively. For $s \in S$ we write $\delta_s$ for the (Dirac) distribution over $S$ with $\delta_{s}(s) = 1$ and $\delta_{s}(r) = 0$ for $r \in S \setminus \{s\}$.

 %total variation distance
%Given two distributions $\mu \in \Dist(S)$ and $\nu \in \Dist(S)$, the \emph{total variation distance} between $\mu$ and $\nu$ is defined as follows: $d_{\tv}(\mu, \nu) = \sup_{E \subseteq S } |\mu(E) - \nu(E)| = \frac{1}{2} |\mu - \nu|$.

A partition of the states $S$ is a set $X$ consisting of pairwise disjoint subsets $E$ of $S$ with $\bigcup_{E \in X} = S$. For an equivalence relation $\R \subseteq S \times S$, $S /_{\R}$ denotes its quotient set and $[s]_\R$ denotes the $\R$-equivalence class of $s \in S$.

%LMC definition
A \emph{labelled Markov chain} (LMC) is a quadruple $<S, L, \tau, \ell>$ consisting of a nonempty finite set $S$ of states, a nonempty finite set $L$ of labels, a transition function $\tau : S \to \Dist(S)$, and a labelling function $\ell: S \to L$.

We denote by $\tau(s)(t)$ the transition probability from $s$ to $t$. Similarly, we denote by $\tau(s)(E) = \sum_{t \in E} \tau(s)(t)$ the transition probability from $s$ to $E \subseteq S$.
For the remainder of the paper,  we fix an LMC $\M = <S,  L, \tau, \ell>$. Let $|\M|$ denote the number of states, $|S|$.

% direct sum of two LMCs
The direct sum $\M_1 \oplus \M_2$ of two LMCs $\M_1 = <S_1, L_1, \tau_1, \ell_1>$ and $\M_2 = <S_2, L_2, \tau_2, \ell_2>$ is the LMC formed by combining the state spaces of $\M_1$ and $\M_2$.

% probabilistic bisimulation and probabilistic bisimilarity distances
An equivalence relation $\R \subseteq S \times S$ is a \emph{probabilistic bisimulation} if for all $(s, t) \in \R$, $\ell(s) = \ell(t)$ and
$\tau(s)(E) = \tau(t)(E)$ for each $\R$-equivalence class $E$. \emph{Probabilistic bisimilarity}, denoted by $\mathord{\sim_{\M}}$ (or $\mathord{\sim}$ when $\M$ is clear), is the largest probabilistic bisimulation.

Any probabilistic bisimulation $\R$ on $\M$ induces a quotient LMC denoted by $\M/_{\R} = <S/_{\R}, L, \tau/_{\R}, \ell/_{\R}>$ where the transition function $\tau/_{\R}([s]_{\R})([t]_{\R}) = \tau(s)([t]_{\R})$ and the labelling function $\ell/_{\R}([s]_\R) = \ell(s)$.

We define the notion of an \emph{approximate quotient}.  Let $\epsilon \ge 0$. An LMC $\Q$ is an \emph{$\epsilon$-quotient} of $\M$ if and only if there is transition function $\tau': S \to \Dist(S)$ such that for all $s \in S$ we have $\|\tau'(s) - \tau(s)\|_1 \le \epsilon$ and $\Q$ is the (exact) bisimulation quotient of the LMC $\M' = <S, L, \tau',\ell>$, that is, $\Q = \M'/_{\sim_{\M'}} $. Since the choice of $\tau'$ is not unique, there might be multiple $\epsilon$-quotients of $\M$. We are interested in the problem of obtaining an $\epsilon$-quotient of $\M$ with small state space. We retrieve the notion of (exact) quotient when $\epsilon = 0$. For $s$ from $\M$, denote the state in $\Q$ which corresponds to $s$ by $[s]^{\epsilon}_{\Q}$ (or $[s]^{\epsilon}$ when $\Q$ is clear).
%\item
%for any $s' \in S'$ and $s \in f^{-1}(s')$ we have $\ell'(s') = \ell(s)$;
%The pseudometric \emph{probabilistic bisimilarity distance} of Desharnais et al. \cite{DGJP1999} , which we denote by $d_{\pb}$, is a function from $S \times S$ to $[0, 1]$, that is, an element of $[0, 1]^{S \times S}$. It can be defined as the least fixed point of the following function:
%\[
%\Delta(d)(s, t) = \left \{
%\begin{array}{ll}
%1 & \mbox{if $\ell(s) \not= \ell(t)$}\\
%\displaystyle \min_{\omega \in \Omega(\tau(s), \tau(t))} \sum_{u, v \in S} \omega(u, v) \; d(u, v) & \mbox{otherwise}
%\end{array}
%\right .
%\]
%12

The set $\Omega(\mu, \nu)$ of \emph{couplings} of $\mu,\nu \in \Dist(S)$ is defined as $\Omega(\mu, \nu)=\left \{ \, \omega \in \Dist(S \times S) \suchthat \sum_{t \in S} \omega(s, t) = \mu(s) \wedge \sum_{s \in S} \omega(s, t) = \nu(t) \, \right \}$. Note that a coupling $\omega \in \Omega$ is a joint probability distribution with marginals $\mu$ and $\nu$ (see, e.g., \cite[page 260-262]{billingsley1995}).

%For all $s,t \in S$, $s \sim t$ if and only if $d_{\pb}(s,t) = 0$ \cite[Theorem~1]{DGJP1999}.

%quotient LMC  an LMC $\M = <S, L, \tau, \ell>$


%distances
%epsilon-bisimulation
%\begin{definition}
%	\label{definition:epsilon-lifting}
	The \emph{$\epsilon$-lifting} of a relation $\R\subseteq S \times S$ proposed by Tracol et al.~\cite{TracolDesharnaisZhioua2011} is the relation $\lifting{\R}_{\epsilon} \subseteq \Dist(S) \times \Dist(S)$ defined by $(\mu, \nu) \in \lifting{\R}_{\epsilon}$ if there exists $\omega \in \Omega(\mu, \nu)$ such that $\sum_{ (u,v) \in \R} \omega(u, v) \ge 1 - \epsilon$.
%\end{definition}

%(Desharnais et al.)
The \emph{$\epsilon$-bisimulation} ($\sim_{\epsilon}$) by Desharnais et al.~\cite{DesharnaisLavoletteTracol2008} is a relation $\R \subseteq S \times S$ in which for all $(s, t) \in \R$ we have  $\ell(s) = \ell(t)$ and $(\tau(s), \tau(t)) \in \lifting{\R}_{\epsilon}$. The $\epsilon$-bisimulation is reflexive and symmetric, but in general not transitive; hence, it is not an equivalence relation.
%\begin{itemize}
%	\item
%	$\ell(s) = \ell(t)$, and
%	\item  $(\tau(s), \tau(t)) \in \lifting{\mathcal{R}}_{\epsilon}$.
%\end{itemize}

%global epsilon bisimulation
%Given an LMC $\M_1 = <S_1, L, \tau_1, \ell_{1}>$ and $\M_2 = <S_2, L, \tau_2, \ell_2>$.

\section{Properties of Approximate Quotients}
\label{section:approximate-quotient-properties}

\begin{figure}[h]
	\centering
	
	\begin{tikzpicture}[xscale=.6,>=latex',shorten >=1pt,node distance=3cm,on grid,auto]
	
	\node[label] (M) at (0,0) {$\M$};
	\node[label] (MQ) at (8,0) {$\M /_{\sim_{\M}}$};
	\node[label] (Me) at (0,-1) {$\Hyp$};
	\node[label] (MeQ) at (8,-1) {$\Q$};
	
	\path[->] (M) edge node [midway, above] {quotient} (MQ);
	\path[->] (M) edge node [midway, left] {perturbation} (Me);
	\path[->] (Me) edge node [midway, above] {approximate quotient} (MeQ);		
	\path[dashed] (MeQ) edge node [midway, right] {$\Q$ is not much bigger than $\M /_{\sim_{\M}}$} (MQ);		
	%\path[->] (Meq) edge node [midway, below] {minimisation} (MOut);				
	\end{tikzpicture}
	\caption{Problem setup.}\label{fig:problem-setup}
\end{figure}
%NP-hard to obtain minimum approximate quotient

Recall from the introduction that we are given an LMC $\Hyp$, which is a slightly perturbed version of an (unknown) LMC $\M$. By slightly perturbed we mean that for each state the successor distributions in $\Hyp$ and~$\M$ have small (say, less than~$\epsilon$) $L_1$-distance. For example, with sampling we can obtain with high probability a perturbed system that has small distance with $\M$. Assume there are many symmetries, that is, lots of probabilistic bisimilar states in $\M$. The state space of $\M$ can then be compressed a lot by (exact) quotienting. Since the transition probabilities are perturbed in $\Hyp$, the states that are probabilistic bisimilar in $\M$ might become inequivalent in $\Hyp$; as a result, the (exact) bisimulation quotient of $\Hyp$ is much larger than that of $\M$. Given a small compression parameter $\errorParam \gr 0$, we aim to compute an approximate quotient $\Q$, an $\epsilon'$-quotient of $\Hyp$ that satisfies two conditions: \modify{(1) $\epsilon'$ should be small, so that little precision is
sacrificed; and (2) the state space of the quotient should be small, to speed up
verification algorithms. Our contribution consists of approximate quotienting algorithms with (a) theoretical guarantees on goal (1) in \cref{theorem:bounding-global-distance} and
\cref{corollary:bounding-quotient-error}, applying to both algorithms: $\epsilon'$ is bounded (and can be controlled) by a compression parameter $\epsilon_2$ and
the number of iterations $i$; (b) empirical results on goal (2): the experiments show that our algorithms produce small quotients.} %(1) $\epsilon'$ is not much bigger than $\errorParam$; and (2) $\Q$ is not much bigger than $\M/_{\sim_{\M}}$, the quotient of $\M$. \cref{fig:problem-setup} shows the setup of our problem.

We first show that it is hard to find an $\errorParam$-quotient of $\Hyp$ with minimum number of states: $\Q^{*}= \arg\min\{|\Q| : \Q \text{ is an } \errorParam\text{-quotient of }\Hyp\}.$ If there are several $\errorParam$-quotients of $\Hyp$ of minimum size, $\Q^{*}$ can be taken to be any one of them. Unfortunately, this problem is unlikely to have an efficient (polynomial-time) solution, as we will see from the next theorem that the decision version of this problem is $\sf NP$-complete.


\begin{figure}[h]
	\begin{minipage}{0.45\linewidth}
		\centering
		
		\begin{tikzpicture}[xscale=.6,>=latex',shorten >=1pt,node distance=3cm,on grid,auto]
		
		\node[state] (us) at (3,4) {$s$};
		\node[state] (u1) at (0.5,2.5) {$s_{1}$};
		\node[label] at (3,2.5) {$\cdots$};
		\node[state] (un) at (5.5,2.5) {$s_{n}$};
		
		
		\node[state] (qb) at (0.5,1) {$s_a$};
		\node[state, fill=green] (qc) at (5.5,1) {$s_b$};
		
		\path[->] (us) edge node  [midway,above] {$\frac{p_1}{T}$} (u1);
		\path[->] (us) edge node  [midway,above] {$\frac{p_n}{T}$} (un);
		
		
		\path[->] (qc) edge  [loop right] node [right] {$1$} (qc);
		\path[->] (qb) edge [loop left] node [left]{$1$} (qb);
		
		\path[->] (u1) edge node  [midway,left] {$\frac{1}{2}$} (qb);
		\path[->] (u1) edge node  [very near start, below,xshift=0.1cm] {$\frac{1}{2}$} (qc);
		\path[->] (un) edge node [very near start, below,xshift=-0.1cm] {$\frac{1}{2}$} (qb);
		\path[->] (un) edge node [midway,right] {$\frac{1}{2}$} (qc);
		
		\end{tikzpicture}
		
	\end{minipage}
	\hspace{0.5cm}
	\begin{minipage}{0.45\linewidth}
		\centering
		
		\begin{tikzpicture}[xscale=.6,>=latex',shorten >=1pt,node distance=3cm,on grid,auto]
		
		\node[state] (t) at (2,4) {$t$};
		
		\node[state] (t1) at (-0.5,2.5) {$t_1$};
		\node[state] (t2) at (4.5,2.5) {$t_2$};
		
		\node[state] (qb) at (-0.5,1) {$t_a$};
		\node[state, fill=green] (qc) at (4.5,1) {$t_b$};
		
		\path[->] (t) edge node [midway, above] {$\frac{N}{T}$} (t1);
		\path[->] (t) edge node [midway, above, xshift=0.25cm] {$1-\frac{N}{T}$} (t2);
		
		\path[-] (t1) edge node [midway, right] {} (qb);
		\path[->] (t1) edge node [near start, left] {$\frac{1}{2} - \epsilon$} (qb);
		\path[->] (t1) edge node [near start, above,xshift=0.05cm,yshift=0.1cm] {$\frac{1}{2} + \epsilon$} (qc);
		
		\path[-] (t2) edge node [midway, right] {} (qc);		
		\path[->] (t2) edge node [near start, above, yshift=0.1cm] {$\frac{1}{2}+ \epsilon$} (qb);
		\path[->] (t2) edge node [near start, right,xshift=0.15cm] {$\frac{1}{2} - \epsilon$} (qc);
		
		\path[->] (qc) edge [loop right] node [midway, right] {$1$} (qc);
		\path[->] (qb) edge [loop left]   node [midway, left]{$1$} (qb);
		
		\end{tikzpicture}
	\end{minipage}
	\caption{The LMC in the reduction for {\sf NP}-hardness. All states have the same label $a$ except $s_b$ and $t_b$ which have label $b$.
	}\label{fig:reductionfromSubsset}
\end{figure}

Given an LMC $\Hyp$, a compression parameter $\errorParam \gr 0$ and a constant $k \in \integer^{+}$, it is {\sf NP}-complete to decide whether there exists an $\errorParam$-quotient of $\Hyp$ of size $k$. The hardness result is by reduction from the Subset Sum problem. Given a set $P =\{p_1, \ldots, p_n\}$ and $N \in \nat$, Subset Sum asks whether there exists a set $Q \subseteq P$ such that $\sum_{p_i \in Q} p_i = N$. Given an instance of Subset Sum $<P, N>$ where $P =\{p_1, \ldots, p_n\}$ and $N \in \nat$, we construct an LMC; see \cref{fig:reductionfromSubsset}. Let $T = \sum_{p_i \in P} p_i$, $\epsilon = \frac{1}{2T}$ and $k = 5$. \modify{In the LMC, state $s$ transitions to state $s_i$ with probability $p_i / T$ for all $1 \le  i \le n$. Each state $s_i$ transitions to $s_a$ and $s_b$ with equal probabilities. State $t$ transitions to $t_1$ and $t_2$ with probability $N / T$ and $1 - N / T$, respectively. State $t_1$ (resp. $t_2$) transitions to $t_a$ (resp. $t_b$) and $t_b$ (resp. $t_a$) with probability $\frac{1}{2} - \epsilon$ and $\frac{1}{2} + \epsilon$, respectively. All the remaining states transition to the successor state with probability one. States $s_b$ and $t_b$ have label $b$ and all other states have label $a$.} We can show that $<P, N> \in {\mbox{Subset Sum}} \iff$ there exists an $\frac{1}{2T}$-quotient of $\Hyp$ of size $5$.

\begin{restatable}{theorem}{theoremMinimumApproximateQuotientNPComplete}\label{theorem: minimum-approximate-quotient-NP-complete}
	Given an LMC $\Hyp$, $\errorParam \in (0, 1]$ and $k \in \integer^{+}$. The problem whether there exists an $\errorParam$-quotient of $\Hyp$ of size $k$ is $\sf NP$-complete. It is $\sf NP$-hard even for (fixed) $k=5$.
\end{restatable}

Due to the $\sf NP$-hardness result, we hope to develop practical algorithms to compute approximate quotients of $\Hyp$ that are small but not necessarily of minimum size. To do that, an intuitive idea is to merge ``similar'' states. As we have discussed in the introduction, merging states with small probabilistic bisimilarity distances might be insufficient. Consider the LMC shown in \cref{fig:intro3}. Assume $\epsilon \gr 0$. The states $s_1$ and $t_1$ ($s_2$ and $t_2$) have probabilistic bisimilarity distance one. Thus, to merge $s_1$, $t_1$ or $s_2$, $t_2$, one needs to merge states with probabilistic bisimilarity distance one. Alternatively, we explore the relation of approximate quotient and $\epsilon$-bisimulation. It is not hard to prove the following proposition:%that if $\Q$ is an $\errorParam$-quotient of $\Hyp$, in the LMC $\Hyp \oplus \Q$, for all $s$ from $\Hyp$ we have $s \sim_{\frac{\errorParam}{2}} [s]^{\errorParam}$. %We will see later by examples that the idea of merging states with small approximate probabilistic bisimilar distances does not work.


\begin{restatable}{proposition}{propositionApproximateGlobalRelationSubset}\label{proposition:approximate-quotient-implies-approximate-bisimulation}
	Let $\Q$ be an $\errorParam$-quotient of $\Hyp$. Then in the LMC $\Hyp \oplus \Q$, we have $s \sim_{\frac{\errorParam}{2}} [s]^{\errorParam}_{\Q}$ for all $s$ from $\Hyp$.
\end{restatable}

\begin{figure}[t]
	\centering
	\begin{tikzpicture}
	\tikzstyle{BoxStyle} = [draw, circle, fill=black, scale=0.05,minimum width = 0.001pt, minimum height = 0.001pt]
	\node[state] (s) at (3,4) {$s_1$};
	\node[state] (s1) at (2,2.5) {$s_{2}$};
	\node[state, fill=green] (x) at (4,2.5) {$x$};
	\path[->] (s) edge [loop left] node  [midway,left] {$\frac{1}{2}$} (s);
	\path[->] (s) edge node  [midway,left,xshift=-0.1cm,yshift=0.1cm] {$\frac{1}{4}$} (s1);
	\path[->] (s) edge node  [midway,right,xshift=0.1cm,yshift=0.1cm] {$\frac{1}{4}$} (x);
	\path[->] (x) edge [loop right] node [midway, right] {$1$} (x);
	\path[->] (s1) edge node  [midway,below] {$\frac{1}{4}-2\epsilon$} (x);
	\path[->] (s1) edge [loop left] node [midway, left] {$\frac{3}{4} + 2\epsilon$} (s1);
	
	\node[state] (t) at (8,4) {$s_3$};
	\node[state, fill=green] (y) at (8,2.5) {$x$};
	\path[->] (t) edge [loop left] node  [midway,left] {$\frac{3}{4}+\epsilon$} (t);
	\path[->] (t) edge node  [midway,right,xshift=0.1cm] {$\frac{1}{4}-\epsilon$} (y);
	\path[->] (y) edge [loop right] node [midway, right] {$1$} (y);
	
	%	\node[state] (ss1) at (7,4) {$s_1$};
	%	\node[state, fill=green] (z) at (7,2.5) {$x$};
	%	\path[->] (ss1) edge [loop left] node  [midway,left] {$\frac{3}{4}+2\epsilon$} (ss1);
	%	\path[->] (ss1) edge node  [midway,right,xshift=0.1cm] {$\frac{1}{4}-2\epsilon$} (z);
	%	\path[->] (z) edge [loop right] node [midway, right] {$1$} (z);
	\end{tikzpicture}
	\caption{An LMC in which $s_1 \sim_{\epsilon} s_3 \sim_{\epsilon} s_2$.}
	\label{fig:exampleGlobal}
\end{figure}
\begin{figure}[t]
	\centering
	\begin{tikzpicture}
	\tikzstyle{BoxStyle} = [draw, circle, fill=black, scale=0.05,minimum width = 0.001pt, minimum height = 0.001pt]
	\node[state] (s) at (3,4) {$s_{13}$};
	\node[state] (s1) at (2,2.5) {$s_{2}$};
	\node[state, fill=green] (x) at (4,2.5) {$x$};
	\path[->] (s) edge [loop left] node  [midway,left] {$\frac{5}{8}+\frac{\epsilon}{2}$} (s);
	\path[->] (s) edge node  [midway,left,xshift=-0.1cm,yshift=0.1cm] {$\frac{1}{8}$} (s1);
	\path[->] (s) edge node  [midway,right,xshift=0.1cm,yshift=0.1cm] {$\frac{1}{4}-\frac{\epsilon}{2}$} (x);
	\path[->] (x) edge [loop right] node [midway, right] {$1$} (x);
	\path[->] (s1) edge node  [midway,below] {$\frac{1}{4}-2\epsilon$} (x);
	\path[->] (s1) edge [loop left] node [midway, above] {$\frac{3}{4} + 2\epsilon$} (s1);
	\node at (3, 1.5) {(a) $\epsilon'$-quotient};
	
	\node[state] (s) at (8,4) {$s_1$};
	\node[state] (s1) at (7,2.5) {$s_{23}$};
	\node[state, fill=green] (x) at (9,2.5) {$x$};
	\path[->] (s) edge [loop left] node  [midway,left] {$\frac{1}{2}$} (s);
	\path[->] (s) edge node  [midway,left,xshift=-0.1cm,yshift=0.1cm] {$\frac{1}{4}$} (s1);
	\path[->] (s) edge node  [midway,right,xshift=0.1cm,yshift=0.1cm] {$\frac{1}{4}$} (x);
	\path[->] (x) edge [loop right] node [midway, right] {$1$} (x);
	\path[->] (s1) edge node  [midway,below] {$\frac{1}{4}-\frac{3\epsilon}{2}$} (x);
	\path[->] (s1) edge [loop left] node [midway, above,yshift=0.1cm] {$\frac{3}{4} + \frac{3\epsilon}{2}$} (s1);	
	\node at (8, 1.5) {(b) $\epsilon$-quotient};
	
	\node[state] (t) at (12,4) {$s_{123}$};
	\node[state, fill=green] (y) at (12,2.5) {$x$};
	\path[->] (t) edge [loop left] node  [midway,left] {$\frac{3}{4}+\epsilon$} (t);
	\path[->] (t) edge node  [midway,right,xshift=0.1cm] {$\frac{1}{4}-\epsilon$} (y);
	\path[->] (y) edge [loop right] node [midway, right] {$1$} (y);
	\node at (12, 1.5) {(c) $2\epsilon$-quotient};	
	%	\node[state] (ss1) at (7,4) {$s_1$};
	%	\node[state, fill=green] (z) at (7,2.5) {$x$};
	%	\path[->] (ss1) edge [loop left] node  [midway,left] {$\frac{3}{4}+2\epsilon$} (ss1);
	%	\path[->] (ss1) edge node  [midway,right,xshift=0.1cm] {$\frac{1}{4}-2\epsilon$} (z);
	%	\path[->] (z) edge [loop right] node [midway, right] {$1$} (z);
	\end{tikzpicture}
	\caption{(a) An $\epsilon'$-quotient obtained by merging $s_1$ and $s_3$ where $\epsilon'$ is at least $\frac{1}{4}+\epsilon$; (b) An $\epsilon$-quotient obtained by merging $s_2$ and $s_3$; (b) A $2\epsilon$-quotient obtained by merging $s_1$, $s_2$ and $s_3$.}
	\label{fig:exampleMerge}
\end{figure}

\cref{proposition:approximate-quotient-implies-approximate-bisimulation} suggests that $\epsilon_2$-quotients and $\epsilon_2$-bisimulation are related.  The runtime of the algorithm to compute the $\epsilon_2$-bisimulation in \cite{DesharnaisLavoletteTracol2008} is $O(|S|^7)$ which makes it not practical for large systems. Furthermore, the algorithms based on merging states that are $\epsilon_2$-bisimilar may produce an $\epsilon'$-quotient where $\epsilon'$ is large, violating the first condition of a satisfying approximate quotient. Assume the positive number $\epsilon$ is much smaller than $\frac{1}{8}$. Let us choose the compression parameter $\epsilon_2$ to be the same as $\epsilon$. We compute the $\epsilon$-bisimulation of the LMC shown in \cref{fig:exampleGlobal} and get $s_1 \sim_{\epsilon} s_3 \sim_{\epsilon} s_2$.  Since $\epsilon$-bisimulation is not an equivalence relation, $s_1  \sim_{\epsilon} s_2$ does not necessarily follow. Indeed, in this LMC, we have $s_1  \sim_{2\epsilon} s_2$ but not $s_1  \sim_{\epsilon} s_2$. If $s_2$ and $s_3$, related by $\sim_{\epsilon}$, are chosen to be merged, the resulting LMC in \cref{fig:exampleMerge}(b) is an $\epsilon$-quotient. However, if $s_1$ and $s_3$ are (unfortunately) chosen to be merged, the resulting LMC, shown in \cref{fig:exampleMerge}(a), is an $\epsilon'$-quotient where $\epsilon'$ cannot be smaller than $\frac{1}{4}+\epsilon$. This $\epsilon'$, much bigger than $\epsilon$ under the assumption that $\epsilon$ is much smaller than $\frac{1}{8}$, makes the resulting LMC undesirable. This example shows that arbitrarily merging states that are $\epsilon$-bisimilar may not work. The LMC in \cref{fig:exampleMerge}(c) is obtained by merging $s_1$, $s_2$ and $s_3$, the states that are related by the transitive closure of $\sim_{\epsilon}$. We show in the appendix that for any $n \in \integer^{+}$ there exists an LMC $\M(n)$ such that merging all states in $\M(n)$  that are related by the transitive closure of $\sim_{\epsilon}$ results in an $\epsilon'$-quotient where $\epsilon'$ is at least $n\epsilon$.

%Since $s R_{\epsilon'} s_1$ does not hold for $\epsilon' \ls 2\epsilon$, we have $s R_{\epsilon'} t$ does not hold for $\epsilon' \ls 2\epsilon$, that is, $s \mathord{R_{\epsilon}} t$ does not hold. This implies that $\mathord{\sim_{\epsilon}} \not\subseteq \mathord{R_{\epsilon}}$. %Together with the previous proposition, we have $R_{\epsilon} \subset \sim_{\epsilon}$.



\cref{lemma:additivity-property}, the additivity lemma, asserts an additivity property of approximate quotients. In \cref{section:minimisation-algorithms}, this lemma will be applied as the two minimisation algorithms successively compute a sequence of approximate quotients.



%TODO parameter continuous

\begin{restatable}{lemma}{lemmaAdditivityProperty}\label{lemma:additivity-property}
		Consider three LMCs $\M_1$, $\M_2$ and $\M_3$. Let $\epsilon_1 \ge 0$ and $\M_2$ be an $\epsilon_1$-quotient of $\M_1$. Let $\epsilon_2 \ge 0$ and $\M_3$ be an $\epsilon_2$-quotient of $\M_2$. Then $\M_3$ is an $(\epsilon_1+\epsilon_2)$-quotient of $\M_1$. %= <S_i, L, \tau_i, \ell_i>$ where $i \in \{1,2,3\}$ and the $S_i$ are pairwise disjoint sets.
\end{restatable}



\section{Approximate Minimisation Algorithms}\label{section:minimisation-algorithms}

\begin{figure}[h]
	\centering
	
	\begin{tikzpicture}[xscale=.6,>=latex',shorten >=1pt,node distance=3cm,on grid,auto]
	
	\node[label] (Me) at (0,0) {$\Hyp$};
	\node[label] (Q0) at (4,0) {$\Q_{0}$};
	\node[label] (Q1) at (9,0) {$\Q_{1}$};
	\node[label] (Qd) at (14,0) {$\cdots$};
	\node[label] (Qi) at (19,0) {$\Q_{i}$};
	
	\path[->] (Me) edge node [midway, above] {exact} (Q0);
	\path[->] (Me) edge node [midway, below] {quotient} (Q0);	
	\path[->] (Q0) edge node [midway, above] {approximate} (Q1);
	\path[->] (Q0) edge node [midway, below] {quotient} (Q1);	
	\path[->] (Q1) edge node [midway, above] {approximate} (Qd);	
	\path[->] (Q1) edge node [midway, below] {quotient} (Qd);				
	\path[->] (Qd) edge node [midway, above] {approximate} (Qi);		
	\path[->] (Qd) edge node [midway, below] {quotient} (Qi);		
	%\path[->] (Meq) edge node [midway, below] {minimisation} (MOut);				
	\end{tikzpicture}
	\caption{Overview of the minimisation algorithms. \cref{lemma:additivity-property} applies to $\Hyp$, $\Q_{0}$, $\Q_{1}, \cdots, \Q_{i}$.}\label{fig:algorithms}
\end{figure}

In this section, we present two practical minimisation algorithms that compute approximate quotients of $\Hyp$. Given an LMC $\Hyp = <S, L, \tauHyp, \ell>$ with perturbed transition probabilities and a small compression parameter $\epsilon_2$. Both algorithms start by computing $\Q_0$, the exact quotient of $\Hyp$.  They proceed in iterations and compute a sequence of approximate quotients where the approximate quotient ($\Q_{i}$) computed at the end of the $i$th iteration is an $\epsilon_2$-quotient of the quotient ($\Q_{i-1}$) given at the beginning of that iteration. Using the additivity lemma, we can show that the (approximate) quotient $\Q_i$ after the $i$th iteration is an $i\epsilon_2$-quotient of $\Hyp$. See \cref{fig:algorithms} for an overview of this approach. Each iteration computes a partition of the state space, lumps the states that are together in the partition and concludes with taking the exact quotient.

%In this section, we present two practical minimisation algorithms that compute approximate quotients of $\Hyp$. Both algorithms proceed in iterations and compute a sequence of approximate quotients. In each iteration, the algorithms first compute a partition of the state space of an approximate quotient and then lump the states that are together in the partition. The exact quotient of the LMC after lumping is used as the new approximate quotient. The first approximate quotient $\Q_0$ is obtained by (exact) quotienting $\Hyp$ and it is guaranteed that the new approximate quotient obtained after lumping is an $\epsilon_2$-approximate quotient of the LMC before lumping. Using the additivity lemma, we can show that the approximate quotient generated by either algorithm after the $i$'s iteration is an $i\epsilon_2$-quotient of $\Hyp$. See \cref{fig:algorithms} for an overview of the algorithms.

\subsection{Local Bisimilarity Distance}\label{subsection:local-bisimilarity-distances}

We define the notion of \emph{local bisimilarity distance}, denoted by $d_{\local}^{\M}$ (or $d_{\local}$ when $\M$ is clear). Intuitively, two states $s$ and $t$ are at small local bisimilarity distance if they are probabilistic bisimilar in an LMC which is slightly perturbed only at the successor distributions of $s$ and~$t$. We provide a polynomial-time algorithm to compute the local bisimilarity distance. Given an LMC $\Hyp = <S, L, \tauHyp, \ell>$ (with perturbed transition probabilities) and a small compression parameter $\epsilon_2$, we propose an iterative minimisation algorithm to compute approximate quotients of $\Hyp$ by merging state pairs with small local bisimilarity distances. In each iteration of the algorithm, we select the state pair with the same label and the minimum local bisimilarity distance if such distance is at most $\epsilon_2$. We compute a partition in which this state pair are together and lump together the states that are together in the partition. The algorithm terminates when no pairs can be lumped, that is, all state pairs have local bisimilarity distances greater than $\epsilon_2$. %The LMC after lumping a state pair is an $\epsilon_2$-approximate quotient of the LMC before merging. Using the additivity lemma, the approximate quotient obtained by this algorithm after the $i$'s iteration is an $i\epsilon_2$-quotient of $\Hyp$.

% We then merge the states for which this distance is small. For the remainder of the section, we fix an approximation LMC $\Hyp = <S, L, \tauHyp, \ell>$ and a small error parameter $\epsilon_2$. The minimisation algorithm runs in iterations. In each iteration, we compute the local bisimilarity distances for all pairs of states with the same label. We merge the pair of states $s$ and $t$ with the same label such that it has the minimum local bisimilarity distance at most $\epsilon_2$. The algorithm terminates when no pairs can be merged, that is, all pairs of states have local bisimilarity distance greater than $\epsilon_2$.

 \paragraph*{Computing Local Bisimilarity Distances}\label{subsubsection:computeLocalBisimilarityDistances}
Given two different states $s, t \in S$ with the same label. We want to compute a new transition function $\tauHyp'$ by only changing the successor distributions of $s$ and $t$ ($\tauHyp(s)$ and $\tauHyp(t)$, respectively) such that $\{s, t\}$ belongs to an $\R$-induced partition where $\R$ is a probabilistic bisimulation of the LMC $\Hyp' = <S, L, \tauHyp', \ell>$. Let ${\rm T}$ be the set of the all transition functions that satisfy this condition, more precisely, we define ${\rm T} = \{\tauHyp' \suchthat \tauHyp'(x) = \tauHyp(x) \; \forall x \not\in \{s, t\}  \land \{s, t\} \in S /_{\R} \text{ where } \R \text{ is a probabilistic bisimulation of the LMC } \Hyp' = <S, L, \tauHyp', \ell>\}$. The local bisimilarity distance is defined as $d_{\local}^{\Hyp}(s, t) = \textstyle\inf_{\tau' \in {\rm T}} \max\{\|\tau'(s) - \tauHyp(s)\|_1, \|\tau'(t) - \tauHyp(t)\|_1 \}$. It is not immediately clear how to compute it. %Since for all $\tau' \in {\rm T}$, we have $s \sim t$ in the LMC $<S, T, \tau', \ell>$, it is not hard to see that $d^{\Hyp}_{\glob}(s, t) \le d_{\local}^{\Hyp}(s, t)$, that is, $d_{\local}^{\Hyp}(s, t)$ is an upper bound of $d^{\Hyp}_{\glob}(s, t)$.

%[not working] the required partition can be obtained simply by the following three steps: (1) construct a new LMS M’ from M by making both s and t absorbing; (2) compute the equivalence classes Y of M’ according to probabilistic bisimilarity; (3) delete s and t from Y and put back a new equivalence class {s, t}.
By the definition of ${\rm T}$, the probabilistic bisimulation $\R$ is the same for any LMC $<S, L, \tauHyp', \ell>$ with $\tauHyp' \in {\rm T}$. Let us define the partition $X = S /_\R$ where $\R$ is the common probabilistic bisimulation. The local bisimilarity distance can be computed by using $X$:
%Let us fix a transition function $\tauHyp' \in {\rm T}$ and fix an LMC $\Hyp' = <S, L, \tauHyp', \ell>$. To compute the local bisimilarity distance, we first compute the partition $X$ containing $\{s, t\}$ which is induced by the probabilistic bisimulation $\R$ of the LMC $\Hyp'$. It is noted that by the definition of ${\rm T}$, the probabilistic bisimulation $\R$ is the same for any LMC $<S, L, \tau', \ell>$ with $\tau' \in {\rm T}$. We then show that the local bisimilarity distance is half of the $L_1$-distance between $(\tau_{\epsilon}(s)(E))_{E \in X}$ and $(\tau_{\epsilon}(t)(E))_{E \in X}$: %, that is, $d_{\local}^{\Hyp}(s, t) = \frac{1}{2} |(\tau_{\epsilon}(s)(E))_{E \in X} -(\tau_{\epsilon}(s)(E))_{E \in X}|$.

%The partition $X = S /_{\R}$ can simply be computed by the following two steps: (1) construct a new LMC from $\Hyp$ by introducing a new label, labelling both $s$ and $t$ with the new label and making both $s$ and $t$ absorbing; (2) compute the equivalence classes of this new LMC according to probabilistic bisimilarity. The resulting equivalence classes is $X$.  %by \cref{alg:s-t-partition-refinement}. Given a partition $Y$, we say $u \equiv_{Y} v$ in the LMC $<S, L, \tauHyp, \ell>$ if and only if $\ell(u) = \ell(v)$ and $\tauHyp(u)(E) = \tauHyp(v)(E)$ for all $E \in Y$.


%By definition of the set $\rm T$, the probabilistic bisimulation which contains $\{s, t\}$ of an LMC $\Hyp' = <S, L, \tauHyp', \ell>$ is the same for any $\tauHyp' \in {\rm T}$. This common probabilistic bisimulation induces a common partition over $S$, denoted by $X$, which is computed by \cref{alg:s-t-partition-refinement}.



%\begin{algorithm}[h]
%	\DontPrintSemicolon
%	
%	$i = 0; Y_0 := \big\{ \{s, t\}, S \setminus \{s, t\} \big\}$\;
%	\Repeat{$Y_i = Y_{i-1}$}{
%		$i := i+1$\;
%		$Y_{i} :=S/\mathord{\equiv_{Y_{i-1}}}$ \;
%		$Y_{i} := \big( (Y_{i} \setminus \{s\} ) \setminus \{t\} \big)  \cup \{\{s, t\}\}$\;
%	}
%	\caption{Partition refinement with $\{s, t\}$ in the final partition.}
%	\label{alg:s-t-partition-refinement}
%\end{algorithm}


\begin{restatable}{proposition}{propositionAdjustTransitionFunction}\label{proposition:adjust-transition-function-s-t}
%We have $d_{\local}^{\Hyp}(s, t) = \max\{|\tauHyp'(s) - \tauHyp(s)|, |\tauHyp'(t) - \tauHyp(t)| \} $ where $\tauHyp' \in {\rm T}$ is defined as \[
%\tauHyp'(x) = \left \{
%\begin{array}{ll}
%\tauHyp(x)& \mbox{if $x \not\in \{s, t\}$}\\
%\nu_x & \mbox{otherwise}
%\end{array}
%\right .
%\]
%Furthermore,
We have $d_{\local}^{\Hyp}(s, t) =	 \frac{1}{2} \|(\tau_{\epsilon}(s)(E))_{E \in X} -(\tau_{\epsilon}(t)(E))_{E \in X}\|_1$.
%We have $d_{\local}^{\Hyp}(s, t) = \max\{ |\tauHyp'(s) - \tauHyp(s)| , |\tauHyp'(t) - \tauHyp(t)|\}$ where
%$\tauHyp' \in {\rm T}$ is defined as $\tauHyp'(x) = \tauHyp(x)$ for all $x \not\in \{s, t\}$ and $\tauHyp'(x) = \nu_x$ if $x \in \{s, t\}$.
\end{restatable}

It turns out that $X$ can simply be computed by \cref{alg:local-distance-partition}. As this algorithm is basically taking the (exact) quotient of the LMC constructed on line~1, it runs in polynomial time. It follows from \cref{proposition:adjust-transition-function-s-t} that the local bisimilarity distance can be computed in polynomial time.

%The partition $X = S /_{\R}$ can simply be computed by the following two steps: (1) construct a new LMC from $\Hyp$ by introducing a new label, labelling both $s$ and $t$ with the new label and making both $s$ and $t$ absorbing; (2) compute the equivalence classes of this new LMC according to probabilistic bisimilarity. The resulting equivalence classes constitute $X$.  %by \cref{alg:s-t-partition-refinement}. Given a partition $Y$, we say $u \equiv_{Y} v$ in the LMC $<S, L, \tauHyp, \ell>$ if and only if $\ell(u) = \ell(v)$ and $\tauHyp(u)(E) = \tauHyp(v)(E)$ for all $E \in Y$.

\begin{algorithm}[h]
	\DontPrintSemicolon
	\KwIn{An LMC $\Hyp = <S, L, \tauHyp, \ell>$, a state pair $(s, t) \in S \times S$}
	\KwOut{A partition $X$ over $S$ containing $\{s, t\}$}
	Construct a new LMC $\Hyp'$ from $\Hyp$ by introducing a new label, labelling both $s$ and $t$ with the new label and making both $s$ and $t$ absorbing\footnotemark\;
	$X := S /_{\sim_{\Hyp'}}$\;
	\caption{Compute Partition for Local Bisimilarity Distances}
	\label{alg:local-distance-partition}
\end{algorithm}
\footnotetext{\modify{An absorbing state is a state that, once entered, cannot be left; that is, a state with self-loop.}}

\begin{example}\label{example:intro3}
 Assume $\epsilon \ls \frac{1}{2}$. Consider the LMC shown in \cref{fig:intro3}. Let $\tauHyp$ denote its transition function. To compute the local bisimilarity distance of $s_1$ and $t_1$, we first compute the partition containing $\{s_1, t_1\}$: $X = \big\{ \{ s_1, t_1\}, \{ s_2\},\{ t_2\} \big\}$. We have $(\tauHyp(s_1)(E))_{E \in X} = (\frac{1}{2}, \frac{1}{2}, 0)$ and $(\tauHyp(t_1)(E))_{E \in X} = (\frac{1}{2}+\epsilon, 0, \frac{1}{2}-\epsilon)$. By \cref{proposition:adjust-transition-function-s-t}, the local bisimilarity distance is $d_{\local}(s_1, t_1) = \frac{1}{2}\|(\tauHyp(s_1)(E))_{E \in X} -(\tauHyp(t_1)(E))_{E \in X}\|_1 = \frac{1}{2}$. Similarly, we have~$d_{\local}(s_2, t_2) =~\frac{1}{2}$.%~\qed
\end{example}

\begin{algorithm}[h]
	\DontPrintSemicolon
	\KwIn{An LMC $\Hyp = <S, L, \tauHyp, \ell>$, a compression parameter $\epsilon_2$}
	\KwOut{An LMC $\Q_{i}$}
	$i := 0$\\
	%$\M^{i}: = <S, L, \tau', \ell>$ \\	
	%Compute $\sim_{\Hyp}$\\
	$\Q_{i} := \Hyp/_{\sim_{\Hyp}}$ and $\Q_{i} = <S^{\Q_{i}}, L, \tau^{\Q_{i}}, \ell^{\Q_{i}}>$\;
	%$\Q_{i} = <S^{\Q_{i}}, L, \tau^{\Q_{i}}, \ell^{\Q_{i}}> := \M^{i}/_{\sim_{\M^{i}}}$ where $\M^{i} = \Hyp$\\	
	%$\Hyp^{i} = <S^{\Hyp^{i}}, L, \tau^{\Hyp^{i}}, \ell^{\Hyp^{i}}> := \M^{i}/_{\sim_{\M^{i}}}$ where $\M^{i} = \Hyp$\\
	\While{$\exists u, v \in S^{\Q_{i}} \text{ such that }  u \not= v \text{ and }  \ell^{\Q_{i}}(u) = \ell^{\Q_{i}}(v) \text{ and }  d_{\local}^{\Q_{i}} (u, v) \le \epsilon_2$ }{
		$(s, t) = \arg\min \{d_{\local}^{\Q_{i}} (u, v) \suchthat (u ,v) \in S^{\Q_{i}} \times S^{\Q_{i}} \land u \not= v \land   \ell^{\Q_{i}}(u) = \ell^{\Q_{i}}(v)\}$\\
		%Select two states $s, t \in S^{\Hyp^{i}}$ where $\ell^{\Hyp^{i}}(s) = \ell^{\Hyp^{i}}(t)$ and $d_{\local}^{\Hyp^{i}} (s, t) = \min\limits_{u ,v \in S^{\Hyp^{i}} \text{ and } \ell^{\Hyp^{i}}(u) = \ell^{\Hyp^{i}}(v)} d_{\local}^{\Hyp^{i}} (u, v)$\\
		%Compute $\sim_{\Hyp^{i}}$\\
		%		Compute $X$ by Algorithm~\ref{alg:s-t-partition-refinement} with the input LMC $\Hyp^{i}$ and $\{s, t\}$\;
		%		$\gamma := \frac{(\tau^{\Hyp^{i}}(s))_{E \in X} + (\tau^{\Hyp^{i}}(t))_{E \in X}}{2}$ \;
		%		$\nu_s$ (resp. $\nu_t$) is obtained by running \cref{alg:adjust-transition-probability} with $X$, $\tau^{\Hyp^{i}}(s)$ (resp. $\tau^{\Hyp^{i}}(t)$) and $\gamma$\;
		%		$
		%		\tau'(x) := \left \{
		%		\begin{array}{ll}
		%		\tau^{\Hyp^{i}}(x)& \mbox{if $x \not\in \{s, t\}$}\\
		%		\nu_x & \mbox{otherwise}
		%		\end{array}
		%		\right .
		%		$\;
		%		$\M^{i+1} := <S^{\Hyp^{i}}, L, \tau', \ell^{\Hyp^{i}}>$ \;
		%Construct an LMC $\Q_{i}'$ from $\Q_{i}$ by adding a new label, labelling $s$ and $t$ with this new label and making $s$ and $t$ absorbing \; %$\M_{i+1}$
		%Compute $\M_{i+1}: = \Q_{i}'/_{\sim_{\Q_{i}'}}$ with state space $X_i := S^{\Q_{i}'} /_{\sim_{\Q_{i}'}}$\;
		Compute $X_i$ by running \cref{alg:local-distance-partition} with input $Q_i$ and $(s, t)$\;
		Construct an LMC $\M_{i+1}:= <X_i, L, \tau^{\M_{i+1}}, \ell^{\M_{i+1}}>$ from $\Q_{i}$ where
		$
		\tau^{\M_{i+1}}(E) := \left \{
		\begin{array}{l}
		 (\tau^{\Q_{i}}(u)(E'))_{E' \in X_i}  \mbox{ for any $u \in E$ if $E \in X_i$  and $E \not= \{s, t\}$}\\
		\frac{(\tau^{\Q_{i}}(s)(E'))_{E' \in X_i} + (\tau^{\Q_{i}}(t)(E'))_{E' \in X_i}}{2} \;\; \mbox{if $E = \{s, t\}$}\\
		\end{array}
		\right .
		$ and $\ell^{\M_{i+1}} (E) := \ell^{\Q_i}(u)$ for $E \in X_i$ and any $u \in E$\;
		
		%from $\Q_{i}$ by restoring the labels of $\{s, t\}$ and setting the probability distribution from $\{s, t\}$ as $\frac{(\tau^{\Q_{i}}(s))_{E \in X_i} + (\tau^{\Q_{i}}(t))_{E \in X_i}}{2}$\;
		%Obtain the new approximate quotient LMC $\Q_{i+1}$ by quotienting $\M_{i+1}'$, that is,
		$\Q_{i+1} := \M_{i+1} /_{\sim_{\M_{i+1}}}$\;
		$i := i+1$\;
	}
	\caption{LMC Minimisation Using Local Bisimilarity Distances}
	\label{alg:local-distance-merge-algorithm}
\end{algorithm}

\paragraph*{Minimisation Algorithm Using Local Bisimilarity Distances}
\label{subsubsection:minimisation-algorithm-local-bisimilarity-distances}
\cref{alg:local-distance-merge-algorithm} shows the minimisation algorithm using local bisimilarity distances. The input is an LMC $\Hyp$ and a compression parameter $\epsilon_2$. We start by initializing an index $i$ to $0$ and building the quotient LMC $\Q_{0} = \Hyp /_{\sim_{\Hyp}}$. If there are no states in $\Q_{i}$ with local bisimilarity distance less than $\epsilon_2$, the algorithm terminates. Otherwise, it steps into the $i$'th iteration of the loop and computes the local bisimilarity distances for all pairs of states in $\Q_{i}$ with the same label. It selects the state pair $(s, t)$ which has the smallest local bisimilarity distance on line~4. It then computes the new approximate quotient by merging states $s$ and $t$ on line~$5$-$7$. This computation is in three steps where the first step is to compute the partition $X_i$ (line~5) by running \cref{alg:local-distance-partition} with input $\Q_i$ and the state pair $(s, t)$. The second step is to construct a new LMC $\M_{i+1}$ by setting $X_i$ as its state space (line~6). The final step is to compute a new approximate quotient $\Q_{i+1}$ by taking the exact quotient of the LMC $\M_{i+1}$ obtained from the previous step.
%A new transition function $\tau'$ (line~5-8) as described by \cref{proposition:adjust-transition-function-s-t} and a new LMC $\M^{i+1}$ with $\tau'$ as its transition function is constructed on line~9. We then compute its quotient $\Hyp^{i+1} = \M^{i+1} /_{\sim_{\M^{i+1}}}$. The size of the state space of $\Hyp^{i+1}$ will decrease by at least one as $s \sim_{\M^{i+1}} t$.
We increment $i$ at the end of the iteration and continue with another iteration if there are states in $\Q_{i+1}$ with local bisimilarity distance at most $\epsilon_2$. Since there are finitely many states and it is polynomial time to compute the local bisimilarity distances, the algorithm always terminates and runs in polynomial time.
%The first minimisation algorithm is shown in Algorithm~\ref{alg:local-distance-merge-algorithm} . The input is a hypothesis LMC $\Hyp$ and an error parameter $\epsilon_2$. We start by building the quotient LMC $\Hyp^{i} = \Hyp /_{\sim_{\Hyp}}$ with $i$ initialized to $0$. It then steps into a loop with incrementing $i$ at the beginning. In the $i$'th iteration of the loop, it computes the local bisimilarity distances for all pairs of states in $\Hyp^{i-1}$ with the same label and selects the pair $s$ and $t$ which have the least local bisimilarity distance. We obtain a new LMC $\M^{i}$ by adjusting the transition probability distribution at $s$ and $t$. It is done by replacing the transition function $\tau^{\Hyp^{i-1}}$ of $\Hyp^{i-1}$ with $\tau'' = \adjust(\tau^{\Hyp^{i-1}}, s, t, u)$ where $\tau''$ yields the local bisimilarity distance of $s$ and $t$, that is, $d_{\local}^{\Hyp^{i-1}} (s, t) = \max\{|\tau^{\Hyp^{i-1}}(s) - \tau''(s) |, |\tau^{\Hyp^{i-1}}(t) - \tau''(t)|\}$. We then build the new quotient LMC $\Hyp^{i} = \M^{i} /_{\sim_{\M^{i}}}$. If the size of the state space of $\Hyp^{i}$ decreases we continue the iteration. Otherwise, the algorithm terminates since there are no states in $\Hyp^{i}$ with local bisimilarity distance less than $\epsilon_2$. Since there are finitely many states, the algorithm always terminates.

%%$\Pr(d_{\glob}^{\M \oplus \Hyp^{i}} (x, x') \le \epsilon + i \epsilon_2) \ge (1-\delta)^{|S|}$
%\begin{restatable}{theorem}{theoremBoundingGlobalDistance}\label{theorem:bounding-global-distance} Let $d_{\glob}^{\M \oplus \Hyp} (x, x') \le \epsilon$ for all state $x$ in $\M$ and the corresponding state $x'$ in $\Hyp$. For all $i \in \nat$, we have $d_{\glob}^{\M \oplus \Hyp^{i}} (x, x') \le \epsilon + i \epsilon_2$ where $x$ is a state in $\M$ and $x'$ is the corresponding state in $\Hyp^{i}$.
%\end{restatable}
%
%
%\begin{corollary}\label{corollary:bounding-global-distance-local-distance}
%	For all $i \in \nat$, we have $\Pr(d_{\glob}^{\M \oplus \Hyp^{i}} (x, x') \le \epsilon + i \epsilon_2) \ge (1-\delta)^{|S|}$ where $x$ is a state in $\M$ and $x'$ is the corresponding state in $\Hyp^{i}$.
%\end{corollary}
%\begin{proof}
%Let $i \in \nat$. Let $x$ be a state from $\M$ and $x'$ be the corresponding state from $\Hyp^{i}$. Let $x_h$ of $\Hyp$ correspond to $x$ of $\M$. By $\Pr(d_{\glob}^{\M \oplus \Hyp} (x, x_h) \le \epsilon ) \ge (1-\delta)^{|S|}$ and \cref{theorem:bounding-global-distance}, we have $\Pr(d_{\glob}^{\M \oplus \Hyp^{i}} (x, x') \le \epsilon + i \epsilon_2) \ge (1-\delta)^{|S|}$.
%\end{proof}

%Merging by approximate partition refinement
\subsection{Minimisation by Approximate Partition Refinement}\label{subsection:approximate-partition-refinement}
%an example LMC that could not be merged by the previous algorithm
Consider the LMC in \cref{fig:intro3}. Assume $\epsilon \ls \frac{1}{2}$ and $\epsilon_2 \ls \frac{1}{2}$. The minimisation algorithm using local bisimilarity distance (Algorithm~\ref{alg:local-distance-merge-algorithm}) cannot merge states $s_1, t_1$ (or $s_2, t_2$) as $d_{\local}(s_1, t_1) = d_{\local}(s_2, t_2)  =\frac{1}{2} \gr \epsilon_2$ as shown by \cref{example:intro3}.

We introduce an approximate partition refinement, a polynomial algorithm similar to the exact partition refinement, which can fix this problem. In the exact partition refinement algorithm, the states will only remain in the same set in an iteration if they have the same label and their probability distributions over the previous partition are the same. Similarly, we design the approximate partition refinement such that states only remain in the same set in an iteration if they have the same label and the $L_1$-distance between the probability distributions over the previous partition is small, say, at most $\epsilon_2$. Given an LMC $\Hyp = <S, L, \tauHyp, \ell>$ with perturbed transition probabilities, the minimisation algorithm using the approximate partition refinement also proceeds in iterations. In each iteration, the approximate partition refinement computes a partition $X$ and then the states which are together in $X$ are  lumped to form a new LMC. To make sure the new LMC is a quotient, we take the (exact) quotient of this LMC as our new approximate quotient. The algorithm continues when there are states that could be lumped, and it terminates when all sets in the partition computed by the approximate partition refinement are singletons, that is, no states can be lumped. %We prove that the LMC after merging is an $\epsilon_2$-approximate quotient of the LMC before merging. Similar to the previous minimisation algorithm, using the additivity lemma, the approximate quotient obtained by this algorithm after the $i$'s iteration is an $i\epsilon_2$-quotient of $\Hyp$.%Using the additivity lemma, we can then bound the error of the final approximate quotient obtained by this algorithm.% guarantees that states in an equivalence class of the final partition have small global bisimilarity distances and thus can be merged.

\modify{
	\begin{example}\label{example:approximate-partition-intro}	
	Consider again the LMC in \cref{fig:intro3}. Assume $\epsilon \ls \frac{1}{2}$ and the compression parameter $\epsilon_2 \ge 2\epsilon$.  We run the above-mentioned minimisation algorithm using the approximate partition refinement. It will only run for one iteration of approximate partition refinement, as we will see in the following. \Cref{fig:example-intro-approximate-partition-refinement}(a) shows the partitions of this iteration. At the beginning of the approximate partition refinement, we have partition $X_0$ as all states are in the same set. The states are then split by the labels and we get partition $X_1$. There is no further split since the $L_1$-distance between the probability distributions over $X_1$ from $s_1$ and $t_1$ (resp. $s_2$ and $t_2$) is $2\epsilon$ which is bounded by the compression parameter $\epsilon_2$, that is, $\|(\tau(s_1)(E))_{E \in X_1}  - (\tau(t_1)(E))_{E \in X_1}\|_1 = \|(\tau(s_2)(E))_{E \in X_1}  - (\tau(t_2)(E))_{E \in X_1}\|_1 = 2\epsilon \le \epsilon_2$. The states together in $X_1$ are then lumped to form the new LMC shown in \cref{fig:example-intro-approximate-partition-refinement}(b). The algorithm terminates as no states in the new LMC can be lumped.
	\end{example}
}

\begin{figure}
\begin{minipage}{0.45\textwidth}
	\centering	\vspace{0.2cm}
	\begin{tabular}{l}
		\\\\\\
		\hline%wd{0.5pt}
		$X_0 = \{S\}$ \\
		$X_1 = \big\{  \{s_1,t_1\} , \{s_2,t_2\} \big\}$\\
		\hline
		\\\\
		(a) The partitions.
	\end{tabular}
	%\captionof{table}{The partitions.} \label{tab:example-intro-approximate-partition-refinement}
\end{minipage}
\hfill
\begin{minipage}{0.45\textwidth}
	\centering
\tikzstyle{BoxStyle} = [draw, circle, fill=black, scale=0.4,minimum width = 1pt, minimum height = 1pt]

\begin{tikzpicture}[scale=.6,>=latex',shorten >=1pt,node distance=3cm,on grid,auto]

%\node[label]  at (2,2.7) {the MC~$\C$};

\node[state] (s) at (-1,0) {$s$};
\node[state, fill=green] (s2) at (4,0) {$t$};


%	\node[state] (u) at (19,0) {$u_1$};
%	\node[state, fill=green] (u2) at (24,0) {$u_2$};

\path[->] (s) edge [out=30,in=150] node [midway, above] {$\frac{1}{2}-\frac{\epsilon}{2}$} (s2);
\path[->] (s2) edge [out=210,in=-30] node [midway, below] {$\frac{1}{2}-\frac{\epsilon}{2}$} (s);

	\path[->] (s) edge [loop above]node [midway, above] {$\frac{1}{2} +\frac{\epsilon}{2}$} (s);
\path[->] (s2) edge [loop above] node [midway, above] {$\frac{1}{2} +\frac{\epsilon}{2} $} (s2);

\node at (1.5,-3.5) {(b) The final LMC.};
\end{tikzpicture}
%	\captionof{figure}{The final LMC.} \label{fig:example-intro-approximate-partition-refinement}
\end{minipage}
\caption{Example of running the minimisation algorithm using approximate partition refinement (Algorithm~\ref{alg:polynomial-optimistic-partition-refinement}) on the LMC in \cref{fig:intro3}.} \label{fig:example-intro-approximate-partition-refinement}
\end{figure}

%the approximate partition refinement
\paragraph*{Approximate Partition Refinement}
\begin{algorithm}[h]
	%		\setstretch{1.12}
	\DontPrintSemicolon
	\KwIn{An LMC $\Hyp = <S, L, \tauHyp, \ell>$, a compression parameter $\errorParam$}
	\KwOut{A partition $X$ over $S$}	
	$i := 0; X_0 := \{S\}$\;
	\Repeat{$X_i = X_{i-1}$}{
		$i := i+1$; $X_i := \emptyset$\;
		\ForEach{$E \in X_{i-1}$}{
			$X_E := \emptyset$\;
			%add $E': = \{s\}$ to $X_E$ where $s \in E$ \;		
			\For{$s \in E$}{
				$ESet := \{E'\in X_{E} \suchthat \text{for all } t\in E' \text{ we have } \ell(s)=\ell(t) \text{ and}\linebreak \|(\tauHyp(s)(E))_{E \in X_{i-1}} - (\tauHyp(t)(E))_{E \in X_{i-1}}\|_1 \le \epsilon_2 \}$\;
				\lIf{$ESet = \emptyset$}{
					$E': = \{s\}$
				}\Else{
					$E' := \arg\min\limits_{E' \in ESet}\big\{ \frac{\sum_{t \in  E'} \|(\tauHyp(s)(E))_{E \in X_{i-1}} - (\tauHyp(t)(E))_{E \in X_{i-1}}\|_1}{|E'|} \big\}$\;
					remove $E'$ from $X_E$; $E' := E' \cup \{ s \}$\;
				}
				add $E'$ to $X_E$ \;
			}
			$X_i := X_i \cup X_E$\;
		}
	}
	\caption{\mbox{Approximate Partition Refinement}}
	\label{alg:polynomial-optimistic-partition-refinement}
\end{algorithm}

Given a compression parameter $\epsilon_{2}$, the approximate partition refinement is shown in Algorithm~\ref{alg:polynomial-optimistic-partition-refinement}. At the beginning, an index $i$ is initialized to zero and we have $X_0 = \{S\}$, that is, all states are in the same set. In a refinement step, we increment $i$ and split each set $E \in X_{i-1}$ into one or more sets. We iterate though all $E \in X_{i-1}$ and for each $E$ we construct a set $X_E$, a partition of $E$. Starting with $X_E = \emptyset$, we iterate over all $s \in E$ (line~6). After each iteration, the current $s \in E$ appears in one set in $X_E$: either as a singleton or as an additional state in an already existing set in $X_E$. We give more details on this loop (lines~6-14) below.
After having partitioned~$E$ into~$X_E$, we add all sets in~$X_E$ to the new partition~$X_i$.
The way we split the sets ensures that for any two states from the same set in~$X_i$ the $L_1$-distance between the successor distributions over $X_{i-1}$ is at most $\epsilon_2$. The algorithm terminates when no splitting can be done.
Let $X$ be the final partition produced by the approximate partition refinement. For any two states $s, t \in E$ where $E \in X$, we have $\ell(s) = \ell(t)$ and $\|(\tauHyp(s)(E'))_{E' \in X} - (\tauHyp(t)(E'))_{E' \in X}\|_1 \le \epsilon_2$.

Let us give more details on the loop (lines~6-14) that partitions an $E \in X_i$.
For a state $s \in E$, a candidate set $\mathit{ESet}$ is computed such that for all $E' \in \mathit{ESet}$ the state $s$ and all $x \in E'$ have the same label and the $L_1$-distance between the successor distributions over $X_{i-1}$ of $s$ and any $x \in E'$ is at most $\epsilon_2$ (line~7). If $\mathit{ESet}$ is empty, we add the singleton $\{s\}$ into $X_E$ (line~8 and~13). If there is only one set $E'$ in $\mathit{ESet}$, we add $s$ to the set $E'$. Otherwise, if there are multiple elements in $\mathit{ESet}$ that satisfy the condition, we select the one as $E'$ such that the average $L_1$-distance between the successor distributions of $s$ and $x \in E'$ is the smallest (line~10). We add $s$ to the selected set $E'$ and include $E'$ in $X_{E}$ (line~10-13).


%which is also a set of $X_E$ by adding $s$

%If such $\mathit{ESet}$ is not empty, we select a set $E'$ from $\mathit{ESet}$ such that the average $L_1$-distance between $s$ and $x \in E'$ is the smallest (line~12). We then update $E'$ which is also a set of $X_E$ by adding $s$ (line~13-16).

\paragraph*{Minimisation Algorithm Using Approximate Partition Refinement}


\begin{algorithm}[h]
	\DontPrintSemicolon
	\KwIn{An LMC $\Hyp = <S, L, \tauHyp, \ell>$, a compression parameter $\epsilon_2$}
	\KwOut{An LMC $\Q_{i}$}
	$i := 0$\\
	%$\M^{i}: = <S, L, \tau', \ell>$ \\	
	%Compute $\sim_{\Hyp}$\\
	$\Q_{i} := \M_{i}/_{\sim_{\M_{i}}}$ where $\M_{i} = \Hyp$ and $\Q_{i} = <S^{\Q_{i}}, L, \tau^{\Q_{i}}, \ell^{\Q_{i}}>$ \;
	%$\omega' = \omega$\\
	%$d_{u, v} = 0 \forall u, v$ \\
	\Repeat{$|S^{\Q_{i}}| = |S^{\Q_{i-1}}|$}{
		Compute $X_i$ by running Algorithm~\ref{alg:polynomial-optimistic-partition-refinement} with $\Q_{i} $ and $\epsilon_2$ as input\;
		Construct an LMC $\M_{i+1} :=  <X_i, L , \tau^{\M_{i+1}}, \ell^{\M_{i+1}} >$ from $\Q_i$ where $\tau^{\M_{i+1}}(E) := \sum_{u \in  E}  \frac{(\tau^{\Q_{i}}(u)(E'))_{E' \in X_i}}{|E|}$ and $\ell^{\M_{i+1}}(E) := \ell^{\Q_{i}}(x)$ for all $E \in X_i$ and any $x \in E$\;
		% such that $\M_{i+1} :=  <X_i, L , \tau^{\M_{i+1}}, \ell^{\M_{i+1}} >$
		%		\ForEach{$x \in S^{\Hyp^{i-1}}$}{
		%			$\gamma_x := \sum_{u \in  E_x} \frac{(\tau^{\Hyp^{i-1}}(u)(E))_{E \in X}}{|E_x|}$ where $x \in E_x$ and $E_x \in X$\;
		%			$\nu_x$ is obtained by running \cref{alg:adjust-transition-probability} with $X$, $\tau^{\Hyp^{i-1}}(x)$ and $\gamma_x$\;
		%			$\tau'(x) = \nu_x$ \;
		%		}
		%		$\M^{i}: = <S^{\Hyp^{i-1}}, L, \tau', \ell^{\Hyp^{i-1}}>$\;
		%		$\Hyp^{i} :=\M^{i} / \sim_{\M^{i}}$ and $\Hyp^{i} = <S^{\Hyp^{i}}, L, \tau^{\Hyp^{i}}, \ell^{\Hyp^{i}}>$ \;
		%Obtain the new approximate quotient $\Q_{i+1}$ by exact quotienting $\M_{i+1}$, that is,
		$\Q_{i+1} := \M_{i+1} /_{\sim_{\M_{i+1}}}$\;
		$i := i+1$ \;
	}
	\caption{LMC Minimisation by Approximate Partition Refinement}
	\label{alg:approximate-partition-refinement-merge-algorithm}
\end{algorithm}

The minimisation algorithm using approximate partition refinement is shown in \cref{alg:approximate-partition-refinement-merge-algorithm}. The input is the same as the first minimisation algorithm: an LMC $\Hyp$ and a compression parameter $\epsilon_2$. An index $i$ is initialised to $0$. Similar to the approximate minimisation algorithm using local bisimilarity distances, we also start by computing the quotient LMC $\Q_{0} = \Hyp/_{\sim_{\Hyp}}$. It then steps into a loop. We compute the approximate partition $X_i$ of $\Q_{i}$ on line~4 and construct a new LMC $\M_{i+1}$ by setting $X_i$ as its state space on line~5. For any state $E \in X_i$, we set the probability distribution as the average probability distribution over $X_i$ from all $u \in E$. The label of any $E \in X$ is set to $\ell^{\Q_{i}}(u)$ where $u$ can be any state from $E$. A new approximate quotient $\Q_{i+1} $ is obtained by taking the exact quotient of $\M_{i+1}$. We increment $i$ at the end of the iteration and continue another iteration if the size of the state space of the new approximate quotient decreases. Otherwise, the algorithm terminates as we have no states to merge. As there are finitely many states, the algorithm always terminates.

%On line~5-10, we construct a new transition function $\tau'$ as described by \cref{proposition:transition-function-small-difference} and obtain a new LMC $\M^{i}$ by using $\tau'$ as its transition function.


Let $i \in \nat$. The following theorem applies to both the LMCs $\Q_{i}$ from \cref{alg:local-distance-merge-algorithm} and those from \cref{alg:approximate-partition-refinement-merge-algorithm}.
\begin{restatable}{theorem}{theoremBoundGlobalDistanceApproximatePartitionRefinement}\label{theorem:bounding-global-distance}
 %Let $d_{\glob}^{\M \oplus \Hyp} (x, x') \le \epsilon$ for all state $x$ in $\M$ and the corresponding state $x'$ in $\Hyp$.
 For all $i \in \nat$, we have that $\Q_{i+1}$ is an $\epsilon_2$-quotient of $\Q_i$. Furthermore, by the additivity lemma, we have that $\Q_i$ is an $i\epsilon_2$-quotient of $\Hyp$. %$d_{\glob}^{\Hyp^{0} \oplus \Hyp^{i}} (x, x_i) \le i \epsilon_2$ where $x$ is a state from $\Hyp^{0}$ and $x_i$ is the corresponding state in $\Hyp^{i}$.
\end{restatable}

In the case that $\Hyp=<S, L, \tauHyp, \ell>$ is a slightly perturbed version of $\M = <S, L,\tau,\ell>$, that is, for all $s \in S$ we have $\|\tau(s) - \tauHyp(s)\|_1 \le \epsilon$, the following corollary holds:
\begin{restatable}{corollary}{corollaryBoundQuotientError}\label{corollary:bounding-quotient-error}
 For all $i \in \nat$, we have that $\Q_i$ is an $(\epsilon + i\epsilon_2)$-quotient of $\M$.
\end{restatable}

 %with possibly imprecise transition probabilities.
%Let $\epsilon \gr 0$ be an error parameter. By slightly perturbed we mean that for each state the successor distributions in $\M_\epsilon$ and~$\M$ have small $L_1$-distance, i.e., at most $\epsilon$.	
%Since we have $R_{\epsilon} \subseteq \mathord{\sim_{\epsilon}}$ by \cref{proposition:approximate-global-relation-subset}, it follows from \cref{theorem:bounding-global-distance} that for all $i \in \nat$, $x \mathord{\sim_{i \epsilon_2}} x_i$ where $x$ is a state from $\Hyp^{0}$ and $x_i$ is the corresponding state in $\Hyp^{i}$.



\section{Active LMC Learning}
\label{section:active-LMC-learning}
%SETUP
%\begin{figure}[h]
%	\centering
%	
%	\begin{tikzpicture}[xscale=.6,>=latex',shorten >=1pt,node distance=3cm,on grid,auto]
%	
%	\node[label] (M) at (0,0) {$\M$};
%	\node[label] (Me) at (6,0) {$\Hyp$};
%	\node[label] (Meq) at (12,0) {$\Q_{0}$};
%	\node[label] (MOut) at (18,0) {$\Q_{i}$};
%	
%	\path[->] (M) edge node [midway, above] {perturbation} (Me);
%	\path[->] (M) edge node [midway, below] {(e.g., sampling)} (Me);
%	\path[->] (Me) edge node [midway, above] {exact quotient} (Meq);		
%	\path[->] (Meq) edge node [midway, above] {approximate} (MOut);		
%	\path[->] (Meq) edge node [midway, below] {minimisation} (MOut);				
%	\end{tikzpicture}
%	\caption{Workflow. \cref{lemma:additivity-property} is applied to $\M$, $\Q_{0}$ and $\Q_{i}$.}\label{fig:workflow}
%\end{figure}

%\subsection{Overall Approach}\label{subsection:learning-overall-approach}
%TODO In the first one (say "4A" for now), we describe our overall approach. I think we should say that we have an imprecise version of a (usually unknown) system. These two systems have small distance. For example, with sampling we can get this distance small with a high probability. (Notice that "high probability" only makes sense in the sampling context, not in general.) We also put the description of sampling in Section 4A.

%Let us assume that we do not have access to the transition probabilities of an LMC~$\M$. Rather, we have a slightly perturbed version $\Hyp=<S, L, \tauHyp, \ell>$ of $\M$. %with possibly imprecise transition probabilities.
%Let $\epsilon \gr 0$ be an error parameter. By slightly perturbed we mean that for each state the successor distributions in $\M_\epsilon$ and~$\M$ have small $L_1$-distance, i.e., at most $\epsilon$. For example, with sampling we can obtain with high probability a perturbed system that has small distance with $\M$.
%
%%TODO Then, still in 4A, we say that in the next section, say "4B" for now, we will present approximate minimisation algorithms. They produce a minimised system (i.e., a 3rd system) which has small distance from the perturbed system. We might give forward references to the theorems in 4B that claim this small distance.
%
%To compute an approximate quotient system of $\Hyp$, our first idea is to compute and merge states with small global bisimilarity distances. As it is $\sf NP$-complete to compute the global bisimilarity distances by \cref{theorem: global-epsilon-bisimulation-NP-complete}, this approach is not straightforward. Instead, in \cref{section:minimisation-algorithms}, we present two polynomial-time approximate minimisation algorithms. These two algorithms start by computing an LMC $\Hyp^{0}$, the exact quotient of $\Hyp$. With a compression parameter $\epsilon_2$ as input, they produce a minimised system $\Hyp^{i}$ where $i$ is the number of loop iterations of the two minimisation algorithms, respectively. From \cref{theorem:bounding-global-distance}, $\Hyp^{i}$ has global bisimilarity distance at most $i\epsilon_2$ from $\Hyp^{0}$, the exact quotient of the perturbed system.
%
%Together with the fact that $\M$ and $\Hyp$ (or its exact quotient $\Hyp^{0}$) have global bisimilarity distance at most $\epsilon$, it follows from \cref{lemma:additivity-property} that the minimised system $\Hyp^{i}$ and $\M$ have small global bisimilarity distance $\epsilon + i\epsilon_2$. \cref{fig:workflow} shows the workflow of our approach.

%TODO Then, still in 4A, we say that putting these two things together, by the triangle inequality (does it hold?), it follows that the unknown system has small distance to the minimised one. This holds with high probability if the imprecise chain has been obtained by sampling, and I think we can state that as a corollary (of the sampling theorem and the forward references).


%Let $\epsilon \gr 0$ be an error parameter and $\delta \gr 0$ be an error bound. Given a labelled Markov chain~$\M$ with possibly imprecise transition probabilities and a slightly perturbed version $\Hyp=<S, L, \tauHyp, \ell>$ of $\M$,  we hope to compute a compressed version of $\M$. By slightly perturbed we mean that for each state the successor distributions in $\Hyp$ and~$\M$ have $L_1$-distance which is at most~$\epsilon$ with probability at least $1 - \delta$. Next, we present how to obtain such approximation LMC by sampling.

%Our first idea is to merge states which have small global bisimilarity distances. However, as we have shown in \cref{theorem: global-epsilon-bisimulation-NP-complete} that it is $\sf NP$-complete to decide the global $\epsilon$-bisimulation, directly computing the global bisimilarity distances is highly inefficient. Alternatively, in \cref{subsection:local-bisimilarity-distances}, we provide a polynomial-time algorithm to compute an upper bound of the global bisimilarity distance. We can then merge the states for which this upper bound is small. In \cref{subsection:approximate-partition-refinement}, we introduce an approximate partition refinement, a polynomial algorithm similar to the exact partition refinement, which guarantees that states in an equivalence class of the final partition have small global bisimilarity distances and thus can be merged.

%\paragraph*{Approximation LMC by Sampling} \label{subsubsection:sampling}
We apply our approximate minimisation algorithms in a setting of active learning. Before that, we first describe how to obtain a perturbed LMC $\Hyp$ by sampling. Assume that we want to learn the transition probabilities of an LMC $\M$, that is, the state space, the labelling and the transitions are known. We also assume the system under learning (SUL) $\M$ could answer the query $\nxt$ which takes a state $s$ as input and returns a successor state of $s$ according to the transition probability distribution $\tau(s)$.

%Sampling
%We can approximate the transition probabilities via sampling.
Given a state $s$ of the LMC. We denote by $x_s$ the number of successor states of $s$ and by~$n_s$ the number of times we query the SUL on $\nxt(s)$. Let $N_{s,t}$ be the frequency counts of the query result $t$, that is, the number of times a successor state $t$ appears as the result returned by the queries. We approximate the transition probability distribution by $\tauHyp(s)$ where $\tauHyp(s)(t) = \frac{N_{s, t}}{n_s}$ for all successor states $t$ of $s$. (Such an estimator is called an empirical estimator in the literature.)

Intuitively, the more queries we ask the SUL, the more accurate the approximate probability distribution $\tauHyp(s)$ would be. In fact, the following theorem holds~\cite[Section~6.4]{BazilleGenestJegourelSun2020},~\cite{Chen2015}.

\begin{theorem}\label{theorem:sampling-size} Let $\epsilon \gr 0$ be an error parameter and $\delta \gr 0$ be an error bound. Let $s \in S$. We have $\Pr(\|\tau(s) - \tauHyp(s)\|_1 \le \epsilon) \ge 1 -\delta$ for $n_s \ge \frac{1}{2 \epsilon^{2}}\ln(\frac{2x_s}{\delta})$.
\end{theorem}

%Equivalently, for any $s \in S$, we have $\Pr(|\tau(s) - \tauHyp(s)| \le \epsilon) \ge (1-\delta)$ for $n_s \ge \frac{1}{2 \epsilon^{2}}\ln(\frac{2x_s}{\delta})$.
For each state $s \in S$, we query the SUL on $\nxt(s)$ for $n_s \ge \frac{1}{2 \epsilon^{2}}\ln(\frac{2x_s}{\delta})$ times. We can make $\delta$ small since it appears in the logarithmic term. We then approximate the transition function by $\tauHyp$ and construct a hypothesis LMC $\Hyp = <S, L, \tauHyp, \ell>$. Since the queries $\nxt(s)$ and $\nxt(t)$ for all $s,t \in S$ and $s \not= t$ are mutually independent, by Theorem~\ref{theorem:sampling-size}, we have that $\Pr(\forall s\in S: \|\tau(s) - \tauHyp(s)\|_1 \le \epsilon) \ge (1-\delta)^{|S|} $.
%%make delta small as it is in log

We then apply the minimisation algorithms with compression parameter $\epsilon_2$ on $\Hyp$ and obtain a minimised system $\Q_{i}$ which is an $i\epsilon_2$-quotient of $\Hyp$, the LMC constructed by sampling. Since with high probability the LMC $\Hyp$ (or its exact quotient $\Q_{0}$) has small distance $\epsilon$ with the SUL $\M$, it follows from \cref{corollary:bounding-quotient-error} that with high probability the minimised system $\Q_{i}$ is an $\epsilon'$-quotient of $\M$ where $\epsilon'$ is small:  for all $i \in \nat$, we have $\Pr(\Q_i \text{ is an }\epsilon'\text{-quotient of } \M \text{ with } \epsilon' \le \epsilon + i \epsilon_2) \ge (1-\delta)^{|S|}$. The probability does not come from our minimisation algorithms and depends solely on the sampling procedure.% and \cref{theorem:bounding-global-distance} holds that $\Q_i$ is an $i \epsilon_2$-quotient of $\Hyp$ for all $i \in \nat$.

%\cancel{
%	\begin{restatable}{corollary}{corollaryBoundGlobalDistance}\label{corollary:bounding-global-distance}
%		For all $i \in \nat$, we have $\Pr(d_{\glob}^{\M \oplus \Hyp^{i}} (x, x_i) \le \epsilon + i \epsilon_2) \ge (1-\delta)^{|S|}$ where $x$ is a state from the SUL $\M$ and $x_i$ is the corresponding state in $\Hyp^{i}$.
%	\end{restatable}
%}
%Assume we have constructed a hypothesis LMC  $\Hyp = <S, L, \tauHyp, \ell>$.

%After a hypothesis LMC  $\Hyp$ has been constructed, suppose we want to minimise $\Hyp$ such that with high probability the difference between the new LMC after minimisation and the SUL is small. We use the global bisimilarity distance to measure the difference between the hypothesis LMC and the SUL. It suffices to compute and merge states with small global bisimilarity distances. As it is $\sf NP$-complete to compute the global bisimilarity distances by \cref{theorem: global-epsilon-bisimulation-NP-complete}, it is not straightforward. Instead, we propose two polynomial-time algorithms to minimise the hypothesis LMC.




\section{Experiments}
\label{section:experiments}
In this section, we evaluate the performance of approximate minimisation algorithms on a number of LMCs. These LMCs model randomised algorithms and probabilistic protocols that are part of the probabilistic model checker PRISM \cite{KNP11}. The LMCs we run experiments on have less than $100,000$ states and model the following protocols or randomised algorithms: Herman's self-stabilisation algorithm \cite{Her90}, the synchronous leader election protocol by Itai and Rodeh \cite{ItaiR90}, the bounded retransmission protocol \cite{DarhenioJJL01}, the Crowds protocol \cite{ReiterR98} and the contract signing protocol by Even, Goldreich and Lempel \cite{EvenGL85}.

We implemented algorithms to obtain the slightly perturbed LMCs $\Hyp$. We call LMCs with fewer than $300$ states small; otherwise we call them large. For small LMCs, we sample the successor distribution for each state and obtain an approximation of it with error parameter $\epsilon$ and error bound $\delta$. For large LMCs, sampling is not practical as the sample size required by \cref{theorem:sampling-size} is very large. For these LMCs, we perturb the successor distribution by adding small noise to the successor transition probabilities so that for each state with at least probability $1-\delta$ the $L_1$-distance of the successor distributions in the perturbed and unperturbed systems is at most $\epsilon$ and otherwise the $L_1$-distance is $2\epsilon$. We vary the error parameter $\epsilon$ in the range of $\{0.00001, 0.0001, 0.001, 0.01\}$ and fix the error bound $\delta = 0.01$. For each unperturbed LMC and a pair of $\epsilon$ and $\delta$, we generate $5$ perturbed LMCs.

We also implemented the two minimisation algorithms in Java: \cref{alg:local-distance-merge-algorithm} and \cref{alg:approximate-partition-refinement-merge-algorithm}. The source code is publicly available\footnote{\url{https://github.com/qiyitang71/approximate-quotienting}}. We show some representative results in \cref{appendix:more-results}. The full experimental results are publicly available\footnote{\url{https://bit.ly/3vcpblY}}.

\begin{table}[t]
\begin{tabularx}{\textwidth}{@{}X@{}X@{}}
	\noindent\begin{tabular}{|c|c|c|c|}
			\hline %\\$\epsilon = 0.0001$
			\multirow{1}{*}{\shortstack[l]{Herman5}}&
			\# states&	\# trans&	\# iter\\
			\hline 						
			$\M$ \& $\Hyp$	&		 		32	  &			244				&\\
			$\M/_{\sim_{\M}}$	&		 		4	  &			11				&\\		
			$\Hyp/_{\sim_{\Hyp}}$      & 		23  &	       167             & \\
			\hline
			\multicolumn{4}{|c|}{Perturbed LMC \#1}\\
			\hline
			\multicolumn{4}{|c|}{$\epsilon_2 = 0.00001$}\\
			\hline
			local \& apr     & 23 & 167 & 0  \\
			%apr		  & 23 & 167 & 0  \\
			\hline
			\multicolumn{4}{|c|}{$\epsilon_2 = 0.0001$}\\
			\hline
			local \& apr     & 22 & 143 & 1  \\
			%apr		  & 22 & 143 & 1  \\
			\hline
			\multicolumn{4}{|c|}{$\epsilon_2 \in \{0.001, 0.01, 0.1\}$}\\
			\hline
			local    & 22 & 143 & 1    \\
			\rowcolor{yellow}
			apr		   & 4 & 11 & 1   \\
			\hline
			\rowcolor{white}
			\multicolumn{4}{c}{}\\			
		\end{tabular}
	~
		\noindent\begin{tabular}{|c|c|c|c|}
		\hline
		\multirow{1}{*}{\shortstack[l]{BRP32-2%\\$\epsilon = 0.0001$
		}} &
		\# states&	\# trans&	\# iter\\
		\hline
		$\M$ \& $\Hyp$ &	1349	 &			1731    & \\
		$\M/_{\sim_{\M}}$	&		 		647	  &			903				&\\								
		$\Hyp/_{\sim_{\Hyp}}$     & 		961	   & 	   	  1343        & \\
		\hline
		\multicolumn{4}{|c|}{Perturbed LMC \#1}\\
		\hline
		\multicolumn{4}{|c|}{$\epsilon_2 = 0.00001$}\\
		\hline
		apr		&   879 & 1230 & 2 \\
		\hline
		\multicolumn{4}{|c|}{$\epsilon_2 =  0.0001$}\\
		\hline
		apr		 &  705 & 986 & 2 \\
		\hline
		\multicolumn{4}{|c|}{$\epsilon_2 \in \{0.001, 0.01\}$}\\
		\hline
		\rowcolor{yellow}
		apr		 &  647 & 903 & 1\\
		\rowcolor{white}
		\hline
		\multicolumn{4}{|c|}{$\epsilon_2 = 0.1$}\\
		\hline
		\rowcolor{red!40}
		apr		 & 196  & 387 & 1 \\
		\hline
%		\rowcolor{white}
%		\multicolumn{4}{c}{}\\
	\end{tabular}	
\end{tabularx}
\caption{In the tables, local and apr stand for the minimisation algorithms using local bisimilarity distance and approximate partition refinement, respectively. The tables show the results for the first perturbed LMC (labeled with \#1) among the five perturbed LMCs generated by sampling or perturbing with $\epsilon =  0.0001$. (Left) Results of running the two minimisation algorithms on the LMC that models Herman's self-stabilisation algorithm with $5$ processes. (Right) Results of running apr on the LMC that models the bounded retransmission protocol with $N =32$ and $\mathit{MAX} = 2$. } \label{table:results}%\belowcaptionskip
\end{table}


%Due to the page limit, we only show partial results.

For the small LMCs, we apply both approximate minimisation algorithms to the perturbed LMCs with $\epsilon_2 \in \{0.00001, 0.0001, 0.001, 0.01, 0.1\}$. The results for a small LMC which models the Herman's self-stabilisation algorithm is shown on the left of \cref{table:results}. For the large LMCs, we only apply the approximate minimisation algorithm using approximate partition refinement to the perturbed LMCs, since the other minimisation algorithm could not finish on the large LMCs with timeout of two hours. The results for a large LMC which models the bounded retransmission protocol is shown on the right of \cref{table:results}.   %The rows are highlighted in red when $\epsilon_2$ is too big, i.e, the minimisation algorithms wrongly merge some states in the quotient of the original model. %when the structure of the quotient of the original model is successfully recovered.

For almost all models, given a perturbed LMC, we are able to recover the structure of the quotient of the unperturbed LMC when $\epsilon_2$ is appropriately chosen, that is, $\epsilon_2$ is no less than $\epsilon$ and is not too big; for example, see \cref{table:results} where the rows are highlighted in yellow. However, when $\epsilon_2$ is too big, the approximate minimisation algorithms may aggressively merge some states in the perturbed LMC and result in a quotient \modify{whose size is even smaller than that of the quotient of the unperturbed LMC}, as highlighted in red in \cref{table:results}. Also, we find that,  as expected, the exact partition refinement in general could not recover the structure of quotient of the original LMCs, except for the LMCs which model the synchronous leader election protocol by Itai and Rodeh. Furthermore, compared to the other approximate minimisation algorithm using the local bisimilarity distance, the one using approximate partition refinement performs much better in terms of running time and the ability to recover the structure of the quotient of the original model.

%The rows are highlighted in yellow when the structure of the quotient of the original model is successfully recovered. The rows are highlighted in red when $\epsilon_2$ is too big, i.e, the minimisation algorithms wrongly merge some states in the quotient of the original model.



One might ask whether the minimisation algorithm using approximate partition refinement always performs better than the one using the local bisimilarity distances. \modify{In general, this is not the case as shown by \cref{example:local-merging-better}. %in the appendix.

\begin{example}\label{example:local-merging-better}	
	Consider the LMC $\M = <S, L, \tau, \ell>$ shown in \cref{fig:example-local-merging-better}. Let $\epsilon_2 = 0.1$. First, we run Algorithm~\ref{alg:local-distance-merge-algorithm}. It proceeds in two iterations. In the first iteration, it computes the local bisimilarity distances for all pairs of states with the same label. We have $d_{\local}(s_1, s_2) = d_{\local}(s_2, s_3)= 0.54$ and $d_{\local}(s_1, s_3) = 0.04$. It then selects the pair $s_1$ and $s_3$ of which the local bisimilarity distance is less than $\epsilon_2$ and is the smallest. These two states are merged into $s_{13}$ in the LMC shown on the left of \cref{fig:example-local-merging-better2}. In the second iteration, the only pair of states with the same label are $s_{13}$ and $s_2$. Since $d_{\local}(s_{13}, s_2) = 0.06 \le \epsilon_2$, they are merged and we arrive at the final LMC shown on the right of \cref{fig:example-local-merging-better2}.
	
	Next, we run \cref{alg:approximate-partition-refinement-merge-algorithm} with the same inputs. In the first iteration, we run approximate partition refinement on line~5 (\cref{alg:polynomial-optimistic-partition-refinement}) and present \cref{tab:example-approximate-partition-refinement} as the possible partitions of the algorithm. At the beginning of the approximate partition refinement, we have partition $X_0$ as all states are in the same set. The states are then split by the labels and we get partition $X_1$. Next, we work on the set $\{s_1, s_2, s_3\}$. Suppose that we see $s_1$ and $s_2$ before $s_3$. We have $s_1$ and $s_2$ remain together as $\|(\tau(s_1)(E))_{E \in X_1}  - (\tau(s_2)(E))_{E \in X_1}\|_1 = 0.08 \le \epsilon_2$. However, since $\|(\tau(s_3)(E))_{E \in X_1}  - (\tau(s_2)(E))_{E \in X_1}\|_1 = 0.16 \gr \epsilon_2$, we have $ESet = \emptyset$ for $s_3$ on line~9 of Algorithm~\ref{alg:polynomial-optimistic-partition-refinement} and it is split out. In the next iteration, since $\|(\tau(s_1)(E))_{E \in X_2}  - (\tau(s_2)(E))_{E \in X_2}\|_1 = 0.54 \gr \epsilon_2$, $\{s_1, s_2\}$ is split into two singleton sets. The final partition $X_3$ in which all sets are singletons suggests no merging can be done and we are left with the original LMC $\M$.
	
	This example also shows that the order of iterating through the states matters for the approximate partition refinement algorithm. Indeed, suppose we iterate though $s_1$ and $s_3$ before $s_2$ after arriving at the partition $X_1$, we will have \cref{tab:example-approximate-partition-refinement2} as the partitions and finally get the  LMC on the right of \cref{fig:example-local-merging-better2} just as the other minimisation algorithm. %. We can then merge $s_1$ and $s_3$ to get the LMC on the left of \cref{fig:example-local-merging-better2}. Moreover, we will proceed into running approximate partition refinement algorithm again and get the final LMC on the right of \cref{fig:example-local-merging-better2} just as the other minimisation algorithm.
	\qed
	
\end{example}
}

\begin{minipage}{0.5\textwidth}
	
	\begin{tikzpicture}[xscale=.6,>=latex',shorten >=1pt,node distance=3cm,on grid,auto]
	
	\node[state] (s) at (-6,0) {$s_1$};
	\node[state] (t) at (-2,0) {$s_2$};
	\node[state] (u) at (2,0) {$s_3$};
	\node[state, fill=green] (v) at (-2,-1.5) {$v$};
	
	\path[->] (s) edge [out=20,in=160] node [midway, above] {$0.5$} (u);
	\path[->] (s) edge node [midway, left, xshift=-0.1cm] {$0.5$} (v);
	
	\path[->] (t) edge node [midway, left] {$0.46$} (v);
	\path[->] (t) edge node [midway, above]  {$0.54$} (s);%[out=160,in=20]
	
	\path[->] (u) edge node [midway, right, xshift=0.1cm] {$0.54$} (v);
	\path[->] (u) edge [loop right] node [midway, right] {$0.46$} (u);
	
	\path[->] (v) edge [out=-70, in=-110, looseness=4] node [near start, right] {$1$} (v);
	\end{tikzpicture}
	\captionof{figure}{The LMC for which Algorithm~\ref{alg:local-distance-merge-algorithm} may perform better than Algorithm~\ref{alg:approximate-partition-refinement-merge-algorithm}.}
	\label{fig:example-local-merging-better}
\end{minipage}
\hfill
\begin{minipage}{0.45\textwidth}
	\centering	\vspace{0.1cm}
	\begin{tabular}{l}
		\\\\\\
		\hline%wd{0.5pt}
		$X_0 = \{S\}$ \\
		$X_1 = \big\{  \{s_1,s_2,s_3\} , \{v\} \big\}$\\
		$X_2 = \big\{  \{s_1, s_2\}, \{s_3\} , \{v\} \big\}$\\
		$X_3 = \big\{  \{s_1\},\{s_2\}, \{s_3\} , \{v\} \big\}$\\
		\hline
		\\\\
	\end{tabular}
	\captionof{table}{Example of running Algorithm~\ref{alg:polynomial-optimistic-partition-refinement} on the LMC in \cref{fig:example-local-merging-better}. (Suppose we iterate through $s_1$ and $s_2$ before $s_3$.)} \label{tab:example-approximate-partition-refinement}
\end{minipage}

\begin{minipage}{0.5\textwidth}
	\begin{tikzpicture}[xscale=.6,>=latex',shorten >=1pt,node distance=3cm,on grid,auto]
	
	\node[state] (s13) at (1.5,0) {$s_{13}$};
	\node[state] (s2) at (6.5,0) {$s_2$};
	\node[state, fill=green] (v1) at (4,-1.5) {$v$};
	
	\path[->] (s13) edge node [midway, left, xshift=-0.1cm] {$0.52$} (v1);
	\path[->] (s13) edge [loop left] node [near end, left] {$0.48$} (s13);
	
	\path[->] (s2) edge node [midway, left] {$0.46$} (v1);
	\path[->] (s2) edge node [midway, above]  {$0.54$} (s13);
	
	\path[->] (v1) edge [out=-70, in=-110, looseness=4] node [near start, right] {$1$} (v1);
	
	\node[state] (ss) at (8.5,0) {$s_{123}$};
	\node[state, fill=green] (vv) at (8.5,-1.5) {$v$};
	
	\path[->] (ss) edge node [midway, right] {$0.49$} (vv);
	\path[->] (ss) edge [loop right] node [midway, right] {$0.51$} (ss);
	\path[->] (vv) edge [out=-70, in=-110, looseness=4] node [near start, right] {$1$} (vv);
	
	\end{tikzpicture}
	\captionof{figure}{Two Steps of Running Algorithm~\ref{alg:local-distance-merge-algorithm}.}\label{fig:example-local-merging-better2}
\end{minipage}
\hfill
\begin{minipage}{0.45\textwidth}
	\centering\vspace{0.2cm}
	\begin{tabular}{l}
		\\\\
		\hline%wd{0.5pt}
		$X_0 = \{S\}$ \\
		$X_1 = \big\{  \{s_1,s_3,s_2\} , \{v\} \big\}$\\
		$X_2 = \big\{  \{s_1, s_3\}, \{s_2\} , \{v\} \big\}$\\
		\hline
		\\
	\end{tabular}
	\captionof{table}{Example of running Algorithm~\ref{alg:polynomial-optimistic-partition-refinement} on the LMC in \cref{fig:example-local-merging-better}. (Suppose we iterate through $s_1$ and $s_3$ before $s_2$.)} \label{tab:example-approximate-partition-refinement2}
\end{minipage}


\section{Conclusion}\label{section:conclusion}

We have developed and analysed algorithms for minimising probabilistic systems via approximate bisimulation.
These algorithms are based on $\epsilon$-quotients, a novel yet natural notion of approximate quotients.
We have obtained theoretical bounds on the discrepancy between the minimised and the non-minimised systems.
In our experiments, approximate partition refinement does well in minimising labelled Markov chains with perturbed  transition probabilities,
suggesting that approximate partition refinement is a practical approach for ``recognising'' and exploiting approximate bisimulation.

Future work might consider the following questions: Does approximate minimisation allow for further forms of active learning? Can our techniques be transferred to Markov decision processes?

\bibliography{paper}

\newpage\appendix\label{section:appendix}
\input{appendix.tex}
\end{document}