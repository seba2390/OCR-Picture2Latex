\section{Introduction}
\label{sec:introduction}
Recommender systems are intelligent E-commerce applications. They assist users in their information-seeking tasks by suggesting items (products, services, or information) that best fit their needs and preferences. Recommender systems have become increasingly popular in recent years, and have been utilized in a variety of domains including movies, music, books, point of interests, and social events\cite{zhao2019model,resnick1997recommender,ricci2011introduction,zhao2016exploring,zhao2018recommendations,zhao2018deep,zhao2018reinforcement,guo2016cosolorec}. Most existing recommender systems consider the recommendation procedure as a static process and make recommendations following a fixed greedy strategy. However, these approaches may fail given the dynamic nature of the users' preferences. Furthermore, the majority of existing recommender systems are designed to maximize the immediate (short-term) reward of recommendations, i.e., to make users order the recommended items, while completely overlooking whether these recommended items will lead to more likely or more profitable (long-term) rewards in the future~\cite{shani2005mdp}. 

In this paper, we consider the recommendation procedure as sequential interactions between users and recommender agent; and leverage Reinforcement Learning (RL) to automatically learn the optimal recommendation strategies. Recommender systems based on reinforcement learning have two advantages. First, they are able to continuously update their strategies during the interactions, until the system converges to the optimal strategy that generates recommendations best fitting users' dynamic preferences. Second, the optimal strategy is made by maximizing the expected long-term cumulative reward from users. Therefore, the system can identify the item with a small immediate reward but making big contribution to the rewards for future recommendations.

\begin{figure*}
	\centering
	\includegraphics[width=166mm]{Fig1_DQN.jpg}
	\caption{DQN architecture selection.}
	\label{fig:selection}
\end{figure*}

Efforts have been made on utilizing reinforcement learning for recommender systems, such as POMDP\cite{shani2005mdp} and Q-learning\cite{taghipour2008hybrid}. However, these methods may become inflexible with the increasing number of items for recommendations. This prevents them to be adopted by practical recommender systems. Thus, we leverage Deep Reinforcement Learning\cite{lillicrap2015continuous} with (adapted) artificial neural networks as the non-linear approximators to estimate the action-value function in RL. This model-free reinforcement learning method does not estimate the transition probability and not store the Q-value table. This makes it flexible to support huge amount of items in recommender systems. 

\subsection{List-wise Recommendations}
Users in practical recommender systems are typically recommended a list of items at one time.  List-wise recommendations are more desired in practice since they allow the systems to provide diverse and complementary options to their users.  For list-wise recommendations, we have a list-wise action space, where each action is a set of multiple interdependent sub-actions (items). Existing reinforcement learning recommender methods also could recommend a list of items. For example, DQN\cite{mnih2013playing} can calculate Q-values of all recalled items separately, and recommend a list of items with highest Q-values. However, these approaches recommend items based on one same state, and ignore relationship among the recommended items. As a consequence, the recommended items are similar. In practice, a bundling with complementary items may receive higher rewards than recommending all similar items. For instance, in real-time news feed recommendations, a user may want to read diverse topics of interest, and an action (i.e. recommendation) from the recommender agent would consist of a set of news articles that are not all similar in topics\cite{yue2011linear}. Therefore, in this paper, we propose a principled approach to capture relationship among recommended items and generate a list of complementary items to enhance the performance. 



\subsection{Architecture Selection}
Generally, there exist two Deep Q-learning architectures, shown in Fig.\ref{fig:selection} (a)(b). Traditional deep Q-learning adopts the first architecture as shown in Fig.\ref{fig:selection}(a), which inputs only the state space and outputs Q-values of all actions. This architecture is suitable for the scenario with high state space and small action space, like playing Atari\cite{mnih2013playing}. However, one drawback is that it cannot handle large and dynamic action space scenario, like recommender systems. The second Q-learning architecture, shown Fig.\ref{fig:selection}(b), treats the state and the action as the input of Neural Networks and outputs the Q-value corresponding to this action. This architecture does not need to store each Q-value in memory and thus can deal with large action space or even continuous action space. A challenging problem of leveraging the second architecture is temporal complexity, i.e., this architecture computes Q-value for all potential actions, separately. To tackle this problem, in this paper, our recommending policy builds upon the Actor-Critic framework\cite{sutton1998reinforcement}, shown in Fig.\ref{fig:selection} (c). The Actor inputs the current state and aims to output the parameters of a state-specific scoring function. Then the RA scores all items and selects an item with the highest score. Next, the Critic uses an approximation architecture to learn a value function (Q-value), which is a judgment of whether the selected action matches the current state. Note that Critic shares the same architecture with the DQN in Fig.\ref{fig:selection}(b).  Finally, according to the judgment from Critic, the Actor updates its' policy parameters in a direction of recommending performance improvement to output properer actions in the following iterations. This architecture is suitable for large action space, while can also reduce redundant computation simultaneously.

\subsection{Online Environment Simulator}
Unlike the Deep Q-learning method applied in playing Online Game like Atari, which can take arbitrary action and obtain timely feedback/reward, the online reward is hard to obtain before the recommender system is applied online. In practice, it is necessary to pre-train parameters offline and evaluate the model before applying it online, thus how to train our framework and evaluate the performance of our framework offline is a challenging task. To tackle this challenge, we propose an online environment simulator, which inputs current state and a selected action and outputs a simulated online reward, which enables the framework to train the parameters offline based on the simulated reward. More specifically, we build the simulator by users' historical records. The intuition is no matter what algorithms a recommender system adopt, given the same state ( or a user's historical records) and the same action (recommending the same items to the user), the user will make the same feedbacks to the items.

To evaluate the performance of a recommender system before applying it online, a practical way is to test it based on users' historical clicking/ordering records. However, we only have the ground truth feedbacks (rewards) of the existing items in the users' historical records, which are sparse compared with the enormous item space of current recommender system. Thus we cannot get the feedbacks (rewards) of items that are not in users' historical records. This may result in inconsistent results between offline and online measurements. Our proposed online environment simulator can also mitigate this challenge by producing simulated online rewards given any state-action pair, so that the recommender system can rate items from the whole item space. Based on offline training and evaluation, the well trained parameters can be utilized as the initial parameters when we launch our framework online, which can be updated and improved via on-policy exploitation and exploration.

\subsection{Our Contributions}
We summarize our major contributions as follows: 
\begin{itemize}[leftmargin=*]
	\item We build an online user-agent interacting environment simulator, which is suitable for offline parameters pre-training and evaluation before applying a recommender system online;
	\item We propose a {\bf LI}st-wise {\bf R}ecommendation framework based on {\bf D}eep reinforcement learning LIRD, which can be applied in scenarios with large and dynamic item space and can reduce redundant computation significantly; and 
	\item We demonstrate the effectiveness of the proposed framework in a real-world e-commerce dataset and validate the importance of list-wise recommendation for accurate recommendations.
\end{itemize}

The rest of this paper is organized as follows. In Section 2, we first formally define the problem of recommender system via reinforcement learning. Then, we provide approaches to model the recommending procedure as a sequential user-agent interactions and introduce details about employing Actor-Critic framework to automatically learn the optimal recommendation strategies via a online simulator. Section 3 carries out experiments based on real-word e-commerce site and presents experimental results.  Section 4 briefly reviews related work. Finally, Section 5 concludes this paper and discusses our future work.
