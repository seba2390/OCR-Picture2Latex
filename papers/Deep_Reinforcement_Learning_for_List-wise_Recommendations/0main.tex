\documentclass[sigconf]{acmart}
\usepackage{booktabs} % For formal tables
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{comment}
\usepackage{footmisc}
\usepackage{mathrsfs}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{multirow}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{multicol}  
\usepackage{tikz}
\usetikzlibrary{bayesnet}
\usepackage{enumitem}
\usepackage{array}
\usepackage{float}
\usepackage{hyperref}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08emT\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\copyrightyear{2019}
\acmYear{2019}
\setcopyright{acmcopyright}
\acmConference[DRL4KDD '19]{The 1st Workshop on Deep Reinforcement Learning for Knowledge Discovery}{August 5, 2019}{Anchorage, AK, USA}

\acmBooktitle{The 1st Workshop on Deep Reinforcement Learning for Knowledge Discovery (DRL4KDD '19), August 5, 2019, Anchorage, AK, USA}
%\fancyhead{}

\begin{document}
\title{Deep Reinforcement Learning for List-wise Recommendations}
\author{Xiangyu Zhao}
\affiliation{
	\institution{Michigan State University}
}
\email{zhaoxi35@msu.edu}

\author{Liang Zhang}
\affiliation{
	\institution{JD.com}
}
\email{zhangliang16@jd.com}

\author{Long Xia}
\affiliation{
	\institution{JD.com}
}
\email{xialong@jd.com}

\author{Zhuoye Ding}
\affiliation{
	\institution{JD.com}
}
\email{dingzhuoye@jd.com}

\author{Dawei Yin}
\affiliation{
	\institution{JD.com}
}
\email{yindawei@acm.org}

\author{Jiliang Tang}
\affiliation{
	\institution{Michigan State University}
}
\email{tangjili@msu.edu}
\renewcommand{\shortauthors}{Xiangyu Zhao et al.}
\begin{abstract}
	Recommender systems play a crucial role in mitigating the problem of information overload by suggesting users' personalized items or services. The vast majority of traditional recommender systems consider the recommendation procedure as a static process and make recommendations following a fixed strategy. In this paper, we propose a novel recommender system with the capability of continuously improving its strategies during the interactions with users. We model the sequential interactions between users and a recommender system as a Markov Decision Process (MDP) and leverage Reinforcement Learning (RL) to automatically learn the optimal strategies via recommending trial-and-error items and receiving reinforcements of these items from users' feedbacks. In particular, we introduce an online user-agent interacting environment simulator, which can pre-train and evaluate model parameters offline before applying the model online. Moreover, we validate the importance of list-wise recommendations during the interactions between users and agent, and develop a novel approach to incorporate them into the proposed framework LIRD for list-wide recommendations. The experimental results based on a real-world e-commerce dataset demonstrate the effectiveness of the proposed framework. 
\end{abstract}

\keywords{List-Wise Recommender System, Deep Reinforcement Learning, Actor-Crtic, Online Environment Simulator.}


\maketitle

\input{1introduction}
\input{2framework}
\input{3experiment}
\input{4relatedwork}
\input{5conclusion}

\bibliographystyle{ACM-Reference-Format}
\bibliography{6rl} 

\end{document}
