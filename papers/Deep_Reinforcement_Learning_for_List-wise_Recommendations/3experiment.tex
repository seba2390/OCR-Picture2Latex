\section{Experiments}
\label{sec:experiments}
In this section, we conduct extensive experiments with a dataset from a real e-commerce site to evaluate the effectiveness of the proposed framework. We mainly focus on two questions: (1) how the proposed framework performs compared to representative baselines; and (2) how the list-wise strategy contributes to the performance. We first introduce experimental settings. Then we seek answers to the above two questions. Finally, we study the impact of important parameters on the performance of the proposed framework. 

\subsection{Experimental Settings}
\label{sec:experimental_settings}

We evaluate our method on a dataset of July, 2017 from a real e-commerce site. We randomly collect 100,000 recommendation sessions (1,156,675 items) in temporal order, and use the first 70\% sessions as the training set and the later 30\% sessions as the testing set. For a given session, the initial state is collected from the previous sessions of the user. In this paper, we leverage $N = 10$ previously clicked/ordered items as the positive state. Each time the RA recommends a list of $K = 4$ items to users. The reward $r$ of skipped/clicked/ordered items are empirically set as 0, 1, and 5, respectively. The dimension of the item embedding is 50, and we set the discounted factor $\gamma = 0.75$. For the parameters of the proposed framework such as $K$ and $\gamma$, we select them via cross-validation. Correspondingly, we also do parameter-tuning for baselines for a fair comparison. We will discuss more details about parameter selection for the proposed framework in the following subsections. 

To evaluate the performance of the proposed framework, we select \textbf{MAP}~\cite{turpin2006user} and \textbf{NDCG}~\cite{jarvelin2002cumulated} as the metrics to measure the performance. The difference of ours from traditional Learn-to-Rank methods is that we rank both clicked and ordered items together, and set them by different rewards, rather than only rank clicked items as that in Learn-to-Rank problems.

\subsection{Performance Comparison for Item Recommendations}
\label{sec:ev_overall}

To answer the the first question, we compare the proposed framework with the following representative baseline methods: 

\begin{itemize}[leftmargin=*]
	\item \textbf{CF}: Collaborative filtering\cite{breese1998empirical} is a method of making automatic predictions about the interests of a user by collecting preference information from many users, which is based on the hypothesis that people often get the best recommendations from someone with similar tastes to themselves. 
	\item \textbf{FM}: Factorization Machines\cite{rendle2010factorization} combine the advantages of support vector machines with factorization models. Compared with matrix factorization, higher order interactions can be modeled using the dimensionality parameter.
	\item \textbf{DNN}: We choose a deep neural network with back propagation technique as a baseline to recommend the items in a given session. The input of DNN is the embeddings of users' historical clicked/ordered items. We train the DNN to output the next recommended item.
	\item \textbf{RNN}: This baseline utilizes the basic RNN to predict what user will buy next based on the clicking/ordering histories. To minimize the computation costs, it only keeps a  finite number of the latest states.
%	\cite{wu2016personal}
    \item \textbf{DQN}: We use a Deep Q-network\cite{mnih2013playing} with embeddings of users' historical clicked/ordered items (state) and a recommended item (action) as input, and train this baseline following Eq. \ref{equ:Q*sa}. Note that the DQN shares the same architecture with the Critic in our framework. 
\end{itemize}

\begin{figure}[t]
	\centering
	\includegraphics[width=81mm]{Fig4_www_all}
	\caption{Overall performance comparison.}
	\label{fig:overall}
	\vspace{-3mm}
\end{figure}

As the testing stage is based on the simulator, we can artificially control the length of recommendation sessions to study the performance in short and long sessions. We define short sessions have less than 50 recommended items, while long sessions have more than 50 recommended items. The results are shown in Figure \ref{fig:overall}. We make following observations:
\begin{itemize}[leftmargin=*]
\item In both short and long sessions, CF, FM and DNN achieve worse performance than RNN, DQN and LIRN, since CF, FM and DNN ignore the temporal sequence of the users' browsing history, while RNN can capture the temporal sequence, DQN and LIRN are able to continuously update their strategies during the interactions.
\item In short recommendation sessions, RNN, DQN and LIRD achieve comparable performance. In other words, RNN models and reinforcement learning models like DQN and LIRD can both recommend proper items matching users' short-term interests.
\item In long recommendation sessions, DQN and LIRD outperforms RNN significantly, because RNN is designed to maximize the immediate reward for recommendations, while reinforcement learning models like DQN and LIRD are designed to achieve the trade-off between short-term and long-term rewards. This result suggests that introducing reinforcement learning can improve the performance of recommendations.
\item LIRD performs similar to DQN, but the training speed of LIRD is much faster than DQN, since DQN computes Q-value for all potential actions, while LIRD can reduce this redundant computation. This result indicates that LIRD is suitable for practical recommender systems with the enormous action space.
\end{itemize}

To sum up, we can draw the answer to the first question -- the proposed framework outperforms most representative baselines in terms of recommendation performance; while LIRD can be efficiently trained compared to DQN. 

\subsection{Performance of List-Wise Recommendations}

To validate the effectiveness of the list-wise recommendation strategy, we investigate how the proposed framework LIRD performs with the changes of the length of the recommendation list, i.e., $K$, in long-term sessions, while fixing other parameters. Note that $K =1$ is the item-wise recommendation.

\begin{figure}[t]
	\centering
	\includegraphics[width=81mm]{Fig5_www_l}
	\caption{Performance with Recommendations Length $K$.}
	\label{fig:architecture}
%	\vspace{-3mm}
\end{figure}

The results are shown in Figure \ref{fig:architecture}. It can be observed: 

\begin{itemize}[leftmargin=*]
\item In general, the recommendation performance first increases and then decreases with the increase of the length of the recommended list. 
\item The proposed framework achieves the best performance when $K = 4$. In other words, LIRD with a smaller $K$ could lose some correlations among the items in the same recommendation list; while the proposed framework with a larger $K$ will introduce noises. 
\end{itemize}

In summary, the list-wise recommendation strategy with appropriately selected $K$ can boost the recommendation performance, which answers the second question. 

\subsection{Performance of Simulator}
\label{sec:parametric}

The online simulator has one key parameter, i.e., $\alpha$, which controls the trade-off between state and action similarity in simulator, see Eq.(\ref{equ:probability1}). To study the impact of this parameter, we investigate how the proposed framework LIRD works with the changes of $\alpha$ in long-term sessions, while fixing other parameters.


The results are shown in Figure \ref{fig:overall}. We note that the proposed framework achieves the best performance when $\alpha = 0.2$. In other words, when we map current state-action pair $p_t (s_t, a_t)$ to a reward (the probability is based on the similarity between $p_t$ and historical historical state-action pair $m_i(s_i, a_i)$ in the memory), the action-similarity makes more contribution, while state-similarity also influences the reward mapping process.

\subsection{Discussion of Positional and Temporal order}
%In this section, we further discuss advantages and limitations of this work. 
%
%\noindent \textbf{Positive Signals v.s. Negative Signals:}
%When designing recommender systems, the positive signals (clicked/ordered feedback) represent the users' preference and thus become the most important information to make recommendations. In reality, users also show various negative signals such as skipping some recommended items during a recommendation procedure. The number of skipped items (or negative signals) is typically far larger than that of positive signals. These skipped items influence user's click/order behaviors \cite{dupret2008user}, which can help us gain better understandings about users' preferences. 
%
%In reality, the items skipped by users may not be caused by users disliking them, but just not preferring as more as the items clicked/ordered or not viewing them in details at all. The week/wrong negative signals may not improve or even reduce the performance when we consider the negative signals. To capture stronger negative signals, more information like dwell time can be recorded in users' behavior log and used in this project. For instance, the more dwell time a user spends on an item but still skips it, the stronger negative signal of this item. 
%

%\noindent \textbf{Positional order v.s. Temporal order:}
%\label{sec:discussion_position}
We build our LIRD framework under an assumption that in a page of recommended items, the user will browse items following positional order, i.e., user will observe the page of items from top to bottom. In this way the previous positional items can influence latter positional items, but not vice versa. However, sometimes users may add items to shopping cart, continue to browse latter positional items, and then make decision whether they order the items in shopping cart. Thus latter positional items could also influence previous items in reverse, i.e., positional order is not strictly equal to temporal order. We will leave it as one future investigation direction. 


\begin{figure}[t]
	\centering
	\includegraphics[width=81mm]{Fig6_www_s}
	\caption{Parameter sensitiveness of $\alpha$}
	\label{fig:parameters}
%	\vspace{-3mm}
\end{figure}