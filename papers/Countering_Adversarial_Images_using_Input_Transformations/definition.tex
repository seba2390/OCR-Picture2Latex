% !TeX root = ../main.tex

We study defenses against non-targeted adversarial examples for image-recognition systems. Let $\mathcal{X} = [0,1]^{H \times W \times C}$ be the image space. Given an image classifier
$h(\cdot)$ and a source image $\bx \in \mathcal{X}$, a \emph{non-targeted\footnote{Given a target class $c$, a \emph{targeted adversarial example} $\bx'$ is an example that satisfies $h(\bx') = c$. We do not consider targeted attacks in this study.} adversarial example} of
$\bx$ is a perturbed image $\bx' \in \mathcal{X}$ such that $h(\bx) \neq h(\bx')$ and
$d(\bx, \bx') \leq \rho$ for some dissimilarity function $d(\cdot, \cdot)$ and $\rho \geq 0$. Ideally, $d(\cdot, \cdot)$ measures the perceptual difference between $\bx$ and $\bx'$ but, in practice, the Euclidean distance $d(\bx, \bx') = \| \bx - \bx' \|_2$ or the Chebyshev distance
$d(\bx, \bx') = \| \bx - \bx' \|_\infty$ is most commonly used.

Given a set of $N$ images $\{\bx_1,\ldots,\bx_N\}$ and a target classifier $h(\cdot)$, an
\emph{adversarial attack} aims to generate $\{\bx_1',\ldots,\bx_N'\}$ such that each $\bx_n'$
is an adversarial example for $\bx_n$. The \emph{success rate} of an attack is measured by the proportion of predictions that was altered by an attack: $\frac{1}{N} \sum_{n=1}^N \mathds{1}[h(\bx_n) \neq h(\bx_n')]$. The success rate is generally measured as a function of the magnitude of the perturbations performed by the attack, using the \emph{normalized $L_2$-dissimilarity}:
\begin{equation}
\label{l2-dissim}
\frac{1}{N} \sum_{n=1}^N \frac{\| \bx_n - \bx_n' \|_2}{\| \bx_n \|_2}.
\end{equation}
A strong adversarial attack has a high success rate whilst its normalized $L_2$-dissimilarity is low.

In most practical settings, an adversary does not have direct access to the model $h(\cdot)$ and has to do a \emph{black-box} attack. However, prior work has shown successful attacks by transferring adversarial examples generated for a separately-trained model to an unknown target model \citep{liu2016delving}. Therefore, we investigate both the black-box and a more difficult \emph{gray-box} attack setting: in our gray-box setting, the adversary has access to the model architecture and the model parameters, but is unaware of the defense strategy that is being used.

A \emph{defense} is an approach that aims make the prediction on an adversarial example $h(\bx')$ equal to the prediction on the corresponding clean example $h(\bx)$. In this study, we focus on \emph{image-transformation defenses} $g(\bx)$ that perform prediction via $h(g(\bx'))$. Ideally, $g(\cdot)$ is a complex, non-differentiable, and potentially stochastic function: this makes it difficult for an adversary to attack the prediction model $h(g(\bx))$ even when the adversary knows both $h(\cdot)$ and $g(\cdot)$.
