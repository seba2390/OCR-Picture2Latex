%Several recent studies have presented approaches that successfully alter the predictions of (deep) machine-learning systems by making small, adversarial perturbations to the inputs into those systems \cite{}. These approaches compute or estimate the gradient of the model's predictive distribution with respect to its input, and (iteratively) use this gradient to alter the input in a way that substantially changes this predictive distribution. The resulting \emph{adversarial examples} pose a threat to real-world applications of machine-learning systems: for instance, they can be used to alter images in (nearly) imperceptible ways in order to bypass spam classifiers \cite{} or to alter the decisions of self-driving cars \cite{}. 
%
%This paper focuses on the development of \emph{defenses} against adversarial examples in the context of image classification. Such defenses are not only of practical importance, but may also help us to better understand the nature of adversarial perturbations. The most common defense methods either train the model using a loss function that is robust to small perturbations of the input \cite{}, or they train a meta-classifier that aims to detect and filter out adversarial examples before processing inputs using the actual machine-learning model. This paper takes a different approach, and studies to what extent \emph{input transformations} can be used to defend against adversarial examples. Specifically, we focus on image transformations that do not have easy-to-compute gradients, which prevents the aforementioned gradient-based attacks. Examples of transformations we found to be effective include random image cropping and rescaling, random pixel dropping, total variance minimization \cite{}, and image quilting \cite{}. While the transformations we study are specific to images, related transformations may also be effective in other domains such as speech recognition \cite{}.
%
%To maximize the effectiveness of defenses based on input transformations, we train an ensemble of models on the various transformed inputs, and average their predictions. For stochastic input transformation such as pixel dropping, we also average predictions over samples from the stochastic process. Combined with model transfer, for instance, from residual networks \cite{} to densely connected networks \cite{}, our approaches are able to successfully defend against over 80\% of the most popular attacks (with small perturbation norms). We conclude that there likely exists a large spectrum of input transformations that can be employed to defend against adversarial examples.

As the use of machine intelligence increases in security-sensitive applications~\citep{bojarski2016end, amodei2015deep}, robustness has become a critical feature to guarantee the reliability of deployed machine-learning systems. Unfortunately, recent research has demonstrated that existing models are not robust to small, adversarially designed perturbations of the input \citep{biggio2013evasion, szegedy2013intriguing, goodfellow2015explaining, kurakin2016adversarial, cisse2017houdini}. Adversarially perturbed examples have been deployed to attack image classification services \citep{liu2016delving}, speech recognition systems \citep{cisse2017houdini}, and robot vision \citep{melis2017robot}. The existence of these \emph{adversarial examples} has motivated proposals for approaches that increase the robustness of learning systems to such examples \citep{papernot2016distillation, kurakin2016adversarial, cisse2017parseval}. 

The robustness of machine learning models to adversarial examples depends both on the properties of the model (\emph{i.e.}, Lipschitzness) and on the nature of the problem considered, \emph{e.g.}, on the input dimensionality and the Bayes error of the problem \citep{fawzi2015analysis, fawzi2016robustness}. Consequently, \emph{defenses} that aim to increase robustness against adversarial examples fall in one of two main categories. The first category comprises \emph{model-specific} strategies that enforce model properties such as invariance and smoothness via the learning algorithm or regularization scheme \citep{shaham2015understanding, kurakin2016adversarial, cisse2017parseval}, potentially exploiting knowledge about the adversary's attack strategy \citep{goodfellow2015explaining}.  %For example, adversarial training~\citep{goodfellow2015explaining} solves a robust optimization problem by continuously generating and training on perturbed inputs. As a result, the model is invariant to small adversarial perturbations in the vicinity of the legitimate examples. 
The second category of defenses are \emph{model-agnostic}: they try to remove adversarial perturbations from the input. For example, in the context of image classification, adversarial perturbations can be partly removed via JPEG compression \citep{dziugaite2016study} or image re-scaling~\citep{lu2017no}. Hitherto, none of these defenses has been shown to be very effective. Specifically, model-agnostic defenses appear too simple to sufficiently remove adversarial perturbations from input images. By contrast, model-specific defenses make strong assumptions about the nature of the adversary (\emph{e.g.}, on the norm that the adversary minimizes or on the number of iterations it uses to generate the perturbation). Consequently, they do not satisfy \citet{kerckhoffs1883} principle: the adversary can alter its attack to circumvent such model-specific defenses. 

In this paper, we focus on increasing the effectiveness of model-agnostic defense strategies by developing approaches that (1) remove the adversarial perturbations from input images, (2) maintain sufficient information in input images to correctly classify them, and (3) are still effective in settings in which the adversary has information on the defense strategy being used. We explore transformations based on image cropping and rescaling, bit-depth reduction, JPEG compression \citep{dziugaite2016study}, total variance minimization \citep{rudin1992tvm}, and image quilting \citep{efros2001quilting}. We show that these defenses can be surprisingly effective against existing attacks, in particular, when the convolutional network is trained on images that are transformed in a similar way. The image transformations are good at countering the (iterative) fast gradient sign method \citep{kurakin2016adversarial}, Deepfool \citep{dezfooli2016deepfool}, and the \citet{carlini2017towards} attack, even in gray-box settings in which the model architecture and parameters are public. Our strongest defenses are based on total variation minimization and image quilting: these defenses are non-differentiable and inherently random, which makes it difficult for an adversary to get around them. \emph{Our best defenses eliminate $60\%$ of gray-box attacks and $90\%$ of black-box attacks by four major attack methods that perturb pixel values by $8\%$ on average.}

%The effectiveness of current defenses that transform the input image can be mitigated in a similar way: the adversary can enhance its attack by incorporating such defenses in the training of the model~\cite{athalye2017synthesizing}.

% \begin{displayquote}
% \emph{Can we destroy the adversarial perturbation contained in a given image while retaining enough of the semantic information to make a correct prediction?} 
% \end{displayquote}

%Ideally, a strong defense against adversarial examples in image classification must be robust in the worst case scenario of a white box iterative attack. While both of the previously mentioned types of solutions have so far been ineffective in this setting, we believe image transformation based methods are more appealing since they are typically \emph{model-agnostic} and do not make any assumption about the adversary. The following question summarizes the rationale underlying this category of defense: 

% Naturally, if the transformation can be incorporated into the adversary's strategy, the resulting perturbation will be intrinsically robust to it. Hence the answer to the question will be negative. However, when it is difficult to combine the transformation with gradient-based attacks, it will be a strong defense irrespective of the attacker's strategy. In this work, we take an essential step in this direction and explore various transformations starting with image \emph{cropping-rescaling, random pixel dropping} and \emph{total variance minimization}~\cite{}. We conduct extensive experiments showing that each of these transformations is a valid defense against widely adopted attacks such as the (iterative) \emph{fast gradient sign method}~\citep{kurakin2016adversarial}, \emph{Deepfool}~\citep{moosavi2015deepfool} or \emph{Carlini-Wagner}~\citep{carlini2017towards}, \emph{provided the attack is not enhanced by a transformation}. Next, we turn to \emph{image quilting}~\citep{efros2001quilting} as a defense against adversarial examples. Quilting uses a database of image patches to reconstruct the image. Consequently, the classifier never sees the original image and its accompanying adversarial perturbation. The private database of image patches is an additional layer of security making an attack difficult even when the parameters of the model are public. {\color{red} Our experiments validate the effectiveness of image quilting against white box iterative attacks. To the best of our knowledge, it is the first defense enjoying this property.}














