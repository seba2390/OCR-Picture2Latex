% !TeX root = ../main.tex

One of the first successful attack methods
is the \textbf{fast gradient sign method} (FGSM; \cite{goodfellow2015explaining}). Let $\ell(\cdot, \cdot)$ be the differentiable loss function that was used to train the classifier $h(\cdot)$, \emph{e.g.}, the cross-entropy loss. The FGSM adversarial example corresponding to a source input $\bx$ and true label $y$ is:
\begin{equation}
\label{fgsm}
\bx' = \bx + \epsilon \cdot \sign\left(\nabla_\bx \ell(\bx, y)\right),
\end{equation}
for some $\epsilon \!>\! 0$ that governs the perturbation magnitude. A stronger variant of this attack, called
\textbf{iterative FGSM} (I-FGSM; \cite{kurakin2016physical}), iteratively applies the FGSM update:
\begin{equation}
\label{ifgsm}
\bx^{(m)} = \bx^{(m-1)} + \epsilon \cdot \sign\left(\nabla_{\bx^{(m-1)}} \ell(\bx^{(m-1)}, y)\right),
\end{equation}
where $m = 1,\ldots,M$; $\bx^{(0)} = \bx$; and $\bx' = \bx^{(M)}$. The number of iterations $M$ is set such that $h(\bx') \neq h(\bx)$. Both FGSM and I-FGSM approximately minimize the Chebyshev distance between the inputs and the adversarial
examples they generate. 

Alternative attacks aim to minimize the Euclidean distance between the input and the adversarial example instead. For instance, assuming $h(\cdot)$ is a binary classifier, \textbf{DeepFool}
\citep{dezfooli2016deepfool} projects $\bx$ onto a linearization of the decision boundary defined by $h(\cdot)$ for $M$ iterations:
\begin{equation}
\bx^{(m)} = \bx^{(m-1)} - \epsilon \cdot \frac{h(\bx^{(m-1)})}{\lVert \nabla_{\bx^{(m-1)}} h(\bx^{(m-1)}) \rVert_2^2} \nabla_{\bx^{(m-1)}} h\left(\bx^{(m-1)}\right),
\end{equation}
where $\bx^{(0)}$ and $\bx'$ are defined as in I-FGSM. The multi-class variant of DeepFool performs the projection onto the nearest class boundaries. The linearization performed in DeepFool is particularly well suited for ReLU-networks, as these represent piecewise linear class boundaries. 

\textbf{Carlini-Wagner's $L_2$ attack} (CW-L2; \cite{carlini2017towards}) is an optimization-based attack that
combines a differentiable surrogate for the model's classification accuracy with an $L_2$-penalty
term. Let $Z(\bx)$ be the operation that computes the logit vector (\emph{i.e.}, the output before the softmax layer)
for an input $\bx$, and $Z(\bx)_k$ be the logit value corresponding to class $k$. The untargeted variant of CW-L2 finds a solution to the unconstrained
optimization problem
\begin{equation}
\label{cwl2}
\min_{\bx'} \left[ \| \bx - \bx' \|_2^2 + \lambda_f \max\left(-\kappa, Z(\bx')_{h(\bx)} - \max \{Z(\bx')_k : k \neq h(\bx)\}\right) \right],
\end{equation}
where $\kappa$ denotes a margin parameter, and where the parameter $\lambda_f$ trades off the perturbation norm and the hinge loss of predicting a different class. We perform the minimization over $\bx'$ using the Adam optimizer \citep{kingma2014adam} for $100$ iterations with an initial learning rate of $0.001$. 

\begin{figure*}[t!]
    \centering
    \includegraphics[width=0.95\textwidth]{figures/samples.pdf}
    \caption{Adversarial images and corresponding perturbations at five levels of normalized $L_2$-dissimilarity for all four attacks.}\label{samples}
\end{figure*}

All of the aforementioned attacks enforce that $\bx' \in \mathcal{X}$ by clipping values between $0$ and $1$. Figure~\ref{samples} shows adversarial images produced by all four attacks at five normalized $L_2$-dissimilarity levels.
