% !TeX root = ../main.tex
The results from this study suggest there exists a range of image transformations that have the potential to remove adversarial perturbations while preserving the visual content of the image: one merely has to train the convolutional network on images that were transformed in the same way. A critical property that governs which image transformations are most effective \emph{in practice} is whether an adversary can incorporate the transformation in its attack. For instance, median filtering likely is a weak remedy because one can backpropagate through the median filter, which is sufficient to perform any of the attacks described in Section~\ref{attacks}. A strong input-transformation defense should, therefore, be non-differentiable and randomized, a strategy has been previously shown to be effective \citep{wang2016adversary, wang2016learning}. %Its strength may be further increased by adhering to Kerckhoffs' principle by having a ``secret key'': a variable on which the attacker cannot obtain information. 
Our two best defenses possess both properties:
\begin{enumerate}
\item Both total variation minimization and image quilting are difficult to differentiate through. Specifically, total variation minimization involves solving a complex minimization of a function that is inherently random. Image quilting involves a discrete variable that selects the patch from the database, which is a non-differentiable operation, and the graph-cut optimization complicates the use of differentiable approximations \citep{maddison16concrete}.
\item Both total variation minimization and image quilting give rise to \emph{randomized} defenses. Total variation minimization randomly selects the pixels it uses to measure reconstruction error on when creating the denoised image. Image quilting randomly selects one of the $K$ nearest neighbors uniformly at random. The inherent randomness of our defenses makes it difficult to attack the model: it implies the adversary has to find a perturbation that alters the prediction for the entire distribution of images that could be used as input, which is harder than perturbing a single image \citep{moosavi2017universal}.
% \item The patch database used in image quilting is a kind of \emph{secret key}: it is difficult for an adversary to get insight into the contents of the database because it never observes the quilted images but only the model predictions. The strength of the secret key depends in part on the number of patches in the database: for patches of size $P \!\times\! P$ and 24-bit colors, the secret nature of the database would be lost entirely if it contained $\left(2^{24}\right)^{P \times P}$ different patches.
\end{enumerate}

Our results with gray-box attacks suggest that randomness is particularly crucial in developing strong defenses. Therefore, we surmise that total variation minimization, image quilting, and related methods \citep{dong11centralized} are stronger defenses than deterministic denoising procedures such as bit-depth reduction, JPEG compression, or non-local means \citep{buades05nonlocal}. Defenses based on total variation minimization and image quilting also have an advantage over adversarial-training approaches \citep{kurakin2016adversarial}: an adversarially trained network is differentiable, which implies that it can be attacked using the methods in Section~\ref{attacks}. An additional disadvantage of adversarial training is that it focuses on a particular attack; by contrast, transformation-based defenses generalize well across attack methods because they are model-agnostic.

While our study focuses exclusively on image classification, we expect similar defenses to be useful in other domains for which successful attacks have been developed, such as semantic segmentation and speech recognition \citep{cisse2017houdini, zhang17dolphin}. In speech recognition, for example, total variance minimization can be used to remove perturbations from waveforms, and one could develop ``spectrogram quilting'' techniques that reconstruct a spectrogram by concatenating ``spectrogram patches'' along the temporal dimension. We leave such extensions to future work. In future work, we also intend to study combinations of our input-transformation defenses with ensemble adversarial training \citep{tramer2017ensemble}, and we intend to investigate new attack methods that are specifically designed to circumvent our input-transformation defenses. 