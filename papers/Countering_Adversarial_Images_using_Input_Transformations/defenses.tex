
% !TeX root = ../main.tex

% One explanation for the principle behind adversarial perturbations is that they alter the
% behavior of lower level convolutional filters that detect edges and textures. This misrepresented
% information propagates forward through the network and combine at the upper layers to produce
% features that are indistinguishable from the class that the example is designed to mimic.
% In summary, despite the per-pixel perturbation being relatively small, they combine across
% channels and spatial coordinates to fool the network.

% The intuition behind our defense mechanisms is to remove or replace pixels from the input image so that combination of per-pixel perturbations cannot occur. We investigate the effect of cropping, random pixel dropping, and image quilting on adversarial examples. \autoref{ladybug} shows a sample image and a corresponding adversarial image under these transformations.

Adversarial attacks alter particular statistics of the input image in order to change the model prediction. Indeed, adversarial perturbations $\bx \!-\! \bx'$ have a particular structure, as illustrated by Figure~\ref{samples}. We design and experiment with image transformations that alter the structure of these perturbations, and investigate whether the alterations undo the effects of the adversarial attack. We investigate five image transformations: (1) image cropping and rescaling, (2) bit-depth reduction, (3) JPEG compression, (4) total variance minimization, and (5) image quilting.

\begin{wrapfigure}{r}{0.5\textwidth}
    \vspace{-2em}
    \includegraphics[width=0.5\textwidth]{figures/ladybug.pdf}
    \caption{Illustration of total variance minimization and image quilting applied to an original and an adversarial image (produced using I-FGSM with $\epsilon \!=\! 0.03$, corresponding to a normalized $L_2$-dissimilarity of 0.075). From left to right, the columns correspond to: (1) no transformation, (2) total variance minimization, and (3) image quilting. From top to bottom, rows correspond to: (1) the original image, (2) the corresponding adversarial image produced by I-FGSM, and (3) the absolute difference between the two images above. Difference images were multiplied by a constant scaling factor to increase visibility.}
    \label{ladybug}
    \vspace{-2em}
\end{wrapfigure}

\subsection{Image cropping-rescaling, bit-depth reduction, and compression}

% \begin{figure}[t!]
%     \centering
%     \begin{subfigure}[t]{0.5\columnwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figures/ifgs_0_0025_resnet50_crop_top1.png}
%     \end{subfigure}%
%     \begin{subfigure}[t]{0.5\columnwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figures/ifgs_0_0250_resnet50_crop_top1.png}
%     \end{subfigure}
%     \caption{Effect of different crop and downsample ratios on adversarial examples.}
%     \label{crop-fig}
% \end{figure}

We first introduce three simple image transformations: image cropping-rescaling, bit-depth reduction \citep{xu2017feature}, and JPEG compression and decompression \citep{dziugaite2016study}. \emph{Image cropping-rescaling} has the effect of altering the spatial positioning of the adversarial perturbation, which is important in making attacks successful. Following \citet{he2016residual}, we crop and rescale images at training time as part of the data augmentation. At test time, we average predictions over random image crops. \emph{Bit-depth reduction} \citep{xu2017feature} perform a simple type of quantization that can removes small (adversarial) variations in pixel values from an image; we reduce images to $3$ bits in our experiments. \emph{JPEG compression} \citep{dziugaite2016study} removes small perturbations in a similar way; we perform compression at quality level $75$ (out of $100$).



% LAURENS: If we want to show this, this should move to the experimental section.
% \autoref{crop-fig} Shows the effectiveness of the random cropping for various values of $r$. The adversarial examples are generated by I-FGSM at $\epsilon = 0.001$ (left)
% and $\epsilon = 0.01$ (right) on a pretrained ResNet-50 model. The model achieves a Top-1
% validation accuracy of 76.02\% on clean images. At $r=1$, the adversarial image is unmodified and the corresponding
% accuracy reflects the base model's accuracy without any defense. For smaller perturbation, cropping is very effective even at higher values of $r$, which shows that these adversarial
% perturbations are very unstable against a few dropped pixels. At larger perturbation,
% cropping at much lower $r$ is the most effective by a large margin.

% We suspect that having a lower crop ratio always strictly increases effectiveness against
% adversarial examples. However, this comes at a large cost of losing the object of interest
% in crops of smaller size. To counter this effect, we can ensemble the prediction of different
% crops by averaging their class probability vectors. By taking the ensemble of 30 random crops,
% we have obtain a much higher accuracy on adversarial examples when using smaller crop ratios.

% \subsection{Pixel dropout}
% \label{pixel_dropout}

% \begin{figure}[t!]
%     \centering
%     \begin{subfigure}[t]{0.5\columnwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figures/ifgs_0_0025_resnet50_drop_top1.png}
%     \end{subfigure}%
%     \begin{subfigure}[t]{0.5\columnwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figures/ifgs_0_0250_resnet50_drop_top1.png}
%     \end{subfigure}
%     \caption{Effect of different random pixel drop rates on adversarial examples.}
%     \label{pixel-drop}
% \end{figure}

% Another method that breaks the structure of adversarial perturbations whilst keeping semantic content intact is  pixel dropout. As images channels are normalized to be zero-mean before they are used as input into the model, we implement pixel dropout by sampling a Bernoulli random variable $X(i, j, k)$ for each pixel location $(i, j, k)$ and replace the corresponding pixel value by $0$ whenever the draw equals $X(i, j, k) = 1$.

% LAURENS: Idem. If we want to show this, it should go in the experimental section.
% \autoref{pixel-drop} demonstrates the effect of random pixel drops on the same adversarial examples
% as in \autoref{crop-fig}. This defense effectively mitigates adversarial examples even when
% the drop rate is very small (e.g. $p = 0.05$). However, since the model does not have exposure to this type of transformation at training time, accuracy sharply deteriorates at larger $p$.


\subsection{Total variance minimization}
An alternative way of removing adversarial perturbations is via a compressed sensing approach that combines pixel dropout with total variation minimization \citep{rudin1992tvm}. This approach randomly selects a small set of pixels, and reconstructs the ``simplest'' image that is consistent with the selected pixels. The reconstructed image does not contain the adversarial perturbations because these perturbations tend to be small and localized.

% Because adversarial perturbations tend to be small and localized, compressed-sensing approaches that combine pixel dropout with total variation minimization are likely to remove these fine-scale perturbations without affecting the coarser-scale information in the image that contains most of its semantic information. The key idea of this approach is to randomly remove the majority of the pixel values from the image as to remove the adversarial perturbation, and then to reconstruct the image from the non-removed pixels. The reconstructed image likely does not contain much of the adversarial image.

Specifically, we first select a random set of pixels by sampling a Bernoulli random variable $X(i, j, k)$ for each pixel location $(i, j, k)$; we maintain a pixel when $X(i, j, k) = 1$. Next, we use total variation minimization to constructs an image $\bz$ that is similar to the (perturbed) input image $\bx$ for the selected set of pixels, whilst also being ``simple'' in terms of total variation by solving:
\begin{equation}
\label{tvcs}
\min_\bz \| (1-X) \odot (\bz - \bx) \|_2 + \lambda_{\tv} \cdot \tv_p(\bz).
\end{equation}
Herein, $\odot$ denotes element-wise multiplication, and $\tv_p(\bz)$ represents the $L_p$-total variation of $\bz$:
\begin{equation}
\label{lptv}
\tv_p(\bz) = \sum_{k=1}^{K} \left[ \sum_{i=2}^{N} \|\bz(i,:,k) - \bz(i-1,:,k)\|_p + \sum_{j=2}^{N} \|\bz(:,j,k) - \bz(:,j-1,k)\|_p \right].
\end{equation}
The total variation (TV) measures the amount of fine-scale variation in the image $\bz$, as a result of which TV minimization encourages removal of small (adversarial) perturbations in the image. The objective function~(\ref{tvcs}) is convex in $\bz$, which makes solving for $\bz$ straightforward. In our implementation, we set $p \!=\! 2$ and employ a special-purpose solver based on the split Bregman method
\citep{goldstein2009split} to perform total variance minimization efficiently.

The effectiveness of TV minimization is illustrated by the images in the middle column of Figure~\ref{ladybug}: in particular, note that the adversarial perturbations that were present in the background for the non-transformed image (see bottom-left image) have nearly completely disappeared in the TV-minimized adversarial image (bottom-center image). As expected, TV minimization also changes image structure in non-homogeneous regions of the image, but as these perturbations were not adversarially designed we expect the negative effect of these changes to be limited.

%When defending against adversaries that minimize the $L_\infty$-dissimiarity
%(such as FGSM and I-FGSM), it seems intuitive that the $L_2$ reconstruction loss in
%\autoref{tvcs} should be replaced by $L_\infty$. However, since optimizing the $L_\infty$-norm
%directly is intractable, we solve the equivalent constrained optimization problem instead.
%\begin{align}
%\label{tvinf}
%&\min_\bz \tv_p(\bz) \nonumber \\
%\text{s.t. } \bx(i,j,k) - \tau &\leq \bz(i,j,k) \leq \bx(i,j,k) + \tau \\
%&\hspace{36pt} \text{for all } X(i,j,k) = 0. \nonumber
%\end{align}
%This gives rise to a box-constrained convex optimization problem. However, due to the large number
%of variables, we chose to use L-BFGS with box constraint \cite{byrd1995limited} instead.

% LAURENS: This should move to experiments, also.

% We apply TV compressed sensing to the same adversarial examples as in \autoref{crop-fig}.
% We use isotropic TV minimization and choose $\lambda_{\tv} $ to optimize the visual quality
% of reconstruction images while removing a large portion of adversarial perturbation. As shown
% in \autoref{pixel-drop}, applying total variation reconstruction significantly improves accuracy
% compared to random pixel drops, especially for larger drop rates (e.g. $p \geq 0.8$).

\subsection{Image quilting}

% \begin{figure}[t!]
%     \centering
%     \includegraphics[width=0.5\textwidth]{figures/ifgs_resnet50_quilt_top1.png}
%     \caption{Effect of different patch size for quilting on adversarial examples.}
%     \label{quilting}
% \end{figure}

Image quilting \citep{efros2001quilting} is a non-parametric technique that synthesizes images by piecing together small patches that are taken from a database of image patches. The algorithm places appropriate patches in the database for a predefined set of grid points, and computes minimum graph cuts \citep{boykov2001graphcuts} in all overlapping boundary regions to remove edge artifacts.

Image quilting can be used to remove adversarial perturbations by constructing a patch database that only contains patches from ``clean'' images (without adversarial perturbations); the patches used to create the synthesized image are selected by finding the $K$ nearest neighbors (in pixel space) of the corresponding patch from the adversarial image in the patch database, and picking one of these neighbors uniformly at random. The motivation for this defense is that the resulting image only consists of pixels that were not modified by the adversary --- the database of real patches is unlikely to contain the structures that appear in adversarial images.

The right-most column of Figure~\ref{ladybug} illustrates the effect of image quilting on adversarial images. Whilst interpretation of these images is more complicated due to the quantization errors that image quilting introduces, it is interesting to note that the absolute differences between quilted original and the quilted adversarial image appear to be smaller in non-homogeneous regions of the image. This suggests that TV minimization and image quilting lead to inherently different defenses.

% \autoref{quilting} shows the effect of image quilting with various patch sizes. For smaller $b$, the model's accuracy on clean images is very high, but is not as robust against adversarial images. This is likely due to adversarial perturbation affecting the choice of the nearest neighbor from $\mathcal{B}$. For larger patch size, this is more difficult, but at the cost of image fidelity and accuracy on clean images.
