% !TeX root = ../main.tex
\begin{figure*}[t!]
    \centering
    \includegraphics[width=\textwidth]{figures/Adversarial_block_diagram.pdf}
    \caption{Block diagram detailing the differences between the experimental setups in Section \ref{results_part1}, \ref{results_part2}, and \ref{results_part3}. We train networks (a) on regular images or (b) on transformed images; we test the networks on transformed adversarial images. For each of the three setups, dashed arrows indicate which model is used by the adversary and which model is used by the classification model.}\label{fig-setup}
\end{figure*}

We performed five experiments to test the efficacy of our defenses. The experiment in Section \ref{results_part1} considers gray-box attacks: it applies the defenses on adversarial images before using them as input into a convolutional network trained to classify ``clean'' images. In this setting, the adversary has access to the model architecture and parameters but is unaware of the defense strategy. The experiment in Section \ref{results_part2} focuses on a black-box setting: it replaces the convolutional network by networks that were trained on images with a particular input-transformation. The experiment in Section \ref{results_part3} combines our defenses with ensembling and model transfer. The experiment in Section \ref{results_part4} investigates to what extent networks trained on image-transformations can be attacked in a gray-box setting. The experiment in Section \ref{results_part5} compares our defenses with prior work. The setup of our gray-box and black-box experiments is illustrated in Figure~\ref{fig-setup}.
%We will release code to reproduce our results in the near future.
% for ICLR 2018 submission
Code to reproduce our results is available at \url{https://github.com/facebookresearch/adversarial_image_defenses}.

\subsection{Experimental Setup}

\begin{figure*}[t!]
    \centering
    \includegraphics[width=0.75\textwidth]{figures/resnet50_compare_all.pdf}
    \caption{Top-1 classification accuracy of ResNet-50 \emph{tested} on transformed adversarial images produced by four attacks using five image transformations \emph{in a gray-box setting}: (1) cropping-rescaling, (2) bit-depth reduction, (3) JPEG compression, (4) total variance minimization, and (5) image quilting. The dotted line shows the top-1 accuracy of the ResNet-50 model on non-adversarial images, providing an upper bound on the effectiveness of a defense. An $L_2$-dissimilarity of $0.00$ corresponds to the classification accuracy on non-adversarial images. Higher is better.}\label{adv-acc}
\end{figure*}

We performed experiments on the ImageNet image classification dataset. The dataset comprises $1.2$ million training images and $50,000$ test images that correspond to one of $1,000$ classes. Our adversarial images are produced by attacking a ResNet-50 model \citep{he2016residual}. We evaluate our defense strategies against the four adversarial attacks presented in Section~\ref{attacks}. We measure the strength of an adversary in terms of its normalized $L_2$-dissimilarity and report classification accuracies as a function of the normalized $L_2$-dissimilarity. To produce adversarial images like those in Figure~\ref{samples}, we set the normalized $L_2$-dissimilarity for each of the attacks as follows:
\begin{itemize}
\setlength\itemsep{-1em}
\item \emph{FGSM.} Increasing the step size $\epsilon$ increases the normalized $L_2$-dissimilarity.\\
\item \emph{I-FGSM.} We fix $M \!=\! 10$, and increase $\epsilon$ to increase the normalized $L_2$-dissimilarity.\\
\item \emph{DeepFool.} We fix $M \!=\! 5$, and increase $\epsilon$ to increase the normalized $L_2$-dissimilarity.\\
\item \emph{CW-L2.} We fix $\kappa \!=\! 0$ and $\lambda_f \!=\! 10$, and multiply the resulting perturbation by an appropriately chosen $\epsilon \geq 1$ to alter the normalized $L_2$-dissimilarity.
\end{itemize}
We fixed the hyperparameters of our defenses in all experiments: specifically, we set pixel dropout probability $p \!=\! 0.5$ and the regularization parameter of the total variation minimizer $\lambda_{\tv} \!=\! 0.03$. We use a quilting patch size of $5 \! \times \! 5$ and a database of $1,000,000$ patches that were randomly selected from the ImageNet training set. We use the nearest neighbor patch (\emph{i.e.}, $K \! = \! 1$) for experiments in Sections \ref{results_part1} and \ref{results_part2}, and randomly select a patch from one of $K \! = \! 10$ nearest neighbors in all other experiments. In the cropping defense, we sample $30$ crops of size $90 \!\times\! 90$ from the $224 \!\times\! 224$ input image, rescale the crops to $224 \!\times\! 224$, and average the model predictions over all crops.

\subsection{Gray Box: Image Transformations at Test Time}
\label{results_part1}

\begin{figure*}[t!]
    \centering
    \includegraphics[width=0.75\textwidth]{figures/resnet50_trained_compare_all.pdf}
    \caption{Top-1 classification accuracy of ResNet-50 \emph{trained and tested} on transformed adversarial images produced by four attacks using five image transformations \emph{in a black-box setting}: (1) cropping-rescaling, (2) bit-depth reduction, (3) JPEG compression, (4) total variance minimization, and (5) image quilting. The dotted line represents the top-1 accuracy of the ResNet-50 model on non-adversarial images, providing an upper bound on the effectiveness of a defense. An $L_2$-dissimilarity of $0.00$ corresponds to the classification accuracy on non-adversarial images. Higher is better.}\label{adv-acc-trained}
\end{figure*}

Figure~\ref{adv-acc} shows the top-1 accuracy of a ResNet-50 tested on transformed adversarial images as a function of the adversary strength for each of the four attacks. Each plot shows results for five different transformations we apply to the images at test time (\emph{viz.}, image cropping-rescaling, bit-depth reduction, JPEG compression, total variation minimization, and image quilting). The dotted line shows the classification error of the ResNet-50 model on images that are not adversarially perturbed, \emph{i.e.}, it gives an upper bound on the accuracy that defenses can achieve.

In line with the results reported in the literature, the four adversaries successfully attack the ResNet-50 model in nearly all cases (FGSM has a slightly lower favorable attack rate of $80 \!-\! 90$\%) when the input images are not transformed. The results also show that the proposed image transformations are capable of partly eliminating the effect of the attacks. In particular, ensembling $30$ predictions over different, random image crops is very efficient: these predictions are correct for $40 \!-\! 60$\% of the images (note that $76$\% is the highest accuracy that one can expect to achieve). This result suggests that adversarial examples are susceptible to changes in the location and scale of the adversarial perturbations. While not as effective, image transformations based on total variation minimization and image quilting also successfully defend against adversarial examples from all four attacks: applying these transformations allows us to classify $30 \!-\! 40$\% of the images correctly. This result suggests that total variation minimization and image quilting can successfully remove part of the perturbations from adversarial images. In particular, the accuracy of the image-quilting defense hardly deteriorates as the strength of the adversary increases. However, the quilting transformation does severely impact the model's accuracy on non-adversarial images.

\subsection{Black Box: Image Transformations at Training and Test Time}
\label{results_part2}

The high relative performance of image cropping-rescaling in~\ref{results_part1} may be partly explained by the fact that the convolutional network was trained on randomly cropped-rescaled images\footnote{We trained the ResNet-50 model using the data-augmentation scheme of \citet{he2016residual}.}, but not on any of the other transformations. This implies that independent of whether an image is adversarial or not, the network is more robust to image cropping-rescaling than it is to those transformations. The results in Figure~\ref{adv-acc} suggest that this negatively affects the effectiveness of these defenses, even if the defenses are successful in removing the adversarial perturbation. To investigate this, we trained ResNet-50 models on transformed ImageNet training images. We adopt the standard data augmentation from \citet{he2016residual}, but apply bit-depth reduction, JPEG compression, TV minimization, or image quilting on the resized image crop before feeding it to the network. We measure the classification accuracy of the resulting networks on the same adversarial images as before. Note that this implies that we assume a black-box setting in this experiment. 

We present the results of these experiments in Figure~\ref{adv-acc-trained}. Training convolutional networks on images that are transformed in the same way as at test time, indeed, dramatically improves the effectiveness of all transformation defenses. In our experiments, the image-quilting defense is particularly effective against strong attacks: it successfully defends against $80 \!-\! 90\%$ of all four attacks, even when the normalized $L_2$-dissimilarity of the attack approaches $0.08$. %This is an encouraging result, in particular, because image quilting has properties that make it difficult for an attacker to circumvent the defense even in a white-box setting; we discuss these properties in Section~\ref{conclusion}.

\subsection{Black Box: Ensembling and Model Transfer}
\label{results_part3}

\begin{table}[!t]
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{lccccccccccccccc}
\toprule
&& \multicolumn{4}{c}{\bf Quilting} && \multicolumn{4}{c}{\bf TVM + Quilting} && \multicolumn{4}{c}{\bf Cropping + TVM + Quilting}\\
&~& \bf RN50 & \bf RN101 & \bf DN169 & \bf Iv4 &~& \bf RN50 & \bf RN101 & \bf DN169 & \bf Iv4 &~~~& \bf RN50 & \bf RN101 & \bf DN169 & \bf Iv4\\\midrule
\bf No Attack && 70.07 & 72.56 & 70.18 & 73.01 && 72.38 & 74.74 & 73.10 & \bf 75.55 && 72.14 & 74.53 & 72.92 & 75.10\\
\rule{0pt}{2.5ex}\bf FGSM && 65.45 & 68.50 & 65.96 & 67.53 && 65.70 & 68.77 & 67.09 & 69.19 && 66.65 & 69.75 & 67.86 & \bf 70.37\\
\bf I-FGSM && 65.59 & 68.72 & 66.16 & 69.29 && 65.84 & 69.10 & 67.32 & 71.05 && 67.03 & 70.14 & 68.20 & \bf 71.52\\
\bf DeepFool && 65.20 & 68.73 & 65.86 & 68.70 && 65.80 & 69.34 & 67.40 & 71.03 && 67.11 & 70.49 & 68.62 & \bf 71.47\\
\bf CW-L2 && 64.11 & 67.72 & 65.00 & 68.14 && 63.99 & 68.20 & 66.08 & 70.13 && 65.31 & 69.14 & 66.96 & \bf 70.50\\\bottomrule
\end{tabular}
}
\caption{Top-1 classification accuracy of ensemble and model transfer defenses (columns) against four black-box attacks (rows). The four networks we use to classify images are ResNet-50 (RN50), ResNet-101 (RN101), DenseNet-169 (DN169), and Inception-v4 (Iv4). Adversarial images are generated by running attacks against the ResNet-50 model, aiming for an average normalized $L_2$-dissimilarity of $0.06$. Higher is better. The best defense against each attack is typeset in boldface.}\label{table:transfer}
\end{table}

We evaluate the efficacy of (1) ensembling different defenses and (2) ``transferring'' attacks to different network architectures (in a black-box setting). Specifically, we measured the accuracy of four networks using ensembles of defenses on adversarial images generated to attack a ResNet-50; the four networks we consider are ResNet-50, ResNet-101, DenseNet-169 \citep{huang2016densely}, and Inception-v4 \citep{szegedy2017inception}. To ensemble the image quilting and TVM defenses, we average the image-quilting prediction (using a weight of $0.5$) with model predictions for $10$ different TVM reconstructions (with a weight of $0.05$ each), re-sampling the pixels used to measure the reconstruction error each time. To combine cropping with other transformations, we first apply those transformations and average predictions over $10$ random crops from the transformed images. 

The results of our ensembling experiments are presented in Table~\ref{table:transfer}. The results show that gains of $1 \!-\! 2\%$ in classification accuracy can be achieved by ensembling different defenses, whereas transferring attacks to different convolutional network architectures can lead to an improvement of $2 \!-\! 3\%$. Inception-v4 performs best in our experiments, but this may be partly due to that network having a higher accuracy even in non-adversarial settings. Our best black-box defense achieves an accuracy of about $71\%$ against all four defenses: the attacks deteriorate the accuracy of our best classifier (which combines cropping, TVM, image quilting, and model transfer) by at most $6\%$.

\subsection{Gray Box: Image Transformations at Training and Test Time}
\label{results_part4}

\begin{figure*}[t!]
    \centering
    \includegraphics[width=0.75\textwidth]{figures/resnet50_trained_whitebox_compare_all.pdf}
    \caption{Top-1 classification accuracy of ResNet-50 \emph{trained and tested} on transformed adversarial images produced by four attacks using five image transformations \emph{in a gray-box setting}: (1) cropping-rescaling, (2) bit-depth reduction, (3) JPEG compression, (4) total variance minimization, and (5) image quilting. The dotted line represents the top-1 accuracy of the ResNet-50 model on non-adversarial images, providing an upper bound on the effectiveness of a defense. $L_2$-dissimilarity of 0 corresponds to clean image accuracy. Higher is better.}\label{adv-acc-trained-whitebox}
\end{figure*}

The previous experiments demonstrated the effectiveness of image transformations against adversarial images, in particular, when convolutional networks are re-trained to be robust to those image transformations. In this experiment, we investigate to what extent the resulting networks can be attacked in a gray-box setting in which the adversary has access to those networks (but does not have access to the input transformations applied at test time). We use the four attack methods to generate novel adversarial images against the transformation-robust networks trained in~\ref{results_part2}, and measure the accuracy of the networks on these novel adversarial images in Figure~\ref{adv-acc-trained-whitebox}.

The results show that bit-depth reduction and JPEG compression are weak defenses in such a gray-box setting. Whilst their relative ordering varies between attack methods, image cropping and rescaling, total variation minimization, and image quilting are fairly robust defenses in the white-box setting. Specifically, networks using these defenses classify up to $50\%$ of adversarial images correctly. 

\subsection{Comparison with Prior Work}
\label{results_part5}

In our final set of experiments, we compare our defenses with the state-of-the-art \emph{ensemble adversarial training} approach proposed by \citet{tramer2017ensemble}. Ensemble adversarial training fits the parameters of a convolutional network on adversarial examples that were generated to attack an ensemble of pre-trained models. These adversarial examples are very diverse, which makes the convolutional network being trained robust to a variety of adversarial perturbation. In our experiments, we used the model released by \citet{tramer2017ensemble}: an Inception-Resnet-v2 \citep{szegedy2016rethinking} trained on adversarial examples generated by FGSM against Inception-Resnet-v2 and Inception-v3 models. We compare the model to our ResNet-50 models with image cropping, total variance minimization, and image quilting defenses. We note that there are two small differences in terms of the assumptions that ensemble adversarial training makes and the assumptions our defenses make: (1) in contrast to ensemble adversarial training, our defenses assume that part of the defense strategy (\emph{viz.}, the input transformation) is unknown to the adversary, and (2) in contrast to ensemble adversarial training, our defenses assume no prior knowledge of the attacks being used. The former difference is advantageous to our defenses, whereas the latter difference gives our defenses a disadvantage compared to ensemble adversarial training.

Table~\ref{table:ensemble} compares the classification accuracies of the defense strategief on adversarial examples with a normalized $L_2$-dissimilarity of $0.06$. The results show that ensemble adversarial training works better on FGSM attacks (which it uses at training time), but is outperformed by each of the transformation-based defenses all other attacks. Input transformations particularly outperform ensemble adversarial training against the iterative attacks: our defense are are $18 \!-\! 24 \times$ more robust than ensemble adversarial training against DeepFool attacks. Combining cropping, TVM, and quilting increases the accuracy of our defenses against DeepFool gray-box attacks to $51.51\%$ (compared to $1.84\%$ for ensemble adversarial training).

\begin{scriptsize}
\begin{table}[!t]
\centering
\resizebox{0.6\textwidth}{!}{%
\begin{tabular}{lcccc}
\toprule
~ & \bf Cropping & \bf TVM & \bf Quilting & \bf \begin{tabular}{@{}c@{}}Ensemble Training \\ \citep{tramer2017ensemble}\end{tabular} \\\midrule
\bf No Attack & 65.41 & 66.29 & 69.66 & \bf 80.3 \\
\rule{0pt}{2.5ex}\bf FGSM & 49.52 & 31.37 & 39.55 & \bf 69.15 \\
\bf I-FGSM & \bf 43.89 & 40.99 & 33.22 & 5.07\\
\bf DeepFool & \bf 44.92 & 44.69 & 34.54 & 1.84 \\
\bf CW-L2 & 41.06 & \bf 48.41 & 30.51 & 22.23 \\\bottomrule
\end{tabular}
}
\caption{Top-1 classification accuracy on images perturbed using  attacks against ResNet-50 models trained on input-transformed images, and an Inception-v4 model trained using ensemble adversarial. Adversarial images are generated by running attacks against the models, aiming for an average normalized $L_2$-dissimilarity of $0.06$. The best defense against each attack is typeset in boldface.}\label{table:ensemble}
\end{table}
\end{scriptsize}
