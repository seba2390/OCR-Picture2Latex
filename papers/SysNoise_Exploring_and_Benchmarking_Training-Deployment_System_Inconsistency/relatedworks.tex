\section{Related Work}
\textbf{Noises Types and Benchmarks.} Extensive shreds of evidence have shown that deep learning models are unstable towards different noises, including adversarial noises and natural noises. \emph{Adversarial noises}, which are imperceptible to human vision, could easily make neural networks misclassify the input images~\cite{szegedy2013intriguing,fgsm,liu2022harnessing,liu2023x,Liu2019Perceptual,liang2021parallel,Wang_2021_CVPR,liu2020bias,liu2020spatiotemporal}. To benchmark and evaluate adversarial robustness, \cite{su2018is} first investigated the adversarial robustness of 18 models on ImageNet; \cite{Ling2019Deepsec} built the  platform DEEPSEC for adversarial robustness analysis including 16 adversarial attacks, and 13 adversarial defenses; meanwhile, RealSafe \cite{Dong2020Benchmarking} open-sourced and benchmarked adversarial robustness on image classification tasks. More recently, large-scale benchmarks on adversarial robustness regarding defense strategies (RobustBench \cite{croce2020robustbench}) and model architectures (RobustART \cite{tang2021robustart,liu2023exploring}) were developed. Besides adversarial noises, there exist another type of model-agnostic noise named \emph{natural noises} (also deemed as corruptions), which are commonly witnessed in the real-world scenario, \eg, blur, snow, and frost. Some representative datasets are constructed to simulate and benchmark the natural noises, such as ImageNet-P, ImageNet-C~\cite{hendrycks2019robustness}, and ImageNet-A, ImageNet-O~\cite{hendrycks2021nae}. \cite{hendrycks2020many} also introduced new real-world distribution shift datasets including changes in image style, geographic location \etc. However, these studies only focus on noises brought during data acquisition, while ignoring the impacts of the whole inference pipeline caused by different system implementations.

Furthermore, some studies introduce the influence of individual SysNoise. \cite{yan2021real, boltaevich2019estimationresize} show how image pre-processing progresses including image decoder, resize method, and color conversion generate noise. However, they only introduce one or two noises in image pre-processing and lack investigation on the whole training-deployment progress as well as the combination of system noise.
\cite{biterror} introduces the bit error that is caused by the Low-voltage operation of DNN accelerators, which does illustrate that training-deployment system inconsistency can bring error. And \cite{randomnessintraining} show the random noise caused by different training systems. But this work only focuses on differences in the training system and ignores the deployment system.
In addition, ~\cite{jia2021exploiting} takes the first step towards the influence of the floating-point value representation. They highlight that, to achieve practically reliable verification of neural networks, the system must accurately model the effects of any floating-point computations. However, this paper only conducts a preliminary attempt at the effect of floating-point numerical error for neural network verifiers. 

% By contrast, \emph{this paper for the first time proposes system noises, which is caused by the training-deployment system inconsistency, and accordingly builds a benchmark to comprehensively study the effect on model performance.}



\textbf{Approaches to Improving Model Robustness.} To improve model robustness against \emph{adversarial noises}, a long line of adversarial defense works have been proposed including: (1) adversarial training that adversarially train deep models using adversarial examples \cite{goodfellow2014explaining,PGD,tramer2017ensemble,shafahi2019adversarial,liu2021ANP,zhang2021interpreting}; and (2) adversarial detection that distinguishes the clean example and adversarial example \cite{grosse2017statistical,gong2017adversarial,jiang2020attack}. 

To effectively tackle the \emph{natural noises}, several studies have been devoted primarily from the perspective of data augmentation. By producing an elementwise convex combination of two images, Mixup~\cite{mixup2017} could regularize neural networks to favor simple linear behavior in-between training examples and improve model performance. Different from Mixup, AutoAugment \cite{2018AutoAugment} adopts and tunes a group of augmentations to optimize performance on a downstream task. To further improve model robustness against natural noises, AugMix \cite{hendrycks2020augmix} was proposed to mix multiple augmented images. And APR-SP~\cite{chen2021amplitude} was proposed to force the CNN to pay more attention on the structured information from phase components and keep robust to the variation of the amplitude which can help with the model's robustness of natural noise.

Test-time adaptation is another way to improve the model's performance at inference. It refers to adapting a machine learning model to a target domain at test time, without access to the source data or even any additional labeled/unlabeled samples from the target distribution to fine-tune the source model. \cite{tent} propose a method to reduce generalization error by reducing the entropy of model predictions on test data, and it reduces error for image classification on corrupted ImageNet and CIFAR-10/100 and reaches a new state-of-the-art error on ImageNet-C.



 %The adversarial examples were first introduced by~\cite{szegedy2013intriguing}. From then on, various adversarial attacks were proposed, such as FGSM~\cite{fgsm}, PGD~\cite{PGD}, C\&W~\cite{CW} and some black-box attack methods~\cite{black_box}. Driven by the emergence of adversarial noises, corresponding defense techniques also arose, including adversarial training~\cite{ganin2016domain,tramer2017ensemble,shafahi2019adversarial}, data augmentation~\cite{Maxup} and regularization~\cite{LabelSmoothing}. The adversarial examples are always dependent on the model to attack, especially for the block-box attack. Thus they suffer a low transferability and rarely occur in the practical scenario.

%\textbf{Natural Noises. } Besides the adversarial examples, the community also realizes the importance of natural noises that are widely existing in the real world. Some representative datasets are constructed to simulate the natural noise, such as ImageNet-P, ImageNet-C~\cite{hendrycks2019robustness}, and ImageNet-A, ImageNet-O~\cite{hendrycks2021nae}. These noises are model-agnostic and may cause perceptible perturbation. Natural noises such as Snow noise and Frost noise can measure the robustness of a model in the wild.


