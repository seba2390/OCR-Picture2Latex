\section{Experiment and Analysis}

In this section, we conduct a thorough benchmark and analysis on the SysNoise.
In~\autoref{sec_cls}, we illustrate the experimental setting for image classification, detection, segmentation, and NLP tasks; in~\autoref{sec_results} we extensively evaluate all types of SysNoise on these four tasks; in~\autoref{sec_interp}, we interpret the SysNoise by comparing it with natural noise and adversarial noise as well as some visualizations. 

\subsection{Experimental Setting}
\textbf{Classification Task.}
\label{sec_cls}
We benchmark SysNoise on the ImageNet dataset for the classification task, including both Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs). 
For CNNs, we evaluate ResNet~\cite{he2016deep}, MobileNetV2~\cite{sandler2018mobilenetv2}, RegNet~\cite{radosavovic2020designing}, and EfficientNet~\cite{tan2020efficientnet} families. 
In addition, we evaluate an extremely small architecture â€” MCUNet~\cite{lin2020mcunet}, which only has 0.74MB parameters. 
For ViTs we evaluate the original Vision Transformer~\cite{dosovitskiy2020vit} and the Swin Transformer~\cite{liu2021Swin} families.
Each family covers different computation and memory budgets to ensure both large and tiny models are verified. During training, we use Nvidia DALI ~\cite{dali} to prepare data, \ie, image decode, resize and color space are configured by default function in DALI. All models take an input shape of $224\times224$ except EfficientNet. We train the default model using FP32 format as this is the standard format in GPU training. For ResNet, we train it with the floor mode of its max-pooling layer. All other training settings follow the original settings of the model.

\textbf{Detection and Segmentation Task.}
\label{sec_detseg}
%\yh{Check the model description in this section.}
For object detection, we use COCO dataset and adopt 3 backbones: ResNet-34, ResNet-50, and MobileNetV2 in both Faster RCNN~\cite{ren2015faster} with FPN ~\cite{faster-fpn} and RetinaNet~\cite{lin2017focal}.
We use the CitySpace dataset to benchmark SysNoise on Segmentation Task, where we evaluate two architectures (Deeplabv3 and U-Net). As for DeepLabv3, following~\cite{deeplabv32018}, Resnet-50 and Resnet-101 backbones are used. During Training, we use the Pillow package and choose bilinear as an image resize interpolation method to prepare data. Following~\cite{faster-fpn} ,we resize images by keeping the ratio the same as the original image and make the maximum size of the image to be $1333\times800$. Following common practice, all backbones are pre-trained on ImageNet. We train the default model using FP32 format and train the original upsample layers with the nearest-neighbor interpolation. For the models with the ResNet backbone, we train it with the floor mode of its max-pooling layer. All other implementations follow the original settings of the model.

\textbf{Natural Language Processing Task}
For natural language processing tasks, we use pre-trained OPT ~\cite{opt} models which are transformer-based models with 125M to 175B parameters. For different natural language processing tasks, we use different datasets including PIQA~\cite{piqa}, LAMBADA~\cite{paperno-etal-2016-lambada}, HellaSwag~\cite{hellaswag} and WINOGRANDE~\cite{winogrande}.
Compared with computer vision, natural language tasks have less noise during pre-processing and post-processing progress. For simplicity, we use model inference noise, or data precision noise to measure SysNoise in these tasks.

To benchmark the robustness against SysNoise, we train deep neural networks with one fixed setting, also commonly used in the PyTorch framework, and evaluate the task performance under other settings depending on the different types of SysNoise (~\autoref{benchmark_sysnoise}).

% \yh{Not sure we should keep this paragraph or not.} Other implementation for ImageNet classification follows the practice in ~\cite{ResNet}. The image is resized with its shorter side randomly sampled in $[256,480]$ for scale augmentation. Then a $224 \times 224$ crop is randomly sampled from an image or its horizontal flip. The standard color augmentation is used. All models are trained from scratch with SGD optimizer for 100 epochs.
% In testing, the image input to the model is resized to $256 \times 256 $ before a $224 \times 224$ center crop is applied.
\subsection{Experimental Results}
\label{sec_results}

\textbf{Impact from single SysNoise.}
Our evaluation is summarized in \autoref{tab_cls} for ImageNet classification, \autoref{tab_det} for COCO detection, \autoref{tab_seg} for CitySpace segmentation, and \autoref{tab_nlp} for natural language processing. It can be observed that different types of SysNoise cause different levels of performance drop. \emph{For classification}, The color mode and FP16 precision have a subtle impact on the performance of CNNs, while the image decode and resize can have a 0.6-2.3\% accuracy decrease on average. In model inference noise, the ceiling mode has a profound effect, where the accuracy drops by 0.8-2.7\%. \emph{For detection and segmentation tasks}, there are extra types of SysNoise, the interpolation for upsample layer, and the proposal operation for post-processing. 
Notably, these two types cause a considerable performance drop. They cause Faster RCNN with ResNet-50 backbone drop of 1.7 and 2.4 mAP, respectively. 
\emph{For natural language processing tasks}, the impact of data precision has a greater relationship with the datasets.
In addition, we find SysNoise behaves differently at the task level. For example, The resize noise has a relatively larger impact on the detection task than the classification task. While the decoder noise nearly has no impact on detection and segmentation tasks but can affect classification models. 
% In general, for classification tasks, we can summarize the order as Ceiling Mode$>$Decode$\approx$Resize$>$INT8$>$Color Mode$>$FP16. 
% resize > ceil mode when report max (fix)

\textbf{Architecture-wise robustness again SysNoise.}
We also observe some relationships between architecture and SysNoise for classification. 
\textit{First, in the same architecture family, a larger model tends to have low accuracy degradation}. For instance, in ResNet and RegNet family the average accuracy decrease by decode noise reduces from 1.6\% to 0.6\% when switching from tiny to large models. The same trend is also found in other noises and tasks.
\textit{Second, the lightweight architecture family is more prone to SysNoise.} Specifically, the MobileNetV2 family shows a larger accuracy decrease than other architecture families.
The largest MobileNetV2 drops 1.65\% accuracy due to different resize methods while the similar-accuracy-level ResNet-50 only drops 0.75\%. 
Furthermore, the MCUNet for STM32F746 with just 320KB memory has the worst robustness among all models, which suffers from an average 4.0\% accuracy drop and a maximum 9.3\% accuracy drop in resizing noise.
%For data precision, the contrast is more substantial. This is somewhat concurrent with the fact that MobileNetV2 is prone to be compressed~\cite{nagel2019data}.
\textit{Third, ViTs demonstrate different robustness compared with CNNs.} The Swin Transformers are more robust than CNNs when attacked by decoder noise. Interestingly, both ViTs and Swin Transformers suffer from higher accuracy lost in color mode noise than CNNs. These results demonstrate the extremely high diversity of SysNoise. 

 


\begin{table*}[t]
%\vspace{-0.25in}
\centering
\caption{\textbf{ Measuring SysNoise on ImageNet classification benchmark.} We record Top-1 accuracy and the difference, $\Delta$ACC = ACC$_{\mathrm{original}}$-ACC$_{\mathrm{SysNoise}}$. We report both mean and max $\Delta$ACC for decode and resize. \emph{The lower} $\Delta$ACC \emph{the better.}}
\adjustbox{max width=\textwidth}{
\begin{tabular}{l c c c c c c c ccccc}
\toprule
\textbf{Architecture} & \multicolumn{1}{c}{\textbf{Trained}} & \multicolumn{1}{c}{\textbf{Decode}} &   \multicolumn{1}{c}{\textbf{Resize}} & \multicolumn{1}{c}{\textbf{Color Mode}}  &  \multicolumn{2}{c}{\textbf{Precision (FP16/INT8)}} & \multicolumn{1}{c}{\textbf{Ceil Mode}} & \multicolumn{1}{c}{\textbf{Combined}} \\
\cmidrule(l{2pt}r{2pt}){3-5} \cmidrule(l{2pt}r{2pt}){6-8} \cmidrule(l{2pt}r{2pt}){9-9} 
& ACC &  $\Delta$ACC & $\Delta$ACC & $\Delta$ACC & $\Delta$ACC & $\Delta$ACC & $\Delta$ACC & $\Delta$ACC \\
\midrule
% ResNet18x0.125 & 33.62 & 2.10\mypm{5.86E-01} & 2.26\mypm{1.21E-02} & 0.15  & -0.01 & 1.16  & 2.97 \\
MCUNet-293KB & 63.40 & 0.41\mypm{0.42} & 4.02\mypm{9.31} & 0.20  & 0.01 & 0.04  & - & 9.97\\
\midrule
ResNet18x0.25 & 48.96  & 1.98\mypm{2.12}  & 2.11\mypm{3.71} & 0.14  & -0.01 & 0.82  & 2.34 & 6.61 \\
ResNet18x0.5 & 61.64 & 1.67\mypm{1.76} & 1.76\mypm{3.25} & 0.19  & -0.01 & 0.15  & 2.72 & 6.10 \\
ResNet-18 & 69.96 & 1.02\mypm{1.03} & 1.01\mypm{2.05} & 0.13  & 0.00 & 0.20  & 2.40 & 4.97 \\
ResNet-34 & 73.59 & 0.99\mypm{1.00} & 0.77\mypm{1.67} & 0.14   & 0.00 & 0.04  & 0.85 & 4.25 \\
ResNet-50 & 76.39   & 0.98\mypm{0.98} & 0.75\mypm{1.69} & 0.09  & 0.00 & 0.06 & 1.24 & 3.95 \\
ResNet-101 & 78.10  & 0.68\mypm{0.69} & 0.62\mypm{1.47} & 0.24   & 0.01 & 0.69  & 0.75 & 4.50 \\
\midrule
MobileNetV2-0.5 & 64.94 & 1.98\mypm{2.00} & 2.04\mypm{3.14} & 0.18 &  0.01 & 0.57 & - & 5.81 \\
MobileNetV2-0.75 & 70.26 & 1.39\mypm{1.39} & 1.47\mypm{2.56} & 0.16   & 0.01 & 0.72 & - & 5.58 \\
MobileNetV2-1 & 73.12 & 1.39\mypm{1.39} & 1.48\mypm{2.43} & 0.07 & 0.02  & 0.77  & - & 5.03  \\
MobileNetV2-1.4 & 75.84 & 1.01\mypm{1.02} &  1.65\mypm{2.15} & 0.10  & 0.01  & 0.53  & - & 5.04   \\
\midrule
RegNetX-400M & 70.97  & 1.63\mypm{1.63} &  1.42\mypm{2.65} & 0.07 & 0.01 & 0.09  & - & 5.70  \\
RegNetX-800M & 74.04  & 1.12\mypm{1.14} &	0.97\mypm{2.00} & 0.19  & 0.02 & 0.24 & - & 4.38  \\
RegNetX-1.6G & 76.29  & 0.84\mypm{0.85} & 0.79\mypm{1.88} & 0.20  & 0.01 & 0.19  & - & 4.15 \\
RegNetX-3.2G & 77.89   & 0.61\mypm{0.62} & 0.53\mypm{1.42} & 0.20  & 0.00 & 0.24 & - & 3.70 \\
\midrule
EfficientNet-B0 & 76.83 & 0.75\mypm{0.76} &  1.70\mypm{3.79} & 0.15 & 0.03 & 0.19  & - & 4.39 \\
EfficientNet-B1 & 78.13 & 0.57\mypm{0.58} &  1.18\mypm{2.84} & 0.26 & 0.01 & 0.39  & - & 3.26 \\
EfficientNet-B2 & 79.97 & 0.57\mypm{0.58} &  1.13\mypm{2.31} & 0.05 & 0.04 & 0.41  & - & 3.10 \\
EfficientNet-B3 & 82.03 & 0.71\mypm{0.72} &  0.99\mypm{1.74} & 0.16 & 0.05 & 0.38  & - & 2.65 \\
EfficientNet-B4 & 83.43 & 0.29\mypm{0.30} &  0.45\mypm{0.93} & 0.17 & 0.02 & 0.26  & - & 2.32 \\
\midrule
ViT-Tiny & 75.61 & 1.04\mypm{1.04} & 0.99\mypm{1.79} & 0.46 & 0.01 & 0.68 & - & 3.21 \\
ViT-Small & 81.58 & 0.57\mypm{0.58} & 0.37\mypm{1.01} & 0.80 & -0.01 & 0.80 & - & 2.68 \\
Vit-Base & 84.63 & 0.61\mypm{0.62} & 0.43\mypm{0.74} & 0.93 & -0.01 & 1.12 & - & 2.89 \\
\midrule
Swin-Tiny & 81.32 & 0.18\mypm{0.19} & 0.42\mypm{1.76} & 1.21 & 0.00 & 0.76 & - & 4.93 \\
Swin-Small & 83.03 & 0.18\mypm{0.18} & 0.23\mypm{1.33} & 1.00 & 0.00 & 0.45 & - & 3.51 \\
Swin-Base & 83.54 & 0.11\mypm{0.30} & 0.21\mypm{1.27} & 0.97 & -0.01 & 0.55 & - & 3.59 \\
\bottomrule
\end{tabular}
}
\label{tab_cls}
%\vspace{-0.15in}
\end{table*}


\begin{table*}[htbp]
%\vspace{-0.25in}
\centering
\caption{\textbf{Measuring SysNoise on MS COCO detection.} We record mAP and the difference $\Delta$mAP = mAP$_{\mathrm{original}}$-mAP$_{\mathrm{SysNoise}}$. We report both mean and max $\Delta$mAP for decode and resize. \emph{The lower} $\Delta$mAP \emph{the better}. }
\adjustbox{max width=\textwidth}{
\begin{tabular}{l l c c c c c c c c c}
\toprule
\textbf{Method} & \textbf{Architecture} & \multicolumn{1}{l}{\textbf{Trained}} & \multicolumn{1}{c}{\textbf{Decode}} &   \multicolumn{1}{c}{\textbf{Resize}} & \multicolumn{1}{c}{\textbf{Color Mode}}  & \multicolumn{1}{c}{\textbf{Upsample}} & \multicolumn{1}{c}{\textbf{Precision INT8}} & \multicolumn{1}{c}{\textbf{Ceil Mode}}  & \multicolumn{1}{c}{\textbf{Post-processing}} & \multicolumn{1}{c}{\textbf{Combined}} \\
\cmidrule(l{2pt}r{2pt}){4-6} \cmidrule(l{2pt}r{2pt}){7-9} \cmidrule(l{2pt}r{2pt}){10-10} \cmidrule(l{2pt}r{2pt}){11-11} 
& & mAP &  $\Delta$mAP & $\Delta$mAP & $\Delta$mAP & $\Delta$mAP & $\Delta$mAP & $\Delta$mAP  & $\Delta$mAP & $\Delta$mAP \\
\midrule
& ResNet-34 & 36.76 & 0.02\mypm{0.04} & 0.93\mypm{2.63} & 0.25  & 1.28 & 0.06  & 2.50 &  2.29   & 10.25\\
Faster RCNN & ResNet-50 & 37.36  & 0.02\mypm{0.01} & 1.12\mypm{3.15} & 0.10 & 1.66 & 0.10 & 3.14 & 2.39 & 10.67   \\
& MobileNetV2 & 30.32  & 0.01\mypm{0.01} & 0.38\mypm{1.14} & 0.24  & 0.96  & 0.07 & - & 2.23 & 3.45 \\
\midrule
\multirow{2}{*}{RetinaNet} & ResNet-34 &  35.71  & 0.01\mypm{0.01} & 0.77\mypm{2.20} & 0.29  & 0.35 & 0.10 & 2.72 & 3.44 & 8.21  \\
& ResNet-50&  36.59 & 0.01\mypm{0.02} & 0.99\mypm{2.78} & 0.36 & 0.69 & 0.03 & 3.12 & 3.00 & 8.93  \\
\bottomrule
\end{tabular}
}
\label{tab_det}
%\vspace{-0.15in}
\end{table*}

\begin{table*}[htbp]
%\vspace{-0.1in}
\centering
\caption{\textbf{Measuring SysNoise on CitySpace segmentation.} We record mIOU and the difference $\Delta$mIOU = mIOU$_{\mathrm{original}}$-mIOU$_{\mathrm{SysNoise}}$. We report both mean and max $\Delta$mIoU for decode and resize. \emph{The lower} $\Delta$mIOU \emph{the better}. }
\adjustbox{max width=\textwidth}{
\begin{tabular}{l l c c c c c c c c}
\toprule
\textbf{Method} & \textbf{Architecture} & \multicolumn{1}{l}{\textbf{Trained}} & \multicolumn{1}{c}{\textbf{Decode}} &   \multicolumn{1}{c}{\textbf{Resize}} & \multicolumn{1}{c}{\textbf{Color Mode}}  & \multicolumn{1}{c}{\textbf{Upsample}} & \multicolumn{1}{c}{\textbf{Precision INT8}} & \multicolumn{1}{c}{\textbf{Ceil Mode}} & \multicolumn{1}{c}{\textbf{Combined}} \\
\cmidrule(l{2pt}r{2pt}){4-6} \cmidrule(l{2pt}r{2pt}){7-9} \cmidrule(l{2pt}r{2pt}){10-10} 
& & mIoU &  $\Delta$mIoU & $\Delta$mIoU & $\Delta$mIoU & $\Delta$mIoU & $\Delta$mIoU & $\Delta$mIoU & $\Delta$mIoU \\
\midrule
\multirow{2}{*}{DeepLabV3} & ResNet-50 & 78.05 & 0.001\mypm{0.001} & 0.02\mypm{0.04} & 0.02  & 3.06 & 0.01  & 4.02 & 4.51 \\
& ResNet-101 & 79.88 & 0.001\mypm{0.001} & 0.01\mypm{0.02} & 0.02  & 3.85 & 0.01  & 4.65 & 5.11 \\
\midrule
\multirow{1}{*}{U-Net} & - & 61.98 & 0.003\mypm{0.005} & 0.04\mypm{0.06} & 0.04  & 2.74 & 0.02  & - & 2.85 \\
\bottomrule
\end{tabular}
}
\label{tab_seg}
%\vspace{-0.15in}
\end{table*}


\begin{table}[htbp]
%\vspace{-0.1in}
\centering
\caption{\textbf{Measuring SysNoise on Multiple NLP Datasets.} We record ACC on FP32 data precision and the difference $\Delta$ACC = ACC$_{\mathrm{original}}$-ACC$_{\mathrm{SysNoise}}$ on FP16 and INT8 data precision. \emph{The lower} $\Delta$ACC \emph{the better}. }
\adjustbox{max width=\textwidth/2}{
\begin{tabular}{l c c c c}
\toprule
\multicolumn{1}{l}{\textbf{Architecture}} & \multicolumn{1}{c}{\textbf{PIQA}} & \multicolumn{1}{c}{\textbf{LAMBADA}} & \multicolumn{1}{c}{\textbf{HellaSwag}}  & \multicolumn{1}{c}{\textbf{WINOGRANDE}} \\
\cmidrule(l{2pt}r{2pt}){2-5} 
\multicolumn{1}{c}{} & \multicolumn{4}{c}{FP32(ACC)/FP16($\Delta$ACC)/INT8($\Delta$ACC)} \\
% & & mIoU &  $\Delta$mIoU & $\Delta$mIoU & $\Delta$mIoU & $\Delta$mIoU & $\Delta$mIoU & $\Delta$mIoU & $\Delta$mIoU \\
\midrule
OPT-125M & 63.00/0.05/-0.06 & 37.90/0.04/0.37 & 29.20/0.01/0.15 & 50.28/0.00/-0.31 \\
OPT-350M & 64.36/-0.11/-0.33 & 45.16/0.00/-0.10 & 32.04/0.00/0.05 & 52.33/-0.08/0.24 \\
OPT-1.3B & 71.71/-0.05/0.16 & 58.06/0.07/0.19 & 41.45/-0.03/0.08 & 59.67/0.00/-0.24 \\
OPT-2.7B & 73.78/0.06/0.16 & 63.65/0.09/0.02 & 45.85/-0.01/0.01 & 61.01/0.00/0.00 \\
OPT-6.7B & 76.06/0.16/0.22 & 67.61/0.06/0.33 & 50.46/0.01/0.03 & 65.04/0.00/0.48 \\
OPT-13B & 75.90/0.11/0.06 & 68.72/0.07/0.29 & 52.44/-0.02/0.01 & 65.11/0.00/0.39 \\
OPT-30B & 77.69/0.11/0.16 & 71.47/0.02/0.00 & 54.29/-0.01/0.01 & 68.19/0.02/0.23 \\
\bottomrule
\end{tabular}
}
\label{tab_nlp}
%\vspace{-0.15in}
\end{table}



\textbf{Impact from multiple SysNoise.}
The single noise type may only have limited impacts on task performance. However, it is likely that SysNoise will happen in multiple stages during inference, and will have a combined effect with multiple noises to bring further influences on the accuracy.

We show how combined SysNoise affects a single model step by step in \autoref{fig_wc}. For example, on ResNet-50, we select the most influential SysNoise type and gradually add them to impose noise coherently. As shown in \autoref{fig_wc}, we show that some SysNoise is lessened while others are strengthened when combined together. For instance, adding resize to ResNet-50 incurs 0.71\% extra accuracy loss which is even lower compared to the average 0.75\% accuracy loss in \autoref{tab_cls}. On the contrary, the INT8 quantization increases its damage from 0.06\% to 1.09\%. This reveals two discoveries. First, different types of pre-processing noise can overlap with each other. Second, model inference noise might be magnified with other noises. We will provide more in-depth future studies. Interestingly, we show that the impact from SysNoise can be magnified especially in detection tasks whose model has ceil mode and upsample noise together. We deduce that this may be because they are both noises about the relative position and value of the model's feature map and the superposition of these two noises can cause effects beyond their own noise.

In \autoref{tab_cls}, \autoref{tab_det} and \autoref{tab_seg}, we show how combined SysNoise affects different models on different model architecture.
As shown in \autoref{tab_cls} and \autoref{tab_det}, adding all SysNoise to ResNet-50 together can damage 3.95\% accuracy for classification and 10.67\% mAP for detection, which equals degenerating a ResNet-50 lower than ResNet-34. Adding all SysNoise to EfficientNet-B4 makes it lower than the B3 variant. According to the original paper~\cite{tan2020efficientnet}, B4 consumes 2.3$\times$ more FLOPs than B3 and 1.6$\times$ higher parameters, yet only 1.4\% accuracy improvement. However, SysNoise can easily make the architecture improvement useless, with up to 2.3\% accuracy degradation to EfficientNet-B4.


\begin{figure}[htbp]
    \centering
    \subfigure[ResNet-50 ImageNet]{\includegraphics[width = 0.48\linewidth]{figure/worst_case_imagenet.pdf}} 
    \subfigure[Faster-RCNN ResNet-50 COCO]{\includegraphics[width = 0.5\linewidth]{figure/worst_case_coco.pdf}}
    
    \caption{\textbf{Illustration of the worst-case study by combining multiple SysNoise types step by step}.}
    \label{fig_wc}
\end{figure}




% Figure 3 comment out
% \begin{figure}[t]
% \centering
% \subfigure[Classification]{\includegraphics[width = 0.48\linewidth]{figure/diff_worse_classification.pdf}} 
% \subfigure[Detection]{\includegraphics[width = 0.24\linewidth]{figure/diff_worse_det.pdf}}
% \subfigure[Segmentation]{\includegraphics[width = 0.24\linewidth]{figure/diff_worse_seg.pdf}}

% \caption{\textbf{}.\yh{Figure font size too small.}} 
% \label{Fig:Fail_Example}
% \end{figure}






% We can observe that the influence order of SysNoise on detection models is Post-processing$>$Upsample$>$Resize$>$Color Mode$>$INT8$>$Ceil Mode$\approx$Decode. 

% \subsection{Evaluating SysNoise}



% \begin{figure}[t]
%     \centering
%     \subfigure[ResNet-50 ImageNet]{\includegraphics[width = 0.48\linewidth]{figure/worst_case_imagenet.pdf}} 
%     \subfigure[ResNet-50 COCO]{\includegraphics[width = 0.48\linewidth]{figure/worst_case_coco.pdf}}
    
%     \caption{\textbf{Illustration of the worst-case study by combining multiple SysNoise types}.}
%     \label{fig_wc}
% \end{figure}



% \vspace{-0.15in}
\subsection{Interpreting SysNoise}

\label{sec_interp}

\begin{figure*}[htbp]
%\vspace{-0.25in}
    \centering
    \includegraphics[width=0.95\linewidth]{figure/aug_adv.pdf}
    \caption{\textbf{Illustration of data augmentations and adversarial training for SysNoise on ImageNet}. }
    \label{fig_aug_adv}
    % \vspace{-0.15in}
\end{figure*}

\begin{figure*}[htbp]
%\centering
\adjustbox{margin=1em,width=\textwidth,set height=0.4cm,set depth=0.1cm,center}{Original Image\ \ \ \ \ \ \ \ \ \ \ \ \ Decode\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ Resize\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ Color Mode\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ INT8\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ Ceil Mode\ \ }
\subfigure{
\includegraphics[width=.15\linewidth]{figure/diff_figure/ILSVRC2012_val_00046286_or.png}
}
\subfigure{
\includegraphics[width=.15\linewidth]{figure/diff_figure/ILSVRC2012_val_00046286_ppb_opb.png}
}
\subfigure{
\includegraphics[width=.15\linewidth]{figure/diff_figure/ILSVRC2012_val_00046286_ppb_yuv.png}
}
\subfigure{
\includegraphics[width=.15\linewidth]{figure/diff_figure/ILSVRC2012_val_00046286ppb_pon.png}
}
\subfigure{
\includegraphics[width=.15\linewidth]{figure/diff_figure/ILSVRC2012_val_00046286_gapint8.png}
}
\subfigure{
\includegraphics[width=.15\linewidth ]{figure/diff_figure/ILSVRC2012_val_00046286_ceildif.png}
}
\\
\vskip -15pt
\subfigure{
\includegraphics[width=.15\linewidth]{figure/diff_figure/ILSVRC2012_val_00006155_or.png}
}
\subfigure{
\includegraphics[width=.15\linewidth]{figure/diff_figure/ILSVRC2012_val_00006155_ppb_opb.png}
}
\subfigure{
\includegraphics[width=.15\linewidth]{figure/diff_figure/ILSVRC2012_val_00006155_ppb_yuv.png}
}
\subfigure{
\includegraphics[width=.15\linewidth]{figure/diff_figure/ILSVRC2012_val_00006155ppb_pon.png}
}
\subfigure{
\includegraphics[width=.15\linewidth]{figure/diff_figure/ILSVRC2012_val_00006155_gapint8.png}
}
\subfigure{
\includegraphics[width=.15\linewidth ]{figure/diff_figure/ILSVRC2012_val_00006155_ceildif.png}
}
\\
\vskip -15pt
\subfigure{
\includegraphics[width=.15\linewidth]{figure/diff_figure/ILSVRC2012_val_00025871_or.png}
}
\subfigure{
\includegraphics[width=.15\linewidth]{figure/diff_figure/ILSVRC2012_val_00025871_ppb_opb.png}
}
\subfigure{
\includegraphics[width=.15\linewidth]{figure/diff_figure/ILSVRC2012_val_00025871_ppb_yuv.png}
}
\subfigure{
\includegraphics[width=.15\linewidth]{figure/diff_figure/ILSVRC2012_val_00025871ppb_pon.png}
}
\subfigure{
\includegraphics[width=.15\linewidth]{figure/diff_figure/ILSVRC2012_val_00025871_gapint8.png}
}
\subfigure{
\includegraphics[width=.15\linewidth ]{figure/diff_figure/ILSVRC2012_val_00025871_ceildif.png}
}
\\
\vskip -15pt
\subfigure{
\includegraphics[width=.15\linewidth]{figure/diff_figure/ILSVRC2012_val_00033432_or.png}
}
\subfigure{
\includegraphics[width=.15\linewidth]{figure/diff_figure/ILSVRC2012_val_00033432_ppb_opb.png}
}
\subfigure{
\includegraphics[width=.15\linewidth]{figure/diff_figure/ILSVRC2012_val_00033432_ppb_yuv.png}
}
\subfigure{
\includegraphics[width=.15\linewidth]{figure/diff_figure/ILSVRC2012_val_00033432ppb_pon.png}
}
\subfigure{
\includegraphics[width=.15\linewidth]{figure/diff_figure/ILSVRC2012_val_00033432_gapint8.png}
}
\subfigure{
\includegraphics[width=.15\linewidth ]{figure/diff_figure/ILSVRC2012_val_00033432_ceildif.png}
}
\caption{\textbf{Visualization of SysNoise. To make the noise more perceptible, we scale it to [0, 255]}.}
\label{fig:vis}
%\vspace{-0.15in}
\end{figure*}


\textbf{Does data augmentation improve model robustness against SysNoise?} %Deep neural networks often fail to generalize the unseen data, whose special case can be SysNoise.
Studies have shown that data augmentation techniques can be used to improve model robustness against natural noises~\cite{hendrycks2020augmix,hendrycks2021nae}. We, therefore, employ these augmentation methods to see whether they could also improve the robustness against SysNoise. In particular, we train a ResNet-50 model using standard augmentation~\cite{DBLP:journals/corr/HeZRS15}, APR-SP~\cite{chen2021amplitude}, Deepaug~\cite{hendrycks2020many}, AugMix~\cite{hendrycks2020augmix}, and Deepaug combined with the other two methods (denoted ``Deepaug+APR-SP'' and ``Deepaug+AugMix''). As shown in \autoref{fig_aug_adv}, we could reach several observations as follows (1) \emph{there exist no single data augmentation methods that could universally achieve positive effects on all of the five different SysNoise types}; (2) specifically, data augmentations could improve model robustness against image decoder, the ceiling mode of the max-pooling layers (lower $\Delta$ACC). However, they fail to generalize for data precision and image resize (higher $\Delta$ACC). These indicate that SysNoise is highly diverse and inherently different from natural noises.



\textbf{Does adversarial training improve model robustness against SysNoise?}
Besides natural noises, another axis to analyze SysNoise is to examine whether adversarially-robust models could be also effective against SysNoise. Here, we use adversarial training (the most effective method to defend adversarial noises)~\cite{PGD}, and adversarially train ResNet-50 and RegNetX-3.2GF models with $\ell_{\infty}$-PGD attacks~\cite{PGD} using the standard setting~\cite{tang2021robustart,croce2020robustbench}. The results are summarized in \autoref{fig_aug_adv}, from which we can tell that \emph{adversarial training has limited effect on improving model robustness against SysNoise} (significantly higher $\Delta$ACC on 80\% SysNoise types). In some cases, like image decode and resize, adversarial training even significantly damages the model performance on SysNoise (significantly high $\Delta$ACC). Together with the data augmentation analysis, we show that SysNoise differs from both natural noises and adversarial noise, and the effectiveness of defenses that are designed for adversarial and natural noises is limited for SysNoise. We hope all these observations could inspire more in-depth future studies on building robust models against SysNoise.

\textbf{Does test-time adaptation improve model robustness against SysNoise?}
Different from data augmentation and adversarial training that try to solve issues during the training process, test-time adaptation tries to solve data shifts problem during the testing process. A fully test-time adaptation method called TENT \cite{tent} was proposed which is taken effect by minimizing the entropy of model predictions during model inference. Experiments were carried out to find whether test-time adaptation improves model robustness against SysNoise. And results are shown in \autoref{tab_tent}, from which we can tell that TENT harms the model robustness against SysNoise, except the ViT model zoo on color mode noise. It may be because the data shifts caused by SysNoise are so small compared with that caused by other corruption mentioned in this paper that test-time adaptation harms the performance of the model.


\begin{table}[t]
%\vspace{-0.25in}
\centering
\caption{\textbf{ Measuring SysNoise on models with/without TENT.} We record Top-1 accuracy and the difference, $\Delta$ACC = ACC$_{\mathrm{original}}$-ACC$_{\mathrm{SysNoise}}$. We report both mean and max $\Delta$ACC for decode and resize. \emph{The lower} $\Delta$ACC \emph{the better.}}
\adjustbox{max width= \textwidth/2}{
\begin{tabular}{l c c c c c c c ccccc}
\toprule
\textbf{Architecture} & \multicolumn{1}{c}{\textbf{Trained}} & \multicolumn{1}{c}{\textbf{Decode}} &   \multicolumn{1}{c}{\textbf{Resize}} & \multicolumn{1}{c}{\textbf{Color Mode}}\\  
& ACC &  $\Delta$ACC & $\Delta$ACC & $\Delta$ACC  \\
\midrule
% ResNet18x0.125 & 33.62 & 2.10\mypm{5.86E-01} & 2.26\mypm{1.21E-02} & 0.15  & -0.01 & 1.16  & 2.97 \\
MCUNet-293KB(w/o TENT) & 63.40 & 0.41\mypm{0.42} & 4.02\mypm{9.31} & 0.20 \\
MCUNet-293KB(w/ TENT) & 63.40 & 5.03\mypm{7.25} & 4.76\mypm{6.22}  & 0.95\\
\midrule
ResNet-18(w/o TENT) & 69.96 & 1.02\mypm{1.03} & 1.01\mypm{2.05} & 0.13 \\
ResNet-18(w/ TENT) & 69.96 & 4.01\mypm{4.22} & 3.33\mypm{4.06} & 2.16 \\
ResNet-34(w/o TENT) & 73.59 & 0.99\mypm{1.00} & 0.77\mypm{1.67} & 0.14  \\
ResNet-34(w/ TENT) & 73.59 & 4.02\mypm{4.39} & 2.99\mypm{3.51} & 1.89  \\
ResNet-50(w/o TENT) & 76.39   & 0.98\mypm{0.98} & 0.75\mypm{1.69} & 0.09 \\
ResNet-50(w/ TENT) & 76.39   & 4.81\mypm{5.44} & 4.67\mypm{5.41} & 3.78 \\
\midrule
MobileNetV2-0.5(w/o TENT) & 64.94 & 1.98\mypm{2.00} & 2.04\mypm{3.14} & 0.18  \\
MobileNetV2-0.5(w/ TENT) & 64.94 & 7.30\mypm{8.24} & 3.67\mypm{4.76} & 1.04  \\
MobileNetV2-1(w/o TENT) & 73.12 & 1.39\mypm{1.39} & 1.48\mypm{2.43} & 0.07  \\
MobileNetV2-1(w/ TENT) & 73.12 & 5.58\mypm{6.09} & 2.26\mypm{3.07} & 0.77  \\
\midrule
ViT-Tiny(w/o TENT) & 75.61 & 1.04\mypm{1.04} & 0.99\mypm{1.79} & 0.46 \\
ViT-Tiny(w/ TENT) & 75.61 & 1.38\mypm{1.57} & 1.16\mypm{1.81} & 0.28 \\
Vit-Base(w/o TENT) & 84.63 & 0.61\mypm{0.62} & 0.43\mypm{0.74} & 0.93 \\
Vit-Base(w/ TENT) & 84.63 & 1.71\mypm{1.91} & 1.07\mypm{1.37} & 0.90 \\
\midrule
Swin-Tiny(w/o TENT) & 81.32 & 0.18\mypm{0.19} & 0.42\mypm{1.76} & 1.21  \\
Swin-Tiny(w/ TENT) & 81.32 & 7.32\mypm{9.11} & 3.68\mypm{4.95} & 2.28  \\
Swin-Base(w/o TENT) & 83.54 & 0.11\mypm{0.30} & 0.21\mypm{1.27} & 0.97 \\
Swin-Base(w/ TENT) & 83.54 & 5.98\mypm{6.57} & 3.43\mypm{4.47} & 2.68 \\
\bottomrule
\end{tabular}
}
\label{tab_tent}
%\vspace{-0.15in}
\end{table}


%\textbf{Does learning-based decoder improve model robustness against SysNoise?}



\textbf{Potential methods to improve robustness against SysNoise.}
To solve SysNoise on the decoder and resize, a natural way is to make the model "see" all kinds of decoders and resize methods during the training process. Based on this principle, we introduce \textit{mix training} method to enhance the model's robustness on system noise. The main process of mix training is to select the decoder or resize method randomly instead of just using one kind of method during the whole process of training. The pseudocode of our algorithm is shown in
\autoref{algo1}.

To test the effect of mix training, we set up the following experiment. We use ResNet50 as the base model of this experiment.
To comprehensively demonstrate the training effect, we train single decoding and resize as well as our mix training models. 
We set the default decoder as Pillow and the default resize method as Pillow bilinear when conducting ablation studies on resizing method or decoder, respectively. Then we use top-1 accuracy as well as their mean and standard deviation as assessments.
The results of this experiment are shown in \autoref{tab:mix_decoder} and \autoref{tab:mix_resize}. From these tables, we can conclude that: (1) The model has a better performance (usually the best) when we train and test using the same decoder and resize method. (2) Mix training can improve the robustness of a model on system noise greatly without hurting the clean accuracy. The $Std.$ using mix training drop from $0.36$ to $0.0653$ on decoder experiment, and drop from $0.463$ to $0.270$ on resize experiment. Meanwhile, it can maintain the model's accuracy at about 76\%. In a contrast, the same ResNet50 model using $L_{\infty}-Robust$ adversarial training drops the $Std.$ from $1.07$ to $0.420$ by paying a $19.2\%$ drop of clean accuracy.

\begin{table*}[h]
\caption{Mix training on resize method.}
\label{tab:mix_resize}
\adjustbox{max width=\textwidth}{
\begin{tabular}{l|llllll|ll}
\toprule
\diagbox{Train}{Test} & Pillow-bilinear & Pillow-nearest & Pillow-cubic & OpenCV-nearest & OpenCV-bilinear & OpenCV-cubic & Mean & Std. \\
\midrule
Pillow-bilinear  & 76.572 & 72.168  & 76.512  & 72.090          & 75.346      & 74.072    & 74.460 & 2.02E+00 \\
Pillow-nearest               & 74.872       & 75.988      & 75.548    & 75.970          & 76.002          & 76.056       & 75.739	& 4.63E-01 \\
Pillow-cubic                 & 76.312       & 72.828      & 76.596    & 72.876         & 75.810           & 74.666       & 74.848	& 1.68E+00 \\
OpenCV-nearest            & 74.818       & 76.298      & 75.474    & 76.092         & 76.082          & 76.192       & 75.826	& 5.71E-01\\
OpenCV-bilinear           & 75.840        & 75.268      & 76.446    & 75.248         & 76.682          & 76.436       & 75.987 & 6.29E-01 \\
OpenCV-cubic              & 76.194       & 72.812      & 76.510     & 72.940          & 75.736          & 74.818       & 74.835 & 1.62E+00 \\
mix                       & 76.154       & 75.876      & 76.344    & 75.786         & 76.444          & 76.330        & \textbf{76.156} & \textbf{2.70E-01} \\
\bottomrule
\end{tabular}
}
\end{table*}




\begin{algorithm}[h]
\caption{Mixed training for improving robustness on systematic noise.}
\label{algo1}
\KwIn{Resize set $\mathbb{RS}$; Decoder set $\mathbb{D}$; Model to train}
Set Pillow-bilinear as default Resize; \\
Set Pillow as default Decoder; \\
\For{all $j=1,2,\dots,T$-iteration in training}
{
    \If{use mix-decoder strategy}{
        Randomly sample a Decoder from $\mathbb{D}$;
    }
    \If{use mix-resize strategy}{
        Randomly sample a Resize function from $\mathbb{RS}$;
    }
    Load the images from the file system according to the Decoder type and Resize type; \\
    Model Optimization.
}    
\KwRet{An optimized robust model for systematical noise.}

\end{algorithm}




\begin{table}[h]
\centering
\caption{Mix training on the decoder.}
\label{tab:mix_decoder}
\adjustbox{max width=0.48\textwidth}{
\begin{tabular}{l|ccc|cc}
\toprule
\diagbox{Train}{Test} & Pillow            & OpenCV          & FFmpeg          & Mean & Std.               \\
\midrule
Pillow  & 76.430  & 76.426 & 75.310 & 76.055 & 6.45E-01          \\
OpenCV  & 76.510   & 76.510  & 75.368   & 76.126 & 6.56E-01   \\
FFmpeg & 75.730  & 75.664 & 76.318 & 75.904 & 3.60E-01  \\
mix  & \textbf{76.53} & \textbf{76.524}  & \textbf{76.414} & \textbf{76.489} & \textbf{6.53E-02}\\
\bottomrule
\end{tabular}
}
\end{table}



\textbf{Visualization.} Here, we visualize the SysNoise by showing the \emph{difference in pixels}. In specific, we calculate the differences between the clean image (or feature) and corrupted ones using SysNoise. As shown in \autoref{fig:vis}, we can draw several interesting observations as follows. For the decode noise, it seems to be irregular (totally random or centered around the edge). As for resize and color mode noise, we observe that the differences often appear in edges or corners of an object (\ie, shape). Specifically, Resize noise tends to mismatch in the red channel while color mode noise mismatches in all 3 channels. For Ceil Mode noise, it injects two bands of noises at the bottom right of the image. There is no obvious pattern for the INT8 noise.


% Acknowledgements should only appear in the accepted version.
%\section*{Acknowledgements}