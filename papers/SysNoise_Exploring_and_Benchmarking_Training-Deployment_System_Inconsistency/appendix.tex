\section{Mathematical Difference of SysNoise}
\label{appendix_math}
In this section, we try to use equations to describe how different processing, operations are formulated. Note that our explanation might not be exactly the same with third-party implementations, as there are always some hyper-parameters to determine. Our goal is to provide an intuition rather than a strict comparison. 

\textbf{\emph{Image Decode.}} In the decoding process, the inverse discrete cosine transform (iDCT) occupies the majority of the computation. Given a transformed matrix $\hat{\rmX}$ with shape $N\times N$ (excluding channels), the original image $\rmX$ at coordinates $(m, n)$ can be given by
\begin{equation}
\begin{split}
  f[m,n]=\sum_{k=0}^{N-1}\sum_{l=0}^{N-1}&\alpha(k)\alpha(l)F(k,l)\\
  &cos[\frac{(2m+1){\pi}k}{2N}]cos[\frac{(2n+1){\pi}l}{2N}] \\
\end{split}
\end{equation}
where,
\begin{equation}
  \alpha(k)= \begin{cases}
  \sqrt{\frac{1}{N}} \, \text{  if  } \, k = 0 \\
  \sqrt{\frac{2}{N}} \, \text{  if  } \, k \neq 0
  \end{cases}.
\end{equation}
The iDCT costs a lot of operations and some implementations choose to utilize Fast DCT and Fast iDCT~\cite{chen1977fast} where the computation is sped up by matrix decomposition. Due to its complexity, we do not display the equations here. Note that the de-quantization in decode will also bring different values, which will be introduced in the data precision section. 

\textbf{\emph{Resize Interpolation.}} Formally, considered an image $\rmX$ to be resized where a pixel in some position needs to be predicted and yet its neighbors are already known or predicted. Different interpolation algorithms rely on different functions to determine the unknown pixel. (1) Nearest interpolation, this method simply copy the nearest neighbor's pixel value, \ie, the neighbor with the lowest Euclidean distance, given by $\rmX[\arg\min_{x, y}((x-x^{\prime})^2 +(y-y^{\prime})^2 )]$. Here, the $x, y$ is the coordinates of the known neighbor and $x^{\prime}, y^{\prime}$ is the coordinates of the pixel that needs to be determined. (2) Bilinear interpolation, determines the pixel by linearly calculating the ratio of distance. Assume we have four spatially-close coordinates:  $Q_{11} = (x1, y1)$, $Q_{12} = (x1, y2)$, $Q_{21} = (x2, y1)$, and $Q_{22} = (x2, y2)$. Their values are already know, for example $f(Q_{11})$. The formulation of bilinear interpolation is given by:
\begin{equation}
    f(x, y) = \frac{y_2 - y}{y_2 - y_1}f(x, y_1) + \frac{y - y_1}{y_2 - y_1}f(x, y_2),
\end{equation}
where,
\begin{align}
    f(x, y_1) &= \frac{x_2 - x}{x_2 - x_1}f(Q_{11}) + \frac{x - x_1}{x_2 - x_1}f(Q_{21}), \notag \\
    f(x, y_2) &= \frac{x_2 - x}{x_2 - x_1}f(Q_{12}) + \frac{x - x_1}{x_2 - x_1}f(Q_{22}). \notag
\end{align}
(3) Bicubic interpolation, in contrast to the bilinear interpolation which only takes 4 pixels ($2\times 2$), the bicubic interpolation takes 16 pixels ($4\times 4$). The algorithm tries to use existing known pixel values to fit a binary cubic function
\begin{equation}
    f(x, y) = \sum_{i=0}^3 \sum_{j=0}^3 a_{ij}x^iy^j
\end{equation}
To find the total 16 coefficients $a_{ij}, ij\in\{0,1,2,3\}$, we need to solve a system of linear equations $A\alpha=x$. Due to the complexity of this algorithm, we refer the readers to this link\footnote{\url{https://www.ece.mcmaster.ca/~xwu/interp_1.pdf}} for more details. Bicubic interpolation yields better performance than the previous two algorithms, however, it also needs huge time to solve the linear equations to find optimal interpolated values. We omit other interpolations methods as they are more complex that these three methods. 

\textbf{\emph{YUV color mode.}} As a matter of fact, there are tons of encoding standards for YUV color space. The formats described here all use 8 bits per pixel location to encode the Y channel (also called the luma channel), and use 8 bits per sample to encode each U or V chroma sample. However, most YUV formats use fewer than 24 bits per pixel on average, because they contain fewer samples of U and V than of Y. The full-size YUV (32 bits per pixel) is represented as 4:4:4, which means no downsampling of chroma channels. Following BT.601~\cite{rec1993bt}, converting RGB to YUV 4:4:4 can be formulated by 

\begin{equation}
\resizebox{0.9\linewidth}{!}{$
\left\{
\begin{aligned}
Y & = \mathrm{round}( 0.256788 \times R + 0.504129 \times G + 0.097906 \times B) +  16 \\ 
U & = \mathrm{round}(-0.148223 \times R - 0.290993 \times G + 0.439216 \times B) + 128 \\
V & = \mathrm{round}( 0.439216 \times R - 0.367788 \times G - 0.071427 \times B) + 128
\end{aligned}
\right..$}
\end{equation}


% \begin{equation}
% \adjustbox{max width=\linewidth}{
% \left\{
% \begin{aligned}
% Y & = \mathrm{round}( 0.256788 \times R + 0.504129 \times G + 0.097906 \times B) +  16 \\ 
% U & = \mathrm{round}(-0.148223 \times R - 0.290993 \times G + 0.439216 \times B) + 128 \\
% V & = \mathrm{round}( 0.439216 \times R - 0.367788 \times G - 0.071427 \times B) + 128
% \end{aligned}
% \right
% }

% \end{equation}


Here, we can derive an inverse transform from YUV to RGB, 

\begin{equation}
\resizebox{0.9\linewidth}{!}{$
\left\{
\begin{aligned}
R & = \mathrm{clip}(\mathrm{round}( 1.164383 * C                   + 1.596027 * E  ) ) \\
G & = \mathrm{clip}(\mathrm{round}( 1.164383 * C - (0.391762 * D) - (0.812968 * E) ) ) \\
B & = \mathrm{clip}(\mathrm{round}( 1.164383 * C +  2.017232 * D                   ) ) 
\end{aligned}
\right., \text{where  } 
\left\{
\begin{aligned}
C & = Y - 16 \\ 
D & = U - 128 \\ 
E & = V - 128 \\
\end{aligned}
\right..
\label{eq_yuv2rgb}
$}
\end{equation}

Here, $\mathrm{clip}(\cdot)$ denotes clipping to a range of $[0, 255]$. In some implementation~\cite{wiki_yuv}, \autoref{eq_yuv2rgb} can be approximated by:
\begin{equation}
\resizebox{0.9\linewidth}{!}{$
\left\{
\begin{aligned}
R & = \mathrm{clip}(( 298 * C           + 409 * E + 128) >> 8) \\
G & = \mathrm{clip}(( 298 * C - 100 * D - 208 * E + 128) >> 8) \\
B & = \mathrm{clip}(( 298 * C + 516 * D           + 128) >> 8)
\end{aligned}
\right..
$}
\end{equation}
As we could see, the conversion cannot be lossless with the existence of rounding and clipping operations, which could be generally summarized to \emph{quantization-dequantization} conversion. In addition, usually, the hardware supports YUV 4:2:0 rather than 4:4:4, making the conversion to RGB more unstable cause YUV 4:2:0 should be transformed to YUV 4:4:4 and then transformed to RGB format~\cite{wood2005rec}.  


\textbf{\emph{Ceiling mode.}} For pooling layers, the output shape of the feature map is calculated by
\begin{equation}
    O = \left\lfloor \frac{W-K+2P}{S} \right\rfloor + 1,
    \label{eq_ceilmode}
\end{equation}
where $W$ is the width (we assume the feature map is square), $K$ is the kernel size, $P$ is the padding size, and $S$ is the stride of pooling layers. The above equation uses floor operation $\lfloor \cdot \rfloor$ to compute the size of the output feature while we can use ceiling operation $\lceil \cdot \rceil$ operation in ceiling mode. Therefore, the border of the output feature is dependent on the ceiling mode. 

\textbf{\emph{Data Precision.}} We here discuss two types of precision: FP16 and INT8. The FP16 still uses floating-point numbers with less bitwidth. According to IEEE 754, the FP32 format uses 1 bit for sign, 8 bits for the exponent, and the rest 23 bits for fraction, while the FP16 uses 1 bit for sign, 5 bits for the exponent, and 10 bits for fraction. Normally, converting FP32 to FP16 only causes a negligible error, as shown in our experiments. For INT8, this is usually done by quantization and de-quantization functions:
\begin{align}
    \bar{\rmX} &= \mathrm{clip}\left(\lfloor\frac{\rmX}{s}\rceil +z, N_{min}, N_{max}\right) \\
    \hat{\rmX} &= s * (\bar{\rmX} - z),
\end{align}
where $\lfloor\cdot\rceil$ is the rounding-to-nearest function. $N_{min}, N_{max}$ are the range of integers that can be represented. For INT8, $N_{min}=-128$ and $N_{max}=127$. $s\in\mathcal{R}$ and $z\in\mathcal{Z}$ are the scale and zero point parameters to fit the original FP32 tensor's range. For more details of quantization, readers are recommended to~\cite{li2021mqbench}. 


\textbf{\emph{Post-processing. }}
For object detection, the post-processing involves multiple operations: 1. calculate the anchors, 2. get the offsets for anchors from the predicted outputs, 3. calculate the final bounding box. Some details of these operations are easy to bring the noise. Some details of these operations are easy to cause noise. The following code shows an example procedure for post-processing. For different hardware implementations, the $\mathbf{ALIGNED\_FLAG.offset}$ in the code often has different values of 0 or 1. This minor difference will bring a perturbation to the final accuracy performance. Besides, other operations like the rounding from float-point output to integer coordinate or the precision of exponential also need to be treated carefully.

\definecolor{codeblue}{rgb}{0.25,0.5,0.5}
\definecolor{codekw}{rgb}{0.85, 0.18, 0.50}
\begin{lstlisting}[language=python,basicstyle=\ttfamily, breaklines=true]
# anchors from xyxy format to xywh format
ctr_x, ctr_y, widths, heights = xyxy2xywh(boxes)

# normalize the offsets predicted from the neural network
means = offset.new_tensor(means).view(1, -1).repeat(1, offset.size(-1) // 4)
stds = offset.new_tensor(stds).view(1, -1).repeat(1, offset.size(-1) // 4)
offset = offset * stds + means

# calculate the delta of x, y, w and h
wx, wy, ww, wh = weights
dx = offset[:, 0::4] / wx
dy = offset[:, 1::4] / wy
dw = offset[:, 2::4] / ww
dh = offset[:, 3::4] / wh

dw = torch.clamp(dw, max=np.log(1000. / 16.))
dh = torch.clamp(dh, max=np.log(1000. / 16.))

# calculate the predicted coordinate of center point, 
# and the height & weight of bbox
pred_ctr_x = dx * widths[:, None] + ctr_x[:, None]
pred_ctr_y = dy * heights[:, None] + ctr_y[:, None]
pred_w = torch.exp(dw) * widths[:, None]
pred_h = torch.exp(dh) * heights[:, None]

# calculate the final bbox
pred_boxes = offset.new_zeros(offset.shape)
# x1
pred_boxes[:, 0::4] = pred_ctr_x - 0.5 * pred_w
# y1
pred_boxes[:, 1::4] = pred_ctr_y - 0.5 * pred_h
# x2
pred_boxes[:, 2::4] = pred_ctr_x + 0.5 * pred_w - ALIGNED_FLAG.offset
# y1
pred_boxes[:, 3::4] = pred_ctr_y + 0.5 * pred_h - ALIGNED_FLAG.offset
\end{lstlisting}



\section{Dose Learning-Based Decoder Improve Model Robustness against SysNoise?}
\label{learningbaseddecoder}
Different from the traditional image encoding/decoding method some work uses a learning-based image codec to minimize the gap between the original image and the encoded image. \cite{learningbased} introduce a learning-based image compression method, which achieves about 32.625dB for the CLIC2020 validation dataset. To explore whether the learning-based method can improve the model's robustness on SysNoise, we carried out experiments on the ImageNet dataset using the decoder trained on the CLIC2020 dataset. We used ResNet-50 as a base model, and compare it with the other 2 commonly used decoder methods in \autoref{tab:learn_based}. We can see that there is no obvious gain in using the learning-based decoder.


\begin{table}[htbp]
\centering
\caption{Compare Performance on Learning-Based Decoder}
\label{tab:learn_based}
\adjustbox{max width=0.48\textwidth}{
\begin{tabular}{l|ccc|cc}
\toprule
\diagbox{Train}{Test} & Pillow & OpenCV & Learning-Based & Mean & Std. \\
\midrule
Pillow  & 76.430  & 76.426 & 75.310 & 76.055 & 6.45E-01          \\
OpenCV  & 76.510   & 76.510  & 75.368   & 76.126 & 6.56E-01   \\
Learning-Based & 75.340  & 76.441 & 76.530 & 76.104 & 6.63E-01  \\
\bottomrule
\end{tabular}
}
\end{table}

\section{Preliminary Results for SysNoise on Text-to-Speech Task}
For evaluating SysNoise on text-to-speech tasks, we use FastSpeech 2 \cite{FastSpeech2} and Tacotron 2 \cite{Tacotron2} these two commonly used models. LJ Speech dataset \cite{ljspeech17}, which contains 13,100 English audio clips (about 24 hours) and corresponding text transcripts, was chosen for the training and testing process. Different from other text-to-speech work using MOS(mean opinion score) to evaluate audio quality, we use MSE(mean square error) since we pay more attention to the difference between the generated audio and the original audio under the influence of SysNoise. The result is shown in \autoref{tab_tts}.
From this result, we can tell that the text-to-speech task has a unique SysNoise when doing STFT(short-time Fourier transform). SysNoise introduced by different operators in STFT can harm the model's performance during model inference.

\begin{table}[htbp]
%\vspace{-0.1in}
\centering
\caption{\textbf{Measuring SysNoise on Text-to-Speech Taks.} We record MSE. \emph{The lower} MSE \emph{the better}. }
\adjustbox{max width=0.48\textwidth}{
\begin{tabular}{l c c c c}
\toprule
\textbf{Method} &\multicolumn{2}{c}{\textbf{Precision (FP16/INT8)}} & \multicolumn{1}{c}{\textbf{STFT}} & \multicolumn{1}{c}{\textbf{Combined}} \\
\cmidrule(l{2pt}r{2pt}){2-3} \cmidrule(l{2pt}r{2pt}){4-4} \cmidrule(l{2pt}r{2pt}){5-5}  
& MSE & MSE & MSE & MSE\\
\midrule
\multirow{1}{*}{FastSpeech 2} & 0.82 & 1.41 & 2.14 & 4.12 \\
\multirow{1}{*}{Tacotron 2} & 0.71 & 1.21 & 3.01 & 5.02 \\
\bottomrule
\end{tabular}
}
\label{tab_tts}
%\vspace{-0.15in}
\end{table}



\section{Broader Impacts and limitations}
\label{limitations}

Together with existing benchmarks on adversarial and natural noises, we could build a more comprehensive and general understanding and ecosystems for robustness benchmarking involving more perspectives. We hope this benchmark could draw the attention of both algorithm researchers and hardware vendors to this inevitable and urgent-to-solve problem, and open a new research direction for building robust deep learning deployment systems. 

Though having investigated several types of SysNoise in this paper, there may still exist other noises that would cause model performance degeneration during deployment. In the future, we will keep the benchmark growing.

%Our benchmark has several limitations, and we list them as follows. (1) We primarily evaluate SysNoise on image classification, object detection, and segmentation tasks in the first version of our benchmark, and we will continuously develop the benchmark to include more challenging tasks, such as point cloud. (2) We presented many intriguing insights based on our large-scale experiments, but have not dived into the nature behind. In the future, we will conduct more thorough studies based on these insights, and will keep the benchmark up-to-date to make the ecosystem grows.

\section{Consistency of Results}
\label{error bar}
To maintain consistency of results, we use following method. (1) Fix in the requirements torch==1.8.1, opencv==4.1.1.26 and Pillow==6.2.1 in our framework. (2) Set torch.backends.cudnn.benchmark=True in the code.
We test the ResNet-18 Model on all kinds of noise multiple times in our framework and observe little different result ($<0.0001\%$) on accuracy. This result also holds for object detection and instance segmentation task. So other factors are less likely to affect the results of the model inference process. 


\section{Reproducibility and Run Time}
\label{computing_resources}
We provide the code to run this benchmark on GitHub where everyone can download from freely. As for the setup steps and instructions about our code, we provided them in the README file. The installation instructions are also provided in the README file, users can easily install the required run time environment of this codebase. For some noises that need to be generated on specific hardware and are not easy to reproduce, we provide our own resulting datasets generated on specific hardware, which involve ImageNet validation set and COCO validation set. All these datasets can be freely downloaded on our website.


Since our benchmark experiments need us to train multiple models and evaluate them on different kinds of noises, it needs a large amount of GPU resources. The total cost of our GPU resources to build this benchmark is about 5 GPU years. Most of our experiments are run on  Nvidia Tesla V100 GPU. For one training experiment, we run it on 16 GPUs parallel. For inference experiments, we run it on 4 GPUs parallel.

For other users who just want to test their trained model with our framework, the GPU time they require will be greatly reduced. In most cases it only takes 10 to 40 minutes of GPU time to test the effect of one noise on one model, depending on the GPU type they are using.

\section{Future Work}
Based on the research conducted in this paper, our future work will focus on extending the SysNoise to other fields such as speech and audio. We will explore how SysNoise occurs in the different steps of the ML pipeline and benchmark it. We will keep updating our website and the final results will release on it at \url{https://modeltc.github.io/systemnoise_web}


\section{License}
\label{license}
Our code is released under Apache License 2.0. Most model architectures are added to the code with the license chosen by the original author. The ImageNet-1K, COCO, and CitySpace datasets we use are downloaded from the official release. Some system noise datasets we generated from the original dataset follow the license of its original dataset.
