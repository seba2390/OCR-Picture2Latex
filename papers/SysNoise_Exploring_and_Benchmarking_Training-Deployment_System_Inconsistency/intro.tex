\section{Introduction}



Deep neural networks have demonstrated remarkable success in handling multiple tasks~\cite{AlexNet,VGG,ResNet,devlin2018bert,GPT3}, yet they are vulnerable against noises. Despite the progress devoted to noises made by human-being or nature  (\eg, adversarial noises \cite{goodfellow2014explaining} and natural noises~\cite{hendrycks2019robustness}), little is known about model robustness on noises caused by different system implementations. In practice, the model deployment often faces diverse implementation platforms spanning from general (\eg, CPU, GPU) to specialized (\eg, NPU, ASIC) computing hardware; from the cloud server to edge devices; and often with different back-ends (\eg, TensorRT~\cite{tensorrt} for GPUs, SNPE~\cite{snpe} for DSPs, CANN~\cite{cann} for Ascend). These different software-hardware system implementations would bring certain noises resulting in considerable model performance degeneration. More importantly, these noises cannot be completely prohibited as long as a trained model will be deployed to multiple target platforms.

Thus, in this paper, we pioneeringly discuss an unwanted yet non-negligible type of noise caused by the inconsistency of the training-deployment system (see \autoref{fig_frontpage} for illustration), deemed as system noise~(\textit{abbrev. SysNoise}). Based on where SysNoise could happen, we classify it into three different types. \ding{192}~\textit{Pre-processing:} Depends on the implementation of input data. For example, different image decoding (JPEG2RGB) algorithms and different interpolation methods for image resize and crop. %Note that SysNoise caused by pre-processing can be viewed as another type of image corruption example, which lowers the model accuracy.
\ding{193}~\textit{Model Inference:} Caused by different implementations of the model during inference. For instance, models with the same parameters can have different results when the upsampling operator is different. Using different data types (INT8, FP16, FP32) also leads to different accuracy. \ding{194}~\textit{Post-processing:} Includes the further manipulation of inference results, \eg, applying softmax function in classification tasks and calculating the bounding box in detection tasks. Overall, SysNoise exhibits its impact on the whole inference pipeline, leading to an undesired performance drop.

\begin{figure*}[t]

    \centering
 %\vspace{-0.25in}
    \includegraphics[width=1.0\linewidth]{figure/frontpage.pdf}
    \caption{An illustration of SysNoise (a) and its negative effect on model robustness (b-d). Here we take noises from the decoder as an example. We usually use the DALI library from NVIDIA on GPU during training and the DVPP library from HUAWEI on Ascend during deployment for decoding acceleration, which results in minor decoding differences and would mislead the prediction.}
    \label{fig_frontpage}
    %\vspace{-0.2in}
        
\end{figure*}

To better understand and comprehensively evaluate the influence of SysNoise on the deployed model, we provide a thorough quantitative benchmark on 3 common computer vision tasks (\ie, classification, detection, and segmentation) with 20+ representative models and typical baselines. As for natural language processing, we provide a benchmark on OPT~\cite{opt} model on 4 datasets. Our large-scale experiments reveal several insights: (1) though these noises are not chosen by any adversary, SysNoise would bring considerable impacts on model robustness, and could cause up to $9.97\%$ and $10.67\%$ drops on classification and detection tasks respectively; (2) different architecture families induce different robustness on SysNoises (\eg, ViTs and CNNs), even in the same architecture family, a larger model tends to have low variance and low accuracy degradation on SysNoise; and (3) SysNoise seems to be highly diverse and different from adversarial and natural noises, where common mitigations like data augmentation and adversarial training show limited effects on it. Together with existing benchmarks on adversarial and natural noises, we could build a more comprehensive and general understanding and ecosystems for robustness benchmarking involving more perspectives. This benchmark for evaluating robustness to system noises provides useful information, and hopefully, it can open a new research direction for building robust deep learning deployment systems. 

In conclusion, our contributions can be summarized as threefold:
\begin{enumerate}[nosep, leftmargin=*]
\item For the first time, we identify and systematic research on an important problem named SysNoise (ranging from pre-processing, model inference, and post-processing noise), which is caused by the training-deployments system inconsistency. %The proposed SysNoise is orthogonal to other algorithmic problems, which inspires potential future research topics and engineering optimization.  
%\item To comprehensively study SysNoise, we classify it into three categories based on the scene where SysNoise takes place, namely pre-processing noise, model inference noise, and post-processing noise. In total, we discuss and evaluate 7 possibilities. 
\item We build a benchmark and framework to quantitatively evaluate SysNoise on 20+ deep neural networks, including image classification (ImageNet), detection (MS COCO), segmentation (CitySpace), and natural language processing. 

\item We conducted in-depth analyses and found several insights, which revealed that SysNoise is an inevitable and urgent-to-solve problem for both algorithm researchers and hardware vendors. 
\end{enumerate} 



%For example, extensive evidences have demonstrated that the deployed neural networks are highly vulnerable towards noises (\eg, adversarial noises \cite{goodfellow2014explaining} and natural noises~\cite{hendrycks2019robustness,hendrycks2021nae}), which would inevitably influence the reliability of model inference results.

%These adversarial noises are imperceptible to human but could easily mislead deep neural networks, thereby bringing potential security threats to practical deep learning applications. Although adversarial noises must be generated with specific model information such as gradients, there also exist some other model-agnostic natural noises. These noises such as snow and blur are not strangers in real-world scenarios, which present critical challenges for the building of strong neural networks. In the open world, these noises are often included in the input images, which would inevitably influence the reliability of model inference results.

%Researchers have demonstrated how vulnerable the deployed neural network can be when some adversarial noises, even imperceptible to human beings, are injected to the input image~\cite{goodfellow2014explaining}. Although the adversarial noise must be generated with the specific model information such as gradients calculated in back-propagation, there are some model-agnostic natural noise~\cite{hendrycks2019robustness,hendrycks2021nae}. When these natural corruptions happens in in real-world inference, the prediction performance of neural network degrades. Apart from this adversarial attack, the out-of-distribution~(OOD) detection is also important in model deployments. In an open-world, the inference results would not be reliable if the input data point comes from a distribution different from training. 
%To prevent the potential adversarial attack, training algorithms~\cite{ganin2016domain, tramer2017ensemble} are proposed.


%As we recollected some algorithmic issues in model deployment that influence model performance to a certain extent, we argue that issues caused by \textbf{systems} are overlooked in the literature. 

%Apart from these noises made by human-being or nature, in this paper, we for the first time indicate a new type of noise caused by the inconsistency of training-deployment system.
%Unlike the training process which relies on certain deep learning frameworks and hardware, the model deployment spans from general (\eg, CPU, GPU) to specialized (\eg, NPU, ASIC) computing hardware; from the cloud server to edge devices; and often with different back-ends (\eg, TensorRT~\cite{tensorrt} for GPUs, SNPE~\cite{snpe} for DSPs, CANN~\cite{cann} for Ascend). These different yet often neglected software-hardware system implementations would cause considerably heterogeneous model behaviors, resulting in performance degeneration in practice. 


