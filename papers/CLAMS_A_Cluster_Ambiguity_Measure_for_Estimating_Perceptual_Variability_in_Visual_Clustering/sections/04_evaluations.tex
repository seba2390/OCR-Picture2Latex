\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{figures/top_bottom_annot.pdf}
    \vspace{-5mm}
    \caption{The top eight \blue{clear} scatterplots (low cluster ambiguity; first row) and the top eight \red{ambiguous} scatterplots (high cluster ambiguity; second row) based on \measure score, which are picked within 60 scatterplots we used in the accuracy evaluation (\autoref{sec:mainstudy}). \vspace{-4mm}}
    \label{fig:topbottom}
\end{figure*}

\section{Quantitative Evaluation}

We conducted two evaluations to demonstrate the validity and effectiveness of \measure. We first  examined the performance of our regression module and the significance of its features through an ablation study (\autoref{sec:regmodeleval}). We then evaluated the accuracy (\autoref{sec:mainstudy}) of \measure by comparing it against both computational methods for estimating cluster ambiguity and human annotators. 

\subsection{Ablation Study for the Regression Module}

\label{sec:regmodeleval}

We report the results of the ablation study examining (1) the performance of our regression module (\autoref{sec:regmodel}) and (2) the significance of the features we designed (\autoref{tab:feature_eng}; \autoref{sec:feateng}).

\subsubsection{Study Design}

\textbf{Objectives and Design:}
The study aimed to achieve two objectives: (1) to investigate the accuracy of our regression module in estimating human-judged separability and (2) to analyze 
%the significance of each feature in contributing 
how much each feature contributed to the model performance (i.e., the significance of features). 
For the first objective, we evaluated the performance of our model as it is (i.e., used all features we discovered). To accomplish the second objective, we switched off each feature individually and examined the extent to which the performance deteriorated. Additionally, to take into account the interplay between features, we repeated the process by disabling feature pairs. 



\noindent \textbf{Measurement:}
We conducted a five-fold cross-validation to examine the performance of the model while using $\mathbf{X}$ and $\mathbf{S}$ as input and target, respectively. 
% For each combination of features, we ran the Auto-sklearn~\cite{feurer15nips} for 120 minutes to achieve the best performance.  
We used $R^2$ for the performance metric as it is interpretable~\cite{cameron97joe} and, moreover, unbiased if the number of points is fixed (as in our case).




% (2) We want to know the extent to which each feature contributes to the model performance (i.e., the significance of features). (3) We would like to seek how the features interact with each other. While the first and goal is to check the effectiveness of the features we designed, the second and third goal is to set to gain  broader insight into the factors of visual clustering (\autoref{sec:preexpresults}). To achieve the first goal, we evaluated the performance of our model as it is (i.e., used the entire features we discovered). For the second purpose, we switched off each of the features and examine the degradation of the performance, and for the third, we do so by pair of features. 


\subsubsection{Results and Discussions}

{\renewcommand{\arraystretch}{1.2}

\begin{table}[t]
    \centering
    \begin{subtable}[]{\linewidth}
        \resizebox{\linewidth}{!}{
        \begin{tabular}{rccccccc}
         \toprule
            & & \multicolumn{6}{c}{Excluded feature} \\
            \cmidrule{3-8}
            & \textit{orig.} & DC & DSR & DD & SD & ED & AC \\
            \midrule
            $R^2$ & .9106 & .9075  & .8574 & .9205 & .8790 & .8758 & .9019  \\ 
            Change & 0\% & -0.34\% &  -5.84\% & +1.08\% & -3.47\% & -3.82\% & -0.95\% \\  
         \bottomrule
        \end{tabular}
        }
        \caption{Model performance with entire (\textit{orig.}) features or without a single feature\vspace{1.5mm}}
    \end{subtable}
    \begin{subtable}[]{\linewidth}
        \resizebox{\linewidth}{!}{
        \begin{tabular}{rrcccccc}
         \toprule
            & & \multicolumn{6}{c}{Excluded feature} \\
            \cmidrule{3-8}
                                     &      & DC      & DSR     & DD      & SD      & ED      & AC     \\
            \midrule
            \multirow{6}{*}{\makecell{Excluded \\ Feature}} & DC   &         & .5817   & .8751   & .8917    & .8495   & .8333  \\
                                     & DSR  & -36.1\% &         & .8521   & .8985    & .8560   & .8902  \\
                                     & DD   & -3.89\% & -6.42\% &         & .8925    & .8790   & .8721  \\
                                     & SD   & -2.08\% & -1.33\% & -1.99\% &          & .8750   & .9041 \\
                                     & ED   & -6.71\% & -6.00\% & -3.47\% & -3.91\%  &         & .8888  \\
                                     & AC   & -8.49\% & -2.24\% & -4.23\% & -0.71\%  & -2.40\% & \\
         \bottomrule
        \end{tabular}
        }
        \caption{Model performance without a pair of features (in corresponding row and column)}
        
    \end{subtable}

    
    % \vspace{0.5mm}
    \caption{The result of an ablation study (\autoref{sec:regmodeleval}) analyzing the performance of a regression module estimating human-judged separability scores. We seek how the performance of the module varies as we remove a single (a) or a pair (b) of features, reporting the $R^2$ score (first row in (a), upper right half in (b)), and the change rate compared to the model using entire features (second row in (a), lower left half in (b)). }
    \label{tab:ablation}
\end{table}
}


\textbf{Model Performance:}
As in \autoref{tab:ablation} (a), the model achieved $R^2$ score over 0.9 with all features (which we mark as \textit{orig.}). 
This result validates the effectiveness of the features and also the reliability of our study-driven approach to designing features.

\noindent \textbf{Significance of the Features:}
According to \autoref{tab:ablation} (a), DSR caused the biggest degradation of the performance when removed. ED was next, and SD was third. Meanwhile, removing DC, DD, and AC made only a subtle decrement ($<1\%$) in the performance or even made it better, indicating that these three features barely explained visual clustering alone. 
However, we cannot 
%naively 
conclude that the bottom-three features (DC, DD, AC) have less significance than the top-three features (DSR, ED, SD) due to the existence of interplay between features (see \autoref{tab:ablation} (b)). For example, degradation resulting from switching off AC and DC together was substantially higher than the sum of degradation caused by switching off the features individually and was also higher than the largest degradation possible by removing a single feature (DSR). The same phenomenon occurs for ED and DC or DC and DSR. The interplay between DC and DSR was especially large compared to other combinations, validating the influence of the proximity factor in visual clustering. 
%Such results inform us that the study 
The significance of feature pairs suggests that future studies on visual clustering factors should focus on examining feature interplay in detail rather than analyzing features individually.

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{figures/amb_corr_bar.pdf}
    \vspace{-5mm}
    \caption{The comparison of the performance of \measure and human annotators (i.e., participants in our user study; \autoref{sec:mainstudydesc}) in estimating ground truth cluster ambiguity ranking.
             Each bar represents the rank correlation between the cluster ambiguity ranking made by the subjective response of a single participant and the ground truth ambiguity ranking.
             The orange and blue colors denote the participants who showed less and more accurate performance compared to \measure (red dashed line), respectively. 
             The purple dotted line depicts the average performance (i.e., rank correlation with ground truth ambiguity) of participants. 
             We found that \measure's performance is better than the average performance of participants but fails to outperform the performance made by the best annotator.\vspace{-4mm}
             }
    \label{fig:ambcoorbar}
\end{figure*}


\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/amb_rank_coor_annot.pdf}
    \caption{The rank correlation between ground truth ambiguity extracted based on five EVMs (\autoref{sec:groundtruthextract}) and the rankings made by computational ways to measure cluster ambiguity (\measure, variability of clustering techniques). Overall, \measure outperformed the competitors regardless of the used EVM, objectively having a strong correlation ($\rho > 0.6$) with ground truth~\cite{prion14csn} for the majority of the cases.  }
    \label{fig:ambrankcoor}
\end{figure}




\subsection{Accuracy Evaluation of \measure}

\label{sec:mainstudy}


% 
We evaluated the accuracy of \measure by (1) constructing the ground truth cluster ambiguity ranking of scatterplots and (2) checking how well the ranking made by our measure matched the ground truth. 
The ground truth ambiguity of scatterplots was estimated based on a user study with 18 participants. 
Our results showed that \measure precisely estimated ground truth cluster ambiguity, outperforming the tested alternatives (e.g., human annotations, variability of clustering techniques).

\subsubsection{User Study for Constructing Ground Truth Ambiguity}

\label{sec:mainstudydesc}

\noindent
\textbf{Objectives and Tasks:}
% \label{sec-mainstudy_task}
Our user study aimed to construct the ground truth cluster ambiguity of scatterplots. We formulated two experimental tasks relevant to our research questions.

\begin{itemize}[noitemsep] %\setlength
    \item[\textbf{(T1)}] Lasso the clusters in the given scatterplot using the mouse.
    \item[\textbf{(T2)}] Subjectively determine the ambiguity of the scatterplot.
\end{itemize}
T1 aims to directly collect the visual clustering results of participants so that we could 
%estimate their difference to 
compute ground truth cluster ambiguity. We additionally asked participants to conduct T2 to check how well the subjective ambiguity annotated by individuals matches the ground truth ambiguity compared to \measure.

\noindent
\textbf{Procedure:}
The entire study consisted of three phases. 
% i) Participants provided their informed consent. 
We first collected the participants' non-identifying demographics.
We then asked the participants to conduct the two tasks listed above, viewing 60 selected stimuli (i.e., scatterplots) in random order.
Finally, participants answered the post-study interview questions: (1) \textit{``Which characteristics of the scatterplots do you think are most important in determining separation?''} (2) \textit{``What makes scatterplots ambiguous or clear based upon your response on the shown scatterplots?''}



\noindent
\textbf{Datasets and Preprocessing:} Our primary consideration in generating scatterplot stimuli was maximizing the diversity of cluster patterns (related to C2 in \autoref{sec:decon}). For this purpose, we first generated a large number of scatterplots with a high diversity of patterns. This was done by applying eight dimensionality reduction (DR) techniques ($t$-SNE~\cite{maaten08jmlr}, UMAP~\cite{mcinnes2020arxiv}, Densmap~\cite{narayan2021assessing}, Isomap~\cite{tenenbaum00aaas}, LLE~\cite{roweis00science}, MDS~\cite{kruskal64psycho}, PCA~\cite{pearson01pmjs}, and Random Projection) to 96 high-dimensional datasets~\cite{jeon22arxiv}. Except for PCA and MDS, we generated 20 scatterplots while randomly adjusting the hyperparameters to further diversify the patterns. We obtained $(20 \cdot 6 + 2) \cdot 96  = 11,712$ scatterplots in total.

We consecutively applied a stratified sampling, following Pandey et al.~\cite{pandey2016towards} and Abbas et al.~\cite{abbas19cgf}, to 
%effectively 
%get rid of 
remove scatterplots with similar patterns~\cite{abbas19cgf}.  
To do so, we first grouped candidate scatterplots based on their number of clusters. We set the number as the optimal number of Gaussian components computed by GMM. We then computed Scagnostics~\cite{dang2014transforming} of scatterplots, representing each scatterplot as a vector consisting of Scagnostics scores (i.e., a vector abstracting the pattern of a scatterplot). Finally, we applied $K$-Means~\cite{likas03pr} to vectors representing each group of scatterplots and sampled scatterplots that were closest to centroids of resulting clusters. We 
set $K$ as 12. If the number of scatterplots in a group was less than 12, we sampled all scatterplots from the group.  
\rev{$K$ value is set to limit the number of sampled scatterplots to be around 100, making it possible to manually investigate each by eye.}
%As a result, 
114 scatterplots were sampled. 
We manually sampled these scatterplots to minimize similar patterns, resulting in the final 60 scatterplots. 


\noindent
\textbf{Participants:} 
\measure is designed for use in data analytics. We, thus, required participants to have experience in data analysis using scatterplots to ensure that they understood the concept of visual clustering. Based on snowball sampling \cite{goodman61ams}, we recruited 18 participants from the data visualization, human-computer interaction, and data mining communities (13 males and five females, aged 23-33 [27.2 $\pm$ 2.7]).
While 15 participants were graduate students, the remaining three were working professionals.
\rev{
Two participants self-reported as novice data analysts, two as intermediate, and 15 as experts. For familiarity with data analysis with scatterplots, 14 participants reported that they were at the expert level, and two participants said they were at an intermediate level. The remaining two participants reported themselves as novices.} 
We compensated each participant with the equivalent of \$20 for their time. 


\noindent
\textbf{Apparatus:}
The experiment was conducted over a recorded Zoom session.
We developed a website for the study participants.
Participants were asked to access the website and share their screens so that the instructor could monitor and guide the experiment. 
\rev{As with the factor exploration study (\autoref{sec:preexp}), we fixed the stimuli size to 700px $\times$ 700px and constrained participants to use a laptop or desktop monitor.}
The studies were 45-60 minutes long. We instructed the participants on tutorials and experimental tasks, and conducted interviews at the end. 



\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/qualitative_userstudy.pdf} \vspace{-6mm}
    \caption{We identified the factors that participants considered when lassoing clusters and rating the scatterplot on a scale of very ambiguous to very clear in our main study (\autoref{sec:mainstudy}). 
    The length of the filled bars and white bars depicts the ratio of the participants who denote that the corresponding factor affects cluster separability and ambiguity, respectively.
    As the past studies~\cite{sedlmair2012taxonomy, sadahiro1997cluster} have demonstrated, we observed density, proximity, and shape as main factors.}
    \label{fig:qual_study}
\end{figure}


\subsubsection{Extracting Ground Truth Cluster Ambiguity} 

\label{sec:groundtruthextract}

We extracted ground truth cluster ambiguity for the sampled scatterplots by measuring the extent to which visual clustering results (i.e., lassoing result) differed by participant.
To compute the difference, we used external clustering validation measures (EVMs)~\cite{wu09kdd}, following previous research on visual clustering~\cite{desjardins07iui, jeon22arxivdistortion}. As EVMs can be applied only to a pair of clusters, we computed EVM scores of visual clustering results for each cluster pair and averaged the scores to get the final ground truth ambiguity. For EVMs, we used adjusted mutual information (\texttt{ami})~\cite{vinh10jmlr}, adjusted rand index (\texttt{arand})~\cite{steinley04pm}, v-measure (\texttt{vm})~\cite{rosenberg07emnlp}, homogeneity (\texttt{homo})~\cite{rosenberg07emnlp}, and completeness (\texttt{comp})~\cite{rosenberg07emnlp}.
\rev{We chose these measures as they are widely adopted in clustering and data mining communities \cite{rezaei16tkde, jeon22arxiv}.}
As a result, we obtained five ground truth cluster ambiguity rankings of scatterplots, where each corresponds to an individual EVM.
We only used EVMs that are ``adjusted'' so that the scores can be compared across different datasets~\cite{wu09kdd}.





\subsubsection{Results and Discussions} 

\label{sec:mainstudyresults}

\noindent
\textbf{Performance Analysis:}
The results validate the \measure's preciseness in estimating cluster ambiguity.
As seen in \autoref{fig:ambrankcoor}, the rankings made by \measure generally showed a strong correlation with the ground truth ranking ($\rho > 0.6$) based on a criterion of Prion and Haerling~\cite{prion14csn}. 
The correlation was, however, low for \texttt{arand}. 
This is because, unlike other EVMs that interpret clustering assignments as a probabilistic event, \texttt{arand} discretely ``counts'' the disagreement of clustering results and thus generates results that do not align with our probabilistic approach.
\measure also substantially outperformed clustering techniques in terms of predicting ground truth for all EVMs, verifying that \measure is the best of the tested computational options. 
%all computational options available.

% In addition, as 
As depicted in \autoref{fig:ambcoorbar}, 
\measure showed better performance in estimating ground truth cluster ambiguity compared to the average human annotators. 
Except for the case of \texttt{arand}, \measure always showed a better correlation with ground truth (red dashed line) compared to the average correlation of human annotators (purple dotted line) regardless of the EVM choice.
On average, 9.8 out of 18 participants (54\%; the proportion of orange bars) made less accurate predictions compared to \measure.
This result indicates that it is difficult to estimate ambiguity for the judgment of a single person. Therefore, our automatic solution offers increased value in accurately evaluating ambiguity.
% \fix{However, \measure failed to outperform every participant, which means that our measure is biased or cannot see some factors related to the ambiguity that human experts can detect.
% searching for such defects and improving \measure will be an interesting future work; we discuss this issue in \autoref{sec:discuss}ㅁ.}


\noindent
\textbf{Post-Study Interview:}
As described in \autoref{sec:mainstudydesc}, we interviewed participants 
%on two questions 
to identify the reasoning behind their responses. We found that the main factors behind the lassoing (i.e., visual clustering) and determining the scatterplot ambiguity levels were density, proximity, shape, and distribution (as shown in \autoref{fig:qual_study}). We elaborate on these findings in detail in \autoref{sec:discuss}.


