

\section{\measure}

% \fix{Why CLAMS came into existence}

We introduce \measure, a VQM for estimating the cluster ambiguity of a monochrome scatterplot.
Trained over perceptual experiment data, our approach \rev{acts} as a proxy for human perception.
Moreover, by decomposing an input scatterplot with a Gaussian mixture model (GMM) and measuring ambiguity in a component-pairwise manner, \measure can deal with a wide range of cluster patterns while maintaining scalability. 

\subsection{Design Considerations}



\label{sec:decon}

Based on a thorough examination of the related work on cluster perception (\autoref{sec:cps}) and VQMs (\autoref{sec:vqmcp}), we set three design considerations in estimating cluster ambiguity.
% and to be readily used in practice.


\setlist{topsep=0.1em}
\begin{itemize}[noitemsep] %\setlength
    \item[\textbf{(C1)}] \textbf{Provide a proxy for human perception:} To assess the variability of people's perception, the measure should accurately reflect how people identify and analyze clusters in practice \cite{quadri21tvcg, abbas19cgf, aupetit19vis}. 
    Refer to Step 2 in \autoref{sec:pipeline} for how we achieved this consideration.
    \item[\textbf{(C2)}] \textbf{Work with a wide range of scatterplot patterns:} Scatterplots
    have a wide range of cluster patterns; they can have clusters with diverse characteristics (e.g., shape, density, size)~\cite{jeon22tvcg} and varying numbers of clusters~\cite{quadri21tvcg}. 
    %Here, 
    \measure should 
    %be able to 
    properly estimate the ambiguity of an arbitrary scatterplot and should properly estimate cluster ambiguity across a diverse array of patterns
    % fully covering the diversity of cluster patterns 
    \cite{quadri21tvcg, abbas19cgf}. 
    Check Step 1 in \autoref{sec:pipeline} to see how our measure design regards this consideration.
    \item[\textbf{(C3)}] \textbf{Be scalable:} To be readily used in practice (\autoref{sec:appl}), 
    %the measure should be scalable
    \measure should scale to large numbers of data points and complex patterns \cite{abbas19cgf}. 
    Throughout the entire pipeline (Steps 1, 2, and 3), we maintain scalability as a key consideration.
    %the complexity of cluster patterns.
\end{itemize}

\subsection{\measure Pipeline}

\label{sec:pipeline}



% Modeling the direct relationship between scatterplots and their cluster ambiguity hardly covers the diversity of realistic grouping patterns, as the model may perform poorly when faced with ``unseen'' patterns, which makes it difficult to precisely predict the ambiguity of arbitrary scatterplots (C2). 
%Therefore, inspired by previous measures from various domains (e.g., VQM~\cite{abbas19cgf, abbas21arxiv}, clustering~\cite{halkidi01icdm, rousseeuw87jcam}, dimensionality reduction~\cite{venna06nn, lee2007springer}), 
We draw on previous approaches (e.g., VQM~\cite{abbas19cgf}, clustering~\cite{halkidi01icdm, rousseeuw87jcam}, dimensionality reduction~\cite{venna06nn, lee2007springer}) to design our measure to (Step 1) decompose a given scatterplot into smaller components, (Step 2) compute component-pairwise local ambiguity scores, and (Step 3) estimate the global ambiguity score by aggregating the local scores (see \autoref{fig:pipeline}). As the diversity of local components is 
%smaller 
inherently much lower than 
%the one of the original 
that of the full scatterplot, this decomposition
%such a pipeline 
helps \measure to readily work with a wide range of scatterplot patterns (C2). 
%The detailed explanation of each step is as follows:
% Our approach is as follows:

\subsubsection*{(Step 1) Scatterplot Decomposition}

\measure starts by decomposing an input scatterplot into smaller components. 
We use the Gaussian Mixture Model (GMM), which decomposes a given dataset as a mixture of multidimensional Gaussian distributions. 
We select GMM due to several advantages. 
First, GMM does not require hyperparameter selection~\cite{abbas19cgf}. The number of Gaussian distributions (components) can be automatically determined based on fixed statistics, such as Bayesian information criteria (BIC)~\cite{schwarz78ans}. Moreover, as each component is represented as a Gaussian distribution, complex patterns can be abstracted into a concise statistical summary (e.g., mean, covariance matrix; C2).
Note that \rev{GMM decomposition has been shown to be applicable to visual identification problems \cite{abbas19cgf, abbas21arxiv}, accurately representing a wide range of smooth cluster patterns.}
Finally, the complexity of GMM for 2D scatterplots is $O(NK)$, where $N$ and $K$ denote the number of points and components, respectively, ensuring that the technique is highly scalable (C3).
 

In contrast, a conventional approach for scatterplot decomposition, which involves using clustering techniques (e.g., $K$-Means~\cite{likas03pr}, HDBSCAN~\cite{mcinnes17icdmw}), falls short in enabling \measure to satisfy our target considerations.
First, clustering results can vary significantly due to the sensitivity of the outcomes to changes in hyperparameters. We can find an optimal hyperparameter setting using clustering validation measures~\cite{xiong2018clustering}, but the optimal setting may depend on the selection criteria~\cite{liu10icdm}. Clustering techniques also provide a partition of data points, meaning they neither abstract nor simplify clustering patterns. Lastly, clustering techniques that are widely known to be able to capture complex patterns (e.g., density-based clustering \cite{mcinnes17icdmw}, density-peak clustering \cite{liu18is}) often suffer from scalability issues.


Note that while applying GMM to the input scatterplot, we determine the optimal number of Gaussian components based on BIC scores and the elbow rule. The elbow is found using the Kneedle~\cite{satopaa11icdcsw} algorithm.

%Here, a typical option is to 
% Such decomposition typically uses clustering techniques (e.g., $K$-Means~\cite{likas03pr}, HDBSCAN~\cite{mcinnes17icdmw}). However, different clustering techniques detect 
% %totally 
% different clustering patterns~\cite{luxburg12icml}, and no single technique that outperforms others~\cite{kleinberg02nips}. 
% %the selection of technique is nontrivial. 
% Clustering results also vary 
% %a lot 
% significantly due to 
% %the change of 
% changing hyperparameters. We can find an optimal hyperparameter setting using clustering validation measures~\cite{xiong2018clustering}, but the optimal setting may depend on the selection criteria~\cite{liu10icdm}. Moreover, clustering techniques provide a partition of data points, meaning they neither abstract nor simplify clustering patterns. Therefore, we cannot ensure that the diversity of the patterns is reduced. 

% We address these challenges using a Gaussian Mixture Model (GMM), which decomposes a given dataset as a mixture of multidimensional Gaussian distributions. 
% %, escapes from the aforementioned problem. First of all, 
% GMM 
% %is a technique free from
% does not require hyperparameter selection~\cite{abbas19cgf}. Instead, the number of Gaussian distributions (components) can be automatically determined based on fixed statistic, such as Bayesian information criteria (BIC)~\cite{schwarz78ans}. Moreover, as each component is represented as a Gaussian distribution, complex patterns can be abstracted into a 
% %few numbers representing a
% concise statistical summary (e.g., mean, covariance matrix; C2). Note that GMM 
% decomposition
% %is justified and validated to be
% has been shown to be \textit{universal}, which means that it can accurately represent a wide range of arbitrary cluster patterns~\cite{abbas19cgf}. 
% Finally, GMM's complexity for 2D scatterplots is $O(NK)$, where $N$ and $K$ denote the number of points and components, which ensures the technique's scalability (C3).

% %Due to such advantages, we 
% \measure uses GMM to decompose an input scatterplot into smaller components. 

\subsubsection*{(Step 2) Component-Pairwise Local Ambiguity Computation}

We 
%consecutively 
predict local ambiguity for every pair of Gaussian components so that \measure can consider every possible interaction between 
%the 
components. 
%The following is a description of how we constructed a system that makes such predictions.
This process is as follows: 

\noindent
\textbf{Dataset:}
We use the human-judged cluster separability dataset $\mathbf{X}$ from the ClustMe study~\cite{abbas19cgf} (C1). $\mathbf{X}$ contains 1,000 scatterplots $\{X_1, X_2, \cdots, X_{1000}\}$, where each scatterplot consists of a pair of Gaussian components with diverse statistics (e.g., mean, covariance). 
The separability scores $\mathbf{S} = \{S_1, S_2, \cdots, S_{1000}\}$ are computed by aggregating the judgments of 34 participants on each scatterplot gathered by the ClustMe study~\cite{abbas19cgf}. These scores reflect the judgments of the participants 
%were asked to answer
 regarding whether they 
%can see 
saw one or multiple clusters in each scatterplot. 
%and 
$S_i$ represents the proportion of participants who perceived more than one cluster in $X_i$. 

\noindent
\textbf{Deriving Cluster Ambiguity from Separation Score:}
%Cluster ambiguity of 
We compute the cluster ambiguity of each Gaussian components pair with separation score $S$ 
%is defined 
as: $A(S) = - S \log S - (1-S) \log (1-S)$. Theoretically, $A$ is defined as the Shannon entropy of a binomial distribution representing the probabilistic event of counting the number of clusters. We use entropy as it is an efficient and mathematically grounded function representing the level of ``uncertainty'' of the associated event~\cite{massen88aps}
%, and also easy to compute 
(C3). 
%Practically, 
$A$ is minimized when $S$ is 0 or 1 (i.e., every participant 
%answered in an identical way;
gave the same answer, meaning there was no variability in visual clustering), and is maximized when $S$ is 0.5 (i.e., half of the participants saw a single cluster and the other half saw more than one cluster, maximizing variability). 
%; maximum variability in visual clustering).
As the scatterplots and original separability scores have proven to be effective in modeling cluster perception~\cite{abbas19cgf}, we can also expect the ambiguity scores 
%to well 
to reliably represent human perception. Note that our evaluation validates this expectation (\autoref{sec:mainstudy}; C1). 

\noindent
\textbf{Predicting Ambiguity:}
To predict local ambiguity for each Gaussian component, we train a regression module $f$ estimating the separability score $f(X)$ of an arbitrary pair of Gaussian components $X$, using $\mathbf{X}$ and $\mathbf{S}$. For an arbitrary pair of Gaussian components $X$, the local ambiguity score of a pair is computed as $A(f(X))$. 
Although 
%the training of 
training the regression module requires heavy computation (\autoref{sec:regmodel}), 
%the inference of the
inferring local ambiguity scores can be done in constant time regardless of the number of Gaussian components, making Step 2 scalable (C3). 
Please refer to \autoref{sec:regmodel} for a detailed explanation of how we designed and implemented the model.

\subsubsection*{(Step 3) Aggregating Local Ambiguity}

Finally, we aggregate the component-pairwise ambiguity scores by averaging the scores.
Note that the final score can be interpreted as the joint entropy of a set of events corresponding to each local ambiguity score with an assumption that the events are mutually independent. 


\subsection{Computational Complexity}

As the time complexity of GMM on a 2D scatterplot is $O(NK)$, the time complexity of Step 1 is $O(NK_{max}^2)$, where $K_{max}$ is the maximum number of components we consider in finding the optimal number of components and $N$ is the number of points. The complexity for both Steps 2 and 3 is $O(K_{opt}^2)$, where $K_{opt}$ denotes the optimal number of components found in Step 1. Thus, the computational complexity of \measure is $O(NK_{max}^2 + K_{opt}^2)$. As $N \gg K_{max}$ and $N \gg K_{opt}$, the effect of $K_{max}$ and $K_{opt}$ is negligible, making \measure a scalable measure that is only linear to the number of points. Refer to Appendix B for the quantitative experiment demonstrating the scalability of \measure.


\section{Estimating Human-Judged Cluster Separability}

\label{sec:regmodel}

%We discuss how we built the model for predicting the separability of a Gaussian components pair. At first, w
We constructed a regression module to predict the separability of a given Gaussian components pair. 
We conducted a user study exploring the factors affecting visual clustering to ground our model in human reasoning processes. We trained the model to estimate ambiguity from the features extracted from each pair, where the features were engineered based on the study findings.



% \definecolor{myblue}{RGB}{7, 115, 193}
% \definecolor{myred}{RGB}{255, 10, 9}
\definecolor{mypurple}{RGB}{123, 64, 167}
\definecolor{myorange}{RGB}{255, 118, 38}



% \newcommand{\blue}[1]{{\color{myblue}#1}}
% \newcommand{\red}[1]{{\color{myred}#1}}
\newcommand{\purple}[1]{{\color{mypurple}#1}}
\newcommand{\orange}[1]{{\color{myorange}#1}}


{\renewcommand{\arraystretch}{1.5}

\begin{table*}[t]
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{c|lllc}
     \toprule
     \multirow{7}{*}{\includegraphics[height=1.45in]{figures/feature_engineering.pdf}} 
         & \textbf{\textit{Factors}} &   \textbf{Features}       & \textbf{Formula}  & \textbf{Used} \\
    \cline{2-5}
        & \textit{Proximity} & Distance between centers (DC) & $\parallel x_1 - x_2 \parallel$ & \small{$\bigcirc$}\\ 
        & \textit{Clarity of a gap} & Distance-size ratio (DSR) &  $\parallel x_1 - x_2 \parallel / (\sqrt{\parallel a_1\parallel ^2 + \parallel b_1 \parallel ^2} +\sqrt{\parallel a_2\parallel ^2 + \parallel b_2 \parallel ^2} )$ & \small{$\bigcirc$}\\ 
        & \textit{Density Difference} & Density difference (DD) & $ \mid (n_1 / (2 \cdot \parallel a_1\parallel \cdot \parallel b_1\parallel)) - (n_2 / (2 \cdot \parallel a_2\parallel \cdot \parallel b_2\parallel)) \mid$ & $\times$ \\
        & \textit{Size Difference} & Size difference (SD) & $\mid \sqrt{\parallel a_1\parallel ^2 + \parallel b_1 \parallel ^2} -\sqrt{\parallel a_2\parallel ^2 + \parallel b_2 \parallel ^2} \mid $ & \small{$\bigcirc$}\\
        & \textit{Ellipticity Difference} & Ellipticity difference (ED) & $\mid (\parallel a_1\parallel / \parallel b_1\parallel - \parallel a_2\parallel / \parallel b_2\parallel )\mid$&\small{$\bigcirc$} \\
        & \textit{Direction}&  Angle between components (AC) & $ \min(\Delta_\theta, 2\pi - \Delta_\theta)$ where $\Delta_\theta = \mid \theta_1 - \theta_2 \mid$ & \small{$\bigcirc$}\\
         \bottomrule
        \addlinespace[0.05cm]
        \multicolumn{5}{l}{
        \footnotesize
            $a_1, a_2$ \& $b_1, b_2$: Standard deviation along the major \& minor axis of Gaussian components / $x_1, x_2$: the center of Gaussian components
            
         \vspace{-1.78mm}
        } \\ 
        \multicolumn{5}{l}{
        \footnotesize
        $\theta_1, \theta_2$: 
Angle between the major axis of Gaussian components and the horizontal line / $n_1, n_2$: number of points in Gaussian components
        }
    \end{tabular}
    }
    \vspace{-2mm}
    \caption{The 
    %list of 
    visual clustering factors revealed by our preliminary user study (\autoref{sec:preexp}; first column) and the corresponding features designed for the regression module predicting the human-judged separability of Gaussian components (\autoref{sec:feateng}; second column). The third column depicts how we compute each feature, and the fourth column represents whether or not the feature is used in the final regression module (\autoref{sec:regmodeleval}). Empowered by the designed features, the module succeeds in precisely estimating the separability scores.  \vspace{-4mm}}
    \label{tab:feature_eng}
\end{table*}

}



\subsection{Factor Exploration Study}
\label{sec:preexp}

To design features that relate to the human-judged cluster separability,
%of clusters, 
we first explored 
%the 
factors that affect visual clustering via a qualitative experiment. In the study, 
%we randomly exposed a series of scatterplots to the participants and had them conduct visual clustering tasks,
participants completed a series of visual clustering tasks and reported on 
%explicitly asking about the 
factors that affected task results. 

\subsubsection{Study Design}

\noindent
\textbf{Procedure and Tasks}: 
The experiment began with informed consent and a basic demographic survey. Participants completed the rest of the study in three phases: (1) identifying clusters in a single scatterplot, (2) performing pairwise comparisons between scatterplots, and (3) participating in an interview to elicit core factors influencing visual clustering. 
%The main experiment, which was held after collecting demographic information, consists of two phases. 
In the first phase, we randomly sampled 12 scatterplots from $\mathbf{X}$ and showed them in sequence. For each scatterplot, we first asked participants to select whether there existed one or more than one cluster, following the original ClustMe~\cite{abbas19cgf} study. We additionally asked participants about their confidence in 
%selecting the number of perceived clusters. 
their response using a Likert scale
%. We then requested participants 
and asked them to describe the reasoning process
%s for making the choices, 
both for the number of clusters and their confidence. 

In the second phase, we randomly sampled 12 pairs of scatterplots (24 in total) from $\mathbf{X}$. 
%and showed a pair side by side for each trial. 
For each trial, we asked participants to report which scatterplot in a given pair was more separated. As in the first phase, we asked for the participants' confidence and had them report the reasoning behind their choices and confidence. 
%Note that we  also inquired about the reasoning behind their confidence to let participants think once more about their selection, so that we could increase the diversity of responses. 
% We divided phases one and two for the same reason; we wanted participants to develop reasons from two different perspectives. 
After the two phases of the experiment, we conducted a semi-structured post hoc interview, asking the participants about the salient factors they felt were affected by cluster separation.
%of the clusters.
All participants finished the experiment within one hour.

\noindent
\textbf{Participants:}
We recruited 10 participants from a local university (six males and four females, aged 19--28 [$23.5\pm 2.6$]). Six of the participants were undergraduates, three were graduate students, and one had just completed their Bachelor's degree.
We selected only participants who had experience in data analysis using scatterplots to better ground our results in real-world data analysis and to ensure that they understood the concept of clusters.
\rev{Three participants reported that they are at a novice level in data analysis, which meant that they did not regularly conduct analyses but have some experience. Five and two participants reported themselves as at intermediate and expert levels, respectively. Half of the participants reported being at the novice level of data analysis using a scatterplot and the other half reported themselves as intermediate.}
\rev{We also confirmed that participants had not read three papers \cite{abbas19cgf, abbas21arxiv, aupetit19vis} that incorporate ClustMe data (i.e., $\mathbf{X}$). }
Participants were compensated with the equivalent of \$20.

\noindent
\textbf{Apparatus:}
The experiment was conducted over Zoom, and the sessions were recorded. We developed a website in which participants could see scatterplots and make their selection (number of clusters and confidence) with a mouse click. 
\rev{
We fixed the stimuli size to 700px $\times$ 700px and constrained participants to use a laptop or desktop screen to minimize the impact of the display on study results.
}
They were asked to access the website and share their screens so that the experimenter could monitor and guide the experiment. 

\subsubsection{Results}
\label{sec:preexpresults}

We analyzed the responses from the main experiment and the post hoc interview using axial coding done by two authors, one of whom works in machine learning while the other's primary expertise is in visualization.
% 
The coders individually developed a separate codebook in the first stage. Two codebooks were then merged, 
%where the total number of extracted factors was 13. 
resulting in 13 total extracted factors. Finally, based on two stages of discussions, the coders agreed to categorize the codes into six main factors
(see \autoref{tab:feature_eng})---proximity, clarity of gap, density difference, size difference, ellipticity difference, and direction.

The most important factor was the \textit{proximity} between the clusters, 
which was mentioned by seven out of ten participants in the interviews. 
%. In the post-hoc interview, seven out of 10 (70\%) participants explicitly mentioned the distance or proximity between clusters as a salient factor that affects cluster separation. 
%We also found that 
The perception of proximity was mainly affected by the \textit{clarity of a gap} between clusters. Participants reported that if the gap between clusters was bigger and 
%had a relatively smaller density 
less dense than the clusters, the clusters were perceived to be more distinct. Five out of ten participants explicitly mentioned the gap between clusters as an important factor in the interviews. 
%post-hoc interviews.

%Another important factor we found was the \textit{density difference} between clusters. We observed that p
Participants tended to perceive more than one cluster when two Gaussian components had noticeably \textit{different densities}, even if there was no gap between them or if they overlapped (i.e., when they had high proximity). However, two participants explicitly noted that if the density of a point group was too low, the points within the group 
%tend to be 
were perceived as outliers rather than a cluster. We found that the \textit{size difference} between clusters similarly affects visual clustering: 
%Participants reported that 
if a cluster was too small, participants often wanted to interpret the cluster as a set of outliers. In the 
%post-hoc 
interviews, 30\% (three out of 10) of participants noted that the differences in density were salient features that affected their choices, whereas 
%10\% (1 out of 10) 
one noted that differences in cluster size played a role in their decisions.

%We also noticed 
\textit{Ellipticity difference} between clusters 
%was an important factor in visual clustering. 
also affected perceived separability.
Participants mentioned that if the ellipticity of a cluster was high, it 
%was recognized to 
could be fit by linear regression where the regression line accorded with the major axis of the ellipse, thus would be more likely to be perceived as a single cluster. 
We also found that if the two clusters had high ellipticity, their \textit{direction} (i.e., the direction of the first principal axis) also played a crucial role in their separation. 
Participants noted that two clusters with high ellipticity and different directions were likely to fit into two independent regression lines, thus more likely to be perceived as two independent clusters. 
In the 
%post-hoc 
interviews, five out of ten participants mentioned ellipticity and direction as salient factors, while four additionally mentioned linear regression or correlation lines. 

\subsection{Feature Engineering} 

\label{sec:feateng}


We designed features representing the characteristic of a pair of Gaussian components based on the six factors 
%revealed by the exploration 
from the study (\autoref{sec:preexp}). 
To maintain the simplicity and explainability of the model, we aimed to keep the features as simple and few as possible.
%and minimize their quantity. 
We thus designed the features to be (1) be bijective to the factors and (2) directly computed from the statistical summary of the Gaussian components pair (see \autoref{tab:feature_eng}). 
%A detailed explanation of each feature is as follows:
The features are as follows: 

\noindent
\textbf{Distance between Centers (DC):}
For 
%the feature corresponding to 
\textit{proximity}, we selected the distance between the centers of Gaussian components. We used the distance between centers as it is the sole metric representing proximity that can be directly derived from the population statistics (i.e., mean) and is also widely used for measuring the proximity between clusters~\cite{liu10icdm}. 

\noindent
\textbf{Distance-Size Ratio (DSR):}
The gap between two Gaussian components generally becomes bigger if (1) the distance between the center of two Gaussian components increases or (2) the size of the components becomes smaller. 
%Based on such intuition, we defined 
We used this property to construct a feature corresponding to the \textit{clarity of the gap} as the ratio of the distance between centers of Gaussian components over the sum of the size of two components. Note that we defined the size of a component as its standard deviation---the root sum square of the standard deviations along the first and second principal axes.

\noindent
\textbf{Density Difference (DD):}
The \textit{density difference} factor can be directly represented
%as a feature, where we compute 
by computing the density of a component as the ratio of the number of points over the area covered by the component. 
%Here, w
We defined the area of a component as 
%the one 
that of an ellipse where the major and minor axes' length is identical to the standard deviation along the first and second principal axes, respectively. 

\noindent
\textbf{Size Difference (SD):}
%The 
\textit{Size difference} 
%factor 
can be also directly represented as a feature. We defined the size of the component as its standard deviation, as we do for the distance-size ratio feature.

\noindent
\textbf{Ellipticity Difference (ED):}
We directly used \textit{ellipticity difference} 
%factor 
as a feature, 
%while 
defining the ellipticity of a component as the one of an ellipse having the standard deviation along the first and second principal axes as major and minor axes' lengths, respectively.

\noindent
\textbf{Angle between Components (AC):}
%While converting the \textit{direction} of components as a feature, we focused on the fact that p
Participants reported \textit{direction} as salient only if they think two components are heading toward different directions. 
%Thus, we selected the ``angle'' 
We therefore define direction as the angle between components, which can be represented as the angle between the first principal axes of two components. 
%, as a feature corresponding to the factor. 

\subsection{Modeling Training} 

We trained a regression module for estimating human-judged separability of Gaussian components pair using $\mathbf{X}$ and $\mathbf{S}$. 
\rev{We used a regression module as we aimed to predict continuous scores.}
For a given pair of Gaussian components, the model first extracts the features as defined above and then predicts a separability score from the features. 
We implemented our module using AutoML~\cite{he21kbs} supported by the \verb|auto-sklearn|~\cite{feurer15nips} library. 
We report the performance of our model and the significance of the extracted features in \autoref{sec:regmodeleval}.


 

% \subsubsection{Feature Engineering}




% Through the preliminary experiment (\autoref{sec:preexp}), we extracted six main factors affecting visual clustering: \textit{proximity}, \textit{existence of a gap}, \textit{density difference}, \textit{size difference}, \textit{ellipticity difference}, and \textit{angle}. 
% To maintain the simplicity and explainability of the model, we wanted to make features as simple as possible and minimize the number of features. 
% Therefore, we design the features corresponding to each factor, while designing them to be directly computed from the statistical summary of the Gaussian components (See \autoref{tab:feature_eng}). 
% A detailed explanation of each feature is as follows:


% \textbf{Distance between centers, \texttt{cdist}} We select the distance between the center of Gaussian components, which we call \texttt{cdist}, as the feature representing the proximity between clusters. We use \texttt{cdist} as the distance between cluster centroids is widely used as a measure for assessing the proximity between clusters~\cite{liu10icdm}. Moreover, it is easy to compute, driven by the statistical summary (i.e., mean). 

% \textbf{Ratio of the distance between centers over the size of components, \texttt{cdist\_size\_ratio}} The gap between two Gaussian components generally becomes bigger if (1) the distance between the center of two Gaussian components increases, or (2) the size of components becomes smaller. Based on such intuition, we defined a feature corresponding to the clarity of the gap as the ratio of the distance between centers of Gaussian components over the sum of the size of components. Here, while the numerator is identical to \texttt{cdist}, we define the denominator as the sum of the standard deviation of Gaussian components.

% \textbf{Density difference, \texttt{density\_diff}} 
% The third factor, density difference, can be directly represented as a feature, which we call \texttt{density\_diff}. We define the density of a Gaussian component as the ratio of the number of points over the area covered by the component. 
% The area of the component is defined as the one of an ellipse where the major and minor axes' length is identical to the standard deviation along the first and second principal axes, respectively. 

% \textbf{Size Difference, \texttt{size\_diff}}
% The difference between the size of the components can be also directly represented as a feature; here, the size of the component is defined as its standard deviation, as we do for \texttt{cdist\_size\_ratio}.

% \textbf{Ellipticity Difference, \texttt{ellipticity}}
% We also directly converted the Ellipticity difference into a feature called \texttt{ellipticity\_diff}. We define the ellipticity of a Gaussian component as the one of an ellipse having the standard deviation along the first and second principal axes as major and minor axes' lengths, respectively. Note that the ellipticity of an ellipse can be computed as a ratio of the length of the minor axis over the one of the major axis.

% \textbf{Angle between components, \texttt{angle}}
% While converting the angle between Gaussian components as a feature, we focus on the fact that the participants tend to denote the angle as the salient factor if they recognized the component to better fit by linear regression. This means that participants perceived a component to be ``directing'' toward the direction of the first principal axis. Therefore, we simply compute the angle between two Gaussian components, which we call \texttt{angle}, as the angle between the first principal axis of two components.


% Using the dataset, 



% We use the human-judged cluster separability dataset $\mathcal{X}$ from ClustMe study~\cite{abbas19cgf}, to construct a dataset defining the relationship between  a scatterplot consisting of pair of Gaussian components and its ambiguity. $\mathcal{X}$ consists of 1,000 scatterplots $\{X_1, X_2, \cdots, X_{1000}\}$, where each scatterplot consists of a pair of Gaussian clusters with diverse hyperparameters, such as their position and covariance. The perceived separability score $S_i$ of a scatterplot $X_i$ is computed by aggregating the judgments of 34 participants on the scatterplot. The participants were asked to answer whether they can see one or more than one cluster in scatterplots, and $S_i$ is defined as the proportion of participants who perceived more than one cluster in $X_i$. 

% Here, a cluster ambiguity score $A_i$ of scatterplot $X_i$ is defined as: $A_i = - S_i \log S_i - (1-S_i) \log (1-S_i)$. Theoretically, $A_i$ is defined as the Shannon entropy of a binomial distribution representing the probabilistic event of counting the number of clusters. We use Shannon entropy as it is a mathematically grounded function representing the level of ``uncertainty'' of the associated event~\cite{massen88aps}, and also easy to compute. Practically, $A_i$ is minimized when $S_i$ is 0 or 1 (i.e., every participant answered in an identical way; no variability in conducting the task), and is maximized when $S_i$ is 0.5 (i.e., half of the participants saw a single cluster, and another half saw more than one cluster; maximum variability in conducting the task).
% As the scatterplots and original separability scores are proven to be effective in modeling human cluster perception~\cite{abbas19cgf}, we can also expect the ambiguity scores to well represent the perception. 

% As aforementioned, \measure produces the final global cluster ambiguity by aggregating local ambiguity scores in a component-pairwise manner. In the sequel, we will discuss how we train the model that predicts the local ambiguity using the scatterplots and corresponding ambiguity score (\autoref{sec:localambtrain}). 





% \subsection{Modeling Approach}

% In this section, we discuss two basic modeling approaches of our measure: decomposing scatterplots using Gaussian Mixture Model (GMM)~\cite{bensmail96jasa, fraley02jasa} and defining the local cluster ambiguity between the decomposed components using human-judged cluster separability dataset~\cite{abbas19cgf}.


% \subsubsection{Decomposing Scatterplots using Gaussian Mixture Model}
% Modeling the direct relationship between scatterplots and their cluster ambiguity hardly covers the diversity of realistic grouping patterns; the model may suffer from the ``unseen'' patterns, which makes it unable to properly predict the ambiguity of arbitrary scatterplots (C2). Therefore, inspired by previous measures from various domains (e.g., VQM~\cite{abbas19cgf, abbas21arxiv}, clustering~\cite{halkidi01icdm, rousseeuw87jcam}, dimensionality reduction~\cite{venna06nn, lee2007springer}), we design our measure to (1) decompose a given scatterplot into smaller components, (2) compute component-level local ambiguity scores, and (3) estimate global ambiguity score by aggregating the local scores. As we now only need to model the relationship between cluster ambiguity and local components, where their diversity is much smaller than the original scatterplots, we can expect the measure to more precisely estimate the cluster ambiguity of arbitrary clusters. 

% Here, the problem is about selecting the way to decompose scatterplots. The na\'ive way is to use clustering techniques, e.g., $K$-Means~\cite{likas03pr}, HDBSCAN~\cite{mcinnes17icdmw}. However, as different clustering techniques detect totally different clustering patterns~\cite{luxburg12icml} and there is no single technique that always outperforms others~\cite{kleinberg02nips}, so the selection of technique is nontrivial. Clustering results also vary a lot due to the change of hyperparameters, such as the number of clusters (i.e., $K$) in $K$-Means. We can find an optimal hyperparameter setting using clustering validation measures~\cite{xiong2018clustering}, but the optimal setting may depend on the selection of criteria~\cite{liu10icdm}. Moreover, clustering techniques just provide a partition of data points, thus neither abstract nor simplify clustering patterns; we thus cannot ensure that the diversity of the patterns is reduced. 

% We find that the Gaussian Mixture Model (GMM), which decomposes a given data as a mixture of multidimensional Gaussian distributions, escapes from the aforementioned problem. First of all, GMM is an algorithm free from hyperparameter selection~\cite{abbas19cgf}, where the number of distributions (components) can be automatically determined based on statistical criteria. Each component made by GMM is also represented as a Gaussian distribution, which can be abstracted into a few scores representing a statistical summary (e.g., mean, covariance matrix). Moreover, GMM is justified and validated to be \textit{universal}, which means that it can accurately represent a wide range of arbitrary cluster patterns~\cite{abbas19cgf}. 

% Due to such advantages, we decide to use GMM to represent the input scatterplot and divide it into smaller components. 


% \subsubsection{Defining Local Cluster Ambiguity}

% \label{sec:localambdefine}


% We use the human-judged cluster separability dataset $\mathcal{X}$ from ClustMe study~\cite{abbas19cgf}, to construct a dataset defining the relationship between  a scatterplot consisting of pair of Gaussian components and its ambiguity. $\mathcal{X}$ consists of 1,000 scatterplots $\{X_1, X_2, \cdots, X_{1000}\}$, where each scatterplot consists of a pair of Gaussian clusters with diverse hyperparameters, such as their position and covariance. The perceived separability score $S_i$ of a scatterplot $X_i$ is computed by aggregating the judgments of 34 participants on the scatterplot. The participants were asked to answer whether they can see one or more than one cluster in scatterplots, and $S_i$ is defined as the proportion of participants who perceived more than one cluster in $X_i$. 

% Here, a cluster ambiguity score $A_i$ of scatterplot $X_i$ is defined as: $A_i = - S_i \log S_i - (1-S_i) \log (1-S_i)$. Theoretically, $A_i$ is defined as the Shannon entropy of a binomial distribution representing the probabilistic event of counting the number of clusters. We use Shannon entropy as it is a mathematically grounded function representing the level of ``uncertainty'' of the associated event~\cite{massen88aps}, and also easy to compute. Practically, $A_i$ is minimized when $S_i$ is 0 or 1 (i.e., every participant answered in an identical way; no variability in conducting the task), and is maximized when $S_i$ is 0.5 (i.e., half of the participants saw a single cluster, and another half saw more than one cluster; maximum variability in conducting the task).
% As the scatterplots and original separability scores are proven to be effective in modeling human cluster perception~\cite{abbas19cgf}, we can also expect the ambiguity scores to well represent the perception. 

% As aforementioned, \measure produces the final global cluster ambiguity by aggregating local ambiguity scores in a component-pairwise manner. In the sequel, we will discuss how we train the model that predicts the local ambiguity using the scatterplots and corresponding ambiguity score (\autoref{sec:localambtrain}). 






% denotes the ambiguity of $X_i$, is defined as: 


% based on a simple proposition: \textit{The more ambiguous a scatterplot is, the more likely the task result on the scatterplot vary}. 

% Following the proposition, $A_i$ should be minimum if $S_i$ is 0 or 1 (i.e., every participant answered in an identical way), and should be maximum if the score is 0.5 (i.e., half of the participants saw a single cluster, and another half saw more than one cluster). 
% Here, we define $A_i = - S_i \log S_i - (1-S_i) \log (1-S_i)$, which is the Shannon entropy of a binomial distribution representing the probabilistic event of counting the number of clusters. 




% Following the proposition, the ambiguity of $X_i$ should be minimum if $S_i$ is 0 or 1 (i.e., every participant answered identically), and should be maximum if the score is 0.5 (i.e., half of the participants saw a single cluster, and another half saw more than one cluster). Here, we define $A_i = - S_i \log S_i - (1-S_i) \log (1-S_i)$, which is the Shannon entropy of a binomial distribution representing the probabilistic event of counting the number of clusters. As 





% Inspired by previous measures from various domains (e.g., VQM~\cite{abbas19cgf, abbas21arxiv}, clustering~\cite{halkidi01icdm, rousseeuw87jcam}, dimensionality reduction~\cite{venna06nn, lee2007springer}), our measure quantifies cluster ambiguity by (1) decomposing the input scatterplot into atomic components, (2) computing block-level ambiguity scores, and (3) aggregating the scores. 


% \noindent
% \textbf{(C1) Be a proxy for human perception } 
% As a measure estimating cluster ambiguity, which is defined as a potential perceptual variability of visual clustering, \measure should accurately reflect human perception. 
% To do so, we follow the data-driven approach~\cite{abbas19cgf, sedlmair2015data}, training our model using the data gathered by a user study conducted in previous research~\cite{abbas19cgf}. The model is moreover empowered by feature engineering based on perceptual factors about cluster perception extracted via a qualitative user study. 

% \noindent
% \textbf{(C2) Be able to deal with a wide range of scatterplot patterns } 
% The measure should properly estimate the ambiguity of an arbitrary scatterplot, where scatterplots have a wide range of cluster patterns.  
% Here, modeling the direct relationship between scatterplots and cluster ambiguity can hardly cover the diversity of realistic grouping patterns (i.e., the model can always suffer from the ``unseen'' patterns). Therefore, we a

% As fully covering the diversity of realistic grouping patterns by directly can be hardly achieved (i.e., the model can always suffer from the ``unseen'' pattern)

% \section{\measure}

% \subsection{Predicting Ambiguity of Two Gaussian Components}

% \subsubsection{Ground Truth Ambiguity Score}

% \subsubsection{User Study for Extracting Features}

% \subsubsection{Model Construction}