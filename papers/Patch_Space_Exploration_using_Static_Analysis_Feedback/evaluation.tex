\section{Evaluation}

We aim to answer the following research questions about our approach which is embodied in our tool \tool.
\begin{itemize}
    \item RQ1: How does \tool perform against other similar tools?
    \item RQ2: How efficient are the equivalence classes in reducing the validation costs?
    \item RQ3: How effective is the PCFG in guiding the navigation search space of program patches?
\end{itemize}

\textbf{Implementation.} We implemented our approach on top of \pulse\footnote{the version which comes shipped with \ourinfer}, a sound static analyser for bug finding in the \infertool toolchain used at Meta. We use \pulse to detect bugs, to derive method summaries which we then use to inspect the effect patches have on the symbolic heap, and to validate patches. 
We use a number of custom \codeql queries for collecting patch ingredients.
For finding fix locations, we use a bespoke instance of SBFL. 
For checking program path subsumptions we invoke CVC4, and for quantifier elimination when dealing with logical variables in path formulas we use Z3.

\textbf{Dataset.} We constructed our dataset from the benchmarks of \saver \cite{Le2022} and \pulse \cite{HongLLO20} 
collecting all the subjects containing memory leaks from former, and all memory leaks and NPE from OpensSSL, \pulse's benchmark.
 \saver and \footpatch rely on Separation Logic, a logic which over-approximates program states.
 This conservative approach may discover more bugs but it is prone to false positives, thus risking to put APR tools in the position of fixing non-bugs, e.g. fixing a false memory leak may lead to a double free. Instead, we built on \pulse's ISL which under-approximates states, thus missing some bugs, but it guarantees we only fix true bugs.
We only focus on overlapping bugs that all three tools  detect. 
In total, there are 27 memory issues in our benchmark: 20 memory leaks and 7 NPEs.  \autoref{tab:comparison} contains a summary of these bugs.

Before conducting experiments on \tool, we ran \codeql and \pulse checker on each subject to generate static analysis database and bug detection reports, which serve as inputs to \tool.



\subsection{RQ1: Comparison with Other Tools}
\label{sec:rq1}

\input{table-2.tex}

We compare the efficacy of \tool against two state-of-the-art APR tools for memory bugs, \saver~\cite{HongLLO20} and \footpatch~\cite{TonderG18}. 
We set a timeout of 20 minutes for \tool and \saver, since most developers prefer APR tools to produce repairs in under 30 minutes \cite{noller2022trust}.
\footpatch was given a timeout of 1 hour because no patch was produced with the 20 minute timeout.

\autoref{tab:comparison} summarizes the results. 
Columns \emph{Plausible Patches} and \emph{Correct Patches} indicate the number of bugs for which each tool found plausible and correct patches, respectively.
A patch is plausible if it passes the analysis check, e.g. \pulse, and correct if it additionally passes manual inspection.
For memory leaks, \tool and \saver are similarly effective, with each tool finding a correct patch for 15 and 13 bugs respectively, while \footpatch finds correct patches for 2 bugs.
For NPEs, \tool finds correct patches for 4 bugs. \saver is not applicable (NA) to NPEs since it uses pre-defined fix strategies. \footpatch applies to NPEs, but did not generate plausible patches.


\autoref{fig:venn-plausible} captures the number of \textit{unique} bugs each tool finds plausible patches for. 
\tool found plausible patches for 8 unique bugs while \saver and \footpatch for 1 unique bug each. 
\autoref{fig:venn-correct} shows a similar diagram for correct patches, which shows \tool finds correct patches for 7 unique bugs.

We note that although \tool applies to NPE while \saver does not, \tool still correctly fixes 3 additional memory leaks compared to \saver (out of the 7 unique bugs in \autoref{fig:venn-correct}).
For these 3 bugs, \saver's custom analysis either fails to analyse the bug reported by Infer, or produces a patch with wrong path condition.
\saver generated a correct patch for one bug
for which \tool did not. \tool failed to generate a patch 
due to the large (automatically) constructed search space, which could have been alleviated by using a more strict selection criteria for patch ingredients.

Compared to \tool and \saver, \footpatch found plausible/correct patches for fewer bugs.
One possible reason is that \footpatch searches for candidate repair statements within the program, which could have two consequences. One is that it does not scale well for large codebases such as Snort and OpenSSL.
In fact, \footpatch times out for these two programs in our experiments.
Another consequence is that it fails to find a patch which requires new expressions.

\begin{figure}[t]
\scriptsize
\centering

\begin{subfigure}[t]{0.47\textwidth}
\centering
\begin{minipage}[t]{0.47\textwidth}
\centering

\includegraphics[width=0.9\textwidth]{plausible.png}
\caption{Plausible patches}
\label{fig:venn-plausible}
\end{minipage}\hfill
\begin{minipage}[t]{0.47\textwidth}
\centering

\includegraphics[width=0.9\textwidth]{correct.png}
\caption{Correct patches}
\label{fig:venn-correct}
\end{minipage}

\end{subfigure}

\vspace*{-0.05in}
\caption{Venn diagram of bugs for which each repair tool was able to generate a plausible patch and a correct patch.}
\label{fig:venn-comparison}
\end{figure}


\subsection{RQ2: Efficiency of  Patch Clustering}
\label{sec:rq2}

\input{table-1.tex}

We evaluated \tool's strategy of clustering patches based on their effects.
\autoref{tab:efffix-results} details our results\footnote{\noindent We target two extra OpenSSL bugs which could not be included in the previous experiment for fairness reasons since \footpatch cannot detect them.}. We focus on the columns under \emph{\tool}, and postpone the discussion of those under  \emph{\tooluni}
to \autoref{sec:rq3}.
To counter for the randomness in the patch synthesis component,
we conducted the experiments for five trials and report the average results where appropriate.
We used a 20-minute timeout for each run, which includes ingredients collection, patch synthesis and clustering.
After the timeout, all patches in plausible clusters are considered as \textit{locally plausible}  (column~\#P\textsubscript{lp}) - the bug was fixed locally in the function.
Since all patches within one cluster are equivalent in the defined abstract domain, only 
one representative patch per cluster is selected as 
candidate for validation (based on its AST size).
We refer to these patches as the \textit{representative} locally plausible patches (column~\#P\textsubscript{lp-r}).
A plausible patch is found when a representative locally plausible patch passes the
whole-program validation.

\textbf{Results.} 
Column \#P\textsubscript{lp} and \#P\textsubscript{lp-r} highlight the effect of patch clustering.
On average, \tool generated 47 locally plausible patches for each bug, and, courtesy to patch clustering, only an average of 3.6 patches are selected for validation purposes. 
In other words, patch clustering reduced the validation efforts by $\sim$13x
with the validation oracle being invoked 3.6 times on average for each bug instead of 47 times.
The reduction in validation costs benefits not only the automated validation oracles such as static analyzers, but also the human developers who might want to examine the plausible patches. 
We note that \tool did not generate plausible patches for 3 bugs (Bug 1, 15 and 24) in all trials. 
The main reason for not finding plausible patches within the timeout is likely the resulted large search space (Bug 1 and 15). 
This larger search space is due to the relatively higher numbers of fix locations and other patch ingredients.
Besides, \tool failed to generate plausible patches for Bug 24 because its bug trace spans multiple functions, which is not supported by our prototype.


\subsection{RQ3: Effectiveness of Probabilistic Grammar}\label{sec:rq3}

\begin{figure}[t]
\scriptsize
\centering

\includegraphics[width=\columnwidth]{plausible-bar.png}
\caption{Distribution of the average plausible patch count (in log scale) for each bug, for \tool and \tooluni.}
\label{fig:plausible-distribution}
\end{figure}

We next investigate the effects of using a probabilistic grammar to navigate the search space. 
We performed an ablation study by disabling the probability learning in the PCFG.
In other words, the same PCFG with a uniform probability distribution 
is used for both  the patch synthesis and the clustering process.
We refer to this version of \tool as \tooluni (with uniform probability distribution).


\textbf{Results.} The results of evaluating \tooluni are shown in \autoref{tab:efffix-results}, under the columns for \emph{\tooluni}.
Overall, \tooluni generated plausible patches for less number of bugs (19 vs. 23) when considering all five trials, as compared to \tool.
In general, \tooluni failed to generate plausible patches for bugs whose patch requires specific path conditions and pointers (e.g. \texttt{if (a == -1) free(p1);}).
These patches can be sparse in the search space, and a random exploration is less likely to reach the region of plausible patches.
In contrast, \tool  progressively reaches this region by identifying patches in \textit{nearby} regions and biasing the search towards that neighbourhood.
For example, \tool would consider the patch \texttt{if (true) free(p1);} as affecting the bug's memory footprint, and consider the patch \texttt{if (a <= -1) free(p2);} as affecting the bug's path.
By rewarding the production rules used to derive these two patches, the probabilistic grammar can bias the search towards the neighbourhood of plausible patches.

Moreover, the results also show that \tooluni finds lesser locally plausible patches on average, compared to \tool (8.3 vs 47). 
The difference in numbers of plausible patches (that passed whole-program validation)
is then captured in Figure~\ref{fig:plausible-distribution}, which shows the numbers of plausible patches for each bug in log scale.
For 4 bugs with small search spaces (Bug 19, 23, 26, 28) 
the results are similar.
However, overall \tool generated significantly more plausible patches than \tooluni.
This difference is likely due to the search bias: if the search is guided towards regions of plausible patches, more plausible patches would be synthesized within the same time budget.
Higher number of plausible patches also results in a higher number of plausible \textit{regions} being explored: on average, \tool finds 3.6 plausible clusters while \tooluni finds 2.4.
Nonetheless, \tooluni synthesized more patches on average (294 vs. 251)
and created more clusters (65 vs. 54) indicating that it could potentially explore more different regions in the search space.
We note that although it synthesized less patches and explored fewer regions, \tool focused on more plausible regions, thus being more effective.


\subsubsection*{Impact of Probabilities}

\input{figure-snort.tex}

We next share some insights on how the probabilities are being learnt to guide the search. We conducted this study on all bugs in the Snort subject, since \tool has more varying performance among bugs (in terms of \#T\textsubscript{p}) within this subject.
All these eight bugs require a fix on a specific path.
We examine whether the search has been gradually directed towards the neighbourhood of conditional patches in each \tool run.
The probability of entering this region at any timestamp is computed by considering all paths in the PCFG derivation tree leading to this sentential form at that timestamp.
Figure~\ref{fig:probability-distribution} plots the probabilities for entering this region over time, for Bug 11-18.
The solid line shows the median trend across 5 trials, and the shaded region denotes 1st-3rd quantile (25\%-75\%).
The maximum x-axis values vary for each plot, because a different number of fix locations were being considered for each bug, and, for brevity, we only plot one location per bug.
The plots show that, with PCFG, the probability of entering the correct region increased over time, directing the search towards this region for all eight bugs.
Another observation is that, although all increasing, the trends for each bug are increasing at different rates.
In fact, this difference is aligned with the stats in \autoref{tab:efffix-results} (column \#P\textsubscript{lp} under \tool). 
For bugs that reached a high probability early and stayed at high probability for some time (e.g Bug 11, 12, 17), \tool was finding considerably more plausible patches, compared to bugs that have a slower increasing trend.
Overall, the plots demonstrate that PCFG can gradually guide the search towards the plausible regions in the search space.



\subsection{Discussion}
\textbf{Limitations.} \tool fixes memory errors but may change the code's intended functionality.
A human oracle currently checks for plausible but incorrect patches which break functionality beyond changes in the heap's shape. 
To ensure that we do not fix false positive bugs, we chose to build on top of \pulse since it has been shown to be sound, although incomplete. This takes care of one of the concerns with static analysis - where it is known to produce lots of false positives.


\textbf{Threats to Validity.} The benchmarks we chose for evaluation might not be representative for the classes of bugs we tackle, but we had to restrict ourselves to the bugs discovered by \pulse. Also, we evaluate \tool against \saver and \footpatch.
However, these tools each tackle different categories of bugs, e.g. \saver cannot handle NPE but fixes use-after-free and double-free, while \footpatch targets resource leaks too. 
