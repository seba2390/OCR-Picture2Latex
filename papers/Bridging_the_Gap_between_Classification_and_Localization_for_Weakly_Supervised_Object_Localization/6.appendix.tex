\section{Societal Impact}
As deep neural networks require large amounts of data, data-related industries are expanding. One of the prevalent business models of the industries is data annotation. However, the cost of data annotation is burdensome for general users. To reduce the cost, approaches for weakly supervised learning have been proposed, which only requires weaker supervision than fully supervised learning. Since we propose a method of weakly supervised object localization, image-level annotation is sufficient. Our method may threaten the business model of data labeling companies that provide fine-grained labels such as pixel-level annotations and bounding box annotations.

\section{Experimental Details}
We employ SGD optimizer with momentum 0.9 and weight decay $5 \times 10^{-4}$. Following the work of \mbox{Choe~\etal~\cite{choe2020evaluation}}, the networks are divided into two parts, and the learning rate is set differently for those two. VGG16~\cite{simonyan2014very} is divided into old layers and newly added layers when modifying it to VGG16-GAP~\cite{zhou2016learning}, and ResNet50~\cite{he2016deep} is divided into the layers prior to the fourth layer and the others. On the CUB-200-2011 dataset~\cite{welinder2010caltech}, the learning rate is set to $4 \times 10^{-3}$ and $2 \times 10^{-3}$ for the former part of VGG16 and ResNet50, respectively. It is set to $2 \times 10^{-2}$ for the latter part of both backbones. On the ImageNet-1K dataset~\cite{russakovsky2015imagenet}, the learning rate is set to $2 \times 10^{-5}$ and $1 \times 10^{-5}$ for the former part of VGG16 and ResNet50, respectively. It is set to $1 \times 10^{-4}$ for the latter part of both backbones. The proposed method is implemented using PyTorch~\cite{paszke2017automatic}.

Following the work of Choe~\etal~\cite{choe2020evaluation}, we use \texttt{train-fullsup}~\cite{choe2020evaluation} of each dataset as a validation set to select the best model for \texttt{MaxBoxAccV2}~\cite{choe2020evaluation} scores. Please refer to the work of Choe~\etal~\cite{choe2020evaluation} for the details of the dataset.

\section{Additional Results and Discussions}
Additional results and discussions are presented to support the experimental results in the main paper.

\input{fig_tab/fig_supp_maxboxplot}

\subsection{Sensitivity to Bounding Box Threshold}
A threshold is required to draw a bounding box around an object from a continuous localization map. Fig.~\ref{fig:plot_maxbox} shows that the change of localization accuracy with various IoUs when varying the threshold. In each plot with IoU $\delta$, the maximum value becomes \texttt{MaxBoxAccV2($\delta$)} score. For all $\delta$, the curve of our method is consistently above the curve of the vanilla method, which shows that the superiority of the localization performance of our method does not depend on the threshold. When $\delta$ is 0.3 and 0.5, the curve of our method nearby a maximum value is flatter than the curve of the vanilla method. This shows that our method is less sensitive to the threshold for a bounding box than the vanilla method. When $\delta$ is 0.7, our method is sharper than the vanilla method, but the localization accuracy of our method is more than twice that of the vanilla method, so the flatness comparison is meaningless.

\subsection{Feature Direction Alignment}
Fig.~\ref{fig:supp_sim} shows some examples of CAM, $\mathcal{F}$, and $\mathcal{S}$ from the vanilla method and our method on the CUB-200-2011 and ImageNet-1K datasets. In $\mathcal{S}$ from the vanilla method, the overall values are similar and some regions that belong to the object have low values. For instance, in the `car' example (in the second row and second column of the ImageNet-1K dataset), the middle part of the object has low similarity, resulting in low activation in the CAM. Different from the vanilla method, the values of $\mathcal{S}$ from our method are high in the object regions and low in the background regions. Furthermore, the values of $\mathcal{F}$ are high across the entire object region. This makes the CAM that captures more object region.

\subsection{Consistency with Attentive Dropout}
We compare consistency with attentive dropout with EIL~\cite{mai2020erasing}, the most recent one among the previous erasing methods~\cite{choe2019attention, mai2020erasing,zhang2018adversarial}, on the CUB-200-2011 using VGG16. As mentioned in the main paper, attentive dropout directly regularizes feature activation, whereas EIL indirectly influences the activation through class prediction. Fig.~\ref{fig:hist_max} shows the different effect on feature activation of consistency with attentive dropout and EIL. To encourage a model to predict correct class without highly activated region, EIL enhances the maximum value. In contrast, consistency with attentive dropout reduces the maximum value through regularization. As a result, attentive dropout distributes the activations more effectively than EIL (Fig.~\ref{fig:hist}(b) in the main paper).

\input{fig_tab/fig_supp_hist}

\subsection{Localization Results}
Fig.~\ref{fig:supp_cam} compares the localization results from the vanilla method~\cite{zhou2016learning} and our method on the CUB-200-2011 and ImageNet-1K datasets. While the vanilla method misses the less discriminative parts, \eg, wings and tails of birds and bodies of animals, our method successfully captures the entire object region.

\subsection{Sensitivity to Hyperparameters}
In the main paper, we mention as a limitation that there are several hyperparameters to be decided in our method. We provide further analysis with a different dataset and backbone than that presented in the main paper.

\noindent\textbf{CUB-200-2011 on ResNet50.}
We find the best localization performance at 0.6 for $\lambda_\text{sim}$, 0.07 for $\lambda_\text{norm}$, and 2 for $\lambda_\text{drop}$, respectively. The thresholds $\tau_\text{fg}$ and $\tau_\text{bg}$ for $\mathcal{L}_\text{sim}$ are set to 0.4 and 0.2, respectively. The hyperparameters $\gamma$ and $p$ for $\mathcal{L}_\text{drop}$ are set to 0.8 and 0.25, respectively.
Fig.~\ref{fig:supp_plot_hyperparams}(a) shows the change of GT Loc when varying the hyperparameters. The sensitivity to each hyperparameter is similar to that on the CUB-200-2011 dataset using VGG16 as a backbone. $\lambda_\text{sim}$ affects the localization performance the most among hyperparameters.

\noindent\textbf{ImageNet-1K on VGG16.}
The best localization performance is found at 0.5 for $\lambda_\text{sim}$, 0.2 for $\lambda_\text{norm}$, and 3 for $\lambda_\text{drop}$, respectively. The hyperparameters $\tau_\text{fg}$, $\tau_\text{bg}$, $\gamma$, and $p$ are set to 0.5, 0.3, 0.8, and 0.5, respectively.
Fig.~\ref{fig:supp_plot_hyperparams}(b) shows the GT Loc at various hyperparameter values. Different from the CUB-200-2011 dataset, the localization performance is most affected by $\lambda_\text{drop}$. The sensitivities to the other hyperparameters are similar to those with a different dataset and backbone.

\input{fig_tab/tab_supp_openimage}

\noindent\textbf{Discussion.}
The hyperparameters are set differently on the two datasets. This is because their tasks are somewhat different; the classification on the CUB-200-2011 dataset is a fine-grained classification. We additionally evaluate our methods on OpenImages30K~\cite{benenson2019large, choe2020evaluation}, where the task is similar to that on the ImageNet-1K dataset. We use VGG16 as a backbone and set the hyperparameters the same as those on the ImageNet-1K dataset. Note that the OpenImages30K dataset is annotated with a mask, and we use \texttt{PxAP} metric for evaluation, following the work of Choe~\etal~\cite{choe2020evaluation}. As shown in the Tab.~\ref{tab:openimage_maxbox}, our method outperforms the recent methods by a large margin on the OpenImages30K dataset as well, which shows the hyperparameters used on the ImageNet-1K dataset can be applied successfully to a different dataset.

\clearpage
\input{fig_tab/fig_supp_sim}
\input{fig_tab/fig_supp_cam}
\input{fig_tab/fig_supp_hyperparams}