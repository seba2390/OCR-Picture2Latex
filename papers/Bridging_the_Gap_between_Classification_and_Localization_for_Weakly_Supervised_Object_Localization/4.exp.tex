\section{Experiments}

\subsection{Experimental Settings}
\noindent\textbf{Datasets.}
We evaluate our method on two popular benchmarks: CUB-200-2011~\cite{welinder2010caltech} and ImageNet-1K~\cite{russakovsky2015imagenet}.
In the CUB-200-2011 dataset, there are 5,994 images for training and 5,794 for testing from 200 bird species. In the ImageNet-1K, there are approximately 1.3 million images in the training set and 50,000 in the validation set from 1,000 different classes.

\noindent\textbf{Evaluation Metrics.}
Following the work of Russakovsky~\etal~\cite{russakovsky2015imagenet}, we use Top-1 localization accuracy (Top-1 Loc), Top-5 localization accuracy (Top-5 Loc), and localization accuracy with ground-truth class (GT Loc) as our evaluation metrics.
Top-$k$ Loc is the proportion of the images whose predicted bounding box has more than 50\% intersection over union (IoU) with the ground-truth bounding box and whose predicted top-$k$ classes include the ground-truth class.
GT Loc is the localization accuracy with the ground-truth class, which does not consider the classification result.
We also use \texttt{MaxBoxAccV2}~\cite{choe2020evaluation} to evaluate our method.
\texttt{MaxBoxAccV2}$(\delta)$ measures the localization accuracy with ground-truth class with multiple IoU thresholds $\delta\in\{0.3, 0.5, 0.7\}$.

\input{fig_tab/fig_comparison_cam}
\input{fig_tab/tab_cub_vgg}

\noindent\textbf{Implementation Details.}
We evaluate our method using VGG16~\cite{simonyan2014very} and ResNet50~\cite{he2016deep} as backbone networks.
For VGG16, we adopt the GAP layer following the training settings of the previous work~\cite{zhou2016learning}.
For ResNet50, we set the stride of the third layer to 1.
The attentive dropout is applied before the last pooling layer in VGG16 and after the first block in the fourth layer in ResNet50.
We initialize the networks with the pretrained weights using ImageNet-1K~\cite{russakovsky2015imagenet}. We use a min-max normalization to draw the bounding box from the generated CAM.

\subsection{Comparison with State-of-the-art Methods}
We compare our method to the recent WSOL methods. For other WSOL methods, we report the localization performance of the original papers or that reproduced by \cite{choe2020evaluation,kim2021normalization,bae2020rethinking,tan2020dual}\footnote{https://github.com/clovaai/wsolevaluation}. Our method consistently outperforms existing WSOL methods using a single branch, across the datasets and the backbones by a large margin.

Tab.~\ref{tab:cub_top1loc_vgg} shows the localization performance on the CUB-200-2011~\cite{welinder2010caltech} test set, using VGG16 as a backbone. Our method achieves an 11.87\%p improvement in Top-1 Loc and a 16.87\%p improvement in GT Loc over the work of Bae~\etal~\cite{bae2020rethinking}, which is the state-of-the-art method among the CAM-based methods.
Furthermore, our method outperforms the methods adopting an additional branch for localization. Our method improves Top-1 Loc by 1.57\%p and GT-Loc by 3.91\%p improvement in GT Loc compared to FAM~\cite{meng2021foreground}.

Tab.~\ref{tab:cub_top1loc} shows the results using ResNet50 as a backbone. It shows that our method consistently outperforms the existing methods by a large margin ($>$13\%p), using a different backbone.
\textcolor{black}{Tab.~\ref{tab:imagenet_top1loc} shows the localization performance on the ImageNet-1K~\cite{russakovsky2015imagenet} validation set, based on VGG16 and ResNet50.
Our method achieves the state-of-the-art performance in the ImageNet-1K dataset regardless of the backbone, and only Top-1 Loc with ResNet50 is the second best after I$^2$C with a marginal difference.}

Additionally, we compare our \texttt{MaxBoxAccV2}~\cite{choe2020evaluation} scores with other state-of-the-art methods on the CUB-200-2011 and ImageNet-1K in Tab.~\ref{tab:total_maxbox}.
It shows that our method outperforms the most recent methods by a large margin for all IoU thresholds with various backbones and datasets. Especially, our method improves the score with IoU threshold of 0.7, which is strict accuracy, by 21.0\%p and 17.4\%p with VGG16 and ResNet50 on the CUB-200-2011 dataset, respectively, compared with the work of Ki~\etal~\cite{ki2020sample}.

\input{fig_tab/tab_cub_res}

Fig.~\ref{fig:compare_cam} shows some examples of localization results from the vanilla method~\cite{zhou2016learning} and from our method on the CUB-200-2011 and ImageNet-1K datasets. It shows that the model trained with our method captures the target object region more accurately than the vanilla model. On the CUB-200-2011 dataset, while the vanilla model fails to identify the tails, legs, and wings of birds, the classifier trained with our method successfully identifies them.

\input{fig_tab/tab_imagenet}
\input{fig_tab/tab_maxbox_total}
\input{fig_tab/fig_comparison_sim}

\subsection{Discussion}
\noindent\textbf{Feature Direction Alignment.}
Through the feature direction alignment, we force $\mathcal{S}$ and $\hat{\mathcal{F}}$ to be high in the object region and to be low in the background region. As Fig.~\ref{fig:compare_sim} shows, the classifier trained with our method yields $\mathcal{S}$ that has a high value in the object region and low value in the background region, different from the vanilla model. It also generates $\hat{\mathcal{F}}$ that has higher activation in less discriminative parts than the vanilla model does.
This makes CAM successfully identify the entire object region. As mentioned in Sec.~\ref{sec:feature_directions}, the feature direction alignment makes $\hat{\mathcal{F}}$ and $\mathcal{S}$ similar, resulting that CAM becomes also similar with them.
We generate a localization map with $\mathcal{F}$ and $\mathcal{S}$ and evaluate the localization performance for each case. We use a min-max normalization when drawing bounding boxes from $\mathcal{F}$. Since negative values in $\mathcal{S}$ denote the background region, we apply a max-normalization on $\mathcal{S}$. Tab.~\ref{tab:perf_sim_norm_cam} shows that the localization results with $\mathcal{F}$ and $\mathcal{S}$ also achieve similar localization performance with CAM. This proves the coincidence between CAM, $\mathcal{F}$, and $\mathcal{S}$ with our method.

Fig.~\ref{fig:hist}(a) shows the distribution of $\mathcal{S}_u$ inside the ground truth bounding boxes from the vanilla method and our method. Note that the bounding boxes include not only the target object but also the background region.
As the training progresses with our method, the similarity gradually splits into negative and large positive values.
This shows that our method effectively increases the similarity for the foreground region and decreases it for the background region.
In contrast, for the vanilla method, the similarity is clustered in small positive values, making no distinction between \mbox{the two}.

\input{fig_tab/tab_perf_sim_norm_cam}
\input{fig_tab/fig_hist}

\noindent\textbf{Consistency with Attentive Dropout.}
Fig.~\ref{fig:hist}(b) compares the effect of our consistency with attentive dropout on the distributions of $\hat{\mathcal{F}}_u$ with the vanilla method and EIL~\cite{mai2020erasing}, the state-of-the-art erasing WSOL method.
Here, the feature direction alignment with $\mathcal{L}_\text{sim}$ and $\mathcal{L}_\text{norm}$ is not applied.
With the vanilla training, most of $\hat{\mathcal{F}}_u$ are very low.
With EIL, overall $\hat{\mathcal{F}}_u$ increase compared with the vanilla method, implying that less discriminative parts become to be highly activated.
With consistency with attentive dropout, the distribution of $\hat{\mathcal{F}}_u$ shifts even more to the right.
This indirectly shows that our proposed method, consistency with attentive dropout, distributes the activation more over the target object region than the other methods. This results that the consistency with attentive dropout achieves higher performance than EIL when used along with feature direction alignment, as shown in Tab.~\ref{tab:compare_eil}. We provide a more detailed analysis in appendix.

\input{fig_tab/tab_compare_eil}

\subsection{Ablation Study}\label{sec:ablation}
We perform a series of ablation studies on the CUB-200-2011 dataset using VGG16 as the backbone.

\noindent\textbf{Effect of Each Component.}
Tab.~\ref{tab:ablation} shows the localization performance of the classifier trained with and without each loss term.
Compared to the performance without the proposed loss terms, $\mathcal{L}_\text{drop}$ improves the Top-1 Loc by 7.4\%p and GT Loc by 14.32\%p.
The feature direction alignment using only $\mathcal{L}_\text{sim}$ improves the Top-1 Loc by 9.71\%p and GT Loc by 15.36\%p, which shows the largest improvement among the components.
Adopting $\mathcal{L}_\text{norm}$ improves all metrics more than 5\%p. The feature direction alignment using both $\mathcal{L}_\text{sim}$ and $\mathcal{L}_\text{norm}$ achieves 62.27\% of Top-1 Loc and 81.93\% of GT Loc, which is higher than the performance reported by Pan~\etal~\cite{pan2021unveiling}.
Adoption of all components shows the best performance in all metrics.

\noindent\textbf{Sensitivity to Hyperparameters.}
We analyze the effect of the balancing factors in the loss and the hyperparameters of each loss.

For the balancing factors in loss, we find the best localization performance at 0.5 for $\lambda_\text{sim}$, 0.15 for $\lambda_\text{norm}$, and 3 for $\lambda_\text{drop}$, respectively.
As shown in Fig.~\ref{fig:hyperparams}(a), the localization performance is most sensitively affected by $\lambda_\text{sim}$. $\lambda_\text{norm}$ insignificantly changes the performance.
The performance tends to decrease when the constraint with $\lambda_\text{drop}$ becomes too strong as 4.
\input{fig_tab/tab_ablation}
\input{fig_tab/fig_hyperparams}
For the hyperparameters of the feature direction alignment, we set $\tau_\text{fg}$ and $\tau_\text{bg}$ for $\mathcal{L}_\text{sim}$ to 0.6 and 0.1, respectively.
They determine the coarse foreground and background regions.
Fig.~\ref{fig:hyperparams}(b) shows that varying those thresholds has little effect on the performance.
The hyperparameters $\gamma$ and $p$ determine the drop of the activation in the intermediate feature map. $\gamma$ and $p$ for $\mathcal{L}_\text{drop}$ are set to 0.8 and 0.5, respectively.
When $\gamma$ is moderately large between 0.7 and 0.9, there is no significant change in the performance, but when $\gamma$ is too low, \ie, 0.6, the performance decreases.
From the results with various $p$, we observe that stochastic dropout produces little change of GT Loc regardless of the drop probability, but deterministic dropout with a probability of 1.0 yields a significant drop in the localization performance. This indicates that less but sufficient discriminative information should be maintained for a good localization performance.