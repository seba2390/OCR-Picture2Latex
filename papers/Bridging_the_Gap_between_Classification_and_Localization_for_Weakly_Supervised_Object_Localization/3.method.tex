\section{Finding the Gap with CAM Decomposition}
Given an input image $x$ and a typical image classifier comprising convolutional layers and a GAP followed by an FC layer, a CAM for target class $c$ is computed as follows:
\begin{equation}\label{eq:cam}
\texttt{CAM}(x) = \mathbf{w}^\intercal_c F(x).
\end{equation}
$F(x)\in\mathbb{R}^{H \times W \times D}$ is the feature map before the GAP, and $\mathbf{w}_c\in\mathbb{R}^{D}$ is the weight of the FC layer connected to class $c$, where $H$, $W$, and $D$ are the height, width, and dimension, respectively.
Eq.~\ref{eq:cam} implies that the value of CAM at each spatial location is the dot product of two vectors, $\mathbf{w}_c$ and $F_u(x)$, where $u\in\{1, ..., HW\}$ is the index of spatial location.
It can be decomposed as follows:
\begin{equation}\label{eq:cam_each}
\begin{aligned}
\texttt{CAM}_u(x) = & \mathbf{w}_c \cdot F_u(x) \\
= & \|\mathbf{w}_c\|\|F_u(x)\| \underbrace{\frac{\mathbf{w}_c \cdot F_u(x)}{\|\mathbf{w}_c\|\|F_u(x)\|}}_{\textstyle S(\mathbf{w}_c,F_u(x))},
\end{aligned}
\end{equation}
where $S(\mathbf{a},\mathbf{b})$ is the cosine similarity between the two vectors, $\mathbf{a}$ and $\mathbf{b}$.
When generating a CAM, target class $c$ is fixed and $\|\mathbf{w}_c\|$ is the same for every $u$.
The CAM value at each position can now be interpreted as the product of the norm of the feature vector at the corresponding location and the similarity between the feature vector and class-specific weight vector.
Let $\mathcal{F}\in\mathbb{R}^{H \times W}$ and $\mathcal{S}\in\mathbb{R}^{H \times W}$ be the norm map and the similarity map, respectively, where $\mathcal{F}_u=\|F_u\|$ and $\mathcal{S}_u=S(\mathbf{w}_c, F_u(x))$. Subsequently, CAM can be rewritten as
\begin{equation}\label{eq:cam_abb}
\texttt{CAM}(x) = \|\mathbf{w}_c\|\cdot\mathcal{F}\odot\mathcal{S}.
\end{equation}
To localize the target object accurately, both $\mathcal{F}_u$ and $\mathcal{S}_u$ should be large for $u$ belonging to the object.

Likewise, the classification score can be interpreted with the output of the GAP, $f(x)=\text{GAP}(F(x))\in\mathbb{R}^{\rm{D}}$.
\begin{equation}\label{eq:logit}
\begin{aligned}
\texttt{logit}_c(x) = & \mathbf{w}_c \cdot f(x) \\
= & \|\mathbf{w}_c\|\left\lVert f(x)\right\rVert S\left( \mathbf{w}_c,f(x)\right).
\end{aligned}
\end{equation}
Because $\left\lVert f(x)\right\rVert$ is fixed for $x$, $\|\mathbf{w}_c\|$ and $S\left( \mathbf{w}_c,f(x)\right)$ determine the logit score of each class $c$.
The scale variation of $\|\mathbf{w}_c\|$ across classes is not very large.
Therefore, to classify $x$ correctly, $S(\mathbf{w}_c, f(x))$ must be large for the ground truth class $c$.
Here exists the gap between classification and localization.
The classifier is trained to increase $S(\mathbf{w}_c, f(x))$, not $S(\mathbf{w}_c, F_u(x))$ for $u$ belonging to an object region. Cosine similarity is interpreted as the degree of alignment between the directions of the two vectors, meaning that the input feature vector at the object region and class-specific weight vector are not ensured to be aligned with training only for classification.
This causes the model to fail to localize the entire object in a CAM.

Fig.~\ref{fig:cam_norm_sim} shows some examples of norm map $\mathcal{F}$, similarity map $\mathcal{S}$, and CAM from a vanilla model.
The less discriminative but object-belonging regions also have noticeably high activation in $\mathcal{F}$, including wings and bodies of birds. 
However, those regions are not activated in the final CAMs, due to the small values in $\mathcal{S}$.
Although $\mathcal{F}$ contains considerable information for localization, its effect diminishes because of the misalignment of the feature directions with the class-specific weight.

In the next section, we propose a method to bridge the gap between classification and localization by aligning feature directions: adjusting the cosine similarity between input features and class-specific weights. 


\section{Bridging the Gap through Alignment}
We describe how to align feature directions in Sec.~\ref{sec:feature_directions}. An additional strategy to enhance the effect of the feature direction alignment, consistency with attentive dropout, is introduced in Sec.~\ref{sec:consistency_drop}. In Sec.~\ref{sec:training_scheme}, we describe the overall training scheme.
Fig.~\ref{fig:overall} shows the overview of our proposed method.


\subsection{Alignment of Feature Directions}\label{sec:feature_directions}
To enhance the activation of the entire object region in CAM, we want the cosine similarity between $F_u$ and $\mathbf{w}_{c}$ to be high for $u$ belonging to the target object and low for the background region.
Because high activation in $\mathcal{F}$ implies that there is a cue for classification at the corresponding location, we divide the region of the feature map into coarse foreground region $\mathcal{R}^\text{norm}_\text{fg}$ and background region $\mathcal{R}^\text{norm}_\text{bg}$ based on a normalized $\mathcal{F}$.
\begin{equation}
\begin{aligned}
&\mathcal{R}^\text{norm}_\text{fg}=\{u|\hat{\mathcal{F}}_u>\tau_\text{fg}\},\\
&\mathcal{R}^\text{norm}_\text{bg}=\{u|\hat{\mathcal{F}}_u<\tau_\text{bg}\},\\
&\text{where}~\hat{\mathcal{F}}=\frac{\mathcal{F}-\min_{i}{\mathcal{F}_i}}{\max_{i}{\mathcal{F}_i}-\min_{i}{\mathcal{F}_i}}.
\end{aligned}
\end{equation}
$\tau_\text{fg}$ and $\tau_\text{bg}$ are constant thresholds that determine the foreground and background regions, respectively. Note that $\tau_\text{fg}$ and $\tau_\text{bg}$ are not the same; therefore, there is an unknown region that is not included in either $\mathcal{R}^\text{norm}_\text{fg}$ or $\mathcal{R}^\text{norm}_\text{bg}$.
To increase $\mathcal{S}_u$ in $\mathcal{R}^\text{norm}_\text{fg}$ and suppress it in $\mathcal{R}^\text{norm}_\text{bg}$, we define the similarity loss as follows:
\begin{equation}\label{eq:loss_sim}
\begin{aligned}
\mathcal{L}_\text{sim} = -\frac{1}{|\mathcal{R}^\text{norm}_\text{fg}|}\sum_{u\in \mathcal{R}^\text{norm}_\text{fg}}{\mathcal{S}_u} +\frac{1}{|\mathcal{R}^\text{norm}_\text{bg}|}\sum_{u\in \mathcal{R}^\text{norm}_\text{bg}}{\mathcal{S}_u}.
\end{aligned}
\end{equation}

There still remains a possibility that some parts of the object region have low activation in $\hat{\mathcal{F}}$.
In this case, $\mathcal{L}_\text{sim}$ may not be sufficient for the alignment.
Therefore, we introduce an additional loss term to increase $\hat{\mathcal{F}}$ in every candidate region belonging to the target object.
Because a positive $\mathcal{S}_u$ indicates that $u$ is making a positive contribution to increasing the classification logit, the regions with positive similarity can be treated as candidates for the object region. Therefore, we force this area to be activated.
We estimate the object region, $\mathcal{R}^\text{sim}_\text{fg}$, and background region, $\mathcal{R}^\text{sim}_\text{bg}$, based on $\mathcal{S}_u$ as
\begin{equation}
\begin{aligned}
&\mathcal{R}^\text{sim}_\text{fg}=\{u|\mathcal{S}_u>0\},\\
&\mathcal{R}^\text{sim}_\text{bg}=\{u|\mathcal{S}_u<0\}.
\end{aligned}
\end{equation}
With each estimated region, we define the norm loss in a manner similar to Eq.~\ref{eq:loss_sim}, as follows:
\begin{equation}\label{eq:loss_norm}
\begin{aligned}
\mathcal{L}_\text{norm} = -\frac{1}{|\mathcal{\mathcal{R}^\text{sim}_\text{fg}}|}\sum_{u\in \mathcal{R}^\text{sim}_\text{fg}}{\hat{\mathcal{F}}_u} +\frac{1}{|\mathcal{R}^\text{sim}_\text{bg}|}\sum_{u\in \mathcal{R}^\text{sim}_\text{bg}}{\hat{\mathcal{F}}_u}.
\end{aligned}
\end{equation}

For fine-grained classification, such as bird species classification, the object to be recognized is the same across classes. In this case, we define the region with a non-positive similarity with any class as $\mathcal{R}^\text{sim}_\text{bg}$ and the other as $\mathcal{R}^\text{sim}_\text{fg}$. In general, the regions $\mathcal{R}^\text{sim}_\text{bg}$ and $\mathcal{R}^\text{sim}_\text{fg}$ are defined with a similarity with a target class.

The two loss terms $\mathcal{L}_\text{sim}$ and $\mathcal{L}_\text{norm}$ operate complementary.
Through the minimization of $\mathcal{L}_\text{sim}$, the value of $\mathcal{S}$ in the region that is highly activated in $\hat{\mathcal{F}}$ increases.
Through the minimization of $\mathcal{L}_\text{norm}$, the value of $\hat{\mathcal{F}}$ in the region with high similarity increases.
After the joint minimization of $\mathcal{L}_\text{sim}$ and $\mathcal{L}_\text{norm}$, the activated region in $\hat{\mathcal{F}}$  and that in $\mathcal{S}$ become similar.


\subsection{Consistency with Attentive Dropout}\label{sec:consistency_drop}
We can expect the successful alignment by $\mathcal{L}_\text{sim}$ when the estimation of $\mathcal{R}^\text{norm}_\text{fg}$ and $\mathcal{R}^\text{norm}_\text{bg}$ is accurate: $\hat{\mathcal{F}}$ is consistently large over the entire object region and small over the background region.
\input{fig_tab/fig_dropconsistency}
Because the value of $\mathcal{F}$ at the most discriminative region is significantly larger than that at the other region, the value of the normalized map $\hat{\mathcal{F}}$ at the less discriminative part but belonging to the object region becomes small.

We introduce consistency with attentive dropout, a method to distribute the activation to the target object region.
We adopt $L_1$ loss between the two feature maps $F$ and $F_\text{drop}$: $F$ is the feedforward result of an intermediate feature map $F'$, and $F_\text{drop}$ is the feedforward result of $F'_\text{drop}$ obtained by intentionally dropping large activations from $F'$.
Fig.~\ref{fig:dropconsistency} shows the overall process of obtaining $F'_\text{drop}$ for consistency with attentive dropout.
In $F'$, the activation at the spatial location whose channel-wise averaged activation is larger than $\gamma$ is dropped with probability $p$. The stochastic dropout prevents all information in the highly activated area from being eliminated. The loss for consistency with attentive dropout is as follows:
\begin{equation}\label{eq:loss_er}
\mathcal{L}_\text{drop} = \|F(x)- F_\text{drop}(x)\|_{1}.
\end{equation}

There have been several attempts that utilize a similar erasing mechanism~\cite{mai2020erasing,choe2019attention,zhang2018adversarial}.
They train a classifier to preserve the predicted labels before and after erasing highly activated features.
In contrast, our method explicitly regularizes a model to yield a similar feature map even after the highly activated features are dropped.
This decreases the dependency on the dropped features, resulting in more evenly distributed activation compared to the other methods.

\subsection{Training Scheme}\label{sec:training_scheme}
With cross-entropy loss for classification, $\mathcal{L}_\text{CE}$, the total cost function is defined as follows:
\begin{equation}\label{eq:loss_tot_step2}
\mathcal{L_\text{total}} = \mathcal{L}_\text{CE} + \lambda_\text{drop} \mathcal{L}_\text{drop} + \lambda_\text{sim} \mathcal{L}_\text{sim} + \lambda_\text{norm} \mathcal{L}_\text{norm},
\end{equation}
where $\lambda_\text{drop}$, $\lambda_\text{sim}$, and $\lambda_\text{norm}$ are hyperparameters for balancing the losses.
The feature direction alignment is better applied after training the classifier to some extent to obtain a suitable feature map for classification.
Thus, for the first few epochs (\ie, the warm stage), we train a model only with $\mathcal{L}_\text{CE}$ and $\mathcal{L}_\text{drop}$:
\vspace{-1pt}
\begin{equation}\label{eq:loss_tot_step1}
\mathcal{L_\text{warm}} = \mathcal{L}_\text{CE} + \lambda_\text{drop} \mathcal{L}_\text{drop}.
\end{equation}