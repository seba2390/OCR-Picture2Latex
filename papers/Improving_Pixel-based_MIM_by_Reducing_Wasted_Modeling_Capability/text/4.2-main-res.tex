\subsection{Main Results}
\label{sec:main_results}
\input{table/main_results}
The application of multi-level feature fusion to MAE \cite{MAE} and PixMIM \cite{pixmim} resulted in significant improvements in various downstream tasks, as shown in \autoref{tab:comparison}. After pre-training the model for 300 epochs, we achieve a 0.5\%, 1.8\%, and 3.6\% improvement over MAE in fine-tuning, linear probing, and semantic segmentation, respectively. Additionally, our model exhibits scalability across pre-training epochs and consistently outperforms these base methods by a substantial margin. Compared to these methods, using an extra heavy tokenizer, \eg CLIP, we also gradually close the performance gap with them. Although fine-tuning accuracy is often considered a reliable measure of the quality of non-linear features in a model, we find that it is not a sensitive metric, as compared to other metrics presented in \autoref{tab:comparison}. This may be attributed to pre-training and fine-tuning following the same data distribution, and the size of the training set and model capacity being sufficient to offset the performance gap between different methods. To address this limitation, we adopt the following two workarounds:
\input{fig/vit-s}
\paragraph{Low-shot fine-tuning.} This protocol has also been adopted by many previous works, \eg \cite{simclr}. Rather than utilizing the entire training set, we fine-tune the pre-trained model end-to-end using only 1\% and 10\% of the training set. As indicated by \autoref{tab:comparison}, the performance gap between MFF and the base methods is much more prominent when using low-shot fine-tuning, which further verifies the effectiveness of MFF.

\paragraph{Pre-train with ViT-S.} To mitigate the influence brought by model capacity, we pre-train MAE using ViT-S and compare their performance using fine-tuning, linear probing, and semantic segmentation. Since our objective is to evaluate the improvement brought by MFF to these base methods, we do not specifically tune hyper-parameters for the experiments with ViT-S to achieve state-of-the-art performance, but rather use the same settings as for ViT-B. Due to its smaller capacity compared to ViT-B, ViT-S requires a pre-training method that can effectively capture semantic features to perform well on downstream tasks. As demonstrated in \autoref{fig:vit-s}, the method with MFF significantly outperforms their base method, further validating the effectiveness of MFF.

We also evaluate our pre-trained models with the object detection protocol and report the AP$^\text{box}$ and AP$^\text{mask}$. As shown in \autoref{tab:detection}, \ourmethod can still bring non-trivial improvements for object detection.
\input{table/detection}


