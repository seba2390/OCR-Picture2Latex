\appendix

\section{Appendix}

\subsection{Pre-training}

The settings for pre-training strictly follows those in MAE\cite{MAE} and PixMIM\cite{pixmim}, with details shown below:
\input{appendix/pre-train}

\subsection{Fine-tuning and linear probing}
We also stick to the settings in MAE\cite{MAE} for the ViT-B\cite{ViT} model concerning fine-tuning and linear probing. Since our objective is to measure the enhancement brought by MFF and not attain the state-of-the-art (SOTA) performance, we employ the same settings as ViT-B without any specific adjustments for ViT-S.

\input{appendix/linear_prob}
\input{appendix/fine-tuning}

\subsection{Object detection and segmentation in COCO}
All these settings also strictly follow those in MAE\cite{MAE} but choose the commonly used 2$\times$ settings, which fine-tunes the model on COCO\cite{coco} for 25 epochs.

\subsection{Semantic segmentation in ADE20K}
We stick to the settings used in MAE\cite{MAE} and PixMIM\cite{pixmim}, fine-tuning the pre-trained model end-to-end for 16k iterations with a batch size of 16.

\subsection{Selected indices of the ablation study}
Inspired by the results of the pilot experiment depicted in Figure 1 of the main paper, we choose layer$_0$ as the shallow layer, and layer$_{10}$ as the deep layer for the ablation experiment outlined in Table 3(a). Additionally, for ablation study in Table 3(b), we have selected additional two, four, and ten layers, evenly distributed between layer$_0$ and the output layer (layer$_{11}$). The detailed indices for Table 3(b) is shown in the \autoref{tab:indices}. 

\begin{table}[h]
\centering
\tabcolsep 1.5pt
\begin{tabular}{cc}
num layers & indices \\
\shline
1 & 11 \\
2 & 0,11 \\
4 & 0,4,8,11 \\
6 & 0,2,4,6,8,11 \\ 
12 & 0,1,2,3,4,5,6,7,8,9,10,11 \\
\end{tabular}
\vspace{-0.5em}
\caption{\textbf{Detailed indices for Table 3(b) of the main paper.} We try to make the additionally selected indices are evenly distributed between the first layer and last layer.}
\label{tab:indices}
\end{table}

In addition, similar to the pilot experiment in Figure 1 of the main paper, we observe the weight for each layer of all experiments in Table 3(b) of the main paper. Just as shown in \autoref{fig:layer_weights}, no matter in which case, the model increasing relies on these shallow layers for the reconstruction tasks, indicating the significance of injecting low-level information into the output layer.
\input{appendix/figure/weight}


\subsection{Transfer learning}
We also study transfer learning where we pre-train on ImageNet-1K and fine-tune on several smaller datasets. We follow the training recipe and protocol in DINO\cite{dino}. \ourmethod$_\text{MAE}$ consistently outperforms MAE on CIFAR10, CIFAR100, and Stanford Cars. As shown in the following table, \ourmethod$_\text{MAE}$ consistently improves MAE on all datasets.

\begin{table}[h]
\centering
% \scalebox{1}{
\tabcolsep 1.5pt
\begin{tabular}{lllll}
Method& Epoch & CIFAR10 & CIFAR100 & Cars\\
\shline
MAE & 800 & 98.4 & 89.4 & 94.3 \\
\ourmethod$_\text{MAE}$ & 800 & 98.6 \more{(+0.2)} & 90.3 \more{(+0.9)} & 94.7 \more{(+0.4)} \\
\end{tabular}
\vspace{-0.5em}
\caption{\textbf{Transfer learning on smaller datasets.}}
\label{tab:transfer}
\end{table}

\subsection{Feature-based MIM does not Suffer from being Biased toward Low-level Feature}
To supplement the findings in \autoref{fig:weight_vs_layer}, we apply multi-level feature fusion (MFF) to EVA\cite{EVA} and MILAN\cite{MILAN}, and evaluate their performance with linear probing, fine-tuning and semantic segmentation. Detailed results are shown below:

\begin{table}[h]
\centering
\tabcolsep 1.5pt
\begin{tabular}{llccc}
Method&Epoch&lin&seg&ft                               \\
\shline
EVA                & 400 & 69.0 & 49.5 & 83.7         \\
MFF$_\text{EVA}$   & 400 & 68.9  & 49.4  & 83.8       \\
MILAN                & 400 & 79.9 & 52.7 & 85.4       \\
MFF$_\text{MILAN}$   & 400 & 79.7  & 52.9  & 85.0     \\
\end{tabular}
\end{table}

\noindent As shown in the table above, MFF brings marginal improvements to feature-based MIMs, consistent with the findings in \autoref{fig:weight_vs_layer}. 

\subsection{The Effect of Deep Supervision}
To exclude the influence of deep supervision\cite{ren2023deepmim}, we detach all shallow layers before fusing with the last layer (MFF$_\text{MAE}^\text{detach}$), ensuring that gradients do not propagate through these shortcuts to the shallow layers. As shown in the table below, deep supervision alone does not improve MAE, and MFF's improvements come from alleviating the problem of being biased toward high-freq components.

\input{table/deep_supervision}







 







