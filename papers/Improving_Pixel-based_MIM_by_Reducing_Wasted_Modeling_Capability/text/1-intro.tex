\section{Introduction}
\label{sec:intro}
Self-supervised learning (SSL) has made remarkable progress in language and computer vision. Masked Image Modeling (MIM) is an effective framework in image SSL, which boasts a simple training pipeline, a few handcrafted data augmentations, and high performance across downstream tasks. In the pioneering work of BEiT~\cite{BEiT}, 40\% of the input image is masked, and the model is trained to capture the semantics of the masked patches by reconstructing the DALL-E~\cite{DALLE} output features. To simplify pre-training and reduce computational overhead, MAE~\cite{MAE} only feeds the visible tokens into the encoder and encourages the decoder to reconstruct the raw pixels of masked patches. More recently, follow-up works have focused on adding auxiliary tasks or using large-scale pre-trained models to produce reconstruction targets. For instance, CMAE~\cite{CMAE} explicitly adds a contrastive task and optimizes it in conjunction with the MIM task, while MILAN~\cite{MILAN} and BEiT-v2~\cite{BEiTv2} employ multimodal pre-trained models such as CLIP~\cite{CLIP} to generate the reconstruction features. 
\input{fig/teaser}
Among the various MIM methods available, pixel-based approaches such as MAE \cite{MAE} are particularly interesting because of their simple pre-training pipeline and minimal computational overhead. However, these methods are typically biased towards capturing high-frequency details due to their emphasis on reconstructing raw pixels \cite{BEiT, pixmim}. As a result, they waste a significant amount of modeling capability that could be better utilized to capture low-frequency semantics. Our objective is to reduce this waste of modeling capacity, aiming for an improved quality of learned representation for downstream visual tasks. Toward this goal, we design two pilot experiments based on the representative work of MAE \cite{MAE} to uncover its neglected design aspects.

\begin{enumerate}[label={\bf {{(\arabic*)}}},leftmargin=*,topsep=0.5ex,itemsep=0ex,partopsep=0.75ex,parsep=0.75ex,partopsep=0pt,wide, labelwidth=0pt,labelindent=0pt]
    \comment{\item \textbf{Reconstruction Shortcut}: As the learning objective of MAE is to recover the original images, introducing the low-level features as the shortcut into the decoding process is a straightforward idea. We first examine the influence of utilizing multi-level features for reconstruction quantitatively. Specifically, we apply a parametric weighted average to features of all layers for decoding in MAE and plot the curve of the weight of multi-level features during the model optimization in \autoref{fig:teaser}.
    We found that the model increasingly relies on the features of these shallow layers, such as the first layer, throughout the entire training process.}

    \item \textbf{Fuse Shallow Layers}: Rather than solely using the output layer for pixel reconstruction, we implement a weight-average strategy to fuse the output layer with all previous layers. The weights assigned to each layer are normalized and dynamically updated during the pre-training process, with their absolute values indicating the significance of each layer for the reconstruction task. We track the changes in these weights and illustrate them in \autoref{fig:teaser}. As depicted in the figure, the model increasingly relies on the features of shallow layers as training progresses.
    
    \item \textbf{Frequency Analysis}: To further understand the property of the representation learned with MAE, we analyze the frequency response of each layer's feature. We adopt the tools proposed by \cite{howdo} to transform the encoder features into the frequency domain and visualize the relative log amplitude of the transformed representation in \autoref{fig:freq_ana}. Typically, a higher amplitude indicates that the feature produced by one layer contains \textit{more high-frequency information}. We empirically find that the shallow layers contain significantly more high-frequency components than deep layers, which are mostly related to low-level details (\eg, textures).


    
    \comment{\item \textbf{Frequency Analysis}: We further analyze the frequency response of each layer in the MAE model. By transforming the representation of each layer into the frequency domain, we calculate the relative log amplitude of the Fourier-transformed representation, similar to the approach in \cite{howdo}. A higher amplitude indicates that a layer contains more high-frequency information. Empirically, we categorize layers above the 6th layer as deep layers and those below the 6th layer as shallow layers. As shown in \autoref{fig:freq_ana}, compared to deep layers, shallow layers contain significantly more high-frequency components, which are mostly related to low-level details (\eg, textures).}
    
\end{enumerate}

\input{fig/mae_freq_analysis}

\comment{Based on the two experiments described above, it is evident that the raw pixel reconstruction task causes a bias toward low-level details, which may be a unique drawback of these pixel-based MIM methods (not exist in these MIM approaches, using another tokenizer). Furthermore, the low accuracy of linear probing directly reflects the limitations of using raw image as target since linear separability necessitates that the extracted features be semantically distinct from each other. To compensate for this drawback, we explicitly incorporate low-level features into the output layer by reusing the \textbf{M}ulti-level \textbf{F}eature \textbf{F}usion (\red{\ourmethod}) strategy employed in the first pilot experiment. This strategy is straightforward and intuitive and can be easily integrated into most existing pixel-based MIM approaches without incurring significant computational resources. Despite its simplicity, it has the following two implications:}

\comment{\syadd{Based on the results of the pilot experiments, it is evident that the representation learned in MAE is high-frequency biased and find that the raw pixel reconstruction task aggressively requires the low-level feature when introducing shortcuts. Moreover, the biased representation  causes the poor linear separability in MAE(such as the low accuracy of linear probing). To compensate the limitation of pixel-based MIM, we propose a simple yet effective strategy by incorporating the low-features for pixel reconstruction in this work.}}

Based on our analysis of the pilot experiments, we can conclude that the pixel reconstruction task exhibits a bias towards low-level details. This is evident from the low linear probing accuracy, which highlights the constraints of the pixel reconstruction task. Namely, it requires features that are semantically distinct enough to achieve linear separability. To address this limitation, we propose to explicitly incorporate low-level features obtained from shallow layers into the output layer for the pixel reconstruction task. By doing so, we alleviate the burden of the model having to focus excessively on these low-level details, allowing it to spend its modeling abilities to capture these high-level semantics.

We denote the proposed method as \textbf{M}ulti-level \textbf{F}eature \textbf{F}usion (\red{\ourmethod}). Specifically, we extend the usage of the fusion strategy in the first pilot experiment and systematically investigate the design choices of multi-feature fusion, such as feature selection and fusion strategies. Despite the simplicity of the proposed method, it is a drop-in solution for unleashing the full modeling potential of pixel-based MIM approaches and has the following advantages:

\begin{enumerate}[label={\bf {{(\arabic*)}}},leftmargin=*,topsep=0.5ex,itemsep=-0.5ex,partopsep=0.75ex,parsep=0.75ex,partopsep=0pt,wide, labelwidth=0pt,labelindent=0pt]
    \item Employing multi-level feature fusion can enhance the training efficiency of MAE by approximately $\sim$5x, thus helping to reduce the carbon footprint. For example, by pre-training MAE with this strategy for only 300 epochs, we achieve semantic segmentation results that are on par with those obtained after 1600 epochs in the original paper. 
    \item We also consistently and significantly improve performance across all downstream tasks, including semi-supervised fine-tuning and linear probing. Notably, with a small model such as ViT-S, we outperform MAE by 2.8\% on linear probing, 2.6\% on semantic segmentation, and 1.2\% on fine-tuning.
    \item After evaluating our model on four out-of-distribution datasets, we observe that the approach with multi-level feature fusion exhibits greater robustness than the base method.
\end{enumerate}

Furthermore, we conduct a thorough analysis to unveil how multi-feature fusion works for representation learning. Given the exploratory experiments from the perspective of latent features and optimization, we find that the fusion strategy attenuates high-frequency information in the latent features and flattens the loss landscapes. To summarize, our contributions are three-fold:

% \noindent To summarize, our contributions are three-fold:
\begin{itemize}[leftmargin=*,topsep=0pt,itemsep=0pt,noitemsep]
    \item Firstly, we develop a multi-level feature fusion strategy for isotropic backbones such as ViT, achieving superior results compared to various pixel-based MIM approaches.
    \item Secondly, we have conducted a thorough analysis of how this multi-level feature fusion strategy enhances the model from the perspectives of latent features and optimization. Our examination is meticulous and provides valuable insights.
    \item Lastly, we have performed extensive and rigorous ablation studies on the design details, which also strengthens the validity of our findings.
\end{itemize}

