\section{Experiment}
In~\autoref{sec:exp:settings}, we present the experimental settings for pre-training and evaluation. Next, in \autoref{sec:main_results}, we apply MFF to two MIM baselines, namely MAE~\cite{MAE} and PixMIM~\cite{pixmim}, and show the improvements brought by such design. In addition, we also evaluate the effectiveness of MFF using a smaller model (\eg, ViT-S) and fine-tune the pre-trained model under a low-shot setting. To evaluate the robustness of our proposed method, \autoref{sec:robust_eval} includes additional analyses that assess the robustness of pre-trained models against out-of-distribution (OOD) ImageNet variants. Finally, \autoref{sec:ablation} presents comprehensive ablation studies of our method.


\subsection{Experiment Settings}
\label{sec:exp:settings}
To ensure the efficacy of our methods and design components, we conducted a series of extensive experiments on image classification using ImageNet-1K\cite{ImageNet-1K}, object detection on COCO~\cite{coco} and semantic segmentation on ADE20K\cite{ADE20K}. Unless otherwise stated, our default settings are based on ViT-B.


\paragraph{ImageNet-1K~\cite{ImageNet-1K}} The ImageNet-1K dataset comprises 1.3 million images belonging to 1,000 categories and is divided into training and validation sets. To ensure the fairness of our experiments while applying our methods to MAE~\cite{MAE} and PixMIM\cite{pixmim}, we strictly follow their original pre-training and evaluation settings on ImageNet-1K. This includes following the pre-training schedule, network architecture, learning rate setup, and fine-tuning protocols. Furthermore, in addition to the conventional fine-tuning protocol, we fine-tune the model using a low-shot setting, where only a fraction of the training set (\eg 1\% and 10\%) is used. This approach is consistent with previous works\cite{simclr} and we ensure that the low-shot fine-tuning setting also strictly follows that of the conventional fine-tuning.

\paragraph{ADE20K~\cite{ADE20K}} To conduct the semantic segmentation experiments on ADE20K, we utilize the off-the-shelf settings from MAE\cite{MAE}. With this approach, we fine-tune a UperNet\cite{upernet} for 160k iterations with a batch size of 16 and initialize the relative position bias to zero. 

\paragraph{COCO~\cite{coco}} For our object detection experiments on COCO, we adopt the Mask R-CNN approach\cite{maskrcnn} that enables simultaneous production of bounding boxes and instance masks, with the ViT serving as the backbone. As in MAE, we evaluate the box and mask AP as the metrics for this task. However, we note that there is no universal agreement for the setting of object detection fine-tuning epochs. We have chosen the commonly used 2$\times$ setting, which fine-tunes the model for 25 epochs. Other settings strictly follow those in ViTDet~\cite{ViTDet}.

\paragraph{Ablation studies} We conduct all of our ablation studies based on the customary MAE settings~\cite{MixMIM, CAE}. We pre-train all model variants on ImageNet-1K for 300 epochs and conduct a comprehensive performance comparison on linear probing, fine-tuning, and semantic segmentation. All other settings are consistent with those discussed previously.
\input{text/4.2-main-res}
\input{text/4.3-fewshot-robust}
\input{text/4.4-ablation}