\subsection{Ablation Studies}
\label{sec:ablation}
\paragraph{Is shallow layer important?} In order to determine the significance of low-level features from shallow layers, we explore the fusion of the output layer with either a deep or shallow layer. So we try to fuse the output layer with an extra shallow or deep layer, selected from the previous 11 layers of ViT-B\cite{ViT}. And the specific index of the selected layer will be detailed in the appendix. As illustrated in \autoref{tab:shallow_layer}, fusing the output layer with a deep layer only results in marginal improvements. However, incorporating low-level features directly from the shallow layer into the output layer leads to a significant performance boost, as it enables the model to focus on semantic information. Therefore, we have decided to use a shallow layer (\ie the first layer) for multi-level feature fusion.

\paragraph{How many layers are used for fusion ?} Aside from the output layer and the shallow layer picked in the previous selection, it is reasonable to consider using intermediate layers for fusion, as they may contain additional low-level features or high-level meanings that could assist in the reconstruction task. However, selecting these intermediate layers is a daunting task due to the large search space involved. To simplify the process, we pick an additional 1, 2, and 5 layers evenly spaced between the shallow layer and output layer selected in \autoref{tab:shallow_layer}. The specific indices for these selected layers are placed in the appendix. As shown in \autoref{tab:layers}, introducing more layers brings consistent improvements because they may contain unique features, such as textures or colors, that help the model complete the reconstruction task. Nevertheless, when we fuse all these layers, we witness a performance drop in all downstream tasks. This drop may result from the difficulty of optimization, because of the redundancy in these layers.

\input{table/ood}

\paragraph{Do the projection layer and fusion strategy matter?} In \autoref{eq:proj}, we investigate the influence of the projection layer on the final results. Our findings indicate that a simple linear projection layer is sufficient to achieve satisfactory results, as compared to using no projection layer or a nonlinear projection layer. Incorporating a single linear projection layer offers benefits in mitigating the domain or distribution gap between different layers, as compared to using no projection layer. However, the addition of a nonlinear projection layer, which includes an extra linear projection and GELU activation before the linear projection, introduces computational overhead and is more challenging to optimize. As a result, the non-linear projection achieves sub-optimal performance. With regard to the fusion strategy, we found that the \textbf{weight-average pooling} strategy, which assigns a dynamic weight to each layer and then performs element-wise addition, achieves the best performance. Compared to \textbf{attn}, this strategy shares the merits of simplicity and smaller computational overhead.
