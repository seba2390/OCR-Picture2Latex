We further investigate the instantiation of the projection layer and the fusion layer. 

\noindent\textbf{Projection Layer}: In terms of the projection layer, indicated as $\mathcal{P}$, we focus on two popular options, namely linear projection and non-linear projection. Specifically, we instantiate the non-linear projection with the Linear-GELU-Linear structure. Our experiment has revealed that a simple linear layer is sufficient and effective within our framework. 

\noindent\textbf{Fusion Layer}: The fusion layer aims to gather low-level information from the features of shallower layers feature. We evaluate two commonly employed fusion methods: weighted average pooling and self-attention-based fusion. 

\begin{equation}
    \label{eq:pooling}
    O = \sum_{i \in \mathcal{W}}w_i\mathcal{P}_i(x_i) + w_{N-1}x_{N-1}
\end{equation}

 \noindent The weighted average pooling fusion is illustrated in \autoref{eq:pooling}. In this equation, $w_i$ refers to the weight assigned to each of the $M$ selected layers, while $w_{N-1}$ is assigned to the output layer. During the training process, all these weights are dynamically updated and summed up to 1. As for the self-attention method, we use an off-the-shelf transformer layer.

\begin{equation}
    \label{eq:attn}
    \hat{O} = \text{MultiHeadAttention}(([{\mathcal{P}_i(x_i)}_{i \in \mathcal{W}}, x_{N-1}])
\end{equation}

\noindent After the multi-head attention layer, we extract the transformed tokens corresponding to $x_{N-1}$ from $\hat{O}$ to use in pixel reconstruction. Experimental results demonstrate that the weighted average pooling strategy is comparable to self-attention for this purpose while also being simpler and more computationally efficient.

