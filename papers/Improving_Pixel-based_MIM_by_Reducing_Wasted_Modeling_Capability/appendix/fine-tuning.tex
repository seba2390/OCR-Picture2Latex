\begin{table}[h]
\tablestyle{6pt}{1.02}
% \scriptsize
\begin{tabular}{y{96}|y{68}}
config & value \\
\shline
optimizer & AdamW\cite{adamw} \\
base learning rate & 1e-3 \\
weight decay & 0.05 \\
optimizer momentum & $\beta_1, \beta_2{=}0.9, 0.999$ \\
layer-wise lr decay \cite{electra, BEiT} & 0.75 \\
batch size & 1024 \\
learning rate schedule & cosine decay \\
warmup epochs & 5 \\
training epochs & 100  \\
augmentation & RandAug (9, 0.5) \cite{randaug} \\
label smoothing \cite{smooth} & 0.1 \\
mixup \cite{mixup} & 0.8 \\
cutmix \cite{cutmix} & 1.0 \\
drop path \cite{droppath} & 0.1 \\
\end{tabular}
\caption{{End-to-end fine-tuning setting of \ourmethod$_\text{MAE}$, \ourmethod$_\text{PixMIM}$}}
\label{tab:impl_mae_finetune} 
\end{table}