%We begin this section with an overview of our algebraic FMM algorithm
%broken into a number of separate tasks.
%We then detail our novel techniques for applying the algorithm in a
%geometry-oblivious manner.
%Finally, we discuss our runtime scheduler for executing
%the tasks in parallel with high efficiency.

Given $K\in\mathbb{R}^{N\times N}$, \gofmm{} aims to construct an \hmatrix{}
$\sk{K}$ in the form of \eqref{e:hmatrix} such that we can approximate 
\begin{equation}
  u=Kw\approx \sk{K}w,\ \mbox{~for~}\ w\in\mathbb{R}^{N}.
  \label{e:appromatvec}
\end{equation}
When points $\{x_i\}^{N}_{i=1}$ are available such that $K_{ij} = \Ker(x_i,x_j)$,
the recursive partitioning on $D$ and the low-rank structure  $UV$ use \emph{distances} between $x_i$ and $x_j$.
Existing FMM methods approximate $K_{ij}$ when $x_i$ and $x_j$ are sufficiently \emph{far} from each other.
Otherwise, $K_{ij}$ is not approximated and it is placed either in $D$ or in $S$. 
We call this distance-based criterion \ipoint{near-far pruning}. 

To define such a pruning scheme without $\{x_i\}^{N}_{i=1}$,
we need a notion of distance between two matrix indices $i$ and $j$. We define such a distance in the next section.
With it, we can permute $K$ and define neighbors for each index $i$.
In \secref{s:algfmm}, we describe a task-based algebraic FMM that only relies on the distance we define.
Finally in \secref{s:par}, we discuss task parallelism and scheduling. 


% We start with a sketch of hierarchical low-rank plus sparse
% matrices, deriving in \secref{s:hier} how the fast multipole method (FMM) can use near-far pruning
% to compute matrix-vector multiplications with $\MA{O}(N)$ work.
% We then describe a heuristic in \secref{s:seq} that defines 
% \emph{distance} between columns and rows of $K$ such that neighbor-based sampling and near-far pruning is applicable without geometric coordinates.
% By introducing the dependencies between each process in \secref{s:seq},
% we present an \emph{out-of-order} scheduling scheme in \secref{s:par}  
% to resolve the parallelism diminishing issue of our tree-based algorithms.
% The runtime scheduler is general enough such that our method can 
% support multiple architectures and accelerators efficiently.

