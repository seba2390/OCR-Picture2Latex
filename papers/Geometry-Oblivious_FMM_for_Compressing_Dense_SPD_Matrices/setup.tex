We perform  experiments on Haswell, KNL, ARM, and NVIDIA GPU architectures
with four different setups to examine the accuracy and efficiency of
our methods. We demonstrate 
(1) the robustness and effectiveness of our geometry-oblivious FMM,
(2) the scalability of our runtime system against other parallel schemes, 
(3) the accuracy and cost comparison with other software, and 
(4) the absolute efficiency (in percentage of peak performance).

\textbf{Implementation and hardware.} Please refer to \secref{s:sup_setup} 
for all configuration in the reproducibility artifact section. Our
tests were conducted on TACC's Lonestar 5, (two 12-core, 2.6GHz, Xeon
E5-2690 v3 ``Haswell''), TACC's Stampede 2 (68-core, 1.4GHz, Xeon Phi
7250 ``KNL'') and CSCS's Piz Daint (12-core, 2.3GHz, Xeon E5-2650 v3
and NVIDIA Tesla P100). 

%We conducted experiments on
%Lonestar5 (two 12-core, 2.6GHz, Xeon E5-2690 v3  ``Haswell''
%per node) and Stampede (68-core, 1.4GHz, Xeon Phi 7250 ``KNL'' per node)
%clusters at the Texas Advanced Computing Center,
%Piz Daint (12-core, 2.3GHz, Xeon  E5-2650 v3 and NVIDIA Tesla P100)
%at Swiss National Supercomputing Centre,
%and an Intrinsyc Open-Q 820 Development Kit (quad-core, 2.2GHz Qualcomm Kyro).
%
%The theoretical peak performance\footnote{\scriptsize{We estimate the peak 
%according to the clockrate
%and the \texttt{FMA} throughput. For 24 Haswell cores,
%$998=2\times12\times2.6\times16$. For 68 KNL cores,
%$3046=68\times1.4\times32$. For 4 Arm cores, $35.2=4\times 2.2\times 4$.
%The peak of P100 is reported as 4.7 \texttt{TFLOPS}.
%As a reference, MKL \texttt{GEMM} can achieve $87\%$ on a Haswell node and 
%$69\%$ on a KNL node. QSML \texttt{GEMM} can achieve $89\%$ on Open-Q 820.
%\texttt{cublasXgemm} can achieve $95\%$ on P100.
%We assume two KNL VPUs can dual issue \texttt{DFMA}s~\cite{sodani2016knights}.
%However, Intel processors may have a different frequency while fully issuing 
%  \texttt{FMA}, and the clockrate may drop to 1.0 GHz. This may be the reason
%why MKL \texttt{DGEMM} can only achieve 2.1 TFLOPS on KNL.}} in double precision is $998$ GFLOPS per Haswell node, $3,046$ GFLOPS
%per KNL node, ($4,700+416$) GFLOPS per Tesla P100 node, and $35.2$ GFLOPS 
%per Open-Q820.
%The peak \texttt{GFLOPS} doubles for single precision computations.
%\texttt{GOFMM} is developed in \texttt{C++}, employing \texttt{OpenMP} 
%for shared memory parallelism using a self-contained runtime system.
%All software (including \texttt{HODLR}, \texttt{STRUMPACK} and \texttt{ASKIT}) 
%are compiled with 
%\texttt{intel-16.0 -O3}  on
%Lonestar5 and Piz Daint. Stampede uses \texttt{intel-17.0 -O3 -xMIC-AVX512}.
%The GPU part uses \texttt{nvcc-8.0 -O3 -arch$=$sm\_60}. 
%For Open-Q 820, we cross compile our software with NDK using 
%\texttt{gcc-4.9 -O3}.
%All CPU and KNL BLAS/LAPACK routines use MKL.
%GPU BLAS routines use CUBLAS; on Arm we use QSML
%(Qualcomm Snapdragon Math Library).
%KNL experiments use Cache-Quadrant configuration. 
%\texttt{OpenMP} uses $\texttt{OMP\_PROC\_BIND=spread}$.
%We use ``\rm{Comp}'' and ``\rm{Eval}'' to refer the the 
%compression and evaluation time in seconds, and ``\rm{GFs}'' to 
%\rm{GFLOPS} per node.

%Maverick nodes have two 10-core Intel Xeon E5-2680 v2 (3.1GHz) processors and 

\paragraph{\textbf{Matrices}}
We generated 22 matrices emulating different problems.
{\bf K02} is a 2D regularized inverse Laplacian squared, 
resembling the Hessian operator of a PDE-constrained optimization problem. 
The Laplacian is discretized using a 5-stencil finite-difference scheme 
with Dirichlet boundary conditions on a regular grid. 
{\bf K03} has the same setup with the 
oscillatory Helmholtz operator and 10 points per wave length. 
{\bf K04--K10} are kernel matrices in six dimensions 
(Gaussians with different bandwidths, narrow and wide; 
Laplacian Green's function, polynomial and cosine-similarity).
{\bf K12--K14} are 2D advection-diffusion operators on a regular 
grid with highly variable coefficients. 
{\bf K15,K16} are 2D pseudo-spectral  advection-diffusion-reaction 
operators with variable coefficients. 
{\bf K17} is a 3D pseudo-spectral operator with variable coefficients. 
{\bf K18} is the inverse squared Laplacian in 3D with variable coefficients. 
{\bf G01--G05} are the inverse Laplacian of the  \textbf{powersim}, 
\textbf{poli\_large}, \textbf{rgg\_n\_2\_16\_s0}, 
\textbf{denormal}, and \textbf{conf6\_0-8x8-30} graphs from 
\href{http://yifanhu.net/GALLERY/GRAPHS/search.html}{UFL}.  

\textbf{K02}--\textbf{K03}, 
\textbf{K12}--\textbf{K14}, and \textbf{K18} resemble inverse 
covariance matrices and Hessian operators from optimization and 
uncertainty quantification problems. 
\textbf{K04}--\textbf{K10} resemble 
classical kernel/Green function matrices but in high dimensions. 
\textbf{K15}--\textbf{K17} resemble pseudo-spectral operators.
\textbf{G01}--\textbf{G05} ($N=$ 15838, 15575, 65536, 89400, 49152)
are graphs for which we do not have geometric information.
For \textbf{K02}--\textbf{K18}, we use $N=65536$ if not specified.

Also, we use kernel matrices from machine learning: 
\textbf{COVTYPE} (100K, 54D, cartographic variables); and
\textbf{HIGGS} (500K, 28D,  physics)~\cite{Lichman:2013};
\textbf{MNIST} (60K, 780D, digit recognition)~\cite{chang2011libsvm}.
For these datasets, we use a Gaussian kernel with bandwidth $h$.
%\begin{equation}\label{e:gaussian}\Ker(\bx_i,\bx_j) = \exp\left(-\frac{1}{2} \frac{\|\bx_i-\bx_j\|_2^2}{h^2}\right),\end{equation}
%The Gaussian is one of the hardest kernel function to compress
%in high dimensions.

\gofmm{} supports both double and single precision. All experiments with matrices \textbf{K02--K18} and \textbf{G01--G05} are in single precision. The results for {\bf COVTYPE, HIGGS, MNIST} are in double precision.


\textbf{Parameter selection and accuracy metrics.}
We control $m$ (leaf node size), 
$s$ (maximum rank), $\tau$ (adaptive tolerance), $\kappa$ (number of
neighbors), \ipoint{budget} (a key paramter for amount of direct evaluations
and for switching between HSS and FMM) and partitioning (\textbf{Kernel},
\textbf{Angle}, \textbf{Lexicographic}, \textbf{geometric}, \textbf{random}).
We use $m=$256--512; on average this gives good overall time.  The adaptive
tolerance $\tau$, reflects the error of the subsampled  block and may not
correspond to the output error $\epsilon_2$. Depending on the problem, $\tau$ may misestimate the rank.
Similarly, this may occur in \texttt{HODLR}, \texttt{STRUMPACK} and \texttt{ASKIT}.
% For compressed matrices, the cost is controlled by $\tau$ (usually $1E-2$ or $1E-5$). While $\tau$ underestimates the desired accuracy, we may increase $\tau$ to 1E-7 or 1E-10.
%For the uncompressed matrices, adaptive ID will reach the maximum rank $s$ regardless how small $\tau$ is.In this case, the cost is controlled by $s$ and the budget. 
We use $\tau$ between 1E-2 and 1E-7, $s=m$, $k=32$ and $3\%$ budget. To enforce a HSS approximation, we use $0\%$ budget. The Gaussian bandwidth values are taken from~\cite{march-xiao-biros-e15} and produce optimal learning rates.

%% Paragraph about parameters
%Despite the goal in this study to keep the number of parameters low, the described method still has the following parameters to control its computational cost and its accuracy: 
%\begin{itemize}
%\item $m$: the number of rows and columns per leaf node. It controls the error and the runtime
%since it governs the trade-off between near and far interactions.
%\item $k$: the number of nearest neighbors for sampling and pruning. Larger $k$ improves accuracy by improving the sampled {\it ID} and with a more accurate pruning strategy; this comes at a computational cost.
%\item $s$: maximum skeleton size. This parameter restricts the approximation rank for off-diagonal blocks.
%\item $tol$: approximation tolerance. This parameter is used for adaptive approximation rank selection using an estimate of the singular value for the compression. Ideally this corresponds to the error tolerance for the entire approximation.
%% TODO: Adapt to whatever we decide to use
%\item $budget$: the percentage of direct evaluations. Results indicated that simple distance- or angle- based pruning can be prohibitively expensive. This parameter steers the amount of work the user is willing to spend in the evaluation phase.
%\item {\it STRATEGY}: partitioning and sampling. For certain application cases, $L_2$-distance or angles in Gram  vector space can be more effective.  
%% depending on whether we want to report kernel neighbor and angle
%\end{itemize}

% Paragraph about accuracy metrics

Throughout we use relative error $\epsilon_2$ defined as the following
\begin{equation}
  \epsilon_2 = \|\sk{K}w-Kw\|_F / \|Kw\|_F,\ \mbox{where} \ w \in \mathbb{R}^{N\times r}.
\end{equation}
This metric requires $\MA{O}(rN^2)$ work; to reduce the computational effort we instead sample 100 rows of $K$.
In all tables, we use ``\rm{Comp}'' and ``\rm{Eval}'' to refer the the 
compression and evaluation time in seconds, and ``\rm{GFs}'' to 
\rm{GFLOPS} per node.

%The accuracy reported here and relevant for the user is compared to an exact calculation $u$. To keep computational effort low we choose $100$ test points, and compute the relative error to evaluated $\hat{u}$ by 
%$\epsilon_{rel} = \sum_{i}\frac{u_i-\hat{u_i}}{u_i}$.
