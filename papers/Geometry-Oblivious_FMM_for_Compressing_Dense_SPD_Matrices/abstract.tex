We present \gofmm{} (geometry-oblivious FMM), a novel method that creates a
hierarchical low-rank approximation, or \emph{``compression,''} of an arbitrary
dense symmetric positive definite (SPD) matrix. For many applications, \gofmm{}
enables an approximate matrix-vector multiplication in $N \log N$ or even $N$
time, where $N$ is the matrix size. Compression requires $N \log N$ storage and
work.  In general, our scheme belongs to the family of hierarchical matrix
approximation methods. In particular, it generalizes the fast multipole method
(FMM) to a purely algebraic setting by only requiring the ability to sample
matrix entries. Neither geometric information (i.e., point coordinates) nor
knowledge of how the matrix entries have been generated is required,  thus the
term \emph{``geometry-oblivious.''} Also, we introduce a shared-memory parallel scheme for hierarchical matrix computations that reduces synchronization barriers. We present results on the Intel Knights Landing and Haswell architectures, and on the NVIDIA Pascal architecture for a variety of matrices.
