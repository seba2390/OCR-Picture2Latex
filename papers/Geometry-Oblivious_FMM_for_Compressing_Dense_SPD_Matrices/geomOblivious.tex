In this section, we introduce the machinery for using \gofmm{} in a
geometry-oblivious manner. Throughout the following discussion, we refer to a
set of indices $\mathcal{I} = \{1,\dots,N\}$, where index $i$ corresponds to
the $i$th row (or column) of the matrix $K$ in the original ordering. Our
objective is to find a permutation of $\mathcal{I}$ so that $K$  can be
approximated by an \hmatrix{}. The key is to define a distance  between a pair
of indices $i,j\in\mathcal{I}$, denoted as $d_{ij}$. Using the distances, we
then perform a hierarchical clustering of $\mathcal{I}$, which is used to
define the permutation, and determine which interaction go into
%the permuted order of $\mathcal{I}$, as well as the interactions that go to 
the sparse correction $S$ (using nearest neighbors). 

\paragraph{Three measures of distance.} We will define the point-based Euclidean distance, a Gram-space Euclidean distance, and a Gram-space angle distance.

\textbf{Geometric-$\ell^2$.} If we are given points $\{x_i\}_{i=1}^N$, then 
$d_{ij} = \lVert x_i - x_j \rVert_2$ is
the \emph{geometric $\ell^2$ distance}. This will be the \emph{geometry-aware}
reference implementation for cases where points are given.

\textbf{Gram-$\ell^2$ (or \emph{``kernel''} distance)}. Since $K$ is SPD, it is
the \emph{Gram matrix} of some set of \ipoint{ unknown Gram vectors},
$\{\phi_i\}_{i=1}^N \subset \mathbb{R}^N$  ( \cite{scholkopf-smola-02},
proposition 2.16, page 44).   That is, $K_{ij} = (\phi_i, \phi_j)$, where
$(\cdot,\cdot)$ denotes the $\ell^2$ inner product in $\mathbb{R}^N$.  Then we
define the \emph{Gram $\ell^2$ distance} as $d_{ij} = \lVert \phi_i - \phi_j
\rVert$. Computing the kernel distance only requires three entries of $K$:
\begin{equation}
  d_{ij}^2 = \lVert \phi_i \rVert^2 + \lVert \phi_j \rVert^2 - 2 (\phi_i, \phi_j) = K_{ii} + K_{jj} - 2K_{ij}. 
\end{equation}

\textbf{Gram angles (or \emph{``angle''} distance)}.
Our third measure of distance considers angles between Gram vectors, which is based on the standard sine distance (cosine similarity) in inner product spaces. 
%% It is motivated by an observation relating the rank of an off-diagonal block
%% to the degree of orthogonality between sets of Gram vectors.
%% Consider disjoint index sets $\alpha, \beta \subset \mathcal{I}$
%% and the corresponding matrix of interactions $K_{\alpha\beta}$.
%% If we define $\phi_\alpha$ to be the matrix with columns $\{\phi_i\}_{i\in\alpha}$,
%% and define $\{\phi_j\}_{j\in\beta}$ similarly,
%% then $K_{\alpha\beta} = \phi_\alpha^T \phi_\beta$.
%% We may view $K_{\alpha\beta}$ as a projection of $\phi_\beta$ onto the span of $\phi_\alpha$,
%% so the rank of $K_{\alpha\beta}$ is equal to the dimension of the projection.
%% Thus, we require a measure of distance that clusters Gram vectors
%% to form orthogonal subspaces, rather than small volumes.
We define the Gram angle distance as
$d_{ij} = \sin^2\left(\angle(\phi_i,\phi_j)\right)\in [0, 1]$.
This expression is chosen so that $d_{ij}$ is small for nearly collinear
Gram vectors, large for nearly orthogonal Gram vectors, and $d_{ij}$ is
inexpensive to compute. Although the value $d_{ij}$ may seem arbitrary,
we only compare values for the purpose of ordering,
so any equivalent metric will do.
Computing an angle distance only requires three entries of $K$:
\begin{equation}
  d_{ij} = 1 - \cos^2 \left(\angle(\phi_i,\phi_j)\right)
         %= 1 - \frac{K_{ij}^2}{K_{ii} K_{jj}}.
          = 1 - K_{ij}^2/(K_{ii} K_{jj}).
\end{equation}
To reiterate for emphasis, $d_{ij}$ define proper distances (metrics) because $K$ is SPD. And with distances, we can apply FMM.

\paragraph{Tree partitioning and nearest neighbor searches}
$K$ is permuted using a balanced binary tree.  The root node is assigned the full set of points, and the tree is constructed recursively by splitting a node's points evenly between two child nodes. The splitting terminates at nodes with some pre-determined \ipoint{leaf size} $m$. The leaf nodes then define a partial ordering of the indices: if leaf $\alpha$ is anywhere to the left of leaf $\beta$, then the indices of $\alpha$ precede those of $\beta$. We use this ordering to permute rows and columns of $K$. In the remainder of this paper, we use the notation $\alpha, \beta$ to refer interchangeably to a node or the set of indices belonging to the node.

 \begin{algorithm}[!t]
   \caption{{} $[\lc,\rc]=\texttt{metricSplit}(\alpha)$}
 \begin{algorithmic}
   \STATE $p = \mbox{argmax}\,( \{d_{ic}  \lvert i \in \alpha \})$; $q =
   \mbox{argmax}( \{ d_{ip} \lvert i \in \alpha \})$;
   \STATE $[\lc,\rc]=\mbox{medianSplit}(\{ d_{ip} - d_{iq} \lvert i \in
   \alpha\})$;
 \end{algorithmic}
 \label{a:split}
 \end{algorithm}  

In our implementation, we use a ball tree~\cite{march-xiao-yu-biros-sisc16}.
For geometric distances it costs $\bigO(N \log N)$. But Gram distances require
sampling to avoid $\bigO(N^2)$ costs.   Suppose we use one of the Gram
distances to split an interior node $\alpha$ between its left child $\lc$ and
right child $\rc$.  We define $c = \frac{1}{n_{\rm{c}}} \sum \phi_i$ to be an 
approximate centroid taken over a small sample of $n_{\rm{c}}$ Gram vectors belonging to $\alpha$. 
$n_{\rm{c}}$ is $\bigO(1)$.
Next, we find the point $p$ that is farthest away in distance from $c$, and the point $q$ that is farthest away from $p$. Then we split the indices $i \in \alpha$ on the values $d_{ip} - d_{iq}$, which measures the degree to which $i$ is closer to $p$ than to $q$. This approach is outlined in \algref{a:split}. 

%For the geometric and Gram $\ell^2$ distances, we apply the usual ball tree
%method of computing projections along the axis from $p$ to $q$ and splitting
%on those values.

We perform \emph{all nearest neighbors} (\texttt{ANN}) search  using randomized trees that are constructed in exactly the same way as the partitioning tree, except that $p$ and $q$ are chosen randomly. The search algorithm is described in~\cite{xiao-biros16} and (briefly) in the next section.
%For each randomized tree, we compute distances between pairs of points belonging to the same leaf node and update neighbor lists accordingly. The algorithm we the one described in. 

% \textbf{Remark.}
% We emphasize that our partitioning schemes do not produce a hierarchical permutation for all SPD matrices.
% In some cases, such a permutation may not exist.

