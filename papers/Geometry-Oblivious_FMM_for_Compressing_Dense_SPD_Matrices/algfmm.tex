\hmatrix{} methods (including algebraic FMM) have two phases: \ipoint{compression} and \ipoint{evaluation}. 
As we discussed in the introduction, $K$ is compressed recursively using a binary tree
such that
\begin{equation}
  \label{e:partitioning}
  \sk{K}_{\alpha\alpha} =
\begin{bmatrix}
  \sk{K}_{\lc\lc} & 0 \\ 
  0 & \sk{K}_{\rc\rc} \\ 
\end{bmatrix} + 
\begin{bmatrix} 
0 & S_{\lc\rc} \\ 
S_{\rc\lc} & 0 \\ 
\end{bmatrix}+
\begin{bmatrix} 
  0 & UV_{\lc\rc} \\ 
  UV_{\rc\lc} & 0 \\ 
\end{bmatrix}, 
\end{equation} 
where $\lc$ and $\rc$ are \ipoint{left} and \ipoint{right child} of the treenode $\alpha$.
Each node $\alpha$ contains a set of matrix indices and the two 
children evenly split the indices such that $\alpha = \lc \cup \rc$.
(We overload the notation $\alpha$, $\beta$, $\lc$ and $\rc$ to
denote the matrix indices that those treenode own.)
In \figref{fig:tree}, the blue blocks depict $S$ (at all levels) and $D$ 
(in the leaf level), and the pink blocks depict the $UV$  matrices.

We use \ipoint{four tree traversals} to describe the algorithms in \gofmm{}:
postorder (\textbf{POST}), preorder (\textbf{PRE}),
any order (\textbf{ANY}),
and any order-leaves only (\textbf{LEAF}).
By  \ipoint{``task''} we refer to a computation that occurs when we visit a tree node during a traversal. 
We list all tasks required by \algref{a:compress} and \algref{a:evaluate} in \tabref{tab:tasks}.

We start by creating the binary metric ball tree in \algref{a:compress}
that represents the binary partitioning (and encodes a symmetric permutation of matrix $K$).
This requires the \emph{distance} metric $d_{ij}$ and a preorder traversal
(\textbf{PRE}) of the first task \texttt{SPLI($\alpha$)} in \tabref{tab:tasks}. 


\begin{table}[!t]
\centering
{
\begin{tabular}{|>{\columncolor[gray]{0.8}}r|l|r|} 
\hline 
  \rowcolor[gray]{0.8}
Task & Operations & \texttt{FLOPS} \\
\hline 
\texttt{SPLI($\alpha$)} & split $\alpha$ into $\lc$ and $\rc$ \algref{a:split} & $\lvert \alpha \rvert$ \\
\hline 
\texttt{ANN($\alpha$)} & update $\MA{N}_{\alpha}$ with \texttt{KNN($K_{\alpha\alpha}$)} & $m^2$ \\ 
\hline 
\texttt{SKEL($\alpha$)} & $\sk{\alpha}$ in \algref{a:skeletonize} & $2s^3+2m^3$ \\ 
\hline 
\texttt{COEF($\alpha$)} & $P_{\sk{\alpha} \alpha}$ or $P_{\sk{\alpha} [\sk{\lc} \sk{\rc}]}$ in \algref{a:skeletonize} & $s^3$ \\ 
\hline 
\texttt{N2S($\alpha$)} & \texttt{\bf if} $\alpha$ is leaf \texttt{\bf then} $\sk{w}_{\alpha} = P_{\sk{\alpha}\alpha}w_{\alpha}$ & $2msr$ \\
                       & \texttt{\bf else} $\sk{w}_{\alpha} =
P_{\sk{\alpha}[\sk{\lc}\sk{\rc}]}[ \sk{w}_{\lc}; \sk{w}_{\rc}]$ & $2s^2r$\\
\hline
\texttt{SKba($\beta$)}  & $\forall\alpha\in\mathtt{Far}(\beta)$,
$K_{\sk{\beta}\sk{\alpha}} = K(\sk{\beta}, \sk{\alpha})$ & $ds^2\lvert \mathtt{Far}(\beta) \rvert$ \\ 
\hline 
\texttt{S2S($\beta$)} & $\sk{u}_{\beta} =
\sum_{\alpha\in \mathtt{Far}(\beta)} K_{\sk{\beta}\sk{\alpha}}\sk{w}_{\alpha}$
& $2s^2r\lvert \mathtt{Far}(\beta) \rvert$\\ 
\hline 
\texttt{S2N($\beta$)} & \texttt{\bf if} $\alpha$ is leaf \texttt{\bf then} $u_{\beta} = P_{\sk{\beta}\beta}^{T}\sk{u}_{\beta}$ & $2msr$ \\ 
                      & \texttt{\bf else} $[\sk{u}_{\lc};\sk{u}_{\rc}] += P_{\sk{\beta}[\sk{\lc}\sk{\rc}]}^{T}\sk{u}_{\beta}$ & $2s^2r$ \\
\hline
\texttt{Kba($\beta$)}  & $\forall\alpha\in\mathtt{Near}(\beta)$, $K_{\beta\alpha} = K(\beta, \alpha)$ & $m^2\lvert\mathtt{Near}(\beta)\rvert$ \\ 
\hline  
\texttt{L2L($\beta$)} & $u_{\beta} += \sum_{\alpha\in
\mathtt{Near}(\beta)}K_{\beta\alpha}w_{\alpha}$ & $2m^2r\lvert\mathtt{Near}(\beta) \rvert$\\ 
\hline 
\end{tabular}
}
\caption{Tasks and their costs in \texttt{FLOPS}.
  \texttt{SPLI} (tree splitting), \texttt{ANN} (all nearest-neighbors), \texttt{SKEL} (skeletonization), 
  \texttt{COEF} (interpolation) \texttt{SKba} and 
  \texttt{Kba} (caching submatrices) occur in the compression phase.
  Interactions \texttt{N2S} (nodes to skeletons), \texttt{S2S} (skeletons to skeletons),
  \texttt{S2N} (skeletons to nodes), and \texttt{L2L} (leaves to leaves) occur in 
  the evaluation phase.}
\label{tab:tasks}
\end{table} 

\begin{algorithm}[!t]
\caption{{} \texttt{Compress}($K$)}
\begin{algorithmic}[1]
  \STATE \texttt{\bf for each} randomized tree \texttt{\bf do} \# iterative neighbor search
  \STATE \gap (\texttt{\bf PRE}) \texttt{SPLI($\alpha$)} \# create a random projection tree 
  \STATE \gap (\texttt{\bf LEAF}) \texttt{ANN($\alpha$)} \# search $\kappa$ neighbors in leaf nodes
  \STATE (\texttt{\bf PRE}) \texttt{SPLI($\alpha$)} \# create a metric ball tree 
  \STATE (\texttt{\bf LEAF}) \texttt{LeafNear($\beta$)} \# build \texttt{Near}$(\beta)$ using $\MA{N}(\beta)$
  \STATE (\texttt{\bf LEAF}) \texttt{FindFar($\beta$,root)} \# find \texttt{Far}$(\beta)$ using \texttt{MortonID}
  \STATE (\texttt{\bf POST}) \texttt{MergeFar($\alpha$)} \# merge \texttt{Far}$(\lc)$, \texttt{Far}$(\rc)$ to \texttt{Far}$(\alpha)$
  \STATE (\texttt{\bf POST}) \texttt{SKEL($\alpha$)} \# compute skeletons $\sk{\alpha}$
  \STATE (\texttt{\bf ANY}) \texttt{COEF($\alpha$)} \# compute the coefficient matrix $P$
  \STATE (\texttt{\bf ANY}) \texttt{Kba($\beta$)} \# optionally evaluate and cache $K_{\beta\alpha}$ 
  \STATE (\texttt{\bf ANY}) \texttt{SKba($\beta$)} \# optionally evaluate and cache $K_{\sk{\beta}\sk{\alpha}}$
\end{algorithmic}
\label{a:compress}
\end{algorithm}


\begin{figure}[h]
  \centering
  \includegraphics[scale=.3]{figures/nearfar.pdf}
  \caption{A partitioning tree (left) and corresponding hierarchically
    low-rank plus sparse matrix (right).
    The off-diagonal blocks are combinations of low-rank matrices (pink)
    and sparse matrices (blue).
    The $\bigstar$ symbol denotes an entry that cannot
    be approximated (because the corresponding interaction is between neighbors).
    The solid edges in the tree mark the path traversed by
    \texttt{FindFar}($\beta$,\texttt{0}).
    Since $K_{\beta\alpha}$ does not contain any neighbor interactions ($\bigstar$),
    this traversal adds $\alpha$ to \texttt{Far}$(\beta)$.
    In this example,
    \texttt{FindFar(\lc,0)} computes \texttt{Far}$(\lc)=\{\rc,4,2\}$, and
    \texttt{FindFar(\rc,0)} computes \texttt{Far}$(\rc)=\{\lc,4,2\}$.
    \algref{a:farfar} (\texttt{MergeFar}) then moves \texttt{Far}$(\lc) \cap Far(\rc)$
    into \texttt{Far}$(\alpha)$ so that
    \texttt{Far}$(\alpha)=\{4,2\}$, 
    \texttt{Far}$(\lc)=\{\rc\}$ and
    \texttt{Far}$(\rc)=\{\lc\}$.
  }
  \label{fig:tree}
\end{figure}


\begin{algorithm}[!t]
\caption{{} \texttt{LeafNear}($\beta$)}
\begin{algorithmic}
  %\STATE \texttt{\bf for each} leaf node $\alpha$ \texttt{\bf do}
  %\STATE \gap \texttt{\bf if} $\alpha \cap \MA{N}(\beta)$ \texttt{\bf then}
  %         \texttt{Near}$(\beta) = Near(\beta) \cup \alpha$;
  \STATE  \texttt{Near}$(\beta)$ = \{\texttt{MortonID}$(i)$ : $\forall i\ \in \MA{N}(\beta)$\}
\end{algorithmic}
\label{a:nearnear}
\end{algorithm}

\begin{algorithm}[!t]
  \caption{{} \texttt{FindFar}($\beta=\mathtt{leaf}$, $\alpha$)}
\begin{algorithmic}
  \STATE \texttt{\bf if} $\alpha \cap Near(\beta) \neq \phi$ using \texttt{MortonID} \texttt{\bf then}
  \STATE \gap \texttt{FindFar}($\beta$,\lc); \texttt{FindFar}($\beta$,\rc); 
  \STATE \texttt{\bf else} \texttt{Far}$(\beta) = Far(\beta) \cup \alpha$;
\end{algorithmic}
\label{a:nearfar}
\end{algorithm}

\begin{algorithm}[!t]
\caption{{} \texttt{MergeFar}($\alpha$)}
\begin{algorithmic}
  \STATE \texttt{MergeFar}($\lc$); \texttt{MergeFar}($\rc$);
  %\STATE \texttt{\bf if} $\alpha$ is not leaf \texttt{\bf then}
  \STATE \texttt{Far}$(\alpha) = Far(\lc) \cap Far(\rc)$; 
  \STATE \texttt{Far}$(\lc) = Far(\lc) \backslash Far(\alpha)$; \texttt{Far}$(\rc) = Far(\rc) \backslash Far(\alpha)$;
\end{algorithmic}
\label{a:farfar}
\end{algorithm}

\textbf{Node lists and near-far pruning.}
\gofmm{} tasks require that every tree node maintains three lists.
For a node $\alpha$, these lists are the neighbor list $\MA{N}(\alpha)$,
near interaction list $\mathtt{Near}(\alpha)$, and far interation list
$\mathtt{Far}(\alpha)$. Computing these lists requires
defining neighbors for indices based on the distance $d_{ij}$ 
and the Morton ID.

%\ipoint{Morton ID:} 
%The Morton ID is a bit array that codes the path from the root to a tree node or index $i$. The Morton ID of an index $i$ is the Morton ID of the leaf node (in \gofmm{} ball metric tree) that contains it.
%We use \texttt{MortonID()} to denote this. 

A pair of nodes $\alpha$ and $\beta$ is said to be \emph{far} if
$K_{\beta\alpha}$ is low-rank and \emph{near} otherwise.
We use neighbor-based pruning~\cite{march-xiao-yu-biros-sisc16}
to determine the near-far relation.
Neighbors are  defined based on the specified distance
$d_{ij}$. For each $i$, we search for the $\kappa$ indices $j$
that result in the smallest $d_{ij}$.
The Morton ID is a bit array that codes the path from the root to a tree node or index $i$. The Morton ID of an index $i$ is the Morton ID of the leaf node (in \gofmm{} ball metric tree) that contains it.
We use \texttt{MortonID()} to denote this. 


\ipoint{Index nearest neighbor list} $\MA{N}$($i$): As we discussed, \gofmm{}
requires a preprocessing step in which we compute the nearest neighbors for 
every index $i$ using a greedy search (steps 1--3 in~\algref{a:compress}). 
This constructs a list of $\kappa$ nearest-neighbor for each $i\in\alpha$
iteratively.
In each iteration, we create a randomized projection
tree~\cite{dasgupta-freund08,liberty2007randomized,march-xiao-yu-biros-sisc16},
and we search for neighbors of $i$ only in the leaf node $\alpha$ that contains
$i$ using an exhaustive search~\cite{yu2015performance}. That is, for each $i\in\alpha$, 
we only search for small $d_{ij}$ where $j\in\alpha$ as well. 
The iteration stops after reaching $80\%$ accuracy or 10 iterations.

\ipoint{Node neighbor list} $\MA{N}$($\alpha$): 
Then we construct the neighbor list $\MA{N}(\alpha)$ of a leaf node 
$\alpha$ by merging all neighbors of $i \in \alpha$. For non-leaf nodes 
the list is constructed recursively~\cite{march-xiao-biros-fmm-e15}.
%This approach uses nearest-neighbor information
%(denoted by $\bigstar$ in \figref{fig:tree}) to construct
%a set of \emph{neighbors} indices $\MA{N}(\beta)$ for each node $\beta$.
%Notice that although we do not have coordinates, but we have $d_{ij}$ that 
%denotes the distance between indices $i$ and $j$.

\ipoint{Near list of a node} \texttt{Near}$(\alpha)$:
Leaf nodes $\alpha, \beta$ are considered near if 
$\alpha \cap \MA{N}(\beta)$ is nonempty
(i.e., $K_{\alpha \beta}$ contains at least one neighbor 
($\bigstar$) in \figref{fig:tree}). The \texttt{Near} list is defined only for leaf nodes and contains only leaf nodes.  For each leaf node $\beta$, \texttt{Near}$(\beta)$ is constructed
using \texttt{LeafNear} (\algref{a:nearnear}). For each neighbor $i\in\MA{N}(\beta)$, \texttt{LeafNear($\beta$)}  adds \texttt{MortonID}$(i)$ to \texttt{Near}$(\beta)$. 
Notice that the size of \texttt{Near}$(\beta)$ determines the number
of direct evaluations in the off-diagonal blocks. To prevent the cost from growing too fast,
we introduce a user-defined parameter \textbf{budget} such that
\begin{equation}
  \lvert \mathtt{Near}(\beta)\rvert < \mathrm{budget} \times (N/m). 
\label{e:budget}
\end{equation}
While looping over $i\in\MA{N}(\beta)$, instead of directly adding \texttt{MortonID}$(i)$
to \texttt{Near}$(\beta)$, we only mark it with a ballot.
Then we insert candidates to \texttt{Near}$(\beta)$ according to their votes
until \eqref{e:budget} is reached.
To enforce symmetry of $\tilde{K}$, we loop over all \texttt{Near} lists and enforce the following:
if $\alpha \in$ \texttt{Near}$(\beta)$ then $\beta \in$ \texttt{Near}$(\alpha)$.

%Since each index $i$ has $\kappa$ neighbors,
%overall the work is $\MA{O}(\kappa N)$.

\ipoint{Far list of a node} \texttt{Far}$(\alpha)$:
\texttt{Far}$(\alpha)$ is constructed in two steps in \algref{a:compress}.
First for each leaf node $\beta$, we invoke \texttt{FindFar}($\beta$, \texttt{root}) (\algref{a:nearfar}).
Upon visiting $\alpha$, we check whether $\alpha$ is a parent of any 
leaf node in \texttt{Near}$(\beta)$ using \texttt{MortonID}.
If so, we add $\alpha$ to \texttt{Far}$(\beta)$; otherwise, we recurse to the 
two children of $\alpha$.
The second step is a postorder traversal on \texttt{MergeFar(root)}
(\algref{a:farfar}).
This process merges the common nodes from two children lists \texttt{Far}$(\lc)$ and \texttt{Far}$(\rc)$.
These common nodes are removed from the children and added to 
their parent list \texttt{Far}$(\alpha)$.
In \figref{fig:tree}, 
\texttt{FindFar} can be identified by the smallest square pink blocks, 
and \texttt{MergeFar} merges small pink blocks into larger blocks.

\textbf{Low-rank approximation.}
We approximate off-diagonal matrix blocks with a nested
interpolative decomposition (ID)~\cite{halko-martinsson-tropp11}.
Let $\beta$ be the indices in a leaf node and
$I=\{1,...,N\}\backslash \beta$ be the set complement.
The \emph{skeletonization} of $\beta$ is a rank-$s$
approximation of its off-diagonal blocks $K_{I\beta}$ using the ID, 
which we write as
\begin{equation}
K_{I\beta} \approx K_{I\sk{\beta}}P_{\sk{\beta}\beta},
\end{equation}
where $\sk{\beta} \subset \beta$ is the \emph{skeleton} of $\beta$.
$K_{I\sk{\beta}} \in \mathbb{R}^{(N - \lvert \beta \rvert) \times s}$ is a column
submatrix of $K_{I \beta}$, and $P_{\sk{\beta}\beta} \in
\mathbb{R}^{s\times \lvert \beta \rvert}$ is a matrix of interpolation
coefficients, where $\ns$ is the approximation rank.

To efficiently compute this approximation, we select a sample subset $I'
\subset I$
using neighbor-based importance sampling~\cite{march-xiao-yu-biros-sisc16}. 
We then perform a rank-revealing QR factorization (\texttt{GEQP3}) on $K_{I' \beta}$.
The skeletons $\sk{\beta}$ are selected to be the first $\ns$ pivots, and
the matrix $P_{\sk{\beta}\beta}$ is computed by a triangular solve (\texttt{TRSM})
using the triangular factor $R$.
The rank $\ns$ is chosen adaptively such that $\sigma_{\ns+1}(K_{I' \beta}) < \idtol$,
where $\sigma_{\ns+1}(K_{I' \beta})$ is the estimated $\ns+1$ singular value and
$\idtol$ is related to a user-specified error tolerance.

For an internal node $\alpha$, we form the skeletonization in the same way,
except that the columns are also sampled using the skeletons of the children of $\alpha$.
That is, the ID is computed for $K_{I'[\sk{\lc}\sk{\rc}]}$,
where $[\sk{\lc}\sk{\rc}] = \sk{\lc} \cup \sk{\rc}$ contains the skeletons of the children
of $\alpha$:
\begin{equation}
K_{I[\sk{\lc}\sk{\rc}]} \approx K_{I\sk{\alpha}}P_{\sk{\alpha}[\sk{\lc}\sk{\rc}]}.
\end{equation}
This way, the skeletons are nested:
$\sk{\alpha} \subset \sk{\lc} \cup \sk{\rc}$.

As a consequence of the nesting property,
we can use $P_{\sk{\lc}\lc}$ and $P_{\sk{\rc}\rc}$ to construct an approximation of
the full block $K_{I\alpha}$:
\begin{equation}
K_{I\alpha}
\approx
K_{I[\sk{\lc}\sk{\rc}]}
\begin{bmatrix}
  P_{\sk{\lc}\lc} & \\
                  & P_{\sk{\rc}\rc} \\
\end{bmatrix}
\approx
K_{I\sk{\alpha}} P_{\sk{\alpha}[\sk{\lc}\sk{\rc}]}
\begin{bmatrix}
  P_{\sk{\lc}\lc} & \\
                  & P_{\sk{\rc}\rc} \\
\end{bmatrix}.
\end{equation}
Then we have a \emph{telescoping} expression for the full coefficient matrix:
\begin{equation}
P_{\sk{\alpha}\alpha} =
P_{\sk{\alpha}[\sk{\lc}\sk{\rc}]}
\begin{bmatrix}
  P_{\sk{\lc}\lc} & \\
                  & P_{\sk{\rc}\rc} \\
\end{bmatrix}.
\label{e:telescope}
\end{equation}
We never explicitly form $P_{\sk{\alpha}\alpha}$, but instead use the
telescoping expression during evaluation.

\begin{algorithm}[!t]
\caption{{} [$\sk{\alpha}, P_{\sk{\alpha} \alpha}$]=\texttt{Skeleton}($\alpha$)}
\begin{algorithmic}
  \STATE \texttt{\bf if} $\alpha$ is leaf \texttt{\bf then return} [$\sk{\alpha}, P_{\sk{\alpha} \alpha}$] = {\tt ID}$(\alpha)$;
  \STATE $[\sk{\lc},]=\texttt{Skeleton}(\lc)$; $[\sk{\rc},]=\texttt{Skeleton}(\rc)$;
  \STATE \texttt{\bf return} [$\sk{\alpha}, P_{\sk{\alpha} [\sk{\lc} \sk{\rc}]
  }$] = {\tt ID}($[\sk{\lc} \sk{\rc}]$);
\end{algorithmic}
\label{a:skeletonize}
\end{algorithm}

\algref{a:skeletonize} computes the skeletonization for all tree nodes with a 
postorder traversal.
There are two tasks for each tree node $\alpha$ listed in \tabref{tab:tasks}: 
(1) \texttt{SKEL($\alpha$)} selects $\sk{\alpha}$ (in the critical path) and
(2) \texttt{COEF($\alpha$)} computes $P_{\sk{\alpha}[\sk{\lc}\sk{\rc}]}$.
Notice that in \algref{a:compress} only \texttt{SKEL($\alpha$)} needs to be
executed in postorder (\textbf{POST}), but \texttt{COEF($\alpha$)} can be in any
order (\textbf{ANY})
as long as \texttt{SKEL($\alpha$)} is finished. Such parallelism can
only be specified at the task level, which later inspires our task-based
parallelism in \secref{s:par}.
At the end of the compression, we can optionally evaluate and cache 
all $K_{\beta\alpha}$ in \texttt{Near}$(\beta)$ and all
$K_{\sk{\beta}\sk{\alpha}}$ in \texttt{Far}$(\beta)$ by executing \texttt{Kba($\beta$)} 
and \texttt{SKba($\beta$)} in any order.
Given enough memory (at least $\MA{O}(N)$ for all 
$K_{\beta\alpha}$ and $\K_{\sk{\beta}\sk{\alpha}}$), caching 
can reduce the time spent evaluating and gathering submatrices. 
%Since the parallelism of \algref{a:skeletonize} diminishes during the 
%tree traversal, a proper schedule is required to maximize efficiency. 

\textbf{Evaluation.}
Following~\cite{march-xiao-biros-fmm-e15}, we present
\algref{a:evaluate} a four-step process for computing
\eqref{e:appromatvec}.
The idea is to approximate each \textbf{matvec} $u_{\beta}+=K_{\beta\alpha}w_{\alpha}$
in \texttt{Far}$(\beta)$ using 
%For tree nodes $\alpha, \beta$, we can express the contribution of points in
%$\alpha$ to $u_\beta$ as $K_{\beta\alpha} w_{\alpha}$.
%If $\alpha, \beta$ are far, we use 
a two-sided ID to accumulate
$P_{\sk{\beta}\beta}^{T}K_{\sk{\beta}\sk{\alpha}}P_{\sk{\alpha}\alpha}w_{\alpha}$,
where $P_{\sk{\alpha}\alpha}, P_{\sk{\beta}\beta}$ are given by the
telescoping expression \eqref{e:telescope}. 
For more details, see~\cite{march-xiao-biros-fmm-e15}.

\begin{algorithm}
\caption{{} \texttt{Evaluate}($u$,$w$)}
\begin{algorithmic}[1]
  \STATE (\textbf{POST}) \texttt{N2S($\alpha$)} \# compute skeleton weights $\sk{w}$
  \STATE (\textbf{ANY}) \texttt{S2S($\beta$)} \# apply skeleton basis $K_{\sk{\beta}\sk{\alpha}}$
  \STATE (\textbf{PRE}) \texttt{S2N($\beta$)} \# accumulate skeleton potentials $\sk{u}$
  \STATE (\textbf{ANY}) \texttt{L2L($\beta$)} \# accumulate direct \textbf{matvec} to $u$
\end{algorithmic}
\label{a:evaluate}
\end{algorithm}

The first step is to perform a postorder traversal (\textbf{POST}) on 
\texttt{N2S($\alpha$)} (Nodes To Skeletons).
This computes the \emph{skeleton weights} $\sk{w}_{\alpha}=P_{\sk{\alpha}\alpha}w_{\alpha}$
for each leaf node, and 
$\sk{w}_{\alpha} = P_{\sk{\alpha}[\sk{\lc}\sk{\rc}]}[ \sk{w}_{\lc}; \sk{w}_{\rc}]$
for each inner node.
Recall that in \texttt{COEF($\alpha$)}, we have computed $P_{\sk{\alpha}\alpha}$
for each leaf node and $P_{\sk{\alpha}[\sk{\lc}\sk{\rc}]}$ for each 
internal node. 
\texttt{S2S($\beta$)} (Skeletons to Skeletons) applies the \emph{skeleton basis} $K_{\sk{\beta}\sk{\alpha}}$
and accumulates \emph{skeleton potentials} $\sk{u}$ for each node:
$\sk{u}_{\beta}=\sum_{\alpha\in Far(\beta)}
K_{\sk{\beta}\sk{\alpha}}\sk{w}_{\alpha}$.
As soon as $\sk{w}_{\alpha}$ are computed in \texttt{N2S}, \texttt{S2S} can
be executed in any order.
\texttt{S2N($\beta$)} (Skeletons To Nodes) performs interpolation on the left
and accumulates
$\sk{u}$ with a preorder traversal. This uses the transpose of
\eqref{e:telescope}.
For each node $\beta$, we accumulate
$[\sk{u}_{\lc};\sk{u}_{\rc}] +=
P_{\sk{\beta}[\sk{\lc}\sk{\rc}]}^{T}\sk{u}_{\beta}$ to its children.
In the leaf node, $u_{\beta} = P_{\sk{\beta}\beta}^{T}\sk{u}_{\beta}$
directly accumulates to the output.
These three tasks compute all \textbf{matvec} for the \emph{far} nodes (pink
blocks in \figref{fig:tree}).
All \textbf{matvec} on $K_{\beta\alpha}$ in \texttt{Near}$(\beta)$ (blue blocks)
are computed by \texttt{L2L($\beta$)} (Leaves To Leaves) and directly accumulated to $u_{\beta}$.

\textbf{Complexity.}
The worst case cost of \algref{a:evaluate} is $\MA{O}(N^2)$, when 
$\lvert\mathtt{Near}(\alpha)\rvert=(N/m)$ for all $\alpha$.
The best case occurs when each \texttt{Near}$(\alpha)$ only contains $\alpha$ itself.
We fix the rank $s$ and %consider a balanced binary tree with 
leaf size $m$.
The tree has $\MA{O}(N/m)$ leaf nodes and $\MA{O}(N/m)$ interior nodes, so in the best case,
overall
\texttt{N2S} has $\MA{O}(2ms(N/m)+2s^2(N/m))$ work,
\texttt{S2S} has  $\MA{O}(2s^2(N/m))$ work,
\texttt{S2N} has $\MA{O(2ms(N/m)+2s^2(N/m))}$ work,
and \texttt{L2} has $\MA{O}(2m^2(N/m))$.
When $s$ and $m$ are held constant, the total work is $O(N)$ per right hand side.
In \gofmm{}, this is controlled by the \textbf{budget}.

