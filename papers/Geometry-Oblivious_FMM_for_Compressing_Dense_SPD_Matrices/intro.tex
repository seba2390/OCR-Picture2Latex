We present \gofmm{}, a novel  algorithm for the approximation of dense symmetric positive definite (SPD) matrices. \gofmm{} can be used for compressing a dense matrix and accelerating matrix-vector multiplication operations. As an example, in~\figref{fig:quadratic} we report timings for an SGEMM operation using an optimized dense matrix library and compare with the \gofmm{}-compressed version.
\begin{figure}[!t]
  \centering
  \includegraphics[scale=.27]{figures/quadratic_screen.pdf}
  %\includegraphics[scale=.14]{figures/k1.eps}
  \caption{
					Comparison of \ipoint{runtime in seconds (y-axis)} versus \ipoint{problem
					size $N$ (x-axis)} to multiply test matrix K02 (see \secref{s:setup}) of size $N \times N$
	with a matrix of size $N \times r$, for $r = 512, 1024, 2048$.
	Results are plotted against a linear scale (left) and a logarithmic scale (right).
	The top three curves demonstrate $\MA{O}(N^2)$ scaling of Intel MKL \texttt{SGEMM} for
	each value of $r$.
	The middle curve shows the time for \gofmm{} to compress K02,
	which scales as $\MA{O}(N\log{N})$ in these cases.
	The bottom three curves show the $\MA{O}(N)$ scaling of the time for \gofmm{}
	to evaluate the matrix product for each value of $r$
	after compression is already completed.
	The \gofmm{} results reach accuracies of \num{1e-2} to \num{4e-4} in single precision.
	In these experiments, the crossover (including compression time) is $N=\num{16384}$,
	and for $N=\num{147456}$, we observe an 18$\times$ speedup over \texttt{SGEMM}.
	}
%     Runtime of Intel MKL \texttt{SGEMM} and \texttt{GOFMM} in \ipoint{seconds
%   (y-axis)} versus the \ipoint{matrix size $N$ (x-axis)} using test matrix K02
%   (see \secref{s:setup}). Runtime is plotted against a linear scale (left) and a logarithmic scale (right).
% We demonstrate the difference between a vendor-optimized \texttt{SGEMM} with complexity 
% $\MA{O}(N^2)$ (the top 3 curves) and our $\MA{O}(N\log{N})$ fast multiplication 
% (the bottom 3 lines) in both plots. 
% We also report \gofmm{}'s $\MA{O}(N\log{N})$ compression cost (the middle line). 
% For this problem, the cost of the multiplication operation is $\MA{O}(N)$,
% and \texttt{GOFMM} can reach $1E-2$ to $4E-4$ accuracy in single precision. 
% While this accuracy is sufficient, we can observe a 18$\times$ speedup over the 
% \texttt{SGEMM} for $N=147,456$. We see that, for this problem, 
% the crossover (including compression costs) is $N=16,384$.}
  \label{fig:quadratic}
\end{figure}
 %TODO: Can we you put lines in the left figure and use a differnt shape or differnt thickness for compression? The colors are very hard to see; also change ``perfect ideal time'' to ``ideal O(N) time''

Let $K \in \mathbb{R}^{N\times N}$ be a dense SPD matrix, with $K=K^T$ and $x^T
K x > 0,\ \forall x\in \mathbb{R}^N,\ x \neq 0$. Since $K$ is dense it requires $\bigO(N^2)$ storage  and $\bigO(N^2)$ work for a matrix-vector multiplication (hereby \ipoint{``matvec''}). Using $\bigO(N\log N)$ memory and work, we construct an approximation $\tilde{K}$ such that $\|\tilde{K}-K\| \leq \epsilon \|K\|$, where $\epsilon$ is a user-defined error tolerance. Assuming the evaluation of a single matrix entry $K_{ij}$ requires $\bigO(1)$ work, a matvec  with $\tilde{K}$  requires $\bigO(N \log N)$ or $\bigO(N)$ work depending on the properties of $K$ and the \gofmm{} variant. Our scheme belongs to the class of hierarchical matrix approximation methods.

{\bf Problem statement:} given any SPD matrix $K$, our task is to construct a hierarchically low-rank matrix $\tilde{K}$ so that $\|K-\tilde{K}\|/\|K\|$ is small.  The only required input to our algorithm is a routine that returns $K_{IJ}$, for arbitrary row and column index sets $I$ and $J$. The constant in the complexity estimate depends on the structure of the underlying matrix. Let us remark and emphasize that our scheme \emph{cannot guarantee both accuracy and work complexity} simultaneously since an arbitrary SPD matrix may not admit a good hierarchical matrix  approximation (see ~\secref{s:methods}). 

We say that a matrix $\tilde{K}$ has a \ipoint{hierarchically low-rank structure}, i.e., $\tilde{K}$ is an \hmatrix{}~\cite{hackbusch15,bebendorf08}, if 
\begin{equation}\label{e:hmatrix}
\tilde{K}=D+S+UV,
\end{equation}
where $D$ is \ipoint{block-diagonal} with \ipoint{every block being an \hmatrix{}}, $U$ and $V$  are
\ipoint{low rank}, and $S$ is \ipoint{sparse}.  At the base case of this
recursive definition, the blocks of $D$ are small dense matrices. An \hmatrix{}
matvec requires $\bigO(N \log N)$ work the constant depending on the rank of
$U$ and $V$. Depending on the construction algorithm, this complexity can go down to $\bigO(N)$. Although such matrices are rare in real-world  applications, it is quite common to find matrices that  can be approximated \emph{arbitrarily} well by an \hmatrix{}.

One important observation is that \emph{this hierarchical low-rank structure  is not invariant to row and column permutations}. Therefore any algorithm for constructing $\tilde{K}$ must first appropriately permute $K$ before constructing the matrices $U, V, D$, and $S$. Existing algorithms  rely on the matrix entries $K_{ij}$ being ``interactions'' (pairwise functions) between \ipoint{points} $\{x_i\}_{i=1}^N$ in $\mathbb{R}^d$ and permute $K$ either by clustering the points (typically using some tree data-structure) or by using graph partitioning techniques (if $K$ is sparse). \gofmm{} does not require such geometric information. 

\paragraph{\textbf{Background and significance}}
% TODO: more citations in the following paragraph
Dense SPD matrices appear in scientific computing, statistical inference, and data analytics. They appear in Cholesky and LU factorization~\cite{grasedyck-kriemann-leborne08}, in Schur complement matrices for saddle point problems~\cite{benzi-golub-liesen05}, in Hessian operators in optimization~\cite{biegler-ghattas01}, in kernel methods for statistical learning~\cite{hofmann-scholkopf-smola08,gray-moore01}, and in N-body methods and integral equations~\cite{greengard94,hackbusch15}. In many applications, the entries of the input matrix $K$ are given by $K_{ij} = \Ker(x_i, x_j):\mathbb{R}^d\times\mathbb{R}^d\rightarrow \mathbb{R}$, where $\Ker$ is a \ipoint{kernel function}. Examples of kernel functions are radial basis functions, Green's functions, and angle similarity functions. For such \emph{kernel matrices}, the input is not a matrix, but only the points $\{x_i\}_{i=1}^N$. The  points are used to appropriately permute the matrix using spatial data structures. Furthermore, the construction of the sparse correction $S$ uses  nearest-neighbor structure of the input points. The low-rank matrices $U,V$ can be either analytically computed using expansions of the kernel function, or semi-algebraically computed using fictitious points (or equivalent points), or using algebraic sampling-based methods that use geometric information. In a nutshell, geometric information is used in all aspects of an \hmatrix{} method.

In many cases however, such points and kernel functions are not available. For example, in dense graphs in data analysis (e.g., social networks, protein interactions). Related matrices include graph Laplacian operators and their inverses. Additional examples include frontal matrices and Schur complements in factorization of sparse matrices; Hessian operators in optimization; and kernel methods in machine learning without points (e.g., word sequences and diffusion on graphs~\cite{cancedda-e03,kondor-lafferty02}).

\paragraph{\textbf{Contributions}}
\gofmm{} is inspired by the rich literature of algorithms for matrix sketching, hierarchical matrices, and fast multipole methods.  Its unique feature is that by using only matrix evaluations it generalizes FMM ideas to compressing arbitrary SPD matrices.  In more detail, our contributions are summarized below.
\begin{itemize}[leftmargin=*]\zapspace
  \item A result from reproducing kernel Hilbert space theory is that any SPD
    matrix corresponds to a Gram matrix of vectors in some, unknown Gram (or
    feature) space~\cite{hofmann-scholkopf-smola08}. Based on this result, the matrix entries are inner products, which we use to define distances. These distances allow us to design an efficient, purely algebraic FMM method. 
  \item The key algorithmic components of \gofmm{} (and other hierarchical
    matrix and FMM codes) are tree traversals. %: post order, pre order, and level-by-level. 
    We test parallel level-by-level traversals, \emph{out-of-order} traversals
    using \texttt{OpenMP}'s advanced task scheduling and an in-house tree-task scheduler. 
    We found that scheduling significantly improves the performance when compared to 
    level-by-level tree traversals. We also use this scheduling to support heterogeneous architectures. 
  \item We conduct extensive experiments to demonstrate the feasibility of the
    proposed approach. We test our code on 22 different matrices related to
    machine learning, stencil PDEs, spectral PDEs, inverse problems, and graph
    Laplacian operators. We perform numerical experiments on Intel Haswell
    and KNL, Qualcomm ARM, and NVIDIA Pascal architectures. 
    Finally, we compare with three state-of-the-art codes: \texttt{HODLR},
    \texttt{STRUMPACK}, and \texttt{ASKIT}. 
\end{itemize}
\gofmm{} also has several additional capabilities. If points and kernel
functions (or Green's) function are available, they can be utilized in a
similar way to the algebraic FMM code \texttt{ASKIT} described
in~\cite{march-xiao-yu-biros15,march-xiao-biros-fmm-e15}. \gofmm{} currently
supports three different measures of distance: geometric point-based (if
available), Gram-space $\ell^2$ distance, and Gram-space angle distance.  \gofmm{} has support for matvecs with \ipoint{multiple vectors}, which is useful for Monte-Carlo sampling, optimization, and block Krylov methods.

\paragraph{\textbf{Limitations}}
\gofmm{} is restricted to SPD matrices. (However, if we are given points, the method becomes similar to existing methods).
\gofmm{} guarantees symmetry of $\tilde{K}$, but if $\|K-\tilde{K}\|/\|K\|$ is large, positive definiteness may be compromised. 
To reiterate, \gofmm{} cannot simultaneously guarantee both accuracy and work complexity.
This initial implementation of \gofmm{} supports shared-memory parallelism and accelerators, but not distributed memory architectures.
The current version of \gofmm{} also has several parameters that require manual tuning.
Often, the main goal of building \hmatrix{} approximations is to construct a factorization of $K$, a topic we do not discuss in this paper.  Our method requires the ability to evaluate kernel entries and the complexity estimates require that these entries can be computed in $\bigO(1)$ time. If $K$ is only available through matrix-free interfaces, these assumptions may not be satisfied. Other algorithms, like \texttt{STRUMPACK}, have inherent support for such matrix-free compression.

\paragraph{\textbf{Related work.}} The literature on hierarchical matrix methods and fast multipole methods is vast. Our discussion is brief and limited to the most related work.
                              
\ipoint{Low-rank approximations.} The most popular approach for compressing arbitrary matrices is a global low-rank approximation using randomized linear algebra. In~\eqref{e:hmatrix}, this is equivalent to setting $D$ and $S$ to zero and constructing only $U$ and $V$. Examples include the CUR~\cite{mahoney-drineas09} factorization, the Nystrom approximation~\cite{williams-seeger01}, the adaptive cross approximation~\cite{bebendorf-rjasanow03}, and randomized rank-revealing factorizations~\cite{martinsson-rokhlin-tygert10,halko-martinsson-tropp11}. These techniques can also be used for \hmatrix{} approximations when $D$ is not zero. Instead of applying them to $K$, we can apply them to the off-diagonal blocks of $K$. FMM-specific techniques that are
a mix between analytic and algebraic methods include
kernel-independent methods~\cite{martinsson-rokhlin07,ying-biros-zorin-03} and the
black-box FMM~\cite{fong-darve09}. Constructing both $U$ and $V$ accurately and with optimal complexity is hard. The most robust algorithms require $\bigO(N^2)$ complexity or higher (randomized methods and leverage-score sampling) since they require one to ``touch'' all the entries of the matrix (or block) to be approximated.

\ipoint{Permuting the matrix.} When $K$ is sparse, the method of choice uses graph-partitioning. This doesn't scale to dense matrices because practical graph partitioning  algorithms scale at least linearly with the number of edges and thus the construction cost would be at least $\bigO(N^2)$~\cite{agullo-darve-e16,karypis-kumar98}. 
%All existing codes for dense matrices we know of are based on spatial de compositions of the underlying geometric points.

\ipoint{\hmatrix{} methods and software.} Treecodes and fast multipole methods originally were developed for N-body problems and integral equations. Algebraic variants led the way to the abstraction of \hmatrix{} methods and the application to the factorization of sparse systems arising from the discretization of elliptic PDEs~\cite{hackbusch15,bebendorf08,ambikasaran-13,greengard-gueyffier-martinsson-rokhlin09,ho-greengard12,xia-e10}. 

Let us briefly summarize the \hmatrix{} classification. Recall the decomposition $K=D+UV+S$, \eqref{e:hmatrix}. If $S$ is zero the approximation is called a hierarchically off-diagonal low rank (HODLR) scheme.  In addition to $S$ being zero, if the \hmatrix{} decomposition of $D$ is used to construct $U$, $V$ we have a hierarchically semi-separable (HSS) scheme. If $S$ is not zero we have a generic \hmatrix{}; but if the $U,V$ terms are constructed in a nested way then we have an $\mathcal{H}^2$-matrix or an FMM depending on more technical details.  HSS and HODLR matrices lead to very efficient approximation algorithms for $K^{-1}$. However, $\MA{H}^2$ and FMM compression schemes better control the maximum rank of the $U$ and $V$ matrices  than HODLR and HSS schemes. For the latter, the rank of $U$ and $V$ can grow with $N$~\cite{chandrasekaran-gu-e10} and the complexity bounds are no longer valid. Recently, here have been algorithms to effectively compress FMM and $\MA{H}^2$-matrices~\cite{coulier-pouransari-darve16,yokota-ibeid-keyes16}. 
%
\begin{table}
%\begin{tabular}{|l|c|c|c|c|}
\begin{tabular}{|>{\columncolor[gray]{0.8}}l|c|c|c|c|} 
\hline
\rowcolor[gray]{0.8}
\textbf{METHOD}  & \textbf{MATRIX} &  \textbf{LOW-RANK} & \textbf{PERM} & $S$ \\
\hline
\texttt{FMM}~\cite{cheng-greengard-rokhlin-99}
      &  $\Ker(x_i,x_j)$  &  EXP & OCTREE & Y \\
\texttt{KIFMM}~\cite{ying-biros-zorin-03}
      &  $\Ker(x_i,x_j)$  &  EQU & OCTREE & Y \\
\texttt{BBFMM}~\cite{fong-darve09}
      &  $\Ker(x_i,x_j)$  &  EQU & OCTREE & Y \\
\texttt{HODLR}~\cite{ambikasaran-darve13}
      &  $K_{ij}$       &  ALG & NONE & N \\
\texttt{STRUMPACK}~\cite{rouet-li-e16}
      &  $K_{ij}$       &  ALG & NONE & N \\            
\texttt{ASKIT}~\cite{march-xiao-yu-biros-sisc16}
&  $\Ker(x_i,x_j)$  & ALG  & TREE & Y \\
\texttt{MLPACK}~\cite{mlpack13}
& $\Ker(x_i,x_j)$ &  EQU  & TREE & Y \\
\textbf{\gofmm} &  $K_{ij}$     & ALG & TREE & Y \\
\hline
\end{tabular}
\caption{We summarize the main features of different \hmatrix{} methods/codes
for dense matrices. ``\textbf{MATRIX}'' indicates whether the method requires a
kernel function and points---indicated by $\Ker(x_i,x_j)$---or it just requires kernel entries---indicated by $K_{ij}$.  ``\textbf{LOW-RANK}'' indicates the method used for the off-diagonal low-rank approximations: ``EXP'' indicates kernel function-dependent analytic expansions; ``EQU'' indicates the use  of equivalent points (restricted to low $d$ problems); ``ALG'' indicates an algebraic method. ``\textbf{PERM}'' indicates the permutation scheme used for dense matrices: ``OCTREE'' indicates that the scheme doesn't generalize to high dimensions; ``NONE'' indicates that the input lexicographic order is used; and ``TREE'' indicates geometric partitioning that scales to high dimensions.  $S$ indicates whether a sparse correction (FMM or $\MA{H^2}$) is supported. In~\secref{s:results}, we present comparisons with \texttt{ASKIT}, STRUMPACK, and HODLR.}\label{t:codes}
\end{table}
%
One of the most scalable methods is \href{http://portal.nersc.gov/project/sparse/strumpack/}{STRUMPACK}~\cite{ghysels-li-e16,rouet-li-e16,martinsson16}, which constructs an HSS approximation of a square matrix (not necessarily SPD) and then uses it to construct an approximate factorization. For dense matrices STRUMPACK uses the lexicographic ordering. If no fast matrix-vector multiplication is available, STRUMPACK requires $\bigO(N^2)$ work for compressing a dense SPD matrix, and $\bigO(N)$ work for the matvec.





%% this should go in the results section
%\paragraph{\textbf{Matrices}}
%{\bf K02} : is a 2D regularized inverse Laplacian squared, resembling the Hessian operator of a PDE-constrained optimization problem. The Laplacian is discretized using a 5-stencil finite-difference scheme with Dirichlet boundary conditions on a regular grid. {\bf K03}: same setup but now we use the 2D regularized oscillatory Helmholtz operator with 10 points per wave lenght. {\bf K04--K10} are kernel matrices in six dimensions (Gaussians with different bandwidths, narrow and wide; Laplacian Green's function, polynomial and cosine-similarity); {\bf K12--K14} are 2D advection-diffusion operators on a regular grid with highly variable coefficients; {\bf K15,K16} are 2D pseudo-spectral advection-diffusion-reaction operators with variable coefficients; {\bf K17} is a 3D pseudo-spectral operator with variable coefficients; {\bf K18} is the inverse squared Laplacian in 3D with variable coefficients; {\bf G01--G05} are the inverse Laplacian of the  ``powersim'', ``poli\_large'', ``rgg\_n\_2\_16\_s0'', ``denormal'', and ``conf6\_0-8x8-30 '' graphs from \href{http://yifanhu.net/GALLERY/GRAPHS/search.html}{UFL}.  In this experiments matrices K02--K03, K12--K14, and K18, resemble inverse covariance matrices and Hessian operators from optimization and uncertainty quantification problems. K04--K10 resemble classical kernel/Green function matrices but in high dimensions. K15--K17 resemble pseudo-spectral operators; and G01--G05 are graphs for which we don't have geometric information.
          
