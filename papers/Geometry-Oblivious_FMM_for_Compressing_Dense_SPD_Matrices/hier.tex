% Hierarchical matrices can be seen as algebraic generalizations of 
% the FMM where a matrix is compressed using both low-rank and sparse representations
% hierarchically ~\cite{bebendorf08}. 
% We consider a matrix $K\in \mathbb{R}^{N\times N}$ to be
% \emph{hierarchical} if it can be approximated as
% \begin{equation}
%   \label{e:partitioning}
%   \sk{K} =
% \begin{bmatrix}
% K_{\lc\lc} & 0 \\ 
% 0 & K_{\rc\rc} \\ 
% \end{bmatrix} + 
% \begin{bmatrix} 
%   0 & UV_{\lc\rc} \\ 
%   UV_{\rc\lc} & 0 \\ 
% \end{bmatrix} +
% \begin{bmatrix} 
% 0 & S_{\lc\rc} \\ 
% S_{\rc\lc} & 0 \\ 
% \end{bmatrix}
% \end{equation} 
% where the \emph{off-diagonal} blocks $K_{\lc\rc}$ and $K_{\rc\lc}$ 
% are \emph{approximated} by some low-rank factorizations
% $UV$ plus a sparse correction matrix $S$, and
% the \emph{on-diagonal} blocks $K_{\lc\lc}$ and $K_{\rc\rc}$ are
% themselves hierarchical.
% For example, on the right side of \figref{fig:tree}, 
% the sparsity of $S$ is in blue, and $UV$ contains
% many submatrices with different low-rank approximations in pink.
% %For simplicity, we write th approximation as $K\approx \sk{K}=D+UV+S$, 
% %where $UV$ are \emph{far} (approximation), $S$ is \emph{near} (no approximation)
% %and $D$ is again hierarchical.
% 
% %Note that the low-rank plus sparse structure is \emph{not
% %invariant} on permutations, it very strongly depends on the ordering
% %of the columns (or rows since the matrix is symmetric).
% 
% \begin{figure}[h]
%   \centering
%   \includegraphics[scale=.3]{figures/nearfar.pdf}
%   \caption{A hierarchical low-rank plus sparse matrix (right) and 
%     its tree representation.
%     The off-diagonal blocks are combinations of low-rank matrices (pink)
%     and sparse matrices (blue). The on-diagonal blocks are further
%     partitioned as two child nodes in the tree.
%     The $\bigstar$ symbols denote the entries (neighbors) that cannot
%     be approximated.
%     \algref{a:nearnear} computes the sparse pattern  in blue.
%     The low-rank pattern in pink is computed by \algref{a:nearfar}
%     and \algref{a:farfar} with a preorder and postorder traversal.
%     The solid edges in the tree show the traversing paths of
%     \texttt{NearFar($\beta$,0)}. The preorder traversal stops and add
%     $\alpha$ to $Far(\beta)$,
%     because $K_{\beta\alpha}$ does not contain any $\bigstar$.
%     \algref{a:farfar} merges common codes from two children's Far lists.
%     For example, after \texttt{NearFar(\lc,0)} and \texttt{NearFar(\rc,0)},
%     $Far(\lc)=\{\rc,4,4\}$ and $Far(\rc)=\{\lc,4,2\}$.
%     The common nodes will then be removed and merged to their parent such that 
%     $Near(\alpha)=\{4,2\}$, 
%     $Near(\lc)=\{\rc\}$ and
%     $Near(\rc)=\{\lc\}$.
%   }
%   \label{fig:tree}
% \end{figure}
% 
% 
% 
% 
% \textbf{Treecodes.}
% Note that the low-rank structure strongly depends on the ordering
% of the columns (or rows since the matrix is symmetric).
% Such ordering is typically exploited by a geometric tree structure, and
% equation \eqref{fig:tree} resembles the relation between a parent
% node $\alpha$ and the two children $\lc$ and $\rc$ in a tree.
% In \figref{fig:tree},
% a pair of treenodes $\alpha$ and $\beta$ correspond to a submatrix
% $K_{\beta\alpha} = \{K_{ij}\lvert i\in{\beta},j\in{\alpha}\}$.
% To create such approximation in \figref{fig:tree} for each
% $K_{\beta\alpha}$, we need to decide whether $\alpha$ and $\beta$
% are \emph{near} (in blue, cannot approximate) or \emph{far} 
% (in pink, prunable) from each other.
% %from the root to the leaf level.
% 
% \begin{algorithm}[!t]
% \caption{{} \texttt{NearNear}()}
% \begin{algorithmic}
%   \STATE \texttt{\bf for each} $\alpha$ and $\beta$ is leaf \texttt{\bf do}
%   \STATE \gap \texttt{\bf if} $\alpha \cap \MA{N}(\beta)$ \texttt{\bf then}
%            $Near(\beta) = Near(\beta) \cup \alpha$;
% \end{algorithmic}
% \label{a:nearnear}
% \end{algorithm}
% 
% \begin{algorithm}[!t]
% \caption{{} \texttt{NearFar}($\beta$, $\alpha$)}
% \begin{algorithmic}
%   \STATE \texttt{\bf if} $\alpha \cap Near(\beta)$ \texttt{\bf then}
%   \texttt{NearFar}($\beta$,\lc); \texttt{NearFar}($\beta$,\rc); 
%   \STATE \texttt{\bf else} $Far(\beta) = Far(\beta) \cup \alpha$;
% \end{algorithmic}
% \label{a:nearfar}
% \end{algorithm}
% 
% \begin{algorithm}[!t]
% \caption{{} \texttt{FarFar}($\alpha$)}
% \begin{algorithmic}
%   \STATE \texttt{FarFar}($\lc$); \texttt{FarFar}($\rc$);
%   %\STATE \texttt{\bf if} $\alpha$ is not leaf \texttt{\bf then}
%   \STATE $Far(\alpha) = Far(\lc) \cap Far(\rc)$; 
%   \STATE $Far(\lc) = Far(\lc) \backslash Far(\alpha)$; $Far(\rc) = Far(\rc) \backslash Far(\alpha)$;
% \end{algorithmic}
% \label{a:farfar}
% \end{algorithm}
% 
% \textbf{Near-far pruning.}
% Let $\MA{N}(\beta)$ be the set of \emph{neighbors} of $\beta$ ($\beta$ itself and indices 
% shown as $\bigstar$ in \figref{fig:tree}).
% We say $\beta$ and $\alpha$ are \emph{near} if $\alpha \cap \MA{N}(\beta)$ 
% is not empty (i.e., $K_{\beta\alpha}$ contains a least one $\bigstar$).
% \algref{a:nearnear} shows how we construct a list of leaf nodes
% $Near(\beta)$ to denote the non-prunable submatrices (blue) in
% \figref{fig:tree}. Here $Near(\beta) = \{\mu, \beta\}$, because $\mu$ 
% contains a neighbor of $\beta$.
% The prunable submatrices (pink) are
% identified with a preorder and postorder traversal in \algref{a:nearfar}
% and \algref{a:farfar} (see the caption of \figref{fig:tree} for examples). 
% For each leaf node $\beta$, \algref{a:nearfar}
% traverses the tree from the root. 
% If $\alpha \notin Near(\beta)$, then we add $\alpha$ into $Far(\beta)$.
% Otherwise, we recurse to the two children of $\alpha$ to see
% if we can add $\lc$ or $\rc$ to $Far(\beta)$.
% %In \figref{fig:tree}, the solid edges denote the path 
% %of \texttt{NearFar($\beta$,0)}.
% %The traversal stops at $\alpha$, 
% %because $\alpha$ is not in $Near(\beta)$. 
% %On the other hands, $Near(\beta) \in \beta \subset$ node 4; thus the
% %traversal continues after visiting node 4.
% Once we have a prunable list $Far(\beta)$ for each leaf node, 
% \algref{a:farfar} traverses bottom-up to merge the common nodes
% from two children lists $Far(\lc)$ and $Far(\rc)$.
% These common nodes are removed from the children and added to 
% their parent's prunable list $Far(\alpha)$, which
% creates larger pink submatrices.
% %For example, after \texttt{NearFar(\lc,0)} and \texttt{NearFar(\rc,0)},
% %$Near(\lc)=\{\rc,4,4\}$ and $Near(\rc)=\{\lc,4,2\}$.
% %The common nodes will then be removed and merged to their parent such that 
% %$Near(\alpha)=\{4,2\}$, 
% %$Near(\lc)=\{\rc\}$ and
% %$Near(\rc)=\{\lc\}$.
% The number of these blue and pink submatrices are usually linear in $N$.
% If the size (for blue) and the rank (for pink) of these submatrices 
% are also constant in $N$, then matrix-vector multiplication may only require
% $\MA{O(N)}$ work with the following scheme.
% 
% \textbf{Fast multiplication.}
% The key for FMM to reach linear time multiplication is to
% compress a pink $K_{\beta\alpha}$ on both sides 
% using nested column and row bases.
% Here 
% we use a two-sided nested Interpolative Decomposition (ID) to
% approximate $UV_{\lc\rc}$ as
% $P_{\sk{\lc}\lc}^{T}K_{\sk{\lc}\sk{\rc}}P_{\sk{\rc}\rc}$.
% Given $\sk{\lc} \subset \lc$
% and $\sk{\rc} \subset \rc$, $K_{\sk{\lc}\sk{\rc}}$ denotes a submatrix
% of $K_{\lc\rc}$, and $P_{\sk{\lc}\lc}$, $P_{\sk{\rc}\rc}$ are
% matrices of coefficients used to interpolate other entries.
% For an inner node $\alpha$, its skeleton
% $\sk{\alpha} \subset \sk{\lc} \cup \sk{\rc}$ is subselected from its
% children's skeletons. With this nested basis, the parent 
% coefficient matrix $P_{\sk{\alpha}\alpha}$ can be computed as a
% linear combination of two children's coefficients as shown in the box
% \begin{equation}
% \sk{w}_{\alpha} = 
% \framebox[1.1\width]{
% $P_{\sk{\alpha}\alpha}$
% }w_{\alpha}
% =
% \framebox[1.1\width]{$
% P_{\sk{\alpha}[\sk{\lc}\sk{\rc}]}
% \begin{bmatrix}
%   P_{\sk{\lc}\lc} & \\
%                   & P_{\sk{\rc}\rc} \\
% \end{bmatrix}
% $}
% \begin{bmatrix}
% w_{\lc} \\
% w_{\rc} \\
% \end{bmatrix}=
% P_{\sk{\alpha}[\sk{\lc}\sk{\rc}]}
% \begin{bmatrix}
%   \sk{w}_{\lc} \\
%   \sk{w}_{\rc} \\
% \end{bmatrix}
% \label{e:telescope}
% \end{equation}
% While multiplying weight $w$ from the right, \eqref{e:telescope}
% allows us to compute
% the skeleton weights $\sk{w}_{\alpha}$ for all $\alpha$ in 
% linear time with a postorder traversal (\emph{telescoping}). 
% If we assume there is only a linear number of submatrices $K_{\beta\alpha}$, then
% accumulating the approximate sum for each node $\beta$ as
% \begin{equation}
%   \sk{u}_{\beta} = 
%   \sum_{\alpha \in Far(\beta)} K_{\sk{\beta}\sk{\alpha}}
%   \sk{w}_{\alpha} = 
%   \sum_{\alpha \in Far(\beta)} K_{\sk{\beta}\sk{\alpha}}
%   P_{\sk{\alpha}\alpha}
% \end{equation}
% takes linear time. Similarly, we can apply the same \emph{telescoping} 
% relation from the left (preorder) such that all skeleton potentials 
% \begin{equation}
%   \begin{bmatrix}
%     \sk{u}_l \\
%     \sk{u}_r \\
%   \end{bmatrix} +=
% \framebox[1.1\width]{$
% \begin{bmatrix}
%   P_{\sk{\lc}\lc} & \\
%                   & P_{\sk{\rc}\rc} \\
% \end{bmatrix}^{T}
% P_{\sk{\beta}[\sk{\lc}\sk{\rc}]}^{T}
% $}
% \sk{u}_{\beta}
% \end{equation}
% can also be computed in linear time. Finally, we assume that
% the size of $Near(\beta)$ is constant for each leaf node.
% Then computing $u_{\beta} = \sk{u}_{\beta} + \sum_{\alpha \in
% Near(\beta)}K_{\beta\alpha}w_{\alpha}$ for all leaf node $\beta$
% can also be done in linear tme.
% 
% Overall, FMM assumes that $Near(\alpha)$ and $Far(\alpha)$ 
% are both $\MA{O}(1)$ and uses nested basis on both sides to achieve
% linear time complexity.
% For hierarchical partitioning directly derived from the geometric space, this 
% near-far pruning may be straight forward.
% However, so far we have not yet defined the neighbors 
% ($\bigstar$) and how to partition $K$ as a tree for an arbitrary 
% SPD matrix. 
% In \secref{s:seq}, we illustrate an idea that generalizes this
% near-far pruning to general SPD matrices using the Gramian vector space.
