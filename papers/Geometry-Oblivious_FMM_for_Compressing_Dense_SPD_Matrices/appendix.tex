\subsection{Abstract}
This artifact description appendix comprises the source code, datasets and installation
instruction on a GitHub repository that will be open source and
used to reproduce results for our SC'17 paper.
We also provide all hardware and software configuration in 
\secref{s:sup_setup}.
Due to the double-blind peer reviewing policy, we can only provide 
the url to the repository upon the acceptance.

\subsection{Description} \label{s:sup_setup}

\textbf{Check-list.} 
We briefly describe all meta information.
This program implements an algebraic Fast Multipole Method
with geometric oblivious technique that generalizes to 
SPD matrices.
\begin{itemize}[leftmargin=*]\zapspace
  \item \textbf{Program.} \texttt{GOFMM} is developed in \texttt{C++} (with
    \texttt{C++11} features)
  and \texttt{CUDA}, employing \texttt{OpenMP} 
  for shared memory parallelism using a self-contained runtime system.
\item \textbf{Hardware.}We conducted experiments on
        Lonestar5 (two 12-core, 2.6GHz, Xeon E5-2690 v3  ``Haswell''
        per node) and Stampede (68-core, 1.4GHz, Xeon Phi 7250 ``KNL'' per node)
        clusters at the Texas Advanced Computing Center,
Piz Daint (12-core, 2.3GHz, Xeon  E5-2650 v3 and NVIDIA Tesla P100)
at Swiss National Supercomputing Centre,
and an Intrinsyc Open-Q 820 Development Kit (quad-core, 2.2GHz Qualcomm Kyro).
\item \textbf{Compilation.} All software (including \texttt{HODLR}, \texttt{STRUMPACK} and \texttt{ASKIT}) 
are compiled with 
\texttt{intel-16.0 -O3}  on
Lonestar5 and Piz Daint. Stampede uses \texttt{intel-17.0 -O3 -xMIC-AVX512}.
The GPU part uses \texttt{nvcc-8.0 -O3 -arch$=$sm\_60}. 
For Open-Q 820, we cross compile our software with NDK using 
\texttt{gcc-4.9 -O3}.
All CPU and KNL BLAS/LAPACK routines use MKL.
GPU BLAS routines use CUBLAS; on ARM we use QSML
(Qualcomm Snapdragon Math Library).
KNL experiments use Cache-Quadrant configuration. 
\texttt{OpenMP} uses $\texttt{OMP\_PROC\_BIND=spread}$.
\item \textbf{Datasets.} Our 22 matrices \textbf{K02}--\textbf{G05} can be generated using 
  MATLAB scripts (provided in the repo). 
  The urls of the five graphs \textbf{G01}--\textbf{G05} and the
  For real world datasets, we provide urls in \secref{s:setup}.
\item \textbf{Output.} Runtime and total \texttt{FLOPS} of the compression
  and evaluation phase, accuracy $\epsilon_2$ of the first 10 entries
  and the average of 100 entries.
\item \textbf{Experiment workflow.} \texttt{git clone} projects;
  generate datasets; run test scripts; observe the results;
\end{itemize}

\textbf{How delivered.}
Upon acceptance, we will provide the url to the \gofmm{} 
repository on GitHub. The software comprises code, build, and evaluation
instructions, and is provided under GPL-3.0 license.

%Our datasets can be generated using MATLAB scripts (provided
%    in the repo). For real world datasets, we will provide urls in
%the README file as well.
%For a standard execution, the program will execute \algref{a:compress}
%and \algref{a:evaluate} once. 
%The output contains runtime (in second), \texttt{FLOPS} and 
%accuracy report on 100 entries. 

\textbf{Hardware dependencies.}
For adequate reproducibility, we suggest that reproducers use
the same environment as mentioned above. Notice that we report
absolute \texttt{GFLOPS} and the ratio to the peak performance
in the paper. For approximately reproducing the same results
on a different environment, reproducer should look for platform
that has similar capability.
The theoretical peak performance\footnote{\scriptsize{We estimate the peak 
according to the clockrate
and the \texttt{FMA} throughput. For 24 Haswell cores,
$998=2\times12\times2.6\times16$. For 68 KNL cores,
$3046=68\times1.4\times32$. For 4 ARM cores, $35.2=4\times 2.2\times 4$.
The peak of P100 is reported as 4.7 \texttt{TFLOPS}.
As a reference, MKL \texttt{GEMM} can achieve $87\%$ on a Haswell node and 
$69\%$ on a KNL node. QSML \texttt{GEMM} can achieve $89\%$ on Open-Q 820.
\texttt{cublasXgemm} can achieve $95\%$ on P100.
We assume two KNL VPUs can dual issue \texttt{DFMA}s~\cite{sodani2016knights}.
However, Intel processors may have a different frequency while fully issuing 
  \texttt{FMA}, and the clockrate may drop to 1.0 GHz. This may be the reason
why MKL \texttt{DGEMM} can only achieve 2.1 TFLOPS on KNL.}} in double precision is $998$ GFLOPS per Haswell node, $3,046$ GFLOPS
per KNL node, ($4,700+416$) GFLOPS per Tesla P100 node, and $35.2$ GFLOPS 
per Open-Q820.
The peak \texttt{GFLOPS} doubles for single precision computations.

\textbf{Software dependencies.}
Compilation requires \texttt{C/C++} compilers that support \texttt{c++11}
features and \texttt{OpenMP}. \gofmm{} also requires full functionality 
of BLAS and LAPACK routines.

\subsection{Installation} \label{s:sup_install}
Given the repository url, you should be able to clone the \texttt{master}
branch of the repository. The first step is to edit \texttt{set\_env.sh}
to select the proper compiler and architecture.

\
\begin{verbnobox}[\small]
export GOFMM_USE_INTEL = true  % to use Intel compilers.
export GOFMM_USE_INTEL = false % to use GNU compilers.
export GOFMM_USE_CUDA  = true  % to compile the CUDA code
export GOFMM_MIC_AVX512= true  % to compile for KNL
\end{verbnobox}
\

If user want to compile the CUDA code for the hybrid CPU-GPU implementation
then the following variables have to be exported.

\
\begin{verbnobox}[\small]
export GOFMM_GPU_ARCH_MAJOR=gpu
export GOFMM_GPU_ARCH_MINOR=pascal
\end{verbnobox}
\

There are three options for the host (ARM, x86-64 or KNL). Users must 
choose at least one major and minor architecture to compile.
This can be arm/armv8a, x86-64/haswell or mic/knl.

\
\begin{verbnobox}[\small]
export GOFMM_ARCH_MAJOR=arm
export GOFMM_ARCH_MINOR=armv8a

export GOFMM_ARCH_MAJOR=x86_64
export GOFMM_ARCH_MINOR=haswell

export GOFMM_ARCH_MAJOR=mic
export GOFMM_ARCH_MINOR=knl
\end{verbnobox}
\

Although we use \texttt{cmake} to identify BLAS/LAPACK libraies, but we
suggest that user manually setup the path using

\
\begin{verbnobox}[\small]
export GOFMM_QSML_DIR = /path-to-qsml
export GOFMM_MKL_DIR = /path-to-mkl
export GOFMM_CUDA_DIR  = /path-to-cudatoolkit
\end{verbnobox}
\

Finally, users must setup the OpenMP option to enable parallel implementation.
Here for example, we use 68 threads for KNL and \texttt{spread}
\texttt{OpenMP} thread binding. 

\
\begin{verbnobox}[\small]
export OMP_PROC_BIND=spread
export OMP_NUM_THREADS=68
\end{verbnobox}
\

With all these options setup, now we use \texttt{cmake} for compilation. Users can use the following commends.

\
\begin{verbnobox}[\small]
source set_env.sh
mkdir build
cd build
cmake ..
make
make install
cd bin
./run_gofmm_x86
./run_gofmm_gpu
./run_gofmm_knl
\end{verbnobox}
\

\textbf{Cross compilation.}
If your ARM runs with OS that has native compiler and \texttt{cmake} support, 
then the installation instructions above should work just fine. However, while your
target runs an Android OS, which currently does not have a native
\texttt{C/C++} compiler, you will need to cross compile this software on your 
Linux or OSX first.
Although there are many ways to do cross compilation, we suggest that users
follow these instructions:
\begin{itemize}[leftmargin=*]\zapspace
  \item Install Android Studio with LLDB, cmake and NDK support.
  \item Create stand-alone-toolchain from NDK.
  \item Install adb (Android Debug Bridge)
  \item Compile with cmake. It will look for your arm \texttt{gcc/g++},
    \texttt{ar} and \texttt{ranlib} support.
  \item Use the following instructions to push executables and scripts in
    /build/bin to the Android ARM device.
\end{itemize}

\
\begin{verbnobox}[\small]
adb devices
adb push /build/bin/* /data/local/tmp
adb shell
cd /data/local/tmp
./run_gofmm_arm.sh
\end{verbnobox}
\

\subsection{Dataset}
The 22 matrices we use can be generated using MATLAB scripts with the 
corresponding coordinates or graphs. The five graphs we use in the paper
are reported in \secref{s:setup}, and the MATLAB scripts will be provided
as parts of the source code upon acceptance.

\subsection{Experiment workflow}
With the repository url, \texttt{git clone} projects.
Generate datasets using the provided MATLAB script.
Compile \gofmm{} with the instructions in \secref{s:sup_install}.
Run the test script for each architecture. Observe the results.

%Notice that the user interface of \gofmm{} is not yet finalized. Here
%we briefly introduce the interface of our test suit, and provide 
%the prototype of two main routines (compress and evaluate). 
%Here we demonstrate the example code that compresses an $5000\times5000$
%random SPD matrix that is generated by our routines.
%
%\begin{verbnobox}[\small]
%  /** type */
%  using T = float;
%  Data<T> *X = NULL;
%  SPDMatrix<T> K;
%
%  /** parameters */
%  //const SplitScheme SPLIT = SPLIT_RANDOM;
%  //const SplitScheme SPLIT = SPLIT_LEXIGRAPHICAL;
%  //const SplitScheme SPLIT = SPLIT_POINT_DISTANCE;
%  //const SplitScheme SPLIT = SPLIT_KERNEL_DISTANCE;
%  const SplitScheme SPLIT = SPLIT_ANGLE;
%  const bool ADAPTIVE = true;
%  const bool LEVELRESTRICTION = false;
%  std::string filename = std::string( "K02N65536.bin" );
%  std::size_t n = 65536;
%  std::size_t m = 512;
%  std::size_t k = 32;
%  std::size_t s = 512;
%  T tau = 1E-5;
%  std::size_t r = 512;
%
%  /** if coordinates are provided */
%  std::size_t d = 2;
%  std::string pointfilename = std::string( "X2DN65536.points.bin" );
%  X = new Data<T>( d, n, pointfilename );
%
%  /** read the matrix */
%  K.resize( n, n );
%  K.read( n, n, filename );
%
%  /** provide neighbors if any, otherwise using ANN  */
%  Data<std::pair<T, std::size_t>> NN;
%
%  gofmm_setup<ADAPTIVE, LEVELRESTRICTION, SPLIT, T>
%  ( X, K, NN, n, m, k, s, tau, r );
%\end{verbnobox}

\subsection{Evaluation and expected result}
For x86-64, ARM and KNL execution, the program will start from
the iterative ANN. The accuracy is reported in every iteration.
Once the neighbor search is done (or skipped), the metric ball
tree partitioning follows.
The program reports runtime and total \texttt{FLOPS} of the compression
  and evaluation phase.
Finally, the accuracy $\epsilon_2$ is reported in two parts: 
the error of the first 10 entries, and the average error of 100 entries.
Notice that in a CPU-GPU hybrid environment, \gofmm{} will
first try to detect the available GPU device. 
If successful, the device name and the available global memory
size should be displayed.
The rest of the execution is the same as our architectures. 
