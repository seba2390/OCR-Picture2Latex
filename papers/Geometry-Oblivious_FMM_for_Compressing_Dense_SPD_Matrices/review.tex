---REVIEWER 1---

Summary and High Level Discussion
Authors describe an algebraic variant of FMM (GOFMM) where geometry
information is not needed. The distance metric in the absence of geometric
coordinates is the Gram distance calculated from a set of unknown vectors that
come from the Gramian matrix. 
Strength of the papers: 
1- Very elaborate implementation of the method, and detailed analysis of
complexity.
2- Solid literature review and clear narrative of contributions.
3- Steps to reproduce results were mentioned in the appendix. 

Weaknesses of the paper:
1- Sometimes paragraphs are incoherent and hard to follow. Lots of
proofreading needs to be done.
2- No elaborate comparison of multicore and manycore archs. although there are
strong claims about share memory optimizations made in the contributions.

Comments for Rebuttal
1- This looks like a specialization of (ASKIT: An Efficient, Parallel Library
    for High-Dimensional Kernel Summations). You use specific distance
measures, i.e, gram distance and angle. But ASKIT relaxes the distance
measure (using statistical measures) and supports higher dimensions. So what
are the key advantages of your method over ASKIT besides performance?

2- It's known that omp tasks are not favorable to task-based parallelism, and
there's plenty of DAG based runtime schedulers, e.g. StarPU, Ompss? Why did
you have to write your own scheduler?

3- On KNL and Haswell experiments, why didn't you run the code serially on a
single core? Why did you not utilize the hyperthreads of KNL (68 x 2 and 68 x
    4) to support the claims made about the strength of your DAG-based
scheduler. 
Also, since you are using KNL chips, it is not possible to achieve the
desired floating point performance without using the vector units. I didn't
see that in the current work or in the future work. You are getting a maximum
of around 1000 Gflops, which is 1/6 of the maximum single precision peak.

Detailed Comments for Authors
1- The paper has serious readability issue with many incoherent fragments. 
2- Very tedious background and introduction. Literature review and
background sections should be merged for example. 
3- Details from previous algorithm are mentioned in text, like
near-far-pruning. This could've been avoided by just making brief references
to literature. 
3- In Figure 1, you have the same figure twice with/without log scale.
4- In Figure 6, the same experiments need to be done for HSS and GOFMM.
Currently the experiments look misaligned. If this is not possible
otherwise, a clear explanation should be given on why this is not the case.
5- The captions under the figures are long and distracting. They should be
at most 3 lines, and all the text should go to the body. 
6- A lot of vague abbreviations are used in the text, although explained.
It's very hard to remember things like s, g, m, the ones used in the
experiments sections.

---REVIEWER 2---

Summary and High Level Discussion
The paper presents a fast multipole method for compression of dense symmetric
positive definite matrices that does not rely on apriori knowledge of the
underlying graph. The approach is sound, however, there is no discussion on
applicability of the approach on a distributed memory environment.

Comments for Rebuttal
There is no mention of the extension of this method to a distributed
environment.

Detailed Comments for Authors
Overall: The paper presents a fast multipole method for compression of dense
symmetric positive definite matrices that does not rely on apriori knowledge
of the underlying graph. The approach is sound, however, there is no
discussion on applicability of the approach on a distributed memory
environment.

Strength: The presentation is clear and the approach is solid.

Insufficiency: There is no mention of the extension of this method to a
distributed environment.

Some of the plots are hard to read. For example, in Figure 1, the order of
the entries in the legend should match the order of the lines in the plot.
This would improve readability and make it easier to match a line to a
legend entry. Also, in Figure 5, there is no concept of continuity when
working with discrete matrix examples, hence, why are the entries connected
with lines. A bar graph of some wort would be better suited here.

---REVIEWER 3---

Summary and High Level Discussion
This paper considers using the fast multi-pole method as a tool for the
compression of black-box SPD matrices. The authors compare their approach
against similar libraries, albeit using slightly different algorithms.

While the paper has good use of figures to illustrate the discussed
algorithms, some of the details of the algorithms employed are unclearly
written, e.g., the algorithmic description is extremely dense, with a lot of
details that makes it a bit of a challenge to fully understand the algorithm
(certainly at first pass). Disappointing to see no application to a
distributed setting.

Comments for Rebuttal
At the end of section 4, there is the comment that no one architecture is
suitable for the algorithm, since it contains both extremely parallel and
serial portions. Can the authors expand this discussion in the context of how
HPC hardware is trending to being increasingly parallel. Will this pose
problems, e.g., in the Exascale, if there is a lack of serial processing
performance. Is there a minimum about serial processing throughput required
in order to ensure the algorithm is efficient. For example a minimum CPU to
GPU ratio or in the case of more homogenous architecture, a maximum vector
size that the algorithm will tolerate. 

The conclusion is relatively weak and simply sums up the paper in a couple of
sentences and decribes future objectives. I would like to see a discussion of
the relevance of this algorithm, and similar algorithms in the future. What
makes these algorithms attractive for computational science? Will the
techniques propsed in the paper have increased relevance in computational
science?

Detailed Comments for Authors
Just a comment that the poor strong scaling seen may be caused by NUMA
issues on the Haswell dual socket system, and memory coherence issues on
KNL. It is likely that significantly improved performance would be observed
if MPI was used on both platforms (2-way and 4-way on Haswell and KNL
    respectively).

---REVIEWER 4---

Summary and High Level Discussion
This paper proposes a new method to compress dense symmetric positive definite
(SPD) matrices. A fast multipole method is generalized to only require
sampling matrix entries. A shared-memory implementation is provided, with
results on 22 different matrices on several different architectures.
Strengths:
Good, consistent terminology is used throughout the paper.
A variety of matrices are analyzed on a range of (well-described) hardware
systems.
Weaknesses:
The paper is quite dense, especially in Section 2.
Occasionally there are sentence fragments and other grammatical issues that
distract from the content.

Comments for Rebuttal
What versions of software were used?
In the index nearest neighbor list (lines 297-305), why were 80% accuracy and
10 iterations chosen as stopping criteria?

Detailed Comments for Authors
Line 172 re-orders the sum in equation 1; please be consistent.
Lines 609-629: does s refer to seconds or max. rank (or both)? For instance,
line 616 reports 0.9s in compression.
Tables 3 and 4 may benefit by bold-facing the best results in each row.
The many different experiments run are well organized in the charts, with
good analysis provided.
Table 5 claims to provide efficiency results in GFLOPS; generally,
efficiency is a percentage of peak performance.


