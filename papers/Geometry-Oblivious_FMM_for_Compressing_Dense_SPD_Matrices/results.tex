We label all experiments from \#\ref{exp:covtypehaswel} to
\#\ref{exp:g04knl} in tables and figures.
We perform \ipoint{strong scaling results} on a single Haswell and KNL node
in \figref{fig:scaling}, comparing different scheduling schemes.
In \figref{fig:robust}, we examine the \ipoint{accuracy} of \gofmm{}  for the different matrices; notice that not all 22 matrices admit good hierarchical low-rank
structures in the original order (lexicographic). In \figref{fig:fmmvshss}, we
\ipoint {compare FMM} ($S\neq 0$ in \eqref{e:hmatrix}) \ipoint{to HSS} ($S=0$)
and show an example in which increasing direct evaluations in FMM results in higher accuracy and shorter wall-clock time. 
In \figref{fig:partition}, we present a comparison between 
\ipoint{five permutation} schemes; matrix-defined Gram distances work quite well.

For reference, we compare \gofmm{} to \ipoint{three other codes}:
\texttt{HODLR} and \texttt{STRUMPACK} ($S=0$ in these codes) in
\tabref{tab:existingSoftware} and \texttt{ASKIT} (high-$d$ FMM) in \tabref{tab:askitcomparison}. 
The two first codes do not permute $K$. \texttt{ASKIT} is similar to \texttt{GOFMM} but uses level-by-level traversals, does not produce a symmetric $\tilde{K}$,  and requires points.   Finally, we test \gofmm{} on \ipoint{four different architectures} in \tabref{tab:arch}; the performance of \texttt{GOFMM} correlates with the performance of BLAS/LAPACK.

\begin{figure}[!t]
  \centering
  \includegraphics[scale=.3]{figures/strong_screen.pdf}
  \caption{
  Strong scaling on a single Haswell and KNL node
  (y-axis, time in seconds on the right, absolute efficiency to the
  peak \texttt{GFLOPS} on the left).
  We use $s512$, $\tau1E-5$ and $r512$.
  \#\ref{exp:covtypehaswel} and \ref{exp:covtypeknl}
  use \textbf{COVTYPE} to create a Gaussian kernel matrix with 
  $m800$ and $12\%$ budget ($h=0.1$), achieving $\epsilon_2 = \num{2E-3}$
  with average rank $487$.
  \#\ref{exp:k02haswell} and \#\ref{exp:k02knl} use \textbf{K02}
  with $m512$ and $3\%$ budget, achieving $\epsilon_2=\num{5E-5}$
  but only with average rank $35$. 
  We increase the number of cores up to 24 Haswell cores and 68 KNL
  cores. Each set of experiments contains compression time and evaluation time on
  three different parallel schemes: wall-clock time, level-by-level and omp tasks.
  We cannot perform scaling experiments for the hybrid CPU-GPU platform
  (see \tabref{tab:arch} for GPU performance).
  }
  \label{fig:scaling}
\end{figure}


\textbf{Strong scaling (\figref{fig:scaling}).}
In 
\#\rownumber\label{exp:covtypehaswel}, 
\#\rownumber\label{exp:covtypeknl}, 
\#\rownumber\label{exp:k02haswell}, 
\#\rownumber\label{exp:k02knl}, 
we use a 24-core Haswell and a 68-core KNL to perform strong scaling
experiments. Each set of experiments contains 6 bars including 3 different
parallel schemes on both \algref{a:compress} and \algref{a:evaluate}.
The blue dot indicates the absolute efficiency (ratio to the peak) of 
our \textbf{evaluation} using dynamic scheduling.
\#\ref{exp:covtypehaswel} and
\#\ref{exp:covtypeknl} require $12\%$ budget with 
average rank $487$ to achieve \num{2E-3}. 
This compute-bound problem can reach $65\%$ peak performance on Haswell and $33\%$ on KNL.
However, \#\ref{exp:k02haswell} and \#\ref{exp:k02knl}
only require $3\%$ budget with average rank $35$ 
to achieve $\num{5E-5}$. As a result, this memory-bound problem does not 
scale ($46\%$ and $8\%$\footnote{\scriptsize
The average rank of \#\ref{exp:k02knl} is too small. Except for
\texttt{L2L} tasks, other tasks can only reach about $5\%$
of the peak during the evaluation.
We suspect that MKL' \texttt{SGEMM} uses a $30\times16$ 
micro-kernel to perform a $30\times256\times16$ rank-$k$ update
each time. For an $m\times k\times n$ \texttt{SGEMM} to be efficient, 
$m$ and $n$ usually need to be at least four times of the
micro-kernel size in each way. 
In \#\ref{exp:k02knl}, many \texttt{SGEMM}s have $m<30$. 
Still the micro-kernel must compute $2\times30\times256\times16$ FLOPS.
These sparse FLOPS are not counted in our experiments.
}) very well.  In \#\ref{exp:k02knl}, we can even observe slow down from 34-core to 68-core.
This is because the wall-clock time is bounded by the task in the critical path;
thus, increasing the number of cores does not help.

Throughout, we can observe that the wall-clock time for compression is less than
the level-by-level and \texttt{omp task} traversals. While the work
of \texttt{SKEL} is bounded by $2s^3$, parallel \texttt{GEQP3} in the
level-by-level traversal does not scale (especially on KNL).  On the
other hand, task based implementations can execute \texttt{COEF}
and \texttt{Kba} out-of-order to maintain the parallelism. Our wall-clock time
is better that \texttt{omp task} since we use the cost-estimate model
for scheduling.


%\textbf{Comparison to existing software (\tabref{tab:existingSoftware}).}
%We compare our methods to two existing software packages:
%\texttt{HODLR}~\cite{ambikasaran2013mathcal} and 
%\texttt{STRUMPACK}~\cite{Rouet2016}.
%Both methods use hierarchical low-rank matrices, but there
%are some subtle difference in the complexity.
%Although, \texttt{HODLR} refers to a class of matrices, 
%but here we refer to the original implementation that uses
%Adaptive Cross Approximation (ACA, partial pivoted LU) and the
%\texttt{Eigen} template. Notice that ACA does not use nested
%low-rank basis; thus, its evalaution requires $O(N\log{N})$ work.
%On the other hand, \texttt{STRUMPACK} constructs a HSS representation
%in $O(N\log{N})$ work.
%This is done by using randomized ID according to \cite{liberty2007randomized}. 
%Users may specify random vectors for sampling, but in many cases this is unknown; 
%we used a black-box compression routine using a uniform random distribution 
%and a householder rank-revealing QR.
%Once the matrix is compressed, the evaluation time is $O(N)$
%per right hand side.
%
%In \tabref{tab:existingSoftware}, we try to fix the accuracy $\epsilon_2$
%such that we can better compare the compression and evaluation time.
%\texttt{GOFMM} uses \textbf{Angle} distance for neighbor search and
%tree partitioning. \texttt{HODLR} and \texttt{STRUMPACK} do not have 
%built-in partitioning schemes; thus, both software use lexicographic
%order.
%Observer that fixing the accuracy, \texttt{GOFMM} usually has the 
%shortest compression and evaluation time.
%\texttt{STRUMPACK} fails to compress K04 (Gaussian kernel in 6D) and
%K07 (Laplace kernel in 6D). We suspect that this is because the
%lexicographic order is equvilent to a random order for these two
%cases. K17 is difficult to compress with a pure hierarhical low-rank
%matrix. Still \texttt{GOFMM} can reach competitive accuracy
%but with lower cost using direct evaluation as a sparse correction
%of the off-diagonal blocks.
%Finally, G03 can be approximated with low-rank plus sparse easily.
%\texttt{HODLR} and \texttt{STRUMPACK} can only increase the off-diagonal 
%ranks to improve accuracy. As a result, \texttt{GOFMM} is about
%$25\times$ faster in compression and about $1.5\times$ faster in evaluation.
%


%There exist a few software packages on hierarchical off-diagonal low-rank matrices. 
%HODLR \cite{ambikasaran2013mathcal} \& STRUMPACK \cite{Rouet2016} 
%can be applied black-box on stored matrices and in the explicit matrix version assume that given ordering allows hierarchical semi-separability (hss). Here, we are only interested in the compression and evaluation methods (but both codes also offer factorization). We compared it to our method that only requires access to entries $K_{ij}$.

%For compression, HODLR uses a partial pivoted LU decomposition for off-diagonal blocks; 
%STRUMPACK uses a 
%randomized interpolative decomposition according to \cite{liberty2007randomized}. The user may specify random vectors for sampling, but in many cases this is unknown; 
%we used a black-box compression routine using a uniform random distribution 
%and a householder rank-revealing QR. 
%Results for a few interesting cases are listed in table \ref{tab:existingSoftware}; in some of them notable differences occur. K04 is a Gaussian kernel and K07 a Laplace kernel for Gaussian random points; the applied ordering is random and STRUMPACK can lead to very inaccurate results. 
%



\begin{figure}[!t]
  \centering
  \includegraphics[scale=.24]{figures/robust_screen.pdf}
  \caption{
    \#\ref{exp:robust}, relative error $\epsilon_2$ 
    (y-axis, the smaller the better) 
    on all matrices (x-axis) using angle distance. 
    Blue bars use $\tau\num{1E-2}$ and $1\%$ budget
    (except for \textbf{K6}, \textbf{K15}, \textbf{K16}, \textbf{K17}, other matrices take $0.8$s
    to compress and $0.1$ to evaluate in average).
    Green bars use $\tau\num{1E-5}$ and $3\%$ budget
    (in average, compression takes $1$s and
    evaluation takes $0.2$s).
    Red labels denotes matrices that do not compress.
    \textbf{K13} and \textbf{K14} have hierarchical low-rank structure, but the adaptive ID underestimates the rank.
    \textbf{K13} and \textbf{K14} can reach high accuracy (yellow plots) 
    with $\tau\num{1E-10}$ and $3\%$ budget ($1.0$s in compression
    and $0.2$s in evaluation).
  }
  \label{fig:robust}
\end{figure}


\textbf{Accuracy (\figref{fig:robust}).}
We conduct \#\rownumber\label{exp:robust} to examine the accuracy of \gofmm{} (up to single precision).
Given $m512$, $s512$ and $r512$, we report relative error $\epsilon_2$
on {\bf K02-18} and {\bf G01-G05} using the \textbf{Angle} distance with
two tolerances: $\num{1E-2}$ (in blue) and $\num{1E-5}$ (in green). 
Throughout, except for \textbf{K06}, \textbf{K15}--\textbf{K17} (high rank),
\textbf{K13}, \textbf{K14} (underestimating the rank), and
\textbf{G01}--\textbf{G03} (requiring smaller leaf size $m$),
other matrices can usually achieve high accuracy with tolerance $\num{1E-5}$
($0.9$s in compression and $0.2$s in evaluation).
Our adaptive ID underestimates the rank of \textbf{K13} and \textbf{K14} 
such that $\epsilon_2$ is high. By imposing a smaller tolerance 
$\num{1E-10}$ (yellow plots), both matrices 
reach $\num{1E-5}$ ($1$s in compression and $0.2$s in evaluation).
\textbf{K6}, \textbf{K15}--\textbf{K17} have high ranks in the 
off-diagonal blocks; thus they cannot be compressed with $s512$ 
and $3\%$ budget. \textbf{G01}--\textbf{G03} requires direct evaluation in the off-diagonal 
blocks to reach high accuracy. When we reduce the leaf node size from $512$ to $64$, 
we can can still reach $\num{1E-5}$ (orange plots).
However, decreasing leaf size to $64$ results in a longer wall-clock time
($0.8s$ in evaluation), because small $m$ hurts performance.
Overall, we can observe that \texttt{GOFMM} can quite robustly discover
low-rank plus sparse structure from different SPD matrices.
We now investigate how increasing the cost (either with higher rank
or more direct evaluations) can  improve accuracy.

\begin{figure}[!t]
  \centering
  \includegraphics[scale=.29]{figures/fmmvshss_screen.pdf}
  \caption{Comparison between HSS and FMM in
  wall-clock time (seconds, green bars, right y-axis) and accuracy ($\epsilon_2$, blue
  plots, left y-axis).
  In \#\ref{exp:k02robust}, \#\ref{exp:k15robust}
  and \#\ref{exp:covtyperobust},
  we use \textbf{K02}, \textbf{K15} ($m512$) and \textbf{COVTYPE} ($m800$)
  datasets. 
  The fixed rank and budget are labeled on x-axis.
  The green bar is the total wall-clock time including compression and evaluation
  on $512$ right hand sides. 
  For some experiments, we also provide wall-clock time for evaluation to contrast
  the trade-off of using high rank and high budget.
  }
  \label{fig:fmmvshss}
\end{figure}


\textbf{Comparison between FMM and HSS (\figref{fig:fmmvshss}).}
We use \#\rownumber\label{exp:k02robust}, \#\rownumber\label{exp:k15robust},
and \#\rownumber\label{exp:covtyperobust} to show that even with more
evaluations, FMM can be faster than HSS for the same accuracy. 
For HSS the relative error in \#\ref{exp:k02robust} (blue plots) plateaus at
\num{5E-4}. Further increasing rank from 256 to 512 (or even 1,024) results
in $O(s^3)$ work (green bars). Using a combination of low-rank
($s64$) and $3\%$ direct evaluation, FMM can achieve higher accuracy
with little increment in the evaluation time (compression time remains
the same).  Similarly, in \#\ref{exp:covtyperobust} we can observe that
by using $s512$ and $3\%$ budget we achieve better accuracy than the
HSS approximation ($s2048$) in less time.

% \begin{table}[!t]
% \setlength\tabcolsep{4.8pt}
% \centering
% {
%   \begin{tabular}{|r|>{\columncolor[gray]{0.8}}r|rrr|rrr|} 
%   \hline
%   & Setup & \multicolumn{3}{c|}{$\tau=1E-2$, budget $1\%$} &
%   \multicolumn{3}{c|}{$\tau=1E-5$, budget $3\%$} \\
%   \hline
%   \rowcolor[gray]{0.8}
%   \# & $K\#$ & $\epsilon_2$ & Comp & Eval & $\epsilon_2$ & Comp & Eval  \\
%   \hline
%   & K02 & 2E-2 & 0.8 & 0.1 & 5E-5 & 0.9 & 0.2 \\
%   & K03 & 2E-9 & 0.7 & 0.1 & 2E-9 & 0.9 & 0.2 \\
%   & K04 & 2E-2 & 0.8 & 0.1 & 6E-5 & 1.0 & 0.2 \\
%   & K05 & 5E-2 & 0.8 & 0.1 & 9E-5 & 1.0 & 0.2 \\
%   & K06 & 1E-1 & 8.1 & 0.3 & 1E-1 & 8.5 & 0.4 \\
%   & K07 & 2E-1 & 0.8 & 0.1 & 1E-4 & 0.9 & 0.2 \\
%   & K08 & 5E-2 & 0.8 & 0.1 & 4E-5 & 1.1 & 0.2 \\
%   & K09 & 3E-2 & 0.8 & 0.1 & 4E-5 & 1.1 & 0.2 \\
%   & K10 & 8E-2 & 0.8 & 0.1 & 2E-6 & 0.9 & 0.2 \\
%   & K11 & 3E-3 & 0.8 & 0.1 & 4E-4 & 0.9 & 0.2 \\
%   & K12 & 6E-2 & 0.8 & 0.1 & 5E-5 & 0.9 & 0.2 \\
%   & K13 & 9E-1 & 0.8 & 0.1 & 4E-1 & 1.0 & 0.2 \\
%   & K14 & 1E+1 & 0.8 & 0.1 & 3E-1 & 0.9 & 0.2 \\
%   & K15 & 8E-1 & 2.7 & 0.2 & 6E-1 & 4.1 & 0.4 \\
%   & K16 & 5E-1 & 11.3 & 0.4 & 4E-1 & 11.7 & 0.5 \\
%   \hline
%   & K17 & 9E-1 & 349 & 9.9 & 9E-1 & 348 & 10.5 \\
%   & K17 & 9E-1 & 3.4 & 0.2 & 9E-1 & 3.5 &  0.2 \\
%   & K18 & 4E-2 & 0.4 & $<0.1$ & 2E-5 &  0.6 & 0.1  \\
%   \hline
%   & K19 & 1E-5 & 0.2 & $<0.1$ & 5E-6 & 0.3 & $<0.1$ \\
%   & K20 & 4E-6 & 0.7 & 0.1 & 4E-6 & 0.9 & 0.1 \\
%   & K21 & 6E-5 & 1.1 & 0.1 & 6E-5 & 1.1 & 0.1 \\
%   \hline
%   & G01 & 1E-2 & 0.3 & $<0.1$ & 2E-3 & 0.4 & $<0.1$ \\
%   & G02 & 2E-2 & 0.2 & $<0.1$ & 9E-3 & 0.3 & $<0.1$ \\
%   & G03 & 8E-3 & 1.1 & 0.2 & 4E-4 & 3.0 & 0.2 \\
%   & G04 & 4E-3 & 1.4 & 0.3 & 4E-6 & 1.9 & 0.3 \\
%   & G05 & 9E-4 & 0.8 & 0.1 & 4E-5 & 1.4 & 0.2 \\
%   \hline
%   & Setup & \multicolumn{3}{c|}{$\tau=1E-8$, budget $3\%$} &
%   \multicolumn{3}{c|}{$\tau=1E-10$, budget $3\%$} \\
%   \hline
%   \rowcolor[gray]{0.8}
%   \# & $K\#$ & $\epsilon_2$ & Comp & Eval & $\epsilon_2$ & Comp & Eval  \\
%   \hline
%   & K11 &      &     &     &      &     &     \\
%   & K13 & 5E-4 & 1.0 & 0.2 & 1E-5 & 1.3 & 0.2 \\
%   & K14 & 6E-4 & 1.0 & 0.2 & 8E-6 & 0.3 & 0.2 \\
%   \hline
%   \rowcolor[gray]{0.8}
%   & Setup & \multicolumn{3}{c|}{$m=64$, budget $3\%$} &
%   \multicolumn{3}{c|}{$m=128$, budget $3\%$} \\
%   \hline
%   & K17 & 5E-1 & 3.9 & 0.6 & 6E-1 & 3.5 & 0.4  \\
%   & G01 & 1E-5 & 1.6 & 0.2  & 7E-5  & 1.5 & 0.1  \\
%   & G02 & 2E-6 & 1.0 & 0.1  & 2E-3 & 0.8  & 0.1 \\
%   & G03 & 2E-5 & 8.2 & 0.8  & 3E-5 & 8.3  & 0.8  \\
%   \hline
%   \end{tabular}
% }
% \caption{ }
% \end{table}


\begin{figure}[!t]
  \centering
  \includegraphics[scale=.23]{figures/partition_screen.pdf}
  \caption{
    Accuracy (left y-axis) and rank (right, x-axis) comparison:
    \textbf{Lexicographic}, \textbf{Random}, \textbf{Kernel 2-norm},
    \textbf{Angle} and \textbf{Geometric}.
    We use $\tau\num{1E-7}$, $s512$, $m64$. For methods that define 
    \emph{distance}, we use $k32$ and $3\%$ budget.
    \textbf{G03} is a graph Laplacian; thus, using
    \textbf{Geometric} distance is impossible.
  }
  \label{fig:partition}
\end{figure}

\textbf{Permutations (\figref{fig:partition}).}
Here we test different permutations
(\#\rownumber\label{exp:k02partition},
\#\rownumber\label{exp:k09partition},
\#\rownumber\label{exp:covtypepartition}, and
\#\rownumber\label{exp:g03partition}) 
to discuss the different distances in \gofmm{}.
In each set of experiments, we present relative
error (blue plots) and average rank (green bars)
for five different schemes.
The first two schemes use lexicographic or random order
to recursively permute $K$. 
Since there is no \emph{distance} defined, 
these two schemes can only use HSS approximation.
The \textbf{Angle} and \textbf{Kernel}
\emph{distance} use the corresponding Gram distances~\secref{s:oblivious}.
Finally, we also use standard geometric distance from points. 
For the last three schemes, we use $\kappa32$ and $3\%$
budget. Overall, we can observe that the distance metric
is important in discovering low-rank structure
and improving accuracy.
For example, in \#\ref{exp:k02partition}, \textbf{Kernel} and 
\textbf{Geometric} show much lower average rank
than others.
In \#\ref{exp:k09partition} and \#\ref{exp:covtypepartition},
although the average ranks are not significantly
different, distance-based methods usually have higher accuracy. 
Finally, we observe for matrix \textbf{G03} in \#\ref{exp:g03partition} where no coordinate information exists,
our geometry-oblivious methods can still compress
the matrix. Although the lexicographic permutation has
very low rank, the error is large. This is because
the uniform samples for the low-rank approximation are poor. 
\textbf{Angle} and \textbf{Kernel} distance use neighbors for importantance sampling, 
which greatly improves the quality of the low-rank approximation.


\begin{table}
\input{tables/otherSoftware2.tex}
\caption{
  Wall-clock time comparison (in seconds) between \texttt{HODLR},
  \texttt{STRUMPACK}, and \texttt{GOFMM}.
  For \textbf{K02}--\textbf{K12}, we use $N=36K$. \textbf{K17} uses $N=32K$,
  and  \textbf{G03} uses $N=65K$.
  For all software, we use leaf node size $m512$ and $1024$ right hand sides.
  We control other parameters ($\tau$ and $s$) for each software to target 
  the same relative error (\num{1E-4}).
%  
%Internal parameters reasonably adapted such that target error 
%accuracy of around $1E-4$ is reached. Results in single precision computed on a Intel Haswell 
%node with 24 cores, 2.6 GHz, 64 GB DDR4-2133.
}
\label{tab:existingSoftware}
\end{table}

\begin{table}
\input{tables/askit_comparison.tex}
\caption{
  Wall-clock time (in seconds) and accuracy $\epsilon_2$ comparison with \texttt{ASKIT}.
  For both methods, we use $\kappa=32$, $m = s = 512$ and $r1$.
  \texttt{ASKIT} use the $\tau$ reported in the table, and we adjust
  the tolerance of \texttt{GOFMM} to match the accuracy.
  For all experiments, \texttt{GOFMM} uses $7\%$ budget.
  The amount of direct evaluation performed by \texttt{ASKIT} is decided
  by $\kappa$.
}
\label{tab:askitcomparison}
\end{table}



\textbf{Comparison to existing software (\tabref{tab:existingSoftware}, \tabref{tab:askitcomparison}).}
We compare our methods to 
\texttt{HODLR}~\cite{ambikasaran-darve13}, 
\texttt{STRUMPACK}~\cite{rouet-li-e16}, and 
\texttt{ASKIT}~\cite{march-xiao-yu-biros-sisc16}.
Let us summarize some key differences. \texttt{HODLR} uses the
Adaptive Cross Approximation (ACA, partial pivoted LU) for constructing the low-rank blocks (using the \texttt{Eigen} library). Its evaluation requires $O(N\log{N})$ work since the $U$, $V$ matrices are not nested. 
\texttt{STRUMPACK} constructs an HSS representation
in $O(N\log{N})$ work.
This is done by using a randomized ID according to \cite{liberty2007randomized}. 
We used their black-box compression routine with a uniform random distribution 
and a Householder rank-revealing QR.
Once the matrix is compressed, the evaluation time is $O(N)$
per right hand side. \texttt{STRUMPACK} supports multiple right hand sides.
\texttt{ASKIT}'s FMM evaluation has similar complexity as \texttt{GOFMM},
but the amount of direct evaluation is only decided by $\kappa$.
For \texttt{GOFMM}, we further introduce the budget to restrict the
cost. For all comparisons, we try to match the accuracy by controlling
different parameters ($\tau$, $s$, and $\kappa$). Notice
that \texttt{ASKIT} and \texttt{STRUMPACK} support MPI,
whereas \gofmm{} does not. We have not used MPI for distributed 
environment in our experiments.


In \tabref{tab:existingSoftware}, we target final accuracy $\epsilon_2 =
\num{1E-4}$. 
\texttt{GOFMM} uses \textbf{Angle} distance for neighbor search and
tree partitioning. \texttt{HODLR} and \texttt{STRUMPACK} do not have 
built-in partitioning schemes for dense matrices. 
\texttt{STRUMPACK} fails to compress \textbf{K04} (Gaussian kernel in 6D) and
\textbf{K07} (Laplace kernel in 6D). The first two ones, this is because
the lexicographic order does not admit a good \hmatrix{} approximation. The matrix needs to be permuted.
\textbf{K17} is difficult to compress with a pure hierarchical low-rank
matrix. Finally, \textbf{G03} performs better when $S\neq 0$. 
\texttt{HODLR} and \texttt{STRUMPACK} must increase the off-diagonal 
ranks to match the accuracy and thus the cost increases. \texttt{GOFMM} just uses $S$ and it is about
$25\times$ faster in compression and about $1.5\times$ faster in evaluation.

%JAMES add a section to discuss ASKIT
In \tabref{tab:askitcomparison}, we compare \gofmm{} (with geometric distances) to \texttt{ASKIT}. 
\texttt{ASKIT} uses level-by-level traversals in both compression and evaluation.
%We fix the desired accuracy and adjust the tolerance $\tau$ of \texttt{GOFMM} to match the same accuracy.
Since \texttt{ASKIT} only evaluates a single right hand side,
we use $r=1$. The compression time is inconclusive for
\#\ref{exp:k04askit1}--\#\ref{exp:k04askit4}; the average ranks
used in two methods are quite different. 
The benefit of \emph{out-of-order} traversal appears in
\#\ref{exp:k06askit1}--\#\ref{exp:k06askit4} where both methods reach 
the maximum rank $s$. The speedup in evaluation is not significant. 
\texttt{GOFMM} can get up to $2\times$ speedup in compression. 

\begin{table}[!t]
\setlength\tabcolsep{4.8pt}
\centering
{
  \begin{tabular}{|r|>{\columncolor[gray]{0.8}}r|rr|rr|rr|} 
  \hline
  \rowcolor[gray]{0.8}
  \# & Arch & Budget & $\epsilon_2$ & Comp & GFs & Eval & GFs \\
  \hline
  \rowcolor[gray]{0.8}
  \multicolumn{8}{|c|}{\textbf{MNIST60K}, $h1$, $\kappa32$, $m512$, $s128$, $r256$} \\
  \hline
  \rownumber\label{exp:mnistarm}   & ARM & $5\%$ & 5E-3 & 285 &  3 & 520 & 12 \\
  \hline
  \rowcolor[gray]{0.8}
  \multicolumn{8}{|c|}{\textbf{COVTYPE100K}, $h1$, $\kappa32$, $m512$, $s128$, $r256$} \\
  \hline
  \rownumber\label{exp:covtypearm}   & ARM & $5\%$  & 8E-4 & 71 &  2 &  61 & 10 \\
  \hline
  \rowcolor[gray]{0.8}
  \multicolumn{8}{|c|}{\textbf{COVTYPE100K}, $h0.1$, $\kappa32$,  $m800$, $s512$,
  $r512$} \\
  \hline
  \rownumber\label{exp:covtypecpu} & CPU & $12\%$ & 2E-3 & 30 & 30 & 4.1 &  679 \\
  \rownumber\label{exp:covtypegpu}   & CPU$+$GPU & $12\%$ & 3E-3 & 33 & 29 & 1.7 & 1952  \\
  \rownumber\label{exp:covtypemic}   & KNL & $12\%$ & 2E-3 & 48 & 25 & 3.2 & 1125  \\
  \hline
  \rowcolor[gray]{0.8}
  \multicolumn{8}{|c|}{\textbf{HIGGS500K}, $h0.9$, $\kappa64$, $m1024$, $s256$, $r512$} \\
  \hline
  \rownumber\label{exp:higgscpu} & CPU & $0.3\%$  & 2E-1 & 102 & 18 & 3.3 & 592 \\
  \rownumber\label{exp:higgsgpu}   & CPU$+$GPU & $0.3\%$ & 2E-1 & 180 & 12 & 1.7 & 1147  \\
  \rownumber\label{exp:higgsknl}   & KNL & $0.3\%$ & 2E-1 & 121 & 17 & 2.2 & 872  \\
  \hline
  \rowcolor[gray]{0.8}
  \multicolumn{8}{|c|}{\textbf{K02}, $N65536$, $\kappa32$, $m512$, $s512$, $r512$} \\
  \hline
  \rownumber\label{exp:k02cpu} & CPU & $3\%$  & 9E-5 &      1 & 25 & 0.2 &  889 \\
  \rownumber\label{exp:k02gpu} & CPU$+$GPU & $3\%$ & 1E-4 & 2 & 12 & 0.1 & 2175 \\
  \rownumber\label{exp:k02mic} & KNL & $3\%$ & 1E-4        & 3 & 11 & 0.3 & 530  \\
  \hline
  \rowcolor[gray]{0.8}
  \multicolumn{8}{|c|}{\textbf{K15}, $N65536$, $\kappa32$, $m512$, $s512$, $r1024$} \\
  \hline
  \rownumber\label{exp:k15cpu} & CPU & $10\%$  & 2E-1 &  6.0  & 81 & 1.1  &
  1495  \\
  \rownumber\label{exp:k15gpu} & CPU$+$GPU & $10\%$ & 2E-1  & 7.8  & 62  &
  0.66 &
  2514 \\
  \rownumber\label{exp:k15knl} & KNL & $10\%$ & 2E-1        & 9.2 & 53 & 1.3 & 1549\\
  \hline
  \rowcolor[gray]{0.8}
  \multicolumn{8}{|c|}{\textbf{G03}, $N65536$, $\kappa32$, $m128$, $s512$, $r512$} \\
  \hline
  \rownumber\label{exp:g03cpu} & CPU & $3\%$       & 4E-5 &  4.8  & 37 & 0.5 &
  1122  \\
  \rownumber\label{exp:g03gpu} & CPU$+$GPU & $3\%$ & 3E-5 &  7.9  & 19  & 0.53 & 962 \\
  \rownumber\label{exp:g03knl} & KNL & $3\%$       & 5E-5        & 11.8 & 9.1 &
  0.6 & 741  \\
  \hline
  \rowcolor[gray]{0.8}
  \multicolumn{8}{|c|}{\textbf{G04}, $N89400$, $\kappa32$, $m512$, $s512$, $r512$} \\
  \hline
  \rownumber\label{exp:g04cpu} & CPU & $3\%$       & 4E-6 &  1.8  & 21 & 0.3  &
  787\\
  \rownumber\label{exp:g04gpu} & CPU$+$GPU & $3\%$ & 4E-6 & 4.0  & 10  & 0.13 & 2277 \\
  \rownumber\label{exp:g04knl} & KNL & $3\%$       & 4E-6 & 4.2 & 9 & 1.5 &
  215  \\
  \hline
  \end{tabular}
}
\caption{
  Accuracy $\epsilon_2$, wall-clock time (in seconds) and efficiency (in \texttt{GFLOPS})
  on four architectures. Because our ARM platform only has a 8GB SD card and
  2GB DRAM, we only perform kernel matrices ($K_{ij}$ computed on the fly) 
  with small $r$ and $s$. Note that in the CPU$+$GPU experiment, the compression is run on the CPU (see ~\secref{s:par}).
}
\label{tab:arch}
\end{table}


\textbf{Different architectures.}
In \tabref{tab:arch}, we present wall-clock time and \texttt{GFLOPS}
of \gofmm{} on four architectures for different problems.
We want to show that the efficiency of \texttt{GOFMM} is
portable and only relies on BLAS/LAPACK libraries.  

In \#\ref{exp:mnistarm} and \#\ref{exp:covtypearm},
we show that a quad-core ARM processor can handle
up to 100K fast matrix-multiplication. Because we only have limited 
memory (2GB) and storage (8GB), in \texttt{GOFMM} we
compute $K_{ij}$ on the fly (in detail, we compute
$K_{\beta\alpha}$ with a \texttt{GEMM} using the 2-norm expansion).
\#\ref{exp:mnistarm} takes much longer than \#\ref{exp:covtypearm}
because the cost of evaluating $K_{ij}$ is proportional to the point dimensions
of the dataset (\textbf{MNIST} in 780D and \texttt{COVTYPE} in 54D).
Because there is no active cooling, the ARM gets overheated and is
forced to reduce its clockrate. That is why we can only reach $30\%$ of peak
during the evaluation. 

Experiments \#\ref{exp:covtypecpu} to \#\ref{exp:higgsknl} are computed
in double precision. % (coordinates are stored in double).
With $12\%$ budget,
our evaluation can reach $68\%$ peak performance on Haswell, $37\%$ on KNL and $38\%$ 
on a hybrid Haswell-P100 system.
The performance degrades in \#\ref{exp:higgscpu}--\ref{exp:higgsknl} because
the rank is limited to 256, and $0.3\%$ direct evaluation 
is not enough to create large \texttt{GEMM} calls.
For kernel matrices, the \texttt{GFLOPS} for compression are usually
higher because computing $K_{ij}$ requires floating point operations.
For example, compression of \textbf{COVTYPE} (in 54D) has higher 
\texttt{GFLOPS} than \textbf{HIGGS} (in 28D).
This is not only because \textbf{COVTYPE} is a dataset with high 
dimensionality, but we also use a higher rank $s512$ such that 
\texttt{GEQP3} and \texttt{TRSM} can be more efficient.


Finally, we present performance results on several matrices 
(\#\ref{exp:k02cpu}--\ref{exp:g04knl}) in single precision.
With $10\%$ budget in \textbf{K15},
our evaluation can reach $75\%$ peak on Haswell, $25\%$ on KNL and $25\%$ 
on a hybrid Haswell-P100 system.
This performance requires large leaf node size $m$ and sufficient
direct evaluations (e.g. \#\ref{exp:k02cpu}--\#\ref{exp:g04knl}).
Since \textbf{G03} requires small $m$, our \texttt{GFLOPS} efficiency
degrades due to the dependency on the BLAS/LAPACK routines. 
Notice that $m128$ is not large enough for \texttt{GEMM}
to reach high performance on KNL and GPUs.
For \textbf{G04}, we use $m512$ but KNL (\#\ref{exp:g04knl}) does 
not perform very well. 
The same problem occurs in \figref{fig:scaling}: 
the average rank in \textbf{G04} is too small. 
Additionally, we do not observe huge performance degradation on GPUs
(\#\ref{exp:g04gpu}). 
This is because we enforce our scheduler to schedule \texttt{L2L} tasks
to the GPU; thus, tasks with small ranks (\texttt{N2S} and
\texttt{S2N}) are mostly consumed by the host CPU.
The comparison between \#\ref{exp:g04gpu} and \#\ref{exp:g04knl} 
is a good example that highlights the goal of heterogeneous parallel
architectures. CPUs with short vector lengths are suitable for 
tasks with very low ranks  (\texttt{N2S} and \texttt{S2N}). 
On the contrary, GPUs are the method of choice for \texttt{FLOPS} intensive tasks (\texttt{L2L}).
We cannot solve such problems with only one architecture  efficiently.


