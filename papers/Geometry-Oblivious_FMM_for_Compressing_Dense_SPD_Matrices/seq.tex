% To transform the high level discussion of near-far pruning into algorithms, 
% we separate the whole
% method into two phases: (1) an one-time \textbf{compression}
% (\algref{a:compress}),
% and (2) an \textbf{evaluation} (\algref{a:evaluate}) for each $w$ instance.
% These algorithms are described in terms of binary tree traversals: 
% postorder \textbf{POST}, preorder \textbf{PRE},
% any order \textbf{ANY} and \textbf{LEAF} (only traverse leaf nodes).
% The computation that occurs in each tree
% node ($\alpha$ or $\beta$) is a task listed in 
% \tabref{tab:tasks}.
% 
% \begin{algorithm}[!t]
% \caption{{} \texttt{Compress}($K$)}
% \begin{algorithmic}
%   \STATE \texttt{\bf for each} randomize projection tree
%   \STATE \gap \texttt{\bf PRE} \texttt{SPLI($\alpha$)} \# random projection tree 
%   \STATE \gap \texttt{\bf LEAF} \texttt{ANN($\alpha$)} \#\textbf{RAW} on \texttt{SPLI}
%   \STATE \texttt{\bf PRE} \texttt{SPLI($\alpha$)} \# metric ball tree 
%   \STATE \texttt{NearNear()} \#\textbf{RAW} on \#\texttt{SPLI} 
%   \STATE \texttt{\bf ANY} \texttt{Kba($\alpha$)} \#\textbf{RAW} on \texttt{NearNear}
%   \STATE \texttt{\bf LEAF} \texttt{NearFar($\beta$,root)} \#\textbf{RAW} on \texttt{NearNear}
%   \STATE \texttt{\bf POST} \texttt{FarFar($\alpha$)} \#\textbf{RAW} on \texttt{NearFar}
%   \STATE \texttt{\bf POST} \texttt{SKEL($\alpha$)} \#\textbf{RAW} on \texttt{SPLI}
%   \STATE \texttt{\bf ANY} \texttt{COEF($\alpha$)} \#\textbf{RAW} on \texttt{SKEL}
%   \STATE \texttt{\bf ANY} \texttt{SKba($\alpha$)} \#\textbf{RAW} \texttt{FarFar}
%   and \texttt{SKEL}
% \end{algorithmic}
% \label{a:compress}
% \end{algorithm}
% 
% 
% \begin{table}[!t]
% \centering
% {
% \begin{tabular}{|rlr|}
% \hline 
% Task & Operations & \texttt{FLOPS} \\
% \hline 
% \texttt{SPLI($\alpha$)} & split $\alpha$ into $\lc$ and $\rc$ \algref{a:split} & $\lvert \alpha \rvert$ \\
% \hline 
% \texttt{ANN($\alpha$)} & update $\MA{N}_{\alpha}$ with \texttt{KNN($K_{\alpha\alpha}$)} & $dm^2$ \\ 
% \hline 
% \texttt{SKEL($\alpha$)} & $\sk{\alpha}$ in \algref{a:skeletonize} & $2s^3+2m^3$ \\ 
% \hline 
% \texttt{COEF($\alpha$)} & $P_{\sk{\alpha} \alpha}$ and $P_{\sk{\alpha} [\sk{\lc} \sk{\rc}]}$ in \algref{a:skeletonize} & $s^3$ \\ 
% \hline 
% \texttt{N2S($\alpha$)} & \texttt{\bf if} $\alpha$ is leaf \texttt{\bf then} $\sk{w_{\alpha}} = P_{\sk{\alpha}\alpha}w_{\alpha}$ & $2msr$ \\
%              & \texttt{\bf else} $\sk{w_{\alpha}} =
% P_{\sk{\alpha}[\sk{\lc}\sk{\rc}]}[ \sk{w_{\lc}}; \sk{w_{\rc}}]$ & $2s^2r$\\
% \hline
% \texttt{SKba($\beta$)}  & $\forall\alpha\in Far(\beta)$, $K_{\sk{\beta}\sk{\alpha}} = K(\sk{\beta}, \sk{\alpha})$ & $ds^2\lvert Far(\beta) \rvert$ \\ 
% \hline 
% \texttt{S2S($\beta$)} & $\sk{u_{\beta}} =
%   \sum_{\alpha\in Far(\beta)} K_{\sk{\beta}\sk{\alpha}}\sk{w_{\alpha}}$ & $2s^2r\lvert Far(\beta) \rvert$\\ 
% \hline 
% \texttt{S2N($\beta$)} & \texttt{\bf if} $\alpha$ is leaf \texttt{\bf then} $u_{\beta} = P_{\sk{\beta}\beta}^{T}\sk{u_{\beta}}$ & $2msr$ \\ 
%                       & \texttt{\bf else} $[\sk{u_{\lc}};\sk{u_{\rc}}] += P_{\sk{\beta}[\sk{\lc}\sk{\rc}]}^{T}\sk{u_{\beta}}$ & $2s^2r$ \\
% \hline
% \texttt{Kba($\beta$)}  & $\forall\alpha\in Near(\beta)$, $K_{\beta\alpha} = K(\beta, \alpha)$ & $dm^2\lvert Near(\beta) \rvert$ \\ 
% \hline  
% \texttt{L2L($\beta$)} & $u_{\beta} += \sum_{\alpha\in Near(\beta)}K_{\beta\alpha}w_{\alpha}$ & $2m^2r\lvert Near(\beta) \rvert$\\ 
% \hline 
% \end{tabular}
% }
% \caption{Tasks and their costs in \texttt{FLOPS}.
%   \texttt{SPLI} (tree splitting), \texttt{ANN} (all nearest-neighbor), \texttt{SKEL} (skeletonization), 
%   \texttt{COEF} (interpolation) \texttt{SKba} and 
%   \texttt{Kba} (caching submatrices) appear in the setup phase.
%   Interactions \texttt{N2S} (nodes to skeletons), \texttt{S2S} (skeletons to skeletons)
%   , \texttt{S2N} (skeletons to nodes) and \texttt{L2L} (leaves to leaves) appear in 
% the evaluation phase.}
% \label{tab:tasks}
% \end{table} 
% 
% 
% \textbf{Compression.}
% In this paper we apply a black-box geometry-oblivious heuristic
% to define \emph{distance} between pairs of rows (and columns).
% This \emph{distance} allows us to define neighbors $\MA{N}$ for
% each index, and partition $K$ with a metric ball tree~\cite{}.
% Given the capability of accessing any $K_{ij}$ element, 
% we first explain how such \emph{distance} is defined by only using
% raw elements from $K$; then we present the compression algorithm,
% which follows~\cite{} but in the flavor of \emph{out-of-order} using
% task scheduling.
% 
% 
% %\begin{enumerate}
% %\item hierarchical binary partitioning of the input matrix
% %\item a {\it distance} metric for neighbor sampling
% %\end{enumerate}
% %\input{hierarchical.tex}
% 
% 
% 
% Any symmetric positive-definite (SPD) matrix can be described as 
% a \emph{Gramian matrix} of some set of vectors $\{\phi_i\}$.
% Given a matrix $K$ (e.g. from a kernel function $K_{ij}=\MA{K}(x_i,x_j)$,
% where $x_{i} \in \mathbb{R}^{d}$) being symmetric 
% positive definite, we can assume that there exists a set of vectors 
% $\phi_i \in \mathcal{V}$ which define this matrix by an inner product,  
% $K_{ij}=\mathcal{K} (x_i,x_j)=\phi_i^{T}\phi_j$.
% Hence, we can define \emph{distance} $d_{ij}$ between $\phi_i$ and $\phi_j$
% in the Gramian space. We use two metrics:
% \begin{itemize}
% \item the $L^2$ distance $\|\phi_i-\phi_j\|_2^2=K_{ii}+K_{jj}-2K_{ij}$, or
% \item the $\sin{}$ similarity $1- \K_{ij}^2 / (K_{ii}K_{jj}) $.
% \end{itemize}
% These \emph{distances} define hierarchical subdivisions.
% 
% \begin{algorithm}[!t]
%   \caption{{} $[\lc,\rc]=\texttt{metricSplit}(\alpha)$}
% \begin{algorithmic}
%   \STATE $p = argmax( \{\sum_{j\in\texttt{c}}d_{ij} / \lvert \texttt{c} \rvert
%   \lvert i \in \alpha \})$
%   \STATE $q = argmax( \{ d_{ip} \lvert i \in \alpha \})$
%   \STATE $[\lc,\rc]=medianSplit(\{ d_{ip} - d_{iq} \lvert i \in \alpha\})$
% \end{algorithmic}
% \label{a:split}
% \end{algorithm}
% 
% 
% Starting from all indicies (\texttt{root}), we use a metric ball 
% tree to recursively partition indicies into two
% children $\lc$ and $\rc$. 
% This task is described as \texttt{SPLI($\alpha$)}
% in \tabref{tab:tasks} using \algref{a:split}. 
% Let's define an approximate centroid by $\hat{\phi_\alpha}= \frac{1}{\lvert \texttt{c}\rvert}\sum_{i\in\texttt{c}}\phi_{i}$,
% where $\texttt{c} \subset \alpha$ contains a constant number of samples, in order to avoid accessing $\MA{O}(N)$ entries.
% With this approximate centroid, we first find $p$ as the most far away
% index from the centroid in $\MA{O}(\lvert c \rvert\lvert\alpha\rvert)$, and we find $q$
% corresponding to $p$ in $\MA{O}(\lvert\alpha\rvert)$.
% 
% Next, we compute a \emph{distance} for each index $i$ towards $p$ and $q$, i.e. $d_{ip} - d_{iq}$. 
% Then we perform a median split on this metric $d_{ip} - d_{iq}$.
% This results in an even split, and $p$, $q$ will be in a different child.
% \texttt{SPLI($\alpha$)} stops when $\lvert \alpha \rvert \leq m$, a user defined leaf node size.
% %To avoid $\MA{O}(N^2)$ search on $d_{pq}$, an approximate centroid is defined as $\sum_{i\in\texttt{c}}\phi_{i} / \lvert \texttt{c} \rvert$, where $\texttt{c} \subset \alpha$ contains constant number of samples.
% 
% 
% The neighbors $\MA{N}$ are defined using the same metric $d_{ij}$.
% We employ an iterative neighbor search~\cite{} in \algref{a:compress} 
% that creates a binary 
% tree using randomized projection tree. That is, instead of searching for $p$ and
% $q$ in \algref{a:split}, we randomly select $p$ and $q$ from $\alpha$.
% In each iteration, exact neighbor search
% is only performed in the leaf level as \texttt{ANN($\alpha$)}
% shown in \tabref{tab:tasks}. 
% In this case, while searching for $k$ neighbors, we can restrict the
% candidate numbers to $\lvert \alpha \rvert$ (usually $2k$).
% Due to the randomness, these $2k$ candidates will be different 
% each time, and $\MA{N}$ may graduatelly converge\footnote{
% \scriptsize The convergence speed is affected by different factors; typically this is related to the intrinsic dimension of $K$.}.
% 
% 
% \textbf{Discussion.}
% Notice that in traditional FMM the \emph{distance} metric measures the 
% \emph{difficulty} of approximating the element. 
% 
% %This tree-like partition permutes $K$ into a hierarchical matrix whose 
% %off-diagonal blocks are potentially low-rank.
% %%We first define distance metrics for each column and row based on the symmetric factorization of $K$.
% %
% %For doing so, we consider a direction spanned by the two farthest 
% %points $\phi_\alpha$ and $\phi_\beta$ and split indices $i$ binary by
% %\begin{itemize}
% %\item[a.] a $L^2$ based splitting $<\phi_i,\phi_\alpha-\phi_\beta>$
% %\item[b.] a $d-1$-dimensional two-fold cone spanned in direction 
% %  $\phi_\alpha-\phi_\beta$, and its opposite direction 
% %  $\phi_\beta-\phi_\alpha$, 
% %  i.e. \\ $\frac{1}{\norm{\phi_i}}\lvert<\phi_i,\phi_\alpha-\phi_\beta>\rvert$
% %\end{itemize}
% 
% 
% 
% 
% %This tree-like partition permutes $K$ into a hierarchical matrix whose off-diagonal blocks are potentially low-rank.
% %We first define distance metrics for each column and row based on the symmetric factorization of $K$.
% %We then use these metrics to find neighbors and define the binary tree. 
% 
% %
% % define distance metrics here
% %
% 
% % section is reworded. please comment if unhappy
% %We then use these metrics to find neighbors; neighbors are computed 
% %with randomized projection tree based
% %algorithms. For each iteration, we select a random direction,
% %and split a tree node $\alpha$
% %into two children $\lc$ and $\rc$
% %\begin{itemize}
% %\item[a.] by a fictive orthogonal hyper-plane defined by the median
% % \item[b.] a fictive hyper-cone defined by a (roughly) equal-sized split 
% %\end{itemize}
% %An exhaustive neighbor search \texttt{ANN} is performed in each leaf node
% %$\alpha$ to update its neighbor list $\MA{N}_{\alpha}$.
% %When a pre-computed neighbor list is provided, this step is skipped.
% 
% % obsolete
% %The permutation for $K$ is computed with a ball tree. Given the distance metric, the two most far away points are select to form the projection direction. Again, a tree node $\alpha$ is split with the corresponding orthogonal hyper-plane. With this tree-based permutation, we can now compute the low-rank approximations for the off-diagonal blocks.
% 
% 
% \textbf{Low-rank approximations.}
% We now explain how skeleton $\sk{\alpha}$ is computed by the nested Interpolative Decomposition (ID) 
% in this work.
% Let $\alpha$ be the points in a tree node, and $S=\{1,...,N\}\backslash \alpha$
% be the set complement. The skeletonization of $\alpha$ is a rank-$s$
% approximation of its off-diagonal blocks $K_{S\alpha}$ using ID, 
% written as
% \begin{equation}
% K_{S\alpha} \approx K_{S\sk{\alpha}}P_{\sk{\alpha}\alpha}.
% \end{equation}
% Here $\sk{\alpha} \subset \alpha$ is the \emph{skeleton} of $\alpha$.
% $K_{S\sk{\alpha}}$ contains $s$ columns basis, and $P_{\sk{\alpha}\alpha} \in
% \mathbb{R}^{s\times \lvert \alpha \rvert}$ contains all interpolation
% coefficients.
% We use the first $s$ pivots of a rank-revealing QR factorization (\texttt{GEQP3}) 
% on $K_{S\sk{\alpha}}$ to select $\alpha$.
% With QR factorization of $K_{S\sk{\alpha}}$, we can compute
% $P_{\sk{\alpha}\alpha} = K_{S\sk{\alpha}}^{\dagger}K_{S\alpha}$ 
% with a triangular solver (\texttt{TRSM}).
% This scheme however results in $\bigO(d N^2\ppl)$ complexity for the
% overall factorization. We can turn it to a $\bigO(d \log N \ppl)$
% scheme by sampling a small subset $S'$ of $S$ and using it instead of
% $S$. With $\MA{N}(\alpha)$, we can perform importance sampling as shown 
% in~\cite{march-xiao-biros-e15}.
% The approximation rank $\ns$ is chosen such that
% $\sigma_{\ns+1}(K_{S' \alpha}) < \idtol$,
% where $\idtol$ is user-specified and $\sigma$ are the singular
% values estimated by the diagonal of the rank-revealing QR.
% 
% \begin{algorithm}[!t]
% \caption{{} [$\sk{\alpha}, P_{\sk{\alpha} \alpha}$]=\texttt{Skeleton}($\alpha$)}
% \begin{algorithmic}
%   \STATE \texttt{\bf if} $\alpha$ is leaf \texttt{\bf then return} [$\sk{\alpha}, P_{\sk{\alpha} \alpha}$] = {\tt ID}$(\alpha)$;
%   \STATE $[\sk{\lc},]=\texttt{Skeleton}(\lc)$; $[\sk{\rc},]=\texttt{Skeleton}(\rc)$;
%   \STATE \texttt{\bf return} [$\sk{\alpha}, P_{\sk{\alpha} [\sk{\lc} \sk{\rc}]
%   }$] = {\tt ID}($[\sk{\lc} \sk{\rc}]$);
% \end{algorithmic}
% \label{a:skeletonize}
% \end{algorithm}
% 
% \algref{a:skeletonize} computes skeletonization for all tree nodes with a 
% postorder traversal. As we have discussed in \secref{s:hier}, 
% for a non-leaf node $\alpha$, we use nested basis from children.
% Thus, instead of selecting 
% $\sk{\alpha}$ from $\alpha$, a greedy algorithm is used to only sub-select
% from children's skeleton $\sk{\lc}\cup\sk{\rc}$. Subsequently, 
% %instead of creating the whole coefficient matrix
% %$P_{\sk{\alpha}\alpha}$, we only compute the interpolation coefficients
% %$P_{\sk{\alpha}[\sk{\lc}\sk{\rc}]}$. 
% we have the \emph{telescoping} relationship shown in \eqref{e:telescoping}.
% Overall, \algref{a:skeletonize} has
% two tasks for each tree node $\alpha$ shown in \tabref{tab:tasks}: 
% (1) \texttt{SKEL($\alpha$)} selects $\sk{\alpha}$ (critical path) and
% (2) \texttt{COEF($\alpha$)} computes $P_{\sk{\alpha}[\sk{\lc}\sk{\rc}]}$ 
% (depending on $\sk{\alpha}$).
% Since the parallelism diminishes in \algref{a:skeletonize} during the 
% traversal, a proper schedule is required to improve the efficiency. 
% 
% \textbf{Evaluation.}
% While partial matrix-multiplication $u_{\beta} = K_{\beta\alpha}w_{\alpha}$
% can be interpreted as interaction between tree nodes $\beta$ and $\alpha$,
% We approximate $K_{\beta\alpha}$ with both column and row skeletons as
% $P_{\sk{\beta}\beta}^{T}K_{\sk{\beta}\sk{\alpha}}P_{\sk{\alpha}\alpha}$.
% Notice that due to the nested relation of $\alpha \subset \sk{\lc} \cup \sk{\rc}$,
% $P_{\sk{\alpha}\alpha}$ and $P_{\sk{\beta}\beta}$
% can be \emph{telescoped} recursively as
% \begin{equation}
% P_{\sk{\alpha}\alpha} =
% P_{\sk{\alpha}[\sk{\lc}\sk{\rc}]}
% \begin{bmatrix}
%   P_{\sk{\lc}\lc} & \\
%                   & P_{\sk{\rc}\rc} \\
% \end{bmatrix}.
% % \label{e:telescope}
% \end{equation}
% Recursive interpolation from both sides (left and right) is the key to achieve
% fast evaluation (less than quadratic). Still we need to first decide
% which pair of $\beta$ and $\alpha$ can be approximated and how to compute
% all pairs of interaction efficiently.
% 
% In \algref{a:nearfar} and \algref{a:farfar}, we decide which pair of
% interaction can be approximated and which cannot by traversing the tree
% top-down. 
% We collect all neighbors
% of a leaf node $\beta$ (including $\beta$ itself) named as $\MA{N}(\beta)$, 
% and we invoke \texttt{NearFar}($\beta$,\texttt{root}) for each $\beta$. 
% While traversing to $\alpha$,
% we say $\beta$ can prune $\alpha$ if none of $\MA{N}(\beta)$ appears in $\alpha$.
% We append $\alpha$ to $Far(\beta)$, which denotes all interactions that can be
% approximated with $\beta$. 
% Otherwise, we recurse to visit the children $\lc$ and $\rc$ of $\alpha$.
% While reaching the leaf level, if $\alpha$ still contains neighbors of $\beta$,
% then we append $\alpha$ to $Near(\beta)$, which denotes all interactions that
% cannot be approximated.
% \algref{a:nearfar} generates a list of prunable interactions for all leaf
% nodes. To generate prunable interaction lists for inner nodes, we merge
% two prunable lists from children to find the intersection.
% \algref{a:farfar} performs a post-order traversal, merging the far interaction
% lists from children, then removing duplications.
% 
% 
% % \begin{algorithm}
% % \caption{{} \texttt{Node2Skel}($\alpha$)}
% % \begin{algorithmic}
% % \STATE \texttt{\bf if} $\alpha$ is leaf \texttt{\bf then} $\sk{w_{\alpha}} = P_{\sk{\alpha}\alpha}w_{\alpha}$;
% % \texttt{\bf else} $\sk{w_{\alpha}} = P_{\sk{\alpha}[\sk{\lc}\sk{\rc}]}[ \sk{w_{\lc}}; \sk{w_{\rc}}]$;
% % \end{algorithmic}
% % \label{a:n2s}
% % \end{algorithm}
% % 
% % 
% % \begin{algorithm}
% % \caption{{} \texttt{Skel2Skel}()}
% % \begin{algorithmic}
% %   \STATE \texttt{\bf for each} $\beta$ \texttt{\bf do} $\sk{u_{\beta}} =
% %   \sum_{\alpha\in Far(\beta)} K_{\sk{\beta}\sk{\alpha}}\sk{w_{\alpha}}$;
% % \end{algorithmic}
% % \label{a:n2n}
% % \end{algorithm}
% % 
% % \begin{algorithm}
% % \caption{{} \texttt{Skel2Node}($\beta$)}
% % \begin{algorithmic}
% % \STATE \texttt{\bf if} $\beta$ is leaf \texttt{\bf then} $u_{\beta} = P_{\sk{\beta}\beta}^{T}\sk{u_{\beta}} + \sum_{\alpha\in Near(\beta)}K_{\beta\alpha}w_{\alpha}$;
% % \STATE \texttt{\bf else} $[\sk{u_{\lc}};\sk{u_{\rc}}] +=
% % P_{\sk{\beta}[\sk{\lc}\sk{\rc}]}^{T}\sk{u_{\beta}}$;
% % \end{algorithmic}
% % \label{a:n2s}
% % \end{algorithm}
% 
% 
% \begin{algorithm}
% \caption{{} \texttt{Evaluate}($u$,$w$)}
% \begin{algorithmic}
% \STATE \textbf{POST} \texttt{N2S($\alpha$)}
% \STATE \textbf{ANY} \texttt{S2S($\beta$)} \#\textbf{RAW} on \texttt{N2S}
% \STATE \textbf{PRE} \texttt{S2N($\beta$)} \#\textbf{RAW} on \texttt{S2S}
% \STATE \textbf{ANY} \texttt{L2L($\beta$)} \#\textbf{RAR} on \texttt{S2N}
% \end{algorithmic}
% \label{a:evaluate}
% \end{algorithm}
% 
% With the near and far interaction lists in hand, approximating the 
% matrix-multiplication of $K$ is described in \algref{a:evaluation}
% as a three-step process. 
% 
% \texttt{Node2Skel} performs a postorder traversal
% and computes the skeleton weight $\sk{w_{\alpha}}=P_{\sk{\alpha}\alpha}w_{\alpha}$ 
% for each tree node by multiplying the coefficient matrix on the left. 
% Recall from \eqref{e:telescope} that for a non-leaf node,
% $P_{\sk{\alpha}\alpha}$ can be \emph{telescoped} fast. 
% Thus, $\sk{w_{\alpha}}$ can be computed as
% $P_{\sk{\alpha}[\sk{\lc}\sk{\rc}]}[\sk{w_{\sk{\lc}}}\sk{w_{\sk{\rc}}}]$
% in the postorder traversal, because skeleton weights of both children have
% been computed. \texttt{Skel2Skel} traverses each node and compute the skeleton potentials
% $\sk{u_{\beta}} = \sum_{\alpha\in Far(\beta)}
% K_{\sk{\beta}\sk{\alpha}}\sk{w_{\alpha}}$.
% Finally \texttt{Skel2Node} performs a preorder traveral to accumulate
% all skeleton potentials. While traversing to leaf nodes, the aggregate
% potential is the summation of the skeleton potentials
% $P_{\sk{\beta}\beta}^{T}\sk{u_{\beta}}$
% and all direct interactions  
% $\sum_{\alpha\in Near(\beta)}K_{\beta\alpha}w_{\alpha}$.
% 
% \textbf{Complexity.} 
% While the worst case estimate of \algref{a:evalaute} is $\MA{O}(N^2)$
% (no approximation takes place), the best case appears when each 
% $Near(\alpha)$ only contains $\alpha$ itself.
% For a complete binary tree with $\MA{O}(N/m)$ leaf nodes, there are also 
% $\MA{O}(N/m)$ inner nodes.
% The cost for \texttt{Node2Skel} is $\MA{O}(sm(N/m)+2s^2(N/m))$.
% The cost for \texttt{Skel2Skel} is $\MA{O}(2s^2(N/m))$.
% Finally the cost for \texttt{Skel2Node} is
% $\MA{O(m^2(N/m)+sm(N/m)+2s^2(N/m))}$.
% When $s$ and $m$ are constant in $N$, the asympotoptic work 
% is $O(N)$ per right hand side.
% 
