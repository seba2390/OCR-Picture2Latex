In \hmatrix{} methods and FMM the main algorithmic pattern is a tree traversal. A traversal may exhibit high parallelism at the leaf level but due to the dependencies the parallelism typically diminishes near  the root level. In addition, if the workload per tree node varies, load balancing becomes an issue.  Most static scheduling codes employ level-by-level traversals, which introduces unnecessary synchronizations. In \gofmm{}, we observe significant workload variations during the compression (\algref{a:skeletonize}) and during the evaluation (tasks \texttt{N2S} and \texttt{S2N}). 

One solution is to exploit parallelism in finer granularity. For example, when the number of tree nodes
in the single tree level is less than the number of cores, we can use
multi-threaded BLAS/LAPACK on a single tree node. However, this is insufficient
if the workload does not increase significantly (e.g. growing with
$\lvert\alpha\rvert$)
while approaching the root. %because there will not be enough work for all threads. 
(That is, the workload must be within the strong scaling range of BLAS/LAPACK to
be efficient).

To partially address these challenges we abandon the convenient level-by-level
traversal and explore an \emph{out-of-order} approach using dynamic scheduling.
To this end we test two approaches and compare them with a level-by-level
traversal. In the first approach, we introduce a self-contained runtime system.
In the second approach we test the same ideas with \texttt{OpenMP}'s \texttt{omp task depend} feature. 

\begin{figure}[!t]
  \centering
  \includegraphics[scale=.4]{figures/n2s2n.pdf}
  \caption{Dependency graph for steps 1--3 of \algref{a:evaluate} (step 4 is completely independent of steps 1--3).
    Each tree node denotes a task, and the arrows between nodes imply a
    dependency. Here $\mathtt{Near}(\alpha)$ only contains itself (HSS). For example, yellow node $\beta$ has a \textbf{RAW}
    dependency following blue $\alpha$, because \texttt{S2S($\beta$)} computes 
  $\sk{u}_{\beta} = \sum_{\alpha \in
  \mathtt{Near}(\beta)}K_{\sk{\beta}\sk{\alpha}}\sk{w}_{\alpha}$.
  When $\mathtt{Near}(\beta)$ contains more than just itself. The dependencies
    are unknown at compile time and thus, \texttt{omp task depend} fails to describe 
    the dependencies between \texttt{N2S} and \texttt{S2S}.
  }
  \label{fig:n2s2n}
\end{figure}


\textbf{Dependency analysis.}
Recursive preorder and postorder traversals 
inherently encode \textbf{R}ead/\textbf{W}rite dependencies 
between tree nodes. Following \algref{a:compress} and \algref{a:evaluate}, we can describe dependencies between different tasks. However, due to dynamic granularity of tasks we need a data flow 
analysis at runtime.  For example, dependencies between \texttt{N2S} and \texttt{S2S} 
cannot be discovered at compile time, because the \textbf{RAW}
(read after write) dependencies on $\sk{w}_{\alpha}$ are computed by neighbors
$\MA{N}(\alpha)$. In order to build dependencies at runtime as a direct acyclic graph (DAG), 
we perform a \emph{symbolic} execution on \algref{a:compress} 
and \algref{a:evaluate}. For simplicity, below we just discuss the evaluation phase for the HSS case 
(the FMM case is more involved).

%During the symbolic execution, we only annotate the dependencies without performing any numerical
%computation. (I think that is what symbolic means) 
\figref{fig:n2s2n} depicts task dependencies (by tasks we mean algorithmic
tasks defined in~\tabref{tab:tasks}) during the evaluation
phase~\algref{a:evaluate} for \texttt{N2S}, \texttt{S2S} and \texttt{S2N} where
the off-diagonal blocks are  low-rank (HSS) with $S=0$. This task dependency
tree is generated by our runtime using symbolic traversals. The \texttt{N2S},
\texttt{S2S}, and \texttt{S2N} execution order is performed on a binary
tree\footnote{\scriptsize Execution order from left to right: dependencies are
easier to follow if one rotates the page by $90^{\circ}$ counter-clockwise}.

We use three symbolic traversals of~\algref{a:evaluate}. In the first traversal (postorder) we
find that  $\sk{w}_{\lc}$ is written by $\lc$. Going from $\sk{w}_{\lc}$ to
$\sk{w}_\beta$, we annotate that $\sk{w}_{\lc}$ is read by $\beta$, i.e.
$\sk{w}_{\beta}= P_{\sk{\beta}[\sk{\lc}\sk{\rc}]}[\sk{w}_{\lc};\sk{w}_{\rc}]$.
This \textbf{RAW} dependency is an edge from $\lc$ to $\beta$ in the DAG.

Intertask dependencies are discovered by  the \emph{symbolic}
execution of the yellow tree. At node $\beta$ (in yellow), the relation
$\sk{u}_{\alpha}=K_{\sk{\alpha}\sk{\beta}\sk{w}_{\beta}}$
will read $\sk{w_{\beta}}$. Again this is a \textbf{RAW} dependency, hence the edge from the blue $\beta$ to the yellow $\alpha$. The whole dependency graph for steps 1--3 is built after the green postorder tree traversal. Step 4 in \algref{a:evaluate} is independent of steps 1--3. Although this runtime data flow analysis has some overhead, the amount is almost negligible ($<1\%$) compared to the total execution time.  

\textbf{Runtime.}
With a dependency graph, scheduling can be done in 
\emph{static} or \emph{dynamic} fashion. 
Due to unknown adaptive rank $s$ at compile time, we implement a 
light-weight dynamic Heterogeneous Earliest Finish Time 
(HEFT)~\cite{topcuoglu2002performance} using \texttt{OpenMP} threads.
Each worker (thread) in the runtime can use more than one physical 
core with either  a nested \texttt{OpenMP} construct or by employing a device  (accelerator) as a slave.
Tasks that satisfy all dependencies in the DAG  will be 
dispatched to a ``ready'' queue. Each worker keeps 
consuming tasks in its own queue until no tasks are left.

Although we estimate a cost for each task\footnote{\scriptsize 
We divide costs for tasks by the theoretical peak 
\texttt{FLOPS} of the target architecture and a discount factor. 
For memory-bound tasks we use the theoretical \texttt{MOPS}
instead.} 
in \tabref{tab:tasks}, the runtime of a normal worker (or one with an accelerator) depends on the problem and can only be determined at runtime.
The HEFT schedule is implemented using an estimated finish time of all pending tasks in a specific worker's  ready queue. Each task dispatched from the DAG is assigned to a queue such that the maximum estimated finish time of each queue  is minimized. For the case where the estimation is inaccurate, we also implement
a job stealing mechanism. 

\textbf{Other parallel implementation.}
We briefly introduce other possible parallel implementations and conduct a strong scaling experiment in \secref{s:results}.
Here we implemented parallel level-by-level traversals for all tasks 
that require preorder and postorder traversals and do not exploit out-of-order parallelism. 
For tasks that can be executed in any order, we simply use 
\texttt{omp parallel for} with dynamic scheduling. 
If there are not enough tree nodes in a tree level, we use
nested parallelism with inner \texttt{OpenMP} constructs 
and multi-threaded BLAS/LAPACK.

The \texttt{omp task} version is implemented using recursive 
preorder or postorder traversals. Due to the overhead 
of the deep call stack, this implementation can be much slower than others. Although we tested it we do not report results because it is not competitive.

We also implemented (and report results for) \texttt{omp task depend}, since
\texttt{OpenMP-4.5} supports task parallelism with dependencies. However there
are two issues. First,
\texttt{omp task depend} requires all dependencies to be known at compile time,
which is not the case for the FMM (tasks \texttt{N2S} and \texttt{S2S}).
Second, without knowledge of the estimated finish time, the \texttt{OpenMP} scheduler will be suboptimal.  Finally for CPU-GPU hybrid architectures, scheduling GPU tasks purely with 
\texttt{omp task} can be very challenging. 

\textbf{CPU-GPU hybrid.}
GPUs usually offer high computing capacity, but  performance 
can easily be bounded by the PCI-E bandwidth. 
Because most computations in 
\algref{a:compress} are complex and memory bound\footnote{\scriptsize Although \texttt{GEQP3} 
and \texttt{TRSM} can be performed on GPUs with \texttt{MAGMA}
(\url{http://icl.cs.utk.edu/magma/})
and \texttt{cublas}, we find this inefficient for our methods.},
we do not use GPUs for the compression.
Instead we only pre-fetch submatrices $K_{\beta\alpha}$ and
$K_{\sk{\beta}\sk{\alpha}}$ to the device memory to overlap with
computations on the host (CPUs).
During the evaluation, our runtime will decide--depending on the number of
\texttt{FLOPS}-- whether to issue a batch
of tasks (up to 8) to the GPU in concurrent (using \texttt{stream}). This usually 
occurs in \texttt{N2S} and \texttt{S2N} where the size of 
\texttt{cublasXgemm}
%\footnote{\scriptsize 
%We cannot use \texttt{cublasXgemmBatched}, because due to adaptive rank approximations we have 
%different sizes of the \texttt{GEMM}.} 
is bounded by $s$ and $m$.
Furthermore, to hide communication time between CPU and GPU, 
all arguments of the next task in queue are pre-fetched using
asynchronous communication for pipelining. 
Finally, because a worker with a GPU is usually 
$50\times$ to $100\times$ more capable than others, we disable job stealing balancing for GPU workers.
This optimization prevents the GPU from idling.

%\textbf{Distributed.}
