%Given $N^2$ entries in a dense system matrix it seems to be unavoidable to construct algorithms of complexity less than $\mathcal{O}(N^2)$; for several application problems [insert cites], however, low rank approximations can be applied to some blocks in the matrix. 
%For several application problems a suitable hierarchical subdivision structure can be found by a spatial tree in the physical problem domain ; in high-dimensions ASKIT... make use of randomized projection trees. 


Kernel summation methods - be it analytic, semi-analytic or algebraic - usually compute low rank approximations based on the far-field in the physical problem domain. Analytic and semi-analytic approaches fail in high dimensions as the number of required check and seed points grow exponentially.
In many applications (e.g. data analysis) the high dimensional feature space can be reduced to a low intrinsic dimension; for such kernel summations an algebraic skeletonization-based approach using a randomized projection tree has been introduced \cite{march-xiao-yu-biros15}. Near- and Far-Field separation is based on euclidean neighbor information; i.e. the near iteraction of each row $K_{i:}$ is defined by the
neighbor entries $\MA{N}_i$, and the far interaction is 
computed based on a nested binary partition of $K$.

Any symmetric positive-definite (SPD) matrix can be described as a Gramian matrix of some set of vectors $\phi$.
Given a matrix $K$ (e.g. from a kernel function) being symmetric positive definite, we can assume that there exists a set of vectors $\phi_i \in \mathcal{V}$ which define this matrix by a scalar product,  $$K_{ij}=\mathcal{K} (x_i,x_j)=<\phi_i,\phi_j>$$ 
%short version
%We consider a given symmetric positive-definite (SPD)  matrix $K$ in terms of an underlying Gramian set of vectors as  $$K_{ij}=\mathcal{K} (x_i,x_j)=<\phi_i,\phi_j>$$ 

%Note that we do 	not have direct access to the underlying vector set of $\phi_i$, but only to the scalar product between each other.

Hence, we use the Gramian space in defining a hierarchical subdivision where entries $i$,$j$ are considered near in terms of 
\begin{itemize}
\item[a.] the $L_2$ distance $\norm{\phi_i-\phi_j}_2^2=K_{ii}+K_{jj}-2K_{ij}$
\item[b.] the spanned angle $\sphericalangle(\phi_i,\phi_j)=arccos(\frac{K_{ij}}{K_{ii}K{jj}})$
\end{itemize}

This tree-like partition permutes $K$ into a hierarchical matrix whose off-diagonal blocks are potentially low-rank.
%We first define distance metrics for each column and row based on the symmetric factorization of $K$.

For doing so, we consider a direction spanned by the two farthest points $\phi_\alpha$ and $\phi_\beta$ and split indices $i$ binary by
\begin{itemize}
\item[a.] a $L_2$ based splitting $<\phi_i,\phi_\alpha-\phi_\beta>$
\item[b.] a $d-1$-dimensional two-fold cone spanned in direction $\phi_\alpha-\phi_\beta$, and its opposite direction $\phi_\beta-\phi_\alpha$, i.e. \\ $\frac{1}{\norm{\phi_i}}\norm{<\phi_i,\phi_\alpha-\phi_\beta>}$
\end{itemize}

