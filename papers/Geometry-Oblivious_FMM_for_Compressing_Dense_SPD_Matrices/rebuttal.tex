We are truly grateful to the reviewers for their time and their extensive and helpful comments. We will try to address all their detailed comments in the final paper. 

---SEVERAL REVIEWERS---

>Why not MPI and DISTRIBUTED ALGORITHMS?

First, we believe that there is no space for this discussion in this paper. 
GOFMM is a completely new algorithm and implementation. Thus, we needed to
demonstrate correctness and single node performance. We tested several
matrices from machine learning, graphs, partial differential equations and
inverse problems; tested different problem sizes; and compared 
Haswell, KNL, and GPU architectures. 
We also demonstrated GOFMM's unique capabilities
(lines 94--96) and addressed the ``permuting problem'' for dense matrices (lines 160--164).  
So we believe this material is enough for an SC paper. 
Several reviewers mentioned that the paper is already too dense.

Second, the MPI extension requires new algorithms, which require a separate paper.  We are currently working on the MPI version of GOFMM.   The basic philosophy of MPI parallelism follows [31] and [43].  New challenges include parallelizing matrix access, integrating the task-scheduling with MPI, 
accounting for off-node dependencies from other ranks, and load-balancing. 
Inter-process job stealing may result in extra communication. 

---REVIEWER 1---

>Specialization of ASKIT.

GOFMM is not a specialization of ASKIT, but rather a generalization.
GOFMM can be applied to an arbitrary SPD matrix, while ASKIT only applies to kernel matrices. ASKIT requires *point coordinates* and a *kernel function* as inputs. Distances are defined using point coordinates, and matrix entries are computed by applying the kernel function to pairs of points. GOFMM only requires the ability to sample entries of the matrix, which is not necessarily a kernel matrix. GOFMM also supports kernel matrices with arbitrary dimensions of data points, like ASKIT. Our examples include such matrices (K04-K10).

>WHY...WRITE YOUR OWN SCHEDULER?

We do so to remove non-standard software dependencies for improving portability and usability. 
GOFMM only requires C++11, OpenMP-3.0, and BLAS/LAPACK.
Our scheduler does not use "omp task". Workers are implemented using "omp parallel for" with static thread binding.

>RUN THE CODE SERIALLY

We can add the results in the revision (space permitting).
Here we report the single core performance of matrix K02 on HASWELL.
Compression takes 9.6 seconds, and evaluation takes 2.8 seconds (54 GFLOPS).

>HYPER-THREADING AND VECTORIZATION ON KNL

Vectors: We do use vectorization throughout.
The computations in Table 2 invoke Intel MKL's BLAS/LAPACK routines,
which use AVX512. (The peak of MKL's GEMM on KNL is about 70\%.)
Our neighbor-search is also highly optimized as  described in [46] and uses vectorization. 

Threads: Hyperthreading is a good idea if we have sufficient tasks. But in GOFMM, we don't have that many tasks. In certain phases of GOFMM, we even assign more than one physical core to a single task when there are not enough tasks.

Performance: The problem sizes of GEMM in GOFMM vary due to the adaptive scheme
(see footnote 4), and many of them are measured to less than 20\% of the GEMM's theoretical peak.

---REVIEWER 2---

>DISTRIBUTED
Please see above.


---REVIEWER 3---

>EXPAND...IN THE CONTEXT OF HOW HPC HARDWARE IS TRENDING TO BEING

Indeed, due to very different kinds of tasks, GOFMM may require
both high throughput (GPU and KNL) and low latency (CPU) units
to be efficient.  Your questions are excellent but we don't have a short answer.  There are several algorithmic opportunities to push GOFMM further into the  high-throughput regime.   We will revise the conclusions to discuss as much as space permits.

>DISCUSSION OF THE RELEVANCE

Please see lines 65-70. Significant applications will be in ML and
fast solvers. We will revise conclusions to add more examples.

>DISTRIBUTED 
Please see above. 

---REVIEWER 4---

>SOFTWARE VERSIONS

Please see lines 910-917.

>NEAREST NEIGHBORS

80\% and 10 iterations are chosen empirically. 
In our experiments, we found that more accurate nearest neighbors do not improve
the GOFMM approximation.  The reason is that the neighbors are used for sampling and selecting groups of direct evaluations between leaves. 
Typically even smaller values are sufficient (70\%).  We use 80\% to be conservative.  More specifically, nearest-neighbors provide a guess of important matrix entries,  which are used in importance sampling and selecting near interactions. According to [32], whether neighbors can improve the accuracy depends on the data (gram-vectors) in different scales.
Typically, neighbor-pruning works better if the intrinsic dimensionality
of the data is low. Consequently, random projection based ANN methods 
also converge faster [43]. Otherwise, it is likely that
neighbors won't improve the accuracy too much.


