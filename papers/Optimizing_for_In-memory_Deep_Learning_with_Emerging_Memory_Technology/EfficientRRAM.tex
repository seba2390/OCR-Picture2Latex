%% 


\documentclass[10pt,journal,compsoc]{IEEEtran}

\usepackage{flushend}
\usepackage{xcolor}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{tabularx}
\usepackage{graphicx}

% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}

\title{Optimizing for In-memory Deep Learning with Emerging Memory Technology}% Storage Elements}

% considers remove storage elements.


\author{Zhehui~Wang, Tao~Luo, Rick~Siow~Mong~Goh, Wei~Zhang, Weng-Fai~Wong
\thanks{Zhehui Wang, Tao Luo and Rick~Siow~Mong~Goh are with the Institute of High Performance Computing (IHPC), Agency for Science, Technology and Research (A*STAR), Singapore.}
\thanks{Wei Zhang is with the Hong Kong University of Science and Technology.}
\thanks{Weng-Fai Wong is with the National University of Singapore.}
}



% The paper headers
%\markboth{Journal of \LaTeX\ Class Files,~Vol.~14, No.~8, August~2015}%
%{Shell \MakeLowercase{\textit{et al.}}: Bare Demo of IEEEtran.cls for IEEE Journals}
% The only time the second header will appear is for the odd numbered pages
% after the title page when using the twoside option.





% As a general rule, do not put math, special symbols or citations
% in the abstract or keywords.
\IEEEtitleabstractindextext{

\begin{abstract}



In-memory deep learning computes neural network models where they are stored, thus avoiding long distance communication between memory and computation units, resulting in considerable savings in energy and time. In-memory deep learning has already demonstrated orders of magnitude higher performance density and energy efficiency. The use of emerging memory technology promises to increase the gains in density, energy, and performance even further. However, emerging memory technology is intrinsically unstable, resulting in random fluctuations of data reads. This can translate to non-negligible accuracy loss, potentially nullifying the gains. In this paper, we propose three optimization techniques that can mathematically overcome the instability problem of emerging memory technology. They can improve the accuracy of the in-memory deep learning model while maximizing its energy efficiency. Experiments show that our solution can fully recover most models' state-of-the-art accuracy, and achieves at least an order of magnitude higher energy efficiency than the state-of-the-art.



% the advantage is just for MNIST!!!this accrayc on larger dataset

% advantages of in-memory deep learning.
%shows singfiicnatly imrpivemen throug0ut and energy efficency.
\end{abstract}
% make the title area

\begin{IEEEkeywords}
Deep learning, In-memory computing, Optimization, Emerging memory technology
\end{IEEEkeywords}
}

\maketitle


%In-memory deep learning is a promising candidate for green AI, 
 
%\newpage
 %https://ieeexplore.ieee.org/document/9171556


\IEEEpeerreviewmaketitle

\section{Introduction}

Deep learning neural networks (DNNs) are widely used today in many applications such as image classification, object detection, etc. High throughput and high energy efficiency are two of the pressing demands on DNNs these days. However, current computation mechanisms are not the best choices for DNN in terms of efficiency. As DNN models become increasingly complex, the computation process takes a lot of time and energy~\cite{inference2015performance}. In traditional von Neumann architectures, storage memory is separated from computation operations. To compute a output feature map, we need to read the input feature map and weights from the memory units, send them to the computation module to compute before writing the results back to the memory units. During the whole process, the system spends a large portion of energy and time in data movement~\cite{yang2017designing}. This is made worse with advances in process technology, making the relative distances involved even longer and thus more costly.

{\em Emerging memory technologies} (EMT) including PCRAM, STT-RRAM and FeRAM~\cite{meena2014} promise better density and energy efficiency, especially since many are non-volatile and well suited to the read-mostly applications. 
In-memory deep learning using EMT cells, especially in {\em analog} mode, has already demonstrated an order of magnitude better performance density and two orders of magnitude better energy efficiency than the traditional deep learning on the MNIST dataset~\cite{yao2020fully}~\cite{cai2019fully}~\cite{luo2021nc}.
%It has become a promising technology for DNN developers who are targeting high-throughput and low energy consumption. 
As in-memory deep learning integrates computation with the memory operations~\cite{sebastian2020memory}, the computation results can be directly read from the memory modules using a single instruction.  This is different from traditional deep learning, where the memory operations are executed separately from the computation operations. Computing where data is stored reduces the need to move large amounts of data around frequently. Especially when technology scales and on-chip distances become longer, we can expect substantial savings of time and energy if the emerging memory technology is used in the in-memory deep learning paradigm. 

A big challenge of in-memory computing is the instability of EMT cells~\cite{raghavan2013rtn}~\cite{luo2018fpga}. Unlike traditional memory technology, where the data are stored in stable memory cells, data stored in analog mode EMT cells may fluctuate and different values can be output. For instance, suppose we store the weight $w$ in an EMT cell. When we read it from the EMT cell, the output may become $w+\Delta w$ instead of $w$. Here $\Delta w$ is a fluctuating amplitude of that memory cell. Because of this instability, in-memory deep learning using EMT especially in an analog manner may make incorrect classification~\cite{du2020exploring}. This can severely limit its application in the real world.

Another challenge of in-memory deep learning is the ineffectiveness of traditional energy reduction techniques, such as pruning, quantization, etc. Pruning reduces energy consumption by decreasing the number of operations~\cite{yang2017designing}~\cite{DeepCompression}. However, the energy of in-memory deep learning is roughly proportional to the weight values. Pruning usually removes those weights with smaller absolute values, which only contribute to a small portion of the energy consumption. Quantization reduces the energy consumption by decreasing the complexity of operations using low precision data~\cite{wang2019haq}~\cite{wang2021evolutionary}. Unfortunately, in EMT cells, low precision data consumes almost the same amount of energy as that of the high-precision data. Therefore, traditional pruning and quantization technologies are less effective for in-memory deep learning.

In this paper, we solve these problems by proposing three techniques. They can effectively recover the model accuracy loss in in-memory deep learning and minimize energy consumption. Our innovations are:
\begin{itemize}
\item\textbf{Device enhanced Dataset:} We propose augmenting the standard training dataset with device information integrated into the dataset. In this way, the models will also learn the fluctuation patterns of the memory cells. This extra information can also help the optimizer avoid overfitting during training and thus improve the model's accuracy.
\item \textbf{Energy Regularization:} We optimize the energy coefficients of memory cells and the model parameters by adding a new term into the loss function. This new term represents the energy consumption of the model. The optimizers can automatically search for the optimal coefficients and parameters, improving the model accuracy and energy efficiency at the same time. 
\item \textbf{Low fluctuation Decomposition:} We decompose the computation into several time steps. This is customized for in-memory deep learning and can compensate for the fluctuation of memory cells. This decomposed computation mechanism can improve the model accuracy and energy efficiency.
\end{itemize}
We prove the effectiveness of all these techniques both theoretically and experimentally. They can substantially improve the model accuracy and energy efficiency of models for in-memory deep learning. We organize the rest of the paper as follows: Section~\ref{s2} gives related work; Section~\ref{s3} presents the background knowledge; Section~\ref{s4} introduces our proposed optimization method; Section~\ref{s5} shows the experiment results; Section~\ref{s6} concludes.

\begin{figure*}[!t]
  \centering
  \includegraphics[width=7.0in] {Figure/Overview.eps}
  \caption{(a) Comparison between the traditional memory cell and the EMT cell; (b) The computation mechanism for traditional deep learning; (c) The computation mechanism for EMT-based in-memory deep learning, where we integrate multiplication and addition into the read operations.}
  \label{f:work_overview}
\end{figure*}

\begin{figure}[!t]
  \centering
  \includegraphics[width=3.5in] {Figure/Baisc.eps}
  \caption{(a) The relationship between the energy consumption of the EMT cell and the value of the weight; (b) The probability distribution of the weight fluctuation, with different energy coefficients $\rho$. The traditional cell is marked in grey color for reference.}
  \label{f:basic}
\end{figure}

\section{Related Work}
\label{s2}

The first category of work to improve the accuracy of the in-memory deep learning model is called \textit{binarized encoding}. The information in each memory cell is digitized into one bit instead of being stored as a full precision number. In other words, the data stored in each memory cell is either $1$ or $0$~\cite{emara2014differential}. Theoretically, the one-bit data is more robust than a high-precision number at the same level of fluctuation.  Several previous works used the one-bit design to compute the binarized neural networks. Sun \textit{et al.} used the single-bit cell to execute the XNOR operations in XNOR-Net~\cite{sun2018xnor}. Chen \textit{et al.}~\cite{chen201865nm} used the single-bit cell to execute the basic operations in binary DNN. Tang \textit{et al.} proposed a customized binary network for the single-bit cell ~\cite{tang2017binary}. However, either the XNOR-Net or the binary neural network has a large accuracy drop compared with the full-precision model~\cite{rastegari2016xnor}. Recently, new progress in this research direction is to store a high-precision weight using a group of single-bit memory cells. For example, Zhu \textit{et al.}~\cite{zhu2019configurable} used single-bit memory cells $\times N$ to store a $N$-bit weight. Such a method can increase the model accuracy because it can increase the effective precision of weights. Compared with the traditional design, it uses more memory cells.

The second category of work to improve the accuracy of the in-memory deep learning model is called \textit{weight scaling}. Theoretically, we can reduce the amplitude of weight fluctuation by scaling up the weight values stored in the memory cell~\cite{peng2019optimizing}. After computation, we scale the result down using the same scaling factor. In the literature, many research works have found other physical ways to reduce the amplitude of weight fluctuation. For example, He \textit{et al.} found that we can reduce the fluctuation amplitude by lowering the operation frequency~\cite{he2019noise}. Chai \textit{et al.} found one material, which has lower fluctuation amplitude than the other types of material~\cite{chai2018impact}. However, these methods demand strict physical conditions. Compared with them, weight scaling is a more general method that can reduce the fluctuation amplitude of memory cells in most conditions~\cite{sorbaro2020optimizing}. However, Choi \textit{et al.} found that although the memory cell using scaled weights showed smaller fluctuation amplitude, it also consumed higher energy consumption~\cite{choi2014random}. Ielmini \textit{et al.} modeled the relationship between the scaling factor, the fluctuation amplitude, and the energy consumption~\cite{ielmini2010resistance}, which could help us to find the optimal scaling factor for in-memory computing computation.

The third category of work to improve the accuracy of the in-memory deep learning model is called \textit{fluctuation compensation}. To alleviate the instability, they first read the memory cell by multiple times and then record the statistical result such as mean and standard deviation~\cite{puglisi2015statistical}. Afterward, they either calibrate the model parameter or the model output directly based on that statistical results~\cite {shim2020two}. This method is also widely used during the memory cell programming stage. For example, Joshi \textit{et al.} compensated the programming fluctuation by tuning the batch normalization layer parameters~\cite{joshi2020accurate}.
Alternatively, Zhang \textit{et al.} compensated the programming fluctuation by offsetting the weight values~\cite{zhang2020reliable}. These methods are effective in a static device environment. If we face a dynamic environment, a more general way is needed. One popular approach is to have many equivalent models running in parallel. Then we calculate the mean of the results. Joksas \textit{et al.} did this by applying the committee machine theory into the in-memory computing devices~\cite{joksas2020committee}. Wan \textit{et al.} optimized this process by running a single model on the same device and reading the memory cells multiple times~\cite{wan2020voltage}. This method can average out the weight fluctuation and get a more stable result.

\section{Preliminaries}
\label{s3}

EMT-based in-memory deep learning can be very efficient~\cite{yao2020fully} because of its analog operation. Fig.~\ref{f:work_overview}(a) shows the difference between the traditional and EMT memory cells. When we read a weight $w$ from the traditional memory cell, the input to the corresponding memory cell is $1$, meaning that the read request to that memory cell is enabled. Afterward, the memory cell returns $w$ as an output. An EMT cell for in-memory deep learning is quite different. When we read the weight $w$, the input to the memory cell is a variable $x$ instead of the fixed data $1$. The memory cell then returns $x\cdot w$ directly, the product of the input signals $x$ and the stored weight $w$. In other words, the EMT cell integrates the multiplication operation into the read operation.

Analog EMT is more efficient than traditional memory not only in multiplication operations but also in addition operations~\cite{pedretti2021memory}. To better explain this, we show how traditional cells and EMT cells execute the {\em multiply-accumulate} (MAC) operation in Fig.~\ref{f:work_overview}(b) and Fig.~\ref{f:work_overview}(c), respectively. For traditional memory cells, we first read weight $w_i$ from the corresponding memory cells. Afterward, the output $w_i$ is multiplied by the activation $x_i$ using a multiplier. Finally, we sum all products $x_i\cdot w_i$ from each of the multipliers together either by a single adder sequentially, or use a tree of adders to perform the sum in parallel. To achieve the same computation using EMT-based in-memory computing, we just need to connect the output of each memory cell to the same port. Physically, the sum of all the memory cell outputs $\sum x_i\cdot w_i$ can be obtained from that port directly. This is also referred to as a {\em current sum}.
%As we can see, the emerging memory cell also integrates the addition operation into the read operation.

\subsection{Challenges of Analog In-memory Deep Learning}

The energy consumption of a EMT cell is much less than that of traditional memory cells when they execute the MAC operation. There are important differences. In traditional memory cells, energy consumption is not related to the weight value that is stored. In analog EMT, it is proportional to the weight value~\cite{wang2020ncpower}, as shown in Fig.~\ref{f:basic}(a). We use a parameter $\rho$ to denote this energy coefficient. This parameter is tunable. We can use this parameter to optimize its energy consumption. Theoretically, a small coefficient $\rho$ can help us  improve the energy efficiency of models.

A big challenge in using the merging memory technology is that regardless of the actual technology, their memory cells do not output stable results~\cite{raghavan2013rtn}. Physically, each memory cell has multiple states, and it changes its state with time randomly. Whenever we read the memory cell, it can be in any of the states. At $l$-th state, the weight value read from the memory cell is $r_l(w,\rho)$, where $w$ is the pre-stored weight and $\rho$ is the energy coefficient. Given an input $x$, the output data becomes $x\cdot r_l(w,\rho)$ instead of $x\cdot w$. In Fig.~\ref{f:basic}(b), we show an example of a memory cell with two states. In this example, each state has a $50$\% probability to show. If the store weight is $w$, and the energy coefficient is $\rho$, the output result can be either $x\cdot r_0(w,\rho)$ or $x\cdot r_1(w,\rho)$ depending on its state, where $x$ is the input activation. The bottom three sub-graphs in Fig.~\ref{f:basic}(b) corresponds to memory cells with three different energy coefficients $\rho$.

The fluctuation shown in Fig.~\ref{f:basic}(b) is a simplified example. In practical EMT cells, the number of fluctuation states and the probability of each state are more complicated. There are many works in the literature studying these fluctuations~\cite{gong2018signal}.
For deep learning, such a phenomenon can cause a non-negligible accuracy drop. It is the biggest challenge that limits the application of in-memory computing. As we can see from Fig.~\ref{f:basic}(b), the fluctuation amplitude, defined as the average distance among $r_l(w,\rho)$, is related to the energy coefficient $\rho$. Theoretically, a higher $\rho$ will result in less fluctuation of the weight and thus higher model accuracy, but it also means higher energy consumption of memory cells. This trade off is the focus of this paper.

\subsection{Incompatibility of Traditional Training Method}
\label{s:risk}

Fig.~\ref{f:overview_trad} shows the standard training process in deep learning. The loss function takes the dataset and the model and generates a measure of the distance between the current parameter values and their optimal values. The optimizer uses gradient descent to reduce this distance by updating the parameters. This process would iterate for many epochs until the optimizer can find an optimal set of parameters. We define $\mathbf{X}\in \mathcal{X} $ as the image data and $\mathbf{Y}\in \mathcal{Y} $ as the label data. Here $\mathcal{X}$ and $\mathcal{Y}$ denote the spaces for images $\mathbf{X}$ and labels $\mathbf{Y}$. For simplicity, we express a one-layer neural network model in Equation~(\ref{e:whole}), where the weight $\mathbf{W}$ and the bias $\mathbf{B}$ are both trainable parameters.
\begin{equation}
\mathbf{Y}  = f(\mathbf{X}) = \mathbf{W}\mathbf{X}+\mathbf{B} 
\label{e:whole}
\end{equation}
Let's define the function class $\mathcal{F}\subset \mathcal{Y}^\mathcal{X}$ as the search space of function $f$. Let $ \mathcal{Z} = \mathcal{X} \times \mathcal{Y}$ be the combination of $\mathcal{X}$ and $\mathcal{Y}$, and $\textbf{Z}=(\textbf{X},\textbf{Y})$ be the combination of $\textbf{X}$ and $\textbf{Y}$. Define $\mathfrak{D}$ as the unknown distribution of data $\textbf{Z}$ on space $\mathcal{Z}$. Given the above definitions, the loss function can be expressed as $\mathcal{L}:\mathcal{F} \times \mathcal{Z} \rightarrow \mathbb{R}$. It is the mapping from the combination of $\mathcal{F}$ and $\mathcal{Z}$ to the real number space $\mathbb{R}$. Theoretically, the training process of the neural network is to find the optimal function $f$ from the function class $\mathcal{F}$ that can minimize the risk, i.e., the expectation of the loss function. We express it in Equation~(\ref{e:risk_old}). 
\begin{equation}
R[{f}]=\mathbb{E}_{{z}\sim {\mathfrak{D}}}\big[\mathcal{L}({f},{z})\big]
\label{e:risk_old}
\end{equation}

\begin{figure}[!t]
  \centering
  \includegraphics[width=3.5in] {Figure/MethodsOld.eps}
  \caption{The training method for traditional deep learning. The dataset (green blocks) includes images and labels.}
  \label{f:overview_trad}
\end{figure}

The difficulty in solving this problem is that the distribution $\mathfrak{D}$ is unknown. What we have is just the dataset $\{\textbf{Z}_1,\cdots,\textbf{Z}_N\}$, which are {\em independent and identically distributed} (i.i.d.) samples from the distribution $\mathfrak{D}$. Alternatively, the traditional training process of the neural network is to find the optimal model $f$ from the function class $\mathcal{F}$ that can minimize the empirical risk, i.e., the average of loss functions on the sampled dataset. We express it in Equation~(\ref{e:risk_oldd_sample}).
\begin{equation}
R_s[{f}]=\frac{1}{N}\sum_{i=1}^{N}{\mathcal{L}({f},{z_i})}
\label{e:risk_oldd_sample}
\end{equation}
The distance between the risk and the empirical risk is called the {\em generalization error} $\epsilon$, expressed in Equation~(\ref{e:risk_old_delta}). The crucial problem of the training process is to make sure that the generalization error $\epsilon$ can be bounded. 
\begin{equation}
\epsilon= R[{f}] - R_s[{f}]
\label{e:risk_old_delta}
\end{equation}

Researchers have since solved this math problem for traditional deep learning models. Many theoretical studies have shown that neural network optimizers, such as {\em stochastic gradient descent} (SGD)~\cite{bottou2012stochastic} or {\em adaptive moment estimation} (Adam)~\cite{zhang2018improved} can efficiently find the optimized function $f$. However, for in-memory deep learning models, because of the fluctuation of weight matrix $\mathbf{\widetilde{W}}$, the sampled function $f$ becomes $\widetilde{f}$ and the distance of risks becomes $\widetilde{\epsilon}$, as expressed in Equation~(\ref{e:risk_old_delta_2}). Therefore, the traditional training method no longer works for this problem. We need a new training method that is suitable for in-memory deep learning models.
\begin{equation}
\widetilde{\epsilon} = R[{f}] - R_s[\widetilde{f}]
\label{e:risk_old_delta_2}
\end{equation}

\begin{figure}[!t]
  \centering
  \includegraphics[width=3.5in] {Figure/Methods.eps}
  \caption{The proposed training method for in-memory deep learning. The dataset (green blocks) includes extra fluctuation data. We mark the proposed three optimization techniques in red color.}
  \label{f:overview}
\end{figure}

\begin{figure*}[!t]
  \centering
  \includegraphics[width=7in] {Figure/Train_1.eps}
  \caption{(a)-(f) Visualization of the device-enhanced datasets. Pixels forming letters \texttt{A} and \texttt{B} denote the image data labeled by class A and class B, respectively. The blurred pixels in (e) and (f) indicate the distribution of images $\mathbf{X}$ and fluctuation $\mathbf{S}$.}
  \label{f:process_path1}
\end{figure*}

\begin{figure*}[!t]
  \centering
  \includegraphics[width=7in] {Figure/Train_2.eps}
  \caption{(a)-(c) Visualization of the traditional datasets. The orange straight lines in (a) and (b) indicate the absence of device information.}
  \label{f:process_path2}
\end{figure*}

\section{Optimizing for In-memory Deep Learning}
\label{s4}

Our training method for in-memory computing can effectively improve the model accuracy, with improved energy efficiency. Fig.~\ref{f:overview} shows the overview picture. Essentially, we propose three optimization techniques and integrate them into the training process. The first technique is called {\em device-enhanced datasets}. This technique integrates device information as additional data into the dataset. This will make the model more robust to the fluctuations of the device. The second technique is called {\em energy regularization} which we add a new regularization term into the loss function that makes the optimizer reduce the energy consumption automatically. The third technique is called {\em low-fluctuation decomposition} by which we decompose the computation involved into several time steps. This decomposition achieves high model accuracy and energy efficiency. We shall now give the mathematical basis of these three techniques.

\subsection{Device-Enhanced Dataset}

Our first optimization technique is to enhance the dataset with device information. In addition to the regular image data $\mathbf{X}$ and the label data $\mathbf{Y}$, our enhanced dataset has another source of data, the fluctuation $\mathbf{S}$, which reflects the random behavior of memory cells. Fig.~\ref{f:process_path1} shows an visualized example of $\mathbf{X}$, $\mathbf{Y}$ and $\mathbf{S}$, with four training data (Fig.~\ref{f:process_path1}(a))-(Fig.~\ref{f:process_path1}(d)). They can be classified as either letter \texttt{A} or letter \texttt{B}. Images in the same class can have different variants. Take the letter \texttt{A} for instance. It can be in any font, either normal (Fig.~\ref{f:process_path1}(a)) or italic (Fig.~\ref{f:process_path1}(c)). No matter what the variant is, its pixels must follow $\mathfrak{D}_\texttt{A}$, the distribution for class \texttt{A}. After training, we can accurately classify any image that belongs to class \texttt{A} as long as its pixels follow the distribution $\mathfrak{D}_A$ (Fig.~\ref{f:process_path1}(e)). This truth also holds for images of letter \texttt{B}, whose pixels follow distribution $\mathfrak{D}_\texttt{B}$. The fluctuation data $\mathbf{S}$ reflects the random states of memory cells. In our visualized example, the pixels indicate the states of the memory cells. The patterns for fluctuation $\mathbf{S}$ also follow a certain distribution $\mathfrak{R}$, which can be learned during training. Using this enhanced dataset, the model can make correct predictions for in-memory deep learning because now it becomes aware of the fluctuations (Fig.~\ref{f:process_path1}(e))-(Fig.~\ref{f:process_path1}(f)).

As we integrate the fluctuation device data into the dataset, the model will not overfit during training. In Fig.~\ref{f:process_path2}, we visualized an example of training using only the dataset $\mathbf{X}$ and do not consider the device information. All pixels in the data $\mathbf{S}$ are in the center of the matrix, indicating the absence of device information (Fig.~\ref{f:process_path2}(a))-(Fig.~\ref{f:process_path2}(b)). During training, the model will overfit this static data $\mathbf{S}$ because it does not have any variant. As we can see from Fig.~\ref{f:process_path2}(c), the distribution learned by the model is only a straight line (orange), which is different from the real fluctuation of memory cells (purple). Therefore, the model will mis-classify the images. On the other hand, if we include the fluctuation data $\mathbf{S}$, the overfit can be avoided. As we can see from Fig.~\ref{f:process_path2}(e) and Fig.~\ref{f:process_path2}(f), the fluctuation of memory cells (purple) will follow the learned distribution (orange) so that the model can accurately classify the images.

We developed a method to integrate the fluctuation data $\mathbf{S}$ into the training process. To simplified this problem, we first decompose the computation of neural network model (Equation~(\ref{e:whole})) into several sub-tasks. For example, each element $y_{ij}$ in the output matrix $\mathbf{Y}$ can be computed independently using Equation~(\ref{e:workload}). Here vector $\mathbf{w}_i$ is the $i$-th row in weight matrix $\mathbf{W}$, vector $\mathbf{x}_j$ is the $j$-th column in the input matrix $\mathbf{X}$, and $b_{ij}$ is the element in the bias matrix $\mathbf{B}$ at $i$-th row and $j$-th column.
\begin{equation}
y_{ij}  = \mathbf{w}_i\mathbf{x}_j+b_{ij}
\label{e:workload}
\end{equation}
As we showed in Fig.~\ref{f:basic}, the weight we read from the EMT cell is unpredictable. Physically, the memory cell changes its status randomly, and the exact output value depends on the state of the memory cell when we are reading it. We denote $w_{ik}$ be the $k$-th element in vector $\mathbf{w}_i$. $\widetilde{w}_{ik}(j)$ is the sampled data when we read $w_{ik}$ from the memory cell, and multiply it with the input vector $\mathbf{x}_j$. Mathematically, we can express $\widetilde{w}_{ik}(j)$ as a polynomial, as shown in Equation~(\ref{e:combination}).
\begin{equation}
\widetilde{w}_{ik}(j) = \sum_{l=1}^m{r_l(w_{ik}, \rho)\cdot s_{ijkl}} 
\label{e:combination}
\end{equation}
We use $r_l(w_{ik},\rho)$ to denote the weight retrieved when the memory cell is at $l$-th state. It can be considered as a function of the pre-stored weight $w_{ik}$ and energy coefficient $\rho$. At any moment, each memory cell can only be in one state. Hence, we use a one-hot encoded vector [$s_{ijkl}$]$_{1\leq l\leq m}$ to indicate the state of the memory cell when $w_{ik}$ is sampled. The value of $s_{ijkl}$ is shown in Equation~(\ref{e:coeff}). For given indexes $i$, $j$, and $k$, if the corresponding memory cell is at $l_{0}$-th state, only $s_{ijkl_0}$ equals $1$ and all the other coefficients equal $0$.
\begin{equation}
s_{ijkl}= 
\begin{cases}
    1  ~& \text{if } l = l_{0}\\
    0  ~& \text{if } l \neq l_{0}
\end{cases}
\label{e:coeff}
\end{equation}

As we can see from Equation~(\ref{e:combination}), the sampled weight $\widetilde{w}_{ij}$ from the memory cell consists of two parts:

\begin{itemize}
\item The {{\textit{deterministic}}} parameter $r_l(w_{ik},\rho)$ is a function indicating the returned value from the memory cell storing weight $w_{ik}$ for a memory cell that is in the $l$-th state. We denote the matrix $[r_l(w_{ik}),\rho]$, as $\mathbf{r}(\mathbf{w}_i,\rho)$, shown in Equation~(\ref{e:matrix1}). $\mathbf{r}(\mathbf{w_i},\rho)$ can be considered a function of the weight vector $\mathbf{{w}}_i$ and the energy coefficient $\rho$.

\item The {{\textit{stochastic}}} parameter $s_{ijkl}$ is a random coefficient indicating whether the memory cell storing weight $w_{ik}$ is at $l$-th state when it is sampled and multiplied with the input vector $\mathbf{x}_j$. We denote matrix $[s_{ijkl}]$ as $\mathbf{S}_{ij}$, shown in Equation~(\ref{e:matrix2}). $\mathbf{S}_{ij}$ can be considered as a part of fluctuation data $\mathbf{S}$. 
\end{itemize}
\begin{align}
\mathbf{r}(\mathbf{w}_i,\rho) =& 
\begin{bmatrix}
r_1(w_{i1},\rho) &r_1(w_{i2},\rho) &\cdots &r_{1}(w_{in},\rho) \\
r_2(w_{i1},\rho) &r_2(w_{i2},\rho) &\cdots &r_{2}(w_{1n},\rho) \\
\vdots  &\vdots  &\ddots &\vdots  \\
r_m(w_{i1},\rho) &r_m(w_{i2},\rho) &\cdots &r_{m}(w_{in},\rho)
\end{bmatrix}\label{e:matrix1}\\
\mathbf{S}_{ij} = &
\begin{bmatrix}
s_{ij11} &s_{ij21} &\cdots &s_{ijn1} \\
s_{ij12} &s_{ij22} &\cdots &s_{ijn2} \\
\vdots  &\vdots  &\ddots &\vdots  \\
s_{ij1m} &s_{ij2m} &\cdots &s_{ijnm}
\end{bmatrix}
\label{e:matrix2}
\end{align}
We can now integrate the fluctuation data $\mathbf{S}$ into the training process for in-memory deep learning. Each element $y_{ij}$ in the output matrix $\mathbf{Y}$ can be calculated using Equation~(\ref{e:sample}). $y_{ij}$ is a function of both the deterministic and stochastic parameters. Here $\mathbf{\widetilde{w}}_i$ refers to the sampled weight vector read from the memory cells. For simplicity, we assume the bias $b_{ij}$ is a deterministic parameter. In some cases, the bias is also fluctuating. We can use the same method to separate deterministic and stochastic parameters for $b_{ij}$.
\begin{equation}
y_{ij}  = \mathbf{\widetilde{w}}_i\mathbf{x}_j + b_{ij} =\mathbf{1}\big(\mathbf{r}(\mathbf{w}_i,\rho)\circ \mathbf{S}_{ij}\big)\mathbf{x}_j + b_{ij}
\label{e:sample}
\end{equation}
The $\circ$ operator between $r(\mathbf{w}_i, \rho)$ and $\mathbf{S_{ij}}$ is the {\em Hadamard product}, i.e., element-wise product. The unit vector $\mathbf{1}$ is expressed in Equation~(\ref{e:unit}). We use it to sum up the entire column of the target matrix. 
\begin{equation}
\mathbf{1} = 
{\small \underbrace{\begin{bmatrix}
1 &1  &\cdots &1 \\
\end{bmatrix}}_{m}}
\label{e:unit}
\end{equation}

\subsection{Energy Regularization}

\begin{figure}[!t]
  \centering
  \includegraphics[width=3.5in] {Figure/EnergyMove.eps}
  \caption{Energy-saving from the energy regularization term. It can decrease both the energy coefficient $\rho$ and the mean of weight values $\overline{w}$.}
  \label{f:energy_term}
\end{figure}

Our second optimization technique adds an energy regularization term into the loss functions during training. From Equation~(\ref{e:sample}) we can infer that the loss function of the model $\mathcal{L}_0$ is a function of weights $\mathbf{w}$ and energy coefficient $\rho$. The target of our optimization technique is to find the optimal energy coefficient $\rho$ that can improve both the model accuracy and energy efficiency. However, it is not an easy task. We prefer a smaller $\rho$ for higher energy efficiency. However, as we see in Fig.~\ref{f:basic}, the higher $\rho$ causes a larger fluctuation amplitude of the weights, which results in accuracy loss. On the other hand, if we choose a larger coefficient $\rho$, the model accuracy would be less affected by the weight fluctuation, but the energy consumption becomes larger.

Our new loss function $\mathcal{L}$ is expressed in Equation~(\ref{e:energy_reg}). The first term $\mathcal{L}_0$ is the original loss function of the model, and the second term represents the energy consumption of the model. $\lambda$ is a hyper-parameter indicating the significance of the energy regularization term. $\alpha_t$ is a constant indicating the number of reading operations from the memory cell 
storing weight $w_t$. The overall loss function $\mathcal{L}$ can be considered as a function of $\textbf{w}$ ($w_t$ is the $t$-th element of $\textbf{w}$) and $\rho$, which are both trainable parameters. We can use any popular optimizer (such as SGD optimizer~\cite{bottou2012stochastic} or Adam optimizer~\cite{zhang2018improved}) to search for the optimal weight $\mathbf{w}$ and energy coefficient $\rho$.
\begin{equation}
\mathcal{L}(\textbf{w},\rho)= \mathcal{L}_0(\textbf{w}, \rho) + \lambda \sum_{t}{\alpha_t \rho |w_t|}
\label{e:energy_reg}
\end{equation}

During training, gradient descent will minimize the loss function $\mathcal{L}$. After optimization, both $\rho$ and $w_t$ will become smaller. We show this process in Fig.~\ref{f:energy_term}. With the help of the energy regularization term, we can improve both the model accuracy and energy efficiency simultaneously.

\subsection{Low-fluctuation Decomposition}

The third optimization technique is to decompose the computation process into multiple time steps. We can visualize the computation involved in Fig.~\ref{f:coding}. The input activation $x$ and the weight $w$ equals the length of the horizontal bar and the vertical bar, respectively. The computation result got from the memory cell equals the area of the square, whose two edges have the same length as the horizontal bar and the vertical bar. In the example of original computing (Fig.~\ref{f:coding}(a)), the lengths of the horizontal bar and vertical bar are 
seven ($x=7$) and one ($w=1$), respectively. The area of the output square is thus seven ($x\cdot w=7$).


Theoretically, we can express any input $x$ as a polynomial, as shown in Equation~(\ref{e:decom}). Here the fraction bit $\delta_p$ is binary data, which equals either $0$ or $1$, and $2^p$ is the scaling factor for that term. For example, if the input equals $7$, we can decompose it into three parts: $1\cdot 2^0$, $1\cdot 2^1$, and $1\cdot 2^2$. 
\begin{equation}
x= \sum{(\delta_p\cdot 2^p)}
\label{e:decom}
\end{equation}
In our low-fluctuation decomposed computation mechanism (Fig.~\ref{f:coding}(b)), we read each memory cell in multiple time steps instead of once. As Equation~(\ref{e:accum}) shows, at each time step, we only input the fraction bit $\delta_p$ to the memory cell, obtain the value of $\delta_p\cdot w$, and then scale the output from the memory cell by the factor $2^p$. Finally, we sum up all the results from each time step. In this example,  we use three steps to process input seven ($x=7$). The scaling factor of each time step is $2^0$, $2^1$, and $2^2$, respectively. As the weight is one ($w=1$), the final accumulated result is seven ($x\cdot w=7$), the same result as the original computing mechanism.
\begin{equation}
x\cdot w= \sum{(\delta_p\cdot w \cdot 2^p)}
\label{e:accum}
\end{equation}

\begin{figure}[!t]
  \centering
  \includegraphics[width=3.5in] {Figure/coding.eps}
  \caption{(a) Original computation mechanism; (b) low-fluctuation decomposed computation mechanism. The length of the bar denotes activation/weight value. The area of the square denotes their product. The solid/hollow yellow block represents positive/negative fluctuation.}
  \label{f:coding}
\end{figure}

As the name indicates, our low-fluctuation decomposition can alleviate the fluctuations of the memory cell effectively. We can explain this using Fig.~\ref{f:coding}, where we show the fluctuation amplitude in the yellow blocks. The block and hollow block denote positive and negative fluctuation amplitudes, respectively. As we can see from Fig.~\ref{f:coding}(b), using the decomposed computation mechanism,  the negative fluctuation amplitude (hollow block) at the third time step can partially average out the positive fluctuation amplitude at the second time step (solid block). Statistically, the accumulated fluctuations from the decomposed computation mechanism have a lower standard deviation than that of the original computation mechanism.

We can mathematically compare their standard deviations. Equation~(\ref{e:o_trad}) shows the standard deviation of the original computation mechanism, where $O_{\text{ori}}$ is the original output, and $\sigma(w)$ is the standard deviation of $w$ when we read it from the memory cell. Equation~(\ref{e:o_new}) shows the standard deviation of our low-fluctuation decomposed computation mechanism, where $O_{\text{new}}$ is the new output, and $w(p)$ is the weight sampled from the memory cell at $p$-th time step. Since reading memory cells can be considered as independent events, we have $\sigma(w(p)) = \sigma(w)$.
\begin{align}
\ \sigma(O_{\text{ori}}) =\sigma(x\cdot w) = \sum{2^{p}\delta_p}\cdot \sigma(w)
\label{e:o_trad}
\end{align}
\begin{equation}
\begin{aligned}
\sigma(O_{\text{new}})= \sigma\big(\sum{2^{p}\delta_pw(p)}\big) = &\sqrt{\sum{2^{2p}\delta^2_p\sigma^2\big (w(p)\big )}} \\
=& \sqrt{\sum{2^{2p}\delta^2_p}}\cdot \sigma(w)\\
<& \sqrt{(\sum{2^{p}\delta_p})^2}\cdot \sigma(w)
\label{e:o_new}
\end{aligned}
\end{equation}
We can infer from Equation~(\ref{e:o_trad}) and~(\ref{e:o_new}) that our decomposed computation result has a lower standard deviation than the original computation result, leading to high model accuracy (Equation~(\ref{e:o_compare})).
\begin{align}
\sigma(O_{\text{new}}) < \sigma(O_{ori})  
\label{e:o_compare}
\end{align}

Our low-fluctuation decomposition can also improve energy efficiency. To prove this, we express the energy consumption of the two computation mechanisms in Equation~(\ref{e:e_list}). From the equation, we can infer that our decomposed computation mechanism consumes less energy than the original one (Equation~(\ref{e:e_compare})).
\begin{align}
&E(O_{\text{ori}}) = \rho\cdot x \ \ \ \ \ \  E(O_{\text{new}}) = \rho \sum \delta_p \label{e:e_list} \\
&E(O_{\text{new}}) < E(O_{\text{ori}})  
\label{e:e_compare}
\end{align}


\begin{figure*}[!t]
  \centering
  \includegraphics[width=7in] {Figure/ListAcc.eps}
  \caption{The comparison between our proposed optimization solutions and the traditional optimizer. We test models on the CIFAR-10 dataset. The first row of sub-figures shows the accuracies in a zoomed range, and the second row of sub-figures shows accuracies in the full range.}
  \label{f:performance}
\end{figure*}

\subsection{Convergence of the Training Method}

We shall now mathematically prove the convergence of our training method. 
Equation~(\ref{e:whole2}) shows the basic relationship between image data $\mathbf{X}$ and label data $\mathbf{Y}$ in the traditional deep learning network. The output matrix $\mathbf{Y}\in \mathcal{Y}$ is a function $f$ of the input matrix $\mathbf{X}\in \mathcal{X}$. Here $\mathcal{X}$ and $\mathcal{Y}$ denote the space of $\mathbf{X}$ and $\mathbf{Y}$, respectively. 
\begin{equation}
\mathbf{Y}  = f(\mathbf{X})% = \mathbf{W}\mathbf{X}+\mathbf{B} 
\label{e:whole2}
\end{equation}
For in-memory deep learning applications, the computation becomes unpredictable because of weight fluctuation. As can be seen from Equation~(\ref{e:sample}), output $y_{ij}$ is a function of both the input $\mathbf{x}_j$ and the fluctuation data $\mathbf{S}_{ij}$. To generalize this, the output matrix $\mathbf{Y}\in \mathcal{Y}$ can be defined a function $\widetilde{f}$ of the input matrix $\mathbf{X}\in \mathcal{X}$ and the fluctuation data $\mathbf{S}\in \mathcal{S}$, shown in Equation~(\ref{e:PIM}). Here $\mathcal{S}$ denotes the space of $\mathbf{S}$.
\begin{equation}
\mathbf{Y} = \widetilde{f}(\mathbf{X, S})
\label{e:PIM}
\end{equation}
We define a new data $\widetilde{\mathbf{Z}}\in \widetilde{\mathcal{Z}}$ as the combination of images $\mathbf{X}$, labels $\mathbf{Y}$ and fluctuations $\mathbf{S}$.
Here the space $\widetilde{\mathcal{Z}}$ is the combination of $\mathcal{X}$, $\mathcal{Y}$, and  $\mathcal{S}$. Since  $\mathcal{Z}$ is the combination of $\mathcal{X}$ and $\mathcal{Y}$, we can express the space $\widetilde{\mathcal{Z}}$ as the combination of the spaces $\mathcal{Z}$ and $\mathcal{S}$. We show their relationships in Equation~(\ref{e:parameter1})-(\ref{e:parameter2}).
\begin{align}
\widetilde{\mathbf{Z}}=&(\mathbf{X}, \mathbf{Y}, \mathbf{S})\label{e:parameter1}\\
\widetilde{\mathcal{Z}}=&\mathcal{X}\times \mathcal{Y}\times\mathcal{S}= \mathcal{Z}\times \mathcal{S}
\label{e:parameter2}
\end{align}
Given the fact that $\mathcal{Z}$ follows distribution $\mathfrak{D}$, while $\mathcal{S}$ follows distribution $\mathfrak{R}$, we can infer that the space $\widetilde{\mathcal{Z}}$ (the combination of $\mathcal{Z}$ and $\mathcal{S}$) must follow a distribution $\widetilde{\mathfrak{D}}$ (the combination of $\mathfrak{D}$ and $\mathfrak{R}$). We show this in Equation~(\ref{e:new_dist}).
\begin{equation}
       \mathcal{Z}\sim\mathfrak{D}, \mathcal{S}\sim\mathfrak{R} 
\ \ \ \Rightarrow
\ \ \ \widetilde{\mathcal{Z}}\sim \widetilde{\mathfrak{D}}
\label{e:new_dist}
\end{equation}

Finally, our proposed training process for in-memory deep learning models can be concluded as follows: given a function $\widetilde{f}$ in the space $\widetilde{\mathcal{F}}$, and a loss function $\mathcal{L}:\widetilde{\mathcal{F}} \times \widetilde{\mathcal{Z}}\rightarrow \mathbb{R}$, we would like to find $\widetilde{f}\in \widetilde{\mathcal{F}}$ that can minimize the risk, \textit{i.e.}, the expectation of the loss function (Equation~(\ref{e:risk})).
\begin{equation}
R[\widetilde{f}]=\mathbb{E}_{\widetilde{z}\sim \widetilde{\mathfrak{D}}}\big[\mathcal{L}(\widetilde{f},\widetilde{z})\big]
\label{e:risk}
\end{equation}

Thus, we convert the convergence problem of the new training process for in-memory computing into the convergence problem for regular neural networks (see Section~\ref{s:risk}). Finding the optimal function is well studied, and we can use various existing optimizers, such as SGD or Adam, to find the optimal function $\widetilde{f}$.



\section{Experiments}
\label{s5}

We trained the models on the Pytorch platform and evaluate the energy consumption in an in-memory deep learning simulation platform~\cite{lee2019system}~\cite{wang2020ncpower}. To accelerate the training process, we start each experiment from a well-trained model with full-precision weights~\cite{pytorchzoo} and then fine-tune the model by applying our proposed optimizations. During fine-tuning, we quantize both the activations and weights. To form the device-enhance dataset, we fetch the images and labels from the regular datasets (such as CIFAR-10 and ImageNet) as data $\textbf{X}$ and data $\textbf{Y}$, respectively. The fluctuation data $\textbf{S}$ are obtained from state-of-the-art device models~\cite{ielmini2010resistance}. We use a workstation with an Nvidia 2080TI graphic card to train the model. For the CIFAR-10/ImageNet dataset, each experiment can finish in about an hour/an entire day.

We proposed three solutions, denoted as `A',  `A+B', and `A+B+C'. As shown in Fig.~\ref{f:overview}, the notations A, B, C stand for the device-enhanced dataset, energy regularization, and low-fluctuation decomposition, respectively. Solution A uses only the first technique; solution A+B applies the first two; solution A+B+C combines all three. We evaluate popular models including VGG-16, ResNet-18/34 and MobileNet. VGG-16 is a regular deep neural network with only $3\times 3$ kernels. ResNet-18/34 are popular models that achieve competitive accuracy by adding residual links between layers. MobileNet is a small-size model that achieves high efficiency due to its special depthwise layer. We compare our work with three state-of-the-art solutions: binarized encoding~\cite{zhu2019configurable}, weight scaling~\cite{ielmini2010resistance}, and fluctuation compensation~\cite{wan2020voltage} as described in detail in Section~\ref{s2}.

\begin{figure*}[!t]
  \centering
  \includegraphics[width=7in] {Figure/RobInfo.eps}
  \caption{The energy comparison between our proposed solutions and the state-of-the-art, under three levels of fluctuation intensity. We test models on the ImageNet dataset. Both our solutions and the state-of-the-art are free to tune the energy coefficient $\rho$.}
  \label{f:rob}
\end{figure*}




\begin{figure*}[!t]
  \centering
  \includegraphics[width=7in] {Figure/SOTA.eps}
  \caption{The accuracy comparison between our proposed solutions and the state-of-the-art. We test models on the ImageNet dataset. The solid bar and hollow bar denote the top-1 accuracy and top-5 accuracy, respectively. The dash line denotes baseline model accuracy on GPU.}
  \label{f:sota}
\end{figure*}

%\setlength{\parskip}{0.1 em}

\subsection{Ablation Study of Proposed Techniques}

Models optimized by our proposed methods have much higher accuracy than the model trained by the traditional optimizer. In Fig.~\ref{f:performance}, we show the accuracy achieved by solution A, solution A+B, and solution A+B+C under different energy budgets. As a reference, we also give the model accuracy trained by the traditional optimizer. As we can see from the figure, at 16 $\mu$J energy budget, the accuracy of solution A+B+C is very close to baseline accuracy (shown as dashed lines in the top sub-figures). On the other hand, the traditional optimizer exhibits relatively low accuracy due to its unawareness of memory fluctuation.

We can see that when the energy budget is decreased, models trained using the traditional optimizer show a dramatic decrease in accuracy. On the contrary, our solution A+B+C can achieve high model accuracy even if we reduce the energy budget. Even just using Solutions A and A+B is enough to maintain a relatively high accuracy, only to be outperformed by Solution A+B+C. This observation further proves the effectiveness of our proposed three techniques on in-memory computing.
% WWF - redundant statements
%: device-enhanced dataset (A), energy regularization (B), and low-fluctuation decomposition (C). From solution A to solution A+B to solution A+B+C, the more techniques we applied during training, the higher model accuracy we can achieve for in-memory deep learning.

We can also see that under 16 $\mu$J energy consumption, the ResNet-18 trained by the traditional optimizer shows much lower accuracy than the VGG-16. By using our solution A+B+C, ResNet-18 can fully recover the accuracy and thus outperforms VGG-16. This experiment shows that MobileNet is not suitable for in-memory deep learning. Under the same energy budget, MobileNet shows lower accuracy than VGG-16 and ResNet-18. We attribute this to its depthwise layer. When we compute a regular convolution layer, the system reads hundreds of memory cells at once. However, to process the depthwise layer, it only read nine memory cells at once. Therefore, a large portion of the energy is consumed in the peripheral circuits, causing a significant amount of energy overhead.

\setlength{\tabcolsep}{4pt}

\renewcommand{\arraystretch}{1.26}

\begin{table*}[!t]
\caption{Comparison between our solutions and the state-of-the-art on the CIFAR-10 dataset}
\resizebox{1.00\linewidth}{!}{
\begin{tabular}{l  c | c    c  c | c c c   | c c  c}
\hline	
\hline	
& &\multicolumn{3}{c|}{0\% accuracy drop}&\multicolumn{3}{c|}{1\% accuracy drop} &\multicolumn{3}{c}{2\% accuracy drop}\\
\hline											
VGG-16 (93.6\% Acc.)	&Ref.	&Energy ($\mu$J)	&\#Cells	&Delay ($\mu$S)	&Energy ($\mu$J)	&\#Cells	&Delay ($\mu$S)	&Energy ($\mu$J)	&\#Cells	&Delay ($\mu$S)	\\
\hline											
Binarized Encoding	&\cite{zhu2019configurable}	&378	&74M	&2.8	&135	&74M	&2.8	&94	&74M	&2.8	\\
Weight Scaling	&\cite{ielmini2010resistance}	&444	&15M	&2.8	&78	&15M	&2.8	&49	&15M	&2.8	\\
Fluctuation Compensation	&\cite{wan2020voltage}	&1091	&15M	&14	&157	&15M	&14	&82	&15M	&14	\\
\textbf{Ours (A+B)}	&	&\textbf{36}	&\textbf{15M}	&\textbf{2.8}	&\textbf{16}	&\textbf{15M}	&\textbf{2.8}	&\textbf{11}	&\textbf{15M}	&\textbf{2.8}	\\
\textbf{Ours (A+B+C)}	&	&\textbf{4.1}	&\textbf{15M}	&\textbf{14}	&\textbf{1.0}	&\textbf{15M}	&\textbf{14}	&\textbf{0.5}	&\textbf{15M}	&\textbf{14}	\\
\hline											
ResNet-18 (95.2\% Acc.)	&Ref.	&Energy ($\mu$J)	&\#Cells	&Delay ($\mu$S)	&Energy ($\mu$J)	&\#Cells	&Delay ($\mu$S)	&Energy ($\mu$J)	&\#Cells	&Delay ($\mu$S)	\\
\hline											
Binarized Encoding	&\cite{zhu2019configurable}	&876	&56M	&6.8	&389	&56M	&6.8	&286	&56M	&6.8	\\
Weight Scaling	&\cite{ielmini2010resistance}	&1127	&11M	&6.8	&209	&11M	&6.8	&158	&11M	&6.8	\\
Fluctuation Compensation	&\cite{wan2020voltage}	&2217	&11M	&34	&474	&11M	&34	&347	&11M	&34	\\
\textbf{Ours (A+B)}	&	&\textbf{83}	&\textbf{11M}	&\textbf{6.8}	&\textbf{22}	&\textbf{11M}	&\textbf{6.8}	&\textbf{10}	&\textbf{11M}	&\textbf{6.8}	\\
\textbf{Ours (A+B+C)}	&	&\textbf{6.9}	&\textbf{11M}	&\textbf{34}	&\textbf{1.1}	&\textbf{11M}	&\textbf{34}	&\textbf{0.7}	&\textbf{11M}	&\textbf{34}	\\
\hline											
MobileNet (91.3\% Acc.)	&Ref.	&Energy ($\mu$J)	&\#Cells	&Delay ($\mu$S)	&Energy ($\mu$J)	&\#Cells	&Delay ($\mu$S)	&Energy ($\mu$J)	&\#Cells	&Delay ($\mu$S)	\\
\hline											
Binarized Encoding	&\cite{zhu2019configurable}	&392	&16M	&4.6	&81	&16M	&4.6	&62	&16M	&4.6	\\
Weight Scaling	&\cite{ielmini2010resistance}	&232	&3.2M	&4.6	&57	&3.2M	&4.6	&42	&3.2M	&4.6	\\
Fluctuation Compensation	&\cite{wan2020voltage}	&659	&3.2M	&23	&126	&3.2M	&23	&91	&3.2M	&23	\\
\textbf{Ours (A+B)}	&	&\textbf{75}	&\textbf{3.2M}	&\textbf{4.6}	&\textbf{23}	&\textbf{3.2M}	&\textbf{4.6}	&\textbf{13}	&\textbf{3.2M}	&\textbf{4.6}	\\
\textbf{Ours (A+B+C)}	&	&\textbf{12.2}	&\textbf{3.2M}	&\textbf{23}	&\textbf{1.8}	&\textbf{3.2M}	&\textbf{23}	&\textbf{1.3}	&\textbf{3.2M}	&\textbf{23}	\\
\hline											
\hline																																																
\label{t:sota1}
\end{tabular}
}
\end{table*}

%\setlength{\tabcolsep}{4pt}

\begin{table*}[!t]
\caption{Comparison between our solutions and the state-of-the-art on the ImageNet dataset}
\resizebox{1.00\linewidth}{!}{
\begin{tabular}{l  c | c    c  c | c c c   | c c  c}
\hline	
\hline	
& &\multicolumn{3}{c|}{0\% accuracy drop}&\multicolumn{3}{c|}{1\% accuracy drop} &\multicolumn{3}{c}{2\% accuracy drop}\\
\hline											
ResNet-18(69.8\% Acc.)	&Ref.	&Energy ($\mu$J)	&\#Cells	&Delay ($\mu$S)	&Energy ($\mu$J)	&\#Cells	&Delay ($\mu$S)	&Energy ($\mu$J)	&\#Cells	&Delay ($\mu$S)	\\
\hline											
Binarized Encoding	&\cite{zhu2019configurable}	&23k (\textcolor{red}{-0.4\%})	&58M	&151	&2338	&58M	&151	&1336	&58M	&151	\\
Weight Scaling	&\cite{ielmini2010resistance}	&21k (\textcolor{red}{-0.3\%})	&12M	&151	&3544	&12M	&151	&1933	&12M	&151	\\
Fluctuation Compensation	&\cite{wan2020voltage}	&71k (\textcolor{red}{-0.3\%})	&12M	&756	&8505	&12M	&756	&4725	&12M	&756	\\
\textbf{Ours (A+B)}	&	&\textbf{1951}	&\textbf{12M}	&\textbf{151}	&\textbf{897}	&\textbf{12M}	&\textbf{151}	&\textbf{659}	&\textbf{12M}	&\textbf{151}	\\
\textbf{Ours (A+B+C)}	&	&\textbf{142}	&\textbf{12M}	&\textbf{756}	&\textbf{71}	&\textbf{12M}	&\textbf{756}	&\textbf{54}	&\textbf{12M}	&\textbf{756}	\\
\hline											
ResNet-34 (73.3\% Acc.)	&Ref.	&Energy ($\mu$J)	&\#Cells	&Delay ($\mu$S)	&Energy ($\mu$J)	&\#Cells	&Delay ($\mu$S)	&Energy ($\mu$J)	&\#Cells	&Delay ($\mu$S)	\\
\hline											
Binarized Encoding	&\cite{zhu2019configurable}	&28k (\textcolor{red}{-0.2\%})	&109M	&207	&2844	&109M	&207	&1778	&109M	&207	\\
Weight Scaling	&\cite{ielmini2010resistance}	&25k (\textcolor{red}{-0.1\%})	&22M	&207	&3302	&22M	&207	&2154	&22M	&207	\\
Fluctuation Compensation	&\cite{wan2020voltage}	&83k (\textcolor{red}{-0.1\%})	&22M	&1033	&7990	&22M	&1033	&4857	&22M	&1033	\\
\textbf{Ours (A+B)}	&	&\textbf{2496}	&\textbf{22M}	&\textbf{207}	&\textbf{1044}	&\textbf{22M}	&\textbf{207}	&\textbf{729}	&\textbf{22M}	&\textbf{207}	\\
\textbf{Ours (A+B+C)}	&	&\textbf{168}	&\textbf{22M}	&\textbf{1033}	&\textbf{90}	&\textbf{22M}	&\textbf{1033}	&\textbf{62}	&\textbf{22M}	&\textbf{1033}	\\
\hline											
\hline											
\label{t:sota2}
\end{tabular}
}
\end{table*}



\subsection{Robustness to Different Devices}

Today, academia and industry have developed various types of  EMT cells, which have different levels of fluctuation intensity. Hence, it is necessary to prove the robustness of our solutions under any level of fluctuation intensity. In Fig.~\ref{f:rob}, we test our solutions and the state-of-the-art under three intensity levels~\cite{raghavan2013microscopic}:  weak,  normal, and strong. The experiment is conducted on the ImageNet dataset using two ResNet models. All solutions, including the state-of-the-art, are free to tune the energy coefficient $\rho$. We compare the energy consumption when the model achieves its maximum accuracy. Noted that on the ImageNet dataset, our solutions can achieve the same accuracy as the baseline model running on GPU, where the state-of-the-art cannot.

The results show the robustness of our solutions. At any level of fluctuation intensity, our solution has almost the same performance on energy reduction. When fluctuation intensity is increased, both ours and the state-of-the-art solutions prefer a higher energy coefficient $\rho$ to maximize the model accuracy, resulting in a higher energy consumption. However, our solutions still outperform the state-of-the-art. Our solutions A+B and A+B+C shows one and two orders of magnitude energy reduction, respectively. From this point onwards, we shall assume the fluctuation intensity to be normal. Results under any other intensity level follow a similar trend.



\subsection{Verification of the Optimization Solutions}

In Fig.~\ref{f:sota}, we verified our optimization solution by testing two ResNet models on the ImageNet dataset. The dashed line in the figure shows the baseline accuracy. We defined it as the highest accuracy we can achieve on GPU. We also list the accuracy of the state-of-the-art for comparison. Among all, our solution A+B+C has the highest top-1 and top-5 accuracies, which are the same as the baseline accuracies. The accuracy of solution A+B also shows higher accuracy than the state-of-the-art. We can observe a small accuracy loss under a smaller energy budget. By contrast, models optimized by the state-of-the-art have significant accuracy losses. For example, we can see at least 0.9\% and 0.8\% top-1 accuracy losses of the ResNet-18 and ResNet-34, respectively. We can also see that the ResNet-18 on the ImageNet dataset consumes more energy than the same model on the CIFAR-10 dataset. It is because ImageNet has a larger image size than CIFAR-10.

\subsection{Holistic Comparison with the State-of-the-Art}

Our proposed solutions have better performance not only on energy reduction but also reduce cost and latencies. In Table~\ref{t:sota1} and Table~\ref{t:sota2}, we give a holistic comparison of our solutions with the state-of-the-art on energy consumption, the number of cells, and latency, under the same accuracy loss. We test various models on the CIFAR-10 and the ImageNet datasets. From the tables, our method has the lowest energy consumption, the least number of cells, and the shorted latency. Specifically, solution A+B shows one order of magnitude improvement in energy consumption, and solution A+B+C can achieve two orders of magnitude improvement, compared with the state-of-the-art. However, one limitation of Solution A+B+C is that its latency is longer because the low-fluctuation decomposed computation takes time to accumulate the results. The trade-off between energy consumption and latency depends on the specific application. Therefore, we list the results of solution A+B and solution A+B+C. Developers can choose one of them based on their demands.

Another observation is that our solutions are the only ones that can fully recover the accuracy loss on the ImageNet datasets. As can be seen from the third column of Table~\ref{t:sota2}, all the state-of-the-art solutions are unable attain no accuracy loss (we mark their actual accuracy drops in red text). In a neural network, some parameters are vital to achieving high model accuracy. However, the state-of-the-art usually applies a general optimization rule to all the model parameters. After the optimization, those important parameters still have relatively large fluctuations, which constraints the recovery of model accuracy. Unlike the state-of-the-art, our solutions can automatically identify those significant parameters and minimize their fluctuations. %Therefore, we can effectively improve the model accuracy and energy efficiency.

\section{Conclusion}
\label{s6}

In-memory deep learning has a promising future in the AI industry because of its high energy efficiency over traditional deep learning. This is even more so if the potential of emerging memory technology (EMT) especially in analog computing mode is used. Unfortunately, one of the major limitations of EMT is the intrinsic instability of EMT cells, which can cause a significant loss in accuracy. On the other hand, falling back on a digital mode of operation will erode the potential gains. In this work, we propose three optimization techniques that can fully recover the accuracy of EMT-based analog in-memory deep learning models while minimizing their energy consumption: They include the device-enhanced dataset, energy regularization, and low-fluctuation decomposition. Based on the experiment results, we offer two solutions. Developers can either apply the first two optimization techniques or apply all three to the target model. Both solutions can achieve higher accuracy than their state-of-the-art counterparts. The first solution shows at least one order of magnitude improvement in energy efficiency, with the least hardware cost and latency. The second solution further improves energy efficiency by another order of magnitude at the cost of higher latency.





\bibliography{myref}
\bibliographystyle{IEEEtran}

\end{document}


%It can be proved that the training process can finally converge, and we able to find the optimal function $\widetilde{f}$. 

%The STE (straight-through estimator) method is used to avoid zero gradient during backpropagation.
%(noted that Fig.~\ref{f:performance} and Fig.~\ref{f:sota} have the same unit for energy consumption). There are two reasons. First, we are testing the energy consumption of the whole validation set, and the ImageNet has a much larger validation set than the CIFAR-10. The number of images in the ImageNet validation set is 5X the size of the CIFAR-10. Second, models on the CIFAR-10 have more redundant parameters because of the simplicity of the dataset. The optimizer will find a low energy coefficient for those redundant weights to minimize energy consumption. As a result, the model for the CIFAR-10 consumes less energy than the model for the ImageNet. 
%For example, if we assume $O=P\circ Q$, then we have $o_{ij}=p_{ij}q_{ij}$. It has $m$ elements, all equaling $1$.
%The characteristic of in-memory computing device is quite different from the traditional device, where the output value of memory cells $w$ is $100$\% fixed. 

% In the the hardware for in-memory processing, we can use a column of memory cells (Fig.X) to compute $y_{ij}$. During the setting up stage, each memory cell in the column is tuned so that the internal value of the $k$-th memory cell $C_k$ equals weight $w_{ik}$ (i.e. the $k$-th element in vector $\mathbf{w}_i$).
% During computation, the input of the $k$-th resistor $V_k$ equals $x_{kj}$ (i.e. the $k$-th element in vector $\mathbf{x}_j$). Hence, by physical law, the output of the $k$-th memory cell equals $w_{ik}x_{kj}$. Therefore, if we add an extra data equals the bias $b_{ij}$, the value of $y_{ij}$ can be obtained by reading the accumulated output $I$ from the end of the column. Since elements in the $i$-th row of the output matrix $\mathbf{Y}$ share the same weight vector $\mathbf{w}_i$, they can be calculated using the same column of memory cells.
% Hence, the weight fclucations are compentsed.
% to compenets the flciations.
% Theortically This method could average out the flucations and thus allevaite the flicaiotns~\cite{shim2020two}.
% To compenetat the fliucations caused by wirtiing. 
% Thi emthod is very straighforwad 
% Insteadf of reading the memory cell once,
% the noises is stattically counted/observed, and then we can use the statical result to compenet the noise.
% During testing of memory cell,  it is mendatory to read the memory cell for many times to see the real dataa  clalsical during reading while  meansuement of noise is  statistically measured the results  This is a very comon way to  
% d to compenet the writing noise of in-memoru comptuing.
% Several work has been propsed to compented the writing fleucaitonss
% J ~\cite{zhang2020reliable}
% To compements the flcaitions of more bgenral cases.
% To to compennted  genral noeise.. 
% xxx et al propseod to appluthe 
% for example,  The resilt is the combiatnion of diffeirent models.
% xxx et al proposed the way to repeat several times and average out the noises~\cite{wan2020voltage}


% %Usinh this model, the weigth scaling could xxx
% %and Then the trade-off becaome a maht problme 
% %the only consertn is the energy consumptin is also increased.
% %A more genral way s to make the trade off between the resistance and fliacaotn ampliftue.
% $a math problem incfease the resistance of the memroy cells, which reduce the fliuation amplideute.
% %then a more wise way is to use low xx et al. reduc the noise ampltoiyde by direcley lower the resistrnce of cells..
% %the devation reduaiton of .weight can be sovte{ielmini2010resistance}
% % xxx et al ,proposed a loop cicut to observ%  by stataicall measure the noise and then compenste the noises.% for gausnanl distrucbtin cit% for nature oaper write noisses cite loop back circuits nwrite nosied cite% the above work 

%Here $s$ is a random variable indicating the state of the memory cell, which equals either $0$ or $1$ at the any given moment.
%make the model be adpavie to the randmosnm of memoruy cells and thus  This can help model to avaodi any ovefit on the noise, 
%In the literures, tehre are waus to compmennt tjhe noise, for examl,e using more bits.

%We call such  flcuaitons of output results as RTN (Random Telegraph) nosirs.

%As we can see in this example, for in-meoru comptuing device,
%at any given value of $\rho$, the memory cell has two possbileb the effective %wieght: w and.  

%the memoryc ell has two staate.  
%at the first staate the effective weight is xw and at the sendod state the effecitve weught becaome  x

%our models is close to the baseline accuracy runing on GPU. both for top-1 and top-5 accuracy. Weiht less energy constrained. only slightly accruacy drip is observed for method A+B.

%
%The reasone is that the coding schems and we are better to compenets the noises and avtively optimizatoin of.energy cosnumption. 
%
%not only on energy but also on crlls and latency 
%the current will acculmate on the RRAM column and 

%based on that ~\cite{chen201865nm}igitally.~\cite{zhu2019configurable} They use muckltiple bits wchih %There are th.the first catgeory is call dinmgle bit  which is to ignor th enoise of the ;;ed singale bit, considered it as either 1 or 0. which can elmeintate most of the noises. which measn use one memory cell for one bit of informtion.. or evuer things. d RRAM is very effiet iun rtn as a 
%  this can effective reduce the enrgy consumptino of treaditnal deep leraning becuse  each computation operations cosumes almost the same amount of enregy.  Hence,the  priu awways  weigths only contributed a small protiion to the toaal energy consu7mptin. 
% Unlike tradintl deep learning, whereforof
% the difccult to futher reduce ites nery cosnumptni.  previous model compression technologieos  such as prining and qunatizaiotn~\cite{1} beciems ineffecive to in-mempru depe lerning . 
% netwrk which also consumes less energy consumptin. Threifer, oruning is effectve to energy redcutions.
% those insignifcant weight also consumes little energy, hence pruning is no longer effecitvley to the mode. and other as each mmeory cell is no long realted ti the opreciosn of the wiegth, so quntion is also no long er usable. 
%pruning and quantization doest not work
%the RRAM  columen is able to generate ouptut data yi instantly.

%then only the coeficent corespsings Here sijk means that the memeory cell for wiegh ij is in Sth state when it is samueled and if the memroy is in the sth state.% If the resitor is on the m t state, te % is one-hot enc ded. which means that for given % the resistor can have m states, f(w)is the data we read when we read wij from nthstates,. each possiblely,e r(w) is the fucntion of w,  the possibleity factor a is a one-hot data,  for which only wo,lonly one aij equals 1. So the value aij is etiher 1 or 0, and it is totally randomws due to theRTN nodies.e
%On one hand, as we see from Equation x, space $\mathcal{Z}$ follows distribution $\mathfrak{D}$. and  On the other hand, due to the characteristics of memory cells, space $\mathcal{S}$ follows distribution $\mathfrak{R}$. Therefore, we ca
%The problem we are facing in RRAM circuist is that Every time we read the weigh vecro wi  from the machine, it is not exaxcly what we want.i., instead of reading w from the RRAm reisistors, what we read is w~, which stnad for the fulcuated withhs,.
%,  given the weight and biasThe weight Here, the wiehm vecor w is written in into the RRAM reisosptrs in that column,  each element in wj correpsonds is mapped to each RRAM reisitor.
% The nx1 vecor y is a function of the nxk matrix X. The nx1 weigthr vector W, and nx1 bias vector b are both strainable Parametres 
% For conviltion layers, equation (1) is able to compute one output channel. n equlas the number of pixels of each output channel., and m equlas squaired kersize multipleid by the number of input channels.  For linear layers, equation 1 is able to compute one  output pixel n equals 1, and m euqals the number of input features.

%compared with classical coding scmes, our new schems acheive more less, In the hardware for inmemoruy conmputing, this mean less effors applied to memory cells, which is the energy consumpotion,. Noted that  thuis can slow dow the processing of cinmouatiun, sayk if the actiona is quantizzed into n bits, then we need X times  of time steps to do the computatino,
%ALthounght it takes N cycles to finsih, this can be companet4ed by two factors, first, these re[etiatnce happend ein the the same memrou cell] as the same weight ,we save a lot of resource.s , so compared with mtheod, in , we are still much faster gvien the same number of memrou cells. Second, as the weight is read mulcitled time, the noise can be compenstaed wihch makes it more stable.

%the fist part coressponds to one bit in the binary format..and the second part coresprodhs to the scale factor at the time step.


%Each time when we read the memory cessl, the input vector correpsonds to the one digit in the binary code.
%the input of the mmeory cell is 1, depending on the valyue of th ebits if the bit is 0, then the input of the mmeroy cell at this mmoment is 0.

%To know how it works in each time, , we need to convert the input vector into binary format,In this examle, the input 7 can   If 1 is 

 

%the input is first encodined into binary fomrat, hence, 7 can be encoded as "111", Each time, 
% that's all folks




%for simplicity, In equation x, we only show the case when the indexs of t e,i, Aij 
%When we do computing for in-emeru comptuing. Not only the multiplu peraiatoim,, but also the add operatios are also imebeded ito the read otpiaitn.s 


% we can also use the graph to exaplina why the traidiaotnl training does  in the PIM it will very obvious caluase accurayc with the ignoarance of nouises. .In figx, and fig . IN a and b we can see that the noise data is fixed,the data S, the picels  shown in orignal color, is alibned in the center as a vertical line, indicating that theery are all in the same defalt state , and they do not follow any distritons, During traing, the models will fit ont the noises. It will learnng that any noise that the nosei is a deterstimci value, As a result, during inferece the model the real noise from the meory cessl , shown in purpel color. there froe, it cannot accuracy classify the image.(c)
% lt optimizaers serach for the optmial result.
% to optimize for the energy efficencyl. 
% %The energy consumption of each memory cell is the product of the absolute values of weights stored in that memory cell and the energy coefficient, denoted as $\rho$. 
%  the flcuatioation amplidet of memroy cells is positively correlated to the energy coefficient
% Mathematically, with a higher e, the los fucitons.  
% % Thereetoricall, A larger energu-to-weight ratio means larger flucation amplfited, . as we see in figures x, a large energy to ratio. 
% Therefore, the value of $\rho$ is difficult to choose. 
% %It equals the total number of read operations from the associated memory cell during inference.

% \begin{tabularx}
% \begin{equation}
% A \sim \mathcal{T}
% \end{equation}&
% \begin{equation}
% A \sim \mathcal{T}
% \end{equation}
% \end{tabularx}
%given the dataset and models, we 
%the input is dataset  and models, we have a loss funciton, then
% \begin{equation}
% \mathbf{y}  = \widetilde{g}(\mathbf{\widetilde{X}})
% \label{1}
% \end{equation}
%opitmal vaule off e, and also the  optimal combiatnion of weigths. which shows the minmimal enenrgy consumptions of the model at the same time maintain the accuracy.%on, which implems the efforst we want to reduce during training. w reprensets the weight.We assume it%$ \alpha_t$ is the value raleted to the inptu data X.% For a given model This values is a constanct parameter t% needed in second column of first page if using\IEEEpubid%\IEEEpubidadjcol% The idea is that we% IN calssical encoding, the read effort to read the memroy cell is protopoitnal to the strgentg of the singal. To make sure enough resolutions, we nend 16 voltages to support. % For effcienct encoding, the % Hnecc, IN staed of reading the cell in one time, instead, we read by N timew, assume the activations is a N bit data.
%One more imporntat parameter is which we want it to c be a trainable paratemr whchic we will obtain from trainbg.%on the other hand, high weight measn higher energy consumtpion.%$x_t$ is the scale factor for the memory cell. A larger scale factor means large enrgy cosnutiopn but the memroy woucl dbe  it is less vunlable to noises. On the contray, a smaller scel facotr is perfenreced if we want to acheive high energy effciency at the cost of higher noise impact. %$\eta_t$ and $w$ are trainable paratemrs whose optimcal value will be automatically found by the optimizer during train

% orange one is the color for non noise
% purpural color indiation the interstics randowm noise

% it natually takes both image and nose for inferece, if we only train the iamge.s，when we do the infrece, a noise will blkur the images and cause accuracy decratein
% If we take image X and nosie S into the infrece, we are easylly see that the nosiue S will cause signficant accracy loss to the model because tjhese nosise pattern will interferecne the predication of models. Imaging noise S is 

% Our goal is to let the neuon models learn such kind of distriubtuin  so that dyrub ubferece tge models can avoid the interfecece of nouses from the unstaablizedm emory cells. 




% During inferce, the flucaetin of weighs from the memory is already known by the mmodel so that high accruay can be achieved

% so that any randomw behavious from the memoruy cells can be 

% In other perspervet, we  an thing image x and noise S a single images, a more is able to learning image X and noises concurrently, the model accuracy cna be matinaed,                                                                                                    Physically
% During traing, the noise and the input S are trained togehter this            


%If the whole models is unstable, we would easy to observe accuracy drops.
. 
%know z=XxY ,as distinatin of D, so that z as a distytion of D

% \begin{tabularx}{0.5\textwidth}{@{}XXX@{}}
%   \begin{equation}
% A \sim \mathcal{T}
%   \end{equation} &
%   \begin{equation}
% Z \sim \mathcal{D}
%   \end{equation}
% \end{tabularx}
% \begin{equation}
% \mathbf{A} \sim \mathcal{R}
% \end{equation}%an be inferetefece by the models. 
% for which make the cureent DNN suitable, AI technomguy becuase of its high efciiency and high . it is a non-voiman architecure, which we read the memroy cesls directily to obtain the result. 


% However, in0meoeyr coputing using RRAM decixse the memroy celss faces unstable noises, make it unstable and the weight fulcutwianed, which cause the model accuracy drop

% We propsoed a technquley to compenetate the noise and 

% %DNN is not suitaale for current von-namna architecure,

% suhch  when we compute the DNN, the m are seperatesed. T Such process is teigours, which  brought about  two major problmes. Firsta lagrre  and lager size models, we need to move the data in and out, the energy comsumes during dagta movement is huge. a lot of energy, Second, ti sand also a lagre delay. 


%z5Each element in A is either 0 or 1, depending on the staes of memory cells durng samping.

% the first   part is the determoned. thing.olied value rij is the cofrmed data whil thecoeffe and the ranmd and the seocnd part unprediaabtlane coefectc aijk  
% Tjerefore, in order to seperate the randwoms and non-randowms part of thematrc, we define two types of matrix.

% we have vector \textbf{$a_n$}, which is the vector for weight $w_n$
% where the ith elemetn in \textbf{$a_n$} is  $a_n$ is either 1 or 0, depeneding on if the correct weight apperas.
%Therefore the weight read from the resitor wij can be consider as the , the vweigth we obatained depends on its currenet states.the resistor can be in any random state, each state have its unique value..conducetance of the resiators, shown in Eequation x return  a differetn data, donetaed by wij. The exact vlauye depends on the random state of the resistorinstead of the pre-setted weight wij. The exqact value of wij can have many possble ,depents on the staate of the resistor when we read. which is expressed in equationa,x it can be n any steate, each state has a differet weight value.The resistor be in a radnsowm state, each staaet hagve a differetn weitghit will return wij instead.It can be in one of the m states. Any time we read from the Hecec, we can express wij as  a linear combination of all possible weight read from  , shown in Equaiton x.
%For contialial lyaers, the operations can be transferrsed to. For the linear layers, The input X and output Y are one dimesntonal vectors. 
% \begin{equation}
% \mathbf{Y}  = f(\mathbf{X}) = \widetilde{\mathbf{W}}\mathbf{X}+\mathbf{B} 
% \label{e:whole}
% \end{equation}
%\subsection{Mathematical Prove
%we can prove the effecitveness of adapative trainibg on an arbritary neuerl network modelsThe development of DNN bring problems in its compiutaiton.

%with such noise, shcem, which we furether reduce the noise of enenrgy consumptinin  the xxx can be wile the enegry consumoption nor
% We add a a new energy regualziaoitn term in the loos funciton. D
% In this equation, the weight matrix $\mathbf{W}$ and bias matrix $\mathbf{B}$ are both trainable parameters.
% uring trainng 

% aamreter scale 
%In addtion to dataset and models an extra infomeraiton called noise input is which reflect the real ocharatszticssos  of in-memoru computing devcices,  SecondlyUsally, we need the dastase tnad mode for trainng, in our adapativetraing , we need a, %this could help the model be  the loss function inlcude daaset and models. %We proposed three methods, shown in red text,in our deisgn. he first thing is adaoative training, in addtin to dataset and model, we include an extral noise module into the loss function, which make the model be more adapative to the noise. The secodn thing is , where we add anotther terms to the loss funciton, the therid rhing is the  where the daaset and models are need to gothrough this encoding scheme.
% whcih a speical  to lower the ennergy cosuption by fine uruning the wieghts and at the same time keeps the accuracy to be high.

%A mtheod to compenat the noised in memoru clels, by inclduing random noises data as a trainig datasset. 
%for in-memoru comptuing devices, there are two majoor advantgaes. First, both the mutlopers and the add operatieds are emebeded into the read operitons.
%This is one of the major reaons why in-emmeor computinh is effienct than traditnal computinhg machnisms.



%There are two major reaonss why in-memru computing is efficency taht traacial comptuing machnimes. 


%ther ei snot need to do mitlopleds and there is no need to do adders.
%The learned distrunbutin will also be a goupd of pixcels aligned inthe center. 
%The key idea in the environment enhanced dataset is to inc
 %This is because the real fluctuation data is stochastic, which reflect the practical behaviours of memory cells. When we do inference, as lude the fluctuation data as an input to the training process. Otherwise, the model will face serious accuracy drop. We visualized this process in Fig.~\ref{f:process_path2}. In sub-figure (a) and (b), we train the model using only dataset $X$, which do not consider the environment. The fluctuation data $S$ is in its default value, indicating that the states of the memory cells are unknown to us. Hence, all the pixels in the fake noise $S$ is aligned in the center of the matrix, which is a deterministic value. During training, the models will overfit on this regular noise data $S$. As we see from sub-figure (c), when we do inference, since the memory cells now have real noises (purple color), which are quite different from the distribution of fluctuation we used in the training data (original color), the model will misclassify the images.
%On the contrary, as we show in Fig.~\ref{f:process_path2}, if we include the noises data $S$ into the training process, the overfit will not happen. This is because the real fluctuation data is stochastic, which reflect the practical behaviours of memory cells. When we do inference, as the noise from the memory cells (purple) must follow the distribution of fluctuations in the training data (orange), the model can accurately classify the images.





% expressed in Equation~(\ref{e:space}), Space $\widetilde{\mathcal{Z}}$ is  and a new data $\widetilde{\mathbf{Z}}\in \widetilde{\mathcal{Z}}$, expressed in Equation~(\ref{e:parameter}).  Similarly, the data $\widetilde{\mathbf{Z}}$ is the 
%As we defined in Section~\ref{s:risk}, space $\mathcal{Z}$ is the combination of space $\mathcal{X}$ and space $\mathcal{Y}$. Hence, we can also express space $\widetilde{\mathcal{Z}}$ as the combination of space $\mathcal{Z}$ and $\mathcal{S}$.
%If we use the classical hardware accelerators to compute the model, we can obtain a precise output matrix $\mathbf{Y}$ using Equation~(\ref{e:whole}).
%due to the fluctuation of weights returned from the memory cells.  
%Therefore, 
%However, if we use the in-memory processing hardware to compute the model,

%  average noises of both encoding schemes are the same because the mean value of the noise However,
%  only depends on the physical characteristics of the memory cell.
% As the name indicate, unless at each time step, the noises are all positive or all negative.
% As we can see
%Next, we define $O_{new}$ as the accumulated output of the memory cell using. Its standard deviations can be expressed in . 
%First, we define $O_{trad}$ as the output of the memory cells using the traditional encoding scheme.

%We can mathematically prove this.the sd of x is ,and the sd of x
%The training process for traditional deep learning is shown in Fig.~\ref{f:overview_trad}. Mathematically, the loss function  takes the dataset and the model as the input, and outputs a real value indicating the distance between current values of model parameters and their theoretical optimized values. To reduce this distance, the optimizer do gradient decent on the lose function and then update the parameters in the model. This process would iterate many epochs until an optimal set of parameters can be found and the lose function is minimized. We have the following notations for the dataset and model. Let $\mathcal{X}$ and $\mathcal{Y}$ be the space for the images data $\textbf{X}$ and labels data $\textbf{Y}$, respectively. Denote $Z= \mathcal{X} \times \mathcal{X}$ be the the combination of space $\mathcal{X}$ and space $\mathcal{Y}$, and data $\textbf{Z}=(\textbf{X},\textbf{Y})$ be the combination of data $\textbf{X}$ and data $\textbf{Y}$. Let $\mathfrak{D}$ be the unknown distribution of data $\textbf{Z}$ on space $\mathcal{Z}$. Let function class $\mathcal{F}\subset \mathcal{Y}^\mathcal{X}$ be the searching space of model function, expressed in Equation~(\ref{e:whole}) (weight $\mathbf{W}$ and bias $\mathbf{B}$ are both function parameters). Given the above definitions and notations, the loss function can be defined as $\mathcal{L}:\mathcal{F} \times \mathcal{Z} \rightarrow \mathbb{R}$, \textit{i.e.}, the mapping from the combination of space $\mathcal{F}$ and space $\mathcal{Z}$ to the real number space $\mathbb{R}$.
%reach. %We wil show the details ofin later expermimetns.%The state ofhthe art is free to choose rou%Here we imrpve The minmuma energyc required to recover the accuracy loss cause by the in-memoru deep learningl compared wNoted that the state-of-the-art donnot have the same accuracy as the baseline accurqayc, and we find the min,uam enrgy that can reach its highest accurayc.%artucaly the energy reguation temerm can make a better trade ofof betwee of $\rho$%%f strong flcutn   are not persnat phusically, Hecen ,it is improant to show the robubust of our soluaotn under difefetn envrtonel impacets. %although our soltiuions demands higher energy consumptin, the statepf-the art demands even higher energy cosnumotnio.%The models under the strong flucnations interhsnteisy level% Although under sroing flucations, we need larger eenergy consuymption to improve the acccurac, the state-of-the-art also requres larger energy consumotn. This experiments well prove the robuses of our solusitn.  %The energy consumotion is positively correlated to the intesitsy of flcutiaonts.  both our method and the s More, .%This is beacsue they choose higher energy effoceint $\rho$ to lower the impact of weight flucations.% . we model The stensgth of the environemtnal flucations under the noraml settings We show t buy they are hard to reach the baseline.
%he energy consumptino under different stenght of environmetal flucations ours and the stateof the art.  }

%In hardware accelerators for in-memory computing, the reading energy of the memory cell is proportional to the value of the input data.
%stored in the memory cell The output from the memory cell is represented by a square . The value of the data is represented by the length of the bar.is the product of the activation $x$ and the weight $w$.A new computation method is applied in our low-noise encoding scheme. As Equation~(\ref{e:accum}) shows,  At each time step, we only input a fraction of the data x to the memory cell, and accumulate the output results from every time step. At the first time step, we input activation $1$, the output $1W$ is scaled by the factor of $2^0$ and the accumulated result is $1w$ . At the second time step, we input activation $1$, the output $1w$ is scaled by a factor $2^1$, and the accumulated  result is $3W$. After the third time step